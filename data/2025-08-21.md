<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.CV](#cs.CV) [Total: 73]
- [cs.AI](#cs.AI) [Total: 10]
- [cs.LG](#cs.LG) [Total: 73]
- [cs.NI](#cs.NI) [Total: 12]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [astro-ph.SR](#astro-ph.SR) [Total: 2]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.IR](#cs.IR) [Total: 5]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.SE](#cs.SE) [Total: 4]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [From Image Captioning to Visual Storytelling](https://arxiv.org/abs/2508.14045)
*Admitos Passadakis,Yingjin Song,Albert Gatt*

Main category: cs.CL

TL;DR: 本文提出一种新颖的视觉故事生成方法，将故事生成视为图像描述的超集，通过先生成图像描述再转化为连贯故事，有效提升了故事质量，缩短了训练时间，并引入了新的评估指标“理想度”。


<details>
  <summary>Details</summary>
Motivation: 视觉故事生成是一项具有挑战性的多模态任务，需要故事既能扎根于图像序列，又具有叙事性和连贯性。现有方法难以平衡这些方面，本文旨在通过一种不同于以往研究的方法来解决这一挑战。

Method: 本研究将视觉故事生成视为图像描述的超集。首先，利用视觉到语言模型获取输入图像的描述；然后，使用语言到语言的方法将这些描述转化为连贯的叙事。

Result: 将图像描述和故事生成整合到一个统一框架中，显著提升了生成故事的质量。此外，该方法加速了训练时间，并使框架更易于复用和复现。本文还提出了一种名为“理想度”的新度量工具，用于模拟视觉故事生成中与“预言家模型”的差距和类人度。

Conclusion: 本文提出的两阶段方法（先描述后叙事）有效平衡了视觉故事生成的扎根性和连贯性，提升了故事质量和训练效率。新引入的“理想度”度量工具也为评估视觉故事生成的人性化程度提供了有价值的手段，使该框架具有广泛的应用前景。

Abstract: Visual Storytelling is a challenging multimodal task between Vision &
Language, where the purpose is to generate a story for a stream of images. Its
difficulty lies on the fact that the story should be both grounded to the image
sequence but also narrative and coherent. The aim of this work is to balance
between these aspects, by treating Visual Storytelling as a superset of Image
Captioning, an approach quite different compared to most of prior relevant
studies. This means that we firstly employ a vision-to-language model for
obtaining captions of the input images, and then, these captions are
transformed into coherent narratives using language-to-language methods. Our
multifarious evaluation shows that integrating captioning and storytelling
under a unified framework, has a positive impact on the quality of the produced
stories. In addition, compared to numerous previous studies, this approach
accelerates training time and makes our framework readily reusable and
reproducible by anyone interested. Lastly, we propose a new metric/tool, named
ideality, that can be used to simulate how far some results are from an oracle
model, and we apply it to emulate human-likeness in visual storytelling.

</details>


### [2] [Benchmarking Sociolinguistic Diversity in Swahili NLP: A Taxonomy-Guided Approach](https://arxiv.org/abs/2508.14051)
*Kezia Oketch,John P. Lalor,Ahmed Abbasi*

Main category: cs.CL

TL;DR: 首次对斯瓦希里语NLP进行基于分类法的评估，关注社会语言学多样性对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决斯瓦希里语NLP中社会语言学多样性评估的不足，并首次引入分类法指导的评估。

Method: 收集2170个来自肯尼亚讲者的斯瓦希里语自由文本数据集（包含部落影响、城市方言、语码混合和借词）；开发结构化分类法；利用该分类法评估预训练和指令调优语言模型的预测错误。

Result: 推动了以文化为基础的评估框架发展，并强调社会语言学变异对模型性能的关键作用。

Conclusion: 社会语言学变异显著影响模型性能，以分类法指导的文化背景评估对提高NLP模型鲁棒性至关重要。

Abstract: We introduce the first taxonomy-guided evaluation of Swahili NLP, addressing
gaps in sociolinguistic diversity. Drawing on health-related psychometric
tasks, we collect a dataset of 2,170 free-text responses from Kenyan speakers.
The data exhibits tribal influences, urban vernacular, code-mixing, and
loanwords. We develop a structured taxonomy and use it as a lens for examining
model prediction errors across pre-trained and instruction-tuned language
models. Our findings advance culturally grounded evaluation frameworks and
highlight the role of sociolinguistic variation in shaping model performance.

</details>


### [3] [Contrastive Analysis of Constituent Order Preferences Within Adverbial Roles in English and Chinese News: A Large-Language-Model-Driven Approach](https://arxiv.org/abs/2508.14054)
*Yiran Rex Ma*

Main category: cs.CL

TL;DR: 本研究基于LLM标注的英汉新闻语料库，分析了英汉新闻中状语性功能组块的语序差异。发现英文倾向核心信息先行后置状语，中文倾向背景先行前置状语，且语序兼具系统偏好与动态适应性。


<details>
  <summary>Details</summary>
Motivation: 探讨基于大型语言模型（LLM）标注的英汉可比新闻语料库中，英汉新闻在状语性功能组块的语序、典型位置偏好及分布模式上的差异。

Method: 利用经LLM标注的英汉可比新闻语料库，从状语性功能组块的视角，分析英汉新闻的语序差异、位置偏好和分布模式。

Result: 1. 英文新闻倾向核心信息先行，状语性功能组块多后置；中文新闻倾向背景先行，状语性功能组块常前置。
2. 在SVO结构中，中文前置状语倾向更显著，英文后置倾向相对温和。
3. 当功能组块共现时，英汉新闻均表现出高度灵活性，语序调整受信息和语用目的驱动。

Conclusion: 语序兼具系统性偏好和动态适应性。本研究为英汉信息结构的对比提供了新的实证支持。

Abstract: Based on comparable English-Chinese news corpora annotated by Large Language
Model (LLM), this paper attempts to explore the differences in constituent
order of English-Chinese news from the perspective of functional chunks with
adverbial roles, and analyze their typical positional preferences and
distribution patterns. It is found that: (1) English news prefers linear
narrative of core information first, and functional chunks are mostly
post-positioned, while Chinese news prefers overall presentation mode of
background first, and functional chunks are often pre-positioned; (2) In SVO
structure, both English and Chinese news show differences in the distribution
of functional chunks, but the tendency of Chinese pre-positioning is more
significant, while that of English post-positioning is relatively mild; (3)
When function blocks are co-occurring, both English and Chinese news show high
flexibility, and the order adjustment is driven by information and pragmatic
purposes. The study reveals that word order has both systematic preference and
dynamic adaptability, providing new empirical support for contrastive study of
English-Chinese information structure.

</details>


### [4] [T-REX: Table -- Refute or Entail eXplainer](https://arxiv.org/abs/2508.14055)
*Tim Luka Horstmann,Baptiste Geisenberger,Mehwish Alam*

Main category: cs.CL

TL;DR: 本文介绍T-REX，一个实时交互工具，利用先进的LLM使非专业人士也能进行多模态多语言表格的事实核查。


<details>
  <summary>Details</summary>
Motivation: 文本声明与结构化表格数据的事实核查是NLP中一项关键且具挑战性的任务，对现实世界影响广泛。尽管LLM在此领域取得进展，但现有解决方案对非专业用户不友好。

Method: 引入T-REX（Table -- Refute or Entail eXplainer），一个实时、交互式工具，利用最先进的指令微调推理LLM，对多模态、多语言表格进行声明验证。

Result: T-REX是首个此类工具，它旨在提高准确性和透明度，使非专业用户能够使用先进的事实核查技术。该系统已在线公开可用。

Conclusion: T-REX通过提供易于访问的先进事实核查技术，赋能非专业用户对表格数据进行准确和透明的声明验证。

Abstract: Verifying textual claims against structured tabular data is a critical yet
challenging task in Natural Language Processing with broad real-world impact.
While recent advances in Large Language Models (LLMs) have enabled significant
progress in table fact-checking, current solutions remain inaccessible to
non-experts. We introduce T-REX (T-REX: Table -- Refute or Entail eXplainer),
the first live, interactive tool for claim verification over multimodal,
multilingual tables using state-of-the-art instruction-tuned reasoning LLMs.
Designed for accuracy and transparency, T-REX empowers non-experts by providing
access to advanced fact-checking technology. The system is openly available
online.

</details>


### [5] [Confidence Estimation for Text-to-SQL in Large Language Models](https://arxiv.org/abs/2508.14056)
*Sepideh Entezari Maleki,Mohammadreza Pourreza,Davood Rafiei*

Main category: cs.CL

TL;DR: 本研究探讨了在无法访问标准答案时，如何评估大型语言模型（LLM）生成的Text-to-SQL查询的置信度。我们评估了黑盒（基于一致性）和白盒（SQL语法感知）方法，并发现基于执行的验证能有效提升两者性能。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLM）生成的Text-to-SQL查询中，在无法访问正确答案且模型权重和梯度受限的情况下，评估所生成SQL查询的可靠性是一个重要挑战。

Method: 本研究探索了黑盒和白盒两种置信度估计策略。在黑盒模型中，评估了基于一致性的方法；在白盒设置中，研究了SQL语法感知方法以解释LLM的logits。此外，将基于执行的查询验证作为有价值的补充信号，以提高上述两种方法的有效性。

Result: 评估结果表明，在黑盒模型中，基于一致性的方法表现出卓越的性能。在白盒设置中，SQL语法感知方法在解释LLM logits方面具有优势。此外，基于执行的查询验证提供了一个有价值的补充信号，显著提高了黑盒和白盒两种方法的有效性。

Conclusion: 本研究为大型语言模型在Text-to-SQL任务中的置信度估计提供了有效策略，强调了基于一致性方法、SQL语法感知方法的重要性，并指出基于执行的查询验证是一个有力的补充信号。

Abstract: Confidence estimation for text-to-SQL aims to assess the reliability of
model-generated SQL queries without having access to gold answers. We study
this problem in the context of large language models (LLMs), where access to
model weights and gradients is often constrained. We explore both black-box and
white-box confidence estimation strategies, evaluating their effectiveness on
cross-domain text-to-SQL benchmarks. Our evaluation highlights the superior
performance of consistency-based methods among black-box models and the
advantage of SQL-syntax-aware approaches for interpreting LLM logits in
white-box settings. Furthermore, we show that execution-based grounding of
queries provides a valuable supplementary signal, improving the effectiveness
of both approaches.

</details>


### [6] [Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models](https://arxiv.org/abs/2508.14062)
*Badrinath Ramakrishnan,Akshaya Balaji*

Main category: cs.CL

TL;DR: 本文分析了微调LLM的数据记忆隐私风险，并提出了一个多层隐私保护框架，能有效将数据泄露率降至0%并保持高模型效用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在微调过程中容易记忆训练数据，导致严重的隐私泄露风险。

Method: 通过对GPT-2、Phi-3和Gemma-2进行受控实验，分析了微调LLM的数据记忆现象。提出并评估了四种互补的隐私保护方法：语义数据去重、生成过程中的差分隐私、基于熵的过滤和基于模式的内容过滤。

Result: 微调重复敏感数据使隐私泄露率从0-5%增至60-75%（平均增长64.2%）。所提方法能将数据泄露率降至0%，同时保持94.7%的模型效用。

Conclusion: 微调LLM的数据记忆问题带来显著隐私风险，而本文提出的多层隐私保护框架能有效解决此问题，在消除数据泄露的同时最大限度地保留模型实用性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse natural language processing tasks, but their tendency to memorize
training data poses significant privacy risks, particularly during fine-tuning
processes. This paper presents a comprehensive empirical analysis of data
memorization in fine-tuned LLMs and introduces a novel multi-layered privacy
protection framework. Through controlled experiments on modern LLM
architectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that
fine-tuning with repeated sensitive data increases privacy leakage rates from
baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across
tested models. We propose and rigorously evaluate four complementary privacy
protection methods: semantic data deduplication, differential privacy during
generation, entropy-based filtering, and pattern-based content filtering. Our
experimental results show that these techniques can reduce data leakage to 0%
while maintaining 94.7% of original model utility.

</details>


### [7] [Punctuation and Predicates in Language Models](https://arxiv.org/abs/2508.14067)
*Sonakshi Chauhan,Maheep Chaudhary,Koby Choy,Samuel Nellessen,Nandi Schoots*

Main category: cs.CL

TL;DR: 本文探究了大型语言模型（LLM）中信息收集与传播机制，重点分析了标点符号的重要性及其在不同模型间的差异，并研究了LLM对不同推理规则的处理方式。


<details>
  <summary>Details</summary>
Motivation: 探究LLM中信息如何收集与在层间传播。具体地，研究标点符号的计算重要性及其对模型性能的必要性和充分性。此外，还旨在了解LLM如何处理输入的不同组成部分以及不同推理规则。

Method: 使用基于干预的技术，评估标点符号在GPT-2、DeepSeek和Gemma模型中对性能的必要性和充分性。通过交换干预和层交换实验，探究LLM对条件语句（if, then）和全称量化（for all）等不同推理规则的处理方式。

Result: 研究结果显示，标点符号的重要性存在显著的模型特异性：对于GPT-2，标点符号在多层中既必要又充分；而在DeepSeek中重要性远低于GPT-2；在Gemma中则完全不重要。此外，LLM对条件语句和全称量化等不同推理规则的处理方式差异很大。

Conclusion: 本研究为LLM中标点符号的使用和推理的内部机制提供了新见解，并对模型的可解释性具有启示意义。

Abstract: In this paper we explore where information is collected and how it is
propagated throughout layers in large language models (LLMs). We begin by
examining the surprising computational importance of punctuation tokens which
previous work has identified as attention sinks and memory aids. Using
intervention-based techniques, we evaluate the necessity and sufficiency (for
preserving model performance) of punctuation tokens across layers in GPT-2,
DeepSeek, and Gemma. Our results show stark model-specific differences: for
GPT-2, punctuation is both necessary and sufficient in multiple layers, while
this holds far less in DeepSeek and not at all in Gemma. Extending beyond
punctuation, we ask whether LLMs process different components of input (e.g.,
subjects, adjectives, punctuation, full sentences) by forming early static
summaries reused across the network, or if the model remains sensitive to
changes in these components across layers. Extending beyond punctuation, we
investigate whether different reasoning rules are processed differently by
LLMs. In particular, through interchange intervention and layer-swapping
experiments, we find that conditional statements (if, then), and universal
quantification (for all) are processed very differently. Our findings offer new
insight into the internal mechanisms of punctuation usage and reasoning in LLMs
and have implications for interpretability.

</details>


### [8] [DLLMQuant: Quantizing Diffusion-based Large Language Models](https://arxiv.org/abs/2508.14090)
*Chen Xu,Dawei Yang*

Main category: cs.CL

TL;DR: 本文提出DLLMQuant，一个为扩散式大语言模型（DLLM）定制的后训练量化（PTQ）框架，旨在解决现有PTQ方法在DLLM上精度下降和泛化性能差的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散式大语言模型（DLLM）在非自回归文本生成方面前景广阔，但其庞大的模型尺寸和高计算成本限制了部署。现有针对大型语言模型（LLM）的后训练量化（PTQ）方法直接应用于DLLM时，会导致严重的精度下降和泛化性能降低。本文旨在探究并解决DLLM的关键机制（动态掩码、迭代生成、双向注意力）与量化之间的冲突。

Method: 本文提出DLLMQuant，一个为DLLM量身定制的PTQ框架，包含三项创新技术：1) **时间-掩码自适应采样 (TMAS)**：一种新的校准方法，考虑时间和掩码因素，以捕获不同时间步的分布。2) **交互感知激活量化 (IA-AQ)**：利用双向注意力的交互信号动态分配量化资源。3) **确定性引导量化 (CGQ)**：将掩码状态和token分数作为关键权重标准集成到误差补偿中，使权重量化更适合DLLM。

Result: 实验结果表明，DLLMQuant在显著提升性能的同时增强了效率。

Conclusion: DLLMQuant有效解决了现有PTQ方法在DLLM上的局限性，通过定制化的策略克服了DLLM机制与量化间的冲突，实现了DLLM的性能提升和效率增强。

Abstract: Diffusion-based large language models (DLLMs) have shown promise for
non-autoregressive text generation, but their deployment is constrained by
large model sizes and heavy computational costs. Post-training quantization
(PTQ), a widely used method for compressing and accelerating Large Language
Models (LLMs), suffers from severe accuracy degradation and reduced
generalization performance when directly applied to DLLMs (e.g., AWQ suffers a
16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key
mechanisms - dynamic masking, iterative generation, bidirectional attention -
clash with quantization. We identify three core issues: 1) Iterative generation
and dynamic masking ratios lead to distinct token distributions across decoding
steps, which are not adequately captured by existing PTQ calibration methods;
2) Quantization errors are accumulated and amplified progressively during
iteration in DLLMs, causing quantized models to perform worse as decoding steps
progress; 3) Unmasked tokens stabilize while masked remain probabilistic,
making overall feature distribution incompatible with existing PTQ methods. To
address these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs,
which incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling
(TMAS), a calibration method that accounts for both time and mask factors, with
the capacity to capture distributions across timesteps. 2) Interaction-Aware
Activation Quantization (IA-AQ), which utilizes bidirectional attention's
interaction signals to dynamically allocate quantization resources. 3)
Certainty-Guided Quantization (CGQ), which integrates mask status and token
scores as key weighting criteria into error compensation, making weight
quantization more suitable for DLLMs. Experiments show that DLLMQuant achieves
significant performance gains while enhancing efficiency.

</details>


### [9] [MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation](https://arxiv.org/abs/2508.14146)
*Xian Gao,Jiacheng Ruan,Zongyun Zhang,Jingsheng Gao,Ting Liu,Yuzhuo Fu*

Main category: cs.CL

TL;DR: 针对LLMs在同行评审中缺乏统一且支持多模态内容的评估基准，本文提出了MMReview，一个涵盖多学科和多模态的综合性基准，旨在全面评估LLMs和多模态LLMs的评审能力。


<details>
  <summary>Details</summary>
Motivation: 随着学术出版物快速增长，同行评审耗时且重要。尽管LLMs已被用于辅助评审意见生成，但现有LLM驱动的评审任务缺乏统一的评估基准，无法严格评估模型生成全面、准确且与人类对齐的评审能力，尤其是在处理包含图表的多模态内容时。

Method: 本文提出了MMReview，一个跨越多学科和多模态的综合性基准。MMReview包含来自240篇论文的多模态内容和专家评审意见，涵盖四大主要学术领域的17个研究方向。该基准设计了13项任务，分为四大核心类别，用于评估LLMs和多模态LLMs在逐步评审生成、结果制定、与人类偏好对齐以及对抗性输入鲁棒性方面的表现。

Result: 对16个开源模型和5个先进闭源模型进行的大量实验证明了MMReview基准的全面性和有效性。

Conclusion: MMReview被视为建立自动化同行评审系统标准化基础的关键一步。

Abstract: With the rapid growth of academic publications, peer review has become an
essential yet time-consuming responsibility within the research community.
Large Language Models (LLMs) have increasingly been adopted to assist in the
generation of review comments; however, current LLM-based review tasks lack a
unified evaluation benchmark to rigorously assess the models' ability to
produce comprehensive, accurate, and human-aligned assessments, particularly in
scenarios involving multimodal content such as figures and tables. To address
this gap, we propose \textbf{MMReview}, a comprehensive benchmark that spans
multiple disciplines and modalities. MMReview includes multimodal content and
expert-written review comments for 240 papers across 17 research domains within
four major academic disciplines: Artificial Intelligence, Natural Sciences,
Engineering Sciences, and Social Sciences. We design a total of 13 tasks
grouped into four core categories, aimed at evaluating the performance of LLMs
and Multimodal LLMs (MLLMs) in step-wise review generation, outcome
formulation, alignment with human preferences, and robustness to adversarial
input manipulation. Extensive experiments conducted on 16 open-source models
and 5 advanced closed-source models demonstrate the thoroughness of the
benchmark. We envision MMReview as a critical step toward establishing a
standardized foundation for the development of automated peer review systems.

</details>


### [10] [DPad: Efficient Diffusion Language Models with Suffix Dropout](https://arxiv.org/abs/2508.14148)
*Xinhua Chen,Sitao Huang,Cong Guo,Chiyue Wei,Yintao He,Jianyi Zhang,Hai "Hellen" Li,Yiran Chen*

Main category: cs.CL

TL;DR: 提出DPad，一种无训练方法，通过限制注意力范围显著提升扩散式大语言模型（dLLMs）的文本生成速度，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 扩散式大语言模型（dLLMs）在并行文本生成中存在计算开销大的问题，因为它们在每一步都会预测所有未来的后缀标记，但只保留其中一小部分。

Method: 本文提出Diffusion Scratchpad (DPad)，一种无需训练的方法，通过将注意力限制在附近一小组后缀标记来消除冗余。DPad整合了两种策略：(i) 滑动窗口，保持固定长度的后缀窗口；(ii) 距离衰减丢弃，在注意力计算前确定性地移除远距离后缀标记。该设计简单，兼容现有优化，且仅需少量代码即可实现。

Result: 在LLaDA-1.5和Dream模型上的多项基准测试表明，DPad相比于普通的dLLMs，可实现高达61.4倍的速度提升，同时保持相当的准确性。

Conclusion: DPad展示了其在高效和可扩展长序列推理方面的巨大潜力，有效解决了dLLMs的计算效率问题。

Abstract: Diffusion-based Large Language Models (dLLMs) parallelize text generation by
framing decoding as a denoising process, but suffer from high computational
overhead since they predict all future suffix tokens at each step while
retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a
training-free method that restricts attention to a small set of nearby suffix
tokens, preserving fidelity while eliminating redundancy. DPad integrates two
strategies: (i) a sliding window, which maintains a fixed-length suffix window,
and (ii) distance-decay dropout, which deterministically removes distant suffix
tokens before attention computation. This simple design is compatible with
existing optimizations such as prefix caching and can be implemented with only
a few lines of code. Comprehensive evaluations across multiple benchmarks on
LLaDA-1.5 and Dream models demonstrate that DPad delivers up to
$\mathbf{61.4\times}$ speedup over vanilla dLLMs while maintaining comparable
accuracy, highlighting its potential for efficient and scalable long-sequence
inference. Our code is available at https://github.com/Crys-Chen/DPad.

</details>


### [11] [Comparing energy consumption and accuracy in text classification inference](https://arxiv.org/abs/2508.14170)
*Johannes Zschache,Tilman Hartwig*

Main category: cs.CL

TL;DR: 研究大型语言模型（LLMs）推理阶段的能耗与精度权衡，发现高精度模型亦可节能，大型LLMs能耗高但精度低，能耗受多因素影响并与运行时间强相关。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在NLP任务中广泛部署，能效和可持续性问题日益突出。现有研究多关注模型训练阶段的能耗，而推理阶段的能耗则相对较少被关注。

Method: 本研究系统评估了文本分类推理任务中，多种模型架构与硬件配置下，模型精度与能耗之间的权衡关系，并进行了实证分析。

Result: 1. 精度表现最佳的模型也可能具有高能效。
2. 大型LLMs倾向于消耗更多能量，且分类精度更低。
3. 推理能耗存在显著差异（<mWh至>kWh），受模型类型、大小和硬件规格影响。
4. 推理能耗与模型运行时间之间存在强相关性，表明运行时间可作为能耗的实用替代指标。

Conclusion: 这些发现为可持续AI发展提供了可操作的见解，有助于研究人员、行业从业者和政策制定者在NLP应用中平衡性能与资源效率。

Abstract: The increasing deployment of large language models (LLMs) in natural language
processing (NLP) tasks raises concerns about energy efficiency and
sustainability. While prior research has largely focused on energy consumption
during model training, the inference phase has received comparatively less
attention. This study systematically evaluates the trade-offs between model
accuracy and energy consumption in text classification inference across various
model architectures and hardware configurations. Our empirical analysis shows
that the best-performing model in terms of accuracy can also be
energy-efficient, while larger LLMs tend to consume significantly more energy
with lower classification accuracy. We observe substantial variability in
inference energy consumption ($<$mWh to $>$kWh), influenced by model type,
model size, and hardware specifications. Additionally, we find a strong
correlation between inference energy consumption and model runtime, indicating
that execution time can serve as a practical proxy for energy usage in settings
where direct measurement is not feasible. These findings have implications for
sustainable AI development, providing actionable insights for researchers,
industry practitioners, and policymakers seeking to balance performance and
resource efficiency in NLP applications.

</details>


### [12] [Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper](https://arxiv.org/abs/2508.14273)
*Krishna Garg,Firoz Shaikh,Sambaran Bandyopadhyay,Cornelia Caragea*

Main category: cs.CL

TL;DR: 本文引入SciIG任务评估大语言模型生成科研论文引言的能力，发现LLaMA-4 Maverick表现最佳，且三样本提示效果优于少样本。


<details>
  <summary>Details</summary>
Motivation: 随着研究人员日益将大型语言模型（LLMs）用作写作助手，生成高质量的科研论文引言仍然充满挑战且至关重要。

Method: 引入“科学引言生成”（SciIG）任务，评估LLMs根据标题、摘要和相关工作生成连贯引言的能力。构建了NAACL 2025和ICLR 2025论文新数据集，评估了五种最先进模型（包括开源的DeepSeek-v3、Gemma-3-12B、LLaMA 4-Maverick、MistralAI Small 3.1和闭源的GPT-4o）。评估维度包括词汇重叠、语义相似性、内容覆盖、忠实性、一致性、引用正确性和叙事质量，结合自动化指标和LLM作为评委的评估方法。

Result: LLaMA-4 Maverick在大多数指标上表现优越，尤其在语义相似性和忠实性方面。此外，三样本提示方法（three-shot prompting）持续优于少样本方法。

Conclusion: 这些发现为开发有效的科研写作助手提供了实用见解，并为LLM辅助学术写作设定了现实预期。为促进可复现性和未来研究，将公开发布所有代码和数据集。

Abstract: As researchers increasingly adopt LLMs as writing assistants, generating
high-quality research paper introductions remains both challenging and
essential. We introduce Scientific Introduction Generation (SciIG), a task that
evaluates LLMs' ability to produce coherent introductions from titles,
abstracts, and related works. Curating new datasets from NAACL 2025 and ICLR
2025 papers, we assess five state-of-the-art models, including both open-source
(DeepSeek-v3, Gemma-3-12B, LLaMA 4-Maverick, MistralAI Small 3.1) and
closed-source GPT-4o systems, across multiple dimensions: lexical overlap,
semantic similarity, content coverage, faithfulness, consistency, citation
correctness, and narrative quality. Our comprehensive framework combines
automated metrics with LLM-as-a-judge evaluations. Results demonstrate LLaMA-4
Maverick's superior performance on most metrics, particularly in semantic
similarity and faithfulness. Moreover, three-shot prompting consistently
outperforms fewer-shot approaches. These findings provide practical insights
into developing effective research writing assistants and set realistic
expectations for LLM-assisted academic writing. To foster reproducibility and
future research, we will publicly release all code and datasets.

</details>


### [13] [Disentangling concept semantics via multilingual averaging in Sparse Autoencoders](https://arxiv.org/abs/2508.14275)
*Cliff O'Reilly,Ernesto Jimenez-Ruiz,Tillman Weyde*

Main category: cs.CL

TL;DR: 提出一种通过平均多语言概念激活来隔离LLM中概念语义的方法，该方法能更准确地揭示本体类间的真实关系，有望提升LLM内部状态的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM中通过嵌入和稀疏自编码器表示的文本语义与语法及语言特定信息纠缠，难以纯粹捕捉概念语义，阻碍了LLM与形式化知识表示和推理的有效结合。

Method: 从OWL本体类生成英、法、中多语言文本，输入Gemma 2B LLM，利用Gemma Scope稀疏自编码器提取概念激活，并通过平均多语言激活获得“概念平均值”，最后与本体类间真实映射进行关联验证。

Result: 研究结果强烈表明，与单一语言相比，通过多语言激活平均得到的“概念平均值”能更好地与本体类间的真实关系对齐。

Conclusion: 该方法为LLM内部网络状态的机制性解释提供了更高精度的新技术可能性。

Abstract: Connecting LLMs with formal knowledge representation and reasoning is a
promising approach to address their shortcomings. Embeddings and sparse
autoencoders are widely used to represent textual content, but the semantics
are entangled with syntactic and language-specific information. We propose a
method that isolates concept semantics in Large Langue Models by averaging
concept activations derived via Sparse Autoencoders. We create English text
representations from OWL ontology classes, translate the English into French
and Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the
open source Gemma Scope suite of Sparse Autoencoders, we obtain concept
activations for each class and language version. We average the different
language activations to derive a conceptual average. We then correlate the
conceptual averages with a ground truth mapping between ontology classes. Our
results give a strong indication that the conceptual average aligns to the true
relationship between classes when compared with a single language by itself.
The result hints at a new technique which enables mechanistic interpretation of
internal network states with higher accuracy.

</details>


### [14] [GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs](https://arxiv.org/abs/2508.14279)
*Adrian-Marius Dumitran,Alexandra-Mihaela Danila,Angela-Liliana Dumitran*

Main category: cs.CL

TL;DR: 研究者发布了GRILE基准测试，评估了大型语言模型在罗马尼亚语（低资源语言）语法推理和语言解释方面的表现，发现现有模型在解释准确性方面存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）革新了自然语言处理（NLP），但它们在低资源语言方面的教学价值尚不明确。因此，需要评估LLMs在低资源语言中选择正确答案和提供语言学准确解释的能力。

Method: 研究构建了GRILE，一个包含1,151道多项选择题的开放基准测试集，这些题目来源于罗马尼亚语的高风险考试。利用GRILE，评估了七种最先进的多语言和罗马尼亚语专用LLMs的两种能力：选择正确答案和生成语言学准确的解释。通过专家评审来评估解释的质量，并进行了详细的错误分析。

Result: Gemini 2.5 Pro达到了83%的准确率，但大多数开源模型的准确率低于65%。根据专家评审，其中48%的模型解释存在事实或教学上的缺陷。详细的错误分析指出了形态学和应用最新DOOM3正字法规范方面的系统性弱点。

Conclusion: 研究结果揭示了在低资源环境下可信赖的教育NLP所面临的开放挑战。GRILE被确立为一个用于可控解释生成和评估的新测试平台。

Abstract: LLMs (Large language models) have revolutionized NLP (Natural Language
Processing), yet their pedagogical value for low-resource languages remains
unclear. We present GRILE (Grammar Romanian Inference and Language
Explanations) , the first open benchmark of 1,151 multiple-choice questions
harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate,
university admissions). GRILE enables us to probe two complementary abilities
of seven state-of-the-art multilingual and Romanian-specific LLMs: (i)
selecting the correct answer, and (ii) producing linguistically accurate
explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight
models stay below 65%, and 48% of their explanations contain factual or
pedagogical flaws according to expert review. A detailed error analysis
pinpoints systematic weaknesses in morphology and in applying the latest DOOM3
orthographic norms. All data, code and a public web demo are released to
catalyze future research. Our findings expose open challenges for trustworthy
educational NLP in low-resource settings and establish GRILE as a new test-bed
for controllable explanation generation and evaluation.

</details>


### [15] [Tokens with Meaning: A Hybrid Tokenization Approach for NLP](https://arxiv.org/abs/2508.14292)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Sercan Karakaş,Banu Diri,Savaş Yıldırım,Demircan Çelik*

Main category: cs.CL

TL;DR: 本文提出了一种混合分词框架，结合了基于规则的形态学分析和统计子词分割，旨在为形态丰富的语言（如土耳其语）生成更具语言学意义的词元，并在基准测试中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有的子词分词方法（如BPE和WordPiece）主要依赖词频而非语言结构，因此在处理形态丰富和黏着语时效果不佳，导致分词结果难以体现语言学意义。

Method: 该方法采用混合分词框架，融合了基于规则的形态分析与统计子词分割。具体包括：音韵归一化、词根-词缀词典、一种平衡语素保留与词汇效率的新算法。它为音韵变体词缀（如-ler和-lar）及变形词根（如kitap与kitabı）分配共享标识符，并引入特殊词元标记空格和大小写（含UPPERCASE标记）。同时，整合BPE以处理未登录词，但不损害形态连贯性。

Result: 在TR-MMLU基准测试中，该分词器实现了最高的土耳其语词元百分比（90.29%）和纯词元百分比（85.8%）。与LLaMA、Gemma和GPT的分词器相比，其生成的词元更具语言学意义和连贯性。

Conclusion: 尽管主要针对土耳其语进行演示，但该方法具有语言无关性，可适应其他语言，为构建更具可解释性和高效的多语言自然语言处理系统提供了实用途径。

Abstract: Tokenization plays a pivotal role in natural language processing (NLP),
shaping how text is segmented and interpreted by language models. While subword
methods such as Byte Pair Encoding (BPE) and WordPiece have been effective,
they often struggle with morphologically rich and agglutinative languages
because they rely on frequency rather than linguistic structure. We introduce a
hybrid tokenization framework that combines rule-based morphological analysis
with statistical subword segmentation. The method uses phonological
normalization, root-affix dictionaries, and a novel algorithm that balances
morpheme preservation with vocabulary efficiency. It assigns shared identifiers
to phonologically variant affixes (e.g., -ler and -lar) and altered root forms
(e.g., kitap vs. kitab{\i}), reducing redundancy while maintaining semantic
integrity. Special tokens are added for whitespace and case, including an
UPPERCASE marker to avoid vocabulary inflation from capitalization. BPE is
integrated for out-of-vocabulary coverage without harming morphological
coherence. On the TR-MMLU benchmark, the tokenizer achieves the highest Turkish
Token Percentage (90.29\%) and Pure Token Percentage (85.8\%). Comparisons with
tokenizers from LLaMA, Gemma, and GPT show more linguistically meaningful and
coherent tokens. Although demonstrated on Turkish, the approach is
language-independent and adaptable to other languages, offering a practical
path toward more interpretable and effective multilingual NLP systems.

</details>


### [16] [A Joint Multitask Model for Morpho-Syntactic Parsing](https://arxiv.org/abs/2508.14307)
*Demian Inostroza,Mel Mistica,Ekaterina Vylomova,Chris Guest,Kemal Kurniawan*

Main category: cs.CL

TL;DR: 一个联合多任务模型在UniDive 2025形态句法解析共享任务中取得最佳表现，能够同时预测形态和句法分析。


<details>
  <summary>Details</summary>
Motivation: 参与UniDive 2025形态句法解析共享任务，旨在开发一个鲁棒系统，以预测新的UD标注方案下的形态和句法分析。

Method: 该系统采用共享的XLM-RoBERTa编码器，并配备三个专门的解码器，分别用于内容词识别、依存句法解析和形态句法特征预测。

Result: 该模型在涵盖九种不同类型语言的共享任务排行榜上取得了最佳整体表现，平均MSLAS为78.7%，LAS为80.1%，Feats F1为90.3%。消融研究表明，匹配任务的金标准分词和内容词识别对模型性能至关重要。错误分析揭示，模型在处理核心语法格（特别是主格-宾格）和跨语言的名词特征方面存在困难。

Conclusion: 所提出的联合多任务模型在UniDive 2025共享任务中表现出色，位居榜首，但在处理核心语法格和名词特征方面仍有改进空间。

Abstract: We present a joint multitask model for the UniDive 2025 Morpho-Syntactic
Parsing shared task, where systems predict both morphological and syntactic
analyses following novel UD annotation scheme. Our system uses a shared
XLM-RoBERTa encoder with three specialized decoders for content word
identification, dependency parsing, and morphosyntactic feature prediction. Our
model achieves the best overall performance on the shared task's leaderboard
covering nine typologically diverse languages, with an average MSLAS score of
78.7 percent, LAS of 80.1 percent, and Feats F1 of 90.3 percent. Our ablation
studies show that matching the task's gold tokenization and content word
identification are crucial to model performance. Error analysis reveals that
our model struggles with core grammatical cases (particularly Nom-Acc) and
nominal features across languages.

</details>


### [17] [Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency](https://arxiv.org/abs/2508.14314)
*Aman Goel,Daniel Schwartz,Yanjun Qi*

Main category: cs.CL

TL;DR: Finch-Zk是一个黑盒框架，通过跨模型一致性检查来检测并纠正大型语言模型（LLMs）的幻觉，无需外部知识源，并在幻觉检测和回答准确性方面取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）尽管能力强大，但存在生成看似合理却包含事实错误的“幻觉”问题，这影响了其可靠性。

Method: 提出Finch-Zk框架，该框架是一个黑盒解决方案，不依赖外部知识源。它包含两大创新：1) 跨模型一致性检查策略：通过比较不同LLM对语义等效提示的响应来识别细粒度不准确性。2) 目标性缓解技术：对问题片段进行精确修正，同时保留正确内容。

Result: 在FELM数据集上，Finch-Zk将幻觉检测的F1分数比现有方法提高了6-39%。在GPQA-diamond数据集上，应用于Llama 4 Maverick和Claude 4 Sonnet等模型时，Finch-Zk使回答准确性提高了7-8个百分点。

Conclusion: Finch-Zk是一个实用且可部署的解决方案，能够有效增强生产环境中LLM系统的实际可靠性和事实准确性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, but they remain susceptible to hallucinations--generating
content that appears plausible but contains factual inaccuracies. We present
Finch-Zk, a black-box framework that leverages FINe-grained Cross-model
consistency to detect and mitigate Hallucinations in LLM outputs without
requiring external knowledge sources. Finch-Zk introduces two key innovations:
1) a cross-model consistency checking strategy that reveals fine-grained
inaccuracies by comparing responses generated by diverse models from
semantically-equivalent prompts, and 2) a targeted mitigation technique that
applies precise corrections to problematic segments while preserving accurate
content. Experiments on the FELM dataset show Finch-Zk improves hallucination
detection F1 scores by 6-39\% compared to existing approaches. For mitigation,
Finch-Zk achieves 7-8 absolute percentage points improvement in answer accuracy
on the GPQA-diamond dataset when applied to state-of-the-art models like Llama
4 Maverick and Claude 4 Sonnet. Extensive evaluation across multiple models
demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for
enhancing factual reliability in production LLM systems.

</details>


### [18] [SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing](https://arxiv.org/abs/2508.14317)
*Jing Chen,Zhiheng Yang,Yixian Shen,Jie Liu,Adam Belloum,Chrysa Papagainni,Paola Grosso*

Main category: cs.CL

TL;DR: 本文提出SurveyGen-I框架，通过结合粗细粒度检索、自适应规划和记忆引导生成，解决了现有大语言模型在生成综述时连贯性差和引文覆盖不足的问题，并在实验中展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的综述生成方法在处理长篇多节综述时，难以保持内容连贯性，且引文覆盖不够全面。

Method: 引入SurveyGen-I框架，该框架结合了粗粒度到细粒度检索、自适应规划和记忆引导生成。具体方法包括：首先进行综述级检索以构建初始大纲和写作计划；在生成过程中通过存储已写内容和术语的记忆机制动态优化，确保子节间的连贯性；当上下文不足时，触发细粒度子节级检索。

Result: 在四个科学领域的实验结果表明，SurveyGen-I在内容质量、一致性和引文覆盖方面持续优于现有的自动综述生成方法。

Conclusion: SurveyGen-I框架成功解决了现有LLM综述生成方法的局限性，显著提升了自动生成综述的质量、连贯性和引文完整性。

Abstract: Survey papers play a critical role in scientific communication by
consolidating progress across a field. Recent advances in Large Language Models
(LLMs) offer a promising solution by automating key steps in the
survey-generation pipeline, such as retrieval, structuring, and summarization.
However, existing LLM-based approaches often struggle with maintaining
coherence across long, multi-section surveys and providing comprehensive
citation coverage. To address these limitations, we introduce SurveyGen-I, an
automatic survey generation framework that combines coarse-to-fine retrieval,
adaptive planning, and memory-guided generation. SurveyGen-I first performs
survey-level retrieval to construct the initial outline and writing plan, and
then dynamically refines both during generation through a memory mechanism that
stores previously written content and terminology, ensuring coherence across
subsections. When the system detects insufficient context, it triggers
fine-grained subsection-level retrieval. During generation, SurveyGen-I
leverages this memory mechanism to maintain coherence across subsections.
Experiments across four scientific domains demonstrate that SurveyGen-I
consistently outperforms previous works in content quality, consistency, and
citation coverage.

</details>


### [19] [Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever](https://arxiv.org/abs/2508.14323)
*Yixin Chen,Ying Xiong,Shangyu Wu,Yufei Cui,Xue Liu,Nan Guan,Chun Jason Xue*

Main category: cs.CL

TL;DR: 本文提出行为对齐检索器（BAR），通过提供行为一致的示例，显著提高工具增强型LLM的函数调用准确性，降低错误率。


<details>
  <summary>Details</summary>
Motivation: 工具增强型LLM的函数调用不准确会导致效率和成本问题。现有方法（如微调或基于演示的提示）存在训练开销大和演示样本不一致的缺陷。

Method: 训练一个行为对齐检索器（BAR）。方法是：构建包含调用和非调用行为的语料库，并使用对比学习框架，结合自定义正/负样本对和双负对比损失，以检索行为一致的示例。

Result: 实验证明，该方法显著减少了错误的函数调用，同时保持了高任务性能。

Conclusion: BAR为工具增强型LLM提供了一个成本效益高且高效的解决方案，有效提升了函数调用决策的准确性。

Abstract: Tool-augmented large language models (LLMs) leverage external functions to
extend their capabilities, but inaccurate function calls can lead to
inefficiencies and increased costs.Existing methods address this challenge by
fine-tuning LLMs or using demonstration-based prompting, yet they often suffer
from high training overhead and fail to account for inconsistent demonstration
samples, which misguide the model's invocation behavior. In this paper, we
trained a behavior-aligned retriever (BAR), which provides behaviorally
consistent demonstrations to help LLMs make more accurate tool-using decisions.
To train the BAR, we construct a corpus including different function-calling
behaviors, i.e., calling or non-calling.We use the contrastive learning
framework to train the BAR with customized positive/negative pairs and a
dual-negative contrastive loss, ensuring robust retrieval of behaviorally
consistent examples.Experiments demonstrate that our approach significantly
reduces erroneous function calls while maintaining high task performance,
offering a cost-effective and efficient solution for tool-augmented LLMs.

</details>


### [20] [ISCA: A Framework for Interview-Style Conversational Agents](https://arxiv.org/abs/2508.14344)
*Charles Welch,Allison Lahnala,Vasudha Varadarajan,Lucie Flek,Rada Mihalcea,J. Lomax Boyd,João Sedoc*

Main category: cs.CL

TL;DR: 本文介绍了一种低计算量、非生成式的对话代理系统，用于访谈式定性数据收集和定量分析，可通过在线管理面板轻松调整，无需编码。


<details>
  <summary>Details</summary>
Motivation: 需要一种可控、标准化的对话流程来收集定性数据和进行定量分析，尤其适用于追踪态度形成或行为变化等场景，且要求工具易于使用。

Method: 开发了一个低计算量、非生成式的访谈式对话代理系统。该系统通过在线管理面板进行配置，支持非编码人员创建和调整访谈。通过两个案例研究（COVID-19表达性访谈系统和新兴神经技术公众舆论调查）来展示其应用。

Result: 该系统成功实现了受控交互的定性数据收集和定量分析。用户无需编程即可通过在线管理面板轻松创建和调整访谈。两个案例研究验证了其在不同访谈场景中的有效性和适用性。

Conclusion: 所提出的系统提供了一种无需编码、易于访问且可控的访谈式数据收集工具，适用于追踪态度或行为变化等复杂现象，并通过开源代码促进进一步开发和扩展。

Abstract: We present a low-compute non-generative system for implementing
interview-style conversational agents which can be used to facilitate
qualitative data collection through controlled interactions and quantitative
analysis. Use cases include applications to tracking attitude formation or
behavior change, where control or standardization over the conversational flow
is desired. We show how our system can be easily adjusted through an online
administrative panel to create new interviews, making the tool accessible
without coding. Two case studies are presented as example applications, one
regarding the Expressive Interviewing system for COVID-19 and the other a
semi-structured interview to survey public opinion on emerging neurotechnology.
Our code is open-source, allowing others to build off of our work and develop
extensions for additional functionality.

</details>


### [21] [ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities](https://arxiv.org/abs/2508.14377)
*Wenhan Dong,Zhen Sun,Yuemeng Zhao,Zifan Peng,Jun Wu,Jingyi Zheng,Yule Liu,Xinlei He,Yu Wang,Ruiming Wang,Xinyi Huang,Lei Mo*

Main category: cs.CL

TL;DR: 本研究引入了ZPD-SCA基准，旨在评估大型语言模型（LLMs）对不同年龄段学生中文阅读理解难度的认知对齐能力。结果显示LLMs在零样本学习中表现不佳，但在上下文示例的帮助下表现显著提升，同时揭示了其评估能力存在的局限性和偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在教育应用中展现潜力，但其评估阅读材料与学生认知发展阶段（特别是依据最近发展区ZPD原则）对齐程度的能力尚未得到充分探索。尤其是在中文教育背景下，缺乏关于LLMs评估不同年龄段学生阅读理解难度的综合研究。

Method: 为填补空白，研究引入了ZPD-SCA这一新颖基准，专门用于评估阶段性中文阅读理解难度。该基准由60位特级教师（占全国在职教师的0.15%）进行标注。研究在零样本和上下文学习场景下对LLMs进行了实验评估。

Result: 实验结果表明，LLMs在零样本学习场景中表现不佳，部分模型甚至低于随机猜测。然而，在提供上下文示例后，LLMs的性能显著提升，部分模型准确率接近零样本基线两倍。这揭示了LLMs评估阅读难度的潜在能力，但也暴露了其在教育判断方面当前训练的局限性。值得注意的是，即使是表现最好的模型也存在系统性方向偏差，且模型性能在不同体裁间存在显著差异。

Conclusion: ZPD-SCA基准为评估和改进LLMs在认知对齐教育应用中的表现提供了基础。研究结果表明LLMs在评估阅读难度方面具有新兴能力，但也需要在教育判断方面进行进一步的优化和训练，以克服现有局限和偏差。

Abstract: Large language models (LLMs) have demonstrated potential in educational
applications, yet their capacity to accurately assess the cognitive alignment
of reading materials with students' developmental stages remains insufficiently
explored. This gap is particularly critical given the foundational educational
principle of the Zone of Proximal Development (ZPD), which emphasizes the need
to match learning resources with Students' Cognitive Abilities (SCA). Despite
the importance of this alignment, there is a notable absence of comprehensive
studies investigating LLMs' ability to evaluate reading comprehension
difficulty across different student age groups, especially in the context of
Chinese language education. To fill this gap, we introduce ZPD-SCA, a novel
benchmark specifically designed to assess stage-level Chinese reading
comprehension difficulty. The benchmark is annotated by 60 Special Grade
teachers, a group that represents the top 0.15% of all in-service teachers
nationwide. Experimental results reveal that LLMs perform poorly in zero-shot
learning scenarios, with Qwen-max and GLM even falling below the probability of
random guessing. When provided with in-context examples, LLMs performance
improves substantially, with some models achieving nearly double the accuracy
of their zero-shot baselines. These results reveal that LLMs possess emerging
abilities to assess reading difficulty, while also exposing limitations in
their current training for educationally aligned judgment. Notably, even the
best-performing models display systematic directional biases, suggesting
difficulties in accurately aligning material difficulty with SCA. Furthermore,
significant variations in model performance across different genres underscore
the complexity of task. We envision that ZPD-SCA can provide a foundation for
evaluating and improving LLMs in cognitively aligned educational applications.

</details>


### [22] [Credence Calibration Game? Calibrating Large Language Models through Structured Play](https://arxiv.org/abs/2508.14390)
*Ke Fang,Tianyi Zhao,Lu Cheng*

Main category: cs.CL

TL;DR: 提出一种受置信度校准游戏启发的新型提示词校准框架，通过反馈交互动态提升大型语言模型（LLMs）的置信度校准能力。


<details>
  <summary>Details</summary>
Motivation: 在LLMs广泛应用于决策关键领域时，确保其置信度估计的准确性至关重要。现有校准方法常需额外监督或参数更新，存在局限。

Method: 受置信度校准游戏启发，提出一个新颖的基于提示词的校准框架。该方法通过构建结构化交互循环，使LLMs根据预测置信度与实际正确性的一致性获得反馈，并利用反馈驱动的提示词和历史表现的自然语言总结动态改进模型校准。

Result: 在多种模型和游戏配置下的广泛实验表明，评估指标持续获得提升。

Conclusion: 研究结果揭示了基于游戏的提示词作为LLM校准的有效策略的巨大潜力。

Abstract: As Large Language Models (LLMs) are increasingly deployed in
decision-critical domains, it becomes essential to ensure that their confidence
estimates faithfully correspond to their actual correctness. Existing
calibration methods have primarily focused on post-hoc adjustments or auxiliary
model training; however, many of these approaches necessitate additional
supervision or parameter updates. In this work, we propose a novel prompt-based
calibration framework inspired by the Credence Calibration Game. Our method
establishes a structured interaction loop wherein LLMs receive feedback based
on the alignment of their predicted confidence with correctness. Through
feedback-driven prompting and natural language summaries of prior performance,
our framework dynamically improves model calibration. Extensive experiments
across models and game configurations demonstrate consistent improvements in
evaluation metrics. Our results highlight the potential of game-based prompting
as an effective strategy for LLM calibration. Code and data are available at
https://anonymous.4open.science/r/LLM-Calibration/.

</details>


### [23] [DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement](https://arxiv.org/abs/2508.14391)
*Yupei Yang,Fan Feng,Lin Yang,Wanxi Deng,Lin Qu,Biwei Huang,Shikui Tu,Lei Xu*

Main category: cs.CL

TL;DR: DEPTH是一个针对大型语言模型在关系抽取中易产生幻觉问题的框架，通过依赖感知简化和分层细化，显著降低了幻觉率并提高了抽取准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在关系抽取中主要关注关系分类，但往往难以可靠判断关系是否存在，尤其在复杂句式中易产生虚假预测（幻觉），导致知识图谱质量下降。

Method: 提出DEPTH框架，包含两阶段：1. Grounding模块：利用最短依赖路径简化句子，提取最小关系上下文以减少语法噪声并保留关键语义。2. Refinement模块：聚合局部预测，并基于对句子的整体理解进行修正。此外，引入因果驱动的奖励模型，通过强化学习与人类反馈进行鲁棒微调以缓解奖励套利。

Result: 在六个基准测试中，DEPTH将平均幻觉率降低至7.0%，同时比现有最佳基线在平均F1分数上提高了17.2%。

Conclusion: DEPTH框架成功解决了LLMs在关系抽取中易产生幻觉的问题，显著提升了关系抽取的准确性和可靠性。

Abstract: Relation extraction enables the construction of structured knowledge for many
downstream applications. While large language models (LLMs) have shown great
promise in this domain, most existing methods concentrate on relation
classification, which predicts the semantic relation type between a related
entity pair. However, we observe that LLMs often struggle to reliably determine
whether a relation exists, especially in cases involving complex sentence
structures or intricate semantics, which leads to spurious predictions. Such
hallucinations can introduce noisy edges in knowledge graphs, compromising the
integrity of structured knowledge and downstream reliability. To address these
challenges, we propose DEPTH, a framework that integrates Dependency-aware
sEntence simPlification and Two-tiered Hierarchical refinement into the
relation extraction pipeline. Given a sentence and its candidate entity pairs,
DEPTH operates in two stages: (1) the Grounding module extracts relations for
each pair by leveraging their shortest dependency path, distilling the sentence
into a minimal yet coherent relational context that reduces syntactic noise
while preserving key semantics; (2) the Refinement module aggregates all local
predictions and revises them based on a holistic understanding of the sentence,
correcting omissions and inconsistencies. We further introduce a
causality-driven reward model that mitigates reward hacking by disentangling
spurious correlations, enabling robust fine-tuning via reinforcement learning
with human feedback. Experiments on six benchmarks demonstrate that DEPTH
reduces the average hallucination rate to 7.0\% while achieving a 17.2\%
improvement in average F1 score over state-of-the-art baselines.

</details>


### [24] [Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs](https://arxiv.org/abs/2508.14408)
*Yinghan Zhou,Weifeng Zhu,Juan Wen,Wanli Peng,Zhengxian Wu,Yiming Xue*

Main category: cs.CL

TL;DR: 本文研究大型语言模型（LLMs）在单文本识别自我生成内容时表现不佳的问题，发现其原因在于“隐性领域意识（ITA）”未被表达。为此，提出“认知手术（CoSur）”框架，通过唤醒ITA显著提升了LLMs在该任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在配对呈现范式下表现出自识别能力，但在单文本呈现范式（IPP）下性能急剧下降。这种现象已被观察到，但其根本原因尚未得到系统分析。本研究旨在探究并解决LLMs在IPP场景下自识别能力弱的问题。

Method: 首先，复制现有发现以确认LLMs在IPP下难以区分自我生成和他人生成的文本。然后，将这种失败归因于“隐性领域意识（Implicit Territorial Awareness, ITA）”——模型在表征空间中区分自我和他人文本的潜在能力，但在输出中未被表达。为唤醒LLMs的ITA，提出了“认知手术（Cognitive Surgery, CoSur）”新框架，该框架包含表征提取、领域构建、作者识别和认知编辑四个主要模块。

Result: 实验结果表明，所提出的CoSur方法显著提升了三种不同LLMs在IPP场景下的性能，平均准确率分别达到83.25%、66.19%和88.01%。

Conclusion: 通过唤醒LLMs的隐性领域意识（ITA），所提出的认知手术（CoSur）框架能够有效提升LLMs在单文本呈现范式（IPP）下的自识别能力，为理解和改进LLMs的自我认知机制提供了新的方法。

Abstract: Large language models (LLMs) have been shown to possess a degree of
self-recognition capability-the ability to identify whether a given text was
generated by themselves. Prior work has demonstrated that this capability is
reliably expressed under the Pair Presentation Paradigm (PPP), where the model
is presented with two texts and asked to choose which one it authored. However,
performance deteriorates sharply under the Individual Presentation Paradigm
(IPP), where the model is given a single text to judge authorship. Although
this phenomenon has been observed, its underlying causes have not been
systematically analyzed. In this paper, we first replicate existing findings to
confirm that LLMs struggle to distinguish self- from other-generated text under
IPP. We then investigate the reasons for this failure and attribute it to a
phenomenon we term Implicit Territorial Awareness (ITA)-the model's latent
ability to distinguish self- and other-texts in representational space, which
remains unexpressed in its output behavior. To awaken the ITA of LLMs, we
propose Cognitive Surgery (CoSur), a novel framework comprising four main
modules: representation extraction, territory construction, authorship
discrimination and cognitive editing. Experimental results demonstrate that our
proposed method improves the performance of three different LLMs in the IPP
scenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%,
respectively.

</details>


### [25] [Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models](https://arxiv.org/abs/2508.14427)
*Wuyang Zhang,Yexin Tian,Xiandong Meng,Mengjie Wang,Junliang Du*

Main category: cs.CL

TL;DR: 本文提出一种基于知识图谱注入的微调框架，旨在解决大型语言模型在处理结构化知识任务时推理链缺失和实体级语义理解不足的问题，有效提升了模型在复杂语义任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理需要结构化知识的任务时，存在推理链不完整和实体层面语义理解不足的挑战。

Method: 该方法构建于预训练语言模型之上，通过知识图谱注入进行微调。它使用图神经网络编码实体和关系以构建图语义表示，并设计了融合机制来联合建模知识图谱嵌入和语言模型的上下文表示。为增强鲁棒性，引入门控机制动态平衡语言语义与结构化知识的贡献，并通过联合损失函数优化任务性能和结构对齐。

Result: 实验结果表明，该方法在实体识别、问答和语言生成等任务中表现出有效性和稳定性。它显著增强了模型表示复杂语义单元的能力，并在结构化推理和实体抽取场景中展现出更优的语义一致性和上下文逻辑建模。

Conclusion: 所提出的结构感知微调框架显著提升了模型处理涉及结构化推理和实体抽取任务的能力，增强了语义一致性与上下文逻辑建模水平。

Abstract: This paper addresses the problems of missing reasoning chains and
insufficient entity-level semantic understanding in large language models when
dealing with tasks that require structured knowledge. It proposes a fine-tuning
algorithm framework based on knowledge graph injection. The method builds on
pretrained language models and introduces structured graph information for
auxiliary learning. A graph neural network is used to encode entities and their
relations, constructing a graph-based semantic representation. A fusion
mechanism is then designed to jointly model the knowledge graph embeddings with
the contextual representations from the language model. To enhance the
robustness of knowledge integration, a gating mechanism is introduced to
dynamically balance the contributions of linguistic semantics and structural
knowledge. This effectively mitigates conflicts between different
representational spaces. During training, a joint loss function is constructed
to account for both task performance and structural alignment objectives. This
helps improve the accuracy of entity prediction and semantic reasoning. The
study also includes a series of systematic sensitivity experiments. It
evaluates the effects of learning rate, graph coverage, and structural
perturbations on model performance. The results further validate the
effectiveness and stability of the proposed method across tasks such as entity
recognition, question answering, and language generation. Experimental findings
show that the proposed structure-aware fine-tuning framework significantly
enhances the model's ability to represent complex semantic units. It
demonstrates better semantic consistency and contextual logic modeling in
scenarios involving structural reasoning and entity extraction.

</details>


### [26] [NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model](https://arxiv.org/abs/2508.14444)
*NVIDIA,:,Aarti Basant,Abhijit Khairnar,Abhijit Paithankar,Abhinav Khattar,Adi Renduchintala,Adithya Renduchintala,Aditya Malte,Akhiad Bercovich,Akshay Hazare,Alejandra Rico,Aleksander Ficek,Alex Kondratenko,Alex Shaposhnikov,Ali Taghibakhshi,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amy Shen,Andrew Tao,Ann Guan,Anna Shors,Anubhav Mandarwal,Arham Mehta,Arun Venkatesan,Ashton Sharabiani,Ashwath Aithal,Ashwin Poojary,Ayush Dattagupta,Balaram Buddharaju,Banghua Zhu,Barnaby Simkin,Bilal Kartal,Bita Darvish Rouhani,Bobby Chen,Boris Ginsburg,Brandon Norick,Brian Yu,Bryan Catanzaro,Charles Wang,Charlie Truong,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christian Munley,Christopher Parisien,Dan Su,Daniel Afrimi,Daniel Korzekwa,Daniel Rohrer,Daria Gitman,David Mosallanezhad,Deepak Narayanan,Dima Rekesh,Dina Yared,Dmytro Pykhtar,Dong Ahn,Duncan Riach,Eileen Long,Elliott Ning,Eric Chung,Erick Galinkin,Evelina Bakhturina,Gargi Prasad,Gerald Shen,Haim Elisha,Harsh Sharma,Hayley Ross,Helen Ngo,Herman Sahota,Hexin Wang,Hoo Chang Shin,Hua Huang,Iain Cunningham,Igor Gitman,Ivan Moshkov,Jaehun Jung,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jimmy Zhang,Jinze Xue,Jocelyn Huang,Joey Conway,John Kamalu,Jonathan Cohen,Joseph Jennings,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kari Briski,Katherine Cheung,Katherine Luna,Keith Wyss,Keshav Santhanam,Kezhi Kong,Krzysztof Pawelec,Kumar Anik,Kunlun Li,Kushan Ahmadian,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Luis Vega,Maer Rodrigues de Melo,Makesh Narsimhan Sreedhar,Marcin Chochowski,Mark Cai,Markus Kliegl,Marta Stepniewska-Dziubinska,Matvei Novikov,Mehrzad Samadi,Meredith Price,Meriem Boubdir,Michael Boone,Michael Evans,Michal Bien,Michal Zawalski,Miguel Martinez,Mike Chrzanowski,Mohammad Shoeybi,Mostofa Patwary,Namit Dhameja,Nave Assaf,Negar Habibi,Nidhi Bhatia,Nikki Pope,Nima Tajbakhsh,Nirmal Kumar Juluru,Oleg Rybakov,Oleksii Hrinchuk,Oleksii Kuchaiev,Oluwatobi Olabiyi,Pablo Ribalta,Padmavathy Subramanian,Parth Chadha,Pavlo Molchanov,Peter Dykas,Peter Jin,Piotr Bialecki,Piotr Januszewski,Pradeep Thalasta,Prashant Gaikwad,Prasoon Varshney,Pritam Gundecha,Przemek Tredak,Rabeeh Karimi Mahabadi,Rajen Patel,Ran El-Yaniv,Ranjit Rajan,Ria Cheruvu,Rima Shahbazyan,Ritika Borkar,Ritu Gala,Roger Waleffe,Ruoxi Zhang,Russell J. Hewett,Ryan Prenger,Sahil Jain,Samuel Kriman,Sanjeev Satheesh,Saori Kaji,Sarah Yurick,Saurav Muralidharan,Sean Narenthiran,Seonmyeong Bak,Sepehr Sameni,Seungju Han,Shanmugam Ramasamy,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shizhe Diao,Shreya Gopal,Shrimai Prabhumoye,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Siddhartha Jain,Somshubra Majumdar,Stefania Alborghetti,Syeda Nahida Akter,Terry Kong,Tim Moon,Tomasz Hliwiak,Tomer Asida,Tony Wang,Twinkle Vashishth,Tyler Poon,Udi Karpas,Vahid Noroozi,Venkat Srinivasan,Vijay Korthikanti,Vikram Fugro,Vineeth Kalluru,Vitaly Kurin,Vitaly Lavrukhin,Wasi Uddin Ahmad,Wei Du,Wonmin Byeon,Ximing Lu,Xin Dong,Yashaswi Karnati,Yejin Choi,Yian Zhang,Ying Lin,Yonggan Fu,Yoshi Suhara,Zhen Dong,Zhiyu Li,Zhongbo Zhu,Zijia Chen*

Main category: cs.CL

TL;DR: 本文提出Nemotron-Nano-9B-v2，一个混合Mamba-Transformer语言模型，旨在提高推理任务的吞吐量，并在同等规模模型中实现领先的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在处理需要生成长“思考轨迹”的推理工作负载时，推理速度和吞吐量是瓶颈，需要开发一种在保持高准确率的同时显著提高效率的模型。

Method: Nemotron-Nano-9B-v2构建于Nemotron-H架构，用Mamba-2层替换了Transformer架构中的大部分自注意力层。模型通过预训练一个120亿参数的模型（Nemotron-Nano-12B-v2-Base）在20万亿tokens上，并采用FP8训练配方。随后，对该模型进行对齐，并使用Minitron策略进行压缩和蒸馏，以实现在单张NVIDIA A10G GPU上对高达128k tokens的推理。

Result: 与现有同等规模模型（如Qwen3-8B）相比，Nemotron-Nano-9B-v2在推理基准测试中表现出同等或更优的准确率。在推理设置（如8k输入和16k输出tokens）下，其推理吞吐量提高了高达6倍。

Conclusion: Nemotron-Nano-9B-v2成功地结合了Mamba和Transformer的优势，为处理推理工作负载提供了高效的解决方案，显著提升了吞吐量，同时保持了顶级的准确性。

Abstract: We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model
designed to increase throughput for reasoning workloads while achieving
state-of-the-art accuracy compared to similarly-sized models.
Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the
majority of the self-attention layers in the common Transformer architecture
are replaced with Mamba-2 layers, to achieve improved inference speed when
generating the long thinking traces needed for reasoning. We create
Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model
(Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe.
After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to
compress and distill the model with the goal of enabling inference on up to
128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision).
Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that
Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks
while achieving up to 6x higher inference throughput in reasoning settings like
8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2,
Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with
the majority of our pre- and post-training datasets on Hugging Face.

</details>


### [27] [In2x at WMT25 Translation Task](https://arxiv.org/abs/2508.14472)
*Lei Pang,Hanyi Mao,Quanjia Xiao,HaiXiao Liu,Xiangyi Li*

Main category: cs.CL

TL;DR: In2x团队为WMT25机器翻译任务提交开放系统，旨在探索将大型语言模型（LLMs）扩展到其他语言的通用范式，重点关注日语，并期望提升低资源语言的性能。


<details>
  <summary>Details</summary>
Motivation: 探索一种通用的范式，将大型语言模型（LLMs）扩展到除常用语言之外的其他语言，最终目标是使LLM系统在低资源或不常用语言中达到卓越的翻译性能。

Method: 该研究提出的范式涵盖了数据构建方法和奖励模型设计等方面。

Result: 本文介绍了In2x研究团队为WMT25通用机器翻译共享任务提交的开放系统，该系统专注于日语相关翻译任务，并展示了其探索LLM通用扩展范式的方法。

Conclusion: 研究旨在通过探索包含数据构建和奖励模型设计的通用范式，赋能大型语言模型系统在低资源或不常用语言中实现卓越性能。

Abstract: This paper presents the open-system submission by the In2x research team for
the WMT25 General Machine Translation Shared Task. Our submission focuses on
Japanese-related translation tasks, aiming to explore a generalizable paradigm
for extending large language models (LLMs) to other languages. This paradigm
encompasses aspects such as data construction methods and reward model design.
The ultimate goal is to enable large language model systems to achieve
exceptional performance in low-resource or less commonly spoken languages.

</details>


### [28] [Reasoning is about giving reasons](https://arxiv.org/abs/2508.14488)
*Krunal Shah,Dan Roth*

Main category: cs.CL

TL;DR: 本文提出一种名为“论证逻辑结构表示”（RLS）的中间表示方法，以解决现有Transformer模型在自然语言推理中可解释性差和泛化能力受限的问题。RLS能够捕捉论证的逻辑结构，从而实现确定性、深层次的推理，并经实验证明能高精度提取逻辑结构，显著提升解释生成和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型虽能进行简单规则链式推理，但难以阐明论证的“原因”（缺乏可解释性），且无法泛化到溯因或矛盾识别等其他等效推理任务。

Method: 引入一种中间表示——“论证逻辑结构表示”（RLS），该表示理解自然语言论证的逻辑结构，包括逻辑原子和它们所包含的规则。一旦获取逻辑结构，推理就变得确定且易于计算，从而支持所有依赖逻辑结构的推理形式，包括任意深度的推理、即时错误修正和交互式讨论。

Result: 在三个流行推理数据集中，该方法能以高精度识别和提取自然语言论证的逻辑结构。这支持了论证解释的生成，并显著扩展了推理能力。

Conclusion: 通过识别和利用自然语言论证的逻辑结构作为中间表示（RLS），可以有效解决当前推理模型在可解释性和泛化性方面的局限，从而实现更强大、确定性且具有解释能力的自然语言推理。

Abstract: Convincing someone of the truth value of a premise requires understanding and
articulating the core logical structure of the argument which proves or
disproves the premise. Understanding the logical structure of an argument
refers to understanding the underlying "reasons" which make up the proof or
disproof of the premise - as a function of the "logical atoms" in the argument.
While it has been shown that transformers can "chain" rules to derive simple
arguments, the challenge of articulating the "reasons" remains. Not only do
current approaches to chaining rules suffer in terms of their interpretability,
they are also quite constrained in their ability to accommodate extensions to
theoretically equivalent reasoning tasks - a model trained to chain rules
cannot support abduction or identify contradictions. In this work we suggest
addressing these shortcomings by identifying an intermediate representation
(which we call the Representation of the Logical Structure (RLS) of the
argument) that possesses an understanding of the logical structure of a natural
language argument - the logical atoms in the argument and the rules
incorporating them. Given the logical structure, reasoning is deterministic and
easy to compute. Therefore, our approach supports all forms of reasoning that
depend on the logical structure of the natural language argument, including
arbitrary depths of reasoning, on-the-fly mistake rectification and interactive
discussion with respect to an argument. We show that we can identify and
extract the logical structure of natural language arguments in three popular
reasoning datasets with high accuracies, thus supporting explanation generation
and extending the reasoning capabilities significantly.

</details>


### [29] [EmoTale: An Enacted Speech-emotion Dataset in Danish](https://arxiv.org/abs/2508.14548)
*Maja J. Hjuler,Harald V. Skat-Rørdam,Line H. Clemmensen,Sneha Das*

Main category: cs.CL

TL;DR: 提出了一个新的丹麦语和英语情感语音语料库EmoTale，并通过语音情感识别模型验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前针对丹麦语等小语种的情感语音数据集匮乏，现有唯一的丹麦语情感语音数据库DES发布于1997年，已较为陈旧。

Method: 构建了包含丹麦语和英语情感语音录音及其情感标注的新语料库EmoTale；通过开发语音情感识别（SER）模型，并使用自监督语音模型（SSLM）嵌入和openSMILE特征提取器，验证了该数据集的有效性。

Result: 研究发现SSLM嵌入的表现优于手工提取的特征；最佳模型在使用留一说话人交叉验证时，在EmoTale语料库上获得了64.1%的非加权平均召回率（UAR）。

Conclusion: EmoTale语料库是一个有效且有用的丹麦语情感语音数据集，其性能与现有老旧的DES数据库相当，弥补了该领域的数据集空白。

Abstract: While multiple emotional speech corpora exist for commonly spoken languages,
there is a lack of functional datasets for smaller (spoken) languages, such as
Danish. To our knowledge, Danish Emotional Speech (DES), published in 1997, is
the only other database of Danish emotional speech. We present EmoTale; a
corpus comprising Danish and English speech recordings with their associated
enacted emotion annotations. We demonstrate the validity of the dataset by
investigating and presenting its predictive power using speech emotion
recognition (SER) models. We develop SER models for EmoTale and the reference
datasets using self-supervised speech model (SSLM) embeddings and the openSMILE
feature extractor. We find the embeddings superior to the hand-crafted
features. The best model achieves an unweighted average recall (UAR) of 64.1%
on the EmoTale corpus using leave-one-speaker-out cross-validation, comparable
to the performance on DES.

</details>


### [30] [Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning](https://arxiv.org/abs/2508.14574)
*Guilhem Fauré,Mostafa Sadeghi,Sam Bigeard,Slim Ouni*

Main category: cs.CL

TL;DR: 为提高手语生成模型对肢体形态和风格变化的鲁棒性，研究提出在Progressive Transformers中引入基于四元数的骨骼旋转编码和语义引导的对比损失，并取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 神经手语生成（SLP）面临的主要挑战是手语内部的高度类内变异性，这源于手语者的形态特征和训练数据中的风格多样性。

Method: 研究对标准Progressive Transformers架构进行了两项增强：1) 使用四元数空间编码骨骼旋转，并采用测地线损失来提高角度关节运动的准确性和清晰度。2) 引入对比损失，通过词汇重叠或基于SBERT的句子相似性来结构化解码器嵌入，旨在过滤掉与语义无关的解剖和风格特征。

Result: 在Phoenix14T数据集上，仅使用对比损失就使关键点正确概率（PCK）比基线PT模型提高了16%。当结合基于四元数的姿态编码时，模型在平均骨骼角度误差（MBAE）方面减少了6%。

Conclusion: 这些结果表明，将骨骼结构建模和语义引导的对比目标引入到Transformer基SLP模型的训练中，对于手语姿态表示具有显著益处。

Abstract: One of the main challenges in neural sign language production (SLP) lies in
the high intra-class variability of signs, arising from signer morphology and
stylistic variety in the training data. To improve robustness to such
variations, we propose two enhancements to the standard Progressive
Transformers (PT) architecture (Saunders et al., 2020). First, we encode poses
using bone rotations in quaternion space and train with a geodesic loss to
improve the accuracy and clarity of angular joint movements. Second, we
introduce a contrastive loss to structure decoder embeddings by semantic
similarity, using either gloss overlap or SBERT-based sentence similarity,
aiming to filter out anatomical and stylistic features that do not convey
relevant semantic information. On the Phoenix14T dataset, the contrastive loss
alone yields a 16% improvement in Probability of Correct Keypoint over the PT
baseline. When combined with quaternion-based pose encoding, the model achieves
a 6% reduction in Mean Bone Angle Error. These results point to the benefit of
incorporating skeletal structure modeling and semantically guided contrastive
objectives on sign pose representations into the training of Transformer-based
SLP models.

</details>


### [31] [Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek](https://arxiv.org/abs/2508.14586)
*Mukhammadsaid Mamasaidov,Azizullah Aral,Abror Shopulatov,Mironshoh Inomjonov*

Main category: cs.CL

TL;DR: 为解决南乌兹别克语在NLP中的资源匮乏问题，本研究发布了新的机器翻译数据集、微调模型，并提出一种后处理方法以改善形态边界处理。


<details>
  <summary>Details</summary>
Motivation: 南乌兹别克语（uzs）虽有约500万使用者且与北乌兹别克语差异显著，但在自然语言处理领域资源稀缺，急需相关研究和工具。

Method: 研究团队构建了新的南乌兹别克语机器翻译资源，包括一个997句的FLORES+开发集和39,994句平行语料。同时，微调了NLLB-200模型（lutfiy），并开发了一种恢复阿拉伯语文字半空格字符的后处理方法，以优化形态边界处理。

Result: 成功构建并发布了南乌兹别克语机器翻译的新数据集、微调模型及工具。所提出的后处理方法有效改善了形态边界处理。

Conclusion: 所有数据集、模型和工具均已公开，旨在促进南乌兹别克语及其他低资源语言的未来研究。

Abstract: Southern Uzbek (uzs) is a Turkic language variety spoken by around 5 million
people in Afghanistan and differs significantly from Northern Uzbek (uzn) in
phonology, lexicon, and orthography. Despite the large number of speakers,
Southern Uzbek is underrepresented in natural language processing. We present
new resources for Southern Uzbek machine translation, including a 997-sentence
FLORES+ dev set, 39,994 parallel sentences from dictionary, literary, and web
sources, and a fine-tuned NLLB-200 model (lutfiy). We also propose a
post-processing method for restoring Arabic-script half-space characters, which
improves handling of morphological boundaries. All datasets, models, and tools
are released publicly to support future work on Southern Uzbek and other
low-resource languages.

</details>


### [32] [Continuous sentiment scores for literary and multilingual contexts](https://arxiv.org/abs/2508.14620)
*Laurits Lyngbaek,Pascale Feldkamp,Yuri Bizzoni,Kristoffer Nielbo,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 针对文学文本情感分析的挑战，提出一种基于概念向量投影的新型连续情感评分方法，在多语言文学数据上表现优于现有工具，且与人类评分高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析工具在处理文学文本时面临比喻语言、文体歧义等独特挑战，且传统工具表现不佳，大型模型缺乏细粒度分析能力，尤其在低资源语言方面存在不足。研究旨在解决文学文本中细微情感表达的量化问题。

Method: 研究提出一种新颖的基于概念向量投影的连续情感评分方法。该方法在多语言文学数据上进行训练，以有效捕捉跨流派、语言和历史时期的细微情感表达。

Result: 该方法在英语和丹麦语文本上表现优于现有工具。其生成的情感分数分布与人类评分高度匹配，从而实现文学中更准确的分析和情感弧线建模。

Conclusion: 本研究提出的连续情感评分方法成功克服了文学文本情感分析的挑战，显著提升了细粒度情感量化的准确性，为文学研究提供了更精确的工具，尤其是在多语言和历史背景下的情感弧线建模。

Abstract: Sentiment Analysis is widely used to quantify sentiment in text, but its
application to literary texts poses unique challenges due to figurative
language, stylistic ambiguity, as well as sentiment evocation strategies.
Traditional dictionary-based tools often underperform, especially for
low-resource languages, and transformer models, while promising, typically
output coarse categorical labels that limit fine-grained analysis. We introduce
a novel continuous sentiment scoring method based on concept vector projection,
trained on multilingual literary data, which more effectively captures nuanced
sentiment expressions across genres, languages, and historical periods. Our
approach outperforms existing tools on English and Danish texts, producing
sentiment scores whose distribution closely matches human ratings, enabling
more accurate analysis and sentiment arc modeling in literature.

</details>


### [33] [Improving in-context learning with a better scoring function](https://arxiv.org/abs/2508.14685)
*Omar Naim,Swarnadeep Bhar,Jérôme Bolte,Nicholas Asher*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在处理一阶量词和线性函数时上下文学习（ICL）的局限性，并指出Softmax是原因之一。作者提出了一种名为“标度符号平均”（SSA）的新方法替代Softmax，实验证明SSA显著提升了LLMs在目标任务上的表现，并在多种语言探测任务上与基于Softmax的模型持平或超越。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）的上下文学习（ICL）能力在处理涉及一阶量词（如“所有”、“一些”）和线性函数任务时的局限性，并识别导致这些限制的关键因素，特别是注意力机制中的Softmax评分函数。

Method: 提出并评估了“标度符号平均”（scaled signed averaging, SSA）作为Softmax的一种新颖替代方案。将SSA应用于编码器-仅限和解码器-仅限的Transformer模型，并在涉及一阶量词和线性函数的特定任务以及多种语言探测任务上进行实证测试。

Result: 经验结果表明，SSA显著提高了LLMs在处理一阶量词和线性函数任务上的性能。此外，结合SSA的编码器-仅限和解码器-仅限Transformer模型在各种语言探测任务上的表现与基于Softmax的对应模型相当或更优。

Conclusion: Softmax是导致LLMs上下文学习在特定任务上受限的一个因素。通过引入SSA作为Softmax的替代，可以显著提升LLMs在处理一阶量词和线性函数时的上下文学习能力，同时在更广泛的语言任务上保持或超越现有性能。

Abstract: Large language models (LLMs) exhibit a remarkable capacity to learn by
analogy, known as in-context learning (ICL). However, recent studies have
revealed limitations in this ability. In this paper, we examine these
limitations on tasks involving first-order quantifiers such as {\em all} and
{\em some}, as well as on ICL with linear functions. We identify Softmax, the
scoring function in attention mechanism, as a contributing factor to these
constraints. To address this, we propose \textbf{scaled signed averaging
(SSA)}, a novel alternative to Softmax. Empirical results show that SSA
dramatically improves performance on our target tasks. Furthermore, we evaluate
both encoder-only and decoder-only transformers models with SSA, demonstrating
that they match or exceed their Softmax-based counterparts across a variety of
linguistic probing tasks.

</details>


### [34] [ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine](https://arxiv.org/abs/2508.14706)
*Junying Chen,Zhenyang Cai,Zhiheng Liu,Yunjin Yang,Rongsheng Wang,Qingying Xiao,Xiangyi Feng,Zhan Su,Jing Guo,Xiang Wan,Guangjun Yu,Haizhou Li,Benyou Wang*

Main category: cs.CL

TL;DR: ShizhenGPT是首个为中医领域定制的多模态大语言模型，通过构建大规模多模态数据集并进行预训练和指令微调，克服了中医数据稀缺和诊断多模态性挑战，在多项中医任务上表现优异，推动了中医多模态感知与诊断的发展。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在多领域取得成功，但其在中医药（TCM）领域的潜力尚未充分挖掘，主要受限于两大关键障碍：1) 高质量中医数据稀缺；2) 中医诊断的本质多模态性（望、闻、问、切），超出了传统LLMs的能力范围。

Method: 本文提出了ShizhenGPT，首个为中医量身定制的多模态大语言模型。为解决数据稀缺，构建了迄今为止最大的中医数据集（包含100GB+文本和200GB+多模态数据，如1.2M图像、200小时音频和生理信号）。ShizhenGPT经过预训练和指令微调，以获得深厚的中医知识和多模态推理能力。评估时使用了近期全国中医资格考试和新建的药物识别及视觉诊断视觉基准。

Result: 实验结果表明，ShizhenGPT性能优于同等规模的LLMs，并能与更大的专有模型竞争。此外，它在现有多模态LLMs中，中医视觉理解能力处于领先地位，并展示了对声音、脉搏、气味和视觉等模态的统一感知能力。

Conclusion: 本工作为实现中医药领域全面的多模态感知和诊断铺平了道路。研究中涉及的数据集、模型和代码均已公开，旨在启发该领域的进一步探索。

Abstract: Despite the success of large language models (LLMs) in various domains, their
potential in Traditional Chinese Medicine (TCM) remains largely underexplored
due to two critical barriers: (1) the scarcity of high-quality TCM data and (2)
the inherently multimodal nature of TCM diagnostics, which involve looking,
listening, smelling, and pulse-taking. These sensory-rich modalities are beyond
the scope of conventional LLMs. To address these challenges, we present
ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data
scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text
and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and
physiological signals. ShizhenGPT is pretrained and instruction-tuned to
achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect
recent national TCM qualification exams and build a visual benchmark for
Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that
ShizhenGPT outperforms comparable-scale LLMs and competes with larger
proprietary models. Moreover, it leads in TCM visual understanding among
existing multimodal LLMs and demonstrates unified perception across modalities
like sound, pulse, smell, and vision, paving the way toward holistic multimodal
perception and diagnosis in TCM. Datasets, models, and code are publicly
available. We hope this work will inspire further exploration in this field.

</details>


### [35] [The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation](https://arxiv.org/abs/2508.14718)
*Shubham Pundhir,Ganesh Bagler*

Main category: cs.CL

TL;DR: 本研究为文本食谱生成任务建立了基准，对比了GPT-2模型与传统基线。通过定制化分词策略，大型GPT-2模型在性能上显著优于基线，但在事实准确性方面仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 为文本食谱生成任务建立严格的基准。解决通用分词器在食谱中无法有效保留结构和精确数量的局限性，以增强领域特异性。

Method: 在RecipeDB的5菜系语料库上，对比微调的GPT-2大、小模型以及LSTM/RNN基线模型。核心方法是引入包含分数和结构标记的定制化分词策略。通过七种自动指标（如BLEU-4、METEOR、ROUGE-L、BERTScore）评估性能。

Result: 大型Transformer模型在BERTScore (F1) 上比最佳循环网络基线相对提升20%以上（0.92 vs 0.72），同时将困惑度降低了69.8%。

Conclusion: 大型Transformer模型在食谱生成方面表现出色，但事实准确性仍是待解决的挑战。本研究为未来结合现实约束和多模态输入的食谱生成研究奠定了基础。

Abstract: We established a rigorous benchmark for text-based recipe generation, a
fundamental task in natural language generation. We present a comprehensive
comparative study contrasting a fine-tuned GPT-2 large (774M) model against the
GPT-2 small (124M) model and traditional LSTM/RNN baselines on the 5-cuisine
corpus from RecipeDB. Our key contribution is a targeted tokenization strategy
that augments the vocabulary with 23 common fraction tokens and custom
structural markers. This approach addresses a critical limitation of generic
tokenizers by preserving essential recipe structures and precise numerical
quantities, thereby enhancing domain specificity. Performance is evaluated
using a comprehensive suite of seven automatic metrics spanning fluency
(BLEU-4, METEOR), coherence (ROUGE-L), semantic relevance (BERTScore), and
diversity. Our experiments show that the large transformer-based approach
yields a >20% relative improvement in BERTScore (F1) (0.92 vs 0.72) over the
best recurrent baseline, while reducing perplexity by 69.8%. We conclude with a
discussion of remaining challenges, particularly regarding factual accuracy,
and outline how this foundational study paves the way for integrating
real-world constraints and multi-modal inputs in advanced recipe generation
research.

</details>


### [36] [Transplant Then Regenerate: A New Paradigm for Text Data Augmentation](https://arxiv.org/abs/2508.14723)
*Guangzhan Wang,Hongyu Zhang,Beijun Shen,Xiaodong Gu*

Main category: cs.CL

TL;DR: LMTransplant是一种基于大型语言模型（LLMs）的新型文本增强范式，通过“移植-再生成”策略生成多样化内容级变体，性能优于现有方法并具有卓越的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强方法（如回译）主要侧重于词汇级重述，生成的变体缺乏多样性。虽然LLMs增强了文本增强能力，但控制其输出的风格和结构具有挑战性，需要复杂的提示工程。

Method: 提出LMTransplant，其核心思想是“移植-再生成”：将种子文本整合到LLM扩展的上下文中，并要求LLM基于该扩展上下文重新生成变体。该策略旨在充分利用LLM的知识，创建更多样化、更具创意的内容级变体，同时保留原始文本的核心属性。

Result: LMTransplant在多种文本相关任务中表现出优于现有文本增强方法的性能。此外，随着增强数据量的增加，LMTransplant展现出卓越的可扩展性。

Conclusion: LMTransplant提供了一种有效且可扩展的LLM驱动文本增强方法，能够生成多样化的内容级变体，克服了传统方法和直接使用LLM的局限性。

Abstract: Data augmentation is a critical technique in deep learning. Traditional
methods like Back-translation typically focus on lexical-level rephrasing,
which primarily produces variations with the same semantics. While large
language models (LLMs) have enhanced text augmentation by their "knowledge
emergence" capability, controlling the style and structure of these outputs
remains challenging and requires meticulous prompt engineering. In this paper,
we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.
The core idea of LMTransplant is transplant-then-regenerate: incorporating seed
text into a context expanded by LLM, and asking the LLM to regenerate a variant
based on the expanded context. This strategy allows the model to create more
diverse and creative content-level variants by fully leveraging the knowledge
embedded in LLMs, while preserving the core attributes of the original text. We
evaluate LMTransplant across various text-related tasks, demonstrating its
superior performance over existing text augmentation methods. Moreover,
LMTransplant demonstrates exceptional scalability as the size of augmented data
grows.

</details>


### [37] [Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference](https://arxiv.org/abs/2508.14735)
*Samir Abdaljalil,Erchin Serpedin,Khalid Qaraqe,Hasan Kurban*

Main category: cs.CL

TL;DR: 通过构建多语言逻辑NLI数据集，研究LLM跨语言推理能力。发现语码转换不降反升LLM性能，并提出其可作为提升多语言鲁棒性的有效手段。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大型语言模型（LLMs）在多语言环境中保持逻辑一致性和跨语言对齐的能力探索不足。

Method: 提出一个受控的多语言自然语言推理（NLI）评估框架，生成合成的逻辑前提-假设对，并翻译成多种语言。在单语言和语码转换（code-switched）条件下进行测试，并通过嵌入相似性分析验证语义保真度。

Result: 令人惊讶的是，语码转换不仅没有降低LLM性能，反而有时能提高性能，这表明翻译引起的词汇变异可能起到正则化作用。研究揭示了当前LLM跨语言推理的潜力和脆弱性。

Conclusion: LLM的跨语言推理能力兼具潜力和脆弱性；语码转换被确定为提高LLM多语言鲁棒性的一个有前景的策略。

Abstract: Large language models (LLMs) are increasingly applied in multilingual
contexts, yet their capacity for consistent, logically grounded alignment
across languages remains underexplored. We present a controlled evaluation
framework for multilingual natural language inference (NLI) that generates
synthetic, logic-based premise-hypothesis pairs and translates them into a
typologically diverse set of languages. This design enables precise control
over semantic relations and allows testing in both monolingual and
mixed-language (code-switched) conditions. Surprisingly, code-switching does
not degrade, and can even improve, performance, suggesting that
translation-induced lexical variation may serve as a regularization signal. We
validate semantic preservation through embedding-based similarity analyses and
cross-lingual alignment visualizations, confirming the fidelity of translated
pairs. Our findings expose both the potential and the brittleness of current
LLM cross-lingual reasoning, and identify code-switching as a promising lever
for improving multilingual robustness. Code available at:
https://github.com/KurbanIntelligenceLab/nli-stress-testing

</details>


### [38] [TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting](https://arxiv.org/abs/2508.14782)
*Jiaming Leng,Yunying Bi,Chuan Qin,Bing Yin,Yanyong Zhang,Chao Wang*

Main category: cs.CL

TL;DR: TransLLM是一个统一框架，将时空建模与大型语言模型结合，通过动态提示解决城市交通多任务挑战，展现出强大的泛化能力和跨任务适应性。


<details>
  <summary>Details</summary>
Motivation: 城市交通系统面临多种任务挑战，现有小型深度学习模型任务特异性强且泛化能力差，而大型语言模型难以处理结构化时空数据和数值推理，限制了其在交通领域的应用。

Method: 提出TransLLM框架，其核心是通过可学习的提示组合将时空建模与大型语言模型（LLM）融合。包含一个轻量级时空编码器（利用扩张时间卷积和双邻接图注意力网络捕捉复杂依赖），通过结构化嵌入与LLM交互。引入基于强化学习的实例级提示路由机制，根据输入动态个性化提示。框架通过编码时空模式、动态组合提示、引导LLM推理并经由输出层生成预测来运作。

Result: TransLLM在七个数据集和三项任务上进行了实验，在监督和零样本设置下均表现出卓越的有效性。与十个基线模型相比，它在回归和规划问题上均达到有竞争力的性能，并展现出强大的泛化能力和跨任务适应性。

Conclusion: TransLLM为城市交通系统提供了一个统一且可泛化的解决方案，成功克服了现有方法的局限性，有效整合了时空数据处理与大型语言模型的推理能力，为多任务交通问题提供了新的范式。

Abstract: Urban transportation systems encounter diverse challenges across multiple
tasks, such as traffic forecasting, electric vehicle (EV) charging demand
prediction, and taxi dispatch. Existing approaches suffer from two key
limitations: small-scale deep learning models are task-specific and
data-hungry, limiting their generalizability across diverse scenarios, while
large language models (LLMs), despite offering flexibility through natural
language interfaces, struggle with structured spatiotemporal data and numerical
reasoning in transportation domains. To address these limitations, we propose
TransLLM, a unified foundation framework that integrates spatiotemporal
modeling with large language models through learnable prompt composition. Our
approach features a lightweight spatiotemporal encoder that captures complex
dependencies via dilated temporal convolutions and dual-adjacency graph
attention networks, seamlessly interfacing with LLMs through structured
embeddings. A novel instance-level prompt routing mechanism, trained via
reinforcement learning, dynamically personalizes prompts based on input
characteristics, moving beyond fixed task-specific templates. The framework
operates by encoding spatiotemporal patterns into contextual representations,
dynamically composing personalized prompts to guide LLM reasoning, and
projecting the resulting representations through specialized output layers to
generate task-specific predictions. Experiments across seven datasets and three
tasks demonstrate the exceptional effectiveness of TransLLM in both supervised
and zero-shot settings. Compared to ten baseline models, it delivers
competitive performance on both regression and planning problems, showing
strong generalization and cross-task adaptability. Our code is available at
https://github.com/BiYunying/TransLLM.

</details>


### [39] [Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs](https://arxiv.org/abs/2508.14817)
*Skatje Myers,Dmitriy Dligach,Timothy A. Miller,Samantha Barr,Yanjun Gao,Matthew Churpek,Anoop Mayampurath,Majid Afshar*

Main category: cs.CL

TL;DR: 本研究发现，对于冗长、嘈杂的电子健康记录（EHRs）信息提取，检索增强生成（RAG）方法表现出色，其性能与使用完整上下文的LLM相当，但显著减少了输入令牌。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）冗长、嘈杂且冗余，给临床医生带来挑战。大型语言模型（LLMs）虽有潜力，但其上下文窗口往往无法容纳过长的临床笔记。检索增强生成（RAG）通过检索相关段落来减少所需输入令牌，为该问题提供了一个有前景的解决方案。

Method: 研究提出了三个可复制的临床任务：1) 提取影像学检查程序，2) 生成抗生素使用时间线，3) 识别关键诊断。使用实际住院患者的EHRs，测试了三种先进的LLM，通过目标文本检索（RAG）或最新临床笔记提供不同量的上下文进行比较。

Result: 研究发现，RAG的性能与使用最新笔记的方法相当或更优，并且接近使用模型完整上下文的性能，但所需输入令牌大大减少。

Conclusion: 研究结果表明，即使随着新模型处理更长文本的能力增强，RAG在EHR分析中仍然是一种具有竞争力和高效的方法。

Abstract: Electronic health records (EHRs) are long, noisy, and often redundant, posing
a major challenge for the clinicians who must navigate them. Large language
models (LLMs) offer a promising solution for extracting and reasoning over this
unstructured text, but the length of clinical notes often exceeds even
state-of-the-art models' extended context windows. Retrieval-augmented
generation (RAG) offers an alternative by retrieving task-relevant passages
from across the entire EHR, potentially reducing the amount of required input
tokens. In this work, we propose three clinical tasks designed to be replicable
across health systems with minimal effort: 1) extracting imaging procedures, 2)
generating timelines of antibiotic use, and 3) identifying key diagnoses. Using
EHRs from actual hospitalized patients, we test three state-of-the-art LLMs
with varying amounts of provided context, using either targeted text retrieval
or the most recent clinical notes. We find that RAG closely matches or exceeds
the performance of using recent notes, and approaches the performance of using
the models' full context while requiring drastically fewer input tokens. Our
results suggest that RAG remains a competitive and efficient approach even as
newer models become capable of handling increasingly longer amounts of text.

</details>


### [40] [Long Chain-of-Thought Reasoning Across Languages](https://arxiv.org/abs/2508.14828)
*Josh Barua,Seun Eisape,Kayo Yin,Alane Suhr*

Main category: cs.CL

TL;DR: 本研究系统分析了多语言长链思维（CoT）推理，发现英语作为枢纽语言的有效性因语言而异，多语言预训练有助于缩小但不能消除性能差距，且数据质量与规模的权衡也具有语言依赖性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）通过长链思维（CoT）展现出强大的推理能力，但该推理过程几乎完全以英语为中心，缺乏对多语言CoT推理的系统性研究。

Method: 构建了两个流行英语推理数据集的翻译版本，并使用Qwen 2.5 (7B) 和 Qwen 3 (8B) 模型进行了微调，系统研究了法语、日语、拉脱维亚语和斯瓦希里语中的长链思维生成。

Result: ['使用英语作为枢纽语言的有效性因语言而异：对法语无益；作为日语和拉脱维亚语的推理语言可提高性能；对斯瓦希里语不足，任务理解和推理仍较差。', 'Qwen 3中广泛的多语言预训练缩小但并未消除跨语言性能差距，少量微调（1k痕迹）仍可使斯瓦希里语性能提高30%以上。', '数据质量与规模的权衡取决于语言：英语和法语小而精的数据集足矣，而斯瓦希里语和拉脱维亚语则更大的、有噪声的语料库更有效。']

Conclusion: 研究结果阐明了长链思维何时以及为何能跨语言迁移，并提供了翻译数据集，以促进公平的多语言推理研究。

Abstract: Scaling inference through long chains-of-thought (CoTs) has unlocked
impressive reasoning capabilities in large language models (LLMs), yet the
reasoning process remains almost exclusively English-centric. We construct
translated versions of two popular English reasoning datasets, fine-tune Qwen
2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT
generation across French, Japanese, Latvian, and Swahili. Our experiments
reveal three key findings. First, the efficacy of using English as a pivot
language varies by language: it provides no benefit for French, improves
performance when used as the reasoning language for Japanese and Latvian, and
proves insufficient for Swahili where both task comprehension and reasoning
remain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but
does not eliminate the cross-lingual performance gap. A lightweight fine-tune
using only 1k traces still improves performance by over 30\% in Swahili. Third,
data quality versus scale trade-offs are language dependent: small, carefully
curated datasets suffice for English and French, whereas larger but noisier
corpora prove more effective for Swahili and Latvian. Together, these results
clarify when and why long CoTs transfer across languages and provide translated
datasets to foster equitable multilingual reasoning research.

</details>


### [41] [MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework](https://arxiv.org/abs/2508.14880)
*Ailing Yu,Lan Yao,Jingnan Liu,Zhe Chen,Jiajun Yin,Yuan Wang,Xinhao Liao,Zhiling Ye,Ji Li,Yun Yue,Hansong Xiao,Hualei Zhou,Chunxiao Guo,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: MedResearcher-R1-32B是专为医疗领域设计的LLM代理，通过创新数据合成和定制检索工具，克服通用LLM在医学知识和工具上的局限，在医疗基准测试中实现最先进性能，并证明领域特异性创新的重要性。


<details>
  <summary>Details</summary>
Motivation: 通用大型语言模型（LLM）代理在医疗领域面临显著挑战，表现出有限的准确性。主要限制在于模型缺乏足够的密集医学知识进行临床推理，以及缺少针对医疗上下文的专业检索工具。

Method: 本文提出了MedResearcher-R1-32B医疗深度研究代理，通过两项核心创新解决上述挑战：1) 开发了一种利用医学知识图谱的数据合成框架，从罕见医学实体周围的子图中提取最长链以生成复杂的多跳问答对。2) 集成了一个定制的私有医疗检索引擎以及通用工具。模型采用两阶段训练范式，结合了监督微调和带有复合奖励的在线强化学习。

Result: MedResearcher-R1-32B模型生成了2100多条跨12个医学专业的不同轨迹，平均每次工具交互4.2次。模型在医疗基准测试中取得了新的最先进（state-of-the-art）结果，同时在通用深度研究任务上保持了竞争力。

Conclusion: 本研究表明，在架构、工具设计和训练数据构建方面进行战略性的领域特定创新，可以使较小的开源模型在专业领域超越更大的专有系统。

Abstract: Recent developments in Large Language Model (LLM)-based agents have shown
impressive capabilities spanning multiple domains, exemplified by deep research
systems that demonstrate superior performance on complex information-seeking
and synthesis tasks. While general-purpose deep research agents have shown
impressive capabilities, they struggle significantly with medical domain
challenges, as evidenced by leading proprietary systems achieving limited
accuracy on complex medical benchmarks. The key limitations are: (1) the model
lacks sufficient dense medical knowledge for clinical reasoning, and (2) the
framework is constrained by the absence of specialized retrieval tools tailored
for medical contexts.We present a medical deep research agent that addresses
these challenges through two core innovations. First, we develop a novel data
synthesis framework using medical knowledge graphs, extracting the longest
chains from subgraphs around rare medical entities to generate complex
multi-hop question-answer pairs. Second, we integrate a custom-built private
medical retrieval engine alongside general-purpose tools, enabling accurate
medical information synthesis. Our approach generates 2100+ diverse
trajectories across 12 medical specialties, each averaging 4.2 tool
interactions.Through a two-stage training paradigm combining supervised
fine-tuning and online reinforcement learning with composite rewards, our
MedResearcher-R1-32B model demonstrates exceptional performance, establishing
new state-of-the-art results on medical benchmarks while maintaining
competitive performance on general deep research tasks. Our work demonstrates
that strategic domain-specific innovations in architecture, tool design, and
training data construction can enable smaller open-source models to outperform
much larger proprietary systems in specialized domains.

</details>


### [42] [Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs](https://arxiv.org/abs/2508.14896)
*Haokun Lin,Haobo Xu,Yichen Wu,Ziyu Guo,Renrui Zhang,Zhichao Lu,Ying Wei,Qingfu Zhang,Zhenan Sun*

Main category: cs.CL

TL;DR: 本文首次系统研究了扩散大语言模型（dLLMs）的量化问题，识别了激活异常值这一关键挑战，并全面评估了不同配置下后训练量化（PTQ）对dLLMs的影响，为高效部署提供了实用见解。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（dLLMs）尽管在自然语言生成方面潜力巨大，但其庞大的参数规模和高资源需求使其难以在边缘设备上部署。尽管后训练量化（PTQ）已广泛应用于压缩自回归大语言模型（AR LLMs），但其在dLLMs上的适用性仍未被充分探索。

Method: 本研究首先识别了dLLMs中存在的、阻碍低位量化的激活异常值。随后，实施了最先进的后训练量化（PTQ）方法，并从位宽、量化方法、任务类别和模型类型四个关键维度，对多种任务类型和模型变体进行了全面评估。

Result: 通过多视角评估，本研究揭示了dLLMs在不同配置下的量化行为的实用见解，并强调了激活异常值对低位量化所带来的挑战。

Conclusion: 本研究的发现为未来高效部署扩散大语言模型的相关研究奠定了基础。

Abstract: Recent advances in diffusion large language models (dLLMs) have introduced a
promising alternative to autoregressive (AR) LLMs for natural language
generation tasks, leveraging full attention and denoising-based decoding
strategies. However, the deployment of these models on edge devices remains
challenging due to their massive parameter scale and high resource demands.
While post-training quantization (PTQ) has emerged as a widely adopted
technique for compressing AR LLMs, its applicability to dLLMs remains largely
unexplored. In this work, we present the first systematic study on quantizing
diffusion-based language models. We begin by identifying the presence of
activation outliers, characterized by abnormally large activation values that
dominate the dynamic range. These outliers pose a key challenge to low-bit
quantization, as they make it difficult to preserve precision for the majority
of values. More importantly, we implement state-of-the-art PTQ methods and
conduct a comprehensive evaluation across multiple task types and model
variants. Our analysis is structured along four key dimensions: bit-width,
quantization method, task category, and model type. Through this
multi-perspective evaluation, we offer practical insights into the quantization
behavior of dLLMs under different configurations. We hope our findings provide
a foundation for future research in efficient dLLM deployment. All codes and
experimental setups will be released to support the community.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [A comparative study of some wavelet and sampling operators on various features of an image](https://arxiv.org/abs/2508.14043)
*Digvijay Singh,Rahul Shukla,Karunesh Kumar Singh*

Main category: cs.CV

TL;DR: 研究正采样Kantorovich算子及其变体的收敛性和近似性质，评估它们在图像特征分析中的表现。


<details>
  <summary>Details</summary>
Motivation: 全面分析正采样Kantorovich算子及其相关变体的局部和全局近似性质，以理解它们在非理想图像条件下的适用性。

Method: 1. 引入并研究正采样Kantorovich (SK) 算子及其收敛性。2. 在SK算子框架内，使用SK、高斯、双边和基于阈值小波的算子分析局部和全局近似属性。3. 引入基本逼近定理 (FTA) 并施加条件。4. 测量误差及一系列数学参数（MSE, SI, SSI, SMPI, ENL）。5. 通过理想条件下的示例和2D Shepp-Logan Phantom的数值示例验证算子特性及FTA。

Result: 在图像的非理想（不均匀）条件下，不同的算子在处理特定图像特征时表现出各自的优势，即某些算子对特定特征有效，而另一些则不然。

Conclusion: 图像的非均匀性决定了不同正采样Kantorovich算子及其变体对特定图像特征的适用性各异，强调了根据具体图像特性选择合适算子的重要性。

Abstract: This research includes the study of some positive sampling Kantorovich
operators (SK operators) and their convergence properties. A comprehensive
analysis of both local and global approximation properties is presented using
sampling Kantorovich (SK), Gaussian, Bilateral and the thresholding
wavelet-based operators in the framework of SK-operators. Explicitly, we start
the article by introducing the basic terminology and state the fundamental
theorem of approximation (FTA) by imposing the various required conditions
corresponding to the various defined operators. We measure the error and study
the other mathematical parameters such as the mean square error (MSE), the
speckle index (SI), the speckle suppression index (SSI), the speckle mean
preservation index (SMPI), and the equivalent number of looks (ENL) at various
levels of resolution parameters. The nature of these operators are demonstrated
via an example under ideal conditions in tabulated form at a certain level of
samples. Eventually, another numerical example is illustrated to discuss the
region of interest (ROI) via SI, SSI and SMPI of 2D Shepp-Logan Phantom taken
slice from the 3D image, which gives the justification of the fundamental
theorem of approximation (FTA). At the end of the derivation and illustrations
we observe that the various operators have their own significance while
studying the various features of the image because of the uneven nature of an
image (non-ideal condition). Therefore, to some extent, some operators work
well and some do not for some specific features of the image.

</details>


### [44] [Federated Action Recognition for Smart Worker Assistance Using FastPose](https://arxiv.org/abs/2508.14113)
*Vinit Hegiste,Vidit Goyal,Tatjana Legler,Martin Ruskowski*

Main category: cs.CV

TL;DR: 本文提出一个基于联邦学习的姿态骨架工人行为识别框架，在智能制造环境中显著提升识别准确性和跨用户泛化能力，同时保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 智能制造中准确实时识别工人行为至关重要，但现有骨架行为识别方法依赖集中式数据集，在注重隐私的工业场景中不切实际。

Method: 提出一个基于联邦学习（FL）的姿态行为识别（HAR）框架。利用包含8种工业相关上半身手势的自定义骨架数据集（来自5名参与者，通过改进的FastPose模型处理）。评估了LSTM和Transformer两种时间骨干网络，并在集中式、本地、加权联邦平均（FedAvg）和联邦集成学习（FedEnsemble）四种范式下进行训练和评估。

Result: 在全局测试集上，联邦学习Transformer比集中式训练提升12.4个百分点，联邦集成学习提升16.3个百分点。在未见的外部客户端上，联邦学习和联邦集成学习的准确率分别超过集中式方法52.6和58.3个百分点。

Conclusion: 联邦学习不仅能保护隐私，还能显著增强跨用户泛化能力，使其成为异构工业环境中可扩展、注重隐私的HAR的实用解决方案。

Abstract: In smart manufacturing environments, accurate and real-time recognition of
worker actions is essential for productivity, safety, and human-machine
collaboration. While skeleton-based human activity recognition (HAR) offers
robustness to lighting, viewpoint, and background variations, most existing
approaches rely on centralized datasets, which are impractical in
privacy-sensitive industrial scenarios. This paper presents a federated
learning (FL) framework for pose-based HAR using a custom skeletal dataset of
eight industrially relevant upper-body gestures, captured from five
participants and processed using a modified FastPose model. Two temporal
backbones, an LSTM and a Transformer encoder, are trained and evaluated under
four paradigms: centralized, local (per-client), FL with weighted federated
averaging (FedAvg), and federated ensemble learning (FedEnsemble). On the
global test set, the FL Transformer improves over centralized training by +12.4
percentage points, with FedEnsemble delivering a +16.3 percentage points gain.
On an unseen external client, FL and FedEnsemble exceed centralized accuracy by
+52.6 and +58.3 percentage points, respectively. These results demonstrate that
FL not only preserves privacy but also substantially enhances cross-user
generalization, establishing it as a practical solution for scalable,
privacy-aware HAR in heterogeneous industrial settings.

</details>


### [45] [LENS: Learning to Segment Anything with Unified Reinforced Reasoning](https://arxiv.org/abs/2508.14153)
*Lianghui Zhu,Bin Ouyang,Yuxuan Zhang,Tianheng Cheng,Rui Hu,Haocheng Shen,Longjin Ran,Xiaoxin Chen,Li Yu,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 针对文本提示图像分割模型在泛化性方面的不足，本文提出了LENS，一个基于强化学习的框架，通过端到端地联合优化推理过程和分割任务，并引入统一的多级奖励，显著提升了模型性能，证明了强化学习驱动的“思维链”推理对提升模型泛化能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本提示图像分割监督微调方法在测试时忽略了显式的“思维链”（CoT）推理，这限制了它们对未见过提示和领域的泛化能力。

Method: 本文提出了LENS，一个可扩展的强化学习框架，端到端地联合优化推理过程和分割任务。该框架设计了统一的强化学习奖励，涵盖句子、边界框和分割区域级别的信息，以鼓励模型生成有用的CoT推理过程并提高掩膜质量。LENS基于30亿参数的视觉-语言模型Qwen2.5-VL-3B-Instruct。

Result: LENS在RefCOCO、RefCOCO+和RefCOCOg基准测试中取得了81.2%的平均cIoU，相比强大的微调方法GLaMM，性能提升高达5.6%。

Conclusion: 强化学习驱动的“思维链”推理是文本提示分割的强大先验，为实现更具泛化能力的“万物可分割”模型提供了可行路径。

Abstract: Text-prompted image segmentation enables fine-grained visual understanding
and is critical for applications such as human-computer interaction and
robotics. However, existing supervised fine-tuning methods typically ignore
explicit chain-of-thought (CoT) reasoning at test time, which limits their
ability to generalize to unseen prompts and domains. To address this issue, we
introduce LENS, a scalable reinforcement-learning framework that jointly
optimizes the reasoning process and segmentation in an end-to-end manner. We
propose unified reinforcement-learning rewards that span sentence-, box-, and
segment-level cues, encouraging the model to generate informative CoT
rationales while refining mask quality. Using a publicly available
3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS
achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg
benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to
5.6%. These results demonstrate that RL-driven CoT reasoning serves as a robust
prior for text-prompted segmentation and offers a practical path toward more
generalizable Segment Anything models. Code is available at
https://github.com/hustvl/LENS.

</details>


### [46] [RynnEC: Bringing MLLMs into Embodied World](https://arxiv.org/abs/2508.14160)
*Ronghao Dang,Yuqian Yuan,Yunxuan Mao,Kehan Li,Jiangpin Liu,Zhikai Wang,Xin Li,Fan Wang,Deli Zhao*

Main category: cs.CV

TL;DR: RynnEC是一个面向具身认知的视频多模态大语言模型，通过区域级视频交互实现精细感知。它在多项任务上达到SOTA，并引入了数据生成管道和具身认知评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以实现具身智能体对物理世界的精细感知和精确交互；缺乏带标注的3D具身认知数据集。

Method: 1. 提出RynnEC模型：基于通用视听基础模型，整合区域编码器和掩码解码器，支持区域级视频交互。2. 设计基于以自我为中心视频的数据生成管道，以缓解数据集稀缺。3. 创建RynnEC-Bench，一个区域中心的具身认知能力评估基准。

Result: RynnEC在物体属性理解、物体分割和空间推理方面取得了最先进（SOTA）的性能。

Conclusion: RynnEC有望推动具身智能体通用认知核心的发展，并促进其在多样化具身任务中的泛化能力。

Abstract: We introduce RynnEC, a video multimodal large language model designed for
embodied cognition. Built upon a general-purpose vision-language foundation
model, RynnEC incorporates a region encoder and a mask decoder, enabling
flexible region-level video interaction. Despite its compact architecture,
RynnEC achieves state-of-the-art performance in object property understanding,
object segmentation, and spatial reasoning. Conceptually, it offers a
region-centric video paradigm for the brain of embodied agents, providing
fine-grained perception of the physical world and enabling more precise
interactions. To mitigate the scarcity of annotated 3D datasets, we propose an
egocentric video based pipeline for generating embodied cognition data.
Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for
evaluating embodied cognitive capabilities. We anticipate that RynnEC will
advance the development of general-purpose cognitive cores for embodied agents
and facilitate generalization across diverse embodied tasks. The code, model
checkpoints, and benchmark are available at:
https://github.com/alibaba-damo-academy/RynnEC

</details>


### [47] [Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer](https://arxiv.org/abs/2508.14187)
*Md Ashiqur Rahman,Chiao-An Yang,Michael N. Cheng,Lim Jun Hao,Jeremiah Jiang,Teck-Yian Lim,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 本研究提出深度平衡规范器（DEC），以增强模型局部尺度等变性，有效应对计算机视觉中的尺度变化挑战，并在ImageNet上提升了主流模型的性能和尺度一致性。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉中尺度变化是一个核心挑战，因为同一类物体尺寸各异，且其感知大小受相机距离影响。这些变化在图像内是局部的，即不同物体尺寸可能在同一图像中以不同方式变化，需要有效处理。

Method: 研究者提出了一种深度平衡规范器（DEC），旨在提高模型的局部尺度等变性。DEC设计为可轻松整合到现有网络架构中，并能适应预训练模型。

Result: 在竞争激烈的ImageNet基准测试中，DEC显著提升了ViT、DeiT、Swin和BEiT等四种主流预训练深度网络的模型性能和局部尺度一致性。

Conclusion: 深度平衡规范器（DEC）提供了一种有效且易于集成的方法，能够解决计算机视觉中的尺度变化问题，显著提升现有模型的性能和尺度鲁棒性。

Abstract: Scale variation is a fundamental challenge in computer vision. Objects of the
same class can have different sizes, and their perceived size is further
affected by the distance from the camera. These variations are local to the
objects, i.e., different object sizes may change differently within the same
image. To effectively handle scale variations, we present a deep equilibrium
canonicalizer (DEC) to improve the local scale equivariance of a model. DEC can
be easily incorporated into existing network architectures and can be adapted
to a pre-trained model. Notably, we show that on the competitive ImageNet
benchmark, DEC improves both model performance and local scale consistency
across four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our
code is available at https://github.com/ashiq24/local-scale-equivariance.

</details>


### [48] [CLIPSym: Delving into Symmetry Detection with CLIP](https://arxiv.org/abs/2508.14197)
*Tinghan Yang,Md Ashiqur Rahman,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 本文提出了CLIPSym模型，利用CLIP的视觉-语言能力、旋转等变解码器和新型语义感知提示分组技术，实现了对称性检测领域的最新水平。


<details>
  <summary>Details</summary>
Motivation: 对称性检测是计算机视觉中的一个基本且持续的挑战。研究旨在探索预训练的视觉-语言模型（如CLIP）是否能通过利用自然图像描述中包含的额外对称性线索来辅助并改进对称性检测。

Method: 本文提出了CLIPSym模型，该模型利用CLIP的图像和语言编码器。它设计了一个基于Transformer和G-Convolution混合的旋转等变解码器，用于检测旋转和反射对称性。为充分利用CLIP的语言编码器，开发了一种新颖的“语义感知提示分组（SAPG）”技术，该技术通过聚合多样化的常见基于对象的提示来更好地整合用于对称性检测的语义线索。

Result: 实验结果表明，CLIPSym在DENDI、SDRW和LDRS三个标准对称性检测数据集上均优于当前最先进的方法。详细的消融实验验证了CLIP预训练、所提出的等变解码器以及SAPG技术的有效性。

Conclusion: CLIPSym成功地将视觉-语言模型的强大能力应用于对称性检测，通过创新的模型架构和提示技术显著提升了检测性能，并验证了多模态信息在几何特征检测中的有效性和潜力。

Abstract: Symmetry is one of the most fundamental geometric cues in computer vision,
and detecting it has been an ongoing challenge. With the recent advances in
vision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP
model can aid symmetry detection by leveraging the additional symmetry cues
found in the natural image descriptions. We propose CLIPSym, which leverages
CLIP's image and language encoders and a rotation-equivariant decoder based on
a hybrid of Transformer and $G$-Convolution to detect rotation and reflection
symmetries. To fully utilize CLIP's language encoder, we have developed a novel
prompting technique called Semantic-Aware Prompt Grouping (SAPG), which
aggregates a diverse set of frequent object-based prompts to better integrate
the semantic cues for symmetry detection. Empirically, we show that CLIPSym
outperforms the current state-of-the-art on three standard symmetry detection
datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations
verifying the benefits of CLIP's pre-training, the proposed equivariant
decoder, and the SAPG technique. The code is available at
https://github.com/timyoung2333/CLIPSym.

</details>


### [49] [A Survey on Video Anomaly Detection via Deep Learning: Human, Vehicle, and Environment](https://arxiv.org/abs/2508.14203)
*Ghazal Alinezhad Noghre,Armin Danesh Pazho,Hamed Tabkhi*

Main category: cs.CV

TL;DR: 本综述全面分析了视频异常检测（VAD）领域，系统梳理了现有文献，识别了当前方法的贡献与局限性，并指出了未来的研究方向和实际部署挑战。


<details>
  <summary>Details</summary>
Motivation: 视频异常检测（VAD）是计算机视觉中的一项关键任务，虽有深度学习的进展，但该领域仍存在碎片化问题，需要一个全面的视角来整合并推动其理论与应用发展。

Method: 本文作为一篇综述，系统性地组织了VAD文献，涵盖了不同监督级别、自适应学习方法（如在线、主动、持续学习）以及以人为中心、以车辆为中心和以环境为中心的三大应用场景，并分析了各场景的挑战和设计考量。

Result: 通过综合分析，本综述识别了当前VAD方法的基本贡献和局限性。

Conclusion: 本综述旨在为VAD研究提供一个结构化基础，促进其理论理解和实际应用，并为研究人员提供参考，同时强调了异常检测领域的开放性挑战和实际部署障碍。

Abstract: Video Anomaly Detection (VAD) has emerged as a pivotal task in computer
vision, with broad relevance across multiple fields. Recent advances in deep
learning have driven significant progress in this area, yet the field remains
fragmented across domains and learning paradigms. This survey offers a
comprehensive perspective on VAD, systematically organizing the literature
across various supervision levels, as well as adaptive learning methods such as
online, active, and continual learning. We examine the state of VAD across
three major application categories: human-centric, vehicle-centric, and
environment-centric scenarios, each with distinct challenges and design
considerations. In doing so, we identify fundamental contributions and
limitations of current methodologies. By consolidating insights from subfields,
we aim to provide the community with a structured foundation for advancing both
theoretical understanding and real-world applicability of VAD systems. This
survey aims to support researchers by providing a useful reference, while also
drawing attention to the broader set of open challenges in anomaly detection,
including both fundamental research questions and practical obstacles to
real-world deployment.

</details>


### [50] [Accelerating Image Classification with Graph Convolutional Neural Networks using Voronoi Diagrams](https://arxiv.org/abs/2508.14218)
*Mustafa Mohammadi Gharasuie,Luis Rueda*

Main category: cs.CV

TL;DR: 本文提出了一种名为归一化Voronoi图卷积网络（NVGCN）的新框架，结合GCNs和Voronoi图进行图像分类。该方法采用图像的图表示，显著提升了预处理时间和分类精度，尤其在复杂场景和细粒度分类中超越了现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络在处理复杂数据结构方面可能存在局限。研究旨在利用图卷积网络（GCNs）建模关系数据的卓越能力，通过引入一种新颖的图基学习范式，提升图像分类性能，特别是在复杂场景和细粒度分类任务中，并开发一种更高效的GCN变体。

Method: 本研究提出了一种创新框架，将GCNs与Voronoi图结合用于图像分类。该方法将图像像素或区域视为图的顶点，并通过对应的Delaunay三角剖分进行简化，形成图像的图表示。文中特别提出了一种新型且更快的GCN变体，命名为归一化Voronoi图卷积网络（NVGCN）。

Result: 该模型在多个基准数据集上显著提高了预处理时间和分类精度。它超越了现有最先进的模型，尤其在涉及复杂场景和细粒度类别的场景中表现出色。实验结果还表明，提出的NVGCN比常规GCN更快。

Conclusion: 将GCNs与Voronoi图（特别是NVGCN）集成在推动图像分类任务方面展现出巨大潜力。这项研究为图像分类提供了一种新颖方法，并为计算机视觉和非结构化数据领域中图基学习范式的发展开辟了新途径。

Abstract: Recent advances in image classification have been significantly propelled by
the integration of Graph Convolutional Networks (GCNs), offering a novel
paradigm for handling complex data structures. This study introduces an
innovative framework that employs GCNs in conjunction with Voronoi diagrams to
peform image classification, leveraging their exceptional capability to model
relational data. Unlike conventional convolutional neural networks, our
approach utilizes a graph-based representation of images, where pixels or
regions are treated as vertices of a graph, which are then simplified in the
form of the corresponding Delaunay triangulations. Our model yields significant
improvement in pre-processing time and classification accuracy on several
benchmark datasets, surpassing existing state-of-the-art models, especially in
scenarios that involve complex scenes and fine-grained categories. The
experimental results, validated via cross-validation, underscore the potential
of integrating GCNs with Voronoi diagrams in advancing image classification
tasks. This research contributes to the field by introducing a novel approach
to image classification, while opening new avenues for developing graph-based
learning paradigms in other domains of computer vision and non-structured data.
In particular, we have proposed a new version of the GCN in this paper, namely
normalized Voronoi Graph Convolution Network (NVGCN), which is faster than the
regular GCN.

</details>


### [51] [Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models](https://arxiv.org/abs/2508.14264)
*Thanh-Dat Truong,Huu-Thien Tran,Tran Thai Son,Bhiksha Raj,Khoa Luu*

Main category: cs.CV

TL;DR: 本文提出一种新的学习机制，通过引入顺序重构任务（图像和文本）以及定向token方法和引导损失，显著改善了大型多模态模型（LMMs）的视觉-文本对齐、鲁棒性和泛化性，并在多项基准测试中达到了最先进（SoTA）的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态模型（LMMs）在理解任务中表现出色，但由于视觉和文本特征的对齐与关联问题，它们在鲁棒性和泛化性方面仍存在基本局限性。

Method: 引入一种学习机制，通过解决“洗牌问题”来改善视觉和文本模态间的鲁棒对齐。具体方法包括：1) 在LMM预训练和微调阶段引入重构图像顺序和文本顺序的新任务；2) 提出一种“定向token”方法捕获视觉和文本知识以重构视觉输入顺序；3) 引入“Image-to-Response Guided loss”以提升LMM的视觉理解能力。

Result: 所提出的方法在学术任务导向型和指令遵循型LMM基准测试中，相较于之前的LMMs，始终达到最先进（SoTA）的性能。

Conclusion: 该方法通过解决顺序重构问题和引入新的损失函数，有效提升了LMMs的推理能力、视觉理解和跨模态对齐，显著增强了模型的鲁棒性和泛化性，并实现了领先的性能。

Abstract: Large multimodal models (LMMs) have gained impressive performance due to
their outstanding capability in various understanding tasks. However, these
models still suffer from some fundamental limitations related to robustness and
generalization due to the alignment and correlation between visual and textual
features. In this paper, we introduce a simple but efficient learning mechanism
for improving the robust alignment between visual and textual modalities by
solving shuffling problems. In particular, the proposed approach can improve
reasoning capability, visual understanding, and cross-modality alignment by
introducing two new tasks: reconstructing the image order and the text order
into the LMM's pre-training and fine-tuning phases. In addition, we propose a
new directed-token approach to capture visual and textual knowledge, enabling
the capability to reconstruct the correct order of visual inputs. Then, we
introduce a new Image-to-Response Guided loss to further improve the visual
understanding of the LMM in its responses. The proposed approach consistently
achieves state-of-the-art (SoTA) performance compared with prior LMMs on
academic task-oriented and instruction-following LMM benchmarks.

</details>


### [52] [Effect of Data Augmentation on Conformal Prediction for Diabetic Retinopathy](https://arxiv.org/abs/2508.14266)
*Rizwan Ahamed,Annahita Amireskandari,Joel Palko,Carol Laxson,Binod Bhattarai,Prashnna Gyawali*

Main category: cs.CV

TL;DR: 本研究系统性地探讨了不同数据增强策略对糖尿病视网膜病变分级中保形预测器性能和不确定性量化的影响，发现样本混合策略（如Mixup和CutMix）能提高准确性并产生更可靠的不确定性估计，而CLAHE则可能产生负面影响。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在糖尿病视网膜病变（DR）分级等高风险临床任务中，虽有高准确率，但缺乏鲁棒的不确定性量化限制了其临床应用。尽管保形预测（CP）能提供统计保障，但标准训练实践（如数据增强）与这些保障有效性之间的相互作用尚不明确，有必要深入探究。

Method: 本研究利用DDR数据集，评估了ResNet-50和CoaT两种骨干架构。模型在五种数据增强策略下训练：无增强、标准几何变换、CLAHE、Mixup和CutMix。研究分析了这些策略对保形指标的下游影响，包括经验覆盖率、平均预测集大小和正确效率。

Result: 研究结果表明，Mixup和CutMix等样本混合策略不仅提高了模型的预测准确性，还带来了更可靠、更高效的不确定性估计。相反，CLAHE等方法可能对模型的确定性产生负面影响。

Conclusion: 研究强调在构建用于医疗影像的真正可信赖AI系统时，必须协同设计数据增强策略与下游的不确定性量化方法。

Abstract: The clinical deployment of deep learning models for high-stakes tasks such as
diabetic retinopathy (DR) grading requires demonstrable reliability. While
models achieve high accuracy, their clinical utility is limited by a lack of
robust uncertainty quantification. Conformal prediction (CP) offers a
distribution-free framework to generate prediction sets with statistical
guarantees of coverage. However, the interaction between standard training
practices like data augmentation and the validity of these guarantees is not
well understood. In this study, we systematically investigate how different
data augmentation strategies affect the performance of conformal predictors for
DR grading. Using the DDR dataset, we evaluate two backbone architectures --
ResNet-50 and a Co-Scale Conv-Attentional Transformer (CoaT) -- trained under
five augmentation regimes: no augmentation, standard geometric transforms,
CLAHE, Mixup, and CutMix. We analyze the downstream effects on conformal
metrics, including empirical coverage, average prediction set size, and correct
efficiency. Our results demonstrate that sample-mixing strategies like Mixup
and CutMix not only improve predictive accuracy but also yield more reliable
and efficient uncertainty estimates. Conversely, methods like CLAHE can
negatively impact model certainty. These findings highlight the need to
co-design augmentation strategies with downstream uncertainty quantification in
mind to build genuinely trustworthy AI systems for medical imaging.

</details>


### [53] [Tooth-Diffusion: Guided 3D CBCT Synthesis with Fine-Grained Tooth Conditioning](https://arxiv.org/abs/2508.14276)
*Said Djafar Said,Torkan Gholamalizadeh,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: 提出一种条件扩散模型，通过牙齿属性控制生成高保真3D牙齿CBCT扫描。


<details>
  <summary>Details</summary>
Motivation: 尽管牙科CBCT扫描日益重要，但生成具有精细控制的解剖真实扫描在医学图像合成中仍是挑战。

Method: 开发了一种新颖的条件扩散框架，通过牙齿级别的二元属性（控制牙齿存在与配置）指导3D牙齿体积生成。该方法整合了基于小波的去噪扩散、FiLM条件作用和掩膜损失函数。

Result: 模型在牙齿增减和全牙列合成等任务中表现出强大的保真度和泛化能力，FID分数低，修复性能稳健，对未见扫描的SSIM值仍高于0.91。

Conclusion: 该工作实现了牙列的局部真实修改而无需重新扫描，为口腔手术规划、医患沟通以及牙科AI工作流中的定向数据增强提供了新机遇。

Abstract: Despite the growing importance of dental CBCT scans for diagnosis and
treatment planning, generating anatomically realistic scans with fine-grained
control remains a challenge in medical image synthesis. In this work, we
propose a novel conditional diffusion framework for 3D dental volume
generation, guided by tooth-level binary attributes that allow precise control
over tooth presence and configuration. Our approach integrates wavelet-based
denoising diffusion, FiLM conditioning, and masked loss functions to focus
learning on relevant anatomical structures. We evaluate the model across
diverse tasks, such as tooth addition, removal, and full dentition synthesis,
using both paired and distributional similarity metrics. Results show strong
fidelity and generalization with low FID scores, robust inpainting performance,
and SSIM values above 0.91 even on unseen scans. By enabling realistic,
localized modification of dentition without rescanning, this work opens
opportunities for surgical planning, patient communication, and targeted data
augmentation in dental AI workflows. The codes are available at:
https://github.com/djafar1/tooth-diffusion.

</details>


### [54] [GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting](https://arxiv.org/abs/2508.14278)
*Elena Alegret Regalado,Kunyi Li,Sen Wang,Siyun Liang,Michael Niemeyer,Stefano Gasperini,Nassir Navab,Federico Tombari*

Main category: cs.CV

TL;DR: GALA是一个新颖的框架，利用3D高斯泼溅技术，通过自监督对比学习和创新的交叉注意力模块，实现了从2D图像进行细粒度、语言感知的开放词汇3D场景理解，并在2D和3D任务中表现卓越。


<details>
  <summary>Details</summary>
Motivation: 尽管3D场景重建和理解日益普及，但现有方法仍难以从2D图像中捕获细粒度、语言感知的3D表示。

Method: GALA框架基于3D高斯泼溅，首先通过自监督对比学习提炼场景特定的3D实例特征场。其核心贡献是一个带有两个可学习码本的交叉注意力模块，用于编码与视角无关的语义嵌入，从而泛化到语言特征场，并支持无缝的2D和3D开放词汇查询，同时减少内存消耗。

Result: 在真实世界数据集上进行的广泛实验表明，GALA在2D和3D开放词汇性能方面均表现出色。

Conclusion: GALA成功解决了从2D图像捕获细粒度、语言感知3D表示的难题，为开放词汇3D场景理解提供了一个高性能的解决方案。

Abstract: 3D scene reconstruction and understanding have gained increasing popularity,
yet existing methods still struggle to capture fine-grained, language-aware 3D
representations from 2D images. In this paper, we present GALA, a novel
framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting
(3DGS). GALA distills a scene-specific 3D instance feature field via
self-supervised contrastive learning. To extend to generalized language feature
fields, we introduce the core contribution of GALA, a cross-attention module
with two learnable codebooks that encode view-independent semantic embeddings.
This design not only ensures intra-instance feature similarity but also
supports seamless 2D and 3D open-vocabulary queries. It reduces memory
consumption by avoiding per-Gaussian high-dimensional feature learning.
Extensive experiments on real-world datasets demonstrate GALA's remarkable
open-vocabulary performance on both 2D and 3D.

</details>


### [55] [Multi-Rationale Explainable Object Recognition via Contrastive Conditional Inference](https://arxiv.org/abs/2508.14280)
*Ali Rasekh,Sepehr Kazemi Ranjbar,Simon Gottschalk*

Main category: cs.CV

TL;DR: 本文针对基于CLIP的可解释目标识别中现有方法的局限性，提出了一个多理由可解释目标识别基准和对比条件推理(CCI)框架，实现了分类准确性和理由质量的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有可解释目标识别方法（基于CLIP）依赖提示词，受限于CLIP文本编码器，且对解释结构条件弱。此外，现有数据集通常仅含单一、有噪声的理由，未能捕捉图像判别性特征的多样性。

Method: 本文引入了一个多理由可解释目标识别基准，包含每个图像标注多个真实理由的数据集和更全面的评估指标。同时，提出对比条件推理（CCI）框架，显式建模图像嵌入、类别标签和理由之间的概率关系，无需训练即可有效利用理由预测目标类别。

Result: 该方法在多理由可解释目标识别基准上取得了最先进（SOTA）的结果，包括强大的零样本性能，并在分类准确性和理由质量方面树立了新标准。

Conclusion: 本工作与所提出的基准一起，为未来可解释目标识别模型的评估提供了一个更全面的框架。

Abstract: Explainable object recognition using vision-language models such as CLIP
involves predicting accurate category labels supported by rationales that
justify the decision-making process. Existing methods typically rely on
prompt-based conditioning, which suffers from limitations in CLIP's text
encoder and provides weak conditioning on explanatory structures. Additionally,
prior datasets are often restricted to single, and frequently noisy, rationales
that fail to capture the full diversity of discriminative image features. In
this work, we introduce a multi-rationale explainable object recognition
benchmark comprising datasets in which each image is annotated with multiple
ground-truth rationales, along with evaluation metrics designed to offer a more
comprehensive representation of the task. To overcome the limitations of
previous approaches, we propose a contrastive conditional inference (CCI)
framework that explicitly models the probabilistic relationships among image
embeddings, category labels, and rationales. Without requiring any training,
our framework enables more effective conditioning on rationales to predict
accurate object categories. Our approach achieves state-of-the-art results on
the multi-rationale explainable object recognition benchmark, including strong
zero-shot performance, and sets a new standard for both classification accuracy
and rationale quality. Together with the benchmark, this work provides a more
complete framework for evaluating future models in explainable object
recognition. The code will be made available online.

</details>


### [56] [OccluNet: Spatio-Temporal Deep Learning for Occlusion Detection on DSA](https://arxiv.org/abs/2508.14286)
*Anushka A. Kore,Frank G. te Nijenhuis,Matthijs van der Sluijs,Wim van Zwam,Charles Majoie,Geert Lycklama à Nijeholt,Danny Ruijters,Frans Vos,Sandra Cornelissen,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: 提出OccluNet，一个结合YOLOX和Transformer的时空深度学习模型，用于自动化DSA血管闭塞检测，性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 在急性缺血性卒中治疗中，准确检测血管闭塞至关重要，但手动解释数字减影血管造影（DSA）图像因解剖复杂性和时间限制而面临挑战。

Method: 提出OccluNet时空深度学习模型，该模型整合了YOLOX单阶段目标检测器与基于Transformer的时间注意力机制，以自动化DSA序列中的闭塞检测。与YOLOv11基线模型（在单个帧或最小强度投影上训练）进行比较，并探索了纯时间注意力和分段时空注意力两种OccluNet变体。

Result: 在MR CLEAN注册的DSA图像上评估，OccluNet能够捕获时间上一致的特征，精确度和召回率分别达到89.02%和74.87%。OccluNet显著优于基线模型，并且两种注意力变体表现相似。

Conclusion: OccluNet提供了一种有效且自动化的血管闭塞检测解决方案，在DSA序列中表现优异，尤其在时间特征捕获方面具有优势，为急性缺血性卒中的诊疗提供了潜在支持。

Abstract: Accurate detection of vascular occlusions during endovascular thrombectomy
(EVT) is critical in acute ischemic stroke (AIS). Interpretation of digital
subtraction angiography (DSA) sequences poses challenges due to anatomical
complexity and time constraints. This work proposes OccluNet, a spatio-temporal
deep learning model that integrates YOLOX, a single-stage object detector, with
transformer-based temporal attention mechanisms to automate occlusion detection
in DSA sequences. We compared OccluNet with a YOLOv11 baseline trained on
either individual DSA frames or minimum intensity projections. Two
spatio-temporal variants were explored for OccluNet: pure temporal attention
and divided space-time attention. Evaluation on DSA images from the MR CLEAN
Registry revealed the model's capability to capture temporally consistent
features, achieving precision and recall of 89.02% and 74.87%, respectively.
OccluNet significantly outperformed the baseline models, and both attention
variants attained similar performance. Source code is available at
https://github.com/anushka-kore/OccluNet.git

</details>


### [57] [Pixels to Play: A Foundation Model for 3D Gameplay](https://arxiv.org/abs/2508.14295)
*Yuguang Yue,Chris Green,Samuel Hunt,Irakli Salia,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.CV

TL;DR: Pixels2Play-0.1 (P2P0.1)是一个基础模型，通过像素流学习以类似人类的方式玩各种3D视频游戏。


<details>
  <summary>Details</summary>
Motivation: 为了满足AI队友、可控NPC、个性化直播员和辅助测试等新兴应用场景的需求，研究人员旨在开发一个能从像素流输入进行学习并以最少游戏特定工程泛化到新游戏的智能体。

Method: P2P0.1采用端到端的行为克隆进行训练。数据来源包括带标注的人类游戏演示，以及通过逆动力学模型推断出动作的无标注公共视频。模型架构是一个解码器专属的Transformer，采用自回归动作输出以处理大的动作空间，并能在单个消费级GPU上保持低延迟。

Result: 定性结果表明，P2P0.1在简单的Roblox和经典的MS-DOS游戏中表现出合格的游戏能力，并进行了无标注数据的消融实验。

Conclusion: P2P0.1展示了通过像素流实现多种游戏合格玩法的潜力，为未来通过扩展和评估达到专家级、文本条件控制的游戏智能体奠定了基础。

Abstract: We introduce Pixels2Play-0.1 (P2P0.1), a foundation model that learns to play
a wide range of 3D video games with recognizable human-like behavior. Motivated
by emerging consumer and developer use cases - AI teammates, controllable NPCs,
personalized live-streamers, assistive testers - we argue that an agent must
rely on the same pixel stream available to players and generalize to new titles
with minimal game-specific engineering. P2P0.1 is trained end-to-end with
behavior cloning: labeled demonstrations collected from instrumented human
game-play are complemented by unlabeled public videos, to which we impute
actions via an inverse-dynamics model. A decoder-only transformer with
auto-regressive action output handles the large action space while remaining
latency-friendly on a single consumer GPU. We report qualitative results
showing competent play across simple Roblox and classic MS-DOS titles,
ablations on unlabeled data, and outline the scaling and evaluation steps
required to reach expert-level, text-conditioned control.

</details>


### [58] [MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation](https://arxiv.org/abs/2508.14327)
*Guile Wu,David Huang,Dongfeng Bai,Bingbing Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多模态多视角视频生成方法，用于自动驾驶场景合成，通过统一的扩散变换器模型实现，能够生成高保真度和可控性的RGB、深度和语义等多模态视频。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶视频生成方法主要集中于RGB视频，缺乏对深度图和语义图等多模态数据的支持，而多模态数据对于全面的城市场景理解至关重要。使用多个模型生成不同模态会增加部署难度，且未能利用模态间的互补信息。

Method: 本文提出一种新颖的多模态多视角视频生成方法。具体而言，构建了一个统一的扩散变换器模型，该模型由模态共享和模态特定的组件组成。通过利用多样化的条件输入，将可控的场景结构和内容线索编码到统一的扩散模型中，以实现多模态多视角视频生成。

Result: 该方法能够在统一的框架下生成多模态多视角的驾驶场景视频。在具挑战性的nuScenes自动驾驶数据集上的实验表明，该方法能够生成高保真度和可控性的多模态多视角城市场景视频，并超越了现有最先进的方法。

Conclusion: 所提出的统一扩散变换器模型有效解决了多模态视频生成在自动驾驶领域的挑战，实现了高保真度和可控性的多模态多视角场景视频合成，为全面的城市场景理解提供了有力支持。

Abstract: Video generation has recently shown superiority in urban scene synthesis for
autonomous driving. Existing video generation approaches to autonomous driving
primarily focus on RGB video generation and lack the ability to support
multi-modal video generation. However, multi-modal data, such as depth maps and
semantic maps, are crucial for holistic urban scene understanding in autonomous
driving. Although it is feasible to use multiple models to generate different
modalities, this increases the difficulty of model deployment and does not
leverage complementary cues for multi-modal data generation. To address this
problem, in this work, we propose a novel multi-modal multi-view video
generation approach to autonomous driving. Specifically, we construct a unified
diffusion transformer model composed of modal-shared components and
modal-specific components. Then, we leverage diverse conditioning inputs to
encode controllable scene structure and content cues into the unified diffusion
model for multi-modal multi-view video generation. In this way, our approach is
capable of generating multi-modal multi-view driving scene videos in a unified
framework. Our experiments on the challenging real-world autonomous driving
dataset, nuScenes, show that our approach can generate multi-modal multi-view
urban scene videos with high fidelity and controllability, surpassing the
state-of-the-art methods.

</details>


### [59] [Inter-Class Relational Loss for Small Object Detection: A Case Study on License Plates](https://arxiv.org/abs/2508.14343)
*Dian Ning,Dong Seog Han*

Main category: cs.CV

TL;DR: 针对单阶段多目标检测中小物体梯度更新不足的问题，本文提出了一种利用类间空间关系的“类间关系损失”（ICR loss），通过惩罚不合理的空间预测来有效提升小目标检测性能，并发布了新的车牌数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于IoU的损失函数在单阶段多目标检测中对小目标的梯度更新不足，导致梯度平坦且学习效率低，严重影响小目标检测性能。

Method: 提出了一种“类间关系损失”（ICR loss），基于物体间的空间关系（如车牌附着在汽车上）。当预测的小目标边界框不在其相关联的大目标内时，施加惩罚，惩罚强度与两者重叠面积成反比，以引导小目标学习。该损失可轻松整合至现有IoU损失中。同时，构建了一个新的小车辆多车牌数据集（SVMLP）。

Result: 提出的ICR损失显著提升了标准mAP指标，在YOLOv12-T和UAV-DETR上，mAP$_{50}$分别提高了10.3%和1.6%，且无需额外超参数调整。

Conclusion: 通过利用类间空间关系，所提出的ICR损失能有效解决小目标梯度更新不足的问题，提升检测性能，并且易于集成到现有基于IoU的检测框架中。

Abstract: In one-stage multi-object detection tasks, various intersection over union
(IoU)-based solutions aim at smooth and stable convergence near the targets
during training. However, IoU-based losses fail to correctly update the
gradient of small objects due to an extremely flat gradient. During the update
of multiple objects, the learning of small objects' gradients suffers more
because of insufficient gradient updates. Therefore, we propose an inter-class
relational loss to efficiently update the gradient of small objects while not
sacrificing the learning efficiency of other objects based on the simple fact
that an object has a spatial relationship to another object (e.g., a car plate
is attached to a car in a similar position). When the predicted car plate's
bounding box is not within its car, a loss punishment is added to guide the
learning, which is inversely proportional to the overlapped area of the car's
and predicted car plate's bounding box. By leveraging the spatial relationship
at the inter-class level, the loss guides small object predictions using larger
objects and enhances latent information in deeper feature maps. In this paper,
we present twofold contributions using license plate detection as a case study:
(1) a new small vehicle multi-license plate dataset (SVMLP), featuring diverse
real-world scenarios with high-quality annotations; and (2) a novel inter-class
relational loss function designed to promote effective detection performance.
We highlight the proposed ICR loss penalty can be easily added to existing
IoU-based losses and enhance the performance. These contributions improve the
standard mean Average Precision (mAP) metric, achieving gains of 10.3% and 1.6%
in mAP$^{\text{test}}_{50}$ for YOLOv12-T and UAV-DETR, respectively, without
any additional hyperparameter tuning. Code and dataset will be available soon.

</details>


### [60] [HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation](https://arxiv.org/abs/2508.14345)
*Gaston Gustavo Rios*

Main category: cs.CV

TL;DR: 为解决手语识别（SLR）数据不足问题，本文提出一种基于CMLPe的轻量级手语生成模型，结合合成数据预训练，显著提升了识别精度，并在多个数据集上达到最先进（SOTA）水平。


<details>
  <summary>Details</summary>
Motivation: 手语识别（SLR）模型面临训练数据不足导致的显著性能限制。

Method: 引入一种基于CMLPe的新型轻量级手语生成模型，并结合合成数据预训练方法。该方法与Mamba-SL和Transformer-SL分类器结合使用。

Result: 一致性地提高了识别精度，在LSFB和DiSPLaY数据集上取得了新的最先进（SOTA）结果。研究发现，合成数据预训练在某些情况下优于传统数据增强方法，并能与其产生互补效益。

Conclusion: 所提出的计算高效方法使手语生成和合成数据预训练在SLR领域得以普及，并在不同数据集上实现了显著的性能提升。

Abstract: Sign Language Recognition (SLR) models face significant performance
limitations due to insufficient training data availability. In this article, we
address the challenge of limited data in SLR by introducing a novel and
lightweight sign generation model based on CMLPe. This model, coupled with a
synthetic data pretraining approach, consistently improves recognition
accuracy, establishing new state-of-the-art results for the LSFB and DiSPLaY
datasets using our Mamba-SL and Transformer-SL classifiers. Our findings reveal
that synthetic data pretraining outperforms traditional augmentation methods in
some cases and yields complementary benefits when implemented alongside them.
Our approach democratizes sign generation and synthetic data pretraining for
SLR by providing computationally efficient methods that achieve significant
performance improvements across diverse datasets.

</details>


### [61] [Deep Learning for Taxol Exposure Analysis: A New Cell Image Dataset and Attention-Based Baseline Model](https://arxiv.org/abs/2508.14349)
*Sean Fletcher,Gabby Scott,Douglas Currie,Xin Zhang,Yuqi Song,Bruce MacLeod*

Main category: cs.CV

TL;DR: 为解决现有紫杉醇细胞效应监测方法的局限性及公共数据集的缺失，本研究创建了一个新的显微图像数据集，并提出了一个基于深度学习的基线模型ResAttention-KNN，用于紫杉醇浓度分类。


<details>
  <summary>Details</summary>
Motivation: 细胞水平上监测化疗药物紫杉醇的效果对临床评估和生物医学研究至关重要。然而，现有检测方法成本高昂、劳动密集，不适用于高通量或实时分析。尽管深度学习在医学图像分析中潜力巨大，但目前缺乏用于紫杉醇暴露细胞形态自动分析的公开数据集。

Method: 为解决数据缺乏问题，本文引入了一个新的显微图像数据集，其中包含用不同浓度紫杉醇处理的C6胶质瘤细胞。为有效分类紫杉醇浓度并为未来研究建立基准，研究提出了名为ResAttention-KNN的基线模型，该模型结合了ResNet-50、卷积块注意力模块，并在学习到的嵌入空间中使用k-近邻分类器。

Result: 所提出的ResAttention-KNN模型为紫杉醇浓度分类提供了一个有效解决方案和基准，通过集成基于注意力的精炼和非参数分类，增强了模型的鲁棒性和可解释性。

Conclusion: 本研究公开发布了所创建的数据集和模型实现，以支持可重复性并促进未来基于视觉的生物医学分析研究，填补了紫杉醇自动效应监测领域的数据和方法空白。

Abstract: Monitoring the effects of the chemotherapeutic agent Taxol at the cellular
level is critical for both clinical evaluation and biomedical research.
However, existing detection methods require specialized equipment, skilled
personnel, and extensive sample preparation, making them expensive,
labor-intensive, and unsuitable for high-throughput or real-time analysis. Deep
learning approaches have shown great promise in medical and biological image
analysis, enabling automated, high-throughput assessment of cellular
morphology. Yet, no publicly available dataset currently exists for automated
morphological analysis of cellular responses to Taxol exposure. To address this
gap, we introduce a new microscopy image dataset capturing C6 glioma cells
treated with varying concentrations of Taxol. To provide an effective solution
for Taxol concentration classification and establish a benchmark for future
studies on this dataset, we propose a baseline model named ResAttention-KNN,
which combines a ResNet-50 with Convolutional Block Attention Modules and uses
a k-Nearest Neighbors classifier in the learned embedding space. This model
integrates attention-based refinement and non-parametric classification to
enhance robustness and interpretability. Both the dataset and implementation
are publicly released to support reproducibility and facilitate future research
in vision-based biomedical analysis.

</details>


### [62] [Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation](https://arxiv.org/abs/2508.14358)
*Zhujun Li,Shuo Zhang,Ioannis Stamos*

Main category: cs.CV

TL;DR: 本文提出HRC-Pose，一个基于深度图的类别级物体姿态估计框架，利用对比学习捕获6D姿态的内在连续性，并在基准测试中超越现有SOTA方法并实现实时运行。


<details>
  <summary>Details</summary>
Motivation: 现有类别级物体姿态估计方法仅依赖6D姿态作为监督信号，未能显式捕获姿态的内在连续性，导致预测不一致并降低对未见姿态的泛化能力。

Method: 提出HRC-Pose框架，该框架仅使用深度信息，并利用对比学习来学习保留6D姿态连续性的点云表示。HRC-Pose将物体姿态解耦为旋转和平移分量并分别编码。引入了一种基于6D姿态感知分层排序方案的多任务、多类别对比学习策略，通过考虑旋转、平移差异和类别信息来对比点云。此外，设计了分别处理旋转感知和平移感知嵌入的姿态估计模块。

Result: 实验证明HRC-Pose成功学习到连续的特征空间。在REAL275和CAMERA25基准测试中，该方法持续优于现有仅使用深度信息的最先进方法，并且能够实时运行。

Conclusion: HRC-Pose通过学习连续的特征空间，有效解决了现有方法的局限性，在类别级物体姿态估计任务中展现出卓越的性能和实时性，具有现实世界应用的潜力。

Abstract: Category-level object pose estimation aims to predict the 6D pose and 3D size
of objects within given categories. Existing approaches for this task rely
solely on 6D poses as supervisory signals without explicitly capturing the
intrinsic continuity of poses, leading to inconsistencies in predictions and
reduced generalization to unseen poses. To address this limitation, we propose
HRC-Pose, a novel depth-only framework for category-level object pose
estimation, which leverages contrastive learning to learn point cloud
representations that preserve the continuity of 6D poses. HRC-Pose decouples
object pose into rotation and translation components, which are separately
encoded and leveraged throughout the network. Specifically, we introduce a
contrastive learning strategy for multi-task, multi-category scenarios based on
our 6D pose-aware hierarchical ranking scheme, which contrasts point clouds
from multiple categories by considering rotational and translational
differences as well as categorical information. We further design pose
estimation modules that separately process the learned rotation-aware and
translation-aware embeddings. Our experiments demonstrate that HRC-Pose
successfully learns continuous feature spaces. Results on REAL275 and CAMERA25
benchmarks show that our method consistently outperforms existing depth-only
state-of-the-art methods and runs in real-time, demonstrating its effectiveness
and potential for real-world applications. Our code is at
https://github.com/zhujunli1993/HRC-Pose.

</details>


### [63] [Taming Transformer for Emotion-Controllable Talking Face Generation](https://arxiv.org/abs/2508.14359)
*Ziqi Zhang,Cheng Deng*

Main category: cs.CV

TL;DR: 提出一种离散方法，通过情感锚点表示和自回归Transformer实现情感可控的说话人脸生成。


<details>
  <summary>Details</summary>
Motivation: 当前情感可控说话人脸生成面临两大挑战：如何有效建模与特定情感相关的多模态关系，以及如何利用此关系合成保留身份的情感视频。

Method: 该方法采取离散方式。具体而言，它：1) 采用两种预训练策略将音频解耦为独立分量并将视频量化为视觉token组合；2) 提出情感锚点（EA）表示，将情感信息整合到视觉token中；3) 引入自回归Transformer来建模视觉token的全局分布，并预测索引序列以合成受控视频。

Result: 在MEAD数据集上进行的广泛实验证明，该方法在定性和定量方面均表现出优越性。

Conclusion: 该方法能有效解决情感可控说话人脸生成中的关键挑战，实现了高质量的情感驱动视频合成。

Abstract: Talking face generation is a novel and challenging generation task, aiming at
synthesizing a vivid speaking-face video given a specific audio. To fulfill
emotion-controllable talking face generation, current methods need to overcome
two challenges: One is how to effectively model the multimodal relationship
related to the specific emotion, and the other is how to leverage this
relationship to synthesize identity preserving emotional videos. In this paper,
we propose a novel method to tackle the emotion-controllable talking face
generation task discretely. Specifically, we employ two pre-training strategies
to disentangle audio into independent components and quantize videos into
combinations of visual tokens. Subsequently, we propose the emotion-anchor (EA)
representation that integrates the emotional information into visual tokens.
Finally, we introduce an autoregressive transformer to model the global
distribution of the visual tokens under the given conditions and further
predict the index sequence for synthesizing the manipulated videos. We conduct
experiments on the MEAD dataset that controls the emotion of videos conditioned
on multiple emotional audios. Extensive experiments demonstrate the
superiorities of our method both qualitatively and quantitatively.

</details>


### [64] [FastTracker: Real-Time and Accurate Visual Tracking](https://arxiv.org/abs/2508.14370)
*Hamidreza Hashempoor,Yu Dong Hwang*

Main category: cs.CV

TL;DR: 提出一种广义多目标跟踪框架，通过结合遮挡感知重识别和道路结构感知轨迹细化，提升车辆及多类别目标跟踪性能，并发布了新的车辆数据集。


<details>
  <summary>Details</summary>
Motivation: 现有传统多目标跟踪（MOT）系统主要针对行人，对其他目标类别（特别是车辆）的泛化能力有限。

Method: 该方法包含两个关键组件：1) 遮挡感知重识别机制，增强严重遮挡物体的身份保持；2) 道路结构感知轨迹细化策略，利用车道方向、人行横道等语义场景先验，改善轨迹连续性和准确性。此外，还引入了一个新的包含多种车辆类别的基准数据集。

Result: 在新建的车辆数据集和多个公共基准上均表现出鲁棒性能，尤其在MOT17测试集上HOTA得分66.4，在MOT20测试集上HOTA得分65.7。

Conclusion: 所提出的框架有效实现了多类别目标跟踪的泛化，尤其在复杂交通场景的车辆跟踪中表现突出，同时在传统基准上也能保持强大性能。

Abstract: Conventional multi-object tracking (MOT) systems are predominantly designed
for pedestrian tracking and often exhibit limited generalization to other
object categories. This paper presents a generalized tracking framework capable
of handling multiple object types, with a particular emphasis on vehicle
tracking in complex traffic scenes. The proposed method incorporates two key
components: (1) an occlusion-aware re-identification mechanism that enhances
identity preservation for heavily occluded objects, and (2) a
road-structure-aware tracklet refinement strategy that utilizes semantic scene
priors such as lane directions, crosswalks, and road boundaries to improve
trajectory continuity and accuracy. In addition, we introduce a new benchmark
dataset comprising diverse vehicle classes with frame-level tracking
annotations, specifically curated to support evaluation of vehicle-focused
tracking methods. Extensive experimental results demonstrate that the proposed
approach achieves robust performance on both the newly introduced dataset and
several public benchmarks, highlighting its effectiveness in general-purpose
object tracking. While our framework is designed for generalized multi-class
tracking, it also achieves strong performance on conventional benchmarks, with
HOTA scores of 66.4 on MOT17 and 65.7 on MOT20 test sets. Code and Benchmark
are available: github.com/Hamidreza-Hashempoor/FastTracker,
huggingface.co/datasets/Hamidreza-Hashemp/FastTracker-Benchmark.

</details>


### [65] [TCFNet: Bidirectional face-bone transformation via a Transformer-based coarse-to-fine point movement network](https://arxiv.org/abs/2508.14373)
*Runshi Zhang,Bimeng Jie,Yang He,Junchen Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为TCFNet的Transformer-based粗到细点云运动网络，用于骨性正畸外科手术规划中准确、高效地模拟面骨形状变换。


<details>
  <summary>Details</summary>
Motivation: 传统生物力学模拟方法计算耗时、数据处理劳动密集且精度低。现有深度学习方法无法处理大规模点云、感受野有限导致噪声，并依赖复杂的配准预处理和后处理操作，这些限制了其性能和广泛适用性。

Method: 本文提出了TCFNet，一个端到端的Transformer-based粗到细点云运动网络。它包含两个阶段：第一阶段采用基于Transformer的网络处理全局特征，第二阶段采用局部信息聚合网络（LIA-Net）补偿Transformer在局部精度上的不足，通过建模局部几何结构（边缘、方向、相对位置特征）来生成精确的点运动路径。此外，借鉴可变形医学图像配准，提出了一种辅助损失函数，利用专家知识重建关键器官。

Result: 与现有最先进（SOTA）方法相比，TCFNet在收集的数据集上取得了出色的评估指标和可视化结果。

Conclusion: TCFNet通过其创新的粗到细 Transformer 架构和局部信息聚合机制，显著提高了骨性正畸外科手术模拟中面骨形状变换的准确性和效率，克服了传统方法和现有深度学习方法的局限性。

Abstract: Computer-aided surgical simulation is a critical component of orthognathic
surgical planning, where accurately simulating face-bone shape transformations
is significant. The traditional biomechanical simulation methods are limited by
their computational time consumption levels, labor-intensive data processing
strategies and low accuracy. Recently, deep learning-based simulation methods
have been proposed to view this problem as a point-to-point transformation
between skeletal and facial point clouds. However, these approaches cannot
process large-scale points, have limited receptive fields that lead to noisy
points, and employ complex preprocessing and postprocessing operations based on
registration. These shortcomings limit the performance and widespread
applicability of such methods. Therefore, we propose a Transformer-based
coarse-to-fine point movement network (TCFNet) to learn unique, complicated
correspondences at the patch and point levels for dense face-bone point cloud
transformations. This end-to-end framework adopts a Transformer-based network
and a local information aggregation network (LIA-Net) in the first and second
stages, respectively, which reinforce each other to generate precise point
movement paths. LIA-Net can effectively compensate for the neighborhood
precision loss of the Transformer-based network by modeling local geometric
structures (edges, orientations and relative position features). The previous
global features are employed to guide the local displacement using a gated
recurrent unit. Inspired by deformable medical image registration, we propose
an auxiliary loss that can utilize expert knowledge for reconstructing critical
organs.Compared with the existing state-of-the-art (SOTA) methods on gathered
datasets, TCFNet achieves outstanding evaluation metrics and visualization
results. The code is available at https://github.com/Runshi-Zhang/TCFNet.

</details>


### [66] [QuadINR: Hardware-Efficient Implicit Neural Representations Through Quadratic Activation](https://arxiv.org/abs/2508.14374)
*Wenyong Zhou,Boyu Li,Jiachen Ren,Taiqiang Wu,Zhilin Ai,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: 提出QuadINR，一种基于分段二次激活函数的硬件高效隐式神经表示（INR），它在提升高频信号表达能力的同时，大幅减少硬件资源消耗并提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有隐式神经表示（INR）依赖复杂激活函数处理频谱偏差，但这通常导致显著的硬件开销。

Method: 引入QuadINR，其核心是使用分段二次激活函数，并通过神经切线核（NTK）分析验证其丰富的谐波内容能增强高频信号表达。开发了统一的N级流水线框架以实现高效硬件部署，并在FPGA（VCU128平台）和28nm ASIC上进行了实现。

Result: 在图像和视频任务中，QuadINR比现有工作PSNR提升高达2.06dB。硬件方面，面积仅为1914μm²，动态功耗6.14mW，资源和功耗分别降低高达97%，延迟提高高达93%。

Conclusion: QuadINR成功解决了INR的硬件效率问题，在保证甚至提升性能的同时，显著减少了硬件资源和功耗，为INR的实际应用提供了高效且实用的解决方案。

Abstract: Implicit Neural Representations (INRs) encode discrete signals continuously
while addressing spectral bias through activation functions (AFs). Previous
approaches mitigate this bias by employing complex AFs, which often incur
significant hardware overhead. To tackle this challenge, we introduce QuadINR,
a hardware-efficient INR that utilizes piecewise quadratic AFs to achieve
superior performance with dramatic reductions in hardware consumption. The
quadratic functions encompass rich harmonic content in their Fourier series,
delivering enhanced expressivity for high-frequency signals, as verified
through Neural Tangent Kernel (NTK) analysis. We develop a unified $N$-stage
pipeline framework that facilitates efficient hardware implementation of
various AFs in INRs. We demonstrate FPGA implementations on the VCU128 platform
and an ASIC implementation in a 28nm process. Experiments across images and
videos show that QuadINR achieves up to 2.06dB PSNR improvement over prior
work, with an area of only 1914$\mu$m$^2$ and a dynamic power of 6.14mW,
reducing resource and power consumption by up to 97\% and improving latency by
up to 93\% vs existing baselines.

</details>


### [67] [Img2ST-Net: Efficient High-Resolution Spatial Omics Prediction from Whole Slide Histology Images via Fully Convolutional Image-to-Image Learning](https://arxiv.org/abs/2508.14393)
*Junchao Zhu,Ruining Deng,Junlin Guo,Tianyuan Yao,Juming Xiong,Chongyu Qu,Mengmeng Yin,Yu Wang,Shilin Zhao,Haichun Yang,Daguang Xu,Yucheng Tang,Yuankai Huo*

Main category: cs.CV

TL;DR: 高分辨率空间转录组（ST）数据生成面临计算和建模挑战，本文提出Img2ST-Net框架，可高效并行地从组织病理图像预测高分辨率ST数据，并引入专为高分辨率ST分析的评估指标SSIM-ST。


<details>
  <summary>Details</summary>
Motivation: 多模态AI生成空间转录组（ST）数据成本高昂且耗时。随着ST分辨率（如Visium HD）的提高，传统逐点回归框架变得低效且不稳定，且高分辨率ST数据固有的极端稀疏性和低表达水平进一步增加了预测和评估的难度。

Method: 本文提出Img2ST-Net，一个新颖的组织病理学到ST数据生成框架，用于高效并行的高分辨率ST预测。它采用全卷积架构并行生成密集的HD基因表达图，通过将HD ST数据建模为超像素表示，将任务重构为超内容图像生成问题。此外，为增强稀疏表达模式下的鲁棒性，引入了基于结构相似度的新型评估指标SSIM-ST。

Result: Img2ST-Net提高了计算效率，并更好地保留了空间组学数据固有的空间组织。该框架实现了可扩展、生物学上连贯的高分辨率ST预测，提供了高效准确的ST推断解决方案。

Conclusion: Img2ST-Net为高分辨率ST预测提供了一个原则性的解决方案，其贡献为开发下一代鲁棒且分辨率感知的ST建模奠定了基础。

Abstract: Recent advances in multi-modal AI have demonstrated promising potential for
generating the currently expensive spatial transcriptomics (ST) data directly
from routine histology images, offering a means to reduce the high cost and
time-intensive nature of ST data acquisition. However, the increasing
resolution of ST, particularly with platforms such as Visium HD achieving 8um
or finer, introduces significant computational and modeling challenges.
Conventional spot-by-spot sequential regression frameworks become inefficient
and unstable at this scale, while the inherent extreme sparsity and low
expression levels of high-resolution ST further complicate both prediction and
evaluation. To address these limitations, we propose Img2ST-Net, a novel
histology-to-ST generation framework for efficient and parallel high-resolution
ST prediction. Unlike conventional spot-by-spot inference methods, Img2ST-Net
employs a fully convolutional architecture to generate dense, HD gene
expression maps in a parallelized manner. By modeling HD ST data as super-pixel
representations, the task is reformulated from image-to-omics inference into a
super-content image generation problem with hundreds or thousands of output
channels. This design not only improves computational efficiency but also
better preserves the spatial organization intrinsic to spatial omics data. To
enhance robustness under sparse expression patterns, we further introduce
SSIM-ST, a structural-similarity-based evaluation metric tailored for
high-resolution ST analysis. We present a scalable, biologically coherent
framework for high-resolution ST prediction. Img2ST-Net offers a principled
solution for efficient and accurate ST inference at scale. Our contributions
lay the groundwork for next-generation ST modeling that is robust and
resolution-aware. The source code has been made publicly available at
https://github.com/hrlblab/Img2ST-Net.

</details>


### [68] [CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities](https://arxiv.org/abs/2508.14405)
*Yue Gong,Shanyuan Liu,Liuzhuozheng Li,Jian Zhu,Bo Cheng,Liebucha Wu,Xiaoyu Wu,Yuhang Ma,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: 我们提出了CTA-Flux，一种适配方法，使基于英文训练的Flux文生图模型能够理解中文输入，并显著提升中文图像生成质量和文化真实性，同时保持参数效率和插件兼容性。


<details>
  <summary>Details</summary>
Motivation: Flux等文生图模型主要基于英文语料训练，导致其在处理中文提示词时性能不佳，难以准确捕捉中文特有的语言和文化语义，从而影响图像真实性和质量。现有方法（如翻译或微调）无法有效解决文化特异性语义问题。

Method: 我们引入了Chinese Text Adapter-Flux (CTA-Flux)，该方法利用多模态扩散Transformer (MMDiT) 直接控制Flux模型的骨干网络，从而在显著减少参数量的同时，增强模型对中文语义的理解，而非依赖ControlNet等大规模参数架构。

Result: 经验评估表明，CTA-Flux能够支持中文和英文提示词输入，并实现了卓越的图像生成质量、视觉真实感以及对中文语义的忠实描绘。它在不需大量重新训练整个模型的情况下，显著提升了生成质量和文化真实性，并兼容现有文生图插件。

Conclusion: CTA-Flux成功地弥合了中文语义理解与英文中心文生图模型的兼容性，有效解决了现有模型在处理中文输入时的局限性，提供了一种高效、高质量且兼容性强的中文文本到图像生成解决方案。

Abstract: We proposed the Chinese Text Adapter-Flux (CTA-Flux). An adaptation method
fits the Chinese text inputs to Flux, a powerful text-to-image (TTI) generative
model initially trained on the English corpus. Despite the notable image
generation ability conditioned on English text inputs, Flux performs poorly
when processing non-English prompts, particularly due to linguistic and
cultural biases inherent in predominantly English-centric training datasets.
Existing approaches, such as translating non-English prompts into English or
finetuning models for bilingual mappings, inadequately address culturally
specific semantics, compromising image authenticity and quality. To address
this issue, we introduce a novel method to bridge Chinese semantic
understanding with compatibility in English-centric TTI model communities.
Existing approaches relying on ControlNet-like architectures typically require
a massive parameter scale and lack direct control over Chinese semantics. In
comparison, CTA-flux leverages MultiModal Diffusion Transformer (MMDiT) to
control the Flux backbone directly, significantly reducing the number of
parameters while enhancing the model's understanding of Chinese semantics. This
integration significantly improves the generation quality and cultural
authenticity without extensive retraining of the entire model, thus maintaining
compatibility with existing text-to-image plugins such as LoRA, IP-Adapter, and
ControlNet. Empirical evaluations demonstrate that CTA-flux supports Chinese
and English prompts and achieves superior image generation quality, visual
realism, and faithful depiction of Chinese semantics.

</details>


### [69] [MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing](https://arxiv.org/abs/2508.14423)
*Jeahun Sung,Changhyun Roh,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: 本文提出MoCHA-former模型，有效解决了相机捕获屏幕内容时由频率混叠引起的摩尔纹问题，该模型通过解耦、自适应处理和时空融合，在量化指标上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 便携式成像技术使得基于摄像头的屏幕捕捉变得普遍，但相机色彩滤光阵列（CFA）与显示器子像素之间的频率混叠会引起摩尔纹，严重降低捕获的照片和视频质量。现有去摩尔纹模型存在局限性，包括帧内伪影强度空间变化、大规模全局结构、通道依赖性统计以及帧间快速时间波动。

Method: 本文提出Moiré Conditioned Hybrid Adaptive Transformer (MoCHA-former)模型，包含两个核心组件：解耦摩尔纹自适应去摩尔纹（DMAD）和时空自适应去摩尔纹（STAD）。DMAD通过摩尔纹解耦模块（MDB）和细节解耦模块（DDB）分离摩尔纹和内容，并通过摩尔纹条件模块（MCB）生成摩尔纹自适应特征进行目标修复。STAD引入带窗口注意力的空间融合模块（SFB）捕捉大尺度结构，并使用特征通道注意力（FCA）对RAW帧中的通道依赖性进行建模。为确保时间一致性，MoCHA-former实现了隐式帧对齐。

Result: MoCHA-former在涵盖RAW和sRGB域的两个视频数据集上进行了定性和定量评估，在PSNR、SSIM和LPIPS等指标上始终优于现有方法。

Conclusion: MoCHA-former模型通过其创新的解耦和时空自适应机制，成功解决了摩尔纹伪影的复杂特性（包括空间变化、大规模结构、通道依赖性和时间波动），实现了卓越的去摩尔纹性能。

Abstract: Recent advances in portable imaging have made camera-based screen capture
ubiquitous. Unfortunately, frequency aliasing between the camera's color filter
array (CFA) and the display's sub-pixels induces moir\'e patterns that severely
degrade captured photos and videos. Although various demoir\'eing models have
been proposed to remove such moir\'e patterns, these approaches still suffer
from several limitations: (i) spatially varying artifact strength within a
frame, (ii) large-scale and globally spreading structures, (iii)
channel-dependent statistics and (iv) rapid temporal fluctuations across
frames. We address these issues with the Moir\'e Conditioned Hybrid Adaptive
Transformer (MoCHA-former), which comprises two key components: Decoupled
Moir\'e Adaptive Demoir\'eing (DMAD) and Spatio-Temporal Adaptive Demoir\'eing
(STAD). DMAD separates moir\'e and content via a Moir\'e Decoupling Block (MDB)
and a Detail Decoupling Block (DDB), then produces moir\'e-adaptive features
using a Moir\'e Conditioning Block (MCB) for targeted restoration. STAD
introduces a Spatial Fusion Block (SFB) with window attention to capture
large-scale structures, and a Feature Channel Attention (FCA) to model channel
dependence in RAW frames. To ensure temporal consistency, MoCHA-former performs
implicit frame alignment without any explicit alignment module. We analyze
moir\'e characteristics through qualitative and quantitative studies, and
evaluate on two video datasets covering RAW and sRGB domains. MoCHA-former
consistently surpasses prior methods across PSNR, SSIM, and LPIPS.

</details>


### [70] [HyperDiff: Hypergraph Guided Diffusion Model for 3D Human Pose Estimation](https://arxiv.org/abs/2508.14431)
*Bing Han,Yuhua Huang,Pan Gao*

Main category: cs.CV

TL;DR: HyperDiff提出了一种结合扩散模型和HyperGCN的单目3D人体姿态估计方法，有效解决深度模糊和遮挡，并利用多尺度特征，实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 单目3D人体姿态估计（HPE）在2D到3D提升过程中面临深度模糊和遮挡问题。传统方法在利用骨架结构信息时，忽略了多尺度骨架特征，影响了姿态估计的准确性。

Method: 本文提出了HyperDiff方法，该方法将扩散模型与HyperGCN相结合。扩散模型有效捕获数据不确定性以缓解深度模糊和遮挡。HyperGCN作为去噪器，利用多粒度结构精确建模关节之间的高阶关联，提高了模型在复杂姿态下的去噪能力。

Result: 实验结果表明，HyperDiff在Human3.6M和MPI-INF-3DHP数据集上均取得了最先进的性能，并且能够灵活适应不同的计算资源以平衡性能和效率。

Conclusion: HyperDiff通过集成扩散模型和HyperGCN，有效解决了单目3D人体姿态估计中的深度模糊、遮挡和多尺度特征利用不足等挑战，实现了卓越的性能和良好的资源适应性。

Abstract: Monocular 3D human pose estimation (HPE) often encounters challenges such as
depth ambiguity and occlusion during the 2D-to-3D lifting process.
Additionally, traditional methods may overlook multi-scale skeleton features
when utilizing skeleton structure information, which can negatively impact the
accuracy of pose estimation. To address these challenges, this paper introduces
a novel 3D pose estimation method, HyperDiff, which integrates diffusion models
with HyperGCN. The diffusion model effectively captures data uncertainty,
alleviating depth ambiguity and occlusion. Meanwhile, HyperGCN, serving as a
denoiser, employs multi-granularity structures to accurately model high-order
correlations between joints. This improves the model's denoising capability
especially for complex poses. Experimental results demonstrate that HyperDiff
achieves state-of-the-art performance on the Human3.6M and MPI-INF-3DHP
datasets and can flexibly adapt to varying computational resources to balance
performance and efficiency.

</details>


### [71] [FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation](https://arxiv.org/abs/2508.14437)
*Gabriel Tjio,Jie Zhang,Xulei Yang,Yun Xing,Nhat Chung,Xiaofeng Cao,Ivor W. Tsang,Chee Keong Kwoh,Qing Guo*

Main category: cs.CV

TL;DR: 提出FOCUS，一种基于频率的扩散驱动输入适应方法，旨在解决测试时间适应中知识保留与领域适应之间的平衡问题，并在语义分割和深度估计任务上实现SOTA性能并减轻灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 测试时间适应（TTA）在应对不断变化的领域时面临挑战，即如何在保留已有知识和适应领域偏移之间取得平衡。适应领域偏移可能导致任务相关知识的遗忘（灾难性遗忘），现有方法难以解决这一矛盾。

Method: 提出FOCUS，一种新颖的基于频率的条件化方法，置于扩散驱动的输入适应框架内。该方法利用学习到的、空间自适应的频率先验来条件化去噪过程的反向步骤，以保留密集预测任务中的语义信息。核心组件包括：1) 轻量级Y形频率预测网络（Y-FPN），用于分离高低频信息并降低计算成本；2) FrequencyMix，一种新型数据增强方法，通过扰动不同频率带来训练Y-FPN，提高鲁棒性。FOCUS去噪后的图像还能生成伪标签，为现有模型适应方法提供额外监督。

Result: FOCUS在15种腐蚀类型和三个数据集上的语义分割和单目深度估计任务中表现出有效性，取得了最先进的平均性能。此外，FOCUS还能通过提供伪标签来补充现有模型适应方法，即使在有限的伪标签监督下，也能显著减轻现有模型适应方法的灾难性遗忘。

Conclusion: FOCUS成功解决了测试时间适应中知识保留与领域适应的平衡难题。它在多腐蚀场景下实现了最先进的性能，并通过提供高质量的伪标签，有效增强了现有模型适应方法，特别是减轻了灾难性遗忘。该方法为鲁棒的输入适应提供了一个高效且互补的解决方案。

Abstract: Test-time adaptation enables models to adapt to evolving domains. However,
balancing the tradeoff between preserving knowledge and adapting to domain
shifts remains challenging for model adaptation methods, since adapting to
domain shifts can induce forgetting of task-relevant knowledge. To address this
problem, we propose FOCUS, a novel frequency-based conditioning approach within
a diffusion-driven input-adaptation framework. Utilising learned, spatially
adaptive frequency priors, our approach conditions the reverse steps during
diffusion-driven denoising to preserve task-relevant semantic information for
dense prediction.
  FOCUS leverages a trained, lightweight, Y-shaped Frequency Prediction Network
(Y-FPN) that disentangles high and low frequency information from noisy images.
This minimizes the computational costs involved in implementing our approach in
a diffusion-driven framework. We train Y-FPN with FrequencyMix, a novel data
augmentation method that perturbs the images across diverse frequency bands,
which improves the robustness of our approach to diverse corruptions.
  We demonstrate the effectiveness of FOCUS for semantic segmentation and
monocular depth estimation across 15 corruption types and three datasets,
achieving state-of-the-art averaged performance. In addition to improving
standalone performance, FOCUS complements existing model adaptation methods
since we can derive pseudo labels from FOCUS-denoised images for additional
supervision. Even under limited, intermittent supervision with the pseudo
labels derived from the FOCUS denoised images, we show that FOCUS mitigates
catastrophic forgetting for recent model adaptation methods.

</details>


### [72] [MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion](https://arxiv.org/abs/2508.14440)
*Fei Peng,Junqiang Wu,Yan Li,Tingting Gao,Di Zhang,Huiyuan Fu*

Main category: cs.CV

TL;DR: MUSE通过引入串联交叉注意力（CCA）和两阶段训练策略，解决了扩散模型在多主体布局可控图像合成中空间精度和主体身份难以同时兼顾的挑战，实现了零样本高质量生成。


<details>
  <summary>Details</summary>
Motivation: 现有文生图扩散模型在多主体合成中，难以同时实现精确的空间控制和忠实的主体身份重建。尤其在布局可控多主体合成（LMS）任务中，现有方法难以兼顾空间精度和身份保持。

Method: 提出MUSE统一合成框架。主要方法包括：1. 串联交叉注意力（CCA），通过显式语义空间扩展，无缝整合布局规格和文本指导，实现空间约束与文本描述的双向模态对齐。2. 渐进式两阶段训练策略，将LMS任务分解为可学习的子目标进行有效优化。

Result: MUSE在零样本端到端生成中，相比现有解决方案，实现了卓越的空间准确性和主体身份一致性，成功解决了多主体布局可控合成的难题。

Conclusion: MUSE框架显著提升了可控图像合成的能力，特别是解决了多主体布局控制合成的挑战，推动了该领域的发展，且代码和模型已开源。

Abstract: Existing text-to-image diffusion models have demonstrated remarkable
capabilities in generating high-quality images guided by textual prompts.
However, achieving multi-subject compositional synthesis with precise spatial
control remains a significant challenge. In this work, we address the task of
layout-controllable multi-subject synthesis (LMS), which requires both faithful
reconstruction of reference subjects and their accurate placement in specified
regions within a unified image. While recent advancements have separately
improved layout control and subject synthesis, existing approaches struggle to
simultaneously satisfy the dual requirements of spatial precision and identity
preservation in this composite task. To bridge this gap, we propose MUSE, a
unified synthesis framework that employs concatenated cross-attention (CCA) to
seamlessly integrate layout specifications with textual guidance through
explicit semantic space expansion. The proposed CCA mechanism enables
bidirectional modality alignment between spatial constraints and textual
descriptions without interference. Furthermore, we design a progressive
two-stage training strategy that decomposes the LMS task into learnable
sub-objectives for effective optimization. Extensive experiments demonstrate
that MUSE achieves zero-shot end-to-end generation with superior spatial
accuracy and identity consistency compared to existing solutions, advancing the
frontier of controllable image synthesis. Our code and model are available at
https://github.com/pf0607/MUSE.

</details>


### [73] [Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting](https://arxiv.org/abs/2508.14443)
*Gyusam Chang,Tuan-Anh Vu,Vivek Alumootil,Harris Song,Deanna Pham,Sangpil Kim,M. Khalid Jawed*

Main category: cs.CV

TL;DR: 本文针对农业3D重建的挑战，提出多模态数据集NIRPlant和创新的NIRSplat高斯泼溅架构，融合近红外与文本元数据，显著提升了复杂农业场景下的3D重建性能。


<details>
  <summary>Details</summary>
Motivation: 尽管3D高斯泼溅技术(3DGS)发展迅速，但其在农业领域的应用尚待探索。农业场景因光照不均、遮挡和有限视场等独特挑战，对3D重建方法提出了较高要求。

Method: 1. 引入多模态数据集NIRPlant，整合近红外(NIR)、RGB图像、文本元数据、深度和LiDAR数据。2. 利用NIR数据增强鲁棒性并提供植物学见解。3. 运用来自植被指数（如NDVI、NDWI）的文本元数据丰富上下文理解。4. 提出NIRSplat，一种多模态高斯泼溅架构，结合交叉注意力机制与3D点位姿编码，以提供鲁棒的几何先验。

Result: NIRSplat在综合实验中表现出色，性能优于现有主流方法（包括3DGS、CoR-GS和InstantSplat），证明了其在挑战性农业场景中的有效性。

Conclusion: NIRSplat通过有效整合多模态数据（尤其是近红外信息和植被指数），成功解决了农业3D重建的独特难题，显著提升了该领域高斯泼溅技术的性能和适用性。

Abstract: While 3D Gaussian Splatting (3DGS) has rapidly advanced, its application in
agriculture remains underexplored. Agricultural scenes present unique
challenges for 3D reconstruction methods, particularly due to uneven
illumination, occlusions, and a limited field of view. To address these
limitations, we introduce \textbf{NIRPlant}, a novel multimodal dataset
encompassing Near-Infrared (NIR) imagery, RGB imagery, textual metadata, Depth,
and LiDAR data collected under varied indoor and outdoor lighting conditions.
By integrating NIR data, our approach enhances robustness and provides crucial
botanical insights that extend beyond the visible spectrum. Additionally, we
leverage text-based metadata derived from vegetation indices, such as NDVI,
NDWI, and the chlorophyll index, which significantly enriches the contextual
understanding of complex agricultural environments. To fully exploit these
modalities, we propose \textbf{NIRSplat}, an effective multimodal Gaussian
splatting architecture employing a cross-attention mechanism combined with 3D
point-based positional encoding, providing robust geometric priors.
Comprehensive experiments demonstrate that \textbf{NIRSplat} outperforms
existing landmark methods, including 3DGS, CoR-GS, and InstantSplat,
highlighting its effectiveness in challenging agricultural scenarios. The code
and dataset are publicly available at:
https://github.com/StructuresComp/3D-Reconstruction-NIR

</details>


### [74] [Generalizable Engagement Estimation in Conversation via Domain Prompting and Parallel Attention](https://arxiv.org/abs/2508.14448)
*Yangche Yu,Yin Chen,Jia Li,Peng Jia,Yu Zhang,Li Dai,Zhenzhen Hu,Meng Wang,Richang Hong*

Main category: cs.CV

TL;DR: DAPA是一个新颖的领域自适应并行注意力框架，通过引入领域提示机制和并行交叉注意力模块，显著提升了对话参与度估计的泛化能力和SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 准确的参与度估计对自适应人机交互系统至关重要，但其鲁棒部署受限于跨领域泛化性差以及难以建模复杂交互动态的问题。

Method: 本文提出了DAPA（Domain-Adaptive Parallel Attention）框架。DAPA引入了领域提示机制，通过在输入前添加可学习的领域特定向量来显式地根据数据来源调整模型。同时，DAPA包含一个并行交叉注意力模块，显式对齐参与者之间的反应性（前向BiLSTM）和预期性（后向BiLSTM）状态，以捕捉交互同步性。

Result: DAPA在多个跨文化和跨语言基准测试中达到了新的最先进性能，特别是在NoXi-J测试集上，相比强基线，其CCC（一致性相关系数）绝对值提高了0.45。该方法还在MultiMediate'25多领域参与度估计挑战赛中获得第一名。

Conclusion: DAPA框架通过有效解决跨领域泛化和复杂交互动态建模的挑战，显著提升了对话参与度估计的准确性和鲁棒性，证实了其优越性。

Abstract: Accurate engagement estimation is essential for adaptive human-computer
interaction systems, yet robust deployment is hindered by poor generalizability
across diverse domains and challenges in modeling complex interaction
dynamics.To tackle these issues, we propose DAPA (Domain-Adaptive Parallel
Attention), a novel framework for generalizable conversational engagement
modeling. DAPA introduces a Domain Prompting mechanism by prepending learnable
domain-specific vectors to the input, explicitly conditioning the model on the
data's origin to facilitate domain-aware adaptation while preserving
generalizable engagement representations. To capture interactional synchrony,
the framework also incorporates a Parallel Cross-Attention module that
explicitly aligns reactive (forward BiLSTM) and anticipatory (backward BiLSTM)
states between participants.Extensive experiments demonstrate that DAPA
establishes a new state-of-the-art performance on several cross-cultural and
cross-linguistic benchmarks, notably achieving an absolute improvement of 0.45
in Concordance Correlation Coefficient (CCC) over a strong baseline on the
NoXi-J test set. The superiority of our method was also confirmed by winning
the first place in the Multi-Domain Engagement Estimation Challenge at
MultiMediate'25.

</details>


### [75] [D^3-Talker: Dual-Branch Decoupled Deformation Fields for Few-Shot 3D Talking Head Synthesis](https://arxiv.org/abs/2508.14449)
*Yuhang Guo,Kaijun Deng,Siyang Song,Jindong Xie,Wenhui Ma,Linlin Shen*

Main category: cs.CV

TL;DR: D^3-Talker提出一种新颖方法，通过解耦通用和个性化形变，仅用少量数据即可实现高保真3D说话人像合成和准确的音唇同步。


<details>
  <summary>Details</summary>
Motivation: 现有3D说话人像合成方法依赖长时间视频训练，且在少量数据下难以实现真实唇部行为映射，导致唇部同步差和图像质量低，因为音频中包含与唇部运动无关的信息。

Method: 本研究提出D^3-Talker，构建静态3D高斯属性场，并利用音频和面部运动信号独立控制两个不同的高斯属性形变场，以有效解耦通用和个性化形变预测。预训练阶段设计了一种新颖的相似性对比损失函数以实现更彻底的解耦。此外，整合了Coarse-to-Fine模块，以优化渲染图像质量并减轻头部运动引起的模糊。

Result: 大量实验表明，D^3-Talker在有限训练数据下，无论在高保真渲染还是准确的音唇同步方面，均优于现有最先进的方法。

Conclusion: D^3-Talker通过创新的解耦机制和精炼模块，有效解决了3D说话人像合成中对大量数据依赖的问题，显著提升了生成质量和同步精度。

Abstract: A key challenge in 3D talking head synthesis lies in the reliance on a
long-duration talking head video to train a new model for each target identity
from scratch. Recent methods have attempted to address this issue by extracting
general features from audio through pre-training models. However, since audio
contains information irrelevant to lip motion, existing approaches typically
struggle to map the given audio to realistic lip behaviors in the target face
when trained on only a few frames, causing poor lip synchronization and talking
head image quality. This paper proposes D^3-Talker, a novel approach that
constructs a static 3D Gaussian attribute field and employs audio and Facial
Motion signals to independently control two distinct Gaussian attribute
deformation fields, effectively decoupling the predictions of general and
personalized deformations. We design a novel similarity contrastive loss
function during pre-training to achieve more thorough decoupling. Furthermore,
we integrate a Coarse-to-Fine module to refine the rendered images, alleviating
blurriness caused by head movements and enhancing overall image quality.
Extensive experiments demonstrate that D^3-Talker outperforms state-of-the-art
methods in both high-fidelity rendering and accurate audio-lip synchronization
with limited training data. Our code will be provided upon acceptance.

</details>


### [76] [Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and Inverse Rendering](https://arxiv.org/abs/2508.14461)
*Shanlin Sun,Yifan Wang,Hanwen Zhang,Yifeng Xiong,Qin Ren,Ruogu Fang,Xiaohui Xie,Chenyu You*

Main category: cs.CV

TL;DR: Ouroboros框架利用两个相互强化的单步扩散模型，解决了正向和逆向渲染中循环不一致和推理速度慢的问题，并在图像和视频分解中实现了SOTA性能和更快的速度。


<details>
  <summary>Details</summary>
Motivation: 现有多步扩散模型在正向和逆向渲染上独立处理，导致循环不一致和推理速度缓慢，需要一个能协同处理并提升效率的方法。

Method: 提出了Ouroboros框架，包含两个相互强化的单步扩散模型，分别处理正向和逆向渲染。该方法将内在分解扩展到室内外场景，并引入循环一致性机制。此外，可无训练地应用于视频分解。

Result: 在多种场景下达到最先进的性能，且推理速度比其他扩散方法快得多。应用于视频分解时，显著减少了时间不一致性，同时保持了高质量的逐帧逆向渲染。

Conclusion: Ouroboros通过其创新的双向单步扩散和循环一致性机制，有效提升了正向和逆向渲染的一致性和效率，并在图像和视频分解任务中展现了卓越的性能和实用性。

Abstract: While multi-step diffusion models have advanced both forward and inverse
rendering, existing approaches often treat these problems independently,
leading to cycle inconsistency and slow inference speed. In this work, we
present Ouroboros, a framework composed of two single-step diffusion models
that handle forward and inverse rendering with mutual reinforcement. Our
approach extends intrinsic decomposition to both indoor and outdoor scenes and
introduces a cycle consistency mechanism that ensures coherence between forward
and inverse rendering outputs. Experimental results demonstrate
state-of-the-art performance across diverse scenes while achieving
substantially faster inference speed compared to other diffusion-based methods.
We also demonstrate that Ouroboros can transfer to video decomposition in a
training-free manner, reducing temporal inconsistency in video sequences while
maintaining high-quality per-frame inverse rendering.

</details>


### [77] [DreamSwapV: Mask-guided Subject Swapping for Any Customized Video Editing](https://arxiv.org/abs/2508.14465)
*Weitao Wang,Zichen Wang,Hongdeng Shen,Yulei Lu,Xirui Fan,Suhui Wu,Jun Zhang,Haoqian Wang,Hao Zhang*

Main category: cs.CV

TL;DR: DreamSwapV是一种遮罩引导、主题无关的端到端视频主体替换框架，能高效替换视频中的任意主体，通过多条件融合和自适应遮罩策略提升效果，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 定制化视频编辑需求激增，其中主体替换是关键但尚未充分探索的领域。现有方法多局限于狭窄领域或依赖间接、模糊的编辑方式，导致最终保真度受损。

Method: 提出DreamSwapV框架，一个遮罩引导、主题无关的端到端方案，通过用户指定遮罩和参考图像替换视频中的任意主体。为实现细粒度指导，引入多条件和专用条件融合模块。设计自适应遮罩策略以适应不同尺度和属性的主体，并改善替换主体与周围环境的互动。采用精心设计的两阶段数据集构建和训练方案。

Result: DreamSwapV在VBench指标和首次引入的DreamSwapV-Benchmark上进行的全面实验验证中，表现优于现有方法。

Conclusion: DreamSwapV为视频主体替换提供了一个通用、高质量的解决方案，有效克服了现有方法的局限性，并在性能上超越了现有技术。

Abstract: With the rapid progress of video generation, demand for customized video
editing is surging, where subject swapping constitutes a key component yet
remains under-explored. Prevailing swapping approaches either specialize in
narrow domains--such as human-body animation or hand-object interaction--or
rely on some indirect editing paradigm or ambiguous text prompts that
compromise final fidelity. In this paper, we propose DreamSwapV, a mask-guided,
subject-agnostic, end-to-end framework that swaps any subject in any video for
customization with a user-specified mask and reference image. To inject
fine-grained guidance, we introduce multiple conditions and a dedicated
condition fusion module that integrates them efficiently. In addition, an
adaptive mask strategy is designed to accommodate subjects of varying scales
and attributes, further improving interactions between the swapped subject and
its surrounding context. Through our elaborate two-phase dataset construction
and training scheme, our DreamSwapV outperforms existing methods, as validated
by comprehensive experiments on VBench indicators and our first introduced
DreamSwapV-Benchmark.

</details>


### [78] [LookOut: Real-World Humanoid Egocentric Navigation](https://arxiv.org/abs/2508.14466)
*Boxiao Pan,Adam W. Harley,C. Karen Liu,Leonidas J. Guibas*

Main category: cs.CV

TL;DR: 本文提出从第一人称视频预测未来6D头部姿态的问题，并开发了基于时间聚合3D特征的预测框架。为解决数据缺乏，还发布了Aria Navigation Dataset (AND)。模型能学习并泛化人类导航行为。


<details>
  <summary>Details</summary>
Motivation: 预测从自我中心视角出发的未来无碰撞轨迹在机器人、VR/AR和辅助导航等领域至关重要。本文旨在通过预测未来6D头部姿态（包括平移和旋转）来学习主动的信息获取行为。同时，该领域训练数据极度缺乏。

Method: 1. 提出了一个框架，通过对时间聚合的3D潜在特征进行推理，来预测未来6D头部姿态，并建模环境中静态和动态部分的几何及语义约束。2. 为解决数据稀缺问题，利用Project Aria眼镜设计并实现了数据收集流程，并发布了Aria Navigation Dataset (AND)，该数据集包含4小时用户在真实世界导航的记录。

Result: 实验结果表明，所提出的模型能够学习并模仿人类的导航行为，如等待/减速、重新规划路线和环顾交通情况，并且能够泛化到未见过的环境。

Conclusion: 本工作成功定义并解决了从第一人称视频预测6D头部姿态的问题，并贡献了一个重要的真实世界导航数据集。所提出的模型能够有效捕捉并泛化人类导航行为，为未来学习真实世界第一人称导航策略提供了宝贵资源。

Abstract: The ability to predict collision-free future trajectories from egocentric
observations is crucial in applications such as humanoid robotics, VR / AR, and
assistive navigation. In this work, we introduce the challenging problem of
predicting a sequence of future 6D head poses from an egocentric video. In
particular, we predict both head translations and rotations to learn the active
information-gathering behavior expressed through head-turning events. To solve
this task, we propose a framework that reasons over temporally aggregated 3D
latent features, which models the geometric and semantic constraints for both
the static and dynamic parts of the environment. Motivated by the lack of
training data in this space, we further contribute a data collection pipeline
using the Project Aria glasses, and present a dataset collected through this
approach. Our dataset, dubbed Aria Navigation Dataset (AND), consists of 4
hours of recording of users navigating in real-world scenarios. It includes
diverse situations and navigation behaviors, providing a valuable resource for
learning real-world egocentric navigation policies. Extensive experiments show
that our model learns human-like navigation behaviors such as waiting / slowing
down, rerouting, and looking around for traffic while generalizing to unseen
environments. Check out our project webpage at
https://sites.google.com/stanford.edu/lookout.

</details>


### [79] [Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration](https://arxiv.org/abs/2508.14483)
*Haoran Bai,Xiaoxu Chen,Canqian Yang,Zongyao He,Sibin Deng,Ying Chen*

Main category: cs.CV

TL;DR: Vivid-VR是一种基于DiT的生成式视频修复方法，它利用先进的T2V模型和ControlNet，并通过概念蒸馏训练策略和重设计的控制架构，解决了传统方法中分布漂移和控制性不足的问题，显著提升了视频修复的真实感和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 传统可控的生成管道在微调时，常因多模态对齐不完美而导致分布漂移，从而损害修复后的视频纹理真实感和时间连贯性。

Method: Vivid-VR通过以下两方面解决问题：1) 提出概念蒸馏训练策略，利用预训练的T2V模型合成带有嵌入文本概念的训练样本，以蒸馏其概念理解能力，从而保持纹理和时间质量。2) 重设计控制架构，包含一个控制特征投影器（过滤降级伪影）和一个新型双分支ControlNet连接器（结合MLP特征映射和交叉注意力机制，实现动态控制特征检索和自适应控制信号调制）。

Result: Vivid-VR在合成、真实世界及AIGC视频基准测试中，表现优于现有方法，实现了令人印象深刻的纹理真实感、视觉生动度和时间一致性。

Conclusion: Vivid-VR通过其创新的概念蒸馏训练和重设计的控制架构，有效解决了生成式视频修复中的关键挑战，显著提升了修复视频的质量和可控性。

Abstract: We present Vivid-VR, a DiT-based generative video restoration method built
upon an advanced T2V foundation model, where ControlNet is leveraged to control
the generation process, ensuring content consistency. However, conventional
fine-tuning of such controllable pipelines frequently suffers from distribution
drift due to limitations in imperfect multimodal alignment, resulting in
compromised texture realism and temporal coherence. To tackle this challenge,
we propose a concept distillation training strategy that utilizes the
pretrained T2V model to synthesize training samples with embedded textual
concepts, thereby distilling its conceptual understanding to preserve texture
and temporal quality. To enhance generation controllability, we redesign the
control architecture with two key components: 1) a control feature projector
that filters degradation artifacts from input video latents to minimize their
propagation through the generation pipeline, and 2) a new ControlNet connector
employing a dual-branch design. This connector synergistically combines
MLP-based feature mapping with cross-attention mechanism for dynamic control
feature retrieval, enabling both content preservation and adaptive control
signal modulation. Extensive experiments show that Vivid-VR performs favorably
against existing approaches on both synthetic and real-world benchmarks, as
well as AIGC videos, achieving impressive texture realism, visual vividness,
and temporal consistency. The codes and checkpoints are publicly available at
https://github.com/csbhr/Vivid-VR.

</details>


### [80] [WeedSense: Multi-Task Learning for Weed Segmentation, Height Estimation, and Growth Stage Classification](https://arxiv.org/abs/2508.14486)
*Toqi Tahamid Sarker,Khaled R Ahmed,Taminul Islam,Cristiana Bernardi Rankrape,Karla Gage*

Main category: cs.CV

TL;DR: 本文提出WeedSense，一种新型多任务学习架构，用于杂草语义分割、高度估计和生长阶段分类。该模型在自建数据集上表现出色，实现高精度、实时推理，并大幅提升效率。


<details>
  <summary>Details</summary>
Motivation: 杂草管理是农业中的关键挑战，严重影响作物产量并消耗大量资源。有效的杂草监测和分析策略对实施可持续农业实践和精准管理至关重要。

Method: 引入WeedSense，一个新颖的多任务学习架构，能同时执行语义分割、高度估计和生长阶段分类。为此构建了一个独特的数据集，包含16种杂草在11周生长周期内的像素级标注和高度测量。WeedSense采用双路径编码器（包含Universal Inverted Bottleneck块）和多任务分叉解码器（基于Transformer的特征融合）。

Result: WeedSense在评估中超越了其他先进模型。在自建多任务数据集上，其语义分割mIoU达到89.78%，高度估计MAE为1.67cm，生长阶段分类准确率达99.99%。模型能以160 FPS进行实时推理，比顺序单任务执行快3倍，参数量减少32.4%。

Conclusion: WeedSense提供了一种全面、高精度、高效率的杂草监测和分析解决方案，有望支持精准农业和可持续农业实践。

Abstract: Weed management represents a critical challenge in agriculture, significantly
impacting crop yields and requiring substantial resources for control.
Effective weed monitoring and analysis strategies are crucial for implementing
sustainable agricultural practices and site-specific management approaches. We
introduce WeedSense, a novel multi-task learning architecture for comprehensive
weed analysis that jointly performs semantic segmentation, height estimation,
and growth stage classification. We present a unique dataset capturing 16 weed
species over an 11-week growth cycle with pixel-level annotations, height
measurements, and temporal labels. WeedSense leverages a dual-path encoder
incorporating Universal Inverted Bottleneck blocks and a Multi-Task Bifurcated
Decoder with transformer-based feature fusion to generate multi-scale features
and enable simultaneous prediction across multiple tasks. WeedSense outperforms
other state-of-the-art models on our comprehensive evaluation. On our
multi-task dataset, WeedSense achieves mIoU of 89.78% for segmentation, 1.67cm
MAE for height estimation, and 99.99% accuracy for growth stage classification
while maintaining real-time inference at 160 FPS. Our multitask approach
achieves 3$\times$ faster inference than sequential single-task execution and
uses 32.4% fewer parameters. Please see our project page at
weedsense.github.io.

</details>


### [81] [SATURN: Autoregressive Image Generation Guided by Scene Graphs](https://arxiv.org/abs/2508.14502)
*Thanh-Nhan Vo,Trong-Thuan Nguyen,Tam V. Nguyen,Minh-Triet Tran*

Main category: cs.CV

TL;DR: 本文提出SATURN，一种VAR-CLIP的轻量级扩展，通过将场景图转换为有序的token序列，使冻结的CLIP-VQ-VAE骨干能够理解图结构，从而显著提升文本到图像模型在布局和物体关系方面的生成质量和速度。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的文本到图像模型在照片级渲染方面表现出色，但难以捕捉复杂提示中隐含的布局和对象关系。以往基于场景图的方法依赖于笨重的GAN或扩散管道，在速度和保真度上落后于现代自回归架构。

Method: 引入SATURN（结构化三元组统一渲染网络），作为VAR-CLIP的轻量级扩展。该方法将场景图转换为显著性排序的token序列，使冻结的CLIP-VQ-VAE骨干能够解释图结构，同时仅微调VAR转换器。

Result: 在Visual Genome数据集上，SATURN将FID从56.45%降低到21.62%，并将Inception Score从16.03提高到24.78。其性能优于SG2IM和SGDiff等现有方法，且无需额外模块或多阶段训练。定性结果也证实了物体数量保真度和空间关系准确性的改进。

Conclusion: SATURN有效地将结构感知与最先进的自回归保真度相结合，克服了先前模型在处理复杂布局和对象关系方面的局限性，在文本到图像生成领域取得了显著进展。

Abstract: State-of-the-art text-to-image models excel at photorealistic rendering but
often struggle to capture the layout and object relationships implied by
complex prompts. Scene graphs provide a natural structural prior, yet previous
graph-guided approaches have typically relied on heavy GAN or diffusion
pipelines, which lag behind modern autoregressive architectures in both speed
and fidelity. We introduce SATURN (Structured Arrangement of Triplets for
Unified Rendering Networks), a lightweight extension to VAR-CLIP that
translates a scene graph into a salience-ordered token sequence, enabling a
frozen CLIP-VQ-VAE backbone to interpret graph structure while fine-tuning only
the VAR transformer. On the Visual Genome dataset, SATURN reduces FID from
56.45% to 21.62% and increases the Inception Score from 16.03 to 24.78,
outperforming prior methods such as SG2IM and SGDiff without requiring extra
modules or multi-stage training. Qualitative results further confirm
improvements in object count fidelity and spatial relation accuracy, showing
that SATURN effectively combines structural awareness with state-of-the-art
autoregressive fidelity.

</details>


### [82] [PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments](https://arxiv.org/abs/2508.14504)
*Bernd Hofmann,Albert Scheck,Joerg Franke,Patrick Bruendl*

Main category: cs.CV

TL;DR: 本文提出PB-IAD框架，利用基础模型的多模态和推理能力进行工业异常检测，通过语义指令解决了数据稀疏和动态适应性问题，并在数据稀疏场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测方法依赖大量标注数据且在动态生产环境下适应性差，限制了其可用性。基础模型感知能力的发展为解决这些挑战提供了新机遇。

Method: 提出PB-IAD（基于提示的工业异常检测）框架，利用基础模型（如GPT-4.1）的多模态和推理能力。该框架通过提示模板集成领域知识，并包含将用户输入转换为系统提示的预处理模块，旨在解决数据稀疏性、敏捷适应性和领域用户中心性问题。通过在多种场景、数据模态和消融研究中评估，并与现有SOTA方法进行基准测试。

Result: 研究结果表明，PB-IAD表现出卓越的性能，特别是在数据稀疏和少样本设置下，仅通过语义指令即可实现。

Conclusion: PB-IAD框架利用基础模型和语义指令，为工业异常检测提供了一种强大且适应性强的解决方案，尤其适用于数据有限和需要灵活定制的生产环境。

Abstract: The detection of anomalies in manufacturing processes is crucial to ensure
product quality and identify process deviations. Statistical and data-driven
approaches remain the standard in industrial anomaly detection, yet their
adaptability and usability are constrained by the dependence on extensive
annotated datasets and limited flexibility under dynamic production conditions.
Recent advances in the perception capabilities of foundation models provide
promising opportunities for their adaptation to this downstream task. This
paper presents PB-IAD (Prompt-based Industrial Anomaly Detection), a novel
framework that leverages the multimodal and reasoning capabilities of
foundation models for industrial anomaly detection. Specifically, PB-IAD
addresses three key requirements of dynamic production environments: data
sparsity, agile adaptability, and domain user centricity. In addition to the
anomaly detection, the framework includes a prompt template that is
specifically designed for iteratively implementing domain-specific process
knowledge, as well as a pre-processing module that translates domain user
inputs into effective system prompts. This user-centric design allows domain
experts to customise the system flexibly without requiring data science
expertise. The proposed framework is evaluated by utilizing GPT-4.1 across
three distinct manufacturing scenarios, two data modalities, and an ablation
study to systematically assess the contribution of semantic instructions.
Furthermore, PB-IAD is benchmarked to state-of-the-art methods for anomaly
detection such as PatchCore. The results demonstrate superior performance,
particularly in data-sparse scenarios and low-shot settings, achieved solely
through semantic instructions.

</details>


### [83] [Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles](https://arxiv.org/abs/2508.14527)
*Jiangfan Liu,Yongkang Guo,Fangzhi Zhong,Tianyuan Zhang,Zonglei Jing,Siyuan Liang,Jiakai Wang,Mingchuan Zhang,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: ScenGE框架利用大语言模型生成自动驾驶安全关键场景，有效发现更多碰撞案例并提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆部署前，模拟环境中的安全评估至关重要。然而，现有场景生成方法依赖预定义模式或基于规则的策略，限制了发现多样化和不可预见故障模式的能力。

Method: 本文提出ScenGE框架，通过推理新颖的对抗性案例并利用复杂交通流放大来生成安全关键场景。首先，通过“元场景生成”阶段，利用基于结构化驾驶知识的大语言模型，根据良性场景提示推断出合理且具挑战性的对抗性智能体行为，并将其转化为可执行代码。其次，通过“复杂场景演化”阶段，利用背景车辆放大核心威胁，构建对抗性协作图以优化智能体轨迹，从而减少自车操纵空间并创建关键遮挡。

Result: 实验结果表明，ScenGE平均比现有最佳基线发现更多严重的碰撞案例（+31.96%）。此外，ScenGE可应用于基于大型模型的自动驾驶系统并部署在不同模拟器上，且在ScenGE场景上进行的对抗性训练能提升模型鲁棒性。通过真实世界车辆测试和人工评估，验证了生成场景的合理性和关键性。

Conclusion: ScenGE框架提供了一种有效且创新的方法，能够生成更真实、更具挑战性的自动驾驶安全关键场景，有助于发现未知故障模式，提升模型鲁棒性，从而为建立公众信任和确保自动驾驶车辆的安全部署奠定基础。

Abstract: The generation of safety-critical scenarios in simulation has become
increasingly crucial for safety evaluation in autonomous vehicles prior to road
deployment in society. However, current approaches largely rely on predefined
threat patterns or rule-based strategies, which limit their ability to expose
diverse and unforeseen failure modes. To overcome these, we propose ScenGE, a
framework that can generate plentiful safety-critical scenarios by reasoning
novel adversarial cases and then amplifying them with complex traffic flows.
Given a simple prompt of a benign scene, it first performs Meta-Scenario
Generation, where a large language model, grounded in structured driving
knowledge, infers an adversarial agent whose behavior poses a threat that is
both plausible and deliberately challenging. This meta-scenario is then
specified in executable code for precise in-simulator control. Subsequently,
Complex Scenario Evolution uses background vehicles to amplify the core threat
introduced by Meta-Scenario. It builds an adversarial collaborator graph to
identify key agent trajectories for optimization. These perturbations are
designed to simultaneously reduce the ego vehicle's maneuvering space and
create critical occlusions. Extensive experiments conducted on multiple
reinforcement learning based AV models show that ScenGE uncovers more severe
collision cases (+31.96%) on average than SoTA baselines. Additionally, our
ScenGE can be applied to large model based AV systems and deployed on different
simulators; we further observe that adversarial training on our scenarios
improves the model robustness. Finally, we validate our framework through
real-world vehicle tests and human evaluation, confirming that the generated
scenarios are both plausible and critical. We hope our paper can build up a
critical step towards building public trust and ensuring their safe deployment.

</details>


### [84] [WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion](https://arxiv.org/abs/2508.14537)
*Yonghan Shin,SeungKyu Kim,Won-Ki Jeong*

Main category: cs.CV

TL;DR: 针对计算病理学中全切片图像（WSI）编码耗时过长的问题，本文提出了WISE-FUSE框架，通过选择性处理诊断相关区域，显著缩短编码时间，同时保持或提升诊断性能。


<details>
  <summary>Details</summary>
Motivation: 计算病理学中的全切片图像（WSI）因其巨大的像素规模，导致编码成本高昂，预处理和训练时间长达数天甚至数周，成为实际部署中显著的性能瓶颈。

Method: 本文提出WISE-FUSE，一个自适应WSI编码框架。它利用病理学领域的视觉-语言模型和大型语言模型，通过选择性处理诊断相关区域来解决问题。具体方法包括：首先，使用知识蒸馏机制计算低分辨率图像块与类别特定文本描述的相似度；其次，根据相似度分数选择一小部分信息丰富的区域；最后，对相应的高分辨率图像块进行选择性编码，并与文本嵌入融合以增强诊断上下文。

Result: 实验证明，WISE-FUSE将WSI编码时间缩短了三倍以上，同时诊断性能与穷举式图像块处理相当或更优。

Conclusion: WISE-FUSE为计算病理学提供了一个可扩展且实用的WSI编码解决方案，有效解决了其在实际部署中的计算瓶颈。

Abstract: Whole slide images (WSIs) in computational pathology (CPath) pose a major
computational challenge due to their gigapixel scale, often requiring the
processing of tens to hundreds of thousands of high-resolution patches per
slide. This results in prohibitive encoding costs, with preprocessing and
training times extending to days or even weeks-making WSI encoding the most
significant bottleneck in real-world deployment. In this work, we propose
WISE-FUSE, an adaptive WSI encoding framework that leverages pathology-domain
vision-language models and large language models to address this challenge by
selectively processing diagnostically relevant regions. WISE-FUSE first
computes similarity scores between low-resolution patches and class-specific
textual descriptions using a knowledge distillation mechanism that preserves
fine-grained diagnostic features. Based on these similarity scores, we select a
small subset of informative regions for the target task, which quickly
eliminates irrelevant patches at the coarse level. The corresponding
high-resolution patches are then selectively encoded and fused with textual
embeddings to reinforce diagnostic context. Extensive experiments demonstrate
that WISE-FUSE reduces WSI encoding time by over threefold while achieving
diagnostic performance comparable to or surpassing that of exhaustive patch
processing, offering a scalable and practical solution for CPath.

</details>


### [85] [Improving OCR using internal document redundancy](https://arxiv.org/abs/2508.14557)
*Diego Belzarena,Seginus Mowlavi,Aitor Artola,Camilo Mariño,Marina Gardella,Ignacio Ramírez,Antoine Tadros,Roy He,Natalia Bottaioli,Boshra Rajaei,Gregory Randall,Jean-Michel Morel*

Main category: cs.CV

TL;DR: 本文提出一种无监督方法，利用文档内部字符形状冗余来校正OCR系统的不完善输出并改进聚类，有效提升了对低质量文档的识别表现。


<details>
  <summary>Details</summary>
Motivation: 现有OCR系统在处理低质量印刷文档时识别能力有限，难以泛化，且未能充分利用文档内部固有的冗余信息进行错误校正。

Method: 引入一种扩展的高斯混合模型（GMM），该模型结合了期望最大化（EM）算法、簇内重新对齐过程和正态性统计检验。

Result: 该方法在多种程度降级的文档上（包括恢复的乌拉圭军事档案和17至20世纪中期的欧洲报纸）均展示了改进效果。

Conclusion: 所提出的无监督方法能有效利用文档冗余来修正OCR输出并改善聚类，从而显著提升了对低质量历史文档的识别精度。

Abstract: Current OCR systems are based on deep learning models trained on large
amounts of data. Although they have shown some ability to generalize to unseen
data, especially in detection tasks, they can struggle with recognizing
low-quality data. This is particularly evident for printed documents, where
intra-domain data variability is typically low, but inter-domain data
variability is high. In that context, current OCR methods do not fully exploit
each document's redundancy. We propose an unsupervised method by leveraging the
redundancy of character shapes within a document to correct imperfect outputs
of a given OCR system and suggest better clustering. To this aim, we introduce
an extended Gaussian Mixture Model (GMM) by alternating an
Expectation-Maximization (EM) algorithm with an intra-cluster realignment
process and normality statistical testing. We demonstrate improvements in
documents with various levels of degradation, including recovered Uruguayan
military archives and 17th to mid-20th century European newspapers.

</details>


### [86] [A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives](https://arxiv.org/abs/2508.14558)
*Juepeng Zheng,Zi Ye,Yibin Wen,Jianxi Huang,Zhiwei Zhang,Qingmei Li,Qiong Hu,Baodong Xu,Lingyuan Zhao,Haohuan Fu*

Main category: cs.CV

TL;DR: 本综述系统回顾了基于遥感图像的农田地块及边界划定（APBD）方法，对其进行了分类，并讨论了相关问题、应用及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 高空间分辨率遥感图像为自动化、高精度农业普查和分析提供了巨大潜力。为了实现对每个农田地块的清查，需要有效的农田地块及边界划定（APBD）方法。本研究旨在系统梳理和提供现有APBD研究的清晰知识图谱。

Method: 本研究是一篇综述论文，对近期APBD相关文献进行了全面审查和元数据分析，包括算法、研究地点、作物类型、传感器类型和评估方法等。文章将APBD方法分为三类：传统图像处理方法（基于像素、边缘、区域）、传统机器学习方法（如随机森林、决策树）和深度学习方法。重点讨论了深度学习中的语义分割、目标检测和Transformer等方法。此外，还探讨了多传感器数据、单任务与多任务学习比较、不同算法和APBD任务比较等五个APBD相关问题。

Result: 本综述对APBD方法进行了系统分类，并详细讨论了深度学习方法在其中的主导作用。揭示了APBD领域中多传感器数据应用、学习范式选择、算法与任务对比等关键议题。最后，提出了APBD相关的应用场景，以及未来研究中令人期待的展望和潜在热点课题。

Conclusion: 本综述旨在帮助APBD领域的研究人员了解该领域的发展和趋势，提供清晰的知识图谱，并为未来的研究指明方向。

Abstract: Powered by advances in multiple remote sensing sensors, the production of
high spatial resolution images provides great potential to achieve
cost-efficient and high-accuracy agricultural inventory and analysis in an
automated way. Lots of studies that aim at providing an inventory of the level
of each agricultural parcel have generated many methods for Agricultural Parcel
and Boundary Delineation (APBD). This review covers APBD methods for detecting
and delineating agricultural parcels and systematically reviews the past and
present of APBD-related research applied to remote sensing images. With the
goal to provide a clear knowledge map of existing APBD efforts, we conduct a
comprehensive review of recent APBD papers to build a meta-data analysis,
including the algorithm, the study site, the crop type, the sensor type, the
evaluation method, etc. We categorize the methods into three classes: (1)
traditional image processing methods (including pixel-based, edge-based and
region-based); (2) traditional machine learning methods (such as random forest,
decision tree); and (3) deep learning-based methods. With deep
learning-oriented approaches contributing to a majority, we further discuss
deep learning-based methods like semantic segmentation-based, object
detection-based and Transformer-based methods. In addition, we discuss five
APBD-related issues to further comprehend the APBD domain using remote sensing
data, such as multi-sensor data in APBD task, comparisons between single-task
learning and multi-task learning in the APBD domain, comparisons among
different algorithms and different APBD tasks, etc. Finally, this review
proposes some APBD-related applications and a few exciting prospects and
potential hot topics in future APBD research. We hope this review help
researchers who involved in APBD domain to keep track of its development and
tendency.

</details>


### [87] [Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization](https://arxiv.org/abs/2508.14561)
*Sukhyun Jeong,Hong-Gi Shin,Yong-Hoon Choi*

Main category: cs.CV

TL;DR: 本文提出一种利用残差矢量量化 (RVQ) 增强离散姿态码的文本到动作生成方法，以捕获细粒度运动细节并提高可控性。


<details>
  <summary>Details</summary>
Motivation: 现有可控动作生成 (CoMo) 方法依赖离散姿态码，无法捕捉细粒度动作细节，限制了表达能力。

Method: 提出一种方法，通过残差矢量量化 (RVQ) 技术，用连续运动特征增强基于姿态码的潜在表示。此设计旨在保留姿态码的可解释性和可操作性，同时捕获高频细节等微妙运动特征。

Result: 在 HumanML3D 数据集上，模型将 Frechet Inception Distance (FID) 从 0.041 降低到 0.015，Top-1 R-Precision 从 0.508 提高到 0.510。定性分析也证实了模型在动作编辑方面的可控性。

Conclusion: 该方法成功克服了离散姿态码的局限性，通过融入连续运动特征，显著提升了文本到动作生成的细节捕捉能力和可控性。

Abstract: Recent progress in text-to-motion has advanced both 3D human motion
generation and text-based motion control. Controllable motion generation
(CoMo), which enables intuitive control, typically relies on pose code
representations, but discrete pose codes alone cannot capture fine-grained
motion details, limiting expressiveness. To overcome this, we propose a method
that augments pose code-based latent representations with continuous motion
features using residual vector quantization (RVQ). This design preserves the
interpretability and manipulability of pose codes while effectively capturing
subtle motion characteristics such as high-frequency details. Experiments on
the HumanML3D dataset show that our model reduces Frechet inception distance
(FID) from 0.041 to 0.015 and improves Top-1 R-Precision from 0.508 to 0.510.
Qualitative analysis of pairwise direction similarity between pose codes
further confirms the model's controllability for motion editing.

</details>


### [88] [Locality-aware Concept Bottleneck Model](https://arxiv.org/abs/2508.14562)
*Sujin Jeon,Hyundo Lee,Eungseo Kim,Sanghack Lee,Byoung-Tak Zhang,Inwoo Hwang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Locality-aware Concept Bottleneck Model (LCBM) 的框架，通过结合基础模型和原型学习，显著改善了无标签概念瓶颈模型中概念的定位准确性，同时保持了可比的分类性能。


<details>
  <summary>Details</summary>
Motivation: 当前无标签概念瓶颈模型（CBMs）利用基础模型来获取概念，但常在预测概念存在时未能准确地将概念定位到相关区域，而是关注到视觉上不相关的区域。

Method: LCBM为每个概念分配一个原型，该原型旨在代表该概念的原型图像特征。这些原型通过鼓励其编码相似的局部区域进行学习，并利用基础模型来确保每个原型与其关联概念的相关性。随后，这些原型被用于促进学习过程，以识别每个概念应被预测的正确局部区域，从而确保准确的空间定位。

Result: 实验结果表明，LCBM能有效识别图像中存在的概念，并展现出改进的概念定位能力，同时保持了与现有方法相当的分类性能。

Conclusion: LCBM通过引入原型学习和有效利用基础模型的信息，成功解决了无标签概念瓶颈模型中概念定位不准确的问题，提高了模型的可解释性，同时不牺牲预测性能。

Abstract: Concept bottleneck models (CBMs) are inherently interpretable models that
make predictions based on human-understandable visual cues, referred to as
concepts. As obtaining dense concept annotations with human labeling is
demanding and costly, recent approaches utilize foundation models to determine
the concepts existing in the images. However, such label-free CBMs often fail
to localize concepts in relevant regions, attending to visually unrelated
regions when predicting concept presence. To this end, we propose a framework,
coined Locality-aware Concept Bottleneck Model (LCBM), which utilizes rich
information from foundation models and adopts prototype learning to ensure
accurate spatial localization of the concepts. Specifically, we assign one
prototype to each concept, promoted to represent a prototypical image feature
of that concept. These prototypes are learned by encouraging them to encode
similar local regions, leveraging foundation models to assure the relevance of
each prototype to its associated concept. Then we use the prototypes to
facilitate the learning process of identifying the proper local region from
which each concept should be predicted. Experimental results demonstrate that
LCBM effectively identifies present concepts in the images and exhibits
improved localization while maintaining comparable classification performance.

</details>


### [89] [GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels](https://arxiv.org/abs/2508.14563)
*Xingyuan Yang,Min Wei*

Main category: cs.CV

TL;DR: GOGS是一个两阶段框架，利用2D高斯曲面元解决光泽物体逆渲染中现有方法的局限性，在几何、材料分离和真实感光照重打方面实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 从RGB图像逆渲染光泽物体存在固有限制和歧义。现有方法（如NeRF）计算成本高昂；3D高斯泼溅在镜面反射下表现不佳，产生多视角不一致、表面噪声和结构伪影；简化渲染方程导致材料属性模糊和不真实的光照重打结果。

Method: 提出GOGS，一个基于2D高斯曲面元的两阶段框架。第一阶段：通过基于物理的渲染（split-sum近似）并结合基础模型的几何先验，实现鲁棒的表面重建。第二阶段：利用蒙特卡洛重要性采样进行材料分解，通过可微分2D高斯光线追踪建模间接光照，并通过基于球面mipmap的方向编码（捕捉各向异性高光）细化高频镜面细节。

Result: 在几何重建、材料分离和新颖光照下的真实感重打光方面，GOGS展现出最先进的性能，并优于现有逆渲染方法。

Conclusion: GOGS框架通过其创新方法有效克服了光泽物体逆渲染的固有挑战，显著提升了几何重建、材料分离和光照重打的质量，达到了最先进的水平。

Abstract: Inverse rendering of glossy objects from RGB imagery remains fundamentally
limited by inherent ambiguity. Although NeRF-based methods achieve
high-fidelity reconstruction via dense-ray sampling, their computational cost
is prohibitive. Recent 3D Gaussian Splatting achieves high reconstruction
efficiency but exhibits limitations under specular reflections. Multi-view
inconsistencies introduce high-frequency surface noise and structural
artifacts, while simplified rendering equations obscure material properties,
leading to implausible relighting results. To address these issues, we propose
GOGS, a novel two-stage framework based on 2D Gaussian surfels. First, we
establish robust surface reconstruction through physics-based rendering with
split-sum approximation, enhanced by geometric priors from foundation models.
Second, we perform material decomposition by leveraging Monte Carlo importance
sampling of the full rendering equation, modeling indirect illumination via
differentiable 2D Gaussian ray tracing and refining high-frequency specular
details through spherical mipmap-based directional encoding that captures
anisotropic highlights. Extensive experiments demonstrate state-of-the-art
performance in geometry reconstruction, material separation, and photorealistic
relighting under novel illuminations, outperforming existing inverse rendering
approaches.

</details>


### [90] [Safety-Critical Learning for Long-Tail Events: The TUM Traffic Accident Dataset](https://arxiv.org/abs/2508.14567)
*Walter Zimmer,Ross Greer,Xingcheng Zhou,Rui Song,Marc Pavel,Daniel Lehmberg,Ahmed Ghita,Akshay Gopalkrishnan,Mohan Trivedi,Alois Knoll*

Main category: cs.CV

TL;DR: 本文发布了TUMTraf-A真实高速公路事故数据集（包含2D/3D标注）以及Accid3nD事故检测模型，该模型结合了规则与学习方法，并经验证具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管已投入大量工作提升交通网络安全，交通事故仍频繁发生且不可避免，需要对其进行深入理解和检测。

Method: 本文构建了TUMTraf-A数据集，包含10个高速公路事故序列，共48,144帧数据，利用路边摄像头和激光雷达采集并标注了2D/3D边界框。同时，提出了Accid3nD事故检测模型，该模型结合了基于规则和基于学习的方法。

Result: 在TUMTraf-A数据集上的实验和消融研究表明，所提出的Accid3nD方法具有良好的鲁棒性。

Conclusion: 该研究通过提供大规模真实世界事故数据集和有效的事故检测模型，为交通事故分析与检测领域提供了重要资源和解决方案，有助于提升相关研究水平。

Abstract: Even though a significant amount of work has been done to increase the safety
of transportation networks, accidents still occur regularly. They must be
understood as an unavoidable and sporadic outcome of traffic networks. We
present the TUM Traffic Accident (TUMTraf-A) dataset, a collection of
real-world highway accidents. It contains ten sequences of vehicle crashes at
high-speed driving with 294,924 labeled 2D and 93,012 labeled 3D boxes and
track IDs within 48,144 labeled frames recorded from four roadside cameras and
LiDARs at 10 Hz. The dataset contains ten object classes and is provided in the
OpenLABEL format. We propose Accid3nD, an accident detection model that
combines a rule-based approach with a learning-based one. Experiments and
ablation studies on our dataset show the robustness of our proposed method. The
dataset, model, and code are available on our project website:
https://tum-traffic-dataset.github.io/tumtraf-a.

</details>


### [91] [Controllable Latent Space Augmentation for Digital Pathology](https://arxiv.org/abs/2508.14588)
*Sofiène Boutaj,Marin Scalbert,Pierre Marza,Florent Couzinie-Devy,Maria Vakalopoulou,Stergios Christodoulidis*

Main category: cs.CV

TL;DR: HistAug是一种快速高效的生成模型，用于数字病理学WSI分析中在潜在空间进行可控数据增强，能有效提高MIL模型性能，尤其在数据量不足的情况下。


<details>
  <summary>Details</summary>
Motivation: 数字病理学中的全玻片图像（WSI）分析面临高分辨率和监督信号稀疏的挑战。多示例学习（MIL）虽适用于幻灯片级任务，但需要大量多样化数据。现有图像增强技术难以有效实施：传统补丁级增强成本过高，而现有特征级增强又缺乏对变换语义的控制。

Method: 本文提出了HistAug，一个快速高效的生成模型，用于在数字病理学潜在空间中进行可控数据增强。HistAug通过以明确的补丁级变换（如色相、腐蚀）为条件，生成逼真的增强嵌入，同时保留原始语义信息，从而实现大量补丁的单次高效处理。

Result: HistAug能高效处理大量补丁，并持续改善MIL模型的性能。在多个幻灯片级任务和不同器官上的实验表明，HistAug优于现有方法，尤其在低数据量环境下表现突出。消融研究证实了学习变换优于基于噪声的扰动，并强调了统一WSI级增强的重要性。

Conclusion: HistAug通过创新的潜在空间可控数据增强方法，有效解决了数字病理WSI分析中数据稀缺和现有增强技术局限性的问题，显著提升了MIL模型的性能和效率，特别是在低数据量环境下表现突出。

Abstract: Whole slide image (WSI) analysis in digital pathology presents unique
challenges due to the gigapixel resolution of WSIs and the scarcity of dense
supervision signals. While Multiple Instance Learning (MIL) is a natural fit
for slide-level tasks, training robust models requires large and diverse
datasets. Even though image augmentation techniques could be utilized to
increase data variability and reduce overfitting, implementing them effectively
is not a trivial task. Traditional patch-level augmentation is prohibitively
expensive due to the large number of patches extracted from each WSI, and
existing feature-level augmentation methods lack control over transformation
semantics. We introduce HistAug, a fast and efficient generative model for
controllable augmentations in the latent space for digital pathology. By
conditioning on explicit patch-level transformations (e.g., hue, erosion),
HistAug generates realistic augmented embeddings while preserving initial
semantic information. Our method allows the processing of a large number of
patches in a single forward pass efficiently, while at the same time
consistently improving MIL model performance. Experiments across multiple
slide-level tasks and diverse organs show that HistAug outperforms existing
methods, particularly in low-data regimes. Ablation studies confirm the
benefits of learned transformations over noise-based perturbations and
highlight the importance of uniform WSI-wise augmentation. Code is available at
https://github.com/MICS-Lab/HistAug.

</details>


### [92] [Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling](https://arxiv.org/abs/2508.14597)
*Nitish Kumar Mahala,Muzammil Khan,Pushpendra Kumar*

Main category: cs.CV

TL;DR: 该研究提出了一种基于单目图像的两阶段不确定性感知位移窗口Transformer模型，用于鲁棒可靠的烟雾检测，并构建了一个基于光流的烟雾分割数据集。


<details>
  <summary>Details</summary>
Motivation: 火灾对生命和基础设施构成严重威胁，需要高精度早期预警系统来检测烟雾。然而，烟雾复杂的时空动态（受光照、气流和环境噪声影响）降低了传统探测器的可靠性。为避免多传感器阵列的复杂性，研究旨在通过单目图像信息融合解决这些挑战。

Method: 核心方法是“两阶段不确定性感知位移窗口Transformer”。首先，通过“四色定理启发式双相水平集分数阶变分模型”进行光流估计以捕捉运动不连续性，并构建新的烟雾分割数据集。然后，将彩色编码的光流图与外观特征通过高斯混合模型融合，生成烟雾区域的二值分割掩码。这些融合表示被输入到Transformer中，该Transformer增强了多尺度不确定性估计头。模型训练分为两阶段：第一阶段优化检测精度，第二阶段通过联合建模偶然不确定性和认知不确定性来估计预测的可信度。

Result: 通过多项评估指标进行的广泛实验，并与现有最先进方法进行对比分析，结果表明该模型具有卓越的泛化能力和鲁棒性。

Conclusion: 该研究为监控、工业安全和自主监测应用中的早期火灾检测提供了一个可靠的解决方案。

Abstract: Fire outbreaks pose critical threats to human life and infrastructure,
necessitating high-fidelity early-warning systems that detect combustion
precursors such as smoke. However, smoke plumes exhibit complex spatiotemporal
dynamics influenced by illumination variability, flow kinematics, and
environmental noise, undermining the reliability of traditional detectors. To
address these challenges without the logistical complexity of multi-sensor
arrays, we propose an information-fusion framework by integrating smoke feature
representations extracted from monocular imagery. Specifically, a Two-Phase
Uncertainty-Aware Shifted Windows Transformer for robust and reliable smoke
detection, leveraging a novel smoke segmentation dataset, constructed via
optical flow-based motion encoding, is proposed. The optical flow estimation is
performed with a four-color-theorem-inspired dual-phase level-set
fractional-order variational model, which preserves motion discontinuities. The
resulting color-encoded optical flow maps are fused with appearance cues via a
Gaussian Mixture Model to generate binary segmentation masks of the smoke
regions. These fused representations are fed into the novel Shifted-Windows
Transformer, which is augmented with a multi-scale uncertainty estimation head
and trained under a two-phase learning regimen. First learning phase optimizes
smoke detection accuracy, while during the second phase, the model learns to
estimate plausibility confidence in its predictions by jointly modeling
aleatoric and epistemic uncertainties. Extensive experiments using multiple
evaluation metrics and comparative analysis with state-of-the-art approaches
demonstrate superior generalization and robustness, offering a reliable
solution for early fire detection in surveillance, industrial safety, and
autonomous monitoring applications.

</details>


### [93] [Incremental Object Detection with Prompt-based Methods](https://arxiv.org/abs/2508.14599)
*Matthias Neuwirth-Trapp,Maarten Bieshaar,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: 本文分析了视觉提示方法在增量目标检测(IOD)中的应用。研究发现，纯提示方法性能不佳，但结合少量数据回放的提示方法表现最优，为IOD中的增量学习提供了见解。


<details>
  <summary>Details</summary>
Motivation: 视觉提示方法在图像分类的增量学习中已引起广泛关注并显示出高效性，但其在增量目标检测（IOD）领域的通用性和应用情况尚不明确。

Method: 本研究在复杂的领域增量学习环境下，分析了三种不同的视觉提示方法，并提供了广泛的参考基线进行比较。此外，还进行了关于提示长度和初始化的额外实验。

Result: 经验性结果表明，所测试的纯视觉提示方法在增量目标检测设置下表现不佳。然而，一种结合了视觉提示和少量先前数据回放的实用方法，取得了最佳的检测结果。

Conclusion: 本研究的发现，结合提示长度和初始化等额外实验，为推进增量目标检测中基于提示的增量学习提供了宝贵的见解。

Abstract: Visual prompt-based methods have seen growing interest in incremental
learning (IL) for image classification. These approaches learn additional
embedding vectors while keeping the model frozen, making them efficient to
train. However, no prior work has applied such methods to incremental object
detection (IOD), leaving their generalizability unclear. In this paper, we
analyze three different prompt-based methods under a complex domain-incremental
learning setting. We additionally provide a wide range of reference baselines
for comparison. Empirically, we show that the prompt-based approaches we tested
underperform in this setting. However, a strong yet practical method, combining
visual prompts with replaying a small portion of previous data, achieves the
best results. Together with additional experiments on prompt length and
initialization, our findings offer valuable insights for advancing prompt-based
IL in IOD.

</details>


### [94] [Virtual Community: An Open World for Humans, Robots, and Society](https://arxiv.org/abs/2508.14893)
*Qinhong Zhou,Hongxin Zhang,Xiangye Lin,Zheyuan Zhang,Yutian Chen,Wenjun Liu,Zunzhe Zhang,Sunli Chen,Lixing Fang,Qiushi Lyu,Xinyu Sun,Jincheng Yang,Zeyuan Wang,Bao Chi Dang,Zhehuan Chen,Daksha Ladia,Jiageng Liu,Chuang Gan*

Main category: cs.CV

TL;DR: 本文提出了“Virtual Community”平台，这是一个基于真实3D场景的开放世界模拟平台，旨在研究大规模人机共存和具身社交智能。平台支持多智能体物理模拟和社区生成，并基于此提出了两个挑战（社区规划和社区机器人挑战），评估结果显示出高层任务规划和低层协作控制的复杂性。


<details>
  <summary>Details</summary>
Motivation: 随着AI和机器人技术的发展，人类与机器人将共享社区，这带来机遇与挑战。研究这种共存模式，特别是大规模具身社交智能，包括机器人如何智能合作与竞争、人类如何建立社会关系以及智能机器人与人类如何在开放世界中共存。

Method: 开发并介绍了“Virtual Community”平台。该平台是一个开放世界仿真环境，基于通用物理引擎和真实世界3D场景构建。它包含一个开源多智能体物理模拟器，支持机器人、人类及其社会互动。同时，其具备大规模、真实世界对齐的社区生成管线。基于该平台，提出了“社区规划挑战”和“社区机器人挑战”两个新任务，用于评估多智能体在开放世界中的推理、规划和协作能力，并使用多种基线进行了评估。

Result: 对所提出的挑战进行的基线评估显示，在开放世界中的高层任务规划和低层协作控制方面存在显著困难和挑战，表明当前智能体在复杂人机共存场景下的能力仍有待提升。

Conclusion: “Virtual Community”平台及其提出的挑战有望促进对开放世界环境中人机共存的进一步深入研究，为探索具身社交智能和人机交互提供一个有力的研究工具和基准。

Abstract: The rapid progress in AI and Robotics may lead to a profound societal
transformation, as humans and robots begin to coexist within shared
communities, introducing both opportunities and challenges. To explore this
future, we present Virtual Community-an open-world platform for humans, robots,
and society-built on a universal physics engine and grounded in real-world 3D
scenes. With Virtual Community, we aim to study embodied social intelligence at
scale: 1) How robots can intelligently cooperate or compete; 2) How humans
develop social relations and build community; 3) More importantly, how
intelligent robots and humans can co-exist in an open world. To support these,
Virtual Community features: 1) An open-source multi-agent physics simulator
that supports robots, humans, and their interactions within a society; 2) A
large-scale, real-world aligned community generation pipeline, including vast
outdoor space, diverse indoor scenes, and a community of grounded agents with
rich characters and appearances. Leveraging Virtual Community, we propose two
novel challenges. The Community Planning Challenge evaluates multi-agent
reasoning and planning ability in open-world settings, such as cooperating to
help agents with daily activities and efficiently connecting other agents. The
Community Robot Challenge requires multiple heterogeneous robots to collaborate
in solving complex open-world tasks. We evaluate various baselines on these
tasks and demonstrate the challenges in both high-level open-world task
planning and low-level cooperation controls. We hope that Virtual Community
will unlock further study of human-robot coexistence within open-world
environments.

</details>


### [95] [UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling](https://arxiv.org/abs/2508.14604)
*Peiming Li,Ziyi Wang,Yulin Yuan,Hong Liu,Xiangming Meng,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: 本文提出统一时空状态空间模型（UST-SSM），通过引入空间-时间选择扫描、时空结构聚合和时间交互采样，有效解决了点云视频中无序数据对选择性状态空间模型（SSM）单向建模的挑战，显著提升了人体动作识别的性能。


<details>
  <summary>Details</summary>
Motivation: 点云视频擅长捕捉动态3D运动，且不受光照和视角变化影响，适用于识别细微连续的人体动作。尽管选择性状态空间模型（SSM）在序列建模中表现出色，但点云视频的时空无序性阻碍了SSM直接展开为一维序列进行单向建模。

Method: 本文提出了统一时空状态空间模型（UST-SSM）。具体而言，UST-SSM包含三个关键组件：1. 空间-时间选择扫描（STSS）：通过提示引导聚类将无序点重组为语义感知的序列，以有效利用空间和时间上相距较远但相似的点。2. 时空结构聚合（STSA）：聚合时空特征以弥补缺失的4D几何和运动细节。3. 时间交互采样（TIS）：通过利用非锚点帧和扩大感受野来增强采样序列中的细粒度时间依赖性，从而改善时间交互。

Result: 在MSR-Action3D、NTU RGB+D和Synthia 4D数据集上的实验结果验证了所提出方法的有效性。

Conclusion: UST-SSM及其创新的组件（STSS、STSA、TIS）成功克服了点云视频中时空无序性对SSM建模的挑战，为点云视频动作识别提供了一个高效且鲁棒的解决方案。

Abstract: Point cloud videos capture dynamic 3D motion while reducing the effects of
lighting and viewpoint variations, making them highly effective for recognizing
subtle and continuous human actions. Although Selective State Space Models
(SSMs) have shown good performance in sequence modeling with linear complexity,
the spatio-temporal disorder of point cloud videos hinders their unidirectional
modeling when directly unfolding the point cloud video into a 1D sequence
through temporally sequential scanning. To address this challenge, we propose
the Unified Spatio-Temporal State Space Model (UST-SSM), which extends the
latest advancements in SSMs to point cloud videos. Specifically, we introduce
Spatial-Temporal Selection Scanning (STSS), which reorganizes unordered points
into semantic-aware sequences through prompt-guided clustering, thereby
enabling the effective utilization of points that are spatially and temporally
distant yet similar within the sequence. For missing 4D geometric and motion
details, Spatio-Temporal Structure Aggregation (STSA) aggregates
spatio-temporal features and compensates. To improve temporal interaction
within the sampled sequence, Temporal Interaction Sampling (TIS) enhances
fine-grained temporal dependencies through non-anchor frame utilization and
expanded receptive fields. Experimental results on the MSR-Action3D, NTU RGB+D,
and Synthia 4D datasets validate the effectiveness of our method. Our code is
available at https://github.com/wangzy01/UST-SSM.

</details>


### [96] [SMTrack: End-to-End Trained Spiking Neural Networks for Multi-Object Tracking in RGB Videos](https://arxiv.org/abs/2508.14607)
*Pengzhi Zhong,Xinzhe Wang,Dan Zeng,Qihua Zhou,Feixiang He,Shuiwang Li*

Main category: cs.CV

TL;DR: 提出首个直接训练的基于脉冲神经网络（SNN）的多目标跟踪（MOT）框架SMTrack，在RGB视频上实现了与传统人工神经网络（ANN）相当的性能。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）在低功耗计算方面潜力巨大，但其在复杂时间视觉任务（如多目标跟踪）和标准RGB视频上的直接应用尚未充分探索，主要限于图像分类、目标检测或基于事件的跟踪。

Method: 本文提出了SMTrack，首个针对标准RGB视频的端到端多目标跟踪的直接训练深度SNN框架。该方法引入了自适应尺度感知归一化Wasserstein距离损失（Asa-NWDLoss），通过动态调整归一化因子（根据训练批次内的平均目标大小），以提高在不同目标尺度和密度下的检测和定位性能，尤其增强对小目标的敏感性。在关联阶段，集成了TrackTrack身份模块以维护鲁棒且一致的目标轨迹。

Result: 在BEE24、MOT17、MOT20和DanceTrack等数据集上的广泛评估表明，SMTrack的性能与领先的基于ANN的多目标跟踪方法相当。

Conclusion: SMTrack推动了在复杂场景下鲁棒且准确的基于SNN的多目标跟踪技术的发展。

Abstract: Brain-inspired Spiking Neural Networks (SNNs) exhibit significant potential
for low-power computation, yet their application in visual tasks remains
largely confined to image classification, object detection, and event-based
tracking. In contrast, real-world vision systems still widely use conventional
RGB video streams, where the potential of directly-trained SNNs for complex
temporal tasks such as multi-object tracking (MOT) remains underexplored. To
address this challenge, we propose SMTrack-the first directly trained deep SNN
framework for end-to-end multi-object tracking on standard RGB videos. SMTrack
introduces an adaptive and scale-aware Normalized Wasserstein Distance loss
(Asa-NWDLoss) to improve detection and localization performance under varying
object scales and densities. Specifically, the method computes the average
object size within each training batch and dynamically adjusts the
normalization factor, thereby enhancing sensitivity to small objects. For the
association stage, we incorporate the TrackTrack identity module to maintain
robust and consistent object trajectories. Extensive evaluations on BEE24,
MOT17, MOT20, and DanceTrack show that SMTrack achieves performance on par with
leading ANN-based MOT methods, advancing robust and accurate SNN-based tracking
in complex scenarios.

</details>


### [97] [AnchorSync: Global Consistency Optimization for Long Video Editing](https://arxiv.org/abs/2508.14609)
*Zichi Liu,Yinggui Wang,Tao Wei,Chao Ma*

Main category: cs.CV

TL;DR: 提出AnchorSync扩散模型框架，通过解耦编辑和插帧，实现高质量长视频编辑，克服现有方法的一致性问题。


<details>
  <summary>Details</summary>
Motivation: 编辑长视频（数千帧）面临全局一致性和时间连贯性难以保持的挑战。现有方法常出现结构漂移或时间伪影，尤其在分钟级别的长序列中表现更明显。

Method: 引入AnchorSync，一个新型扩散模型框架。该方法将长视频编辑任务解耦为稀疏锚点帧编辑和平滑中间帧插值。它通过渐进式去噪过程强制执行结构一致性，并通过多模态引导保留时间动态。

Result: 大量实验证明，AnchorSync能够产生连贯、高保真度的编辑，并在视觉质量和时间稳定性方面超越了现有方法。

Conclusion: AnchorSync通过其独特的解耦和引导机制，成功解决了长视频编辑中结构一致性和时间连贯性的难题，实现了高质量且稳定的视频编辑效果。

Abstract: Editing long videos remains a challenging task due to the need for
maintaining both global consistency and temporal coherence across thousands of
frames. Existing methods often suffer from structural drift or temporal
artifacts, particularly in minute-long sequences. We introduce AnchorSync, a
novel diffusion-based framework that enables high-quality, long-term video
editing by decoupling the task into sparse anchor frame editing and smooth
intermediate frame interpolation. Our approach enforces structural consistency
through a progressive denoising process and preserves temporal dynamics via
multimodal guidance. Extensive experiments show that AnchorSync produces
coherent, high-fidelity edits, surpassing prior methods in visual quality and
temporal stability.

</details>


### [98] [Towards PerSense++: Advancing Training-Free Personalized Instance Segmentation in Dense Images](https://arxiv.org/abs/2508.14660)
*Muhammad Ibraheem Siddiqui,Muhammad Umer Sheikh,Hassan Abid,Kevin Henry,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: PerSense是一个针对密集图像中个性化实例分割的无训练、模型无关的一次性框架。PerSense++是其增强版本，通过引入多种新模块提升了在复杂密集场景下的鲁棒性，并在多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 在密集视觉场景中，由于遮挡、背景杂乱和尺度变化，实例分割面临显著挑战。

Method: 该研究引入了PerSense框架，一个端到端、无训练且模型无关的一次性个性化实例分割框架。其核心方法包括：1) 实例检测模块(IDM)利用密度图生成实例级候选点提示；2) 点提示选择模块(PPSM)通过自适应阈值和空间门控过滤误报；3) 反馈机制自动选择有效示例以提升密度图质量。在此基础上，提出了增强版PerSense++，新增了三个组件：a) 多样性感知示例选择策略，利用特征和尺度多样性优化密度图生成；b) 混合IDM，结合轮廓和峰值生成提示以改善复杂密度模式中的实例分离；c) 无关掩码拒绝模块(IMRM)，通过异常值分析剔除空间不一致的掩码。此外，研究还推出了专用的密集图像个性化分割基准PerSense-D。

Result: 广泛的实验表明，PerSense++在多个基准测试中，尤其是在密集场景下，性能优于现有方法。

Conclusion: PerSense++通过其创新的无训练、模型无关框架及针对密集场景的增强模块，有效解决了个性化实例分割的挑战，并设立了新的性能标准，同时引入了专用数据集以推动该领域发展。

Abstract: Segmentation in dense visual scenes poses significant challenges due to
occlusions, background clutter, and scale variations. To address this, we
introduce PerSense, an end-to-end, training-free, and model-agnostic one-shot
framework for Personalized instance Segmentation in dense images. PerSense
employs a novel Instance Detection Module (IDM) that leverages density maps
(DMs) to generate instance-level candidate point prompts, followed by a Point
Prompt Selection Module (PPSM) that filters false positives via adaptive
thresholding and spatial gating. A feedback mechanism further enhances
segmentation by automatically selecting effective exemplars to improve DM
quality. We additionally present PerSense++, an enhanced variant that
incorporates three additional components to improve robustness in cluttered
scenes: (i) a diversity-aware exemplar selection strategy that leverages
feature and scale diversity for better DM generation; (ii) a hybrid IDM
combining contour and peak-based prompt generation for improved instance
separation within complex density patterns; and (iii) an Irrelevant Mask
Rejection Module (IMRM) that discards spatially inconsistent masks using
outlier analysis. Finally, to support this underexplored task, we introduce
PerSense-D, a dedicated benchmark for personalized segmentation in dense
images. Extensive experiments across multiple benchmarks demonstrate that
PerSense++ outperforms existing methods in dense settings.

</details>


### [99] [GeMS: Efficient Gaussian Splatting for Extreme Motion Blur](https://arxiv.org/abs/2508.14682)
*Gopi Raju Matta,Trisha Reddypalli,Vemunuri Divya Madhuri,Kaushik Mitra*

Main category: cs.CV

TL;DR: 本文提出GeMS，一个直接从严重运动模糊图像重建3D场景的3D高斯泼溅（3DGS）框架。GeMS通过集成深度学习SfM、基于MCMC的高斯初始化和联合优化来解决传统方法的局限性。其扩展版本GeMS-E进一步整合事件相机数据进行图像恢复，两者均实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的去模糊方法（如ExBluRF, Deblur-GS）以及基于3DGS的方法在处理严重运动模糊图像时，通常假设能够获取清晰图像进行相机姿态估计和点云生成，或依赖在严重模糊下失效的COLMAP初始化（因特征对应不可靠），这些假设在实际中是不切实际的。

Method: 本文提出GeMS框架，直接从极度模糊图像重建场景。GeMS集成了：(1) VGGSfM，一个基于深度学习的SfM管道，直接从模糊输入估计姿态并生成点云；(2) 3DGS-MCMC，通过将高斯视为概率分布样本实现鲁棒的场景初始化，避免启发式密度化和剪枝；(3) 相机轨迹和高斯参数的联合优化，以实现稳定重建。为进一步提升，还提出了GeMS-E，通过(4) 基于事件的双积分(EDI)去模糊技术恢复更清晰图像，并将其输入GeMS进行渐进式细化。

Result: GeMS和GeMS-E在合成和真实世界数据集上均实现了最先进的性能。

Conclusion: GeMS是首个直接从严重模糊输入解决3D高斯泼溅中极端运动模糊问题的框架。它通过创新的组件集成，有效克服了现有方法的局限性，展示了强大的场景重建能力。

Abstract: We introduce GeMS, a framework for 3D Gaussian Splatting (3DGS) designed to
handle severely motion-blurred images. State-of-the-art deblurring methods for
extreme blur, such as ExBluRF, as well as Gaussian Splatting-based approaches
like Deblur-GS, typically assume access to sharp images for camera pose
estimation and point cloud generation, an unrealistic assumption. Methods
relying on COLMAP initialization, such as BAD-Gaussians, also fail due to
unreliable feature correspondences under severe blur. To address these
challenges, we propose GeMS, a 3DGS framework that reconstructs scenes directly
from extremely blurred images. GeMS integrates: (1) VGGSfM, a deep
learning-based Structure-from-Motion pipeline that estimates poses and
generates point clouds directly from blurred inputs; (2) 3DGS-MCMC, which
enables robust scene initialization by treating Gaussians as samples from a
probability distribution, eliminating heuristic densification and pruning; and
(3) joint optimization of camera trajectories and Gaussian parameters for
stable reconstruction. While this pipeline produces strong results,
inaccuracies may remain when all inputs are severely blurred. To mitigate this,
we propose GeMS-E, which integrates a progressive refinement step using events:
(4) Event-based Double Integral (EDI) deblurring restores sharper images that
are then fed into GeMS, improving pose estimation, point cloud generation, and
overall reconstruction. Both GeMS and GeMS-E achieve state-of-the-art
performance on synthetic and real-world datasets. To our knowledge, this is the
first framework to address extreme motion blur within 3DGS directly from
severely blurred inputs.

</details>


### [100] [Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models](https://arxiv.org/abs/2508.14707)
*Jiabo Huang,Chen Chen,Lingjuan Lyu*

Main category: cs.CV

TL;DR: 本文提出一种模型驱动的新方法，通过整合多个预训练教师模型的知识来构建通用视觉基础模型（VFM），无需大量标注数据，并在多项视觉任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有VFM主要依赖海量标注数据训练，对多数机构而言存在数据和GPU瓶颈。虽有大量开源领域特定预训练模型，但其在构建通用VFM方面的潜力尚未被充分利用。

Method: 提出一种模型驱动的VFM训练方法，结合知识迁移与保存。该方法将多个预训练教师模型统一到共享潜在空间，以解决分布差异导致的“不平衡迁移”。同时，采用知识保存策略，以通用教师模型为基础，通过适配器模块整合其他特定教师模型的知识。

Result: 所构建的VFM能提供通用视觉特征并支持多种下游任务。在图像分类、目标检测、语义分割和实例分割四项基本视觉任务上，本文提出的VFM表现优于现有的数据中心模型。

Conclusion: 通过统一和聚合现有预训练模型，成功构建了强大的通用VFM，有效解决了传统方法对大数据和GPU的依赖，并在多项视觉任务上取得了超越现有模型的性能。

Abstract: Vision foundation models (VFMs) are predominantly developed using
data-centric methods. These methods require training on vast amounts of data
usually with high-quality labels, which poses a bottleneck for most
institutions that lack both large-scale data and high-end GPUs. On the other
hand, many open-source vision models have been pretrained on domain-specific
data, enabling them to distill and represent core knowledge in a form that is
transferable across diverse applications. Even though these models are highly
valuable assets, they remain largely under-explored in empowering the
development of a general-purpose VFM. In this paper, we presents a new
model-driven approach for training VFMs through joint knowledge transfer and
preservation. Our method unifies multiple pre-trained teacher models in a
shared latent space to mitigate the ``imbalanced transfer'' issue caused by
their distributional gaps. Besides, we introduce a knowledge preservation
strategy to take a general-purpose teacher as a knowledge base for integrating
knowledge from the remaining purpose-specific teachers using an adapter module.
By unifying and aggregating existing models, we build a powerful VFM to inherit
teachers' expertise without needing to train on a large amount of labeled data.
Our model not only provides generalizable visual features, but also inherently
supports multiple downstream tasks. Extensive experiments demonstrate that our
VFM outperforms existing data-centric models across four fundamental vision
tasks, including image classification, object detection, semantic and instance
segmentation.

</details>


### [101] [GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting](https://arxiv.org/abs/2508.14717)
*Jiaxin Wei,Stefan Leutenegger,Simon Schaefer*

Main category: cs.CV

TL;DR: GSFix3D是一个新颖框架，通过将扩散模型的先验知识蒸馏到3D表示中，解决了3D高斯溅射在极端视角渲染和扩散模型在3D重建中的局限性，实现了对未见相机姿态的稳健新视角修复和缺失区域修复，达到最先进性能且仅需少量微调。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射在极端新视角或部分观测区域生成高质量渲染图时仍面临挑战。扩散模型虽有强大生成能力，但对文本提示的依赖和对特定场景信息感知的不足，阻碍了其在精确3D重建任务中的应用。

Method: 提出GSFix3D框架，通过将扩散模型的先验知识蒸馏到3D表示中，同时保持与观测场景细节的一致性，以改善非约束区域的视觉保真度。其核心是GSFixer，一个通过定制微调协议获得的潜在扩散模型，能利用网格和3D高斯适应预训练生成模型，实现对未见相机姿态的稳健新视角修复。此外，还提出了随机掩码增强策略，使GSFixer能够合理地修复缺失区域。

Result: 在具有挑战性的基准测试中，GSFix3D和GSFixer实现了最先进的性能，且仅需对捕获数据进行最少的场景特定微调。真实世界测试进一步证实了其对潜在姿态错误的弹性。

Conclusion: GSFix3D框架有效解决了当前3D高斯溅射和扩散模型在高质量3D重建中的关键限制，通过创新的方法提升了极端视角和部分观测区域的视觉保真度及新视角修复能力，展现出卓越的性能、高效性和对姿态错误的鲁棒性。

Abstract: Recent developments in 3D Gaussian Splatting have significantly enhanced
novel view synthesis, yet generating high-quality renderings from extreme novel
viewpoints or partially observed regions remains challenging. Meanwhile,
diffusion models exhibit strong generative capabilities, but their reliance on
text prompts and lack of awareness of specific scene information hinder
accurate 3D reconstruction tasks. To address these limitations, we introduce
GSFix3D, a novel framework that improves the visual fidelity in
under-constrained regions by distilling prior knowledge from diffusion models
into 3D representations, while preserving consistency with observed scene
details. At its core is GSFixer, a latent diffusion model obtained via our
customized fine-tuning protocol that can leverage both mesh and 3D Gaussians to
adapt pretrained generative models to a variety of environments and artifact
types from different reconstruction methods, enabling robust novel view repair
for unseen camera poses. Moreover, we propose a random mask augmentation
strategy that empowers GSFixer to plausibly inpaint missing regions.
Experiments on challenging benchmarks demonstrate that our GSFix3D and GSFixer
achieve state-of-the-art performance, requiring only minimal scene-specific
fine-tuning on captured data. Real-world test further confirms its resilience
to potential pose errors. Our code and data will be made publicly available.
Project page: https://gsfix3d.github.io.

</details>


### [102] [Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving](https://arxiv.org/abs/2508.14729)
*Leila Cheshmi,Mennatullah Siam*

Main category: cs.CV

TL;DR: 本研究提出一种高效的多尺度视频Transformer，通过运动线索实现类别无关分割，无需光流，解决了自动驾驶中未知物体检测和计算效率问题，并在多项基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需处理未知物体和意外场景以确保安全；现有视频分割方法依赖已知类别，无法识别新颖类别；基于大型语言模型的视觉接地计算成本高昂，尤其在像素级输出时。

Method: 开发了一种高效的端到端训练多尺度视频Transformer，用于类别无关分割，不使用光流。该方法采用多阶段多尺度查询-记忆解码、尺度特异性随机丢弃token，并利用共享可学习记忆模块，以记忆为中心的设计保持多尺度高分辨率时空特征。

Result: 在DAVIS'16、KITTI和Cityscapes数据集上进行评估。结果显示，该方法持续优于多尺度基线，同时在GPU内存和运行时间方面表现出高效性。

Conclusion: 该方法为安全关键型机器人领域中的实时、鲁棒密集预测提供了一个有前景的方向。

Abstract: Ensuring safety in autonomous driving is a complex challenge requiring
handling unknown objects and unforeseen driving scenarios. We develop
multiscale video transformers capable of detecting unknown objects using only
motion cues. Video semantic and panoptic segmentation often relies on known
classes seen during training, overlooking novel categories. Recent visual
grounding with large language models is computationally expensive, especially
for pixel-level output. We propose an efficient video transformer trained
end-to-end for class-agnostic segmentation without optical flow. Our method
uses multi-stage multiscale query-memory decoding and a scale-specific random
drop-token to ensure efficiency and accuracy, maintaining detailed
spatiotemporal features with a shared, learnable memory module. Unlike
conventional decoders that compress features, our memory-centric design
preserves high-resolution information at multiple scales. We evaluate on
DAVIS'16, KITTI, and Cityscapes. Our method consistently outperforms multiscale
baselines while being efficient in GPU memory and run-time, demonstrating a
promising direction for real-time, robust dense prediction in safety-critical
robotics.

</details>


### [103] [Improved Mapping Between Illuminations and Sensors for RAW Images](https://arxiv.org/abs/2508.14730)
*Abhijith Punnappurath,Luxi Zhao,Hoang Le,Abdelrahman Abdelhamed,SaiKiran Kumar Tedla,Michael S. Brown*

Main category: cs.CV

TL;DR: 提出首个RAW图像照明与传感器映射数据集及轻量级神经网络，以解决深度学习中RAW图像数据采集和泛化难题。


<details>
  <summary>Details</summary>
Motivation: RAW图像因传感器和照明条件特异性，导致深度学习数据集采集成本高昂且难以泛化。急需针对特定传感器的照明增强和跨传感器RAW图像映射方法来降低数据采集负担。

Method: 构建了一个前所未有的RAW图像数据集，包含使用定制可调光谱灯箱在多种照明下通过不同相机拍摄的场景，共计390种照明、4台相机和18个场景。在此数据集基础上，提出了一种轻量级神经网络方法用于照明和传感器映射。

Result: 所提出的轻量级神经网络在照明与传感器映射任务上性能优于现有方法，并成功应用于训练神经ISP的下游任务。

Conclusion: 通过引入专用数据集和高效的神经网络方法，本研究有效缓解了RAW图像在深度学习应用中数据采集的挑战，并提升了模型泛化能力，为神经ISP等领域提供了支持。

Abstract: RAW images are unprocessed camera sensor output with sensor-specific RGB
values based on the sensor's color filter spectral sensitivities. RAW images
also incur strong color casts due to the sensor's response to the spectral
properties of scene illumination. The sensor- and illumination-specific nature
of RAW images makes it challenging to capture RAW datasets for deep learning
methods, as scenes need to be captured for each sensor and under a wide range
of illumination. Methods for illumination augmentation for a given sensor and
the ability to map RAW images between sensors are important for reducing the
burden of data capture. To explore this problem, we introduce the
first-of-its-kind dataset comprising carefully captured scenes under a wide
range of illumination. Specifically, we use a customized lightbox with tunable
illumination spectra to capture several scenes with different cameras. Our
illumination and sensor mapping dataset has 390 illuminations, four cameras,
and 18 scenes. Using this dataset, we introduce a lightweight neural network
approach for illumination and sensor mapping that outperforms competing
methods. We demonstrate the utility of our approach on the downstream task of
training a neural ISP. Link to project page:
https://github.com/SamsungLabs/illum-sensor-mapping.

</details>


### [104] [Fusing Monocular RGB Images with AIS Data to Create a 6D Pose Estimation Dataset for Marine Vessels](https://arxiv.org/abs/2508.14767)
*Fabian Holst,Emre Gülsoylu,Simone Frintrop*

Main category: cs.CV

TL;DR: 本文提出了一种通过融合单目RGB图像和AIS数据来创建船舶6D姿态估计数据集的新技术，自动生成3D边界框。该方法克服了纯AIS数据的局限性，并引入了一个名为BONK-pose的新型公开数据集。


<details>
  <summary>Details</summary>
Motivation: 为船舶创建6D姿态估计数据集面临挑战，传统上纯依赖AIS数据存在设备可靠性、数据篡改和传输延迟等问题。此外，手动标注6D姿态数据耗时费力，需要一种无需手动标注的自动化生成方法。

Method: 该研究通过融合单目RGB图像和AIS（自动识别系统）数据来生成船舶的6D姿态。具体步骤包括：使用YOLOX-X目标检测网络从RGB图像中检测船舶；将检测到的船舶与AIS信息结合，生成表示船舶6D姿态（空间和旋转维度）的3D边界框。文中还比较了不同的目标检测模型以及两种转换方法（单应性变换和透视-n-点（PnP）方法）用于对齐AIS数据与图像坐标。

Result: 研究结果表明，透视-n-点（PnP）方法相较于之前使用的基于单应性变换的方法，实现了显著更低的投影误差。YOLOX-X模型在相关船舶类别上，当IoU阈值为0.5时，平均精度（mAP）达到0.80。该方法无需手动标注即可创建6D姿态估计数据集。此外，本文还发布了一个名为BONK-pose的公开数据集，包含3753张带有3D边界框姿态估计标注的图像，以及一个包含1000张带有2D边界框船舶检测标注的图像集。

Conclusion: 所提出的单目RGB图像与AIS数据融合技术，结合YOLOX-X检测和PnP变换，能够有效、自动化地创建船舶6D姿态估计数据集，解决了纯AIS数据和手动标注的局限性。新创建的BONK-pose数据集为6D姿态估计网络的训练和评估提供了宝贵的资源。

Abstract: The paper presents a novel technique for creating a 6D pose estimation
dataset for marine vessels by fusing monocular RGB images with Automatic
Identification System (AIS) data. The proposed technique addresses the
limitations of relying purely on AIS for location information, caused by issues
like equipment reliability, data manipulation, and transmission delays. By
combining vessel detections from monocular RGB images, obtained using an object
detection network (YOLOX-X), with AIS messages, the technique generates 3D
bounding boxes that represent the vessels' 6D poses, i.e. spatial and
rotational dimensions. The paper evaluates different object detection models to
locate vessels in image space. We also compare two transformation methods
(homography and Perspective-n-Point) for aligning AIS data with image
coordinates. The results of our work demonstrate that the Perspective-n-Point
(PnP) method achieves a significantly lower projection error compared to
homography-based approaches used before, and the YOLOX-X model achieves a mean
Average Precision (mAP) of 0.80 at an Intersection over Union (IoU) threshold
of 0.5 for relevant vessel classes. We show indication that our approach allows
the creation of a 6D pose estimation dataset without needing manual annotation.
Additionally, we introduce the Boats on Nordelbe Kehrwieder (BONK-pose), a
publicly available dataset comprising 3753 images with 3D bounding box
annotations for pose estimation, created by our data fusion approach. This
dataset can be used for training and evaluating 6D pose estimation networks. In
addition we introduce a set of 1000 images with 2D bounding box annotations for
ship detection from the same scene.

</details>


### [105] [6-DoF Object Tracking with Event-based Optical Flow and Frames](https://arxiv.org/abs/2508.14776)
*Zhichao Li,Arren Glover,Chiara Bartolozzi,Lorenzo Natale*

Main category: cs.CV

TL;DR: 本文提出一种结合事件相机和RGB相机的方法，用于高速运动物体在6自由度下的实时姿态跟踪。


<details>
  <summary>Details</summary>
Motivation: 实时6自由度(6-DoF)物体姿态跟踪是机器人领域的基础问题。传统相机在物体高速运动时，由于帧率限制和运动模糊，难以有效跟踪，这构成了研究的挑战和动机。

Method: 本研究提出一种利用事件相机和RGB相机各自优势的混合方法。具体而言，开发了一个基于事件光流的算法，用于高频测量物体运动并实现6-DoF速度跟踪。然后，将该高速率的6-DoF物体速度信息与RGB相机提供的低频全局物体姿态估计器相结合，从而在物体高速运动时也能实现姿态跟踪。

Result: 所提出的算法在合成数据和真实世界数据上都进行了测试和验证，结果表明其有效性，尤其是在高速运动场景中表现出色。

Conclusion: 该方法成功克服了传统相机在高速运动姿态跟踪中的局限性，通过融合事件相机的高时间分辨率和RGB相机的丰富视觉信息，实现了对高速运动物体的鲁棒6-DoF姿态跟踪。

Abstract: Tracking the position and orientation of objects in space (i.e., in 6-DoF) in
real time is a fundamental problem in robotics for environment interaction. It
becomes more challenging when objects move at high-speed due to frame rate
limitations in conventional cameras and motion blur. Event cameras are
characterized by high temporal resolution, low latency and high dynamic range,
that can potentially overcome the impacts of motion blur. Traditional RGB
cameras provide rich visual information that is more suitable for the
challenging task of single-shot object pose estimation. In this work, we
propose using event-based optical flow combined with an RGB based global object
pose estimator for 6-DoF pose tracking of objects at high-speed, exploiting the
core advantages of both types of vision sensors. Specifically, we propose an
event-based optical flow algorithm for object motion measurement to implement
an object 6-DoF velocity tracker. By integrating the tracked object 6-DoF
velocity with low frequency estimated pose from the global pose estimator, the
method can track pose when objects move at high-speed. The proposed algorithm
is tested and validated on both synthetic and real world data, demonstrating
its effectiveness, especially in high-speed motion scenarios.

</details>


### [106] [Adversarial Hospital-Invariant Feature Learning for WSI Patch Classification](https://arxiv.org/abs/2508.14779)
*Mengliang Zhang,Jacob M. Luber*

Main category: cs.CV

TL;DR: 研究了病理基础模型(PFMs)中的医院域偏差问题，并提出了一种轻量级对抗框架，通过移除潜在的医院特定特征，有效减轻域偏差，同时保持或提升疾病分类性能。


<details>
  <summary>Details</summary>
Motivation: 病理基础模型(PFMs)在全切片图像(WSI)诊断中潜力巨大，但不同医院的图像差异（扫描硬件、预处理风格）导致PFMs可能无意中学到医院特有特征，形成域偏差，从而影响其临床部署的风险。目前缺乏对PFMs中源于医院特征的域偏差的系统研究。

Method: 1. 构建量化PFMs域偏差的流程。2. 评估和比较多个模型的性能。3. 提出一种轻量级对抗框架，通过引入可训练的适配器(adapter)和连接梯度反转层(GRL)的域分类器，从冻结表示中移除潜在的医院特定特征，从而学习任务判别性但域不变的表示。

Result: 在多中心组织病理学数据集上的实验表明，该方法显著降低了域可预测性，同时保持甚至改善了疾病分类性能，特别是在域外（未见过医院）场景。医院检测和特征空间可视化等进一步分析证实了该方法在减轻医院偏差方面的有效性。

Conclusion: 本工作首次系统研究了病理基础模型中的医院域偏差，并提出的轻量级对抗框架能有效缓解医院偏差，增强了PFMs在不同临床环境下的泛化能力和部署安全性。

Abstract: Pathology foundation models (PFMs) have demonstrated remarkable potential in
whole-slide image (WSI) diagnosis. However, pathology images from different
hospitals often vary due to differences in scanning hardware and preprocessing
styles, which may lead PFMs to inadvertently learn hospital-specific features,
posing risks for clinical deployment. In this work, we present the first
systematic study of domain bias in PFMs arising from hospital source
characteristics. Specifically, we (1) construct a pipeline for quantifying
domain bias in PFMs, (2) evaluate and compare the performance of multiple
models, and (3) propose a lightweight adversarial framework that removes latent
hospital-specific features from frozen representations without modifying the
encoder itself. By introducing a trainable adapter and a domain classifier
connected through a gradient reversal layer (GRL), our method learns
task-discriminative yet domain-invariant representations. Experiments on
multi-center histopathology datasets demonstrate that our approach
substantially reduces domain predictability while maintaining or even improving
disease classification performance, particularly in out-of-domain (unseen
hospital) scenarios. Further analyses, including hospital detection and feature
space visualization, confirm the effectiveness of our method in mitigating
hospital bias. We will provide our code based on acceptance.

</details>


### [107] [MF-LPR$^2$: Multi-Frame License Plate Image Restoration and Recognition using Optical Flow](https://arxiv.org/abs/2508.14797)
*Kihyun Na,Junseok Oh,Youngkwan Cho,Bumjin Kim,Sungmin Cho,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: 本文提出MF-LPR$^2$框架，通过对行车记录仪图像中低质量车牌序列进行多帧对齐和聚合，实现高精度车牌恢复和识别，避免了预训练模型引入的伪影，并创建了新的RLPR数据集，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 车牌识别（LPR）对交通执法、犯罪调查和监控至关重要。然而，行车记录仪图像中的车牌区域常因低分辨率、运动模糊和眩光而质量低下，导致识别困难。现有依赖预训练先验的生成模型无法可靠地恢复此类图像，常引入严重伪影和失真。

Method: 我们提出了一种新颖的多帧车牌恢复和识别框架MF-LPR$^2$。该方法通过对齐和聚合相邻帧来解决低质量图像的模糊性，而非依赖预训练知识。为实现准确的帧对齐，我们采用先进的光流估计算法，并结合精心设计的算法，利用车牌图像序列固有的时空一致性来检测和纠正错误的光流估计。此外，我们构建了一个新的Realistic LPR（RLPR）数据集，包含200对低质量车牌图像序列和高质量伪地面真值图像，以评估MF-LPR$^2$。

Result: MF-LPR$^2$在PSNR、SSIM和LPIPS指标上显著优于八个最新的恢复模型。在识别方面，MF-LPR$^2$达到了86.44%的准确率，超过了十一个基线模型中表现最佳的单帧LPR（14.04%）和多帧LPR（82.55%）。消融研究结果证实，我们的过滤和细化算法对这些改进有显著贡献。

Conclusion: MF-LPR$^2$通过利用多帧信息，成功解决了低质量车牌图像的恢复和识别挑战，避免了传统生成模型引入的伪影。该方法在图像质量和识别准确性上均取得了显著提升，并保留了输入图像的证据内容。

Abstract: License plate recognition (LPR) is important for traffic law enforcement,
crime investigation, and surveillance. However, license plate areas in dash cam
images often suffer from low resolution, motion blur, and glare, which make
accurate recognition challenging. Existing generative models that rely on
pretrained priors cannot reliably restore such poor-quality images, frequently
introducing severe artifacts and distortions. To address this issue, we propose
a novel multi-frame license plate restoration and recognition framework,
MF-LPR$^2$, which addresses ambiguities in poor-quality images by aligning and
aggregating neighboring frames instead of relying on pretrained knowledge. To
achieve accurate frame alignment, we employ a state-of-the-art optical flow
estimator in conjunction with carefully designed algorithms that detect and
correct erroneous optical flow estimations by leveraging the spatio-temporal
consistency inherent in license plate image sequences. Our approach enhances
both image quality and recognition accuracy while preserving the evidential
content of the input images. In addition, we constructed a novel Realistic LPR
(RLPR) dataset to evaluate MF-LPR$^2$. The RLPR dataset contains 200 pairs of
low-quality license plate image sequences and high-quality pseudo ground-truth
images, reflecting the complexities of real-world scenarios. In experiments,
MF-LPR$^2$ outperformed eight recent restoration models in terms of PSNR, SSIM,
and LPIPS by significant margins. In recognition, MF-LPR$^2$ achieved an
accuracy of 86.44%, outperforming both the best single-frame LPR (14.04%) and
the multi-frame LPR (82.55%) among the eleven baseline models. The results of
ablation studies confirm that our filtering and refinement algorithms
significantly contribute to these improvements.

</details>


### [108] [DINOv3 with Test-Time Training for Medical Image Registration](https://arxiv.org/abs/2508.14809)
*Shansong Wang,Mojtaba Safari,Mingzhe Hu,Qiang Li,Chih-Wei Chang,Richard LJ Qiu,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本文提出一种无需训练的医学图像配准方法，利用冻结的DINOv3编码器和测试时优化，在两个基准测试中均展现出优异的配准精度和形变规律性，为临床应用提供了无需大量数据的实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像配准方法，特别是基于学习的方法，通常需要大量训练数据，这限制了其临床应用。

Method: 提出一种无需训练的配准流程，该流程依赖于一个冻结的DINOv3编码器，并在特征空间中进行测试时的形变场优化。

Result: 在腹部MR-CT数据集上，该方法获得了最佳平均Dice分数(0.790)，最低95%Hausdorff距离(4.9)和最低对数雅可比标准差(0.08)。在ACDC心脏MRI数据集上，将平均DSC提高到0.769，SDLogJ降至0.11，HD95降至4.8，相较于初始对齐有显著提升。

Conclusion: 在测试时于紧凑的基础特征空间中操作，为临床图像配准提供了一种无需额外训练的实用且通用的解决方案。

Abstract: Prior medical image registration approaches, particularly learning-based
methods, often require large amounts of training data, which constrains
clinical adoption. To overcome this limitation, we propose a training-free
pipeline that relies on a frozen DINOv3 encoder and test-time optimization of
the deformation field in feature space. Across two representative benchmarks,
the method is accurate and yields regular deformations. On Abdomen MR-CT, it
attained the best mean Dice score (DSC) of 0.790 together with the lowest 95th
percentile Hausdorff Distance (HD95) of 4.9+-5.0 and the lowest standard
deviation of Log-Jacobian (SDLogJ) of 0.08+-0.02. On ACDC cardiac MRI, it
improves mean DSC to 0.769 and reduces SDLogJ to 0.11 and HD95 to 4.8, a marked
gain over the initial alignment. The results indicate that operating in a
compact foundation feature space at test time offers a practical and general
solution for clinical registration without additional training.

</details>


### [109] [Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization](https://arxiv.org/abs/2508.14811)
*Canyu Zhao,Xiaoman Li,Tianjian Feng,Zhiyue Zhao,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: Tinker是一个通用的3D编辑框架，无需逐场景微调，即可利用少量图像实现高保真、多视图一致的3D内容创作。


<details>
  <summary>Details</summary>
Motivation: 现有3D编辑技术需要大量的逐场景优化或多个输入视图才能保证一致性。研究动机是开发一种能从少量图像（甚至单张）生成鲁棒、多视图一致编辑的方法，并通过利用预训练扩散模型的3D感知能力来解决这一挑战，同时填补大规模多视图编辑数据集的空白。

Method: 该方法通过重新利用预训练扩散模型来解锁其潜在的3D感知能力。为此，项目构建了首个大规模多视图编辑数据集。在此基础上，开发了一个包含两个核心组件的框架：1) 参照式多视图编辑器，用于实现跨视角的精确、一致性编辑；2) 任意视图到视频合成器，利用视频扩散模型的时空先验进行高质量的场景补全和新视图生成。

Result: Tinker能够从少量（一或两张）图像生成鲁棒、多视图一致的编辑。它显著降低了通用3D内容创作的门槛，并在编辑、新视图合成和渲染增强任务上实现了最先进的性能。

Conclusion: Tinker代表了向真正可扩展、零样本3D编辑迈出的关键一步。

Abstract: We introduce Tinker, a versatile framework for high-fidelity 3D editing that
operates in both one-shot and few-shot regimes without any per-scene
finetuning. Unlike prior techniques that demand extensive per-scene
optimization to ensure multi-view consistency or to produce dozens of
consistent edited input views, Tinker delivers robust, multi-view consistent
edits from as few as one or two images. This capability stems from repurposing
pretrained diffusion models, which unlocks their latent 3D awareness. To drive
research in this space, we curate the first large-scale multi-view editing
dataset and data pipeline, spanning diverse scenes and styles. Building on this
dataset, we develop our framework capable of generating multi-view consistent
edited views without per-scene training, which consists of two novel
components: (1) Referring multi-view editor: Enables precise, reference-driven
edits that remain coherent across all viewpoints. (2) Any-view-to-video
synthesizer: Leverages spatial-temporal priors from video diffusion to perform
high-quality scene completion and novel-view generation even from sparse
inputs. Through extensive experiments, Tinker significantly reduces the barrier
to generalizable 3D content creation, achieving state-of-the-art performance on
editing, novel-view synthesis, and rendering enhancement tasks. We believe that
Tinker represents a key step towards truly scalable, zero-shot 3D editing.
Project webpage: https://aim-uofa.github.io/Tinker

</details>


### [110] [Repeating Words for Video-Language Retrieval with Coarse-to-Fine Objectives](https://arxiv.org/abs/2508.14812)
*Haoyu Zhao,Jiaxi Gu,Shicong Wang,Xing Zhang,Hang Xu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本研究提出一种新颖的视频-语言检索框架，通过学习细粒度特征和创新的推理管道，在不增加训练成本的情况下，显著提升了检索性能并缓解了对大规模预训练的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有视频-语言检索方法过度依赖大规模预训练，导致计算成本高昂，且视频和文本中的细粒度信息未被充分利用，限制了检索准确性。

Method: 1. **特征学习框架**: 采用从粗到细的目标（包括对比学习和匹配学习）来理解视频-文本对的语义信息，以实现更好的对齐。2. **细粒度数据获取**: 通过粒度感知表示模块，基于视频帧与文本词之间的相似性分析，获取用于训练的细粒度数据。3. **推理管道**: 基于关键词重复现象，提出一个新颖且有效的推理管道，结合投票机制和新的匹配熵度量，无需额外预训练即可提升检索性能。

Result: 在四个基准测试中，所提方法均优于现有方法。其中，推理管道在MSR-VTT数据集上使Recall@1提升2.1%，在DiDeMo数据集上提升1.6%，取得了显著的性能提升。

Conclusion: 该研究通过引入细粒度特征学习和创新的推理管道，在解决视频-语言检索高计算成本和细粒度信息利用不足问题的同时，显著提升了检索性能，为该领域提供了高效的解决方案。

Abstract: The explosive growth of video streaming presents challenges in achieving high
accuracy and low training costs for video-language retrieval. However, existing
methods rely on large-scale pre-training to improve video retrieval
performance, resulting in significant computational demands. Additionally, the
fine-grained information in videos and texts remains underexplored. To
alleviate these problems, we propose a novel framework to learn fine-grained
features for better alignment and introduce an inference pipeline to improve
performance without additional training. Specifically, we employ coarse-to-fine
objectives to understand the semantic information of video-text pairs,
including contrastive and matching learning. The fine-grained data used for
training is obtained through the Granularity-Aware Representation module, which
is designed based on similarity analysis between video frames and words in
captions. Furthermore, we observe that the repetition of keywords in the
original captions, referred to as "Repetition", can enhance retrieval
performance and improve alignment between video and text. Based on this
insight, we propose a novel and effective inference pipeline that incorporates
a voting mechanism and a new Matching Entropy metric to achieve better
retrieval performance without requiring additional pre-training. Experimental
results on four benchmarks demonstrate that the proposed method outperforms
previous approaches. Additionally, our inference pipeline achieves significant
performance improvements, with a 2.1% increase in Recall@1 on the MSR-VTT
dataset and a 1.6% increase on the DiDeMo dataset.

</details>


### [111] [TransLight: Image-Guided Customized Lighting Control with Generative Decoupling](https://arxiv.org/abs/2508.14814)
*Zongming Li,Lianghui Zhu,Haocheng Shen,Longjin Ran,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: TransLight是一个新颖框架，通过生成式解耦将复杂光照效果从参考图高保真、高自由度地迁移到目标图，实现精细化光照控制。


<details>
  <summary>Details</summary>
Motivation: 现有光照编辑方法难以同时提供定制化光照控制和保持内容完整性，尤其在将复杂光照效果从参考图迁移到指定目标图时效率低下。

Method: 提出TransLight框架。关键是“生成式解耦”，利用两个微调的扩散模型准确分离图像内容和光照效果，并创建了百万级的图像-内容-光照三元组数据集。随后，以IC-Light为生成模型，使用该数据集训练模型，并额外注入参考光照图像作为条件信号。

Result: TransLight是首个成功实现跨不同图像光照效果迁移的方法，相比现有技术提供了更定制化的光照控制。生成式解耦策略赋予TransLight高度灵活的光照控制能力。

Conclusion: TransLight为光照和谐化和编辑领域的研究开辟了新方向。

Abstract: Most existing illumination-editing approaches fail to simultaneously provide
customized control of light effects and preserve content integrity. This makes
them less effective for practical lighting stylization requirements, especially
in the challenging task of transferring complex light effects from a reference
image to a user-specified target image. To address this problem, we propose
TransLight, a novel framework that enables high-fidelity and high-freedom
transfer of light effects. Extracting the light effect from the reference image
is the most critical and challenging step in our method. The difficulty lies in
the complex geometric structure features embedded in light effects that are
highly coupled with content in real-world scenarios. To achieve this, we first
present Generative Decoupling, where two fine-tuned diffusion models are used
to accurately separate image content and light effects, generating a newly
curated, million-scale dataset of image-content-light triplets. Then, we employ
IC-Light as the generative model and train our model with our triplets,
injecting the reference lighting image as an additional conditioning signal.
The resulting TransLight model enables customized and natural transfer of
diverse light effects. Notably, by thoroughly disentangling light effects from
reference images, our generative decoupling strategy endows TransLight with
highly flexible illumination control. Experimental results establish TransLight
as the first method to successfully transfer light effects across disparate
images, delivering more customized illumination control than existing
techniques and charting new directions for research in illumination
harmonization and editing.

</details>


### [112] [EventSSEG: Event-driven Self-Supervised Segmentation with Probabilistic Attention](https://arxiv.org/abs/2508.14856)
*Lakshmi Annamalai,Chetan Singh Thakur*

Main category: cs.CV

TL;DR: 为解决自动驾驶道路分割中传统帧相机的延迟和计算限制，该研究引入EventSSEG方法，利用事件相机进行纯事件计算和概率注意力机制，并通过自监督学习克服标注数据稀缺问题，在DSEC-Semantic和DDD17数据集上以少量标注数据达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的道路分割至关重要，但传统帧相机难以实现低延迟和低计算量。事件相机提供了一种有前景的替代方案，可利用其低功耗感知能力。然而，纯事件计算在迁移预训练权重和获取大量标注数据方面面临挑战。

Method: 引入EventSSEG方法，用于道路分割。该方法采用纯事件计算（event only computing）和概率注意力机制（probabilistic attention mechanism）。为克服标注数据稀缺问题，EventSSEG还采用了基于事件的自监督学习（event-based self supervised learning）。

Result: 在DSEC-Semantic和DDD17数据集上的实验表明，EventSSEG在仅使用极少量标注事件的情况下，实现了最先进（state of the art）的性能。

Conclusion: EventSSEG方法最大化了事件相机的能力，并成功解决了事件数据标注稀缺的问题。

Abstract: Road segmentation is pivotal for autonomous vehicles, yet achieving low
latency and low compute solutions using frame based cameras remains a
challenge. Event cameras offer a promising alternative. To leverage their low
power sensing, we introduce EventSSEG, a method for road segmentation that uses
event only computing and a probabilistic attention mechanism. Event only
computing poses a challenge in transferring pretrained weights from the
conventional camera domain, requiring abundant labeled data, which is scarce.
To overcome this, EventSSEG employs event-based self supervised learning,
eliminating the need for extensive labeled data. Experiments on DSEC-Semantic
and DDD17 show that EventSSEG achieves state of the art performance with
minimal labeled events. This approach maximizes event cameras capabilities and
addresses the lack of labeled events.

</details>


### [113] [Lifespan Pancreas Morphology for Control vs Type 2 Diabetes using AI on Largescale Clinical Imaging](https://arxiv.org/abs/2508.14878)
*Lucas W. Remedios,Chloe Cho,Trent M. Schwartz,Dingjie Su,Gaurav Rudravaram,Chenyu Gao,Aravind R. Krishnan,Adam M. Saunders,Michael E. Kim,Shunxing Bao,Thomas A. Lasko,Alvin C. Powers,Bennett A. Landman,John Virostko*

Main category: cs.CV

TL;DR: 本研究利用CT/MRI和AI方法分析了不同年龄段胰腺的形态学变化，建立了正常衰老趋势，并发现2型糖尿病患者的胰腺大小和形状与非糖尿病患者显著不同，且通常更小。


<details>
  <summary>Details</summary>
Motivation: 了解胰腺形态变化对于检测2型糖尿病及其他胰腺疾病的异常至关重要。研究旨在识别可靠的AI胰腺测量影像模态，建立规范的胰腺形态学衰老趋势，并检测2型糖尿病患者的潜在偏差。

Method: 分析了2533名患者的腹部CT或MRI图像，将扫描重采样至3mm各向同性分辨率，使用自动化方法分割胰腺，并提取13个形态学特征。评估了CT和MRI测量结果的一致性；表征了按年龄和性别分层的规范形态学模式；使用GAMLSS回归模型分析了1350名匹配患者（包括2型糖尿病患者和非糖尿病对照）的胰腺形态趋势，以识别与2型糖尿病相关的偏差。

Result: 调整混杂因素后，2型糖尿病患者与非糖尿病对照组之间，13个形态学特征中有10个的衰老趋势存在显著差异（多重比较校正后p < 0.05）。此外，AI方法显示MRI与CT在胰腺测量上存在差异。

Conclusion: 研究提供了生命周期趋势，表明2型糖尿病患者的胰腺大小和形状发生改变，且胰腺通常更小。本研究还为临床环境中大量非糖尿病对照患者的生命周期胰腺形态提供了参考。

Abstract: Purpose: Understanding how the pancreas changes is critical for detecting
deviations in type 2 diabetes and other pancreatic disease. We measure pancreas
size and shape using morphological measurements from ages 0 to 90. Our goals
are to 1) identify reliable clinical imaging modalities for AI-based pancreas
measurement, 2) establish normative morphological aging trends, and 3) detect
potential deviations in type 2 diabetes.
  Approach: We analyzed a clinically acquired dataset of 2533 patients imaged
with abdominal CT or MRI. We resampled the scans to 3mm isotropic resolution,
segmented the pancreas using automated methods, and extracted 13 morphological
pancreas features across the lifespan. First, we assessed CT and MRI
measurements to determine which modalities provide consistent lifespan trends.
Second, we characterized distributions of normative morphological patterns
stratified by age group and sex. Third, we used GAMLSS regression to model
pancreas morphology trends in 1350 patients matched for age, sex, and type 2
diabetes status to identify any deviations from normative aging associated with
type 2 diabetes.
  Results: When adjusting for confounders, the aging trends for 10 of 13
morphological features were significantly different between patients with type
2 diabetes and non-diabetic controls (p < 0.05 after multiple comparisons
corrections). Additionally, MRI appeared to yield different pancreas
measurements than CT using our AI-based method.
  Conclusions: We provide lifespan trends demonstrating that the size and shape
of the pancreas is altered in type 2 diabetes using 675 control patients and
675 diabetes patients. Moreover, our findings reinforce that the pancreas is
smaller in type 2 diabetes. Additionally, we contribute a reference of lifespan
pancreas morphology from a large cohort of non-diabetic control patients in a
clinical setting.

</details>


### [114] [MS-CLR: Multi-Skeleton Contrastive Learning for Human Action Recognition](https://arxiv.org/abs/2508.14889)
*Mert Kiray,Alvaro Ritter,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: 提出多骨架对比学习（MS-CLR），通过对齐多骨架姿态表示来提高骨架行为识别的泛化性，并在NTU RGB+D数据集上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的骨架行为识别对比学习方法依赖单一骨架约定，限制了其在不同关节结构和解剖覆盖数据集上的泛化能力。

Method: 提出多骨架对比学习（MS-CLR）框架，通过对齐同一序列中提取的多种骨架约定下的姿态表示，促使模型学习结构不变性并捕获多样的解剖线索。同时，调整ST-GCN架构以统一表示不同关节布局和尺度的骨架。

Result: 在NTU RGB+D 60和120数据集上的实验表明，MS-CLR持续优于现有的单骨架对比学习基线。多骨架集成进一步提升了性能，在这两个数据集上取得了新的最先进（SOTA）结果。

Conclusion: MS-CLR有效解决了单一骨架约定在骨架行为识别泛化性上的局限，通过学习结构不变性和多样解剖特征，显著提升了模型性能，并达到了SOTA水平，证明了多骨架学习策略的有效性。

Abstract: Contrastive learning has gained significant attention in skeleton-based
action recognition for its ability to learn robust representations from
unlabeled data. However, existing methods rely on a single skeleton convention,
which limits their ability to generalize across datasets with diverse joint
structures and anatomical coverage. We propose Multi-Skeleton Contrastive
Learning (MS-CLR), a general self-supervised framework that aligns pose
representations across multiple skeleton conventions extracted from the same
sequence. This encourages the model to learn structural invariances and capture
diverse anatomical cues, resulting in more expressive and generalizable
features. To support this, we adapt the ST-GCN architecture to handle skeletons
with varying joint layouts and scales through a unified representation scheme.
Experiments on the NTU RGB+D 60 and 120 datasets demonstrate that MS-CLR
consistently improves performance over strong single-skeleton contrastive
learning baselines. A multi-skeleton ensemble further boosts performance,
setting new state-of-the-art results on both datasets.

</details>


### [115] [GaussianArt: Unified Modeling of Geometry and Motion for Articulated Objects](https://arxiv.org/abs/2508.14891)
*Licheng Shen,Saining Zhang,Honghan Li,Peilin Yang,Zihao Huang,Zongzheng Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: 提出一种使用关节式3D高斯函数统一建模几何和运动的方法，显著提升多关节物体重建的可伸缩性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在重建关节式物体时，通常将几何与运动解耦，导致重建流程复杂、可伸缩性受限，尤其难以处理复杂的多部件关节式物体。

Method: 引入一种统一表示，利用关节式3D高斯函数联合建模物体的几何和运动。为系统评估可伸缩性和泛化能力，提出了包含90个关节式物体的新基准数据集MPArt-90。

Result: 该方法提高了运动分解的鲁棒性，支持多达20个关节部件（远超现有方法的2-3个部件），并在广泛的物体类型上，在零件级几何重建和运动估计方面持续获得卓越精度。

Conclusion: 统一的关节式表示在可伸缩物理建模中具有巨大潜力，并可应用于机器人仿真和人-场景交互建模等下游任务。

Abstract: Reconstructing articulated objects is essential for building digital twins of
interactive environments. However, prior methods typically decouple geometry
and motion by first reconstructing object shape in distinct states and then
estimating articulation through post-hoc alignment. This separation complicates
the reconstruction pipeline and restricts scalability, especially for objects
with complex, multi-part articulation. We introduce a unified representation
that jointly models geometry and motion using articulated 3D Gaussians. This
formulation improves robustness in motion decomposition and supports
articulated objects with up to 20 parts, significantly outperforming prior
approaches that often struggle beyond 2--3 parts due to brittle initialization.
To systematically assess scalability and generalization, we propose MPArt-90, a
new benchmark consisting of 90 articulated objects across 20 categories, each
with diverse part counts and motion configurations. Extensive experiments show
that our method consistently achieves superior accuracy in part-level geometry
reconstruction and motion estimation across a broad range of object types. We
further demonstrate applicability to downstream tasks such as robotic
simulation and human-scene interaction modeling, highlighting the potential of
unified articulated representations in scalable physical modeling.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [116] [Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli](https://arxiv.org/abs/2508.14214)
*Mattson Ogg,Chace Ashcraft,Ritwik Bose,Raphael Norman-Tenazas,Michael Wolmetz*

Main category: cs.AI

TL;DR: 该研究评估了LLM（尤其是GPT-4o）在情感刺激评估上与人类行为的对齐程度，发现GPT-4o在多模态和多种情感维度上与人类表现高度相似，但在唤醒度评估上偏差较大，且LLM的评分同质性更高。


<details>
  <summary>Details</summary>
Motivation: 鉴于情感对人类行为和认知的重要影响，以及LLM日益融入日常生活的趋势，了解LLM如何评估情感刺激对于其有效性和在特定角色中的应用至关重要。

Method: 研究人员收集了多个流行LLM（如GPT-4o）对情感词汇和图像数据集的评分，这些数据集此前已有人类情感评分。通过比较LLM与人类的评分来评估对齐程度。

Result: GPT-4o在处理情感刺激时，与人类参与者在多模态、刺激类型和多数评分量表上表现出高度相似性（多处相关系数r≥0.9）。然而，在唤醒度评估上，LLM与人类评分的对齐度较低，而幸福感评估的对齐度最高。总体而言，LLM在五类别情感框架（幸福、愤怒、悲伤、恐惧、厌恶）下的对齐度优于二维（唤醒和效价）组织。此外，LLM的评分比人类评分更为同质。

Conclusion: 这些发现初步描述了LLM代理如何解释情感刺激，并揭示了人工智能与生物智能在关键行为领域（情感评估）中的异同点。

Abstract: Emotions exert an immense influence over human behavior and cognition in both
commonplace and high-stress tasks. Discussions of whether or how to integrate
large language models (LLMs) into everyday life (e.g., acting as proxies for,
or interacting with, human agents), should be informed by an understanding of
how these tools evaluate emotionally loaded stimuli or situations. A model's
alignment with human behavior in these cases can inform the effectiveness of
LLMs for certain roles or interactions. To help build this understanding, we
elicited ratings from multiple popular LLMs for datasets of words and images
that were previously rated for their emotional content by humans. We found that
when performing the same rating tasks, GPT-4o responded very similarly to human
participants across modalities, stimuli and most rating scales (r = 0.9 or
higher in many cases). However, arousal ratings were less well aligned between
human and LLM raters, while happiness ratings were most highly aligned. Overall
LLMs aligned better within a five-category (happiness, anger, sadness, fear,
disgust) emotion framework than within a two-dimensional (arousal and valence)
organization. Finally, LLM ratings were substantially more homogenous than
human ratings. Together these results begin to describe how LLM agents
interpret emotional stimuli and highlight similarities and differences among
biological and artificial intelligence in key behavioral domains.

</details>


### [117] [Explaining Hitori Puzzles: Neurosymbolic Proof Staging for Sequential Decisions](https://arxiv.org/abs/2508.14294)
*Maria Leonor Pacheco,Fabio Somenzi,Dananjay Srinivas,Ashutosh Trivedi*

Main category: cs.AI

TL;DR: 提出一种结合决策过程与大型语言模型（LLM）的神经符号方法，旨在解释复杂的决策序列，并通过Hitori谜题的应用验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效解释复杂的决策序列。Hitori谜题因其包含局部（适合证明）和连接（适合视觉）两种不同类型的约束，为结合SAT求解器和LLM的灵活解释方法提供了理想的测试场景。

Method: 提出并实现了一种神经符号方法，该方法融合了决策过程（如SAT求解器）和大型语言模型（LLMs）的优势。通过为Hitori谜题的解决方案生成解释来演示此方法，并开发了一个辅助人类解谜的工具。

Result: 实验证据表明所开发的工具在协助人类解决Hitori谜题方面是有效的。

Conclusion: 结合SAT求解器和LLM的神经符号方法能有效解释Hitori谜题等复杂决策序列，证明了这种混合方法的可行性和潜力。

Abstract: We propose a neurosymbolic approach to the explanation of complex sequences
of decisions that combines the strengths of decision procedures and Large
Language Models (LLMs). We demonstrate this approach by producing explanations
for the solutions of Hitori puzzles. The rules of Hitori include local
constraints that are effectively explained by short resolution proofs. However,
they also include a connectivity constraint that is more suitable for visual
explanations. Hence, Hitori provides an excellent testing ground for a flexible
combination of SAT solvers and LLMs. We have implemented a tool that assists
humans in solving Hitori puzzles, and we present experimental evidence of its
effectiveness.

</details>


### [118] [Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning](https://arxiv.org/abs/2508.14410)
*Beinuo Yang,Qishen Zhou,Junyi Li,Xingchen Su,Simon Hu*

Main category: cs.AI

TL;DR: 本文提出LogiOR基准和ORThought框架，通过纠正现有数据错误并利用思维链推理，显著提升LLM在优化建模（OM）中的自动化水平和性能，尤其在复杂问题上表现突出。


<details>
  <summary>Details</summary>
Motivation: 优化建模（OM）虽然对解决复杂决策问题至关重要，但其过程耗时且易错，高度依赖领域专家。尽管大型语言模型（LLMs）展现出潜力，但当前方法存在三大局限：基准标注错误率高（高达42%），评估范围狭窄（仅考虑最优值），以及计算效率低下（过度依赖多智能体系统或模型微调）。

Method: 1. 改进现有数据集：通过系统错误校正和更全面的标注来增强。2. 引入新基准：提出LogiOR，一个来自物流领域、包含更复杂问题和标准化标注的优化建模基准。3. 提出新框架：推出ORThought，一个利用专家级优化建模原则并通过思维链推理自动化OM过程的新颖框架。

Result: ORThought在广泛的实证评估中超越了现有方法（包括多智能体框架），尤其在复杂优化问题上展现出显著优势。

Conclusion: ORThought框架能够有效自动化优化建模过程。本研究对方法进行的系统分析，识别了关键成功因素和失败模式，为未来基于LLM的优化建模研究提供了宝贵见解。

Abstract: Optimization Modeling (OM) is essential for solving complex decision-making
problems. However, the process remains time-consuming and error-prone, heavily
relying on domain experts. While Large Language Models (LLMs) show promise in
addressing these challenges through their natural language understanding and
reasoning capabilities, current approaches face three critical limitations:
high benchmark labeling error rates reaching up to 42\%, narrow evaluation
scope that only considers optimal values, and computational inefficiency due to
heavy reliance on multi-agent systems or model fine-tuning. In this work, we
first enhance existing datasets through systematic error correction and more
comprehensive annotation. Additionally, we introduce LogiOR, a new optimization
modeling benchmark from the logistics domain, containing more complex problems
with standardized annotations. Furthermore, we present ORThought, a novel
framework that leverages expert-level optimization modeling principles through
chain-of-thought reasoning to automate the OM process. Through extensive
empirical evaluation, we demonstrate that ORThought outperforms existing
approaches, including multi-agent frameworks, with particularly significant
advantages on complex optimization problems. Finally, we provide a systematic
analysis of our method, identifying critical success factors and failure modes,
providing valuable insights for future research on LLM-based optimization
modeling.

</details>


### [119] [The Agent Behavior: Model, Governance and Challenges in the AI Digital Age](https://arxiv.org/abs/2508.14415)
*Qiang Zhang,Pei Yan,Yijia Xu,Chuanpo Fu,Yong Fang,Yang Liu*

Main category: cs.AI

TL;DR: 该论文提出“网络行为生命周期”和“人机行为差异（HABD）”模型，以区分网络环境中人类和AI代理的行为，旨在解决信任、责任、安全等挑战，并促进安全可信的人机协作。


<details>
  <summary>Details</summary>
Motivation: 随着AI在网络环境中日益模仿人类行为，人与AI的界限模糊，带来了信任、责任、伦理和安全等方面的严峻挑战。代理行为监管困难可能导致数据污染和责任不清等问题，亟需有效方案解决。

Method: 论文提出了“网络行为生命周期”模型，将网络行为划分为6个阶段并系统分析人与代理的行为差异。在此基础上，引入“Agent for Agent (A4A)”范式和“人机行为差异（HABD）”模型，从决策机制、执行效率、意图行为一致性、行为惯性、非理性模式5个维度审视人机行为的根本区别。通过红队渗透和蓝队防御等真实案例验证了模型的有效性。

Result: 通过“网络行为生命周期”模型系统分析了人与代理在网络行为各阶段的差异；利用“HABD”模型在5个关键维度上深入探讨了人机行为的根本区别；模型有效性通过红队渗透和蓝队防御等真实世界案例得到验证。

Conclusion: 本研究为人机安全可信协作提供了理论基础和技术路线图，并展望了动态认知治理架构、行为差异量化和元治理协议栈等未来研究方向。

Abstract: Advancements in AI have led to agents in networked environments increasingly
mirroring human behavior, thereby blurring the boundary between artificial and
human actors in specific contexts. This shift brings about significant
challenges in trust, responsibility, ethics, security and etc. The difficulty
in supervising of agent behaviors may lead to issues such as data contamination
and unclear accountability. To address these challenges, this paper proposes
the "Network Behavior Lifecycle" model, which divides network behavior into 6
stages and systematically analyzes the behavioral differences between humans
and agents at each stage. Based on these insights, the paper further introduces
the "Agent for Agent (A4A)" paradigm and the "Human-Agent Behavioral Disparity
(HABD)" model, which examine the fundamental distinctions between human and
agent behaviors across 5 dimensions: decision mechanism, execution efficiency,
intention-behavior consistency, behavioral inertia, and irrational patterns.
The effectiveness of the model is verified through real-world cases such as red
team penetration and blue team defense. Finally, the paper discusses future
research directions in dynamic cognitive governance architecture, behavioral
disparity quantification, and meta-governance protocol stacks, aiming to
provide a theoretical foundation and technical roadmap for secure and
trustworthy human-agent collaboration.

</details>


### [120] [Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs](https://arxiv.org/abs/2508.14564)
*Luca Annese,Sabrina Patania,Silvia Serino,Tom Foulsham,Silvia Rossi,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.AI

TL;DR: 本研究探索通过结构化示例提升LLM代理的视角采纳能力，发现单独的结构化示例不足以应对复杂情境，强调需引入信念追踪和成本建模。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型（LLM）的系统在主动感知、协作推理和视角采纳（理解其他代理所见或所知）等任务中面临持续挑战。

Method: 在ReAct框架下，利用Fast Downward规划器生成的转换解图，提出一种结构化解决方案处理流程，生成三种类型（G型、E型、L型）的“思-行”示例，通过提示LLM明确决策推理。

Result: L型示例略微减少了澄清请求和行动步骤，但未能带来持续一致的改进。代理在基本注意力过滤任务中成功，但在需要对遮挡空间进行心理推断或权衡认知行动成本的场景中遇到困难。

Conclusion: 单独的结构化示例不足以实现鲁棒的视角采纳。为使基于LLM的代理实现社会化协作，亟需引入明确的信念追踪、成本建模和更丰富的环境。

Abstract: Recent advances in large language models (LLMs) and reasoning frameworks have
opened new possibilities for improving the perspective -taking capabilities of
autonomous agents. However, tasks that involve active perception, collaborative
reasoning, and perspective taking (understanding what another agent can see or
knows) pose persistent challenges for current LLM-based systems. This study
investigates the potential of structured examples derived from transformed
solution graphs generated by the Fast Downward planner to improve the
performance of LLM-based agents within a ReAct framework. We propose a
structured solution-processing pipeline that generates three distinct
categories of examples: optimal goal paths (G-type), informative node paths
(E-type), and step-by-step optimal decision sequences contrasting alternative
actions (L-type). These solutions are further converted into ``thought-action''
examples by prompting an LLM to explicitly articulate the reasoning behind each
decision. While L-type examples slightly reduce clarification requests and
overall action steps, they do not yield consistent improvements. Agents are
successful in tasks requiring basic attentional filtering but struggle in
scenarios that required mentalising about occluded spaces or weighing the costs
of epistemic actions. These findings suggest that structured examples alone are
insufficient for robust perspective-taking, underscoring the need for explicit
belief tracking, cost modelling, and richer environments to enable socially
grounded collaboration in LLM-based agents.

</details>


### [121] [LeanGeo: Formalizing Competitional Geometry problems in Lean](https://arxiv.org/abs/2508.14644)
*Chendong Song,Zihan Wang,Frederick Pu,Haiming Wang,Xiaohan Lin,Junqi Liu,Jia Li,Zhengying Liu*

Main category: cs.AI

TL;DR: 本文介绍了LeanGeo，一个基于Lean 4的形式化系统，用于统一表示和解决竞赛级几何问题，并附带了LeanGeo-Bench基准测试集，用于评估大型语言模型在几何推理上的表现，揭示了该领域的进步需求。


<details>
  <summary>Details</summary>
Motivation: 现有的几何求解系统缺乏统一的框架，难以与其他数学领域整合；同时，几何证明高度依赖直观图，使其验证过程极具挑战性。

Method: 研究者引入了LeanGeo，一个在Lean 4定理证明器中实现的统一形式化系统，用于形式化和解决竞赛级几何问题。LeanGeo包含一个基于Lean基础逻辑的高级几何定理库，支持严格的证明验证并能与Mathlib无缝集成。此外，还提出了LeanGeo-Bench，一个包含国际数学奥林匹克(IMO)等来源问题的形式化几何基准测试集。

Result: 通过在LeanGeo-Bench上对最先进的大型语言模型进行评估，展示了它们在几何推理方面的能力和局限性。

Conclusion: 研究结果强调了在自动化几何推理领域中，仍需进一步的进展。LeanGeo的定理库和基准测试集已开源。

Abstract: Geometry problems are a crucial testbed for AI reasoning capabilities. Most
existing geometry solving systems cannot express problems within a unified
framework, thus are difficult to integrate with other mathematical fields.
Besides, since most geometric proofs rely on intuitive diagrams, verifying
geometry problems is particularly challenging. To address these gaps, we
introduce LeanGeo, a unified formal system for formalizing and solving
competition-level geometry problems within the Lean 4 theorem prover. LeanGeo
features a comprehensive library of high-level geometric theorems with Lean's
foundational logic, enabling rigorous proof verification and seamless
integration with Mathlib. We also present LeanGeo-Bench, a formal geometry
benchmark in LeanGeo, comprising problems from the International Mathematical
Olympiad (IMO) and other advanced sources. Our evaluation demonstrates the
capabilities and limitations of state-of-the-art Large Language Models on this
benchmark, highlighting the need for further advancements in automated
geometric reasoning. We open source the theorem library and the benchmark of
LeanGeo at https://github.com/project-numina/LeanGeo/tree/master.

</details>


### [122] [Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration](https://arxiv.org/abs/2508.14654)
*Peilin Ji,Xiao Xue,Simeng Wang,Wenhao Yan*

Main category: cs.AI

TL;DR: 本研究提出H-J分层多智能体框架，结合知识引导提示、熵约束生成和反馈驱动优化，以解决城市极端降雨下紧急调度系统的挑战，并在交通流畅性、任务成功率和系统鲁棒性方面优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 城市极端降雨事件对紧急调度系统构成严峻挑战，引发交通拥堵和服务中断。现有决策面临多重目标（交通流、任务完成、风险缓解）动态权衡、静态规则无法应对快速变化环境，以及LLM生成策略的语义不稳定和执行不一致等问题。此外，现有方法未能统一感知、全局优化和多智能体协调。

Method: 引入H-J分层多智能体框架，该框架整合了知识引导提示、熵约束生成和反馈驱动优化。它建立了一个从多源感知到战略执行和持续优化的闭环管道。

Result: 在真实城市拓扑结构和降雨数据下，针对极端降雨、间歇性降雨和日常小雨三种代表性条件进行评估。实验结果显示，H-J在交通流畅性、任务成功率和系统鲁棒性方面均优于基于规则和强化学习的基线方法。

Conclusion: 研究结果突出了不确定性感知、知识约束的LLM方法在增强城市洪涝响应韧性方面的巨大潜力。

Abstract: In recent years, the increasing frequency of extreme urban rainfall events
has posed significant challenges to emergency scheduling systems. Urban
flooding often leads to severe traffic congestion and service disruptions,
threatening public safety and mobility. However, effective decision making
remains hindered by three key challenges: (1) managing trade-offs among
competing goals (e.g., traffic flow, task completion, and risk mitigation)
requires dynamic, context-aware strategies; (2) rapidly evolving environmental
conditions render static rules inadequate; and (3) LLM-generated strategies
frequently suffer from semantic instability and execution inconsistency.
Existing methods fail to align perception, global optimization, and multi-agent
coordination within a unified framework. To tackle these challenges, we
introduce H-J, a hierarchical multi-agent framework that integrates
knowledge-guided prompting, entropy-constrained generation, and feedback-driven
optimization. The framework establishes a closed-loop pipeline spanning from
multi-source perception to strategic execution and continuous refinement. We
evaluate H-J on real-world urban topology and rainfall data under three
representative conditions: extreme rainfall, intermittent bursts, and daily
light rain. Experiments show that H-J outperforms rule-based and
reinforcement-learning baselines in traffic smoothness, task success rate, and
system robustness. These findings highlight the promise of uncertainty-aware,
knowledge-constrained LLM-based approaches for enhancing resilience in urban
flood response.

</details>


### [123] [MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers](https://arxiv.org/abs/2508.14704)
*Ziyang Luo,Zhiqi Shen,Wenzhuo Yang,Zirui Zhao,Prathyusha Jwalapuram,Amrita Saha,Doyen Sahoo,Silvio Savarese,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: 本文提出了MCP-Universe，首个用于评估大型语言模型（LLMs）在真实、复杂Model Context Protocol (MCP)应用中性能的综合基准，发现现有最先进模型表现不佳，并开源了评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准过于简化，未能捕捉现实应用中的长程推理和大型陌生工具空间等挑战，无法有效评估通过Model Context Protocol (MCP)连接外部数据源和工具的LLM。

Method: 引入MCP-Universe基准，涵盖6个核心领域和11个真实MCP服务器（如导航、代码库管理、金融分析等）。采用执行基评估器，包括格式评估器、静态评估器和动态评估器，以确保评估的严谨性。此外，还开源了支持UI的可扩展评估框架。

Result: 实验发现，即使是GPT-5、Grok-4和Claude-4.0-Sonnet等最先进的LLM也表现出显著的性能限制（最高43.72%）。该基准对LLM代理构成了长上下文和未知工具的重大挑战。企业级代理（如Cursor）表现并未优于标准ReAct框架。

Conclusion: MCP-Universe成功揭示了当前LLM在处理真实MCP应用中长上下文和未知工具方面的局限性。开源的评估框架将有助于推动MCP生态系统中LLM能力的研究和创新。

Abstract: The Model Context Protocol has emerged as a transformative standard for
connecting large language models to external data sources and tools, rapidly
gaining adoption across major AI providers and development platforms. However,
existing benchmarks are overly simplistic and fail to capture real application
challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To
address this critical gap, we introduce MCP-Universe, the first comprehensive
benchmark specifically designed to evaluate LLMs in realistic and hard tasks
through interaction with real-world MCP servers. Our benchmark encompasses 6
core domains spanning 11 different MCP servers: Location Navigation, Repository
Management, Financial Analysis, 3D Design, Browser Automation, and Web
Searching. To ensure rigorous evaluation, we implement execution-based
evaluators, including format evaluators for agent format compliance, static
evaluators for time-invariant content matching, and dynamic evaluators that
automatically retrieve real-time ground truth for temporally sensitive tasks.
Through extensive evaluation of leading LLMs, we find that even SOTA models
such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit
significant performance limitations. In addition, our benchmark poses a
significant long-context challenge for LLM agents, as the number of input
tokens increases rapidly with the number of interaction steps. Moreover, it
introduces an unknown-tools challenge, as LLM agents often lack familiarity
with the precise usage of the MCP servers. Notably, enterprise-level agents
like Cursor cannot achieve better performance than standard ReAct frameworks.
Beyond evaluation, we open-source our extensible evaluation framework with UI
support, enabling researchers and practitioners to seamlessly integrate new
agents and MCP servers while fostering innovation in the rapidly evolving MCP
ecosystem.

</details>


### [124] [Data-Driven Probabilistic Evaluation of Logic Properties with PAC-Confidence on Mealy Machines](https://arxiv.org/abs/2508.14710)
*Swantje Plambeck,Ali Salamati,Eyke Huellermeier,Goerschwin Fey*

Main category: cs.AI

TL;DR: 提出一种数据驱动的PAC主动学习方法，用于确定离散抽象CPS的安全概率，并通过自动车道保持系统进行了验证。


<details>
  <summary>Details</summary>
Motivation: 复杂的信息物理系统（CPS）在验证、诊断和调试等任务中需要强大的模型，但合适的模型通常难以获得或手动提取困难。

Method: 该研究提出了一种数据驱动方法，用于确定基于梅利机（Mealy machine）离散抽象的CPS在有限时间步内的安全概率。该方法基于可能近似正确（PAC）学习范式，结合了离散逻辑和概率可达性分析，并采用主动学习范式引导式地采样新数据。

Result: 通过一个自动车道保持系统案例研究，验证了所提出方法的有效性。

Conclusion: 该数据驱动方法为CPS的安全概率分析提供了一种可靠途径，通过PAC学习显著增加了确定概率的置信度，有助于应对模型获取挑战。

Abstract: Cyber-Physical Systems (CPS) are complex systems that require powerful models
for tasks like verification, diagnosis, or debugging. Often, suitable models
are not available and manual extraction is difficult. Data-driven approaches
then provide a solution to, e.g., diagnosis tasks and verification problems
based on data collected from the system. In this paper, we consider CPS with a
discrete abstraction in the form of a Mealy machine. We propose a data-driven
approach to determine the safety probability of the system on a finite horizon
of n time steps. The approach is based on the Probably Approximately Correct
(PAC) learning paradigm. Thus, we elaborate a connection between discrete logic
and probabilistic reachability analysis of systems, especially providing an
additional confidence on the determined probability. The learning process
follows an active learning paradigm, where new learning data is sampled in a
guided way after an initial learning set is collected. We validate the approach
with a case study on an automated lane-keeping system.

</details>


### [125] [Privileged Self-Access Matters for Introspection in AI](https://arxiv.org/abs/2508.14802)
*Siyuan Song,Harvey Lederman,Jennifer Hu,Kyle Mahowald*

Main category: cs.AI

TL;DR: 本文针对AI内省定义缺乏共识的问题，提出了一个更严格的AI内省定义，并通过实验证明大型语言模型（LLMs）虽可能表现出“轻量级”内省，但未能满足新定义下的真正内省能力。


<details>
  <summary>Details</summary>
Motivation: AI模型能否内省是一个日益重要的实践问题，但目前对“内省”的定义尚未达成共识。

Method: 本文从现有“轻量级”定义出发，提出了一种更严格的AI内省定义：内省是任何通过比计算成本相同或更低的第三方可用过程更可靠的方式，获取内部状态信息的机制。通过实验，让LLMs推理其内部“温度”参数。

Result: 实验结果表明，LLMs可能表现出符合“轻量级”定义的内省表象，但未能根据本文提出的更严格定义进行有意义的内省。

Conclusion: 需要一个更严格的定义来准确评估AI的内省能力，当前LLMs的内省表现未能达到本文提出的真实内省标准。

Abstract: Whether AI models can introspect is an increasingly important practical
question. But there is no consensus on how introspection is to be defined.
Beginning from a recently proposed ''lightweight'' definition, we argue instead
for a thicker one. According to our proposal, introspection in AI is any
process which yields information about internal states through a process more
reliable than one with equal or lower computational cost available to a third
party. Using experiments where LLMs reason about their internal temperature
parameters, we show they can appear to have lightweight introspection while
failing to meaningfully introspect per our proposed definition.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [126] [Deep Learning for School Dropout Detection: A Comparison of Tabular and Graph-Based Models for Predicting At-Risk Students](https://arxiv.org/abs/2508.14057)
*Pablo G. Almeida,Guilherme A. L. Silva,Valéria Santos,Gladston Moreira,Pedro Silva,Eduardo Luz*

Main category: cs.LG

TL;DR: 研究如何将学生表格数据转换为图结构，并使用图神经网络（GNNs）预测学生辍学，发现特定GNN配置在图结构上表现优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 学生辍学造成巨大的社会经济成本，及时预测风险学生至关重要。传统机器学习模型有前景，但GNNs有望通过捕获学生数据中的复杂关系提供更优预测。

Method: 将表格学生数据通过聚类（K-Means, HDBSCAN）和降维（PCA, UMAP）转换为图结构，然后使用GNNs（自定义GCN, GraphSAGE）进行辍学预测。将GNN性能与传统表格模型（Random Forest, XGBoost, TabNet）进行比较。

Result: 实验显示，在PCA-KMeans聚类生成的图上运行的GraphSAGE表现最佳，相比于最强的表格模型（XGBoost），宏观F1分数提升约7个百分点，准确率提升近2个百分点。但其他GNN配置和图构建方法未持续超越表格模型。

Conclusion: GNNs在学生辍学预测方面潜力巨大，但图生成策略和GNN架构选择对性能至关重要，揭示了将表格数据优化转换为图结构以进行图学习的挑战。

Abstract: Student dropout is a significant challenge in educational systems worldwide,
leading to substantial social and economic costs. Predicting students at risk
of dropout allows for timely interventions. While traditional Machine Learning
(ML) models operating on tabular data have shown promise, Graph Neural Networks
(GNNs) offer a potential advantage by capturing complex relationships inherent
in student data if structured as graphs. This paper investigates whether
transforming tabular student data into graph structures, primarily using
clustering techniques, enhances dropout prediction accuracy. We compare the
performance of GNNs (a custom Graph Convolutional Network (GCN) and GraphSAGE)
on these generated graphs against established tabular models (Random Forest
(RF), XGBoost, and TabNet) using a real-world student dataset. Our experiments
explore various graph construction strategies based on different clustering
algorithms (K-Means, HDBSCAN) and dimensionality reduction techniques
(Principal Component Analysis (PCA), Uniform Manifold Approximation and
Projection (UMAP)). Our findings demonstrate that a specific GNN configuration,
GraphSAGE on a graph derived from PCA-KMeans clustering, achieved superior
performance, notably improving the macro F1-score by approximately 7 percentage
points and accuracy by nearly 2 percentage points over the strongest tabular
baseline (XGBoost). However, other GNN configurations and graph construction
methods did not consistently surpass tabular models, emphasizing the critical
role of the graph generation strategy and GNN architecture selection. This
highlights both the potential of GNNs and the challenges in optimally
transforming tabular data for graph-based learning in this domain.

</details>


### [127] [Load Forecasting on A Highly Sparse Electrical Load Dataset Using Gaussian Interpolation](https://arxiv.org/abs/2508.14069)
*Chinmoy Biswas,Nafis Faisal,Vivek Chowdhury,Abrar Al-Shadid Abir,Sabir Mahmud,Mithon Rahman,Shaikh Anowarul Fattah,Hafiz Imtiaz*

Main category: cs.LG

TL;DR: 本研究探讨了如何利用高斯插值处理高达62%稀疏的电力负荷数据（假设为宽平稳数据）进行负荷预测，并通过比较多种机器学习和深度学习模型，发现高斯插值是一种有效的插值方法，且LSTM模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 实际数据集常存在稀疏性，尤其在电力负荷预测中。现有插值方法多适用于严格平稳数据。本研究旨在探索高斯插值是否能有效处理稀疏的宽平稳电力负荷数据，并将其用于准确的负荷预测。

Method: 对一个具有约62%稀疏度的电力工厂小时负荷数据集进行统计分析。采用高斯插值方法填充缺失数据，假设数据为宽平稳（WSS）。随后，训练多种机器学习和深度学习模型（包括长短期记忆网络LSTM）进行负荷预测，并比较其性能。

Result: 研究结果经验性地表明，高斯插值是处理负荷预测问题中稀疏数据的合适选项。此外，长短期记忆（LSTM）神经网络模型在所测试的各类经典和神经网络模型中表现最佳。

Conclusion: 高斯插值能够有效利用稀疏的宽平稳数据集进行电力负荷预测。在所评估的模型中，LSTM模型在处理此类负荷预测任务时展现出最优性能。

Abstract: Sparsity, defined as the presence of missing or zero values in a dataset,
often poses a major challenge while operating on real-life datasets. Sparsity
in features or target data of the training dataset can be handled using various
interpolation methods, such as linear or polynomial interpolation, spline,
moving average, or can be simply imputed. Interpolation methods usually perform
well with Strict Sense Stationary (SSS) data. In this study, we show that an
approximately 62\% sparse dataset with hourly load data of a power plant can be
utilized for load forecasting assuming the data is Wide Sense Stationary (WSS),
if augmented with Gaussian interpolation. More specifically, we perform
statistical analysis on the data, and train multiple machine learning and deep
learning models on the dataset. By comparing the performance of these models,
we empirically demonstrate that Gaussian interpolation is a suitable option for
dealing with load forecasting problems. Additionally, we demonstrate that Long
Short-term Memory (LSTM)-based neural network model offers the best performance
among a diverse set of classical and neural network-based models.

</details>


### [128] [Edge-Selector Model Applied for Local Search Neighborhood for Solving Vehicle Routing Problems](https://arxiv.org/abs/2508.14071)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux,Daniele Vigo*

Main category: cs.LG

TL;DR: 提出一种混合机器学习与元启发式机制，通过预测禁止移动来指导车辆路径问题（VRPs）的局部搜索，从而提高性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 旨在解决车辆路径问题（VRPs），通过机器学习识别局部搜索中的禁止移动，从而引导搜索过程并提升元启发式算法的性能。

Method: 核心是边缘解决方案选择器模型，它通过分类解决方案边缘来识别局部搜索中的禁止移动。该模型有两种实现方式：1) 基于表格的二元分类器，采用梯度提升树和前馈神经网络，并调整决策阈值处理类别不平衡；2) 图神经网络（GNN），利用图结构直接预测禁止移动。这些混合机制被应用于现有的元启发式基线算法中。

Result: 该方法在不同的元启发式基线算法、各种问题规模和变体（包括CVRP和CVRPTW）上，均表现出可扩展性和泛化能力，并实现了性能提升。在高达30,000个客户节点的基准数据集上进行了实验评估，并通过成对统计分析验证了观察到的改进。

Conclusion: 所提出的混合机器学习与元启发式方法能够有效提高车辆路径问题（VRPs）求解器的性能、可扩展性和泛化能力。

Abstract: This research proposes a hybrid Machine Learning and metaheuristic mechanism
that is designed to solve Vehicle Routing Problems (VRPs). The main of our
method is an edge solution selector model, which classifies solution edges to
identify prohibited moves during the local search, hence guiding the search
process within metaheuristic baselines. Two learning-based mechanisms are used
to develop the edge selector: a simple tabular binary classifier and a Graph
Neural Network (GNN). The tabular classifier employs Gradient Boosting Trees
and Feedforward Neural Network as the baseline algorithms. Adjustments to the
decision threshold are also applied to handle the class imbalance in the
problem instance. An alternative mechanism employs the GNN to utilize graph
structure for direct solution edge prediction, with the objective of guiding
local search by predicting prohibited moves. These hybrid mechanisms are then
applied in state-fo-the-art metaheuristic baselines. Our method demonstrates
both scalability and generalizability, achieving performance improvements
across different baseline metaheuristics, various problem sizes and variants,
including the Capacitated Vehicle Routing Problem (CVRP) and CVRP with Time
Windows (CVRPTW). Experimental evaluations on benchmark datasets up to 30,000
customer nodes, supported by pair-wise statistical analysis, verify the
observed improvements.

</details>


### [129] [Multi-Objective Bayesian Optimization with Independent Tanimoto Kernel Gaussian Processes for Diverse Pareto Front Exploration](https://arxiv.org/abs/2508.14072)
*Anabel Yong*

Main category: cs.LG

TL;DR: GP-MOBO是一种新型多目标贝叶斯优化算法，通过高效处理分子指纹维度，在分子优化中超越传统方法，能发现更高质量分子并更广阔地探索化学空间。


<details>
  <summary>Details</summary>
Motivation: 提升分子优化领域的效率和质量，解决传统方法在处理高维稀疏分子指纹时计算资源消耗大或性能不足的问题。

Method: 提出GP-MOBO，一种基于精确高斯过程（GPs）的多目标贝叶斯优化算法。该方法集成了快速轻量级的GP包，能够高效处理稀疏分子指纹的完整维度，且无需大量计算资源。

Result: GP-MOBO持续优于传统方法（如GP-BO），识别出更高质量且有效的SMILES。该模型实现了更广泛的化学搜索空间探索，在所有测试场景中显示出优越的帕累托前沿接近度。在DockSTRING数据集上，GP-MOBO在20次迭代中获得更高的几何平均值。

Conclusion: GP-MOBO在解决复杂多目标优化挑战方面具有显著的有效性、效率和极低的计算开销，代表了分子优化领域的重大进展。

Abstract: We present GP-MOBO, a novel multi-objective Bayesian Optimization algorithm
that advances the state-of-the-art in molecular optimization. Our approach
integrates a fast minimal package for Exact Gaussian Processes (GPs) capable of
efficiently handling the full dimensionality of sparse molecular fingerprints
without the need for extensive computational resources. GP-MOBO consistently
outperforms traditional methods like GP-BO by fully leveraging fingerprint
dimensionality, leading to the identification of higher-quality and valid
SMILES. Moreover, our model achieves a broader exploration of the chemical
search space, as demonstrated by its superior proximity to the Pareto front in
all tested scenarios. Empirical results from the DockSTRING dataset reveal that
GP-MOBO yields higher geometric mean values across 20 Bayesian optimization
iterations, underscoring its effectiveness and efficiency in addressing complex
multi-objective optimization challenges with minimal computational overhead.

</details>


### [130] [MCLPD:Multi-view Contrastive Learning for EEG-based PD Detection Across Datasets](https://arxiv.org/abs/2508.14073)
*Qian Zhanga,Ruilin Zhang,Jun Xiao,Yifan Liu,Zhe Wang*

Main category: cs.LG

TL;DR: 本文提出MCLPD半监督学习框架，利用多视图对比预训练和轻量级微调，解决EEG帕金森病跨数据集检测中数据标注成本高、数据集泛化性差的问题，显著提高检测性能并减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）在帕金森病（PD）检测中有效，但EEG数据标注成本高导致数据集规模有限，且不同数据集间存在采集协议和受试者差异，严重影响模型在跨数据集检测场景中的鲁棒性和泛化能力。

Method: 提出MCLPD半监督学习框架，结合多视图对比预训练和轻量级监督微调。预训练阶段在未标注的UNM数据集上进行自监督学习，通过时域和频域的双重数据增强构建对比对。微调阶段仅使用UI和UC数据集中的少量标注数据进行监督优化。

Result: 实验结果显示，MCLPD在使用1%标注数据时，在UI和UC数据集上的F1分数分别达到0.91和0.81；当使用5%标注数据时，F1分数分别进一步提高至0.97和0.87。

Conclusion: MCLPD框架显著提高了跨数据集泛化能力，并降低了对标注数据的依赖，证明了所提框架在帕金森病检测中的有效性。

Abstract: Electroencephalography has been validated as an effective technique for
detecting Parkinson's disease,particularly in its early stages.However,the high
cost of EEG data annotation often results in limited dataset size and
considerable discrepancies across datasets,including differences in acquisition
protocols and subject demographics,significantly hinder the robustness and
generalizability of models in cross-dataset detection scenarios.To address such
challenges,this paper proposes a semi-supervised learning framework named
MCLPD,which integrates multi-view contrastive pre-training with lightweight
supervised fine-tuning to enhance cross-dataset PD detection performance.During
pre-training,MCLPD uses self-supervised learning on the unlabeled UNM
dataset.To build contrastive pairs,it applies dual augmentations in both time
and frequency domains,which enrich the data and naturally fuse time-frequency
information.In the fine-tuning phase,only a small proportion of labeled data
from another two datasets (UI and UC)is used for supervised
optimization.Experimental results show that MCLPD achieves F1 scores of 0.91 on
UI and 0.81 on UC using only 1%of labeled data,which further improve to 0.97
and 0.87,respectively,when 5%of labeled data is used.Compared to existing
methods,MCLPD substantially improves cross-dataset generalization while
reducing the dependency on labeled data,demonstrating the effectiveness of the
proposed framework.

</details>


### [131] [GEPD:GAN-Enhanced Generalizable Model for EEG-Based Detection of Parkinson's Disease](https://arxiv.org/abs/2508.14074)
*Qian Zhang,Ruilin Zhang,Biaokai Zhu,Xun Han,Jun Xiao,Yifan Liu,Zhe Wang*

Main category: cs.LG

TL;DR: 提出GEPD模型，一个基于GAN的EEG帕金森病跨数据集泛化分类方法，旨在解决现有方法在不同数据集间泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于EEG的帕金森病检测方法在单一数据集上表现良好，但在跨数据集场景下，由于数据集间的变异性和小样本量，难以训练出具有良好泛化能力的模型。

Method: 本文提出了一个名为GEPD的GAN增强泛化模型。首先，设计一个生成网络，通过控制生成数据与真实数据之间的分布相似性来创建融合EEG数据，并辅以EEG信号质量评估模型以确保生成数据质量。其次，设计一个分类网络，该网络结合了多个卷积神经网络，以有效捕捉EEG信号的时频特征，同时保持泛化结构和易于收敛。

Result: 评估结果表明，所提出的模型在跨数据集设置中表现与最先进模型相当，达到了84.3%的准确率和84.0%的F1分数，验证了其泛化能力。

Conclusion: GEPD模型在EEG帕金森病跨数据集分类中展现出良好的泛化能力，有望为神经系统疾病的诊断和监测提供智能方法支持。

Abstract: Electroencephalography has been established as an effective method for
detecting Parkinson's disease, typically diagnosed early.Current Parkinson's
disease detection methods have shown significant success within individual
datasets, however, the variability in detection methods across different EEG
datasets and the small size of each dataset pose challenges for training a
generalizable model for cross-dataset scenarios. To address these issues, this
paper proposes a GAN-enhanced generalizable model, named GEPD, specifically for
EEG-based cross-dataset classification of Parkinson's disease.First, we design
a generative network that creates fusion EEG data by controlling the
distribution similarity between generated data and real data.In addition, an
EEG signal quality assessment model is designed to ensure the quality of
generated data great.Second, we design a classification network that utilizes a
combination of multiple convolutional neural networks to effectively capture
the time-frequency characteristics of EEG signals, while maintaining a
generalizable structure and ensuring easy convergence.This work is dedicated to
utilizing intelligent methods to study pathological manifestations, aiming to
facilitate the diagnosis and monitoring of neurological diseases.The evaluation
results demonstrate that our model performs comparably to state-of-the-art
models in cross-dataset settings, achieving an accuracy of 84.3% and an
F1-score of 84.0%, showcasing the generalizability of the proposed model.

</details>


### [132] [Explainable Graph Spectral Clustering For Text Embeddings](https://arxiv.org/abs/2508.14075)
*Mieczysław A. Kłopotek,Sławomir T. Wierzchoń,Bartłomiej Starosta,Piotr Borkowski,Dariusz Czerski,Eryk Laskowski*

Main category: cs.LG

TL;DR: 本文旨在通过引入GloVe等其他文档嵌入方法，推广此前关于文本图谱谱聚类结果可解释性的研究。


<details>
  <summary>Details</summary>
Motivation: 将先前针对特定文档相似度计算（词向量空间余弦相似度）的文本图谱谱聚类可解释性研究，推广并验证其在采用更丰富的文档嵌入（如GloVe）时的适用性与表现。

Method: 通过考虑使用基于GloVe等理念的文档嵌入来计算文档相似度，并在此基础上对图谱谱聚类结果的可解释性进行泛化研究。

Result: 摘要中未明确提及具体研究结果，主要阐述了研究方法的泛化方向。

Conclusion: 摘要未给出明确的结论，主要描述了本研究旨在推广前一工作的方法论。

Abstract: In a previous paper, we proposed an introduction to the explainability of
Graph Spectral Clustering results for textual documents, given that document
similarity is computed as cosine similarity in term vector space.
  In this paper, we generalize this idea by considering other embeddings of
documents, in particular, based on the GloVe embedding idea.

</details>


### [133] [PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning](https://arxiv.org/abs/2508.14076)
*Mengdi Li,Guanqiao Chen,Xufeng Zhao,Haochen Wen,Shu Yang,Di Wang*

Main category: cs.LG

TL;DR: 本文提出PersRM-R1，一个基于推理的奖励模型框架，能从少量示例中识别并表示用户偏好，通过合成数据和两阶段训练，实现了高效的个性化LLM。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型难以在有限数据和多样化领域下捕捉细致的用户特定偏好，导致LLM个性化对齐效果不佳。

Method: 引入PersRM-R1，首个基于推理的奖励建模框架，专门从一个或几个个人范例中识别个人因素。采用合成数据生成结合两阶段训练流程，包括监督微调和强化微调，以解决数据限制和泛化性挑战。

Result: 实验结果表明，PersRM-R1在准确性和泛化性方面优于同等大小的现有模型，并可媲美更大模型的性能。

Conclusion: PersRM-R1为开发更有效的个性化LLM铺平了道路，通过其推理能力和高效的训练策略实现了卓越的个性化表现。

Abstract: Reward models (RMs), which are central to existing post-training methods, aim
to align LLM outputs with human values by providing feedback signals during
fine-tuning. However, existing RMs struggle to capture nuanced, user-specific
preferences, especially under limited data and across diverse domains. Thus, we
introduce PersRM-R1, the first reasoning-based reward modeling framework
specifically designed to identify and represent personal factors from only one
or a few personal exemplars. To address challenges including limited data
availability and the requirement for robust generalization, our approach
combines synthetic data generation with a two-stage training pipeline
consisting of supervised fine-tuning followed by reinforcement fine-tuning.
Experimental results demonstrate that PersRM-R1 outperforms existing models of
similar size and matches the performance of much larger models in both accuracy
and generalizability, paving the way for more effective personalized LLMs.

</details>


### [134] [Label Smoothing is a Pragmatic Information Bottleneck](https://arxiv.org/abs/2508.14077)
*Sota Kudo*

Main category: cs.LG

TL;DR: 本研究将标签平滑解释为一种实用的信息瓶颈方法，证明其能探索最优解并对无关因素不敏感。


<details>
  <summary>Details</summary>
Motivation: 旨在通过信息瓶颈的视角，重新审视并深入理解标签平滑的内在机制和特性。

Method: 在模型具有足够灵活性且无冲突标签的假设下，通过理论分析和实验验证，将标签平滑与信息瓶颈的最优解建立关联。

Result: 标签平滑的模型输出能够探索信息瓶颈的最优解；标签平滑可被解释为一种易于实现的信息瓶颈方法；作为信息瓶颈方法，标签平滑对不包含目标信息或在给定条件下不提供额外信息的因素表现出不敏感性。

Conclusion: 标签平滑是信息瓶颈原理的一种简单且有效的实际实现，能够探索最优解并对无关信息具有鲁棒性。

Abstract: This study revisits label smoothing via a form of information bottleneck.
Under the assumption of sufficient model flexibility and no conflicting labels
for the same input, we theoretically and experimentally demonstrate that the
model output obtained through label smoothing explores the optimal solution of
the information bottleneck. Based on this, label smoothing can be interpreted
as a practical approach to the information bottleneck, enabling simple
implementation. As an information bottleneck method, we experimentally show
that label smoothing also exhibits the property of being insensitive to factors
that do not contain information about the target, or to factors that provide no
additional information about it when conditioned on another variable.

</details>


### [135] [Out-of-Sample Hydrocarbon Production Forecasting: Time Series Machine Learning using Productivity Index-Driven Features and Inductive Conformal Prediction](https://arxiv.org/abs/2508.14078)
*Mohamed Hassan Abdalla Idris,Jakub Marek Cebula,Jebraeel Gholinezhad,Shamsul Masum,Hongjie Ma*

Main category: cs.LG

TL;DR: 本研究提出一种新的机器学习框架，结合生产力指数（PI）特征选择和归纳共形预测（ICP），旨在增强油气生产样本外预测的鲁棒性及不确定性量化。研究评估了LSTM、BiLSTM、GRU和XGBoost等模型，发现LSTM表现最佳，且该框架有效提升了预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 提高油气生产预测（特别是多变量时间序列分析中的样本外预测）的鲁棒性。

Method: 引入新机器学习框架：整合了基于生产力指数（PI）的特征选择（源于储层工程领域知识）和归纳共形预测（ICP）以进行严格的不确定性量化。利用Volve和Norne油田的历史数据，评估了LSTM、BiLSTM、GRU和XGBoost等多种预测算法对历史产油率（OPR_H）的预测效果。模型性能通过MAE、预测偏差和预测方向准确性（PDA）等指标进行全面评估。

Result: PI-based特征选择有效降低了输入维度。ICP框架成功实现了不依赖分布假设的有效不确定性量化，保证了预测区间的覆盖率。LSTM模型表现出卓越性能，在PF14井的测试集和真实样本外预测数据上均实现了最低MAE（分别为19.468和29.638），并在Norne井E1H上得到进一步验证。

Conclusion: 结合领域专业知识（如PI）与先进机器学习技术（如LSTM和ICP）能显著提升油气生产预测的可靠性。

Abstract: This research introduces a new ML framework designed to enhance the
robustness of out-of-sample hydrocarbon production forecasting, specifically
addressing multivariate time series analysis. The proposed methodology
integrates Productivity Index (PI)-driven feature selection, a concept derived
from reservoir engineering, with Inductive Conformal Prediction (ICP) for
rigorous uncertainty quantification. Utilizing historical data from the Volve
(wells PF14, PF12) and Norne (well E1H) oil fields, this study investigates the
efficacy of various predictive algorithms-namely Long Short-Term Memory (LSTM),
Bidirectional LSTM (BiLSTM), Gated Recurrent Unit (GRU), and eXtreme Gradient
Boosting (XGBoost) - in forecasting historical oil production rates (OPR_H).
All the models achieved "out-of-sample" production forecasts for an upcoming
future timeframe. Model performance was comprehensively evaluated using
traditional error metrics (e.g., MAE) supplemented by Forecast Bias and
Prediction Direction Accuracy (PDA) to assess bias and trend-capturing
capabilities. The PI-based feature selection effectively reduced input
dimensionality compared to conventional numerical simulation workflows. The
uncertainty quantification was addressed using the ICP framework, a
distribution-free approach that guarantees valid prediction intervals (e.g.,
95% coverage) without reliance on distributional assumptions, offering a
distinct advantage over traditional confidence intervals, particularly for
complex, non-normal data. Results demonstrated the superior performance of the
LSTM model, achieving the lowest MAE on test (19.468) and genuine out-of-sample
forecast data (29.638) for well PF14, with subsequent validation on Norne well
E1H. These findings highlight the significant potential of combining
domain-specific knowledge with advanced ML techniques to improve the
reliability of hydrocarbon production forecasts.

</details>


### [136] [A Guide to Robust Generalization: The Impact of Architecture, Pre-training, and Optimization Strategy](https://arxiv.org/abs/2508.14079)
*Maxime Heuillet,Rishika Bhagwatkar,Jonas Ngnawé,Yann Pequignot,Alexandre Larouche,Christian Gagné,Irina Rish,Ola Ahmad,Audrey Durand*

Main category: cs.LG

TL;DR: 本研究对鲁棒性微调中的设计选择如何影响鲁棒泛化能力进行了大规模实证分析，发现大规模数据集上监督预训练的CNN模型通常表现最佳。


<details>
  <summary>Details</summary>
Motivation: 图像深度学习模型易受小扰动攻击，鲁棒性训练成本高。鲁棒性微调作为替代方案，其多种设计选择（如模型更新协议、损失函数、架构类型、预训练表示）如何影响模型的鲁棒泛化能力，仍是未解决且具有重要实际意义的问题。

Method: 进行了一项大规模实证研究，构建了迄今为止最全面多样的鲁棒性微调基准。研究涵盖6个数据集、40种预训练架构、2种专用损失函数和3种适应协议，共生成1440种训练配置，并对五种扰动类型进行了7200次鲁棒性测量。

Result: 分析发现，尽管基于注意力机制的架构和鲁棒性预训练表示日益流行，但在大型数据集上以监督方式预训练的卷积神经网络（CNN）通常表现最佳。研究结果既证实也挑战了先前的设计假设。

Conclusion: 该研究为鲁棒性微调的设计选择提供了实用指导，并指出了有前景的研究方向。

Abstract: Deep learning models operating in the image domain are vulnerable to small
input perturbations. For years, robustness to such perturbations was pursued by
training models from scratch (i.e., with random initializations) using
specialized loss objectives. Recently, robust fine-tuning has emerged as a more
efficient alternative: instead of training from scratch, pretrained models are
adapted to maximize predictive performance and robustness. To conduct robust
fine-tuning, practitioners design an optimization strategy that includes the
model update protocol (e.g., full or partial) and the specialized loss
objective. Additional design choices include the architecture type and size,
and the pretrained representation. These design choices affect robust
generalization, which is the model's ability to maintain performance when
exposed to new and unseen perturbations at test time. Understanding how these
design choices influence generalization remains an open question with
significant practical implications. In response, we present an empirical study
spanning 6 datasets, 40 pretrained architectures, 2 specialized losses, and 3
adaptation protocols, yielding 1,440 training configurations and 7,200
robustness measurements across five perturbation types. To our knowledge, this
is the most diverse and comprehensive benchmark of robust fine-tuning to date.
While attention-based architectures and robust pretrained representations are
increasingly popular, we find that convolutional neural networks pretrained in
a supervised manner on large datasets often perform best. Our analysis both
confirms and challenges prior design assumptions, highlighting promising
research directions and offering practical guidance.

</details>


### [137] [KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World Knowledge](https://arxiv.org/abs/2508.14080)
*Guanghao Jin,Jingpei Wu,Tianpei Guo,Yiyi Niu,Weidong Zhou,Guoyang Liu*

Main category: cs.LG

TL;DR: 本文提出KnowDR-REC新基准，用于评估多模态大模型(MLLMs)在知识驱动的指代表达理解(REC)中的推理能力，发现现有MLLMs在该任务上表现不佳，并存在文本理解与视觉定位脱节的问题。


<details>
  <summary>Details</summary>
Motivation: 现有指代表达理解(REC)基准过于依赖图像内线索或缺乏细粒度标注，不足以评估多模态大语言模型(MLLMs)的复杂推理能力，特别是知识驱动的多模态推理。

Method: 提出了一个名为KnowDR-REC的新基准，它基于真实世界知识、包含通过细粒度编辑构建的负样本以评估模型鲁棒性，并引入三种新型评估指标来探究模型内部推理过程。研究评估了16个最先进的多模态模型。

Result: 实验结果表明，现有MLLMs在知识驱动的视觉定位任务上仍面临挑战；观察到MLLMs中文本理解与视觉定位之间存在脱节现象；许多模型受到记忆捷径关联的显著影响，这严重阻碍了其真正的多模态推理能力。

Conclusion: 提出的KnowDR-REC基准有望启发未来研究，开发更鲁棒、可解释和知识密集型的视觉定位框架，从而促进复杂真实世界场景中更可靠和强大的多模态系统发展。

Abstract: Referring Expression Comprehension (REC) is a popular multimodal task that
aims to accurately detect target objects within a single image based on a given
textual expression. However, due to the limitations of earlier models,
traditional REC benchmarks either rely solely on intra-image cues or lack
sufficiently fine-grained instance annotations, making them inadequate for
evaluating the reasoning capabilities of Multi-modal Large Language Models
(MLLMs). To address this gap, we propose a new benchmark, KnowDR-REC,
characterized by three key features: Firstly, it is built upon real-world
knowledge, requiring fine-grained multimodal reasoning across text and image.
Secondly, the dataset includes elaborately constructed negative samples via
fine-grained expression editing, designed to evaluate a model's robustness and
anti-hallucination ability. Lastly, we introduce three novel evaluation metrics
to systematically explore the model's internal reasoning process. We evaluate
16 state-of-the-art multimodal models on KnowDR-REC, with experimental results
showing that existing MLLMs still struggle with knowledge-driven visual
grounding tasks. Furthermore, we observe a decoupling between textual
understanding and visual grounding in MLLMs, where many models are
significantly influenced by memorized shortcut correlations, which severely
affect their behavior on our benchmark and hinder genuine multimodal reasoning.
We anticipate that the proposed benchmark will inspire future research towards
developing more robust, interpretable, and knowledge-intensive visual grounding
frameworks, driving the development of more reliable and robust multimodal
systems for complex real-world scenarios.

</details>


### [138] [Toward Lifelong Learning in Equilibrium Propagation: Sleep-like and Awake Rehearsal for Enhanced Stability](https://arxiv.org/abs/2508.14081)
*Yoshimasa Kubo,Jean Erik Delanois,Maxim Bazhenov*

Main category: cs.LG

TL;DR: 该研究提出了一种“睡眠式重放巩固（SRC）”算法，有效解决了EP训练的循环神经网络在连续学习中的灾难性遗忘问题，并在多个数据集上实现了优异性能。


<details>
  <summary>Details</summary>
Motivation: EP训练的循环神经网络在连续学习中面临灾难性遗忘问题，即学习新任务时会覆盖旧知识。这与人脑通过睡眠记忆巩固来整合新旧知识的能力形成对比。

Method: 提出了一种“睡眠式重放巩固（SRC）”算法，并将其应用于EP训练的多层循环神经网络（MRNN-EP），在每次新任务训练后实施。研究还探讨了SRC与“清醒重放”（rehearsal）结合的效果。

Result: ['SRC显著提高了RNN在连续学习中对灾难性遗忘的抵抗力。', '在类别增量学习中，带有SRC的MRNN-EP模型比采用多种正则化技术的传统前馈网络表现更优。', '在MNIST数据集上，配备SRC的MRNN-EP与BPTT训练的MRNN性能相当；但在Fashion MNIST、Kuzushiji-MNIST、CIFAR10和ImageNet数据集上，MRNN-EP超越了基于BPTT的模型。', '将SRC与重放（即“清醒重放”）结合使用，进一步增强了网络在学习新任务的同时保留长期知识的能力。']

Conclusion: 研究表明“睡眠式重放”技术适用于循环神经网络，并突出了将类人学习行为整合到人工神经网络中的潜力。

Abstract: Recurrent neural networks (RNNs) trained using Equilibrium Propagation (EP),
a biologically plausible training algorithm, have demonstrated strong
performance in various tasks such as image classification and reinforcement
learning. However, these networks face a critical challenge in continuous
learning: catastrophic forgetting, where previously acquired knowledge is
overwritten when new tasks are learned. This limitation contrasts with the
human brain's ability to retain and integrate both old and new knowledge, aided
by processes like memory consolidation during sleep through the replay of
learned information. To address this challenge in RNNs, here we propose a
sleep-like replay consolidation (SRC) algorithm for EP-trained RNNs. We found
that SRC significantly improves RNN's resilience to catastrophic forgetting in
continuous learning scenarios. In class-incremental learning with SRC
implemented after each new task training, the EP-trained multilayer RNN model
(MRNN-EP) performed significantly better compared to feedforward networks
incorporating several well-established regularization techniques. The MRNN-EP
performed on par with MRNN trained using Backpropagation Through Time (BPTT)
when both were equipped with SRC on MNIST data and surpassed BPTT-based models
on the Fashion MNIST, Kuzushiji-MNIST, CIFAR10, and ImageNet datasets.
Combining SRC with rehearsal, also known as "awake replay", further boosted the
network's ability to retain long-term knowledge while continuing to learn new
tasks. Our study reveals the applicability of sleep-like replay techniques to
RNNs and highlights the potential for integrating human-like learning behaviors
into artificial neural networks (ANNs).

</details>


### [139] [Toward Generalist Semi-supervised Regression via Decoupled Representation Distillation](https://arxiv.org/abs/2508.14082)
*Ye Su,Hezhe Qiao,Wei Huang,Lin Chen*

Main category: cs.LG

TL;DR: 本文提出DRILL框架，通过离散分布估计和解耦分布对齐，解决半监督回归中伪标签质量差及过拟合问题，取得了显著的性能提升和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有半监督回归方法过度依赖伪标签质量，且直接回归难以学习标签分布并易导致过拟合。

Method: 引入端到端解耦表示蒸馏框架（DRILL），将回归任务转化为多桶离散分布估计（DDE），以更好地捕获标签分布并减轻过拟合。随后，采用解耦分布对齐（DDA）在桶分布层面校准师生模型的目标桶和非目标桶，以促进学生模型学习更鲁棒和泛化的知识。

Result: 在多个领域数据集上进行的广泛实验表明，所提出的DRILL框架具有强大的泛化能力，并优于现有竞争方法。

Conclusion: DRILL框架通过创新的离散分布估计和解耦分布对齐机制，有效解决了半监督回归中的核心挑战，展现出优异的性能和泛化能力。

Abstract: Semi-supervised regression (SSR), which aims to predict continuous scores of
samples while reducing reliance on a large amount of labeled data, has recently
received considerable attention across various applications, including computer
vision, natural language processing, and audio and medical analysis. Existing
semi-supervised methods typically apply consistency regularization on the
general regression task by generating pseudo-labels. However, these methods
heavily rely on the quality of pseudo-labels, and direct regression fails to
learn the label distribution and can easily lead to overfitting. To address
these challenges, we introduce an end-to-end Decoupled Representation
distillation framework (DRILL) which is specially designed for the
semi-supervised regression task where we transform the general regression task
into a Discrete Distribution Estimation (DDE) task over multiple buckets to
better capture the underlying label distribution and mitigate the risk of
overfitting associated with direct regression. Then we employ the Decoupled
Distribution Alignment (DDA) to align the target bucket and non-target bucket
between teacher and student on the distribution of buckets, encouraging the
student to learn more robust and generalized knowledge from the teacher.
Extensive experiments conducted on datasets from diverse domains demonstrate
that the proposed DRILL has strong generalization and outperforms the competing
methods.

</details>


### [140] [GeoMAE: Masking Representation Learning for Spatio-Temporal Graph Forecasting with Missing Values](https://arxiv.org/abs/2508.14083)
*Songyu Ke,Chenyu Wu,Yuxuan Liang,Xiuwen Yi,Yanping Sun,Junbo Zhang,Yu Zheng*

Main category: cs.LG

TL;DR: 本文提出一个自监督对比学习框架（CSST），通过在大量未标记的时空噪声数据上进行预训练，有效解决了从低质量数据中准确推断兴趣点（POI）人群流量的挑战，并表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 准确获取兴趣点（POI）的人群流量对于交通管理、公共服务和城市规划至关重要。然而，受限于城市传感技术，多数数据源质量不足以监测每个POI的人群流量，导致从低质量数据推断准确人群流量成为一项关键且具挑战性的任务。挑战性源于：1）标记数据稀缺；2）POI间复杂的时空依赖；3）精确人群流量与GPS报告间的多重关联。

Method: 将人群流量推断问题重新定义为自监督属性图表示学习任务，并引入名为CSST（Contrastive Self-learning framework for Spatio-Temporal data）的新型对比自学习框架。该方法首先构建基于POI及其距离的空间邻接图，然后利用对比学习技术从大量未标记的时空数据中学习表示，采用交换预测（swapped prediction）方法从相似实例中预测目标子图的表示。预训练完成后，模型将使用准确的人群流量数据进行微调。

Result: 在两个真实世界数据集上的实验结果表明，在大量噪声数据上预训练的CSST模型，其性能始终优于从头开始训练的模型。

Conclusion: CSST框架通过将人群流量推断问题重构为自监督图表示学习，并利用对比学习技术有效利用大量未标记数据进行预训练，成功克服了从低质量数据中准确推断人群流量的挑战，显著提升了预测性能。

Abstract: Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal
for effective traffic management, public service, and urban planning. Despite
this importance, due to the limitations of urban sensing techniques, the data
quality from most sources is inadequate for monitoring crowd flow at each POI.
This renders the inference of accurate crowd flow from low-quality data a
critical and challenging task. The complexity is heightened by three key
factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The
intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad
correlations between precise crowd flow and GPS reports}.
  To address these challenges, we recast the crowd flow inference problem as a
self-supervised attributed graph representation learning task and introduce a
novel \underline{C}ontrastive \underline{S}elf-learning framework for
\underline{S}patio-\underline{T}emporal data (\model). Our approach initiates
with the construction of a spatial adjacency graph founded on the POIs and
their respective distances. We then employ a contrastive learning technique to
exploit large volumes of unlabeled spatio-temporal data. We adopt a swapped
prediction approach to anticipate the representation of the target subgraph
from similar instances. Following the pre-training phase, the model is
fine-tuned with accurate crowd flow data. Our experiments, conducted on two
real-world datasets, demonstrate that the \model pre-trained on extensive noisy
data consistently outperforms models trained from scratch.

</details>


### [141] [Parameter-Aware Ensemble SINDy for Interpretable Symbolic SGS Closure](https://arxiv.org/abs/2508.14085)
*Hanseul Kang,Shervin Karimkashi,Ville Vuorinen*

Main category: cs.LG

TL;DR: 提出一个可扩展、参数感知的稀疏回归框架，用于从多参数模拟数据中发现可解释的偏微分方程和次网格尺度闭合。该框架在Burgers方程上验证了其能力，自主发现了Smagorinsky型闭合，并显示出优于传统方法的预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏回归方法（如SINDy）在处理多参数数据、确保物理一致性、提高可伸缩性及模型鲁棒性方面存在局限性。研究旨在开发一种能自主、数据驱动地发现可解释PDE和SGS闭合的通用框架。

Method: 基于SINDy，提出参数感知稀疏回归框架，通过四项创新解决上述问题：1) 符号化参数化实现统一回归中物理参数变化；2) 维度相似性过滤器确保单位一致性并精简候选库；3) 内存高效的Gram矩阵累积支持批处理；4) 集成共识与系数稳定性分析确保模型鲁棒性。

Result: 1) 在一维基准测试中，可靠恢复了跨参数范围的控制方程。2) 应用于Burgers数据集，自主发现Smagorinsky型SGS闭合（τSGS = 0.1603·Δ²(∂ū/∂x)²，Smagorinsky常数约0.4004）。3) 该模型在不同滤波尺度下R²达0.886，预测精度优于经典闭合模型。

Conclusion: 该框架能自主识别具有物理意义的SGS形式并校准系数，为湍流建模提供新方法。此工作推动了数据驱动闭合发现领域的发展。

Abstract: We present a scalable, parameter-aware sparse regression framework for
discovering interpretable partial differential equations and subgrid-scale
closures from multi-parameter simulation data. Building on SINDy (Sparse
Identification of Nonlinear Dynamics), our approach addresses key limitations
through four innovations: symbolic parameterisation enabling physical
parameters to vary within unified regression; Dimensional Similarity Filter
enforcing unit-consistency whilst reducing candidate libraries;
memory-efficient Gram-matrix accumulation enabling batch processing; and
ensemble consensus with coefficient stability analysis for robust model
identification.
  Validation on canonical one-dimensional benchmarks demonstrates reliable
recovery of governing equations across parameter ranges. Applied to filtered
Burgers datasets, the framework discovers an SGS closure $\tau_{\mathrm{SGS}} =
0.1603\cdot\Delta^2\left(\frac{\partial \bar{u}}{\partial x}\right)^2$,
corresponding to a Smagorinsky constant of approximately 0.4004. This
represents autonomous discovery of Smagorinsky-type closure structure from data
without prior theoretical assumptions.
  The discovered model achieves $R^2 = 0.886$ across filter scales and
demonstrates improved prediction accuracy compared to classical closures. The
framework's ability to identify physically meaningful SGS forms and calibrate
coefficients offers a complementary approach to existing turbulence modelling
methods, contributing to the growing field of data-driven closure discovery.

</details>


### [142] [EEGDM: EEG Representation Learning via Generative Diffusion Model](https://arxiv.org/abs/2508.14086)
*Jia Hong Puah,Sim Kuan Goh,Ziwei Zhang,Zixuan Ye,Chow Khuen Chan,Kheng Seang Lim,Si Lei Fong,Kok Sin Woon*

Main category: cs.LG

TL;DR: 本文提出EEGDM框架，结合生成扩散模型与结构化状态空间模型（SSMDP），用于高效学习EEG信号表征，并在分类任务中性能超越现有EEG基础模型，同时计算成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）信号分析面临标注数据有限和信号变异性高等挑战。现有EEG基础模型（FMs）虽具潜力，但计算成本高昂，且模型增大后性能提升有限，难以有效学习有意义的EEG表征。

Method: 本文提出EEG表征学习框架EEGDM，其核心是生成扩散模型。具体而言，开发了用于扩散预训练的结构化状态空间模型（SSMDP）以更好地捕捉EEG信号的时间动态，并采用去噪扩散概率模型进行训练。最终生成的潜在EEG表征通过所提出的潜在融合Transformer（LFT）用于下游分类任务。

Result: 在多事件Temple University EEG事件语料库上的实证结果表明，EEGDM的性能优于包括EEG基础模型在内的现有最先进方法，且计算量轻约19倍。

Conclusion: EEGDM为当前的EEG基础模型提供了一个有前景的替代方案，它在实现卓越性能的同时，显著降低了计算成本，证明了其在EEG表征学习领域的有效性和高效性。

Abstract: While electroencephalogram (EEG) has been a crucial tool for monitoring the
brain and diagnosing neurological disorders (e.g., epilepsy), learning
meaningful representations from raw EEG signals remains challenging due to
limited annotations and high signal variability. Recently, EEG foundation
models (FMs) have shown promising potential by adopting transformer
architectures and self-supervised pre-training methods from large language
models (e.g., masked prediction) to learn representations from diverse EEG
data, followed by fine-tuning on specific EEG tasks. Nonetheless, these large
models often incurred high computational costs during both training and
inference, with only marginal performance improvements as model size increases.
In this work, we proposed EEG representation learning framework building upon
Generative Diffusion Model (EEGDM). Specifically, we developed structured
state-space model for diffusion pretraining (SSMDP) to better capture the
temporal dynamics of EEG signals and trained the architecture using a Denoising
Diffusion Probabilistic Model. The resulting latent EEG representations were
then used for downstream classification tasks via our proposed latent fusion
transformer (LFT). To evaluate our method, we used the multi-event Temple
University EEG Event Corpus and compared EEGDM with current state-of-the-art
approaches, including EEG FMs. Empirical results showed that our method
outperformed existing methods while being approximately 19x more lightweight.
These findings suggested that EEGDM offered a promising alternative to current
FMs. Our code is available at: https://github.com/jhpuah/EEGDM.

</details>


### [143] [FM4NPP: A Scaling Foundation Model for Nuclear and Particle Physics](https://arxiv.org/abs/2508.14087)
*David Park,Shuhang Li,Yi Huang,Xihaier Luo,Haiwang Yu,Yeonju Go,Christopher Pinkenburg,Yuewei Lin,Shinjae Yoo,Joseph Osborn,Jin Huang,Yihui Ren*

Main category: cs.LG

TL;DR: 本文提出一种新型自监督训练方法，并构建大型数据集，成功为粒子物理数据开发出可扩展且泛化能力强的基础模型，性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型范式虽已启发科学基础模型发展，但将此能力应用于粒子物理学极具挑战，因探测器数据稀疏且空间分布，与自然语言截然不同。本研究旨在探讨粒子物理基础模型能否在多样化任务中实现规模化和泛化。

Method: 引入一个包含1100多万粒子碰撞事件的新数据集，并设计了一系列下游任务和标注数据用于评估。提出一种新颖的探测器数据自监督训练方法，并验证了其神经可扩展性，模型参数高达1.88亿。通过冻结模型权重并使用任务特定适配器进行评估。

Result: 该基础模型在所有下游任务中均显著优于基线模型，并展现出强大的数据高效适应能力。进一步分析表明，该模型提取的表示是任务无关的，但可通过简单的线性映射针对不同下游任务进行特化。

Conclusion: 粒子物理领域的基础模型能够实现规模化和跨任务泛化，克服了探测器数据特有的挑战。该模型学习到的表示具有任务无关性，能通过简单适配应用于不同任务，为实验粒子物理学中通用人工智能的发展奠定了基础。

Abstract: Large language models have revolutionized artificial intelligence by enabling
large, generalizable models trained through self-supervision. This paradigm has
inspired the development of scientific foundation models (FMs). However,
applying this capability to experimental particle physics is challenging due to
the sparse, spatially distributed nature of detector data, which differs
dramatically from natural language. This work addresses if an FM for particle
physics can scale and generalize across diverse tasks. We introduce a new
dataset with more than 11 million particle collision events and a suite of
downstream tasks and labeled data for evaluation. We propose a novel
self-supervised training method for detector data and demonstrate its neural
scalability with models that feature up to 188 million parameters. With frozen
weights and task-specific adapters, this FM consistently outperforms baseline
models across all downstream tasks. The performance also exhibits robust
data-efficient adaptation. Further analysis reveals that the representations
extracted by the FM are task-agnostic but can be specialized via a single
linear mapping for different downstream tasks.

</details>


### [144] [CoBAD: Modeling Collective Behaviors for Human Mobility Anomaly Detection](https://arxiv.org/abs/2508.14088)
*Haomin Wen,Shurui Cao,Leman Akoglu*

Main category: cs.LG

TL;DR: 本文提出CoBAD模型，一种用于人类出行集体异常检测的新型方法，通过双阶段注意力机制和预训练，有效捕捉个体间时空依赖，并在大规模数据集上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统的出行异常检测主要关注个体模式，而集体出行异常（如儿童独处家中父母不在）仍是未充分探索的挑战。这类异常涉及个体间的时空依赖，比个体异常更复杂，但对公共安全和城市规划至关重要。

Method: 提出CoBAD模型，将问题表述为基于集体事件序列(CES)和共现事件图的无监督学习。CoBAD采用双阶段注意力机制，建模个体出行模式及个体间的互动。通过蒙版事件和链接重构任务在大规模集体行为数据上进行预训练，能够检测意外共现异常和此前被忽视的缺失异常。

Result: CoBAD在大规模出行数据集上的实验表明，显著优于现有异常检测基线，在AUCROC上提升13%-18%，在AUCPR上提升19%-70%。

Conclusion: CoBAD模型有效解决了人类出行集体异常检测的复杂性，通过建模个体间的时空依赖，不仅能够检测传统的共现异常，还能发现此前被忽视的缺失异常，并在实际应用中展现出显著优越的性能。

Abstract: Detecting anomalies in human mobility is essential for applications such as
public safety and urban planning. While traditional anomaly detection methods
primarily focus on individual movement patterns (e.g., a child should stay at
home at night), collective anomaly detection aims to identify irregularities in
collective mobility behaviors across individuals (e.g., a child is at home
alone while the parents are elsewhere) and remains an underexplored challenge.
Unlike individual anomalies, collective anomalies require modeling
spatiotemporal dependencies between individuals, introducing additional
complexity. To address this gap, we propose CoBAD, a novel model designed to
capture Collective Behaviors for human mobility Anomaly Detection. We first
formulate the problem as unsupervised learning over Collective Event Sequences
(CES) with a co-occurrence event graph, where CES represents the event
sequences of related individuals. CoBAD then employs a two-stage attention
mechanism to model both the individual mobility patterns and the interactions
across multiple individuals. Pre-trained on large-scale collective behavior
data through masked event and link reconstruction tasks, CoBAD is able to
detect two types of collective anomalies: unexpected co-occurrence anomalies
and absence anomalies, the latter of which has been largely overlooked in prior
work. Extensive experiments on large-scale mobility datasets demonstrate that
CoBAD significantly outperforms existing anomaly detection baselines, achieving
an improvement of 13%-18% in AUCROC and 19%-70% in AUCPR. All source code is
available at https://github.com/wenhaomin/CoBAD.

</details>


### [145] [Logical Expressivity and Explanations for Monotonic GNNs with Scoring Functions](https://arxiv.org/abs/2508.14091)
*Matthew Morris,David J. Tena Cucala,Bernardo Cuenca Grau*

Main category: cs.LG

TL;DR: 本文提出了一种将图神经网络（GNNs）和评分函数适配为单调的方法，从而为基于评分函数的链接预测任务提取可解释的Datalog规则，并在保持性能的同时获得许多可靠的规则。


<details>
  <summary>Details</summary>
Motivation: 现有从GNN中提取Datalog规则以解释链接预测的方法，仅限于一种受限的、低表达力的图编码/解码方式。然而，更通用和流行的方法是使用评分函数解码GNN输出进行事实预测。因此，需要研究如何在更通用的评分函数范式下，解决GNN在知识图谱上链接预测任务的可解释性问题。

Method: 作者将GNNs和评分函数改造为单调的，利用这种单调性提取用于解释预测的可靠规则，并利用现有关于评分函数可捕获规则类型的结果。同时，定义了为特定类别的单调GNNs及其评分函数获取等效Datalog程序的步骤。

Result: 实验结果表明，在链接预测基准测试中，单调GNNs和评分函数在实践中表现良好，并且能产生许多可靠的解释规则。

Conclusion: 研究表明，即使在更通用且流行的基于评分函数的链接预测方法中，通过引入单调性，也能从GNN中有效地提取出可解释的、可靠的Datalog规则，同时保持其预测性能。

Abstract: Graph neural networks (GNNs) are often used for the task of link prediction:
predicting missing binary facts in knowledge graphs (KGs). To address the lack
of explainability of GNNs on KGs, recent works extract Datalog rules from GNNs
with provable correspondence guarantees. The extracted rules can be used to
explain the GNN's predictions; furthermore, they can help characterise the
expressive power of various GNN models. However, these works address only a
form of link prediction based on a restricted, low-expressivity graph
encoding/decoding method. In this paper, we consider a more general and popular
approach for link prediction where a scoring function is used to decode the GNN
output into fact predictions. We show how GNNs and scoring functions can be
adapted to be monotonic, use the monotonicity to extract sound rules for
explaining predictions, and leverage existing results about the kind of rules
that scoring functions can capture. We also define procedures for obtaining
equivalent Datalog programs for certain classes of monotonic GNNs with scoring
functions. Our experiments show that, on link prediction benchmarks, monotonic
GNNs and scoring functions perform well in practice and yield many sound rules.

</details>


### [146] [Physics-Informed Reward Machines](https://arxiv.org/abs/2508.14093)
*Daniel Ajeleye,Ashutosh Trivedi,Majid Zamani*

Main category: cs.LG

TL;DR: 引入物理信息奖励机 (pRMs)，通过支持反事实经验和奖励塑形，显著提升强化学习在复杂任务中的学习效率和表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有奖励机 (RMs) 虽能提高强化学习 (RL) 的表达性和可编程性，但为应对更复杂的学习目标和奖励结构，并进一步提升RL的学习效率，本研究旨在引入物理信息奖励机 (pRMs)，以实现更可编程、表达性和高效的学习。

Method: ['引入物理信息奖励机 (pRMs)，一种旨在表达复杂学习目标和奖励结构的符号机器。', '开发能够通过反事实经验生成和奖励塑形来利用 pRMs 的强化学习算法。', '在有限和连续的物理环境中进行实验，以验证 pRMs 的表达性和有效性。']

Result: ['所提出的技术（pRMs结合反事实经验和奖励塑形）加速了强化学习训练阶段的奖励获取。', '实验证明了 pRMs 在有限和连续物理环境中的表达性和有效性。', '将 pRMs 整合到控制任务中显著提高了学习效率。']

Conclusion: 物理信息奖励机 (pRMs) 通过提供结构化的奖励指定方式，并支持反事实经验和奖励塑形等技术，能够显著提高强化学习在复杂控制任务中的学习效率、可编程性和表达能力。

Abstract: Reward machines (RMs) provide a structured way to specify non-Markovian
rewards in reinforcement learning (RL), thereby improving both expressiveness
and programmability. Viewed more broadly, they separate what is known about the
environment, captured by the reward mechanism, from what remains unknown and
must be discovered through sampling. This separation supports techniques such
as counterfactual experience generation and reward shaping, which reduce sample
complexity and speed up learning. We introduce physics-informed reward machines
(pRMs), a symbolic machine designed to express complex learning objectives and
reward structures for RL agents, thereby enabling more programmable,
expressive, and efficient learning. We present RL algorithms capable of
exploiting pRMs via counterfactual experiences and reward shaping. Our
experimental results show that these techniques accelerate reward acquisition
during the training phases of RL. We demonstrate the expressiveness and
effectiveness of pRMs through experiments in both finite and continuous
physical environments, illustrating that incorporating pRMs significantly
improves learning efficiency across several control tasks.

</details>


### [147] [Hard Examples Are All You Need: Maximizing GRPO Post-Training Under Annotation Budgets](https://arxiv.org/abs/2508.14094)
*Benjamin Pikus,Pratyush Ranjan Tiwari,Burton Ye*

Main category: cs.LG

TL;DR: 在语言模型微调预算有限的情况下，研究发现优先使用最困难的训练样本，通过GRPO方法，能显著提升模型性能（高达47%），为资源受限的后训练提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 语言模型微调的高质量训练数据获取成本昂贵，预算受限。研究旨在探讨在固定数据获取预算下，实践者应优先选择简单、中等、困难还是随机难度的样本，以实现资源受限下的最佳模型对齐。

Method: 本研究采用Group Relative Policy Optimization (GRPO) 微调方法，对不同模型尺寸和家族进行了实验。通过基模型的多样本评估获取难度估计，并比较了从同一未标记池中选择的四种子集选择策略（易、中、难、随机难度）。

Result: 实验结果显示，训练最困难的样本可带来最大的性能提升（高达47%），而训练简单样本的性能提升最小。分析表明，这是因为困难样本在GRPO训练过程中提供了更多的学习机会。

Conclusion: 在预算受限的后训练阶段，尤其是在使用GRPO时，优先选择和训练困难样本能够显著提高模型在推理任务上的性能，为实际应用提供了有效指导。

Abstract: Collecting high-quality training examples for language model fine-tuning is
expensive, with practical budgets limiting the amount of data that can be
procured. We investigate a critical question for resource-constrained
alignment: under a fixed acquisition budget, should practitioners prioritize
examples that are easy, medium, hard, or of random difficulty? We study Group
Relative Policy Optimization (GRPO) fine-tuning across different model sizes
and families, comparing four subset selection policies chosen from the same
unlabeled pool using base-model difficulty estimates obtained via multi-sample
evaluation. Our experiments reveal that training on the hardest examples yields
the largest performance gains, up to 47%, while training on easy examples yield
the smallest gains. Analysis reveals that this effect arises from harder
examples providing more learnable opportunities during GRPO training. These
findings provide practical guidance for budget-constrained post-training:
prioritizing hard examples yields substantial performance gains on reasoning
tasks when using GRPO.

</details>


### [148] [Implicit Hypergraph Neural Network](https://arxiv.org/abs/2508.14101)
*Akash Choudhuri,Yongjian Zhong,Bijaya Adhikari*

Main category: cs.LG

TL;DR: 现有超图神经网络（HNNs）难以捕获长距离高阶依赖且增加消息传递轮次会导致性能下降。本文提出一种新型隐式超图神经网络（IHNN），通过学习节点和超边的定点表示来解决此问题，并在节点分类任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 超图神经网络（HNNs）通常只进行少量消息传递轮次，导致其表示仅捕获局部信息，无法有效捕捉长距离高阶依赖。然而，盲目增加消息传递轮次又会降低其性能。尽管隐式图神经网络已在标准图上成功解决长距离依赖问题，但在超图神经网络中，此问题尚未得到深入研究。

Method: 本文首先证明现有超图神经网络在聚合更多信息以捕获长距离依赖时会损失预测能力。随后，提出隐式超图神经网络（IHNN），这是一个新颖的框架，通过端到端的方式联合学习节点和超边的定点表示来解决长距离依赖问题。为高效训练模型，IHNN利用隐式微分，引入了一种可行的投影梯度下降方法。

Result: 在多个真实世界超图上进行的节点分类任务的广泛实验表明，IHNN在大多数设置下均优于最接近的现有工作。

Conclusion: IHNN成功解决了超图神经网络中长距离高阶依赖的挑战，并在超图学习领域建立了新的最先进水平。

Abstract: Hypergraphs offer a generalized framework for capturing high-order
relationships between entities and have been widely applied in various domains,
including healthcare, social networks, and bioinformatics. Hypergraph neural
networks, which rely on message-passing between nodes over hyperedges to learn
latent representations, have emerged as the method of choice for predictive
tasks in many of these domains. These approaches typically perform only a small
number of message-passing rounds to learn the representations, which they then
utilize for predictions. The small number of message-passing rounds comes at a
cost, as the representations only capture local information and forego
long-range high-order dependencies. However, as we demonstrate, blindly
increasing the message-passing rounds to capture long-range dependency also
degrades the performance of hyper-graph neural networks.
  Recent works have demonstrated that implicit graph neural networks capture
long-range dependencies in standard graphs while maintaining performance.
Despite their popularity, prior work has not studied long-range dependency
issues on hypergraph neural networks. Here, we first demonstrate that existing
hypergraph neural networks lose predictive power when aggregating more
information to capture long-range dependency. We then propose Implicit
Hypergraph Neural Network (IHNN), a novel framework that jointly learns
fixed-point representations for both nodes and hyperedges in an end-to-end
manner to alleviate this issue. Leveraging implicit differentiation, we
introduce a tractable projected gradient descent approach to train the model
efficiently. Extensive experiments on real-world hypergraphs for node
classification demonstrate that IHNN outperforms the closest prior works in
most settings, establishing a new state-of-the-art in hypergraph learning.

</details>


### [149] [Beyond Fixed Morphologies: Learning Graph Policies with Trust Region Compensation in Variable Action Spaces](https://arxiv.org/abs/2508.14102)
*Thomas Gallien*

Main category: cs.LG

TL;DR: 本文通过理论分析和实证评估，研究了信赖域强化学习方法（TRPO/PPO）在动作空间维度变化下（与形态泛化相关）的行为及其对优化过程的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管信赖域优化方法在连续控制任务中表现稳定且性能优异，但其在动作空间维度变化（尤其是在形态泛化背景下，即策略需适应不同运动学结构）时的行为尚不明确，阻碍了可伸缩和可重用控制策略的开发。

Method: 研究采用理论分析方法，聚焦于信赖域策略优化（TRPO）及其一阶近似（PPO）。同时，辅以在Gymnasium Swimmer环境中进行的实证评估，该环境允许系统地改变运动学结构以研究形态泛化。

Result: 通过理论分析，旨在揭示动作空间维度变化如何影响优化过程，特别是在KL散度或策略裁剪惩罚的约束下。实证评估则在形态变化条件下验证了理论见解。

Conclusion: 本研究旨在深入理解信赖域方法在不同动作空间维度下的行为，为开发能适应多样化运动学结构的泛化控制策略提供理论基础和实证支持。

Abstract: Trust region-based optimization methods have become foundational
reinforcement learning algorithms that offer stability and strong empirical
performance in continuous control tasks. Growing interest in scalable and
reusable control policies translate also in a demand for morphological
generalization, the ability of control policies to cope with different
kinematic structures. Graph-based policy architectures provide a natural and
effective mechanism to encode such structural differences. However, while these
architectures accommodate variable morphologies, the behavior of trust region
methods under varying action space dimensionality remains poorly understood. To
this end, we conduct a theoretical analysis of trust region-based policy
optimization methods, focusing on both Trust Region Policy Optimization (TRPO)
and its widely used first-order approximation, Proximal Policy Optimization
(PPO). The goal is to demonstrate how varying action space dimensionality
influence the optimization landscape, particularly under the constraints
imposed by KL-divergence or policy clipping penalties. Complementing the
theoretical insights, an empirical evaluation under morphological variation is
carried out using the Gymnasium Swimmer environment. This benchmark offers a
systematically controlled setting for varying the kinematic structure without
altering the underlying task, making it particularly well-suited to study
morphological generalization.

</details>


### [150] [From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery](https://arxiv.org/abs/2508.14111)
*Jiaqi Wei,Yuejin Yang,Xiang Zhang,Yuhan Chen,Xiang Zhuang,Zhangyang Gao,Dongzhan Zhou,Guangshuai Wang,Zhiqiang Gao,Juntai Cao,Zijie Qiu,Xuming He,Qiang Zhang,Chenyu You,Shuangjia Zheng,Ning Ding,Wanli Ouyang,Nanqing Dong,Yu Cheng,Siqi Sun,Lei Bai,Bowen Zhou*

Main category: cs.LG

TL;DR: 本文综述了人工智能从辅助工具发展为自主研究伙伴的“智能体科学”范式，并构建了一个综合框架。


<details>
  <summary>Details</summary>
Motivation: 人工智能正在深刻改变科学发现，从单一的计算工具演变为自主的研究伙伴。本文旨在将“智能体科学”定义为AI赋能科学范式中的一个关键阶段，并探讨其实现全面科学智能体的潜力。

Method: 本文是一项领域导向的综述，通过统一过程导向、自主性导向和机制导向三种视角，提出了一个连接基础能力、核心过程和领域特定实现的综合框架。基于该框架，作者追溯了AI用于科学的演变，识别了支撑科学智能体的五项核心能力，将发现建模为动态的四阶段工作流，并综述了其在生命科学、化学、材料科学和物理学中的应用。

Result: 研究结果表明，智能体AI在假设生成、实验设计、执行、分析和迭代优化等方面展现出类人能力。本文构建了一个综合框架，识别了五项核心能力，将科学发现建模为四阶段工作流，并全面回顾了智能体AI在不同科学领域的应用，同时指出了关键挑战和未来机遇。

Conclusion: 这项工作对自主科学发现进行了领域导向的综合，并将智能体科学确立为一个结构化的范式，以推动AI驱动的科学研究进展。

Abstract: Artificial intelligence (AI) is reshaping scientific discovery, evolving from
specialized computational tools into autonomous research partners. We position
Agentic Science as a pivotal stage within the broader AI for Science paradigm,
where AI systems progress from partial assistance to full scientific agency.
Enabled by large language models (LLMs), multimodal systems, and integrated
research platforms, agentic AI shows capabilities in hypothesis generation,
experimental design, execution, analysis, and iterative refinement -- behaviors
once regarded as uniquely human. This survey provides a domain-oriented review
of autonomous scientific discovery across life sciences, chemistry, materials
science, and physics. We unify three previously fragmented perspectives --
process-oriented, autonomy-oriented, and mechanism-oriented -- through a
comprehensive framework that connects foundational capabilities, core
processes, and domain-specific realizations. Building on this framework, we (i)
trace the evolution of AI for Science, (ii) identify five core capabilities
underpinning scientific agency, (iii) model discovery as a dynamic four-stage
workflow, (iv) review applications across the above domains, and (v) synthesize
key challenges and future opportunities. This work establishes a
domain-oriented synthesis of autonomous scientific discovery and positions
Agentic Science as a structured paradigm for advancing AI-driven research.

</details>


### [151] [A Cost-Effective Framework for Predicting Parking Availability Using Geospatial Data and Machine Learning](https://arxiv.org/abs/2508.14125)
*Madyan Bagosher,Tala Mustafa,Mohammad Alsmirat,Amal Al-Ali,Isam Mashhour Al Jawarneh*

Main category: cs.LG

TL;DR: 本文提出一个无需传感器的智能框架，通过整合多源数据（如街道地图、出行和气象数据）和定位服务，预测城市（特别是大学校园）的停车行为。研究评估了多种机器学习模型，其中随机森林回归表现最佳，但指出长短期记忆网络(LSTM)在更多数据下有进一步提升的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着城市人口增长，停车管理和停车位占用率确定面临巨大挑战。在大学校园中，学生在课堂期间快速找到空闲停车位尤为困难，停车位有限，因此迫切需要一个高效系统来有效分配空闲停车位。

Method: 研究提出了一个智能框架，通过空间连接操作整合多源数据，包括街道地图、出行数据和气象数据，以捕获连续3天、每天7点至15点之间的停车行为和车辆移动模式。该系统不依赖任何物理传感器，所有数据均通过定位服务收集。框架利用预期的停车入口和时间来指定合适的停车区域。评估了线性回归、支持向量回归(SVR)、随机森林回归(RFR)和长短期记忆网络(LSTM)等多种预测模型。模型通过网格搜索进行超参数调优，性能评估指标包括均方根误差(RMSE)、平均绝对误差(MAE)和决定系数(R2)。

Result: 随机森林回归模型表现出最佳性能，其均方根误差(RMSE)为0.142，是所有模型中最低的；其决定系数(R2)为0.582，是所有模型中最高的。

Conclusion: 尽管随机森林回归在此次评估中表现最佳，但考虑到停车预测任务的时间序列性质，研究指出长短期记忆网络(LSTM)模型在获得更多数据和更长时间步长的情况下，可能会实现更优的性能。

Abstract: As urban populations continue to grow, cities face numerous challenges in
managing parking and determining occupancy. This issue is particularly
pronounced in university campuses, where students need to find vacant parking
spots quickly and conveniently during class timings. The limited availability
of parking spaces on campuses underscores the necessity of implementing
efficient systems to allocate vacant parking spots effectively. We propose a
smart framework that integrates multiple data sources, including street maps,
mobility, and meteorological data, through a spatial join operation to capture
parking behavior and vehicle movement patterns over the span of 3 consecutive
days with an hourly duration between 7AM till 3PM. The system will not require
any sensing tools to be installed in the street or in the parking area to
provide its services since all the data needed will be collected using location
services. The framework will use the expected parking entrance and time to
specify a suitable parking area. Several forecasting models, namely, Linear
Regression, Support Vector Regression (SVR), Random Forest Regression (RFR),
and Long Short-Term Memory (LSTM), are evaluated. Hyperparameter tuning was
employed using grid search, and model performance is assessed using Root Mean
Squared Error (RMSE), Mean Absolute Error (MAE) and Coefficient of
Determination (R2). Random Forest Regression achieved the lowest RMSE of 0.142
and highest R2 of 0.582. However, given the time-series nature of the task, an
LSTM model may perform better with additional data and longer timesteps.

</details>


### [152] [Comparison of derivative-free and gradient-based minimization for multi-objective compositional design of shape memory alloys](https://arxiv.org/abs/2508.14127)
*S. Josyula,Y. Noiman,E. J. Payton,T. Giovannelli*

Main category: cs.LG

TL;DR: 本研究结合机器学习（树模型与神经网络）和数值优化（COBYLA与TRUST-CONSTR），以实验数据为基础，优化形状记忆合金(SMA)成分以实现目标马氏体相变起始温度并降低成本。结果显示，神经网络结合梯度优化器TRUST-CONSTR表现更优。


<details>
  <summary>Details</summary>
Motivation: 设计高性能、经济且可持续的形状记忆合金(SMA)面临挑战，特别是需在满足特定性能（如马氏体相变起始温度Ms）的同时最小化成本。

Method: 利用机器学习模型（树集成模型和神经网络）作为替代预测器，结合数值优化方法（COBYLA和TRUST-CONSTR）搜索合金组合。模型通过实验数据和物理信息特征进行训练，其中树模型与无导数优化器COBYLA配对，神经网络与基于梯度的优化器TRUST-CONSTR配对。

Result: 两类机器学习模型预测Ms的精度相似。然而，与神经网络配对的TRUST-CONSTR优化器能更稳定地找到更优解，而COBYLA常收敛到次优结果。TRUST-CONSTR在达到同时满足温度和成本目标方面表现更佳。

Conclusion: 该研究展示了一种结合物理信息数据、机器学习和优化算法来探索新型SMA成分的实用方法。尽管数据集规模有限，但使用实验数据提高了预测可靠性。此方法可推广应用于其他数据有限且存在设计权衡的材料领域。

Abstract: Designing shape memory alloys (SMAs) that meet performance targets while
remaining affordable and sustainable is a complex challenge. In this work, we
focus on optimizing SMA compositions to achieve a desired martensitic start
temperature (Ms) while minimizing cost. To do this, we use machine learning
models as surrogate predictors and apply numerical optimization methods to
search for suitable alloy combinations. We trained two types of machine
learning models, a tree-based ensemble and a neural network, using a dataset of
experimentally characterized alloys and physics-informed features. The
tree-based model was used with a derivative-free optimizer (COBYLA), while the
neural network, which provides gradient information, was paired with a
gradient-based optimizer (TRUST-CONSTR). Our results show that while both
models predict Ms with similar accuracy, the optimizer paired with the neural
network finds better solutions more consistently. COBYLA often converged to
suboptimal results, especially when the starting guess was far from the target.
The TRUST-CONSTR method showed more stable behavior and was better at reaching
alloy compositions that met both objectives. This study demonstrates a
practical approach to exploring new SMA compositions by combining
physics-informed data, machine learning models, and optimization algorithms.
Although the scale of our dataset is smaller than simulation-based efforts, the
use of experimental data improves the reliability of the predictions. The
approach can be extended to other materials where design trade-offs must be
made with limited data.

</details>


### [153] [ERIS: An Energy-Guided Feature Disentanglement Framework for Out-of-Distribution Time Series Classification](https://arxiv.org/abs/2508.14134)
*Xin Wu,Fei Teng,Ji Zhang,Xingwang Li,Yuxuan Liang*

Main category: cs.LG

TL;DR: 针对时间序列分类中的OOD鲁棒性问题，本文提出ERIS框架，通过能量引导、权重正交和对抗训练实现特征解耦，有效提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类在处理分布外（OOD）数据时性能不佳，核心障碍在于模型将领域特定特征与标签相关特征混杂，导致虚假关联。现有特征解耦方法缺乏语义指导，无法有效分离通用特征。

Method: 本文提出端到端框架ERIS (Energy-Regularized Information for Shift-Robustness)，通过以下机制实现引导式特征解耦：1) 能量引导校准机制，为特征分离提供语义指导并实现模型自校准。2) 权重级正交策略，强制领域特定特征与标签相关特征之间结构独立。3) 辅助对抗训练机制，通过注入结构化扰动增强鲁棒性。

Result: 实验表明，ERIS在四个基准测试中平均提升了4.04%的准确率，超越了现有的最先进基线模型。

Conclusion: ERIS框架通过结合数学约束和语义指导，有效解决了时间序列分类在OOD数据上的鲁棒性问题，显著提升了模型的泛化能力。

Abstract: An ideal time series classification (TSC) should be able to capture invariant
representations, but achieving reliable performance on out-of-distribution
(OOD) data remains a core obstacle. This obstacle arises from the way models
inherently entangle domain-specific and label-relevant features, resulting in
spurious correlations. While feature disentanglement aims to solve this,
current methods are largely unguided, lacking the semantic direction required
to isolate truly universal features. To address this, we propose an end-to-end
Energy-Regularized Information for Shift-Robustness (\textbf{ERIS}) framework
to enable guided and reliable feature disentanglement. The core idea is that
effective disentanglement requires not only mathematical constraints but also
semantic guidance to anchor the separation process. ERIS incorporates three key
mechanisms to achieve this goal. Specifically, we first introduce an
energy-guided calibration mechanism, which provides crucial semantic guidance
for the separation, enabling the model to self-calibrate. Additionally, a
weight-level orthogonality strategy enforces structural independence between
domain-specific and label-relevant features, thereby mitigating their
interference. Moreover, an auxiliary adversarial training mechanism enhances
robustness by injecting structured perturbations. Experiments demonstrate that
ERIS improves upon state-of-the-art baselines by an average of 4.04% accuracy
across four benchmarks.

</details>


### [154] [Towards Agent-based Test Support Systems: An Unsupervised Environment Design Approach](https://arxiv.org/abs/2508.14135)
*Collins O. Ogbodo,Timothy J. Rogers,Mattia Dal Borgo,David J. Wagg*

Main category: cs.LG

TL;DR: 本文提出了一种基于智能体的决策支持框架，通过强化学习实现动态模态测试环境中传感器的自适应布局，以解决传统静态方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统模态测试设计方法是静态的，无法适应不断变化的测试参数或传感器配置，导致测试精度和适应性受损。

Method: 研究引入了一种基于智能体的决策支持框架，用于自适应传感器布局。该框架将问题建模为欠定部分可观测马尔可夫决策过程（POMDP），并通过双重课程学习策略训练一个通用型强化学习智能体。

Result: 通过在一个钢悬臂结构上的案例研究，验证了所提出方法在优化跨频率段传感器位置方面的有效性、鲁棒性及实际适用性。

Conclusion: 所提出的基于智能体的框架能够有效解决传统方法的局限性，在动态变化的模态测试环境中实现自适应且鲁棒的传感器布局，提升测试精度和适应性。

Abstract: Modal testing plays a critical role in structural analysis by providing
essential insights into dynamic behaviour across a wide range of engineering
industries. In practice, designing an effective modal test campaign involves
complex experimental planning, comprising a series of interdependent decisions
that significantly influence the final test outcome. Traditional approaches to
test design are typically static-focusing only on global tests without
accounting for evolving test campaign parameters or the impact of such changes
on previously established decisions, such as sensor configurations, which have
been found to significantly influence test outcomes. These rigid methodologies
often compromise test accuracy and adaptability. To address these limitations,
this study introduces an agent-based decision support framework for adaptive
sensor placement across dynamically changing modal test environments. The
framework formulates the problem using an underspecified partially observable
Markov decision process, enabling the training of a generalist reinforcement
learning agent through a dual-curriculum learning strategy. A detailed case
study on a steel cantilever structure demonstrates the efficacy of the proposed
method in optimising sensor locations across frequency segments, validating its
robustness and real-world applicability in experimental settings.

</details>


### [155] [Topological Data Analysis for Unsupervised Anomaly Detection and Customer Segmentation on Banking Data](https://arxiv.org/abs/2508.14136)
*Leonardo Aldo Alejandro Barberi,Linda Maria De Cave*

Main category: cs.LG

TL;DR: 本文利用拓扑数据分析（TDA）技术，对银行数据进行无监督异常检测和客户分群。


<details>
  <summary>Details</summary>
Motivation: 旨在通过发掘银行客户数据中的拓扑信息，发现有意义的模式，并为行业提供实用的可操作见解，解决无监督异常检测和客户分群需求。

Method: 采用拓扑数据分析（TDA）的先进技术，特别是Mapper算法和持久同源性（persistent homology），开发无监督程序。

Result: 所提出的框架能揭示银行数据中有意义的模式，并产生可操作的见解。

Conclusion: 该研究将抽象的拓扑学与现实生活中的行业用例相结合，为行业提供了有用的可操作见解。

Abstract: This paper introduces advanced techniques of Topological Data Analysis (TDA)
for unsupervised anomaly detection and customer segmentation in banking data.
Using the Mapper algorithm and persistent homology, we develop unsupervised
procedures that uncover meaningful patterns in customers' banking data by
exploiting topological information. The framework we present in this paper
yields actionable insights that combine the abstract mathematical subject of
topology with real-life use cases that are useful in industry.

</details>


### [156] [Learning to Learn the Macroscopic Fundamental Diagram using Physics-Informed and meta Machine Learning techniques](https://arxiv.org/abs/2508.14137)
*Amalie Roark,Serio Agriesti,Francisco Camara Pereira,Guido Cantelmo*

Main category: cs.LG

TL;DR: 该研究提出一个基于元学习的框架，结合物理信息神经网络，旨在解决宏观基本图（MFD）估计中环形检测器数据稀缺的问题，并在多城市数据上验证其在有限数据下的泛化能力和性能提升。


<details>
  <summary>Details</summary>
Motivation: 宏观基本图（MFD）在交通流描述和控制中有广泛应用，但其准确估计需要大量环形检测器数据，这在实际中往往难以满足，导致数据稀缺的挑战。

Method: 研究提出了一个利用元学习的框架，将其应用于一个专门用于MFD估计的定制多任务物理信息神经网络（Multi-Task Physics-Informed Neural Network）。该模型通过利用多城市数据进行训练和测试，旨在为其他具有不同检测器密度和拓扑结构的城市建模MFD。同时，该框架还与传统的迁移学习方法和现有非参数模型（FitFun）进行了对比验证，以证明其可迁移性。

Result: 实验结果表明，所提出的元学习框架在流量预测方面实现了显著的平均均方误差（MSE）改进，改进范围在约17500到36000之间（取决于所测试的环形检测器子集）。这表明该框架成功地在多样化的城市环境中实现了泛化，并在数据有限的城市中提升了性能。

Conclusion: 该研究证明了元学习在解决宏观基本图估计中数据稀缺问题方面的巨大潜力。在环形检测器数量有限的情况下，所提出的框架能够有效泛化并提高不同城市环境下的预测性能，为交通数据受限的应用场景提供了有效的解决方案。

Abstract: The Macroscopic Fundamental Diagram is a popular tool used to describe
traffic dynamics in an aggregated way, with applications ranging from traffic
control to incident analysis. However, estimating the MFD for a given network
requires large numbers of loop detectors, which is not always available in
practice. This article proposes a framework harnessing meta-learning, a
subcategory of machine learning that trains models to understand and adapt to
new tasks on their own, to alleviate the data scarcity challenge. The developed
model is trained and tested by leveraging data from multiple cities and
exploiting it to model the MFD of other cities with different shares of
detectors and topological structures. The proposed meta-learning framework is
applied to an ad-hoc Multi-Task Physics-Informed Neural Network, specifically
designed to estimate the MFD. Results show an average MSE improvement in flow
prediction ranging between ~ 17500 and 36000 (depending on the subset of loop
detectors tested). The meta-learning framework thus successfully generalizes
across diverse urban settings and improves performance on cities with limited
data, demonstrating the potential of using meta-learning when a limited number
of detectors is available. Finally, the proposed framework is validated against
traditional transfer learning approaches and tested with FitFun, a
non-parametric model from the literature, to prove its transferability.

</details>


### [157] [STAS: Spatio-Temporal Adaptive Computation Time for Spiking Transformers](https://arxiv.org/abs/2508.14138)
*Donghwa Kang,Doohyun Kim,Sang-Ki Ko,Jinkyu Lee,Brent ByungHoon Kang,Hyeongboo Baek*

Main category: cs.LG

TL;DR: 为解决尖峰神经网络（SNNs）高延迟问题，本文提出STAS框架，通过协同设计架构和计算策略，实现尖峰Transformer的时空自适应计算，显著降低能耗并提升准确率。


<details>
  <summary>Details</summary>
Motivation: 尖峰神经网络（SNNs）虽能效高，但因其多时间步操作导致高延迟和计算开销。现有动态计算方法零散。自适应计算时间（ACT）原则难以应用于基于SNN的视觉Transformer（ViTs），主要受限于其时间相似性前提不符及静态架构问题。

Method: 本文提出STAS（Spatio-Temporal Adaptive computation time for Spiking transformers）框架，该框架协同设计静态架构和动态计算策略。具体包括：引入集成脉冲补丁分割（I-SPS）模块以建立时间稳定性并创建统一输入表示；在此基础上，设计自适应脉冲自注意力（A-SSA）模块，实现在空间和时间轴上的二维令牌剪枝。

Result: STAS框架在尖峰Transformer架构上实现并于CIFAR-10、CIFAR-100和ImageNet数据集上进行验证。结果显示，能耗分别降低了高达45.9%、43.8%和30.1%，同时在准确率上超越了现有最先进模型。

Conclusion: STAS框架通过解决SNN-ViTs中时间相似性缺失和架构静态的问题，成功实现了时空自适应计算，显著降低了能耗并提升了模型性能，为高效SNN模型的开发提供了有效途径。

Abstract: Spiking neural networks (SNNs) offer energy efficiency over artificial neural
networks (ANNs) but suffer from high latency and computational overhead due to
their multi-timestep operational nature. While various dynamic computation
methods have been developed to mitigate this by targeting spatial, temporal, or
architecture-specific redundancies, they remain fragmented. While the
principles of adaptive computation time (ACT) offer a robust foundation for a
unified approach, its application to SNN-based vision Transformers (ViTs) is
hindered by two core issues: the violation of its temporal similarity
prerequisite and a static architecture fundamentally unsuited for its
principles. To address these challenges, we propose STAS (Spatio-Temporal
Adaptive computation time for Spiking transformers), a framework that
co-designs the static architecture and dynamic computation policy. STAS
introduces an integrated spike patch splitting (I-SPS) module to establish
temporal stability by creating a unified input representation, thereby solving
the architectural problem of temporal dissimilarity. This stability, in turn,
allows our adaptive spiking self-attention (A-SSA) module to perform
two-dimensional token pruning across both spatial and temporal axes.
Implemented on spiking Transformer architectures and validated on CIFAR-10,
CIFAR-100, and ImageNet, STAS reduces energy consumption by up to 45.9%, 43.8%,
and 30.1%, respectively, while simultaneously improving accuracy over SOTA
models.

</details>


### [158] [Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs](https://arxiv.org/abs/2508.14140)
*Orestis Konstantaropoulos,Stelios Manolis Smirnakis,Maria Papadopouli*

Main category: cs.LG

TL;DR: 受生物神经回路启发，本文提出G2GNet，一种具有稀疏模块化连接的ANN架构，结合动态稀疏训练和类赫布重连规则，在显著减少参数和计算量（最高75%稀疏度）的同时，在图像识别基准测试上实现了更高的精度（最高4.3%提升）。


<details>
  <summary>Details</summary>
Motivation: 生物神经电路（模块化、层级化、稀疏连接）在布线成本、功能特化和鲁棒性之间实现了高效权衡，为深度和大规模人工神经网络（ANN）设计提供了宝贵启示。研究动机在于探索小鼠视觉皮层中的功能连接模式（特别是群组间通信）如何指导ANN设计，以利用稀疏性来降低内存和计算需求，提高速度并增强泛化能力。

Method: 1. 引入G2GNet，一种新型ANN架构，在跨前馈层中施加稀疏、模块化的连接性，并首次将生物观测到的功能连接模式作为ANN设计的结构偏置。2. 通过动态稀疏训练（DST）机制补充这种静态结构偏置，该机制在训练过程中修剪并重新生长连接。3. 提出一种基于激活关联的类赫布（Hebbian-inspired）重连规则，借鉴生物可塑性原理。

Result: 1. G2GNet在标准视觉基准测试上实现了卓越的精度，尽管其参数量显著少于全连接模型。2. 实现了高达75%的稀疏度。3. 在Fashion-MNIST、CIFAR-10和CIFAR-100等基准测试上，精度提升高达4.3%。4. 以远低于稠密基线的计算量，性能优于稠密模型。

Conclusion: 将生物启发的稀疏模块化连接（如G2GNet）与动态训练机制相结合，可以构建出更高效、更高性能的人工神经网络，这验证了将神经科学原理应用于人工智能设计的巨大价值。

Abstract: The structure of biological neural circuits-modular, hierarchical, and
sparsely interconnected-reflects an efficient trade-off between wiring cost,
functional specialization, and robustness. These principles offer valuable
insights for artificial neural network (ANN) design, especially as networks
grow in depth and scale. Sparsity, in particular, has been widely explored for
reducing memory and computation, improving speed, and enhancing generalization.
Motivated by systems neuroscience findings, we explore how patterns of
functional connectivity in the mouse visual cortex-specifically,
ensemble-to-ensemble communication, can inform ANN design. We introduce G2GNet,
a novel architecture that imposes sparse, modular connectivity across
feedforward layers. Despite having significantly fewer parameters than fully
connected models, G2GNet achieves superior accuracy on standard vision
benchmarks. To our knowledge, this is the first architecture to incorporate
biologically observed functional connectivity patterns as a structural bias in
ANN design. We complement this static bias with a dynamic sparse training (DST)
mechanism that prunes and regrows edges during training. We also propose a
Hebbian-inspired rewiring rule based on activation correlations, drawing on
principles of biological plasticity. G2GNet achieves up to 75% sparsity while
improving accuracy by up to 4.3% on benchmarks, including Fashion-MNIST,
CIFAR-10, and CIFAR-100, outperforming dense baselines with far fewer
computations.

</details>


### [159] [Beyond Turing: Memory-Amortized Inference as a Foundation for Cognitive Computation](https://arxiv.org/abs/2508.14143)
*Xin Li*

Main category: cs.LG

TL;DR: 本文提出记忆摊销推理（MAI）框架，将智能建模为记忆中潜在循环的推理，而非从头计算，强调结构复用与记忆在智能中的核心作用，并探讨其对AGI的意义。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型（如梯度下降）未能捕捉智能的非遍历性特征，即智能源于对过往推理轨迹的结构化复用而非从头优化，这导致了计算瓶颈。

Method: 引入记忆摊销推理（MAI）作为形式化框架，将认知建模为对记忆中潜在循环的推理；通过结构复用编码归纳偏见，最小化熵；利用delta-同调理论，将皮层柱建模为局部推理算子。

Result: MAI为Mountcastle的普遍皮层算法提供了理论基础，并建立了MAI与强化学习（RL）之间的时间反演对偶性，指出MAI通过从记忆逆向重建潜在原因，有助于实现节能推理和解决当前AI的计算瓶颈。

Conclusion: MAI提供了一个统一的、生物学基础的智能理论，以结构、复用和记忆为核心，并对实现通用人工智能（AGI）具有深远意义。

Abstract: Intelligence is fundamentally non-ergodic: it emerges not from uniform
sampling or optimization from scratch, but from the structured reuse of prior
inference trajectories. We introduce Memory-Amortized Inference (MAI) as a
formal framework in which cognition is modeled as inference over latent cycles
in memory, rather than recomputation through gradient descent. MAI systems
encode inductive biases via structural reuse, minimizing entropy and enabling
context-aware, structure-preserving inference. This approach reframes cognitive
systems not as ergodic samplers, but as navigators over constrained latent
manifolds, guided by persistent topological memory. Through the lens of
delta-homology, we show that MAI provides a principled foundation for
Mountcastle's Universal Cortical Algorithm, modeling each cortical column as a
local inference operator over cycle-consistent memory states. Furthermore, we
establish a time-reversal duality between MAI and reinforcement learning:
whereas RL propagates value forward from reward, MAI reconstructs latent causes
backward from memory. This inversion paves a path toward energy-efficient
inference and addresses the computational bottlenecks facing modern AI. MAI
thus offers a unified, biologically grounded theory of intelligence based on
structure, reuse, and memory. We also briefly discuss the profound implications
of MAI for achieving artificial general intelligence (AGI).

</details>


### [160] [Noise Robust One-Class Intrusion Detection on Dynamic Graphs](https://arxiv.org/abs/2508.14192)
*Aleksei Liuliakov,Alexander Schulz,Luca Hermes,Barbara Hammer*

Main category: cs.LG

TL;DR: 本研究提出了一种概率版的时间图网络支持向量数据描述（TGN-SVDD）模型，旨在提高网络入侵检测在噪声数据输入下的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在网络入侵检测领域，如何应对受污染和噪声数据输入的鲁棒性是一个关键挑战。

Method: 本研究引入了TGN-SVDD模型的概率版本，通过预测每个网络事件的高斯分布参数来处理噪声。模型在经过合成噪声修改的CIC-IDS2017数据集上进行了实验。

Result: 与基线TGN-SVDD模型相比，该模型在检测性能上取得了显著改进，尤其是在噪声水平增加时表现更优。

Conclusion: 所提出的概率TGN-SVDD模型能有效解决噪声问题，提高了网络入侵检测的鲁棒性和准确性。

Abstract: In the domain of network intrusion detection, robustness against contaminated
and noisy data inputs remains a critical challenge. This study introduces a
probabilistic version of the Temporal Graph Network Support Vector Data
Description (TGN-SVDD) model, designed to enhance detection accuracy in the
presence of input noise. By predicting parameters of a Gaussian distribution
for each network event, our model is able to naturally address noisy
adversarials and improve robustness compared to a baseline model. Our
experiments on a modified CIC-IDS2017 data set with synthetic noise demonstrate
significant improvements in detection performance compared to the baseline
TGN-SVDD model, especially as noise levels increase.

</details>


### [161] [Reliability comparison of vessel trajectory prediction models via Probability of Detection](https://arxiv.org/abs/2508.14198)
*Zahra Rastin,Kathrin Donandt,Dirk Söffker*

Main category: cs.LG

TL;DR: 评估不同深度学习船舶轨迹预测模型在各种交通复杂度下的性能和可靠性，并引入概率检测分析来量化可靠性。


<details>
  <summary>Details</summary>
Motivation: 评估船舶轨迹预测（VTP）中不同深度学习方法的性能和可靠性，特别是在不同交通复杂度的场景下，以弥补现有VTP模型忽略交通复杂性和缺乏可靠性评估的不足。

Method: 使用概率检测分析量化模型在不同交通场景下的可靠性，超越传统的误差分布分析。所有模型均在根据预测范围内交通状况分类的测试样本上进行评估，并对每个类别获取性能指标和可靠性估计。

Result: 综合评估结果揭示了不同预测方法的优缺点，以及它们在不同预测时长（可保证安全预测）下的可靠性。

Conclusion: 研究结果有助于开发更可靠的船舶轨迹预测方法，从而提高未来内河航行的安全性和效率。

Abstract: This contribution addresses vessel trajectory prediction (VTP), focusing on
the evaluation of different deep learning-based approaches. The objective is to
assess model performance in diverse traffic complexities and compare the
reliability of the approaches. While previous VTP models overlook the specific
traffic situation complexity and lack reliability assessments, this research
uses a probability of detection analysis to quantify model reliability in
varying traffic scenarios, thus going beyond common error distribution
analyses. All models are evaluated on test samples categorized according to
their traffic situation during the prediction horizon, with performance metrics
and reliability estimates obtained for each category. The results of this
comprehensive evaluation provide a deeper understanding of the strengths and
weaknesses of the different prediction approaches, along with their reliability
in terms of the prediction horizon lengths for which safe forecasts can be
guaranteed. These findings can inform the development of more reliable vessel
trajectory prediction approaches, enhancing safety and efficiency in future
inland waterways navigation.

</details>


### [162] [Graph Concept Bottleneck Models](https://arxiv.org/abs/2508.14255)
*Haotian Xu,Tsui-Wei Weng,Lam M. Nguyen,Tengfei Ma*

Main category: cs.LG

TL;DR: 现有概念瓶颈模型(CBMs)忽略概念间关系。本文提出GraphCBMs，通过构建潜在概念图捕获这些关系，从而提升模型性能、可解释性及干预效果。


<details>
  <summary>Details</summary>
Motivation: 现有CBMs假设概念在给定标签下条件独立且相互隔离，未能捕获概念间固有的关联性（即改变一个概念会影响其相关概念），这限制了其对复杂概念关系的建模能力和潜在性能。

Method: 本文提出GraphCBMs，一种新型CBM变体。它通过构建潜在概念图来显式地建模和利用概念之间的关系，并将其与CBMs结合，旨在在保持可解释性的同时提升模型性能。

Result: 在真实图像分类任务中，GraphCBMs展示出以下优势：1) 在图像分类任务中性能更优，并提供更丰富的概念结构信息以增强可解释性；2) 能够利用潜在概念图进行更有效的概念干预；3) 在不同的训练和架构设置下表现出稳健的性能。

Conclusion: GraphCBMs通过有效建模概念间的内在关联性，成功克服了现有CBMs的局限性，在提升模型性能、提供更深层次可解释性以及实现更精确的概念干预方面均表现出色。

Abstract: Concept Bottleneck Models (CBMs) provide explicit interpretations for deep
neural networks through concepts and allow intervention with concepts to adjust
final predictions. Existing CBMs assume concepts are conditionally independent
given labels and isolated from each other, ignoring the hidden relationships
among concepts. However, the set of concepts in CBMs often has an intrinsic
structure where concepts are generally correlated: changing one concept will
inherently impact its related concepts. To mitigate this limitation, we propose
GraphCBMs: a new variant of CBM that facilitates concept relationships by
constructing latent concept graphs, which can be combined with CBMs to enhance
model performance while retaining their interpretability. Our experiment
results on real-world image classification tasks demonstrate Graph CBMs offer
the following benefits: (1) superior in image classification tasks while
providing more concept structure information for interpretability; (2) able to
utilize latent concept graphs for more effective interventions; and (3) robust
in performance across different training and architecture settings.

</details>


### [163] [Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2508.14285)
*Liyi Zhang,Jake Snell,Thomas L. Griffiths*

Main category: cs.LG

TL;DR: ABMLL是一种针对LoRA微调LLM的新方法，旨在提高模型的泛化能力和不确定性量化，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）使用低秩适应（LoRA）进行微调虽然经济高效，但其泛化能力（即对未见数据集的性能）通常不明确。现有通过上下文提示或元学习改进泛化的方法，因需要长上下文、保存参数副本或使用二阶梯度更新而计算和内存成本高昂。

Method: 本文提出Amortized Bayesian Meta-Learning for LoRA (ABMLL)。该方法基于小型模型的摊销贝叶斯元学习，并将其适应于LLM，同时保持计算效率。它重新定义了LoRA中的任务特定和全局参数，并引入了一组新的超参数来平衡重建精度和任务特定参数与全局参数的一致性。通过贝叶斯框架，ABMLL还改进了不确定性量化。

Result: ABMLL提供了有效的泛化能力，并可扩展到Llama3-8B等大型模型。得益于贝叶斯框架，它还提供了改进的不确定性量化。在Unified-QA和CrossFit数据集上的测试表明，ABMLL在准确性和预期校准误差方面均优于现有方法。

Conclusion: ABMLL为LLM的LoRA微调提供了一种计算高效且有效的解决方案，显著提升了模型的泛化能力和不确定性量化，并在基准测试中超越了现有方法。

Abstract: Fine-tuning large language models (LLMs) with low-rank adaptaion (LoRA) is a
cost-effective way to incorporate information from a specific dataset. However,
it is often unclear how well the fine-tuned LLM will generalize, i.e., how well
it will perform on unseen datasets. Methods have been proposed to improve
generalization by optimizing with in-context prompts, or by using meta-learning
to fine-tune LLMs. However, these methods are expensive in memory and
computation, requiring either long-context prompts or saving copies of
parameters and using second-order gradient updates. To address these
challenges, we propose Amortized Bayesian Meta-Learning for LoRA (ABMLL). This
method builds on amortized Bayesian meta-learning for smaller models, adapting
this approach to LLMs while maintaining its computational efficiency. We
reframe task-specific and global parameters in the context of LoRA and use a
set of new hyperparameters to balance reconstruction accuracy and the fidelity
of task-specific parameters to the global ones. ABMLL provides effective
generalization and scales to large models such as Llama3-8B. Furthermore, as a
result of using a Bayesian framework, ABMLL provides improved uncertainty
quantification. We test ABMLL on Unified-QA and CrossFit datasets and find that
it outperforms existing methods on these benchmarks in terms of both accuracy
and expected calibration error.

</details>


### [164] [GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation](https://arxiv.org/abs/2508.14302)
*Amirmohsen Sattarifard,Sepehr Lavasani,Ehsan Imani,Kunlin Zhang,Hanlin Xu,Fengyu Sun,Negar Hassanpour,Chao Gao*

Main category: cs.LG

TL;DR: 针对在边缘硬件上部署LLM的挑战，本文提出A/I-GLASS，一种无需训练的动态剪枝方法，通过聚合局部提示和全局模型统计信息，在长文本生成场景中显著优于现有方法且无额外开销。


<details>
  <summary>Details</summary>
Motivation: 在边缘硬件上部署大型语言模型（LLMs）需要高效的、对提示敏感的动态剪枝技术来减少计算量并保持模型质量。然而，现有方法（如静态剪枝或基于预测器的方法）存在局限性，例如固定稀疏模式或引入额外运行时开销；而近期零样本方法在短提示或长生成场景下表现不佳。

Method: 本文提出A/I-GLASS，即基于激活和影响的全局-局部神经重要性聚合方法，是两种无需训练的动态剪枝技术。该方法通过对提示局部和模型内在全局神经元统计数据进行排序聚合，动态选择前馈网络（FFN）单元进行稀疏化。

Result: 经验结果表明，在多个LLM和基准测试中，GLASS显著优于先前的无训练方法，尤其在具有挑战性的长文本生成场景中表现突出，且无需依赖辅助预测器或增加任何推理开销。

Conclusion: A/I-GLASS成功解决了LLM在边缘设备上动态剪枝的挑战，提供了一种无需训练、高效且性能优越的解决方案，特别适用于对计算资源敏感且需要处理长文本生成的应用场景。

Abstract: Deploying Large Language Models (LLMs) on edge hardware demands aggressive,
prompt-aware dynamic pruning to reduce computation without degrading quality.
Static or predictor-based schemes either lock in a single sparsity pattern or
incur extra runtime overhead, and recent zero-shot methods that rely on
statistics from a single prompt fail on short prompt and/or long generation
scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local
neural importance Aggregation for feed-forward network SparSification, two
training-free methods that dynamically select FFN units using a
rank-aggregation of prompt local and model-intrinsic global neuron statistics.
Empirical results across multiple LLMs and benchmarks demonstrate that GLASS
significantly outperforms prior training-free methods, particularly in
challenging long-form generation scenarios, without relying on auxiliary
predictors or adding any inference overhead.

</details>


### [165] [Learning Time-Varying Convexifications of Multiple Fairness Measures](https://arxiv.org/abs/2508.14311)
*Quan Zhou,Jakub Marecek,Robert Shorten*

Main category: cs.LG

TL;DR: 研究在有限图结构反馈下，动态学习多重公平性度量（如群体和个体公平性）时变凸化的方法。


<details>
  <summary>Details</summary>
Motivation: 随着对多重公平性度量（包括群体和个体公平性）的日益重视，其相对权重是先验未知且可能时变的，需要实时学习。

Method: 本文考虑在有限图结构反馈的条件下，学习多重公平性度量的时变凸化。

Result: 抽象中未提及具体研究结果。

Conclusion: 抽象中未提及具体研究结论。

Abstract: There is an increasing appreciation that one may need to consider multiple
measures of fairness, e.g., considering multiple group and individual fairness
notions. The relative weights of the fairness regularisers are a priori
unknown, may be time varying, and need to be learned on the fly. We consider
the learning of time-varying convexifications of multiple fairness measures
with limited graph-structured feedback.

</details>


### [166] [Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS](https://arxiv.org/abs/2508.14313)
*Can Jin,Yang Zhou,Qixin Zhang,Hongwu Peng,Di Zhang,Marco Pavone,Ligong Han,Zhang-Wei Hong,Tong Che,Dimitris N. Metaxas*

Main category: cs.LG

TL;DR: AIRL-S统一了LLM测试时尺度扩展中的RL和搜索范式，通过对抗逆强化学习从正确推理轨迹中学习动态奖励模型，无需人工标注，显著提升了LLM在复杂推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: LLM测试时尺度扩展（TTS）现有方法存在局限：RL方法（如不稳定性、样本效率低）和基于搜索的方法（如需要昂贵标注、易受分布偏移影响）。目前缺乏一种能自然统一这两种范式的方法。

Method: 本文提出了AIRL-S，核心思想是RL训练中学习到的奖励函数可作为指导下游搜索的理想过程奖励模型（PRM）。具体来说，AIRL-S结合对抗逆强化学习（AIRL）和组相对策略优化（GRPO），直接从正确推理轨迹中学习密集、动态的PRM，完全消除了对标注中间过程数据的需求。在推理时，该PRM既作为RL探索的评论器，也作为引导搜索过程的启发式方法。

Result: 在数学、科学推理和代码生成等八个基准测试中，AIRL-S的统一方法相比基础模型平均性能提升9%，与GPT-4o表现相当。此外，该PRM在集成到多种搜索算法中时，持续优于所有使用标注数据训练的基线PRM。

Conclusion: 研究结果强调，用于RL的奖励函数确实是用于搜索的最佳PRM，为LLM中的复杂推理任务提供了一种鲁棒且经济高效的解决方案。AIRL-S成功地统一了RL和搜索范式，并展现出优越的性能和泛化能力。

Abstract: Test-time scaling (TTS) for large language models (LLMs) has thus far fallen
into two largely separate paradigms: (1) reinforcement learning (RL) methods
that optimize sparse outcome-based rewards, yet suffer from instability and low
sample efficiency; and (2) search-based techniques guided by independently
trained, static process reward models (PRMs), which require expensive human- or
LLM-generated labels and often degrade under distribution shifts. In this
paper, we introduce AIRL-S, the first natural unification of RL-based and
search-based TTS. Central to AIRL-S is the insight that the reward function
learned during RL training inherently represents the ideal PRM for guiding
downstream search. Specifically, we leverage adversarial inverse reinforcement
learning (AIRL) combined with group relative policy optimization (GRPO) to
learn a dense, dynamic PRM directly from correct reasoning traces, entirely
eliminating the need for labeled intermediate process data. At inference, the
resulting PRM simultaneously serves as the critic for RL rollouts and as a
heuristic to effectively guide search procedures, facilitating robust reasoning
chain extension, mitigating reward hacking, and enhancing cross-task
generalization. Experimental results across eight benchmarks, including
mathematics, scientific reasoning, and code generation, demonstrate that our
unified approach improves performance by 9 % on average over the base model,
matching GPT-4o. Furthermore, when integrated into multiple search algorithms,
our PRM consistently outperforms all baseline PRMs trained with labeled data.
These results underscore that, indeed, your reward function for RL is your best
PRM for search, providing a robust and cost-effective solution to complex
reasoning tasks in LLMs.

</details>


### [167] [FedRAIN-Lite: Federated Reinforcement Algorithms for Improving Idealised Numerical Weather and Climate Models](https://arxiv.org/abs/2508.14315)
*Pritthijit Nath,Sebastian Schemm,Henry Moss,Peter Haynes,Emily Shuckburgh,Mark Webb*

Main category: cs.LG

TL;DR: 引入FedRAIN-Lite联邦强化学习框架，通过地理分解实现气候模型子网格参数的在线自适应学习。DDPG算法表现卓越，为构建可适应气候变化的在线学习气候模型提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 传统气候模型中的子网格参数化是静态且离线调优的，这限制了模型对不断变化气候状态的适应能力。

Method: 提出FedRAIN-Lite联邦强化学习(FedRL)框架，通过将智能体分配到纬度带，模拟通用环流模型(GCMs)的空间分解，实现局部参数学习与周期性全局聚合。在多层级简化能量平衡气候模型（ebm-v1至ebm-v3）上，对三种RL算法在不同FedRL配置下进行了基准测试。

Result: 深度确定性策略梯度(DDPG)算法在ebm-v2和ebm-v3设置中，持续优于静态和单智能体基线，展现出更快的收敛速度和更低的面加权RMSE（尤其在热带和中纬度区域）。DDPG具备跨超参数迁移能力且计算成本低，非常适用于地理自适应参数学习。

Conclusion: 该框架为构建高复杂度通用环流模型提供了可扩展路径，并为开发能够随气候变化而演进的、物理对齐的在线学习气候模型提供了原型。

Abstract: Sub-grid parameterisations in climate models are traditionally static and
tuned offline, limiting adaptability to evolving states. This work introduces
FedRAIN-Lite, a federated reinforcement learning (FedRL) framework that mirrors
the spatial decomposition used in general circulation models (GCMs) by
assigning agents to latitude bands, enabling local parameter learning with
periodic global aggregation. Using a hierarchy of simplified energy-balance
climate models, from a single-agent baseline (ebm-v1) to multi-agent ensemble
(ebm-v2) and GCM-like (ebm-v3) setups, we benchmark three RL algorithms under
different FedRL configurations. Results show that Deep Deterministic Policy
Gradient (DDPG) consistently outperforms both static and single-agent
baselines, with faster convergence and lower area-weighted RMSE in tropical and
mid-latitude zones across both ebm-v2 and ebm-v3 setups. DDPG's ability to
transfer across hyperparameters and low computational cost make it well-suited
for geographically adaptive parameter learning. This capability offers a
scalable pathway towards high-complexity GCMs and provides a prototype for
physically aligned, online-learning climate models that can evolve with a
changing climate. Code accessible at
https://github.com/p3jitnath/climate-rl-fedrl.

</details>


### [168] [Multi-view Graph Condensation via Tensor Decomposition](https://arxiv.org/abs/2508.14330)
*Nícolas Roque dos Santos,Dawon Ahn,Diego Minatel,Alneu de Andrade Lopes,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 本文提出了一种基于张量分解的新型图凝聚方法GCTD，旨在解决大规模图神经网络训练的计算挑战，并提升模型性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在实际应用中表现出色，但其在大规模图上的训练面临巨大的计算资源挑战。现有图凝聚方法虽然能降低资源需求，但普遍依赖计算密集型的双层优化，且缺乏合成节点与原始节点间的映射关系，限制了模型可解释性。分解技术具有透明和低资源消耗的优点，但尚未应用于图凝聚。

Method: 本文提出了一种名为“基于张量分解的多视图图凝聚”（Multi-view Graph Condensation via Tensor Decomposition, GCTD）的新方法。GCTD旨在利用张量分解技术，从原始图数据中合成一个信息更丰富、规模更小的图，同时确保保持GNN在下游任务中的预测性能。

Result: 在六个真实世界数据集上的广泛实验表明，GCTD能够有效减小图的规模，同时保持GNN的性能。在六个数据集中的三个上，GCTD的准确率提升高达4.0%。与现有方法相比，GCTD在大规模图上展现出有竞争力的性能。

Conclusion: GCTD的成功表明，张量分解是一种有效且可行的图凝聚方法，它不仅能够应对大规模GNN训练的计算挑战，还能在保持甚至提升模型性能的同时，克服现有方法的局限性。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable results in various
real-world applications, including drug discovery, object detection, social
media analysis, recommender systems, and text classification. In contrast to
their vast potential, training them on large-scale graphs presents significant
computational challenges due to the resources required for their storage and
processing. Graph Condensation has emerged as a promising solution to reduce
these demands by learning a synthetic compact graph that preserves the
essential information of the original one while maintaining the GNN's
predictive performance. Despite their efficacy, current graph condensation
approaches frequently rely on a computationally intensive bi-level
optimization. Moreover, they fail to maintain a mapping between synthetic and
original nodes, limiting the interpretability of the model's decisions. In this
sense, a wide range of decomposition techniques have been applied to learn
linear or multi-linear functions from graph data, offering a more transparent
and less resource-intensive alternative. However, their applicability to graph
condensation remains unexplored. This paper addresses this gap and proposes a
novel method called Multi-view Graph Condensation via Tensor Decomposition
(GCTD) to investigate to what extent such techniques can synthesize an
informative smaller graph and achieve comparable downstream task performance.
Extensive experiments on six real-world datasets demonstrate that GCTD
effectively reduces graph size while preserving GNN performance, achieving up
to a 4.0\ improvement in accuracy on three out of six datasets and competitive
performance on large graphs compared to existing approaches. Our code is
available at https://anonymous.4open.science/r/gctd-345A.

</details>


### [169] [NeRC: Neural Ranging Correction through Differentiable Moving Horizon Location Estimation](https://arxiv.org/abs/2508.14336)
*Xu Weng,K. V. Ling,Haochen Liu,Bingheng Wang,Kun Cao*

Main category: cs.LG

TL;DR: 本文提出一个端到端的神经测距校正（NeRC）框架，通过利用地面真实位置或欧氏距离场（EDF）代价图训练神经网络来纠正城市环境下移动设备GNSS定位中的测距误差，显著提升了定位精度并实现了实时性能。


<details>
  <summary>Details</summary>
Motivation: 在城市环境中，日常移动设备的GNSS定位面临挑战，复杂信号传播和低质量GNSS硬件导致的测距误差严重影响定位精度。数据驱动方法虽有望解决此问题，但测距误差的繁琐标注阻碍了其发展。

Method: 本文提出一个鲁棒的端到端神经测距校正（NeRC）框架。该框架以定位相关指标作为神经网络的训练目标，并利用相对易于获取的地面真实位置进行训练，避免了直接标注测距误差。通过可微分移动视界位置估计（MHE）来处理一系列测量数据并反向传播梯度以支持训练。此外，还引入了基于欧氏距离场（EDF）代价图的新训练范式，进一步降低了对标注位置的需求。

Result: 所提出的NeRC框架在公开基准和自采数据集上进行了评估，结果表明其在定位精度方面有显著提升。同时，NeRC在边缘设备上的部署也验证了其对移动设备的实时性能。

Conclusion: NeRC框架通过创新的端到端学习方法，解决了城市环境下移动设备GNSS定位中测距误差校正的标注难题，显著提高了定位精度，并具备实时应用能力。

Abstract: GNSS localization using everyday mobile devices is challenging in urban
environments, as ranging errors caused by the complex propagation of satellite
signals and low-quality onboard GNSS hardware are blamed for undermining
positioning accuracy. Researchers have pinned their hopes on data-driven
methods to regress such ranging errors from raw measurements. However, the
grueling annotation of ranging errors impedes their pace. This paper presents a
robust end-to-end Neural Ranging Correction (NeRC) framework, where
localization-related metrics serve as the task objective for training the
neural modules. Instead of seeking impractical ranging error labels, we train
the neural network using ground-truth locations that are relatively easy to
obtain. This functionality is supported by differentiable moving horizon
location estimation (MHE) that handles a horizon of measurements for
positioning and backpropagates the gradients for training. Even better, as a
blessing of end-to-end learning, we propose a new training paradigm using
Euclidean Distance Field (EDF) cost maps, which alleviates the demands on
labeled locations. We evaluate the proposed NeRC on public benchmarks and our
collected datasets, demonstrating its distinguished improvement in positioning
accuracy. We also deploy NeRC on the edge to verify its real-time performance
for mobile devices.

</details>


### [170] [On the Interplay between Graph Structure and Learning Algorithms in Graph Neural Networks](https://arxiv.org/abs/2508.14338)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 本文研究图结构、GNNs与学习算法间的相互作用，关注GNN在有噪声情况下的泛化性能，并揭示其与图结构、超平滑问题之间的耦合关系。


<details>
  <summary>Details</summary>
Motivation: 现有GNN学习动态的理论研究主要集中于无噪声条件下的收敛率，且图结构与学习动态的关联性表述粗略。本文旨在通过研究GNN在有噪声（泛化）条件下的过量风险（泛化性能），弥补这一研究空白。

Method: 将传统学习理论设定扩展到GNNs，利用谱图理论推导随机梯度下降（SGD）和岭回归的过量风险剖面，并将其与图结构联系起来。通过比较分析，探讨不同图结构（规则图与幂律图）对算法性能的影响，并将分析扩展到多层线性GNNs。

Result: 推导了SGD和岭回归在GNNs中的过量风险剖面，并利用谱图理论将其与图结构相关联。研究表明不同图结构会影响算法性能。对多层线性GNNs的分析揭示了过量风险剖面日益增强的非各向同性效应，为GNN的超平滑问题提供了新见解。经验结果与理论预测一致。

Conclusion: 研究成果共同揭示了图结构、GNNs和学习算法之间存在的耦合关系，为GNN算法的实际设计和选择提供了重要见解。

Abstract: This paper studies the interplay between learning algorithms and graph
structure for graph neural networks (GNNs). Existing theoretical studies on the
learning dynamics of GNNs primarily focus on the convergence rates of learning
algorithms under the interpolation regime (noise-free) and offer only a crude
connection between these dynamics and the actual graph structure (e.g., maximum
degree). This paper aims to bridge this gap by investigating the excessive risk
(generalization performance) of learning algorithms in GNNs within the
generalization regime (with noise). Specifically, we extend the conventional
settings from the learning theory literature to the context of GNNs and examine
how graph structure influences the performance of learning algorithms such as
stochastic gradient descent (SGD) and Ridge regression. Our study makes several
key contributions toward understanding the interplay between graph structure
and learning in GNNs. First, we derive the excess risk profiles of SGD and
Ridge regression in GNNs and connect these profiles to the graph structure
through spectral graph theory. With this established framework, we further
explore how different graph structures (regular vs. power-law) impact the
performance of these algorithms through comparative analysis. Additionally, we
extend our analysis to multi-layer linear GNNs, revealing an increasing
non-isotropic effect on the excess risk profile, thereby offering new insights
into the over-smoothing issue in GNNs from the perspective of learning
algorithms. Our empirical results align with our theoretical predictions,
\emph{collectively showcasing a coupling relation among graph structure, GNNs
and learning algorithms, and providing insights on GNN algorithm design and
selection in practice.}

</details>


### [171] [A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations](https://arxiv.org/abs/2508.14340)
*Konur Tholl,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.LG

TL;DR: 本研究通过在模拟网络环境中引入教师引导技术，显著提升了自主网络操作（ACO）中强化学习智能体的训练效率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有的自主网络操作（ACO）应用中，强化学习智能体需从零开始学习，导致收敛缓慢和早期性能不佳。尽管教师引导技术在其他领域已展现出潜力，但尚未应用于ACO。

Method: 本研究在模拟的CybORG环境中，实现了四种不同的教师引导技术，并进行了比较评估。

Result: 研究结果表明，整合教师引导技术能显著提高训练效率，具体体现在早期策略性能和收敛速度的提升。

Conclusion: 教师引导技术在自主网络安全领域具有巨大潜力，能够有效提升强化学习智能体的训练效率。

Abstract: Autonomous Cyber Operations (ACO) rely on Reinforcement Learning (RL) to
train agents to make effective decisions in the cybersecurity domain. However,
existing ACO applications require agents to learn from scratch, leading to slow
convergence and poor early-stage performance. While teacher-guided techniques
have demonstrated promise in other domains, they have not yet been applied to
ACO. In this study, we implement four distinct teacher-guided techniques in the
simulated CybORG environment and conduct a comparative evaluation. Our results
demonstrate that teacher integration can significantly improve training
efficiency in terms of early policy performance and convergence speed,
highlighting its potential benefits for autonomous cybersecurity.

</details>


### [172] [DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization](https://arxiv.org/abs/2508.14460)
*Shuaijie She,Yu Bao,Yu Lu,Lu Xu,Tao Li,Wenhao Zhu,Shujian Huang,Shanbo Cheng,Lu Lu,Yuxuan Wang*

Main category: cs.LG

TL;DR: DuPO是一种基于广义对偶学习的偏好优化框架，无需人工标注即可生成反馈。它解决了现有方法对昂贵标注的依赖和对偶任务限制，并在翻译、数学推理和重排序等多种任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法（如RLVR）需要大量昂贵标注且仅适用于可验证任务；传统的对偶学习仅限于严格的对偶任务对。本研究旨在克服这些局限性，提供一种更通用、无需标注的优化范式。

Method: DuPO将原始任务的输入分解为已知和未知部分，并构建对偶任务来利用原始任务的输出和已知信息重建未知部分（例如，逆转数学解决方案以恢复隐藏变量）。这种重建的质量作为自监督奖励来优化原始任务，并通过大型语言模型实现两种任务的实例化。

Result: DuPO在多个任务中均取得显著进展：翻译质量平均提升2.13 COMET（756个方向），数学推理准确率在三个基准测试上平均提升6.4点，作为推理时重排序器性能提升9.3点。

Conclusion: DuPO作为一种可扩展、通用且无需标注的大型语言模型优化范式，具有广阔的应用前景。

Abstract: We present DuPO, a dual learning-based preference optimization framework that
generates annotation-free feedback via a generalized duality. DuPO addresses
two key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s
reliance on costly labels and applicability restricted to verifiable tasks, and
traditional dual learning's restriction to strictly dual task pairs (e.g.,
translation and back-translation). Specifically, DuPO decomposes a primal
task's input into known and unknown components, then constructs its dual task
to reconstruct the unknown part using the primal output and known information
(e.g., reversing math solutions to recover hidden variables), broadening
applicability to non-invertible tasks. The quality of this reconstruction
serves as a self-supervised reward to optimize the primal task, synergizing
with LLMs' ability to instantiate both tasks via a single model. Empirically,
DuPO achieves substantial gains across diverse tasks: it enhances the average
translation quality by 2.13 COMET over 756 directions, boosts the mathematical
reasoning accuracy by an average of 6.4 points on three challenge benchmarks,
and enhances performance by 9.3 points as an inference-time reranker (trading
computation for accuracy). These results position DuPO as a scalable, general,
and annotation-free paradigm for LLM optimization.

</details>


### [173] [Generative AI Against Poaching: Latent Composite Flow Matching for Wildlife Conservation](https://arxiv.org/abs/2508.14342)
*Lingkai Kong,Haichuan Wang,Charles A. Emogor,Vincent Börsch-Supan,Lily Xu,Milind Tambe*

Main category: cs.LG

TL;DR: 本文提出了一种结合流匹配与基于占用模型的生成方法，用于预测偷猎行为，旨在解决现有方法在处理复杂时空模式、偷猎事件检测不完善和数据稀缺性方面的局限性，并在实际数据上展示了更高的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 偷猎对野生动物和生物多样性构成严重威胁。预测偷猎者行为能有效指导巡逻规划和保护干预措施。现有基于线性模型或决策树的偷猎预测方法无法捕捉复杂、非线性的时空模式，需要更灵活的替代方案。

Method: 利用生成模型（特别是流匹配）作为核心方法。为解决检测不完善问题，将流匹配与基于占用的检测模型结合，并在潜在空间训练流以推断底层占用状态。为缓解数据稀缺问题，采用复合流，其初始化并非标准扩散模型中的随机噪声，而是来自线性模型预测，以注入先验知识并提高泛化能力。

Result: 在乌干达两个国家公园的数据集上进行评估，结果显示预测准确性持续提高。

Conclusion: 所提出的结合了流匹配与占用模型的生成方法，有效地解决了真实世界偷猎数据中检测不完善和数据稀缺两大挑战，显著提升了偷猎行为的预测准确性，为野生动物保护提供了更有效的工具。

Abstract: Poaching poses significant threats to wildlife and biodiversity. A valuable
step in reducing poaching is to forecast poacher behavior, which can inform
patrol planning and other conservation interventions. Existing poaching
prediction methods based on linear models or decision trees lack the
expressivity to capture complex, nonlinear spatiotemporal patterns. Recent
advances in generative modeling, particularly flow matching, offer a more
flexible alternative. However, training such models on real-world poaching data
faces two central obstacles: imperfect detection of poaching events and limited
data. To address imperfect detection, we integrate flow matching with an
occupancy-based detection model and train the flow in latent space to infer the
underlying occupancy state. To mitigate data scarcity, we adopt a composite
flow initialized from a linear-model prediction rather than random noise which
is the standard in diffusion models, injecting prior knowledge and improving
generalization. Evaluations on datasets from two national parks in Uganda show
consistent gains in predictive accuracy.

</details>


### [174] [A Non-Asymptotic Convergent Analysis for Scored-Based Graph Generative Model via a System of Stochastic Differential Equations](https://arxiv.org/abs/2508.14351)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 首次对基于分数的图生成模型（SGGMs）进行了非渐近收敛性分析，揭示了影响收敛的独特因素，并提供了实践指导。


<details>
  <summary>Details</summary>
Motivation: SGGMs在应用中有效，但其理论行为（特别是收敛性）未被充分探索。与常见基于分数的生成模型（SGMs）不同，SGGMs涉及耦合随机微分方程（SDEs），导致现有SGM的收敛性分析不适用于SGGMs。

Method: 本研究提出了SGGMs的首次非渐近收敛性分析，重点关注在三种关键图生成范式（固定图结构下的特征生成、固定节点特征下的图结构生成、图结构与节点特征联合生成）中的收敛界限（生成误差风险）。

Result: 分析揭示了SGGMs特有的影响收敛界限的因素（如图结构的拓扑性质）。研究提供了超参数选择（如采样步数和扩散长度）的理论见解，并倡导使用归一化等技术来改善收敛性。通过合成图模型的受控实证研究验证了理论发现。

Conclusion: 本工作深化了对SGGMs的理论理解，展示了其在关键领域的适用性，并为设计有效的SGGMs提供了实践指导。

Abstract: Score-based graph generative models (SGGMs) have proven effective in critical
applications such as drug discovery and protein synthesis. However, their
theoretical behavior, particularly regarding convergence, remains
underexplored. Unlike common score-based generative models (SGMs), which are
governed by a single stochastic differential equation (SDE), SGGMs involve a
system of coupled SDEs. In SGGMs, the graph structure and node features are
governed by separate but interdependent SDEs. This distinction makes existing
convergence analyses from SGMs inapplicable for SGGMs. In this work, we present
the first non-asymptotic convergence analysis for SGGMs, focusing on the
convergence bound (the risk of generative error) across three key graph
generation paradigms: (1) feature generation with a fixed graph structure, (2)
graph structure generation with fixed node features, and (3) joint generation
of both graph structure and node features. Our analysis reveals several unique
factors specific to SGGMs (e.g., the topological properties of the graph
structure) which affect the convergence bound. Additionally, we offer
theoretical insights into the selection of hyperparameters (e.g., sampling
steps and diffusion length) and advocate for techniques like normalization to
improve convergence. To validate our theoretical findings, we conduct a
controlled empirical study using synthetic graph models, and the results align
with our theoretical predictions. This work deepens the theoretical
understanding of SGGMs, demonstrates their applicability in critical domains,
and provides practical guidance for designing effective models.

</details>


### [175] [SBGD: Improving Graph Diffusion Generative Model via Stochastic Block Diffusion](https://arxiv.org/abs/2508.14352)
*Junwei Su,Shan Wu*

Main category: cs.LG

TL;DR: 本文提出随机块图扩散模型（SBGD），通过将图表示精炼到块图空间，显著提升了图扩散生成模型（GDGMs）的可扩展性和尺寸泛化能力，同时保持或超越了现有性能。


<details>
  <summary>Details</summary>
Motivation: 现有图扩散生成模型（GDGMs）面临可扩展性和尺寸泛化能力差的挑战。它们通常在全图空间操作，导致高内存需求，难以处理大型图；同时，生成与训练数据不同尺寸图的能力有限，限制了其应用范围。

Method: 本文提出了随机块图扩散（SBGD）模型。该模型将图表示精炼到块图空间，通过引入基于真实世界图模式的结构先验，显著降低了内存复杂度。块表示法还有助于捕获基本图结构，从而改善尺寸泛化能力。

Result: 实验结果表明，SBGD在保持与最先进方法相当或更优的图生成性能的同时，实现了显著的内存改进（高达6倍）。此外，SBGD在泛化到未见过的图尺寸方面表现更佳。

Conclusion: SBGD不仅是一个可扩展且高效的图扩散生成模型，更体现了生成建模中的模块化原则，为通过将复杂任务分解为更易管理的组件来探索生成模型提供了新途径。

Abstract: Graph diffusion generative models (GDGMs) have emerged as powerful tools for
generating high-quality graphs. However, their broader adoption faces
challenges in \emph{scalability and size generalization}. GDGMs struggle to
scale to large graphs due to their high memory requirements, as they typically
operate in the full graph space, requiring the entire graph to be stored in
memory during training and inference. This constraint limits their feasibility
for large-scale real-world graphs. GDGMs also exhibit poor size generalization,
with limited ability to generate graphs of sizes different from those in the
training data, restricting their adaptability across diverse applications. To
address these challenges, we propose the stochastic block graph diffusion
(SBGD) model, which refines graph representations into a block graph space.
This space incorporates structural priors based on real-world graph patterns,
significantly reducing memory complexity and enabling scalability to large
graphs. The block representation also improves size generalization by capturing
fundamental graph structures. Empirical results show that SBGD achieves
significant memory improvements (up to 6$\times$) while maintaining comparable
or even superior graph generation performance relative to state-of-the-art
methods. Furthermore, experiments demonstrate that SBGD better generalizes to
unseen graph sizes. The significance of SBGD extends beyond being a scalable
and effective GDGM; it also exemplifies the principle of modularization in
generative modeling, offering a new avenue for exploring generative models by
decomposing complex tasks into more manageable components.

</details>


### [176] [Organ-Agents: Virtual Human Physiology Simulator via LLMs](https://arxiv.org/abs/2508.14357)
*Rihao Chang,He Jiao,Weizhi Nie,Honglin Guo,Keliang Xie,Zhenhua Wu,Lina Zhao,Yunpeng Bai,Yongtao Ma,Lanjun Wang,Yuting Su,Xi Gao,Weijie Wang,Nicu Sebe,Bruno Lepri,Bingwei Sun*

Main category: cs.LG

TL;DR: 本文介绍Organ-Agents，一个由大语言模型驱动的多智能体框架，通过模拟人类生理系统，实现高精度和生理合理性，并能进行反事实模拟，为重症监护提供数字孪生。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型在模拟复杂生理系统方面的最新进展，以创建能够模拟人类生理的框架。

Method: 引入Organ-Agents，一个多智能体框架，其中每个LLM驱动的智能体（模拟器）建模一个特定生理系统。训练包括对系统特定时间序列数据进行监督微调，随后通过动态参考选择和纠错进行强化引导协调。数据来源于7,134名败血症患者和7,895名对照组，生成了9个系统和125个变量的高分辨率轨迹。

Result: 在4,509名保留患者上实现了高模拟精度（每系统MSE <0.16），并对不同严重程度分层具有鲁棒性。在外部验证中表现稳定。忠实再现关键多系统事件，并获得15位重症医生的高度认可（Likert评分3.9和3.7）。支持反事实模拟，且生成轨迹与真实患者数据一致。基于合成数据训练的分类器在下游预警任务中性能下降极小（AUROC下降<0.04）。

Conclusion: Organ-Agents被定位为重症监护领域中用于精准诊断、治疗模拟和假设检验的可信、可解释和通用化的数字孪生。

Abstract: Recent advances in large language models (LLMs) have enabled new
possibilities in simulating complex physiological systems. We introduce
Organ-Agents, a multi-agent framework that simulates human physiology via
LLM-driven agents. Each Simulator models a specific system (e.g.,
cardiovascular, renal, immune). Training consists of supervised fine-tuning on
system-specific time-series data, followed by reinforcement-guided coordination
using dynamic reference selection and error correction. We curated data from
7,134 sepsis patients and 7,895 controls, generating high-resolution
trajectories across 9 systems and 125 variables. Organ-Agents achieved high
simulation accuracy on 4,509 held-out patients, with per-system MSEs <0.16 and
robustness across SOFA-based severity strata. External validation on 22,689 ICU
patients from two hospitals showed moderate degradation under distribution
shifts with stable simulation. Organ-Agents faithfully reproduces critical
multi-system events (e.g., hypotension, hyperlactatemia, hypoxemia) with
coherent timing and phase progression. Evaluation by 15 critical care
physicians confirmed realism and physiological plausibility (mean Likert
ratings 3.9 and 3.7). Organ-Agents also enables counterfactual simulations
under alternative sepsis treatment strategies, generating trajectories and
APACHE II scores aligned with matched real-world patients. In downstream early
warning tasks, classifiers trained on synthetic data showed minimal AUROC drops
(<0.04), indicating preserved decision-relevant patterns. These results
position Organ-Agents as a credible, interpretable, and generalizable digital
twin for precision diagnosis, treatment simulation, and hypothesis testing in
critical care.

</details>


### [177] [Online Incident Response Planning under Model Misspecification through Bayesian Learning and Belief Quantization](https://arxiv.org/abs/2508.14385)
*Kim Hammar,Tao Li*

Main category: cs.LG

TL;DR: 本文提出MOBAL，一种在模型错误指定下进行网络攻击事件响应规划的在线方法，通过贝叶斯学习迭代优化模型并利用动态规划进行响应规划，实验证明其在适应性和鲁棒性方面优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 网络攻击事件响应需要快速决策，即使信息不完整或不准确。然而，大多数现有的决策支持框架依赖于详细的系统模型，这限制了它们的实际应用性。

Method: 该研究提出MOBAL（Misspecified Online Bayesian Learning）方法。MOBAL通过贝叶斯学习迭代地修正对模型的推测，随着新信息的获取进行模型自适应。为实现高效的在线响应规划，将推测模型量化为有限马尔可夫模型，并利用动态规划进行决策。此外，还证明了贝叶斯学习对信息反馈的渐近一致性，并建立了模型错误指定和量化误差的边界。

Result: 在CAGE-2基准上的实验表明，MOBAL在适应性和对模型错误指定的鲁棒性方面均优于现有最先进的方法。

Conclusion: MOBAL提供了一种在信息不完整或不准确情况下进行网络事件响应规划的有效在线方法，其出色的适应性和鲁棒性使其在实际应用中具有重要价值。

Abstract: Effective responses to cyberattacks require fast decisions, even when
information about the attack is incomplete or inaccurate. However, most
decision-support frameworks for incident response rely on a detailed system
model that describes the incident, which restricts their practical utility. In
this paper, we address this limitation and present an online method for
incident response planning under model misspecification, which we call MOBAL:
Misspecified Online Bayesian Learning. MOBAL iteratively refines a conjecture
about the model through Bayesian learning as new information becomes available,
which facilitates model adaptation as the incident unfolds. To determine
effective responses online, we quantize the conjectured model into a finite
Markov model, which enables efficient response planning through dynamic
programming. We prove that Bayesian learning is asymptotically consistent with
respect to the information feedback. Additionally, we establish bounds on
misspecification and quantization errors. Experiments on the CAGE-2 benchmark
show that MOBAL outperforms the state of the art in terms of adaptability and
robustness to model misspecification.

</details>


### [178] [Disentanglement in T-space for Faster and Distributed Training of Diffusion Models with Fewer Latent-states](https://arxiv.org/abs/2508.14413)
*Samarth Gupta,Raghudeep Gadde,Rui Chen,Aleix M. Martinez*

Main category: cs.LG

TL;DR: 该研究挑战了扩散模型需要大量时间步长的传统假设，提出通过优化噪声调度和引入单潜态解耦模型，实现在少量时间步长下保持高性能，并显著加速收敛。


<details>
  <summary>Details</summary>
Motivation: 挑战扩散模型训练中关于需要大量潜态或时间步长才能使逆向生成过程接近高斯分布的根本假设。

Method: 1. 通过精心选择噪声调度，使少量潜态（如T≈32）的扩散模型性能与大量潜态（如T≈1000）模型相当。2. 将所需潜态数量极限推至单个潜态，称之为T空间中的“完全解耦”。3. 通过组合多个独立训练的单潜态模型来构建解耦模型，以生成高质量样本。

Result: 1. 在精心选择噪声调度下，少量潜态（T≈32）的扩散模型性能可与大量潜态（T≈1000）模型媲美。2. 组合独立训练的单潜态模型所得到的解耦模型能够轻松生成高质量样本。3. 所提出的解耦模型在多种指标和两个不同数据集上，收敛速度提高了4-6倍。

Conclusion: 扩散模型不一定需要大量时间步长进行训练。该研究证明，通过优化噪声调度和引入单潜态解耦模型，可以在极少甚至单个潜态下实现高质量生成并显著提高收敛效率，挑战了扩散模型的核心假设，为更高效的模型训练提供了新方向。

Abstract: We challenge a fundamental assumption of diffusion models, namely, that a
large number of latent-states or time-steps is required for training so that
the reverse generative process is close to a Gaussian. We first show that with
careful selection of a noise schedule, diffusion models trained over a small
number of latent states (i.e. $T \sim 32$) match the performance of models
trained over a much large number of latent states ($T \sim 1,000$). Second, we
push this limit (on the minimum number of latent states required) to a single
latent-state, which we refer to as complete disentanglement in T-space. We show
that high quality samples can be easily generated by the disentangled model
obtained by combining several independently trained single latent-state models.
We provide extensive experiments to show that the proposed disentangled model
provides 4-6$\times$ faster convergence measured across a variety of metrics on
two different datasets.

</details>


### [179] [Personalized Counterfactual Framework: Generating Potential Outcomes from Wearable Data](https://arxiv.org/abs/2508.14432)
*Ajan Subramanian,Amir M. Rahmani*

Main category: cs.LG

TL;DR: 一个利用可穿戴传感器数据构建个性化反事实模型的框架，旨在探索生活方式改变的“假设”情景，并理解个体化的生理反应。


<details>
  <summary>Details</summary>
Motivation: 可穿戴传感器数据为个性化健康监测提供了机会，但从其复杂、纵向的数据流中获取可操作的洞察具有挑战性。

Method: 该框架首先通过多模态相似性分析，用相似患者的数据增强个体数据集；接着，采用时间PC算法的变体发现预测关系；然后，基于这些关系训练梯度提升机以量化个体效应；最后，利用这些模型驱动反事实引擎，预测假设干预下的生理轨迹。评估通过一步超前预测验证和干预合理性评估进行。

Result: 评估显示其预测精度合理（如平均心率MAE 4.71 bpm），反事实合理性高（中位数0.9643）。该框架揭示了对假设生活方式变化响应的显著个体间差异，验证了其在个性化洞察方面的潜力。

Conclusion: 该工作提供了一个工具，用于探索个性化健康动态并生成关于个体对生活方式变化响应的假设，有助于实现个性化健康洞察。

Abstract: Wearable sensor data offer opportunities for personalized health monitoring,
yet deriving actionable insights from their complex, longitudinal data streams
is challenging. This paper introduces a framework to learn personalized
counterfactual models from multivariate wearable data. This enables exploring
what-if scenarios to understand potential individual-specific outcomes of
lifestyle choices. Our approach first augments individual datasets with data
from similar patients via multi-modal similarity analysis. We then use a
temporal PC (Peter-Clark) algorithm adaptation to discover predictive
relationships, modeling how variables at time t-1 influence physiological
changes at time t. Gradient Boosting Machines are trained on these discovered
relationships to quantify individual-specific effects. These models drive a
counterfactual engine projecting physiological trajectories under hypothetical
interventions (e.g., activity or sleep changes). We evaluate the framework via
one-step-ahead predictive validation and by assessing the plausibility and
impact of interventions. Evaluation showed reasonable predictive accuracy
(e.g., mean heart rate MAE 4.71 bpm) and high counterfactual plausibility
(median 0.9643). Crucially, these interventions highlighted significant
inter-individual variability in response to hypothetical lifestyle changes,
showing the framework's potential for personalized insights. This work provides
a tool to explore personalized health dynamics and generate hypotheses on
individual responses to lifestyle changes.

</details>


### [180] [Fast Symbolic Regression Benchmarking](https://arxiv.org/abs/2508.14481)
*Viktor Martinek*

Main category: cs.LG

TL;DR: 本文提出了一种改进的符号回归（SR）基准测试方法，通过引入可接受表达式列表和早期终止机制，提高了模型发现率并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的符号回归基准测试存在局限性：过分强调发现“唯一的”精确表达式形式；仅依赖计算机代数系统评估成功；以及在发现表达式后仍继续搜索，导致效率低下。

Method: 引入了精选的“可接受表达式列表”，以更灵活地评估成功；实现了早期终止回调机制，以提高效率。文章将此新方法应用于Yoshitomo等人提出的科学发现符号回归（SRSD）基准问题，并对SymbolicRegression.jl和TiSR这两个SR包进行了基准测试。

Result: 使用新方法后，SymbolicRegression.jl的重发现率从26.7%提高到44.7%，计算开销减少了41.2%。TiSR的重发现率为69.4%，基准测试时间节省了63%。

Conclusion: 所提出的新基准测试方法显著提高了符号回归算法的重发现率，并大幅降低了计算成本，证明了其在评估SR算法方面的优越性和效率。

Abstract: Symbolic regression (SR) uncovers mathematical models from data. Several
benchmarks have been proposed to compare the performance of SR algorithms.
However, existing ground-truth rediscovery benchmarks overemphasize the
recovery of "the one" expression form or rely solely on computer algebra
systems (such as SymPy) to assess success. Furthermore, existing benchmarks
continue the expression search even after its discovery. We improve upon these
issues by introducing curated lists of acceptable expressions, and a callback
mechanism for early termination. As a starting point, we use the symbolic
regression for scientific discovery (SRSD) benchmark problems proposed by
Yoshitomo et al., and benchmark the two SR packages SymbolicRegression.jl and
TiSR. The new benchmarking method increases the rediscovery rate of
SymbolicRegression.jl from 26.7%, as reported by Yoshitomo et at., to 44.7%.
Performing the benchmark takes 41.2% less computational expense. TiSR's
rediscovery rate is 69.4%, while performing the benchmark saves 63% time.

</details>


### [181] [On the notion of missingness for path attribution explainability methods in medical settings: Guiding the selection of medically meaningful baselines](https://arxiv.org/abs/2508.14482)
*Alexander Geiger,Lars Wagner,Daniel Rueckert,Dirk Wilhelm,Alissa Jell*

Main category: cs.LG

TL;DR: 针对医疗领域深度学习可解释性中归因方法基线无意义的问题，本文提出一种反事实引导的方法，利用生成模型（如VAE）生成临床有意义的基线，并证明其能产生更忠实、医学相关性更强的归因。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医疗领域的可解释性对临床信任和透明度至关重要。现有路径归因方法（如Integrated Gradients）依赖的“缺失”基线输入（如全零）在医疗背景下缺乏语义意义，且现有方法缺乏针对每个输入动态选择基线的原则性方法。

Method: 研究了医疗环境中“缺失”的概念及其对基线选择的影响；提出一种反事实引导方法，认为“临床正常但与输入接近的反事实”能更好地表示特征的有意义缺失；利用变分自编码器（VAE）生成反事实基线，该方法概念上独立于具体的生成模型。

Result: 在三个不同的医疗数据集上进行评估后，经验证明，与标准基线相比，反事实基线能够产生更忠实且更具医学相关性的归因。

Conclusion: 通过反事实引导的基线选择，显著提高了医疗领域深度学习模型归因的忠实性和医学相关性，为可解释性提供了一个有前景的解决方案。

Abstract: The explainability of deep learning models remains a significant challenge,
particularly in the medical domain where interpretable outputs are critical for
clinical trust and transparency. Path attribution methods such as Integrated
Gradients rely on a baseline input representing the absence of relevant
features ("missingness"). Commonly used baselines, such as all-zero inputs, are
often semantically meaningless, especially in medical contexts where
missingness can itself be informative. While alternative baseline choices have
been explored, existing methods lack a principled approach to dynamically
select baselines tailored to each input. In this work, we examine the notion of
missingness in the medical setting, analyze its implications for baseline
selection, and introduce a counterfactual-guided approach to address the
limitations of conventional baselines. We argue that a clinically normal but
input-close counterfactual represents a more accurate representation of a
meaningful absence of features in medical data. To implement this, we use a
Variational Autoencoder to generate counterfactual baselines, though our
concept is generative-model-agnostic and can be applied with any suitable
counterfactual method. We evaluate the approach on three distinct medical data
sets and empirically demonstrate that counterfactual baselines yield more
faithful and medically relevant attributions compared to standard baseline
choices.

</details>


### [182] [Semantic Energy: Detecting LLM Hallucination Beyond Entropy](https://arxiv.org/abs/2508.14496)
*Huan Ma,Jiadong Pan,Jing Liu,Yan Chen,Joey Tianyi Zhou,Guangyu Wang,Qinghua Hu,Hua Wu,Changqing Zhang,Haifeng Wang*

Main category: cs.LG

TL;DR: 针对大语言模型幻觉问题，本文提出“Semantic Energy”框架，通过直接处理logits并结合语义聚类和能量分布，显著提升了幻觉检测和不确定性估计的准确性，弥补了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在实际应用中易产生幻觉，导致错误决策。现有不确定性估计方法（如语义熵）依赖于后softmax概率，无法有效捕捉模型固有不确定性，在某些场景下表现不佳，因此需要更有效的方法来检测幻觉。

Method: 本文引入“Semantic Energy”不确定性估计框架，通过直接操作模型倒数第二层的logits来利用LLMs的固有置信度。该方法结合了语义聚类和玻尔兹曼（Boltzmann）启发的能量分布，以更好地捕获现有语义熵方法失效情况下的不确定性。

Result: 在多个基准测试中，Semantic Energy显著改善了幻觉检测和不确定性估计的效果，为下游应用（如幻觉检测）提供了更可靠的信号。

Conclusion: Semantic Energy框架通过直接作用于LLMs的内部表示，克服了现有不确定性估计方法的局限性，能够更有效地检测模型幻觉并提供更可靠的不确定性估计，从而提升LLMs在实际应用中的可靠性。

Abstract: Large Language Models (LLMs) are being increasingly deployed in real-world
applications, but they remain susceptible to hallucinations, which produce
fluent yet incorrect responses and lead to erroneous decision-making.
Uncertainty estimation is a feasible approach to detect such hallucinations.
For example, semantic entropy estimates uncertainty by considering the semantic
diversity across multiple sampled responses, thus identifying hallucinations.
However, semantic entropy relies on post-softmax probabilities and fails to
capture the model's inherent uncertainty, causing it to be ineffective in
certain scenarios. To address this issue, we introduce Semantic Energy, a novel
uncertainty estimation framework that leverages the inherent confidence of LLMs
by operating directly on logits of penultimate layer. By combining semantic
clustering with a Boltzmann-inspired energy distribution, our method better
captures uncertainty in cases where semantic entropy fails. Experiments across
multiple benchmarks show that Semantic Energy significantly improves
hallucination detection and uncertainty estimation, offering more reliable
signals for downstream applications such as hallucination detection.

</details>


### [183] [Exact Shapley Attributions in Quadratic-time for FANOVA Gaussian Processes](https://arxiv.org/abs/2508.14499)
*Majid Mohammadi,Krikamol Muandet,Ilaria Tiddi,Annette Ten Teije,Siu Lun Chau*

Main category: cs.LG

TL;DR: 该研究提出了一种二次时间复杂度的精确方法，用于计算FANOVA高斯过程模型的局部和全局Shapley值，解决了Shapley值在概率模型中计算效率低的问题。


<details>
  <summary>Details</summary>
Motivation: Shapley值是机器学习特征重要性归因的公认方法，但其精确计算复杂度随特征数量呈指数增长，严重限制了实际应用。对于高斯过程等输出为随机变量的概率模型，计算复杂度进一步提高，需要额外建模高阶矩。

Method: 本研究提出，对于FANOVA高斯过程，可以以二次时间复杂度精确计算局部和全局Shapley值。对于局部实例解释，定义了一个基于函数分量的随机合作博弈，计算包含期望贡献和不确定性的精确随机Shapley值。对于全局解释，引入了一个确定性的、基于方差的价值函数，计算量化特征对模型整体敏感度贡献的精确Shapley值。核心方法是利用FANOVA分解的闭合形式（随机）莫比乌斯表示和受牛顿恒等式启发的递归算法，高效计算Shapley值的均值和方差。

Result: 研究结果表明，对于FANOVA高斯过程，无论是局部还是全局的精确Shapley归因都可以在二次时间内计算。局部解释能捕获预测的期望贡献和不确定性，而全局解释能量化特征对模型总体敏感度的贡献。实证研究证明了该方法增强了可解释AI的实用性。

Conclusion: 本工作通过为结构化概率模型（如FANOVA GP）的预测提供更具可扩展性、公理化合理性且感知不确定性的解释，显著提升了可解释人工智能的效用。

Abstract: Shapley values are widely recognized as a principled method for attributing
importance to input features in machine learning. However, the exact
computation of Shapley values scales exponentially with the number of features,
severely limiting the practical application of this powerful approach. The
challenge is further compounded when the predictive model is probabilistic - as
in Gaussian processes (GPs) - where the outputs are random variables rather
than point estimates, necessitating additional computational effort in modeling
higher-order moments. In this work, we demonstrate that for an important class
of GPs known as FANOVA GP, which explicitly models all main effects and
interactions, *exact* Shapley attributions for both local and global
explanations can be computed in *quadratic time*. For local, instance-wise
explanations, we define a stochastic cooperative game over function components
and compute the exact stochastic Shapley value in quadratic time only,
capturing both the expected contribution and uncertainty. For global
explanations, we introduce a deterministic, variance-based value function and
compute exact Shapley values that quantify each feature's contribution to the
model's overall sensitivity. Our methods leverage a closed-form (stochastic)
M\"{o}bius representation of the FANOVA decomposition and introduce recursive
algorithms, inspired by Newton's identities, to efficiently compute the mean
and variance of Shapley values. Our work enhances the utility of explainable
AI, as demonstrated by empirical studies, by providing more scalable,
axiomatically sound, and uncertainty-aware explanations for predictions
generated by structured probabilistic models.

</details>


### [184] [Artificial Intelligence-Based Multiscale Temporal Modeling for Anomaly Detection in Cloud Services](https://arxiv.org/abs/2508.14503)
*Lian Lian,Yilin Li,Song Han,Renzi Meng,Sibo Wang,Ming Wang*

Main category: cs.LG

TL;DR: 本研究提出一种基于Transformer架构并整合多尺度特征感知的异常检测方法，旨在解决云服务环境中时序建模和尺度感知特征表示的局限性，并在实验中展现出优于主流基线模型的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决云服务环境中现有异常检测方法在时序建模和尺度感知特征表示方面的局限性。

Method: 该方法首先采用改进的Transformer模块对高维监控数据进行时序建模，捕获长程依赖；其次，引入多尺度特征构建路径，通过下采样和并行编码提取不同粒度的时序特征；然后，设计注意力加权融合模块动态调整各尺度贡献。输入阶段构建标准化的多维时间序列（包含CPU、内存等），并使用位置编码增强时序感知。通过对比实验和超参数敏感性分析进行系统评估。

Result: 实验结果表明，所提出的方法在精确率、召回率、AUC和F1-score等关键指标上优于主流基线模型，并在各种扰动条件下保持了强大的稳定性和检测性能。

Conclusion: 该方法在复杂云环境中展现出卓越的异常检测能力。

Abstract: This study proposes an anomaly detection method based on the Transformer
architecture with integrated multiscale feature perception, aiming to address
the limitations of temporal modeling and scale-aware feature representation in
cloud service environments. The method first employs an improved Transformer
module to perform temporal modeling on high-dimensional monitoring data, using
a self-attention mechanism to capture long-range dependencies and contextual
semantics. Then, a multiscale feature construction path is introduced to
extract temporal features at different granularities through downsampling and
parallel encoding. An attention-weighted fusion module is designed to
dynamically adjust the contribution of each scale to the final decision,
enhancing the model's robustness in anomaly pattern modeling. In the input
modeling stage, standardized multidimensional time series are constructed,
covering core signals such as CPU utilization, memory usage, and task
scheduling states, while positional encoding is used to strengthen the model's
temporal awareness. A systematic experimental setup is designed to evaluate
performance, including comparative experiments and hyperparameter sensitivity
analysis, focusing on the impact of optimizers, learning rates, anomaly ratios,
and noise levels. Experimental results show that the proposed method
outperforms mainstream baseline models in key metrics, including precision,
recall, AUC, and F1-score, and maintains strong stability and detection
performance under various perturbation conditions, demonstrating its superior
capability in complex cloud environments.

</details>


### [185] [Great GATsBi: Hybrid, Multimodal, Trajectory Forecasting for Bicycles using Anticipation Mechanism](https://arxiv.org/abs/2508.14523)
*Kevin Riehl,Shaimaa K. El-Baklish,Anastasios Kouvelas,Michail A. Makridis*

Main category: cs.LG

TL;DR: 本文提出了一个名为Great GATsBi的混合多模态自行车轨迹预测框架，它结合了物理和社交建模，并在预测性能上超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶和高级驾驶辅助系统越来越需要准确预测道路使用者行为，这对道路安全至关重要。尽管自行车事故死亡人数最多，但以往研究主要关注行人和机动车，对自行车关注不足。

Method: 本文提出了Great GATsBi，一个基于领域知识的混合多模态自行车轨迹预测框架。该模型结合了物理建模（受机动车启发）和社交建模（受行人运动启发），以解释自行车运动的双重性质。社交互动通过图注意力网络建模，并包含衰减的历史和预期的未来邻近轨迹数据。研究还进行了受控的大规模自行车实验以验证框架性能。

Result: 研究结果表明，所提出的物理模型（擅长短期预测）和社交模型（擅长长期预测）的组合性能超越了现有最佳方法。

Conclusion: Great GATsBi框架通过结合物理和社交模型，有效地预测了自行车轨迹并建模了其与道路使用者的社交互动，为自行车运动预测提供了更准确和安全的解决方案。

Abstract: Accurate prediction of road user movement is increasingly required by many
applications ranging from advanced driver assistance systems to autonomous
driving, and especially crucial for road safety. Even though most traffic
accident fatalities account to bicycles, they have received little attention,
as previous work focused mainly on pedestrians and motorized vehicles. In this
work, we present the Great GATsBi, a domain-knowledge-based, hybrid, multimodal
trajectory prediction framework for bicycles. The model incorporates both
physics-based modeling (inspired by motorized vehicles) and social-based
modeling (inspired by pedestrian movements) to explicitly account for the dual
nature of bicycle movement. The social interactions are modeled with a graph
attention network, and include decayed historical, but also anticipated, future
trajectory data of a bicycles neighborhood, following recent insights from
psychological and social studies. The results indicate that the proposed
ensemble of physics models -- performing well in the short-term predictions --
and social models -- performing well in the long-term predictions -- exceeds
state-of-the-art performance. We also conducted a controlled mass-cycling
experiment to demonstrate the framework's performance when forecasting bicycle
trajectories and modeling social interactions with road users.

</details>


### [186] [Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks](https://arxiv.org/abs/2508.14536)
*Saman Yazdannik,Morteza Tayefi,Shamim Sanisales*

Main category: cs.LG

TL;DR: 本文提出Chebyshev-DQN (Ch-DQN)，通过整合Chebyshev多项式基改进DQN，以更有效表示值函数。在CartPole-v1上，Ch-DQN性能显著优于标准DQN，但多项式次数的选择至关重要。


<details>
  <summary>Details</summary>
Motivation: 标准DQN的性能受其神经网络能否准确逼近动作值函数的影响。传统函数逼近器（如多层感知机）在表示复杂强化学习问题中的值函数时可能效率低下。

Method: 引入一种新的Chebyshev-DQN (Ch-DQN) 架构，将Chebyshev多项式基集成到DQN框架中，以创建更有效的特征表示。在CartPole-v1基准测试上与参数数量相似的标准DQN进行比较评估。

Result: Ch-DQN在适中多项式次数（N=4）下，渐进性能显著优于基线模型约39%。然而，多项式次数的选择是关键超参数，过高次数（N=8）反而不利于学习。

Conclusion: 验证了在深度强化学习中使用正交多项式基的潜力，并强调了模型复杂性带来的权衡。

Abstract: The performance of Deep Q-Networks (DQN) is critically dependent on the
ability of its underlying neural network to accurately approximate the
action-value function. Standard function approximators, such as multi-layer
perceptrons, may struggle to efficiently represent the complex value landscapes
inherent in many reinforcement learning problems. This paper introduces a novel
architecture, the Chebyshev-DQN (Ch-DQN), which integrates a Chebyshev
polynomial basis into the DQN framework to create a more effective feature
representation. By leveraging the powerful function approximation properties of
Chebyshev polynomials, we hypothesize that the Ch-DQN can learn more
efficiently and achieve higher performance. We evaluate our proposed model on
the CartPole-v1 benchmark and compare it against a standard DQN with a
comparable number of parameters. Our results demonstrate that the Ch-DQN with a
moderate polynomial degree (N=4) achieves significantly better asymptotic
performance, outperforming the baseline by approximately 39\%. However, we also
find that the choice of polynomial degree is a critical hyperparameter, as a
high degree (N=8) can be detrimental to learning. This work validates the
potential of using orthogonal polynomial bases in deep reinforcement learning
while also highlighting the trade-offs involved in model complexity.

</details>


### [187] [FedEve: On Bridging the Client Drift and Period Drift for Cross-device Federated Learning](https://arxiv.org/abs/2508.14539)
*Tao Shen,Zexi Li,Didi Zhu,Ziyu Zhao,Chao Wu,Fei Wu*

Main category: cs.LG

TL;DR: 联邦学习中数据异构性导致客户端漂移和周期漂移。本文提出FedEve框架，通过使两种漂移相互补偿来减轻其负面影响，并在跨设备非独立同分布数据上表现优越。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的数据异构性导致收敛差和性能下降。现有研究关注客户端漂移，但跨设备联邦学习中因客户端部分参与导致的“周期漂移”未被充分研究，且其可能比客户端漂移更具危害性。

Method: 提出一个“预测-观察”框架及其具体实现方法FedEve。该方法旨在让客户端漂移和周期漂移相互补偿，从而减轻它们的整体影响。理论上证明该方法可以减少模型更新的方差。

Result: 研究发现，随着数据异构性程度的增加，周期漂移对跨设备联邦学习的负面影响尤为显著。FedEve方法在跨设备非独立同分布（non-iid）数据上，其性能优于现有替代方案。

Conclusion: 本文通过提出的FedEve框架有效地解决了跨设备联邦学习中客户端漂移和周期漂移带来的双重挑战，尤其是在数据异构性较高的情况下，通过使这两种漂移相互补偿来稳定模型更新，提升了性能。

Abstract: Federated learning (FL) is a machine learning paradigm that allows multiple
clients to collaboratively train a shared model without exposing their private
data. Data heterogeneity is a fundamental challenge in FL, which can result in
poor convergence and performance degradation. Client drift has been recognized
as one of the factors contributing to this issue resulting from the multiple
local updates in FedAvg. However, in cross-device FL, a different form of drift
arises due to the partial client participation, but it has not been studied
well. This drift, we referred as period drift, occurs as participating clients
at each communication round may exhibit distinct data distribution that
deviates from that of all clients. It could be more harmful than client drift
since the optimization objective shifts with every round.
  In this paper, we investigate the interaction between period drift and client
drift, finding that period drift can have a particularly detrimental effect on
cross-device FL as the degree of data heterogeneity increases. To tackle these
issues, we propose a predict-observe framework and present an instantiated
method, FedEve, where these two types of drift can compensate each other to
mitigate their overall impact. We provide theoretical evidence that our
approach can reduce the variance of model updates. Extensive experiments
demonstrate that our method outperforms alternatives on non-iid data in
cross-device settings.

</details>


### [188] [Adaptively Robust LLM Inference Optimization under Prediction Uncertainty](https://arxiv.org/abs/2508.14544)
*Zixi Chen,Yinyu Ye,Zijie Zhou*

Main category: cs.LG

TL;DR: 研究并提出了一种LLM推理调度算法($\mathcal{A}_{\min}$)，通过利用机器学习预测的输出长度下限并动态调整，有效解决了输出长度不确定性问题，显著降低了总延迟和能耗，表现接近最优。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型(LLM)推理是一个在线、多任务且高度耗能的服务过程。随着大量请求的涌入，提高其调度效率和降低功耗至关重要。主要挑战在于，虽然输入（提示）长度已知，但严重影响内存和处理时间的输出长度是未知的。

Method: 针对输出长度的不确定性，提出利用机器学习预测输出长度的区间分类（最小-最大范围）。首先设计了保守算法$\mathcal{A}_{\max}$（基于预测上限调度），但其过于保守。随后提出了自适应算法$\mathcal{A}_{\min}$，该算法初步将预测下限作为输出长度，并在推理过程中动态细化估计。

Result: $\mathcal{A}_{\min}$算法被证明能达到对数级的竞争比，并通过数值模拟显示其性能常接近于事后最优调度器，凸显了其在实际场景中的效率和鲁棒性。此外，$\mathcal{A}_{\min}$仅依赖于预测区间的下限，这在输出长度上限难以精确预测的情况下更具优势。

Conclusion: $\mathcal{A}_{\min}$是一种高效且鲁棒的LLM推理调度算法，能有效应对输出长度不确定性。它通过利用易于预测的输出长度下限并动态调整，在最小化总延迟和能耗方面表现出色，并能接近事后最优性能。

Abstract: We study the problem of optimizing Large Language Model (LLM) inference
scheduling to minimize total latency. LLM inference is an online and multi-task
service process and also heavily energy consuming by which a pre-trained LLM
processes input requests and generates output tokens sequentially. Therefore,
it is vital to improve its scheduling efficiency and reduce the power
consumption while a great amount of prompt requests are arriving. A key
challenge in LLM inference scheduling is that while the prompt length is known
upon arrival, the output length, which critically impacts memory usage and
processing time, is unknown. To address this uncertainty, we propose algorithms
that leverage machine learning to predict output lengths, assuming the
prediction provides an interval classification (min-max range) for each
request.
  We first design a conservative algorithm, $\mathcal{A}_{\max}$, which
schedules requests based on the upper bound of predicted output lengths to
prevent memory overflow. However, this approach is overly conservative: as
prediction accuracy decreases, performance degrades significantly due to
potential overestimation. To overcome this limitation, we propose
$\mathcal{A}_{\min}$, an adaptive algorithm that initially treats the predicted
lower bound as the output length and dynamically refines this estimate during
inferencing. We prove that $\mathcal{A}_{\min}$ achieves a log-scale
competitive ratio. Through numerical simulations, we demonstrate that
$\mathcal{A}_{\min}$ often performs nearly as well as the hindsight scheduler,
highlighting both its efficiency and robustness in practical scenarios.
Moreover, $\mathcal{A}_{\min}$ relies solely on the lower bound of the
prediction interval--an advantageous design choice since upper bounds on output
length are typically more challenging to predict accurately.

</details>


### [189] [Cooperative SGD with Dynamic Mixing Matrices](https://arxiv.org/abs/2508.14565)
*Soumya Sarkar,Shweta Jain*

Main category: cs.LG

TL;DR: 本文提出一个统一框架，为采用动态拓扑的分布式本地更新随机梯度下降（SGD）算法提供了改进或匹配的收敛理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有分布式SGD算法常假设固定拓扑和均匀节点贡献，但实验证明这些假设并非最优，动态拓扑和非均匀聚合能显著提升性能。

Method: 详细介绍了一个统一框架，该框架涵盖了多种基于本地更新的分布式SGD算法，并支持动态拓扑。

Result: 与现有工作相比，提供了改进或匹配的收敛理论保证。

Conclusion: 该统一框架有效解决了分布式SGD在动态拓扑下的收敛性问题，并提供了更强或相当的理论保障。

Abstract: One of the most common methods to train machine learning algorithms today is
the stochastic gradient descent (SGD). In a distributed setting, SGD-based
algorithms have been shown to converge theoretically under specific
circumstances. A substantial number of works in the distributed SGD setting
assume a fixed topology for the edge devices. These papers also assume that the
contribution of nodes to the global model is uniform. However, experiments have
shown that such assumptions are suboptimal and a non uniform aggregation
strategy coupled with a dynamically shifting topology and client selection can
significantly improve the performance of such models. This paper details a
unified framework that covers several Local-Update SGD-based distributed
algorithms with dynamic topologies and provides improved or matching
theoretical guarantees on convergence compared to existing work.

</details>


### [190] [A Comprehensive Evaluation of the Sensitivity of Density-Ratio Estimation Based Fairness Measurement in Regression](https://arxiv.org/abs/2508.14576)
*Abdalwahab Almajed,Maryam Tabar,Peyman Najafirad*

Main category: cs.LG

TL;DR: 本研究发现，回归问题中基于密度比估计的公平性度量方法，其核心算法选择会显著影响结果，甚至导致不一致的公平性评估，表明该方法存在可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习中算法偏见普遍，促使公平性度量研究增多。回归中的公平性度量复杂，现有方法将其视为密度比估计问题。然而，此前研究未探讨不同密度比估计算法对公平性度量结果敏感性的影响。

Method: 本文开发了一系列采用不同密度比估计算法的公平性度量方法，并深入研究了这些不同核心算法对公平性水平的影响。

Result: 实验结果表明，密度比估计算法的选择会显著影响公平性度量结果，甚至导致对不同算法相对公平性评估的不一致。

Conclusion: 这些发现表明基于密度比估计的回归公平性度量方法存在重大问题，未来需要进一步研究以提高其可靠性。

Abstract: The prevalence of algorithmic bias in Machine Learning (ML)-driven approaches
has inspired growing research on measuring and mitigating bias in the ML
domain. Accordingly, prior research studied how to measure fairness in
regression which is a complex problem. In particular, recent research proposed
to formulate it as a density-ratio estimation problem and relied on a Logistic
Regression-driven probabilistic classifier-based approach to solve it. However,
there are several other methods to estimate a density ratio, and to the best of
our knowledge, prior work did not study the sensitivity of such fairness
measurement methods to the choice of underlying density ratio estimation
algorithm. To fill this gap, this paper develops a set of fairness measurement
methods with various density-ratio estimation cores and thoroughly investigates
how different cores would affect the achieved level of fairness. Our
experimental results show that the choice of density-ratio estimation core
could significantly affect the outcome of fairness measurement method, and
even, generate inconsistent results with respect to the relative fairness of
various algorithms. These observations suggest major issues with density-ratio
estimation based fairness measurement in regression and a need for further
research to enhance their reliability.

</details>


### [191] [DualNILM: Energy Injection Identification Enabled Disaggregation with Deep Multi-Task Learning](https://arxiv.org/abs/2508.14600)
*Xudong Wang,Guoming Tang,Junyu Xue,Srinivasan Keshav,Tongxin Li,Chris Ding*

Main category: cs.LG

TL;DR: 针对智能家居中表后能源（如太阳能、电池）注入导致传统NILM性能下降的问题，本文提出DualNILM，一个基于Transformer的深度多任务学习框架，能同时进行家电状态识别和注入能量识别，并表现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统的非侵入式负荷监测（NILM）方法仅依赖总表数据，但随着智能家居中表后能源（如太阳能板、电池储能）的普及，其注入的能量会模糊单个家电的功率特征，导致现有NILM方法性能显著下降，因此需要新的解决方案。

Method: 本文提出DualNILM，一个深度多任务学习框架，专为NILM中的家电状态识别和注入能量识别双重任务设计。它采用基于Transformer的架构，并整合了序列到点（sequence-to-point）和序列到序列（sequence-to-sequence）策略，以有效捕捉总功率消耗模式中的多尺度时间依赖性。

Result: 通过在自收集和合成的开放NILM数据集（包含家电级能耗和能量注入数据）上进行验证，实验结果表明DualNILM在双重任务中均保持了出色的性能，并且显著优于传统方法。

Conclusion: DualNILM成功应对了NILM中表后能源注入带来的挑战，能够准确地进行家电状态识别和注入能量识别，为复杂的智能家居环境提供了更可靠的NILM解决方案。

Abstract: Non-Intrusive Load Monitoring (NILM) offers a cost-effective method to obtain
fine-grained appliance-level energy consumption in smart homes and building
applications. However, the increasing adoption of behind-the-meter energy
sources, such as solar panels and battery storage, poses new challenges for
conventional NILM methods that rely solely on at-the-meter data. The injected
energy from the behind-the-meter sources can obscure the power signatures of
individual appliances, leading to a significant decline in NILM performance. To
address this challenge, we present DualNILM, a deep multi-task learning
framework designed for the dual tasks of appliance state recognition and
injected energy identification in NILM. By integrating sequence-to-point and
sequence-to-sequence strategies within a Transformer-based architecture,
DualNILM can effectively capture multi-scale temporal dependencies in the
aggregate power consumption patterns, allowing for accurate appliance state
recognition and energy injection identification. We conduct validation of
DualNILM using both self-collected and synthesized open NILM datasets that
include both appliance-level energy consumption and energy injection. Extensive
experimental results demonstrate that DualNILM maintains an excellent
performance for the dual tasks in NILM, much outperforming conventional
methods.

</details>


### [192] [Measuring IIA Violations in Similarity Choices with Bayesian Models](https://arxiv.org/abs/2508.14615)
*Hugo Sales Corrêa,Suryanarayana Sankagiri,Daniel Ratton Figueiredo,Matthias Grossglauser*

Main category: cs.LG

TL;DR: 本研究调查了相似性选择数据中的“无关备选项独立性”（IIA）假设，提出了两种IIA统计测试方法（包括基于PPC的贝叶斯方法），并在实验中发现显著的IIA违背，指出违背源于上下文效应，强调需要新的相似性选择模型。


<details>
  <summary>Details</summary>
Motivation: 经典的相似性选择模型（如基于度量的模型）通常假设“无关备选项独立性”（IIA），但IIA违背在许多离散选择场景中已被发现。然而，在相似性选择场景中，由于其选择具有目标依赖性，IIA违背的研究和测试受到了忽视，存在测试复杂性。

Method: 提出了两种统计方法来测试IIA：一种是经典的拟合优度检验，另一种是基于后验预测检验（PPC）的贝叶斯对应方法（该研究的主要技术贡献，能够量化IIA违背的程度）。构建了两个数据集：一个旨在引出IIA违背，另一个包含随机生成的选择集。此外，还设计了一种新颖的PPC测试方法来检验群体同质性。

Result: 在两个数据集上都确认了显著的IIA违背，并且两者之间违背程度相似。人口同质性测试结果显示群体确实是同质的，这表明IIA违背是由上下文效应（具体来说是选择集内部的相互作用）驱动的。

Conclusion: 研究结果突出表明，迫切需要开发能够解释这些上下文效应的新相似性选择模型。

Abstract: Similarity choice data occur when humans make choices among alternatives
based on their similarity to a target, e.g., in the context of information
retrieval and in embedding learning settings. Classical metric-based models of
similarity choice assume independence of irrelevant alternatives (IIA), a
property that allows for a simpler formulation. While IIA violations have been
detected in many discrete choice settings, the similarity choice setting has
received scant attention. This is because the target-dependent nature of the
choice complicates IIA testing. We propose two statistical methods to test for
IIA: a classical goodness-of-fit test and a Bayesian counterpart based on the
framework of Posterior Predictive Checks (PPC). This Bayesian approach, our
main technical contribution, quantifies the degree of IIA violation beyond its
mere significance. We curate two datasets: one with choice sets designed to
elicit IIA violations, and another with randomly generated choice sets from the
same item universe. Our tests confirmed significant IIA violations on both
datasets, and notably, we find a comparable degree of violation between them.
Further, we devise a new PPC test for population homogeneity. Results show that
the population is indeed homogenous, suggesting that the IIA violations are
driven by context effects -- specifically, interactions within the choice sets.
These results highlight the need for new similarity choice models that account
for such context effects.

</details>


### [193] [A Fuzzy-Enhanced Explainable AI Framework for Flight Continuous Descent Operations Classification](https://arxiv.org/abs/2508.14618)
*Amin Noroozi,Sandaruwan K. Sethunge,Elham Norouzi,Phat T. Phan,Kavinda U. Waduge,Md. Arafatur Rahman*

Main category: cs.LG

TL;DR: 本研究提出一种模糊增强可解释人工智能（FEXAI）框架，结合机器学习、SHAP和模糊逻辑，分析了影响连续下降运行（CDO）性能的关键因素。该框架利用ADS-B数据，识别出最重要的预测因子，并生成可解释的规则，准确率超过90%，为航空运营决策提供支持。


<details>
  <summary>Details</summary>
Motivation: 尽管连续下降运行（CDO）具有显著的运营和环境效益，但对其性能影响因素的系统性研究有限。此外，现有相关方法（如轨迹优化）缺乏航空领域所需的透明度与可解释性，这对于安全和信任至关重要。

Method: 本研究提出了FEXAI框架，整合模糊逻辑、机器学习和SHAP分析。收集了1094个航班的ADS-B数据，包含29个特征（11个运行相关，18个天气相关）。利用机器学习模型和SHAP对CDO符合度进行分类和特征重要性排序。识别出最重要的三个特征后，用于构建模糊规则分类器，并提取可解释的模糊规则。

Result: 所有模型的分类准确率均超过90%。FEXAI方法为运行用户提供了有意义、易读的规则。研究发现，到达航路内的平均下降率、下降段数量和下降期间方向航向的平均变化是CDO性能最强的预测因子。

Conclusion: 本研究提出的FEXAI方法为航空运营决策支持提供了一条新途径，可集成到航空工具中，实现实时咨询，以在不同运行条件下保持CDO符合度。

Abstract: Continuous Descent Operations (CDO) involve smooth, idle-thrust descents that
avoid level-offs, reducing fuel burn, emissions, and noise while improving
efficiency and passenger comfort. Despite its operational and environmental
benefits, limited research has systematically examined the factors influencing
CDO performance. Moreover, many existing methods in related areas, such as
trajectory optimization, lack the transparency required in aviation, where
explainability is critical for safety and stakeholder trust. This study
addresses these gaps by proposing a Fuzzy-Enhanced Explainable AI (FEXAI)
framework that integrates fuzzy logic with machine learning and SHapley
Additive exPlanations (SHAP) analysis. For this purpose, a comprehensive
dataset of 29 features, including 11 operational and 18 weather-related
features, was collected from 1,094 flights using Automatic Dependent
Surveillance-Broadcast (ADS-B) data. Machine learning models and SHAP were then
applied to classify flights' CDO adherence levels and rank features by
importance. The three most influential features, as identified by SHAP scores,
were then used to construct a fuzzy rule-based classifier, enabling the
extraction of interpretable fuzzy rules. All models achieved classification
accuracies above 90%, with FEXAI providing meaningful, human-readable rules for
operational users. Results indicated that the average descent rate within the
arrival route, the number of descent segments, and the average change in
directional heading during descent were the strongest predictors of CDO
performance. The FEXAI method proposed in this study presents a novel pathway
for operational decision support and could be integrated into aviation tools to
enable real-time advisories that maintain CDO adherence under varying
operational conditions.

</details>


### [194] [Clinical semantics for lung cancer prediction](https://arxiv.org/abs/2508.14627)
*Luis H. John,Jan A. Kors,Jenna M. Reps,Peter R. Rijnbeek,Egill A. Fridgeirsson*

Main category: cs.LG

TL;DR: 研究利用Poincaré嵌入将SNOMED医学术语层次结构映射到双曲空间，并将其整合到深度学习模型中，以改善肺癌发病预测。


<details>
  <summary>Details</summary>
Motivation: 现有临床预测模型忽略临床概念间的语义关系，本研究旨在通过整合领域特异性语义信息来提高肺癌发病预测的准确性。

Method: 使用Optum EHR数据集构建基于SNOMED分类法的临床知识图谱，通过黎曼随机梯度下降生成Poincaré嵌入。这些嵌入被集成到ResNet和Transformer两种深度学习架构中，并通过判别力（ROC曲线下面积）和校准度（观察与预测概率的平均绝对差）进行评估。

Result: 与基线模型相比，整合预训练的Poincaré嵌入在判别力方面带来适度且一致的提升。ResNet模型（特别是使用10维Poincaré嵌入的）显示出增强的校准性，而Transformer模型在不同配置下保持了稳定的校准性。

Conclusion: 将临床知识图谱嵌入双曲空间并将其表示集成到深度学习模型中，通过保留临床术语的层次结构，可以改善肺癌发病预测。该方法为结合数据驱动的特征提取与既有临床知识提供了一种可行途径。

Abstract: Background: Existing clinical prediction models often represent patient data
using features that ignore the semantic relationships between clinical
concepts. This study integrates domain-specific semantic information by mapping
the SNOMED medical term hierarchy into a low-dimensional hyperbolic space using
Poincar\'e embeddings, with the aim of improving lung cancer onset prediction.
  Methods: Using a retrospective cohort from the Optum EHR dataset, we derived
a clinical knowledge graph from the SNOMED taxonomy and generated Poincar\'e
embeddings via Riemannian stochastic gradient descent. These embeddings were
then incorporated into two deep learning architectures, a ResNet and a
Transformer model. Models were evaluated for discrimination (area under the
receiver operating characteristic curve) and calibration (average absolute
difference between observed and predicted probabilities) performance.
  Results: Incorporating pre-trained Poincar\'e embeddings resulted in modest
and consistent improvements in discrimination performance compared to baseline
models using randomly initialized Euclidean embeddings. ResNet models,
particularly those using a 10-dimensional Poincar\'e embedding, showed enhanced
calibration, whereas Transformer models maintained stable calibration across
configurations.
  Discussion: Embedding clinical knowledge graphs into hyperbolic space and
integrating these representations into deep learning models can improve lung
cancer onset prediction by preserving the hierarchical structure of clinical
terminologies used for prediction. This approach demonstrates a feasible method
for combining data-driven feature extraction with established clinical
knowledge.

</details>


### [195] [Understanding Data Influence with Differential Approximation](https://arxiv.org/abs/2508.14648)
*Haoru Tan,Sitong Wu,Xiuzhe Wu,Wang Wang,Bo Zhao,Zeke Xie,Gui-Song Xia,Xiaojuan Qi*

Main category: cs.LG

TL;DR: 提出Diff-In，一种高效、可扩展的二阶样本影响力分析方法，解决了现有工具的准确性和凸性假设限制，并在多项数据中心任务中表现卓越。


<details>
  <summary>Details</summary>
Motivation: 人工智能发展中数据分析至关重要，但现有数据分析工具准确性不足，且常错误地假设神经网络损失函数为凸，限制了其应用。

Method: 本文提出Diff-In，通过累积连续学习步骤中样本影响力的差异来近似样本影响力。该方法采用二阶近似，无需模型凸性假设，并通过高效计算Hessian与梯度乘积，保持与一阶方法相当的计算复杂度。

Result: 理论分析表明Diff-In的近似误差远低于现有影响力估计器。在数据清洗、数据删除和核心集选择等数据中心任务中，实验结果证实其在多个基准数据集上性能优越，并能扩展到数百万数据点。

Conclusion: Diff-In提供了一种更准确、高效且无需凸性假设的样本影响力分析方法，有效解决了现有工具的局限性，并在大规模数据应用中展现出强大的性能。

Abstract: Data plays a pivotal role in the groundbreaking advancements in artificial
intelligence. The quantitative analysis of data significantly contributes to
model training, enhancing both the efficiency and quality of data utilization.
However, existing data analysis tools often lag in accuracy. For instance, many
of these tools even assume that the loss function of neural networks is convex.
These limitations make it challenging to implement current methods effectively.
In this paper, we introduce a new formulation to approximate a sample's
influence by accumulating the differences in influence between consecutive
learning steps, which we term Diff-In. Specifically, we formulate the
sample-wise influence as the cumulative sum of its changes/differences across
successive training iterations. By employing second-order approximations, we
approximate these difference terms with high accuracy while eliminating the
need for model convexity required by existing methods. Despite being a
second-order method, Diff-In maintains computational complexity comparable to
that of first-order methods and remains scalable. This efficiency is achieved
by computing the product of the Hessian and gradient, which can be efficiently
approximated using finite differences of first-order gradients. We assess the
approximation accuracy of Diff-In both theoretically and empirically. Our
theoretical analysis demonstrates that Diff-In achieves significantly lower
approximation error compared to existing influence estimators. Extensive
experiments further confirm its superior performance across multiple benchmark
datasets in three data-centric tasks: data cleaning, data deletion, and coreset
selection. Notably, our experiments on data pruning for large-scale
vision-language pre-training show that Diff-In can scale to millions of data
points and outperforms strong baselines.

</details>


### [196] [ELATE: Evolutionary Language model for Automated Time-series Engineering](https://arxiv.org/abs/2508.14667)
*Andrew Murray,Danial Dervovic,Michael Cashmore*

Main category: cs.LG

TL;DR: ELATE是一个结合进化框架和语言模型的自动化时序特征工程方法，可显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 时序预测中特征工程对模型性能至关重要，但其手工操作耗时且费力；现有自动化方法依赖穷举，计算成本高且缺乏领域洞察力。

Method: 本文提出了ELATE（用于自动化时序工程的进化语言模型），它在一个进化框架内利用语言模型自动化时序数据特征工程。ELATE通过时序统计度量和特征重要性指标指导和筛选特征，同时语言模型提出新的、上下文相关的特征转换。

Result: 实验证明，ELATE将各种领域内的预测准确性平均提高了8.4%。

Conclusion: ELATE是一种有效且高效的自动化时序特征工程方法，能够显著提升预测准确性。

Abstract: Time-series prediction involves forecasting future values using machine
learning models. Feature engineering, whereby existing features are transformed
to make new ones, is critical for enhancing model performance, but is often
manual and time-intensive. Existing automation attempts rely on exhaustive
enumeration, which can be computationally costly and lacks domain-specific
insights. We introduce ELATE (Evolutionary Language model for Automated
Time-series Engineering), which leverages a language model within an
evolutionary framework to automate feature engineering for time-series data.
ELATE employs time-series statistical measures and feature importance metrics
to guide and prune features, while the language model proposes new,
contextually relevant feature transformations. Our experiments demonstrate that
ELATE improves forecasting accuracy by an average of 8.4% across various
domains.

</details>


### [197] [Improving Fairness in Graph Neural Networks via Counterfactual Debiasing](https://arxiv.org/abs/2508.14683)
*Zengyi Wo,Chang Liu,Yumeng Wang,Minglai Shao,Wenjun Wang*

Main category: cs.LG

TL;DR: 图神经网络（GNNs）易受偏见影响，现有缓解方法常损害预测精度。本文提出Fair-ICD，通过反事实数据增强和对抗性学习，在保持高预测性能的同时显著提升GNN的公平性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在图数据建模中表现出色，但与其他机器学习模型一样，其预测可能存在基于种族、性别等属性的偏见，且图结构和消息传递机制会加剧此问题。现有的偏见缓解策略（如过滤敏感信息）可能无意中删除非敏感特征，导致预测精度与公平性之间难以平衡。

Method: 本文提出一种新颖的方法Fair-ICD，利用反事实数据增强来缓解偏见。具体而言，该方法在消息传递前利用反事实数据创建多样化的邻域，以学习无偏见的节点表示。随后，引入对抗判别器来进一步减少传统GNN分类器预测中的偏见。

Result: 实验结果表明，Fair-ICD在适度条件下能有效确保GNN的公平性。在标准数据集上，通过三种GNN骨干网络的测试，Fair-ICD显著提升了公平性指标，同时保持了较高的预测性能。

Conclusion: Fair-ICD通过创新的反事实数据增强和对抗性学习机制，成功解决了GNN中的偏见问题，有效平衡了公平性和预测性能，克服了现有偏见缓解方法的局限性。

Abstract: Graph Neural Networks (GNNs) have been successful in modeling
graph-structured data. However, similar to other machine learning models, GNNs
can exhibit bias in predictions based on attributes like race and gender.
Moreover, bias in GNNs can be exacerbated by the graph structure and
message-passing mechanisms. Recent cutting-edge methods propose mitigating bias
by filtering out sensitive information from input or representations, like edge
dropping or feature masking. Yet, we argue that such strategies may
unintentionally eliminate non-sensitive features, leading to a compromised
balance between predictive accuracy and fairness. To tackle this challenge, we
present a novel approach utilizing counterfactual data augmentation for bias
mitigation. This method involves creating diverse neighborhoods using
counterfactuals before message passing, facilitating unbiased node
representations learning from the augmented graph. Subsequently, an adversarial
discriminator is employed to diminish bias in predictions by conventional GNN
classifiers. Our proposed technique, Fair-ICD, ensures the fairness of GNNs
under moderate conditions. Experiments on standard datasets using three GNN
backbones demonstrate that Fair-ICD notably enhances fairness metrics while
preserving high predictive performance.

</details>


### [198] [Addressing Graph Anomaly Detection via Causal Edge Separation and Spectrum](https://arxiv.org/abs/2508.14684)
*Zengyi Wo,Wenjun Wang,Minglai Shao,Chang Liu,Yumeng Wang,Yueheng Sun*

Main category: cs.LG

TL;DR: 针对异配图上的异常检测难题，本研究发现异常节点的异配性会导致谱能量向高频转移。为此，我们提出了一种名为CES2-GAD的谱神经网络，它通过因果干预分离同配和异配边，并结合混合谱滤波器来有效识别异配图中的异常。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，异常实体在隐藏直接链接的同时会增加合法连接，导致异常网络呈现异配结构，而大多数GNN难以处理。现有空间域方法忽略了节点结构编码、特征与上下文的复杂关系，且谱域异配问题研究有限。研究发现异常节点的异配性导致谱能量从低频向高频转移，需要专门方法解决。

Method: 提出基于因果边分离的谱神经网络CES2-GAD。首先，通过因果干预将原始图分离为同配边和异配边。随后，使用多种混合谱滤波器从分割后的图中捕获信号。最后，将来自多信号的表示拼接起来，并输入分类器以预测异常。

Result: 在真实世界数据集上进行的大量实验证明了所提出方法的有效性。

Conclusion: CES2-GAD通过创新的因果边分离和混合谱滤波机制，有效解决了异配图上异常检测中因异常节点异配性导致的谱能量高频转移问题，展现出卓越的性能。

Abstract: In the real world, anomalous entities often add more legitimate connections
while hiding direct links with other anomalous entities, leading to
heterophilic structures in anomalous networks that most GNN-based techniques
fail to address. Several works have been proposed to tackle this issue in the
spatial domain. However, these methods overlook the complex relationships
between node structure encoding, node features, and their contextual
environment and rely on principled guidance, research on solving spectral
domain heterophilic problems remains limited. This study analyzes the spectral
distribution of nodes with different heterophilic degrees and discovers that
the heterophily of anomalous nodes causes the spectral energy to shift from low
to high frequencies. To address the above challenges, we propose a spectral
neural network CES2-GAD based on causal edge separation for anomaly detection
on heterophilic graphs. Firstly, CES2-GAD will separate the original graph into
homophilic and heterophilic edges using causal interventions. Subsequently,
various hybrid-spectrum filters are used to capture signals from the segmented
graphs. Finally, representations from multiple signals are concatenated and
input into a classifier to predict anomalies. Extensive experiments with
real-world datasets have proven the effectiveness of the method we proposed.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [199] [StarStream: Live Video Analytics over Space Networking](https://arxiv.org/abs/2508.14222)
*Miao Zhang,Jiaxing Li,Haoyuan Zhao,Linfeng Shen,Jiangchuan Liu*

Main category: cs.NI

TL;DR: 本文探讨了在低地球轨道（LEO）卫星网络（LSN）上进行实时视频分析（LVA）的挑战与潜力。通过对Starlink进行实际测量，发现上行链路瓶颈和网络波动是关键问题。为此，提出并开发了StarStream，一个LVA的LSN自适应流媒体框架，该框架利用基于Transformer的网络性能预测器和内容感知配置优化器，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有实时视频分析（LVA）系统主要基于地面网络，在自然灾害和偏远地区的应用受限，而这些地区对实时视觉数据和场景分析有迫切需求。随着低地球轨道（LEO）卫星网络（如Starlink）的兴起，全球高速互联网接入成为可能，因此研究LVA在现代LSN上的可行性成为必要。

Method: 1. **性能测量与分析：** 使用Starlink作为测试平台，进行了广泛的实地测量，以深入了解其在LVA方面的性能。 2. **系统开发：** 开发了名为“StarStream”的新型LSN自适应LVA流媒体框架。其核心包括一个针对LSN的基于Transformer的网络性能预测器和一个内容感知配置优化器。 3. **效果评估：** 通过基于真实网络和视频处理数据的追踪驱动实验，展示了StarStream的有效性和优越性。

Result: 1. 测量结果显示，当前LSN中的上行链路瓶颈以及不稳定的网络条件会显著影响LVA的服务质量，并需要及时的适应。 2. StarStream框架能够有效应对LSN的网络挑战，提升LVA的服务质量，并展现出优越的性能。

Conclusion: LEO卫星网络为在传统地面网络无法覆盖的区域进行实时视频分析提供了新的可能性，但其固有的上行链路瓶颈和网络波动性是需要克服的关键挑战。StarStream框架通过智能预测和自适应优化，成功解决了这些问题，为LSN上的LVA提供了一个有效的解决方案。

Abstract: Streaming videos from resource-constrained front-end devices over networks to
resource-rich cloud servers has long been a common practice for surveillance
and analytics. Most existing live video analytics (LVA) systems, however, have
been built over terrestrial networks, limiting their applications during
natural disasters and in remote areas that desperately call for real-time
visual data delivery and scene analysis. With the recent advent of space
networking, in particular, Low Earth Orbit (LEO) satellite constellations such
as Starlink, high-speed truly global Internet access is becoming available and
affordable. This paper examines the challenges and potentials of LVA over
modern LEO satellite networking (LSN). Using Starlink as the testbed, we have
carried out extensive in-the-wild measurements to gain insights into its
achievable performance for LVA. The results reveal that the uplink bottleneck
in today's LSN, together with the volatile network conditions, can
significantly affect the service quality of LVA and necessitate prompt
adaptation. We accordingly develop StarStream, a novel LSN-adaptive streaming
framework for LVA. At its core, StarStream is empowered by a Transformer-based
network performance predictor tailored for LSN and a content-aware
configuration optimizer. We discuss a series of key design and implementation
issues of StarStream and demonstrate its effectiveness and superiority through
trace-driven experiments with real-world network and video processing data.

</details>


### [200] [OmniSense: Towards Edge-Assisted Online Analytics for 360-Degree Videos](https://arxiv.org/abs/2508.14237)
*Miao Zhang,Yifei Zhu,Linfeng Shen,Fangxin Wang,Jiangchuan Liu*

Main category: cs.NI

TL;DR: 针对360度视频分析面临的计算和网络资源挑战，OmniSense提出一种边缘辅助框架，通过预测球形感兴趣区域和智能调整视觉模型，实现低延迟、高精度和高效率的分析。


<details>
  <summary>Details</summary>
Motivation: 随着全向相机成本降低和扩展现实应用普及，360度视频被大量捕获。为了充分发挥其潜力，需要先进的视频分析技术来提取无盲点的可操作洞察和情境知识，但这面临巨大的计算和网络资源挑战。

Method: 提出OmniSense，一个新颖的边缘辅助在线沉浸式视频分析框架。它引入了轻量级球形感兴趣区域（SRoI）预测算法以裁剪360度帧中的冗余信息。随后，结合视频内容和网络动态，智能调整视觉模型以分析预测的SRoI，从而优化资源利用。

Result: 与资源无关的基线相比，OmniSense在保持相似端到端延迟的情况下，准确性提高了19.8%至114.6%。同时，在保持与基线最高准确度持平的情况下，实现了2.0倍至2.4倍的速度提升。

Conclusion: OmniSense通过优化资源利用，成功解决了360度视频分析中计算和网络资源的挑战，显著提高了分析的准确性和效率，同时保持了低延迟。

Abstract: With the reduced hardware costs of omnidirectional cameras and the
proliferation of various extended reality applications, more and more
$360^\circ$ videos are being captured. To fully unleash their potential,
advanced video analytics is expected to extract actionable insights and
situational knowledge without blind spots from the videos. In this paper, we
present OmniSense, a novel edge-assisted framework for online immersive video
analytics. OmniSense achieves both low latency and high accuracy, combating the
significant computation and network resource challenges of analyzing
$360^\circ$ videos. Motivated by our measurement insights into $360^\circ$
videos, OmniSense introduces a lightweight spherical region of interest (SRoI)
prediction algorithm to prune redundant information in $360^\circ$ frames.
Incorporating the video content and network dynamics, it then smartly scales
vision models to analyze the predicted SRoIs with optimized resource
utilization. We implement a prototype of OmniSense with commodity devices and
evaluate it on diverse real-world collected $360^\circ$ videos. Extensive
evaluation results show that compared to resource-agnostic baselines, it
improves the accuracy by $19.8\%$ -- $114.6\%$ with similar end-to-end
latencies. Meanwhile, it hits $2.0\times$ -- $2.4\times$ speedups while keeping
the accuracy on par with the highest accuracy of baselines.

</details>


### [201] [A Distributed Learned Hash Table](https://arxiv.org/abs/2508.14239)
*Shengze Wang,Yi Liu,Xiaoxue Zhang,Liting Hu,Chen Qian*

Main category: cs.NI

TL;DR: 本文提出LEAD系统，通过将学习模型集成到DHT中，显著优化了分布式哈希表（DHT）的范围查询性能，将查询延迟和消息成本降低了80%至90%以上，同时保持了高可伸缩性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 分布式哈希表（DHTs）在分布式网络系统中至关重要，但它们在处理范围查询时面临巨大挑战。然而，范围查询对于LLM服务、分布式存储、数据库、内容分发网络和区块链等应用至关重要。

Method: 本文提出了LEAD系统，它通过在DHT结构中引入学习模型来优化范围查询性能。LEAD使用递归机器学习模型来映射和检索分布式系统中的数据，同时保留数据的固有顺序。其设计旨在最小化范围查询延迟和消息成本，同时保持高可伸缩性和对网络波动的弹性。

Result: 通过在测试平台和模拟中的全面评估，LEAD在大型分布式系统中的系统效率方面取得了巨大优势，相比现有范围查询方法，查询延迟和消息成本降低了80%到90%以上。此外，LEAD展现出卓越的可伸缩性和对抗系统波动的鲁棒性。

Conclusion: LEAD为分布式键值系统中高效的数据检索提供了一个鲁棒、可伸缩的解决方案，显著提升了DHT的范围查询能力，解决了现有DHT在处理此类查询时的主要限制。

Abstract: Distributed Hash Tables (DHTs) are pivotal in numerous high-impact key-value
applications built on distributed networked systems, offering a decentralized
architecture that avoids single points of failure and improves data
availability. Despite their widespread utility, DHTs face substantial
challenges in handling range queries, which are crucial for applications such
as LLM serving, distributed storage, databases, content delivery networks, and
blockchains. To address this limitation, we present LEAD, a novel system
incorporating learned models within DHT structures to significantly optimize
range query performance. LEAD utilizes a recursive machine learning model to
map and retrieve data across a distributed system while preserving the inherent
order of data. LEAD includes the designs to minimize range query latency and
message cost while maintaining high scalability and resilience to network
churn. Our comprehensive evaluations, conducted in both testbed implementation
and simulations, demonstrate that LEAD achieves tremendous advantages in system
efficiency compared to existing range query methods in large-scale distributed
systems, reducing query latency and message cost by 80% to 90%+. Furthermore,
LEAD exhibits remarkable scalability and robustness against system churn,
providing a robust, scalable solution for efficient data retrieval in
distributed key-value systems.

</details>


### [202] [DeeP-TE: Data-enabled Predictive Traffic Engineering](https://arxiv.org/abs/2508.14281)
*Zhun Yin,Xiaotian Li,Lifan Mei,Yong Liu,Zhong-Ping Jiang*

Main category: cs.NI

TL;DR: 论文提出了DeeP-TE（数据驱动预测性流量工程）算法，该算法无需直接估计流量矩阵，通过历史数据自适应调整路由配置，有效降低网络拥塞并显著减少路由变化，实现接近最优的控制效果。


<details>
  <summary>Details</summary>
Motivation: 为保持网络性能，路由配置需随流量变化而调整。自适应路由面临两大挑战：如何准确测量/估计时变流量矩阵，以及如何控制频繁路由变更造成的网络性能下降。

Method: 开发了一种新颖的数据驱动预测性流量工程（DeeP-TE）算法。该算法能够直接从历史路由数据和相应的链路速率数据生成路由更新，从而无需直接测量或估计流量矩阵，以最小化网络拥塞。

Result: 在真实网络拓扑和流量矩阵上的数值实验表明，所提出的DeeP-TE路由自适应算法能实现接近最优的控制效果，并且路由变化显著低于基线方法。

Conclusion: DeeP-TE算法通过数据驱动的方式，在不直接依赖流量矩阵估计的前提下，有效解决了自适应路由的挑战，实现了接近最优的网络性能和更低的路由波动。

Abstract: Routing configurations of a network should constantly adapt to traffic
variations to achieve good network performance. Adaptive routing faces two main
challenges: 1) how to accurately measure/estimate time-varying traffic
matrices? 2) how to control the network and application performance degradation
caused by frequent route changes? In this paper, we develop a novel
data-enabled predictive traffic engineering (DeeP-TE) algorithm that minimizes
the network congestion by gracefully adapting routing configurations over time.
Our control algorithm can generate routing updates directly from the historical
routing data and the corresponding link rate data, without direct traffic
matrix measurement or estimation. Numerical experiments on real network
topologies with real traffic matrices demonstrate that the proposed DeeP-TE
routing adaptation algorithm can achieve close-to-optimal control effectiveness
with significantly lower routing variations than the baseline methods.

</details>


### [203] [Design and Simulation of Fault-Tolerant Network Switching System Using Python-Based Algorithms](https://arxiv.org/abs/2508.14305)
*Terlumun Gbaden,Mterorga Ukor,Grace Erdoo Ateata*

Main category: cs.NI

TL;DR: 该论文设计并模拟了一个基于Python的容错网络交换系统，通过动态检测故障和重路由，显著提升了网络可靠性和数据流连续性。


<details>
  <summary>Details</summary>
Motivation: 现代网络，尤其是在可靠性和响应性至关重要的环境中，需要强大的容错机制来确保数据流不中断。

Method: 使用Python算法设计并模拟了一个容错网络交换系统。利用NetworkX建模企业级局域网（LAN）的交换机-路由器互连（包含冗余链路），通过Scapy注入链路故障和拥塞等故障场景，并采用自定义Python逻辑实现自动故障转移和重路由。

Result: 该系统通过动态检测路径故障、通过冗余链路重新分配网络流量并最大限度地减少停机时间，展现出强大的弹性。性能评估显示，与非容错基线相比，系统在数据包传输连续性、更快的恢复时间和更低的数据包丢失率方面都有显著改善。

Conclusion: 该实现提供了一种可扩展且轻量级的方法，用于将容错功能集成到中型网络中，在企业信息技术基础设施和学术模拟中具有潜在应用。

Abstract: Ensuring uninterrupted data flow in modern networks requires robust
fault-tolerant mechanisms, especially in environments where reliability and
responsiveness are critical. This paper presents the design and simulation of a
fault-tolerant network switching system using Python-based algorithms. A
simulated enterprise-level Local Area Network (LAN) was modeled using NetworkX
to represent switch-router interconnectivity with redundant links. Fault
scenarios, including link failure and congestion, were injected using Scapy,
while automatic failover and rerouting were implemented via custom Python
logic. The system demonstrates resilience by dynamically detecting path
failures, redistributing network traffic through redundant links, and
minimizing downtime. Performance evaluations reveal significant improvements in
packet delivery continuity, faster recovery times, and reduced packet loss
compared to non-fault-tolerant baselines. The implementation provides a
scalable and lightweight approach to integrating fault-tolerance features into
mid-scale networks, with potential application in enterprise information
technology infrastructures and academic simulations.

</details>


### [204] [The Small-World Beneath LEO Satellite Coverage: Ground Hubs in Multi-Shell Constellations](https://arxiv.org/abs/2508.14335)
*Hailong Su,Jinshu Su,Yusheng Xia,Haibin Li*

Main category: cs.NI

TL;DR: 通过复杂网络分析，研究了大型多壳层LEO卫星星座的结构和动态，揭示了其高效路由、连接增强、负载不均及高韧性等特性，为星座设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 解决大型低轨卫星（LEO）星座，特别是多壳层架构中路由效率和壳间通信的关键挑战。

Method: 利用复杂网络分析工具，对一个由10,956颗卫星和198个网关站（GS）组成的六壳层巨型星座的结构特性和网络动态进行深入研究。

Result: ['星座具有强大的小世界特性，能实现高效路由。', '网关站（GS）中继在增强壳间连接方面发挥关键作用。', '馈线链路显著缩短了平均路径长度，使长途通信更可行。', '中介性分析揭示了网关站之间存在负载不平衡，需采取流量感知管理策略。', '该架构提供出色的空间覆盖和弹性，即使在GS故障下也能保持连接和低路由成本。']

Conclusion: 研究结果解释了现有巨型星座的设计原理，并为未来卫星网络基础设施的发展提供了宝贵指导。

Abstract: In recent years, the emergence of large-scale Low-Earth-Orbit (LEO) satellite
constellations has introduced unprecedented opportunities for global
connectivity. However, routing efficiency and inter-shell communication remain
key challenges in multi-shell architectures. This paper investigates the
structural properties and network dynamics of a representative six-shell
mega-constellation composed of 10,956 satellites and 198 gateway stations
(GSs). Leveraging tools from complex network analysis, we identify several
critical findings: (1) the constellation exhibits strong small-world
characteristics, enabling efficient routing despite large network diameters;
(2) GS relays play a pivotal role in enhancing inter-shell connectivity by
bridging otherwise disconnected components; (3) feeder links significantly
reduce average path length, making long-haul communication more feasible; (4)
betweenness analysis reveals load imbalances among GSs, indicating the need for
traffic-aware management strategies; (5) the architecture offers excellent
spatial coverage and resilience, maintaining connectivity and low routing costs
even under GS failures. These insights not only explain the design rationale
behind current mega-constellations like SpaceX Starlink, but also provide
valuable guidance for the evolution of future satellite network
infrastructures.

</details>


### [205] [Availability-Aware VNF Placement and Request Routing in MEC-Enabled 5G Networks](https://arxiv.org/abs/2508.14435)
*Aqsa Sayeed,Samaresh Bera*

Main category: cs.NI

TL;DR: 本文研究在5G MEC网络中实现uRLLC应用高可靠性和低延迟的虚拟网络功能（VNF）部署问题，提出随机舍入和贪婪启发式方法，并验证其性能和冗余部署的优势。


<details>
  <summary>Details</summary>
Motivation: 在5G MEC网络中，为满足uRLLC应用严苛的可靠性和低延迟要求，VNF部署是一个关键且NP难的约束优化问题，旨在最大化网络服务提供商的服务请求总收益。

Method: 将VNF部署问题建模为NP难的约束优化问题。提出一种近似随机舍入方法在多项式时间内解决该问题，并证明其性能保证。此外，提出一种贪婪启发式方法解决资源约束违反问题。还使用开源软件平台实现了5G网络软件原型，进行冗余VNF部署验证。

Result: 仿真结果表明，所提出的随机舍入和贪婪方法在总收益方面分别达到最优解的5%和10%以内。与现有不考虑可用性要求的方法相比，所提出的贪婪方法表现更优，揭示了延迟敏感型uRLLC应用中可用性与资源效率之间的权衡。原型实现结果显示，冗余VNF部署在不同故障概率下能提高数据包交付率和降低延迟。

Conclusion: 所提出的随机舍入和贪婪启发式方法能有效解决5G MEC网络中的VNF部署问题，满足uRLLC应用的严格要求，并能获得接近最优的收益。研究强调了在设计此类系统时，考虑可用性要求的重要性及其与资源效率之间的权衡。冗余VNF部署在实际网络中对提升性能和可靠性具有积极作用。

Abstract: In this paper, we study the virtual network function (VNF) placement problem
in mobile edge computing (MEC)-enabled 5G networks to meet the stringent
reliability and latency requirements of uRLLC applications. We pose it as a
constrained optimization problem, which is NP-hard, to maximize the total
reward obtained by a network service provider by serving uRLLC service
requests. We propose an approximated randomized rounding approach to solve the
NP-hard optimization problem in polynomial time. We prove that the proposed
randomized approach achieves performance guarantees while violating the
resource constraints boundedly. Furthermore, we present a greedy-heuristic
approach to tackle the violations of resource constraints.
  Simulation results show that the proposed randomized rounding and greedy
approaches achieve a total reward which is within 5% and 10% of the optimal
solution, respectively. Furthermore, we compare the proposed greedy approach
with the existing schemes that do not consider the availability requirements.
We observe that the existing schemes perform poorly in terms of total reward,
as negligence to the availability requirements negatively impacts the number of
successfully served requests. These findings highlight the trade-off between
availability and resource efficiency in latency-sensitive uRLLC applications.
We also implement a software prototype of a 5G network using open-source
software platforms with redundant placement of VNFs. The results on packet
delivery ratio and latency obtained from the prototype implementation are also
improved in the redundant VNFs with different failure probabilities.

</details>


### [206] [Transforming Next-generation Network Planning assisted by Data Acquisition of Top Three Spanish MNOs](https://arxiv.org/abs/2508.14445)
*M. Umar Khan*

Main category: cs.NI

TL;DR: 本文强调了利用传统移动流量数据进行5G网络规划的必要性，提出数据获取和处理程序，并应用于西班牙移动运营商数据以识别高用户密度区域，支持高效的gNB部署。


<details>
  <summary>Details</summary>
Motivation: 为实现更高效的5G网络规划设计（尤其在拓扑和成本方面），有必要从传统移动基础设施的流量数据中提取有用信息并进行网络规模设计。

Method: 使用西班牙三大移动网络运营商的真实开放数据库。提出一套数据获取程序，旨在清洗数据库、提取有意义的流量信息并可视化交通密度模式，以支持新的gNB部署。文章还介绍了网络数据现状，并详细描述了所使用的数据库。

Result: 文章讨论了相应的结果，但摘要中未具体展开。结果旨在估算流量并识别用户密度最高的区域，为新服务的部署提供依据。

Conclusion: （摘要未直接给出明确结论，但可推断为）通过应用所提出的数据获取与处理程序，利用传统移动流量数据，可以有效识别高用户密度区域，支持5G网络更高效的规划和新gNB的部署，从而实现更高效、低成本的网络设计。

Abstract: In this paper, we address the necessity of data related to mobile traffic of
the legacy infrastructure to extract useful information and perform network
dimensioning for 5G. These data can help us achieve a more efficient network
planning design, especially in terms of topology and cost. To that end, a real
open database of top three Spanish mobile network operators (MNOs) is used to
estimate the traffic and to identify the area of highest user density for the
deployment of new services. We propose the data acquisition procedure described
to clean the database, to extract meaningful traffic information and to
visualize traffic density patterns for new gNB deployments. We present the
state of the art in Network Data. We describe the considered network database
in detail. The Network Data Acquisition entity along with the proposed
procedure is explained. The corresponding results are discussed, following the
conclusions.

</details>


### [207] [Adaptive Network Selection for Latency-Aware V2X Systems under Varying Network and Vehicle Densities](https://arxiv.org/abs/2508.14471)
*Muhammad Z. Haq,Nadia N. Qadri,Omer Chughtai,Sadiq A. Ahmad,Waqas Khalid,Heejung Yu*

Main category: cs.NI

TL;DR: 本文提出ANS-V2X，一个为延迟敏感V2X系统设计的自适应网络选择框架，通过启发式方法，在接近最优性能的同时大幅缩短决策时间并降低延迟，适用于实时部署。


<details>
  <summary>Details</summary>
Motivation: 现代V2X通信需要低延迟高吞吐量，但实时网络选择受限于多样的应用需求和4G、5G、自组网等异构无线接入技术的共存。现有优化方法（如MILP）计算耗时过长，而基于强化学习的方法（如Q学习）训练缓慢且在动态场景中收敛困难。

Method: 本文提出ANS-V2X框架，采用启发式方法，根据应用敏感度、延迟、计算负载和方向性约束来为车辆分配网络。通过与提供最优解的混合整数线性规划（MILP）方法和基于Q学习的强化学习方法进行仿真对比验证。

Result: 仿真结果表明，ANS-V2X达到了接近最优的性能（与MILP相比，效用通常在5%到10%以内），同时执行时间减少了85%以上。ANS-V2X的决策时间在15毫秒以内，并始终比MILP（常超过100毫秒）和Q学习方案提供更低的延迟。Q学习虽具适应性但训练耗时且动态场景收敛慢。

Conclusion: ANS-V2X框架因其快速决策和持续低延迟的特点，非常适合部署在对延迟要求严苛的V2X系统的实时、边缘级环境中。

Abstract: This paper presents ANS-V2X, an Adaptive Network Selection framework tailored
for latency-aware V2X systems operating under varying vehicle densities and
heterogeneous network conditions. Modern vehicular environments demand
low-latency and high-throughput communication, yet real-time network selection
is hindered by diverse application requirements and the coexistence of multiple
Radio Access Technologies (RATs) such as 4G, 5G, and ad hoc links. ANS-V2X
employs a heuristic-driven approach to assign vehicles to networks by
considering application sensitivity, latency, computational load, and
directionality constraints. The framework is benchmarked against a
Mixed-Integer Linear Programming (MILP) formulation for optimal solutions and a
Q-learning-based method representing reinforcement learning. Simulation results
demonstrate that ANS-V2X achieves near-optimal performance, typically within 5
to 10% of the utility achieved by MILP-V2X, while reducing execution time by
more than 85%. Although MILP-V2X offers globally optimal results, its
computation time often exceeds 100 milliseconds, making it unsuitable for
real-time applications. The Q-learning-based method is more adaptable but
requires extensive training and converges slowly in dynamic scenarios. In
contrast, ANS-V2X completes decisions in under 15 milliseconds and consistently
delivers lower latency than both alternatives. This confirms its suitability
for real-time, edge-level deployment in latency-critical V2X systems

</details>


### [208] [Multi-Tier UAV Edge Computing for Low Altitude Networks Towards Long-Term Energy Stability](https://arxiv.org/abs/2508.14601)
*Yufei Ye,Shijian Gao,Xinhu Zheng,Liuqing Yang*

Main category: cs.NI

TL;DR: 提出一种新型多层无人机辅助边缘计算系统，利用Lyapunov优化最小化任务延迟并确保低层无人机（L-UAV）的能量稳定。


<details>
  <summary>Details</summary>
Motivation: 旨在最小化多层无人机辅助边缘计算系统中的任务执行延迟，并解决低层无人机（L-UAV）续航和计算资源限制，确保其长期能量稳定性。

Method: 设计了由车载用户、L-UAV（边缘服务器）和H-UAV（移动备份服务器）组成的多层系统。采用Lyapunov优化将问题解耦为时隙确定性问题，并根据L-UAV实时能量状态自适应调整任务延迟和能耗优先级。优化内容包括任务分配、计算资源分配及L-UAV和H-UAV的轨迹规划。

Result: 仿真结果表明，该方法使L-UAV的传输能量至少减少26%，且相比现有基准表现出更优越的能量稳定性。

Conclusion: 所提出的方法有效降低了任务执行延迟，并显著提升了低空网络中L-UAV的能量效率和稳定性。

Abstract: This paper presents a novel multi-tier UAV-assisted edge computing system
designed for low-altitude networks. The system comprises vehicle users,
lightweight Low-Tier UAVs (L-UAVs), and High-Tier UAV (H-UAV). L-UAVs function
as small-scale edge servers positioned closer to vehicle users, while the
H-UAV, equipped with more powerful server and larger-capacity battery, serves
as mobile backup server to address the limitations in endurance and computing
resources of L-UAVs. The primary objective is to minimize task execution delays
while ensuring long-term energy stability for L-UAVs. To address this
challenge, the problem is first decoupled into a series of deterministic
problems for each time slot using Lyapunov optimization. The priorities of task
delay and energy consumption for L-UAVs are adaptively adjusted based on
real-time energy status. The optimization tasks include assignment of tasks,
allocation of computing resources, and trajectory planning for both L-UAVs and
H-UAV. Simulation results demonstrate that the proposed approach achieves a
reduction of at least 26% in transmission energy for L-UAVs and exhibits
superior energy stability compared to existing benchmarks.

</details>


### [209] [Adaptive Vision-Based Coverage Optimization in Mobile Wireless Sensor Networks: A Multi-Agent Deep Reinforcement Learning Approach](https://arxiv.org/abs/2508.14676)
*Parham Soltani,Mehrshad Eskandarpour,Sina Heidari,Farnaz Alizadeh,Hossein Soleimani*

Main category: cs.NI

TL;DR: 本文提出一种基于深度强化学习和多智能体强化学习的移动无线传感器网络自主部署策略，利用视觉系统优化区域覆盖和系统效率。


<details>
  <summary>Details</summary>
Motivation: 传统无线传感器网络部署存在过度重叠和能源耗尽问题。移动无线传感器网络虽能解决静态覆盖局限，但仍需更有效的自主部署策略以应对传感器故障并最大化覆盖。

Method: 提出一种无需预设策略的部署方案：移动传感器自主定位以最大化区域覆盖。采用实时摄像头系统结合深度强化学习（DRL）监控传感器LED指示器并评估实时覆盖。通过多智能体强化学习（MARL）框架，依据覆盖效率和传感器移动计算奖励，实现去中心化、协作式的传感器控制。

Result: 相比传统基于距离的定位方法，该方法在覆盖率上提高26.5%，能耗降低32%，冗余减少22%，网络寿命延长45%。主要贡献包括：低成本视觉覆盖评估、可扩展的MARL-DRL框架以及自重构系统以应对能源耗尽。

Conclusion: 该方法显著增强了移动无线传感器网络的适应性、能源效率和鲁棒性，为物联网框架内的部署提供了实用的解决方案。

Abstract: Traditional Wireless Sensor Networks (WSNs) typically rely on pre-analysis of
the target area, network size, and sensor coverage to determine initial
deployment. This often results in significant overlap to ensure continued
network operation despite sensor energy depletion. With the emergence of Mobile
Wireless Sensor Networks (MWSNs), issues such as sensor failure and static
coverage limitations can be more effectively addressed through mobility. This
paper proposes a novel deployment strategy in which mobile sensors autonomously
position themselves to maximize area coverage, eliminating the need for
predefined policies. A live camera system, combined with deep reinforcement
learning (DRL), monitors the network by detecting sensor LED indicators and
evaluating real-time coverage. Rewards based on coverage efficiency and sensor
movement are computed at each learning step and shared across the network
through a Multi-Agent Reinforcement Learning (MARL) framework, enabling
decentralized, cooperative sensor control. Key contributions include a
vision-based, low-cost coverage evaluation method; a scalable MARL-DRL
framework for autonomous deployment; and a self-reconfigurable system that
adjusts sensor positioning in response to energy depletion. Compared to
traditional distance-based localization, the proposed method achieves a 26.5%
improvement in coverage, a 32% reduction in energy consumption, and a 22%
decrease in redundancy, extending network lifetime by 45%. This approach
significantly enhances adaptability, energy efficiency, and robustness in
MWSNs, offering a practical deployment solution within the IoT framework.

</details>


### [210] [Energy-Efficient Routing Algorithm for Wireless Sensor Networks: A Multi-Agent Reinforcement Learning Approach](https://arxiv.org/abs/2508.14679)
*Parham Soltani,Mehrshad Eskandarpour,Amir Ahmadizad,Hossein Soleimani*

Main category: cs.NI

TL;DR: 本文提出一种基于强化学习和混合多跳路由算法的无线传感器网络（WSNs）能量管理新方法，通过动态路径适应和负载均衡，显著提高网络生命周期和弹性。


<details>
  <summary>Details</summary>
Motivation: 在无线传感器网络（WSNs）中，高效的能源管理对于延长网络寿命和确保可靠数据传输至关重要。

Method: 该研究提出一种结合强化学习（基于Q学习的多智能体系统）的簇头选择和混合多跳路由算法。每个传感器节点被建模为自主智能体，观察自身残余能量、到汇聚点距离、跳数和热点邻近度等状态参数，选择最大化长期能效的路由动作。通过精心设计的奖励函数，激励负载均衡、避免热点和节能转发。学习过程可去中心化或通过云控制器执行。此外，该方法将强化学习驱动的路由决策与最小能量路由算法（MERA）和最小生成树（MST）等经典图论方法融合，以优化能耗和负载均衡。

Result: 仿真结果证实，所提出的方法显著提高了节点存活率，降低了荷电状态（SoC）方差，并增强了网络弹性。

Conclusion: 该方法为动态传感器部署和物联网应用中的能量受限无线传感器网络提供了一个可扩展且自适应的解决方案。

Abstract: Efficient energy management is essential in Wireless Sensor Networks (WSNs)
to extend network lifetime and ensure reliable data transmission. This paper
presents a novel method using reinforcement learning-based cluster-head
selection and a hybrid multi-hop routing algorithm, which leverages Q-learning
within a multi-agent system to dynamically adapt transmission paths based on
the energy distribution across sensor nodes. Each sensor node is modeled as an
autonomous agent that observes local state parameters, such as residual energy,
distance to sink, hop count, and hotspot proximity, and selects routing actions
that maximize long-term energy efficiency. After computing the optimal paths,
each sensor aggregates sensed data and forwards it through intermediate nodes
to a selected transmitter node, chosen based on the highest remaining State of
Charge (SoC), thereby avoiding premature node depletion. To promote efficient
learning, a carefully designed reward function incentivizes balanced load
distribution, hotspot avoidance, and energy-aware forwarding while maintaining
signal quality. The learning process occurs either in a decentralized manner or
via a cloud-based controller that offloads computation in large-scale
deployments. Moreover, the RL-driven routing decisions are fused with classical
graph-based methods, Minimum Energy Routing Algorithm (MERA) and Minimum
Spanning Tree (MST), to optimize energy consumption and load balancing.
Simulations confirm that the proposed approach significantly improves node
survival rate, reduces SoC variance, and enhances network resilience, making it
a scalable and adaptive solution for energy-constrained WSNs in dynamic sensor
deployments and IoT applications.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [211] [High-Throughput Low-Cost Segmentation of Brightfield Microscopy Live Cell Images](https://arxiv.org/abs/2508.14106)
*Surajit Das,Gourav Roy,Pavel Zun*

Main category: q-bio.QM

TL;DR: 本研究开发了一种低成本的基于CNN的图像分割管线，用于明场显微镜下未染色活细胞的分割。该模型对低对比度、噪声和模糊图像具有鲁棒性，并在多种成像条件下表现出强大的泛化能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有显微图像分割方法难以一致地解决高通量明场活细胞成像的挑战，包括时间表型变化、低对比度、噪声和细胞运动引起的模糊。

Method: 开发了一种基于CNN的管线，该管线在一个统一的U-Net架构中结合了冻结编码器、注意力机制、实例感知系统、自适应损失函数、困难实例重训练、动态学习率、渐进机制（以减轻过拟合）和集成技术。

Result: 在低对比度、有噪声和模糊的图像上，测试准确率达到93%，平均F1分数达到89%。与最先进方法具有持续竞争力，并能有效泛化到相差显微镜数据集，表现出强大的跨模态泛化能力和鲁棒性。

Conclusion: 该管线因其鲁棒性、高精度、低计算需求和高适应性，在真实实验室环境中具有巨大的部署潜力，并优于现有明场显微镜分割方法。

Abstract: Live cell culture is crucial in biomedical studies for analyzing cell
properties and dynamics in vitro. This study focuses on segmenting unstained
live cells imaged with bright-field microscopy. While many segmentation
approaches exist for microscopic images, none consistently address the
challenges of bright-field live-cell imaging with high throughput, where
temporal phenotype changes, low contrast, noise, and motion-induced blur from
cellular movement remain major obstacles. We developed a low-cost CNN-based
pipeline incorporating comparative analysis of frozen encoders within a unified
U-Net architecture enhanced with attention mechanisms, instance-aware systems,
adaptive loss functions, hard instance retraining, dynamic learning rates,
progressive mechanisms to mitigate overfitting, and an ensemble technique. The
model was validated on a public dataset featuring diverse live cell variants,
showing consistent competitiveness with state-of-the-art methods, achieving 93%
test accuracy and an average F1-score of 89% (std. 0.07) on low-contrast,
noisy, and blurry images. Notably, the model was trained primarily on
bright-field images with limited exposure to phase-contrast microscopy (<10%),
yet it generalized effectively to the phase-contrast LIVECell dataset,
demonstrating modality, robustness and strong performance. This highlights its
potential for real-world laboratory deployment across imaging conditions. The
model requires minimal compute power and is adaptable using basic deep learning
setups such as Google Colab, making it practical for training on other cell
variants. Our pipeline outperforms existing methods in robustness and precision
for bright-field microscopy segmentation. The code and dataset are available
for reproducibility

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [212] [The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models](https://arxiv.org/abs/2508.14869)
*Hend Al-Khalifa,Raneem Almansour,Layan Abdulrahman Alhuasini,Alanood Alsaleh,Mohamad-Hani Temsah,Mohamad-Hani_Temsah,Ashwag Rafea S Alruwaili*

Main category: q-bio.NC

TL;DR: 本研究使用fMRI探讨了提示工程专家与中级用户在大语言模型交互中大脑功能连接和网络活动的神经基础。


<details>
  <summary>Details</summary>
Motivation: 提示工程作为与大语言模型有效交互的关键技能日益重要，但其认知和神经基础尚不明确。

Method: 采用横断面试点fMRI研究，比较了提示工程专家和中级用户的脑功能连接和网络活动差异。

Result: 研究发现，高提示工程素养与独特的神经特征相关，包括左侧颞中回和左侧额极等脑区的功能连接增强，以及关键认知网络中功率-频率动态的变化。

Conclusion: 这些发现为提示工程熟练度的神经生物学基础提供了初步见解，有助于设计更直观的人机交互界面，丰富大语言模型交互的认知模型，并指导开发更符合人类认知流程的AI系统。

Abstract: Prompt engineering has rapidly emerged as a critical skill for effective
interaction with large language models (LLMs). However, the cognitive and
neural underpinnings of this expertise remain largely unexplored. This paper
presents findings from a cross-sectional pilot fMRI study investigating
differences in brain functional connectivity and network activity between
experts and intermediate prompt engineers. Our results reveal distinct neural
signatures associated with higher prompt engineering literacy, including
increased functional connectivity in brain regions such as the left middle
temporal gyrus and the left frontal pole, as well as altered power-frequency
dynamics in key cognitive networks. These findings offer initial insights into
the neurobiological basis of prompt engineering proficiency. We discuss the
implications of these neurocognitive markers in Natural Language Processing
(NLP). Understanding the neural basis of human expertise in interacting with
LLMs can inform the design of more intuitive human-AI interfaces, contribute to
cognitive models of LLM interaction, and potentially guide the development of
AI systems that better align with human cognitive workflows. This
interdisciplinary approach aims to bridge the gap between human cognition and
machine intelligence, fostering a deeper understanding of how humans learn and
adapt to complex AI systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [213] [No More Marching: Learning Humanoid Locomotion for Short-Range SE(2) Targets](https://arxiv.org/abs/2508.14098)
*Pranay Dugar,Mohitvishnu S. Gadde,Jonah Siekmann,Yesh Godse,Aayam Shrestha,Alan Fern*

Main category: cs.RO

TL;DR: 本文提出一种基于强化学习的新方法，通过引入创新的奖励函数，优化人形机器人在短距离内快速、高效地到达SE(2)目标姿态，并在仿真和硬件上均表现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的人形机器人移动方法主要优化速度跟踪而非直接姿态到达，导致在短距离SE(2)目标任务中效率低下，表现为不自然的行进式行为，无法满足实际应用中对快速、鲁棒、节能的要求。

Method: 开发了一种基于强化学习的方法，直接优化人形机器人向SE(2)目标的运动。核心是引入了一个新的“星座式”（constellation-based）奖励函数，以促进自然、高效的目标导向运动。同时，构建了一个基准测试框架，用于评估能耗、到达目标时间及步数。

Result: 所提出的方法在短距离SE(2)目标任务上持续优于标准方法，并且能够成功从仿真环境迁移到硬件平台。

Conclusion: 针对性的奖励设计对于实现实用、高效的短距离人形机器人移动至关重要，本文方法验证了其有效性。

Abstract: Humanoids operating in real-world workspaces must frequently execute
task-driven, short-range movements to SE(2) target poses. To be practical,
these transitions must be fast, robust, and energy efficient. While
learning-based locomotion has made significant progress, most existing methods
optimize for velocity-tracking rather than direct pose reaching, resulting in
inefficient, marching-style behavior when applied to short-range tasks. In this
work, we develop a reinforcement learning approach that directly optimizes
humanoid locomotion for SE(2) targets. Central to this approach is a new
constellation-based reward function that encourages natural and efficient
target-oriented movement. To evaluate performance, we introduce a benchmarking
framework that measures energy consumption, time-to-target, and footstep count
on a distribution of SE(2) goals. Our results show that the proposed approach
consistently outperforms standard methods and enables successful transfer from
simulation to hardware, highlighting the importance of targeted reward design
for practical short-range humanoid locomotion.

</details>


### [214] [Domain Translation of a Soft Robotic Arm using Conditional Cycle Generative Adversarial Network](https://arxiv.org/abs/2508.14100)
*Nilay Kushawaha,Carlo Alessi,Lorenzo Fruzzetti,Egidio Falotico*

Main category: cs.RO

TL;DR: 本文提出了一种基于条件循环生成对抗网络（CCGAN）的域迁移框架，旨在解决软体机器人深度学习模型在物理属性变化下（如材料降解）的知识迁移问题，并成功将姿态控制器从标准仿真环境自适应到高粘度环境，提高了软体机器人控制器的泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统的分析方法难以精确建模软体机器人的复杂非线性动力学。虽然深度学习提供了强大的建模能力，但其学习到的映射无法直接迁移到物理特性不同的新领域（如材料随时间降解），这限制了软体机器人控制器的适应性和泛化能力。

Method: 引入了一种基于条件循环生成对抗网络（CCGAN）的域翻译框架。该方法采用动态学习策略，将一个在标准仿真环境中训练的姿态控制器适应到粘度增加十倍的目标域。模型在两个域中都通过输入压力信号，并以相应的末端执行器位置和方向为条件进行学习。

Result: 通过在五种不同形状上的轨迹跟踪实验，以及噪声扰动和周期性测试，验证了CCGAN-GP模型能有效促进跨域技能迁移。实验结果表明，该框架在不同物理特性领域间实现了有效的知识转移。

Conclusion: 所提出的CCGAN-GP框架能够有效促进软体机器人的跨域技能迁移，为开发更具适应性和泛化能力的软体机器人控制器铺平了道路，解决了因材料降解等物理特性变化导致的域适应挑战。

Abstract: Deep learning provides a powerful method for modeling the dynamics of soft
robots, offering advantages over traditional analytical approaches that require
precise knowledge of the robot's structure, material properties, and other
physical characteristics. Given the inherent complexity and non-linearity of
these systems, extracting such details can be challenging. The mappings learned
in one domain cannot be directly transferred to another domain with different
physical properties. This challenge is particularly relevant for soft robots,
as their materials gradually degrade over time. In this paper, we introduce a
domain translation framework based on a conditional cycle generative
adversarial network (CCGAN) to enable knowledge transfer from a source domain
to a target domain. Specifically, we employ a dynamic learning approach to
adapt a pose controller trained in a standard simulation environment to a
domain with tenfold increased viscosity. Our model learns from input pressure
signals conditioned on corresponding end-effector positions and orientations in
both domains. We evaluate our approach through trajectory-tracking experiments
across five distinct shapes and further assess its robustness under noise
perturbations and periodicity tests. The results demonstrate that CCGAN-GP
effectively facilitates cross-domain skill transfer, paving the way for more
adaptable and generalizable soft robotic controllers.

</details>


### [215] [SimGenHOI: Physically Realistic Whole-Body Humanoid-Object Interaction via Generative Modeling and Reinforcement Learning](https://arxiv.org/abs/2508.14120)
*Yuhang Lin,Yijia Xie,Jiahong Xie,Yuehao Huang,Ruoyu Wang,Jiajun Lv,Yukai Ma,Xingxing Zuo*

Main category: cs.RO

TL;DR: SimGenHOI是一个结合生成模型与强化学习的统一框架，用于生成可控且物理真实的类人与物体交互动作，解决了现有方法中的物理不真实问题，并显著提高了模拟中的跟踪成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的人形-物体交互（HOI）生成方法（如扩散模型）常出现不真实的接触、穿透和非现实全身动作等伪影，阻碍了其在物理环境中的实际应用。

Method: 该研究引入SimGenHOI框架：1. 使用基于Diffusion Transformers（DiT）的生成模型，根据文本提示、物体几何、稀疏路径点和初始姿态预测关键动作，并插值为平滑运动轨迹以支持长周期生成。2. 设计了基于强化学习的接触感知全身控制策略，用于跟踪生成动作并纠正物理伪影（如穿透和足部滑动）。3. 引入了生成模型和控制策略之间的相互微调策略，以迭代提升运动真实性和跟踪鲁棒性。

Result: SimGenHOI能够生成逼真、多样且物理可信的人形-物体交互，实验证明其显著提高了模拟中的跟踪成功率，并成功实现了长周期操作任务。

Conclusion: SimGenHOI通过创新性地结合生成建模和强化学习，成功克服了现有HOI生成中的物理真实性挑战，实现了高质量、可控且在模拟环境中可执行的类人与物体交互。

Abstract: Generating physically realistic humanoid-object interactions (HOI) is a
fundamental challenge in robotics. Existing HOI generation approaches, such as
diffusion-based models, often suffer from artifacts such as implausible
contacts, penetrations, and unrealistic whole-body actions, which hinder
successful execution in physical environments. To address these challenges, we
introduce SimGenHOI, a unified framework that combines the strengths of
generative modeling and reinforcement learning to produce controllable and
physically plausible HOI. Our HOI generative model, based on Diffusion
Transformers (DiT), predicts a set of key actions conditioned on text prompts,
object geometry, sparse object waypoints, and the initial humanoid pose. These
key actions capture essential interaction dynamics and are interpolated into
smooth motion trajectories, naturally supporting long-horizon generation. To
ensure physical realism, we design a contact-aware whole-body control policy
trained with reinforcement learning, which tracks the generated motions while
correcting artifacts such as penetration and foot sliding. Furthermore, we
introduce a mutual fine-tuning strategy, where the generative model and the
control policy iteratively refine each other, improving both motion realism and
tracking robustness. Extensive experiments demonstrate that SimGenHOI generates
realistic, diverse, and physically plausible humanoid-object interactions,
achieving significantly higher tracking success rates in simulation and
enabling long-horizon manipulation tasks. Code will be released upon acceptance
on our project page: https://xingxingzuo.github.io/simgen_hoi.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [216] [SuryaBench: Benchmark Dataset for Advancing Machine Learning in Heliophysics and Space Weather Prediction](https://arxiv.org/abs/2508.14107)
*Sujit Roy,Dinesha V. Hegde,Johannes Schmude,Amy Lin,Vishal Gaur,Rohit Lal,Kshitiz Mandal,Talwinder Singh,Andrés Muñoz-Jaramillo,Kang Yang,Chetraj Pandey,Jinsu Hong,Berkay Aydin,Ryan McGranaghan,Spiridon Kasapis,Vishal Upendran,Shah Bahauddin,Daniel da Silva,Marcus Freitag,Iksha Gurung,Nikolai Pogorelov,Campbell Watson,Manil Maskey,Juan Bernabe-Moreno,Rahul Ramachandran*

Main category: astro-ph.SR

TL;DR: 本文介绍了一个高分辨率、机器学习就绪的日球物理数据集，用于推动太阳物理和空间天气预报中的AI应用。


<details>
  <summary>Details</summary>
Motivation: 旨在促进机器学习在太阳物理和空间天气预报中的应用，并通过建立统一标准的数据集来弥合太阳物理、机器学习和业务预报之间的差距。

Method: 数据集来源于NASA的太阳动力学观测站（SDO）的AIA和HMI仪器，时间跨度为2010年5月至2024年7月。为确保数据适用于机器学习任务，进行了预处理，包括航天器侧倾角校正、轨道调整、曝光归一化和退化补偿。同时，还提供了辅助的应用基准数据集，涵盖活跃区域分割、爆发预测、日冕场外推、太阳耀斑预测、太阳EUV光谱预测和太阳风速度估计等任务。

Result: 成功构建了一个统一、标准化的机器学习就绪的日球物理数据集，以及一套用于特定日球物理和空间天气任务的辅助基准数据集。

Conclusion: 该数据集的建立将有助于促进基准测试、增强研究的可重复性，并加速用于关键空间天气预测任务的AI驱动模型的开发。

Abstract: This paper introduces a high resolution, machine learning-ready heliophysics
dataset derived from NASA's Solar Dynamics Observatory (SDO), specifically
designed to advance machine learning (ML) applications in solar physics and
space weather forecasting. The dataset includes processed imagery from the
Atmospheric Imaging Assembly (AIA) and Helioseismic and Magnetic Imager (HMI),
spanning a solar cycle from May 2010 to July 2024. To ensure suitability for ML
tasks, the data has been preprocessed, including correction of spacecraft roll
angles, orbital adjustments, exposure normalization, and degradation
compensation. We also provide auxiliary application benchmark datasets
complementing the core SDO dataset. These provide benchmark applications for
central heliophysics and space weather tasks such as active region
segmentation, active region emergence forecasting, coronal field extrapolation,
solar flare prediction, solar EUV spectra prediction, and solar wind speed
estimation. By establishing a unified, standardized data collection, this
dataset aims to facilitate benchmarking, enhance reproducibility, and
accelerate the development of AI-driven models for critical space weather
prediction tasks, bridging gaps between solar physics, machine learning, and
operational forecasting.

</details>


### [217] [Surya: Foundation Model for Heliophysics](https://arxiv.org/abs/2508.14112)
*Sujit Roy,Johannes Schmude,Rohit Lal,Vishal Gaur,Marcus Freitag,Julian Kuehnert,Theodore van Kessel,Dinesha V. Hegde,Andrés Muñoz-Jaramillo,Johannes Jakubik,Etienne Vos,Kshitiz Mandal,Ata Akbari Asanjan,Joao Lucas de Sousa Almeida,Amy Lin,Talwinder Singh,Kang Yang,Chetraj Pandey,Jinsu Hong,Berkay Aydin,Thorsten Kurth,Ryan McGranaghan,Spiridon Kasapis,Vishal Upendran,Shah Bahauddin,Daniel da Silva,Nikolai V. Pogorelov,Campbell Watson,Manil Maskey,Madhulika Guhathakurta,Juan Bernabe-Moreno,Rahul Ramachandran*

Main category: astro-ph.SR

TL;DR: 提出Surya，一个面向日球物理学的通用基础模型，能从SDO多仪器观测数据中学习太阳表示，并在多种太阳现象预测任务中展现出卓越的泛化和预测能力。


<details>
  <summary>Details</summary>
Motivation: 尽管有长期高分辨率观测数据，现有日球物理模型多为特定任务，受限于稀缺标注数据，导致其在不同太阳现象间的泛化能力不足。

Method: 引入3.66亿参数的Surya基础模型，采用包含光谱门控和长短程注意力的时空Transformer架构。该模型利用SDO的多仪器观测数据（AIA和HMI产品），通过高分辨率太阳图像预测任务进行预训练，并利用自回归展开调优进一步优化。

Result: Surya在零样本评估中展示了预测太阳动力学和耀斑事件的能力。通过参数高效的LoRA微调，该模型在太阳风预测、活动区分割、太阳耀斑预测和EUV光谱等下游任务中表现出强大的性能。

Conclusion: Surya是日球物理学领域首个利用时间演进作为预训练任务的基础模型。其新颖的架构和优异的性能表明，该模型能够学习太阳演化背后的深层物理规律。

Abstract: Heliophysics is central to understanding and forecasting space weather events
and solar activity. Despite decades of high-resolution observations from the
Solar Dynamics Observatory (SDO), most models remain task-specific and
constrained by scarce labeled data, limiting their capacity to generalize
across solar phenomena. We introduce Surya, a 366M parameter foundation model
for heliophysics designed to learn general-purpose solar representations from
multi-instrument SDO observations, including eight Atmospheric Imaging Assembly
(AIA) channels and five Helioseismic and Magnetic Imager (HMI) products. Surya
employs a spatiotemporal transformer architecture with spectral gating and
long--short range attention, pretrained on high-resolution solar image
forecasting tasks and further optimized through autoregressive rollout tuning.
Zero-shot evaluations demonstrate its ability to forecast solar dynamics and
flare events, while downstream fine-tuning with parameter-efficient Low-Rank
Adaptation (LoRA) shows strong performance on solar wind forecasting, active
region segmentation, solar flare forecasting, and EUV spectra. Surya is the
first foundation model in heliophysics that uses time advancement as a pretext
task on full-resolution SDO data. Its novel architecture and performance
suggest that the model is able to learn the underlying physics behind solar
evolution.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [218] [An Improved Multi-Agent Algorithm for Cooperative and Competitive Environments by Identifying and Encouraging Cooperation among Agents](https://arxiv.org/abs/2508.14131)
*Junjie Qi,Siqi Mao,Tianyi Tan*

Main category: cs.MA

TL;DR: 提出一种通过识别和激励合作行为来改进多智能体强化学习（基于MADDPG）的算法，显著提升了智能体的团队和个体奖励。


<details>
  <summary>Details</summary>
Motivation: 现有算法在处理多智能体强化学习问题时，未能有效识别和鼓励合作行为。

Method: 在现有MADDPG算法的基础上，引入一个新参数，以增加智能体在识别到合作行为时可获得的奖励。通过在PettingZoo环境中的实验，将改进算法与MADDPG进行比较。

Result: 新算法使智能体能够获得更高的团队奖励和个体奖励。

Conclusion: 通过识别并激励多智能体环境中的合作行为，所提出的改进算法能有效提高智能体的整体表现和个体回报。

Abstract: We propose an improved algorithm by identifying and encouraging cooperative
behavior in multi-agent environments. First, we analyze the shortcomings of
existing algorithms in addressing multi-agent reinforcement learning problems.
Then, based on the existing algorithm MADDPG, we introduce a new parameter to
increase the reward that an agent can obtain when cooperative behavior among
agents is identified. Finally, we compare our improved algorithm with MADDPG in
environments from PettingZoo. The results show that the new algorithm helps
agents achieve both higher team rewards and individual rewards.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [219] [The Statistical Validation of Innovation Lens](https://arxiv.org/abs/2508.14139)
*Giacomo Radaelli,Jonah Lynch*

Main category: cs.DL

TL;DR: 本研究通过训练一个分类器，成功预测了高引用研究论文，从而为科学发现可能存在内在结构提供了统计证据。


<details>
  <summary>Details</summary>
Motivation: 由于信息过载和科学进步的快速发展，评估和分配新研究提案的资源变得日益困难。研究旨在探讨科学发现是否存在某种结构，以指导此类决策。

Method: 通过训练一个分类器来实现预测。

Result: 该分类器成功预测了2010-2024年间计算机科学、物理学和PubMed领域的高引用研究论文，提供了科学发现存在结构的统计证据。

Conclusion: 科学发现中存在可预测的结构，这可以为新研究提案的评估和资源分配提供信息。

Abstract: Information overload and the rapid pace of scientific advancement make it
increasingly difficult to evaluate and allocate resources to new research
proposals. Is there a structure to scientific discovery that could inform such
decisions? We present statistical evidence for such structure, by training a
classifier that successfully predicts high-citation research papers between
2010-2024 in the Computer Science, Physics, and PubMed domains.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [220] [PAPPL: Personalized AI-Powered Progressive Learning Platform](https://arxiv.org/abs/2508.14109)
*Shayan Bafandkar,Sungyong Chung,Homa Khosravian,Alireza Talebpour*

Main category: cs.CY

TL;DR: 针对工程教育个性化学习不足的现状，本文提出了PAPPL平台，一个基于GPT-4o的AI智能辅导系统，旨在提供适应性强的个性化学习体验，并为教师提供教学分析。


<details>
  <summary>Details</summary>
Motivation: 传统的工程教育框架僵化，忽视学生多元学习需求和兴趣，且在采纳在线及个性化教育创新方面落后。现有评估方法也未能满足个体化学习体验，阻碍了学生个性化发展。

Method: 本文介绍了“个性化AI驱动渐进式学习 (PAPPL)”平台，一个专为工程教育设计的先进智能辅导系统 (ITS)。该平台是一个可扩展、数据驱动的辅导环境，集成了专家、学生、导师模块和用户界面，并利用GPT-4o大语言模型根据学生互动提供情境化和教学上合理的提示。

Result: PAPPL系统能够记录学生尝试、检测常见误解，并生成渐进式、有针对性的反馈，提供动态适应学生学习档案的个性化帮助。此外，它还为教师提供详细分析数据，支持基于证据的教学策略调整。

Conclusion: 本研究为可扩展到所有教育阶段的生成式智能辅导系统提供了一个基础框架，为个性化渐进式学习以及生成式AI在教育领域的更广泛应用提供了重要视角。

Abstract: Engineering education has historically been constrained by rigid,
standardized frameworks, often neglecting students' diverse learning needs and
interests. While significant advancements have been made in online and
personalized education within K-12 and foundational sciences, engineering
education at both undergraduate and graduate levels continues to lag in
adopting similar innovations. Traditional evaluation methods, such as exams and
homework assignments, frequently overlook individual student requirements,
impeding personalized educational experiences. To address these limitations,
this paper introduces the Personalized AI-Powered Progressive Learning (PAPPL)
platform, an advanced Intelligent Tutoring System (ITS) designed specifically
for engineering education. It highlights the development of a scalable,
data-driven tutoring environment leveraging cutting-edge AI technology to
enhance personalized learning across diverse academic disciplines, particularly
in STEM fields. PAPPL integrates core ITS components including the expert
module, student module, tutor module, and user interface, and utilizes GPT-4o,
a sophisticated large language model (LLM), to deliver context-sensitive and
pedagogically sound hints based on students' interactions. The system uniquely
records student attempts, detects recurring misconceptions, and generates
progressively targeted feedback, providing personalized assistance that adapts
dynamically to each student's learning profile. Additionally, PAPPL offers
instructors detailed analytics, empowering evidence-based adjustments to
teaching strategies. This study provides a fundamental framework for the
progression of Generative ITSs scalable to all education levels, delivering
important perspectives on personalized progressive learning and the wider
possibilities of Generative AI in the field of education.

</details>


### [221] [Enriching Moral Perspectives on AI: Concepts of Trust amongst Africans](https://arxiv.org/abs/2508.14116)
*Lameck Mbangula Amugongo,Nicola J Bidwell,Joseph Mwatukange*

Main category: cs.CY

TL;DR: 本研究调查了非洲AI专业人士如何理解AI信任，发现他们的看法受教育、流动性和原籍国等因素影响，并融入了非洲关系主义的视角。


<details>
  <summary>Details</summary>
Motivation: AI的信任度对其应用至关重要，但现有研究多集中于西方社会，非洲地区关于AI信任的研究不足且缺乏当地专业人士的视角，未能反映当地独特的社会和文化语境。

Method: 本研究通过问卷调查了157位来自25个非洲国家、对AI有专业或教育兴趣的人士，以探究他们对AI信任的理解方式。

Result: 受访者的教育背景、跨国流动性和原籍国影响了他们对AI系统的担忧、对特定AI应用的不信任程度以及对信任原则的侧重。他们普遍强调社区价值观和集体关系优先于个人自由，并能将非洲关系主义的细微差别融入国际话语中的信任概念（如可靠性）。

Conclusion: 本探索性研究表明，非洲AI专业人士对信任的理解受多重因素影响，并融入了独特的非洲关系主义视角。这激励未来应开展更多关于AI在非洲社会现实中如何被实践、体验和治理的实证研究。

Abstract: The trustworthiness of AI is considered essential to the adoption and
application of AI systems. However, the meaning of trust varies across
industry, research and policy spaces. Studies suggest that professionals who
develop and use AI regard an AI system as trustworthy based on their personal
experiences and social relations at work. Studies about trust in AI and the
constructs that aim to operationalise trust in AI (e.g., consistency,
reliability, explainability and accountability). However, the majority of
existing studies about trust in AI are situated in Western, Educated,
Industrialised, Rich and Democratic (WEIRD) societies. The few studies about
trust and AI in Africa do not include the views of people who develop, study or
use AI in their work. In this study, we surveyed 157 people with professional
and/or educational interests in AI from 25 African countries, to explore how
they conceptualised trust in AI. Most respondents had links with workshops
about trust and AI in Africa in Namibia and Ghana. Respondents' educational
background, transnational mobility, and country of origin influenced their
concerns about AI systems. These factors also affected their levels of distrust
in certain AI applications and their emphasis on specific principles designed
to foster trust. Respondents often expressed that their values are guided by
the communities in which they grew up and emphasised communal relations over
individual freedoms. They described trust in many ways, including applying
nuances of Afro-relationalism to constructs in international discourse, such as
reliability and reliance. Thus, our exploratory study motivates more empirical
research about the ways trust is practically enacted and experienced in African
social realities of AI design, use and governance.

</details>


### [222] [Documenting Deployment with Fabric: A Repository of Real-World AI Governance](https://arxiv.org/abs/2508.14119)
*Mackenzie Jorgensen,Kendall Brogle,Katherine M. Collins,Lujain Ibrahim,Arina Shah,Petra Ivanovic,Noah Broestl,Gabriel Piles,Paul Dongha,Hatim Abdulhussein,Adrian Weller,Jillian Powers,Umang Bhatt*

Main category: cs.CY

TL;DR: Fabric是一个关于已部署AI用例的公开存储库，旨在通过访谈和图表分析实际的AI治理机制、监督空白和人类监督模式。


<details>
  <summary>Details</summary>
Motivation: 现有AI部署研究主要关注风险和危害，本研究旨在填补空白，识别并分析已部署AI的实际治理机制和保障措施。

Method: 通过对实践者的半结构化访谈，收集了20个AI用例，并与实践者共同设计AI工作流程图。构建了Fabric存储库，其中包含AI用例的可视化图表和系统描述，用于分析监督机制和防护措施。

Result: 利用Fabric存储库，研究发现了治理中的空白，并识别了已部署AI系统中人类监督的常见模式。

Conclusion: Fabric旨在成为一个可扩展、不断发展的工具，供研究人员研究AI治理的有效性。

Abstract: Artificial intelligence (AI) is increasingly integrated into society, from
financial services and traffic management to creative writing. Academic
literature on the deployment of AI has mostly focused on the risks and harms
that result from the use of AI. We introduce Fabric, a publicly available
repository of deployed AI use cases to outline their governance mechanisms.
Through semi-structured interviews with practitioners, we collect an initial
set of 20 AI use cases. In addition, we co-design diagrams of the AI workflow
with the practitioners. We discuss the oversight mechanisms and guardrails used
in practice to safeguard AI use. The Fabric repository includes visual diagrams
of AI use cases and descriptions of the deployed systems. Using the repository,
we surface gaps in governance and find common patterns in human oversight of
deployed AI systems. We intend for Fabric to serve as an extendable, evolving
tool for researchers to study the effectiveness of AI governance.

</details>


### [223] [Incident Analysis for AI Agents](https://arxiv.org/abs/2508.14231)
*Carson Ezell,Xavier Roberts-Gaal,Alan Chan*

Main category: cs.CY

TL;DR: 随着AI代理的普及，危害事件增多，现有报告机制不足。本文提出了一个AI代理事件分析框架，识别潜在致因和所需信息，并就事件报告和数据保留提供了建议，以助风险管理。


<details>
  <summary>Details</summary>
Motivation: AI代理的广泛部署导致其引发的危害事件（如提示注入导致数据泄露）日益增多。现有事件报告流程主要依赖公开数据，无法捕获代理的思维链等敏感而关键的信息，因此不足以深入理解事件成因并有效预防。

Method: 本文借鉴系统安全方法，提出了一个AI代理事件分析框架。该框架将事件致因分为三类：系统相关（如训练数据）、上下文相关（如提示注入）和认知相关（如误解请求）。同时，识别了澄清事件因素所需的具体信息，包括活动日志、系统文档和代理工具信息。

Result: 基于所提出的框架，研究给出了两方面建议：1) 事件报告应包含的信息类型；2) 开发者和部署者应保留并应调查人员要求提供的关键信息。

Conclusion: 在AI代理日益普及的背景下，理解AI代理事件对于有效管理风险至关重要。本文的框架和建议为改进事件分析和报告奠定了基础。

Abstract: As AI agents become more widely deployed, we are likely to see an increasing
number of incidents: events involving AI agent use that directly or indirectly
cause harm. For example, agents could be prompt-injected to exfiltrate private
information or make unauthorized purchases. Structured information about such
incidents (e.g., user prompts) can help us understand their causes and prevent
future occurrences. However, existing incident reporting processes are not
sufficient for understanding agent incidents. In particular, such processes are
largely based on publicly available data, which excludes useful, but
potentially sensitive, information such as an agent's chain of thought or
browser history. To inform the development of new, emerging incident reporting
processes, we propose an incident analysis framework for agents. Drawing on
systems safety approaches, our framework proposes three types of factors that
can cause incidents: system-related (e.g., CBRN training data), contextual
(e.g., prompt injections), and cognitive (e.g., misunderstanding a user
request). We also identify specific information that could help clarify which
factors are relevant to a given incident: activity logs, system documentation
and access, and information about the tools an agent uses. We provide
recommendations for 1) what information incident reports should include and 2)
what information developers and deployers should retain and make available to
incident investigators upon request. As we transition to a world with more
agents, understanding agent incidents will become increasingly crucial for
managing risks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [224] [Minimizing Task-Oriented Age of Information for Remote Monitoring with Pre-Identification](https://arxiv.org/abs/2508.14575)
*Shuying Gan,Xijun Wang,Chao Xu,Xiang Chen*

Main category: cs.IT

TL;DR: 引入任务导向信息年龄（TAoI）新度量，并基于SMDP/MDP为无线监控系统设计了优化的门限传输策略，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 在任务导向通信范式中，缺乏一个全面、通用且实用的度量标准来评估信息内容与系统任务的相关性，这限制了该范式的潜力。

Method: 1. 提出任务导向信息年龄（TAoI）作为衡量信息内容与系统任务相关性的新度量。2. 将动态传输问题建模为半马尔可夫决策过程（SMDP），并转换为等效的马尔可夫决策过程（MDP），以最小化TAoI并确定最优传输策略。3. 提出一种低复杂度的相对值迭代算法来推导最优的门限传输策略。4. 引入一种简化的单门限策略以实现更快的收敛。

Result: 1. 分析表明最优传输策略是基于TAoI的门限策略。2. 实验和仿真验证了所提出的最优传输策略相比两种现有基线方法具有优越的性能。

Conclusion: 所提出的TAoI度量和基于此设计的门限传输策略，能有效协助系统高效完成指定任务，并通过实验证明其在无线监控系统中的优越性，且最优策略是门限式的，所提出的算法具有实用价值。

Abstract: The emergence of new intelligent applications has fostered the development of
a task-oriented communication paradigm, where a comprehensive, universal, and
practical metric is crucial for unleashing the potential of this paradigm. To
this end, we introduce an innovative metric, the Task-oriented Age of
Information (TAoI), to measure whether the content of information is relevant
to the system task, thereby assisting the system in efficiently completing
designated tasks. We apply TAoI to a wireless monitoring system tasked with
identifying targets and transmitting their images for subsequent analysis. To
minimize TAoI and determine the optimal transmission policy, we formulate the
dynamic transmission problem as a Semi-Markov Decision Process (SMDP) and
transform it into an equivalent Markov Decision Process (MDP). Our analysis
demonstrates that the optimal policy is threshold-based with respect to TAoI.
Building on this, we propose a low-complexity relative value iteration
algorithm tailored to this threshold structure to derive the optimal
transmission policy. Additionally, we introduce a simpler single-threshold
policy, which, despite a slight performance degradation, offers faster
convergence. Comprehensive experiments and simulations validate the superior
performance of our optimal transmission policy compared to two established
baseline approaches.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [225] [Non-Dissipative Graph Propagation for Non-Local Community Detection](https://arxiv.org/abs/2508.14097)
*William Leeney,Alessio Gravina,Davide Bacciu*

Main category: cs.SI

TL;DR: uAGNN，一种基于非耗散动力系统和反对称权重矩阵的无监督图神经网络，通过有效传播长距离信息，显著提高了异质图上的社区检测性能。


<details>
  <summary>Details</summary>
Motivation: 在异质图中，相似节点往往距离较远，传统图神经网络因其局部消息传递机制难以有效进行社区检测。研究动机在于解决如何在异质图中通过传播长距离信息来提升社区检测能力。

Method: 提出无监督反对称图神经网络（uAGNN）。该方法利用非耗散动力系统确保稳定性并有效传播长距离信息，同时通过采用反对称权重矩阵捕获局部和全局图结构，从而克服异质性带来的限制。

Result: 在十个数据集上的广泛实验表明，uAGNN 在高和中等异质性设置下表现出卓越的性能，尤其在传统方法无法利用长距离依赖的情况下优势显著。

Conclusion: uAGNN 作为一个强大的无监督社区检测工具，在多样化的图环境中具有巨大潜力，尤其擅长处理异质图，解决了现有方法在长距离信息传播上的不足。

Abstract: Community detection in graphs aims to cluster nodes into meaningful groups, a
task particularly challenging in heterophilic graphs, where nodes sharing
similarities and membership to the same community are typically distantly
connected. This is particularly evident when this task is tackled by graph
neural networks, since they rely on an inherently local message passing scheme
to learn the node representations that serve to cluster nodes into communities.
In this work, we argue that the ability to propagate long-range information
during message passing is key to effectively perform community detection in
heterophilic graphs. To this end, we introduce the Unsupervised Antisymmetric
Graph Neural Network (uAGNN), a novel unsupervised community detection approach
leveraging non-dissipative dynamical systems to ensure stability and to
propagate long-range information effectively. By employing antisymmetric weight
matrices, uAGNN captures both local and global graph structures, overcoming the
limitations posed by heterophilic scenarios. Extensive experiments across ten
datasets demonstrate uAGNN's superior performance in high and medium
heterophilic settings, where traditional methods fail to exploit long-range
dependencies. These results highlight uAGNN's potential as a powerful tool for
unsupervised community detection in diverse graph environments.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [226] [FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering](https://arxiv.org/abs/2508.14052)
*Chanyeol Choi,Jihoon Kwon,Alejandro Lopez-Lira,Chaewoon Kim,Minjae Kim,Juneha Hwang,Jaeseon Ha,Hojun Choi,Suyeol Yun,Yongjin Kim,Yongjae Lee*

Main category: cs.IR

TL;DR: 本文介绍了FinAgentBench，首个用于评估金融领域中大语言模型多步推理（即“代理式检索”）能力的大规模基准数据集，旨在解决传统信息检索的不足，并量化分析大语言模型在该复杂任务中的行为。


<details>
  <summary>Details</summary>
Motivation: 金融领域准确的信息检索至关重要，但传统检索方法在捕获语义相似性、进行文档结构和领域知识的细粒度推理方面表现不佳。尽管大型语言模型（LLMs）为多步推理检索提供了新机会，但金融领域缺乏评估其此类能力的基准。

Method: 引入了FinAgentBench，包含3,429个基于S&P-100上市公司、由专家标注的示例。该基准评估LLM代理是否能（1）识别最相关的文档类型，和（2）精确定位选定文档中的关键段落。评估框架明确分离这两个推理步骤以克服上下文限制。研究中评估了一系列先进模型，并展示了目标微调如何显著提升代理式检索性能。

Result: FinAgentBench为理解金融领域以检索为中心的大语言模型行为提供了量化基础。研究表明，通过对最先进模型的评估，以及有针对性的微调，可以显著提高代理式检索的性能。

Conclusion: FinAgentBench为研究金融领域复杂、特定领域任务中以检索为中心的大语言模型行为奠定了基础。作者计划在论文接受后公开发布数据集，并将其扩展至S&P 500及更广范围。

Abstract: Accurate information retrieval (IR) is critical in the financial domain,
where investors must identify relevant information from large collections of
documents. Traditional IR methods-whether sparse or dense-often fall short in
retrieval accuracy, as it requires not only capturing semantic similarity but
also performing fine-grained reasoning over document structure and
domain-specific knowledge. Recent advances in large language models (LLMs) have
opened up new opportunities for retrieval with multi-step reasoning, where the
model ranks passages through iterative reasoning about which information is
most relevant to a given query. However, there exists no benchmark to evaluate
such capabilities in the financial domain. To address this gap, we introduce
FinAgentBench, the first large-scale benchmark for evaluating retrieval with
multi-step reasoning in finance -- a setting we term agentic retrieval. The
benchmark consists of 3,429 expert-annotated examples on S&P-100 listed firms
and assesses whether LLM agents can (1) identify the most relevant document
type among candidates, and (2) pinpoint the key passage within the selected
document. Our evaluation framework explicitly separates these two reasoning
steps to address context limitations. This design enables to provide a
quantitative basis for understanding retrieval-centric LLM behavior in finance.
We evaluate a suite of state-of-the-art models and further demonstrated how
targeted fine-tuning can significantly improve agentic retrieval performance.
Our benchmark provides a foundation for studying retrieval-centric LLM behavior
in complex, domain-specific tasks for finance. We will release the dataset
publicly upon acceptance of the paper and plan to expand and share dataset for
the full S&P 500 and beyond.

</details>


### [227] [Dual-Phase Playtime-guided Recommendation: Interest Intensity Exploration and Multimodal Random Walks](https://arxiv.org/abs/2508.14058)
*Jingmao Zhang,Zhiting Zhao,Yunqi Lin,Jianghong Ma,Tianjun Wei,Haijun Zhang,Xiaofeng Zhang*

Main category: cs.IR

TL;DR: 本文提出DP2Rec，一个双阶段游玩时间引导的推荐模型，通过整合游玩时间信号和多模态信息，旨在同时优化游戏推荐的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 游戏行业快速发展，现有推荐系统亟需提升。现有模型未充分利用游戏特有的“游玩时间”这一丰富行为信号，且忽视了多模态信息提升推荐多样性的潜力。

Method: DP2Rec模型包含两个核心模块：1. 游玩时间引导的兴趣强度探索模块：通过双Beta建模分离用户强弱偏好，实现精细化用户画像，提升准确性。2. 游玩时间引导的多模态随机游走模块：利用游玩时间兴趣相似性和多模态语义相似性引导玩家探索，在保留核心偏好的同时，通过语义关联和类别平衡促进跨类别发现，提升多样性。

Result: 在真实游戏数据集上的大量实验表明，DP2Rec在推荐准确性和多样性方面均优于现有方法。

Conclusion: DP2Rec成功解决了现有游戏推荐系统在利用游玩时间信号和多模态信息方面的不足，有效提升了推荐的准确性和多样性，为游戏行业提供了更优的推荐解决方案。

Abstract: The explosive growth of the video game industry has created an urgent need
for recommendation systems that can scale with expanding catalogs and maintain
user engagement. While prior work has explored accuracy and diversity in
recommendations, existing models underutilize playtime, a rich behavioral
signal unique to gaming platforms, and overlook the potential of multimodal
information to enhance diversity. In this paper, we propose DP2Rec, a novel
Dual-Phase Playtime-guided Recommendation model designed to jointly optimize
accuracy and diversity. First, we introduce a playtime-guided interest
intensity exploration module that separates strong and weak preferences via
dual-beta modeling, enabling fine-grained user profiling and more accurate
recommendations. Second, we present a playtime-guided multimodal random walks
module that simulates player exploration using transitions guided by both
playtime-derived interest similarity and multimodal semantic similarity. This
mechanism preserves core preferences while promoting cross-category discovery
through latent semantic associations and adaptive category balancing. Extensive
experiments on a real-world game dataset show that DP2Rec outperforms existing
methods in both recommendation accuracy and diversity.

</details>


### [228] [A Multi-Agent Approach to Neurological Clinical Reasoning](https://arxiv.org/abs/2508.14063)
*Moran Sorka,Alon Gorenshtein,Dvir Aran,Shahar Shelly*

Main category: cs.IR

TL;DR: 本研究评估了大型语言模型（LLMs）在神经内科推理方面的表现，并开发了一个多智能体系统。结果表明，多智能体框架通过模拟认知功能，显著优于基础LLMs和RAG，尤其在处理复杂神经推理任务时表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在医疗领域显示出潜力，但它们处理专业神经内科推理的能力尚未得到系统性评估。当前的LLM和检索增强生成（RAG）方法可能不足以应对复杂的神经内科任务，因此需要探索更有效的方法。

Method: 研究开发了一个包含305道以色列神经内科医师认证考试题的综合基准，并根据知识深度、概念整合和推理复杂度进行分类。评估了十个LLMs，使用了基础模型、检索增强生成（RAG）和一种新颖的多智能体系统。该多智能体系统将神经推理分解为问题分析、知识检索、答案合成和验证等认知功能。此外，研究还使用了一个独立的MedQA数据集进行验证。

Result: LLMs表现差异显著，OpenAI-o1基础性能最佳（90.9%），而专用医疗模型表现不佳（Meditron-70B为52.9%）。RAG仅带来有限的提升，对复杂推理题效果不明显。相比之下，多智能体框架取得了显著进步，尤其对中等性能模型。基于LLaMA 3.3-70B的智能体系统准确率从69.5%提升至89.2%，并在三级复杂度问题上表现出实质性提升，实现了子专业表现的统一优化。

Conclusion: 研究结果证实，旨在模拟专业认知过程的结构化多智能体方法能够显著增强复杂的医疗推理能力。这为在挑战性临床环境中提供AI辅助指明了有前景的方向。

Abstract: Large language models (LLMs) have shown promise in medical domains, but their
ability to handle specialized neurological reasoning requires systematic
evaluation. We developed a comprehensive benchmark using 305 questions from
Israeli Board Certification Exams in Neurology, classified along three
complexity dimensions: factual knowledge depth, clinical concept integration,
and reasoning complexity. We evaluated ten LLMs using base models,
retrieval-augmented generation (RAG), and a novel multi-agent system. Results
showed significant performance variation. OpenAI-o1 achieved the highest base
performance (90.9% accuracy), while specialized medical models performed poorly
(52.9% for Meditron-70B). RAG provided modest benefits but limited
effectiveness on complex reasoning questions. In contrast, our multi-agent
framework, decomposing neurological reasoning into specialized cognitive
functions including question analysis, knowledge retrieval, answer synthesis,
and validation, achieved dramatic improvements, especially for mid-range
models. The LLaMA 3.3-70B-based agentic system reached 89.2% accuracy versus
69.5% for its base model, with substantial gains on level 3 complexity
questions. The multi-agent approach transformed inconsistent subspecialty
performance into uniform excellence, addressing neurological reasoning
challenges that persisted with RAG enhancement. We validated our approach using
an independent dataset of 155 neurological cases from MedQA. Results confirm
that structured multi-agent approaches designed to emulate specialized
cognitive processes significantly enhance complex medical reasoning, offering
promising directions for AI assistance in challenging clinical contexts.

</details>


### [229] [An automatic patent literature retrieval system based on LLM-RAG](https://arxiv.org/abs/2508.14064)
*Yao Ding,Yuqing Wu,Ziyang Ding*

Main category: cs.IR

TL;DR: 本研究提出一个结合LLM和RAG的自动化专利检索框架，在专利数据集上实现了高准确率和召回率，显著优于基线LLM方法，并对跨领域任务展现出强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统关键词和规则专利检索方法难以处理复杂查询意图或捕捉跨领域语义关联，导致结果不完整且相关性低。在技术创新加速背景下，高效专利检索对知识产权管理和企业研发至关重要。

Method: 该研究提出一个集成大型语言模型（LLMs）与检索增强生成（RAG）技术的自动化专利检索框架，包含三个模块：1) 专利数据预处理；2) 利用LLM生成嵌入的高效向量检索引擎；3) 结合外部文档检索和上下文感知响应生成的RAG增强查询模块。

Result: 在Google专利数据集（2006-2024）上评估，gpt35turbo0125RAG配置实现了80.5%的语义匹配准确率和92.1%的召回率，比基线LLM方法高出28个百分点。该框架在跨领域分类和语义聚类任务中也表现出强大的泛化能力。

Conclusion: LLM与RAG的集成验证了智能专利检索的有效性，为下一代AI驱动的知识产权分析平台奠定了基础。

Abstract: With the acceleration of technological innovation efficient retrieval and
classification of patent literature have become essential for intellectual
property management and enterprise RD Traditional keyword and rulebased
retrieval methods often fail to address complex query intents or capture
semantic associations across technical domains resulting in incomplete and
lowrelevance results This study presents an automated patent retrieval
framework integrating Large Language Models LLMs with RetrievalAugmented
Generation RAG technology The system comprises three components: 1) a
preprocessing module for patent data standardization, 2) a highefficiency
vector retrieval engine leveraging LLMgenerated embeddings, and 3) a
RAGenhanced query module that combines external document retrieval with
contextaware response generation Evaluations were conducted on the Google
Patents dataset 20062024 containing millions of global patent records with
metadata such as filing date domain and status The proposed gpt35turbo0125RAG
configuration achieved 805 semantic matching accuracy and 92.1% recall
surpassing baseline LLM methods by 28 percentage points The framework also
demonstrated strong generalization in crossdomain classification and semantic
clustering tasks These results validate the effectiveness of LLMRAG integration
for intelligent patent retrieval providing a foundation for nextgeneration
AIdriven intellectual property analysis platforms

</details>


### [230] [Retrieval-Augmented Generation in Industry: An Interview Study on Use Cases, Requirements, Challenges, and Evaluation](https://arxiv.org/abs/2508.14066)
*Lorenz Brehme,Benedikt Dornauer,Thomas Ströhle,Maximilian Ehrhart,Ruth Breu*

Main category: cs.IR

TL;DR: 分析RAG在工业界的实际应用现状、挑战及需求。


<details>
  <summary>Details</summary>
Motivation: RAG在工业界的应用日益增多，但缺乏对其在实际工业场景中应用的研究，本研究旨在填补这一空白。

Method: 对13位行业从业者进行了半结构化访谈。

Result: 当前RAG应用主要限于领域特定问答且处于原型阶段；行业需求侧重数据保护、安全和质量，对伦理、偏见和可扩展性关注较少；数据预处理仍是主要挑战；系统评估主要由人工而非自动化方法进行。

Conclusion: RAG在工业界的实践应用尚处于早期，主要集中于领域特定问答，并面临数据预处理、人工评估等挑战。未来需在关注核心需求的同时，进一步拓展应用场景并解决伦理与可扩展性等深层问题。

Abstract: Retrieval-Augmented Generation (RAG) is a well-established and rapidly
evolving field within AI that enhances the outputs of large language models by
integrating relevant information retrieved from external knowledge sources.
While industry adoption of RAG is now beginning, there is a significant lack of
research on its practical application in industrial contexts. To address this
gap, we conducted a semistructured interview study with 13 industry
practitioners to explore the current state of RAG adoption in real-world
settings. Our study investigates how companies apply RAG in practice, providing
(1) an overview of industry use cases, (2) a consolidated list of system
requirements, (3) key challenges and lessons learned from practical
experiences, and (4) an analysis of current industry evaluation methods. Our
main findings show that current RAG applications are mostly limited to
domain-specific QA tasks, with systems still in prototype stages; industry
requirements focus primarily on data protection, security, and quality, while
issues such as ethics, bias, and scalability receive less attention; data
preprocessing remains a key challenge, and system evaluation is predominantly
conducted by humans rather than automated methods.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [231] [RAG-Boost: Retrieval-Augmented Generation Enhanced LLM-based Speech Recognition](https://arxiv.org/abs/2508.14048)
*Pengcheng Wang,Sheng Li,Takahiro Shinozaki*

Main category: eess.AS

TL;DR: 本文提出RAG-Boost系统，通过实时检索增强生成（RAG）模块，提升了基于LLM的ASR系统对MLC-SLM挑战任务的性能，通过查询音频-文本对和领域术语修正识别错误。


<details>
  <summary>Details</summary>
Motivation: 为了增强基线LLM-based ASR系统（MLC-SLM挑战任务I），解决识别错误并提高响应质量。

Method: 提出RAG-Boost系统，其核心在于一个实时的检索增强生成（RAG）模块。每个部分ASR假设会查询一个包含音频-文本对和领域术语的向量存储。检索到的结果与实时的ASR假设融合，以修正识别错误。融合后的假设随后传递给大型语言模型（LLM），以生成改进的响应。

Result: 该方法产生了改进的响应。

Conclusion: RAG-Boost系统通过实时RAG模块，有效增强了LLM-based ASR系统的识别错误修正能力，从而提升了系统性能。

Abstract: In this paper, we propose RAG-Boost (ST-ShinozakiLab Task I system), which
enhances the baseline LLM-based ASR system of the MLC-SLM Challenge (task I)
with a retrieval-augmented generation (RAG) module on the fly. Each partial ASR
hypothesis queries a vector store of audio-text pairs and domain terms, and the
retrieved results are fused with the live ASR hypotheses to fix recognition
errors. The fused hypotheses are passed to the LLM, yielding improved
responses.

</details>


### [232] [MahaTTS: A Unified Framework for Multilingual Text-to-Speech Synthesis](https://arxiv.org/abs/2508.14049)
*Jaskaran Singh,Amartya Roy Chowdhury,Raghav Prabhakar,Varshul C. W*

Main category: eess.AS

TL;DR: MahaTTS-v2是一个多语言多说话人文本转语音系统，专注于印度语言，通过结合Wav2Vec2.0、LM和CFM技术，实现了高质量的语音合成。


<details>
  <summary>Details</summary>
Motivation: 现有文本转语音模型主要针对英语和欧洲语言，限制了大量使用印度语言人群获取信息的途径，因此亟需开发针对印度语言、具有优秀多语言表达能力的TTS系统。

Method: 提出MahaTTS-v2系统，该系统在约2万小时的印度语言数据上进行训练。其方法利用Wav2Vec2.0令牌进行语义提取，使用语言模型(LM)进行文本到语义建模，并采用条件流模型(CFM)生成语义到梅尔频谱图。

Result: 实验结果表明，所提出的方法在效果上优于其他现有框架。

Conclusion: MahaTTS-v2有效解决了印度语言的多语言TTS挑战，提供了卓越的多语言表达能力，提升了信息的可及性。

Abstract: Current Text-to-Speech models pose a multilingual challenge, where most of
the models traditionally focus on English and European languages, thereby
hurting the potential to provide access to information to many more people. To
address this gap, we introduce MahaTTS-v2 a Multilingual Multi-speaker
Text-To-Speech (TTS) system that has excellent multilingual expressive
capabilities in Indic languages. The model has been trained on around 20K hours
of data specifically focused on Indian languages. Our approach leverages
Wav2Vec2.0 tokens for semantic extraction, and a Language Model (LM) for
text-to-semantic modeling. Additionally, we have used a Conditional Flow Model
(CFM) for semantics to melspectogram generation. The experimental results
indicate the effectiveness of the proposed approach over other frameworks. Our
code is available at https://github.com/dubverse-ai/MahaTTSv2

</details>


### [233] [Towards Low-Latency Tracking of Multiple Speakers With Short-Context Speaker Embeddings](https://arxiv.org/abs/2508.14115)
*Taous Iatariene,Alexandre Guérin,Romain Serizel*

Main category: eess.AS

TL;DR: 提出一种基于知识蒸馏的训练方法，用于从两人混合语音中提取短上下文说话人嵌入，并通过波束成形处理重叠，旨在改善跟踪系统中的身份重分配，并探索低延迟的块级重分配。


<details>
  <summary>Details</summary>
Motivation: 现有说话人嵌入提取器在短上下文和重叠语音下性能受限，导致依赖长时身份重分配，但这会增加跟踪系统出错的概率。因此，需要一种能有效应对短上下文和重叠语音的嵌入提取方法，以实现鲁棒且低延迟的身份重分配。

Method: 1. 提出基于知识蒸馏（KD）的训练方法，用于从两人混合语音中提取短上下文说话人嵌入。2. 利用波束成形技术，借助目标说话人的空间信息来减少语音重叠。3. 研究了固定大小块的身份重分配（块级身份重分配）的可行性，以实现低延迟的说话人嵌入跟踪系统。

Result: 1. 实验结果表明，所提出的蒸馏模型在短上下文嵌入提取方面表现有效。2. 蒸馏模型对重叠语音更具鲁棒性。3. 块级重分配的结果显示，仍需进一步工作来更有效地处理同步语音。

Conclusion: 本文提出的基于知识蒸馏的训练方法，能有效提高短上下文说话人嵌入的提取效果及其对重叠语音的鲁棒性。尽管在块级重分配中处理同步语音仍面临挑战，但本研究为构建低延迟的说话人嵌入跟踪系统提供了重要进展和未来研究方向。

Abstract: Speaker embeddings are promising identity-related features that can enhance
the identity assignment performance of a tracking system by leveraging its
spatial predictions, i.e, by performing identity reassignment. Common speaker
embedding extractors usually struggle with short temporal contexts and
overlapping speech, which imposes long-term identity reassignment to exploit
longer temporal contexts. However, this increases the probability of tracking
system errors, which in turn impacts negatively on identity reassignment. To
address this, we propose a Knowledge Distillation (KD) based training approach
for short context speaker embedding extraction from two speaker mixtures. We
leverage the spatial information of the speaker of interest using beamforming
to reduce overlap. We study the feasibility of performing identity reassignment
over blocks of fixed size, i.e., blockwise identity reassignment, to go towards
a low-latency speaker embedding based tracking system. Results demonstrate that
our distilled models are effective at short-context embedding extraction and
more robust to overlap. Although, blockwise reassignment results indicate that
further work is needed to handle simultaneous speech more effectively.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [234] [MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing](https://arxiv.org/abs/2508.14300)
*Youssef Maklad,Fares Wael,Ali Hamdi,Wael Elsersy,Khaled Shaban*

Main category: cs.CR

TL;DR: MultiFuzz是一个新型的基于密集检索的多智能体协议模糊测试系统，通过结合语义感知上下文检索和协作代理，解决了传统及现有LLM辅助模糊测试的局限性，显著提高了协议状态覆盖率和语法依从性。


<details>
  <summary>Details</summary>
Motivation: 传统协议模糊测试技术因缺乏语义理解和僵化变异策略而效果不佳。尽管ChatAFL等LLM辅助方法有所改进，但仍面临输出不可靠、LLM幻觉和对协议规范知识的假设等问题。

Method: 论文提出了MultiFuzz，一个密集检索多智能体系统。它通过将协议文档（RFC）嵌入向量数据库，实现检索增强生成（RAG），使代理能生成更可靠和结构化的输出。该框架将模糊测试分解为模块化代理组，这些代理通过思维链推理协作，基于检索到的上下文知识动态调整模糊策略。

Result: 在实时流协议（RTSP）上的实验评估表明，MultiFuzz显著提高了分支覆盖率，并比NSFuzz、AFLNet和ChatAFL等最先进的模糊器探索了更深层的协议状态和转换。

Conclusion: MultiFuzz通过结合密集检索、智能体协同和语言模型推理，建立了自主协议模糊测试的新范式，为未来智能代理式模糊测试系统的研究提供了可扩展的基础。

Abstract: Traditional protocol fuzzing techniques, such as those employed by AFL-based
systems, often lack effectiveness due to a limited semantic understanding of
complex protocol grammars and rigid seed mutation strategies. Recent works,
such as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol
fuzzing and address these limitations, pushing protocol fuzzers to wider
exploration of the protocol state space. But ChatAFL still faces issues like
unreliable output, LLM hallucinations, and assumptions of LLM knowledge about
protocol specifications. This paper introduces MultiFuzz, a novel dense
retrieval-based multi-agent system designed to overcome these limitations by
integrating semantic-aware context retrieval, specialized agents, and
structured tool-assisted reasoning. MultiFuzz utilizes agentic chunks of
protocol documentation (RFC Documents) to build embeddings in a vector database
for a retrieval-augmented generation (RAG) pipeline, enabling agents to
generate more reliable and structured outputs, enhancing the fuzzer in mutating
protocol messages with enhanced state coverage and adherence to syntactic
constraints. The framework decomposes the fuzzing process into modular groups
of agents that collaborate through chain-of-thought reasoning to dynamically
adapt fuzzing strategies based on the retrieved contextual knowledge.
Experimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate
that MultiFuzz significantly improves branch coverage and explores deeper
protocol states and transitions over state-of-the-art (SOTA) fuzzers such as
NSFuzz, AFLNet, and ChatAFL. By combining dense retrieval, agentic
coordination, and language model reasoning, MultiFuzz establishes a new
paradigm in autonomous protocol fuzzing, offering a scalable and extensible
foundation for future research in intelligent agentic-based fuzzing systems.

</details>


### [235] [CoFacS -- Simulating a Complete Factory to Study the Security of Interconnected Production](https://arxiv.org/abs/2508.14526)
*Stefan Lenz,David Schachtschneider,Simon Jonas,Liam Tirpitz,Sandra Geisler,Martin Henze*

Main category: cs.CR

TL;DR: 本文提出了CoFacS，一个完整的工厂模拟平台，能够复制整条生产线并集成真实工业应用。它已被验证具有高精度（与物理模型偏差最大0.11%），能用于端到端评估工业网络安全攻击及其防御措施，并支持对新型安全机制的研究。


<details>
  <summary>Details</summary>
Motivation: 工业工厂数字化带来严重网络攻击风险。现有安全测试平台仅关注生产线的局部，无法端到端评估网络攻击的影响及防御措施的有效性，从而阻碍了新型安全机制的研究。

Method: 开发了CoFacS（COmplete FACtory Simulation），这是一个能够复制整个生产线并集成真实工业应用的完整工厂模拟平台。通过与广泛用于安全研究的物理模型工厂进行对比，验证了CoFacS的准确性，并展示了其在攻击检测和5G工业通信抗干扰性方面的应用案例。

Result: CoFacS与物理参考模型的最大偏差仅为0.11%，这表明它能准确捕捉真实世界行为，从而支持对物理攻击或网络层面网络攻击的研究。此外，通过案例研究展示了CoFacS在攻击检测和5G工业通信对干扰的韧性研究方面的能力。

Conclusion: CoFacS作为一个高度准确且完整的工厂模拟平台，解决了现有测试平台无法进行端到端评估的问题。它为工业网络安全研究，特别是对新型安全机制、攻击影响和防御措施有效性的研究提供了重要的工具和支持。

Abstract: While the digitization of industrial factories provides tremendous
improvements for the production of goods, it also renders such systems
vulnerable to serious cyber-attacks. To research, test, and validate security
measures protecting industrial networks against such cyber-attacks, the
security community relies on testbeds to simulate industrial systems, as
utilizing live systems endangers costly components or even human life. However,
existing testbeds focus on individual parts of typically complex production
lines in industrial factories. Consequently, the impact of cyber-attacks on
industrial networks as well as the effectiveness of countermeasures cannot be
evaluated in an end-to-end manner. To address this issue and facilitate
research on novel security mechanisms, we present CoFacS, the first COmplete
FACtory Simulation that replicates an entire production line and affords the
integration of real-life industrial applications. To showcase that CoFacS
accurately captures real-world behavior, we validate it against a physical
model factory widely used in security research. We show that CoFacS has a
maximum deviation of 0.11% to the physical reference, which enables us to study
the impact of physical attacks or network-based cyber-attacks. Moreover, we
highlight how CoFacS enables security research through two cases studies
surrounding attack detection and the resilience of 5G-based industrial
communication against jamming.

</details>


### [236] [Special-Character Adversarial Attacks on Open-Source Language Model](https://arxiv.org/abs/2508.14070)
*Ephraiem Sarabamoun*

Main category: cs.CR

TL;DR: 大型语言模型（LLMs）表现出色，但对字符级对抗性攻击敏感，对实际部署构成重大安全挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在自然语言处理任务中表现卓越，但其对字符级对抗性操纵的脆弱性，给其实际部署带来了显著的安全挑战，因此需要研究和解决这一问题。

Method: 抽象中未提及具体研究方法。

Result: 抽象中未提及具体研究结果，仅指出LLMs存在字符级对抗性攻击的脆弱性。

Conclusion: 字符级对抗性攻击是大型语言模型在实际应用中面临的一个重要安全问题，需要高度关注并寻求解决方案。

Abstract: Large language models (LLMs) have achieved remarkable performance across
diverse natural language processing tasks, yet their vulnerability to
character-level adversarial manipulations presents significant security
challenges for real-world deployments.

</details>


### [237] [Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text](https://arxiv.org/abs/2508.14190)
*Zixin Rao,Youssef Mohamed,Shang Liu,Zeyan Liu*

Main category: cs.CR

TL;DR: 本文提出了DA-MTL，一个多任务学习框架，旨在同时解决大语言模型（LLM）生成文本的检测与作者归属问题，并在多语言和多LLM来源上展示了强大的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型安全对策主要集中于区分AI生成内容与人工文本，且多限于英语。然而，确定特定LLM作者的归属问题对取证分析至关重要，但关注度不足。

Method: 提出DA-MTL，一个多任务学习框架，它同时处理文本检测和作者归属任务。该框架能够捕捉各任务独特特性并共享见解，以提升双方性能。

Result: DA-MTL在九个数据集和四个骨干模型上进行了评估，在多种语言和LLM来源上均展现出强大的性能。通过任务间见解共享，提升了两任务的性能。此外，研究还分析了跨模态和跨语言模式，并验证了框架对抗性混淆技术的鲁棒性。

Conclusion: 研究结果为大语言模型行为以及文本检测和作者归属的泛化能力提供了宝贵见解，证明了DA-MTL框架在应对LLM安全和完整性挑战方面的有效性和普适性。

Abstract: Large Language Models (LLMs), such as GPT-4 and Llama, have demonstrated
remarkable abilities in generating natural language. However, they also pose
security and integrity challenges. Existing countermeasures primarily focus on
distinguishing AI-generated content from human-written text, with most
solutions tailored for English. Meanwhile, authorship attribution--determining
which specific LLM produced a given text--has received comparatively little
attention despite its importance in forensic analysis. In this paper, we
present DA-MTL, a multi-task learning framework that simultaneously addresses
both text detection and authorship attribution. We evaluate DA-MTL on nine
datasets and four backbone models, demonstrating its strong performance across
multiple languages and LLM sources. Our framework captures each task's unique
characteristics and shares insights between them, which boosts performance in
both tasks. Additionally, we conduct a thorough analysis of cross-modal and
cross-lingual patterns and assess the framework's robustness against
adversarial obfuscation techniques. Our findings offer valuable insights into
LLM behavior and the generalization of both detection and authorship
attribution.

</details>


### [238] [CCFC: Core & Core-Full-Core Dual-Track Defense for LLM Jailbreak Protection](https://arxiv.org/abs/2508.14128)
*Jiaming Hu,Haoyu Wang,Debarghya Mukherjee,Ioannis Ch. Paschalidis*

Main category: cs.CR

TL;DR: 提出CCFC（Core & Core-Full-Core），一种双轨、提示层面的防御框架，旨在缓解大语言模型（LLMs）的提示注入和结构感知越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 越狱攻击对大语言模型（LLMs）的安全部署构成了严重挑战。

Method: CCFC首先通过少样本提示隔离用户查询的语义核心，然后通过两条互补轨道进行评估：一是“仅核心”轨道以忽略对抗性干扰，二是“核心-全核心”轨道以扰乱结构模式。最终响应基于两条轨道的安全一致性检查来选择，以确保鲁棒性。

Result: CCFC将最先进防御方法面对的攻击成功率降低了50-75%，且不牺牲对良性查询的准确性，持续优于现有提示级防御。

Conclusion: CCFC为LLM的安全部署提供了一个实用且有效的解决方案。

Abstract: Jailbreak attacks pose a serious challenge to the safe deployment of large
language models (LLMs). We introduce CCFC (Core & Core-Full-Core), a
dual-track, prompt-level defense framework designed to mitigate LLMs'
vulnerabilities from prompt injection and structure-aware jailbreak attacks.
CCFC operates by first isolating the semantic core of a user query via few-shot
prompting, and then evaluating the query using two complementary tracks: a
core-only track to ignore adversarial distractions (e.g., toxic suffixes or
prefix injections), and a core-full-core (CFC) track to disrupt the structural
patterns exploited by gradient-based or edit-based attacks. The final response
is selected based on a safety consistency check across both tracks, ensuring
robustness without compromising on response quality. We demonstrate that CCFC
cuts attack success rates by 50-75% versus state-of-the-art defenses against
strong adversaries (e.g., DeepInception, GCG), without sacrificing fidelity on
benign queries. Our method consistently outperforms state-of-the-art
prompt-level defenses, offering a practical and effective solution for safer
LLM deployment.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [239] [The Hidden Cost of Readability: How Code Formatting Silently Consumes Your LLM Budget](https://arxiv.org/abs/2508.13666)
*Dangfeng Pan,Zhensu Sun,Cenyuan Zhang,David Lo,Xiaoning Du*

Main category: cs.SE

TL;DR: 研究表明，在大型语言模型（LLMs）处理代码时，移除格式化元素（如缩进、换行）可显著降低输入token数（平均24.5%）并提升效率，同时不影响性能。通过提示或微调，还能进一步缩短输出代码长度。为此，研究开发了双向代码转换工具，以兼顾人类可读性与LLM效率。


<details>
  <summary>Details</summary>
Motivation: 代码格式化对人类可读性有益，但LLMs将其视为线性token序列，可能无法从中受益，反而因额外token增加计算成本和响应时间。本研究旨在评估代码格式对LLM性能和效率的影响，以探索通过移除格式化来优化成本的可能性。

Method: 通过大规模实证研究，在Java、Python、C++、C#四种编程语言上，使用十款商业和开源LLMs执行“代码中间填充”任务。系统分析移除格式化元素后，LLM的输入/输出token数量和性能变化，并进一步探究提示工程和微调对输出代码长度的影响。为实现实际应用，开发了双向代码转换工具。

Result: LLMs在有格式和无格式代码上均能保持性能，同时输入token平均减少24.5%，输出token减少可忽略不计，证实移除代码格式是提高LLM效率的实用策略。进一步发现，通过提示或微调LLMs，可在不损害正确性的前提下，将输出代码长度缩短高达36.1%。

Conclusion: 移除代码格式是提高LLM处理代码效率的有效且实用方法，能显著减少计算成本而不影响模型性能。通过提示和微调可进一步优化输出长度。所开发的双向代码转换工具可无缝集成到LLM推理工作流中，兼顾代码的人类可读性和LLM的效率。

Abstract: Source code is usually formatted with elements like indentation and newlines
to improve readability for human developers. However, these visual aids do not
seem to be beneficial for large language models (LLMs) in the same way since
the code is processed as a linear sequence of tokens. Furthermore, these
additional tokens can lead to increased computational costs and longer response
times for LLMs. If such formatting elements are non-essential to LLMs, we can
reduce such costs by removing them from the code. To figure out the role played
by formatting elements, we conduct a comprehensive empirical study to evaluate
the impact of code formatting on LLM performance and efficiency. Through
large-scale experiments on Fill-in-the-Middle Code Completion tasks across four
programming languages (Java, Python, C++, C\#) and ten LLMs-including both
commercial and open-source models-we systematically analyze token count and
performance when formatting elements are removed. Key findings indicate that
LLMs can maintain performance across formatted code and unformatted code,
achieving an average input token reduction of 24.5\% with negligible output
token reductions. This makes code format removal a practical optimization
strategy for improving LLM efficiency. Further exploration reveals that both
prompting and fine-tuning LLMs can lead to significant reductions (up to
36.1\%) in output code length without compromising correctness. To facilitate
practical applications, we develop a bidirectional code transformation tool for
format processing, which can be seamlessly integrated into existing LLM
inference workflows, ensuring both human readability and LLM efficiency.

</details>


### [240] [You Don't Know Until You Click:Automated GUI Testing for Production-Ready Software Evaluation](https://arxiv.org/abs/2508.14104)
*Yutong Bian,Xianhao Lin,Yupeng Xie,Tianyang Liu,Mingchen Zhuge,Siyuan Lu,Haoming Tang,Jinlin Wang,Jiayi Zhang,Jiaqi Chen,Xiangru Tang,Yongxin Ni,Sirui Hong,Chenglin Wu*

Main category: cs.SE

TL;DR: 新框架RealDevWorld通过模拟GUI交互，自动评估LLM生成的可交互生产级软件，实现高准确性和人类一致性。


<details>
  <summary>Details</summary>
Motivation: 现有评估大型语言模型（LLMs）生成生产级软件（特别是带GUI的交互式应用）的基准不足，无法捕获运行时交互行为和动态，导致无法有效评估实际可用性。

Method: 提出RealDevWorld评估框架，包含：1. RealDevBench：一个包含194个跨领域、多模态的开放式软件工程任务集；2. AppEvalPilot：一个“智能体即评判者”的评估系统，模拟真实GUI用户交互，自动、全面评估软件的功能正确性、视觉保真度和运行时行为。

Result: RealDevWorld提供了有效、自动化且与人类评估高度一致的评估结果，其准确率达到0.92，与专家人工评估的相关性达到0.85，并显著减少了对人工审查的依赖。

Conclusion: RealDevWorld框架能够实现对LLMs生成生产级软件的可扩展、与人类评估对齐的自动化评估。

Abstract: Large Language Models (LLMs) and code agents in software development are
rapidly evolving from generating isolated code snippets to producing
full-fledged software applications with graphical interfaces, interactive
logic, and dynamic behaviors. However, current benchmarks fall short in
evaluating such production-ready software, as they often rely on static checks
or binary pass/fail scripts, failing to capture the interactive behaviors and
runtime dynamics that define real-world usability - qualities that only emerge
when an application is actively used. This is the blind spot of current
evaluation: you don't know if an app works until you click through it, interact
with it, and observe how it responds. To bridge this gap, we introduce
RealDevWorld, a novel evaluation framework for automated end-to-end assessment
of LLMs' ability to generate production-ready repositories from scratch. It
features two key components: (1) RealDevBench, a diverse collection of 194
open-ended software engineering tasks across multiple domains, incorporating
multimodal elements to reflect real-world complexity; and (2) AppEvalPilot, a
new agent-as-a-judge evaluation system that simulates realistic, GUI-based user
interactions to automatically and holistically assess software functional
correctness, visual fidelity, and runtime behavior. The framework delivers
fine-grained, task-specific diagnostic feedback, supporting nuanced evaluation
beyond simple success/failure judgments. Empirical results show that
RealDevWorld delivers effective, automatic, and human-aligned evaluations,
achieving an accuracy of 0.92 and a correlation of 0.85 with expert human
assessments, while significantly reducing the reliance on manual review. This
enables scalable, human-aligned assessment of production-level software
generated by LLMs. Our code is available on GitHub.

</details>


### [241] [Measuring LLM Code Generation Stability via Structural Entropy](https://arxiv.org/abs/2508.14288)
*Yewei Song,Tiezhu Sun,Xunzhu Tang,Prateek Rajput,Tegawende F. Bissyande,Jacques Klein*

Main category: cs.SE

TL;DR: 该研究提出一种基于抽象语法树（AST）分析和结构熵的新方法，用于评估大型语言模型（LLMs）生成代码的稳定性。该方法无需参考代码、与语言无关且不依赖执行，能有效揭示模型的一致性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）代码生成结果的稳定性对于判断其在实际开发中的可靠性至关重要。

Method: 该研究将“结构熵概念”扩展到程序领域，结合抽象语法树（AST）分析。具体做法是：对于固定提示，收集每个生成程序中深度受限的AST子树的多重集，并将其相对频率视为概率分布。然后，通过詹森-香农散度（Jensen-Shannon divergence）和结构交叉熵比率（Structural Cross-Entropy ratio）两种方式衡量稳定性。这两种度量都支持仅结构和标记感知变体。该方法与传统度量（如pass@k、BLEU、CodeBLEU）不同，无需参考代码、与语言无关且不依赖执行。

Result: 通过对多个领先LLMs在标准代码生成任务上进行基准测试，结果表明基于AST的结构熵能揭示模型一致性和鲁棒性方面的细微差别。该方法运行时间复杂度为O(n,d)，无需外部测试，是一种轻量级的代码生成评估工具。

Conclusion: 该研究提出的基于AST的结构熵方法是代码生成评估工具包的轻量级补充，能够有效、高效地评估LLMs生成代码的稳定性、一致性和鲁棒性，且具有无需参考、与语言无关、不依赖执行的优势。

Abstract: Assessing the stability of code generation from large language models (LLMs)
is essential for judging their reliability in real-world development. We extend
prior "structural-entropy concepts" to the program domain by pairing entropy
with abstract syntax tree (AST) analysis. For any fixed prompt, we collect the
multiset of depth-bounded subtrees of AST in each generated program and treat
their relative frequencies as a probability distribution. We then measure
stability in two complementary ways: (i) Jensen-Shannon divergence, a
symmetric, bounded indicator of structural overlap, and (ii) a Structural
Cross-Entropy ratio that highlights missing high-probability patterns. Both
metrics admit structural-only and token-aware variants, enabling separate views
on control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or
CodeBLEU, our metrics are reference-free, language-agnostic, and
execution-independent. We benchmark several leading LLMs on standard code
generation tasks, demonstrating that AST-driven structural entropy reveals
nuances in model consistency and robustness. The method runs in O(n,d) time
with no external tests, providing a lightweight addition to the code-generation
evaluation toolkit.

</details>


### [242] [Ambiguity Resolution with Human Feedback for Code Writing Tasks](https://arxiv.org/abs/2508.14114)
*Aditey Nandan,Viraj Kumar*

Main category: cs.SE

TL;DR: 本文提出并评估了一种基于ARHF（通过人工反馈解决歧义）的新型系统，该系统能识别自然语言代码规范中的歧义，通过收集有限的人工反馈来解决它们，并生成明确的代码。


<details>
  <summary>Details</summary>
Motivation: 代码编写任务的规范通常以自然语言表达，可能存在歧义，要求程序员识别并解决这些歧义。

Method: 开发了一个基于ARHF（通过人工反馈解决歧义）的系统原型。该系统能够：1) 建议可能存在歧义的特定输入；2) 寻求对代码在这些输入上期望行为的有限人工反馈；3) 利用此反馈生成解决这些歧义的代码。

Result: 对原型系统的有效性进行了评估。文章还讨论了此类辅助系统对计算机科学教育的意义。

Conclusion: 该系统能有效帮助解决代码规范中的歧义，并对计算机科学教育具有潜在影响。

Abstract: Specifications for code writing tasks are usually expressed in natural
language and may be ambiguous. Programmers must therefore develop the ability
to recognize ambiguities in task specifications and resolve them by asking
clarifying questions. We present and evaluate a prototype system, based on a
novel technique (ARHF: Ambiguity Resolution with Human Feedback), that (1)
suggests specific inputs on which a given task specification may be ambiguous,
(2) seeks limited human feedback about the code's desired behavior on those
inputs, and (3) uses this feedback to generate code that resolves these
ambiguities. We evaluate the efficacy of our prototype, and we discuss the
implications of such assistive systems on Computer Science education.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [243] [New Insights into Automatic Treatment Planning for Cancer Radiotherapy Using Explainable Artificial Intelligence](https://arxiv.org/abs/2508.14229)
*Md Mainul Abrar,Xun Jia,Yujie Chi*

Main category: physics.med-ph

TL;DR: 本研究利用可解释AI分析揭示了ACER代理在放射治疗计划中的决策过程，发现高性能代理能有效识别剂量违规并采取智能调整策略，实现高质量规划，其决策模式与人类专家相似。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在揭示人工智能（AI）代理在自动治疗计划中不透明的决策过程，以提高其可解释性。

Method: 研究检查了基于Actor-Critic with Experience Replay (ACER) 网络的AI代理，该代理能自动调整用于前列腺癌调强放疗逆向计划的治疗计划参数（TPPs）。研究选取了不同训练阶段的ACER代理，应用可解释AI (EXAI) 方法分析剂量-体积直方图（DVH）输入到TPP调整决策的归因。同时评估了各代理的规划效能、效率、策略及最终TPP调整空间。

Result: 归因分析显示，ACER代理逐步学会从DVH输入中识别剂量违规区域并促使适当的TPP调整以缓解违规。DVH归因与剂量违规减少之间的器官相似度在0.25到0.5之间。具有更高归因-违规相似度的代理所需调整步骤更少（约12-13步对比22步），TPP调整空间更集中（熵值更低，约0.3对比0.6），最终集中调整少数TPPs，并且实际与理论调整步骤之间的差异更小。高性能ACER代理能有效识别剂量违规并采用全局调整策略实现高质量治疗规划，与熟练的人类规划师相似。

Conclusion: 提高AI代理决策过程的可解释性，有望增强临床医生的信任，并启发自动治疗规划的新策略。

Abstract: Objective: This study aims to uncover the opaque decision-making process of
an artificial intelligence (AI) agent for automatic treatment planning.
  Approach: We examined a previously developed AI agent based on the
Actor-Critic with Experience Replay (ACER) network, which automatically tunes
treatment planning parameters (TPPs) for inverse planning in prostate cancer
intensity modulated radiotherapy. We selected multiple checkpoint ACER agents
from different stages of training and applied an explainable AI (EXAI) method
to analyze the attribution from dose-volume histogram (DVH) inputs to
TPP-tuning decisions. We then assessed each agent's planning efficacy and
efficiency and evaluated their policy and final TPP tuning spaces. Combining
these analyses, we systematically examined how ACER agents generated
high-quality treatment plans in response to different DVH inputs.
  Results: Attribution analysis revealed that ACER agents progressively learned
to identify dose-violation regions from DVH inputs and promote appropriate
TPP-tuning actions to mitigate them. Organ-wise similarities between DVH
attributions and dose-violation reductions ranged from 0.25 to 0.5 across
tested agents. Agents with stronger attribution-violation similarity required
fewer tuning steps (~12-13 vs. 22), exhibited a more concentrated TPP-tuning
space with lower entropy (~0.3 vs. 0.6), converged on adjusting only a few
TPPs, and showed smaller discrepancies between practical and theoretical tuning
steps. Putting together, these findings indicate that high-performing ACER
agents can effectively identify dose violations from DVH inputs and employ a
global tuning strategy to achieve high-quality treatment planning, much like
skilled human planners.
  Significance: Better interpretability of the agent's decision-making process
may enhance clinician trust and inspire new strategies for automatic treatment
planning.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [244] [Fracture Detection and Localisation in Wrist and Hand Radiographs using Detection Transformer Variants](https://arxiv.org/abs/2508.14129)
*Aditya Bagri,Vasanthakumar Venugopal,Anandakumar D,Revathi Ezhumalai,Kalyan Sivasailam,Bargava Subramanian,VarshiniPriya,Meenakumari K S,Abi M,Renita S*

Main category: eess.IV

TL;DR: 本研究开发并评估了一种基于Co-DETR的AI流水线，用于在X射线上自动检测腕部和手部骨折，表现出高准确性和临床实用性，可用于实时部署。


<details>
  <summary>Details</summary>
Motivation: 腕部和手部骨折的X射线诊断在急诊中至关重要，但人工判读耗时且易出错。尽管Transformer模型在医学图像分析中显示出潜力，但其在四肢骨折诊断中的应用有限，本研究旨在填补这一空白。

Method: 研究人员使用专有临床数据集中超过26,000张带标注的X射线图像，对预训练的RT-DETR和Co-DETR模型进行微调。每个图像都用边界框标注了骨折。此外，还训练了一个ResNet-50分类器来细化异常分类，并使用监督对比学习来增强嵌入质量。性能通过AP@50、精确度和召回率进行评估，并在真实世界X射线上进行了额外测试。

Result: RT-DETR表现中等 (AP@50 = 0.39)，而Co-DETR表现更优 (AP@50 = 0.615) 且收敛更快。整合后的流水线在真实世界X射线上实现了83.1%的准确率、85.1%的精确率和96.4%的召回率，并展示了对13种骨折类型的强大泛化能力。目视检查也证实了准确的定位。

Conclusion: 本研究开发的基于Co-DETR的流水线在腕部和手部骨折检测中展现出高准确性和临床相关性，能够提供可靠的定位和骨折类型区分。该系统具有可扩展性、高效性，适用于医院工作流程的实时部署，从而提高肌肉骨骼放射诊断的速度和可靠性。

Abstract: Background: Accurate diagnosis of wrist and hand fractures using radiographs
is essential in emergency care, but manual interpretation is slow and prone to
errors. Transformer-based models show promise in improving medical image
analysis, but their application to extremity fractures is limited. This study
addresses this gap by applying object detection transformers to wrist and hand
X-rays.
  Methods: We fine-tuned the RT-DETR and Co-DETR models, pre-trained on COCO,
using over 26,000 annotated X-rays from a proprietary clinical dataset. Each
image was labeled for fracture presence with bounding boxes. A ResNet-50
classifier was trained on cropped regions to refine abnormality classification.
Supervised contrastive learning was used to enhance embedding quality.
Performance was evaluated using AP@50, precision, and recall metrics, with
additional testing on real-world X-rays.
  Results: RT-DETR showed moderate results (AP@50 = 0.39), while Co-DETR
outperformed it with an AP@50 of 0.615 and faster convergence. The integrated
pipeline achieved 83.1% accuracy, 85.1% precision, and 96.4% recall on
real-world X-rays, demonstrating strong generalization across 13 fracture
types. Visual inspection confirmed accurate localization.
  Conclusion: Our Co-DETR-based pipeline demonstrated high accuracy and
clinical relevance in wrist and hand fracture detection, offering reliable
localization and differentiation of fracture types. It is scalable, efficient,
and suitable for real-time deployment in hospital workflows, improving
diagnostic speed and reliability in musculoskeletal radiology.

</details>


### [245] [Automated surgical planning with nnU-Net: delineation of the anatomy in hepatobiliary phase MRI](https://arxiv.org/abs/2508.14133)
*Karin A. Olthof,Matteo Fusagli,Bianca Güttner,Tiziano Natali,Bram Westerink,Stefanie Speidel,Theo J. M. Ruers,Koert F. D. Kuhlmann,Andrey Zhylka*

Main category: eess.IV

TL;DR: 研究开发并评估了一种基于nnU-Net的深度学习方法，用于从MRI图像中自动分割肝脏解剖结构和肿瘤，旨在简化肝脏手术的术前规划，并已在临床中展现出高精度和实用价值。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在开发并评估一种基于深度学习的自动化分割方法，用于从钆塞酸增强MRI的肝胆期图像中准确识别肝脏解剖结构（如肝实质、肿瘤、门静脉、肝静脉和胆道树），以简化肝脏手术的术前规划工作流程。

Method: 研究对90名患者的肝胆期MRI扫描进行了手动分割，并使用nnU-Net v1深度学习网络在72名患者的数据上进行训练，特别关注薄结构和拓扑保持。模型性能通过Dice相似系数（DSC）在18名患者的测试集上与手动分割进行比较。此外，通过生成10份分割结果并手动优化以量化临床应用所需的调整量，进一步评估了模型的实用性。

Result: 在测试集中，肝实质的DSC为0.97+/-0.01，肝静脉0.80+/-0.04，胆道树0.79+/-0.07，肿瘤0.77+/-0.17，门静脉0.74+/-0.06。肿瘤平均检测率为76.6+/-24.1%。临床评估数据显示，肝实质、门静脉和肝静脉的3D模型在临床使用中仅需微调，DSC值分别为1.00+/-0.00、0.98+/-0.01和0.95+/-0.07。肿瘤分割变异性较大（DSC 0.80+/-0.27）。在实际临床应用中，该模型额外检测出了3个放射科医生最初遗漏的肿瘤。

Conclusion: 所提出的基于nnU-Net的分割方法能够实现肝脏解剖结构（包括肿瘤）的准确、自动化勾画。这使得3D规划能够高效地应用于每位接受肝脏手术的患者，有望成为标准护理流程。

Abstract: Background: The aim of this study was to develop and evaluate a deep
learning-based automated segmentation method for hepatic anatomy (i.e.,
parenchyma, tumors, portal vein, hepatic vein and biliary tree) from the
hepatobiliary phase of gadoxetic acid-enhanced MRI. This method should ease the
clinical workflow of preoperative planning.
  Methods: Manual segmentation was performed on hepatobiliary phase MRI scans
from 90 consecutive patients who underwent liver surgery between January 2020
and October 2023. A deep learning network (nnU-Net v1) was trained on 72
patients with an extra focus on thin structures and topography preservation.
Performance was evaluated on an 18-patient test set by comparing automated and
manual segmentations using Dice similarity coefficient (DSC). Following
clinical integration, 10 segmentations (assessment dataset) were generated
using the network and manually refined for clinical use to quantify required
adjustments using DSC.
  Results: In the test set, DSCs were 0.97+/-0.01 for liver parenchyma,
0.80+/-0.04 for hepatic vein, 0.79+/-0.07 for biliary tree, 0.77+/-0.17 for
tumors, and 0.74+/-0.06 for portal vein. Average tumor detection rate was
76.6+/-24.1%, with a median of one false-positive per patient. The assessment
dataset showed minor adjustments were required for clinical use of the 3D
models, with high DSCs for parenchyma (1.00+/-0.00), portal vein (0.98+/-0.01)
and hepatic vein (0.95+/-0.07). Tumor segmentation exhibited greater
variability (DSC 0.80+/-0.27). During prospective clinical use, the model
detected three additional tumors initially missed by radiologists.
  Conclusions: The proposed nnU-Net-based segmentation method enables accurate
and automated delineation of hepatic anatomy. This enables 3D planning to be
applied efficiently as a standard-of-care for every patient undergoing liver
surgery.

</details>


### [246] [A Systematic Study of Deep Learning Models and xAI Methods for Region-of-Interest Detection in MRI Scans](https://arxiv.org/abs/2508.14151)
*Justin Yiu,Kushank Arora,Daniel Steinberg,Rohit Ghiya*

Main category: eess.IV

TL;DR: 本研究系统评估了多种深度学习架构（包括CNN和Transformer）结合可解释AI技术，用于膝关节MRI扫描中的ROI自动检测。结果显示，ResNet50在MRNet数据集上表现最佳，基于CNN的迁移学习是当前最有效的方法，Grad-CAM提供了最有临床意义的解释。


<details>
  <summary>Details</summary>
Motivation: 手动解释膝关节MRI切片耗时且易受观察者间差异影响，需要开发自动且可解释的ROI检测工具。

Method: 系统评估了ResNet50、InceptionV3、Vision Transformers (ViT) 和多种U-Net变体（增强MLP分类器）等深度学习架构。结合了监督式和自监督式学习。集成了Grad-CAM和显著性图等可解释AI (xAI) 技术。使用AUC评估分类，PSNR/SSIM评估重建质量，并进行ROI可视化。

Result: ResNet50在分类和ROI识别上表现始终优于Transformer模型（在MRNet数据集约束下）。混合U-Net + MLP方法在空间特征利用和可解释性上有潜力，但分类性能较低。Grad-CAM提供了最具临床意义的解释。基于CNN的迁移学习被证明是该数据集最有效的方法。

Conclusion: 对于当前数据集，基于CNN的迁移学习是膝关节MRI ROI自动检测最有效的方法。未来大规模预训练可能有助于释放Transformer模型的潜力。

Abstract: Magnetic Resonance Imaging (MRI) is an essential diagnostic tool for
assessing knee injuries. However, manual interpretation of MRI slices remains
time-consuming and prone to inter-observer variability. This study presents a
systematic evaluation of various deep learning architectures combined with
explainable AI (xAI) techniques for automated region of interest (ROI)
detection in knee MRI scans. We investigate both supervised and self-supervised
approaches, including ResNet50, InceptionV3, Vision Transformers (ViT), and
multiple U-Net variants augmented with multi-layer perceptron (MLP)
classifiers. To enhance interpretability and clinical relevance, we integrate
xAI methods such as Grad-CAM and Saliency Maps. Model performance is assessed
using AUC for classification and PSNR/SSIM for reconstruction quality, along
with qualitative ROI visualizations. Our results demonstrate that ResNet50
consistently excels in classification and ROI identification, outperforming
transformer-based models under the constraints of the MRNet dataset. While
hybrid U-Net + MLP approaches show potential for leveraging spatial features in
reconstruction and interpretability, their classification performance remains
lower. Grad-CAM consistently provided the most clinically meaningful
explanations across architectures. Overall, CNN-based transfer learning emerges
as the most effective approach for this dataset, while future work with
larger-scale pretraining may better unlock the potential of transformer models.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [247] [MOHAF: A Multi-Objective Hierarchical Auction Framework for Scalable and Fair Resource Allocation in IoT Ecosystems](https://arxiv.org/abs/2508.14830)
*Kushagra Agrawal,Polat Goktas,Anjan Bandopadhyay,Debolina Ghosh,Junali Jasmine Jena,Mahendra Kumar Gourisaria*

Main category: cs.DC

TL;DR: 提出MOHAF，一个多目标分层拍卖框架，用于解决物联网分布式资源分配问题，实现了高效率和完美公平性。


<details>
  <summary>Details</summary>
Motivation: 物联网生态系统快速增长，异构资源在高度动态、分布式环境中高效分配面临挑战。传统中心化机制和单目标拍卖模型（如仅关注成本最小化或收益最大化）难以提供均衡的系统性能。

Method: 本文提出了多目标分层拍卖框架（MOHAF），这是一个分布式资源分配机制，它联合优化了成本、服务质量（QoS）、能源效率和公平性。MOHAF集成了分层聚类以降低计算复杂度，并采用了一种保证(1-1/e)近似比的贪婪、次模优化策略。同时，引入了动态定价机制以实时适应资源利用率，增强市场稳定性和分配质量。

Result: 在包含3,553个请求和888个资源的Google集群数据集上的广泛实验表明，MOHAF的分配效率（0.263）优于Greedy（0.185）、First-Price（0.138）和Random（0.101）拍卖，同时实现了完美的公平性（Jain's index = 1.000）。消融研究揭示了成本和QoS组件在维持平衡多目标结果中的关键影响。

Conclusion: MOHAF具有近线性可伸缩性、理论保证和稳健的实证性能，为大规模物联网部署提供了一个实用且适应性强的解决方案，有效地协调了分布式资源协调中的效率、公平性和可持续性。

Abstract: The rapid growth of Internet of Things (IoT) ecosystems has intensified the
challenge of efficiently allocating heterogeneous resources in highly dynamic,
distributed environments. Conventional centralized mechanisms and
single-objective auction models, focusing solely on metrics such as cost
minimization or revenue maximization, struggle to deliver balanced system
performance. This paper proposes the Multi-Objective Hierarchical Auction
Framework (MOHAF), a distributed resource allocation mechanism that jointly
optimizes cost, Quality of Service (QoS), energy efficiency, and fairness.
MOHAF integrates hierarchical clustering to reduce computational complexity
with a greedy, submodular optimization strategy that guarantees a (1-1/e)
approximation ratio. A dynamic pricing mechanism adapts in real time to
resource utilization, enhancing market stability and allocation quality.
Extensive experiments on the Google Cluster Data trace, comprising 3,553
requests and 888 resources, demonstrate MOHAF's superior allocation efficiency
(0.263) compared to Greedy (0.185), First-Price (0.138), and Random (0.101)
auctions, while achieving perfect fairness (Jain's index = 1.000). Ablation
studies reveal the critical influence of cost and QoS components in sustaining
balanced multi-objective outcomes. With near-linear scalability, theoretical
guarantees, and robust empirical performance, MOHAF offers a practical and
adaptable solution for large-scale IoT deployments, effectively reconciling
efficiency, equity, and sustainability in distributed resource coordination.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [248] [MAHL: Multi-Agent LLM-Guided Hierarchical Chiplet Design with Adaptive Debugging](https://arxiv.org/abs/2508.14053)
*Jinwei Tang,Jiayin Qin,Nuo Xu,Pragnya Sudershan Nalla,Yu Cao,Yang,Zhao,Caiwen Ding*

Main category: cs.AR

TL;DR: 提出MAHL，一个分层LLM驱动的芯粒设计生成框架，克服现有LLM在2.5D集成中的局限，显著提升设计生成精度和PPA。


<details>
  <summary>Details</summary>
Motivation: 随着AI等高维工作负载的增长，现有LLM在HDL生成表现出色，但在2.5D芯粒集成中面临扁平化设计、高验证成本和参数优化不精确等挑战，需创新方法克服。

Method: 提出MAHL框架，一个基于LLM的分层芯粒设计生成框架，包含六个协作代理，支持分层描述生成、检索增强代码生成、基于多样流的验证和多粒度设计空间探索，旨在实现AI算法到硬件的有效映射。

Result: MAHL显著提高了简单RTL设计的生成精度，并将真实芯粒设计的Pass@5从0提升至0.72（相较传统LLM）。在PPA方面，MAHL与先进的CLARIE相当或更优。

Conclusion: MAHL成功地通过其分层多代理协作方法，解决了LLM在复杂芯粒设计生成中的核心挑战，有效提升了设计效率和PPA优化。

Abstract: As program workloads (e.g., AI) increase in size and algorithmic complexity,
the primary challenge lies in their high dimensionality, encompassing computing
cores, array sizes, and memory hierarchies. To overcome these obstacles,
innovative approaches are required. Agile chip design has already benefited
from machine learning integration at various stages, including logic synthesis,
placement, and routing. With Large Language Models (LLMs) recently
demonstrating impressive proficiency in Hardware Description Language (HDL)
generation, it is promising to extend their abilities to 2.5D integration, an
advanced technique that saves area overhead and development costs. However,
LLM-driven chiplet design faces challenges such as flatten design, high
validation cost and imprecise parameter optimization, which limit its chiplet
design capability. To address this, we propose MAHL, a hierarchical LLM-based
chiplet design generation framework that features six agents which
collaboratively enable AI algorithm-hardware mapping, including hierarchical
description generation, retrieval-augmented code generation, diverseflow-based
validation, and multi-granularity design space exploration. These components
together enhance the efficient generation of chiplet design with optimized
Power, Performance and Area (PPA). Experiments show that MAHL not only
significantly improves the generation accuracy of simple RTL design, but also
increases the generation accuracy of real-world chiplet design, evaluated by
Pass@5, from 0 to 0.72 compared to conventional LLMs under the best-case
scenario. Compared to state-of-the-art CLARIE (expert-based), MAHL achieves
comparable or even superior PPA results under certain optimization objectives.

</details>


### [249] [Revisit Choice Network for Synthesis and Technology Mapping](https://arxiv.org/abs/2508.14068)
*Chen Chen,Jiaqi Yin,Cunxi Yu*

Main category: cs.AR

TL;DR: 本文提出了一种名为Cristal的新方法和框架，用于构建布尔选择网络。它通过生成更少但更高质量的选择，在技术映射后阶段显著优于现有技术，实现了面积、延迟和运行时间的显著优化。


<details>
  <summary>Details</summary>
Motivation: 现有布尔优化中选择网络构建方法在生成选择时忽视了其质量，导致这些选择对有效技术映射的贡献存疑，因此需要一种能生成高质量选择的新方法。

Method: Cristal引入了一套新的选择网络合成与映射流程，包括：代表性逻辑锥搜索、通过等式饱和进行结构变异以生成多样化选择结构，以及优先级排序的选择选择、选择网络构建与验证。

Result: 实验结果显示，Cristal在后映射阶段优于ABC中现有的布尔选择网络构建方法：在延迟优先模式下，面积/延迟平均降低3.85%/8.35%；在面积优先模式下，面积/延迟平均降低0.11%/2.74%；在大规模案例中，运行时减少了63.77%。

Conclusion: Cristal成功地构建了数量更少但质量更高的布尔选择网络，显著提升了技术映射的效果，并在面积、延迟和运行时方面全面超越了现有最先进的方法。

Abstract: Choice network construction is a critical technique for alleviating
structural bias issues in Boolean optimization, equivalence checking, and
technology mapping. Previous works on lossless synthesis utilize independent
optimization to generate multiple snapshots, and use simulation and SAT solvers
to identify functionally equivalent nodes. These nodes are then merged into a
subject graph with choice nodes. However, such methods often neglect the
quality of these choices, raising the question of whether they truly contribute
to effective technology mapping.
  This paper introduces Cristal, a novel methodology and framework for
constructing Boolean choice networks. Specifically, Cristal introduces a new
flow of choice network-based synthesis and mapping, including representative
logic cone search, structural mutation for generating diverse choice structures
via equality saturation, and priority-ranking choice selection along with
choice network construction and validation. Through these techniques, Cristal
constructs fewer but higher-quality choices.
  Our experimental results demonstrate that Cristal outperforms the
state-of-the-art Boolean choice network construction implemented in ABC in the
post-mapping stage, achieving average reductions of 3.85%/8.35% (area/delay) in
delay-oriented mode, 0.11%/2.74% in area-oriented mode, and a 63.77% runtime
reduction on large-scale cases across a diverse set of combinational circuits
from the IWLS 2005, ISCAS'89, and EPFL benchmark suites.

</details>


### [250] [AI Agents for Photonic Integrated Circuit Design Automation](https://arxiv.org/abs/2508.14123)
*Ankita Sharma,YuQi Fu,Vahid Ansari,Rishabh Iyer,Fiona Kuang,Kashish Mistry,Raisa Islam Aishy,Sara Ahmad,Joaquin Matres,Dirk R. Englund,Joyce K. S. Poon*

Main category: cs.AR

TL;DR: PhIDO是一个多智能体框架，能将自然语言光子集成电路（PIC）设计请求转换为布局掩模文件。该研究比较了7个大型语言模型（LLM）在此框架下的性能，单器件设计成功率高达91%，对于15个组件以下的查询，某些LLM达到了57%的成功率，其中Gemini-2.5-pro表现出最低成本。


<details>
  <summary>Details</summary>
Motivation: 实现将自然语言转换为光子集成电路（PIC）布局掩模文件的自动化设计，以推动自主PIC开发。

Method: 提出了一个名为PhIDO的多智能体框架，用于将自然语言的PIC设计请求转换为布局掩模文件。研究通过一个包含102个设计描述（从单器件到112个组件）的测试基准，比较了7个推理大型语言模型（LLM）在PhIDO框架中的性能。

Result: 单器件设计的成功率高达91%。对于组件数不超过15个的设计查询，o1、Gemini-2.5-pro和Claude Opus 4取得了最高的端到端pass@5成功率，约为57%。其中，Gemini-2.5-pro所需的输出tokens最少，成本最低。

Conclusion: PhIDO框架展示了通过自然语言实现PIC自动化设计的潜力。未来，通过标准化知识表示、扩展数据集、加强验证和机器人自动化等步骤，将进一步推动自主PIC开发。

Abstract: We present Photonics Intelligent Design and Optimization (PhIDO), a
multi-agent framework that converts natural-language photonic integrated
circuit (PIC) design requests into layout mask files. We compare 7 reasoning
large language models for PhIDO using a testbench of 102 design descriptions
that ranged from single devices to 112-component PICs. The success rate for
single-device designs was up to 91%. For design queries with less than or equal
to 15 components, o1, Gemini-2.5-pro, and Claude Opus 4 achieved the highest
end-to-end pass@5 success rates of approximately 57%, with Gemini-2.5-pro
requiring the fewest output tokens and lowest cost. The next steps toward
autonomous PIC development include standardized knowledge representations,
expanded datasets, extended verification, and robotic automation.

</details>
