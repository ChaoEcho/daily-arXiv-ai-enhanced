<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.NI](#cs.NI) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TokenShapley: Token Level Context Attribution with Shapley Value](https://arxiv.org/abs/2507.05261)
*Yingtai Xiao,Yuqing Zhu,Sirat Samyoun,Wanrong Zhang,Jiachen T. Wang,Jian Du*

Main category: cs.CL

TL;DR: 提出TokenShapley，一种结合Shapley值和KNN检索的token级归因方法，用于验证LLM生成内容的准确性，并在实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在情境学习方面能力强大，但其生成回复的正确性难以验证。现有归因方法停留在句子级别，无法满足用户对回复中特定关键词（如数字、年份、名称）进行精细归因的需求。

Method: 提出TokenShapley，一种新颖的token级归因方法。该方法结合了基于Shapley值的数据归因与受KNN增强LLMs启发的KNN检索技术。通过利用预计算的数据存储进行上下文检索并计算Shapley值来量化token的重要性，TokenShapley提供了一种细粒度的数据归因方法。

Result: 在四项基准测试中进行了广泛评估，结果显示TokenShapley在token级归因方面优于现有最先进的基线方法，准确性提高了11-23%。

Conclusion: TokenShapley能够有效解决LLM生成内容中关键词的细粒度归因难题，并显著提升归因准确性，证明了其在验证LLM回复正确性方面的优越性。

Abstract: Large language models (LLMs) demonstrate strong capabilities in in-context
learning, but verifying the correctness of their generated responses remains a
challenge. Prior work has explored attribution at the sentence level, but these
methods fall short when users seek attribution for specific keywords within the
response, such as numbers, years, or names. To address this limitation, we
propose TokenShapley, a novel token-level attribution method that combines
Shapley value-based data attribution with KNN-based retrieval techniques
inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed
datastore for contextual retrieval and computing Shapley values to quantify
token importance, TokenShapley provides a fine-grained data attribution
approach. Extensive evaluations on four benchmarks show that TokenShapley
outperforms state-of-the-art baselines in token-level attribution, achieving an
11-23% improvement in accuracy.

</details>


### [2] [User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs](https://arxiv.org/abs/2507.05266)
*Sougata Saha,Monojit Choudhury*

Main category: cs.CL

TL;DR: 由于数据污染，衡量大型语言模型（LLMs）的泛化能力具有挑战性。本文提出用户行为预测作为一种理论上合理、可扩展且稳健的替代方法，并引入了一个新框架，通过电影和音乐推荐数据集测试了GPT-4o、GPT-4o-mini和Llama-3.1-8B-Instruct，结果显示GPT-4o表现最佳，但所有模型仍有很大改进空间。


<details>
  <summary>Details</summary>
Motivation: 由于数据污染，衡量大型语言模型（LLMs）的泛化能力非常困难。随着模型规模增大和计算成本降低，确保训练阶段任务和测试用例未被见过将变得几乎不可能。知识检索和推理任务不适合衡量泛化能力，因为LLMs并非为特定任务训练。因此，需要一种理论上合理、可扩展且稳健的替代方法来衡量泛化能力，其中用户行为预测是一个关键方面。

Method: 提出并引入了一种新的框架，使用用户行为预测来衡量LLMs的泛化能力。将该方法应用于电影和音乐推荐数据集，并对GPT-4o、GPT-4o-mini和Llama-3.1-8B-Instruct模型进行了测试。

Result: 实验结果与所提框架的预测一致。GPT-4o的表现优于GPT-4o-mini和Llama模型。尽管如此，所有测试模型（尤其是Llama）在用户行为预测方面仍有很大的改进空间。

Conclusion: 用户行为预测是一个理论上合理、可扩展且稳健的替代方案，用于衡量大型语言模型的泛化能力。当前LLMs在该领域表现出不同程度的能力，GPT-4o表现出相对优势，但整体仍有显著的提升潜力。

Abstract: Measuring the generalization ability of Large Language Models (LLMs) is
challenging due to data contamination. As models grow and computation becomes
cheaper, ensuring tasks and test cases are unseen during training phases will
become nearly impossible. We argue that knowledge-retrieval and reasoning tasks
are not ideal for measuring generalization, as LLMs are not trained for
specific tasks. Instead, we propose user behavior prediction, also a key aspect
of personalization, as a theoretically sound, scalable, and robust alternative.
We introduce a novel framework for this approach and test it on movie and music
recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.
Results align with our framework's predictions, showing GPT-4o outperforms
GPT-4o-mini and Llama, though all models have much room for improvement,
especially Llama.

</details>


### [3] [An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks](https://arxiv.org/abs/2507.05271)
*Mohammad Zia Ur Rehman,Aditya Shah,Nagendra Kumar*

Main category: cs.CL

TL;DR: 本文提出ASCEND，一个自适应监督对比学习框架，用于检测社交媒体中的隐性性别歧视。通过引入基于阈值的对比学习来优化嵌入空间，并结合多种特征，ASCEND在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上仇恨内容（包括隐性性别歧视）泛滥，但传统检测方法常忽略或难以识别这些隐性形式。

Method: 引入ASCEND框架，核心是基于阈值的对比学习：仅当嵌入的余弦相似度超过一个可学习阈值时，才将样本对视为正例，以此优化嵌入空间。模型联合优化对比损失和交叉熵损失。文本特征通过词级注意力模块增强，并结合情感、情绪和毒性特征。

Result: 在EXIST2021和MLSC数据集上评估，ASCEND显著优于现有方法，在多任务中平均Macro F1分数分别提升9.86%、29.63%和32.51%。

Conclusion: ASCEND能够有效捕捉隐性性别歧视语言的细微线索，证明其在检测此类内容方面的强大效力。

Abstract: The global reach of social media has amplified the spread of hateful content,
including implicit sexism, which is often overlooked by conventional detection
methods. In this work, we introduce an Adaptive Supervised Contrastive lEarning
framework for implicit sexism detectioN (ASCEND). A key innovation of our
method is the incorporation of threshold-based contrastive learning: by
computing cosine similarities between embeddings, we selectively treat only
those sample pairs as positive if their similarity exceeds a learnable
threshold. This mechanism refines the embedding space by robustly pulling
together representations of semantically similar texts while pushing apart
dissimilar ones, thus reducing false positives and negatives. The final
classification is achieved by jointly optimizing a contrastive loss with a
cross-entropy loss. Textual features are enhanced through a word-level
attention module. Additionally, we employ sentiment, emotion, and toxicity
features. Evaluations on the EXIST2021 and MLSC datasets demonstrate that
ASCEND significantly outperforms existing methods, with average Macro F1
improvements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting
its efficacy in capturing the subtle cues of implicit sexist language.

</details>


### [4] [Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion](https://arxiv.org/abs/2507.05285)
*Miloud Mihoubi,Meriem Zerkouk,Belkacem Chikhaoui*

Main category: cs.CL

TL;DR: 本文提出一个创新的AI框架，通过整合RAG（检索增强生成）进行领域情感分析、提示工程识别学业压力源以及跨模态注意力融合，显著提升了远程学习学生辍学预测的准确性，并能生成可解释的干预策略，有效解决辍学挑战。


<details>
  <summary>Details</summary>
Motivation: 远程学习中的学生辍学是一个关键挑战，带来了深远的社会和经济后果。传统机器学习模型虽然利用结构化的社会人口和行为数据，但未能捕捉非结构化学生互动中细致的情感和情境因素，因此需要一种更全面的方法来提高预测准确性。

Method: 本研究引入了一个变革性的AI框架，其核心方法包括：1) 利用RAG（检索增强生成）技术，基于教学内容知识库进行领域特定情感分析，以深度解读学生评论；2) 通过提示工程优化，精确识别并解码学生面临的学业压力指标；3) 采用跨模态注意力融合层，动态整合文本（情感、压力）、行为（参与模式）和社会人口统计学数据，构建全面的学生风险画像。

Result: 该框架在一个包含4423名学生的纵向数据集上进行了评估，实现了89%的准确率和0.88的F1分数。与传统模型相比，其性能提高了7%，并将假阴性减少了21%。此外，该系统不仅能预测辍学风险，还能通过检索情境相关的策略（例如，为孤立学习者提供导师计划）生成可解释的干预建议。

Conclusion: 这项工作成功弥合了预测分析与可操作教学法之间的差距，提供了一个可扩展的解决方案，有望在全球教育系统中有效降低学生辍学风险。它为教育者提供了精准的预测能力和实用的干预指导。

Abstract: Student dropout in distance learning remains a critical challenge, with
profound societal and economic consequences. While classical machine learning
models leverage structured socio-demographic and behavioral data, they often
fail to capture the nuanced emotional and contextual factors embedded in
unstructured student interactions. This paper introduces a transformative AI
framework that redefines dropout prediction through three synergistic
innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment
analysis, prompt engineering to decode academic stressors, and cross-modal
attention fusion to dynamically align textual, behavioral, and
socio-demographic insights. By grounding sentiment analysis in a curated
knowledge base of pedagogical content, our RAG-enhanced BERT model interprets
student comments with unprecedented contextual relevance, while optimized
prompts isolate indicators of academic distress (e.g., "isolation," "workload
anxiety"). A cross-modal attention layer then fuses these insights with
temporal engagement patterns, creating holistic risk profiles. Evaluated on a
longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and
an F1-score of 0.88, outperforming conventional models by 7% and reducing false
negatives by 21%. Beyond prediction, the system generates interpretable
interventions by retrieving contextually aligned strategies (e.g., mentorship
programs for isolated learners). This work bridges the gap between predictive
analytics and actionable pedagogy, offering a scalable solution to mitigate
dropout risks in global education systems

</details>


### [5] [LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review](https://arxiv.org/abs/2507.05319)
*Cheng Yuan,Xinkai Rui,Yongqi Fan,Yawei Fan,Boyang Zhong,Jiacheng Wang,Weiyan Zhang,Tong Ruan*

Main category: cs.CL

TL;DR: LCDS系统通过文本相似度映射和逻辑规则，解决了LLM在出院摘要生成中的幻觉和溯源问题，生成可靠摘要并支持专家反馈以微调LLM。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自动化出院摘要生成方面表现出色，但仍存在幻觉问题（如生成不准确内容或凭空捏造信息）。此外，电子病历通常数据量大，导致LLM难以对生成内容进行溯源。

Method: 提出LCDS，一个逻辑控制的出院摘要生成系统。LCDS通过计算电子病历与出院摘要之间的文本相似度构建溯源映射表，以限制摘要内容的范围。同时，LCDS整合了一套全面的逻辑规则，以生成更可靠的“银质”出院摘要。系统还支持对生成内容进行溯源，便于专家审查、反馈和纠错。

Result: LCDS能够生成更可靠的、针对不同临床领域的“银质”出院摘要。系统支持内容溯源，使得专家能够高效审查、提供反馈并纠正错误。由此产生的“金质”出院摘要可用于LLM的增量微调。

Conclusion: LCDS通过解决LLM的幻觉和溯源难题，显著提升了出院摘要的可靠性和可信度。其生成的“金质”摘要为LLM的增量微调提供了高质量数据，有望进一步提高LLM在医疗领域的应用性能。

Abstract: Despite the remarkable performance of Large Language Models (LLMs) in
automated discharge summary generation, they still suffer from hallucination
issues, such as generating inaccurate content or fabricating information
without valid sources. In addition, electronic medical records (EMRs) typically
consist of long-form data, making it challenging for LLMs to attribute the
generated content to the sources. To address these challenges, we propose LCDS,
a Logic-Controlled Discharge Summary generation system. LCDS constructs a
source mapping table by calculating textual similarity between EMRs and
discharge summaries to constrain the scope of summarized content. Moreover,
LCDS incorporates a comprehensive set of logical rules, enabling it to generate
more reliable silver discharge summaries tailored to different clinical fields.
Furthermore, LCDS supports source attribution for generated content, allowing
experts to efficiently review, provide feedback, and rectify errors. The
resulting golden discharge summaries are subsequently recorded for incremental
fine-tuning of LLMs. Our project and demo video are in the GitHub repository
https://github.com/ycycyc02/LCDS.

</details>


### [6] [MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents](https://arxiv.org/abs/2507.05330)
*Ming Gong,Xucheng Huang,Chenghan Yang,Xianhan Peng,Haoxin Wang,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: MindFlow是首个面向电商的开源多模态LLM智能体，显著提升了复杂场景下客户服务的处理能力和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在电商客服中取得进展，但在复杂的、多模态的场景中仍存在能力局限性。

Method: 提出MindFlow，一个基于CoALA框架的开源多模态LLM智能体。它集成了记忆、决策和行动模块，并采用模块化的“MLLM即工具”策略进行有效的视觉-文本推理。

Result: 通过在线A/B测试和基于模拟的消融实验评估，MindFlow在处理复杂查询、提高用户满意度和降低运营成本方面表现出显著提升，在实际部署中观察到93.53%的相对改进。

Conclusion: MindFlow成功解决了LLMs在复杂多模态电商客服场景中的限制，大幅提升了服务效率和用户体验，具有显著的实际应用价值。

Abstract: Recent advances in large language models (LLMs) have enabled new applications
in e-commerce customer service. However, their capabilities remain constrained
in complex, multimodal scenarios. We present MindFlow, the first open-source
multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it
integrates memory, decision-making, and action modules, and adopts a modular
"MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via
online A/B testing and simulation-based ablation, MindFlow demonstrates
substantial gains in handling complex queries, improving user satisfaction, and
reducing operational costs, with a 93.53% relative improvement observed in
real-world deployments.

</details>


### [7] [LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2507.05346)
*William Fleshman,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 针对微调语言模型专家泛滥的问题，提出LAG方法以高效选择和组合LoRA适配器，无需额外训练，并在知识密集型任务中表现出色，且兼容RAG。


<details>
  <summary>Details</summary>
Motivation: 随着大量特定任务和领域微调语言模型专家的出现，亟需高效选择和组合这些专家的方法。

Method: 提出LoRA增强生成（LAG）方法，用于利用大型知识库和任务专用LoRA适配器。LAG无需额外训练或数据，能够以逐令牌和逐层的方式高效过滤、检索和应用专家。

Result: 在多种知识密集型任务上，LAG的性能优于现有无数据方法。在有额外数据可用时，LAG还展示了与检索增强生成（RAG）等替代解决方案的兼容性。

Conclusion: LAG提供了一种无需额外训练即可高效利用现有LoRA专家库的有效途径，显著提升了知识密集型任务的性能，并具备良好的兼容性。

Abstract: The proliferation of fine-tuned language model experts for specific tasks and
domains signals the need for efficient selection and combination methods. We
propose LoRA-Augmented Generation (LAG) for leveraging large libraries of
knowledge and task-specific LoRA adapters. LAG requires no additional training
or access to data, and efficiently filters, retrieves, and applies experts on a
per-token and layer basis. We evaluate LAG on various knowledge-intensive
tasks, achieving superior performance over existing data-free methods. We
explore scenarios where additional data is available, demonstrating LAG's
compatibility with alternative solutions such as retrieval-augmented generation
(RAG).

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)](https://arxiv.org/abs/2507.05300)
*Nicholas Merchant,Haitz Sáez de Ocáriz Borde,Andrei Cristian Popescu,Carlos Garcia Jurado Suarez*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We argue that generative text-to-image models often struggle with prompt
adherence due to the noisy and unstructured nature of large-scale datasets like
LAION-5B. This forces users to rely heavily on prompt engineering to elicit
desirable outputs. In this work, we propose that enforcing a consistent caption
structure during training can significantly improve model controllability and
alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of
Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by
a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part
template: subject, setting, aesthetics, and camera details. We fine-tune
PixArt-$\Sigma$ and Stable Diffusion 2 using both structured and randomly
shuffled captions, and show that structured versions consistently yield higher
text-image alignment scores using visual question answering (VQA) models. The
dataset is publicly available at
https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.

</details>


### [9] [CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection](https://arxiv.org/abs/2507.05302)
*Binjia Zhou,Hengrui Lou,Lizhe Chen,Haoyuan Li,Dawei Luo,Shuai Chen,Jie Lei,Zunlei Feng,Yijun Bei*

Main category: cs.CV

TL;DR: 本文提出了一个名为CorrDetail的视觉细节增强自校正框架，用于可解释的人脸伪造检测，解决了现有方法在细节解释性和幻觉问题上的不足，并取得了先进的检测性能。


<details>
  <summary>Details</summary>
Motivation: 人脸深度伪造技术迅速发展，对安全领域构成重大挑战，急需有效的检测方法。现有的人脸伪造检测方法（包括基于视觉和多模态方法）普遍存在对伪造细节解释不清或容易产生幻觉响应的问题。

Method: 本文引入了一个名为CorrDetail的视觉细节增强自校正框架。该框架旨在通过误差引导提问来纠正真实的伪造细节，以揭示而非产生幻觉响应的伪造细节。此外，它整合了一个视觉细粒度细节增强模块，以提供更精确的伪造视觉细节。最终，采用融合决策策略，结合视觉信息补偿和模型偏差减少，以增强模型对极端样本的判别能力。

Result: 实验结果表明，CorrDetail框架不仅在性能上达到了最先进的水平，而且在准确识别伪造细节方面表现出色，并展现出强大的泛化能力。

Conclusion: CorrDetail框架有效解决了现有深度伪造检测方法在可解释性和准确性上的局限性，为构建更可靠、更可解释的人脸伪造检测系统提供了新的SOTA解决方案。

Abstract: With the swift progression of image generation technology, the widespread
emergence of facial deepfakes poses significant challenges to the field of
security, thus amplifying the urgent need for effective deepfake
detection.Existing techniques for face forgery detection can broadly be
categorized into two primary groups: visual-based methods and multimodal
approaches. The former often lacks clear explanations for forgery details,
while the latter, which merges visual and linguistic modalities, is more prone
to the issue of hallucinations.To address these shortcomings, we introduce a
visual detail enhanced self-correction framework, designated CorrDetail, for
interpretable face forgery detection. CorrDetail is meticulously designed to
rectify authentic forgery details when provided with error-guided questioning,
with the aim of fostering the ability to uncover forgery details rather than
yielding hallucinated responses. Additionally, to bolster the reliability of
its findings, a visual fine-grained detail enhancement module is incorporated,
supplying CorrDetail with more precise visual forgery details. Ultimately, a
fusion decision strategy is devised to further augment the model's
discriminative capacity in handling extreme samples, through the integration of
visual information compensation and model bias reduction.Experimental results
demonstrate that CorrDetail not only achieves state-of-the-art performance
compared to the latest methodologies but also excels in accurately identifying
forged details, all while exhibiting robust generalization capabilities.

</details>


### [10] [YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries](https://arxiv.org/abs/2507.05376)
*Aquino Joctum,John Kandiri*

Main category: cs.CV

TL;DR: 本文提出YOLO-APD，一个基于YOLOv8的深度学习架构，通过集成多种创新模块，显著提升了自动驾驶车辆在S型弯道等复杂路况下行人的检测精度和实时性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在S型弯道等几何复杂路面上，标准RGB相机行人检测鲁棒性不足，现有方法面临局限，亟需提升感知系统性能。

Method: YOLO-APD基于YOLOv8框架，集成无参数SimAM注意力机制、高效C3Ghost模块、新型SimSPPF多尺度特征池化模块、Mish激活函数以及IGD特征融合模块。同时引入利用车辆转向动态进行自适应感兴趣区域处理的概念。在自定义CARLA数据集和KITTI数据集上进行评估。

Result: YOLO-APD在自定义CARLA数据集上达到77.7% mAP@0.5:0.95，行人召回率超过96%，显著优于YOLOv8等基线模型，并保持100 FPS的实时处理能力。消融研究验证了各组件的协同贡献。KITTI数据集评估确认其潜力但指出域适应需求。

Conclusion: 该研究推进了基于成本效益传感器的高精度、高效、适应性强的感知系统发展，有助于提升自动驾驶车辆在挑战性、非结构化驾驶环境中的安全性和可靠性。

Abstract: Autonomous vehicle perception systems require robust pedestrian detection,
particularly on geometrically complex roadways like Type-S curved surfaces,
where standard RGB camera-based methods face limitations. This paper introduces
YOLO-APD, a novel deep learning architecture enhancing the YOLOv8 framework
specifically for this challenge. YOLO-APD integrates several key architectural
modifications: a parameter-free SimAM attention mechanism, computationally
efficient C3Ghost modules, a novel SimSPPF module for enhanced multi-scale
feature pooling, the Mish activation function for improved optimization, and an
Intelligent Gather & Distribute (IGD) module for superior feature fusion in the
network's neck. The concept of leveraging vehicle steering dynamics for
adaptive region-of-interest processing is also presented. Comprehensive
evaluations on a custom CARLA dataset simulating complex scenarios demonstrate
that YOLO-APD achieves state-of-the-art detection accuracy, reaching 77.7%
mAP@0.5:0.95 and exceptional pedestrian recall exceeding 96%, significantly
outperforming baseline models, including YOLOv8. Furthermore, it maintains
real-time processing capabilities at 100 FPS, showcasing a superior balance
between accuracy and efficiency. Ablation studies validate the synergistic
contribution of each integrated component. Evaluation on the KITTI dataset
confirms the architecture's potential while highlighting the need for domain
adaptation. This research advances the development of highly accurate,
efficient, and adaptable perception systems based on cost-effective sensors,
contributing to enhanced safety and reliability for autonomous navigation in
challenging, less-structured driving environments.

</details>


### [11] [Foreground-aware Virtual Staining for Accurate 3D Cell Morphological Profiling](https://arxiv.org/abs/2507.05383)
*Alexandr A. Kalinin,Paula Llanos,Theresa Maria Sommer,Giovanni Sestini,Xinhai Hou,Jonathan Z. Sexton,Xiang Wan,Ivo D. Dinov,Brian D. Athey,Nicolas Rivron,Anne E. Carpenter,Beth Cimini,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: Spotlight是一种新型虚拟染色方法，通过改进损失函数使其专注于细胞结构，从而提高虚拟染色图像质量，更利于后续分析。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟染色方法的损失函数对所有像素同等对待，导致模型重现背景噪声和伪影，而非关注生物学有意义的信号。

Method: 引入名为“Spotlight”的虚拟染色方法，该方法利用基于直方图的前景估计遮蔽像素级损失，并通过对软阈值预测计算Dice损失，实现形状感知学习。

Result: 在3D基准数据集上的应用表明，Spotlight提高了形态学表示，同时保持了像素级精度。

Conclusion: Spotlight生成的虚拟染色图像更适合用于下游任务，例如细胞分割和特征分析。

Abstract: Microscopy enables direct observation of cellular morphology in 3D, with
transmitted-light methods offering low-cost, minimally invasive imaging and
fluorescence microscopy providing specificity and contrast. Virtual staining
combines these strengths by using machine learning to predict fluorescence
images from label-free inputs. However, training of existing methods typically
relies on loss functions that treat all pixels equally, thus reproducing
background noise and artifacts instead of focusing on biologically meaningful
signals. We introduce Spotlight, a simple yet powerful virtual staining
approach that guides the model to focus on relevant cellular structures.
Spotlight uses histogram-based foreground estimation to mask pixel-wise loss
and to calculate a Dice loss on soft-thresholded predictions for shape-aware
learning. Applied to a 3D benchmark dataset, Spotlight improves morphological
representation while preserving pixel-level accuracy, resulting in virtual
stains better suited for downstream tasks such as segmentation and profiling.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Strongly Solving $7 \times 6$ Connect-Four on Consumer Grade Hardware](https://arxiv.org/abs/2507.05267)
*Markus Böck*

Main category: cs.AI

TL;DR: 通过高效的二进制决策图（BDD）符号搜索，成功生成了89.6GB的四子棋查找表，推翻了先前关于其不可行的观念。


<details>
  <summary>Details</summary>
Motivation: 尽管四子棋已被数学解决并通过搜索方法可计算最佳走法，但以查找表形式存在的强大解决方案此前被认为不可行。

Method: 重新审视并采用基于二进制决策图（BDD）的符号搜索方法，并进行了高效实现。

Result: 在单CPU核心（128GB内存）上，用47小时成功生成了89.6GB的7x6标准四子棋查找表。此外，其开源工件还包含Alpha-Beta搜索功能，以找到最快获胜或最慢失败的走法。

Conclusion: 论文证明了通过高效的符号搜索实现大型四子棋查找表的可行性和实用性，为该游戏提供了强大的胜平负评估及最佳走法指导，挑战了此前不可行的观点。

Abstract: While the game Connect-Four has been solved mathematically and the best move
can be effectively computed with search based methods, a strong solution in the
form of a look-up table was believed to be infeasible. In this paper, we
revisit a symbolic search method based on binary decision diagrams to produce
strong solutions. With our efficient implementation we were able to produce a
89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main
memory for the standard $7 \times 6$ board size. In addition to this
win-draw-loss evaluation, we include an alpha-beta search in our open source
artifact to find the move which achieves the fastest win or slowest loss.

</details>


### [13] [Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management](https://arxiv.org/abs/2507.05283)
*Yue Wang,Miao Zhou,Guijing Huang,Rui Zhuo,Chao Yi,Zhenliang Ma*

Main category: cs.AI

TL;DR: Chat2SPaT利用大型语言模型（LLMs）将用户对交通信号控制计划的自然语言描述转换为精确的信号相位和时序（SPaT）结果，大大简化了交通信号计划的创建和管理过程。


<details>
  <summary>Details</summary>
Motivation: 现有的预设交通信号控制计划（如按时段或按周计划）创建和更新工作繁琐且耗费大量人工，一个交叉口常需关联多个计划，导致重复性人工参数输入问题。

Method: 本研究提出了Chat2SPaT方法。它首先通过精心设计的提示词，利用大型语言模型（LLMs）理解用户半结构化和模糊的计划描述，并将其重构为JSON格式的相位序列和属性结果。接着，利用Python脚本处理LLM输出，以定位周期内的相位、处理交通信号控制的细微差别，并最终组装完整的交通信号控制计划。该流程可迭代用于计划编辑。

Result: 实验结果显示，Chat2SPaT在包含300多个计划描述的测试数据集上，对英语和中文计划的生成准确率均超过94%。

Conclusion: Chat2SPaT是首个评估LLMs理解交通信号控制计划描述能力的基准，为交通从业者和研究人员提供了一个易于使用的计划管理流程，为LLMs在智能交通系统（ITS）领域更准确、更通用地应用提供了潜在的新基础。

Abstract: Pre-timed traffic signal control, commonly used for operating signalized
intersections and coordinated arterials, requires tedious manual work for
signaling plan creating and updating. When the time-of-day or day-of-week plans
are utilized, one intersection is often associated with multiple plans, leading
to further repetitive manual plan parameter inputting. To enable a
user-friendly traffic signal control plan management process, this study
proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous
descriptions on the signal control plan to exact signal phase and timing (SPaT)
results, which could further be transformed into structured stage-based or
ring-based plans to interact with intelligent transportation system (ITS)
software and traffic signal controllers. With curated prompts, Chat2SPaT first
leverages large language models' (LLMs) capability of understanding users' plan
descriptions and reformulate the plan as a combination of phase sequence and
phase attribute results in the json format. Based on LLM outputs, python
scripts are designed to locate phases in a cycle, address nuances of traffic
signal control, and finally assemble the complete traffic signal control plan.
Within a chat, the pipeline can be utilized iteratively to conduct further plan
editing. Experiments show that Chat2SPaT can generate plans with an accuracy of
over 94% for both English and Chinese cases, using a test dataset with over 300
plan descriptions. As the first benchmark for evaluating LLMs' capability of
understanding traffic signal control plan descriptions, Chat2SPaT provides an
easy-to-use plan management pipeline for traffic practitioners and researchers,
serving as a potential new building block for a more accurate and versatile
application of LLMs in the field of ITS. The source codes, prompts and test
dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.

</details>


### [14] [Fuzzy Classification Aggregation for a Continuum of Agents](https://arxiv.org/abs/2507.05297)
*Zijun Meng*

Main category: cs.AI

TL;DR: 研究证明，在特定条件下，连续模糊分类的聚合函数必然是加权算术平均值。


<details>
  <summary>Details</summary>
Motivation: 旨在识别和刻画在满足最优性、独立性和零一致性等特定性质时，处理连续个体分类的模糊分类聚合函数的内在结构。

Method: 采用数学证明方法，通过理论推导来证明聚合函数的具体形式。

Result: 对于将至少3个对象分为2到m种类型的连续个体分类，任何最优、独立且零一致的模糊分类聚合函数都必须是加权算术平均值。

Conclusion: 在所设定的公理化条件下，模糊分类聚合函数的唯一形式是加权算术平均值，这为模糊分类的聚合提供了一个明确的理论基础。

Abstract: We prove that any optimal, independent, and zero unanimous fuzzy
classification aggregation function of a continuum of individual
classifications of $m\ge 3$ objects into $2\le p\le m$ types must be a weighted
arithmetic mean.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [Rethinking Over-Smoothing in Graph Neural Networks: A Perspective from Anderson Localization](https://arxiv.org/abs/2507.05263)
*Kaichen Ouyang*

Main category: cs.LG

TL;DR: 深入探讨GNN过平滑问题，通过与Anderson局域化类比，分析了其内在机制，并提出可通过降低信息传播的无序性来缓解过平滑。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络（GNN）在深层网络中日益严重的过平滑问题，导致节点表示失去独特性。

Method: 将GNN过平滑机制类比为Anderson局域化，引入“参与度”作为量化指标。系统回顾两者潜在联系，并进行理论分析。

Result: 发现GNN过平滑可理解为低频模式的扩展（参与度增加）和高频模式的局域化（参与度降低），导致节点特征在多层消息传递后同质化。

Conclusion: 通过理论分析，提出降低信息传播中的无序性是缓解GNN过平滑的一种潜在途径。

Abstract: Graph Neural Networks (GNNs) have shown great potential in graph data
analysis due to their powerful representation capabilities. However, as the
network depth increases, the issue of over-smoothing becomes more severe,
causing node representations to lose their distinctiveness. This paper analyzes
the mechanism of over-smoothing through the analogy to Anderson localization
and introduces participation degree as a metric to quantify this phenomenon.
Specifically, as the depth of the GNN increases, node features homogenize after
multiple layers of message passing, leading to a loss of distinctiveness,
similar to the behavior of vibration modes in disordered systems. In this
context, over-smoothing in GNNs can be understood as the expansion of
low-frequency modes (increased participation degree) and the localization of
high-frequency modes (decreased participation degree). Based on this, we
systematically reviewed the potential connection between the Anderson
localization behavior in disordered systems and the over-smoothing behavior in
Graph Neural Networks. A theoretical analysis was conducted, and we proposed
the potential of alleviating over-smoothing by reducing the disorder in
information propagation.

</details>


### [16] [Temporal Window Smoothing of Exogenous Variables for Improved Time Series Prediction](https://arxiv.org/abs/2507.05284)
*Mustafa Kamal,Niyaz Bin Hashem,Robin Krambroeckers,Nabeel Mohammed,Shafin Rahman*

Main category: cs.LG

TL;DR: 本文提出一种通过白化外部输入来减少冗余并增强长期依赖捕获能力的时序预测方法，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的时序预测模型在利用外部输入时，面临数据冗余（当内外部输入源相同时）和固定回溯窗口导致的长期依赖捕获能力不足等挑战。

Method: 本研究提出一种方法，通过基于全局统计量白化外部输入来减少数据冗余，并使其更好地感知长期模式和趋势。在不增加回溯窗口长度的情况下，将这种精炼、全局上下文感知的外部输入与内部输入结合，以提升预测性能。

Result: 该方法在四个基准数据集上均达到了最先进的性能，并持续优于11个基线模型。

Conclusion: 研究结果表明，本文提出的方法是一种在时间序列预测中有效且稳健地使用外部输入的替代方案。

Abstract: Although most transformer-based time series forecasting models primarily
depend on endogenous inputs, recent state-of-the-art approaches have
significantly improved performance by incorporating external information
through exogenous inputs. However, these methods face challenges, such as
redundancy when endogenous and exogenous inputs originate from the same source
and limited ability to capture long-term dependencies due to fixed look-back
windows. In this paper, we propose a method that whitens the exogenous input to
reduce redundancy that may persist within the data based on global statistics.
Additionally, our approach helps the exogenous input to be more aware of
patterns and trends over extended periods. By introducing this refined,
globally context-aware exogenous input to the endogenous input without
increasing the lookback window length, our approach guides the model towards
improved forecasting. Our approach achieves state-of-the-art performance in
four benchmark datasets, consistently outperforming 11 baseline models. These
results establish our method as a robust and effective alternative for using
exogenous inputs in time series forecasting.

</details>


### [17] [Compressing Deep Neural Networks Using Explainable AI](https://arxiv.org/abs/2507.05286)
*Kimia Soroush,Mohsen Raji,Behnam Ghavami*

Main category: cs.LG

TL;DR: 本文提出了一种利用可解释人工智能（XAI）方法（具体为LRP）进行深度神经网络（DNN）压缩的新方法，通过根据权重重要性分数进行剪枝和混合精度量化，实现了模型尺寸的大幅缩小和准确率的提升。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络计算成本和内存占用高，难以部署在资源受限的边缘设备上。现有压缩技术（如剪枝和量化）可以缓解此问题。作者旨在利用XAI理解DNN内部工作机制（如参数重要性），从而开发出一种高效且精度损失可忽略的DNN压缩方法。

Method: 该方法通过梯度基XAI技术（分层相关性传播LRP）计算DNN参数（权重）的重要性得分。然后，利用这些得分进行压缩：1) 剪枝并移除重要性得分负或零的参数；2) 对重要性得分高的权重采用更高位数，得分低的权重采用更低位数的混合精度量化。

Result: 实验结果显示，该压缩方法将模型尺寸减小了64%，同时相比于最先进的XAI基压缩方法，准确率提高了42%。

Conclusion: 该研究表明，利用XAI指导的DNN压缩方法能有效显著地减小模型尺寸，并在准确率上超越现有XAI基压缩方法，验证了其在资源受限环境部署DNN的潜力。

Abstract: Deep neural networks (DNNs) have demonstrated remarkable performance in many
tasks but it often comes at a high computational cost and memory usage.
Compression techniques, such as pruning and quantization, are applied to reduce
the memory footprint of DNNs and make it possible to accommodate them on
resource-constrained edge devices. Recently, explainable artificial
intelligence (XAI) methods have been introduced with the purpose of
understanding and explaining AI methods. XAI can be utilized to get to know the
inner functioning of DNNs, such as the importance of different neurons and
features in the overall performance of DNNs. In this paper, a novel DNN
compression approach using XAI is proposed to efficiently reduce the DNN model
size with negligible accuracy loss. In the proposed approach, the importance
score of DNN parameters (i.e. weights) are computed using a gradient-based XAI
technique called Layer-wise Relevance Propagation (LRP). Then, the scores are
used to compress the DNN as follows: 1) the parameters with the negative or
zero importance scores are pruned and removed from the model, 2)
mixed-precision quantization is applied to quantize the weights with
higher/lower score with higher/lower number of bits. The experimental results
show that, the proposed compression approach reduces the model size by 64%
while the accuracy is improved by 42% compared to the state-of-the-art
XAI-based compression method.

</details>


### [18] [Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity](https://arxiv.org/abs/2507.05291)
*Manuel Ricardo Guevara Garban,Yves Chemisky,Étienne Prulière,Michaël Clément*

Main category: cs.LG

TL;DR: 提出P-DivGNN框架，一个物理信息机器学习方法，用于在多尺度模拟中，根据周期性微观结构和宏观应力值重建微观尺度的局部应力场。


<details>
  <summary>Details</summary>
Motivation: 局部应力场的预测对于断裂分析或局部疲劳准则的定义至关重要。

Method: P-DivGNN框架基于将周期性微观结构表示为图，并结合消息传递图神经网络。模型在训练中融入物理约束以确保局部应力场平衡，并采用周期性图表示来强制周期边界条件。该方法在线性和非线性超弹性响应以及不同几何形状上进行了评估。

Result: 能够从宏观平均应力值（由平均场降阶模型或有限元模拟提供）中获取并重建局部应力场分布。在非线性超弹性情况下，与有限元模拟相比，该方法实现了显著的计算加速。

Conclusion: P-DivGNN在处理非线性超弹性问题时展现出卓越的计算效率，使其在大规模应用中具有特别的吸引力。

Abstract: We propose a physics-informed machine learning framework called P-DivGNN to
reconstruct local stress fields at the micro-scale, in the context of
multi-scale simulation given a periodic micro-structure mesh and mean,
macro-scale, stress values. This method is based in representing a periodic
micro-structure as a graph, combined with a message passing graph neural
network. We are able to retrieve local stress field distributions, providing
average stress values produced by a mean field reduced order model (ROM) or
Finite Element (FE) simulation at the macro-scale. The prediction of local
stress fields are of utmost importance considering fracture analysis or the
definition of local fatigue criteria. Our model incorporates physical
constraints during training to constraint local stress field equilibrium state
and employs a periodic graph representation to enforce periodic boundary
conditions. The benefits of the proposed physics-informed GNN are evaluated
considering linear and non linear hyperelastic responses applied to varying
geometries. In the non-linear hyperelastic case, the proposed method achieves
significant computational speed-ups compared to FE simulation, making it
particularly attractive for large-scale applications.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [19] [Baton: Compensate for Missing Wi-Fi Features for Practical Device-free Tracking](https://arxiv.org/abs/2507.05597)
*Yiming Zhao,Xuanqi Meng,Xinyu Tong,Xiulong Liu,Xin Xie,Wenyu Qu*

Main category: cs.NI

TL;DR: 针对现有Wi-Fi感知系统在通信中断或特征不足时性能下降的问题，本文提出Baton系统和STAP算法，通过挖掘Wi-Fi特征关联性，在低通信占空比下实现精准目标跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有Wi-Fi感知系统常需长时间通信，且短暂中断即导致显著性能下降，尤其在Wi-Fi特征严重不足的场景下，这限制了其智能感知应用的部署。

Method: 提出Baton系统，核心是探索Wi-Fi特征矩阵的横向（跨链路）和纵向（跨时隙）关联性。基于此原理，提出同时跟踪与预测（STAP）算法，实现Wi-Fi特征在时间和链路间的无缝传递。

Result: 在商用设备上实现并验证，即使通信占空比低至20.00%，系统跟踪误差中位数仍为0.46m。与最先进方案相比，在Wi-Fi特征严重不足场景下，跟踪误差降低了79.19%。

Conclusion: Baton系统有效解决了Wi-Fi特征缺失导致的跟踪精度问题，显著提升了在挑战性环境下的目标跟踪性能，优于现有解决方案。

Abstract: Wi-Fi contact-free sensing systems have attracted widespread attention due to
their ubiquity and convenience. The integrated sensing and communication (ISAC)
technology utilizes off-the-shelf Wi-Fi communication signals for sensing,
which further promotes the deployment of intelligent sensing applications.
However, current Wi-Fi sensing systems often require prolonged and unnecessary
communication between transceivers, and brief communication interruptions will
lead to significant performance degradation. This paper proposes Baton, the
first system capable of accurately tracking targets even under severe Wi-Fi
feature deficiencies. To be specific, we explore the relevance of the Wi-Fi
feature matrix from both horizontal and vertical dimensions. The horizontal
dimension reveals feature correlation across different Wi-Fi links, while the
vertical dimension reveals feature correlation among different time slots.
Based on the above principle, we propose the Simultaneous Tracking And
Predicting (STAP) algorithm, which enables the seamless transfer of Wi-Fi
features over time and across different links, akin to passing a baton. We
implement the system on commercial devices, and the experimental results show
that our system outperforms existing solutions with a median tracking error of
0.46m, even when the communication duty cycle is as low as 20.00%. Compared
with the state-of-the-art, our system reduces the tracking error by 79.19% in
scenarios with severe Wi-Fi feature deficiencies.

</details>


### [20] [A Satellite-Ground Synergistic Large Vision-Language Model System for Earth Observation](https://arxiv.org/abs/2507.05731)
*Yuxin Zhang,Jiahao Yang,Zhe Chen,Wenjun Zhu,Jin Zhao,Yue Gao*

Main category: cs.NI

TL;DR: 该论文提出了SpaceVerse，一个高效的星地协同大型视觉-语言模型（LVLM）推理系统，旨在解决低地球轨道（LEO）卫星地球观测图像的实时处理中面临的数据下载挑战，并显著提升了处理精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM虽能强大分析LEO卫星地球观测图像，但其在数据中心运行，而卫星的快速移动、与地面站短暂的接触窗口以及图像的巨大尺寸共同构成了数据下载的巨大挑战，阻碍了近实时地球观测应用（如灾害和极端天气监测）的实现。因此，需要在LEO卫星网络中探索LVLM的部署方案。

Method: 论文设计了SpaceVerse系统，一个高效的星地协同LVLM推理系统。具体方法包括：1) 将紧凑型LVLM部署在卫星上执行轻量级任务，而常规LVLM在地面站处理计算密集型任务。2) 提出了一个计算与通信协同设计框架，该框架包含一个渐进置信度网络（用于识别星上推理数据）和一个基于注意力的多尺度预处理模块（用于在星地传输前减少数据冗余）。

Result: SpaceVerse系统在真实世界的LEO卫星星座和数据集上进行了实现和评估，结果显示，与现有最佳基线相比，系统平均精度提高了31.2%，延迟降低了51.2%。

Conclusion: SpaceVerse系统通过创新的星地协同LVLM部署和计算通信协同设计，有效克服了LEO卫星地球观测图像实时处理中的数据下载瓶颈，显著提升了性能，为实现近实时地球观测应用提供了可靠且高效的解决方案。

Abstract: Recently, large vision-language models (LVLMs) unleash powerful analysis
capabilities for low Earth orbit (LEO) satellite Earth observation images in
the data center. However, fast satellite motion, brief satellite-ground station
(GS) contact windows, and large size of the images pose a data download
challenge. To enable near real-time Earth observation applications (e.g.,
disaster and extreme weather monitoring), we should explore how to deploy LVLM
in LEO satellite networks, and design SpaceVerse, an efficient satellite-ground
synergistic LVLM inference system. To this end, firstly, we deploy compact
LVLMs on satellites for lightweight tasks, whereas regular LVLMs operate on GSs
to handle computationally intensive tasks. Then, we propose a computing and
communication co-design framework comprised of a progressive confidence network
and an attention-based multi-scale preprocessing, used to identify on-satellite
inferring data, and reduce data redundancy before satellite-GS transmission,
separately. We implement and evaluate SpaceVerse on real-world LEO satellite
constellations and datasets, achieving a 31.2% average gain in accuracy and a
51.2% reduction in latency compared to state-of-the-art baselines.

</details>


### [21] [Intra-DP: A High Performance Collaborative Inference System for Mobile Edge Computing](https://arxiv.org/abs/2507.05829)
*Zekai Sun,Xiuxian Guan,Zheng Lin,Zihan Fang,Xiangming Cai,Zhe Chen,Fangming Liu,Heming Cui,Jie Xiong,Wei Ni,Chau Yuen*

Main category: cs.NI

TL;DR: Intra-DP系统通过并行化本地算子的计算和传输，显著提升了移动边缘计算中深度神经网络推理的速度和能效。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的移动设备上部署深度神经网络面临实时性能、计算资源和电池寿命的挑战。虽然移动边缘计算（MEC）通过与GPU服务器协作推理提供解决方案，但现有方法依赖于层级模型分区，并因DNN操作的顺序执行导致严重的传输瓶颈。

Method: 本文提出了Intra-DP系统，一个专为MEC上DNN推理优化的协作推理系统。它采用了一种新颖的并行计算技术，基于局部算子（如卷积核），将其计算分解为独立的子操作，并通过并行执行重叠计算和传输，从而缓解传输瓶颈。

Result: 评估结果表明，与现有先进基线相比，Intra-DP将每次推理的延迟降低了高达50%，能耗降低了高达75%，且不牺牲准确性。

Conclusion: Intra-DP成功地通过创新的并行计算技术解决了MEC中深度神经网络推理的传输瓶颈，实现了更快速、更节能的推理性能。

Abstract: Deploying deep neural networks (DNNs) on resource-constrained mobile devices
presents significant challenges, particularly in achieving real-time
performance while simultaneously coping with limited computational resources
and battery life. While Mobile Edge Computing (MEC) offers collaborative
inference with GPU servers as a promising solution, existing approaches
primarily rely on layer-wise model partitioning and undergo significant
transmission bottlenecks caused by the sequential execution of DNN operations.
To address this challenge, we present Intra-DP, a high-performance
collaborative inference system optimized for DNN inference on MEC. Intra DP
employs a novel parallel computing technique based on local operators (i.e.,
operators whose minimum unit input is not the entire input tensor, such as the
convolution kernel). By decomposing their computations (operations) into
several independent sub-operations and overlapping the computation and
transmission of different sub-operations through parallel execution, Intra-DP
mitigates transmission bottlenecks in MEC, achieving fast and energy-efficient
inference. The evaluation demonstrates that Intra-DP reduces per-inference
latency by up to 50% and energy consumption by up to 75% compared to
state-of-the-art baselines, without sacrificing accuracy.

</details>


### [22] [OLAF: Programmable Data Plane Acceleration for Asynchronous Distributed Reinforcement Learning](https://arxiv.org/abs/2507.05876)
*Nehal Baganal Krishna,Anam Tahir,Firas Khamis,Mina Tahmasbi Arashloo,Michael Zink,Amr Rizk*

Main category: cs.NI

TL;DR: 异步DRL训练中模型更新延迟导致收敛性下降，本文提出一种网络数据平面加速架构，通过在网内处理更新来减少延迟，提高收敛速度。


<details>
  <summary>Details</summary>
Motivation: 异步分布式强化学习(DRL)在大规模训练时，网络拥塞和丢包导致模型更新滞后，进而降低收敛性能。

Method: 提出一种网络数据平面加速架构，支持对DRL模型更新的在线处理；设计一种新型排队机制，机会性地合并兼容更新；引入轻量级传输控制机制；并提出Age-of-Model (AoM)指标评估模型新鲜度，采用形式化验证保证公平性和响应性。

Result: 评估结果表明，该架构显著减少了更新的滞后性和网络拥塞。

Conclusion: 该架构最终提升了异步DRL工作负载的收敛速度。

Abstract: Asynchronous Distributed Reinforcement Learning (DRL) can suffer from
degraded convergence when model updates become stale, often the result of
network congestion and packet loss during large-scale training. This work
introduces a network data-plane acceleration architecture that mitigates such
staleness by enabling inline processing of DRL model updates as they traverse
the accelerator engine. To this end, we design and prototype a novel queueing
mechanism that opportunistically combines compatible updates sharing a network
element, reducing redundant traffic and preserving update utility.
Complementing this we provide a lightweight transmission control mechanism at
the worker nodes that is guided by feedback from the in-network accelerator. To
assess model utility at line rate, we introduce the Age-of-Model (AoM) metric
as a proxy for staleness and verify global fairness and responsiveness
properties using a formal verification method. Our evaluations demonstrate that
this architecture significantly reduces update staleness and congestion,
ultimately improving the convergence rate in asynchronous DRL workloads.

</details>
