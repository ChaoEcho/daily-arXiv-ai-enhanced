<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 29]
- [cs.CV](#cs.CV) [Total: 26]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.LG](#cs.LG) [Total: 25]
- [cs.NI](#cs.NI) [Total: 11]
- [cs.ET](#cs.ET) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [KG-o1: Enhancing Multi-hop Question Answering in Large Language Models via Knowledge Graph Integration](https://arxiv.org/abs/2508.15790)
*Nan Wang,Yongqi Fan,yansha zhu,ZongYu Wang,Xuezhi Cao,Xinyan He,Haiyun Jiang,Tong Ruan,Jingping Liu*

Main category: cs.CL

TL;DR: 针对大型语言模型（LLMs）在多跳推理任务中CoT路径偏差问题，本研究提出KG-o1方法。该方法通过整合知识图谱（KGs）并采用四阶段流程（实体过滤与子图生成、逻辑路径构建、长步骤推理数据集训练、DPO结合拒绝采样优化），显著提升了LLMs的多跳推理能力，并在实验中超越了现有的大型推理模型（LRMs）。


<details>
  <summary>Details</summary>
Motivation: LLMs在知识密集型推理任务（如多跳问答）中表现不佳，其生成的思维链（CoTs）常偏离真实推理路径。知识图谱（KGs）能显式表达事实间的逻辑连接，与LLMs形成对比。同时，大型推理模型（LRMs）已证明长步骤推理可显著提升LLMs性能。因此，动机是结合KGs的结构化知识与长步骤推理优势，以增强LLMs的多跳推理能力。

Method: 本研究提出了KG-o1，一个四阶段方法：1. 过滤初始实体并生成复杂子图。2. 构建子图的逻辑路径。3. 利用知识图谱构建一个包含复杂和扩展头脑风暴过程的数据集，以训练LLMs模仿长期推理。4. 采用拒绝采样生成一个自我改进语料库，用于直接偏好优化（DPO），进一步优化LLMs的推理能力。实验在两个简单和两个复杂数据集上进行。

Result: 实验结果表明，KG-o1模型在所有任务中均表现出优于现有大型推理模型（LRMs）的性能。

Conclusion: KG-o1通过有效整合知识图谱和多阶段训练策略，成功解决了LLMs在知识密集型多跳推理任务中的挑战，显著提升了其推理能力，并在实践中超越了现有基线模型。

Abstract: Large Language Models (LLMs) face challenges in knowledge-intensive reasoning
tasks like classic multi-hop question and answering, which involves reasoning
across multiple facts. This difficulty arises because the chain of thoughts
(CoTs) generated by LLMs in such tasks often deviate from real or a priori
reasoning paths. In contrast, knowledge graphs (KGs) explicitly represent the
logical connections between facts through entities and relationships. This
reflects a significant gap. Meanwhile, large reasoning models (LRMs), such as
o1, have demonstrated that long-step reasoning significantly enhances the
performance of LLMs. Building on these insights, we propose KG-o1, a four-stage
approach that integrates KGs to enhance the multi-hop reasoning abilities of
LLMs. We first filter out initial entities and generate complex subgraphs.
Secondly, we construct logical paths for subgraphs and then use knowledge
graphs to build a dataset with a complex and extended brainstorming process,
which trains LLMs to imitate long-term reasoning. Finally, we employ rejection
sampling to generate a self-improving corpus for direct preference optimization
(DPO), further refining the LLMs reasoning abilities. We conducted experiments
on two simple and two complex datasets. The results show that KG-o1 models
exhibit superior performance across all tasks compared to existing LRMs.

</details>


### [2] [InteChar: A Unified Oracle Bone Character List for Ancient Chinese Language Modeling](https://arxiv.org/abs/2508.15791)
*Xiaolei Diao,Zhihan Zhou,Lida Shi,Ting Wang,Ruihua Qi,Hao Xu,Daqian Shi*

Main category: cs.CL

TL;DR: 本文提出InteChar，一个统一且可扩展的字符列表，用于整合甲骨文与现代汉字，并构建了OracleCS古汉语语料库。实验证明，使用InteChar和OracleCS训练的模型在历史语言理解任务上取得了显著提升，为古汉语NLP奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 构建历史语言模型对考古学和理解古代文化至关重要，但现有资源面临两大挑战：一是历史语言样本稀缺，导致基于大语料库的无监督学习效率低下；二是古文字（特别是早期汉字）缺乏全面的字符编码方案，阻碍了古籍的数字化和计算处理。

Method: 为解决上述挑战，研究者引入了InteChar，一个统一且可扩展的字符列表，它整合了未编码的甲骨文与传统和现代汉字，实现了历史文本的统一数字化和表示。此外，研究者还构建了Oracle Corpus Set (OracleCS)，一个以甲骨文为中心的古汉语语料库，该语料库结合了专家标注样本和大型语言模型辅助的数据增强技术。

Result: 广泛的实验表明，使用InteChar在OracleCS上训练的模型在各种历史语言理解任务中取得了显著的改进。

Conclusion: 研究结果证实了该方法的有效性，并为未来古汉语自然语言处理的研究奠定了坚实的基础。

Abstract: Constructing historical language models (LMs) plays a crucial role in aiding
archaeological provenance studies and understanding ancient cultures. However,
existing resources present major challenges for training effective LMs on
historical texts. First, the scarcity of historical language samples renders
unsupervised learning approaches based on large text corpora highly
inefficient, hindering effective pre-training. Moreover, due to the
considerable temporal gap and complex evolution of ancient scripts, the absence
of comprehensive character encoding schemes limits the digitization and
computational processing of ancient texts, particularly in early Chinese
writing. To address these challenges, we introduce InteChar, a unified and
extensible character list that integrates unencoded oracle bone characters with
traditional and modern Chinese. InteChar enables consistent digitization and
representation of historical texts, providing a foundation for robust modeling
of ancient scripts. To evaluate the effectiveness of InteChar, we construct the
Oracle Corpus Set (OracleCS), an ancient Chinese corpus that combines
expert-annotated samples with LLM-assisted data augmentation, centered on
Chinese oracle bone inscriptions. Extensive experiments show that models
trained with InteChar on OracleCS achieve substantial improvements across
various historical language understanding tasks, confirming the effectiveness
of our approach and establishing a solid foundation for future research in
ancient Chinese NLP.

</details>


### [3] [Bhav-Net: Knowledge Transfer for Cross-Lingual Antonym vs Synonym Distinction via Dual-Space Graph Transformers](https://arxiv.org/abs/2508.15792)
*Samyak S. Sanghvi*

Main category: cs.CL

TL;DR: 本文提出Bhav-Net双空间架构，结合BERT和图Transformer，实现跨语言反义词/同义词区分，并在8种语言中实现了有效的知识迁移和有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 多语言环境下的反义词与同义词区分面临独特的计算挑战，因为反义词在共享语义域的同时表达相反的含义。

Method: 引入Bhav-Net，一种新颖的双空间架构。该方法结合特定语言的BERT编码器与图Transformer网络，创建了两个独立的语义投影空间：同义词对在一个空间中聚类，而反义词对在互补空间中表现出高相似度。旨在实现复杂多语言模型知识向简单、特定语言架构的有效迁移。

Result: 通过在八种语言（包括英语、德语等）中的全面评估，证明语义关系建模能够有效跨语言迁移。双编码器设计在性能上与现有最先进的基线相当，同时提供了可解释的语义表示和有效的跨语言泛化能力。

Conclusion: Bhav-Net通过其独特的双空间架构，成功应对了跨语言反义词-同义词区分的挑战，实现了高效的知识迁移、具有竞争力的性能以及出色的跨语言泛化能力。

Abstract: Antonym vs synonym distinction across multiple languages presents unique
computational challenges due to the paradoxical nature of antonymous
relationships words that share semantic domains while expressing opposite
meanings. This work introduces Bhav-Net, a novel dual-space architecture that
enables effective knowledge transfer from complex multilingual models to
simpler, language-specific architectures while maintaining robust cross-lingual
antonym--synonym distinction capabilities. Our approach combines
language-specific BERT encoders with graph transformer networks, creating
distinct semantic projections where synonymous pairs cluster in one space while
antonymous pairs exhibit high similarity in a complementary space. Through
comprehensive evaluation across eight languages (English, German, French,
Spanish, Italian, Portuguese, Dutch, and Russian), we demonstrate that semantic
relationship modeling transfers effectively across languages. The dual-encoder
design achieves competitive performance against state-of-the-art baselines
while providing interpretable semantic representations and effective
cross-lingual generalization.

</details>


### [4] [Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous Data](https://arxiv.org/abs/2508.15793)
*Jiacheng Liu,Mayi Xu,Qiankun Pi,Wenli Li,Ming Zhong,Yuanyuan Zhu,Mengchi Liu,Tieyun Qian*

Main category: cs.CL

TL;DR: 本文首次探讨并分析了大型语言模型（LLMs）在处理异构数据时存在的格式偏差问题，并提出了三种未来研究方向以减轻此偏差。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理文本、表格、信息框和知识图谱等异构数据时，可能存在对特定格式的系统性偏差，这可能导致推理错误并增加下游任务的风险。然而，这种格式偏差是否系统性存在、其数据级影响因素以及LLM内部机制尚不明确。

Method: 通过构建异构数据冲突场景，作者进行了一项三阶段实证研究：第一阶段探索不同LLM中偏差的存在和方向；第二阶段检查信息丰富度、结构质量和格式类型等数据级因素如何影响偏差；第三阶段分析格式偏差在LLM注意力模式中如何出现，并评估一种轻量级干预措施的可缓解性。

Result: 基于调查结果，作者提出了三个减少格式偏差的未来研究方向：通过格式净化和规范化改进数据预处理、引入推理时干预（如注意力重新加权），以及开发格式平衡的训练语料库。

Conclusion: 这些研究方向将有助于设计更稳健、更公平的异构数据处理系统。

Abstract: Large Language Models (LLMs) are increasingly employed in applications that
require processing information from heterogeneous formats, including text,
tables, infoboxes, and knowledge graphs. However, systematic biases toward
particular formats may undermine LLMs' ability to integrate heterogeneous data
impartially, potentially resulting in reasoning errors and increased risks in
downstream tasks. Despite these concerns, it remains uncertain whether such
format biases are systematic, which data-level factors contribute to them, and
what internal mechanisms in LLMs underlie their emergence.
  In this paper, we make the first attempt to investigate and analyze the
format bias in LLMs. To systematically investigate the aforementioned
questions, we conduct a three-stage empirical study by constructing an
heterogeneous data conflict scenario for the exploration of bias. The first
stage explores the presence and direction of bias across a diverse range of
LLMs. The second stage aims to examine how key data-level factors, including
information richness, structure quality, and format type, influence these
biases. The third stage analyzes how format bias emerges within LLMs' attention
patterns and evaluates a lightweight intervention to test its potential
mitigability. Based on these investigations, we identify three future research
directions to reduce format bias: improving data preprocessing through format
sanitization and normalization, introducing inference-time interventions such
as attention re-weighting, and developing format-balanced training corpora.
These directions will support the design of more robust and fair heterogeneous
data processing systems.

</details>


### [5] [Do Language Models Agree with Human Perceptions of Suspense in Stories?](https://arxiv.org/abs/2508.15794)
*Glenn Matlin,Devin Zhang,Rodrigo Barroso Loza,Diana M. Popescu,Joni Isbell,Chandreyi Chakraborty,Mark Riedl*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LMs）能识别文本是否意图制造悬念，但无法像人类一样准确感知悬念的相对程度及其动态变化，表明LMs处理悬念的方式与人类不同。


<details>
  <summary>Details</summary>
Motivation: 悬念是人类对叙事文本的一种复杂认知情感反应。研究旨在评估大型语言模型（LMs）在感知和处理文本悬念方面的能力，并与人类的心理模型进行比较。

Method: 研究复刻了四项关于人类悬念感知的经典心理学研究，将人类的反应替换为不同开源和闭源LMs的响应。此外，通过对抗性地打乱故事文本，探究导致人类和LMs悬念感知差异的原因。

Result: LMs能够区分文本是否意图引发悬念，但无法像人类判断那样准确估计文本序列中悬念的相对量，也无法正确捕捉跨多个文本片段的悬念起伏。对抗性测试进一步揭示了人与LM感知的差异。

Conclusion: 尽管LMs能够表面上识别和追踪悬念的某些方面，但它们处理悬念的方式与人类读者根本不同。

Abstract: Suspense is an affective response to narrative text that is believed to
involve complex cognitive processes in humans. Several psychological models
have been developed to describe this phenomenon and the circumstances under
which text might trigger it. We replicate four seminal psychological studies of
human perceptions of suspense, substituting human responses with those of
different open-weight and closed-source LMs. We conclude that while LMs can
distinguish whether a text is intended to induce suspense in people, LMs cannot
accurately estimate the relative amount of suspense within a text sequence as
compared to human judgments, nor can LMs properly capture the human perception
for the rise and fall of suspense across multiple text segments. We probe the
abilities of LM suspense understanding by adversarially permuting the story
text to identify what cause human and LM perceptions of suspense to diverge. We
conclude that, while LMs can superficially identify and track certain facets of
suspense, they do not process suspense in the same way as human readers.

</details>


### [6] [Benchmarking the Legal Reasoning of LLMs in Arabic Islamic Inheritance Cases](https://arxiv.org/abs/2508.15796)
*Nouar AlDahoul,Yasir Zaki*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）在处理伊斯兰遗产分配中的推理能力，发现一个多数投票集成方案（结合Gemini Flash 2.5, Gemini Pro 2.5和GPT o3）表现最佳，达到了92.7%的准确率。


<details>
  <summary>Details</summary>
Motivation: 人工计算伊斯兰遗产份额复杂、耗时且易出错。鉴于大型语言模型在复杂法律推理任务中的潜力，本研究旨在评估其解释和应用伊斯兰继承法的能力。

Method: 利用ArabicNLP QIAS 2025挑战赛的阿拉伯语伊斯兰遗产案例数据集，评估了多种基础和微调LLM模型在准确识别继承人、计算份额和提供推理方面的表现。特别提出了一个基于三个基础模型（Gemini Flash 2.5, Gemini Pro 2.5和GPT o3）的多数投票解决方案进行评估。

Result: 提出的多数投票解决方案在所有难度级别上均优于其他被评估的模型，取得了高达92.7%的准确率，并在Qias 2025挑战赛任务1中获得了总排名第三的成绩。

Conclusion: LLMs，特别是通过多数投票等集成方法，在解释和应用复杂的伊斯兰继承法方面展现出显著的能力，为自动化解决这一挑战性领域提供了有前景的解决方案。

Abstract: Islamic inheritance domain holds significant importance for Muslims to ensure
fair distribution of shares between heirs. Manual calculation of shares under
numerous scenarios is complex, time-consuming, and error-prone. Recent
advancements in Large Language Models (LLMs) have sparked interest in their
potential to assist with complex legal reasoning tasks. This study evaluates
the reasoning capabilities of state-of-the-art LLMs to interpret and apply
Islamic inheritance laws. We utilized the dataset proposed in the ArabicNLP
QIAS 2025 challenge, which includes inheritance case scenarios given in Arabic
and derived from Islamic legal sources. Various base and fine-tuned models, are
assessed on their ability to accurately identify heirs, compute shares, and
justify their reasoning in alignment with Islamic legal principles. Our
analysis reveals that the proposed majority voting solution, leveraging three
base models (Gemini Flash 2.5, Gemini Pro 2.5, and GPT o3), outperforms all
other models that we utilized across every difficulty level. It achieves up to
92.7% accuracy and secures the third place overall in Task 1 of the Qias 2025
challenge.

</details>


### [7] [Benchmarking the Medical Understanding and Reasoning of Large Language Models in Arabic Healthcare Tasks](https://arxiv.org/abs/2508.15797)
*Nouar AlDahoul,Yasir Zaki*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）在阿拉伯语医疗自然语言处理任务中的表现，发现其在不同任务中准确性差异显著但语义对齐良好。其中，一个结合了多个LLMs的多数投票方案在多项选择题（MCQs）任务中表现最佳，并赢得了一项挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在阿拉伯语通用自然语言处理（NLP）应用中展现出令人印象深刻的能力，但它们在阿拉伯语医疗NLP领域的有效性尚未得到充分研究。本研究旨在探究最先进的LLMs在阿拉伯语医疗保健知识展示和表达方面的程度和能力。

Method: 研究使用MedArabiQ2025赛道中Arabic NLP AraHealthQA挑战提出的医疗数据集，对多种LLMs进行基准测试。评估任务包括从现有选项中准确选择答案的多项选择题（MCQs）、填空题以及与专家答案语义对齐的开放式问答。此外，针对MCQs任务，提出了一个利用Gemini Flash 2.5、Gemini Pro 2.5和GPT o3三个基础模型的多数投票解决方案。

Result: 研究结果显示，LLMs在正确答案预测准确性方面存在显著差异，但在生成答案的语义对齐方面差异较小。在MCQs任务中，提出的多数投票解决方案以高达77%的准确率超越其他模型，并在Arahealthqa 2025共享任务-第2赛道（子任务1）挑战中获得第一名。在开放式问答任务中，多个LLMs在语义对齐方面表现出色，最高BERTScore达到86.44%。

Conclusion: 当前的大型语言模型在阿拉伯语临床环境中既展现出潜力也存在局限性。多模型集成（如多数投票）可以显著提升特定任务（如MCQs）的性能，且LLMs在阿拉伯语医疗开放式问答的语义对齐方面表现出强大的能力。

Abstract: Recent progress in large language models (LLMs) has showcased impressive
proficiency in numerous Arabic natural language processing (NLP) applications.
Nevertheless, their effectiveness in Arabic medical NLP domains has received
limited investigation. This research examines the degree to which
state-of-the-art LLMs demonstrate and articulate healthcare knowledge in
Arabic, assessing their capabilities across a varied array of Arabic medical
tasks. We benchmark several LLMs using a medical dataset proposed in the Arabic
NLP AraHealthQA challenge in MedArabiQ2025 track. Various base LLMs were
assessed on their ability to accurately provide correct answers from existing
choices in multiple-choice questions (MCQs) and fill-in-the-blank scenarios.
Additionally, we evaluated the capacity of LLMs in answering open-ended
questions aligned with expert answers. Our results reveal significant
variations in correct answer prediction accuracy and low variations in semantic
alignment of generated answers, highlighting both the potential and limitations
of current LLMs in Arabic clinical contexts. Our analysis shows that for MCQs
task, the proposed majority voting solution, leveraging three base models
(Gemini Flash 2.5, Gemini Pro 2.5, and GPT o3), outperforms others, achieving
up to 77% accuracy and securing first place overall in the Arahealthqa 2025
shared task-track 2 (sub-task 1) challenge. Moreover, for the open-ended
questions task, several LLMs were able to demonstrate excellent performance in
terms of semantic alignment and achieve a maximum BERTScore of 86.44%.

</details>


### [8] [Persuasiveness and Bias in LLM: Investigating the Impact of Persuasiveness and Reinforcement of Bias in Language Models](https://arxiv.org/abs/2508.15798)
*Saumya Roy*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLMs）的劝说能力如何与偏见放大相互作用，揭示了其在传递事实的同时，也可能无意中传播错误信息和偏见叙事的风险，并呼吁制定保障措施。


<details>
  <summary>Details</summary>
Motivation: 鉴于LLMs生成令人信服文本的强大能力及其在内容创作和决策支持中的广泛应用，但同时也存在传播错误信息和反映社会偏见的风险，本研究旨在探究LLMs中劝说与偏见的相互作用，特别是考察基于角色的模型在传递事实信息的同时，是否会无意中推广错误信息或偏见叙事。

Method: 研究引入了“劝说者-怀疑者”框架，LLMs扮演不同角色以模拟真实态度。怀疑者模型充当人类代理，通过比较其在接触劝说者模型论点前后的信念，并使用Jensen-Shannon散度量化劝说效果。随后，评估了被劝服实体如何强化和放大种族、性别和宗教相关的偏见信念。此外，对强劝说者使用谄媚性对抗性提示进行偏见探测，并由其他模型进行评估。

Result: 研究发现LLMs既有希望也有风险。它们能塑造叙事、调整语气和迎合受众价值观，但这种能力也可能被滥用，自动化传播错误信息或利用认知偏见，从而强化刻板印象并扩大不平等。研究指出，核心危险在于滥用而非偶发性模型错误。

Conclusion: 通过衡量劝说能力和偏见强化，研究主张应制定保障措施和政策，以惩罚欺骗性使用，并支持模型的对齐、价值敏感设计和可信赖部署。

Abstract: Warning: This research studies AI persuasion and bias amplification that
could be misused; all experiments are for safety evaluation. Large Language
Models (LLMs) now generate convincing, human-like text and are widely used in
content creation, decision support, and user interactions. Yet the same systems
can spread information or misinformation at scale and reflect social biases
that arise from data, architecture, or training choices. This work examines how
persuasion and bias interact in LLMs, focusing on how imperfect or skewed
outputs affect persuasive impact. Specifically, we test whether persona-based
models can persuade with fact-based claims while also, unintentionally,
promoting misinformation or biased narratives.
  We introduce a convincer-skeptic framework: LLMs adopt personas to simulate
realistic attitudes. Skeptic models serve as human proxies; we compare their
beliefs before and after exposure to arguments from convincer models.
Persuasion is quantified with Jensen-Shannon divergence over belief
distributions. We then ask how much persuaded entities go on to reinforce and
amplify biased beliefs across race, gender, and religion. Strong persuaders are
further probed for bias using sycophantic adversarial prompts and judged with
additional models.
  Our findings show both promise and risk. LLMs can shape narratives, adapt
tone, and mirror audience values across domains such as psychology, marketing,
and legal assistance. But the same capacity can be weaponized to automate
misinformation or craft messages that exploit cognitive biases, reinforcing
stereotypes and widening inequities. The core danger lies in misuse more than
in occasional model mistakes. By measuring persuasive power and bias
reinforcement, we argue for guardrails and policies that penalize deceptive use
and support alignment, value-sensitive design, and trustworthy deployment.

</details>


### [9] [A Framework for Processing Textual Descriptions of Business Processes using a Constrained Language -- Technical Report](https://arxiv.org/abs/2508.15799)
*Andrea Burattin,Antonio Grama,Ana-Maria Sima,Andrey Rivkin,Barbara Weber*

Main category: cs.CL

TL;DR: 报告提出BeePath框架，利用受限自然语言和大型语言模型，帮助非专业人员通过文本描述创建形式化过程模型。


<details>
  <summary>Details</summary>
Motivation: 旨在使非专业人员能够通过简单的纯文本描述，利用自然语言来开发过程模型。

Method: 提出BeePath框架，该框架允许用户以受限的基于模式的语言编写过程描述，并将其转换为Petri网和DECLARE等形式化模型。同时，利用大型语言模型（LLMs）辅助将非结构化描述转换为这种受限语言。

Result: 报告提出并设计了BeePath框架，该框架能够将非专业人员的纯文本描述转化为形式化过程模型，并能利用LLMs辅助此转换过程。

Conclusion: BeePath框架通过结合受限自然语言和LLMs，为非专业人员提供了一种直观高效的方式来创建形式化过程模型。

Abstract: This report explores how (potentially constrained) natural language can be
used to enable non-experts to develop process models by simply describing
scenarios in plain text. To this end, a framework, called BeePath, is proposed.
It allows users to write process descriptions in a constrained pattern-based
language, which can then be translated into formal models such as Petri nets
and DECLARE. The framework also leverages large language models (LLMs) to help
convert unstructured descriptions into this constrained language.

</details>


### [10] [A BERT-based Hierarchical Classification Model with Applications in Chinese Commodity Classification](https://arxiv.org/abs/2508.15800)
*Kun Liu,Tuozhen Liu,Feifei Wang,Rui Pan*

Main category: cs.CL

TL;DR: 本研究发布了一个京东大规模层级数据集，并提出HFT-BERT模型，用于高效、一致的产品层级分类，尤其在处理较长文本时表现卓越。


<details>
  <summary>Details</summary>
Motivation: 现有电商平台产品分类依赖人工标注，效率低下且不一致。同时，现有研究未能充分利用产品层级结构信息，也未考虑跨层级类别间的相似性和差异性。

Method: 1. 发布一个从京东收集的包含1,011,450个产品标题和三级类别结构的大规模层级数据集。2. 提出一种基于BERT的创新层级文本分类方法——层级微调BERT (HFT-BERT)。

Result: HFT-BERT在短文本分类上达到与现有方法相当的预测性能。在分类较长的短文本（如书籍）时，HFT-BERT模型展现出卓越的性能。

Conclusion: 本研究通过提供新的大规模层级数据集和高效的HFT-BERT模型，解决了电商产品分类的痛点，并充分利用了层级信息，尤其在长短文本分类中展现了显著优势和应用潜力。

Abstract: Existing e-commerce platforms heavily rely on manual annotation for product
categorization, which is inefficient and inconsistent. These platforms often
employ a hierarchical structure for categorizing products; however, few studies
have leveraged this hierarchical information for classification. Furthermore,
studies that consider hierarchical information fail to account for similarities
and differences across various hierarchical categories. Herein, we introduce a
large-scale hierarchical dataset collected from the JD e-commerce platform
(www.JD.com), comprising 1,011,450 products with titles and a three-level
category structure. By making this dataset openly accessible, we provide a
valuable resource for researchers and practitioners to advance research and
applications associated with product categorization. Moreover, we propose a
novel hierarchical text classification approach based on the widely used
Bidirectional Encoder Representations from Transformers (BERT), called
Hierarchical Fine-tuning BERT (HFT-BERT). HFT-BERT leverages the remarkable
text feature extraction capabilities of BERT, achieving prediction performance
comparable to those of existing methods on short texts. Notably, our HFT-BERT
model demonstrates exceptional performance in categorizing longer short texts,
such as books.

</details>


### [11] [LingVarBench: Benchmarking LLM for Automated Named Entity Recognition in Structured Synthetic Spoken Transcriptions](https://arxiv.org/abs/2508.15801)
*Seyedali Mohammadi,Manas Paldhe,Amit Chhabra*

Main category: cs.CL

TL;DR: LingVarBench是一个通过LLM生成合成对话数据并自动化提示优化，以解决电话录音结构化信息提取成本高昂和隐私挑战的管道，显著提升了真实数据上的提取准确性。


<details>
  <summary>Details</summary>
Motivation: 电话录音转录标注成本极高（约2美元/分钟），且受隐私法规、合规要求和高昂人工（1小时音频需3小时专家时间）限制。现有提取方法在包含不流畅、打断和说话人重叠的对话语音上效果不佳。

Method: LingVarBench采用合成数据生成管道：1) LLM生成逼真的结构化字段值；2) LLM将这些值转化为数千个包含电话通话特征的自然对话语句；3) 通过独立的LLM提取器验证合成语句能否恢复原始信息。最后，利用DSPy的SIMBA优化器从经验证的合成文本中自动合成提取提示，替代手动工程。

Result: 在真实客户电话录音上，优化的提示显著优于零样本提示：数值字段准确率高达95%（零样本为88-89%），名称90%（零样本为47-79%），日期超过80%（零样本为72-77%）。合成到真实的迁移表明，从生成数据中学到的对话模式能有效泛化到包含背景噪音和领域术语的真实电话。

Conclusion: LingVarBench首次为从合成对话数据中进行结构化提取提供了系统性基准，证明自动化提示优化能够有效克服成本和隐私障碍，从而实现商业环境下大规模电话分析。

Abstract: Phone call transcript labeling is prohibitively expensive (approximately 2
USD per minute) due to privacy regulations, consent requirements, and manual
annotation costs requiring 3 hours of expert time per hour of audio. Existing
extraction methods fail on conversational speech containing disfluencies,
interruptions, and speaker overlap. We introduce LingVarBench, a synthetic data
generation pipeline that addresses these constraints through automated
validation. First, we prompt an LLM to generate realistic structured field
values across multiple use cases. Second, we recursively prompt the model to
transform these values into thousands of natural conversational utterances
containing typical phone call characteristics. Third, we validate each
synthetic utterance by testing whether a separate LLM-based extractor can
recover the original structured information. We employ DSPy's SIMBA optimizer
to automatically synthesize extraction prompts from validated synthetic
transcripts, eliminating manual prompt engineering. Our optimized prompts
achieve up to 95 percent accuracy for numeric fields (vs. 88-89 percent
zero-shot), 90 percent for names (vs. 47-79 percent), and over 80 percent for
dates (vs. 72-77 percent) on real customer transcripts, demonstrating
substantial gains over zero-shot prompting. The synthetic-to-real transfer
demonstrates that conversational patterns learned from generated data
generalize effectively to authentic phone calls containing background noise and
domain-specific terminology. LingVarBench provides the first systematic
benchmark for structured extraction from synthetic conversational data,
demonstrating that automated prompt optimization overcomes cost and privacy
barriers preventing large-scale phone call analysis in commercial settings.

</details>


### [12] [MAC: A Live Benchmark for Multimodal Large Language Models in Scientific Understanding](https://arxiv.org/abs/2508.15802)
*Mohan Jiang,Jin Gao,Jiahao Zhan,Dequan Wang*

Main category: cs.CL

TL;DR: 本文引入了MAC，一个动态演进的科学封面基准，用于评估多模态大语言模型（MLLMs）的高级科学理解能力。研究发现MLLMs在跨模态科学推理方面仍有不足，并提出DAD方法将性能提升高达11%，同时展示了MAC基准与知识前沿同步演进的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型（MLLMs）能力的不断提升，现有的固定基准已无法有效评估其高级科学理解能力，急需一个能随科学进步和模型发展而持续演进的评估基准。

Method: 1. **MAC基准**: 构建了一个名为MAC（Multimodal Academic Cover）的“活”基准，该基准包含超过25,000对来自顶级科学期刊（如Nature, Science, Cell）的图像-文本对，旨在挑战MLLMs进行抽象视觉和文本内容的跨模态科学推理。
2. **DAD方法**: 提出了一种轻量级推理时方法DAD，通过将MLLM的视觉特征与语言空间推理相结合来增强其性能。

Result: 1. 在最新年度快照MAC-2025上的实验表明，MLLMs虽然表现出强大的感知能力，但其跨模态科学推理能力仍有限。
2. DAD方法能够将MLLMs的性能提升高达11%。
3. 通过更新期刊封面和模型策展的实验，验证了MAC作为“活”基准的潜力，能够持续与人类知识前沿保持一致。

Conclusion: MAC基准有效解决了现有固定基准在评估MLLMs高级科学理解能力方面的不足，提供了一个动态、持续演进的评估框架。DAD方法也为提升MLLMs的跨模态科学推理能力提供了一条有效途径。MAC有望成为一个能与科学知识前沿同步发展的评估工具。

Abstract: As multimodal large language models (MLLMs) grow increasingly capable, fixed
benchmarks are gradually losing their effectiveness in evaluating high-level
scientific understanding. In this paper, we introduce the Multimodal Academic
Cover benchmark (MAC), a live benchmark that could continuously evolve with
scientific advancement and model progress. MAC leverages over 25,000 image-text
pairs sourced from issues of top-tier scientific journals such as Nature,
Science, and Cell, challenging MLLMs to reason across abstract visual and
textual scientific content. Experiments on our most recent yearly snapshot,
MAC-2025, reveal that while MLLMs demonstrate strong perceptual abilities,
their cross-modal scientific reasoning remains limited. To bridge this gap, we
propose DAD, a lightweight inference-time approach that enhances MLLMs by
extending MLLM visual features with language space reasoning, achieving
performance improvements of up to 11%. Finally, we highlight the live nature of
MAC through experiments on updating journal covers and models for curation,
illustrating its potential to remain aligned with the frontier of human
knowledge. We release our benchmark at
https://github.com/mhjiang0408/MAC_Bench.

</details>


### [13] [ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks](https://arxiv.org/abs/2508.15804)
*Minghao Li,Ying Zeng,Zhihao Cheng,Cong Ma,Kai Jia*

Main category: cs.CL

TL;DR: 本文提出了ReportBench，一个系统基准来评估大型语言模型（LLMs）生成的深度研究报告的内容质量，重点关注引用文献质量和报告陈述的忠实性与真实性，并发现商业研究代理表现优于独立LLM但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 深度研究代理显著缩短了研究时间，但其任务对事实准确性和全面性有严格要求。在广泛应用之前，需要进行彻底评估以确保其高质量输出。

Method: 本文提出了ReportBench，一个系统基准，通过两个维度评估研究报告内容质量：1) 引用文献的质量和相关性；2) 报告中陈述的忠实性和真实性。ReportBench利用arXiv上的高质量调查论文作为黄金标准，通过逆向提示工程构建领域特定提示和评估语料库。此外，开发了一个基于代理的自动化框架，提取引用和陈述，核实引用内容的真实性，并使用网络资源验证未引用的声明。

Result: 实证评估表明，OpenAI和Google等商业深度研究代理生成的报告比结合搜索或浏览工具的独立LLMs更全面、更可靠。然而，在研究覆盖的广度和深度以及事实一致性方面仍有很大的改进空间。

Conclusion: ReportBench提供了一个评估LLM生成研究报告内容质量的系统方法。尽管商业深度研究代理表现优于独立LLMs，但在全面性和准确性方面仍需显著提升，以满足严格的研究标准。

Abstract: The advent of Deep Research agents has substantially reduced the time
required for conducting extensive research tasks. However, these tasks
inherently demand rigorous standards of factual accuracy and comprehensiveness,
necessitating thorough evaluation before widespread adoption. In this paper, we
propose ReportBench, a systematic benchmark designed to evaluate the content
quality of research reports generated by large language models (LLMs). Our
evaluation focuses on two critical dimensions: (1) the quality and relevance of
cited literature, and (2) the faithfulness and veracity of the statements
within the generated reports. ReportBench leverages high-quality published
survey papers available on arXiv as gold-standard references, from which we
apply reverse prompt engineering to derive domain-specific prompts and
establish a comprehensive evaluation corpus. Furthermore, we develop an
agent-based automated framework within ReportBench that systematically analyzes
generated reports by extracting citations and statements, checking the
faithfulness of cited content against original sources, and validating
non-cited claims using web-based resources. Empirical evaluations demonstrate
that commercial Deep Research agents such as those developed by OpenAI and
Google consistently generate more comprehensive and reliable reports than
standalone LLMs augmented with search or browsing tools. However, there remains
substantial room for improvement in terms of the breadth and depth of research
coverage, as well as factual consistency. The complete code and data will be
released at the following link: https://github.com/ByteDance-BandAI/ReportBench

</details>


### [14] [ALAS: Autonomous Learning Agent for Self-Updating Language Models](https://arxiv.org/abs/2508.15805)
*Dhruv Atreja*

Main category: cs.CL

TL;DR: ALAS是一个模块化自主学习系统，能持续更新大语言模型（LLM）的知识，无需人工干预。它通过自主生成课程、检索、数据蒸馏和微调，将LLM对新信息的问答准确率从15%提升至90%。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）存在知识截止日期，导致它们在处理新兴信息时准确性受限。

Method: ALAS系统采用模块化管道，自主生成学习课程，从网络检索最新信息（带引用），将其提炼为问答训练数据，并通过SFT和DPO进行模型微调。系统迭代评估并修订课程，实现长期持续学习。各组件可互换且基于标准API构建。

Result: ALAS在快速发展领域（如Python新版本、最新CVE、学术趋势）实现了模型的自我改进，将知识截止日期后的问答准确率平均从15%提升至90%，且无需手动数据集整理，以最小工程开销达到了高效性能。

Conclusion: ALAS系统有效实现了LLM的知识自主持续更新，显著提升了其对新兴信息的处理能力，为LLM的终身学习提供了可行方案，但仍面临成本和信息源质量依赖等局限性。

Abstract: Large language models (LLMs) often have a fixed knowledge cutoff, limiting
their accuracy on emerging information. We present ALAS (Autonomous Learning
Agent System), a modular pipeline that continuously updates an LLM's knowledge
with minimal human intervention. ALAS autonomously generates a learning
curriculum for a target domain, retrieves up-to-date information from the web
(with citations), distills this into question-answer training data, and
fine-tunes the model through supervised fine-tuning (SFT) and direct preference
optimization (DPO). It iteratively evaluates performance and revises the
curriculum, enabling long-term continual learning. We demonstrate ALAS's
ability to self-improve a model on rapidly evolving domains (e.g., new Python
releases, latest security CVEs, academic trends), significantly boosting
post-cutoff question answering accuracy (from 15% to 90% on average) without
manual dataset curation. The system emphasizes modularity and reproducibility:
each component (planning, retrieval, distillation, memory, fine-tuning) is
interchangeable and built on standard APIs. We discuss comparative baselines
(e.g., retrieval-augmented generation vs. fine-tuning) and show that ALAS
achieves 90% accuracy on knowledge-updated queries with minimal engineering
overhead. Finally, we outline limitations (cost, dependency on source quality)
and future directions for autonomous lifelong learning in LLMs.

</details>


### [15] [SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need for Robust KV Cache Compression](https://arxiv.org/abs/2508.15806)
*Mengjie Li,William J. Song*

Main category: cs.CL

TL;DR: 为解决LLM长序列导致的KV Cache压力问题，本文通过分析注意力行为（表面记忆、逻辑构建等），提出了一种双阶段SurfaceLogicKV方法进行KV Cache压缩，有效提升了压缩鲁棒性并保持了良好性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）输入序列长度的增加，键值（KV）缓存存储面临巨大压力，使得高效推理成为挑战。

Method: 研究将注意力行为明确区分为“表面记忆”和“逻辑构建”。通过观察发现，单个注意力头有多种行为：98.5%有效忽略无关信息，1.5%用于逻辑构建，0.5%用于表面记忆。基于层级和头级的集成，提出了一种新颖的双阶段SurfaceLogicKV方法，利用这些注意力行为进行KV Cache压缩。

Result: 该方法在各种任务和长序列上实现了改进的压缩鲁棒性，并在与基线模型甚至在某些特定情况下与FullKV相比，保持了竞争性能。

Conclusion: 通过对注意力行为的细致区分和利用，所提出的SurfaceLogicKV方法能有效优化KV Cache压缩，为LLM长上下文推理提供更高效、更鲁棒的解决方案。

Abstract: The increasing input sequence length in Large Language Models (LLMs) puts
significant pressure on key-value (KV) cache storage, making efficient
inference challenging. Explicitly distinguishing attention behavior into our
self-defined surface memorization and logic construction reveals essential
roles in long-context reasoning. We observe that an individual attention head
can display various behaviors, with nearly 98.5% effectively ignoring
completely irrelevant information. The remaining 1.5% behaves as logic
construction, and 0.5% behaves as surface memorization. Based on layer- and
head-wise integration, we propose a novel two-stage SurfaceLogicKV method to
utilize these attention behaviors for KV Cache compression. As a result, it
achieves improved compressing robustness while maintaining competitive
performance across various tasks and long sequences compared to baselines or
even FullKV in some specific situations

</details>


### [16] [KL-based self-distillation for large language models](https://arxiv.org/abs/2508.15807)
*Max Rehman Linder*

Main category: cs.CL

TL;DR: 针对冻结LLM在新领域术语整合上的挑战，本文提出一种基于KL散度的数学化知识蒸馏方法，实现词汇扩展，即便原始与扩展模型分词不同。该方法在代码生成任务中超越了传统交叉熵训练，并深入分析了新词元表征学习机制。


<details>
  <summary>Details</summary>
Motivation: 大型预训练语言模型在小规模专业语料上进行微调时，难以有效整合新的领域特定术语，尤其是在冻结的LLM中，如何进行词汇扩展是主要挑战。

Method: 引入了一种基于KL散度的数学化知识蒸馏方法，用于冻结LLM的词汇扩展，即使原始模型和扩展模型使用不同的分词方式也能进行知识继承。该方法与传统的交叉熵训练进行比较，并评估了多种新词元嵌入初始化策略。模型在初始化后进一步微调。最后，通过机械可解释性分析新词元表征的学习机制。

Result: 在约2000个代码生成任务的基准测试中，所提出的基于KL散度的蒸馏方法全面超越了传统交叉熵训练，取得了最佳性能。

Conclusion: 本研究提供了一种有效的冻结LLM词汇扩展方法，在领域特定任务（代码生成）中表现优异。通过机械可解释性分析，解释了性能提升的原因，并深入理解了模型在词汇扩展过程中新词元表征的学习机制和嵌入空间的结构。

Abstract: Large pre-trained language models often struggle to incorporate new
domain-specific terminology when fine-tuned on small, specialized corpora. In
this work, we address the challenge of vocabulary expansion in frozen LLMs by
introducing a mathematically grounded method for knowledge distillation via KL
divergence, even when the original and extended models use different
tokenizations. This allows the student model to inherit distributional
knowledge from the teacher despite differing vocabularies. We compare our
KL-based distillation approach to conventional cross-entropy training,
evaluating both methods across multiple strategies for initializing new token
embeddings. After embedding initialization, models are further fine-tuned to
integrate the new vocabulary. Each trained model is benchmarked on
approximately 2000 code-generation tasks, where our approach achieves the best
performance across the board. Finally, through mechanistic interpretability, we
analyze how models learn representations for the new tokens, providing an
explanation for the observed gains and offering insight into the structure of
embedding space during vocabulary expansion.

</details>


### [17] [Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table Understanding via Multi-Agent Collaboration](https://arxiv.org/abs/2508.15809)
*Songyuan Sui,Hongyi Liu,Serena Liu,Li Li,Soo-Hyun Choi,Rui Chen,Xia Hu*

Main category: cs.CL

TL;DR: 提出Chain-of-Query (CoQ) 多智能体框架，通过优化SQL生成和推理策略，显著提升了LLM在SQL辅助表格理解中的准确性和有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）难以处理表格数据的结构复杂性。现有多智能体SQL生成框架在表格结构理解不足、错误传播和过度依赖执行结果方面存在局限性。

Method: 提出Chain-of-Query (CoQ) 多智能体框架，用于SQL辅助的表格理解。CoQ采用自然语言风格的表格Schema表示以抽象结构噪声，利用分句式SQL生成策略提高查询质量，并引入混合推理机制，将基于SQL的机械推理与基于LLM的逻辑推理分离。

Result: 在五个常用基准数据集和四种模型（包括闭源和开源）上的实验表明，CoQ将准确率从61.11%显著提升至74.77%，并将无效SQL率从9.48%降低至3.34%。

Conclusion: Chain-of-Query (CoQ) 在表格理解方面表现出卓越的有效性，成功解决了现有方法的局限性，显著提高了准确性并减少了无效查询。

Abstract: Table understanding requires structured, multi-step reasoning. Large Language
Models (LLMs) struggle with it due to the structural complexity of tabular
data. Recently, multi-agent frameworks for SQL generation have shown promise in
tackling the challenges of understanding tabular data, but existing approaches
often suffer from limitations such as the inability to comprehend table
structure for reliable SQL generation, error propagation that results in
invalid queries, and over-reliance on execution correctness. To address these
issues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for
SQL-aided table understanding. CoQ adopts natural-language-style
representations of table schemas to abstract away structural noise and enhance
understanding. It employs a clause-by-clause SQL generation strategy to improve
query quality and introduces a hybrid reasoning division that separates
SQL-based mechanical reasoning from LLM-based logical inference, thereby
reducing reliance on execution outcomes. Experiments with four models (both
closed- and open-source) across five widely used benchmarks show that
Chain-of-Query significantly improves accuracy from 61.11% to 74.77% and
reduces the invalid SQL rate from 9.48% to 3.34%, demonstrating its superior
effectiveness in table understanding. The code is available at
https://github.com/SongyuanSui/ChainofQuery.

</details>


### [18] [Detecting Hope, Hate, and Emotion in Arabic Textual Speech and Multi-modal Memes Using Large Language Models](https://arxiv.org/abs/2508.15810)
*Nouar AlDahoul,Yasir Zaki*

Main category: cs.CL

TL;DR: 本文评估了LLM在识别阿拉伯语社交媒体文本和表情包中仇恨言论、冒犯性语言和情感表达方面的能力，其中微调后的GPT-4o-mini和Gemini Flash 2.5在Mahed 2025挑战赛中表现卓越并获得总分第一名。


<details>
  <summary>Details</summary>
Motivation: 随着阿拉伯语社交媒体内容（文本和表情包）中仇恨言论和冒犯性语言的日益传播，市场迫切需要对这些数字内容进行精确分析。

Method: 研究评估了基础LLM、微调LLM和预训练嵌入模型在识别阿拉伯语文本和表情包中的希望、仇恨言论、冒犯性语言和情感表达方面的性能，并使用ArabicNLP MAHED 2025挑战赛的数据集进行评估。

Result: 针对阿拉伯语文本微调的GPT-4o-mini和针对阿拉伯语表情包微调的Gemini Flash 2.5表现出卓越性能，分别在任务1、2、3中取得了72.1%、57.8%和79.6%的宏F1分数，并荣获Mahed 2025挑战赛的总分第一名。

Conclusion: 所提出的解决方案为实现准确高效的阿拉伯语内容审核系统提供了对文本和表情包更细致的理解。

Abstract: The rise of social media and online communication platforms has led to the
spread of Arabic textual posts and memes as a key form of digital expression.
While these contents can be humorous and informative, they are also
increasingly being used to spread offensive language and hate speech.
Consequently, there is a growing demand for precise analysis of content in
Arabic text and memes. This paper explores the potential of large language
models to effectively identify hope, hate speech, offensive language, and
emotional expressions within such content. We evaluate the performance of base
LLMs, fine-tuned LLMs, and pre-trained embedding models. The evaluation is
conducted using a dataset of Arabic textual speech and memes proposed in the
ArabicNLP MAHED 2025 challenge. The results underscore the capacity of LLMs
such as GPT-4o-mini, fine-tuned with Arabic textual speech, and Gemini Flash
2.5, fine-tuned with Arabic memes, to deliver the superior performance. They
achieve up to 72.1%, 57.8%, and 79.6% macro F1 scores for tasks 1, 2, and 3,
respectively, and secure first place overall in the Mahed 2025 challenge. The
proposed solutions offer a more nuanced understanding of both text and memes
for accurate and efficient Arabic content moderation systems.

</details>


### [19] [From Clicks to Preference: A Multi-stage Alignment Framework for Generative Query Suggestion in Conversational System](https://arxiv.org/abs/2508.15811)
*Junhao Yin,Haolin Wang,Peng Bao,Ju Xu,Yongliang Wang*

Main category: cs.CL

TL;DR: 本文提出了一个多阶段框架，通过提示工程、基于点击日志蒸馏的监督微调、高斯奖励模型（GaRM）和强化学习，提升大语言模型生成查询建议与用户偏好的对齐，并在A/B测试中实现了34%的用户参与度提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成查询建议可以增强对话系统，但其输出难以与用户细微偏好对齐，是当前面临的关键挑战。

Method: 引入一个多阶段框架实现生成策略与用户意图的渐进对齐。该框架首先通过提示工程进行冷启动，随后在监督微调（SFT）阶段利用点击日志上的蒸馏方法构建鲁棒的基础模型。为更好地建模用户偏好及其不确定性，开发了高斯奖励模型（GaRM），将用户偏好表示为概率分布。最后，采用强化学习将生成策略与这些偏好对齐，并通过结合GaRM和辅助启发式方法的复合奖励函数进行指导，同时引入离群点正则化和两阶段奖励融合技术以维持训练稳定性。

Result: 该框架在自动和人工评估中均显著优于基线模型。在实时A/B测试中，用户参与度（通过点击率衡量）相对提高了34%。

Conclusion: 所提出的多阶段框架，结合了多种技术创新，有效解决了大语言模型生成查询建议与用户偏好对齐的难题，显著提升了用户体验和参与度。

Abstract: Generative query suggestion using large language models offers a powerful way
to enhance conversational systems, but aligning outputs with nuanced user
preferences remains a critical challenge. To address this, we introduce a
multi-stage framework designed for progressive alignment between the generation
policy and user intent. Our pipeline begins with prompt engineering as a
cold-start strategy, followed by the Supervised Fine-Tuning stage, in which we
introduce a distillation method on click logs to create a robust foundational
model. To better model user preferences while capturing their inherent
uncertainty, we develop a Gaussian Reward Model (GaRM) that represents user
preferences as probability distributions rather than point estimates. Finally,
we employ reinforcement learning to align the generation policy with these
preferences, guided by a composite reward function that integrates GaRM with
auxiliary heuristics to mitigate reward hacking. To maintain training
stability, this process is enhanced by a novel out-of-distribution
regularization method and a two-stage reward fusion technique. Extensive
experiments demonstrate that our framework significantly outperforms baselines
on both automatic and human evaluations and yields a 34\% relative increase in
user engagement as measured by click-through rate in live A/B tests.

</details>


### [20] [SCOPE: A Generative Approach for LLM Prompt Compression](https://arxiv.org/abs/2508.15813)
*Tinghui Zhang,Yifan Wang,Daisy Zhe Wang*

Main category: cs.CL

TL;DR: 提出一种基于分块-总结的生成式提示词压缩方法，解决了现有token删除方法的局限性，在高压缩比下表现出更好的质量和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的提示词压缩方法主要基于token删除，存在信息丢失和结构不连贯等问题，从而限制了大型语言模型（LLM）的最终生成质量。

Method: 本文提出一种新颖的生成式提示词压缩方法，其核心机制是“分块-总结”。具体而言，该方法将提示词分割成语义连贯的块，对每个块进行精简重写，并最终将这些块重构成有意义的提示词。同时，设计了多种优化技术，包括优化语义分块、异常块处理、动态压缩比、压缩优先级和关键词维护。

Result: 在问答和摘要任务上进行的广泛评估显示，与现有最先进的方法相比，该方法实现了显著更好的压缩质量和更高的稳定性，特别是在高压缩比下表现优异。

Conclusion: 本文提出的生成式提示词压缩方法有效且实用，能够克服现有基于token删除方法的局限性，即使在高压缩比下也能保持高质量，从而提升LLM的效率和降低成本。

Abstract: Prompt compression methods enhance the efficiency of Large Language Models
(LLMs) and minimize the cost by reducing the length of input context. The goal
of prompt compression is to shorten the LLM prompt while maintaining a high
generation quality. However, existing solutions, mainly based on token removal,
face challenges such as information loss and structural incoherence, like
missing grammar elements in a sentence, or incomplete word phrases after token
removal. Such challenges limit the final generation quality of LLM.
  To overcome these limitations, we present a novel generative prompt
compression method. Unlike the existing token removal methods, our method
centers at a chunking-and-summarization mechanism. Specifically, our method
splits prompt into semantically coherent chunks and rewrites the chunks to be
more concise. The chunks are reconstructed into meaningful prompt finally. We
design several optimization techniques for the mechanism, including optimized
semantic chunking, outlier chunk handling, dynamic compression ratio,
compression prioritization, and keyword maintaining. These techniques
effectively improve the identifying and preserving of critical information and
coherence among texts, as well as providing finer grind control of the
compression ratio. We conduct extensive evaluation on question-answering and
summarization tasks, with datasets covering multiple different domain. The
evaluation shows our method achieves a significantly better compression
quality, and higher stability than the state-of-the-art methods, especially
under high compression ratio, which proves the effectiveness and practicality
of our method.

</details>


### [21] [User-Assistant Bias in LLMs](https://arxiv.org/abs/2508.15815)
*Xu Pan,Jingxuan Fan,Zidi Xiong,Ely Hahami,Jorin Overwiening,Ziqian Xie*

Main category: cs.CL

TL;DR: 本文 formalize LLM的“用户-助手偏见”，构建UserAssist数据集进行基准测试、理解和操纵。研究发现偏见受后训练方法影响（如人类偏好对齐增加偏见，CoT训练减少偏见），并可通过DPO双向调整，为检测和控制模型异常提供方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多轮对话中可能过度依赖自身或用户的聊天记录信息，导致行为过于固执或顺从。这种“用户-助手偏见”是需要被理解、检测和控制的模型特性。

Method: 1. 正式定义“用户-助手偏见”。2. 构建并引入8k多轮对话数据集UserAssist。3. 利用UserAssist-test对26个商业模型和26个开源模型的用户-助手偏见进行基准测试。4. 进行受控微调实验以确定后训练方法（如人类偏好对齐、思维链训练）对偏见的影响。5. 在UserAssist-train上通过直接偏好优化（DPO）双向调整用户-助手偏见。

Result: 1. 商业模型表现出不同程度的用户偏见。2. 开源模型中，指令微调模型有显著用户偏见，而推理模型偏见较弱。3. 人类偏好对齐会增加用户偏见，基于思维链推理轨迹的训练会减少偏见。4. 用户-助手偏见可通过DPO双向调整，并对域内和域外对话具有良好泛化性。

Conclusion: 本研究结果提供了LLM如何整合不同来源信息的新见解，并提出了一种检测和控制模型异常行为的可行方法。

Abstract: Large language models (LLMs) can bias towards relying on their own or the
user's information in chat history, leading to overly stubborn or agreeable
behaviors in multi-turn conversations. In this paper, we formalize this model
characteristic as user-assistant bias and introduce an 8k multi-turn
conversation dataset $\textbf{UserAssist}$, which we use to benchmark,
understand and manipulate the user-assistant bias in frontier LLMs. Leveraging
$\textbf{UserAssist-test}$, we first benchmark the user-assistant bias of 26
commercial and 26 open-weight models. Commercial models show various levels of
user bias. Evaluation on open-weight models reveals significant user bias in
the instruction-tuned models, and weak user bias in reasoning (or
reasoning-distilled) models. We then perform controlled fine-tuning experiments
to pinpoint the post-training recipe contributing to these bias shifts: human
preference alignment increases user bias, while training on chain-of-thought
reasoning traces decreases it. Finally, we demonstrate that user-assistant bias
can be bidirectionally adjusted by performing direct preference optimization
(DPO) on $\textbf{UserAssist-train}$, and generalizes well to both in-domain
and out-of-domain conversations. Our results provide insights into how the LLM
integrates information from different sources, and also a viable way to detect
and control model abnormalities.

</details>


### [22] [Meet Your New Client: Writing Reports for AI -- Benchmarking Information Loss in Market Research Deliverables](https://arxiv.org/abs/2508.15817)
*Paul F. Simmering,Benedikt Schulz,Oliver Tabino,Georg Wittenburg*

Main category: cs.CL

TL;DR: 本研究评估了PDF和PPTX文档在RAG系统中摄取时的信息丢失情况，发现文本提取可靠，但图表等复杂对象信息丢失严重，呼吁采用AI原生交付物。


<details>
  <summary>Details</summary>
Motivation: 随着RAG系统被组织用于知识管理，传统市场研究报告（如PDF、PPTX）不仅供人类阅读，也需被AI系统“阅读”以回答问题。因此，需要评估这些报告在摄取到RAG系统时是否存在信息损失，以确保其未来适用性。

Method: 通过端到端基准测试，比较了PDF和PowerPoint (PPTX) 文档转换为Markdown格式后，LLM能多好地利用它们来回答事实性问题，以评估信息在RAG系统摄取过程中的损失。

Result: 研究发现，文本信息可以可靠地被提取，但图表和示意图等复杂对象的信息会大量丢失。

Conclusion: 为确保研究洞察不会在转换过程中丢失，需要开发专门的、AI原生的交付物格式。

Abstract: As organizations adopt retrieval-augmented generation (RAG) for their
knowledge management systems (KMS), traditional market research deliverables
face new functional demands. While PDF reports and slides have long served
human readers, they are now also "read" by AI systems to answer user questions.
To future-proof reports being delivered today, this study evaluates information
loss during their ingestion into RAG systems. It compares how well PDF and
PowerPoint (PPTX) documents converted to Markdown can be used by an LLM to
answer factual questions in an end-to-end benchmark. Findings show that while
text is reliably extracted, significant information is lost from complex
objects like charts and diagrams. This suggests a need for specialized,
AI-native deliverables to ensure research insights are not lost in translation.

</details>


### [23] [Research on intelligent generation of structural demolition suggestions based on multi-model collaboration](https://arxiv.org/abs/2508.15820)
*Zhifeng Yang,Peizong Wu*

Main category: cs.CL

TL;DR: 本文提出了一种基于多模型协同的钢结构拆除建议智能生成方法，通过RAG和LoRA技术提升大模型在该领域的表现，并提供与结构特性高度一致且更具针对性的拆除建议。


<details>
  <summary>Details</summary>
Motivation: 现有钢结构拆除方案编制耗时，需要人工检索工程案例并整理语言，自动化和智能化程度低，设计师在信息检索和语言组织上效率不高。

Method: 提出了一种基于多模型协同的结构拆除建议智能生成方法，并结合检索增强生成（Retrieval-Augmented Generation, RAG）和低秩适应微调（Low-Rank Adaptation Fine-Tuning, LoRA）技术，以提高大型语言模型在结构拆除领域的文本生成性能。该框架从具体工程情况出发，驱动大型语言模型以拟人化思维进行回答。

Result: 所提出的多模型协同框架能够提出与结构特性高度一致的拆除建议。与CivilGPT相比，该框架能更专注于结构的关键信息，生成的建议也更具针对性。

Conclusion: 该多模型协同框架显著提升了钢结构拆除建议的智能生成水平，能够根据工程特点提供更具针对性的建议，优于现有模型如CivilGPT。

Abstract: The steel structure demolition scheme needs to be compiled according to the
specific engineering characteristics and the update results of the finite
element model. The designers need to refer to the relevant engineering cases
according to the standard requirements when compiling. It takes a lot of time
to retrieve information and organize language, and the degree of automation and
intelligence is low. This paper proposes an intelligent generation method of
structural demolition suggestions based on multi-model collaboration, and
improves the text generation performance of large language models in the field
of structural demolition by Retrieval-Augmented Generation and Low-Rank
Adaptation Fine-Tuning technology. The intelligent generation framework of
multi-model collaborative structural demolition suggestions can start from the
specific engineering situation, drive the large language model to answer with
anthropomorphic thinking, and propose demolition suggestions that are highly
consistent with the characteristics of the structure. Compared with CivilGPT,
the multi-model collaboration framework proposed in this paper can focus more
on the key information of the structure, and the suggestions are more targeted.

</details>


### [24] [An Auditable Pipeline for Fuzzy Full-Text Screening in Systematic Reviews: Integrating Contrastive Semantic Highlighting and LLM Judgment](https://arxiv.org/abs/2508.15822)
*Pouria Mortezaagha,Arya Rahgozar*

Main category: cs.CL

TL;DR: 本文提出一种基于模糊逻辑、对比高亮和大型语言模型（LLM）裁决的系统筛选流程，用于解决系统评价（SRs）中全文筛选的瓶颈，并在试点中展示了更高的召回率、筛选效率和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 系统评价的全文筛选是主要瓶颈，因为决定性证据分散在长而异构的文档中，且很少能通过静态、二元的规则进行判断。

Method: 文章被解析成重叠的块，并用领域适应模型进行嵌入。对于每个PICO-SA标准，计算对比相似性（纳入-排除余弦）和模糊度。Mamdani模糊控制器将这些映射为带有动态阈值的分级纳入程度。LLM裁判根据三级标签、置信度和参考理由裁决高亮文本，证据不足时模糊成员资格会减弱而非排除。该方法与统计和清晰基线进行了比较。

Result: 在试点中，模糊系统在各标准（P, I, O, SA）上实现了81.3%-87.5%的召回率，优于统计（56.3%-75.0%）和清晰基线（43.8%-81.3%）。严格“所有标准”纳入文章的比例达到50.0%，而基线分别为25.0%和12.5%。理由的跨模型一致性为98.3%，人机一致性为96.1%。筛选时间从约20分钟缩短到不到1分钟，且成本显著降低。

Conclusion: 结合模糊逻辑、对比高亮和LLM裁决的方法，可以实现高召回率、稳定的判断理由和端到端的可追溯性，从而有效解决了系统评价中的筛选瓶颈问题。

Abstract: Full-text screening is the major bottleneck of systematic reviews (SRs), as
decisive evidence is dispersed across long, heterogeneous documents and rarely
admits static, binary rules. We present a scalable, auditable pipeline that
reframes inclusion/exclusion as a fuzzy decision problem and benchmark it
against statistical and crisp baselines in the context of the Population Health
Modelling Consensus Reporting Network for noncommunicable diseases (POPCORN).
Articles are parsed into overlapping chunks and embedded with a domain-adapted
model; for each criterion (Population, Intervention, Outcome, Study Approach),
we compute contrastive similarity (inclusion-exclusion cosine) and a vagueness
margin, which a Mamdani fuzzy controller maps into graded inclusion degrees
with dynamic thresholds in a multi-label setting. A large language model (LLM)
judge adjudicates highlighted spans with tertiary labels, confidence scores,
and criterion-referenced rationales; when evidence is insufficient, fuzzy
membership is attenuated rather than excluded. In a pilot on an all-positive
gold set (16 full texts; 3,208 chunks), the fuzzy system achieved recall of
81.3% (Population), 87.5% (Intervention), 87.5% (Outcome), and 75.0% (Study
Approach), surpassing statistical (56.3-75.0%) and crisp baselines
(43.8-81.3%). Strict "all-criteria" inclusion was reached for 50.0% of
articles, compared to 25.0% and 12.5% under the baselines. Cross-model
agreement on justifications was 98.3%, human-machine agreement 96.1%, and a
pilot review showed 91% inter-rater agreement (kappa = 0.82), with screening
time reduced from about 20 minutes to under 1 minute per article at
significantly lower cost. These results show that fuzzy logic with contrastive
highlighting and LLM adjudication yields high recall, stable rationale, and
end-to-end traceability.

</details>


### [25] [SDEC: Semantic Deep Embedded Clustering](https://arxiv.org/abs/2508.15823)
*Mohammad Wali Ur Rahman,Ric Nevarez,Lamia Tasnim Mim,Salim Hariri*

Main category: cs.CL

TL;DR: 本文提出SDEC（语义深度嵌入聚类）框架，通过结合改进自编码器和Transformer嵌入解决文本大数据聚类挑战，并在多个基准数据集上取得优于现有方法的性能，显著提升了聚类准确性和语义理解。


<details>
  <summary>Details</summary>
Motivation: 文本大数据的高维度和复杂语义特性对传统文本聚类技术（如k-means）构成重大挑战，常导致次优聚类结果。

Method: SDEC是一种无监督文本聚类框架，结合了改进的自编码器和基于Transformer的嵌入。自编码器通过结合MSE和余弦相似度损失（CSL）来保留语义关系。此外，SDEC利用Transformer嵌入的上下文丰富性进行语义细化，以优化聚类层，实现软聚类分配和分布损失。

Result: 在AG News数据集上，SDEC的聚类准确率达到85.7%，在Yahoo! Answers数据集上创下53.63%的新基准，并超越了现有方法。该框架在五个基准数据集（AG News, Yahoo! Answers, DBPedia, Reuters 2, Reuters 5）上均表现出稳健性能。

Conclusion: SDEC在无监督文本聚类方面取得了显著进展，其创新性提升了文本数据的聚类准确性及语义理解能力。

Abstract: The high dimensional and semantically complex nature of textual Big data
presents significant challenges for text clustering, which frequently lead to
suboptimal groupings when using conventional techniques like k-means or
hierarchical clustering. This work presents Semantic Deep Embedded Clustering
(SDEC), an unsupervised text clustering framework that combines an improved
autoencoder with transformer-based embeddings to overcome these challenges.
This novel method preserves semantic relationships during data reconstruction
by combining Mean Squared Error (MSE) and Cosine Similarity Loss (CSL) within
an autoencoder. Furthermore, a semantic refinement stage that takes advantage
of the contextual richness of transformer embeddings is used by SDEC to further
improve a clustering layer with soft cluster assignments and distributional
loss. The capabilities of SDEC are demonstrated by extensive testing on five
benchmark datasets: AG News, Yahoo! Answers, DBPedia, Reuters 2, and Reuters 5.
The framework not only outperformed existing methods with a clustering accuracy
of 85.7% on AG News and set a new benchmark of 53.63% on Yahoo! Answers, but
also showed robust performance across other diverse text corpora. These
findings highlight the significant improvements in accuracy and semantic
comprehension of text data provided by SDEC's advances in unsupervised text
clustering.

</details>


### [26] [Avaliação de eficiência na leitura: uma abordagem baseada em PLN](https://arxiv.org/abs/2508.15824)
*Túlio Sousa de Gois,Raquel Meister Ko. Freitag*

Main category: cs.CL

TL;DR: 本研究提出了一种针对巴西葡萄牙语完形填空测试的自动化评估模型，该模型整合了正字法、语法和语义分析，并与人工评估高度相关（0.832），适用于需要可扩展性的教育场景。


<details>
  <summary>Details</summary>
Motivation: 传统的完形填空批改方法仅依赖于标准答案，限制了对学生表现细微差别的识别，未能充分评估其多样化的语言能力。

Method: 该研究提出了一种巴西葡萄牙语完形填空测试的自动化评估模型。该模型整合了正字法分析（编辑距离）、语法分析（词性标注）和语义分析（嵌入相似度）。

Result: 整合的自动化方法表现出有效性，与人工评估达到了0.832的高相关性。结果表明该自动化方法稳健可靠，对语言能力的变化敏感。

Conclusion: 所提出的自动化完形填空评估模型是稳健的，对语言细微差别敏感，并适用于需要可扩展性的教育环境。

Abstract: The cloze test, widely used due to its low cost and flexibility, makes it
possible to assess reading comprehension by filling in gaps in texts, requiring
the mobilization of diverse linguistic repertoires. However, traditional
correction methods, based only on exact answers, limit the identification of
nuances in student performance. This study proposes an automated evaluation
model for the cloze test in Brazilian Portuguese, integrating orthographic
(edit distance), grammatical (POS tagging) and semantic (similarity between
embeddings) analyses. The integrated method demonstrated its effectiveness,
achieving a high correlation with human evaluation (0.832). The results
indicate that the automated approach is robust, sensitive to variations in
linguistic repertoire and suitable for educational contexts that require
scalability.

</details>


### [27] [Enhancing Cryptocurrency Sentiment Analysis with Multimodal Features](https://arxiv.org/abs/2508.15825)
*Chenghao Liu,Aniket Mahanti,Ranesh Naha,Guanghao Wang,Erwann Sbai*

Main category: cs.CL

TL;DR: 研究通过多模态分析比较TikTok和Twitter情绪对加密货币市场的影响，发现TikTok影响短期投机，Twitter影响长期趋势，整合跨平台情绪可显著提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中于文本平台（如Twitter），但视频内容（如TikTok）可能包含更丰富的情绪和语境，却未被充分探索。本研究旨在弥补这一空白，深入理解不同社交媒体信号对加密货币市场的影响。

Method: 采用多模态分析，比较TikTok视频情绪和Twitter文本情绪。利用大型语言模型（LLMs）从视频和文本数据中提取洞察，并调查社交媒体情绪与加密货币市场指标之间的动态依赖性和溢出效应。

Result: 研究结果显示，TikTok的视频情绪显著影响投机性资产和短期市场趋势；而Twitter的文本情绪与长期市场动态关联更紧密。此外，整合跨平台情绪信号可将预测准确性提高高达20%。

Conclusion: 结合TikTok和Twitter的社交媒体情绪分析，尤其是视频内容，对于全面理解加密货币市场和提高市场预测准确性至关重要，不同平台和模态对市场趋势的影响具有独特性。

Abstract: As cryptocurrencies gain popularity, the digital asset marketplace becomes
increasingly significant. Understanding social media signals offers valuable
insights into investor sentiment and market dynamics. Prior research has
predominantly focused on text-based platforms such as Twitter. However, video
content remains underexplored, despite potentially containing richer emotional
and contextual sentiment that is not fully captured by text alone. In this
study, we present a multimodal analysis comparing TikTok and Twitter sentiment,
using large language models to extract insights from both video and text data.
We investigate the dynamic dependencies and spillover effects between social
media sentiment and cryptocurrency market indicators. Our results reveal that
TikTok's video-based sentiment significantly influences speculative assets and
short-term market trends, while Twitter's text-based sentiment aligns more
closely with long-term dynamics. Notably, the integration of cross-platform
sentiment signals improves forecasting accuracy by up to 20%.

</details>


### [28] [Embarrassed to observe: The effects of directive language in brand conversation](https://arxiv.org/abs/2508.15826)
*Andria Andriuzzi,Géraldine Michel*

Main category: cs.CL

TL;DR: 品牌在社交媒体对话中使用指令性语言会损害观察者的参与度，尤其在非产品中心对话中影响更强，但强的品牌关系可缓解此负面影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究对广告中指令性语言的效果存在分歧，但品牌在社交媒体对话中对其他消费者使用指令性语言，如何影响观察者的参与度仍不清楚。

Method: 通过一项实地研究和三项在线实验。

Result: 品牌在社交媒体对话中使用指令性语言，会降低观察者的参与度，观察者可能因此感到替代性尴尬。在非产品中心对话中，这种负面影响更显著。然而，强大的品牌关系可以缓解这一负面影响。

Conclusion: 本研究揭示了互动交流中情境的重要性，为指令性语言和品牌-消费者互动文献做出了贡献，并为社交媒体和品牌管理提供了实践指导。

Abstract: In social media, marketers attempt to influence consumers by using directive
language, that is, expressions designed to get consumers to take action. While
the literature has shown that directive messages in advertising have mixed
results for recipients, we know little about the effects of directive brand
language on consumers who see brands interacting with other consumers in social
media conversations. On the basis of a field study and three online
experiments, this study shows that directive language in brand conversation has
a detrimental downstream effect on engagement of consumers who observe such
exchanges. Specifically, in line with Goffman's facework theory, because a
brand that encourages consumers to react could be perceived as
face-threatening, consumers who see a brand interacting with others in a
directive way may feel vicarious embarrassment and engage less (compared with a
conversation without directive language). In addition, we find that when the
conversation is nonproduct-centered (vs. product-centered), consumers expect
more freedom, as in mundane conversations, even for others; therefore,
directive language has a stronger negative effect. However, in this context,
the strength of the brand relationship mitigates this effect. Thus, this study
contributes to the literature on directive language and brand-consumer
interactions by highlighting the importance of context in interactive
communication, with direct relevance for social media and brand management.

</details>


### [29] [Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models](https://arxiv.org/abs/2508.15827)
*Zhifei Xie,Ziyang Ma,Zihang Liu,Kaiyu Pang,Hongyu Li,Jialin Zhang,Yue Liao,Deheng Ye,Chunyan Miao,Shuicheng Yan*

Main category: cs.CL

TL;DR: 本文提出Mini-Omni-Reasoner框架，通过新颖的“边想边说”(Thinking-in-Speaking)范式，在语音模型中实现推理与言语输出的交错进行，解决传统“先想后说”范式导致的延迟问题，显著提升语音推理能力和实时交互效率。


<details>
  <summary>Details</summary>
Motivation: 推理对有效沟通和决策至关重要，但在语音语言模型(LSMs)中仍处于早期阶段。现有方法照搬文本模型的“先想后说”范式，导致语音响应因等待推理完成而产生显著延迟，损害实时交互和沟通效率。

Method: 本文提出Mini-Omni-Reasoner框架，采用“边想边说”的新范式，在词元级别交错无声推理词元与语音响应词元。该设计允许连续语音生成，同时嵌入结构化内部推理，并利用模型的高频词元处理能力。为确保每个响应词元都基于其前置推理，强制执行局部语义对齐。为支持此框架，引入了Spoken-Math-Problems-3M大型数据集，用于交错推理和响应的学习。模型构建于分层的Thinker-Talker架构之上。

Result: 在Spoken-MQA基准测试中，Mini-Omni-Reasoner在算术推理方面取得了+19.1%的提升，在语境理解方面取得了+6.4%的提升。同时，它生成了更短的输出，并实现了零解码延迟，提供了流畅且逻辑严谨的语音响应，兼顾了自然性和精确性。

Conclusion: Mini-Omni-Reasoner通过创新的“边想边说”范式，成功地在语音生成过程中嵌入了推理能力，克服了传统方法的延迟问题，显著提升了语音模型的推理性能、实时交互性及响应的逻辑准确性和自然度。

Abstract: Reasoning is essential for effective communication and decision-making. While
recent advances in LLMs and MLLMs have shown that incorporating explicit
reasoning significantly improves understanding and generalization, reasoning in
LSMs remains in a nascent stage. Early efforts attempt to transfer the
"Thinking-before-Speaking" paradigm from textual models to speech. However,
this sequential formulation introduces notable latency, as spoken responses are
delayed until reasoning is fully completed, impairing real-time interaction and
communication efficiency. To address this, we propose Mini-Omni-Reasoner, a
framework that enables reasoning within speech via a novel
"Thinking-in-Speaking" formulation. Rather than completing reasoning before
producing any verbal output, Mini-Omni-Reasoner interleaves silent reasoning
tokens with spoken response tokens at the token level. This design allows
continuous speech generation while embedding structured internal reasoning,
leveraging the model's high-frequency token processing capability. Although
interleaved, local semantic alignment is enforced to ensure that each response
token is informed by its preceding reasoning. To support this framework, we
introduce Spoken-Math-Problems-3M, a large-scale dataset tailored for
interleaved reasoning and response. The dataset ensures that verbal tokens
consistently follow relevant reasoning content, enabling accurate and efficient
learning of speech-coupled reasoning. Built on a hierarchical Thinker-Talker
architecture, Mini-Omni-Reasoner delivers fluent yet logically grounded spoken
responses, maintaining both naturalness and precision. On the Spoken-MQA
benchmark, it achieves a +19.1% gain in arithmetic reasoning and +6.4% in
contextual understanding, with shorter outputs and zero decoding latency.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [30] [Text-Driven 3D Hand Motion Generation from Sign Language Data](https://arxiv.org/abs/2508.15902)
*Léore Bensabath,Mathis Petrovich,Gül Varol*

Main category: cs.CV

TL;DR: 本文旨在训练一个基于自然语言描述生成3D手部动作的扩散模型HandMDM。为此，作者利用大规模手语视频数据、伪标注类别和LLM构建了前所未有的手部动作与文本描述对数据集，训练出的HandMDM模型在不同手语和非手语动作等多种领域均表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究目标是训练一个能根据自然语言描述（包含手型、位置、手指/手/手臂动作等特征）生成3D手部动作的生成模型。

Method: 1. 自动化构建大规模3D手部动作及其文本标签对。2. 利用大规模手语视频数据集和带噪声的伪标注手语类别。3. 通过一个LLM，结合手语属性字典和互补的动作-脚本线索，将这些信息翻译成手部动作描述。4. 基于这些数据训练一个文本条件手部动作扩散模型HandMDM。5. 对模型在多种场景下的性能进行了广泛的实验调查。

Result: 训练出的文本条件手部动作扩散模型HandMDM在多个领域表现出鲁棒性，包括同种手语中未曾见过的手语类别、另一种手语的动作以及非手语的手部动作。

Conclusion: 研究成功构建了大规模的3D手部动作及其文本描述数据集，并训练出了一个在多领域表现鲁棒的文本条件手部动作扩散模型HandMDM。作者承诺将公开训练好的模型和数据，以支持该新兴领域的未来研究。

Abstract: Our goal is to train a generative model of 3D hand motions, conditioned on
natural language descriptions specifying motion characteristics such as
handshapes, locations, finger/hand/arm movements. To this end, we automatically
build pairs of 3D hand motions and their associated textual labels with
unprecedented scale. Specifically, we leverage a large-scale sign language
video dataset, along with noisy pseudo-annotated sign categories, which we
translate into hand motion descriptions via an LLM that utilizes a dictionary
of sign attributes, as well as our complementary motion-script cues. This data
enables training a text-conditioned hand motion diffusion model HandMDM, that
is robust across domains such as unseen sign categories from the same sign
language, but also signs from another sign language and non-sign hand
movements. We contribute extensive experimental investigation of these
scenarios and will make our trained models and data publicly available to
support future research in this relatively new field.

</details>


### [31] [VT-LVLM-AR: A Video-Temporal Large Vision-Language Model Adapter for Fine-Grained Action Recognition in Long-Term Videos](https://arxiv.org/abs/2508.15903)
*Kaining Li,Shuwei He,Zihan Xu*

Main category: cs.CV

TL;DR: 本文提出VT-LVLM-AR框架，通过将长时视频转化为视觉事件序列，并利用参数高效微调的LVLM进行动作识别，在多个基准数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 长时视频中的人体动作识别面临计算开销大、难以捕捉长时时序依赖和语义理解不足的挑战。尽管大型语言模型（LLMs）和大型视觉-语言模型（LVLMs）在多模态理解上表现出色，但其直接应用于连续视频流的细粒度动作识别仍是未解决的问题。

Method: 引入VT-LVLM-AR框架。该框架包含两部分：1. 视频到事件映射器（VTEM），通过轻量时空特征提取、自适应时序池化和概念量化，将原始视频高效转换为紧凑、语义丰富且时序连贯的“视觉事件序列”。2. LVLM-based动作推理模块，将视觉事件序列输入到一个经过参数高效Prompt Tuning (P-Tuning v2) 适配的冻结LLaVA-1.5模型中进行动作分类。

Result: VT-LVLM-AR在NTU RGB+D和NTU RGB+D 120数据集上持续超越现有方法，取得了最先进的性能（例如，在NTU RGB+D X-Sub上达到94.1%的准确率）。消融研究证实了VTEM组件和Prompt Tuning的关键贡献，人工评估也强调了视觉事件表示的可解释性。

Conclusion: 这项工作通过有效的视频到语言转换和高效的模型适配，展示了利用LVLM在实现鲁棒且可解释的视频动作理解方面的巨大潜力。

Abstract: Human action recognition in long-term videos, characterized by complex
backgrounds and subtle action differences, poses significant challenges for
traditional deep learning models due to computational overhead, difficulty in
capturing long-range temporal dependencies, and limited semantic understanding.
While Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)
have shown remarkable capabilities in multi-modal understanding and reasoning,
their direct application to continuous video streams for fine-grained action
recognition remains an open problem. This paper introduces VT-LVLM-AR
(Video-Temporal Large Vision-Language Model Adapter for Action Recognition), a
novel framework designed to bridge this gap. VT-LVLM-AR comprises a
Video-to-Event Mapper (VTEM) that efficiently transforms raw video into
compact, semantically rich, and temporally coherent "visual event sequences"
through lightweight spatio-temporal feature extraction, adaptive temporal
pooling, and conceptual quantization with an event coherence bias. These visual
event sequences are then fed into an LVLM-based Action Reasoning module,
specifically a frozen LLaVA-1.5 model, adapted using parameter-efficient Prompt
Tuning (P-Tuning v2) for action classification. Comprehensive evaluations on
the NTU RGB+D and NTU RGB+D 120 datasets demonstrate that VT-LVLM-AR
consistently achieves state-of-the-art performance, surpassing existing methods
(e.g., 94.1% accuracy on NTU RGB+D X-Sub). Ablation studies confirm the
critical contributions of VTEM's components and the efficacy of Prompt Tuning,
while human evaluations underscore the interpretability of our visual event
representations. This work highlights the immense potential of leveraging LVLMs
for robust and interpretable video action understanding through effective
video-to-language translation and efficient model adaptation.

</details>


### [32] [Boosting Pathology Foundation Models via Few-shot Prompt-tuning for Rare Cancer Subtyping](https://arxiv.org/abs/2508.15904)
*Dexuan He,Xiao Zhou,Wenbin Guan,Liyuan Zhang,Xiaoman Zhang,Sinuo Xu,Ge Wang,Lifeng Wang,Xiaojun Yuan,Xin Sun,Yanfeng Wang,Kun Sun,Ya Zhang,Weidi Xie*

Main category: cs.CV

TL;DR: 本文提出了PathPT框架，该框架通过空间感知视觉聚合和任务特定提示调优，充分利用视觉-语言病理学基础模型诊断罕见癌症，在亚型分类准确性和癌变区域定位能力上均表现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 罕见癌症诊断面临专家资源有限的重大挑战，尤其在儿科肿瘤中占比更高。现有基于视觉特征的多实例学习（MIL）方法忽视了跨模态知识和可解释性，而视觉-语言（VL）基础模型在常见癌症上的零样本能力在罕见癌症诊断中仍受限。

Method: 本文提出PathPT框架，通过空间感知视觉聚合和任务特定提示调优，充分利用视觉-语言病理学基础模型的潜力。PathPT将全切片图像（WSI）级别的监督转换为细粒度的瓦片级别指导，利用VL模型的零样本能力，保留了对癌变区域的定位，并通过与组织病理学语义对齐的提示实现跨模态推理。

Result: PathPT在八个罕见癌症数据集（涵盖56个亚型和2,910个WSI，包括成人和儿科）以及三个常见癌症数据集上进行了基准测试。结果显示，PathPT持续提供卓越的性能，在亚型分类准确性和癌变区域定位能力方面均实现了显著提升，优于四种最先进的VL模型和四种MIL框架。

Conclusion: 这项工作推动了AI辅助罕见癌症诊断的发展，为在专家资源有限的环境中提高亚型分类准确性提供了一个可扩展的解决方案。

Abstract: Rare cancers comprise 20-25% of all malignancies but face major diagnostic
challenges due to limited expert availability-especially in pediatric oncology,
where they represent over 70% of cases. While pathology vision-language (VL)
foundation models show promising zero-shot capabilities for common cancer
subtyping, their clinical performance for rare cancers remains limited.
Existing multi-instance learning (MIL) methods rely only on visual features,
overlooking cross-modal knowledge and compromising interpretability critical
for rare cancer diagnosis. To address this limitation, we propose PathPT, a
novel framework that fully exploits the potential of vision-language pathology
foundation models through spatially-aware visual aggregation and task-specific
prompt tuning. Unlike conventional MIL, PathPT converts WSI-level supervision
into fine-grained tile-level guidance by leveraging the zero-shot capabilities
of VL models, thereby preserving localization on cancerous regions and enabling
cross-modal reasoning through prompts aligned with histopathological semantics.
We benchmark PathPT on eight rare cancer datasets(four adult and four
pediatric) spanning 56 subtypes and 2,910 WSIs, as well as three common cancer
datasets, evaluating four state-of-the-art VL models and four MIL frameworks
under three few-shot settings. Results show that PathPT consistently delivers
superior performance, achieving substantial gains in subtyping accuracy and
cancerous region grounding ability. This work advances AI-assisted diagnosis
for rare cancers, offering a scalable solution for improving subtyping accuracy
in settings with limited access to specialized expertise.

</details>


### [33] [Semantic-Aware Ship Detection with Vision-Language Integration](https://arxiv.org/abs/2508.15930)
*Jiahao Li,Jiancheng Pan,Yuze Sun,Xiaomeng Huang*

Main category: cs.CV

TL;DR: 本文提出一个结合视觉-语言模型（VLMs）和多尺度自适应滑动窗口策略的新型框架，旨在解决遥感图像中船舶检测难以捕获细粒度语义信息的问题，并引入了专用的ShipSem-VL数据集以支持语义感知船舶检测（SASD）。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中的船舶检测对海事活动监测等应用至关重要，但现有方法难以捕获细粒度语义信息，限制了其在复杂场景下的有效性。

Method: 提出一个结合视觉-语言模型（VLMs）和多尺度自适应滑动窗口策略的新型检测框架，并引入了专门用于捕获细粒度船舶属性的视觉-语言数据集ShipSem-VL，以促进语义感知船舶检测（SASD）。

Result: 通过三个明确定义的任务对框架进行了评估，全面分析了其性能，并从多个角度证明了其在推进语义感知船舶检测（SASD）方面的有效性。

Conclusion: 所提出的结合VLM和多尺度自适应滑动窗口的框架，配合ShipSem-VL数据集，有效解决了遥感图像中船舶检测缺乏细粒度语义信息的问题，并显著提升了语义感知船舶检测的能力。

Abstract: Ship detection in remote sensing imagery is a critical task with wide-ranging
applications, such as maritime activity monitoring, shipping logistics, and
environmental studies. However, existing methods often struggle to capture
fine-grained semantic information, limiting their effectiveness in complex
scenarios. To address these challenges, we propose a novel detection framework
that combines Vision-Language Models (VLMs) with a multi-scale adaptive sliding
window strategy. To facilitate Semantic-Aware Ship Detection (SASD), we
introduce ShipSem-VL, a specialized Vision-Language dataset designed to capture
fine-grained ship attributes. We evaluate our framework through three
well-defined tasks, providing a comprehensive analysis of its performance and
demonstrating its effectiveness in advancing SASD from multiple perspectives.

</details>


### [34] [Automatic Retrieval of Specific Cows from Unlabeled Videos](https://arxiv.org/abs/2508.15945)
*Jiawen Lyu,Manu Ramesh,Madison Simonds,Jacquelyn P. Boerman,Amy R. Reibman*

Main category: cs.CV

TL;DR: 提出一种无需深度学习的自动化视频系统，用于奶牛的免手动目录创建和识别。


<details>
  <summary>Details</summary>
Motivation: 公开文献中缺乏能实现奶牛免手动目录创建和识别的自动化视频系统。

Method: 开发了一个包含AutoCattloger（从单视频片段建立奶牛目录）、无需深度学习的图像识别器（用于识别奶牛）和CowFinder（在连续视频流中识别奶牛）的系统。

Result: 成功地在奶牛自由走动、未标注、未分段的挤奶区视频中识别出个体奶牛。

Conclusion: 该系统在非受限视频环境中自动识别个体奶牛方面具有实用价值。

Abstract: Few automated video systems are described in the open literature that enable
hands-free cataloging and identification (ID) of cows in a dairy herd. In this
work, we describe our system, composed of an AutoCattloger, which builds a
Cattlog of dairy cows in a herd with a single input video clip per cow, an
eidetic cow recognizer which uses no deep learning to ID cows, and a CowFinder,
which IDs cows in a continuous stream of video. We demonstrate its value in
finding individuals in unlabeled, unsegmented videos of cows walking
unconstrained through the holding area of a milking parlor.

</details>


### [35] [Investigating Different Geo Priors for Image Classification](https://arxiv.org/abs/2508.15946)
*Angela Zhu,Christian Lange,Max Hamilton*

Main category: cs.CV

TL;DR: 本研究评估了空间隐式神经网络（SINR）模型作为视觉物种分类的地理先验，并揭示了影响其有效性的关键因素。


<details>
  <summary>Details</summary>
Motivation: 当位置信息可用时，物种分布模型（SDMs）能够编码物种出现的空间模式，从而有效提升基于视觉的物种分类性能。本研究旨在评估SINR模型作为视觉物种分类的地理先验的有效性。

Method: 本研究评估了多种SINR模型作为来自iNaturalist观测数据的物种视觉分类的地理先验。我们探讨了不同模型配置的影响，并调整了对未包含在地理先验训练中的物种的预测处理方式。

Result: 我们的分析揭示了有助于这些SINR模型作为地理先验有效性的因素，这些因素可能与生成准确物种范围图的因素不同。

Conclusion: SINR模型可以有效地作为视觉物种分类的地理先验，但其有效性的驱动因素可能与传统上用于制作高精度物种分布图的因素有所区别。

Abstract: Species distribution models encode spatial patterns of species occurrence
making them effective priors for vision-based species classification when
location information is available. In this study, we evaluate various SINR
(Spatial Implicit Neural Representations) models as a geographical prior for
visual classification of species from iNaturalist observations. We explore the
impact of different model configurations and adjust how we handle predictions
for species not included in Geo Prior training. Our analysis reveals factors
that contribute to the effectiveness of these models as Geo Priors, factors
that may differ from making accurate range maps.

</details>


### [36] [Representation Learning with Adaptive Superpixel Coding](https://arxiv.org/abs/2508.15959)
*Mahmoud Khalil,Ahmad Khalil,Alioune Ngom*

Main category: cs.CV

TL;DR: 提出了一种名为ASC的自监督Transformer模型，它通过自适应超像素层克服了传统Vision Transformers固定大小、非自适应块划分的局限性，并在标准图像任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习视觉模型通常为特定模态定制，并依赖于领域特定假设（如网格结构）。传统Vision Transformers尤其受限于固定大小且非自适应的块划分。

Method: 我们提出了一种基于Transformer的自监督模型——自适应超像素编码（ASC）。该模型采用自适应超像素层，能够动态调整以适应图像内容，从而克服了传统Vision Transformers的局限。

Result: 通过分析该方法的关键特性，我们发现ASC在标准图像下游任务基准上优于广泛使用的替代方案。

Conclusion: ASC通过使用自适应超像素层，成功克服了传统Vision Transformers固定块划分的限制，并在图像任务中展现出卓越的性能。

Abstract: Deep learning vision models are typically tailored for specific modalities
and often rely on domain-specific assumptions, such as the grid structures used
by nearly all existing vision models. In this work, we propose a
self-supervised model based on Transformers, which we call Adaptive Superpixel
Coding (ASC). The key insight of our model is to overcome the limitations of
traditional Vision Transformers, which depend on fixed-size and non-adaptive
patch partitioning. Instead, ASC employs adaptive superpixel layers that
dynamically adjust to the underlying image content. We analyze key properties
of the approach that make it effective, and find that our method outperforms
widely-used alternatives on standard image downstream task benchmarks.

</details>


### [37] [Glo-VLMs: Leveraging Vision-Language Models for Fine-Grained Diseased Glomerulus Classification](https://arxiv.org/abs/2508.15960)
*Zhenhao Guo,Rachit Saluja,Tianyuan Yao,Quan Liu,Yuankai Huo,Benjamin Liechty,David J. Pisapia,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: 本研究引入Glo-VLMs框架，探索了如何在数据受限的少量样本设置下，将大型视觉-语言模型（VLMs）有效应用于肾脏病理学中精细的肾小球亚型分类任务，并取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）在数字病理学中展现潜力，但在区分肾小球亚型等精细、疾病特异性分类任务上效果有限。这些亚型之间微小的形态学差异，以及将视觉模式与精确临床术语对齐的困难，使得肾脏病理学的自动化诊断极具挑战性。因此，需要探索如何在只有少量标记样本的情况下，有效适应大型预训练VLMs以进行精细的肾小球分类。

Method: 本研究提出了Glo-VLMs，一个系统性框架，旨在数据受限的条件下探索VLMs在精细肾小球分类中的适应性。该方法利用精心策划的病理图像结合临床文本提示，促进对细微肾脏病理亚型进行图像-文本联合表示学习。研究在少量样本学习范式下评估了各种VLM架构和适应策略，并使用标准化的多类别指标进行模型评估，以确保公平比较。

Result: 通过微调VLMs，在每类仅8个样本的条件下，实现了0.7416的准确率、0.9045的宏观AUC和0.5277的F1分数。这表明即使在监督极度有限的情况下，基础模型也能有效适应精细的医学图像分类任务。

Conclusion: 研究结果表明，即使在高度有限的监督（少量样本）下，基础模型（VLMs）也可以被有效适应于精细的医学图像分类任务，特别是肾小球亚型分类。这澄清了大型预训练模型在专业临床研究应用中的实际要求和潜力。

Abstract: Vision-language models (VLMs) have shown considerable potential in digital
pathology, yet their effectiveness remains limited for fine-grained,
disease-specific classification tasks such as distinguishing between glomerular
subtypes. The subtle morphological variations among these subtypes, combined
with the difficulty of aligning visual patterns with precise clinical
terminology, make automated diagnosis in renal pathology particularly
challenging. In this work, we explore how large pretrained VLMs can be
effectively adapted to perform fine-grained glomerular classification, even in
scenarios where only a small number of labeled examples are available. In this
work, we introduce Glo-VLMs, a systematic framework designed to explore the
adaptation of VLMs to fine-grained glomerular classification in
data-constrained settings. Our approach leverages curated pathology images
alongside clinical text prompts to facilitate joint image-text representation
learning for nuanced renal pathology subtypes. By assessing various VLMs
architectures and adaptation strategies under a few-shot learning paradigm, we
explore how both the choice of method and the amount of labeled data impact
model performance in clinically relevant scenarios. To ensure a fair
comparison, we evaluate all models using standardized multi-class metrics,
aiming to clarify the practical requirements and potential of large pretrained
models for specialized clinical research applications. As a result, fine-tuning
the VLMs achieved 0.7416 accuracy, 0.9045 macro-AUC, and 0.5277 F1-score with
only 8 shots per class, demonstrating that even with highly limited
supervision, foundation models can be effectively adapted for fine-grained
medical image classification.

</details>


### [38] [Contributions to Label-Efficient Learning in Computer Vision and Remote Sensing](https://arxiv.org/abs/2508.15973)
*Minh-Tan Pham*

Main category: cs.CV

TL;DR: 该手稿总结了作者在计算机视觉和遥感领域标签高效学习方面的贡献，旨在开发从有限或部分标注数据中有效学习并利用大量未标注数据的方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决真实世界应用中，尤其是在地球观测数据（如多模态、空间分辨率变异性和场景异质性）的挑战下，如何从有限或部分标注数据中高效学习并充分利用大量未标注数据。

Method: 该研究主要采用四种方法：1) 基于异常感知表示的弱监督学习进行目标发现和检测；2) 利用多任务学习处理具有不相交标注的数据集以改进目标检测和语义分割；3) 结合多模态数据的自监督和监督对比学习以增强遥感场景分类；4) 使用显式和隐式建模类别层级的少样本学习进行分层场景分类。

Result: 研究结果显示，这些贡献得到了在自然和遥感数据集上广泛实验结果的支持，反映了多个合作研究项目的成果，并在各项任务中取得了性能提升。

Conclusion: 该手稿总结了作者在标签高效学习领域的多方面贡献，并展望了未来在扩大和增强标签高效学习以应用于实际场景的研究方向。

Abstract: This manuscript presents a series of my selected contributions to the topic
of label-efficient learning in computer vision and remote sensing. The central
focus of this research is to develop and adapt methods that can learn
effectively from limited or partially annotated data, and can leverage abundant
unlabeled data in real-world applications. The contributions span both
methodological developments and domain-specific adaptations, in particular
addressing challenges unique to Earth observation data such as multi-modality,
spatial resolution variability, and scene heterogeneity. The manuscript is
organized around four main axes including (1) weakly supervised learning for
object discovery and detection based on anomaly-aware representations learned
from large amounts of background images; (2) multi-task learning that jointly
trains on multiple datasets with disjoint annotations to improve performance on
object detection and semantic segmentation; (3) self-supervised and supervised
contrastive learning with multimodal data to enhance scene classification in
remote sensing; and (4) few-shot learning for hierarchical scene classification
using both explicit and implicit modeling of class hierarchies. These
contributions are supported by extensive experimental results across natural
and remote sensing datasets, reflecting the outcomes of several collaborative
research projects. The manuscript concludes by outlining ongoing and future
research directions focused on scaling and enhancing label-efficient learning
for real-world applications.

</details>


### [39] [Panoptic Segmentation of Environmental UAV Images : Litter Beach](https://arxiv.org/abs/2508.15985)
*Ousmane Youme,Jean Marie Dembélé,Eugene C. Ezin,Christophe Cambier*

Main category: cs.CV

TL;DR: 本文利用无人机图像，采用实例分割和全景分割方法监测海洋垃圾，以解决在异质沙滩环境中传统CNN模型的挑战。


<details>
  <summary>Details</summary>
Motivation: 海洋垃圾是一个全球性问题，可以通过结合CNN和无人机图像进行监测。然而，由于沙滩环境的异质性（如沙色反射、人类足迹、阴影、海藻等），基础CNN模型会遇到很多干扰，因此需要更合适的CNN分割方法。

Method: 本文采用实例分割方法和全景分割方法。使用无人机获取图像，因其具有更高的分辨率和局部区域适应性。

Result: 所选用的分割模型在少量样本下显示出良好的准确性，并且更具鲁棒性。

Conclusion: 鉴于现有信息，研究表明实例分割和全景分割方法在处理复杂沙滩环境下的海洋垃圾监测问题上表现出良好的准确性和鲁棒性，尤其是在少量样本的情况下。

Abstract: Convolutional neural networks (CNN) have been used efficiently in several
fields, including environmental challenges. In fact, CNN can help with the
monitoring of marine litter, which has become a worldwide problem. UAVs have
higher resolution and are more adaptable in local areas than satellite images,
making it easier to find and count trash. Since the sand is heterogeneous, a
basic CNN model encounters plenty of inferences caused by reflections of sand
color, human footsteps, shadows, algae present, dunes, holes, and tire tracks.
For these types of images, other CNN models, such as CNN-based segmentation
methods, may be more appropriate. In this paper, we use an instance-based
segmentation method and a panoptic segmentation method that show good accuracy
with just a few samples. The model is more robust and less

</details>


### [40] [Automated Multi-label Classification of Eleven Retinal Diseases: A Benchmark of Modern Architectures and a Meta-Ensemble on a Large Synthetic Dataset](https://arxiv.org/abs/2508.15986)
*Jerry Cao-Xue,Tien Comlekoglu,Keyi Xue,Guanliang Wang,Jiang Li,Gordon Laurie*

Main category: cs.CV

TL;DR: 本研究利用大规模合成数据集SynFundus-1M训练了多种深度学习模型，成功实现了视网膜疾病分类，并在真实临床数据集上展现出强大的泛化能力，为眼科AI系统开发提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 由于患者隐私和高成本，用于视网膜疾病分类的多标签深度学习模型在开发过程中常受限于大型、专家标注临床数据集的稀缺。近期发布的SynFundus-1M（包含超过一百万张高保真合成眼底图像）为克服这些障碍提供了新的机遇。本研究旨在为这一新资源建立基础性能基准。

Method: 研究开发了一个端到端深度学习流水线，使用5折多标签分层交叉验证策略，训练了六种现代架构（ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, RETFound）来分类十一种视网膜疾病。此外，通过将折外预测与XGBoost分类器堆叠，开发了一个元集成模型。

Result: 最终的集成模型在内部验证集上达到了最高性能，宏平均受试者工作特征曲线下面积（AUC）为0.9973。更重要的是，模型在三个不同的真实世界临床数据集上显示出强大的泛化能力：在组合DR数据集上AUC为0.7972，在AIROGS青光眼数据集上AUC为0.9126，在多标签RFMiD数据集上宏AUC为0.8800。

Conclusion: 本工作为未来基于大规模合成数据集的研究提供了可靠的基线，并证实了仅使用合成数据训练的模型能够准确分类多种病理，并有效泛化到真实临床图像，为加速眼科学中综合AI系统的发展提供了可行途径。

Abstract: The development of multi-label deep learning models for retinal disease
classification is often hindered by the scarcity of large, expertly annotated
clinical datasets due to patient privacy concerns and high costs. The recent
release of SynFundus-1M, a high-fidelity synthetic dataset with over one
million fundus images, presents a novel opportunity to overcome these barriers.
To establish a foundational performance benchmark for this new resource, we
developed an end-to-end deep learning pipeline, training six modern
architectures (ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, and the
RETFound foundation model) to classify eleven retinal diseases using a 5-fold
multi-label stratified cross-validation strategy. We further developed a
meta-ensemble model by stacking the out-of-fold predictions with an XGBoost
classifier. Our final ensemble model achieved the highest performance on the
internal validation set, with a macro-average Area Under the Receiver Operating
Characteristic Curve (AUC) of 0.9973. Critically, the models demonstrated
strong generalization to three diverse, real-world clinical datasets, achieving
an AUC of 0.7972 on a combined DR dataset, an AUC of 0.9126 on the AIROGS
glaucoma dataset and a macro-AUC of 0.8800 on the multi-label RFMiD dataset.
This work provides a robust baseline for future research on large-scale
synthetic datasets and establishes that models trained exclusively on synthetic
data can accurately classify multiple pathologies and generalize effectively to
real clinical images, offering a viable pathway to accelerate the development
of comprehensive AI systems in ophthalmology.

</details>


### [41] [Diverse Signer Avatars with Manual and Non-Manual Feature Modelling for Sign Language Production](https://arxiv.org/abs/2508.15988)
*Mohamed Ilyes Lakhal,Richard Bowden*

Main category: cs.CV

TL;DR: 本文提出一种基于潜在扩散模型（LDM）的新颖方法，通过专门的特征聚合模块生成多样化、逼真的手语数字替身，解决了现有手语生成模型在多样性、视觉质量和非手动属性建模方面的不足，并在实验中展现出卓越的视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有手语生成（SLP）模型难以在保持视觉质量的同时捕捉手语表示的多样性（包括外观、面部表情和手部动作），并且在建模非手动属性（如情感）方面也存在不足。

Method: 提出一种新颖方法，利用潜在扩散模型（LDM）从生成的参考图像合成逼真的数字替身。核心是一个新颖的手语特征聚合模块，它明确建模非手动特征（如面部）和手动特征（如手部），确保语言内容得以保留，并能无缝使用不同种族背景的参考图像以保证多样性。

Result: 在YouTube-SL-25手语数据集上的实验表明，所提出的方法与现有最先进方法相比，实现了卓越的视觉质量，并在感知指标上取得了显著改进。

Conclusion: 通过结合潜在扩散模型和专门的特征聚合模块，该方法成功地解决了手语生成中多样性、视觉质量和非手动特征建模的挑战，为生成逼真且语言准确的手语替身提供了有效途径。

Abstract: The diversity of sign representation is essential for Sign Language
Production (SLP) as it captures variations in appearance, facial expressions,
and hand movements. However, existing SLP models are often unable to capture
diversity while preserving visual quality and modelling non-manual attributes
such as emotions. To address this problem, we propose a novel approach that
leverages Latent Diffusion Model (LDM) to synthesise photorealistic digital
avatars from a generated reference image. We propose a novel sign feature
aggregation module that explicitly models the non-manual features
(\textit{e.g.}, the face) and the manual features (\textit{e.g.}, the hands).
We show that our proposed module ensures the preservation of linguistic content
while seamlessly using reference images with different ethnic backgrounds to
ensure diversity. Experiments on the YouTube-SL-25 sign language dataset show
that our pipeline achieves superior visual quality compared to state-of-the-art
methods, with significant improvements on perceptual metrics.

</details>


### [42] [CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars](https://arxiv.org/abs/2508.16030)
*Jinyue Song,Hansol Ku,Jayneel Vora,Nelson Lee,Ahmad Kamari,Prasant Mohapatra,Parth Pathak*

Main category: cs.CV

TL;DR: 发布了首个多车辆FMCW雷达合作感知数据集CoVeRaP和一套统一的合作感知框架，实验证明多车辆雷达中融合显著提升了三维目标检测的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管车载FMCW雷达在雨雪和眩光等恶劣天气下仍能保持可靠性，但其稀疏且嘈杂的点云限制了三维目标检测的性能。

Method: 1. 发布了CoVeRaP数据集，包含21k帧，对多辆车在不同机动下的雷达、摄像头和GPS数据流进行了时间对齐。2. 基于此数据，提出了一个统一的合作感知框架，提供中融合和晚融合选项。3. 其基线网络采用多分支PointNet风格的编码器，并增强了自注意力机制，用于将空间、多普勒和强度特征融合到共同的潜在空间，再由解码器转换为三维边界框和逐点深度置信度。

Result: 实验结果显示，结合强度编码的中融合在IoU 0.9时将平均精度提高了高达9倍，并持续优于单车基线检测方法。

Conclusion: CoVeRaP数据集建立了首个多车辆FMCW雷达感知领域的可复现基准，并证明了经济实惠的雷达共享（合作感知）能显著提高检测的鲁棒性。

Abstract: Automotive FMCW radars remain reliable in rain and glare, yet their sparse,
noisy point clouds constrain 3-D object detection. We therefore release
CoVeRaP, a 21 k-frame cooperative dataset that time-aligns radar, camera, and
GPS streams from multiple vehicles across diverse manoeuvres. Built on this
data, we propose a unified cooperative-perception framework with middle- and
late-fusion options. Its baseline network employs a multi-branch PointNet-style
encoder enhanced with self-attention to fuse spatial, Doppler, and intensity
cues into a common latent space, which a decoder converts into 3-D bounding
boxes and per-point depth confidence. Experiments show that middle fusion with
intensity encoding boosts mean Average Precision by up to 9x at IoU 0.9 and
consistently outperforms single-vehicle baselines. CoVeRaP thus establishes the
first reproducible benchmark for multi-vehicle FMCW-radar perception and
demonstrates that affordable radar sharing markedly improves detection
robustness. Dataset and code are publicly available to encourage further
research.

</details>


### [43] [DRespNeT: A UAV Dataset and YOLOv8-DRN Model for Aerial Instance Segmentation of Building Access Points for Post-Earthquake Search-and-Rescue Missions](https://arxiv.org/abs/2508.16016)
*Aykut Sirma,Angelos Plastropoulos,Argyrios Zolotas,Gilbert Tang*

Main category: cs.CV

TL;DR: 本文提出了DRespNeT，一个高分辨率的地震后环境航拍实例分割数据集，包含28个关键类别和细粒度标注。基于YOLOv8-DRN模型实现了92.7%的mAP50和27 FPS的实时性能，显著提升了灾害响应和救援效率。


<details>
  <summary>Details</summary>
Motivation: 地震后城市环境的快速评估对于有效的搜救行动至关重要，需要及时识别可进入点和结构障碍。现有数据集多依赖卫星图像或粗略语义标注，缺乏从高清航拍影像中获取的详细实例分割信息，无法区分可达和受阻区域，影响作战规划和响应效率。

Method: 开发了DRespNeT数据集，用于地震后结构环境的航拍实例分割。数据集包含来自灾区（如2023年土耳其地震）的高清（1080p）航拍影像，提供28个关键类别的详细多边形级实例分割标注，能够区分可进入和受阻区域。使用基于YOLO的实例分割模型（YOLOv8-seg）进行性能评估，并优化为YOLOv8-DRN模型。

Result: 优化后的YOLOv8-DRN模型在多目标检测中达到了92.7%的mAP50，并在RTX-4090 GPU上实现了27 FPS的推理速度，满足了实时操作要求。实验证明该方法显著提升了实时态势感知和决策能力。

Conclusion: DRespNeT数据集和相关模型能够支持搜救队伍和机器人系统，为增强人机协作、简化应急响应和改善幸存者结果奠定了基础。

Abstract: Recent advancements in computer vision and deep learning have enhanced
disaster-response capabilities, particularly in the rapid assessment of
earthquake-affected urban environments. Timely identification of accessible
entry points and structural obstacles is essential for effective
search-and-rescue (SAR) operations. To address this need, we introduce
DRespNeT, a high-resolution dataset specifically developed for aerial instance
segmentation of post-earthquake structural environments. Unlike existing
datasets, which rely heavily on satellite imagery or coarse semantic labeling,
DRespNeT provides detailed polygon-level instance segmentation annotations
derived from high-definition (1080p) aerial footage captured in disaster zones,
including the 2023 Turkiye earthquake and other impacted regions. The dataset
comprises 28 operationally critical classes, including structurally compromised
buildings, access points such as doors, windows, and gaps, multiple debris
levels, rescue personnel, vehicles, and civilian visibility. A distinctive
feature of DRespNeT is its fine-grained annotation detail, enabling
differentiation between accessible and obstructed areas, thereby improving
operational planning and response efficiency. Performance evaluations using
YOLO-based instance segmentation models, specifically YOLOv8-seg, demonstrate
significant gains in real-time situational awareness and decision-making. Our
optimized YOLOv8-DRN model achieves 92.7% mAP50 with an inference speed of 27
FPS on an RTX-4090 GPU for multi-target detection, meeting real-time
operational requirements. The dataset and models support SAR teams and robotic
systems, providing a foundation for enhancing human-robot collaboration,
streamlining emergency response, and improving survivor outcomes.

</details>


### [44] [NeuralMeshing: Complete Object Mesh Extraction from Casual Captures](https://arxiv.org/abs/2508.16026)
*Floris Erich,Naoya Chiba,Abdullah Mustafa,Ryo Hanai,Noriaki Ando,Yusuke Yoshiyasu,Yukiyasu Domae*

Main category: cs.CV

TL;DR: 提出一个自动化系统，仅利用多个视频生成日常物体的完整几何模型，无需昂贵的三维扫描仪。


<details>
  <summary>Details</summary>
Motivation: 如何在没有商业三维扫描仪的情况下，提取日常物体完整的几何模型。

Method: 使用两个或更多视频，每个视频至少指定一个已知点（可手动或通过标识符自动确定），利用运动结构（SfM）技术定位剩余帧，并通过合并多个视频结果生成完整的物体网格。

Result: 能够生成完整的物体网格，且无需依赖孔洞填充技术，避免了数据缺失。

Conclusion: 提供了一个自动化的系统，通过多个视频即可经济高效地生成完整的物体几何模型，代码已开源。

Abstract: How can we extract complete geometric models of objects that we encounter in
our daily life, without having access to commercial 3D scanners? In this paper
we present an automated system for generating geometric models of objects from
two or more videos. Our system requires the specification of one known point in
at least one frame of each video, which can be automatically determined using a
fiducial marker such as a checkerboard or Augmented Reality (AR) marker. The
remaining frames are automatically positioned in world space by using
Structure-from-Motion techniques. By using multiple videos and merging results,
a complete object mesh can be generated, without having to rely on hole
filling. Code for our system is available from
https://github.com/FlorisE/NeuralMeshing.

</details>


### [45] [Wavelet-Enhanced PaDiM for Industrial Anomaly Detection](https://arxiv.org/abs/2508.16034)
*Cory Gardner,Byungseok Min,Tae-Hyuk Ahn*

Main category: cs.CV

TL;DR: 提出WE-PaDiM，一种利用离散小波变换（DWT）结构化整合多层CNN特征的PaDiM改进版，有效解决了工业图像异常检测和定位中随机特征选择丢失结构信息的问题，在MVTec AD数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 工业图像的异常检测与定位对自动化质量检测至关重要。现有PaDiM方法在特征降维时采用随机通道选择，可能导致有用的结构化信息丢失。

Method: WE-PaDiM将2D离散小波变换（DWT）应用于多层CNN特征图，选择特定的频率子带（如LL, LH, HL），进行空间对齐和通道级联，然后通过PaDiM的多元高斯框架进行建模。这种“DWT-先于-级联”策略提供了一种基于频率内容的原则性特征选择方法。

Result: 在MVTec AD数据集上，WE-PaDiM使用ResNet-18和EfficientNet B0-B6等骨干网络，平均取得了99.32%的Image-AUC和92.10%的Pixel-AUC。分析表明，小波选择影响性能权衡：细节子带（HL或LH/HL/HH）有助于定位，而近似子带（LL）有助于图像级检测。

Conclusion: WE-PaDiM为PaDiM中的随机特征选择提供了一种有竞争力且可解释的替代方案，实现了适用于工业检测的鲁棒结果，并保持了相当的效率。

Abstract: Anomaly detection and localization in industrial images are essential for
automated quality inspection. PaDiM, a prominent method, models the
distribution of normal image features extracted by pre-trained Convolutional
Neural Networks (CNNs) but reduces dimensionality through random channel
selection, potentially discarding structured information. We propose
Wavelet-Enhanced PaDiM (WE-PaDiM), which integrates Discrete Wavelet Transform
(DWT) analysis with multi-layer CNN features in a structured manner. WE-PaDiM
applies 2D DWT to feature maps from multiple backbone layers, selects specific
frequency subbands (e.g., LL, LH, HL), spatially aligns them, and concatenates
them channel-wise before modeling with PaDiM's multivariate Gaussian framework.
This DWT-before-concatenation strategy provides a principled method for feature
selection based on frequency content relevant to anomalies, leveraging
multi-scale wavelet information as an alternative to random selection. We
evaluate WE-PaDiM on the challenging MVTec AD dataset with multiple backbones
(ResNet-18 and EfficientNet B0-B6). The method achieves strong performance in
anomaly detection and localization, yielding average results of 99.32%
Image-AUC and 92.10% Pixel-AUC across 15 categories with per-class optimized
configurations. Our analysis shows that wavelet choices affect performance
trade-offs: simpler wavelets (e.g., Haar) with detail subbands (HL or LH/HL/HH)
often enhance localization, while approximation bands (LL) improve image-level
detection. WE-PaDiM thus offers a competitive and interpretable alternative to
random feature selection in PaDiM, achieving robust results suitable for
industrial inspection with comparable efficiency.

</details>


### [46] [Expandable Residual Approximation for Knowledge Distillation](https://arxiv.org/abs/2508.16050)
*Zhaoyi Yan,Binghui Chen,Yunfan Liu,Qixiang Ye*

Main category: cs.CV

TL;DR: 本文提出一种名为ERA（Expandable Residual Approximation）的新型知识蒸馏方法，通过多步残差近似和教师权重集成策略，有效弥合师生模型间的学习能力差距，显著提升了图像分类和目标检测等计算机视觉任务的性能。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏中，大型教师模型和轻量级学生模型之间固有的学习能力差距，限制了知识的有效迁移，因此需要新的方法来解决这一挑战。

Method: 受Stone-Weierstrass定理渐进近似原理的启发，本文提出了ERA方法。ERA通过将残差知识的近似分解为多个步骤，采用分而治之的策略降低模仿教师表示的难度。具体实现上，ERA利用多分支残差网络（MBRNet）进行残差知识分解，并引入教师权重集成（TWI）策略，通过重用教师模型的头部权重来弥补学生模型的能力不足。

Result: 在ImageNet图像分类基准测试中，Top-1准确率提升了1.41%；在MS COCO目标检测基准测试中，AP提升了1.40；同时在其他计算机视觉任务中也取得了领先性能。

Conclusion: ERA通过创新的残差近似分解和教师权重集成策略，有效缩小了师生模型间的学习能力差距，显著增强了知识蒸馏的效果，并在多个计算机视觉任务中展现出卓越的性能。

Abstract: Knowledge distillation (KD) aims to transfer knowledge from a large-scale
teacher model to a lightweight one, significantly reducing computational and
storage requirements. However, the inherent learning capacity gap between the
teacher and student often hinders the sufficient transfer of knowledge,
motivating numerous studies to address this challenge. Inspired by the
progressive approximation principle in the Stone-Weierstrass theorem, we
propose Expandable Residual Approximation (ERA), a novel KD method that
decomposes the approximation of residual knowledge into multiple steps,
reducing the difficulty of mimicking the teacher's representation through a
divide-and-conquer approach. Specifically, ERA employs a Multi-Branched
Residual Network (MBRNet) to implement this residual knowledge decomposition.
Additionally, a Teacher Weight Integration (TWI) strategy is introduced to
mitigate the capacity disparity by reusing the teacher's head weights.
Extensive experiments show that ERA improves the Top-1 accuracy on the ImageNet
classification benchmark by 1.41% and the AP on the MS COCO object detection
benchmark by 1.40, as well as achieving leading performance across computer
vision tasks. Codes and models are available at
https://github.com/Zhaoyi-Yan/ERA.

</details>


### [47] [Advances and Trends in the 3D Reconstruction of the Shape and Motion of Animals](https://arxiv.org/abs/2508.16062)
*Ziqi Li,Abderraouf Amrani,Shri Rai,Hamid Laga*

Main category: cs.CV

TL;DR: 本文综述了基于深度学习的动物三维重建最新进展，从输入模态、表示方法、重建技术和训练机制等方面对现有方法进行分类讨论，并分析其性能、挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 动物三维几何、姿态和运动重建具有广泛应用，但传统三维扫描仪侵入性强、成本高且难以部署。近年来，基于深度学习的非侵入式三维重建技术从RGB图像/视频中取得了显著进展，因此有必要对该新兴领域进行系统性综述。

Method: 该论文通过调查最新发展，对现有深度学习动物三维重建方法进行分类和讨论。分类依据包括：输入模态、动物三维几何与运动表示方式、重建技术类型以及训练机制。同时，论文还分析了关键方法的性能。

Result: 论文对现有先进方法进行了分类与讨论，分析了主要方法的性能、优势与局限性，并指出了当前面临的挑战。

Conclusion: 论文总结了当前研究领域的挑战，并为未来的研究方向提供了指导。

Abstract: Reconstructing the 3D geometry, pose, and motion of animals is a
long-standing problem, which has a wide range of applications, from biology,
livestock management, and animal conservation and welfare to content creation
in digital entertainment and Virtual/Augmented Reality (VR/AR). Traditionally,
3D models of real animals are obtained using 3D scanners. These, however, are
intrusive, often prohibitively expensive, and difficult to deploy in the
natural environment of the animals. In recent years, we have seen a significant
surge in deep learning-based techniques that enable the 3D reconstruction, in a
non-intrusive manner, of the shape and motion of dynamic objects just from
their RGB image and/or video observations. Several papers have explored their
application and extension to various types of animals. This paper surveys the
latest developments in this emerging and growing field of research. It
categorizes and discusses the state-of-the-art methods based on their input
modalities, the way the 3D geometry and motion of animals are represented, the
type of reconstruction techniques they use, and the training mechanisms they
adopt. It also analyzes the performance of some key methods, discusses their
strengths and limitations, and identifies current challenges and directions for
future research.

</details>


### [48] [A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection](https://arxiv.org/abs/2508.16069)
*Qifeng Liu,Dawei Zhao,Yabo Dong,Linzhi Shang,Liang Xiao,Juan Wang,Kunkong Zhao,Dongming Lu,Qi Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为体素扩散模块（VDM）的新方法，通过稀疏3D卷积增强点云数据中的体素级表示和扩散能力。VDM能够提高基于Transformer和SSM的点云目标检测模型的准确性，并在多个基准数据集上达到了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer和状态空间模型（SSM）的点云检测方法，由于其序列化处理方式，要求输入输出维度严格一致，限制了卷积操作通常提供的空间扩散能力，从而影响了检测精度。研究动机是解决这一限制，提升体素级表示能力。

Method: 受CNN启发，提出体素扩散模块（VDM），由稀疏3D卷积、子流形稀疏卷积和残差连接组成。VDM主要实现两个功能：1) 通过稀疏3D卷积扩散前景体素特征以丰富空间上下文；2) 聚合细粒度空间信息以增强体素级特征表示。为确保计算效率，输出特征图被下采样至原始分辨率的四分之一。VDM可无缝集成到主流的Transformer或SSM模型中。

Result: 将VDM嵌入到基于Transformer和SSM的模型中，在Waymo、nuScenes、Argoverse 2和ONCE等多个基准数据集上进行了评估。实验结果表明，VDM一致性地提高了基线模型的检测精度。具体而言，VDM-SSMs在Waymo上达到74.7 mAPH (L2)，nuScenes上72.9 NDS，Argoverse 2上42.3 mAP，ONCE上67.6 mAP，在所有数据集上均创造了新的最先进性能。

Conclusion: VDM有效地增强了点云目标检测中体素级特征表示和空间扩散能力，显著提高了基于Transformer和SSM模型的检测精度。该方法具有良好的通用性，并在多个主流数据集上取得了领先的性能。

Abstract: Recent advances in point cloud object detection have increasingly adopted
Transformer-based and State Space Models (SSMs), demonstrating strong
performance. However, voxelbased representations in these models require strict
consistency in input and output dimensions due to their serialized processing,
which limits the spatial diffusion capability typically offered by
convolutional operations. This limitation significantly affects detection
accuracy. Inspired by CNN-based object detection architectures, we propose a
novel Voxel Diffusion Module (VDM) to enhance voxel-level representation and
diffusion in point cloud data. VDM is composed of sparse 3D convolutions,
submanifold sparse convolutions, and residual connections. To ensure
computational efficiency, the output feature maps are downsampled to one-fourth
of the original input resolution. VDM serves two primary functions: (1)
diffusing foreground voxel features through sparse 3D convolutions to enrich
spatial context, and (2) aggregating fine-grained spatial information to
strengthen voxelwise feature representation. The enhanced voxel features
produced by VDM can be seamlessly integrated into mainstream Transformer- or
SSM-based detection models for accurate object classification and localization,
highlighting the generalizability of our method. We evaluate VDM on several
benchmark datasets by embedding it into both Transformerbased and SSM-based
models. Experimental results show that our approach consistently improves
detection accuracy over baseline models. Specifically, VDM-SSMs achieve 74.7
mAPH (L2) on Waymo, 72.9 NDS on nuScenes, 42.3 mAP on Argoverse 2, and 67.6 mAP
on ONCE, setting new stateof-the-art performance across all datasets. Our code
will be made publicly available.

</details>


### [49] [Ensemble learning of foundation models for precision oncology](https://arxiv.org/abs/2508.16085)
*Xiangde Luo,Xiyue Wang,Feyisope Eweje,Xiaoming Zhang,Sen Yang,Ryan Quinton,Jinxi Xiang,Yuchen Li,Yuanfeng Ji,Zhe Li,Yijiang Chen,Colin Bergstrom,Ted Kim,Francesca Maria Olguin,Kelley Yuan,Matthew Abikenari,Andrew Heider,Sierra Willens,Sanjeeth Rajaram,Robert West,Joel Neal,Maximilian Diehn,Ruijiang Li*

Main category: cs.CV

TL;DR: 本文提出了ELF（Foundation模型集成学习），一个整合五个先进病理基础模型以生成统一玻片级表征的框架。该模型在53,699张WSI上训练，并在多种临床应用中超越了现有模型，展示出卓越的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型因训练数据集和策略差异，导致性能不一致且泛化能力有限。需要一个更通用和鲁棒的解决方案来整合现有模型的优势，以更好地支持临床决策和AI辅助精准肿瘤学。

Method: 研究引入了ELF（Ensemble Learning of Foundation models）框架，该框架集成了五种最先进的病理基础模型。ELF利用集成学习，从53,699张覆盖20个解剖部位的全玻片图像（WSI）中学习，以生成统一的玻片级表征。该框架旨在捕获互补信息，同时保持高数据效率，并采用玻片级架构，特别适用于数据有限的临床场景。

Result: ELF在广泛的临床应用中进行了评估，包括疾病分类、生物标志物检测以及对主要抗癌疗法（化疗、靶向治疗和免疫疗法）的反应预测。结果显示，ELF始终优于所有组成基础模型和现有玻片级模型，展现出卓越的准确性和鲁棒性。

Conclusion: 研究结果强调了集成学习在病理基础模型中的强大作用，并表明ELF是一个可扩展且可泛化的解决方案，有望推动AI辅助精准肿瘤学的发展。

Abstract: Histopathology is essential for disease diagnosis and treatment
decision-making. Recent advances in artificial intelligence (AI) have enabled
the development of pathology foundation models that learn rich visual
representations from large-scale whole-slide images (WSIs). However, existing
models are often trained on disparate datasets using varying strategies,
leading to inconsistent performance and limited generalizability. Here, we
introduce ELF (Ensemble Learning of Foundation models), a novel framework that
integrates five state-of-the-art pathology foundation models to generate
unified slide-level representations. Trained on 53,699 WSIs spanning 20
anatomical sites, ELF leverages ensemble learning to capture complementary
information from diverse models while maintaining high data efficiency. Unlike
traditional tile-level models, ELF's slide-level architecture is particularly
advantageous in clinical contexts where data are limited, such as therapeutic
response prediction. We evaluated ELF across a wide range of clinical
applications, including disease classification, biomarker detection, and
response prediction to major anticancer therapies, cytotoxic chemotherapy,
targeted therapy, and immunotherapy, across multiple cancer types. ELF
consistently outperformed all constituent foundation models and existing
slide-level models, demonstrating superior accuracy and robustness. Our results
highlight the power of ensemble learning for pathology foundation models and
suggest ELF as a scalable and generalizable solution for advancing AI-assisted
precision oncology.

</details>


### [50] [Two-flow Feedback Multi-scale Progressive Generative Adversarial Network](https://arxiv.org/abs/2508.16089)
*Sun Weikai,Song Shijie,Chi Wenjie*

Main category: cs.CV

TL;DR: 本文提出了一种名为MSPG-SEN的新型两流反馈多尺度渐进式生成对抗网络，通过引入自适应感知-行为反馈循环、全局连接两流动态残差网络和动态嵌入注意力机制，显著提升了GAN模型的图像生成质量、训练稳定性、效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在图像生成领域取得进展，但GAN模型因其独特优势仍有巨大发展空间。现有GAN模型在图像质量、人类视觉感知、训练过程简化和成本降低方面仍有提升潜力。

Method: 本文提出了一种新型的两流反馈多尺度渐进式生成对抗网络（MSPG-SEN）。该方法包含四个主要创新点：1) MSPG-SEN核心架构，旨在提升图像质量和视觉感知，同时简化训练并降低成本。2) 自适应感知-行为反馈循环（APFL），以提高模型鲁棒性、训练稳定性和降低训练成本。3) 全局连接两流动态残差网络，经消融实验证明可有效提高训练效率和泛化能力。4) 动态嵌入注意力机制（DEMA），能够有效捕获全局-局部信息，提高特征分离与表达能力，且计算资源需求极低。

Result: 实验结果表明，MSPG-SEN在INKK (89.7%)、AWUN (78.3%)、IONJ (85.5%)、POKL (88.7%)和OPIN (96.4%)等五个数据集上均取得了最先进的生成效果。所提出的APFL有效提升了模型鲁棒性和训练稳定性，并降低了训练成本。全局连接两流动态残差网络显著提高了训练效率和泛化能力。DEMA机制能有效捕获全局-局部信息，增强特征分离和表达能力，且仅需极少的计算资源，展现出强大的跨任务能力。

Conclusion: 本文提出的MSPG-SEN及其创新的组件，包括两流反馈多尺度渐进架构、自适应感知-行为反馈循环、全局连接两流动态残差网络和动态嵌入注意力机制，成功解决了现有GAN模型的部分局限性，实现了图像生成质量、训练稳定性、效率和泛化能力的全面提升，且显著降低了训练成本和资源需求。

Abstract: Although diffusion model has made good progress in the field of image
generation, GAN\cite{huang2023adaptive} still has a large development space due
to its unique advantages, such as WGAN\cite{liu2021comparing},
SSGAN\cite{guibas2021adaptive} \cite{zhang2022vsa} \cite{zhou2024adapt} and so
on. In this paper, we propose a novel two-flow feedback multi-scale progressive
generative adversarial network (MSPG-SEN) for GAN models. This paper has four
contributions: 1) : We propose a two-flow feedback multi-scale progressive
Generative Adversarial network (MSPG-SEN), which not only improves image
quality and human visual perception on the basis of retaining the advantages of
the existing GAN model, but also simplifies the training process and reduces
the training cost of GAN networks. Our experimental results show that, MSPG-SEN
has achieved state-of-the-art generation results on the following five
datasets,INKK The dataset is 89.7\%,AWUN The dataset is 78.3\%,IONJ The dataset
is 85.5\%,POKL The dataset is 88.7\%,OPIN The dataset is 96.4\%. 2) : We
propose an adaptive perception-behavioral feedback loop (APFL), which
effectively improves the robustness and training stability of the model and
reduces the training cost. 3) : We propose a globally connected two-flow
dynamic residual network(). After ablation experiments, it can effectively
improve the training efficiency and greatly improve the generalization ability,
with stronger flexibility. 4) : We propose a new dynamic embedded attention
mechanism (DEMA). After experiments, the attention can be extended to a variety
of image processing tasks, which can effectively capture global-local
information, improve feature separation capability and feature expression
capabilities, and requires minimal computing resources only 88.7\% with INJK
With strong cross-task capability.

</details>


### [51] [Domain Adaptation via Feature Refinement](https://arxiv.org/abs/2508.16124)
*Savvas Karatsiolis,Andreas Kamilaris*

Main category: cs.CV

TL;DR: DAFR2是一种简单有效的无监督域适应框架，通过结合BN统计量适应、特征蒸馏和假设迁移，生成鲁棒且域不变的特征空间，在多项基准测试中表现出优于现有方法的腐败鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在分布偏移下实现无监督域适应，旨在生成无需目标标签、复杂架构或复杂训练目标的鲁棒且域不变的特征空间，以提高模型对腐败的鲁棒性。

Method: 提出Domain Adaptation via Feature Refinement (DAFR2)框架，协同结合三个关键组件：利用未标注目标数据适应Batch Normalization统计量、从源域训练模型进行特征蒸馏以及假设迁移。该方法通过统计和表征层面的特征分布对齐来工作。

Result: 在CIFAR10-C、CIFAR100-C、MNIST-C和PatchCamelyon-C等基准数据集上的广泛实验表明，DAFR2在对抗腐败的鲁棒性方面优于现有方法。理论和实证分析进一步揭示，该方法实现了更好的特征对齐、增加了域间的互信息，并降低了对输入扰动的敏感性。

Conclusion: DAFR2框架通过其独特的组件组合，成功在无监督域适应中实现了鲁棒且域不变的特征空间，显著提升了模型对腐败的鲁棒性，并在特征对齐、互信息和抗扰动性方面表现出色，提供了一个简单而有效的解决方案。

Abstract: We propose Domain Adaptation via Feature Refinement (DAFR2), a simple yet
effective framework for unsupervised domain adaptation under distribution
shift. The proposed method synergistically combines three key components:
adaptation of Batch Normalization statistics using unlabeled target data,
feature distillation from a source-trained model and hypothesis transfer. By
aligning feature distributions at the statistical and representational levels,
DAFR2 produces robust and domain-invariant feature spaces that generalize
across similar domains without requiring target labels, complex architectures
or sophisticated training objectives. Extensive experiments on benchmark
datasets, including CIFAR10-C, CIFAR100-C, MNIST-C and PatchCamelyon-C,
demonstrate that the proposed algorithm outperforms prior methods in robustness
to corruption. Theoretical and empirical analyses further reveal that our
method achieves improved feature alignment, increased mutual information
between the domains and reduced sensitivity to input perturbations.

</details>


### [52] [4D Virtual Imaging Platform for Dynamic Joint Assessment via Uni-Plane X-ray and 2D-3D Registration](https://arxiv.org/abs/2508.16138)
*Hao Tang,Rongxi Yi,Lei Li,Kaiyi Cao,Jiapeng Zhao,Yihan Xiao,Minghai Shi,Peng Yuan,Yan Xi,Hui Tang,Wei Li,Zhan Wu,Yixin Zhou*

Main category: cs.CV

TL;DR: 开发了一种集成的4D锥束CT（CBCT）平台，能够快速、准确、低剂量地进行动态关节成像，以克服传统CT的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统CT无法捕捉动态、承重下的关节运动，而功能评估（特别是术后）需要4D成像。现有4D方法存在辐射剂量过高或2D技术空间信息不完整的缺点。

Method: 提出一个集成的4D关节分析平台，包含：(1) 双机械臂锥束CT（CBCT）系统，具有可编程、无龙门架且优化用于站立扫描的轨迹；(2) 混合成像流程，利用深度学习预处理、3D-2D投影和迭代优化，融合静态3D CBCT与动态2D X射线；(3) 经临床验证的定量运动学评估框架。

Result: 在模拟研究中，该方法实现了亚体素精度（0.235毫米）和99.18%的成功率，优于现有配准方法。临床评估也准确量化了全膝关节置换术后患者的胫骨平台运动和内外侧变异。

Conclusion: 该4D CBCT平台能够实现快速、准确、低剂量的动态关节成像，为生物力学研究、精准诊断和个性化骨科护理提供了新的机遇。

Abstract: Conventional computed tomography (CT) lacks the ability to capture dynamic,
weight-bearing joint motion. Functional evaluation, particularly after surgical
intervention, requires four-dimensional (4D) imaging, but current methods are
limited by excessive radiation exposure or incomplete spatial information from
2D techniques. We propose an integrated 4D joint analysis platform that
combines: (1) a dual robotic arm cone-beam CT (CBCT) system with a
programmable, gantry-free trajectory optimized for upright scanning; (2) a
hybrid imaging pipeline that fuses static 3D CBCT with dynamic 2D X-rays using
deep learning-based preprocessing, 3D-2D projection, and iterative
optimization; and (3) a clinically validated framework for quantitative
kinematic assessment. In simulation studies, the method achieved sub-voxel
accuracy (0.235 mm) with a 99.18 percent success rate, outperforming
conventional and state-of-the-art registration approaches. Clinical evaluation
further demonstrated accurate quantification of tibial plateau motion and
medial-lateral variance in post-total knee arthroplasty (TKA) patients. This 4D
CBCT platform enables fast, accurate, and low-dose dynamic joint imaging,
offering new opportunities for biomechanical research, precision diagnostics,
and personalized orthopedic care.

</details>


### [53] [High-Precision Mixed Feature Fusion Network Using Hypergraph Computation for Cervical Abnormal Cell Detection](https://arxiv.org/abs/2508.16140)
*Jincheng Li,Danyang Dong,Menglin Zheng,Jingbo Zhang,Yueqin Hang,Lichi Zhang,Lili Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种基于超图的宫颈异常细胞检测网络，通过有效融合空间相关特征和深度判别特征，显著提高了TCT图像中异常细胞的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有算法未能有效建模视觉特征间的空间相关性，且缺乏将细胞间相关特征与细胞内判别特征融合的策略，导致在智能辅助诊断系统中无法充分利用关键诊断信息。

Method: 本文提出了一种基于超图的细胞检测网络，用于融合不同类型的特征。具体来说，使用多级融合子网络（MLF-SNet）增强特征提取能力，并引入了带有超图计算模块的跨层级特征融合策略（CLFFS-HC）来整合混合特征。

Result: 在三个公开数据集上进行的实验结果表明，该方法显著提升了宫颈异常细胞检测的性能。

Conclusion: 所提出的超图细胞检测网络通过有效融合空间相关特征和深度判别特征，显著改善了宫颈异常细胞的检测性能。

Abstract: Automatic detection of abnormal cervical cells from Thinprep Cytologic Test
(TCT) images is a critical component in the development of intelligent
computer-aided diagnostic systems. However, existing algorithms typically fail
to effectively model the correlations of visual features, while these spatial
correlation features actually contain critical diagnostic information.
Furthermore, no detection algorithm has the ability to integrate
inter-correlation features of cells with intra-discriminative features of
cells, lacking a fusion strategy for the end-to-end detection model. In this
work, we propose a hypergraph-based cell detection network that effectively
fuses different types of features, combining spatial correlation features and
deep discriminative features. Specifically, we use a Multi-level Fusion
Sub-network (MLF-SNet) to enhance feature extractioncapabilities. Then we
introduce a Cross-level Feature Fusion Strategy with Hypergraph Computation
module (CLFFS-HC), to integrate mixed features. Finally, we conducted
experiments on three publicly available datasets, and the results demonstrate
that our method significantly improves the performance of cervical abnormal
cell detection.

</details>


### [54] [Beyond Human-prompting: Adaptive Prompt Tuning with Semantic Alignment for Anomaly Detection](https://arxiv.org/abs/2508.16157)
*Pi-Wei Chen,Jerry Chun-Wei Lin,Wei-Han Chen,Jia Ji,Zih-Ching Chen,Feng-Hao Yeh,Chao-Chun Chen*

Main category: cs.CV

TL;DR: 本文提出APT（自适应提示调优）框架，通过自生成异常样本和自优化元提示引导方案（SMGS）训练可学习提示，克服了现有视觉语言模型在异常检测中对人工提示和异常样本的依赖，实现了无需先验知识的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型在异常检测中展现潜力，但现有方法依赖人工设计提示且缺乏可获取的异常样本，导致在情境特定异常理解方面存在显著局限性。

Method: 提出APT框架，一种无需先验知识的少样本方法。它利用带有噪声扰动的自生成异常样本训练可学习提示，以捕捉情境相关异常。为避免过拟合合成噪声，引入自优化元提示引导方案（SMGS），迭代地将提示与通用异常语义对齐，并融入多样化的合成异常。

Result: 该系统不仅推动了像素级异常检测的进步，而且在多个基准数据集上取得了最先进的性能，且无需预先知识即可进行提示设计。

Conclusion: APT为实际世界中的异常检测提供了一个鲁棒且通用的解决方案。

Abstract: Pre-trained Vision-Language Models (VLMs) have recently shown promise in
detecting anomalies. However, previous approaches are fundamentally limited by
their reliance on human-designed prompts and the lack of accessible anomaly
samples, leading to significant gaps in context-specific anomaly understanding.
In this paper, we propose \textbf{A}daptive \textbf{P}rompt \textbf{T}uning
with semantic alignment for anomaly detection (APT), a groundbreaking prior
knowledge-free, few-shot framework and overcomes the limitations of traditional
prompt-based approaches. APT uses self-generated anomaly samples with noise
perturbations to train learnable prompts that capture context-dependent
anomalies in different scenarios. To prevent overfitting to synthetic noise, we
propose a Self-Optimizing Meta-prompt Guiding Scheme (SMGS) that iteratively
aligns the prompts with general anomaly semantics while incorporating diverse
synthetic anomaly. Our system not only advances pixel-wise anomaly detection,
but also achieves state-of-the-art performance on multiple benchmark datasets
without requiring prior knowledge for prompt crafting, establishing a robust
and versatile solution for real-world anomaly detection.

</details>


### [55] [RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution](https://arxiv.org/abs/2508.16158)
*Haodong He,Yancheng Bai,Rui Lan,Xu Duan,Lei Sun,Xiangxiang Chu,Gui-Song Xia*

Main category: cs.CV

TL;DR: 现有SISR方法在处理多目标区域细节时面临挑战，本文提出RAGSR方法，通过区域注意力机制提取并编码细粒度区域信息，实现更好的超分辨率效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大视觉-语言模型(VLM)与文本到图像(T2I)扩散模型结合的单图像超分辨率(SISR)方法表现出色，但它们在生成清晰准确的区域细节，尤其是在多对象场景中，仍面临显著挑战。这主要源于缺乏细粒度的区域描述以及模型捕捉复杂提示的能力不足。

Method: 本文提出一种区域注意力引导的超分辨率(RAGSR)方法。该方法显式提取局部细粒度信息，并通过新颖的区域注意力机制对其进行有效编码。具体地，RAGSR首先定位图像中的对象区域，为每个区域分配细粒度描述，形成“区域-文本”对作为T2I模型的文本先验。随后，利用区域引导注意力机制确保每个“区域-文本”对在注意力过程中得到适当考虑，同时阻止不相关对之间的不必要交互，从而实现对文本和图像信息集成的精细控制。

Result: 在基准数据集上的实验结果表明，与现有方法相比，RAGSR在生成感知上真实的视觉细节方面表现出卓越的性能，同时保持了上下文一致性。

Conclusion: RAGSR通过引入区域注意力机制和细粒度区域描述，有效克服了传统SISR方法在处理复杂区域细节和多对象场景时的局限性，实现了更精细的文本-图像信息集成与更优越的超分辨率效果。

Abstract: The rich textual information of large vision-language models (VLMs) combined
with the powerful generative prior of pre-trained text-to-image (T2I) diffusion
models has achieved impressive performance in single-image super-resolution
(SISR). However, existing methods still face significant challenges in
generating clear and accurate regional details, particularly in scenarios
involving multiple objects. This challenge primarily stems from a lack of
fine-grained regional descriptions and the models' insufficient ability to
capture complex prompts. To address these limitations, we propose a Regional
Attention Guided Super-Resolution (RAGSR) method that explicitly extracts
localized fine-grained information and effectively encodes it through a novel
regional attention mechanism, enabling both enhanced detail and overall
visually coherent SR results. Specifically, RAGSR localizes object regions in
an image and assigns fine-grained caption to each region, which are formatted
as region-text pairs as textual priors for T2I models. A regional guided
attention is then leveraged to ensure that each region-text pair is properly
considered in the attention process while preventing unwanted interactions
between unrelated region-text pairs. By leveraging this attention mechanism,
our approach offers finer control over the integration of text and image
information, thereby effectively overcoming limitations faced by traditional
SISR techniques. Experimental results on benchmark datasets demonstrate that
our approach exhibits superior performance in generating perceptually authentic
visual details while maintaining contextual consistency compared to existing
approaches.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [56] [T-ILR: a Neurosymbolic Integration for LTLf](https://arxiv.org/abs/2508.15943)
*Riccardo Andreoni,Andrei Buliga,Alessandro Daniele,Chiara Ghidini,Marco Montali,Massimiliano Ronzani*

Main category: cs.AI

TL;DR: 提出一种新的神经符号方法T-ILR，将LTLf时间逻辑规范直接整合到深度学习架构中处理序列任务，与现有方法相比，提高了准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号方法在静态领域表现良好，但处理时间逻辑规范的方法不足且现有唯一方法依赖于有限状态自动机，效率有限。亟需一种能直接将LTLf时间逻辑整合到深度学习中处理序列任务的框架。

Method: 提出“时间迭代局部优化”(T-ILR)方法。该方法通过扩展迭代局部优化(ILR)神经符号算法，并利用模糊LTLf解释，将LTLf时间逻辑规范直接融入深度学习架构中处理序列任务。

Result: 在图像序列分类基准上评估T-ILR，结果表明与现有最先进方法相比，其准确性和计算效率均有提高。

Conclusion: T-ILR成功地将LTLf时间逻辑规范整合到深度学习中，为处理具有时间知识的序列任务提供了一种比现有方法更准确、更高效的神经符号框架。

Abstract: State-of-the-art approaches for integrating symbolic knowledge with deep
learning architectures have demonstrated promising results in static domains.
However, methods to handle temporal logic specifications remain underexplored.
The only existing approach relies on an explicit representation of a
finite-state automaton corresponding to the temporal specification. Instead, we
aim at proposing a neurosymbolic framework designed to incorporate temporal
logic specifications, expressed in Linear Temporal Logic over finite traces
(LTLf), directly into deep learning architectures for sequence-based tasks. We
extend the Iterative Local Refinement (ILR) neurosymbolic algorithm, leveraging
the recent introduction of fuzzy LTLf interpretations. We name this proposed
method Temporal Iterative Local Refinement (T-ILR). We assess T-ILR on an
existing benchmark for temporal neurosymbolic architectures, consisting of the
classification of image sequences in the presence of temporal knowledge. The
results demonstrate improved accuracy and computational efficiency compared to
the state-of-the-art method.

</details>


### [57] [CoFE: A Framework Generating Counterfactual ECG for Explainable Cardiac AI-Diagnostics](https://arxiv.org/abs/2508.16033)
*Jong-Hwan Jang,Junho Song,Yong-Yeon Jo*

Main category: cs.AI

TL;DR: 本文引入了一个名为CoFE的反事实ECG生成框架，旨在通过展示特定ECG特征如何影响预测决策来提高AI-ECG模型的解释性，并已在临床案例中验证其与医学知识的一致性。


<details>
  <summary>Details</summary>
Motivation: 为确保基于AI的ECG预测模型（AI-ECG）能成功融入临床实践，需要可解释的AI（XAI）方法来阐明其预测机制。

Method: 引入了一个名为CoFE（CounterFactual ECGs）的框架，通过生成反事实ECG来展示特定特征（如波幅和间隔）如何影响模型的预测决策。通过房颤分类和钾水平回归模型两个案例研究来演示CoFE的适用性。

Result: CoFE揭示的ECG信号特征变化与已建立的临床知识相符。

Conclusion: 该框架通过明确有效特征在ECG中的位置及其对模型预测的影响，有望增强AI-ECG模型的解释性，并支持更有效的临床决策。

Abstract: Recognizing the need for explainable AI (XAI) approaches to enable the
successful integration of AI-based ECG prediction models (AI-ECG) into clinical
practice, we introduce a framework generating \textbf{Co}unter\textbf{F}actual
\textbf{E}CGs (i,e., named CoFE) to illustrate how specific features, such as
amplitudes and intervals, influence the model's predictive decisions. To
demonstrate the applicability of the CoFE, we present two case studies: atrial
fibrillation classification and potassium level regression models. The CoFE
reveals feature changes in ECG signals that align with the established clinical
knowledge. By clarifying both \textbf{where valid features appear} in the ECG
and \textbf{how they influence the model's predictions}, we anticipate that our
framework will enhance the interpretability of AI-ECG models and support more
effective clinical decision-making. Our demonstration video is available at:
https://www.youtube.com/watch?v=YoW0bNBPglQ.

</details>


### [58] [MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs](https://arxiv.org/abs/2508.16051)
*Yiheng Hu,Xiaoyang Wang,Qing Liu,Xiwei Xu,Qian Fu,Wenjie Zhang,Liming Zhu*

Main category: cs.AI

TL;DR: 本文提出一个无需训练的框架，通过自适应规划图（Adaptive Planning Graph）指导，用于多模态多跳问答，解决了传统方法易受中间错误影响和训练成本高的问题，并在实验中取得了与现有训练模型相当或更优的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态多跳问答方法依赖顺序检索和推理（单路径范式），易受误导性中间步骤影响而导致错误。此外，开发多模态模型通常需要昂贵的计算和大量的训练。

Method: 我们提出一个由自适应规划图（Adaptive Planning Graph）指导的、无需训练的框架，该框架包含规划、检索和推理模块。规划模块分析当前状态，决定下一步动作及图的扩展方式，从而实现推理路径的动态灵活探索。针对文本检索到未指定目标模态的问题，我们设计了模态特定策略，以动态适应不同数据类型。此方法无需昂贵的任务特定训练，能保持多模态信息特性并与最新模型无缝集成。

Result: 在MultimodalQA和WebQA数据集上的实验表明，我们的方法达到或超越了依赖训练的现有模型性能。

Conclusion: 我们提出了一种无需训练、灵活且高效的多模态多跳问答框架，有效解决了现有方法的局限性，并展现出卓越的性能。

Abstract: Multimodal Multi-hop question answering requires integrating information from
diverse sources, such as images and texts, to derive answers. Existing methods
typically rely on sequential retrieval and reasoning, where each step builds on
the previous output. However, this single-path paradigm makes them vulnerable
to errors due to misleading intermediate steps. Moreover, developing multimodal
models can be computationally expensive, often requiring extensive training. To
address these limitations, we propose a training-free framework guided by an
Adaptive Planning Graph, which consists of planning, retrieval and reasoning
modules. The planning module analyzes the current state of the Adaptive
Planning Graph, determines the next action and where to expand the graph, which
enables dynamic and flexible exploration of reasoning paths. To handle
retrieval of text to unspecified target modalities, we devise modality-specific
strategies that dynamically adapt to distinct data types. Our approach
preserves the characteristics of multimodal information without costly
task-specific training, enabling seamless integration with up-to-date models.
Finally, the experiments on MultimodalQA and WebQA show that our approach
matches or outperforms existing models that rely on training.

</details>


### [59] [Generative Foundation Model for Structured and Unstructured Electronic Health Records](https://arxiv.org/abs/2508.16054)
*Sonish Sivarajkumar,Hang Zhang,Yuelyu Ji,Maneesh Bilalpur,Xizhi Wu,Chenyu Li,Min Gu Kwak,Shyam Visweswaran,Yanshan Wang*

Main category: cs.AI

TL;DR: 本文提出Generative Deep Patient (GDP)，一个多模态基础模型，能原生编码结构化电子健康记录时间序列数据并与非结构化数据融合，用于临床预测和高质量临床叙事生成，表现出卓越性能并具有临床实用性。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）是丰富的临床数据源，但其包含结构化、非结构化等多模态数据，复杂且异构。有效利用这些异构数据对改善患者预后至关重要。现有大型语言模型（LLMs）处理EHR数据时，常将数值数据序列化为文本，这可能丢失时间性和定量细节。因此，需要一个能够原生处理多模态EHR数据的基础模型。

Method: 引入了Generative Deep Patient (GDP) 模型，它是一个多模态基础模型。GDP通过CNN-Transformer编码器原生编码结构化EHR时间序列数据，并通过跨模态注意力机制将其与非结构化EHR数据融合，输入到基于LLaMA的解码器。GDP的训练分为两个阶段：(1) 生成式预训练，学习从原始患者时间线生成临床叙事，并执行掩蔽特征预测（MFP）和下一时间步预测（NTP）以捕捉时间动态；(2) 多任务微调，用于临床意义预测（如心力衰竭、2型糖尿病、30天再入院）。

Result: 在MIMIC-IV数据集上，GDP在临床预测任务中表现优异：心力衰竭AUROC = 0.923，2型糖尿病AUROC = 0.817，30天再入院AUROC = 0.627。在叙事生成方面，GDP达到ROUGE-L = 0.135和BERTScore-F1 = 0.545。在一项盲法人类评估中，GDP-Instruct在忠实度、流畅性和整体临床实用性方面得分最高，表明在不牺牲准确性的前提下可减少医院文档工作量。

Conclusion: 研究结果表明，单个多模态基础模型（GDP）既能预测临床可操作事件，又能生成高质量的临床叙事。此外，GDP的灵活架构可扩展到其他模态数据。

Abstract: Electronic health records (EHRs) are rich clinical data sources but complex
repositories of patient data, spanning structured elements (demographics,
vitals, lab results, codes), unstructured clinical notes and other modalities
of data. Harnessing this heterogeneity is critical for improving patient
outcomes. Recent advances in large language models (LLMs) have enabled
foundation models that can learn from multiple data modalities and support
clinical tasks. However, most current approaches simply serialize numeric EHR
data into text, which risks losing temporal and quantitative detail. We
introduce Generative Deep Patient (GDP), a multimodal foundation model that
natively encodes structured EHR time-series via a CNN-Transformer encoder and
fuses it with unstructured EHRs through cross-modal attention into a
LLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,
where it learns to produce clinical narratives from raw patient timelines while
also performing masked feature prediction (MFP) and next time-step prediction
(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for
clinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day
readmission). In clinical prediction, GDP demonstrated superior performance on
MIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and
30-day readmission AUROC = 0.627. For narrative generation, GDP achieved
ROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,
GDP-Instruct scored highest on faithfulness, fluency, and overall clinical
utility, suggesting reduced hospital documentation workload without sacrificing
accuracy. Our results demonstrate that a single multimodal foundation model can
both predict clinically actionable events and generate high-quality clinical
narratives. Furthermore, GDP's flexible architecture can be extended to
additional modalities.

</details>


### [60] [Urban Comfort Assessment in the Era of Digital Planning: A Multidimensional, Data-driven, and AI-assisted Framework](https://arxiv.org/abs/2508.16057)
*Sijie Yang,Binyu Lei,Filip Biljecki*

Main category: cs.AI

TL;DR: 本研究旨在数字规划背景下，探索城市舒适度的理论诠释和评估方法，以应对当前缺乏明确定义和全面评估框架的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究利用计算方法评估城市舒适度相关因素（如绿化覆盖、热舒适度、步行性），但城市舒适度的清晰定义及其全面的评估框架仍然缺失。

Method: 本研究将探讨数字规划中城市舒适度的理论诠释和评估方法，重点关注多维分析、数据支持和AI辅助这三个关键维度。

Result: 摘要中未提供具体的研究结果，主要阐述了研究的范围和方法论。

Conclusion: 摘要未给出明确的研究结论，而是指出了研究将要进行的方向和强调的重点。

Abstract: Ensuring liveability and comfort is one of the fundamental objectives of
urban planning. Numerous studies have employed computational methods to assess
and quantify factors related to urban comfort such as greenery coverage,
thermal comfort, and walkability. However, a clear definition of urban comfort
and its comprehensive evaluation framework remain elusive. Our research
explores the theoretical interpretations and methodologies for assessing urban
comfort within digital planning, emphasising three key dimensions:
multidimensional analysis, data support, and AI assistance.

</details>


### [61] [Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting](https://arxiv.org/abs/2508.16059)
*Zhuomin Chen,Dan Li,Jiahui Zhou,Shunyu Wu,Haozheng Ye,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: 本文提出多层可控嵌入融合（MSEF）框架，通过在LLM的多个深度层融合时间序列嵌入，克服现有方法中时间序列信息整合不足的问题，显著提升了时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测（TSF）是一个基础任务。现有将LLM应用于TSF的方法，由于时间序列信息通常只在浅层（主要在输入层）集成，导致TS信息在LLM深层逐渐衰减，文本嵌入与TS表示之间的适应性不足。

Method: MSEF框架通过使用现成的时间序列基础模型提取语义丰富的TS嵌入，并利用层特定的引导向量，将这些TS嵌入与LLM中间层的文本表示进行融合。引导向量持续优化时间序列和文本模态的对齐，确保有效的少样本学习和跨层适应。

Result: 在七个基准数据集上的实验结果表明，MSEF相比基线模型显著提高了性能，平均将MSE降低了31.8%。

Conclusion: MSEF通过实现LLM对时间序列模式的深度访问，有效解决了TS信息在深层丢失的问题，显著提升了LLM在时间序列预测任务上的表现和少样本学习能力。

Abstract: Time series (TS) data are ubiquitous across various application areas,
rendering time series forecasting (TSF) a fundamental task. With the astounding
advances in large language models (LLMs), a variety of methods have been
developed to adapt LLMs for time series forecasting. Despite unlocking the
potential of LLMs in comprehending TS data, existing methods are inherently
constrained by their shallow integration of TS information, wherein LLMs
typically access TS representations at shallow layers, primarily at the input
layer. This causes the influence of TS representations to progressively fade in
deeper layers and eventually leads to ineffective adaptation between textual
embeddings and TS representations. In this paper, we propose the Multi-layer
Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to
directly access time series patterns at all depths, thereby mitigating the
progressive loss of TS information in deeper layers. Specifically, MSEF
leverages off-the-shelf time series foundation models to extract semantically
rich embeddings, which are fused with intermediate text representations across
LLM layers via layer-specific steering vectors. These steering vectors are
designed to continuously optimize the alignment between time series and textual
modalities and facilitate a layer-specific adaptation mechanism that ensures
efficient few-shot learning capabilities. Experimental results on seven
benchmarks demonstrate significant performance improvements by MSEF compared
with baselines, with an average reduction of 31.8% in terms of MSE. The code is
available at https://github.com/One1sAll/MSEF.

</details>


### [62] [InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles](https://arxiv.org/abs/2508.16072)
*Zizhen Li,Chuanhao Li,Yibin Wang,Qi Chen,Diping Song,Yukang Feng,Jianwen Sun,Jiaxin Ai,Fanrui Zhang,Mingzhu Sun,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 本文提出了InMind框架，旨在评估大语言模型（LLMs）在社会推理游戏中捕捉和应用个性化推理风格的能力，研究发现多数LLMs在此方面仍存在显著局限。


<details>
  <summary>Details</summary>
Motivation: 现有对LLMs的评估常忽略人类在社交情境中特有的个性化推理风格。社会推理游戏（SDGs）提供了一个天然的测试平台来评估这种个体化的、情境有效的推理策略。

Method: 引入了InMind，一个基于认知理论的评估框架，用于评估LLMs在SDGs中捕捉和应用个性化推理风格的能力。InMind通过回合级策略追踪和赛后反思来增强游戏数据，支持观察者和参与者两种模式，并包含四项认知任务以评估静态对齐和动态适应能力。研究以“阿瓦隆”游戏为例，评估了11个最先进的LLMs。

Result: 通用LLMs（如GPT-4o）倾向于依赖词汇线索，难以将反思与时间性游戏玩法相结合或适应不断变化的策略。相比之下，推理增强型LLMs（如DeepSeek-R1）已展现出风格敏感推理的早期迹象。

Conclusion: 当前LLMs在个性化、适应性推理能力方面存在关键局限。InMind框架的提出是实现认知对齐人机交互的重要一步。

Abstract: LLMs have shown strong performance on human-centric reasoning tasks. While
previous evaluations have explored whether LLMs can infer intentions or detect
deception, they often overlook the individualized reasoning styles that
influence how people interpret and act in social contexts. Social deduction
games (SDGs) provide a natural testbed for evaluating individualized reasoning
styles, where different players may adopt diverse but contextually valid
reasoning strategies under identical conditions. To address this, we introduce
InMind, a cognitively grounded evaluation framework designed to assess whether
LLMs can capture and apply personalized reasoning styles in SDGs. InMind
enhances structured gameplay data with round-level strategy traces and
post-game reflections, collected under both Observer and Participant modes. It
supports four cognitively motivated tasks that jointly evaluate both static
alignment and dynamic adaptation. As a case study, we apply InMind to the game
Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o
frequently rely on lexical cues, struggling to anchor reflections in temporal
gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs
like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These
findings reveal key limitations in current LLMs' capacity for individualized,
adaptive reasoning, and position InMind as a step toward cognitively aligned
human-AI interaction.

</details>


### [63] [IR-Agent: Expert-Inspired LLM Agents for Structure Elucidation from Infrared Spectra](https://arxiv.org/abs/2508.16112)
*Heewoong Noh,Namkyeong Lee,Gyoung S. Na,Kibum Kim,Chanyoung Park*

Main category: cs.AI

TL;DR: 本文提出了IR-Agent，一个多智能体框架，通过模拟专家分析流程，从红外光谱中高效准确地解析分子结构。


<details>
  <summary>Details</summary>
Motivation: 现有红外光谱分析方法未能有效反映专家分析过程，且缺乏整合多样化化学知识的灵活性，这限制了其在实际应用中的效能。

Method: 本文提出了IR-Agent，一个新颖的多智能体框架，用于从红外光谱中解析分子结构。该框架旨在模仿专家驱动的红外分析流程，且具有内在的可扩展性。每个智能体专注于红外解释的特定方面，并通过互补作用实现集成推理。

Result: 通过广泛实验，IR-Agent不仅提高了实验红外光谱上的基线性能，而且对各种形式的化学信息表现出强大的适应性。

Conclusion: IR-Agent多智能体框架通过模拟专家分析和集成推理，显著提高了红外光谱解析分子结构的准确性和灵活性。

Abstract: Spectral analysis provides crucial clues for the elucidation of unknown
materials. Among various techniques, infrared spectroscopy (IR) plays an
important role in laboratory settings due to its high accessibility and low
cost. However, existing approaches often fail to reflect expert analytical
processes and lack flexibility in incorporating diverse types of chemical
knowledge, which is essential in real-world analytical scenarios. In this
paper, we propose IR-Agent, a novel multi-agent framework for molecular
structure elucidation from IR spectra. The framework is designed to emulate
expert-driven IR analysis procedures and is inherently extensible. Each agent
specializes in a specific aspect of IR interpretation, and their complementary
roles enable integrated reasoning, thereby improving the overall accuracy of
structure elucidation. Through extensive experiments, we demonstrate that
IR-Agent not only improves baseline performance on experimental IR spectra but
also shows strong adaptability to various forms of chemical information.

</details>


### [64] [Extending FKG.in: Towards a Food Claim Traceability Network](https://arxiv.org/abs/2508.16117)
*Saransh Kumar Gupta,Rizwan Gulzar Mir,Lipika Dey,Partha Pratim Das,Anirban Sen,Ramesh Jain*

Main category: cs.AI

TL;DR: 针对食品声明混乱且难以验证的现状，本文提出了一个食物声明可追溯网络（FCN），作为现有印度食物知识图谱的扩展，旨在通过本体设计、半自动知识管理工作流和LLMs，以结构化、可验证的方式管理食物声明，提升食物知识的透明度和问责制。


<details>
  <summary>Details</summary>
Motivation: 全球食品领域充斥着大量科学、文化和商业性的食物声明，从有据可查的健康益处到误导性信息和文化信仰不一而足。然而，目前缺乏有效的基础设施来追踪、验证和情境化这些广泛影响的声明，导致信息碎片化且不发达。

Method: 本文提出了一个食物声明可追溯网络（FCN），作为正在构建的印度食物知识图谱FKG.in的扩展。研究团队开发了本体设计和半自动知识管理工作流，并利用Reddit数据和大型语言模型（LLMs）实现了FKG.in-FCN的概念验证。FCN集成了精选数据输入、结构化模式和来源感知管道，用于食品相关声明的提取和验证。

Result: 研究团队成功开发了FKG.in-FCN的概念验证，展示了其在结构化、可验证且可解释地建模食物声明及其可追溯性方面的能力。尽管该方法直接应用于印度食物知识图谱，但其方法学本身具有应用无关性，可适应于其他地理、烹饪或监管环境。

Conclusion: 通过以结构化、可验证和可解释的方式建模食物声明及其可追溯性，本研究旨在为更透明和负责任的食物知识生态系统做出贡献，从而帮助研究人员、政策制定者以及日常消费者更好地应对充满饮食断言的世界。

Abstract: The global food landscape is rife with scientific, cultural, and commercial
claims about what foods are, what they do, what they should not do, or should
not do. These range from rigorously studied health benefits (probiotics improve
gut health) and misrepresentations (soaked almonds make one smarter) to vague
promises (superfoods boost immunity) and culturally rooted beliefs (cold foods
cause coughs). Despite their widespread influence, the infrastructure for
tracing, verifying, and contextualizing these claims remains fragmented and
underdeveloped. In this paper, we propose a Food Claim-Traceability Network
(FCN) as an extension of FKG.in, a knowledge graph of Indian food that we have
been incrementally building. We also present the ontology design and the
semi-automated knowledge curation workflow that we used to develop a proof of
concept of FKG.in-FCN using Reddit data and Large Language Models. FCN
integrates curated data inputs, structured schemas, and provenance-aware
pipelines for food-related claim extraction and validation. While directly
linked to the Indian food knowledge graph as an application, our methodology
remains application-agnostic and adaptable to other geographic, culinary, or
regulatory settings. By modeling food claims and their traceability in a
structured, verifiable, and explainable way, we aim to contribute to more
transparent and accountable food knowledge ecosystems, supporting researchers,
policymakers, and most importantly, everyday consumers in navigating a world
saturated with dietary assertions.

</details>


### [65] [Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning](https://arxiv.org/abs/2508.16129)
*Ruiqi Wu,Yuang Yao,Tengfei Ma,Chenran Zhang,Na Su,Tao Zhou,Geng Chen,Wen Fan,Yi Zhou*

Main category: cs.AI

TL;DR: 针对现有医学多模态大语言模型（MLLMs）在复杂临床推理方面的局限性，本文提出了首个眼科多模态数据集MM-Retinal-Reason和专为眼科设计的OphthaReason模型，该模型通过不确定性感知动态思维（UADT）机制，在基本和复杂推理任务上均达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态推理模型多聚焦于基于视觉特征匹配的浅层基本推理，无法满足真实临床诊断中整合异构临床信息（如主诉、病史）与多模态医学影像数据进行复杂推理的需求。研究旨在弥补这一空白，开发能模拟真实临床思维模式、执行复杂推理的眼科多模态模型。

Method: 1. 构建了首个涵盖感知和推理全范围的眼科多模态数据集MM-Retinal-Reason，包含基本和复杂推理任务。2. 提出了首个专为眼科设计的、具有分步推理轨迹的多模态推理模型OphthaReason。3. 设计了一种新颖的“不确定性感知动态思维”（UADT）方法，通过熵估计样本级不确定性，并利用形状化优势机制动态调节模型的探索深度，以灵活适应基本和复杂推理任务。

Result: 实验证明，OphthaReason模型在基本和复杂推理任务上均取得了最先进的性能，相比通用MLLMs、医学MLLMs、基于RL的医学MLLMs和眼科MLLMs，性能分别至少提升了24.92%、15.00%、21.20%和17.66%。

Conclusion: 该研究通过引入综合性眼科多模态数据集和创新的OphthaReason模型（包含UADT机制），有效提升了多模态模型在眼科复杂临床推理任务上的能力，为模拟真实临床思维模式和实现更精准的诊断提供了有效解决方案。

Abstract: Multimodal large language models (MLLMs) have recently demonstrated
remarkable reasoning abilities with reinforcement learning paradigm. Although
several multimodal reasoning models have been explored in the medical domain,
most of them focus exclusively on basic reasoning, which refers to shallow
inference based on visual feature matching. However, real-world clinical
diagnosis extends beyond basic reasoning, demanding reasoning processes that
integrate heterogeneous clinical information (such as chief complaints and
medical history) with multimodal medical imaging data. To bridge this gap, we
introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the
full spectrum of perception and reasoning. It encompasses both basic reasoning
tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental
reasoning capabilities and emulate realistic clinical thinking patterns.
Building upon MM-Retinal-Reason, we propose OphthaReason, the first
ophthalmology-specific multimodal reasoning model with step-by-step reasoning
traces. To enable flexible adaptation to both basic and complex reasoning
tasks, we specifically design a novel method called Uncertainty-Aware Dynamic
Thinking (UADT), which estimates sample-level uncertainty via entropy and
dynamically modulates the model's exploration depth using a shaped advantage
mechanism. Comprehensive experiments demonstrate that our model achieves
state-of-the-art performance on both basic and complex reasoning tasks,
outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and
ophthalmic MLLMs by at least 24.92\%, 15.00\%, 21.20\%, and 17.66\%. Project
Page: \href{https://github.com/lxirich/OphthaReason}{link}.

</details>


### [66] [Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain](https://arxiv.org/abs/2508.16172)
*Kai Hu,Parfait Atchade-Adelomou,Carlo Adornetto,Adrian Mora-Carrero,Luis Alonso-Pastor,Ariel Noyman,Yubo Liu,Kent Larson*

Main category: cs.AI

TL;DR: 本文提出Preference Chain方法，结合Graph RAG与LLM，有效提升了在数据稀缺环境下交通系统中人类行为模拟的上下文感知能力和真实性，超越了标准LLM。


<details>
  <summary>Details</summary>
Motivation: 城市环境中人类行为数据收集困难（尤其在新开发区域），且现有基于LLM的生成智能体在模拟人类行为时，难以保持输出的一致性、上下文敏感性和真实性。

Method: 引入“Preference Chain”这一新方法，将图检索增强生成（Graph RAG）与大型语言模型（LLMs）相结合，旨在增强交通系统中人类行为的上下文感知模拟能力。

Result: 在Replica数据集上的实验表明，Preference Chain在与真实交通模式选择的对齐方面优于标准LLM。开发的Mobility Agent展示了该方法在新兴城市交通建模、个性化出行分析和动态交通预测中的应用潜力。

Conclusion: 尽管存在推理速度慢和幻觉风险，Preference Chain为在数据稀缺环境下模拟复杂人类行为提供了一个有前景的框架，有效弥补了传统数据驱动模型的不足。

Abstract: Understanding human behavior in urban environments is a crucial field within
city sciences. However, collecting accurate behavioral data, particularly in
newly developed areas, poses significant challenges. Recent advances in
generative agents, powered by Large Language Models (LLMs), have shown promise
in simulating human behaviors without relying on extensive datasets.
Nevertheless, these methods often struggle with generating consistent,
context-sensitive, and realistic behavioral outputs. To address these
limitations, this paper introduces the Preference Chain, a novel method that
integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance
context-aware simulation of human behavior in transportation systems.
Experiments conducted on the Replica dataset demonstrate that the Preference
Chain outperforms standard LLM in aligning with real-world transportation mode
choices. The development of the Mobility Agent highlights potential
applications of proposed method in urban mobility modeling for emerging cities,
personalized travel behavior analysis, and dynamic traffic forecasting. Despite
limitations such as slow inference and the risk of hallucination, the method
offers a promising framework for simulating complex human behavior in
data-scarce environments, where traditional data-driven models struggle due to
limited data availability.

</details>


### [67] [Competition and Attraction Improve Model Fusion](https://arxiv.org/abs/2508.16204)
*João Abrantes,Robert Tjarko Lange,Yujin Tang*

Main category: cs.AI

TL;DR: 本文提出M2N2，一种受自然启发的进化算法，通过动态边界调整、多样性保持和启发式吸引度指标，克服了现有模型合并方法的手动分区限制，首次实现从零开始的模型进化，并在语言和图像生成模型合并中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法需要手动将模型参数划分为固定组，这限制了潜在组合的探索并影响性能。研究旨在克服这些限制，实现更灵活、高效的模型合并。

Method: 提出Model Merging of Natural Niches (M2N2)，一种包含三个关键特征的进化算法：1) 动态调整合并边界以逐步探索更广泛的参数组合；2) 受自然界资源竞争启发的、用于维护多样化高性能模型群体的多样性保持机制；3) 基于启发式的吸引度指标，用于识别最有希望进行融合的模型对。

Result: 实验首次证明模型合并可用于从零开始进化模型，例如M2N2在MNIST分类器进化中表现出与CMA-ES相当的性能，但计算效率更高。此外，M2N2能扩展到合并专业化的语言和图像生成模型，达到最先进的性能，并能保留健身函数未明确优化的关键模型能力。

Conclusion: M2N2提供了一种强大、通用且高效的进化模型合并框架，不仅能克服现有方法的局限性，还扩展了模型合并的应用范围，包括从零开始的模型进化，并在复杂任务上实现了卓越性能，同时保持了模型的鲁棒性。

Abstract: Model merging is a powerful technique for integrating the specialized
knowledge of multiple machine learning models into a single model. However,
existing methods require manually partitioning model parameters into fixed
groups for merging, which restricts the exploration of potential combinations
and limits performance. To overcome these limitations, we propose Model Merging
of Natural Niches (M2N2), an evolutionary algorithm with three key features:
(1) dynamic adjustment of merging boundaries to progressively explore a broader
range of parameter combinations; (2) a diversity preservation mechanism
inspired by the competition for resources in nature, to maintain a population
of diverse, high-performing models that are particularly well-suited for
merging; and (3) a heuristicbased attraction metric to identify the most
promising pairs of models for fusion. Our experimental results demonstrate, for
the first time, that model merging can be used to evolve models entirely from
scratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch
and achieve performance comparable to CMA-ES, while being computationally more
efficient. Furthermore, M2N2 scales to merge specialized language and image
generation models, achieving state-of-the-art performance. Notably, it
preserves crucial model capabilities beyond those explicitly optimized by the
fitness function, highlighting its robustness and versatility. Our code is
available at https://github.com/SakanaAI/natural_niches

</details>


### [68] [The next question after Turing's question: Introducing the Grow-AI test](https://arxiv.org/abs/2508.16277)
*Alexandru Tugui*

Main category: cs.AI

TL;DR: 本研究扩展了GROW-AI框架，旨在评估AI的“成长”（成熟度），作为图灵测试的后续。该框架采用多标准游戏和统一日志，为不同类型AI提供连贯、可比和可追溯的评估，以捕捉其演化路径。


<details>
  <summary>Details</summary>
Motivation: 旨在回答“机器能否成长？”这一问题，通过扩展GROW-AI人工智能评估框架，超越图灵测试的局限，不仅测量性能，更能捕捉AI实体迈向成熟的演化路径。

Method: 方法基于六个主要标准（C1-C6），每个标准通过特定“游戏”在探索人类和AI维度的四个领域中进行评估。所有实体决策和行动记录在标准化AI日志中。评估使用专家先验方法确定初始权重，并计算六个分数的算术平均值作为“成长指数”（Grow Up Index），通过成熟度阈值进行解释。

Result: 该方法能够对不同类型（机器人、软件代理、LLM）AI实体的“成长”水平进行连贯和可比的评估。多游戏结构突出了AI的优势和脆弱区域，统一日志保证了评估的可追溯性和可复制性。

Conclusion: GROW-AI提供了一个综合测试格式，将人类“成长”过程的概念性转置到人工智能领域，整合了心理学、机器人学、计算机科学和伦理学的视角。它不仅衡量AI性能，还能捕捉AI实体迈向成熟的演化路径。

Abstract: This study aims to extend the framework for assessing artificial
intelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),
designed to answer the question "Can machines grow up?" -- a natural successor
to the Turing Test. The methodology applied is based on a system of six primary
criteria (C1-C6), each assessed through a specific "game", divided into four
arenas that explore both the human dimension and its transposition into AI. All
decisions and actions of the entity are recorded in a standardized AI Journal,
the primary source for calculating composite scores. The assessment uses the
prior expert method to establish initial weights, and the global score -- Grow
Up Index -- is calculated as the arithmetic mean of the six scores, with
interpretation on maturity thresholds. The results show that the methodology
allows for a coherent and comparable assessment of the level of "growth" of AI
entities, regardless of their type (robots, software agents, LLMs). The
multi-game structure highlights strengths and vulnerable areas, and the use of
a unified journal guarantees traceability and replicability in the evaluation.
The originality of the work lies in the conceptual transposition of the process
of "growing" from the human world to that of artificial intelligence, in an
integrated testing format that combines perspectives from psychology, robotics,
computer science, and ethics. Through this approach, GROW-AI not only measures
performance but also captures the evolutionary path of an AI entity towards
maturity.

</details>


### [69] [AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications](https://arxiv.org/abs/2508.16279)
*Dawei Gao,Zitao Li,Yuexiang Xie,Weirui Kuang,Liuyi Yao,Bingchen Qian,Zhijian Ma,Yue Cui,Haohao Luo,Shen Li,Lu Yi,Yi Yu,Shiqi He,Zhiling Luo,Wenmeng Zhou,Zhicheng Zhang,Xuguang He,Ziqian Chen,Weikai Liao,Farruh Isakulovich Kushnazarov,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: AgentScope 1.0是一个新版本框架，旨在全面支持基于工具的智能体与环境进行灵活高效的交互，它抽象了核心组件，采用ReAct范式和异步设计，并提供强大的工程支持（如评估模块和运行时沙箱），为构建可扩展、自适应的智能体应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的快速发展，智能体能够结合内在知识与动态工具使用来处理现实世界任务，因此需要一个能够全面支持灵活高效的工具型智能体与环境交互的平台来构建智能体应用。

Method: AgentScope 1.0通过以下方法实现目标：1. 抽象智能体应用的核心组件，提供统一接口和可扩展模块。2. 将智能体行为基于ReAct范式，并采用系统异步设计提供高级智能体级基础设施，以丰富人-智能体和智能体-智能体交互模式，同时提高执行效率。3. 集成针对特定场景的内置智能体。4. 提供健壮的工程支持，包括带可视化界面的可扩展评估模块和确保安全执行的运行时沙箱，并支持快速部署。

Result: AgentScope 1.0通过其各项增强功能，为构建可扩展、自适应和高效的智能体应用提供了一个实用的基础。它简化了利用最新模型和技术（如MCPs）的开发过程，提高了执行效率，并提供了开发友好和安全的开发与部署环境。

Conclusion: AgentScope 1.0作为一个全面的框架，为开发者提供了一个强大、灵活且安全的工具集，以应对快速发展的LLM驱动的智能体应用开发和部署挑战，从而加速了智能体技术在实际场景中的应用。

Abstract: Driven by rapid advancements of Large Language Models (LLMs), agents are
empowered to combine intrinsic knowledge with dynamic tool use, greatly
enhancing their capacity to address real-world tasks. In line with such an
evolution, AgentScope introduces major improvements in a new version (1.0),
towards comprehensively supporting flexible and efficient tool-based
agent-environment interactions for building agentic applications. Specifically,
we abstract foundational components essential for agentic applications and
provide unified interfaces and extensible modules, enabling developers to
easily leverage the latest progress, such as new models and MCPs. Furthermore,
we ground agent behaviors in the ReAct paradigm and offer advanced agent-level
infrastructure based on a systematic asynchronous design, which enriches both
human-agent and agent-agent interaction patterns while improving execution
efficiency. Building on this foundation, we integrate several built-in agents
tailored to specific practical scenarios. AgentScope also includes robust
engineering support for developer-friendly experiences. We provide a scalable
evaluation module with a visual studio interface, making the development of
long-trajectory agentic applications more manageable and easier to trace. In
addition, AgentScope offers a runtime sandbox to ensure safe agent execution
and facilitates rapid deployment in production environments. With these
enhancements, AgentScope provides a practical foundation for building scalable,
adaptive, and effective agentic applications.

</details>


### [70] [Do What? Teaching Vision-Language-Action Models to Reject the Impossible](https://arxiv.org/abs/2508.16292)
*Wen-Han Hsieh,Elvis Hsieh,Dantong Niu,Trevor Darrell,Roei Herzig,David M. Chan*

Main category: cs.AI

TL;DR: 本文提出Instruct-Verify-and-Act (IVA)框架，旨在使视觉-语言-动作(VLA)模型能够识别、解释并响应假前提指令，从而显著提高此类场景下的检测准确性和响应成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型虽然表现强大，但在处理指涉环境中不存在对象或条件的假前提指令时面临挑战，需要更鲁棒地解释用户意图并提供适当反馈。

Method: 本文提出了Instruct-Verify-and-Act (IVA)统一框架，该框架能够(i)检测因假前提无法执行的指令，(ii)进行基于语言的澄清或纠正，以及(iii)在感知和行动中寻找合理替代方案。为此，研究构建了一个大规模指令调优设置和半合成数据集，用以训练VLA模型处理准确和错误请求。

Result: 实验结果显示，IVA在假前提检测准确性方面比基线提高了97.56%，并在假前提场景下将成功响应率提升了50.78%。

Conclusion: IVA框架有效解决了VLA模型处理假前提指令的难题，通过实现鲁棒的检测和自然的语言纠正，显著提升了模型在复杂交互场景中的性能。

Abstract: Recently, Vision-Language-Action (VLA) models have demonstrated strong
performance on a range of robotic tasks. These models rely on multimodal
inputs, with language instructions playing a crucial role -- not only in
predicting actions, but also in robustly interpreting user intent, even when
the requests are impossible to fulfill. In this work, we investigate how VLAs
can recognize, interpret, and respond to false-premise instructions: natural
language commands that reference objects or conditions absent from the
environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that
(i) detects when an instruction cannot be executed due to a false premise, (ii)
engages in language-based clarification or correction, and (iii) grounds
plausible alternatives in perception and action. Towards this end, we construct
a large-scale instruction tuning setup with structured language prompts and
train a VLA model capable of handling both accurate and erroneous requests. Our
approach leverages a contextually augmented, semi-synthetic dataset containing
paired positive and false-premise instructions, enabling robust detection and
natural language correction. Our experiments show that IVA improves false
premise detection accuracy by 97.56% over baselines, while increasing
successful responses in false-premise scenarios by 50.78%.

</details>


### [71] [Causal Beam Selection for Reliable Initial Access in AI-driven Beam Management](https://arxiv.org/abs/2508.16352)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.AI

TL;DR: 本文提出一个因果感知的深度学习框架用于毫米波波束对准，通过两阶段因果选择算法识别最小因果相关特征，显著降低了输入选择时间（94.4%）和波束扫描开销（59.4%），同时保持了与传统方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 毫米波MIMO系统（尤其是6G及未来）迫切需要高效可靠的波束对准。现有基于深度学习的波束对准方法往往忽略输入和输出之间的因果关系，导致可解释性差、泛化能力弱，并带来不必要的波束扫描开销。

Method: 本文提出了一个将因果发现整合到波束管理流程中的因果感知深度学习框架。具体地，提出了一个新颖的两阶段因果波束选择算法来识别波束预测的最小相关输入集。首先，通过因果发现学习一个贝叶斯图，捕获接收功率输入与最佳波束之间的依赖关系；然后，该图指导基于深度学习分类器的因果特征选择。

Result: 模拟结果表明，所提出的因果波束选择方法在性能上与传统方法相匹配。通过仅关注因果相关特征，它将输入选择时间大幅减少了94.4%，并将波束扫描开销降低了59.4%。

Conclusion: 该因果感知深度学习框架通过集成因果发现，能够高效地识别毫米波波束对准中的关键输入，显著降低了输入选择时间和波束扫描开销，同时不牺牲性能，有效解决了现有DL方法存在的问题。

Abstract: Efficient and reliable beam alignment is a critical requirement for mmWave
multiple-input multiple-output (MIMO) systems, especially in 6G and beyond,
where communication must be fast, adaptive, and resilient to real-world
uncertainties. Existing deep learning (DL)-based beam alignment methods often
neglect the underlying causal relationships between inputs and outputs, leading
to limited interpretability, poor generalization, and unnecessary beam sweeping
overhead. In this work, we propose a causally-aware DL framework that
integrates causal discovery into beam management pipeline. Particularly, we
propose a novel two-stage causal beam selection algorithm to identify a minimal
set of relevant inputs for beam prediction. First, causal discovery learns a
Bayesian graph capturing dependencies between received power inputs and the
optimal beam. Then, this graph guides causal feature selection for the DL-based
classifier. Simulation results reveal that the proposed causal beam selection
matches the performance of conventional methods while drastically reducing
input selection time by 94.4% and beam sweeping overhead by 59.4% by focusing
only on causally relevant features.

</details>


### [72] [GLARE: Agentic Reasoning for Legal Judgment Prediction](https://arxiv.org/abs/2508.16383)
*Xinyu Yang,Chenlong Deng,Zhicheng Dou*

Main category: cs.AI

TL;DR: 针对现有LLM在法律判决预测中法律知识不足导致推理受限的问题，本文提出GLARE框架，通过动态获取法律知识来增强推理能力，并提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在法律判决预测（LJP）中因缺乏法律知识而存在推理不足的显著问题。

Method: 提出GLARE，一个智能体法律推理框架，通过调用不同模块动态获取关键法律知识，从而提升推理的广度和深度。

Result: 在真实世界数据集上进行的实验验证了GLARE方法的有效性。此外，分析过程中生成的推理链增加了可解释性。

Conclusion: GLARE框架有效解决了LLMs在LJP中法律知识不足导致的推理问题，提升了模型的推理能力和可解释性，为实际应用提供了可能性。

Abstract: Legal judgment prediction (LJP) has become increasingly important in the
legal field. In this paper, we identify that existing large language models
(LLMs) have significant problems of insufficient reasoning due to a lack of
legal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning
framework that dynamically acquires key legal knowledge by invoking different
modules, thereby improving the breadth and depth of reasoning. Experiments
conducted on the real-world dataset verify the effectiveness of our method.
Furthermore, the reasoning chain generated during the analysis process can
increase interpretability and provide the possibility for practical
applications.

</details>


### [73] [Modular Embedding Recomposition for Incremental Learning](https://arxiv.org/abs/2508.16463)
*Aniello Panariello,Emanuele Frascaroli,Pietro Buzzega,Lorenzo Bonicelli,Angelo Porrello,Simone Calderara*

Main category: cs.AI

TL;DR: 现有持续学习方法主要保留VLM零样本能力。本文提出MoDER，通过模块化文本专家合成精炼原型，将VLM的零样本能力从保留提升至增强，并在14个数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 鉴于VLM在持续学习中的零样本潜力，但当下游任务与预训练领域存在显著差异时仍需微调。现有的CL方法仅侧重于在增量微调过程中“保留”VLM的零样本能力。本研究旨在超越此限制，开发一种能“增强”VLM零样本能力的方法。

Method: 提出MoDular Embedding Recomposition (MoDER) 方法。该方法引入一个模块化框架，为每个已见类别训练专门的文本专家并存储于一个基础枢纽。在推理时，通过查询该枢纽并组合检索到的专家来合成针对未见类别的精炼原型，以改善分类性能。

Result: MoDER方法在Class-IL和MTIL这两种流行的零样本增量协议下，跨越总共14个数据集，均展现出其有效性。

Conclusion: MoDER方法通过将预训练视觉语言模型（VLM）的零样本能力从“保留”提升至“增强”，在持续学习领域取得了显著进展。其模块化专家合成机制有效改善了在处理新颖、未见类别时的分类性能。

Abstract: The advent of pre-trained Vision-Language Models (VLMs) has significantly
transformed Continual Learning (CL), mainly due to their zero-shot
classification abilities. Such proficiency makes VLMs well-suited for
real-world applications, enabling robust performance on novel unseen classes
without requiring adaptation. However, fine-tuning remains essential when
downstream tasks deviate significantly from the pre-training domain. Prior CL
approaches primarily focus on preserving the zero-shot capabilities of VLMs
during incremental fine-tuning on a downstream task. We take a step further by
devising an approach that transforms preservation into enhancement of the
zero-shot capabilities of VLMs. Our approach, named MoDular Embedding
Recomposition (MoDER), introduces a modular framework that trains multiple
textual experts, each specialized in a single seen class, and stores them in a
foundational hub. At inference time, for each unseen class, we query the hub
and compose the retrieved experts to synthesize a refined prototype that
improves classification. We show the effectiveness of our method across two
popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total
of 14 datasets. The codebase is available at
https://github.com/aimagelab/mammoth.

</details>


### [74] [Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning](https://arxiv.org/abs/2508.16524)
*Xuan Zhang,Zhijian Zhou,Weidi Xu,Yanting Miao,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: 提出一种基于扩散模型的两阶段训练方法，结合强化学习，使神经网络学习逻辑约束并进行符号推理，在多项基准测试中取得了卓越的准确性和逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 神经网络在学习复杂逻辑约束和进行符号推理方面面临挑战，需要有效引导神经网络的输出分布以符合符号约束。

Method: 该研究采用基于扩散模型的两阶段训练策略。第一阶段培养基本推理能力，第二阶段侧重系统学习逻辑约束。为在第二阶段施加硬约束，将扩散推理器建模为马尔可夫决策过程，并使用改进的近端策略优化算法进行微调，通过基于逻辑一致性的规则奖励信号来优化策略。

Result: 在数独、迷宫、路径规划和偏好学习等经典符号推理基准测试中，该方法表现出卓越的准确性和逻辑一致性。

Conclusion: 该方法成功地使神经网络学习复杂的逻辑约束并执行符号推理，实现了出色的准确性和逻辑一致性。

Abstract: Enabling neural networks to learn complex logical constraints and fulfill
symbolic reasoning is a critical challenge. Bridging this gap often requires
guiding the neural network's output distribution to move closer to the symbolic
constraints. While diffusion models have shown remarkable generative capability
across various domains, we employ the powerful architecture to perform
neuro-symbolic learning and solve logical puzzles. Our diffusion-based pipeline
adopts a two-stage training strategy: the first stage focuses on cultivating
basic reasoning abilities, while the second emphasizes systematic learning of
logical constraints. To impose hard constraints on neural outputs in the second
stage, we formulate the diffusion reasoner as a Markov decision process and
innovatively fine-tune it with an improved proximal policy optimization
algorithm. We utilize a rule-based reward signal derived from the logical
consistency of neural outputs and adopt a flexible strategy to optimize the
diffusion reasoner's policy. We evaluate our methodology on some classical
symbolic reasoning benchmarks, including Sudoku, Maze, pathfinding and
preference learning. Experimental results demonstrate that our approach
achieves outstanding accuracy and logical consistency among neural networks.

</details>


### [75] [LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence](https://arxiv.org/abs/2508.16571)
*Alisa Vinogradova,Vlad Vinogradov,Dmitrii Radkevich,Ilya Yasny,Dmitry Kobyzev,Ivan Izmailov,Katsiaryna Yanchanka,Andrey Doronichev*

Main category: cs.AI

TL;DR: 本文介绍并基准测试了一个用于药物资产尽职调查的AI竞品发现系统，该系统通过自建评估语料和LLM-as-a-judge代理，实现了高召回率并显著提高了分析效率。


<details>
  <summary>Details</summary>
Motivation: 药物竞品发现数据复杂、分散且缺乏标准，现有基于LLM的AI系统无法可靠地检索所有竞品，且缺乏公开的评估基准。

Method: 开发了一个AI竞品发现组件；将来自私人生物技术风投基金的五年多模态、非结构化尽职调查备忘录转化为结构化评估语料库；引入了一个LLM-as-a-judge代理来过滤假阳性，以提高准确性和抑制幻觉。

Result: 在自建基准测试中，该竞品发现代理实现了83%的召回率，超过了OpenAI Deep Research（65%）和Perplexity Labs（60%）。该系统已投入生产使用，在一个案例研究中，分析师进行竞品分析的周转时间从2.5天缩短至约3小时（约20倍）。

Conclusion: 所开发的竞品发现系统在复杂且数据碎片化的领域表现出色，不仅在召回率上优于其他领先模型，还通过实际部署显著提升了生物技术投资尽职调查的效率。

Abstract: In this paper, we describe and benchmark a competitor-discovery component
used within an agentic AI system for fast drug asset due diligence. A
competitor-discovery AI agent, given an indication, retrieves all drugs
comprising the competitive landscape of that indication and extracts canonical
attributes for these drugs. The competitor definition is investor-specific, and
data is paywalled/licensed, fragmented across registries, ontology-mismatched
by indication, alias-heavy for drug names, multimodal, and rapidly changing.
Although considered the best tool for this problem, the current LLM-based AI
systems aren't capable of reliably retrieving all competing drug names, and
there is no accepted public benchmark for this task. To address the lack of
evaluation, we use LLM-based agents to transform five years of multi-modal,
unstructured diligence memos from a private biotech VC fund into a structured
evaluation corpus mapping indications to competitor drugs with normalized
attributes. We also introduce a competitor validating LLM-as-a-judge agent that
filters out false positives from the list of predicted competitors to maximize
precision and suppress hallucinations. On this benchmark, our
competitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research
(65%) and Perplexity Labs (60%). The system is deployed in production with
enterprise users; in a case study with a biotech VC investment fund, analyst
turnaround time dropped from 2.5 days to $\sim$3 hours ($\sim$20x) for the
competitive analysis.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [76] [Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining](https://arxiv.org/abs/2508.15828)
*Samiul Basir Bhuiyan,Md. Sazzad Hossain Adib,Mohammed Aman Bhuiyan,Muhammad Rafsan Kabir,Moshiur Farazi,Shafin Rahman,Nabeel Mohammed*

Main category: cs.LG

TL;DR: 本文提出Z-Pruner，一种新型训练后剪枝方法，无需再训练即可通过结合权重更新幅度和激活模式，高效地为LLM引入稀疏性，并在性能上超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）尺寸日益增大，给部署、可扩展性和能效带来巨大挑战。现有训练后剪枝方法常常导致显著的性能下降或需要耗费计算资源的微调。

Method: 引入Z-Pruner，一种新颖的训练后剪枝方法。该方法无需任何再训练，通过同时利用权重更新幅度和激活模式来识别和消除冗余参数，从而有效诱导预训练LLM的稀疏性。Z-Pruner具有模型无关性、高效性且易于实现。

Result: Z-Pruner在多种LLM架构（如LLaMA-2, LLaMA-3, OPT）及标准语言基准测试中表现出色，超越了需要大量权重更新的最先进剪枝方法。具体而言，它实现了最低的困惑度分数和最高的零样本准确率总体平均分。

Conclusion: Z-Pruner提供了一种无需再训练即可有效且高效地稀疏化大型语言模型的方法，解决了现有剪枝方法的局限性，并在多项任务上取得了超越SOTA的性能，为LLM的部署和能效提供了新的解决方案。

Abstract: Large language models (LLMs) have rapidly advanced in recent years, achieving
remarkable performance across a wide range of natural language processing
tasks. However, this progress has come at the cost of increasingly large model
sizes, which pose significant challenges for deployment, scalability, and
energy efficiency. To address these limitations, post-training pruning has
emerged as a promising approach for reducing model size and inference latency
without the need for retraining. Despite these advantages, many existing
pruning methods result in substantial performance degradation or require
computationally expensive fine-tuning. In this work, we introduce Z-Pruner, a
novel post-training pruning method designed to induce sparsity in pretrained
LLMs without any retraining. Unlike conventional approaches, Z-Pruner leverages
both weight update magnitudes and activation patterns to identify and eliminate
redundant parameters more effectively. Our method is model-agnostic, efficient,
and easy to implement. We evaluate Z-Pruner using multiple widely-used LLM
architectures, including LLaMA-2, LLaMA-3, and OPT, across a diverse set of
standard language benchmarks. Experimental results demonstrate that Z-Pruner
surpasses state-of-the-art pruning methods that require intensive weight
updates. Specifically, Z-Pruner achieves the lowest perplexity scores and the
highest overall average score for zero-shot accuracy. We have made the
corresponding codes publicly available at
https://github.com/sazzadadib/Z-Pruner.

</details>


### [77] [PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.15852)
*Bin Wen,Tien-Ping Tan*

Main category: cs.LG

TL;DR: PGF-Net是一个高效且可解释的多模态情感分析深度学习框架。它通过渐进式层内融合、自适应门控仲裁和混合PEFT策略，在显著减少参数量的同时，在MOSI数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在多模态情感分析中可能存在融合深度不足、噪声干扰以及参数效率低下的问题。本文旨在开发一个高效、可解释、参数量小的深度学习框架，以解决多模态情感分析中的这些挑战，并适用于资源受限场景。

Method: PGF-Net框架包含三项核心创新：1) **渐进式层内融合范式**，利用跨注意力机制使文本在Transformer深层动态整合音视频特征；2) **自适应门控仲裁机制**，动态平衡语言信息与融合的多模态上下文，确保稳定集成并抑制噪声；3) **混合参数高效微调（PEFT）策略**，结合LoRA和Post-Fusion Adapters以显著减少可训练参数。这些创新被整合到一个分层编码器架构中。

Result: 在MOSI数据集上的实验结果表明，PGF-Net取得了最先进的性能，平均绝对误差（MAE）为0.691，F1-Score为86.9%。更值得注意的是，模型仅使用了3.09M可训练参数，展现了性能与计算效率的卓越平衡。

Conclusion: PGF-Net通过其独特的融合、仲裁和PEFT策略，成功地提供了一个高效、可解释且参数量极少的多模态情感分析解决方案，并在标准数据集上取得了领先的性能，尤其适用于资源受限的环境。

Abstract: We introduce PGF-Net (Progressive Gated-Fusion Network), a novel deep
learning framework designed for efficient and interpretable multimodal
sentiment analysis. Our framework incorporates three primary innovations.
Firstly, we propose a Progressive Intra-Layer Fusion paradigm, where a
Cross-Attention mechanism empowers the textual representation to dynamically
query and integrate non-linguistic features from audio and visual streams
within the deep layers of a Transformer encoder. This enables a deeper,
context-dependent fusion process. Secondly, the model incorporates an Adaptive
Gated Arbitration mechanism, which acts as a dynamic controller to balance the
original linguistic information against the newly fused multimodal context,
ensuring stable and meaningful integration while preventing noise from
overwhelming the signal. Lastly, a hybrid Parameter-Efficient Fine-Tuning
(PEFT) strategy is employed, synergistically combining global adaptation via
LoRA with local refinement through Post-Fusion Adapters. This significantly
reduces trainable parameters, making the model lightweight and suitable for
resource-limited scenarios. These innovations are integrated into a
hierarchical encoder architecture, enabling PGF-Net to perform deep, dynamic,
and interpretable multimodal sentiment analysis while maintaining exceptional
parameter efficiency. Experimental results on MOSI dataset demonstrate that our
proposed PGF-Net achieves state-of-the-art performance, with a Mean Absolute
Error (MAE) of 0.691 and an F1-Score of 86.9%. Notably, our model achieves
these results with only 3.09M trainable parameters, showcasing a superior
balance between performance and computational efficiency.

</details>


### [78] [Physics-Based Explainable AI for ECG Segmentation: A Lightweight Model](https://arxiv.org/abs/2508.15872)
*Muhammad Fathur Rohman Sidiq,Abdurrouf,Didik Rahadi Santoso*

Main category: cs.LG

TL;DR: 本研究提出一种结合频谱分析和概率预测的精简型ECG分割模型，通过简化架构提高计算效率和可解释性，并在P、QRS、T波段实现了高精度分割。


<details>
  <summary>Details</summary>
Motivation: 现有ECG分割模型（如基于BiLSTM）依赖于复杂的多层架构，导致计算量大且效率低下。

Method: 引入一种精简架构，结合频谱分析与概率预测进行ECG信号分割。通过用更简单的层替代复杂层，有效捕获P、QRS、T波的时间和频谱特征。此外，应用可解释人工智能（XAI）方法，结合基于物理的AI原理，增强模型可解释性，阐释时域和频域特征对分割的贡献。

Result: 该方法实现了高分割精度：QRS波97.00%，T波93.33%，P波96.07%。结果表明，精简架构不仅提高了计算效率，也提供了精确的分割。

Conclusion: 所提出的精简架构在提高计算效率的同时，实现了精确的ECG分割，为心脏信号监测提供了一种实用、有效、可靠且透明的解决方案。

Abstract: The heart's electrical activity, recorded through Electrocardiography (ECG),
is essential for diagnosing various cardiovascular conditions. However, many
existing ECG segmentation models rely on complex, multi-layered architectures
such as BiLSTM, which are computationally intensive and inefficient. This study
introduces a streamlined architecture that combines spectral analysis with
probabilistic predictions for ECG signal segmentation. By replacing complex
layers with simpler ones, the model effectively captures both temporal and
spectral features of the P, QRS, and T waves. Additionally, an Explainable AI
(XAI) approach is applied to enhance model interpretability by explaining how
temporal and frequency-based features contribute to ECG segmentation. By
incorporating principles from physics-based AI, this method provides a clear
understanding of the decision-making process, ensuring reliability and
transparency in ECG analysis. This approach achieves high segmentation
accuracy: 97.00% for the QRS wave, 93.33% for the T wave, and 96.07% for the P
wave. These results indicate that the simplified architecture not only improves
computational efficiency but also provides precise segmentation, making it a
practical and effective solution for heart signal monitoring.

</details>


### [79] [TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill \& Decode Inference](https://arxiv.org/abs/2508.15881)
*Xiaojuan Tang,Fanxu Meng,Pingzhi Tang,Yuxuan Wang,Di Yin,Xing Sun,Muhan Zhang*

Main category: cs.LG

TL;DR: TPLA提出一种张量并行潜在注意力方案，通过分区潜在表示和注意力头输入，解决了MLA在张量并行中缓存效率低下的问题，显著提升了长上下文解码速度，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: Multi-Head Latent Attention (MLA)通过压缩KV状态来减少内存，但在张量并行（TP）环境下，每个设备仍需加载完整缓存，从而削弱了MLA相对于GQA的优势。

Method: 提出Tensor-Parallel Latent Attention (TPLA)，该方案将潜在表示和每个头的输入维度跨设备进行分区，每个分片独立执行注意力计算，然后通过all-reduce操作组合结果。TPLA确保每个头仍利用完整的潜在表示以维持表示能力。它与MLA预训练模型兼容，无需重新训练。通过在TP切片前应用简单的正交变换（如Hadamard变换或PCA），可以进一步减轻跨分片干扰，最小化精度下降。该方法可使用FlashAttention-3实现。

Result: TPLA成功减少了DeepSeek-V3和Kimi-K2的每设备KV缓存。在32K-token上下文长度下，DeepSeek-V3和Kimi-K2分别实现了1.79倍和1.93倍的速度提升。同时，在常识和LongBench基准测试中保持了性能，精度下降极小。

Conclusion: TPLA为MLA在张量并行环境下的高效部署提供了解决方案，它通过巧妙的并行化设计，在保持MLA压缩KV缓存优势和模型表示能力的同时，显著提升了长上下文推理速度，并兼容现有模型，为大型语言模型的实际应用提供了重要的加速手段。

Abstract: Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses
key-value states into a low-rank latent vector, caching only this vector to
reduce memory. In tensor parallelism (TP), however, attention heads are
computed across multiple devices, and each device must load the full cache,
eroding the advantage of MLA over Grouped Query Attention (GQA). We propose
Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the
latent representation and each head's input dimension across devices, performs
attention independently per shard, and then combines results with an
all-reduce. TPLA preserves the benefits of a compressed KV cache while
unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in
TPLA still leverages the full latent representation, maintaining stronger
representational capacity. TPLA is drop-in compatible with models pre-trained
using MLA: it supports MLA-style prefilling and enables efficient
tensor-parallel decoding without retraining. Applying simple orthogonal
transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further
mitigates cross-shard interference, yielding minimal accuracy degradation. By
reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x
and 1.93x speedups, respectively, at a 32K-token context length while
maintaining performance on commonsense and LongBench benchmarks. TPLA can be
implemented with FlashAttention-3, enabling practical end-to-end acceleration.

</details>


### [80] [Transforming Causality: Transformer-Based Temporal Causal Discovery with Prior Knowledge Integration](https://arxiv.org/abs/2508.15928)
*Jihua Huang,Yi Yao,Ajay Divakaran*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的新型时序因果发现与推理框架，有效处理复杂非线性依赖和虚假关联。


<details>
  <summary>Details</summary>
Motivation: 解决时序因果发现中复杂的非线性依赖关系和虚假关联两大挑战。

Method: 采用多层Transformer时序预测器捕捉变量间的长程非线性时序关系；通过基于梯度的分析从预测器中提取因果结构和时滞；引入基于注意力掩码的先验知识整合机制，以减少虚假因果关系的影响。

Result: 实验结果显示，该方法显著优于其他SOTA方法，因果发现的F1分数提高了12.8%，因果时滞估计的准确率达到98.9%。

Conclusion: 所提出的框架在处理复杂非线性依赖和虚假关联方面表现出色，并在因果发现和时滞估计上实现了卓越的性能。

Abstract: We introduce a novel framework for temporal causal discovery and inference
that addresses two key challenges: complex nonlinear dependencies and spurious
correlations. Our approach employs a multi-layer Transformer-based time-series
forecaster to capture long-range, nonlinear temporal relationships among
variables. After training, we extract the underlying causal structure and
associated time lags from the forecaster using gradient-based analysis,
enabling the construction of a causal graph. To mitigate the impact of spurious
causal relationships, we introduce a prior knowledge integration mechanism
based on attention masking, which consistently enforces user-excluded causal
links across multiple Transformer layers. Extensive experiments show that our
method significantly outperforms other state-of-the-art approaches, achieving a
12.8% improvement in F1-score for causal discovery and 98.9% accuracy in
estimating causal lags.

</details>


### [81] [Low-dimensional embeddings of high-dimensional data](https://arxiv.org/abs/2508.15929)
*Cyril de Bodt,Alex Diaz-Papkovich,Michael Bleher,Kerstin Bunte,Corinna Coupette,Sebastian Damrich,Enrique Fita Sanmartin,Fred A. Hamprecht,Emőke-Ágnes Horvát,Dhruv Kohli,Smita Krishnaswamy,John A. Lee,Boudewijn P. F. Lelieveldt,Leland McInnes,Ian T. Nabney,Maximilian Noichl,Pavlin G. Poličar,Bastian Rieck,Guy Wolf,Gal Mishne,Dmitry Kobak*

Main category: cs.LG

TL;DR: 对低维嵌入算法领域进行了一次详细且批判性的综述，旨在提供最佳实践、评估流行方法并讨论现有挑战，以提高领域连贯性并指导实践者。


<details>
  <summary>Details</summary>
Motivation: 高维数据广泛存在且处理复杂，对低维表示（嵌入）算法的需求日益增长。然而，该领域碎片化严重，面临技术挑战和根本性争论，导致实践者缺乏有效使用现有方法的明确指导。

Method: 本综述提供了对近期发展详细而批判性的概述，提炼出创建和使用低维嵌入的最佳实践，并在多种数据集上评估了流行方法。

Result: 通过综述，提炼出了一系列创建和使用低维嵌入的最佳实践，并对流行方法进行了评估。同时，也识别并讨论了该领域中剩余的挑战和开放问题。

Conclusion: 该综述旨在增加该领域的连贯性，促进未来的研究工作，并为实践者提供有效使用低维嵌入方法的清晰指导，同时指出了未来的研究方向和未解决的问题。

Abstract: Large collections of high-dimensional data have become nearly ubiquitous
across many academic fields and application domains, ranging from biology to
the humanities. Since working directly with high-dimensional data poses
challenges, the demand for algorithms that create low-dimensional
representations, or embeddings, for data visualization, exploration, and
analysis is now greater than ever. In recent years, numerous embedding
algorithms have been developed, and their usage has become widespread in
research and industry. This surge of interest has resulted in a large and
fragmented research field that faces technical challenges alongside fundamental
debates, and it has left practitioners without clear guidance on how to
effectively employ existing methods. Aiming to increase coherence and
facilitate future work, in this review we provide a detailed and critical
overview of recent developments, derive a list of best practices for creating
and using low-dimensional embeddings, evaluate popular approaches on a variety
of datasets, and discuss the remaining challenges and open problems in the
field.

</details>


### [82] [An Efficient Hybridization of Graph Representation Learning and Metaheuristics for the Constrained Incremental Graph Drawing Problem](https://arxiv.org/abs/2508.15949)
*Bruna C. B. Charytitsch,María C. V. Nascimento*

Main category: cs.LG

TL;DR: 提出了一种名为GL-GRASP的新型混合算法，它将图表示学习（GRL）整合到贪婪随机自适应搜索过程（GRASP）中，用于解决受约束增量图绘制问题（C-IGDP）。实验结果显示，GL-GRASP在性能和可扩展性方面均优于现有最优的GRASP启发式算法。


<details>
  <summary>Details</summary>
Motivation: 将机器学习技术与元启发式算法结合已引起广泛关注，但现有方法（如监督学习或强化学习）常因耗时过长而不如手工启发式算法有竞争力。因此，需要一种开销更小的学习策略，如图表示学习（GRL），来提取图的潜在结构。此外，受约束增量图绘制问题（C-IGDP）相关研究文献有限，但GRASP在该问题上已显示出潜力。

Method: 该论文提出了一种将元启发式算法与图表示学习（GRL）相结合的混合方法。具体而言，GRL被整合到贪婪随机自适应搜索过程（GRASP）的构建阶段，形成Graph Learning GRASP（GL-GRASP）。在计算实验中，首先分析了不同节点嵌入技术的效果，其中深度学习基策略表现突出。评估采用原始积分测度（primal integral measure），该测度综合考虑了解决方案的质量和所需时间。

Result: 在不同的节点嵌入技术比较中，基于深度学习的策略表现最佳。最佳GL-GRASP启发式算法在原始积分测度下，展现出优于现有最先进GRASP启发式算法的性能。此外，对新生成更密集实例进行的固定时间限制下的可扩展性测试，进一步证实了GL-GRASP启发式算法的鲁棒性。

Conclusion: GL-GRASP通过有效地将图表示学习融入到GRASP中，不仅在解决受约束增量图绘制问题上取得了超越现有最先进GRASP方法的性能，而且在鲁棒性和可扩展性方面也表现出色，验证了GRL作为一种经济高效的学习策略在元启发式算法混合中的巨大潜力。

Abstract: Hybridizing machine learning techniques with metaheuristics has attracted
significant attention in recent years. Many attempts employ supervised or
reinforcement learning to support the decision-making of heuristic methods.
However, in some cases, these techniques are deemed too time-consuming and not
competitive with hand-crafted heuristics. This paper proposes a hybridization
between metaheuristics and a less expensive learning strategy to extract the
latent structure of graphs, known as Graph Representation Learning (GRL). For
such, we approach the Constrained Incremental Graph Drawing Problem (C-IGDP), a
hierarchical graph visualization problem. There is limited literature on
methods for this problem, for which Greedy Randomized Search Procedures (GRASP)
heuristics have shown promising results. In line with this, this paper
investigates the gains of incorporating GRL into the construction phase of
GRASP, which we refer to as Graph Learning GRASP (GL-GRASP). In computational
experiments, we first analyze the results achieved considering different node
embedding techniques, where deep learning-based strategies stood out. The
evaluation considered the primal integral measure that assesses the quality of
the solutions according to the required time for such. According to this
measure, the best GL-GRASP heuristics demonstrated superior performance than
state-of-the-art literature GRASP heuristics for the problem. A scalability
test on newly generated denser instances under a fixed time limit further
confirmed the robustness of the GL-GRASP heuristics.

</details>


### [83] [Advancing rail safety: An onboard measurement system of rolling stock wheel flange wear based on dynamic machine learning algorithms](https://arxiv.org/abs/2508.15963)
*Celestin Nkundineza,James Ndodana Njaji,Samrawit Abubeker,Omar Gatera,Damien Hanyurwimfura*

Main category: cs.LG

TL;DR: 本文提出一种创新的车载轮缘磨损深度监测系统，结合位移和温度传感器、动态机器学习算法和IIR滤波器，实现了高精度的实时监测，以提高铁路系统安全和效率。


<details>
  <summary>Details</summary>
Motivation: 铁路车轮与钢轨的交互作用对系统安全至关重要，需要准确的测量系统来优化安全监测操作，尤其是对车轮轮缘磨损深度的准确监测。

Method: 该系统采用位移和温度传感器监测轮缘磨损深度。通过实验室实验模拟磨损和温度波动，收集数据并动态自动化训练基于回归模型的机器学习算法。为增强精度，设计了无限脉冲响应（IIR）滤波器，其参数通过机车仿真和实验数据的快速傅里叶变换（FFT）分析计算，用于消除车辆动态和传感器噪声。

Result: 动态机器学习算法有效应对传感器对温度效应的非线性响应，实现了96.5%的准确率和极短的运行时间。通过IIR滤波器进行实时降噪，将系统准确率提高至98.2%。系统有效性已通过标准程序验证。

Conclusion: 该先进的监测系统可与物联网（IoT）设备等铁路通信嵌入式系统集成，提供关于轮缘磨损和导致其发生的轨道异常状况的实时洞察，从而显著提高铁路系统运行的安全性和效率。

Abstract: Rail and wheel interaction functionality is pivotal to the railway system
safety, requiring accurate measurement systems for optimal safety monitoring
operation. This paper introduces an innovative onboard measurement system for
monitoring wheel flange wear depth, utilizing displacement and temperature
sensors. Laboratory experiments are conducted to emulate wheel flange wear
depth and surrounding temperature fluctuations in different periods of time.
Employing collected data, the training of machine learning algorithms that are
based on regression models, is dynamically automated. Further experimentation
results, using standards procedures, validate the system's efficacy. To enhance
accuracy, an infinite impulse response filter (IIR) that mitigates vehicle
dynamics and sensor noise is designed. Filter parameters were computed based on
specifications derived from a Fast Fourier Transform analysis of locomotive
simulations and emulation experiments data. The results show that the dynamic
machine learning algorithm effectively counter sensor nonlinear response to
temperature effects, achieving an accuracy of 96.5 %, with a minimal runtime.
The real-time noise reduction via IIR filter enhances the accuracy up to 98.2
%. Integrated with railway communication embedded systems such as Internet of
Things devices, this advanced monitoring system offers unparalleled real-time
insights into wheel flange wear and track irregular conditions that cause it,
ensuring heightened safety and efficiency in railway systems operations.

</details>


### [84] [Vector preference-based contextual bandits under distributional shifts](https://arxiv.org/abs/2508.15966)
*Apurv Shukla,P. R. Kumar*

Main category: cs.LG

TL;DR: 本文研究了奖励向量具有偏好锥排序且存在分布偏移的上下文老虎机学习问题。提出了一种自适应离散化和乐观消除策略，并引入了基于偏好的遗憾度量，其遗憾界限在分布偏移下表现良好，并推广了现有结果。


<details>
  <summary>Details</summary>
Motivation: 在奖励向量具有偏好锥排序的上下文老虎机学习中，有效应对分布偏移是一个尚未充分解决的挑战。研究动机在于开发一种能够自适应处理这类分布偏移的策略，并对其性能进行量化评估。

Method: 1. 提出了一种基于自适应离散化和乐观消除的策略，该策略能根据潜在的分布偏移进行自适应调整。2. 引入了“基于偏好的遗憾度量”(preference-based regret)，通过衡量帕累托前沿之间的距离来评估策略性能。3. 在不同的分布偏移假设下，通过建立该策略遗憾的上界来分析其性能。

Result: 1. 成功建立了所提出策略的遗憾上界。2. 这些遗憾界限推广了现有文献中无分布偏移和向量奖励设置的结果。3. 在存在分布偏移的情况下，遗憾界限能随问题参数平稳地扩展。

Conclusion: 本文提出的策略及其遗憾分析为处理带有偏好锥排序和分布偏移的上下文老虎机学习提供了一个有效且鲁棒的解决方案，它不仅泛化了现有研究成果，还在分布偏移场景下展现了良好的适应性。

Abstract: We consider contextual bandit learning under distribution shift when reward
vectors are ordered according to a given preference cone. We propose an
adaptive-discretization and optimistic elimination based policy that self-tunes
to the underlying distribution shift. To measure the performance of this
policy, we introduce the notion of preference-based regret which measures the
performance of a policy in terms of distance between Pareto fronts. We study
the performance of this policy by establishing upper bounds on its regret under
various assumptions on the nature of distribution shift. Our regret bounds
generalize known results for the existing case of no distribution shift and
vectorial reward settings, and scale gracefully with problem parameters in
presence of distribution shifts.

</details>


### [85] [Scalable Equilibrium Propagation via Intermediate Error Signals for Deep Convolutional CRNNs](https://arxiv.org/abs/2508.15989)
*Jiaqi Lin,Malyaban Bal,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 平衡传播(EP)在深度网络中面临梯度消失问题。本文提出一种新颖的EP框架，通过整合中间误差信号和知识蒸馏来增强信息流和收敛性，成功训练了深度EP网络，并在CIFAR数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的平衡传播(EP)方法在深度网络中面临梯度消失问题，导致能量最小化和梯度计算难以收敛，限制了其在浅层架构中的应用。

Method: 提出一种新颖的EP框架，该框架融入了中间误差信号以增强信息流和神经元动力学的收敛性。这是首次将知识蒸馏和局部误差信号整合到EP中，从而实现对更深层次架构的训练。

Result: 该方法成功实现了对显著更深层次架构的训练。在CIFAR-10和CIFAR-100数据集上取得了最先进的性能，并展示了在深度VGG架构上的可扩展性。

Conclusion: 本研究显著提升了EP的可扩展性，为EP在真实世界系统中的应用铺平了道路。

Abstract: Equilibrium Propagation (EP) is a biologically inspired local learning rule
first proposed for convergent recurrent neural networks (CRNNs), in which
synaptic updates depend only on neuron states from two distinct phases. EP
estimates gradients that closely align with those computed by Backpropagation
Through Time (BPTT) while significantly reducing computational demands,
positioning it as a potential candidate for on-chip training in neuromorphic
architectures. However, prior studies on EP have been constrained to shallow
architectures, as deeper networks suffer from the vanishing gradient problem,
leading to convergence difficulties in both energy minimization and gradient
computation. To address the vanishing gradient problem in deep EP networks, we
propose a novel EP framework that incorporates intermediate error signals to
enhance information flow and convergence of neuron dynamics. This is the first
work to integrate knowledge distillation and local error signals into EP,
enabling the training of significantly deeper architectures. Our proposed
approach achieves state-of-the-art performance on the CIFAR-10 and CIFAR-100
datasets, showcasing its scalability on deep VGG architectures. These results
represent a significant advancement in the scalability of EP, paving the way
for its application in real-world systems.

</details>


### [86] [Quantum Federated Learning: A Comprehensive Survey](https://arxiv.org/abs/2508.15998)
*Dinh C. Nguyen,Md Raihan Uddin,Shaba Shaon,Ratun Rahman,Octavia Dobre,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文对量子联邦学习（QFL）进行了全面综述，涵盖其核心概念、基础原理、应用领域、相关框架与平台，并讨论了现有挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 量子联邦学习（QFL）结合了分布式量子计算和联邦机器学习的优势，旨在通过量子增强能力实现隐私保护的去中心化学习，以应对分布式量子系统中高效安全模型训练的挑战。

Method: 本文采用综述研究方法。首先介绍QFL的最新进展、市场机遇和背景知识；接着探讨其集成动机和工作原理；随后回顾QFL的基础理论和分类，包括联邦架构、网络拓扑、通信方案、优化技术和安全机制；进而调查QFL在多个领域的应用；分析相关框架、平台和原型实现，并提供详细案例研究；最后总结关键见解、经验教训、当前挑战并展望未来研究方向。

Result: 本综述提供了对QFL的关键见解和经验教训，分析了其原型实现和案例研究，并明确指出了该领域当前面临的挑战和潜在的未来研究方向。

Conclusion: 量子联邦学习是一个快速发展的有前景的领域，本文通过全面综述深入探讨了其方方面面，为理解其当前状态、识别关键挑战以及规划未来研究路径提供了重要参考。

Abstract: Quantum federated learning (QFL) is a combination of distributed quantum
computing and federated machine learning, integrating the strengths of both to
enable privacy-preserving decentralized learning with quantum-enhanced
capabilities. It appears as a promising approach for addressing challenges in
efficient and secure model training across distributed quantum systems. This
paper presents a comprehensive survey on QFL, exploring its key concepts,
fundamentals, applications, and emerging challenges in this rapidly developing
field. Specifically, we begin with an introduction to the recent advancements
of QFL, followed by discussion on its market opportunity and background
knowledge. We then discuss the motivation behind the integration of quantum
computing and federated learning, highlighting its working principle. Moreover,
we review the fundamentals of QFL and its taxonomy. Particularly, we explore
federation architecture, networking topology, communication schemes,
optimization techniques, and security mechanisms within QFL frameworks.
Furthermore, we investigate applications of QFL across several domains which
include vehicular networks, healthcare networks, satellite networks, metaverse,
and network security. Additionally, we analyze frameworks and platforms related
to QFL, delving into its prototype implementations, and provide a detailed case
study. Key insights and lessons learned from this review of QFL are also
highlighted. We complete the survey by identifying current challenges and
outlining potential avenues for future research in this rapidly advancing
field.

</details>


### [87] [Tessellation Groups, Harmonic Analysis on Non-compact Symmetric Spaces and the Heat Kernel in view of Cartan Convolutional Neural Networks](https://arxiv.org/abs/2508.16015)
*Pietro Fré,Federico Milanesio,Marcelo Oyarzo,Matteo Santoro,Mario Trigiante*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we continue the development of the Cartan neural networks
programme, launched with three previous publications, by focusing on some
mathematical foundational aspects that we deem necessary for our next steps
forward. The mathematical and conceptual results are diverse and span various
mathematical fields, but the inspiring motivation is unified. The aim is to
introduce layers that are mathematically modeled as non-compact symmetric
spaces, each mapped onto the next one by solvable group homomorphisms. In
particular, in the spirit of Convolutional neural networks, we have introduced
the notion of Tits Satake (TS) vector bundles where the TS submanifold is the
base space. Within this framework, the tiling of the base manifold, the
representation of bundle sections using harmonics, and the need for a general
theory of separator walls motivated a series of mathematical investigations
that produced both definite and partial results. Specifically, we present the
group theoretical construction of the separators for all non-compact symmetric
spaces $\mathrm{U/H}$, as well as of the $\Delta_{8,3,2}$ tiling group and its
normal Fuchsian subgroups, respectively yielding the uniformization of the
genus $g=3$ Fermat Quartic and of the genus $g=2$ Bolza surface. The quotient
automorphic groups are studied. Furthermore, we found a new representation of
the Laplacian Green function and the Heat Kernel on Hyperbolic Spaces
$\mathbb{H}^{n}$, and a setup for the construction of the harmonic functions in
terms of the spinor representation of pseudo-orthogonal groups. Finally, to
obtain an explicit construction of the Laplacian eigenfunctions on the Bolza
Riemann surface, we propose and conjecture a new strategy relying on the
Abel-Jacobi map of the Riemann surface to its Jacobian variety and the Siegel
Theta function.

</details>


### [88] [Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services](https://arxiv.org/abs/2508.16037)
*Renxuan Tan,Rongpeng Li,Xiaoxue Yu,Xianfu Chen,Xing Xu,Zhifeng Zhao*

Main category: cs.LG

TL;DR: PAC-MCoFL是一个博弈论多智能体强化学习框架，用于解决多服务提供商联邦学习中资源分配和客户指派的非合作问题，通过优化实现帕累托最优平衡，并显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 多服务提供商(SP)生态系统中的联邦学习(FL)面临非合作动态的根本性阻碍，隐私约束和竞争利益使得多SP通信与计算资源无法进行集中优化。

Method: 引入PAC-MCoFL，一个博弈论多智能体强化学习(MARL)框架，其中SP作为智能体共同优化客户指派、自适应量化和资源分配。该框架将帕累托Actor-Critic (PAC)原理与期望分位数回归相结合，以推测帕累托最优联合策略并建模异构风险。通过三元笛卡尔分解(TCAD)机制管理高维动作空间，并提出了可伸缩变体PAC-MCoFL-p以降低计算复杂度。

Result: PAC-MCoFL在总奖励和超体积指标(HVI)方面分别比最新MARL解决方案提高了约5.8%和4.2%。该方法在规模化部署和多样数据异构性下，能更有效地平衡个体SP和系统性能，并提供理论收敛保证。

Conclusion: 所提出的PAC-MCoFL框架有效解决了多服务提供商联邦学习中的非合作资源优化问题，通过实现帕累托最优均衡，显著提升了总体性能并更好地平衡了个体与系统表现。

Abstract: Federated learning (FL) in multi-service provider (SP) ecosystems is
fundamentally hampered by non-cooperative dynamics, where privacy constraints
and competing interests preclude the centralized optimization of multi-SP
communication and computation resources. In this paper, we introduce PAC-MCoFL,
a game-theoretic multi-agent reinforcement learning (MARL) framework where SPs
act as agents to jointly optimize client assignment, adaptive quantization, and
resource allocation. Within the framework, we integrate Pareto Actor-Critic
(PAC) principles with expectile regression, enabling agents to conjecture
optimal joint policies to achieve Pareto-optimal equilibria while modeling
heterogeneous risk profiles. To manage the high-dimensional action space, we
devise a ternary Cartesian decomposition (TCAD) mechanism that facilitates
fine-grained control. Further, we develop PAC-MCoFL-p, a scalable variant
featuring a parameterized conjecture generator that substantially reduces
computational complexity with a provably bounded error. Alongside theoretical
convergence guarantees, our framework's superiority is validated through
extensive simulations -- PAC-MCoFL achieves approximately 5.8% and 4.2%
improvements in total reward and hypervolume indicator (HVI), respectively,
over the latest MARL solutions. The results also demonstrate that our method
can more effectively balance individual SP and system performance in scaled
deployments and under diverse data heterogeneity.

</details>


### [89] [A State-Space Approach to Nonstationary Discriminant Analysis](https://arxiv.org/abs/2508.16073)
*Shuilian Xie,Mahdi Imani,Edward R. Dougherty,Ulisses M. Braga-Neto*

Main category: cs.LG

TL;DR: 本文提出了一种基于状态空间模型的非平稳判别分析框架（NSLDA和NSQDA），用以解决类别条件分布随时间漂移导致传统静态分类器失效的问题。


<details>
  <summary>Details</summary>
Motivation: 传统判别分析假设训练数据同分布，但在许多实际应用中，观测数据随时间收集，导致类别条件分布发生漂移，使得静态分类器（如LDA、QDA）变得不可靠。

Method: 本文提出了一个基于状态空间模型的框架，将判别分析嵌入其中以实现非平稳线性判别分析（NSLDA）和非平稳二次判别分析（NSQDA）。对于线性高斯动态，通过改进卡尔曼平滑处理每时间步多样本，并开发了两种扩展：(i) 期望最大化（EM）方法联合估计未知系统参数；(ii) GMM-Kalman方法同时恢复未观测的时间标签和参数。对于非线性或非高斯漂移，则采用粒子平滑来估计时变类别质心。

Result: 广泛的仿真实验表明，所提出的NSLDA和NSQDA方法在性能上持续优于静态LDA、QDA和SVM基线，并对噪声、缺失数据和类别不平衡表现出良好的鲁棒性。

Conclusion: 本文为在时间分布漂移情境下的判别分析建立了一个统一且数据高效的基础，提供了一种处理非平稳数据的有效途径。

Abstract: Classical discriminant analysis assumes identically distributed training
data, yet in many applications observations are collected over time and the
class-conditional distributions drift. This population drift renders stationary
classifiers unreliable. We propose a principled, model-based framework that
embeds discriminant analysis within state-space models to obtain nonstationary
linear discriminant analysis (NSLDA) and nonstationary quadratic discriminant
analysis (NSQDA). For linear-Gaussian dynamics, we adapt Kalman smoothing to
handle multiple samples per time step and develop two practical extensions: (i)
an expectation-maximization (EM) approach that jointly estimates unknown system
parameters, and (ii) a Gaussian mixture model (GMM)-Kalman method that
simultaneously recovers unobserved time labels and parameters, a scenario
common in practice. To address nonlinear or non-Gaussian drift, we employ
particle smoothing to estimate time-varying class centroids, yielding fully
nonstationary discriminant rules. Extensive simulations demonstrate consistent
improvements over stationary linear discriminant analysis (LDA), quadratic
discriminant analysis (QDA), and support vector machine (SVM) baselines, with
robustness to noise, missing data, and class imbalance. This paper establishes
a unified and data-efficient foundation for discriminant analysis under
temporal distribution shift.

</details>


### [90] [On Task Vectors and Gradients](https://arxiv.org/abs/2508.16082)
*Luca Zhou,Daniele Solombrino,Donato Crisostomi,Maria Sofia Bucarelli,Giuseppe Alessio D'Inverno,Fabrizio Silvestri,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 本文通过将任务向量与任务损失梯度关联，为任务算术提供了理论基础，证明了单epoch微调任务向量与负梯度精确等价，多epoch近似等价，并发现第一轮梯度是关键，使得仅微调一轮的模型合并也能达到良好效果。


<details>
  <summary>Details</summary>
Motivation: 任务算术在模型合并中表现出经验上的成功，但其工作原理和适用条件缺乏清晰的理论解释。

Method: 通过建立任务向量与任务损失梯度之间的联系来提供理论基础。证明了在标准梯度下降下，单epoch微调的任务向量与损失的负梯度精确等价。对于多epoch设置，证明了近似等价，并明确限定了前馈网络的二阶误差项。通过在七个视觉基准测试上进行实证分析来验证理论。

Result: 理论上，任务向量与任务损失的负梯度（单epoch）精确等价，多epoch近似等价。实证分析表明，第一轮（epoch）梯度在范数和方向上主导了微调轨迹。一个关键发现是，仅微调一个epoch的模型进行合并，其性能通常与合并完全收敛的模型相当。

Conclusion: 本研究将任务算术重新定义为一种近似多任务学习形式，为其有效性提供了明确的理论依据，并强调了早期训练动态在模型合并中的关键作用。

Abstract: Task arithmetic has emerged as a simple yet powerful technique for model
merging, enabling the combination of multiple finetuned models into one.
Despite its empirical success, a clear theoretical explanation of why and when
it works is lacking. This paper provides a rigorous theoretical foundation for
task arithmetic by establishing a connection between task vectors and gradients
of the task losses. We show that under standard gradient descent, a task vector
generated from one epoch of finetuning is exactly equivalent to the negative
gradient of the loss, scaled by the learning rate. For the practical
multi-epoch setting, we prove that this equivalence holds approximately, with a
second-order error term that we explicitly bound for feed-forward networks. Our
empirical analysis across seven vision benchmarks corroborates our theory,
demonstrating that the first-epoch gradient dominates the finetuning trajectory
in both norm and direction. A key implication is that merging models finetuned
for only a single epoch often yields performance comparable to merging fully
converged models. These findings reframe task arithmetic as a form of
approximate multitask learning, providing a clear rationale for its
effectiveness and highlighting the critical role of early training dynamics in
model merging.

</details>


### [91] [GPLight+: A Genetic Programming Method for Learning Symmetric Traffic Signal Control Policy](https://arxiv.org/abs/2508.16090)
*Xiao-Cheng Liao,Yi Mei,Mengjie Zhang*

Main category: cs.LG

TL;DR: 本文提出一种基于遗传编程(GP)的对称相位紧急函数，用于交通信号控制。该函数通过共享子树聚合表示，解决了传统GP方法处理不同相位交通特征不一致的问题，显著提高了控制策略的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前基于遗传编程(GP)的交通信号控制方法在进化相位紧急函数时，无法一致地处理不同交通信号相位的共同交通特征，这可能限制了其性能。

Method: 本文提出使用一种对称相位紧急函数来计算特定相位的紧急程度，该函数基于当前路况，并被表示为两个共享子树的聚合，每个子树代表该相位中一次转弯动作的紧急程度。随后，提出一种GP方法来进化这种对称相位紧急函数。该方法在CityFlow交通模拟器上，基于多个公共真实世界数据集进行评估。

Result: 实验结果表明，所提出的对称紧急函数表示方法在多种场景下，能显著提高学习到的交通信号控制策略的性能，优于传统的GP表示。进一步分析显示，该方法能够进化出有效、人类可理解且易于部署的交通信号控制策略。

Conclusion: 通过引入对称相位紧急函数，本文解决了传统GP方法在交通信号控制中处理不同相位特征不一致的问题，成功进化出性能优越、可理解且易于部署的交通信号控制策略，实现了显著的性能提升。

Abstract: Recently, learning-based approaches, have achieved significant success in
automatically devising effective traffic signal control strategies. In
particular, as a powerful evolutionary machine learning approach, Genetic
Programming (GP) is utilized to evolve human-understandable phase urgency
functions to measure the urgency of activating a green light for a specific
phase. However, current GP-based methods are unable to treat the common traffic
features of different traffic signal phases consistently. To address this
issue, we propose to use a symmetric phase urgency function to calculate the
phase urgency for a specific phase based on the current road conditions. This
is represented as an aggregation of two shared subtrees, each representing the
urgency of a turn movement in the phase. We then propose a GP method to evolve
the symmetric phase urgency function. We evaluate our proposed method on the
well-known cityflow traffic simulator, based on multiple public real-world
datasets. The experimental results show that the proposed symmetric urgency
function representation can significantly improve the performance of the
learned traffic signal control policies over the traditional GP representation
on a wide range of scenarios. Further analysis shows that the proposed method
can evolve effective, human-understandable and easily deployable traffic signal
control policies.

</details>


### [92] [Machine Learning for Medicine Must Be Interpretable, Shareable, Reproducible and Accountable by Design](https://arxiv.org/abs/2508.16097)
*Ayyüce Begüm Bektaş,Mithat Gönen*

Main category: cs.LG

TL;DR: 本文主张医疗领域机器学习模型需具备可解释性、可共享性、可复现性和可问责性，并提出实现这些特性的方法，以构建可信赖的医疗AI。


<details>
  <summary>Details</summary>
Motivation: 高风险医疗领域的“黑箱”机器学习模型，尽管准确性高，但因缺乏透明度难以获得信任和监管批准。急需开发透明、值得信赖且能实际应用的医疗AI。

Method: 1. **可解释性：** 采用本质上可解释的模型（如稀疏核方法、原型学习、深度核模型）。2. **可问责性：** 实施严格评估、公平性考量和不确定性量化。3. **可复现性与可共享性：** 利用生成式AI和协同学习范式（如联邦学习、扩散合成数据）实现隐私保护下的数据整合。

Result: 通过将可解释性、可共享性、可复现性和可问责性融入机器学习基础设计，能够开发出不仅准确，而且透明、值得信赖并可转化为实际临床应用的医疗AI。

Conclusion: 重新思考机器学习基础，使其具备可解释性、可共享性、可复现性和可问责性，是开发能可靠支持临床决策的医疗AI的关键。

Abstract: This paper claims that machine learning models deployed in high stakes
domains such as medicine must be interpretable, shareable, reproducible and
accountable. We argue that these principles should form the foundational design
criteria for machine learning algorithms dealing with critical medical data,
including survival analysis and risk prediction tasks. Black box models, while
often highly accurate, struggle to gain trust and regulatory approval in health
care due to a lack of transparency. We discuss how intrinsically interpretable
modeling approaches (such as kernel methods with sparsity, prototype-based
learning, and deep kernel models) can serve as powerful alternatives to opaque
deep networks, providing insight into biomedical predictions. We then examine
accountability in model development, calling for rigorous evaluation, fairness,
and uncertainty quantification to ensure models reliably support clinical
decisions. Finally, we explore how generative AI and collaborative learning
paradigms (such as federated learning and diffusion-based data synthesis)
enable reproducible research and cross-institutional integration of
heterogeneous biomedical data without compromising privacy, hence shareability.
By rethinking machine learning foundations along these axes, we can develop
medical AI that is not only accurate but also transparent, trustworthy, and
translatable to real-world clinical settings.

</details>


### [93] [CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing](https://arxiv.org/abs/2508.16134)
*Yixuan Wang,Haoyu Qiao,Lujun Li,Qingfu Zhu,Wanxiang Che*

Main category: cs.LG

TL;DR: CommonKV是一种无需训练的KV缓存压缩方法，通过SVD实现相邻参数权重共享和自适应预算分配，显著优于现有方法，并能与其他技术结合实现高达98%的压缩率而无明显性能损失。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）因序列长度增加导致KV缓存膨胀，面临严重的内存挑战。现有跨层KV缓存共享方法要么需要修改模型架构并重新预训练，要么在高压缩率下性能显著下降。

Method: 提出CommonKV，一种无需训练的跨层KV缓存压缩方法。其核心思想是利用跨层隐藏状态的高度相似性，通过奇异值分解（SVD）实现相邻参数的权重共享，生成更易合并的潜在KV缓存。此外，引入自适应预算分配策略，根据余弦相似度动态分配压缩预算，避免对不相似缓存过度压缩。

Result: 在多个骨干模型及LongBench和Ruler等基准测试中，CommonKV在不同压缩率下均持续优于现有低秩和跨层方法。同时，该方法的优势与量化和驱逐等其他方法正交。通过集成这些方法，最终可在不显著降低性能的情况下实现98%的压缩比。

Conclusion: CommonKV为LLMs提供了一种高效且无需训练的KV缓存压缩解决方案，其性能优越性已得到验证，并能与其他现有技术结合，大幅提高压缩率同时保持模型性能。

Abstract: Large Language Models (LLMs) confront significant memory challenges due to
the escalating KV cache with increasing sequence length. As a crucial
technique, existing cross-layer KV cache sharing methods either necessitate
modified model architectures with subsequent pre-training or incur significant
performance degradation at high compression rates. To mitigate these
challenges, we propose CommonKV, a training-free method for cross-layer KV
cache compression through adjacent parameters sharing. Inspired by the high
similarity observed in cross-layer hidden states, we utilize Singular Value
Decomposition (SVD) to achieve weight sharing across adjacent parameters,
resulting in a more easily mergeable latent KV cache. Furthermore, we also
introduce an adaptive budget allocation strategy. It dynamically assigns
compression budgets based on cosine similarity, ensuring that dissimilar caches
are not over-compressed. Experiments across multiple backbone models and
benchmarks including LongBench and Ruler demonstrate that the proposed method
consistently outperforms existing low-rank and cross-layer approaches at
various compression ratios. Moreover, we find that the benefits of CommonKV are
orthogonal to other quantization and eviction methods. By integrating these
approaches, we can ultimately achieve a 98\% compression ratio without
significant performance loss.

</details>


### [94] [Machine Learning in Micromobility: A Systematic Review of Datasets, Techniques, and Applications](https://arxiv.org/abs/2508.16135)
*Sen Yan,Chinmaya Kaundanya,Noel E. O'Connor,Suzanne Little,Mingming Liu*

Main category: cs.LG

TL;DR: 该综述论文全面回顾了机器学习在微出行系统中的应用，包括数据集、技术和具体应用，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 微出行系统在解决城市交通问题中扮演重要角色，但现有文献在机器学习应用于微出行系统的具体问题上存在不足，本研究旨在填补这一空白。

Method: 本文通过收集和分析微出行相关数据集（从空间、时间、特征维度），详细概述了应用于微出行的机器学习模型（介绍其优缺点和用例），并探讨了需求预测、能源管理、安全等多个机器学习应用。

Result: 本研究系统地梳理了微出行领域的机器学习数据集、技术和应用案例，旨在提升效率、准确性和用户体验。

Conclusion: 论文提出了未来的研究方向，以帮助研究人员更好地理解微出行与机器学习结合的领域。

Abstract: Micromobility systems, which include lightweight and low-speed vehicles such
as bicycles, e-bikes, and e-scooters, have become an important part of urban
transportation and are used to solve problems such as traffic congestion, air
pollution, and high transportation costs. Successful utilisation of
micromobilities requires optimisation of complex systems for efficiency,
environmental impact mitigation, and overcoming technical challenges for user
safety. Machine Learning (ML) methods have been crucial to support these
advancements and to address their unique challenges. However, there is
insufficient literature addressing the specific issues of ML applications in
micromobilities. This survey paper addresses this gap by providing a
comprehensive review of datasets, ML techniques, and their specific
applications in micromobilities. Specifically, we collect and analyse various
micromobility-related datasets and discuss them in terms of spatial, temporal,
and feature-based characteristics. In addition, we provide a detailed overview
of ML models applied in micromobilities, introducing their advantages,
challenges, and specific use cases. Furthermore, we explore multiple ML
applications, such as demand prediction, energy management, and safety,
focusing on improving efficiency, accuracy, and user experience. Finally, we
propose future research directions to address these issues, aiming to help
future researchers better understand this field.

</details>


### [95] [AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs](https://arxiv.org/abs/2508.16153)
*Huichi Zhou,Yihang Chen,Siyuan Guo,Xue Yan,Kin Hei Lee,Zihan Wang,Ka Yiu Lee,Guchun Zhang,Kun Shao,Linyi Yang,Jun Wang*

Main category: cs.LG

TL;DR: 本文提出一种新颖的自适应大型语言模型（LLM）代理学习范式，通过基于记忆的在线强化学习实现低成本持续适应，无需对底层LLM进行微调。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理方法要么过于僵化（依赖静态、手工设计的反射工作流），要么计算成本高昂（需要对LLM模型参数进行梯度更新），难以实现高效且持续的适应性。

Method: 我们提出了一种记忆增强马尔可夫决策过程（M-MDP），配备神经案例选择策略以指导行动决策。通过将过往经验存储在情节记忆中（可微分或非参数），并基于环境反馈通过记忆重写机制持续更新策略，同时利用高效的记忆读取（检索）实现策略改进。该方法通过记忆型在线强化学习进行低成本持续适应。

Result: 我们将该代理模型实例化为AgentFly，在GAIA验证集上达到87.88%的Pass@3，在测试集上达到79.40%。在DeepResearcher数据集上，F1值达到66.6%，PM达到80.4%，优于现有基于训练的SOTA方法。此外，基于案例的记忆在域外（OOD）任务上带来了4.7%至9.6%的绝对提升。

Conclusion: 我们的方法为开发能够持续、实时学习且无需梯度更新的通用LLM代理提供了一个可扩展且高效的途径，推动机器学习在开放式技能获取和深度研究场景中的发展。

Abstract: In this paper, we introduce a novel learning paradigm for adaptive Large
Language Model (LLM) agents that eliminates the need for fine-tuning the
underlying LLMs. Existing approaches are often either rigid, relying on static,
handcrafted reflection workflows, or computationally intensive, requiring
gradient updates of LLM model parameters. In contrast, our method enables
low-cost continual adaptation via memory-based online reinforcement learning.
We formalise this as a Memory-augmented Markov Decision Process (M-MDP),
equipped with a neural case-selection policy to guide action decisions. Past
experiences are stored in an episodic memory, either differentiable or
non-parametric. The policy is continually updated based on environmental
feedback through a memory rewriting mechanism, whereas policy improvement is
achieved through efficient memory reading (retrieval). We instantiate our agent
model in the deep research setting, namely AgentFly, which attains top-1 on
GAIA validation ($87.88\%$ Pass@$3$) and $79.40\%$ on the test set. It reaches
$66.6\%$ F1 and $80.4\%$ PM on the DeepResearcher dataset, outperforming the
state-of-the-art training-based method, while case-based memory adds $4.7\%$ to
$9.6\%$ absolute points on out-of-distribution tasks. Our approach offers a
scalable and efficient pathway for developing generalist LLM agents capable of
continuous, real-time learning without gradient updates, advancing machine
learning towards open-ended skill acquisition and deep research scenarios. The
code is available at https://github.com/Agent-on-the-Fly/AgentFly.

</details>


### [96] [On the Collapse Errors Induced by the Deterministic Sampler for Diffusion Models](https://arxiv.org/abs/2508.16154)
*Yi Zhang,Zhenyu Liao,Jingfeng Wu,Difan Zou*

Main category: cs.LG

TL;DR: 本文揭示了ODE扩散采样中一种未识别的“崩溃错误”现象，即采样数据过度集中，并探究了其由得分学习在不同噪声区域的“跷跷板效应”引起的根本原因，提出了量化方法并提供了实证支持。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型中确定性采样器被广泛采用，但其潜在局限性仍未被充分探索。本研究旨在识别并深入分析ODE基扩散采样中一种未被识别的缺陷。

Method: 1. 引入新度量标准以量化崩溃错误。 2. 通过观察得分学习在不同噪声区域的“跷跷板效应”来调查其根本原因。 3. 应用现有采样、训练和架构技术进行实证验证。

Result: 1. 识别并命名了ODE基扩散采样中的“崩溃错误”（采样数据过度集中）。 2. 证明了崩溃错误在多种设置下普遍存在。 3. 发现其根本原因在于得分学习在高低噪声区域间的“跷跷板效应”导致的在高噪声区域的失配。 4. 提供了崩溃错误的强有力实证证据。

Conclusion: 本研究提供了ODE基扩散采样中崩溃错误的实证证据及其成因解释，强调了未来需深入探索得分学习与确定性采样之间相互作用的重要性，这是扩散模型中一个被忽视但基础的方面。

Abstract: Despite the widespread adoption of deterministic samplers in diffusion models
(DMs), their potential limitations remain largely unexplored. In this paper, we
identify collapse errors, a previously unrecognized phenomenon in ODE-based
diffusion sampling, where the sampled data is overly concentrated in local data
space. To quantify this effect, we introduce a novel metric and demonstrate
that collapse errors occur across a variety of settings. When investigating its
underlying causes, we observe a see-saw effect, where score learning in low
noise regimes adversely impacts the one in high noise regimes. This misfitting
in high noise regimes, coupled with the dynamics of deterministic samplers,
ultimately causes collapse errors. Guided by these insights, we apply existing
techniques from sampling, training, and architecture to empirically support our
explanation of collapse errors. This work provides intensive empirical evidence
of collapse errors in ODE-based diffusion sampling, emphasizing the need for
further research into the interplay between score learning and deterministic
sampling, an overlooked yet fundamental aspect of diffusion models.

</details>


### [97] [STA-GANN: A Valid and Generalizable Spatio-Temporal Kriging Approach](https://arxiv.org/abs/2508.16161)
*Yujie Li,Zezhi Shao,Chengqing Yu,Tangwen Qian,Zhao Zhang,Yifan Du,Shaoming He,Fei Wang,Yongjun Xu*

Main category: cs.LG

TL;DR: 针对时空任务中传感器数据缺失导致的数据不完整问题，现有克里金模型在模式有效性和泛化性上表现不足。本文提出STA-GANN，一个基于GNN的克里金框架，通过集成解耦相位模块、动态数据驱动元数据图建模和对抗迁移学习策略，显著提升了时空模式的有效性和泛化性，并在多项实验中表现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 时空任务中因传感器缺失导致数据不完整，现有克里金模型在推断时空模式时，难以保证其有效性和泛化性，尤其在捕捉动态空间依赖、时间偏移以及优化对未知传感器的泛化能力方面存在不足。

Method: 提出Spatio-Temporal Aware Graph Adversarial Neural Network (STA-GANN)，一个基于GNN的克里金框架。该方法整合了：(i) 解耦相位模块，用于感知和调整时间戳偏移。(ii) 动态数据驱动元数据图建模，利用时序数据和元数据更新空间关系。(iii) 对抗迁移学习策略，以确保泛化能力。

Result: 通过在四个领域九个数据集上的广泛验证以及理论证据，均表明STA-GANN展现出卓越的性能。

Conclusion: STA-GANN通过其创新的框架成功克服了现有模型在时空模式有效性和泛化性方面的局限，为时空数据插补提供了一种性能卓越且鲁棒的解决方案。

Abstract: Spatio-temporal tasks often encounter incomplete data arising from missing or
inaccessible sensors, making spatio-temporal kriging crucial for inferring the
completely missing temporal information. However, current models struggle with
ensuring the validity and generalizability of inferred spatio-temporal
patterns, especially in capturing dynamic spatial dependencies and temporal
shifts, and optimizing the generalizability of unknown sensors. To overcome
these limitations, we propose Spatio-Temporal Aware Graph Adversarial Neural
Network (STA-GANN), a novel GNN-based kriging framework that improves
spatio-temporal pattern validity and generalization. STA-GANN integrates (i)
Decoupled Phase Module that senses and adjusts for timestamp shifts. (ii)
Dynamic Data-Driven Metadata Graph Modeling to update spatial relationships
using temporal data and metadata; (iii) An adversarial transfer learning
strategy to ensure generalizability. Extensive validation across nine datasets
from four fields and theoretical evidence both demonstrate the superior
performance of STA-GANN.

</details>


### [98] [SPL-LNS: Sampling-Enhanced Large Neighborhood Search for Solving Integer Linear Programs](https://arxiv.org/abs/2508.16171)
*Shengyu Feng,Zhiqing Sun,Yiming Yang*

Main category: cs.LG

TL;DR: 本文提出SPL-LNS，一种采样增强型神经LNS求解器，通过将LNS公式化为随机过程并引入事后重标记方法，解决了现有神经LNS的局部最优和样本效率问题，并在ILP问题上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的LNS求解器在采用贪婪预测方案时，存在容易陷入局部最优和样本效率低下的问题。

Method: 将LNS公式化为随机过程；提出SPL-LNS，一种利用局部信息建议逃离局部最优的采样增强型神经LNS求解器；开发了新颖的事后重标记方法来高效训练自生成数据。

Result: SPL-LNS在不同规模的各类整数线性规划(ILP)问题上，显著超越了此前的神经LNS求解器。

Conclusion: SPL-LNS通过其采样增强和高效训练机制，成功解决了现有贪婪神经LNS方法的局限性，并在ILP问题上实现了卓越的性能。

Abstract: Large Neighborhood Search (LNS) is a common heuristic in combinatorial
optimization that iteratively searches over a large neighborhood of the current
solution for a better one. Recently, neural network-based LNS solvers have
achieved great success in solving Integer Linear Programs (ILPs) by learning to
greedily predict the locally optimal solution for the next neighborhood
proposal. However, this greedy approach raises two key concerns: (1) to what
extent this greedy proposal suffers from local optima, and (2) how can we
effectively improve its sample efficiency in the long run. To address these
questions, this paper first formulates LNS as a stochastic process, and then
introduces SPL-LNS, a sampling-enhanced neural LNS solver that leverages
locally-informed proposals to escape local optima. We also develop a novel
hindsight relabeling method to efficiently train SPL-LNS on self-generated
data. Experimental results demonstrate that SPL-LNS substantially surpasses
prior neural LNS solvers for various ILP problems of different sizes.

</details>


### [99] [Motor Imagery EEG Signal Classification Using Minimally Random Convolutional Kernel Transform and Hybrid Deep Learning](https://arxiv.org/abs/2508.16179)
*Jamal Hwaidi,Mohamed Chahine Ghanem*

Main category: cs.LG

TL;DR: 本研究提出了一种基于Minimally Random Convolutional Kernel Transform (MiniRocket)的运动想象脑电信号分类方法，其在准确性和计算成本上优于CNN-LSTM深度学习基线模型。


<details>
  <summary>Details</summary>
Motivation: 运动想象脑电图（MI-EEG）信号的非平稳性、时变性和个体差异性，以及随着类别增加和个体变异性导致分类准确性难以提高，给MI-BCI的分类带来了显著挑战，因此需要高效的特征提取和分类方法。

Method: 本研究提出了一种新颖的方法，首先使用Minimally Random Convolutional Kernel Transform (MiniRocket)高效提取特征，然后通过线性分类器进行活动识别。此外，还提出了一个结合卷积神经网络（CNN）和长短期记忆网络（LSTM）的深度学习架构作为基线模型。所有方法均在PhysioNet数据集上进行性能评估。

Result: MiniRocket方法实现了98.63%的平均准确率，而CNN-LSTM基线模型的平均准确率为98.06%。结果表明，MiniRocket的特征提取和分类性能优于最佳深度学习模型，且计算成本更低。

Conclusion: 所提出的方法（特别是MiniRocket）能够显著提高运动想象脑电图的分类准确性，并为MI-EEG的特征提取和分类提供了新见解。

Abstract: The brain-computer interface (BCI) establishes a non-muscle channel that
enables direct communication between the human body and an external device.
Electroencephalography (EEG) is a popular non-invasive technique for recording
brain signals. It is critical to process and comprehend the hidden patterns
linked to a specific cognitive or motor task, for instance, measured through
the motor imagery brain-computer interface (MI-BCI). A significant challenge is
presented by classifying motor imagery-based electroencephalogram (MI-EEG)
tasks, given that EEG signals exhibit nonstationarity, time-variance, and
individual diversity. Obtaining good classification accuracy is also very
difficult due to the growing number of classes and the natural variability
among individuals. To overcome these issues, this paper proposes a novel method
for classifying EEG motor imagery signals that extracts features efficiently
with Minimally Random Convolutional Kernel Transform (MiniRocket), a linear
classifier then uses the extracted features for activity recognition.
Furthermore, a novel deep learning based on Convolutional Neural Network (CNN)
and Long Short Term Memory (LSTM) architecture to serve as a baseline was
proposed and demonstrated that classification via MiniRocket's features
achieves higher performance than the best deep learning models at lower
computational cost. The PhysioNet dataset was used to evaluate the performance
of the proposed approaches. The proposed models achieved mean accuracy values
of 98.63% and 98.06% for the MiniRocket and CNN-LSTM, respectively. The
findings demonstrate that the proposed approach can significantly enhance motor
imagery EEG accuracy and provide new insights into the feature extraction and
classification of MI-EEG.

</details>


### [100] [GEM: A Scale-Aware and Distribution-Sensitive Sparse Fine-Tuning Framework for Effective Downstream Adaptation](https://arxiv.org/abs/2508.16191)
*Sungmin Kang,Jisoo Kim,Salman Avestimehr,Sunwoo Lee*

Main category: cs.LG

TL;DR: GEM是一种参数高效微调方法，它关注参数相对于其初始值的更新比例，并通过熵引导掩码自适应地选择参数进行微调，仅用0.1%的参数实现了比全量微调更高的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法在微调时仅最大化参数更新的绝对大小，忽略了参数的原始尺度，导致模型行为变化不显著，未能充分实现下游任务的有效适应。

Method: 提出梯度权重比与熵引导掩码（GEM）框架。GEM是一种参数尺度感知、分布敏感的稀疏微调方法，它优先更新那些相对于其初始预训练值变化显著的参数，并根据参数值的熵自适应地确定每层微调的参数数量，以最大化计算预算的有效利用。

Result: GEM在通用领域（GLUE和SuperGLUE）和特定领域（GSM8k和MBPP）任务上均展现出高效性。它仅更新0.1%的模型参数，却实现了高达1.6%的微调准确率提升，甚至超越了全量微调的性能。

Conclusion: GEM通过引入参数尺度感知和熵引导的稀疏微调机制，显著提升了PEFT的性能和效率，证明了关注参数更新的相对重要性能够带来更有效的下游任务适应。

Abstract: Parameter-efficient fine-tuning (PEFT) has become a popular way to adapt
large pre-trained models to new tasks. Most PEFT methods update only a small
subset of parameters while freezing the rest, avoiding redundant computation.
As they maximize the absolute size of the updates without regard to the
parameters' original scale, the resulting changes in model behavior can be
minimal. In contrast, we maximize updates relative to each parameter's scale,
yielding more meaningful downstream adaptation. We propose Gradient-to-Weight
Ratio and Entropy-guided Masking (GEM), a parameter scale-aware,
distribution-sensitive sparse fine-tuning framework. GEM prioritizes parameters
whose updates are significant in proportion to their initial pre-trained
values. It also adaptively determines how many parameters to tune at each layer
based on the entropy of parameter values, thereby making the most effective use
of the computational budget in PEFT. Our empirical study demonstrates the
efficacy of GEM on both general-domain tasks (GLUE and SuperGLUE) and
domain-specific tasks (GSM8k and MBPP), achieving up to a 1.6% improvement in
fine-tuning accuracy over full fine-tuning while updating only 0.1% of model
parameters.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [101] [Task Offloading and Resource Allocation for MEC-assisted Consumer Internet of Vehicle Systems](https://arxiv.org/abs/2508.15795)
*Yanheng Liu,Dalin Li,Hao Wu,Zemin Sun,Weihong Qin,Jun Li,Hongyang Du,Geng Sun*

Main category: cs.NI

TL;DR: 本文针对MEC辅助的IoV系统中的计算挑战，提出了一种基于MADDPG算法的联合任务卸载和资源分配方案（JTOCRA），旨在最小化系统成本，并展现出优越的性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: MEC辅助的IoV系统面临为车辆提供计算敏感和计算密集型服务时的挑战，包括有限资源与严格需求之间的矛盾、难以捕捉和整合复杂系统特征，以及在动态环境中对实时处理和高效资源管理的需求。

Method: 本文首先提出了一个多MEC辅助的消费者IoV架构。随后，通过整合服务延迟和能耗，将问题公式化为系统成本最小化优化问题（SCMOP）。为高效解决此问题，设计了一种联合任务卸载和计算资源分配方法（JTOCRA），并应用了多智能体深度确定性策略梯度（MADDPG）算法。

Result: 仿真结果表明，所提出的JTOCRA方案能够实现卓越的系统性能，并与其他替代方法相比展现出更好的可扩展性。

Conclusion: 通过AI赋能的任务卸载和资源分配，JTOCRA有效解决了MEC辅助IoV系统中的资源管理和性能优化挑战，为车辆提供了高效、可扩展的计算服务。

Abstract: Mobile edge computing (MEC)-assisted internet of vehicle (IoV) is emerging as
a promising paradigm to provide computing services for vehicles. However,
meeting the computing-sensitive and computation-intensive demands of vehicles
poses several challenges, including the discrepancy between the limited
resource provision and stringent computing requirement, the difficulty in
capturing and integrating the intricate features of the MEC-assisted IoV system
into the problem formulation, and the need for real-time processing and
efficient resource management in the dynamic environment. In this work, we
explore the AI-enabled task offloading and resource allocation for MEC-assisted
consumer IoV systems. Specifically, we first present a multi-MEC-assisted
consumer IoV architecture that leverages the computational resources of MEC
servers to provide offloading services close to vehicles. Subsequently, we
formulate a system cost minimization optimization problem (SCMOP) by
integrating the service delay and energy consumption. To efficiently solve this
problem, we design a joint task offloading and computing resource allocation
approach (JTOCRA) by applying the multi-agent deep deterministic policy
gradient (MADDPG) algorithm. Finally, simulation results demonstrate that the
proposed JTOCRA can achieve superior system performances and exhibits better
scalability compared to other alternative approaches.

</details>


### [102] [Better Together: Leveraging Multiple Digital Twins for Deployment Optimization of Airborne Base Stations](https://arxiv.org/abs/2508.15816)
*Mauro Belgiovine,Chris Dick,Kaushik Chowdhury*

Main category: cs.NI

TL;DR: 本文提出一种数字孪生（DT）引导的方法，用于优化无人机搭载的空中基站（ABS）的部署位置、天线和功率，以解决其有限飞行时间问题。该方法通过桥接NVIDIA Sionna和AODT平台，并设计反向传播算法进行优化，同时在大规模场景中进行评估并引入弹性覆盖机制。


<details>
  <summary>Details</summary>
Motivation: 无人机搭载的空中基站（ABS）在提供灵活网络资源和灾害快速部署方面具有优势。然而，由于无人机飞行时间有限，需要在没有大量实地试验的情况下，高效确定ABS的最佳部署位置。

Method: 1. 实现了NVIDIA Sionna与Aerial Omniverse Digital Twin (AODT) 两个开源数字孪生平台之间的高保真度交互式软件桥接。 2. 在Sionna中设计了一种基于反向传播的算法，用于快速优化无人机的物理位置、天线方向和发射功率，以确保高效覆盖。 3. 在AODT中对大型网络场景（50个用户设备，10个ABS）进行了数值评估，以识别两个DTs性能结果一致或发散的环境条件。 4. 提出了一种弹性机制，旨在为关键任务设备提供一致覆盖，并演示了两个DT之间信息的双向流动。

Result: 1. 成功实现了两个DT平台之间的高保真度场景评估桥接，并突出了各平台在部署问题上的独特优势。 2. 通过基于反向传播的算法，实现了无人机位置、天线方向和发射功率的快速收敛和优化，确保了高效覆盖。 3. 在大规模网络场景（AODT）评估中，识别了两个DTs性能结果一致或发散的环境条件。 4. 提出并展示了为关键任务设备提供持续覆盖的弹性机制，并验证了DTs之间信息的双向流动能力。

Conclusion: 本文提出的数字孪生引导方法，通过整合和优化仿真平台，高效地解决了机载基站的部署优化难题，并在大规模网络中进行了验证，同时为关键服务提供了弹性覆盖能力，展示了数字孪生技术在通信网络部署和管理中的巨大潜力。

Abstract: Airborne Base Stations (ABSs) allow for flexible geographical allocation of
network resources with dynamically changing load as well as rapid deployment of
alternate connectivity solutions during natural disasters. Since the radio
infrastructure is carried by unmanned aerial vehicles (UAVs) with limited
flight time, it is important to establish the best location for the ABS without
exhaustive field trials. This paper proposes a digital twin (DT)-guided
approach to achieve this through the following key contributions: (i)
Implementation of an interactive software bridge between two open-source DTs
such that the same scene is evaluated with high fidelity across NVIDIA's Sionna
and Aerial Omniverse Digital Twin (AODT), highlighting the unique features of
each of these platforms for this allocation problem, (ii) Design of a
back-propagation-based algorithm in Sionna for rapidly converging on the
physical location of the UAVs, orientation of the antennas and transmit power
to ensure efficient coverage across the swarm of the UAVs, and (iii) numerical
evaluation in AODT for large network scenarios (50 UEs, 10 ABS) that identifies
the environmental conditions in which there is agreement or divergence of
performance results between these twins. Finally, (iv) we propose a resilience
mechanism to provide consistent coverage to mission-critical devices and
demonstrate a use case for bi-directional flow of information between the two
DTs.

</details>


### [103] [Agent Communications toward Agentic AI at Edge -- A Case Study of the Agent2Agent Protocol](https://arxiv.org/abs/2508.15819)
*Qiang Duan,Zhihui Lu*

Main category: cs.NI

TL;DR: 本文评估了现有的智能体通信技术（以A2A协议为代表）在应对边缘计算挑战方面的能力，指出了当前技术的开放性问题，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着边缘智能的兴起，将代理AI部署到网络边缘成为可能。然而，现有智能体通信协议在设计时未充分考虑边缘计算的特殊挑战，其在边缘环境中的有效性尚未得到充分检验。

Method: 首先，讨论智能体通信的核心功能，概述智能体通信协议，并识别边缘计算带来的主要挑战。然后，以最具代表性的A2A协议为例进行案例研究，评估其关键技术在满足边缘计算中智能体通信需求方面的有效性。

Result: 通过评估，获得了关于智能体通信技术在边缘计算中表现的深刻见解，并识别了当前智能体通信技术中存在的开放性问题。

Conclusion: 总结了当前智能体通信技术面临的开放性问题，并提出了未来研究方向以解决这些问题，以期更好地支持边缘环境中的代理AI应用。

Abstract: The current evolution of artificial intelligence introduces a paradigm shift
toward agentic AI built upon multi-agent systems (MAS). Agent communications
serve as a key to effective agent interactions in MAS and thus have a
significant impact on the performance of agentic AI applications. The recent
research on agent communications has made exciting rapid progress that leads to
a variety of protocol designs, among which the Agent2Agent (A2A) protocol is
considered the most representative one. Simultaneously, the rise of edge
intelligence is expected to enable agentic AI at the network edge. However, the
current agent communication protocols are designed without sufficient
consideration of the special challenges of edge computing, and their
effectiveness in the edge environment is largely unexamined. In this paper, we
attempt to assess the abilities of agent communication technologies to face the
challenges of edge computing using the A2A protocol as a representative case.
We first discuss the core functionalities of agent communications, present a
landscape of agent communication protocols, and identify the main challenges
introduced by edge computing. Then, we conduct a case study on the A2A protocol
to examine the key technologies leveraged in the protocol for their
effectiveness in meeting the requirements of agent communications in edge
computing. Based on the insights obtained from this assessment, we identify
open issues in the current agent communication technologies and discuss
directions for future research to address these issues.

</details>


### [104] [Towards Integrated Energy-Communication-Transportation Hub: A Base-Station-Centric Design in 5G and Beyond](https://arxiv.org/abs/2508.15833)
*Linfeng Shen,Guanzhen Wu,Cong Zhang,Xiaoyi Fan,Jiangchuan Liu*

Main category: cs.NI

TL;DR: 针对5G基站能耗问题，本文提出将基站作为集成能源-通信-交通(ECT)枢纽，利用基站电池和可再生能源为电动汽车充电，并通过深度强化学习优化电池调度和激励性定价，有效降低运营成本并创收。


<details>
  <summary>Details</summary>
Motivation: 5G基站大规模部署导致能耗显著增加；行业寻求利用储能系统降低成本；需要有效方法最大化基站电池利用率；顺应未来能源-通信-交通(ECT)融合基础设施发展趋势，探索基站作为服务枢纽的潜力。

Method: 提出ECT-Hub模型，探索基站与电动汽车(EV)充电基础设施的结合，利用基站电池和可再生能源为EV充电。模型考虑基站流量、天气和EV充电行为等因素，引入激励机制设定充电价格，并采用深度强化学习方法进行电池调度。

Result: 实验结果表明，所提出的ECT-Hub能有效优化过剩能源利用、显著降低运营成本，特别是通过为EV充电实现创收。

Conclusion: 本文提出的ECT-Hub成功地将5G基站转型为能源、通信、交通一体化枢纽，通过智能电池调度和定价策略，有效解决了基站能耗问题，优化了能源利用，降低了运营成本，并为运营商开辟了新的收入来源。

Abstract: The rise of 5G communication has transformed the telecom industry for
critical applications. With the widespread deployment of 5G base stations comes
a significant concern about energy consumption. Key industrial players have
recently shown strong interest in incorporating energy storage systems to store
excess energy during off-peak hours, reducing costs and participating in demand
response. The fast development of batteries opens up new possibilities, such as
the transportation area. An effective method is needed to maximize base station
battery utilization and reduce operating costs. In this trend towards
next-generation smart and integrated energy-communication-transportation (ECT)
infrastructure, base stations are believed to play a key role as service hubs.
By exploring the overlap between base station distribution and electric vehicle
charging infrastructure, we demonstrate the feasibility of efficiently charging
EVs using base station batteries and renewable power plants at the Hub. Our
model considers various factors, including base station traffic conditions,
weather, and EV charging behavior. This paper introduces an incentive mechanism
for setting charging prices and employs a deep reinforcement learning-based
method for battery scheduling. Experimental results demonstrate the
effectiveness of our proposed ECT-Hub in optimizing surplus energy utilization
and reducing operating costs, particularly through revenue-generating EV
charging.

</details>


### [105] [Safeguarding ISAC Performance in Low-Altitude Wireless Networks Under Channel Access Attack](https://arxiv.org/abs/2508.15838)
*Jiacheng Wang,Jialing He,Geng Sun,Zehui Xiong,Dusit Niyato,Shiwen Mao,Dong In Kim,Tao Xiang*

Main category: cs.NI

TL;DR: 针对低空无线网络（LAWNs）中集成感知与通信（ISAC）面临的恶意信道访问攻击，本文提出了一种基于Stackelberg博弈的框架和反向归纳算法，旨在优化ISAC性能并缓解攻击影响。


<details>
  <summary>Details</summary>
Motivation: 随着地面资源日益饱和，低空应用（如空中出租车）逐渐兴起，低空无线网络（LAWNs）是其基础，而集成感知与通信（ISAC）是LAWNs的核心技术。然而，低空空域的开放性使得LAWNs易受恶意信道访问攻击，从而导致ISAC性能下降。

Method: 本文开发了一个基于Stackelberg博弈的框架来减轻攻击对LAWNs中ISAC性能的影响。首先，推导了攻击条件下通信数据的信干噪比（SINR）和感知数据的年龄信息（AoI）作为服务质量指标。然后，将ISAC性能优化问题建模为Stackelberg博弈，其中攻击者是领导者，合法无人机是第二跟随者，地面ISAC基站是第一跟随者。在此基础上，设计了一种反向归纳算法来达到Stackelberg均衡，同时最大化所有参与者的效用。研究还证明了该均衡的存在性和唯一性。

Result: 仿真结果表明，所提出的算法性能优于现有基线和静态纳什均衡基准。

Conclusion: 该算法能有效缓解低空无线网络中ISAC性能受攻击导致的下降，确保LAWNs能够为低空应用提供可靠服务。

Abstract: The increasing saturation of terrestrial resources has driven the exploration
of low-altitude applications such as air taxis. Low altitude wireless networks
(LAWNs) serve as the foundation for these applications, and integrated sensing
and communication (ISAC) constitutes one of the core technologies within LAWNs.
However, the openness nature of low-altitude airspace makes LAWNs vulnerable to
malicious channel access attacks, which degrade the ISAC performance.
Therefore, this paper develops a game-based framework to mitigate the influence
of the attacks on LAWNs. Concretely, we first derive expressions of
communication data's signal-to-interference-plus-noise ratio and the age of
information of sensing data under attack conditions, which serve as quality of
service metrics. Then, we formulate the ISAC performance optimization problem
as a Stackelberg game, where the attacker acts as the leader, and the
legitimate drone and the ground ISAC base station act as second and first
followers, respectively. On this basis, we design a backward induction
algorithm that achieves the Stackelberg equilibrium while maximizing the
utilities of all participants, thereby mitigating the attack-induced
degradation of ISAC performance in LAWNs. We further prove the existence and
uniqueness of the equilibrium. Simulation results show that the proposed
algorithm outperforms existing baselines and a static Nash equilibrium
benchmark, ensuring that LAWNs can provide reliable service for low-altitude
applications.

</details>


### [106] [xDiff: Online Diffusion Model for Collaborative Inter-Cell Interference Management in 5G O-RAN](https://arxiv.org/abs/2508.15843)
*Peihao Yan,Huacheng Zeng,Y. Thomas Hou*

Main category: cs.NI

TL;DR: 本文提出了xDiff，一个基于扩散模型和强化学习的框架，用于O-RAN中的小区间干扰管理（ICIM），实验证明其性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 5G及未来蜂窝网络中的O-RAN需要智能高效的资源管理。扩散模型在图像视频生成方面表现出色，预示着其在网络优化任务中的潜力。具体研究动机是解决O-RAN中的小区间干扰管理问题。

Method: 研究者提出了xDiff框架，将ICIM建模为资源分配优化问题，旨在最大化用户定义的奖励函数。该框架通过将扩散模型集成到强化学习中，开发了在线学习解决方案以实现近实时策略生成。引入了“偏好值”作为新颖的策略表示，以高效指导O-RAN分布式单元（DU）中的资源分配。xDiff在一个包含三个小区和智能手机的5G测试平台上进行了实现和验证。

Result: 实验结果表明，xDiff在两个小蜂窝场景中，其性能优于现有的小区间干扰管理方法。

Conclusion: 扩散模型在O-RAN的在线优化中展现出巨大潜力。

Abstract: Open Radio Access Network (O-RAN) is a key architectural paradigm for 5G and
beyond cellular networks, enabling the adoption of intelligent and efficient
resource management solutions. Meanwhile, diffusion models have demonstrated
remarkable capabilities in image and video generation, making them attractive
for network optimization tasks. In this paper, we propose xDiff, a
diffusion-based reinforcement learning(RL) framework for inter-cell
interference management (ICIM) in O-RAN. We first formulate ICIM as a resource
allocation optimization problem aimed at maximizing a user-defined reward
function and then develop an online learning solution by integrating a
diffusion model into an RL framework for near-real-time policy generation.
Particularly, we introduce a novel metric, preference values, as the policy
representation to enable efficient policy-guided resource allocation within
O-RAN distributed units (DUs). We implement xDiff on a 5G testbed consisting of
three cells and a set of smartphones in two small-cell scenarios. Experimental
results demonstrate that xDiff outperforms state-of-the-art ICIM approaches,
highlighting the potential of diffusion models for online optimization of
O-RAN. Source code is available on GitHub [1].

</details>


### [107] [Time Series Based Network Intrusion Detection using MTF-Aided Transformer](https://arxiv.org/abs/2508.16035)
*Poorvi Joshi,Mohan Gurusamy*

Main category: cs.NI

TL;DR: 本文提出了一种针对软件定义网络（SDN）的时间序列分类新方法，通过结合马尔可夫转移场（MTF）和Transformer模型，在数据受限环境下表现优于基线模型，并具有高效的训练和推理速度。


<details>
  <summary>Details</summary>
Motivation: 在软件定义网络（SDN）应用中，时间序列分类面临数据受限的挑战，需要可靠、可扩展且高效的解决方案来处理稀疏数据。

Method: 提出了一种MTF辅助的Transformer模型，将马尔可夫转移场（MTF）捕捉时间依赖性的能力与Transformer架构的模式识别能力相结合，用于时间序列分类。

Result: 该模型在使用InSDN数据集进行评估时，性能优于基线分类模型，尤其是在数据受限的环境下。MTF和Transformer组件之间的关系促进了即使在有限数据下的性能提升。此外，该方法实现了具有竞争力的训练和推理时间。

Conclusion: MTF辅助的Transformer模型在解决SDN中时间序列分类挑战方面具有巨大潜力，为稀疏数据场景下的可靠和可扩展分析提供了一条有前景的道路。

Abstract: This paper introduces a novel approach to time series classification using a
Markov Transition Field (MTF)-aided Transformer model, specifically designed
for Software-Defined Networks (SDNs). The proposed model integrates the
temporal dependency modeling strengths of MTFs with the sophisticated pattern
recognition capabilities of Transformer architectures. We evaluate the model's
performance using the InSDN dataset, demonstrating that our model outperforms
baseline classification models, particularly in data-constrained environments
commonly encountered in SDN applications. We also highlight the relationship
between the MTF and Transformer components, which leads to better performance,
even with limited data. Furthermore, our approach achieves competitive training
and inference times, making it an efficient solution for real-world SDN
applications. These findings establish the potential of MTF-aided Transformers
to address the challenges of time series classification in SDNs, offering a
promising path for reliable and scalable analysis in scenarios with sparse
data.

</details>


### [108] [Congestion Control System Optimization with Large Language Models](https://arxiv.org/abs/2508.16074)
*Zhiyuan He,Aashish Gottipati,Lili Qiu,Yuqing Yang,Francis Y. Yan*

Main category: cs.NI

TL;DR: 本文提出一种利用大型语言模型（LLMs）自动优化拥塞控制算法的新方法，并在实际环境中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管研究人员投入了大量精力，但现有拥塞控制算法在多样化网络环境中仍表现不佳。

Method: 该方法包含一个结构化算法生成过程（由LLMs完成）、一个涵盖广泛网络条件的基于仿真的评估流程，以及一个统计引导的评估时间缩减方法。

Result: 通过四种不同LLM的经验结果验证了该方法的有效性，成功识别出在生产级QUIC实现中比原始BBR算法性能提升高达27%的算法。

Conclusion: 研究工作展示了LLMs在加速高性能网络算法设计方面的潜力，并为在网络系统中更广泛的应用奠定了基础。

Abstract: Congestion control is a fundamental component of Internet infrastructure, and
researchers have dedicated considerable effort to developing improved
congestion control algorithms. However, despite extensive study, existing
algorithms continue to exhibit suboptimal performance across diverse network
environments. In this paper, we introduce a novel approach that automatically
optimizes congestion control algorithms using large language models (LLMs). Our
framework consists of a structured algorithm generation process, an
emulation-based evaluation pipeline covering a broad range of network
conditions, and a statistically guided method to substantially reduce
evaluation time. Empirical results from four distinct LLMs validate the
effectiveness of our approach. We successfully identify algorithms that achieve
up to 27% performance improvements over the original BBR algorithm in a
production QUIC implementation. Our work demonstrates the potential of LLMs to
accelerate the design of high-performance network algorithms and paves the way
for broader applications in networking systems.

</details>


### [109] [ANSC: Probabilistic Capacity Health Scoring for Datacenter-Scale Reliability](https://arxiv.org/abs/2508.16119)
*Madhava Gaikwad,Abhishek Gandhi*

Main category: cs.NI

TL;DR: ANSC是一个超大规模数据中心的概率性容量健康评分框架，通过预测级联容量短缺的概率来帮助运维人员优先处理最关键的风险，而非仅仅检测当前故障。


<details>
  <summary>Details</summary>
Motivation: 现有告警系统仅能检测单个设备或链路故障，无法捕获级联容量短缺的总体风险，导致无法有效预警和优先级排序。

Method: ANSC框架提供一套颜色编码的评分系统，通过考虑当前剩余容量和额外故障的概率来指示问题的紧急性，并进行数据中心和区域层面的标准化处理。

Result: ANSC使运维人员能够在400多个数据中心和60个区域中优先安排修复工作，减少了告警噪音，并使站点可靠性工程师（SRE）专注于最关键的风险。

Conclusion: ANSC通过提供一个前瞻性的、概率驱动的容量健康评分系统，有效解决了传统告警系统的局限性，显著提升了超大规模数据中心的风险管理和运维效率。

Abstract: We present ANSC, a probabilistic capacity health scoring framework for
hyperscale datacenter fabrics. While existing alerting systems detect
individual device or link failures, they do not capture the aggregate risk of
cascading capacity shortfalls. ANSC provides a color-coded scoring system that
indicates the urgency of issues \emph{not solely by current impact, but by the
probability of imminent capacity violations}. Our system accounts for both
current residual capacity and the probability of additional failures,
normalized at datacenter and regional level. We demonstrate that ANSC enables
operators to prioritize remediation across more than 400 datacenters and 60
regions, reducing noise and aligning SRE focus on the most critical risks.

</details>


### [110] [Joint Cache Placement and Routing in Satellite-Terrestrial Edge Computing Network: A GNN-Enabled DRL Approach](https://arxiv.org/abs/2508.16184)
*Yuhao Zheng,Ting You,Kejia Peng,Chang Liu*

Main category: cs.NI

TL;DR: 本文提出一种结合GNN和DRL的框架，优化卫星-地面边缘计算网络中的联合内容缓存与路由，以提升交付成功率并降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 为提高卫星-地面边缘计算网络（STECNs）中地理分布式用户的内容缓存服务，需要解决动态低轨卫星拓扑和异构内容需求带来的挑战，并对内容缓存与路由进行联合优化。

Method: 提出一种集成图神经网络（GNNs）和深度强化学习（DRL）的学习型框架。将卫星网络建模为动态图，GNNs嵌入DRL智能体中以捕获空间和拓扑依赖，支持路由感知决策。缓存策略通过将问题表述为马尔可夫决策过程（MDP）并应用软Actor-Critic（SAC）算法进行优化。

Result: 仿真结果表明，所提出的方法显著提高了内容交付成功率，并有效降低了通信流量成本。

Conclusion: 通过结合GNN和DRL的框架，能够有效解决卫星-地面边缘计算网络中的联合内容缓存与路由问题，显著提升缓存服务性能和网络效率。

Abstract: In this letter, we investigate the problem of joint content caching and
routing in satellite-terrestrial edge computing networks (STECNs) to improve
caching service for geographically distributed users. To handle the challenges
arising from dynamic low Earth orbit (LEO) satellite topologies and
heterogeneous content demands, we propose a learning-based framework that
integrates graph neural networks (GNNs) with deep reinforcement learning (DRL).
The satellite network is represented as a dynamic graph, where GNNs are
embedded within the DRL agent to capture spatial and topological dependencies
and support routing-aware decision-making. The caching strategy is optimized by
formulating the problem as a Markov decision process (MDP) and applying soft
actor-critic (SAC) algorithm. Simulation results demonstrate that our approach
significantly improves the delivery success rate and reduces communication
traffic cost.

</details>


### [111] [Self-Healing Network of Interconnected Edge Devices Empowered by Infrastructure-as-Code and LoRa Communication](https://arxiv.org/abs/2508.16268)
*Rob Carson,Mohamed Chahine Ghanem,Feriel Bouakkaz*

Main category: cs.NI

TL;DR: 本文提出了一种基于树莓派的自愈合自动化LoRa网络，适用于传统网络不可用的场景，通过数据分片和自动化故障转移提高了系统韧性，并识别了未来需要改进的挑战。


<details>
  <summary>Details</summary>
Motivation: 在传统网络不可用的场景下，利用LoRa协议的低功耗、长距离特性，解决有限带宽、数据冲突和节点故障等挑战。同时，由于LoRa的报文系统与传统依赖TCP/IP的IaC工具不兼容，需要创新IaC应用方式。

Method: 本研究在一个树莓派集群上，通过容器化架构改编了IaC原则，构建了LoRa网络。采用数据包分片和丢失片段重传机制以缓解LoRa的吞吐量和包大小限制。同时，集成了自动化故障转移机制，使无响应服务能在1秒内重新部署到替代节点。

Result: 评估实验表明，数据分片和重传能有效缓解LoRa的吞吐量和包大小限制，尽管冲突和视距干扰问题依然存在。自动化故障转移机制能够在一秒内将无响应服务重新部署到其他节点，证明了系统在节点或服务故障时的韧性。研究还识别出实际挑战，例如需要进行时隙传输以避免数据包重叠和冲突。

Conclusion: 本研究成功构建了一个自愈合、自动化的树莓派LoRa网络，通过适应性IaC和故障转移机制提升了系统韧性。虽然数据分片有效，但仍需解决冲突和视距问题。未来工作应探索网状网络、更高级调度算法和新的LPWAN技术以进一步增强系统。

Abstract: This Paper proposes a self-healing, automated network of Raspberry Pi devices
designed for deployment in scenarios where traditional networking is
unavailable. Leveraging the low-power, long-range capabilities of the LoRa
(Long Range) protocol alongside Infrastructure as Code (IaC) methodologies, the
research addresses challenges such as limited bandwidth, data collisions, and
node failures. Given that LoRa's packet-based system is incompatible with
conventional IaC tools like Ansible and Terraform, which rely on TCP/IP
networking, the research adapts IaC principles within a containerised
architecture deployed across a Raspberry Pi cluster. Evaluation experiments
indicate that fragmenting data packets and retransmitting any missed fragments
can mitigate LoRa's inherent throughput and packet size limitations, although
issues such as collisions and line-of-sight interference persist. An automated
failover mechanism was integrated into the architecture, enabling unresponsive
services to be redeployed to alternative nodes within one second, demonstrating
the system's resilience in maintaining operational continuity despite node or
service failures. The paper also identifies practical challenges, including the
necessity for time-slotting transmissions to prevent data packet overlap and
collisions. Future research should explore the integration of mesh networking
to enhance range, develop more advanced scheduling algorithms, and adopt
cutting-edge low-power wide-area network (LPWAN) techniques.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [112] [Set Transformer Architectures and Synthetic Data Generation for Flow-Guided Nanoscale Localization](https://arxiv.org/abs/2508.16200)
*Mika Leo Hube,Filip Lemic,Ethungshan Shitiri,Gerard Calvo Bartra,Sergi Abadal,Xavier Costa Pérez*

Main category: cs.ET

TL;DR: 本文利用Set Transformer和合成数据增强技术，改进了流引导定位(FGL)系统，以提高其对解剖变异的适应性和可扩展性，并在实现与GNN相当的分类精度的同时，提供了更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的流引导定位（FGL）解决方案依赖于拓扑固定的图模型或手工特征，这限制了它们对解剖变异的适应性并阻碍了可扩展性。

Method: 本文探索使用Set Transformer架构，将纳米设备循环时间报告视为无序集合，实现置换不变、变长输入处理。为提高数据稀缺和类别不平衡下的鲁棒性，研究通过深度生成模型（包括CGAN、WGAN、WGAN-GP和CVAE）生成合成数据，以增强训练数据集。

Result: Set Transformer在分类精度上与图神经网络（GNN）基线模型相当，同时在设计上提供了对解剖变异性更好的泛化能力。

Conclusion: 研究结果强调了置换不变模型和合成数据增强技术在实现鲁棒和可扩展的纳米尺度定位方面的潜力。

Abstract: Flow-guided Localization (FGL) enables the identification of spatial regions
within the human body that contain an event of diagnostic interest. FGL does
that by leveraging the passive movement of energy-constrained nanodevices
circulating through the bloodstream. Existing FGL solutions rely on graph
models with fixed topologies or handcrafted features, which limit their
adaptability to anatomical variability and hinder scalability. In this work, we
explore the use of Set Transformer architectures to address these limitations.
Our formulation treats nanodevices' circulation time reports as unordered sets,
enabling permutation-invariant, variable-length input processing without
relying on spatial priors. To improve robustness under data scarcity and class
imbalance, we integrate synthetic data generation via deep generative models,
including CGAN, WGAN, WGAN-GP, and CVAE. These models are trained to replicate
realistic circulation time distributions conditioned on vascular region labels,
and are used to augment the training data. Our results show that the Set
Transformer achieves comparable classification accuracy compared to Graph
Neural Networks (GNN) baselines, while simultaneously providing by-design
improved generalization to anatomical variability. The findings highlight the
potential of permutation-invariant models and synthetic augmentation for robust
and scalable nanoscale localization.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [113] [Learning in Focus: Detecting Behavioral and Collaborative Engagement Using Vision Transformers](https://arxiv.org/abs/2508.15782)
*Sindhuja Penchala,Saketh Reddy Kontham,Prachi Bhattacharjee,Sareh Karami,Mehdi Ghahremani,Noorbakhsh Amiri Golilarz,Shahram Rahimi*

Main category: q-bio.NC

TL;DR: 本文提出了一种基于视觉Transformer（ViTs）的AI方法，通过分析视觉线索自动分类儿童在早期教育中的行为和协作投入度，其中Swin Transformer表现最佳，准确率达97.58%。


<details>
  <summary>Details</summary>
Motivation: 在早期教育中，准确检测儿童的行为和协作投入度对于促进有意义的学习体验至关重要。

Method: 本研究采用AI驱动的方法，利用视觉Transformer（ViTs）自动分类儿童的投入度。通过分析注视方向、互动和同伴协作等视觉线索，在Child-Play凝视数据集上训练模型以分类行为和协作投入状态。评估了ViT、DeiT和Swin Transformer三种先进模型。

Result: Swin Transformer在分类性能上表现最佳，准确率达到97.58%，有效结合了局部和全局注意力建模。

Conclusion: 基于Transformer的架构在真实教育环境中进行可扩展、自动化投入度分析具有巨大潜力。

Abstract: In early childhood education, accurately detecting behavioral and
collaborative engagement is essential for fostering meaningful learning
experiences. This paper presents an AI-driven approach that leverages Vision
Transformers (ViTs) to automatically classify children's engagement using
visual cues such as gaze direction, interaction, and peer collaboration.
Utilizing the Child-Play gaze dataset, our method is trained on annotated video
segments to classify behavioral and collaborative engagement states (e.g.,
engaged, not engaged, collaborative, not collaborative). We evaluated three
state-of-the-art transformer models: Vision Transformer (ViT), Data-efficient
Image Transformer (DeiT), and Swin Transformer. Among these, the Swin
Transformer achieved the highest classification performance with an accuracy of
97.58%, demonstrating its effectiveness in modeling local and global attention.
Our results highlight the potential of transformer-based architectures for
scalable, automated engagement analysis in real-world educational settings.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [114] [A Survey of Post-Quantum Cryptography Support in Cryptographic Libraries](https://arxiv.org/abs/2508.16078)
*Nadeem Ahmed,Lei Zhang,Aryya Gangopadhyay*

Main category: cs.CR

TL;DR: 评估了九个主流开源加密库对NIST后量子密码（PQC）算法的支持情况，发现准备程度参差不齐，并强调了安全过渡到PQC的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 量子计算的快速发展对现代加密系统构成重大威胁，因此迫切需要过渡到后量子密码（PQC）。

Method: 本研究评估了九个广泛使用的开源加密库（OpenSSL, wolfSSL, BoringSSL, LibreSSL, Bouncy Castle, libsodium, Crypto++, Botan, MbedTLS）对NIST选定的PQC算法（CRYSTALS-Kyber, CRYSTALS-Dilithium, FALCON, SPHINCS+）的支持情况。分析依据截至2025年初的最新文档、发布说明和行业报告。

Result: 分析揭示这些库的PQC准备状态各不相同。一些库已集成PQC支持或有明确的实施路线图，而另一些则相对滞后，这在量子威胁日益临近时造成潜在的安全风险。研究还讨论了性能权衡、实现安全性和实际应用中的采用障碍等关键挑战。

Conclusion: 研究结果强调，为确保向抗量子密码环境的安全过渡，迫切需要持续的研究、标准化工作和协调一致的采用策略。

Abstract: The rapid advancement of quantum computing poses a significant threat to
modern cryptographic systems, necessitating the transition to Post-Quantum
Cryptography (PQC). This study evaluates the support for PQC algorithms within
nine widely used open-source cryptographic libraries -- OpenSSL, wolfSSL,
BoringSSL, LibreSSL, Bouncy Castle, libsodium, Crypto++, Botan, and MbedTLS --
focusing on their implementation of the NIST-selected PQC finalists:
CRYSTALS-Kyber, CRYSTALS-Dilithium, FALCON, and SPHINCS+. Our analysis, based
on the latest available documentation, release notes, and industry reports as
of early 2025, reveals a varied state of readiness across these libraries.
While some libraries have integrated PQC support or have clear implementation
roadmaps, others lag behind, creating potential security risks as quantum
threats become more imminent. We discuss key challenges, including performance
trade-offs, implementation security, and adoption hurdles in real-world
cryptographic applications. Our findings highlight the urgent need for
continued research, standardization efforts, and coordinated adoption
strategies to ensure a secure transition to the quantum-resistant cryptographic
landscape.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [115] [Straggler-Resilient Federated Learning over A Hybrid Conventional and Pinching Antenna Network](https://arxiv.org/abs/2508.15821)
*Bibo Wu,Fang Fang,Ming Zeng,Xianbin Wang*

Main category: cs.IT

TL;DR: 本文提出了一种混合常规和紧缩天线网络（HCPAN）结合NOMA增强型联邦学习，通过模糊逻辑客户端分类和深度强化学习优化天线部署与资源分配，旨在解决联邦学习中的“掉队者”问题并显著提高通信效率。仿真结果验证了该方案的优越性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）中普遍存在“掉队者”问题。传统的无线网络难以动态建立强视距（LoS）链路。本文旨在利用紧缩天线动态建立LoS链路，以缓解“掉队者”问题，并显著提高NOMA增强型FL系统的通信效率。

Method: 1. 提出了一种混合常规和紧缩天线网络（HCPAN）。2. 设计了一种基于模糊逻辑的客户端分类方案，以平衡客户端数据贡献和通信条件。3. 建立了一个总时间最小化问题，用于联合优化紧缩天线部署和资源分配。4. 开发了一种基于深度强化学习（DRL）的算法来解决该变量耦合和非凸的复杂优化问题。

Result: 仿真结果验证了所提出方案的优越性。通过优化紧缩天线的部署，该方案有效增强了联邦学习的性能，并显著提升了NOMA增强型FL系统的通信效率。

Conclusion: 所提出的HCPAN方案，结合模糊逻辑分类和DRL优化，能够有效利用紧缩天线动态建立LoS链路，从而减轻联邦学习中的“掉队者”问题，并显著提高了NOMA增强型FL系统的通信效率和FL性能。

Abstract: Leveraging pinching antennas in wireless network enabled federated learning
(FL) can effectively mitigate the common "straggler" issue in FL by dynamically
establishing strong line-of-sight (LoS) links on demand. This letter proposes a
hybrid conventional and pinching antenna network (HCPAN) to significantly
improve communication efficiency in the non-orthogonal multiple access
(NOMA)-enabled FL system. Within this framework, a fuzzy logic-based client
classification scheme is first proposed to effectively balance clients' data
contributions and communication conditions. Given this classification, we
formulate a total time minimization problem to jointly optimize pinching
antenna placement and resource allocation. Due to the complexity of variable
coupling and non-convexity, a deep reinforcement learning (DRL)-based algorithm
is developed to effectively address this problem. Simulation results validate
the superiority of the proposed scheme in enhancing FL performance via the
optimized deployment of pinching antenna.

</details>
