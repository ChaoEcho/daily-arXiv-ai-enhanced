{"id": "2507.12547", "pdf": "https://arxiv.org/pdf/2507.12547", "abs": "https://arxiv.org/abs/2507.12547", "authors": ["Lionel Wong", "Katherine M. Collins", "Lance Ying", "Cedegao E. Zhang", "Adrian Weller", "Tobias Gersternberg", "Timothy O'Donnell", "Alexander K. Lew", "Jacob D. Andreas", "Joshua B. Tenenbaum", "Tyler Brooke-Wilson"], "title": "Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models", "categories": ["cs.CL", "cs.AI", "cs.PL"], "comment": "Presented at CogSci 2025", "summary": "When faced with novel situations, people are able to marshal relevant\nconsiderations from a wide range of background knowledge and put these to use\nin inferences and predictions. What permits us to draw in globally relevant\ninformation and reason over it coherently? Here, we explore the hypothesis that\npeople use a combination of distributed and symbolic representations to\nconstruct bespoke mental models tailored to novel situations. We propose a\ncomputational implementation of this idea -- a ``Model Synthesis Architecture''\n(MSA) -- using language models to implement global relevance-based retrieval\nand model synthesis and probabilistic programs to implement bespoke, coherent\nworld models. We evaluate our MSA as a model of human judgments on a novel\nreasoning dataset. The dataset -- built around a `Model Olympics` domain of\nsports vignettes -- tests models' capacity for human-like, open-ended reasoning\nby requiring (i) judgments about novel causal structures described in language;\n(ii) drawing on large bodies of background knowledge; and (iii) doing both in\nlight of observations that introduce arbitrary novel variables. Our MSA\napproach captures human judgments better than language model-only baselines,\nunder both direct and chain-of-thought generations from the LM that supports\nmodel synthesis. These results suggest that MSAs can be implemented in a way\nthat mirrors people's ability to deliver locally coherent reasoning over\nglobally relevant variables, offering a path to understanding and replicating\nhuman reasoning in open-ended domains.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4eba\u7c7b\u5728\u964c\u751f\u60c5\u5883\u4e0b\u6574\u5408\u5168\u5c40\u4fe1\u606f\u5e76\u8fdb\u884c\u8fde\u8d2f\u63a8\u7406\u7684\u80fd\u529b\u3002\u63d0\u51fa\u4e00\u79cd\u201c\u6a21\u578b\u5408\u6210\u67b6\u6784\u201d\uff08MSA\uff09\uff0c\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u548c\u6982\u7387\u7a0b\u5e8f\uff0c\u7528\u4e8e\u6784\u5efa\u5b9a\u5236\u5fc3\u7406\u6a21\u578b\u3002\u5728\u65b0\u7684\u63a8\u7406\u6570\u636e\u96c6\u4e0a\uff0cMSA\u8868\u73b0\u4f18\u4e8e\u7eaf\u8bed\u8a00\u6a21\u578b\u57fa\u7ebf\uff0c\u8868\u660e\u5176\u80fd\u6a21\u62df\u4eba\u7c7b\u5f00\u653e\u5f0f\u63a8\u7406\u3002", "motivation": "\u5f53\u9762\u5bf9\u65b0\u9896\u60c5\u5883\u65f6\uff0c\u4eba\u7c7b\u5982\u4f55\u4ece\u5e7f\u6cdb\u7684\u80cc\u666f\u77e5\u8bc6\u4e2d\u63d0\u53d6\u76f8\u5173\u8003\u91cf\u5e76\u7528\u4e8e\u63a8\u7406\u548c\u9884\u6d4b\uff1f\u662f\u4ec0\u4e48\u4f7f\u6211\u4eec\u80fd\u591f\u5438\u7eb3\u5168\u5c40\u76f8\u5173\u4fe1\u606f\u5e76\u8fdb\u884c\u8fde\u8d2f\u63a8\u7406\uff1f", "method": "\u63d0\u51fa\u5e76\u63a2\u7d22\u4e86\u4eba\u7c7b\u4f7f\u7528\u5206\u5e03\u5f0f\u548c\u7b26\u53f7\u8868\u5f81\u7ed3\u5408\u6765\u6784\u5efa\u5b9a\u5236\u5fc3\u7406\u6a21\u578b\u7684\u5047\u8bbe\u3002\u8ba1\u7b97\u5b9e\u73b0\u4e3a\u201c\u6a21\u578b\u5408\u6210\u67b6\u6784\u201d\uff08MSA\uff09\uff0c\u5176\u4e2d\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u5168\u5c40\u76f8\u5173\u6027\u68c0\u7d22\u548c\u6a21\u578b\u5408\u6210\uff0c\u6982\u7387\u7a0b\u5e8f\u7528\u4e8e\u5b9e\u73b0\u5b9a\u5236\u7684\u3001\u8fde\u8d2f\u7684\u4e16\u754c\u6a21\u578b\u3002\u5728\u540d\u4e3a\u201c\u6a21\u578b\u5965\u6797\u5339\u514b\u201d\u7684\u4f53\u80b2\u60c5\u5883\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30MSA\uff0c\u8be5\u6570\u636e\u96c6\u8981\u6c42\u6a21\u578b\u5bf9\u8bed\u8a00\u63cf\u8ff0\u7684\u65b0\u56e0\u679c\u7ed3\u6784\u8fdb\u884c\u5224\u65ad\uff0c\u5229\u7528\u5927\u91cf\u80cc\u666f\u77e5\u8bc6\uff0c\u5e76\u7ed3\u5408\u5f15\u5165\u4efb\u610f\u65b0\u53d8\u91cf\u7684\u89c2\u5bdf\u7ed3\u679c\u3002\u5c06MSA\u4e0e\u4ec5\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "MSA\u5728\u6355\u6349\u4eba\u7c7b\u5224\u65ad\u65b9\u9762\u4f18\u4e8e\u4ec5\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7ebf\uff08\u5305\u62ec\u76f4\u63a5\u751f\u6210\u548c\u601d\u7ef4\u94fe\u751f\u6210\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660eMSA\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u4eba\u7c7b\u5728\u5f00\u653e\u5f0f\u9886\u57df\u4e2d\uff0c\u5bf9\u5168\u5c40\u76f8\u5173\u53d8\u91cf\u8fdb\u884c\u5c40\u90e8\u8fde\u8d2f\u63a8\u7406\u80fd\u529b\u7684\u6a21\u62df\uff0c\u4e3a\u7406\u89e3\u548c\u590d\u5236\u4eba\u7c7b\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u9014\u5f84\u3002"}}
{"id": "2507.12553", "pdf": "https://arxiv.org/pdf/2507.12553", "abs": "https://arxiv.org/abs/2507.12553", "authors": ["Michael A. Lepori", "Jennifer Hu", "Ishita Dasgupta", "Roma Patel", "Thomas Serre", "Ellie Pavlick"], "title": "Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language models (LMs) are used for a diverse range of tasks, from question\nanswering to writing fantastical stories. In order to reliably accomplish these\ntasks, LMs must be able to discern the modal category of a sentence (i.e.,\nwhether it describes something that is possible, impossible, completely\nnonsensical, etc.). However, recent studies have called into question the\nability of LMs to categorize sentences according to modality (Michaelov et al.,\n2025; Kauf et al., 2023). In this work, we identify linear representations that\ndiscriminate between modal categories within a variety of LMs, or modal\ndifference vectors. Analysis of modal difference vectors reveals that LMs have\naccess to more reliable modal categorization judgments than previously\nreported. Furthermore, we find that modal difference vectors emerge in a\nconsistent order as models become more competent (i.e., through training steps,\nlayers, and parameter count). Notably, we find that modal difference vectors\nidentified within LM activations can be used to model fine-grained human\ncategorization behavior. This potentially provides a novel view into how human\nparticipants distinguish between modal categories, which we explore by\ncorrelating projections along modal difference vectors with human participants'\nratings of interpretable features. In summary, we derive new insights into LM\nmodal categorization using techniques from mechanistic interpretability, with\nthe potential to inform our understanding of modal categorization in humans.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8bc6\u522b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6a21\u6001\u5dee\u5f02\u5411\u91cf\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u6bd4\u5148\u524d\u8ba4\u4e3a\u7684\u66f4\u80fd\u53ef\u9760\u5730\u8fdb\u884c\u6a21\u6001\u5206\u7c7b\uff0c\u5e76\u53d1\u73b0\u8fd9\u4e9b\u5411\u91cf\u80fd\u4ee5\u4e00\u81f4\u7684\u987a\u5e8f\u51fa\u73b0\uff0c\u751a\u81f3\u53ef\u4ee5\u6a21\u62df\u4eba\u7c7b\u7684\u5206\u7c7b\u884c\u4e3a\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u6267\u884c\u5404\u79cd\u4efb\u52a1\u65f6\u9700\u8981\u8bc6\u522b\u53e5\u5b50\u7684\u6a21\u6001\u7c7b\u522b\uff08\u5982\u53ef\u80fd\u6027\u3001\u4e0d\u53ef\u80fd\u6027\u3001\u8352\u8c2c\u6027\uff09\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u5bf9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6a21\u6001\u5206\u7c7b\u7684\u80fd\u529b\u63d0\u51fa\u4e86\u8d28\u7591\u3002", "method": "\u7814\u7a76\u8005\u5728\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u4e2d\u8bc6\u522b\u51fa\u533a\u5206\u6a21\u6001\u7c7b\u522b\u7684\u7ebf\u6027\u8868\u793a\uff0c\u79f0\u4e4b\u4e3a\u201c\u6a21\u6001\u5dee\u5f02\u5411\u91cf\u201d\u3002\u4ed6\u4eec\u5206\u6790\u4e86\u8fd9\u4e9b\u5411\u91cf\u5728\u6a21\u578b\u80fd\u529b\u63d0\u5347\uff08\u8bad\u7ec3\u6b65\u9aa4\u3001\u5c42\u6570\u3001\u53c2\u6570\u91cf\uff09\u65f6\u51fa\u73b0\u7684\u987a\u5e8f\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u5411\u91cf\u7684\u6fc0\u6d3b\u6765\u5efa\u6a21\u4eba\u7c7b\u7ec6\u7c92\u5ea6\u7684\u5206\u7c7b\u884c\u4e3a\uff0c\u901a\u8fc7\u4e0e\u4eba\u7c7b\u5bf9\u53ef\u89e3\u91ca\u7279\u5f81\u7684\u8bc4\u5206\u8fdb\u884c\u5173\u8054\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u62e5\u6709\u6bd4\u5148\u524d\u62a5\u544a\u66f4\u53ef\u9760\u7684\u6a21\u6001\u5206\u7c7b\u5224\u65ad\u80fd\u529b\u3002\u6a21\u6001\u5dee\u5f02\u5411\u91cf\u968f\u7740\u6a21\u578b\u7684\u6210\u719f\u4ee5\u4e00\u81f4\u7684\u987a\u5e8f\u51fa\u73b0\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u5411\u91cf\u53ef\u7528\u4e8e\u5efa\u6a21\u4eba\u7c7b\u7ec6\u7c92\u5ea6\u7684\u6a21\u6001\u5206\u7c7b\u884c\u4e3a\u3002", "conclusion": "\u672c\u7814\u7a76\u5229\u7528\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6280\u672f\u6df1\u5165\u7406\u89e3\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u6001\u5206\u7c7b\u80fd\u529b\uff0c\u5e76\u4e3a\u7406\u89e3\u4eba\u7c7b\u5982\u4f55\u533a\u5206\u6a21\u6001\u7c7b\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2507.12672", "pdf": "https://arxiv.org/pdf/2507.12672", "abs": "https://arxiv.org/abs/2507.12672", "authors": ["Abu-Viskhan A. Umishov", "Vladislav A. Grigorian"], "title": "The first open machine translation system for the Chechen language", "categories": ["cs.CL"], "comment": "7 pages", "summary": "We introduce the first open-source model for translation between the\nvulnerable Chechen language and Russian, and the dataset collected to train and\nevaluate it. We explore fine-tuning capabilities for including a new language\ninto a large language model system for multilingual translation NLLB-200. The\nBLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for\ntranslation from Russian to Chechen and reverse direction, respectively. The\nrelease of the translation models is accompanied by the distribution of\nparallel words, phrases and sentences corpora and multilingual sentence encoder\nadapted to the Chechen language.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u5f00\u6e90\u7684\u8f66\u81e3\u8bed\u4e0e\u4fc4\u8bed\u4e4b\u95f4\u7684\u7ffb\u8bd1\u6a21\u578b\u53ca\u5176\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5fae\u8c03NLLB-200\u5b9e\u73b0\u4e86\u591a\u8bed\u8a00\u7ffb\u8bd1\uff0c\u5e76\u516c\u5e03\u4e86\u76f8\u5173\u8bed\u6599\u548c\u53e5\u7f16\u7801\u5668\u3002", "motivation": "\u65e8\u5728\u4e3a\u6fd2\u5371\u7684\u8f66\u81e3\u8bed\u63d0\u4f9b\u7ffb\u8bd1\u6a21\u578b\uff0c\u5e76\u63a2\u7d22\u5c06\u65b0\u8bed\u8a00\u6574\u5408\u5230\u5927\u578b\u591a\u8bed\u8a00\u7ffb\u8bd1\u7cfb\u7edf\uff08\u5982NLLB-200\uff09\u4e2d\u7684\u5fae\u8c03\u80fd\u529b\uff0c\u586b\u8865\u8be5\u8bed\u8a00\u7ffb\u8bd1\u8d44\u6e90\u7684\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u4e86\u9996\u4e2a\u8f66\u81e3\u8bed\u4e0e\u4fc4\u8bed\u4e4b\u95f4\u7684\u5f00\u6e90\u7ffb\u8bd1\u6a21\u578b\uff0c\u6536\u96c6\u5e76\u4f7f\u7528\u4e86\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u63a2\u7d22\u4e86\u5c06\u65b0\u8bed\u8a00\uff08\u8f66\u81e3\u8bed\uff09\u7eb3\u5165\u5927\u578b\u591a\u8bed\u8a00\u7ffb\u8bd1\u7cfb\u7edfNLLB-200\u7684\u5fae\u8c03\u80fd\u529b\uff0c\u5e76\u53d1\u5e03\u4e86\u5e76\u884c\u8bcd\u8bed\u3001\u77ed\u8bed\u548c\u53e5\u5b50\u8bed\u6599\u5e93\u4ee5\u53ca\u9002\u5e94\u8f66\u81e3\u8bed\u7684\u591a\u8bed\u8a00\u53e5\u7f16\u7801\u5668\u3002", "result": "\u6240\u6784\u5efa\u7684\u6a21\u578b\u5728\u4fc4\u8bed\u5230\u8f66\u81e3\u8bed\u65b9\u5411\u7684BLEU/ChrF++\u5f97\u5206\u5206\u522b\u4e3a8.34/34.69\uff0c\u5728\u53cd\u65b9\u5411\uff08\u8f66\u81e3\u8bed\u5230\u4fc4\u8bed\uff09\u5206\u522b\u4e3a20.89/44.55\u3002\u540c\u65f6\u53d1\u5e03\u4e86\u7ffb\u8bd1\u6a21\u578b\u3001\u5e76\u884c\u8bed\u6599\uff08\u5305\u62ec\u8bcd\u3001\u77ed\u8bed\u548c\u53e5\u5b50\uff09\u4ee5\u53ca\u9002\u5e94\u8f66\u81e3\u8bed\u7684\u591a\u8bed\u8a00\u53e5\u7f16\u7801\u5668\u3002", "conclusion": "\u6210\u529f\u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u5f00\u6e90\u7684\u8f66\u81e3\u8bed-\u4fc4\u8bed\u7ffb\u8bd1\u6a21\u578b\u53ca\u5176\u914d\u5957\u6570\u636e\u96c6\u548c\u5de5\u5177\uff0c\u4e3a\u6fd2\u5371\u8bed\u8a00\u7684\u673a\u5668\u7ffb\u8bd1\u7814\u7a76\u548c\u5e94\u7528\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u5fae\u8c03\u5c06\u65b0\u8bed\u8a00\u6574\u5408\u5230\u73b0\u6709\u5927\u578b\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.12910", "pdf": "https://arxiv.org/pdf/2507.12910", "abs": "https://arxiv.org/abs/2507.12910", "authors": ["Xudong Wang", "Hongyang Du", "Lei Feng", "Kaibin Huang"], "title": "Energy-Efficient RSMA-enabled Low-altitude MEC Optimization Via Generative AI-enhanced Deep Reinforcement Learning", "categories": ["cs.NI"], "comment": "13 pages, 10 figures", "summary": "The growing demand for low-latency computing in 6G is driving the use of\nUAV-based low-altitude mobile edge computing (MEC) systems. However, limited\nspectrum often leads to severe uplink interference among ground terminals\n(GTs). In this paper, we investigate a rate-splitting multiple access\n(RSMA)-enabled low-altitude MEC system, where a UAV-based edge server assists\nmultiple GTs in concurrently offloading their tasks over a shared uplink. We\nformulate a joint optimization problem involving the UAV 3D trajectory, RSMA\ndecoding order, task offloading decisions, and resource allocation, aiming to\nmitigate multi-user interference and maximize energy efficiency. Given the high\ndimensionality, non-convex nature, and dynamic characteristics of this\noptimization problem, we propose a generative AI-enhanced deep reinforcement\nlearning (DRL) framework to solve it efficiently. Specifically, we embed a\ndiffusion model into the actor network to generate high-quality action samples,\nimproving exploration in hybrid action spaces and avoiding local optima. In\naddition, a priority-based RSMA decoding strategy is designed to facilitate\nefficient successive interference cancellation with low complexity. Simulation\nresults demonstrate that the proposed method for low-altitude MEC systems\noutperforms baseline methods, and that integrating GDM with RSMA can achieve\nsignificantly improved energy efficiency performance.", "AI": {"tldr": "\u9488\u5bf9\u65e0\u4eba\u673a\u4f4e\u7a7aMEC\u7cfb\u7edf\u4e2d\u7684\u4e0a\u884c\u5e72\u6270\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eRSMA\u5e76\u7ed3\u5408\u751f\u6210\u5f0fAI\u589e\u5f3a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u65e8\u5728\u6700\u5927\u5316\u80fd\u6548\u5e76\u6709\u6548\u6291\u5236\u5e72\u6270\u3002", "motivation": "6G\u65f6\u4ee3\u4f4e\u5ef6\u8fdf\u8ba1\u7b97\u9700\u6c42\u50ac\u751f\u4e86\u65e0\u4eba\u673a\u4f4e\u7a7a\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u4f46\u6709\u9650\u9891\u8c31\u5bfc\u81f4\u5730\u9762\u7ec8\u7aef\u95f4\u5b58\u5728\u4e25\u91cd\u7684\u4e0a\u884c\u5e72\u6270\u95ee\u9898\uff0c\u4e9f\u9700\u9ad8\u6548\u65b9\u6848\u5e94\u5bf9\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2aRSMA\u8d4b\u80fd\u7684\u4f4e\u7a7aMEC\u7cfb\u7edf\uff0c\u5e76\u8054\u5408\u4f18\u5316\u65e0\u4eba\u673a3D\u8f68\u8ff9\u3001RSMA\u89e3\u7801\u987a\u5e8f\u3001\u4efb\u52a1\u5378\u8f7d\u51b3\u7b56\u53ca\u8d44\u6e90\u5206\u914d\uff0c\u4ee5\u6700\u5927\u5316\u80fd\u6548\u548c\u6291\u5236\u591a\u7528\u6237\u5e72\u6270\u3002\u9488\u5bf9\u8be5\u4f18\u5316\u95ee\u9898\u7684\u9ad8\u7ef4\u3001\u975e\u51f8\u548c\u52a8\u6001\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5f0fAI\u589e\u5f3a\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6846\u67b6\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728Actor\u7f51\u7edc\u4e2d\u5d4c\u5165\u6269\u6563\u6a21\u578b\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u52a8\u4f5c\u6837\u672c\uff0c\u63d0\u9ad8\u6df7\u5408\u52a8\u4f5c\u7a7a\u95f4\u7684\u63a2\u7d22\u80fd\u529b\u5e76\u89c4\u907f\u5c40\u90e8\u6700\u4f18\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5148\u7ea7\u7684RSMA\u89e3\u7801\u7b56\u7565\u4ee5\u5b9e\u73b0\u4f4e\u590d\u6742\u5ea6\u7684\u8fde\u7eed\u5e72\u6270\u6d88\u9664\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4f4e\u7a7aMEC\u7cfb\u7edf\u4e2d\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u7279\u522b\u5730\uff0c\u5c06\u751f\u6210\u6269\u6563\u6a21\u578b\uff08GDM\uff09\u4e0eRSMA\u7ed3\u5408\u80fd\u591f\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u7684\u80fd\u91cf\u6548\u7387\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u7ed3\u5408\u751f\u6210\u5f0fAI\u589e\u5f3aDRL\u4e0eRSMA\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u4f4e\u7a7aMEC\u7cfb\u7edf\u4e2d\u7684\u4e0a\u884c\u5e72\u6270\u548c\u80fd\u6548\u95ee\u9898\uff0c\u4e3a\u672a\u67656G\u901a\u4fe1\u73af\u5883\u4e0b\u7684\u4f4e\u5ef6\u8fdf\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12679", "pdf": "https://arxiv.org/pdf/2507.12679", "abs": "https://arxiv.org/abs/2507.12679", "authors": ["Arthur J. Funnell", "Panayiotis Petousis", "Fabrice Harel-Canada", "Ruby Romero", "Alex A. T. Bui", "Adam Koncsol", "Hritika Chaturvedi", "Chelsea Shover", "David Goodman-Meza"], "title": "Improving Drug Identification in Overdose Death Surveillance using Large Language Models", "categories": ["cs.CL", "q-bio.QM", "I.2.7; J.3"], "comment": "30 pages, 1 figure, 4 tables, 2 supplemental figures, 4 supplemental\n  tables, submitted to Journal of Forensic Sciences (JFS)", "summary": "The rising rate of drug-related deaths in the United States, largely driven\nby fentanyl, requires timely and accurate surveillance. However, critical\noverdose data are often buried in free-text coroner reports, leading to delays\nand information loss when coded into ICD (International Classification of\nDisease)-10 classifications. Natural language processing (NLP) models may\nautomate and enhance overdose surveillance, but prior applications have been\nlimited. A dataset of 35,433 death records from multiple U.S. jurisdictions in\n2020 was used for model training and internal testing. External validation was\nconducted using a novel separate dataset of 3,335 records from 2023-2024.\nMultiple NLP approaches were evaluated for classifying specific drug\ninvolvement from unstructured death certificate text. These included\ntraditional single- and multi-label classifiers, as well as fine-tuned\nencoder-only language models such as Bidirectional Encoder Representations from\nTransformers (BERT) and BioClinicalBERT, and contemporary decoder-only large\nlanguage models such as Qwen 3 and Llama 3. Model performance was assessed\nusing macro-averaged F1 scores, and 95% confidence intervals were calculated to\nquantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect\nperformance, with macro F1 scores >=0.998 on the internal test set. External\nvalidation confirmed robustness (macro F1=0.966), outperforming conventional\nmachine learning, general-domain BERT models, and various decoder-only large\nlanguage models. NLP models, particularly fine-tuned clinical variants like\nBioClinicalBERT, offer a highly accurate and scalable solution for overdose\ndeath classification from free-text reports. These methods can significantly\naccelerate surveillance workflows, overcoming the limitations of manual ICD-10\ncoding and supporting near real-time detection of emerging substance use\ntrends.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u6a21\u578b\uff0c\u7279\u522b\u662f\u5fae\u8c03\u7684BioClinicalBERT\uff0c\u5b9e\u73b0\u4e86\u5bf9\u81ea\u7531\u6587\u672c\u6b7b\u4ea1\u62a5\u544a\u4e2d\u836f\u7269\u8fc7\u91cf\u6b7b\u4ea1\u7684\u51c6\u786e\u5206\u7c7b\uff0c\u6709\u6548\u52a0\u901f\u4e86\u836f\u7269\u76d1\u6d4b\u5e76\u514b\u670d\u4e86\u4f20\u7edf\u7f16\u7801\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7f8e\u56fd\u82ac\u592a\u5c3c\u5bfc\u81f4\u7684\u836f\u7269\u76f8\u5173\u6b7b\u4ea1\u7387\u4e0d\u65ad\u4e0a\u5347\uff0c\u9700\u8981\u53ca\u65f6\u51c6\u786e\u7684\u76d1\u6d4b\u3002\u7136\u800c\uff0c\u5173\u952e\u7684\u8fc7\u91cf\u6570\u636e\u5e38\u57cb\u85cf\u4e8e\u81ea\u7531\u6587\u672c\u7684\u9a8c\u5c38\u5b98\u62a5\u544a\u4e2d\uff0c\u4f20\u7edf\u7f16\u7801\uff08ICD-10\uff09\u5bfc\u81f4\u6570\u636e\u5ef6\u8fdf\u548c\u4fe1\u606f\u4e22\u5931\u3002\u73b0\u6709NLP\u5728\u836f\u7269\u8fc7\u91cf\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\u6709\u9650\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e862020\u5e74\u6765\u81ea\u591a\u4e2a\u7f8e\u56fd\u53f8\u6cd5\u7ba1\u8f96\u533a\u768435,433\u4efd\u6b7b\u4ea1\u8bb0\u5f55\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u5185\u90e8\u6d4b\u8bd5\uff0c\u5e76\u5229\u75282023-2024\u5e743,335\u4efd\u65b0\u8bb0\u5f55\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\u3002\u8bc4\u4f30\u4e86\u591a\u79cdNLP\u65b9\u6cd5\uff0c\u5305\u62ec\u4f20\u7edf\u5355/\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u3001\u5fae\u8c03\u7684\u7f16\u7801\u5668\u6a21\u578b\uff08BERT, BioClinicalBERT\uff09\u548c\u89e3\u7801\u5668\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Qwen 3, Llama 3\uff09\uff0c\u4ee5\u4ece\u975e\u7ed3\u6784\u5316\u6b7b\u4ea1\u8bc1\u660e\u6587\u672c\u4e2d\u5206\u7c7b\u7279\u5b9a\u836f\u7269\u7684\u6d89\u53ca\u60c5\u51b5\u3002\u6a21\u578b\u6027\u80fd\u901a\u8fc7\u5b8f\u89c2\u5e73\u5747F1\u5206\u6570\u548c95%\u7f6e\u4fe1\u533a\u95f4\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5fae\u8c03\u540e\u7684BioClinicalBERT\u6a21\u578b\u5728\u5185\u90e8\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u6027\u80fd\uff08\u5b8f\u89c2F1\u5206\u6570\u22650.998\uff09\u3002\u5916\u90e8\u9a8c\u8bc1\u8bc1\u5b9e\u4e86\u5176\u9c81\u68d2\u6027\uff08\u5b8f\u89c2F1=0.966\uff09\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3001\u901a\u7528BERT\u6a21\u578b\u4ee5\u53ca\u5404\u79cd\u89e3\u7801\u5668\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "NLP\u6a21\u578b\uff0c\u7279\u522b\u662f\u7ecf\u8fc7\u5fae\u8c03\u7684\u4e34\u5e8a\u9886\u57df\u53d8\u4f53\uff08\u5982BioClinicalBERT\uff09\uff0c\u4e3a\u4ece\u81ea\u7531\u6587\u672c\u62a5\u544a\u4e2d\u5206\u7c7b\u8fc7\u91cf\u6b7b\u4ea1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u5ea6\u51c6\u786e\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u52a0\u901f\u76d1\u6d4b\u5de5\u4f5c\u6d41\u7a0b\uff0c\u514b\u670d\u624b\u52a8ICD-10\u7f16\u7801\u7684\u5c40\u9650\u6027\uff0c\u5e76\u652f\u6301\u8fd1\u5b9e\u65f6\u5730\u68c0\u6d4b\u65b0\u5174\u836f\u7269\u6ee5\u7528\u8d8b\u52bf\u3002"}}
{"id": "2507.13140", "pdf": "https://arxiv.org/pdf/2507.13140", "abs": "https://arxiv.org/abs/2507.13140", "authors": ["Kuiyuan Ding", "Caili Guo", "Yang Yang", "Jianzhang Guo"], "title": "RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and Intention-Driven Agents", "categories": ["cs.NI"], "comment": "6 pages, 7 figures", "summary": "Sixth generation (6G) networks demand tight integration of artificial\nintelligence (AI) into radio access networks (RANs) to meet stringent quality\nof service (QoS) and resource efficiency requirements. Existing solutions\nstruggle to bridge the gap between high level user intents and the low level,\nparameterized configurations required for optimal performance. To address this\nchallenge, we propose RIDAS, a multi agent framework composed of representation\ndriven agents (RDAs) and an intention driven agent (IDA). RDAs expose open\ninterface with tunable control parameters (rank and quantization bits, enabling\nexplicit trade) offs between distortion and transmission rate. The IDA employs\na two stage planning scheme (bandwidth pre allocation and reallocation) driven\nby a large language model (LLM) to map user intents and system state into\noptimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\\%\nmore users than WirelessAgent under equivalent QoS constraints. These results\nvalidate ability of RIDAS to capture user intent and allocate resources more\nefficiently in AI RAN environments.", "AI": {"tldr": "\u63d0\u51faRIDAS\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u7528\u6237\u610f\u56fe\u8f6c\u5316\u4e3a6G AI RAN\u8d44\u6e90\u914d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u7528\u6237\u652f\u6301\u6570\u91cf\u3002", "motivation": "\u7b2c\u516d\u4ee3\uff086G\uff09\u7f51\u7edc\u8981\u6c42\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u4e0e\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff08RAN\uff09\u7d27\u5bc6\u96c6\u6210\uff0c\u4ee5\u6ee1\u8db3\u4e25\u683c\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u548c\u8d44\u6e90\u6548\u7387\u9700\u6c42\u3002\u7136\u800c\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u96be\u4ee5\u5c06\u9ad8\u5c42\u7528\u6237\u610f\u56fe\u8f6c\u5316\u4e3a\u4f4e\u5c42\u3001\u53c2\u6570\u5316\u7684\u914d\u7f6e\u4ee5\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002", "method": "\u63d0\u51faRIDAS\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7531\u8868\u5f81\u9a71\u52a8\u4ee3\u7406\uff08RDAs\uff09\u548c\u610f\u56fe\u9a71\u52a8\u4ee3\u7406\uff08IDA\uff09\u7ec4\u6210\u3002RDAs\u63d0\u4f9b\u53ef\u8c03\u63a7\u5236\u53c2\u6570\u7684\u5f00\u653e\u63a5\u53e3\uff08\u5982\u79e9\u548c\u91cf\u5316\u6bd4\u7279\uff09\uff0c\u5b9e\u73b0\u5931\u771f\u4e0e\u4f20\u8f93\u901f\u7387\u4e4b\u95f4\u7684\u660e\u786e\u6743\u8861\u3002IDA\u91c7\u7528\u7531\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u4e24\u9636\u6bb5\u89c4\u5212\u65b9\u6848\uff08\u5e26\u5bbd\u9884\u5206\u914d\u548c\u91cd\u5206\u914d\uff09\uff0c\u5c06\u7528\u6237\u610f\u56fe\u548c\u7cfb\u7edf\u72b6\u6001\u6620\u5c04\u5230\u6700\u4f73RDA\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u76f8\u540c\u7684QoS\u7ea6\u675f\u4e0b\uff0cRIDAS\u6bd4WirelessAgent\u652f\u6301\u7684\u7528\u6237\u6570\u91cf\u591a44.71%\u3002", "conclusion": "RIDAS\u80fd\u591f\u6709\u6548\u6355\u83b7\u7528\u6237\u610f\u56fe\uff0c\u5e76\u5728AI RAN\u73af\u5883\u4e2d\u66f4\u9ad8\u6548\u5730\u5206\u914d\u8d44\u6e90\u3002"}}
{"id": "2507.12490", "pdf": "https://arxiv.org/pdf/2507.12490", "abs": "https://arxiv.org/abs/2507.12490", "authors": ["Maximiliano Hormaz\u00e1bal Lagos", "H\u00e9ctor Cerezo-Costas", "Dimosthenis Karatzas"], "title": "Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "This work has been accepted for presentation at the 16th Conference\n  and Labs of the Evaluation Forum (CLEF 2025) and will be published in the\n  proceedings by Springer in the Lecture Notes in Computer Science (LNCS)\n  series. Please cite the published version when available", "summary": "We introduce EaGERS, a fully training-free and model-agnostic pipeline that\n(1) generates natural language rationales via a vision language model, (2)\ngrounds these rationales to spatial sub-regions by computing multimodal\nembedding similarities over a configurable grid with majority voting, and (3)\nrestricts the generation of responses only from the relevant regions selected\nin the masked image. Experiments on the DocVQA dataset demonstrate that our\nbest configuration not only outperforms the base model on exact match accuracy\nand Average Normalized Levenshtein Similarity metrics but also enhances\ntransparency and reproducibility in DocVQA without additional model\nfine-tuning.", "AI": {"tldr": "EaGERS\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u7684DocVQA\u6d41\u6c34\u7ebf\uff0c\u5b83\u901a\u8fc7VLM\u751f\u6210\u7406\u7531\uff0c\u5c06\u5176\u5b9a\u4f4d\u5230\u56fe\u50cf\u533a\u57df\uff0c\u5e76\u9650\u5236\u5728\u76f8\u5173\u533a\u57df\u751f\u6210\u54cd\u5e94\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u6027\u80fd\u3001\u900f\u660e\u5ea6\u548c\u53ef\u590d\u73b0\u6027\u3002", "motivation": "\u63d0\u9ad8DocVQA\u4efb\u52a1\u7684\u6027\u80fd\u3001\u900f\u660e\u5ea6\u548c\u53ef\u590d\u73b0\u6027\uff0c\u540c\u65f6\u907f\u514d\u989d\u5916\u7684\u6a21\u578b\u5fae\u8c03\u3002", "method": "\u5f15\u5165EaGERS\u6d41\u6c34\u7ebf\uff0c\u8be5\u65b9\u6cd5\u5b8c\u5168\u65e0\u9700\u8bad\u7ec3\u4e14\u4e0e\u6a21\u578b\u65e0\u5173\u3002\u5176\u6b65\u9aa4\u5305\u62ec\uff1a1) \u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u81ea\u7136\u8bed\u8a00\u7406\u7531\uff1b2) \u901a\u8fc7\u8ba1\u7b97\u591a\u6a21\u6001\u5d4c\u5165\u76f8\u4f3c\u5ea6\u5e76\u5728\u53ef\u914d\u7f6e\u7f51\u683c\u4e0a\u8fdb\u884c\u591a\u6570\u6295\u7968\uff0c\u5c06\u7406\u7531\u5b9a\u4f4d\u5230\u7a7a\u95f4\u5b50\u533a\u57df\uff1b3) \u9650\u5236\u54cd\u5e94\u4ec5\u4ece\u63a9\u7801\u56fe\u50cf\u4e2d\u9009\u62e9\u7684\u76f8\u5173\u533a\u57df\u751f\u6210\u3002", "result": "\u5728DocVQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEaGERS\u7684\u6700\u4f73\u914d\u7f6e\u5728\u7cbe\u786e\u5339\u914d\u51c6\u786e\u7387\u548c\u5e73\u5747\u5f52\u4e00\u5316Levenshtein\u76f8\u4f3c\u5ea6\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u5728\u4e0d\u8fdb\u884c\u989d\u5916\u6a21\u578b\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a\u4e86DocVQA\u7684\u900f\u660e\u5ea6\u548c\u53ef\u590d\u73b0\u6027\u3002", "conclusion": "EaGERS\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86DocVQA\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8fd8\u663e\u8457\u589e\u5f3a\u4e86\u7ed3\u679c\u7684\u900f\u660e\u5ea6\u548c\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2507.12484", "pdf": "https://arxiv.org/pdf/2507.12484", "abs": "https://arxiv.org/abs/2507.12484", "authors": ["Jaros\u0142aw A. Chudziak", "Adam Kostka"], "title": "AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education", "categories": ["cs.AI", "cs.MA"], "comment": "8 pages, 5 figures", "summary": "The growing ubiquity of artificial intelligence (AI), in particular large\nlanguage models (LLMs), has profoundly altered the way in which learners gain\nknowledge and interact with learning material, with many claiming that AI\npositively influences their learning achievements. Despite this advancement,\ncurrent AI tutoring systems face limitations associated with their reactive\nnature, often providing direct answers without encouraging deep reflection or\nincorporating structured pedagogical tools and strategies. This limitation is\nmost apparent in the field of mathematics, in which AI tutoring systems remain\nunderdeveloped. This research addresses the question: How can AI tutoring\nsystems move beyond providing reactive assistance to enable structured,\nindividualized, and tool-assisted learning experiences? We introduce a novel\nmulti-agent AI tutoring platform that combines adaptive and personalized\nfeedback, structured course generation, and textbook knowledge retrieval to\nenable modular, tool-assisted learning processes. This system allows students\nto learn new topics while identifying and targeting their weaknesses, revise\nfor exams effectively, and practice on an unlimited number of personalized\nexercises. This article contributes to the field of artificial intelligence in\neducation by introducing a novel platform that brings together pedagogical\nagents and AI-driven components, augmenting the field with modular and\neffective systems for teaching mathematics.", "AI": {"tldr": "\u9488\u5bf9\u5f53\u524dAI\u5bb6\u6559\u7cfb\u7edf\u5728\u6570\u5b66\u9886\u57df\u53cd\u5e94\u5f0f\u4e14\u7f3a\u4e4f\u6df1\u5ea6\u6559\u5b66\u7684\u5c40\u9650\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53AI\u5bb6\u6559\u5e73\u53f0\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u53cd\u9988\u3001\u7ed3\u6784\u5316\u8bfe\u7a0b\u751f\u6210\u548c\u77e5\u8bc6\u68c0\u7d22\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u3001\u4e2a\u6027\u5316\u4e14\u5de5\u5177\u8f85\u52a9\u7684\u6570\u5b66\u5b66\u4e60\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709AI\u5bb6\u6559\u7cfb\u7edf\uff0c\u5c24\u5176\u5728\u6570\u5b66\u9886\u57df\uff0c\u4ec5\u63d0\u4f9b\u53cd\u5e94\u5f0f\u7b54\u6848\uff0c\u672a\u80fd\u6709\u6548\u9f13\u52b1\u6df1\u5ea6\u53cd\u601d\u6216\u6574\u5408\u7ed3\u6784\u5316\u6559\u5b66\u5de5\u5177\u4e0e\u7b56\u7565\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u6559\u5b66\u6548\u679c\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53AI\u5bb6\u6559\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u901a\u8fc7\u7ed3\u5408\u81ea\u9002\u5e94\u548c\u4e2a\u6027\u5316\u53cd\u9988\u3001\u7ed3\u6784\u5316\u8bfe\u7a0b\u751f\u6210\u4ee5\u53ca\u6559\u6750\u77e5\u8bc6\u68c0\u7d22\uff0c\u5b9e\u73b0\u4e86\u6a21\u5757\u5316\u3001\u5de5\u5177\u8f85\u52a9\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u5e2e\u52a9\u5b66\u751f\u5b66\u4e60\u65b0\u4e3b\u9898\u5e76\u8bc6\u522b\u5f31\u70b9\u3001\u6709\u6548\u590d\u4e60\u8003\u8bd5\uff0c\u4ee5\u53ca\u8fdb\u884c\u65e0\u9650\u91cf\u7684\u4e2a\u6027\u5316\u7ec3\u4e60\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u6574\u5408\u6559\u5b66\u667a\u80fd\u4f53\u548cAI\u9a71\u52a8\u7ec4\u4ef6\u7684\u65b0\u5e73\u53f0\uff0c\u4e3a\u6559\u80b2\u9886\u57df\u7684AI\u7814\u7a76\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u3001\u9ad8\u6548\u7684\u6570\u5b66\u6559\u5b66\u7cfb\u7edf\u3002"}}
{"id": "2507.12507", "pdf": "https://arxiv.org/pdf/2507.12507", "abs": "https://arxiv.org/abs/2507.12507", "authors": ["Mingjie Liu", "Shizhe Diao", "Jian Hu", "Ximing Lu", "Xin Dong", "Hao Zhang", "Alexander Bukharin", "Shaokun Zhang", "Jiaqi Zeng", "Makesh Narsimhan Sreedhar", "Gerald Shen", "David Mosallanezhad", "Di Zhang", "Jonas Yang", "June Yang", "Oleksii Kuchaiev", "Guilin Liu", "Zhiding Yu", "Pavlo Molchanov", "Yejin Choi", "Jan Kautz", "Yi Dong"], "title": "Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "14 pages, 7 figures", "summary": "Recent advancements in reasoning-focused language models such as OpenAI's O1\nand DeepSeek-R1 have shown that scaling test-time computation-through\nchain-of-thought reasoning and iterative exploration-can yield substantial\nimprovements on complex tasks like mathematics and code generation. These\nbreakthroughs have been driven by large-scale reinforcement learning (RL),\nparticularly when combined with verifiable reward signals that provide\nobjective and grounded supervision. In this report, we investigate the effects\nof prolonged reinforcement learning on a small language model across a diverse\nset of reasoning domains. Our work identifies several key ingredients for\neffective training, including the use of verifiable reward tasks, enhancements\nto Group Relative Policy Optimization (GRPO), and practical techniques to\nimprove training stability and generalization. We introduce controlled KL\nregularization, clipping ratio, and periodic reference policy resets as\ncritical components for unlocking long-term performance gains. Our model\nachieves significant improvements over strong baselines, including +14.7% on\nmath, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate\ncontinued research, we release our model publicly.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u957f\u671f\u5f3a\u5316\u5b66\u4e60\u5bf9\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u63a8\u7406\u9886\u57df\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u9a8c\u8bc1\u5956\u52b1\u3001\u589e\u5f3aGRPO\u548c\u4f18\u5316\u8bad\u7ec3\u6280\u672f\uff0c\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u903b\u8f91\u8c1c\u9898\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982O1\u548cDeepSeek-R1\uff09\u5df2\u901a\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u5728\u6570\u5b66\u548c\u4ee3\u7801\u751f\u6210\u7b49\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u91cd\u5927\u7a81\u7834\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u957f\u671f\u5f3a\u5316\u5b66\u4e60\u5bf9\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6548\u679c\uff0c\u5e76\u8bc6\u522b\u6709\u6548\u8bad\u7ec3\u7684\u5173\u952e\u8981\u7d20\u3002", "method": "\u5bf9\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u957f\u671f\u5f3a\u5316\u5b66\u4e60\uff0c\u5e94\u7528\u4e8e\u591a\u6837\u5316\u7684\u63a8\u7406\u9886\u57df\u3002\u6838\u5fc3\u65b9\u6cd5\u5305\u62ec\uff1a\u4f7f\u7528\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u4efb\u52a1\uff0c\u589e\u5f3aGroup Relative Policy Optimization (GRPO)\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u53d7\u63a7KL\u6b63\u5219\u5316\u3001\u88c1\u526a\u6bd4\u4f8b\u548c\u5468\u671f\u6027\u53c2\u8003\u7b56\u7565\u91cd\u7f6e\u7b49\u5b9e\u8df5\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8be5\u6a21\u578b\u5728\u5f3a\u57fa\u7ebf\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff1a\u6570\u5b66\u4efb\u52a1\u63d0\u534714.7%\uff0c\u7f16\u7a0b\u4efb\u52a1\u63d0\u534713.9%\uff0c\u903b\u8f91\u8c1c\u9898\u4efb\u52a1\u63d0\u534754.8%\u3002\u4e3a\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\uff0c\u6a21\u578b\u5df2\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "\u901a\u8fc7\u957f\u671f\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u53ef\u9a8c\u8bc1\u5956\u52b1\u3001GRPO\u7684\u589e\u5f3a\u4ee5\u53ca\u7279\u5b9a\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u6280\u672f\uff08\u5982\u53d7\u63a7KL\u6b63\u5219\u5316\u548c\u5468\u671f\u6027\u53c2\u8003\u7b56\u7565\u91cd\u7f6e\uff09\uff0c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e5f\u80fd\u5728\u591a\u4e2a\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u663e\u8457\u4e14\u6301\u4e45\u7684\u6027\u80fd\u63d0\u5347\u3002\u8fd9\u4e9b\u8981\u7d20\u5bf9\u4e8e\u91ca\u653e\u957f\u671f\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.12695", "pdf": "https://arxiv.org/pdf/2507.12695", "abs": "https://arxiv.org/abs/2507.12695", "authors": ["S M Rafiuddin", "Sadia Kamal", "Mohammed Rakib", "Arunkumar Bagavathi", "Atriya Sen"], "title": "AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis", "categories": ["cs.CL"], "comment": "12 pages (including references), 2 figures (Fig. 1 overview, Fig. 2\n  hyperparameter sensitivity with two subplots), 6 tables (performance,\n  ablation, dataset stats, case studies, etc.), accepted at ASONAM 2025 (Social\n  Network Analysis and Mining)", "summary": "We introduce AdaptiSent, a new framework for Multimodal Aspect-Based\nSentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms\nto improve sentiment classification and aspect term extraction from both text\nand images. Our model integrates dynamic modality weighting and\ncontext-adaptive attention, enhancing the extraction of sentiment and\naspect-related information by focusing on how textual cues and visual context\ninteract. We tested our approach against several baselines, including\ntraditional text-based models and other multimodal methods. Results from\nstandard Twitter datasets show that AdaptiSent surpasses existing models in\nprecision, recall, and F1 score, and is particularly effective in identifying\nnuanced inter-modal relationships that are crucial for accurate sentiment and\naspect term extraction. This effectiveness comes from the model's ability to\nadjust its focus dynamically based on the context's relevance, improving the\ndepth and accuracy of sentiment analysis across various multimodal data sets.\nAdaptiSent sets a new standard for MABSA, significantly outperforming current\nmethods, especially in understanding complex multimodal information.", "AI": {"tldr": "AdaptiSent\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u65b9\u9762\u60c5\u611f\u5206\u6790\uff08MABSA\uff09\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u5347\u4e86\u4ece\u6587\u672c\u548c\u56fe\u50cf\u4e2d\u8fdb\u884c\u60c5\u611f\u5206\u7c7b\u548c\u65b9\u9762\u672f\u8bed\u63d0\u53d6\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u65b9\u9762\u60c5\u611f\u5206\u6790\u4e2d\u5bf9\u6587\u672c\u548c\u56fe\u50cf\u7684\u7ed3\u5408\u5904\u7406\u4e0d\u591f\u9ad8\u6548\uff0c\u7279\u522b\u662f\u96be\u4ee5\u51c6\u786e\u6355\u6349\u8de8\u6a21\u6001\u7684\u7ec6\u5fae\u5173\u7cfb\uff0c\u4ece\u800c\u5f71\u54cd\u60c5\u611f\u5206\u7c7b\u548c\u65b9\u9762\u672f\u8bed\u63d0\u53d6\u7684\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165AdaptiSent\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u52a8\u6001\u6a21\u6001\u52a0\u6743\u548c\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e13\u6ce8\u4e8e\u6587\u672c\u7ebf\u7d22\u548c\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u4ea4\u4e92\u3002\u901a\u8fc7\u5728\u6807\u51c6Twitter\u6570\u636e\u96c6\u4e0a\u4e0e\u4f20\u7edf\u6587\u672c\u6a21\u578b\u53ca\u5176\u4ed6\u591a\u6a21\u6001\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u6d4b\u8bd5\u6765\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "\u5728\u6807\u51c6Twitter\u6570\u636e\u96c6\u4e0a\uff0cAdaptiSent\u5728\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u3002\u5b83\u5c24\u5176\u64c5\u957f\u8bc6\u522b\u5bf9\u4e8e\u51c6\u786e\u60c5\u611f\u548c\u65b9\u9762\u672f\u8bed\u63d0\u53d6\u81f3\u5173\u91cd\u8981\u7684\u7ec6\u5fae\u8de8\u6a21\u6001\u5173\u7cfb\u3002", "conclusion": "AdaptiSent\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5173\u6ce8\u70b9\u4ee5\u9002\u5e94\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u7684\u6df1\u5ea6\u548c\u51c6\u786e\u6027\u3002\u5b83\u4e3aMABSA\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u7279\u522b\u662f\u5728\u7406\u89e3\u590d\u6742\u591a\u6a21\u6001\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u5353\u8d8a\u3002"}}
