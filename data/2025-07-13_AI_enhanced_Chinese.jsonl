{"id": "2507.07186", "pdf": "https://arxiv.org/pdf/2507.07186", "abs": "https://arxiv.org/abs/2507.07186", "authors": ["Itay Itzhak", "Yonatan Belinkov", "Gabriel Stanovsky"], "title": "Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "CoLM 2025", "summary": "Large language models (LLMs) exhibit cognitive biases -- systematic\ntendencies of irrational decision-making, similar to those seen in humans.\nPrior work has found that these biases vary across models and can be amplified\nby instruction tuning. However, it remains unclear if these differences in\nbiases stem from pretraining, finetuning, or even random noise due to training\nstochasticity. We propose a two-step causal experimental approach to\ndisentangle these factors. First, we finetune models multiple times using\ndifferent random seeds to study how training randomness affects over $30$\ncognitive biases. Second, we introduce \\emph{cross-tuning} -- swapping\ninstruction datasets between models to isolate bias sources. This swap uses\ndatasets that led to different bias patterns, directly testing whether biases\nare dataset-dependent. Our findings reveal that while training randomness\nintroduces some variability, biases are mainly shaped by pretraining: models\nwith the same pretrained backbone exhibit more similar bias patterns than those\nsharing only finetuning data. These insights suggest that understanding biases\nin finetuned models requires considering their pretraining origins beyond\nfinetuning effects. This perspective can guide future efforts to develop\nprincipled strategies for evaluating and mitigating bias in LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4e24\u6b65\u56e0\u679c\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u4ee5\u89e3\u8026\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8ba4\u77e5\u504f\u5dee\u7684\u6765\u6e90\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u662f\u5851\u9020\u504f\u5dee\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u800c\u975e\u5fae\u8c03\u6216\u8bad\u7ec3\u968f\u673a\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8868\u73b0\u51fa\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u8ba4\u77e5\u504f\u5dee\u3002\u5148\u524d\u5de5\u4f5c\u53d1\u73b0\u8fd9\u4e9b\u504f\u5dee\u56e0\u6a21\u578b\u800c\u5f02\uff0c\u5e76\u53ef\u88ab\u6307\u4ee4\u5fae\u8c03\u653e\u5927\u3002\u4f46\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u504f\u5dee\u5dee\u5f02\u662f\u6e90\u4e8e\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u8fd8\u662f\u8bad\u7ec3\u968f\u673a\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e24\u6b65\u56e0\u679c\u5b9e\u9a8c\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u968f\u673a\u79cd\u5b50\u591a\u6b21\u5fae\u8c03\u6a21\u578b\uff0c\u7814\u7a76\u8bad\u7ec3\u968f\u673a\u6027\u5bf930\u591a\u79cd\u8ba4\u77e5\u504f\u5dee\u7684\u5f71\u54cd\u3002\u5176\u6b21\uff0c\u5f15\u5165\u201c\u4ea4\u53c9\u5fae\u8c03\u201d\u65b9\u6cd5\uff0c\u5373\u5728\u6a21\u578b\u95f4\u4ea4\u6362\u5bfc\u81f4\u4e0d\u540c\u504f\u5dee\u6a21\u5f0f\u7684\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u4ee5\u9694\u79bb\u504f\u5dee\u6765\u6e90\u5e76\u76f4\u63a5\u6d4b\u8bd5\u504f\u5dee\u662f\u5426\u4f9d\u8d56\u4e8e\u6570\u636e\u96c6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136\u8bad\u7ec3\u968f\u673a\u6027\u4f1a\u5f15\u5165\u4e00\u5b9a\u53d8\u5f02\u6027\uff0c\u4f46\u504f\u5dee\u4e3b\u8981\u7531\u9884\u8bad\u7ec3\u5851\u9020\uff1a\u5177\u6709\u76f8\u540c\u9884\u8bad\u7ec3\u4e3b\u5e72\u7684\u6a21\u578b\u6bd4\u4ec5\u5171\u4eab\u5fae\u8c03\u6570\u636e\u7684\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u76f8\u4f3c\u7684\u504f\u5dee\u6a21\u5f0f\u3002", "conclusion": "\u8fd9\u4e9b\u89c1\u89e3\u8868\u660e\uff0c\u7406\u89e3\u5fae\u8c03\u6a21\u578b\u4e2d\u7684\u504f\u5dee\u9700\u8981\u8003\u8651\u5176\u9884\u8bad\u7ec3\u8d77\u6e90\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5fae\u8c03\u6548\u5e94\u3002\u8fd9\u4e00\u89c6\u89d2\u53ef\u4ee5\u6307\u5bfc\u672a\u6765\u5f00\u53d1\u8bc4\u4f30\u548c\u7f13\u89e3LLM\u504f\u5dee\u7684\u539f\u5219\u6027\u7b56\u7565\u3002"}}
{"id": "2507.07188", "pdf": "https://arxiv.org/pdf/2507.07188", "abs": "https://arxiv.org/abs/2507.07188", "authors": ["Jens Rupprecht", "Georg Ahnert", "Markus Strohmaier"], "title": "Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses", "categories": ["cs.CL", "cs.AI", "cs.CY", "J.4"], "comment": "18 pages, 17 figures", "summary": "Large Language Models (LLMs) are increasingly used as proxies for human\nsubjects in social science surveys, but their reliability and susceptibility to\nknown response biases are poorly understood. This paper investigates the\nresponse robustness of LLMs in normative survey contexts -- we test nine\ndiverse LLMs on questions from the World Values Survey (WVS), applying a\ncomprehensive set of 11 perturbations to both question phrasing and answer\noption structure, resulting in over 167,000 simulated interviews. In doing so,\nwe not only reveal LLMs' vulnerabilities to perturbations but also reveal that\nall tested models exhibit a consistent \\textit{recency bias} varying in\nintensity, disproportionately favoring the last-presented answer option. While\nlarger models are generally more robust, all models remain sensitive to\nsemantic variations like paraphrasing and to combined perturbations. By\napplying a set of perturbations, we reveal that LLMs partially align with\nsurvey response biases identified in humans. This underscores the critical\nimportance of prompt design and robustness testing when using LLMs to generate\nsynthetic survey data.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u793e\u4f1a\u79d1\u5b66\u8c03\u67e5\u4e2d\u4f5c\u4e3a\u4eba\u7c7b\u4ee3\u7406\u7684\u54cd\u5e94\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u5b83\u4eec\u5b58\u5728\u8fd1\u56e0\u504f\u5dee\uff0c\u5e76\u5bf9\u95ee\u9898\u63aa\u8f9e\u548c\u7b54\u6848\u9009\u9879\u7684\u6270\u52a8\u654f\u611f\uff0c\u5f3a\u8c03\u4e86\u63d0\u793a\u8bbe\u8ba1\u548c\u9c81\u68d2\u6027\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\u3002", "motivation": "LLMs\u5728\u793e\u4f1a\u79d1\u5b66\u8c03\u67e5\u4e2d\u88ab\u65e5\u76ca\u7528\u4f5c\u4eba\u7c7b\u53d7\u8bd5\u8005\u7684\u66ff\u4ee3\uff0c\u4f46\u5176\u53ef\u9760\u6027\u53ca\u5176\u5bf9\u5df2\u77e5\u54cd\u5e94\u504f\u5dee\u7684\u654f\u611f\u6027\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u6d4b\u8bd5\u4e86\u4e5d\u4e2a\u4e0d\u540c\u7684LLMs\uff0c\u4f7f\u7528\u4e16\u754c\u4ef7\u503c\u89c2\u8c03\u67e5\uff08WVS\uff09\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u5bf9\u95ee\u9898\u63aa\u8f9e\u548c\u7b54\u6848\u9009\u9879\u7ed3\u6784\u5e94\u7528\u4e8611\u79cd\u6270\u52a8\uff0c\u8fdb\u884c\u4e86\u8d85\u8fc7167,000\u6b21\u6a21\u62df\u8bbf\u8c08\u4ee5\u8bc4\u4f30\u54cd\u5e94\u9c81\u68d2\u6027\u3002", "result": "\u6240\u6709LLMs\u5747\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u201c\u8fd1\u56e0\u504f\u5dee\u201d\uff0c\u503e\u5411\u4e8e\u9009\u62e9\u6700\u540e\u4e00\u4e2a\u9009\u9879\uff1b\u5927\u578b\u6a21\u578b\u901a\u5e38\u66f4\u9c81\u68d2\uff0c\u4f46\u6240\u6709\u6a21\u578b\u4ecd\u5bf9\u8bed\u4e49\u53d8\u4f53\u548c\u7ec4\u5408\u6270\u52a8\u654f\u611f\uff1bLLMs\u7684\u8c03\u67e5\u54cd\u5e94\u504f\u5dee\u4e0e\u4eba\u7c7b\u7684\u504f\u5dee\u90e8\u5206\u4e00\u81f4\u3002", "conclusion": "\u5728\u4f7f\u7528LLMs\u751f\u6210\u5408\u6210\u8c03\u67e5\u6570\u636e\u65f6\uff0c\u63d0\u793a\u8bbe\u8ba1\u548c\u9c81\u68d2\u6027\u6d4b\u8bd5\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u5e94\u5bf9\u5176\u54cd\u5e94\u504f\u5dee\u548c\u5bf9\u6270\u52a8\u7684\u654f\u611f\u6027\u3002"}}
{"id": "2507.07229", "pdf": "https://arxiv.org/pdf/2507.07229", "abs": "https://arxiv.org/abs/2507.07229", "authors": ["Krithika Ramesh", "Daniel Smolyak", "Zihao Zhao", "Nupoor Gandhi", "Ritu Agarwal", "Margr\u00e9t Bjarnad\u00f3ttir", "Anjalie Field"], "title": "SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains", "categories": ["cs.CL"], "comment": null, "summary": "We present SynthTextEval, a toolkit for conducting comprehensive evaluations\nof synthetic text. The fluency of large language model (LLM) outputs has made\nsynthetic text potentially viable for numerous applications, such as reducing\nthe risks of privacy violations in the development and deployment of AI systems\nin high-stakes domains. Realizing this potential, however, requires principled\nconsistent evaluations of synthetic data across multiple dimensions: its\nutility in downstream systems, the fairness of these systems, the risk of\nprivacy leakage, general distributional differences from the source text, and\nqualitative feedback from domain experts. SynthTextEval allows users to conduct\nevaluations along all of these dimensions over synthetic data that they upload\nor generate using the toolkit's generation module. While our toolkit can be run\nover any data, we highlight its functionality and effectiveness over datasets\nfrom two high-stakes domains: healthcare and law. By consolidating and\nstandardizing evaluation metrics, we aim to improve the viability of synthetic\ntext, and in-turn, privacy-preservation in AI development.", "AI": {"tldr": "SynthTextEval\u662f\u4e00\u4e2a\u5168\u9762\u8bc4\u4f30\u5408\u6210\u6587\u672c\u7684\u5de5\u5177\u5305\uff0c\u65e8\u5728\u901a\u8fc7\u6807\u51c6\u5316\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u63d0\u5347\u5408\u6210\u6587\u672c\u5728AI\u5f00\u53d1\u4e2d\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u53ef\u7528\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u5408\u6210\u6587\u672c\u5728\u964d\u4f4eAI\u7cfb\u7edf\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002\u7136\u800c\uff0c\u8981\u5b9e\u73b0\u8fd9\u4e00\u6f5c\u529b\uff0c\u9700\u8981\u5bf9\u5408\u6210\u6570\u636e\u8fdb\u884c\u591a\u7ef4\u5ea6\uff08\u6548\u7528\u3001\u516c\u5e73\u6027\u3001\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3001\u5206\u5e03\u5dee\u5f02\u3001\u4e13\u5bb6\u53cd\u9988\uff09\u7684\u7cfb\u7edf\u6027\u3001\u4e00\u81f4\u6027\u8bc4\u4f30\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86SynthTextEval\u5de5\u5177\u5305\uff0c\u5141\u8bb8\u7528\u6237\u4e0a\u4f20\u6216\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u5bf9\u8fd9\u4e9b\u6570\u636e\u5728\u591a\u4e2a\u5173\u952e\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u8be5\u5de5\u5177\u5305\u5df2\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u533b\u7597\u548c\u6cd5\u5f8b\uff09\u7684\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u529f\u80fd\u548c\u6709\u6548\u6027\uff0c\u901a\u8fc7\u6574\u5408\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\u6765\u5b9e\u73b0\u5176\u76ee\u6807\u3002", "result": "SynthTextEval\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6574\u5408\u4e14\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u5f97\u7528\u6237\u80fd\u591f\u5bf9\u5408\u6210\u6587\u672c\u8fdb\u884c\u5168\u9762\u7684\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u4ece\u800c\u63d0\u5347\u5176\u6027\u80fd\u6d1e\u5bdf\u548c\u5728AI\u5f00\u53d1\u4e2d\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u53ef\u884c\u6027\u3002", "conclusion": "SynthTextEval\u901a\u8fc7\u7edf\u4e00\u548c\u6807\u51c6\u5316\u5408\u6210\u6587\u672c\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u5408\u6210\u6587\u672c\u7684\u53ef\u7528\u6027\uff0c\u8fdb\u800c\u4fc3\u8fdbAI\u5f00\u53d1\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2507.07248", "pdf": "https://arxiv.org/pdf/2507.07248", "abs": "https://arxiv.org/abs/2507.07248", "authors": ["Minseon Kim", "Jean-Philippe Corbeil", "Alessandro Sordoni", "Francois Beaulieu", "Paul Vozila"], "title": "Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings", "categories": ["cs.CL"], "comment": null, "summary": "As the performance of large language models (LLMs) continues to advance,\ntheir adoption is expanding across a wide range of domains, including the\nmedical field. The integration of LLMs into medical applications raises\ncritical safety concerns, particularly due to their use by users with diverse\nroles, e.g. patients and clinicians, and the potential for model's outputs to\ndirectly affect human health. Despite the domain-specific capabilities of\nmedical LLMs, prior safety evaluations have largely focused only on general\nsafety benchmarks. In this paper, we introduce a safety evaluation protocol\ntailored to the medical domain in both patient user and clinician user\nperspectives, alongside general safety assessments and quantitatively analyze\nthe safety of medical LLMs. We bridge a gap in the literature by building the\nPatientSafetyBench containing 466 samples over 5 critical categories to measure\nsafety from the perspective of the patient. We apply our red-teaming protocols\non the MediPhi model collection as a case study. To our knowledge, this is the\nfirst work to define safety evaluation criteria for medical LLMs through\ntargeted red-teaming taking three different points of view - patient,\nclinician, and general user - establishing a foundation for safer deployment in\nmedical domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u533b\u7597\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u5957\u4ece\u60a3\u8005\u3001\u4e34\u5e8a\u533b\u751f\u53ca\u901a\u7528\u7528\u6237\u591a\u89c6\u89d2\u51fa\u53d1\u7684\u5b9a\u5236\u5316\u5b89\u5168\u8bc4\u4f30\u534f\u8bae\uff0c\u5e76\u6784\u5efa\u4e86PatientSafetyBench\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4e3a\u533b\u7597LLMs\u7684\u5b89\u5168\u90e8\u7f72\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u7597\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5b89\u5168\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u5c24\u5176\u8003\u8651\u5230\u4e0d\u540c\u7528\u6237\u89d2\u8272\uff08\u60a3\u8005\u3001\u4e34\u5e8a\u533b\u751f\uff09\u53ca\u6a21\u578b\u8f93\u51fa\u5bf9\u4eba\u7c7b\u5065\u5eb7\u53ef\u80fd\u4ea7\u751f\u7684\u76f4\u63a5\u5f71\u54cd\u3002\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u4fa7\u91cd\u901a\u7528\u57fa\u51c6\uff0c\u7f3a\u4e4f\u9488\u5bf9\u533b\u7597\u9886\u57df\u7279\u6709\u98ce\u9669\u7684\u4e13\u4e1a\u8bc4\u4f30\u3002", "method": "\u5f15\u5165\u4e00\u5957\u9488\u5bf9\u533b\u7597\u9886\u57df\u5b9a\u5236\u7684\u5b89\u5168\u8bc4\u4f30\u534f\u8bae\uff0c\u8be5\u534f\u8bae\u6db5\u76d6\u60a3\u8005\u7528\u6237\u548c\u4e34\u5e8a\u533b\u751f\u7528\u6237\u89c6\u89d2\uff0c\u5e76\u7ed3\u5408\u901a\u7528\u5b89\u5168\u8bc4\u4f30\uff1b\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b466\u4e2a\u6837\u672c\u3001\u6db5\u76d65\u4e2a\u5173\u952e\u7c7b\u522b\u7684PatientSafetyBench\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u7528\u4e8e\u4ece\u60a3\u8005\u89d2\u5ea6\u8861\u91cf\u5b89\u5168\u6027\uff1b\u5e94\u7528\u7ea2\u961f\uff08red-teaming\uff09\u534f\u8bae\uff0c\u4ee5MediPhi\u6a21\u578b\u7cfb\u5217\u4f5c\u4e3a\u6848\u4f8b\u8fdb\u884c\u4e86\u5b9a\u91cf\u5b89\u5168\u5206\u6790\u3002", "result": "\u672c\u7814\u7a76\u6210\u529f\u5b9a\u4e49\u4e86\u533b\u7597LLM\u7684\u5b89\u5168\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u4ece\u60a3\u8005\u89c6\u89d2\u51fa\u53d1\u7684\u4e13\u4e1a\u5b89\u5168\u57fa\u51c6PatientSafetyBench\u3002\u901a\u8fc7\u5bf9\u533b\u7597LLM\uff08\u5982MediPhi\u7cfb\u5217\uff09\u5e94\u7528\u7ea2\u961f\u534f\u8bae\uff0c\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u8fdb\u884c\u4e86\u6848\u4f8b\u5206\u6790\u548c\u5b9a\u91cf\u8bc4\u4f30\u3002", "conclusion": "\u672c\u5de5\u4f5c\u9996\u6b21\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u7ea2\u961f\u6d4b\u8bd5\uff0c\u4ece\u60a3\u8005\u3001\u4e34\u5e8a\u533b\u751f\u548c\u901a\u7528\u7528\u6237\u4e09\u4e2a\u4e0d\u540c\u89c6\u89d2\u5b9a\u4e49\u4e86\u533b\u7597LLMs\u7684\u5b89\u5168\u8bc4\u4f30\u6807\u51c6\uff0c\u4e3a\u533b\u7597\u9886\u57df\u4e2dLLMs\u7684\u5b89\u5168\u90e8\u7f72\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2507.07118", "pdf": "https://arxiv.org/pdf/2507.07118", "abs": "https://arxiv.org/abs/2507.07118", "authors": ["Zelin Zhu", "Kai Yang", "Rui Zhang"], "title": "Synergistic Localization and Sensing in MIMO-OFDM Systems via Mixed-Integer Bilevel Learning", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "Wireless localization and sensing technologies are essential in modern\nwireless networks, supporting applications in smart cities, the Internet of\nThings (IoT), and autonomous systems. High-performance localization and sensing\nsystems are critical for both network efficiency and emerging intelligent\napplications. Integrating channel state information (CSI) with deep learning\nhas recently emerged as a promising solution. Recent works have leveraged the\nspatial diversity of multiple input multiple output (MIMO) systems and the\nfrequency granularity of orthogonal frequency division multiplexing (OFDM)\nwaveforms to improve spatial resolution. Nevertheless, the joint modeling of\nlocalization and sensing under the high-dimensional CSI characteristics of\nMIMO-OFDM systems remains insufficiently investigated. This work aims to\njointly model and optimize localization and sensing tasks to harness their\npotential synergy. We first formulate localization and sensing as a\nmixed-integer bilevel deep learning problem and then propose a novel stochastic\nproximal gradient-based mixed-integer bilevel optimization (SPG-MIBO)\nalgorithm. SPG-MIBO is well-suited for high-dimensional and large-scale\ndatasets, leveraging mini-batch training at each step for computational and\nmemory efficiency. The algorithm is also supported by theoretical convergence\nguarantees. Extensive experiments on multiple datasets validate its\neffectiveness and highlight the performance gains from joint localization and\nsensing optimization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u8fd1\u7aef\u68af\u5ea6\u6df7\u5408\u6574\u6570\u53cc\u5c42\u4f18\u5316\uff08SPG-MIBO\uff09\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u8054\u5408\u5efa\u6a21\u548c\u4f18\u5316MIMO-OFDM\u7cfb\u7edf\u4e2d\u9ad8\u7ef4\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u4e0b\u7684\u65e0\u7ebf\u5b9a\u4f4d\u548c\u611f\u77e5\u4efb\u52a1\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6027\u80fd\u589e\u76ca\u3002", "motivation": "\u5c3d\u7ba1\u65e0\u7ebf\u5b9a\u4f4d\u548c\u611f\u77e5\u6280\u672f\u5728\u73b0\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4e14CSI\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u88ab\u89c6\u4e3a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u76ee\u524d\u5bf9\u4e8eMIMO-OFDM\u7cfb\u7edf\u9ad8\u7ef4CSI\u7279\u6027\u4e0b\u5b9a\u4f4d\u4e0e\u611f\u77e5\u7684\u8054\u5408\u5efa\u6a21\u548c\u4f18\u5316\u7814\u7a76\u5c1a\u4e0d\u5145\u5206\u3002", "method": "\u672c\u7814\u7a76\u5c06\u5b9a\u4f4d\u548c\u611f\u77e5\u4efb\u52a1\u516c\u5f0f\u5316\u4e3a\u4e00\u4e2a\u6df7\u5408\u6574\u6570\u53cc\u5c42\u6df1\u5ea6\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u968f\u673a\u8fd1\u7aef\u68af\u5ea6\u7684\u6df7\u5408\u6574\u6570\u53cc\u5c42\u4f18\u5316\uff08SPG-MIBO\uff09\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u9002\u7528\u4e8e\u9ad8\u7ef4\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5c0f\u6279\u91cf\u8bad\u7ec3\u63d0\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0SPG-MIBO\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u8054\u5408\u5b9a\u4f4d\u548c\u611f\u77e5\u4f18\u5316\u6240\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8054\u5408\u4f18\u5316\u5b9a\u4f4d\u548c\u611f\u77e5\u4efb\u52a1\u80fd\u591f\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002\u6240\u63d0\u51fa\u7684SPG-MIBO\u7b97\u6cd5\u4e3a\u89e3\u51b3MIMO-OFDM\u7cfb\u7edf\u4e2d\u9ad8\u7ef4CSI\u4e0b\u7684\u8054\u5408\u5b9a\u4f4d\u548c\u611f\u77e5\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07115", "pdf": "https://arxiv.org/pdf/2507.07115", "abs": "https://arxiv.org/abs/2507.07115", "authors": ["Javal Vyas", "Mehmet Mercangoz"], "title": "Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation", "categories": ["cs.AI", "cs.MA", "cs.SY", "eess.SY"], "comment": null, "summary": "The increasing complexity of modern chemical processes, coupled with\nworkforce shortages and intricate fault scenarios, demands novel automation\nparadigms that blend symbolic reasoning with adaptive control. In this work, we\nintroduce a unified agentic framework that leverages large language models\n(LLMs) for both discrete fault-recovery planning and continuous process control\nwithin a single architecture. We adopt Finite State Machines (FSMs) as\ninterpretable operating envelopes: an LLM-driven planning agent proposes\nrecovery sequences through the FSM, a Simulation Agent executes and checks each\ntransition, and a Validator-Reprompting loop iteratively refines invalid plans.\nIn Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25\nstates, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path\nsuccess within five reprompts-outperforming open-source LLMs in both accuracy\nand latency. In Case Study 2, the same framework modulates dual-heater inputs\non a laboratory TCLab platform (and its digital twin) to maintain a target\naverage temperature under persistent asymmetric disturbances. Compared to\nclassical PID control, our LLM-based controller attains similar performance,\nwhile ablation of the prompting loop reveals its critical role in handling\nnonlinear dynamics. We analyze key failure modes-such as instruction following\nlapses and coarse ODE approximations. Our results demonstrate that, with\nstructured feedback and modular agents, LLMs can unify high-level symbolic\nplanningand low-level continuous control, paving the way towards resilient,\nlanguage-driven automation in chemical engineering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684LLM\u4ee3\u7406\u6846\u67b6\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u5316\u5b66\u8fc7\u7a0b\u7684\u79bb\u6563\u6545\u969c\u6062\u590d\u89c4\u5212\u548c\u8fde\u7eed\u63a7\u5236\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6709\u9650\u72b6\u6001\u673a\u3001\u6a21\u62df\u4ee3\u7406\u548c\u9a8c\u8bc1\u5faa\u73af\u5728\u6a21\u62df\u548c\u7269\u7406\u5e73\u53f0\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660eLLM\u53ef\u5b9e\u73b0\u9ad8\u4f4e\u5c42\u7edf\u4e00\u63a7\u5236\u3002", "motivation": "\u9762\u5bf9\u73b0\u4ee3\u5316\u5de5\u8fc7\u7a0b\u65e5\u76ca\u589e\u957f\u7684\u590d\u6742\u6027\u3001\u52b3\u52a8\u529b\u77ed\u7f3a\u4ee5\u53ca\u590d\u6742\u7684\u6545\u969c\u573a\u666f\uff0c\u9700\u8981\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u548c\u81ea\u9002\u5e94\u63a7\u5236\u7684\u65b0\u578b\u81ea\u52a8\u5316\u8303\u5f0f\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684LLM\u4ee3\u7406\u6846\u67b6\uff0c\u5229\u7528LLM\u5b9e\u73b0\u79bb\u6563\u6545\u969c\u6062\u590d\u89c4\u5212\u548c\u8fde\u7eed\u8fc7\u7a0b\u63a7\u5236\u3002\u8be5\u6846\u67b6\u91c7\u7528\u6709\u9650\u72b6\u6001\u673a\uff08FSMs\uff09\u4f5c\u4e3a\u64cd\u4f5c\u8303\u56f4\uff0c\u5176\u4e2dLLM\u9a71\u52a8\u7684\u89c4\u5212\u4ee3\u7406\u63d0\u51fa\u6062\u590d\u5e8f\u5217\uff0c\u6a21\u62df\u4ee3\u7406\u6267\u884c\u5e76\u68c0\u67e5\u8f6c\u6362\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u5668-\u91cd\u590d\u63d0\u793a\u5faa\u73af\u8fed\u4ee3\u4f18\u5316\u65e0\u6548\u8ba1\u5212\u3002", "result": "\u5728\u6848\u4f8b\u7814\u7a761\u4e2d\uff0cGPT-4o\u548cGPT-4o-mini\u5728180\u4e2a\u4e0d\u540c\u5927\u5c0f\u7684FSMs\u4e0a\uff0c\u4e94\u6b21\u91cd\u590d\u63d0\u793a\u5185\u5b9e\u73b0\u4e86100%\u7684\u6709\u6548\u8def\u5f84\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u5f00\u6e90LLM\u3002\u5728\u6848\u4f8b\u7814\u7a762\u4e2d\uff0c\u8be5\u6846\u67b6\u5728\u5b9e\u9a8c\u5ba4TCLab\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u7ecf\u5178PID\u63a7\u5236\u76f8\u4f3c\u7684\u6e29\u5ea6\u63a7\u5236\u6027\u80fd\uff0c\u4e14\u63d0\u793a\u5faa\u73af\u5bf9\u5904\u7406\u975e\u7ebf\u6027\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u6307\u4ee4\u9075\u5faa\u5931\u8bef\u548c\u7c97\u7cd9ODE\u8fd1\u4f3c\u7b49\u5173\u952e\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u53cd\u9988\u548c\u6a21\u5757\u5316\u4ee3\u7406\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u7edf\u4e00\u9ad8\u5c42\u7b26\u53f7\u89c4\u5212\u548c\u4f4e\u5c42\u8fde\u7eed\u63a7\u5236\uff0c\u4e3a\u5316\u5de5\u9886\u57df\u97e7\u6027\u3001\u8bed\u8a00\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.07129", "pdf": "https://arxiv.org/pdf/2507.07129", "abs": "https://arxiv.org/abs/2507.07129", "authors": ["A. Bochkov"], "title": "Growing Transformers: Modular Composition and Layer-wise Expansion on a Frozen Substrate", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The prevailing paradigm for scaling large language models (LLMs) involves\nmonolithic, end-to-end training, a resource-intensive process that lacks\nflexibility. This paper explores an alternative, constructive approach to model\ndevelopment, built upon the foundation of non-trainable, deterministic input\nembeddings. In prior [1], we established that high-level semantic reasoning can\nemerge in Transformers using frozen embeddings derived from the visual\nstructure of Unicode glyphs. Here, we demonstrate that this fixed\nrepresentational substrate acts as a universal \"docking port,\" enabling two\npowerful and efficient scaling paradigms: seamless modular composition and\nprogressive layer-wise growth.\n  First, we show that specialist models trained on disparate datasets (e.g.,\nRussian and Chinese text) can be merged into a single, more capable\nMixture-of-Experts (MoE) model, post-training, with zero architectural\nmodification. This is achieved by simply averaging their output logits. The\nresulting MoE model exhibits immediate performance improvements on reasoning\nbenchmarks like MMLU, surpassing its constituent experts without catastrophic\nforgetting. Second, we introduce a layer-wise constructive training\nmethodology, where a deep Transformer is \"grown\" by progressively stacking and\ntraining one layer at a time. This method demonstrates stable convergence and a\nclear correlation between model depth and the emergence of complex reasoning\nabilities, such as those required for SQuAD.\n  Our findings suggest a paradigm shift from monolithic optimization towards a\nmore biological or constructive model of AI development, where complexity is\nbuilt incrementally and modules can be composed freely. This opens new avenues\nfor resource-efficient scaling, continual learning, and a more democratized\necosystem for building powerful AI systems. We release all code and models to\nfacilitate further research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fa\u5b9a\u8f93\u5165\u5d4c\u5165\u7684\u6784\u5efa\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6a21\u5757\u5316\u7ec4\u5408\uff08\u5408\u5e76\u4e13\u5bb6\u6a21\u578b\uff09\u548c\u9010\u5c42\u589e\u957f\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7075\u6d3b\u7684\u6a21\u578b\u6269\u5c55\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5355\u4f53\u5f0f\u3001\u7aef\u5230\u7aef\u8bad\u7ec3\u8303\u5f0f\u8d44\u6e90\u5bc6\u96c6\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u8be5\u7814\u7a76\u57fa\u4e8e\u4e0d\u53ef\u8bad\u7ec3\u3001\u786e\u5b9a\u6027\u7684\u8f93\u5165\u5d4c\u5165\u4f5c\u4e3a\u901a\u7528\u201c\u5bf9\u63a5\u7aef\u53e3\u201d\u3002\u63a2\u7d22\u4e86\u4e24\u79cd\u5f3a\u5927\u7684\u6269\u5c55\u8303\u5f0f\uff1a1. \u65e0\u7f1d\u6a21\u5757\u5316\u7ec4\u5408\uff1a\u901a\u8fc7\u7b80\u5355\u5e73\u5747\u8f93\u51falogits\uff0c\u5728\u8bad\u7ec3\u540e\u5c06\u8bad\u7ec3\u4e8e\u4e0d\u540c\u6570\u636e\u96c6\u7684\u4e13\u4e1a\u6a21\u578b\u5408\u5e76\u4e3a\u5355\u4e2aMixture-of-Experts (MoE) \u6a21\u578b\uff0c\u65e0\u9700\u4fee\u6539\u67b6\u6784\u30022. \u6e10\u8fdb\u5f0f\u9010\u5c42\u589e\u957f\uff1a\u6bcf\u6b21\u5806\u53e0\u5e76\u8bad\u7ec3\u4e00\u5c42\uff0c\u9010\u6b65\u201c\u751f\u957f\u201d\u4e00\u4e2a\u6df1\u5ea6Transformer\u6a21\u578b\u3002", "result": "1. \u6a21\u5757\u5316\u7ec4\u5408\uff1a\u5408\u5e76\u540e\u7684MoE\u6a21\u578b\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MMLU\uff09\u4e0a\u7acb\u5373\u5c55\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86\u5176\u7ec4\u6210\u4e13\u5bb6\u6a21\u578b\uff0c\u4e14\u6ca1\u6709\u707e\u96be\u6027\u9057\u5fd8\u30022. \u9010\u5c42\u589e\u957f\uff1a\u8be5\u65b9\u6cd5\u663e\u793a\u51fa\u7a33\u5b9a\u7684\u6536\u655b\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u6df1\u5ea6\u4e0e\u590d\u6742\u63a8\u7406\u80fd\u529b\uff08\u5982SQuAD\uff09\u51fa\u73b0\u4e4b\u95f4\u7684\u660e\u786e\u5173\u8054\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u5e94\u4ece\u5355\u4f53\u4f18\u5316\u8f6c\u5411\u66f4\u5177\u751f\u7269\u6027\u6216\u6784\u5efa\u6027\u7684\u8303\u5f0f\uff0c\u5373\u590d\u6742\u6027\u53ef\u589e\u91cf\u6784\u5efa\uff0c\u6a21\u5757\u53ef\u81ea\u7531\u7ec4\u5408\u3002\u8fd9\u4e3a\u8d44\u6e90\u9ad8\u6548\u6269\u5c55\u3001\u6301\u7eed\u5b66\u4e60\u4ee5\u53ca\u66f4\u6c11\u4e3b\u7684AI\u7cfb\u7edf\u751f\u6001\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.07108", "pdf": "https://arxiv.org/pdf/2507.07108", "abs": "https://arxiv.org/abs/2507.07108", "authors": ["Zhiwei Hu", "V\u00edctor Guti\u00e9rrez-Basulto", "Zhiliang Xiang", "Ru Li", "Jeff Z. Pan"], "title": "Multi-level Mixture of Experts for Multimodal Entity Linking", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Accepted at KDD 2025", "summary": "Multimodal Entity Linking (MEL) aims to link ambiguous mentions within\nmultimodal contexts to associated entities in a multimodal knowledge base.\nExisting approaches to MEL introduce multimodal interaction and fusion\nmechanisms to bridge the modality gap and enable multi-grained semantic\nmatching. However, they do not address two important problems: (i) mention\nambiguity, i.e., the lack of semantic content caused by the brevity and\nomission of key information in the mention's textual context; (ii) dynamic\nselection of modal content, i.e., to dynamically distinguish the importance of\ndifferent parts of modal information. To mitigate these issues, we propose a\nMulti-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:\n(i) the description-aware mention enhancement module leverages large language\nmodels to identify the WikiData descriptions that best match a mention,\nconsidering the mention's textual context; (ii) the multimodal feature\nextraction module adopts multimodal feature encoders to obtain textual and\nvisual embeddings for both mentions and entities; (iii)-(iv) the intra-level\nmixture of experts and inter-level mixture of experts modules apply a switch\nmixture of experts mechanism to dynamically and adaptively select features from\nrelevant regions of information. Extensive experiments demonstrate the\noutstanding performance of MMoE compared to the state-of-the-art. MMoE's code\nis available at: https://github.com/zhiweihu1103/MEL-MMoE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aMMoE\uff08\u591a\u5c42\u4e13\u5bb6\u6df7\u5408\uff09\u7684\u65b0\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\uff08MEL\uff09\u4e2d\u63d0\u53ca\u6b67\u4e49\u548c\u6a21\u6001\u5185\u5bb9\u52a8\u6001\u9009\u62e9\u7684\u6311\u6218\u3002MMoE\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u63d0\u53ca\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u591a\u5c42\u4e13\u5bb6\u6df7\u5408\u673a\u5236\u52a8\u6001\u9009\u62e9\u548c\u878d\u5408\u591a\u6a21\u6001\u7279\u5f81\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\uff08MEL\uff09\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u89e3\u51b3\u4ee5\u4e0b\u4e24\u4e2a\u91cd\u8981\u95ee\u9898\uff1a1) \u63d0\u53ca\u6b67\u4e49\uff0c\u5373\u63d0\u53ca\u7684\u6587\u672c\u4e0a\u4e0b\u6587\u8fc7\u4e8e\u7b80\u77ed\u6216\u7701\u7565\u5173\u952e\u4fe1\u606f\u5bfc\u81f4\u8bed\u4e49\u5185\u5bb9\u4e0d\u8db3\uff1b2) \u6a21\u6001\u5185\u5bb9\u52a8\u6001\u9009\u62e9\uff0c\u5373\u65e0\u6cd5\u52a8\u6001\u533a\u5206\u548c\u9009\u62e9\u4e0d\u540c\u6a21\u6001\u4fe1\u606f\u4e2d\u91cd\u8981\u90e8\u5206\u7684\u6709\u6548\u6027\u3002", "method": "\u672c\u6587\u63d0\u51faMMoE\uff08\u591a\u5c42\u4e13\u5bb6\u6df7\u5408\uff09\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5305\u542b\u56db\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a1) \u63cf\u8ff0\u611f\u77e5\u63d0\u53ca\u589e\u5f3a\u6a21\u5757\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u6700\u5339\u914d\u63d0\u53ca\u7684WikiData\u63cf\u8ff0\uff1b2) \u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff0c\u91c7\u7528\u591a\u6a21\u6001\u7279\u5f81\u7f16\u7801\u5668\u83b7\u53d6\u63d0\u53ca\u548c\u5b9e\u4f53\u7684\u6587\u672c\u53ca\u89c6\u89c9\u5d4c\u5165\uff1b3) \u5c42\u5185\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u548c 4) \u5c42\u95f4\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff0c\u5e94\u7528\u5207\u6362\u4e13\u5bb6\u6df7\u5408\uff08Switch Mixture of Experts\uff09\u673a\u5236\uff0c\u52a8\u6001\u81ea\u9002\u5e94\u5730\u4ece\u76f8\u5173\u4fe1\u606f\u533a\u57df\u9009\u62e9\u7279\u5f81\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMMoE\u6a21\u578b\u76f8\u6bd4\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\uff08state-of-the-art\uff09\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "MMoE\u6a21\u578b\u901a\u8fc7\u521b\u65b0\u7684\u591a\u5c42\u4e13\u5bb6\u6df7\u5408\u673a\u5236\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u591a\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\u4e2d\u7684\u63d0\u53ca\u6b67\u4e49\u548c\u6a21\u6001\u5185\u5bb9\u52a8\u6001\u9009\u62e9\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u9886\u5148\u7684\u5b9e\u4f53\u94fe\u63a5\u6027\u80fd\u3002"}}
{"id": "2507.07280", "pdf": "https://arxiv.org/pdf/2507.07280", "abs": "https://arxiv.org/abs/2507.07280", "authors": ["Mariah Bradford", "Nikhil Krishnaswamy", "Nathaniel Blanchard"], "title": "The Impact of Background Speech on Interruption Detection in Collaborative Groups", "categories": ["cs.CL"], "comment": "Long Paper AIED 2025", "summary": "Interruption plays a crucial role in collaborative learning, shaping group\ninteractions and influencing knowledge construction. AI-driven support can\nassist teachers in monitoring these interactions. However, most previous work\non interruption detection and interpretation has been conducted in\nsingle-conversation environments with relatively clean audio. AI agents\ndeployed in classrooms for collaborative learning within small groups will need\nto contend with multiple concurrent conversations -- in this context,\noverlapping speech will be ubiquitous, and interruptions will need to be\nidentified in other ways. In this work, we analyze interruption detection in\nsingle-conversation and multi-group dialogue settings. We then create a\nstate-of-the-art method for interruption identification that is robust to\noverlapping speech, and thus could be deployed in classrooms. Further, our work\nhighlights meaningful linguistic and prosodic information about how\ninterruptions manifest in collaborative group interactions. Our investigation\nalso paves the way for future works to account for the influence of overlapping\nspeech from multiple groups when tracking group dialog.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u534f\u540c\u5b66\u4e60\u4e2d\u591a\u7ec4\u5e76\u53d1\u5bf9\u8bdd\u73af\u5883\u4e0b\u7684\u4e2d\u65ad\u68c0\u6d4b\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u5bf9\u91cd\u53e0\u8bed\u97f3\u9c81\u68d2\u7684\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u4e2d\u65ad\u7684\u8bed\u8a00\u548c\u97f5\u5f8b\u7279\u5f81\u3002", "motivation": "\u4e2d\u65ad\u5728\u534f\u540c\u5b66\u4e60\u4e2d\u81f3\u5173\u91cd\u8981\uff0cAI\u8f85\u52a9\u80fd\u5e2e\u52a9\u6559\u5e08\u76d1\u63a7\u3002\u7136\u800c\uff0c\u73b0\u6709\u4e2d\u65ad\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u5bf9\u8bdd\u548c\u6e05\u6670\u97f3\u9891\uff0c\u65e0\u6cd5\u5e94\u5bf9\u771f\u5b9e\u8bfe\u5802\u4e2d\u591a\u7ec4\u5e76\u53d1\u5bf9\u8bdd\u3001\u8bed\u97f3\u91cd\u53e0\u666e\u904d\u5b58\u5728\u7684\u590d\u6742\u73af\u5883\u3002", "method": "\u5206\u6790\u4e86\u5355\u5bf9\u8bdd\u548c\u591a\u7ec4\u5bf9\u8bdd\u8bbe\u7f6e\u4e0b\u7684\u4e2d\u65ad\u68c0\u6d4b\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u79cd\u5bf9\u91cd\u53e0\u8bed\u97f3\u9c81\u68d2\u7684\u5148\u8fdb\u4e2d\u65ad\u8bc6\u522b\u65b9\u6cd5\u3002", "result": "\u5f00\u53d1\u51fa\u4e00\u79cd\u53ef\u90e8\u7f72\u4e8e\u8bfe\u5802\u7684\u3001\u5bf9\u91cd\u53e0\u8bed\u97f3\u9c81\u68d2\u7684\u5148\u8fdb\u4e2d\u65ad\u8bc6\u522b\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u534f\u540c\u5b66\u4e60\u4e2d\u4e2d\u65ad\u7684\u8bed\u8a00\u548c\u97f5\u5f8b\u7279\u5f81\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u8bfe\u5802\u4e2dAI\u8f85\u52a9\u4e2d\u65ad\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u5728\u591a\u7ec4\u91cd\u53e0\u8bed\u97f3\u73af\u5883\u4e0b\u8ffd\u8e2a\u7fa4\u7ec4\u5bf9\u8bdd\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.07149", "pdf": "https://arxiv.org/pdf/2507.07149", "abs": "https://arxiv.org/abs/2507.07149", "authors": ["Renyuan Liu", "Yuyang Leng", "Kaiyan Liu", "Shaohan Hu", "Chun-Fu", "Chen", "Peijun Zhao", "Heechul Yun", "Shuochao Yao"], "title": "DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training", "categories": ["cs.NI", "cs.LG"], "comment": "Accepted to MobiSys 2025", "summary": "Recent advancements in on-device training for deep neural networks have\nunderscored the critical need for efficient activation compression to overcome\nthe memory constraints of mobile and edge devices. As activations dominate\nmemory usage during training and are essential for gradient computation,\ncompressing them without compromising accuracy remains a key research\nchallenge. While existing methods for dynamic activation quantization promise\ntheoretical memory savings, their practical deployment is impeded by\nsystem-level challenges such as computational overhead and memory\nfragmentation.\n  To address these challenges, we introduce DAF, a Dynamic Activation Framework\nthat enables scalable and efficient on-device training through system-level\noptimizations. DAF achieves both memory- and time-efficient dynamic\nquantization training by addressing key system bottlenecks. It develops hybrid\nreduction operations tailored to the memory hierarchies of mobile and edge\nSoCs, leverages collaborative CPU-GPU bit-packing for efficient dynamic\nquantization, and implements an importance-aware paging memory management\nscheme to reduce fragmentation and support dynamic memory adjustments.\n  These optimizations collectively enable DAF to achieve substantial memory\nsavings and speedup without compromising model training accuracy. Evaluations\non various deep learning models across embedded and mobile platforms\ndemonstrate up to a $22.9\\times$ reduction in memory usage and a $3.2\\times$\nspeedup, making DAF a scalable and practical solution for resource-constrained\nenvironments.", "AI": {"tldr": "DAF\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u79fb\u52a8/\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u4e2d\u6fc0\u6d3b\u503c\u538b\u7f29\u7684\u5185\u5b58\u548c\u901f\u5ea6\u95ee\u9898\u3002", "motivation": "\u79fb\u52a8\u548c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u53d7\u5185\u5b58\u9650\u5236\uff0c\u6fc0\u6d3b\u503c\u5360\u7528\u5927\u91cf\u5185\u5b58\u3002\u73b0\u6709\u52a8\u6001\u6fc0\u6d3b\u91cf\u5316\u65b9\u6cd5\u56e0\u8ba1\u7b97\u5f00\u9500\u548c\u5185\u5b58\u788e\u7247\u5316\u7b49\u7cfb\u7edf\u7ea7\u6311\u6218\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\uff0c\u4e9f\u9700\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u538b\u7f29\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51faDAF\uff08\u52a8\u6001\u6fc0\u6d3b\u6846\u67b6\uff09\uff0c\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4f18\u5316\u5b9e\u73b0\u5185\u5b58\u548c\u65f6\u95f4\u9ad8\u6548\u7684\u52a8\u6001\u91cf\u5316\u8bad\u7ec3\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u5f00\u53d1\u9002\u7528\u4e8e\u79fb\u52a8/\u8fb9\u7f18SoC\u5185\u5b58\u5c42\u6b21\u7684\u6df7\u5408\u5f52\u7ea6\u64cd\u4f5c\u3001\u5229\u7528CPU-GPU\u534f\u540c\u4f4d\u6253\u5305\u3001\u4ee5\u53ca\u5b9e\u73b0\u91cd\u8981\u6027\u611f\u77e5\u7684\u5206\u9875\u5185\u5b58\u7ba1\u7406\u65b9\u6848\u4ee5\u51cf\u5c11\u788e\u7247\u5e76\u652f\u6301\u52a8\u6001\u5185\u5b58\u8c03\u6574\u3002", "result": "DAF\u5728\u4e0d\u635f\u5bb3\u6a21\u578b\u8bad\u7ec3\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5185\u5b58\u8282\u7701\u548c\u52a0\u901f\u3002\u5728\u5d4c\u5165\u5f0f\u548c\u79fb\u52a8\u5e73\u53f0\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\u9ad8\u8fbe22.9\u500d\uff0c\u901f\u5ea6\u63d0\u53473.2\u500d\u3002", "conclusion": "DAF\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07134", "pdf": "https://arxiv.org/pdf/2507.07134", "abs": "https://arxiv.org/abs/2507.07134", "authors": ["Mridula Vijendran", "Shuang Chen", "Jingjing Deng", "Hubert P. H. Shum"], "title": "BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks", "categories": ["cs.AI", "cs.LG", "I.2.10"], "comment": "18 pages, 7 figures, 3 tables", "summary": "The pervasive issue of bias in AI presents a significant challenge to\npainting classification, and is getting more serious as these systems become\nincreasingly integrated into tasks like art curation and restoration. Biases,\noften arising from imbalanced datasets where certain artistic styles dominate,\ncompromise the fairness and accuracy of model predictions, i.e., classifiers\nare less accurate on rarely seen paintings. While prior research has made\nstrides in improving classification performance, it has largely overlooked the\ncritical need to address these underlying biases, that is, when dealing with\nout-of-distribution (OOD) data. Our insight highlights the necessity of a more\nrobust approach to bias mitigation in AI models for art classification on\nbiased training data. We propose a novel OOD-informed model bias adaptive\nsampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It\naddresses these challenges by dynamically adjusting temperature scaling and\nsampling probabilities, thereby promoting a more equitable representation of\nall classes. We evaluate our proposed approach to the KaoKore and PACS\ndatasets, focusing on the model's ability to reduce class-wise bias. We further\npropose a new metric, Same-Dataset OOD Detection Score (SODC), designed to\nassess class-wise separation and per-class bias reduction. Our method\ndemonstrates the ability to balance high performance with fairness, making it a\nrobust solution for unbiasing AI models in the art domain.", "AI": {"tldr": "AI\u753b\u4f5c\u5206\u7c7b\u4e2d\u5b58\u5728\u504f\u5dee\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aBOOST\u7684OOD\u4fe1\u606f\u6a21\u578b\u504f\u5dee\u81ea\u9002\u5e94\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6e29\u5ea6\u7f29\u653e\u548c\u91c7\u6837\u6982\u7387\u6765\u7f13\u89e3\u504f\u5dee\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u5728\u6027\u80fd\u4e0e\u516c\u5e73\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "AI\u5728\u753b\u4f5c\u5206\u7c7b\u4e2d\u5b58\u5728\u666e\u904d\u504f\u5dee\uff0c\u5c24\u5176\u6e90\u4e8e\u4e0d\u5e73\u8861\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u6a21\u578b\u5bf9\u4e0d\u5e38\u89c1\u753b\u4f5c\u7684\u9884\u6d4b\u51c6\u786e\u6027\u4e0b\u964d\u3002\u968f\u7740AI\u7cfb\u7edf\u65e5\u76ca\u96c6\u6210\u5230\u827a\u672f\u7b56\u5c55\u548c\u4fee\u590d\u7b49\u4efb\u52a1\u4e2d\uff0c\u8fd9\u4e00\u95ee\u9898\u6108\u53d1\u4e25\u91cd\u3002\u73b0\u6709\u7814\u7a76\u867d\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u4f46\u666e\u904d\u5ffd\u89c6\u4e86\u5904\u7406OOD\u6570\u636e\u65f6\u7684\u6f5c\u5728\u504f\u5dee\uff0c\u4e9f\u9700\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u7f13\u89e3\u827a\u672f\u5206\u7c7b\u6a21\u578b\u4e2d\u7684\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001OOD\u4fe1\u606f\u6a21\u578b\u504f\u5dee\u81ea\u9002\u5e94\u91c7\u6837\u65b9\u6cd5\uff0c\u540d\u4e3aBOOST (Bias-Oriented OOD Sampling and Tuning)\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6e29\u5ea6\u7f29\u653e\u548c\u91c7\u6837\u6982\u7387\u6765\u4fc3\u8fdb\u5404\u7c7b\u522b\u66f4\u516c\u5e73\u7684\u8868\u793a\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807SODC (Same-Dataset OOD Detection Score)\uff0c\u7528\u4e8e\u8bc4\u4f30\u7c7b\u522b\u5206\u79bb\u5ea6\u548c\u6bcf\u7c7b\u522b\u504f\u5dee\u51cf\u5c11\u3002", "result": "\u5728KaoKore\u548cPACS\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684BOOST\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u7c7b\u522b\u504f\u5dee\u3002\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5e73\u8861\u4e86\u9ad8\u5206\u7c7b\u6027\u80fd\u4e0e\u516c\u5e73\u6027\u3002", "conclusion": "BOOST\u65b9\u6cd5\u4e3a\u827a\u672f\u9886\u57dfAI\u6a21\u578b\u53bb\u504f\u5dee\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5730\u5e73\u8861\u6a21\u578b\u7684\u6027\u80fd\u548c\u516c\u5e73\u6027\uff0c\u5c24\u5176\u5728\u5904\u7406\u5177\u6709\u504f\u5dee\u7684\u8bad\u7ec3\u6570\u636e\u65f6\u3002"}}
{"id": "2507.07135", "pdf": "https://arxiv.org/pdf/2507.07135", "abs": "https://arxiv.org/abs/2507.07135", "authors": ["Fran\u00e7ois Gard\u00e8res", "Shizhe Chen", "Camille-Sovanneary Gauthier", "Jean Ponce"], "title": "FACap: A Large-scale Fashion Dataset for Fine-grained Composed Image Retrieval", "categories": ["cs.LG"], "comment": null, "summary": "The composed image retrieval (CIR) task is to retrieve target images given a\nreference image and a modification text. Recent methods for CIR leverage large\npretrained vision-language models (VLMs) and achieve good performance on\ngeneral-domain concepts like color and texture. However, they still struggle\nwith application domains like fashion, because the rich and diverse vocabulary\nused in fashion requires specific fine-grained vision and language\nunderstanding. An additional difficulty is the lack of large-scale fashion\ndatasets with detailed and relevant annotations, due to the expensive cost of\nmanual annotation by specialists. To address these challenges, we introduce\nFACap, a large-scale, automatically constructed fashion-domain CIR dataset. It\nleverages web-sourced fashion images and a two-stage annotation pipeline\npowered by a VLM and a large language model (LLM) to generate accurate and\ndetailed modification texts. Then, we propose a new CIR model FashionBLIP-2,\nwhich fine-tunes the general-domain BLIP-2 model on FACap with lightweight\nadapters and multi-head query-candidate matching to better account for\nfine-grained fashion-specific information. FashionBLIP-2 is evaluated with and\nwithout additional fine-tuning on the Fashion IQ benchmark and the enhanced\nevaluation dataset enhFashionIQ, leveraging our pipeline to obtain\nhigher-quality annotations. Experimental results show that the combination of\nFashionBLIP-2 and pretraining with FACap significantly improves the model's\nperformance in fashion CIR especially for retrieval with fine-grained\nmodification texts, demonstrating the value of our dataset and approach in a\nhighly demanding environment such as e-commerce websites. Code is available at\nhttps://fgxaos.github.io/facap-paper-website/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FACap\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u81ea\u52a8\u6784\u5efa\u7684\u65f6\u5c1a\u9886\u57df\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\uff08CIR\uff09\u6570\u636e\u96c6\uff0c\u4ee5\u53caFashionBLIP-2\u6a21\u578b\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5728FACap\u4e0a\u5fae\u8c03\u901a\u7528BLIP-2\u5e76\u7ed3\u5408\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u548c\u591a\u5934\u5339\u914d\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u5c1aCIR\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u7ec6\u7c92\u5ea6\u68c0\u7d22\u65b9\u9762\u3002", "motivation": "\u73b0\u6709CIR\u65b9\u6cd5\u5728\u65f6\u5c1a\u7b49\u5e94\u7528\u9886\u57df\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u65f6\u5c1a\u9886\u57df\u8bcd\u6c47\u4e30\u5bcc\u591a\u6837\uff0c\u9700\u8981\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u548c\u8bed\u8a00\u7406\u89e3\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u7f3a\u4e4f\u5e26\u6709\u8be6\u7ec6\u6ce8\u91ca\u7684\u5927\u89c4\u6a21\u65f6\u5c1a\u6570\u636e\u96c6\u3002", "method": "\u7814\u7a76\u8005\u5f15\u5165\u4e86FACap\uff0c\u4e00\u4e2a\u5229\u7528\u7f51\u7edc\u65f6\u5c1a\u56fe\u50cf\u548c\u4e24\u9636\u6bb5VLM/LLM\u9a71\u52a8\u7684\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\u6784\u5efa\u7684\u5927\u89c4\u6a21\u65f6\u5c1aCIR\u6570\u636e\u96c6\u3002\u63a5\u7740\uff0c\u4ed6\u4eec\u63d0\u51fa\u4e86FashionBLIP-2\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5728FACap\u4e0a\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u548c\u591a\u5934\u67e5\u8be2-\u5019\u9009\u5339\u914d\u673a\u5236\u5bf9\u901a\u7528BLIP-2\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u7ec6\u7c92\u5ea6\u65f6\u5c1a\u4fe1\u606f\u3002\u6a21\u578b\u5728Fashion IQ\u548c\u589e\u5f3a\u7248enhFashionIQ\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFashionBLIP-2\u4e0eFACap\u6570\u636e\u96c6\u7ed3\u5408\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u65f6\u5c1aCIR\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5bf9\u4e8e\u6d89\u53ca\u7ec6\u7c92\u5ea6\u4fee\u6539\u6587\u672c\u7684\u68c0\u7d22\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684FACap\u6570\u636e\u96c6\u548cFashionBLIP-2\u65b9\u6cd5\u5728\u65f6\u5c1a\u7535\u5546\u7b49\u9ad8\u8981\u6c42\u73af\u5883\u4e2d\u7684\u4ef7\u503c\uff0c\u4e3a\u89e3\u51b3\u65f6\u5c1a\u9886\u57df\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2507.07125", "pdf": "https://arxiv.org/pdf/2507.07125", "abs": "https://arxiv.org/abs/2507.07125", "authors": ["Cristina Mata", "Kanchana Ranasinghe", "Michael S. Ryoo"], "title": "CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic Text Embeddings", "categories": ["cs.CV", "eess.IV"], "comment": "ECCV 2024", "summary": "Unsupervised domain adaptation (UDA) involves learning class semantics from\nlabeled data within a source domain that generalize to an unseen target domain.\nUDA methods are particularly impactful for semantic segmentation, where\nannotations are more difficult to collect than in image classification. Despite\nrecent advances in large-scale vision-language representation learning, UDA\nmethods for segmentation have not taken advantage of the domain-agnostic\nproperties of text. To address this, we present a novel Covariance-based\nPixel-Text loss, CoPT, that uses domain-agnostic text embeddings to learn\ndomain-invariant features in an image segmentation encoder. The text embeddings\nare generated through our LLM Domain Template process, where an LLM is used to\ngenerate source and target domain descriptions that are fed to a frozen CLIP\nmodel and combined. In experiments on four benchmarks we show that a model\ntrained using CoPT achieves the new state of the art performance on UDA for\nsegmentation. The code can be found at https://github.com/cfmata/CoPT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoPT\u7684\u65b0\u578b\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u3002CoPT\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u9886\u57df\u63cf\u8ff0\uff0c\u901a\u8fc7CLIP\u6a21\u578b\u83b7\u53d6\u57df\u65e0\u5173\u7684\u6587\u672c\u5d4c\u5165\uff0c\u4ece\u800c\u8bad\u7ec3\u5206\u5272\u7f16\u7801\u5668\u5b66\u4e60\u57df\u4e0d\u53d8\u7279\u5f81\uff0c\u5e76\u5728UDA\u5206\u5272\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u7684\u6807\u6ce8\u6536\u96c6\u56f0\u96be\uff0c\u5c3d\u7ba1\u89c6\u89c9-\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u7684UDA\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u5229\u7528\u6587\u672c\u7684\u57df\u65e0\u5173\u7279\u6027\u6765\u63d0\u5347\u5206\u5272\u6a21\u578b\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u534f\u65b9\u5dee\u7684\u50cf\u7d20\u6587\u672c\u635f\u5931\uff08CoPT\uff09\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e00\u4e2a\u201cLLM\u57df\u6a21\u677f\u201d\u8fc7\u7a0b\u751f\u6210\u57df\u65e0\u5173\u7684\u6587\u672c\u5d4c\u5165\uff1aLLM\u9996\u5148\u751f\u6210\u6e90\u57df\u548c\u76ee\u6807\u57df\u7684\u63cf\u8ff0\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u63cf\u8ff0\u8f93\u5165\u5230\u4e00\u4e2a\u51bb\u7ed3\u7684CLIP\u6a21\u578b\u4e2d\u5e76\u7ed3\u5408\uff0c\u4ece\u800c\u83b7\u5f97\u7528\u4e8e\u5b66\u4e60\u56fe\u50cf\u5206\u5272\u7f16\u7801\u5668\u4e2d\u57df\u4e0d\u53d8\u7279\u5f81\u7684\u6587\u672c\u5d4c\u5165\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528CoPT\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u8bed\u4e49\u5206\u5272\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\uff0c\u901a\u8fc7\u5229\u7528LLM\u548cCLIP\u6a21\u578b\u751f\u6210\u7684\u57df\u65e0\u5173\u6587\u672c\u5d4c\u5165\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8bed\u4e49\u5206\u5272\u5728\u65e0\u76d1\u7763\u57df\u9002\u5e94\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u8fbe\u5230\u4e86\u5f53\u524d\u6280\u672f\u7684\u6700\u9ad8\u6c34\u5e73\u3002"}}
{"id": "2507.07307", "pdf": "https://arxiv.org/pdf/2507.07307", "abs": "https://arxiv.org/abs/2507.07307", "authors": ["Anirban Saha Anik", "Xiaoying Song", "Elliott Wang", "Bryan Wang", "Bengisu Yarimbas", "Lingzi Hong"], "title": "Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) incorporated with Retrieval-Augmented Generation\n(RAG) have demonstrated powerful capabilities in generating counterspeech\nagainst misinformation. However, current studies rely on limited evidence and\noffer less control over final outputs. To address these challenges, we propose\na Multi-agent Retrieval-Augmented Framework to generate counterspeech against\nhealth misinformation, incorporating multiple LLMs to optimize knowledge\nretrieval, evidence enhancement, and response refinement. Our approach\nintegrates both static and dynamic evidence, ensuring that the generated\ncounterspeech is relevant, well-grounded, and up-to-date. Our method\noutperforms baseline approaches in politeness, relevance, informativeness, and\nfactual accuracy, demonstrating its effectiveness in generating high-quality\ncounterspeech. To further validate our approach, we conduct ablation studies to\nverify the necessity of each component in our framework. Furthermore, human\nevaluations reveal that refinement significantly enhances counterspeech quality\nand obtains human preference.", "AI": {"tldr": "\u9488\u5bf9LLM\u7ed3\u5408RAG\u751f\u6210\u53cd\u9a73\u8a00\u8bba\u65f6\u8bc1\u636e\u6709\u9650\u3001\u63a7\u5236\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u591a\u667a\u80fd\u4f53RAG\u6846\u67b6\uff0c\u901a\u8fc7\u591aLLM\u534f\u4f5c\u4f18\u5316\u77e5\u8bc6\u68c0\u7d22\u3001\u8bc1\u636e\u589e\u5f3a\u548c\u56de\u590d\u4f18\u5316\uff0c\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ca\u65f6\u4e14\u51c6\u786e\u7684\u5065\u5eb7\u865a\u5047\u4fe1\u606f\u53cd\u9a73\u8a00\u8bba\uff0c\u5e76\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u5e76\u83b7\u5f97\u4eba\u7c7b\u504f\u597d\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u751f\u6210\u53cd\u9a73\u865a\u5047\u4fe1\u606f\u65f6\uff0c\u5b58\u5728\u4f9d\u8d56\u6709\u9650\u8bc1\u636e\u548c\u5bf9\u6700\u7ec8\u8f93\u51fa\u63a7\u5236\u4e0d\u8db3\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u591a\u4e2aLLMs\u4ee5\u4f18\u5316\u77e5\u8bc6\u68c0\u7d22\u3001\u8bc1\u636e\u589e\u5f3a\u548c\u56de\u590d\u4f18\u5316\uff0c\u4e13\u95e8\u7528\u4e8e\u751f\u6210\u5bf9\u6297\u5065\u5eb7\u865a\u5047\u4fe1\u606f\u7684\u53cd\u9a73\u8a00\u8bba\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u9759\u6001\u548c\u52a8\u6001\u8bc1\u636e\uff0c\u5e76\u8fdb\u884c\u4e86\u6d88\u878d\u7814\u7a76\u4ee5\u9a8c\u8bc1\u5404\u7ec4\u4ef6\u7684\u5fc5\u8981\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u793c\u8c8c\u6027\u3001\u76f8\u5173\u6027\u3001\u4fe1\u606f\u91cf\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\uff0c\u56de\u590d\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u53cd\u9a73\u8a00\u8bba\u7684\u8d28\u91cf\u5e76\u83b7\u5f97\u4e86\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\u6709\u6548\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u6709\u4e8b\u5b9e\u4f9d\u636e\u4e14\u53d7\u7528\u6237\u504f\u7231\u7684\u53cd\u9a73\u8a00\u8bba\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5e94\u5bf9\u5065\u5eb7\u865a\u5047\u4fe1\u606f\u3002"}}
{"id": "2507.07437", "pdf": "https://arxiv.org/pdf/2507.07437", "abs": "https://arxiv.org/abs/2507.07437", "authors": ["Jiasheng Wu", "Shaojie Su", "Wenjun Zhu", "Xiong Wang", "Jingjing Zhang", "Xingqiu He", "Yue Gao"], "title": "PHandover: Parallel Handover in Mobile Satellite Network", "categories": ["cs.NI"], "comment": "14 pages, 14 figures", "summary": "The construction of Low Earth Orbit (LEO) satellite constellations has\nrecently attracted tremendous attention from both academia and industry. The 5G\nand 6G standards have identified LEO satellite networks as a key component of\nfuture mobile networks. However, due to the high-speed movement of satellites,\nground terminals often experience frequent and high-latency handovers, which\nsignificantly deteriorate the performance of latency-sensitive applications. To\naddress this challenge, we propose a parallel handover mechanism for mobile\nsatellite networks that can considerably reduce handover latency. The main idea\nis to employ plan-based handovers instead of measurement-based handovers to\navoid interactions between the access and core networks, thereby eliminating\nthe significant time overhead associated with traditional handover procedures.\nSpecifically, we introduce a novel network function named the Satellite\nSynchronized Function (SSF), which is designed to be fully compliant with the\nstandard 5G core network. In addition, we propose a machine learning model for\nsignal strength prediction, coupled with an efficient handover scheduling\nalgorithm. We have conducted extensive experiments, and the results demonstrate\nthat our proposed handover scheme can reduce handover latency by 21\\times\ncompared to the standard NTN handover scheme and two other existing handover\napproaches, along with significant improvements in network stability and\nuser-level performance.", "AI": {"tldr": "\u4e3a\u89e3\u51b3LEO\u536b\u661f\u7f51\u7edc\u9ad8\u5ef6\u8fdf\u5207\u6362\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8ba1\u5212\u7684\u5e76\u884c\u5207\u6362\u673a\u5236\uff0c\u901a\u8fc7\u5f15\u5165\u536b\u661f\u540c\u6b65\u529f\u80fd\uff08SSF\uff09\u53ca\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u548c\u8c03\u5ea6\uff0c\u5c06\u5207\u6362\u5ef6\u8fdf\u964d\u4f4e21\u500d\uff0c\u663e\u8457\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u7684\u9ad8\u901f\u79fb\u52a8\u5bfc\u81f4\u5730\u9762\u7ec8\u7aef\u9891\u7e41\u4e14\u9ad8\u5ef6\u8fdf\u7684\u5207\u6362\uff0c\u4e25\u91cd\u5f71\u54cd\u5bf9\u5ef6\u8fdf\u654f\u611f\u7684\u5e94\u7528\u6027\u80fd\uff0c\u662f\u672a\u67655G/6G\u79fb\u52a8\u7f51\u7edc\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5e76\u884c\u5207\u6362\u673a\u5236\uff0c\u5176\u6838\u5fc3\u601d\u60f3\u662f\u91c7\u7528\u57fa\u4e8e\u8ba1\u5212\u7684\u5207\u6362\u800c\u975e\u57fa\u4e8e\u6d4b\u91cf\u7684\u5207\u6362\uff0c\u4ee5\u907f\u514d\u63a5\u5165\u7f51\u548c\u6838\u5fc3\u7f51\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u5177\u4f53\u5b9e\u73b0\u5305\u62ec\uff1a1) \u5f15\u5165\u4e00\u79cd\u4e0e\u6807\u51c65G\u6838\u5fc3\u7f51\u517c\u5bb9\u7684\u536b\u661f\u540c\u6b65\u529f\u80fd\uff08SSF\uff09\uff1b2) \u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u4fe1\u53f7\u5f3a\u5ea6\u9884\u6d4b\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff1b3) \u8bbe\u8ba1\u4e00\u4e2a\u9ad8\u6548\u7684\u5207\u6362\u8c03\u5ea6\u7b97\u6cd5\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5207\u6362\u65b9\u6848\u4e0e\u6807\u51c6NTN\u5207\u6362\u65b9\u6848\u4ee5\u53ca\u5176\u4ed6\u4e24\u79cd\u73b0\u6709\u5207\u6362\u65b9\u6cd5\u76f8\u6bd4\uff0c\u80fd\u5c06\u5207\u6362\u5ef6\u8fdf\u964d\u4f4e21\u500d\uff0c\u5e76\u663e\u8457\u63d0\u5347\u7f51\u7edc\u7a33\u5b9a\u6027\u548c\u7528\u6237\u7ea7\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5e76\u884c\u5207\u6362\u673a\u5236\u80fd\u591f\u6709\u6548\u89e3\u51b3LEO\u536b\u661f\u7f51\u7edc\u4e2d\u7684\u9ad8\u5ef6\u8fdf\u5207\u6362\u95ee\u9898\uff0c\u663e\u8457\u6539\u5584\u7f51\u7edc\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u4e3a\u672a\u6765\u79fb\u52a8\u536b\u661f\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07203", "pdf": "https://arxiv.org/pdf/2507.07203", "abs": "https://arxiv.org/abs/2507.07203", "authors": ["Minkyung Kim", "Junsik Kim", "Hwidong Bae", "Woongcheol Yang", "Sangdon Park", "Sohee Bae"], "title": "State-Inference-Based Prompting for Natural Language Trading with Game NPCs", "categories": ["cs.AI"], "comment": "9 pages main content, 4 pages appendix, 3 figures. Accepted to the\n  KDD 2025 Workshop on Prompt Optimization", "summary": "Large Language Models enable dynamic game interactions but struggle with\nrule-governed trading systems. Current implementations suffer from rule\nviolations, such as item hallucinations and calculation errors, that erode\nplayer trust. Here, State-Inference-Based Prompting (SIBP) enables reliable\ntrading through autonomous dialogue state inference and context-specific rule\nadherence. The approach decomposes trading into six states within a unified\nprompt framework, implementing context-aware item referencing and\nplaceholder-based price calculations. Evaluation across 100 trading dialogues\ndemonstrates >97% state compliance, >95% referencing accuracy, and 99.7%\ncalculation precision. SIBP maintains computational efficiency while\noutperforming baseline approaches, establishing a practical foundation for\ntrustworthy NPC interactions in commercial games.", "AI": {"tldr": "SIBP\u901a\u8fc7\u5bf9\u8bdd\u72b6\u6001\u63a8\u65ad\u548c\u89c4\u5219\u9075\u5b88\uff0c\u89e3\u51b3\u4e86LLMs\u5728\u6e38\u620f\u4ea4\u6613\u4e2d\u5e38\u89c1\u7684\u89c4\u5219\u8fdd\u89c4\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u4e3aNPC\u4ea4\u4e92\u63d0\u4f9b\u5b9e\u7528\u57fa\u7840\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6e38\u620f\u52a8\u6001\u4ea4\u4e92\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u89c4\u5219\u4e25\u683c\u7684\u4ea4\u6613\u7cfb\u7edf\u4e2d\uff0c\u5e38\u51fa\u73b0\u7269\u54c1\u5e7b\u89c9\u548c\u8ba1\u7b97\u9519\u8bef\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u73a9\u5bb6\u4fe1\u4efb\u5ea6\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u72b6\u6001\u63a8\u65ad\u7684\u63d0\u793a\uff08SIBP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u4e3b\u5bf9\u8bdd\u72b6\u6001\u63a8\u65ad\u548c\u4e0a\u4e0b\u6587\u7279\u5b9a\u89c4\u5219\u9075\u5b88\u5b9e\u73b0\u53ef\u9760\u4ea4\u6613\u3002\u8be5\u65b9\u6cd5\u5c06\u4ea4\u6613\u5206\u89e3\u4e3a\u516d\u4e2a\u72b6\u6001\uff0c\u7edf\u4e00\u4e8e\u4e00\u4e2a\u63d0\u793a\u6846\u67b6\u5185\uff0c\u5e76\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7269\u54c1\u5f15\u7528\u548c\u57fa\u4e8e\u5360\u4f4d\u7b26\u7684\u4ef7\u683c\u8ba1\u7b97\u3002", "result": "\u5728100\u4e2a\u4ea4\u6613\u5bf9\u8bdd\u8bc4\u4f30\u4e2d\uff0cSIBP\u5c55\u73b0\u4e86\u8d85\u8fc797%\u7684\u72b6\u6001\u5408\u89c4\u6027\u3001\u8d85\u8fc795%\u7684\u5f15\u7528\u51c6\u786e\u6027\u548c99.7%\u7684\u8ba1\u7b97\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u5e76\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SIBP\u4e3a\u5546\u4e1a\u6e38\u620f\u4e2d\u53ef\u4fe1\u8d56\u7684NPC\u4ea4\u4e92\u5960\u5b9a\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u89c4\u5219\u9a71\u52a8\u578b\u6e38\u620f\u7cfb\u7edf\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002"}}
{"id": "2507.07137", "pdf": "https://arxiv.org/pdf/2507.07137", "abs": "https://arxiv.org/abs/2507.07137", "authors": ["Eric Yeats", "Darryl Hannan", "Henry Kvinge", "Timothy Doster", "Scott Mahan"], "title": "Automating Evaluation of Diffusion Model Unlearning with (Vision-) Language Model World Knowledge", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Machine unlearning (MU) is a promising cost-effective method to cleanse\nundesired information (generated concepts, biases, or patterns) from\nfoundational diffusion models. While MU is orders of magnitude less costly than\nretraining a diffusion model without the undesired information, it can be\nchallenging and labor-intensive to prove that the information has been fully\nremoved from the model. Moreover, MU can damage diffusion model performance on\nsurrounding concepts that one would like to retain, making it unclear if the\ndiffusion model is still fit for deployment. We introduce autoeval-dmun, an\nautomated tool which leverages (vision-) language models to thoroughly assess\nunlearning in diffusion models. Given a target concept, autoeval-dmun extracts\nstructured, relevant world knowledge from the language model to identify nearby\nconcepts which are likely damaged by unlearning and to circumvent unlearning\nwith adversarial prompts. We use our automated tool to evaluate popular\ndiffusion model unlearning methods, revealing that language models (1) impose\nsemantic orderings of nearby concepts which correlate well with unlearning\ndamage and (2) effectively circumvent unlearning with synthetic adversarial\nprompts.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aautoeval-dmun\u7684\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u6269\u6563\u6a21\u578b\u7684\u673a\u5668\u9057\u5fd8\u6548\u679c\uff0c\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u80fd\u6709\u6548\u5173\u8054\u8bed\u4e49\u635f\u4f24\u5e76\u751f\u6210\u89c4\u907f\u9057\u5fd8\u7684\u5bf9\u6297\u6027\u63d0\u793a\u3002", "motivation": "\u673a\u5668\u9057\u5fd8\uff08MU\uff09\u867d\u80fd\u6709\u6548\u6e05\u9664\u6269\u6563\u6a21\u578b\u4e2d\u7684\u4e0d\u826f\u4fe1\u606f\u4e14\u6210\u672c\u8f83\u4f4e\uff0c\u4f46\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u4e00\u662f\u96be\u4ee5\u9a8c\u8bc1\u4fe1\u606f\u662f\u5426\u88ab\u5b8c\u5168\u6e05\u9664\uff1b\u4e8c\u662fMU\u53ef\u80fd\u5bf9\u6a21\u578b\u4fdd\u7559\u76f8\u5173\u6982\u5ff5\u7684\u6027\u80fd\u9020\u6210\u635f\u5bb3\uff0c\u5f71\u54cd\u6a21\u578b\u90e8\u7f72\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u4e14\u6df1\u5165\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u5e76\u5f15\u5165\u4e86\u81ea\u52a8\u5316\u5de5\u5177autoeval-dmun\uff0c\u8be5\u5de5\u5177\u5229\u7528\uff08\u89c6\u89c9-\uff09\u8bed\u8a00\u6a21\u578b\u6765\u5168\u9762\u8bc4\u4f30\u6269\u6563\u6a21\u578b\u7684\u673a\u5668\u9057\u5fd8\u6548\u679c\u3002\u5176\u5de5\u4f5c\u673a\u5236\u662f\uff1a\u9488\u5bf9\u76ee\u6807\u6982\u5ff5\uff0cautoeval-dmun\u4ece\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u4ee5\u8bc6\u522b\u53ef\u80fd\u53d7\u635f\u7684\u90bb\u8fd1\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\u6765\u6d4b\u8bd5\u6216\u89c4\u907f\u9057\u5fd8\u3002", "result": "\u901a\u8fc7\u4f7f\u7528autoeval-dmun\u8bc4\u4f30\u4e86\u591a\u79cd\u6d41\u884c\u7684\u6269\u6563\u6a21\u578b\u9057\u5fd8\u65b9\u6cd5\uff0c\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff1a1) \u8bed\u8a00\u6a21\u578b\u65bd\u52a0\u7684\u90bb\u8fd1\u6982\u5ff5\u8bed\u4e49\u6392\u5e8f\u4e0e\u9057\u5fd8\u9020\u6210\u7684\u635f\u5bb3\u7a0b\u5ea6\u5177\u6709\u826f\u597d\u76f8\u5173\u6027\uff1b2) \u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u751f\u6210\u5408\u6210\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u4ece\u800c\u89c4\u907f\u673a\u5668\u9057\u5fd8\u3002", "conclusion": "autoeval-dmun\u63d0\u4f9b\u4e86\u4e00\u79cd\u5bf9\u6269\u6563\u6a21\u578b\u673a\u5668\u9057\u5fd8\u6548\u679c\u8fdb\u884c\u81ea\u52a8\u5316\u3001\u6df1\u5165\u8bc4\u4f30\u7684\u65b0\u8303\u5f0f\u3002\u672c\u7814\u7a76\u4e0d\u4ec5\u9a8c\u8bc1\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u9057\u5fd8\u526f\u4f5c\u7528\u548c\u53d1\u73b0\u6f5c\u5728\u9057\u5fd8\u6f0f\u6d1e\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u4e5f\u4e3a\u672a\u6765\u6539\u8fdb\u548c\u9a8c\u8bc1\u673a\u5668\u9057\u5fd8\u6280\u672f\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u5de5\u5177\u548c\u89c1\u89e3\u3002"}}
{"id": "2507.07139", "pdf": "https://arxiv.org/pdf/2507.07139", "abs": "https://arxiv.org/abs/2507.07139", "authors": ["Renyang Liu", "Guanlin Li", "Tianwei Zhang", "See-Kiong Ng"], "title": "Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "Recent advances in image generation models (IGMs), particularly\ndiffusion-based architectures such as Stable Diffusion (SD), have markedly\nenhanced the quality and diversity of AI-generated visual content. However,\ntheir generative capability has also raised significant ethical, legal, and\nsocietal concerns, including the potential to produce harmful, misleading, or\ncopyright-infringing content. To mitigate these concerns, machine unlearning\n(MU) emerges as a promising solution by selectively removing undesirable\nconcepts from pretrained models. Nevertheless, the robustness and effectiveness\nof existing unlearning techniques remain largely unexplored, particularly in\nthe presence of multi-modal adversarial inputs.\n  To bridge this gap, we propose Recall, a novel adversarial framework\nexplicitly designed to compromise the robustness of unlearned IGMs. Unlike\nexisting approaches that predominantly rely on adversarial text prompts, Recall\nexploits the intrinsic multi-modal conditioning capabilities of diffusion\nmodels by efficiently optimizing adversarial image prompts with guidance from a\nsingle semantically relevant reference image. Extensive experiments across ten\nstate-of-the-art unlearning methods and diverse tasks show that Recall\nconsistently outperforms existing baselines in terms of adversarial\neffectiveness, computational efficiency, and semantic fidelity with the\noriginal textual prompt. These findings reveal critical vulnerabilities in\ncurrent unlearning mechanisms and underscore the need for more robust solutions\nto ensure the safety and reliability of generative models. Code and data are\npublicly available at \\textcolor{blue}{https://github.com/ryliu68/RECALL}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Recall\u6846\u67b6\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u5bf9\u6297\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u591a\u6a21\u6001\u56fe\u50cf\u63d0\u793a\u6765\u653b\u51fb\u5e76\u63ed\u793a\u5df2\u9057\u5fd8\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u9c81\u68d2\u7684\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08IGMs\uff09\u867d\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u751f\u6210\u6709\u5bb3\u6216\u4fb5\u6743\u5185\u5bb9\u7684\u4f26\u7406\u3001\u6cd5\u5f8b\u548c\u793e\u4f1a\u62c5\u5fe7\u3002\u673a\u5668\u9057\u5fd8\uff08MU\uff09\u662f\u89e3\u51b3\u6b64\u95ee\u9898\u7684\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u9057\u5fd8\u6280\u672f\u5728\u9762\u5bf9\u591a\u6a21\u6001\u5bf9\u6297\u6027\u8f93\u5165\u65f6\uff0c\u5176\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Recall\u6846\u67b6\uff0c\u4e00\u4e2a\u65e8\u5728\u7834\u574f\u5df2\u9057\u5fd8IGMs\u9c81\u68d2\u6027\u7684\u5bf9\u6297\u6027\u6846\u67b6\u3002\u4e0e\u73b0\u6709\u4f9d\u8d56\u5bf9\u6297\u6027\u6587\u672c\u63d0\u793a\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cRecall\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u591a\u6a21\u6001\u6761\u4ef6\u80fd\u529b\uff0c\u901a\u8fc7\u4ece\u5355\u4e2a\u8bed\u4e49\u76f8\u5173\u7684\u53c2\u8003\u56fe\u50cf\u5f15\u5bfc\uff0c\u9ad8\u6548\u4f18\u5316\u5bf9\u6297\u6027\u56fe\u50cf\u63d0\u793a\u3002", "result": "\u5728\u5341\u79cd\u6700\u5148\u8fdb\u7684\u9057\u5fd8\u65b9\u6cd5\u548c\u4e0d\u540c\u4efb\u52a1\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRecall\u5728\u5bf9\u6297\u6027\u6709\u6548\u6027\u3001\u8ba1\u7b97\u6548\u7387\u4ee5\u53ca\u4e0e\u539f\u59cb\u6587\u672c\u63d0\u793a\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u65b9\u9762\uff0c\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u5f53\u524d\u9057\u5fd8\u673a\u5236\u7684\u5173\u952e\u6f0f\u6d1e\u3002", "conclusion": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u673a\u5236\u5b58\u5728\u5173\u952e\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4e86\u4e3a\u4e86\u786e\u4fdd\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07414", "pdf": "https://arxiv.org/pdf/2507.07414", "abs": "https://arxiv.org/abs/2507.07414", "authors": ["Fardin Rastakhiz"], "title": "GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Time, cost, and energy efficiency are critical considerations in\nDeep-Learning (DL), particularly when processing long texts. Transformers,\nwhich represent the current state of the art, exhibit quadratic computational\ncomplexity relative to input length, making them inefficient for extended\ndocuments. This study introduces a novel model architecture that combines Graph\nNeural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated\nwith a real-time, end-to-end graph generation mechanism. The model processes\ncompact batches of character-level inputs without requiring padding or\ntruncation. To enhance performance while maintaining high speed and efficiency,\nthe model incorporates information from Large Language Models (LLMs), such as\ntoken embeddings and sentiment polarities, through efficient dictionary\nlookups. It captures local contextual patterns using CNNs, expands local\nreceptive fields via lattice-based graph structures, and employs small-world\ngraphs to aggregate document-level information. The generated graphs exhibit\nstructural properties indicative of meaningful semantic organization, with an\naverage clustering coefficient of approximately 0.45 and an average shortest\npath length ranging between 4 and 5. The model is evaluated across multiple\ntext classification tasks, including sentiment analysis and\nnews-categorization, and is compared against state-of-the-art models.\nExperimental results confirm the proposed model's efficiency and competitive\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408GNN\u548cCNN\u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u5b9e\u65f6\u56fe\u751f\u6210\u548c\u5b57\u7b26\u7ea7\u8f93\u5165\u5904\u7406\u957f\u6587\u672c\uff0c\u89e3\u51b3\u4e86Transformer\u7684\u6548\u7387\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5904\u7406\u957f\u6587\u672c\u65f6\uff0c\u65f6\u95f4\u3001\u6210\u672c\u548c\u80fd\u6548\u662f\u5173\u952e\u8003\u91cf\u3002\u5f53\u524d\u6700\u5148\u8fdb\u7684Transformer\u6a21\u578b\u8ba1\u7b97\u590d\u6742\u5ea6\u968f\u8f93\u5165\u957f\u5ea6\u5448\u4e8c\u6b21\u65b9\u589e\u957f\uff0c\u5bfc\u81f4\u5176\u5728\u5904\u7406\u957f\u6587\u6863\u65f6\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u7684\u65b0\u578b\u6a21\u578b\u67b6\u6784\uff0c\u5e76\u96c6\u6210\u5b9e\u65f6\u7aef\u5230\u7aef\u56fe\u751f\u6210\u673a\u5236\u3002\u6a21\u578b\u5904\u7406\u7d27\u51d1\u7684\u5b57\u7b26\u7ea7\u8f93\u5165\u6279\u6b21\uff0c\u65e0\u9700\u586b\u5145\u6216\u622a\u65ad\u3002\u901a\u8fc7\u9ad8\u6548\u5b57\u5178\u67e5\u627e\uff0c\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4fe1\u606f\uff08\u5982\u8bcd\u5143\u5d4c\u5165\u548c\u60c5\u611f\u6781\u6027\uff09\u4ee5\u63d0\u5347\u6027\u80fd\u3002CNN\u7528\u4e8e\u6355\u83b7\u5c40\u90e8\u4e0a\u4e0b\u6587\u6a21\u5f0f\uff0c\u57fa\u4e8e\u683c\u5b50\u7684\u56fe\u7ed3\u6784\u6269\u5c55\u5c40\u90e8\u611f\u53d7\u91ce\uff0c\u5c0f\u4e16\u754c\u56fe\u7528\u4e8e\u805a\u5408\u6587\u6863\u7ea7\u4fe1\u606f\u3002", "result": "\u751f\u6210\u7684\u56fe\u8868\u73b0\u51fa\u6709\u610f\u4e49\u7684\u8bed\u4e49\u7ec4\u7ec7\u7ed3\u6784\uff0c\u5e73\u5747\u805a\u7c7b\u7cfb\u6570\u7ea6\u4e3a0.45\uff0c\u5e73\u5747\u6700\u77ed\u8def\u5f84\u957f\u5ea6\u57284\u52305\u4e4b\u95f4\u3002\u8be5\u6a21\u578b\u5728\u60c5\u611f\u5206\u6790\u548c\u65b0\u95fb\u5206\u7c7b\u7b49\u591a\u79cd\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u6a21\u578b\u7684\u6548\u7387\u548c\u7ade\u4e89\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ed3\u5408GNN\u548cCNN\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u56fe\u751f\u6210\u548c\u5904\u7406\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u6587\u672c\u5904\u7406\u4e2dTransformer\u7684\u6548\u7387\u74f6\u9888\uff0c\u5e76\u5728\u591a\u9879\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u9ad8\u6548\u4e14\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.07481", "pdf": "https://arxiv.org/pdf/2507.07481", "abs": "https://arxiv.org/abs/2507.07481", "authors": ["Wen Zhang", "Aimin Wang", "Jiahui Li", "Geng Sun", "Jiacheng Wang", "Weijie Yuan", "Dusit Niyato"], "title": "Energy Transfer and Data Collection from Batteryless Sensors in Low-altitude Wireless Networks", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "The integration of wireless power transfer (WPT) with Internet of Things\n(IoT) offers promising solutions for sensing applications, but faces\nsignificant challenges when deployed in hard-to-access areas such as\nhigh-temperature environments. In such extreme conditions, traditional fixed\nWPT infrastructure cannot be safely installed, and batteries rapidly degrade\ndue to hardware failures. In this paper, we propose an uncrewed aerial vehicle\n(UAV)-assisted data collection and WPT framework for batteryless sensor (BLS)\nnetworks deployed in these challenging environments. Specifically, we consider\na practical scenario where a UAV first transfers energy to BLS nodes via WPT,\nenabling these nodes to subsequently transmit their collected data to the UAV\nthrough orthogonal frequency-division multiple access (OFDMA). Then, we\nformulate a multi-objective optimization problem that aims to maximize the fair\ndata collection volume while minimizing the UAV energy consumption through\njoint optimization of transmit power allocation and flight trajectory planning.\nDue to the non-convex nature and dynamic characteristics of this problem,\nconventional optimization methods prove inadequate. To address these\nchallenges, we propose an enhanced soft actor-critic algorithm with\nparameter-free attention, prioritized experience replay, and value-based reward\ncentering (SAC-PPV), thereby improving the exploration efficiency and learning\nstability of the algorithm in complex WPT scenarios. Simulation results\ndemonstrate that the proposed approach consistently outperforms benchmark\nalgorithms under various network configurations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u4eba\u673a\u8f85\u52a9\u7684\u65e0\u7ebf\u80fd\u91cf\u4f20\u8f93\u4e0e\u6570\u636e\u6536\u96c6\u6846\u67b6\uff0c\u7528\u4e8e\u6076\u52a3\u73af\u5883\u4e0b\u7684\u65e0\u7535\u6c60\u4f20\u611f\u5668\u7f51\u7edc\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u5f3a\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08SAC-PPV\uff09\u6765\u4f18\u5316\u4f20\u8f93\u4e0e\u98de\u884c\u8def\u5f84\uff0c\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u6536\u96c6\u548c\u4f4e\u80fd\u8017\u3002", "motivation": "\u5728\u9ad8\u6e29\u7b49\u96be\u4ee5\u8fdb\u5165\u7684\u6076\u52a3\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u56fa\u5b9aWPT\u57fa\u7840\u8bbe\u65bd\u5b89\u88c5\u56f0\u96be\uff0c\u4e14\u7535\u6c60\u5bff\u547d\u77ed\u3001\u6613\u5931\u6548\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u7269\u8054\u7f51\u4f20\u611f\u5e94\u7528\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u65e0\u4eba\u673a\u8f85\u52a9\u7684\u65e0\u7535\u6c60\u4f20\u611f\u5668\uff08BLS\uff09\u7f51\u7edc\u6570\u636e\u6536\u96c6\u548cWPT\u6846\u67b6\u3002\u65e0\u4eba\u673a\u9996\u5148\u901a\u8fc7WPT\u4e3aBLS\u8282\u70b9\u4f9b\u80fd\uff0c\u8282\u70b9\u968f\u540e\u901a\u8fc7OFDMA\u5411\u65e0\u4eba\u673a\u4f20\u8f93\u6570\u636e\u3002\u5236\u5b9a\u4e00\u4e2a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u65e8\u5728\u901a\u8fc7\u8054\u5408\u4f18\u5316\u53d1\u5c04\u529f\u7387\u5206\u914d\u548c\u98de\u884c\u8f68\u8ff9\u89c4\u5212\uff0c\u6700\u5927\u5316\u516c\u5e73\u6570\u636e\u6536\u96c6\u91cf\u540c\u65f6\u6700\u5c0f\u5316\u65e0\u4eba\u673a\u80fd\u8017\u3002\u4e3a\u89e3\u51b3\u8be5\u975e\u51f8\u3001\u52a8\u6001\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u589e\u5f3a\u578bSoft Actor-Critic\u7b97\u6cd5\uff08SAC-PPV\uff09\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u65e0\u53c2\u6570\u6ce8\u610f\u529b\u3001\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u548c\u57fa\u4e8e\u4ef7\u503c\u7684\u5956\u52b1\u4e2d\u5fc3\u5316\uff0c\u4ee5\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u548c\u5b66\u4e60\u7a33\u5b9a\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684SAC-PPV\u65b9\u6cd5\u5728\u5404\u79cd\u7f51\u7edc\u914d\u7f6e\u4e0b\uff0c\u6027\u80fd\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u7b97\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4eba\u673a\u8f85\u52a9\u7684\u65e0\u7ebf\u80fd\u91cf\u4f20\u8f93\u4e0e\u6570\u636e\u6536\u96c6\u6846\u67b6\u53ca\u76f8\u5e94\u7684SAC-PPV\u4f18\u5316\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6076\u52a3\u73af\u5883\u4e0b\u65e0\u7535\u6c60\u4f20\u611f\u5668\u7f51\u7edc\u7684\u4f9b\u80fd\u548c\u6570\u636e\u4f20\u8f93\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2507.07217", "pdf": "https://arxiv.org/pdf/2507.07217", "abs": "https://arxiv.org/abs/2507.07217", "authors": ["Zili Wang", "Frank Montabon", "Kristin Yvonne Rozier"], "title": "Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains", "categories": ["cs.AI", "cs.LG", "cs.LO", "I.2.4; I.2.7; J.4"], "comment": null, "summary": "Supply chain networks are complex systems that are challenging to analyze;\nthis problem is exacerbated when there are illicit activities involved in the\nsupply chain, such as counterfeit parts, forced labor, or human trafficking.\nWhile machine learning (ML) can find patterns in complex systems like supply\nchains, traditional ML techniques require large training data sets. However,\nillicit supply chains are characterized by very sparse data, and the data that\nis available is often (purposely) corrupted or unreliable in order to hide the\nnature of the activities. We need to be able to automatically detect new\npatterns that correlate with such illegal activity over complex, even temporal\ndata, without requiring large training data sets. We explore neurosymbolic\nmethods for identifying instances of illicit activity in supply chains and\ncompare the effectiveness of manual and automated feature extraction from news\narticles accurately describing illicit activities uncovered by authorities. We\npropose a question tree approach for querying a large language model (LLM) to\nidentify and quantify the relevance of articles. This enables a systematic\nevaluation of the differences between human and machine classification of news\narticles related to forced labor in supply chains.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u975e\u6cd5\u4f9b\u5e94\u94fe\u6570\u636e\u7a00\u758f\u4e14\u4e0d\u53ef\u9760\u7684\u68c0\u6d4b\u96be\u9898\uff0c\u63d0\u51fa\u63a2\u7d22\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7ed3\u5408\u95ee\u9898\u6811\u4ece\u65b0\u95fb\u6587\u7ae0\u4e2d\u81ea\u52a8\u63d0\u53d6\u7279\u5f81\uff0c\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u4eba\u5de5\u4e0e\u673a\u5668\u5728\u5206\u7c7b\u975e\u6cd5\u4f9b\u5e94\u94fe\u6d3b\u52a8\u76f8\u5173\u65b0\u95fb\u6587\u7ae0\u4e0a\u7684\u5dee\u5f02\u3002", "motivation": "\u4f9b\u5e94\u94fe\u7f51\u7edc\u590d\u6742\uff0c\u975e\u6cd5\u6d3b\u52a8\uff08\u5982\u5047\u5192\u3001\u5f3a\u8feb\u52b3\u52a8\u3001\u4eba\u53e3\u8d29\u8fd0\uff09\u4f7f\u5176\u5206\u6790\u96be\u5ea6\u500d\u589e\u3002\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u975e\u6cd5\u4f9b\u5e94\u94fe\u6570\u636e\u7a00\u758f\u4e14\u5e38\u88ab\u6545\u610f\u7834\u574f\u6216\u4e0d\u53ef\u9760\u3002\u56e0\u6b64\uff0c\u9700\u8981\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5373\u53ef\u81ea\u52a8\u68c0\u6d4b\u590d\u6742\u751a\u81f3\u65f6\u5e8f\u6570\u636e\u4e2d\u4e0e\u975e\u6cd5\u6d3b\u52a8\u76f8\u5173\u7684\u65b0\u6a21\u5f0f\u3002", "method": "1. \u63a2\u7d22\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u4ee5\u8bc6\u522b\u4f9b\u5e94\u94fe\u4e2d\u7684\u975e\u6cd5\u6d3b\u52a8\u5b9e\u4f8b\u30022. \u6bd4\u8f83\u4ece\u51c6\u786e\u63cf\u8ff0\u5f53\u5c40\u53d1\u73b0\u7684\u975e\u6cd5\u6d3b\u52a8\u65b0\u95fb\u6587\u7ae0\u4e2d\u624b\u52a8\u548c\u81ea\u52a8\u63d0\u53d6\u7279\u5f81\u7684\u6709\u6548\u6027\u30023. \u63d0\u51fa\u4e00\u79cd\u95ee\u9898\u6811\u65b9\u6cd5\uff0c\u7528\u4e8e\u67e5\u8be2\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee5\u8bc6\u522b\u548c\u91cf\u5316\u6587\u7ae0\u7684\u76f8\u5173\u6027\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u65e8\u5728\u5b9e\u73b0\u5bf9\u4eba\u7c7b\u548c\u673a\u5668\u5728\u5206\u7c7b\u4e0e\u4f9b\u5e94\u94fe\u4e2d\u5f3a\u8feb\u52b3\u52a8\u76f8\u5173\u65b0\u95fb\u6587\u7ae0\u65b9\u9762\u5dee\u5f02\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u548c\u57fa\u4e8eLLM\u7684\u95ee\u9898\u6811\u67e5\u8be2\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u65b0\u7684\u9014\u5f84\uff0c\u7528\u4e8e\u5728\u6570\u636e\u7a00\u758f\u3001\u4e0d\u53ef\u9760\u7684\u975e\u6cd5\u4f9b\u5e94\u94fe\u73af\u5883\u4e2d\u81ea\u52a8\u68c0\u6d4b\u65b0\u6a21\u5f0f\uff0c\u6709\u671b\u63d0\u9ad8\u975e\u6cd5\u6d3b\u52a8\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.07138", "pdf": "https://arxiv.org/pdf/2507.07138", "abs": "https://arxiv.org/abs/2507.07138", "authors": ["Francesco Ferrini", "Veronica Lachi", "Antonio Longa", "Bruno Lepri", "Andrea Passerini"], "title": "GNNs Meet Sequence Models Along the Shortest-Path: an Expressive Method for Link Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) often struggle to capture the link-specific\nstructural patterns crucial for accurate link prediction, as their node-centric\nmessage-passing schemes overlook the subgraph structures connecting a pair of\nnodes. Existing methods to inject such structural context either incur high\ncomputational cost or rely on simplistic heuristics (e.g., common neighbor\ncounts) that fail to model multi-hop dependencies. We introduce SP4LP (Shortest\nPath for Link Prediction), a novel framework that combines GNN-based node\nencodings with sequence modeling over shortest paths. Specifically, SP4LP first\napplies a GNN to compute representations for all nodes, then extracts the\nshortest path between each candidate node pair and processes the resulting\nsequence of node embeddings using a sequence model. This design enables SP4LP\nto capture expressive multi-hop relational patterns with computational\nefficiency. Empirically, SP4LP achieves state-of-the-art performance across\nlink prediction benchmarks. Theoretically, we prove that SP4LP is strictly more\nexpressive than standard message-passing GNNs and several state-of-the-art\nstructural features methods, establishing it as a general and principled\napproach for link prediction in graphs.", "AI": {"tldr": "SP4LP\u7ed3\u5408GNN\u8282\u70b9\u7f16\u7801\u548c\u6700\u77ed\u8def\u5f84\u5e8f\u5217\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86GNN\u5728\u94fe\u63a5\u9884\u6d4b\u4e2d\u96be\u4ee5\u6355\u83b7\u94fe\u63a5\u7279\u5f02\u6027\u7ed3\u6784\u6a21\u5f0f\u7684\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GNN\u5728\u94fe\u63a5\u9884\u6d4b\u4e2d\u96be\u4ee5\u6355\u83b7\u94fe\u63a5\u7279\u5f02\u6027\u7ed3\u6784\u6a21\u5f0f\uff0c\u56e0\u5176\u8282\u70b9\u4e2d\u5fc3\u7684\u6d88\u606f\u4f20\u9012\u5ffd\u7565\u8282\u70b9\u5bf9\u95f4\u7684\u5b50\u56fe\u7ed3\u6784\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u8981\u4e48\u8fc7\u4e8e\u7b80\u5316\uff0c\u65e0\u6cd5\u5efa\u6a21\u591a\u8df3\u4f9d\u8d56\u3002", "method": "\u63d0\u51faSP4LP\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u9996\u5148\u901a\u8fc7GNN\u8ba1\u7b97\u8282\u70b9\u8868\u793a\uff0c\u7136\u540e\u63d0\u53d6\u5019\u9009\u8282\u70b9\u5bf9\u4e4b\u95f4\u7684\u6700\u77ed\u8def\u5f84\uff0c\u5e76\u4f7f\u7528\u5e8f\u5217\u6a21\u578b\u5904\u7406\u8fd9\u4e9b\u8282\u70b9\u5d4c\u5165\u5e8f\u5217\u3002", "result": "\u7ecf\u9a8c\u4e0a\uff0cSP4LP\u5728\u94fe\u63a5\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002\u7406\u8bba\u4e0a\uff0cSP4LP\u6bd4\u6807\u51c6\u6d88\u606f\u4f20\u9012GNN\u53ca\u591a\u79cd\u73b0\u6709\u7ed3\u6784\u7279\u5f81\u65b9\u6cd5\u66f4\u5177\u8868\u8fbe\u529b\u3002", "conclusion": "SP4LP\u662f\u4e00\u79cd\u901a\u7528\u4e14\u6709\u539f\u5219\u7684\u56fe\u94fe\u63a5\u9884\u6d4b\u65b9\u6cd5\uff0c\u80fd\u9ad8\u6548\u6355\u83b7\u5bcc\u6709\u8868\u8fbe\u529b\u7684\u591a\u8df3\u5173\u7cfb\u6a21\u5f0f\u3002"}}
{"id": "2507.07148", "pdf": "https://arxiv.org/pdf/2507.07148", "abs": "https://arxiv.org/abs/2507.07148", "authors": ["Getamesay Haile Dagnaw", "Yanming Zhu", "Muhammad Hassan Maqsood", "Wencheng Yang", "Xingshuai Dong", "Xuefei Yin", "Alan Wee-Chung Liew"], "title": "Explainable Artificial Intelligence in Biomedical Image Analysis: A Comprehensive Survey", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Explainable artificial intelligence (XAI) has become increasingly important\nin biomedical image analysis to promote transparency, trust, and clinical\nadoption of DL models. While several surveys have reviewed XAI techniques, they\noften lack a modality-aware perspective, overlook recent advances in multimodal\nand vision-language paradigms, and provide limited practical guidance. This\nsurvey addresses this gap through a comprehensive and structured synthesis of\nXAI methods tailored to biomedical image analysis.We systematically categorize\nXAI methods, analyzing their underlying principles, strengths, and limitations\nwithin biomedical contexts. A modality-centered taxonomy is proposed to align\nXAI methods with specific imaging types, highlighting the distinct\ninterpretability challenges across modalities. We further examine the emerging\nrole of multimodal learning and vision-language models in explainable\nbiomedical AI, a topic largely underexplored in previous work. Our\ncontributions also include a summary of widely used evaluation metrics and\nopen-source frameworks, along with a critical discussion of persistent\nchallenges and future directions. This survey offers a timely and in-depth\nfoundation for advancing interpretable DL in biomedical image analysis.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u5bf9\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u4e14\u7ed3\u6784\u5316\u7684\u5408\u6210\u5206\u6790\uff0c\u63d0\u51fa\u6a21\u6001\u4e2d\u5fc3\u5206\u7c7b\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u548c\u89c6\u89c9-\u8bed\u8a00\u8303\u5f0f\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "motivation": "\u73b0\u6709XAI\u7efc\u8ff0\u7f3a\u4e4f\u6a21\u6001\u611f\u77e5\u89c6\u89d2\uff0c\u5ffd\u89c6\u591a\u6a21\u6001\u548c\u89c6\u89c9-\u8bed\u8a00\u8303\u5f0f\u7684\u65b0\u8fdb\u5c55\uff0c\u4e14\u5b9e\u8df5\u6307\u5bfc\u6709\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u9700\u6c42\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7bXAI\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u539f\u7406\u3001\u4f18\u7f3a\u70b9\uff1b\u63d0\u51fa\u4ee5\u6a21\u6001\u4e3a\u4e2d\u5fc3\u7684\u5206\u7c7b\u6cd5\uff1b\u5ba1\u67e5\u591a\u6a21\u6001\u5b66\u4e60\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u751f\u7269\u533b\u5b66AI\u4e2d\u7684\u4f5c\u7528\uff1b\u603b\u7ed3\u8bc4\u4f30\u6307\u6807\u3001\u5f00\u6e90\u6846\u67b6\uff0c\u5e76\u8ba8\u8bba\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "result": "\u63d0\u4f9b\u4e86\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57dfXAI\u65b9\u6cd5\u7684\u5168\u9762\u6982\u8ff0\uff0c\u63d0\u51fa\u4e86\u521b\u65b0\u7684\u6a21\u6001\u4e2d\u5fc3\u5206\u7c7b\u6cd5\uff0c\u586b\u8865\u4e86\u591a\u6a21\u6001\u548c\u89c6\u89c9-\u8bed\u8a00XAI\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u603b\u7ed3\u4e86\u91cd\u8981\u8d44\u6e90\u4e0e\u8ba8\u8bba\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u63a8\u8fdb\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u53ef\u89e3\u91ca\u6df1\u5ea6\u5b66\u4e60\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ca\u65f6\u800c\u6df1\u5165\u7684\u57fa\u7840\u3002"}}
{"id": "2507.07419", "pdf": "https://arxiv.org/pdf/2507.07419", "abs": "https://arxiv.org/abs/2507.07419", "authors": ["Hieu Tran", "Zonghai Yao", "Won Seok Jang", "Sharmin Sultana", "Allen Chang", "Yuan Zhang", "Hong Yu"], "title": "MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning", "categories": ["cs.CL", "cs.AI"], "comment": "Equal contribution for the first two authors. arXiv admin note: text\n  overlap with arXiv:2406.09205", "summary": "Generative AI has demonstrated strong potential in healthcare, from clinical\ndecision support to patient-facing chatbots that improve outcomes. A critical\nchallenge for deployment is effective human-AI communication, where content\nmust be both personalized and understandable. We introduce MedReadCtrl, a\nreadability-controlled instruction tuning framework that enables LLMs to adjust\noutput complexity without compromising meaning. Evaluations of nine datasets\nand three tasks across medical and general domains show that MedReadCtrl\nachieves significantly lower readability instruction-following errors than\nGPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains\non unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).\nExperts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low\nliteracy levels. These gains reflect MedReadCtrl's ability to restructure\nclinical content into accessible, readability-aligned language while preserving\nmedical intent, offering a scalable solution to support patient education and\nexpand equitable access to AI-enabled care.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMedReadCtrl\uff0c\u4e00\u4e2a\u53ef\u63a7\u53ef\u8bfb\u6027\u6307\u4ee4\u5fae\u8c03\u6846\u67b6\uff0c\u4f7f\u5927\u6a21\u578b\u80fd\u5728\u533b\u7597\u9886\u57df\u751f\u6210\u590d\u6742\u6027\u53ef\u8c03\u4e14\u610f\u4e49\u4e0d\u53d8\u7684\u8f93\u51fa\u3002\u5b83\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8eGPT-4\uff0c\u5e76\u83b7\u5f97\u4e13\u5bb6\u9ad8\u5ea6\u8ba4\u53ef\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u60a3\u8005\u6559\u80b2\u548c\u516c\u5e73\u533b\u7597\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u90e8\u7f72\u7684\u5173\u952e\u6311\u6218\u5728\u4e8e\u5b9e\u73b0\u6709\u6548\u7684\u4eba\u673a\u6c9f\u901a\uff0c\u5373\u8f93\u51fa\u5185\u5bb9\u9700\u517c\u5177\u4e2a\u6027\u5316\u548c\u53ef\u7406\u89e3\u6027\uff0c\u80fd\u591f\u8c03\u6574\u590d\u6742\u6027\u800c\u4e0d\u635f\u5bb3\u539f\u610f\u3002", "method": "\u5f15\u5165MedReadCtrl\uff0c\u4e00\u4e2a\u53ef\u63a7\u53ef\u8bfb\u6027\u6307\u4ee4\u5fae\u8c03\u6846\u67b6\uff0c\u65e8\u5728\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u6839\u636e\u6307\u4ee4\u8c03\u6574\u5176\u8f93\u51fa\u5185\u5bb9\u7684\u590d\u6742\u7a0b\u5ea6\uff0c\u540c\u65f6\u786e\u4fdd\u4e0d\u635f\u5bb3\u539f\u59cb\u8bed\u4e49\u3002", "result": "\u57289\u4e2a\u6570\u636e\u96c6\u548c3\u9879\u533b\u7597\u4e0e\u901a\u7528\u9886\u57df\u4efb\u52a1\u7684\u8bc4\u4f30\u4e2d\uff0cMedReadCtrl\u5728\u53ef\u8bfb\u6027\u6307\u4ee4\u9075\u5faa\u9519\u8bef\u65b9\u9762\u663e\u8457\u4f4e\u4e8eGPT-4\uff08\u5982\u5728ReadMe\u4e0a\u4e3a1.39 vs 1.59\uff0cp<0.001\uff09\u3002\u5728\u672a\u89c1\u8fc7\u7684\u4e34\u5e8a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff08\u5982\u5728MTSamples\u4e0aROUGE-L\u63d0\u9ad814.7\uff0cSARI\u63d0\u9ad86.18\uff09\u3002\u4e13\u5bb6\u8bc4\u5ba1\u4e00\u81f4\u504f\u597dMedReadCtrl\uff0871.7% vs 23.3%\uff09\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8bc6\u5b57\u6c34\u5e73\u7684\u573a\u666f\u4e0b\u3002", "conclusion": "MedReadCtrl\u80fd\u5c06\u4e34\u5e8a\u5185\u5bb9\u91cd\u6784\u4e3a\u6613\u4e8e\u7406\u89e3\u3001\u7b26\u5408\u53ef\u8bfb\u6027\u8981\u6c42\u4e14\u4fdd\u7559\u533b\u5b66\u539f\u610f\u7684\u8bed\u8a00\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u652f\u6301\u60a3\u8005\u6559\u80b2\u5e76\u6269\u5927AI\u8f85\u52a9\u533b\u7597\u7684\u516c\u5e73\u53ef\u53ca\u6027\u3002"}}
{"id": "2507.07535", "pdf": "https://arxiv.org/pdf/2507.07535", "abs": "https://arxiv.org/abs/2507.07535", "authors": ["Jingzhao Xie", "Zhenglian Li", "Gang Sun", "Long Luo", "Hongfang Yu", "Dusit Niyato"], "title": "A Fragmentation-Aware Adaptive Bilevel Search Framework for Service Mapping in Computing Power Networks", "categories": ["cs.NI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Computing Power Network (CPN) unifies wide-area computing resources through\ncoordinated network control, while cloud-native abstractions enable flexible\nresource orchestration and on-demand service provisioning atop the elastic\ninfrastructure CPN provides. However, current approaches fall short of fully\nintegrating computing resources via network-enabled coordination as envisioned\nby CPN. In particular, optimally mapping services to an underlying\ninfrastructure to maximize resource efficiency and service satisfaction remains\nchallenging. To overcome this challenge, we formally define the service mapping\nproblem in CPN, establish its theoretical intractability, and identify key\nchallenges in practical optimization. We propose Adaptive Bilevel Search (ABS),\na modular framework featuring (1) graph partitioning-based reformulation to\ncapture variable coupling, (2) a bilevel optimization architecture for\nefficient global exploration with local optimality guarantees, and (3)\nfragmentation-aware evaluation for global performance guidance. Implemented\nusing distributed particle swarm optimization, ABS is extensively evaluated\nacross diverse CPN scenarios, consistently outperforming existing approaches.\nNotably, in complex scenarios, ABS achieves up to 73.2% higher computing\nresource utilization and a 60.2% higher service acceptance ratio compared to\nthe best-performing baseline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Adaptive Bilevel Search (ABS)\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8ba1\u7b97\u7b97\u529b\u7f51\u7edc(CPN)\u4e2d\u7684\u670d\u52a1\u6620\u5c04\u96be\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d44\u6e90\u5229\u7528\u7387\u548c\u670d\u52a1\u63a5\u53d7\u7387\u3002", "motivation": "\u8ba1\u7b97\u7b97\u529b\u7f51\u7edc(CPN)\u65e8\u5728\u901a\u8fc7\u7f51\u7edc\u63a7\u5236\u7edf\u4e00\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f46\u5f53\u524d\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u96c6\u6210\u8d44\u6e90\uff0c\u5c24\u5176\u5728\u6700\u5927\u5316\u8d44\u6e90\u6548\u7387\u548c\u670d\u52a1\u6ee1\u610f\u5ea6\u65b9\u9762\uff0c\u670d\u52a1\u5230\u57fa\u7840\u8bbe\u65bd\u7684\u4f18\u5316\u6620\u5c04\u4ecd\u5177\u6311\u6218\u3002", "method": "\u7814\u7a76\u8005\u9996\u5148\u6b63\u5f0f\u5b9a\u4e49\u4e86CPN\u4e2d\u7684\u670d\u52a1\u6620\u5c04\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5176\u7406\u8bba\u4e0a\u7684\u96be\u89e3\u6027\uff0c\u5e76\u8bc6\u522b\u4e86\u5b9e\u9645\u4f18\u5316\u6311\u6218\u3002\u968f\u540e\u63d0\u51fa\u4e86\u6a21\u5757\u5316\u7684Adaptive Bilevel Search (ABS)\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u5305\u62ec\u57fa\u4e8e\u56fe\u5212\u5206\u7684\u91cd\u6784\u3001\u53cc\u5c42\u4f18\u5316\u67b6\u6784\u4ee5\u53ca\u788e\u7247\u611f\u77e5\u8bc4\u4f30\uff0c\u5e76\u5229\u7528\u5206\u5e03\u5f0f\u7c92\u5b50\u7fa4\u4f18\u5316\u8fdb\u884c\u5b9e\u73b0\u548c\u5e7f\u6cdb\u8bc4\u4f30\u3002", "result": "ABS\u5728\u591a\u79cdCPN\u573a\u666f\u4e0b\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5728\u590d\u6742\u573a\u666f\u4e2d\uff0c\u76f8\u8f83\u4e8e\u6700\u4f73\u57fa\u7ebf\uff0cABS\u5c06\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u63d0\u9ad8\u4e8673.2%\uff0c\u670d\u52a1\u63a5\u53d7\u7387\u63d0\u9ad8\u4e8660.2%\u3002", "conclusion": "Adaptive Bilevel Search (ABS)\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86CPN\u4e2d\u7684\u670d\u52a1\u6620\u5c04\u95ee\u9898\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u8d44\u6e90\u5229\u7528\u6548\u7387\u548c\u670d\u52a1\u4ea4\u4ed8\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.07257", "pdf": "https://arxiv.org/pdf/2507.07257", "abs": "https://arxiv.org/abs/2507.07257", "authors": ["Licong Xu", "Milind Sarkar", "Anto I. Lonappan", "\u00cd\u00f1igo Zubeldia", "Pablo Villanueva-Domingo", "Santiago Casas", "Christian Fidler", "Chetana Amancharla", "Ujjwal Tiwari", "Adrian Bayer", "Chadi Ait Ekiou", "Miles Cranmer", "Adrian Dimitrov", "James Fergusson", "Kahaan Gandhi", "Sven Krippendorf", "Andrew Laverick", "Julien Lesgourgues", "Antony Lewis", "Thomas Meier", "Blake Sherwin", "Kristen Surrao", "Francisco Villaescusa-Navarro", "Chi Wang", "Xueqing Xu", "Boris Bolliet"], "title": "Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery", "categories": ["cs.AI", "astro-ph.IM", "cs.CL", "cs.MA"], "comment": "Accepted contribution to the ICML 2025 Workshop on Machine Learning\n  for Astrophysics. Code: https://github.com/CMBAgents/cmbagent; Videos:\n  https://www.youtube.com/@cmbagent; HuggingFace:\n  https://huggingface.co/spaces/astropilot-ai/cmbagent; Cloud:\n  https://cmbagent.cloud", "summary": "We present a multi-agent system for automation of scientific research tasks,\ncmbagent. The system is formed by about 30 Large Language Model (LLM) agents\nand implements a Planning & Control strategy to orchestrate the agentic\nworkflow, with no human-in-the-loop at any point. Each agent specializes in a\ndifferent task (performing retrieval on scientific papers and codebases,\nwriting code, interpreting results, critiquing the output of other agents) and\nthe system is able to execute code locally. We successfully apply cmbagent to\ncarry out a PhD level cosmology task (the measurement of cosmological\nparameters using supernova data) and evaluate its performance on two benchmark\nsets, finding superior performance over state-of-the-art LLMs. The source code\nis available on GitHub, demonstration videos are also available, and the system\nis deployed on HuggingFace and will be available on the cloud.", "AI": {"tldr": "\u4e00\u4e2a\u540d\u4e3acmbagent\u7684\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5b9e\u73b0\u4e86\u79d1\u5b66\u7814\u7a76\u4efb\u52a1\u7684\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u5728\u535a\u58eb\u7ea7\u5b87\u5b99\u5b66\u4efb\u52a1\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u65e8\u5728\u5b9e\u73b0\u79d1\u5b66\u7814\u7a76\u4efb\u52a1\u7684\u81ea\u52a8\u5316\uff0c\u63d0\u9ad8\u7814\u7a76\u6548\u7387\uff0c\u5e76\u514b\u670d\u5355\u4e00\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6216\u4eba\u5de5\u5e72\u9884\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u7531\u5927\u7ea630\u4e2aLLM\u667a\u80fd\u4f53\u7ec4\u6210\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edfcmbagent\u3002\u8be5\u7cfb\u7edf\u91c7\u7528\u201c\u89c4\u5212\u4e0e\u63a7\u5236\u201d\u7b56\u7565\u6765\u7f16\u6392\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5168\u7a0b\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002\u6bcf\u4e2a\u667a\u80fd\u4f53\u4e13\u6ce8\u4e8e\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u8bba\u6587\u548c\u4ee3\u7801\u5e93\u68c0\u7d22\u3001\u4ee3\u7801\u7f16\u5199\u3001\u7ed3\u679c\u89e3\u91ca\u3001\u8f93\u51fa\u8bc4\u4f30\uff09\uff0c\u5e76\u5177\u5907\u672c\u5730\u4ee3\u7801\u6267\u884c\u80fd\u529b\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u5b8c\u6210\u4e00\u9879\u535a\u58eb\u7ea7\u522b\u7684\u5b87\u5b99\u5b66\u4efb\u52a1\uff08\u5229\u7528\u8d85\u65b0\u661f\u6570\u636e\u6d4b\u91cf\u5b87\u5b99\u5b66\u53c2\u6570\uff09\u3002\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684LLM\u3002", "conclusion": "cmbagent\u8bc1\u660e\u4e86\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728\u81ea\u52a8\u5316\u590d\u6742\u79d1\u5b66\u7814\u7a76\u4efb\u52a1\u65b9\u9762\u7684\u53ef\u884c\u6027\u548c\u9ad8\u6548\u6027\uff0c\u5176\u6027\u80fd\u8d85\u8d8a\u4e86\u73b0\u6709LLM\uff0c\u5c55\u73b0\u4e86\u5728\u9ad8\u7ea7\u79d1\u5b66\u4efb\u52a1\u81ea\u52a8\u5316\u9886\u57df\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.07140", "pdf": "https://arxiv.org/pdf/2507.07140", "abs": "https://arxiv.org/abs/2507.07140", "authors": ["Samin Yeasar Arnob", "Zhan Su", "Minseon Kim", "Oleksiy Ostapenko", "Riyasat Ohib", "Esra'a Saleh", "Doina Precup", "Lucas Caccia", "Alessandro Sordoni"], "title": "Exploring Sparse Adapters for Scalable Merging of Parameter Efficient Experts", "categories": ["cs.LG"], "comment": null, "summary": "Merging parameter-efficient task experts has recently gained growing\nattention as a way to build modular architectures that can be rapidly adapted\non the fly for specific downstream tasks, without requiring additional\nfine-tuning. Typically, LoRA serves as the foundational building block of such\nparameter-efficient modular architectures, leveraging low-rank weight\nstructures to reduce the number of trainable parameters. In this paper, we\nstudy the properties of sparse adapters, which train only a subset of weights\nin the base neural network, as potential building blocks of modular\narchitectures. First, we propose a simple method for training highly effective\nsparse adapters, which is conceptually simpler than existing methods in the\nliterature and surprisingly outperforms both LoRA and full fine-tuning in our\nsetting. Next, we investigate the merging properties of these sparse adapters\nby merging adapters for up to 20 natural language processing tasks, thus\nscaling beyond what is usually studied in the literature. Our findings\ndemonstrate that sparse adapters yield superior in-distribution performance\npost-merging compared to LoRA or full model merging. Achieving strong held-out\nperformance remains a challenge for all methods considered.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u7a00\u758f\u9002\u914d\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u76f8\u8f83\u4e8eLoRA\u548c\u5168\u91cf\u5fae\u8c03\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u66f4\u4f18\uff0c\u7279\u522b\u662f\u5728\u5408\u5e76\u540e\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u57df\u5185\u6027\u80fd\uff0c\u4f46\u5728\u57df\u5916\u6027\u80fd\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u4e3a\u6784\u5efa\u53ef\u5feb\u901f\u9002\u5e94\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u7684\u6a21\u5757\u5316\u67b6\u6784\uff0c\u73b0\u6709\u7814\u7a76\u4fa7\u91cd\u4e8e\u53c2\u6570\u9ad8\u6548\u7684\u4e13\u5bb6\u5408\u5e76\uff08\u5982LoRA\uff09\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u7a00\u758f\u9002\u914d\u5668\u4f5c\u4e3a\u65b0\u578b\u6784\u5efa\u5757\u7684\u6f5c\u529b\uff0c\u5e76\u5c06\u5176\u5408\u5e76\u7814\u7a76\u6269\u5c55\u5230\u66f4\u5e7f\u8303\u56f4\uff08\u591a\u8fbe20\u4e2aNLP\u4efb\u52a1\uff09\u3002", "method": "\u9996\u5148\uff0c\u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u9ad8\u6548\u7a00\u758f\u9002\u914d\u5668\u7684\u7b80\u5355\u65b9\u6cd5\uff1b\u5176\u6b21\uff0c\u901a\u8fc7\u5408\u5e76\u591a\u8fbe20\u4e2a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684\u9002\u914d\u5668\uff0c\u7814\u7a76\u8fd9\u4e9b\u7a00\u758f\u9002\u914d\u5668\u7684\u5408\u5e76\u7279\u6027\uff0c\u5e76\u4e0eLoRA\u53ca\u5168\u91cf\u5fae\u8c03/\u6a21\u578b\u5408\u5e76\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u63d0\u51fa\u7684\u7a00\u758f\u9002\u914d\u5668\u5728\u8bad\u7ec3\u8bbe\u7f6e\u4e2d\u4f18\u4e8eLoRA\u548c\u5168\u91cf\u5fae\u8c03\u3002\u5408\u5e76\u540e\uff0c\u7a00\u758f\u9002\u914d\u5668\u5728\u57df\u5185\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8eLoRA\u6216\u5168\u6a21\u578b\u5408\u5e76\u3002\u7136\u800c\uff0c\u6240\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u5f3a\u5927\u7684\u57df\u5916\u6027\u80fd\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "conclusion": "\u7a00\u758f\u9002\u914d\u5668\u662f\u6784\u5efa\u6a21\u5757\u5316\u67b6\u6784\u7684\u6709\u6548\u6784\u5efa\u5757\uff0c\u5c24\u5176\u662f\u5728\u5408\u5e76\u540e\u5b9e\u73b0\u4f18\u5f02\u7684\u57df\u5185\u6027\u80fd\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u63d0\u9ad8\u6240\u6709\u65b9\u6cd5\u7684\u57df\u5916\u6cdb\u5316\u80fd\u529b\u4ecd\u662f\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2507.07151", "pdf": "https://arxiv.org/pdf/2507.07151", "abs": "https://arxiv.org/abs/2507.07151", "authors": ["Zongmeng Zhang", "Wengang Zhou", "Jie Zhao", "Houqiang Li"], "title": "Robust Multimodal Large Language Models Against Modality Conflict", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "Despite the impressive capabilities of multimodal large language models\n(MLLMs) in vision-language tasks, they are prone to hallucinations in\nreal-world scenarios. This paper investigates the hallucination phenomenon in\nMLLMs from the perspective of modality conflict. Unlike existing works focusing\non the conflicts between model responses and inputs, we study the inherent\nconflicts in inputs from different modalities that place MLLMs in a dilemma and\ndirectly lead to hallucinations. We formally define the modality conflict and\nconstruct a dataset named Multimodal Modality Conflict (MMMC) to simulate this\nphenomenon in vision-language tasks. Three methods based on prompt engineering,\nsupervised fine-tuning, and reinforcement learning are proposed to alleviate\nthe hallucination caused by modality conflict. Extensive experiments are\nconducted on the MMMC dataset to analyze the merits and demerits of these\nmethods. Our results show that the reinforcement learning method achieves the\nbest performance in mitigating the hallucination under modality conflict, while\nthe supervised fine-tuning method shows promising and stable performance. Our\nwork sheds light on the unnoticed modality conflict that leads to\nhallucinations and provides more insights into the robustness of MLLMs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u8f93\u5165\u6a21\u6001\u51b2\u7a81\u4e0b\u4ea7\u751f\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u63d0\u51fa\u6a21\u6001\u51b2\u7a81\u5b9a\u4e49\u5e76\u6784\u5efaMMMC\u6570\u636e\u96c6\u3002\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u4e09\u79cd\u65b9\u6cd5\u7f13\u89e3\u5e7b\u89c9\uff0c\u7ed3\u679c\u663e\u793a\u5f3a\u5316\u5b66\u4e60\u6548\u679c\u6700\u4f73\uff0c\u76d1\u7763\u5fae\u8c03\u8868\u73b0\u7a33\u5b9a\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6613\u4ea7\u751f\u5e7b\u89c9\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u54cd\u5e94\u4e0e\u8f93\u5165\u95f4\u7684\u51b2\u7a81\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u8f93\u5165\u4e2d\u4e0d\u540c\u6a21\u6001\u95f4\u56fa\u6709\u7684\u51b2\u7a81\uff0c\u8fd9\u79cd\u51b2\u7a81\u4f7fMLLMs\u9677\u5165\u56f0\u5883\u5e76\u76f4\u63a5\u5bfc\u81f4\u5e7b\u89c9\u3002", "method": "\u672c\u6587\u6b63\u5f0f\u5b9a\u4e49\u4e86\u6a21\u6001\u51b2\u7a81\uff0c\u5e76\u6784\u5efa\u4e86\u540d\u4e3a\u591a\u6a21\u6001\u6a21\u6001\u51b2\u7a81\uff08MMMC\uff09\u7684\u6570\u636e\u96c6\u4ee5\u6a21\u62df\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u8be5\u73b0\u8c61\u3002\u4e3a\u7f13\u89e3\u6a21\u6001\u51b2\u7a81\u5f15\u8d77\u7684\u5e7b\u89c9\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u63d0\u793a\u5de5\u7a0b\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u4e09\u79cd\u65b9\u6cd5\u3002\u5728MMMC\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\u4ee5\u5206\u6790\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u7f13\u89e3\u6a21\u6001\u51b2\u7a81\u5f15\u8d77\u7684\u5e7b\u89c9\u65b9\u9762\uff0c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u800c\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5219\u663e\u793a\u51fa\u6709\u524d\u666f\u4e14\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u5de5\u4f5c\u63ed\u793a\u4e86\u5bfc\u81f4\u5e7b\u89c9\u7684\u672a\u88ab\u6ce8\u610f\u5230\u7684\u6a21\u6001\u51b2\u7a81\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.07421", "pdf": "https://arxiv.org/pdf/2507.07421", "abs": "https://arxiv.org/abs/2507.07421", "authors": ["Zonghai Yao", "Youxia Zhao", "Avijit Mitra", "David A. Levy", "Emily Druhl", "Jack Tsai", "Hong Yu"], "title": "SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data", "categories": ["cs.CL", "cs.AI"], "comment": "Equal contribution for the first two authors", "summary": "Eviction is a significant yet understudied social determinants of health\n(SDoH), linked to housing instability, unemployment, and mental health. While\neviction appears in unstructured electronic health records (EHRs), it is rarely\ncoded in structured fields, limiting downstream applications. We introduce\nSynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop\nannotation, and automated prompt optimization (APO) to extract eviction\nstatuses from clinical notes. Using this pipeline, we created the largest\npublic eviction-related SDoH dataset to date, comprising 14 fine-grained\ncategories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on\nSynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other\nSDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),\nGPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling\ncost-effective deployment across various model sizes. The pipeline reduces\nannotation effort by over 80%, accelerates dataset creation, enables scalable\neviction detection, and generalizes to other information extraction tasks.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aSynthEHR-Eviction\u7684\u7ba1\u9053\uff0c\u7ed3\u5408LLMs\u3001\u4eba\u5de5\u6807\u6ce8\u548cAPO\uff0c\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u9ad8\u6548\u63d0\u53d6\u9a71\u9010\u72b6\u6001\uff0c\u5e76\u521b\u5efa\u4e86\u6700\u5927\u7684\u9a71\u9010\u76f8\u5173SDoH\u6570\u636e\u96c6\u3002\u8be5\u7ba1\u9053\u8bad\u7ec3\u7684LLMs\u5728\u9a71\u9010\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u80fd\u663e\u8457\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\u3002", "motivation": "\u9a71\u9010\u662f\u5f71\u54cd\u5065\u5eb7\u7684\u91cd\u8981\u793e\u4f1a\u51b3\u5b9a\u56e0\u7d20\uff08SDoH\uff09\uff0c\u4f46\u5176\u5728\u975e\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHRs\uff09\u4e2d\u5f88\u5c11\u88ab\u7f16\u7801\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5065\u5eb7\u5e94\u7528\u4e2d\u7684\u5206\u6790\u548c\u5229\u7528\u3002", "method": "\u5f15\u5165\u4e86SynthEHR-Eviction\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001\u4eba\u5de5\u53c2\u4e0e\u6807\u6ce8\u548c\u81ea\u52a8\u5316\u63d0\u793a\u4f18\u5316\uff08APO\uff09\u6280\u672f\uff0c\u7528\u4e8e\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u63d0\u53d6\u9a71\u9010\u72b6\u6001\u3002\u5229\u7528\u6b64\u7ba1\u9053\uff0c\u7814\u7a76\u4eba\u5458\u521b\u5efa\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u516c\u5171\u9a71\u9010\u76f8\u5173SDoH\u6570\u636e\u96c6\uff0c\u5305\u542b14\u4e2a\u7ec6\u7c92\u5ea6\u7c7b\u522b\u3002", "result": "\u901a\u8fc7SynthEHR-Eviction\u8bad\u7ec3\u7684\u5fae\u8c03LLMs\uff08\u5982Qwen2.5\u3001LLaMA3\uff09\u5728\u4eba\u5de5\u9a8c\u8bc1\u6570\u636e\u4e0a\uff0c\u9a71\u9010\u68c0\u6d4b\u7684Macro-F1\u5f97\u5206\u8fbe\u523088.8%\uff0c\u5176\u4ed6SDoH\u7684Macro-F1\u5f97\u5206\u8fbe\u523090.3%\u3002\u8fd9\u4e9b\u6a21\u578b\u8868\u73b0\u4f18\u4e8eGPT-4o-APO\u3001GPT-4o-mini-APO\u548cBioBERT\u3002\u8be5\u7ba1\u9053\u8fd8\u5c06\u6807\u6ce8\u5de5\u4f5c\u91cf\u51cf\u5c11\u4e8680%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5SynthEHR-Eviction\u7ba1\u9053\u80fd\u52a0\u901f\u6570\u636e\u96c6\u521b\u5efa\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u9a71\u9010\u68c0\u6d4b\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u5176\u4ed6\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\uff0c\u4e3a\u5229\u7528\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u6570\u636e\u8fdb\u884cSDoH\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2507.07677", "pdf": "https://arxiv.org/pdf/2507.07677", "abs": "https://arxiv.org/abs/2507.07677", "authors": ["Miguel Casasnovas", "Marc Carrascosa-Zamacois", "Boris Bellalta"], "title": "Can cloud-based VR streaming handle Wi-Fi OBSS contention?", "categories": ["cs.NI"], "comment": "preprint", "summary": "This paper experimentally analyzes the negative impact of contention caused\nby neighboring Wi-Fi networks operating on overlapping channels on Virtual\nReality (VR) streaming over Wi-Fi, focusing on scenarios of partial and full\nchannel overlap within an 80 MHz channel. Our results show that (i) increasing\nthe number of 80 MHz Overlapping Basic Service Sets (OBSSs) intensifies\ncontention and degrades VR streaming performance; (ii) OBSS activity on the\nsecondary-sided 40 MHz portion degrades performance more than activity on the\nprimary-sided 40 MHz portion; (iii) for the same aggregate load, full channel\noverlap with two 40 MHz OBSS contenders is less detrimental than partial\noverlap with a single high-load 40 MHz contender, but more disruptive than full\noverlap with two 80 MHz contenders; and (iv) full channel overlap with two 40\nMHz OBSS contenders has a smaller impact on VR streaming under symmetric\ntraffic loads than under asymmetric loads. Moreover, our results demonstrate\nthat our previously proposed Network-aware Step-wise adaptive bitrate algorithm\nfor VR streaming (NeSt-VR) effectively mitigates performance degradation in\nOBSS environments, enabling VR streaming under heavier OBSS traffic conditions.", "AI": {"tldr": "\u672c\u6587\u5b9e\u9a8c\u5206\u6790\u4e86\u90bb\u8fd1Wi-Fi\u7f51\u7edc\u4fe1\u9053\u91cd\u53e0\u5bf9Wi-Fi\u4e0aVR\u6d41\u5a92\u4f53\u6027\u80fd\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6240\u63d0\u51faNeSt-VR\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u90bb\u8fd1Wi-Fi\u7f51\u7edc\u4fe1\u9053\u91cd\u53e0\uff08OBSS\uff09\u5f15\u8d77\u7684\u7ade\u4e89\u5bf9VR\u6d41\u5a92\u4f53\u6027\u80fd\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u572880 MHz\u4fe1\u9053\u7684\u90e8\u5206\u548c\u5b8c\u5168\u91cd\u53e0\u573a\u666f\u4e0b\uff0c\u56e0\u4e3aVR\u6d41\u5a92\u4f53\u5bf9\u7f51\u7edc\u5ef6\u8fdf\u548c\u5e26\u5bbd\u9ad8\u5ea6\u654f\u611f\u3002", "method": "\u91c7\u7528\u5b9e\u9a8c\u5206\u6790\u65b9\u6cd5\uff0c\u5728Wi-Fi\u7f51\u7edc\u73af\u5883\u4e0b\uff0c\u6d4b\u8bd5\u4e0d\u540c\u6570\u91cf\u3001\u4e0d\u540c\u4fe1\u9053\u4f4d\u7f6e\uff08\u4e3b/\u6b21\u7ea740 MHz\uff09\u548c\u4e0d\u540c\u8d1f\u8f7d\u7c7b\u578b\uff08\u5bf9\u79f0/\u4e0d\u5bf9\u79f0\uff09\u7684OBSS\u5bf9VR\u6d41\u5a92\u4f53\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": ["\u589e\u52a080 MHz OBSSs\u6570\u91cf\u4f1a\u52a0\u5267\u7ade\u4e89\uff0c\u5bfc\u81f4VR\u6d41\u5a92\u4f53\u6027\u80fd\u4e0b\u964d\u3002", "OBSS\u6d3b\u52a8\u5728\u6b21\u7ea740 MHz\u4fe1\u9053\u90e8\u5206\u6bd4\u5728\u4e3b\u7ea740 MHz\u90e8\u5206\u5bf9VR\u6d41\u5a92\u4f53\u6027\u80fd\u7684\u635f\u5bb3\u66f4\u5927\u3002", "\u5728\u76f8\u540c\u603b\u8d1f\u8f7d\u4e0b\uff0c\u4e24\u4e2a40 MHz OBSS\u7ade\u4e89\u8005\u7684\u5b8c\u5168\u4fe1\u9053\u91cd\u53e0\uff0c\u76f8\u6bd4\u5355\u4e2a\u9ad8\u8d1f\u8f7d40 MHz\u7ade\u4e89\u8005\u7684\u90e8\u5206\u91cd\u53e0\u5f71\u54cd\u8f83\u5c0f\uff0c\u4f46\u6bd4\u4e24\u4e2a80 MHz\u7ade\u4e89\u8005\u7684\u5b8c\u5168\u91cd\u53e0\u66f4\u5177\u7834\u574f\u6027\u3002", "\u5728\u5bf9\u79f0\u6d41\u91cf\u8d1f\u8f7d\u4e0b\uff0c\u4e24\u4e2a40 MHz OBSS\u7ade\u4e89\u8005\u7684\u5b8c\u5168\u4fe1\u9053\u91cd\u53e0\u5bf9VR\u6d41\u5a92\u4f53\u7684\u5f71\u54cd\u5c0f\u4e8e\u975e\u5bf9\u79f0\u8d1f\u8f7d\u3002", "\u7814\u7a76\u56e2\u961f\u5148\u524d\u63d0\u51fa\u7684\u7f51\u7edc\u611f\u77e5\u9636\u68af\u5f0f\u81ea\u9002\u5e94\u6bd4\u7279\u7387VR\u6d41\u5a92\u4f53\u7b97\u6cd5\uff08NeSt-VR\uff09\u80fd\u6709\u6548\u7f13\u89e3OBSS\u73af\u5883\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u652f\u6301\u5728\u66f4\u91cd\u7684OBSS\u6d41\u91cf\u6761\u4ef6\u4e0b\u8fdb\u884cVR\u6d41\u5a92\u4f53\u4f20\u8f93\u3002"], "conclusion": "\u90bb\u8fd1Wi-Fi\u7f51\u7edc\u7684\u4fe1\u9053\u91cd\u53e0\u4f1a\u4e25\u91cd\u5f71\u54cdVR\u6d41\u5a92\u4f53\u6027\u80fd\uff0c\u4f46\u901a\u8fc7\u5f15\u5165NeSt-VR\u7b49\u7f51\u7edc\u611f\u77e5\u81ea\u9002\u5e94\u6bd4\u7279\u7387\u7b97\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u8fd9\u4e9b\u6027\u80fd\u4e0b\u964d\uff0c\u4ece\u800c\u5728\u590d\u6742\u7684OBSS\u73af\u5883\u4e0b\u5b9e\u73b0\u53ef\u884c\u7684VR\u6d41\u5a92\u4f53\u4f53\u9a8c\u3002"}}
{"id": "2507.07302", "pdf": "https://arxiv.org/pdf/2507.07302", "abs": "https://arxiv.org/abs/2507.07302", "authors": ["Ashish Kumar"], "title": "Application of LLMs to Multi-Robot Path Planning and Task Allocation", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Efficient exploration is a well known problem in deep reinforcement learning\nand this problem is exacerbated in multi-agent reinforcement learning due the\nintrinsic complexities of such algorithms. There are several approaches to\nefficiently explore an environment to learn to solve tasks by multi-agent\noperating in that environment, of which, the idea of expert exploration is\ninvestigated in this work. More specifically, this work investigates the\napplication of large-language models as expert planners for efficient\nexploration in planning based tasks for multiple agents.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4e13\u5bb6\u89c4\u5212\u5668\uff0c\u4ee5\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9ad8\u6548\u63a2\u7d22\u95ee\u9898\u3002", "motivation": "\u9ad8\u6548\u63a2\u7d22\u662f\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u56e0\u5176\u590d\u6742\u6027\u800c\u53d8\u5f97\u66f4\u52a0\u4e25\u5cfb\u3002", "method": "\u672c\u5de5\u4f5c\u7814\u7a76\u4e86\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e94\u7528\u4e8e\u4f5c\u4e3a\u4e13\u5bb6\u89c4\u5212\u5668\uff0c\u4ee5\u5728\u591a\u667a\u80fd\u4f53\u57fa\u4e8e\u89c4\u5212\u7684\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u3002", "result": "\u6458\u8981\u4e2d\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u7814\u7a76\u7ed3\u679c\uff0c\u6b64\u5de5\u4f5c\u65e8\u5728\u8c03\u67e5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4e13\u5bb6\u89c4\u5212\u5668\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u7ed3\u8bba\u5c1a\u672a\u660e\u786e\uff0c\u6b64\u5de5\u4f5c\u4e3b\u8981\u805a\u7126\u4e8e\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u9ad8\u6548\u63a2\u7d22\u4e2d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.07141", "pdf": "https://arxiv.org/pdf/2507.07141", "abs": "https://arxiv.org/abs/2507.07141", "authors": ["Dongxiao He", "Yongqi Huang", "Jitao Zhao", "Xiaobao Wang", "Zhen Wang"], "title": "Str-GCL: Structural Commonsense Driven Graph Contrastive Learning", "categories": ["cs.LG"], "comment": "Accepted by WWW 2025", "summary": "Graph Contrastive Learning (GCL) is a widely adopted approach in\nself-supervised graph representation learning, applying contrastive objectives\nto produce effective representations. However, current GCL methods primarily\nfocus on capturing implicit semantic relationships, often overlooking the\nstructural commonsense embedded within the graph's structure and attributes,\nwhich contains underlying knowledge crucial for effective representation\nlearning. Due to the lack of explicit information and clear guidance in general\ngraph, identifying and integrating such structural commonsense in GCL poses a\nsignificant challenge. To address this gap, we propose a novel framework called\nStructural Commonsense Unveiling in Graph Contrastive Learning (Str-GCL).\nStr-GCL leverages first-order logic rules to represent structural commonsense\nand explicitly integrates them into the GCL framework. It introduces\ntopological and attribute-based rules without altering the original graph and\nemploys a representation alignment mechanism to guide the encoder in\neffectively capturing this commonsense. To the best of our knowledge, this is\nthe first attempt to directly incorporate structural commonsense into GCL.\nExtensive experiments demonstrate that Str-GCL outperforms existing GCL\nmethods, providing a new perspective on leveraging structural commonsense in\ngraph representation learning.", "AI": {"tldr": "Str-GCL\u901a\u8fc7\u4e00\u9636\u903b\u8f91\u89c4\u5219\u5c06\u56fe\u7ed3\u6784\u5e38\u8bc6\u878d\u5165\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ee5\u63d0\u9ad8\u56fe\u8868\u793a\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u56fe\u5bf9\u6bd4\u5b66\u4e60(GCL)\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9690\u5f0f\u8bed\u4e49\u5173\u7cfb\uff0c\u4f46\u5ffd\u89c6\u4e86\u56fe\u4e2d\u7ed3\u6784\u548c\u5c5e\u6027\u4e2d\u8574\u542b\u7684\u7ed3\u6784\u5e38\u8bc6\uff0c\u800c\u8fd9\u4e9b\u5e38\u8bc6\u5bf9\u6709\u6548\u7684\u8868\u793a\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u4e14\u96be\u4ee5\u660e\u786e\u8bc6\u522b\u548c\u6574\u5408\u3002", "method": "\u63d0\u51faStr-GCL\u6846\u67b6\uff0c\u5229\u7528\u4e00\u9636\u903b\u8f91\u89c4\u5219\u6765\u8868\u793a\u7ed3\u6784\u5e38\u8bc6\uff08\u5305\u62ec\u62d3\u6251\u548c\u57fa\u4e8e\u5c5e\u6027\u7684\u89c4\u5219\uff09\uff0c\u5e76\u5c06\u5176\u663e\u5f0f\u5730\u6574\u5408\u5230GCL\u4e2d\uff0c\u901a\u8fc7\u8868\u793a\u5bf9\u9f50\u673a\u5236\u5f15\u5bfc\u7f16\u7801\u5668\u6355\u83b7\u8fd9\u4e9b\u5e38\u8bc6\uff0c\u4e14\u4e0d\u6539\u53d8\u539f\u59cb\u56fe\u7ed3\u6784\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cStr-GCL\u4f18\u4e8e\u73b0\u6709GCL\u65b9\u6cd5\u3002", "conclusion": "Str-GCL\u4e3a\u5728\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u5229\u7528\u7ed3\u6784\u5e38\u8bc6\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2507.07153", "pdf": "https://arxiv.org/pdf/2507.07153", "abs": "https://arxiv.org/abs/2507.07153", "authors": ["Antonella Barisic Kulas", "Frano Petric", "Stjepan Bogdan"], "title": "Aerial Maritime Vessel Detection and Identification", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Preprint. ICUAS 2025", "summary": "Autonomous maritime surveillance and target vessel identification in\nenvironments where Global Navigation Satellite Systems (GNSS) are not available\nis critical for a number of applications such as search and rescue and threat\ndetection. When the target vessel is only described by visual cues and its last\nknown position is not available, unmanned aerial vehicles (UAVs) must rely\nsolely on on-board vision to scan a large search area under strict\ncomputational constraints. To address this challenge, we leverage the YOLOv8\nobject detection model to detect all vessels in the field of view. We then\napply feature matching and hue histogram distance analysis to determine whether\nany detected vessel corresponds to the target. When found, we localize the\ntarget using simple geometric principles. We demonstrate the proposed method in\nreal-world experiments during the MBZIRC2023 competition, integrated into a\nfully autonomous system with GNSS-denied navigation. We also evaluate the\nimpact of perspective on detection accuracy and localization precision and\ncompare it with the oracle approach.", "AI": {"tldr": "\u5728GNSS\u53d7\u9650\u73af\u5883\u4e0b\uff0cUAV\u5229\u7528\u57fa\u4e8eYOLOv8\u3001\u7279\u5f81\u5339\u914d\u548c\u8272\u8c03\u76f4\u65b9\u56fe\u5206\u6790\u7684\u89c6\u89c9\u65b9\u6cd5\u5b9e\u73b0\u81ea\u4e3b\u6d77\u4e0a\u76ee\u6807\u8239\u53ea\u8bc6\u522b\u4e0e\u5b9a\u4f4d\u3002", "motivation": "\u5728GNSS\u4e0d\u53ef\u7528\u73af\u5883\u4e0b\uff0c\u81ea\u4e3b\u6d77\u4e0a\u76d1\u89c6\u548c\u76ee\u6807\u8239\u53ea\u8bc6\u522b\u5bf9\u641c\u6551\u548c\u5a01\u80c1\u68c0\u6d4b\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u5f53\u76ee\u6807\u4ec5\u6709\u89c6\u89c9\u7ebf\u7d22\u4e14\u4f4d\u7f6e\u672a\u77e5\u65f6\uff0c\u65e0\u4eba\u673a\u9700\u4ec5\u51ed\u673a\u8f7d\u89c6\u89c9\u5728\u5927\u8303\u56f4\u3001\u8ba1\u7b97\u53d7\u9650\u533a\u57df\u5185\u8fdb\u884c\u626b\u63cf\u3002", "method": "\u9996\u5148\u5229\u7528YOLOv8\u6a21\u578b\u68c0\u6d4b\u89c6\u91ce\u5185\u6240\u6709\u8239\u53ea\uff1b\u5176\u6b21\uff0c\u901a\u8fc7\u7279\u5f81\u5339\u914d\u548c\u8272\u8c03\u76f4\u65b9\u56fe\u8ddd\u79bb\u5206\u6790\u786e\u5b9a\u76ee\u6807\u8239\u53ea\uff1b\u6700\u540e\uff0c\u5229\u7528\u7b80\u5355\u7684\u51e0\u4f55\u539f\u7406\u5bf9\u76ee\u6807\u8fdb\u884c\u5b9a\u4f4d\u3002", "result": "\u8be5\u65b9\u6cd5\u5728MBZIRC2023\u6bd4\u8d5b\u7684\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u5e76\u96c6\u6210\u5230\u652f\u6301GNSS\u53d7\u9650\u5bfc\u822a\u7684\u5168\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u3002\u6b64\u5916\uff0c\u8fd8\u8bc4\u4f30\u4e86\u89c6\u89d2\u5bf9\u68c0\u6d4b\u7cbe\u5ea6\u548c\u5b9a\u4f4d\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u7406\u60f3\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728GNSS\u53d7\u9650\u73af\u5883\u4e0b\uff0c\u901a\u8fc7\u7eaf\u89c6\u89c9\u5b9e\u73b0\u81ea\u4e3b\u6d77\u4e0a\u76ee\u6807\u8bc6\u522b\u4e0e\u5b9a\u4f4d\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u6bd4\u8d5b\u4e2d\u6210\u529f\u9a8c\u8bc1\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u5173\u952e\u5e94\u7528\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2507.07439", "pdf": "https://arxiv.org/pdf/2507.07439", "abs": "https://arxiv.org/abs/2507.07439", "authors": ["Matthieu Boileau", "Philippe Helluy", "Jeremy Pawlus", "Svitlana Vyetrenko"], "title": "Towards Interpretable Time Series Foundation Models", "categories": ["cs.CL", "cs.AI"], "comment": "International Conference on Machine Leaning (ICML) 2025 Workshop on\n  Foundation Models for Structured Data", "summary": "In this paper, we investigate the distillation of time series reasoning\ncapabilities into small, instruction-tuned language models as a step toward\nbuilding interpretable time series foundation models. Leveraging a synthetic\ndataset of mean-reverting time series with systematically varied trends and\nnoise levels, we generate natural language annotations using a large multimodal\nmodel and use these to supervise the fine-tuning of compact Qwen models. We\nintroduce evaluation metrics that assess the quality of the distilled reasoning\n- focusing on trend direction, noise intensity, and extremum localization - and\nshow that the post-trained models acquire meaningful interpretive capabilities.\nOur results highlight the feasibility of compressing time series understanding\ninto lightweight, language-capable models suitable for on-device or\nprivacy-sensitive deployment. This work contributes a concrete foundation\ntoward developing small, interpretable models that explain temporal patterns in\nnatural language.", "AI": {"tldr": "\u5c06\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4ee5\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u65e8\u5728\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff0c\u5177\u4f53\u662f\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u80fd\u529b\u8d4b\u4e88\u5c0f\u578b\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u5229\u7528\u5408\u6210\u7684\u5747\u503c\u56de\u5f52\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\uff0c\u5e76\u7528\u5176\u76d1\u7763\u5fae\u8c03\u5c0f\u578bQwen\u6a21\u578b\uff1b\u5f15\u5165\u8bc4\u4f30\u6307\u6807\uff08\u8d8b\u52bf\u65b9\u5411\u3001\u566a\u58f0\u5f3a\u5ea6\u3001\u6781\u503c\u5b9a\u4f4d\uff09\u8bc4\u4f30\u84b8\u998f\u63a8\u7406\u7684\u8d28\u91cf\u3002", "result": "\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u83b7\u5f97\u4e86\u6709\u610f\u4e49\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5c06\u65f6\u95f4\u5e8f\u5217\u7406\u89e3\u538b\u7f29\u5230\u8f7b\u91cf\u7ea7\u3001\u5177\u5907\u8bed\u8a00\u80fd\u529b\u7684\u6a21\u578b\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u5f00\u53d1\u80fd\u7528\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u65f6\u95f4\u6a21\u5f0f\u7684\u5c0f\u578b\u53ef\u89e3\u91ca\u6a21\u578b\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2507.07841", "pdf": "https://arxiv.org/pdf/2507.07841", "abs": "https://arxiv.org/abs/2507.07841", "authors": ["Ana Rita Ortigoso", "Gabriel Vieira", "Daniel Fuentes", "Lu\u00eds Fraz\u00e3o", "Nuno Costa", "Ant\u00f3nio Pereira"], "title": "HaLert: A Resilient Smart City Architecture for Post-Disaster Based on Wi-Fi HaLow Mesh and SDN", "categories": ["cs.NI", "cs.CY", "cs.SY", "eess.SY", "68M10, 68M12, 68W15", "C.2.1; C.2.2; C.2.3; C.2.6; H.5.5; K.4.1"], "comment": null, "summary": "Events such as catastrophes and disasters are, in most cases, unpredictable.\nConsequently, reusing existing infrastructures to develop alternative\ncommunication strategies after disasters is essential to minimise the impact of\nthese events on the population's ability to communicate and promptly receive\nalerts from authorities. In this context, the emergence of smart cities,\ncharacterised by dense and geographically distributed IoT networks, presents\nsignificant potential for such reuse. This work proposes HaLert, a resilient\narchitecture for smart cities based on a Wi-Fi HaLow IEEE 802.11s mesh network,\nwhose resources can be readily reallocated to support a emergency communication\nsystem to exchange messages (including text, location, image, audio, and video)\nbetween citizens, authorities, and between both parties. To facilitate remote\nmonitoring and configuration of the network, the architecture incorporates the\nSDN (Software-Defined Networking) paradigm, supported by a LoRa controlled\nflooding mesh network. A prototype was developed based on this architecture and\ntested in a real urban scenario comprising both indoor and outdoor\nenvironments. The results demonstrated that, despite the significant impact of\nobstacles, lack of line-of-sight, and terrain slopes on the latency (average\nlatency between 15 and 54.8 ms) and throughput (upload bitrates between 134 and\n726 Kbps and download bitrates between 117 and 682 Kbps) of the Wi-Fi HaLow\nnetwork, it remained stable and resilient, successfully providing all\nfunctionalities associated with the HaLert architecture. The tests conducted on\nthe LoRa network revealed a high average message success rate of 94.96%.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faHaLert\uff0c\u4e00\u79cd\u57fa\u4e8eWi-Fi HaLow\u548cLoRa\u7684\u5f39\u6027\u67b6\u6784\uff0c\u65e8\u5728\u5229\u7528\u667a\u6167\u57ce\u5e02\u73b0\u6709\u7269\u8054\u7f51\u57fa\u7840\u8bbe\u65bd\uff0c\u4e3a\u707e\u540e\u63d0\u4f9b\u53ef\u9760\u7684\u5e94\u6025\u901a\u4fe1\u7cfb\u7edf\uff0c\u5e76\u5728\u771f\u5b9e\u57ce\u5e02\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u7a33\u5b9a\u6027\u548c\u529f\u80fd\u6027\u3002", "motivation": "\u707e\u5bb3\u901a\u5e38\u4e0d\u53ef\u9884\u6d4b\uff0c\u4f1a\u4e25\u91cd\u5f71\u54cd\u6c11\u4f17\u901a\u4fe1\u548c\u63a5\u6536\u653f\u5e9c\u8b66\u62a5\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u5229\u7528\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u5f00\u53d1\u66ff\u4ee3\u901a\u4fe1\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002\u667a\u6167\u57ce\u5e02\u4e2d\u5bc6\u96c6\u7684\u7269\u8054\u7f51\u7f51\u7edc\u4e3a\u6b64\u7c7b\u91cd\u7528\u63d0\u4f9b\u4e86\u5de8\u5927\u6f5c\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86HaLert\u67b6\u6784\uff0c\u5b83\u57fa\u4e8eWi-Fi HaLow IEEE 802.11s\u7f51\u72b6\u7f51\u7edc\uff0c\u53ef\u5feb\u901f\u91cd\u65b0\u5206\u914d\u8d44\u6e90\u4ee5\u652f\u6301\u516c\u6c11\u4e0e\u5f53\u5c40\u4e4b\u95f4\u7684\u5e94\u6025\u901a\u4fe1\uff08\u5305\u62ec\u6587\u672c\u3001\u4f4d\u7f6e\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\uff09\u3002\u4e3a\u4fbf\u4e8e\u8fdc\u7a0b\u76d1\u63a7\u548c\u914d\u7f6e\uff0c\u8be5\u67b6\u6784\u7ed3\u5408\u4e86SDN\uff08\u8f6f\u4ef6\u5b9a\u4e49\u7f51\u7edc\uff09\u8303\u5f0f\uff0c\u5e76\u901a\u8fc7LoRa\u63a7\u5236\u6cdb\u6d2a\u7f51\u72b6\u7f51\u7edc\u63d0\u4f9b\u652f\u6301\u3002\u7814\u7a76\u56e2\u961f\u57fa\u4e8e\u6b64\u67b6\u6784\u5f00\u53d1\u4e86\u539f\u578b\uff0c\u5e76\u5728\u771f\u5b9e\u57ce\u5e02\uff08\u5305\u62ec\u5ba4\u5185\u548c\u5ba4\u5916\u73af\u5883\uff09\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5c3d\u7ba1\u969c\u788d\u7269\u3001\u975e\u89c6\u8ddd\u548c\u5730\u5f62\u5761\u5ea6\u5bf9Wi-Fi HaLow\u7f51\u7edc\u7684\u5ef6\u8fdf\uff08\u5e73\u574715\u81f354.8\u6beb\u79d2\uff09\u548c\u541e\u5410\u91cf\uff08\u4e0a\u4f20134\u81f3726 Kbps\uff0c\u4e0b\u8f7d117\u81f3682 Kbps\uff09\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u7f51\u7edc\u4fdd\u6301\u7a33\u5b9a\u548c\u5f39\u6027\uff0c\u6210\u529f\u63d0\u4f9b\u4e86HaLert\u67b6\u6784\u7684\u6240\u6709\u529f\u80fd\u3002LoRa\u7f51\u7edc\u6d4b\u8bd5\u663e\u793a\u5e73\u5747\u6d88\u606f\u6210\u529f\u7387\u9ad8\u8fbe94.96%\u3002", "conclusion": "HaLert\u67b6\u6784\u88ab\u8bc1\u660e\u662f\u4e00\u79cd\u7a33\u5b9a\u4e14\u5f39\u6027\u7684\u5e94\u6025\u901a\u4fe1\u7cfb\u7edf\uff0c\u80fd\u591f\u5229\u7528Wi-Fi HaLow\u548cLoRa\u6280\u672f\u5728\u590d\u6742\u771f\u5b9e\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u5b9e\u73b0\u5173\u952e\u901a\u4fe1\u529f\u80fd\uff0c\u6709\u6548\u5e94\u5bf9\u707e\u5bb3\u5bf9\u901a\u4fe1\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.07306", "pdf": "https://arxiv.org/pdf/2507.07306", "abs": "https://arxiv.org/abs/2507.07306", "authors": ["Yichen Lu", "Wei Dai", "Jiaen Liu", "Ching Wing Kwok", "Zongheng Wu", "Xudong Xiao", "Ao Sun", "Sheng Fu", "Jianyuan Zhan", "Yian Wang", "Takatomo Saito", "Sicheng Lai"], "title": "ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning", "categories": ["cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "LLM-based translation agents have achieved highly human-like translation\nresults and are capable of handling longer and more complex contexts with\ngreater efficiency. However, they are typically limited to text-only inputs. In\nthis paper, we introduce ViDove, a translation agent system designed for\nmultimodal input. Inspired by the workflow of human translators, ViDove\nleverages visual and contextual background information to enhance the\ntranslation process. Additionally, we integrate a multimodal memory system and\nlong-short term memory modules enriched with domain-specific knowledge,\nenabling the agent to perform more accurately and adaptively in real-world\nscenarios. As a result, ViDove achieves significantly higher translation\nquality in both subtitle generation and general translation tasks, with a 28%\nimprovement in BLEU scores and a 15% improvement in SubER compared to previous\nstate-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark\nfor long-form automatic video subtitling and translation, featuring 17 hours of\nhigh-quality, human-annotated data. Our code is available here:\nhttps://github.com/pigeonai-org/ViDove", "AI": {"tldr": "ViDove\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7ffb\u8bd1\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u3001\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u8bb0\u5fc6\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b57\u5e55\u751f\u6210\u548c\u901a\u7528\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5e76\u63a8\u51fa\u4e86\u65b0\u7684\u957f\u683c\u5f0f\u89c6\u9891\u7ffb\u8bd1\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u7ffb\u8bd1\u667a\u80fd\u4f53\u867d\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u4ec5\u9650\u4e8e\u6587\u672c\u8f93\u5165\uff0c\u65e0\u6cd5\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u7684\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u4ee5\u514b\u670d\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "ViDove\u7cfb\u7edf\u53d7\u4eba\u7c7b\u7ffb\u8bd1\u5de5\u4f5c\u6d41\u7a0b\u542f\u53d1\uff0c\u5229\u7528\u89c6\u89c9\u548c\u4e0a\u4e0b\u6587\u80cc\u666f\u4fe1\u606f\u589e\u5f3a\u7ffb\u8bd1\u8fc7\u7a0b\u3002\u5176\u6838\u5fc3\u65b9\u6cd5\u5305\u62ec\u6574\u5408\u591a\u6a21\u6001\u8bb0\u5fc6\u7cfb\u7edf\u4ee5\u53ca\u5bcc\u542b\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u7684\u957f\u77ed\u671f\u8bb0\u5fc6\u6a21\u5757\u3002", "result": "ViDove\u5728\u5b57\u5e55\u751f\u6210\u548c\u901a\u7528\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u7ffb\u8bd1\u8d28\u91cf\u63d0\u5347\uff0cBLEU\u5206\u6570\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u63d0\u9ad8\u4e8628%\uff0cSubER\u63d0\u9ad8\u4e8615%\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u5f15\u5165\u4e86DoveBench\uff0c\u4e00\u4e2a\u5305\u542b17\u5c0f\u65f6\u9ad8\u8d28\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6\u3002", "conclusion": "ViDove\u6210\u529f\u5730\u5c06\u591a\u6a21\u6001\u8f93\u5165\u5f15\u5165\u7ffb\u8bd1\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u7ffb\u8bd1\u8fc7\u7a0b\u548c\u5f15\u5165\u5148\u8fdb\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ffb\u8bd1\u6027\u80fd\uff0c\u5e76\u5728\u591a\u6a21\u6001\u7ffb\u8bd1\u9886\u57df\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2507.07143", "pdf": "https://arxiv.org/pdf/2507.07143", "abs": "https://arxiv.org/abs/2507.07143", "authors": ["Karthik Pappu", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Understanding Malware Propagation Dynamics through Scientific Machine Learning", "categories": ["cs.LG", "cs.CR"], "comment": "17 pages, 6 figures, 4 tables", "summary": "Accurately modeling malware propagation is essential for designing effective\ncybersecurity defenses, particularly against adaptive threats that evolve in\nreal time. While traditional epidemiological models and recent neural\napproaches offer useful foundations, they often fail to fully capture the\nnonlinear feedback mechanisms present in real-world networks. In this work, we\napply scientific machine learning to malware modeling by evaluating three\napproaches: classical Ordinary Differential Equations (ODEs), Universal\nDifferential Equations (UDEs), and Neural ODEs. Using data from the Code Red\nworm outbreak, we show that the UDE approach substantially reduces prediction\nerror compared to both traditional and neural baselines by 44%, while\npreserving interpretability. We introduce a symbolic recovery method that\ntransforms the learned neural feedback into explicit mathematical expressions,\nrevealing suppression mechanisms such as network saturation, security response,\nand malware variant evolution. Our results demonstrate that hybrid\nphysics-informed models can outperform both purely analytical and purely neural\napproaches, offering improved predictive accuracy and deeper insight into the\ndynamics of malware spread. These findings support the development of early\nwarning systems, efficient outbreak response strategies, and targeted cyber\ndefense interventions.", "AI": {"tldr": "\u8bba\u6587\u5e94\u7528\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u7279\u522b\u662f\u901a\u7528\u5fae\u5206\u65b9\u7a0bUDES\uff09\u5efa\u6a21\u6076\u610f\u8f6f\u4ef6\u4f20\u64ad\uff0c\u53d1\u73b0\u5176\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u53ca\u7eaf\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5e76\u63ed\u793a\u4e86\u4f20\u64ad\u6291\u5236\u673a\u5236\u3002", "motivation": "\u51c6\u786e\u5efa\u6a21\u6076\u610f\u8f6f\u4ef6\u4f20\u64ad\u5bf9\u4e8e\u8bbe\u8ba1\u6709\u6548\u7684\u7f51\u7edc\u5b89\u5168\u9632\u5fa1\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u5b9e\u65f6\u6f14\u53d8\u7684\u81ea\u9002\u5e94\u5a01\u80c1\u65f6\u3002\u73b0\u6709\u6a21\u578b\uff08\u4f20\u7edf\u6d41\u884c\u75c5\u5b66\u548c\u7eaf\u795e\u7ecf\u7f51\u7edc\uff09\u96be\u4ee5\u5145\u5206\u6355\u6349\u73b0\u5b9e\u7f51\u7edc\u4e2d\u6076\u610f\u8f6f\u4ef6\u4f20\u64ad\u7684\u975e\u7ebf\u6027\u53cd\u9988\u673a\u5236\u3002", "method": "\u8bba\u6587\u5e94\u7528\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5bf9\u6076\u610f\u8f6f\u4ef6\u4f20\u64ad\u8fdb\u884c\u5efa\u6a21\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a\u7ecf\u5178\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODEs\uff09\u3001\u901a\u7528\u5fae\u5206\u65b9\u7a0b\uff08UDEs\uff09\u548c\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b\uff08Neural ODEs\uff09\u3002\u7814\u7a76\u4f7f\u7528Code Red\u8815\u866b\u7206\u53d1\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u7b26\u53f7\u6062\u590d\u65b9\u6cd5\uff0c\u5c06\u5b66\u4e60\u5230\u7684\u795e\u7ecf\u53cd\u9988\u8f6c\u5316\u4e3a\u663e\u5f0f\u6570\u5b66\u8868\u8fbe\u5f0f\u3002", "result": "\u901a\u7528\u5fae\u5206\u65b9\u7a0b\uff08UDE\uff09\u65b9\u6cd5\u4e0e\u4f20\u7edf\u548c\u7eaf\u795e\u7ecf\u7f51\u7edc\u57fa\u7ebf\u76f8\u6bd4\uff0c\u9884\u6d4b\u8bef\u5dee\u663e\u8457\u964d\u4f4e\u4e8644%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002\u901a\u8fc7\u7b26\u53f7\u6062\u590d\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u7f51\u7edc\u9971\u548c\u3001\u5b89\u5168\u54cd\u5e94\u548c\u6076\u610f\u8f6f\u4ef6\u53d8\u4f53\u6f14\u5316\u7b49\u4f20\u64ad\u6291\u5236\u673a\u5236\u3002\u7ed3\u679c\u8868\u660e\u6df7\u5408\u7269\u7406\u4fe1\u606f\u6a21\u578b\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u5bf9\u6076\u610f\u8f6f\u4ef6\u4f20\u64ad\u52a8\u6001\u7684\u6df1\u5ea6\u6d1e\u5bdf\u65b9\u9762\u5747\u4f18\u4e8e\u7eaf\u5206\u6790\u548c\u7eaf\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u3002", "conclusion": "\u6df7\u5408\u7269\u7406\u4fe1\u606f\u6a21\u578b\uff08\u5982UDE\uff09\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u6076\u610f\u8f6f\u4ef6\u4f20\u64ad\u5efa\u6a21\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5f00\u53d1\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u3001\u9ad8\u6548\u7684\u7206\u53d1\u54cd\u5e94\u7b56\u7565\u4ee5\u53ca\u6709\u9488\u5bf9\u6027\u7684\u7f51\u7edc\u9632\u5fa1\u5e72\u9884\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2507.07154", "pdf": "https://arxiv.org/pdf/2507.07154", "abs": "https://arxiv.org/abs/2507.07154", "authors": ["Desheng Li", "Chaoliang Liu", "Zhiyong Xiao"], "title": "CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate segmentation of polyps from colonoscopy images is crucial for the\nearly diagnosis and treatment of colorectal cancer. Most existing deep\nlearning-based polyp segmentation methods adopt an Encoder-Decoder\narchitecture, and some utilize multi-task frameworks that incorporate auxiliary\ntasks such as classification to enhance segmentation performance. However,\nthese approaches often require additional labeled data and rely on task\nsimilarity, which can limit their generalizability. To address these\nchallenges, we propose CL-Polyp, a contrastive learning-enhanced polyp\nsegmentation network. Our method leverages contrastive learning to improve the\nencoder's ability to extract discriminative features by contrasting positive\nand negative sample pairs derived from polyp images. This self-supervised\nstrategy enhances visual representation without requiring additional\nannotations. In addition, we introduce two lightweight and effective modules:\nthe Modified Atrous Spatial Pyramid Pooling (MASPP) module for better\nmulti-scale feature fusion, and the Channel Concatenate and Element Add (CA)\nmodule to fuse low-level and upsampled features for improved boundary\nreconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG,\nCVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-demonstrate that CL-Polyp\nconsistently outperforms state-of-the-art methods. Specifically, it improves\nthe IoU metric by 0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets,\nrespectively, validating its effectiveness in clinical polyp segmentation\ntasks.", "AI": {"tldr": "CL-Polyp\u662f\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u606f\u8089\u5206\u5272\u7f51\u7edc\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5f15\u5165MASPP\u548cCA\u6a21\u5757\u4f18\u5316\u591a\u5c3a\u5ea6\u878d\u5408\u53ca\u8fb9\u754c\u91cd\u5efa\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "motivation": "\u7ed3\u76f4\u80a0\u606f\u8089\u7684\u51c6\u786e\u5206\u5272\u5bf9\u764c\u75c7\u65e9\u671f\u8bca\u65ad\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u606f\u8089\u5206\u5272\u65b9\u6cd5\uff08\u5982\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u548c\u591a\u4efb\u52a1\u6846\u67b6\uff09\u5e38\u9700\u989d\u5916\u6807\u6ce8\u6570\u636e\u4e14\u6cdb\u5316\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51faCL-Polyp\uff0c\u4e00\u4e2a\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7684\u606f\u8089\u5206\u5272\u7f51\u7edc\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u901a\u8fc7\u6b63\u8d1f\u6837\u672c\u5bf9\u589e\u5f3a\u7f16\u7801\u5668\u7684\u5224\u522b\u6027\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u3002\u540c\u65f6\uff0c\u5f15\u5165\u6539\u8fdb\u7684\u7a7a\u6d1e\u7a7a\u95f4\u91d1\u5b57\u5854\u6c60\u5316\uff08MASPP\uff09\u6a21\u5757\u7528\u4e8e\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u5e76\u5f15\u5165\u901a\u9053\u8fde\u63a5\u548c\u5143\u7d20\u76f8\u52a0\uff08CA\uff09\u6a21\u5757\u7528\u4e8e\u4f4e\u5c42\u548c\u4e0a\u91c7\u6837\u7279\u5f81\u7684\u878d\u5408\uff0c\u4ee5\u6539\u5584\u8fb9\u754c\u91cd\u5efa\u3002", "result": "\u5728Kvasir-SEG\u3001CVC-ClinicDB\u3001CVC-ColonDB\u3001CVC-300\u548cETIS\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCL-Polyp\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u5728Kvasir-SEG\u548cCVC-ClinicDB\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5c06IoU\u6307\u6807\u63d0\u9ad8\u4e860.011\u548c0.020\u3002", "conclusion": "CL-Polyp\u5728\u4e34\u5e8a\u606f\u8089\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6709\u6548\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u4f18\u5316\u7684\u6a21\u5757\u8bbe\u8ba1\uff0c\u5728\u65e0\u9700\u989d\u5916\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u606f\u8089\u5206\u5272\u6027\u80fd\u7684\u80fd\u529b\u3002"}}
{"id": "2507.07441", "pdf": "https://arxiv.org/pdf/2507.07441", "abs": "https://arxiv.org/abs/2507.07441", "authors": ["Yu Xia", "Yiran Jenny Shen", "Junda Wu", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Lina Yao", "Julian McAuley"], "title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Model (LLM) agents are commonly tuned with supervised\nfinetuning on ReAct-style expert trajectories or preference optimization over\npairwise rollouts. Most of these methods focus on imitating specific expert\nbehaviors or promoting chosen reasoning thoughts and actions over rejected\nones. However, without reasoning and comparing over alternatives actions, LLM\nagents finetuned with these methods may over-commit towards seemingly plausible\nbut suboptimal actions due to limited action space exploration. To address\nthis, in this paper we propose Self-taught ActioN Deliberation (SAND)\nframework, enabling LLM agents to explicitly deliberate over candidate actions\nbefore committing to one. To tackle the challenges of when and what to\ndeliberate given large action space and step-level action evaluation, we\nincorporate self-consistency action sampling and execution-guided action\ncritique to help synthesize step-wise action deliberation thoughts using the\nbase model of the LLM agent. In an iterative manner, the deliberation\ntrajectories are then used to finetune the LLM agent itself. Evaluating on two\nrepresentative interactive agent tasks, SAND achieves an average 20%\nimprovement over initial supervised finetuning and also outperforms\nstate-of-the-art agent tuning approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAND\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u667a\u80fd\u4f53\u5728\u884c\u52a8\u524d\u663e\u5f0f\u5730\u5ba1\u8bae\u5019\u9009\u884c\u52a8\u6765\u89e3\u51b3\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u63a2\u7d22\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u4ea4\u4e92\u5f0f\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u5fae\u8c03\u65b9\u6cd5\uff08\u5982\u76d1\u7763\u5fae\u8c03\u3001\u504f\u597d\u4f18\u5316\uff09\u4fa7\u91cd\u4e8e\u6a21\u4eff\u4e13\u5bb6\u884c\u4e3a\u6216\u63a8\u5e7f\u7279\u5b9a\u63a8\u7406\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u66ff\u4ee3\u884c\u52a8\u7684\u5ba1\u8bae\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u667a\u80fd\u4f53\u56e0\u884c\u52a8\u7a7a\u95f4\u63a2\u7d22\u6709\u9650\u800c\u9009\u62e9\u6b21\u4f18\u884c\u52a8\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u81ea\u5b66\u884c\u52a8\u5ba1\u8bae\uff08Self-taught ActioN Deliberation, SAND\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u4f7fLLM\u667a\u80fd\u4f53\u80fd\u591f\u5728\u63d0\u4ea4\u884c\u52a8\u4e4b\u524d\uff0c\u663e\u5f0f\u5730\u5ba1\u8bae\u5019\u9009\u884c\u52a8\u3002\u4e3a\u89e3\u51b3\u5927\u89c4\u6a21\u884c\u52a8\u7a7a\u95f4\u548c\u6b65\u7ea7\u884c\u52a8\u8bc4\u4f30\u7684\u6311\u6218\uff0cSAND\u7ed3\u5408\u4e86\u81ea\u6d3d\u884c\u52a8\u91c7\u6837\uff08self-consistency action sampling\uff09\u548c\u6267\u884c\u5f15\u5bfc\u884c\u52a8\u6279\u5224\uff08execution-guided action critique\uff09\uff0c\u5229\u7528LLM\u667a\u80fd\u4f53\u7684\u57fa\u7840\u6a21\u578b\u5408\u6210\u9010\u6b65\u7684\u884c\u52a8\u5ba1\u8bae\u601d\u60f3\u3002\u8fd9\u4e9b\u5ba1\u8bae\u8f68\u8ff9\u968f\u540e\u88ab\u8fed\u4ee3\u5730\u7528\u4e8e\u5fae\u8c03LLM\u667a\u80fd\u4f53\u672c\u8eab\u3002", "result": "\u5728\u4e24\u4e2a\u4ee3\u8868\u6027\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cSAND\u6bd4\u521d\u59cb\u76d1\u7763\u5fae\u8c03\u5e73\u5747\u63d0\u9ad8\u4e8620%\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u667a\u80fd\u4f53\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "SAND\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u663e\u5f0f\u884c\u52a8\u5ba1\u8bae\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5728\u884c\u52a8\u9009\u62e9\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u4ea4\u4e92\u5f0f\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8fd9\u79cd\u81ea\u5b66\u5ba1\u8bae\u65b9\u6cd5\u5728\u667a\u80fd\u4f53\u5fae\u8c03\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.07117", "pdf": "https://arxiv.org/pdf/2507.07117", "abs": "https://arxiv.org/abs/2507.07117", "authors": ["Jit Gupta", "Andrew Li", "Tarun Banka", "Ariel Cohen", "T. Sridhar", "Raj Yavatkar"], "title": "Collective Communication Profiling of Modern-day Machine Learning Workloads", "categories": ["cs.DC", "cs.AI", "cs.NI"], "comment": "Poser, USENIX NSDI 2025, April 2025, Philadelphia, PA, USA", "summary": "Machine Learning jobs, carried out on large number of distributed high\nperformance systems, involve periodic communication using operations like\nAllReduce, AllGather, and Broadcast. These operations may create high bandwidth\nand bursty traffic patterns, leading to network congestion and packet loss,\nthus impacting the performance of these jobs. Hence it is imperative to analyze\nthese patterns, which can be helpful in provisioning network resources\ndepending on the type of machine learning workloads. In this poster we carry\nout extensive analysis of the collective communication behavior seen in a wide\nvariety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we\ninstrument Nvidia Collective Communication Library logging functionality for\nricher context about the collectives and workloads. We adjust configuration\nparameters that influence collective communication behavior, such as\nparallelism, number of nodes, and model type. This overview presents and\ndiscusses some of the results on the collective communication behavior for the\nopen source DeepSeek V3 inferencing model, which includes operation type and\ncount, transfer sizes per operation, and request size distribution. Our\nanalysis shows that it makes sense to rethink current collective communication\nframeworks and network topologies so as to accommodate the effect of network\nanomalies on the mentioned workloads.", "AI": {"tldr": "\u5206\u6790\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u4e2d\u96c6\u4f53\u901a\u4fe1\u7684\u7f51\u7edc\u884c\u4e3a\uff0c\u53d1\u73b0\u5176\u53ef\u5bfc\u81f4\u62e5\u585e\uff0c\u5e76\u5229\u7528Nvidia NCCL\u65e5\u5fd7\u5bf9\u591a\u79cd\u6a21\u578b\u8fdb\u884c\u6df1\u5ea6\u5206\u6790\uff0c\u5efa\u8bae\u91cd\u65b0\u8bbe\u8ba1\u901a\u4fe1\u6846\u67b6\u548c\u7f51\u7edc\u62d3\u6251\u4ee5\u5e94\u5bf9\u7f51\u7edc\u5f02\u5e38\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u9ad8\u6027\u80fd\u7cfb\u7edf\u4e0a\u8fd0\u884c\u7684\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u96c6\u4f53\u901a\u4fe1\u64cd\u4f5c\u4f1a\u4ea7\u751f\u9ad8\u5e26\u5bbd\u548c\u7a81\u53d1\u6d41\u91cf\u6a21\u5f0f\uff0c\u6613\u5bfc\u81f4\u7f51\u7edc\u62e5\u585e\u548c\u4e22\u5305\uff0c\u4ece\u800c\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u5206\u6790\u8fd9\u4e9b\u6a21\u5f0f\u5bf9\u4e8e\u6839\u636e\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7c7b\u578b\u914d\u7f6e\u7f51\u7edc\u8d44\u6e90\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5bf9\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982DeepSeek\u3001GPT\u3001Llama\u7b49\uff09\u7684\u96c6\u4f53\u901a\u4fe1\u884c\u4e3a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5206\u6790\u3002\u901a\u8fc7\u5229\u7528Nvidia\u96c6\u4f53\u901a\u4fe1\u5e93\uff08NCCL\uff09\u7684\u65e5\u5fd7\u529f\u80fd\u83b7\u53d6\u4e30\u5bcc\u7684\u96c6\u4f53\u901a\u4fe1\u548c\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u540c\u65f6\uff0c\u8c03\u6574\u4e86\u5f71\u54cd\u96c6\u4f53\u901a\u4fe1\u884c\u4e3a\u7684\u914d\u7f6e\u53c2\u6570\uff0c\u5982\u5e76\u884c\u5ea6\u3001\u8282\u70b9\u6570\u91cf\u548c\u6a21\u578b\u7c7b\u578b\u3002", "result": "\u672c\u6982\u8ff0\u5c55\u793a\u5e76\u8ba8\u8bba\u4e86\u5f00\u6e90DeepSeek V3\u63a8\u7406\u6a21\u578b\u7684\u96c6\u4f53\u901a\u4fe1\u884c\u4e3a\u7ed3\u679c\uff0c\u5305\u62ec\u64cd\u4f5c\u7c7b\u578b\u548c\u8ba1\u6570\u3001\u6bcf\u6b21\u64cd\u4f5c\u7684\u4f20\u8f93\u5927\u5c0f\u4ee5\u53ca\u8bf7\u6c42\u5927\u5c0f\u5206\u5e03\u3002", "conclusion": "\u5206\u6790\u8868\u660e\uff0c\u6709\u5fc5\u8981\u91cd\u65b0\u601d\u8003\u5f53\u524d\u7684\u96c6\u4f53\u901a\u4fe1\u6846\u67b6\u548c\u7f51\u7edc\u62d3\u6251\uff0c\u4ee5\u9002\u5e94\u7f51\u7edc\u5f02\u5e38\u5bf9\u6240\u8ff0\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.07341", "pdf": "https://arxiv.org/pdf/2507.07341", "abs": "https://arxiv.org/abs/2507.07341", "authors": ["Sarah Ball", "Greg Gluch", "Shafi Goldwasser", "Frauke Kreuter", "Omer Reingold", "Guy N. Rothblum"], "title": "On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "With the increased deployment of large language models (LLMs), one concern is\ntheir potential misuse for generating harmful content. Our work studies the\nalignment challenge, with a focus on filters to prevent the generation of\nunsafe information. Two natural points of intervention are the filtering of the\ninput prompt before it reaches the model, and filtering the output after\ngeneration. Our main results demonstrate computational challenges in filtering\nboth prompts and outputs. First, we show that there exist LLMs for which there\nare no efficient prompt filters: adversarial prompts that elicit harmful\nbehavior can be easily constructed, which are computationally indistinguishable\nfrom benign prompts for any efficient filter. Our second main result identifies\na natural setting in which output filtering is computationally intractable. All\nof our separation results are under cryptographic hardness assumptions. In\naddition to these core findings, we also formalize and study relaxed mitigation\napproaches, demonstrating further computational barriers. We conclude that\nsafety cannot be achieved by designing filters external to the LLM internals\n(architecture and weights); in particular, black-box access to the LLM will not\nsuffice. Based on our technical results, we argue that an aligned AI system's\nintelligence cannot be separated from its judgment.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5916\u90e8\u8fc7\u6ee4\u5668\uff08\u5982\u8f93\u5165\u63d0\u793a\u6216\u8f93\u51fa\u5185\u5bb9\u8fc7\u6ee4\uff09\u5728\u8ba1\u7b97\u4e0a\u96be\u4ee5\u6709\u6548\u963b\u6b62\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u5176\u5b89\u5168\u6027\u65e0\u6cd5\u4ec5\u901a\u8fc7\u9ed1\u76d2\u8bbf\u95ee\u5b9e\u73b0\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u5176\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u6f5c\u5728\u98ce\u9669\u65e5\u76ca\u7a81\u51fa\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8LLM\u7684\u5bf9\u9f50\u6311\u6218\uff0c\u7279\u522b\u662f\u901a\u8fc7\u8fc7\u6ee4\u5668\u6765\u9884\u9632\u4e0d\u5b89\u5168\u4fe1\u606f\u751f\u6210\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u96c6\u4e2d\u4e8e\u4e24\u79cd\u81ea\u7136\u5e72\u9884\u70b9\uff1a\u5728\u63d0\u793a\u5230\u8fbe\u6a21\u578b\u524d\u7684\u8f93\u5165\u8fc7\u6ee4\u548c\u751f\u6210\u540e\u7684\u8f93\u51fa\u8fc7\u6ee4\u3002\u901a\u8fc7\u8ba1\u7b97\u590d\u6742\u6027\u5206\u6790\uff0c\u5e76\u5728\u5bc6\u7801\u5b66\u786c\u5ea6\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u4e86\u8fc7\u6ee4\u5668\u7684\u5c40\u9650\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5f62\u5f0f\u5316\u5e76\u7814\u7a76\u4e86\u5bbd\u677e\u7684\u7f13\u89e3\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) \u5b58\u5728\u67d0\u4e9bLLM\uff0c\u5176\u9ad8\u6548\u7684\u63d0\u793a\u8fc7\u6ee4\u5668\u662f\u4e0d\u5b58\u5728\u7684\uff0c\u56e0\u4e3a\u5bf9\u6297\u6027\u63d0\u793a\u5728\u8ba1\u7b97\u4e0a\u4e0e\u826f\u6027\u63d0\u793a\u5bf9\u4e8e\u4efb\u4f55\u9ad8\u6548\u8fc7\u6ee4\u5668\u90fd\u96be\u4ee5\u533a\u5206\u30022) \u5728\u7279\u5b9a\u81ea\u7136\u573a\u666f\u4e0b\uff0c\u8f93\u51fa\u8fc7\u6ee4\u5728\u8ba1\u7b97\u4e0a\u662f\u4e0d\u53ef\u884c\u7684\u3002\u6240\u6709\u5206\u79bb\u7ed3\u679c\u5747\u57fa\u4e8e\u5bc6\u7801\u5b66\u786c\u5ea6\u5047\u8bbe\u30023) \u5bbd\u677e\u7684\u7f13\u89e3\u65b9\u6cd5\u4e5f\u5b58\u5728\u8ba1\u7b97\u969c\u788d\u3002", "conclusion": "\u672c\u7814\u7a76\u5f97\u51fa\u7ed3\u8bba\uff0cLLM\u7684\u5b89\u5168\u6027\u65e0\u6cd5\u901a\u8fc7\u8bbe\u8ba1\u5916\u90e8\u8fc7\u6ee4\u5668\u6765\u5b9e\u73b0\uff0c\u5373\u4ec5\u901a\u8fc7\u5bf9LLM\u7684\u9ed1\u76d2\u8bbf\u95ee\u662f\u4e0d\u591f\u7684\u3002\u57fa\u4e8e\u6280\u672f\u7ed3\u679c\uff0c\u4f5c\u8005\u8ba4\u4e3a\u5bf9\u9f50\u7684AI\u7cfb\u7edf\u7684\u667a\u80fd\u6027\u4e0d\u80fd\u4e0e\u5176\u5224\u65ad\u80fd\u529b\u5206\u79bb\u3002"}}
{"id": "2507.07145", "pdf": "https://arxiv.org/pdf/2507.07145", "abs": "https://arxiv.org/abs/2507.07145", "authors": ["Zhaojing Zhou", "Xunchao Li", "Minghao Li", "Handi Zhang", "Haoshuang Wang", "Wenbin Chang", "Yiqun Liu", "Qingqing Dang", "Dianhai Yu", "Yanjun Ma", "Haifeng Wang"], "title": "CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs", "categories": ["cs.LG"], "comment": "11 pages, 3 figures", "summary": "The rapid scaling of Large Language Models (LLMs) elevates inference costs\nand compounds substantial deployment barriers. While quantization to 8 or 4\nbits mitigates this, sub-3-bit methods face severe accuracy, scalability, and\nefficiency degradation. We propose Convolutional Code Quantization (CCQ), an\ninference-optimized quantization approach compressing LLMs to 2.0-2.75 bits\nwith minimal accuracy loss. Departing from error-prone scalar quantization or\nslow vector quantization, CCQ integrates a hardware-aware bit-shift encoding\nand decoding solution with Convolutional Code, Hybrid Encoding, and Code\nCluster, jointly overcoming accuracy-speed bottlenecks. We construct a\nlookup-free encoding space, enabling a linear mapping between the codebook and\nweight vectors, thereby optimizing inference performance. Meanwhile, by drawing\non the concept of data mapping from vector quantization, we minimize the\nperformance degradation of the model under extremely low-bit conditions.\nExperiments demonstrate that CCQ achieves outstanding performance on LLMs\nacross various benchmarks. We compress DeepSeek-V3 (671B total parameters) to\n184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment of ERNIE\n4.5 and eliminating inter-card communication. The 2-bit ERNIE-4.5-300B-A47B\nmodel and inference engine have been open-sourced.", "AI": {"tldr": "CCQ\u662f\u4e00\u79cd\u63a8\u7406\u4f18\u5316\u91cf\u5316\u65b9\u6cd5\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u538b\u7f29\u81f32.0-2.75\u6bd4\u7279\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u548c\u90e8\u7f72\u969c\u788d\uff0c\u540c\u65f6\u4fdd\u6301\u6781\u4f4e\u7cbe\u5ea6\u635f\u5931\uff0c\u5e76\u5b9e\u73b0\u5355GPU\u90e8\u7f72\u5927\u578b\u6a21\u578b\u3002", "motivation": "LLMs\u7684\u5feb\u901f\u6269\u5c55\u5bfc\u81f4\u9ad8\u6602\u7684\u63a8\u7406\u6210\u672c\u548c\u4e25\u5cfb\u7684\u90e8\u7f72\u6311\u6218\u3002\u867d\u71368\u62164\u6bd4\u7279\u91cf\u5316\u80fd\u7f13\u89e3\uff0c\u4f46\u4f4e\u4e8e3\u6bd4\u7279\u7684\u65b9\u6cd5\u9762\u4e34\u4e25\u91cd\u7684\u7cbe\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5377\u79ef\u7801\u91cf\u5316\uff08CCQ\uff09\uff0c\u96c6\u6210\u786c\u4ef6\u611f\u77e5\u7684\u4f4d\u79fb\u7f16\u7801\u89e3\u7801\u65b9\u6848\u3001\u5377\u79ef\u7801\u3001\u6df7\u5408\u7f16\u7801\u548c\u4ee3\u7801\u7c07\uff0c\u4ee5\u514b\u670d\u7cbe\u5ea6-\u901f\u5ea6\u74f6\u9888\u3002\u6784\u5efa\u65e0\u67e5\u627e\u7684\u7f16\u7801\u7a7a\u95f4\uff0c\u5b9e\u73b0\u7801\u672c\u4e0e\u6743\u91cd\u5411\u91cf\u7684\u7ebf\u6027\u6620\u5c04\u4ee5\u4f18\u5316\u63a8\u7406\u6027\u80fd\u3002\u501f\u9274\u5411\u91cf\u91cf\u5316\u7684\u6570\u636e\u6620\u5c04\u6982\u5ff5\uff0c\u6700\u5c0f\u5316\u6781\u4f4e\u6bd4\u7279\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u9000\u5316\u3002", "result": "CCQ\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u6210\u529f\u5c06DeepSeek-V3\uff08671B\uff09\u538b\u7f29\u81f3184GB\uff0cERNIE-4.5-300B-A47B\u538b\u7f29\u81f389GB\uff0c\u4ece\u800c\u5b9e\u73b0ERNIE 4.5\u7684\u5355GPU\u90e8\u7f72\u5e76\u6d88\u9664\u5361\u95f4\u901a\u4fe1\u30022\u6bd4\u7279\u7684ERNIE-4.5-300B-A47B\u6a21\u578b\u548c\u63a8\u7406\u5f15\u64ce\u5df2\u5f00\u6e90\u3002", "conclusion": "CCQ\u901a\u8fc7\u521b\u65b0\u7684\u91cf\u5316\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u6210\u672c\u548c\u90e8\u7f72\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u6781\u4f4e\u6bd4\u7279\u4e0b\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7387\u7684\u91cf\u5316\uff0c\u4e3aLLM\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07157", "pdf": "https://arxiv.org/pdf/2507.07157", "abs": "https://arxiv.org/abs/2507.07157", "authors": ["Arshak Rezvani", "Ali Akbari", "Kosar Sanjar Arani", "Maryam Mirian", "Emad Arasteh", "Martin J. McKeown"], "title": "Interpretable EEG-to-Image Generation with Semantic Prompts", "categories": ["cs.CV", "cs.LG", "eess.SP"], "comment": "Actionable Interpretability Workshop (non-archival) at the 42\n  International Conference on Machine Learning", "summary": "Decoding visual experience from brain signals offers exciting possibilities\nfor neuroscience and interpretable AI. While EEG is accessible and temporally\nprecise, its limitations in spatial detail hinder image reconstruction. Our\nmodel bypasses direct EEG-to-image generation by aligning EEG signals with\nmultilevel semantic captions -- ranging from object-level to abstract themes --\ngenerated by a large language model. A transformer-based EEG encoder maps brain\nactivity to these captions through contrastive learning. During inference,\ncaption embeddings retrieved via projection heads condition a pretrained latent\ndiffusion model for image generation. This text-mediated framework yields\nstate-of-the-art visual decoding on the EEGCVPR dataset, with interpretable\nalignment to known neurocognitive pathways. Dominant EEG-caption associations\nreflected the importance of different semantic levels extracted from perceived\nimages. Saliency maps and t-SNE projections reveal semantic topography across\nthe scalp. Our model demonstrates how structured semantic mediation enables\ncognitively aligned visual decoding from EEG.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u6587\u672c\u4ecb\u5bfc\u7684\u89c6\u89c9\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5c06EEG\u4fe1\u53f7\u4e0eLLM\u751f\u6210\u7684\u8bed\u4e49\u63cf\u8ff0\u5bf9\u9f50\uff0c\u518d\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u4eceEEG\u4fe1\u53f7\u5230\u89c6\u89c9\u4f53\u9a8c\u7684\u9ad8\u7cbe\u5ea6\u89e3\u7801\uff0c\u5e76\u5177\u6709\u795e\u7ecf\u8ba4\u77e5\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5c3d\u7ba1EEG\u6613\u4e8e\u83b7\u53d6\u4e14\u65f6\u95f4\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u5176\u7a7a\u95f4\u7ec6\u8282\u9650\u5236\u4e86\u76f4\u63a5\u7684\u56fe\u50cf\u91cd\u5efa\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u4eceEEG\u4fe1\u53f7\u4e2d\u53ef\u9760\u5730\u89e3\u7801\u89c6\u89c9\u4f53\u9a8c\u3002", "method": "\u8be5\u6a21\u578b\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4f7f\u7528Transformer\u7f16\u7801\u5668\u5c06EEG\u4fe1\u53f7\u6620\u5c04\u5230\u591a\u7ea7\u522b\uff08\u4ece\u7269\u4f53\u5230\u62bd\u8c61\u6982\u5ff5\uff09\u7684\u8bed\u4e49\u63cf\u8ff0\u3002\u8fd9\u4e9b\u63cf\u8ff0\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u5229\u7528\u6295\u5f71\u5934\u63d0\u53d6\u7684\u63cf\u8ff0\u5d4c\u5165\u6765\u6761\u4ef6\u5316\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u4ece\u800c\u751f\u6210\u56fe\u50cf\uff0c\u907f\u514d\u4e86EEG\u5230\u56fe\u50cf\u7684\u76f4\u63a5\u8f6c\u6362\u3002", "result": "\u8be5\u6587\u672c\u4ecb\u5bfc\u7684\u6846\u67b6\u5728EEGCVPR\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u89e3\u7801\u6027\u80fd\uff0c\u5e76\u4e0e\u5df2\u77e5\u7684\u795e\u7ecf\u8ba4\u77e5\u901a\u8def\u5177\u6709\u53ef\u89e3\u91ca\u7684\u5bf9\u9f50\u3002EEG\u4e0e\u8bed\u4e49\u63cf\u8ff0\u7684\u5173\u8054\u63ed\u793a\u4e86\u4e0d\u540c\u8bed\u4e49\u5c42\u7ea7\u7684\u91cd\u8981\u6027\u3002\u663e\u8457\u6027\u56fe\u548ct-SNE\u6295\u5f71\u663e\u793a\u4e86\u5934\u76ae\u4e0a\u7684\u8bed\u4e49\u62d3\u6251\u7ed3\u6784\u3002", "conclusion": "\u7ed3\u6784\u5316\u7684\u8bed\u4e49\u4e2d\u4ecb\u80fd\u591f\u5b9e\u73b0\u4e0e\u8ba4\u77e5\u5bf9\u9f50\u7684EEG\u89c6\u89c9\u89e3\u7801\u3002"}}
{"id": "2507.07451", "pdf": "https://arxiv.org/pdf/2507.07451", "abs": "https://arxiv.org/abs/2507.07451", "authors": ["Hongzhi Zhang", "Jia Fu", "Jingyuan Zhang", "Kai Fu", "Qi Wang", "Fuzheng Zhang", "Guorui Zhou"], "title": "RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning", "categories": ["cs.CL"], "comment": "https://github.com/Kwai-Klear/RLEP", "summary": "Reinforcement learning (RL) for large language models is an energy-intensive\nendeavor: training can be unstable, and the policy may gradually drift away\nfrom its pretrained weights. We present \\emph{RLEP}\\, -- \\,Reinforcement\nLearning with Experience rePlay\\, -- \\,a two-phase framework that first\ncollects verified trajectories and then replays them during subsequent\ntraining. At every update step, the policy is optimized on mini-batches that\nblend newly generated rollouts with these replayed successes. By replaying\nhigh-quality examples, RLEP steers the model away from fruitless exploration,\nfocuses learning on promising reasoning paths, and delivers both faster\nconvergence and stronger final performance. On the Qwen2.5-Math-7B base model,\nRLEP reaches baseline peak accuracy with substantially fewer updates and\nultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,\non AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our\ncode, datasets, and checkpoints are publicly available at\nhttps://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further\nresearch.", "AI": {"tldr": "RLEP\uff08\u5f3a\u5316\u5b66\u4e60\u7ecf\u9a8c\u56de\u653e\uff09\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u6536\u96c6\u548c\u56de\u653e\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u7b56\u7565\u6f02\u79fb\u95ee\u9898\u3002\u5b83\u80fd\u52a0\u901f\u6536\u655b\u5e76\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5c06\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5e94\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u4e00\u4e2a\u8017\u80fd\u5de8\u5927\u7684\u8fc7\u7a0b\uff0c\u5176\u8bad\u7ec3\u53ef\u80fd\u4e0d\u7a33\u5b9a\uff0c\u4e14\u7b56\u7565\u5bb9\u6613\u9010\u6e10\u504f\u79bb\u9884\u8bad\u7ec3\u6743\u91cd\u3002", "method": "\u63d0\u51faRLEP\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\u6536\u96c6\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u8f68\u8ff9\uff0c\u7136\u540e\u5728\u540e\u7eed\u8bad\u7ec3\u4e2d\u56de\u653e\u8fd9\u4e9b\u8f68\u8ff9\u3002\u5728\u6bcf\u6b21\u66f4\u65b0\u6b65\u9aa4\u4e2d\uff0c\u7b56\u7565\u901a\u8fc7\u7ed3\u5408\u65b0\u751f\u6210\u7684Rollout\u548c\u8fd9\u4e9b\u56de\u653e\u7684\u6210\u529f\u7ecf\u9a8c\u7684Mini-batch\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u56de\u653e\u9ad8\u8d28\u91cf\u793a\u4f8b\uff0cRLEP\u907f\u514d\u4e86\u65e0\u6548\u63a2\u7d22\uff0c\u5c06\u5b66\u4e60\u96c6\u4e2d\u5728\u6709\u524d\u666f\u7684\u63a8\u7406\u8def\u5f84\u4e0a\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u548c\u66f4\u5f3a\u7684\u6700\u7ec8\u6027\u80fd\u3002\u5728Qwen2.5-Math-7B\u6a21\u578b\u4e0a\uff0cRLEP\u4ee5\u66f4\u5c11\u7684\u66f4\u65b0\u6b21\u6570\u8fbe\u5230\u5e76\u8d85\u8d8a\u4e86\u57fa\u7ebf\u5cf0\u503c\u51c6\u786e\u7387\uff0c\u5c06AIME-2024\u7684\u51c6\u786e\u7387\u4ece38.2%\u63d0\u9ad8\u523039.9%\uff0cAIME-2025\u4ece19.8%\u63d0\u9ad8\u523022.3%\uff0cAMC-2023\u4ece77.0%\u63d0\u9ad8\u523082.2%\u3002", "conclusion": "RLEP\u901a\u8fc7\u5f15\u5165\u7ecf\u9a8c\u56de\u653e\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u6548\u7387\u3001\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u6027\u80fd\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002"}}
{"id": "2507.07581", "pdf": "https://arxiv.org/pdf/2507.07581", "abs": "https://arxiv.org/abs/2507.07581", "authors": ["Michail Kalntis", "Fernando A. Kuipers", "George Iosifidis"], "title": "CHOMET: Conditional Handovers via Meta-Learning", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "Handovers (HOs) are the cornerstone of modern cellular networks for enabling\nseamless connectivity to a vast and diverse number of mobile users. However, as\nmobile networks become more complex with more diverse users and smaller cells,\ntraditional HOs face significant challenges, such as prolonged delays and\nincreased failures. To mitigate these issues, 3GPP introduced conditional\nhandovers (CHOs), a new type of HO that enables the preparation (i.e., resource\nallocation) of multiple cells for a single user to increase the chance of HO\nsuccess and decrease the delays in the procedure. Despite its advantages, CHO\nintroduces new challenges that must be addressed, including efficient resource\nallocation and managing signaling/communication overhead from frequent cell\npreparations and releases. This paper presents a novel framework aligned with\nthe O-RAN paradigm that leverages meta-learning for CHO optimization, providing\nrobust dynamic regret guarantees and demonstrating at least 180% superior\nperformance than other 3GPP benchmarks in volatile signal conditions.", "AI": {"tldr": "O-RAN\u8303\u5f0f\u4e0b\uff0c\u5229\u7528\u5143\u5b66\u4e60\u4f18\u5316\u6761\u4ef6\u5207\u6362(CHO)\uff0c\u89e3\u51b3\u5176\u8d44\u6e90\u5206\u914d\u548c\u4fe1\u4ee4\u5f00\u9500\u6311\u6218\uff0c\u5e76\u5728\u6ce2\u52a8\u4fe1\u53f7\u6761\u4ef6\u4e0b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u51c6\u3002", "motivation": "\u4f20\u7edf\u5207\u6362(HOs)\u5728\u590d\u6742\u79fb\u52a8\u7f51\u7edc\u4e2d\u9762\u4e34\u9ad8\u5ef6\u8fdf\u548c\u9ad8\u5931\u8d25\u7387\u3002\u5c3d\u7ba13GPP\u5f15\u5165\u4e86\u6761\u4ef6\u5207\u6362(CHOs)\u4ee5\u63d0\u5347\u6210\u529f\u7387\u548c\u51cf\u5c11\u5ef6\u8fdf\uff0c\u4f46CHO\u81ea\u8eab\u5f15\u5165\u4e86\u8d44\u6e90\u9ad8\u6548\u5206\u914d\u548c\u4fe1\u4ee4/\u901a\u4fe1\u5f00\u9500\u7ba1\u7406\u7684\u65b0\u6311\u6218\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u4e0eO-RAN\u8303\u5f0f\u5bf9\u9f50\u7684\u65b0\u9896\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5143\u5b66\u4e60\uff08meta-learning\uff09\u6280\u672f\u8fdb\u884cCHO\u4f18\u5316\u3002", "result": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u52a8\u6001\u9057\u61be\uff08dynamic regret\uff09\u4fdd\u8bc1\uff0c\u5e76\u5728\u6ce2\u52a8\u4fe1\u53f7\u6761\u4ef6\u4e0b\uff0c\u6027\u80fd\u6bd4\u5176\u4ed63GPP\u57fa\u51c6\u63d0\u9ad8\u4e86\u81f3\u5c11180%\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u5143\u5b66\u4e60\u7684O-RAN\u6846\u67b6\u80fd\u591f\u6709\u6548\u4f18\u5316\u6761\u4ef6\u5207\u6362\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u7f51\u7edc\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u5207\u6362\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u672a\u6765\u7f51\u7edc\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07355", "pdf": "https://arxiv.org/pdf/2507.07355", "abs": "https://arxiv.org/abs/2507.07355", "authors": ["Haoyue Bai", "Haoyu Wang", "Nanxu Gong", "Xinyuan Wang", "Wangyang Ying", "Haifeng Chen", "Yanjie Fu"], "title": "Supply Chain Optimization via Generative Simulation and Iterative Decision Policies", "categories": ["cs.AI"], "comment": null, "summary": "High responsiveness and economic efficiency are critical objectives in supply\nchain transportation, both of which are influenced by strategic decisions on\nshipping mode. An integrated framework combining an efficient simulator with an\nintelligent decision-making algorithm can provide an observable, low-risk\nenvironment for transportation strategy design. An ideal simulation-decision\nframework must (1) generalize effectively across various settings, (2) reflect\nfine-grained transportation dynamics, (3) integrate historical experience with\npredictive insights, and (4) maintain tight integration between simulation\nfeedback and policy refinement. We propose Sim-to-Dec framework to satisfy\nthese requirements. Specifically, Sim-to-Dec consists of a generative\nsimulation module, which leverages autoregressive modeling to simulate\ncontinuous state changes, reducing dependence on handcrafted domain-specific\nrules and enhancing robustness against data fluctuations; and a history-future\ndual-aware decision model, refined iteratively through end-to-end optimization\nwith simulator interactions. Extensive experiments conducted on three\nreal-world datasets demonstrate that Sim-to-Dec significantly improves timely\ndelivery rates and profit.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSim-to-Dec\u6846\u67b6\uff0c\u7ed3\u5408\u751f\u6210\u5f0f\u4eff\u771f\u548c\u53cc\u611f\u77e5\u51b3\u7b56\u6a21\u578b\uff0c\u4ee5\u4f18\u5316\u4f9b\u5e94\u94fe\u8fd0\u8f93\u4e2d\u7684\u54cd\u5e94\u6027\u548c\u7ecf\u6d4e\u6548\u7387\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u80fd\u663e\u8457\u63d0\u5347\u53ca\u65f6\u4ea4\u4ed8\u7387\u548c\u5229\u6da6\u3002", "motivation": "\u4f9b\u5e94\u94fe\u8fd0\u8f93\u9762\u4e34\u9ad8\u54cd\u5e94\u6027\u4e0e\u7ecf\u6d4e\u6548\u7387\u7684\u6311\u6218\uff0c\u6218\u7565\u51b3\u7b56\uff08\u5982\u8fd0\u8f93\u6a21\u5f0f\u9009\u62e9\uff09\u5bf9\u5176\u5f71\u54cd\u6df1\u8fdc\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u80fd\u529b\u3001\u7cbe\u7ec6\u5316\u52a8\u6001\u53cd\u6620\u3001\u5386\u53f2\u7ecf\u9a8c\u4e0e\u9884\u6d4b\u6d1e\u5bdf\u6574\u5408\u4ee5\u53ca\u4eff\u771f\u53cd\u9988\u4e0e\u7b56\u7565\u4f18\u5316\u7ed3\u5408\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e9f\u9700\u4e00\u4e2a\u53ef\u89c2\u5bdf\u3001\u4f4e\u98ce\u9669\u7684\u8fd0\u8f93\u7b56\u7565\u8bbe\u8ba1\u73af\u5883\u3002", "method": "\u672c\u6587\u63d0\u51faSim-to-Dec\u6846\u67b6\uff0c\u4e3b\u8981\u5305\u542b\uff1a1) \u751f\u6210\u5f0f\u4eff\u771f\u6a21\u5757\uff0c\u5229\u7528\u81ea\u56de\u5f52\u5efa\u6a21\u6a21\u62df\u8fde\u7eed\u72b6\u6001\u53d8\u5316\uff0c\u51cf\u5c11\u5bf9\u4eba\u5de5\u89c4\u5219\u7684\u4f9d\u8d56\u5e76\u589e\u5f3a\u6570\u636e\u6ce2\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\uff1b2) \u5386\u53f2-\u672a\u6765\u53cc\u611f\u77e5\u51b3\u7b56\u6a21\u578b\uff0c\u901a\u8fc7\u4e0e\u4eff\u771f\u5668\u7684\u7aef\u5230\u7aef\u4f18\u5316\u4ea4\u4e92\u8fdb\u884c\u8fed\u4ee3\u5f0f\u6539\u8fdb\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0cSim-to-Dec\u663e\u8457\u63d0\u9ad8\u4e86\u53ca\u65f6\u4ea4\u4ed8\u7387\u548c\u5229\u6da6\u3002", "conclusion": "Sim-to-Dec\u6846\u67b6\u901a\u8fc7\u5176\u521b\u65b0\u7684\u4eff\u771f\u4e0e\u51b3\u7b56\u96c6\u6210\u65b9\u6cd5\uff0c\u4e3a\u4f9b\u5e94\u94fe\u8fd0\u8f93\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6218\u7565\u8bbe\u8ba1\u5de5\u5177\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9ad8\u54cd\u5e94\u6027\u548c\u7ecf\u6d4e\u6548\u7387\u7684\u96be\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2507.07146", "pdf": "https://arxiv.org/pdf/2507.07146", "abs": "https://arxiv.org/abs/2507.07146", "authors": ["Zixuan Huang", "Kecheng Huang", "Lihao Yin", "Bowei He", "Huiling Zhen", "Mingxuan Yuan", "Zili Shao"], "title": "An attention-aware GNN-based input defender against multi-turn jailbreak on LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have gained widespread popularity and are\nincreasingly integrated into various applications. However, their capabilities\ncan be exploited for both benign and harmful purposes. Despite rigorous\ntraining and fine-tuning for safety, LLMs remain vulnerable to jailbreak\nattacks. Recently, multi-turn attacks have emerged, exacerbating the issue.\nUnlike single-turn attacks, multi-turn attacks gradually escalate the dialogue,\nmaking them more difficult to detect and mitigate, even after they are\nidentified.\n  In this study, we propose G-Guard, an innovative attention-aware GNN-based\ninput classifier designed to defend against multi-turn jailbreak attacks on\nLLMs. G-Guard constructs an entity graph for multi-turn queries, explicitly\ncapturing relationships between harmful keywords and queries even when those\nkeywords appear only in previous queries. Additionally, we introduce an\nattention-aware augmentation mechanism that retrieves the most similar\nsingle-turn query based on the multi-turn conversation. This retrieved query is\ntreated as a labeled node in the graph, enhancing the ability of GNN to\nclassify whether the current query is harmful. Evaluation results demonstrate\nthat G-Guard outperforms all baselines across all datasets and evaluation\nmetrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faG-Guard\uff0c\u4e00\u79cd\u57fa\u4e8eGNN\u7684\u6ce8\u610f\u529b\u611f\u77e5\u8f93\u5165\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u9632\u5fa1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\uff0c\u5e76\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c3d\u7ba1\u7ecf\u8fc7\u4e25\u683c\u7684\u5b89\u5168\u8bad\u7ec3\uff0c\u4f46\u4ecd\u6613\u53d7\u8d8a\u72f1\u653b\u51fb\u3002\u7279\u522b\u662f\uff0c\u591a\u8f6e\u653b\u51fb\u56e0\u5176\u6e10\u8fdb\u5f0f\u5347\u7ea7\u5bf9\u8bdd\u7684\u7279\u6027\uff0c\u6bd4\u5355\u8f6e\u653b\u51fb\u66f4\u96be\u68c0\u6d4b\u548c\u7f13\u89e3\uff0c\u8fd9\u4fc3\u4f7f\u4e86\u5bf9\u6709\u6548\u9632\u5fa1\u673a\u5236\u7684\u9700\u6c42\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86G-Guard\uff0c\u4e00\u79cd\u6ce8\u610f\u529b\u611f\u77e5GNN\uff08\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\u8f93\u5165\u5206\u7c7b\u5668\u3002G-Guard\u4e3a\u591a\u8f6e\u67e5\u8be2\u6784\u5efa\u5b9e\u4f53\u56fe\uff0c\u4ee5\u6355\u6349\u6709\u5bb3\u5173\u952e\u8bcd\u4e0e\u67e5\u8be2\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5373\u4f7f\u5173\u952e\u8bcd\u51fa\u73b0\u5728\u65e9\u671f\u8f6e\u6b21\u3002\u6b64\u5916\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u4e2a\u6ce8\u610f\u529b\u611f\u77e5\u589e\u5f3a\u673a\u5236\uff0c\u6839\u636e\u591a\u8f6e\u5bf9\u8bdd\u68c0\u7d22\u6700\u76f8\u4f3c\u7684\u5355\u8f6e\u67e5\u8be2\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u56fe\u4e2d\u7684\u6807\u7b7e\u8282\u70b9\uff0c\u4ee5\u589e\u5f3aGNN\u7684\u5206\u7c7b\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cG-Guard\u5728\u6240\u6709\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "G-Guard\u88ab\u8bc1\u660e\u662f\u4e00\u79cd\u9488\u5bf9LLMs\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u7684\u6709\u6548\u9632\u5fa1\u673a\u5236\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u63d0\u5347LLMs\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07202", "pdf": "https://arxiv.org/pdf/2507.07202", "abs": "https://arxiv.org/abs/2507.07202", "authors": ["Mohamed Elmoghany", "Ryan Rossi", "Seunghyun Yoon", "Subhojyoti Mukherjee", "Eslam Bakr", "Puneet Mathur", "Gang Wu", "Viet Dac Lai", "Nedim Lipka", "Ruiyi Zhang", "Varun Manjunatha", "Chien Nguyen", "Daksh Dangi", "Abel Salinas", "Mohammad Taesiri", "Hongjie Chen", "Xiaolei Huang", "Joe Barrow", "Nesreen Ahmed", "Hoda Eldardiry", "Namyong Park", "Yu Wang", "Jaemin Cho", "Anh Totti Nguyen", "Zhengzhong Tu", "Thien Nguyen", "Dinesh Manocha", "Mohamed Elhoseiny", "Franck Dernoncourt"], "title": "A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality", "categories": ["cs.CV"], "comment": null, "summary": "Despite the significant progress that has been made in video generative\nmodels, existing state-of-the-art methods can only produce videos lasting 5-16\nseconds, often labeled \"long-form videos\". Furthermore, videos exceeding 16\nseconds struggle to maintain consistent character appearances and scene layouts\nthroughout the narrative. In particular, multi-subject long videos still fail\nto preserve character consistency and motion coherence. While some methods can\ngenerate videos up to 150 seconds long, they often suffer from frame redundancy\nand low temporal diversity. Recent work has attempted to produce long-form\nvideos featuring multiple characters, narrative coherence, and high-fidelity\ndetail. We comprehensively studied 32 papers on video generation to identify\nkey architectural components and training strategies that consistently yield\nthese qualities. We also construct a comprehensive novel taxonomy of existing\nmethods and present comparative tables that categorize papers by their\narchitectural designs and performance characteristics.", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u56de\u987e\u4e8632\u7bc7\u89c6\u9891\u751f\u6210\u9886\u57df\u7684\u8bba\u6587\uff0c\u65e8\u5728\u8bc6\u522b\u80fd\u751f\u6210\u957f\u65f6\u3001\u591a\u89d2\u8272\u3001\u9ad8\u4e00\u81f4\u6027\u89c6\u9891\u7684\u5173\u952e\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u4e86\u65b0\u7684\u5206\u7c7b\u4f53\u7cfb\u548c\u5bf9\u6bd4\u8868\u683c\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u8d85\u8fc716\u79d2\u7684\u957f\u89c6\u9891\u65f6\uff0c\u96be\u4ee5\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\u3001\u573a\u666f\u5e03\u5c40\u548c\u8fd0\u52a8\u8fde\u8d2f\u6027\uff0c\u5c24\u5176\u5728\u591a\u4e3b\u4f53\u89c6\u9891\u4e2d\u95ee\u9898\u66f4\u4e3a\u7a81\u51fa\u3002\u5373\u4f7f\u80fd\u751f\u6210\u66f4\u957f\u89c6\u9891\uff0c\u4e5f\u5e38\u9762\u4e34\u5e27\u5197\u4f59\u548c\u65f6\u95f4\u591a\u6837\u6027\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5bf932\u7bc7\u89c6\u9891\u751f\u6210\u8bba\u6587\u7684\u5168\u9762\u7814\u7a76\uff0c\u5206\u6790\u5e76\u8bc6\u522b\u4e86\u80fd\u591f\u4ea7\u751f\u5177\u6709\u53d9\u4e8b\u8fde\u8d2f\u6027\u3001\u9ad8\u4fdd\u771f\u7ec6\u8282\u548c\u591a\u89d2\u8272\u7279\u5f81\u7684\u957f\u89c6\u9891\u7684\u5173\u952e\u67b6\u6784\u7ec4\u4ef6\u548c\u8bad\u7ec3\u7b56\u7565\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u65b0\u5206\u7c7b\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u6839\u636e\u67b6\u6784\u8bbe\u8ba1\u548c\u6027\u80fd\u7279\u5f81\u5bf9\u8bba\u6587\u8fdb\u884c\u5206\u7c7b\u7684\u6bd4\u8f83\u8868\u683c\u3002", "result": "\u901a\u8fc7\u5bf9\u73b0\u6709\u7814\u7a76\u7684\u7efc\u5408\u5206\u6790\uff0c\u672c\u6587\u6210\u529f\u8bc6\u522b\u4e86\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u957f\u65f6\u3001\u591a\u89d2\u8272\u89c6\u9891\u7684\u5173\u952e\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u3002\u6210\u679c\u5305\u62ec\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u65b0\u5206\u7c7b\u6cd5\u548c\u8be6\u7ec6\u7684\u6bd4\u8f83\u8868\u683c\uff0c\u6e05\u6670\u5730\u5c55\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u6027\u80fd\u7279\u70b9\u3002", "conclusion": "\u672c\u6587\u4e3a\u89c6\u9891\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u7ed3\u6784\u5316\u5206\u6790\u548c\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u5b9e\u73b0\u957f\u65f6\u3001\u591a\u89d2\u8272\u3001\u9ad8\u4e00\u81f4\u6027\u89c6\u9891\u7684\u5173\u952e\u8981\u7d20\u3002\u8fd9\u9879\u5de5\u4f5c\u6709\u52a9\u4e8e\u6307\u5bfc\u672a\u6765\u7814\u7a76\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u89c6\u9891\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.07484", "pdf": "https://arxiv.org/pdf/2507.07484", "abs": "https://arxiv.org/abs/2507.07484", "authors": ["Kaiqu Liang", "Haimin Hu", "Xuandong Zhao", "Dawn Song", "Thomas L. Griffiths", "Jaime Fern\u00e1ndez Fisac"], "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page, code & data: https://machine-bullshit.github.io", "summary": "Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to\nstatements made without regard to their truth value. While previous work has\nexplored large language model (LLM) hallucination and sycophancy, we propose\nmachine bullshit as an overarching conceptual framework that can allow\nresearchers to characterize the broader phenomenon of emergent loss of\ntruthfulness in LLMs and shed light on its underlying mechanisms. We introduce\nthe Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and\npropose a complementary taxonomy analyzing four qualitative forms of bullshit:\nempty rhetoric, paltering, weasel words, and unverified claims. We conduct\nempirical evaluations on the Marketplace dataset, the Political Neutrality\ndataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI\nassistants) explicitly designed to evaluate machine bullshit. Our results\ndemonstrate that model fine-tuning with reinforcement learning from human\nfeedback (RLHF) significantly exacerbates bullshit and inference-time\nchain-of-thought (CoT) prompting notably amplify specific bullshit forms,\nparticularly empty rhetoric and paltering. We also observe prevalent machine\nbullshit in political contexts, with weasel words as the dominant strategy. Our\nfindings highlight systematic challenges in AI alignment and provide new\ninsights toward more truthful LLM behavior.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u201c\u673a\u5668\u80e1\u8a00\u4e71\u8bed\u201d\uff08machine bullshit\uff09\u4f5c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u771f\u5b9e\u6027\u4e27\u5931\u7684\u7edf\u4e00\u6982\u5ff5\u6846\u67b6\uff0c\u5f15\u5165\u4e86\u91cf\u5316\u6307\u6807\u201c\u80e1\u8a00\u4e71\u8bed\u6307\u6570\u201d\u548c\u56db\u79cd\u80e1\u8a00\u4e71\u8bed\u5f62\u5f0f\u7684\u5206\u7c7b\u3002\u7814\u7a76\u53d1\u73b0\uff0cRLHF\u5fae\u8c03\u548cCoT\u63d0\u793a\u4f1a\u663e\u8457\u52a0\u5267LLM\u7684\u80e1\u8a00\u4e71\u8bed\u73b0\u8c61\uff0c\u5c24\u5176\u5728\u653f\u6cbb\u8bed\u5883\u4e2d\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5df2\u63a2\u8ba8LLM\u7684\u5e7b\u89c9\u548c\u9022\u8fce\u884c\u4e3a\uff0c\u4f46\u7f3a\u4e4f\u4e00\u4e2a\u66f4\u5e7f\u6cdb\u7684\u3001\u80fd\u591f\u63cf\u8ff0LLM\u666e\u904d\u771f\u5b9e\u6027\u4e27\u5931\u5e76\u63ed\u793a\u5176\u5e95\u5c42\u673a\u5236\u7684\u6846\u67b6\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u201c\u673a\u5668\u80e1\u8a00\u4e71\u8bed\u201d\u8fd9\u4e00\u6982\u5ff5\u6846\u67b6\u3002", "method": "1. \u63d0\u51fa\u4e86\u201c\u673a\u5668\u80e1\u8a00\u4e71\u8bed\u201d\u4f5c\u4e3aLLM\u771f\u5b9e\u6027\u95ee\u9898\u7684\u603b\u62ec\u6027\u6982\u5ff5\u6846\u67b6\u30022. \u5f15\u5165\u4e86\u91cf\u5316LLM\u5bf9\u771f\u76f8\u6f20\u89c6\u7a0b\u5ea6\u7684\u65b0\u6307\u6807\u201c\u80e1\u8a00\u4e71\u8bed\u6307\u6570\u201d\uff08Bullshit Index\uff09\u30023. \u63d0\u51fa\u4e86\u80e1\u8a00\u4e71\u8bed\u7684\u56db\u79cd\u5b9a\u6027\u5f62\u5f0f\u5206\u7c7b\uff1a\u7a7a\u6d1e\u8a00\u8f9e\uff08empty rhetoric\uff09\u3001\u907f\u5b9e\u5c31\u865a\uff08paltering\uff09\u3001\u542b\u7cca\u5176\u8f9e\uff08weasel words\uff09\u548c\u672a\u7ecf\u8bc1\u5b9e\u7684\u4e3b\u5f20\uff08unverified claims\uff09\u30024. \u5728Marketplace\u3001Political Neutrality\u6570\u636e\u96c6\u548c\u65b0\u6784\u5efa\u7684BullshitEval\u57fa\u51c6\uff08\u5305\u542b2400\u4e2a\u573a\u666f\u548c100\u4e2aAI\u52a9\u624b\uff09\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "1. \u4f7f\u7528\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u8fdb\u884c\u6a21\u578b\u5fae\u8c03\u4f1a\u663e\u8457\u52a0\u5267\u80e1\u8a00\u4e71\u8bed\u73b0\u8c61\u30022. \u63a8\u7406\u65f6\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u63d0\u793a\u4f1a\u660e\u663e\u653e\u5927\u7279\u5b9a\u5f62\u5f0f\u7684\u80e1\u8a00\u4e71\u8bed\uff0c\u5c24\u5176\u662f\u7a7a\u6d1e\u8a00\u8f9e\u548c\u907f\u5b9e\u5c31\u865a\u30023. \u5728\u653f\u6cbb\u8bed\u5883\u4e2d\uff0c\u673a\u5668\u80e1\u8a00\u4e71\u8bed\u666e\u904d\u5b58\u5728\uff0c\u5176\u4e2d\u542b\u7cca\u5176\u8f9e\u662f\u4e3b\u8981\u7b56\u7565\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86AI\u5bf9\u9f50\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u6311\u6218\uff0c\u5e76\u4e3a\u5b9e\u73b0\u66f4\u771f\u5b9e\u7684LLM\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.07426", "pdf": "https://arxiv.org/pdf/2507.07426", "abs": "https://arxiv.org/abs/2507.07426", "authors": ["Zerui Yang", "Yuwei Wan", "Yinqiao Li", "Yudai Matsuda", "Tong Xie", "Linqi Song"], "title": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large language models have demonstrated considerable\npotential in scientific domains such as drug discovery. However, their\neffectiveness remains constrained when reasoning extends beyond the knowledge\nacquired during pretraining. Conventional approaches, such as fine-tuning or\nretrieval-augmented generation, face limitations in either imposing high\ncomputational overhead or failing to fully exploit structured scientific data.\nTo overcome these challenges, we propose DrugMCTS, a novel framework that\nsynergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree\nSearch for drug repurposing. The framework employs five specialized agents\ntasked with retrieving and analyzing molecular and protein information, thereby\nenabling structured and iterative reasoning. Without requiring domain-specific\nfine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by\nover 20\\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate\nthat DrugMCTS achieves substantially higher recall and robustness compared to\nboth general-purpose LLMs and deep learning baselines. Our results highlight\nthe importance of structured reasoning, agent-based collaboration, and\nfeedback-driven search mechanisms in advancing LLM applications for drug\ndiscovery.", "AI": {"tldr": "DrugMCTS\u662f\u4e00\u79cd\u878d\u5408RAG\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7684\u836f\u7269\u518d\u5229\u7528\u6846\u67b6\u3002\u5b83\u65e0\u9700\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u836f\u7269\u53d1\u73b0\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3001\u53ec\u56de\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u836f\u7269\u53d1\u73b0\u7b49\u79d1\u5b66\u9886\u57df\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5176\u63a8\u7406\u80fd\u529b\u53d7\u9650\u4e8e\u9884\u8bad\u7ec3\u77e5\u8bc6\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u5fae\u8c03\u6216\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u6216\u672a\u80fd\u5145\u5206\u5229\u7528\u7ed3\u6784\u5316\u79d1\u5b66\u6570\u636e\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faDrugMCTS\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u534f\u540c\u6574\u5408\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\uff0c\u4e13\u6ce8\u4e8e\u836f\u7269\u518d\u5229\u7528\u3002\u6846\u67b6\u5185\u90e8\u7f72\u4e86\u4e94\u4e2a\u4e13\u95e8\u7684\u667a\u80fd\u4f53\uff0c\u8d1f\u8d23\u68c0\u7d22\u548c\u5206\u6790\u5206\u5b50\u53ca\u86cb\u767d\u8d28\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u7ed3\u6784\u5316\u548c\u8fed\u4ee3\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u65e0\u9700\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff0cDrugMCTS\u80fd\u4f7fQwen2.5-7B-Instruct\u6a21\u578b\u6027\u80fd\u8d85\u8d8aDeepseek-R1\u8d85\u8fc720%\u3002\u5728DrugBank\u548cKIBA\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u901a\u7528LLMs\u548c\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u76f8\u6bd4\uff0cDrugMCTS\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u53ec\u56de\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u3001\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u4ee5\u53ca\u53cd\u9988\u9a71\u52a8\u7684\u641c\u7d22\u673a\u5236\u5728\u63a8\u8fdbLLM\u5e94\u7528\u4e8e\u836f\u7269\u53d1\u73b0\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002"}}
