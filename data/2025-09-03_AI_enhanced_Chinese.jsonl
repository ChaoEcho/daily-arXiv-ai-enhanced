{"id": "2509.00033", "pdf": "https://arxiv.org/pdf/2509.00033", "abs": "https://arxiv.org/abs/2509.00033", "authors": ["Tahoshin Alam Ishat"], "title": "Deep Learning-Driven Multimodal Detection and Movement Analysis of Objects in Culinary", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 9 figures", "summary": "This is a research exploring existing models and fine tuning them to combine\na YOLOv8 segmentation model, a LSTM model trained on hand point motion sequence\nand a ASR (whisper-base) to extract enough data for a LLM (TinyLLaMa) to\npredict the recipe and generate text creating a step by step guide for the\ncooking procedure. All the data were gathered by the author for a robust task\nspecific system to perform best in complex and challenging environments proving\nthe extension and endless application of computer vision in daily activities\nsuch as kitchen work. This work extends the field for many more crucial task of\nour day to day life.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7ed3\u5408YOLOv8\u3001LSTM\u548cASR\u6a21\u578b\u63d0\u53d6\u6570\u636e\uff0c\u8f93\u5165\u81f3LLM\uff08TinyLLaMa\uff09\u4ee5\u9884\u6d4b\u98df\u8c31\u5e76\u751f\u6210\u5206\u6b65\u70f9\u996a\u6307\u5357\uff0c\u5c55\u793a\u8ba1\u7b97\u673a\u89c6\u89c9\u5728\u65e5\u5e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u9c81\u68d2\u4e14\u7279\u5b9a\u4efb\u52a1\u7684\u7cfb\u7edf\uff0c\u4f7f\u5176\u5728\u590d\u6742\u548c\u6311\u6218\u6027\u73af\u5883\u4e2d\u8868\u73b0\u6700\u4f73\u3002\u540c\u65f6\uff0c\u9a8c\u8bc1\u8ba1\u7b97\u673a\u89c6\u89c9\u5728\u65e5\u5e38\u6d3b\u52a8\uff08\u5982\u53a8\u623f\u5de5\u4f5c\uff09\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u548c\u65e0\u9650\u6269\u5c55\u6027\u3002", "method": "\u8be5\u7814\u7a76\u63a2\u7d22\u5e76\u5fae\u8c03\u4e86\u73b0\u6709\u6a21\u578b\uff0c\u5305\u62ecYOLOv8\u5206\u5272\u6a21\u578b\u3001\u57fa\u4e8e\u624b\u90e8\u70b9\u8fd0\u52a8\u5e8f\u5217\u8bad\u7ec3\u7684LSTM\u6a21\u578b\u4ee5\u53caASR\uff08Whisper-base\uff09\u6a21\u578b\u3002\u8fd9\u4e9b\u6a21\u578b\u88ab\u7528\u4e8e\u63d0\u53d6\u8db3\u591f\u6570\u636e\uff0c\u4ee5\u4f9b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08TinyLLaMa\uff09\u9884\u6d4b\u98df\u8c31\u5e76\u751f\u6210\u5206\u6b65\u70f9\u996a\u6307\u5357\u3002\u6240\u6709\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u5747\u7531\u4f5c\u8005\u81ea\u884c\u6536\u96c6\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u7cfb\u7edf\uff0c\u80fd\u591f\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u8f93\u5165\uff08\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u624b\u52bf\u3001\u8bed\u97f3\uff09\u4f7fLLM\uff08TinyLLaMa\uff09\u9884\u6d4b\u98df\u8c31\u5e76\u751f\u6210\u8be6\u7ec6\u7684\u70f9\u996a\u6b65\u9aa4\u6307\u5357\u3002\u8fd9\u8bc1\u660e\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u5728\u65e5\u5e38\u6d3b\u52a8\uff08\u5982\u53a8\u623f\u5de5\u4f5c\uff09\u4e2d\u7684\u6709\u6548\u6269\u5c55\u548c\u5e7f\u6cdb\u5e94\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u6709\u6548\u5c55\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u5728\u7ed3\u5408\u5176\u4ed6AI\u6a21\u578b\u540e\uff0c\u5728\u5904\u7406\u65e5\u5e38\u590d\u6742\u4efb\u52a1\uff08\u5982\u53a8\u623f\u70f9\u996a\uff09\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u5e76\u4e3a\u5c06\u6b64\u6280\u672f\u6269\u5c55\u5230\u66f4\u591a\u65e5\u5e38\u5173\u952e\u4efb\u52a1\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.00039", "pdf": "https://arxiv.org/pdf/2509.00039", "abs": "https://arxiv.org/abs/2509.00039", "authors": ["Yuqi Li", "Chuanguang Yang", "Junhao Dong", "Zhengtao Yao", "Haoyan Xu", "Zeyu Dong", "Hansheng Zeng", "Zhulin An", "Yingli Tian"], "title": "AMMKD: Adaptive Multimodal Multi-teacher Distillation for Lightweight Vision-Language Models", "categories": ["cs.CV"], "comment": "9 pages", "summary": "The success of large-scale visual language pretraining (VLP) models has\ndriven widespread adoption of image-text retrieval tasks. However, their\ndeployment on mobile devices remains limited due to large model sizes and\ncomputational complexity. We propose Adaptive Multi-Modal Multi-Teacher\nKnowledge Distillation (AMMKD), a novel framework that integrates multi-modal\nfeature fusion, multi-teacher distillation, and adaptive optimization to\ndeliver lightweight yet effective retrieval models. Specifically, our method\nbegins with a feature fusion network that extracts and merges discriminative\nfeatures from both the image and text modalities. To reduce model parameters\nand further improve performance, we design a multi-teacher knowledge\ndistillation framework to pre-train two CLIP teacher models. We decouple\nmodalities by pre-computing and storing text features as class vectors via the\nteacher text encoder to enhance efficiency. To better align teacher and student\noutputs, we apply KL scatter for probability distribution matching. Finally, we\ndesign an adaptive dynamic weighting scheme that treats multi-teacher\ndistillation as a multi-objective optimization problem. By leveraging gradient\nspace diversity, we dynamically adjust the influence of each teacher, reducing\nconflicts and guiding the student toward more optimal learning directions.\nExtensive experiments on three benchmark datasets demonstrate that AMMKD\nachieves superior performance while significantly reducing model complexity,\nvalidating its effectiveness and flexibility.", "AI": {"tldr": "\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u53d7\u9650\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86AMMKD\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u3001\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u548c\u81ea\u9002\u5e94\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6027\u80fd\u7684\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\uff08VLP\uff09\u6a21\u578b\u5728\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u6a21\u578b\u5e9e\u5927\u548c\u8ba1\u7b97\u590d\u6742\u6027\u9ad8\uff0c\u5176\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u53d7\u5230\u9650\u5236\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u800c\u6709\u6548\u7684\u68c0\u7d22\u6a21\u578b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u81ea\u9002\u5e94\u591a\u6a21\u6001\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\uff08AMMKD\uff09\u6846\u67b6\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1) \u4f7f\u7528\u7279\u5f81\u878d\u5408\u7f51\u7edc\u63d0\u53d6\u5e76\u878d\u5408\u56fe\u50cf\u548c\u6587\u672c\u6a21\u6001\u7684\u5224\u522b\u6027\u7279\u5f81\u30022) \u8bbe\u8ba1\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5229\u7528\u4e24\u4e2a\u9884\u8bad\u7ec3\u7684CLIP\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u84b8\u998f\uff0c\u5e76\u901a\u8fc7\u9884\u8ba1\u7b97\u5e76\u5b58\u50a8\u6587\u672c\u7279\u5f81\u4f5c\u4e3a\u7c7b\u522b\u5411\u91cf\u6765\u89e3\u8026\u6a21\u6001\u4ee5\u63d0\u9ad8\u6548\u7387\u30023) \u5e94\u7528KL\u6563\u5ea6\u8fdb\u884c\u6982\u7387\u5206\u5e03\u5339\u914d\uff0c\u4ee5\u66f4\u597d\u5730\u5bf9\u9f50\u6559\u5e08\u548c\u5b66\u751f\u7684\u8f93\u51fa\u30024) \u8bbe\u8ba1\u81ea\u9002\u5e94\u52a8\u6001\u52a0\u6743\u65b9\u6848\uff0c\u5c06\u591a\u6559\u5e08\u84b8\u998f\u89c6\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u68af\u5ea6\u7a7a\u95f4\u591a\u6837\u6027\u52a8\u6001\u8c03\u6574\u6bcf\u4e2a\u6559\u5e08\u7684\u5f71\u54cd\uff0c\u4ee5\u51cf\u5c11\u51b2\u7a81\u5e76\u5f15\u5bfc\u5b66\u751f\u5b66\u4e60\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAMMKD\u5728\u663e\u8457\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "AMMKD\u662f\u4e00\u4e2a\u6709\u6548\u4e14\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4e3a\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u800c\u9ad8\u6027\u80fd\u7684\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5927\u578bVLP\u6a21\u578b\u5728\u79fb\u52a8\u8bbe\u5907\u90e8\u7f72\u4e0a\u7684\u6311\u6218\u3002"}}
{"id": "2509.00042", "pdf": "https://arxiv.org/pdf/2509.00042", "abs": "https://arxiv.org/abs/2509.00042", "authors": ["Poyraz Baydemir"], "title": "ARTPS: Depth-Enhanced Hybrid Anomaly Detection and Learnable Curiosity Score for Autonomous Rover Target Prioritization", "categories": ["cs.CV", "cs.AI", "68T45, 68T07, 68U10", "I.2.10; I.4.8; I.5.4; J.2"], "comment": "18 pages, 12 figures, 4 table, autonomous exploration, Mars rover,\n  computer vision, anomaly detection, depth estimation, curiosity-driven\n  exploration", "summary": "We present ARTPS (Autonomous Rover Target Prioritization System), a novel\nhybrid AI system that combines depth estimation, anomaly detection, and\nlearnable curiosity scoring for autonomous exploration of planetary surfaces.\nOur approach integrates monocular depth estimation using Vision Transformers\nwith multi-component anomaly detection and a weighted curiosity score that\nbalances known value, anomaly signals, depth variance, and surface roughness.\nThe system achieves state-of-the-art performance with AUROC of 0.94, AUPRC of\n0.89, and F1-Score of 0.87 on Mars rover datasets. We demonstrate significant\nimprovements in target prioritization accuracy through ablation studies and\nprovide comprehensive analysis of component contributions. The hybrid fusion\napproach reduces false positives by 23% while maintaining high detection\nsensitivity across diverse terrain types.", "AI": {"tldr": "\u63d0\u51faARTPS\uff0c\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u4f30\u8ba1\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u53ef\u5b66\u4e60\u597d\u5947\u5fc3\u8bc4\u5206\u7684\u6df7\u5408AI\u7cfb\u7edf\uff0c\u7528\u4e8e\u884c\u661f\u8868\u9762\u81ea\u4e3b\u63a2\u7d22\uff0c\u5e76\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u884c\u661f\u8868\u9762\u81ea\u4e3b\u63a2\u7d22\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u7684\u76ee\u6807\u4f18\u5148\u7ea7\u6392\u5e8f\u7cfb\u7edf\uff0c\u4ee5\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u548c\u5e94\u5bf9\u590d\u6742\u73af\u5883\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86ARTPS\uff08\u81ea\u4e3b\u6f2b\u6e38\u8f66\u76ee\u6807\u4f18\u5148\u7ea7\u7cfb\u7edf\uff09\uff0c\u4e00\u4e2a\u6df7\u5408AI\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u6574\u5408\u4e86\u4f7f\u7528Vision Transformers\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3001\u591a\u7ec4\u4ef6\u5f02\u5e38\u68c0\u6d4b\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5e73\u8861\u5df2\u77e5\u4ef7\u503c\u3001\u5f02\u5e38\u4fe1\u53f7\u3001\u6df1\u5ea6\u65b9\u5dee\u548c\u8868\u9762\u7c97\u7cd9\u5ea6\u7684\u52a0\u6743\u597d\u5947\u5fc3\u8bc4\u5206\u3002\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u6765\u5206\u6790\u5404\u7ec4\u4ef6\u7684\u8d21\u732e\u3002", "result": "\u5728\u706b\u661f\u63a2\u6d4b\u5668\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e860.94\u7684AUROC\u30010.89\u7684AUPRC\u548c0.87\u7684F1\u5206\u6570\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6df7\u5408\u878d\u5408\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u68c0\u6d4b\u7075\u654f\u5ea6\u7684\u540c\u65f6\uff0c\u5c06\u5047\u9633\u6027\u51cf\u5c11\u4e8623%\u3002", "conclusion": "ARTPS\u7684\u6df7\u5408\u878d\u5408\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u76ee\u6807\u4f18\u5148\u7ea7\u6392\u5e8f\u7684\u51c6\u786e\u6027\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5047\u9633\u6027\uff0c\u5e76\u5728\u591a\u6837\u5316\u7684\u884c\u661f\u5730\u5f62\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u63a2\u6d4b\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.00045", "pdf": "https://arxiv.org/pdf/2509.00045", "abs": "https://arxiv.org/abs/2509.00045", "authors": ["Xiang Li", "Chong Zhang", "Hongpeng Wang", "Shreyank Narayana Gowda", "Yushi Li", "Xiaobo Jin"], "title": "Performance is not All You Need: Sustainability Considerations for Algorithms", "categories": ["cs.CV", "cs.PF"], "comment": "18 pages, 6 figures. Accepted Chinese Conference on Pattern\n  Recognition and Computer Vision 2025", "summary": "This work focuses on the high carbon emissions generated by deep learning\nmodel training, specifically addressing the core challenge of balancing\nalgorithm performance and energy consumption. It proposes an innovative\ntwo-dimensional sustainability evaluation system. Different from the\ntraditional single performance-oriented evaluation paradigm, this study\npioneered two quantitative indicators that integrate energy efficiency ratio\nand accuracy: the sustainable harmonic mean (FMS) integrates accumulated energy\nconsumption and performance parameters through the harmonic mean to reveal the\nalgorithm performance under unit energy consumption; the area under the\nsustainability curve (ASC) constructs a performance-power consumption curve to\ncharacterize the energy efficiency characteristics of the algorithm throughout\nthe cycle. To verify the universality of the indicator system, the study\nconstructed benchmarks in various multimodal tasks, including image\nclassification, segmentation, pose estimation, and batch and online learning.\nExperiments demonstrate that the system can provide a quantitative basis for\nevaluating cross-task algorithms and promote the transition of green AI\nresearch from theory to practice. Our sustainability evaluation framework code\ncan be found here, providing methodological support for the industry to\nestablish algorithm energy efficiency standards.", "AI": {"tldr": "\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60\u9ad8\u80fd\u8017\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u521b\u65b0\u7684\u4e8c\u7ef4\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u7cfb\u7edf\uff0c\u5305\u542bFMS\u548cASC\u4e24\u4e2a\u6307\u6807\uff0c\u65e8\u5728\u91cf\u5316\u5e73\u8861\u7b97\u6cd5\u6027\u80fd\u4e0e\u80fd\u8017\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u591a\u6a21\u6001\u4efb\u52a1\u9a8c\u8bc1\u4e86\u5176\u901a\u7528\u6027\uff0c\u4e3a\u7eff\u8272AI\u7814\u7a76\u548c\u884c\u4e1a\u80fd\u6548\u6807\u51c6\u5236\u5b9a\u63d0\u4f9b\u4e86\u91cf\u5316\u4f9d\u636e\u548c\u65b9\u6cd5\u8bba\u652f\u6301\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u4ea7\u751f\u5927\u91cf\u78b3\u6392\u653e\uff0c\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u5982\u4f55\u5728\u7b97\u6cd5\u6027\u80fd\u548c\u80fd\u6e90\u6d88\u8017\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u91cf\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u521b\u65b0\u7684\u4e8c\u7ef4\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u7cfb\u7edf\uff0c\u5305\u542b\u4e24\u4e2a\u91cf\u5316\u6307\u6807\uff1a1) \u53ef\u6301\u7eed\u8c03\u548c\u5e73\u5747\u503c\uff08FMS\uff09\uff0c\u901a\u8fc7\u8c03\u548c\u5e73\u5747\u6570\u6574\u5408\u7d2f\u79ef\u80fd\u8017\u548c\u6027\u80fd\u53c2\u6570\uff0c\u63ed\u793a\u5355\u4f4d\u80fd\u8017\u4e0b\u7684\u7b97\u6cd5\u8868\u73b0\uff1b2) \u53ef\u6301\u7eed\u6027\u66f2\u7ebf\u4e0b\u9762\u79ef\uff08ASC\uff09\uff0c\u6784\u5efa\u6027\u80fd-\u529f\u8017\u66f2\u7ebf\uff0c\u8868\u5f81\u7b97\u6cd5\u5168\u5468\u671f\u7684\u80fd\u6548\u7279\u6027\u3002\u4e3a\u9a8c\u8bc1\u901a\u7528\u6027\uff0c\u7814\u7a76\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u5206\u5272\u3001\u59ff\u6001\u4f30\u8ba1\u7b49\u591a\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u4ee5\u53ca\u6279\u91cf\u548c\u5728\u7ebf\u5b66\u4e60\u4e2d\u6784\u5efa\u4e86\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u4e3a\u8de8\u4efb\u52a1\u7b97\u6cd5\u8bc4\u4f30\u63d0\u4f9b\u91cf\u5316\u4f9d\u636e\u3002", "conclusion": "\u8be5\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u7cfb\u7edf\u80fd\u6709\u6548\u63a8\u52a8\u7eff\u8272AI\u7814\u7a76\u4ece\u7406\u8bba\u8d70\u5411\u5b9e\u8df5\uff0c\u5e76\u4e3a\u884c\u4e1a\u5efa\u7acb\u7b97\u6cd5\u80fd\u6548\u6807\u51c6\u63d0\u4f9b\u65b9\u6cd5\u8bba\u652f\u6301\u3002"}}
{"id": "2509.00058", "pdf": "https://arxiv.org/pdf/2509.00058", "abs": "https://arxiv.org/abs/2509.00058", "authors": ["Eric Zhang", "Li Wei", "Sarah Chen", "Michael Wang"], "title": "A Comparative Study of Controllability, Explainability, and Performance in Dysfluency Detection Models", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in dysfluency detection have introduced a variety of modeling\nparadigms, ranging from lightweight object-detection inspired networks\n(YOLOStutter) to modular interpretable frameworks (UDM). While performance on\nbenchmark datasets continues to improve, clinical adoption requires more than\naccuracy: models must be controllable and explainable. In this paper, we\npresent a systematic comparative analysis of four representative\napproaches--YOLO-Stutter, FluentNet, UDM, and SSDM--along three dimensions:\nperformance, controllability, and explainability. Through comprehensive\nevaluation on multiple datasets and expert clinician assessment, we find that\nYOLO-Stutter and FluentNet provide efficiency and simplicity, but with limited\ntransparency; UDM achieves the best balance of accuracy and clinical\ninterpretability; and SSDM, while promising, could not be fully reproduced in\nour experiments. Our analysis highlights the trade-offs among competing\napproaches and identifies future directions for clinically viable dysfluency\nmodeling. We also provide detailed implementation insights and practical\ndeployment considerations for each approach.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u56db\u79cd\u53e3\u5403\u68c0\u6d4b\u6a21\u578b\uff0c\u8bc4\u4f30\u5176\u6027\u80fd\u3001\u53ef\u63a7\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u53d1\u73b0UDM\u5728\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u4e0a\u5e73\u8861\u6700\u4f73\u3002", "motivation": "\u5c3d\u7ba1\u53e3\u5403\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u6301\u7eed\u63d0\u5347\uff0c\u4f46\u4e34\u5e8a\u5e94\u7528\u4e0d\u4ec5\u9700\u8981\u51c6\u786e\u6027\uff0c\u8fd8\u9700\u6a21\u578b\u5177\u6709\u53ef\u63a7\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u73b0\u6709\u6a21\u578b\u5728\u6b64\u65b9\u9762\u5404\u6709\u4fa7\u91cd\u3002", "method": "\u5bf9YOLO-Stutter\u3001FluentNet\u3001UDM\u548cSSDM\u56db\u79cd\u4ee3\u8868\u6027\u53e3\u5403\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u6bd4\u8f83\u5206\u6790\uff0c\u8bc4\u4f30\u7ef4\u5ea6\u5305\u62ec\u6027\u80fd\u3001\u53ef\u63a7\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u8bc4\u4f30\u901a\u8fc7\u591a\u6570\u636e\u96c6\u548c\u4e34\u5e8a\u4e13\u5bb6\u8bc4\u5ba1\u5b8c\u6210\u3002", "result": "\u7814\u7a76\u53d1\u73b0YOLO-Stutter\u548cFluentNet\u9ad8\u6548\u7b80\u5355\u4f46\u900f\u660e\u5ea6\u6709\u9650\uff1bUDM\u5728\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\uff1bSSDM\u867d\u6709\u524d\u666f\u4f46\u5728\u5b9e\u9a8c\u4e2d\u672a\u80fd\u5b8c\u5168\u590d\u73b0\u3002", "conclusion": "\u8be5\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u95f4\u7684\u6743\u8861\uff0c\u5e76\u4e3a\u4e34\u5e8a\u53ef\u884c\u7684\u53e3\u5403\u5efa\u6a21\u6307\u660e\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u5404\u65b9\u6cd5\u7684\u5b9e\u73b0\u7ec6\u8282\u548c\u5b9e\u9645\u90e8\u7f72\u8003\u91cf\u3002"}}
{"id": "2509.00026", "pdf": "https://arxiv.org/pdf/2509.00026", "abs": "https://arxiv.org/abs/2509.00026", "authors": ["Abu Shad Ahammed", "Sayeri Mukherjee", "Roman Obermaisser"], "title": "Diagnosing Psychiatric Patients: Can Large Language and Machine Learning Models Perform Effectively in Emergency Cases?", "categories": ["cs.LG", "cs.CY"], "comment": null, "summary": "Mental disorders are clinically significant patterns of behavior that are\nassociated with stress and/or impairment in social, occupational, or family\nactivities. People suffering from such disorders are often misjudged and poorly\ndiagnosed due to a lack of visible symptoms compared to other health\ncomplications. During emergency situations, identifying psychiatric issues is\nthat's why challenging but highly required to save patients. In this paper, we\nhave conducted research on how traditional machine learning and large language\nmodels (LLM) can assess these psychiatric patients based on their behavioral\npatterns to provide a diagnostic assessment. Data from emergency psychiatric\npatients were collected from a rescue station in Germany. Various machine\nlearning models, including Llama 3.1, were used with rescue patient data to\nassess if the predictive capabilities of the models can serve as an efficient\ntool for identifying patients with unhealthy mental disorders, especially in\nrescue cases.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5fb7\u56fd\u6551\u63f4\u7ad9\u7684\u6025\u8bca\u7cbe\u795e\u75c5\u60a3\u8005\u884c\u4e3a\u6570\u636e\uff0c\u8bc4\u4f30\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982Llama 3.1\uff09\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u8bca\u65ad\u7cbe\u795e\u969c\u788d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7cbe\u795e\u969c\u788d\u56e0\u7f3a\u4e4f\u53ef\u89c1\u75c7\u72b6\u5e38\u88ab\u8bef\u5224\u548c\u8bca\u65ad\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u8bc6\u522b\u81f3\u5173\u91cd\u8981\u4f46\u6781\u5177\u6311\u6218\u6027\uff0c\u53ef\u80fd\u5371\u53ca\u60a3\u8005\u751f\u547d\u3002", "method": "\u7814\u7a76\u6536\u96c6\u4e86\u6765\u81ea\u5fb7\u56fd\u6551\u63f4\u7ad9\u7684\u6025\u8bca\u7cbe\u795e\u75c5\u60a3\u8005\u6570\u636e\u3002\u4f7f\u7528\u5305\u62ecLlama 3.1\u5728\u5185\u7684\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5206\u6790\u60a3\u8005\u884c\u4e3a\u6a21\u5f0f\uff0c\u65e8\u5728\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u662f\u5426\u80fd\u6709\u6548\u8bc6\u522b\u60a3\u6709\u7cbe\u795e\u969c\u788d\u7684\u60a3\u8005\u3002", "result": "\u62bd\u8c61\u4e2d\u672a\u660e\u786e\u7ed9\u51fa\u5177\u4f53\u7814\u7a76\u7ed3\u679c\uff0c\u4ec5\u8868\u660e\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5305\u62ecLlama 3.1\uff09\u7684\u9884\u6d4b\u80fd\u529b\u662f\u5426\u80fd\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u5de5\u5177\uff0c\u7528\u4e8e\u8bc6\u522b\u60a3\u6709\u4e0d\u5065\u5eb7\u7cbe\u795e\u969c\u788d\u7684\u6025\u8bca\u60a3\u8005\uff0c\u5c24\u5176\u662f\u5728\u6551\u63f4\u6848\u4f8b\u4e2d\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u884c\u4e3a\u6a21\u5f0f\u6570\u636e\uff0c\u4e3a\u7cbe\u795e\u969c\u788d\u60a3\u8005\u63d0\u4f9b\u8bca\u65ad\u8bc4\u4f30\uff0c\u4ee5\u671f\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u5f00\u53d1\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u8bc6\u522b\u5de5\u5177\uff0c\u6539\u5584\u8bca\u65ad\u56f0\u5883\u3002"}}
{"id": "2509.00030", "pdf": "https://arxiv.org/pdf/2509.00030", "abs": "https://arxiv.org/abs/2509.00030", "authors": ["Marshall Thomas", "Edward Fish", "Richard Bowden"], "title": "MultiStream-LLM: Bridging Modalities for Robust Sign Language Translation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Despite progress in gloss-free Sign Language Translation (SLT), monolithic\nend-to-end models consistently fail on two critical components of natural\nsigning: the precise recognition of high-speed fingerspelling and the\nintegration of asynchronous non-manual cues from the face. Recent progress in\nAutomated Sign Language Translation with Large Language Models has side stepped\nthis challenge, forcing a single network to learn these simultaneously\nresulting in poor performance when tasked with translating crucial information\nsuch as names,places, and technical terms. We introduce MultiStream-LLM, a\nmodular framework designed to overcome these limitations. Our approach employs\nseparate, specialized predictors for continuous signing, fingerspelling, and\nlipreading. Each expert network first decodes its specific modality into a\nsequence of tokens. These parallel streams are then fused by a lightweight\ntransformer that resolves temporal misalignments before passing the combined\nrepresentation to a Large Language Model (LLM) for final sentence generation.\nOur method establishes a new state-of-the-art on the How2Sign benchmark with a\nBLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging\nChicagoFSWildPlus fingerspelling dataset. These results validate our core\nhypothesis: by isolating and solving distinct recogni tion tasks before fusion,\nour multi-expert approach provides a more powerful and effective pathway to\nrobust, high-fidelity sign language translation.", "AI": {"tldr": "\u73b0\u6709\u624b\u8bed\u7ffb\u8bd1\u6a21\u578b\u96be\u4ee5\u5904\u7406\u9ad8\u901f\u624b\u6307\u62fc\u5199\u548c\u5f02\u6b65\u9762\u90e8\u7ebf\u7d22\u3002\u672c\u6587\u63d0\u51faMultiStream-LLM\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u5e76\u4f7f\u7528\u4e13\u95e8\u9884\u6d4b\u5668\u5904\u7406\u4e0d\u540c\u6a21\u6001\uff08\u8fde\u7eed\u624b\u8bed\u3001\u624b\u6307\u62fc\u5199\u3001\u5507\u8bfb\uff09\uff0c\u518d\u901a\u8fc7Transformer\u878d\u5408\u5e76\u8f93\u5165LLM\u8fdb\u884c\u751f\u6210\uff0c\u5728How2Sign\u57fa\u51c6\u6d4b\u8bd5\u548c\u624b\u6307\u62fc\u5199\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u65e0\u5149\u6cfd\u624b\u8bed\u7ffb\u8bd1\uff08SLT\uff09\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u5355\u4e00\u540c\u6784\u7684\u7aef\u5230\u7aef\u6a21\u578b\u5728\u8bc6\u522b\u9ad8\u901f\u624b\u6307\u62fc\u5199\u548c\u6574\u5408\u5f02\u6b65\u975e\u624b\u52a8\u9762\u90e8\u7ebf\u7d22\u8fd9\u4e24\u4e2a\u5173\u952e\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u5728\u7ffb\u8bd1\u59d3\u540d\u3001\u5730\u70b9\u548c\u6280\u672f\u672f\u8bed\u7b49\u5173\u952e\u4fe1\u606f\u65f6\u6027\u80fd\u4f4e\u4e0b\u3002", "method": "\u5f15\u5165MultiStream-LLM\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u72ec\u7acb\u7684\u3001\u4e13\u95e8\u7684\u9884\u6d4b\u5668\u5206\u522b\u5904\u7406\u8fde\u7eed\u624b\u8bed\u3001\u624b\u6307\u62fc\u5199\u548c\u5507\u8bfb\u3002\u6bcf\u4e2a\u4e13\u5bb6\u7f51\u7edc\u9996\u5148\u5c06\u5176\u7279\u5b9a\u6a21\u6001\u89e3\u7801\u4e3atoken\u5e8f\u5217\u3002\u7136\u540e\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7Transformer\u878d\u5408\u8fd9\u4e9b\u5e76\u884c\u6d41\u4ee5\u89e3\u51b3\u65f6\u95f4\u9519\u4f4d\uff0c\u5e76\u5c06\u5408\u5e76\u540e\u7684\u8868\u793a\u4f20\u9012\u7ed9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u6700\u7ec8\u53e5\u5b50\u751f\u6210\u3002", "result": "\u5728How2Sign\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e8623.5\u7684BLEU-4\u5206\u6570\uff0c\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u8bb0\u5f55\u3002\u5728\u5177\u6709\u6311\u6218\u6027\u7684ChicagoFSWildPlus\u624b\u6307\u62fc\u5199\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u73b0\u4e8673.2%\u7684\u5b57\u6bcd\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u5728\u878d\u5408\u4e4b\u524d\u9694\u79bb\u5e76\u89e3\u51b3\u4e0d\u540c\u7684\u8bc6\u522b\u4efb\u52a1\uff0c\u672c\u6587\u63d0\u51fa\u7684\u591a\u4e13\u5bb6\u65b9\u6cd5\u4e3a\u5b9e\u73b0\u7a33\u5065\u3001\u9ad8\u4fdd\u771f\u624b\u8bed\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u4e00\u6761\u66f4\u5f3a\u5927\u548c\u6709\u6548\u7684\u9014\u5f84\uff0c\u9a8c\u8bc1\u4e86\u5176\u6838\u5fc3\u5047\u8bbe\u3002"}}
{"id": "2509.00056", "pdf": "https://arxiv.org/pdf/2509.00056", "abs": "https://arxiv.org/abs/2509.00056", "authors": ["Luu Tu Nguyen", "Vu Tram Anh Khuong", "Thanh Ha Le", "Thi Duyen Ngo"], "title": "MESTI-MEGANet: Micro-expression Spatio-Temporal Image and Micro-expression Gradient Attention Networks for Micro-expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Micro-expression recognition (MER) is a challenging task due to the subtle\nand fleeting nature of micro-expressions. Traditional input modalities, such as\nApex Frame, Optical Flow, and Dynamic Image, often fail to adequately capture\nthese brief facial movements, resulting in suboptimal performance. In this\nstudy, we introduce the Micro-expression Spatio-Temporal Image (MESTI), a novel\ndynamic input modality that transforms a video sequence into a single image\nwhile preserving the essential characteristics of micro-movements.\nAdditionally, we present the Micro-expression Gradient Attention Network\n(MEGANet), which incorporates a novel Gradient Attention block to enhance the\nextraction of fine-grained motion features from micro-expressions. By combining\nMESTI and MEGANet, we aim to establish a more effective approach to MER.\nExtensive experiments were conducted to evaluate the effectiveness of MESTI,\ncomparing it with existing input modalities across three CNN architectures\n(VGG19, ResNet50, and EfficientNetB0). Moreover, we demonstrate that replacing\nthe input of previously published MER networks with MESTI leads to consistent\nperformance improvements. The performance of MEGANet, both with MESTI and\nDynamic Image, is also evaluated, showing that our proposed network achieves\nstate-of-the-art results on the CASMEII and SAMM datasets. The combination of\nMEGANet and MESTI achieves the highest accuracy reported to date, setting a new\nbenchmark for micro-expression recognition. These findings underscore the\npotential of MESTI as a superior input modality and MEGANet as an advanced\nrecognition network, paving the way for more effective MER systems in a variety\nof applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5fae\u8868\u60c5\u65f6\u7a7a\u56fe\u50cf\uff08MESTI\uff09\u8f93\u5165\u6a21\u6001\u548c\u5fae\u8868\u60c5\u68af\u5ea6\u6ce8\u610f\u529b\u7f51\u7edc\uff08MEGANet\uff09\uff0c\u7ed3\u5408\u4e24\u8005\u663e\u8457\u63d0\u5347\u4e86\u5fae\u8868\u60c5\u8bc6\u522b\uff08MER\uff09\u7684\u6027\u80fd\uff0c\u5e76\u5728CASMEII\u548cSAMM\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "motivation": "\u5fae\u8868\u60c5\u8bc6\u522b\uff08MER\uff09\u56e0\u5176\u7ec6\u5fae\u548c\u77ed\u6682\u7684\u7279\u6027\u800c\u6781\u5177\u6311\u6218\u6027\u3002\u4f20\u7edf\u7684\u8f93\u5165\u6a21\u6001\uff08\u5982Apex Frame\u3001\u5149\u6d41\u3001\u52a8\u6001\u56fe\u50cf\uff09\u672a\u80fd\u6709\u6548\u6355\u6349\u8fd9\u4e9b\u77ed\u6682\u7684\u9762\u90e8\u8fd0\u52a8\uff0c\u5bfc\u81f4\u8bc6\u522b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u5fae\u8868\u60c5\u65f6\u7a7a\u56fe\u50cf\uff08MESTI\uff09\uff0c\u4e00\u79cd\u5c06\u89c6\u9891\u5e8f\u5217\u8f6c\u6362\u4e3a\u5355\u4e00\u56fe\u50cf\uff0c\u540c\u65f6\u4fdd\u7559\u5fae\u52a8\u4f5c\u6838\u5fc3\u7279\u5f81\u7684\u65b0\u578b\u52a8\u6001\u8f93\u5165\u6a21\u6001\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u5fae\u8868\u60c5\u68af\u5ea6\u6ce8\u610f\u529b\u7f51\u7edc\uff08MEGANet\uff09\uff0c\u8be5\u7f51\u7edc\u5305\u542b\u4e00\u4e2a\u65b0\u9896\u7684\u68af\u5ea6\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u7528\u4e8e\u589e\u5f3a\u5fae\u8868\u60c5\u7684\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u7279\u5f81\u63d0\u53d6\u3002\u901a\u8fc7\u7ed3\u5408MESTI\u548cMEGANet\u6765\u5efa\u7acb\u66f4\u6709\u6548\u7684MER\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0cMESTI\u5728\u4e0e\u73b0\u6709\u8f93\u5165\u6a21\u6001\u8fdb\u884c\u6bd4\u8f83\u65f6\uff0c\u5728\u4e09\u79cdCNN\u67b6\u6784\uff08VGG19\u3001ResNet50\u548cEfficientNetB0\uff09\u4e0b\u5747\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u5c06MESTI\u5e94\u7528\u4e8e\u5df2\u53d1\u5e03\u7684MER\u7f51\u7edc\u8f93\u5165\uff0c\u80fd\u5e26\u6765\u4e00\u81f4\u6027\u7684\u6027\u80fd\u63d0\u5347\u3002MEGANet\uff08\u65e0\u8bba\u7ed3\u5408MESTI\u8fd8\u662f\u52a8\u6001\u56fe\u50cf\uff09\u5728CASMEII\u548cSAMM\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u8bc6\u522b\u7ed3\u679c\u3002MEGANet\u4e0eMESTI\u7684\u7ec4\u5408\u8fbe\u5230\u4e86\u8fc4\u4eca\u4e3a\u6b62\u62a5\u544a\u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u4e3a\u5fae\u8868\u60c5\u8bc6\u522b\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "MESTI\u4f5c\u4e3a\u4e00\u79cd\u5353\u8d8a\u7684\u8f93\u5165\u6a21\u6001\uff0c\u4ee5\u53caMEGANet\u4f5c\u4e3a\u4e00\u79cd\u5148\u8fdb\u7684\u8bc6\u522b\u7f51\u7edc\uff0c\u5176\u6f5c\u529b\u5f97\u5230\u4e86\u5145\u5206\u8bc1\u5b9e\u3002\u4e24\u8005\u7684\u7ed3\u5408\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u5fae\u8868\u60c5\u8bc6\u522b\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u5177\u6709\u5e7f\u9614\u524d\u666f\u3002"}}
{"id": "2509.00072", "pdf": "https://arxiv.org/pdf/2509.00072", "abs": "https://arxiv.org/abs/2509.00072", "authors": ["Terry Jingchen Zhang", "Gopal Dev", "Ning Wang", "Nicole Ni", "Wenyuan Jiang", "Yinya Huang", "Bernhard Sch\u00f6lkopf", "Mrinmaya Sachan", "Zhijing Jin"], "title": "Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy Against Benchmark Contamination", "categories": ["cs.AI"], "comment": "Code and Dataset: https://github.com/TerryJCZhang/BeyondMemorization", "summary": "Capability evaluation of large language models (LLMs) is increasingly\nshadowed by rising concerns of data contamination that cast doubts on whether\nstatic benchmarks measure genuine reasoning or mere memorization. We present an\nempirical study using an infinitely scalable framework to synthesize\nresearch-level QA directly from arXiv papers, harnessing the natural temporal\nstructure of research publications where performance decay after knowledge\ncutoffs may indicate potential contamination. We evaluated 4 frontier model\nrepresented by 2 models of different knowledge cutoff dates per family on 1,643\nmulti-step reasoning questions synthesized from 20,277 arXiv papers stratified\nover 26 months, covering at least 6 months before and after all cutoff dates.\nOur results consistently showed a lack of significant performance decay near\nknowledge cutoff dates for models of various sizes, developers, and release\ndates. We further performed a comparative analysis with previous longitudinal\nstudies that reported significant post-cutoff performance decay using directly\nretrieved questions based on public data. we hypothesize that the multi-step\nreasoning required by our synthesis pipeline offered additional complexity that\ngoes deeper than shallow memorization, which effectively serves a mitigation\nstrategy against benchmark contamination. We fully open source our code and\ndataset to aid reproducibility and advocate for a paradigm shift that\nprioritize reasoning-driven synthesis to construct benchmarks over simply\ncollecting newly released questions periodically.", "AI": {"tldr": "LLM\u57fa\u51c6\u8bc4\u4f30\u9762\u4e34\u6570\u636e\u6c61\u67d3\u8d28\u7591\uff0c\u672c\u7814\u7a76\u901a\u8fc7arXiv\u8bba\u6587\u751f\u6210\u591a\u6b65\u63a8\u7406QA\uff0c\u53d1\u73b0\u524d\u6cbf\u6a21\u578b\u5728\u77e5\u8bc6\u622a\u6b62\u671f\u9644\u8fd1\u65e0\u663e\u8457\u6027\u80fd\u8870\u51cf\uff0c\u8868\u660e\u6b64\u65b9\u6cd5\u80fd\u6709\u6548\u5bf9\u6297\u57fa\u51c6\u6c61\u67d3\u3002", "motivation": "\u73b0\u6709LLM\u57fa\u51c6\u6d4b\u8bd5\u9762\u4e34\u6570\u636e\u6c61\u67d3\u8d28\u7591\uff0c\u96be\u4ee5\u533a\u5206\u6a21\u578b\u7684\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u4e0e\u5355\u7eaf\u8bb0\u5fc6\uff0c\u8fd9\u4f7f\u5f97LLM\u80fd\u529b\u8bc4\u4f30\u7684\u6709\u6548\u6027\u53d7\u5230\u6311\u6218\u3002", "method": "1. \u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u65e0\u9650\u6269\u5c55\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u4ecearXiv\u8bba\u6587\u5408\u6210\u7814\u7a76\u7ea7\u591a\u6b65\u95ee\u7b54\uff08QA\uff09\u9898\u30022. \u5229\u7528\u51fa\u7248\u7269\u7684\u65f6\u95f4\u7ed3\u6784\uff0c\u901a\u8fc7\u6a21\u578b\u5728\u77e5\u8bc6\u622a\u6b62\u65e5\u671f\u540e\u7684\u6027\u80fd\u8870\u51cf\u6765\u68c0\u6d4b\u6f5c\u5728\u7684\u6570\u636e\u6c61\u67d3\u30023. \u8bc4\u4f30\u4e864\u4e2a\u524d\u6cbfLLM\uff08\u6bcf\u4e2a\u5bb6\u65cf2\u4e2a\u4e0d\u540c\u77e5\u8bc6\u622a\u6b62\u65e5\u671f\u7684\u6a21\u578b\uff09\uff0c\u4f7f\u7528\u4ece20,277\u7bc7arXiv\u8bba\u6587\u4e2d\u751f\u6210\u76841,643\u4e2a\u591a\u6b65\u63a8\u7406\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u65f6\u95f4\u8de8\u5ea626\u4e2a\u6708\uff0c\u8986\u76d6\u6240\u6709\u622a\u6b62\u65e5\u671f\u524d\u540e\u81f3\u5c116\u4e2a\u6708\u30024. \u4e0e\u5148\u524d\u62a5\u544a\u4f7f\u7528\u516c\u5171\u6570\u636e\u76f4\u63a5\u68c0\u7d22\u95ee\u9898\u51fa\u73b0\u663e\u8457\u6027\u80fd\u8870\u51cf\u7684\u7eb5\u5411\u7814\u7a76\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u7ed3\u679c\u4e00\u81f4\u8868\u660e\uff0c\u5bf9\u4e8e\u5404\u79cd\u89c4\u6a21\u3001\u5f00\u53d1\u8005\u548c\u53d1\u5e03\u65e5\u671f\u7684\u6a21\u578b\uff0c\u5728\u77e5\u8bc6\u622a\u6b62\u65e5\u671f\u9644\u8fd1\u5747\u672a\u51fa\u73b0\u663e\u8457\u7684\u6027\u80fd\u8870\u51cf\u3002", "conclusion": "1. \u7814\u7a76\u8005\u63a8\u65ad\uff0c\u5176\u5408\u6210\u7ba1\u9053\u6240\u9700\u7684\u591a\u6b65\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u8d85\u8d8a\u6d45\u5c42\u8bb0\u5fc6\u7684\u66f4\u9ad8\u590d\u6742\u6027\uff0c\u6709\u6548\u8d77\u5230\u4e86\u7f13\u89e3\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u6c61\u67d3\u7684\u4f5c\u7528\u30022. \u547c\u5401\u4e00\u79cd\u8303\u5f0f\u8f6c\u53d8\uff0c\u5373\u4f18\u5148\u91c7\u7528\u63a8\u7406\u9a71\u52a8\u7684\u5408\u6210\u65b9\u6cd5\u6765\u6784\u5efa\u57fa\u51c6\uff0c\u800c\u975e\u7b80\u5355\u5730\u5b9a\u671f\u6536\u96c6\u65b0\u53d1\u5e03\u7684\u95ee\u9898\u30023. \u5df2\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4ee5\u652f\u6301\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2509.00027", "pdf": "https://arxiv.org/pdf/2509.00027", "abs": "https://arxiv.org/abs/2509.00027", "authors": ["Elie Thellier", "Huiyu Li", "Nicholas Ayache", "Herv\u00e9 Delingette"], "title": "Mitigating Data Exfiltration Attacks through Layer-Wise Learning Rate Decay Fine-Tuning", "categories": ["cs.LG"], "comment": null, "summary": "Data lakes enable the training of powerful machine learning models on\nsensitive, high-value medical datasets, but also introduce serious privacy\nrisks due to potential leakage of protected health information. Recent studies\nshow adversaries can exfiltrate training data by embedding latent\nrepresentations into model parameters or inducing memorization via multi-task\nlearning. These attacks disguise themselves as benign utility models while\nenabling reconstruction of high-fidelity medical images, posing severe privacy\nthreats with legal and ethical implications. In this work, we propose a simple\nyet effective mitigation strategy that perturbs model parameters at export time\nthrough fine-tuning with a decaying layer-wise learning rate to corrupt\nembedded data without degrading task performance. Evaluations on DermaMNIST,\nChestMNIST, and MIMIC-CXR show that our approach maintains utility task\nperformance, effectively disrupts state-of-the-art exfiltration attacks,\noutperforms prior defenses, and renders exfiltrated data unusable for training.\nAblations and discussions on adaptive attacks highlight challenges and future\ndirections. Our findings offer a practical defense against data leakage in data\nlake-trained models and centralized federated learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9632\u5fa1\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u6a21\u578b\u5bfc\u51fa\u65f6\u5fae\u8c03\u6270\u52a8\u6a21\u578b\u53c2\u6570\uff0c\u6709\u6548\u963b\u6b62\u6570\u636e\u6e56\u4e2d\u654f\u611f\u533b\u7597\u6570\u636e\u7684\u7a83\u53d6\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6570\u636e\u6e56\u5728\u654f\u611f\u533b\u7597\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65f6\uff0c\u5b58\u5728\u4e25\u91cd\u7684\u9690\u79c1\u98ce\u9669\uff0c\u53ef\u80fd\u5bfc\u81f4\u53d7\u4fdd\u62a4\u5065\u5eb7\u4fe1\u606f\u6cc4\u9732\u3002\u73b0\u6709\u7814\u7a76\u8868\u660e\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u5728\u6a21\u578b\u53c2\u6570\u4e2d\u5d4c\u5165\u6f5c\u5728\u8868\u793a\u6216\u8bf1\u5bfc\u8bb0\u5fc6\u6765\u7a83\u53d6\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u91cd\u5efa\u9ad8\u4fdd\u771f\u533b\u7597\u56fe\u50cf\uff0c\u6784\u6210\u4e25\u91cd\u7684\u9690\u79c1\u5a01\u80c1\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u7f13\u89e3\u7b56\u7565\uff1a\u5728\u6a21\u578b\u5bfc\u51fa\u65f6\uff0c\u901a\u8fc7\u4f7f\u7528\u8870\u51cf\u7684\u9010\u5c42\u5b66\u4e60\u7387\u8fdb\u884c\u5fae\u8c03\uff0c\u6270\u52a8\u6a21\u578b\u53c2\u6570\uff0c\u4ee5\u7834\u574f\u5d4c\u5165\u6570\u636e\uff0c\u540c\u65f6\u4e0d\u964d\u4f4e\u4efb\u52a1\u6027\u80fd\u3002", "result": "\u5728DermaMNIST\u3001ChestMNIST\u548cMIMIC-CXR\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\uff0c\u6709\u6548\u5e72\u6270\u6700\u5148\u8fdb\u7684\u6570\u636e\u7a83\u53d6\u653b\u51fb\uff0c\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\uff0c\u5e76\u4f7f\u7a83\u53d6\u7684\u6570\u636e\u65e0\u6cd5\u7528\u4e8e\u8bad\u7ec3\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6570\u636e\u6e56\u8bad\u7ec3\u6a21\u578b\u548c\u96c6\u4e2d\u5f0f\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u6cc4\u9732\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u9632\u5fa1\u65b9\u6848\u3002\u5bf9\u6d88\u878d\u5b9e\u9a8c\u548c\u81ea\u9002\u5e94\u653b\u51fb\u7684\u8ba8\u8bba\u4e5f\u63ed\u793a\u4e86\u672a\u6765\u7684\u6311\u6218\u548c\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.00038", "pdf": "https://arxiv.org/pdf/2509.00038", "abs": "https://arxiv.org/abs/2509.00038", "authors": ["Teo Susnjak"], "title": "Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) offer significant potential to accelerate\nsystematic literature reviews (SLRs), yet current approaches often rely on\nbrittle, manually crafted prompts that compromise reliability and\nreproducibility. This fragility undermines scientific confidence in\nLLM-assisted evidence synthesis. In response, this work adapts recent advances\nin declarative prompt optimisation, developed for general-purpose LLM\napplications, and demonstrates their applicability to the domain of SLR\nautomation. This research proposes a structured, domain-specific framework that\nembeds task declarations, test suites, and automated prompt tuning into a\nreproducible SLR workflow. These emerging methods are translated into a\nconcrete blueprint with working code examples, enabling researchers to\nconstruct verifiable LLM pipelines that align with established principles of\ntransparency and rigour in evidence synthesis. This is a novel application of\nsuch approaches to SLR pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5c06\u58f0\u660e\u5f0f\u63d0\u793a\u4f18\u5316\u6280\u672f\u5e94\u7528\u4e8e\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff08SLR\uff09\u81ea\u52a8\u5316\u9886\u57df\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709LLM\u8f85\u52a9SLR\u4e2d\u63d0\u793a\u8106\u5f31\u3001\u53ef\u91cd\u590d\u6027\u5dee\u7684\u95ee\u9898\uff0c\u4ece\u800c\u6784\u5efa\u53ef\u9a8c\u8bc1\u4e14\u900f\u660e\u7684LLM\u6d41\u6c34\u7ebf\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u52a0\u901f\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff08SLR\uff09\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8106\u5f31\u4e14\u624b\u52a8\u5236\u4f5c\u7684\u63d0\u793a\uff0c\u8fd9\u635f\u5bb3\u4e86\u53ef\u9760\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u4ece\u800c\u524a\u5f31\u4e86\u5bf9LLM\u8f85\u52a9\u8bc1\u636e\u7efc\u5408\u7684\u79d1\u5b66\u4fe1\u5fc3\u3002", "method": "\u8be5\u7814\u7a76\u5c06\u901a\u7528\u7684\u58f0\u660e\u5f0f\u63d0\u793a\u4f18\u5316\uff08declarative prompt optimisation\uff09\u6280\u672f\u5e94\u7528\u4e8eSLR\u81ea\u52a8\u5316\u9886\u57df\u3002\u5177\u4f53\u65b9\u6cd5\u662f\u63d0\u51fa\u4e00\u4e2a\u7ed3\u6784\u5316\u3001\u9886\u57df\u7279\u5b9a\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u4efb\u52a1\u58f0\u660e\u3001\u6d4b\u8bd5\u5957\u4ef6\u548c\u81ea\u52a8\u5316\u63d0\u793a\u8c03\u6574\u5d4c\u5165\u5230\u53ef\u91cd\u590d\u7684SLR\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u5e76\u63d0\u4f9b\u5177\u4f53\u84dd\u56fe\u548c\u53ef\u64cd\u4f5c\u4ee3\u7801\u793a\u4f8b\u3002", "result": "\u901a\u8fc7\u6b64\u65b9\u6cd5\uff0c\u7814\u7a76\u4eba\u5458\u80fd\u591f\u6784\u5efa\u53ef\u9a8c\u8bc1\u7684LLM\u6d41\u6c34\u7ebf\uff0c\u4f7f\u5176\u4e0e\u8bc1\u636e\u7efc\u5408\u4e2d\u65e2\u5b9a\u7684\u900f\u660e\u5ea6\u548c\u4e25\u8c28\u6027\u539f\u5219\u4fdd\u6301\u4e00\u81f4\u3002\u8be5\u7814\u7a76\u4e3aSLR\u6d41\u6c34\u7ebf\u63d0\u4f9b\u4e86\u6b64\u7c7b\u65b9\u6cd5\u7684\u65b0\u9896\u5e94\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u58f0\u660e\u5f0f\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u4e3aLLM\u8f85\u52a9\u7684\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u63d0\u793a\u8106\u5f31\u6027\u95ee\u9898\u7684\u65b0\u9014\u5f84\uff0c\u4ece\u800c\u63d0\u5347\u4e86LLM\u5728\u8bc1\u636e\u7efc\u5408\u4e2d\u7684\u53ef\u9760\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u79d1\u5b66\u4fe1\u5fc3\u3002"}}
{"id": "2509.00062", "pdf": "https://arxiv.org/pdf/2509.00062", "abs": "https://arxiv.org/abs/2509.00062", "authors": ["Justin Jung"], "title": "Scaffold Diffusion: Sparse Multi-Category Voxel Structure Generation with Discrete Diffusion", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Generating realistic sparse multi-category 3D voxel structures is difficult\ndue to the cubic memory scaling of voxel structures and moreover the\nsignificant class imbalance caused by sparsity. We introduce Scaffold\nDiffusion, a generative model designed for sparse multi-category 3D voxel\nstructures. By treating voxels as tokens, Scaffold Diffusion uses a discrete\ndiffusion language model to generate 3D voxel structures. We show that discrete\ndiffusion language models can be extended beyond inherently sequential domains\nsuch as text to generate spatially coherent 3D structures. We evaluate on\nMinecraft house structures from the 3D-Craft dataset and demonstrate that,\nunlike prior baselines and an auto-regressive formulation, Scaffold Diffusion\nproduces realistic and coherent structures even when trained on data with over\n98% sparsity. We provide an interactive viewer where readers can visualize\ngenerated samples and the generation process. Our results highlight discrete\ndiffusion as a promising framework for 3D sparse voxel generative modeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faScaffold Diffusion\uff0c\u4e00\u4e2a\u5229\u7528\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7a00\u758f\u591a\u7c7b\u522b3D\u4f53\u7d20\u7ed3\u6784\u7684\u751f\u6210\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u9ad8\u7a00\u758f\u5ea6\u4e0b\u7684\u751f\u6210\u96be\u9898\uff0c\u5e76\u5c55\u73b0\u4e86\u5176\u5728\u751f\u6210\u903c\u771f\u8fde\u8d2f\u7ed3\u6784\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u751f\u6210\u903c\u771f\u7684\u7a00\u758f\u591a\u7c7b\u522b3D\u4f53\u7d20\u7ed3\u6784\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u4f53\u7d20\u7ed3\u6784\u7684\u5185\u5b58\u5360\u7528\u5448\u7acb\u65b9\u589e\u957f\uff0c\u4ee5\u53ca\u7a00\u758f\u6027\u5bfc\u81f4\u7684\u663e\u8457\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u5f15\u5165Scaffold Diffusion\u751f\u6210\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u4f53\u7d20\u89c6\u4e3atokens\uff0c\u5e76\u5229\u7528\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u6765\u751f\u62103D\u4f53\u7d20\u7ed3\u6784\u3002\u6587\u7ae0\u8bc1\u660e\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u4ece\u6587\u672c\u7b49\u56fa\u6709\u7684\u5e8f\u5217\u9886\u57df\u6269\u5c55\u5230\u751f\u6210\u7a7a\u95f4\u8fde\u8d2f\u76843D\u7ed3\u6784\u3002", "result": "\u57283D-Craft\u6570\u636e\u96c6\u7684Minecraft\u623f\u5c4b\u7ed3\u6784\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793aScaffold Diffusion\u6a21\u578b\u80fd\u591f\u751f\u6210\u903c\u771f\u4e14\u8fde\u8d2f\u7684\u7ed3\u6784\uff0c\u5373\u4f7f\u5728\u6570\u636e\u7a00\u758f\u5ea6\u8d85\u8fc798%\u7684\u60c5\u51b5\u4e0b\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u548c\u81ea\u56de\u5f52\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u8868\u660e\uff0c\u79bb\u6563\u6269\u6563\u662f\u4e00\u4e2a\u6709\u524d\u666f\u76843D\u7a00\u758f\u4f53\u7d20\u751f\u6210\u5efa\u6a21\u6846\u67b6\u3002"}}
{"id": "2509.00074", "pdf": "https://arxiv.org/pdf/2509.00074", "abs": "https://arxiv.org/abs/2509.00074", "authors": ["C\u00e9dric Colas", "Tracey Mills", "Ben Prystawski", "Michael Henry Tessler", "Noah Goodman", "Jacob Andreas", "Joshua Tenenbaum"], "title": "Language and Experience: A Computational Model of Social Learning in Complex Tasks", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The ability to combine linguistic guidance from others with direct experience\nis central to human development, enabling safe and rapid learning in new\nenvironments. How do people integrate these two sources of knowledge, and how\nmight AI systems? We present a computational framework that models social\nlearning as joint probabilistic inference over structured, executable world\nmodels given sensorimotor and linguistic data. We make this possible by turning\na pretrained language model into a probabilistic model of how humans share\nadvice conditioned on their beliefs, allowing our agents both to generate\nadvice for others and to interpret linguistic input as evidence during Bayesian\ninference. Using behavioral experiments and simulations across 10 video games,\nwe show how linguistic guidance can shape exploration and accelerate learning\nby reducing risky interactions and speeding up key discoveries in both humans\nand models. We further explore how knowledge can accumulate across generations\nthrough iterated learning experiments and demonstrate successful knowledge\ntransfer between humans and models -- revealing how structured,\nlanguage-compatible representations might enable human-machine collaborative\nlearning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u6982\u7387\u6a21\u578b\uff0c\u4f7fAI\u80fd\u7ed3\u5408\u8bed\u8a00\u6307\u5bfc\u548c\u76f4\u63a5\u7ecf\u9a8c\u8fdb\u884c\u793e\u4f1a\u5b66\u4e60\u3002\u5728\u89c6\u9891\u6e38\u620f\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u52a0\u901f\u4e86\u4eba\u7c7b\u548c\u6a21\u578b\u7684\u5b66\u4e60\uff0c\u5e76\u5b9e\u73b0\u4e86\u4eba\u673a\u95f4\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4e3a\u6784\u5efa\u4eba\u673a\u534f\u4f5c\u5b66\u4e60\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u5982\u4f55\u6574\u5408\u8bed\u8a00\u6307\u5bfc\u4e0e\u76f4\u63a5\u7ecf\u9a8c\u4ee5\u5b9e\u73b0\u5b89\u5168\u5feb\u901f\u5b66\u4e60\uff0c\u53ca\u5176\u5bf9\u4eba\u7c7b\u53d1\u5c55\u7684\u6838\u5fc3\u4f5c\u7528\u3002\u63a2\u7d22AI\u7cfb\u7edf\u5982\u4f55\u4e5f\u80fd\u6709\u6548\u7ed3\u5408\u8fd9\u4e24\u79cd\u77e5\u8bc6\u6765\u6e90\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\uff0c\u5c06\u793e\u4f1a\u5b66\u4e60\u5efa\u6a21\u4e3a\u57fa\u4e8e\u7ed3\u6784\u5316\u3001\u53ef\u6267\u884c\u4e16\u754c\u6a21\u578b\u7684\u8054\u5408\u6982\u7387\u63a8\u7406\uff0c\u5229\u7528\u611f\u89c9\u8fd0\u52a8\u548c\u8bed\u8a00\u6570\u636e\u3002\u5c06\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u8f6c\u5316\u4e3a\u4e00\u4e2a\u6982\u7387\u6a21\u578b\uff0c\u6a21\u62df\u4eba\u7c7b\u57fa\u4e8e\u4fe1\u5ff5\u5206\u4eab\u5efa\u8bae\u7684\u65b9\u5f0f\uff0c\u4f7fAI\u4ee3\u7406\u80fd\u751f\u6210\u5efa\u8bae\u5e76\u5728\u8d1d\u53f6\u65af\u63a8\u7406\u4e2d\u5c06\u8bed\u8a00\u8f93\u5165\u4f5c\u4e3a\u8bc1\u636e\u3002\u901a\u8fc7\u572810\u6b3e\u89c6\u9891\u6e38\u620f\u4e2d\u7684\u884c\u4e3a\u5b9e\u9a8c\u548c\u6a21\u62df\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8bed\u8a00\u6307\u5bfc\u80fd\u5851\u9020\u63a2\u7d22\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u51cf\u5c11\u5371\u9669\u4ea4\u4e92\u548c\u52a0\u901f\u5173\u952e\u53d1\u73b0\uff0c\u663e\u8457\u52a0\u901f\u4eba\u7c7b\u548c\u6a21\u578b\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002\u901a\u8fc7\u8fed\u4ee3\u5b66\u4e60\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u77e5\u8bc6\u5982\u4f55\u5728\u4ee3\u9645\u95f4\u79ef\u7d2f\u3002\u6210\u529f\u5b9e\u73b0\u4e86\u4eba\u7c7b\u4e0e\u6a21\u578b\u4e4b\u95f4\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u7ed3\u6784\u5316\u3001\u517c\u5bb9\u8bed\u8a00\u7684\u8868\u5f81\u80fd\u591f\u4fc3\u8fdb\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\u5b66\u4e60\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u7406\u89e3\u4eba\u7c7b\u793e\u4f1a\u5b66\u4e60\u673a\u5236\u4ee5\u53ca\u8bbe\u8ba1\u5177\u6709\u7c7b\u4f3c\u80fd\u529b\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2509.00031", "pdf": "https://arxiv.org/pdf/2509.00031", "abs": "https://arxiv.org/abs/2509.00031", "authors": ["Qitao Tan", "Xiaoying Song", "Jin Lu", "Guoming Li", "Jun Liu", "Lingzi Hong", "Caiwen Ding", "Jundong Li", "Xiaoming Zhai", "Shaoyi Huang", "Wei Niu", "Geng Yuan"], "title": "ZeroQAT: Your Quantization-aware Training but Efficient", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Quantization is an effective technique to reduce the deployment cost of large\nlanguage models (LLMs), and post-training quantization (PTQ) has been widely\nstudied due to its efficiency. However, existing low-bit PTQ methods suffer\nfrom accuracy degradation because their layer-wise optimization introduces\ncumulative error propagation and misalignment between local reconstruction\nobjectives and downstream performance. While quantization-aware training (QAT)\nprovides a principled solution, its reliance on backpropagation incurs\nprohibitive data, time, and memory costs, limiting its practicality. To address\nthese challenges, we propose ZeroQAT, a zeroth-order optimization-based QAT\nframework. ZeroQAT leverages forward-only gradient estimation to eliminate the\nneed for backpropagation, significantly reducing computational and memory\noverhead while retaining the benefits of end-to-end optimization. Moreover,\nZeroQAT jointly learns quantized weights, weight clipping thresholds, and\nequivalent transformations to mitigate quantization error and handle activation\noutliers. Experiments demonstrate that ZeroQAT achieves the efficiency of PTQ\nwhile retaining the accuracy of QAT, offering a practical solution for\nhigh-quality low-bit quantization of LLMs.", "AI": {"tldr": "ZeroQAT\u662f\u4e00\u4e2a\u57fa\u4e8e\u96f6\u9636\u4f18\u5316\u7684QAT\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u524d\u5411\u68af\u5ea6\u4f30\u8ba1\u907f\u514d\u53cd\u5411\u4f20\u64ad\uff0c\u5b9e\u73b0LLM\u4f4e\u6bd4\u7279\u91cf\u5316\u7684\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u4f4e\u6bd4\u7279PTQ\u65b9\u6cd5\u56e0\u7d2f\u79ef\u8bef\u5dee\u548c\u76ee\u6807\u4e0d\u5339\u914d\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\uff1b\u800cQAT\u867d\u7cbe\u786e\u4f46\u53cd\u5411\u4f20\u64ad\u5e26\u6765\u9ad8\u6602\u7684\u8ba1\u7b97\u3001\u65f6\u95f4\u3001\u5185\u5b58\u6210\u672c\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faZeroQAT\u6846\u67b6\uff0c\u5229\u7528\u4ec5\u524d\u5411\u7684\u96f6\u9636\u68af\u5ea6\u4f30\u8ba1\u6765\u907f\u514d\u53cd\u5411\u4f20\u64ad\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002ZeroQAT\u8fd8\u8054\u5408\u5b66\u4e60\u91cf\u5316\u6743\u91cd\u3001\u6743\u91cd\u88c1\u526a\u9608\u503c\u548c\u7b49\u6548\u53d8\u6362\uff0c\u4ee5\u51cf\u8f7b\u91cf\u5316\u8bef\u5dee\u5e76\u5904\u7406\u6fc0\u6d3b\u5f02\u5e38\u503c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cZeroQAT\u5728\u4fdd\u6301QAT\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e86PTQ\u7684\u6548\u7387\u3002", "conclusion": "ZeroQAT\u4e3aLLM\u7684\u9ad8\u8d28\u91cf\u4f4e\u6bd4\u7279\u91cf\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00185", "pdf": "https://arxiv.org/pdf/2509.00185", "abs": "https://arxiv.org/abs/2509.00185", "authors": ["Jian Wu", "Sarah Rajtmajer"], "title": "What Are Research Hypotheses?", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, accepted by Sci-K'25: International Workshop on Scientific\n  Knowledge", "summary": "Over the past decades, alongside advancements in natural language processing,\nsignificant attention has been paid to training models to automatically\nextract, understand, test, and generate hypotheses in open and scientific\ndomains. However, interpretations of the term \\emph{hypothesis} for various\nnatural language understanding (NLU) tasks have migrated from traditional\ndefinitions in the natural, social, and formal sciences. Even within NLU, we\nobserve differences defining hypotheses across literature. In this paper, we\noverview and delineate various definitions of hypothesis. Especially, we\ndiscern the nuances of definitions across recently published NLU tasks. We\nhighlight the importance of well-structured and well-defined hypotheses,\nparticularly as we move toward a machine-interpretable scholarly record.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u81ea\u7136\u8bed\u8a00\u7406\u89e3(NLU)\u9886\u57df\u4e2d\u201c\u5047\u8bbe\u201d\u4e00\u8bcd\u5b9a\u4e49\u7684\u591a\u6837\u6027\u4e0e\u504f\u79bb\u4f20\u7edf\u79d1\u5b66\u5b9a\u4e49\u7684\u73b0\u8c61\uff0c\u5e76\u5f3a\u8c03\u4e86\u6e05\u6670\u5b9a\u4e49\u5047\u8bbe\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u4fc3\u8fdb\u673a\u5668\u53ef\u89e3\u91ca\u7684\u5b66\u672f\u8bb0\u5f55\u3002", "motivation": "\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3(NLU)\u4efb\u52a1\u4e2d\uff0c\u201c\u5047\u8bbe\u201d\u4e00\u8bcd\u7684\u89e3\u91ca\u5df2\u504f\u79bb\u4f20\u7edf\u79d1\u5b66\u5b9a\u4e49\uff0c\u4e14\u5728NLU\u6587\u732e\u5185\u90e8\u4e5f\u5b58\u5728\u5b9a\u4e49\u4e0a\u7684\u5dee\u5f02\u548c\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u672c\u6587\u6982\u8ff0\u5e76\u63cf\u7ed8\u4e86\u201c\u5047\u8bbe\u201d\u7684\u5404\u79cd\u5b9a\u4e49\uff0c\u5c24\u5176\u8fa8\u522b\u4e86\u8fd1\u671fNLU\u4efb\u52a1\u4e2d\u5b9a\u4e49\u7684\u7ec6\u5fae\u5dee\u522b\u3002", "result": "\u901a\u8fc7\u5206\u6790\uff0c\u63ed\u793a\u4e86NLU\u4efb\u52a1\u4e2d\u201c\u5047\u8bbe\u201d\u5b9a\u4e49\u7684\u590d\u6742\u6027\u3001\u591a\u6837\u6027\u548c\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u660e\u786e\u548c\u826f\u597d\u5b9a\u4e49\u7684\u5047\u8bbe\u5bf9\u4e8e\u6784\u5efa\u673a\u5668\u53ef\u89e3\u91ca\u7684\u5b66\u672f\u8bb0\u5f55\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.00108", "pdf": "https://arxiv.org/pdf/2509.00108", "abs": "https://arxiv.org/abs/2509.00108", "authors": ["Anas M. Ali", "Anis Koubaa", "Bilel Benjdira"], "title": "Dual-Stage Global and Local Feature Framework for Image Dehazing", "categories": ["cs.CV"], "comment": null, "summary": "Addressing the challenge of removing atmospheric fog or haze from digital\nimages, known as image dehazing, has recently gained significant traction in\nthe computer vision community. Although contemporary dehazing models have\ndemonstrated promising performance, few have thoroughly investigated\nhigh-resolution imagery. In such scenarios, practitioners often resort to\ndownsampling the input image or processing it in smaller patches, which leads\nto a notable performance degradation. This drop is primarily linked to the\ndifficulty of effectively combining global contextual information with\nlocalized, fine-grained details as the spatial resolution grows. In this\nchapter, we propose a novel framework, termed the Streamlined Global and Local\nFeatures Combinator (SGLC), to bridge this gap and enable robust dehazing for\nhigh-resolution inputs. Our approach is composed of two principal components:\nthe Global Features Generator (GFG) and the Local Features Enhancer (LFE). The\nGFG produces an initial dehazed output by focusing on broad contextual\nunderstanding of the scene. Subsequently, the LFE refines this preliminary\noutput by enhancing localized details and pixel-level features, thereby\ncapturing the interplay between global appearance and local structure. To\nevaluate the effectiveness of SGLC, we integrated it with the Uformer\narchitecture, a state-of-the-art dehazing model. Experimental results on\nhigh-resolution datasets reveal a considerable improvement in peak\nsignal-to-noise ratio (PSNR) when employing SGLC, indicating its potency in\naddressing haze in large-scale imagery. Moreover, the SGLC design is\nmodel-agnostic, allowing any dehazing network to be augmented with the proposed\nglobal-and-local feature fusion mechanism. Through this strategy, practitioners\ncan harness both scene-level cues and granular details, significantly improving\nvisual fidelity in high-resolution environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSGLC\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u53bb\u96fe\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53bb\u96fe\u6027\u80fd\u5e76\u53ef\u5e94\u7528\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u53bb\u96fe\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5e38\u9700\u964d\u91c7\u6837\u6216\u5206\u5757\u5904\u7406\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u4e3b\u8981\u539f\u56e0\u662f\u96be\u4ee5\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u6709\u6548\u7ed3\u5408\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0e\u5c40\u90e8\u7cbe\u7ec6\u7ec6\u8282\u3002", "method": "\u63d0\u51faStreamlined Global and Local Features Combinator (SGLC) \u6846\u67b6\uff0c\u5305\u542b\u4e24\u90e8\u5206\uff1a\u5168\u5c40\u7279\u5f81\u751f\u6210\u5668(GFG)\u8d1f\u8d23\u521d\u6b65\u53bb\u96fe\u548c\u573a\u666f\u7406\u89e3\uff0c\u4ee5\u53ca\u5c40\u90e8\u7279\u5f81\u589e\u5f3a\u5668(LFE)\u7528\u4e8e\u7ec6\u5316\u5c40\u90e8\u7ec6\u8282\u548c\u50cf\u7d20\u7ea7\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u53ef\u4e0e\u4efb\u610f\u53bb\u96fe\u7f51\u7edc\u7ed3\u5408\uff08\u5982Uformer\uff09\u3002", "result": "\u5728\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSGLC\u663e\u8457\u63d0\u5347\u4e86\u5cf0\u503c\u4fe1\u566a\u6bd4(PSNR)\uff0c\u8bc1\u660e\u5176\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u53bb\u96fe\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\u3002\u5176\u6a21\u578b\u65e0\u5173\u6027\u4f7f\u5176\u80fd\u589e\u5f3a\u4efb\u4f55\u53bb\u96fe\u7f51\u7edc\u3002", "conclusion": "SGLC\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5730\u878d\u5408\u573a\u666f\u7ea7\u7ebf\u7d22\u548c\u7ec6\u8282\u4fe1\u606f\uff0c\u663e\u8457\u6539\u5584\u4e86\u9ad8\u5206\u8fa8\u7387\u73af\u5883\u4e2d\u7684\u56fe\u50cf\u53bb\u96fe\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u53bb\u96fe\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00079", "pdf": "https://arxiv.org/pdf/2509.00079", "abs": "https://arxiv.org/abs/2509.00079", "authors": ["Andrew G. A. Correa", "Ana C. H de Matos"], "title": "Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation", "categories": ["cs.AI", "cs.LG"], "comment": "9 pages, 2 figures, 4 tables", "summary": "Reasoning models often outperform smaller models but at 3--5$\\times$ higher\ncost and added latency. We present entropy-guided refinement: a lightweight,\ntest-time loop that uses token-level uncertainty to trigger a single, targeted\nrefinement pass. We extract logprobs, compute Shannon entropy on top-$k$\nalternatives, and apply a simple OR-logic trigger over perplexity, maximum\ntoken entropy, and low-confidence-token count. Unlike approaches that use\nentropy only for measurement or decoding, we pass a compact uncertainty report\n(tokens, confidences, alternatives, context) back to the model to guide\ncorrective edits. On representative technical queries across reasoning,\nmathematics, and code generation tasks, a small model with our loop approaches\n95\\% of a reference reasoning model's quality at approximately one-third of the\ncost. The method achieves selective refinement on ~31\\% of responses while\nimproving accuracy by 16 percentage points over single-pass inference. We\ndemonstrate that this uncertainty-aware loop provides an effective middle\nground between single-pass inference and expensive reasoning chains, making it\npractical for production deployments where both quality and cost matter.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u71b5\u5f15\u5bfc\u4f18\u5316\u5faa\u73af\uff0c\u901a\u8fc7\u5229\u7528token\u7ea7\u4e0d\u786e\u5b9a\u6027\u89e6\u53d1\u7cbe\u70bc\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\u5728\u5927\u5e45\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u63a5\u8fd1\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u6210\u672c\u9ad8\u6602\u4e14\u5ef6\u8fdf\u5927\u3002\u7814\u7a76\u65e8\u5728\u5bfb\u627e\u4e00\u79cd\u80fd\u5728\u4fdd\u8bc1\u8d28\u91cf\u7684\u540c\u65f6\u964d\u4f4e\u6210\u672c\u548c\u5ef6\u8fdf\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u201c\u71b5\u5f15\u5bfc\u4f18\u5316\u201d\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u6d4b\u8bd5\u65f6\u5faa\u73af\u3002\u8be5\u65b9\u6cd5\u63d0\u53d6\u5bf9\u6570\u6982\u7387\uff0c\u8ba1\u7b97\u524dk\u4e2a\u5907\u9009\u9879\u7684\u9999\u519c\u71b5\uff0c\u5e76\u7ed3\u5408\u56f0\u60d1\u5ea6\u3001\u6700\u5927token\u71b5\u548c\u4f4e\u7f6e\u4fe1\u5ea6token\u6570\u91cf\u4f7f\u7528OR\u903b\u8f91\u89e6\u53d1\u5668\u3002\u5b83\u5c06\u5305\u542btoken\u3001\u7f6e\u4fe1\u5ea6\u3001\u5907\u9009\u9879\u548c\u4e0a\u4e0b\u6587\u7684\u7d27\u51d1\u4e0d\u786e\u5b9a\u6027\u62a5\u544a\u53cd\u9988\u7ed9\u6a21\u578b\uff0c\u4ee5\u6307\u5bfc\u7ea0\u6b63\u6027\u7f16\u8f91\u3002", "result": "\u5728\u63a8\u7406\u3001\u6570\u5b66\u548c\u4ee3\u7801\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528\u8be5\u5faa\u73af\u7684\u5c0f\u578b\u6a21\u578b\u80fd\u591f\u8fbe\u5230\u53c2\u8003\u63a8\u7406\u6a21\u578b95%\u7684\u8d28\u91cf\uff0c\u800c\u6210\u672c\u4ec5\u4e3a\u5176\u7ea6\u4e09\u5206\u4e4b\u4e00\u3002\u8be5\u65b9\u6cd5\u5bf9\u5927\u7ea631%\u7684\u54cd\u5e94\u8fdb\u884c\u4e86\u9009\u62e9\u6027\u4f18\u5316\uff0c\u5e76\u5c06\u51c6\u786e\u6027\u6bd4\u5355\u6b21\u63a8\u7406\u63d0\u9ad8\u4e8616\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5faa\u73af\u5728\u5355\u6b21\u63a8\u7406\u548c\u6602\u8d35\u7684\u63a8\u7406\u94fe\u4e4b\u95f4\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6298\u4e2d\u65b9\u6848\uff0c\u5bf9\u4e8e\u540c\u65f6\u6ce8\u91cd\u8d28\u91cf\u548c\u6210\u672c\u7684\u751f\u4ea7\u90e8\u7f72\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.00034", "pdf": "https://arxiv.org/pdf/2509.00034", "abs": "https://arxiv.org/abs/2509.00034", "authors": ["Mert Sehri", "Ana Cardoso", "Francisco de Assis Boldt", "Patrick Dumond"], "title": "Industrial Steel Slag Flow Data Loading Method for Deep Learning Applications", "categories": ["cs.LG"], "comment": null, "summary": "Steel casting processes are vulnerable to financial losses due to slag flow\ncontamination, making accurate slag flow condition detection essential. This\nstudy introduces a novel cross-domain diagnostic method using vibration data\ncollected from an industrial steel foundry to identify various stages of slag\nflow. A hybrid deep learning model combining one-dimensional convolutional\nneural networks and long short-term memory layers is implemented, tested, and\nbenchmarked against a standard one-dimensional convolutional neural network.\nThe proposed method processes raw time-domain vibration signals from\naccelerometers and evaluates performance across 16 distinct domains using a\nrealistic cross-domain dataset split. Results show that the hybrid\nconvolutional neural network and long short-term memory architecture, when\ncombined with root mean square preprocessing and a selective embedding data\nloading strategy, achieves robust classification accuracy, outperforming\ntraditional models and loading techniques. The highest test accuracy of 99.10\n+/- 0.30 demonstrates the method's capability for generalization and industrial\nrelevance. This work presents a practical and scalable solution for real-time\nslag flow monitoring, contributing to improved reliability and operational\nefficiency in steel manufacturing.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u632f\u52a8\u6570\u636e\u7684\u8de8\u57df\u8bca\u65ad\u65b9\u6cd5\uff0c\u5229\u7528\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7089\u6e23\u6d41\u6001\u76d1\u6d4b\uff0c\u63d0\u9ad8\u94a2\u94c1\u5236\u9020\u6548\u7387\u3002", "motivation": "\u94a2\u6c34\u94f8\u9020\u8fc7\u7a0b\u4e2d\uff0c\u7089\u6e23\u6d41\u52a8\u6c61\u67d3\u6613\u5bfc\u81f4\u7ecf\u6d4e\u635f\u5931\uff0c\u56e0\u6b64\u51c6\u786e\u68c0\u6d4b\u7089\u6e23\u6d41\u6001\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u632f\u52a8\u6570\u636e\u7684\u8de8\u57df\u8bca\u65ad\u65b9\u6cd5\u3002\u4f7f\u7528\u7ed3\u5408\u4e00\u7ef4\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff081D-CNN\uff09\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08LSTM\uff09\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5904\u7406\u52a0\u901f\u8ba1\u7684\u539f\u59cb\u65f6\u57df\u632f\u52a8\u4fe1\u53f7\u3002\u7ed3\u5408\u5747\u65b9\u6839\uff08RMS\uff09\u9884\u5904\u7406\u548c\u9009\u62e9\u6027\u5d4c\u5165\u6570\u636e\u52a0\u8f7d\u7b56\u7565\uff0c\u5e76\u572816\u4e2a\u4e0d\u540c\u57df\u7684\u771f\u5b9e\u8de8\u57df\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408RMS\u9884\u5904\u7406\u548c\u9009\u62e9\u6027\u5d4c\u5165\u6570\u636e\u52a0\u8f7d\u7b56\u7565\u7684\u6df7\u54081D-CNN-LSTM\u67b6\u6784\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u5206\u7c7b\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u548c\u52a0\u8f7d\u6280\u672f\u3002\u6700\u9ad8\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe99.10% \u00b1 0.30%\uff0c\u663e\u793a\u51fa\u8be5\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5de5\u4e1a\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5b9e\u65f6\u7089\u6e23\u6d41\u6001\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u94a2\u94c1\u5236\u9020\u7684\u53ef\u9760\u6027\u548c\u8fd0\u884c\u6548\u7387\u3002"}}
{"id": "2509.00190", "pdf": "https://arxiv.org/pdf/2509.00190", "abs": "https://arxiv.org/abs/2509.00190", "authors": ["Sheldon Yu", "Yuxin Xiong", "Junda Wu", "Xintong Li", "Tong Yu", "Xiang Chen", "Ritwik Sinha", "Jingbo Shang", "Julian McAuley"], "title": "Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 4 figures", "summary": "Recent advances in chain-of-thought (CoT) prompting have enabled large\nlanguage models (LLMs) to perform multi-step reasoning. However, the\nexplainability of such reasoning remains limited, with prior work primarily\nfocusing on local token-level attribution, such that the high-level semantic\nroles of reasoning steps and their transitions remain underexplored. In this\npaper, we introduce a state-aware transition framework that abstracts CoT\ntrajectories into structured latent dynamics. Specifically, to capture the\nevolving semantics of CoT reasoning, each reasoning step is represented via\nspectral analysis of token-level embeddings and clustered into semantically\ncoherent latent states. To characterize the global structure of reasoning, we\nmodel their progression as a Markov chain, yielding a structured and\ninterpretable view of the reasoning process. This abstraction supports a range\nof analyses, including semantic role identification, temporal pattern\nvisualization, and consistency evaluation.", "AI": {"tldr": "CoT\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u9ad8\u7ea7\u8bed\u4e49\u5c42\u9762\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u72b6\u6001\u611f\u77e5\u8f6c\u6362\u6846\u67b6\uff0c\u901a\u8fc7\u8c31\u5206\u6790\u548c\u9a6c\u5c14\u53ef\u592b\u94fe\u5c06CoT\u8f68\u8ff9\u62bd\u8c61\u4e3a\u7ed3\u6784\u5316\u6f5c\u5728\u52a8\u6001\uff0c\u4ee5\u589e\u5f3a\u5176\u89e3\u91ca\u6027\u5e76\u652f\u6301\u591a\u7ef4\u5206\u6790\u3002", "motivation": "\u73b0\u6709CoT\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u4fa7\u91cd\u4e8e\u5c40\u90e8token\u5c42\u9762\u5f52\u56e0\uff0c\u800c\u9ad8\u5c42\u8bed\u4e49\u89d2\u8272\u53ca\u63a8\u7406\u6b65\u9aa4\u95f4\u7684\u8f6c\u6362\u673a\u5236\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u72b6\u6001\u611f\u77e5\u8f6c\u6362\u6846\u67b6\uff0c\u5c06CoT\u8f68\u8ff9\u62bd\u8c61\u4e3a\u7ed3\u6784\u5316\u6f5c\u5728\u52a8\u6001\u3002\u5177\u4f53\u5730\uff0c\u901a\u8fc7\u5bf9token\u7ea7\u5d4c\u5165\u8fdb\u884c\u8c31\u5206\u6790\u5c06\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u805a\u7c7b\u6210\u8bed\u4e49\u8fde\u8d2f\u7684\u6f5c\u5728\u72b6\u6001\uff1b\u63a5\u7740\uff0c\u5c06\u8fd9\u4e9b\u6f5c\u5728\u72b6\u6001\u7684\u6f14\u8fdb\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u4ee5\u523b\u753b\u63a8\u7406\u7684\u5168\u5c40\u7ed3\u6784\u3002", "result": "\u8be5\u62bd\u8c61\u6846\u67b6\u652f\u6301\u4e00\u7cfb\u5217\u5206\u6790\uff0c\u5305\u62ec\u8bed\u4e49\u89d2\u8272\u8bc6\u522b\u3001\u65f6\u95f4\u6a21\u5f0f\u53ef\u89c6\u5316\u548c\u4e00\u81f4\u6027\u8bc4\u4f30\u3002", "conclusion": "\u672c\u6846\u67b6\u901a\u8fc7\u63d0\u4f9b\u7ed3\u6784\u5316\u4e14\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\u89c6\u89d2\uff0c\u663e\u8457\u63d0\u5347\u4e86CoT\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.00131", "pdf": "https://arxiv.org/pdf/2509.00131", "abs": "https://arxiv.org/abs/2509.00131", "authors": ["Ivan Slootweg", "Natalia P. Garc\u00eda-De-La-Puente", "Geert Litjens", "Salma Dammak"], "title": "Self-supervised large-scale kidney abnormality detection in drug safety assessment studies", "categories": ["cs.CV", "eess.IV", "q-bio.QM"], "comment": null, "summary": "Kidney abnormality detection is required for all preclinical drug\ndevelopment. It involves a time-consuming and costly examination of hundreds to\nthousands of whole-slide images per drug safety study, most of which are\nnormal, to detect any subtle changes indicating toxic effects. In this study,\nwe present the first large-scale self-supervised abnormality detection model\nfor kidney toxicologic pathology, spanning drug safety assessment studies from\n158 compounds. We explore the complexity of kidney abnormality detection on\nthis scale using features extracted from the UNI foundation model (FM) and show\nthat a simple k-nearest neighbor classifier on these features performs at\nchance, demonstrating that the FM-generated features alone are insufficient for\ndetecting abnormalities. We then demonstrate that a self-supervised method\napplied to the same features can achieve better-than-chance performance, with\nan area under the receiver operating characteristic curve of 0.62 and a\nnegative predictive value of 89%. With further development, such a model can be\nused to rule out normal slides in drug safety assessment studies, reducing the\ncosts and time associated with drug development.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u81ea\u76d1\u7763\u80be\u810f\u6bd2\u7406\u75c5\u7406\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u836f\u7269\u5b89\u5168\u8bc4\u4f30\u7684\u6548\u7387\u3002", "motivation": "\u836f\u7269\u4e34\u5e8a\u524d\u5f00\u53d1\u4e2d\u7684\u80be\u810f\u5f02\u5e38\u68c0\u6d4b\u8fc7\u7a0b\u8017\u65f6\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4eba\u5de5\u68c0\u67e5\u6570\u5343\u5f20\u5168\u5207\u7247\u56fe\u50cf\uff08\u5176\u4e2d\u5927\u90e8\u5206\u662f\u6b63\u5e38\u7684\uff09\uff0c\u4ee5\u53d1\u73b0\u6307\u793a\u6bd2\u6027\u4f5c\u7528\u7684\u7ec6\u5fae\u53d8\u5316\u3002", "method": "\u8be5\u7814\u7a76\u5229\u7528UNI\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u9996\u5148\u63a2\u7d22\u4e86\u4f7f\u7528\u7b80\u5355k-\u8fd1\u90bb\u5206\u7c7b\u5668\u5728\u8fd9\u4e9b\u7279\u5f81\u4e0a\u7684\u6027\u80fd\u3002\u968f\u540e\uff0c\u5c06\u81ea\u76d1\u7763\u65b9\u6cd5\u5e94\u7528\u4e8e\u76f8\u540c\u7684\u7279\u5f81\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7528\u4e8e\u80be\u810f\u6bd2\u7406\u75c5\u7406\u5b66\u5f02\u5e38\u68c0\u6d4b\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4f7f\u7528\u4e86\u6765\u81ea158\u79cd\u5316\u5408\u7269\u7684\u836f\u7269\u5b89\u5168\u8bc4\u4f30\u7814\u7a76\u6570\u636e\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5355\u72ec\u4f7f\u7528UNI\u57fa\u7840\u6a21\u578b\u751f\u6210\u7684\u7279\u5f81\u7ed3\u5408\u7b80\u5355k-\u8fd1\u90bb\u5206\u7c7b\u5668\u65f6\uff0c\u5176\u6027\u80fd\u63a5\u8fd1\u968f\u673a\uff0c\u8868\u660eFM\u7279\u5f81\u672c\u8eab\u4e0d\u8db3\u4ee5\u68c0\u6d4b\u5f02\u5e38\u3002\u7136\u800c\uff0c\u5c06\u81ea\u76d1\u7763\u65b9\u6cd5\u5e94\u7528\u4e8e\u76f8\u540c\u7279\u5f81\u540e\uff0c\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u968f\u673a\uff0c\u53d6\u5f97\u4e860.62\u7684\u63a5\u6536\u8005\u64cd\u4f5c\u7279\u5f81\u66f2\u7ebf\u4e0b\u9762\u79ef\uff08AUC\uff09\u548c89%\u7684\u9634\u6027\u9884\u6d4b\u503c\uff08NPV\uff09\u3002", "conclusion": "\u8be5\u81ea\u76d1\u7763\u6a21\u578b\u5728\u8fdb\u4e00\u6b65\u5f00\u53d1\u540e\uff0c\u6709\u671b\u7528\u4e8e\u836f\u7269\u5b89\u5168\u8bc4\u4f30\u7814\u7a76\u4e2d\u6392\u9664\u6b63\u5e38\u5207\u7247\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u836f\u7269\u5f00\u53d1\u7684\u76f8\u5173\u6210\u672c\u548c\u65f6\u95f4\u3002"}}
{"id": "2509.00080", "pdf": "https://arxiv.org/pdf/2509.00080", "abs": "https://arxiv.org/abs/2509.00080", "authors": ["David Freire-Obreg\u00f3n"], "title": "Wrong Face, Wrong Move: The Social Dynamics of Emotion Misperception in Agent-Based Models", "categories": ["cs.AI"], "comment": "Accepted for presentation at the International Workshop on\n  Agent-Based Modelling of Human Behaviour (ABMHuB 2025)", "summary": "The ability of humans to detect and respond to others' emotions is\nfundamental to understanding social behavior. Here, agents are instantiated\nwith emotion classifiers of varying accuracy to study the impact of perceptual\naccuracy on emergent emotional and spatial behavior. Agents are visually\nrepresented with face photos from the KDEF database and endowed with one of\nthree classifiers trained on the JAFFE (poor), CK+ (medium), or KDEF (high)\ndatasets. Agents communicate locally on a 2D toroidal lattice, perceiving\nneighbors' emotional state based on their classifier and responding with\nmovement toward perceived positive emotions and away from perceived negative\nemotions. Note that the agents respond to perceived, instead of ground-truth,\nemotions, introducing systematic misperception and frustration. A battery of\nexperiments is carried out on homogeneous and heterogeneous populations and\nscenarios with repeated emotional shocks. Results show that low-accuracy\nclassifiers on the part of the agent reliably result in diminished trust,\nemotional disintegration into sadness, and disordered social organization. By\ncontrast, the agent that develops high accuracy develops hardy emotional\nclusters and resilience to emotional disruptions. Even in emotionally neutral\nscenarios, misperception is enough to generate segregation and disintegration\nof cohesion. These findings underscore the fact that biases or imprecision in\nemotion recognition may significantly warp social processes and disrupt\nemotional integration.", "AI": {"tldr": "\u7814\u7a76\u4e0d\u540c\u60c5\u7eea\u8bc6\u522b\u51c6\u786e\u5ea6\u5bf9\u4eba\u5de5\u4ee3\u7406\u793e\u4f1a\u884c\u4e3a\u3001\u60c5\u7eea\u548c\u7a7a\u95f4\u7ec4\u7ec7\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4f4e\u51c6\u786e\u5ea6\u5bfc\u81f4\u4e0d\u4fe1\u4efb\u3001\u60c5\u7eea\u89e3\u4f53\u548c\u793e\u4f1a\u6df7\u4e71\uff0c\u800c\u9ad8\u51c6\u786e\u5ea6\u5219\u5f62\u6210\u7a33\u5b9a\u60c5\u7eea\u7fa4\u843d\u5e76\u62b5\u6297\u5e72\u6270\u3002\u5373\u4f7f\u662f\u8bef\u611f\u77e5\u4e5f\u8db3\u4ee5\u5bfc\u81f4\u793e\u4f1a\u74e6\u89e3\u3002", "motivation": "\u4eba\u7c7b\u8bc6\u522b\u548c\u56de\u5e94\u4ed6\u4eba\u60c5\u7eea\u7684\u80fd\u529b\u662f\u7406\u89e3\u793e\u4f1a\u884c\u4e3a\u7684\u57fa\u7840\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4eba\u5de5\u4ee3\u7406\uff0c\u63a2\u8ba8\u611f\u77e5\u51c6\u786e\u6027\u5bf9\u7fa4\u4f53\u6d8c\u73b0\u60c5\u7eea\u548c\u7a7a\u95f4\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u4eba\u5de5\u4ee3\u7406\uff0c\u8d4b\u4e88\u5176\u4e0d\u540c\u51c6\u786e\u5ea6\uff08\u4f4e\u3001\u4e2d\u3001\u9ad8\uff09\u7684\u60c5\u7eea\u5206\u7c7b\u5668\u3002\u4ee3\u7406\u5728\u4e8c\u7ef4\u73af\u5f62\u7f51\u683c\u4e0a\u5c40\u90e8\u4ea4\u6d41\uff0c\u6839\u636e\u5176\u5206\u7c7b\u5668\u611f\u77e5\u90bb\u5c45\u60c5\u7eea\uff0c\u5e76\u5411\u611f\u77e5\u5230\u7684\u79ef\u6781\u60c5\u7eea\u79fb\u52a8\uff0c\u8fdc\u79bb\u6d88\u6781\u60c5\u7eea\u3002\u4ee3\u7406\u5bf9\u7684\u662f\u611f\u77e5\u800c\u975e\u771f\u5b9e\u60c5\u7eea\u3002\u5728\u540c\u8d28\u3001\u5f02\u8d28\u7fa4\u4f53\u548c\u91cd\u590d\u60c5\u7eea\u51b2\u51fb\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u5b9e\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f4e\u51c6\u786e\u5ea6\u5206\u7c7b\u5668\u663e\u8457\u5bfc\u81f4\u4fe1\u4efb\u5ea6\u4e0b\u964d\u3001\u60c5\u7eea\u89e3\u4f53\u4e3a\u60b2\u4f24\u4ee5\u53ca\u793e\u4f1a\u7ec4\u7ec7\u6df7\u4e71\u3002\u76f8\u53cd\uff0c\u9ad8\u51c6\u786e\u5ea6\u5206\u7c7b\u5668\u4f7f\u4ee3\u7406\u5f62\u6210\u575a\u56fa\u7684\u60c5\u7eea\u7fa4\u843d\u5e76\u5bf9\u60c5\u7eea\u5e72\u6270\u8868\u73b0\u51fa\u97e7\u6027\u3002\u5373\u4f7f\u5728\u60c5\u7eea\u4e2d\u7acb\u7684\u573a\u666f\u4e2d\uff0c\u8bef\u611f\u77e5\u4e5f\u8db3\u4ee5\u5f15\u53d1\u9694\u79bb\u548c\u51dd\u805a\u529b\u74e6\u89e3\u3002", "conclusion": "\u60c5\u7eea\u8bc6\u522b\u4e2d\u7684\u504f\u89c1\u6216\u4e0d\u51c6\u786e\u6027\u53ef\u80fd\u663e\u8457\u626d\u66f2\u793e\u4f1a\u8fc7\u7a0b\u5e76\u7834\u574f\u60c5\u7eea\u6574\u5408\u3002"}}
{"id": "2509.00035", "pdf": "https://arxiv.org/pdf/2509.00035", "abs": "https://arxiv.org/abs/2509.00035", "authors": ["Yuxuan Yin", "Rebecca Chen", "Boxun Xu", "Chen He", "Peng Li"], "title": "Transfer Learning for Minimum Operating Voltage Prediction in Advanced Technology Nodes: Leveraging Legacy Data and Silicon Odometer Sensing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate prediction of chip performance is critical for ensuring energy\nefficiency and reliability in semiconductor manufacturing. However, developing\nminimum operating voltage ($V_{min}$) prediction models at advanced technology\nnodes is challenging due to limited training data and the complex relationship\nbetween process variations and $V_{min}$. To address these issues, we propose a\nnovel transfer learning framework that leverages abundant legacy data from the\n16nm technology node to enable accurate $V_{min}$ prediction at the advanced\n5nm node. A key innovation of our approach is the integration of input features\nderived from on-chip silicon odometer sensor data, which provide fine-grained\ncharacterization of localized process variations -- an essential factor at the\n5nm node -- resulting in significantly improved prediction accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u82af\u7247\u91cc\u7a0b\u8868\u4f20\u611f\u5668\u6570\u636e\uff0c\u5b9e\u73b05nm\u5148\u8fdb\u8282\u70b9\u6700\u5c0f\u5de5\u4f5c\u7535\u538b($V_{min}$)\u7684\u51c6\u786e\u9884\u6d4b\u3002", "motivation": "\u5728\u5148\u8fdb\u6280\u672f\u8282\u70b9\uff08\u59825nm\uff09\uff0c\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u6709\u9650\u4ee5\u53ca\u5de5\u827a\u53d8\u5f02\u4e0e$V_{min}$\u7684\u590d\u6742\u5173\u7cfb\uff0c\u5f00\u53d1\u51c6\u786e\u7684$V_{min}$\u9884\u6d4b\u6a21\u578b\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u752816nm\u6280\u672f\u8282\u70b9\u7684\u5927\u91cf\u9057\u7559\u6570\u636e\u6765\u9884\u6d4b5nm\u8282\u70b9\u7684$V_{min}$\u3002\u5173\u952e\u521b\u65b0\u662f\u6574\u5408\u4e86\u4ece\u7247\u4e0a\u7845\u91cc\u7a0b\u8868\u4f20\u611f\u5668\u6570\u636e\u4e2d\u63d0\u53d6\u7684\u8f93\u5165\u7279\u5f81\uff0c\u4ee5\u7cbe\u7ec6\u8868\u5f81\u5c40\u90e8\u5de5\u827a\u53d8\u5f02\u3002", "result": "\u901a\u8fc7\u6574\u5408\u82af\u7247\u91cc\u7a0b\u8868\u4f20\u611f\u5668\u6570\u636e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86$V_{min}$\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5c24\u5176\u662f\u57285nm\u8282\u70b9\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\u4f20\u611f\u5668\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5148\u8fdb\u8282\u70b9$V_{min}$\u9884\u6d4b\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u590d\u6742\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5bf9\u534a\u5bfc\u4f53\u5236\u9020\u7684\u80fd\u6548\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.00245", "pdf": "https://arxiv.org/pdf/2509.00245", "abs": "https://arxiv.org/abs/2509.00245", "authors": ["Seiji Maekawa", "Hayate Iso", "Nikita Bhutani"], "title": "The Rarity Blind Spot: A Framework for Evaluating Statistical Reasoning in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Effective decision-making often relies on identifying what makes each\ncandidate distinctive. While existing benchmarks for LLMs emphasize retrieving\nor summarizing information relevant to a given query, they do not evaluate a\nmodel's ability to identify globally distinctive features across a set of\ndocuments. We introduce Distinctive Feature Mining (DFM), a new task that\nchallenges models to analyze a small-to-medium collection (10-40 documents) and\nsurface features that are rare in the global context (e.g., appearing in less\nthan 10% of documents). This setting mirrors real-world scenarios such as\ncandidate selection or product differentiation, where statistical reasoning,\nnot retrieval, is key. To enable systematic evaluation of this capability, we\npresent DiFBench, a configurable benchmark creation framework with controllable\nparameters such as document set size and distinctiveness thresholds. Using\nDiFBench, we perform a large-scale assessment of distinctive feature mining\nacross ten state-of-the-art LLMs. Our findings reveal a significant performance\ngap between general-purpose and reasoning-enhanced models. All models, however,\nsubstantially degrade as the task complexity and document count increase. We\nalso find that a common failure mode is misidentifying frequent features as\ndistinctive. These insights reveal core limitations in contemporary LLMs'\nabilities to perform fine-grained, statistical reasoning and rarity detection.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86\u201c\u72ec\u7279\u6027\u7279\u5f81\u6316\u6398\uff08DFM\uff09\u201d\u65b0\u4efb\u52a1\u548c\u201cDiFBench\u201d\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u6863\u96c6\u5408\u4e2d\u8bc6\u522b\u5168\u5c40\u72ec\u7279\u7279\u5f81\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709LLMs\u5728\u7edf\u8ba1\u63a8\u7406\u548c\u7a00\u6709\u6027\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u3002", "motivation": "\u73b0\u6709LLM\u57fa\u51c6\u4fa7\u91cd\u4e8e\u4fe1\u606f\u68c0\u7d22\u6216\u6458\u8981\uff0c\u672a\u80fd\u8bc4\u4f30\u6a21\u578b\u8bc6\u522b\u6587\u6863\u96c6\u4e2d\u5168\u5c40\u72ec\u7279\u6027\u7279\u5f81\u7684\u80fd\u529b\uff0c\u800c\u8fd9\u5bf9\u4e8e\u51b3\u7b56\u5236\u5b9a\uff08\u5982\u5019\u9009\u4eba\u9009\u62e9\u3001\u4ea7\u54c1\u5dee\u5f02\u5316\uff09\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7edf\u8ba1\u63a8\u7406\u800c\u975e\u7b80\u5355\u68c0\u7d22\u3002", "method": "\u5f15\u5165\u4e86\u201c\u72ec\u7279\u6027\u7279\u5f81\u6316\u6398\uff08DFM\uff09\u201d\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u5206\u679010-40\u4efd\u6587\u6863\u96c6\u5408\uff0c\u8bc6\u522b\u5168\u5c40\u8303\u56f4\u5185\u7a00\u6709\uff08\u51fa\u73b0\u9891\u7387\u4f4e\u4e8e10%\uff09\u7684\u7279\u5f81\u3002\u521b\u5efa\u4e86\u53ef\u914d\u7f6e\u7684\u201cDiFBench\u201d\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u6b64\u80fd\u529b\u3002\u4f7f\u7528DiFBench\u5bf9\u5341\u4e2a\u5148\u8fdb\u7684LLMs\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u7528\u578b\u6a21\u578b\u4e0e\u63a8\u7406\u589e\u5f3a\u578b\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u6240\u6709\u6a21\u578b\u5728\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u6587\u6863\u6570\u91cf\u589e\u52a0\u65f6\uff0c\u6027\u80fd\u5747\u5927\u5e45\u4e0b\u964d\u3002\u4e00\u4e2a\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u662f\u5c06\u9891\u7e41\u7279\u5f81\u8bef\u8bc6\u522b\u4e3a\u72ec\u7279\u6027\u7279\u5f81\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728\u6267\u884c\u7ec6\u7c92\u5ea6\u7edf\u8ba1\u63a8\u7406\u548c\u7a00\u6709\u6027\u68c0\u6d4b\u65b9\u9762\u7684\u6838\u5fc3\u5c40\u9650\u6027\u3002"}}
{"id": "2509.00130", "pdf": "https://arxiv.org/pdf/2509.00130", "abs": "https://arxiv.org/abs/2509.00130", "authors": ["Chang Liu", "T. D. Khoa Le", "Rahul Saini", "Kishor C. Joshi", "George Exarchakos"], "title": "VOTA: Parallelizing 6G-RAN Experimentation with Virtualized Over-The-Air Workloads", "categories": ["cs.NI"], "comment": null, "summary": "Testbed sharing, a practice in which different researchers concurrently\ndevelop independent use cases on top of the same testbed, is ubiquitous in\nwireless experimental research. Its key drawback is experimental inconvenience:\none must delay experiments or tolerate compute and RF interference that harms\nexperimental fidelity. In this paper, we propose \\textbf{VOTA}, an open-source,\nsoftware-only testbed scaling method that leverages real-time virtualization\nand frequency tuning to maximize parallel experiments while controlling\ninterference. In a demonstration of two interference-sensitive 6G use cases --\n\\textit{MIMO iDFT/DFT Offloading} and \\textit{O-RAN DoS Attack} -- running\nside-by-side on a 32-core host, we showcase VOTA capabilities:\n\\textbf{dedicated-like} results while allowing \\textbf{2.67$\\times$} more\nsharing opportunities.", "AI": {"tldr": "VOTA\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u7eaf\u8f6f\u4ef6\u6d4b\u8bd5\u5e73\u53f0\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u65f6\u865a\u62df\u5316\u548c\u9891\u7387\u8c03\u8c10\uff0c\u5728\u5171\u4eab\u65e0\u7ebf\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u6700\u5927\u5316\u5e76\u884c\u5b9e\u9a8c\uff0c\u540c\u65f6\u63a7\u5236\u5e72\u6270\uff0c\u5b9e\u73b0\u63a5\u8fd1\u4e13\u7528\u8d44\u6e90\u7684\u6548\u679c\u5e76\u589e\u52a0\u5171\u4eab\u673a\u4f1a\u3002", "motivation": "\u65e0\u7ebf\u5b9e\u9a8c\u7814\u7a76\u4e2d\uff0c\u6d4b\u8bd5\u5e73\u53f0\u5171\u4eab\u666e\u904d\u5b58\u5728\uff0c\u4f46\u5176\u4e3b\u8981\u7f3a\u70b9\u662f\u5b9e\u9a8c\u4e0d\u4fbf\uff0c\u7814\u7a76\u4eba\u5458\u9700\u8981\u5ef6\u8fdf\u5b9e\u9a8c\u6216\u5fcd\u53d7\u8ba1\u7b97\u548c\u5c04\u9891\u5e72\u6270\uff0c\u8fd9\u4f1a\u635f\u5bb3\u5b9e\u9a8c\u7684\u4fdd\u771f\u5ea6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86VOTA\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u7eaf\u8f6f\u4ef6\u7684\u6d4b\u8bd5\u5e73\u53f0\u6269\u5c55\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u5b9e\u65f6\u865a\u62df\u5316\u548c\u9891\u7387\u8c03\u8c10\u6765\u6700\u5927\u5316\u5e76\u884c\u5b9e\u9a8c\uff0c\u540c\u65f6\u63a7\u5236\u5e72\u6270\u3002", "result": "\u901a\u8fc7\u5728\u4e00\u4e2a32\u6838\u4e3b\u673a\u4e0a\u5e76\u884c\u8fd0\u884c\u4e24\u4e2a\u5bf9\u5e72\u6270\u654f\u611f\u76846G\u7528\u4f8b\uff08MIMO iDFT/DFT\u5378\u8f7d\u548cO-RAN DoS\u653b\u51fb\uff09\uff0cVOTA\u5c55\u793a\u4e86\u5176\u80fd\u529b\uff1a\u5728\u5141\u8bb82.67\u500d\u66f4\u591a\u5171\u4eab\u673a\u4f1a\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u4e13\u7528\u8d44\u6e90\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "VOTA\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5171\u4eab\u6d4b\u8bd5\u5e73\u53f0\u4e2d\u7684\u5e72\u6270\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u5e76\u884c\u5b9e\u9a8c\u7684\u6570\u91cf\u548c\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u9a8c\u7ed3\u679c\u7684\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2509.00176", "pdf": "https://arxiv.org/pdf/2509.00176", "abs": "https://arxiv.org/abs/2509.00176", "authors": ["Muhammad Ali", "Salman Khan"], "title": "Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have paved the way for\nVision Large Language Models (VLLMs) capable of performing a wide range of\nvisual understanding tasks. While LLMs have demonstrated impressive performance\non standard natural images, their capabilities have not been thoroughly\nexplored in cluttered datasets where there is complex environment having\ndeformed shaped objects. In this work, we introduce a novel dataset\nspecifically designed for waste classification in real-world scenarios,\ncharacterized by complex environments and deformed shaped objects. Along with\nthis dataset, we present an in-depth evaluation approach to rigorously assess\nthe robustness and accuracy of VLLMs. The introduced dataset and comprehensive\nanalysis provide valuable insights into the performance of VLLMs under\nchallenging conditions. Our findings highlight the critical need for further\nadvancements in VLLM's robustness to perform better in complex environments.\nThe dataset and code for our experiments will be made publicly available.", "AI": {"tldr": "VLLMs\u5728\u6807\u51c6\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u590d\u6742\u3001\u6742\u4e71\u4e14\u5305\u542b\u53d8\u5f62\u7269\u4f53\u7684\u73af\u5883\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u5f15\u5165\u4e00\u4e2a\u9488\u5bf9\u771f\u5b9e\u4e16\u754c\u5783\u573e\u5206\u7c7b\u7684\u65b0\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u65e8\u5728\u8bc4\u4f30VLLM\u5728\u6b64\u7c7b\u6311\u6218\u6027\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u6307\u51faVLLM\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1VLLMs\u5728\u6807\u51c6\u81ea\u7136\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u590d\u6742\u73af\u5883\u3001\u6742\u4e71\u6570\u636e\u96c6\u4ee5\u53ca\u5305\u542b\u53d8\u5f62\u7269\u4f53\u7684\u573a\u666f\uff08\u5982\u771f\u5b9e\u4e16\u754c\u5783\u573e\u5206\u7c7b\uff09\u4e2d\u7684\u80fd\u529b\u548c\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u9488\u5bf9\u771f\u5b9e\u4e16\u754c\u5783\u573e\u5206\u7c7b\u7684\u65b0\u578b\u6570\u636e\u96c6\uff0c\u5176\u7279\u70b9\u662f\u590d\u6742\u73af\u5883\u548c\u53d8\u5f62\u7269\u4f53\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5165\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e25\u683c\u8bc4\u4f30VLLMs\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "result": "\u5f15\u5165\u7684\u6570\u636e\u96c6\u548c\u5168\u9762\u7684\u5206\u6790\u4e3aVLLMs\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03VLLMs\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u597d\u9700\u8981\u8fdb\u4e00\u6b65\u63d0\u5347\u9c81\u68d2\u6027\u3002\u8be5\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2509.00091", "pdf": "https://arxiv.org/pdf/2509.00091", "abs": "https://arxiv.org/abs/2509.00091", "authors": ["Ephraiem Sarabamoun"], "title": "Ensemble Debates with Local Large Language Models for AI Alignment", "categories": ["cs.AI", "cs.CL"], "comment": "9 pages, 2 tables", "summary": "As large language models (LLMs) take on greater roles in high-stakes\ndecisions, alignment with human values is essential. Reliance on proprietary\nAPIs limits reproducibility and broad participation. We study whether local\nopen-source ensemble debates can improve alignmentoriented reasoning. Across\n150 debates spanning 15 scenarios and five ensemble configurations, ensembles\noutperform single-model baselines on a 7-point rubric (overall: 3.48 vs. 3.13),\nwith the largest gains in reasoning depth (+19.4%) and argument quality\n(+34.1%). Improvements are strongest for truthfulness (+1.25 points) and human\nenhancement (+0.80). We provide code, prompts, and a debate data set, providing\nan accessible and reproducible foundation for ensemble-based alignment\nevaluation.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u672c\u5730\u5f00\u6e90\u96c6\u6210\u8fa9\u8bba\u6a21\u578b\u76f8\u6bd4\u5355\u4e00\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ef7\u503c\u5bf9\u9f50\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u63a8\u7406\u6df1\u5ea6\u548c\u8bba\u8bc1\u8d28\u91cf\u65b9\u9762\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u57fa\u7840\u3002", "motivation": "\u9274\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5173\u952e\u51b3\u7b56\u4e2d\u7684\u65e5\u76ca\u91cd\u8981\u4f5c\u7528\uff0c\u786e\u4fdd\u5176\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002\u76ee\u524d\u5bf9\u4e13\u6709API\u7684\u4f9d\u8d56\u9650\u5236\u4e86\u7814\u7a76\u7684\u590d\u73b0\u6027\u548c\u5e7f\u6cdb\u53c2\u4e0e\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5f00\u6e90\u4e14\u53ef\u590d\u73b0\u7684\u65b9\u6cd5\u6765\u63d0\u5347LLM\u7684\u5bf9\u9f50\u63a8\u7406\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u672c\u5730\u5f00\u6e90\u96c6\u6210\u8fa9\u8bba\u662f\u5426\u80fd\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u5bfc\u5411\u63a8\u7406\u80fd\u529b\u3002\u901a\u8fc7\u572815\u4e2a\u573a\u666f\u548c5\u79cd\u96c6\u6210\u914d\u7f6e\u4e0b\u8fdb\u884c150\u573a\u8fa9\u8bba\uff0c\u7814\u7a76\u4eba\u5458\u4f7f\u75287\u70b9\u91cf\u8868\u8bc4\u4f30\u4e86\u96c6\u6210\u6a21\u578b\u76f8\u5bf9\u4e8e\u5355\u4e00\u6a21\u578b\u57fa\u7ebf\u7684\u6027\u80fd\u3002", "result": "\u96c6\u6210\u6a21\u578b\u57287\u70b9\u91cf\u8868\u4e0a\u7684\u603b\u4f53\u8868\u73b0\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u57fa\u7ebf\uff083.48 vs. 3.13\uff09\uff0c\u5728\u63a8\u7406\u6df1\u5ea6\uff08+19.4%\uff09\u548c\u8bba\u8bc1\u8d28\u91cf\uff08+34.1%\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5927\u589e\u76ca\u3002\u5728\u771f\u5b9e\u6027\uff08+1.25\u5206\uff09\u548c\u4eba\u7c7b\u589e\u5f3a\uff08+0.80\u5206\uff09\u65b9\u9762\uff0c\u6539\u8fdb\u6700\u4e3a\u663e\u8457\u3002", "conclusion": "\u672c\u5730\u5f00\u6e90\u96c6\u6210\u8fa9\u8bba\u662f\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u590d\u73b0\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u5bfc\u5411\u63a8\u7406\u80fd\u529b\u3002\u7814\u7a76\u8005\u63d0\u4f9b\u7684\u4ee3\u7801\u3001\u63d0\u793a\u548c\u8fa9\u8bba\u6570\u636e\u96c6\u4e3a\u57fa\u4e8e\u96c6\u6210\u6a21\u578b\u7684\u5bf9\u9f50\u8bc4\u4f30\u5960\u5b9a\u4e86\u53ef\u8bbf\u95ee\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u7840\u3002"}}
{"id": "2509.00036", "pdf": "https://arxiv.org/pdf/2509.00036", "abs": "https://arxiv.org/abs/2509.00036", "authors": ["Cheng Jin", "Zhenyu Xiao", "Yuantao Gu"], "title": "A-FloPS: Accelerating Diffusion Sampling with Adaptive Flow Path Sampler", "categories": ["cs.LG", "cs.CV", "68T07, 60H10, 65C30", "I.2.6; G.1.7"], "comment": "14 pages,9 figures", "summary": "Diffusion models deliver state-of-the-art generative performance across\ndiverse modalities but remain computationally expensive due to their inherently\niterative sampling process. Existing training-free acceleration methods\ntypically improve numerical solvers for the reverse-time ODE, yet their\neffectiveness is fundamentally constrained by the inefficiency of the\nunderlying sampling trajectories. We propose A-FloPS (Adaptive Flow Path\nSampler), a principled, training-free framework that reparameterizes the\nsampling trajectory of any pre-trained diffusion model into a flow-matching\nform and augments it with an adaptive velocity decomposition. The\nreparameterization analytically maps diffusion scores to flow-compatible\nvelocities, yielding integration-friendly trajectories without retraining. The\nadaptive mechanism further factorizes the velocity field into a linear drift\nterm and a residual component whose temporal variation is actively suppressed,\nrestoring the accuracy benefits of high-order integration even in extremely\nlow-NFE regimes. Extensive experiments on conditional image generation and\ntext-to-image synthesis show that A-FloPS consistently outperforms\nstate-of-the-art training-free samplers in both sample quality and efficiency.\nNotably, with as few as $5$ function evaluations, A-FloPS achieves\nsubstantially lower FID and generates sharper, more coherent images. The\nadaptive mechanism also improves native flow-based generative models,\nunderscoring its generality. These results position A-FloPS as a versatile and\neffective solution for high-quality, low-latency generative modeling.", "AI": {"tldr": "A-FloPS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8f68\u8ff9\u91cd\u65b0\u53c2\u6570\u5316\u4e3a\u6d41\u5339\u914d\u5f62\u5f0f\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u901f\u5ea6\u5206\u89e3\uff0c\u663e\u8457\u52a0\u901f\u4e86\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6837\u672c\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u5728\u751f\u6210\u6027\u80fd\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u4f46\u5176\u8fed\u4ee3\u91c7\u6837\u8fc7\u7a0b\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u65e0\u9700\u8bad\u7ec3\u7684\u52a0\u901f\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u9006\u65f6ODE\u7684\u6570\u503c\u6c42\u89e3\u5668\uff0c\u4f46\u53d7\u9650\u4e8e\u5e95\u5c42\u91c7\u6837\u8f68\u8ff9\u7684\u4f4e\u6548\u6027\u3002", "method": "A-FloPS\u5c06\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8f68\u8ff9\u91cd\u65b0\u53c2\u6570\u5316\u4e3a\u6d41\u5339\u914d\u5f62\u5f0f\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u901f\u5ea6\u5206\u89e3\u8fdb\u884c\u589e\u5f3a\u3002\u8be5\u91cd\u53c2\u6570\u5316\u5c06\u6269\u6563\u5206\u6570\u89e3\u6790\u5730\u6620\u5c04\u5230\u4e0e\u6d41\u517c\u5bb9\u7684\u901f\u5ea6\uff0c\u4ece\u800c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u4ea7\u751f\u6613\u4e8e\u79ef\u5206\u7684\u8f68\u8ff9\u3002\u81ea\u9002\u5e94\u673a\u5236\u8fdb\u4e00\u6b65\u5c06\u901f\u5ea6\u573a\u5206\u89e3\u4e3a\u7ebf\u6027\u6f02\u79fb\u9879\u548c\u6b8b\u5dee\u5206\u91cf\uff0c\u5e76\u79ef\u6781\u6291\u5236\u6b8b\u5dee\u5206\u91cf\u7684\u65f6\u95f4\u53d8\u5316\uff0c\u4ee5\u5728\u6781\u4f4eNFE\uff08\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\uff09\u4e0b\u6062\u590d\u9ad8\u9636\u79ef\u5206\u7684\u51c6\u786e\u6027\u4f18\u52bf\u3002", "result": "\u5728\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u548c\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u65b9\u9762\uff0cA-FloPS\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65e0\u9700\u8bad\u7ec3\u91c7\u6837\u5668\uff0c\u65e0\u8bba\u662f\u5728\u6837\u672c\u8d28\u91cf\u8fd8\u662f\u6548\u7387\u4e0a\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cA-FloPS\u5728\u4ec5\u67095\u6b21\u51fd\u6570\u8bc4\u4f30\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u4f4e\u7684FID\uff0c\u5e76\u751f\u6210\u4e86\u66f4\u9510\u5229\u3001\u66f4\u8fde\u8d2f\u7684\u56fe\u50cf\u3002\u81ea\u9002\u5e94\u673a\u5236\u4e5f\u6539\u8fdb\u4e86\u539f\u751f\u7684\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u3002", "conclusion": "A-FloPS\u88ab\u5b9a\u4f4d\u4e3a\u4e00\u79cd\u591a\u529f\u80fd\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684\u751f\u6210\u5efa\u6a21\u3002"}}
{"id": "2509.00248", "pdf": "https://arxiv.org/pdf/2509.00248", "abs": "https://arxiv.org/abs/2509.00248", "authors": ["Zachary K. Stine", "James E. Deitrick"], "title": "The Differential Meaning of Models: A Framework for Analyzing the Structural Consequences of Semantic Modeling Decisions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The proliferation of methods for modeling of human meaning-making constitutes\na powerful class of instruments for the analysis of complex semiotic systems.\nHowever, the field lacks a general theoretical framework for describing these\nmodeling practices across various model types in an apples-to-apples way. In\nthis paper, we propose such a framework grounded in the semiotic theory of C.\nS. Peirce. We argue that such models measure latent symbol geometries, which\ncan be understood as hypotheses about the complex of semiotic agencies\nunderlying a symbolic dataset. Further, we argue that in contexts where a\nmodel's value cannot be straightforwardly captured by proxy measures of\nperformance, models can instead be understood relationally, so that the\nparticular interpretive lens of a model becomes visible through its contrast\nwith other models. This forms the basis of a theory of model semantics in which\nmodels, and the modeling decisions that constitute them, are themselves treated\nas signs. In addition to proposing the framework, we illustrate its empirical\nuse with a few brief examples and consider foundational questions and future\ndirections enabled by the framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eC. S. Peirce\u7b26\u53f7\u5b66\u7406\u8bba\u7684\u901a\u7528\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u63cf\u8ff0\u548c\u6bd4\u8f83\u5404\u79cd\u4eba\u7c7b\u610f\u4e49\u6784\u5efa\u6a21\u578b\uff0c\u5e76\u5c06\u6a21\u578b\u53ca\u5176\u51b3\u7b56\u89c6\u4e3a\u7b26\u53f7\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u7c7b\u610f\u4e49\u6784\u5efa\u6a21\u578b\u65b9\u6cd5\u4f17\u591a\uff0c\u4f46\u9886\u57df\u5185\u7f3a\u4e4f\u4e00\u4e2a\u901a\u7528\u7684\u7406\u8bba\u6846\u67b6\uff0c\u65e0\u6cd5\u4ee5\u7edf\u4e00\u7684\u65b9\u5f0f\u63cf\u8ff0\u548c\u6bd4\u8f83\u8fd9\u4e9b\u4e0d\u540c\u7c7b\u578b\u7684\u5efa\u6a21\u5b9e\u8df5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eC. S. Peirce\u7b26\u53f7\u5b66\u7406\u8bba\u7684\u901a\u7528\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u6a21\u578b\u89c6\u4e3a\u6d4b\u91cf\u6f5c\u5728\u7b26\u53f7\u51e0\u4f55\u4f53\uff0c\u5e76\u5c06\u5176\u7406\u89e3\u4e3a\u5173\u4e8e\u7b26\u53f7\u6570\u636e\u80cc\u540e\u590d\u6742\u7b26\u53f7\u80fd\u52a8\u6027\u7684\u5047\u8bbe\u3002\u6b64\u5916\uff0c\u5f53\u6a21\u578b\u4ef7\u503c\u65e0\u6cd5\u901a\u8fc7\u6027\u80fd\u4ee3\u7406\u6307\u6807\u76f4\u63a5\u8861\u91cf\u65f6\uff0c\u8be5\u6846\u67b6\u5efa\u8bae\u901a\u8fc7\u4e0e\u5176\u4ed6\u6a21\u578b\u7684\u5bf9\u6bd4\u6765\u5173\u7cfb\u6027\u5730\u7406\u89e3\u6a21\u578b\uff0c\u4ece\u800c\u5f62\u6210\u4e00\u4e2a\u6a21\u578b\u8bed\u4e49\u7406\u8bba\uff0c\u5176\u4e2d\u6a21\u578b\u53ca\u5176\u5efa\u6a21\u51b3\u7b56\u672c\u8eab\u88ab\u89c6\u4e3a\u7b26\u53f7\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u7edf\u4e00\u63cf\u8ff0\u548c\u6bd4\u8f83\u4eba\u7c7b\u610f\u4e49\u6784\u5efa\u6a21\u578b\u7684\u901a\u7528\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u53d1\u5c55\u4e86\u4e00\u4e2a\u6a21\u578b\u8bed\u4e49\u7406\u8bba\uff0c\u5c06\u6a21\u578b\u89c6\u4e3a\u7b26\u53f7\u3002\u901a\u8fc7\u7b80\u77ed\u7684\u5b9e\u8bc1\u4f8b\u5b50\uff0c\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u6240\u542f\u7528\u7684\u57fa\u7840\u6027\u95ee\u9898\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u63cf\u8ff0\u3001\u6bd4\u8f83\u548c\u89e3\u91ca\u5404\u79cd\u4eba\u7c7b\u610f\u4e49\u6784\u5efa\u6a21\u578b\u7684\u8fde\u8d2f\u65b9\u5f0f\uff0c\u5c06\u6a21\u578b\u53ca\u5176\u5efa\u6a21\u51b3\u7b56\u672c\u8eab\u89c6\u4e3a\u7b26\u53f7\uff0c\u4e3a\u7406\u89e3\u8fd9\u4e9b\u590d\u6742\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u5f00\u542f\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.00286", "pdf": "https://arxiv.org/pdf/2509.00286", "abs": "https://arxiv.org/abs/2509.00286", "authors": ["Rakshitha De Silva", "Shiva Raj Pokhrel", "Jonathan Kua", "Sithamparanathan Kandeepan"], "title": "Intelligent Spectrum Management in Satellite Communications", "categories": ["cs.NI", "cs.AI"], "comment": "30 pages, Under review in IEEE Communications Surveys & Tutorials", "summary": "Satellite Communication (SatCom) networks represent a fundamental pillar in\nmodern global connectivity, facilitating reliable service and extensive\ncoverage across a plethora of applications. The expanding demand for\nhigh-bandwidth services and the proliferation of mega satellite constellations\nhighlight the limitations of traditional exclusive satellite spectrum\nallocation approaches. Cognitive Radio (CR) leading to Cognitive Satellite\n(CogSat) networks through Dynamic Spectrum Management (DSM), which enables the\ndynamic adaptability of radio equipment to environmental conditions for optimal\nperformance, presents a promising solution for the emerging spectrum scarcity.\nIn this survey, we explore the adaptation of intelligent DSM methodologies to\nSatCom, leveraging satellite network integrations. We discuss contributions and\nhurdles in regulations and standardizations in realizing intelligent DSM in\nSatCom, and deep dive into DSM techniques, which enable CogSat networks.\nFurthermore, we extensively evaluate and categorize state-of-the-art Artificial\nIntelligence (AI)/Machine Learning (ML) methods leveraged for DSM while\nexploring operational resilience and robustness of such integrations. In\naddition, performance evaluation metrics critical for adaptive resource\nmanagement and system optimization in CogSat networks are thoroughly\ninvestigated. This survey also identifies open challenges and outlines future\nresearch directions in regulatory frameworks, network architectures, and\nintelligent spectrum management, paving the way for sustainable and scalable\nSatCom networks for enhanced global connectivity.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u8ba4\u77e5\u65e0\u7ebf\u7535/\u536b\u661f\uff08CogSat\uff09\u548c\u52a8\u6001\u9891\u8c31\u7ba1\u7406\uff08DSM\uff09\uff0c\u7279\u522b\u662f\u5229\u7528\u4eba\u5de5\u667a\u80fd/\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u89e3\u51b3\u536b\u661f\u901a\u4fe1\uff08SatCom\uff09\u65e5\u76ca\u589e\u957f\u7684\u9891\u8c31\u7a00\u7f3a\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u5168\u7403\u8fde\u63a5\u6027\u7684\u589e\u5f3a\u3002", "motivation": "\u73b0\u4ee3\u536b\u661f\u901a\u4fe1\u7f51\u7edc\u5bf9\u9ad8\u5e26\u5bbd\u670d\u52a1\u7684\u9700\u6c42\u4e0d\u65ad\u6269\u5927\uff0c\u4ee5\u53ca\u5de8\u578b\u536b\u661f\u661f\u5ea7\u7684\u6fc0\u589e\uff0c\u66b4\u9732\u51fa\u4f20\u7edf\u72ec\u5360\u5f0f\u536b\u661f\u9891\u8c31\u5206\u914d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u9891\u8c31\u7a00\u7f3a\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7ba1\u7406\u65b9\u6848\u3002", "method": "\u672c\u6587\u91c7\u7528\u7efc\u8ff0\u7814\u7a76\u65b9\u6cd5\uff0c\u63a2\u8ba8\u5c06\u667a\u80fd\u52a8\u6001\u9891\u8c31\u7ba1\u7406\uff08DSM\uff09\u6280\u672f\u9002\u5e94\u4e8e\u536b\u661f\u901a\u4fe1\uff08SatCom\uff09\uff0c\u4ee5\u5b9e\u73b0\u8ba4\u77e5\u536b\u661f\uff08CogSat\uff09\u7f51\u7edc\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u8ba8\u8bba\u76f8\u5173\u6cd5\u89c4\u548c\u6807\u51c6\u5316\u7684\u8d21\u732e\u4e0e\u969c\u788d\uff0c\u6df1\u5165\u5206\u6790DSM\u6280\u672f\uff0c\u8bc4\u4f30\u5e76\u5206\u7c7b\u7528\u4e8eDSM\u7684\u5148\u8fdbAI/ML\u65b9\u6cd5\uff0c\u63a2\u8ba8\u5176\u8fd0\u884c\u97e7\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u5e76\u7814\u7a76\u5173\u952e\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u672c\u7efc\u8ff0\u63a2\u7d22\u4e86\u667a\u80fdDSM\u5728SatCom\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u8ba8\u8bba\u4e86\u5728\u6cd5\u89c4\u548c\u6807\u51c6\u5316\u65b9\u9762\u7684\u8d21\u732e\u4e0e\u969c\u788d\uff0c\u6df1\u5165\u5206\u6790\u4e86DSM\u6280\u672f\u4ee5\u5b9e\u73b0CogSat\u7f51\u7edc\u3002\u6b64\u5916\uff0c\u672c\u6587\u5e7f\u6cdb\u8bc4\u4f30\u548c\u5206\u7c7b\u4e86\u7528\u4e8eDSM\u7684AI/ML\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u96c6\u6210\u7684\u8fd0\u884c\u97e7\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u8003\u5bdf\u4e86\u5173\u952e\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "\u667a\u80fd\u52a8\u6001\u9891\u8c31\u7ba1\u7406\uff0c\u7279\u522b\u662f\u7ed3\u5408AI/ML\u6280\u672f\uff0c\u4e3a\u89e3\u51b3\u536b\u661f\u901a\u4fe1\u7684\u9891\u8c31\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u8ba4\u77e5\u536b\u661f\uff08CogSat\uff09\u7f51\u7edc\uff0c\u53ef\u4ee5\u6784\u5efa\u53ef\u6301\u7eed\u3001\u53ef\u6269\u5c55\u7684\u536b\u661f\u901a\u4fe1\u7f51\u7edc\uff0c\u4ece\u800c\u589e\u5f3a\u5168\u7403\u8fde\u63a5\u6027\u3002\u672c\u7efc\u8ff0\u8fd8\u660e\u786e\u4e86\u76d1\u7ba1\u6846\u67b6\u3001\u7f51\u7edc\u67b6\u6784\u548c\u667a\u80fd\u9891\u8c31\u7ba1\u7406\u9886\u57df\u7684\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.00177", "pdf": "https://arxiv.org/pdf/2509.00177", "abs": "https://arxiv.org/abs/2509.00177", "authors": ["Faizan Farooq Khan", "Vladan Stojni\u0107", "Zakaria Laskar", "Mohamed Elhoseiny", "Giorgos Tolias"], "title": "Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders", "categories": ["cs.CV"], "comment": "BMVC 2025", "summary": "This work explores text-to-image retrieval for queries that specify or\ndescribe a semantic category. While vision-and-language models (VLMs) like CLIP\noffer a straightforward open-vocabulary solution, they map text and images to\ndistant regions in the representation space, limiting retrieval performance. To\nbridge this modality gap, we propose a two-step approach. First, we transform\nthe text query into a visual query using a generative diffusion model. Then, we\nestimate image-to-image similarity with a vision model. Additionally, we\nintroduce an aggregation network that combines multiple generated images into a\nsingle vector representation and fuses similarity scores across both query\nmodalities. Our approach leverages advancements in vision encoders, VLMs, and\ntext-to-image generation models. Extensive evaluations show that it\nconsistently outperforms retrieval methods relying solely on text queries.\nSource code is available at: https://github.com/faixan-khan/cletir", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4e24\u6b65\u6cd5\u8fdb\u884c\u8bed\u4e49\u7c7b\u522b\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\uff1a\u9996\u5148\u5229\u7528\u751f\u6210\u6269\u6563\u6a21\u578b\u5c06\u6587\u672c\u67e5\u8be2\u8f6c\u6362\u4e3a\u89c6\u89c9\u67e5\u8be2\uff0c\u7136\u540e\u901a\u8fc7\u89c6\u89c9\u6a21\u578b\u8ba1\u7b97\u56fe\u50cf\u95f4\u76f8\u4f3c\u5ea6\uff0c\u5e76\u5f15\u5165\u805a\u5408\u7f51\u7edc\u878d\u5408\u591a\u6a21\u6001\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u6587\u672c\u67e5\u8be2\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u4e2d\uff0c\u5c06\u6587\u672c\u548c\u56fe\u50cf\u6620\u5c04\u5230\u8868\u793a\u7a7a\u95f4\u4e2d\u8ddd\u79bb\u8f83\u8fdc\u7684\u533a\u57df\uff0c\u9650\u5236\u4e86\u68c0\u7d22\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u6307\u5b9a\u6216\u63cf\u8ff0\u8bed\u4e49\u7c7b\u522b\u7684\u67e5\u8be2\u65f6\u5b58\u5728\u6a21\u6001\u9e3f\u6c9f\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u4e24\u6b65\u6cd5\u6765\u5f25\u5408\u6a21\u6001\u9e3f\u6c9f\uff1a1. \u4f7f\u7528\u751f\u6210\u6269\u6563\u6a21\u578b\u5c06\u6587\u672c\u67e5\u8be2\u8f6c\u6362\u4e3a\u89c6\u89c9\u67e5\u8be2\u30022. \u4f7f\u7528\u89c6\u89c9\u6a21\u578b\u8bc4\u4f30\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u76f8\u4f3c\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u805a\u5408\u7f51\u7edc\uff0c\u7528\u4e8e\u5c06\u591a\u4e2a\u751f\u6210\u7684\u56fe\u50cf\u7ec4\u5408\u6210\u5355\u4e2a\u5411\u91cf\u8868\u793a\uff0c\u5e76\u878d\u5408\u8de8\u4e24\u79cd\u67e5\u8be2\u6a21\u6001\u7684\u76f8\u4f3c\u6027\u5206\u6570\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u6587\u672c\u67e5\u8be2\u7684\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ed3\u5408\u4e86\u6587\u672c\u5230\u89c6\u89c9\u751f\u6210\u548c\u56fe\u50cf\u5230\u56fe\u50cf\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u7684\u4e24\u6b65\u6cd5\uff0c\u8f85\u4ee5\u805a\u5408\u7f51\u7edc\uff0c\u80fd\u6709\u6548\u5f25\u5408\u6587\u672c\u4e0e\u56fe\u50cf\u4e4b\u95f4\u7684\u6a21\u6001\u9e3f\u6c9f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u7c7b\u522b\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u7684\u6027\u80fd\u3002"}}
{"id": "2509.00100", "pdf": "https://arxiv.org/pdf/2509.00100", "abs": "https://arxiv.org/abs/2509.00100", "authors": ["Rahul Anand"], "title": "MODE: Mixture of Document Experts for RAG", "categories": ["cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter.", "AI": {"tldr": "MODE\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7RAG\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u805a\u7c7b\u548c\u8def\u7531\u68c0\u7d22\uff0c\u4e3a\u5c0f\u578b\u9886\u57df\u7279\u5b9a\u8bed\u6599\u5e93\u63d0\u4f9b\u9ad8\u6548\u3001\u51c6\u786e\u7684\u7b54\u6848\uff0c\u65e0\u9700\u5927\u578b\u5411\u91cf\u6570\u636e\u5e93\u548c\u91cd\u6392\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u5e38\u4f9d\u8d56\u5927\u578b\u5411\u91cf\u6570\u636e\u5e93\u548c\u4ea4\u53c9\u7f16\u7801\u5668\uff0c\u5bf9\u4e8e\u5c0f\u578b\u3001\u9886\u57df\u7279\u5b9a\u7684\u8bed\u6599\u5e93\u6765\u8bf4\u8fc7\u4e8e\u590d\u6742\u548c\u8d44\u6e90\u5bc6\u96c6\u3002", "method": "\u672c\u6587\u63d0\u51faMODE\uff08Mixture of Document Experts\uff09\uff0c\u7528\u201c\u805a\u7c7b\u4e0e\u8def\u7531\u201d\u68c0\u7d22\u66ff\u4ee3\u7ec6\u7c92\u5ea6\u8fd1\u90bb\u641c\u7d22\u3002\u6587\u6863\u88ab\u5d4c\u5165\u3001\u805a\u7c7b\u6210\u8bed\u4e49\u8fde\u8d2f\u7684\u7ec4\uff0c\u5e76\u7531\u7f13\u5b58\u7684\u8d28\u5fc3\u8868\u793a\u3002\u67e5\u8be2\u65f6\uff0c\u7cfb\u7edf\u8def\u7531\u5230\u6700\u4f73\u8d28\u5fc3\uff0c\u4ec5\u5728\u8fd9\u4e9b\u96c6\u7fa4\u5185\u68c0\u7d22\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u65e0\u9700\u5916\u90e8\u5411\u91cf\u6570\u636e\u5e93\u548c\u91cd\u6392\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u3002", "result": "\u5728HotpotQA\u548cSQuAD\uff08100-500\u4e2a\u5206\u5757\uff09\u4e0a\uff0cMODE\u7684\u7b54\u6848\u8d28\u91cf\u4e0e\u5bc6\u96c6\u68c0\u7d22\u57fa\u7ebf\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u5e76\u7f29\u77ed\u4e86\u7aef\u5230\u7aef\u68c0\u7d22\u65f6\u95f4\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0c\u96c6\u7fa4\u7c92\u5ea6\u548c\u591a\u96c6\u7fa4\u8def\u7531\u63a7\u5236\u53ec\u56de\u7387/\u51c6\u786e\u7387\u6743\u8861\uff0c\u66f4\u7d27\u5bc6\u7684\u96c6\u7fa4\u80fd\u63d0\u9ad8\u4e0b\u6e38\u51c6\u786e\u6027\u3002", "conclusion": "MODE\u4e3a\u5c0f\u578b\u548c\u4e2d\u578b\u8bed\u6599\u5e93\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\uff0c\u7b80\u6d01\u6027\u3001\u901f\u5ea6\u548c\u4e3b\u9898\u805a\u7126\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.00046", "pdf": "https://arxiv.org/pdf/2509.00046", "abs": "https://arxiv.org/abs/2509.00046", "authors": ["Chunming Ye", "Songzhou Li", "Xu Xu"], "title": "Exploring and Reshaping the Weight Distribution in LLM", "categories": ["cs.LG", "cs.AI", "68T50", "I.2.7"], "comment": "19 pages,16 figures", "summary": "The performance of Large Language Models is influenced by their\ncharacteristics such as architecture, model sizes, decoding methods and so on.\nDue to differences in structure or function, the weights in different layers of\nlarge models have varying distributions. This paper explores the correlations\nbetween different types of layers in terms of weights distribution and studies\nthe potential impact of these correlations on LoRA training effectiveness.\nFirstly, the study reveals that in the model the cosine distances between\nweights of different layers manifest power-law distribution. We extract\nQuery-projection, down-projection and other weight matrices from the\nself-attention layers and MLP layers, calculate the singular values of the\nmatrices using singular value decomposition, and organize a certain number of\nsingular values into matrices according to projection's type. By analyzing the\nprobability distribution of the cosine distances between these matrices, it is\nfound that the cosine distances values between them have distinct power-law\ndistribution characteristics. Secondly, based on the results of distance\ncalculations and analysis across different layers of model, a qualitative\nmethod is proposed to describe the distribution characteristics of different\nmodels. Next, to construct weights that align with the distribution\ncharacteristics, a data generator is designed using a combination of Gaussian\nprocess and Pareto distribution functions. The generator is used to simulate\nthe generation of data that aligns with specific distribution characteristics.\nFinally, based on the aforementioned distribution characteristics and data\ngeneration method, the weights in LoRA initialization are reshaped for\ntraining. Experimental results indicate that, without altering the model\nstructure or training process, this method achieves a certain improvement in\nthe performance of LoRA training.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0d\u540c\u5c42\u95f4\u6743\u91cd\u5206\u5e03\u7684\u76f8\u5173\u6027\uff0c\u53d1\u73b0\u5176\u4f59\u5f26\u8ddd\u79bb\u5448\u5e42\u5f8b\u5206\u5e03\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e00\u79cd\u91cd\u5851LoRA\u521d\u59cb\u5316\u6743\u91cd\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u63d0\u5347LoRA\u8bad\u7ec3\u6027\u80fd\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0d\u540c\u5c42\u6743\u91cd\u5206\u5e03\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5e76\u5206\u6790\u8fd9\u4e9b\u76f8\u5173\u6027\u5bf9LoRA\u8bad\u7ec3\u6548\u679c\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "1. \u63d0\u53d6\u6a21\u578b\uff08\u5982\u81ea\u6ce8\u610f\u529b\u5c42\u3001MLP\u5c42\uff09\u7684\u6743\u91cd\u77e9\u9635\uff08\u5982Query-projection\u3001down-projection\uff09\uff0c\u8fdb\u884c\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u30022. \u8ba1\u7b97\u5e76\u5206\u6790\u5947\u5f02\u503c\u77e9\u9635\u95f4\u7684\u4f59\u5f26\u8ddd\u79bb\uff0c\u53d1\u73b0\u5176\u5177\u6709\u5e42\u5f8b\u5206\u5e03\u7279\u5f81\u30023. \u57fa\u4e8e\u8ddd\u79bb\u5206\u6790\u7ed3\u679c\uff0c\u63d0\u51fa\u63cf\u8ff0\u4e0d\u540c\u6a21\u578b\u5206\u5e03\u7279\u6027\u7684\u5b9a\u6027\u65b9\u6cd5\u30024. \u8bbe\u8ba1\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u548c\u5e15\u7d2f\u6258\u5206\u5e03\u51fd\u6570\u7684\u6570\u636e\u751f\u6210\u5668\uff0c\u6a21\u62df\u751f\u6210\u7b26\u5408\u7279\u5b9a\u5206\u5e03\u7279\u5f81\u7684\u6743\u91cd\u6570\u636e\u30025. \u5229\u7528\u8fd9\u4e9b\u5206\u5e03\u7279\u5f81\u548c\u751f\u6210\u65b9\u6cd5\uff0c\u91cd\u5851LoRA\u7684\u521d\u59cb\u5316\u6743\u91cd\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "1. \u6a21\u578b\u4e2d\u4e0d\u540c\u5c42\u6743\u91cd\u4e4b\u95f4\u7684\u4f59\u5f26\u8ddd\u79bb\u5c55\u73b0\u51fa\u663e\u8457\u7684\u5e42\u5f8b\u5206\u5e03\u7279\u5f81\u30022. \u5728\u4e0d\u6539\u53d8\u6a21\u578b\u7ed3\u6784\u6216\u8bad\u7ec3\u8fc7\u7a0b\u7684\u524d\u63d0\u4e0b\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728LoRA\u8bad\u7ec3\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u4e00\u5b9a\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u7406\u89e3\u548c\u5229\u7528\u5927\u578b\u6a21\u578b\u5c42\u95f4\u6743\u91cd\u5206\u5e03\u7684\u5e42\u5f8b\u7279\u6027\uff0c\u5e76\u4f18\u5316LoRA\u521d\u59cb\u5316\u6743\u91cd\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347LoRA\u7684\u8bad\u7ec3\u8868\u73b0\u3002"}}
{"id": "2509.00250", "pdf": "https://arxiv.org/pdf/2509.00250", "abs": "https://arxiv.org/abs/2509.00250", "authors": ["Hugo Sousa", "Ricardo Campos", "Al\u00edpio Jorge"], "title": "The Temporal Game: A New Perspective on Temporal Relation Extraction", "categories": ["cs.CL"], "comment": null, "summary": "In this paper we demo the Temporal Game, a novel approach to temporal\nrelation extraction that casts the task as an interactive game. Instead of\ndirectly annotating interval-level relations, our approach decomposes them into\npoint-wise comparisons between the start and end points of temporal entities.\nAt each step, players classify a single point relation, and the system applies\ntemporal closure to infer additional relations and enforce consistency. This\npoint-based strategy naturally supports both interval and instant entities,\nenabling more fine-grained and flexible annotation than any previous approach.\nThe Temporal Game also lays the groundwork for training reinforcement learning\nagents, by treating temporal annotation as a sequential decision-making task.\nTo showcase this potential, the demo presented in this paper includes a Game\nmode, in which users annotate texts from the TempEval-3 dataset and receive\nfeedback based on a scoring system, and an Annotation mode, that allows custom\ndocuments to be annotated and resulting timeline to be exported. Therefore,\nthis demo serves both as a research tool and an annotation interface. The demo\nis publicly available at https://temporal-game.inesctec.pt, and the source code\nis open-sourced to foster further research and community-driven development in\ntemporal reasoning and annotation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecdTemporal Game\uff0c\u4e00\u79cd\u5c06\u65f6\u95f4\u5173\u7cfb\u63d0\u53d6\u4efb\u52a1\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u6e38\u620f\u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u901a\u8fc7\u70b9\u5bf9\u70b9\u6bd4\u8f83\u5b9e\u73b0\u66f4\u7075\u6d3b\u3001\u7ec6\u7c92\u5ea6\u7684\u6807\u6ce8\uff0c\u5e76\u4e3a\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u8bad\u7ec3\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u7684\u533a\u95f4\u7ea7\u65f6\u95f4\u5173\u7cfb\u6807\u6ce8\u4e0d\u591f\u7075\u6d3b\u548c\u7ec6\u7c92\u5ea6\uff0c\u4e14\u672a\u5145\u5206\u5229\u7528\u4ea4\u4e92\u5f0f\u65b9\u6cd5\u6f5c\u529b\uff1b\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u7075\u6d3b\u7684\u6807\u6ce8\u65b9\u6cd5\uff0c\u5e76\u4e3a\u672a\u6765\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u8fdb\u884c\u65f6\u95f4\u6807\u6ce8\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u65f6\u95f4\u5173\u7cfb\u63d0\u53d6\u4efb\u52a1\u89c6\u4e3a\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6e38\u620f\uff0c\u5c06\u533a\u95f4\u7ea7\u5173\u7cfb\u5206\u89e3\u4e3a\u65f6\u95f4\u5b9e\u4f53\u8d77\u70b9\u548c\u7ec8\u70b9\u4e4b\u95f4\u7684\u70b9\u5bf9\u70b9\u6bd4\u8f83\u3002\u73a9\u5bb6\u5728\u6bcf\u4e00\u6b65\u5206\u7c7b\u4e00\u4e2a\u5355\u4e00\u7684\u65f6\u95f4\u70b9\u5173\u7cfb\uff0c\u7cfb\u7edf\u5e94\u7528\u65f6\u95f4\u95ed\u5305\u6765\u63a8\u65ad\u989d\u5916\u5173\u7cfb\u5e76\u5f3a\u5236\u4fdd\u6301\u4e00\u81f4\u6027\u3002\u8fd9\u79cd\u57fa\u4e8e\u70b9\u7684\u65b9\u6cd5\u540c\u65f6\u652f\u6301\u533a\u95f4\u548c\u77ac\u65f6\u5b9e\u4f53\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6f14\u793a\u7cfb\u7edf\uff0c\u5305\u542b\u6e38\u620f\u6a21\u5f0f\uff08\u7528\u6237\u5728TempEval-3\u6570\u636e\u96c6\u4e0a\u6807\u6ce8\u5e76\u83b7\u5f97\u8bc4\u5206\u53cd\u9988\uff09\u548c\u6807\u6ce8\u6a21\u5f0f\uff08\u5141\u8bb8\u6807\u6ce8\u81ea\u5b9a\u4e49\u6587\u6863\u5e76\u5bfc\u51fa\u65f6\u95f4\u7ebf\uff09\u3002\u8be5\u7cfb\u7edf\u4f5c\u4e3a\u7814\u7a76\u5de5\u5177\u548c\u6807\u6ce8\u754c\u9762\uff0c\u5df2\u516c\u5f00\u53d1\u5e03\u5e76\u5f00\u6e90\u3002", "conclusion": "Temporal Game\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u3001\u66f4\u7ec6\u7c92\u5ea6\u548c\u7075\u6d3b\u7684\u65f6\u95f4\u6807\u6ce8\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u63d0\u4f9b\u4e86\u57fa\u7840\u3002\u5176\u516c\u5f00\u53ef\u7528\u548c\u5f00\u6e90\u7684\u6f14\u793a\u7cfb\u7edf\u5c06\u4fc3\u8fdb\u65f6\u95f4\u63a8\u7406\u548c\u6807\u6ce8\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2509.00397", "pdf": "https://arxiv.org/pdf/2509.00397", "abs": "https://arxiv.org/abs/2509.00397", "authors": ["Murayyiam Parvez", "Annus Zulfiqar", "Roman Beltiukov", "Shir Landau Feibish", "Walter Willinger", "Arpit Gupta", "Muhammad Shahbaz"], "title": "SpliDT: Partitioned Decision Trees for Scalable Stateful Inference at Line Rate", "categories": ["cs.NI"], "comment": "12 pages", "summary": "Machine learning (ML) is increasingly being deployed in programmable data\nplanes (switches and SmartNICs) to enable real-time traffic analysis, security\nmonitoring, and in-network decision-making. Decision trees (DTs) are\nparticularly well-suited for these tasks due to their interpretability and\ncompatibility with data-plane architectures, i.e., match-action tables (MATs).\nHowever, existing in-network DT implementations are constrained by the need to\ncompute all input features upfront, forcing models to rely on a small, fixed\nset of features per flow. This significantly limits model accuracy and\nscalability under stringent hardware resource constraints.\n  We present SPLIDT, a system that rethinks DT deployment in the data plane by\nenabling partitioned inference over sliding windows of packets. SPLIDT\nintroduces two key innovations: (1) it assigns distinct, variable feature sets\nto individual sub-trees of a DT, grouped into partitions, and (2) it leverages\nan in-band control channel (via recirculation) to reuse data-plane resources\n(both stateful registers and match keys) across partitions at line rate. These\ninsights allow SPLIDT to scale the number of stateful features a model can use\nwithout exceeding hardware limits. To support this architecture, SPLIDT\nincorporates a custom training and design-space exploration (DSE) framework\nthat jointly optimizes feature allocation, tree partitioning, and DT model\ndepth. Evaluation across multiple real-world datasets shows that SPLIDT\nachieves higher accuracy while supporting up to 5x more stateful features than\nprior approaches (e.g., NetBeacon and Leo). It maintains the same low\ntime-to-detection (TTD) as these systems, while scaling to millions of flows\nwith minimal recirculation overhead (<0.05%).", "AI": {"tldr": "SPLIDT\u901a\u8fc7\u5206\u533a\u63a8\u7406\u548c\u8d44\u6e90\u590d\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u5e73\u9762\u51b3\u7b56\u6811\u7684\u7279\u5f81\u4f7f\u7528\u91cf\u3001\u51c6\u786e\u6027\u548c\u53ef\u4f38\u7f29\u6027\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u786c\u4ef6\u9650\u5236\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\uff08\u7279\u522b\u662f\u51b3\u7b56\u6811\uff09\u5728\u53ef\u7f16\u7a0b\u6570\u636e\u5e73\u9762\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u9700\u8981\u9884\u5148\u8ba1\u7b97\u6240\u6709\u8f93\u5165\u7279\u5f81\uff0c\u5bfc\u81f4\u6a21\u578b\u53ea\u80fd\u4f9d\u8d56\u5c11\u91cf\u56fa\u5b9a\u7279\u5f81\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u5728\u4e25\u683c\u786c\u4ef6\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684\u6a21\u578b\u51c6\u786e\u6027\u548c\u53ef\u4f38\u7f29\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86SPLIDT\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u6570\u636e\u5305\u6ed1\u52a8\u7a97\u53e3\u4e0a\u542f\u7528\u5206\u533a\u63a8\u7406\u6765\u91cd\u65b0\u601d\u8003\u6570\u636e\u5e73\u9762\u4e2d\u7684\u51b3\u7b56\u6811\u90e8\u7f72\u3002\u5176\u5173\u952e\u521b\u65b0\u5305\u62ec\uff1a1) \u4e3a\u51b3\u7b56\u6811\u7684\u72ec\u7acb\u5b50\u6811\u5206\u914d\u4e0d\u540c\u7684\u3001\u53ef\u53d8\u7684\u7279\u5f81\u96c6\uff0c\u5e76\u5c06\u5176\u5206\u7ec4\u5230\u5206\u533a\u4e2d\uff1b2) \u5229\u7528\u5e26\u5185\u63a7\u5236\u901a\u9053\uff08\u901a\u8fc7\u5faa\u73af\u56de\u704c\uff09\u4ee5\u7ebf\u901f\u8de8\u5206\u533a\u590d\u7528\u6570\u636e\u5e73\u9762\u8d44\u6e90\uff08\u5305\u62ec\u6709\u72b6\u6001\u5bc4\u5b58\u5668\u548c\u5339\u914d\u952e\uff09\u3002SPLIDT\u8fd8\u5305\u542b\u4e00\u4e2a\u81ea\u5b9a\u4e49\u8bad\u7ec3\u548c\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff08DSE\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u5408\u4f18\u5316\u7279\u5f81\u5206\u914d\u3001\u6811\u5206\u533a\u548c\u51b3\u7b56\u6811\u6a21\u578b\u6df1\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cSPLIDT\u5728\u4fdd\u6301\u4e0e\u73b0\u6709\u7cfb\u7edf\u76f8\u540c\u4f4e\u68c0\u6d4b\u65f6\u95f4\uff08TTD\uff09\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u5e76\u652f\u6301\u6bd4\u73b0\u6709\u65b9\u6cd5\uff08\u5982NetBeacon\u548cLeo\uff09\u591a\u8fbe5\u500d\u7684\u72b6\u6001\u7279\u5f81\u3002\u5b83\u80fd\u591f\u6269\u5c55\u5230\u6570\u767e\u4e07\u6d41\uff0c\u4e14\u5faa\u73af\u56de\u704c\u5f00\u9500\u6781\u5c0f\uff08<0.05%\uff09\u3002", "conclusion": "SPLIDT\u901a\u8fc7\u5176\u521b\u65b0\u7684\u5206\u533a\u63a8\u7406\u548c\u8d44\u6e90\u590d\u7528\u673a\u5236\uff0c\u6210\u529f\u514b\u670d\u4e86\u6570\u636e\u5e73\u9762\u51b3\u7b56\u6811\u5728\u7279\u5f81\u4f7f\u7528\u548c\u786c\u4ef6\u8d44\u6e90\u65b9\u9762\u7684\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3001\u7279\u5f81\u5229\u7528\u7387\u548c\u53ef\u4f38\u7f29\u6027\uff0c\u4e3a\u6570\u636e\u5e73\u9762\u4e2d\u7684\u5b9e\u65f6\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00192", "pdf": "https://arxiv.org/pdf/2509.00192", "abs": "https://arxiv.org/abs/2509.00192", "authors": ["Younggun Kim", "Sirnam Swetha", "Fazil Kagdi", "Mubarak Shah"], "title": "Safe-LLaVA: A Privacy-Preserving Vision-Language Dataset and Benchmark for Biometric Safety", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in vision-language tasks. However, these models often infer and\nreveal sensitive biometric attributes - such as race, gender, age, body weight,\nand eye color - even when such information is not explicitly requested. This\nraises critical concerns, particularly in real-world applications and\nsocially-sensitive domains. Despite increasing awareness, no publicly available\ndataset or benchmark exists to comprehensively evaluate or mitigate biometric\nleakage in MLLMs. To address this gap, we introduce PRISM (Privacy-aware\nEvaluation of Responses in Sensitive Modalities), a new benchmark designed to\nassess MLLMs on two fronts: (1) refuse biometric-related queries and (2)\nimplicit biometric leakage in general responses while maintaining semantic\nfaithfulness. Further, we conduct a detailed audit of the widely used LLaVA\ndatasets and uncover extensive biometric leakage across pretraining and\ninstruction data. To address this, we present Safe-LLaVA dataset, the first\nprivacy-preserving MLLM training dataset constructed by systematically removing\nexplicit and implicit biometric information from LLaVA dataset. Our evaluations\non PRISM reveal biometric leakages across MLLMs for different attributes,\nhighlighting the detailed privacy-violations. We also fine-tune a model on\nSafe-LLaVA dataset and show that it substantially reduces the biometric\nleakages. Together, Safe-LLaVA & PRISM set a new standard for privacy-aligned\ndevelopment and evaluation of MLLMs. The Safe-LLaVA dataset & PRISM benchmark\nare publicly available at https://huggingface.co/datasets/kyh9191/Safe-LLaVA,\nand the source code is available at\nhttps://github.com/Kimyounggun99/Safe-LLaVA.git.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5b58\u5728\u751f\u7269\u7279\u5f81\u4fe1\u606f\u6cc4\u9732\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86PRISM\u57fa\u51c6\u6d4b\u8bd5\u548cSafe-LLaVA\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u548c\u7f13\u89e3\u8fd9\u4e00\u9690\u79c1\u98ce\u9669\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u672a\u88ab\u660e\u786e\u8981\u6c42\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u4f1a\u63a8\u65ad\u5e76\u6cc4\u9732\u654f\u611f\u7684\u751f\u7269\u7279\u5f81\u5c5e\u6027\uff08\u5982\u79cd\u65cf\u3001\u6027\u522b\u3001\u5e74\u9f84\u7b49\uff09\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u548c\u654f\u611f\u9886\u57df\u4e2d\u5f15\u53d1\u4e86\u4e25\u91cd\u9690\u79c1\u62c5\u5fe7\u3002\u76ee\u524d\u7f3a\u4e4f\u516c\u5f00\u7684\u7efc\u5408\u6027\u6570\u636e\u96c6\u6216\u57fa\u51c6\u6765\u8bc4\u4f30\u6216\u7f13\u89e3MLLMs\u4e2d\u7684\u751f\u7269\u7279\u5f81\u6cc4\u9732\u95ee\u9898\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86PRISM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30MLLMs\u5728(1)\u62d2\u7edd\u751f\u7269\u7279\u5f81\u76f8\u5173\u67e5\u8be2\u548c(2)\u5728\u901a\u7528\u54cd\u5e94\u4e2d\u9690\u5f0f\u751f\u7269\u7279\u5f81\u6cc4\u9732\u65b9\u9762\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u5bf9\u5e7f\u6cdb\u4f7f\u7528\u7684LLaVA\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u8be6\u7ec6\u5ba1\u8ba1\uff0c\u53d1\u73b0\u4e86\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u6570\u636e\u4e2d\u5b58\u5728\u7684\u5927\u91cf\u751f\u7269\u7279\u5f81\u6cc4\u9732\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u6784\u5efa\u4e86Safe-LLaVA\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u901a\u8fc7\u7cfb\u7edf\u6027\u79fb\u9664LLaVA\u6570\u636e\u96c6\u4e2d\u663e\u5f0f\u548c\u9690\u5f0f\u751f\u7269\u7279\u5f81\u4fe1\u606f\u800c\u6784\u5efa\u7684\u9690\u79c1\u4fdd\u62a4MLLM\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7PRISM\u8bc4\u4f30\u53d1\u73b0\uff0cMLLMs\u666e\u904d\u5b58\u5728\u4e0d\u540c\u5c5e\u6027\u7684\u751f\u7269\u7279\u5f81\u4fe1\u606f\u6cc4\u9732\u95ee\u9898\uff0c\u51f8\u663e\u4e86\u8be6\u7ec6\u7684\u9690\u79c1\u4fb5\u72af\u3002\u5728Safe-LLaVA\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684\u6a21\u578b\u663e\u8457\u51cf\u5c11\u4e86\u751f\u7269\u7279\u5f81\u4fe1\u606f\u6cc4\u9732\u3002", "conclusion": "Safe-LLaVA\u6570\u636e\u96c6\u548cPRISM\u57fa\u51c6\u6d4b\u8bd5\u5171\u540c\u4e3aMLLMs\u7684\u9690\u79c1\u5bf9\u9f50\u5f00\u53d1\u548c\u8bc4\u4f30\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002\u8fd9\u4e24\u4e2a\u8d44\u6e90\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u4ee5\u4fc3\u8fdb\u9690\u79c1\u4fdd\u62a4\u7684MLLM\u53d1\u5c55\u3002"}}
{"id": "2509.00115", "pdf": "https://arxiv.org/pdf/2509.00115", "abs": "https://arxiv.org/abs/2509.00115", "authors": ["Manish Shukla"], "title": "Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Agentic artificial intelligence (AI) -- multi-agent systems that combine\nlarge language models with external tools and autonomous planning -- are\nrapidly transitioning from research laboratories into high-stakes domains. Our\nearlier \"Basic\" paper introduced a five-axis framework and proposed preliminary\nmetrics such as goal drift and harm reduction but did not provide an\nalgorithmic instantiation or empirical evidence. This \"Advanced\" sequel fills\nthat gap. First, we revisit recent benchmarks and industrial deployments to\nshow that technical metrics still dominate evaluations: a systematic review of\n84 papers from 2023--2025 found that 83% report capability metrics while only\n30% consider human-centred or economic axes [2]. Second, we formalise an\nAdaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises\nheterogeneous metrics, applies per-axis exponentially weighted moving-average\nthresholds and performs joint anomaly detection via the Mahalanobis distance.\nThird, we conduct simulations and real-world experiments. AMDM cuts\nanomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and\nreduces false-positive rates from 4.5% to 0.9% compared with static thresholds.\nWe present a comparison table and ROC/PR curves, and we reanalyse case studies\nto surface missing metrics. Code, data and a reproducibility checklist\naccompany this paper to facilitate replication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u591a\u7ef4\u5ea6\u76d1\u63a7\uff08AMDM\uff09\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u5173\u952e\u9886\u57df\u76d1\u63a7\u4ee3\u7406AI\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u5f02\u6784\u6307\u6807\u548c\u8054\u5408\u5f02\u5e38\u68c0\u6d4b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5f02\u5e38\u68c0\u6d4b\u5ef6\u8fdf\u548c\u8bef\u62a5\u7387\u3002", "motivation": "\u65e9\u524d\u5173\u4e8e\u4ee3\u7406AI\u7684\u6846\u67b6\u7f3a\u4e4f\u7b97\u6cd5\u5b9e\u73b0\u548c\u5b9e\u8bc1\u652f\u6301\u3002\u5f53\u524d\u5bf9\u4ee3\u7406AI\u7684\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u4e8e\u6280\u672f\u6307\u6807\uff0c\u5ffd\u89c6\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u548c\u7ecf\u6d4e\u7ef4\u5ea6\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5176\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u76d1\u63a7\u9700\u6c42\u3002", "method": "1. \u7cfb\u7edf\u56de\u987e\u4e8684\u7bc7\u8bba\u6587\uff0c\u53d1\u73b0\u73b0\u6709\u8bc4\u4f30\u4e3b\u5bfc\u6307\u6807\u4e3a\u80fd\u529b\u6307\u6807\uff0883%\uff09\uff0c\u800c\u4eba\u673a\u6216\u7ecf\u6d4e\u7ef4\u5ea6\u5173\u6ce8\u4e0d\u8db3\uff0830%\uff09\u30022. \u5f62\u5f0f\u5316\u4e86\u81ea\u9002\u5e94\u591a\u7ef4\u5ea6\u76d1\u63a7\uff08AMDM\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5bf9\u5f02\u6784\u6307\u6807\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u5e94\u7528\u6bcf\u8f74\u6307\u6570\u52a0\u6743\u79fb\u52a8\u5e73\u5747\u9608\u503c\uff0c\u5e76\u901a\u8fc7\u9a6c\u54c8\u62c9\u8bfa\u6bd4\u65af\u8ddd\u79bb\u8fdb\u884c\u8054\u5408\u5f02\u5e38\u68c0\u6d4b\u30023. \u8fdb\u884c\u4e86\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u3002", "result": "1. AMDM\u5728\u6a21\u62df\u76ee\u6807\u6f02\u79fb\u4e2d\u5c06\u5f02\u5e38\u68c0\u6d4b\u5ef6\u8fdf\u4ece12.3\u79d2\u7f29\u77ed\u81f35.6\u79d2\u30022. \u76f8\u6bd4\u9759\u6001\u9608\u503c\uff0cAMDM\u5c06\u8bef\u62a5\u7387\u4ece4.5%\u964d\u4f4e\u81f30.9%\u30023. \u63d0\u4f9b\u4e86\u6bd4\u8f83\u8868\u3001ROC/PR\u66f2\u7ebf\uff0c\u5e76\u91cd\u65b0\u5206\u6790\u6848\u4f8b\u7814\u7a76\u4ee5\u63ed\u793a\u7f3a\u5931\u6307\u6807\u3002", "conclusion": "AMDM\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7406AI\u76d1\u63a7\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff08\u66f4\u4f4e\u5ef6\u8fdf\u3001\u66f4\u5c11\u8bef\u62a5\uff09\uff0c\u5f25\u8865\u4e86\u5f53\u524d\u8bc4\u4f30\u5b9e\u8df5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.00047", "pdf": "https://arxiv.org/pdf/2509.00047", "abs": "https://arxiv.org/abs/2509.00047", "authors": ["Jina Kim"], "title": "Teaching AI to Remember: Insights from Brain-Inspired Replay in Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Artificial neural networks (ANNs) continue to face challenges in continual\nlearning, particularly due to catastrophic forgetting, the loss of previously\nlearned knowledge when acquiring new tasks. Inspired by memory consolidation in\nthe human brain, we investigate the internal replay mechanism proposed\nby~\\citep{brain_inspired_replay1}, which reactivates latent representations of\nprior experiences during learning. As internal replay was identified as the\nmost influential component among the brain-inspired mechanisms in their\nframework, it serves as the central focus of our in-depth investigation. Using\nthe CIFAR-100 dataset in a class-incremental setting, we evaluate the\neffectiveness of internal replay, both in isolation and in combination with\nSynaptic Intelligence (SI). Our experiments show that internal replay\nsignificantly mitigates forgetting, especially when paired with SI, but at the\ncost of reduced initial task accuracy, highlighting a trade-off between memory\nstability and learning plasticity. Further analyses using log-likelihood\ndistributions, reconstruction errors, silhouette scores, and UMAP projections\nreveal that internal replay increases representational overlap in latent space,\npotentially limiting task-specific differentiation. These results underscore\nthe limitations of current brain-inspired methods and suggest future directions\nfor balancing retention and adaptability in continual learning systems.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\uff0c\u53d7\u5927\u8111\u542f\u53d1\u7684\u5185\u90e8\u91cd\u653e\u673a\u5236\u5982\u4f55\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5185\u90e8\u91cd\u653e\u80fd\u663e\u8457\u51cf\u5c11\u9057\u5fd8\uff08\u5c24\u5176\u7ed3\u5408SI\u65f6\uff09\uff0c\u4f46\u4f1a\u727a\u7272\u521d\u59cb\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u5e76\u5bfc\u81f4\u6f5c\u5728\u7a7a\u95f4\u8868\u5f81\u91cd\u53e0\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8111\u542f\u53d1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u7684\u6311\u6218\uff0c\u5373\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u4f1a\u4e22\u5931\u5148\u524d\u5b66\u4e60\u7684\u77e5\u8bc6\uff0c\u9700\u8981\u6709\u6548\u7684\u673a\u5236\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u53d7\u4eba\u8111\u8bb0\u5fc6\u5de9\u56fa\u542f\u53d1\uff0c\u6df1\u5165\u7814\u7a76\u4e86\u901a\u8fc7\u91cd\u65b0\u6fc0\u6d3b\u5148\u524d\u7ecf\u9a8c\u6f5c\u5728\u8868\u5f81\u7684\u5185\u90e8\u91cd\u653e\u673a\u5236\u3002\u5728CIFAR-100\u6570\u636e\u96c6\u7684\u7c7b\u522b\u589e\u91cf\u8bbe\u7f6e\u4e0b\uff0c\u5355\u72ec\u53ca\u7ed3\u5408\u7a81\u89e6\u667a\u80fd\uff08SI\uff09\u8bc4\u4f30\u5176\u6709\u6548\u6027\u3002\u8fdb\u4e00\u6b65\u901a\u8fc7\u5bf9\u6570\u4f3c\u7136\u5206\u5e03\u3001\u91cd\u6784\u8bef\u5dee\u3001\u8f6e\u5ed3\u5206\u6570\u548cUMAP\u6295\u5f71\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5185\u90e8\u91cd\u653e\u663e\u8457\u7f13\u89e3\u4e86\u9057\u5fd8\uff0c\u5c24\u5176\u4e0eSI\u7ed3\u5408\u65f6\u6548\u679c\u66f4\u4f73\uff0c\u4f46\u4ee3\u4ef7\u662f\u521d\u59cb\u4efb\u52a1\u51c6\u786e\u6027\u964d\u4f4e\uff0c\u8fd9\u51f8\u663e\u4e86\u8bb0\u5fc6\u7a33\u5b9a\u6027\u4e0e\u5b66\u4e60\u53ef\u5851\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u5206\u6790\u8fd8\u53d1\u73b0\uff0c\u5185\u90e8\u91cd\u653e\u589e\u52a0\u4e86\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8868\u5f81\u91cd\u53e0\uff0c\u53ef\u80fd\u9650\u5236\u4e86\u4efb\u52a1\u7279\u5f02\u6027\u533a\u5206\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u5f53\u524d\u8111\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u5e73\u8861\u77e5\u8bc6\u4fdd\u7559\u4e0e\u9002\u5e94\u6027\u7684\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.00276", "pdf": "https://arxiv.org/pdf/2509.00276", "abs": "https://arxiv.org/abs/2509.00276", "authors": ["Yuxiang Liu", "Tian Wang", "Gourab Kundu", "Tianyu Cao", "Guang Cheng", "Zhen Ge", "Jianshu Chen", "Qingjun Cui", "Trishul Chilimbi"], "title": "Exploring Reasoning-Infused Text Embedding with Large Language Models for Zero-Shot Dense Retrieval", "categories": ["cs.CL"], "comment": "CIKM 2025", "summary": "Transformer-based models such as BERT and E5 have significantly advanced text\nembedding by capturing rich contextual representations. However, many complex\nreal-world queries require sophisticated reasoning to retrieve relevant\ndocuments beyond surface-level lexical matching, where encoder-only retrievers\noften fall short. Decoder-only large language models (LLMs), known for their\nstrong reasoning capabilities, offer a promising alternative. Despite this\npotential, existing LLM-based embedding methods primarily focus on contextual\nrepresentation and do not fully exploit the reasoning strength of LLMs. To\nbridge this gap, we propose Reasoning-Infused Text Embedding (RITE), a simple\nbut effective approach that integrates logical reasoning into the text\nembedding process using generative LLMs. RITE builds upon existing language\nmodel embedding techniques by generating intermediate reasoning texts in the\ntoken space before computing embeddings, thereby enriching representations with\ninferential depth. Experimental results on BRIGHT, a reasoning-intensive\nretrieval benchmark, demonstrate that RITE significantly enhances zero-shot\nretrieval performance across diverse domains, underscoring the effectiveness of\nincorporating reasoning into the embedding process.", "AI": {"tldr": "\u63d0\u51faRITE\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4e2d\u95f4\u63a8\u7406\u6587\u672c\uff0c\u5c06\u903b\u8f91\u63a8\u7406\u96c6\u6210\u5230\u751f\u6210\u5f0fLLM\u7684\u6587\u672c\u5d4c\u5165\u8fc7\u7a0b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u5bc6\u96c6\u578b\u68c0\u7d22\u4efb\u52a1\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709Transformer\u7f16\u7801\u5668\u6a21\u578b\u96be\u4ee5\u5904\u7406\u9700\u63a8\u7406\u7684\u590d\u6742\u67e5\u8be2\uff1b\u5c3d\u7ba1\u89e3\u7801\u5668LLM\u5177\u6709\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u73b0\u6709LLM\u5d4c\u5165\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u5176\u63a8\u7406\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u63a8\u7406\u6ce8\u5165\u6587\u672c\u5d4c\u5165\uff08RITE\uff09\uff0c\u5229\u7528\u751f\u6210\u5f0fLLM\uff0c\u5728\u8ba1\u7b97\u5d4c\u5165\u524d\u751f\u6210\u4e2d\u95f4\u63a8\u7406\u6587\u672c\uff0c\u5c06\u903b\u8f91\u63a8\u7406\u878d\u5165\u6587\u672c\u5d4c\u5165\u8fc7\u7a0b\uff0c\u4ee5\u589e\u52a0\u8868\u793a\u7684\u63a8\u7406\u6df1\u5ea6\u3002", "result": "\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u68c0\u7d22\u57fa\u51c6BRIGHT\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRITE\u663e\u8457\u63d0\u5347\u4e86\u8de8\u9886\u57df\u96f6\u6837\u672c\u68c0\u7d22\u6027\u80fd\u3002", "conclusion": "\u5c06\u63a8\u7406\u80fd\u529b\u878d\u5165\u5d4c\u5165\u8fc7\u7a0b\u662f\u6709\u6548\u4e14\u53ef\u884c\u7684\uff0cRITE\u7684\u6210\u529f\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u70b9\u3002"}}
{"id": "2509.00567", "pdf": "https://arxiv.org/pdf/2509.00567", "abs": "https://arxiv.org/abs/2509.00567", "authors": ["P. Kumar"], "title": "Interference Between FM Cell Sites and CDMA Cell Sites", "categories": ["cs.NI"], "comment": null, "summary": "Interference is the major problem now days in telecommunication sector. One\ntype of interference which is very common now days is FM Cell sites\ninterference between CDMA Cell sites. Which are the types of interference and\nvarious observations during this interference is discussed below in this paper.", "AI": {"tldr": "\u672c\u8bba\u6587\u4e3b\u8981\u63a2\u8ba8\u7535\u4fe1\u9886\u57df\u4e2d\uff0cFM\u8702\u7a9d\u57fa\u7ad9\u5bf9CDMA\u8702\u7a9d\u57fa\u7ad9\u9020\u6210\u7684\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u5e72\u6270\u662f\u5f53\u524d\u7535\u4fe1\u9886\u57df\u7684\u4e3b\u8981\u95ee\u9898\u3002\u5176\u4e2d\uff0cFM\u8702\u7a9d\u57fa\u7ad9\u5bf9CDMA\u8702\u7a9d\u57fa\u7ad9\u7684\u5e72\u6270\u662f\u4e00\u79cd\u5e38\u89c1\u4e14\u7a81\u51fa\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u901a\u8fc7\u8ba8\u8bba\u5e72\u6270\u7684\u7c7b\u578b\u5e76\u5448\u73b0\u76f8\u5173\u7684\u89c2\u5bdf\u7ed3\u679c\uff0c\u6765\u5206\u6790FM\u4e0eCDMA\u8702\u7a9d\u57fa\u7ad9\u4e4b\u95f4\u7684\u5e72\u6270\u3002", "result": "\u6458\u8981\u4e2d\u672a\u5177\u4f53\u7ed9\u51fa\u7814\u7a76\u7ed3\u679c\uff0c\u4ec5\u6307\u51fa\u8bba\u6587\u5c06\u8ba8\u8bba\u5e72\u6270\u7c7b\u578b\u53ca\u5728\u5e72\u6270\u8fc7\u7a0b\u4e2d\u83b7\u5f97\u7684\u5404\u79cd\u89c2\u5bdf\u53d1\u73b0\u3002", "conclusion": "\u6458\u8981\u4e2d\u672a\u63d0\u4f9b\u7814\u7a76\u7ed3\u8bba\u3002"}}
{"id": "2509.00210", "pdf": "https://arxiv.org/pdf/2509.00210", "abs": "https://arxiv.org/abs/2509.00210", "authors": ["Jinzhou Tang", "Jusheng zhang", "Sidi Liu", "Waikit Xiu", "Qinhan Lv", "Xiying Li"], "title": "Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Achieving human-like reasoning in deep learning models for complex tasks in\nunknown environments remains a critical challenge in embodied intelligence.\nWhile advanced vision-language models (VLMs) excel in static scene\nunderstanding, their limitations in spatio-temporal reasoning and adaptation to\ndynamic, open-set tasks like task-oriented navigation and embodied question\nanswering (EQA) persist due to inadequate modeling of fine-grained\nspatio-temporal cues and physical world comprehension. To address this, we\npropose VEME, a novel cross-modal alignment method that enhances generalization\nin unseen scenes by learning an ego-centric, experience-centered world model.\nOur framework integrates three key components: (1) a cross-modal alignment\nframework bridging objects, spatial representations, and visual semantics with\nspatio-temporal cues to enhance VLM in-context learning; (2) a dynamic,\nimplicit cognitive map activated by world embedding to enable task-relevant\ngeometric-semantic memory recall; and (3) an instruction-based navigation and\nreasoning framework leveraging embodied priors for long-term planning and\nefficient exploration. By embedding geometry-aware spatio-temporal episodic\nexperiences, our method significantly improves reasoning and planning in\ndynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate\n1%-3% accuracy and exploration efficiency improvement compared to traditional\napproaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVEME\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u7ecf\u9a8c\u4e16\u754c\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5177\u8eab\u667a\u80fd\u5728\u672a\u77e5\u52a8\u6001\u73af\u5883\u4e2d\u7684\u65f6\u7a7a\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709VLM\u5728\u5177\u8eab\u667a\u80fd\u7684\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u96be\u4ee5\u5b9e\u73b0\u7c7b\u4eba\u63a8\u7406\uff0c\u5c24\u5176\u5728\u672a\u77e5\u52a8\u6001\u73af\u5883\u4e0b\u7684\u65f6\u7a7a\u63a8\u7406\u3001\u9002\u5e94\u6027\u4ee5\u53ca\u5bf9\u7cbe\u7ec6\u65f6\u7a7a\u7ebf\u7d22\u548c\u7269\u7406\u4e16\u754c\u7684\u7406\u89e3\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u4efb\u52a1\u578b\u5bfc\u822a\u548c\u5177\u8eab\u95ee\u7b54\u7b49\u5e94\u7528\u3002", "method": "\u672c\u6587\u63d0\u51faVEME\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u3001\u4ee5\u7ecf\u9a8c\u4e3a\u4e2d\u5fc3\u7684\u4e16\u754c\u6a21\u578b\u6765\u589e\u5f3a\u672a\u89c1\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u7ed3\u5408\u7269\u4f53\u3001\u7a7a\u95f4\u8868\u5f81\u548c\u89c6\u89c9\u8bed\u4e49\u4e0e\u65f6\u7a7a\u7ebf\u7d22\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\uff0c\u4ee5\u589e\u5f3aVLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff1b2) \u7531\u4e16\u754c\u5d4c\u5165\u6fc0\u6d3b\u7684\u52a8\u6001\u9690\u5f0f\u8ba4\u77e5\u5730\u56fe\uff0c\u7528\u4e8e\u4efb\u52a1\u76f8\u5173\u7684\u51e0\u4f55-\u8bed\u4e49\u8bb0\u5fc6\u53ec\u56de\uff1b3) \u57fa\u4e8e\u6307\u4ee4\u7684\u5bfc\u822a\u4e0e\u63a8\u7406\u6846\u67b6\uff0c\u5229\u7528\u5177\u8eab\u5148\u9a8c\u8fdb\u884c\u957f\u671f\u89c4\u5212\u548c\u9ad8\u6548\u63a2\u7d22\u3002", "result": "\u5728VSI-Bench\u548cVLN-CE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVEME\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u548c\u63a2\u7d22\u6548\u7387\u65b9\u9762\u5b9e\u73b0\u4e861%-3%\u7684\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5d4c\u5165\u51e0\u4f55\u611f\u77e5\u7684\u65f6\u7a7a\u60c5\u666f\u7ecf\u9a8c\uff0cVEME\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5177\u8eab\u667a\u80fd\u5728\u590d\u6742\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.00125", "pdf": "https://arxiv.org/pdf/2509.00125", "abs": "https://arxiv.org/abs/2509.00125", "authors": ["Ang Li", "Zhihang Yuan", "Yang Zhang", "Shouda Liu", "Yisen Wang"], "title": "Know When to Explore: Difficulty-Aware Certainty as a Guide for LLM Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Reinforcement Learning with Verifiable Feedback (RLVF) has become a key\ntechnique for enhancing the reasoning abilities of Large Language Models\n(LLMs). However, its reliance on sparse, outcome based rewards, which only\nindicate if a final answer is correct or not, fails to provide granular\nguidance on the reasoning process itself. This limitation hinders efficient\nlearning, as the model cannot distinguish between high quality and inefficient\nsolutions, nor can it learn effectively from different types of failures. To\naddress this, we observe that an LLMs self-certainty often correlates with task\ndifficulty and solution quality. We introduce Difficulty Aware Certainty guided\nExploration (DACE), a novel RL algorithm that leverages this insight to\ndynamically balance the exploration exploitation trade-off. DACE assesses task\ndifficulty online based on the policys success rate. It then uses this signal\nto modulate an intrinsic reward: for difficult tasks where the model is\nstruggling, DACE encourages exploration by penalizing high certainty; for\neasier tasks, it encourages learning efficiency by rewarding high certainty.\nExperiments on challenging mathematical reasoning benchmarks (AIME, MATH) show\nthat DACE significantly outperforms strong baselines. The DACE-trained models\nnot only achieve higher accuracy but also demonstrate more robust performance\nwhen scaling test-time compute, validating that our adaptive approach fosters\neffective exploration without sacrificing precision.", "AI": {"tldr": "\u73b0\u6709RLVF\u5728LLM\u63a8\u7406\u4e2d\u4f9d\u8d56\u7a00\u758f\u5956\u52b1\uff0cDACE\u7b97\u6cd5\u5229\u7528\u6a21\u578b\u81ea\u786e\u4fe1\u5ea6\u611f\u77e5\u4efb\u52a1\u96be\u5ea6\uff0c\u52a8\u6001\u8c03\u6574\u63a2\u7d22\u4e0e\u5229\u7528\u7b56\u7565\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\u3002", "motivation": "RLVF\u4f9d\u8d56\u7a00\u758f\u3001\u7ed3\u679c\u5bfc\u5411\u7684\u5956\u52b1\uff0c\u65e0\u6cd5\u4e3aLLM\u7684\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u6307\u5bfc\uff0c\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u533a\u5206\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff0c\u4e5f\u65e0\u6cd5\u6709\u6548\u4ece\u4e0d\u540c\u7c7b\u578b\u7684\u5931\u8d25\u4e2d\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u96be\u5ea6\u611f\u77e5\u786e\u4fe1\u5ea6\u5f15\u5bfc\u63a2\u7d22(DACE)\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u5728\u7ebf\u8bc4\u4f30\u4efb\u52a1\u96be\u5ea6\uff08\u57fa\u4e8e\u7b56\u7565\u6210\u529f\u7387\uff09\uff0c\u5e76\u5229\u7528\u6b64\u4fe1\u53f7\u8c03\u5236\u5185\u5728\u5956\u52b1\uff1a\u5bf9\u4e8e\u56f0\u96be\u4efb\u52a1\uff0c\u901a\u8fc7\u60e9\u7f5a\u9ad8\u786e\u4fe1\u5ea6\u6765\u9f13\u52b1\u63a2\u7d22\uff1b\u5bf9\u4e8e\u7b80\u5355\u4efb\u52a1\uff0c\u901a\u8fc7\u5956\u52b1\u9ad8\u786e\u4fe1\u5ea6\u6765\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "result": "\u5728AIME\u548cMATH\u7b49\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDACE\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002DACE\u8bad\u7ec3\u7684\u6a21\u578b\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u800c\u4e14\u5728\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\u65f6\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "DACE\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u4fc3\u8fdb\u4e86\u6709\u6548\u7684\u63a2\u7d22\uff0c\u4ece\u800c\u9a8c\u8bc1\u4e86\u5176\u80fd\u6709\u6548\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.00049", "pdf": "https://arxiv.org/pdf/2509.00049", "abs": "https://arxiv.org/abs/2509.00049", "authors": ["Mohammad Nooraiepour", "Mohammad Masoudi", "Zezhang Song", "Helge Hellevang"], "title": "Adaptive Physics-Informed Neural Networks with Multi-Category Feature Engineering for Hydrogen Sorption Prediction in Clays, Shales, and Coals", "categories": ["cs.LG"], "comment": null, "summary": "Accurate prediction of hydrogen sorption in clays, shales, and coals is vital\nfor advancing underground hydrogen storage, natural hydrogen exploration, and\nradioactive waste containment. Traditional experimental methods, while\nfoundational, are time-consuming, error-prone, and limited in capturing\ngeological heterogeneity. This study introduces an adaptive physics-informed\nneural network (PINN) framework with multi-category feature engineering to\nenhance hydrogen sorption prediction. The framework integrates classical\nisotherm models with thermodynamic constraints to ensure physical consistency\nwhile leveraging deep learning flexibility. A comprehensive dataset consisting\nof 155 samples, which includes 50 clays, 60 shales, and 45 coals, was employed,\nincorporating diverse compositional properties and experimental conditions.\nMulti-category feature engineering across seven categories captured complex\nsorption dynamics. The PINN employs deep residual networks with multi-head\nattention, optimized via adaptive loss functions and Monte Carlo dropout for\nuncertainty quantification. K-fold cross-validation and hyperparameter\noptimization achieve significant accuracy (R2 = 0.979, RMSE = 0.045 mol per kg)\nwith 67% faster convergence despite 15-fold increased complexity. The framework\ndemonstrates robust lithology-specific performance across clay minerals (R2 =\n0.981), shales (R2 = 0.971), and coals (R2 = 0.978), maintaining 85-91%\nreliability scores. Interpretability analysis via SHAP, accumulated local\neffects, and Friedman's H-statistics reveal that hydrogen adsorption capacity\ndominates predictions, while 86.7% of feature pairs exhibit strong\ninteractions, validating the necessity of non-linear modeling approaches. This\nadaptive physics-informed framework accelerates site screening and enables\nrisk-informed decision-making through robust uncertainty quantification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u7c7b\u522b\u7279\u5f81\u5de5\u7a0b\u7684\u81ea\u9002\u5e94\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7c98\u571f\u3001\u9875\u5ca9\u548c\u7164\u4e2d\u6c22\u5438\u9644\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u7c98\u571f\u3001\u9875\u5ca9\u548c\u7164\u4e2d\u7684\u6c22\u5438\u9644\u5bf9\u4e8e\u5730\u4e0b\u50a8\u6c22\u3001\u5929\u7136\u6c22\u52d8\u63a2\u548c\u653e\u5c04\u6027\u5e9f\u7269\u5904\u7f6e\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f20\u7edf\u5b9e\u9a8c\u65b9\u6cd5\u8017\u65f6\u3001\u6613\u51fa\u9519\u4e14\u96be\u4ee5\u6355\u6349\u5730\u8d28\u5f02\u8d28\u6027\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u591a\u7c7b\u522b\u7279\u5f81\u5de5\u7a0b\u3002\u5b83\u6574\u5408\u4e86\u7ecf\u5178\u7b49\u6e29\u7ebf\u6a21\u578b\u4e0e\u70ed\u529b\u5b66\u7ea6\u675f\u4ee5\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5229\u7528\u4e86\u6df1\u5ea6\u5b66\u4e60\u7684\u7075\u6d3b\u6027\u3002\u6a21\u578b\u4f7f\u7528\u5305\u542b155\u4e2a\u6837\u672c\uff0850\u4e2a\u7c98\u571f\u300160\u4e2a\u9875\u5ca9\u300145\u4e2a\u7164\uff09\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528\u4e86\u4e03\u4e2a\u7c7b\u522b\u7684\u591a\u7c7b\u522b\u7279\u5f81\u5de5\u7a0b\u3002PINN\u6a21\u578b\u91c7\u7528\u5e26\u6709\u591a\u5934\u6ce8\u610f\u529b\u7684\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u635f\u5931\u51fd\u6570\u548c\u8499\u7279\u5361\u6d1bdropout\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u901a\u8fc7K\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff08R2 = 0.979, RMSE = 0.045 mol/kg\uff09\uff0c\u5e76\u5728\u590d\u6742\u6027\u589e\u52a015\u500d\u7684\u60c5\u51b5\u4e0b\u4ecd\u4f7f\u6536\u655b\u901f\u5ea6\u52a0\u5feb\u4e8667%\u3002\u8be5\u6846\u67b6\u5728\u7c98\u571f\u77ff\u7269\uff08R2 = 0.981\uff09\u3001\u9875\u5ca9\uff08R2 = 0.971\uff09\u548c\u7164\uff08R2 = 0.978\uff09\u4e2d\u5747\u8868\u73b0\u51fa\u7a33\u5065\u7684\u5ca9\u6027\u7279\u5f02\u6027\u6027\u80fd\uff0c\u53ef\u9760\u6027\u8bc4\u5206\u4fdd\u6301\u572885-91%\u3002\u53ef\u89e3\u91ca\u6027\u5206\u6790\u8868\u660e\u6c22\u5438\u9644\u5bb9\u91cf\u4e3b\u5bfc\u9884\u6d4b\uff0c\u4e1486.7%\u7684\u7279\u5f81\u5bf9\u5b58\u5728\u5f3a\u76f8\u4e92\u4f5c\u7528\uff0c\u8bc1\u5b9e\u4e86\u975e\u7ebf\u6027\u5efa\u6a21\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u8be5\u81ea\u9002\u5e94\u7269\u7406\u4fe1\u606f\u6846\u67b6\u80fd\u591f\u52a0\u901f\u573a\u5730\u7b5b\u9009\uff0c\u5e76\u901a\u8fc7\u7a33\u5065\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u652f\u6301\u98ce\u9669\u77e5\u60c5\u7684\u51b3\u7b56\u3002"}}
{"id": "2509.00285", "pdf": "https://arxiv.org/pdf/2509.00285", "abs": "https://arxiv.org/abs/2509.00285", "authors": ["Mir Tafseer Nayeem", "Davood Rafiei"], "title": "OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "COLM 2025", "summary": "We study the problem of opinion highlights generation from large volumes of\nuser reviews, often exceeding thousands per entity, where existing methods\neither fail to scale or produce generic, one-size-fits-all summaries that\noverlook personalized needs. To tackle this, we introduce OpinioRAG, a\nscalable, training-free framework that combines RAG-based evidence retrieval\nwith LLMs to efficiently produce tailored summaries. Additionally, we propose\nnovel reference-free verification metrics designed for sentiment-rich domains,\nwhere accurately capturing opinions and sentiment alignment is essential. These\nmetrics offer a fine-grained, context-sensitive assessment of factual\nconsistency. To facilitate evaluation, we contribute the first large-scale\ndataset of long-form user reviews, comprising entities with over a thousand\nreviews each, paired with unbiased expert summaries and manually annotated\nqueries. Through extensive experiments, we identify key challenges, provide\nactionable insights into improving systems, pave the way for future research,\nand position OpinioRAG as a robust framework for generating accurate, relevant,\nand structured summaries at scale.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faOpinioRAG\u6846\u67b6\uff0c\u7ed3\u5408RAG\u548cLLMs\uff0c\u4ece\u6d77\u91cf\u7528\u6237\u8bc4\u8bba\u4e2d\u9ad8\u6548\u751f\u6210\u4e2a\u6027\u5316\u89c2\u70b9\u6458\u8981\uff0c\u5e76\u5f15\u5165\u65b0\u578b\u65e0\u53c2\u8003\u9a8c\u8bc1\u6307\u6807\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u6d77\u91cf\u7528\u6237\u8bc4\u8bba\u7684\u89c2\u70b9\u6458\u8981\u751f\u6210\u65f6\uff0c\u5b58\u5728\u53ef\u6269\u5c55\u6027\u5dee\u6216\u751f\u6210\u901a\u7528\u6458\u8981\u65e0\u6cd5\u6ee1\u8db3\u4e2a\u6027\u5316\u9700\u6c42\u7684\u95ee\u9898\u3002", "method": "1. \u63d0\u51faOpinioRAG\u6846\u67b6\uff0c\u5229\u7528RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u8bc1\u636e\u68c0\u7d22\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7ed3\u5408\uff0c\u9ad8\u6548\u751f\u6210\u5b9a\u5236\u5316\u6458\u8981\u3002 2. \u63d0\u51fa\u9488\u5bf9\u60c5\u611f\u4e30\u5bcc\u9886\u57df\u7684\u65b0\u578b\u65e0\u53c2\u8003\u9a8c\u8bc1\u6307\u6807\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u5730\u8bc4\u4f30\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002 3. \u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u957f\u7bc7\u7528\u6237\u8bc4\u8bba\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e13\u5bb6\u6458\u8981\u548c\u4eba\u5de5\u6807\u6ce8\u67e5\u8be2\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u6539\u8fdb\u7cfb\u7edf\u7684\u53ef\u884c\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002", "conclusion": "OpinioRAG\u88ab\u5b9a\u4f4d\u4e3a\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5927\u89c4\u6a21\u751f\u6210\u51c6\u786e\u3001\u76f8\u5173\u4e14\u7ed3\u6784\u5316\u7684\u89c2\u70b9\u6458\u8981\u3002"}}
{"id": "2509.00603", "pdf": "https://arxiv.org/pdf/2509.00603", "abs": "https://arxiv.org/abs/2509.00603", "authors": ["Osama Abu Hamdan", "Hao Che", "Engin Arslan", "Md Arifuzzaman"], "title": "SmartFLow: A Communication-Efficient SDN Framework for Cross-Silo Federated Learning", "categories": ["cs.NI"], "comment": "Submitted to IEEE CCNC 2026", "summary": "Cross-silo Federated Learning (FL) enables multiple institutions to\ncollaboratively train machine learning models while preserving data privacy. In\nsuch settings, clients repeatedly exchange model weights with a central server,\nmaking the overall training time highly sensitive to network performance.\nHowever, conventional routing methods often fail to prevent congestion, leading\nto increased communication latency and prolonged training. Software-Defined\nNetworking (SDN), which provides centralized and programmable control over\nnetwork resources, offers a promising way to address this limitation. To this\nend, we propose SmartFLow, an SDN-based framework designed to enhance\ncommunication efficiency in cross-silo FL. SmartFLow dynamically adjusts\nrouting paths in response to changing network conditions, thereby reducing\ncongestion and improving synchronization efficiency. Experimental results show\nthat SmartFLow decreases parameter synchronization time by up to 47% compared\nto shortest-path routing and 41% compared to capacity-aware routing.\nFurthermore, it achieves these gains with minimal computational overhead and\nscales effectively to networks of up to 50 clients, demonstrating its\npracticality for real-world FL deployments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSmartFLow\uff0c\u4e00\u4e2a\u57fa\u4e8eSDN\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8def\u7531\u8def\u5f84\u6765\u6709\u6548\u51cf\u5c11\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u7f51\u7edc\u62e5\u585e\uff0c\u663e\u8457\u63d0\u5347\u901a\u4fe1\u6548\u7387\u548c\u53c2\u6570\u540c\u6b65\u901f\u5ea6\u3002", "motivation": "\u5728\u8de8\u7b52\u4ed3\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u5ba2\u6237\u7aef\u4e0e\u4e2d\u5fc3\u670d\u52a1\u5668\u9891\u7e41\u4ea4\u6362\u6a21\u578b\u6743\u91cd\uff0c\u4f7f\u5f97\u8bad\u7ec3\u65f6\u95f4\u5bf9\u7f51\u7edc\u6027\u80fd\u9ad8\u5ea6\u654f\u611f\u3002\u7136\u800c\uff0c\u4f20\u7edf\u8def\u7531\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u9632\u6b62\u62e5\u585e\uff0c\u5bfc\u81f4\u901a\u4fe1\u5ef6\u8fdf\u589e\u52a0\u548c\u8bad\u7ec3\u65f6\u95f4\u5ef6\u957f\u3002", "method": "\u63d0\u51faSmartFLow\uff0c\u4e00\u4e2a\u57fa\u4e8e\u8f6f\u4ef6\u5b9a\u4e49\u7f51\u7edc\uff08SDN\uff09\u7684\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3a\u8de8\u7b52\u4ed3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u6548\u7387\u3002SmartFLow\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8def\u7531\u8def\u5f84\u4ee5\u54cd\u5e94\u53d8\u5316\u7684\u7f51\u7edc\u6761\u4ef6\uff0c\u4ece\u800c\u51cf\u5c11\u62e5\u585e\u5e76\u63d0\u9ad8\u540c\u6b65\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u77ed\u8def\u5f84\u8def\u7531\u76f8\u6bd4\uff0cSmartFLow\u5c06\u53c2\u6570\u540c\u6b65\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe47%\uff1b\u4e0e\u5bb9\u91cf\u611f\u77e5\u8def\u7531\u76f8\u6bd4\uff0c\u51cf\u5c11\u9ad8\u8fbe41%\u3002\u6b64\u5916\uff0c\u5b83\u5177\u6709\u6781\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u80fd\u6709\u6548\u6269\u5c55\u523050\u4e2a\u5ba2\u6237\u7aef\u7684\u7f51\u7edc\u3002", "conclusion": "SmartFLow\u901a\u8fc7SDN\u52a8\u6001\u8def\u7531\u663e\u8457\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u901a\u4fe1\u6548\u7387\uff0c\u6709\u6548\u7f29\u77ed\u4e86\u53c2\u6570\u540c\u6b65\u65f6\u95f4\uff0c\u5e76\u5c55\u73b0\u51fa\u4f4e\u8ba1\u7b97\u5f00\u9500\u548c\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u8054\u90a6\u5b66\u4e60\u90e8\u7f72\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.00213", "pdf": "https://arxiv.org/pdf/2509.00213", "abs": "https://arxiv.org/abs/2509.00213", "authors": ["Farhan Fuad Abir", "Abigail Elliott Daly", "Kyle Anderman", "Tolga Ozmen", "Laura J. Brattain"], "title": "Multimodal Deep Learning for Phyllodes Tumor Classification from Ultrasound and Clinical Data", "categories": ["cs.CV", "cs.AI"], "comment": "IEEE-EMBS International Conference on Body Sensor Networks (IEEE-EMBS\n  BSN 2025)", "summary": "Phyllodes tumors (PTs) are rare fibroepithelial breast lesions that are\ndifficult to classify preoperatively due to their radiological similarity to\nbenign fibroadenomas. This often leads to unnecessary surgical excisions. To\naddress this, we propose a multimodal deep learning framework that integrates\nbreast ultrasound (BUS) images with structured clinical data to improve\ndiagnostic accuracy. We developed a dual-branch neural network that extracts\nand fuses features from ultrasound images and patient metadata from 81 subjects\nwith confirmed PTs. Class-aware sampling and subject-stratified 5-fold\ncross-validation were applied to prevent class imbalance and data leakage. The\nresults show that our proposed multimodal method outperforms unimodal baselines\nin classifying benign versus borderline/malignant PTs. Among six image\nencoders, ConvNeXt and ResNet18 achieved the best performance in the multimodal\nsetting, with AUC-ROC scores of 0.9427 and 0.9349, and F1-scores of 0.6720 and\n0.7294, respectively. This study demonstrates the potential of multimodal AI to\nserve as a non-invasive diagnostic tool, reducing unnecessary biopsies and\nimproving clinical decision-making in breast tumor management.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4e73\u817a\u8d85\u58f0\u56fe\u50cf\u548c\u4e34\u5e8a\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53f6\u72b6\u80bf\u7624\u7684\u672f\u524d\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u65e8\u5728\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u624b\u672f\u5207\u9664\u3002", "motivation": "\u53f6\u72b6\u80bf\u7624\u672f\u524d\u96be\u4ee5\u4e0e\u826f\u6027\u7ea4\u7ef4\u817a\u7624\u533a\u5206\uff0c\u5bfc\u81f4\u5927\u91cf\u4e0d\u5fc5\u8981\u7684\u624b\u672f\u5207\u9664\uff0c\u4e9f\u9700\u6539\u8fdb\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53cc\u5206\u652f\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u878d\u5408\u6765\u81ea81\u540d\u60a3\u8005\u7684\u4e73\u817a\u8d85\u58f0\u56fe\u50cf\u548c\u7ed3\u6784\u5316\u4e34\u5e8a\u6570\u636e\u3002\u91c7\u7528\u7c7b\u522b\u5e73\u8861\u91c7\u6837\u548c\u5206\u5c425\u6298\u4ea4\u53c9\u9a8c\u8bc1\u6765\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\u3002", "result": "\u8be5\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u533a\u5206\u826f\u6027\u4e0e\u4ea4\u754c/\u6076\u6027\u53f6\u72b6\u80bf\u7624\u65b9\u9762\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\u3002\u5176\u4e2d\uff0cConvNeXt\u548cResNet18\u5728\u591a\u6a21\u6001\u8bbe\u7f6e\u4e0b\u8868\u73b0\u6700\u4f73\uff0cAUC-ROC\u5206\u522b\u4e3a0.9427\u548c0.9349\uff0cF1-score\u5206\u522b\u4e3a0.6720\u548c0.7294\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u591a\u6a21\u6001AI\u4f5c\u4e3a\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u8bca\u65ad\u5de5\u5177\uff0c\u5728\u4e73\u817a\u80bf\u7624\u7ba1\u7406\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u6d3b\u68c0\u5e76\u6539\u5584\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2509.00135", "pdf": "https://arxiv.org/pdf/2509.00135", "abs": "https://arxiv.org/abs/2509.00135", "authors": ["Davin Choo", "Yohai Trabelsi", "Fentabil Getnet", "Samson Warkaye Lamma", "Wondesen Nigatu", "Kasahun Sime", "Lisa Matay", "Milind Tambe", "St\u00e9phane Verguet"], "title": "Optimizing Health Coverage in Ethiopia: A Learning-augmented Approach and Persistent Proportionality Under an Online Budget", "categories": ["cs.AI"], "comment": null, "summary": "As part of nationwide efforts aligned with the United Nations' Sustainable\nDevelopment Goal 3 on Universal Health Coverage, Ethiopia's Ministry of Health\nis strengthening health posts to expand access to essential healthcare\nservices. However, only a fraction of this health system strengthening effort\ncan be implemented each year due to limited budgets and other competing\npriorities, thus the need for an optimization framework to guide prioritization\nacross the regions of Ethiopia. In this paper, we develop a tool, Health Access\nResource Planner (HARP), based on a principled decision-support optimization\nframework for sequential facility planning that aims to maximize population\ncoverage under budget uncertainty while satisfying region-specific\nproportionality targets at every time step. We then propose two algorithms: (i)\na learning-augmented approach that improves upon expert recommendations at any\nsingle-step; and (ii) a greedy algorithm for multi-step planning, both with\nstrong worst-case approximation estimation. In collaboration with the Ethiopian\nPublic Health Institute and Ministry of Health, we demonstrated the empirical\nefficacy of our method on three regions across various planning scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u6b3e\u540d\u4e3aHARP\u7684\u4f18\u5316\u5de5\u5177\u53ca\u4e24\u79cd\u7b97\u6cd5\uff0c\u65e8\u5728\u5e2e\u52a9\u57c3\u585e\u4fc4\u6bd4\u4e9a\u536b\u751f\u90e8\u5728\u9884\u7b97\u6709\u9650\u548c\u4e0d\u786e\u5b9a\u7684\u60c5\u51b5\u4e0b\uff0c\u4f18\u5316\u533b\u7597\u70b9\u5efa\u8bbe\u4f18\u5148\u7ea7\uff0c\u4ee5\u6700\u5927\u5316\u4eba\u53e3\u8986\u76d6\u7387\u5e76\u6ee1\u8db3\u533a\u57df\u76ee\u6807\uff0c\u5e76\u5df2\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u57c3\u585e\u4fc4\u6bd4\u4e9a\u536b\u751f\u90e8\u4e3a\u5b9e\u73b0\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u68073\uff0c\u6b63\u5728\u52a0\u5f3a\u533b\u7597\u70b9\u4ee5\u6269\u5927\u57fa\u672c\u533b\u7597\u670d\u52a1\u3002\u7136\u800c\uff0c\u7531\u4e8e\u9884\u7b97\u6709\u9650\u548c\u4f18\u5148\u4e8b\u9879\u7ade\u4e89\uff0c\u6bcf\u5e74\u80fd\u5b9e\u65bd\u7684\u536b\u751f\u7cfb\u7edf\u5f3a\u5316\u5de5\u4f5c\u4ec5\u5360\u4e00\u5c0f\u90e8\u5206\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u4f18\u5316\u6846\u67b6\u6765\u6307\u5bfc\u5404\u5730\u533a\u7684\u4f18\u5148\u6392\u5e8f\u3002", "method": "\u5f00\u53d1\u4e86\u5065\u5eb7\u53ef\u53ca\u8d44\u6e90\u89c4\u5212\u5668\uff08HARP\uff09\u5de5\u5177\uff0c\u8be5\u5de5\u5177\u57fa\u4e8e\u5e8f\u8d2f\u8bbe\u65bd\u89c4\u5212\u7684\u51b3\u7b56\u652f\u6301\u4f18\u5316\u6846\u67b6\uff0c\u65e8\u5728\u9884\u7b97\u4e0d\u786e\u5b9a\u6027\u4e0b\u6700\u5927\u5316\u4eba\u53e3\u8986\u76d6\u7387\uff0c\u5e76\u6ee1\u8db3\u5404\u533a\u57df\u7279\u5b9a\u6bd4\u4f8b\u76ee\u6807\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a\u4e00\u79cd\u662f\u5b66\u4e60\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u5355\u6b65\u4e13\u5bb6\u5efa\u8bae\uff1b\u53e6\u4e00\u79cd\u662f\u591a\u6b65\u89c4\u5212\u7684\u8d2a\u5a6a\u7b97\u6cd5\uff0c\u4e24\u8005\u5747\u5177\u6709\u5f3a\u5927\u7684\u6700\u574f\u60c5\u51b5\u8fd1\u4f3c\u4f30\u8ba1\u3002", "result": "\u901a\u8fc7\u4e0e\u57c3\u585e\u4fc4\u6bd4\u4e9a\u516c\u5171\u536b\u751f\u7814\u7a76\u6240\u548c\u536b\u751f\u90e8\u5408\u4f5c\uff0c\u5728\u4e09\u4e2a\u5730\u533a\u7684\u4e0d\u540c\u89c4\u5212\u573a\u666f\u4e0b\uff0c\u6210\u529f\u9a8c\u8bc1\u4e86\u672c\u6587\u65b9\u6cd5\u7684\u7ecf\u9a8c\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684HARP\u5de5\u5177\u53ca\u4f18\u5316\u7b97\u6cd5\uff0c\u4e3a\u57c3\u585e\u4fc4\u6bd4\u4e9a\u5728\u9884\u7b97\u4e0d\u786e\u5b9a\u6027\u4e0b\u4f18\u5316\u533b\u7597\u70b9\u5efa\u8bbe\u3001\u6269\u5927\u533b\u7597\u670d\u52a1\u8986\u76d6\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u7ecf\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u7684\u51b3\u7b56\u652f\u6301\u6846\u67b6\u3002"}}
{"id": "2509.00050", "pdf": "https://arxiv.org/pdf/2509.00050", "abs": "https://arxiv.org/abs/2509.00050", "authors": ["David Kurtenbach", "Megan Manly", "Zach Metzinger"], "title": "Applying Deep Learning to Anomaly Detection of Russian Satellite Activity for Indications Prior to Military Activity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We apply deep learning techniques for anomaly detection to analyze activity\nof Russian-owned resident space objects (RSO) prior to the Ukraine invasion and\nassess the results for any findings that can be used as indications and\nwarnings (I&W) of aggressive military behavior for future conflicts. Through\nanalysis of anomalous activity, an understanding of possible tactics and\nprocedures can be established to assess the existence of statistically\nsignificant changes in Russian RSO pattern of life/pattern of behavior\n(PoL/PoB) using publicly available two-line element (TLE) data. This research\nlooks at statistical and deep learning approaches to assess anomalous activity.\nThe deep learning methods assessed are isolation forest (IF), traditional\nautoencoder (AE), variational autoencoder (VAE), Kolmogorov Arnold Network\n(KAN), and a novel anchor-loss based autoencoder (Anchor AE). Each model is\nused to establish a baseline of on-orbit activity based on a five-year data\nsample. The primary investigation period focuses on the six months leading up\nto the invasion date of February 24, 2022. Additional analysis looks at RSO\nactivity during an active combat period by sampling TLE data after the invasion\ndate. The deep learning autoencoder models identify anomalies based on\nreconstruction errors that surpass a threshold sigma. To capture the nuance and\nunique characteristics of each RSO an individual model was trained for each\nobserved space object. The research made an effort to prioritize explainability\nand interpretability of the model results thus each observation was assessed\nfor anomalous behavior of the individual six orbital elements versus analyzing\nthe input data as a single monolithic observation. The results demonstrate not\nonly statistically significant anomalies of Russian RSO activity but also\ndetails anomalous findings to the individual orbital element.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5206\u6790\u4fc4\u519b\u5728\u4e4c\u514b\u5170\u5165\u4fb5\u524d\u540e\u592a\u7a7a\u8d44\u4ea7\uff08RSO\uff09\u7684\u5f02\u5e38\u6d3b\u52a8\uff0c\u65e8\u5728\u8bc6\u522b\u53ef\u4f5c\u4e3a\u672a\u6765\u51b2\u7a81\u4e2d\u519b\u4e8b\u4fb5\u7565\u884c\u4e3a\u6307\u793a\u548c\u9884\u8b66\uff08I&W\uff09\u7684\u53d1\u73b0\u3002\u7814\u7a76\u53d1\u73b0\u4fc4\u519bRSO\u6d3b\u52a8\u5b58\u5728\u7edf\u8ba1\u5b66\u4e0a\u7684\u663e\u8457\u5f02\u5e38\uff0c\u5e76\u80fd\u7ec6\u5316\u5230\u5355\u4e2a\u8f68\u9053\u5143\u7d20\u3002", "motivation": "\u8bc4\u4f30\u4fc4\u7f57\u65af\u62e5\u6709\u7684RSO\u5728\u4e4c\u514b\u5170\u5165\u4fb5\u524d\u7684\u6d3b\u52a8\u662f\u5426\u80fd\u4f5c\u4e3a\u672a\u6765\u519b\u4e8b\u51b2\u7a81\u4e2d\u4fb5\u7565\u884c\u4e3a\u7684\u6307\u793a\u548c\u9884\u8b66\u3002\u901a\u8fc7\u5206\u6790\u5f02\u5e38\u6d3b\u52a8\uff0c\u7406\u89e3\u53ef\u80fd\u7684\u6218\u672f\u548c\u7a0b\u5e8f\uff0c\u4ee5\u8bc6\u522b\u4fc4\u7f57\u65afRSO\u751f\u6d3b/\u884c\u4e3a\u6a21\u5f0f\uff08PoL/PoB\uff09\u4e2d\u7edf\u8ba1\u5b66\u4e0a\u663e\u8457\u7684\u53d8\u5316\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7edf\u8ba1\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u8bc4\u4f30\u5f02\u5e38\u6d3b\u52a8\u3002\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5305\u62ecIsolation Forest (IF)\u3001\u4f20\u7edf\u81ea\u7f16\u7801\u5668 (AE)\u3001\u53d8\u5206\u81ea\u7f16\u7801\u5668 (VAE)\u3001Kolmogorov Arnold Network (KAN) \u548c\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u951a\u70b9\u635f\u5931\u7684\u81ea\u7f16\u7801\u5668 (Anchor AE)\u3002\u6bcf\u4e2a\u6a21\u578b\u90fd\u57fa\u4e8e\u4e94\u5e74\u7684\u6570\u636e\u6837\u672c\u5efa\u7acb\u8f68\u9053\u6d3b\u52a8\u57fa\u7ebf\u3002\u4e3b\u8981\u8c03\u67e5\u671f\u805a\u7126\u4e8e2022\u5e742\u670824\u65e5\u5165\u4fb5\u524d\u7684\u516d\u4e2a\u6708\uff0c\u5e76\u5bf9\u5165\u4fb5\u540e\u7684\u6d3b\u8dc3\u4f5c\u6218\u671f\u95f4\u7684RSO\u6d3b\u52a8\u8fdb\u884c\u989d\u5916\u5206\u6790\u3002\u4e3a\u6355\u83b7\u6bcf\u4e2aRSO\u7684\u72ec\u7279\u7279\u6027\uff0c\u4e3a\u6bcf\u4e2a\u89c2\u6d4b\u5230\u7684\u592a\u7a7a\u7269\u4f53\u8bad\u7ec3\u4e86\u5355\u72ec\u7684\u6a21\u578b\u3002\u81ea\u7f16\u7801\u5668\u6a21\u578b\u6839\u636e\u8d85\u8fc7\u9608\u503c\u897f\u683c\u739b\u7684\u91cd\u5efa\u8bef\u5dee\u6765\u8bc6\u522b\u5f02\u5e38\u3002\u7814\u7a76\u4f18\u5148\u8003\u8651\u6a21\u578b\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u56e0\u6b64\u8bc4\u4f30\u4e86\u5355\u4e2a\u516d\u4e2a\u8f68\u9053\u5143\u7d20\u7684\u5f02\u5e38\u884c\u4e3a\uff0c\u800c\u975e\u5c06\u8f93\u5165\u6570\u636e\u4f5c\u4e3a\u5355\u4e00\u6574\u4f53\u89c2\u6d4b\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u4ec5\u53d1\u73b0\u4e86\u4fc4\u7f57\u65afRSO\u6d3b\u52a8\u4e2d\u7edf\u8ba1\u5b66\u4e0a\u663e\u8457\u7684\u5f02\u5e38\uff0c\u800c\u4e14\u5c06\u8fd9\u4e9b\u5f02\u5e38\u53d1\u73b0\u8be6\u7ec6\u81f3\u5355\u4e2a\u8f68\u9053\u5143\u7d20\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u80fd\u591f\u6210\u529f\u8bc6\u522b\u4fc4\u7f57\u65afRSO\u6d3b\u52a8\u4e2d\u7edf\u8ba1\u5b66\u4e0a\u663e\u8457\u7684\u5f02\u5e38\uff0c\u8fd9\u4e9b\u53d1\u73b0\uff08\u5c24\u5176\u662f\u4e2a\u4f53\u8f68\u9053\u5c42\u9762\u7684\u5f02\u5e38\uff09\u53ef\u4f5c\u4e3a\u672a\u6765\u51b2\u7a81\u4e2d\u4fb5\u7565\u6027\u519b\u4e8b\u884c\u4e3a\u7684\u6307\u793a\u548c\u9884\u8b66\u3002"}}
{"id": "2509.00290", "pdf": "https://arxiv.org/pdf/2509.00290", "abs": "https://arxiv.org/abs/2509.00290", "authors": ["Taihei Sone"], "title": "Wage Sentiment Indices Derived from Survey Comments via Large Language Models", "categories": ["cs.CL"], "comment": "Submitted to IEEE Big Data 2025. 10 pages, 2 tables, 16 figures", "summary": "The emergence of generative Artificial Intelligence (AI) has created new\nopportunities for economic text analysis. This study proposes a Wage Sentiment\nIndex (WSI) constructed with Large Language Models (LLMs) to forecast wage\ndynamics in Japan. The analysis is based on the Economy Watchers Survey (EWS),\na monthly survey conducted by the Cabinet Office of Japan that captures\nreal-time economic assessments from workers in industries highly sensitive to\nbusiness conditions. The WSI extends the framework of the Price Sentiment Index\n(PSI) used in prior studies, adapting it specifically to wage related\nsentiment. To ensure scalability and adaptability, a data architecture is also\ndeveloped that enables integration of additional sources such as newspapers and\nsocial media. Experimental results demonstrate that WSI models based on LLMs\nsignificantly outperform both baseline approaches and pretrained models. These\nfindings highlight the potential of LLM-driven sentiment indices to enhance the\ntimeliness and effectiveness of economic policy design by governments and\ncentral banks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u5de5\u8d44\u60c5\u7eea\u6307\u6570(WSI)\uff0c\u7528\u4e8e\u9884\u6d4b\u65e5\u672c\u7684\u5de5\u8d44\u52a8\u6001\uff0c\u5e76\u53d1\u73b0\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4e3a\u7ecf\u6d4e\u6587\u672c\u5206\u6790\u521b\u9020\u4e86\u65b0\u673a\u9047\uff0c\u65e8\u5728\u5229\u7528LLM\u5f00\u53d1\u4e00\u79cd\u66f4\u53ca\u65f6\u3001\u6709\u6548\u7684\u5de5\u5177\u6765\u76d1\u6d4b\u548c\u9884\u6d4b\u7ecf\u6d4e\u6307\u6807\uff08\u5982\u5de5\u8d44\uff09\uff0c\u4ee5\u8f85\u52a9\u653f\u5e9c\u548c\u592e\u884c\u7684\u7ecf\u6d4e\u653f\u7b56\u5236\u5b9a\u3002", "method": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u5de5\u8d44\u60c5\u7eea\u6307\u6570\uff08WSI\uff09\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u65e5\u672c\u5185\u9601\u5e9c\u7684\u201c\u7ecf\u6d4e\u89c2\u5bdf\u8005\u8c03\u67e5\u201d\uff08EWS\uff09\u6570\u636e\u8fdb\u884c\u5206\u6790\u3002WSI\u6269\u5c55\u4e86\u6b64\u524d\u4ef7\u683c\u60c5\u7eea\u6307\u6570\uff08PSI\uff09\u7684\u6846\u67b6\uff0c\u5e76\u4e3a\u786e\u4fdd\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u636e\u67b6\u6784\uff0c\u5141\u8bb8\u6574\u5408\u62a5\u7eb8\u548c\u793e\u4ea4\u5a92\u4f53\u7b49\u989d\u5916\u6570\u636e\u6e90\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eLLM\u6784\u5efa\u7684WSI\u6a21\u578b\u5728\u9884\u6d4b\u5de5\u8d44\u52a8\u6001\u65b9\u9762\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86LLM\u9a71\u52a8\u7684\u60c5\u7eea\u6307\u6570\u5728\u63d0\u5347\u653f\u5e9c\u548c\u4e2d\u592e\u94f6\u884c\u7ecf\u6d4e\u653f\u7b56\u8bbe\u8ba1\u7684\u53ca\u65f6\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.00621", "pdf": "https://arxiv.org/pdf/2509.00621", "abs": "https://arxiv.org/abs/2509.00621", "authors": ["Osama Abu Hamdan", "Hao Che", "Engin Arslan", "Md Arifuzzaman"], "title": "FLEET: A Federated Learning Emulation and Evaluation Testbed for Holistic Research", "categories": ["cs.NI"], "comment": null, "summary": "Federated Learning (FL) presents a robust paradigm for privacy-preserving,\ndecentralized machine learning. However, a significant gap persists between the\ntheoretical design of FL algorithms and their practical performance, largely\nbecause existing evaluation tools often fail to model realistic operational\nconditions. Many testbeds oversimplify the critical dynamics among algorithmic\nefficiency, client-level heterogeneity, and continuously evolving network\ninfrastructure. To address this challenge, we introduce the Federated Learning\nEmulation and Evaluation Testbed (FLEET). This comprehensive platform provides\na scalable and configurable environment by integrating a versatile,\nframework-agnostic learning component with a high-fidelity network emulator.\nFLEET supports diverse machine learning frameworks, customizable real-world\nnetwork topologies, and dynamic background traffic generation. The testbed\ncollects holistic metrics that correlate algorithmic outcomes with detailed\nnetwork statistics. By unifying the entire experiment configuration, FLEET\nenables researchers to systematically investigate how network constraints, such\nas limited bandwidth, high latency, and packet loss, affect the convergence and\nefficiency of FL algorithms. This work provides the research community with a\nrobust tool to bridge the gap between algorithmic theory and real-world network\nconditions, promoting the holistic and reproducible evaluation of federated\nlearning systems.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u8bc4\u4f30\u5de5\u5177\u7f3a\u4e4f\u771f\u5b9e\u6027\uff0c\u672c\u6587\u63d0\u51fa\u4e86FLEET\uff0c\u4e00\u4e2a\u96c6\u6210\u5b66\u4e60\u7ec4\u4ef6\u4e0e\u7f51\u7edc\u4eff\u771f\u5668\u7684\u65b0\u578b\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u5728\u771f\u5b9e\u7f51\u7edc\u6761\u4ef6\u4e0b\u5168\u9762\u8bc4\u4f30\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u8bc4\u4f30\u5de5\u5177\u672a\u80fd\u6709\u6548\u6a21\u62df\u771f\u5b9e\u64cd\u4f5c\u6761\u4ef6\uff0c\u5bfc\u81f4\u8054\u90a6\u5b66\u4e60 (FL) \u7b97\u6cd5\u7406\u8bba\u8bbe\u8ba1\u4e0e\u5b9e\u9645\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5c24\u5176\u5728\u7b97\u6cd5\u6548\u7387\u3001\u5ba2\u6237\u7aef\u5f02\u6784\u6027\u53ca\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u52a8\u6001\u6027\u65b9\u9762\u3002", "method": "\u5f15\u5165\u4e86\u8054\u90a6\u5b66\u4e60\u4eff\u771f\u4e0e\u8bc4\u4f30\u6d4b\u8bd5\u5e73\u53f0 (FLEET)\u3002\u8be5\u5e73\u53f0\u901a\u8fc7\u6574\u5408\u901a\u7528\u3001\u6846\u67b6\u65e0\u5173\u7684\u5b66\u4e60\u7ec4\u4ef6\u548c\u9ad8\u4fdd\u771f\u7f51\u7edc\u4eff\u771f\u5668\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u548c\u53ef\u914d\u7f6e\u7684\u73af\u5883\uff0c\u652f\u6301\u591a\u6837\u673a\u5668\u5b66\u4e60\u6846\u67b6\u3001\u53ef\u5b9a\u5236\u7684\u771f\u5b9e\u7f51\u7edc\u62d3\u6251\u548c\u52a8\u6001\u80cc\u666f\u6d41\u91cf\u751f\u6210\uff0c\u5e76\u6536\u96c6\u5173\u8054\u7b97\u6cd5\u7ed3\u679c\u4e0e\u8be6\u7ec6\u7f51\u7edc\u7edf\u8ba1\u6570\u636e\u7684\u6574\u4f53\u6307\u6807\u3002", "result": "FLEET\u4f7f\u5f97\u7814\u7a76\u4eba\u5458\u80fd\u591f\u7cfb\u7edf\u5730\u8c03\u67e5\u7f51\u7edc\u7ea6\u675f\uff08\u5982\u6709\u9650\u5e26\u5bbd\u3001\u9ad8\u5ef6\u8fdf\u548c\u4e22\u5305\uff09\u5982\u4f55\u5f71\u54cd\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u7684\u6536\u655b\u6027\u548c\u6548\u7387\u3002", "conclusion": "FLEET\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u65e8\u5728\u5f25\u5408\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u7406\u8bba\u4e0e\u771f\u5b9e\u7f51\u7edc\u6761\u4ef6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4fc3\u8fdb\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u4e0e\u53ef\u91cd\u73b0\u8bc4\u4f30\u3002"}}
{"id": "2509.00226", "pdf": "https://arxiv.org/pdf/2509.00226", "abs": "https://arxiv.org/abs/2509.00226", "authors": ["Ren\u00e9 Parlange", "Juan C. Cuevas-Tello", "Octavio Valenzuela", "Omar de J. Cabrera-Rosas", "Tom\u00e1s Verdugo", "Anupreeta More", "Anton T. Jaelani"], "title": "GraViT: Transfer Learning with Vision Transformers and MLP-Mixer for Strong Gravitational Lens Discovery", "categories": ["cs.CV", "astro-ph.GA"], "comment": "Our publicly available fine-tuned models provide a scalable transfer\n  learning solution for gravitational lens finding in LSST. Submitted to MNRAS.\n  Comments welcome", "summary": "Gravitational lensing offers a powerful probe into the properties of dark\nmatter and is crucial to infer cosmological parameters. The Legacy Survey of\nSpace and Time (LSST) is predicted to find O(10^5) gravitational lenses over\nthe next decade, demanding automated classifiers. In this work, we introduce\nGraViT, a PyTorch pipeline for gravitational lens detection that leverages\nextensive pretraining of state-of-the-art Vision Transformer (ViT) models and\nMLP-Mixer. We assess the impact of transfer learning on classification\nperformance by examining data quality (source and sample size), model\narchitecture (selection and fine-tuning), training strategies (augmentation,\nnormalization, and optimization), and ensemble predictions. This study\nreproduces the experiments in a previous systematic comparison of neural\nnetworks and provides insights into the detectability of strong gravitational\nlenses on that common test sample. We fine-tune ten architectures using\ndatasets from HOLISMOKES VI and SuGOHI X, and benchmark them against\nconvolutional baselines, discussing complexity and inference-time analysis.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86GraViT\uff0c\u4e00\u4e2a\u57fa\u4e8ePyTorch\u7684\u5f15\u529b\u900f\u955c\u81ea\u52a8\u68c0\u6d4b\u7ba1\u9053\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684Vision Transformer\u548cMLP-Mixer\u6a21\u578b\u53ca\u8fc1\u79fb\u5b66\u4e60\uff0c\u4ee5\u5e94\u5bf9LSST\u6d77\u91cf\u6570\u636e\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f3a\u5f15\u529b\u900f\u955c\u53ef\u68c0\u6d4b\u6027\u7684\u89c1\u89e3\u3002", "motivation": "\u5f15\u529b\u900f\u955c\u662f\u63a2\u6d4b\u6697\u7269\u8d28\u5c5e\u6027\u548c\u63a8\u65ad\u5b87\u5b99\u5b66\u53c2\u6570\u7684\u91cd\u8981\u624b\u6bb5\u3002\u5373\u5c06\u5230\u6765\u7684LSST\u8c03\u67e5\u9884\u8ba1\u5c06\u53d1\u73b0\u7ea610^5\u4e2a\u5f15\u529b\u900f\u955c\uff0c\u8fd9\u8feb\u5207\u9700\u8981\u81ea\u52a8\u5316\u7684\u5206\u7c7b\u5668\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86GraViT\uff0c\u4e00\u4e2a\u5229\u7528\u6700\u5148\u8fdb\u7684Vision Transformer (ViT) \u6a21\u578b\u548cMLP-Mixer\u8fdb\u884c\u5e7f\u6cdb\u9884\u8bad\u7ec3\u7684PyTorch\u7ba1\u9053\u3002\u901a\u8fc7\u8003\u5bdf\u6570\u636e\u8d28\u91cf\u3001\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u96c6\u6210\u9884\u6d4b\uff0c\u8bc4\u4f30\u4e86\u8fc1\u79fb\u5b66\u4e60\u5bf9\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u8be5\u7814\u7a76\u590d\u73b0\u4e86\u4e4b\u524d\u795e\u7ecf\u7f51\u7edc\u7cfb\u7edf\u6bd4\u8f83\u7684\u5b9e\u9a8c\uff0c\u5e76\u4f7f\u7528HOLISMOKES VI\u548cSuGOHI X\u6570\u636e\u96c6\u5bf9\u5341\u79cd\u67b6\u6784\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u5c06\u5176\u4e0e\u5377\u79ef\u57fa\u7ebf\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5206\u6790\u4e86\u590d\u6742\u6027\u548c\u63a8\u7406\u65f6\u95f4\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u5728\u901a\u7528\u6d4b\u8bd5\u6837\u672c\u4e0a\u5f3a\u5f15\u529b\u900f\u955c\u53ef\u68c0\u6d4b\u6027\u7684\u89c1\u89e3\u3002\u901a\u8fc7\u4e0e\u5377\u79ef\u57fa\u7ebf\u5bf9\u6bd4\uff0c\u8ba8\u8bba\u4e86\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u7684\u590d\u6742\u6027\u548c\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "GraViT\u4f5c\u4e3a\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u548c\u5148\u8fdbVision Transformer\u6a21\u578b\u7684\u81ea\u52a8\u5316\u5206\u7c7b\u5668\uff0c\u5728\u5f15\u529b\u900f\u955c\u68c0\u6d4b\u65b9\u9762\u5c55\u73b0\u4e86\u6f5c\u529b\uff0c\u5e76\u4e3a\u7406\u89e3\u5f3a\u5f15\u529b\u900f\u955c\u7684\u53ef\u68c0\u6d4b\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u6d1e\u5bdf\u3002"}}
{"id": "2509.00184", "pdf": "https://arxiv.org/pdf/2509.00184", "abs": "https://arxiv.org/abs/2509.00184", "authors": ["Alexandru Baltag", "Malvin Gattinger", "Djanira Gomes"], "title": "Virtual Group Knowledge and Group Belief in Topological Evidence Models (Extended Version)", "categories": ["cs.AI", "cs.LO", "cs.MA", "03B42", "I.2.4"], "comment": null, "summary": "We study notions of (virtual) group knowledge and group belief within\nmulti-agent evidence models, obtained by extending the topological semantics of\nevidence-based belief and fallible knowledge from individuals to groups. We\ncompletely axiomatize and show the decidability of the logic of (\"hard\" and\n\"soft\") group evidence, and do the same for an especially interesting fragment\nof it: the logic of group knowledge and group belief. We also extend these\nlanguages with dynamic evidence-sharing operators, and completely axiomatize\nthe corresponding logics, showing that they are co-expressive with their static\nbases.", "AI": {"tldr": "\u5728\u591a\u667a\u80fd\u4f53\u8bc1\u636e\u6a21\u578b\u4e2d\uff0c\u672c\u6587\u7814\u7a76\u4e86\u7fa4\u7ec4\u77e5\u8bc6\u548c\u4fe1\u5ff5\uff0c\u5bf9\u76f8\u5173\u903b\u8f91\u8fdb\u884c\u4e86\u5b8c\u5168\u516c\u7406\u5316\u548c\u53ef\u5224\u5b9a\u6027\u8bc1\u660e\uff0c\u5e76\u6269\u5c55\u4e86\u52a8\u6001\u8bc1\u636e\u5171\u4eab\u64cd\u4f5c\u7b26\u3002", "motivation": "\u5c06\u57fa\u4e8e\u8bc1\u636e\u7684\u4e2a\u4f53\u4fe1\u5ff5\u548c\u53ef\u9519\u77e5\u8bc6\u7684\u62d3\u6251\u8bed\u4e49\u6269\u5c55\u5230\u7fa4\u7ec4\uff0c\u4ee5\u5728\u591a\u667a\u80fd\u4f53\u8bc1\u636e\u6a21\u578b\u4e2d\u5f62\u5f0f\u5316\u548c\u7406\u89e3\uff08\u865a\u62df\uff09\u7fa4\u7ec4\u77e5\u8bc6\u548c\u7fa4\u7ec4\u4fe1\u5ff5\u7684\u6982\u5ff5\u3002", "method": "1. \u5c06\u57fa\u4e8e\u8bc1\u636e\u7684\u4e2a\u4f53\u4fe1\u5ff5\u548c\u53ef\u9519\u77e5\u8bc6\u7684\u62d3\u6251\u8bed\u4e49\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u8bc1\u636e\u6a21\u578b\u4e2d\u7684\u7fa4\u7ec4\u30022. \u5bf9\u201c\u786c\u201d\u548c\u201c\u8f6f\u201d\u7fa4\u7ec4\u8bc1\u636e\u903b\u8f91\u53ca\u5176\u91cd\u8981\u7684\u5b50\u96c6\uff08\u7fa4\u7ec4\u77e5\u8bc6\u548c\u7fa4\u7ec4\u4fe1\u5ff5\u903b\u8f91\uff09\u8fdb\u884c\u5b8c\u5168\u516c\u7406\u5316\u548c\u53ef\u5224\u5b9a\u6027\u8bc1\u660e\u30023. \u5f15\u5165\u52a8\u6001\u8bc1\u636e\u5171\u4eab\u64cd\u4f5c\u7b26\u6269\u5c55\u8fd9\u4e9b\u8bed\u8a00\uff0c\u5e76\u5bf9\u76f8\u5e94\u7684\u903b\u8f91\u8fdb\u884c\u5b8c\u5168\u516c\u7406\u5316\u3002", "result": "1. \u5f97\u5230\u4e86\u201c\u786c\u201d\u548c\u201c\u8f6f\u201d\u7fa4\u7ec4\u8bc1\u636e\u903b\u8f91\u7684\u5b8c\u5168\u516c\u7406\u5316\u548c\u53ef\u5224\u5b9a\u6027\u30022. \u5f97\u5230\u4e86\u7fa4\u7ec4\u77e5\u8bc6\u548c\u7fa4\u7ec4\u4fe1\u5ff5\u903b\u8f91\u7684\u5b8c\u5168\u516c\u7406\u5316\u548c\u53ef\u5224\u5b9a\u6027\u30023. \u5f97\u5230\u4e86\u6269\u5c55\u4e86\u52a8\u6001\u8bc1\u636e\u5171\u4eab\u64cd\u4f5c\u7b26\u7684\u76f8\u5e94\u903b\u8f91\u7684\u5b8c\u5168\u516c\u7406\u5316\u30024. \u8bc1\u660e\u4e86\u8fd9\u4e9b\u52a8\u6001\u903b\u8f91\u4e0e\u5176\u9759\u6001\u57fa\u7840\u5177\u6709\u5171\u8868\u8fbe\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u591a\u667a\u80fd\u4f53\u8bc1\u636e\u6a21\u578b\u4e2d\u7684\u7fa4\u7ec4\u77e5\u8bc6\u548c\u7fa4\u7ec4\u4fe1\u5ff5\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u903b\u8f91\u6846\u67b6\uff0c\u5305\u62ec\u5176\u52a8\u6001\u6027\uff0c\u5e76\u901a\u8fc7\u5b8c\u5168\u516c\u7406\u5316\u548c\u53ef\u5224\u5b9a\u6027\u8bc1\u660e\uff0c\u6df1\u5316\u4e86\u5bf9\u590d\u6742\u7fa4\u4f53\u8ba4\u77e5\u73b0\u8c61\u7684\u7406\u89e3\u3002"}}
{"id": "2509.00057", "pdf": "https://arxiv.org/pdf/2509.00057", "abs": "https://arxiv.org/abs/2509.00057", "authors": ["Yousuf Moiz Ali", "Jaroslaw E. Prilepsky", "Nicola Sambo", "Joao Pedro", "Mohammad M. Hosseini", "Antonio Napoli", "Sergei K. Turitsyn", "Pedro Freire"], "title": "From Data to Decision: A Multi-Stage Framework for Class Imbalance Mitigation in Optical Network Failure Analysis", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Machine learning-based failure management in optical networks has gained\nsignificant attention in recent years. However, severe class imbalance, where\nnormal instances vastly outnumber failure cases, remains a considerable\nchallenge. While pre- and in-processing techniques have been widely studied,\npost-processing methods are largely unexplored. In this work, we present a\ndirect comparison of pre-, in-, and post-processing approaches for class\nimbalance mitigation in failure detection and identification using an\nexperimental dataset. For failure detection, post-processing\nmethods-particularly Threshold Adjustment-achieve the highest F1 score\nimprovement (up to 15.3%), while Random Under-Sampling provides the fastest\ninference. In failure identification, GenAI methods deliver the most\nsubstantial performance gains (up to 24.2%), whereas post-processing shows\nlimited impact in multi-class settings. When class overlap is present and\nlatency is critical, over-sampling methods such as the SMOTE are most\neffective; without latency constraints, Meta-Learning yields the best results.\nIn low-overlap scenarios, Generative AI approaches provide the highest\nperformance with minimal inference time.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u9884\u5904\u7406\u3001\u5185\u90e8\u5904\u7406\u548c\u540e\u5904\u7406\u65b9\u6cd5\u5728\u7f13\u89e3\u5149\u7f51\u7edc\u6545\u969c\u7ba1\u7406\u4e2d\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u4e0a\u7684\u6548\u679c\u3002\u7ed3\u679c\u663e\u793a\uff0c\u540e\u5904\u7406\u5728\u6545\u969c\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800c\u751f\u6210\u5f0fAI\u5728\u6545\u969c\u8bc6\u522b\u4e2d\u6027\u80fd\u7a81\u51fa\u3002\u6700\u4f73\u65b9\u6cd5\u9009\u62e9\u53d6\u51b3\u4e8e\u7c7b\u522b\u91cd\u53e0\u548c\u5ef6\u8fdf\u9700\u6c42\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u5149\u7f51\u7edc\u6545\u969c\u7ba1\u7406\u4e2d\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u4e14\u540e\u5904\u7406\u65b9\u6cd5\u5728\u6b64\u9886\u57df\u7684\u63a2\u7d22\u8f83\u5c11\u3002", "method": "\u4f7f\u7528\u5b9e\u9a8c\u6570\u636e\u96c6\uff0c\u76f4\u63a5\u6bd4\u8f83\u4e86\u9884\u5904\u7406\u3001\u5185\u90e8\u5904\u7406\u548c\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u4ee5\u7f13\u89e3\u6545\u969c\u68c0\u6d4b\u548c\u8bc6\u522b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u6545\u969c\u68c0\u6d4b\u4e2d\uff0c\u540e\u5904\u7406\u65b9\u6cd5\uff08\u7279\u522b\u662f\u9608\u503c\u8c03\u6574\uff09F1\u5206\u6570\u63d0\u5347\u6700\u9ad8\uff08\u8fbe15.3%\uff09\uff0c\u968f\u673a\u6b20\u91c7\u6837\u63a8\u7406\u901f\u5ea6\u6700\u5feb\u3002\u6545\u969c\u8bc6\u522b\u4e2d\uff0c\u751f\u6210\u5f0fAI\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u6700\u663e\u8457\uff08\u8fbe24.2%\uff09\uff0c\u800c\u540e\u5904\u7406\u5f71\u54cd\u6709\u9650\u3002\u5728\u7c7b\u522b\u91cd\u53e0\u4e14\u5ef6\u8fdf\u5173\u952e\u65f6\uff0cSMOTE\u7b49\u8fc7\u91c7\u6837\u65b9\u6cd5\u6700\u6709\u6548\uff1b\u65e0\u5ef6\u8fdf\u7ea6\u675f\u65f6\uff0c\u5143\u5b66\u4e60\u6548\u679c\u6700\u4f73\u3002\u5728\u4f4e\u91cd\u53e0\u573a\u666f\u4e2d\uff0c\u751f\u6210\u5f0fAI\u65b9\u6cd5\u6027\u80fd\u6700\u9ad8\u4e14\u63a8\u7406\u65f6\u95f4\u6700\u77ed\u3002", "conclusion": "\u9488\u5bf9\u5149\u7f51\u7edc\u6545\u969c\u7ba1\u7406\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e0d\u540c\u5904\u7406\u65b9\u6cd5\u5728\u6545\u969c\u68c0\u6d4b\u548c\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u5404\u5f02\u3002\u540e\u5904\u7406\u5728\u6545\u969c\u68c0\u6d4b\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u751f\u6210\u5f0fAI\u5728\u6545\u969c\u8bc6\u522b\u4e2d\u6548\u679c\u663e\u8457\u3002\u65b9\u6cd5\u7684\u9009\u62e9\u5e94\u6839\u636e\u5177\u4f53\u573a\u666f\uff08\u5982\u7c7b\u522b\u91cd\u53e0\u7a0b\u5ea6\u3001\u5ef6\u8fdf\u8981\u6c42\uff09\u8fdb\u884c\u4f18\u5316\u3002"}}
{"id": "2509.00309", "pdf": "https://arxiv.org/pdf/2509.00309", "abs": "https://arxiv.org/abs/2509.00309", "authors": ["Chen Zheng", "Yiyuan Ma", "Yuan Yang", "Deyi Liu", "Jing Liu", "Zuquan Song", "Yuxin Song", "Cheng Ren", "Hang Zhu", "Xin Liu", "Yiyuan Ma", "Siyuan Qiao", "Xun Zhou", "Liang Xiang", "Yonghui Wu"], "title": "Balanced Actor Initialization: Stable RLHF Training of Distillation-Based Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "The development of alignment and reasoning capabilities in large language\nmodels has seen remarkable progress through two paradigms: instruction tuning\nand reinforcement learning from human feedback (RLHF) alignment paradigm, and\ndistillation-based reasoning fine-tuning paradigm. While both approaches prove\neffective independently, the third paradigm of applying RLHF to\ndistillation-trained models presents significant challenges. Our investigation\nreveals two critical phenomena that emerge in this paradigm: Sequence Length\nCollapse, where language generation dramatically reduces during early RLHF\ntraining, and the Reward Hockey Stick Curve, featuring severe reward score\ndrops followed by gradual recovery. These instabilities fundamentally\ncompromise the model's alignment and reasoning capabilities. To address these\nchallenges, we propose Balanced Actor Initialization (BAI), a two-stage\nweighted model merging approach. BAI first merges instruction-following and\ndistillation-based reasoning fine-tuned models, then further combines this\nintermediate model with the pretrained model to preserve foundational\nknowledge. Through comprehensive experiments across diverse benchmarks and\ndetailed analysis of training experiments, we demonstrate that BAI resolves\nSequence Length Collapse, mitigates the Reward Hockey Stick Curve, and enables\ncontinuous sequence length improvement during training. Additionally, our\nanalysis reveals that balanced merging ratios achieve optimal trade-offs\nbetween training stability and reasoning capability preservation. Our work\nprovides the effective solution for stable training in this third paradigm,\nenabling more capable reasoning models that combine distillation efficiency\nwith RLHF alignment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5e73\u8861Actor\u521d\u59cb\u5316 (BAI) \u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u5c06RLHF\u5e94\u7528\u4e8e\u84b8\u998f\u8bad\u7ec3\u6a21\u578b\u65f6\u51fa\u73b0\u7684\u5e8f\u5217\u957f\u5ea6\u5d29\u6e83\u548c\u5956\u52b1\u66f2\u68cd\u7403\u6746\u66f2\u7ebf\u7b49\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4ece\u800c\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u5e76\u7ed3\u5408\u84b8\u998f\u6548\u7387\u4e0eRLHF\u5bf9\u9f50\u3002", "motivation": "\u5c3d\u7ba1\u6307\u4ee4\u5fae\u8c03/RLHF\u548c\u84b8\u998f\u63a8\u7406\u5fae\u8c03\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u72ec\u7acb\u6709\u6548\uff0c\u4f46\u5c06RLHF\u5e94\u7528\u4e8e\u7ecf\u8fc7\u84b8\u998f\u8bad\u7ec3\u7684\u6a21\u578b\uff08\u7b2c\u4e09\u8303\u5f0f\uff09\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u8868\u73b0\u4e3a\u5e8f\u5217\u957f\u5ea6\u5d29\u6e83\u548c\u5956\u52b1\u66f2\u68cd\u7403\u6746\u66f2\u7ebf\uff0c\u4e25\u91cd\u635f\u5bb3\u6a21\u578b\u7684\u5bf9\u9f50\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5e73\u8861Actor\u521d\u59cb\u5316 (Balanced Actor Initialization, BAI)\u3002\u8fd9\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u52a0\u6743\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff1a\u9996\u5148\u5408\u5e76\u6307\u4ee4\u9075\u5faa\u6a21\u578b\u548c\u84b8\u998f\u63a8\u7406\u5fae\u8c03\u6a21\u578b\uff0c\u7136\u540e\u5c06\u6b64\u4e2d\u95f4\u6a21\u578b\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u4e00\u6b65\u7ed3\u5408\uff0c\u4ee5\u4fdd\u7559\u57fa\u7840\u77e5\u8bc6\u3002", "result": "BAI\u89e3\u51b3\u4e86\u5e8f\u5217\u957f\u5ea6\u5d29\u6e83\u95ee\u9898\uff0c\u7f13\u89e3\u4e86\u5956\u52b1\u66f2\u68cd\u7403\u6746\u66f2\u7ebf\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u4e86\u5e8f\u5217\u957f\u5ea6\u7684\u6301\u7eed\u6539\u8fdb\u3002\u5206\u6790\u8868\u660e\uff0c\u5e73\u8861\u7684\u5408\u5e76\u6bd4\u4f8b\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0e\u63a8\u7406\u80fd\u529b\u4fdd\u7559\u4e4b\u95f4\u8fbe\u5230\u4e86\u6700\u4f73\u6743\u8861\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u7b2c\u4e09\u8303\u5f0f\u4e0b\u7684\u7a33\u5b9a\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u80fd\u591f\u6784\u5efa\u7ed3\u5408\u84b8\u998f\u6548\u7387\u548cRLHF\u5bf9\u9f50\u7684\u66f4\u5f3a\u5927\u7684\u63a8\u7406\u6a21\u578b\u3002"}}
{"id": "2509.00701", "pdf": "https://arxiv.org/pdf/2509.00701", "abs": "https://arxiv.org/abs/2509.00701", "authors": ["Kun Qiu", "Ying Wang", "Baoqian Li", "Wenjun Zhu"], "title": "Unsupervised Dataset Cleaning Framework for Encrypted Traffic Classification", "categories": ["cs.NI", "cs.AI"], "comment": "Accepted in IEEE ICNP 2025 Poster", "summary": "Traffic classification, a technique for assigning network flows to predefined\ncategories, has been widely deployed in enterprise and carrier networks. With\nthe massive adoption of mobile devices, encryption is increasingly used in\nmobile applications to address privacy concerns. Consequently, traditional\nmethods such as Deep Packet Inspection (DPI) fail to distinguish encrypted\ntraffic. To tackle this challenge, Artificial Intelligence (AI), in particular\nMachine Learning (ML), has emerged as a promising solution for encrypted\ntraffic classification. A crucial prerequisite for any ML-based approach is\ntraffic data cleaning, which removes flows that are not useful for training\n(e.g., irrelevant protocols, background activity, control-plane messages, and\nlong-lived sessions). Existing cleaning solutions depend on manual inspection\nof every captured packet, making the process both costly and time-consuming. In\nthis poster, we present an unsupervised framework that automatically cleans\nencrypted mobile traffic. Evaluation on real-world datasets shows that our\nframework incurs only a 2%~2.5% reduction in classification accuracy compared\nwith manual cleaning. These results demonstrate that our method offers an\nefficient and effective preprocessing step for ML-based encrypted traffic\nclassification.", "AI": {"tldr": "\u9488\u5bf9\u52a0\u5bc6\u79fb\u52a8\u6d41\u91cf\u5206\u7c7b\u4e2d\u673a\u5668\u5b66\u4e60\u5bf9\u6570\u636e\u6e05\u6d17\u7684\u9700\u6c42\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u81ea\u52a8\u6e05\u6d17\u6846\u67b6\uff0c\u5176\u6027\u80fd\u4e0e\u624b\u52a8\u6e05\u6d17\u76f8\u5f53\uff0c\u4ec5\u5bfc\u81f42%~2.5%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u4e3aML-based\u5206\u7c7b\u63d0\u4f9b\u9ad8\u6548\u9884\u5904\u7406\u3002", "motivation": "\u968f\u7740\u79fb\u52a8\u8bbe\u5907\u666e\u53ca\uff0c\u52a0\u5bc6\u6280\u672f\u5e7f\u6cdb\u5e94\u7528\u4e8e\u79fb\u52a8\u5e94\u7528\uff0c\u5bfc\u81f4\u4f20\u7edf\u6d41\u91cf\u5206\u7c7b\u65b9\u6cd5\uff08\u5982DPI\uff09\u65e0\u6cd5\u8bc6\u522b\u52a0\u5bc6\u6d41\u91cf\u3002\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\u662f\u89e3\u51b3\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u7684\u6709\u6548\u65b9\u6848\uff0c\u4f46\u5176\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u800c\u73b0\u6709\u624b\u52a8\u6570\u636e\u6e05\u6d17\u8fc7\u7a0b\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u52a8\u6e05\u6d17\u52a0\u5bc6\u79fb\u52a8\u6d41\u91cf\u6570\u636e\uff0c\u53bb\u9664\u4e0d\u9002\u7528\u4e8e\u8bad\u7ec3\u7684\u6d41\u91cf\uff08\u4f8b\u5982\u65e0\u5173\u534f\u8bae\u3001\u540e\u53f0\u6d3b\u52a8\u3001\u63a7\u5236\u5e73\u9762\u6d88\u606f\u548c\u957f\u65f6\u95f4\u4f1a\u8bdd\uff09\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u624b\u52a8\u6e05\u6d17\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u4ec5\u5bfc\u81f4\u5206\u7c7b\u51c6\u786e\u7387\u4e0b\u964d2%~2.5%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65e0\u76d1\u7763\u81ea\u52a8\u6e05\u6d17\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u6709\u6548\u7684\u9884\u5904\u7406\u6b65\u9aa4\u3002"}}
{"id": "2509.00231", "pdf": "https://arxiv.org/pdf/2509.00231", "abs": "https://arxiv.org/abs/2509.00231", "authors": ["Danil Kazimirov", "Dmitry Nikolaev"], "title": "A High-Accuracy Fast Hough Transform with Linear-Log-Cubed Computational Complexity for Arbitrary-Shaped Images", "categories": ["cs.CV"], "comment": "8 pages, 4 figures. Accepted to International Conference on Machine\n  Vision 2025 (ICMV 2025)", "summary": "The Hough transform (HT) is a fundamental tool across various domains, from\nclassical image analysis to neural networks and tomography. Two key aspects of\nthe algorithms for computing the HT are their computational complexity and\naccuracy - the latter often defined as the error of approximation of continuous\nlines by discrete ones within the image region. The fast HT (FHT) algorithms\nwith optimal linearithmic complexity - such as the Brady-Yong algorithm for\npower-of-two-sized images - are well established. Generalizations like $FHT2DT$\nextend this efficiency to arbitrary image sizes, but with reduced accuracy that\nworsens with scale. Conversely, accurate HT algorithms achieve constant-bounded\nerror but require near-cubic computational cost. This paper introduces $FHT2SP$\nalgorithm - a fast and highly accurate HT algorithm. It builds on our\ndevelopment of Brady's superpixel concept, extending it to arbitrary shapes\nbeyond the original power-of-two square constraint, and integrates it into the\n$FHT2DT$ algorithm. With an appropriate choice of the superpixel's size, for an\nimage of shape $w \\times h$, the $FHT2SP$ algorithm achieves near-optimal\ncomputational complexity $\\mathcal{O}(wh \\ln^3 w)$, while keeping the\napproximation error bounded by a constant independent of image size, and\ncontrollable via a meta-parameter. We provide theoretical and experimental\nanalyses of the algorithm's complexity and accuracy.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.00189", "pdf": "https://arxiv.org/pdf/2509.00189", "abs": "https://arxiv.org/abs/2509.00189", "authors": ["Jinzhou Tang", "Jusheng Zhang", "Qinhan Lv", "Sidi Liu", "Jing Yang", "Chengpei Tang", "Keze Wang"], "title": "HiVA: Self-organized Hierarchical Variable Agent via Goal-driven Semantic-Topological Evolution", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Autonomous agents play a crucial role in advancing Artificial General\nIntelligence, enabling problem decomposition and tool orchestration through\nLarge Language Models (LLMs). However, existing paradigms face a critical\ntrade-off. On one hand, reusable fixed workflows require manual reconfiguration\nupon environmental changes; on the other hand, flexible reactive loops fail to\ndistill reasoning progress into transferable structures. We introduce\nHierarchical Variable Agent (HiVA), a novel framework modeling agentic\nworkflows as self-organized graphs with the Semantic-Topological Evolution\n(STEV) algorithm, which optimizes hybrid semantic-topological spaces using\ntextual gradients as discrete-domain surrogates for backpropagation. The\niterative process comprises Multi-Armed Bandit-infused forward routing,\ndiagnostic gradient generation from environmental feedback, and coordinated\nupdates that co-evolve individual semantics and topology for collective\noptimization in unknown environments. Experiments on dialogue, coding,\nLong-context Q&A, mathematical, and agentic benchmarks demonstrate improvements\nof 5-10% in task accuracy and enhanced resource efficiency over existing\nbaselines, establishing HiVA's effectiveness in autonomous task execution.", "AI": {"tldr": "HiVA\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u7ec4\u7ec7\u56fe\u548cSTEV\u7b97\u6cd5\u7684\u81ea\u4e3b\u4ee3\u7406\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u5728\u53ef\u91cd\u7528\u6027\u548c\u7075\u6d3b\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u7cbe\u5ea6\u548c\u8d44\u6e90\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u4ee3\u7406\u8303\u5f0f\u5b58\u5728\u7f3a\u9677\uff1a\u56fa\u5b9a\u5de5\u4f5c\u6d41\u9700\u624b\u52a8\u91cd\u65b0\u914d\u7f6e\uff0c\u800c\u53cd\u5e94\u5f0f\u5faa\u73af\u65e0\u6cd5\u5c06\u63a8\u7406\u8fdb\u5c55\u8f6c\u5316\u4e3a\u53ef\u8fc1\u79fb\u7ed3\u6784\uff0c\u9650\u5236\u4e86LLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u548cAGI\u53d1\u5c55\u3002", "method": "\u5f15\u5165\u5206\u5c42\u53ef\u53d8\u4ee3\u7406(HiVA)\u6846\u67b6\uff0c\u5c06\u4ee3\u7406\u5de5\u4f5c\u6d41\u5efa\u6a21\u4e3a\u81ea\u7ec4\u7ec7\u56fe\u3002\u91c7\u7528\u8bed\u4e49\u62d3\u6251\u6f14\u5316(STEV)\u7b97\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u68af\u5ea6\u4f18\u5316\u6df7\u5408\u8bed\u4e49\u62d3\u6251\u7a7a\u95f4\u3002\u8fed\u4ee3\u8fc7\u7a0b\u5305\u62ec\u591a\u81c2\u8d4c\u535a\u673a\u9a71\u52a8\u7684\u6b63\u5411\u8def\u7531\u3001\u73af\u5883\u53cd\u9988\u8bca\u65ad\u68af\u5ea6\u751f\u6210\uff0c\u4ee5\u53ca\u8bed\u4e49\u4e0e\u62d3\u6251\u7684\u534f\u540c\u66f4\u65b0\u3002", "result": "\u5728\u5bf9\u8bdd\u3001\u7f16\u7a0b\u3001\u957f\u6587\u672c\u95ee\u7b54\u3001\u6570\u5b66\u548c\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u9ad8\u4e865-10%\uff0c\u8d44\u6e90\u6548\u7387\u4e5f\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "HiVA\u6846\u67b6\u5728\u81ea\u4e3b\u4efb\u52a1\u6267\u884c\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6709\u6548\u6027\uff0c\u901a\u8fc7\u4f18\u5316\u5de5\u4f5c\u6d41\u7684\u81ea\u7ec4\u7ec7\u80fd\u529b\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2509.00066", "pdf": "https://arxiv.org/pdf/2509.00066", "abs": "https://arxiv.org/abs/2509.00066", "authors": ["Chuanxiang Yang", "Yuanfeng Zhou", "Guangshun Wei", "Siyu Ren", "Yuan Liu", "Junhui Hou", "Wenping Wang"], "title": "T-MLP: Tailed Multi-Layer Perceptron for Level-of-Detail Signal Representation", "categories": ["cs.LG", "cs.GR", "eess.IV"], "comment": null, "summary": "Level-of-detail (LoD) representation is critical for efficiently modeling and\ntransmitting various types of signals, such as images and 3D shapes. In this\nwork, we present a novel neural architecture that supports LoD signal\nrepresentation. Our architecture is based on an elaborate modification of the\nwidely used Multi-Layer Perceptron (MLP), which inherently operates at a single\nscale and therefore lacks native support for LoD. Specifically, we introduce\nthe Tailed Multi-Layer Perceptron (T-MLP) that extends the MLP by attaching\nmultiple output branches, also called tails, to its hidden layers, enabling\ndirect supervision at multiple depths. Our loss formulation and training\nstrategy allow each hidden layer to effectively learn a target signal at a\nspecific LoD, thus enabling multi-scale modeling. Extensive experimental\nresults show that our T-MLP outperforms other neural LoD baselines across a\nvariety of signal representation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aT-MLP\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u5728MLP\u4e2d\u6dfb\u52a0\u591a\u4e2a\u8f93\u51fa\u5206\u652f\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7ec6\u8282\u5c42\u6b21\uff08LoD\uff09\u4fe1\u53f7\u8868\u793a\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u7ec6\u8282\u5c42\u6b21\uff08LoD\uff09\u8868\u793a\u5bf9\u4e8e\u9ad8\u6548\u5efa\u6a21\u548c\u4f20\u8f93\u56fe\u50cf\u30013D\u5f62\u72b6\u7b49\u5404\u7c7b\u4fe1\u53f7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5e7f\u6cdb\u4f7f\u7528\u7684\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u672c\u8d28\u4e0a\u662f\u5355\u5c3a\u5ea6\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u5bf9LoD\u7684\u539f\u751f\u652f\u6301\u3002", "method": "\u5f15\u5165\u4e86Tailed Multi-Layer Perceptron (T-MLP)\uff0c\u901a\u8fc7\u5728MLP\u7684\u9690\u85cf\u5c42\u9644\u52a0\u591a\u4e2a\u8f93\u51fa\u5206\u652f\uff08\u5373\u201c\u5c3e\u5df4\u201d\uff09\uff0c\u5b9e\u73b0\u591a\u6df1\u5ea6\u76f4\u63a5\u76d1\u7763\u3002\u8bbe\u8ba1\u4e86\u7279\u5b9a\u7684\u635f\u5931\u51fd\u6570\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u4f7f\u6bcf\u4e2a\u9690\u85cf\u5c42\u80fd\u6709\u6548\u5b66\u4e60\u7279\u5b9aLoD\u7684\u76ee\u6807\u4fe1\u53f7\uff0c\u4ece\u800c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u5efa\u6a21\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cT-MLP\u5728\u5404\u79cd\u4fe1\u53f7\u8868\u793a\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u795e\u7ecfLoD\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "T-MLP\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u80fd\u591f\u539f\u751f\u652f\u6301\u591a\u5c3a\u5ea6\u7684\u7ec6\u8282\u5c42\u6b21\uff08LoD\uff09\u4fe1\u53f7\u8868\u793a\u3002"}}
{"id": "2509.00325", "pdf": "https://arxiv.org/pdf/2509.00325", "abs": "https://arxiv.org/abs/2509.00325", "authors": ["Rinku Dewri"], "title": "GIER: Gap-Driven Self-Refinement for Large Language Models", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We introduce GIER (Gap-driven Iterative Enhancement of Responses), a general\nframework for improving large language model (LLM) outputs through\nself-reflection and revision based on conceptual quality criteria. Unlike\nprompting strategies that rely on demonstrations, examples, or chain-of-thought\ntemplates, GIER utilizes natural language descriptions of reasoning gaps, and\nprompts a model to iteratively critique and refine its own outputs to better\nsatisfy these criteria. Across three reasoning-intensive tasks (SciFact,\nPrivacyQA, and e-SNLI) and four LLMs (GPT-4.1, GPT-4o Mini, Gemini 1.5 Pro, and\nLlama 3.3 70B), GIER improves rationale quality, grounding, and reasoning\nalignment without degrading task accuracy. Our analysis demonstrates that\nmodels can not only interpret abstract conceptual gaps but also translate them\ninto concrete reasoning improvements.", "AI": {"tldr": "GIER\u662f\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u548c\u57fa\u4e8e\u6982\u5ff5\u8d28\u91cf\u6807\u51c6\u7684\u8fed\u4ee3\u4fee\u8ba2\u6765\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8f93\u51fa\u3002\u5b83\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u63a8\u7406\u5dee\u8ddd\uff0c\u63d0\u5347\u4e86\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u4e2dLLM\u7684\u8f93\u51fa\u8d28\u91cf\uff08\u5982\u63a8\u7406\u8d28\u91cf\u3001\u57fa\u7840\u548c\u63a8\u7406\u5bf9\u9f50\uff09\uff0c\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u7b56\u7565\uff08\u5982\u6f14\u793a\u3001\u793a\u4f8b\u3001\u601d\u7ef4\u94fe\uff09\u672a\u80fd\u6709\u6548\u5229\u7528LLM\u7684\u81ea\u6211\u53cd\u601d\u80fd\u529b\u6765\u7cfb\u7edf\u6027\u5730\u6539\u8fdb\u8f93\u51fa\u8d28\u91cf\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u901a\u7528\u6846\u67b6\uff0c\u4f7fLLM\u80fd\u57fa\u4e8e\u62bd\u8c61\u7684\u6982\u5ff5\u8d28\u91cf\u6807\u51c6\uff0c\u901a\u8fc7\u8fed\u4ee3\u4fee\u8ba2\u6765\u586b\u8865\u63a8\u7406\u5dee\u8ddd\uff0c\u4ece\u800c\u63d0\u5347\u5176\u8f93\u51fa\u8d28\u91cf\u3002", "method": "\u5f15\u5165GIER (Gap-driven Iterative Enhancement of Responses) \u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u793a\u4f8b\u6216\u601d\u7ef4\u94fe\uff0c\u800c\u662f\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u201c\u63a8\u7406\u5dee\u8ddd\u201d\uff0c\u5f15\u5bfcLLM\u8fed\u4ee3\u5730\u6279\u5224\u5e76\u7cbe\u70bc\u81ea\u5df1\u7684\u8f93\u51fa\uff0c\u4ee5\u66f4\u597d\u5730\u6ee1\u8db3\u9884\u8bbe\u7684\u6982\u5ff5\u8d28\u91cf\u6807\u51c6\u3002", "result": "GIER\u5728\u4e09\u4e2a\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\uff08SciFact, PrivacyQA, e-SNLI\uff09\u548c\u56db\u79cdLLM\uff08GPT-4.1, GPT-4o Mini, Gemini 1.5 Pro, Llama 3.3 70B\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\uff0cGIER\u663e\u8457\u63d0\u9ad8\u4e86\u8f93\u51fa\u7684\u201c\u63a8\u7406\u8d28\u91cf\u3001\u57fa\u7840\u548c\u63a8\u7406\u5bf9\u9f50\u201d\uff0c\u540c\u65f6\u6ca1\u6709\u964d\u4f4e\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cLLM\u4e0d\u4ec5\u80fd\u591f\u7406\u89e3\u62bd\u8c61\u7684\u6982\u5ff5\u6027\u5dee\u8ddd\uff0c\u8fd8\u80fd\u5c06\u5176\u8f6c\u5316\u4e3a\u5177\u4f53\u7684\u63a8\u7406\u6539\u8fdb\u3002GIER\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfcLLM\u8fdb\u884c\u81ea\u6211\u53cd\u601d\u548c\u57fa\u4e8e\u5dee\u8ddd\u7684\u8fed\u4ee3\u4fee\u8ba2\u6765\u63d0\u5347\u5176\u63a8\u7406\u80fd\u529b\u548c\u8f93\u51fa\u8d28\u91cf\u3002"}}
{"id": "2509.00708", "pdf": "https://arxiv.org/pdf/2509.00708", "abs": "https://arxiv.org/abs/2509.00708", "authors": ["Jingyi Guan", "Kun Qiu", "Jin Zhao"], "title": "ReWeave: Traffic Engineering with Robust Path Weaving for Localized Link Failure Recover", "categories": ["cs.NI"], "comment": "Accepted in IEEE ICNP 2025", "summary": "Link failures occur frequently in Internet Service Provider (ISP) networks\nand pose significant challenges for Traffic Engineering (TE). Existing TE\nschemes either reroute traffic over vulnerable static paths, leading to\nperformance degradation, or precompute backup routes for a broad range of\nfailure scenarios, which introduces high overhead and limits scalability.\nHence, an effective failure recovery mechanism is required to offer sufficient\npath diversity under constrained overhead, thereby ensuring robust and\nperformant network operation. This paper presents ReWeave, a scalable and\nefficient link-level TE scheme that enables localized rerouting by equipping\neach link with a compact set of adjacent-only backup paths. Upon detecting a\nfailure, only the routers at both ends of the failed link reroute traffic\ndynamically using SRv6-based detours, without controller intervention or\nfull-path recomputation. Evaluation results on large-scale backbone networks\ndemonstrate that ReWeave outperforms existing TE schemes in link failure\nscenarios. Compared to HARP, the state-of-the-art failure recovery scheme based\non centralized control and dynamic traffic reallocation, our approach reduces\nthe average maximum link utilization by 10.5%~20.1%, and lowers the worst-case\nutilization by 29.5%~40.9%. When compared with Flexile, a protection-based\nscheme that precomputes routes for multi-failure scenarios, ReWeave achieves a\nsimilarly low packet loss rate in 90% of failure cases, while maintaining a\nresponse speed comparable to the fastest router-based local rerouting schemes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReWeave\uff0c\u4e00\u79cd\u53ef\u6269\u5c55\u9ad8\u6548\u7684\u94fe\u8def\u7ea7\u6d41\u91cf\u5de5\u7a0b\u65b9\u6848\uff0c\u901a\u8fc7\u4e3a\u6bcf\u6761\u94fe\u8def\u914d\u5907\u7d27\u51d1\u7684\u90bb\u8fd1\u5907\u4efd\u8def\u5f84\uff0c\u5229\u7528SRv6\u5b9e\u73b0\u6545\u969c\u94fe\u8def\u4e24\u7aef\u8def\u7531\u5668\u7684\u672c\u5730\u5316\u52a8\u6001\u91cd\u8def\u7531\uff0c\u65e0\u9700\u63a7\u5236\u5668\u5e72\u9884\uff0c\u5728\u5927\u578b\u9aa8\u5e72\u7f51\u7edc\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6545\u969c\u6062\u590d\u65b9\u6848\uff0c\u964d\u4f4e\u4e86\u94fe\u8def\u5229\u7528\u7387\u5e76\u4fdd\u6301\u4e86\u5feb\u901f\u54cd\u5e94\u901f\u5ea6\u3002", "motivation": "ISP\u7f51\u7edc\u4e2d\u94fe\u8def\u6545\u969c\u9891\u7e41\uff0c\u5bf9\u6d41\u91cf\u5de5\u7a0b\uff08TE\uff09\u6784\u6210\u4e25\u5cfb\u6311\u6218\u3002\u73b0\u6709TE\u65b9\u6848\u5b58\u5728\u4e0d\u8db3\uff1a\u8981\u4e48\u901a\u8fc7\u8106\u5f31\u7684\u9759\u6001\u8def\u5f84\u91cd\u8def\u7531\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u8981\u4e48\u9884\u8ba1\u7b97\u5927\u91cf\u6545\u969c\u573a\u666f\u4e0b\u7684\u5907\u4efd\u8def\u7531\u5f15\u5165\u9ad8\u5f00\u9500\u5e76\u9650\u5236\u53ef\u6269\u5c55\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u6545\u969c\u6062\u590d\u673a\u5236\uff0c\u5728\u6709\u9650\u5f00\u9500\u4e0b\u63d0\u4f9b\u8db3\u591f\u7684\u8def\u5f84\u591a\u6837\u6027\uff0c\u4ee5\u786e\u4fdd\u7f51\u7edc\u8fd0\u884c\u7684\u9c81\u68d2\u6027\u548c\u9ad8\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86ReWeave\uff0c\u4e00\u79cd\u53ef\u6269\u5c55\u9ad8\u6548\u7684\u94fe\u8def\u7ea7\u6d41\u91cf\u5de5\u7a0b\u65b9\u6848\uff0c\u901a\u8fc7\u4e3a\u6bcf\u6761\u94fe\u8def\u914d\u5907\u4e00\u5957\u7d27\u51d1\u7684\u3001\u4ec5\u57fa\u4e8e\u90bb\u8fd1\u8def\u5f84\u7684\u5907\u4efd\u8def\u5f84\u6765\u5b9e\u73b0\u672c\u5730\u5316\u91cd\u8def\u7531\u3002\u5f53\u68c0\u6d4b\u5230\u6545\u969c\u65f6\uff0c\u53ea\u6709\u6545\u969c\u94fe\u8def\u4e24\u7aef\u7684\u8def\u7531\u5668\u4f1a\u5229\u7528\u57fa\u4e8eSRv6\u7684\u7ed5\u884c\u8def\u5f84\u52a8\u6001\u91cd\u8def\u7531\u6d41\u91cf\uff0c\u65e0\u9700\u63a7\u5236\u5668\u5e72\u9884\u6216\u5168\u8def\u5f84\u91cd\u65b0\u8ba1\u7b97\u3002", "result": "\u5728\u5927\u578b\u9aa8\u5e72\u7f51\u7edc\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cReWeave\u5728\u94fe\u8def\u6545\u969c\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709TE\u65b9\u6848\u3002\u4e0eHARP\uff08\u96c6\u4e2d\u5f0f\u63a7\u5236\u548c\u52a8\u6001\u6d41\u91cf\u91cd\u65b0\u5206\u914d\u7684\u6700\u65b0\u6545\u969c\u6062\u590d\u65b9\u6848\uff09\u76f8\u6bd4\uff0cReWeave\u5c06\u5e73\u5747\u6700\u5927\u94fe\u8def\u5229\u7528\u7387\u964d\u4f4e\u4e8610.5%~20.1%\uff0c\u5c06\u6700\u5dee\u60c5\u51b5\u4e0b\u7684\u5229\u7528\u7387\u964d\u4f4e\u4e8629.5%~40.9%\u3002\u4e0eFlexile\uff08\u4e3a\u591a\u6545\u969c\u573a\u666f\u9884\u8ba1\u7b97\u8def\u7531\u7684\u4fdd\u62a4\u65b9\u6848\uff09\u76f8\u6bd4\uff0cReWeave\u572890%\u7684\u6545\u969c\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u76f8\u4f3c\u7684\u4f4e\u4e22\u5305\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u6700\u5feb\u7684\u57fa\u4e8e\u8def\u7531\u5668\u7684\u672c\u5730\u91cd\u8def\u7531\u65b9\u6848\u76f8\u5f53\u7684\u54cd\u5e94\u901f\u5ea6\u3002", "conclusion": "ReWeave\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u94fe\u8def\u7ea7\u6d41\u91cf\u5de5\u7a0b\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u672c\u5730\u5316\u91cd\u8def\u7531\u5b9e\u73b0\u5feb\u901f\u6545\u969c\u6062\u590d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u5728\u94fe\u8def\u6545\u969c\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u964d\u4f4e\u4e86\u94fe\u8def\u5229\u7528\u7387\uff0c\u5e76\u4fdd\u6301\u4e86\u5feb\u901f\u54cd\u5e94\uff0c\u6709\u6548\u89e3\u51b3\u4e86ISP\u7f51\u7edc\u4e2d\u6d41\u91cf\u5de5\u7a0b\u9762\u4e34\u7684\u6311\u6218\u3002"}}
{"id": "2509.00284", "pdf": "https://arxiv.org/pdf/2509.00284", "abs": "https://arxiv.org/abs/2509.00284", "authors": ["Liang Gong", "Tommy", "Wang", "Sara Chaker", "Yanchen Dong", "Fouad Bousetouane", "Brenden Morton", "Mark Mendez"], "title": "Generative AI for Industrial Contour Detection: A Language-Guided Vision System", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 5 figures", "summary": "Industrial computer vision systems often struggle with noise, material\nvariability, and uncontrolled imaging conditions, limiting the effectiveness of\nclassical edge detectors and handcrafted pipelines. In this work, we present a\nlanguage-guided generative vision system for remnant contour detection in\nmanufacturing, designed to achieve CAD-level precision. The system is organized\ninto three stages: data acquisition and preprocessing, contour generation using\na conditional GAN, and multimodal contour refinement through vision-language\nmodeling, where standardized prompts are crafted in a human-in-the-loop process\nand applied through image-text guided synthesis. On proprietary FabTrack\ndatasets, the proposed system improved contour fidelity, enhancing edge\ncontinuity and geometric alignment while reducing manual tracing. For the\nrefinement stage, we benchmarked several vision-language models, including\nGoogle's Gemini 2.0 Flash, OpenAI's GPT-image-1 integrated within a VLM-guided\nworkflow, and open-source baselines. Under standardized conditions, GPT-image-1\nconsistently outperformed Gemini 2.0 Flash in both structural accuracy and\nperceptual quality. These findings demonstrate the promise of VLM-guided\ngenerative workflows for advancing industrial computer vision beyond the\nlimitations of classical pipelines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u8a00\u5f15\u5bfc\u7684\u751f\u6210\u5f0f\u89c6\u89c9\u7cfb\u7edf\uff0c\u7528\u4e8e\u5de5\u4e1a\u5236\u9020\u4e2d\u7684\u6b8b\u4f59\u8f6e\u5ed3\u68c0\u6d4b\uff0c\u65e8\u5728\u5b9e\u73b0CAD\u7ea7\u522b\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7VLM\u6307\u5bfc\u7684\u5de5\u4f5c\u6d41\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u5de5\u4e1a\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u5728\u566a\u58f0\u3001\u6750\u6599\u591a\u6837\u6027\u548c\u4e0d\u53d7\u63a7\u6210\u50cf\u6761\u4ef6\u4e0b\uff0c\u96be\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u8f6e\u5ed3\u68c0\u6d4b\uff0c\u9650\u5236\u4e86\u7ecf\u5178\u8fb9\u7f18\u68c0\u6d4b\u5668\u548c\u624b\u5de5\u7ba1\u9053\u7684\u6709\u6548\u6027\u3002", "method": "\u7cfb\u7edf\u5206\u4e3a\u4e09\u9636\u6bb5\uff1a\u6570\u636e\u91c7\u96c6\u4e0e\u9884\u5904\u7406\uff1b\u4f7f\u7528\u6761\u4ef6GAN\u8fdb\u884c\u8f6e\u5ed3\u751f\u6210\uff1b\u4ee5\u53ca\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u591a\u6a21\u6001\u8f6e\u5ed3\u7ec6\u5316\uff0c\u5176\u4e2d\u4eba\u5de5\u53c2\u4e0e\uff08human-in-the-loop\uff09\u5236\u5b9a\u6807\u51c6\u5316\u63d0\u793a\u8bcd\uff0c\u5e76\u901a\u8fc7\u56fe\u50cf-\u6587\u672c\u5f15\u5bfc\u5408\u6210\u5e94\u7528\u3002\u5728\u7ec6\u5316\u9636\u6bb5\uff0c\u57fa\u51c6\u6d4b\u8bd5\u4e86\u5305\u62ecGoogle Gemini 2.0 Flash\u548cOpenAI GPT-image-1\u5728\u5185\u7684\u591a\u4e2aVLM\u3002", "result": "\u5728FabTrack\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u63d0\u9ad8\u4e86\u8f6e\u5ed3\u4fdd\u771f\u5ea6\uff0c\u589e\u5f3a\u4e86\u8fb9\u7f18\u8fde\u7eed\u6027\u548c\u51e0\u4f55\u5bf9\u9f50\uff0c\u5e76\u51cf\u5c11\u4e86\u624b\u52a8\u63cf\u8ff9\u3002\u5728\u7ec6\u5316\u9636\u6bb5\uff0cGPT-image-1\u5728\u7ed3\u6784\u7cbe\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8eGemini 2.0 Flash\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0cVLM\u5f15\u5bfc\u7684\u751f\u6210\u5f0f\u5de5\u4f5c\u6d41\u6709\u671b\u63a8\u52a8\u5de5\u4e1a\u8ba1\u7b97\u673a\u89c6\u89c9\u8d85\u8d8a\u4f20\u7edf\u7ba1\u9053\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.00244", "pdf": "https://arxiv.org/pdf/2509.00244", "abs": "https://arxiv.org/abs/2509.00244", "authors": ["Peter Belcak", "Pavlo Molchanov"], "title": "Universal Deep Research: Bring Your Own Model and Strategy", "categories": ["cs.AI"], "comment": null, "summary": "Deep research tools are among the most impactful and most commonly\nencountered agentic systems today. We observe, however, that each deep research\nagent introduced so far is hard-coded to carry out a particular research\nstrategy using a fixed choice of tools. We introduce Universal Deep Research\n(UDR), a generalist agentic system that wraps around any language model and\nenables the user to create, edit, and refine their own entirely custom deep\nresearch strategies without any need for additional training or finetuning. To\nshowcase the generality of our system, we equip UDR with example minimal,\nexpansive, and intensive research strategies, and provide a user interface to\nfacilitate experimentation with the system.", "AI": {"tldr": "UDR\u662f\u4e00\u4e2a\u901a\u7528\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u53ef\u5c01\u88c5\u4efb\u610f\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u81ea\u5b9a\u4e49\u6df1\u5ea6\u7814\u7a76\u7b56\u7565\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u786c\u7f16\u7801\u7b56\u7565\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u7cfb\u7edf\u90fd\u91c7\u7528\u786c\u7f16\u7801\u7684\u7279\u5b9a\u7814\u7a76\u7b56\u7565\u548c\u56fa\u5b9a\u5de5\u5177\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u7528\u6237\u81ea\u5b9a\u4e49\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u901a\u7528\u6df1\u5ea6\u7814\u7a76\uff08UDR\uff09\u7cfb\u7edf\uff0c\u5b83\u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u578b\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u53ef\u5c01\u88c5\u4efb\u4f55\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u521b\u5efa\u3001\u7f16\u8f91\u548c\u5b8c\u5584\u5b8c\u5168\u81ea\u5b9a\u4e49\u7684\u6df1\u5ea6\u7814\u7a76\u7b56\u7565\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5fae\u8c03\u3002\u4e3a\u5c55\u793a\u5176\u901a\u7528\u6027\uff0cUDR\u914d\u5907\u4e86\u793a\u4f8b\u6027\u7684\u6700\u5c0f\u3001\u6269\u5c55\u548c\u5bc6\u96c6\u7814\u7a76\u7b56\u7565\uff0c\u5e76\u63d0\u4f9b\u4e86\u7528\u6237\u754c\u9762\u4ee5\u65b9\u4fbf\u5b9e\u9a8c\u3002", "result": "UDR\u7cfb\u7edf\u5c55\u793a\u4e86\u5176\u901a\u7528\u6027\uff0c\u80fd\u591f\u652f\u6301\u548c\u6267\u884c\u591a\u79cd\u81ea\u5b9a\u4e49\u7814\u7a76\u7b56\u7565\uff08\u5982\u6700\u5c0f\u3001\u6269\u5c55\u3001\u5bc6\u96c6\uff09\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u754c\u9762\u65b9\u4fbf\u4e86\u7cfb\u7edf\u7684\u5b9e\u9a8c\u548c\u9a8c\u8bc1\u3002", "conclusion": "UDR\u6210\u529f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u53ef\u5b9a\u5236\u7684\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u81ea\u4e3b\u8bbe\u8ba1\u548c\u8c03\u6574\u7814\u7a76\u6d41\u7a0b\uff0c\u514b\u670d\u4e86\u73b0\u6709\u7cfb\u7edf\u7b56\u7565\u50f5\u5316\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.00069", "pdf": "https://arxiv.org/pdf/2509.00069", "abs": "https://arxiv.org/abs/2509.00069", "authors": ["Prasasthy Balasubramanian", "Dumindu Kankanamge", "Ekaterina Gilman", "Mourad Oussalah"], "title": "AnomalyExplainer Explainable AI for LLM-based anomaly detection using BERTViz and Captum", "categories": ["cs.LG"], "comment": null, "summary": "Conversational AI and Large Language Models (LLMs) have become powerful tools\nacross domains, including cybersecurity, where they help detect threats early\nand improve response times. However, challenges such as false positives and\ncomplex model management still limit trust. Although Explainable AI (XAI) aims\nto make AI decisions more transparent, many security analysts remain uncertain\nabout its usefulness. This study presents a framework that detects anomalies\nand provides high-quality explanations through visual tools BERTViz and Captum,\ncombined with natural language reports based on attention outputs. This reduces\nmanual effort and speeds up remediation. Our comparative analysis showed that\nRoBERTa offers high accuracy (99.6 %) and strong anomaly detection,\noutperforming Falcon-7B and DeBERTa, as well as exhibiting better flexibility\nthan large-scale Mistral-7B on the HDFS dataset from LogHub. User feedback\nconfirms the chatbot's ease of use and improved understanding of anomalies,\ndemonstrating the ability of the developed framework to strengthen\ncybersecurity workflows.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684AI\u6846\u67b6\uff0c\u7528\u4e8e\u7f51\u7edc\u5b89\u5168\u5f02\u5e38\u68c0\u6d4b\uff0c\u7ed3\u5408RoBERTa\u3001\u53ef\u89c6\u5316\u5de5\u5177\u548c\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u5e76\u63d0\u5347\u4e86\u7528\u6237\u7406\u89e3\u3002", "motivation": "\u5bf9\u8bdd\u5f0fAI\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u4ecd\u9762\u4e34\u8bef\u62a5\u3001\u6a21\u578b\u7ba1\u7406\u590d\u6742\u548c\u5bf9\u53ef\u89e3\u91caAI\uff08XAI\uff09\u4fe1\u4efb\u4e0d\u8db3\u7b49\u6311\u6218\uff0c\u9700\u8981\u66f4\u900f\u660e\u6709\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u5f02\u5e38\u68c0\u6d4b\u548c\u9ad8\u8d28\u91cf\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u5229\u7528BERTViz\u548cCaptum\u7b49\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u6ce8\u610f\u529b\u8f93\u51fa\u7684\u81ea\u7136\u8bed\u8a00\u62a5\u544a\u3002\u901a\u8fc7\u5728LogHub\u7684HDFS\u6570\u636e\u96c6\u4e0a\uff0c\u5c06RoBERTa\u4e0eFalcon-7B\u3001DeBERTa\u548cMistral-7B\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\uff0c\u5e76\u6536\u96c6\u4e86\u7528\u6237\u53cd\u9988\u3002", "result": "RoBERTa\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u8fbe99.6%\uff0c\u4f18\u4e8eFalcon-7B\u548cDeBERTa\uff0c\u5e76\u6bd4Mistral-7B\u66f4\u5177\u7075\u6d3b\u6027\u3002\u7528\u6237\u53cd\u9988\u8bc1\u5b9e\u4e86\u804a\u5929\u673a\u5668\u4eba\u7684\u6613\u7528\u6027\uff0c\u5e76\u63d0\u9ad8\u4e86\u5bf9\u5f02\u5e38\u7684\u7406\u89e3\u3002", "conclusion": "\u6240\u5f00\u53d1\u7684\u6846\u67b6\u901a\u8fc7\u63d0\u4f9b\u9ad8\u51c6\u786e\u5ea6\u3001\u53ef\u89e3\u91ca\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u7f51\u7edc\u5b89\u5168\u5de5\u4f5c\u6d41\u7a0b\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\u5e76\u63d0\u5347\u4e86\u5b89\u5168\u5206\u6790\u5e08\u7684\u7406\u89e3\u3002"}}
{"id": "2509.00375", "pdf": "https://arxiv.org/pdf/2509.00375", "abs": "https://arxiv.org/abs/2509.00375", "authors": ["Ziyi Xia", "Kun Luo", "Hongjin Qian", "Zheng Liu"], "title": "Open Data Synthesis For Deep Research", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly expected to go beyond simple\nfactual queries toward Deep Research-tasks that require decomposing questions\ninto sub-problems, coordinating multi-step reasoning, and synthesizing evidence\nfrom diverse sources. We formalize Deep Research tasks with verifiable answers\nas Hierarchical Constraint Satisfaction Problems (HCSPs), which are\nfundamentally different from single-constraint, multi-hop, or flat CSP\nformulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA)\nfail to capture this complexity, while recent synthetic datasets often\nintroduce shortcut reasoning, knowledge leakage, or lack sufficient structural\ndepth. To address this gap, we introduce InfoSeek, a scalable framework for\nsynthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to\nrecursively build a Research Tree from large-scale webpages, blurring\nintermediate nodes into valid sub-problems, and converting these trees into\nnatural language questions that require traversing the full hierarchy. It also\nenables rapid scaling, yielding over 50K training examples, a curated test set,\nand reasoning trajectories generated via reject sampling. Experiments show that\nmodels trained on InfoSeek consistently outperform strong baselines. On a\nchallenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass\nmuch larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash),\nwhile achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro).\nBy preserving meta-information such as intermediate steps and retrieval labels,\nInfoSeek further supports advanced optimization strategies, including compound\nreward design and trajectory-level exploration. We provide our codes and\ndatasets in \\href{https://github.com/VectorSpaceLab/InfoSeek}{this repository}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faInfoSeek\uff0c\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u4ee3\u7406\u7cfb\u7edf\u4ece\u7f51\u9875\u6784\u5efa\u7814\u7a76\u6811\u6765\u5408\u6210\u590d\u6742\u7684\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\uff08HCSPs\uff09\u3002InfoSeek\u8bad\u7ec3\u51fa\u7684\u6a21\u578b\u5728\u6311\u6218\u6027\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3001\u751a\u81f3\u66f4\u5927\u7684\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u5546\u4e1aAPI\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9700\u8981\u5904\u7406\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\uff0c\u8fd9\u6d89\u53ca\u95ee\u9898\u5206\u89e3\u3001\u591a\u6b65\u9aa4\u63a8\u7406\u548c\u4fe1\u606f\u7efc\u5408\u3002\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u6355\u6349\u8fd9\u79cd\u590d\u6742\u6027\uff0c\u800c\u73b0\u6709\u5408\u6210\u6570\u636e\u96c6\u5b58\u5728\u5feb\u6377\u63a8\u7406\u3001\u77e5\u8bc6\u6cc4\u9732\u6216\u7ed3\u6784\u6df1\u5ea6\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u65b0\u7684\u57fa\u51c6\u6765\u5f25\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5c06\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u5206\u5c42\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff08HCSPs\uff09\u3002\u5f15\u5165InfoSeek\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u4ee3\u7406\u7cfb\u7edf\u4ece\u5927\u89c4\u6a21\u7f51\u9875\u9012\u5f52\u6784\u5efa\u7814\u7a76\u6811\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u9700\u8981\u904d\u5386\u5b8c\u6574\u5c42\u7ea7\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u3002\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u751f\u6210\u8d85\u8fc75\u4e07\u4e2a\u8bad\u7ec3\u6837\u672c\u548c\u6d4b\u8bd5\u96c6\uff0c\u5e76\u751f\u6210\u62d2\u7edd\u91c7\u6837\u7684\u63a8\u7406\u8f68\u8ff9\u3002\u540c\u65f6\uff0cInfoSeek\u4fdd\u7559\u4e86\u4e2d\u95f4\u6b65\u9aa4\u548c\u68c0\u7d22\u6807\u7b7e\u7b49\u5143\u4fe1\u606f\uff0c\u4ee5\u652f\u6301\u9ad8\u7ea7\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728InfoSeek\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u3002\u5728\u6311\u6218\u6027\u57fa\u51c6BrowseComp-Plus\u4e0a\uff0c\u7ecf\u8fc7InfoSeek\u4f18\u5316\u76843B LLMs\u8d85\u8d8a\u4e86\u66f4\u5927\u768432B\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u5546\u4e1aAPI\uff08\u5982Gemini2.5-Flash\uff09\uff0c\u5e76\u53d6\u5f97\u4e86\u4e0e\u66f4\u5f3aAPI\uff08\u5982Gemini2.5-Pro\uff09\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "InfoSeek\u901a\u8fc7\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u590d\u6742\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u5408\u6210\u6846\u67b6\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\u3002\u5b83\u80fd\u663e\u8457\u63d0\u5347LLMs\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\u4e5f\u80fd\u4e0e\u5927\u578b\u6a21\u578b\u548c\u5546\u4e1aAPI\u5ab2\u7f8e\uff0c\u4e14\u5176\u5143\u4fe1\u606f\u4fdd\u7559\u529f\u80fd\u652f\u6301\u8fdb\u4e00\u6b65\u7684\u9ad8\u7ea7\u4f18\u5316\u3002"}}
{"id": "2509.00868", "pdf": "https://arxiv.org/pdf/2509.00868", "abs": "https://arxiv.org/abs/2509.00868", "authors": ["Yong Su", "Yiyi Chen", "Shenghong Yi", "Hui Feng", "Yuedong Xu", "Wang Xiang", "Bo Hu"], "title": "A Modular and Scalable Simulator for Connected-UAVs Communication in 5G Networks", "categories": ["cs.NI"], "comment": "a short version is accepted by MSWiM 2025", "summary": "Cellular-connected UAV systems have enabled a wide range of low-altitude\naerial services. However, these systems still face many challenges, such as\nfrequent handovers and the inefficiency of traditional transport protocols. To\nbetter study these issues, we develop a modular and scalable simulation\nplatform specifically designed for UAVs communication leveraging the research\necology in wireless communication of MATLAB. The platform supports flexible 5G\nNR node deployment, customizable UAVs mobility models, and\nmulti-network-interface extensions. It also supports multiple transport\nprotocols including TCP, UDP, QUIC, etc., allowing to investigate how different\ntransport protocols affect UAVs communication performance.In addition, the\nplatform includes a handover management module, enabling the evaluation of both\ntraditional and learning-based handover strategies. Our platform can serve as a\ntestbed for the development and evaluation of advanced transmission strategies\nin cellular-connected UAV systems.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eMATLAB\u7684\u6a21\u5757\u5316\u4eff\u771f\u5e73\u53f0\uff0c\u7528\u4e8e\u7814\u7a76\u8702\u7a9d\u8fde\u63a5\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u7684\u4f20\u8f93\u534f\u8bae\u548c\u5207\u6362\u7b56\u7565\uff0c\u4ee5\u5e94\u5bf9\u9891\u7e41\u5207\u6362\u548c\u534f\u8bae\u6548\u7387\u4f4e\u4e0b\u7b49\u6311\u6218\u3002", "motivation": "\u8702\u7a9d\u8fde\u63a5\u65e0\u4eba\u673a\u7cfb\u7edf\u9762\u4e34\u9891\u7e41\u5207\u6362\u548c\u4f20\u7edf\u4f20\u8f93\u534f\u8bae\u6548\u7387\u4f4e\u4e0b\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u4e2a\u4e13\u7528\u5e73\u53f0\u6765\u6df1\u5165\u7814\u7a76\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eMATLAB\u7684\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u4eff\u771f\u5e73\u53f0\u3002\u8be5\u5e73\u53f0\u652f\u63015G NR\u8282\u70b9\u90e8\u7f72\u3001\u53ef\u5b9a\u5236\u7684\u65e0\u4eba\u673a\u79fb\u52a8\u6a21\u578b\u3001\u591a\u7f51\u7edc\u63a5\u53e3\u6269\u5c55\u3001\u591a\u79cd\u4f20\u8f93\u534f\u8bae\uff08\u5982TCP, UDP, QUIC\uff09\u4ee5\u53ca\u8bc4\u4f30\u4f20\u7edf\u548c\u5b66\u4e60\u578b\u5207\u6362\u7b56\u7565\u7684\u5207\u6362\u7ba1\u7406\u6a21\u5757\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u7075\u6d3b\u6a21\u62df\u548c\u8bc4\u4f30\u4e0d\u540c\u4f20\u8f93\u534f\u8bae\u53ca\u5207\u6362\u7b56\u7565\u5bf9\u65e0\u4eba\u673a\u901a\u4fe1\u6027\u80fd\u5f71\u54cd\u7684\u4eff\u771f\u5e73\u53f0\u3002", "conclusion": "\u8be5\u5e73\u53f0\u53ef\u4f5c\u4e3a\u8702\u7a9d\u8fde\u63a5\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u9ad8\u7ea7\u4f20\u8f93\u7b56\u7565\u5f00\u53d1\u548c\u8bc4\u4f30\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2509.00305", "pdf": "https://arxiv.org/pdf/2509.00305", "abs": "https://arxiv.org/abs/2509.00305", "authors": ["Ghassen Baklouti", "Maxime Zanella", "Ismail Ben Ayed"], "title": "Language-Aware Information Maximization for Transductive Few-Shot CLIP", "categories": ["cs.CV"], "comment": null, "summary": "Transductive few-shot learning has triggered an abundant literature focusing\non vision-only models, but is still at a nascent stage within the recent\ncontext of foundational vision-language models (VLMs). Only a few recent\nmethods addressed the problem, pointing to the potential of tranduction in VLMs\nand to the need for VLM-tailored methods. Building on this momentum, we\nleverage information-theoretic concepts and recent progress in\nparameter-efficient fine-tuning (PEFT), developing a highly competitive\ntransductive few-shot CLIP method. Specifically, we introduce a novel\nLanguage-aware Information MaximizatiOn (LIMO) loss integrating three\ncomplementary terms: (i) the mutual information between the vision inputs and\nthe textual class descriptions; (ii) a Kullback-Leibler (KL) divergence\npenalizing deviation of the network's probabilistic outputs from the\ntext-driven zero-shot predictions; and (iii) a standard cross-entropy loss\nbased on the labeled shots. Furthermore, we challenge the commonly followed\nfine-tuning practices in the context of transductive few-shot learning, and\nexplore PEFT strategies, completely overlooked in this context. Surprisingly,\nwe observe substantial boosts in performances, which points to the potential of\nadapting a subset of the model's parameters in the transductive few-shot\nsetting. We report comprehensive evaluations, which show that LIMO outperforms\nthe very recent transductive few-shot CLIP methods by a large margin and yields\nsignificant gains over the best-performing inductive methods. Our code is\npublicly available at:\\[\n\\href{https://github.com/ghassenbaklouti/LIMO}{\\text{here}} \\]", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u548cPEFT\u7684\u65b0\u578b\u8f6c\u5bfc\u5f0f\u5c0f\u6837\u672cCLIP\u65b9\u6cd5LIMO\uff0c\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u8f6c\u5bfc\u5f0f\u5c0f\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8f6c\u5bfc\u5f0f\u5c0f\u6837\u672c\u5b66\u4e60\u5728\u89c6\u89c9\u6a21\u578b\u4e2d\u7814\u7a76\u4e30\u5bcc\uff0c\u4f46\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u80cc\u666f\u4e0b\u4ecd\u5904\u4e8e\u65e9\u671f\uff0c\u73b0\u6709\u65b9\u6cd5\u8f83\u5c11\uff0c\u8feb\u5207\u9700\u8981\u4e3aVLM\u91cf\u8eab\u5b9a\u5236\u7684\u8f6c\u5bfc\u5f0f\u5c0f\u6837\u672c\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLIMO\uff08Language-aware Information MaximizatiOn\uff09\u7684\u635f\u5931\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u6574\u5408\u4e86\u4e09\u4e2a\u4e92\u8865\u9879\uff1a\u89c6\u89c9\u8f93\u5165\u4e0e\u6587\u672c\u7c7b\u522b\u63cf\u8ff0\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u3001\u60e9\u7f5a\u7f51\u7edc\u6982\u7387\u8f93\u51fa\u504f\u79bb\u6587\u672c\u9a71\u52a8\u96f6\u6837\u672c\u9884\u6d4b\u7684KL\u6563\u5ea6\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6807\u6ce8\u6837\u672c\u7684\u6807\u51c6\u4ea4\u53c9\u71b5\u635f\u5931\u3002\u6b64\u5916\uff0c\u63a2\u7d22\u5e76\u5e94\u7528\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u7b56\u7565\u3002", "result": "LIMO\u5728\u8f6c\u5bfc\u5f0f\u5c0f\u6837\u672cCLIP\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u65b0\u7684\u540c\u7c7b\u65b9\u6cd5\uff0c\u5e76\u76f8\u5bf9\u6700\u4f73\u5f52\u7eb3\u5f0f\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u4e4b\u524d\u88ab\u5ffd\u89c6\u7684PEFT\u7b56\u7565\u80fd\u5927\u5e45\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7LIMO\u635f\u5931\u51fd\u6570\u548c\u5bf9PEFT\u7b56\u7565\u7684\u6709\u6548\u5229\u7528\uff0c\u6210\u529f\u63a8\u52a8\u4e86VLM\u5728\u8f6c\u5bfc\u5f0f\u5c0f\u6837\u672c\u5b66\u4e60\u9886\u57df\u7684\u53d1\u5c55\uff0c\u786e\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u6746\uff0c\u5e76\u63ed\u793a\u4e86PEFT\u5728\u8be5\u8bbe\u7f6e\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.00251", "pdf": "https://arxiv.org/pdf/2509.00251", "abs": "https://arxiv.org/abs/2509.00251", "authors": ["Rimom Costa"], "title": "Instruction-Level Weight Shaping: A Framework for Self-Improving AI Agents", "categories": ["cs.AI", "I.2.7; I.2.6; I.2.11"], "comment": "12 pages, 1 figure, 2 tables", "summary": "Large language models (LLMs) are fluent but largely static after\npre-training; new or shifting knowledge is typically added with\nretrieval-augmented generation (RAG) or fine-tuning. RAG raises latency and\nengineering overhead and often fails to integrate facts; prompt engineering is\nbrittle and can conflict with prior knowledge; fine-tuning is costly and risks\ncatastrophic forgetting. We propose Instruction-Level Weight Shaping (ILWS):\ncurated system instructions act as external, auditable pseudo-parameters\nupdated after each session via reflection and user feedback. A Reflection\nEngine inspects conversation traces, diagnoses reasoning successes and\nfailures, and proposes typed deltas $\\Delta K=(\\Delta S,\\Delta U,\\Delta T)$\nover instructions, user preferences, and tools. Deltas are version-controlled,\nevaluated with a sliding window of 1-5 star ratings, auto-repaired on first\nfailure, and rolled back on repeated failure. When an edit budget crosses a\nthreshold, the agent compiles a rating-weighted synthetic set and distills\nmatured instruction-space gains into parameters, converting prompt-space\nimprovements into weight-space without downtime. ILWS makes explicit the\nlow-rank shaping induced by context in transformer blocks, preserves\ngovernance, and removes per-call retrieval. In enterprise support it increased\nthroughput 2.4-5.0x and cut audited hallucinations by about 80% versus a frozen\nbaseline. In an Adobe Commerce Cloud proof of concept \"L0 Support\", it achieved\n4-5x more tickets per hour and about 80% lower time per ticket, with autonomous\ninstruction updates and optional tool synthesis. Because ILWS operates at the\ninstruction layer until controlled distillation, it generalizes to dynamic\ndomains (legal, medical, engineering) requiring adaptive reasoning, tool\ncreation, and low-latency deployment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.00071", "pdf": "https://arxiv.org/pdf/2509.00071", "abs": "https://arxiv.org/abs/2509.00071", "authors": ["Shang Liu", "Jing Wang", "Wenji Fang", "Zhiyao Xie"], "title": "SynCircuit: Automated Generation of New Synthetic RTL Circuits Can Enable Big Data in Circuits", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by DAC'25", "summary": "In recent years, AI-assisted IC design methods have demonstrated great\npotential, but the availability of circuit design data is extremely limited,\nespecially in the public domain. The lack of circuit data has become the\nprimary bottleneck in developing AI-assisted IC design methods. In this work,\nwe make the first attempt, SynCircuit, to generate new synthetic circuits with\nvalid functionalities in the HDL format. SynCircuit automatically generates\nsynthetic data using a framework with three innovative steps: 1) We propose a\ncustomized diffusion-based generative model to resolve the Directed Cyclic\nGraph (DCG) generation task, which has not been well explored in the AI\ncommunity. 2) To ensure our circuit is valid, we enforce the circuit\nconstraints by refining the initial graph generation outputs. 3) The Monte\nCarlo tree search (MCTS) method further optimizes the logic redundancy in the\ngenerated graph. Experimental results demonstrate that our proposed SynCircuit\ncan generate more realistic synthetic circuits and enhance ML model performance\nin downstream circuit design tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSynCircuit\uff0c\u4e00\u4e2a\u4e09\u6b65\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u5236\u6269\u6563\u6a21\u578b\u3001\u7ea6\u675f\u7ec6\u5316\u548cMCTS\u4f18\u5316\uff0c\u751f\u6210\u5177\u6709\u6709\u6548\u529f\u80fd\u7684HDL\u683c\u5f0f\u5408\u6210\u7535\u8def\u6570\u636e\uff0c\u4ee5\u89e3\u51b3AI\u8f85\u52a9IC\u8bbe\u8ba1\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u74f6\u9888\uff0c\u5e76\u63d0\u5347\u4e0b\u6e38ML\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709AI\u8f85\u52a9IC\u8bbe\u8ba1\u65b9\u6cd5\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7535\u8def\u8bbe\u8ba1\u6570\u636e\uff08\u5c24\u5176\u516c\u5171\u9886\u57df\u6570\u636e\uff09\u6781\u5176\u6709\u9650\uff0c\u5df2\u6210\u4e3a\u5f00\u53d1\u8fd9\u4e9b\u65b9\u6cd5\u7684\u9996\u8981\u74f6\u9888\u3002", "method": "SynCircuit\u91c7\u7528\u4e09\u6b65\u521b\u65b0\u6846\u67b6\u81ea\u52a8\u751f\u6210\u5408\u6210\u6570\u636e\uff1a1) \u63d0\u51fa\u5b9a\u5236\u7684\u6269\u6563\u751f\u6210\u6a21\u578b\u89e3\u51b3\u6709\u5411\u5faa\u73af\u56fe\uff08DCG\uff09\u751f\u6210\u4efb\u52a1\u30022) \u901a\u8fc7\u7ec6\u5316\u521d\u59cb\u56fe\u751f\u6210\u8f93\u51fa\u6765\u5f3a\u5236\u6267\u884c\u7535\u8def\u7ea6\u675f\uff0c\u786e\u4fdd\u7535\u8def\u6709\u6548\u6027\u30023) \u5229\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u65b9\u6cd5\u8fdb\u4e00\u6b65\u4f18\u5316\u751f\u6210\u56fe\u4e2d\u7684\u903b\u8f91\u5197\u4f59\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684SynCircuit\u80fd\u591f\u751f\u6210\u66f4\u771f\u5b9e\u7684\u5408\u6210\u7535\u8def\uff0c\u5e76\u63d0\u5347\u4e0b\u6e38\u7535\u8def\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "SynCircuit\u9996\u6b21\u5c1d\u8bd5\u751f\u6210\u5177\u6709\u6709\u6548\u529f\u80fd\u7684HDL\u683c\u5f0f\u5408\u6210\u7535\u8def\uff0c\u6210\u529f\u7f13\u89e3\u4e86AI\u8f85\u52a9IC\u8bbe\u8ba1\u9886\u57df\u7684\u6570\u636e\u77ed\u7f3a\u95ee\u9898\uff0c\u5e76\u88ab\u8bc1\u660e\u80fd\u589e\u5f3a\u76f8\u5173\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8868\u73b0\u3002"}}
{"id": "2509.00388", "pdf": "https://arxiv.org/pdf/2509.00388", "abs": "https://arxiv.org/abs/2509.00388", "authors": ["Xuelin Li", "Xiangqi Jin", "Linfeng Zhang"], "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction", "categories": ["cs.CL"], "comment": null, "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github.", "AI": {"tldr": "GraphKV\u662f\u4e00\u4e2a\u56fe\u57fa\u7684KV\u7f13\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u4ee4\u724c\u91cd\u8981\u6027\u6765\u4f18\u5316LLM\u5728\u957f\u5e8f\u5217\u5904\u7406\u4e2d\u7684KV\u7f13\u5b58\u538b\u7f29\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9759\u6001\u6027\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5904\u7406\u957f\u6587\u672c\u5e8f\u5217\u65f6\uff0cKV\u7f13\u5b58\u7ba1\u7406\u6548\u7387\u4f4e\u4e0b\u4e14\u53d7\u9650\u4e8e\u5185\u5b58\u3002\u4f20\u7edfKV\u6dd8\u6c70\u7b56\u7565\uff08\u5982\u57fa\u4e8e\u6ce8\u610f\u529b\u5206\u6570\u7684top-k\u9009\u62e9\uff09\u91c7\u7528\u9759\u6001\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u65e0\u6cd5\u6355\u83b7\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4ee4\u724c\u4e4b\u95f4\u52a8\u6001\u6f14\u53d8\u7684\u9690\u5f0f\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51faGraphKV\uff0c\u4e00\u4e2a\u56fe\u57fa\u7684KV\u7f13\u5b58\u538b\u7f29\u4ee4\u724c\u9009\u62e9\u6846\u67b6\u3002\u5c06\u4ee4\u724c\u5efa\u6a21\u4e3a\u5e26\u6709\u91cd\u8981\u6027\u5206\u6570\u7684\u8282\u70b9\uff0c\u8fb9\u4ee3\u8868\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u5173\u7cfb\u3002\u901a\u8fc7\u201c\u8870\u51cf\u4fe1\u53f7\u4f20\u64ad\u673a\u5236\u201d\u5728\u56fe\u4e0a\u4f20\u64ad\u4fe1\u606f\uff0c\u52a8\u6001\u66f4\u65b0\u4ee4\u724c\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u81ea\u9002\u5e94\u4fdd\u7559\u6700\u5177\u4e0a\u4e0b\u6587\u610f\u4e49\u7684\u4ee4\u724c\u3002", "result": "GraphKV\u80fd\u591f\u91cd\u65b0\u5b9a\u4e49KV\u7f13\u5b58\u538b\u7f29\u4e2d\u7684\u4ee4\u724c\u9009\u62e9\uff0c\u5b9e\u73b0\u52a8\u6001\u3001\u81ea\u9002\u5e94\u5730\u4fdd\u7559\u6700\u5177\u4e0a\u4e0b\u6587\u610f\u4e49\u7684\u4ee4\u724c\u3002\u5b83\u8fd8\u53ef\u4ee5\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\uff0c\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709KV\u7f13\u5b58\u6dd8\u6c70\u65b9\u6cd5\uff08\u5982SnapKV\u548cPyramidKV\uff09\u4e2d\u3002", "conclusion": "GraphKV\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u56fe\u673a\u5236\uff0c\u514b\u670d\u4e86\u4f20\u7edfKV\u7f13\u5b58\u6dd8\u6c70\u7b56\u7565\u7684\u9759\u6001\u6027\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9LLM\u957f\u5e8f\u5217\u5904\u7406\u4e2d\u4e0a\u4e0b\u6587\u5173\u952e\u4ee4\u724c\u7684\u81ea\u9002\u5e94\u4fdd\u7559\uff0c\u5e76\u53ef\u7075\u6d3b\u5e94\u7528\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86KV\u7f13\u5b58\u7ba1\u7406\u7684\u6548\u7387\u3002"}}
{"id": "2509.00885", "pdf": "https://arxiv.org/pdf/2509.00885", "abs": "https://arxiv.org/abs/2509.00885", "authors": ["Yi-Chia Cheng", "Cheng-Shang Chang"], "title": "Efficient Multichannel Rendezvous Algorithms without Global Channel Enumeration", "categories": ["cs.NI"], "comment": "Part of this work has been presented in IEEE 2024 33rd Wireless and\n  Optical Communications Conference (WOCC)", "summary": "The multichannel rendezvous problem (MRP) is a critical challenge for\nneighbor discovery in IoT applications, requiring two users to find each other\nby hopping among available channels over time. This paper addresses the MRP in\nscenarios where a global channel enumeration system is unavailable. To tackle\nthis challenge, we propose a suite of low-complexity multichannel rendezvous\nalgorithms based on locality-sensitive hashing (LSH), tailored for environments\nwhere channel labels are unique L-bit identifiers rather than globally\ncoordinated indices. Inspired by consistent hashing techniques in distributed\nsystems, we develop the LC-LSH and LC-LSH4 algorithms for synchronous and\nasynchronous settings, respectively. These algorithms significantly reduce\nimplementation complexity while maintaining expected time-to-rendezvous (ETTR)\nperformance comparable to state-of-the-art methods that require global channel\nenumeration. To ensure bounded maximum time-to-rendezvous (MTTR) in the\nasynchronous setting, we further introduce the ASYM-LC-LSH4 and QR-LC-LSH4\nalgorithms by embedding multiset-enhanced modular clock and quasi-random\ntechniques into our framework. Extensive simulations demonstrate that the\nproposed algorithms achieve performance comparable to state-of-the-art LSH\nalgorithms in both synchronous and asynchronous settings, even without a global\nchannel enumeration system.", "AI": {"tldr": "\u9488\u5bf9\u7269\u8054\u7f51\u65e0\u5168\u5c40\u4fe1\u9053\u679a\u4e3e\u7684\u591a\u4fe1\u9053\u4f1a\u5408\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u57fa\u4e8eLSH\u7684\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u8bbe\u5907\u53d1\u73b0\uff0c\u6027\u80fd\u5ab2\u7f8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u7269\u8054\u7f51\u90bb\u5c45\u53d1\u73b0\u4e2d\u7684\u591a\u4fe1\u9053\u4f1a\u5408\u95ee\u9898\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4fe1\u9053\u4ec5\u6709\u5c40\u90e8L\u4f4d\u6807\u8bc6\u7b26\u800c\u65e0\u5168\u5c40\u679a\u4e3e\u7cfb\u7edf\u7684\u573a\u666f\u4e0b\uff0c\u73b0\u6709\u65b9\u6848\u96be\u4ee5\u9002\u7528\uff0c\u4e9f\u9700\u521b\u65b0\u7b97\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u57fa\u4e8e\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\uff08LSH\uff09\u7684\u4f4e\u590d\u6742\u5ea6\u591a\u4fe1\u9053\u4f1a\u5408\u7b97\u6cd5\u3002\u5177\u4f53\u5305\u62ec\uff1a\u9488\u5bf9\u540c\u6b65\u548c\u5f02\u6b65\u8bbe\u7f6e\u7684LC-LSH\u53caLC-LSH4\u7b97\u6cd5\uff1b\u4ee5\u53ca\u4e3a\u4fdd\u8bc1\u5f02\u6b65\u8bbe\u7f6eMTTR\u6709\u754c\u800c\u878d\u5165\u591a\u96c6\u589e\u5f3a\u6a21\u5757\u65f6\u949f\u548c\u51c6\u968f\u673a\u6280\u672f\u7684ASYM-LC-LSH4\u548cQR-LC-LSH4\u7b97\u6cd5\u3002\u8fd9\u4e9b\u7b97\u6cd5\u53d7\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e00\u81f4\u6027\u54c8\u5e0c\u542f\u53d1\u3002", "result": "\u6240\u63d0\u7b97\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u5b9e\u73b0\u590d\u6742\u6027\uff0c\u540c\u65f6\u5728\u671f\u671b\u4f1a\u5408\u65f6\u95f4\uff08ETTR\uff09\u6027\u80fd\u4e0a\u4e0e\u9700\u5168\u5c40\u4fe1\u9053\u679a\u4e3e\u7684\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u3002\u4eff\u771f\u8868\u660e\uff0c\u5728\u540c\u6b65\u548c\u5f02\u6b65\u73af\u5883\u4e0b\uff0c\u5373\u4f7f\u6ca1\u6709\u5168\u5c40\u4fe1\u9053\u679a\u4e3e\u7cfb\u7edf\uff0c\u5176\u6027\u80fd\u4e5f\u4e0e\u73b0\u6709LSH\u7b97\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8eLSH\u7684\u591a\u4fe1\u9053\u4f1a\u5408\u7b97\u6cd5\uff0c\u5728\u7f3a\u4e4f\u5168\u5c40\u4fe1\u9053\u679a\u4e3e\u7684\u7269\u8054\u7f51\u73af\u5883\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u90bb\u5c45\u53d1\u73b0\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4f4e\u590d\u6742\u5ea6\u3001\u9ad8\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00311", "pdf": "https://arxiv.org/pdf/2509.00311", "abs": "https://arxiv.org/abs/2509.00311", "authors": ["Hikmat Khan", "Syed Farhan Alam Zaidi", "Pir Masoom Shah", "Kiruthika Balakrishnan", "Rabia Khan", "Muhammad Waqas", "Jia Wu"], "title": "MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification", "categories": ["cs.CV"], "comment": null, "summary": "Domain generalization in computational histopathology is hindered by\nheterogeneity in whole slide images (WSIs), caused by variations in tissue\npreparation, staining, and imaging conditions across institutions. Unlike\nmachine learning systems, pathologists rely on domain-invariant morphological\ncues such as nuclear atypia (enlargement, irregular contours, hyperchromasia,\nchromatin texture, spatial disorganization), structural atypia (abnormal\narchitecture and gland formation), and overall morphological atypia that remain\ndiagnostic across diverse settings. Motivated by this, we hypothesize that\nexplicitly modeling biologically robust nuclear morphology and spatial\norganization will enable the learning of cancer representations that are\nresilient to domain shifts. We propose MorphGen (Morphology-Guided\nGeneralization), a method that integrates histopathology images, augmentations,\nand nuclear segmentation masks within a supervised contrastive learning\nframework. By aligning latent representations of images and nuclear masks,\nMorphGen prioritizes diagnostic features such as nuclear and morphological\natypia and spatial organization over staining artifacts and domain-specific\nfeatures. To further enhance out-of-distribution robustness, we incorporate\nstochastic weight averaging (SWA), steering optimization toward flatter minima.\nAttention map analyses revealed that MorphGen primarily relies on nuclear\nmorphology, cellular composition, and spatial cell organization within tumors\nor normal regions for final classification. Finally, we demonstrate resilience\nof the learned representations to image corruptions (such as staining\nartifacts) and adversarial attacks, showcasing not only OOD generalization but\nalso addressing critical vulnerabilities in current deep learning systems for\ndigital pathology. Code, datasets, and trained models are available at:\nhttps://github.com/hikmatkhan/MorphGen", "AI": {"tldr": "MorphGen\u662f\u4e00\u79cd\u57fa\u4e8e\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u6838\u5206\u5272\u63a9\u6a21\uff0c\u660e\u786e\u5efa\u6a21\u7ec6\u80de\u6838\u5f62\u6001\u548c\u7a7a\u95f4\u7ec4\u7ec7\uff0c\u4ee5\u5b66\u4e60\u5bf9\u75c5\u7406\u56fe\u50cf\u57df\u504f\u79fb\u548c\u56fe\u50cf\u635f\u574f\u5177\u6709\u9c81\u68d2\u6027\u7684\u764c\u75c7\u8868\u5f81\uff0c\u4ece\u800c\u63d0\u9ad8\u8ba1\u7b97\u7ec4\u7ec7\u75c5\u7406\u5b66\u4e2d\u7684\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u8ba1\u7b97\u7ec4\u7ec7\u75c5\u7406\u5b66\u4e2d\u7684\u57df\u6cdb\u5316\u80fd\u529b\u53d7\u5168\u73bb\u7247\u56fe\u50cf\uff08WSIs\uff09\u5f02\u8d28\u6027\uff08\u7531\u7ec4\u7ec7\u5236\u5907\u3001\u67d3\u8272\u548c\u6210\u50cf\u6761\u4ef6\u5f15\u8d77\uff09\u7684\u963b\u788d\u3002\u4e0e\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e0d\u540c\uff0c\u75c5\u7406\u5b66\u5bb6\u4f9d\u8d56\u57df\u4e0d\u53d8\u7684\u5f62\u6001\u5b66\u7279\u5f81\uff08\u5982\u7ec6\u80de\u6838\u5f02\u578b\u6027\u3001\u7ed3\u6784\u5f02\u578b\u6027\uff09\u3002\u672c\u7814\u7a76\u5047\u8bbe\uff0c\u660e\u786e\u5efa\u6a21\u751f\u7269\u5b66\u4e0a\u9c81\u68d2\u7684\u7ec6\u80de\u6838\u5f62\u6001\u548c\u7a7a\u95f4\u7ec4\u7ec7\uff0c\u5c06\u4f7f\u5b66\u4e60\u5230\u7684\u764c\u75c7\u8868\u5f81\u5bf9\u57df\u504f\u79fb\u5177\u6709\u5f39\u6027\u3002", "method": "\u63d0\u51faMorphGen\u65b9\u6cd5\uff0c\u5b83\u5728\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u5185\u6574\u5408\u7ec4\u7ec7\u75c5\u7406\u56fe\u50cf\u3001\u589e\u5f3a\u6280\u672f\u548c\u6838\u5206\u5272\u63a9\u6a21\u3002\u901a\u8fc7\u5bf9\u9f50\u56fe\u50cf\u548c\u6838\u63a9\u6a21\u7684\u6f5c\u5728\u8868\u5f81\uff0cMorphGen\u4f18\u5148\u8003\u8651\u7ec6\u80de\u6838\u548c\u5f62\u6001\u5f02\u578b\u6027\u3001\u7a7a\u95f4\u7ec4\u7ec7\u7b49\u8bca\u65ad\u7279\u5f81\uff0c\u800c\u975e\u67d3\u8272\u4f2a\u5f71\u548c\u57df\u7279\u5f02\u6027\u7279\u5f81\u3002\u4e3a\u8fdb\u4e00\u6b65\u589e\u5f3a\u5206\u5e03\u5916\uff08OOD\uff09\u9c81\u68d2\u6027\uff0c\u8be5\u65b9\u6cd5\u8fd8\u7ed3\u5408\u4e86\u968f\u673a\u6743\u91cd\u5e73\u5747\uff08SWA\uff09\uff0c\u4ee5\u5c06\u4f18\u5316\u5f15\u5411\u66f4\u5e73\u5766\u7684\u6700\u5c0f\u503c\u3002", "result": "\u6ce8\u610f\u529b\u56fe\u5206\u6790\u663e\u793a\uff0cMorphGen\u4e3b\u8981\u4f9d\u8d56\u7ec6\u80de\u6838\u5f62\u6001\u3001\u7ec6\u80de\u7ec4\u6210\u548c\u80bf\u7624\u6216\u6b63\u5e38\u533a\u57df\u5185\u7684\u7a7a\u95f4\u7ec6\u80de\u7ec4\u7ec7\u8fdb\u884c\u6700\u7ec8\u5206\u7c7b\u3002\u7814\u7a76\u8bc1\u660e\uff0c\u6240\u5b66\u8868\u5f81\u5bf9\u56fe\u50cf\u635f\u574f\uff08\u5982\u67d3\u8272\u4f2a\u5f71\uff09\u548c\u5bf9\u6297\u6027\u653b\u51fb\u5177\u6709\u5f39\u6027\uff0c\u4e0d\u4ec5\u5c55\u793a\u4e86OOD\u6cdb\u5316\u80fd\u529b\uff0c\u8fd8\u89e3\u51b3\u4e86\u5f53\u524d\u6570\u5b57\u75c5\u7406\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\u3002", "conclusion": "MorphGen\u901a\u8fc7\u660e\u786e\u5229\u7528\u751f\u7269\u5b66\u4e0a\u9c81\u68d2\u7684\u7ec6\u80de\u6838\u5f62\u6001\u548c\u7a7a\u95f4\u7ec4\u7ec7\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u8ba1\u7b97\u7ec4\u7ec7\u75c5\u7406\u5b66\u7684\u57df\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u56fe\u50cf\u635f\u574f\u4e0e\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u6570\u5b57\u75c5\u7406\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.00272", "pdf": "https://arxiv.org/pdf/2509.00272", "abs": "https://arxiv.org/abs/2509.00272", "authors": ["Boqi Chen", "Kua Chen", "Jos\u00e9 Antonio Hern\u00e1ndez L\u00f3pez", "Gunter Mussbacher", "D\u00e1niel Varr\u00f3", "Amir Feizpour"], "title": "SHERPA: A Model-Driven Framework for Large Language Model Execution", "categories": ["cs.AI", "cs.SE"], "comment": "MODELS 2025", "summary": "Recently, large language models (LLMs) have achieved widespread application\nacross various fields. Despite their impressive capabilities, LLMs suffer from\na lack of structured reasoning ability, particularly for complex tasks\nrequiring domain-specific best practices, which are often unavailable in the\ntraining data. Although multi-step prompting methods incorporating human best\npractices, such as chain-of-thought and tree-of-thought, have gained\npopularity, they lack a general mechanism to control LLM behavior. In this\npaper, we propose SHERPA, a model-driven framework to improve the LLM\nperformance on complex tasks by explicitly incorporating domain-specific best\npractices into hierarchical state machines. By structuring the LLM execution\nprocesses using state machines, SHERPA enables more fine-grained control over\ntheir behavior via rules or decisions driven by machine learning-based\napproaches, including LLMs. We show that SHERPA is applicable to a wide variety\nof tasks-specifically, code generation, class name generation, and question\nanswering-replicating previously proposed approaches while further improving\nthe performance. We demonstrate the effectiveness of SHERPA for the\naforementioned tasks using various LLMs. Our systematic evaluation compares\ndifferent state machine configurations against baseline approaches without\nstate machines. Results show that integrating well-designed state machines\nsignificantly improves the quality of LLM outputs, and is particularly\nbeneficial for complex tasks with well-established human best practices but\nlacking data used for training LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSHERPA\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9886\u57df\u7279\u5b9a\u6700\u4f73\u5b9e\u8df5\u878d\u5165\u5206\u5c42\u72b6\u6001\u673a\uff0c\u5b9e\u73b0\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u884c\u4e3a\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8LLMs\u5728\u4ee3\u7801\u751f\u6210\u3001\u7c7b\u540d\u751f\u6210\u548c\u95ee\u7b54\u7b49\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1LLMs\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u5728\u9700\u8981\u9886\u57df\u7279\u5b9a\u6700\u4f73\u5b9e\u8df5\u7684\u590d\u6742\u4efb\u52a1\u4e0a\uff0c\u5b83\u4eec\u7f3a\u4e4f\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u8fd9\u4e9b\u5b9e\u8df5\u5f80\u5f80\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u7f3a\u5931\u3002\u73b0\u6709\u591a\u6b65\u63d0\u793a\u65b9\u6cd5\uff08\u5982CoT\uff09\u7f3a\u4e4f\u901a\u7528\u673a\u5236\u6765\u7cbe\u7ec6\u63a7\u5236LLM\u884c\u4e3a\u3002", "method": "\u63d0\u51faSHERPA\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u9886\u57df\u7279\u5b9a\u6700\u4f73\u5b9e\u8df5\u660e\u786e\u5730\u6574\u5408\u5230\u5206\u5c42\u72b6\u6001\u673a\u4e2d\uff0c\u6765\u6539\u8fdbLLMs\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002SHERPA\u5229\u7528\u72b6\u6001\u673a\u7ed3\u6784\u5316LLM\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u89c4\u5219\u6216\u673a\u5668\u5b66\u4e60\uff08\u5305\u62ecLLMs\uff09\u9a71\u52a8\u7684\u51b3\u7b56\uff0c\u5b9e\u73b0\u5bf9LLM\u884c\u4e3a\u7684\u7cbe\u7ec6\u63a7\u5236\u3002", "result": "SHERPA\u6846\u67b6\u9002\u7528\u4e8e\u4ee3\u7801\u751f\u6210\u3001\u7c7b\u540d\u751f\u6210\u548c\u95ee\u7b54\u7b49\u591a\u79cd\u4efb\u52a1\uff0c\u5e76\u80fd\u590d\u73b0\u5e76\u8fdb\u4e00\u6b65\u63d0\u9ad8\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u7cfb\u7edf\u8bc4\u4f30\u8868\u660e\uff0c\u6574\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u72b6\u6001\u673a\u80fd\u663e\u8457\u63d0\u9ad8LLM\u8f93\u51fa\u7684\u8d28\u91cf\uff0c\u5c24\u5176\u5bf9\u4e8e\u5177\u6709\u826f\u597d\u4eba\u7c7b\u6700\u4f73\u5b9e\u8df5\u4f46LLM\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u7684\u590d\u6742\u4efb\u52a1\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u72b6\u6001\u673a\u878d\u5165LLM\u7684\u6267\u884c\u6d41\u7a0b\uff0c\u5e76\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u6700\u4f73\u5b9e\u8df5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347LLM\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8f93\u51fa\u8d28\u91cf\u548c\u63a7\u5236\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u4f46\u6709\u6e05\u6670\u4eba\u7c7b\u6700\u4f73\u5b9e\u8df5\u7684\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2509.00073", "pdf": "https://arxiv.org/pdf/2509.00073", "abs": "https://arxiv.org/abs/2509.00073", "authors": ["Ankit Shetgaonkar", "Dipen Pradhan", "Lakshit Arora", "Sanjay Surendranath Girija", "Shashank Kapoor", "Aman Raj"], "title": "Mitigating Clinician Information Overload: Generative AI for Integrated EHR and RPM Data Analysis", "categories": ["cs.LG"], "comment": "Accepted at IEEE COMPSAC 2025", "summary": "Generative Artificial Intelligence (GenAI), particularly Large Language\nModels (LLMs), offer powerful capabilities for interpreting the complex data\nlandscape in healthcare. In this paper, we present a comprehensive overview of\nthe capabilities, requirements and applications of GenAI for deriving clinical\ninsights and improving clinical efficiency. We first provide some background on\nthe forms and sources of patient data, namely real-time Remote Patient\nMonitoring (RPM) streams and traditional Electronic Health Records (EHRs). The\nsheer volume and heterogeneity of this combined data present significant\nchallenges to clinicians and contribute to information overload. In addition,\nwe explore the potential of LLM-powered applications for improving clinical\nefficiency. These applications can enhance navigation of longitudinal patient\ndata and provide actionable clinical decision support through natural language\ndialogue. We discuss the opportunities this presents for streamlining clinician\nworkflows and personalizing care, alongside critical challenges such as data\nintegration complexity, ensuring data quality and RPM data reliability,\nmaintaining patient privacy, validating AI outputs for clinical safety,\nmitigating bias, and ensuring clinical acceptance. We believe this work\nrepresents the first summarization of GenAI techniques for managing clinician\ndata overload due to combined RPM / EHR data complexities.", "AI": {"tldr": "\u672c\u6587\u6982\u8ff0\u4e86\u751f\u6210\u5f0fAI\uff08\u7279\u522b\u662fLLMs\uff09\u5728\u533b\u7597\u9886\u57df\u4e2d\uff0c\u5982\u4f55\u5229\u7528RPM\u548cEHR\u6570\u636e\u6765\u89e3\u51b3\u4e34\u5e8a\u533b\u751f\u4fe1\u606f\u8fc7\u8f7d\u95ee\u9898\uff0c\u63d0\u5347\u4e34\u5e8a\u6548\u7387\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5e94\u7528\u6f5c\u529b\u4e0e\u6311\u6218\u3002", "motivation": "\u4e34\u5e8a\u533b\u751f\u9762\u4e34\u6765\u81ea\u8fdc\u7a0b\u60a3\u8005\u76d1\u6d4b(RPM)\u548c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55(EHR)\u76f8\u7ed3\u5408\u7684\u5e9e\u5927\u4e14\u5f02\u6784\u6570\u636e\u5e26\u6765\u7684\u5de8\u5927\u6311\u6218\uff0c\u5bfc\u81f4\u4fe1\u606f\u8fc7\u8f7d\uff0c\u6025\u9700\u5229\u7528AI\u6280\u672f\u6765\u89e3\u6790\u590d\u6742\u6570\u636e\uff0c\u63d0\u5347\u4e34\u5e8a\u6d1e\u5bdf\u529b\u548c\u6548\u7387\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5bf9\u751f\u6210\u5f0fAI\uff08\u5c24\u5176\u662fLLMs\uff09\u5728\u533b\u7597\u9886\u57df\u7684\u80cc\u666f\u3001\u80fd\u529b\u3001\u9700\u6c42\u548c\u5e94\u7528\u8fdb\u884c\u5168\u9762\u6982\u8ff0\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u63a2\u8ba8LLM\u9a71\u52a8\u7684\u5e94\u7528\u5982\u4f55\u6539\u5584\u60a3\u8005\u6570\u636e\u5bfc\u822a\u548c\u63d0\u4f9b\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\uff1b\u8ba8\u8bba\u7b80\u5316\u5de5\u4f5c\u6d41\u7a0b\u548c\u4e2a\u6027\u5316\u62a4\u7406\u7684\u673a\u9047\uff1b\u5206\u6790\u6570\u636e\u6574\u5408\u3001\u8d28\u91cf\u3001\u9690\u79c1\u3001AI\u8f93\u51fa\u9a8c\u8bc1\u3001\u504f\u89c1\u7f13\u89e3\u53ca\u4e34\u5e8a\u63a5\u53d7\u5ea6\u7b49\u5173\u952e\u6311\u6218\u3002", "result": "LLM\u9a71\u52a8\u7684\u5e94\u7528\u80fd\u591f\u589e\u5f3a\u7eb5\u5411\u60a3\u8005\u6570\u636e\u5bfc\u822a\uff0c\u5e76\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3002\u672c\u6587\u8bc6\u522b\u4e86\u7b80\u5316\u4e34\u5e8a\u533b\u751f\u5de5\u4f5c\u6d41\u7a0b\u548c\u5b9e\u73b0\u4e2a\u6027\u5316\u62a4\u7406\u7684\u673a\u9047\uff0c\u5e76\u9610\u660e\u4e86\u5b9e\u65bdGenAI\u7684\u5173\u952e\u6311\u6218\uff0c\u5982\u6570\u636e\u96c6\u6210\u590d\u6742\u6027\u3001\u6570\u636e\u8d28\u91cf\u3001\u60a3\u8005\u9690\u79c1\u3001AI\u8f93\u51fa\u4e34\u5e8a\u5b89\u5168\u6027\u9a8c\u8bc1\u3001\u504f\u89c1\u7f13\u89e3\u4ee5\u53ca\u4e34\u5e8a\u63a5\u53d7\u5ea6\u3002", "conclusion": "\u751f\u6210\u5f0fAI\uff0c\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u89e3\u8bfb\u590d\u6742\u7684\u533b\u7597\u6570\u636e\u548c\u63d0\u9ad8\u4e34\u5e8a\u6548\u7387\u65b9\u9762\u5177\u6709\u5f3a\u5927\u6f5c\u529b\u3002\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u603b\u7ed3\u4e86GenAI\u6280\u672f\u5728\u7ba1\u7406\u7531RPM/EHR\u6570\u636e\u590d\u6742\u6027\u5f15\u8d77\u7684\u4e34\u5e8a\u533b\u751f\u6570\u636e\u8fc7\u8f7d\u65b9\u9762\u7684\u5e94\u7528\u3002\u5c3d\u7ba1\u5b58\u5728\u5de8\u5927\u673a\u9047\uff0c\u4f46\u6210\u529f\u90e8\u7f72GenAI\u4ecd\u9700\u89e3\u51b3\u4e00\u7cfb\u5217\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.00391", "pdf": "https://arxiv.org/pdf/2509.00391", "abs": "https://arxiv.org/abs/2509.00391", "authors": ["Yuting Tan", "Xuying Li", "Zhuo Li", "Huizhen Shu", "Peikang Hu"], "title": "The Resurgence of GCG Adversarial Attacks on Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "Gradient-based adversarial prompting, such as the Greedy Coordinate Gradient\n(GCG) algorithm, has emerged as a powerful method for jailbreaking large\nlanguage models (LLMs). In this paper, we present a systematic appraisal of GCG\nand its annealing-augmented variant, T-GCG, across open-source LLMs of varying\nscales. Using Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B, we evaluate attack\neffectiveness on both safety-oriented prompts (AdvBench) and\nreasoning-intensive coding prompts. Our study reveals three key findings: (1)\nattack success rates (ASR) decrease with model size, reflecting the increasing\ncomplexity and non-convexity of larger models' loss landscapes; (2)\nprefix-based heuristics substantially overestimate attack effectiveness\ncompared to GPT-4o semantic judgments, which provide a stricter and more\nrealistic evaluation; and (3) coding-related prompts are significantly more\nvulnerable than adversarial safety prompts, suggesting that reasoning itself\ncan be exploited as an attack vector. In addition, preliminary results with\nT-GCG show that simulated annealing can diversify adversarial search and\nachieve competitive ASR under prefix evaluation, though its benefits under\nsemantic judgment remain limited. Together, these findings highlight the\nscalability limits of GCG, expose overlooked vulnerabilities in reasoning\ntasks, and motivate further development of annealing-inspired strategies for\nmore robust adversarial evaluation.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u68af\u5ea6\u5bf9\u6297\u6027\u63d0\u793a\u7b97\u6cd5GCG\u53ca\u5176\u53d8\u4f53T-GCG\u5728\u4e0d\u540c\u89c4\u6a21\u5f00\u6e90LLM\u4e0a\u7684\u8d8a\u72f1\u80fd\u529b\u3002\u4e3b\u8981\u53d1\u73b0\u5305\u62ec\uff1a\u653b\u51fb\u6210\u529f\u7387\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5927\u800c\u964d\u4f4e\uff0c\u7f16\u7801\u4efb\u52a1\u6bd4\u5b89\u5168\u4efb\u52a1\u66f4\u6613\u53d7\u653b\u51fb\uff0c\u4e14\u57fa\u4e8e\u524d\u7f00\u7684\u8bc4\u4f30\u65b9\u6cd5\u4f1a\u663e\u8457\u9ad8\u4f30\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u9274\u4e8e\u68af\u5ea6\u5bf9\u6297\u6027\u63d0\u793a\uff08\u5982GCG\uff09\u5df2\u6210\u4e3a\u8d8a\u72f1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5f3a\u5927\u65b9\u6cd5\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5bf9GCG\u53ca\u5176\u6a21\u62df\u9000\u706b\u589e\u5f3a\u53d8\u4f53T-GCG\u5728\u4e0d\u540c\u89c4\u6a21\u5f00\u6e90LLM\u4e0a\u7684\u6709\u6548\u6027\u548c\u5c40\u9650\u6027\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86Qwen2.5-0.5B\u3001LLaMA-3.2-1B\u548cGPT-OSS-20B\u7b49\u5f00\u6e90LLM\uff0c\u5728\u5b89\u5168\u5bfc\u5411\u63d0\u793a\uff08AdvBench\uff09\u548c\u63a8\u7406\u5bc6\u96c6\u578b\u7f16\u7801\u63d0\u793a\u4e0a\u8bc4\u4f30\u4e86\u653b\u51fb\u6709\u6548\u6027\u3002\u8bc4\u4f30\u65b9\u6cd5\u5bf9\u6bd4\u4e86\u57fa\u4e8e\u524d\u7f00\u7684\u542f\u53d1\u5f0f\u8bc4\u4f30\u4e0eGPT-4o\u8bed\u4e49\u5224\u65ad\u7684\u4e25\u683c\u8bc4\u4f30\u3002", "result": "1. \u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5927\u800c\u964d\u4f4e\uff0c\u8fd9\u53cd\u6620\u4e86\u5927\u578b\u6a21\u578b\u635f\u5931\u666f\u89c2\u7684\u590d\u6742\u6027\u30022. \u57fa\u4e8e\u524d\u7f00\u7684\u542f\u53d1\u5f0f\u8bc4\u4f30\u65b9\u6cd5\u663e\u8457\u9ad8\u4f30\u4e86\u653b\u51fb\u6548\u679c\uff0c\u800cGPT-4o\u8bed\u4e49\u5224\u65ad\u63d0\u4f9b\u4e86\u66f4\u4e25\u683c\u548c\u771f\u5b9e\u7684\u8bc4\u4f30\u30023. \u7f16\u7801\u76f8\u5173\u63d0\u793a\u8bcd\u6bd4\u5bf9\u6297\u6027\u5b89\u5168\u63d0\u793a\u8bcd\u66f4\u6613\u53d7\u653b\u51fb\uff0c\u8868\u660e\u63a8\u7406\u672c\u8eab\u53ef\u4ee5\u88ab\u5229\u7528\u4f5c\u4e3a\u653b\u51fb\u5411\u91cf\u30024. T-GCG\uff08\u6a21\u62df\u9000\u706b\u589e\u5f3a\uff09\u80fd\u4f7f\u5bf9\u6297\u6027\u641c\u7d22\u591a\u6837\u5316\uff0c\u5e76\u5728\u524d\u7f00\u8bc4\u4f30\u4e0b\u83b7\u5f97\u6709\u7ade\u4e89\u529b\u7684ASR\uff0c\u4f46\u5728\u8bed\u4e49\u5224\u65ad\u4e0b\u5176\u76ca\u5904\u6709\u9650\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86GCG\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u66b4\u9732\u4e86\u63a8\u7406\u4efb\u52a1\u4e2d\u88ab\u5ffd\u89c6\u7684\u6f0f\u6d1e\uff0c\u5e76\u4fc3\u4f7f\u7814\u7a76\u4eba\u5458\u8fdb\u4e00\u6b65\u5f00\u53d1\u53d7\u6a21\u62df\u9000\u706b\u542f\u53d1\u7684\u65b0\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u5bf9\u6297\u6027\u8bc4\u4f30\u3002"}}
{"id": "2509.00967", "pdf": "https://arxiv.org/pdf/2509.00967", "abs": "https://arxiv.org/abs/2509.00967", "authors": ["Nadjib Achir", "Philippe Jacquet"], "title": "BUBBLE-BLUE a multihop private network based on Bluetooth", "categories": ["cs.NI"], "comment": "preprint", "summary": "The BUBBLE-BLUE (BB) project aims to create private Bluetooth bubbles on top\nof smartphones and to create a kind of terrestrial STARLINK network based on\nusers smartphones.. In each private bubble, participants will be able to\ncommunicate autonomously, without recourse to private operator networks,\nneither data nor cellular, relying solely on the Bluetooth technology of\nsmartphones. The routing strategy is based on dynamic Connected Dominant Sets\n(CDS). We present the specific features of a BB network as well as some\nsimulation results on their routing performance.", "AI": {"tldr": "BUBBLE-BLUE (BB) \u9879\u76ee\u65e8\u5728\u5229\u7528\u667a\u80fd\u624b\u673a\u7684\u84dd\u7259\u529f\u80fd\uff0c\u6784\u5efa\u79c1\u6709\u3001\u8131\u79bb\u8fd0\u8425\u5546\u7684\u81ea\u4e3b\u901a\u4fe1\u7f51\u7edc\uff0c\u5e76\u57fa\u4e8e\u52a8\u6001\u8fde\u901a\u652f\u914d\u96c6\uff08CDS\uff09\u8fdb\u884c\u8def\u7531\uff0c\u5c55\u793a\u4e86\u5176\u7f51\u7edc\u7279\u6027\u53ca\u8def\u7531\u6027\u80fd\u7684\u4eff\u771f\u7ed3\u679c\u3002", "motivation": "\u521b\u5efa\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u84dd\u7259\u7684\u79c1\u6709\u901a\u4fe1\u6c14\u6ce1\uff0c\u5b9e\u73b0\u7528\u6237\u95f4\u65e0\u9700\u4f9d\u8d56\u8fd0\u8425\u5546\u7f51\u7edc\uff08\u6570\u636e\u6216\u8702\u7a9d\uff09\u7684\u81ea\u4e3b\u901a\u4fe1\uff0c\u76ee\u6807\u662f\u6784\u5efa\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684\u5730\u9762STARLINK\u7f51\u7edc\u3002", "method": "\u8def\u7531\u7b56\u7565\u57fa\u4e8e\u52a8\u6001\u8fde\u901a\u652f\u914d\u96c6\uff08Connected Dominant Sets, CDS\uff09\u3002", "result": "\u8bba\u6587\u4ecb\u7ecd\u4e86BB\u7f51\u7edc\u7684\u5177\u4f53\u7279\u5f81\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u8def\u7531\u6027\u80fd\u7684\u4e00\u4e9b\u4eff\u771f\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u84dd\u7259\u7684\u81ea\u4e3b\u901a\u4fe1\u7f51\u7edc\u65b9\u6848\uff0c\u5e76\u63d0\u4f9b\u4e86\u521d\u6b65\u7684\u7f51\u7edc\u529f\u80fd\u53ca\u8def\u7531\u6027\u80fd\u8bc4\u4f30\u3002"}}
{"id": "2509.00320", "pdf": "https://arxiv.org/pdf/2509.00320", "abs": "https://arxiv.org/abs/2509.00320", "authors": ["Hao Zhang", "Mengsi Lyu", "Chenrui He", "Yulong Ao", "Yonghua Lin"], "title": "Towards Adaptive Visual Token Pruning for Large Multimodal Models", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Large Multimodal Models (LMMs) have achieved significant success across\nvarious tasks. These models usually encode visual inputs into dense token\nsequences, which are then concatenated with textual tokens and jointly\nprocessed by a language model. However, the increased token count substantially\nraises computational and memory costs during inference. Token pruning has\nemerged as a promising approach to address this issue. Existing token pruning\nmethods often rely on costly calibration or suboptimal importance metrics,\nleading to redundant retained tokens. In this paper, we analyze the redundancy\ndifferences between visual and textual tokens and propose pruning exclusively\non visual tokens. Based on this, we propose a visual token pruning strategy\nthat explicitly preserves both cross-modal alignment and intra-modal\ninformational diversity. We introduce a mutual information-based token pruning\nstrategy that removes visual tokens semantically misaligned with textual\ntokens, effectively preserving the alignment between the visual and textual\nmodalities. To further improve the representational quality of the retained\ntokens, we additionally prune redundant visual tokens by maximizing the\nexpected pairwise distances in the embedding space, which is solved efficiently\nwith a greedy algorithm. Extensive experiments demonstrate that our method\nmaintains strong performance while reducing tokens by 88.9% on models such as\nLLaVA-1.5-7B and LLaVA-NEXT-7B, resulting in a 56.7% improvement in inference\nspeed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u7684\u89c6\u89c9token\u526a\u679d\u7b56\u7565\uff0c\u901a\u8fc7\u4fdd\u7559\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u6a21\u6001\u5185\u4fe1\u606f\u591a\u6837\u6027\uff0c\u663e\u8457\u51cf\u5c11token\u6570\u91cf\uff0888.9%\uff09\u5e76\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0856.7%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5c06\u89c6\u89c9\u8f93\u5165\u7f16\u7801\u4e3a\u5bc6\u96c6token\u5e8f\u5217\uff0c\u4e0e\u6587\u672ctoken\u5408\u5e76\u540e\u5bfc\u81f4token\u6570\u91cf\u5927\u5e45\u589e\u52a0\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002\u73b0\u6709token\u526a\u679d\u65b9\u6cd5\u5b58\u5728\u6821\u51c6\u6210\u672c\u9ad8\u6216\u91cd\u8981\u6027\u5ea6\u91cf\u6b21\u4f18\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u4fdd\u7559\u5197\u4f59token\u3002", "method": "\u5206\u6790\u89c6\u89c9\u548c\u6587\u672ctoken\u7684\u5197\u4f59\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4ec5\u5bf9\u89c6\u89c9token\u8fdb\u884c\u526a\u679d\u3002\u5177\u4f53\u7b56\u7565\u5305\u62ec\uff1a1) \u57fa\u4e8e\u4e92\u4fe1\u606f\u79fb\u9664\u4e0e\u6587\u672ctoken\u8bed\u4e49\u672a\u5bf9\u9f50\u7684\u89c6\u89c9token\uff0c\u4ee5\u4fdd\u7559\u8de8\u6a21\u6001\u5bf9\u9f50\uff1b2) \u901a\u8fc7\u6700\u5927\u5316\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u9884\u671f\u6210\u5bf9\u8ddd\u79bb\uff0c\u5229\u7528\u8d2a\u5a6a\u7b97\u6cd5\u526a\u679d\u5197\u4f59\u89c6\u89c9token\uff0c\u4ee5\u63d0\u9ad8\u4fdd\u7559token\u7684\u8868\u5f81\u8d28\u91cf\u548c\u6a21\u6001\u5185\u591a\u6837\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728LLaVA-1.5-7B\u548cLLaVA-NEXT-7B\u7b49\u6a21\u578b\u4e0a\uff0c\u5728\u4fdd\u6301\u5f3a\u5927\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e8688.9%\u7684token\u51cf\u5c11\u91cf\uff0c\u5e76\u5c06\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u4e8656.7%\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u4fdd\u7559\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u6a21\u6001\u5185\u4fe1\u606f\u591a\u6837\u6027\uff0c\u672c\u6587\u63d0\u51fa\u7684\u89c6\u89c9token\u526a\u679d\u7b56\u7565\u80fd\u6709\u6548\u964d\u4f4e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u7ef4\u6301\u6027\u80fd\u3002"}}
{"id": "2509.00287", "pdf": "https://arxiv.org/pdf/2509.00287", "abs": "https://arxiv.org/abs/2509.00287", "authors": ["Brian Wang", "Mani Srivastava"], "title": "SIGMUS: Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces", "categories": ["cs.AI", "cs.CY"], "comment": "9 pages, accepted at UrbComp 2025 KDD 2025", "summary": "Modern urban spaces are equipped with an increasingly diverse set of sensors,\nall producing an abundance of multimodal data. Such multimodal data can be used\nto identify and reason about important incidents occurring in urban landscapes,\nsuch as major emergencies, cultural and social events, as well as natural\ndisasters. However, such data may be fragmented over several sources and\ndifficult to integrate due to the reliance on human-driven reasoning for\nidentifying relationships between the multimodal data corresponding to an\nincident, as well as understanding the different components which define an\nincident. Such relationships and components are critical to identifying the\ncauses of such incidents, as well as producing forecasting the scale and\nintensity of future incidents as they begin to develop. In this work, we create\nSIGMUS, a system for Semantic Integration for Knowledge Graphs in Multimodal\nUrban Spaces. SIGMUS uses Large Language Models (LLMs) to produce the necessary\nworld knowledge for identifying relationships between incidents occurring in\nurban spaces and data from different modalities, allowing us to organize\nevidence and observations relevant to an incident without relying and\nhuman-encoded rules for relating multimodal sensory data with incidents. This\norganized knowledge is represented as a knowledge graph, organizing incidents,\nobservations, and much more. We find that our system is able to produce\nreasonable connections between 5 different data sources (new article text, CCTV\nimages, air quality, weather, and traffic measurements) and relevant incidents\noccurring at the same time and location.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SIGMUS\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06\u57ce\u5e02\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u8bed\u4e49\u6574\u5408\u5230\u77e5\u8bc6\u56fe\u4e2d\uff0c\u4ee5\u8bc6\u522b\u548c\u5206\u6790\u57ce\u5e02\u4e8b\u4ef6\uff0c\u514b\u670d\u6570\u636e\u788e\u7247\u5316\u548c\u4eba\u5de5\u63a8\u7406\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u4ee3\u57ce\u5e02\u4f20\u611f\u5668\u4ea7\u751f\u5927\u91cf\u788e\u7247\u5316\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u96be\u4ee5\u6574\u5408\u4ee5\u8bc6\u522b\u548c\u63a8\u7406\u57ce\u5e02\u4e8b\u4ef6\uff08\u5982\u7d27\u6025\u60c5\u51b5\u3001\u6587\u5316\u6d3b\u52a8\u3001\u81ea\u7136\u707e\u5bb3\uff09\u53ca\u5176\u56e0\u679c\u5173\u7cfb\uff0c\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u7684\u89c4\u6a21\u548c\u5f3a\u5ea6\uff0c\u4e14\u76ee\u524d\u9ad8\u5ea6\u4f9d\u8d56\u4eba\u5de5\u63a8\u7406\u6765\u5efa\u7acb\u6570\u636e\u4e0e\u4e8b\u4ef6\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "method": "\u5f00\u53d1\u4e86SIGMUS\uff08Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces\uff09\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u5fc5\u8981\u7684\u201c\u4e16\u754c\u77e5\u8bc6\u201d\uff0c\u4ee5\u8bc6\u522b\u57ce\u5e02\u4e8b\u4ef6\u4e0e\u4e0d\u540c\u6a21\u6001\u6570\u636e\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ece\u800c\u7ec4\u7ec7\u4e0e\u4e8b\u4ef6\u76f8\u5173\u7684\u8bc1\u636e\u548c\u89c2\u6d4b\u7ed3\u679c\uff0c\u5e76\u5c06\u5176\u8868\u793a\u4e3a\u77e5\u8bc6\u56fe\u8c31\uff0c\u907f\u514d\u4f9d\u8d56\u4eba\u5de5\u7f16\u7801\u89c4\u5219\u3002", "result": "SIGMUS\u7cfb\u7edf\u80fd\u591f\u5728\u65b0\u6587\u7ae0\u6587\u672c\u3001\u95ed\u8def\u7535\u89c6\u56fe\u50cf\u3001\u7a7a\u6c14\u8d28\u91cf\u3001\u5929\u6c14\u548c\u4ea4\u901a\u6d4b\u91cf\u8fd95\u79cd\u4e0d\u540c\u6570\u636e\u6e90\u4e0e\u540c\u65f6\u540c\u5730\u53d1\u751f\u7684\u4e8b\u4ef6\u4e4b\u95f4\u5efa\u7acb\u5408\u7406\u7684\u5173\u8054\u3002", "conclusion": "SIGMUS\u7cfb\u7edf\u901a\u8fc7\u5229\u7528LLMs\u6709\u6548\u6574\u5408\u4e86\u591a\u6837\u5316\u7684\u57ce\u5e02\u6570\u636e\uff0c\u5e76\u5c06\u5176\u7ec4\u7ec7\u6210\u77e5\u8bc6\u56fe\u8c31\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u57ce\u5e02\u4e8b\u4ef6\u7684\u81ea\u52a8\u5316\u5206\u6790\u548c\u5173\u7cfb\u8bc6\u522b\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u4eba\u5de5\u6570\u636e\u96c6\u6210\u548c\u89c4\u5219\u4f9d\u8d56\u7684\u6311\u6218\u3002"}}
{"id": "2509.00076", "pdf": "https://arxiv.org/pdf/2509.00076", "abs": "https://arxiv.org/abs/2509.00076", "authors": ["Zachery Dahm", "Konstantinos Vasili", "Vasileios Theos", "Konstantinos Gkouliaras", "William Richards", "True Miller", "Brian Jowers", "Stylianos Chatzidakis"], "title": "Experimental Assessment of a Multi-Class AI/ML Architecture for Real-Time Characterization of Cyber Events in a Live Research Reactor", "categories": ["cs.LG"], "comment": null, "summary": "There is increased interest in applying Artificial Intelligence and Machine\nLearning (AI/ML) within the nuclear industry and nuclear engineering community.\nEffective implementation of AI/ML could offer benefits to the nuclear domain,\nincluding enhanced identification of anomalies, anticipation of system\nfailures, and operational schedule optimization. However, limited work has been\ndone to investigate the feasibility and applicability of AI/ML tools in a\nfunctioning nuclear reactor. Here, we go beyond the development of a single\nmodel and introduce a multi-layered AI/ML architecture that integrates both\ninformation technology and operational technology data streams to identify,\ncharacterize, and differentiate (i) among diverse cybersecurity events and (ii)\nbetween cyber events and other operational anomalies. Leveraging Purdue\nUniversitys research reactor, PUR-1, we demonstrate this architecture through a\nrepresentative use case that includes multiple concurrent false data injections\nand denial-of-service attacks of increasing complexity under realistic reactor\nconditions. The use case includes 14 system states (1 normal, 13 abnormal) and\nover 13.8 million multi-variate operational and information technology data\npoints. The study demonstrated the capability of AI/ML to distinguish between\nnormal, abnormal, and cybersecurity-related events, even under challenging\nconditions such as denial-of-service attacks. Combining operational and\ninformation technology data improved classification accuracy but posed\nchallenges related to synchronization and collection during certain cyber\nevents. While results indicate significant promise for AI/ML in nuclear\ncybersecurity, the findings also highlight the need for further refinement in\nhandling complex event differentiation and multi-class architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u591a\u5c42AI/ML\u67b6\u6784\uff0c\u901a\u8fc7\u6574\u5408\u6838\u53cd\u5e94\u5806\u7684IT\u548cOT\u6570\u636e\u6d41\uff0c\u4ee5\u533a\u5206\u7f51\u7edc\u5b89\u5168\u4e8b\u4ef6\u548c\u5176\u4ed6\u8fd0\u884c\u5f02\u5e38\u3002\u7814\u7a76\u5728\u771f\u5b9e\u53cd\u5e94\u5806\u6761\u4ef6\u4e0b\u8fdb\u884c\uff0c\u7ed3\u679c\u8868\u660eAI/ML\u5728\u6838\u7f51\u7edc\u5b89\u5168\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6570\u636e\u540c\u6b65\u548c\u590d\u6742\u4e8b\u4ef6\u533a\u5206\u7684\u6311\u6218\u3002", "motivation": "\u6838\u5de5\u4e1a\u5bf9AI/ML\u7684\u5e94\u7528\u5174\u8da3\u65e5\u76ca\u589e\u52a0\uff0c\u4ee5\u5b9e\u73b0\u5f02\u5e38\u8bc6\u522b\u3001\u6545\u969c\u9884\u6d4b\u548c\u4f18\u5316\u3002\u7136\u800c\uff0c\u5728\u5b9e\u9645\u8fd0\u884c\u7684\u6838\u53cd\u5e94\u5806\u4e2d\uff0cAI/ML\u5de5\u5177\u5728\u7f51\u7edc\u5b89\u5168\u65b9\u9762\u7684\u53ef\u884c\u6027\u548c\u9002\u7528\u6027\u7814\u7a76\u5c1a\u6709\u9650\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u5c42AI/ML\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u6574\u5408\u4e86\u4fe1\u606f\u6280\u672f\uff08IT\uff09\u548c\u64cd\u4f5c\u6280\u672f\uff08OT\uff09\u6570\u636e\u6d41\u3002\u5229\u7528\u666e\u6e21\u5927\u5b66\u7684PUR-1\u7814\u7a76\u5806\uff0c\u5728\u4e00\u4e2a\u5305\u542b\u591a\u91cd\u5e76\u53d1\u865a\u5047\u6570\u636e\u6ce8\u5165\u548c\u62d2\u7edd\u670d\u52a1\u653b\u51fb\u7684\u4ee3\u8868\u6027\u7528\u4f8b\u4e2d\u8fdb\u884c\u4e86\u6f14\u793a\u548c\u9a8c\u8bc1\u3002\u8be5\u7528\u4f8b\u5305\u542b14\u79cd\u7cfb\u7edf\u72b6\u6001\u548c\u8d85\u8fc71380\u4e07\u4e2a\u591a\u53d8\u91cf\u8fd0\u884c\u53caIT\u6570\u636e\u70b9\u3002", "result": "\u7814\u7a76\u8868\u660eAI/ML\u80fd\u591f\u533a\u5206\u6b63\u5e38\u3001\u5f02\u5e38\u548c\u7f51\u7edc\u5b89\u5168\u76f8\u5173\u4e8b\u4ef6\uff0c\u5373\u4f7f\u5728\u62d2\u7edd\u670d\u52a1\u653b\u51fb\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u4e5f\u8868\u73b0\u51fa\u8272\u3002\u7ed3\u5408OT\u548cIT\u6570\u636e\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u4f46\u5728\u67d0\u4e9b\u7f51\u7edc\u4e8b\u4ef6\u4e2d\uff0c\u6570\u636e\u540c\u6b65\u548c\u6536\u96c6\u9762\u4e34\u6311\u6218\u3002", "conclusion": "AI/ML\u5728\u6838\u7f51\u7edc\u5b89\u5168\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002\u7814\u7a76\u7ed3\u679c\u4e5f\u5f3a\u8c03\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u5b8c\u5584\u590d\u6742\u4e8b\u4ef6\u7684\u533a\u5206\u80fd\u529b\u548c\u591a\u7c7b\u522b\u67b6\u6784\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u540c\u6b65\u548c\u5904\u7406\u65b9\u9762\u3002"}}
{"id": "2509.00414", "pdf": "https://arxiv.org/pdf/2509.00414", "abs": "https://arxiv.org/abs/2509.00414", "authors": ["Juraj Vladika", "Florian Matthes"], "title": "MedSEBA: Synthesizing Evidence-Based Answers Grounded in Evolving Medical Literature", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to CIKM 2025", "summary": "In the digital age, people often turn to the Internet in search of medical\nadvice and recommendations. With the increasing volume of online content, it\nhas become difficult to distinguish reliable sources from misleading\ninformation. Similarly, millions of medical studies are published every year,\nmaking it challenging for researchers to keep track of the latest scientific\nfindings. These evolving studies can reach differing conclusions, which is not\nreflected in traditional search tools. To address these challenges, we\nintroduce MedSEBA, an interactive AI-powered system for synthesizing\nevidence-based answers to medical questions. It utilizes the power of Large\nLanguage Models to generate coherent and expressive answers, but grounds them\nin trustworthy medical studies dynamically retrieved from the research database\nPubMed. The answers consist of key points and arguments, which can be traced\nback to respective studies. Notably, the platform also provides an overview of\nthe extent to which the most relevant studies support or refute the given\nmedical claim, and a visualization of how the research consensus evolved\nthrough time. Our user study revealed that medical experts and lay users find\nthe system usable and helpful, and the provided answers trustworthy and\ninformative. This makes the system well-suited for both everyday health\nquestions and advanced research insights.", "AI": {"tldr": "MedSEBA\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u4ea4\u4e92\u5f0f\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ecePubMed\u52a8\u6001\u68c0\u7d22\u5e76\u6574\u5408\u53ef\u4fe1\u533b\u5b66\u7814\u7a76\uff0c\u4e3a\u533b\u5b66\u95ee\u9898\u63d0\u4f9b\u57fa\u4e8e\u8bc1\u636e\u7684\u7b54\u6848\uff0c\u5e76\u5c55\u793a\u7814\u7a76\u5171\u8bc6\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u60c5\u51b5\u3002", "motivation": "\u5728\u6570\u5b57\u65f6\u4ee3\uff0c\u4eba\u4eec\u96be\u4ee5\u533a\u5206\u7f51\u4e0a\u533b\u5b66\u4fe1\u606f\u7684\u53ef\u9760\u6027\uff1b\u6bcf\u5e74\u53d1\u8868\u6570\u767e\u4e07\u533b\u5b66\u7814\u7a76\uff0c\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u8ffd\u8e2a\u6700\u65b0\u53d1\u73b0\u548c\u5176\u53ef\u80fd\u5b58\u5728\u7684\u4e0d\u540c\u7ed3\u8bba\uff1b\u4f20\u7edf\u641c\u7d22\u5de5\u5177\u65e0\u6cd5\u53cd\u6620\u8fd9\u4e9b\u4e0d\u65ad\u6f14\u53d8\u7684\u7814\u7a76\u7ed3\u679c\u3002", "method": "\u5f00\u53d1\u4e86MedSEBA\uff0c\u4e00\u4e2a\u4ea4\u4e92\u5f0fAI\u7cfb\u7edf\uff0c\u5b83\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8fde\u8d2f\u4e14\u5bcc\u6709\u8868\u8fbe\u529b\u7684\u7b54\u6848\uff0c\u5e76\u901a\u8fc7\u4ecePubMed\u68c0\u7d22\u7684\u53ef\u4fe1\u533b\u5b66\u7814\u7a76\u6765\u652f\u6491\u8fd9\u4e9b\u7b54\u6848\u3002\u7b54\u6848\u5305\u542b\u53ef\u8ffd\u6eaf\u5230\u5177\u4f53\u7814\u7a76\u7684\u5173\u952e\u70b9\u548c\u8bba\u636e\uff0c\u5e76\u63d0\u4f9b\u76f8\u5173\u7814\u7a76\u652f\u6301\u6216\u9a73\u65a5\u7279\u5b9a\u533b\u5b66\u4e3b\u5f20\u7684\u7a0b\u5ea6\u6982\u8ff0\uff0c\u4ee5\u53ca\u7814\u7a76\u5171\u8bc6\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u89c6\u89c9\u5316\u5448\u73b0\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u533b\u5b66\u4e13\u5bb6\u548c\u666e\u901a\u7528\u6237\u90fd\u8ba4\u4e3a\u8be5\u7cfb\u7edf\u6613\u4e8e\u4f7f\u7528\u3001\u6709\u5e2e\u52a9\uff0c\u4e14\u63d0\u4f9b\u7684\u7b54\u6848\u503c\u5f97\u4fe1\u8d56\u548c\u4fe1\u606f\u4e30\u5bcc\u3002\u8fd9\u4f7f\u5f97\u8be5\u7cfb\u7edf\u975e\u5e38\u9002\u5408\u65e5\u5e38\u5065\u5eb7\u54a8\u8be2\u548c\u9ad8\u7ea7\u7814\u7a76\u6d1e\u5bdf\u3002", "conclusion": "MedSEBA\u901a\u8fc7\u63d0\u4f9b\u57fa\u4e8e\u8bc1\u636e\u3001\u53ef\u8ffd\u6eaf\u4e14\u80fd\u5c55\u793a\u7814\u7a76\u5171\u8bc6\u6f14\u53d8\u7684\u7b54\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5728\u7ebf\u533b\u7597\u4fe1\u606f\u53ef\u9760\u6027\u4f4e\u548c\u7814\u7a76\u8ffd\u8e2a\u56f0\u96be\u7684\u6311\u6218\uff0c\u4e3a\u533b\u5b66\u4e13\u4e1a\u4eba\u58eb\u548c\u516c\u4f17\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2509.01008", "pdf": "https://arxiv.org/pdf/2509.01008", "abs": "https://arxiv.org/abs/2509.01008", "authors": ["Fatma Chaouech", "Javier Villegas", "Ant\u00f3nio Pereira", "Carlos Baena", "Sergio Fortes", "Raquel Barco", "Dominic Gribben", "Mohammad Dib", "Alba Villarino", "Aser Cortines", "Rom\u00e1n Or\u00fas"], "title": "Quantum-based QoE Optimization in Advanced Cellular Networks: Integration and Cloud Gaming Use Case", "categories": ["cs.NI", "cs.LG", "quant-ph"], "comment": null, "summary": "This work explores the integration of Quantum Machine Learning (QML) and\nQuantum-Inspired (QI) techniques for optimizing end-to-end (E2E) network\nservices in telecommunication systems, particularly focusing on 5G networks and\nbeyond. The application of QML and QI algorithms is investigated, comparing\ntheir performance with classical Machine Learning (ML) approaches. The present\nstudy employs a hybrid framework combining quantum and classical computing\nleveraging the strengths of QML and QI, without the penalty of quantum hardware\navailability. This is particularized for the optimization of the Quality of\nExperience (QoE) over cellular networks. The framework comprises an estimator\nfor obtaining the expected QoE based on user metrics, service settings, and\ncell configuration, and an optimizer that uses the estimation to choose the\nbest cell and service configuration. Although the approach is applicable to any\nQoE-based network management, its implementation is particularized for the\noptimization of network configurations for Cloud Gaming services. Then, it is\nevaluated via performance metrics such as accuracy and model loading and\ninference times for the estimator, and time to solution and solution score for\nthe optimizer. The results indicate that QML models achieve similar or superior\naccuracy to classical ML models for estimation, while decreasing inference and\nloading times. Furthermore, potential for better performance is observed for\nhigher-dimensional data, highlighting promising results for higher complexity\nproblems. Thus, the results demonstrate the promising potential of QML in\nadvancing network optimization, although challenges related to data\navailability and integration complexities between quantum and classical ML are\nidentified as future research lines.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u548c\u91cf\u5b50\u542f\u53d1\uff08QI\uff09\u6280\u672f\u5728\u4f18\u53165G\u53ca\u672a\u6765\u7535\u4fe1\u7f51\u7edc\u7aef\u5230\u7aef\u670d\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u8d28\u91cf\uff08QoE\uff09\u65b9\u9762\u3002\u7ed3\u679c\u663e\u793a\uff0cQML\u6a21\u578b\u5728\u4f30\u8ba1\u7cbe\u5ea6\u4e0a\u4e0e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u540c\u65f6\u663e\u8457\u7f29\u77ed\u4e86\u63a8\u7406\u548c\u52a0\u8f7d\u65f6\u95f4\uff0c\u5728\u9ad8\u7ef4\u6570\u636e\u5904\u7406\u4e0a\u5c55\u73b0\u51fa\u66f4\u5927\u6f5c\u529b\u3002", "motivation": "\u4f18\u53165G\u53ca\u672a\u6765\u7535\u4fe1\u7f51\u7edc\u7684\u7aef\u5230\u7aef\uff08E2E\uff09\u670d\u52a1\u662f\u91cd\u8981\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u8d28\u91cf\uff08QoE\uff09\u65b9\u9762\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22QML\u548cQI\u6280\u672f\u5728\u6b64\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u4e0e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u65b9\u6cd5\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u4e2a\u7ed3\u5408\u91cf\u5b50\u4e0e\u7ecf\u5178\u8ba1\u7b97\u7684\u6df7\u5408\u6846\u67b6\uff0c\u65e8\u5728\u5229\u7528QML\u548cQI\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u907f\u514d\u5b8c\u5168\u4f9d\u8d56\u91cf\u5b50\u786c\u4ef6\u3002\u8be5\u6846\u67b6\u5305\u542b\uff1a1) \u4e00\u4e2a\u57fa\u4e8e\u7528\u6237\u6307\u6807\u3001\u670d\u52a1\u8bbe\u7f6e\u548c\u8702\u7a9d\u914d\u7f6e\u6765\u4f30\u8ba1\u9884\u671fQoE\u7684\u4f30\u7b97\u5668\uff1b2) \u4e00\u4e2a\u5229\u7528\u4f30\u7b97\u7ed3\u679c\u9009\u62e9\u6700\u4f73\u8702\u7a9d\u548c\u670d\u52a1\u914d\u7f6e\u7684\u4f18\u5316\u5668\u3002\u8be5\u65b9\u6cd5\u7279\u522b\u5e94\u7528\u4e8e\u4e91\u6e38\u620f\u670d\u52a1\u7684\u7f51\u7edc\u914d\u7f6e\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u4f30\u7b97\u5668\u7684\u51c6\u786e\u6027\u3001\u6a21\u578b\u52a0\u8f7d\u548c\u63a8\u7406\u65f6\u95f4\uff0c\u4ee5\u53ca\u4f18\u5316\u5668\u7684\u6c42\u89e3\u65f6\u95f4\u548c\u65b9\u6848\u5f97\u5206\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cQML\u6a21\u578b\u5728\u4f30\u8ba1\u7cbe\u5ea6\u65b9\u9762\u4e0e\u7ecf\u5178ML\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u7684\u63a8\u7406\u548c\u52a0\u8f7d\u65f6\u95f4\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u66f4\u9ad8\u7ef4\u5ea6\u7684\u6570\u636e\uff08\u5373\u66f4\u590d\u6742\u7684\u95ee\u9898\uff09\uff0cQML\u6a21\u578b\u5c55\u73b0\u51fa\u5b9e\u73b0\u66f4\u597d\u6027\u80fd\u7684\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86QML\u5728\u63a8\u52a8\u7f51\u7edc\u4f18\u5316\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u6570\u636e\u53ef\u7528\u6027\u4ee5\u53ca\u91cf\u5b50\u4e0e\u7ecf\u5178ML\u4e4b\u95f4\u96c6\u6210\u590d\u6742\u6027\u7b49\u6311\u6218\u88ab\u786e\u5b9a\u4e3a\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.00332", "pdf": "https://arxiv.org/pdf/2509.00332", "abs": "https://arxiv.org/abs/2509.00332", "authors": ["Wei Ao", "Vishnu Naresh Boddeti"], "title": "CryptoFace: End-to-End Encrypted Face Recognition", "categories": ["cs.CV", "cs.CR"], "comment": "CVPR 2025", "summary": "Face recognition is central to many authentication, security, and\npersonalized applications. Yet, it suffers from significant privacy risks,\nparticularly arising from unauthorized access to sensitive biometric data. This\npaper introduces CryptoFace, the first end-to-end encrypted face recognition\nsystem with fully homomorphic encryption (FHE). It enables secure processing of\nfacial data across all stages of a face-recognition process--feature\nextraction, storage, and matching--without exposing raw images or features. We\nintroduce a mixture of shallow patch convolutional networks to support\nhigher-dimensional tensors via patch-based processing while reducing the\nmultiplicative depth and, thus, inference latency. Parallel FHE evaluation of\nthese networks ensures near-resolution-independent latency. On standard face\nrecognition benchmarks, CryptoFace significantly accelerates inference and\nincreases verification accuracy compared to the state-of-the-art FHE neural\nnetworks adapted for face recognition. CryptoFace will facilitate secure face\nrecognition systems requiring robust and provable security. The code is\navailable at https://github.com/human-analysis/CryptoFace.", "AI": {"tldr": "CryptoFace\u662f\u9996\u4e2a\u57fa\u4e8e\u5168\u540c\u6001\u52a0\u5bc6\uff08FHE\uff09\u7684\u7aef\u5230\u7aef\u52a0\u5bc6\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6d45\u5c42\u8865\u4e01\u5377\u79ef\u7f51\u7edc\uff0c\u5728\u786e\u4fdd\u9690\u79c1\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u8bc6\u522b\u7cbe\u5ea6\u3002", "motivation": "\u4eba\u8138\u8bc6\u522b\u6280\u672f\u5e7f\u6cdb\u5e94\u7528\u4e8e\u8ba4\u8bc1\u3001\u5b89\u5168\u548c\u4e2a\u6027\u5316\u670d\u52a1\uff0c\u4f46\u5176\u654f\u611f\u7684\u751f\u7269\u7279\u5f81\u6570\u636e\u5b58\u5728\u4e25\u91cd\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u5b89\u5168\u5904\u7406\u4eba\u8138\u6570\u636e\u7684\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86CryptoFace\uff0c\u4e00\u4e2a\u5229\u7528\u5168\u540c\u6001\u52a0\u5bc6\uff08FHE\uff09\u5b9e\u73b0\u7aef\u5230\u7aef\u52a0\u5bc6\u7684\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u3002\u5b83\u901a\u8fc7\u5f15\u5165\u6d45\u5c42\u8865\u4e01\u5377\u79ef\u7f51\u7edc\u7684\u6df7\u5408\u4f53\uff0c\u652f\u6301\u57fa\u4e8e\u8865\u4e01\u7684\u9ad8\u7ef4\u5f20\u91cf\u5904\u7406\uff0c\u4ece\u800c\u51cf\u5c11\u4e58\u6cd5\u6df1\u5ea6\u5e76\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002\u901a\u8fc7\u5e76\u884cFHE\u8bc4\u4f30\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5206\u8fa8\u7387\u65e0\u5173\u7684\u5ef6\u8fdf\uff0c\u786e\u4fdd\u4eba\u8138\u6570\u636e\uff08\u7279\u5f81\u63d0\u53d6\u3001\u5b58\u50a8\u3001\u5339\u914d\uff09\u5728\u6574\u4e2a\u8fc7\u7a0b\u4e2d\u4e0d\u88ab\u66b4\u9732\u3002", "result": "\u5728\u6807\u51c6\u4eba\u8138\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCryptoFace\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684FHE\u795e\u7ecf\u7f51\u7edc\u76f8\u6bd4\uff0c\u663e\u8457\u52a0\u5feb\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u5e76\u63d0\u9ad8\u4e86\u9a8c\u8bc1\u51c6\u786e\u6027\u3002", "conclusion": "CryptoFace\u4e3a\u9700\u8981\u5f3a\u5927\u4e14\u53ef\u8bc1\u660e\u5b89\u5168\u4fdd\u969c\u7684\u52a0\u5bc6\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c06\u63a8\u52a8\u5b89\u5168\u4eba\u8138\u8bc6\u522b\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.00446", "pdf": "https://arxiv.org/pdf/2509.00446", "abs": "https://arxiv.org/abs/2509.00446", "authors": ["Yen-Che Chien", "Kuang-Da Wang", "Wei-Yao Wang", "Wen-Chih Peng"], "title": "NEWSAGENT: Benchmarking Multimodal Agents as Journalists with Real-World Newswriting Tasks", "categories": ["cs.AI"], "comment": "Preprint", "summary": "Recent advances in autonomous digital agents from industry (e.g., Manus AI\nand Gemini's research mode) highlight potential for structured tasks by\nautonomous decision-making and task decomposition; however, it remains unclear\nto what extent the agent-based systems can improve multimodal web data\nproductivity. We study this in the realm of journalism, which requires\niterative planning, interpretation, and contextual reasoning from multimodal\nraw contents to form a well structured news. We introduce NEWSAGENT, a\nbenchmark for evaluating how agents can automatically search available raw\ncontents, select desired information, and edit and rephrase to form a news\narticle by accessing core journalistic functions. Given a writing instruction\nand firsthand data as how a journalist initiates a news draft, agents are\ntasked to identify narrative perspectives, issue keyword-based queries,\nretrieve historical background, and generate complete articles. Unlike typical\nsummarization or retrieval tasks, essential context is not directly available\nand must be actively discovered, reflecting the information gaps faced in\nreal-world news writing. NEWSAGENT includes 6k human-verified examples derived\nfrom real news, with multimodal contents converted to text for broad model\ncompatibility. We evaluate open- and closed-sourced LLMs with commonly-used\nagentic frameworks on NEWSAGENT, which shows that agents are capable of\nretrieving relevant facts but struggling with planning and narrative\nintegration. We believe that NEWSAGENT serves a realistic testbed for iterating\nand evaluating agent capabilities in terms of multimodal web data manipulation\nto real-world productivity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NEWSAGENT\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u81ea\u4e3b\u667a\u80fd\u4f53\u4ece\u591a\u6a21\u6001\u7f51\u7edc\u6570\u636e\u751f\u6210\u65b0\u95fb\u6587\u7ae0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u667a\u80fd\u4f53\u5728\u4e8b\u5b9e\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u89c4\u5212\u548c\u53d9\u4e8b\u6574\u5408\u65b9\u9762\u4ecd\u6709\u6b20\u7f3a\u3002", "motivation": "\u5c3d\u7ba1\u884c\u4e1a\u5185\u7684\u81ea\u4e3b\u6570\u5b57\u667a\u80fd\u4f53\u5728\u7ed3\u6784\u5316\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u63d0\u9ad8\u591a\u6a21\u6001\u7f51\u7edc\u6570\u636e\u751f\u4ea7\u529b\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\u3002\u65b0\u95fb\u91c7\u5199\u662f\u4e00\u4e2a\u9700\u8981\u8fed\u4ee3\u89c4\u5212\u3001\u89e3\u8bfb\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u590d\u6742\u4efb\u52a1\uff0c\u4e14\u5e38\u9762\u4e34\u4fe1\u606f\u7f3a\u5931\uff0c\u8fd9\u4f7f\u5176\u6210\u4e3a\u7814\u7a76\u667a\u80fd\u4f53\u80fd\u529b\u63d0\u5347\u7684\u7406\u60f3\u9886\u57df\u3002", "method": "\u7814\u7a76\u8005\u5f15\u5165\u4e86NEWSAGENT\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53\u5982\u4f55\u81ea\u52a8\u641c\u7d22\u3001\u9009\u62e9\u3001\u7f16\u8f91\u548c\u91cd\u6784\u539f\u59cb\u5185\u5bb9\u4ee5\u5f62\u6210\u65b0\u95fb\u6587\u7ae0\u3002\u667a\u80fd\u4f53\u9700\u6839\u636e\u5199\u4f5c\u6307\u4ee4\u548c\u4e00\u624b\u8d44\u6599\uff0c\u8bc6\u522b\u53d9\u4e8b\u89c6\u89d2\u3001\u53d1\u51fa\u5173\u952e\u8bcd\u67e5\u8be2\u3001\u68c0\u7d22\u5386\u53f2\u80cc\u666f\u5e76\u751f\u6210\u5b8c\u6574\u6587\u7ae0\u3002\u8be5\u57fa\u51c6\u5305\u542b6000\u4e2a\u6765\u81ea\u771f\u5b9e\u65b0\u95fb\u7684\u4eba\u5de5\u9a8c\u8bc1\u793a\u4f8b\uff0c\u5e76\u5c06\u591a\u6a21\u6001\u5185\u5bb9\u8f6c\u6362\u4e3a\u6587\u672c\u4ee5\u517c\u5bb9\u4e3b\u6d41\u6a21\u578b\u3002\u7814\u7a76\u8005\u8bc4\u4f30\u4e86\u5f00\u6e90\u548c\u95ed\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5e38\u7528\u667a\u80fd\u4f53\u6846\u67b6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u667a\u80fd\u4f53\u80fd\u591f\u6709\u6548\u68c0\u7d22\u76f8\u5173\u4e8b\u5b9e\uff0c\u4f46\u5728\u65b0\u95fb\u5185\u5bb9\u7684\u89c4\u5212\u548c\u53d9\u4e8b\u6574\u5408\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "conclusion": "NEWSAGENT\u63d0\u4f9b\u4e86\u4e00\u4e2a\u771f\u5b9e\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u8fed\u4ee3\u548c\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u591a\u6a21\u6001\u7f51\u7edc\u6570\u636e\u5904\u7406\u65b9\u9762\u5b9e\u73b0\u5b9e\u9645\u751f\u4ea7\u529b\u7684\u80fd\u529b\u3002"}}
{"id": "2509.00083", "pdf": "https://arxiv.org/pdf/2509.00083", "abs": "https://arxiv.org/abs/2509.00083", "authors": ["Laksh Patel", "Neel Shanbhag"], "title": "Data Cartography for Detecting Memorization Hotspots and Guiding Data Interventions in Generative Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "6 pages, 2 figures, 1 table; Presented at the 42nd International\n  Conference on Machine Learning (ICML), winning the \"Best Poster\" award at\n  ICML's workshop for data in generative models (DIG-BUGS)", "summary": "Modern generative models risk overfitting and unintentionally memorizing rare\ntraining examples, which can be extracted by adversaries or inflate benchmark\nperformance. We propose Generative Data Cartography (GenDataCarto), a\ndata-centric framework that assigns each pretraining sample a difficulty score\n(early-epoch loss) and a memorization score (frequency of ``forget events''),\nthen partitions examples into four quadrants to guide targeted pruning and\nup-/down-weighting. We prove that our memorization score lower-bounds classical\ninfluence under smoothness assumptions and that down-weighting\nhigh-memorization hotspots provably decreases the generalization gap via\nuniform stability bounds. Empirically, GenDataCarto reduces synthetic canary\nextraction success by over 40\\% at just 10\\% data pruning, while increasing\nvalidation perplexity by less than 0.5\\%. These results demonstrate that\nprincipled data interventions can dramatically mitigate leakage with minimal\ncost to generative performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGenerative Data Cartography (GenDataCarto)\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u8bad\u7ec3\u6837\u672c\u7684\u96be\u5ea6\u548c\u8bb0\u5fc6\u5ea6\u6765\u6307\u5bfc\u6570\u636e\u5e72\u9884\uff0c\u6709\u6548\u964d\u4f4e\u751f\u6210\u6a21\u578b\u7684\u8bb0\u5fc6\u98ce\u9669\u548c\u6570\u636e\u6cc4\u9732\uff0c\u540c\u65f6\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002", "motivation": "\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u6613\u4e8e\u65e0\u610f\u4e2d\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u7a00\u6709\u6837\u672c\u3002\u8fd9\u4e0d\u4ec5\u53ef\u80fd\u5bfc\u81f4\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u7684\u6570\u636e\u6cc4\u9732\uff0c\u8fd8\u4f1a\u865a\u9ad8\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86Generative Data Cartography (GenDataCarto)\u6846\u67b6\u3002\u8be5\u6846\u67b6\u4e3a\u6bcf\u4e2a\u9884\u8bad\u7ec3\u6837\u672c\u5206\u914d\u4e00\u4e2a\u96be\u5ea6\u5206\u6570\uff08\u901a\u8fc7\u65e9\u671fepoch\u635f\u5931\u8861\u91cf\uff09\u548c\u4e00\u4e2a\u8bb0\u5fc6\u5ea6\u5206\u6570\uff08\u901a\u8fc7\u201c\u9057\u5fd8\u4e8b\u4ef6\u201d\u7684\u9891\u7387\u8861\u91cf\uff09\u3002\u968f\u540e\uff0c\u5c06\u6837\u672c\u5212\u5206\u4e3a\u56db\u4e2a\u8c61\u9650\uff0c\u4ee5\u6307\u5bfc\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u526a\u679d\u3001\u589e\u6743\u6216\u964d\u6743\u3002\u7406\u8bba\u4e0a\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u8bb0\u5fc6\u5ea6\u5206\u6570\u5728\u5e73\u6ed1\u6027\u5047\u8bbe\u4e0b\u662f\u7ecf\u5178\u5f71\u54cd\u7684\u4e0b\u9650\uff0c\u5e76\u4e14\u5bf9\u9ad8\u8bb0\u5fc6\u5ea6\u70ed\u70b9\u8fdb\u884c\u964d\u6743\u80fd\u591f\u901a\u8fc7\u7edf\u4e00\u7a33\u5b9a\u6027\u754c\u9650\uff0c\u53ef\u8bc1\u660e\u5730\u964d\u4f4e\u6cdb\u5316\u5dee\u8ddd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGenDataCarto\u5728\u4ec5\u526a\u679d10%\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u5408\u6210\u91d1\u4e1d\u96c0\uff08synthetic canary\uff09\u63d0\u53d6\u6210\u529f\u7387\u964d\u4f4e\u4e8640%\u4ee5\u4e0a\uff0c\u800c\u9a8c\u8bc1\u96c6\u56f0\u60d1\u5ea6\u4ec5\u589e\u52a0\u4e86\u4e0d\u52300.5%\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u6709\u539f\u5219\u7684\u6570\u636e\u5e72\u9884\u63aa\u65bd\u80fd\u591f\u663e\u8457\u51cf\u8f7b\u751f\u6210\u6a21\u578b\u7684\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u4e14\u5bf9\u6a21\u578b\u7684\u751f\u6210\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002"}}
{"id": "2509.00425", "pdf": "https://arxiv.org/pdf/2509.00425", "abs": "https://arxiv.org/abs/2509.00425", "authors": ["Fenghua Liu", "Yulong Chen", "Yixuan Liu", "Zhujun Jin", "Solomon Tsai", "Ming Zhong"], "title": "The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang", "categories": ["cs.CL"], "comment": "Working in progress", "summary": "Large Language Models (LLMs) achieve gold-medal performance across many\nbenchmarks, yet it remains unclear whether such success reflects genuine\nreasoning or pattern matching. From a cognitive science perspective, an\ninformative test is whether models can master an unfamiliar language through\nexplicit metalinguistic deductive learning, a paradigm where human learners can\nreliably internalise grammatical systems through metalinguistic reasoning. We\naddress this question with Camlang, a novel constructed language that exhibits\nnaturalistic yet unattested feature combinations. Camlang consists of two\nexplicit resources, a grammar book and a bilingual dictionary, which mirror\nadult second-language learning via explicit grammar rules and lexical lookup,\nand enable us to disentangle errors in morpho-syntax, lexical semantics, and\nsentence-level reasoning. Human experiments show that these resources are\nsufficient for participants to acquire Camlang and successfully solve Camlang\ntasks. To operationalise evaluation, we adapt CommonsenseQA into Camlang,\ncreating Camlang-CSQA-v0, the first task in a broader suite where solving\nquestions requires applying grammar rules and lexical mappings. Experimental\nresults show that GPT-5 achieves 98\\% EM accuracy in English but only 47\\% in\nCamlang, far below human performance at 87\\%, while other state-of-the-art\nreasoning LLMs perform even worse. Human verification further reveals that most\nmodel successes stem from shallow lexical alignment while GPT-5 shows emerging\nmetalinguistic awareness to a limited extent but not systematic grammatical\nmastery as humans. Camlang establishes a cognitively grounded evaluation\nparadigm that exposes fundamental gaps between current models and human\nmetalinguistic competence.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLMs) \u5b66\u4e60\u4e00\u79cd\u65b0\u6784\u9020\u8bed\u8a00Camlang\uff0c\u5e76\u8fdb\u884c\u8ba4\u77e5\u79d1\u5b66\u8303\u5f0f\u4e0b\u7684\u5143\u8bed\u8a00\u6f14\u7ece\u5b66\u4e60\u6d4b\u8bd5\u3002\u7ed3\u679c\u663e\u793a\uff0cLLMs\u5728Camlang\u4e0a\u7684\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u8868\u660e\u5176\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u8bed\u6cd5\u638c\u63e1\u548c\u771f\u6b63\u7684\u5143\u8bed\u8a00\u80fd\u529b\uff0c\u800c\u4e3b\u8981\u4f9d\u8d56\u6d45\u5c42\u8bcd\u6c47\u5339\u914d\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u8bb8\u591a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6210\u529f\u662f\u5426\u53cd\u6620\u4e86\u771f\u6b63\u7684\u63a8\u7406\u80fd\u529b\u800c\u975e\u6a21\u5f0f\u5339\u914d\u4ecd\u4e0d\u660e\u786e\u3002\u4ece\u8ba4\u77e5\u79d1\u5b66\u89d2\u5ea6\uff0c\u6d4b\u8bd5\u6a21\u578b\u80fd\u5426\u901a\u8fc7\u5143\u8bed\u8a00\u6f14\u7ece\u5b66\u4e60\u638c\u63e1\u4e00\u95e8\u964c\u751f\u8bed\u8a00\uff0c\u662f\u8861\u91cf\u5176\u80fd\u5426\u5185\u5316\u8bed\u6cd5\u7cfb\u7edf\u7684\u6709\u6548\u65b9\u5f0f\uff0c\u8fd9\u5728\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86Camlang\uff0c\u4e00\u79cd\u5177\u6709\u81ea\u7136\u4f46\u672a\u88ab\u9a8c\u8bc1\u7279\u5f81\u7ec4\u5408\u7684\u65b0\u6784\u9020\u8bed\u8a00\uff0c\u5e76\u4e3a\u5176\u63d0\u4f9b\u8bed\u6cd5\u4e66\u548c\u53cc\u8bed\u8bcd\u5178\uff0c\u6a21\u62df\u4eba\u7c7b\u4e8c\u8bed\u5b66\u4e60\u8fc7\u7a0b\u3002\u901a\u8fc7\u5c06CommonsenseQA\u6539\u7f16\u4e3aCamlang-CSQA-v0\u4efb\u52a1\uff0c\u8bc4\u4f30LLMs\uff08\u5305\u62ecGPT-5\uff09\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u540c\u65f6\uff0c\u8fdb\u884c\u4eba\u7c7b\u5b9e\u9a8c\u5efa\u7acb\u57fa\u7ebf\uff0c\u5e76\u5bf9\u6a21\u578b\u6210\u529f\u6848\u4f8b\u8fdb\u884c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u4ee5\u533a\u5206\u6d45\u5c42\u8bcd\u6c47\u5bf9\u9f50\u548c\u7cfb\u7edf\u6027\u8bed\u6cd5\u638c\u63e1\u3002", "result": "GPT-5\u5728\u82f1\u8bed\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u8fbe98%\uff0c\u4f46\u5728Camlang\u4e2d\u4ec5\u4e3a47%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768487%\u3002\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u63a8\u7406LLMs\u8868\u73b0\u66f4\u5dee\u3002\u4eba\u5de5\u9a8c\u8bc1\u53d1\u73b0\uff0c\u6a21\u578b\u5927\u90e8\u5206\u6210\u529f\u5f52\u56e0\u4e8e\u6d45\u5c42\u8bcd\u6c47\u5bf9\u9f50\uff0cGPT-5\u867d\u663e\u793a\u51fa\u6709\u9650\u7684\u5143\u8bed\u8a00\u610f\u8bc6\uff0c\u4f46\u672a\u5c55\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u5f53\u7684\u7cfb\u7edf\u6027\u8bed\u6cd5\u638c\u63e1\u3002", "conclusion": "Camlang\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8ba4\u77e5\u79d1\u5b66\u57fa\u7840\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLMs\u4e0e\u4eba\u7c7b\u5143\u8bed\u8a00\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u7684\u6839\u672c\u6027\u5dee\u8ddd\u3002LLMs\u5728\u5904\u7406\u9700\u8981\u7cfb\u7edf\u6027\u8bed\u6cd5\u638c\u63e1\u548c\u5143\u8bed\u8a00\u63a8\u7406\u7684\u964c\u751f\u8bed\u8a00\u65f6\uff0c\u4ecd\u8fdc\u672a\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u3002"}}
{"id": "2509.01201", "pdf": "https://arxiv.org/pdf/2509.01201", "abs": "https://arxiv.org/abs/2509.01201", "authors": ["Suhwan Jung", "Seokwoo Choi", "Youngkeun Yoon", "Ho-kyung Son", "Hyoil Kim"], "title": "Modeling and Analysis of Coexistence Between MLO NSTR-based Wi-Fi 7 and Legacy Wi-Fi", "categories": ["cs.NI"], "comment": null, "summary": "Wi-Fi 7 introduces Multi-link operation (MLO) to enhance throughput and\nlatency performance compared to legacy Wi-Fi standards. MLO enables\nsimultaneous transmission and reception through multiple links, departing from\nconventional single-link operations (SLO). To fully exploit MLO's potential, it\nis essential to investigate Wi-Fi 7's coexistence performance with legacy Wi-Fi\ndevices. Existing approaches, however, have overlooked some crucial aspects of\nMLO, necessitating the development of a standards-compliant analytical\nframework to model the actual channel access mechanism of MLO. Therefore, this\npaper tries to fill the gap by proposing a set of novel Markov chains (MC) to\naccurately model the MLO operation aligned with multi-link backoff behaviors\nspecified by the standard. Specifically, we design two separate MCs for AP and\nnon-AP multi-link devices (MLD) respectively, based on which transmit and\ncollision probabilities are derived under the saturated traffic condition.\nThen, we also derive closed-form expressions for the throughput of various\ndevice types in the coexistence scenario between Wi-Fi 7 and legacy Wi-Fi,\nincluding AP MLD, non- AP MLD, and legacy devices. To validate the accuracy of\nour proposed models, we developed an ns-3 based simulator by implementing both\nSTR(simultaneous transmission and reception) and NSTR(non-STR) based MLO\noperations. Our ns-3 based extensive simulations have demonstrated that the\nproposed analytic model provides accurate estimates on the per device\nthroughput performance, while also revealing the dynamics of inter-WLAN\ncoexistence scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u5206\u6790Wi-Fi 7 MLO\u4e0e\u4f20\u7edfWi-Fi\u5171\u5b58\u6027\u80fd\uff0c\u5e76\u901a\u8fc7ns-3\u4eff\u771f\u9a8c\u8bc1\u3002", "motivation": "Wi-Fi 7\u5f15\u5165MLO\u4ee5\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86MLO\u7684\u5173\u952e\u65b9\u9762\uff0c\u7f3a\u4e4f\u7b26\u5408\u6807\u51c6\u7684\u5206\u6790\u6846\u67b6\u6765\u7814\u7a76\u5176\u4e0e\u4f20\u7edfWi-Fi\u8bbe\u5907\u7684\u5171\u5b58\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u5957\u65b0\u9896\u7684\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\uff0c\u5206\u522b\u7528\u4e8e\u5efa\u6a21AP\u548c\u975eAP\u591a\u94fe\u8def\u8bbe\u5907\u7684MLO\u64cd\u4f5c\u53ca\u5176\u591a\u94fe\u8def\u9000\u907f\u884c\u4e3a\u3002\u57fa\u4e8e\u6b64\u63a8\u5bfc\u51fa\u9971\u548c\u6d41\u91cf\u4e0b\u7684\u4f20\u8f93\u548c\u78b0\u649e\u6982\u7387\uff0c\u5e76\u8fdb\u4e00\u6b65\u63a8\u5bfc\u51faWi-Fi 7\u4e0e\u4f20\u7edfWi-Fi\u5171\u5b58\u573a\u666f\u4e0b\u5404\u7c7b\u8bbe\u5907\u7684\u541e\u5410\u91cf\u95ed\u5408\u8868\u8fbe\u5f0f\u3002\u901a\u8fc7\u5f00\u53d1\u57fa\u4e8ens-3\u7684\u6a21\u62df\u5668\uff08\u5b9e\u73b0STR\u548cNSTR MLO\u64cd\u4f5c\uff09\u6765\u9a8c\u8bc1\u6a21\u578b\u51c6\u786e\u6027\u3002", "result": "\u63d0\u51fa\u7684\u5206\u6790\u6a21\u578b\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u6bcf\u8bbe\u5907\u7684\u541e\u5410\u91cf\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86WLAN\u95f4\u5171\u5b58\u573a\u666f\u7684\u52a8\u6001\u7279\u6027\u3002", "conclusion": "\u8be5\u5206\u6790\u6a21\u578b\u80fd\u51c6\u786e\u8bc4\u4f30Wi-Fi 7 MLO\u4e0e\u4f20\u7edfWi-Fi\u7684\u5171\u5b58\u6027\u80fd\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u5176\u52a8\u6001\u4ea4\u4e92\u3002"}}
