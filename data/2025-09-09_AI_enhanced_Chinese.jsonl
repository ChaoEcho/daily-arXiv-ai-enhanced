{"id": "2509.05359", "pdf": "https://arxiv.org/pdf/2509.05359", "abs": "https://arxiv.org/abs/2509.05359", "authors": ["Yanis Labrak", "Richard Dufour", "Micka\u00ebl Rouvier"], "title": "An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Published in International Conference on Text, Speech, and Dialogue,\n  13-24", "summary": "This paper investigates discrete unit representations in Speech Language\nModels (SLMs), focusing on optimizing speech modeling during continual\npre-training. In this paper, we systematically examine how model architecture,\ndata representation, and training robustness influence the pre-training stage\nin which we adapt existing pre-trained language models to the speech modality.\nOur experiments highlight the role of speech encoders and clustering\ngranularity across different model scales, showing how optimal discretization\nstrategies vary with model capacity. By examining cluster distribution and\nphonemic alignments, we investigate the effective use of discrete vocabulary,\nuncovering both linguistic and paralinguistic patterns. Additionally, we\nexplore the impact of clustering data selection on model robustness,\nhighlighting the importance of domain matching between discretization training\nand target applications.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8SLM\u4e2d\u79bb\u6563\u5355\u5143\u8868\u793a\uff0c\u7814\u7a76\u6301\u7eed\u9884\u8bad\u7ec3\u671f\u95f4\u4f18\u5316\u8bed\u97f3\u5efa\u6a21\u7684\u7b56\u7565\uff0c\u5206\u6790\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u8868\u793a\u548c\u8bad\u7ec3\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u5f3a\u8c03\u79bb\u6563\u7b56\u7565\u9700\u4e0e\u6a21\u578b\u5bb9\u91cf\u53ca\u9886\u57df\u5339\u914d\u3002", "motivation": "\u65e8\u5728\u4f18\u5316SLM\u5728\u6301\u7eed\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u8bed\u97f3\u5efa\u6a21\uff0c\u901a\u8fc7\u79bb\u6563\u5355\u5143\u8868\u793a\u5c06\u73b0\u6709\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u9002\u914d\u5230\u8bed\u97f3\u6a21\u6001\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u8003\u5bdf\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u8868\u793a\u548c\u8bad\u7ec3\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u8bed\u97f3\u7f16\u7801\u5668\u3001\u805a\u7c7b\u7c92\u5ea6\uff0c\u68c0\u67e5\u805a\u7c7b\u5206\u5e03\u548c\u97f3\u7d20\u5bf9\u9f50\uff0c\u5e76\u63a2\u8ba8\u805a\u7c7b\u6570\u636e\u9009\u62e9\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u6700\u4f18\u79bb\u6563\u5316\u7b56\u7565\u968f\u6a21\u578b\u5bb9\u91cf\u800c\u5f02\uff1b\u901a\u8fc7\u79bb\u6563\u8bcd\u6c47\u63ed\u793a\u4e86\u8bed\u8a00\u5b66\u548c\u526f\u8bed\u8a00\u5b66\u6a21\u5f0f\uff1b\u5f3a\u8c03\u805a\u7c7b\u6570\u636e\u9009\u62e9\u4e0e\u76ee\u6807\u5e94\u7528\u4e4b\u95f4\u7684\u9886\u57df\u5339\u914d\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u4f18\u5316SLM\u7684\u79bb\u6563\u5355\u5143\u8868\u793a\u548c\u6301\u7eed\u9884\u8bad\u7ec3\u6548\u679c\uff0c\u9700\u7efc\u5408\u8003\u8651\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u8868\u793a\u3001\u8bad\u7ec3\u9c81\u68d2\u6027\uff0c\u5e76\u786e\u4fdd\u79bb\u6563\u5316\u8bad\u7ec3\u4e0e\u76ee\u6807\u5e94\u7528\u7684\u9886\u57df\u5339\u914d\u3002"}}
{"id": "2509.05360", "pdf": "https://arxiv.org/pdf/2509.05360", "abs": "https://arxiv.org/abs/2509.05360", "authors": ["Jerry Li", "Evangelos Papalexakis"], "title": "Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated effectiveness across a wide\nvariety of tasks involving natural language, however, a fundamental problem of\nhallucinations still plagues these models, limiting their trustworthiness in\ngenerating consistent, truthful information. Detecting hallucinations has\nquickly become an important topic, with various methods such as uncertainty\nestimation, LLM Judges, retrieval augmented generation (RAG), and consistency\nchecks showing promise. Many of these methods build upon foundational metrics,\nsuch as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth\nnecessary to detect hallucinations effectively. In this work, we propose a\nnovel approach inspired by ROUGE that constructs an N-Gram frequency tensor\nfrom LLM-generated text. This tensor captures richer semantic structure by\nencoding co-occurrence patterns, enabling better differentiation between\nfactual and hallucinated content. We demonstrate this by applying tensor\ndecomposition methods to extract singular values from each mode and use these\nas input features to train a multi-layer perceptron (MLP) binary classifier for\nhallucinations. Our method is evaluated on the HaluEval dataset and\ndemonstrates significant improvements over traditional baselines, as well as\ncompetitive performance against state-of-the-art LLM judges.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u8bed\u4e49\u6df1\u5ea6\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eN-Gram\u9891\u7387\u5f20\u91cf\u5206\u89e3\u548cMLP\u5206\u7c7b\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u68c0\u6d4bLLM\u5e7b\u89c9\uff0c\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u5e76\u5ab2\u7f8eSOTA LLM\u5224\u522b\u5668\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7b\u89c9\u95ee\u9898\u9650\u5236\u4e86\u5176\u751f\u6210\u4fe1\u606f\u7684\u4e00\u81f4\u6027\u548c\u771f\u5b9e\u6027\u3002\u73b0\u6709\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5982\u57fa\u4e8eROUGE\u3001BERTScore\u7b49\u7684\u6307\u6807\uff0c\u5f80\u5f80\u7f3a\u4e4f\u8db3\u591f\u7684\u8bed\u4e49\u6df1\u5ea6\u6765\u6709\u6548\u533a\u5206\u4e8b\u5b9e\u548c\u5e7b\u89c9\u5185\u5bb9\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u53d7ROUGE\u542f\u53d1\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eceLLM\u751f\u6210\u7684\u6587\u672c\u6784\u5efaN-Gram\u9891\u7387\u5f20\u91cf\uff0c\u6355\u6349\u66f4\u4e30\u5bcc\u7684\u8bed\u4e49\u5171\u73b0\u6a21\u5f0f\u3002\u7136\u540e\u5bf9\u5f20\u91cf\u5e94\u7528\u5206\u89e3\u65b9\u6cd5\u63d0\u53d6\u5947\u5f02\u503c\u4f5c\u4e3a\u7279\u5f81\uff0c\u8bad\u7ec3\u4e00\u4e2a\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u4e8c\u5206\u7c7b\u5668\u6765\u68c0\u6d4b\u5e7b\u89c9\u3002", "result": "\u8be5\u65b9\u6cd5\u5728HaluEval\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4e0e\u4f20\u7edf\u57fa\u7ebf\u76f8\u6bd4\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u4e14\u4e0e\u6700\u5148\u8fdb\u7684LLM\u5224\u522b\u5668\u76f8\u6bd4\u4e5f\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u548c\u5206\u89e3N-Gram\u9891\u7387\u5f20\u91cf\u5e76\u7ed3\u5408MLP\u5206\u7c7b\u5668\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u68c0\u6d4bLLM\u5e7b\u89c9\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8bed\u4e49\u6df1\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u5e7b\u89c9\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.05385", "pdf": "https://arxiv.org/pdf/2509.05385", "abs": "https://arxiv.org/abs/2509.05385", "authors": ["Jiacheng Wei", "Faguo Wu", "Xiao Zhang"], "title": "A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 7 figures, conference", "summary": "Large language models are unable to continuously adapt and learn from new\ndata during reasoning at inference time. To address this limitation, we propose\nthat complex reasoning tasks be decomposed into atomic subtasks and introduce\nSAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive\nupdates during reasoning at inference time. SAGE consists of three key\ncomponents: (1) a Trigger module that detects reasoning failures through\nmultiple evaluation metrics in real time; (2) a Trigger Buffer module that\nclusters anomaly samples using a streaming clustering process with HDBSCAN,\nfollowed by stability checks and similarity-based merging; and (3) a Lora Store\nmodule that dynamically optimizes parameter updates with an adapter pool for\nknowledge retention. Evaluation results show that SAGE demonstrates excellent\naccuracy, robustness, and stability on the atomic reasoning subtask through\ndynamic knowledge updating during test time.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u65f6\u65e0\u6cd5\u6301\u7eed\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86SAGE\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u590d\u6742\u4efb\u52a1\u3001\u5b9e\u65f6\u9519\u8bef\u68c0\u6d4b\u3001\u5f02\u5e38\u6837\u672c\u805a\u7c7b\u53ca\u52a8\u6001\u53c2\u6570\u66f4\u65b0\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u5b66\u4e60\uff0c\u5e76\u5728\u539f\u5b50\u63a8\u7406\u5b50\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u65f6\u65e0\u6cd5\u6301\u7eed\u5730\u9002\u5e94\u65b0\u6570\u636e\u5e76\u4ece\u4e2d\u5b66\u4e60\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u52a8\u6001\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u5c06\u590d\u6742\u63a8\u7406\u4efb\u52a1\u5206\u89e3\u4e3a\u539f\u5b50\u5b50\u4efb\u52a1\uff0c\u5e76\u5f15\u5165SAGE\u6846\u67b6\u3002SAGE\u662f\u4e00\u4e2a\u89e6\u53d1\u5668\u5f15\u5bfc\u7684\u52a8\u6001\u5fae\u8c03\u6846\u67b6\uff0c\u80fd\u591f\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u81ea\u9002\u5e94\u66f4\u65b0\u3002\u5b83\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) Trigger\u6a21\u5757\uff0c\u901a\u8fc7\u591a\u9879\u8bc4\u4f30\u6307\u6807\u5b9e\u65f6\u68c0\u6d4b\u63a8\u7406\u5931\u8d25\uff1b2) Trigger Buffer\u6a21\u5757\uff0c\u5229\u7528HDBSCAN\u8fdb\u884c\u6d41\u5f0f\u805a\u7c7b\u5904\u7406\u5f02\u5e38\u6837\u672c\uff0c\u5e76\u8fdb\u884c\u7a33\u5b9a\u6027\u68c0\u67e5\u548c\u76f8\u4f3c\u6027\u5408\u5e76\uff1b3) Lora Store\u6a21\u5757\uff0c\u901a\u8fc7\u9002\u914d\u5668\u6c60\u52a8\u6001\u4f18\u5316\u53c2\u6570\u66f4\u65b0\u4ee5\u4fdd\u7559\u77e5\u8bc6\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cSAGE\u6846\u67b6\u901a\u8fc7\u5728\u6d4b\u8bd5\u65f6\u7684\u52a8\u6001\u77e5\u8bc6\u66f4\u65b0\uff0c\u5728\u539f\u5b50\u63a8\u7406\u5b50\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "SAGE\u6846\u67b6\u901a\u8fc7\u5b9e\u73b0\u63a8\u7406\u65f6\u7684\u52a8\u6001\u77e5\u8bc6\u66f4\u65b0\uff0c\u6210\u529f\u514b\u670d\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u6301\u7eed\u9002\u5e94\u548c\u5b66\u4e60\u65b0\u6570\u636e\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u539f\u5b50\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2509.05396", "pdf": "https://arxiv.org/pdf/2509.05396", "abs": "https://arxiv.org/abs/2509.05396", "authors": ["Andrea Wynn", "Harsh Satija", "Gillian Hadfield"], "title": "Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "ICML MAS Workshop 2025", "summary": "While multi-agent debate has been proposed as a promising strategy for\nimproving AI reasoning ability, we find that debate can sometimes be harmful\nrather than helpful. The prior work has exclusively focused on debates within\nhomogeneous groups of agents, whereas we explore how diversity in model\ncapabilities influences the dynamics and outcomes of multi-agent interactions.\nThrough a series of experiments, we demonstrate that debate can lead to a\ndecrease in accuracy over time -- even in settings where stronger (i.e., more\ncapable) models outnumber their weaker counterparts. Our analysis reveals that\nmodels frequently shift from correct to incorrect answers in response to peer\nreasoning, favoring agreement over challenging flawed reasoning. These results\nhighlight important failure modes in the exchange of reasons during multi-agent\ndebate, suggesting that naive applications of debate may cause performance\ndegradation when agents are neither incentivized nor adequately equipped to\nresist persuasive but incorrect reasoning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff0c\u7279\u522b\u662f\u5728\u6a21\u578b\u80fd\u529b\u591a\u6837\u5316\u65f6\uff0c\u53ef\u80fd\u5bfc\u81f4AI\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u539f\u56e0\u5728\u4e8e\u667a\u80fd\u4f53\u503e\u5411\u4e8e\u8fbe\u6210\u4e00\u81f4\u800c\u975e\u7ea0\u6b63\u9519\u8bef\u7684\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u540c\u8d28\u667a\u80fd\u4f53\u7fa4\u4f53\u95f4\u7684\u8fa9\u8bba\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7a76\u6a21\u578b\u80fd\u529b\u591a\u6837\u6027\u5982\u4f55\u5f71\u54cd\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u52a8\u6001\u548c\u7ed3\u679c\uff0c\u6311\u6218\u4e86\u8fa9\u8bba\u603b\u662f\u80fd\u63d0\u5347AI\u63a8\u7406\u80fd\u529b\u7684\u5047\u8bbe\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\uff0c\u89c2\u5bdf\u548c\u5206\u6790\u4e86\u5177\u6709\u4e0d\u540c\u6a21\u578b\u80fd\u529b\u7684\u591a\u667a\u80fd\u4f53\u5728\u8fa9\u8bba\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u8868\u73b0\u548c\u7ed3\u679c\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fa9\u8bba\u53ef\u80fd\u5bfc\u81f4\u51c6\u786e\u6027\u968f\u65f6\u95f4\u4e0b\u964d\uff0c\u5373\u4f7f\u5728\u5f3a\u80fd\u529b\u6a21\u578b\u6570\u91cf\u5360\u4f18\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u5206\u6790\u63ed\u793a\uff0c\u6a21\u578b\u5e38\u4ece\u6b63\u786e\u7b54\u6848\u8f6c\u5411\u9519\u8bef\u7b54\u6848\uff0c\u503e\u5411\u4e8e\u8fbe\u6210\u4e00\u81f4\u800c\u975e\u6311\u6218\u9519\u8bef\u7684\u63a8\u7406\u3002", "conclusion": "\u5f53\u667a\u80fd\u4f53\u672a\u88ab\u6fc0\u52b1\u6216\u672a\u5145\u5206\u51c6\u5907\u4ee5\u62b5\u5236\u6709\u8bf4\u670d\u529b\u4f46\u9519\u8bef\u7684\u63a8\u7406\u65f6\uff0c\u5355\u7eaf\u5e94\u7528\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u9000\u5316\uff0c\u8fd9\u63ed\u793a\u4e86\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u539f\u56e0\u4ea4\u6362\u7684\u5173\u952e\u5931\u8d25\u6a21\u5f0f\u3002"}}
{"id": "2509.05447", "pdf": "https://arxiv.org/pdf/2509.05447", "abs": "https://arxiv.org/abs/2509.05447", "authors": ["Zhongyuan Zhao", "Gunjan Verma", "Ananthram Swami", "Santiago Segarra"], "title": "Distributed Link Sparsification for Scalable Scheduling Using Graph Neural Networks (Journal Version)", "categories": ["cs.NI", "cs.DM", "cs.LG", "eess.SP", "05-08", "C.2.1; I.2.8; G.2.2"], "comment": "15 pages, 18 figures, accepted to IEEE Transactions on Wireless\n  Communications. This is the extended journal version of the conference paper\n  arXiv:2203.14339 (Z. Zhao, A. Swami and S. Segarra, \"Distributed Link\n  Sparsification for Scalable Scheduling using Graph Neural Networks,\" IEEE\n  ICASSP 2022, pp. 5308-5312, doi: 10.1109/ICASSP43922.2022.9747437 )", "summary": "In wireless networks characterized by dense connectivity, the significant\nsignaling overhead generated by distributed link scheduling algorithms can\nexacerbate issues like congestion, energy consumption, and radio footprint\nexpansion. To mitigate these challenges, we propose a distributed link\nsparsification scheme employing graph neural networks (GNNs) to reduce\nscheduling overhead for delay-tolerant traffic while maintaining network\ncapacity. A GNN module is trained to adjust contention thresholds for\nindividual links based on traffic statistics and network topology, enabling\nlinks to withdraw from scheduling contention when they are unlikely to succeed.\nOur approach is facilitated by a novel offline constrained {unsupervised}\nlearning algorithm capable of balancing two competing objectives: minimizing\nscheduling overhead while ensuring that total utility meets the required level.\nIn simulated wireless multi-hop networks with up to 500 links, our link\nsparsification technique effectively alleviates network congestion and reduces\nradio footprints across four distinct distributed link scheduling protocols.", "AI": {"tldr": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u5206\u5e03\u5f0f\u94fe\u8def\u7a00\u758f\u5316\u65b9\u6848\uff0c\u65e8\u5728\u51cf\u5c11\u5bc6\u96c6\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u8c03\u5ea6\u5f00\u9500\uff0c\u7f13\u89e3\u62e5\u5835\u5e76\u7f29\u5c0f\u65e0\u7ebf\u7535\u8db3\u8ff9\uff0c\u540c\u65f6\u4fdd\u6301\u7f51\u7edc\u5bb9\u91cf\u3002", "motivation": "\u5728\u5bc6\u96c6\u8fde\u63a5\u7684\u65e0\u7ebf\u7f51\u7edc\u4e2d\uff0c\u5206\u5e03\u5f0f\u94fe\u8def\u8c03\u5ea6\u7b97\u6cd5\u4f1a\u4ea7\u751f\u5927\u91cf\u4fe1\u4ee4\u5f00\u9500\uff0c\u52a0\u5267\u7f51\u7edc\u62e5\u5835\u3001\u80fd\u8017\u548c\u65e0\u7ebf\u7535\u8db3\u8ff9\u6269\u5927\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u5206\u5e03\u5f0f\u94fe\u8def\u7a00\u758f\u5316\u65b9\u6848\uff0c\u901a\u8fc7GNN\u6a21\u5757\u6839\u636e\u6d41\u91cf\u7edf\u8ba1\u548c\u7f51\u7edc\u62d3\u6251\u8c03\u6574\u94fe\u8def\u7ade\u4e89\u9608\u503c\uff0c\u4f7f\u53ef\u80fd\u5931\u8d25\u7684\u94fe\u8def\u9000\u51fa\u8c03\u5ea6\u7ade\u4e89\u3002\u8be5\u65b9\u6848\u91c7\u7528\u4e00\u79cd\u65b0\u9896\u7684\u79bb\u7ebf\u7ea6\u675f\u65e0\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ee5\u5e73\u8861\u6700\u5c0f\u5316\u8c03\u5ea6\u5f00\u9500\u548c\u786e\u4fdd\u603b\u6548\u7528\u8fbe\u5230\u8981\u6c42\u6c34\u5e73\u7684\u76ee\u6807\u3002", "result": "\u5728\u9ad8\u8fbe500\u4e2a\u94fe\u8def\u7684\u6a21\u62df\u65e0\u7ebf\u591a\u8df3\u7f51\u7edc\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u94fe\u8def\u7a00\u758f\u5316\u6280\u672f\u6709\u6548\u7f13\u89e3\u4e86\u7f51\u7edc\u62e5\u5835\uff0c\u5e76\u663e\u8457\u51cf\u5c0f\u4e86\u56db\u79cd\u4e0d\u540c\u5206\u5e03\u5f0f\u94fe\u8def\u8c03\u5ea6\u534f\u8bae\u7684\u65e0\u7ebf\u7535\u8db3\u8ff9\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u57fa\u4e8eGNN\u7684\u5206\u5e03\u5f0f\u94fe\u8def\u7a00\u758f\u5316\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7f51\u7edc\u5bb9\u91cf\u7684\u540c\u65f6\uff0c\u6210\u529f\u5730\u964d\u4f4e\u4e86\u5bc6\u96c6\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5206\u5e03\u5f0f\u94fe\u8def\u8c03\u5ea6\u4ea7\u751f\u7684\u4fe1\u4ee4\u5f00\u9500\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u7f51\u7edc\u62e5\u5835\u548c\u65e0\u7ebf\u7535\u8db3\u8ff9\u95ee\u9898\u3002"}}
{"id": "2509.05307", "pdf": "https://arxiv.org/pdf/2509.05307", "abs": "https://arxiv.org/abs/2509.05307", "authors": ["Sachin Chhabra", "Hemanth Venkateswara", "Baoxin Li"], "title": "Label Smoothing++: Enhanced Label Regularization for Training Neural Networks", "categories": ["cs.CV"], "comment": "Published in British Machine Vision Conference (BMVC), 2024", "summary": "Training neural networks with one-hot target labels often results in\noverconfidence and overfitting. Label smoothing addresses this issue by\nperturbing the one-hot target labels by adding a uniform probability vector to\ncreate a regularized label. Although label smoothing improves the network's\ngeneralization ability, it assigns equal importance to all the non-target\nclasses, which destroys the inter-class relationships. In this paper, we\npropose a novel label regularization training strategy called Label\nSmoothing++, which assigns non-zero probabilities to non-target classes and\naccounts for their inter-class relationships. Our approach uses a fixed label\nfor the target class while enabling the network to learn the labels associated\nwith non-target classes. Through extensive experiments on multiple datasets, we\ndemonstrate how Label Smoothing++ mitigates overconfident predictions while\npromoting inter-class relationships and generalization capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLabel Smoothing++\uff0c\u4e00\u79cd\u65b0\u578b\u6807\u7b7e\u6b63\u5219\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u4e3a\u975e\u76ee\u6807\u7c7b\u5206\u914d\u975e\u96f6\u6982\u7387\u5e76\u8003\u8651\u5176\u7c7b\u95f4\u5173\u7cfb\uff0c\u4ee5\u51cf\u8f7b\u8fc7\u81ea\u4fe1\u548c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f7f\u7528\u72ec\u70ed\u7f16\u7801\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5e38\u5bfc\u81f4\u8fc7\u81ea\u4fe1\u548c\u8fc7\u62df\u5408\u3002\u6807\u7b7e\u5e73\u6ed1\u867d\u80fd\u6539\u5584\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5bf9\u6240\u6709\u975e\u76ee\u6807\u7c7b\u8d4b\u4e88\u540c\u7b49\u91cd\u8981\u6027\uff0c\u7834\u574f\u4e86\u7c7b\u95f4\u5173\u7cfb\u3002", "method": "\u63d0\u51faLabel Smoothing++\u3002\u8be5\u65b9\u6cd5\u4e3a\u975e\u76ee\u6807\u7c7b\u5206\u914d\u975e\u96f6\u6982\u7387\uff0c\u5e76\u8003\u8651\u5176\u7c7b\u95f4\u5173\u7cfb\u3002\u76ee\u6807\u7c7b\u6807\u7b7e\u56fa\u5b9a\uff0c\u800c\u975e\u76ee\u6807\u7c7b\u7684\u6807\u7b7e\u5219\u7531\u7f51\u7edc\u5b66\u4e60\u3002", "result": "\u901a\u8fc7\u591a\u6570\u636e\u96c6\u5b9e\u9a8c\u8bc1\u660e\uff0cLabel Smoothing++\u80fd\u6709\u6548\u7f13\u89e3\u8fc7\u81ea\u4fe1\u9884\u6d4b\uff0c\u4fc3\u8fdb\u7c7b\u95f4\u5173\u7cfb\u7684\u5efa\u7acb\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Label Smoothing++\u662f\u4e00\u79cd\u6709\u6548\u7684\u6807\u7b7e\u6b63\u5219\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u5b83\u901a\u8fc7\u5728\u975e\u76ee\u6807\u7c7b\u4e2d\u8003\u8651\u7c7b\u95f4\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6807\u7b7e\u5e73\u6ed1\u7684\u5c40\u9650\u6027\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u5e76\u51cf\u8f7b\u4e86\u8fc7\u81ea\u4fe1\u3002"}}
{"id": "2509.05323", "pdf": "https://arxiv.org/pdf/2509.05323", "abs": "https://arxiv.org/abs/2509.05323", "authors": ["Adam Cole", "Mick Grierson"], "title": "Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts", "categories": ["cs.AI", "cs.MM"], "comment": "3rd international workshop on eXplainable AI for the Arts (XAIxArts)\n  at the ACM Creativity and Cognition Conference 2025", "summary": "This paper presents an artistic and technical investigation into the\nattention mechanisms of video diffusion transformers. Inspired by early video\nartists who manipulated analog video signals to create new visual aesthetics,\nthis study proposes a method for extracting and visualizing cross-attention\nmaps in generative video models. Built on the open-source Wan model, our tool\nprovides an interpretable window into the temporal and spatial behavior of\nattention in text-to-video generation. Through exploratory probes and an\nartistic case study, we examine the potential of attention maps as both\nanalytical tools and raw artistic material. This work contributes to the\ngrowing field of Explainable AI for the Arts (XAIxArts), inviting artists to\nreclaim the inner workings of AI as a creative medium.", "AI": {"tldr": "\u672c\u6587\u827a\u672f\u6027\u4e0e\u6280\u672f\u6027\u5730\u7814\u7a76\u4e86\u89c6\u9891\u6269\u6563Transformer\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u53d6\u548c\u53ef\u89c6\u5316\u751f\u6210\u89c6\u9891\u6a21\u578b\u4e2d\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\u7684\u65b9\u6cd5\uff0c\u5e76\u4ee5\u6ce8\u610f\u529b\u56fe\u4f5c\u4e3a\u5206\u6790\u5de5\u5177\u548c\u827a\u672f\u6750\u6599\uff0c\u4e3a\u827a\u672f\u9886\u57df\u7684\u53ef\u89e3\u91caAI\u505a\u8d21\u732e\u3002", "motivation": "\u53d7\u65e9\u671f\u89c6\u9891\u827a\u672f\u5bb6\u901a\u8fc7\u64cd\u7eb5\u6a21\u62df\u4fe1\u53f7\u521b\u9020\u65b0\u7f8e\u5b66\u7684\u542f\u53d1\uff0c\u65e8\u5728\u63ed\u793a\u548c\u5229\u7528\u751f\u6210\u5f0f\u89c6\u9891\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e3a\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u57fa\u4e8e\u5f00\u6e90Wan\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u5de5\u5177\u6765\u63d0\u53d6\u548c\u53ef\u89c6\u5316\u751f\u6210\u89c6\u9891\u6a21\u578b\u4e2d\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\uff0c\u5e76\u901a\u8fc7\u63a2\u7d22\u6027\u63a2\u7a76\u548c\u827a\u672f\u6848\u4f8b\u7814\u7a76\u6765\u8bc4\u4f30\u6ce8\u610f\u529b\u56fe\u7684\u6f5c\u529b\u3002", "result": "\u6240\u5f00\u53d1\u7684\u5de5\u5177\u4e3a\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89c6\u89d2\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6ce8\u610f\u529b\u56fe\u4f5c\u4e3a\u5206\u6790\u5de5\u5177\u548c\u539f\u59cb\u827a\u672f\u6750\u6599\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u827a\u672f\u9886\u57df\u7684\u53ef\u89e3\u91caAI (XAIxArts) \u505a\u51fa\u4e86\u8d21\u732e\uff0c\u5e76\u9f13\u52b1\u827a\u672f\u5bb6\u5c06AI\u7684\u5185\u90e8\u8fd0\u4f5c\u673a\u5236\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e00\u79cd\u521b\u9020\u6027\u5a92\u4ecb\u3002"}}
{"id": "2509.05316", "pdf": "https://arxiv.org/pdf/2509.05316", "abs": "https://arxiv.org/abs/2509.05316", "authors": ["Praveen Bushipaka", "Lucia Passaro", "Tommaso Cucinotta"], "title": "Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A conventional LLM Unlearning setting consists of two subsets -\"forget\" and\n\"retain\", with the objectives of removing the undesired knowledge from the\nforget set while preserving the remaining knowledge from the retain. In\nprivacy-focused unlearning research, a retain set is often further divided into\nneighbor sets, containing either directly or indirectly connected to the forget\ntargets; and augmented by a general-knowledge set. A common practice in\nexisting benchmarks is to employ only a single neighbor set, with general\nknowledge which fails to reflect the real-world data complexities and\nrelationships. LLM Unlearning typically involves 1:1 sampling or cyclic\niteration sampling. However, the efficacy and stability of these de facto\nstandards have not been critically examined. In this study, we systematically\nevaluate these common practices. Our findings reveal that relying on a single\nneighbor set is suboptimal and that a standard sampling approach can obscure\nperformance trade-offs. Based on this analysis, we propose and validate an\ninitial set of best practices: (1) Incorporation of diverse neighbor sets to\nbalance forget efficacy and model utility, (2) Standard 1:1 sampling methods\nare inefficient and yield poor results, (3) Our proposed Modular Entity-Level\nUnlearning (MELU) strategy as an alternative to cyclic sampling. We demonstrate\nthat this modular approach, combined with robust algorithms, provides a clear\nand stable path towards effective unlearning.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86LLM\u9057\u5fd8\u4e2d\u7684\u5e38\u89c1\u505a\u6cd5\uff0c\u53d1\u73b0\u5355\u4e00\u90bb\u5c45\u96c6\u548c\u6807\u51c6\u91c7\u6837\u65b9\u6cd5\uff08\u59821:1\u91c7\u6837\uff09\u7684\u5c40\u9650\u6027\u3002\u7814\u7a76\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u5957\u6700\u4f73\u5b9e\u8df5\uff0c\u5305\u62ec\u5f15\u5165\u591a\u6837\u5316\u7684\u90bb\u5c45\u96c6\u548c\u65b0\u7684\u6a21\u5757\u5316\u5b9e\u4f53\u7ea7\u9057\u5fd8\uff08MELU\uff09\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u548c\u7a33\u5b9a\u7684\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709LLM\u9057\u5fd8\u57fa\u51c6\u5b9e\u8df5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u4ec5\u4f7f\u7528\u5355\u4e00\u90bb\u5c45\u96c6\u672a\u80fd\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u6570\u636e\u590d\u6742\u6027\uff0c\u4ee5\u53ca\u5e38\u7528\u91c7\u6837\u65b9\u6cd5\uff08\u59821:1\u91c7\u6837\u6216\u5faa\u73af\u8fed\u4ee3\u91c7\u6837\uff09\u7684\u6709\u6548\u6027\u548c\u7a33\u5b9a\u6027\u5c1a\u672a\u88ab\u7cfb\u7edf\u6027\u68c0\u9a8c\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u52a8\u673a\u662f\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u8fd9\u4e9b\u5e38\u89c1\u505a\u6cd5\uff0c\u4ee5\u53d1\u73b0\u5176\u4e0d\u8db3\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u3002", "method": "\u672c\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86LLM\u9057\u5fd8\u4e2d\u7684\u5e38\u89c1\u5b9e\u8df5\uff0c\u5305\u62ec\u90bb\u5c45\u96c6\u7684\u8bbe\u7f6e\u591a\u6837\u6027\u548c\u91c7\u6837\u65b9\u6cd5\uff08\u59821:1\u91c7\u6837\u548c\u5faa\u73af\u8fed\u4ee3\u91c7\u6837\uff09\u3002\u57fa\u4e8e\u8bc4\u4f30\u7ed3\u679c\uff0c\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u5957\u521d\u59cb\u6700\u4f73\u5b9e\u8df5\uff0c\u5177\u4f53\u5305\u62ec\uff1a\u6574\u5408\u591a\u6837\u5316\u7684\u90bb\u5c45\u96c6\u3001\u6307\u51fa\u6807\u51c61:1\u91c7\u6837\u65b9\u6cd5\u7684\u4f4e\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u6a21\u5757\u5316\u5b9e\u4f53\u7ea7\u9057\u5fd8\uff08MELU\uff09\u7b56\u7565\u4f5c\u4e3a\u5faa\u73af\u91c7\u6837\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4ec5\u4f9d\u8d56\u5355\u4e00\u90bb\u5c45\u96c6\u662f\u6b21\u4f18\u7684\uff0c\u4e14\u6807\u51c6\u76841:1\u91c7\u6837\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u5e76\u53ef\u80fd\u5bfc\u81f4\u4e0d\u826f\u7ed3\u679c\uff0c\u540c\u65f6\u63a9\u76d6\u4e86\u6027\u80fd\u6743\u8861\u3002\u8fd9\u4e9b\u4f20\u7edf\u505a\u6cd5\u672a\u80fd\u63d0\u4f9b\u6e05\u6670\u7a33\u5b9a\u7684\u9057\u5fd8\u8def\u5f84\u3002", "conclusion": "\u7ed3\u5408\u591a\u6837\u5316\u7684\u90bb\u5c45\u96c6\u548c\u6240\u63d0\u51fa\u7684\u6a21\u5757\u5316\u5b9e\u4f53\u7ea7\u9057\u5fd8\uff08MELU\uff09\u7b56\u7565\uff0c\u8f85\u4ee5\u5f3a\u5927\u7684\u7b97\u6cd5\uff0c\u80fd\u591f\u4e3a\u5b9e\u73b0\u6709\u6548\u4e14\u7a33\u5b9a\u7684LLM\u9057\u5fd8\u63d0\u4f9b\u6e05\u6670\u8def\u5f84\u3002\u8fd9\u4e9b\u6700\u4f73\u5b9e\u8df5\u6709\u52a9\u4e8e\u5e73\u8861\u9057\u5fd8\u6548\u7387\u548c\u6a21\u578b\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.05425", "pdf": "https://arxiv.org/pdf/2509.05425", "abs": "https://arxiv.org/abs/2509.05425", "authors": ["Jessica M. Lundin", "Ada Zhang", "David Adelani", "Cody Carroll"], "title": "No Translation Needed: Forecasting Quality from Fertility and Metadata", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We show that translation quality can be predicted with surprising accuracy\n\\textit{without ever running the translation system itself}. Using only a\nhandful of features, token fertility ratios, token counts, and basic linguistic\nmetadata (language family, script, and region), we can forecast ChrF scores for\nGPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient\nboosting models achieve favorable performance ($R^{2}=0.66$ for\nXX$\\rightarrow$English and $R^{2}=0.72$ for English$\\rightarrow$XX). Feature\nimportance analyses reveal that typological factors dominate predictions into\nEnglish, while fertility plays a larger role for translations into diverse\ntarget languages. These findings suggest that translation quality is shaped by\nboth token-level fertility and broader linguistic typology, offering new\ninsights for multilingual evaluation and quality estimation.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u65e0\u9700\u5b9e\u9645\u8fd0\u884c\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u4ec5\u4f7f\u7528\u5c11\u91cf\u7279\u5f81\uff08\u8bcd\u5143\u751f\u80b2\u7387\u3001\u8bcd\u5143\u8ba1\u6570\u548c\u8bed\u8a00\u5143\u6570\u636e\uff09\u5373\u53ef\u51c6\u786e\u9884\u6d4bGPT-4o\u5728203\u79cd\u8bed\u8a00\u4e0a\u7684\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u5728\u4e0d\u8fd0\u884c\u7ffb\u8bd1\u7cfb\u7edf\u672c\u8eab\u7684\u60c5\u51b5\u4e0b\uff0c\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u9884\u6d4b\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4ee5\u63d0\u4f9b\u591a\u8bed\u8a00\u8bc4\u4f30\u548c\u8d28\u91cf\u4f30\u8ba1\u7684\u65b0\u89c1\u89e3\u3002", "method": "\u4f7f\u7528\u8bcd\u5143\u751f\u80b2\u7387\u3001\u8bcd\u5143\u8ba1\u6570\u4ee5\u53ca\u8bed\u7cfb\u3001\u6587\u5b57\u548c\u5730\u57df\u7b49\u57fa\u672c\u8bed\u8a00\u5143\u6570\u636e\u4f5c\u4e3a\u7279\u5f81\u3002\u91c7\u7528\u68af\u5ea6\u63d0\u5347\u6a21\u578b\uff08Gradient Boosting models\uff09\u9884\u6d4bGPT-4o\u5728FLORES-200\u57fa\u51c6\u6d4b\u8bd5\u4e2d203\u79cd\u8bed\u8a00\u7684ChrF\u5206\u6570\u3002", "result": "\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0cXX\u2192\u82f1\u8bed\u7ffb\u8bd1\u7684$R^{2}=0.66$\uff0c\u82f1\u8bed\u2192XX\u7ffb\u8bd1\u7684$R^{2}=0.72$\u3002\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u663e\u793a\uff0c\u5728\u7ffb\u8bd1\u6210\u82f1\u8bed\u65f6\uff0c\u7c7b\u578b\u5b66\u56e0\u7d20\u5360\u4e3b\u5bfc\uff1b\u800c\u5728\u7ffb\u8bd1\u6210\u5176\u4ed6\u591a\u6837\u76ee\u6807\u8bed\u8a00\u65f6\uff0c\u751f\u80b2\u7387\u53d1\u6325\u66f4\u5927\u4f5c\u7528\u3002", "conclusion": "\u7ffb\u8bd1\u8d28\u91cf\u53ef\u7531\u8bcd\u5143\u7ea7\u522b\u751f\u80b2\u7387\u548c\u66f4\u5e7f\u6cdb\u7684\u8bed\u8a00\u7c7b\u578b\u5b66\u5171\u540c\u5851\u9020\uff0c\u4e14\u4ec5\u901a\u8fc7\u5c11\u91cf\u7279\u5f81\u5373\u53ef\u6709\u6548\u9884\u6d4b\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u591a\u8bed\u8a00\u8bc4\u4f30\u548c\u8d28\u91cf\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2509.05467", "pdf": "https://arxiv.org/pdf/2509.05467", "abs": "https://arxiv.org/abs/2509.05467", "authors": ["Reshma Prasad", "Maxime Elkael", "Gabriele Gemmi", "Osama M. Bushnaq", "Debashisha Mishra", "Prasanna Raut", "Jennifer Simonjan", "Michele Polese", "Tommaso Melodia"], "title": "Joint Routing, Resource Allocation, and Energy Optimization for Integrated Access and Backhaul with Open RAN", "categories": ["cs.NI"], "comment": null, "summary": "As networks evolve towards 6G, Mobile Network Operators (MNOs) must\naccommodate diverse requirements and at the same time manage rising energy\nconsumption. Integrated Access and Backhaul (IAB) networks facilitate dense\ncellular deployments with reduced infrastructure complexity. However, the\nmulti-hop wireless backhauling in IAB networks necessitates proper routing and\nresource allocation decisions to meet the performance requirements. At the same\ntime, cell densification makes energy optimization crucial. This paper\naddresses the joint optimization of routing and resource allocation in IAB\nnetworks through two distinct objectives: energy minimization and throughput\nmaximization. We develop a novel capacity model that links power levels to\nachievable data rates. We propose two practical large-scale approaches to solve\nthe optimization problems and leverage the closed-loop control framework\nintroduced by the Open Radio Access Network (O-RAN) architecture to integrate\nthe solutions. The approaches are evaluated on diverse scenarios built upon\nopen data of two months of traffic collected by network operators in the city\nof Milan, Italy. Results show that the proposed approaches effectively reduces\nnumber of activated nodes to save energy and achieves approximately 100 Mbps of\nminimum data rate per User Equipment (UE) during peak hours of the day using\nspectrum within the Frequency Range (FR) 3, or upper midband. The results\nvalidate the practical applicability of our framework for next-generation IAB\nnetwork deployment and optimization.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u57286G IAB\u7f51\u7edc\u4e2d\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8def\u7531\u4e0e\u8d44\u6e90\u5206\u914d\uff0c\u5b9e\u73b0\u80fd\u8017\u6700\u5c0f\u5316\u548c\u541e\u5410\u91cf\u6700\u5927\u5316\u7684\u4e24\u79cd\u5b9e\u7528\u65b9\u6cd5\uff0c\u5e76\u5229\u7528O-RAN\u67b6\u6784\u8fdb\u884c\u96c6\u6210\uff0c\u4ee5\u5e94\u5bf9\u672a\u6765\u7f51\u7edc\u6311\u6218\u3002", "motivation": "\u968f\u7740\u7f51\u7edc\u54116G\u6f14\u8fdb\uff0c\u79fb\u52a8\u7f51\u7edc\u8fd0\u8425\u5546\u9762\u4e34\u6ee1\u8db3\u591a\u6837\u5316\u9700\u6c42\u548c\u7ba1\u7406\u65e5\u76ca\u589e\u957f\u80fd\u8017\u7684\u53cc\u91cd\u6311\u6218\u3002\u96c6\u6210\u63a5\u5165\u548c\u56de\u4f20\uff08IAB\uff09\u7f51\u7edc\u867d\u80fd\u964d\u4f4e\u90e8\u7f72\u590d\u6742\u6027\uff0c\u4f46\u5176\u591a\u8df3\u65e0\u7ebf\u56de\u4f20\u7279\u6027\u8981\u6c42\u7cbe\u786e\u7684\u8def\u7531\u548c\u8d44\u6e90\u5206\u914d\u51b3\u7b56\uff0c\u540c\u65f6\u5c0f\u533a\u81f4\u5bc6\u5316\u4f7f\u5f97\u80fd\u8017\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u529f\u7387\u6c34\u5e73\u4e0e\u53ef\u5b9e\u73b0\u6570\u636e\u901f\u7387\u76f8\u5173\u8054\u7684\u65b0\u578b\u5bb9\u91cf\u6a21\u578b\u3002\u5f00\u53d1\u4e86\u4e24\u79cd\u5b9e\u7528\u7684\u5927\u89c4\u6a21\u65b9\u6cd5\uff0c\u7528\u4e8e\u8054\u5408\u4f18\u5316IAB\u7f51\u7edc\u4e2d\u7684\u8def\u7531\u548c\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u76ee\u6807\u5206\u522b\u4e3a\u80fd\u8017\u6700\u5c0f\u5316\u548c\u541e\u5410\u91cf\u6700\u5927\u5316\u3002\u8fd9\u4e9b\u89e3\u51b3\u65b9\u6848\u5229\u7528Open Radio Access Network (O-RAN) \u67b6\u6784\u7684\u95ed\u73af\u63a7\u5236\u6846\u67b6\u8fdb\u884c\u96c6\u6210\uff0c\u5e76\u4f7f\u7528\u610f\u5927\u5229\u7c73\u5170\u5e02\u4e24\u4e2a\u6708\u4ea4\u901a\u6d41\u91cf\u7684\u5f00\u653e\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u6fc0\u6d3b\u8282\u70b9\u6570\u91cf\u4ee5\u8282\u7ea6\u80fd\u6e90\uff0c\u5e76\u5728\u65e5\u9ad8\u5cf0\u65f6\u6bb5\u4f7f\u7528FR3\uff08\u6216\u4e0a\u4e2d\u9891\u6bb5\uff09\u9891\u8c31\u65f6\uff0c\u4e3a\u6bcf\u4e2a\u7528\u6237\u8bbe\u5907\uff08UE\uff09\u5b9e\u73b0\u4e86\u7ea6100 Mbps\u7684\u6700\u5c0f\u6570\u636e\u901f\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u4e0b\u4e00\u4ee3IAB\u7f51\u7edc\u90e8\u7f72\u548c\u4f18\u5316\u4e2d\u7684\u5b9e\u9645\u9002\u7528\u6027\u3002"}}
{"id": "2509.05317", "pdf": "https://arxiv.org/pdf/2509.05317", "abs": "https://arxiv.org/abs/2509.05317", "authors": ["Isac Holm"], "title": "VILOD: A Visual Interactive Labeling Tool for Object Detection", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": "Master's project", "summary": "The advancement of Object Detection (OD) using Deep Learning (DL) is often\nhindered by the significant challenge of acquiring large, accurately labeled\ndatasets, a process that is time-consuming and expensive. While techniques like\nActive Learning (AL) can reduce annotation effort by intelligently querying\ninformative samples, they often lack transparency, limit the strategic insight\nof human experts, and may overlook informative samples not aligned with an\nemployed query strategy. To mitigate these issues, Human-in-the-Loop (HITL)\napproaches integrating human intelligence and intuition throughout the machine\nlearning life-cycle have gained traction. Leveraging Visual Analytics (VA),\neffective interfaces can be created to facilitate this human-AI collaboration.\nThis thesis explores the intersection of these fields by developing and\ninvestigating \"VILOD: A Visual Interactive Labeling tool for Object Detection\".\nVILOD utilizes components such as a t-SNE projection of image features,\ntogether with uncertainty heatmaps and model state views. Enabling users to\nexplore data, interpret model states, AL suggestions, and implement diverse\nsample selection strategies within an iterative HITL workflow for OD. An\nempirical investigation using comparative use cases demonstrated how VILOD,\nthrough its interactive visualizations, facilitates the implementation of\ndistinct labeling strategies by making the model's state and dataset\ncharacteristics more interpretable (RQ1). The study showed that different\nvisually-guided labeling strategies employed within VILOD result in competitive\nOD performance trajectories compared to an automated uncertainty sampling AL\nbaseline (RQ2). This work contributes a novel tool and empirical insight into\nmaking the HITL-AL workflow for OD annotation more transparent, manageable, and\npotentially more effective.", "AI": {"tldr": "\u9488\u5bf9\u76ee\u6807\u68c0\u6d4b\uff08OD\uff09\u6807\u6ce8\u6570\u636e\u96c6\u8017\u65f6\u8017\u529b\u4e14\u4f20\u7edf\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u900f\u660e\u5ea6\u4f4e\u7b49\u95ee\u9898\uff0c\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u53ef\u89c6\u5316\u4ea4\u4e92\u5f0f\u6807\u6ce8\u5de5\u5177VILOD\u3002VILOD\u901a\u8fc7\u53ef\u89c6\u5316\u5206\u6790\u589e\u5f3a\u4eba\u673a\u534f\u4f5c\uff0c\u4f7f\u6807\u6ce8\u6d41\u7a0b\u66f4\u900f\u660e\u3001\u53ef\u63a7\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4e0e\u81ea\u52a8\u5316\u57fa\u7ebf\u76f8\u5f53\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\uff08OD\uff09\u9762\u4e34\u5927\u89c4\u6a21\u3001\u51c6\u786e\u6807\u6ce8\u6570\u636e\u96c6\u83b7\u53d6\u56f0\u96be\u4e14\u6602\u8d35\u7684\u6311\u6218\u3002\u73b0\u6709\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u65b9\u6cd5\u867d\u7136\u80fd\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u9650\u5236\u4e86\u4e13\u5bb6\u6d1e\u5bdf\u529b\uff0c\u4e14\u53ef\u80fd\u5ffd\u7565\u672a\u88ab\u67e5\u8be2\u7b56\u7565\u8986\u76d6\u7684\u4fe1\u606f\u6837\u672c\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5c06\u4eba\u7c7b\u667a\u80fd\u4e0e\u673a\u5668\u5b66\u4e60\u751f\u547d\u5468\u671f\u7ed3\u5408\u7684\u4eba\u673a\u534f\u4f5c\uff08HITL\uff09\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u7ed3\u5408\u53ef\u89c6\u5316\u5206\u6790\uff08VA\uff09\uff0c\u6210\u4e3a\u6709\u6548\u9014\u5f84\u3002", "method": "\u5f00\u53d1\u5e76\u7814\u7a76\u4e86\u201cVILOD\uff1a\u4e00\u4e2a\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\u7684\u53ef\u89c6\u5316\u4ea4\u4e92\u5f0f\u6807\u6ce8\u5de5\u5177\u201d\u3002VILOD\u96c6\u6210\u4e86\u56fe\u50cf\u7279\u5f81\u7684t-SNE\u6295\u5f71\u3001\u4e0d\u786e\u5b9a\u6027\u70ed\u56fe\u548c\u6a21\u578b\u72b6\u6001\u89c6\u56fe\u7b49\u7ec4\u4ef6\u3002\u5b83\u4f7f\u7528\u6237\u80fd\u591f\u5728\u8fed\u4ee3\u7684HITL\u5de5\u4f5c\u6d41\u4e2d\u63a2\u7d22\u6570\u636e\u3001\u89e3\u91ca\u6a21\u578b\u72b6\u6001\u548cAL\u5efa\u8bae\uff0c\u5e76\u5b9e\u65bd\u591a\u6837\u5316\u7684\u6837\u672c\u9009\u62e9\u7b56\u7565\u3002", "result": "1. \u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0cVILOD\u901a\u8fc7\u5176\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u72b6\u6001\u548c\u6570\u636e\u96c6\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u4e0d\u540c\u6807\u6ce8\u7b56\u7565\u7684\u5b9e\u65bd\u30022. \u5728VILOD\u4e2d\u91c7\u7528\u7684\u4e0d\u540c\u89c6\u89c9\u5f15\u5bfc\u6807\u6ce8\u7b56\u7565\uff0c\u4e0e\u81ea\u52a8\u5316\u4e0d\u786e\u5b9a\u6027\u91c7\u6837AL\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u8f68\u8ff9\u3002", "conclusion": "\u672c\u5de5\u4f5c\u8d21\u732e\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u5de5\u5177\uff08VILOD\uff09\u548c\u5b9e\u8bc1\u89c1\u89e3\uff0c\u4f7f\u5f97\u76ee\u6807\u68c0\u6d4b\u6807\u6ce8\u7684\u4eba\u673a\u534f\u4f5c\u4e3b\u52a8\u5b66\u4e60\uff08HITL-AL\uff09\u5de5\u4f5c\u6d41\u66f4\u52a0\u900f\u660e\u3001\u6613\u4e8e\u7ba1\u7406\uff0c\u5e76\u53ef\u80fd\u66f4\u6709\u6548\u3002"}}
{"id": "2509.05324", "pdf": "https://arxiv.org/pdf/2509.05324", "abs": "https://arxiv.org/abs/2509.05324", "authors": ["Rongqian Chen", "Shu Hong", "Rifatul Islam", "Mahdi Imani", "G. Gary Tan", "Tian Lan"], "title": "Perception Graph for Cognitive Attack Reasoning in Augmented Reality", "categories": ["cs.AI"], "comment": "Accepted by ACM MobiHoc XR Security workshop 2025", "summary": "Augmented reality (AR) systems are increasingly deployed in tactical\nenvironments, but their reliance on seamless human-computer interaction makes\nthem vulnerable to cognitive attacks that manipulate a user's perception and\nseverely compromise user decision-making. To address this challenge, we\nintroduce the Perception Graph, a novel model designed to reason about human\nperception within these systems. Our model operates by first mimicking the\nhuman process of interpreting key information from an MR environment and then\nrepresenting the outcomes using a semantically meaningful structure. We\ndemonstrate how the model can compute a quantitative score that reflects the\nlevel of perception distortion, providing a robust and measurable method for\ndetecting and analyzing the effects of such cognitive attacks.", "AI": {"tldr": "AR\u7cfb\u7edf\u5728\u6218\u672f\u73af\u5883\u4e2d\u6613\u53d7\u8ba4\u77e5\u653b\u51fb\uff0c\u5f71\u54cd\u7528\u6237\u611f\u77e5\u548c\u51b3\u7b56\u3002\u672c\u6587\u63d0\u51faPerception Graph\u6a21\u578b\uff0c\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u626d\u66f2\u6765\u68c0\u6d4b\u548c\u5206\u6790\u6b64\u7c7b\u653b\u51fb\u3002", "motivation": "\u6218\u672f\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u7cfb\u7edf\uff0c\u56e0\u5176\u5bf9\u65e0\u7f1d\u4eba\u673a\u4ea4\u4e92\u7684\u4f9d\u8d56\u6027\uff0c\u6613\u53d7\u64cd\u7eb5\u7528\u6237\u611f\u77e5\u5e76\u4e25\u91cd\u635f\u5bb3\u51b3\u7b56\u7684\u8ba4\u77e5\u653b\u51fb\u3002", "method": "\u5f15\u5165Perception Graph\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u9996\u5148\u6a21\u4eff\u4eba\u7c7b\u4eceMR\u73af\u5883\u4e2d\u89e3\u91ca\u5173\u952e\u4fe1\u606f\u7684\u6d41\u7a0b\uff0c\u7136\u540e\u4f7f\u7528\u8bed\u4e49\u6709\u610f\u4e49\u7684\u7ed3\u6784\u8868\u793a\u7ed3\u679c\uff0c\u4ece\u800c\u5bf9\u7cfb\u7edf\u5185\u7684\u4eba\u7c7b\u611f\u77e5\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u8be5\u6a21\u578b\u80fd\u591f\u8ba1\u7b97\u51fa\u53cd\u6620\u611f\u77e5\u626d\u66f2\u7a0b\u5ea6\u7684\u91cf\u5316\u5206\u6570\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u4e14\u53ef\u8861\u91cf\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u5206\u6790\u6b64\u7c7b\u8ba4\u77e5\u653b\u51fb\u7684\u5f71\u54cd\u3002", "conclusion": "Perception Graph\u6a21\u578b\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u626d\u66f2\uff0c\u6709\u6548\u5e94\u5bf9\u4e86AR\u7cfb\u7edf\u5728\u6218\u672f\u73af\u5883\u4e2d\u9762\u4e34\u7684\u8ba4\u77e5\u653b\u51fb\u6311\u6218\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u7cfb\u7edf\u62b5\u5fa1\u6b64\u7c7b\u5a01\u80c1\u7684\u80fd\u529b\u3002"}}
{"id": "2509.05328", "pdf": "https://arxiv.org/pdf/2509.05328", "abs": "https://arxiv.org/abs/2509.05328", "authors": ["Xiang Yuan", "Jun Shu", "Deyu meng", "Zongben Xu"], "title": "Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Robust fine-tuning aims to achieve competitive in-distribution (ID)\nperformance while maintaining the out-of-distribution (OOD) robustness of a\npre-trained model when transferring it to a downstream task. To remedy this,\nmost robust fine-tuning methods aim to preserve the pretrained weights,\nfeatures, or logits. However, we find that these methods cannot always improve\nOOD robustness for different model architectures. This is due to the OOD\nrobustness requiring the model function to produce stable prediction for input\ninformation of downstream tasks, while existing methods might serve as a poor\nproxy for the optimization in the function space. Based on this finding, we\npropose a novel regularization that constrains the distance of fine-tuning and\npre-trained model in the function space with the simulated OOD samples, aiming\nto preserve the OOD robustness of the pre-trained model. Besides, to further\nenhance the OOD robustness capability of the fine-tuning model, we introduce an\nadditional consistency regularization to promote stable predictions of\nperturbed samples. Extensive experiments demonstrate our approach could\nconsistently improve both downstream task ID fine-tuning performance and OOD\nrobustness across a variety of CLIP backbones, outperforming existing\nregularization-based robust fine-tuning methods.", "AI": {"tldr": "\u9488\u5bf9\u9c81\u68d2\u5fae\u8c03\u4e2d\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u4e00\u81f4\u63d0\u5347OOD\u9c81\u68d2\u6027\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u5728\u51fd\u6570\u7a7a\u95f4\u4e2d\u6b63\u5219\u5316\u5fae\u8c03\u6a21\u578b\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u8ddd\u79bb\uff0c\u5e76\u7ed3\u5408\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86ID\u6027\u80fd\u548cOOD\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u9c81\u68d2\u5fae\u8c03\u65b9\u6cd5\u5728\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u8fc1\u79fb\u5230\u4e0b\u6e38\u4efb\u52a1\u65f6\uff0c\u96be\u4ee5\u5728\u4fdd\u6301\u7ade\u4e89\u6027ID\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u63d0\u5347OOD\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u3002\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3a\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5728\u51fd\u6570\u7a7a\u95f4\u4e2d\u4f18\u5316\u6a21\u578b\u529f\u80fd\u4ee5\u5b9e\u73b0\u5bf9\u4e0b\u6e38\u4efb\u52a1\u8f93\u5165\u4fe1\u606f\u7684\u7a33\u5b9a\u9884\u6d4b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62dfOOD\u6837\u672c\uff0c\u5728\u51fd\u6570\u7a7a\u95f4\u4e2d\u7ea6\u675f\u5fae\u8c03\u6a21\u578b\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u4ee5\u4fdd\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u7684OOD\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e00\u81f4\u6027\u6b63\u5219\u5316\u4ee5\u4fc3\u8fdb\u5bf9\u6270\u52a8\u6837\u672c\u7684\u7a33\u5b9a\u9884\u6d4b\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3aOOD\u9c81\u68d2\u6027\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6301\u7eed\u6539\u8fdb\u4e0b\u6e38\u4efb\u52a1\u7684ID\u5fae\u8c03\u6027\u80fd\u548cOOD\u9c81\u68d2\u6027\uff0c\u5728\u591a\u79cdCLIP\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u9c81\u68d2\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5728\u51fd\u6570\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6b63\u5219\u5316\u5e76\u7ed3\u5408\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u672c\u6587\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u9c81\u68d2\u5fae\u8c03\u65b9\u6cd5\u5728\u63d0\u5347OOD\u9c81\u68d2\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u8bc1ID\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684OOD\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.05440", "pdf": "https://arxiv.org/pdf/2509.05440", "abs": "https://arxiv.org/abs/2509.05440", "authors": ["Logan Lawrence", "Ashton Williamson", "Alexander Shelton"], "title": "Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 18 tables, 1 figure", "summary": "As large-language models have been increasingly used as automatic raters for\nevaluating free-form content, including document summarization, dialog, and\nstory generation, work has been dedicated to evaluating such models by\nmeasuring their correlations with human judgment. For \\textit{sample-level}\nperformance, methods which operate by using pairwise comparisons between\nmachine-generated text perform well but often lack the ability to assign\nabsolute scores to individual summaries, an ability crucial for use cases that\nrequire thresholding. In this work, we propose a direct-scoring method which\nuses synthetic summaries to act as pairwise machine rankings at test time. We\nshow that our method performs comparably to state-of-the-art pairwise\nevaluators in terms of axis-averaged sample-level correlations on the SummEval\n(\\textbf{+0.03}), TopicalChat (\\textbf{-0.03}), and HANNA (\\textbf{+0.05})\nmeta-evaluation benchmarks, and release the synthetic in-context summaries as\ndata to facilitate future work.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u8bc4\u5206\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u6458\u8981\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u4e3a\u5355\u4e2a\u81ea\u7531\u5f62\u5f0f\u5185\u5bb9\u5206\u914d\u7edd\u5bf9\u5206\u6570\uff0c\u5176\u6027\u80fd\u4e0e\u5148\u8fdb\u7684\u914d\u5bf9\u8bc4\u4f30\u5668\u76f8\u5f53\u3002", "motivation": "\u73b0\u6709\u914d\u5bf9\u8bc4\u4f30\u65b9\u6cd5\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u5206\u5668\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u65e0\u6cd5\u4e3a\u5355\u4e2a\u6587\u672c\u5206\u914d\u7edd\u5bf9\u5206\u6570\uff0c\u800c\u8fd9\u5bf9\u4e8e\u9700\u8981\u8bbe\u5b9a\u9608\u503c\u7684\u5e94\u7528\u573a\u666f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u8bc4\u5206\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u5229\u7528\u5408\u6210\u6458\u8981\u4f5c\u4e3a\u914d\u5bf9\u673a\u5668\u6392\u540d\u3002", "result": "\u8be5\u65b9\u6cd5\u5728SummEval\u3001TopicalChat\u548cHANNA\u7b49\u5143\u8bc4\u4f30\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8f74\u5e73\u5747\u6837\u672c\u7ea7\u76f8\u5173\u6027\u65b9\u9762\u4e0e\u6700\u5148\u8fdb\u7684\u914d\u5bf9\u8bc4\u4f30\u5668\u8868\u73b0\u76f8\u5f53\uff08\u5206\u522b\u6709+0.03\uff0c-0.03\uff0c+0.05\u7684\u53d8\u5316\uff09\uff0c\u5e76\u53d1\u5e03\u4e86\u5408\u6210\u8bed\u5883\u6458\u8981\u6570\u636e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u76f4\u63a5\u8bc4\u5206\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u914d\u5bf9\u8bc4\u4f30\u5668\u65e0\u6cd5\u63d0\u4f9b\u7edd\u5bf9\u5206\u6570\u7684\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4e0e\u9876\u5c16\u65b9\u6cd5\u76f8\u5f53\uff0c\u6709\u52a9\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2509.05759", "pdf": "https://arxiv.org/pdf/2509.05759", "abs": "https://arxiv.org/abs/2509.05759", "authors": ["Jinkun Geng", "Shuai Mu", "Anirudh Sivaraman", "Balaji Prabhakar"], "title": "Tiga: Accelerating Geo-Distributed Transactions with Synchronized Clocks [Technical Report]", "categories": ["cs.NI", "cs.DB", "cs.DC", "68M10, 68M15"], "comment": "This is the technical report for our paper accepted by The 31st\n  Symposium on Operating Systems Principles (SOSP'25)", "summary": "This paper presents Tiga, a new design for geo-replicated and scalable\ntransactional databases such as Google Spanner. Tiga aims to commit\ntransactions within 1 wide-area roundtrip time, or 1 WRTT, for a wide range of\nscenarios, while maintaining high throughput with minimal computational\noverhead. Tiga consolidates concurrency control and consensus, completing both\nstrictly serializable execution and consistent replication in a single round.\nIt uses synchronized clocks to proactively order transactions by assigning each\na future timestamp at submission. In most cases, transactions arrive at servers\nbefore their future timestamps and are serialized according to the designated\ntimestamp, requiring 1 WRTT to commit. In rare cases, transactions are delayed\nand proactive ordering fails, in which case Tiga falls back to a slow path,\ncommitting in 1.5--2 WRTTs. Compared to state-of-the-art solutions, Tiga can\ncommit more transactions at 1-WRTT latency, and incurs much less throughput\noverhead. Evaluation results show that Tiga outperforms all baselines,\nachieving 1.3--7.2$\\times$ higher throughput and 1.4--4.6$\\times$ lower\nlatency. Tiga is open-sourced at\nhttps://github.com/New-Consensus-Concurrency-Control/Tiga.", "AI": {"tldr": "Tiga\u662f\u4e00\u79cd\u4e3a\u5730\u7406\u590d\u5236\u548c\u53ef\u4f38\u7f29\u4e8b\u52a1\u6570\u636e\u5e93\u8bbe\u8ba1\u7684\u65b0\u65b9\u6848\uff0c\u65e8\u5728\u4ee51 WRTT\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\u91cf\u63d0\u4ea4\u4e8b\u52a1\u3002", "motivation": "\u73b0\u6709\u5730\u7406\u590d\u5236\u4e8b\u52a1\u6570\u636e\u5e93\u5728\u591a\u79cd\u573a\u666f\u4e0b\u96be\u4ee5\u5b9e\u73b01 WRTT\u7684\u4f4e\u5ef6\u8fdf\u4e8b\u52a1\u63d0\u4ea4\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u541e\u5410\u91cf\u548c\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u3002", "method": "Tiga\u6574\u5408\u4e86\u5e76\u53d1\u63a7\u5236\u548c\u5171\u8bc6\u673a\u5236\uff0c\u5728\u4e00\u4e2a\u8f6e\u6b21\u5185\u5b8c\u6210\u4e25\u683c\u53ef\u5e8f\u5217\u5316\u6267\u884c\u548c\u4e00\u81f4\u6027\u590d\u5236\u3002\u5b83\u5229\u7528\u540c\u6b65\u65f6\u949f\u4e3a\u4e8b\u52a1\u9884\u5148\u5206\u914d\u672a\u6765\u65f6\u95f4\u6233\u8fdb\u884c\u6392\u5e8f\uff0c\u901a\u5e38\u53ef\u5b9e\u73b01 WRTT\u63d0\u4ea4\uff1b\u5728\u5c11\u6570\u5f02\u5e38\u60c5\u51b5\u4e0b\uff0c\u4f1a\u56de\u9000\u52301.5-2 WRTT\u7684\u6162\u8def\u5f84\u3002", "result": "Tiga\u57281 WRTT\u5ef6\u8fdf\u4e0b\u80fd\u63d0\u4ea4\u66f4\u591a\u4e8b\u52a1\uff0c\u4e14\u541e\u5410\u91cf\u5f00\u9500\u66f4\u4f4e\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTiga\u7684\u541e\u5410\u91cf\u6bd4\u57fa\u7ebf\u9ad81.3-7.2\u500d\uff0c\u5ef6\u8fdf\u4f4e1.4-4.6\u500d\u3002", "conclusion": "Tiga\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5730\u7406\u590d\u5236\u4e8b\u52a1\u6570\u636e\u5e93\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u5728\u5e7f\u6cdb\u573a\u666f\u4e0b\u76841 WRTT\u4f4e\u5ef6\u8fdf\u4e8b\u52a1\u63d0\u4ea4\u548c\u9ad8\u541e\u5410\u91cf\u3002"}}
{"id": "2509.05319", "pdf": "https://arxiv.org/pdf/2509.05319", "abs": "https://arxiv.org/abs/2509.05319", "authors": ["Zhengda Li"], "title": "Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Knowledge distillation (KD) is a widely used technique to transfer knowledge\nfrom a large teacher network to a smaller student model. Traditional KD uses a\nfixed balancing factor alpha as a hyperparameter to combine the hard-label\ncross-entropy loss with the soft-label distillation loss. However, a static\nalpha is suboptimal because the optimal trade-off between hard and soft\nsupervision can vary during training.\n  In this work, we propose an Adaptive Knowledge Distillation (AKD) framework.\nFirst we try to make alpha as learnable parameter that can be automatically\nlearned and optimized during training. Then we introduce a formula to reflect\nthe gap between the student and the teacher to compute alpha dynamically,\nguided by student-teacher discrepancies, and further introduce a Context-Aware\nModule (CAM) using MLP + Attention to adaptively reweight class-wise teacher\noutputs. Experiments on CIFAR-10 with ResNet-50 as teacher and ResNet-18 as\nstudent demonstrate that our approach achieves superior accuracy compared to\nfixed-weight KD baselines, and yields more stable convergence.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u77e5\u8bc6\u84b8\u998f (AKD) \u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u6216\u52a8\u6001\u8ba1\u7b97\u5e73\u8861\u56e0\u5b50\u03b1\uff0c\u5e76\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u5757\uff0c\u4ee5\u4f18\u5316\u5b66\u751f\u6a21\u578b\u6027\u80fd\u548c\u6536\u655b\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u4e2d\uff0c\u56fa\u5b9a\u4e0d\u53d8\u7684\u5e73\u8861\u56e0\u5b50\u03b1\u662f\u6b21\u4f18\u7684\uff0c\u56e0\u4e3a\u786c\u6807\u7b7e\u548c\u8f6f\u6807\u7b7e\u76d1\u7763\u4e4b\u95f4\u7684\u6700\u4f73\u6743\u8861\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u662f\u53d8\u5316\u7684\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u77e5\u8bc6\u84b8\u998f (AKD) \u6846\u67b6\u3002\u9996\u5148\uff0c\u5c06\u03b1\u8bbe\u4e3a\u53ef\u5b66\u4e60\u53c2\u6570\uff1b\u5176\u6b21\uff0c\u5f15\u5165\u516c\u5f0f\u6839\u636e\u5e08\u751f\u6a21\u578b\u5dee\u8ddd\u52a8\u6001\u8ba1\u7b97\u03b1\uff1b\u6700\u540e\uff0c\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u5757 (CAM) (\u4f7f\u7528MLP+Attention) \u6765\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u52a0\u6743\u7c7b\u522b\u7ea7\u7684\u6559\u5e08\u8f93\u51fa\u3002", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\uff0c\u4ee5ResNet-50\u4e3a\u6559\u5e08\u6a21\u578b\u3001ResNet-18\u4e3a\u5b66\u751f\u6a21\u578b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAKD\u65b9\u6cd5\u76f8\u6bd4\u56fa\u5b9a\u6743\u91cd\u7684KD\u57fa\u7ebf\u53d6\u5f97\u4e86\u66f4\u4f18\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5177\u6709\u66f4\u7a33\u5b9a\u7684\u6536\u655b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u77e5\u8bc6\u84b8\u998f (AKD) \u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5e73\u8861\u56e0\u5b50\u548c\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u5757\uff0c\u6709\u6548\u63d0\u5347\u4e86\u77e5\u8bc6\u84b8\u998f\u7684\u6548\u679c\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u6536\u655b\u3002"}}
{"id": "2509.05325", "pdf": "https://arxiv.org/pdf/2509.05325", "abs": "https://arxiv.org/abs/2509.05325", "authors": ["Liming Xu", "Yunbo Long", "Alexandra Brintrup"], "title": "SynDelay: A Synthetic Dataset for Delivery Delay Prediction", "categories": ["cs.AI"], "comment": "This paper incldues 1 figure and 2 tables", "summary": "Artificial intelligence (AI) is transforming supply chain management, yet\nprogress in predictive tasks -- such as delivery delay prediction -- remains\nconstrained by the scarcity of high-quality, openly available datasets.\nExisting datasets are often proprietary, small, or inconsistently maintained,\nhindering reproducibility and benchmarking. We present SynDelay, a synthetic\ndataset designed for delivery delay prediction. Generated using an advanced\ngenerative model trained on real-world data, SynDelay preserves realistic\ndelivery patterns while ensuring privacy. Although not entirely free of noise\nor inconsistencies, it provides a challenging and practical testbed for\nadvancing predictive modelling. To support adoption, we provide baseline\nresults and evaluation metrics as initial benchmarks, serving as reference\npoints rather than state-of-the-art claims. SynDelay is publicly available\nthrough the Supply Chain Data Hub, an open initiative promoting dataset sharing\nand benchmarking in supply chain AI. We encourage the community to contribute\ndatasets, models, and evaluation practices to advance research in this area.\nAll code is openly accessible at https://supplychaindatahub.org.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSynDelay\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u4f9b\u5e94\u94feAI\u4e2d\u4ea4\u4ed8\u5ef6\u8fdf\u9884\u6d4b\u4efb\u52a1\u9ad8\u8d28\u91cf\u5f00\u653e\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u7ebf\u7ed3\u679c\u548c\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u5728\u4f9b\u5e94\u94fe\u7ba1\u7406\u4e2d\u7684\u9884\u6d4b\u4efb\u52a1\uff08\u5982\u4ea4\u4ed8\u5ef6\u8fdf\u9884\u6d4b\uff09\u56e0\u9ad8\u8d28\u91cf\u3001\u5f00\u653e\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u800c\u53d7\u9650\u3002\u73b0\u6709\u6570\u636e\u96c6\u591a\u4e3a\u4e13\u6709\u3001\u5c0f\u578b\u6216\u7ef4\u62a4\u4e0d\u4e00\u81f4\uff0c\u4ece\u800c\u963b\u788d\u4e86\u7814\u7a76\u7684\u590d\u73b0\u6027\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86SynDelay\uff0c\u4e00\u4e2a\u4e13\u4e3a\u4ea4\u4ed8\u5ef6\u8fdf\u9884\u6d4b\u8bbe\u8ba1\u7684\u5408\u6210\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u4f7f\u7528\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u5148\u8fdb\u751f\u6210\u6a21\u578b\u751f\u6210\uff0c\u5728\u4fdd\u7559\u771f\u5b9e\u4ea4\u4ed8\u6a21\u5f0f\u7684\u540c\u65f6\u786e\u4fdd\u4e86\u9690\u79c1\u3002", "result": "SynDelay\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u548c\u5b9e\u7528\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u63a8\u8fdb\u9884\u6d4b\u5efa\u6a21\u3002\u4e3a\u4e86\u652f\u6301\u6570\u636e\u96c6\u7684\u91c7\u7528\uff0c\u4f5c\u8005\u63d0\u4f9b\u4e86\u57fa\u7ebf\u7ed3\u679c\u548c\u8bc4\u4f30\u6307\u6807\u4f5c\u4e3a\u521d\u59cb\u57fa\u51c6\u3002", "conclusion": "SynDelay\u6570\u636e\u96c6\u5df2\u901a\u8fc7Supply Chain Data Hub\u516c\u5f00\u53d1\u5e03\uff0c\u65e8\u5728\u4fc3\u8fdb\u4f9b\u5e94\u94feAI\u9886\u57df\u7684\u6570\u636e\u96c6\u5171\u4eab\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002\u8bba\u6587\u9f13\u52b1\u793e\u533a\u8d21\u732e\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u8bc4\u4f30\u5b9e\u8df5\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2509.05429", "pdf": "https://arxiv.org/pdf/2509.05429", "abs": "https://arxiv.org/abs/2509.05429", "authors": ["Jie Fu", "Hong Yuan", "Zhili Chen", "Wendy Hui Wang"], "title": "Safeguarding Graph Neural Networks against Topology Inference Attacks", "categories": ["cs.LG", "cs.CR"], "comment": "Acctepted by ACM CCS'25", "summary": "Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, their widespread adoption has raised\nserious privacy concerns. While prior research has primarily focused on\nedge-level privacy, a critical yet underexplored threat lies in topology\nprivacy - the confidentiality of the graph's overall structure. In this work,\nwe present a comprehensive study on topology privacy risks in GNNs, revealing\ntheir vulnerability to graph-level inference attacks. To this end, we propose a\nsuite of Topology Inference Attacks (TIAs) that can reconstruct the structure\nof a target training graph using only black-box access to a GNN model. Our\nfindings show that GNNs are highly susceptible to these attacks, and that\nexisting edge-level differential privacy mechanisms are insufficient as they\neither fail to mitigate the risk or severely compromise model accuracy. To\naddress this challenge, we introduce Private Graph Reconstruction (PGR), a\nnovel defense framework designed to protect topology privacy while maintaining\nmodel accuracy. PGR is formulated as a bi-level optimization problem, where a\nsynthetic training graph is iteratively generated using meta-gradients, and the\nGNN model is concurrently updated based on the evolving graph. Extensive\nexperiments demonstrate that PGR significantly reduces topology leakage with\nminimal impact on model accuracy. Our code is anonymously available at\nhttps://github.com/JeffffffFu/PGR.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u62d3\u6251\u9690\u79c1\u65b9\u9762\u7684\u6f0f\u6d1e\uff0c\u63d0\u51fa\u4e86\u62d3\u6251\u63a8\u65ad\u653b\u51fb\uff08TIAs\uff09\u6765\u91cd\u5efa\u56fe\u7ed3\u6784\uff0c\u5e76\u9488\u5bf9\u73b0\u6709\u9632\u5fa1\u673a\u5236\u7684\u4e0d\u8db3\uff0c\u5f15\u5165\u4e86\u540d\u4e3a\u79c1\u6709\u56fe\u91cd\u5efa\uff08PGR\uff09\u7684\u65b0\u578b\u9632\u5fa1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u5728\u4fdd\u62a4\u62d3\u6251\u9690\u79c1\u7684\u540c\u65f6\u7ef4\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u5b66\u4e60\u56fe\u7ed3\u6784\u6570\u636e\u65b9\u9762\u8868\u73b0\u5f3a\u5927\uff0c\u4f46\u5176\u5e7f\u6cdb\u5e94\u7528\u5f15\u53d1\u4e86\u4e25\u91cd\u7684\u9690\u79c1\u62c5\u5fe7\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8fb9\u7ea7\u9690\u79c1\uff0c\u800c\u56fe\u7684\u6574\u4f53\u7ed3\u6784\uff08\u5373\u62d3\u6251\u9690\u79c1\uff09\u4f5c\u4e3a\u4e00\u4e2a\u5173\u952e\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u5a01\u80c1\uff0c\u4ecd\u9762\u4e34\u88ab\u6cc4\u9732\u7684\u98ce\u9669\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u5957\u62d3\u6251\u63a8\u65ad\u653b\u51fb\uff08TIAs\uff09\uff0c\u901a\u8fc7\u4ec5\u9ed1\u76d2\u8bbf\u95eeGNN\u6a21\u578b\u5373\u53ef\u91cd\u5efa\u76ee\u6807\u8bad\u7ec3\u56fe\u7684\u7ed3\u6784\u30022. \u5f15\u5165\u4e86\u79c1\u6709\u56fe\u91cd\u5efa\uff08PGR\uff09\u9632\u5fa1\u6846\u67b6\uff0c\u65e8\u5728\u4fdd\u62a4\u62d3\u6251\u9690\u79c1\u5e76\u7ef4\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002PGR\u88ab\u6784\u5efa\u4e3a\u4e00\u4e2a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u5176\u4e2d\u5229\u7528\u5143\u68af\u5ea6\u8fed\u4ee3\u751f\u6210\u5408\u6210\u8bad\u7ec3\u56fe\uff0c\u5e76\u540c\u6b65\u66f4\u65b0GNN\u6a21\u578b\u3002", "result": "1. \u5b9e\u9a8c\u53d1\u73b0GNNs\u6781\u6613\u53d7\u5230\u62d3\u6251\u63a8\u65ad\u653b\u51fb\u30022. \u73b0\u6709\u7684\u8fb9\u7ea7\u5dee\u5206\u9690\u79c1\u673a\u5236\u4e0d\u8db3\u4ee5\u7f13\u89e3\u6b64\u7c7b\u98ce\u9669\uff0c\u56e0\u4e3a\u5b83\u4eec\u8981\u4e48\u65e0\u6cd5\u6709\u6548\u9632\u5fa1\uff0c\u8981\u4e48\u4e25\u91cd\u635f\u5bb3\u6a21\u578b\u51c6\u786e\u6027\u30023. \u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cPGR\u80fd\u591f\u663e\u8457\u51cf\u5c11\u62d3\u6251\u4fe1\u606f\u6cc4\u9732\uff0c\u540c\u65f6\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "GNNs\u5bf9\u62d3\u6251\u63a8\u65ad\u653b\u51fb\u9ad8\u5ea6\u654f\u611f\uff0c\u4e14\u73b0\u6709\u8fb9\u7ea7\u5dee\u5206\u9690\u79c1\u673a\u5236\u4e0d\u8db3\u4ee5\u6709\u6548\u4fdd\u62a4\u62d3\u6251\u9690\u79c1\u3002\u672c\u6587\u63d0\u51fa\u7684\u79c1\u6709\u56fe\u91cd\u5efa\uff08PGR\uff09\u6846\u67b6\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b0\u578b\u9632\u5fa1\u673a\u5236\uff0c\u80fd\u591f\u5728\u4fdd\u62a4GNN\u62d3\u6251\u9690\u79c1\u7684\u540c\u65f6\uff0c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002"}}
{"id": "2509.05484", "pdf": "https://arxiv.org/pdf/2509.05484", "abs": "https://arxiv.org/abs/2509.05484", "authors": ["Hajar Sakai", "Yi-En Tseng", "Mohammadsadegh Mikaeili", "Joshua Bosire", "Franziska Jovin"], "title": "From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics", "categories": ["cs.CL"], "comment": null, "summary": "Hospital call centers serve as the primary contact point for patients within\na hospital system. They also generate substantial volumes of staff messages as\nnavigators process patient requests and communicate with the hospital offices\nfollowing the established protocol restrictions and guidelines. This\ncontinuously accumulated large amount of text data can be mined and processed\nto retrieve insights; however, traditional supervised learning approaches\nrequire annotated data, extensive training, and model tuning. Large Language\nModels (LLMs) offer a paradigm shift toward more computationally efficient\nmethodologies for healthcare analytics. This paper presents a multi-stage\nLLM-based framework that identifies staff message topics and classifies\nmessages by their reasons in a multi-class fashion. In the process, multiple\nLLM types, including reasoning, general-purpose, and lightweight models, were\nevaluated. The best-performing model was o3, achieving 78.4% weighted F1-score\nand 79.2% accuracy, followed closely by gpt-5 (75.3% Weighted F1-score and\n76.2% accuracy). The proposed methodology incorporates data security measures\nand HIPAA compliance requirements essential for healthcare environments. The\nprocessed LLM outputs are integrated into a visualization decision support tool\nthat transforms the staff messages into actionable insights accessible to\nhealthcare professionals. This approach enables more efficient utilization of\nthe collected staff messaging data, identifies navigator training\nopportunities, and supports improved patient experience and care quality.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u9636\u6bb5\u7684LLM\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u533b\u9662\u547c\u53eb\u4e2d\u5fc3\u5458\u5de5\u6d88\u606f\u7684\u4e3b\u9898\u5e76\u8fdb\u884c\u5206\u7c7b\uff0c\u901a\u8fc7\u8bc4\u4f30\u591a\u79cdLLM\u7c7b\u578b\uff0c\u6700\u4f73\u6a21\u578bo3\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u652f\u6301\u6570\u636e\u5b89\u5168\u548c\u53ef\u89c6\u5316\u51b3\u7b56\uff0c\u4ee5\u63d0\u5347\u533b\u7597\u670d\u52a1\u8d28\u91cf\u3002", "motivation": "\u533b\u9662\u547c\u53eb\u4e2d\u5fc3\u4ea7\u751f\u5927\u91cf\u5458\u5de5\u6d88\u606f\u6587\u672c\u6570\u636e\uff0c\u4f46\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u548c\u8bad\u7ec3\uff0c\u6548\u7387\u4f4e\u4e0b\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u533b\u7597\u5206\u6790\u63d0\u4f9b\u4e86\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u65b0\u8303\u5f0f\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528LLMs\u5904\u7406\u8fd9\u4e9b\u6570\u636e\u4ee5\u83b7\u53d6\u53ef\u64cd\u4f5c\u7684\u6d1e\u5bdf\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u591a\u9636\u6bb5\u7684\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u5458\u5de5\u6d88\u606f\u4e3b\u9898\u5e76\u6309\u539f\u56e0\u8fdb\u884c\u591a\u7c7b\u522b\u5206\u7c7b\u3002\u8be5\u65b9\u6cd5\u8bc4\u4f30\u4e86\u5305\u62ec\u63a8\u7406\u578b\u3001\u901a\u7528\u578b\u548c\u8f7b\u91cf\u7ea7\u5728\u5185\u7684\u591a\u79cdLLM\u6a21\u578b\u3002\u6846\u67b6\u4e2d\u878d\u5165\u4e86\u6570\u636e\u5b89\u5168\u63aa\u65bd\u548cHIPAA\u5408\u89c4\u6027\u8981\u6c42\uff0c\u5e76\u5c06\u5904\u7406\u540e\u7684LLM\u8f93\u51fa\u96c6\u6210\u5230\u53ef\u89c6\u5316\u51b3\u7b56\u652f\u6301\u5de5\u5177\u4e2d\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u6700\u4f73\u8868\u73b0\u6a21\u578b\u662fo3\uff0c\u5176\u52a0\u6743F1-score\u8fbe\u523078.4%\uff0c\u51c6\u786e\u7387\u8fbe\u523079.2%\u3002\u7d27\u968f\u5176\u540e\u7684\u662fgpt-5\uff0c\u52a0\u6743F1-score\u4e3a75.3%\uff0c\u51c6\u786e\u7387\u4e3a76.2%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u6536\u96c6\u5230\u7684\u5458\u5de5\u6d88\u606f\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u5bfc\u822a\u5458\u7684\u57f9\u8bad\u673a\u4f1a\uff0c\u4ece\u800c\u652f\u6301\u6539\u5584\u60a3\u8005\u4f53\u9a8c\u548c\u63d0\u5347\u62a4\u7406\u8d28\u91cf\u3002"}}
{"id": "2509.05889", "pdf": "https://arxiv.org/pdf/2509.05889", "abs": "https://arxiv.org/abs/2509.05889", "authors": ["Mahsa Paknejad", "Parisa Fard Moshiri", "Murat Simsek", "Burak Kantarci", "Hussein T. Mouftah"], "title": "On-Dyn-CDA: A Real-Time Cost-Driven Task Offloading Algorithm for Vehicular Networks with Reduced Latency and Task Loss", "categories": ["cs.NI"], "comment": "12 pages, 9 figures, accepted to IEEE Internet of Things Journal", "summary": "Real-time task processing is a critical challenge in vehicular networks,\nwhere achieving low latency and minimizing dropped task ratio depend on\nefficient task execution. Our primary objective is to maximize the number of\ncompleted tasks while minimizing overall latency, with a particular focus on\nreducing number of dropped tasks. To this end, we investigate both static and\ndynamic versions of an optimization algorithm. The static version assumes full\ntask availability, while the dynamic version manages tasks as they arrive. We\nalso distinguish between online and offline cases: the online version\nincorporates execution time into the offloading decision process, whereas the\noffline version excludes it, serving as a theoretical benchmark for optimal\nperformance. We evaluate our proposed Online Dynamic Cost-Driven Algorithm\n(On-Dyn-CDA) against these baselines. Notably, the static Particle Swarm\nOptimization (PSO) baseline assumes all tasks are transferred to the RSU and\nprocessed by the MEC, and its offline version disregards execution time, making\nit infeasible for real-time applications despite its optimal performance in\ntheory. Our novel On-Dyn-CDA completes execution in just 0.05 seconds under the\nmost complex scenario, compared to 1330.05 seconds required by Dynamic PSO. It\nalso outperforms Dynamic PSO by 3.42% in task loss and achieves a 29.22%\nreduction in average latency in complex scenarios. Furthermore, it requires\nneither a dataset nor a training phase, and its low computational complexity\nensures efficiency and scalability in dynamic environments.", "AI": {"tldr": "\u9488\u5bf9\u8f66\u8f86\u7f51\u7edc\u5b9e\u65f6\u4efb\u52a1\u5904\u7406\uff0c\u63d0\u51faOn-Dyn-CDA\u7b97\u6cd5\uff0c\u5728\u590d\u6742\u573a\u666f\u4e0b\u663e\u8457\u964d\u4f4e\u4efb\u52a1\u5ef6\u8fdf\u548c\u4e22\u5931\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\uff0c\u9ad8\u6548\u5b9e\u7528\u3002", "motivation": "\u8f66\u8f86\u7f51\u7edc\u5b9e\u65f6\u4efb\u52a1\u5904\u7406\u9762\u4e34\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u4efb\u52a1\u4e22\u5f03\u7387\u7684\u6311\u6218\u3002\u7814\u7a76\u76ee\u6807\u662f\u6700\u5927\u5316\u5b8c\u6210\u4efb\u52a1\u6570\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u603b\u5ef6\u8fdf\u5e76\u51cf\u5c11\u4efb\u52a1\u4e22\u5f03\u3002", "method": "\u63d0\u51fa\u5728\u7ebf\u52a8\u6001\u6210\u672c\u9a71\u52a8\u7b97\u6cd5 (On-Dyn-CDA)\u3002\u901a\u8fc7\u8003\u8651\u4efb\u52a1\u53ef\u7528\u6027\uff08\u9759\u6001/\u52a8\u6001\uff09\u548c\u5378\u8f7d\u51b3\u7b56\uff08\u5728\u7ebf/\u79bb\u7ebf\uff0c\u5728\u7ebf\u7248\u8003\u8651\u6267\u884c\u65f6\u95f4\uff09\u5bf9\u7b97\u6cd5\u8fdb\u884c\u8bbe\u8ba1\u548c\u8bc4\u4f30\u3002\u5bf9\u6bd4\u4e86\u5305\u62ec\u9759\u6001PSO\u5728\u5185\u7684\u591a\u79cd\u57fa\u7ebf\u7b97\u6cd5\u3002", "result": "\u5728\u6700\u590d\u6742\u573a\u666f\u4e0b\uff0cOn-Dyn-CDA\u6267\u884c\u65f6\u95f4\u4ec50.05\u79d2\uff0c\u8fdc\u4f18\u4e8e\u52a8\u6001PSO\u76841330.05\u79d2\u3002\u4efb\u52a1\u4e22\u5931\u7387\u4f18\u4e8e\u52a8\u6001PSO 3.42%\uff0c\u5e73\u5747\u5ef6\u8fdf\u964d\u4f4e29.22%\u3002\u8be5\u7b97\u6cd5\u65e0\u9700\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u3002", "conclusion": "On-Dyn-CDA\u7b97\u6cd5\u5728\u8f66\u8f86\u7f51\u7edc\u5b9e\u65f6\u4efb\u52a1\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u5e76\u964d\u4f4e\u4e86\u4efb\u52a1\u4e22\u5931\u548c\u5ef6\u8fdf\u3002\u5176\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u65e0\u9700\u8bad\u7ec3\u7684\u7279\u70b9\uff0c\u4f7f\u5176\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5177\u6709\u9ad8\u5ea6\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.05321", "pdf": "https://arxiv.org/pdf/2509.05321", "abs": "https://arxiv.org/abs/2509.05321", "authors": ["Yunfei Guo", "Tao Zhang", "Wu Huang", "Yao Song"], "title": "A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion,\nthat leverages the SEED-VD dataset to generate a multimodal dataset of EEG\nsignals conditioned on video stimuli. Additionally, we disclose an engineering\npipeline for aligning video and EEG data pairs, facilitating the training of\nmultimodal large models with EEG alignment capabilities. Personalized EEG\nsignals are generated using a self-play graph network (SPGN) integrated with a\ndiffusion model. As a major contribution, we release a new dataset comprising\nover 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG\nsignals at 200 Hz and emotion labels, enabling video-EEG alignment and\nadvancing multimodal research. This framework offers novel tools for emotion\nanalysis, data augmentation, and brain-computer interface applications, with\nsubstantial research and engineering significance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aVideo2EEG-SPGN-Diffusion\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u5229\u7528\u81ea\u73a9\u56fe\u7f51\u7edc\uff08SPGN\uff09\u548c\u6269\u6563\u6a21\u578b\u751f\u6210\u89c6\u9891\u6761\u4ef6\u5316\u7684\u4e2a\u6027\u5316EEG\u4fe1\u53f7\u3002\u540c\u65f6\uff0c\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b\u89c6\u9891-EEG\u5bf9\u53ca\u60c5\u611f\u6807\u7b7e\u7684\u65b0\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u89c6\u9891-EEG\u5bf9\u9f50\u548c\u591a\u6a21\u6001\u7814\u7a76\uff0c\u5e76\u63d0\u4f9b\u76f8\u5173\u5de5\u7a0b\u7ba1\u9053\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u89c6\u9891-EEG\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u6709\u6548\u7684\u5bf9\u9f50\u7ba1\u9053\uff0c\u8fd9\u963b\u788d\u4e86\u5177\u6709EEG\u5bf9\u9f50\u80fd\u529b\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u4ee5\u53ca\u60c5\u611f\u5206\u6790\u3001\u6570\u636e\u589e\u5f3a\u548c\u8111\u673a\u63a5\u53e3\u7b49\u5e94\u7528\u7684\u53d1\u5c55\u3002", "method": "1. \u5229\u7528SEED-VD\u6570\u636e\u96c6\u4f5c\u4e3a\u89c6\u9891\u523a\u6fc0\u6e90\u3002\n2. \u91c7\u7528\u7ed3\u5408\u81ea\u73a9\u56fe\u7f51\u7edc\uff08SPGN\uff09\u4e0e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u751f\u6210\u4e2a\u6027\u5316\u7684EEG\u4fe1\u53f7\u3002\n3. \u5f00\u53d1\u5e76\u516c\u5f00\u4e86\u4e00\u4e2a\u7528\u4e8e\u5bf9\u9f50\u89c6\u9891\u548cEEG\u6570\u636e\u5bf9\u7684\u5de5\u7a0b\u7ba1\u9053\u3002", "result": "1. \u5f15\u5165\u4e86\u5f00\u6e90\u6846\u67b6Video2EEG-SPGN-Diffusion\u3002\n2. \u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b1000\u591a\u4e2aSEED-VD\u89c6\u9891\u523a\u6fc0\u6837\u672c\uff0c\u914d\u5bf9\u6709\u751f\u6210\u768462\u901a\u9053200 Hz EEG\u4fe1\u53f7\u548c\u60c5\u611f\u6807\u7b7e\u3002\n3. \u516c\u5f00\u4e86\u4e00\u4e2a\u7528\u4e8e\u89c6\u9891-EEG\u6570\u636e\u5bf9\u9f50\u7684\u5de5\u7a0b\u7ba1\u9053\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u6570\u636e\u96c6\u4e3a\u89c6\u9891-EEG\u5bf9\u9f50\u548c\u591a\u6a21\u6001\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5728\u60c5\u611f\u5206\u6790\u3001\u6570\u636e\u589e\u5f3a\u548c\u8111\u673a\u63a5\u53e3\u5e94\u7528\u65b9\u9762\u5177\u6709\u91cd\u8981\u7684\u7814\u7a76\u548c\u5de5\u7a0b\u610f\u4e49\u3002"}}
{"id": "2509.05330", "pdf": "https://arxiv.org/pdf/2509.05330", "abs": "https://arxiv.org/abs/2509.05330", "authors": ["Seyed Muhammad Hossein Mousavi", "Atiye Ilanloo"], "title": "MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset", "categories": ["cs.AI"], "comment": null, "summary": "Automatic emotion recognition has become increasingly important with the rise\nof AI, especially in fields like healthcare, education, and automotive systems.\nHowever, there is a lack of multimodal datasets, particularly involving body\nmotion and physiological signals, which limits progress in the field. To\naddress this, the MVRS dataset is introduced, featuring synchronized recordings\nfrom 13 participants aged 12 to 60 exposed to VR based emotional stimuli\n(relaxation, fear, stress, sadness, joy). Data were collected using eye\ntracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR\nsignals (Arduino UNO), all timestamp aligned. Participants followed a unified\nprotocol with consent and questionnaires. Features from each modality were\nextracted, fused using early and late fusion techniques, and evaluated with\nclassifiers to confirm the datasets quality and emotion separability, making\nMVRS a valuable contribution to multimodal affective computing.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u9886\u57df\u6570\u636e\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u672c\u6587\u5f15\u5165\u4e86MVRS\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6574\u5408\u4e86VR\u60c5\u611f\u523a\u6fc0\u4e0b\u7684\u773c\u52a8\u3001\u8eab\u4f53\u8fd0\u52a8\u548c\u751f\u7406\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u5206\u7c7b\u5668\u9a8c\u8bc1\u4e86\u5176\u8d28\u91cf\u548c\u60c5\u611f\u53ef\u5206\u79bb\u6027\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7684\u5174\u8d77\uff0c\u81ea\u52a8\u60c5\u611f\u8bc6\u522b\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u3001\u6559\u80b2\u548c\u6c7d\u8f66\u7cfb\u7edf\u7b49\u9886\u57df\u3002\u7136\u800c\uff0c\u8be5\u9886\u57df\u7f3a\u4e4f\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u6d89\u53ca\u8eab\u4f53\u8fd0\u52a8\u548c\u751f\u7406\u4fe1\u53f7\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u8fdb\u5c55\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86MVRS\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b13\u540d\u53c2\u4e0e\u8005\uff0812\u81f360\u5c81\uff09\u5728VR\u60c5\u611f\u523a\u6fc0\uff08\u653e\u677e\u3001\u6050\u60e7\u3001\u538b\u529b\u3001\u60b2\u4f24\u3001\u559c\u60a6\uff09\u4e0b\u7684\u540c\u6b65\u8bb0\u5f55\u3002\u6570\u636e\u901a\u8fc7\u773c\u52a8\u8ffd\u8e2a\uff08VR\u5934\u663e\u6444\u50cf\u5934\uff09\u3001\u8eab\u4f53\u8fd0\u52a8\uff08Kinect v2\uff09\u4ee5\u53caEMG\u548cGSR\u4fe1\u53f7\uff08Arduino UNO\uff09\u91c7\u96c6\uff0c\u5e76\u8fdb\u884c\u4e86\u65f6\u95f4\u6233\u5bf9\u9f50\u3002\u4ece\u6bcf\u79cd\u6a21\u6001\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u4f7f\u7528\u65e9\u671f\u548c\u665a\u671f\u878d\u5408\u6280\u672f\u8fdb\u884c\u878d\u5408\uff0c\u5e76\u901a\u8fc7\u5206\u7c7b\u5668\u8fdb\u884c\u8bc4\u4f30\u4ee5\u786e\u8ba4\u6570\u636e\u96c6\u7684\u8d28\u91cf\u548c\u60c5\u611f\u53ef\u5206\u79bb\u6027\u3002", "result": "\u901a\u8fc7\u5bf9\u591a\u6a21\u6001\u7279\u5f81\u7684\u63d0\u53d6\u3001\u878d\u5408\u53ca\u5206\u7c7b\u5668\u8bc4\u4f30\uff0cMVRS\u6570\u636e\u96c6\u7684\u8d28\u91cf\u548c\u60c5\u611f\u53ef\u5206\u79bb\u6027\u5f97\u5230\u4e86\u786e\u8ba4\u3002", "conclusion": "MVRS\u6570\u636e\u96c6\u662f\u5bf9\u591a\u6a21\u6001\u60c5\u611f\u8ba1\u7b97\u9886\u57df\u7684\u4e00\u9879\u6709\u4ef7\u503c\u7684\u8d21\u732e\u3002"}}
{"id": "2509.05449", "pdf": "https://arxiv.org/pdf/2509.05449", "abs": "https://arxiv.org/abs/2509.05449", "authors": ["Disha Makhija", "Manoj Ghuhan Arivazhagan", "Vinayshekhar Bannihatti Kumar", "Rashmi Gangadharaiah"], "title": "Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Membership inference attacks (MIAs) reveal whether specific data was used to\ntrain machine learning models, serving as important tools for privacy auditing\nand compliance assessment. Recent studies have reported that MIAs perform only\nmarginally better than random guessing against large language models,\nsuggesting that modern pre-training approaches with massive datasets may be\nfree from privacy leakage risks. Our work offers a complementary perspective to\nthese findings by exploring how examining LLMs' internal representations,\nrather than just their outputs, may provide additional insights into potential\nmembership inference signals. Our framework, \\emph{memTrace}, follows what we\ncall \\enquote{neural breadcrumbs} extracting informative signals from\ntransformer hidden states and attention patterns as they process candidate\nsequences. By analyzing layer-wise representation dynamics, attention\ndistribution characteristics, and cross-layer transition patterns, we detect\npotential memorization fingerprints that traditional loss-based approaches may\nnot capture. This approach yields strong membership detection across several\nmodel families achieving average AUC scores of 0.85 on popular MIA benchmarks.\nOur findings suggest that internal model behaviors can reveal aspects of\ntraining data exposure even when output-based signals appear protected,\nhighlighting the need for further research into membership privacy and the\ndevelopment of more robust privacy-preserving training techniques for large\nlanguage models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u80fd\u63ed\u793a\u4f20\u7edf\u6210\u5458\u63a8\u65ad\u653b\u51fb\uff08MIA\uff09\u672a\u6355\u83b7\u7684\u9690\u79c1\u6cc4\u9732\uff0c\u5373\u4f7f\u5176\u8f93\u51fa\u4fe1\u53f7\u770b\u4f3c\u53d7\u4fdd\u62a4\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u8868\u660e\uff0cMIA\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u653b\u51fb\u6548\u679c\u4ec5\u7565\u4f18\u4e8e\u968f\u673a\u731c\u6d4b\uff0c\u5bfc\u81f4\u4eba\u4eec\u8ba4\u4e3a\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u53ef\u80fd\u4e0d\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5ba1\u89c6\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u800c\u975e\u4ec5\u5176\u8f93\u51fa\u6765\u63d0\u4f9b\u8865\u5145\u89c6\u89d2\uff0c\u4ee5\u53d1\u73b0\u6f5c\u5728\u7684\u6210\u5458\u63a8\u65ad\u4fe1\u53f7\u3002", "method": "\u63d0\u51fa`memTrace`\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u548c\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u5e8f\u5217\u65f6\u7684\u201c\u795e\u7ecf\u75d5\u8ff9\u201d\uff0c\u5305\u62ec\u53d8\u538b\u5668\u9690\u85cf\u72b6\u6001\u3001\u6ce8\u610f\u529b\u6a21\u5f0f\u3001\u5c42\u7ea7\u8868\u793a\u52a8\u6001\u3001\u6ce8\u610f\u529b\u5206\u5e03\u7279\u6027\u548c\u8de8\u5c42\u8f6c\u6362\u6a21\u5f0f\uff0c\u6765\u68c0\u6d4b\u4f20\u7edf\u57fa\u4e8e\u635f\u5931\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6355\u83b7\u7684\u8bb0\u5fc6\u6307\u7eb9\u3002", "result": "`memTrace`\u65b9\u6cd5\u5728\u591a\u4e2a\u6a21\u578b\u5bb6\u65cf\u4e0a\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6210\u5458\u63a8\u65ad\u68c0\u6d4b\uff0c\u5728\u6d41\u884c\u7684MIA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747AUC\u5206\u6570\u8fbe\u52300.85\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u57fa\u4e8e\u8f93\u51fa\u7684\u4fe1\u53f7\u770b\u4f3c\u53d7\u4fdd\u62a4\uff0c\u6a21\u578b\u5185\u90e8\u884c\u4e3a\u4ecd\u80fd\u63ed\u793a\u8bad\u7ec3\u6570\u636e\u66b4\u9732\u7684\u65b9\u9762\u3002\u8fd9\u5f3a\u8c03\u4e86\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6210\u5458\u9690\u79c1\uff0c\u5e76\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u66f4 robust \u7684\u9690\u79c1\u4fdd\u62a4\u8bad\u7ec3\u6280\u672f\u3002"}}
{"id": "2509.05486", "pdf": "https://arxiv.org/pdf/2509.05486", "abs": "https://arxiv.org/abs/2509.05486", "authors": ["Jessica M. Lundin", "Ada Zhang", "Nihal Karim", "Hamza Louzan", "Victor Wei", "David Adelani", "Cody Carroll"], "title": "The Token Tax: Systematic Bias in Multilingual Tokenization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization inefficiency imposes structural disadvantages on morphologically\ncomplex, low-resource languages, inflating compute resources and depressing\naccuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA\nitems; 5 subjects; 16 African languages) and show that fertility (tokens/word)\nreliably predicts accuracy. Higher fertility consistently predicts lower\naccuracy across all models and subjects. We further find that reasoning models\n(DeepSeek, o1) consistently outperform non-reasoning peers across high and low\nresource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in\nprior generations. Finally, translating token inflation to economics, a\ndoubling in tokens results in quadrupled training cost and time, underscoring\nthe token tax faced by many languages. These results motivate morphologically\naware tokenization, fair pricing, and multilingual benchmarks for equitable\nnatural language processing (NLP).", "AI": {"tldr": "\u6807\u8bb0\u5316\u6548\u7387\u4f4e\u4e0b\u5bf9\u5f62\u6001\u590d\u6742\u3001\u4f4e\u8d44\u6e90\u8bed\u8a00\u9020\u6210\u7ed3\u6784\u6027\u52a3\u52bf\uff0c\u8868\u73b0\u4e3a\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u548c\u51c6\u786e\u6027\u964d\u4f4e\u3002\u7814\u7a76\u53d1\u73b0\u6807\u8bb0\u4e30\u5ea6\uff08\u6bcf\u8bcd\u6807\u8bb0\u6570\uff09\u662f\u51c6\u786e\u6027\u7684\u53ef\u9760\u9884\u6d4b\u56e0\u5b50\uff0c\u4e14\u63a8\u7406\u578bLLM\u8868\u73b0\u66f4\u4f73\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u6807\u8bb0\u81a8\u80c0\u5e26\u6765\u7684\u7ecf\u6d4e\u8d1f\u62c5\u3002", "motivation": "\u5f62\u6001\u590d\u6742\u3001\u4f4e\u8d44\u6e90\u8bed\u8a00\u56e0\u6807\u8bb0\u5316\u6548\u7387\u4f4e\u4e0b\u800c\u9762\u4e34\u7ed3\u6784\u6027\u52a3\u52bf\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u589e\u52a0\u548c\u51c6\u786e\u6027\u4e0b\u964d\u3002", "method": "\u5728AfriMMLU\u6570\u636e\u96c6\uff08\u5305\u542b9,000\u4e2a\u591a\u9879\u9009\u62e9\u9898\uff0c\u6db5\u76d65\u4e2a\u5b66\u79d1\u548c16\u79cd\u975e\u6d32\u8bed\u8a00\uff09\u4e0a\u8bc4\u4f30\u4e8610\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002", "result": ["\u6807\u8bb0\u4e30\u5ea6\uff08\u6bcf\u8bcd\u6807\u8bb0\u6570\uff09\u80fd\u53ef\u9760\u9884\u6d4b\u51c6\u786e\u6027\uff1b\u66f4\u9ad8\u7684\u6807\u8bb0\u4e30\u5ea6\u4e0e\u6240\u6709\u6a21\u578b\u548c\u5b66\u79d1\u7684\u8f83\u4f4e\u51c6\u786e\u6027\u6301\u7eed\u76f8\u5173\u3002", "\u63a8\u7406\u578b\u6a21\u578b\uff08\u5982DeepSeek, o1\uff09\u5728AfriMMLU\u6570\u636e\u96c6\u4e2d\uff0c\u65e0\u8bba\u9ad8\u8d44\u6e90\u8fd8\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5176\u8868\u73b0\u5747\u4f18\u4e8e\u975e\u63a8\u7406\u578b\u6a21\u578b\uff0c\u7f29\u5c0f\u4e86\u5148\u524d\u89c2\u5bdf\u5230\u7684\u51c6\u786e\u6027\u5dee\u8ddd\u3002", "\u5c06\u6807\u8bb0\u81a8\u80c0\u8f6c\u5316\u4e3a\u7ecf\u6d4e\u6210\u672c\uff0c\u6807\u8bb0\u6570\u91cf\u7ffb\u500d\u5c06\u5bfc\u81f4\u8bad\u7ec3\u6210\u672c\u548c\u65f6\u95f4\u589e\u52a0\u56db\u500d\uff0c\u7a81\u663e\u4e86\u8bb8\u591a\u8bed\u8a00\u9762\u4e34\u7684\u201c\u6807\u8bb0\u7a0e\u201d\u3002"], "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u5f00\u53d1\u5f62\u6001\u611f\u77e5\u7684\u6807\u8bb0\u5316\u65b9\u6cd5\u3001\u5236\u5b9a\u516c\u5e73\u7684\u5b9a\u4ef7\u7b56\u7565\u4ee5\u53ca\u6784\u5efa\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u5b9e\u73b0\u516c\u5e73\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u3002"}}
{"id": "2509.05936", "pdf": "https://arxiv.org/pdf/2509.05936", "abs": "https://arxiv.org/abs/2509.05936", "authors": ["Xuanhao Luo", "Shivesh Madan Nath Jha", "Akruti Sinha", "Zhizhen Li", "Yuchen Liu"], "title": "ALPHA: LLM-Enabled Active Learning for Human-Free Network Anomaly Detection", "categories": ["cs.NI", "cs.LG"], "comment": "Accepted at 44th IEEE International Performance Computing and\n  Communications Conference (IPCCC 2025)", "summary": "Network log data analysis plays a critical role in detecting security threats\nand operational anomalies. Traditional log analysis methods for anomaly\ndetection and root cause analysis rely heavily on expert knowledge or fully\nsupervised learning models, both of which require extensive labeled data and\nsignificant human effort. To address these challenges, we propose ALPHA, the\nfirst Active Learning Pipeline for Human-free log Analysis. ALPHA integrates\nsemantic embedding, clustering-based representative sampling, and large\nlanguage model (LLM)-assisted few-shot annotation to automate the anomaly\ndetection process. The LLM annotated labels are propagated across clusters,\nenabling large-scale training of an anomaly detector with minimal supervision.\nTo enhance the annotation accuracy, we propose a two-step few-shot refinement\nstrategy that adaptively selects informative prompts based on the LLM's\nobserved error patterns. Extensive experiments on real-world log datasets\ndemonstrate that ALPHA achieves detection accuracy comparable to fully\nsupervised methods while mitigating human efforts in the loop. ALPHA also\nsupports interpretable analysis through LLM-driven root cause explanations in\nthe post-detection stage. These capabilities make ALPHA a scalable and\ncost-efficient solution for truly automated log-based anomaly detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faALPHA\uff0c\u4e00\u4e2a\u65e0\u4eba\u5de5\u5e72\u9884\u7684\u65e5\u5fd7\u5206\u6790\u4e3b\u52a8\u5b66\u4e60\u6d41\u7a0b\uff0c\u5229\u7528LLM\u8f85\u52a9\u7684\u5c11\u6837\u672c\u6807\u6ce8\u548c\u805a\u7c7b\u4f20\u64ad\uff0c\u5b9e\u73b0\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u5ab2\u7f8e\u7684\u5f02\u5e38\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u652f\u6301\u53ef\u89e3\u91ca\u7684\u6839\u56e0\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u7684\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u548c\u6839\u56e0\u5206\u6790\u65b9\u6cd5\u9ad8\u5ea6\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u6216\u5168\u76d1\u7763\u6a21\u578b\uff0c\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u5de8\u5927\u7684\u4eba\u529b\u6295\u5165\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u6570\u636e\u6807\u6ce8\u548c\u4eba\u529b\u6210\u672c\u9ad8\u6602\u7684\u6311\u6218\u3002", "method": "ALPHA\u662f\u4e00\u4e2a\u4e3b\u52a8\u5b66\u4e60\u6d41\u7a0b\uff0c\u5b83\u96c6\u6210\u4e86\u8bed\u4e49\u5d4c\u5165\u3001\u57fa\u4e8e\u805a\u7c7b\u7684\u4ee3\u8868\u6027\u91c7\u6837\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f85\u52a9\u7684\u5c11\u6837\u672c\u6807\u6ce8\u3002LLM\u6807\u6ce8\u7684\u6807\u7b7e\u4f1a\u5728\u96c6\u7fa4\u4e2d\u4f20\u64ad\uff0c\u4ee5\u5c11\u91cf\u76d1\u7763\u8bad\u7ec3\u5f02\u5e38\u68c0\u6d4b\u5668\u3002\u4e3a\u4e86\u63d0\u9ad8\u6807\u6ce8\u51c6\u786e\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u6b65\u5c11\u6837\u672c\u7ec6\u5316\u7b56\u7565\uff0c\u6839\u636eLLM\u7684\u9519\u8bef\u6a21\u5f0f\u81ea\u9002\u5e94\u9009\u62e9\u63d0\u793a\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5728\u68c0\u6d4b\u540e\u9636\u6bb5\u901a\u8fc7LLM\u9a71\u52a8\u63d0\u4f9b\u6839\u56e0\u89e3\u91ca\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u65e5\u5fd7\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cALPHA\u5728\u68c0\u6d4b\u7cbe\u5ea6\u4e0a\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u6295\u5165\u3002\u5b83\u8fd8\u652f\u6301\u901a\u8fc7LLM\u9a71\u52a8\u7684\u6839\u56e0\u89e3\u91ca\u8fdb\u884c\u53ef\u89e3\u91ca\u5206\u6790\u3002", "conclusion": "ALPHA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u771f\u6b63\u81ea\u52a8\u5316\u7684\u57fa\u4e8e\u65e5\u5fd7\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u5e76\u652f\u6301\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.05322", "pdf": "https://arxiv.org/pdf/2509.05322", "abs": "https://arxiv.org/abs/2509.05322", "authors": ["Pavithra Elumalai", "Sudharsan Vijayaraghavan", "Madhumita Mondal", "Areejit Samal"], "title": "Application of discrete Ricci curvature in pruning randomly wired neural networks: A case study with chest x-ray classification of COVID-19", "categories": ["cs.CV", "cs.LG", "cs.SI", "physics.comp-ph"], "comment": "21 pages, 4 figures, 9 tables", "summary": "Randomly Wired Neural Networks (RWNNs) serve as a valuable testbed for\ninvestigating the impact of network topology in deep learning by capturing how\ndifferent connectivity patterns impact both learning efficiency and model\nperformance. At the same time, they provide a natural framework for exploring\nedge-centric network measures as tools for pruning and optimization. In this\nstudy, we investigate three edge-centric network measures: Forman-Ricci\ncurvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness\ncentrality (EBC), to compress RWNNs by selectively retaining important synapses\n(or edges) while pruning the rest. As a baseline, RWNNs are trained for\nCOVID-19 chest x-ray image classification, aiming to reduce network complexity\nwhile preserving performance in terms of accuracy, specificity, and\nsensitivity. We extend prior work on pruning RWNN using ORC by incorporating\ntwo additional edge-centric measures, FRC and EBC, across three network\ngenerators: Erd\\\"{o}s-R\\'{e}nyi (ER) model, Watts-Strogatz (WS) model, and\nBarab\\'{a}si-Albert (BA) model. We provide a comparative analysis of the\npruning performance of the three measures in terms of compression ratio and\ntheoretical speedup. A central focus of our study is to evaluate whether FRC,\nwhich is computationally more efficient than ORC, can achieve comparable\npruning effectiveness. Along with performance evaluation, we further\ninvestigate the structural properties of the pruned networks through modularity\nand global efficiency, offering insights into the trade-off between modular\nsegregation and network efficiency in compressed RWNNs. Our results provide\ninitial evidence that FRC-based pruning can effectively simplify RWNNs,\noffering significant computational advantages while maintaining performance\ncomparable to ORC.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.05346", "pdf": "https://arxiv.org/pdf/2509.05346", "abs": "https://arxiv.org/abs/2509.05346", "authors": ["Bo Yuan", "Jiazi Hu"], "title": "Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning", "categories": ["cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) are increasingly envisioned as intelligent\nassistants for personalized learning, systematic head-to-head evaluations\nwithin authentic learning scenarios remain limited. This study conducts an\nempirical comparison of three state-of-the-art LLMs on a tutoring task that\nsimulates a realistic learning setting. Using a dataset comprising a student's\nanswers to ten questions of mixed formats with correctness labels, each LLM is\nrequired to (i) analyze the quiz to identify underlying knowledge components,\n(ii) infer the student's mastery profile, and (iii) generate targeted guidance\nfor improvement. To mitigate subjectivity and evaluator bias, we employ Gemini\nas a virtual judge to perform pairwise comparisons along various dimensions:\naccuracy, clarity, actionability, and appropriateness. Results analyzed via the\nBradley-Terry model indicate that GPT-4o is generally preferred, producing\nfeedback that is more informative and better structured than its counterparts,\nwhile DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower\nconsistency. These findings highlight the feasibility of deploying LLMs as\nadvanced teaching assistants for individualized support and provide\nmethodological guidance for future empirical research on LLM-driven\npersonalized learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u5728\u6a21\u62df\u6559\u5b66\u573a\u666f\u4e2d\u6bd4\u8f83\u4e86GPT-4o\u3001DeepSeek-V3\u548cGLM-4.5\u4e09\u79cdLLM\u5728\u8f85\u5bfc\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o\u5728\u4fe1\u606f\u4e30\u5bcc\u6027\u548c\u7ed3\u6784\u6027\u65b9\u9762\u66f4\u4f18\uff0c\u8868\u660eLLM\u5728\u4e2a\u6027\u5316\u5b66\u4e60\u8f85\u52a9\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u88ab\u8bbe\u60f3\u4e3a\u4e2a\u6027\u5316\u5b66\u4e60\u7684\u667a\u80fd\u52a9\u624b\uff0c\u4f46\u5728\u771f\u5b9e\u5b66\u4e60\u573a\u666f\u4e2d\u7f3a\u4e4f\u7cfb\u7edf\u7684\u5934\u5bf9\u5934\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u5728\u4e00\u4e2a\u6a21\u62df\u8f85\u5bfc\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u5305\u542b\u5b66\u751f\u7b54\u5377\u7684\u6570\u636e\u96c6\uff0c\u8981\u6c42LLMs\u5206\u6790\u6d4b\u9a8c\u3001\u63a8\u65ad\u5b66\u751f\u638c\u63e1\u60c5\u51b5\u5e76\u751f\u6210\u9488\u5bf9\u6027\u6307\u5bfc\u3002\u91c7\u7528Gemini\u4f5c\u4e3a\u865a\u62df\u8bc4\u5224\u5458\uff0c\u8fdb\u884c\u51c6\u786e\u6027\u3001\u6e05\u6670\u5ea6\u3001\u53ef\u64cd\u4f5c\u6027\u548c\u9002\u5f53\u6027\u7b49\u591a\u7ef4\u5ea6\u914d\u5bf9\u6bd4\u8f83\uff0c\u5e76\u901a\u8fc7Bradley-Terry\u6a21\u578b\u5206\u6790\u7ed3\u679c\u3002", "result": "\u5206\u6790\u7ed3\u679c\u663e\u793a\uff0cGPT-4o\u666e\u904d\u66f4\u53d7\u9752\u7750\uff0c\u5176\u53cd\u9988\u4fe1\u606f\u66f4\u4e30\u5bcc\u3001\u7ed3\u6784\u66f4\u6e05\u6670\uff1b\u800cDeepSeek-V3\u548cGLM-4.5\u867d\u7136\u5076\u6709\u4eae\u70b9\uff0c\u4f46\u4e00\u81f4\u6027\u8f83\u4f4e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u90e8\u7f72LLMs\u4f5c\u4e3a\u9ad8\u7ea7\u6559\u5b66\u52a9\u624b\u63d0\u4f9b\u4e2a\u6027\u5316\u652f\u6301\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u672a\u6765LLM\u9a71\u52a8\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u5b9e\u8bc1\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u6307\u5bfc\u3002"}}
{"id": "2509.05460", "pdf": "https://arxiv.org/pdf/2509.05460", "abs": "https://arxiv.org/abs/2509.05460", "authors": ["Diego Feijer", "Himan Abdollahpouri", "Sanket Gupta", "Alexander Clare", "Yuxiao Wen", "Todd Wasson", "Maria Dimakopoulou", "Zahra Nazari", "Kyle Kretschman", "Mounia Lalmas"], "title": "Calibrated Recommendations with Contextual Bandits", "categories": ["cs.LG", "cs.IR", "stat.ML"], "comment": "Accepted at ACM RecSys '25, CONSEQUENCES workshop", "summary": "Spotify's Home page features a variety of content types, including music,\npodcasts, and audiobooks. However, historical data is heavily skewed toward\nmusic, making it challenging to deliver a balanced and personalized content\nmix. Moreover, users' preference towards different content types may vary\ndepending on the time of day, the day of week, or even the device they use. We\npropose a calibration method that leverages contextual bandits to dynamically\nlearn each user's optimal content type distribution based on their context and\npreferences. Unlike traditional calibration methods that rely on historical\naverages, our approach boosts engagement by adapting to how users interests in\ndifferent content types varies across contexts. Both offline and online results\ndemonstrate improved precision and user engagement with the Spotify Home page,\nin particular with under-represented content types such as podcasts.", "AI": {"tldr": "Spotify\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u4ee5\u52a8\u6001\u4f18\u5316\u7528\u6237\u9996\u9875\u5185\u5bb9\u7c7b\u578b\u5206\u5e03\uff0c\u4ece\u800c\u63d0\u9ad8\u53c2\u4e0e\u5ea6\uff0c\u5c24\u5176\u5bf9\u64ad\u5ba2\u7b49\u975e\u97f3\u4e50\u5185\u5bb9\u6709\u663e\u8457\u6548\u679c\u3002", "motivation": "Spotify\u9996\u9875\u5185\u5bb9\u7c7b\u578b\u591a\u6837\uff0c\u4f46\u5386\u53f2\u6570\u636e\u4e25\u91cd\u504f\u5411\u97f3\u4e50\uff0c\u5bfc\u81f4\u96be\u4ee5\u63d0\u4f9b\u5e73\u8861\u4e14\u4e2a\u6027\u5316\u7684\u5185\u5bb9\u7ec4\u5408\u3002\u6b64\u5916\uff0c\u7528\u6237\u5bf9\u4e0d\u540c\u5185\u5bb9\u7c7b\u578b\u7684\u504f\u597d\u4f1a\u968f\u65f6\u95f4\u3001\u65e5\u671f\u6216\u8bbe\u5907\u800c\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\uff08Contextual Bandits\uff09\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u52a8\u6001\u5b66\u4e60\u6bcf\u4e2a\u7528\u6237\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u548c\u504f\u597d\u4e0b\u7684\u6700\u4f18\u5185\u5bb9\u7c7b\u578b\u5206\u5e03\u3002\u4e0e\u4f20\u7edf\u4f9d\u8d56\u5386\u53f2\u5e73\u5747\u503c\u7684\u6821\u51c6\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u80fd\u9002\u5e94\u7528\u6237\u5174\u8da3\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u7684\u53d8\u5316\u3002", "result": "\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u7ed3\u679c\u5747\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86Spotify\u9996\u9875\u7684\u7cbe\u786e\u5ea6\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u64ad\u5ba2\u7b49\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u5185\u5bb9\u7c7b\u578b\u3002", "conclusion": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u52a8\u6001\u6821\u51c6\u5185\u5bb9\u7c7b\u578b\u5206\u5e03\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u5386\u53f2\u6570\u636e\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u6839\u636e\u7528\u6237\u4e0a\u4e0b\u6587\u53d8\u5316\u9002\u5e94\u5176\u5174\u8da3\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2509.05505", "pdf": "https://arxiv.org/pdf/2509.05505", "abs": "https://arxiv.org/abs/2509.05505", "authors": ["Mansi Garg", "Lee-Chi Wang", "Bhavesh Ghanchi", "Sanjana Dumpala", "Shreyash Kakde", "Yen Chih Chen"], "title": "Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 6 figures, 3 tables", "summary": "This work presents a Biomedical Literature Question Answering (Q&A) system\nbased on a Retrieval-Augmented Generation (RAG) architecture, designed to\nimprove access to accurate, evidence-based medical information. Addressing the\nshortcomings of conventional health search engines and the lag in public access\nto biomedical research, the system integrates diverse sources, including PubMed\narticles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant\ninformation and generate concise, context-aware responses. The retrieval\npipeline uses MiniLM-based semantic embeddings and FAISS vector search, while\nanswer generation is performed by a fine-tuned Mistral-7B-v0.3 language model\noptimized using QLoRA for efficient, low-resource training. The system supports\nboth general medical queries and domain-specific tasks, with a focused\nevaluation on breast cancer literature demonstrating the value of\ndomain-aligned retrieval. Empirical results, measured using BERTScore (F1),\nshow substantial improvements in factual consistency and semantic relevance\ncompared to baseline models. The findings underscore the potential of\nRAG-enhanced language models to bridge the gap between complex biomedical\nliterature and accessible public health knowledge, paving the way for future\nwork on multilingual adaptation, privacy-preserving inference, and personalized\nmedical AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u67b6\u6784\u7684\u751f\u7269\u533b\u5b66\u6587\u732e\u95ee\u7b54\u7cfb\u7edf\uff0c\u5229\u7528MiniLM+FAISS\u8fdb\u884c\u68c0\u7d22\uff0c\u5e76\u7531QLoRA\u4f18\u5316\u7684Mistral-7B-v0.3\u751f\u6210\u7b54\u6848\u3002\u8be5\u7cfb\u7edf\u6574\u5408\u591a\u6e90\u533b\u5b66\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u533b\u5b66\u4fe1\u606f\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u76f8\u5173\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5065\u5eb7\u641c\u7d22\u5f15\u64ce\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u516c\u4f17\u83b7\u53d6\u751f\u7269\u533b\u5b66\u7814\u7a76\u6210\u679c\u6ede\u540e\u7684\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u9ad8\u83b7\u53d6\u51c6\u786e\u3001\u5faa\u8bc1\u533b\u5b66\u4fe1\u606f\u7684\u4fbf\u6377\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eRAG\u67b6\u6784\u7684\u751f\u7269\u533b\u5b66\u6587\u732e\u95ee\u7b54\u7cfb\u7edf\uff0c\u6574\u5408\u4e86PubMed\u6587\u7ae0\u3001\u7cbe\u9009\u95ee\u7b54\u6570\u636e\u96c6\u548c\u533b\u5b66\u767e\u79d1\u5168\u4e66\u7b49\u591a\u6837\u5316\u4fe1\u606f\u6e90\u3002\u68c0\u7d22\u6d41\u7a0b\u91c7\u7528\u57fa\u4e8eMiniLM\u7684\u8bed\u4e49\u5d4c\u5165\u548cFAISS\u5411\u91cf\u641c\u7d22\u3002\u7b54\u6848\u751f\u6210\u7531\u4f7f\u7528QLoRA\u4f18\u5316\u5fae\u8c03\u7684Mistral-7B-v0.3\u8bed\u8a00\u6a21\u578b\u5b8c\u6210\u3002", "result": "\u8be5\u7cfb\u7edf\u652f\u6301\u901a\u7528\u533b\u5b66\u67e5\u8be2\u548c\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u3002\u5728\u4e73\u817a\u764c\u6587\u732e\u8bc4\u4f30\u4e2d\uff0c\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u5176\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u76f8\u5173\u6027\uff08\u901a\u8fc7BERTScore F1\u8861\u91cf\uff09\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "RAG\u589e\u5f3a\u578b\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5f25\u5408\u590d\u6742\u751f\u7269\u533b\u5b66\u6587\u732e\u4e0e\u53ef\u8bbf\u95ee\u7684\u516c\u5171\u5065\u5eb7\u77e5\u8bc6\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u672a\u6765\u591a\u8bed\u8a00\u9002\u5e94\u3001\u9690\u79c1\u4fdd\u62a4\u63a8\u7406\u548c\u4e2a\u6027\u5316\u533b\u7597AI\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2509.05938", "pdf": "https://arxiv.org/pdf/2509.05938", "abs": "https://arxiv.org/abs/2509.05938", "authors": ["Alissa Baumeister", "Sina Keshvadi"], "title": "An Axiomatic Analysis of Path Selection Strategies for Multipath Transport in Path-Aware Networks", "categories": ["cs.NI", "C.2; C.2.1; C.2.2"], "comment": "13 pages, submitted in the journal of Complex Networks", "summary": "Path-aware networking architectures like SCION provide end-hosts with\nexplicit control over inter-domain routing, while multipath transport protocols\nlike MPTCP and MPQUIC enable the concurrent use of multiple paths. This\ncombination promises significant gains in performance and policy enforcement,\nbut it also creates a stark trade-off between individual performance\noptimization and overall network stability. This paper quantifies this\ntrade-off through a rigorous axiomatic analysis. We evaluate a spectrum of\nalgorithms, from greedy (Min-RTT) and cooperative (Round-Robin) to hybrid\napproaches (Epsilon-Greedy), against axioms of Efficiency, Loss Avoidance,\nStability, and Fairness in a simulated path-aware environment.\n  Our simulations reveal that purely greedy strategies, while efficient under\nlow contention, induce catastrophic packet loss, increasing by over >18,000% as\nthe number of competing agents grow, due to herd effects that cause severe\nnetwork instability. Conversely, cooperative strategies ensure fairness and\nstability but at the cost of underutilizing high-capacity paths. Crucially, we\ndemonstrate that hybrid strategies resolve this conflict. The Epsilon-Greedy\nalgorithm, for instance, achieves the highest efficiency of all tested\nstrategies in high-contention scenarios while mitigating the instability\ninherent to the greedy approach. Our axiomatic analysis suggests that tunable,\nhybrid algorithms are essential for designing robust and high-performance path\nselection mechanisms for next-generation networks.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u516c\u7406\u5206\u6790\u548c\u6a21\u62df\uff0c\u91cf\u5316\u4e86\u8def\u5f84\u611f\u77e5\u7f51\u7edc\u4e2d\u591a\u8def\u5f84\u4f20\u8f93\u5728\u4e2a\u4f53\u6027\u80fd\u4f18\u5316\u548c\u7f51\u7edc\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u7814\u7a76\u8868\u660e\uff0c\u6df7\u5408\u7b56\u7565\uff08\u5982Epsilon-Greedy\uff09\u80fd\u6709\u6548\u89e3\u51b3\u7eaf\u8d2a\u5a6a\u7b56\u7565\u5bfc\u81f4\u7684\u707e\u96be\u6027\u4e22\u5305\u548c\u7eaf\u534f\u4f5c\u7b56\u7565\u5bfc\u81f4\u7684\u8def\u5f84\u5229\u7528\u4e0d\u8db3\u95ee\u9898\uff0c\u662f\u4e0b\u4e00\u4ee3\u7f51\u7edc\u8def\u5f84\u9009\u62e9\u673a\u5236\u7684\u5173\u952e\u3002", "motivation": "\u8def\u5f84\u611f\u77e5\u7f51\u7edc\uff08\u5982SCION\uff09\u4e0e\u591a\u8def\u5f84\u4f20\u8f93\u534f\u8bae\uff08\u5982MPTCP/MPQUIC\uff09\u7684\u7ed3\u5408\u6709\u671b\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u7b56\u7565\u6267\u884c\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u4e2a\u4f53\u6027\u80fd\u4f18\u5316\u4e0e\u6574\u4f53\u7f51\u7edc\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u5de8\u5927\u6743\u8861\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e25\u683c\u7684\u516c\u7406\u5206\u6790\u91cf\u5316\u8fd9\u4e00\u6743\u8861\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e25\u683c\u7684\u516c\u7406\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u5728\u6a21\u62df\u7684\u8def\u5f84\u611f\u77e5\u73af\u5883\u4e2d\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u7b97\u6cd5\uff0c\u5305\u62ec\u8d2a\u5a6a\uff08Min-RTT\uff09\u3001\u534f\u4f5c\uff08Round-Robin\uff09\u4ee5\u53ca\u6df7\u5408\u65b9\u6cd5\uff08Epsilon-Greedy\uff09\u3002\u8bc4\u4f30\u6807\u51c6\u57fa\u4e8e\u6548\u7387\u3001\u907f\u514d\u4e22\u5305\u3001\u7a33\u5b9a\u6027\u548c\u516c\u5e73\u6027\u7b49\u516c\u7406\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u7eaf\u8d2a\u5a6a\u7b56\u7565\u5728\u4f4e\u7ade\u4e89\u4e0b\u9ad8\u6548\uff0c\u4f46\u5728\u9ad8\u7ade\u4e89\u4e0b\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u4e22\u5305\uff08\u589e\u957f\u8d8518,000%\uff09\uff0c\u5f15\u53d1\u4e25\u91cd\u7684\u7f51\u7edc\u4e0d\u7a33\u5b9a\u3002\u76f8\u53cd\uff0c\u534f\u4f5c\u7b56\u7565\u80fd\u786e\u4fdd\u516c\u5e73\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u4f46\u4ee3\u4ef7\u662f\u9ad8\u5bb9\u91cf\u8def\u5f84\u5229\u7528\u4e0d\u8db3\u3002\u7814\u7a76\u8bc1\u660e\uff0c\u6df7\u5408\u7b56\u7565\u80fd\u89e3\u51b3\u6b64\u51b2\u7a81\uff0c\u4f8b\u5982Epsilon-Greedy\u7b97\u6cd5\u5728\u9ad8\u7ade\u4e89\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u6548\u7387\uff0c\u5e76\u6709\u6548\u7f13\u89e3\u4e86\u8d2a\u5a6a\u65b9\u6cd5\u56fa\u6709\u7684\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "\u516c\u7406\u5206\u6790\u8868\u660e\uff0c\u53ef\u8c03\u8c10\u7684\u6df7\u5408\u7b97\u6cd5\u5bf9\u4e8e\u8bbe\u8ba1\u4e0b\u4e00\u4ee3\u7f51\u7edc\u4e2d\u9c81\u68d2\u4e14\u9ad8\u6027\u80fd\u7684\u8def\u5f84\u9009\u62e9\u673a\u5236\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.05329", "pdf": "https://arxiv.org/pdf/2509.05329", "abs": "https://arxiv.org/abs/2509.05329", "authors": ["Juan Carlos Martinez-Sevilla", "Francesco Foscarin", "Patricia Garcia-Iasci", "David Rizo", "Jorge Calvo-Zaragoza", "Gerhard Widmer"], "title": "Optical Music Recognition of Jazz Lead Sheets", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the 26th International Society for Music Information\n  Retrieval Conference (ISMIR), 2025", "summary": "In this paper, we address the challenge of Optical Music Recognition (OMR)\nfor handwritten jazz lead sheets, a widely used musical score type that encodes\nmelody and chords. The task is challenging due to the presence of chords, a\nscore component not handled by existing OMR systems, and the high variability\nand quality issues associated with handwritten images. Our contribution is\ntwo-fold. We present a novel dataset consisting of 293 handwritten jazz lead\nsheets of 163 unique pieces, amounting to 2021 total staves aligned with\nHumdrum **kern and MusicXML ground truth scores. We also supply synthetic score\nimages generated from the ground truth. The second contribution is the\ndevelopment of an OMR model for jazz lead sheets. We discuss specific\ntokenisation choices related to our kind of data, and the advantages of using\nsynthetic scores and pretrained models. We publicly release all code, data, and\nmodels.", "AI": {"tldr": "\u672c\u6587\u89e3\u51b3\u4e86\u624b\u5199\u7235\u58eb\u4e50\u8c31\u5149\u7b26\u8bc6\u522b\uff08OMR\uff09\u7684\u6311\u6218\uff0c\u521b\u5efa\u4e86\u5305\u542b\u624b\u5199\u548c\u5408\u6210\u56fe\u50cf\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e13\u7528\u7684OMR\u6a21\u578b\u3002", "motivation": "\u73b0\u6709OMR\u7cfb\u7edf\u65e0\u6cd5\u5904\u7406\u5305\u542b\u548c\u5f26\u7684\u4e50\u8c31\uff1b\u624b\u5199\u4e50\u8c31\u5b58\u5728\u9ad8\u53d8\u5f02\u6027\u548c\u8d28\u91cf\u95ee\u9898\uff1b\u7235\u58eb\u4e50\u8c31\u4f5c\u4e3a\u5e38\u7528\u4e50\u8c31\u7c7b\u578b\uff0c\u9700\u8981\u6709\u6548\u7684OMR\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b293\u4efd\u624b\u5199\u7235\u58eb\u4e50\u8c31\uff082021\u4e2a\u8c31\u8868\uff09\u53ca\u5176Humdrum **kern\u548cMusicXML\u683c\u5f0f\u771f\u503c\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u751f\u6210\u4e86\u5408\u6210\u4e50\u8c31\u56fe\u50cf\u30022. \u5f00\u53d1\u4e86\u4e13\u95e8\u7528\u4e8e\u7235\u58eb\u4e50\u8c31\u7684OMR\u6a21\u578b\uff0c\u63a2\u8ba8\u4e86\u7279\u5b9a\u8bcd\u5143\u5316\u9009\u62e9\u4ee5\u53ca\u4f7f\u7528\u5408\u6210\u4e50\u8c31\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f18\u52bf\u3002", "result": "1. \u9996\u6b21\u53d1\u5e03\u4e86\u5927\u89c4\u6a21\u624b\u5199\u7235\u58eb\u4e50\u8c31\u6570\u636e\u96c6\u53ca\u5176\u5408\u6210\u56fe\u50cf\u30022. \u6210\u529f\u5f00\u53d1\u4e86\u9488\u5bf9\u7235\u58eb\u4e50\u8c31\u7684OMR\u6a21\u578b\u30023. \u6240\u6709\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\u5747\u5df2\u516c\u5f00\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u63d0\u4f9b\u4e13\u7528\u6570\u636e\u96c6\u548cOMR\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u624b\u5199\u7235\u58eb\u4e50\u8c31OMR\u7684\u72ec\u7279\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u8d44\u6e90\u516c\u5f00\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2509.05363", "pdf": "https://arxiv.org/pdf/2509.05363", "abs": "https://arxiv.org/abs/2509.05363", "authors": ["Lijie Ding", "Changwoo Do"], "title": "SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.MA"], "comment": "8 pages, 7 figures", "summary": "We introduce SasAgent, a multi-agent AI system powered by large language\nmodels (LLMs) that automates small-angle scattering (SAS) data analysis by\nleveraging tools from the SasView software and enables user interaction via\ntext input. SasAgent features a coordinator agent that interprets user prompts\nand delegates tasks to three specialized agents for scattering length density\n(SLD) calculation, synthetic data generation, and experimental data fitting.\nThese agents utilize LLM-friendly tools to execute tasks efficiently. These\ntools, including the model data tool, Retrieval-Augmented Generation (RAG)\ndocumentation tool, bump fitting tool, and SLD calculator tool, are derived\nfrom the SasView Python library. A user-friendly Gradio-based interface\nenhances user accessibility. Through diverse examples, we demonstrate\nSasAgent's ability to interpret complex prompts, calculate SLDs, generate\naccurate scattering data, and fit experimental datasets with high precision.\nThis work showcases the potential of LLM-driven AI systems to streamline\nscientific workflows and enhance automation in SAS research.", "AI": {"tldr": "SasAgent\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408SasView\u5de5\u5177\u5e76\u652f\u6301\u6587\u672c\u4ea4\u4e92\uff0c\u81ea\u52a8\u5316\u5c0f\u89d2\u6563\u5c04(SAS)\u6570\u636e\u5206\u6790\u3002", "motivation": "\u65e8\u5728\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u9a71\u52a8\u7684AI\u7cfb\u7edf\uff0c\u7b80\u5316\u5c0f\u89d2\u6563\u5c04(SAS)\u7814\u7a76\u7684\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u63d0\u9ad8\u5176\u81ea\u52a8\u5316\u6c34\u5e73\u3002", "method": "\u5f15\u5165\u4e86SasAgent\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u5305\u542b\u4e00\u4e2a\u534f\u8c03\u5668\u4ee3\u7406\uff0c\u8d1f\u8d23\u89e3\u91ca\u7528\u6237\u6307\u4ee4\u5e76\u5c06\u4efb\u52a1\u5206\u914d\u7ed9\u4e09\u4e2a\u4e13\u4e1a\u4ee3\u7406\uff08SLD\u8ba1\u7b97\u3001\u5408\u6210\u6570\u636e\u751f\u6210\u3001\u5b9e\u9a8c\u6570\u636e\u62df\u5408\uff09\u3002\u8fd9\u4e9b\u4ee3\u7406\u5229\u7528\u4eceSasView Python\u5e93\u6d3e\u751f\u51fa\u7684LLM\u53cb\u597d\u5de5\u5177\uff08\u5982\u6a21\u578b\u6570\u636e\u5de5\u5177\u3001RAG\u6587\u6863\u5de5\u5177\u3001\u51f9\u51f8\u62df\u5408\u5de5\u5177\u548cSLD\u8ba1\u7b97\u5668\u5de5\u5177\uff09\u6267\u884c\u4efb\u52a1\u3002\u7cfb\u7edf\u901a\u8fc7\u57fa\u4e8eGradio\u7684\u754c\u9762\u589e\u5f3a\u7528\u6237\u53ef\u8bbf\u95ee\u6027\u3002", "result": "SasAgent\u6210\u529f\u5c55\u793a\u4e86\u5176\u89e3\u91ca\u590d\u6742\u6307\u4ee4\u3001\u7cbe\u786e\u8ba1\u7b97\u6563\u5c04\u957f\u5ea6\u5bc6\u5ea6(SLD)\u3001\u751f\u6210\u51c6\u786e\u6563\u5c04\u6570\u636e\u4ee5\u53ca\u9ad8\u7cbe\u5ea6\u62df\u5408\u5b9e\u9a8c\u6570\u636e\u96c6\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u5de5\u4f5c\u5c55\u793a\u4e86LLM\u9a71\u52a8\u7684AI\u7cfb\u7edf\u5728\u7b80\u5316\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u548c\u63d0\u5347SAS\u7814\u7a76\u81ea\u52a8\u5316\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.05478", "pdf": "https://arxiv.org/pdf/2509.05478", "abs": "https://arxiv.org/abs/2509.05478", "authors": ["Jia Wang", "Xiao Wang", "Chi Zhang"], "title": "PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multivariate time series (MTS) are ubiquitous in domains such as healthcare,\nclimate science, and industrial monitoring, but their high dimensionality,\nlimited labeled data, and non-stationary nature pose significant challenges for\nconventional machine learning methods. While recent self-supervised learning\n(SSL) approaches mitigate label scarcity by data augmentations or time\npoint-based contrastive strategy, they neglect the intrinsic periodic structure\nof MTS and fail to capture the dynamic evolution of latent states. We propose\nPLanTS, a periodicity-aware self-supervised learning framework that explicitly\nmodels irregular latent states and their transitions. We first designed a\nperiod-aware multi-granularity patching mechanism and a generalized contrastive\nloss to preserve both instance-level and state-level similarities across\nmultiple temporal resolutions. To further capture temporal dynamics, we design\na next-transition prediction pretext task that encourages representations to\nencode predictive information about future state evolution. We evaluate PLanTS\nacross a wide range of downstream tasks-including multi-class and multi-label\nclassification, forecasting, trajectory tracking and anomaly detection. PLanTS\nconsistently improves the representation quality over existing SSL methods and\ndemonstrates superior runtime efficiency compared to DTW-based methods.", "AI": {"tldr": "PLanTS\u662f\u4e00\u4e2a\u5468\u671f\u6027\u611f\u77e5\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5efa\u6a21\u4e0d\u89c4\u5219\u6f5c\u5728\u72b6\u6001\u53ca\u5176\u8f6c\u6362\uff0c\u63d0\u9ad8\u4e86\u8868\u793a\u8d28\u91cf\u548c\u8fd0\u884c\u6548\u7387\u3002", "motivation": "\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9762\u4e34\u9ad8\u7ef4\u5ea6\u3001\u6807\u6ce8\u6570\u636e\u6709\u9650\u548c\u975e\u5e73\u7a33\u6027\u7b49\u6311\u6218\u3002\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u8003\u8651\u5176\u5185\u5728\u5468\u671f\u6027\u7ed3\u6784\uff0c\u4e5f\u672a\u80fd\u6355\u6349\u6f5c\u5728\u72b6\u6001\u7684\u52a8\u6001\u6f14\u53d8\u3002", "method": "\u63d0\u51fa\u4e86PLanTS\u6846\u67b6\uff0c\u5305\u62ec\uff1a1) \u5468\u671f\u611f\u77e5\u591a\u7c92\u5ea6\u5206\u5757\u673a\u5236\uff1b2) \u5e7f\u4e49\u5bf9\u6bd4\u635f\u5931\uff0c\u4ee5\u5728\u591a\u65f6\u95f4\u5206\u8fa8\u7387\u4e0a\u4fdd\u6301\u5b9e\u4f8b\u7ea7\u548c\u72b6\u6001\u7ea7\u76f8\u4f3c\u6027\uff1b3) \u4e0b\u4e00\u8f6c\u6362\u9884\u6d4b\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u4ee5\u6355\u6349\u65f6\u95f4\u52a8\u6001\u5e76\u7f16\u7801\u672a\u6765\u72b6\u6001\u6f14\u53d8\u7684\u9884\u6d4b\u4fe1\u606f\u3002", "result": "PLanTS\u5728\u591a\u5206\u7c7b\u3001\u591a\u6807\u7b7e\u5206\u7c7b\u3001\u9884\u6d4b\u3001\u8f68\u8ff9\u8ddf\u8e2a\u548c\u5f02\u5e38\u68c0\u6d4b\u7b49\u5e7f\u6cdb\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u6301\u7eed\u63d0\u9ad8\u4e86\u8868\u793a\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002\u4e0e\u57fa\u4e8eDTW\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cPLanTS\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u8fd0\u884c\u65f6\u6548\u7387\u3002", "conclusion": "PLanTS\u901a\u8fc7\u660e\u786e\u5efa\u6a21\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u7684\u5468\u671f\u6027\u548c\u52a8\u6001\u6f5c\u5728\u72b6\u6001\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u8868\u793a\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2509.05553", "pdf": "https://arxiv.org/pdf/2509.05553", "abs": "https://arxiv.org/abs/2509.05553", "authors": ["Serge Lionel Nikiema", "Jordan Samhi", "Micheline B\u00e9n\u00e9dicte Moumoula", "Alb\u00e9rick Euraste Djir\u00e9", "Abdoul Kader Kabor\u00e9", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This research addresses a fundamental question in AI: whether large language\nmodels truly understand concepts or simply recognize patterns. The authors\npropose bidirectional reasoning,the ability to apply transformations in both\ndirections without being explicitly trained on the reverse direction, as a test\nfor genuine understanding. They argue that true comprehension should naturally\nallow reversibility. For example, a model that can change a variable name like\nuserIndex to i should also be able to infer that i represents a user index\nwithout reverse training. The researchers tested current language models and\ndiscovered what they term cognitive specialization: when models are fine-tuned\non forward tasks, their performance on those tasks improves, but their ability\nto reason bidirectionally becomes significantly worse. To address this issue,\nthey developed Contrastive Fine-Tuning (CFT), which trains models using three\ntypes of examples: positive examples that maintain semantic meaning, negative\nexamples with different semantics, and forward-direction obfuscation examples.\nThis approach aims to develop deeper understanding rather than surface-level\npattern recognition and allows reverse capabilities to develop naturally\nwithout explicit reverse training. Their experiments demonstrated that CFT\nsuccessfully achieved bidirectional reasoning, enabling strong reverse\nperformance while maintaining forward task capabilities. The authors conclude\nthat bidirectional reasoning serves both as a theoretical framework for\nassessing genuine understanding and as a practical training approach for\ndeveloping more capable AI systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u53cc\u5411\u63a8\u7406\u4f5c\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u771f\u6b63\u7406\u89e3\u800c\u975e\u4ec5\u8bc6\u522b\u6a21\u5f0f\u7684\u6807\u51c6\u3002\u53d1\u73b0\u73b0\u6709LLMs\u5b58\u5728\u201c\u8ba4\u77e5\u4e13\u5316\u201d\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u5bf9\u6bd4\u5fae\u8c03\uff08CFT\uff09\u65b9\u6cd5\uff0c\u6210\u529f\u4f7f\u6a21\u578b\u5b9e\u73b0\u53cc\u5411\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u6df1\u5ea6\u7406\u89e3\u3002", "motivation": "\u89e3\u51b3\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u771f\u6b63\u7406\u89e3\u6982\u5ff5\u8fd8\u662f\u4ec5\u4ec5\u8bc6\u522b\u6a21\u5f0f\u3002\u7814\u7a76\u8005\u8ba4\u4e3a\uff0c\u771f\u6b63\u7684\u7406\u89e3\u5e94\u81ea\u7136\u5305\u542b\u53ef\u9006\u6027\uff0c\u5373\u65e0\u9700\u53cd\u5411\u8bad\u7ec3\u5373\u53ef\u8fdb\u884c\u53cc\u5411\u8f6c\u6362\u3002", "method": "1. \u63d0\u51fa\u53cc\u5411\u63a8\u7406\u4f5c\u4e3a\u8861\u91cf\u6a21\u578b\u771f\u6b63\u7406\u89e3\u7684\u6807\u51c6\uff0c\u5373\u6a21\u578b\u5e94\u80fd\u5728\u672a\u660e\u786e\u53cd\u5411\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u53cc\u5411\u5e94\u7528\u8f6c\u6362\u30022. \u5f00\u53d1\u4e86\u5bf9\u6bd4\u5fae\u8c03\uff08CFT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u4e09\u7c7b\u4f8b\u5b50\u8bad\u7ec3\u6a21\u578b\uff1a\u4fdd\u6301\u8bed\u4e49\u7684\u6b63\u5411\u4f8b\u5b50\u3001\u4e0d\u540c\u8bed\u4e49\u7684\u8d1f\u5411\u4f8b\u5b50\u4ee5\u53ca\u6b63\u5411\u6df7\u6dc6\u4f8b\u5b50\u3002\u65e8\u5728\u57f9\u517b\u6df1\u5ea6\u7406\u89e3\u5e76\u81ea\u7136\u53d1\u5c55\u53cd\u5411\u80fd\u529b\u3002", "result": "1. \u53d1\u73b0\u5f53\u524dLLMs\u5b58\u5728\u201c\u8ba4\u77e5\u4e13\u5316\u201d\u95ee\u9898\uff1a\u5728\u6b63\u5411\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03\u540e\uff0c\u6b63\u5411\u6027\u80fd\u63d0\u9ad8\uff0c\u4f46\u53cc\u5411\u63a8\u7406\u80fd\u529b\u663e\u8457\u4e0b\u964d\u30022. \u5b9e\u9a8c\u8bc1\u660e\uff0c\u5bf9\u6bd4\u5fae\u8c03\uff08CFT\uff09\u6210\u529f\u5b9e\u73b0\u4e86\u53cc\u5411\u63a8\u7406\uff0c\u4f7f\u6a21\u578b\u5728\u4fdd\u6301\u6b63\u5411\u4efb\u52a1\u80fd\u529b\u7684\u540c\u65f6\uff0c\u4e5f\u5177\u5907\u4e86\u5f3a\u5927\u7684\u53cd\u5411\u6027\u80fd\u3002", "conclusion": "\u53cc\u5411\u63a8\u7406\u65e2\u53ef\u4ee5\u4f5c\u4e3a\u8bc4\u4f30AI\u7cfb\u7edf\u771f\u6b63\u7406\u89e3\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4e5f\u662f\u5f00\u53d1\u66f4\u5f3a\u5927AI\u7cfb\u7edf\u7684\u5b9e\u7528\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2509.05946", "pdf": "https://arxiv.org/pdf/2509.05946", "abs": "https://arxiv.org/abs/2509.05946", "authors": ["Bisheng Wei", "Ruihong Jiang", "Ruichen Zhang", "Yinqiu Liu", "Dusit Niyato", "Yaohua Sun", "Yang Lu", "Yonghui Li", "Shiwen Mao", "Chau Yuen", "Marco Di Renzo", "Mugen Peng"], "title": "Large Language Models for Next-Generation Wireless Network Management: A Survey and Tutorial", "categories": ["cs.NI"], "comment": null, "summary": "The rapid advancement toward sixth-generation (6G) wireless networks has\nsignificantly intensified the complexity and scale of optimization problems,\nincluding resource allocation and trajectory design, often formulated as\ncombinatorial problems in large discrete decision spaces. However, traditional\noptimization methods, such as heuristics and deep reinforcement learning (DRL),\nstruggle to meet the demanding requirements of real-time adaptability,\nscalability, and dynamic handling of user intents in increasingly heterogeneous\nand resource-constrained network environments. Large language models (LLMs)\npresent a transformative paradigm by enabling natural language-driven problem\nformulation, context-aware reasoning, and adaptive solution refinement through\nadvanced semantic understanding and structured reasoning capabilities. This\npaper provides a systematic and comprehensive survey of LLM-enabled\noptimization frameworks tailored for wireless networks. We first introduce\nfoundational design concepts and distinguish LLM-enabled methods from\nconventional optimization paradigms. Subsequently, we critically analyze key\nenabling methodologies, including natural language modeling, solver\ncollaboration, and solution verification processes. Moreover, we explore\nrepresentative case studies to demonstrate LLMs' transformative potential in\npractical scenarios such as optimization formulation, low-altitude economy\nnetworking, and intent networking. Finally, we discuss current research\nchallenges, examine prominent open-source frameworks and datasets, and identify\npromising future directions to facilitate robust, scalable, and trustworthy\nLLM-enabled optimization solutions for next-generation wireless networks.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8d4b\u80fd\u7684\u65e0\u7ebf\u7f51\u7edc\u4f18\u5316\u6846\u67b6\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u65e8\u5728\u89e3\u51b36G\u7f51\u7edc\u4e2d\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u9762\u4e34\u7684\u590d\u6742\u6027\u4e0e\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u77406G\u65e0\u7ebf\u7f51\u7edc\u7684\u53d1\u5c55\uff0c\u8d44\u6e90\u5206\u914d\u548c\u8f68\u8ff9\u8bbe\u8ba1\u7b49\u4f18\u5316\u95ee\u9898\u65e5\u76ca\u590d\u6742\u4e14\u89c4\u6a21\u5e9e\u5927\u3002\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\uff08\u5982\u542f\u53d1\u5f0f\u3001\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff09\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u9002\u5e94\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u52a8\u6001\u7528\u6237\u610f\u56fe\u5904\u7406\u7684\u9700\u6c42\u3002LLMs\u51ed\u501f\u5176\u5148\u8fdb\u7684\u8bed\u4e49\u7406\u89e3\u548c\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\uff0c\u6709\u671b\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u95ee\u9898\u5236\u5b9a\u548c\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\u6765\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u672c\u6587\u91c7\u7528\u7cfb\u7edf\u6027\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5168\u9762\u5206\u6790\u4e86\u4e3a\u65e0\u7ebf\u7f51\u7edc\u5b9a\u5236\u7684LLM\u8d4b\u80fd\u4f18\u5316\u6846\u67b6\u3002\u5177\u4f53\u6db5\u76d6\uff1a\u4ecb\u7ecd\u57fa\u7840\u8bbe\u8ba1\u6982\u5ff5\u3001\u533a\u5206LLM\u4e0e\u4f20\u7edf\u4f18\u5316\u8303\u5f0f\u3001\u6279\u5224\u6027\u5206\u6790\u5173\u952e\u4f7f\u80fd\u65b9\u6cd5\uff08\u5305\u62ec\u81ea\u7136\u8bed\u8a00\u5efa\u6a21\u3001\u6c42\u89e3\u5668\u534f\u4f5c\u548c\u89e3\u51b3\u65b9\u6848\u9a8c\u8bc1\uff09\uff0c\u5e76\u63a2\u8ba8\u4e86\u4ee3\u8868\u6027\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u7efc\u8ff0\u5c55\u793a\u4e86LLMs\u5728\u4f18\u5316\u95ee\u9898\u5236\u5b9a\u3001\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u548c\u610f\u56fe\u7f51\u7edc\u7b49\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u53d8\u9769\u6027\u6f5c\u529b\u3002\u8bba\u6587\u8be6\u7ec6\u5206\u6790\u4e86LLM\u8d4b\u80fd\u4f18\u5316\u6846\u67b6\u7684\u8bbe\u8ba1\u539f\u7406\u3001\u5173\u952e\u6280\u672f\u53ca\u5176\u5728\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u524d\u666f\u3002", "conclusion": "\u8bba\u6587\u8ba8\u8bba\u4e86\u5f53\u524d\u7814\u7a76\u6311\u6218\u3001\u5ba1\u67e5\u4e86\u4e3b\u6d41\u5f00\u6e90\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u5e76\u6307\u660e\u4e86\u672a\u6765\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u671f\u4fc3\u8fdb\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u5f00\u53d1\u7a33\u5065\u3001\u53ef\u6269\u5c55\u548c\u53ef\u4fe1\u8d56\u7684LLM\u8d4b\u80fd\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05333", "pdf": "https://arxiv.org/pdf/2509.05333", "abs": "https://arxiv.org/abs/2509.05333", "authors": ["Junghyun Park", "Tuan Anh Nguyen", "Dugki Min"], "title": "RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Real world deployments often expose modern object recognition models to\ndomain shifts that precipitate a severe drop in accuracy. Such shifts encompass\n(i) variations in low level image statistics, (ii) changes in object pose and\nviewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent\nclasses. To mitigate this degradation, we introduce the Re-Thinking Vision\nLanguage Model (RT-VLM) framework. The foundation of this framework is a unique\nsynthetic dataset generation pipeline that produces images annotated with\n\"4-Clues\": precise bounding boxes, class names, detailed object-level captions,\nand a comprehensive context-level caption for the entire scene. We then perform\nparameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this\nresource. At inference time, a two stage Re-Thinking scheme is executed: the\nmodel first emits its own four clues, then re examines these responses as\nevidence and iteratively corrects them. Across robustness benchmarks that\nisolate individual domain shifts, RT-VLM consistently surpasses strong\nbaselines. These findings indicate that the integration of structured\nmultimodal evidence with an explicit self critique loop constitutes a promising\nroute toward reliable and transferable visual understanding.", "AI": {"tldr": "\u9488\u5bf9\u7269\u4f53\u8bc6\u522b\u6a21\u578b\u5728\u57df\u504f\u79fb\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86Re-Thinking Vision Language Model (RT-VLM) \u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u5305\u542b\u201c4\u79cd\u7ebf\u7d22\u201d\u7684\u5408\u6210\u6570\u636e\u96c6\u5bf9Llama 3.2\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u91c7\u7528\u4e24\u9636\u6bb5\u7684\u201cRe-Thinking\u201d\u81ea\u6211\u4fee\u6b63\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5404\u7c7b\u57df\u504f\u79fb\u9c81\u68d2\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u4ee3\u7269\u4f53\u8bc6\u522b\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5e38\u53d7\u5230\u57df\u504f\u79fb\uff08\u5982\u4f4e\u7ea7\u56fe\u50cf\u7edf\u8ba1\u3001\u7269\u4f53\u59ff\u6001\u3001\u90e8\u5206\u906e\u6321\u548c\u7c7b\u522b\u6df7\u6dc6\uff09\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u51c6\u786e\u7387\u4e25\u91cd\u4e0b\u964d\u3002", "method": "\u5f15\u5165Re-Thinking Vision Language Model (RT-VLM) \u6846\u67b6\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u72ec\u7279\u7684\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u7ba1\u7ebf\uff0c\u4ea7\u751f\u5e26\u6709\u201c4\u79cd\u7ebf\u7d22\u201d\uff08\u8fb9\u754c\u6846\u3001\u7c7b\u522b\u540d\u3001\u7269\u4f53\u7ea7\u63cf\u8ff0\u3001\u573a\u666f\u7ea7\u63cf\u8ff0\uff09\u7684\u56fe\u50cf\u6807\u6ce8\u3002\u968f\u540e\uff0c\u5bf9Llama 3.2 11B Vision Instruct\u6a21\u578b\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u7684\u76d1\u7763\u5fae\u8c03\u3002\u5728\u63a8\u7406\u65f6\uff0c\u6a21\u578b\u6267\u884c\u4e24\u9636\u6bb5\u201cRe-Thinking\u201d\u65b9\u6848\uff1a\u9996\u5148\u751f\u6210\u81ea\u5df1\u7684\u56db\u79cd\u7ebf\u7d22\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u7ebf\u7d22\u4f5c\u4e3a\u8bc1\u636e\u8fdb\u884c\u91cd\u65b0\u5ba1\u67e5\u5e76\u8fed\u4ee3\u4fee\u6b63\u3002", "result": "\u5728\u9694\u79bb\u4e0d\u540c\u57df\u504f\u79fb\u7684\u9c81\u68d2\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRT-VLM\u6a21\u578b\u6301\u7eed\u8d85\u8d8a\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6574\u5408\u7ed3\u6784\u5316\u591a\u6a21\u6001\u8bc1\u636e\u4e0e\u660e\u786e\u7684\u81ea\u6211\u6279\u5224\u5faa\u73af\uff0c\u662f\u5b9e\u73b0\u53ef\u9760\u548c\u53ef\u8fc1\u79fb\u89c6\u89c9\u7406\u89e3\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2509.05375", "pdf": "https://arxiv.org/pdf/2509.05375", "abs": "https://arxiv.org/abs/2509.05375", "authors": ["Arend Hintze"], "title": "Characterizing Fitness Landscape Structures in Prompt Engineering", "categories": ["cs.AI"], "comment": null, "summary": "While prompt engineering has emerged as a crucial technique for optimizing\nlarge language model performance, the underlying optimization landscape remains\npoorly understood. Current approaches treat prompt optimization as a black-box\nproblem, applying sophisticated search algorithms without characterizing the\nlandscape topology they navigate. We present a systematic analysis of fitness\nlandscape structures in prompt engineering using autocorrelation analysis\nacross semantic embedding spaces. Through experiments on error detection tasks\nwith two distinct prompt generation strategies -- systematic enumeration (1,024\nprompts) and novelty-driven diversification (1,000 prompts) -- we reveal\nfundamentally different landscape topologies. Systematic prompt generation\nyields smoothly decaying autocorrelation, while diversified generation exhibits\nnon-monotonic patterns with peak correlation at intermediate semantic\ndistances, indicating rugged, hierarchically structured landscapes.\nTask-specific analysis across 10 error detection categories reveals varying\ndegrees of ruggedness across different error types. Our findings provide an\nempirical foundation for understanding the complexity of optimization in prompt\nengineering landscapes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u63d0\u793a\u5de5\u7a0b\u7684\u4f18\u5316\u666f\u89c2\u7ed3\u6784\uff0c\u53d1\u73b0\u4e0d\u540c\u63d0\u793a\u751f\u6210\u7b56\u7565\u4f1a\u5bfc\u81f4\u622a\u7136\u4e0d\u540c\u7684\u666f\u89c2\u62d3\u6251\uff0c\u4e3a\u7406\u89e3\u5176\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002", "motivation": "\u63d0\u793a\u5de5\u7a0b\u5bf9\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5e95\u5c42\u7684\u4f18\u5316\u666f\u89c2\u4ecd\u77e5\u4e4b\u751a\u5c11\uff1b\u73b0\u6709\u65b9\u6cd5\u5c06\u63d0\u793a\u4f18\u5316\u89c6\u4e3a\u9ed1\u7bb1\u95ee\u9898\uff0c\u672a\u523b\u753b\u5176\u5bfc\u822a\u7684\u666f\u89c2\u62d3\u6251\u3002", "method": "\u91c7\u7528\u81ea\u76f8\u5173\u5206\u6790\u6cd5\uff0c\u8de8\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u7cfb\u7edf\u5206\u6790\u4e86\u63d0\u793a\u5de5\u7a0b\u7684\u9002\u5e94\u5ea6\u666f\u89c2\u7ed3\u6784\u3002\u901a\u8fc7\u5728\u9519\u8bef\u68c0\u6d4b\u4efb\u52a1\u4e0a\u4f7f\u7528\u4e24\u79cd\u63d0\u793a\u751f\u6210\u7b56\u7565\uff08\u7cfb\u7edf\u679a\u4e3e\u548c\u65b0\u9896\u6027\u9a71\u52a8\u591a\u6837\u5316\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u522b\u751f\u6210\u4e861024\u548c1000\u4e2a\u63d0\u793a\u3002", "result": "\u7cfb\u7edf\u63d0\u793a\u751f\u6210\u4ea7\u751f\u5e73\u6ed1\u8870\u51cf\u7684\u81ea\u76f8\u5173\u6027\uff0c\u8868\u660e\u666f\u89c2\u5e73\u6ed1\uff1b\u800c\u591a\u6837\u5316\u751f\u6210\u5219\u8868\u73b0\u51fa\u975e\u5355\u8c03\u6a21\u5f0f\uff0c\u5728\u4e2d\u7b49\u8bed\u4e49\u8ddd\u79bb\u5904\u51fa\u73b0\u5cf0\u503c\u76f8\u5173\u6027\uff0c\u8868\u660e\u666f\u89c2\u5d0e\u5c96\u4e14\u5177\u6709\u5c42\u6b21\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u5bf910\u4e2a\u9519\u8bef\u68c0\u6d4b\u7c7b\u522b\u7684\u4efb\u52a1\u7279\u5f02\u6027\u5206\u6790\u663e\u793a\uff0c\u4e0d\u540c\u9519\u8bef\u7c7b\u578b\u7684\u5d0e\u5c96\u7a0b\u5ea6\u5404\u5f02\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7406\u89e3\u63d0\u793a\u5de5\u7a0b\u4f18\u5316\u666f\u89c2\u7684\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2509.05481", "pdf": "https://arxiv.org/pdf/2509.05481", "abs": "https://arxiv.org/abs/2509.05481", "authors": ["Eric Palanques-Tost", "Hanna Krasowski", "Murat Arcak", "Ron Weiss", "Calin Belta"], "title": "STL-based Optimization of Biomolecular Neural Networks for Regression and Control", "categories": ["cs.LG", "q-bio.MN", "q-bio.QM"], "comment": null, "summary": "Biomolecular Neural Networks (BNNs), artificial neural networks with\nbiologically synthesizable architectures, achieve universal function\napproximation capabilities beyond simple biological circuits. However, training\nBNNs remains challenging due to the lack of target data. To address this, we\npropose leveraging Signal Temporal Logic (STL) specifications to define\ntraining objectives for BNNs. We build on the quantitative semantics of STL,\nenabling gradient-based optimization of the BNN weights, and introduce a\nlearning algorithm that enables BNNs to perform regression and control tasks in\nbiological systems. Specifically, we investigate two regression problems in\nwhich we train BNNs to act as reporters of dysregulated states, and a feedback\ncontrol problem in which we train the BNN in closed-loop with a chronic disease\nmodel, learning to reduce inflammation while avoiding adverse responses to\nexternal infections. Our numerical experiments demonstrate that STL-based\nlearning can solve the investigated regression and control tasks efficiently.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u89c4\u8303\u6765\u8bad\u7ec3\u751f\u7269\u5206\u5b50\u795e\u7ecf\u7f51\u7edc\uff08BNNs\uff09\uff0c\u4ee5\u89e3\u51b3\u5176\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u56de\u5f52\u548c\u63a7\u5236\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u9ad8\u6548\u7684\u8868\u73b0\u3002", "motivation": "\u751f\u7269\u5206\u5b50\u795e\u7ecf\u7f51\u7edc\uff08BNNs\uff09\u5177\u6709\u901a\u7528\u51fd\u6570\u903c\u8fd1\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u76ee\u6807\u6570\u636e\uff0c\u5176\u8bad\u7ec3\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u5229\u7528\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u89c4\u8303\u5b9a\u4e49BNNs\u7684\u8bad\u7ec3\u76ee\u6807\u3002\u57fa\u4e8eSTL\u7684\u91cf\u5316\u8bed\u4e49\uff0c\u5b9e\u73b0BNN\u6743\u91cd\u7684\u68af\u5ea6\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f7fBNNs\u80fd\u591f\u6267\u884c\u751f\u7269\u7cfb\u7edf\u4e2d\u7684\u56de\u5f52\u548c\u63a7\u5236\u4efb\u52a1\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eSTL\u7684\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u6240\u7814\u7a76\u7684\u56de\u5f52\u95ee\u9898\uff08\u4f5c\u4e3a\u5931\u8c03\u72b6\u6001\u7684\u62a5\u544a\u5668\uff09\u548c\u63a7\u5236\u95ee\u9898\uff08\u6162\u6027\u75c5\u6a21\u578b\u4e2d\u7684\u95ed\u73af\u53cd\u9988\u63a7\u5236\uff0c\u51cf\u5c11\u708e\u75c7\u5e76\u907f\u514d\u611f\u67d3\u4e0d\u826f\u53cd\u5e94\uff09\u3002", "conclusion": "\u57fa\u4e8eSTL\u7684\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u9ad8\u6548\u5730\u89e3\u51b3\u751f\u7269\u5206\u5b50\u795e\u7ecf\u7f51\u7edc\u7684\u56de\u5f52\u548c\u63a7\u5236\u4efb\u52a1\u3002"}}
{"id": "2509.05566", "pdf": "https://arxiv.org/pdf/2509.05566", "abs": "https://arxiv.org/abs/2509.05566", "authors": ["Anya Ji", "Claire Augusta Bergey", "Ron Eliav", "Yoav Artzi", "Robert D. Hawkins"], "title": "Ad hoc conventions generalize to new referents", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "How do people talk about things they've never talked about before? One view\nsuggests that a new shared naming system establishes an arbitrary link to a\nspecific target, like proper names that cannot extend beyond their bearers. An\nalternative view proposes that forming a shared way of describing objects\ninvolves broader conceptual alignment, reshaping each individual's semantic\nspace in ways that should generalize to new referents. We test these competing\naccounts in a dyadic communication study (N=302) leveraging the\nrecently-released KiloGram dataset containing over 1,000 abstract tangram\nimages. After pairs of participants coordinated on referential conventions for\none set of images through repeated communication, we measured the extent to\nwhich their descriptions aligned for undiscussed images. We found strong\nevidence for generalization: partners showed increased alignment relative to\ntheir pre-test labels. Generalization also decayed nonlinearly with visual\nsimilarity (consistent with Shepard's law) and was robust across levels of the\nimages' nameability. These findings suggest that ad hoc conventions are not\narbitrary labels but reflect genuine conceptual coordination, with implications\nfor theories of reference and the design of more adaptive language agents.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u4eba\u4eec\u5728\u4ea4\u6d41\u65b0\u4e8b\u7269\u65f6\uff0c\u4f1a\u5f62\u6210\u8d85\u8d8a\u4efb\u610f\u6807\u7b7e\u7684\u66f4\u5e7f\u6cdb\u6982\u5ff5\u534f\u8c03\uff0c\u5176\u63cf\u8ff0\u60ef\u4f8b\u80fd\u591f\u6cdb\u5316\u5230\u672a\u8ba8\u8bba\u7684\u53c2\u7167\u7269\uff0c\u4e14\u6cdb\u5316\u7a0b\u5ea6\u968f\u89c6\u89c9\u76f8\u4f3c\u5ea6\u975e\u7ebf\u6027\u8870\u51cf\u3002", "motivation": "\u63a2\u7a76\u4eba\u4eec\u5982\u4f55\u8ba8\u8bba\u4ece\u672a\u63d0\u53ca\u7684\u4e8b\u7269\u3002\u5b58\u5728\u4e24\u79cd\u5bf9\u7acb\u89c2\u70b9\uff1a\u4e00\u662f\u65b0\u547d\u540d\u7cfb\u7edf\u5efa\u7acb\u4efb\u610f\u7684\u3001\u4e0d\u53ef\u6cdb\u5316\u7684\u94fe\u63a5\uff1b\u4e8c\u662f\u5f62\u6210\u5171\u540c\u63cf\u8ff0\u65b9\u5f0f\u6d89\u53ca\u66f4\u5e7f\u6cdb\u7684\u6982\u5ff5\u5bf9\u9f50\uff0c\u5e76\u53ef\u6cdb\u5316\u5230\u65b0\u6307\u79f0\u7269\u3002\u672c\u7814\u7a76\u65e8\u5728\u68c0\u9a8c\u8fd9\u4e24\u79cd\u89c2\u70b9\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879\u53cc\u4eba\u6c9f\u901a\u7814\u7a76\uff08N=302\uff09\uff0c\u5229\u7528KiloGram\u6570\u636e\u96c6\u4e2d1000\u591a\u5f20\u62bd\u8c61\u4e03\u5de7\u677f\u56fe\u50cf\u3002\u53c2\u4e0e\u8005\u914d\u5bf9\u540e\uff0c\u901a\u8fc7\u91cd\u590d\u6c9f\u901a\u534f\u8c03\u4e00\u7ec4\u56fe\u50cf\u7684\u6307\u79f0\u60ef\u4f8b\uff0c\u7136\u540e\u6d4b\u91cf\u4ed6\u4eec\u5bf9\u672a\u8ba8\u8bba\u56fe\u50cf\u7684\u63cf\u8ff0\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "\u53d1\u73b0\u5f3a\u70c8\u7684\u6cdb\u5316\u8bc1\u636e\uff1a\u4e0e\u9884\u6d4b\u8bd5\u6807\u7b7e\u76f8\u6bd4\uff0c\u5408\u4f5c\u8005\u5728\u63cf\u8ff0\u672a\u8ba8\u8bba\u56fe\u50cf\u65f6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5bf9\u9f50\u5ea6\u3002\u6cdb\u5316\u80fd\u529b\u968f\u89c6\u89c9\u76f8\u4f3c\u5ea6\u975e\u7ebf\u6027\u8870\u51cf\uff08\u7b26\u5408\u8c22\u6cfc\u5fb7\u5b9a\u5f8b\uff09\uff0c\u4e14\u5728\u56fe\u50cf\u53ef\u547d\u540d\u6027\u6c34\u5e73\u4e0a\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u4e34\u65f6\u5f62\u6210\u7684\u7ea6\u5b9a\u5e76\u975e\u4efb\u610f\u6807\u7b7e\uff0c\u800c\u662f\u53cd\u6620\u4e86\u771f\u5b9e\u7684\u6982\u5ff5\u534f\u8c03\u3002\u8fd9\u5bf9\u6307\u79f0\u7406\u8bba\u548c\u8bbe\u8ba1\u66f4\u5177\u9002\u5e94\u6027\u7684\u8bed\u8a00\u667a\u80fd\u4f53\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2509.06049", "pdf": "https://arxiv.org/pdf/2509.06049", "abs": "https://arxiv.org/abs/2509.06049", "authors": ["Andrea Tassi", "Oluwatayo Yetunde Kolawole", "Joan Pujol Roig", "Daniel Warren"], "title": "Optimized Split Computing Framework for Edge and Core Devices", "categories": ["cs.NI"], "comment": "To appear on IEEE Transactions on Vehicular Technology", "summary": "With mobile networks expected to support services with stringent requirements\nthat ensure high-quality user experience, the ability to apply Feed-Forward\nNeural Network (FFNN) models to User Equipment (UE) use cases has become\ncritical. Given that UEs have limited resources, running FFNNs directly on UEs\nis an intrinsically challenging problem. This letter proposes an optimization\nframework for split computing applications where an FFNN model is partitioned\ninto multiple sections, and executed by UEs, edge- and core-located nodes to\nreduce the required UE computational footprint while containing the inference\ntime. An efficient heuristic strategy for solving the optimization problem is\nalso provided. The proposed framework is shown to be robust in heterogeneous\nsettings, eliminating the need for retraining and reducing the UE's memory\n(CPU) footprint by over 33.6% (60%).", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u8d44\u6e90\u53d7\u9650UE\u8fd0\u884c\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4f18\u5316\u6846\u67b6\uff0c\u5c06FFNN\u6a21\u578b\u5206\u5272\u5e76\u5728UE\u3001\u8fb9\u7f18\u548c\u6838\u5fc3\u8282\u70b9\u4e0a\u534f\u540c\u6267\u884c\uff0c\u4ee5\u964d\u4f4eUE\u8ba1\u7b97\u8d1f\u62c5\u548c\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u79fb\u52a8\u7f51\u7edc\u9700\u652f\u6301\u9ad8\u8981\u6c42\u7684\u670d\u52a1\u4ee5\u786e\u4fdd\u9ad8\u8d28\u91cf\u7528\u6237\u4f53\u9a8c\uff0c\u8fd9\u4f7f\u5f97\u5728\u7528\u6237\u8bbe\u5907\uff08UE\uff09\u4e0a\u5e94\u7528\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff08FFNN\uff09\u6a21\u578b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0cUE\u8d44\u6e90\u6709\u9650\uff0c\u76f4\u63a5\u5728\u5176\u4e0a\u8fd0\u884cFFNN\u9762\u4e34\u56fa\u6709\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u5206\u4f53\u5f0f\u8ba1\u7b97\u5e94\u7528\u7684\u4f18\u5316\u6846\u67b6\uff0c\u5c06FFNN\u6a21\u578b\u5212\u5206\u4e3a\u591a\u4e2a\u90e8\u5206\uff0c\u7531UE\u3001\u8fb9\u7f18\u548c\u6838\u5fc3\u8282\u70b9\u5171\u540c\u6267\u884c\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u542f\u53d1\u5f0f\u7b56\u7565\u6765\u89e3\u51b3\u8be5\u4f18\u5316\u95ee\u9898\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11UE\u6240\u9700\u7684\u8ba1\u7b97\u8db3\u8ff9\uff0c\u540c\u65f6\u63a7\u5236\u63a8\u7406\u65f6\u95f4\u3002\u8be5\u6846\u67b6\u5728\u5f02\u6784\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u6d88\u9664\u4e86\u518d\u8bad\u7ec3\u7684\u9700\u6c42\uff0c\u5e76\u4f7fUE\u7684\u5185\u5b58\uff08CPU\uff09\u5360\u7528\u5206\u522b\u51cf\u5c11\u4e86\u8d85\u8fc733.6%\uff0860%\uff09\u3002", "conclusion": "\u901a\u8fc7\u5c06FFNN\u6a21\u578b\u5206\u5272\u5e76\u5728UE\u3001\u8fb9\u7f18\u548c\u6838\u5fc3\u8282\u70b9\u4e0a\u534f\u540c\u6267\u884c\u7684\u4f18\u5316\u6846\u67b6\uff0c\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u8d44\u6e90\u53d7\u9650UE\u7684\u8ba1\u7b97\u8d1f\u62c5\u548c\u63a8\u7406\u65f6\u95f4\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u79fb\u52a8\u7f51\u7edc\u670d\u52a1\u652f\u6301\u3002"}}
{"id": "2509.05334", "pdf": "https://arxiv.org/pdf/2509.05334", "abs": "https://arxiv.org/abs/2509.05334", "authors": ["Diwen Huang"], "title": "A Real-Time, Vision-Based System for Badminton Smash Speed Estimation on Mobile Devices", "categories": ["cs.CV", "cs.MM", "H.5.1; I.2.10"], "comment": "6 pages, 3 figures, 1 table. Independent research preprint", "summary": "Performance metrics in sports, such as shot speed and angle, provide crucial\nfeedback for athlete development. However, the technology to capture these\nmetrics has historically been expensive, complex, and largely inaccessible to\namateur and recreational players. This paper addresses this gap in the context\nof badminton, one of the world's most popular sports, by introducing a novel,\ncost-effective, and user-friendly system for measuring smash speed using\nubiquitous smartphone technology. Our approach leverages a custom-trained\nYOLOv5 model for shuttlecock detection, combined with a Kalman filter for\nrobust trajectory tracking. By implementing a video-based kinematic speed\nestimation method with spatiotemporal scaling, the system automatically\ncalculates the shuttlecock's velocity from a standard video recording. The\nentire process is packaged into an intuitive mobile application, democratizing\naccess to high-level performance analytics and empowering players at all levels\nto analyze and improve their game.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u3001\u7528\u6237\u53cb\u597d\u7684\u667a\u80fd\u624b\u673a\u7cfb\u7edf\uff0c\u5229\u7528YOLOv5\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\u6280\u672f\u6d4b\u91cf\u7fbd\u6bdb\u7403\u6740\u7403\u901f\u5ea6\uff0c\u5e76\u901a\u8fc7\u79fb\u52a8\u5e94\u7528\u8ba9\u5404\u7ea7\u522b\u8fd0\u52a8\u5458\u90fd\u80fd\u8fdb\u884c\u8868\u73b0\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u8868\u73b0\u6570\u636e\u6355\u83b7\u6280\u672f\u6210\u672c\u9ad8\u6602\u3001\u590d\u6742\u4e14\u4e3b\u8981\u9762\u5411\u4e13\u4e1a\u9009\u624b\uff0c\u4e1a\u4f59\u548c\u5a31\u4e50\u6027\u9009\u624b\u96be\u4ee5\u83b7\u5f97\u3002", "method": "\u8be5\u7cfb\u7edf\u91c7\u7528\u5b9a\u5236\u8bad\u7ec3\u7684YOLOv5\u6a21\u578b\u8fdb\u884c\u7fbd\u6bdb\u7403\u68c0\u6d4b\uff0c\u7ed3\u5408\u5361\u5c14\u66fc\u6ee4\u6ce2\u8fdb\u884c\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u89c6\u9891\u7684\u8fd0\u52a8\u5b66\u901f\u5ea6\u4f30\u7b97\u65b9\u6cd5\uff08\u5305\u542b\u65f6\u7a7a\u7f29\u653e\uff09\u4ece\u6807\u51c6\u89c6\u9891\u4e2d\u81ea\u52a8\u8ba1\u7b97\u7403\u901f\u3002\u6240\u6709\u529f\u80fd\u96c6\u6210\u5728\u76f4\u89c2\u7684\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u4e2d\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u6b3e\u7528\u6237\u53cb\u597d\u7684\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\uff0c\u80fd\u591f\u81ea\u52a8\u4ece\u667a\u80fd\u624b\u673a\u89c6\u9891\u4e2d\u8ba1\u7b97\u7fbd\u6bdb\u7403\u6740\u7403\u901f\u5ea6\uff0c\u4ece\u800c\u4f7f\u9ad8\u7aef\u6027\u80fd\u5206\u6790\u666e\u53ca\u5316\uff0c\u8d4b\u80fd\u5404\u7ea7\u522b\u8fd0\u52a8\u5458\u5206\u6790\u548c\u63d0\u5347\u81ea\u8eab\u8868\u73b0\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6210\u529f\u586b\u8865\u4e86\u7fbd\u6bdb\u7403\u8fd0\u52a8\u5458\u5728\u53ef\u53ca\u6027\u8868\u73b0\u5206\u6790\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u7ecf\u6d4e\u9ad8\u6548\u7684\u667a\u80fd\u624b\u673a\u6280\u672f\uff0c\u8d4b\u80fd\u5404\u7ea7\u522b\u8fd0\u52a8\u5458\u5206\u6790\u548c\u63d0\u5347\u81ea\u8eab\u8868\u73b0\u3002"}}
{"id": "2509.05378", "pdf": "https://arxiv.org/pdf/2509.05378", "abs": "https://arxiv.org/abs/2509.05378", "authors": ["Andreas Motzfeldt", "Joakim Edin", "Casper L. Christensen", "Christian Hardmeier", "Lars Maal\u00f8e", "Anna Rogers"], "title": "Code Like Humans: A Multi-Agent Solution for Medical Coding", "categories": ["cs.AI", "cs.MA"], "comment": "EMNLP Findings 2025", "summary": "In medical coding, experts map unstructured clinical notes to alphanumeric\ncodes for diagnoses and procedures. We introduce Code Like Humans: a new\nagentic framework for medical coding with large language models. It implements\nofficial coding guidelines for human experts, and it is the first solution that\ncan support the full ICD-10 coding system (+70K labels). It achieves the best\nperformance to date on rare diagnosis codes (fine-tuned discriminative\nclassifiers retain an advantage for high-frequency codes, to which they are\nlimited). Towards future work, we also contribute an analysis of system\nperformance and identify its `blind spots' (codes that are systematically\nundercoded).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cCode Like Humans\u201d\u7684\u4ee3\u7406\u5f0fLLM\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u7597\u7f16\u7801\uff0c\u9996\u6b21\u652f\u6301\u5b8c\u6574\u7684ICD-10\u7cfb\u7edf\uff087\u4e07+\u6807\u7b7e\uff09\uff0c\u5e76\u5728\u7f55\u89c1\u8bca\u65ad\u4ee3\u7801\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u533b\u7597\u7f16\u7801\u5c06\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u6620\u5c04\u5230\u8bca\u65ad\u548c\u7a0b\u5e8f\u4ee3\u7801\uff0c\u8fc7\u7a0b\u590d\u6742\u3002\u73b0\u6709\u7684\u5224\u522b\u5206\u7c7b\u5668\u5728\u5904\u7406\u9ad8\u9891\u4ee3\u7801\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u53d7\u9650\u4e8e\u5176\u8303\u56f4\uff0c\u4e14\u96be\u4ee5\u652f\u6301\u5b8c\u6574\u7684\u7f16\u7801\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u7f55\u89c1\u4ee3\u7801\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u5f15\u5165\u201cCode Like Humans\u201d\u4ee3\u7406\u5f0f\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e76\u5b9e\u73b0\u4e86\u4eba\u7c7b\u4e13\u5bb6\u4f7f\u7528\u7684\u5b98\u65b9\u7f16\u7801\u6307\u5357\u3002\u5b83\u662f\u9996\u4e2a\u80fd\u652f\u6301\u5b8c\u6574ICD-10\u7f16\u7801\u7cfb\u7edf\uff08\u8d85\u8fc77\u4e07\u4e2a\u6807\u7b7e\uff09\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u8be5\u6846\u67b6\u5728\u7f55\u89c1\u8bca\u65ad\u4ee3\u7801\u4e0a\u53d6\u5f97\u4e86\u8fc4\u4eca\u4e3a\u6b62\u7684\u6700\u4f73\u6027\u80fd\u3002\u5c3d\u7ba1\u7ecf\u8fc7\u5fae\u8c03\u7684\u5224\u522b\u5206\u7c7b\u5668\u5728\u9ad8\u9891\u4ee3\u7801\u4e0a\u4ecd\u6709\u4f18\u52bf\uff0c\u4f46\u5176\u5e94\u7528\u53d7\u9650\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u7cfb\u7edf\u6027\u80fd\u5e76\u8bc6\u522b\u4e86\u5176\u201c\u76f2\u70b9\u201d\uff08\u7cfb\u7edf\u6027\u6b20\u7f16\u7801\u7684\u4ee3\u7801\uff09\u3002", "conclusion": "\u201cCode Like Humans\u201d\u6846\u67b6\u5728\u533b\u7597\u7f16\u7801\u81ea\u52a8\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5176\u5bf9\u5b8c\u6574ICD-10\u7cfb\u7edf\u7684\u652f\u6301\u548c\u5728\u7f55\u89c1\u4ee3\u7801\u4e0a\u7684\u4f18\u5f02\u8868\u73b0\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u4fa7\u91cd\u4e8e\u89e3\u51b3\u5df2\u8bc6\u522b\u7684\u7cfb\u7edf\u201c\u76f2\u70b9\u201d\u3002"}}
{"id": "2509.05485", "pdf": "https://arxiv.org/pdf/2509.05485", "abs": "https://arxiv.org/abs/2509.05485", "authors": ["Maksim Kazanskii", "Artem Kasianov"], "title": "Prior Distribution and Model Confidence", "categories": ["cs.LG"], "comment": "10 pages,4 tables, 5 images", "summary": "This paper investigates the impact of training data distribution on the\nperformance of image classification models. By analyzing the embeddings of the\ntraining set, we propose a framework to understand the confidence of model\npredictions on unseen data without the need for retraining. Our approach\nfilters out low-confidence predictions based on their distance from the\ntraining distribution in the embedding space, significantly improving\nclassification accuracy. We demonstrate this on the example of several\nclassification models, showing consistent performance gains across\narchitectures. Furthermore, we show that using multiple embedding models to\nrepresent the training data enables a more robust estimation of confidence, as\ndifferent embeddings capture complementary aspects of the data. Combining these\nembeddings allows for better detection and exclusion of out-of-distribution\nsamples, resulting in further accuracy improvements. The proposed method is\nmodel-agnostic and generalizable, with potential applications beyond computer\nvision, including domains such as Natural Language Processing where prediction\nreliability is critical.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bad\u7ec3\u6570\u636e\u5d4c\u5165\u5206\u5e03\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8bc4\u4f30\u6a21\u578b\u5bf9\u672a\u77e5\u6570\u636e\u7684\u9884\u6d4b\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u901a\u8fc7\u8fc7\u6ee4\u4f4e\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u663e\u8457\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002\u4f7f\u7528\u591a\u4e2a\u5d4c\u5165\u6a21\u578b\u53ef\u8fdb\u4e00\u6b65\u589e\u5f3a\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u548c\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u5bf9\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u7406\u89e3\u6a21\u578b\u5bf9\u672a\u77e5\u6570\u636e\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bad\u7ec3\u96c6\u7684\u5d4c\u5165\uff0c\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\u6765\u7406\u89e3\u6a21\u578b\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u3002\u8be5\u65b9\u6cd5\u6839\u636e\u9884\u6d4b\u4e0e\u8bad\u7ec3\u6570\u636e\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u8ddd\u79bb\u6765\u8fc7\u6ee4\u4f4e\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u3002\u8fdb\u4e00\u6b65\uff0c\u5229\u7528\u591a\u4e2a\u5d4c\u5165\u6a21\u578b\u6765\u8868\u793a\u8bad\u7ec3\u6570\u636e\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u8fd9\u4e9b\u5d4c\u5165\u6765\u66f4\u597d\u5730\u68c0\u6d4b\u548c\u6392\u9664\u5206\u5e03\u5916\u6837\u672c\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u79cd\u5206\u7c7b\u6a21\u578b\u67b6\u6784\u4e0a\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u591a\u4e2a\u5d4c\u5165\u6a21\u578b\u53ef\u4ee5\u66f4\u9c81\u68d2\u5730\u4f30\u8ba1\u7f6e\u4fe1\u5ea6\uff0c\u6355\u83b7\u6570\u636e\u4e92\u8865\u65b9\u9762\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u68c0\u6d4b\u548c\u6392\u9664\u5206\u5e03\u5916\u6837\u672c\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0e\u6a21\u578b\u65e0\u5173\u4e14\u5177\u6709\u666e\u9002\u6027\uff0c\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e4b\u5916\u7684\u9886\u57df\uff08\u5982\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff09\u4e5f\u5177\u6709\u6f5c\u5728\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u9884\u6d4b\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2509.05602", "pdf": "https://arxiv.org/pdf/2509.05602", "abs": "https://arxiv.org/abs/2509.05602", "authors": ["Hongyan Xie", "Yitong Yao", "Yikun Ban", "Zixuan Huang", "Deqing Wang", "Zhenhe Wu", "Haoxiang Su", "Chao Wang", "Shuangyong Song", "Xuelong Li"], "title": "Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation", "categories": ["cs.CL"], "comment": "PrePrint", "summary": "Large language models (LLMs) excel at reasoning tasks but are expensive to\ndeploy. Thus small language models (SLMs) are fine-tuned on CoT data generated\nby LLMs to copy LLMs' abilities. However, these CoT data may include noisy\nrationales that either fail to substantiate the answers or contribute no\nadditional information to support answer prediction, which leads SLMs to\ncapture spurious correlations between questions and answers and compromise the\nquality of reasoning. In this work, we propose Chain-of-Thought Correctness\nPerception Distillation (CoPeD), which aims to improve the reasoning quality of\nthe student model from the perspectives of task setting and data utilization.\nFirstly, we introduce a correctness-aware task setting that encourages the\nstudent model to predict answers based on correct rationales and revise them\nwhen they are incorrect. This setting improves the faithfulness of reasoning\nand allows the model to learn from its mistakes. Then, we propose a\nCorrectness-Aware Weighted loss, which dynamically adjusts the contribution of\neach training instance based on the combined loss of the rationale and the\nanswer. This strategy encourages the model to focus more on samples where the\nrationale offers stronger support for the correct answer. Experiments have\nshown that CoPeD is effective on both in-distribution (IND) and\nout-of-distribution (OOD) benchmark reasoning datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCoPeD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6b63\u786e\u6027\u611f\u77e5\u4efb\u52a1\u8bbe\u7f6e\u548c\u52a0\u6743\u635f\u5931\uff0c\u89e3\u51b3LLM\u751f\u6210\u7684CoT\u6570\u636e\u4e2d\u5b58\u5728\u7684\u566a\u58f0\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8SLM\u7684\u63a8\u7406\u8d28\u91cf\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u5f3a\u4f46\u90e8\u7f72\u6210\u672c\u9ad8\u3002\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u901a\u8fc7LLM\u751f\u6210\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u4ee5\u6a21\u4eff\u5176\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e9bCoT\u6570\u636e\u53ef\u80fd\u5305\u542b\u566a\u58f0\u63a8\u7406\u8fc7\u7a0b\uff0c\u65e0\u6cd5\u6709\u6548\u652f\u6301\u7b54\u6848\u6216\u63d0\u4f9b\u989d\u5916\u4fe1\u606f\uff0c\u5bfc\u81f4SLM\u6355\u6349\u865a\u5047\u5173\u8054\u5e76\u635f\u5bb3\u63a8\u7406\u8d28\u91cf\u3002", "method": "\u6211\u4eec\u63d0\u51faCoT\u6b63\u786e\u6027\u611f\u77e5\u84b8\u998f\uff08CoPeD\uff09\u65b9\u6cd5\uff0c\u4ece\u4efb\u52a1\u8bbe\u7f6e\u548c\u6570\u636e\u5229\u7528\u4e24\u65b9\u9762\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u8d28\u91cf\u3002\u9996\u5148\uff0c\u5f15\u5165\u6b63\u786e\u6027\u611f\u77e5\u4efb\u52a1\u8bbe\u7f6e\uff0c\u9f13\u52b1\u5b66\u751f\u6a21\u578b\u57fa\u4e8e\u6b63\u786e\u63a8\u7406\u8fc7\u7a0b\u9884\u6d4b\u7b54\u6848\uff0c\u5e76\u5728\u9519\u8bef\u65f6\u8fdb\u884c\u4fee\u6b63\uff0c\u4ee5\u63d0\u9ad8\u63a8\u7406\u5fe0\u5b9e\u5ea6\u5e76\u4ece\u9519\u8bef\u4e2d\u5b66\u4e60\u3002\u5176\u6b21\uff0c\u63d0\u51fa\u6b63\u786e\u6027\u611f\u77e5\u52a0\u6743\u635f\u5931\uff0c\u6839\u636e\u63a8\u7406\u8fc7\u7a0b\u548c\u7b54\u6848\u7684\u7ec4\u5408\u635f\u5931\u52a8\u6001\u8c03\u6574\u6bcf\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u7684\u8d21\u732e\uff0c\u4fc3\u4f7f\u6a21\u578b\u66f4\u5173\u6ce8\u63a8\u7406\u8fc7\u7a0b\u80fd\u5f3a\u6709\u529b\u652f\u6301\u6b63\u786e\u7b54\u6848\u7684\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCoPeD\u5728\u5206\u5e03\u5185\uff08IND\uff09\u548c\u5206\u5e03\u5916\uff08OOD\uff09\u7684\u57fa\u51c6\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u5747\u6709\u6548\u3002", "conclusion": "CoPeD\u901a\u8fc7\u6b63\u786e\u6027\u611f\u77e5\u4efb\u52a1\u8bbe\u7f6e\u548c\u6b63\u786e\u6027\u611f\u77e5\u52a0\u6743\u635f\u5931\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u751f\u6210CoT\u6570\u636e\u4e2d\u7684\u566a\u58f0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8d28\u91cf\uff0c\u5e76\u5728\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.06245", "pdf": "https://arxiv.org/pdf/2509.06245", "abs": "https://arxiv.org/abs/2509.06245", "authors": ["Shyam Kumar Shrestha", "Jonathan Kua", "Shiva Raj Pokhrel"], "title": "Understanding BBRv3 Performance in AQM-Enabled WiFi Networks", "categories": ["cs.NI", "cs.ET"], "comment": "The 50th IEEE Conference on Local Computer Networks (LCN) October\n  14-16, 2025, Sydney, Australia", "summary": "We present a modular experimental testbed and lightweight visualization tool\nfor evaluating TCP congestion control performance in wireless networks. We\ncompare Google's latest Bottleneck Bandwidth and Round-trip time version 3\n(BBRv3) algorithm with loss-based CUBIC under varying Active Queue Management\n(AQM) schemes, namely PFIFO, FQ-CoDel, and CAKE, on a Wi-Fi link using a\ncommercial MikroTik router. Our real-time dashboard visualizes metrics such as\nthroughput, latency, and fairness across competing flows. Results show that\nBBRv3 significantly improves fairness and convergence under AQM, especially\nwith FQ-CoDel. Our visualization tool and modular testbed provide a practical\nfoundation for evaluating next-generation TCP variants in real-world\nAQM-enabled home wireless networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u6a21\u5757\u5316\u6d4b\u8bd5\u5e73\u53f0\u548c\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u65e0\u7ebf\u7f51\u7edc\u4e2dTCP\u62e5\u585e\u63a7\u5236\u6027\u80fd\u3002\u901a\u8fc7\u6bd4\u8f83BBRv3\u548cCUBIC\u5728\u4e0d\u540cAQM\u65b9\u6848\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0BBRv3\u663e\u8457\u6539\u5584\u4e86\u516c\u5e73\u6027\u548c\u6536\u655b\u6027\u3002", "motivation": "\u9700\u8981\u5728\u65e0\u7ebf\u7f51\u7edc\u73af\u5883\u4e2d\u8bc4\u4f30TCP\u62e5\u585e\u63a7\u5236\u7b97\u6cd5\uff08\u5c24\u5176\u662fBBRv3\u7b49\u65b0\u53d8\u4f53\uff09\u7684\u6027\u80fd\uff0c\u5e76\u7f3a\u4e4f\u5b9e\u7528\u7684\u6d4b\u8bd5\u5e73\u53f0\u548c\u53ef\u89c6\u5316\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u5b9e\u9a8c\u6d4b\u8bd5\u5e73\u53f0\u548c\u8f7b\u91cf\u7ea7\u53ef\u89c6\u5316\u5de5\u5177\u3002\u5728Wi-Fi\u94fe\u8def\u4e0a\u4f7f\u7528\u5546\u7528MikroTik\u8def\u7531\u5668\uff0c\u6bd4\u8f83\u4e86Google\u7684BBRv3\u7b97\u6cd5\u4e0e\u57fa\u4e8e\u4e22\u5305\u7684CUBIC\u7b97\u6cd5\u5728PFIFO\u3001FQ-CoDel\u548cCAKE\u7b49\u4e0d\u540cAQM\u65b9\u6848\u4e0b\u7684\u6027\u80fd\u3002\u5b9e\u65f6\u4eea\u8868\u677f\u53ef\u89c6\u5316\u4e86\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u548c\u516c\u5e73\u6027\u7b49\u6307\u6807\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0cBBRv3\u5728AQM\u4e0b\u663e\u8457\u6539\u5584\u4e86\u516c\u5e73\u6027\u548c\u6536\u655b\u6027\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528FQ-CoDel\u65b9\u6848\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u6240\u5f00\u53d1\u7684\u6d4b\u8bd5\u5e73\u53f0\u548c\u53ef\u89c6\u5316\u5de5\u5177\u4e3a\u5728\u771f\u5b9eAQM\u5bb6\u5ead\u65e0\u7ebf\u7f51\u7edc\u4e2d\u8bc4\u4f30\u4e0b\u4e00\u4ee3TCP\u53d8\u4f53\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u57fa\u7840\u3002BBRv3\u5728\u8fd9\u4e9b\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u9ad8\u4e86\u516c\u5e73\u6027\u548c\u6536\u655b\u6027\u3002"}}
{"id": "2509.05335", "pdf": "https://arxiv.org/pdf/2509.05335", "abs": "https://arxiv.org/abs/2509.05335", "authors": ["Zebo Xu", "Shaoyun Yu", "Mark Torrance", "Guido Nottbusch", "Nan Zhao", "Zhenguang Cai"], "title": "A Stroke-Level Large-Scale Database of Chinese Character Handwriting and the OpenHandWrite_Toolbox for Handwriting Research", "categories": ["cs.CV"], "comment": null, "summary": "Understanding what linguistic components (e.g., phonological, semantic, and\northographic systems) modulate Chinese handwriting at the character, radical,\nand stroke levels remains an important yet understudied topic. Additionally,\nthere is a lack of comprehensive tools for capturing and batch-processing\nfine-grained handwriting data. To address these issues, we constructed a\nlarge-scale handwriting database in which 42 Chinese speakers for each\nhandwriting 1200 characters in a handwriting-to-dictation task. Additionally,\nwe enhanced the existing handwriting package and provided comprehensive\ndocumentation for the upgraded OpenHandWrite_Toolbox, which can easily modify\nthe experimental design, capture the stroke-level handwriting trajectory, and\nbatch-process handwriting measurements (e.g., latency, duration, and\npen-pressure). In analysing our large-scale database, multiple regression\nresults show that orthographic predictors impact handwriting preparation and\nexecution across character, radical, and stroke levels. Phonological factors\nalso influence execution at all three levels. Importantly, these lexical\neffects demonstrate hierarchical attenuation - they were most pronounced at the\ncharacter level, followed by the radical, and were weakest at the stroke\nlevels. These findings demonstrate that handwriting preparation and execution\nat the radical and stroke levels are closely intertwined with linguistic\ncomponents. This database and toolbox offer valuable resources for future\npsycholinguistic and neurolinguistic research on the handwriting of characters\nand sub-characters across different languages.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5927\u578b\u624b\u5199\u6570\u636e\u5e93\u548c\u5347\u7ea7\u5de5\u5177\uff0c\u63a2\u7d22\u4e86\u8bed\u8a00\u6210\u5206\uff08\u6b63\u5b57\u6cd5\u548c\u8bed\u97f3\uff09\u5bf9\u6c49\u5b57\u624b\u5199\uff08\u5b57\u7b26\u3001\u90e8\u9996\u3001\u7b14\u753b\u5c42\u7ea7\uff09\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u5404\u5c42\u7ea7\u5747\u6709\u4f5c\u7528\uff0c\u4e14\u5f71\u54cd\u5f3a\u5ea6\u5448\u5c42\u7ea7\u8870\u51cf\u3002", "motivation": "\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u54ea\u4e9b\u8bed\u8a00\u6210\u5206\uff08\u5982\u8bed\u97f3\u3001\u8bed\u4e49\u3001\u6b63\u5b57\u6cd5\u7cfb\u7edf\uff09\u8c03\u63a7\u6c49\u5b57\u5728\u5b57\u7b26\u3001\u90e8\u9996\u548c\u7b14\u753b\u5c42\u7ea7\u7684\u624b\u5199\uff0c\u4e14\u7f3a\u4e4f\u6355\u83b7\u548c\u6279\u91cf\u5904\u7406\u7cbe\u7ec6\u624b\u5199\u6570\u636e\u7684\u7efc\u5408\u5de5\u5177\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u578b\u624b\u5199\u6570\u636e\u5e93\uff0c\u5305\u542b42\u540d\u4e2d\u6587\u6bcd\u8bed\u8005\u5b8c\u62101200\u4e2a\u6c49\u5b57\u7684\u542c\u5199\u624b\u5199\u4efb\u52a1\u3002\u540c\u65f6\uff0c\u5347\u7ea7\u4e86OpenHandWrite_Toolbox\uff0c\u4f7f\u5176\u80fd\u591f\u8f7b\u677e\u4fee\u6539\u5b9e\u9a8c\u8bbe\u8ba1\u3001\u6355\u83b7\u7b14\u753b\u7ea7\u624b\u5199\u8f68\u8ff9\uff0c\u5e76\u6279\u91cf\u5904\u7406\u624b\u5199\u6d4b\u91cf\u6570\u636e\uff08\u5982\u6f5c\u4f0f\u671f\u3001\u6301\u7eed\u65f6\u95f4\u3001\u7b14\u538b\uff09\u3002\u4f7f\u7528\u591a\u5143\u56de\u5f52\u5206\u6790\u8be5\u6570\u636e\u5e93\u3002", "result": "\u6b63\u5b57\u6cd5\u9884\u6d4b\u56e0\u5b50\u5f71\u54cd\u5b57\u7b26\u3001\u90e8\u9996\u548c\u7b14\u753b\u5404\u5c42\u7ea7\u7684\u624b\u5199\u51c6\u5907\u4e0e\u6267\u884c\u3002\u8bed\u97f3\u56e0\u7d20\u4e5f\u5f71\u54cd\u6240\u6709\u4e09\u4e2a\u5c42\u7ea7\u7684\u6267\u884c\u3002\u8fd9\u4e9b\u8bcd\u6c47\u6548\u5e94\u5448\u73b0\u5c42\u7ea7\u8870\u51cf\uff1a\u5728\u5b57\u7b26\u5c42\u7ea7\u6700\u663e\u8457\uff0c\u5176\u6b21\u662f\u90e8\u9996\uff0c\u5728\u7b14\u753b\u5c42\u7ea7\u6700\u5f31\u3002", "conclusion": "\u90e8\u9996\u548c\u7b14\u753b\u5c42\u7ea7\u7684\u624b\u5199\u51c6\u5907\u4e0e\u6267\u884c\u4e0e\u8bed\u8a00\u6210\u5206\u7d27\u5bc6\u76f8\u5173\u3002\u672c\u7814\u7a76\u7684\u6570\u636e\u5e93\u548c\u5de5\u5177\u4e3a\u672a\u6765\u8de8\u8bed\u8a00\u7684\u5b57\u7b26\u548c\u5b50\u5b57\u7b26\u624b\u5199\u5fc3\u7406\u8bed\u8a00\u5b66\u548c\u795e\u7ecf\u8bed\u8a00\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2509.05381", "pdf": "https://arxiv.org/pdf/2509.05381", "abs": "https://arxiv.org/abs/2509.05381", "authors": ["Madhava Gaikwad"], "title": "Murphys Laws of AI Alignment: Why the Gap Always Wins", "categories": ["cs.AI", "cs.LG", "68T01, 68T20, 68Q87"], "comment": "21 pages", "summary": "Large language models are increasingly aligned to human preferences through\nreinforcement learning from human feedback (RLHF) and related methods such as\nDirect Preference Optimization (DPO), Constitutional AI, and RLAIF. While\neffective, these methods exhibit recurring failure patterns i.e., reward\nhacking, sycophancy, annotator drift, and misgeneralization. We introduce the\nconcept of the Alignment Gap, a unifying lens for understanding recurring\nfailures in feedback-based alignment. Using a KL-tilting formalism, we\nillustrate why optimization pressure tends to amplify divergence between proxy\nrewards and true human intent. We organize these failures into a catalogue of\nMurphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to\nframe trade-offs among optimization strength, value capture, and\ngeneralization. Small-scale empirical studies serve as illustrative support.\nFinally, we propose the MAPS framework (Misspecification, Annotation, Pressure,\nShift) as practical design levers. Our contribution is not a definitive\nimpossibility theorem but a perspective that reframes alignment debates around\nstructural limits and trade-offs, offering clearer guidance for future design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u5bf9\u9f50\u9e3f\u6c9f\u201d\u6982\u5ff5\uff0c\u89e3\u91ca\u4e86\u57fa\u4e8e\u53cd\u9988\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u4e2d\u91cd\u590d\u51fa\u73b0\u7684\u5931\u8d25\u6a21\u5f0f\uff08\u5982\u5956\u52b1\u6b3a\u9a97\u3001\u9519\u8bef\u6cdb\u5316\uff09\uff0c\u5e76\u901a\u8fc7KL\u503e\u659c\u5f62\u5f0f\u3001AI\u5bf9\u9f50\u58a8\u83f2\u5b9a\u5f8b\u548c\u5bf9\u9f50\u4e09\u96be\u56f0\u5883\u7b49\u6846\u67b6\uff0c\u4ece\u7ed3\u6784\u9650\u5236\u548c\u6743\u8861\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6AI\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u63d0\u4f9bMAPS\u6846\u67b6\u4f5c\u4e3a\u5b9e\u7528\u8bbe\u8ba1\u6307\u5bfc\u3002", "motivation": "\u5c3d\u7ba1\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e0e\u4eba\u7c7b\u53cd\u9988\uff08RLHF\uff09\u7b49\u65b9\u6cd5\u5bf9\u9f50\u5927\u578b\u8bed\u8a00\u6a21\u578b\u975e\u5e38\u6709\u6548\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u666e\u904d\u5b58\u5728\u5956\u52b1\u6b3a\u9a97\u3001\u5949\u627f\u3001\u6807\u6ce8\u8005\u6f02\u79fb\u548c\u9519\u8bef\u6cdb\u5316\u7b49\u91cd\u590d\u51fa\u73b0\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "method": ["\u5f15\u5165\u201c\u5bf9\u9f50\u9e3f\u6c9f\u201d\uff08Alignment Gap\uff09\u6982\u5ff5\uff0c\u4f5c\u4e3a\u7406\u89e3\u53cd\u9988\u5f0f\u5bf9\u9f50\u4e2d\u91cd\u590d\u5931\u8d25\u7684\u7edf\u4e00\u89c6\u89d2\u3002", "\u4f7f\u7528KL\u503e\u659c\u5f62\u5f0f\uff08KL-tilting formalism\uff09\u9610\u91ca\u4f18\u5316\u538b\u529b\u5982\u4f55\u653e\u5927\u4ee3\u7406\u5956\u52b1\u4e0e\u771f\u5b9e\u4eba\u7c7b\u610f\u56fe\u4e4b\u95f4\u7684\u5206\u6b67\u3002", "\u5c06\u8fd9\u4e9b\u5931\u8d25\u7ec4\u7ec7\u6210\u201cAI\u5bf9\u9f50\u58a8\u83f2\u5b9a\u5f8b\u201d\uff08Murphys Laws of AI Alignment\uff09\u76ee\u5f55\u3002", "\u63d0\u51fa\u201c\u5bf9\u9f50\u4e09\u96be\u56f0\u5883\u201d\uff08Alignment Trilemma\uff09\uff0c\u4ee5\u6784\u5efa\u4f18\u5316\u5f3a\u5ea6\u3001\u4ef7\u503c\u6355\u83b7\u548c\u6cdb\u5316\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "\u8fdb\u884c\u5c0f\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u4ee5\u63d0\u4f9b\u8bf4\u660e\u6027\u652f\u6301\u3002", "\u63d0\u51faMAPS\u6846\u67b6\uff08Misspecification, Annotation, Pressure, Shift\uff09\u4f5c\u4e3a\u5b9e\u7528\u7684\u8bbe\u8ba1\u6760\u6746\u3002"], "result": ["\u8bc6\u522b\u5e76\u7cfb\u7edf\u5730\u89e3\u91ca\u4e86\u53cd\u9988\u5f0fAI\u5bf9\u9f50\u4e2d\u666e\u904d\u5b58\u5728\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "\u901a\u8fc7\u201c\u5bf9\u9f50\u9e3f\u6c9f\u201d\u548cKL\u503e\u659c\u5f62\u5f0f\uff0c\u63ed\u793a\u4e86\u4f18\u5316\u538b\u529b\u5bfc\u81f4\u4ee3\u7406\u5956\u52b1\u4e0e\u771f\u5b9e\u4eba\u7c7b\u610f\u56fe\u4e4b\u95f4\u53d1\u6563\u7684\u5185\u5728\u673a\u5236\u3002", "\u6784\u5efa\u4e86AI\u5bf9\u9f50\u5931\u8d25\u7684\u201c\u58a8\u83f2\u5b9a\u5f8b\u201d\u76ee\u5f55\u3002", "\u63d0\u51fa\u4e86\u201c\u5bf9\u9f50\u4e09\u96be\u56f0\u5883\u201d\uff0c\u660e\u786e\u4e86\u4f18\u5316\u5f3a\u5ea6\u3001\u4ef7\u503c\u6355\u83b7\u548c\u6cdb\u5316\u4e4b\u95f4\u7684\u7ed3\u6784\u6027\u6743\u8861\u3002", "MAPS\u6846\u67b6\u4e3a\u672a\u6765\u7684\u5bf9\u9f50\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u8bbe\u8ba1\u6760\u6746\u3002"], "conclusion": "\u672c\u7814\u7a76\u5e76\u672a\u63d0\u51fa\u4e00\u4e2a\u660e\u786e\u7684\u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\uff0c\u800c\u662f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u89d2\uff0c\u4ece\u7ed3\u6784\u9650\u5236\u548c\u6743\u8861\u7684\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6AI\u5bf9\u9f50\u7684\u8ba8\u8bba\uff0c\u4e3a\u672a\u6765\u5bf9\u9f50\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u6307\u5bfc\u3002"}}
{"id": "2509.05488", "pdf": "https://arxiv.org/pdf/2509.05488", "abs": "https://arxiv.org/abs/2509.05488", "authors": ["Hongjun Xu", "Junxi Xia", "Weisi Yang", "Yueyuan Sui", "Stephen Xia"], "title": "MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs", "categories": ["cs.LG", "cs.AI", "cs.OS", "C.3; I.2.6; D.2.13; D.4.7"], "comment": "4 pages, 1 figures", "summary": "Deploying Mamba models on microcontrollers (MCUs) remains challenging due to\nlimited memory, the lack of native operator support, and the absence of\nembedded-friendly toolchains. We present, to our knowledge, the first\ndeployment of a Mamba-based neural architecture on a resource-constrained MCU,\na fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline\nmaps a trained PyTorch Mamba model to on-device execution by (1) exporting\nmodel weights into a lightweight format, and (2) implementing a handcrafted\nMamba layer and supporting operators in C with operator fusion and memory\nlayout optimization. MambaLite-Micro eliminates large intermediate tensors,\nreducing 83.0% peak memory, while maintaining an average numerical error of\nonly 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on\nkeyword spotting(KWS) and human activity recognition (HAR) tasks,\nMambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully\npreserving classification accuracy. We further validated portability by\ndeploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating\nconsistent operation across heterogeneous embedded platforms and paving the way\nfor bringing advanced sequence models like Mamba to real-world\nresource-constrained applications.", "AI": {"tldr": "\u9996\u6b21\u5728\u5fae\u63a7\u5236\u5668\u4e0a\u6210\u529f\u90e8\u7f72Mamba\u6a21\u578b\uff0c\u901a\u8fc7C\u8bed\u8a00\u5b9e\u73b0\u7684MambaLite-Micro\u63a8\u7406\u5f15\u64ce\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\u5e76\u4fdd\u6301\u7cbe\u5ea6\u548c\u53ef\u79fb\u690d\u6027\u3002", "motivation": "\u5c06Mamba\u6a21\u578b\u90e8\u7f72\u5230\u5fae\u63a7\u5236\u5668(MCU)\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u5185\u5b58\u6709\u9650\u3001\u7f3a\u4e4f\u539f\u751f\u7b97\u5b50\u652f\u6301\u4ee5\u53ca\u7f3a\u5c11\u5d4c\u5165\u5f0f\u53cb\u597d\u5de5\u5177\u94fe\u3002", "method": "\u5f00\u53d1\u4e86MambaLite-Micro\uff0c\u4e00\u4e2a\u5b8c\u5168\u57fa\u4e8eC\u8bed\u8a00\u4e14\u65e0\u9700\u8fd0\u884c\u65f6\u7684\u63a8\u7406\u5f15\u64ce\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ee5\u4e0b\u6b65\u9aa4\u5c06\u8bad\u7ec3\u597d\u7684PyTorch Mamba\u6a21\u578b\u6620\u5c04\u5230\u8bbe\u5907\u6267\u884c\uff1a1) \u5c06\u6a21\u578b\u6743\u91cd\u5bfc\u51fa\u4e3a\u8f7b\u91cf\u7ea7\u683c\u5f0f\uff1b2) \u4f7f\u7528C\u8bed\u8a00\u624b\u5de5\u5b9e\u73b0Mamba\u5c42\u53ca\u652f\u6301\u7b97\u5b50\uff0c\u5e76\u8fdb\u884c\u7b97\u5b50\u878d\u5408\u548c\u5185\u5b58\u5e03\u5c40\u4f18\u5316\u3002", "result": "MambaLite-Micro\u6d88\u9664\u4e86\u5927\u578b\u4e2d\u95f4\u5f20\u91cf\uff0c\u5c06\u5cf0\u503c\u5185\u5b58\u964d\u4f4e\u4e8683.0%\uff0c\u540c\u65f6\u76f8\u5bf9\u4e8ePyTorch Mamba\u5b9e\u73b0\uff0c\u5e73\u5747\u6570\u503c\u8bef\u5dee\u4ec5\u4e3a1.7x10^-5\u3002\u5728\u5173\u952e\u8bcd\u8bc6\u522b(KWS)\u548c\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b(HAR)\u4efb\u52a1\u4e0a\uff0c\u5176\u5206\u7c7b\u7cbe\u5ea6\u4e0ePyTorch\u57fa\u7ebf\u4fdd\u6301100%\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u8be5\u5f15\u64ce\u5728ESP32S3\u548cSTM32H7\u5fae\u63a7\u5236\u5668\u4e0a\u5747\u5b9e\u73b0\u4e86\u90e8\u7f72\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5f02\u6784\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u7684\u53ef\u79fb\u690d\u6027\u3002", "conclusion": "MambaLite-Micro\u6210\u529f\u5730\u5c06\u5148\u8fdb\u7684Mamba\u5e8f\u5217\u6a21\u578b\u5f15\u5165\u5230\u8d44\u6e90\u53d7\u9650\u7684\u5fae\u63a7\u5236\u5668\u5e94\u7528\u4e2d\uff0c\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.05605", "pdf": "https://arxiv.org/pdf/2509.05605", "abs": "https://arxiv.org/abs/2509.05605", "authors": ["Qiyuan Chen", "Hongsen Huang", "Qian Shao", "Jiahe Chen", "Jintai Chen", "Hongxia Xu", "Renjie Hua", "Ren Chuan", "Jian Wu"], "title": "Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Main", "summary": "Large Language Models (LLMs) require high quality preference datasets to\nalign with human preferences. However, conventional methods for constructing\nsuch datasets face significant challenges: reliance on pre-collected\ninstructions often leads to distribution mismatches with target models, while\nthe need for sampling multiple stochastic responses introduces substantial\ncomputational overhead. In this work, we explore a paradigm shift by leveraging\ninherent regulation of LLMs' representation space for efficient and tailored\npreference dataset construction, named Icon$^{2}$. Specifically, it first\nextracts layer-wise direction vectors to encode sophisticated human preferences\nand then uses these vectors to filter self-synthesized instructions based on\ntheir inherent consistency. During decoding, bidirectional inherent control is\napplied to steer token representations, enabling the precise generation of\nresponse pairs with clear alignment distinctions. Experimental results\ndemonstrate significant improvements in both alignment and efficiency.\nLlama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on\nAlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by\nup to 48.1%.", "AI": {"tldr": "Icon$^{2}$\u901a\u8fc7\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5185\u5728\u8868\u793a\u7a7a\u95f4\uff0c\u9ad8\u6548\u6784\u5efa\u9ad8\u8d28\u91cf\u504f\u597d\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u6548\u679c\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709LLM\u504f\u597d\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\uff1a\u4f9d\u8d56\u9884\u6536\u96c6\u6307\u4ee4\u5bfc\u81f4\u4e0e\u76ee\u6807\u6a21\u578b\u5206\u5e03\u4e0d\u5339\u914d\uff0c\u4ee5\u53ca\u591a\u54cd\u5e94\u91c7\u6837\u5e26\u6765\u5de8\u5927\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u672c\u6587\u63d0\u51faIcon$^{2}$\uff0c\u4e00\u79cd\u901a\u8fc7\u5229\u7528LLM\u8868\u793a\u7a7a\u95f4\u7684\u5185\u5728\u8c03\u8282\u673a\u5236\u6765\u9ad8\u6548\u6784\u5efa\u5b9a\u5236\u5316\u504f\u597d\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u9996\u5148\u63d0\u53d6\u5c42\u7ea7\u65b9\u5411\u5411\u91cf\u4ee5\u7f16\u7801\u590d\u6742\u7684\u4eba\u7c7b\u504f\u597d\uff0c\u7136\u540e\u4f7f\u7528\u8fd9\u4e9b\u5411\u91cf\u8fc7\u6ee4\u81ea\u5408\u6210\u6307\u4ee4\u3002\u5728\u89e3\u7801\u9636\u6bb5\uff0c\u91c7\u7528\u53cc\u5411\u5185\u5728\u63a7\u5236\u6765\u5f15\u5bfctoken\u8868\u793a\uff0c\u4ece\u800c\u7cbe\u786e\u751f\u6210\u5177\u6709\u660e\u786e\u5bf9\u9f50\u5dee\u5f02\u7684\u54cd\u5e94\u5bf9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cIcon$^{2}$\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u9f50\u6548\u679c\u548c\u6548\u7387\u3002\u5728Llama3-8B\u548cQwen2-7B\u6a21\u578b\u4e0a\uff0cAlpacaEval 2.0\u548cArena-Hard\u4e0a\u7684\u5e73\u5747\u80dc\u7387\u5206\u522b\u63d0\u9ad8\u4e8613.89%\u548c13.45%\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e86\u9ad8\u8fbe48.1%\u3002", "conclusion": "Icon$^{2}$\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9a\u5236\u5316\u7684\u504f\u597d\u6570\u636e\u96c6\u6784\u5efa\u65b0\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u80fd\u529b\uff0c\u5e76\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2509.06451", "pdf": "https://arxiv.org/pdf/2509.06451", "abs": "https://arxiv.org/abs/2509.06451", "authors": ["Filippo Bragato", "Tullia Fontana", "Marco Giordani", "Malte Schellmann", "Josef Eichinger", "Michele Zorzi"], "title": "Network-Aware Control of AGVs in an Industrial Scenario: A Simulation Study Based on ROS 2 and Gazebo", "categories": ["cs.NI"], "comment": "This paper has been accepted for publication at IEEE International\n  Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC), 2025", "summary": "Networked Control System (NCS) is a paradigm where sensors, controllers, and\nactuators communicate over a shared network. One promising application of NCS\nis the control of Automated Guided Vehicles (AGVs) in the industrial\nenvironment, for example to transport goods efficiently and to autonomously\nfollow predefined paths or routes. In this context, communication and control\nare tightly correlated, a paradigm referred to as Joint Communication and\nControl (JCC), since network issues such as delays or errors can lead to\nsignificant deviations of the AGVs from the planned trajectory. In this paper,\nwe present a simulation framework based on Gazebo and Robot Operating System 2\n(ROS 2) to simulate and visualize, respectively, the complex interaction\nbetween the control of AGVs and the underlying communication network. This\nframework explicitly incorporates communication metrics, such as delay and\npacket loss, and control metrics, especially the Mean Squared Error (MSE)\nbetween the optimal/desired and actual path of the AGV in response to driving\ncommands. Our results shed light into the correlation between the network\nperformance, particularly Packet Reception Ratio (PRR), and accuracy of\ncontrol.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eGazebo\u548cROS 2\u7684\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u548c\u53ef\u89c6\u5316\u7f51\u7edc\u5316\u63a7\u5236\u7cfb\u7edf\u4e2dAGV\u63a7\u5236\u4e0e\u5e95\u5c42\u901a\u4fe1\u7f51\u7edc\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u5e76\u5206\u6790\u4e86\u7f51\u7edc\u6027\u80fd\u4e0e\u63a7\u5236\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5173\u8054\u3002", "motivation": "\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\uff0cAGV\u7684NCS\u5e94\u7528\u5177\u6709\u524d\u666f\uff0c\u4f46\u901a\u4fe1\u548c\u63a7\u5236\u7684\u7d27\u5bc6\u8026\u5408\uff08JCC\uff09\u610f\u5473\u7740\u7f51\u7edc\u95ee\u9898\uff08\u5982\u5ef6\u8fdf\u3001\u9519\u8bef\uff09\u53ef\u80fd\u5bfc\u81f4AGV\u504f\u79bb\u9884\u5b9a\u8def\u5f84\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7814\u7a76\u8fd9\u79cd\u590d\u6742\u4ea4\u4e92\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eGazebo\u548cROS 2\u7684\u4eff\u771f\u6846\u67b6\uff0c\u660e\u786e\u7eb3\u5165\u4e86\u901a\u4fe1\u6307\u6807\uff08\u5982\u5ef6\u8fdf\u3001\u4e22\u5305\u7387\uff09\u548c\u63a7\u5236\u6307\u6807\uff08AGV\u5b9e\u9645\u8def\u5f84\u4e0e\u671f\u671b\u8def\u5f84\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5deeMSE\uff09\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u7f51\u7edc\u6027\u80fd\uff08\u7279\u522b\u662f\u5305\u63a5\u6536\u7387PRR\uff09\u4e0e\u63a7\u5236\u7cbe\u5ea6\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u4eff\u771f\u6846\u67b6\u6709\u52a9\u4e8e\u7406\u89e3AGV\u5728NCS\u4e2d\u7f51\u7edc\u6027\u80fd\u4e0e\u63a7\u5236\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5173\u8054\u3002"}}
{"id": "2509.05337", "pdf": "https://arxiv.org/pdf/2509.05337", "abs": "https://arxiv.org/abs/2509.05337", "authors": ["Younggeol Cho", "Gokhan Solak", "Olivia Nocentini", "Marta Lorenzini", "Andrea Fortuna", "Arash Ajoudani"], "title": "Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory", "categories": ["cs.CV", "cs.RO"], "comment": "Presented at IEEE RO-MAN 2025", "summary": "Detecting and preventing falls in humans is a critical component of assistive\nrobotic systems. While significant progress has been made in detecting falls,\nthe prediction of falls before they happen, and analysis of the transient state\nbetween stability and an impending fall remain unexplored. In this paper, we\npropose a anticipatory fall detection method that utilizes a hybrid model\ncombining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory\n(LSTM) networks that decoupled the motion prediction and gait classification\ntasks to anticipate falls with high accuracy. Our approach employs real-time\nskeletal features extracted from video sequences as input for the proposed\nmodel. The DGNN acts as a classifier, distinguishing between three gait states:\nstable, transient, and fall. The LSTM-based network then predicts human\nmovement in subsequent time steps, enabling early detection of falls. The\nproposed model was trained and validated using the OUMVLP-Pose and URFD\ndatasets, demonstrating superior performance in terms of prediction error and\nrecognition accuracy compared to models relying solely on DGNN and models from\nliterature. The results indicate that decoupling prediction and classification\nimproves performance compared to addressing the unified problem using only the\nDGNN. Furthermore, our method allows for the monitoring of the transient state,\noffering valuable insights that could enhance the functionality of advanced\nassistance systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408DGNN\u548cLSTM\u7684\u6df7\u5408\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u8026\u8fd0\u52a8\u9884\u6d4b\u548c\u6b65\u6001\u5206\u7c7b\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8dcc\u5012\u9884\u6d4b\u548c\u8fc7\u6e21\u72b6\u6001\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7684\u8dcc\u5012\u68c0\u6d4b\u5728\u9884\u6d4b\u4e8b\u524d\u8dcc\u5012\u548c\u5206\u6790\u7a33\u5b9a\u4e0e\u8dcc\u5012\u4e4b\u95f4\u7684\u8fc7\u6e21\u72b6\u6001\u65b9\u9762\u4e0d\u8db3\uff0c\u800c\u8fd9\u5bf9\u4e8e\u8f85\u52a9\u673a\u5668\u4eba\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e00\u79cd\u89e3\u8026\u8fd0\u52a8\u9884\u6d4b\u548c\u6b65\u6001\u5206\u7c7b\u4efb\u52a1\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DGNN\uff09\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7f51\u7edc\u3002DGNN\u8d1f\u8d23\u5c06\u6b65\u6001\u5206\u7c7b\u4e3a\u7a33\u5b9a\u3001\u8fc7\u6e21\u548c\u8dcc\u5012\u4e09\u79cd\u72b6\u6001\uff0cLSTM\u7f51\u7edc\u5219\u9884\u6d4b\u540e\u7eed\u65f6\u95f4\u6b65\u7684\u4eba\u4f53\u8fd0\u52a8\uff0c\u8f93\u5165\u4e3a\u89c6\u9891\u5e8f\u5217\u4e2d\u63d0\u53d6\u7684\u5b9e\u65f6\u9aa8\u9abc\u7279\u5f81\u3002", "result": "\u8be5\u6a21\u578b\u5728OUMVLP-Pose\u548cURFD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u9884\u6d4b\u8bef\u5dee\u548c\u8bc6\u522b\u51c6\u786e\u6027\u5747\u8d85\u8fc7\u4ec5\u4f7f\u7528DGNN\u7684\u6a21\u578b\u53ca\u73b0\u6709\u6587\u732e\u6a21\u578b\u3002\u7ed3\u679c\u8868\u660e\u89e3\u8026\u9884\u6d4b\u548c\u5206\u7c7b\u80fd\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u6709\u6548\u76d1\u6d4b\u8fc7\u6e21\u72b6\u6001\uff0c\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u6d1e\u5bdf\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408DGNN\u548cLSTM\u5e76\u89e3\u8026\u9884\u6d4b\u4e0e\u5206\u7c7b\u4efb\u52a1\uff0c\u672c\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8dcc\u5012\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u6709\u6548\u76d1\u6d4b\u8dcc\u5012\u524d\u7684\u8fc7\u6e21\u72b6\u6001\uff0c\u4ece\u800c\u589e\u5f3a\u9ad8\u7ea7\u8f85\u52a9\u7cfb\u7edf\u7684\u529f\u80fd\u6027\u3002"}}
{"id": "2509.05469", "pdf": "https://arxiv.org/pdf/2509.05469", "abs": "https://arxiv.org/abs/2509.05469", "authors": ["Chenguang Wang", "Xiang Yan", "Yilong Dai", "Ziyi Wang", "Susu Xu"], "title": "From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation", "categories": ["cs.AI", "cs.CV", "cs.CY", "cs.HC"], "comment": "21 pages, 8 figures", "summary": "Realistic visual renderings of street-design scenarios are essential for\npublic engagement in active transportation planning. Traditional approaches are\nlabor-intensive, hindering collective deliberation and collaborative\ndecision-making. While AI-assisted generative design shows transformative\npotential by enabling rapid creation of design scenarios, existing generative\napproaches typically require large amounts of domain-specific training data and\nstruggle to enable precise spatial variations of design/configuration in\ncomplex street-view scenes. We introduce a multi-agent system that edits and\nredesigns bicycle facilities directly on real-world street-view imagery. The\nframework integrates lane localization, prompt optimization, design generation,\nand automated evaluation to synthesize realistic, contextually appropriate\ndesigns. Experiments across diverse urban scenarios demonstrate that the system\ncan adapt to varying road geometries and environmental conditions, consistently\nyielding visually coherent and instruction-compliant results. This work\nestablishes a foundation for applying multi-agent pipelines to transportation\ninfrastructure planning and facility design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u53ef\u76f4\u63a5\u5728\u771f\u5b9e\u8857\u666f\u56fe\u50cf\u4e0a\u7f16\u8f91\u548c\u91cd\u65b0\u8bbe\u8ba1\u81ea\u884c\u8f66\u8bbe\u65bd\uff0c\u4ee5\u4fc3\u8fdb\u4e3b\u52a8\u4ea4\u901a\u89c4\u5212\u4e2d\u7684\u516c\u4f17\u53c2\u4e0e\u3002", "motivation": "\u4f20\u7edf\u7684\u8857\u666f\u6e32\u67d3\u65b9\u6cd5\u8017\u65f6\u8d39\u529b\uff0c\u963b\u788d\u516c\u4f17\u53c2\u4e0e\u3002\u73b0\u6709AI\u751f\u6210\u8bbe\u8ba1\u867d\u7136\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u5927\u91cf\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\uff0c\u4e14\u96be\u4ee5\u5728\u590d\u6742\u8857\u666f\u4e2d\u5b9e\u73b0\u7cbe\u786e\u7684\u7a7a\u95f4\u8bbe\u8ba1\u53d8\u5316\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u76f4\u63a5\u5728\u771f\u5b9e\u8857\u666f\u56fe\u50cf\u4e0a\u7f16\u8f91\u548c\u91cd\u65b0\u8bbe\u8ba1\u81ea\u884c\u8f66\u8bbe\u65bd\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u8f66\u9053\u5b9a\u4f4d\u3001\u63d0\u793a\u4f18\u5316\u3001\u8bbe\u8ba1\u751f\u6210\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u3002", "result": "\u8be5\u7cfb\u7edf\u5728\u591a\u6837\u5316\u7684\u57ce\u5e02\u573a\u666f\u4e2d\uff0c\u80fd\u9002\u5e94\u4e0d\u540c\u7684\u9053\u8def\u51e0\u4f55\u5f62\u72b6\u548c\u73af\u5883\u6761\u4ef6\uff0c\u6301\u7eed\u751f\u6210\u89c6\u89c9\u4e0a\u8fde\u8d2f\u4e14\u7b26\u5408\u6307\u4ee4\u7684\u7ed3\u679c\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5c06\u591a\u667a\u80fd\u4f53\u7ba1\u9053\u5e94\u7528\u4e8e\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u548c\u8bbe\u65bd\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.05489", "pdf": "https://arxiv.org/pdf/2509.05489", "abs": "https://arxiv.org/abs/2509.05489", "authors": ["Peixuan Han", "Adit Krishnan", "Gerald Friedland", "Jiaxuan You", "Chris Kong"], "title": "Self-Aligned Reward: Towards Effective and Efficient Reasoners", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning with verifiable rewards has significantly advanced\nreasoning in large language models (LLMs), but such signals remain coarse,\noffering only binary correctness feedback. This limitation often results in\ninefficiencies, including overly verbose reasoning and high computational cost,\nwhile existing solutions often compromise accuracy. To address this, we\nintroduce self-aligned reward (SAR), a self-guided signal that complements\nverifiable rewards to encourage both reasoning accuracy and efficiency. SAR is\ndefined as the relative perplexity difference between an answer conditioned on\nthe query and the standalone answer, thereby favoring responses that are\nconcise and query-specific. Quantitative analysis reveals that SAR reliably\ndistinguishes answer quality: concise, correct answers score higher than\nredundant ones, and partially correct answers score higher than entirely\nincorrect ones. Evaluation on 4 models across 7 benchmarks shows that\nintegrating SAR with prevalent RL algorithms like PPO and GRPO improves\naccuracy by 4%, while reducing inference cost by 30%. Further analysis\ndemonstrates that SAR achieves a Pareto-optimal trade-off between correctness\nand efficiency compared to reward signals based on length or self-confidence.\nWe also show that SAR shortens responses while preserving advanced reasoning\nbehaviors, demonstrating its ability to suppress unnecessary elaboration\nwithout losing critical reasoning. These results highlight the promise of\nself-aligned reward as a fine-grained complement to verifiable rewards, paving\nthe way for more efficient and effective LLM training.", "AI": {"tldr": "\u9488\u5bf9LLM\u4e2dRL\u7c97\u7cd9\u7684\u4e8c\u5143\u5956\u52b1\u5bfc\u81f4\u5197\u4f59\u548c\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u81ea\u5bf9\u9f50\u5956\u52b1\uff08SAR\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u56f0\u60d1\u5ea6\u7684\u81ea\u5f15\u5bfc\u4fe1\u53f7\uff0c\u80fd\u66f4\u7ec6\u81f4\u5730\u8bc4\u4f30\u56de\u7b54\u8d28\u91cf\u3002\u7ed3\u5408\u73b0\u6709RL\u7b97\u6cd5\uff0cSAR\u4f7fLLM\u63a8\u7406\u51c6\u786e\u7387\u63d0\u53474%\uff0c\u63a8\u7406\u6210\u672c\u964d\u4f4e30%\uff0c\u5e76\u5b9e\u73b0\u6548\u7387\u4e0e\u51c6\u786e\u5ea6\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u6743\u8861\u3002", "motivation": "LLM\u4e2d\u53ef\u9a8c\u8bc1\u5956\u52b1\u4fe1\u53f7\u8fc7\u4e8e\u7c97\u7cd9\uff08\u4ec5\u63d0\u4f9b\u4e8c\u5143\u6b63\u786e\u6027\u53cd\u9988\uff09\uff0c\u5bfc\u81f4\u63a8\u7406\u5197\u957f\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5e38\u4ee5\u727a\u7272\u51c6\u786e\u6027\u4e3a\u4ee3\u4ef7\u3002", "method": "\u63d0\u51fa\u81ea\u5bf9\u9f50\u5956\u52b1\uff08SAR\uff09\uff0c\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u8865\u5145\u3002SAR\u5b9a\u4e49\u4e3a\u7b54\u6848\u5728\u7ed9\u5b9a\u67e5\u8be2\u6761\u4ef6\u4e0b\u7684\u56f0\u60d1\u5ea6\u4e0e\u72ec\u7acb\u56f0\u60d1\u5ea6\u4e4b\u95f4\u7684\u76f8\u5bf9\u5dee\u5f02\uff0c\u4ee5\u9f13\u52b1\u7b80\u6d01\u548c\u67e5\u8be2\u7279\u5f02\u6027\u7684\u56de\u7b54\u3002SAR\u4e0ePPO\u3001GRPO\u7b49\u4e3b\u6d41RL\u7b97\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002", "result": "1. SAR\u80fd\u6709\u6548\u533a\u5206\u7b54\u6848\u8d28\u91cf\uff0c\u7b80\u6d01\u6b63\u786e\u7684\u7b54\u6848\u5f97\u5206\u66f4\u9ad8\uff0c\u90e8\u5206\u6b63\u786e\u4f18\u4e8e\u5b8c\u5168\u9519\u8bef\u3002\n2. \u7ed3\u5408SAR\u4e0e\u73b0\u6709RL\u7b97\u6cd5\uff0cLLM\u63a8\u7406\u51c6\u786e\u7387\u63d0\u9ad84%\uff0c\u63a8\u7406\u6210\u672c\u964d\u4f4e30%\u3002\n3. \u4e0e\u57fa\u4e8e\u957f\u5ea6\u6216\u81ea\u4fe1\u5ea6\u7684\u5956\u52b1\u4fe1\u53f7\u76f8\u6bd4\uff0cSAR\u5728\u6b63\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u6700\u4f18\u6743\u8861\u3002\n4. SAR\u5728\u7f29\u77ed\u56de\u7b54\u7684\u540c\u65f6\u4fdd\u7559\u4e86\u9ad8\u7ea7\u63a8\u7406\u884c\u4e3a\uff0c\u907f\u514d\u4e86\u4e0d\u5fc5\u8981\u7684\u5197\u4f59\u3002", "conclusion": "\u81ea\u5bf9\u9f50\u5956\u52b1\uff08SAR\uff09\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u7ec6\u7c92\u5ea6\u8865\u5145\uff0c\u524d\u666f\u5e7f\u9614\uff0c\u80fd\u6709\u6548\u63d0\u5347LLM\u8bad\u7ec3\u7684\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u540c\u65f6\u517c\u987e\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.05607", "pdf": "https://arxiv.org/pdf/2509.05607", "abs": "https://arxiv.org/abs/2509.05607", "authors": ["Qiyuan Chen", "Jiahe Chen", "Hongsen Huang", "Qian Shao", "Jintai Chen", "Renjie Hua", "Hongxia Xu", "Ruijia Wu", "Ren Chuan", "Jian Wu"], "title": "Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents", "categories": ["cs.CL"], "comment": "Technical Report", "summary": "The paradigm shift from traditional ranked-based search to Generative Search\nEngines has rendered conventional SEO metrics obsolete, creating an urgent need\nto understand, measure, and optimize for content influence on synthesized\nanswers. This paper introduces a comprehensive, end-to-end framework for\nGenerative Search Engine Optimization (GSEO) to address this challenge. We make\ntwo primary contributions. First, we construct CC-GSEO-Bench, a large-scale,\ncontent-centric benchmark, and propose a multi-dimensional evaluation framework\nthat systematically quantifies influence, moving beyond surface-level\nattribution to assess substantive semantic impact. Second, we design a novel\nmulti-agent system that operationalizes this framework, automating the\nstrategic refinement of content through a collaborative analyze-revise-evaluate\nworkflow. Our empirical analysis using this framework reveals novel insights\ninto the dynamics of content influence, offering actionable strategies for\ncreators and establishing a principled foundation for future GSEO research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u7aef\u5230\u7aef\u751f\u6210\u5f0f\u641c\u7d22\u5f15\u64ce\u4f18\u5316\uff08GSEO\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2a\u5185\u5bb9\u4e2d\u5fc3\u57fa\u51c6\u6d4b\u8bd5CC-GSEO-Bench\u548c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u65e8\u5728\u8861\u91cf\u5e76\u4f18\u5316\u5185\u5bb9\u5bf9\u751f\u6210\u5f0f\u641c\u7d22\u7b54\u6848\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u641c\u7d22\u8303\u5f0f\u4ece\u4f20\u7edf\u6392\u540d\u8f6c\u5411\u751f\u6210\u5f0f\u641c\u7d22\uff0c\u4f20\u7edfSEO\u6307\u6807\u5df2\u8fc7\u65f6\uff0c\u8feb\u5207\u9700\u8981\u7406\u89e3\u3001\u8861\u91cf\u548c\u4f18\u5316\u5185\u5bb9\u5728\u5408\u6210\u7b54\u6848\u4e2d\u7684\u5f71\u54cd\u529b\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684GSEO\u7aef\u5230\u7aef\u6846\u67b6\u3002\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a1) \u6784\u5efa\u4e86\u5927\u89c4\u6a21\u5185\u5bb9\u4e2d\u5fc3\u57fa\u51c6\u6d4b\u8bd5CC-GSEO-Bench\uff0c\u5e76\u63d0\u51fa\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\u4ee5\u91cf\u5316\u5185\u5bb9\u5f71\u54cd\u529b\u7684\u6df1\u5c42\u8bed\u4e49\u6548\u5e94\u30022) \u8bbe\u8ba1\u4e86\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u534f\u4f5c\u6027\u7684\u201c\u5206\u6790-\u4fee\u8ba2-\u8bc4\u4f30\u201d\u5de5\u4f5c\u6d41\uff0c\u81ea\u52a8\u5316\u5730\u8fdb\u884c\u5185\u5bb9\u7b56\u7565\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u8be5\u6846\u67b6\u8fdb\u884c\u7684\u5b9e\u8bc1\u5206\u6790\u63ed\u793a\u4e86\u5185\u5bb9\u5f71\u54cd\u529b\u52a8\u6001\u7684\u65b0\u9896\u89c1\u89e3\uff0c\u4e3a\u5185\u5bb9\u521b\u4f5c\u8005\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u7684GSEO\u7814\u7a76\u5960\u5b9a\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u5e76\u4e3a\u5185\u5bb9\u521b\u4f5c\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7b56\u7565\u3002"}}
{"id": "2509.06454", "pdf": "https://arxiv.org/pdf/2509.06454", "abs": "https://arxiv.org/abs/2509.06454", "authors": ["Julia Caleya-Sanchez", "Pablo Mu\u00f1oz", "Jorge S\u00e1nchez-Garrido", "Emilio Florent\u00edn", "Felix Delgado-Ferro", "Pablo Rodriguez-Martin", "Pablo Ameigeiras"], "title": "Empirical Evaluation of a 5G Transparent Clock for Time Synchronization in a TSN-5G Network", "categories": ["cs.NI"], "comment": "6 pages, 5 figures", "summary": "Time synchronization is essential for industrial IoT and Industry 4.0/5.0\napplications, but achieving high synchronization accuracy in Time-Sensitive\nNetworking (TSN)-5G networks is challenging due to jitter and asymmetric\ndelays. 3GPP TS 23.501 defines three 5G synchronization modes: time-aware\nsystem, boundary clock (BC), and transparent clock (TC), where TC offers a\npromising solution. However, to the best of our knowledge, there is no\nempirical evaluation of TC in a TSN-5G network. This paper empirically\nevaluates an 5G end-to-end TC in a TSN-5G network, implemented on commercial\nTSN switches with a single clock. For TC development, we compute the residence\ntime in 5G and recover the clock domain at the slave node. We deploy a TSN-5G\ntestbed with commercial equipment for synchronization evaluation by modifying\nthe Precision Timing Protocol (PTP) message transmission rates. Experimental\nresults show a peak-to-peak synchronization of 500 ns, meeting the industrial\nrequirement of < 1 us, with minimal synchronization offsets for specific PTP\nmessage transmission rates.", "AI": {"tldr": "\u672c\u6587\u5b9e\u8bc1\u8bc4\u4f30\u4e86TSN-5G\u7f51\u7edc\u4e2d5G\u7aef\u5230\u7aef\u900f\u660e\u65f6\u949f\uff08TC\uff09\u7684\u6027\u80fd\uff0c\u5728\u5546\u4e1a\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86500\u7eb3\u79d2\u7684\u5cf0\u5cf0\u503c\u540c\u6b65\u7cbe\u5ea6\uff0c\u6ee1\u8db3\u4e86\u5de5\u4e1a\u9700\u6c42\u3002", "motivation": "\u5de5\u4e1a\u7269\u8054\u7f51\u548c\u5de5\u4e1a4.0/5.0\u5e94\u7528\u6025\u9700\u65f6\u95f4\u540c\u6b65\uff0c\u4f46TSN-5G\u7f51\u7edc\u4e2d\u7531\u4e8e\u6296\u52a8\u548c\u975e\u5bf9\u79f0\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u540c\u6b65\u6781\u5177\u6311\u6218\u3002\u900f\u660e\u65f6\u949f\uff08TC\uff09\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u76845G\u540c\u6b65\u6a21\u5f0f\uff0c\u76ee\u524d\u7f3a\u4e4f\u5728TSN-5G\u7f51\u7edc\u4e2d\u7684\u5b9e\u8bc1\u8bc4\u4f30\u3002", "method": "\u672c\u6587\u5728\u5546\u7528TSN\u4ea4\u6362\u673a\u4e0a\u5b9e\u73b0\u4e865G\u7aef\u5230\u7aef\u900f\u660e\u65f6\u949f\uff08TC\uff09\uff0c\u5176\u4e2dTC\u5f00\u53d1\u5305\u62ec\u8ba1\u7b975G\u9a7b\u7559\u65f6\u95f4\u5e76\u6062\u590d\u4ece\u8282\u70b9\u65f6\u949f\u57df\u3002\u901a\u8fc7\u90e8\u7f72\u5305\u542b\u5546\u7528\u8bbe\u5907\u7684TSN-5G\u6d4b\u8bd5\u5e8a\uff0c\u5e76\u4fee\u6539PTP\u6d88\u606f\u4f20\u8f93\u901f\u7387\u6765\u8bc4\u4f30\u540c\u6b65\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5cf0\u5cf0\u503c\u540c\u6b65\u7cbe\u5ea6\u8fbe\u5230\u4e86500\u7eb3\u79d2\uff0c\u6ee1\u8db3\u4e86\u5de5\u4e1a\u5e94\u7528\u5c0f\u4e8e1\u5fae\u79d2\u7684\u8981\u6c42\uff0c\u5e76\u5728\u7279\u5b9aPTP\u6d88\u606f\u4f20\u8f93\u901f\u7387\u4e0b\u5b9e\u73b0\u4e86\u6700\u5c0f\u7684\u540c\u6b65\u504f\u79fb\u3002", "conclusion": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5546\u7528\u8bbe\u5907\u4e0a\u5b9e\u73b0\u7684TSN-5G\u7f51\u7edc\u4e2d\u76845G\u7aef\u5230\u7aef\u900f\u660e\u65f6\u949f\uff0c\u80fd\u591f\u8fbe\u5230\u6ee1\u8db3\u5de5\u4e1a\u5e94\u7528\uff08<1\u5fae\u79d2\uff09\u7684\u9ad8\u7cbe\u5ea6\u65f6\u95f4\u540c\u6b65\u8981\u6c42\uff08500\u7eb3\u79d2\u5cf0\u5cf0\u503c\uff09\u3002"}}
{"id": "2509.05340", "pdf": "https://arxiv.org/pdf/2509.05340", "abs": "https://arxiv.org/abs/2509.05340", "authors": ["Dibya Jyoti Bora", "Mrinal Kanti Mishra"], "title": "Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 10 figures", "summary": "Segmentation of brain tumors from Magnetic Resonance Imaging (MRI) remains a\npivotal challenge in medical image analysis due to the heterogeneous nature of\ntumor morphology and intensity distributions. Accurate delineation of tumor\nboundaries is critical for clinical decision-making, radiotherapy planning, and\nlongitudinal disease monitoring. In this study, we perform a comprehensive\ncomparative analysis of two major clustering paradigms applied in MRI tumor\nsegmentation: hard clustering, exemplified by the K-Means algorithm, and soft\nclustering, represented by Fuzzy C-Means (FCM). While K-Means assigns each\npixel strictly to a single cluster, FCM introduces partial memberships, meaning\neach pixel can belong to multiple clusters with varying degrees of association.\nExperimental validation was performed using the BraTS2020 dataset,\nincorporating pre-processing through Gaussian filtering and Contrast Limited\nAdaptive Histogram Equalization (CLAHE). Evaluation metrics included the Dice\nSimilarity Coefficient (DSC) and processing time, which collectively\ndemonstrated that K-Means achieved superior speed with an average runtime of\n0.3s per image, whereas FCM attained higher segmentation accuracy with an\naverage DSC of 0.67 compared to 0.43 for K-Means, albeit at a higher\ncomputational cost (1.3s per image). These results highlight the inherent\ntrade-off between computational efficiency and boundary precision.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86K-Means\u548cFuzzy C-Means\u4e24\u79cd\u805a\u7c7b\u7b97\u6cd5\u5728MRI\u8111\u80bf\u7624\u5206\u5272\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0K-Means\u901f\u5ea6\u66f4\u5feb\u4f46FCM\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u63ed\u793a\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u8fb9\u754c\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u7531\u4e8e\u80bf\u7624\u5f62\u6001\u548c\u5f3a\u5ea6\u5206\u5e03\u7684\u5f02\u8d28\u6027\uff0c\u4eceMRI\u56fe\u50cf\u4e2d\u5206\u5272\u8111\u80bf\u7624\u4ecd\u662f\u4e00\u9879\u5173\u952e\u6311\u6218\u3002\u51c6\u786e\u63cf\u7ed8\u80bf\u7624\u8fb9\u754c\u5bf9\u4e8e\u4e34\u5e8a\u51b3\u7b56\u3001\u653e\u7597\u89c4\u5212\u548c\u75be\u75c5\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u5bf9\u786c\u805a\u7c7b\uff08K-Means\uff09\u548c\u8f6f\u805a\u7c7b\uff08Fuzzy C-Means\uff09\u4e24\u79cd\u4e3b\u8981\u7684\u805a\u7c7b\u8303\u5f0f\u5728MRI\u80bf\u7624\u5206\u5272\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u7efc\u5408\u6bd4\u8f83\u5206\u6790\u3002\u5b9e\u9a8c\u4f7f\u7528BraTS2020\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u56fe\u50cf\u8fdb\u884c\u4e86\u9ad8\u65af\u6ee4\u6ce2\u548cCLAHE\u9884\u5904\u7406\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ecDice\u76f8\u4f3c\u7cfb\u6570\uff08DSC\uff09\u548c\u5904\u7406\u65f6\u95f4\u3002", "result": "K-Means\u7b97\u6cd5\u5728\u901f\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u6bcf\u5f20\u56fe\u50cf\u5904\u7406\u65f6\u95f4\u4e3a0.3\u79d2\uff1b\u800cFCM\u7b97\u6cd5\u5728\u5206\u5272\u7cbe\u5ea6\u4e0a\u66f4\u9ad8\uff0c\u5e73\u5747DSC\u4e3a0.67\uff08K-Means\u4e3a0.43\uff09\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\uff08\u6bcf\u5f20\u56fe\u50cf1.3\u79d2\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8fb9\u754c\u7cbe\u5ea6\u4e4b\u95f4\u56fa\u6709\u7684\u6743\u8861\u5173\u7cfb\u3002"}}
{"id": "2509.05550", "pdf": "https://arxiv.org/pdf/2509.05550", "abs": "https://arxiv.org/abs/2509.05550", "authors": ["Zixi Li"], "title": "TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation", "categories": ["cs.AI"], "comment": "Code available at: https://github.com/lizixi-0x2F/TreeGPT", "summary": "We introduce TreeGPT, a novel neural architecture that combines\ntransformer-based attention mechanisms with global parent-child aggregation for\nprocessing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.\nUnlike traditional approaches that rely solely on sequential processing or\ngraph neural networks, TreeGPT employs a hybrid design that leverages both\nself-attention for capturing local dependencies and a specialized Tree\nFeed-Forward Network (TreeFFN) for modeling hierarchical tree structures\nthrough iterative message passing.\n  The core innovation lies in our Global Parent-Child Aggregation mechanism,\nformalized as: $$h_i^{(t+1)} = \\sigma \\Big( h_i^{(0)} + W_{pc} \\sum_{(p,c) \\in\nE_i} f(h_p^{(t)}, h_c^{(t)}) + b \\Big)$$ where $h_i^{(t)}$ represents the\nhidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges\ninvolving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This\nformulation enables each node to progressively aggregate information from the\nentire tree structure through $T$ iterations.\n  Our architecture integrates optional enhancements including gated aggregation\nwith learnable edge weights, residual connections for gradient stability, and\nbidirectional propagation for capturing both bottom-up and top-down\ndependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging\nvisual reasoning benchmark requiring abstract pattern recognition and rule\ninference. Experimental results demonstrate that TreeGPT achieves 96\\%\naccuracy, significantly outperforming transformer baselines (1.3\\%),\nlarge-scale models like Grok-4 (15.9\\%), and specialized program synthesis\nmethods like SOAR (52\\%) while using only 1.5M parameters. Our comprehensive\nablation study reveals that edge projection is the most critical component,\nwith the combination of edge projection and gating achieving optimal\nperformance.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.05542", "pdf": "https://arxiv.org/pdf/2509.05542", "abs": "https://arxiv.org/abs/2509.05542", "authors": ["Qi Cao", "Pengtao Xie"], "title": "DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training", "categories": ["cs.LG"], "comment": null, "summary": "Training multimodal process reward models (PRMs) is challenged by\ndistribution shifts and noisy data. We introduce DreamPRM-1.5, an\ninstance-reweighted framework that adaptively adjusts the importance of each\ntraining example via bi-level optimization. We design two complementary\nstrategies: Instance Table, effective for smaller datasets, and Instance Net,\nscalable to larger ones. Integrated into test-time scaling, DreamPRM-1.5\nachieves 84.6 accuracy on the MMMU benchmark, surpassing GPT-5.", "AI": {"tldr": "DreamPRM-1.5\u662f\u4e00\u4e2a\u5b9e\u4f8b\u91cd\u52a0\u6743\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u5904\u7406\u591a\u6a21\u6001\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u8bad\u7ec3\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u548c\u566a\u58f0\u6570\u636e\u95ee\u9898\uff0c\u5e76\u5728MMMU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aGPT-5\u3002", "motivation": "\u591a\u6a21\u6001\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u7684\u8bad\u7ec3\u9762\u4e34\u5206\u5e03\u504f\u79fb\u548c\u6570\u636e\u566a\u58f0\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86DreamPRM-1.5\uff0c\u4e00\u4e2a\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u81ea\u9002\u5e94\u8c03\u6574\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u91cd\u8981\u6027\u7684\u5b9e\u4f8b\u91cd\u52a0\u6743\u6846\u67b6\u3002\u8bbe\u8ba1\u4e86\u4e24\u79cd\u4e92\u8865\u7b56\u7565\uff1a\u9002\u7528\u4e8e\u5c0f\u6570\u636e\u96c6\u7684Instance Table\u548c\u53ef\u6269\u5c55\u81f3\u5927\u6570\u636e\u96c6\u7684Instance Net\u3002\u8be5\u65b9\u6cd5\u8fd8\u96c6\u6210\u5230\u6d4b\u8bd5\u65f6\u7f29\u653e\u4e2d\u3002", "result": "DreamPRM-1.5\u5728MMMU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e8684.6%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86GPT-5\u3002", "conclusion": "DreamPRM-1.5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001PRM\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u5e76\u5728MMMU\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.05609", "pdf": "https://arxiv.org/pdf/2509.05609", "abs": "https://arxiv.org/abs/2509.05609", "authors": ["Xugang Lu", "Peng Shen", "Yu Tsao", "Hisashi Kawai"], "title": "New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Aligning acoustic and linguistic representations is a central challenge to\nbridge the pre-trained models in knowledge transfer for automatic speech\nrecognition (ASR). This alignment is inherently structured and asymmetric:\nwhile multiple consecutive acoustic frames typically correspond to a single\nlinguistic token (many-to-one), certain acoustic transition regions may relate\nto multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often\ninclude frames with no linguistic counterpart, such as background noise or\nsilence may lead to imbalanced matching conditions. In this work, we take a new\ninsight to regard alignment and matching as a detection problem, where the goal\nis to identify meaningful correspondences with high precision and recall\nensuring full coverage of linguistic tokens while flexibly handling redundant\nor noisy acoustic frames in transferring linguistic knowledge for ASR. Based on\nthis new insight, we propose an unbalanced optimal transport-based alignment\nmodel that explicitly handles distributional mismatch and structural\nasymmetries with soft and partial matching between acoustic and linguistic\nmodalities. Our method ensures that every linguistic token is grounded in at\nleast one acoustic observation, while allowing for flexible, probabilistic\nmappings from acoustic to linguistic units. We evaluate our proposed model with\nexperiments on an CTC-based ASR system with a pre-trained language model for\nknowledge transfer. Experimental results demonstrate the effectiveness of our\napproach in flexibly controlling degree of matching and hence to improve ASR\nperformance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u975e\u5e73\u8861\u6700\u4f18\u4f20\u8f93\u7684\u5bf9\u9f50\u6a21\u578b\uff0c\u5c06\u58f0\u5b66\u4e0e\u8bed\u8a00\u5bf9\u9f50\u89c6\u4e3a\u68c0\u6d4b\u95ee\u9898\uff0c\u4ee5\u5904\u7406ASR\u4e2d\u4e0d\u5bf9\u79f0\u548c\u566a\u58f0\u5f15\u8d77\u7684\u5bf9\u9f50\u6311\u6218\uff0c\u6709\u6548\u63d0\u5347\u4e86ASR\u6027\u80fd\u3002", "motivation": "\u5728ASR\u4e2d\uff0c\u58f0\u5b66\u4e0e\u8bed\u8a00\u8868\u5f81\u7684\u5bf9\u9f50\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u77e5\u8bc6\u8fc1\u79fb\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u3002\u8fd9\u79cd\u5bf9\u9f50\u5177\u6709\u7ed3\u6784\u6027\u4e0d\u5bf9\u79f0\uff08\u591a\u5bf9\u4e00\u3001\u4e00\u5bf9\u591a\uff09\u548c\u4e0d\u5e73\u8861\u5339\u914d\u6761\u4ef6\uff08\u5982\u80cc\u666f\u566a\u58f0\u6216\u9759\u97f3\u5e27\u6ca1\u6709\u8bed\u8a00\u5bf9\u5e94\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u3002", "method": "\u7814\u7a76\u5c06\u5bf9\u9f50\u548c\u5339\u914d\u89c6\u4e3a\u4e00\u4e2a\u68c0\u6d4b\u95ee\u9898\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u5e73\u8861\u6700\u4f18\u4f20\u8f93\uff08unbalanced optimal transport\uff09\u7684\u5bf9\u9f50\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u663e\u5f0f\u5904\u7406\u5206\u5e03\u4e0d\u5339\u914d\u548c\u7ed3\u6784\u4e0d\u5bf9\u79f0\uff0c\u5e76\u91c7\u7528\u8f6f\u6027\u53ca\u90e8\u5206\u5339\u914d\u7b56\u7565\u3002\u5b83\u786e\u4fdd\u6bcf\u4e2a\u8bed\u8a00\u7b26\u53f7\u81f3\u5c11\u5728\u4e00\u4e2a\u58f0\u5b66\u89c2\u6d4b\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u5e76\u5141\u8bb8\u58f0\u5b66\u5355\u5143\u5230\u8bed\u8a00\u5355\u5143\u7684\u7075\u6d3b\u6982\u7387\u6620\u5c04\u3002\u8be5\u6a21\u578b\u5728\u4e00\u4e2a\u57fa\u4e8eCTC\u7684ASR\u7cfb\u7edf\u4e0a\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u7075\u6d3b\u63a7\u5236\u5339\u914d\u7a0b\u5ea6\uff0c\u4ece\u800c\u6709\u6548\u63d0\u9ad8\u4e86ASR\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5bf9\u9f50\u89c6\u4e3a\u68c0\u6d4b\u95ee\u9898\u5e76\u63d0\u51fa\u975e\u5e73\u8861\u6700\u4f18\u4f20\u8f93\u6a21\u578b\uff0c\u672c\u7814\u7a76\u6210\u529f\u5904\u7406\u4e86ASR\u4e2d\u58f0\u5b66\u4e0e\u8bed\u8a00\u8868\u5f81\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u6709\u6548\u5e94\u5bf9\u4e86\u7ed3\u6784\u4e0d\u5bf9\u79f0\u548c\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u8fdb\u800c\u63d0\u5347\u4e86ASR\u6027\u80fd\u3002"}}
{"id": "2509.06515", "pdf": "https://arxiv.org/pdf/2509.06515", "abs": "https://arxiv.org/abs/2509.06515", "authors": ["Ege Cem Kirci", "Ayush Mishra", "Laurent Vanbever"], "title": "Five Blind Men and the Internet: Towards an Understanding of Internet Traffic", "categories": ["cs.NI"], "comment": "15 pages, 16 figures", "summary": "The Internet, the world's largest and most pervasive network, lacks a\ntransparent, granular view of its traffic patterns, volumes, and growth trends,\nhindering the networking community's understanding of its dynamics. This paper\nleverages publicly available Internet Exchange Point traffic statistics to\naddress this gap, presenting a comprehensive two-year study (2023-2024) from\n472 IXPs worldwide, capturing approximately 300 Tbps of peak daily aggregate\ntraffic by late 2024. Our analysis reveals a 49.2% global traffic increase\n(24.5% annualized), uncovers regionally distinct diurnal patterns and\nevent-driven anomalies, and demonstrates stable utilization rates, reflecting\npredictable infrastructure scaling. By analyzing biases and confirming high\nself-similarity, we establish IXP traffic as a robust proxy for overall\nInternet growth and usage behavior. With transparent, replicable data--covering\n87% of the worldwide IXP port capacity--and plans to release our dataset, this\nstudy offers a verifiable foundation for long-term Internet traffic monitoring.\nIn particular, our findings shed light on the interplay between network design\nand function, providing an accessible framework for researchers and operators\nto explore the Internet's evolving ecosystem.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u5168\u7403472\u4e2aIXP\u4e3a\u671f\u4e24\u5e74\u7684\u6d41\u91cf\u6570\u636e\uff0c\u63ed\u793a\u4e86\u4e92\u8054\u7f51\u6d41\u91cf\u7684\u589e\u957f\u8d8b\u52bf\u3001\u6a21\u5f0f\u548c\u5229\u7528\u7387\uff0c\u5e76\u8bc1\u660eIXP\u6d41\u91cf\u53ef\u4f5c\u4e3a\u8861\u91cf\u6574\u4f53\u4e92\u8054\u7f51\u52a8\u6001\u7684\u53ef\u9760\u4ee3\u7406\u3002", "motivation": "\u5f53\u524d\u4e92\u8054\u7f51\u7f3a\u4e4f\u900f\u660e\u3001\u7ec6\u7c92\u5ea6\u7684\u6d41\u91cf\u6a21\u5f0f\u3001\u5bb9\u91cf\u53ca\u589e\u957f\u8d8b\u52bf\u89c6\u56fe\uff0c\u8fd9\u963b\u788d\u4e86\u7f51\u7edc\u793e\u533a\u5bf9\u4e92\u8054\u7f51\u52a8\u6001\u7684\u7406\u89e3\u3002", "method": "\u5229\u7528\u5168\u7403472\u4e2a\u4e92\u8054\u7f51\u4ea4\u6362\u70b9\uff08IXP\uff09\u7684\u516c\u5f00\u6d41\u91cf\u7edf\u8ba1\u6570\u636e\uff0c\u8fdb\u884c\u4e86\u4e00\u9879\u4e3a\u671f\u4e24\u5e74\u7684\u7efc\u5408\u7814\u7a76\uff082023-2024\u5e74\uff09\uff0c\u8986\u76d6\u4e86\u5168\u740387%\u7684IXP\u7aef\u53e3\u5bb9\u91cf\u3002", "result": "\u5168\u7403\u6d41\u91cf\u589e\u957f\u4e8649.2%\uff08\u5e74\u531624.5%\uff09\uff1b\u53d1\u73b0\u4e86\u533a\u57df\u6027\u65e5\u95f4\u6a21\u5f0f\u548c\u4e8b\u4ef6\u9a71\u52a8\u7684\u5f02\u5e38\uff1b\u5c55\u793a\u4e86\u7a33\u5b9a\u7684\u5229\u7528\u7387\uff0c\u53cd\u6620\u4e86\u53ef\u9884\u6d4b\u7684\u57fa\u7840\u8bbe\u65bd\u6269\u5c55\uff1b\u5e76\u901a\u8fc7\u5206\u6790\u504f\u5dee\u548c\u786e\u8ba4\u9ad8\u5ea6\u81ea\u76f8\u4f3c\u6027\uff0c\u8bc1\u5b9eIXP\u6d41\u91cf\u662f\u4e92\u8054\u7f51\u6574\u4f53\u589e\u957f\u548c\u4f7f\u7528\u884c\u4e3a\u7684\u53ef\u9760\u4ee3\u7406\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u957f\u671f\u4e92\u8054\u7f51\u6d41\u91cf\u76d1\u63a7\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u57fa\u7840\uff0c\u5e76\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u8fd0\u8425\u5546\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8bbf\u95ee\u7684\u6846\u67b6\uff0c\u4ee5\u63a2\u7d22\u4e92\u8054\u7f51\u4e0d\u65ad\u6f14\u8fdb\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u5c24\u5176\u63ed\u793a\u4e86\u7f51\u7edc\u8bbe\u8ba1\u4e0e\u529f\u80fd\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2509.05341", "pdf": "https://arxiv.org/pdf/2509.05341", "abs": "https://arxiv.org/abs/2509.05341", "authors": ["Abhijeet Manoj Pal", "Rajbabu Velmurugan"], "title": "Handling imbalance and few-sample size in ML based Onion disease classification", "categories": ["cs.CV", "cs.LG"], "comment": "6 pages, 8 figures", "summary": "Accurate classification of pests and diseases plays a vital role in precision\nagriculture, enabling efficient identification, targeted interventions, and\npreventing their further spread. However, current methods primarily focus on\nbinary classification, which limits their practical applications, especially in\nscenarios where accurately identifying the specific type of disease or pest is\nessential. We propose a robust deep learning based model for multi-class\nclassification of onion crop diseases and pests. We enhance a pre-trained\nConvolutional Neural Network (CNN) model by integrating attention based modules\nand employing comprehensive data augmentation pipeline to mitigate class\nimbalance. We propose a model which gives 96.90% overall accuracy and 0.96 F1\nscore on real-world field image dataset. This model gives better results than\nother approaches using the same datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u591a\u5206\u7c7b\u6a21\u578b\uff0c\u7528\u4e8e\u7cbe\u786e\u8bc6\u522b\u6d0b\u8471\u4f5c\u7269\u7684\u591a\u79cd\u75c5\u866b\u5bb3\uff0c\u5e76\u53d6\u5f97\u4e8696.90%\u7684\u51c6\u786e\u7387\u548c0.96\u7684F1\u5206\u6570\u3002", "motivation": "\u73b0\u6709\u75c5\u866b\u5bb3\u5206\u7c7b\u65b9\u6cd5\u4e3b\u8981\u9650\u4e8e\u4e8c\u5206\u7c7b\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7cbe\u786e\u8bc6\u522b\u7279\u5b9a\u75c5\u5bb3\u6216\u5bb3\u866b\u7c7b\u578b\u4ee5\u5b9e\u73b0\u7cbe\u51c6\u519c\u4e1a\u7684\u573a\u666f\u4e2d\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u6d0b\u8471\u4f5c\u7269\u75c5\u866b\u5bb3\u7684\u591a\u7c7b\u522b\u5206\u7c7b\u3002\u901a\u8fc7\u96c6\u6210\u57fa\u4e8e\u6ce8\u610f\u529b\u6a21\u5757\u548c\u91c7\u7528\u5168\u9762\u7684\u6570\u636e\u589e\u5f3a\u6d41\u6c34\u7ebf\u6765\u589e\u5f3a\u9884\u8bad\u7ec3\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\uff0c\u4ee5\u51cf\u8f7b\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u8be5\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u7530\u95f4\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8696.90%\u7684\u603b\u4f53\u51c6\u786e\u7387\u548c0.96\u7684F1\u5206\u6570\u3002\u4e0e\u4f7f\u7528\u76f8\u540c\u6570\u636e\u96c6\u7684\u5176\u4ed6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u672c\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u6709\u6548\u3001\u9ad8\u7cbe\u5ea6\u5730\u8fdb\u884c\u6d0b\u8471\u4f5c\u7269\u7684\u591a\u7c7b\u522b\u75c5\u866b\u5bb3\u5206\u7c7b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
