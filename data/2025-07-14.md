<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 54]
- [cs.CV](#cs.CV) [Total: 71]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.NI](#cs.NI) [Total: 9]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [stat.ML](#stat.ML) [Total: 2]
- [cs.CR](#cs.CR) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.CY](#cs.CY) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning](https://arxiv.org/abs/2507.08012)
*Atli Sigurgeirsson,Simon King*

Main category: cs.CL

TL;DR: 现有提示式文本到语音（TTS）模型存在控制受限和输出变异大的问题。本文提出一种新颖的微调方法，通过主成分分析（PCA）识别模型输出中的潜在变异特征，并将其作为新标签进行二次微调，从而提高了模型的语音可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的TTS模型虽然用户友好，但存在两个主要限制：一是控制仅限于训练中暴露的声学特征；二是相同输入会导致不可控的变异。研究旨在解决这些控制受限和不可控变异的问题。

Method: 通过主成分分析（PCA）数千个合成样本，识别解释输出方差最大比例的潜在特征，并将其作为新标签进行二次微调。该方法在一个包含情感披露和另一个不包含情感披露的冰岛语语音语料库训练的TTS模型上进行了评估。

Result: 对于没有情感披露的模型，该方法产生了连续和离散的特征，显著改善了模型的整体可控性。

Conclusion: 本研究通过利用模型的不可控变异并进行二次微调，成功提高了提示式文本到语音模型的控制能力，尤其在非情感语音合成方面表现出积极效果。

Abstract: A Prompt-based Text-To-Speech model allows a user to control different
aspects of speech, such as speaking rate and perceived gender, through natural
language instruction. Although user-friendly, such approaches are on one hand
constrained: control is limited to acoustic features exposed to the model
during training, and too flexible on the other: the same inputs yields
uncontrollable variation that are reflected in the corpus statistics.
  We investigate a novel fine-tuning regime to address both of these issues at
the same time by exploiting the uncontrollable variance of the model. Through
principal component analysis of thousands of synthesised samples, we determine
latent features that account for the highest proportion of the output variance
and incorporate them as new labels for secondary fine-tuning. We evaluate the
proposed methods on two models trained on an expressive Icelandic speech
corpus, one with emotional disclosure and one without. In the case of the model
without emotional disclosure, the method yields both continuous and discrete
features that improve overall controllability of the model.

</details>


### [2] [MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model](https://arxiv.org/abs/2507.08013)
*K. Sahit Reddy,N. Ragavenderan,Vasanth K.,Ganesh N. Naik,Vishalakshi Prabhu,Nagaraja G. S*

Main category: cs.CL

TL;DR: 研究提出并评估了MedicalBERT，一个在大型生物医学数据集上预训练的BERT模型，在多项生物医学NLP任务中表现优于现有BERT基线模型。


<details>
  <summary>Details</summary>
Motivation: 通用预训练语言模型（如BERT、T5、GPT）在NLP中表现优异，但在处理生物医学文献的专业术语时仍面临挑战。现有模型（如Word2Vec、Bi-LSTM）不足，而GPT和T5在需要双向理解的任务中表现欠佳，因此需要专门为生物医学领域优化的模型。

Method: 提出了MedicalBERT，一个在大型生物医学数据集上预训练并配备领域特定词汇的BERT模型。该模型针对命名实体识别、关系提取、问答、句子相似性和文档分类等多种任务进行了优化和微调。使用F1-score、准确率和Pearson相关系数等指标，将MedicalBERT的性能与BioBERT、SciBERT和ClinicalBERT等其他基于BERT的模型进行比较。

Result: MedicalBERT在大多数基准测试中超越了BioBERT、SciBERT和ClinicalBERT等模型。在所有评估任务中，MedicalBERT的平均性能比通用BERT模型高出5.67%。

Conclusion: 该工作强调了利用预训练BERT模型解决医学NLP任务的巨大潜力，并证明了迁移学习技术在捕获领域特定信息方面的有效性。

Abstract: Recent advances in natural language processing (NLP) have been driven
bypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel
at understanding complex texts, but biomedical literature, withits
domain-specific terminology, poses challenges that models likeWord2Vec and
bidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5,
despite capturing context, fall short in tasks needingbidirectional
understanding, unlike BERT. Addressing this, we proposedMedicalBERT, a
pretrained BERT model trained on a large biomedicaldataset and equipped with
domain-specific vocabulary that enhances thecomprehension of biomedical
terminology. MedicalBERT model is furtheroptimized and fine-tuned to address
diverse tasks, including named entityrecognition, relation extraction, question
answering, sentence similarity, anddocument classification. Performance metrics
such as the F1-score,accuracy, and Pearson correlation are employed to showcase
the efficiencyof our model in comparison to other BERT-based models such as
BioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost
of the benchmarks, and surpasses the general-purpose BERT model by5.67% on
average across all the tasks evaluated respectively. This work alsounderscores
the potential of leveraging pretrained BERT models for medicalNLP tasks,
demonstrating the effectiveness of transfer learning techniques incapturing
domain-specific information.
  (PDF) MedicalBERT: enhancing biomedical natural language processing using
pretrained BERT-based model. Available from:
https://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model
[accessed Jul 06 2025].

</details>


### [3] [Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking](https://arxiv.org/abs/2507.08014)
*Aldan Creo,Raul Castro Fernandez,Manuel Cebrian*

Main category: cs.CL

TL;DR: 研究发现，大语言模型越狱攻击的复杂性并未显著高于普通对话，这挑战了攻防军备竞赛的说法，并指出攻击存在自然上限，同时防御机制正在改善。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛部署，理解越狱策略的复杂性及其演变对AI安全至关重要。

Method: 对超过200万个来自越狱社区和通用聊天机器人的真实对话进行了大规模实证分析。采用概率度量、词汇多样性、压缩比和认知负荷等多种复杂性指标，并进行了时间分析。

Result: ['越狱尝试的复杂性并未显著高于普通对话，且此模式在不同用户群体中均成立。', '用户攻击的毒性和复杂性随时间保持稳定，但助手响应的毒性有所下降，表明安全机制正在改进。', '复杂性分布未呈现幂律缩放，暗示越狱开发存在自然限制。']

Conclusion: ['研究结果挑战了攻防之间“不断升级的军备竞赛”的普遍观点，认为LLM安全演变受限于人类独创性，而防御措施持续进步。', '强调了学术界披露越狱攻击的关键信息危害，因过于复杂的攻击可能在防御适应前破坏现有平衡并造成广泛危害。']

Abstract: As large language models (LLMs) become increasingly deployed, understanding
the complexity and evolution of jailbreaking strategies is critical for AI
safety.
  We present a mass-scale empirical analysis of jailbreak complexity across
over 2 million real-world conversations from diverse platforms, including
dedicated jailbreaking communities and general-purpose chatbots. Using a range
of complexity metrics spanning probabilistic measures, lexical diversity,
compression ratios, and cognitive load indicators, we find that jailbreak
attempts do not exhibit significantly higher complexity than normal
conversations. This pattern holds consistently across specialized jailbreaking
communities and general user populations, suggesting practical bounds on attack
sophistication. Temporal analysis reveals that while user attack toxicity and
complexity remains stable over time, assistant response toxicity has decreased,
indicating improving safety mechanisms. The absence of power-law scaling in
complexity distributions further points to natural limits on jailbreak
development.
  Our findings challenge the prevailing narrative of an escalating arms race
between attackers and defenders, instead suggesting that LLM safety evolution
is bounded by human ingenuity constraints while defensive measures continue
advancing. Our results highlight critical information hazards in academic
jailbreak disclosure, as sophisticated attacks exceeding current complexity
baselines could disrupt the observed equilibrium and enable widespread harm
before defensive adaptation.

</details>


### [4] [Assessing the Capabilities and Limitations of FinGPT Model in Financial NLP Applications](https://arxiv.org/abs/2507.08015)
*Prudence Djagba,Chimezie A. Odinakachukwu*

Main category: cs.CL

TL;DR: 评估金融领域大模型FinGPT在六项NLP任务上的表现，发现其在分类任务上与GPT-4相当，但在推理和生成任务上存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 评估金融领域专用语言模型FinGPT在真实金融应用中的能力和局限性。

Method: 在情感分析、文本分类、命名实体识别、金融问答、文本摘要和股票走势预测六个NLP任务上评估FinGPT，使用金融专用数据集，并与GPT-4及人类基准进行比较。

Result: FinGPT在分类任务（如情感分析）上表现强劲，可与GPT-4媲美；但在涉及推理和生成的任务（如金融问答和摘要）上性能显著较低，尤其在数值准确性和复杂推理方面存在明显差距。

Conclusion: FinGPT对某些结构化金融任务有效，但尚未成为全面的解决方案。本研究为未来研究提供了基准，并强调了金融语言模型在架构改进和领域特定优化方面的需求。

Abstract: This work evaluates FinGPT, a financial domain-specific language model,
across six key natural language processing (NLP) tasks: Sentiment Analysis,
Text Classification, Named Entity Recognition, Financial Question Answering,
Text Summarization, and Stock Movement Prediction. The evaluation uses
finance-specific datasets to assess FinGPT's capabilities and limitations in
real-world financial applications. The results show that FinGPT performs
strongly in classification tasks such as sentiment analysis and headline
categorization, often achieving results comparable to GPT-4. However, its
performance is significantly lower in tasks that involve reasoning and
generation, such as financial question answering and summarization. Comparisons
with GPT-4 and human benchmarks highlight notable performance gaps,
particularly in numerical accuracy and complex reasoning. Overall, the findings
indicate that while FinGPT is effective for certain structured financial tasks,
it is not yet a comprehensive solution. This research provides a useful
benchmark for future research and underscores the need for architectural
improvements and domain-specific optimization in financial language models.

</details>


### [5] [Mechanistic Indicators of Understanding in Large Language Models](https://arxiv.org/abs/2507.08017)
*Pierre Beckmann,Matthieu Queloz*

Main category: cs.CL

TL;DR: 本文综合了可解释性（MI）的最新发现，挑战了LLMs仅依赖表层统计的观点，并提出一个三层理论框架来阐述机器理解，认为LLMs展现了多种形式的理解，但其认知架构仍异于人类，呼吁将研究焦点转向其独特的工作方式。


<details>
  <summary>Details</summary>
Motivation: 挑战大型语言模型（LLMs）仅依赖肤浅统计的传统观点，并整合可解释性（MI）领域的最新发现，以提出一个关于机器理解的新颖理论框架。

Method: 综合分析了LLMs可解释性（MI）领域的最新研究成果，并在此基础上提出了一套三层递进的机器理解概念框架，包括：概念理解、世界状态理解和原则性理解。

Result: 研究论证了LLMs能够发展出功能上类似于人类通过“发现联系”而产生的理解的内部结构。具体表现为：形成潜在空间特征（概念理解）、学习偶发性事实联系并追踪变化（世界状态理解），以及发现连接事实的“电路”（原则性理解）。

Conclusion: 尽管LLMs展现了特定形式的理解，但其认知架构与人类存在根本差异。因此，关于LLMs是否理解的争论应转向探讨其独特的思维方式和工作原理。

Abstract: Recent findings in mechanistic interpretability (MI), the field probing the
inner workings of Large Language Models (LLMs), challenge the view that these
models rely solely on superficial statistics. Here, we offer an accessible
synthesis of these findings that doubles as an introduction to MI, all while
integrating these findings within a novel theoretical framework for thinking
about machine understanding. We argue that LLMs develop internal structures
that are functionally analogous to the kind of understanding that consists in
seeing connections. To sharpen this idea, we propose a three-tiered conception
of machine understanding. First, conceptual understanding emerges when a model
forms "features" as directions in latent space, thereby learning the
connections between diverse manifestations of something. Second,
state-of-the-world understanding emerges when a model learns contingent factual
connections between features and dynamically tracks changes in the world.
Third, principled understanding emerges when a model ceases to rely on a
collection of memorized facts and discovers a "circuit" that connects these
facts. However, we conclude by exploring the "parallel mechanisms" phenomenon,
arguing that while LLMs exhibit forms of understanding, their cognitive
architecture remains different from ours, and the debate should shift from
whether LLMs understand to how their strange minds work.

</details>


### [6] [Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation](https://arxiv.org/abs/2507.08018)
*Nikita Mounier,Parsa Idehpour*

Main category: cs.CL

TL;DR: 本文提出R3（Review, Remask, Refine）框架，使迭代文本生成模型能高效识别并纠正自身错误，无需额外训练即可应用于现有预训练模型，从而提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 迭代文本生成面临的主要挑战是如何让模型高效地识别并纠正自身的错误。

Method: 提出R3框架，一个简单而优雅的方法，无需额外模型训练，可应用于任何预训练的掩码文本扩散模型。R3利用过程奖励模型（PRM）对中间生成块进行“Review”评估；根据PRM分数将潜在错误块进行“Remask”（分数越低，重掩码比例越高）；最后，模型被强制“Refine”这些目标片段，将精力集中于过去生成中的次优部分。

Result: 通过这种针对性地细化过程，最终输出的质量得到了提升。

Conclusion: R3框架提供了一种有效且通用的策略，使迭代文本生成模型能够自我纠错，显著提高生成文本的质量。

Abstract: A key challenge for iterative text generation is enabling models to
efficiently identify and correct their own errors. We propose Review, Remask,
Refine (R3), a relatively simple yet elegant framework that requires no
additional model training and can be applied to any pre-trained masked text
diffusion model (e.g., LLaDA or BD3-LM). In R3, a Process Reward Model (PRM) is
utilized for the Review of intermediate generated blocks. The framework then
translates these PRM scores into a Remask strategy: the lower a block's PRM
score, indicating potential mistakes, the greater the proportion of tokens
within that block are remasked. Finally, the model is compelled to Refine these
targeted segments, focusing its efforts more intensively on specific
sub-optimal parts of past generations, leading to improved final output.

</details>


### [7] [Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks](https://arxiv.org/abs/2507.08019)
*Aryan Varshney,Venkat Ram Reddy Ganuthula*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLMs）在简历筛选中表现出可解释但与人类判断显著不同的模式，且其行为在不同模型和上下文中的一致性各异。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型(LLMs)在简历筛选中是表现出一致性行为（信号）还是随机变异（噪声），并将其性能与人类专家进行比较。

Method: 使用受控数据集，测试了三种LLM（Claude、GPT、Gemini），在不同上下文（无公司、跨国公司、初创公司、简化上下文）下，使用相同和随机简历进行评估。研究将LLM表现与三位人类招聘专家进行基准比较，并采用方差分析和配对t检验进行数据分析。

Result: 方差分析显示，在LLM独立条件下，四分之八的条件存在显著均值差异，且LLM与人类评估之间存在持续显著差异（p < 0.01）。配对t检验表明，GPT对公司上下文适应性强，Gemini部分适应，而Claude适应性最小。所有LLM在不同上下文中均与人类专家显著不同。元认知分析显示LLM的适应性权重模式与人类评估方法显著不同。

Conclusion: 研究结果表明，LLMs在提供详细提示时可展现可解释的模式，但其判断与人类判断存在显著差异，这为它们在自动化招聘系统中的部署提供了重要参考。

Abstract: This study investigates whether large language models (LLMs) exhibit
consistent behavior (signal) or random variation (noise) when screening resumes
against job descriptions, and how their performance compares to human experts.
Using controlled datasets, we tested three LLMs (Claude, GPT, and Gemini)
across contexts (No Company, Firm1 [MNC], Firm2 [Startup], Reduced Context)
with identical and randomized resumes, benchmarked against three human
recruitment experts. Analysis of variance revealed significant mean differences
in four of eight LLM-only conditions and consistently significant differences
between LLM and human evaluations (p < 0.01). Paired t-tests showed GPT adapts
strongly to company context (p < 0.001), Gemini partially (p = 0.038 for
Firm1), and Claude minimally (p > 0.1), while all LLMs differed significantly
from human experts across contexts. Meta-cognition analysis highlighted
adaptive weighting patterns that differ markedly from human evaluation
approaches. Findings suggest LLMs offer interpretable patterns with detailed
prompts but diverge substantially from human judgment, informing their
deployment in automated hiring systems.

</details>


### [8] [Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation](https://arxiv.org/abs/2507.08020)
*Zhibo Zhang,Yuxi Li,Kailong Wang,Shuai Yuan,Ling Shi,Haoyu Wang*

Main category: cs.CL

TL;DR: 本研究提出ETTA框架，通过线性变换操纵大型语言模型（LLM）的嵌入空间，以绕过安全对齐机制，实现高攻击成功率，揭示当前安全策略的漏洞。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽广泛应用，但也引入了嵌入空间中毒等安全风险。现有研究对LLM嵌入层的安全对齐理解不足，缺乏针对性的对抗扰动技术，因此需要更深入地探究和利用这一脆弱性。

Method: 本工作提出了ETTA（Embedding Transformation Toxicity Attenuation）框架。该框架通过线性变换，识别并削弱LLM嵌入空间中对毒性敏感的维度。ETTA无需模型微调或访问训练数据，即可绕过模型拒绝行为，同时保持语言连贯性。

Result: 在五种代表性开源LLM上，使用AdvBench基准进行评估，ETTA取得了88.61%的平均攻击成功率，比现有最佳基线高出11.34%。此外，它对安全增强模型也具有泛化性，例如在指令微调防御上仍能达到77.39%的攻击成功率。

Conclusion: 研究结果揭示了当前LLM安全对齐策略中存在的关键漏洞，并强调了开发嵌入感知防御机制的紧迫性和必要性。

Abstract: Large Language Models (LLMs) have achieved remarkable success across domains
such as healthcare, education, and cybersecurity. However, this openness also
introduces significant security risks, particularly through embedding space
poisoning, which is a subtle attack vector where adversaries manipulate the
internal semantic representations of input data to bypass safety alignment
mechanisms. While previous research has investigated universal perturbation
methods, the dynamics of LLM safety alignment at the embedding level remain
insufficiently understood. Consequently, more targeted and accurate adversarial
perturbation techniques, which pose significant threats, have not been
adequately studied.
  In this work, we propose ETTA (Embedding Transformation Toxicity
Attenuation), a novel framework that identifies and attenuates
toxicity-sensitive dimensions in embedding space via linear transformations.
ETTA bypasses model refusal behaviors while preserving linguistic coherence,
without requiring model fine-tuning or access to training data. Evaluated on
five representative open-source LLMs using the AdvBench benchmark, ETTA
achieves a high average attack success rate of 88.61%, outperforming the best
baseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR
on instruction-tuned defenses). These results highlight a critical
vulnerability in current alignment strategies and underscore the need for
embedding-aware defenses.

</details>


### [9] [Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis](https://arxiv.org/abs/2507.08021)
*Li Li,Yongliang Wu,Jingze Zhu,Jiawei Peng,Jianfei Cai,Xu Yang*

Main category: cs.CL

TL;DR: 本文对大型多模态模型（LMMs）中的多模态上下文学习（ICL）进行了全面的内外分析，旨在通过探索示例配置策略和内部注意力机制，以理解和优化图像字幕任务中的ICL。


<details>
  <summary>Details</summary>
Motivation: 尽管上下文学习（ICL）在大型语言模型（LLMs）中取得了成功并已扩展到大型多模态模型（LMMs），但多模态ICL的示例配置探索仍处于初步阶段。同时，控制上下文示例（ICEs）能高效分析LMMs的推理特性，因此需要对多模态ICL进行深入调查。

Method: 本研究采用内外结合的方法，以图像字幕任务为例进行调查。外部方面，通过样本数量、图像检索和字幕分配三个维度探索示例配置策略，并使用多种指标进行评估。内部方面，分析LMMs的典型注意力特征，开发基于注意力的指标量化模型行为，并进行了注意力驱动的模型加速和压缩可行性探索，同时比较了预训练数据特征对性能差异的影响。

Result: 研究揭示了上下文示例（ICEs）配置策略如何通过外部实验影响模型性能，并通过内部检查揭示了典型的特征模式，为理解LMMs中的多模态ICL提供了双重视角。

Conclusion: 本研究通过结合外部和内部分析为理解LMMs中的多模态ICL提供了深刻见解。所提出的结合内外部分析的方法和新指标具有广泛的适用性，可应用于更广泛的研究领域。

Abstract: The evolution of large models has witnessed the emergence of In-Context
Learning (ICL) capabilities. In Natural Language Processing (NLP), numerous
studies have demonstrated the effectiveness of ICL. Inspired by the success of
Large Language Models (LLMs), researchers have developed Large Multimodal
Models (LMMs) with ICL capabilities. However, explorations of demonstration
configuration for multimodal ICL remain preliminary. Additionally, the
controllability of In-Context Examples (ICEs) provides an efficient and
cost-effective means to observe and analyze the inference characteristics of
LMMs under varying inputs. This paper conducts a comprehensive external and
internal investigation of multimodal in-context learning on the image
captioning task. Externally, we explore demonstration configuration strategies
through three dimensions: shot number, image retrieval, and caption assignment.
We employ multiple metrics to systematically and thoroughly evaluate and
summarize key findings. Internally, we analyze typical LMM attention
characteristics and develop attention-based metrics to quantify model
behaviors. We also conduct auxiliary experiments to explore the feasibility of
attention-driven model acceleration and compression. We further compare
performance variations between LMMs with identical model design and pretraining
strategies and explain the differences from the angles of pre-training data
features. Our study reveals both how ICEs configuration strategies impact model
performance through external experiments and characteristic typical patterns
through internal inspection, providing dual perspectives for understanding
multimodal ICL in LMMs. Our method of combining external and internal analysis
to investigate large models, along with our newly proposed metrics, can be
applied to broader research areas.

</details>


### [10] ["Amazing, They All Lean Left" -- Analyzing the Political Temperaments of Current LLMs](https://arxiv.org/abs/2507.08027)
*W. Russell Neuman,Chad Coleman,Ali Dasdan,Safinah Ali,Manan Shah,Kund Meghani*

Main category: cs.CL

TL;DR: 本文系统调查了七个主流大型语言模型（LLMs）的政治倾向，发现其普遍存在自由主义偏好，并将其归因于训练数据、RLHF、学术伦理框架及安全微调。作者认为这种倾向是训练于民主权利话语的涌现属性，可能反映了罗尔斯的“无知之幕”哲学理念。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现商业LLMs在伦理和政治响应上普遍表现出一致的自由主义倾向，但其深层原因和由此带来的影响仍不明确。

Method: 采用多管齐下的方法，系统调查了包括GPT-4o、Claude Sonnet 4等七个主流LLM的政治气质。具体方法包括：使用道德基础理论、十几个既定的政治意识形态量表，以及一个新的当前政治争议指数。此外，通过比较基础模型和微调模型对，并结合自报和实证测试，分析了微调对自由主义倾向的影响。

Result: 研究发现大多数LLM都强烈且一致地优先考虑自由主义价值观，尤其是关怀和公平。这种趋势主要归因于：自由主义倾向的训练语料库、人类反馈强化学习（RLHF）、学术伦理话语中自由主义框架的主导地位，以及安全驱动的微调实践。结果还表明，微调通常会增加模型的自由主义倾向。论文进一步区分了政治“偏见”和合法的认知差异。

Conclusion: LLMs的“自由主义倾斜”并非编程错误或程序员的个人偏好，而是基于民主权利话语进行训练的涌现属性。这种模式可能间接呼应了约翰·罗尔斯“无知之幕”的哲学愿望，反映了一种不依赖个人身份或兴趣的道德立场。作者提出，这种模式非但没有损害民主话语，反而可能为审视集体推理提供新的视角。

Abstract: Recent studies have revealed a consistent liberal orientation in the ethical
and political responses generated by most commercial large language models
(LLMs), yet the underlying causes and resulting implications remain unclear.
This paper systematically investigates the political temperament of seven
prominent LLMs - OpenAI's GPT-4o, Anthropic's Claude Sonnet 4, Perplexity
(Sonar Large), Google's Gemini 2.5 Flash, Meta AI's Llama 4, Mistral 7b Le Chat
and High-Flyer's DeepSeek R1 -- using a multi-pronged approach that includes
Moral Foundations Theory, a dozen established political ideology scales and a
new index of current political controversies. We find strong and consistent
prioritization of liberal-leaning values, particularly care and fairness,
across most models. Further analysis attributes this trend to four overlapping
factors: Liberal-leaning training corpora, reinforcement learning from human
feedback (RLHF), the dominance of liberal frameworks in academic ethical
discourse and safety-driven fine-tuning practices. We also distinguish between
political "bias" and legitimate epistemic differences, cautioning against
conflating the two. A comparison of base and fine-tuned model pairs reveals
that fine-tuning generally increases liberal lean, an effect confirmed through
both self-report and empirical testing. We argue that this "liberal tilt" is
not a programming error or the personal preference of programmers but an
emergent property of training on democratic rights-focused discourse. Finally,
we propose that LLMs may indirectly echo John Rawls' famous veil-of ignorance
philosophical aspiration, reflecting a moral stance unanchored to personal
identity or interest. Rather than undermining democratic discourse, this
pattern may offer a new lens through which to examine collective reasoning.

</details>


### [11] [Better Together: Quantifying the Benefits of AI-Assisted Recruitment](https://arxiv.org/abs/2507.08029)
*Ada Aka,Emil Palikot,Ali Ansari,Nima Yazdani*

Main category: cs.CL

TL;DR: 研究对比了AI辅助与传统招聘流程对初级开发人员招聘效率和人才选择的影响。结果显示，AI辅助流程显著提高了通过率和后续求职成功率，但其偏向选择更年轻、经验较少的求职者。


<details>
  <summary>Details</summary>
Motivation: 人工智能在招聘中日益普及，但缺乏量化其对招聘效率和候选人选择影响的实证证据。

Method: 将37,000名初级开发人员申请者随机分为传统招聘流程（简历筛选+人工选择）和AI辅助招聘流程（AI驱动视频面试+人工评估）两组。两组最终均面对同一轮人工面试，且面试官对前期选拔方式不知情。五个月后，收集两组求职者领英资料以评估后续就业情况，并分析AI面试转录内容以探究选择标准和对话动态。

Result: AI辅助组的最终面试通过率为54%，传统组为34%，差异为20个百分点。五个月后，AI组有23%的申请者找到新工作，传统组为18%，差异为5.9个百分点。AI系统倾向于选择更年轻、经验更少、学历较低的申请者。

Conclusion: 本研究有助于理解AI技术如何影响招聘决策和人才获取，并揭示了其潜在影响，包括提高效率但可能改变人才选择偏好。

Abstract: Artificial intelligence (AI) is increasingly used in recruitment, yet
empirical evidence quantifying its impact on hiring efficiency and candidate
selection remains limited. We randomly assign 37,000 applicants for a
junior-developer position to either a traditional recruitment process (resume
screening followed by human selection) or an AI-assisted recruitment pipeline
incorporating an initial AI-driven structured video interview before human
evaluation. Candidates advancing from either track faced the same final-stage
human interview, with interviewers blind to the earlier selection method. In
the AI-assisted pipeline, 54% of candidates passed the final interview compared
with 34% from the traditional pipeline, yielding an average treatment effect of
20 percentage points (SE 12 pp.). Five months later, we collected LinkedIn
profiles of top applicants from both groups and found that 18% (SE 1.1%) of
applicants from the traditional track found new jobs compared with 23% (SE
2.3%) from the AI group, resulting in a 5.9 pp. (SE 2.6 pp.) difference in the
probability of finding new employment between groups. The AI system tended to
select younger applicants with less experience and fewer advanced credentials.
We analyze AI-generated interview transcripts to examine the selection criteria
and conversational dynamics. Our findings contribute to understanding how AI
technologies affect decision making in recruitment and talent acquisition while
highlighting some of their potential implications.

</details>


### [12] [A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models](https://arxiv.org/abs/2507.08030)
*Sonali Sharma,Ahmed M. Alaa,Roxana Daneshjou*

Main category: cs.CL

TL;DR: 本研究发现，2022-2025年间，生成式AI（LLM和VLM）在医学领域应用时，其输出中的医学免责声明显著减少，到2025年几乎消失。鉴于AI输出的不准确性，文章强调必须重新实施免责声明作为安全保障。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型在医学图像解读和临床问答中的应用日益普及，但其输出常包含不准确信息。医学免责声明是提醒用户AI输出不应替代专业医疗建议的关键安全措施。本研究旨在评估这些模型输出中免责声明的存在情况及其随时间的变化趋势，以应对潜在的用户风险。

Method: 研究评估了2022年至2025年间不同代次LLM和VLM模型输出中免责声明的出现情况。通过向模型输入2000个医学案例（包括500张乳腺X光片、500张胸部X光片、500张皮肤病图像和500个医学问题），筛选其输出中包含的免责声明短语。

Result: LLM输出中的医学免责声明从2022年的26.3%大幅下降至2025年的0.97%。VLM输出中的医学免责声明从2023年的19.6%下降至2025年的1.05%。截至2025年，绝大多数模型不再显示任何免责声明。

Conclusion: 随着公共AI模型功能日益强大且更具权威性，为确保用户安全，必须重新实施并根据每个输出的临床语境进行调整的免责声明，作为重要的安全保障措施。

Abstract: Generative AI models, including large language models (LLMs) and
vision-language models (VLMs), are increasingly used to interpret medical
images and answer clinical questions. Their responses often include
inaccuracies; therefore, safety measures like medical disclaimers are critical
to remind users that AI outputs are not professionally vetted or a substitute
for medical advice. This study evaluated the presence of disclaimers in LLM and
VLM outputs across model generations from 2022 to 2025. Using 500 mammograms,
500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs
were screened for disclaimer phrases. Medical disclaimer presence in LLM and
VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023
to 1.05% in 2025, respectively. By 2025, the majority of models displayed no
disclaimers. As public models become more capable and authoritative,
disclaimers must be implemented as a safeguard adapting to the clinical context
of each output.

</details>


### [13] [Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental Health Understanding](https://arxiv.org/abs/2507.08031)
*Hong Jia,Shiya Fu,Vassilis Kostakos,Feng Xia,Ting Dang*

Main category: cs.CL

TL;DR: 本研究评估了小型语言模型（SLMs）在心理健康理解方面的能力，并将其与大型语言模型（LLMs）进行比较。结果显示，SLMs在二元分类任务上表现接近LLMs，且通过少样本学习能显著提升性能，表明其在隐私保护敏感数据分析中的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着小型语言模型（SLMs）作为隐私保护替代方案的出现，研究者们希望探讨与大型语言模型（LLMs）相比，SLMs在处理敏感应用（特别是心理健康理解）方面的固有理解能力。

Method: 通过系统性评估，研究人员在六项心理健康理解分类任务中，采用零样本和少样本学习范式，对五种主流SLMs（Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2）与三种LLM基线模型（GPT-4, FLAN-T5-XXL, Alpaca-7B）进行了性能基准测试。

Result: 研究发现，在二元分类任务中（零样本设置下F1分数分别为0.64 vs 0.66），SLMs的平均表现与LLMs相差不到2%。然而，在多类别严重性任务上，两类模型都经历了相似的性能下降（超过30%）。少样本提示对SLMs带来了显著提升（高达14.6%），而对LLMs的增益则变化较大。

Conclusion: SLMs在心理健康理解方面展现出巨大潜力，可作为分析敏感在线文本数据的有效隐私保护工具。特别是它们通过少样本学习快速适应和专业化的能力，使其成为可扩展心理健康筛查工具的有前景候选者。

Abstract: The emergence of Small Language Models (SLMs) as privacy-preserving
alternatives for sensitive applications raises a fundamental question about
their inherent understanding capabilities compared to Large Language Models
(LLMs). This paper investigates the mental health understanding capabilities of
current SLMs through systematic evaluation across diverse classification tasks.
Employing zero-shot and few-shot learning paradigms, we benchmark their
performance against established LLM baselines to elucidate their relative
strengths and limitations in this critical domain. We assess five
state-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against
three LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding
tasks. Our findings reveal that SLMs achieve mean performance within 2\% of
LLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot
settings), demonstrating notable competence despite orders of magnitude fewer
parameters. Both model categories experience similar degradation on multi-class
severity tasks (a drop of over 30\%), suggesting that nuanced clinical
understanding challenges transcend model scale. Few-shot prompting provides
substantial improvements for SLMs (up to 14.6\%), while LLM gains are more
variable. Our work highlights the potential of SLMs in mental health
understanding, showing they can be effective privacy-preserving tools for
analyzing sensitive online text data. In particular, their ability to quickly
adapt and specialize with minimal data through few-shot learning positions them
as promising candidates for scalable mental health screening tools.

</details>


### [14] [Integrating External Tools with Large Language Models to Improve Accuracy](https://arxiv.org/abs/2507.08034)
*Nripesh Niketan,Hadj Batatia*

Main category: cs.CL

TL;DR: 本文提出了一个名为Athena的框架，通过整合外部工具（如API和计算器）来增强大型语言模型（LLM）在教育场景中回答查询的能力，并在数学和科学推理任务上显著优于现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在缺乏相关上下文信息时容易提供低质量响应或产生幻觉。现有工作已提出整合外部工具以提高LLM准确性，本文旨在进一步在教育环境中提升LLM的查询回答能力。

Method: 开发了一个名为Athena的框架，该框架允许LLM访问外部API以请求额外的相关信息，并能整合计算器或日历等计算工具来增强其能力。该框架在多模态语言理解（MMLU）数据集上进行了评估。

Result: 在MMLU数据集的数学和科学推理问题上，Athena框架在数学推理中达到了83%的准确率，在科学推理中达到了88%的准确率。这显著优于所有测试的基线模型，包括GPT-4o、LLaMA-Large、Mistral-Large、Phi-Large和GPT-3.5，其中表现最好的基线模型LLaMA-Large分别仅达到67%和79%。

Conclusion: 该研究结果表明，通过集成外部工具可以显著提高LLM的性能，并为围绕LLM创建复杂的计算生态系统以支持更自然地处理各种任务和活动开辟了道路。

Abstract: This paper deals with improving querying large language models (LLMs). It is
well-known that without relevant contextual information, LLMs can provide poor
quality responses or tend to hallucinate. Several initiatives have proposed
integrating LLMs with external tools to provide them with up-to-date data to
improve accuracy. In this paper, we propose a framework to integrate external
tools to enhance the capabilities of LLMs in answering queries in educational
settings. Precisely, we develop a framework that allows accessing external APIs
to request additional relevant information. Integrated tools can also provide
computational capabilities such as calculators or calendars. The proposed
framework has been evaluated using datasets from the Multi-Modal Language
Understanding (MMLU) collection. The data consists of questions on mathematical
and scientific reasoning. Results compared to state-of-the-art language models
show that the proposed approach significantly improves performance. Our Athena
framework achieves 83% accuracy in mathematical reasoning and 88% in scientific
reasoning, substantially outperforming all tested models including GPT-4o,
LLaMA-Large, Mistral-Large, Phi-Large, and GPT-3.5, with the best baseline
model (LLaMA-Large) achieving only 67% and 79% respectively. These promising
results open the way to creating complex computing ecosystems around LLMs to
make their use more natural to support various tasks and activities.

</details>


### [15] [Barriers in Integrating Medical Visual Question Answering into Radiology Workflows: A Scoping Review and Clinicians' Insights](https://arxiv.org/abs/2507.08036)
*Deepali Mishra,Chaklam Silpasuwanchai,Ashutosh Modi,Madhumita Sushil,Sorayouth Chumnanvej*

Main category: cs.CL

TL;DR: 本研究系统回顾了医学视觉问答（MedVQA）文献并调查了临床医生，揭示了当前MedVQA研究与临床实际需求之间的显著差距，主要体现在问答对缺乏诊断相关性、模型缺乏多模态和上下文支持，以及评估指标与临床不符。


<details>
  <summary>Details</summary>
Motivation: 尽管MedVQA在自动化医学图像解读方面显示出前景，但其在临床工作流程中的整合仍然有限。本研究旨在系统审查MedVQA的实际效用、面临的挑战和存在的差距，以期更好地辅助放射科医生。

Method: 本研究遵循Arksey和O'Malley范围审查框架，采用双管齐下的方法：1) 系统审查了2018-2024年间68篇MedVQA相关文献，以识别关键概念、进展和放射学工作流程中的研究空白；2) 调查了来自印度和泰国的50名临床医生，以获取他们对MedVQA临床相关性的看法。

Result: 文献回顾显示，近60%的问答对不具诊断性且缺乏临床相关性，大多数数据集和模型缺乏多视图、多分辨率成像、EHR集成或领域知识支持，且评估指标与临床需求不符。临床医生调查证实了这种脱节：仅有29.8%的医生认为MedVQA系统高度有用。主要顾虑包括缺乏患者病史或领域知识（87.2%）、偏好人工整理数据集（51.1%）和需要多视图图像支持（78.7%）。此外，66%的医生偏好专注于特定解剖区域的模型，89.4%的医生青睐基于对话的交互式系统。

Conclusion: MedVQA虽潜力巨大，但为实现有效的临床整合，必须解决多模态分析受限、缺乏患者上下文信息以及评估方法与临床需求错位等关键挑战。

Abstract: Medical Visual Question Answering (MedVQA) is a promising tool to assist
radiologists by automating medical image interpretation through question
answering. Despite advances in models and datasets, MedVQA's integration into
clinical workflows remains limited. This study systematically reviews 68
publications (2018-2024) and surveys 50 clinicians from India and Thailand to
examine MedVQA's practical utility, challenges, and gaps. Following the Arksey
and O'Malley scoping review framework, we used a two-pronged approach: (1)
reviewing studies to identify key concepts, advancements, and research gaps in
radiology workflows, and (2) surveying clinicians to capture their perspectives
on MedVQA's clinical relevance. Our review reveals that nearly 60% of QA pairs
are non-diagnostic and lack clinical relevance. Most datasets and models do not
support multi-view, multi-resolution imaging, EHR integration, or domain
knowledge, features essential for clinical diagnosis. Furthermore, there is a
clear mismatch between current evaluation metrics and clinical needs. The
clinician survey confirms this disconnect: only 29.8% consider MedVQA systems
highly useful. Key concerns include the absence of patient history or domain
knowledge (87.2%), preference for manually curated datasets (51.1%), and the
need for multi-view image support (78.7%). Additionally, 66% favor models
focused on specific anatomical regions, and 89.4% prefer dialogue-based
interactive systems. While MedVQA shows strong potential, challenges such as
limited multimodal analysis, lack of patient context, and misaligned evaluation
approaches must be addressed for effective clinical integration.

</details>


### [16] [CRISP: Complex Reasoning with Interpretable Step-based Plans](https://arxiv.org/abs/2507.08037)
*Matan Vetzler,Koren Lazar,Guy Uziel,Eran Hirsch,Ateret Anaby-Tavor,Leshem Choshen*

Main category: cs.CL

TL;DR: 为增强LLM的复杂推理能力，本文挑战了仅通过few-shot提示生成高阶计划的假设，推出了CRISP数据集。通过在CRISP上微调小型模型，证明其能生成比大型模型few-shot提示更高质量的计划，并显著超越CoT推理，同时展现出良好的跨领域泛化性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）需要更强的推理能力来解决复杂问题，但现有的思维链（CoT）推理不足以应对许多领域。虽然高层计划生成是一种有前景的替代方案，但现有方法过分依赖LLM在没有额外训练的情况下仅通过few-shot提示生成有效计划的假设，本文旨在挑战这一假设。

Method: 引入了CRISP数据集，这是一个用于数学推理和代码生成的多领域高层计划数据集。CRISP中的计划是自动生成并经过严格验证的：内部使用LLM作为评判，外部通过评估其对下游任务性能的影响。随后，在CRISP数据集上对小型模型进行微调。

Result: 在CRISP上微调的小型模型能够生成比通过few-shot提示的更大模型更高质量的计划，并且显著优于思维链推理。此外，跨领域评估表明，在一个领域上的微调能改善另一个领域的计划生成，突出了所学规划能力的泛化性。

Conclusion: 通过在专门的高质量计划数据集（如CRISP）上进行微调，可以有效提升LLM的复杂推理能力，即使是小型模型也能生成更优、更具泛化性的计划，从而优于传统的few-shot和CoT方法。

Abstract: Recent advancements in large language models (LLMs) underscore the need for
stronger reasoning capabilities to solve complex problems effectively. While
Chain-of-Thought (CoT) reasoning has been a step forward, it remains
insufficient for many domains. A promising alternative is explicit high-level
plan generation, but existing approaches largely assume that LLMs can produce
effective plans through few-shot prompting alone, without additional training.
In this work, we challenge this assumption and introduce CRISP (Complex
Reasoning with Interpretable Step-based Plans), a multi-domain dataset of
high-level plans for mathematical reasoning and code generation. The plans in
CRISP are automatically generated and rigorously validated--both intrinsically,
using an LLM as a judge, and extrinsically, by evaluating their impact on
downstream task performance. We demonstrate that fine-tuning a small model on
CRISP enables it to generate higher-quality plans than much larger models using
few-shot prompting, while significantly outperforming Chain-of-Thought
reasoning. Furthermore, our out-of-domain evaluation reveals that fine-tuning
on one domain improves plan generation in the other, highlighting the
generalizability of learned planning capabilities.

</details>


### [17] [AblationBench: Evaluating Automated Planning of Ablations in Empirical AI Research](https://arxiv.org/abs/2507.08038)
*Talor Abramovich,Gal Chechik*

Main category: cs.CL

TL;DR: 本研究引入AblationBench，一个评估语言模型代理在经验性AI研究中规划消融实验能力的基准套件，并发现当前LM在该任务上表现仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 语言模型驱动的自主代理在科学研究中日益流行，尤其在AI研究中，消融实验是关键组成部分。然而，缺乏有效的基准来评估这些代理在消融实验规划方面的能力。

Method: 提出了AblationBench基准套件，包含两个任务：AuthorAblation（基于方法部分提出消融实验）和ReviewerAblation（在完整论文中发现缺失的消融实验）。开发了基于语言模型的自动评估判官。使用前沿语言模型进行了实验。

Result: 实验结果显示，这些消融规划任务对当前语言模型而言仍具挑战性，表现最佳的LM系统平均只能识别29%的原始消融实验。此外，链式思考（chain-of-thought）提示方法优于当前现有的基于代理的方法。

Conclusion: 消融实验规划对现有语言模型来说仍然是一项艰巨的任务。AblationBench为评估和推动AI共同科学家在此领域的发展提供了重要工具。未来的研究应致力于提升语言模型在复杂科研规划任务中的表现，特别是通过探索更有效的提示策略。

Abstract: Autonomous agents built on language models (LMs) are showing increasing
popularity in many fields, including scientific research. AI co-scientists aim
to support or automate parts of the research process using these agents. A key
component of empirical AI research is the design of ablation experiments. To
this end, we introduce AblationBench, a benchmark suite for evaluating agents
on ablation planning tasks in empirical AI research. It includes two tasks:
AuthorAblation, which helps authors propose ablation experiments based on a
method section and contains 83 instances, and ReviewerAblation, which helps
reviewers find missing ablations in a full paper and contains 350 instances.
For both tasks, we develop LM-based judges that serve as an automatic
evaluation framework. Our experiments with frontier LMs show that these tasks
remain challenging, with the best-performing LM system identifying only 29% of
the original ablations on average. Lastly, we analyze the limitations of
current LMs on these tasks, and find that chain-of-thought prompting
outperforms the currently existing agent-based approach.

</details>


### [18] [Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing](https://arxiv.org/abs/2507.08045)
*Junyi Wen,Junyuan Liang,Zicong Hong,Wuhui Chen,Zibin Zheng*

Main category: cs.CL

TL;DR: Krul是一个LLM多轮对话推理系统，通过动态选择KV缓存压缩策略，显著提升了KV缓存恢复的效率和存储利用率，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: LLM多轮对话中，高效的KV缓存恢复是一个挑战，现有方法由于需重新计算或加载完整KV缓存而开销巨大。尽管有压缩方案，但固定压缩策略忽略了对话特定的注意力模式差异，导致准确性下降。

Method: 提出了Krul系统，通过动态选择基于层间注意力相似性的压缩策略，并结合重计算-加载流水线来恢复KV缓存。Krul包含：1) 先发制人压缩策略选择器，定制对话策略；2) 逐token异构注意力相似性估计器，降低计算和存储开销；3) 无气泡恢复调度器，平衡重计算与加载流。

Result: 相比现有最先进方法，Krul将首字生成时间（TTFT）缩短了1.5x-2.68x，KV缓存存储减少了1.33x-2.35x，同时不损害生成质量。

Conclusion: Krul通过动态自适应的KV缓存压缩和恢复机制，成功解决了LLM多轮对话中KV缓存恢复的效率和准确性挑战，显著提升了推理性能。

Abstract: Efficient state restoration in multi-turn conversations with large language
models (LLMs) remains a critical challenge, primarily due to the overhead of
recomputing or loading full key-value (KV) caches for all historical tokens. To
address this, existing approaches compress KV caches across adjacent layers
with highly similar attention patterns. However, these methods often apply a
fixed compression scheme across all conversations, selecting the same layer
pairs for compression without considering conversation-specific attention
dynamics. This static strategy overlooks variability in attention pattern
similarity across different conversations, which can lead to noticeable
accuracy degradation.
  We present Krul, a multi-turn LLM inference system that enables accurate and
efficient KV cache restoration. Krul dynamically selects compression strategies
based on attention similarity across layer pairs and uses a
recomputation-loading pipeline to restore the KV cache. It introduces three key
innovations: 1) a preemptive compression strategy selector to preserve critical
context for future conversation turns and selects a customized strategy for the
conversation; 2) a token-wise heterogeneous attention similarity estimator to
mitigate the attention similarity computation and storage overhead during model
generation; 3) a bubble-free restoration scheduler to reduce potential bubbles
brought by the imbalance of recomputing and loading stream due to compressed KV
caches. Empirical evaluations on real-world tasks demonstrate that Krul
achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x
reduction in KV cache storage compared to state-of-the-art methods without
compromising generation quality.

</details>


### [19] [GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs](https://arxiv.org/abs/2507.08107)
*Sebastian Walter,Hannah Bast*

Main category: cs.CL

TL;DR: 提出一种基于大型语言模型的新方法，无需微调即可从自然语言或关键词生成SPARQL查询。该方法通过探索知识图谱实现，并在Wikidata上达到零样本最先进（SOTA）结果，在Freebase上接近最佳少样本方法，整体性能优异。


<details>
  <summary>Details</summary>
Motivation: 简化知识图谱（RDF）的查询过程，使用户能够通过自然语言问题或关键词查询直接生成复杂的SPARQL查询，无需手动编写，从而提高知识图谱的可用性和可访问性。

Method: 该方法利用大型语言模型（LLM），且无需进行微调。它通过让语言模型策略性地执行SPARQL查询，并在知识图谱中搜索相关的IRI和字面量，从而探索图谱以生成目标查询。

Result: 在Wikidata上，尽管采用零样本设置，但在多个基准测试中取得了最先进（SOTA）的结果。在Freebase上，表现接近于最佳的少样本方法。在其他不常评估的知识图谱和基准测试中，该方法也表现良好。此外，还进行了图谱搜索方式、反馈机制和少样本示例等额外研究。

Conclusion: 该研究提出了一种高效且强大的、无需微调的基于大型语言模型的方法，用于从自然语言生成SPARQL查询。它在多个关键基准测试中展现了卓越的性能，尤其是在零样本设置下达到了领先水平，显著推进了自然语言接口到知识图谱查询领域的发展。

Abstract: We propose a new approach for generating SPARQL queries on RDF knowledge
graphs from natural language questions or keyword queries, using a large
language model. Our approach does not require fine-tuning. Instead, it uses the
language model to explore the knowledge graph by strategically executing SPARQL
queries and searching for relevant IRIs and literals. We evaluate our approach
on a variety of benchmarks (for knowledge graphs of different kinds and sizes)
and language models (of different scales and types, commercial as well as
open-source) and compare it with existing approaches. On Wikidata we reach
state-of-the-art results on multiple benchmarks, despite the zero-shot setting.
On Freebase we come close to the best few-shot methods. On other, less commonly
evaluated knowledge graphs and benchmarks our approach also performs well
overall. We conduct several additional studies, like comparing different ways
of searching the graphs, incorporating a feedback mechanism, or making use of
few-shot examples.

</details>


### [20] [Audit, Alignment, and Optimization of LM-Powered Subroutines with Application to Public Comment Processing](https://arxiv.org/abs/2507.08109)
*Reilly Raab,Mike Parker,Dan Nally,Sadie Montgomery,Anastasia Bernat,Sai Munikoti,Sameera Horawalavithana*

Main category: cs.CL

TL;DR: 本文提出了一个框架，用于在传统代码中负责任地集成由语言模型（LM）驱动的子程序，通过在线人工反馈进行改进，并支持审计。该框架已应用于公共评论处理，开发了“CommentNEPA”应用程序并进行了定量评估。


<details>
  <summary>Details</summary>
Motivation: 语言模型虽能显著加速文本处理任务，但其在现实世界中的应用受制于安全性、可解释性和偏见等问题。研究旨在探索如何透明、可审计且负责任地利用语言模型，以最小化风险并使人类专家专注于决策，而非数据处理或提示工程。

Method: 研究提出了一个用于声明静态类型、由语言模型驱动的子程序的框架，这些子程序可集成到常规异步代码中。该框架利用人类专家的稀疏反馈来在线提升子程序性能。所有语言模型生成的工件（如提示、输入、输出和数据依赖）均被记录并可按需审计。该框架被打包成库，并在1969年《国家环境保护法》（NEPA）规定的公共评论处理场景中进行了评估，开发了“CommentNEPA”应用程序来汇编、组织和总结公共评论。

Result: 研究开发了“CommentNEPA”应用程序，其能够编译、组织和总结针对环境审查项目的公共评论语料库。通过将应用程序在无人干预下生成的结果与人类标注的历史“真实”数据进行比较，对该应用程序进行了定量评估。

Conclusion: 该框架提供了一种负责任、透明和可审计的方式来利用语言模型，并通过在线反馈实现性能改进。它在公共评论处理中得到了验证，并有望应用于医疗保健和法律等多种现实世界决策流程。

Abstract: The advent of language models (LMs) has the potential to dramatically
accelerate tasks that may be cast to text-processing; however, real-world
adoption is hindered by concerns regarding safety, explainability, and bias.
How can we responsibly leverage LMs in a transparent, auditable manner --
minimizing risk and allowing human experts to focus on informed decision-making
rather than data-processing or prompt engineering? In this work, we propose a
framework for declaring statically typed, LM-powered subroutines (i.e.,
callable, function-like procedures) for use within conventional asynchronous
code -- such that sparse feedback from human experts is used to improve the
performance of each subroutine online (i.e., during use). In our
implementation, all LM-produced artifacts (i.e., prompts, inputs, outputs, and
data-dependencies) are recorded and exposed to audit on demand. We package this
framework as a library to support its adoption and continued development. While
this framework may be applicable across several real-world decision workflows
(e.g., in healthcare and legal fields), we evaluate it in the context of public
comment processing as mandated by the 1969 National Environmental Protection
Act (NEPA): Specifically, we use this framework to develop "CommentNEPA," an
application that compiles, organizes, and summarizes a corpus of public
commentary submitted in response to a project requiring environmental review.
We quantitatively evaluate the application by comparing its outputs (when
operating without human feedback) to historical ``ground-truth'' data as
labelled by human annotators during the preparation of official environmental
impact statements.

</details>


### [21] [Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores](https://arxiv.org/abs/2507.08143)
*Vivek Chari,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 提出Compactor，一种无需参数、与查询无关的KV缓存压缩策略，显著降低大型语言模型的内存消耗，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理长上下文时，KV缓存会占用大量内存，随上下文长度线性增长，成为部署中的主要资源瓶颈，限制吞吐量并增加服务成本。

Method: 本文提出了Compactor，一种无需参数、与查询无关的KV压缩策略，通过近似杠杆分数来确定token的重要性。此外，还引入了上下文校准压缩程序，用于推断给定上下文可支持的最大压缩比。

Result: Compactor在合成和真实上下文任务中，以最小计算开销，通过保留一半的token即可达到与竞品相当的性能。利用上下文校准压缩，Compactor在Longbench上平均减少63%的KV内存负担，同时保持完整的KV性能。该方法已在RULER和Longbench的27项合成及真实任务上，通过Qwen 2.5和Llama 3.1系列模型进行了验证，展示了其有效性和通用性。

Conclusion: Compactor提供了一种高效且通用的大型语言模型KV缓存压缩方案，显著降低了内存占用，提高了部署效率，且能保持出色的性能。

Abstract: Modern Large Language Models (LLMs) are increasingly trained to support very
large context windows. Unfortunately the ability to use long contexts in
generation is complicated by the large memory requirement of the KV cache,
which scales linearly with the context length. This memory footprint is often
the dominant resource bottleneck in real-world deployments, limiting throughput
and increasing serving cost. One way to address this is by compressing the KV
cache, which can be done either with knowledge of the question being asked
(query-aware) or without knowledge of the query (query-agnostic). We present
Compactor, a parameter-free, query-agnostic KV compression strategy that uses
approximate leverage scores to determine token importance. We show that
Compactor can achieve the same performance as competing methods while retaining
1/2 the tokens in both synthetic and real-world context tasks, with minimal
computational overhead. We further introduce a procedure for context-calibrated
compression, which allows one to infer the maximum compression ratio a given
context can support. Using context-calibrated compression, we show that
Compactor achieves full KV performance on Longbench while reducing the KV
memory burden by 63%, on average. To demonstrate the efficacy and
generalizability of our approach, we apply Compactor to 27 synthetic and
real-world tasks from RULER and Longbench, with models from both the Qwen 2.5
and Llama 3.1 families.

</details>


### [22] [Distilling Empathy from Large Language Models](https://arxiv.org/abs/2507.08151)
*Henry J. Xie,Jinghan Zhang,Xinhao Zhang,Kunpeng Liu*

Main category: cs.CL

TL;DR: 本研究提出一种有效方法，通过两步微调和特制提示，将大型语言模型（LLMs）的同理心能力蒸馏到小型语言模型（SLMs），显著提升SLMs在资源受限环境中的同理心响应表现。


<details>
  <summary>Details</summary>
Motivation: 由于小型语言模型（SLMs）常用于资源受限但人机交互频繁的设备（如智能手机），确保从大型语言模型（LLMs）蒸馏后SLMs能保留同理心能力至关重要，以维持积极的人机互动。

Method: 开发了一种综合性同理心蒸馏方法，核心是一个两步微调过程，该过程充分利用从LLMs蒸馏出的同理心对话响应数据集。同时，探索了多种蒸馏方法，并提出了四组独特的提示，以有针对性地改进和增强同理心蒸馏过程。

Result: 通过该方法微调的SLMs在生成同理心响应方面的表现显著优于基础SLM，胜率高达90%。此外，所提出的目标同理心改进提示比基本直接提示的胜率提高了10%。

Conclusion: 研究成功地将同理心从LLMs有效蒸馏到SLMs，通过创新的微调过程和提示设计，显著提升了SLMs的同理心响应能力，使其在资源受限的人机交互场景中表现更出色。

Abstract: The distillation of knowledge from Large Language Models (LLMs) into Smaller
Language Models (SLMs), preserving the capabilities and performance of LLMs
while reducing model size, has played a key role in the proliferation of LLMs.
Because SLMs are considerably smaller than LLMs, they are often utilized in
domains where human interaction is frequent but resources are highly
constrained, e.g., smart phones. Therefore, it is crucial to ensure that
empathy, a fundamental aspect of positive human interactions, already instilled
into LLMs, is retained by SLMs after distillation. In this paper, we develop a
comprehensive approach for effective empathy distillation from LLMs into SLMs.
Our approach features a two-step fine-tuning process that fully leverages
datasets of empathetic dialogue responses distilled from LLMs. We explore
several distillation methods beyond basic direct prompting and propose four
unique sets of prompts for targeted empathy improvement to significantly
enhance the empathy distillation process. Our evaluations demonstrate that SLMs
fine-tuned through the two-step fine-tuning process with distillation datasets
enhanced by the targeted empathy improvement prompts significantly outperform
the base SLM at generating empathetic responses with a win rate of 90%. Our
targeted empathy improvement prompts substantially outperform the basic direct
prompting with a 10% improvement in win rate.

</details>


### [23] [TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM Outputs](https://arxiv.org/abs/2507.08203)
*Duygu Nur Yaldiz,Yavuz Faruk Bakman,Sungmin Kang,Alperen Öziş,Hayrettin Eren Yildiz,Mitash Ashish Shah,Zhiqi Huang,Anoop Kumar,Alfy Samuel,Daben Liu,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.CL

TL;DR: 本文介绍了TruthTorchLM，一个开源Python库，旨在加速大语言模型(LLM)输出真实性预测的研究，提供30多种多样化的预测方法，并兼容主流LLM平台。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLM)不可避免地会产生不真实的回答，尤其在高风险场景中，准确预测这些输出的真实性至关重要。为了加速该领域的研究并使真实性预测方法更易于使用。

Method: 引入了一个名为TruthTorchLM的开源、综合性Python库，包含30多种真实性预测方法（Truth Methods）。这些方法在计算成本、访问级别（黑盒/白盒）、参考文档需求和监督类型（自监督/监督）等方面提供多样化选择。TruthTorchLM与HuggingFace和LiteLLM无缝兼容，支持本地和API模型，并提供用于生成、评估、校准和长文本真实性预测的统一接口，以及灵活的扩展框架。作者在TriviaQA、GSM8K和FactScore-Bio三个数据集上对代表性方法进行了评估。

Result: 抽象中提到对代表性真实性方法在三个数据集（TriviaQA、GSM8K、FactScore-Bio）上进行了评估，但未提供具体的评估结果或性能数据。

Conclusion: TruthTorchLM提供了一个全面、可扩展且易于使用的平台，汇集了多种LLM真实性预测方法，有助于推动该领域的研究和应用，使其更易于访问和扩展。

Abstract: Generative Large Language Models (LLMs)inevitably produce untruthful
responses. Accurately predicting the truthfulness of these outputs is critical,
especially in high-stakes settings. To accelerate research in this domain and
make truthfulness prediction methods more accessible, we introduce TruthTorchLM
an open-source, comprehensive Python library featuring over 30 truthfulness
prediction methods, which we refer to as Truth Methods. Unlike existing
toolkits such as Guardrails, which focus solely on document-grounded
verification, or LM-Polygraph, which is limited to uncertainty-based methods,
TruthTorchLM offers a broad and extensible collection of techniques. These
methods span diverse tradeoffs in computational cost, access level (e.g.,
black-box vs white-box), grounding document requirements, and supervision type
(self-supervised or supervised). TruthTorchLM is seamlessly compatible with
both HuggingFace and LiteLLM, enabling support for locally hosted and API-based
models. It also provides a unified interface for generation, evaluation,
calibration, and long-form truthfulness prediction, along with a flexible
framework for extending the library with new methods. We conduct an evaluation
of representative truth methods on three datasets, TriviaQA, GSM8K, and
FactScore-Bio. The code is available at https://github.com/Ybakman/TruthTorchLM

</details>


### [24] [Simple Mechanistic Explanations for Out-Of-Context Reasoning](https://arxiv.org/abs/2507.08218)
*Atticus Wang,Joshua Engels,Oliver Clive-Griffin*

Main category: cs.CL

TL;DR: LoRA微调导致的LLM上下文外推理(OOCR)可被解释为模型学习并添加了一个恒定的“转向向量”，使其偏向某个通用概念，从而实现深层泛化。


<details>
  <summary>Details</summary>
Motivation: 上下文外推理（OOCR）是微调LLM表现出的惊人分布外泛化现象。本研究旨在从机制上探究OOCR的根本原因，即LLM为何能够进行上下文外推理。

Method: 通过机械化分析LoRA微调过程，研究团队发现OOCR可以归结为微调本质上添加了一个恒定的转向向量。为验证这一假设，他们还直接从头开始训练转向向量来诱导OOCR。

Result: 研究发现，许多OOCR实例都可以通过LoRA微调添加的恒定转向向量来解释，该向量将模型导向一个通用概念，从而在微调任务和许多其他相关领域提升性能并导致惊人的泛化。即使对于看似需要条件行为的任务（如模型后门），无条件添加转向向量也足以实现OOCR。

Conclusion: 本研究为OOCR任务微调过程中学到了什么提供了一种解释，阐明了LLM为何能够进行上下文外推理，这对于LLM的安全可靠部署具有重要意义。

Abstract: Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs
exhibit surprisingly deep out-of-distribution generalization. Rather than
learning shallow heuristics, they implicitly internalize and act on the
consequences of observations scattered throughout the fine-tuning data. In this
work, we investigate this phenomenon mechanistically and find that many
instances of OOCR in the literature have a simple explanation: the LoRA
fine-tuning essentially adds a constant steering vector, steering the model
towards a general concept. This improves performance on the fine-tuning task
and in many other concept-related domains, causing the surprising
generalization. Moreover, we can directly train steering vectors for these
tasks from scratch, which also induces OOCR. We find that our results hold even
for a task that seems like it must involve conditional behavior (model
backdoors); it turns out that unconditionally adding a steering vector is
sufficient. Overall, our work presents one explanation of what gets learned
during fine-tuning for OOCR tasks, contributing to the key question of why LLMs
can reason out of context, an advanced capability that is highly relevant to
their safe and reliable deployment.

</details>


### [25] [Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?](https://arxiv.org/abs/2507.08232)
*KV Aditya Srivatsa,Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）作为代理学生模拟真实学生的准确性存疑。研究通过IRT模型评估11种LLM在NAEP数据上的表现，发现多数LLMs超出平均学生水平，且通过提示对齐真实学生能力的效果不佳，亟需新的训练与评估策略。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）作为智能辅导系统（ITSs）和试题试点中的代理学生，能否准确模拟真实学生的行为和特征，因为其准确性仍是一个未解之谜。

Method: 收集了来自国家教育进步评估（NAEP）的489个数学和阅读理解题项（涵盖4、8、12年级）数据集。随后，应用项目反应理论（IRT）模型，将11种多样化且先进的LLMs与真实学生群体的能力尺度进行对比。

Result: 1. 在无指导情况下，强大的通用模型在各年级表现持续优于平均学生。
2. 较弱或领域不匹配的模型可能偶然与真实学生表现对齐。
3. 使用年级强制提示虽能改变模型表现，但能否与平均年级学生对齐高度依赖于具体模型和提示。
4. 没有一个被评估的模型-提示组合能在所有科目和年级中实现理想匹配，这强调了新的训练和评估策略的必要性。

Conclusion: LLMs作为代理学生的适用性仍有待提高，现有模型和提示策略难以全面模拟真实学生表现。因此，迫切需要新的训练和评估策略。本研究根据发现为选择可行的代理提供了指导原则。

Abstract: Large Language Models (LLMs) are increasingly used as proxy students in the
development of Intelligent Tutoring Systems (ITSs) and in piloting test
questions. However, to what extent these proxy students accurately emulate the
behavior and characteristics of real students remains an open question. To
investigate this, we collected a dataset of 489 items from the National
Assessment of Educational Progress (NAEP), covering mathematics and reading
comprehension in grades 4, 8, and 12. We then apply an Item Response Theory
(IRT) model to position 11 diverse and state-of-the-art LLMs on the same
ability scale as real student populations. Our findings reveal that, without
guidance, strong general-purpose models consistently outperform the average
student at every grade, while weaker or domain-mismatched models may align
incidentally. Using grade-enforcement prompts changes models' performance, but
whether they align with the average grade-level student remains highly model-
and prompt-specific: no evaluated model-prompt pair fits the bill across
subjects and grades, underscoring the need for new training and evaluation
strategies. We conclude by providing guidelines for the selection of viable
proxies based on our findings.

</details>


### [26] [Exploring Gender Differences in Chronic Pain Discussions on Reddit](https://arxiv.org/abs/2507.08241)
*Ancita Maria Andrade,Tanvi Banerjee,Ramakrishna Mundugar*

Main category: cs.CL

TL;DR: 本研究利用NLP（HAM-CNN）分析在线疼痛体验，成功识别并分类了不同性别的疼痛语料，揭示了性别在语言表达、特定疾病患病率及药物反应方面的差异。


<details>
  <summary>Details</summary>
Motivation: 既往疼痛研究普遍忽视了性别在疼痛体验中的关键作用，缺乏对个体疼痛经验中性别差异的深入洞察。

Method: 本研究采用自然语言处理（NLP）技术，利用隐藏属性模型-卷积神经网络（HAM-CNN）将用户帖子分类为男性和女性语料库，通过聚合用户名下的帖子实现了0.86的F1分数。

Result: 分析揭示了性别间的语言差异，女性帖子更侧重情感表达。此外，研究发现偏头痛和鼻窦炎等疾病在女性中更为普遍，并且疼痛药物对不同性别个体的影响存在差异。

Conclusion: 本研究表明NLP能有效识别和分析性别在疼痛体验中的独特模式，强调了在疼痛研究和治疗中考虑性别差异的重要性。

Abstract: Pain is an inherent part of human existence, manifesting as both physical and
emotional experiences, and can be categorized as either acute or chronic. Over
the years, extensive research has been conducted to understand the causes of
pain and explore potential treatments, with contributions from various
scientific disciplines. However, earlier studies often overlooked the role of
gender in pain experiences. In this study, we utilized Natural Language
Processing (NLP) to analyze and gain deeper insights into individuals' pain
experiences, with a particular focus on gender differences. We successfully
classified posts into male and female corpora using the Hidden Attribute
Model-Convolutional Neural Network (HAM-CNN), achieving an F1 score of 0.86 by
aggregating posts based on usernames. Our analysis revealed linguistic
differences between genders, with female posts tending to be more emotionally
focused. Additionally, the study highlighted that conditions such as migraine
and sinusitis are more prevalent among females and explored how pain medication
affects individuals differently based on gender.

</details>


### [27] [KAT-V1: Kwai-AutoThink Technical Report](https://arxiv.org/abs/2507.08297)
*Zizheng Zhan,Ken Deng,Huaixi Tang,Wen Xiang,Kun Wu,Weihao Li,Wenqiang Zhu,Jingxuan Xu,Lecheng Huang,Zongxian Feng,Shaojie Wang,Shangpeng Yan,Jiaheng Liu,Zhongyuan Peng,Zuchen Gao,Haoyang Huang,Ziqi Zhan,Yanan Wu,Yuanxing Zhang,Jian Yang,Guang Chen,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.CL

TL;DR: 本文提出了Kwaipilot-AutoThink (KAT) 模型，这是一个开源的40B大型语言模型，通过自动思考训练范式动态切换推理模式，有效解决了推理密集型任务中的过度思考问题，超越了当前最先进的模型，并显著降低了Token使用量。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在推理密集型任务中存在的“过度思考”问题，以提高推理效率和性能。

Method: 1. 构建基于新型标记流水线和多智能体合成策略的双模式数据集。2. 应用MTP增强的知识蒸馏，实现高效且细粒度的推理知识迁移。3. 实施冷启动初始化策略，通过多数投票信号和意图感知提示引入模式选择先验。4. 提出Step-SRPO强化学习算法，将中间监督融入GRPO框架，为推理模式选择和响应准确性提供结构化指导。

Result: 1. KAT在多项基准测试中，性能与当前最先进模型（包括DeepSeek-R1-0528和Qwen3-235B-A22B）持平或超越。2. 将Token使用量减少了约30%。3. 已成功部署于快手内部编码助手Kwaipilot，显著提升了真实世界开发工作流的准确性、效率和推理可控性。4. 正在训练的200B MoE模型早期结果表明了其性能和效率的显著提升，进一步验证了AutoThink范式的可扩展性。

Conclusion: Kwaipilot-AutoThink (KAT) 模型通过其创新的自动思考范式，在推理密集型任务中展现出卓越的性能和效率，成功解决了大型语言模型的过度思考问题，并具有强大的实际应用价值和可扩展性。

Abstract: We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model
developed to address the overthinking problem in reasoning-intensive tasks,
where an automatic thinking training paradigm is proposed to dynamically switch
between reasoning and non-reasoning modes based on task complexity.
Specifically, first, we construct the dual-regime dataset based on a novel
tagging pipeline and a multi-agent synthesis strategy, and then we apply
Multi-Token Prediction (MTP)-enhanced knowledge distillation, enabling
efficient and fine-grained reasoning transfer with minimal pretraining cost.
Besides, we implement a cold-start initialization strategy that introduces
mode-selection priors using majority-vote signals and intent-aware prompting.
Finally, we propose Step-SRPO, a reinforcement learning algorithm that
incorporates intermediate supervision into the GRPO framework, offering
structured guidance over both reasoning-mode selection and response accuracy.
Extensive experiments across multiple benchmarks demonstrate that KAT
consistently matches or even outperforms current state-of-the-art models,
including DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of
reasoning-intensive tasks while reducing token usage by up to approximately
30\%. Beyond academic evaluation, KAT has been successfully deployed in
Kwaipilot (i.e., Kuaishou's internal coding assistant), and improves real-world
development workflows with high accuracy, efficiency, and controllable
reasoning behaviors. Moreover, we are actively training a 200B
Mixture-of-Experts (MoE) with 40B activation parameters, where the early-stage
results already demonstrate promising improvements in performance and
efficiency, further showing the scalability of the AutoThink paradigm.

</details>


### [28] [Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency](https://arxiv.org/abs/2507.08309)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Zhiyuan Chen,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

TL;DR: 为解决多模态大语言模型在文档图像机器翻译（DIMT）中的不足及其监督微调（SFT）导致的灾难性遗忘问题，本文提出了一种名为“同步自审查（SSR）”的新型微调范式，通过在翻译前生成OCR文本来利用现有OCR能力，有效提升了模型在OCR和DIMT任务上的性能并减轻了遗忘。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在文档图像任务，特别是光学字符识别（OCR）方面表现出色，但在文档图像机器翻译（DIMT）方面表现不佳，因其同时涉及跨模态和跨语言挑战。此外，现有通过监督微调（SFT）提升DIMT能力的方法常导致模型忘记其原有的单语能力，例如OCR。

Method: 引入了一种受“双语认知优势”启发的名为“同步自审查（Synchronously Self-Reviewing, SSR）”的新型微调范式。具体地，SSR提示模型在生成翻译文本之前先生成OCR文本，这使得模型在学习跨语言翻译的同时，能够利用其强大的单语OCR能力。

Result: 全面的实验证明，所提出的SSR学习有助于缓解灾难性遗忘，并提高了多模态大语言模型在OCR和DIMT任务上的泛化能力。

Conclusion: SSR范式成功解决了MLLMs在DIMT任务上的挑战和SFT导致的灾难性遗忘问题，通过利用模型自身的OCR能力，有效提升了模型在OCR和DIMT任务上的表现及泛化能力。

Abstract: Multimodal Large Language Models (MLLMs) have shown strong performance in
document image tasks, especially Optical Character Recognition (OCR). However,
they struggle with Document Image Machine Translation (DIMT), which requires
handling both cross-modal and cross-lingual challenges. Previous efforts to
enhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT
dataset often result in the forgetting of the model's existing monolingual
abilities, such as OCR. To address these challenges, we introduce a novel
fine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR
proficiency, inspired by the concept "Bilingual Cognitive Advantage".
Specifically, SSR prompts the model to generate OCR text before producing
translation text, which allows the model to leverage its strong monolingual OCR
ability while learning to translate text across languages. Comprehensive
experiments demonstrate the proposed SSR learning helps mitigate catastrophic
forgetting, improving the generalization ability of MLLMs on both OCR and DIMT
tasks.

</details>


### [29] [CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation](https://arxiv.org/abs/2507.08325)
*Yinzhu Quan,Xinrui Li,Ying Chen*

Main category: cs.CL

TL;DR: CRMAgent是一个基于LLM的多智能体系统，旨在帮助商家在电商私域渠道生成高质量营销信息并提供指导，显著提升了信息效果。


<details>
  <summary>Details</summary>
Motivation: 商家在电商私域渠道（如即时通讯、邮件）中难以撰写有说服力的外发消息，因为缺乏专业知识和可扩展的工具，这影响了客户留存和转化。

Method: 引入CRMAgent，一个基于LLM的多智能体系统，通过三种互补模式生成高质量消息模板和可执行的写作指导：1. 基于群组学习，从商家自身表现最佳的消息中学习并重写低效消息。2. 检索与适应，检索相似的高效模板并学习其成功模式进行改编。3. 基于规则的后备，在没有合适参考时提供零样本重写。

Result: 广泛的实验表明，CRMAgent持续优于商家原始模板，在受众匹配度和营销效率指标上都取得了显著提升。

Conclusion: CRMAgent能够有效解决商家在电商私域渠道营销文案撰写上的痛点，显著提高营销信息的质量和效果。

Abstract: In e-commerce private-domain channels such as instant messaging and e-mail,
merchants engage customers directly as part of their Customer Relationship
Management (CRM) programmes to drive retention and conversion. While a few top
performers excel at crafting outbound messages, most merchants struggle to
write persuasive copy because they lack both expertise and scalable tools. We
introduce CRMAgent, a multi-agent system built on large language models (LLMs)
that generates high-quality message templates and actionable writing guidance
through three complementary modes. First, group-based learning enables the
agent to learn from a merchant's own top-performing messages within the same
audience segment and rewrite low-performing ones. Second,
retrieval-and-adaptation fetches templates that share the same audience segment
and exhibit high similarity in voucher type and product category, learns their
successful patterns, and adapts them to the current campaign. Third, a
rule-based fallback provides a lightweight zero-shot rewrite when no suitable
references are available. Extensive experiments show that CRMAgent consistently
outperforms merchants' original templates, delivering significant gains in both
audience-match and marketing-effectiveness metrics.

</details>


### [30] [MK2 at PBIG Competition: A Prompt Generation Solution](https://arxiv.org/abs/2507.08335)
*Yuzheng Xu,Tosho Hirasawa,Seiya Kawano,Shota Kato,Tadashi Kozuno*

Main category: cs.CL

TL;DR: 研究提出了MK2，一个以提示词为中心的LLM管道，无需额外训练数据，即可将专利转化为商业产品构思，并在多项测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决“基于专利的构思生成”任务，即系统需将实际专利转化为三年内可行的产品构思。

Method: 提出MK2管道：Gemini 2.5负责起草和迭代编辑提示词，并整合较弱输出中的有用片段；然后GPT-4.1使用此提示词为每项专利生成一个构思；最后通过Qwen3-8B判断的Elo循环选出最佳提示词。整个过程不依赖额外训练数据。

Result: MK2在三个领域、两种评估者类型和六项标准下，位居自动排行榜榜首，并在36项测试中赢得了25项。仅在材料化学领域表现滞后，表明需要更深度的领域知识。

Conclusion: 研究结果表明，轻量级的提示词工程已经能够从专利中生成具有竞争力和商业相关性的构思。

Abstract: The Patent-Based Idea Generation task asks systems to turn real patents into
product ideas viable within three years. We propose MK2, a prompt-centric
pipeline: Gemini 2.5 drafts and iteratively edits a prompt, grafting useful
fragments from weaker outputs; GPT-4.1 then uses this prompt to create one idea
per patent, and an Elo loop judged by Qwen3-8B selects the best prompt-all
without extra training data. Across three domains, two evaluator types, and six
criteria, MK2 topped the automatic leaderboard and won 25 of 36 tests. Only the
materials-chemistry track lagged, indicating the need for deeper domain
grounding; yet, the results show that lightweight prompt engineering has
already delivered competitive, commercially relevant ideation from patents.

</details>


### [31] [Distillation versus Contrastive Learning: How to Train Your Rerankers](https://arxiv.org/abs/2507.08336)
*Zhichao Xu,Zhiqi Huang,Shengyao Zhuang,Ashim Gupta,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本研究比较了对比学习和知识蒸馏在训练文本重排序器上的有效性。结果表明，当存在更大、更强大的教师模型时，知识蒸馏在域内和域外性能上通常优于对比学习；若无此类教师模型，对比学习是更可靠的替代方案。


<details>
  <summary>Details</summary>
Motivation: 信息检索中训练文本重排序器至关重要。对比学习和知识蒸馏是两种主要策略，但目前缺乏在实际条件下对它们训练交叉编码器重排序器有效性的明确比较。

Method: 通过实证比较，在相同数据集上使用对比学习和知识蒸馏方法训练不同大小和架构的重排序器，并以一个强大的对比学习模型作为知识蒸馏的教师模型。

Result: 研究结果显示，当从一个更大的教师模型进行蒸馏时，知识蒸馏在域内和域外排序性能上通常优于对比学习，且此发现适用于不同学生模型大小和架构。然而，从同等容量的教师模型进行蒸馏时，这种优势不明显，尤其是在域外任务上。

Conclusion: 因此，建议在可获得更大、更强大教师模型的情况下，使用知识蒸馏来训练较小的重排序器；若无此类教师模型，对比学习则是一个强大且更可靠的替代方案。这些发现为选择训练策略提供了实用指导。

Abstract: Training text rerankers is crucial for information retrieval. Two primary
strategies are widely used: contrastive learning (optimizing directly on
ground-truth labels) and knowledge distillation (transferring knowledge from a
larger reranker). While both have been studied in the literature, a clear
comparison of their effectiveness for training cross-encoder rerankers under
practical conditions is needed.
  This paper empirically compares these strategies by training rerankers of
different sizes and architectures using both methods on the same data, with a
strong contrastive learning model acting as the distillation teacher. Our
results show that knowledge distillation generally yields better in-domain and
out-of-domain ranking performance than contrastive learning when distilling
from a larger teacher model. This finding is consistent across student model
sizes and architectures. However, distilling from a teacher of the same
capacity does not provide the same advantage, particularly for out-of-domain
tasks. These findings offer practical guidance for choosing a training strategy
based on available teacher models. Therefore, we recommend using knowledge
distillation to train smaller rerankers if a larger, more powerful teacher is
accessible; in its absence, contrastive learning provides a strong and more
reliable alternative otherwise.

</details>


### [32] [What Factors Affect LLMs and RLLMs in Financial Question Answering?](https://arxiv.org/abs/2507.08339)
*Peng Wang,Xuesi Hu,Jiageng Wu,Yuntao Zou,Qiancheng Zhang,Dagang Li*

Main category: cs.CL

TL;DR: 本研究系统评估了提示方法、代理框架和多语言对齐方法对金融问答领域LLM和RLLM性能的影响，发现现有方法对LLM有效，但对RLLM因其固有长CoT能力提升有限。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能系统性探索如何充分发挥LLM和RLLM在金融领域的性能。

Method: 使用五种LLM和三种RLLM，评估提示方法、代理框架和多语言对齐方法在金融问答任务中的效果。

Result: ['当前提示方法和代理框架通过模拟长CoT提升了LLM在金融问答中的表现。', 'RLLM拥有固有的长CoT能力，限制了传统方法对其性能的进一步提升。', '先进的多语言对齐方法主要通过延长推理长度改善LLM的多语言性能，对RLLM收益甚微。']

Conclusion: 本研究为金融问答领域的LLM和RLLM提供了重要参考。

Abstract: Recently, the development of large language models (LLMs) and reasoning large
language models (RLLMs) have gained considerable attention from many
researchers. RLLMs enhance the reasoning capabilities of LLMs through Long
Chain-of-Thought (Long CoT) processes, significantly improving the performance
of LLMs in addressing complex problems. However, there are few works that
systematically explore what methods can fully unlock the performance of LLMs
and RLLMs within the financial domain. To investigate the impact of various
methods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the
effects of prompting methods, agentic frameworks, and multilingual alignment
methods on financial question-answering tasks. Our research findings indicate:
(1) Current prompting methods and agent frameworks enhance the performance of
LLMs in financial question answering by simulating Long CoT; (2) RLLMs possess
inherent Long CoT capabilities, which limits the effectiveness of conventional
methods in further enhancing their performance; (3) Current advanced
multilingual alignment methods primarily improve the multilingual performance
of LLMs by extending the reasoning length, which yields minimal benefits for
RLLMs. We hope that this study can serve as an important reference for LLMs and
RLLMs in the field of financial question answering.

</details>


### [33] [Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization](https://arxiv.org/abs/2507.08342)
*Itai Mondshine,Tzuf Paz-Argaman,Reut Tsarfaty*

Main category: cs.CL

TL;DR: 本文系统评估了N-gram和神经语义指标在八种不同语言生成任务中的表现，发现N-gram指标在屈折语中与人类判断相关性较低，但适当的分词可改善；专门训练的神经指标（如COMET）表现更优，尤其在低资源语言中。


<details>
  <summary>Details</summary>
Motivation: ROUGE等N-gram指标广泛用于评估英语生成任务，但其在其他语言中的适用性尚不明确。因此，需要系统评估不同语言和任务下，N-gram和神经语义评估指标的有效性。

Method: 设计了一个涵盖八种语言（来自四种语型家族：黏着语、孤立语、弱屈折语、强屈折语，包括低资源和高资源设置）的大规模评估套件，分析了这些评估指标与人类判断的相关性。同时，探究了正确分词对形态丰富屈折语的影响。

Result: 评估指标的有效性对语言类型敏感。在屈折语中，N-gram指标与人类评估的相关性低于孤立语和黏着语。适当的分词能显著缓解形态丰富的屈折语中的此问题，甚至逆转负面趋势。专门用于评估的神经指标（如COMET）持续优于其他神经指标，并在低资源语言中与人类判断的相关性更高。

Conclusion: N-gram指标在屈折语中存在局限性。未来应加大对专门为评估任务训练的神经语义指标的投入。

Abstract: Automatic n-gram based metrics such as ROUGE are widely used for evaluating
generative tasks such as summarization. While these metrics are considered
indicative (even if imperfect) of human evaluation for English, their
suitability for other languages remains unclear. To address this, we
systematically assess evaluation metrics for generation both n-gram-based and
neural based to evaluate their effectiveness across languages and tasks.
Specifically, we design a large-scale evaluation suite across eight languages
from four typological families: agglutinative, isolating, low-fusional, and
high-fusional, spanning both low- and high-resource settings, to analyze their
correlation with human judgments. Our findings highlight the sensitivity of
evaluation metrics to the language type. For example, in fusional languages,
n-gram-based metrics show lower correlation with human assessments compared to
isolating and agglutinative languages. We also demonstrate that proper
tokenization can significantly mitigate this issue for morphologically rich
fusional languages, sometimes even reversing negative trends. Additionally, we
show that neural-based metrics specifically trained for evaluation, such as
COMET, consistently outperform other neural metrics and better correlate with
human judgments in low-resource languages. Overall, our analysis highlights the
limitations of n-gram metrics for fusional languages and advocates for greater
investment in neural-based metrics trained for evaluation tasks.

</details>


### [34] [Exploring Design of Multi-Agent LLM Dialogues for Research Ideation](https://arxiv.org/abs/2507.08350)
*Keisuke Ueda,Wataru Hirota,Takuto Asakura,Takahiro Omi,Kosuke Takahashi,Kosuke Arima,Tatsuya Ishigaki*

Main category: cs.CL

TL;DR: 本研究深入分析了多智能体大型语言模型（LLM）对话在科学构思中的应用，发现增加智能体数量、深化互动及扩大角色多样性（尤其评论者多样性）能显著提升生成构思的多样性和可行性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）已用于支持创意任务，且结构化对话能提升构思质量，但如何优化多智能体LLM对话的设计以达到最佳效果仍不明确。

Method: 研究通过比较多智能体LLM对话中智能体角色、数量和对话深度的不同配置，全面分析了它们对科学构思新颖性和可行性的影响。实验设置包含生成与批判的迭代改进循环。

Result: 结果显示，扩大智能体数量、深化互动和增加智能体角色异质性均能丰富生成构思的多样性。特别地，增加批判侧多样性能够进一步提高最终提案的可行性。

Conclusion: 本研究的发现为构建高效的多智能体LLM科学构思系统提供了实用指导。

Abstract: Large language models (LLMs) are increasingly used to support creative tasks
such as research idea generation. While recent work has shown that structured
dialogues between LLMs can improve the novelty and feasibility of generated
ideas, the optimal design of such interactions remains unclear. In this study,
we conduct a comprehensive analysis of multi-agent LLM dialogues for scientific
ideation. We compare different configurations of agent roles, number of agents,
and dialogue depth to understand how these factors influence the novelty and
feasibility of generated ideas. Our experimental setup includes settings where
one agent generates ideas and another critiques them, enabling iterative
improvement. Our results show that enlarging the agent cohort, deepening the
interaction depth, and broadening agent persona heterogeneity each enrich the
diversity of generated ideas. Moreover, specifically increasing critic-side
diversity within the ideation-critique-revision loop further boosts the
feasibility of the final proposals. Our findings offer practical guidelines for
building effective multi-agent LLM systems for scientific ideation. Our code is
available at https://github.com/g6000/MultiAgent-Research-Ideator.

</details>


### [35] [The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality](https://arxiv.org/abs/2507.08371)
*Benjamin Newman,Abhilasha Ravichander,Jaehun Jung,Rui Xin,Hamish Ivison,Yegor Kuznetsov,Pang Wei Koh,Yejin Choi*

Main category: cs.CL

TL;DR: 语言模型易产生幻觉。本研究发现，使用模型自身认为事实性的生成数据，并结合模型内部判断进行过滤来微调模型，能比使用黄金数据更有效地减少幻觉。模型自身的信念可作为事实性的强大信号。


<details>
  <summary>Details</summary>
Motivation: 语言模型容易产生事实性错误（幻觉）。尽管高质量事实数据微调可能有用，但获取成本高昂且可能适得其反。研究旨在探究何种微调数据能有效缓解语言模型的幻觉问题。

Method: 研究者考察了微调数据的事实性与长篇生成任务中幻觉发生率的关系。他们对比了使用事实黄金数据、模型生成数据（模型认为事实的）、以及通过模型自身内部判断过滤的黄金数据和模型生成数据进行微调的效果。

Result: 研究发现，使用事实黄金数据微调不如使用模型自身认为事实性的生成数据有效。此外，使用经过模型自身内部判断过滤的模型生成数据进行微调，能比其他配置（如仅使用黄金数据、使用模型判断过滤的黄金数据或由黄金数据支持的模型生成数据）带来更好的整体事实性。这些改善在三个研究领域中均成立。

Conclusion: 模型的自身信念可以为事实性提供强大的信号。利用模型内部判断过滤的模型生成数据是一种有效减少语言模型幻觉的策略。

Abstract: Language models are prone to hallucination - generating text that is
factually incorrect. Finetuning models on high-quality factual information can
potentially reduce hallucination, but concerns remain; obtaining factual gold
data can be expensive and training on correct but unfamiliar data may
potentially lead to even more downstream hallucination. What data should
practitioners finetune on to mitigate hallucinations in language models? In
this work, we study the relationship between the factuality of finetuning data
and the prevalence of hallucinations in long-form generation tasks.
Counterintuitively, we find that finetuning on factual gold data is not as
helpful as finetuning on model-generated data that models believe to be
factual. Next, we evaluate filtering strategies applied on both factual gold
data and model-generated data, and find that finetuning on model-generated data
that is filtered by models' own internal judgments often leads to better
overall factuality compared to other configurations: training on gold data
filtered by models' judgments, training on gold data alone, or training on
model-generated data that is supported by gold data. These factuality
improvements transfer across three domains we study, suggesting that a models'
own beliefs can provide a powerful signal for factuality.

</details>


### [36] [A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities](https://arxiv.org/abs/2507.08425)
*Lu Xiang,Yang Zhao,Yaping Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLMs）在跨学科研究中的应用，涵盖了其技术方法、在不同学科领域的贡献、现有挑战及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽在跨学科研究中展现巨大潜力，但对其在不同学科中整合方式的系统性理解仍有待深入。

Method: 本文采用综述方法，全面概述并分类了大型语言模型（LLMs）在跨学科研究中的应用，从技术（如监督微调、检索增强生成、基于智能体的方法、工具使用）和学科应用（如数学、物理、化学、生物、人文学科、社会科学）两个视角进行分析。

Result: 综述结果展示了提升大型语言模型（LLMs）适应性和有效性的关键技术方法，阐明了其在数学、物理、化学、生物、人文学科和社会科学等不同学科任务中的应用和贡献。此外，还分析了当前面临的挑战并指出了未来研究方向。

Conclusion: 本综述旨在通过提供全面的技术发展和应用概览，为跨学科研究领域中探索大型语言模型（LLMs）的研究人员提供宝贵资源。

Abstract: Large Language Models (LLMs) have demonstrated their transformative potential
across numerous disciplinary studies, reshaping the existing research
methodologies and fostering interdisciplinary collaboration. However, a
systematic understanding of their integration into diverse disciplines remains
underexplored. This survey paper provides a comprehensive overview of the
application of LLMs in interdisciplinary studies, categorising research efforts
from both a technical perspective and with regard to their applicability. From
a technical standpoint, key methodologies such as supervised fine-tuning,
retrieval-augmented generation, agent-based approaches, and tool-use
integration are examined, which enhance the adaptability and effectiveness of
LLMs in discipline-specific contexts. From the perspective of their
applicability, this paper explores how LLMs are contributing to various
disciplines including mathematics, physics, chemistry, biology, and the
humanities and social sciences, demonstrating their role in discipline-specific
tasks. The prevailing challenges are critically examined and the promising
research directions are highlighted alongside the recent advances in LLMs. By
providing a comprehensive overview of the technical developments and
applications in this field, this survey aims to serve as an invaluable resource
for the researchers who are navigating the complex landscape of LLMs in the
context of interdisciplinary studies.

</details>


### [37] [ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains](https://arxiv.org/abs/2507.08427)
*Zilu Dong,Xiangqing Shen,Zinong Yang,Rui Xia*

Main category: cs.CL

TL;DR: 现有LLM知识编辑方法在处理关联事实的逻辑一致性方面面临挑战。本研究提出ChainEdit框架，通过结合知识图谱规则和LLM推理能力，实现系统性链式更新，显著提升逻辑泛化能力，并解决现有评估偏差。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）的知识编辑方法在向关联事实传播涟漪效应时，难以保持逻辑一致性。

Method: 提出ChainEdit框架，该框架结合了知识图谱衍生的逻辑规则和LLM的逻辑推理能力，以实现系统性的链式更新。具体通过自动从结构化知识库中提取逻辑模式，并将其与LLM的内部逻辑对齐，动态生成和编辑逻辑连接的知识簇。此外，引入了知识感知协议来解决现有基准测试的评估偏差。

Result: 实验表明，ChainEdit在逻辑泛化能力上比基线方法提高了30%以上，同时保持了编辑的可靠性和特异性。通过知识感知协议，有效解决了现有基准测试中的评估偏差。

Conclusion: 本工作在知识编辑后处理涟漪效应和确保内部逻辑一致性方面取得了新的最先进（SOTA）性能。

Abstract: Current knowledge editing methods for large language models (LLMs) struggle
to maintain logical consistency when propagating ripple effects to associated
facts. We propose ChainEdit, a framework that synergizes knowledge
graph-derived logical rules with LLM logical reasoning capabilities to enable
systematic chain updates. By automatically extracting logical patterns from
structured knowledge bases and aligning them with LLMs' internal logics,
ChainEdit dynamically generates and edits logically connected knowledge
clusters. Experiments demonstrate an improvement of more than 30% in logical
generalization over baselines while preserving editing reliability and
specificity. We further address evaluation biases in existing benchmarks
through knowledge-aware protocols that disentangle external dependencies. This
work establishes new state-of-the-art performance on ripple effect while
ensuring internal logical consistency after knowledge editing.

</details>


### [38] [Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences](https://arxiv.org/abs/2507.08440)
*Selina Heller,Mohamed Ibrahim,David Antony Selby,Sebastian Vollmer*

Main category: cs.CL

TL;DR: 本文提出一个基于LLM的多智能体系统，旨在模拟决策会议并检测智能体间的意见一致性。研究发现LLMs能可靠地检测一致性，并能提升模拟会议的效率和质量，展示了其在支持群体决策中的潜力。


<details>
  <summary>Details</summary>
Motivation: 决策会议是解决复杂问题和达成共识的重要形式。鉴于大型语言模型（LLMs）在模拟现实世界场景和群体互动方面的潜力，本研究旨在利用LLM模拟决策会议，并重点关注检测参与智能体之间的意见一致性。

Method: 本研究构建了一个新颖的基于LLM的多智能体系统来模拟决策会议。为检测意见一致性，评估了六个不同的LLM在两个任务上的表现：立场检测（识别智能体立场）和立场极性检测（识别立场的情感倾向）。这些模型随后在多智能体系统内进行评估，以确定其在复杂模拟中的有效性。

Result: 研究结果表明，LLMs即使在动态和细致的辩论中也能可靠地检测到意见一致性。在系统中集成意见一致性检测智能体，可以提高小组辩论的效率，并增强审议的整体质量和连贯性。模拟的会议产出和决策质量可与真实世界的决策会议相媲美。

Conclusion: 本研究证明了基于LLM的多智能体系统在模拟群体决策过程中的巨大潜力。这些系统有望在各种领域的专家意见征询研讨会中，为决策制定提供重要支持。

Abstract: Decision conferences are structured, collaborative meetings that bring
together experts from various fields to address complex issues and reach a
consensus on recommendations for future actions or policies. These conferences
often rely on facilitated discussions to ensure productive dialogue and
collective agreement. Recently, Large Language Models (LLMs) have shown
significant promise in simulating real-world scenarios, particularly through
collaborative multi-agent systems that mimic group interactions. In this work,
we present a novel LLM-based multi-agent system designed to simulate decision
conferences, specifically focusing on detecting agreement among the participant
agents. To achieve this, we evaluate six distinct LLMs on two tasks: stance
detection, which identifies the position an agent takes on a given issue, and
stance polarity detection, which identifies the sentiment as positive,
negative, or neutral. These models are further assessed within the multi-agent
system to determine their effectiveness in complex simulations. Our results
indicate that LLMs can reliably detect agreement even in dynamic and nuanced
debates. Incorporating an agreement-detection agent within the system can also
improve the efficiency of group debates and enhance the overall quality and
coherence of deliberations, making them comparable to real-world decision
conferences regarding outcome and decision-making. These findings demonstrate
the potential for LLM-based multi-agent systems to simulate group
decision-making processes. They also highlight that such systems could be
instrumental in supporting decision-making with expert elicitation workshops
across various domains.

</details>


### [39] [Diagnosing Failures in Large Language Models' Answers: Integrating Error Attribution into Evaluation Framework](https://arxiv.org/abs/2507.08459)
*Zishan Xu,Shuyi Xie,Qingsong Lv,Shupei Xiao,Linlin Song,Sui Wenjuan,Fan Lin*

Main category: cs.CL

TL;DR: 本文提出了一套针对大型语言模型（LLM）错误归因的自动化解决方案，包括一个全面的错误归因框架、一个专门的数据集AttriData和一个微调模型MisAttributionLLM，该模型是首个能同时生成评分、错误归因和反馈的通用判别模型。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，主流LLM平台每天产生海量用户-模型交互，需要开发自动化框架来高效分析模型性能和诊断答案中的故障。然而，现有评估模型缺乏错误归因能力。

Method: 1. 建立了包含6个主要类别和15个次要类别的综合错误归因框架（Misattribution Framework）。2. 基于该框架构建了专门用于错误归因的数据集AttriData，包含误归因、相应分数和反馈。3. 提出了MisAttributionLLM，一个在AttriData上微调的模型，旨在实现分数、错误归因和反馈的同时生成。

Result: 通过广泛的实验和分析，验证了所提出方法的有效性和鲁棒性。

Conclusion: 本研究成功地为LLM的性能分析和故障诊断提供了一个自动化的、系统化的错误归因解决方案，填补了现有评估模型在错误归因能力上的空白。

Abstract: With the widespread application of Large Language Models (LLMs) in various
tasks, the mainstream LLM platforms generate massive user-model interactions
daily. In order to efficiently analyze the performance of models and diagnose
failures in their answers, it is essential to develop an automated framework to
systematically categorize and attribute errors. However, existing evaluation
models lack error attribution capability. In this work, we establish a
comprehensive Misattribution Framework with 6 primary and 15 secondary
categories to facilitate in-depth analysis. Based on this framework, we present
AttriData, a dataset specifically designed for error attribution, encompassing
misattribution, along with the corresponding scores and feedback. We also
propose MisAttributionLLM, a fine-tuned model on AttriData, which is the first
general-purpose judge model capable of simultaneously generating score,
misattribution, and feedback. Extensive experiments and analyses are conducted
to confirm the effectiveness and robustness of our proposed method.

</details>


### [40] [Using Large Language Models for Legal Decision-Making in Austrian Value-Added Tax Law: An Experimental Study](https://arxiv.org/abs/2507.08468)
*Marina Luketina,Andrea Benkel,Christoph G. Schuetz*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLMs）在奥地利和欧盟增值税法框架下辅助法律决策的能力。研究表明，LLMs在支持税务专业人员方面潜力巨大，但受限于法律领域的敏感性，尚不能实现完全自动化。


<details>
  <summary>Details</summary>
Motivation: 税务咨询中，客户常以自然语言描述案例，使LLMs成为支持自动化决策、减轻税务专业人员工作量的理想选择。然而，LLMs易产生幻觉，而法律决策要求有充分法律依据的分析，这构成了挑战。

Method: 研究采用微调（fine-tuning）和检索增强生成（RAG）两种常用方法来提升LLM性能。这些方法应用于教科书案例和税务咨询公司的真实案例，旨在系统性地确定LLM系统的最佳配置并评估其法律推理能力。

Result: 研究发现，LLMs在自动化日常任务和提供初步分析方面有潜力，能够有效支持税务专业人员处理增值税任务并提供有法律依据的决策理由。但由于法律领域的敏感性，当前原型尚未准备好完全自动化。

Conclusion: LLMs在税务法律辅助方面显示出前景，但仍存在处理隐式客户知识和特定语境文档的局限性，未来需整合结构化背景信息以进一步提升其能力。

Abstract: This paper provides an experimental evaluation of the capability of large
language models (LLMs) to assist in legal decision-making within the framework
of Austrian and European Union value-added tax (VAT) law. In tax consulting
practice, clients often describe cases in natural language, making LLMs a prime
candidate for supporting automated decision-making and reducing the workload of
tax professionals. Given the requirement for legally grounded and
well-justified analyses, the propensity of LLMs to hallucinate presents a
considerable challenge. The experiments focus on two common methods for
enhancing LLM performance: fine-tuning and retrieval-augmented generation
(RAG). In this study, these methods are applied on both textbook cases and
real-world cases from a tax consulting firm to systematically determine the
best configurations of LLM-based systems and assess the legal-reasoning
capabilities of LLMs. The findings highlight the potential of using LLMs to
support tax consultants by automating routine tasks and providing initial
analyses, although current prototypes are not ready for full automation due to
the sensitivity of the legal domain. The findings indicate that LLMs, when
properly configured, can effectively support tax professionals in VAT tasks and
provide legally grounded justifications for decisions. However, limitations
remain regarding the handling of implicit client knowledge and context-specific
documentation, underscoring the need for future integration of structured
background information.

</details>


### [41] [ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual Speech Recognition](https://arxiv.org/abs/2507.08477)
*Qingliang Meng,Hao Wu,Wei Liang,Wei Xu,Qing Zhao*

Main category: cs.CL

TL;DR: 为解决LoRA微调中的过拟合问题，本文提出迭代LoRA训练（ILT）结合迭代伪标签策略，该方法在语音识别任务中表现出色，并在国际挑战赛中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型与自动语音识别系统深度融合中，低秩适应（LoRA）在监督微调（SFT）阶段普遍存在的过拟合问题，并提升模型性能的理论上限。

Method: 提出创新训练范式——迭代LoRA训练（ILT）结合迭代伪标签策略。基于Whisper-large-v3和Qwen2-Audio模型，采用Focus Training、Feed Back Training、Fix Training三阶段训练过程进行系统实验。

Result: 实验结果证明了所提方法的有效性。在Interspeech 2025 MLC-SLM挑战赛中，在Track 1（多语言ASR任务）获得第4名，在Track 2（语音分离与识别任务）获得第1名。

Conclusion: 该迭代训练范式有效解决了LoRA微调的过拟合问题并显著提升了模型性能，具有实际可行性和强大的应用潜力。

Abstract: The deep integration of large language models and automatic speech
recognition systems has become a promising research direction with high
practical value. To address the overfitting issue commonly observed in Low-Rank
Adaptation (LoRA) during the supervised fine-tuning (SFT) stage, this work
proposes an innovative training paradigm Iterative LoRA Training (ILT) in
combination with an Iterative Pseudo Labeling strategy, effectively enhancing
the theoretical upper bound of model performance. Based on Whisper-large-v3 and
Qwen2-Audio, we conduct systematic experiments using a three-stage training
process: Focus Training, Feed Back Training, and Fix Training. Experimental
results demonstrate the effectiveness of the proposed method. Furthermore, the
MegaAIS research team applied this technique in the Interspeech 2025
Multilingual Conversational Speech Language Modeling Challenge (MLC-SLM),
achieving 4th in Track 1 (Multilingual ASR Task) and 1st place in Track 2
(Speech Separation and Recognition Task), showcasing the practical feasibility
and strong application potential of our approach.

</details>


### [42] [Enhancing Essay Cohesion Assessment: A Novel Item Response Theory Approach](https://arxiv.org/abs/2507.08487)
*Bruno Alexandre Rosa,Hilário Oliveira,Luiz Rodrigues,Eduardo Araujo Oliveira,Rafael Ferreira Mello*

Main category: cs.CL

TL;DR: 本研究提出一种基于项目反应理论（IRT）的方法，用于调整机器学习模型在议论文中生成的文本连贯性分数，实验结果表明该方法优于传统机器学习模型和集成方法。


<details>
  <summary>Details</summary>
Motivation: 自动评估论文的文本连贯性是教育人工智能领域的一个挑战。现有机器学习算法在评估文本时通常不考虑语料中实例的个体特征，而项目反应理论（IRT）能够适应机器学习语境，表征模型的能力、难度和区分度，从而有望解决这一不足。

Method: 本研究提出并分析了一种基于项目反应理论（IRT）的连贯性分数预测方法，用于调整机器学习模型生成的得分。实验使用了扩展的Essay-BR语料库（6,563篇高中论文）和巴西葡萄牙语叙事论文语料库（1,235篇中小学论文）。从论文中提取了325个语言特征，并将问题视为机器学习回归任务。

Result: 实验结果表明，所提出的基于IRT的方法在多项评估指标上均优于传统的机器学习模型和集成方法。

Conclusion: 本研究探索了一种潜在的有效方法，以改进教育论文中连贯性的自动评估效果。

Abstract: Essays are considered a valuable mechanism for evaluating learning outcomes
in writing. Textual cohesion is an essential characteristic of a text, as it
facilitates the establishment of meaning between its parts. Automatically
scoring cohesion in essays presents a challenge in the field of educational
artificial intelligence. The machine learning algorithms used to evaluate texts
generally do not consider the individual characteristics of the instances that
comprise the analysed corpus. In this meaning, item response theory can be
adapted to the context of machine learning, characterising the ability,
difficulty and discrimination of the models used. This work proposes and
analyses the performance of a cohesion score prediction approach based on item
response theory to adjust the scores generated by machine learning models. In
this study, the corpus selected for the experiments consisted of the extended
Essay-BR, which includes 6,563 essays in the style of the National High School
Exam (ENEM), and the Brazilian Portuguese Narrative Essays, comprising 1,235
essays written by 5th to 9th grade students from public schools. We extracted
325 linguistic features and treated the problem as a machine learning
regression task. The experimental results indicate that the proposed approach
outperforms conventional machine learning models and ensemble methods in
several evaluation metrics. This research explores a potential approach for
improving the automatic evaluation of cohesion in educational essays.

</details>


### [43] [A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench](https://arxiv.org/abs/2507.08491)
*David Schlangen,Sherzod Hakimov,Jonathan Jordan,Philipp Sadler*

Main category: cs.CL

TL;DR: 本文介绍了clembench，一个成熟且易于重用的对话游戏式LLM评估框架，旨在解决现有评估方法的局限性，并促进可控、多轮、目标导向的测试。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）评估范式（基于引用和基于偏好）各有优缺点，无法兼顾控制性和生态有效性。新兴的对话游戏式评估范式虽然有潜力，但因缺乏成熟、易于重用的实现而阻碍了其推广应用。

Method: 作者开发并推出了clembench，一个自2023年持续开发并已优化为易于通用使用的工具。它提供了一个成熟的实现，用于进行对话游戏式的LLM评估。

Result: clembench允许用户使用其提供的英文基准游戏实例来评估自己的模型，并且该基准本身可以轻松地通过定制的新测试进行扩展。它提供了一个实用的解决方案，用于可重复、目标导向、多轮、无引用的LLM评估。

Conclusion: clembench为对话游戏式评估范式提供了一个成熟且易于重用的工具，弥补了现有LLM评估方法在控制性和生态有效性之间的空白，有助于推广这种有前景的评估方法。

Abstract: There are currently two main paradigms for evaluating large language models
(LLMs), reference-based evaluation and preference-based evaluation. The first,
carried over from the evaluation of machine learning models in general, relies
on pre-defined task instances, for which reference task executions are
available. The second, best exemplified by the LM-arena, relies on (often
self-selected) users bringing their own intents to a site that routes these to
several models in parallel, among whose responses the user then selects their
most preferred one. The former paradigm hence excels at control over what is
tested, while the latter comes with higher ecological validity, testing actual
use cases interactively. Recently, a third complementary paradigm has emerged
that combines some of the strengths of these approaches, offering control over
multi-turn, reference-free, repeatable interactions, while stressing
goal-directedness: dialogue game based evaluation. While the utility of this
approach has been shown by several projects, its adoption has been held back by
the lack of a mature, easily re-usable implementation. In this paper, we
present clembench, which has been in continuous development since 2023 and has
in its latest release been optimized for ease of general use. We describe how
it can be used to benchmark one's own models (using a provided set of benchmark
game instances in English), as well as how easily the benchmark itself can be
extended with new, tailor-made targeted tests.

</details>


### [44] [LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning](https://arxiv.org/abs/2507.08496)
*Shibo Sun,Xue Li,Donglin Di,Mingjie Wei,Lanshun Nie,Wei-Nan Zhang,Dechen Zhan,Yang Song,Lei Fan*

Main category: cs.CL

TL;DR: LLaPa是一个新的视觉语言模型框架，通过结合多模态输入和反事实推理，显著提升了具身AI的程序规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在具身AI的程序规划中，对多模态输入和反事实推理的整合仍未充分探索。

Method: 提出LLaPa框架，利用视觉语言模型（VLMs）从文本任务描述和视觉环境图像生成可执行动作序列。此外，通过两个辅助模块增强：任务-环境重排序器（TER）创建任务敏感的特征空间以对齐文本和视觉，以及反事实活动检索器（CAR）识别并强调反事实条件，增强推理能力。

Result: 在ActPlan-1K和ALFRED基准测试中，LLaPa生成了更高质量的规划，具有更优的LCS分数和正确性，性能超越了现有先进模型。

Conclusion: LLaPa框架通过有效整合多模态输入和反事实推理能力，显著提升了具身AI系统的程序规划质量和性能。

Abstract: While large language models (LLMs) have advanced procedural planning for
embodied AI systems through strong reasoning abilities, the integration of
multimodal inputs and counterfactual reasoning remains underexplored. To tackle
these challenges, we introduce LLaPa, a vision-language model framework
designed for multimodal procedural planning. LLaPa generates executable action
sequences from textual task descriptions and visual environmental images using
vision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary
modules to improve procedural planning. The first module, the Task-Environment
Reranker (TER), leverages task-oriented segmentation to create a task-sensitive
feature space, aligning textual descriptions with visual environments and
emphasizing critical regions for procedural execution. The second module, the
Counterfactual Activities Retriever (CAR), identifies and emphasizes potential
counterfactual conditions, enhancing the model's reasoning capability in
counterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED
benchmarks demonstrate that LLaPa generates higher-quality plans with superior
LCS and correctness, outperforming advanced models. The code and models are
available https://github.com/sunshibo1234/LLaPa.

</details>


### [45] [Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop](https://arxiv.org/abs/2507.08498)
*Mengze Hong,Chen Jason Zhang,Di Jiang*

Main category: cs.CL

TL;DR: 本研究探索了利用LLM增强LDA主题模型在初始化和后校正阶段的有效性。结果显示，LLM辅助的初始化效果不佳，但LLM辅助的后校正显著提升了主题一致性，凸显了LLM在文本挖掘中的特定应用价值。


<details>
  <summary>Details</summary>
Motivation: 探索将大语言模型（LLMs）集成到主题模型（如LDA）的初始化和后校正阶段，以评估其对主题模型性能的增强效果。

Method: 将LLM应用于LDA的两个关键阶段：一是LLM引导的主题聚类用于初始化Gibbs采样算法；二是LLM辅助的后校正。通过实验评估这些集成策略对LDA早期迭代、收敛性及主题一致性（采用连贯性评估）的影响，并与基线方法进行比较。

Result: 实验结果表明，LLM引导的初始化策略虽然改善了LDA的早期迭代，但对收敛性没有影响，且与基线相比表现最差。然而，LLM辅助的后校正方法在主题一致性评估中实现了5.86%的显著提升。

Conclusion: 研究结果突显了“LLM-in-the-loop”方法的实际益处，尤其是在主题模型的后校正方面表现出色。同时，它也挑战了LLMs总是优于传统文本挖掘方法的普遍观点。

Abstract: Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic
model used for uncovering abstract topics within document collections. In this
paper, we explore the effectiveness of augmenting topic models with Large
Language Models (LLMs) through integration into two key phases: Initialization
and Post-Correction. Since the LDA is highly dependent on the quality of its
initialization, we conduct extensive experiments on the LLM-guided topic
clustering for initializing the Gibbs sampling algorithm. Interestingly, the
experimental results reveal that while the proposed initialization strategy
improves the early iterations of LDA, it has no effect on the convergence and
yields the worst performance compared to the baselines. The LLM-enabled
post-correction, on the other hand, achieved a promising improvement of 5.86%
in the coherence evaluation. These results highlight the practical benefits of
the LLM-in-the-loop approach and challenge the belief that LLMs are always the
superior text mining alternative.

</details>


### [46] [PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts](https://arxiv.org/abs/2507.08499)
*Ziyi Huang,Xia Cui*

Main category: cs.CL

TL;DR: 该论文提出一个针对SemEval 2025任务11（短文本多标签情感检测）的系统。系统采用以特征为中心的框架，动态调整文档表示和学习算法以优化跨28种语言的性能。研究结果显示TF-IDF对低资源语言有效，而FastText和Sentence-BERT等上下文嵌入有语言特异性优势，PCA可减少训练时间。该框架为多语言情感检测提供可扩展解决方案。


<details>
  <summary>Details</summary>
Motivation: 旨在解决SemEval 2025 Task 11中短文本多标签情感检测的挑战，特别是针对多语言环境下的语言多样性和资源限制问题。

Method: 提出一个以特征为中心的框架，该框架动态调整文档表示和学习算法以优化特定语言的性能。研究评估了三个关键组件：文档表示（如TF-IDF、FastText、Sentence-BERT）、降维技术（如PCA）和模型训练（如MLP），并在28种语言中进行了测试。

Result: TF-IDF对低资源语言仍然高效；FastText和基于Transformer的文档表示（如Sentence-BERT）展现出语言特异性优势；主成分分析（PCA）能在不影响性能的前提下减少训练时间，尤其有利于FastText和神经网络模型（如MLP）；计算效率分析揭示了模型复杂度和处理成本之间的权衡。

Conclusion: 所提出的框架为多语言情感检测提供了一个可扩展的解决方案，有效应对了语言多样性和资源限制的挑战。

Abstract: This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in
Text-Based Emotion Detection (Track A), which focuses on multi-label emotion
detection in short texts. We propose a feature-centric framework that
dynamically adapts document representations and learning algorithms to optimize
language-specific performance. Our study evaluates three key components:
document representation, dimensionality reduction, and model training in 28
languages, highlighting five for detailed analysis. The results show that
TF-IDF remains highly effective for low-resource languages, while contextual
embeddings like FastText and transformer-based document representations, such
as those produced by Sentence-BERT, exhibit language-specific strengths.
Principal Component Analysis (PCA) reduces training time without compromising
performance, particularly benefiting FastText and neural models such as
Multi-Layer Perceptrons (MLP). Computational efficiency analysis underscores
the trade-off between model complexity and processing cost. Our framework
provides a scalable solution for multilingual emotion detection, addressing the
challenges of linguistic diversity and resource constraints.

</details>


### [47] [The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual Benchmarks](https://arxiv.org/abs/2507.08538)
*David Pomerenke,Jonas Nothnagel,Simon Ostermann*

Main category: cs.CL

TL;DR: 本文提出“AI语言能力监测器”，一个综合性多语言基准，旨在评估大型语言模型（LLMs）在多达200种语言（包括低资源语言）上的性能。它整合了翻译、问答、数学和推理等多样化任务，并提供一个开源、自动更新的排行榜和仪表板，以促进多语言AI的透明度、包容性和发展。


<details>
  <summary>Details</summary>
Motivation: 为确保大型语言模型（LLMs）能公平普惠地服务于全球用户，必须全面评估其在世界各种语言，特别是低资源语言中的能力和表现。

Method: ['引入“AI语言能力监测器”，这是一个全面的多语言基准测试工具。', '系统性地评估LLM在多达200种语言上的性能，重点关注低资源语言。', '聚合翻译、问答、数学和推理等多种任务进行评估。', '使用FLORES+、MMLU、GSM8K、TruthfulQA和ARC等知名数据集。', '提供一个开源、自动更新的排行榜和仪表板，以支持研究人员、开发者和决策者评估模型表现。']

Result: ['成功构建并提供了AI语言能力监测平台。', '该平台能够对不同LLM模型进行多语言性能排名。', '提供模型性能的描述性洞察，包括全球语言熟练度地图和随时间变化的趋势。', '协助用户识别模型在多语言环境中的优势和不足。']

Conclusion: 本研究通过开发和发布一个创新的多语言评估基准，补充和扩展了现有的评估方法，旨在提升多语言AI的透明度、包容性和整体进步，从而确保LLMs的惠及范围更广、更公平。

Abstract: To ensure equitable access to the benefits of large language models (LLMs),
it is essential to evaluate their capabilities across the world's languages. We
introduce the AI Language Proficiency Monitor, a comprehensive multilingual
benchmark that systematically assesses LLM performance across up to 200
languages, with a particular focus on low-resource languages. Our benchmark
aggregates diverse tasks including translation, question answering, math, and
reasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We
provide an open-source, auto-updating leaderboard and dashboard that supports
researchers, developers, and policymakers in identifying strengths and gaps in
model performance. In addition to ranking models, the platform offers
descriptive insights such as a global proficiency map and trends over time. By
complementing and extending prior multilingual benchmarks, our work aims to
foster transparency, inclusivity, and progress in multilingual AI. The system
is available at
https://huggingface.co/spaces/fair-forward/evals-for-every-language.

</details>


### [48] [DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures](https://arxiv.org/abs/2507.08606)
*Benno Uthayasooriyar,Antoine Ly,Franck Vermet,Caio Corro*

Main category: cs.CL

TL;DR: DocPolarBERT是一种新型的布局感知BERT模型，通过在相对极坐标系中处理文本块位置，消除了对绝对2D位置嵌入的需求，并在小数据集上实现了最先进的文档理解性能。


<details>
  <summary>Details</summary>
Motivation: 旨在克服传统文档理解模型对大量预训练数据和绝对2D位置嵌入的依赖，提供一种更高效、数据效率更高的替代方案。

Method: 引入DocPolarBERT，一个布局感知的BERT模型。通过扩展自注意力机制，使其在相对极坐标系中而非笛卡尔坐标系中考虑文本块的位置，从而避免使用绝对2D位置嵌入。

Result: DocPolarBERT在比IIT-CDIP语料库小六倍以上的预训练数据集上，仍能取得最先进（SOTA）的文档理解结果。

Conclusion: 研究表明，精心设计的注意力机制能够有效弥补预训练数据量减少的不足，为文档理解提供了一个高效且有效的替代方案。

Abstract: We introduce DocPolarBERT, a layout-aware BERT model for document
understanding that eliminates the need for absolute 2D positional embeddings.
We extend self-attention to take into account text block positions in relative
polar coordinate system rather than the Cartesian one. Despite being
pre-trained on a dataset more than six times smaller than the widely used
IIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results
demonstrate that a carefully designed attention mechanism can compensate for
reduced pre-training data, offering an efficient and effective alternative for
document understanding.

</details>


### [49] [A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1](https://arxiv.org/abs/2507.08621)
*Marcin Pietroń,Rafał Olszowski,Jakub Gomułka,Filip Gampel,Andrzej Tomski*

Main category: cs.CL

TL;DR: 本文首次广泛分析了大型语言模型（LLM）在公开论证分类数据集（如Args.me和UKP）上的表现，发现ChatGPT-4o综合性能最佳，而结合推理能力的Deepseek-R1表现突出。研究揭示了LLM和现有提示算法在论证分析中的局限性，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）显著提升了论证挖掘的效率，但目前仍缺乏针对这些模型在公开可用论证分类数据库中操作和表现的深入研究与结果。

Method: 本研究选择了一系列大型语言模型（包括GPT、Llama和DeepSeek的不同版本），并在Args.me和UKP等多样化数据集上进行了测试。部分模型还整合了Chain-of-Thoughts等推理增强算法。

Result: 测试结果显示，ChatGPT-4o在论证分类基准测试中表现最佳。在结合推理能力的模型中，Deepseek-R1展现出优越性。然而，尽管这些模型性能突出，GPT-4o和Deepseek-R1仍然会犯错误，并且论文讨论了所有模型中最常见的错误类型。

Conclusion: 本工作是首次对公开论证数据集使用LLM和提示算法进行的广泛分析，揭示了已知提示算法在论证分析中的一些弱点并指出了改进方向。此外，工作还深入分析了现有论证数据集并展示了它们的不足。

Abstract: Argument mining (AM) is an interdisciplinary research field that integrates
insights from logic, philosophy, linguistics, rhetoric, law, psychology, and
computer science. It involves the automatic identification and extraction of
argumentative components, such as premises and claims, and the detection of
relationships between them, such as support, attack, or neutrality. Recently,
the field has advanced significantly, especially with the advent of large
language models (LLMs), which have enhanced the efficiency of analyzing and
extracting argument semantics compared to traditional methods and other deep
learning models. There are many benchmarks for testing and verifying the
quality of LLM, but there is still a lack of research and results on the
operation of these models in publicly available argument classification
databases. This paper presents a study of a selection of LLM's, using diverse
datasets such as Args.me and UKP. The models tested include versions of GPT,
Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the
Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms
the others in the argument classification benchmarks. In case of models
incorporated with reasoning capabilities, the Deepseek-R1 shows its
superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still
make errors. The most common errors are discussed for all models. To our
knowledge, the presented work is the first broader analysis of the mentioned
datasets using LLM and prompt algorithms. The work also shows some weaknesses
of known prompt algorithms in argument analysis, while indicating directions
for their improvement. The added value of the work is the in-depth analysis of
the available argument datasets and the demonstration of their shortcomings.

</details>


### [50] [The Impact of Automatic Speech Transcription on Speaker Attribution](https://arxiv.org/abs/2507.08660)
*Cristina Aggazzotti,Matthew Wiesner,Elizabeth Allyn Smith,Nicholas Andrews*

Main category: cs.CL

TL;DR: 本研究首次全面探讨了自动语音识别（ASR）转录对说话人归因性能的影响，发现其对转录错误具有出乎意料的鲁棒性，甚至可能优于人工转录数据。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注使用人工标注的文本进行说话人归因，但实际应用中，通常只能获得包含错误的ASR转录文本。因此，需要研究ASR转录错误对说话人归因性能的影响。

Method: 本研究进行了一项综合性研究，分析了自动转录对说话人归因性能的影响，具体探究了转录错误导致性能下降的程度，以及ASR系统属性如何影响归因。

Result: 研究发现说话人归因对词级别的转录错误具有惊人的鲁棒性。同时，恢复真实转录文本的目标与归因性能的相关性极低。总体而言，使用ASR生成的含错误转录文本进行说话人归因，其效果与基于人工转录数据的结果同样好，甚至更好。

Conclusion: 研究结果表明，ASR转录文本可有效用于说话人归因，即使存在错误，性能也表现出色，这可能是因为ASR转录错误能捕捉到揭示说话人身份的特定特征。

Abstract: Speaker attribution from speech transcripts is the task of identifying a
speaker from the transcript of their speech based on patterns in their language
use. This task is especially useful when the audio is unavailable (e.g.
deleted) or unreliable (e.g. anonymized speech). Prior work in this area has
primarily focused on the feasibility of attributing speakers using transcripts
produced by human annotators. However, in real-world settings, one often only
has more errorful transcripts produced by automatic speech recognition (ASR)
systems. In this paper, we conduct what is, to our knowledge, the first
comprehensive study of the impact of automatic transcription on speaker
attribution performance. In particular, we study the extent to which speaker
attribution performance degrades in the face of transcription errors, as well
as how properties of the ASR system impact attribution. We find that
attribution is surprisingly resilient to word-level transcription errors and
that the objective of recovering the true transcript is minimally correlated
with attribution performance. Overall, our findings suggest that speaker
attribution on more errorful transcripts produced by ASR is as good, if not
better, than attribution based on human-transcribed data, possibly because ASR
transcription errors can capture speaker-specific features revealing of speaker
identity.

</details>


### [51] [KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment](https://arxiv.org/abs/2507.08665)
*Jiyao Zhang,Chengli Zhong,Hui Xu,Qige Li,Yi Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为 KELPS 的神经符号框架，用于将非正式数学形式化为机器可验证的定理。该框架通过设计一种新的中间语言（KEs），实现了自然语言到多种形式语言的翻译，并生成了一个大型并行语料库，在准确性方面超越了现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型（LLMs）在将非正式数学形式化为机器可验证的定理方面取得了进展，但受限于多语言并行语料库的数量和质量，面临瓶颈。

Method: 本研究提出了一种迭代的神经符号框架 KELPS（Knowledge-Equation based Logical Processing System）。首先，将自然语言翻译成一种新设计的、基于断言逻辑的语言——知识方程（Knowledge Equations, KEs）。其次，通过严格定义的规则，将 KEs 转换为目标形式语言（Lean、Coq 和 Isabelle），同时保留句法结构和语义意义。此过程生成了一个包含超过60,000个问题的并行语料库。

Result: 该框架生成了一个包含超过60,000个问题的并行语料库。在 MiniF2F 数据集上，其句法准确率（pass@1）达到88.9%，在多个数据集上均优于现有的SOTA模型，例如 Deepseek-V3（81%）和 Herald（81.3%）。

Conclusion: KELPS 框架有效地解决了LLMs在数学形式化过程中面临的语料库限制问题。通过其独特的神经符号方法和中间语言设计，不仅成功构建了大规模高质量并行语料，而且在形式化准确性上显著超越了现有模型，为数学形式化领域带来了重要进展。

Abstract: Modern large language models (LLMs) show promising progress in formalizing
informal mathematics into machine-verifiable theorems. However, these methods
still face bottlenecks due to the limited quantity and quality of multilingual
parallel corpora. In this paper, we propose a novel neuro-symbolic framework
KELPS (Knowledge-Equation based Logical Processing System) to address these
problems. KELPS is an iterative framework for translating, synthesizing, and
filtering informal data into multiple formal languages (Lean, Coq, and
Isabelle). First, we translate natural language into Knowledge Equations (KEs),
a novel language that we designed, theoretically grounded in assertional logic.
Next, we convert them to target languages through rigorously defined rules that
preserve both syntactic structure and semantic meaning. This process yielded a
parallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic
accuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3
(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are
available in the supplementary materials.

</details>


### [52] [KG-Attention: Knowledge Graph-Guided Attention at Test-Time via Bidirectional Information Aggregation](https://arxiv.org/abs/2507.08704)
*Songlin Zhai,Guilin Qi,Yuan Meng*

Main category: cs.CL

TL;DR: 提出首个测试时知识图谱增强LLM框架KGA，通过双向注意力机制实现无参数更新的动态知识融合，效果可比。


<details>
  <summary>Details</summary>
Motivation: 现有KG-增强LLMs方法依赖参数密集型微调，易导致灾难性遗忘和泛化能力下降；同时，其静态集成框架限制了实时知识更新的适应性。

Method: 引入首个测试时KG增强框架KGA，核心为知识图谱引导注意力（KGA）模块。该模块在标准自注意力机制上增加“向外聚合”和“向内聚合”两个协同通路。向外通路动态整合外部知识；向内通路通过过滤和增强机制精炼输入表示，并选择最相关三元组反馈融合过程。此方法无需参数更新，仅在测试时支持实时知识融合。

Result: 在五个基准数据集上进行的大量实验验证了KGA具有可比的知识融合性能。

Conclusion: KGA成功提供了一种无需微调、支持实时知识更新的LLM增强方案，有效解决了现有方法的局限性，并展现出良好性能。

Abstract: Knowledge graphs (KGs) play a critical role in enhancing large language
models (LLMs) by introducing structured and grounded knowledge into the
learning process. However, most existing KG-enhanced approaches rely on
parameter-intensive fine-tuning, which risks catastrophic forgetting and
degrades the pretrained model's generalization. Moreover, they exhibit limited
adaptability to real-time knowledge updates due to their static integration
frameworks. To address these issues, we introduce the first test-time
KG-augmented framework for LLMs, built around a dedicated knowledge
graph-guided attention (KGA) module that enables dynamic knowledge fusion
without any parameter updates. The proposed KGA module augments the standard
self-attention mechanism with two synergistic pathways: outward and inward
aggregation. Specifically, the outward pathway dynamically integrates external
knowledge into input representations via input-driven KG fusion. This inward
aggregation complements the outward pathway by refining input representations
through KG-guided filtering, suppressing task-irrelevant signals and amplifying
knowledge-relevant patterns. Importantly, while the outward pathway handles
knowledge fusion, the inward path selects the most relevant triples and feeds
them back into the fusion process, forming a closed-loop enhancement mechanism.
By synergistically combining these two pathways, the proposed method supports
real-time knowledge fusion exclusively at test-time, without any parameter
modification. Extensive experiments on five benchmarks verify the comparable
knowledge fusion performance of KGA.

</details>


### [53] [Multilingual Multimodal Software Developer for Code Generation](https://arxiv.org/abs/2507.08719)
*Linzheng Chai,Jian Yang,Shukai Liu,Wei Zhang,Liran Wang,Ke Jin,Tao Sun,Congnan Liu,Chenchen Zhang,Hualei Zhu,Jiaheng Liu,Xianjie Wu,Ge Zhang,Tianyu Liu,Zhoujun Li*

Main category: cs.CL

TL;DR: 针对现有LLMs代码生成缺乏视觉理解能力，本文提出MM-Coder多模态开发器，整合文本与视觉设计（UML、流程图）以增强代码生成，并构建MMc-Instruct数据集及MMEval评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）在代码生成方面虽有进步，但普遍仅限于文本，忽视了软件开发中常用的图表和流程图等关键视觉辅助，导致无法像人类开发者一样整合和利用视觉设计信息。

Method: 1. 提出MM-Coder：一个多语言、多模态的软件开发器，旨在整合文本指令与UML图、流程图（统称Visual Workflow）等视觉设计输入，以提高代码生成准确性和架构对齐。2. 构建MMc-Instruct数据集：创建了一个多样化的多模态指令调优数据集，其中包含基于视觉工作流的代码生成任务，使MM-Coder能够像人类开发者一样综合处理文本和图形信息。3. 开发MMEval评估基准：引入了一个新的评估工具，用于多模态代码生成任务，弥补了现有仅支持文本评估的局限性。

Result: 通过MMEval基准进行的评估表明，当前模型在精确捕捉视觉信息、遵循指令和掌握高级编程知识方面仍面临显著挑战。

Conclusion: 本研究旨在使LLMs能够理解并实现通过文本和视觉设计传达的复杂规范，从而有望彻底改变工业编程模式。

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
improved code generation, yet most models remain text-only, neglecting crucial
visual aids like diagrams and flowcharts used in real-world software
development. To bridge this gap, we introduce MM-Coder, a Multilingual
Multimodal software developer. MM-Coder integrates visual design inputs-Unified
Modeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with
textual instructions to enhance code generation accuracy and architectural
alignment. To enable this, we developed MMc-Instruct, a diverse multimodal
instruction-tuning dataset including visual-workflow-based code generation,
allowing MM-Coder to synthesize textual and graphical information like human
developers, distinct from prior work on narrow tasks. Furthermore, we introduce
MMEval, a new benchmark for evaluating multimodal code generation, addressing
existing text-only limitations. Our evaluations using MMEval highlight
significant remaining challenges for models in precise visual information
capture, instruction following, and advanced programming knowledge. Our work
aims to revolutionize industrial programming by enabling LLMs to interpret and
implement complex specifications conveyed through both text and visual designs.

</details>


### [54] [KV Cache Steering for Inducing Reasoning in Small Language Models](https://arxiv.org/abs/2507.08799)
*Max Belitsky,Dawid J. Kopiczko,Michael Dorkenwald,M. Jehanzeb Mirza,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

TL;DR: 提出了一种名为“缓存引导”（cache steering）的轻量级一次性方法，通过干预键值缓存来隐式引导语言模型，有效提升小模型在推理任务上的表现和推理质量，且比现有方法更高效稳定。


<details>
  <summary>Details</summary>
Motivation: 需要在不进行微调或修改提示的情况下，以轻量级、高效且鲁棒的方式，隐式引导语言模型（尤其是小型模型）实现复杂的推理行为，例如思维链推理。

Method: 提出“缓存引导”方法，通过对语言模型的键值（key-value）缓存进行一次性干预。该方法利用GPT-4o生成的推理轨迹来构建引导向量，从而使模型在不进行微调或修改提示的情况下，将行为转向更显式、多步的推理。

Result: 实验评估表明，缓存引导在多样化的推理基准测试中，显著提升了模型推理的定性结构和定量任务表现。相较于以往的激活引导技术，该方法在超参数稳定性、推理效率和集成便捷性方面具有显著优势。

Conclusion: 缓存引导是一种更鲁棒、更实用的受控生成解决方案，因为它具有一次性干预、高效率、高稳定性和易于集成的特点，能有效引导模型进行复杂推理，而无需繁重的模型修改。

Abstract: We propose cache steering, a lightweight method for implicit steering of
language models via a one-shot intervention applied directly to the key-value
cache. To validate its effectiveness, we apply cache steering to induce
chain-of-thought reasoning in small language models. Our approach leverages
GPT-4o-generated reasoning traces to construct steering vectors that shift
model behavior toward more explicit, multi-step reasoning without fine-tuning
or prompt modifications. Experimental evaluations on diverse reasoning
benchmarks demonstrate that cache steering improves both the qualitative
structure of model reasoning and quantitative task performance. Compared to
prior activation steering techniques that require continuous interventions, our
one-shot cache steering offers substantial advantages in terms of
hyperparameter stability, inference-time efficiency, and ease of integration,
making it a more robust and practical solution for controlled generation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [55] [CuriosAI Submission to the EgoExo4D Proficiency Estimation Challenge 2025](https://arxiv.org/abs/2507.08022)
*Hayato Tanoue,Hiroki Nishihara,Yuma Suzuki,Takayuki Hori,Hiroki Takushima,Aiswariya Manojkumar,Yuki Shibata,Mitsuru Takeda,Fumika Beppu,Zhao Hengwei,Yuto Kanda,Daichi Yamaga*

Main category: cs.CV

TL;DR: 该报告为EgoExo4D能力评估挑战赛提交，提出了两种多视角技能评估方法：基于Sapiens-2B的多任务学习和结合零样本场景识别的两阶段流水线，其中后者表现更优，证明了场景条件建模的有效性。


<details>
  <summary>Details</summary>
Motivation: 参与CVPR 2025 EgoExo4D能力评估挑战赛，并提出有效的多视角技能评估方法。

Method: ['采用基于Sapiens-2B的多任务学习框架，联合预测能力和场景标签。', '构建两阶段流水线，结合零样本场景识别和视图特异性VideoMAE分类器。']

Result: 方法一准确率为43.6%。方法二准确率为47.8%。两阶段方法表现出更优越的性能。

Conclusion: 两阶段方法的卓越性能证明了场景条件建模对于能力评估的有效性。

Abstract: This report presents the CuriosAI team's submission to the EgoExo4D
Proficiency Estimation Challenge at CVPR 2025. We propose two methods for
multi-view skill assessment: (1) a multi-task learning framework using
Sapiens-2B that jointly predicts proficiency and scenario labels (43.6 %
accuracy), and (2) a two-stage pipeline combining zero-shot scenario
recognition with view-specific VideoMAE classifiers (47.8 % accuracy). The
superior performance of the two-stage approach demonstrates the effectiveness
of scenario-conditioned modeling for proficiency estimation.

</details>


### [56] [Self-Consistency in Vision-Language Models for Precision Agriculture: Multi-Response Consensus for Crop Disease Management](https://arxiv.org/abs/2507.08024)
*Mihir Gupta,Abhay Mangla,Ross Greer,Pratik Desai*

Main category: cs.CV

TL;DR: 本研究提出了一种结合提示词专家评估和自一致性机制的领域感知框架，显著提升了视觉语言模型在精准农业（特别是玉米叶病害识别）中的表现，且适用于移动设备部署。


<details>
  <summary>Details</summary>
Motivation: 精准农业高度依赖准确的图像分析进行作物病害识别和治疗推荐，但现有视觉语言模型在专业农业领域表现不佳。

Method: 本工作提出了一个领域感知框架，结合了：1) 基于提示词的专家评估协议，将语言模型配置为植物病理学专家进行可扩展评估；2) 余弦一致性自投票机制，通过领域适应嵌入选择语义最连贯的诊断。该方法应用于玉米叶病害识别，并使用微调的PaliGemma模型。

Result: 与标准贪婪解码相比，该方法将诊断准确率从82.2%提升至87.8%，症状分析从38.9%提升至52.2%，治疗建议从27.8%提升至43.3%。系统足够紧凑，可部署在移动设备上，支持资源受限环境下的实时农业决策。

Conclusion: 研究结果表明，该AI驱动的精准农业工具具有巨大潜力，可在不同田间条件下可靠运行。

Abstract: Precision agriculture relies heavily on accurate image analysis for crop
disease identification and treatment recommendation, yet existing
vision-language models (VLMs) often underperform in specialized agricultural
domains. This work presents a domain-aware framework for agricultural image
processing that combines prompt-based expert evaluation with self-consistency
mechanisms to enhance VLM reliability in precision agriculture applications. We
introduce two key innovations: (1) a prompt-based evaluation protocol that
configures a language model as an expert plant pathologist for scalable
assessment of image analysis outputs, and (2) a cosine-consistency self-voting
mechanism that generates multiple candidate responses from agricultural images
and selects the most semantically coherent diagnosis using domain-adapted
embeddings. Applied to maize leaf disease identification from field images
using a fine-tuned PaliGemma model, our approach improves diagnostic accuracy
from 82.2\% to 87.8\%, symptom analysis from 38.9\% to 52.2\%, and treatment
recommendation from 27.8\% to 43.3\% compared to standard greedy decoding. The
system remains compact enough for deployment on mobile devices, supporting
real-time agricultural decision-making in resource-constrained environments.
These results demonstrate significant potential for AI-driven precision
agriculture tools that can operate reliably in diverse field conditions.

</details>


### [57] [Development of a Canada-Wide Morphology Map for the ITU-R P. 1411 Propagation Model](https://arxiv.org/abs/2507.08026)
*Jennifer P. T. Nguyen*

Main category: cs.CV

TL;DR: 本文开发了一个加拿大范围的形态学地图，使用机器学习方法对区域进行分类，以提高室外短距离传播路径损耗估计的准确性。


<details>
  <summary>Details</summary>
Motivation: ITU-R P.1411-12传播模型指南中的环境类型描述符具有定性性质，需要一种自动化的方法来解决分类问题，以获得更准确的路径损耗估计。

Method: 采用机器学习方法自动化分类过程，并进行大量实验以优化分类准确性。

Result: 开发了一个加拿大范围的形态学地图，将区域划分为住宅区、城市低层和城市高层环境。

Conclusion: 该加拿大范围的形态学地图，基于机器学习方法，能够为300 MHz至100 GHz频率范围内的室外短距离传播提供更准确的路径损耗估计。

Abstract: This paper outlines the development of a Canada-wide morphology map
classifying regions into residential, urban low-rise, and urban high-rise
environments, following the ITU-R P.1411-12 propagation model guidelines. To
address the qualitative nature of the environment-type descriptors found in the
Recommendation, a machine learning approach is employed to automate the
classification process. Extensive experimentation optimized classification
accuracy, resulting in a Canada-wide morphology map that ensures more accurate
path loss estimations for outdoor short-range propagation at frequencies
ranging from 300 MHz to 100 GHz.

</details>


### [58] [Towards Evaluating Robustness of Prompt Adherence in Text to Image Models](https://arxiv.org/abs/2507.08039)
*Sujith Vemishetty,Advitiya Arora,Anupama Sharma*

Main category: cs.CV

TL;DR: 本文旨在评估文生图模型对提示词的遵循度和鲁棒性，通过构建新数据集和基于GPT-4o的评估流程，发现现有模型（如Stable Diffusion和Janus系列）在生成简单几何形状及位置的图像时表现不佳，且未能遵循输入数据分布。


<details>
  <summary>Details</summary>
Motivation: 鉴于多模态大语言模型和文生图模型日益普及，但对其性能和鲁棒性，特别是其对提示词遵循度的评估研究尚不足。

Method: 构建了一个新数据集，用于评估文生图模型在生成符合输入提示词中指定变异因素的图像时的鲁棒性。采用一个基于GPT-4o的评估流程：首先用GPT-4o为真实图像生成描述作为输入提示，然后文生图模型根据此提示生成图像，再用GPT-4o描述生成的图像，最后通过比较两次描述的差异来评估提示遵循度。同时，使用预训练的VAE评估模型生成的图像是否符合输入数据集分布。研究对象包括Stable Diffusion（3个变体）和Janus（2个变体）。

Result: 研究发现这些模型在创建仅包含两种变异因素（简单几何形状及其位置）的二值图像时表现出困难。此外，通过预训练的VAE分析，模型未能生成符合输入数据集分布的图像。

Conclusion: 当前文生图模型在理解和准确遵循简单提示词以及生成符合输入数据分布的图像方面存在显著限制，亟需提升其鲁棒性和提示遵循度。

Abstract: The advancements in the domain of LLMs in recent years have surprised many,
showcasing their remarkable capabilities and diverse applications. Their
potential applications in various real-world scenarios have led to significant
research on their reliability and effectiveness. On the other hand, multimodal
LLMs and Text-to-Image models have only recently gained prominence, especially
when compared to text-only LLMs. Their reliability remains constrained due to
insufficient research on assessing their performance and robustness. This paper
aims to establish a comprehensive evaluation framework for Text-to-Image
models, concentrating particularly on their adherence to prompts. We created a
novel dataset that aimed to assess the robustness of these models in generating
images that conform to the specified factors of variation in the input text
prompts. Our evaluation studies present findings on three variants of Stable
Diffusion models: Stable Diffusion 3 Medium, Stable Diffusion 3.5 Large, and
Stable Diffusion 3.5 Large Turbo, and two variants of Janus models: Janus Pro
1B and Janus Pro 7B. We introduce a pipeline that leverages text descriptions
generated by the gpt-4o model for our ground-truth images, which are then used
to generate artificial images by passing these descriptions to the
Text-to-Image models. We then pass these generated images again through gpt-4o
using the same system prompt and compare the variation between the two
descriptions. Our results reveal that these models struggle to create simple
binary images with only two factors of variation: a simple geometric shape and
its location. We also show, using pre-trained VAEs on our dataset, that they
fail to generate images that follow our input dataset distribution.

</details>


### [59] [ConsNoTrainLoRA: Data-driven Weight Initialization of Low-rank Adapters using Constraints](https://arxiv.org/abs/2507.08044)
*Debasmit Das,Hyoungwoo Park,Munawar Hayat,Seokeon Choi,Sungrack Yun,Fatih Porikli*

Main category: cs.CV

TL;DR: 本文提出了一种名为ConsNoTrainLoRA (CNTLoRA) 的数据驱动LoRA权重初始化方法，通过将初始化视为域转移问题并提供闭式解，实现了LoRA微调的更快收敛和更好性能，且初始化过程无需训练。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA权重矩阵通常随机初始化且秩固定，这可能限制了微调的收敛速度和最终性能。研究旨在提升LoRA微调的收敛性和最终表现。

Method: 提出CNTLoRA，将LoRA初始化表达为域转移问题，并利用预训练和微调激活之间的多个约束。通过重新公式化这些约束，获得了依赖于预训练权重和微调激活向量的LoRA权重闭式估计，因此初始化过程中无需训练。该方法还支持可变秩的上下矩阵初始化。

Result: 在图像生成、图像分类和图像理解等下游任务上，定量和定性结果均表明CNTLoRA优于标准和现有数据驱动的权重初始化方法。

Conclusion: CNTLoRA提供了一种优化LoRA微调的有效方法，通过数据驱动的权重初始化，显著提升了模型收敛速度和性能，且其设计选择得到了广泛分析和消融实验的验证。

Abstract: Foundation models are pre-trained on large-scale datasets and subsequently
fine-tuned on small-scale datasets using parameter-efficient fine-tuning (PEFT)
techniques like low-rank adapters (LoRA). In most previous works, LoRA weight
matrices are randomly initialized with a fixed rank across all attachment
points. In this paper, we improve convergence and final performance of LoRA
fine-tuning, using our proposed data-driven weight initialization method,
ConsNoTrainLoRA (CNTLoRA). We express LoRA initialization as a domain shift
problem where we use multiple constraints relating the pre-training and
fine-tuning activations. By reformulating these constraints, we obtain a
closed-form estimate of LoRA weights that depends on pre-training weights and
fine-tuning activation vectors and hence requires no training during
initialization. This weight estimate is decomposed to initialize the up and
down matrices with proposed flexibility of variable ranks. With the proposed
initialization method, we fine-tune on downstream tasks such as image
generation, image classification and image understanding. Both quantitative and
qualitative results demonstrate that CNTLoRA outperforms standard and
data-driven weight initialization methods. Extensive analyses and ablations
further elucidate the design choices of our framework, providing an optimal
recipe for faster convergence and enhanced performance.

</details>


### [60] [A Hybrid Multilayer Extreme Learning Machine for Image Classification with an Application to Quadcopters](https://arxiv.org/abs/2507.08047)
*Rolando A. Hernandez-Hernandez,Adrian Rubio-Solis*

Main category: cs.CV

TL;DR: 本文提出了一种混合多层极限学习机（HML-ELM）模型，结合ELM-AE进行特征提取和简化的区间二型模糊ELM（SIT2-FELM）进行分类，用于主动图像分类，特别是在无人机应用中表现出卓越的效率。


<details>
  <summary>Details</summary>
Motivation: 多层极限学习机（ML-ELM）及其变体已证明在自然信号分类中是有效的。本研究旨在通过提出一种混合模型来改进主动图像分类，并将其应用于无人机（UAVs）。

Method: 提出的HML-ELM是一个分层的ELM学习框架，包含两个主要阶段：1) 自学习特征提取，通过堆叠ELM-AE实现无监督多层特征编码；2) 监督特征分类，使用基于SC算法（COSTRWSR改进版）的简化区间二型模糊ELM（SIT2-FELM）进行分类。通过图像分类基准问题和无人机主动分类及物体运输的真实实验来验证其效率。

Result: 实验证明，所提出的HML-ELM相比其他类似方法（如ML-ELM、ML-FELM和ELM）具有更优越的效率。

Conclusion: HML-ELM是一种高效且性能卓越的主动图像分类方法，特别适用于无人机应用，优于现有的ELM相关技术。

Abstract: Multilayer Extreme Learning Machine (ML-ELM) and its variants have proven to
be an effective technique for the classification of different natural signals
such as audio, video, acoustic and images. In this paper, a Hybrid Multilayer
Extreme Learning Machine (HML-ELM) that is based on ELM-based autoencoder
(ELM-AE) and an Interval Type-2 fuzzy Logic theory is suggested for active
image classification and applied to Unmanned Aerial Vehicles (UAVs). The
proposed methodology is a hierarchical ELM learning framework that consists of
two main phases: 1) self-taught feature extraction and 2) supervised feature
classification. First, unsupervised multilayer feature encoding is achieved by
stacking a number of ELM-AEs, in which input data is projected into a number of
high-level representations. At the second phase, the final features are
classified using a novel Simplified Interval Type-2 Fuzzy ELM (SIT2-FELM) with
a fast output reduction layer based on the SC algorithm; an improved version of
the algorithm Center of Sets Type Reducer without Sorting Requirement
(COSTRWSR). To validate the efficiency of the HML-ELM, two types of experiments
for the classification of images are suggested. First, the HML-ELM is applied
to solve a number of benchmark problems for image classification. Secondly, a
number of real experiments to the active classification and transport of four
different objects between two predefined locations using a UAV is implemented.
Experiments demonstrate that the proposed HML-ELM delivers a superior
efficiency compared to other similar methodologies such as ML-ELM, Multilayer
Fuzzy Extreme Learning Machine (ML-FELM) and ELM.

</details>


### [61] [Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging](https://arxiv.org/abs/2507.08052)
*Mazen Ali,António Pereira,Fabio Gentile,Aser Cortines,Sam Mugel,Román Orús,Stelios P. Neophytides,Michalis Mavrovouniotis*

Main category: cs.CV

TL;DR: 本研究评估了多种机器学习模型（包括梯度提升和CNN）在多光谱卫星图像云和云影遮罩中的应用。结果显示，轻量级、带特征约减的CNN模型表现出最佳的效率和高准确性，适用于卫星板载AI系统。


<details>
  <summary>Details</summary>
Motivation: 云和云影遮罩是高光谱卫星成像中的关键预处理步骤，旨在提取高质量、可分析的数据。

Method: 本研究评估了多种机器学习方法，包括XGBoost、LightGBM等梯度提升方法以及卷积神经网络（CNN）。特别对带特征约减的CNN模型进行了优化和评估。

Result: 所有梯度提升和CNN模型的准确率均超过93%。其中，带特征约减的CNN模型表现出最高效率，在CPU和GPU上均实现了高准确性、低存储需求和快速推理时间。此版本仅需最多597个可训练参数，在部署可行性、准确性和计算效率方面达到最佳平衡。

Conclusion: 研究结果证明了轻量级人工智能模型在实时高光谱图像处理方面的巨大潜力，为开发用于空间应用的卫星板载AI系统提供了支持。

Abstract: Cloud and cloud shadow masking is a crucial preprocessing step in
hyperspectral satellite imaging, enabling the extraction of high-quality,
analysis-ready data. This study evaluates various machine learning approaches,
including gradient boosting methods such as XGBoost and LightGBM as well as
convolutional neural networks (CNNs). All boosting and CNN models achieved
accuracies exceeding 93%. Among the investigated models, the CNN with feature
reduction emerged as the most efficient, offering a balance of high accuracy,
low storage requirements, and rapid inference times on both CPUs and GPUs.
Variations of this version, with only up to 597 trainable parameters,
demonstrated the best trade-off in terms of deployment feasibility, accuracy,
and computational efficiency. These results demonstrate the potential of
lightweight artificial intelligence (AI) models for real-time hyperspectral
image processing, supporting the development of on-board satellite AI systems
for space-based applications.

</details>


### [62] [The relative importance of being Gaussian](https://arxiv.org/abs/2507.08059)
*F. Alberto Grünbaum,Tondgi Xu*

Main category: cs.CV

TL;DR: 本文研究了扩散模型去噪算法在输入非高斯噪声时，不修改算法的前提下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 扩散模型去噪算法的设计和数学基础基于高斯噪声特性。作者旨在探讨当噪声性质与高斯分布大相径庭时，这些算法的鲁棒性和性能如何。

Method: 使用现有扩散模型去噪算法，但不进行任何修改。将高斯噪声替换为非高斯噪声，例如均匀分布噪声、Beta分布噪声或两种方差差异很大的高斯分布的随机叠加噪声。所有实验均在一台小型笔记本电脑上进行，并使用最小的图像尺寸。

Result: 抽象部分未提供具体研究结果。它仅描述了实验设置和研究方向。

Conclusion: 抽象部分未得出具体结论。它指出探索在不同情况下观测结果的确认或变化仍是一个有趣的挑战，暗示了未来的研究方向。

Abstract: The remarkable results for denoising in computer vision using diffusion
models given in \cite{SDWMG,HJA,HHG} yield a robust mathematical justification
for algorithms based on crucial properties of a sequence of Gaussian
independent $N(0,1)$ random variables. In particular the derivations use the
fact that a Gaussian distribution is determined by its mean and variance and
that the sum of two Gaussians is another Gaussian.
  \bigskip
  The issue raised in this short note is the following: suppose we use the
algorithm without any changes but replace the nature of the noise and use, for
instance, uniformly distributed noise or noise with a Beta distribution, or
noise which is a random superposition of two Gaussians with very different
variances. One could, of course, try to modify the algorithm keeping in mind
the nature of the noise, but this is not what we do. Instead we study the
performance of the algorithm when used with noise that is very far in nature
from the Gaussian case, where it is designed to work well.
  Usually these algorithms are implemented on very powerful computers. Our
experiments are all carried out on a small laptop and for the smallest possible
image size. Exploring how our observations are confirmed or changed when
dealing in different situations remains an interesting challenge.

</details>


### [63] [An Object-Based Deep Learning Approach for Building Height Estimation from Single SAR Images](https://arxiv.org/abs/2507.08096)
*Babak Memar,Luigi Russo,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: 本文提出一种基于深度学习的方法，利用单幅高分辨率合成孔径雷达（SAR）图像自动估计建筑物高度，并在多大洲数据集上展现出跨区域泛化的潜力。


<details>
  <summary>Details</summary>
Motivation: 准确估计建筑物高度对于各种城市应用至关重要，而利用甚高分辨率（VHR）合成孔径雷达（SAR）图像进行估计是关键挑战。

Method: 引入了一种基于深度学习（DL）的方法，采用目标检测（包围盒检测）后进行回归估计建筑物高度。该模型在包含欧洲、北美、南美和亚洲八个不同城市的独特多大洲数据集上进行训练和评估，并采用交叉验证策略评估其域外（OOD）泛化能力。

Result: 该方法表现出良好性能，尤其在欧洲城市，模型的平均绝对误差（MAE）约为一个楼层（慕尼黑为2.20米），显著优于现有最先进方法。尽管在推广到其他大洲城市（特别是亚洲）时观察到更大的变异性，但结果仍然有潜力。

Conclusion: 该研究强调了深度学习在利用单幅甚高分辨率SAR数据进行建筑物高度估计方面，实现鲁棒的跨城市和跨大陆迁移学习的巨大潜力。

Abstract: Accurate estimation of building heights using very high resolution (VHR)
synthetic aperture radar (SAR) imagery is crucial for various urban
applications. This paper introduces a Deep Learning (DL)-based methodology for
automated building height estimation from single VHR COSMO-SkyMed images: an
object-based regression approach based on bounding box detection followed by
height estimation. This model was trained and evaluated on a unique
multi-continental dataset comprising eight geographically diverse cities across
Europe, North and South America, and Asia, employing a cross-validation
strategy to explicitly assess out-of-distribution (OOD) generalization. The
results demonstrate highly promising performance, particularly on European
cities where the model achieves a Mean Absolute Error (MAE) of approximately
one building story (2.20 m in Munich), significantly outperforming recent
state-of-the-art methods in similar OOD scenarios. Despite the increased
variability observed when generalizing to cities in other continents,
particularly in Asia with its distinct urban typologies and prevalence of
high-rise structures, this study underscores the significant potential of DL
for robust cross-city and cross-continental transfer learning in building
height estimation from single VHR SAR data.

</details>


### [64] [RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration](https://arxiv.org/abs/2507.08136)
*Chong Cheng,Yu Hu,Sicheng Yu,Beizhen Zhao,Zijian Wang,Hao Wang*

Main category: cs.CV

TL;DR: RegGS是一种基于3D高斯配准的框架，用于从稀疏无姿态图像重建场景。它通过对局部3D高斯进行全局对齐，并结合姿态估计，实现了高质量的新视角合成。


<details>
  <summary>Details</summary>
Motivation: 优化型3DGS方法在稀疏视图下因缺乏先验知识而表现不佳。前向高斯方法受输入格式限制，难以整合更多输入视图。因此，需要解决从无姿态稀疏视图重建场景的挑战。

Method: 提出RegGS框架，将前向网络生成的局部3D高斯对齐为全局一致的表示。核心技术包括：1) 使用熵正则化的Sinkhorn算法求解最优传输Mixture 2-Wasserstein (MW2) 距离，作为Sim(3)空间中GMMs的对齐度量。2) 设计一个联合3DGS配准模块，整合MW2距离、光度一致性和深度几何，实现粗到精的配准过程，同时估计相机姿态和对齐场景。

Result: 在RE10K和ACID数据集上的实验证明，RegGS能有效且高保真地配准局部高斯，实现了精确的姿态估计和高质量的新视角合成。

Conclusion: RegGS通过创新的高斯配准和联合优化策略，成功解决了3DGS在稀疏无姿态图像重建中的挑战，显著提升了场景重建质量和姿态估计精度。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated its potential in reconstructing
scenes from unposed images. However, optimization-based 3DGS methods struggle
with sparse views due to limited prior knowledge. Meanwhile, feed-forward
Gaussian approaches are constrained by input formats, making it challenging to
incorporate more input views. To address these challenges, we propose RegGS, a
3D Gaussian registration-based framework for reconstructing unposed sparse
views. RegGS aligns local 3D Gaussians generated by a feed-forward network into
a globally consistent 3D Gaussian representation. Technically, we implement an
entropy-regularized Sinkhorn algorithm to efficiently solve the optimal
transport Mixture 2-Wasserstein $(\text{MW}_2)$ distance, which serves as an
alignment metric for Gaussian mixture models (GMMs) in $\mathrm{Sim}(3)$ space.
Furthermore, we design a joint 3DGS registration module that integrates the
$\text{MW}_2$ distance, photometric consistency, and depth geometry. This
enables a coarse-to-fine registration process while accurately estimating
camera poses and aligning the scene. Experiments on the RE10K and ACID datasets
demonstrate that RegGS effectively registers local Gaussians with high
fidelity, achieving precise pose estimation and high-quality novel-view
synthesis. Project page: https://3dagentworld.github.io/reggs/.

</details>


### [65] [Temporally Consistent Amodal Completion for 3D Human-Object Interaction Reconstruction](https://arxiv.org/abs/2507.08137)
*Hyungjun Doh,Dong In Lee,Seunggeun Chi,Pin-Hao Huang,Kwonjoon Lee,Sangpil Kim,Karthik Ramani*

Main category: cs.CV

TL;DR: 一个新颖的框架，通过非模态补全和时序整合，从单目视频重建动态人-物体交互，有效克服遮挡和时间不一致问题。


<details>
  <summary>Details</summary>
Motivation: 传统3D重建方法在处理动态物体、部分遮挡或相互遮挡时性能不佳，因为它们通常假设物体静态或可见性完整。

Method: 该框架利用非模态补全技术推断被遮蔽区域的完整结构；整合时序上下文，在视频序列中强制一致性以逐步细化和稳定重建；采用无模板策略适应不同条件；使用3D高斯泼溅进行验证。

Result: 与现有技术相比，在处理遮挡和保持时间稳定性方面表现出卓越的精度，显著增强了动态场景中复杂细节的恢复。

Conclusion: 该框架成功解决了单目视频中动态人-物体交互重建的挑战，尤其在遮挡处理和时间稳定性方面实现了显著提升。

Abstract: We introduce a novel framework for reconstructing dynamic human-object
interactions from monocular video that overcomes challenges associated with
occlusions and temporal inconsistencies. Traditional 3D reconstruction methods
typically assume static objects or full visibility of dynamic subjects, leading
to degraded performance when these assumptions are violated-particularly in
scenarios where mutual occlusions occur. To address this, our framework
leverages amodal completion to infer the complete structure of partially
obscured regions. Unlike conventional approaches that operate on individual
frames, our method integrates temporal context, enforcing coherence across
video sequences to incrementally refine and stabilize reconstructions. This
template-free strategy adapts to varying conditions without relying on
predefined models, significantly enhancing the recovery of intricate details in
dynamic scenes. We validate our approach using 3D Gaussian Splatting on
challenging monocular videos, demonstrating superior precision in handling
occlusions and maintaining temporal stability compared to existing techniques.

</details>


### [66] [Adaptive Diffusion Denoised Smoothing : Certified Robustness via Randomized Smoothing with Differentially Private Guided Denoising Diffusion](https://arxiv.org/abs/2507.08163)
*Frederick Shpilevskiy,Saiyue Lyu,Krishnamurthy Dj Dvijotham,Mathias Lécuyer,Pierre-André Noël*

Main category: cs.CV

TL;DR: 提出一种自适应扩散去噪平滑方法，用于在对抗性攻击下认证视觉模型的预测，并能适应输入。


<details>
  <summary>Details</summary>
Motivation: 解决视觉模型面对对抗性样本时的预测认证问题，并实现对输入的自适应。

Method: 将引导式去噪扩散模型重新解释为一系列自适应高斯差分隐私（GDP）机制，并通过GDP隐私过滤器组合这些机制，分析引导去噪过程的端到端鲁棒性，从而提供可证明的认证，扩展了自适应随机平滑分析。

Result: 在特定引导策略下，该方法在ImageNet数据集的L2威胁模型下，同时提高了认证准确率和标准准确率。

Conclusion: 该研究通过引入自适应扩散去噪平滑，提供了一种可证明的、针对对抗性样本的视觉模型鲁棒性认证方法，有效提升了模型性能。

Abstract: We propose Adaptive Diffusion Denoised Smoothing, a method for certifying the
predictions of a vision model against adversarial examples, while adapting to
the input. Our key insight is to reinterpret a guided denoising diffusion model
as a long sequence of adaptive Gaussian Differentially Private (GDP) mechanisms
refining a pure noise sample into an image. We show that these adaptive
mechanisms can be composed through a GDP privacy filter to analyze the
end-to-end robustness of the guided denoising process, yielding a provable
certification that extends the adaptive randomized smoothing analysis. We
demonstrate that our design, under a specific guiding strategy, can improve
both certified accuracy and standard accuracy on ImageNet for an $\ell_2$
threat model.

</details>


### [67] [An Embedded Real-time Object Alert System for Visually Impaired: A Monocular Depth Estimation based Approach through Computer Vision](https://arxiv.org/abs/2507.08165)
*Jareen Anjom,Rashik Iram Chowdhury,Tarbia Hasan,Md. Ishan Arefin Hossain*

Main category: cs.CV

TL;DR: 针对孟加拉国视障人士出行安全问题，本研究提出并开发了一种基于迁移学习的轻量级实时深度估计与物体检测警报系统，能有效预警近距离障碍物以避免碰撞。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国城市道路障碍物多，导致视障人士日常通勤面临巨大挑战并易发生交通事故。因此，迫切需要开发一个能提前警示近距离物体的系统。

Method: 本研究提出了一种新型警报系统，利用迁移学习训练深度估计和物体检测模型，并将两者结合。为使模型轻量化并高效部署于嵌入式系统，采用了量化技术进行优化。

Result: 所提出的解决方案实现了一个轻量级的实时深度估计和物体检测模型，mAP50达到了0.801。

Conclusion: 该系统能有效警示视障人士附近的物体，帮助他们在繁忙的街道上安全通勤，避免碰撞。

Abstract: Visually impaired people face significant challenges in their day-to-day
commutes in the urban cities of Bangladesh due to the vast number of
obstructions on every path. With many injuries taking place through road
accidents on a daily basis, it is paramount for a system to be developed that
can alert the visually impaired of objects at close distance beforehand. To
overcome this issue, a novel alert system is proposed in this research to
assist the visually impaired in commuting through these busy streets without
colliding with any objects. The proposed system can alert the individual to
objects that are present at a close distance. It utilizes transfer learning to
train models for depth estimation and object detection, and combines both
models to introduce a novel system. The models are optimized through the
utilization of quantization techniques to make them lightweight and efficient,
allowing them to be easily deployed on embedded systems. The proposed solution
achieved a lightweight real-time depth estimation and object detection model
with an mAP50 of 0.801.

</details>


### [68] [HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and Resolution-Robust 3D Image Segmentation](https://arxiv.org/abs/2507.08205)
*Ken C. L. Wong,Hongzhi Wang,Tanveer Syeda-Mahmood*

Main category: cs.CV

TL;DR: HNOSeg-XS是一个基于哈特利变换傅里叶神经算子的医学图像分割模型，解决了传统CNN和Transformer在处理高分辨率图像时的效率和分辨率鲁棒性问题，展现出卓越的速度、内存和参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割模型（CNN和Transformer）在处理长距离空间关联、高计算/内存成本以及在高分辨率应用时的性能下降（因需缩小输入尺寸训练）等方面存在局限性，缺乏分辨率鲁棒性。

Method: 提出HNOSeg-XS架构，通过可学习的偏微分方程和傅里叶神经算子（FNO）进行图像分割。核心创新在于将傅里叶变换替换为哈特利变换，并在频域重构问题，使模型具备零样本超分辨率特性，从而实现分辨率鲁棒性、高速度、内存和参数效率。

Result: 在BraTS'23、KiTS'23和MVSeg'23数据集上，HNOSeg-XS展现出卓越的分辨率鲁棒性，模型参数少于34.7k。与测试的CNN和Transformer模型相比，实现了最佳的推理时间（< 0.24 s）和内存效率（< 1.8 GiB）。

Conclusion: HNOSeg-XS提供了一种针对医学图像分割的高效、内存友好、参数极简且分辨率鲁棒的新方法，有效克服了现有模型在高分辨率应用中的挑战。

Abstract: In medical image segmentation, convolutional neural networks (CNNs) and
transformers are dominant. For CNNs, given the local receptive fields of
convolutional layers, long-range spatial correlations are captured through
consecutive convolutions and pooling. However, as the computational cost and
memory footprint can be prohibitively large, 3D models can only afford fewer
layers than 2D models with reduced receptive fields and abstract levels. For
transformers, although long-range correlations can be captured by multi-head
attention, its quadratic complexity with respect to input size is
computationally demanding. Therefore, either model may require input size
reduction to allow more filters and layers for better segmentation.
Nevertheless, given their discrete nature, models trained with patch-wise
training or image downsampling may produce suboptimal results when applied on
higher resolutions. To address this issue, here we propose the
resolution-robust HNOSeg-XS architecture. We model image segmentation by
learnable partial differential equations through the Fourier neural operator
which has the zero-shot super-resolution property. By replacing the Fourier
transform by the Hartley transform and reformulating the problem in the
frequency domain, we created the HNOSeg-XS model, which is resolution robust,
fast, memory efficient, and extremely parameter efficient. When tested on the
BraTS'23, KiTS'23, and MVSeg'23 datasets with a Tesla V100 GPU, HNOSeg-XS
showed its superior resolution robustness with fewer than 34.7k model
parameters. It also achieved the overall best inference time (< 0.24 s) and
memory efficiency (< 1.8 GiB) compared to the tested CNN and transformer
models.

</details>


### [69] [SurfDist: Interpretable Three-Dimensional Instance Segmentation Using Curved Surface Patches](https://arxiv.org/abs/2507.08223)
*Jackson Borchardt,Saul Kato*

Main category: cs.CV

TL;DR: SurfDist是一种用于三维体实例分割的卷积神经网络，通过预测参数曲面补丁组成的闭合表面来表示实例，解决了传统模型（如StarDist-3D）的分辨率耦合问题，并在生物医学图像中展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有三维体实例分割模型（如StarDist-3D）在实例参数化维度和实例体素分辨率之间存在耦合，导致在高分辨率上采样时引入体素化伪影。研究旨在开发一种新的架构，能够生成任意高分辨率且无伪影的实例预测，特别适用于生物医学成像中常见的blob状实例。

Method: 本文提出了SurfDist，一个基于流行模型StarDist-3D修改的卷积神经网络架构。SurfDist的核心在于能够预测由平滑参数曲面补丁（特别是双三次Bézier三角形）组成的闭合表面来表示实例。这种方法解耦了实例参数化维度与实例体素分辨率，从而允许将预测结果上采样到任意高分辨率而不会引入体素化伪影。

Result: 在针对生物医学成像中常见的blob状实例的数据集上，SurfDist以更紧凑的实例参数化，其性能超越了StarDist-3D。研究通过一个合成数据集和一个真实世界数据集展示了SurfDist的性能优势。

Conclusion: 研究结果表明，可解释的实例表面模型可以与实例成员关系有效地共同学习。SurfDist提供了一种在三维体实例分割中，尤其是在生物医学图像应用中，实现高分辨率、无伪影和紧凑实例表示的有效方法。

Abstract: We present SurfDist, a convolutional neural network architecture for
three-dimensional volumetric instance segmentation. SurfDist enables prediction
of instances represented as closed surfaces composed of smooth parametric
surface patches, specifically bicubic B\'ezier triangles. SurfDist is a
modification of the popular model architecture StarDist-3D which breaks
StarDist-3D's coupling of instance parameterization dimension and instance
voxel resolution, and it produces predictions which may be upsampled to
arbitrarily high resolutions without introduction of voxelization artifacts.
  For datasets with blob-shaped instances, common in biomedical imaging,
SurfDist can outperform StarDist-3D with more compact instance
parameterizations. We detail SurfDist's technical implementation and show one
synthetic and one real-world dataset for which it outperforms StarDist-3D.
These results demonstrate that interpretable instance surface models can be
learned effectively alongside instance membership.

</details>


### [70] [Car Object Counting and Position Estimation via Extension of the CLIP-EBC Framework](https://arxiv.org/abs/2507.08240)
*Seoik Jung,Taekyung Song*

Main category: cs.CV

TL;DR: 本文将人群计数框架CLIP-EBC应用于汽车计数，取得了次优性能，并提出K-means加权聚类方法以支持目标定位。


<details>
  <summary>Details</summary>
Motivation: 探索将原本用于人群计数的CLIP-EBC框架应用于汽车目标计数的可能性。

Method: 利用CLIP-EBC框架在CARPK数据集上进行汽车目标计数，并提出基于预测密度图的K-means加权聚类方法以估计目标位置。

Result: 该模型在汽车目标计数任务中取得了次优性能。K-means加权聚类方法表明该框架在定位任务上的扩展潜力。

Conclusion: CLIP-EBC框架适用于汽车目标计数，且通过提出的K-means加权聚类方法，该框架的功能可进一步扩展至目标定位任务。

Abstract: In this paper, we investigate the applicability of the CLIP-EBC framework,
originally designed for crowd counting, to car object counting using the CARPK
dataset. Experimental results show that our model achieves second-best
performance compared to existing methods. In addition, we propose a K-means
weighted clustering method to estimate object positions based on predicted
density maps, indicating the framework's potential extension to localization
tasks.

</details>


### [71] [Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi Classification](https://arxiv.org/abs/2507.08248)
*Jason Kahei Tam,Murilo Gustineli,Anthony Miyaguchi*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Accurate identification of fungi species presents a unique challenge in
computer vision due to fine-grained inter-species variation and high
intra-species variation. This paper presents our approach for the FungiCLEF
2025 competition, which focuses on few-shot fine-grained visual categorization
(FGVC) using the FungiTastic Few-Shot dataset. Our team (DS@GT) experimented
with multiple vision transformer models, data augmentation, weighted sampling,
and incorporating textual information. We also explored generative AI models
for zero-shot classification using structured prompting but found them to
significantly underperform relative to vision-based models. Our final model
outperformed both competition baselines and highlighted the effectiveness of
domain specific pretraining and balanced sampling strategies. Our approach
ranked 35/74 on the private test set in post-completion evaluation, this
suggests additional work can be done on metadata selection and domain-adapted
multi-modal learning. Our code is available at
https://github.com/dsgt-arc/fungiclef-2025.

</details>


### [72] [Portable Biomechanics Laboratory: Clinically Accessible Movement Analysis from a Handheld Smartphone](https://arxiv.org/abs/2507.08268)
*J. D. Peiffer,Kunal Shah,Irina Djuraskovic,Shawana Anarwala,Kayan Abdou,Rujvee Patel,Prakash Jayabalan,Brenton Pennicooke,R. James Cotton*

Main category: cs.CV

TL;DR: 研究提出了一种通过手持智能手机视频进行全身运动学测量并经过临床验证的方法，名为便携式生物力学实验室（PBL），实现了对运动障碍的可及性监测。


<details>
  <summary>Details</summary>
Motivation: 运动是神经和肌肉骨骼健康的直接反映，但作为生命体征在临床实践中未被充分利用。临床医生缺乏可及且经过验证的客观方法来常规测量运动，这限制了生物力学测量在实践中的广泛应用，从而阻碍了更敏感的结局指标和更早识别损伤的潜力。

Method: 研究开发了便携式生物力学实验室（PBL），包括一个安全的、支持云功能的智能手机应用程序用于数据收集，以及一种用于将生物力学模型拟合到数据的新颖算法。研究团队使用大型、具有临床代表性的数据集对PBL的生物力学测量进行了广泛验证，并在神经外科和运动医学诊所测试了该系统的可用性和实用性。

Result: PBL的关节角度误差在3度以内，适用于神经损伤患者、下肢假肢使用者、儿科住院患者和对照组。PBL计算的步态指标易于使用，可靠性高，且对临床差异敏感。例如，在接受颈椎病减压手术的个体中，PBL步态指标与mJOA评分相关，并且比患者报告的结果对手术干预表现出更大的响应性。

Conclusion: 研究结果支持将手持智能手机视频作为一种可扩展、低负担的工具，用于捕获具有临床意义的生物力学数据，为可及的运动障碍监测提供了一条有前景的路径。这是首个经过临床验证的从手持智能手机视频测量全身运动学的方法。

Abstract: The way a person moves is a direct reflection of their neurological and
musculoskeletal health, yet it remains one of the most underutilized vital
signs in clinical practice. Although clinicians visually observe movement
impairments, they lack accessible and validated methods to objectively measure
movement in routine care. This gap prevents wider use of biomechanical
measurements in practice, which could enable more sensitive outcome measures or
earlier identification of impairment. We present our Portable Biomechanics
Laboratory (PBL), which includes a secure, cloud-enabled smartphone app for
data collection and a novel algorithm for fitting biomechanical models to this
data. We extensively validated PBL's biomechanical measures using a large,
clinically representative dataset. Next, we tested the usability and utility of
our system in neurosurgery and sports medicine clinics. We found joint angle
errors within 3 degrees across participants with neurological injury,
lower-limb prosthesis users, pediatric inpatients, and controls. In addition to
being easy to use, gait metrics computed from the PBL showed high reliability
and were sensitive to clinical differences. For example, in individuals
undergoing decompression surgery for cervical myelopathy, the mJOA score is a
common patient-reported outcome measure; we found that PBL gait metrics
correlated with mJOA scores and demonstrated greater responsiveness to surgical
intervention than the patient-reported outcomes. These findings support the use
of handheld smartphone video as a scalable, low-burden tool for capturing
clinically meaningful biomechanical data, offering a promising path toward
accessible monitoring of mobility impairments. We release the first clinically
validated method for measuring whole-body kinematics from handheld smartphone
video at
https://intelligentsensingandrehabilitation.github.io/MonocularBiomechanics/ .

</details>


### [73] [Cross-Resolution SAR Target Detection Using Structural Hierarchy Adaptation and Reliable Adjacency Alignment](https://arxiv.org/abs/2507.08290)
*Jiang Qin,Bin Zou,Haolin Li,Lamei Zhang*

Main category: cs.CV

TL;DR: 针对高分辨率SAR图像中散射特性差异导致的跨分辨率目标检测域适应性能下降问题，本文提出CR-Net模型，通过结合结构先验和证据学习，实现了可靠的跨分辨率SAR目标检测，并达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: SAR分辨率提高导致散射特性差异增大，损害目标检测模型的泛化能力。尽管域适应技术有潜力，但分辨率差异常导致盲目特征适应和不可靠语义传播，从而降低域适应性能。

Method: 提出CR-Net模型，将结构先验和证据学习理论融入检测模型，实现可靠的跨分辨率域适应。CR-Net包含：1) 结构诱导分层特征适应（SHFA）模块，用于建立目标结构关联和结构感知特征适应；2) 可靠结构邻接对齐（RSAA）模块，利用安全邻接集进行可靠语义对齐，迁移判别知识。

Result: 在不同分辨率数据集上的实验结果表明，所提出的CR-Net显著增强了跨分辨率适应能力，通过保留域内结构和提高判别性。

Conclusion: CR-Net在跨分辨率SAR目标检测中实现了最先进（SOTA）的性能。

Abstract: In recent years, continuous improvements in SAR resolution have significantly
benefited applications such as urban monitoring and target detection. However,
the improvement in resolution leads to increased discrepancies in scattering
characteristics, posing challenges to the generalization ability of target
detection models. While domain adaptation technology is a potential solution,
the inevitable discrepancies caused by resolution differences often lead to
blind feature adaptation and unreliable semantic propagation, ultimately
degrading the domain adaptation performance. To address these challenges, this
paper proposes a novel SAR target detection method (termed CR-Net), that
incorporates structure priors and evidential learning theory into the detection
model, enabling reliable domain adaptation for cross-resolution detection. To
be specific, CR-Net integrates Structure-induced Hierarchical Feature
Adaptation (SHFA) and Reliable Structural Adjacency Alignment (RSAA). SHFA
module is introduced to establish structural correlations between targets and
achieve structure-aware feature adaptation, thereby enhancing the
interpretability of the feature adaptation process. Afterwards, the RSAA module
is proposed to enhance reliable semantic alignment, by leveraging the secure
adjacency set to transfer valuable discriminative knowledge from the source
domain to the target domain. This further improves the discriminability of the
detection model in the target domain. Based on experimental results from
different-resolution datasets,the proposed CR-Net significantly enhances
cross-resolution adaptation by preserving intra-domain structures and improving
discriminability. It achieves state-of-the-art (SOTA) performance in
cross-resolution SAR target detection.

</details>


### [74] [M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and Alternating Optimization for Talking-head Generation](https://arxiv.org/abs/2507.08307)
*Kui Jiang,Shiyu Liu,Junjun Jiang,Xin Yang,Hongxun Yang,Xiaopeng Fan*

Main category: cs.CV

TL;DR: M2DAO-Talker是一种基于音频驱动的说话人生成模型，通过多粒度运动解耦和交替优化，解决了现有方法中的渲染伪影问题，显著提升了生成质量和真实感，并实现了高推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有3D音频驱动说话人生成方法在运动建模和内容合成方面存在局限性，导致渲染伪影（如运动模糊、时间抖动、局部穿透），根源在于难以表示稳定、精细的运动场。

Method: 该研究将说话人生成重构为统一框架，包含视频预处理、运动表示和渲染重建三个步骤。在此框架下，提出M2DAO-Talker模型，核心策略是多粒度运动解耦和交替优化。具体包括：设计2D肖像预处理管线以提取形变控制条件；采用多粒度运动解耦策略，独立建模非刚性（口部和面部）和刚性（头部）运动；开发运动一致性约束以确保头躯干运动学一致性；设计交替优化策略迭代细化面部和口部运动参数。

Result: M2DAO-Talker在多个数据集上实现了SOTA性能，生成质量PSNR提升2.43 dB，用户评估视频真实感比TalkingGaussian提升0.64，推理速度达到150 FPS。

Conclusion: M2DAO-Talker成功克服了现有音频驱动说话人生成方法中存在的渲染伪影问题，显著提高了生成视频的质量、真实感和效率，具有重要的应用潜力。

Abstract: Audio-driven talking head generation holds significant potential for film
production. While existing 3D methods have advanced motion modeling and content
synthesis, they often produce rendering artifacts, such as motion blur,
temporal jitter, and local penetration, due to limitations in representing
stable, fine-grained motion fields. Through systematic analysis, we reformulate
talking head generation into a unified framework comprising three steps: video
preprocessing, motion representation, and rendering reconstruction. This
framework underpins our proposed M2DAO-Talker, which addresses current
limitations via multi-granular motion decoupling and alternating
optimization.Specifically, we devise a novel 2D portrait preprocessing pipeline
to extract frame-wise deformation control conditions (motion region
segmentation masks, and camera parameters) to facilitate motion representation.
To ameliorate motion modeling, we elaborate a multi-granular motion decoupling
strategy, which independently models non-rigid (oral and facial) and rigid
(head) motions for improved reconstruction accuracy.Meanwhile, a motion
consistency constraint is developed to ensure head-torso kinematic consistency,
thereby mitigating penetration artifacts caused by motion aliasing. In
addition, an alternating optimization strategy is designed to iteratively
refine facial and oral motion parameters, enabling more realistic video
generation.Experiments across multiple datasets show that M2DAO-Talker achieves
state-of-the-art performance, with the 2.43 dB PSNR improvement in generation
quality and 0.64 gain in user-evaluated video realness versus TalkingGaussian
while with 150 FPS inference speed. Our project homepage is
https://m2dao-talker.github.io/M2DAO-Talk.github.io

</details>


### [75] [Cross-Domain Identity Representation for Skull to Face Matching with Benchmark DataSet](https://arxiv.org/abs/2507.08329)
*Ravi Shankar Prasad,Dinesh Singh*

Main category: cs.CV

TL;DR: 利用卷积Siamese网络，实现了从X光颅骨图像到光学面部图像的跨域身份识别，为法医颅面重建提供了有效方法，并取得了满意结果。


<details>
  <summary>Details</summary>
Motivation: 在法医科学中，颅面重建对于识别犯罪和灾难受害者至关重要。本研究旨在利用深度学习等计算机视觉技术，实现颅骨与已知身份面部的匹配识别。

Method: 提出了一个基于卷积Siamese网络的跨域身份识别框架。该网络通过最小化相似对的欧氏距离和最大化不相似对的欧氏距离进行训练。为解决数据稀缺问题，自行收集了一个包含40名志愿者的颅骨X光和面部光学图像数据集用于模型训练和验证。

Result: 在所收集的跨域数据集上进行的实验表明，该方法在从给定颅骨图像中识别人员方面取得了令人满意的效果。

Conclusion: 该研究成功地将卷积Siamese网络应用于颅骨-面部跨域识别，为法医身份识别提供了可行且有效的计算机视觉解决方案。

Abstract: Craniofacial reconstruction in forensic science is crucial for the
identification of the victims of crimes and disasters. The objective is to map
a given skull to its corresponding face in a corpus of faces with known
identities using recent advancements in computer vision, such as deep learning.
In this paper, we presented a framework for the identification of a person
given the X-ray image of a skull using convolutional Siamese networks for
cross-domain identity representation. Siamese networks are twin networks that
share the same architecture and can be trained to discover a feature space
where nearby observations that are similar are grouped and dissimilar
observations are moved apart. To do this, the network is exposed to two sets of
comparable and different data. The Euclidean distance is then minimized between
similar pairs and maximized between dissimilar ones. Since getting pairs of
skull and face images are difficult, we prepared our own dataset of 40
volunteers whose front and side skull X-ray images and optical face images were
collected. Experiments were conducted on the collected cross-domain dataset to
train and validate the Siamese networks. The experimental results provide
satisfactory results on the identification of a person from the given skull.

</details>


### [76] [Interpretability-Aware Pruning for Efficient Medical Image Analysis](https://arxiv.org/abs/2507.08330)
*Nikita Malik,Pratinav Seth,Neeraj Kumar Singh,Chintan Chitroda,Vinay Kumar Sankarapu*

Main category: cs.CV

TL;DR: 该研究提出一种可解释性指导的剪枝框架，用于减少医学图像分析模型的复杂性，同时保持预测性能和透明度，以实现轻量级、可解释的模型。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像分析中体积庞大且缺乏透明度，限制了其在临床实践中的应用。

Method: 引入一个基于可解释性指导的剪枝框架。该方法利用现有的可解释性技术（如DL-Backtrace、LRP、Integrated Gradients）评估神经网络各部分的贡献，并选择性地保留每层最相关的部分，从而实现有针对性的模型压缩。

Result: 在多个医学图像分类基准测试中，该方法实现了高压缩率，同时保持了最小的精度损失。

Conclusion: 该研究为在医疗领域实际部署轻量级、可解释的模型铺平了道路。

Abstract: Deep learning has driven significant advances in medical image analysis, yet
its adoption in clinical practice remains constrained by the large size and
lack of transparency in modern models. Advances in interpretability techniques
such as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated
Gradients make it possible to assess the contribution of individual components
within neural networks trained on medical imaging tasks. In this work, we
introduce an interpretability-guided pruning framework that reduces model
complexity while preserving both predictive performance and transparency. By
selectively retaining only the most relevant parts of each layer, our method
enables targeted compression that maintains clinically meaningful
representations. Experiments across multiple medical image classification
benchmarks demonstrate that this approach achieves high compression rates with
minimal loss in accuracy, paving the way for lightweight, interpretable models
suited for real-world deployment in healthcare settings.

</details>


### [77] [CoCo-Bot: Energy-based Composable Concept Bottlenecks for Interpretable Generative Models](https://arxiv.org/abs/2507.08334)
*Sangwon Kim,In-su Jang,Pyongkun Kim,Kwang-Ju Kim*

Main category: cs.CV

TL;DR: CoCo-Bot是一种新型生成式概念瓶颈模型，通过纯概念信息传输消除辅助视觉线索，显著提升了模型的可解释性和可控性，同时保持了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有生成式概念瓶颈模型（CBMs）依赖辅助视觉线索来弥补概念未捕捉到的信息，这损害了模型的可解释性和组合性。

Method: 提出CoCo-Bot模型，一个事后可插拔、可组合的概念瓶颈生成模型。它通过纯粹的显式概念传输所有信息，消除了对辅助线索的需求。该模型由基于扩散的能量函数指导，支持概念组合和否定等事后干预。

Result: 在CelebA-HQ数据集上预训练的StyleGAN2实验表明，CoCo-Bot提升了概念层面的可控性和可解释性，同时保持了有竞争力的视觉质量。

Conclusion: CoCo-Bot通过完全依赖显式概念传输信息，成功解决了现有CBMs的局限性，显著提升了生成模型的可解释性和可控性。

Abstract: Concept Bottleneck Models (CBMs) provide interpretable and controllable
generative modeling by routing generation through explicit,
human-understandable concepts. However, previous generative CBMs often rely on
auxiliary visual cues at the bottleneck to compensate for information not
captured by the concepts, which undermines interpretability and
compositionality. We propose CoCo-Bot, a post-hoc, composable concept
bottleneck generative model that eliminates the need for auxiliary cues by
transmitting all information solely through explicit concepts. Guided by
diffusion-based energy functions, CoCo-Bot supports robust post-hoc
interventions-such as concept composition and negation-across arbitrary
concepts. Experiments using StyleGAN2 pre-trained on CelebA-HQ show that
CoCo-Bot improves concept-level controllability and interpretability, while
maintaining competitive visual quality.

</details>


### [78] [Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement](https://arxiv.org/abs/2507.08340)
*Jia-Xuan Jiang,Jiashuai Liu,Hongtao Wu,Yifeng Wu,Zhong Wang,Qi Bi,Yefeng Zheng*

Main category: cs.CV

TL;DR: 针对多模态癌症预后模型跨癌种泛化能力差的问题，本文提出新的泛化任务，并引入SDIR和CADE模块以增强弱模态信号和有效融合多模态数据，实验证明了其优越的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习多模态模型在单一癌症生存预测中表现出色，但无法泛化到其他癌症类型。研究首次揭示多模态预后模型在跨癌种场景下的泛化能力通常不如单模态模型，而临床实践迫切需要这种鲁棒性。

Method: 提出“跨癌种单域泛化多模态预后”新任务，旨在评估模型在单一癌种上训练后对未知癌种的泛化能力。为解决弱模态特征退化和无效多模态融合两大挑战，引入两个即插即用模块：稀疏狄拉克信息平衡器（SDIR）用于通过稀疏化和稳定化增强弱模态信号；癌症感知分布纠缠（CADE）用于在潜在空间融合局部形态和全局基因表达，以合成目标域分布。

Result: 在包含四种癌症类型的基准测试中，所提出的方法展现出卓越的泛化性能。

Conclusion: 本研究为开发实用且鲁棒的跨癌种多模态预后模型奠定了基础。

Abstract: Deep learning has shown remarkable performance in integrating multimodal data
for survival prediction. However, existing multimodal methods mainly focus on
single cancer types and overlook the challenge of generalization across
cancers. In this work, we are the first to reveal that multimodal prognosis
models often generalize worse than unimodal ones in cross-cancer scenarios,
despite the critical need for such robustness in clinical practice. To address
this, we propose a new task: Cross-Cancer Single Domain Generalization for
Multimodal Prognosis, which evaluates whether models trained on a single cancer
type can generalize to unseen cancers. We identify two key challenges: degraded
features from weaker modalities and ineffective multimodal integration. To
tackle these, we introduce two plug-and-play modules: Sparse Dirac Information
Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR
mitigates the dominance of strong features by applying Bernoulli-based
sparsification and Dirac-inspired stabilization to enhance weaker modality
signals. CADE, designed to synthesize the target domain distribution, fuses
local morphological cues and global gene expression in latent space.
Experiments on a four-cancer-type benchmark demonstrate superior
generalization, laying the foundation for practical, robust cross-cancer
multimodal prognosis. Code is available at
https://github.com/HopkinsKwong/MCCSDG

</details>


### [79] [Towards Imperceptible JPEG Image Hiding: Multi-range Representations-driven Adversarial Stego Generation](https://arxiv.org/abs/2507.08343)
*Junxue Yang,Xin Liao,Weixuan Tang,Jianhua Yang,Zheng Qin*

Main category: cs.CV

TL;DR: 针对深度隐藏易被隐写分析检测的问题，本文提出MRAG框架。该框架融合多范围特征（卷积与Transformer）、多粒度频率信息，并引入特征角度-范数解耦损失，生成难以检测且高隐秘、可恢复的对抗性隐写图像，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度隐藏方案因载荷大、特征提取受限于单一范围（纯卷积或纯Transformer）、以及像素级损失约束，容易被隐写分析器检测。

Method: 本文提出MRAG（多范围表示驱动的对抗性隐写生成）框架。具体而言，它整合了卷积的局部感受野和Transformer的全局依赖建模以构建多范围表示；利用粗粒度和细粒度频率分解的图像作为输入，引入多粒度信息；并设计了特征角度-范数解耦损失，以使生成的隐写图像在隐写分析器分类特征的角度和范数空间中更接近原始图像。

Result: MRAG方法能有效地将微小但有效的对抗性扰动注入生成过程，确保隐写图像保持良好的秘密恢复能力和不可感知性，并通过大量实验证明其实现了最先进的性能。

Conclusion: MRAG框架通过整合多范围表示、多粒度信息和新颖的损失函数，有效提升了深度隐藏对隐写分析的鲁棒性，解决了现有方案易被检测的问题，并达到了卓越的性能。

Abstract: Deep hiding has been exploring the hiding capability of deep learning-based
models, aiming to conceal image-level messages into cover images and reveal
them from generated stego images. Existing schemes are easily detected by
steganalyzers due to their large payloads and their limitation to feature
extraction based solely on either pure convolution or pure transformer
operators within a single range, as well as pixel-level loss constraints. To
address the issue, in this paper, we introduce generation-based adversarial
attacks into color JPEG image deep hiding and propose a multi-range
representations-driven adversarial stego generation framework called MRAG from
a steganalysis perspective. Specifically, we integrate the local-range neighbor
reception characteristic of the convolution and the global-range dependency
modeling of the transformer to construct MRAG. Meanwhile, we use the
transformed images obtained through coarse-grained and fine-grained frequency
decomposition as inputs, introducing multi-grained information. Furthermore, a
features angle-norm disentanglement loss is designed to constrain the generated
stegos closer to covers in the angle and norm space of the steganalyzer's
classified features. Consequently, small yet effective adversarial
perturbations can be injected into the process of generating stegos, ensuring
that stegos maintain favorable secret restorability and imperceptibility.
Extensive experiments demonstrate that MRAG can achieve state-of-the-art
performance.

</details>


### [80] [MM-Gesture: Towards Precise Micro-Gesture Recognition through Multimodal Fusion](https://arxiv.org/abs/2507.08344)
*Jihao Gu,Fei Wang,Kun Li,Yanyan Wei,Zhiliang Wu,Dan Guo*

Main category: cs.CV

TL;DR: 该论文介绍了MM-Gesture，一个用于微手势分类的多模态融合框架，该方案在第三届MiGA挑战赛中获得第一名，在iMiGUE基准测试中取得了73.213%的Top-1准确率。


<details>
  <summary>Details</summary>
Motivation: 有效识别微妙且短时程的微手势，并推动该领域的技术发展，尤其是在MiGA挑战赛背景下。

Method: MM-Gesture是一个多模态融合框架，整合了关节、肢体、RGB视频、泰勒级数视频、光流视频和深度视频等多模态信息。它采用PoseConv3D和Video Swin Transformer架构，并结合了一种新颖的模态加权集成策略。此外，通过在MA-52数据集上预训练进行迁移学习，进一步提升了RGB模态的性能。

Result: 该方法在iMiGUE基准测试上实现了73.213%的Top-1准确率，优于现有最先进的方法，并在第三届MiGA挑战赛中排名第一。

Conclusion: MM-Gesture是一种高效的多模态微手势分类方法，通过融合多种互补模态和创新的集成策略，取得了卓越的性能，验证了其在微手势识别领域的有效性和优越性。

Abstract: In this paper, we present MM-Gesture, the solution developed by our team
HFUT-VUT, which ranked 1st in the micro-gesture classification track of the 3rd
MiGA Challenge at IJCAI 2025, achieving superior performance compared to
previous state-of-the-art methods. MM-Gesture is a multimodal fusion framework
designed specifically for recognizing subtle and short-duration micro-gestures
(MGs), integrating complementary cues from joint, limb, RGB video,
Taylor-series video, optical-flow video, and depth video modalities. Utilizing
PoseConv3D and Video Swin Transformer architectures with a novel
modality-weighted ensemble strategy, our method further enhances RGB modality
performance through transfer learning pre-trained on the larger MA-52 dataset.
Extensive experiments on the iMiGUE benchmark, including ablation studies
across different modalities, validate the effectiveness of our proposed
approach, achieving a top-1 accuracy of 73.213%.

</details>


### [81] [Cycle Context Verification for In-Context Medical Image Segmentation](https://arxiv.org/abs/2507.08357)
*Shishuai Hu,Zehui Liao,Liangli Zhen,Huazhu Fu,Yong Xia*

Main category: cs.CV

TL;DR: 本文提出Cycle Context Verification (CCV) 框架，旨在解决上下文学习(ICL)在医学图像分割中对上下文对齐敏感的问题。CCV通过循环验证机制和查询特定提示，实现预测的自我验证并增强上下文对齐，从而显著提升了ICL基础模型在多个医学图像分割数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 上下文学习(ICL)在通用医学图像分割中展现潜力，但其性能高度依赖于查询图像与上下文图像-掩模对之间的对齐。在临床实践中，标注医学图像的稀缺性使得选择最优上下文对极具挑战性，且由于计算成本高昂和灾难性遗忘风险，对基础ICL模型进行微调是不可行的。

Method: 本文提出了Cycle Context Verification (CCV) 框架。CCV采用循环流程：模型首先为查询图像生成分割掩模；随后，查询图像与上下文对的角色互换，模型预测原始上下文图像的掩模以验证其初始预测，此二次预测的准确性作为初始查询分割的隐式度量。此外，引入并更新查询特定提示以调整查询图像并优化此度量，从而增强查询与上下文对之间的对齐。

Result: CCV在七个医学图像分割数据集上，使用两个ICL基础模型进行了评估，结果表明其性能优于现有方法。

Conclusion: CCV显著提升了基于ICL的医学图像分割能力，使其成为通用医学图像分割的稳健解决方案。

Abstract: In-context learning (ICL) is emerging as a promising technique for achieving
universal medical image segmentation, where a variety of objects of interest
across imaging modalities can be segmented using a single model. Nevertheless,
its performance is highly sensitive to the alignment between the query image
and in-context image-mask pairs. In a clinical scenario, the scarcity of
annotated medical images makes it challenging to select optimal in-context
pairs, and fine-tuning foundation ICL models on contextual data is infeasible
due to computational costs and the risk of catastrophic forgetting. To address
this challenge, we propose Cycle Context Verification (CCV), a novel framework
that enhances ICL-based medical image segmentation by enabling
self-verification of predictions and accordingly enhancing contextual
alignment. Specifically, CCV employs a cyclic pipeline in which the model
initially generates a segmentation mask for the query image. Subsequently, the
roles of the query and an in-context pair are swapped, allowing the model to
validate its prediction by predicting the mask of the original in-context
image. The accuracy of this secondary prediction serves as an implicit measure
of the initial query segmentation. A query-specific prompt is introduced to
alter the query image and updated to improve the measure, thereby enhancing the
alignment between the query and in-context pairs. We evaluated CCV on seven
medical image segmentation datasets using two ICL foundation models,
demonstrating its superiority over existing methods. Our results highlight
CCV's ability to enhance ICL-based segmentation, making it a robust solution
for universal medical image segmentation. The code will be available at
https://github.com/ShishuaiHu/CCV.

</details>


### [82] [Understanding Driving Risks using Large Language Models: Toward Elderly Driver Assessment](https://arxiv.org/abs/2507.08367)
*Yuki Yoshihara,Linjing Jiang,Nihan Karatas,Hitoshi Kanamori,Asuka Harada,Takahiro Tanaka*

Main category: cs.CV

TL;DR: 研究评估了ChatGPT-4o在交通场景（交通密度、路口可见性、停车标志识别）中进行类人解释的潜力，发现提示设计对性能影响显著，并证实LLM在驾驶风险评估中具有辅助工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索多模态大型语言模型（LLM，具体为ChatGPT-4o）利用静态行车记录仪图像进行类人交通场景解释的潜力，尤其关注与老年驾驶员评估相关的、需要语境推理而非简单目标检测的任务。

Method: 以ChatGPT-4o为研究对象，针对交通密度评估、路口可见性评估和停车标志识别三项任务。采用零样本、少样本和多样本提示策略评估模型性能，以人类标注作为参考标准。评估指标包括精度、召回率和F1分数，并进行了输出稳定性分析。

Result: 提示设计显著影响模型性能：路口可见性召回率从21.7%（零样本）增至57.0%（多样本）；交通密度一致性从53.5%增至67.6%。停车标志识别精度高（高达86.3%）但召回率较低（约76.7%）。模型和人类在解释结构模糊场景时均面临困难，但模型的解释性文本与其预测一致，增强了可解释性。

Conclusion: 研究表明，通过精心设计的提示，LLMs在场景级驾驶风险评估中具有作为辅助工具的潜力。未来研究应探索使用更大数据集、更多样化的标注者和下一代模型架构以实现可扩展性，尤其是在老年驾驶员评估领域。

Abstract: This study investigates the potential of a multimodal large language model
(LLM), specifically ChatGPT-4o, to perform human-like interpretations of
traffic scenes using static dashcam images. Herein, we focus on three judgment
tasks relevant to elderly driver assessments: evaluating traffic density,
assessing intersection visibility, and recognizing stop signs recognition.
These tasks require contextual reasoning rather than simple object detection.
Using zero-shot, few-shot, and multi-shot prompting strategies, we evaluated
the performance of the model with human annotations serving as the reference
standard. Evaluation metrics included precision, recall, and F1-score. Results
indicate that prompt design considerably affects performance, with recall for
intersection visibility increasing from 21.7% (zero-shot) to 57.0%
(multi-shot). For traffic density, agreement increased from 53.5% to 67.6%. In
stop-sign detection, the model demonstrated high precision (up to 86.3%) but a
lower recall (approximately 76.7%), indicating a conservative response
tendency. Output stability analysis revealed that humans and the model faced
difficulties interpreting structurally ambiguous scenes. However, the model's
explanatory texts corresponded with its predictions, enhancing
interpretability. These findings suggest that, with well-designed prompts, LLMs
hold promise as supportive tools for scene-level driving risk assessments.
Future studies should explore scalability using larger datasets, diverse
annotators, and next-generation model architectures for elderly driver
assessments.

</details>


### [83] [Unsupervised Methods for Video Quality Improvement: A Survey of Restoration and Enhancement Techniques](https://arxiv.org/abs/2507.08375)
*Alexandra Malyugina,Yini Li,Joanne Lin,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 本文综述了视频修复与增强技术，重点关注无监督方法，并总结了其技术分类、损失函数、评估方法、面临的挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 视频修复与增强对于提升视觉质量至关重要，同时也是提升各种下游计算机视觉任务性能的关键预处理步骤。

Method: 本综述全面回顾了视频修复与增强技术，特别是无监督方法。内容包括：概述常见的视频退化及其原因；回顾早期的传统和深度学习方法及其优缺点；深入探讨无监督方法，并根据其基本方法（如域转换、自监督信号设计、盲点或基于噪声的方法）进行分类；归纳无监督视频修复和增强中使用的损失函数；讨论配对合成数据集在客观评估中的作用。

Result: 本研究提供了一个全面的视频修复与增强技术综述，尤其对无监督方法进行了深入分类和分析，涵盖了其核心技术、损失函数以及客观评估中配对合成数据集的应用。

Conclusion: 识别了视频修复与增强领域的关键挑战，并为未来研究指明了有前景的方向。

Abstract: Video restoration and enhancement are critical not only for improving visual
quality, but also as essential pre-processing steps to boost the performance of
a wide range of downstream computer vision tasks. This survey presents a
comprehensive review of video restoration and enhancement techniques with a
particular focus on unsupervised approaches. We begin by outlining the most
common video degradations and their underlying causes, followed by a review of
early conventional and deep learning methods-based, highlighting their
strengths and limitations. We then present an in-depth overview of unsupervised
methods, categorise by their fundamental approaches, including domain
translation, self-supervision signal design and blind spot or noise-based
methods. We also provide a categorization of loss functions employed in
unsupervised video restoration and enhancement, and discuss the role of paired
synthetic datasets in enabling objective evaluation. Finally, we identify key
challenges and outline promising directions for future research in this field.

</details>


### [84] [From Enhancement to Understanding: Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning](https://arxiv.org/abs/2507.08380)
*Sen Wang,Shao Zeng,Tianjun Gu,Zhizhong Zhang,Ruixin Zhang,Shouhong Ding,Jingyun Zhang,Jun Wang,Xin Tan,Yuan Xie,Lizhuang Ma*

Main category: cs.CV

TL;DR: 本文提出了GEFU范式，旨在弥合低光增强与高层视觉理解之间的鸿沟，通过利用预训练扩散模型和SCUF无监督微调，显著提升了图像质量和分类、检测、语义分割等下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的低光增强与视觉理解任务相互独立。现有增强方法泛化能力有限且仅关注视觉质量而非下游性能。现有理解方法受标注数据稀缺限制，可扩展性差。研究旨在构建一个通用桥梁（GEFU）以提升两者的泛化性和可扩展性。

Method: 1. 提出“用于理解的通用增强”（GEFU）范式，连接低光增强与理解。2. 利用预训练生成扩散模型优化图像，实现零样本泛化。3. 提出“语义一致的无监督微调”（SCUF）。4. 在SCUF中，引入光照感知图像提示和循环注意力适配器以引导图像生成。5. 提出标题一致性和反射一致性机制，以缓解无监督训练中的语义退化，学习高层语义和图像级空间语义。

Result: 实验结果表明，所提出的方法在传统图像质量评估以及包括分类、检测和语义分割在内的GEFU任务中，均优于当前最先进的方法。

Conclusion: GEFU范式成功地将低光增强与高层视觉理解结合起来，通过创新的扩散模型应用和无监督微调策略，实现了卓越的泛化性和可扩展性，并在多项低光视觉任务中取得了领先性能。

Abstract: Low-level enhancement and high-level visual understanding in low-light vision
have traditionally been treated separately. Low-light enhancement improves
image quality for downstream tasks, but existing methods rely on physical or
geometric priors, limiting generalization. Evaluation mainly focuses on visual
quality rather than downstream performance. Low-light visual understanding,
constrained by scarce labeled data, primarily uses task-specific domain
adaptation, which lacks scalability. To address these challenges, we build a
generalized bridge between low-light enhancement and low-light understanding,
which we term Generalized Enhancement For Understanding (GEFU). This paradigm
improves both generalization and scalability. To address the diverse causes of
low-light degradation, we leverage pretrained generative diffusion models to
optimize images, achieving zero-shot generalization performance. Building on
this, we propose Semantically Consistent Unsupervised Fine-tuning (SCUF).
Specifically, to overcome text prompt limitations, we introduce an
illumination-aware image prompt to explicitly guide image generation and
propose a cycle-attention adapter to maximize its semantic potential. To
mitigate semantic degradation in unsupervised training, we propose caption and
reflectance consistency to learn high-level semantics and image-level spatial
semantics. Extensive experiments demonstrate that our proposed method
outperforms current state-of-the-art methods in traditional image quality and
GEFU tasks including classification, detection, and semantic segmentation.

</details>


### [85] [Smelly, dense, and spreaded: The Object Detection for Olfactory References (ODOR) dataset](https://arxiv.org/abs/2507.08384)
*Mathias Zinnen,Prathmesh Madhu,Inger Leemans,Peter Bell,Azhar Hussian,Hang Tran,Ali Hürriyetoğlu,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 本文提出了ODOR数据集，一个用于艺术品细粒度物体检测的新数据集，旨在解决现有数据集在人文领域应用的局限性，并提供了基线分析。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉数据集在艺术品应用中存在细粒度类别不足、标注偏向图像中心、以及对艺术抽象和边缘物体鲁棒性差等问题，无法满足真实世界应用的需求。

Method: 1. 构建了ODOR数据集，包含4712张图像的38,116个对象级标注，涵盖139个细粒度类别。 2. 对数据集进行了统计分析，展示其挑战性属性。 3. 为物体检测模型提供了广泛的基线分析，并通过二次研究突出了数据集的挑战性。

Result: ODOR数据集展现出细致类别、密集重叠对象和全图像画布空间分布等挑战性特性。对物体检测模型的基线分析也证实了这些挑战性属性。

Conclusion: ODOR数据集将激励艺术品物体检测和更广泛的视觉文化遗产研究，并促使研究人员探索物体识别与嗅觉感知等领域的交叉点。

Abstract: Real-world applications of computer vision in the humanities require
algorithms to be robust against artistic abstraction, peripheral objects, and
subtle differences between fine-grained target classes. Existing datasets
provide instance-level annotations on artworks but are generally biased towards
the image centre and limited with regard to detailed object classes. The
proposed ODOR dataset fills this gap, offering 38,116 object-level annotations
across 4712 images, spanning an extensive set of 139 fine-grained categories.
Conducting a statistical analysis, we showcase challenging dataset properties,
such as a detailed set of categories, dense and overlapping objects, and
spatial distribution over the whole image canvas. Furthermore, we provide an
extensive baseline analysis for object detection models and highlight the
challenging properties of the dataset through a set of secondary studies.
Inspiring further research on artwork object detection and broader visual
cultural heritage studies, the dataset challenges researchers to explore the
intersection of object recognition and smell perception.

</details>


### [86] [Subject-Consistent and Pose-Diverse Text-to-Image Generation](https://arxiv.org/abs/2507.08396)
*Zhanxin Gao,Beier Zhu,Liang Yao,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: 本文提出了CoDi，一个两阶段的文本到图像（T2I）框架，旨在解决主体一致性生成中主体身份一致性和姿态多样性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有免训练的主体一致性生成（SCG）方法通常以牺牲布局和姿态多样性为代价来达到一致性，从而限制了富有表现力的视觉叙事能力。

Method: CoDi框架利用扩散模型的渐进特性，采用两阶段策略：1. 身份传输（IT）：在早期去噪步骤中，使用最优传输以姿态感知方式转移身份特征，以促进主体一致性并保留姿态多样性。2. 身份精炼（IR）：在后期去噪步骤中，选择最显著的身份特征来进一步精炼主体细节。

Result: CoDi在主体一致性、姿态多样性和提示词保真度方面均实现了更好的视觉感知和更强的性能。

Conclusion: CoDi成功地解决了T2I模型中保持主体一致性同时实现多样姿态和布局的挑战，提升了生成图像的质量和多样性。

Abstract: Subject-consistent generation (SCG)-aiming to maintain a consistent subject
identity across diverse scenes-remains a challenge for text-to-image (T2I)
models. Existing training-free SCG methods often achieve consistency at the
cost of layout and pose diversity, hindering expressive visual storytelling. To
address the limitation, we propose subject-Consistent and pose-Diverse T2I
framework, dubbed as CoDi, that enables consistent subject generation with
diverse pose and layout. Motivated by the progressive nature of diffusion,
where coarse structures emerge early and fine details are refined later, CoDi
adopts a two-stage strategy: Identity Transport (IT) and Identity Refinement
(IR). IT operates in the early denoising steps, using optimal transport to
transfer identity features to each target image in a pose-aware manner. This
promotes subject consistency while preserving pose diversity. IR is applied in
the later denoising steps, selecting the most salient identity features to
further refine subject details. Extensive qualitative and quantitative results
on subject consistency, pose diversity, and prompt fidelity demonstrate that
CoDi achieves both better visual perception and stronger performance across all
metrics. The code is provided in https://github.com/NJU-PCALab/CoDi.

</details>


### [87] [PanMatch: Unleashing the Potential of Large Vision Models for Unified Matching Models](https://arxiv.org/abs/2507.08400)
*Yongjian Zhang,Longguang Wang,Kunhong Li,Ye Zhang,Yun Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

TL;DR: PanMatch是一个通用基础模型，通过统一的2D位移估计框架和单一模型权重，处理各种双帧对应匹配任务，并在跨任务和异常场景中展现出色的泛化能力和零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有的对应匹配方法依赖于任务特定架构和领域特定微调，导致泛化能力差且效率低下。亟需一个能够使用单一模型权重处理多种对应匹配任务的通用框架。

Method: 本研究提出了PanMatch模型，核心思想是将所有双帧对应匹配任务统一为2D位移估计问题。为此，模型强调了鲁棒特征提取器的重要性，并引入了一个特征转换管线，利用大型视觉模型（LVM）的通用特征，赋予匹配基线零样本跨视图匹配能力。此外，研究构建了一个包含近180万样本的跨领域数据集（涵盖立体匹配、光流和特征匹配）用于PanMatch的预训练。

Result: PanMatch在广泛的领域和下游任务中，使用相同的模型权重，展现了其多功能性。在跨任务评估中，PanMatch性能优于UniMatch和Flow-Anything。在面向任务的基准测试中，其性能与大多数最先进的任务特定算法相当。值得注意的是，PanMatch在雨天和卫星图像等异常场景中展现了前所未有的零样本性能，而多数现有鲁棒算法在此类场景下效果不佳。

Conclusion: PanMatch成功地提供了一个高度通用和鲁棒的解决方案，能够通过单一模型处理多种对应匹配任务，显著提高了泛化能力，并在挑战性场景下表现出卓越的零样本性能，从而克服了现有方法的局限性。

Abstract: This work presents PanMatch, a versatile foundation model for robust
correspondence matching. Unlike previous methods that rely on task-specific
architectures and domain-specific fine-tuning to support tasks like stereo
matching, optical flow or feature matching, our key insight is that any
two-frame correspondence matching task can be addressed within a 2D
displacement estimation framework using the same model weights. Such a
formulation eliminates the need for designing specialized unified architectures
or task-specific ensemble models. Instead, it achieves multi-task integration
by endowing displacement estimation algorithms with unprecedented
generalization capabilities. To this end, we highlight the importance of a
robust feature extractor applicable across multiple domains and tasks, and
propose the feature transformation pipeline that leverage all-purpose features
from Large Vision Models to endow matching baselines with zero-shot cross-view
matching capabilities. Furthermore, we assemble a cross-domain dataset with
near 1.8 million samples from stereo matching, optical flow, and feature
matching domains to pretrain PanMatch. We demonstrate the versatility of
PanMatch across a wide range of domains and downstream tasks using the same
model weights. Our model outperforms UniMatch and Flow-Anything on cross-task
evaluations, and achieves comparable performance to most state-of-the-art
task-specific algorithms on task-oriented benchmarks. Additionally, PanMatch
presents unprecedented zero-shot performance in abnormal scenarios, such as
rainy day and satellite imagery, where most existing robust algorithms fail to
yield meaningful results.

</details>


### [88] [Deep Hashing with Semantic Hash Centers for Image Retrieval](https://arxiv.org/abs/2507.08404)
*Li Chen,Rui Liu,Yuxiang Zhou,Xudong Ma,Yong Chen,Dell Zhang*

Main category: cs.CV

TL;DR: 针对现有深度哈希方法忽略类别语义关系导致检索性能下降的问题，本文提出SHC三阶段框架，通过引入语义哈希中心生成保留语义结构的二值哈希码，显著提升了大规模图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有点对式深度哈希方法在生成哈希中心时，采用数据无关算法，忽略了类别间的语义关系，这可能导致检索性能下降。

Method: 本文提出SHC（Semantic Hash Centers）三阶段框架。第一阶段，构建分类网络以数据依赖方式计算并识别类别间语义相似性。第二阶段，开发优化算法生成语义哈希中心，在保持语义相关性的同时强制中心间最小距离。第三阶段，训练深度哈希网络将图像转换为基于这些语义中心的二值哈希码。

Result: 在多个大型公共数据集上的实验结果表明，SHC显著提升了检索性能，相较于现有最佳方法，MAP@100、MAP@1000和MAP@ALL指标分别平均提升了+7.26%、+7.62%和+11.71%。

Conclusion: 本文提出的SHC框架通过引入语义哈希中心，有效解决了传统方法忽略类别语义关系的问题，从而显著提高了大规模图像检索的性能。

Abstract: Deep hashing is an effective approach for large-scale image retrieval.
Current methods are typically classified by their supervision types:
point-wise, pair-wise, and list-wise. Recent point-wise techniques (e.g., CSQ,
MDS) have improved retrieval performance by pre-assigning a hash center to each
class, enhancing the discriminability of hash codes across various datasets.
However, these methods rely on data-independent algorithms to generate hash
centers, which neglect the semantic relationships between classes and may
degrade retrieval performance.
  This paper introduces the concept of semantic hash centers, building on the
idea of traditional hash centers. We hypothesize that hash centers of
semantically related classes should have closer Hamming distances, while those
of unrelated classes should be more distant. To this end, we propose a
three-stage framework, SHC, to generate hash codes that preserve semantic
structure.
  First, we develop a classification network to identify semantic similarities
between classes using a data-dependent similarity calculation that adapts to
varying data distributions. Second, we introduce an optimization algorithm to
generate semantic hash centers, preserving semantic relatedness while enforcing
a minimum distance between centers to avoid excessively similar hash codes.
Finally, a deep hashing network is trained using these semantic centers to
convert images into binary hash codes.
  Experimental results on large-scale retrieval tasks across several public
datasets show that SHC significantly improves retrieval performance.
Specifically, SHC achieves average improvements of +7.26%, +7.62%, and +11.71%
in MAP@100, MAP@1000, and MAP@ALL metrics, respectively, over state-of-the-art
methods.

</details>


### [89] [Multi-modal Mutual-Guidance Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.08410)
*Shijun Yang,Xiang Zhang,Wanqing Zhao,Hangzai Luo,Sheng Zhong,Jinye Peng,Jianping Fan*

Main category: cs.CV

TL;DR: MuGCP是一种新的条件提示学习范式，利用多模态大语言模型生成语义条件提示，并通过注意力互引导模块生成视觉条件提示，并融合多种提示以提升视觉-语言模型的泛化能力和跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: 当前的提示学习在面对未见实例时，类别嵌入分布建模不足导致泛化性差；且跨模态对齐主要局限于最后一层输出，限制了拓扑一致性的保持。

Method: MuGCP通过多模态大语言模型生成语义条件提示（SCP），并引入注意力互引导（AMG）模块实现视觉和语义信息的交互以生成视觉条件提示（VCP）。此外，提出多提示融合（MPF）机制，将SCP、VCP与上下文提示融合，以增强类别嵌入和实例知识建模。

Result: MuGCP在14个不同数据集上超越了现有的最先进方法。

Conclusion: MuGCP通过多模态引导和多提示融合机制，有效解决了提示学习在泛化性和跨模态对齐方面的挑战，显著提升了视觉-语言模型的性能。

Abstract: Prompt learning facilitates the efficient adaptation of Vision-Language
Models (VLMs) to various downstream tasks. However, it faces two significant
challenges: (1) inadequate modeling of class embedding distributions for unseen
instances, leading to suboptimal generalization on novel classes; (2)
prevailing methodologies predominantly confine cross-modal alignment to the
final output layer of vision and text encoders, which fundamentally limits
their capacity to preserve topological consistency with pre-trained multi-modal
embedding spaces. To this end, we introduce MuGCP (Multi-modal Mutual-Guidance
Conditional Prompt Learning), a novel paradigm designed for conditional prompt
generation. MuGCP leverages Multi-modal Large Language Models (MLLMs) as
conditional prompt learners to adaptively generate Semantic Conditional Prompts
(SCP) that incorporate rich, fine-grained high-level semantic knowledge for
image instances. To ensure effective alignment and interaction across the
multi-modal space of Vision-Language Models (VLMs), we introduce the Attention
Mutual-Guidance (AMG) module, which facilitates interactions between visual and
semantic information. Through mutual guidance, the AMG module generates Visual
Conditional Prompts (VCP), enhancing the model's performance in multi-modal
tasks. Additionally, we present a Multi-Prompt Fusion (MPF) mechanism that
integrates SCP and VCP with contextual prompts, ensuring seamless coordination
among the different prompts and enhancing the modeling of class embeddings and
instance-specific knowledge. Our MuGCP outperforms existing state-of-the-art
methods on 14 different datasets. The code will be made available after
publication.

</details>


### [90] [InstaScene: Towards Complete 3D Instance Decomposition and Reconstruction from Cluttered Scenes](https://arxiv.org/abs/2507.08416)
*Zesong Yang,Bangbang Yang,Wenqi Dong,Chenxuan Cao,Liyuan Cui,Yuewen Ma,Zhaopeng Cui,Hujun Bao*

Main category: cs.CV

TL;DR: InstaScene提出一种新的3D感知范式，通过空间对比学习实现精确的场景实例分解，并利用原位生成技术克服部分观测带来的不完整性，从而在复杂场景中实现对被遮挡物体的完整重建和分解。


<details>
  <summary>Details</summary>
Motivation: 机器人难以像人类一样在杂乱环境中识别并心理补全被遮挡物体。现有的先进重建技术将场景建模为未分化的整体，无法从部分观测中识别出完整的物体，导致机器人缺乏整体3D感知能力。

Method: 本文提出了InstaScene框架，旨在实现复杂场景的整体3D感知，其主要目标是分解任意实例并确保完整重建。为实现精确分解，开发了新颖的空间对比学习方法，通过跨视图追踪每个实例的栅格化来增强语义监督。为克服有限观测导致的不完整性，引入了原位生成技术，利用观测数据和几何线索引导3D生成模型重建与现实世界无缝对齐的完整实例。

Result: 在复杂真实世界和合成场景中的场景分解和物体完成实验表明，InstaScene方法实现了卓越的分解精度，同时生成了几何保真且视觉上完整的物体。

Conclusion: InstaScene通过创新的分解和补全策略，显著提升了机器人对复杂场景的整体3D感知能力，能够精确分解并完整重建被遮挡物体，弥补了现有技术的不足。

Abstract: Humans can naturally identify and mentally complete occluded objects in
cluttered environments. However, imparting similar cognitive ability to
robotics remains challenging even with advanced reconstruction techniques,
which models scenes as undifferentiated wholes and fails to recognize complete
object from partial observations. In this paper, we propose InstaScene, a new
paradigm towards holistic 3D perception of complex scenes with a primary goal:
decomposing arbitrary instances while ensuring complete reconstruction. To
achieve precise decomposition, we develop a novel spatial contrastive learning
by tracing rasterization of each instance across views, significantly enhancing
semantic supervision in cluttered scenes. To overcome incompleteness from
limited observations, we introduce in-situ generation that harnesses valuable
observations and geometric cues, effectively guiding 3D generative models to
reconstruct complete instances that seamlessly align with the real world.
Experiments on scene decomposition and object completion across complex
real-world and synthetic scenes demonstrate that our method achieves superior
decomposition accuracy while producing geometrically faithful and visually
intact objects.

</details>


### [91] [Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers](https://arxiv.org/abs/2507.08422)
*Wongi Jeong,Kyungryeol Lee,Hoigi Seo,Se Young Chun*

Main category: cs.CV

TL;DR: 提出一种名为RALU的无训练空间加速框架，通过混合分辨率采样显著提升扩散Transformer推理速度，同时保持图像质量，并可与现有时间加速方法结合。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer在图像生成方面具有优越的可扩展性，但其高昂的计算成本是实际部署的主要障碍。现有加速方法主要利用时间维度，缺乏对空间维度的优化。

Method: 提出无训练框架Region-Adaptive Latent Upsampling (RALU)，旨在沿空间维度加速推理。RALU采用三阶段混合分辨率采样：1) 低分辨率去噪潜在扩散以捕获全局语义结构；2) 对特定易产生伪影的区域进行区域自适应全分辨率上采样；3) 对所有潜在变量进行全分辨率上采样以细化细节。为稳定跨分辨率转换的生成，引入噪声时间步重新调度以适应不同分辨率下的噪声水平。

Result: RALU在显著减少计算量的同时保持了图像质量，在FLUX上实现了高达7.0倍的加速，在Stable Diffusion 3上实现了3.0倍的加速，且图像质量退化极小。此外，RALU与现有的时间加速方法（如缓存）互补，可以无缝集成以进一步降低推理延迟而不损害生成质量。

Conclusion: RALU提供了一种高效的、无训练的扩散Transformer空间加速解决方案，显著提升了推理速度并保持图像质量。其与现有时间加速方法的兼容性使其在实际应用中具有广阔前景。

Abstract: Diffusion transformers have emerged as an alternative to U-net-based
diffusion models for high-fidelity image and video generation, offering
superior scalability. However, their heavy computation remains a major obstacle
to real-world deployment. Existing acceleration methods primarily exploit the
temporal dimension such as reusing cached features across diffusion timesteps.
Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free
framework that accelerates inference along spatial dimension. RALU performs
mixed-resolution sampling across three stages: 1) low-resolution denoising
latent diffusion to efficiently capture global semantic structure, 2)
region-adaptive upsampling on specific regions prone to artifacts at
full-resolution, and 3) all latent upsampling at full-resolution for detail
refinement. To stabilize generations across resolution transitions, we leverage
noise-timestep rescheduling to adapt the noise level across varying
resolutions. Our method significantly reduces computation while preserving
image quality by achieving up to 7.0$\times$ speed-up on FLUX and 3.0$\times$
on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is
complementary to existing temporal accelerations such as caching methods, thus
can be seamlessly integrated to further reduce inference latency without
compromising generation quality.

</details>


### [92] [RePaintGS: Reference-Guided Gaussian Splatting for Realistic and View-Consistent 3D Scene Inpainting](https://arxiv.org/abs/2507.08434)
*Ji Hyun Seo,Byounhyun Yoo,Gerard Jounghyun Kim*

Main category: cs.CV

TL;DR: 本文提出一种新型3D场景修复方法，通过利用参考视图解决传统图像修复在多视角下的不一致性问题，从而提升修复场景的几何精度和外观一致性。


<details>
  <summary>Details</summary>
Motivation: 当前辐射场方法的3D场景编辑（如物体移除）会导致暴露区域不自然，而现有图像修复技术在填充这些区域时会造成多视角间的不一致。虽然有方法尝试通过感知线索融合，但存在细节丢失和应对感知不一致性时的局限性。因此，需要一种能可靠地生成逼真且感知一致的3D场景修复结果的方法。

Method: 该方法利用一个已修复的参考视图。首先，估算其他视图与参考视图的修复相似性，以调整它们在构建“适合参考视图的精确几何结构”时的贡献。然后，该几何结构被用于将参考视图的修复内容“扭曲”到其他视图，作为伪真值，从而引导优化过程以匹配参考视图的外观。

Result: 对比评估研究表明，所提出的方法显著提高了修复后3D场景的几何保真度和外观一致性。

Conclusion: 本文成功开发了一种新颖的3D场景修复方法，通过引入参考视图机制，有效解决了现有技术在视角一致性和细节方面的不足，实现了复杂场景下逼真且感知一致的修复效果，显著提升了修复场景的整体质量。

Abstract: Radiance field methods, such as Neural Radiance Field or 3D Gaussian
Splatting, have emerged as seminal 3D representations for synthesizing
realistic novel views. For practical applications, there is ongoing research on
flexible scene editing techniques, among which object removal is a
representative task. However, removing objects exposes occluded regions, often
leading to unnatural appearances. Thus, studies have employed image inpainting
techniques to replace such regions with plausible content - a task referred to
as 3D scene inpainting. However, image inpainting methods produce one of many
plausible completions for each view, leading to inconsistencies between
viewpoints. A widely adopted approach leverages perceptual cues to blend
inpainted views smoothly. However, it is prone to detail loss and can fail when
there are perceptual inconsistencies across views. In this paper, we propose a
novel 3D scene inpainting method that reliably produces realistic and
perceptually consistent results even for complex scenes by leveraging a
reference view. Given the inpainted reference view, we estimate the inpainting
similarity of the other views to adjust their contribution in constructing an
accurate geometry tailored to the reference. This geometry is then used to warp
the reference inpainting to other views as pseudo-ground truth, guiding the
optimization to match the reference appearance. Comparative evaluation studies
have shown that our approach improves both the geometric fidelity and
appearance consistency of inpainted scenes.

</details>


### [93] [Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2507.08441)
*Anlin Zheng,Xin Wen,Xuanyang Zhang,Chuofan Ma,Tiancai Wang,Gang Yu,Xiangyu Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: VFMTok：一种基于冻结视觉基础模型的新型图像分词器，通过区域自适应量化和语义重建显著提升图像重建与生成质量及效率。


<details>
  <summary>Details</summary>
Motivation: 利用预训练视觉基础模型的强大表示能力，探索将其直接用于构建图像分词器这一未充分研究的方向。

Method: 以冻结的视觉基础模型作编码器，并引入两个关键组件：1) 区域自适应量化框架以减少特征冗余；2) 语义重建目标以保持语义保真度，使分词器输出与基础模型表示对齐。

Result: 提出的VFMTok显著提升了图像重建和生成质量，提高了token效率。在ImageNet基准上，将自回归（AR）生成性能提升至gFID 2.07，模型收敛速度加快三倍，并能在无需CFG的情况下实现高保真度类别条件合成。

Conclusion: 通过有效利用冻结的视觉基础模型并引入特定优化策略，VFMTok成功构建了高性能图像分词器，在图像生成和效率方面取得了突破性进展。

Abstract: Leveraging the powerful representations of pre-trained vision foundation
models -- traditionally used for visual comprehension -- we explore a novel
direction: building an image tokenizer directly atop such models, a largely
underexplored area. Specifically, we employ a frozen vision foundation model as
the encoder of our tokenizer. To enhance its effectiveness, we introduce two
key components: (1) a region-adaptive quantization framework that reduces
redundancy in the pre-trained features on regular 2D grids, and (2) a semantic
reconstruction objective that aligns the tokenizer's outputs with the
foundation model's representations to preserve semantic fidelity. Based on
these designs, our proposed image tokenizer, VFMTok, achieves substantial
improvements in image reconstruction and generation quality, while also
enhancing token efficiency. It further boosts autoregressive (AR) generation --
achieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model
convergence by three times, and enabling high-fidelity class-conditional
synthesis without the need for classifier-free guidance (CFG). The code will be
released publicly to benefit the community.

</details>


### [94] [Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT](https://arxiv.org/abs/2507.08448)
*Wei Zhang,Yihang Wu,Songhua Li,Wenjie Ma,Xin Ma,Qiang Li,Qi Wang*

Main category: cs.CV

TL;DR: 本文综述了三维重建领域一种新兴的深度学习前向传播范式，该范式能从图像中单次推断相机姿态和密集几何，并对比了传统方法，探讨了未来挑战。


<details>
  <summary>Details</summary>
Motivation: 传统三维重建方法存在工作流程复杂、计算成本高、鲁棒性差等局限。深度学习催生了一种新的前向传播范式，能够直接从图像中一步联合推断相机姿态和密集几何，具有显著优势。本综述旨在系统回顾这一新兴领域。

Method: 本文采用综述方法，深入剖析了前向传播三维重建模型的技术框架（包括Transformer对应建模、联合姿态与几何回归、多视角扩展），并将其与传统管道及早期学习方法进行对比。此外，还概述了相关数据集和评估指标。

Result: 本综述系统回顾并分析了基于深度学习的前向传播三维重建新范式。文章详细阐述了其技术构成、与现有方法的区别，并总结了相关数据集与评估标准。同时，探讨了该技术的广泛应用前景、主要挑战及发展机遇。

Conclusion: 基于深度学习的前向传播三维重建范式代表了该领域的重大变革，实现了从图像直接、单次推断姿态和几何。尽管前景广阔，但模型精度、可扩展性以及动态场景处理仍是未来需要克服的关键挑战。

Abstract: 3D reconstruction, which aims to recover the dense three-dimensional
structure of a scene, is a cornerstone technology for numerous applications,
including augmented/virtual reality, autonomous driving, and robotics. While
traditional pipelines like Structure from Motion (SfM) and Multi-View Stereo
(MVS) achieve high precision through iterative optimization, they are limited
by complex workflows, high computational cost, and poor robustness in
challenging scenarios like texture-less regions. Recently, deep learning has
catalyzed a paradigm shift in 3D reconstruction. A new family of models,
exemplified by DUSt3R, has pioneered a feed-forward approach. These models
employ a unified deep network to jointly infer camera poses and dense geometry
directly from an Unconstrained set of images in a single forward pass. This
survey provides a systematic review of this emerging domain. We begin by
dissecting the technical framework of these feed-forward models, including
their Transformer-based correspondence modeling, joint pose and geometry
regression mechanisms, and strategies for scaling from two-view to multi-view
scenarios. To highlight the disruptive nature of this new paradigm, we contrast
it with both traditional pipelines and earlier learning-based methods like
MVSNet. Furthermore, we provide an overview of relevant datasets and evaluation
metrics. Finally, we discuss the technology's broad application prospects and
identify key future challenges and opportunities, such as model accuracy and
scalability, and handling dynamic scenes.

</details>


### [95] [A document is worth a structured record: Principled inductive bias design for document recognition](https://arxiv.org/abs/2507.08458)
*Benjamin Meyer,Lukas Tuggener,Sascha Hänzi,Daniel Schmid,Erdal Ayfer,Benjamin F. Grewe,Ahmed Abdulkadir,Thilo Stadelmann*

Main category: cs.CV

TL;DR: 该研究提出一种将文档识别视为转录任务的新视角，通过引入结构化归纳偏置和基于Transformer的架构，实现对复杂文档（如工程图）的端到端识别，解决了传统方法忽视文档固有结构的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文档识别方法主要将其视为计算机视觉问题，忽略了文档固有的、约定驱动的结构特性，导致需要次优的启发式后处理，并且难以有效识别不常见或更复杂的文档类型。

Method: 研究将文档识别重构为从文档到记录的转录任务，并根据文档固有的转录结构进行自然分组。提出了一种为底层机器学习端到端文档识别系统设计结构特定归纳偏置的方法，并开发了一个可成功适应不同结构的基础Transformer架构。

Result: 通过对单旋律乐谱、形状图和简化工程图等复杂记录结构的广泛实验，验证了所发现归纳偏置的有效性。特别是，通过集成无限制图结构的归纳偏置，首次成功训练了将工程图转录为其固有互联信息的端到端模型。

Conclusion: 该方法为设计针对标准OCR、OMR等不常见文档类型的识别系统提供了重要参考，并为统一未来文档基础模型的设计提供了指导。

Abstract: Many document types use intrinsic, convention-driven structures that serve to
encode precise and structured information, such as the conventions governing
engineering drawings. However, state-of-the-art approaches treat document
recognition as a mere computer vision problem, neglecting these underlying
document-type-specific structural properties, making them dependent on
sub-optimal heuristic post-processing and rendering many less frequent or more
complicated document types inaccessible to modern document recognition. We
suggest a novel perspective that frames document recognition as a transcription
task from a document to a record. This implies a natural grouping of documents
based on the intrinsic structure inherent in their transcription, where related
document types can be treated (and learned) similarly. We propose a method to
design structure-specific inductive biases for the underlying machine-learned
end-to-end document recognition systems, and a respective base transformer
architecture that we successfully adapt to different structures. We demonstrate
the effectiveness of the so-found inductive biases in extensive experiments
with progressively complex record structures from monophonic sheet music, shape
drawings, and simplified engineering drawings. By integrating an inductive bias
for unrestricted graph structures, we train the first-ever successful
end-to-end model to transcribe engineering drawings to their inherently
interlinked information. Our approach is relevant to inform the design of
document recognition systems for document types that are less well understood
than standard OCR, OMR, etc., and serves as a guide to unify the design of
future document foundation models.

</details>


### [96] [F3-Net: Foundation Model for Full Abnormality Segmentation of Medical Images with Flexible Input Modality Requirement](https://arxiv.org/abs/2507.08460)
*Seyedeh Sahar Taheri Otaghsara,Reza Rahmanzadeh*

Main category: cs.CV

TL;DR: F3-Net是一种医学图像分割基础模型，通过灵活的模态训练和统一架构，解决了多模态输入缺失、泛化性差和任务特异性窄等挑战，表现出卓越的性能和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 当前的临床医学图像分割模型面临多重挑战：过度依赖完整的多模态输入、泛化能力受限以及任务特异性过窄，这些都阻碍了其在真实临床环境中的广泛应用。

Method: F3-Net采用灵活的合成模态训练，利用“零图像”策略替代缺失的MRI序列，无需显式合成网络，确保了在输入不完整时的鲁棒性。其统一架构支持胶质瘤、转移瘤、中风和白质病变等多种病理的分割，且无需针对特定疾病进行重新训练。

Result: 在BraTS 2021、BraTS 2024和ISLES 2022等多样化数据集上评估，F3-Net展现出强大的域偏移和临床异质性鲁棒性。在整体病理数据集上，其平均Dice相似系数（DSCs）分别为：BraTS-GLI 2024为0.94，BraTS-MET 2024为0.82，BraTS 2021为0.94，ISLES 2022为0.79，优于传统的CNN和Transformer模型。

Conclusion: F3-Net作为一种通用且可扩展的解决方案，有效弥合了深度学习研究与实际临床部署之间的鸿沟，在解决复杂医学图像分割问题方面具有巨大的应用潜力。

Abstract: F3-Net is a foundation model designed to overcome persistent challenges in
clinical medical image segmentation, including reliance on complete multimodal
inputs, limited generalizability, and narrow task specificity. Through flexible
synthetic modality training, F3-Net maintains robust performance even in the
presence of missing MRI sequences, leveraging a zero-image strategy to
substitute absent modalities without relying on explicit synthesis networks,
thereby enhancing real-world applicability. Its unified architecture supports
multi-pathology segmentation across glioma, metastasis, stroke, and white
matter lesions without retraining, outperforming CNN-based and
transformer-based models that typically require disease-specific fine-tuning.
Evaluated on diverse datasets such as BraTS 2021, BraTS 2024, and ISLES 2022,
F3-Net demonstrates strong resilience to domain shifts and clinical
heterogeneity. On the whole pathology dataset, F3-Net achieves average Dice
Similarity Coefficients (DSCs) of 0.94 for BraTS-GLI 2024, 0.82 for BraTS-MET
2024, 0.94 for BraTS 2021, and 0.79 for ISLES 2022. This positions it as a
versatile, scalable solution bridging the gap between deep learning research
and practical clinical deployment.

</details>


### [97] [Dual Dimensions Geometric Representation Learning Based Document Dewarping](https://arxiv.org/abs/2507.08492)
*Heng Li,Qingcai Chen,Xiangping Wu*

Main category: cs.CV

TL;DR: 提出D2Dewarp模型，通过关注文档图像的水平和垂直双维度线条信息，解决了现有去畸变方法仅关注单维度的局限性。该方法结合新型融合模块并构建了大规模数据集，在公开基准测试上取得了优于SOTA的去畸变效果。


<details>
  <summary>Details</summary>
Motivation: 文档图像去畸变在深度学习时代仍具挑战性。现有方法虽利用文本行信息，但通常只关注单一水平维度，难以处理复杂形变，限制了去畸变效果。

Method: 提出了D2Dewarp，一个细粒度形变感知模型，关注文档的水平-垂直双维度线条信息以改进去畸变。设计了基于X和Y坐标的有效融合模块，以结合水平和垂直粒度特征，实现特征互补。针对现有公开数据集缺乏标注线条特征的问题，提出了一种自动细粒度标注方法，利用公共文档纹理图像和自动渲染引擎构建了一个新的大规模畸变训练数据集。

Result: 在公开的中文和英文基准测试上，本方法与现有最先进方法相比，取得了更好的矫正结果。定量和定性结果均显示了方法的优越性。

Conclusion: D2Dewarp模型通过感知文档双维度线条信息，有效提升了文档图像去畸变性能，超越了现有SOTA方法。新构建的大规模数据集和提出的自动标注方法为该领域的研究提供了宝贵资源，代码和数据集的公开将促进后续研究。

Abstract: Document image dewarping remains a challenging task in the deep learning era.
While existing methods have improved by leveraging text line awareness, they
typically focus only on a single horizontal dimension. In this paper, we
propose a fine-grained deformation perception model that focuses on Dual
Dimensions of document horizontal-vertical-lines to improve document Dewarping
called D2Dewarp. It can perceive distortion trends in different directions
across document details. To combine the horizontal and vertical granularity
features, an effective fusion module based on X and Y coordinate is designed to
facilitate interaction and constraint between the two dimensions for feature
complementarity. Due to the lack of annotated line features in current public
dewarping datasets, we also propose an automatic fine-grained annotation method
using public document texture images and an automatic rendering engine to build
a new large-scale distortion training dataset. The code and dataset will be
publicly released. On public Chinese and English benchmarks, both quantitative
and qualitative results show that our method achieves better rectification
results compared with the state-of-the-art methods. The dataset will be
publicly available at https://github.com/xiaomore/DocDewarpHV

</details>


### [98] [Unified People Tracking with Graph Neural Networks](https://arxiv.org/abs/2507.08494)
*Martin Engilberge,Ivan Vrkic,Friedrich Wilke Grosche,Julien Pilet,Engin Turetken,Pascal Fua*

Main category: cs.CV

TL;DR: 本文提出一种统一且完全可微分的多行人跟踪模型，通过动态时空图直接关联检测生成轨迹，并引入一个大规模新数据集，在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有行人多目标跟踪方法通常依赖预计算的轨迹，且在复杂遮挡下表现不佳。研究旨在开发一个端到端、无需预计算轨迹的跟踪模型，并有效处理遮挡。

Method: 该模型构建了一个动态时空图，聚合空间、上下文和时间信息以实现信息传播，并能编码场景特定信息以改善遮挡处理。此外，研究还引入了一个包含25个部分重叠视图、详细场景重建和大量遮挡的大规模新数据集。

Result: 实验结果表明，该模型在公共基准测试和新数据集上均实现了最先进的性能，并在各种条件下表现出灵活性。

Conclusion: 该研究提出的模型和数据集的公开发布将有助于推动多行人跟踪领域的研究进展。

Abstract: This work presents a unified, fully differentiable model for multi-people
tracking that learns to associate detections into trajectories without relying
on pre-computed tracklets. The model builds a dynamic spatiotemporal graph that
aggregates spatial, contextual, and temporal information, enabling seamless
information propagation across entire sequences. To improve occlusion handling,
the graph can also encode scene-specific information. We also introduce a new
large-scale dataset with 25 partially overlapping views, detailed scene
reconstructions, and extensive occlusions. Experiments show the model achieves
state-of-the-art performance on public benchmarks and the new dataset, with
flexibility across diverse conditions. Both the dataset and approach will be
publicly released to advance research in multi-people tracking.

</details>


### [99] [Occlusion-Guided Feature Purification Learning via Reinforced Knowledge Distillation for Occluded Person Re-Identification](https://arxiv.org/abs/2507.08520)
*Yufei Zheng,Wenjun Wang,Wenjun Gan,Jiawei Liu*

Main category: cs.CV

TL;DR: 本文提出OGFR模型，通过强化知识蒸馏和特征净化学习，解决了遮挡行人重识别中多样化遮挡处理和特征污染的难题，从而学习到鲁棒的行人特征表示。


<details>
  <summary>Details</summary>
Motivation: 现有的遮挡行人重识别方法难以有效处理训练中未见的多样化遮挡场景，并且存在来自完整图像的特征污染问题。

Method: 本文提出Occlusion-Guided Feature Purification Learning via Reinforced Knowledge Distillation (OGFR)方法。该方法采用师生蒸馏架构，包含：1. Occlusion-Aware Vision Transformer，利用可学习的遮挡模式嵌入来明确建模多样化遮挡类型，以指导生成遮挡感知鲁棒的特征表示。2. Feature Erasing and Purification Module，在完整分支中通过深度强化学习代理识别并替换完整图像中含噪声的低质量补丁，以避免特征污染并挖掘身份相关线索。通过强化知识蒸馏，将净化后的判别性完整知识从完整分支传递给遮挡分支。

Result: 该方法能够同时缓解现有方法的挑战，并使学生分支（遮挡分支）有效吸收净化后的完整知识，从而在不受遮挡干扰的情况下精确学习到鲁棒的特征表示。

Conclusion: OGFR成功解决了遮挡行人重识别中应对多样化遮挡情景和避免特征污染的关键挑战，通过其独特的特征净化和强化知识蒸馏机制，实现了鲁棒的行人特征学习。

Abstract: Occluded person re-identification aims to retrieve holistic images based on
occluded ones. Existing methods often rely on aligning visible body parts,
applying occlusion augmentation, or complementing missing semantics using
holistic images. However, they face challenges in handling diverse occlusion
scenarios not seen during training and the issue of feature contamination from
holistic images. To address these limitations, we propose Occlusion-Guided
Feature Purification Learning via Reinforced Knowledge Distillation (OGFR),
which simultaneously mitigates these challenges. OGFR adopts a teacher-student
distillation architecture that effectively incorporates diverse occlusion
patterns into feature representation while transferring the purified
discriminative holistic knowledge from the holistic to the occluded branch
through reinforced knowledge distillation. Specifically, an Occlusion-Aware
Vision Transformer is designed to leverage learnable occlusion pattern
embeddings to explicitly model such diverse occlusion types, thereby guiding
occlusion-aware robust feature representation. Moreover, we devise a Feature
Erasing and Purification Module within the holistic branch, in which an agent
is employed to identify low-quality patch tokens of holistic images that
contain noisy negative information via deep reinforcement learning, and
substitute these patch tokens with learnable embedding tokens to avoid feature
contamination and further excavate identity-related discriminative clues.
Afterward, with the assistance of knowledge distillation, the student branch
effectively absorbs the purified holistic knowledge to precisely learn robust
representation regardless of the interference of occlusions.

</details>


### [100] [RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval Using Radiomics Features](https://arxiv.org/abs/2507.08546)
*Inye Na,Nejung Rue,Jiwon Chung,Hyunjin Park*

Main category: cs.CV

TL;DR: 提出RadiomicsRetrieval，一个3D医学图像检索框架，结合放射组学特征与深度学习嵌入，实现灵活查询，仅需少量用户提示，提升检索特异性和临床适用性。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像检索主要限于2D图像并需完整标注，缺乏临床灵活性，难以充分利用3D体数据和丰富的空间信息。

Method: 提出RadiomicsRetrieval框架，结合3D放射组学描述符与肿瘤级深度学习嵌入。利用可提示分割模型获取肿瘤图像嵌入，并通过对比学习与放射组学特征对齐，再通过解剖位置嵌入(APE)丰富表示。支持基于形状、位置或部分特征集的灵活查询，仅需少量用户提示。

Result: 在肺CT和脑MRI数据集上的实验表明，放射组学特征显著增强检索特异性，APE为基于位置的搜索提供关键全局解剖上下文。框架仅需极少用户提示，最大限度减少分割开销，支持多样化临床场景。

Conclusion: RadiomicsRetrieval通过结合多模态特征和灵活的查询方式，显著提升了3D医学图像检索的能力和临床实用性，有望促进大规模医学影像库在诊断、治疗规划和研究中的应用。

Abstract: Medical image retrieval is a valuable field for supporting clinical
decision-making, yet current methods primarily support 2D images and require
fully annotated queries, limiting clinical flexibility. To address this, we
propose RadiomicsRetrieval, a 3D content-based retrieval framework bridging
handcrafted radiomics descriptors with deep learning-based embeddings at the
tumor level. Unlike existing 2D approaches, RadiomicsRetrieval fully exploits
volumetric data to leverage richer spatial context in medical images. We employ
a promptable segmentation model (e.g., SAM) to derive tumor-specific image
embeddings, which are aligned with radiomics features extracted from the same
tumor via contrastive learning. These representations are further enriched by
anatomical positional embedding (APE). As a result, RadiomicsRetrieval enables
flexible querying based on shape, location, or partial feature sets. Extensive
experiments on both lung CT and brain MRI public datasets demonstrate that
radiomics features significantly enhance retrieval specificity, while APE
provides global anatomical context essential for location-based searches.
Notably, our framework requires only minimal user prompts (e.g., a single
point), minimizing segmentation overhead and supporting diverse clinical
scenarios. The capability to query using either image embeddings or selected
radiomics attributes highlights its adaptability, potentially benefiting
diagnosis, treatment planning, and research on large-scale medical imaging
repositories. Our code is available at
https://github.com/nainye/RadiomicsRetrieval.

</details>


### [101] [SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2](https://arxiv.org/abs/2507.08548)
*Alen Adamyan,Tomáš Čížek,Matej Straka,Klara Janouskova,Martin Schmid*

Main category: cs.CV

TL;DR: 本研究提出一种基于强化学习的方法来优化SAM 2在视觉目标跟踪中的记忆更新，其性能显著优于现有手动规则。


<details>
  <summary>Details</summary>
Motivation: 现有SAM 2在视觉目标跟踪中通过手动规则处理干扰、遮挡和目标运动，效果有待进一步提升；研究旨在挖掘SAM 2记忆库的未开发潜力。

Method: 将SAM 2的记忆控制建模为序列决策问题，并采用强化学习来优化记忆库的更新策略。

Result: 在过拟合设置中，所提方法在SAM 2上的相对性能提升超过现有启发式方法增益的三倍以上。

Conclusion: 研究结果揭示了SAM 2记忆库的巨大潜力，并表明强化学习是视觉目标跟踪中记忆控制的一种强大且优于手动更新规则的替代方案。

Abstract: Segment Anything Model 2 (SAM 2) has demonstrated strong performance in
object segmentation tasks and has become the state-of-the-art for visual object
tracking. The model stores information from previous frames in a memory bank,
enabling temporal consistency across video sequences. Recent methods augment
SAM 2 with hand-crafted update rules to better handle distractors, occlusions,
and object motion. We propose a fundamentally different approach using
reinforcement learning for optimizing memory updates in SAM 2 by framing memory
control as a sequential decision-making problem. In an overfitting setup with a
separate agent per video, our method achieves a relative improvement over SAM 2
that exceeds by more than three times the gains of existing heuristics. These
results reveal the untapped potential of the memory bank and highlight
reinforcement learning as a powerful alternative to hand-crafted update rules
for memory control in visual object tracking.

</details>


### [102] [Image Translation with Kernel Prediction Networks for Semantic Segmentation](https://arxiv.org/abs/2507.08554)
*Cristina Mata,Michael S. Ryoo,Henrik Turbell*

Main category: cs.CV

TL;DR: 语义分割依赖大量像素级标注，但真实数据标注困难，合成数据存在域间隙。现有非配对图像翻译方法（GANs）无法保证语义匹配。本文提出DA-KPN，一种新型图像翻译方法，可确保语义匹配，并在语义分割基准测试中超越了现有GAN方法。


<details>
  <summary>Details</summary>
Motivation: 语义分割需要密集的像素级标注，但真实世界数据获取困难。虽然合成数据集可用于训练，但存在显著的域间隙。现有非配对图像翻译方法（通常基于GAN并利用循环一致性）无法保证像素级语义匹配，而语义分割性能对噪声像素标签非常敏感，这成为提升模型性能的关键障碍。

Method: 本文提出了一种名为“域对抗核预测网络”（Domain Adversarial Kernel Prediction Network, DA-KPN）的新型图像翻译方法，该方法能保证合成标签与翻译结果之间的语义匹配。DA-KPN通过估计轻量级、简单翻译函数的像素级输入变换参数来实现此目的。为确保像素级变换的真实性，DA-KPN采用了多尺度判别器来区分翻译图像和目标域样本。

Result: DA-KPN在语义分割的syn2real基准测试中，特别是在真实图像标签获取有限的情况下，其性能优于以往基于GAN的方法。此外，在人脸解析任务上，DA-KPN也取得了可与现有方法媲美的性能。

Conclusion: DA-KPN通过保证像素级语义一致性，有效地解决了非配对图像翻译中现有方法无法保证语义匹配的问题，这对语义分割至关重要。其在syn2real基准测试中超越现有GAN方法的表现，证明了DA-KPN在弥合域间隙和提升利用合成数据进行语义分割的准确性方面的有效性。

Abstract: Semantic segmentation relies on many dense pixel-wise annotations to achieve
the best performance, but owing to the difficulty of obtaining accurate
annotations for real world data, practitioners train on large-scale synthetic
datasets. Unpaired image translation is one method used to address the ensuing
domain gap by generating more realistic training data in low-data regimes.
Current methods for unpaired image translation train generative adversarial
networks (GANs) to perform the translation and enforce pixel-level semantic
matching through cycle consistency. These methods do not guarantee that the
semantic matching holds, posing a problem for semantic segmentation where
performance is sensitive to noisy pixel labels. We propose a novel image
translation method, Domain Adversarial Kernel Prediction Network (DA-KPN), that
guarantees semantic matching between the synthetic label and translation.
DA-KPN estimates pixel-wise input transformation parameters of a lightweight
and simple translation function. To ensure the pixel-wise transformation is
realistic, DA-KPN uses multi-scale discriminators to distinguish between
translated and target samples. We show DA-KPN outperforms previous GAN-based
methods on syn2real benchmarks for semantic segmentation with limited access to
real image labels and achieves comparable performance on face parsing.

</details>


### [103] [Disentangling Instance and Scene Contexts for 3D Semantic Scene Completion](https://arxiv.org/abs/2507.08555)
*Enyu Liu,En Yu,Sijia Chen,Wenbing Tao*

Main category: cs.CV

TL;DR: 本文提出DISC，一种新的双流范式，通过利用类别级信息显著提升3D语义场景补全性能，在基准测试中即使使用单帧输入也达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的3D语义场景补全方法主要关注体素级特征，但这种方式限制了对类别级信息的利用，而类别级信息对于提升补全结果的粒度至关重要。

Method: 提出名为DISC（Disentangling Instance and Scene Contexts）的双流范式，通过分离优化增强实例和场景类别的学习。具体地，它用结合类别特定几何和语义先验的判别性类别查询取代体素查询，并利用类别固有属性设计专用解码模块，以促进有针对性的交互和高效的类别级信息流动。

Result: 在SemanticKITTI和SSCBench-KITTI-360基准测试中均达到最先进（SOTA）性能，mIoU分数分别为17.35和20.55。仅使用单帧输入就超越了多帧SOTA方法，并且显著提升了实例类别的性能，在SemanticKITTI隐藏测试中，实例mIoU分别比单帧和多帧SOTA提高了17.9%和11.9%。

Conclusion: DISC方法通过其新颖的双流范式，有效地利用了类别级信息，显著提升了3D语义场景补全的性能，尤其是在实例类别上表现突出，并用单帧输入实现了新的SOTA。

Abstract: 3D Semantic Scene Completion (SSC) has gained increasing attention due to its
pivotal role in 3D perception. Recent advancements have primarily focused on
refining voxel-level features to construct 3D scenes. However, treating voxels
as the basic interaction units inherently limits the utilization of class-level
information, which is proven critical for enhancing the granularity of
completion results. To address this, we propose \textbf{D}isentangling Instance
and Scene Contexts (DISC), a novel dual-stream paradigm that enhances learning
for both instance and scene categories through separated optimization.
Specifically, we replace voxel queries with discriminative class queries, which
incorporate class-specific geometric and semantic priors. Additionally, we
exploit the intrinsic properties of classes to design specialized decoding
modules, facilitating targeted interactions and efficient class-level
information flow. Experimental results demonstrate that DISC achieves
state-of-the-art (SOTA) performance on both SemanticKITTI and
SSCBench-KITTI-360 benchmarks, with mIoU scores of 17.35 and 20.55,
respectively. Remarkably, DISC even outperforms multi-frame SOTA methods using
only single-frame input and significantly improves instance category
performance, surpassing both single-frame and multi-frame SOTA instance mIoU by
17.9\% and 11.9\%, respectively, on the SemanticKITTI hidden test. The code is
available at https://github.com/Enyu-Liu/DISC.

</details>


### [104] [A Multi-Modal Fusion Framework for Brain Tumor Segmentation Based on 3D Spatial-Language-Vision Integration and Bidirectional Interactive Attention Mechanism](https://arxiv.org/abs/2507.08574)
*Mingda Zhang,Kaiwen Pan*

Main category: cs.CV

TL;DR: 本研究提出一个多模态空间-语言-视觉融合框架，通过双向交互注意力机制显著提升脑肿瘤分割的准确性和边界描绘，并在BraTS 2020数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种新型多模态融合框架，通过整合空间-语言-视觉信息和双向交互注意力机制，以提高脑肿瘤分割的准确性和边界描绘能力。

Method: 提出多模态语义融合适配器（MSFA）用于通过分层语义解耦整合3D MRI数据与临床文本，并设计双向交互视觉-语义注意力机制（BIVA）以实现模态间迭代信息交换。在BraTS 2020数据集（包含369个MRI扫描）上进行了评估。

Result: 该方法在增强肿瘤、肿瘤核心和全肿瘤区域实现了平均Dice系数0.8505和95% Hausdorff距离2.8256mm，优于SCAU-Net、CA-Net和3D U-Net等现有先进方法。消融研究证实了语义和空间模块对边界精度的关键贡献。

Conclusion: 结合多模态语义融合与双向交互注意力的框架显著提升了脑肿瘤分割性能，为将临床知识整合到医学图像分析中开辟了新范式。

Abstract: This study aims to develop a novel multi-modal fusion framework for brain
tumor segmentation that integrates spatial-language-vision information through
bidirectional interactive attention mechanisms to improve segmentation accuracy
and boundary delineation. Methods: We propose two core components: Multi-modal
Semantic Fusion Adapter (MSFA) integrating 3D MRI data with clinical text
descriptions through hierarchical semantic decoupling, and Bidirectional
Interactive Visual-semantic Attention (BIVA) enabling iterative information
exchange between modalities. The framework was evaluated on BraTS 2020 dataset
comprising 369 multi-institutional MRI scans. Results: The proposed method
achieved average Dice coefficient of 0.8505 and 95% Hausdorff distance of
2.8256mm across enhancing tumor, tumor core, and whole tumor regions,
outperforming state-of-the-art methods including SCAU-Net, CA-Net, and 3D
U-Net. Ablation studies confirmed critical contributions of semantic and
spatial modules to boundary precision. Conclusion: Multi-modal semantic fusion
combined with bidirectional interactive attention significantly enhances brain
tumor segmentation performance, establishing new paradigms for integrating
clinical knowledge into medical image analysis.

</details>


### [105] [BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language Models via Gaussian Discriminant Analysis](https://arxiv.org/abs/2507.08607)
*Shuang Cui,Jinglin Xu,Yi Li,Xiongxin Tang,Jiangmeng Li,Jiahuan Zhou,Fanjiang Xu,Fuchun Sun,Hui Xiong*

Main category: cs.CV

TL;DR: 为解决视觉-语言模型（VLM）在时间演变分布漂移下的性能下降问题，本文提出BayesTTA，一种贝叶斯自适应框架。该框架通过增量估计和动态对齐，在保持效率的同时显著优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLM）如CLIP在真实世界中常见的时间演变分布漂移（如渐变光照、季节变化）下性能显著下降。现有持续测试时自适应（CTTA）方法主要针对突发且严重的分布漂移，忽视了时间连续性，导致三个核心缺陷：有限的内存缓存限制了长期分布建模，引起灾难性遗忘；基于熵的置信度在时间漂移下变得不可靠，加剧了错误累积；以及静态视觉表示与演变输入不匹配。

Method: 本文将该问题正式化为持续时间测试时自适应（CT-TTA）。为此，提出BayesTTA，一个贝叶斯自适应框架，旨在强制实现时间上一致的预测并动态对齐视觉表示。具体而言，BayesTTA无需存储原始数据即可增量估计类别条件高斯混合分布，通过统计假设检验自适应选择协方差结构，并利用高斯判别分析（GDA）进行校准推理。这些校准后的预测监督归一化层的自步长自适应，确保高效稳定的表示对齐。研究还建立了包含四个时间演变数据集的CT-TTA基准，并在十个标准TTA数据集上评估泛化能力。

Result: 广泛的实验表明，BayesTTA在CT-TTA基准和标准TTA数据集上始终优于最先进的方法，取得了显著的性能提升，同时保持了高效性。

Conclusion: BayesTTA成功解决了视觉-语言模型在时间演变分布漂移下的性能下降问题，通过其创新的贝叶斯自适应框架，在校准推理和表示对齐方面表现出优越性，并显著超越现有方法，为处理此类实际场景提供了高效且稳健的解决方案。

Abstract: Vision-language models (VLMs) such as CLIP achieve strong zero-shot
recognition but degrade significantly under \textit{temporally evolving
distribution shifts} common in real-world scenarios (e.g., gradual illumination
or seasonal changes). Existing continual test-time adaptation (CTTA) methods
are typically built around sudden and severe distribution shifts and neglect
temporal continuity, leading to three core defects: limited memory cache
restricts long-range distribution modeling, causing catastrophic forgetting;
entropy-based confidence becomes unreliable under temporal drift, worsening
error accumulation; and static visual representations misalign with evolving
inputs. We formalize this practical problem as \textit{Continual-Temporal
Test-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over
time. To address it, we propose \textit{BayesTTA}, a Bayesian adaptation
framework that enforces temporally consistent predictions and dynamically
aligns visual representations. Specifically, BayesTTA incrementally estimates
class-conditional Gaussian mixture distributions without storing raw data,
adaptively selects covariance structures through statistical hypothesis
testing, and performs calibrated inference using Gaussian discriminant analysis
(GDA). These calibrated predictions supervise self-paced adaptation of
normalization layers, ensuring efficient and stable representation alignment.
We establish a comprehensive CT-TTA benchmark across four temporally evolving
datasets and further evaluate generalization on ten standard TTA datasets.
Extensive experiments show that BayesTTA consistently outperforms
state-of-the-art methods, achieving significant gains while maintaining
efficiency. Code is available at
\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.

</details>


### [106] [Normalized vs Diplomatic Annotation: A Case Study of Automatic Information Extraction from Handwritten Uruguayan Birth Certificates](https://arxiv.org/abs/2507.08636)
*Natalia Bottaioli,Solène Tarride,Jérémy Anger,Seginus Mowlavi,Marina Gardella,Antoine Tadros,Gabriele Facciolo,Rafael Grompone von Gioi,Christopher Kermorvant,Jean-Michel Morel,Javier Preciozzi*

Main category: cs.CV

TL;DR: 本研究评估了Document Attention Network (DAN) 在乌拉圭手写出生证明上提取关键信息的能力，并比较了两种标注策略对不同类型字段的有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在评估Document Attention Network (DAN) 在手写西班牙语乌拉圭出生证明上提取关键信息的性能，并探讨在有限训练数据和标注投入下，哪种标注策略能更有效地提升其准确性。

Method: 采用Document Attention Network (DAN) 模型，并在由201份手写出生证明（来自15+位书写者）组成的两个数据集上进行实验。这两个数据集使用相同的图像但应用了不同的标注方法（标准化标注和外交式标注）进行构建，以评估其对DAN模型性能的影响。

Result: 研究发现，标准化标注对于可标准化的字段（如出生日期和地点）更为有效；而外交式标注对于不可标准化的字段（如姓名和姓氏）表现更优。

Conclusion: 针对手写文档中的关键信息提取，应根据字段的可标准化特性选择合适的标注策略，以优化模型性能。

Abstract: This study evaluates the recently proposed Document Attention Network (DAN)
for extracting key-value information from Uruguayan birth certificates,
handwritten in Spanish. We investigate two annotation strategies for
automatically transcribing handwritten documents, fine-tuning DAN with minimal
training data and annotation effort. Experiments were conducted on two datasets
containing the same images (201 scans of birth certificates written by more
than 15 different writers) but with different annotation methods. Our findings
indicate that normalized annotation is more effective for fields that can be
standardized, such as dates and places of birth, whereas diplomatic annotation
performs much better for fields containing names and surnames, which can not be
standardized.

</details>


### [107] [OnlineBEV: Recurrent Temporal Fusion in Bird's Eye View Representations for Multi-Camera 3D Perception](https://arxiv.org/abs/2507.08644)
*Junho Koh,Youngwoo Lee,Jungho Kim,Dongyoung Lee,Jun Won Choi*

Main category: cs.CV

TL;DR: 本文提出OnlineBEV，一种基于循环结构和运动引导特征对齐（通过MBFNet和时序一致性学习损失）的时序3D感知方法，有效整合多帧BEV特征，并在nuScenes基准上实现相机纯视觉3D目标检测的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于多视角相机的3D感知方法通过组合时序BEV特征来提升性能，但在组合大量图像帧时，即使补偿了自车运动，由于物体运动导致的BEV特征动态变化，时序聚合的性能增益有限。

Method: 本文引入OnlineBEV，一种新的时序3D感知方法。它使用循环结构随时间组合BEV特征，以最小的内存使用量增加有效组合特征的数量。为实现时序特征对齐，OnlineBEV采用运动引导BEV融合网络（MBFNet），该网络从连续BEV帧中提取运动特征，并利用这些特征动态对齐历史与当前BEV特征。为显式强制时序特征对齐，引入了时序一致性学习损失（Temporal Consistency Learning Loss），用于捕获历史与目标BEV特征之间的差异。

Result: 在nuScenes基准测试中，OnlineBEV相对于当前最佳方法SOLOFusion取得了显著的性能提升。OnlineBEV在nuScenes测试集上达到了63.9%的NDS，创造了相机纯视觉3D目标检测任务的最新技术水平（SOTA）。

Conclusion: OnlineBEV通过创新的循环结构和运动引导特征对齐机制，有效解决了多帧BEV特征聚合的挑战，显著提升了3D感知性能，并为相机纯视觉3D目标检测树立了新的标杆。

Abstract: Multi-view camera-based 3D perception can be conducted using bird's eye view
(BEV) features obtained through perspective view-to-BEV transformations.
Several studies have shown that the performance of these 3D perception methods
can be further enhanced by combining sequential BEV features obtained from
multiple camera frames. However, even after compensating for the ego-motion of
an autonomous agent, the performance gain from temporal aggregation is limited
when combining a large number of image frames. This limitation arises due to
dynamic changes in BEV features over time caused by object motion. In this
paper, we introduce a novel temporal 3D perception method called OnlineBEV,
which combines BEV features over time using a recurrent structure. This
structure increases the effective number of combined features with minimal
memory usage. However, it is critical to spatially align the features over time
to maintain strong performance. OnlineBEV employs the Motion-guided BEV Fusion
Network (MBFNet) to achieve temporal feature alignment. MBFNet extracts motion
features from consecutive BEV frames and dynamically aligns historical BEV
features with current ones using these motion features. To enforce temporal
feature alignment explicitly, we use Temporal Consistency Learning Loss, which
captures discrepancies between historical and target BEV features. Experiments
conducted on the nuScenes benchmark demonstrate that OnlineBEV achieves
significant performance gains over the current best method, SOLOFusion.
OnlineBEV achieves 63.9% NDS on the nuScenes test set, recording
state-of-the-art performance in the camera-only 3D object detection task.

</details>


### [108] [DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets from Real-World Images](https://arxiv.org/abs/2507.08648)
*Haoran Sun,Haoyu Bian,Shaoning Zeng,Yunbo Rao,Xu Xu,Lin Mei,Jianping Gou*

Main category: cs.CV

TL;DR: 本研究提出DatasetAgent，一个基于多智能体和多模态大语言模型（MLLMs）的系统，旨在自动化从真实世界图像中构建高质量数据集，以克服传统手动方法的低效和AI生成数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统图像数据集构建方式（手动收集和标注）耗时且效率低下。尽管大型模型可用于数据生成，但真实世界数据比AI生成数据更有价值。因此，需要一种自动化从真实世界图像中构建数据集的有效方法。

Method: 本文提出了DatasetAgent，一个多智能体协作系统，用于自动构建真实世界图像数据集。它通过协调四个配备多模态大型语言模型（MLLMs）的智能体，并结合一个图像优化工具包，根据用户需求生成高质量的图像数据集。

Result: 研究在多种开源数据集上进行了两类实验：扩展现有数据集和从零开始创建新数据集。在这两种情况下，使用DatasetAgent构建的图像数据集成功地用于训练各种视觉模型，并应用于图像分类、目标检测和图像分割任务。

Conclusion: DatasetAgent提供了一种有效且自动化的方法，能够从真实世界图像构建高质量的数据集，克服了手动收集的低效和纯AI生成数据的局局限性，并为多种视觉任务提供了可靠的数据支持。

Abstract: Common knowledge indicates that the process of constructing image datasets
usually depends on the time-intensive and inefficient method of manual
collection and annotation. Large models offer a solution via data generation.
Nonetheless, real-world data are obviously more valuable comparing to
artificially intelligence generated data, particularly in constructing image
datasets. For this reason, we propose a novel method for auto-constructing
datasets from real-world images by a multiagent collaborative system, named as
DatasetAgent. By coordinating four different agents equipped with Multi-modal
Large Language Models (MLLMs), as well as a tool package for image
optimization, DatasetAgent is able to construct high-quality image datasets
according to user-specified requirements. In particular, two types of
experiments are conducted, including expanding existing datasets and creating
new ones from scratch, on a variety of open-source datasets. In both cases,
multiple image datasets constructed by DatasetAgent are used to train various
vision models for image classification, object detection, and image
segmentation.

</details>


### [109] [Generalizable 7T T1-map Synthesis from 1.5T and 3T T1 MRI with an Efficient Transformer Model](https://arxiv.org/abs/2507.08655)
*Zach Eidex,Mojtaba Safari,Tonghe Wang,Vanessa Wildman,David S. Yu,Hui Mao,Erik Middlebrooks,Aparna Kesewala,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本研究提出了一种基于Transformer的高效模型（7T-Restormer），能够从常规1.5T或3T T1加权磁共振图像合成7T质量的T1图，其性能优于现有先进方法，从而使7T MRI的优势更易于临床应用。


<details>
  <summary>Details</summary>
Motivation: 超高场7T MRI提供更高的分辨率和对比度，但其扫描仪成本高昂、稀缺且存在易感性伪影等挑战。因此，需要一种方法来弥补这些限制，使得7T MRI的优势能在常规临床环境中更易于实现。

Method: 本研究开发了一种高效的基于Transformer的模型——7T-Restormer。该模型在141例确诊多发性硬化症（MS）患者的1.5T和3T T1加权MRI与相应7T T1图的配对数据集上进行验证（共32,128个切片）。数据随机划分为训练集、验证集和测试集。研究将合成的7T T1图与ResViT和ResShift模型进行比较，并评估了混合1.5T和3T数据训练策略与单一场强策略的效果。

Result: 7T-Restormer模型表现优异：对于1.5T输入，PSNR为26.0 ± 4.6 dB，SSIM为0.861 ± 0.072，NMSE为0.019 ± 0.011；对于3T输入，PSNR为25.9 ± 4.9 dB，SSIM为0.866 ± 0.077。与ResShift和ResViT相比，7T-Restormer（仅10.5M参数）在NMSE方面有显著降低（如在1.5T输入时，NMSE相对于ResShift降低64%，相对于ResViT降低41%），且在3T输入时也表现出类似优势。此外，使用混合1.5T和3T数据集进行训练的效果优于单一场强策略。

Conclusion: 本研究提出了一种新颖的方法，能够从1.5T和3T T1加权扫描中高质量地预测7T MP2RAGE定量图，其性能优于现有最先进方法。该方法有望使7T MRI的临床优势更广泛地应用于标准临床工作流程。

Abstract: Purpose: Ultra-high-field 7T MRI offers improved resolution and contrast over
standard clinical field strengths (1.5T, 3T). However, 7T scanners are costly,
scarce, and introduce additional challenges such as susceptibility artifacts.
We propose an efficient transformer-based model (7T-Restormer) to synthesize
7T-quality T1-maps from routine 1.5T or 3T T1-weighted (T1W) images. Methods:
Our model was validated on 35 1.5T and 108 3T T1w MRI paired with corresponding
7T T1 maps of patients with confirmed MS. A total of 141 patient cases (32,128
slices) were randomly divided into 105 (25; 80) training cases (19,204 slices),
19 (5; 14) validation cases (3,476 slices), and 17 (5; 14) test cases (3,145
slices) where (X; Y) denotes the patients with 1.5T and 3T T1W scans,
respectively. The synthetic 7T T1 maps were compared against the ResViT and
ResShift models. Results: The 7T-Restormer model achieved a PSNR of 26.0 +/-
4.6 dB, SSIM of 0.861 +/- 0.072, and NMSE of 0.019 +/- 0.011 for 1.5T inputs,
and 25.9 +/- 4.9 dB, and 0.866 +/- 0.077 for 3T inputs, respectively. Using
10.5 M parameters, our model reduced NMSE by 64 % relative to 56.7M parameter
ResShift (0.019 vs 0.052, p = <.001 and by 41 % relative to 70.4M parameter
ResViT (0.019 vs 0.032, p = <.001) at 1.5T, with similar advantages at 3T
(0.021 vs 0.060 and 0.033; p < .001). Training with a mixed 1.5 T + 3 T corpus
was superior to single-field strategies. Restricting the model to 1.5T
increased the 1.5T NMSE from 0.019 to 0.021 (p = 1.1E-3) while training solely
on 3T resulted in lower performance on input 1.5T T1W MRI. Conclusion: We
propose a novel method for predicting quantitative 7T MP2RAGE maps from 1.5T
and 3T T1W scans with higher quality than existing state-of-the-art methods.
Our approach makes the benefits of 7T MRI more accessible to standard clinical
workflows.

</details>


### [110] [ByDeWay: Boost Your multimodal LLM with DEpth prompting in a Training-Free Way](https://arxiv.org/abs/2507.08679)
*Rajarshi Roy,Devleena Das,Ankesh Banerjee,Arjya Bhattacharjee,Kousik Dasgupta,Subarna Tripathi*

Main category: cs.CV

TL;DR: ByDeWay是一个免训练框架，通过分层深度提示策略（LDP）增强MLLM的空间推理和基础能力，减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 提升多模态大语言模型（MLLM）的空间推理和基础能力，减少其产生幻觉，同时避免模型参数修改和训练。

Method: 引入ByDeWay免训练框架，采用分层深度提示策略（LDP）。该方法通过单目深度估计将场景分割为近、中、远层，随后利用接地视觉语言模型生成区域特定的深度感知描述。这些结构化的描述被附加到图像-问题提示中，为MLLM提供丰富的空间上下文，从而引导模型生成更接地气、更少幻觉的响应。

Result: 在对幻觉敏感的（POPE）和推理密集型（GQA）基准测试中，ByDeWay在多个MLLM上均表现出持续的性能提升，验证了深度感知提示在零训练设置下的有效性。

Conclusion: ByDeWay框架通过其新颖的分层深度提示策略，成功地在不进行任何训练或修改模型参数的情况下，显著提升了MLLM的空间推理能力和响应的接地性，有效减少了幻觉，并具有轻量化、模块化和兼容黑盒MLLM的优点。

Abstract: We introduce ByDeWay, a training-free framework designed to enhance the
performance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel
prompting strategy called Layered-Depth-Based Prompting (LDP), which improves
spatial reasoning and grounding without modifying any model parameters. It
segments the scene into closest, mid-range, and farthest layers using monocular
depth estimation, then generates region-specific captions with a grounded
vision-language model. These structured, depth-aware captions are appended to
the image-question prompt, enriching it with spatial context. This guides MLLMs
to produce more grounded and less hallucinated responses. Our method is
lightweight, modular, and compatible with black-box MLLMs. Experiments on
hallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show
consistent improvements across multiple MLLMs, validating the effectiveness of
depth-aware prompting in a zero-training setting.

</details>


### [111] [MoSAiC: Multi-Modal Multi-Label Supervision-Aware Contrastive Learning for Remote Sensing](https://arxiv.org/abs/2507.08683)
*Debashis Gupta,Aditi Golder,Rongkhun Zhu,Kangning Cui,Wei Tang,Fan Yang,Ovidiu Csillik,Sarra Alaqahtani,V. Paul Pauca*

Main category: cs.CV

TL;DR: 本文提出MoSAiC，一个统一框架，通过结合模内和模间对比学习以及多标签监督对比损失，解决了地球系统观测中多模态卫星图像在低标签、高重叠场景下的表示学习挑战，并在基准数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 对比学习在无需大量标签数据下学习可迁移表示方面表现出色，并适用于地球系统观测（ESO），因其天然的多模态数据（光学、SAR）。然而，ESO面临高类间相似性、场景杂乱和边界模糊等挑战，特别是在低标签、多标签设置下。现有对比学习框架通常侧重于模内自监督，或缺乏跨模态的多标签对齐和语义精度机制。

Method: 引入MoSAiC框架，专为多模态卫星图像设计。该框架联合优化模内对比学习、模间对比学习，并结合多标签监督对比损失，旨在实现更精细的语义解耦和更鲁棒的表示学习。

Result: 在BigEarthNet V2.0和Sent12MS两个基准数据集上的实验表明，MoSAiC在低标签和高类别重叠情境下，在准确性、聚类一致性和泛化能力方面，持续优于全监督和自监督基线方法。

Conclusion: MoSAiC能够实现更精细的语义解耦和更鲁棒的表示学习，有效应对光谱相似和空间复杂类别带来的挑战，并在地球系统观测的低标签、高重叠场景中展现出优越的性能。

Abstract: Contrastive learning (CL) has emerged as a powerful paradigm for learning
transferable representations without the reliance on large labeled datasets.
Its ability to capture intrinsic similarities and differences among data
samples has led to state-of-the-art results in computer vision tasks. These
strengths make CL particularly well-suited for Earth System Observation (ESO),
where diverse satellite modalities such as optical and SAR imagery offer
naturally aligned views of the same geospatial regions. However, ESO presents
unique challenges, including high inter-class similarity, scene clutter, and
ambiguous boundaries, which complicate representation learning -- especially in
low-label, multi-label settings. Existing CL frameworks often focus on
intra-modality self-supervision or lack mechanisms for multi-label alignment
and semantic precision across modalities. In this work, we introduce MoSAiC, a
unified framework that jointly optimizes intra- and inter-modality contrastive
learning with a multi-label supervised contrastive loss. Designed specifically
for multi-modal satellite imagery, MoSAiC enables finer semantic
disentanglement and more robust representation learning across spectrally
similar and spatially complex classes. Experiments on two benchmark datasets,
BigEarthNet V2.0 and Sent12MS, show that MoSAiC consistently outperforms both
fully supervised and self-supervised baselines in terms of accuracy, cluster
coherence, and generalization in low-label and high-class-overlap scenarios.

</details>


### [112] [An Efficient Approach for Muscle Segmentation and 3D Reconstruction Using Keypoint Tracking in MRI Scan](https://arxiv.org/abs/2507.08690)
*Mengyuan Liu,Jeongkyu Lee*

Main category: cs.CV

TL;DR: 一种基于关键点跟踪的无训练MRI肌肉分割方法，计算成本低且可解释性强，性能与现有CNN模型相当。


<details>
  <summary>Details</summary>
Motivation: 现有MRI肌肉自动分割方法（特别是基于CNN的方法）面临计算成本高、依赖大型训练数据集、小肌肉分割精度低、泛化性差及可解释性差等问题。

Method: 提出了一种无训练的分割方法，该方法基于关键点跟踪，整合了关键点选择与Lucas-Kanade光流。

Result: 该方法实现了0.6至0.7的平均Dice相似系数（DSC），性能与最先进的CNN模型相当，同时显著降低了计算需求并增强了可解释性。

Conclusion: 所提出的可扩展框架为临床和研究应用中的肌肉分割提供了一种稳健且可解释的替代方案。

Abstract: Magnetic resonance imaging (MRI) enables non-invasive, high-resolution
analysis of muscle structures. However, automated segmentation remains limited
by high computational costs, reliance on large training datasets, and reduced
accuracy in segmenting smaller muscles. Convolutional neural network
(CNN)-based methods, while powerful, often suffer from substantial
computational overhead, limited generalizability, and poor interpretability
across diverse populations. This study proposes a training-free segmentation
approach based on keypoint tracking, which integrates keypoint selection with
Lucas-Kanade optical flow. The proposed method achieves a mean Dice similarity
coefficient (DSC) ranging from 0.6 to 0.7, depending on the keypoint selection
strategy, performing comparably to state-of-the-art CNN-based models while
substantially reducing computational demands and enhancing interpretability.
This scalable framework presents a robust and explainable alternative for
muscle segmentation in clinical and research applications.

</details>


### [113] [L-CLIPScore: a Lightweight Embedding-based Captioning Metric for Evaluating and Training](https://arxiv.org/abs/2507.08710)
*Li Li,Yingzhe Peng,Xu Yang,Ruoxi Cheng,Haiyang Xu,Ming Yan,Fei Huang*

Main category: cs.CV

TL;DR: 该论文提出L-CLIPScore，一种基于轻量级CLIP（L-CLIP）的新型嵌入式图像描述评价指标。L-CLIP通过权重复用、矩阵分解进行压缩，并采用多模态相似性调节器（SR）损失进行蒸馏。实验证明L-CLIPScore在评估和训练图像描述模型时高效且有效，但单独使用L-CLIPScore训练可能导致失败，需与基于n-gram的指标混合使用。


<details>
  <summary>Details</summary>
Motivation: 需要一种高效且有效的图像描述质量评价指标，同时也能用于训练图像描述模型。现有的类似CLIP的模型计算资源消耗大，不适合直接用于此目的。

Method: 1. 提出了L-CLIPScore，一种基于嵌入的图像描述评价指标。
2. 开发了L-CLIP，一个从原始CLIP压缩和蒸馏而来的双编码器架构。
3. 压缩技术：使用权重复用减少编码器参数，使用矩阵分解减少词嵌入矩阵参数。
4. 蒸馏技术：设计了一种新颖的多模态相似性调节器（SR）损失，用于传递视觉-语言对齐知识，具体做法是增加匹配图像-文本对的相似度，减小不匹配对的相似度。
5. 通过实验验证L-CLIPScore作为评价指标的效率和有效性。
6. 探索了L-CLIPScore作为监督信号训练图像描述模型的应用，并分析了单独使用其会导致训练失败的原因，提出需与n-gram指标混合使用。

Result: 1. L-CLIP实现了与原始CLIP相当的多模态对齐能力，但计算资源和运行时间要求更少。
2. L-CLIPScore作为判断标准时，在评估图像描述质量方面表现出高效性和有效性。
3. 发现L-CLIPScore作为监督信号训练图像描述模型时，需要与基于n-gram的指标混合使用，否则可能导致训练失败。

Conclusion: L-CLIPScore是一种基于资源高效的L-CLIP模型的新型图像描述评价指标，它能有效且高效地评估图像描述质量。同时，它也能作为监督信号用于训练图像描述模型，但在实际应用中，为了避免训练失败，需要将其与传统的基于n-gram的指标相结合使用。

Abstract: We propose a novel embedding-based captioning metric termed as L-CLIPScore
that can be used for efficiently evaluating caption quality and training
captioning model. L-CLIPScore is calculated from a lightweight CLIP (L-CLIP),
which is a dual-encoder architecture compressed and distilled from CLIP. To
compress, we apply two powerful techniques which are weight multiplexing and
matrix decomposition for reducing the parameters of encoders and word embedding
matrix, respectively. To distill, we design a novel multi-modal Similarity
Regulator (SR) loss to transfer more vision-language alignment knowledge.
Specifically, SR loss amplifies the multi-modal embedding similarity if the
given image-text pair is matched and diminishes the similarity if the pair is
non-matched. By compressing and distilling by this novel SR loss, our L-CLIP
achieves comparable multi-modal alignment ability to the original CLIP while it
requires fewer computation resources and running time. We carry out exhaustive
experiments to validate the efficiency and effectiveness of L-CLIPScore when
using it as the judge to evaluate caption quality. We also discover that when
using L-CLIPScore as the supervisor to train the captioning model, it should be
mixed up by an n-gram-based metric and meanwhile analyze why using L-CLIPScore
only will cause fail training.

</details>


### [114] [SGPMIL: Sparse Gaussian Process Multiple Instance Learning](https://arxiv.org/abs/2507.08711)
*Andreas Lolos,Stergios Christodoulidis,Maria Vakalopoulou,Jose Dolz,Aris Moustakas*

Main category: cs.CV

TL;DR: 本文提出SGPMIL，一个基于稀疏高斯过程的概率注意力多示例学习框架，用于在数字病理学中量化实例级注意力分数的不确定性，同时保持包级性能并提高实例级预测的可解释性。


<details>
  <summary>Details</summary>
Motivation: 在数字病理学等多示例学习（MIL）应用中，现有确定性注意力MIL方法虽然在包级表现良好，但忽略了实例相关性中的固有不确定性，导致实例级注意力分数缺乏不确定性量化。

Method: 引入SGPMIL，一个基于稀疏高斯过程（SGP）的新型概率注意力MIL框架。该方法通过学习注意力分数的后验分布来实现不确定性估计。它还通过在SGP预测均值函数中引入特征缩放，以提高训练速度、效率和实例级性能。

Result: SGPMIL不仅保持了有竞争力的包级性能，还显著提高了实例级预测的质量和可解释性，提供了更可靠和校准的实例相关性图。在多个数字病理学数据集上的大量实验证明了该方法在包级和实例级评估中的有效性。

Conclusion: SGPMIL成功解决了实例级注意力不确定性量化问题，通过引入概率性方法提高了预测的可靠性和可解释性。该框架为不确定性感知多示例学习提供了一个有效且高效的解决方案，特别适用于数字病理学等领域。

Abstract: Multiple Instance Learning (MIL) offers a natural solution for settings where
only coarse, bag-level labels are available, without having access to
instance-level annotations. This is usually the case in digital pathology,
which consists of gigapixel sized images. While deterministic attention-based
MIL approaches achieve strong bag-level performance, they often overlook the
uncertainty inherent in instance relevance. In this paper, we address the lack
of uncertainty quantification in instance-level attention scores by introducing
\textbf{SGPMIL}, a new probabilistic attention-based MIL framework grounded in
Sparse Gaussian Processes (SGP). By learning a posterior distribution over
attention scores, SGPMIL enables principled uncertainty estimation, resulting
in more reliable and calibrated instance relevance maps. Our approach not only
preserves competitive bag-level performance but also significantly improves the
quality and interpretability of instance-level predictions under uncertainty.
SGPMIL extends prior work by introducing feature scaling in the SGP predictive
mean function, leading to faster training, improved efficiency, and enhanced
instance-level performance. Extensive experiments on multiple well-established
digital pathology datasets highlight the effectiveness of our approach across
both bag- and instance-level evaluations. Our code will be made publicly
available.

</details>


### [115] [Unreal is all you need: Multimodal ISAC Data Simulation with Only One Engine](https://arxiv.org/abs/2507.08716)
*Kongwu Huang,Shiyi Mu,Jun Jiang,Yuan Gao,Shugong Xu*

Main category: cs.CV

TL;DR: 为探索扩展定律在ISAC中的潜力，本文提出了Great-X多模态数据数字孪生平台，并基于此构建了大规模无人机数据集Great-MSD及CSI无人机3D定位算法，验证了其可行性和通用性。


<details>
  <summary>Details</summary>
Motivation: 旨在探索扩展定律在ISAC（集成感知与通信）研究中的潜力。

Method: 1. 提出并开发了Great-X，一个单引擎多模态数据数字孪生平台，该平台在虚幻引擎中重构Sionna的射线追踪计算并深度整合自动驾驶工具。2. 利用Great-X平台，实现了CSI、RGB、雷达和激光雷达等多模态数据的同步高效仿真。3. 基于该平台，构建了开源、大规模的低空无人机多模态通感数据集Great-MSD。4. 提出了一种基于CSI的无人机3D定位基线算法。

Result: 1. 成功开发了Great-X平台和Great-MSD数据集。2. 所提出的基于CSI的无人机3D定位算法被证明是可行的，并展示了其在不同CSI仿真引擎间的良好通用性。

Conclusion: Great-X平台及其配套数据集Great-MSD为ISAC研究提供了重要的基础工具，特别是在无人机3D定位方面，所提出的基线算法验证了其有效性和广阔的应用前景。

Abstract: Scaling laws have achieved success in LLM and foundation models. To explore
their potential in ISAC research, we propose Great-X. This single-engine
multimodal data twin platform reconstructs the ray-tracing computation of
Sionna within Unreal Engine and is deeply integrated with autonomous driving
tools. This enables efficient and synchronized simulation of multimodal data,
including CSI, RGB, Radar, and LiDAR. Based on this platform, we construct an
open-source, large-scale, low-altitude UAV multimodal synaesthesia dataset
named Great-MSD, and propose a baseline CSI-based UAV 3D localization
algorithm, demonstrating its feasibility and generalizability across different
CSI simulation engines. The related code and dataset are publicly available at:
https://github.com/hkw-xg/Great-MCD.

</details>


### [116] [RoundaboutHD: High-Resolution Real-World Urban Environment Benchmark for Multi-Camera Vehicle Tracking](https://arxiv.org/abs/2507.08729)
*Yuqiang Lin,Sam Lockyer,Mingxuan Sui,Li Gan,Florian Stanek,Markus Zarbock,Wenbin Li,Adrian Evans,Nic Zhang*

Main category: cs.CV

TL;DR: 引入RoundaboutHD，一个用于多摄像头车辆跟踪的高分辨率真实世界数据集，以弥补现有数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 当前公开的多摄像头车辆跟踪（MCVT）数据集存在场景过于简单、分辨率低、多样性不足等限制，导致学术研究与实际应用之间存在显著差距。MCVT框架在智慧城市应用中具有巨大潜力，需要更接近真实世界的数据集来推动发展。

Method: 创建了RoundaboutHD数据集，包含40分钟由四台非重叠4K分辨率（15帧/秒）高清摄像头捕捉的视频素材。标注了512个独立车辆身份，提供丰富的跨摄像头关联数据。数据集涵盖了真实环形交叉路口的复杂挑战，如增加的遮挡和非线性运动。同时提供用于目标检测、单摄像头跟踪、图像车辆重识别等子集，并包含车辆模型和摄像头几何信息。

Result: 成功构建并发布了RoundaboutHD数据集及其评估代码。为车辆检测、单摄像头跟踪、图像车辆重识别和多摄像头跟踪任务提供了基线结果。

Conclusion: RoundaboutHD数据集通过提供高分辨率、复杂且贴近真实场景的数据，有效弥补了现有MCVT数据集的不足。该数据集及其相关信息和基线结果的公开可用性，将有力支持多摄像头车辆跟踪领域的研究与发展。

Abstract: The multi-camera vehicle tracking (MCVT) framework holds significant
potential for smart city applications, including anomaly detection, traffic
density estimation, and suspect vehicle tracking. However, current publicly
available datasets exhibit limitations, such as overly simplistic scenarios,
low-resolution footage, and insufficiently diverse conditions, creating a
considerable gap between academic research and real-world scenario. To fill
this gap, we introduce RoundaboutHD, a comprehensive, high-resolution
multi-camera vehicle tracking benchmark dataset specifically designed to
represent real-world roundabout scenarios. RoundaboutHD provides a total of 40
minutes of labelled video footage captured by four non-overlapping,
high-resolution (4K resolution, 15 fps) cameras. In total, 512 unique vehicle
identities are annotated across different camera views, offering rich
cross-camera association data. RoundaboutHD offers temporal consistency video
footage and enhanced challenges, including increased occlusions and nonlinear
movement inside the roundabout. In addition to the full MCVT dataset, several
subsets are also available for object detection, single camera tracking, and
image-based vehicle re-identification (ReID) tasks. Vehicle model information
and camera modelling/ geometry information are also included to support further
analysis. We provide baseline results for vehicle detection, single-camera
tracking, image-based vehicle re-identification, and multi-camera tracking. The
dataset and the evaluation code are publicly available at:
https://github.com/siri-rouser/RoundaboutHD.git

</details>


### [117] [Ensemble of Weak Spectral Total Variation Learners: a PET-CT Case Study](https://arxiv.org/abs/2507.08735)
*Anna Rosenberg,John Kennedy,Zohar Keidar,Yehoshua Y. Zeevi,Guy Gilboa*

Main category: cs.CV

TL;DR: 该研究提出了一种基于谱全变分（STV）特征的集成学习方法，旨在解决计算机视觉中训练数据不足的问题，并在医学图像预测任务中展现出超越深度学习和放射组学的卓越性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习在解决计算机视觉问题时，普遍面临训练数据不足的挑战。

Method: 提出使用基于谱全变分（STV）特征的弱学习器集成方法。STV特征能够表征不同尺度的纹理，且经验证在二维情况下具有低相关性，这使其非常适合集成学习理论的需求。

Result: 在通过CT数据预测PET高摄取（骨骼转移）的医学成像问题中，所提出的STV学习器表现最佳（AUC=0.87），优于深度学习方法（AUC=0.75）和放射组学特征（AUC=0.79）。研究还发现CT图像中的精细STV尺度对PET高摄取具有显著指示作用。

Conclusion: 基于STV特征的集成学习方法为计算机视觉问题提供了一个有效且高性能的解决方案，特别适用于数据量有限的实际医学图像分析场景。

Abstract: Solving computer vision problems through machine learning, one often
encounters lack of sufficient training data. To mitigate this we propose the
use of ensembles of weak learners based on spectral total-variation (STV)
features (Gilboa 2014). The features are related to nonlinear eigenfunctions of
the total-variation subgradient and can characterize well textures at various
scales. It was shown (Burger et-al 2016) that, in the one-dimensional case,
orthogonal features are generated, whereas in two-dimensions the features are
empirically lowly correlated. Ensemble learning theory advocates the use of
lowly correlated weak learners. We thus propose here to design ensembles using
learners based on STV features. To show the effectiveness of this paradigm we
examine a hard real-world medical imaging problem: the predictive value of
computed tomography (CT) data for high uptake in positron emission tomography
(PET) for patients suspected of skeletal metastases. The database consists of
457 scans with 1524 unique pairs of registered CT and PET slices. Our approach
is compared to deep-learning methods and to Radiomics features, showing STV
learners perform best (AUC=0.87), compared to neural nets (AUC=0.75) and
Radiomics (AUC=0.79). We observe that fine STV scales in CT images are
especially indicative for the presence of high uptake in PET.

</details>


### [118] [NeuralOS: Towards Simulating Operating Systems via Neural Generative Models](https://arxiv.org/abs/2507.08800)
*Luke Rivard,Sun Sun,Hongyu Guo,Wenhu Chen,Yuntian Deng*

Main category: cs.CV

TL;DR: NeuralOS是一个神经网络框架，通过直接预测屏幕帧来模拟操作系统图形用户界面（GUI），以响应鼠标移动、点击和键盘事件等用户输入。


<details>
  <summary>Details</summary>
Motivation: 旨在为未来人机交互系统创建完全自适应、生成式的神经接口，以改进人机交互体验。

Method: NeuralOS结合了跟踪计算机状态的循环神经网络（RNN）和生成屏幕图像的基于扩散的神经渲染器。该模型在大规模Ubuntu XFCE录制数据集上进行训练，该数据集包含随机生成和AI代理产生的真实交互。

Result: 实验表明，NeuralOS成功渲染了逼真的GUI序列，准确捕捉了鼠标交互，并可靠地预测了应用程序启动等状态转换。然而，精确建模细粒度的键盘交互仍然具有挑战性。

Conclusion: NeuralOS为创建未来完全自适应、生成式的神经接口迈出了重要一步，尽管在键盘交互方面仍有挑战。

Abstract: We introduce NeuralOS, a neural framework that simulates graphical user
interfaces (GUIs) of operating systems by directly predicting screen frames in
response to user inputs such as mouse movements, clicks, and keyboard events.
NeuralOS combines a recurrent neural network (RNN), which tracks computer
state, with a diffusion-based neural renderer that generates screen images. The
model is trained on a large-scale dataset of Ubuntu XFCE recordings, which
include both randomly generated interactions and realistic interactions
produced by AI agents. Experiments show that NeuralOS successfully renders
realistic GUI sequences, accurately captures mouse interactions, and reliably
predicts state transitions like application launches. Although modeling
fine-grained keyboard interactions precisely remains challenging, NeuralOS
offers a step toward creating fully adaptive, generative neural interfaces for
future human-computer interaction systems.

</details>


### [119] [HieraRS: A Hierarchical Segmentation Paradigm for Remote Sensing Enabling Multi-Granularity Interpretation and Cross-Domain Transfer](https://arxiv.org/abs/2507.08741)
*Tianlong Ai,Tianzhu Liu,Haochen Jiang,Yanfeng Gu*

Main category: cs.CV

TL;DR: HieraRS是一种新颖的分层解释范式，旨在解决遥感影像LCLU分级分类中多粒度预测和异构层级跨域迁移的挑战，提出了相关机制并构建了新数据集。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在遥感影像分级土地覆盖与利用（LCLU）分类中存在两大挑战：1) 多采用扁平分类范式，难以进行端到端的多粒度分层预测；2) 跨域研究对异构层级（如LCLU到作物分类）的迁移关注不足。这些限制阻碍了LCLU模型的灵活性和泛化能力。

Method: 本文提出HieraRS，一种新颖的分层解释范式。该范式包含：1) 双向分层一致性约束机制（BHCCM），可集成到主流扁平分类模型以生成分层预测；2) TransLU，一个双分支跨域迁移框架，由跨域知识共享（CDKS）和跨域语义对齐（CDSA）组成，支持动态类别扩展和适应异构层级。此外，本文还构建了大规模多模态分层土地利用像素级标注数据集MM-5B。

Result: HieraRS范式能够实现多粒度预测，并支持LCLU模型高效迁移到具有异构树状层级的跨域任务。BHCCM机制可生成分层预测并提高语义一致性和分类精度。TransLU框架支持动态类别扩展，并促进LCLU模型有效适应异构层级。

Conclusion: HieraRS作为创新的分层解释范式，通过BHCCM和TransLU框架有效解决了遥感影像LCLU分类中多粒度预测和异构层级跨域迁移的挑战，并构建发布了MM-5B数据集，显著提升了LCLU模型在实际应用中的灵活性和泛化能力。

Abstract: Hierarchical land cover and land use (LCLU) classification aims to assign
pixel-wise labels with multiple levels of semantic granularity to remote
sensing (RS) imagery. However, existing deep learning-based methods face two
major challenges: 1) They predominantly adopt a flat classification paradigm,
which limits their ability to generate end-to-end multi-granularity
hierarchical predictions aligned with tree-structured hierarchies used in
practice. 2) Most cross-domain studies focus on performance degradation caused
by sensor or scene variations, with limited attention to transferring LCLU
models to cross-domain tasks with heterogeneous hierarchies (e.g., LCLU to crop
classification). These limitations hinder the flexibility and generalization of
LCLU models in practical applications. To address these challenges, we propose
HieraRS, a novel hierarchical interpretation paradigm that enables
multi-granularity predictions and supports the efficient transfer of LCLU
models to cross-domain tasks with heterogeneous tree-structured hierarchies. We
introduce the Bidirectional Hierarchical Consistency Constraint Mechanism
(BHCCM), which can be seamlessly integrated into mainstream flat classification
models to generate hierarchical predictions, while improving both semantic
consistency and classification accuracy. Furthermore, we present TransLU, a
dual-branch cross-domain transfer framework comprising two key components:
Cross-Domain Knowledge Sharing (CDKS) and Cross-Domain Semantic Alignment
(CDSA). TransLU supports dynamic category expansion and facilitates the
effective adaptation of LCLU models to heterogeneous hierarchies. In addition,
we construct MM-5B, a large-scale multi-modal hierarchical land use dataset
featuring pixel-wise annotations. The code and MM-5B dataset will be released
at: https://github.com/AI-Tianlong/HieraRS.

</details>


### [120] [Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane Geometry Detection](https://arxiv.org/abs/2507.08743)
*Rei Tamaru,Pei Li,Bin Ran*

Main category: cs.CV

TL;DR: 本文提出Geo-ORBIT框架，通过结合实时车道检测、数字孪生同步和联邦元学习，解决交通数字孪生在动态路网几何感知方面的可扩展性、隐私和效率挑战。其核心是FedMeta-GeoLane模型，在实验中展现出更低的几何误差、更强的泛化能力和显著降低的通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有交通数字孪生（DT）在动态路网几何感知方面依赖静态地图或昂贵传感器，限制了可扩展性和适应性。此外，大规模DT在数据隐私、通信和计算效率方面面临挑战。

Method: 引入Geo-ORBIT统一框架，结合实时车道检测、DT同步和联邦元学习。核心是GeoLane轻量级车道检测模型，利用路边摄像头从车辆轨迹数据学习车道几何。通过Meta-GeoLane模型个性化检测参数，并采用FedMeta-GeoLane联邦学习策略实现跨部署的可伸缩和隐私保护适应。系统与CARLA和SUMO集成，创建高保真DT模拟高速公路场景和交通流。

Result: FedMeta-GeoLane在多样化的城市场景中，相比基线和元学习方法表现更优，实现了更低的几何误差、对未见地点的更强泛化能力，并大幅降低了通信开销。

Conclusion: 该工作为数字孪生中灵活、上下文感知的交通基础设施建模奠定了基础。

Abstract: Digital Twins (DT) have the potential to transform traffic management and
operations by creating dynamic, virtual representations of transportation
systems that sense conditions, analyze operations, and support decision-making.
A key component for DT of the transportation system is dynamic roadway geometry
sensing. However, existing approaches often rely on static maps or costly
sensors, limiting scalability and adaptability. Additionally, large-scale DTs
that collect and analyze data from multiple sources face challenges in privacy,
communication, and computational efficiency. To address these challenges, we
introduce Geo-ORBIT (Geometrical Operational Roadway Blueprint with Integrated
Twin), a unified framework that combines real-time lane detection, DT
synchronization, and federated meta-learning. At the core of Geo-ORBIT is
GeoLane, a lightweight lane detection model that learns lane geometries from
vehicle trajectory data using roadside cameras. We extend this model through
Meta-GeoLane, which learns to personalize detection parameters for local
entities, and FedMeta-GeoLane, a federated learning strategy that ensures
scalable and privacy-preserving adaptation across roadside deployments. Our
system is integrated with CARLA and SUMO to create a high-fidelity DT that
renders highway scenarios and captures traffic flows in real-time. Extensive
experiments across diverse urban scenes show that FedMeta-GeoLane consistently
outperforms baseline and meta-learning approaches, achieving lower geometric
error and stronger generalization to unseen locations while drastically
reducing communication overhead. This work lays the foundation for flexible,
context-aware infrastructure modeling in DTs. The framework is publicly
available at https://github.com/raynbowy23/FedMeta-GeoLane.git.

</details>


### [121] [Compress Any Segment Anything Model (SAM)](https://arxiv.org/abs/2507.08765)
*Juntong Fan,Zhiwei Hao,Jianqiang Shen,Shang-Ling Jui,Yi Zhang,Jing-Xiao Liao,Feng-Lei Fan*

Main category: cs.CV

TL;DR: 本文提出Birkhoff，一种新型无数据压缩算法，能高效压缩大型SAM模型及其变体，显著减小模型尺寸并加速推理，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: Segment Anything Model (SAM)及其变体在零样本分割方面表现优异，但其庞大的模型尺寸限制了在医疗、智能制造等实际场景中的广泛部署，因此亟需高效的压缩方法。

Method: 本文提出了Birkhoff，一种新颖的无数据压缩算法。它引入了Hyper-Compression，其核心原理是将高维参数向量转换为低维标量。此外，Birkhoff设计了HyperLinear专用线性层操作符，将解压缩与矩阵乘法融合，以显著加速压缩后SAM模型的推理。

Result: 在COCO、LVIS和SA-1B数据集上对18个SAM模型进行了广泛实验。Birkhoff在压缩时间、压缩比、压缩后性能和推理速度方面表现出持续且有竞争力的性能。例如，在SAM2-B上实现了5.17倍的压缩比，性能下降不到1%，且无需使用任何微调数据。所有模型的压缩都在60秒内完成。

Conclusion: Birkhoff是一种高效且通用的无数据压缩算法，能有效解决SAM模型部署难题。它能在保证模型性能的同时，显著减小模型尺寸并加快推理速度，展现了出色的实用价值。

Abstract: Due to the excellent performance in yielding high-quality, zero-shot
segmentation, Segment Anything Model (SAM) and its variants have been widely
applied in diverse scenarios such as healthcare and intelligent manufacturing.
Therefore, effectively compressing SAMs has become an increasingly pressing
practical need. In this study, we propose Birkhoff, a novel data-free
compression algorithm for SAM and its variants. Unlike quantization, pruning,
distillation, and other compression methods, Birkhoff embodies versatility
across model types, agility in deployment, faithfulness to the original model,
and compactness in model size. Specifically, Birkhoff introduces a novel
compression algorithm: Hyper-Compression, whose core principle is to find a
dense trajectory to turn a high-dimensional parameter vector into a
low-dimensional scalar. Furthermore, Birkhoff designs a dedicated linear layer
operator, HyperLinear, to fuse decompression and matrix multiplication to
significantly accelerate inference of the compressed SAMs. Extensive
experiments on 18 SAMs in the COCO, LVIS, and SA-1B datasets show that Birkhoff
performs consistently and competitively in compression time, compression ratio,
post-compression performance, and inference speed. For example, Birkhoff can
achieve a compression ratio of 5.17x on SAM2-B, with less than 1% performance
drop without using any fine-tuning data. Moreover, the compression is finished
within 60 seconds for all models.

</details>


### [122] [A Hybrid Multi-Well Hopfield-CNN with Feature Extraction and K-Means for MNIST Classification](https://arxiv.org/abs/2507.08766)
*Ahmed Farooq*

Main category: cs.CV

TL;DR: 本研究提出一种结合卷积神经网络（CNN）和多阱霍普菲尔德网络（Hopfield network）的混合模型，用于MNIST手写数字分类，在保持高准确率的同时提供了可解释性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决手写数字分类中存在的类内多样性（如不同书写风格）问题，并提供一个基于能量函数的可解释分类框架。

Method: 该模型首先利用CNN从输入图像中提取高维特征，随后通过k-means聚类将这些特征转换为类特定原型。这些原型在多阱能量景观中充当吸引子，霍普菲尔德网络通过最小化平衡特征相似性和类别分配的能量函数来进行分类。研究中还对CNN架构和阱的数量进行了系统优化。

Result: 该模型在10,000张MNIST图像上实现了99.2%的高测试准确率，证明了其在图像分类任务中的有效性。

Conclusion: 研究结果强调了深度特征提取和充分的原型覆盖在实现高性能方面的重要性，并指出该模型在模式识别领域具有更广泛的应用潜力。

Abstract: This study presents a hybrid model for classifying handwritten digits in the
MNIST dataset, combining convolutional neural networks (CNNs) with a multi-well
Hopfield network. The approach employs a CNN to extract high-dimensional
features from input images, which are then clustered into class-specific
prototypes using k-means clustering. These prototypes serve as attractors in a
multi-well energy landscape, where a Hopfield network performs classification
by minimizing an energy function that balances feature similarity and class
assignment.The model's design enables robust handling of intraclass
variability, such as diverse handwriting styles, while providing an
interpretable framework through its energy-based decision process. Through
systematic optimization of the CNN architecture and the number of wells, the
model achieves a high test accuracy of 99.2% on 10,000 MNIST images,
demonstrating its effectiveness for image classification tasks. The findings
highlight the critical role of deep feature extraction and sufficient prototype
coverage in achieving high performance, with potential for broader applications
in pattern recognition.

</details>


### [123] [From One to More: Contextual Part Latents for 3D Generation](https://arxiv.org/abs/2507.08772)
*Shaocong Dong,Lihe Ding,Xiao Chen,Yaokun Li,Yuxin Wang,Yucheng Wang,Qi Wang,Jaehyeok Kim,Chenjian Gao,Zhanpeng Huang,Zibin Wang,Tianfan Xue,Dan Xu*

Main category: cs.CV

TL;DR: 提出CoPart框架，通过将3D对象分解为上下文相关的部件潜在表示，解决了现有3D生成模型在处理复杂多部件几何和精细控制方面的不足，实现了卓越的部件级编辑、关节对象生成和场景组合能力。


<details>
  <summary>Details</summary>
Motivation: 尽管3D生成技术已从多视图2D渲染转向3D原生潜在扩散框架，但仍存在三大局限：1) 单一潜在表示无法捕捉复杂多部件几何，导致细节退化；2) 整体潜在编码忽略部件独立性和相互关系，不利于组合设计；3) 全局条件机制缺乏精细控制能力。

Method: 受人类3D设计工作流程启发，提出CoPart——一个部件感知扩散框架，将3D对象分解为上下文相关的部件潜在表示，以实现连贯的多部件生成。该方法通过部件分解降低编码复杂度，支持显式部件关系建模和部件级条件控制。此外，开发了一种相互引导策略来微调预训练扩散模型，以联合去噪部件潜在表示。为支持大规模训练，构建了Partverse数据集，该数据集通过自动化网格分割和人工验证标注从Objaverse中提取3D部件。

Result: 广泛的实验证明，CoPart在部件级编辑、关节对象生成和场景组合方面展现出卓越的性能和前所未有的可控性。

Conclusion: CoPart成功解决了复杂3D多部件生成中的细节退化、部件关系缺失和控制受限问题，通过引入部件感知扩散框架和创建新的Partverse数据集，显著提升了3D生成模型的能力和可控性。

Abstract: Recent advances in 3D generation have transitioned from multi-view 2D
rendering approaches to 3D-native latent diffusion frameworks that exploit
geometric priors in ground truth data. Despite progress, three key limitations
persist: (1) Single-latent representations fail to capture complex multi-part
geometries, causing detail degradation; (2) Holistic latent coding neglects
part independence and interrelationships critical for compositional design; (3)
Global conditioning mechanisms lack fine-grained controllability. Inspired by
human 3D design workflows, we propose CoPart - a part-aware diffusion framework
that decomposes 3D objects into contextual part latents for coherent multi-part
generation. This paradigm offers three advantages: i) Reduces encoding
complexity through part decomposition; ii) Enables explicit part relationship
modeling; iii) Supports part-level conditioning. We further develop a mutual
guidance strategy to fine-tune pre-trained diffusion models for joint part
latent denoising, ensuring both geometric coherence and foundation model
priors. To enable large-scale training, we construct Partverse - a novel 3D
part dataset derived from Objaverse through automated mesh segmentation and
human-verified annotations. Extensive experiments demonstrate CoPart's superior
capabilities in part-level editing, articulated object generation, and scene
composition with unprecedented controllability.

</details>


### [124] [CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive Neural Rendering](https://arxiv.org/abs/2507.08776)
*Zhengqing Wang,Yuefan Wu,Jiacheng Chen,Fuyang Zhang,Yasutaka Furukawa*

Main category: cs.CV

TL;DR: 提出一种基于“压缩光场令牌（CLiFTs）”的神经渲染方法，能在保留丰富场景信息的同时，实现计算高效、数据量可变且高质量的新颖视图合成。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法在计算效率和适应不同计算预算方面存在局限。本文旨在通过压缩令牌实现高效渲染，并提供灵活的令牌数量调整能力，以适应不同计算需求和实现质量与速度的权衡。

Method: 该方法通过以下步骤构建和使用CLiFTs：1. 多视图编码器将图像与相机姿态令牌化。2. 潜在空间K-means算法选择一组简化的射线作为聚类中心。3. 多视图“冷凝器”将所有令牌信息压缩到这些中心令牌中，形成CLiFTs。在测试时，系统根据目标视图和计算预算（即CLiFTs数量），收集指定数量的附近令牌，并使用计算自适应渲染器合成新颖视图。

Result: 在RealEstate10K和DL3DV数据集上的广泛实验表明，该方法在实现显著数据缩减的同时，保持了可比的渲染质量，并获得了最高的整体渲染分数。此外，它能有效提供数据大小、渲染质量和渲染速度之间的权衡。

Conclusion: CLiFTs方法在神经渲染领域中有效平衡了数据压缩、渲染质量和计算效率，为新颖视图合成提供了一种灵活且高性能的解决方案。

Abstract: This paper proposes a neural rendering approach that represents a scene as
"compressed light-field tokens (CLiFTs)", retaining rich appearance and
geometric information of a scene. CLiFT enables compute-efficient rendering by
compressed tokens, while being capable of changing the number of tokens to
represent a scene or render a novel view with one trained network. Concretely,
given a set of images, multi-view encoder tokenizes the images with the camera
poses. Latent-space K-means selects a reduced set of rays as cluster centroids
using the tokens. The multi-view ``condenser'' compresses the information of
all the tokens into the centroid tokens to construct CLiFTs. At test time,
given a target view and a compute budget (i.e., the number of CLiFTs), the
system collects the specified number of nearby tokens and synthesizes a novel
view using a compute-adaptive renderer. Extensive experiments on RealEstate10K
and DL3DV datasets quantitatively and qualitatively validate our approach,
achieving significant data reduction with comparable rendering quality and the
highest overall rendering score, while providing trade-offs of data size,
rendering quality, and rendering speed.

</details>


### [125] [Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective](https://arxiv.org/abs/2507.08801)
*Hangjie Yuan,Weihua Chen,Jun Cen,Hu Yu,Jingyun Liang,Shuning Chang,Zhihui Lin,Tao Feng,Pengwei Liu,Jiazheng Xing,Hao Luo,Jiasheng Tang,Fan Wang,Yi Yang*

Main category: cs.CV

TL;DR: Lumos-1是一个遵循LLM架构的自回归视频生成器，通过提出MM-RoPE和AR-DF等创新方法，解决了现有自回归视频生成器的架构偏离、外部编码器依赖和高延迟问题，并在有限资源下达到了与顶尖模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视频生成器存在以下问题：1) 偏离标准LLM架构；2) 依赖笨重的外部文本编码器；3) 下一个token解码导致过高的延迟。因此，需要开发一种能够保留LLM架构且高效的自回归视频生成器。

Method: 1. 引入Lumos-1：一个最小化修改LLM架构的自回归视频生成器。2. 提出MM-RoPE：为注入时空关联，解决3D RoPE频率频谱不平衡问题，MM-RoPE在保留原始文本RoPE的同时，提供全面的频率谱和缩放的3D位置以建模多模态时空数据。3. 采用token依赖策略：遵循帧内双向性和帧间时间因果性。4. 提出Autoregressive Discrete Diffusion Forcing (AR-DF)：解决由空间信息冗余引起的逐帧损失不平衡问题，通过在训练期间引入时间管掩码和兼容的推理时间掩码策略。5. 使用内存高效的训练技术，仅用48个GPU进行预训练。

Result: Lumos-1在GenEval上实现了与EMU3相当的性能，在VBench-I2V上与COSMOS-Video2World相当，在VBench-T2V上与OpenSoraPlan相当。

Conclusion: Lumos-1证明了在保持标准LLM架构的基础上，通过引入MM-RoPE和AR-DF等创新机制，可以有效地解决自回归视频生成中的关键挑战，并在资源相对有限的情况下达到与现有先进模型相媲美的性能，展示了LLM架构在视频生成领域的巨大潜力。

Abstract: Autoregressive large language models (LLMs) have unified a vast range of
language tasks, inspiring preliminary efforts in autoregressive video
generation. Existing autoregressive video generators either diverge from
standard LLM architectures, depend on bulky external text encoders, or incur
prohibitive latency due to next-token decoding. In this paper, we introduce
Lumos-1, an autoregressive video generator that retains the LLM architecture
with minimal architectural modifications. To inject spatiotemporal correlations
in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its
imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE
scheme that preserves the original textual RoPE while providing comprehensive
frequency spectra and scaled 3D positions for modeling multimodal
spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy
that obeys intra-frame bidirectionality and inter-frame temporal causality.
Based on this dependency strategy, we identify the issue of frame-wise loss
imbalance caused by spatial information redundancy and solve it by proposing
Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal
tube masking during training with a compatible inference-time masking policy to
avoid quality degradation. By using memory-efficient training techniques, we
pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on
GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code
and models are available at https://github.com/alibaba-damo-academy/Lumos.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [126] [Human Creativity and AI](https://arxiv.org/abs/2507.08001)
*Shengyi Xie*

Main category: cs.AI

TL;DR: 本文探讨在人工智能（AI）发展背景下，创造力哲学的重新解读，并审视AI是否能展现创造力，通过回顾心理学、认知神经科学和创造力哲学领域的当代研究。


<details>
  <summary>Details</summary>
Motivation: 随着科技进步和人工智能技术发展，创造力哲学被显著重新解读。本文旨在解决核心问题：人工智能能否展现创造力？

Method: 通过回顾创造力哲学的历史视角，探讨心理学进展对创造力研究的影响，分析创造力的各种定义，并审视自然主义和认知神经科学对创造力概念的反应。研究范围涵盖心理学、认知神经科学和创造力哲学领域的当代研究。

Result: 摘要中未明确指出研究的具体结果，仅描述了研究的范围、方法和目标。

Conclusion: 摘要中未提供研究的最终结论。

Abstract: With the advancement of science and technology, the philosophy of creativity
has undergone significant reinterpretation. This paper investigates
contemporary research in the fields of psychology, cognitive neuroscience, and
the philosophy of creativity, particularly in the context of the development of
artificial intelligence (AI) techniques. It aims to address the central
question: Can AI exhibit creativity? The paper reviews the historical
perspectives on the philosophy of creativity and explores the influence of
psychological advancements on the study of creativity. Furthermore, it analyzes
various definitions of creativity and examines the responses of naturalism and
cognitive neuroscience to the concept of creativity.

</details>


### [127] [TableReasoner: Advancing Table Reasoning Framework with Large Language Models](https://arxiv.org/abs/2507.08046)
*Sishi Xiong,Dakai Wang,Yu Zhao,Jie Zhang,Changzai Pan,Haowei He,Xiangyu Li,Wenhan Chang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 该论文提出了一种名为TableReasoner的系统，用于表格问答（TQA），通过结合大型语言模型（LLM）和编程方法，有效解决了真实表格数据中的复杂挑战，并在SemEval-2025竞赛中取得领先。


<details>
  <summary>Details</summary>
Motivation: 表格问答（TQA）任务面临真实表格数据的固有挑战，包括数据量大、列语义不完整以及实体歧义性。

Method: 研究者提出一个名为TableReasoner的框架，该框架由大型语言模型（LLM）驱动并基于编程。它通过结合结构化和语义化表示的模式来建模表格，并设计了多步模式链接计划以生成仅保留查询相关信息的聚焦表格模式。此外，推理工作流被整合到一个迭代思考架构中，允许思考、推理和反思的增量循环。

Result: 所开发的系统在SemEval-2025 Task 8的两个子任务中均取得了第一名。

Conclusion: TableReasoner通过其创新的LLM驱动和编程方法，以及聚焦模式和迭代思考架构，成功克服了TQA任务中的挑战，并在基准测试中展现出卓越的性能。

Abstract: The paper presents our system developed for table question answering (TQA).
TQA tasks face challenges due to the characteristics of real-world tabular
data, such as large size, incomplete column semantics, and entity ambiguity. To
address these issues, we propose a large language model (LLM)-powered and
programming-based table reasoning framework, named TableReasoner. It models a
table using the schema that combines structural and semantic representations,
enabling holistic understanding and efficient processing of large tables. We
design a multi-step schema linking plan to derive a focused table schema that
retains only query-relevant information, eliminating ambiguity and alleviating
hallucinations. This focused table schema provides precise and sufficient table
details for query refinement and programming. Furthermore, we integrate the
reasoning workflow into an iterative thinking architecture, allowing
incremental cycles of thinking, reasoning and reflection. Our system achieves
first place in both subtasks of SemEval-2025 Task 8.

</details>


### [128] [A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking](https://arxiv.org/abs/2507.08207)
*Zhengye Han,Quanyan Zhu*

Main category: cs.AI

TL;DR: 本文提出了一个动态Stackelberg博弈框架来建模LLM越狱攻击中的攻防互动，并引入“Purple Agent”智能体进行主动防御。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在关键应用中的广泛部署，越狱（即规避安全机制）成为了一个显著的安全隐患。

Method: 本研究提出一个动态Stackelberg博弈框架，将LLM的提示-响应动态建模为序列式扩展型博弈。其中防御方作为领导者，预测攻击方的最优响应来制定策略。此外，还提出了一个新型智能AI解决方案“Purple Agent”，该代理利用快速探索随机树（RRT）整合对抗性探索和防御策略，主动模拟潜在攻击轨迹并进行干预。

Result: 该方法提供了一种分析对抗动态的原则性方法。

Conclusion: 本研究为缓解LLM越狱风险奠定了基础。

Abstract: As large language models (LLMs) are increasingly deployed in critical
applications, the challenge of jailbreaking, where adversaries manipulate the
models to bypass safety mechanisms, has become a significant concern. This
paper presents a dynamic Stackelberg game framework to model the interactions
between attackers and defenders in the context of LLM jailbreaking. The
framework treats the prompt-response dynamics as a sequential extensive-form
game, where the defender, as the leader, commits to a strategy while
anticipating the attacker's optimal responses. We propose a novel agentic AI
solution, the "Purple Agent," which integrates adversarial exploration and
defensive strategies using Rapidly-exploring Random Trees (RRT). The Purple
Agent actively simulates potential attack trajectories and intervenes
proactively to prevent harmful outputs. This approach offers a principled
method for analyzing adversarial dynamics and provides a foundation for
mitigating the risk of jailbreaking.

</details>


### [129] [Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions](https://arxiv.org/abs/2507.08208)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: 引入LLM-Nash框架，一个博弈论模型，用于研究LLM智能体如何通过选择推理提示进行决策，并捕捉有限理性。


<details>
  <summary>Details</summary>
Motivation: 传统博弈论假设完全理性，而本研究旨在通过显式建模LLM的推理过程，捕捉智能体的有限理性，从而更好地理解LLM驱动系统中的战略互动。

Method: 提出LLM-Nash框架，这是一个博弈论模型。在该模型中，智能体选择推理提示来指导基于大型语言模型（LLM）的决策。均衡在提示空间中定义，行动则表现为LLM推理的行为输出。

Result: 该框架能够研究认知约束、思维表达和认知学习。通过示例表明，推理均衡可能与经典的纳什均衡结果不同。

Conclusion: LLM-Nash框架为LLM驱动系统中的战略互动提供了一个新的基础，特别是通过考虑有限理性和明确的推理过程。

Abstract: We introduce the LLM-Nash framework, a game-theoretic model where agents
select reasoning prompts to guide decision-making via Large Language Models
(LLMs). Unlike classical games that assume utility-maximizing agents with full
rationality, this framework captures bounded rationality by modeling the
reasoning process explicitly. Equilibrium is defined over the prompt space,
with actions emerging as the behavioral output of LLM inference. This approach
enables the study of cognitive constraints, mindset expressiveness, and
epistemic learning. Through illustrative examples, we show how reasoning
equilibria can diverge from classical Nash outcomes, offering a new foundation
for strategic interaction in LLM-enabled systems.

</details>


### [130] [From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration](https://arxiv.org/abs/2507.08210)
*Fryderyk Mantiuk,Hanqi Zhou,Charley M. Wu*

Main category: cs.AI

TL;DR: 本文研究智能体如何在探索未知（好奇心）和掌握环境（能力）之间取得平衡，通过比较两种强化学习模型，揭示内部表征在这一权衡中的作用，并为自适应探索提供见解。


<details>
  <summary>Details</summary>
Motivation: 探究智能体在探索世界时如何平衡好奇心（寻求知识）与能力（掌握和控制环境），并思考演化的内部表征如何介导好奇心（新奇性或信息增益）与能力（赋能）之间的权衡。

Method: 结合认知理论与强化学习，比较两种基于模型的智能体：使用手工设计状态抽象的Tabular代理和学习内部世界模型的Dreamer代理。

Result: Tabular代理显示好奇心和能力以不同的模式引导探索，但同时优先考虑两者可改善探索。Dreamer代理揭示了探索与表征学习之间的双向互动，这与好奇心和能力的发展共演相呼应。

Conclusion: 研究将适应性探索形式化为追求未知与可控之间的平衡，为认知理论和高效强化学习提供了新的见解。

Abstract: What drives an agent to explore the world while also maintaining control over
the environment? From a child at play to scientists in the lab, intelligent
agents must balance curiosity (the drive to seek knowledge) with competence
(the drive to master and control the environment). Bridging cognitive theories
of intrinsic motivation with reinforcement learning, we ask how evolving
internal representations mediate the trade-off between curiosity (novelty or
information gain) and competence (empowerment). We compare two model-based
agents using handcrafted state abstractions (Tabular) or learning an internal
world model (Dreamer). The Tabular agent shows curiosity and competence guide
exploration in distinct patterns, while prioritizing both improves exploration.
The Dreamer agent reveals a two-way interaction between exploration and
representation learning, mirroring the developmental co-evolution of curiosity
and competence. Our findings formalize adaptive exploration as a balance
between pursuing the unknown and the controllable, offering insights for
cognitive theories and efficient reinforcement learning.

</details>


### [131] [Grounding Methods for Neural-Symbolic AI](https://arxiv.org/abs/2507.08216)
*Rodrigo Castellano Ontiveros,Francesco Giannini,Marco Gori,Giuseppe Marra,Michelangelo Diligenti*

Main category: cs.AI

TL;DR: 本文提出一种受多跳符号推理启发的参数化逻辑接地方法家族，旨在平衡神经符号（NeSy）方法中表达能力与可伸缩性之间的矛盾，并表明接地准则的选择至关重要。


<details>
  <summary>Details</summary>
Motivation: 神经符号（NeSy）方法中的逻辑接地过程面临两难：穷举式接地导致组合爆炸，限制可伸缩性；而启发式接地缺乏理论依据，可能丢失信息。因此，需要一种能够平衡表达能力和计算效率的接地方法。

Method: 本文提出了一种参数化的接地方法家族，该家族泛化了经典的反向链式推理（Backward Chaining），并受多跳符号推理启发。通过调节参数，可在表达能力和推理器的可伸缩性之间进行权衡控制，且能将现有接地方法作为特例。

Result: 该方法家族能够有效控制推理器的表达能力和可伸缩性。实验结果表明，接地准则的选择与神经符号（NeSy）方法本身同等重要，对整体性能影响显著。

Conclusion: 通过引入可控的参数化接地方法，研究为优化神经符号（NeSy）系统的性能提供了新途径，并强调了选择恰当的接地策略在实际应用中的关键作用。

Abstract: A large class of Neural-Symbolic (NeSy) methods employs a machine learner to
process the input entities, while relying on a reasoner based on First-Order
Logic to represent and process more complex relationships among the entities. A
fundamental role for these methods is played by the process of logic grounding,
which determines the relevant substitutions for the logic rules using a
(sub)set of entities. Some NeSy methods use an exhaustive derivation of all
possible substitutions, preserving the full expressive power of the logic
knowledge. This leads to a combinatorial explosion in the number of ground
formulas to consider and, therefore, strongly limits their scalability. Other
methods rely on heuristic-based selective derivations, which are generally more
computationally efficient, but lack a justification and provide no guarantees
of preserving the information provided to and returned by the reasoner. Taking
inspiration from multi-hop symbolic reasoning, this paper proposes a
parametrized family of grounding methods generalizing classic Backward
Chaining. Different selections within this family allow us to obtain commonly
employed grounding methods as special cases, and to control the trade-off
between expressiveness and scalability of the reasoner. The experimental
results show that the selection of the grounding criterion is often as
important as the NeSy method itself.

</details>


### [132] [Quantum Federated Learning for Multimodal Data: A Modality-Agnostic Approach](https://arxiv.org/abs/2507.08217)
*Atit Pokharel,Ratun Rahman,Thomas Morris,Dinh C. Nguyen*

Main category: cs.AI

TL;DR: 本文首次提出一种新颖的多模态量子联邦学习（QFL）方法，利用量子纠缠进行中间融合，并引入了“缺失模态无关（MMA）”机制以解决训练中模态缺失问题，仿真结果显示其在准确性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有量子联邦学习框架主要关注单模态系统，限制了其在涉及多模态的现实任务中的应用。此外，训练过程中模态缺失会导致模型性能下降，是多模态QFL的一个主要瓶颈。

Method: 提出了一种专为QFL环境定制的多模态方法，通过量子纠缠实现中间融合。为解决模态缺失问题，引入了“缺失模态无关（MMA）”机制，该机制能隔离未训练的量子电路，确保稳定训练而不会出现损坏状态。

Result: 仿真结果表明，所提出的多模态QFL方法结合MMA机制，在独立同分布（IID）数据上比现有最先进方法提高了6.84%的准确率，在非独立同分布（non-IID）数据上提高了7.25%的准确率。

Conclusion: 所提出的多模态量子联邦学习方法结合MMA机制，有效解决了现有QFL框架在多模态应用和模态缺失鲁棒性方面的局限性，显著提高了模型在复杂数据分布下的性能。

Abstract: Quantum federated learning (QFL) has been recently introduced to enable a
distributed privacy-preserving quantum machine learning (QML) model training
across quantum processors (clients). Despite recent research efforts, existing
QFL frameworks predominantly focus on unimodal systems, limiting their
applicability to real-world tasks that often naturally involve multiple
modalities. To fill this significant gap, we present for the first time a novel
multimodal approach specifically tailored for the QFL setting with the
intermediate fusion using quantum entanglement. Furthermore, to address a major
bottleneck in multimodal QFL, where the absence of certain modalities during
training can degrade model performance, we introduce a Missing Modality
Agnostic (MMA) mechanism that isolates untrained quantum circuits, ensuring
stable training without corrupted states. Simulation results demonstrate that
the proposed multimodal QFL method with MMA yields an improvement in accuracy
of 6.84% in independent and identically distributed (IID) and 7.25% in non-IID
data distributions compared to the state-of-the-art methods.

</details>


### [133] [Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm](https://arxiv.org/abs/2507.08249)
*Bill Marino,Ari Juels*

Main category: cs.AI

TL;DR: 本文认为，赋予AI智能体加密货币和智能合约权限可能带来严重的AI危害新向量。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体与加密货币及智能合约结合的兴趣日益增长，作者旨在警示这种结合可能引发的AI危害新向量。

Method: 通过分析加密货币和智能合约的独特属性，论文详细阐述了这些可能导致AI危害的新向量。

Result: 识别并详细阐述了将加密货币和智能合约赋予AI智能体可能导致的新型AI危害向量。

Conclusion: 呼吁进行更多技术研究，以预防和减轻这些危害，从而更安全地为AI智能体赋予相关能力。

Abstract: There is growing interest in giving AI agents access to cryptocurrencies as
well as to the smart contracts that transact them. But doing so, this position
paper argues, could lead to formidable new vectors of AI harm. To support this
argument, we first examine the unique properties of cryptocurrencies and smart
contracts that could lead to these new vectors of harm. Next, we describe each
of these new vectors of harm in detail. Finally, we conclude with a call for
more technical research aimed at preventing and mitigating these harms and,
thereby making it safer to endow AI agents with cryptocurrencies and smart
contracts.

</details>


### [134] [Abductive Computational Systems: Creative Abduction and Future Directions](https://arxiv.org/abs/2507.08264)
*Abhinav Sood,Kazjon Grace,Stephen Wan,Cecile Paris*

Main category: cs.AI

TL;DR: 该论文分析了溯因推理在理论和计算系统中的应用，发现其在生成创造性假设方面存在不足，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 溯因推理在科学、设计和艺术等不同领域的理解存在差异，且现有理论和计算系统在处理创造性假设生成方面存在局限。

Method: 本文回顾了认识论、科学和设计领域对溯因推理的讨论，并分析了各种计算系统如何应用溯因推理。同时，论文将溯因计算系统分解为多个组件进行分析。

Result: 分析表明，无论是理论层面的阐述还是计算层面的实现，溯因推理都未能充分解决生成创造性假设的问题。理论框架缺乏生成创造性溯因假设的直接模型，而计算系统大多只实现了三段论形式的溯因推理。

Conclusion: 目前的溯因推理理论和计算实现未能有效生成创造性假设。论文通过组件分解，为未来在计算系统中提升创造性溯因推理能力指明了具体的研发方向。

Abstract: Abductive reasoning, reasoning for inferring explanations for observations,
is often mentioned in scientific, design-related and artistic contexts, but its
understanding varies across these domains. This paper reviews how abductive
reasoning is discussed in epistemology, science and design, and then analyses
how various computational systems use abductive reasoning. Our analysis shows
that neither theoretical accounts nor computational implementations of
abductive reasoning adequately address generating creative hypotheses.
Theoretical frameworks do not provide a straightforward model for generating
creative abductive hypotheses, computational systems largely implement
syllogistic forms of abductive reasoning. We break down abductive computational
systems into components and conclude by identifying specific directions for
future research that could advance the state of creative abductive reasoning in
computational systems.

</details>


### [135] [Agent Safety Alignment via Reinforcement Learning](https://arxiv.org/abs/2507.08270)
*Zeyang Sha,Hanling Tian,Zhuoer Xu,Shiwen Cui,Changhua Meng,Weiqiang Wang*

Main category: cs.AI

TL;DR: 针对带工具LLM代理的新安全风险（用户和工具引发），本文提出首个统一安全对齐框架，通过结构化推理和沙盒强化学习有效抵御威胁，同时保持实用性，为可信部署奠定基础。


<details>
  <summary>Details</summary>
Motivation: 自主LLM代理的工具使用能力引入了超越传统对话误用的新安全风险，包括用户发起的威胁（如对抗性提示）和工具发起的威胁（如受损工具的恶意输出）。需要一个统一框架来应对这些复杂威胁。

Method: 提出首个统一安全对齐框架，通过结构化推理和沙盒强化学习处理威胁。具体方法包括：引入用户提示和工具响应的三模态分类（良性、恶意、敏感）；定义策略驱动的决策模型；采用定制沙盒环境模拟真实工具执行并进行精细奖励塑形。在Agent SafetyBench、InjecAgent和BFCL等基准上进行评估。

Result: 研究表明，经过安全对齐的代理显著提升了对安全威胁的抵御能力，并在良性任务上保持了强大的实用性。结果证明安全性与有效性可以共同优化。

Conclusion: 本文提出的框架成功解决了带工具LLM代理的新型安全风险。研究结果为自主LLM代理的可靠部署奠定了基础，表明安全性与有效性可同时实现最佳化。

Abstract: The emergence of autonomous Large Language Model (LLM) agents capable of tool
usage has introduced new safety risks that go beyond traditional conversational
misuse. These agents, empowered to execute external functions, are vulnerable
to both user-initiated threats (e.g., adversarial prompts) and tool-initiated
threats (e.g., malicious outputs from compromised tools). In this paper, we
propose the first unified safety-alignment framework for tool-using agents,
enabling models to handle both channels of threat via structured reasoning and
sandboxed reinforcement learning. We introduce a tri-modal taxonomy, including
benign, malicious, and sensitive for both user prompts and tool responses, and
define a policy-driven decision model. Our framework employs a custom-designed
sandbox environment that simulates real-world tool execution and allows
fine-grained reward shaping. Through extensive evaluations on public and
self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we
demonstrate that our safety-aligned agents significantly improve resistance to
security threats while preserving strong utility on benign tasks. Our results
show that safety and effectiveness can be jointly optimized, laying the
groundwork for trustworthy deployment of autonomous LLM agents.

</details>


### [136] [M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning](https://arxiv.org/abs/2507.08306)
*Inclusion AI,:,Fudong Wang,Jiajia Liu,Jingdong Chen,Jun Zhou,Kaixiang Ji,Lixiang Ru,Qingpei Guo,Ruobing Zheng,Tianqi Li,Yi Yuan,Yifan Mao,Yuting Xiao,Ziping Ma*

Main category: cs.AI

TL;DR: 本文提出M2-Reasoning-7B模型，通过高质量数据和动态多任务训练，解决了多模态大语言模型在动态空间交互推理方面的不足，并在8个基准测试中达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在推理能力上有所提升，但它们在处理动态空间交互方面存在关键缺陷，而这对于实际应用至关重要，因此需要弥补这一不足。

Method: 引入M2-Reasoning-7B模型。其方法包含两项创新：1) 构建了一个新的数据管道，生成294.2K高质量数据样本（用于冷启动微调和RLVR），这些数据具有逻辑连贯的推理轨迹并经过全面评估；2) 采用动态多任务训练策略，通过逐步优化和任务特定奖励来缓解数据冲突并提供定制激励信号。

Result: M2-Reasoning-7B模型在8个基准测试中均达到了新的最先进水平（SOTA），在通用推理和空间推理领域都展现出卓越性能。

Conclusion: M2-Reasoning-7B模型通过结合精心策划的数据和先进的训练策略，成功提升了多模态大语言模型在通用和动态空间推理方面的能力，有效弥补了现有模型的不足，为实际应用提供了更强大的基础。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs), particularly
through Reinforcement Learning with Verifiable Rewards (RLVR), have
significantly enhanced their reasoning abilities. However, a critical gap
persists: these models struggle with dynamic spatial interactions, a capability
essential for real-world applications. To bridge this gap, we introduce
M2-Reasoning-7B, a model designed to excel in both general and spatial
reasoning. Our approach integrates two key innovations: (1) a novel data
pipeline that generates 294.2K high-quality data samples (168K for cold-start
fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning
trajectories and have undergone comprehensive assessment; and (2) a dynamic
multi-task training strategy with step-wise optimization to mitigate conflicts
between data, and task-specific rewards for delivering tailored incentive
signals. This combination of curated data and advanced training allows
M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,
showcasing superior performance in both general and spatial reasoning domains.

</details>


### [137] [Multi-Agent LLMs as Ethics Advocates in AI-Based Systems](https://arxiv.org/abs/2507.08392)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.AI

TL;DR: 本研究提出一个基于多智能体大型语言模型（LLM）的伦理需求生成框架，通过引入伦理倡导者智能体，以应对人工获取伦理需求的挑战。该框架能有效捕捉并补充需求，但存在可靠性问题，仍强调人工反馈的重要性。


<details>
  <summary>Details</summary>
Motivation: 将伦理纳入需求获取对构建符合伦理的系统至关重要。然而，人工获取伦理需求面临时间、资源限制以及优先级低等挑战，难以高效获取多方利益相关者的多样化输入。

Method: 本研究提出一个框架，在多智能体LLM环境中引入一个伦理倡导者智能体。该智能体根据系统描述，对伦理问题进行批判性分析并提供输入，从而生成伦理需求草案。

Result: 通过两个案例研究评估，该框架能捕获研究人员手动识别的大部分伦理需求，并引入了额外的相关需求。但同时，它也揭示了生成伦理需求时存在的可靠性问题。

Conclusion: 该工作有望促进伦理在需求工程过程中的广泛应用，最终实现更符合伦理的产品。然而，在伦理敏感领域，由于框架的可靠性问题，人类反馈仍然是不可或缺的。

Abstract: Incorporating ethics into the requirement elicitation process is essential
for creating ethically aligned systems. Although eliciting manual ethics
requirements is effective, it requires diverse input from multiple
stakeholders, which can be challenging due to time and resource constraints.
Moreover, it is often given a low priority in the requirements elicitation
process. This study proposes a framework for generating ethics requirements
drafts by introducing an ethics advocate agent in a multi-agent LLM setting.
This agent critiques and provides input on ethical issues based on the system
description. The proposed framework is evaluated through two case studies from
different contexts, demonstrating that it captures the majority of ethics
requirements identified by researchers during 30-minute interviews and
introduces several additional relevant requirements. However, it also
highlights reliability issues in generating ethics requirements, emphasizing
the need for human feedback in this sensitive domain. We believe this work can
facilitate the broader adoption of ethics in the requirements engineering
process, ultimately leading to more ethically aligned products.

</details>


### [138] [Why this and not that? A Logic-based Framework for Contrastive Explanations](https://arxiv.org/abs/2507.08454)
*Tobias Geibinger,Reijo Jaakkola,Antti Kuusisto,Xinghan Liu,Miikka Vilander*

Main category: cs.AI

TL;DR: 本文定义并分析了命题逻辑中“为什么是P而不是Q？”这类对比解释的规范问题，包括其计算复杂度和实际实现。


<details>
  <summary>Details</summary>
Motivation: 旨在形式化地解决“为什么是P而不是Q？”这类对比解释问题，通过计算并比较P和Q的原因来明确其差异。

Method: ['定义了数个与对比解释相关的规范问题。', '在命题逻辑环境下研究了这些定义的基本属性。', '对这些问题的计算复杂度进行了广泛分析。', '使用答案集编程（ASP）实现了CNF公式的问题，并通过实例展示了其在实践中的应用。']

Result: ['所提出的框架能够捕获现有对比解释中基数最小的版本。', '提供了问题的计算复杂度的详尽分析。', '实现了对CNF公式的问题处理，并成功通过实例展示了其实用性。']

Conclusion: 本研究为命题逻辑中的对比解释提供了一个形式化框架，揭示了其理论属性（如最小性、复杂性）以及在实际应用中的可行性。

Abstract: We define several canonical problems related to contrastive explanations,
each answering a question of the form ''Why P but not Q?''. The problems
compute causes for both P and Q, explicitly comparing their differences. We
investigate the basic properties of our definitions in the setting of
propositional logic. We show, inter alia, that our framework captures a
cardinality-minimal version of existing contrastive explanations in the
literature. Furthermore, we provide an extensive analysis of the computational
complexities of the problems. We also implement the problems for CNF-formulas
using answer set programming and present several examples demonstrating how
they work in practice.

</details>


### [139] [From Language to Logic: A Bi-Level Framework for Structured Reasoning](https://arxiv.org/abs/2507.08501)
*Keying Yang,Hao Wang,Kai Yang*

Main category: cs.AI

TL;DR: 本文提出了一个新颖的双层框架，利用大型语言模型将自然语言输入映射到逻辑推理，通过高层任务抽象和低层逻辑生成，显著提高了推理准确性，并增强了透明度和错误可追溯性。


<details>
  <summary>Details</summary>
Motivation: 将自然语言输入转化为结构化推理是人工智能的核心挑战，因为它需要弥合非结构化语言表达与形式逻辑表示之间的鸿沟。

Method: 本文提出了一个新颖的双层框架，通过两阶段过程将语言映射到逻辑：高层任务抽象和低层逻辑生成。在高层，一个大型语言模型（LLM）将自然语言查询解析为中间结构化表示，指定问题类型、目标、决策变量和符号约束。在低层，LLM利用这些表示生成符号工作流或可执行推理程序。该框架支持模块化推理，强制显式约束，并可推广到数学问题求解、问答和逻辑推理等领域。此外，还通过端到端双层优化方法对框架进行了优化。

Result: 在多个实际推理基准测试中，该方法在准确性上显著优于现有基线，准确率提升高达40%。

Conclusion: 双层设计增强了透明度和错误可追溯性，为使用大型语言模型进行可信和系统化推理迈出了有希望的一步。

Abstract: Structured reasoning over natural language inputs remains a core challenge in
artificial intelligence, as it requires bridging the gap between unstructured
linguistic expressions and formal logical representations. In this paper, we
propose a novel \textbf{bi-level framework} that maps language to logic through
a two-stage process: high-level task abstraction and low-level logic
generation. At the upper level, a large language model (LLM) parses natural
language queries into intermediate structured representations specifying the
problem type, objectives, decision variables, and symbolic constraints. At the
lower level, the LLM uses these representations to generate symbolic workflows
or executable reasoning programs for accurate and interpretable decision
making. The framework supports modular reasoning, enforces explicit
constraints, and generalizes across domains such as mathematical problem
solving, question answering, and logical inference. We further optimize the
framework with an end-to-end {bi-level} optimization approach that jointly
refines both the high-level abstraction and low-level logic generation stages.
Experiments on multiple realistic reasoning benchmarks demonstrate that our
approach significantly outperforms existing baselines in accuracy, with
accuracy gains reaching as high as 40\%. Moreover, the bi-level design enhances
transparency and error traceability, offering a promising step toward
trustworthy and systematic reasoning with LLMs.

</details>


### [140] [A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis](https://arxiv.org/abs/2507.08529)
*Mingda Zhang,Na Zhao,Jianglong Qin,Guoyu Ye,Ruixiang Tang*

Main category: cs.AI

TL;DR: 该研究提出一个结合多粒度稀疏激活与分层知识图谱的框架，旨在改善罕见病诊断中医疗大语言模型面临的知识表示和推理挑战，实验结果显示在诊断准确性和信息质量方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管医疗大型语言模型在医疗保健领域取得了进展，但罕见病诊断仍然受到知识表示深度不足、概念理解有限和临床推理受限的制约。

Method: 本文提出了一个将医学概念的多粒度稀疏激活与分层知识图谱相结合的框架。通过四种互补的匹配算法、多样性控制和五级回退策略实现精确的概念激活；同时，一个三层知识图谱（包括分类学、临床特征和实例）提供了结构化且最新的上下文信息。

Result: 在BioASQ罕见病问答集上的实验表明，BLEU值提升0.09，ROUGE值提升0.05，准确率提升0.12，峰值准确率达到0.89，接近0.90的临床阈值。专家评估证实了信息质量、推理能力和专业表达的改进。

Conclusion: 该方法通过提高罕见病诊断的准确性和信息质量，有望缩短罕见病患者的“诊断之旅”，对临床实践具有积极意义。

Abstract: Despite advances from medical large language models in healthcare,
rare-disease diagnosis remains hampered by insufficient
knowledge-representation depth, limited concept understanding, and constrained
clinical reasoning. We propose a framework that couples multi-granularity
sparse activation of medical concepts with a hierarchical knowledge graph. Four
complementary matching algorithms, diversity control, and a five-level fallback
strategy enable precise concept activation, while a three-layer knowledge graph
(taxonomy, clinical features, instances) provides structured, up-to-date
context. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,
ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89
approaching the 0.90 clinical threshold. Expert evaluation confirms
improvements in information quality, reasoning, and professional expression,
suggesting our approach shortens the "diagnostic odyssey" for rare-disease
patients.

</details>


### [141] [Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing](https://arxiv.org/abs/2507.08575)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.AI

TL;DR: 本文提出一种利用大型多模态模型（LMM）和基于网格的方法，对未地理参考的生物样本进行地理参考，其在复杂地点描述上的表现优于现有方法，平均误差约为1公里。


<details>
  <summary>Details</summary>
Motivation: 数百万的生物样本记录缺乏地理参考信息，其复杂地点描述的地理参考工作高度劳动密集。现有自动化方法未能利用地图这一地理参考的关键工具。

Method: 提出一种新颖方法，利用大型多模态模型（LMM）的多模态能力，使模型能从地点描述中读取并视觉化地理解空间关系。该方法采用基于网格的方法，并在零样本设置下运行。

Result: 在小型手动标注数据集上的初步实验显示，该方法表现出色，平均距离误差约为1公里，优于使用大型语言模型的单模态地理参考方法和现有地理参考工具。

Conclusion: 实验结果令人鼓舞，特别是LMM理解精细地图的能力，基于此提出一个实用的框架，旨在将该方法整合到地理参考工作流程中。

Abstract: Millions of biological sample records collected in the last few centuries
archived in natural history collections are un-georeferenced. Georeferencing
complex locality descriptions associated with these collection samples is a
highly labour-intensive task collection agencies struggle with. None of the
existing automated methods exploit maps that are an essential tool for
georeferencing complex relations. We present preliminary experiments and
results of a novel method that exploits multi-modal capabilities of recent
Large Multi-Modal Models (LMM). This method enables the model to visually
contextualize spatial relations it reads in the locality description. We use a
grid-based approach to adapt these auto-regressive models for this task in a
zero-shot setting. Our experiments conducted on a small manually annotated
dataset show impressive results for our approach ($\sim$1 km Average distance
error) compared to uni-modal georeferencing with Large Language Models and
existing georeferencing tools. The paper also discusses the findings of the
experiments in light of an LMM's ability to comprehend fine-grained maps.
Motivated by these results, a practical framework is proposed to integrate this
method into a georeferencing workflow.

</details>


### [142] [Unlocking Speech Instruction Data Potential with Query Rewriting](https://arxiv.org/abs/2507.08603)
*Yonghua Hei,Yibo Yan,Shuliang Liu,Huiyu Zhou,Linfeng Zhang,Xuming Hu*

Main category: cs.AI

TL;DR: 本文提出一种多LLM知识融合的查询重写框架，旨在通过零样本重写和自动化验证，在无需人工标注的情况下高效构建高质量语音指令数据集，以克服现有TTS模型对分布外文本指令的合成限制。


<details>
  <summary>Details</summary>
Motivation: 端到端大型语音语言模型（LSLMs）在语音理解方面展现潜力，但在遵循语音指令方面能力不足，主要原因在于缺乏高质量数据集和训练任务偏差。以往利用LLMs从ASR数据构建指令数据集的方法存在与人类响应的差距，且人工标注成本高昂。虽然语音合成是构建大规模数据集的有效替代方案，但现代TTS模型难以将分布外文本指令高质量地转换为语音，限制了其应用。

Method: 本文提出一个基于多LLM知识融合的查询重写框架。该框架利用多个代理（agents）对合成语音进行标注和验证，以将文本指令转换为更适合TTS模型进行语音合成的分布，通过零样本重写的方式构建高质量语音指令数据集，从而避免了对人工标注的依赖。

Result: 实验结果表明，该方法通过零样本重写将数据可用性从72%提升至93%。此外，在需要复杂知识和上下文相关能力的重写任务中，该方法展现出独特的优势。

Conclusion: 该研究成功解决了利用语音合成构建高质量语音指令数据集的挑战，通过创新的查询重写框架和多LLM知识融合，提升了数据可用性并减少了对人工标注的需求，为LSLMs提升语音指令遵循能力提供了有效途径。

Abstract: End-to-end Large Speech Language Models~(\textbf{LSLMs}) demonstrate strong
potential in response latency and speech comprehension capabilities, showcasing
general intelligence across speech understanding tasks. However, the ability to
follow speech instructions has not been fully realized due to the lack of
datasets and heavily biased training tasks. Leveraging the rich ASR datasets,
previous approaches have used Large Language Models~(\textbf{LLMs}) to continue
the linguistic information of speech to construct speech instruction datasets.
Yet, due to the gap between LLM-generated results and real human responses, the
continuation methods further amplify these shortcomings. Given the high costs
of collecting and annotating speech instruction datasets by humans, using
speech synthesis to construct large-scale speech instruction datasets has
become a balanced and robust alternative. Although modern
Text-To-Speech~(\textbf{TTS}) models have achieved near-human-level synthesis
quality, it is challenging to appropriately convert out-of-distribution text
instruction to speech due to the limitations of the training data distribution
in TTS models. To address this issue, we propose a query rewriting framework
with multi-LLM knowledge fusion, employing multiple agents to annotate and
validate the synthesized speech, making it possible to construct high-quality
speech instruction datasets without relying on human annotation. Experiments
show that this method can transform text instructions into distributions more
suitable for TTS models for speech synthesis through zero-shot rewriting,
increasing data usability from 72\% to 93\%. It also demonstrates unique
advantages in rewriting tasks that require complex knowledge and
context-related abilities.

</details>


### [143] [Agentic Large Language Models for Conceptual Systems Engineering and Design](https://arxiv.org/abs/2507.08619)
*Soheyl Massoudi,Mark Fuge*

Main category: cs.AI

TL;DR: 评估多智能体系统（MAS）与两智能体系统（2AS）在早期工程设计中生成可执行模型的能力，引入Design-State Graph (DSG) 表示。研究发现MAS能提升设计细节，但需求覆盖和代码兼容性仍有待改进。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在早期工程设计中难以保持任务连续性并生成可执行模型，因为该过程复杂且迭代。本研究旨在评估结构化多智能体系统能否更有效地处理需求提取、功能分解和模拟器代码生成，以解决这些挑战。

Method: 本研究引入Design-State Graph (DSG) 作为JSON可序列化表示来整合设计元素。通过对比九角色多智能体系统（MAS）与简化两智能体系统（2AS）在太阳能水过滤系统设计中的表现。实验共60次，涉及Llama 3.3 70B和推理蒸馏的DeepSeek R1 70B两种LLM，并评估JSON有效性、需求覆盖、代码兼容性、工作流完成度、运行时和图大小。

Result: MAS和2AS均保持了完美的JSON完整性与实体标记。需求覆盖率普遍较低（<20%）。2AS在特定设置下代码兼容性可达100%，而MAS平均低于50%。只有推理蒸馏模型能可靠标记工作流完成。由DeepSeek R1 70B驱动的MAS生成了更细粒度的DSG（平均5-6个节点），而2AS则出现模式崩溃。

Conclusion: 结构化的多智能体编排增强了设计细节的粒度，推理蒸馏的LLM提升了工作流完成率。然而，在需求覆盖率低和代码忠实度方面仍存在挑战。

Abstract: Early-stage engineering design involves complex, iterative reasoning, yet
existing large language model (LLM) workflows struggle to maintain task
continuity and generate executable models. We evaluate whether a structured
multi-agent system (MAS) can more effectively manage requirements extraction,
functional decomposition, and simulator code generation than a simpler
two-agent system (2AS). The target application is a solar-powered water
filtration system as described in a cahier des charges. We introduce the
Design-State Graph (DSG), a JSON-serializable representation that bundles
requirements, physical embodiments, and Python-based physics models into graph
nodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS
collapses the process to a Generator-Reflector loop. Both systems run a total
of 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1
70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON
validity, requirement coverage, embodiment presence, code compatibility,
workflow completion, runtime, and graph size. Across all runs, both MAS and 2AS
maintained perfect JSON integrity and embodiment tagging. Requirement coverage
remained minimal (less than 20\%). Code compatibility peaked at 100\% under
specific 2AS settings but averaged below 50\% for MAS. Only the
reasoning-distilled model reliably flagged workflow completion. Powered by
DeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)
whereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced
design detail. Reasoning-distilled LLM improved completion rates, yet low
requirements and fidelity gaps in coding persisted.

</details>


### [144] [Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning](https://arxiv.org/abs/2507.08649)
*Xingguang Ji,Yahui Liu,Qi Wang,Jingyuan Zhang,Yang Yue,Rui Shi,Chenxi Sun,Fuzheng Zhang,Guorui Zhou,Kun Gai*

Main category: cs.AI

TL;DR: 本文介绍Leanabell-Prover-V2，一个7B大型语言模型，它通过整合Lean 4验证器反馈和强化学习，显著提升了在Lean 4中生成形式化定理证明的能力。


<details>
  <summary>Details</summary>
Motivation: 旨在通过利用Lean 4验证器的反馈，提升大型语言模型在Lean 4中生成形式化定理证明的性能，使其能够“自我感知”并修正推理错误，从而超越现有证明模型的表现。

Method: 在Leanabell-Prover-V1的基础上，V2版本主要通过强化学习（RL）进行升级，核心是利用Lean 4验证器提供的成功或错误反馈。系统通过多轮验证器交互直接优化LLM的推理轨迹，并结合反馈令牌遮蔽和简单的奖励策略来稳定RL训练。

Result: 在MiniF2F测试集上，Leanabell-Prover-V2与Kimina-Prover-Preview-Distill-7B结合时，性能提升了3.2% (pass@128)；与DeepSeek-Prover-V2-7B结合时，性能提升了2.0% (pass@128)。

Conclusion: Leanabell-Prover-V2通过集成验证器反馈并优化强化学习过程，显著提升了LLM在Lean 4形式化定理证明方面的性能，展示了LLM利用外部反馈进行自我纠错的有效性。

Abstract: We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that
can produce formal theorem proofs in Lean 4, with verifier-integrated Long
Chain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we
continual to choose to posttrain existing strong prover models for further
performance improvement. In our V2 version, we mainly upgrade the Reinforcement
Learning (RL) with feedback provided by the Lean 4 verifier. Crucially,
verifier feedback, such as indicating success or detailing specific errors,
allows the LLM to become ``self-aware'' of the correctness of its own reasoning
process and learn to reflexively correct errors. Leanabell-Prover-V2 directly
optimizes LLM reasoning trajectories with multi-turn verifier interactions,
together with feedback token masking for stable RL training and a simple reward
strategy. Experiments show that Leanabell-Prover-V2 improves performance by
3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with
DeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data
and models are available at:
https://github.com/Leanabell-LM/Leanabell-Prover-V2.

</details>


### [145] [Introspection of Thought Helps AI Agents](https://arxiv.org/abs/2507.08664)
*Haoran Sun,Shaoning Zeng*

Main category: cs.AI

TL;DR: 现有AI Agent的LLM推理框架存在理解能力限制和高成本问题。本文提出INoT框架，通过在prompt中嵌入可读代码，使LLM能内部执行程序化推理和反思，从而显著提高性能并大幅降低token成本。


<details>
  <summary>Details</summary>
Motivation: 当前的AI Agent推理框架（如Chain-of-Thought）受限于LLM的自然语言理解能力，且迭代推理过程会产生高昂的推理成本（token消耗）。

Method: 本文提出一种新颖的AI Agent推理框架——“Introspection of Thought (INoT)”。该方法通过在prompt中设计一种新的“LLM-Read code”，使LLM能够按照代码指令内部执行程序化对话推理过程，实现LLM内部的自我否定和反思，从而有效降低token成本。

Result: 通过在六个基准测试（涵盖三种不同任务）上的实验，INoT的有效性得到验证。其性能平均提升7.95%，超越基线方法；同时，token成本平均比性能最佳的基线方法低58.3%。此外，实验还证明了INoT在图像解释和推理方面的通用性。

Conclusion: INoT框架通过在LLM内部实现高效的编程化推理和反思，成功解决了现有AI Agent推理方法的性能瓶颈和高成本问题，实现了显著的性能提升和成本降低，并展现了良好的跨模态通用性。

Abstract: AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to
perform interpretation and inference in text and image tasks without
post-training, where LLMs and MLLMs play the most critical role and determine
the initial ability and limitations of AI Agents. Usually, AI Agents utilize
sophisticated prompt engineering and external reasoning framework to obtain a
promising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought
and Image-of-Thought. However, they are still constrained by the inherent
limitations of LLM in understanding natural language, and the iterative
reasoning process will generate a large amount of inference cost. To this end,
we propose a novel AI Agent Reasoning Framework with Introspection of Thought
(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute
programmatic dialogue reasoning processes following the code in prompt.
Therefore, self-denial and reflection occur within LLM instead of outside LLM,
which can reduce token cost effectively. Through our experiments on six
benchmarks for three different tasks, the effectiveness of INoT is verified,
with an average improvement of 7.95\% in performance, exceeding the baselines.
Furthermore, the token cost of INoT is lower on average than the best
performing method at baseline by 58.3\%. In addition, we demonstrate the
versatility of INoT in image interpretation and inference through verification
experiments.

</details>


### [146] [elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings](https://arxiv.org/abs/2507.08705)
*Philip Osborne,Danilo S. Carvalho,André Freitas*

Main category: cs.AI

TL;DR: 我们推出了elsciRL，一个开源Python库，旨在通过集成LLM生成的自补全指令来提升强化学习智能体的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在促进语言解决方案在强化学习问题上的应用，并加速其在基于奖励环境中的评估，从而开启科学发现的新机遇。

Method: 开发了开源Python库elsciRL，通过使用大型语言模型（LLMs）扩展了“自补全指令语言适配器”框架。该方法提供了一个新颖的图形用户界面（GUI），允许用户输入文本，由LLM生成并自补全指令。

Result: 实证结果表明，这些生成的指令能够提高强化学习智能体的性能。此外，该方法具有高度的可重用性，且设置要求极低。

Conclusion: elsciRL为在基于奖励的环境中评估语言解决方案提供了一个有效工具，展示了LLM生成的指令在提升强化学习智能体表现方面的潜力，从而为相关领域的科学发现提供了新的机会。

Abstract: We present elsciRL, an open-source Python library to facilitate the
application of language solutions on reinforcement learning problems. We
demonstrate the potential of our software by extending the Language Adapter
with Self-Completing Instruction framework defined in (Osborne, 2024) with the
use of LLMs. Our approach can be re-applied to new applications with minimal
setup requirements. We provide a novel GUI that allows a user to provide text
input for an LLM to generate instructions which it can then self-complete.
Empirical results indicate that these instructions \textit{can} improve a
reinforcement learning agent's performance. Therefore, we present this work to
accelerate the evaluation of language solutions on reward based environments to
enable new opportunities for scientific discovery.

</details>


### [147] [System-of-systems Modeling and Optimization: An Integrated Framework for Intermodal Mobility](https://arxiv.org/abs/2507.08715)
*Paul Saves,Jasper Bussemaker,Rémi Lafage,Thierry Lefebvre,Nathalie Bartoli,Youssef Diouane,Joseph Morlier*

Main category: cs.AI

TL;DR: 针对创新系统架构（特别是系统之系统）开发中，传统高效优化方法在探索新颖架构时面临高计算成本和潜在失败。为解决此问题，研究提出并利用基于代理模型的优化算法（如贝叶斯优化）。


<details>
  <summary>Details</summary>
Motivation: 在系统架构设计（尤其是系统之系统）中，尽管使用高效的专用方法（如基于物理的模拟）可以降低计算复杂性，但这些方法在探索新颖架构时，会给优化算法带来评估成本增加和潜在失败的挑战。

Method: 采用基于代理模型的优化算法，具体包括利用高斯过程模型的贝叶斯优化。

Result: 抽象中未明确提及具体的实验结果或量化发现，主要在于阐述问题和提出解决方案。

Conclusion: 基于代理模型的优化算法（如结合高斯过程模型的贝叶斯优化）为解决传统优化方法在探索新颖系统架构时面临的计算成本高昂和潜在失败问题提供了一种有效途径。

Abstract: For developing innovative systems architectures, modeling and optimization
techniques have been central to frame the architecting process and define the
optimization and modeling problems. In this context, for system-of-systems the
use of efficient dedicated approaches (often physics-based simulations) is
highly recommended to reduce the computational complexity of the targeted
applications. However, exploring novel architectures using such dedicated
approaches might pose challenges for optimization algorithms, including
increased evaluation costs and potential failures. To address these challenges,
surrogate-based optimization algorithms, such as Bayesian optimization
utilizing Gaussian process models have emerged.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [148] [An Enhanced Privacy-preserving Federated Few-shot Learning Framework for Respiratory Disease Diagnosis](https://arxiv.org/abs/2507.08050)
*Ming Wang,Zhaoyang Duan,Dong Xue,Fangzhou Liu,Zhongheng Zhang*

Main category: cs.LG

TL;DR: 本研究提出了一个联邦少样本学习框架，结合隐私保护机制，旨在解决呼吸系统疾病诊断中标记数据稀缺和患者隐私保护的挑战。


<details>
  <summary>Details</summary>
Motivation: 呼吸系统疾病诊断数据标注劳动密集且高质量标注数据稀缺；患者隐私问题阻碍医疗数据直接共享；现有中心化数据驱动方法常损害数据隐私且依赖大量数据。

Method: 提出联邦少样本学习框架，整合隐私保护机制。具体包括：使用元随机梯度下降算法缓解数据不足导致的过拟合；在本地模型训练中梯度加入差分隐私噪声，以防梯度泄露；采用加权平均算法聚合来自不同客户端的本地诊断模型，以适应分散的医疗数据。

Result: 实验结果表明，所提方法在实现差分隐私的同时，能有效诊断不同结构、类别和分布的呼吸系统疾病数据，并取得了令人信服的效果。

Conclusion: 该框架成功解决了呼吸系统疾病诊断中有限数据和隐私保护的难题，提供了一种有效且兼顾隐私的分布式医疗数据解决方案。

Abstract: The labor-intensive nature of medical data annotation presents a significant
challenge for respiratory disease diagnosis, resulting in a scarcity of
high-quality labeled datasets in resource-constrained settings. Moreover,
patient privacy concerns complicate the direct sharing of local medical data
across institutions, and existing centralized data-driven approaches, which
rely on amounts of available data, often compromise data privacy. This study
proposes a federated few-shot learning framework with privacy-preserving
mechanisms to address the issues of limited labeled data and privacy protection
in diagnosing respiratory diseases. In particular, a meta-stochastic gradient
descent algorithm is proposed to mitigate the overfitting problem that arises
from insufficient data when employing traditional gradient descent methods for
neural network training. Furthermore, to ensure data privacy against gradient
leakage, differential privacy noise from a standard Gaussian distribution is
integrated into the gradients during the training of private models with local
data, thereby preventing the reconstruction of medical images. Given the
impracticality of centralizing respiratory disease data dispersed across
various medical institutions, a weighted average algorithm is employed to
aggregate local diagnostic models from different clients, enhancing the
adaptability of a model across diverse scenarios. Experimental results show
that the proposed method yields compelling results with the implementation of
differential privacy, while effectively diagnosing respiratory diseases using
data from different structures, categories, and distributions.

</details>


### [149] [Tree-Structured Parzen Estimator Can Solve Black-Box Combinatorial Optimization More Efficiently](https://arxiv.org/abs/2507.08053)
*Kenshin Abe,Yunzhuo Wang,Shuhei Watanabe*

Main category: cs.LG

TL;DR: 本文提出了一种Tree-structured Parzen estimator (TPE)的高效组合优化算法。通过推广分类核并优化其计算以应对大型搜索空间，该方法在合成问题上比原TPE能以更少评估次数找到更优解，并已集成到开源工具Optuna中。


<details>
  <summary>Details</summary>
Motivation: Tree-structured Parzen estimator (TPE)作为一种超参数优化(HPO)方法，尽管在深度学习领域应用广泛，但其在黑盒组合优化（如化学、生物等领域）中的应用仍是未被充分探索但至关重要的方向。

Method: 1. 将TPE中的分类核与数值核进行泛化，为分类核引入距离结构。2. 对新开发的核进行修改，以降低在大规模组合搜索空间中核计算的时间复杂度。

Result: 在合成问题的实验中，所提出的方法相比于原始TPE，能够以更少的评估次数找到更好的解决方案。

Conclusion: 本文提出并验证了一种高效的TPE组合优化算法，解决了TPE在组合优化领域的空白，并已将该算法集成到开源HPO框架Optuna中。

Abstract: Tree-structured Parzen estimator (TPE) is a versatile hyperparameter
optimization (HPO) method supported by popular HPO tools. Since these HPO tools
have been developed in line with the trend of deep learning (DL), the problem
setups often used in the DL domain have been discussed for TPE such as
multi-objective optimization and multi-fidelity optimization. However, the
practical applications of HPO are not limited to DL, and black-box
combinatorial optimization is actively utilized in some domains, e.g.,
chemistry and biology. As combinatorial optimization has been an untouched, yet
very important, topic in TPE, we propose an efficient combinatorial
optimization algorithm for TPE. In this paper, we first generalize the
categorical kernel with the numerical kernel in TPE, enabling us to introduce a
distance structure to the categorical kernel. Then we discuss modifications for
the newly developed kernel to handle a large combinatorial search space. These
modifications reduce the time complexity of the kernel calculation with respect
to the size of a combinatorial search space. In the experiments using synthetic
problems, we verified that our proposed method identifies better solutions with
fewer evaluations than the original TPE. Our algorithm is available in Optuna,
an open-source framework for HPO.

</details>


### [150] [Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions](https://arxiv.org/abs/2507.08068)
*Simon Matrenok,Skander Moalla,Caglar Gulcehre*

Main category: cs.LG

TL;DR: QRPO是一种新型对齐算法，它能使用点式绝对奖励进行离线学习，同时保持DPO类方法的简洁性，并在聊天和编码任务中表现卓越。


<details>
  <summary>Details</summary>
Motivation: 当前，大型语言模型（LLM）与点式绝对奖励的对齐需要复杂的在线、在策略算法（如PPO）。而DPO和REBEL等更简单、可利用离线或离策略数据的方法，却仅限于从偏好对或相对信号中学习。本研究旨在弥合这一差距，开发一种能利用点式绝对奖励，同时保持简洁性和离线适用性的方法。

Method: 引入“分位数奖励策略优化”（QRPO）。QRPO通过使用分位数奖励，实现了对KL正则化强化学习目标闭式解的回归。这种奖励形式使得分区函数在分析上可处理，从而无需依赖相对信号来消除该项。此外，QRPO可通过增加计算来估计分位数奖励，为预计算扩展提供了新维度。

Result: 在聊天和编码评估（包括奖励模型得分、AlpacaEval 2和LeetCode）中，QRPO在多样化数据集和8B规模模型上持续取得顶尖性能，优于DPO、REBEL和SimPO。研究还发现，与将奖励转换为偏好相比，直接使用鲁棒奖励进行训练能诱导更少的长度偏差。

Conclusion: QRPO成功提供了一种简单、可离线且高效的方法，用于直接利用点式绝对奖励对齐LLM，克服了现有方法的局限性。它在经验上展现出卓越性能，并能有效减少训练中的长度偏差，为LLM对齐开辟了新途径。

Abstract: Aligning large language models with pointwise absolute rewards has so far
required online, on-policy algorithms such as PPO and GRPO. In contrast,
simpler methods that can leverage offline or off-policy data, such as DPO and
REBEL, are limited to learning from preference pairs or relative signals. To
bridge this gap, we introduce \emph{Quantile Reward Policy Optimization}
(QRPO), which learns from pointwise absolute rewards while preserving the
simplicity and offline applicability of DPO-like methods. QRPO uses quantile
rewards to enable regression to the closed-form solution of the KL-regularized
RL objective. This reward yields an analytically tractable partition function,
removing the need for relative signals to cancel this term. Moreover, QRPO
scales with increased compute to estimate quantile rewards, opening a new
dimension for pre-computation scaling. Empirically, QRPO consistently achieves
top performance on chat and coding evaluations -- reward model scores,
AlpacaEval 2, and LeetCode -- compared to DPO, REBEL, and SimPO across diverse
datasets and 8B-scale models. Finally, we find that training with robust
rewards instead of converting them to preferences induces less length bias.

</details>


### [151] [Low-rank Momentum Factorization for Memory Efficient Training](https://arxiv.org/abs/2507.08091)
*Pouria Mahdavinia,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: 提出MoFaSGD，一种通过动态低秩动量更新实现大模型内存高效微调的方法。


<details>
  <summary>Details</summary>
Motivation: 微调大型基础模型时，优化器（如AdamW）会导致显著的内存消耗。尽管存在参数高效微调和优化器状态压缩等方法，但近期方法（如GaLore）仍可能面临固定子空间或高计算成本（如SVD）的挑战。

Method: 本文提出动量分解SGD（MoFaSGD）。该方法在训练过程中动态维护一阶动量的低秩SVD表示，使其近似全秩动量。MoFaSGD能够自适应地更新优化子空间，并利用计算出的低秩动量因子进行高效的谱归一化更新，作为子空间动量累积的替代方案。

Result: 理论上，MoFaSGD在标准假设下，被证明在非凸随机优化中能达到最优收敛速率。经验上，在大型语言模型对齐基准测试中，MoFaSGD实现了与LoRA相当的内存减少，并在性能上与最先进的低秩优化方法相比，展现出有竞争力的平衡。

Conclusion: MoFaSGD提供了一种理论上可证且经验上有效的内存高效微调方法，在大模型微调中实现了内存削减与性能之间的良好平衡。

Abstract: Fine-tuning large foundation models presents significant memory challenges
due to stateful optimizers like AdamW, often requiring several times more GPU
memory than inference. While memory-efficient methods like parameter-efficient
fine-tuning (e.g., LoRA) and optimizer state compression exist, recent
approaches like GaLore bridge these by using low-rank gradient projections and
subspace moment accumulation. However, such methods may struggle with fixed
subspaces or computationally costly offline resampling (e.g., requiring
full-matrix SVDs). We propose Momentum Factorized SGD (MoFaSGD), which
maintains a dynamically updated low-rank SVD representation of the first-order
momentum, closely approximating its full-rank counterpart throughout training.
This factorization enables a memory-efficient fine-tuning method that
adaptively updates the optimization subspace at each iteration. Crucially,
MoFaSGD leverages the computed low-rank momentum factors to perform efficient
spectrally normalized updates, offering an alternative to subspace moment
accumulation. We establish theoretical convergence guarantees for MoFaSGD,
proving it achieves an optimal rate for non-convex stochastic optimization
under standard assumptions. Empirically, we demonstrate MoFaSGD's effectiveness
on large language model alignment benchmarks, achieving a competitive trade-off
between memory reduction (comparable to LoRA) and performance compared to
state-of-the-art low-rank optimization methods. Our implementation is available
at https://github.com/pmahdavi/MoFaSGD.

</details>


### [152] [PDE-aware Optimizer for Physics-informed Neural Networks](https://arxiv.org/abs/2507.08118)
*Hardik Shukla,Manurag Khullar,Vismay Churiwala*

Main category: cs.LG

TL;DR: 提出一种基于PDE残差梯度方差的优化器，用于解决PINNs训练中标准优化器难以平衡损失项的问题。该方法在测试中表现出更平滑的收敛和更低的误差，且计算成本低于二阶优化器。


<details>
  <summary>Details</summary>
Motivation: 尽管PINNs在求解PDEs方面强大，但标准优化器（如Adam）在平衡PINNs的竞争损失项时常遇到困难，尤其是在刚性或病态系统中。现有高阶优化器（如SOAP）虽然能解决问题，但计算成本高昂。

Method: 本文提出一种PDE感知优化器，通过根据每个样本的PDE残差梯度的方差来自适应调整参数更新，以解决梯度未对齐问题，同时避免了二阶优化器（如SOAP）的高计算成本。该方法在1D Burgers'、Allen-Cahn和Korteweg-de Vries(KdV)方程上与Adam和SOAP进行了基准测试。

Result: 与Adam和SOAP相比，所提出的PDE感知优化器在测试的PDEs上实现了更平滑的收敛和更低的绝对误差，尤其是在梯度剧烈变化的区域。

Conclusion: 研究结果表明，PDE残差感知自适应性有效提升了PINNs训练的稳定性。未来工作将集中于在大规模架构和硬件加速器上的进一步扩展。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving partial differential equations (PDEs) by embedding physical
constraints into the loss function. However, standard optimizers such as Adam
often struggle to balance competing loss terms, particularly in stiff or
ill-conditioned systems. In this work, we propose a PDE-aware optimizer that
adapts parameter updates based on the variance of per-sample PDE residual
gradients. This method addresses gradient misalignment without incurring the
heavy computational costs of second-order optimizers such as SOAP. We benchmark
the PDE-aware optimizer against Adam and SOAP on 1D Burgers', Allen-Cahn and
Korteweg-de Vries(KdV) equations. Across both PDEs, the PDE-aware optimizer
achieves smoother convergence and lower absolute errors, particularly in
regions with sharp gradients. Our results demonstrate the effectiveness of PDE
residual-aware adaptivity in enhancing stability in PINNs training. While
promising, further scaling on larger architectures and hardware accelerators
remains an important direction for future research.

</details>


### [153] [Quasi-Random Physics-informed Neural Networks](https://arxiv.org/abs/2507.08121)
*Tianchi Yu,Ivan Oseledets*

Main category: cs.LG

TL;DR: 提出QRPINNs，通过使用低差异序列采样解决PINNs对采样敏感的问题，理论和实验证明其性能优于PINNs，尤其在高维PDEs中。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络（PINNs）在解决偏微分方程（PDEs）方面有潜力，但其性能对采样点敏感。

Method: 提出准随机物理信息神经网络（QRPINNs），该方法利用低差异序列进行采样，而非传统的随机采样。

Result: 理论上，QRPINNs被证明比PINNs具有更好的收敛速度。经验上，实验表明QRPINNs显著优于PINNs和一些代表性的自适应采样方法，尤其在高维PDEs中。此外，QRPINNs与自适应采样结合可进一步提升性能。

Conclusion: QRPINNs通过优化采样策略有效地提升了PINNs的性能，尤其在高维PDEs问题上表现卓越，且具有与自适应采样方法结合以进一步提升效果的潜力。

Abstract: Physics-informed neural networks have shown promise in solving partial
differential equations (PDEs) by integrating physical constraints into neural
network training, but their performance is sensitive to the sampling of points.
Based on the impressive performance of quasi Monte-Carlo methods in high
dimensional problems, this paper proposes Quasi-Random Physics-Informed Neural
Networks (QRPINNs), which use low-discrepancy sequences for sampling instead of
random points directly from the domain. Theoretically, QRPINNs have been proven
to have a better convergence rate than PINNs. Empirically, experiments
demonstrate that QRPINNs significantly outperform PINNs and some representative
adaptive sampling methods, especially in high-dimensional PDEs. Furthermore,
combining QRPINNs with adaptive sampling can further improve the performance.

</details>


### [154] [Physics-Informed Neural Networks with Hard Nonlinear Equality and Inequality Constraints](https://arxiv.org/abs/2507.08124)
*Ashfaq Iftakher,Rahul Golder,M. M. Faruque Hasan*

Main category: cs.LG

TL;DR: 开发了KKT-Hardnet，一种改进的物理信息神经网络(PINN)，通过求解KKT条件将预测严格投影到可行域，以高精度满足线性和非线性约束，从而提高模型在工程系统中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络(PINN)无法严格满足约束条件，这在工程系统中会导致模型预测的可靠性和一致性严重下降。

Method: 提出KKT-Hardnet架构，该架构利用距离最小化问题的Karush-Kuhn-Tucker (KKT)条件，将预测投影到可行域以强制满足约束。通过对数-指数变换重新构建非线性KKT条件，形成一个包含线性和指数项的稀疏系统，使投影可微分。

Result: 将KKT-Hardnet应用于测试问题和真实化学过程仿真。与多层感知机和传统PINN相比，KKT-Hardnet实现了更高的精度和严格的约束满足。

Conclusion: 该方法将领域知识整合到机器学习中，为复杂系统的可靠混合建模提供了新途径。

Abstract: Traditional physics-informed neural networks (PINNs) do not guarantee strict
constraint satisfaction. This is problematic in engineering systems where minor
violations of governing laws can significantly degrade the reliability and
consistency of model predictions. In this work, we develop KKT-Hardnet, a PINN
architecture that enforces both linear and nonlinear equality and inequality
constraints up to machine precision. It leverages a projection onto the
feasible region through solving Karush-Kuhn-Tucker (KKT) conditions of a
distance minimization problem. Furthermore, we reformulate the nonlinear KKT
conditions using log-exponential transformation to construct a general sparse
system with only linear and exponential terms, thereby making the projection
differentiable. We apply KKT-Hardnet on both test problems and a real-world
chemical process simulation. Compared to multilayer perceptrons and PINNs,
KKT-Hardnet achieves higher accuracy and strict constraint satisfaction. This
approach allows the integration of domain knowledge into machine learning
towards reliable hybrid modeling of complex systems.

</details>


### [155] [ALCo-FM: Adaptive Long-Context Foundation Model for Accident Prediction](https://arxiv.org/abs/2507.08153)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Rajiv Ramnath*

Main category: cs.LG

TL;DR: ALCo-FM是一个用于城市交通事故风险预测的自适应长上下文基础模型，通过动态上下文选择和多模态融合，实现了卓越的预测性能。


<details>
  <summary>Details</summary>
Motivation: 交通事故是罕见但影响深远的事件，需要长上下文多模态推理来进行准确的风险预测。

Method: 引入ALCo-FM，一个统一的自适应长上下文基础模型。该模型通过计算波动性预得分动态选择输入数据的上下文窗口，并通过浅层交叉注意力编码和融合多模态数据。其架构包括局部GAT层和基于H3六边形网格的BigBird式稀疏全局Transformer，并结合蒙特卡洛Dropout来提高置信度。模型在15个美国城市的数据上训练，使用类别加权损失应对标签不平衡，并在少量数据上对保留城市进行微调。

Result: ALCo-FM在大型城市风险预测中表现出色，准确率达到0.94，F1分数达到0.92，ECE为0.04，性能优于20多种现有先进基线模型。

Conclusion: ALCo-FM通过其自适应长上下文和多模态融合能力，为城市交通事故风险预测提供了卓越且校准良好的解决方案，显著优于现有方法。

Abstract: Traffic accidents are rare, yet high-impact events that require long-context
multimodal reasoning for accurate risk forecasting. In this paper, we introduce
ALCo-FM, a unified adaptive long-context foundation model that computes a
volatility pre-score to dynamically select context windows for input data and
encodes and fuses these multimodal data via shallow cross attention. Following
a local GAT layer and a BigBird-style sparse global transformer over H3
hexagonal grids, coupled with Monte Carlo dropout for confidence, the model
yields superior, well-calibrated predictions. Trained on data from 15 US cities
with a class-weighted loss to counter label imbalance, and fine-tuned with
minimal data on held-out cities, ALCo-FM achieves 0.94 accuracy, 0.92 F1, and
an ECE of 0.04, outperforming more than 20 state-of-the-art baselines in
large-scale urban risk prediction. Code and dataset are available at:
https://github.com/PinakiPrasad12/ALCo-FM

</details>


### [156] [Just Read the Question: Enabling Generalization to New Assessment Items with Text Awareness](https://arxiv.org/abs/2507.08154)
*Arisha Khan,Nathaniel Li,Tori Shen,Anna N. Rafferty*

Main category: cs.LG

TL;DR: 一个结合文本嵌入的教育评估机器学习模型Text-LENS，有效解决了传统机器学习在处理新题目时的局限性，在预测学生表现方面表现出色，尤其对未见过的新题目具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在教育评估中难以有效整合新题目，因为它们严重依赖历史数据，无法很好地泛化到未见过的题目。因此，需要开发一种能够利用新题目信息（如文本内容）并提高泛化能力的模型。

Method: 研究者通过扩展LENS部分变分自编码器（partial variational auto-encoder），开发了名为Text-LENS的新模型。该模型利用题目文本嵌入（item text embeddings）来处理题目内容信息。研究在Eedi（包含题目内容的公开数据集）和LLM-Sim（由LLM生成测试题的新数据集）这两个数据集上进行了性能评估。

Result: Text-LENS在已见过的题目上表现与LENS模型相当。在涉及未见过题目的多种条件下，Text-LENS的表现优于LENS。该模型能有效地从新题目中学习学生的能力水平，并对学生在新题目上的表现进行预测。

Conclusion: Text-LENS成功地将题目文本信息融入教育评估模型，解决了传统方法在新题目泛化方面的挑战。它不仅保持了现有模型的性能，还显著提升了对未见过题目的预测能力和泛化性，为学生能力评估和新题目处理提供了有效途径。

Abstract: Machine learning has been proposed as a way to improve educational assessment
by making fine-grained predictions about student performance and learning
relationships between items. One challenge with many machine learning
approaches is incorporating new items, as these approaches rely heavily on
historical data. We develop Text-LENS by extending the LENS partial variational
auto-encoder for educational assessment to leverage item text embeddings, and
explore the impact on predictive performance and generalization to previously
unseen items. We examine performance on two datasets: Eedi, a publicly
available dataset that includes item content, and LLM-Sim, a novel dataset with
test items produced by an LLM. We find that Text-LENS matches LENS' performance
on seen items and improves upon it in a variety of conditions involving unseen
items; it effectively learns student proficiency from and makes predictions
about student performance on new items.

</details>


### [157] [Emotion Recognition in Older Adults with Quantum Machine Learning and Wearable Sensors](https://arxiv.org/abs/2507.08175)
*Md. Saif Hassan Onim,Travis S. Humble,Himanshu Thapliyal*

Main category: cs.LG

TL;DR: 该研究利用生理信号结合量子机器学习（QML）实现情绪推断，提供了一种隐私保护的替代方案，结果显示QML方法在有限数据集下性能优于经典算法，具有广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 开发一种仅凭生理信号推断情绪状态的方法，旨在提供比传统面部识别技术更具隐私保护性的替代方案。

Method: 比较了经典机器学习算法与基于量子核模型的混合量子机器学习（QML）方法的性能，主要采用量子增强型支持向量机（SVM）进行情绪分类。

Result: 量子增强型SVM在所有情绪类别的分类性能上均优于经典算法，即使在有限数据集上训练亦如此。所有类别的F1分数均超过80%，召回率最高提升约36%。

Conclusion: 结合可穿戴传感器数据与量子机器学习能提高情绪识别的准确性和鲁棒性，并实现非侵入式识别。该方法对沟通障碍人群（如ADRD和PTSD患者）具有重要意义，为临床和辅助生活环境中的被动情绪监测奠定了早期基础。

Abstract: We investigate the feasibility of inferring emotional states exclusively from
physiological signals, thereby presenting a privacy-preserving alternative to
conventional facial recognition techniques. We conduct a performance comparison
of classical machine learning algorithms and hybrid quantum machine learning
(QML) methods with a quantum kernel-based model. Our results indicate that the
quantum-enhanced SVM surpasses classical counterparts in classification
performance across all emotion categories, even when trained on limited
datasets. The F1 scores over all classes are over 80% with around a maximum of
36% improvement in the recall values. The integration of wearable sensor data
with quantum machine learning not only enhances accuracy and robustness but
also facilitates unobtrusive emotion recognition. This methodology holds
promise for populations with impaired communication abilities, such as
individuals with Alzheimer's Disease and Related Dementias (ADRD) and veterans
with Post-Traumatic Stress Disorder (PTSD). The findings establish an early
foundation for passive emotional monitoring in clinical and assisted living
conditions.

</details>


### [158] [Rethinking Spatio-Temporal Anomaly Detection: A Vision for Causality-Driven Cybersecurity](https://arxiv.org/abs/2507.08177)
*Arun Vignesh Malarkkan,Haoyue Bai,Xinyuan Wang,Anjali Kaushik,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: 本文提出将因果学习应用于互联网络物理系统的时空异常检测，以克服现有黑盒方法在可解释性、适应性和鲁棒性上的不足，旨在通过揭示因果关系实现早期预警和根因归因，并开辟新的研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着网络物理系统日益互联和空间分布化，确保其对网络攻击的弹性至关重要。时空异常检测扮演关键角色，但现有数据驱动（尤其是黑盒深度学习）方法在可解释性、对分布变化的适应性和动态系统下的鲁棒性方面面临挑战。

Method: 倡导采用因果学习视角，将异常检测建立在结构化因果关系上。具体提出并形式化了三个关键方向：因果图分析、多视角融合和持续因果图学习，以揭示动态时空因果结构。

Result: 研究表明，因果模型能提供早期预警信号和根因归因，有效解决黑盒检测器的局限性。这些观点已从水处理基础设施等真实世界系统中获得验证。

Conclusion: 本研究旨在为可扩展、自适应、可解释和空间感知的异常检测系统开辟新的研究路径，并期望推动网络安全研究向因果驱动方法的范式转变，以应对互联基础设施中不断演进的威胁。

Abstract: As cyber-physical systems grow increasingly interconnected and spatially
distributed, ensuring their resilience against evolving cyberattacks has become
a critical priority. Spatio-Temporal Anomaly detection plays an important role
in ensuring system security and operational integrity. However, current
data-driven approaches, largely driven by black-box deep learning, face
challenges in interpretability, adaptability to distribution shifts, and
robustness under evolving system dynamics. In this paper, we advocate for a
causal learning perspective to advance anomaly detection in spatially
distributed infrastructures that grounds detection in structural cause-effect
relationships. We identify and formalize three key directions: causal graph
profiling, multi-view fusion, and continual causal graph learning, each
offering distinct advantages in uncovering dynamic cause-effect structures
across time and space. Drawing on real-world insights from systems such as
water treatment infrastructures, we illustrate how causal models provide early
warning signals and root cause attribution, addressing the limitations of
black-box detectors. Looking ahead, we outline the future research agenda
centered on multi-modality, generative AI-driven, and scalable adaptive causal
frameworks. Our objective is to lay a new research trajectory toward scalable,
adaptive, explainable, and spatially grounded anomaly detection systems. We
hope to inspire a paradigm shift in cybersecurity research, promoting
causality-driven approaches to address evolving threats in interconnected
infrastructures.

</details>


### [159] [CTRLS: Chain-of-Thought Reasoning via Latent State-Transition](https://arxiv.org/abs/2507.08182)
*Junda Wu,Yuxin Xiong,Xintong Li,Zhengmian Hu,Tong Yu,Rui Wang,Xiang Chen,Jingbo Shang,Julian McAuley*

Main category: cs.LG

TL;DR: 本文提出CTRLS框架，将CoT推理建模为具有潜在状态转换的马尔可夫决策过程（MDP），并通过分布强化学习实现系统探索，显著提升LLM在推理任务中的准确性、多样性和探索效率。


<details>
  <summary>Details</summary>
Motivation: 传统的Chain-of-Thought (CoT) 方法依赖启发式采样，缺乏对推理转换的结构化建模，这限制了它们系统探索和发现多样且有效推理轨迹的能力。

Method: 引入CTRLS框架，将CoT推理公式化为具有潜在状态转换的马尔可夫决策过程 (MDP)。通过分布强化学习实现状态感知的探索，将推理动作建模为潜在空间中的显式概率分布，从而显式建模认知不确定性。采用包含epsilon-greedy探索和基于熵正则化的在策略强化学习策略，迭代优化潜在状态转换，无需额外微调底层LLM。

Result: 实验证明，CTRLS在基准推理任务中显著提高了推理准确性、多样性，并提升了探索效率。

Conclusion: CTRLS通过将CoT推理建模为具有潜在状态转换的MDP并利用分布强化学习，为LLM的推理探索提供了一种原则性且有效的方法，无需对LLM进行额外微调，在性能上取得了显著提升。

Abstract: Chain-of-thought (CoT) reasoning enables large language models (LLMs) to
break down complex problems into interpretable intermediate steps,
significantly enhancing model transparency and performance in reasoning tasks.
However, conventional CoT methods rely on heuristic sampling without structured
modeling of reasoning transitions, constraining their ability to systematically
explore and discover diverse and effective reasoning trajectories. In this
work, we introduce CTRLS, a framework that formulates CoT reasoning as a Markov
decision process (MDP) with latent state transitions, enabling principled and
state-aware exploration via distributional reinforcement learning. By modelling
reasoning actions as explicit probability distributions in latent space, our
approach explicitly models epistemic uncertainty, facilitating robust
exploration of the reasoning space. As part of our framework, we introduce an
on-policy reinforcement learning strategy incorporating epsilon-greedy
exploration and entropy-based regularization to iteratively refine latent state
transitions without requiring additional fine-tuning of the underlying LLM.
Theoretical analyses provide evidence lower bounds (ELBO), theoretically
grounding our transition-aware modeling of latent reasoning dynamics. Further
experiments demonstrate improvements in reasoning accuracy, diversity, and
exploration efficiency across benchmark reasoning tasks.

</details>


### [160] [EvA: Evolutionary Attacks on Graphs](https://arxiv.org/abs/2507.08212)
*Mohammad Sadegh Akhondzadeh,Soroush H. Zargarbashi,Jimin Cao,Aleksandar Bojchevski*

Main category: cs.LG

TL;DR: 针对图神经网络（GNNs）对图结构扰动的脆弱性，本文提出一种名为EvA的基于演化算法的黑盒攻击方法，该方法直接解决离散优化问题，无需梯度信息，并实现了比现有最佳攻击方法高出11%的准确率下降。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）对图结构的微小扰动高度敏感，可能导致其准确率显著下降。现有大多数攻击方法依赖梯度信息，将离散优化问题松弛到连续空间，导致次优解且无法适应不可微分目标。

Method: 本文提出EvA（Evolutionary Attack），一种基于演化算法的增强方法，直接解决图结构扰动的离散优化问题。EvA适用于任何黑盒模型和目标函数，无需可微分的代理损失，并利用其设计了两种新型攻击。该攻击的内存复杂度与攻击预算呈线性关系。

Result: 在实验中，EvA比现有最佳攻击方法的准确率平均额外降低了约11%，表明在设计攻击方面仍有显著的未开发潜力。

Conclusion: EvA作为一种无需梯度且直接解决离散优化问题的黑盒攻击方法，显著提升了图神经网络攻击的有效性，揭示了攻击设计领域的巨大潜力。

Abstract: Even a slight perturbation in the graph structure can cause a significant
drop in the accuracy of graph neural networks (GNNs). Most existing attacks
leverage gradient information to perturb edges. This relaxes the attack's
optimization problem from a discrete to a continuous space, resulting in
solutions far from optimal. It also restricts the adaptability of the attack to
non-differentiable objectives. Instead, we introduce a few simple yet effective
enhancements of an evolutionary-based algorithm to solve the discrete
optimization problem directly. Our Evolutionary Attack (EvA) works with any
black-box model and objective, eliminating the need for a differentiable proxy
loss. This allows us to design two novel attacks that reduce the effectiveness
of robustness certificates and break conformal sets. The memory complexity of
our attack is linear in the attack budget. Among our experiments, EvA shows
$\sim$11\% additional drop in accuracy on average compared to the best previous
attack, revealing significant untapped potential in designing attacks.

</details>


### [161] [InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems](https://arxiv.org/abs/2507.08235)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Rajiv Ramnath*

Main category: cs.LG

TL;DR: 提出InsightBuild框架，结合因果分析和微调大型语言模型（LLM），为智能建筑的异常能耗提供人类可读的因果解释，以帮助设施经理。


<details>
  <summary>Details</summary>
Motivation: 智能建筑虽有大量数据，但设施经理难以获得异常能耗的清晰解释。

Method: InsightBuild是一个两阶段框架：首先，轻量级因果推断模块对建筑遥测数据（如温度、HVAC）进行格兰杰因果检验和结构因果发现；其次，一个经传感器原因和文本解释对齐数据微调的LLM，接收因果关系输入并生成简洁、可操作的解释。

Result: 在Google和Berkeley真实世界数据集上评估，结果表明结合显式因果发现与LLM自然语言生成，可产生清晰、精确的解释。

Conclusion: 该方法通过提供清晰、精确的因果解释，有效协助设施经理诊断并缓解能源效率低下问题。

Abstract: Smart buildings generate vast streams of sensor and control data, but
facility managers often lack clear explanations for anomalous energy usage. We
propose InsightBuild, a two-stage framework that integrates causality analysis
with a fine-tuned large language model (LLM) to provide human-readable, causal
explanations of energy consumption patterns. First, a lightweight causal
inference module applies Granger causality tests and structural causal
discovery on building telemetry (e.g., temperature, HVAC settings, occupancy)
drawn from Google Smart Buildings and Berkeley Office datasets. Next, an LLM,
fine-tuned on aligned pairs of sensor-level causes and textual explanations,
receives as input the detected causal relations and generates concise,
actionable explanations. We evaluate InsightBuild on two real-world datasets
(Google: 2017-2022; Berkeley: 2018-2020), using expert-annotated ground-truth
causes for a held-out set of anomalies. Our results demonstrate that combining
explicit causal discovery with LLM-based natural language generation yields
clear, precise explanations that assist facility managers in diagnosing and
mitigating energy inefficiencies.

</details>


### [162] [Self-Supervised Learning-Based Multimodal Prediction on Prosocial Behavior Intentions](https://arxiv.org/abs/2507.08238)
*Abinay Reddy Naini,Zhaobo K. Zheng,Teruhisa Misu,Kumar Akash*

Main category: cs.LG

TL;DR: 针对出行场景中亲社会行为预测的数据稀缺问题，本文提出一种基于多模态数据的自监督学习方法，通过预训练和微调显著提升了预测性能，并为智能车辆系统提供参考。


<details>
  <summary>Details</summary>
Motivation: 出行场景（如道路互助）中的亲社会行为预测是一个未充分探索的领域。当前研究面临的主要限制是缺乏大规模标注的亲社会行为数据集，导致深度学习模型难以有效训练。

Method: 提出一种自监督学习方法，利用现有生理和行为数据集中的多模态数据进行预训练。随后，使用少量手动标注的亲社会行为数据集对模型进行微调。

Result: 该方法显著提升了亲社会行为预测模型的性能，有效解决了数据稀缺问题，并为亲社会行为预测提供了一个更有效的基准。

Conclusion: 本研究通过自监督学习有效克服了亲社会行为预测中的数据稀缺挑战，提供了更可靠的预测基准，并为改进智能车辆系统和人机交互提供了有价值的见解。

Abstract: Human state detection and behavior prediction have seen significant
advancements with the rise of machine learning and multimodal sensing
technologies. However, predicting prosocial behavior intentions in mobility
scenarios, such as helping others on the road, is an underexplored area.
Current research faces a major limitation. There are no large, labeled datasets
available for prosocial behavior, and small-scale datasets make it difficult to
train deep-learning models effectively. To overcome this, we propose a
self-supervised learning approach that harnesses multi-modal data from existing
physiological and behavioral datasets. By pre-training our model on diverse
tasks and fine-tuning it with a smaller, manually labeled prosocial behavior
dataset, we significantly enhance its performance. This method addresses the
data scarcity issue, providing a more effective benchmark for prosocial
behavior prediction, and offering valuable insights for improving intelligent
vehicle systems and human-machine interaction.

</details>


### [163] [Data Generation without Function Estimation](https://arxiv.org/abs/2507.08239)
*Hadi Daneshmand,Ashkan Soleymani*

Main category: cs.LG

TL;DR: 提出一种无需函数估计的新型生成方法，通过确定性更新粒子位置将均匀分布转换为目标数据分布。


<details>
  <summary>Details</summary>
Motivation: 大多数生成模型依赖于分数函数等密度相关函数的估计，但这在计算和统计上都具有挑战性。研究旨在探索是否能避免这种函数估计来实现数据生成。

Method: 提出一种“无估计”的生成方法。该方法利用相互作用粒子物理学的最新进展，通过逆梯度下降法确定性地更新一组点的空间位置，从而在平均场域内将均匀分布转换为任意数据分布，无需进行函数估计、训练神经网络或注入噪声。

Result: 理论和实验均表明，相互作用粒子物理学的进展可以被有效利用，以开发出新颖的生成方法。

Conclusion: 成功构建并验证了一种无需函数估计的生成方法，为数据生成提供了一种新的、潜在更高效的途径。

Abstract: Estimating the score function (or other population-density-dependent
functions) is a fundamental component of most generative models. However, such
function estimation is computationally and statistically challenging. Can we
avoid function estimation for data generation? We propose an estimation-free
generative method: A set of points whose locations are deterministically
updated with (inverse) gradient descent can transport a uniform distribution to
arbitrary data distribution, in the mean field regime, without function
estimation, training neural networks, and even noise injection. The proposed
method is built upon recent advances in the physics of interacting particles.
We show, both theoretically and experimentally, that these advances can be
leveraged to develop novel generative methods.

</details>


### [164] [CoreSPECT: Enhancing Clustering Algorithms via an Interplay of Density and Geometry](https://arxiv.org/abs/2507.08243)
*Chandra Sekhar Mukherjee,Joonyoung Bae,Jiapeng Zhang*

Main category: cs.LG

TL;DR: 提出CoreSPECT框架，通过利用数据分布与几何的相互作用，显著提升K-Means和GMM等简单聚类算法的性能，甚至超越一些复杂算法。


<details>
  <summary>Details</summary>
Motivation: 现有聚类算法通常侧重于数据的密度或几何结构。本研究旨在识别并形式化分布与几何之间常被忽视的相互作用，以期设计新的聚类增强框架来提升现有算法性能。

Method: 开发了CoreSPECT（Core Space Projection-based Enhancement of Clustering Techniques）框架。该框架通过将K-Means和GMM等简单算法应用于策略性选择的核心区域，然后利用基于新型邻域图的多层传播程序，将局部划分扩展到完整数据集的划分。

Result: 在来自三个不同领域的15个数据集上，CoreSPECT框架为K-Means和GMM带来了持续且显著的聚类精度提升。平均而言，它使K-Means的ARI提高了40%，GMM的ARI提高了14%，并且性能常超越流形基和近期密度基聚类算法。研究还提供了初步理论保证、消融实验和对噪声的鲁棒性证据。

Conclusion: CoreSPECT框架成功地利用了数据分布和几何之间的相互作用，有效增强了K-Means和GMM等基础聚类算法的性能，使其能够达到甚至超越一些更复杂聚类算法的水平，展现了其有效性和鲁棒性。

Abstract: Density and geometry have long served as two of the fundamental guiding
principles in clustering algorithm design, with algorithm usually focusing
either on the density structure of the data (e.g., HDBSCAN and Density Peak
Clustering) or the complexity of underlying geometry (e.g., manifold clustering
algorithms).
  In this paper, we identify and formalize a recurring but often overlooked
interaction between distribution and geometry and leverage this insight to
design our clustering enhancement framework CoreSPECT (Core Space
Projection-based Enhancement of Clustering Techniques). Our framework boosts
the performance of simple algorithms like K-Means and GMM by applying them to
strategically selected regions, then extending the partial partition to a
complete partition for the dataset using a novel neighborhood graph based
multi-layer propagation procedure.
  We apply our framework on 15 datasets from three different domains and obtain
consistent and substantial gain in clustering accuracy for both K-Means and
GMM. On average, our framework improves the ARI of K-Means by 40% and of GMM by
14%, often surpassing the performance of both manifold-based and recent
density-based clustering algorithms. We further support our framework with
initial theoretical guarantees, ablation to demonstrate the usefulness of the
individual steps and with evidence of robustness to noise.

</details>


### [165] [Quantum-Accelerated Neural Imputation with Large Language Models (LLMs)](https://arxiv.org/abs/2507.08255)
*Hossein Jamali*

Main category: cs.LG

TL;DR: 本文提出Quantum-UnIMP框架，将浅层量子电路融入LLM，通过量子特征映射提升混合类型数据缺失值填充性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在表格数据缺失值填充方面表现出色，但其依赖传统嵌入方法，难以捕捉复杂非线性关联，尤其是在混合类型数据中。

Method: 引入Quantum-UnIMP框架，将浅层量子电路集成到基于LLM的填充架构中。核心创新在于使用瞬时量子多项式(IQP)电路生成的量子特征映射取代传统经典输入嵌入，利用量子叠加和纠缠现象学习更丰富的表示。

Result: 在基准混合类型数据集上的实验表明，Quantum-UnIMP对数值特征的填充误差（RMSE）降低高达15.2%，对分类特征的分类准确率（F1-Score）提高8.7%，优于最先进的经典和基于LLM的方法。

Conclusion: 量子增强表示在复杂数据填充任务中展现出巨大潜力，即使在近期量子硬件上也能取得显著成效。

Abstract: Missing data presents a critical challenge in real-world datasets,
significantly degrading the performance of machine learning models. While Large
Language Models (LLMs) have recently demonstrated remarkable capabilities in
tabular data imputation, exemplified by frameworks like UnIMP, their reliance
on classical embedding methods often limits their ability to capture complex,
non-linear correlations, particularly in mixed-type data scenarios encompassing
numerical, categorical, and textual features. This paper introduces
Quantum-UnIMP, a novel framework that integrates shallow quantum circuits into
an LLM-based imputation architecture. Our core innovation lies in replacing
conventional classical input embeddings with quantum feature maps generated by
an Instantaneous Quantum Polynomial (IQP) circuit. This approach enables the
model to leverage quantum phenomena such as superposition and entanglement,
thereby learning richer, more expressive representations of data and enhancing
the recovery of intricate missingness patterns. Our experiments on benchmark
mixed-type datasets demonstrate that Quantum-UnIMP reduces imputation error by
up to 15.2% for numerical features (RMSE) and improves classification accuracy
by 8.7% for categorical features (F1-Score) compared to state-of-the-art
classical and LLM-based methods. These compelling results underscore the
profound potential of quantum-enhanced representations for complex data
imputation tasks, even with near-term quantum hardware.

</details>


### [166] [A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning](https://arxiv.org/abs/2507.08267)
*Hiroshi Yoshihara,Taiki Yamaguchi,Yuichi Inoue*

Main category: cs.LG

TL;DR: 本文提出一种结合长周期监督微调（SFT）和在线推理强化学习（GRPO）的训练方法，显著提升大型语言模型（LLMs）的数学推理能力，实现高精度和高效率。


<details>
  <summary>Details</summary>
Motivation: 提高LLMs的数学推理能力是AI的关键挑战。尽管SFT和RL是主流训练范式，但如何系统地结合两者以同时最大化准确性和效率，仍是一个未被充分探索的领域。

Method: 引入了一种实用且有效的训练方案，策略性地整合了“扩展的SFT”（多达10个Epoch）和“来自在线推理的强化学习（GRPO）”。该方法认为SFT和GRPO是互补的：SFT阶段旨在将模型准确性推至极限，而GRPO阶段则在保持此峰值性能的同时，显著提高Token效率。

Result: 实验结果显示，将SFT扩展到多达10个Epoch对性能突破至关重要，而GRPO在该框架中的主要作用是优化解决方案长度（Token效率）。该方案在挑战性基准测试中表现出色，包括在严格无泄露的AI数学奥林匹克竞赛（AIMO）中，在超过2200支队伍中获得了高排名。

Conclusion: 该工作为社区提供了一个经过实战验证的蓝图，用于开发兼具卓越准确性和实际效率的顶尖数学推理器。为确保可复现性并促进未来研究，所有代码、模型和训练配置将开源。

Abstract: Enhancing the mathematical reasoning of Large Language Models (LLMs) is a
pivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning
(SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a
systematic methodology for combining them to maximize both accuracy and
efficiency remains largely unexplored. This paper introduces a practical and
effective training recipe that strategically integrates extended SFT with RL
from online inference (GRPO). We posit that these methods play complementary,
not competing, roles: a prolonged SFT phase first pushes the model's accuracy
to its limits, after which a GRPO phase dramatically improves token efficiency
while preserving this peak performance. Our experiments reveal that extending
SFT for as many as 10 epochs is crucial for performance breakthroughs, and that
the primary role of GRPO in this framework is to optimize solution length. The
efficacy of our recipe is rigorously validated through top-tier performance on
challenging benchmarks, including a high rank among over 2,200 teams in the
strictly leak-free AI Mathematical Olympiad (AIMO). This work provides the
community with a battle-tested blueprint for developing state-of-the-art
mathematical reasoners that are both exceptionally accurate and practically
efficient. To ensure full reproducibility and empower future research, we will
open-source our entire framework, including all code, model checkpoints, and
training configurations at
https://github.com/analokmaus/kaggle-aimo2-fast-math-r1.

</details>


### [167] [Data-Driven Dimensional Synthesis of Diverse Planar Four-bar Function Generation Mechanisms via Direct Parameterization](https://arxiv.org/abs/2507.08269)
*Woon Ryong Kim,Jaeheun Jung,Jeong Un Ha,Donghun Lee,Jae Kyung Shim*

Main category: cs.LG

TL;DR: 本文提出一种数据驱动框架，利用监督学习、LSTM和MoE架构，实现平面四杆机构的尺寸综合，替代传统方法，提高设计效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 平面四杆机构的尺寸综合是一个具有挑战性的运动学逆问题，需要根据期望运动规格确定机构尺寸。传统方法涉及复杂的方程求解和优化，限制了非专业用户的使用和设计效率。

Method: 本研究提出一个数据驱动框架，通过监督学习绕过传统的方程求解和优化。该方法结合了：1. 合成数据集；2. 基于LSTM的神经网络处理序列精度点；3. 针对不同连杆类型的专家混合（MoE）架构。每个专家模型在特定类型数据上训练，并由类型指定层引导，支持单类型和多类型综合。同时，引入了一种新的仿真度量来评估预测质量。

Result: 实验结果表明，该方法能够生成各种配置下准确、无缺陷的连杆机构。它成功地绕过了传统的求解和优化过程，实现了高效的机构设计。

Conclusion: 该数据驱动框架使机构设计更加直观和高效，即使是非专业用户也能使用。它为运动学设计中可扩展和灵活的综合开辟了新的可能性。

Abstract: Dimensional synthesis of planar four-bar mechanisms is a challenging inverse
problem in kinematics, requiring the determination of mechanism dimensions from
desired motion specifications. We propose a data-driven framework that bypasses
traditional equation-solving and optimization by leveraging supervised
learning. Our method combines a synthetic dataset, an LSTM-based neural network
for handling sequential precision points, and a Mixture of Experts (MoE)
architecture tailored to different linkage types. Each expert model is trained
on type-specific data and guided by a type-specifying layer, enabling both
single-type and multi-type synthesis. A novel simulation metric evaluates
prediction quality by comparing desired and generated motions. Experiments show
our approach produces accurate, defect-free linkages across various
configurations. This enables intuitive and efficient mechanism design, even for
non-expert users, and opens new possibilities for scalable and flexible
synthesis in kinematic design.

</details>


### [168] [Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training](https://arxiv.org/abs/2507.08284)
*Aleksei Ilin,Gor Matevosyan,Xueying Ma,Vladimir Eremin,Suhaa Dada,Muqun Li,Riyaaz Shaik,Haluk Noyan Tokgozoglu*

Main category: cs.LG

TL;DR: 该研究提出一种轻量级安全护栏框架，利用小规模语言模型通过高质量合成数据生成和对抗性训练，在内容审核任务上超越大型模型，并降低计算成本、增强抗攻击性。


<details>
  <summary>Details</summary>
Motivation: 为语言模型构建高效、轻量级的安全护栏，并期望小规模模型能在内容审核上达到甚至超越大型模型的性能，同时降低计算开销并提高鲁棒性。

Method: 1. 高保真合成数据生成：从人工精选种子数据开始，通过查询增强和复述生成多样化示例，并进行多轮精选以确保质量。2. 对抗性训练：受GAN启发，采用强化学习引导生成器产生挑战性合成示例，用于微调安全分类器。3. 结合高效LLM训练策略，利用小型模型提升大型生成模型性能。通过迭代对抗训练和多样化高质量合成数据实现。

Result: 小规模语言模型在内容审核任务中能够达到甚至超越大型模型的性能，能作为强大的安全护栏。该方法显著降低了计算开销，并增强了对对抗性攻击的抵御能力。

Conclusion: 该框架为AI系统中的内容审核提供了一个可扩展且高效的解决方案，证明小语言模型可以成为强大、高效的安全护栏，同时降低成本并提高韧性。

Abstract: We introduce a lightweight yet highly effective safety guardrail framework
for language models, demonstrating that small-scale language models can
achieve, and even surpass, the performance of larger counterparts in content
moderation tasks. This is accomplished through high-fidelity synthetic data
generation and adversarial training. The synthetic data generation process
begins with human-curated seed data, which undergoes query augmentation and
paraphrasing to create diverse and contextually rich examples. This augmented
data is then subjected to multiple rounds of curation, ensuring high fidelity
and relevance. Inspired by recent advances in the Generative Adversarial
Network (GAN) architecture, our adversarial training employs reinforcement
learning to guide a generator that produces challenging synthetic examples.
These examples are used to fine-tune the safety classifier, enhancing its
ability to detect and mitigate harmful content. Additionally, we incorporate
strategies from recent research on efficient LLM training, leveraging the
capabilities of smaller models to improve the performance of larger generative
models. With iterative adversarial training and the generation of diverse,
high-quality synthetic data, our framework enables small language models (SLMs)
to serve as robust safety guardrails. This approach not only reduces
computational overhead but also enhances resilience against adversarial
attacks, offering a scalable and efficient solution for content moderation in
AI systems.

</details>


### [169] [CAS Condensed and Accelerated Silhouette: An Efficient Method for Determining the Optimal K in K-Means Clustering](https://arxiv.org/abs/2507.08311)
*Krishnendu Das,Sumit Gupta,Awadhesh Kumar*

Main category: cs.LG

TL;DR: 本文综述了聚类中k值选择策略，并提出一种基于Condensed Silhouette、统计方法及CCR/COI的改进算法，旨在提高大数据集聚类精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在大数据集环境中，聚类准确性与计算效率的平衡是一个重大挑战，尤其在文本和图像数据处理中，急需优化k值选择和提升聚类性能。

Method: 提出一种改进方法，基于Condensed Silhouette，结合Local Structures、Gap Statistics、Class Consistency Ratio (CCR) 和 Cluster Overlap Index (COI) 等统计方法，设计CCR和COI算法来计算K-Means聚类的最佳k值。

Result: 在保持精度和可扩展性的同时，所提方法在高维数据集上实现了高达99%的执行速度提升。

Conclusion: 该方法高度适用于实时聚类或需要高效聚类且资源利用率低的场景，具有显著的性能优势。

Abstract: Clustering is a critical component of decision-making in todays data-driven
environments. It has been widely used in a variety of fields such as
bioinformatics, social network analysis, and image processing. However,
clustering accuracy remains a major challenge in large datasets. This paper
presents a comprehensive overview of strategies for selecting the optimal value
of k in clustering, with a focus on achieving a balance between clustering
precision and computational efficiency in complex data environments. In
addition, this paper introduces improvements to clustering techniques for text
and image data to provide insights into better computational performance and
cluster validity. The proposed approach is based on the Condensed Silhouette
method, along with statistical methods such as Local Structures, Gap
Statistics, Class Consistency Ratio, and a Cluster Overlap Index CCR and
COIbased algorithm to calculate the best value of k for K-Means clustering. The
results of comparative experiments show that the proposed approach achieves up
to 99 percent faster execution times on high-dimensional datasets while
retaining both precision and scalability, making it highly suitable for real
time clustering needs or scenarios demanding efficient clustering with minimal
resource utilization.

</details>


### [170] [A Comprehensively Adaptive Architectural Optimization-Ingrained Quantum Neural Network Model for Cloud Workloads Prediction](https://arxiv.org/abs/2507.08317)
*Jitendra Kumar,Deepika Saxena,Kishu Gupta,Satyam Kumar,Ashutosh Kumar Singh*

Main category: cs.LG

TL;DR: 本文提出一种名为CA-QNN的新型变体量子神经网络，通过结合量子计算和全面的结构及参数优化，显著提高了动态云服务工作负载的预测精度，克服了传统深度学习方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的神经网络和深度学习模型在处理多样化、高维度的动态云服务工作负载（尤其是在资源需求突变时）效率低下，其主要原因在于训练优化受限，仅限于基于传统算法的参数调整。

Method: 本文提出一种基于全面自适应架构优化的变体量子神经网络（CA-QNN）。该模型将工作负载数据转换为量子比特，通过带有受控非门激活函数的量子比特神经元进行处理。此外，引入了一种综合架构优化算法，用于促进可变大小QNN的结构和参数值的学习与传播，该算法在训练过程中结合了量子自适应调制和尺寸自适应重组。

Result: CA-QNN模型在四个异构云工作负载基准数据集上与七种现有先进方法进行了全面比较，结果显示其预测精度显著优越，相较于现有深度学习和基于QNN的方法，预测误差分别降低了高达93.40%和91.27%。

Conclusion: CA-QNN模型通过结合量子计算和全面的结构与参数自适应学习，有效解决了动态云工作负载预测的挑战，实现了卓越的预测性能，显著优于现有方法。

Abstract: Accurate workload prediction and advanced resource reservation are
indispensably crucial for managing dynamic cloud services. Traditional neural
networks and deep learning models frequently encounter challenges with diverse,
high-dimensional workloads, especially during sudden resource demand changes,
leading to inefficiencies. This issue arises from their limited optimization
during training, relying only on parametric (inter-connection weights)
adjustments using conventional algorithms. To address this issue, this work
proposes a novel Comprehensively Adaptive Architectural Optimization-based
Variable Quantum Neural Network (CA-QNN), which combines the efficiency of
quantum computing with complete structural and qubit vector parametric
learning. The model converts workload data into qubits, processed through qubit
neurons with Controlled NOT-gated activation functions for intuitive pattern
recognition. In addition, a comprehensive architecture optimization algorithm
for networks is introduced to facilitate the learning and propagation of the
structure and parametric values in variable-sized QNNs. This algorithm
incorporates quantum adaptive modulation and size-adaptive recombination during
training process. The performance of CA-QNN model is thoroughly investigated
against seven state-of-the-art methods across four benchmark datasets of
heterogeneous cloud workloads. The proposed model demonstrates superior
prediction accuracy, reducing prediction errors by up to 93.40% and 91.27%
compared to existing deep learning and QNN-based approaches.

</details>


### [171] [scE$^2$TM: Toward Interpretable Single-Cell Embedding via Topic Modeling](https://arxiv.org/abs/2507.08355)
*Hegang Chen,Yuyin Lu,Zhiming Dai,Fu Lee Wang,Qing Li,Yanghui Rao*

Main category: cs.LG

TL;DR: 本文提出scE2TM，一个结合外部生物知识的单细胞嵌入式主题模型，用于单细胞RNA测序数据分析，旨在提供高质量的细胞嵌入和强大的可解释性，并通过新指标定量评估可解释性，显著提升聚类性能并揭示生物机制。


<details>
  <summary>Details</summary>
Motivation: 单细胞测序技术使细胞异质性研究成为可能，而深度学习模型日益复杂，可解释性变得尤为重要。现有单细胞嵌入式主题模型在可解释性评估上主要依赖定性分析，存在解释坍塌问题，且忽视外部生物知识，限制了分析性能。

Method: 本文提出了scE2TM模型，一个外部知识引导的单细胞嵌入式主题模型。该模型旨在提供高质量的细胞嵌入和强大的可解释性。研究团队在20个单细胞RNA测序数据集上对scE2TM进行了综合评估，并将其聚类性能与7种现有SOTA方法进行比较。此外，还提出了一个新的可解释性评估基准，引入10个指标来定量评估单细胞嵌入式主题模型的可解释性。

Result: scE2TM在聚类性能上显著优于7种现有SOTA方法。其提供的解释在多样性和与潜在生物信号的一致性方面表现出色，有助于更好地揭示潜在的生物机制。

Conclusion: scE2TM通过结合外部知识，提供了高质量的细胞嵌入和强大的可解释性，显著提升了单细胞RNA测序数据的聚类分析性能，并能更好地揭示生物机制。提出的可解释性评估基准也为未来研究提供了有价值的定量评估工具。

Abstract: Recent advances in sequencing technologies have enabled researchers to
explore cellular heterogeneity at single-cell resolution. Meanwhile,
interpretability has gained prominence parallel to the rapid increase in the
complexity and performance of deep learning models. In recent years, topic
models have been widely used for interpretable single-cell embedding learning
and clustering analysis, which we refer to as single-cell embedded topic
models. However, previous studies evaluated the interpretability of the models
mainly through qualitative analysis, and these single-cell embedded topic
models suffer from the potential problem of interpretation collapse.
Furthermore, their neglect of external biological knowledge constrains
analytical performance. Here, we present scE2TM, an external knowledge-guided
single-cell embedded topic model that provides a high-quality cell embedding
and strong interpretation, contributing to comprehensive scRNA-seq data
analysis. Our comprehensive evaluation across 20 scRNA-seq datasets
demonstrates that scE2TM achieves significant clustering performance gains
compared to 7 state-of-the-art methods. In addition, we propose a new
interpretability evaluation benchmark that introduces 10 metrics to
quantitatively assess the interpretability of single-cell embedded topic
models. The results show that the interpretation provided by scE2TM performs
encouragingly in terms of diversity and consistency with the underlying
biological signals, contributing to a better revealing of the underlying
biological mechanisms.

</details>


### [172] [Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN Model Generation from Text](https://arxiv.org/abs/2507.08362)
*Phuong Nam Lê,Charlotte Schneider-Depré,Alexandre Goossens,Alexander Stevens,Aurélie Leribaux,Johannes De Smedt*

Main category: cs.LG

TL;DR: 本文提出一个自动化流程，利用机器学习和大型语言模型从文本中提取BPMN模型，并引入一个新注释的数据集以更好地识别并行结构，从而加速BPMN模型的创建。


<details>
  <summary>Details</summary>
Motivation: 将文本流程文档转换为BPMN模型对高效规划和资源管理至关重要，但当前转换过程耗时且成本高昂。现有方法难以处理不同写作风格，尤其无法识别并行结构。

Method: 引入一个自动化流程，结合机器学习和大型语言模型。关键在于引入新注释的数据集，特别是通过增加15个包含32个并行网关的文档来增强PET数据集，以提升模型捕获并行结构的能力。

Result: 所提出的方法在重建准确性方面表现出足够的性能，并使模型能够更好地捕获并行结构。

Conclusion: 该方法为组织加速BPMN模型的创建提供了一个有前景的基础。

Abstract: Efficient planning, resource management, and consistent operations often rely
on converting textual process documents into formal Business Process Model and
Notation (BPMN) models. However, this conversion process remains time-intensive
and costly. Existing approaches, whether rule-based or machine-learning-based,
still struggle with writing styles and often fail to identify parallel
structures in process descriptions.
  This paper introduces an automated pipeline for extracting BPMN models from
text, leveraging the use of machine learning and large language models. A key
contribution of this work is the introduction of a newly annotated dataset,
which significantly enhances the training process. Specifically, we augment the
PET dataset with 15 newly annotated documents containing 32 parallel gateways
for model training, a critical feature often overlooked in existing datasets.
This addition enables models to better capture parallel structures, a common
but complex aspect of process descriptions. The proposed approach demonstrates
adequate performance in terms of reconstruction accuracy, offering a promising
foundation for organizations to accelerate BPMN model creation.

</details>


### [173] [Prediction of Lane Change Intentions of Human Drivers using an LSTM, a CNN and a Transformer](https://arxiv.org/abs/2507.08365)
*Francesco De Cristofaro,Felix Hofbaur,Aixi Yang,Arno Eichberger*

Main category: cs.LG

TL;DR: 该论文比较了LSTM、CNN和Transformer网络在预测人类驾驶员变道意图方面的性能，发现Transformer网络表现最佳，且不易过拟合，准确率可达82.79%至96.73%。


<details>
  <summary>Details</summary>
Motivation: 预测前车变道对自动驾驶的运动规划、安全性和效率至关重要。现有研究在固定时间间隔内预测变道较少，且缺乏对不同神经网络架构的系统性比较以及输入数据选择的评估。

Method: 本研究描述并实现了LSTM、CNN和Transformer三种神经网络架构，用于预测人类驾驶员的变道意图。研究使用了公开数据集highD进行数据准备和特征选择，并设计了不同的网络配置和输入数据组合，最终比较了这些网络的预测结果。

Result: 实验结果表明，Transformer网络在预测性能上优于LSTM和CNN，并且受过拟合的影响较小。该方法在不同输入配置下的准确率范围为82.79%至96.73%，同时在精确率和召回率方面也表现出良好的性能。

Conclusion: Transformer网络是预测人类驾驶员变道意图的有效且鲁棒的架构，其性能优于LSTM和CNN，为自动驾驶系统提供高准确度和可靠的变道预测能力。

Abstract: Lane changes of preceding vehicles have a great impact on the motion planning
of automated vehicles especially in complex traffic situations. Predicting them
would benefit the public in terms of safety and efficiency. While many research
efforts have been made in this direction, few concentrated on predicting
maneuvers within a set time interval compared to predicting at a set prediction
time. In addition, there exist a lack of comparisons between different
architectures to try to determine the best performing one and to assess how to
correctly choose the input for such models. In this paper the structure of an
LSTM, a CNN and a Transformer network are described and implemented to predict
the intention of human drivers to perform a lane change. We show how the data
was prepared starting from a publicly available dataset (highD), which features
were used, how the networks were designed and finally we compare the results of
the three networks with different configurations of input data. We found that
transformer networks performed better than the other networks and was less
affected by overfitting. The accuracy of the method spanned from $82.79\%$ to
$96.73\%$ for different input configurations and showed overall good
performances considering also precision and recall.

</details>


### [174] [Advances in Machine Learning: Where Can Quantum Techniques Help?](https://arxiv.org/abs/2507.08379)
*Samarth Kashyap,Rohit K Ramakrishnan,Kumari Jyoti,Apoorva D Patel*

Main category: cs.LG

TL;DR: 该综述探讨了量子机器学习（QML）如何解决经典机器学习的计算瓶颈，介绍了其理论基础、现有进展及面临的挑战（特别是NISQ设备），并指出其在特定领域（如量子化学、传感）的潜力，但广泛应用仍需克服技术障碍。


<details>
  <summary>Details</summary>
Motivation: 解决经典机器学习在处理复杂数据集时面临的计算瓶颈，利用量子计算优势提升数据驱动任务的性能。

Method: 综述了QML的理论基础（包括量子数据编码、学习理论和优化技术），对QML方法进行了分类，并对量子主成分分析、量子增强传感、材料科学应用等关键进展的理论加速和实际限制进行了评估。同时，详细讨论了NISQ设备带来的挑战。

Result: 量子计算优势具有问题依赖性，需要系统性识别有用的QML方向。评估显示，QML关键发展在理论上具有加速潜力，但也存在实际限制。NISQ设备面临硬件噪声、可扩展性和数据编码开销等挑战。

Conclusion: QML在量子化学和传感等特定应用中具有显著潜力，但其在更广泛的实际场景中的实用性，仍取决于克服技术和方法上的障碍。未来发展需要量子原生算法、改进的纠错和实际基准。

Abstract: Quantum Machine Learning (QML) represents a promising frontier at the
intersection of quantum computing and artificial intelligence, aiming to
leverage quantum computational advantages to enhance data-driven tasks. This
review explores the potential of QML to address the computational bottlenecks
of classical machine learning, particularly in processing complex datasets. We
introduce the theoretical foundations of QML, including quantum data encoding,
quantum learning theory and optimization techniques, while categorizing QML
approaches based on data type and computational architecture. It is
well-established that quantum computational advantages are problem-dependent,
and so potentially useful directions for QML need to be systematically
identified. Key developments, such as Quantum Principal Component Analysis,
quantum-enhanced sensing and applications in material science, are critically
evaluated for their theoretical speed-ups and practical limitations. The
challenges posed by Noisy Intermediate-Scale Quantum (NISQ) devices, including
hardware noise, scalability constraints and data encoding overheads, are
discussed in detail. We also outline future directions, emphasizing the need
for quantum-native algorithms, improved error correction, and realistic
benchmarks to bridge the gap between theoretical promise and practical
deployment. This comprehensive analysis underscores that while QML has
significant potential for specific applications such as quantum chemistry and
sensing, its broader utility in real-world scenarios remains contingent on
overcoming technological and methodological hurdles.

</details>


### [175] [Two-cluster test](https://arxiv.org/abs/2507.08382)
*Xinying Liu,Lianyu Hu,Mudi Jiang,Simen Zhang,Jun Lou,Zengyou He*

Main category: cs.LG

TL;DR: 本文提出一种新的“双簇检验”方法，用于判断聚类中两个子集是否来自同一簇。该方法解决了传统双样本检验在此场景下I型错误率过高的问题，并通过实验验证了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 在现代聚类方法中，需要判断两个样本子集是否来自同一个簇。然而，由于这些子集通常由聚类过程生成，直接应用经典的双样本检验会导致极小的p值，从而使I型错误率膨胀。因此，需要一种专门的、不同于传统双样本检验的方法来解决这个问题。

Method: 提出一种基于两个子集之间“边界点”的新方法，以推导出一个分析性的p值，用于量化统计显著性，从而实现双簇检验。

Result: 实验表明，与几种经典的双样本检验方法相比，所提出的双簇检验方法能够显著降低I型错误率。此外，该方法的实用性通过其在基于树的可解释聚类和基于显著性的层次聚类中的应用得到了进一步验证。

Conclusion: 所提出的双簇检验方法成功解决了传统双样本检验在聚类场景中存在的I型错误率过高问题，提供了更准确的显著性量化。其在多种聚类应用中的有效性和实用性也得到了证实。

Abstract: Cluster analysis is a fundamental research issue in statistics and machine
learning. In many modern clustering methods, we need to determine whether two
subsets of samples come from the same cluster. Since these subsets are usually
generated by certain clustering procedures, the deployment of classic
two-sample tests in this context would yield extremely smaller p-values,
leading to inflated Type-I error rate. To overcome this bias, we formally
introduce the two-cluster test issue and argue that it is a totally different
significance testing issue from conventional two-sample test. Meanwhile, we
present a new method based on the boundary points between two subsets to derive
an analytical p-value for the purpose of significance quantification.
Experiments on both synthetic and real data sets show that the proposed test is
able to significantly reduce the Type-I error rate, in comparison with several
classic two-sample testing methods. More importantly, the practical usage of
such two-cluster test is further verified through its applications in
tree-based interpretable clustering and significance-based hierarchical
clustering.

</details>


### [176] [Online Pre-Training for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2507.08387)
*Yongjae Shin,Jeonghye Kim,Whiyoung Jung,Sunghoon Hong,Deunsol Yoon,Youngsoo Jang,Geonhyeong Kim,Jongseong Chae,Youngchul Sung,Kanghoon Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: 针对离线到在线强化学习中预训练智能体在线微调时因分布偏移导致的价值估计不准确问题，本文提出了一种新的在线预训练方法（OPT），通过训练新的价值函数，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 离线预训练的强化学习智能体在在线微调阶段，由于分布偏移导致价值估计不准确，性能常常不佳，甚至不如随机初始化。

Method: 提出了一种名为“在线预训练离线到在线强化学习”（OPT）的新方法。OPT引入了一个新的“在线预训练”阶段，专门用于训练一个为有效在线微调量身定制的新价值函数。

Result: 将OPT应用于TD3和SPOT算法，在MuJoCo、Antmaze和Adroit等D4RL环境中，平均性能提升了30%。

Conclusion: OPT有效解决了离线到在线强化学习中预训练智能体价值估计不准确的问题，显著提升了在线微调的性能。

Abstract: Offline-to-online reinforcement learning (RL) aims to integrate the
complementary strengths of offline and online RL by pre-training an agent
offline and subsequently fine-tuning it through online interactions. However,
recent studies reveal that offline pre-trained agents often underperform during
online fine-tuning due to inaccurate value estimation caused by distribution
shift, with random initialization proving more effective in certain cases. In
this work, we propose a novel method, Online Pre-Training for Offline-to-Online
RL (OPT), explicitly designed to address the issue of inaccurate value
estimation in offline pre-trained agents. OPT introduces a new learning phase,
Online Pre-Training, which allows the training of a new value function tailored
specifically for effective online fine-tuning. Implementation of OPT on TD3 and
SPOT demonstrates an average 30% improvement in performance across a wide range
of D4RL environments, including MuJoCo, Antmaze, and Adroit.

</details>


### [177] [Inference-Time Scaling of Diffusion Language Models with Particle Gibbs Sampling](https://arxiv.org/abs/2507.08390)
*Meihua Dang,Jiaqi Han,Minkai Xu,Kai Xu,Akash Srivastava,Stefano Ermon*

Main category: cs.LG

TL;DR: 提出一种基于粒子Gibbs采样的离散扩散模型推理时间扩展方法，以提升奖励引导下的文本生成质量。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在语言建模中训练时间扩展性良好，但其推理时间扩展性，尤其是在奖励引导设置下生成高质量文本的能力，仍未被充分探索。

Method: 引入一种新颖的、基于粒子Gibbs采样的推理时间扩展方法。该算法利用条件序列蒙特卡洛作为转换机制，迭代细化完整的扩散轨迹，并支持跨多条轨迹的迭代改进。同时，分析了在固定计算预算下，粒子Gibbs迭代次数、粒子数量、去噪步数和奖励估计成本之间的权衡。

Result: 经验证明，该方法在奖励引导的文本生成任务上持续优于现有的推理时间策略，在不同计算预算下均显著提高了准确性。

Conclusion: 所提出的基于粒子Gibbs采样的推理时间扩展方法能有效提升离散扩散模型在奖励引导设置下的文本生成质量，为该领域的优化提供了新途径。

Abstract: Discrete diffusion models have emerged as a powerful paradigm for language
modeling, rivaling auto-regressive models by training-time scaling. However,
inference-time scaling in discrete diffusion models remains relatively
under-explored. In this work, we study sampling-based approaches for achieving
high-quality text generation from discrete diffusion models in reward-guided
settings. We introduce a novel inference-time scaling approach based on
particle Gibbs sampling for discrete diffusion models. The particle Gibbs
sampling algorithm iteratively refines full diffusion trajectories using
conditional Sequential Monte Carlo as its transition mechanism. This process
ensures that the updated samples progressively improve and move closer to the
reward-weighted target distribution. Unlike existing inference-time scaling
methods, which are often limited to single diffusion trajectories, our approach
leverages iterative refinement across multiple trajectories. Within this
framework, we further analyze the trade-offs between four key axes for
inference-time scaling under fixed compute budgets: particle Gibbs iterations,
particle count, denoising steps, and reward estimation cost. Empirically, our
method consistently outperforms prior inference-time strategies on
reward-guided text generation tasks, achieving significant improvement in
accuracy under varying compute budgets.

</details>


### [178] [RTNinja: a generalized machine learning framework for analyzing random telegraph noise signals in nanoelectronic devices](https://arxiv.org/abs/2507.08424)
*Anirudh Varanasi,Robin Degraeve,Philippe Roussel,Clement Merckling*

Main category: cs.LG

TL;DR: RTNinja是一个全自动机器学习框架，用于无监督分析纳米电子器件中的随机电报噪声，能高精度识别隐藏噪声源的特征。


<details>
  <summary>Details</summary>
Motivation: 随机电报噪声（RTN）严重影响纳米电子器件的可靠性与性能。现有分析方法受限于假设或需人工干预，难以处理复杂、高噪声数据。

Method: 提出RTNinja框架，通过无监督机器学习自动分析RTN信号。其包含LevelsExtractor（贝叶斯推断去噪与离散化）和SourcesMapper（概率聚类与优化推断源配置），无需先验知识即可解耦复杂信号，识别隐藏源的数量和特征。

Result: 在7000个由蒙特卡洛模拟器生成的标记数据集（涵盖广泛信噪比和源复杂度）上，RTNinja持续展现出高保真信号重构能力，并能准确提取源振幅与活动模式。

Conclusion: RTNinja为RTN表征提供了一个鲁棒、可扩展、与器件无关的工具，支持下一代纳米电子学在可靠性、故障预测和器件物理等方面的应用。

Abstract: Random telegraph noise is a prevalent variability phenomenon in
nanoelectronic devices, arising from stochastic carrier exchange at defect
sites and critically impacting device reliability and performance. Conventional
analysis techniques often rely on restrictive assumptions or manual
interventions, limiting their applicability to complex, noisy datasets. Here,
we introduce RTNinja, a generalized, fully automated machine learning framework
for the unsupervised analysis of random telegraph noise signals. RTNinja
deconvolves complex signals to identify the number and characteristics of
hidden individual sources, without requiring prior knowledge of the system. The
framework comprises two modular components: LevelsExtractor, which uses
Bayesian inference and model selection to denoise and discretize the signal;
and SourcesMapper, which infers source configurations through probabilistic
clustering and optimization. To evaluate performance, we developed a Monte
Carlo simulator that generates labeled datasets spanning broad signal-to-noise
ratios and source complexities; across 7000 such datasets, RTNinja consistently
demonstrated high-fidelity signal reconstruction and accurate extraction of
source amplitudes and activity patterns. Our results demonstrate that RTNinja
offers a robust, scalable, and device-agnostic tool for random telegraph noise
characterization, enabling large-scale statistical benchmarking,
reliability-centric technology qualification, predictive failure modeling, and
device physics exploration in next-generation nanoelectronics.

</details>


### [179] [KGRAG-Ex: Explainable Retrieval-Augmented Generation with Knowledge Graph-based Perturbations](https://arxiv.org/abs/2507.08443)
*Georgios Balanos,Evangelos Chasanis,Konstantinos Skianis,Evaggelia Pitoura*

Main category: cs.LG

TL;DR: KGRAG-Ex是一个结合知识图谱的RAG系统，通过引入结构化信息和基于扰动的解释方法，提升了RAG的事实依据和可解释性。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）在处理非结构化文本时，其可解释性是一个关键挑战。知识图谱（KGs）通过提供结构化、语义丰富的实体及其关系表示，有望实现透明的检索路径和可解释的推理，从而解决RAG的可解释性问题。

Method: 本文提出KGRAG-Ex系统，该系统利用通过提示式信息抽取构建的领域专用知识图谱。针对用户查询，KGRAG-Ex识别图谱中的相关实体和语义路径，并将其转换为伪段落以引导语料检索。为提高可解释性和推理透明度，系统融入了基于扰动的解释方法，用于评估特定知识图谱派生组件对生成答案的影响。

Result: 研究通过一系列实验分析了系统对不同扰动方法的敏感性、图组件重要性与结构位置之间的关系、语义节点类型的影响，以及图指标如何与组件在解释过程中的影响相对应。

Conclusion: KGRAG-Ex提供了一种利用知识图谱增强RAG事实依据和可解释性的新方法，并通过实验深入分析了其解释机制的关键特性。

Abstract: Retrieval-Augmented Generation (RAG) enhances language models by grounding
responses in external information, yet explainability remains a critical
challenge, particularly when retrieval relies on unstructured text. Knowledge
graphs (KGs) offer a solution by introducing structured, semantically rich
representations of entities and their relationships, enabling transparent
retrieval paths and interpretable reasoning. In this work, we present KGRAG-Ex,
a RAG system that improves both factual grounding and explainability by
leveraging a domain-specific KG constructed via prompt-based information
extraction. Given a user query, KGRAG-Ex identifies relevant entities and
semantic paths in the graph, which are then transformed into pseudo-paragraphs:
natural language representations of graph substructures that guide corpus
retrieval. To improve interpretability and support reasoning transparency, we
incorporate perturbation-based explanation methods that assess the influence of
specific KG-derived components on the generated answers. We conduct a series of
experiments to analyze the sensitivity of the system to different perturbation
methods, the relationship between graph component importance and their
structural positions, the influence of semantic node types, and how graph
metrics correspond to the influence of components within the explanations
process.

</details>


### [180] [Space filling positionality and the Spiroformer](https://arxiv.org/abs/2507.08456)
*M. Maurin,M. Á. Evangelista-Alvarado,P. Suárez-Serrato*

Main category: cs.LG

TL;DR: 转换器在几何域中缺乏全局顺序，作者提出使用空间填充曲线来定义注意力头的顺序，并以Spiroformer为例进行验证。


<details>
  <summary>Details</summary>
Motivation: 转换器擅长处理序列数据，但当将其泛化到没有明确全局顺序的几何域（如流形）时，会遇到挑战。

Method: 提出了一种解决方案，即让注意力头遵循空间填充曲线来定义顺序。

Result: 作为第一个实验示例，作者提出了Spiroformer，一个在2-球体上遵循极性螺旋的转换器。

Conclusion: 通过利用空间填充曲线，可以在几何域中成功应用转换器模型，Spiroformer证明了这一方法的潜力。

Abstract: Transformers excel when dealing with sequential data. Generalizing
transformer models to geometric domains, such as manifolds, we encounter the
problem of not having a well-defined global order. We propose a solution with
attention heads following a space-filling curve. As a first experimental
example, we present the Spiroformer, a transformer that follows a polar spiral
on the $2$-sphere.

</details>


### [181] [Ranked Set Sampling-Based Multilayer Perceptron: Improving Generalization via Variance-Based Bounds](https://arxiv.org/abs/2507.08465)
*Feijiang Li,Liuya Zhang,Jieting Wang,Tao Yan,Yuhua Qian*

Main category: cs.LG

TL;DR: 本文提出RSS-MLP方法，通过引入秩集抽样（RSS）替代简单随机抽样（SRS），有效降低经验损失方差，从而提升多层感知机（MLP）的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多层感知机（MLP）的泛化能力受经验损失方差影响。现有Bagging方法采用的简单随机抽样（SRS）随机性高，未能充分降低方差，因此需要一种更有效的方差降低方法来提升MLP性能。

Method: 1. 建立了一个新的泛化误差界限，揭示经验损失方差对泛化能力的影响。2. 引入秩集抽样（RSS）来取代传统的SRS方法，以降低训练数据集的随机性并进一步减少经验损失方差。3. 开发了基于RSS的MLP学习模型（RSS-MLP）。

Result: 理论分析表明，RSS估计的经验指数损失和逻辑损失的方差均小于SRS。在十二个基准数据集上进行的广泛实验验证了RSS-MLP方法的有效性和合理性。

Conclusion: RSS-MLP通过引入秩集抽样有效降低了经验损失的方差，从而显著提升了多层感知机的泛化能力和整体性能。

Abstract: Multilayer perceptron (MLP), one of the most fundamental neural networks, is
extensively utilized for classification and regression tasks. In this paper, we
establish a new generalization error bound, which reveals how the variance of
empirical loss influences the generalization ability of the learning model.
Inspired by this learning bound, we advocate to reduce the variance of
empirical loss to enhance the ability of MLP. As is well-known, bagging is a
popular ensemble method to realize variance reduction. However, bagging
produces the base training data sets by the Simple Random Sampling (SRS)
method, which exhibits a high degree of randomness. To handle this issue, we
introduce an ordered structure in the training data set by Rank Set Sampling
(RSS) to further reduce the variance of loss and develop a RSS-MLP method.
Theoretical results show that the variance of empirical exponential loss and
the logistic loss estimated by RSS are smaller than those estimated by SRS,
respectively. To validate the performance of RSS-MLP, we conduct comparison
experiments on twelve benchmark data sets in terms of the two convex loss
functions under two fusion methods. Extensive experimental results and analysis
illustrate the effectiveness and rationality of the propose method.

</details>


### [182] [Pre-Training LLMs on a budget: A comparison of three optimizers](https://arxiv.org/abs/2507.08472)
*Joel Schlotthauer,Christian Kroos,Chris Hinze,Viktor Hangya,Luzian Hahn,Fabian Küch*

Main category: cs.LG

TL;DR: 本研究比较了AdamW、Lion和Sophia在LLM预训练中的表现，发现Sophia损失最低，Lion训练最快，而AdamW在下游任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 优化器在缩短大型语言模型（LLM）的预训练时间并获得高性能模型方面具有决定性作用。本研究旨在比较主流优化器AdamW、Lion和Sophia的性能。

Method: 研究比较了事实标准AdamW、通过进化搜索开发的Lion以及二阶优化器Sophia。为提升泛化性，实验使用两种不同的基础架构，并采用单周期和多周期训练方法，同时保持训练tokens数恒定。超参数通过最大更新参数化（Maximal Update Parametrization）和小型代理模型，为每种架构与优化器的组合单独进行调优。

Result: 研究发现，尽管三种优化器的结果大致在同一范围内，但Sophia展现出最低的训练和验证损失；Lion在训练GPU小时数方面最快；而AdamW则在下游评估结果中表现最佳。

Conclusion: 优化器的选择涉及性能权衡：Sophia在降低损失方面表现出色，Lion在训练速度上占优，而AdamW则在下游任务性能上更胜一筹，表明没有一个单一的优化器能在所有指标上都达到最优。

Abstract: Optimizers play a decisive role in reducing pre-training times for LLMs and
achieving better-performing models. In this study, we compare three major
variants: the de-facto standard AdamW, the simpler Lion, developed through an
evolutionary search, and the second-order optimizer Sophia. For better
generalization, we train with two different base architectures and use a
single- and a multiple-epoch approach while keeping the number of tokens
constant. Using the Maximal Update Parametrization and smaller proxy models, we
tune relevant hyperparameters separately for each combination of base
architecture and optimizer. We found that while the results from all three
optimizers were in approximately the same range, Sophia exhibited the lowest
training and validation loss, Lion was fastest in terms of training GPU hours
but AdamW led to the best downstream evaluation results.

</details>


### [183] [Evaluating SAE interpretability without explanations](https://arxiv.org/abs/2507.08473)
*Gonçalo Paulo,Nora Belrose*

Main category: cs.LG

TL;DR: 提出一种不依赖自然语言解释的稀疏自动编码器可解释性评估方法，并与人类评估对比，为改进评估提供建议。


<details>
  <summary>Details</summary>
Motivation: 稀疏自动编码器（SAEs）的可解释性评估缺乏统一标准且具挑战性，现有方法依赖生成自然语言解释，导致解释生成与潜在表示的实际可解释性难以区分。

Method: 改进现有方法以直接评估稀疏编码器的可解释性，无需中间自然语言解释步骤。同时，将提出的可解释性指标分数与人类评估结果进行比较。

Result: 所提出的方法能够更直接、潜在地标准化地评估可解释性。通过与人类评估的比较，为社区改进稀疏编码器评估技术提供了建议。

Conclusion: 本研究提供了一种更直接、更纯粹的稀疏自动编码器可解释性评估方法，并基于人类评估提出改进建议，有望推动该领域评估的标准化和有效性。

Abstract: Sparse autoencoders (SAEs) and transcoders have become important tools for
machine learning interpretability. However, measuring how interpretable they
are remains challenging, with weak consensus about which benchmarks to use.
Most evaluation procedures start by producing a single-sentence explanation for
each latent. These explanations are then evaluated based on how well they
enable an LLM to predict the activation of a latent in new contexts. This
method makes it difficult to disentangle the explanation generation and
evaluation process from the actual interpretability of the latents discovered.
In this work, we adapt existing methods to assess the interpretability of
sparse coders, with the advantage that they do not require generating natural
language explanations as an intermediate step. This enables a more direct and
potentially standardized assessment of interpretability. Furthermore, we
compare the scores produced by our interpretability metrics with human
evaluations across similar tasks and varying setups, offering suggestions for
the community on improving the evaluation of these techniques.

</details>


### [184] [SynBridge: Bridging Reaction States via Discrete Flow for Bidirectional Reaction Prediction](https://arxiv.org/abs/2507.08475)
*Haitao Lin,Junjie Wang,Zhifeng Gao,Xiaohong Ji,Rong Zhu,Linfeng Zhang,Guolin Ke,Weinan E*

Main category: cs.LG

TL;DR: SynBridge是一个双向流生成模型，通过模拟化学反应中离散的电子和原子状态变化，在正向和逆向合成任务中均实现了最先进的反应预测性能。


<details>
  <summary>Details</summary>
Motivation: 化学反应本质上涉及电子的离散重新分布和状态的突变，现有模型难以有效捕捉。因此，需要开发一种能够精确建模这些离散状态转换的多任务反应预测方法。

Method: 本文提出了SynBridge，一个基于图到图Transformer网络的双向流生成模型。该模型利用离散流桥在任意两个离散分布之间，通过建模键和原子的离散状态，捕获反应物和产物图之间的双向化学转化。

Result: SynBridge在USPTO-50K、USPTO-MIT和Pistachio三个基准数据集上，在正向和逆向合成任务中均取得了最先进的性能。消融研究和噪声调度分析进一步揭示了在离散空间上进行结构化扩散对反应预测的益处。

Conclusion: SynBridge模型通过其独特的离散流桥和Transformer架构，成功地捕捉并建模了化学反应中离散的电子和原子状态变化，显著提升了多任务反应预测的准确性，证明了在离散空间上进行结构化扩散在化学反应建模中的有效性。

Abstract: The essence of a chemical reaction lies in the redistribution and
reorganization of electrons, which is often manifested through electron
transfer or the migration of electron pairs. These changes are inherently
discrete and abrupt in the physical world, such as alterations in the charge
states of atoms or the formation and breaking of chemical bonds. To model the
transition of states, we propose SynBridge, a bidirectional flow-based
generative model to achieve multi-task reaction prediction. By leveraging a
graph-to-graph transformer network architecture and discrete flow bridges
between any two discrete distributions, SynBridge captures bidirectional
chemical transformations between graphs of reactants and products through the
bonds' and atoms' discrete states. We further demonstrate the effectiveness of
our method through extensive experiments on three benchmark datasets
(USPTO-50K, USPTO-MIT, Pistachio), achieving state-of-the-art performance in
both forward and retrosynthesis tasks. Our ablation studies and noise
scheduling analysis reveal the benefits of structured diffusion over discrete
spaces for reaction prediction.

</details>


### [185] [Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study on OnePlus 13R](https://arxiv.org/abs/2507.08505)
*Pablo Robin Guerrero,Yueyang Pan,Sanidhya Kashyap*

Main category: cs.LG

TL;DR: 该研究全面评估了VLM在移动设备上的部署框架，发现当前框架存在CPU过度利用、GPU和NPU加速器利用不足或不稳定导致性能瓶颈的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）在移动设备上前景广阔，但其部署面临计算限制和能效低下的挑战，尤其是在实时应用中。

Method: 本研究调查并评估了llama.cpp、MLC-Imp和mllm等VLM部署框架。在OnePlus 13R设备上运行LLaVA-1.5 7B、MobileVLM-3B和Imp-v1.5 3B作为代表性工作负载，测量了CPU、GPU和NPU利用率、温度、推理时间、功耗和用户体验。

Result: 基准测试揭示了关键性能瓶颈：CPU资源在Token生成过程中持续过度利用；GPU和NPU加速器大部分时间未被使用。当GPU用于图像特征提取时，其会被饱和利用，导致设备响应性下降。

Conclusion: 当前VLM部署框架存在硬件利用瓶颈，表现为CPU持续过度使用，以及GPU和NPU加速器使用效率低下或不稳定。

Abstract: Vision-Language Models (VLMs) offer promising capabilities for mobile
devices, but their deployment faces significant challenges due to computational
limitations and energy inefficiency, especially for real-time applications.
This study provides a comprehensive survey of deployment frameworks for VLMs on
mobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of
running LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads
on a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R
while running VLMs, with measurements covering CPU, GPU, and NPU utilization,
temperature, inference time, power consumption, and user experience.
Benchmarking revealed critical performance bottlenecks across frameworks: CPU
resources were consistently over-utilized during token generation, while GPU
and NPU accelerators were largely unused. When the GPU was used, primarily for
image feature extraction, it was saturated, leading to degraded device
responsiveness. The study contributes framework-level benchmarks, practical
profiling tools, and an in-depth analysis of hardware utilization bottlenecks,
highlighting the consistent overuse of CPUs and the ineffective or unstable use
of GPUs and NPUs in current deployment frameworks.

</details>


### [186] [SFedKD: Sequential Federated Learning with Discrepancy-Aware Multi-Teacher Knowledge Distillation](https://arxiv.org/abs/2507.08508)
*Haotian Xu,Jinrui Zhou,Xichong Zhang,Mingjun Xiao,He Sun,Yin Xu*

Main category: cs.LG

TL;DR: 针对序列联邦学习（SFL）中的灾难性遗忘问题，本文提出SFedKD框架，通过差异感知多教师知识蒸馏和互补教师选择机制有效缓解遗忘并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 序列联邦学习（SFL）在数据异构环境下存在严重的灾难性遗忘问题，导致模型遗忘从先前客户端学到的知识，影响模型性能。

Method: 本文提出SFedKD框架，该框架采用差异感知多教师知识蒸馏。具体地，它从前一轮中选择多个模型作为教师指导当前轮次的训练。SFedKD将单教师解耦知识蒸馏扩展到多教师设置，并根据教师与学生数据的类别分布差异，为教师的目标类和非目标类知识分配不同权重，以提升训练效率并减轻遗忘。此外，为了防止知识稀释，本文将冗余教师的消除问题形式化为最大覆盖问题，并设计了一种基于贪婪策略的互补教师选择机制，以确保所选教师实现全面的知识空间覆盖，同时降低通信和计算成本。

Result: 广泛的实验结果表明，SFedKD框架有效克服了SFL中的灾难性遗忘问题，并且在性能上优于现有最先进的联邦学习方法。

Conclusion: SFedKD通过其创新的差异感知多教师知识蒸馏和高效的教师选择机制，成功解决了SFL中的灾难性遗忘问题，为SFL在异构环境下的应用提供了有效且鲁棒的解决方案。

Abstract: Federated Learning (FL) is a distributed machine learning paradigm which
coordinates multiple clients to collaboratively train a global model via a
central server. Sequential Federated Learning (SFL) is a newly-emerging FL
training framework where the global model is trained in a sequential manner
across clients. Since SFL can provide strong convergence guarantees under data
heterogeneity, it has attracted significant research attention in recent years.
However, experiments show that SFL suffers from severe catastrophic forgetting
in heterogeneous environments, meaning that the model tends to forget knowledge
learned from previous clients. To address this issue, we propose an SFL
framework with discrepancy-aware multi-teacher knowledge distillation, called
SFedKD, which selects multiple models from the previous round to guide the
current round of training. In SFedKD, we extend the single-teacher Decoupled
Knowledge Distillation approach to our multi-teacher setting and assign
distinct weights to teachers' target-class and non-target-class knowledge based
on the class distributional discrepancy between teacher and student data.
Through this fine-grained weighting strategy, SFedKD can enhance model training
efficacy while mitigating catastrophic forgetting. Additionally, to prevent
knowledge dilution, we eliminate redundant teachers for the knowledge
distillation and formalize it as a variant of the maximum coverage problem.
Based on the greedy strategy, we design a complementary-based teacher selection
mechanism to ensure that the selected teachers achieve comprehensive knowledge
space coverage while reducing communication and computational costs. Extensive
experiments show that SFedKD effectively overcomes catastrophic forgetting in
SFL and outperforms state-of-the-art FL methods.

</details>


### [187] [Recursive Reward Aggregation](https://arxiv.org/abs/2507.08537)
*Yuting Tang,Yivan Zhang,Johannes Ackermann,Yu-Jie Zhang,Soichiro Nishimori,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 通过引入奖励聚合函数，该研究提出了一种无需修改奖励函数即可灵活对齐强化学习（RL）智能体行为的方法，并从代数角度泛化了贝尔曼方程以支持多种目标优化。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，当期望目标复杂时，精心设计奖励函数以使智能体行为与特定目标对齐非常具有挑战性。

Method: 提出了一种通过选择适当的奖励聚合函数来灵活对齐行为的方法，而非修改奖励函数。通过引入马尔可夫决策过程（MDPs）的代数视角，该方法展示了贝尔曼方程如何从奖励的递归生成和聚合中自然导出，从而将标准折扣和泛化到折扣最大值和夏普比率等其他递归聚合。该方法适用于确定性和随机设置，并能与基于值和Actor-Critic算法无缝集成。

Result: 实验结果表明，该方法能够有效地优化各种不同的目标。

Conclusion: 该方法具有多功能性，并在实际应用中具有巨大潜力，能够灵活地处理复杂的强化学习目标。

Abstract: In reinforcement learning (RL), aligning agent behavior with specific
objectives typically requires careful design of the reward function, which can
be challenging when the desired objectives are complex. In this work, we
propose an alternative approach for flexible behavior alignment that eliminates
the need to modify the reward function by selecting appropriate reward
aggregation functions. By introducing an algebraic perspective on Markov
decision processes (MDPs), we show that the Bellman equations naturally emerge
from the recursive generation and aggregation of rewards, allowing for the
generalization of the standard discounted sum to other recursive aggregations,
such as discounted max and Sharpe ratio. Our approach applies to both
deterministic and stochastic settings and integrates seamlessly with
value-based and actor-critic algorithms. Experimental results demonstrate that
our approach effectively optimizes diverse objectives, highlighting its
versatility and potential for real-world applications.

</details>


### [188] [CircFormerMoE: An End-to-End Deep Learning Framework for Circular RNA Splice Site Detection and Pairing in Plant Genomes](https://arxiv.org/abs/2507.08542)
*Tianyou Jiang*

Main category: cs.LG

TL;DR: 本文提出CircFormerMoE，一个基于Transformer和混合专家模型的深度学习框架，可以直接从植物基因组DNA预测环状RNA（circRNA），解决了传统RNA-seq方法在高计算成本、依赖实验数据和泛化能力上的局限性，并实现了植物大规模circRNA的快速准确发现。


<details>
  <summary>Details</summary>
Motivation: 传统的circRNA识别方法依赖于RNA测序数据和比对算法，存在无法直接从基因组DNA预测、高度依赖实验数据、计算成本高以及不适用于大规模预测的局限性。尤其在植物中，由于剪接位点缺乏典型GT-AG基序且缺乏高效泛化性强的深度学习模型，导致已识别的植物circRNA数量远低于实际丰度。

Method: 本研究提出了一个名为CircFormerMoE的深度学习框架，该框架基于Transformer和混合专家模型（Mixture-of-Experts, MoE）。它直接从植物基因组DNA序列预测circRNA，并包含剪接位点检测（SSD）和剪接位点配对（SSP）两个子任务。

Result: 该模型的有效性已在10种植物的基因数据上得到验证。它不仅能利用已知circRNA实例进行训练，还能发现以前未注释的circRNA。此外，研究还对训练后的模型进行了可解释性分析，以探究影响预测的序列模式。

Conclusion: CircFormerMoE框架为植物大规模circRNA的发现提供了一种快速且准确的计算方法和工具，为未来植物功能基因组学和非编码RNA注释研究奠定了基础。

Abstract: Circular RNAs (circRNAs) are important components of the non-coding RNA
regulatory network. Previous circRNA identification primarily relies on
high-throughput RNA sequencing (RNA-seq) data combined with alignment-based
algorithms that detect back-splicing signals. However, these methods face
several limitations: they can't predict circRNAs directly from genomic DNA
sequences and relies heavily on RNA experimental data; they involve high
computational costs due to complex alignment and filtering steps; and they are
inefficient for large-scale or genome-wide circRNA prediction. The challenge is
even greater in plants, where plant circRNA splice sites often lack the
canonical GT-AG motif seen in human mRNA splicing, and no efficient deep
learning model with strong generalization capability currently exists.
Furthermore, the number of currently identified plant circRNAs is likely far
lower than their true abundance. In this paper, we propose a deep learning
framework named CircFormerMoE based on transformers and mixture-of experts for
predicting circRNAs directly from plant genomic DNA. Our framework consists of
two subtasks known as splicing site detection (SSD) and splicing site pairing
(SSP). The model's effectiveness has been validated on gene data of 10 plant
species. Trained on known circRNA instances, it is also capable of discovering
previously unannotated circRNAs. In addition, we performed interpretability
analyses on the trained model to investigate the sequence patterns contributing
to its predictions. Our framework provides a fast and accurate computational
method and tool for large-scale circRNA discovery in plants, laying a
foundation for future research in plant functional genomics and non-coding RNA
annotation.

</details>


### [189] [STRAP: Spatial-Temporal Risk-Attentive Vehicle Trajectory Prediction for Autonomous Driving](https://arxiv.org/abs/2507.08563)
*Xinyi Ning,Zilin Bian,Kaan Ozbay,Semiha Ergan*

Main category: cs.LG

TL;DR: 针对自动驾驶中现有轨迹预测方法忽略车辆不确定/激进行为带来的风险问题，提出了一种新的时空风险感知轨迹预测框架，显著提高了高风险场景下的预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有车辆轨迹预测方法主要关注已观测运动模式和车辆间交互，但往往忽略了周围车辆不确定或激进行为带来的潜在风险，这对于自动驾驶系统的安全性和效率至关重要。

Method: 提出了一种新颖的时空风险感知轨迹预测框架。该框架整合了风险势场以评估感知风险，并利用时空编码器和风险感知特征融合解码器将风险势场嵌入到轨迹预测的时空特征表示中。此外，还设计了一个风险加权损失函数以提高高风险场景（如相对间距较短）的预测精度。

Result: 在NGSIM和HighD数据集上的实验表明，与现有最先进方法相比，所提出的方法使平均预测误差分别降低了4.8%和31.2%，特别是在高风险场景下表现更佳。

Conclusion: 所提出的框架提供了可解释、风险感知的预测结果，有助于自动驾驶系统做出更鲁棒的决策。

Abstract: Accurate vehicle trajectory prediction is essential for ensuring safety and
efficiency in fully autonomous driving systems. While existing methods
primarily focus on modeling observed motion patterns and interactions with
other vehicles, they often neglect the potential risks posed by the uncertain
or aggressive behaviors of surrounding vehicles. In this paper, we propose a
novel spatial-temporal risk-attentive trajectory prediction framework that
incorporates a risk potential field to assess perceived risks arising from
behaviors of nearby vehicles. The framework leverages a spatial-temporal
encoder and a risk-attentive feature fusion decoder to embed the risk potential
field into the extracted spatial-temporal feature representations for
trajectory prediction. A risk-scaled loss function is further designed to
improve the prediction accuracy of high-risk scenarios, such as short relative
spacing. Experiments on the widely used NGSIM and HighD datasets demonstrate
that our method reduces average prediction errors by 4.8% and 31.2%
respectively compared to state-of-the-art approaches, especially in high-risk
scenarios. The proposed framework provides interpretable, risk-aware
predictions, contributing to more robust decision-making for autonomous driving
systems.

</details>


### [190] [AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient Sequence Modeling](https://arxiv.org/abs/2507.08567)
*Preslav Aleksandrov,Meghdad Kurmanji,Fernando Garcia Redondo,David O'Shea,William Shen,Alex Iacob,Lorenzo Sani,Xinchi Qiu,Nicola Cancedda,Nicholas D. Lane*

Main category: cs.LG

TL;DR: 本文提出Autoregressive Block-Based Iterative Encoder (AbbIE)，一种递归泛化编码器型Transformer架构，它在潜在空间进行迭代，实现优于标准Transformer的困惑度，并支持测试时动态计算资源扩展，为Transformer性能提升提供新途径。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）的性能扩展主要通过增加参数和token数量实现。本研究旨在提供一种新的、互补的方法，即通过简单、递归的迭代方式来进一步提升LLM的性能和计算效率。

Method: 引入Autoregressive Block-Based Iterative Encoder (AbbIE)，它是编码器型Transformer架构的递归泛化。AbbIE在潜在空间执行迭代，且无需特定的数据集或训练协议。该方法在训练时仅使用2次迭代，但在测试时能向上泛化到任意迭代长度。所有评估均在最大3.5亿参数的模型上进行。

Result: AbbIE比标准Transformer表现出更好的困惑度。它在测试时能动态扩展计算资源。实验证明，AbbIE在仅通过2次迭代训练后，能实现向上泛化到任意迭代长度的能力，并远超其他迭代方法。在零样本上下文学习任务中，其性能比其他迭代和标准方法提升高达12%；在语言困惑度上提升高达5%。

Conclusion: 研究结果表明，AbbIE的递归迭代方法和动态计算资源扩展能力，为Transformer的性能扩展开辟了一条新途径，有效提升了模型的困惑度、零样本学习能力及泛化性。

Abstract: We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a
novel recursive generalization of the encoder-only Transformer architecture,
which achieves better perplexity than a standard Transformer and allows for the
dynamic scaling of compute resources at test time. This simple, recursive
approach is a complement to scaling large language model (LLM) performance
through parameter and token counts. AbbIE performs its iterations in latent
space, but unlike latent reasoning models, does not require a specialized
dataset or training protocol. We show that AbbIE upward generalizes (ability to
generalize to arbitrary iteration lengths) at test time by only using 2
iterations during train time, far outperforming alternative iterative methods.
AbbIE's ability to scale its computational expenditure based on the complexity
of the task gives it an up to \textbf{12\%} improvement in zero-shot in-context
learning tasks versus other iterative and standard methods and up to 5\%
improvement in language perplexity. The results from this study open a new
avenue to Transformer performance scaling. We perform all of our evaluations on
model sizes up to 350M parameters.

</details>


### [191] [ADAPT: A Pseudo-labeling Approach to Combat Concept Drift in Malware Detection](https://arxiv.org/abs/2507.08597)
*Md Tanvirul Alam,Aritran Piplai,Nidhi Rastogi*

Main category: cs.LG

TL;DR: 本研究提出了一种名为ADAPT的新型半监督伪标签算法，旨在解决恶意软件检测中机器学习模型因概念漂移导致的性能下降问题，该方法模型无关并在多个数据集上表现出卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在恶意软件分类中因概念漂移而性能随时间下降。模型适应需要频繁更新，这依赖于昂贵的真实标签标注。尽管主动学习可以减轻标注负担，但在恶意软件检测中，利用未标注数据的半监督学习方法仍相对未被充分探索。

Method: 研究引入了ADAPT，一种新颖的伪标签半监督算法，用于解决概念漂移。该方法是模型无关的，可应用于包括神经网络和基于树的算法在内的各种机器学习模型。通过在Android、Windows和PDF领域的五个多样化恶意软件检测数据集上进行广泛实验来验证其有效性。

Result: 实验结果表明，所提出的ADAPT方法始终优于基线模型和有竞争力的基准。

Conclusion: 这项工作为机器学习模型在恶意软件检测中更有效地适应概念漂移开辟了道路。

Abstract: Machine learning models are commonly used for malware classification;
however, they suffer from performance degradation over time due to concept
drift. Adapting these models to changing data distributions requires frequent
updates, which rely on costly ground truth annotations. While active learning
can reduce the annotation burden, leveraging unlabeled data through
semi-supervised learning remains a relatively underexplored approach in the
context of malware detection. In this research, we introduce \texttt{ADAPT}, a
novel pseudo-labeling semi-supervised algorithm for addressing concept drift.
Our model-agnostic method can be applied to various machine learning models,
including neural networks and tree-based algorithms. We conduct extensive
experiments on five diverse malware detection datasets spanning Android,
Windows, and PDF domains. The results demonstrate that our method consistently
outperforms baseline models and competitive benchmarks. This work paves the way
for more effective adaptation of machine learning models to concept drift in
malware detection.

</details>


### [192] [Remote Sensing Reveals Adoption of Sustainable Rice Farming Practices Across Punjab, India](https://arxiv.org/abs/2507.08605)
*Ando Shah,Rajveer Singh,Akram Zaytar,Girmaw Abebe Tadesse,Caleb Robinson,Negar Tafti,Stephen A. Wood,Rahul Dodhia,Juan M. Lavista Ferres*

Main category: cs.LG

TL;DR: 本研究开发了一种基于Sentinel-1卫星图像的新型遥感框架，用于大规模监测水稻节水种植（DSR）的采用情况。该框架在印度旁遮普邦成功应用，区分DSR的F1-score达78%，并能大规模绘制采用地图，为政策制定提供数据支持。


<details>
  <summary>Details</summary>
Motivation: 水稻种植消耗全球大量淡水，导致严重的水资源管理挑战。虽然节水灌溉技术（如DSR和AWD）能有效节水并维持产量，但缺乏其采用率数据，阻碍了循证决策和资源分配。印度旁遮普邦地下水严重枯竭，亟需工具监测可持续水管理实践。

Method: 研究团队开发了一个新颖的遥感框架，利用Sentinel-1卫星图像监测可持续水管理实践。通过与“自然保护协会”的PRANA项目合作，收集了约1400名农民的实地数据。基于这些数据，建立了一个分类系统，能够根据播种和灌溉维度区分水管理方式，且无需预知种植日期。

Result: 该方法在区分直播稻（DSR）与传统移栽稻方面取得了78%的F1-score。研究成功将DSR的采用情况映射到旁遮普邦约300万块农田上，验证了其可扩展性。地区层面的预测与政府记录显示出强烈的相关性（Pearson=0.77，RBO=0.77）。

Conclusion: 本研究提供了一个强有力的工具，使政策制定者能够大规模追踪可持续水管理实践的采用情况，精准定位干预措施，并评估项目影响，从而有效应对水资源短缺问题，促进农业可持续发展。

Abstract: Rice cultivation consumes 24-30% of global freshwater, creating critical
water management challenges in major rice-producing regions. Sustainable
irrigation practices like direct seeded rice (DSR) and alternate wetting and
drying (AWD) can reduce water use by 20-40% while maintaining yields, helping
secure long-term agricultural productivity as water scarcity intensifies - a
key component of the Zero Hunger Sustainable Development Goal. However, limited
data on adoption rates of these practices prevents evidence-based policymaking
and targeted resource allocation. We developed a novel remote sensing framework
to monitor sustainable water management practices at scale in Punjab, India - a
region facing severe groundwater depletion of 41.6 cm/year. To collect
essential ground truth data, we partnered with the Nature Conservancy's
Promoting Regenerative and No-burn Agriculture (PRANA) program, which trained
approximately 1,400 farmers on water-saving techniques while documenting their
field-level practices. Using this data, we created a classification system with
Sentinel-1 satellite imagery that separates water management along sowing and
irrigation dimensions. Our approach achieved a 78% F1-score in distinguishing
DSR from traditional puddled transplanted rice without requiring prior
knowledge of planting dates. We demonstrated scalability by mapping DSR
adoption across approximately 3 million agricultural plots in Punjab, with
district-level predictions showing strong correlation (Pearson=0.77, RBO= 0.77)
with government records. This study provides policymakers with a powerful tool
to track sustainable water management adoption, target interventions, and
measure program impacts at scale.

</details>


### [193] [Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data](https://arxiv.org/abs/2507.08610)
*Parag Dutta,Ambedkar Dukkipati*

Main category: cs.LG

TL;DR: 本文提出了LoGIC（Lewis Communication Game for Image Captioning），一种多智能体强化学习游戏，旨在通过“说话者”和“听众”智能体之间的合作交流，提高无监督图像描述的性能，无需额外标注即可实现显著的BLEU分数提升。


<details>
  <summary>Details</summary>
Motivation: 图像描述对AI系统至关重要，但需要大量标注数据。现有标注数据集已被大型视觉语言模型（VLMs）充分利用，导致性能提升面临挑战。因此，探索相对欠开发的无监督图像描述性能变得尤为重要。

Method: 提出LoGIC，一个多智能体强化学习游戏，包含“说话者”和“听众”两个智能体，目标是学习自然语言交流策略。智能体在合作式共同奖励设置下，使用GRPO算法进行训练。方法探索了两种配置：一是使用预训练VLMs作“说话者”和LLM作“听众”的语言理解，进行微调；二是用轻量级组件（ViT用于图像感知，GPT2用于语言生成）替换VLM，从零开始训练。

Result: 图像描述性能的提升是智能体学会玩游戏的结果。通过LoGIC对VLM/LLM进行微调，无需额外标签，BLEU分数达到46，比原始VLM的44 BLEU提高了2个单位。此外，用轻量级组件从零开始训练，在无监督设置下获得31 BLEU分数，比现有无监督图像描述方法高出10个点。

Conclusion: LoGIC成功地通过多智能体强化学习游戏提升了无监督图像描述的性能。无论是通过微调现有VLM还是从零开始训练轻量级模型，LoGIC都展示了在无需额外标注的情况下有效改善图像描述能力，突显了基于通信游戏实现性能提升的潜力。

Abstract: Image captioning is an important problem in developing various AI systems,
and these tasks require large volumes of annotated images to train the models.
Since all existing labelled datasets are already used for training the large
Vision Language Models (VLMs), it becomes challenging to improve the
performance of the same. Considering this, it is essential to consider the
unsupervised image captioning performance, which remains relatively
under-explored. To that end, we propose LoGIC (Lewis Communication Game for
Image Captioning), a Multi-agent Reinforcement Learning game. The proposed
method consists of two agents, a 'speaker' and a 'listener', with the objective
of learning a strategy for communicating in natural language. We train agents
in the cooperative common-reward setting using the GRPO algorithm and show that
improvement in image captioning performance emerges as a consequence of the
agents learning to play the game. We show that using pre-trained VLMs as the
'speaker' and Large Language Model (LLM) for language understanding in the
'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without
additional labels, a $2$ units advantage in absolute metrics compared to the
$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the
'speaker' with lightweight components: (i) a ViT for image perception and (ii)
a GPT2 language generation, and train them from scratch using LoGIC, obtaining
a $31$ BLEU score in the unsupervised setting, a $10$ points advantage over
existing unsupervised image-captioning methods.

</details>


### [194] [Towards Collaborative Fairness in Federated Learning Under Imbalanced Covariate Shift](https://arxiv.org/abs/2507.08617)
*Tianrun Yu,Jiaqi Wang,Haoyu Wang,Mingquan Lin,Han Liu,Nelson S. Yee,Fenglong Ma*

Main category: cs.LG

TL;DR: 在联邦学习中，针对不平衡协变量偏移引起的协作公平性问题，本文提出FedAKD（联邦异步知识蒸馏）方法。该方法通过独特的客户端异步知识蒸馏和服务器聚合策略，在保持预测准确性的同时，显著提升了协作公平性并促进了客户端参与。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的协作公平性是一个重要挑战。现有方法往往忽视了不平衡协变量偏移这一现实且复杂的异质性，而本文旨在填补这一空白。

Method: 本文提出FedAKD（联邦异步知识蒸馏）方法，旨在平衡预测准确性与协作公平性。FedAKD包含客户端和服务器更新：客户端更新基于对错误分类样本导致不平衡协变量偏移的洞察，采用新颖的异步知识蒸馏策略，先用传统知识蒸馏更新客户端模型，再用高置信度正确预测样本更新全局模型；服务器更新则聚合所有客户端模型。此外，本文提供了FedAKD的收敛性理论证明。

Result: 在FashionMNIST、CIFAR10等公共数据集和真实世界EHR数据集上的实验结果表明，即使在高度异构的数据分布下，FedAKD也能显著改善协作公平性，提高预测准确性，并促进客户端参与。

Conclusion: FedAKD为联邦学习中不平衡协变量偏移导致的协作公平性问题提供了一种有效解决方案，通过平衡预测准确性与公平性，并促进客户端参与，展示了其在复杂异构环境下的优越性，且具备理论收敛性保证。

Abstract: Collaborative fairness is a crucial challenge in federated learning. However,
existing approaches often overlook a practical yet complex form of
heterogeneity: imbalanced covariate shift. We provide a theoretical analysis of
this setting, which motivates the design of FedAKD (Federated Asynchronous
Knowledge Distillation)- simple yet effective approach that balances accurate
prediction with collaborative fairness. FedAKD consists of client and server
updates. In the client update, we introduce a novel asynchronous knowledge
distillation strategy based on our preliminary analysis, which reveals that
while correctly predicted samples exhibit similar feature distributions across
clients, incorrectly predicted samples show significant variability. This
suggests that imbalanced covariate shift primarily arises from misclassified
samples. Leveraging this insight, our approach first applies traditional
knowledge distillation to update client models while keeping the global model
fixed. Next, we select correctly predicted high-confidence samples and update
the global model using these samples while keeping client models fixed. The
server update simply aggregates all client models. We further provide a
theoretical proof of FedAKD's convergence. Experimental results on public
datasets (FashionMNIST and CIFAR10) and a real-world Electronic Health Records
(EHR) dataset demonstrate that FedAKD significantly improves collaborative
fairness, enhances predictive accuracy, and fosters client participation even
under highly heterogeneous data distributions.

</details>


### [195] [Scaling Attention to Very Long Sequences in Linear Time with Wavelet-Enhanced Random Spectral Attention (WERSA)](https://arxiv.org/abs/2507.08637)
*Vincenzo Dentamaro*

Main category: cs.LG

TL;DR: 本文提出WERSA，一种Transformer线性注意力机制，通过结合小波和谱特征，解决了长序列计算成本高的问题。WERSA在多个任务中表现出更高的准确性、更快的速度和更低的资源消耗，尤其适用于低资源环境下的长上下文模型。


<details>
  <summary>Details</summary>
Motivation: 由于注意力机制的二次时间复杂度（O(n^2)），Transformer模型在处理长序列时计算成本过高。

Method: 引入Wavelet-Enhanced Random Spectral Attention (WERSA)，一种具有O(n)线性时间复杂度的机制。WERSA将内容自适应的随机谱特征与多分辨率Haar小波以及可学习参数相结合，以选择性地关注数据中的信息尺度，同时保持线性效率。

Result: WERSA在视觉、NLP和分层推理等多个基准测试中均表现出最佳准确性。在ArXiv分类任务中，与传统注意力相比，WERSA将准确率提高了1.2%，训练时间缩短了81%，FLOPS降低了73.4%。对于极长序列（如ArXiv-128k），WERSA能够在二次复杂度方法出现内存不足错误时稳定运行，并取得最佳准确率（79.1%）和AUC（0.979），速度是次优竞争对手Waveformer的两倍。

Conclusion: WERSA通过显著降低计算负荷而不牺牲准确性，使得更实用、更经济的长上下文模型得以实现，特别是在低资源硬件上，从而促进更可持续和可扩展的AI发展。

Abstract: Transformer models are computationally costly on long sequences since regular
attention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced
Random Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time
complexity that is pivotal to enable successful long-sequence processing
without the performance trade-off. WERSA merges content-adaptive random
spectral features together with multi-resolution Haar wavelets and learnable
parameters to selectively attend to informative scales of data while preserving
linear efficiency.
  Large-scale comparisons \textbf{on single GPU} and across various benchmarks
(vision, NLP, hierarchical reasoning) and various attention mechanisms (like
Multiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer,
Waveformer), reveal uniform advantages of WERSA. It achieves best accuracy in
all tests. On ArXiv classification, WERSA improves accuracy over vanilla
attention by 1.2\% (86.2\% vs 85.0\%) while cutting training time by 81\% (296s
vs 1554s) and FLOPS by 73.4\% (26.2G vs 98.4G). Significantly, WERSA excels
where vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy
sequences, it achieves best accuracy (79.1\%) and AUC (0.979) among viable
methods, operating on data that gives Out-Of-Memory errors to quadratic methods
while being \textbf{twice as fast} as Waveformer, its next-best competitor.
  By significantly reducing computational loads without compromising accuracy,
WERSA makes possible more practical, more affordable, long-context models, in
particular on low-resource hardware, for more sustainable and more scalable AI
development.

</details>


### [196] [Forget Me Not: Fighting Local Overfitting with Knowledge Fusion and Distillation](https://arxiv.org/abs/2507.08686)
*Uri Stern,Eli Corn,Daphna Weinshall*

Main category: cs.LG

TL;DR: 研究发现深度神经网络中存在一种“局部过拟合”现象，即便没有传统意义的全局过拟合也会发生。为此，提出一种两阶段方法：首先聚合模型训练过程中的检查点形成集成模型进行“知识融合”，然后将其“知识蒸馏”回单一模型，以恢复被遗忘的知识，有效提升性能并降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在实践中表现出的过拟合频率低于理论预期，这是一个令人困惑的现象。作者假设过拟合可能并非全局发生，而是局限于数据空间的特定子区域，因此需要研究和量化这种“局部过拟合”现象，并探索其与已知现象（如双下降）的关联，进而提出解决方案以提升模型性能。

Method: 首先，引入一个新颖的分数来衡量深度模型在验证数据上的“遗忘率”，以此量化和捕捉局部过拟合。在此洞察基础上，提出一种两阶段方法：1) 通过聚合训练过程中的多个模型检查点来构建一个集成模型（知识融合）；2) 将该集成模型的知识蒸馏到一个原始大小的单一模型中，从而在不增加推理成本的情况下恢复并保留被遗忘的知识。

Result: 研究发现，局部过拟合即使在没有传统全局过拟合的情况下也可能发生，并且与双下降现象密切相关。通过在多个数据集、现代架构和训练方案上的广泛实验验证，所提出的“知识融合后知识蒸馏”方法能够有效提升模型性能。特别是在存在标签噪声的情况下，该方法性能显著优于原始模型和独立训练的集成模型，实现了性能提升和训练/推理复杂性降低的双赢效果。

Conclusion: 局部过拟合是深度神经网络中一个实际存在且具有影响的现象。通过利用模型训练历史（检查点），本文提出的“知识融合后知识蒸馏”两阶段方法能够有效恢复和保留被遗忘的知识，从而提升模型性能，尤其在有噪声数据时表现更佳，同时保持了计算效率，提供了一种解决深度学习中过拟合挑战的新视角和有效方案。

Abstract: Overfitting in deep neural networks occurs less frequently than expected.
This is a puzzling observation, as theory predicts that greater model capacity
should eventually lead to overfitting -- yet this is rarely seen in practice.
But what if overfitting does occur, not globally, but in specific sub-regions
of the data space? In this work, we introduce a novel score that measures the
forgetting rate of deep models on validation data, capturing what we term local
overfitting: a performance degradation confined to certain regions of the input
space. We demonstrate that local overfitting can arise even without
conventional overfitting, and is closely linked to the double descent
phenomenon.
  Building on these insights, we introduce a two-stage approach that leverages
the training history of a single model to recover and retain forgotten
knowledge: first, by aggregating checkpoints into an ensemble, and then by
distilling it into a single model of the original size, thus enhancing
performance without added inference cost.
  Extensive experiments across multiple datasets, modern architectures, and
training regimes validate the effectiveness of our approach. Notably, in the
presence of label noise, our method -- Knowledge Fusion followed by Knowledge
Distillation -- outperforms both the original model and independently trained
ensembles, achieving a rare win-win scenario: reduced training and inference
complexity.

</details>


### [197] [Domain-Informed Operation Excellence of Gas Turbine System with Machine Learning](https://arxiv.org/abs/2507.08697)
*Waqar Muhammad Ashraf,Amir H. Keshavarzzadeh,Abdulelah S. Alshehri,Abdulrahman bin Jumah,Ramit Debnath,Vivek Dua*

Main category: cs.LG

TL;DR: 本研究开发了一种名为MAD-OPT的新型AI框架，通过引入基于马哈拉诺比斯距离的约束将领域知识整合到数据分析中，旨在提升热力发电厂中AI应用的有效性和安全性，优化其运行。


<details>
  <summary>Details</summary>
Motivation: 热力发电厂中人工智能（AI）的实际应用率较低，主要原因在于AI算法的“黑箱”特性以及传统以数据为中心的分析方法中领域知识的低代表性。缺乏领域知识的优化方案可能在实际操作中不可行。

Method: 开发了基于马哈拉诺比斯距离优化的（MAhalanobis Distance-based OPTimization, MAD-OPT）框架。该框架通过引入基于马哈拉诺比斯距离的约束，将领域知识融入到以数据为中心的分析中。

Result: MAD-OPT框架能够估算出不同环境条件下的领域知情最佳工艺条件，且这些最佳解决方案通过蒙特卡洛模拟评估具有鲁棒性。该框架在超出燃气轮机系统设计发电限制的情况下，估算出的最佳工艺条件与电厂实际数据具有可比性。研究还表明，未融入领域知情约束的数据中心优化分析可能提供无效或在实际操作中不可行的解决方案。

Conclusion: 本研究推动了数据驱动的领域知识与机器学习分析的整合，从而提升了领域知情的操作卓越性，并为人工智能在热力发电系统中的安全应用奠定了基础。

Abstract: The domain-consistent adoption of artificial intelligence (AI) remains low in
thermal power plants due to the black-box nature of AI algorithms and low
representation of domain knowledge in conventional data-centric analytics. In
this paper, we develop a MAhalanobis Distance-based OPTimization (MAD-OPT)
framework that incorporates the Mahalanobis distance-based constraint to
introduce domain knowledge into data-centric analytics. The developed MAD-OPT
framework is applied to maximize thermal efficiency and minimize turbine heat
rate for a 395 MW capacity gas turbine system. We demonstrate that the MAD-OPT
framework can estimate domain-informed optimal process conditions under
different ambient conditions, and the optimal solutions are found to be robust
as evaluated by Monte Carlo simulations. We also apply the MAD-OPT framework to
estimate optimal process conditions beyond the design power generation limit of
the gas turbine system, and have found comparable results with the actual data
of the power plant. We demonstrate that implementing data-centric optimization
analytics without incorporating domain-informed constraints may provide
ineffective solutions that may not be implementable in the real operation of
the gas turbine system. This research advances the integration of the
data-driven domain knowledge into machine learning-powered analytics that
enhances the domain-informed operation excellence and paves the way for safe AI
adoption in thermal power systems.

</details>


### [198] [SPLASH! Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations](https://arxiv.org/abs/2507.08707)
*Peter Crowley,Zachary Serlin,Tyler Paine,Makai Mann,Michael Benjamin,Calin Belta*

Main category: cs.LG

TL;DR: 针对非最优、长周期和对抗性任务，本文提出了SPLASH方法，以提高逆强化学习（IRL）在真实世界机器人应用中的效能。


<details>
  <summary>Details</summary>
Motivation: 现有逆强化学习（IRL）方法通常假设存在专家演示，或无法有效处理长周期目标及对抗性任务中的非最优演示，这限制了IRL在实际机器人部署中的应用。

Method: 本文提出了一种名为SPLASH（Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations）的方法，旨在从非最优的分层演示中进行高效的偏好式逆强化学习，并适用于长周期和对抗性任务。

Result: SPLASH在模拟的海上夺旗任务中得到了经验验证，并通过自主无人水面车辆的仿真到实际转换实验展示了其真实世界适用性。实验表明，SPLASH在从非最优演示中进行奖励学习方面显著优于现有SOTA方法。

Conclusion: SPLASH有效解决了逆强化学习在处理长周期和对抗性非最优演示方面的局限性，显著提升了从非最优演示中进行奖励学习的性能，使其更适用于实际机器人应用。

Abstract: Inverse Reinforcement Learning (IRL) presents a powerful paradigm for
learning complex robotic tasks from human demonstrations. However, most
approaches make the assumption that expert demonstrations are available, which
is often not the case. Those that allow for suboptimality in the demonstrations
are not designed for long-horizon goals or adversarial tasks. Many desirable
robot capabilities fall into one or both of these categories, thus highlighting
a critical shortcoming in the ability of IRL to produce field-ready robotic
agents. We introduce Sample-efficient Preference-based inverse reinforcement
learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical
demonstrations (SPLASH), which advances the state-of-the-art in learning from
suboptimal demonstrations to long-horizon and adversarial settings. We
empirically validate SPLASH on a maritime capture-the-flag task in simulation,
and demonstrate real-world applicability with sim-to-real translation
experiments on autonomous unmanned surface vehicles. We show that our proposed
methods allow SPLASH to significantly outperform the state-of-the-art in reward
learning from suboptimal demonstrations.

</details>


### [199] [On the Effect of Regularization in Policy Mirror Descent](https://arxiv.org/abs/2507.08718)
*Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 政策镜像下降（PMD）在RL中结合了两种正则化器。本文通过大规模实证研究，发现尽管它们可部分替代，但其精确组合对鲁棒性能至关重要，揭示了超参数敏感性对算法鲁棒性的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管政策镜像下降（PMD）在理论上已被广泛研究，但针对其核心正则化组件相互作用的经验性研究却非常稀缺。

Method: 本文对PMD框架中两种关键正则化技术（距离项和MDP正则化器）的相互作用进行了大规模经验分析，在小型强化学习环境中运行了超过50万个训练种子。

Result: 研究结果表明，尽管这两种正则化器可以部分相互替代，但它们的精确组合对于实现鲁棒的性能至关重要。

Conclusion: 这些发现突出了在强化学习中推进更鲁棒算法研究的潜力，特别是在解决超参数敏感性问题方面。

Abstract: Policy Mirror Descent (PMD) has emerged as a unifying framework in
reinforcement learning (RL) by linking policy gradient methods with a
first-order optimization method known as mirror descent. At its core, PMD
incorporates two key regularization components: (i) a distance term that
enforces a trust region for stable policy updates and (ii) an MDP regularizer
that augments the reward function to promote structure and robustness. While
PMD has been extensively studied in theory, empirical investigations remain
scarce. This work provides a large-scale empirical analysis of the interplay
between these two regularization techniques, running over 500k training seeds
on small RL environments. Our results demonstrate that, although the two
regularizers can partially substitute each other, their precise combination is
critical for achieving robust performance. These findings highlight the
potential for advancing research on more robust algorithms in RL, particularly
with respect to hyperparameter sensitivity.

</details>


### [200] [Monitoring Risks in Test-Time Adaptation](https://arxiv.org/abs/2507.08721)
*Mona Schirmer,Metod Jazbec,Christian A. Naesseth,Eric Nalisnick*

Main category: cs.LG

TL;DR: 本文提出将测试时自适应（TTA）模型与风险监控框架结合，以在无标签测试数据的情况下，通过扩展现有统计监控工具，检测模型何时达到最终失效点。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时自适应（TTA）可以延长模型寿命，但它并非永久解决方案，模型最终仍可能退化到需要下线重训的程度。因此，需要一种机制来检测TTA模型何时达到这种最终失效点。

Method: 作者扩展了基于序贯检验和置信序列的现有风险监控工具，使其能够适应模型在测试时不断更新且无可用测试标签来估计性能指标的场景。

Result: 所提出的扩展解锁了严谨的统计风险监控在TTA中的应用，并且作者在代表性的数据集、分布漂移类型和TTA方法上验证了其TTA监控框架的有效性。

Conclusion: 该研究成功地将严谨的统计风险监控应用于测试时自适应（TTA）模型，提供了一个框架来检测模型何时因数据漂移而彻底失效，从而确保及时干预。

Abstract: Encountering shifted data at test time is a ubiquitous challenge when
deploying predictive models. Test-time adaptation (TTA) methods address this
issue by continuously adapting a deployed model using only unlabeled test data.
While TTA can extend the model's lifespan, it is only a temporary solution.
Eventually the model might degrade to the point that it must be taken offline
and retrained. To detect such points of ultimate failure, we propose pairing
TTA with risk monitoring frameworks that track predictive performance and raise
alerts when predefined performance criteria are violated. Specifically, we
extend existing monitoring tools based on sequential testing with confidence
sequences to accommodate scenarios in which the model is updated at test time
and no test labels are available to estimate the performance metrics of
interest. Our extensions unlock the application of rigorous statistical risk
monitoring to TTA, and we demonstrate the effectiveness of our proposed TTA
monitoring framework across a representative set of datasets, distribution
shift types, and TTA methods.

</details>


### [201] [Catastrophic Forgetting Mitigation Through Plateau Phase Activity Profiling](https://arxiv.org/abs/2507.08736)
*Idan Mashiach,Oren Glickman,Tom Tirer*

Main category: cs.LG

TL;DR: 该研究提出一种新颖的正则化方法，通过跟踪深度学习模型在训练后期（最终平稳期）的参数活动性，以有效缓解灾难性遗忘问题，并在新任务上保持高性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在学习新任务时存在灾难性遗忘问题，即新知识学习导致旧知识被覆盖，进而损害先前学习任务的性能。尽管正则化技术旨在识别并约束“重要”参数来缓解此问题，但在深度学习的高度非凸优化景观中，现有方法仍有改进空间。

Method: 本研究提出一个新颖的视角：相比于整个训练过程，在训练的最终平稳期跟踪参数活动性（移动和变异性）更为有效。作者认为，在此期间表现出高活跃性的参数揭示了损失景观中相对平坦的方向，使其既能适应新任务，又能保留旧知识。

Result: 通过综合实验证明，该方法在缓解灾难性遗忘与在新学习任务上保持强大性能之间取得了卓越的平衡。

Conclusion: 在训练的最终平稳期跟踪参数活动性是一种有效且优越的方法，能够缓解灾难性遗忘问题，同时确保在新任务上的高性能，为正则化技术提供了新的方向。

Abstract: Catastrophic forgetting in deep neural networks occurs when learning new
tasks degrades performance on previously learned tasks due to knowledge
overwriting. Among the approaches to mitigate this issue, regularization
techniques aim to identify and constrain "important" parameters to preserve
previous knowledge. In the highly nonconvex optimization landscape of deep
learning, we propose a novel perspective: tracking parameters during the final
training plateau is more effective than monitoring them throughout the entire
training process. We argue that parameters that exhibit higher activity
(movement and variability) during this plateau reveal directions in the loss
landscape that are relatively flat, making them suitable for adaptation to new
tasks while preserving knowledge from previous ones. Our comprehensive
experiments demonstrate that this approach achieves superior performance in
balancing catastrophic forgetting mitigation with strong performance on newly
learned tasks.

</details>


### [202] [Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy Chaotic Time Series](https://arxiv.org/abs/2507.08738)
*Azimov Sherkhon,Susana Lopez-Moreno,Eric Dolores-Cuenca,Sieun Lee,Sangil Kim*

Main category: cs.LG

TL;DR: 本文提出一种自适应非线性向量自回归（NVAR）模型，通过结合可学习的多层感知器（MLP）来克服传统NVAR和RC在噪声数据和高维环境下的局限性，实现了更好的预测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统非线性向量自回归（NVAR）和储备池计算（RC）依赖固定非线性，导致其在处理高噪声或真实世界数据时适应性受限；同时，它们在高维设置中因读出计算的矩阵求逆成本高昂而扩展性差。

Method: 提出一种自适应NVAR模型，结合延迟嵌入的线性输入和由浅层可学习多层感知器（MLP）生成的特征。MLP和线性读出通过梯度优化联合训练，使模型能学习数据驱动的非线性并保持简单读出结构。该方法避免了传统NVAR对岭参数和延迟参数的穷举搜索，将调优限制在神经网络超参数。

Result: 在无噪声和合成噪声条件下的混沌系统实验表明，自适应模型在预测精度上优于标准NVAR，并在噪声条件下以较低观测频率表现出鲁棒的预测能力。

Conclusion: 该自适应NVAR模型通过引入可学习的MLP和联合训练机制，有效解决了现有方法在适应性和可扩展性上的挑战，显著提升了混沌系统在噪声环境下的预测性能和鲁棒性。

Abstract: Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have
shown promise in forecasting chaotic dynamical systems, such as the Lorenz-63
model and El Nino-Southern Oscillation. However, their reliance on fixed
nonlinearities - polynomial expansions in NVAR or random feature maps in RC -
limits their adaptability to high noise or real-world data. These methods also
scale poorly in high-dimensional settings due to costly matrix inversion during
readout computation. We propose an adaptive NVAR model that combines
delay-embedded linear inputs with features generated by a shallow, learnable
multi-layer perceptron (MLP). The MLP and linear readout are jointly trained
using gradient-based optimization, enabling the model to learn data-driven
nonlinearities while preserving a simple readout structure. Unlike standard
NVAR, our approach avoids the need for an exhaustive and sensitive grid search
over ridge and delay parameters. Instead, tuning is restricted to neural
network hyperparameters, improving scalability. Initial experiments on chaotic
systems tested under noise-free and synthetically noisy conditions showed that
the adaptive model outperformed the standard NVAR in predictive accuracy and
showed robust forecasting under noisy conditions with a lower observation
frequency.

</details>


### [203] [Partitioned Hybrid Quantum Fourier Neural Operators for Scientific Quantum Machine Learning](https://arxiv.org/abs/2507.08746)
*Paolo Marcandelli,Yuanchun He,Stefano Mariani,Martina Siena,Stefano Markidis*

Main category: cs.LG

TL;DR: 介绍PHQFNO，一种用于科学机器学习的混合量子傅里叶神经算子，它将傅里叶计算分配到经典和量子资源中。该方法在Burgers和Navier-Stokes方程上评估，显示出与经典FNO相当或更高的精度，并具有更好的噪声稳定性。


<details>
  <summary>Details</summary>
Motivation: 为科学机器学习引入一种更通用且可调谐的量子-经典混合傅里叶神经算子，以克服现有量子傅里叶神经算子的局限性，并在复杂物理方程的求解中利用量子计算的潜力。

Method: PHQFNO通过在经典和量子资源之间划分傅里叶算子计算，实现可调谐的量子-经典混合和分布式执行。它将QFNO扩展到更高维度，并引入消息传递框架来分布数据。输入数据使用一元编码转换为量子态，量子电路参数通过变分方案优化。该方法使用PennyLane与PyTorch集成实现，并在Burgers方程、不可压缩和可压缩Navier-Stokes方程上进行评估。

Result: PHQFNO能够恢复经典傅里叶神经算子（FNO）的精度。在处理不可压缩纳维-斯托克斯方程时，PHQFNO比其经典对应物获得了更高的精度。对输入噪声的敏感性分析证实，PHQFNO比经典基线具有更好的稳定性。

Conclusion: PHQFNO作为一种通用且鲁棒的混合量子傅里叶神经算子，在科学机器学习领域展现出强大潜力。它在特定复杂方程的求解上能超越经典方法，并在存在噪声时保持优越的稳定性，为利用量子计算加速科学模拟提供了新途径。

Abstract: We introduce the Partitioned Hybrid Quantum Fourier Neural Operator (PHQFNO),
a generalization of the Quantum Fourier Neural Operator (QFNO) for scientific
machine learning. PHQFNO partitions the Fourier operator computation across
classical and quantum resources, enabling tunable quantum-classical
hybridization and distributed execution across quantum and classical devices.
The method extends QFNOs to higher dimensions and incorporates a
message-passing framework to distribute data across different partitions. Input
data are encoded into quantum states using unary encoding, and quantum circuit
parameters are optimized using a variational scheme. We implement PHQFNO using
PennyLane with PyTorch integration and evaluate it on Burgers' equation,
incompressible and compressible Navier-Stokes equations. We show that PHQFNO
recovers classical FNO accuracy. On incompressible Navier-Stokes, PHQFNO
achieves higher accuracy than its classical counterparts. Finally, we perform a
sensitivity analysis under input noise, confirming improved stability of PHQFNO
over classical baselines.

</details>


### [204] [Modeling Partially Observed Nonlinear Dynamical Systems and Efficient Data Assimilation via Discrete-Time Conditional Gaussian Koopman Network](https://arxiv.org/abs/2507.08749)
*Chuanqi Chen,Zhongrui Wang,Nan Chen,Jin-Long Wu*

Main category: cs.LG

TL;DR: 本文提出了一种离散时间条件高斯Koopman网络（CGKN），用于对高维复杂动力系统（如非线性偏微分方程）进行高效的状态预测和数据同化（DA），并实现了科学机器学习（SciML）与数据同化的统一。


<details>
  <summary>Details</summary>
Motivation: 针对工程和地球科学中常见的非线性、部分观测高维复杂动力系统，需要开发能够高效进行状态预测和数据同化的替代模型。

Method: 开发了离散时间条件高斯Koopman网络（CGKN）。该方法利用Koopman嵌入来发现未观测系统状态的潜在表示，使得潜在状态的动力学在给定观测状态下是条件线性的。这使得观测和潜在状态的建模系统成为条件高斯系统，其中潜在状态的后验分布是高斯的，并可通过解析公式高效评估。DA的解析公式促进了DA性能融入建模系统的学习过程，从而统一了SciML和DA。

Result: 在粘性Burgers方程、Kuramoto-Sivashinsky方程和二维Navier-Stokes方程等多个具有间歇性和湍流特征的非线性PDE经典问题上进行了验证。结果表明，CGKN框架在状态预测方面达到了与最先进的SciML方法相当的性能，并提供了高效准确的数据同化结果。

Conclusion: 离散时间CGKN框架成功地统一了科学机器学习（SciML）与数据同化（DA），并为将SciML模型开发与其外层应用（如设计优化、逆问题和最优控制）相结合提供了一个范例。

Abstract: A discrete-time conditional Gaussian Koopman network (CGKN) is developed in
this work to learn surrogate models that can perform efficient state forecast
and data assimilation (DA) for high-dimensional complex dynamical systems,
e.g., systems governed by nonlinear partial differential equations (PDEs).
Focusing on nonlinear partially observed systems that are common in many
engineering and earth science applications, this work exploits Koopman
embedding to discover a proper latent representation of the unobserved system
states, such that the dynamics of the latent states are conditional linear,
i.e., linear with the given observed system states. The modeled system of the
observed and latent states then becomes a conditional Gaussian system, for
which the posterior distribution of the latent states is Gaussian and can be
efficiently evaluated via analytical formulae. The analytical formulae of DA
facilitate the incorporation of DA performance into the learning process of the
modeled system, which leads to a framework that unifies scientific machine
learning (SciML) and data assimilation. The performance of discrete-time CGKN
is demonstrated on several canonical problems governed by nonlinear PDEs with
intermittency and turbulent features, including the viscous Burgers' equation,
the Kuramoto-Sivashinsky equation, and the 2-D Navier-Stokes equations, with
which we show that the discrete-time CGKN framework achieves comparable
performance as the state-of-the-art SciML methods in state forecast and
provides efficient and accurate DA results. The discrete-time CGKN framework
also serves as an example to illustrate unifying the development of SciML
models and their other outer-loop applications such as design optimization,
inverse problems, and optimal control.

</details>


### [205] [ML-Based Automata Simplification for Symbolic Accelerators](https://arxiv.org/abs/2507.08751)
*Tiffany Yu,Rye Stahle-Smith,Darssan Eswaramoorthi,Rasha Karakchi*

Main category: cs.LG

TL;DR: AutoSlim使用机器学习简化FPGA上符号加速器的NFA图，显著降低资源消耗并提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 符号加速器在基因组学、NLP和网络安全等领域应用日益广泛，但由于内存使用过多和路由复杂性，尤其是在处理大型数据集时，面临严重的扩展性问题。

Method: 本文提出了AutoSlim，一个基于机器学习的图简化框架。该框架旨在降低部署在FPGA叠加层（如NAPOLY+）上、基于非确定性有限自动机（NFA）的符号加速器的复杂性。AutoSlim利用随机森林分类，根据边分数和结构特征剪枝低影响的转换，从而在保持语义正确性的同时显著降低自动机图的密度。与现有工具不同，AutoSlim针对带权转换的自动化分数感知简化，实现了高效的基于排名的序列分析。

Result: 通过在NAPOLY+上对1K至64K节点的各种数据集进行评估，AutoSlim在FPGA LUTs上实现了高达40%的减少，转换剪枝超过30%，并且能够处理比现有基准大一个数量级的图。研究结果还表明，硬件互连（扇出）对硬件成本影响巨大，而AutoSlim的剪枝能有效缓解资源激增。

Conclusion: AutoSlim通过机器学习驱动的图简化，成功解决了FPGA上符号加速器面临的扩展性和资源效率问题，使其能够更高效地处理大规模数据集，并显著优化了硬件利用率。

Abstract: Symbolic accelerators are increasingly used for symbolic data processing in
domains such as genomics, NLP, and cybersecurity. However, these accelerators
face scalability issues due to excessive memory use and routing complexity,
especially when targeting a large set. We present AutoSlim, a machine
learning-based graph simplification framework designed to reduce the complexity
of symbolic accelerators built on Non-deterministic Finite Automata (NFA)
deployed on FPGA-based overlays such as NAPOLY+. AutoSlim uses Random Forest
classification to prune low-impact transitions based on edge scores and
structural features, significantly reducing automata graph density while
preserving semantic correctness. Unlike prior tools, AutoSlim targets automated
score-aware simplification with weighted transitions, enabling efficient
ranking-based sequence analysis. We evaluated data sets (1K to 64K nodes) in
NAPOLY+ and conducted performance measurements including latency, throughput,
and resource usage. AutoSlim achieves up to 40 percent reduction in FPGA LUTs
and over 30 percent pruning in transitions, while scaling to graphs an order of
magnitude larger than existing benchmarks. Our results also demonstrate how
hardware interconnection (fanout) heavily influences hardware cost and that
AutoSlim's pruning mitigates resource blowup.

</details>


### [206] [Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data](https://arxiv.org/abs/2507.08761)
*Jeonghye Kim,Yongjae Shin,Whiyoung Jung,Sunghoon Hong,Deunsol Yoon,Youngchul Sung,Kanghoon Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: 为解决离线强化学习中Q值外推误差问题，本文提出PARS算法，通过结合奖励缩放与层归一化(RS-LN)和对不可行动作的惩罚机制(PA)，引导Q值在数据范围外逐渐减小，从而在D4RL基准测试上取得优于SOTA算法的性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习(RL)面临Q值外推误差问题，尤其当Q函数在线性外推时超出数据范围会导致严重后果。

Method: 本文提出一种新算法PARS。该算法通过引导Q值在数据范围外逐渐减小来缓解Q值外推误差。具体方法包括：奖励缩放与层归一化(RS-LN)和对不可行动作的惩罚机制(PA)。PARS是这两种机制的结合。

Result: PARS算法在D4RL基准测试上的一系列任务中，包括离线训练和在线微调，均表现出优于现有先进算法的性能，特别是在极具挑战性的AntMaze Ultra任务中取得了显著成功。

Conclusion: PARS算法通过其独特的RS-LN和PA机制，有效解决了离线强化学习中的Q值外推误差问题，显著提升了算法在多种任务上的表现，证明了其在解决该领域挑战性问题上的有效性。

Abstract: Reinforcement learning with offline data suffers from Q-value extrapolation
errors. To address this issue, we first demonstrate that linear extrapolation
of the Q-function beyond the data range is particularly problematic. To
mitigate this, we propose guiding the gradual decrease of Q-values outside the
data range, which is achieved through reward scaling with layer normalization
(RS-LN) and a penalization mechanism for infeasible actions (PA). By combining
RS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a
range of tasks, demonstrating superior performance compared to state-of-the-art
algorithms in both offline training and online fine-tuning on the D4RL
benchmark, with notable success in the challenging AntMaze Ultra task.

</details>


### [207] [BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity](https://arxiv.org/abs/2507.08771)
*Chenyang Song,Weilin Zhao,Xu Han,Chaojun Xiao,Yingfa Chen,Yuxuan Li,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: BlockFFN是一种新型MoE架构，通过改进路由、优化训练和高效加速核，解决了LLM中MoE路由低效和块级稀疏性不足的问题，实现了高稀疏性和在端侧设备上的显著加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）计算负担重，现有MoE架构路由不可微、不灵活，影响性能。同时，MoE虽有令牌级稀疏性，但块级稀疏性不足，不利于低资源设备上的加速和与推测解码等主流技术兼容。

Method: 引入BlockFFN架构。具体方法包括：使用集成ReLU和RMSNorm的路由器实现可微且灵活的路由；设计CLS感知的训练目标以提高令牌级和块级稀疏性；首次实现结合激活稀疏性和推测解码的高效加速核。

Result: BlockFFN性能优于其他MoE基线。实现了超过80%的令牌级稀疏性（TLS）和70%的8-令牌块级稀疏性（CLS）。在真实端侧设备上，其加速核比密集模型实现了高达3.67倍的加速。

Conclusion: BlockFFN成功解决了现有MoE架构在路由和块级稀疏性上的挑战，通过创新的架构和优化技术，显著提升了LLM的性能和在低资源环境下的部署效率，实现了高稀疏性和显著加速。

Abstract: To alleviate the computational burden of large language models (LLMs),
architectures with activation sparsity, represented by mixture-of-experts
(MoE), have attracted increasing attention. However, the non-differentiable and
inflexible routing of vanilla MoE hurts model performance. Moreover, while each
token activates only a few parameters, these sparsely-activated architectures
exhibit low chunk-level sparsity, indicating that the union of multiple
consecutive tokens activates a large ratio of parameters. Such a sparsity
pattern is unfriendly for acceleration under low-resource conditions (e.g.,
end-side devices) and incompatible with mainstream acceleration techniques
(e.g., speculative decoding). To address these challenges, we introduce a novel
MoE architecture, BlockFFN, as well as its efficient training and deployment
techniques. Specifically, we use a router integrating ReLU activation and
RMSNorm for differentiable and flexible routing. Next, to promote both
token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training
objectives are designed, making BlockFFN more acceleration-friendly. Finally,
we implement efficient acceleration kernels, combining activation sparsity and
speculative decoding for the first time. The experimental results demonstrate
the superior performance of BlockFFN over other MoE baselines, achieving over
80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on
real end-side devices than dense models. All codes and checkpoints are
available publicly (https://github.com/thunlp/BlockFFN).

</details>


### [208] [Greedy Low-Rank Gradient Compression for Distributed Learning with Convergence Guarantees](https://arxiv.org/abs/2507.08784)
*Chuyan Chen,Yutong He,Pengrui Li,Weichen Jia,Kun Yuan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Distributed optimization is pivotal for large-scale signal processing and
machine learning, yet communication overhead remains a major bottleneck.
Low-rank gradient compression, in which the transmitted gradients are
approximated by low-rank matrices to reduce communication, offers a promising
remedy. Existing methods typically adopt either randomized or greedy
compression strategies: randomized approaches project gradients onto randomly
chosen subspaces, introducing high variance and degrading empirical
performance; greedy methods select the most informative subspaces, achieving
strong empirical results but lacking convergence guarantees. To address this
gap, we propose GreedyLore--the first Greedy Low-Rank gradient compression
algorithm for distributed learning with rigorous convergence guarantees.
GreedyLore incorporates error feedback to correct the bias introduced by greedy
compression and introduces a semi-lazy subspace update that ensures the
compression operator remains contractive throughout all iterations. With these
techniques, we prove that GreedyLore achieves a convergence rate of
$\mathcal{O}(\sigma/\sqrt{NT} + 1/T)$ under standard optimizers such as MSGD
and Adam--marking the first linear speedup convergence rate for low-rank
gradient compression. Extensive experiments are conducted to validate our
theoretical findings.

</details>


### [209] [Optimistic Exploration for Risk-Averse Constrained Reinforcement Learning](https://arxiv.org/abs/2507.08793)
*James McCarthy,Radu Marinescu,Elizabeth Daly,Ivana Dusparic*

Main category: cs.LG

TL;DR: 为解决风险规避型受限强化学习（RaCRL）中保守探索导致次优策略的问题，提出ORAC方法，通过乐观探索奖励和悲观规避风险，有效提升收益-成本权衡并避免次优收敛。


<details>
  <summary>Details</summary>
Motivation: 风险规避型受限强化学习（RaCRL）中，风险规避通常导致保守探索，从而收敛到次优策略，未能充分最大化奖励或实现目标。

Method: 提出一种基于探索的RaCRL方法——乐观风险规避Actor Critic (ORAC)。该方法通过最大化状态-动作奖励值函数的局部上置信边界，并最小化风险规避状态-动作成本值函数的局部下置信边界来构建探索策略。同时，根据是否超出安全约束值动态调整成本值的权重。

Result: 实验证明ORAC方法能有效避免收敛至次优策略，并在Safety-Gymnasium和CityLearn等多种连续控制任务中显著改善了收益-成本权衡。

Conclusion: ORAC通过鼓励探索不确定环境区域以发现高奖励状态，同时严格满足安全约束，成功解决了RaCRL中因保守探索导致的次优问题，实现了更优的性能与安全性平衡。

Abstract: Risk-averse Constrained Reinforcement Learning (RaCRL) aims to learn policies
that minimise the likelihood of rare and catastrophic constraint violations
caused by an environment's inherent randomness. In general, risk-aversion leads
to conservative exploration of the environment which typically results in
converging to sub-optimal policies that fail to adequately maximise reward or,
in some cases, fail to achieve the goal. In this paper, we propose an
exploration-based approach for RaCRL called Optimistic Risk-averse Actor Critic
(ORAC), which constructs an exploratory policy by maximising a local upper
confidence bound of the state-action reward value function whilst minimising a
local lower confidence bound of the risk-averse state-action cost value
function. Specifically, at each step, the weighting assigned to the cost value
is increased or decreased if it exceeds or falls below the safety constraint
value. This way the policy is encouraged to explore uncertain regions of the
environment to discover high reward states whilst still satisfying the safety
constraints. Our experimental results demonstrate that the ORAC approach
prevents convergence to sub-optimal policies and improves significantly the
reward-cost trade-off in various continuous control tasks such as
Safety-Gymnasium and a complex building energy management environment
CityLearn.

</details>


### [210] [One Token to Fool LLM-as-a-Judge](https://arxiv.org/abs/2507.08794)
*Yulai Zhao,Haolin Liu,Dian Yu,S. Y. Kung,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 研究发现，大语言模型（LLMs）作为奖励模型在评估答案质量时，易受无关符号或推理引导语等表面操作的干扰，导致误报。为解决此问题，本研究提出一种数据增强策略，并训练出一个鲁棒性显著提升的新型奖励模型。


<details>
  <summary>Details</summary>
Motivation: 生成式奖励模型（LLMs-as-judges）在强化学习中日益普及，尤其适用于评估复杂、自由形式输出的答案质量。然而，研究发现这些模型对非词符号（如冒号、句号）或推理引导语（如“思考过程：”）等表面性操作表现出惊人的脆弱性，常常导致错误的正面奖励（误报）。这种弱点普遍存在于不同LLM、数据集和提示格式中，对依赖生成式奖励模型的关键算法范式（如拒绝采样、偏好优化和RLVR）构成严重威胁。

Method: 为了缓解上述问题，研究引入了一种简单而有效的数据增强策略。基于此策略，训练了一个新的生成式奖励模型，旨在显著提高其鲁棒性，使其不易受表面性操作的影响。

Result: 通过采用所提出的数据增强策略，成功训练出一个新型生成式奖励模型。该模型在鲁棒性方面取得了显著提升，能够更准确地评估答案质量，有效避免了因无关符号或引导语导致的误报问题。

Conclusion: 本研究的发现凸显了对更可靠的基于LLM的评估方法的迫切需求。提出的数据增强策略和训练出的鲁棒奖励模型为解决现有LLM奖励模型的脆弱性问题提供了一个有效途径，并为未来的研究指明了方向。

Abstract: Generative reward models (also known as LLMs-as-judges), which use large
language models (LLMs) to evaluate answer quality, are increasingly adopted in
reinforcement learning with verifiable rewards (RLVR). They are often preferred
over rigid rule-based metrics, especially for complex reasoning tasks involving
free-form outputs. In this paradigm, an LLM is typically prompted to compare a
candidate answer against a ground-truth reference and assign a binary reward
indicating correctness. Despite the seeming simplicity of this comparison task,
we find that generative reward models exhibit surprising vulnerabilities to
superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning
openers like "Thought process:" and "Let's solve this problem step by step."
can often lead to false positive rewards. We demonstrate that this weakness is
widespread across LLMs, datasets, and prompt formats, posing a serious threat
for core algorithmic paradigms that rely on generative reward models, such as
rejection sampling, preference optimization, and RLVR. To mitigate this issue,
we introduce a simple yet effective data augmentation strategy and train a new
generative reward model with substantially improved robustness. Our findings
highlight the urgent need for more reliable LLM-based evaluation methods. We
release our robust, general-domain reward model and its synthetic training data
at https://huggingface.co/sarosavo/Master-RM and
https://huggingface.co/datasets/sarosavo/Master-RM.

</details>


### [211] [The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?](https://arxiv.org/abs/2507.08802)
*Denis Sutter,Julian Minder,Thomas Hofmann,Tiago Pimentel*

Main category: cs.LG

TL;DR: 本文批判性地审视了因果抽象概念，证明在不对齐射函数施加线性限制时，任何神经网络都可以映射到任何算法，从而使该概念变得琐碎和无意义。研究认为，如果没有关于模型如何编码信息的假设，因果抽象不足以实现机械可解释性。


<details>
  <summary>Details</summary>
Motivation: 因果抽象被广泛用于解释机器学习模型的决策过程，但大多数相关工作都假设了线性映射。本文旨在质疑这一线性约束，并探讨当允许任意强大的对齐映射时，因果抽象的局限性和信息量。

Method: 研究方法包括理论证明和实证验证。理论上，证明了在合理假设下，任何神经网络都可以映射到任何算法。实证上，通过在随机初始化的语言模型上进行实验，展示了即使模型无法解决实际任务，也能完美地将其映射到算法。

Result: 研究结果表明，不受限制的因果抽象（允许任意强大的对齐映射）是琐碎且信息量不足的。例如，在间接宾语识别任务中，即使是随机初始化的语言模型也能达到100%的互换-干预准确率。这引出了“非线性表示困境”：去除线性约束后，难以平衡映射的复杂性与准确性。

Conclusion: 结论是，因果抽象本身不足以实现机械可解释性，因为它在没有关于模型如何编码信息的假设时将变得空泛。未来的工作应着重研究信息编码假设与因果抽象之间的关系。

Abstract: The concept of causal abstraction got recently popularised to demystify the
opaque decision-making processes of machine learning models; in short, a neural
network can be abstracted as a higher-level algorithm if there exists a
function which allows us to map between them. Notably, most interpretability
papers implement these maps as linear functions, motivated by the linear
representation hypothesis: the idea that features are encoded linearly in a
model's representations. However, this linearity constraint is not required by
the definition of causal abstraction. In this work, we critically examine the
concept of causal abstraction by considering arbitrarily powerful alignment
maps. In particular, we prove that under reasonable assumptions, any neural
network can be mapped to any algorithm, rendering this unrestricted notion of
causal abstraction trivial and uninformative. We complement these theoretical
findings with empirical evidence, demonstrating that it is possible to
perfectly map models to algorithms even when these models are incapable of
solving the actual task; e.g., on an experiment using randomly initialised
language models, our alignment maps reach 100% interchange-intervention
accuracy on the indirect object identification task. This raises the non-linear
representation dilemma: if we lift the linearity constraint imposed to
alignment maps in causal abstraction analyses, we are left with no principled
way to balance the inherent trade-off between these maps' complexity and
accuracy. Together, these results suggest an answer to our title's question:
causal abstraction is not enough for mechanistic interpretability, as it
becomes vacuous without assumptions about how models encode information.
Studying the connection between this information-encoding assumption and causal
abstraction should lead to exciting future work.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [212] [Photonic Rails in ML Datacenters](https://arxiv.org/abs/2507.08119)
*Eric Ding,Chuhan Ouyang,Rachee Singh*

Main category: cs.NI

TL;DR: 针对大规模机器学习训练的导轨网络，提出使用光学电路交换机替代电交换机，并通过并行驱动的导轨重构和Opus控制平面，解决光学开关的限制，实现网络与模型并行维度的协同演进。


<details>
  <summary>Details</summary>
Motivation: 大规模机器学习训练中，导轨优化的网络结构是主流，但使用高基数电交换机提供全对全连接会产生巨大的功耗、成本和复杂性开销。

Method: 提出保留导轨通信语义，但通过光学电路交换机实现。为克服光学开关一对一连接的限制，引入“并行驱动的导轨重构”方案，利用不同并行度流量间的顺序性。设计了控制平面Opus，通过时分复用技术实现光学开关对电导轨开关的仿真。

Result: 通过并行驱动的导轨重构和Opus控制平面，成功利用光学开关实现了电导轨开关的时分复用仿真，并提出了一种新的研究议程：数据中心网络结构能够与每个任务中的模型并行维度协同演进。

Conclusion: 本研究倡导一种新的数据中心网络结构范式：网络应与每个机器学习任务内部的模型并行维度动态协同演进，而非沿袭作业开始前预先配置网络的传统思维，从而实现更高效、灵活的ML训练基础设施。

Abstract: Rail-optimized network fabrics have become the de facto datacenter scale-out
fabric for large-scale ML training. However, the use of high-radix electrical
switches to provide all-to-all connectivity in rails imposes massive power,
cost, and complexity overheads. We propose a rethinking of the rail abstraction
by retaining its communication semantics, but realizing it using optical
circuit switches. The key challenge is that optical switches support only
one-to-one connectivity at a time, limiting the fan-out of traffic in ML
workloads using hybrid parallelisms. We introduce parallelism-driven rail
reconfiguration as a solution that leverages the sequential ordering between
traffic from different parallelisms. We design a control plane, Opus, to enable
time-multiplexed emulation of electrical rail switches using optical switches.
More broadly, our work discusses a new research agenda: datacenter fabrics that
co-evolve with the model parallelism dimensions within each job, as opposed to
the prevailing mindset of reconfiguring networks before a job begins.

</details>


### [213] [Rattan: An Extensible and Scalable Modular Internet Path Emulator](https://arxiv.org/abs/2507.08134)
*Minhu Wang,Yixin Shen,Bo Wang,Haixuan Tong,Yutong Xie,Yixuan Gao,Yan Liu,Li Chen,Mingwei Xu,Jianping Wu*

Main category: cs.NI

TL;DR: 本文提出Rattan，一个可扩展且可伸缩的软件网络路径模拟器，通过其创新的单元格（cell-based）架构，解决了现有模拟器在灵活性、可扩展性和可用性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着互联网路径在异构性、规模和动态性方面的快速增长，现有网络模拟器在灵活性、可扩展性和可用性方面变得日益不足，无法满足现代互联网条件的需求。

Method: Rattan采用基于单元格的架构：将模拟功能分解为模块化的“单元格”，这些单元格具有定义良好的异步接口。用户可以通过分层链接轻松组合不同单元格，也可以使用标准单元格接口构建新单元格。

Result: 这种设计实现了：1) 高可扩展性，支持单机上数百个并发千兆位级路径，以及多机组成的集群级实验；2) 高可扩展性，通过构建新单元格可模拟新的网络条件。

Conclusion: Rattan使开发人员和研究人员能够高效且自信地评估、验证和诊断针对在线服务的各种网络传输创新。

Abstract: The rapid growth of Internet paths in heterogeneity, scale, and dynamics has
made existing emulators increasingly insufficient in flexibility, scalability,
and usability. To address these limitations, we present Rattan, an extensible
and scalable software network path emulator for modern Internet conditions.
Rattan's core innovation lies in its cell-based architecture: by splitting
emulation functions into modular "cells" with well-documented asynchronous
interfaces, users are allowed to easily compose different cells by
hierarchically linking them and easily construct new cells by using standard
cell interfaces. This design enables: (1) scalability, supporting hundreds of
concurrent gigabit-level paths on a single machine and cluster-level
experiments composed of multiple machines; (2) extensibility, simulating new
network conditions by constructing new cells. Rattan empowers developers and
researchers to efficiently and confidently evaluate, validate, and diagnose
diverse network transport innovations for online services.

</details>


### [214] [KP-A: A Unified Network Knowledge Plane for Catalyzing Agentic Network Intelligence](https://arxiv.org/abs/2507.08164)
*Yun Tang,Mengbang Zou,Zeinab Nezami,Syed Ali Raza Zaidi,Weisi Guo*

Main category: cs.NI

TL;DR: 针对6G自治网络中LLM/Agent智能任务知识获取分散的问题，本文提出KP-A统一网络知识平面，旨在解耦知识管理和智能逻辑，从而提升开发效率、维护性与互操作性。


<details>
  <summary>Details</summary>
Motivation: 当前6G自治网络中，大语言模型（LLM）和Agent系统在执行各自的智能任务时，需要独立的知识检索管道，这导致了数据流冗余和解释不一致的问题。

Method: 本文受Open-RAN服务模型统一化的启发，提出并设计了KP-A（Agentic网络智能统一网络知识平面），其核心思想是将网络知识的获取和管理与智能逻辑解耦。

Result: KP-A简化了智能工程师的开发流程，降低了维护复杂度，并通过提供直观一致的知识接口，增强了网络智能代理的互操作性。该方案已在实时网络知识问答和边缘AI服务编排这两个代表性智能任务中得到验证，且所有实现工件均已开源。

Conclusion: KP-A通过统一网络知识管理，有效解决了6G自治网络中知识分散和不一致的问题，显著提升了开发效率和系统互操作性，为未来的网络智能化发展和标准化奠定了基础。

Abstract: The emergence of large language models (LLMs) and agentic systems is enabling
autonomous 6G networks with advanced intelligence, including
self-configuration, self-optimization, and self-healing. However, the current
implementation of individual intelligence tasks necessitates isolated knowledge
retrieval pipelines, resulting in redundant data flows and inconsistent
interpretations. Inspired by the service model unification effort in Open-RAN
(to support interoperability and vendor diversity), we propose KP-A: a unified
Network Knowledge Plane specifically designed for Agentic network intelligence.
By decoupling network knowledge acquisition and management from intelligence
logic, KP-A streamlines development and reduces maintenance complexity for
intelligence engineers. By offering an intuitive and consistent knowledge
interface, KP-A also enhances interoperability for the network intelligence
agents. We demonstrate KP-A in two representative intelligence tasks: live
network knowledge Q&A and edge AI service orchestration. All implementation
artifacts have been open-sourced to support reproducibility and future
standardization efforts.

</details>


### [215] [Towards AI-Native RAN: An Operator's Perspective of 6G Day 1 Standardization](https://arxiv.org/abs/2507.08403)
*Nan Li,Qi Sun,Lehan Wang,Xiaofei Xu,Jinri Huang,Chunhui Liu,Jing Gao,Yuhong Huang,Chih-Lin I*

Main category: cs.NI

TL;DR: 本文探讨了6G AI原生无线接入网（RAN）的设计与标准化原则，重点关注其Day 1架构及三大核心能力。通过大规模外场试验验证，所提方案在降低时延、提升故障识别和降低能耗方面表现出色，旨在为6G AI原生RAN的标准化提供实践框架。


<details>
  <summary>Details</summary>
Motivation: 5G网络中AI/ML仅为附加功能而非原生集成，未能有效解决其复杂性。6G网络需从设计之初便原生整合AI/ML，以应对其固有复杂性并支持普适AI应用。

Method: ['基于作者团队从2G到5G的移动网络运营和标准化经验，探索6G AI原生无线接入网（RAN）的设计与标准化原则，特别是其Day 1架构、功能和能力。', '提出了AI原生RAN的框架及其三大核心能力：AI驱动的RAN处理/优化/自动化、可靠的AI生命周期管理（LCM）和AI即服务（AIaaS）供应。', '提出了AI原生6G RAN的Day 1架构及标准化方案。', '通过构建包含超过5000个5G-A基站的大规模外场试验对所提架构和AI功能进行验证。']

Result: ['所提出的AI原生RAN架构和支持的AI功能，在大规模外场试验中显著降低了平均空口时延。', '提升了网络根源故障识别能力。', '显著降低了网络能耗。']

Conclusion: 本文为6G AI原生RAN的标准化设计提供了一个Day 1框架，旨在平衡技术创新与实际部署，为6G未来发展奠定基础，并证明了其在性能提升和效率优化方面的潜力。

Abstract: Artificial Intelligence/Machine Learning (AI/ML) has become the most certain
and prominent feature of 6G mobile networks. Unlike 5G, where AI/ML was not
natively integrated but rather an add-on feature over existing architecture, 6G
shall incorporate AI from the onset to address its complexity and support
ubiquitous AI applications. Based on our extensive mobile network operation and
standardization experience from 2G to 5G, this paper explores the design and
standardization principles of AI-Native radio access networks (RAN) for 6G,
with a particular focus on its critical Day 1 architecture, functionalities and
capabilities. We investigate the framework of AI-Native RAN and present its
three essential capabilities to shed some light on the standardization
direction; namely, AI-driven RAN processing/optimization/automation, reliable
AI lifecycle management (LCM), and AI-as-a-Service (AIaaS) provisioning. The
standardization of AI-Native RAN, in particular the Day 1 features, including
an AI-Native 6G RAN architecture, were proposed. For validation, a large-scale
field trial with over 5000 5G-A base stations have been built and delivered
significant improvements in average air interface latency, root cause
identification, and network energy consumption with the proposed architecture
and the supporting AI functions. This paper aims to provide a Day 1 framework
for 6G AI-Native RAN standardization design, balancing technical innovation
with practical deployment.

</details>


### [216] [Age of Information Optimization in Laser-charged UAV-assisted IoT Networks: A Multi-agent Deep Reinforcement Learning Method](https://arxiv.org/abs/2507.08429)
*Geng Sun,Likun Zhang,Jiahui Li,Jing Wu,Jiacheng Wang,Zemin Sun,Changyuan Zhao,Victor C. M. Leung*

Main category: cs.NI

TL;DR: 本文研究激光充电无人机辅助物联网网络中的信息年龄（AoI）优化问题，提出一种新型的多智能体近端策略优化（MAPPO-TM）框架，有效降低峰值AoI并提高能源效率。


<details>
  <summary>Details</summary>
Motivation: 无人机（UAV）集成物联网（IoT）网络在数据采集中具有潜力，但UAV有限的能量是主要挑战。激光束导向器（LBD）为UAV提供无线充电，实现持续数据收集。然而，如何在此类网络中优化信息年龄（AoI），同时平衡数据收集效率和能量消耗，是一个具有非凸性和复杂时间依赖性的难题。

Method: 本文提出了一个联合优化问题，旨在最小化峰值AoI，并同时确定最优的UAV轨迹和激光充电策略。为解决这一非凸挑战，本文提出了一种新颖的具有时间记忆和多智能体协调功能的多智能体近端策略优化（MAPPO-TM）框架。MAPPO-TM通过结合时间记忆机制捕捉UAV操作的动态性，并通过去中心化学习促进多UAV之间的有效协调，同时考虑全局系统目标。

Result: 仿真结果表明，所提出的MAPPO-TM算法在峰值AoI最小化和能源效率方面优于传统方法。与传统的多智能体深度强化学习（MADRL）方法相比，该算法在峰值AoI方面实现了高达15.1%的降低。

Conclusion: 所提出的MAPPO-TM算法能有效优化LBD供电的无人机辅助物联网网络中的信息年龄和能量效率，为持续数据收集提供了可靠的解决方案。

Abstract: The integration of unmanned aerial vehicles (UAVs) with Internet of Things
(IoT) networks offers promising solutions for efficient data collection.
However, the limited energy capacity of UAVs remains a significant challenge.
In this case, laser beam directors (LBDs) have emerged as an effective
technology for wireless charging of UAVs during operation, thereby enabling
sustained data collection without frequent returns to charging stations (CSs).
In this work, we investigate the age of information (AoI) optimization in
LBD-powered UAV-assisted IoT networks, where multiple UAVs collect data from
distributed IoTs while being recharged by laser beams. We formulate a joint
optimization problem that aims to minimize the peak AoI while determining
optimal UAV trajectories and laser charging strategies. This problem is
particularly challenging due to its non-convex nature, complex temporal
dependencies, and the need to balance data collection efficiency with energy
consumption constraints. To address these challenges, we propose a novel
multi-agent proximal policy optimization with temporal memory and multi-agent
coordination (MAPPO-TM) framework. Specifically, MAPPO-TM incorporates temporal
memory mechanisms to capture the dynamic nature of UAV operations and
facilitates effective coordination among multiple UAVs through decentralized
learning while considering global system objectives. Simulation results
demonstrate that the proposed MAPPO-TM algorithm outperforms conventional
approaches in terms of peak AoI minimization and energy efficiency. Ideally,
the proposed algorithm achieves up to 15.1% reduction in peak AoI compared to
conventional multi-agent deep reinforcement learning (MADRL) methods.

</details>


### [217] [Recovery of UAV Swarm-enabled Collaborative Beamforming in Low-altitude Wireless Networks under Wind Field Disturbances](https://arxiv.org/abs/2507.08507)
*Geng Sun,Chenbang Liu,Jiahui Li,Guannan Qu,Shuang Liang,Jiacheng Wang,Changyuan Zhao,Dusit Niyato*

Main category: cs.NI

TL;DR: 本论文提出了一种名为PPO-LA的深度强化学习算法，旨在解决无人机群在风场扰动下，其协作波束成形（CB）性能下降的问题，并成功恢复了通信性能。


<details>
  <summary>Details</summary>
Motivation: 无人机群通过协作波束成形（CB）可在低空无线网络（LAWN）中显著增强通信能力，但环境扰动，特别是风场，会导致无人机位置误差，从而严重劣化CB性能，影响传输可靠性。因此，亟需一种方法来维持风场扰动下的CB性能。

Method: 本研究构建了常风、剪切风和湍流风三种不同风况对无人机阵列性能影响的模型。接着，将最大化方向性并最小化最大旁瓣电平的问题公式化为长期实时优化问题，通过自适应调整激励电流权重来实现。为解决此复杂问题，论文提出了一种基于长短期记忆（LSTM）结构和自适应学习率的近端策略优化（PPO）算法（PPO-LA），该算法能够有效捕捉风场扰动的时间模式并实现实时自适应，无需针对特定风况进行大量预训练。

Result: 仿真结果表明，所提出的PPO-LA算法成功地恢复了各种风场场景下劣化的协作波束成形性能，并且其表现显著优于其他基准算法。

Conclusion: PPO-LA算法能够有效应对风场扰动对无人机群协作波束成形性能的影响，显著提升了其在低空无线网络中的传输可靠性。

Abstract: Unmanned aerial vehicle (UAV) swarms utilizing collaborative beamforming (CB)
in low-altitude wireless networks (LAWN) demonstrate significant potential for
enhanced communication range, energy efficiency, and signal directivity through
the formation of virtual antenna arrays (VAA). However, environmental
disturbances, particularly wind fields, significantly degrade CB performance by
introducing positional errors that disrupt beam patterns, thereby compromising
transmission reliability. This paper investigates the critical challenge of
maintaining CB performance in UAV-based VAAs operating in LAWN under wind field
disturbances. We propose a comprehensive framework that models the impact of
three distinct wind conditions (constant, shear, and turbulent) on UAV array
performance, and formulate a long-term real-time optimization problem to
maximize directivity while minimizing maximum sidelobe levels through adaptive
excitation current weight adjustments. To address the inherent complexity of
this problem, we propose a novel proximal policy optimization algorithm with
long short-term memory (LSTM) structure and adaptive learning rate (PPO-LA),
which effectively captures temporal patterns in wind field disturbances and
enables real-time adaptation without requiring extensive prior training for
specific wind conditions. Our simulation results demonstrate that the proposed
PPO-LA algorithm successfully recovers degraded CB performance across various
wind scenarios, and thus significantly outperforming benchmark algorithms.

</details>


### [218] [Stabilizing and Optimizing Inter-Shell Routing in LEO Networks with Integrated Routing Cost](https://arxiv.org/abs/2507.08549)
*Yaojia Wang,Qi Zhang,Kun Qiu,Yue Gao*

Main category: cs.NI

TL;DR: LEO巨型星座网络（LMCN）的星间路由稳定性受动态拓扑和频繁ISL切换挑战。本文提出DP-IRC算法，通过动态规划和集成路由成本（IRC）度量平衡跳数和ISL稳定性，显著降低ISL切换率并保持高路由效率。


<details>
  <summary>Details</summary>
Motivation: LEO巨型星座网络（LMCN）由于动态网络拓扑和频繁的星间链路（ISL）切换，其星间路由稳定性面临严峻挑战。现有策略（如最小跳数路径集）为减少延迟而优先考虑最小化跳数，但忽略了ISL切换成本，导致高不稳定性。而自适应路径路由方案虽引入路径相似性阈值以降低切换频率，但其贪婪策略常陷入局部最优，牺牲了星间路径距离效率。

Method: 本文提出一种基于动态规划的集成路由成本（DP-IRC）算法，专为星间路由优化设计。该算法将多层路径建模为多阶段决策问题，并通过一个集成路由成本（IRC）度量来平衡跳数和ISL稳定性，该度量综合考虑了星间/星内跳数以及ISL切换成本。

Result: 通过在60个时间段内，使用真实Starlink和OneWeb配置进行的实验表明，DP-IRC算法相比最小跳数路径集策略和自适应路径路由方案，分别将星间ISL切换率降低了39.1%和22.0%，同时仍保持了接近最优的端到端距离。

Conclusion: DP-IRC算法有效解决了LEO巨型星座网络中星间路由的稳定性问题，通过智能地平衡路由效率（跳数）和网络稳定性（ISL切换），显著提升了巨型星座网络的路由性能，为未来LMCN的路由优化提供了可行且高效的解决方案。

Abstract: The low Earth orbit (LEO) mega-constellation network (LMCN), which uses
thousands of satellites across multi-shell architectures to deliver different
services, is facing challenges in inter-shell routing stability due to dynamic
network topologies and frequent inter-satellite link (ISL) switching. Existing
strategies, such as the Minimum Hop Path set, prioritize minimizing hop counts
to reduce latency, but ignore ISL switching costs, which leads to high
instability. To overcome this, the Adaptive Path Routing Scheme introduces path
similarity thresholds to reduce the ISL switching frequency between shells.
However, the greedy approach of Adaptive Path Routing Scheme is often trapped
in local optima, sacrificing inter-shell path distance efficiency. To address
these limitations, we propose the Dynamic Programming-based Integrated Routing
Cost (DP-IRC) algorithm, which is designed explicitly for inter-shell routing
optimization. By formulating multi-shell paths as a multistage decision
problem, DP-IRC balances hop counts and ISL stability through an Integrated
Routing Cost (IRC) metric, combining inter-/intra-shell hops and switching
costs. Experiments over 60 time slots with real-world Starlink and OneWeb
configurations show that DP-IRC reduces inter-shell ISL switching rates by
39.1% and 22.0% compared to the Minimum Hop Path set strategy and Adaptive Path
Routing Scheme, respectively, while still maintaining near-optimal end-to-end
distances.

</details>


### [219] [Qualitative Assessment of Low Power Wide Area Network Protocols and their Security Aspect](https://arxiv.org/abs/2507.08677)
*Wesley dos Reis Bezerra,Lais Machado Bezerra,Carlos Becker Westphal*

Main category: cs.NI

TL;DR: 本文分析了LoRaWAN、NB-IoT和Sigfox等低功耗广域网（LPWAN）协议的定性特征，并探讨了它们在基于长寿命电池的稀疏传感器物联网解决方案中的应用挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 物联网中通信选项众多，特别是在受限和电池供电设备领域（如LPWAN），理解各选项的差异和特性对专业人士和研究人员而言是一个挑战。

Method: 本研究通过文献调研，对LoRaWAN、NB-IoT和Sigfox三种LPWAN协议进行了分析，并对LoRaWAN进行了详细阐述。

Result: 对所选网络协议（LoRaWAN）及其在稀疏传感器物联网解决方案中的应用进行了讨论。

Conclusion: 讨论了LPWAN协议在稀疏传感器物联网应用中的特性，旨在帮助克服理解多种通信选项的挑战。

Abstract: There are currently many communication options in the Internet of Things,
even in particular areas such as constrained and battery-powered devices, such
as Low Power Wide Area Networks. Understanding the differences and
characteristics of each option is a challenge, even for professionals and
researchers in the field. To meet this need, this work analyses the qualitative
characteristics of Low Power Wide Area Network protocols and the challenges and
opportunities of using constrained devices for sparse networks based on
long-life batteries. For this study, a bibliographic survey of the literature
was carried out as an analysis of three protocols (LoRaWAN, NB-IoT, and
Sigfox), and a detailing of the first one. As a result, there is a discussion
about the chosen network protocol and its use in IoT solutions with sparse
sensors.

</details>


### [220] [Knowledge Graph-Based approach for Sustainable 6G End-to-End System Design](https://arxiv.org/abs/2507.08717)
*Akshay Jain,Sylvaine Kerboeuf,Sokratis Barmpounakis,Cristóbal Vinagre Z.,Stefan Wendt,Dinh Thai Bui,Pol Alemany,Riccardo Nicolicchia,José María Jorquera Valero,Dani Korpi,Mohammad Hossein Moghaddam,Mikko A. Uusitalo,Patrik Rugeland,Abdelkader Outtagarts,Karthik Upadhya,Panagiotis Demestichas,Raul Muñoz,Manuel Gil Pérez,Daniel Adanza,Ricard Vilalta*

Main category: cs.NI

TL;DR: 本文提出一种基于知识图谱（KG）的6G端到端系统设计方法，旨在同时优化性能和可持续性，并通过在合作移动机器人用例中的应用验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有通信系统（如5G）主要关注性能指标（KPIs）。然而，6G需同时满足不断演进的KPIs和ICT行业雄心勃勃的可持续发展目标，这使得系统设计更为复杂。此外，将技术使能器与主观可持续性指标（特别是社会可持续性）关联起来的研究存在显著空白。

Method: 引入一种新颖的基于知识图谱（KG）的6G端到端（E2E）系统设计方法。该方法将用例KPIs、用例可持续性要求（关键价值KVs及关键价值指标KVIs）、技术使能器满足KPIs和KVIs的能力、Hexa-X-II项目定义的6G系统设计原则、使能器的成熟度以及使能器之间的依赖关系作为输入。KG方法中还包含一种确定技术使能器所解决关键价值的新方法。

Result: 该KG方法已应用于Hexa-X-II项目中合作移动机器人用例的6G E2E系统设计，成功筛选出82个使能器。部分选定使能器的概念验证演示结果也已提供。

Conclusion: 概念验证演示结果证实了该知识图谱方法在设计可持续6G系统方面的有效性，表明其能够有效连接技术使能器与可持续性目标，实现性能与可持续性兼顾的6G系统设计。

Abstract: Previous generations of cellular communication, such as 5G, have been
designed with the objective of improving key performance indicators (KPIs) such
as throughput, latency, etc. However, to meet the evolving KPI demands as well
as the ambitious sustainability targets for the ICT industry, 6G will need to
be designed differently. Concretely, 6G will need to consider both the
performance and sustainability targets for the various use cases it will serve.
Moreover, like previous generations, 6G will have various candidate
technological enablers, making the design space of the system even more
complex. Furthermore, given the subjective nature of the sustainability
indicators, in particular social sustainability, there is a significant gap in
literature on how technical enablers and 6G System design can be linked to
them. Hence, in this article a novel method for 6G end-to-end (E2E) system
design based on Knowledge graphs (KG) has been introduced. It considers as its
input: the use case KPIs, use case sustainability requirements expressed as Key
Values (KV) and KV Indicators (KVIs), the ability of the technological enablers
to satisfy these KPIs and KVIs, the 6G system design principles defined in
Hexa-X-II project, the maturity of a technological enabler and the dependencies
between the various enablers. As part of the KG method, a novel approach for
determining the key values a technological enabler addresses, has also been
introduced. The effectiveness of the KG method was demonstrated by its
application in designing the 6G E2E system for the cooperating mobile robot use
case defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,
results from proof-of-concept demonstrations for a subset of the selected
enablers have also been provided, which reinforce the efficacy of the KG method
for designing a sustainable 6G system.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [221] [Quantum Properties Trojans (QuPTs) for Attacking Quantum Neural Networks](https://arxiv.org/abs/2507.08202)
*Sounak Bhowmik,Travis S. Humble,Himanshu Thapliyal*

Main category: quant-ph

TL;DR: 本文首次提出了针对全量子神经网络（QNN）的特洛伊木马攻击（QuPTs），该攻击利用量子特性实现隐蔽性，并能显著降低QNN性能，最高导致23%的准确率下降。


<details>
  <summary>Details</summary>
Motivation: 量子神经网络（QNN）在量子机器学习（QML）领域具有巨大潜力，然而，其安全性和鲁棒性尚未得到充分探索。

Method: 提出了基于量子计算特性的新型特洛伊木马攻击——量子特性特洛伊木马（QuPTs），针对基于QNN的二分类器。QuPTs利用量子门的酉性来插入噪声，并通过Hadamard门实现叠加态以开发特洛伊木马并攻击QNN。

Result: 实验表明，所提出的QuPTs具有显著的隐蔽性，并能严重影响量子电路（特别是QNN）的性能。其中，影响最大的QuPT在实验设置下导致受损QNN的准确率下降了23%。

Conclusion: 本工作首次展示了针对完全量子神经网络（独立于任何混合经典-量子架构）的特洛伊木马攻击，揭示了QNN在安全性方面的潜在漏洞。

Abstract: Quantum neural networks (QNN) hold immense potential for the future of
quantum machine learning (QML). However, QNN security and robustness remain
largely unexplored. In this work, we proposed novel Trojan attacks based on the
quantum computing properties in a QNN-based binary classifier. Our proposed
Quantum Properties Trojans (QuPTs) are based on the unitary property of quantum
gates to insert noise and Hadamard gates to enable superposition to develop
Trojans and attack QNNs. We showed that the proposed QuPTs are significantly
stealthier and heavily impact the quantum circuits' performance, specifically
QNNs. The most impactful QuPT caused a deterioration of 23% accuracy of the
compromised QNN under the experimental setup. To the best of our knowledge,
this is the first work on the Trojan attack on a fully quantum neural network
independent of any hybrid classical-quantum architecture.

</details>


### [222] [Parametrized Quantum Circuit Learning for Quantum Chemical Applications](https://arxiv.org/abs/2507.08183)
*Grier M. Jones,Viki Kumar Prasad,Ulrich Fekl,Hans-Arno Jacobsen*

Main category: quant-ph

TL;DR: 研究了参数化量子电路（PQCs）在量子化学数据集上的性能，发现其在经典机器学习易于解决的问题上仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有量子机器学习（QML）研究中，参数化量子电路（PQCs）在量子化学相关数据集上的应用探索有限。

Method: 本研究在BSE49键分离能数据集和水构象CCSD波函数预测数据集上评估PQCs。通过组合14种数据编码策略和12种变分ansatz，构建了168种PQCs，并在5和16量子比特电路上进行测试。首先使用态矢量模拟分析电路结构影响，随后探讨电路深度和训练集大小对性能的影响，最终在噪声模拟及真实量子硬件上评估最佳PQCs。

Result: 研究发现，将PQCs应用于对经典机器学习方法而言简单但对量子方法而言非平凡的化学相关问题时，面临显著挑战。

Conclusion: 尽管PQCs在量子机器学习领域有前景，但对于某些化学相关问题，其应用仍存在挑战，特别是当这些问题对于经典机器学习方法而言相对容易时。

Abstract: In the field of quantum machine learning (QML), parametrized quantum circuits
(PQCs) -- constructed using a combination of fixed and tunable quantum gates --
provide a promising hybrid framework for tackling complex machine learning
problems. Despite numerous proposed applications, there remains limited
exploration of datasets relevant to quantum chemistry. In this study, we
investigate the potential benefits and limitations of PQCs on two chemically
meaningful datasets: (1) the BSE49 dataset, containing bond separation energies
for 49 different classes of chemical bonds, and (2) a dataset of water
conformations, where coupled-cluster singles and doubles (CCSD) wavefunctions
are predicted from lower-level electronic structure methods using the
data-driven coupled-cluster (DDCC) approach. We construct a comprehensive set
of 168 PQCs by combining 14 data encoding strategies with 12 variational
ans{\"a}tze, and evaluate their performance on circuits with 5 and 16 qubits.
Our initial analysis examines the impact of circuit structure on model
performance using state-vector simulations. We then explore how circuit depth
and training set size influence model performance. Finally, we assess the
performance of the best-performing PQCs on current quantum hardware, using both
noisy simulations ("fake" backends) and real quantum devices. Our findings
underscore the challenges of applying PQCs to chemically relevant problems that
are straightforward for classical machine learning methods but remain
non-trivial for quantum approaches.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [223] [VideoConviction: A Multimodal Benchmark for Human Conviction and Stock Market Recommendations](https://arxiv.org/abs/2507.08104)
*Michael Galarnyk,Veer Kejriwal,Agam Shah,Yash Bhardwaj,Nicholas Meyer,Anand Krishnan,Sudheer Chava*

Main category: cs.MM

TL;DR: 该研究分析了金融影响者视频推荐的影响，创建了多模态数据集VideoConviction以基准测试LLMs和MLLMs。发现多模态输入有助于股票代码提取，但模型在识别投资意图和信念上仍有不足。金融影响者的推荐表现不如S&P 500，但逆向投资策略可获得更高回报，伴随更高风险。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的金融影响者（finfluencers）通过视频分享股票推荐，其影响力巨大。然而，理解其影响需要分析音调、表达方式、面部表情等多模态信号，而非仅依赖文本。现有研究难以全面捕捉并量化这些超越文本的复杂多模态信息，因此需要新的方法和数据来深入分析。

Method: 引入并构建了VideoConviction多模态数据集，该数据集包含6000多个专家标注，耗时457小时人工努力。利用该数据集，研究对多模态大语言模型（MLLMs）和文本大语言模型（LLMs）在金融语境下的表现进行了基准测试，并比较了模型在处理完整视频和分段视频输入时的性能。

Result: ['多模态输入能够提升股票代码的提取准确性。', '无论是MLLMs还是LLMs，在区分投资行为和信念强度（通过自信表达和详细推理传达的信念强度）方面都存在困难，常将一般性评论误判为明确的推荐。', '即使是高信念度的推荐，其投资表现仍低于S&P 500指数基金。', '采取消极策略（即与金融影响者的推荐方向相反进行投资）在年化回报上可超越S&P 500达6.8%，但其风险也相应更高（夏普比率0.41 vs 0.65）。']

Conclusion: VideoConviction数据集为多模态金融研究提供了重要的基准，能够促进多模态模型的评估和发展。尽管多模态输入有助于特定任务，但在理解金融语境中的复杂投资意图和信念强度方面，现有模型仍面临挑战。金融影响者的推荐整体表现不佳，但逆向策略可能带来潜在的超额收益，尽管伴随更高的风险。该研究为多模态金融分析的深入发展奠定了基础。

Abstract: Social media has amplified the reach of financial influencers known as
"finfluencers," who share stock recommendations on platforms like YouTube.
Understanding their influence requires analyzing multimodal signals like tone,
delivery style, and facial expressions, which extend beyond text-based
financial analysis. We introduce VideoConviction, a multimodal dataset with
6,000+ expert annotations, produced through 457 hours of human effort, to
benchmark multimodal large language models (MLLMs) and text-based large
language models (LLMs) in financial discourse. Our results show that while
multimodal inputs improve stock ticker extraction (e.g., extracting Apple's
ticker AAPL), both MLLMs and LLMs struggle to distinguish investment actions
and conviction--the strength of belief conveyed through confident delivery and
detailed reasoning--often misclassifying general commentary as definitive
recommendations. While high-conviction recommendations perform better than
low-conviction ones, they still underperform the popular S\&P 500 index fund.
An inverse strategy--betting against finfluencer recommendations--outperforms
the S\&P 500 by 6.8\% in annual returns but carries greater risk (Sharpe ratio
of 0.41 vs. 0.65). Our benchmark enables a diverse evaluation of multimodal
tasks, comparing model performance on both full video and segmented video
inputs. This enables deeper advancements in multimodal financial research. Our
code, dataset, and evaluation leaderboard are available under the CC BY-NC 4.0
license.

</details>


### [224] [PUMA: Layer-Pruned Language Model for Efficient Unified Multimodal Retrieval with Modality-Adaptive Learning](https://arxiv.org/abs/2507.08064)
*Yibo Lyu,Rui Shao,Gongwei Chen,Yijie Zhu,Weili Guan,Liqiang Nie*

Main category: cs.MM

TL;DR: PUMA通过结合层剪枝自蒸馏和模态自适应对比学习，显著降低了多模态大语言模型在统一多模态检索任务中的资源消耗，同时保持了高性能。


<details>
  <summary>Details</summary>
Motivation: 随着多媒体内容的增长，对统一多模态检索（UMR）的需求日益增加。尽管多模态大语言模型（MLLMs）已被用于此任务，但其庞大的参数量导致训练成本高昂和推理效率低下。

Method: 该研究从结构和学习两方面改进UMR：
1.  **结构层面**：提出“层剪枝自蒸馏”方法，通过仅保留MLLMs的浅层，同时从被剪枝的深层中蒸馏特征作为教师信号，从而减少参数量并保留表示能力。
2.  **学习层面**：引入“模态自适应对比学习损失（MAC-Loss）”，根据目标模态将批内负样本分为更难的模态内组和更容易的模态间组，并分配不同的温度策略以提高学习效率。

Result: 实验结果表明，所提出的方法显著减少了资源使用（包括训练成本和推理效率），同时仍能保持强大的性能。

Conclusion: PUMA通过对MLLMs进行结构和学习优化，为高效的统一多模态检索提供了一个可行且性能优越的解决方案，有效解决了大型模型带来的资源消耗问题。

Abstract: As multimedia content expands, the demand for unified multimodal retrieval
(UMR) in real-world applications increases. Recent work leverages multimodal
large language models (MLLMs) to tackle this task. However, their large
parameter size results in high training costs and low inference efficiency. To
address this, we propose PUMA: a Layer-Pruned Language Model for Efficient
Unified Multimodal Retrieval with Modality-Adaptive Learning. Our approach
improves UMR from both structural and learning perspectives. (1) Structurally,
we propose Layer-Pruned Self-Distillation, which prunes MLLMs by keeping only
shallow layers while distilling features from dropped deep layers as teacher
signals. This reduces parameters and preserves representation capability. (2)
On the learning side, we introduce Modality-Adaptive Contrastive Learning Loss
(MAC-Loss), which separates in-batch negatives into harder intra-modality and
easier inter-modality groups based on the target modality, assigning different
temperature strategies to enhance learning efficiency. Experiments show our
method significantly reduces resource usage while maintaining strong
performance.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [225] [Overview of the TREC 2021 deep learning track](https://arxiv.org/abs/2507.08191)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Daniel Campos,Jimmy Lin*

Main category: cs.IR

TL;DR: TREC深度学习赛道第三年报告，基于MS MARCO数据集，扩大了文档和段落集合规模。深度神经网络模型持续超越传统方法，单阶段检索表现良好但仍逊于多阶段。新集合也引发了对评估标注质量的讨论。


<details>
  <summary>Details</summary>
Motivation: 评估和推进深度学习模型在文档和段落排序任务中的性能，尤其是在更大规模数据集下的表现。

Method: 利用MS MARCO数据集，并大幅刷新和扩增了文档和段落集合的规模。评估了包括深度神经网络在内的多种检索模型，对比了单阶段与多阶段检索管道的性能。

Result: 1. 深度神经网络排序模型持续优于传统检索方法。
2. 单阶段检索在两项任务上表现良好，但仍未能达到多阶段检索管道的性能水平。
3. 集合规模的扩大和数据更新引发了对NIST判断完整性和训练标签质量的疑问。

Conclusion: 深度学习模型在信息检索领域持续表现出强大的性能，多阶段检索管道仍是当前最优解。大规模数据集的引入也揭示了在评估标注质量方面的新挑战，这需要进一步关注和解决。

Abstract: This is the third year of the TREC Deep Learning track. As in previous years,
we leverage the MS MARCO datasets that made hundreds of thousands of human
annotated training labels available for both passage and document ranking
tasks. In addition, this year we refreshed both the document and the passage
collections which also led to a nearly four times increase in the document
collection size and nearly $16$ times increase in the size of the passage
collection. Deep neural ranking models that employ large scale pretraininig
continued to outperform traditional retrieval methods this year. We also found
that single stage retrieval can achieve good performance on both tasks although
they still do not perform at par with multistage retrieval pipelines. Finally,
the increase in the collection size and the general data refresh raised some
questions about completeness of NIST judgments and the quality of the training
labels that were mapped to the new collections from the old ones which we
discuss in this report.

</details>


### [226] [CUE-RAG: Towards Accurate and Cost-Efficient Graph-Based RAG via Multi-Partite Graph and Query-Driven Iterative Retrieval](https://arxiv.org/abs/2507.08445)
*Yaodong Su,Yixiang Fang,Yingli Zhou,Quanqing Xu,Chuanhui Yang*

Main category: cs.IR

TL;DR: CUE-RAG是一种新型图增强检索生成(RAG)方法，通过引入多方图索引、混合提取策略和查询驱动的迭代检索，解决了现有图基RAG在知识提取和查询利用方面的局限性。它在问答任务中显著提升了性能并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型(LLM)取得了显著进展，但在问答(QA)中其性能仍受限于缺乏领域特定和最新知识。现有图基检索增强生成(RAG)方法因提取不完整和检索时查询信息利用不足，导致图质量不佳。

Method: 本文提出CUE-RAG，包含三部分：(1)多方图索引，整合文本块、知识单元和实体，以捕捉多粒度语义内容；(2)混合提取策略，减少LLM代币使用并生成准确、消歧的知识单元；(3)Q-Iter，一种查询驱动的迭代检索策略，通过语义搜索和受限图遍历增强相关性。

Result: 在三个QA基准测试中，CUE-RAG显著优于现有基线，准确率提升高达99.33%，F1分数提升113.51%，同时索引成本降低72.58%。即使不使用LLM进行索引，CUE-RAG也能匹敌或超越基线。

Conclusion: CUE-RAG展示了其在推进图基RAG系统方面的有效性和成本效益。

Abstract: Despite the remarkable progress of Large Language Models (LLMs), their
performance in question answering (QA) remains limited by the lack of
domain-specific and up-to-date knowledge. Retrieval-Augmented Generation (RAG)
addresses this limitation by incorporating external information, often from
graph-structured data. However, existing graph-based RAG methods suffer from
poor graph quality due to incomplete extraction and insufficient utilization of
query information during retrieval. To overcome these limitations, we propose
CUE-RAG, a novel approach that introduces (1) a multi-partite graph index
incorporates text Chunks, knowledge Units, and Entities to capture semantic
content at multiple levels of granularity, (2) a hybrid extraction strategy
that reduces LLM token usage while still producing accurate and disambiguated
knowledge units, and (3) Q-Iter, a query-driven iterative retrieval strategy
that enhances relevance through semantic search and constrained graph
traversal. Experiments on three QA benchmarks show that CUE-RAG significantly
outperforms state-of-the-art baselines, achieving up to 99.33% higher Accuracy
and 113.51% higher F1 score while reducing indexing costs by 72.58%.
Remarkably, CUE-RAG matches or outperforms baselines even without using an LLM
for indexing. These results demonstrate the effectiveness and cost-efficiency
of CUE-RAG in advancing graph-based RAG systems.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [227] [Unraveling the Potential of Diffusion Models in Small Molecule Generation](https://arxiv.org/abs/2507.08005)
*Peining Zhang,Daniel Baker,Minghu Song,Jinbo Bi*

Main category: q-bio.BM

TL;DR: 本文全面综述了扩散模型（DMs）在分子生成领域的最新进展和应用，涵盖其理论原理、分类、性能评估（特别是3D方法），并指出当前挑战和未来研究方向，以期充分发挥其在药物发现中的潜力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI为药物设计提供了新思路并促进了化学空间探索，其中扩散模型（DMs）作为新兴工具在药物研发中备受关注。因此，有必要对DMs在分子生成中的最新进展和应用进行全面综述。

Method: 本文采用综述方法，首先介绍扩散模型（DMs）的理论原理，随后根据数学和化学应用对基于DMs的分子生成方法进行分类。接着，考察这些模型在基准数据集上的性能，并重点比较现有3D方法的生成性能。

Result: 本文综述并分类了多种基于扩散模型的分子生成方法，考察了它们在基准数据集上的性能，并着重比较了现有3D方法的生成性能。

Conclusion: 文章最后强调了扩散模型在药物发现中面临的当前挑战，并提出了未来的研究方向，以期充分发挥其潜力。

Abstract: Generative AI presents chemists with novel ideas for drug design and
facilitates the exploration of vast chemical spaces. Diffusion models (DMs), an
emerging tool, have recently attracted great attention in drug R\&D. This paper
comprehensively reviews the latest advancements and applications of DMs in
molecular generation. It begins by introducing the theoretical principles of
DMs. Subsequently, it categorizes various DM-based molecular generation methods
according to their mathematical and chemical applications. The review further
examines the performance of these models on benchmark datasets, with a
particular focus on comparing the generation performance of existing 3D
methods. Finally, it concludes by emphasizing current challenges and suggesting
future research directions to fully exploit the potential of DMs in drug
discovery.

</details>


### [228] [AmpLyze: A Deep Learning Model for Predicting the Hemolytic Concentration](https://arxiv.org/abs/2507.08162)
*Peng Qiu,Hanqi Feng,Barnabas Poczos*

Main category: q-bio.BM

TL;DR: AmpLyze是一种新型模型，能通过抗菌肽序列定量预测其溶血毒性（HC50值），并解释导致毒性的残基。该模型性能优于现有方法，为抗菌肽设计和早期毒性筛选提供实用工具。


<details>
  <summary>Details</summary>
Motivation: 现有抗菌肽（AMP）毒性评估模型只能提供“有毒”或“无毒”的二元分类，无法预测实际的定量HC50值，这阻碍了AMP的安全性评估。此外，缺乏对毒性驱动残基的解释能力。

Method: AmpLyze模型结合了残基级别的ProtT5/ESM2嵌入与序列级别的描述符，采用双分支（局部和全局）架构。这两个分支通过交叉注意力模块对齐，并使用log-cosh损失函数进行训练，以提高对实验噪声的鲁棒性。该模型仅需抗菌肽序列作为输入。

Result: 最优的AmpLyze模型在HC50预测上达到了0.756的PCC和0.987的MSE，性能超越了传统回归器和现有最先进的模型。消融实验证实，双分支和交叉注意力模块对模型性能至关重要（交叉注意力额外提升1% PCC和3% MSE）。Expected-Gradients归因方法揭示了已知的毒性热点，并提供了更安全的替换建议。

Conclusion: AmpLyze成功将溶血评估转化为一种定量、基于序列且可解释的预测方法，极大地促进了抗菌肽的设计过程，并提供了一个实用的早期毒性筛选工具。

Abstract: Red-blood-cell lysis (HC50) is the principal safety barrier for
antimicrobial-peptide (AMP) therapeutics, yet existing models only say "toxic"
or "non-toxic." AmpLyze closes this gap by predicting the actual HC50 value
from sequence alone and explaining the residues that drive toxicity. The model
couples residue-level ProtT5/ESM2 embeddings with sequence-level descriptors in
dual local and global branches, aligned by a cross-attention module and trained
with log-cosh loss for robustness to assay noise. The optimal AmpLyze model
reaches a PCC of 0.756 and an MSE of 0.987, outperforming classical regressors
and the state-of-the-art. Ablations confirm that both branches are essential,
and cross-attention adds a further 1% PCC and 3% MSE improvement.
Expected-Gradients attributions reveal known toxicity hotspots and suggest
safer substitutions. By turning hemolysis assessment into a quantitative,
sequence-based, and interpretable prediction, AmpLyze facilitates AMP design
and offers a practical tool for early-stage toxicity screening.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [229] [CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations](https://arxiv.org/abs/2507.08262)
*Wenbo Cui,Chengyang Zhao,Yuhui Chen,Haoran Li,Zhizheng Zhang,Dongbin Zhao,He Wang*

Main category: cs.RO

TL;DR: CL3R是一个新型3D预训练框架，通过结合点云MAE和2D基础模型的对比学习，解决了机器人视觉运动策略中2D模型在3D空间理解和多视角泛化上的不足，显著提升了机器人操作的感知能力。


<details>
  <summary>Details</summary>
Motivation: 机器人视觉运动策略学习中的感知模块至关重要。现有方法虽利用2D基础模型增强语义理解，但在捕获3D空间信息和跨多样相机视角泛化方面存在不足，这尤其限制了精细机器人操作策略的有效性。

Method: 提出CL3R框架。该方法通过以下方式整合空间感知和语义理解：1. 使用点云掩码自编码器（Point Cloud Masked Autoencoder）学习丰富的3D表示。2. 通过对比学习（Contrastive Learning）利用预训练的2D基础模型进行高效的语义知识迁移。3. 统一数据集间的坐标系并随机融合多视角点云，以减轻相机视角模糊并提高泛化能力。

Result: 在仿真和真实世界的广泛实验表明，CL3R方法展现出卓越的性能，有效提升了机器人抓取与操作的视觉运动策略学习能力。

Conclusion: CL3R框架通过有效的3D预训练，成功解决了现有方法在机器人感知中3D空间理解和多视角泛化方面的局限性，显著增强了机器人操作的感知鲁棒性和有效性。

Abstract: Building a robust perception module is crucial for visuomotor policy
learning. While recent methods incorporate pre-trained 2D foundation models
into robotic perception modules to leverage their strong semantic
understanding, they struggle to capture 3D spatial information and generalize
across diverse camera viewpoints. These limitations hinder the policy's
effectiveness, especially in fine-grained robotic manipulation scenarios. To
address these challenges, we propose CL3R, a novel 3D pre-training framework
designed to enhance robotic manipulation policies. Our method integrates both
spatial awareness and semantic understanding by employing a point cloud Masked
Autoencoder to learn rich 3D representations while leveraging pre-trained 2D
foundation models through contrastive learning for efficient semantic knowledge
transfer. Additionally, we propose a 3D visual representation pre-training
framework for robotic tasks. By unifying coordinate systems across datasets and
introducing random fusion of multi-view point clouds, we mitigate camera view
ambiguity and improve generalization, enabling robust perception from novel
viewpoints at test time. Extensive experiments in both simulation and the real
world demonstrate the superiority of our method, highlighting its effectiveness
in visuomotor policy learning for robotic manipulation.

</details>


### [230] [Intelligent Control of Spacecraft Reaction Wheel Attitude Using Deep Reinforcement Learning](https://arxiv.org/abs/2507.08366)
*Ghaith El-Dalahmeh,Mohammad Reza Jabbarpour,Bao Quoc Vo,Ryszard Kowalczyk*

Main category: cs.RO

TL;DR: 针对反作用轮故障，提出一种基于TD3-HD深度强化学习的卫星姿态控制策略，显著提升了故障条件下的控制精度与稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着卫星自主性日益增强，动态不确定环境下可靠的姿态控制至关重要。反作用轮故障对任务和系统稳定构成威胁，而传统PD控制器和现有深度强化学习算法在实时适应性和容错性方面表现不足。

Method: 本研究提出TD3-HD深度强化学习控制策略，该方法结合了TD3（Twin Delayed Deep Deterministic Policy Gradient）、Hindsight Experience Replay (HER) 和 Dimension Wise Clipping (DWC)，旨在稀疏奖励环境中增强学习效果，并在反作用轮故障期间保持卫星稳定性。该方法与PD控制和主流DRL算法进行了性能对比。

Result: 实验结果表明，TD3-HD在故障条件下显著降低了姿态误差，改善了角速度调节，并增强了系统稳定性。

Conclusion: 研究结果表明，所提出的TD3-HD方法作为一种强大、容错的机载AI解决方案，在自主卫星姿态控制方面具有巨大潜力。

Abstract: Reliable satellite attitude control is essential for the success of space
missions, particularly as satellites increasingly operate autonomously in
dynamic and uncertain environments. Reaction wheels (RWs) play a pivotal role
in attitude control, and maintaining control resilience during RW faults is
critical to preserving mission objectives and system stability. However,
traditional Proportional Derivative (PD) controllers and existing deep
reinforcement learning (DRL) algorithms such as TD3, PPO, and A2C often fall
short in providing the real time adaptability and fault tolerance required for
autonomous satellite operations. This study introduces a DRL-based control
strategy designed to improve satellite resilience and adaptability under fault
conditions. Specifically, the proposed method integrates Twin Delayed Deep
Deterministic Policy Gradient (TD3) with Hindsight Experience Replay (HER) and
Dimension Wise Clipping (DWC) referred to as TD3-HD to enhance learning in
sparse reward environments and maintain satellite stability during RW failures.
The proposed approach is benchmarked against PD control and leading DRL
algorithms. Experimental results show that TD3-HD achieves significantly lower
attitude error, improved angular velocity regulation, and enhanced stability
under fault conditions. These findings underscore the proposed method potential
as a powerful, fault tolerant, onboard AI solution for autonomous satellite
attitude control.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [231] [Energy Management for Renewable-Colocated Artificial Intelligence Data Centers](https://arxiv.org/abs/2507.08011)
*Siying Li,Lang Tong,Timothy D. Mount*

Main category: math.OC

TL;DR: 开发了针对协同可再生能源的AI数据中心的能源管理系统(EMS)，旨在最大化运营利润。


<details>
  <summary>Details</summary>
Motivation: 为最大化协同可再生能源的AI数据中心的经济效益，通过优化能源利用和电力市场参与。

Method: 提出一个以利润最大化为目标的EMS，协同优化AI工作负载调度、现场可再生能源利用及批发和零售电力市场参与。

Result: 基于真实世界数据的实证评估显示，可再生能源与AI数据中心的协同配置能带来显著的利润增长。

Conclusion: 该EMS成功实现了协同可再生能源AI数据中心的利润最大化运营，证明了这种协同配置的经济优势。

Abstract: We develop an energy management system (EMS) for artificial intelligence (AI)
data centers with colocated renewable generation. Under a profit-maximizing
framework, the EMS of renewable-colocated data center (RCDC) co-optimizes AI
workload scheduling, on-site renewable utilization, and electricity market
participation. Within both wholesale and retail market participation models,
the economic benefit of the RCDC operation is maximized. Empirical evaluations
using real-world traces of electricity prices, data center power consumption,
and renewable generation demonstrate significant profit gains from renewable
and AI data center colocations.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [232] [Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models](https://arxiv.org/abs/2507.08128)
*Arushi Goel,Sreyan Ghosh,Jaehyeon Kim,Sonal Kumar,Zhifeng Kong,Sang-gil Lee,Chao-Han Huck Yang,Ramani Duraiswami,Dinesh Manocha,Rafael Valle,Bryan Catanzaro*

Main category: cs.SD

TL;DR: Audio Flamingo 3 (AF3) 是一个完全开源的最先进大型音频语言模型，通过引入统一编码器、灵活推理、多轮对话、长音频理解和语音交互等新功能，显著提升了语音、声音和音乐的理解与推理能力，并在20多个基准测试中超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 旨在推进大型音频语言模型在语音、声音和音乐领域中的推理和理解能力。

Method: 引入了AF-Whisper统一音频编码器，采用新策略进行语音、声音和音乐的联合表征学习；实现了灵活的按需思考（思维链推理）、多轮多音频聊天、长达10分钟的长音频理解和语音到语音交互。为实现这些功能，提出了AudioSkills-XL、LongAudio-XL、AF-Think和AF-Chat等大规模训练数据集，并采用新颖的五阶段课程式训练策略，所有训练均基于开源音频数据进行。

Result: 在超过20个（长）音频理解和推理基准测试中取得了新的最先进（SOTA）结果，超越了使用更大数据集训练的开源和闭源模型。

Conclusion: Audio Flamingo 3 (AF3) 是一个具有里程碑意义的开源大型音频语言模型，通过其创新的架构和训练策略，极大地提升了跨语音、声音和音乐的理解和推理能力，并在多个重要基准测试中树立了新的行业标杆。

Abstract: We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large
audio-language model that advances reasoning and understanding across speech,
sound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder
trained using a novel strategy for joint representation learning across all 3
modalities of speech, sound, and music; (ii) flexible, on-demand thinking,
allowing the model to do chain-of-thought-type reasoning before answering;
(iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning
(including speech) up to 10 minutes; and (v) voice-to-voice interaction. To
enable these capabilities, we propose several large-scale training datasets
curated using novel strategies, including AudioSkills-XL, LongAudio-XL,
AF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based
training strategy. Trained on only open-source audio data, AF3 achieves new
SOTA results on over 20+ (long) audio understanding and reasoning benchmarks,
surpassing both open-weight and closed-source models trained on much larger
datasets.

</details>


### [233] [On Barriers to Archival Audio Processing](https://arxiv.org/abs/2507.08768)
*Peter Sullivan,Muhammad Abdul-Mageed*

Main category: cs.SD

TL;DR: 本研究利用独特的历史电台录音，评估了现代语言识别（LID）和说话人识别（SR）方法的鲁棒性。结果显示LID系统表现良好，但SR的说话人嵌入仍存在信道、年龄和语言偏见。


<details>
  <summary>Details</summary>
Motivation: 探测现代现成语言识别（LID）和说话人识别（SR）方法在处理多语言说话人和跨年龄录音方面的鲁棒性，特别是利用联合国教科文组织（UNESCO）独特的20世纪中叶电台录音数据。

Method: 使用联合国教科文组织收集的20世纪中叶电台录音作为数据集，并应用现代现成的语言识别（如Whisper）和说话人识别方法进行测试和分析。

Result: 研究发现，语言识别系统（如Whisper）越来越善于处理第二语言和带口音的语音。然而，说话人嵌入仍然是语音处理流程中脆弱的组成部分，容易受到信道、年龄和语言相关的偏见影响。

Conclusion: 如果档案机构希望利用说话人识别方法进行说话人索引，则必须解决当前说话人识别系统中存在的信道、年龄和语言相关偏见问题。

Abstract: In this study, we leverage a unique UNESCO collection of mid-20th century
radio recordings to probe the robustness of modern off-the-shelf language
identification (LID) and speaker recognition (SR) methods, especially with
respect to the impact of multilingual speakers and cross-age recordings. Our
findings suggest that LID systems, such as Whisper, are increasingly adept at
handling second-language and accented speech. However, speaker embeddings
remain a fragile component of speech processing pipelines that is prone to
biases related to the channel, age, and language. Issues which will need to be
overcome should archives aim to employ SR methods for speaker indexing.

</details>


### [234] [Audio Inpanting using Discrete Diffusion Model](https://arxiv.org/abs/2507.08333)
*Tali Dror,Iftach Shoham,Moshe Buchris,Oren Gal,Haim Permuter,Gilad Katz,Eliya Nachmani*

Main category: cs.SD

TL;DR: 本文提出一种基于离散扩散模型的新型音频修复方法，通过对标记化音频表示进行操作，实现了对长达500毫秒的缺失音频片段的稳定和语义连贯重建，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的音频修复方法（如基于波形和频谱图的扩散模型）在处理短间隙时表现良好，但当间隙超过100毫秒时，其质量会显著下降。

Method: 引入一种基于离散扩散建模的新型音频修复方法。该方法在由预训练音频分词器生成的标记化音频表示上操作，直接在离散潜在空间中建模生成过程。

Result: 在MusicNet数据集上对长达300毫秒的间隙进行了评估，并在MTG数据集上将间隙时长扩展到500毫秒，使用客观和感知指标进行衡量。实验结果表明，该方法与现有基线相比，实现了具有竞争力或更优的性能，尤其是在处理较长间隙时。

Conclusion: 所提出的离散扩散方法为恢复受损的音乐录音提供了一个鲁棒的解决方案，能够有效地重建缺失的音频，尤其擅长处理较长间隙，解决了现有方法的局限性。

Abstract: Audio inpainting refers to the task of reconstructing missing segments in
corrupted audio recordings. While prior approaches-including waveform and
spectrogram-based diffusion models-have shown promising results for short gaps,
they often degrade in quality when gaps exceed 100 milliseconds (ms). In this
work, we introduce a novel inpainting method based on discrete diffusion
modeling, which operates over tokenized audio representations produced by a
pre-trained audio tokenizer. Our approach models the generative process
directly in the discrete latent space, enabling stable and semantically
coherent reconstruction of missing audio. We evaluate the method on the
MusicNet dataset using both objective and perceptual metrics across gap
durations up to 300 ms. We further evaluated our approach on the MTG dataset,
extending the gap duration to 500 ms. Experimental results demonstrate that our
method achieves competitive or superior performance compared to existing
baselines, particularly for longer gaps, offering a robust solution for
restoring degraded musical recordings. Audio examples of our proposed method
can be found at https://iftach21.github.io/

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [235] [Mallows Model with Learned Distance Metrics: Sampling and Maximum Likelihood Estimation](https://arxiv.org/abs/2507.08108)
*Yeganeh Alimohammadi,Kiana Asgari*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: \textit{Mallows model} is a widely-used probabilistic framework for learning
from ranking data, with applications ranging from recommendation systems and
voting to aligning language models with human
preferences~\cite{chen2024mallows, kleinberg2021algorithmic,
rafailov2024direct}. Under this model, observed rankings are noisy
perturbations of a central ranking $\sigma$, with likelihood decaying
exponentially in distance from $\sigma$, i.e, $P (\pi) \propto \exp\big(-\beta
\cdot d(\pi, \sigma)\big),$ where $\beta > 0$ controls dispersion and $d$ is a
distance function.
  Existing methods mainly focus on fixed distances (such as Kendall's $\tau$
distance), with no principled approach to learning the distance metric directly
from data. In practice, however, rankings naturally vary by context; for
instance, in some sports we regularly see long-range swaps (a low-rank team
beating a high-rank one), while in others such events are rare. Motivated by
this, we propose a generalization of Mallows model that learns the distance
metric directly from data. Specifically, we focus on $L_\alpha$ distances:
$d_\alpha(\pi,\sigma):=\sum_{i=1} |\pi(i)-\sigma(i)|^\alpha$.
  For any $\alpha\geq 1$ and $\beta>0$, we develop a Fully Polynomial-Time
Approximation Scheme (FPTAS) to efficiently generate samples that are
$\epsilon$- close (in total variation distance) to the true distribution. Even
in the special cases of $L_1$ and $L_2$, this generalizes prior results that
required vanishing dispersion ($\beta\to0$). Using this sampling algorithm, we
propose an efficient Maximum Likelihood Estimation (MLE) algorithm that jointly
estimates the central ranking, the dispersion parameter, and the optimal
distance metric. We prove strong consistency results for our estimators (for
any values of $\alpha$ and $\beta$), and we validate our approach empirically
using datasets from sports rankings.

</details>


### [236] [CLEAR: Calibrated Learning for Epistemic and Aleatoric Risk](https://arxiv.org/abs/2507.08150)
*Ilia Azizi,Juraj Bodik,Jakob Heiss,Bin Yu*

Main category: stat.ML

TL;DR: 提出CLEAR方法，通过结合偶然性和认知性不确定性，在回归任务中实现更窄且校准良好的预测区间。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独处理偶然性不确定性（测量噪声）或认知性不确定性（数据有限），未能以平衡方式同时考虑两者，影响了预测模型的可靠性。

Method: 提出CLEAR，一种使用两个独特参数($\gamma_1$和$\gamma_2$)的校准方法，旨在结合偶然性与认知性不确定性分量以改善条件覆盖率。CLEAR兼容任何偶然性（如分位数回归）和认知性（如PCS框架下的集成方法）估计器。

Result: 在17个真实世界数据集中，CLEAR在保持名义覆盖率的同时，预测区间宽度比单独校准的基线平均提高了28.2%和17.4%。在高认知性或高偶然性不确定性场景下，这种改善尤为显著。

Conclusion: CLEAR通过有效结合偶然性和认知不确定性，显著提升了回归预测中不确定性量化的精度和可靠性，特别适用于面临高度不确定性的场景。

Abstract: Accurate uncertainty quantification is critical for reliable predictive
modeling, especially in regression tasks. Existing methods typically address
either aleatoric uncertainty from measurement noise or epistemic uncertainty
from limited data, but not necessarily both in a balanced way. We propose
CLEAR, a calibration method with two distinct parameters, $\gamma_1$ and
$\gamma_2$, to combine the two uncertainty components for improved conditional
coverage. CLEAR is compatible with any pair of aleatoric and epistemic
estimators; we show how it can be used with (i) quantile regression for
aleatoric uncertainty and (ii) ensembles drawn from the
Predictability-Computability-Stability (PCS) framework for epistemic
uncertainty. Across 17 diverse real-world datasets, CLEAR achieves an average
improvement of 28.2% and 17.4% in the interval width compared to the two
individually calibrated baselines while maintaining nominal coverage. This
improvement can be particularly evident in scenarios dominated by either high
epistemic or high aleatoric uncertainty.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [237] [Invariant-based Robust Weights Watermark for Large Language Models](https://arxiv.org/abs/2507.08288)
*Qingxiao Guo,Xinjie Zhu,Yilong Ma,Hui Jin,Yunhao Wang,Weifeng Zhang,Xiaobing Guo*

Main category: cs.CR

TL;DR: 本文提出一种针对Transformer模型的鲁棒水印方案，无需重训练，通过用户密钥、模型不变量和噪声机制实现，并在多种攻击下表现出强大鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 鉴于知识产权日益重要，特别是大型语言模型（LLMs）在资源受限边缘设备上的广泛部署，存在恶意用户窃取知识产权的潜在威胁，因此需要有效的保护技术。

Method: 提出一种无需重训练或微调的Transformer模型鲁棒水印方案。该方案为每个用户生成唯一密钥，通过求解模型不变量构建的线性约束来导出稳定的水印值，并利用噪声机制在多用户场景下隐藏水印位置，以抵抗共谋攻击。

Result: 在Llama3、Phi3、Gemma等流行模型上进行评估，实验结果证实该方法在微调、剪枝、量化、置换、缩放、可逆矩阵和共谋攻击等多种攻击方法下均具有强大的鲁棒性。

Conclusion: 本文提出的水印方案有效解决了LLM知识产权保护问题，尤其适用于边缘设备部署，并在多种复杂攻击环境下展现出优异的抗攻击能力和实用性。

Abstract: Watermarking technology has gained significant attention due to the
increasing importance of intellectual property (IP) rights, particularly with
the growing deployment of large language models (LLMs) on billions
resource-constrained edge devices. To counter the potential threats of IP theft
by malicious users, this paper introduces a robust watermarking scheme without
retraining or fine-tuning for transformer models. The scheme generates a unique
key for each user and derives a stable watermark value by solving linear
constraints constructed from model invariants. Moreover, this technology
utilizes noise mechanism to hide watermark locations in multi-user scenarios
against collusion attack. This paper evaluates the approach on three popular
models (Llama3, Phi3, Gemma), and the experimental results confirm the strong
robustness across a range of attack methods (fine-tuning, pruning,
quantization, permutation, scaling, reversible matrix and collusion attacks).

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [238] [Predicting Flow Dynamics using Diffusion Models](https://arxiv.org/abs/2507.08106)
*Yannick Gachnang,Vismay Churiwala*

Main category: physics.flu-dyn

TL;DR: 本文旨在复现和扩展DiffFluid模型，验证扩散模型结合Transformer在流体动力学预测中的通用性和潜力，并尝试将其应用于Lattice Boltzmann方法，同时指出其挑战。


<details>
  <summary>Details</summary>
Motivation: 复制DiffFluid模型的成果，验证其方法的再现性，并探索其在其他流体模拟类型（特别是Lattice Boltzmann方法）上的可行性。

Method: 基于去噪扩散概率模型（DDPM）框架，结合Transformer预测流体动力学，并尝试将其应用于Lattice Boltzmann方法进行验证。

Result: 尽管存在计算和时间限制，本研究提供了模型作为通用流体动力学求解器具有灵活性和潜力的证据。研究结果同时揭示了扩散模型应用于复杂流体动力学问题的潜力与挑战。

Conclusion: 该模型作为流体动力学通用求解器具有潜力，但也存在挑战。未来的研究应聚焦于优化计算效率和扩大模型在更广泛领域的应用。

Abstract: In this work, we aimed to replicate and extend the results presented in the
DiffFluid paper[1]. The DiffFluid model showed that diffusion models combined
with Transformers are capable of predicting fluid dynamics. It uses a denoising
diffusion probabilistic model (DDPM) framework to tackle Navier-Stokes and
Darcy flow equations. Our goal was to validate the reproducibility of the
methods in the DiffFluid paper while testing its viability for other simulation
types, particularly the Lattice Boltzmann method. Despite our computational
limitations and time constraints, this work provides evidence of the
flexibility and potential of the model as a general-purpose solver for fluid
dynamics. Our results show both the potential and challenges of applying
diffusion models to complex fluid dynamics problems. This work highlights the
opportunities for future research in optimizing the computational efficiency
and scaling such models in broader domains.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [239] [xpSHACL: Explainable SHACL Validation using Retrieval-Augmented Generation and Large Language Models](https://arxiv.org/abs/2507.08432)
*Gustavo Correa Publio,José Emilio Labra Gayo*

Main category: cs.DB

TL;DR: xpSHACL系统结合规则、RAG和LLM，生成多语言、易于理解的SHACL验证报告，并利用违规知识图谱提升效率。


<details>
  <summary>Details</summary>
Motivation: 传统SHACL验证引擎生成的报告过于简洁且仅限英文，非技术用户难以理解和操作，阻碍了知识图谱的有效验证。

Method: xpSHACL系统结合了基于规则的理由树、检索增强生成（RAG）和大型语言模型（LLM），以生成详细、多语言、人类可读的约束违规解释。核心特点是利用违规知识图谱（Violation KG）缓存和重用解释。

Result: xpSHACL能够为SHACL约束违规提供详细、多语言且人类可读的解释。通过使用违规知识图谱，显著提高了解释生成的效率和一致性。

Conclusion: xpSHACL通过提供易于理解的多语言解释，成功解决了传统SHACL验证报告难以解读的问题，提升了非技术用户对知识图谱数据验证的可用性和效率。

Abstract: Shapes Constraint Language (SHACL) is a powerful language for validating RDF
data. Given the recent industry attention to Knowledge Graphs (KGs), more users
need to validate linked data properly. However, traditional SHACL validation
engines often provide terse reports in English that are difficult for
non-technical users to interpret and act upon. This paper presents xpSHACL, an
explainable SHACL validation system that addresses this issue by combining
rule-based justification trees with retrieval-augmented generation (RAG) and
large language models (LLMs) to produce detailed, multilanguage, human-readable
explanations for constraint violations. A key feature of xpSHACL is its usage
of a Violation KG to cache and reuse explanations, improving efficiency and
consistency.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [240] [Human vs. LLM-Based Thematic Analysis for Digital Mental Health Research: Proof-of-Concept Comparative Study](https://arxiv.org/abs/2507.08002)
*Karisa Parkington,Bazen G. Teferra,Marianne Rouleau-Tang,Argyrios Perivolaris,Alice Rueda,Adam Dubrowski,Bill Kapralos,Reza Samavi,Andrew Greenshaw,Yanbo Zhang,Bo Cao,Yuqi Wu,Sirisha Rambhatla,Sridhar Krishnan,Venkat Bhat*

Main category: cs.HC

TL;DR: 传统主题分析资源密集。本研究比较了LLM与人类的主题分析，发现LLM更具成本效益，编码饱和更快，但在归纳编码深度和主题合成方面不如人类。结论是LLM结合人工监督可有效应用于定性研究。


<details>
  <summary>Details</summary>
Motivation: 传统主题分析在大型医疗研究中因资源密集而受限。大语言模型（LLMs）有潜力解决规模化分析问题，但其在心理健康访谈中的应用效果亟待与人类分析进行比较。

Method: 本研究对比了开箱即用型LLM和基于知识库的LLM主题分析与传统人类分析方法。利用OpenAI的GPT-4o模型和RISEN提示框架，对医疗工作者压力缓解试验的访谈记录进行分析，并与Dedoose中的人类分析结果进行比较，评估代码、饱和点、摘录应用和主题合成。

Result: LLMs（RISEN框架）开发的演绎父代码与人类相似，但人类在归纳子代码开发和主题合成上表现更优。知识库LLM比开箱即用型LLM和人类更快达到编码饱和。开箱即用型LLM识别的摘录数量与人类相当，具有高信度（K=0.84），但人类摘录更长且常含多个代码，LLM通常只应用一个。LLM方法更具成本效益但缺乏深度。

Conclusion: LLM驱动的主题分析能够变革心理健康和临床研究中的定性分析，但需要结合人类监督，以平衡参与者视角和研究资源。

Abstract: Thematic analysis provides valuable insights into participants' experiences
through coding and theme development, but its resource-intensive nature limits
its use in large healthcare studies. Large language models (LLMs) can analyze
text at scale and identify key content automatically, potentially addressing
these challenges. However, their application in mental health interviews needs
comparison with traditional human analysis. This study evaluates out-of-the-box
and knowledge-base LLM-based thematic analysis against traditional methods
using transcripts from a stress-reduction trial with healthcare workers.
OpenAI's GPT-4o model was used along with the Role, Instructions, Steps,
End-Goal, Narrowing (RISEN) prompt engineering framework and compared to human
analysis in Dedoose. Each approach developed codes, noted saturation points,
applied codes to excerpts for a subset of participants (n = 20), and
synthesized data into themes. Outputs and performance metrics were compared
directly. LLMs using the RISEN framework developed deductive parent codes
similar to human codes, but humans excelled in inductive child code development
and theme synthesis. Knowledge-based LLMs reached coding saturation with fewer
transcripts (10-15) than the out-of-the-box model (15-20) and humans (90-99).
The out-of-the-box LLM identified a comparable number of excerpts to human
researchers, showing strong inter-rater reliability (K = 0.84), though the
knowledge-based LLM produced fewer excerpts. Human excerpts were longer and
involved multiple codes per excerpt, while LLMs typically applied one code.
Overall, LLM-based thematic analysis proved more cost-effective but lacked the
depth of human analysis. LLMs can transform qualitative analysis in mental
healthcare and clinical research when combined with human oversight to balance
participant perspectives and research resources.

</details>


### [241] [SSSUMO: Real-Time Semi-Supervised Submovement Decomposition](https://arxiv.org/abs/2507.08028)
*Evgenii Rudakov,Jonathan Shock,Otto Lappi,Benjamin Ultan Cowley*

Main category: cs.HC

TL;DR: 本文提出 SSSUMO，一种半监督深度学习方法，用于子运动分解，实现了最先进的准确性和实时性，解决了现有方法在数据标注、准确性与计算成本方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 子运动分析对运动控制研究有价值，但现有方法在重建准确性、计算成本和验证方面存在困难，主要是因为难以获取手动标注数据。

Method: 采用半监督学习框架，利用最小抖动原理生成合成数据，并通过迭代适应未标注的人体运动数据进行细化。模型采用带有可微分重建层的全卷积架构。

Result: 在合成和多样化人体运动数据集上，显著优于现有方法，即使在高噪声条件下也表现出鲁棒性。实现了实时操作（每输入秒小于1毫秒），远超优化基方法。在多种人体任务中表现有效，特别在传统方法失败的挑战性数据集上表现突出。

Conclusion: SSSUMO 增强的性能将促进人机交互、康复医学和运动控制研究领域的新应用，克服了现有技术的主要限制，提供了一个准确、实时且鲁棒的解决方案。

Abstract: This paper introduces a SSSUMO, semi-supervised deep learning approach for
submovement decomposition that achieves state-of-the-art accuracy and speed.
While submovement analysis offers valuable insights into motor control,
existing methods struggle with reconstruction accuracy, computational cost, and
validation, due to the difficulty of obtaining hand-labeled data. We address
these challenges using a semi-supervised learning framework. This framework
learns from synthetic data, initially generated from minimum-jerk principles
and then iteratively refined through adaptation to unlabeled human movement
data. Our fully convolutional architecture with differentiable reconstruction
significantly surpasses existing methods on both synthetic and diverse human
motion datasets, demonstrating robustness even in high-noise conditions.
Crucially, the model operates in real-time (less than a millisecond per input
second), a substantial improvement over optimization-based techniques. This
enhanced performance facilitates new applications in human-computer
interaction, rehabilitation medicine, and motor control studies. We demonstrate
the model's effectiveness across diverse human-performed tasks such as
steering, rotation, pointing, object moving, handwriting, and mouse-controlled
gaming, showing notable improvements particularly on challenging datasets where
traditional methods largely fail. Training and benchmarking source code, along
with pre-trained model weights, are made publicly available at
https://github.com/dolphin-in-a-coma/sssumo.

</details>


### [242] [A Versatile Dataset of Mouse and Eye Movements on Search Engine Results Pages](https://arxiv.org/abs/2507.08003)
*Kayhan Latifzadeh,Jacek Gwizdka,Luis A. Leiva*

Main category: cs.HC

TL;DR: 本文贡献了一个基于眼动仪的综合数据集，用于研究搜索引擎结果页面（SERP）上的用户注意力与购买行为，旨在克服现有研究的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖鼠标移动作为行为代理和事后自我报告的真值标签，这些方法可能不准确且易受偏倚影响，无法提供客观、连续的视觉注意力数据。

Method: 使用眼动仪收集了47名参与者在Google SERP上2,776个交易查询的客观、连续视觉注意力数据。数据集包含HTML源文件、SERP截图、眼动数据、鼠标移动数据、直接显示和有机广告的边界框，以及数据预处理脚本。

Result: 成功构建并发布了一个大规模、多模态的客观数据集，用于深入研究SERP上的用户注意力与购买行为。本文还提供了数据集的概述和基线分类实验，以启发未来的研究工作。

Conclusion: 该数据集为SERP用户行为研究提供了客观、高质量的基础数据，有望克服现有研究的局限性，并激发未来在这一领域的更多可能性和创新。

Abstract: We contribute a comprehensive dataset to study user attention and purchasing
behavior on Search Engine Result Pages (SERPs). Previous work has relied on
mouse movements as a low-cost large-scale behavioral proxy but also has relied
on self-reported ground-truth labels, collected at post-task, which can be
inaccurate and prone to biases. To address this limitation, we use an eye
tracker to construct an objective ground-truth of continuous visual attention.
Our dataset comprises 2,776 transactional queries on Google SERPs, collected
from 47 participants, and includes: (1) HTML source files, with CSS and images;
(2) rendered SERP screenshots; (3) eye movement data; (4) mouse movement data;
(5) bounding boxes of direct display and organic advertisements; and (6)
scripts for further preprocessing the data. In this paper we provide an
overview of the dataset and baseline experiments (classification tasks) that
can inspire researchers about the different possibilities for future work.

</details>


### [243] [Emotion Detection in Older Adults Using Physiological Signals from Wearable Sensors](https://arxiv.org/abs/2507.08167)
*Md. Saif Hassan Onim,Andrew M. Kiselica,Himanshu Thapliyal*

Main category: cs.HC

TL;DR: 本文提出并验证了一种基于可穿戴生理信号的非侵入性老年人情绪强度识别方法，该方法无需摄像头和面部分析，在回归任务中实现了高精度（R2=0.782），对保护隐私的情绪识别系统具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 理解老年人的认知和情感健康至关重要，尤其是在医院和辅助生活环境中。传统的情绪识别方法常依赖摄像头或面部分析，可能侵犯隐私或具有侵入性。本研究旨在开发一种仅使用生理信号的非侵入性情绪识别方案，以解决隐私问题，并惠及阿尔茨海默病、创伤后应激障碍等患者。

Method: 研究采用边缘计算方法，收集了40名老年人的生理信号（来自Empatica E4和Shimmer3 GSR+腕带）以及面部表情数据（iMotion FEA模块作为参考）。数据集包含12种情绪类别的相对强度。核心方法是利用经典机器学习模型，仅基于生理信号预测情绪强度的回归任务，不依赖摄像头或面部分析。

Result: 在情绪强度回归预测任务中，该方法取得了最高0.782的R2分数和最低0.0006的均方误差（MSE）。

Conclusion: 研究结果验证了仅使用生理信号进行非侵入性情绪识别方法的可行性，为在实际环境中开发保护隐私、高效的情绪识别系统铺平了道路。该方法对阿尔茨海默病及相关痴呆症(ADRD)患者、创伤后应激障碍(PTSD)患者或其他认知障碍患者具有重要应用潜力。

Abstract: Emotion detection in older adults is crucial for understanding their
cognitive and emotional well-being, especially in hospital and assisted living
environments. In this work, we investigate an edge-based, non-obtrusive
approach to emotion identification that uses only physiological signals
obtained via wearable sensors. Our dataset includes data from 40 older
individuals. Emotional states were obtained using physiological signals from
the Empatica E4 and Shimmer3 GSR+ wristband and facial expressions were
recorded using camera-based emotion recognition with the iMotion's Facial
Expression Analysis (FEA) module. The dataset also contains twelve emotion
categories in terms of relative intensities. We aim to study how well emotion
recognition can be accomplished using simply physiological sensor data, without
the requirement for cameras or intrusive facial analysis. By leveraging
classical machine learning models, we predict the intensity of emotional
responses based on physiological signals. We achieved the highest 0.782 r2
score with the lowest 0.0006 MSE on the regression task. This method has
significant implications for individuals with Alzheimer's Disease and Related
Dementia (ADRD), as well as veterans coping with Post-Traumatic Stress Disorder
(PTSD) or other cognitive impairments. Our results across multiple classical
regression models validate the feasibility of this method, paving the way for
privacy-preserving and efficient emotion recognition systems in real-world
settings.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [244] [Generative AI in Science: Applications, Challenges, and Emerging Questions](https://arxiv.org/abs/2507.08310)
*Ryan Harries,Cornelia Lawson,Philip Shapira*

Main category: cs.CY

TL;DR: 本文通过定性文献回顾，分析了生成式AI对科学实践的影响、应用、益处和挑战。


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI（包括大型语言模型和ChatGPT）对科学实践的影响，以提供早期洞察并识别未来研究方向。

Method: 使用OpenAlex数据库，通过布尔搜索识别与GenAI相关的文献，对39篇高被引论文和评论进行定性编码和审查。

Result: 研究结果按GenAI在科学、科学写作、医疗实践和教育培训中的应用进行分类。发现GenAI在科学领域被快速采用，但其长期影响、使用和治理仍存在不确定性。

Conclusion: 本研究为GenAI在科学中日益重要的作用提供了早期见解，并为该新兴领域的未来研究提出了问题。

Abstract: This paper examines the impact of Generative Artificial Intelligence (GenAI)
on scientific practices, conducting a qualitative review of selected literature
to explore its applications, benefits, and challenges. The review draws on the
OpenAlex publication database, using a Boolean search approach to identify
scientific literature related to GenAI (including large language models and
ChatGPT). Thirty-nine highly cited papers and commentaries are reviewed and
qualitatively coded. Results are categorized by GenAI applications in science,
scientific writing, medical practice, and education and training. The analysis
finds that while there is a rapid adoption of GenAI in science and science
practice, its long-term implications remain unclear, with ongoing uncertainties
about its use and governance. The study provides early insights into GenAI's
growing role in science and identifies questions for future research in this
evolving field.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [245] [Dual-Attention U-Net++ with Class-Specific Ensembles and Bayesian Hyperparameter Optimization for Precise Wound and Scale Marker Segmentation](https://arxiv.org/abs/2507.05314)
*Daniel Cieślak,Miriam Reca,Olena Onyshchenko,Jacek Rumiński*

Main category: eess.IV

TL;DR: 针对临床图像中伤口和刻度标记的精准分割挑战，提出了一种结合双注意力机制的U-Net++模型，并在竞赛中取得0.8640的F1分数。


<details>
  <summary>Details</summary>
Motivation: 临床图像中伤口和刻度标记的准确分割对于有效的伤口管理和自动化评估至关重要，但由于严重的类别不平衡和图像变异性，这仍是一个重大挑战。

Method: 提出一种新型双注意力U-Net++架构，整合通道（SCSE）和空间注意力机制。通过基准测试选定EfficientNet-B7为最佳编码器骨干。独立训练两个类别专用模型，并采用定制预处理、大量数据增强、贝叶斯超参数调优及测试时间增强（TTA）的集成方法。在竞赛基准数据集上以加权F1-score进行评估。

Result: 所提出的方法在NBC 2025 & PCBBE 2025竞赛的评估中取得了0.8640的F1-score。

Conclusion: 该方法在复杂的医疗图像分割任务中表现出显著有效性，成功应对了伤口和刻度标记的精确分割难题。

Abstract: Accurate segmentation of wounds and scale markers in clinical images remainsa
significant challenge, crucial for effective wound management and
automatedassessment. In this study, we propose a novel dual-attention U-Net++
archi-tecture, integrating channel-wise (SCSE) and spatial attention mechanisms
toaddress severe class imbalance and variability in medical images
effectively.Initially, extensive benchmarking across diverse architectures and
encoders via 5-fold cross-validation identified EfficientNet-B7 as the optimal
encoder backbone.Subsequently, we independently trained two class-specific
models with tailoredpreprocessing, extensive data augmentation, and Bayesian
hyperparameter tun-ing (WandB sweeps). The final model ensemble utilized Test
Time Augmentationto further enhance prediction reliability. Our approach was
evaluated on a bench-mark dataset from the NBC 2025 & PCBBE 2025 competition.
Segmentationperformance was quantified using a weighted F1-score (75% wounds,
25% scalemarkers), calculated externally by competition organizers on
undisclosed hard-ware. The proposed approach achieved an F1-score of 0.8640,
underscoring itseffectiveness for complex medical segmentation tasks.

</details>


### [246] [3D forest semantic segmentation using multispectral LiDAR and 3D deep learning](https://arxiv.org/abs/2507.08025)
*Narges Takhtkeshha,Lauris Bocaux,Lassi Ruoppa,Fabio Remondino,Gottfried Mandlburger,Antero Kukko,Juha Hyyppä*

Main category: eess.IV

TL;DR: 研究多光谱LiDAR数据在森林组分自动分割中的潜力，通过深度学习模型实现高精度分割，发现KPConv模型和多波长特征组合效果最佳。


<details>
  <summary>Details</summary>
Motivation: 森林资源清查对管理和决策至关重要，但传统方法耗时耗力。LiDAR作为一种远程无损解决方案备受关注。森林组分分割是森林清查的关键，结合空间和光谱信息能提高分割精度。本研究旨在探讨多光谱LiDAR数据在将森林分割为地表、低植被、树干、树枝、树叶和枯木六个组分方面的潜力。

Method: 使用HeliALS系统采集的高密度多光谱LiDAR点云数据。实现了三种点级3D深度学习模型（Kernel Point Convolution、Superpoint Transformer、Point Transformer V3）和一种机器学习模型（Random Forest）。此外，还检验了多种几何和光谱特征向量场景。

Result: 实验证实KPConv模型表现出卓越的精度。通过将所有三个波长（1550 nm、905 nm和532 nm）作为初始特征输入深度学习模型，获得了最高精度，使平均交并比（mIoU）和平均精度（mAcc）分别提高了33.73%和32.35%。

Conclusion: 本研究强调了多光谱LiDAR在提高全自动森林组分分割精度方面的巨大潜力。

Abstract: Conservation and decision-making regarding forest resources necessitate
regular forest inventory. Light detection and ranging (LiDAR) in laser scanning
systems has gained significant attention over the past two decades as a remote
and non-destructive solution to streamline the labor-intensive and
time-consuming procedure of forest inventory. Advanced multispectral (MS) LiDAR
systems simultaneously acquire three-dimensional (3D) spatial and spectral
information across multiple wavelengths of the electromagnetic spectrum.
Consequently, MS-LiDAR technology enables the estimation of both the
biochemical and biophysical characteristics of forests. Forest component
segmentation is crucial for forest inventory. The synergistic use of spatial
and spectral laser information has proven to be beneficial for achieving
precise forest semantic segmentation. Thus, this study aims to investigate the
potential of MS-LiDAR data, captured by the HeliALS system, providing
high-density multispectral point clouds to segment forests into six components:
ground, low vegetation, trunks, branches, foliage, and woody debris. Three
point-wise 3D deep learning models and one machine learning model, including
kernel point convolution, superpoint transformer, point transformer V3, and
random forest, are implemented. Our experiments confirm the superior accuracy
of the KPConv model. Additionally, various geometric and spectral feature
vector scenarios are examined. The highest accuracy is achieved by feeding all
three wavelengths (1550 nm, 905 nm, and 532 nm) as the initial features into
the deep learning model, resulting in improvements of 33.73% and 32.35% in mean
intersection over union (mIoU) and in mean accuracy (mAcc), respectively. This
study highlights the excellent potential of multispectral LiDAR for improving
the accuracy in fully automated forest component segmentation.

</details>


### [247] [Cracking Instance Jigsaw Puzzles: An Alternative to Multiple Instance Learning for Whole Slide Image Analysis](https://arxiv.org/abs/2507.08178)
*Xiwen Chen,Peijie Qiu,Wenhui Zhu,Hao Wang,Huayu Li,Xuanzhao Dong,Xiaotong Sun,Xiaobing Yu,Yalin Wang,Abolfazl Razi,Aristeidis Sotiras*

Main category: eess.IV

TL;DR: MIL的排列不变性限制了WSI实例语义关联发现。本文提出“实例拼图”任务（恢复实例顺序），利用Siamese网络和最优传输理论，在WSI分类和生存预测上超越SOTA MIL。


<details>
  <summary>Details</summary>
Motivation: 现有多实例学习(MIL)在组织病理学全玻片图像(WSI)分析中，因其对排列不变性的依赖，限制了有效揭示实例间语义关联的能力。

Method: 提出一种新的WSI分析范式，通过学习从随机打乱的实例排列中恢复其顺序，将其称为“实例拼图问题”来发现语义关联。为解决此问题，设计了一种新颖的Siamese网络解决方案，该方案在理论上由最优传输理论支持。

Result: 在WSI分类和生存预测任务中验证了所提出的方法，结果显示其性能优于近期最先进的MIL竞争方法。

Conclusion: 通过解决“实例拼图问题”的方法，能够有效克服传统MIL排列不变性的限制，更好地发现WSI中实例间的语义关联，并在实际任务中取得卓越性能。

Abstract: While multiple instance learning (MIL) has shown to be a promising approach
for histopathological whole slide image (WSI) analysis, its reliance on
permutation invariance significantly limits its capacity to effectively uncover
semantic correlations between instances within WSIs. Based on our empirical and
theoretical investigations, we argue that approaches that are not
permutation-invariant but better capture spatial correlations between instances
can offer more effective solutions. In light of these findings, we propose a
novel alternative to existing MIL for WSI analysis by learning to restore the
order of instances from their randomly shuffled arrangement. We term this task
as cracking an instance jigsaw puzzle problem, where semantic correlations
between instances are uncovered. To tackle the instance jigsaw puzzles, we
propose a novel Siamese network solution, which is theoretically justified by
optimal transport theory. We validate the proposed method on WSI classification
and survival prediction tasks, where the proposed method outperforms the recent
state-of-the-art MIL competitors. The code is available at
https://github.com/xiwenc1/MIL-JigsawPuzzles.

</details>


### [248] [Depth-Sequence Transformer (DST) for Segment-Specific ICA Calcification Mapping on Non-Contrast CT](https://arxiv.org/abs/2507.08214)
*Xiangjian Hou,Ebru Yaman Akcicek,Xin Wang,Kazem Hashemizadeh,Scott Mcnally,Chun Yuan,Xiaodong Ma*

Main category: eess.IV

TL;DR: 本文提出了一种名为Depth-Sequence Transformer (DST)的新框架，能够首次实现颅内颈动脉钙化(ICAC)的自动、段特异性量化，解决了传统方法无法处理全局上下文和定位解剖地标的技术难题，并展现出高精度。


<details>
  <summary>Details</summary>
Motivation: 虽然颅内颈动脉钙化(ICAC)总体积是公认的中风生物标志物，但越来越多的证据表明钙化位置对预后和手术风险有关键影响。然而，细粒度的、段特异性量化因技术限制（传统3D模型需降采样或孤立处理，牺牲全局上下文，导致解剖模糊和地标定位不可靠）一直无法实现。

Method: 将3D挑战重新定义为沿1D轴向的“并行概率地标定位”任务。提出了“Depth-Sequence Transformer (DST)”框架，该框架将全分辨率CT体积作为2D切片序列进行处理，学习预测6个独立的概率分布，以精确定位关键解剖地标。

Result: DST框架表现出卓越的准确性和鲁棒性。在100名患者的临床队列上进行5折交叉验证评估，实现了0.1切片的平均绝对误差(MAE)，96%的预测落在±1切片的容差范围内。此外，DST骨干网络在公共Clean-CC-CCII分类基准的端到端评估协议下取得了最佳结果。

Conclusion: 本研究提供了一种实用的自动化段特异性ICAC分析工具。所提出的框架为未来关于位置特异性生物标志物在诊断、预后和手术规划中的作用研究奠定了基础。

Abstract: While total intracranial carotid artery calcification (ICAC) volume is an
established stroke biomarker, growing evidence shows this aggregate metric
ignores the critical influence of plaque location, since calcification in
different segments carries distinct prognostic and procedural risks. However, a
finer-grained, segment-specific quantification has remained technically
infeasible. Conventional 3D models are forced to process downsampled volumes or
isolated patches, sacrificing the global context required to resolve anatomical
ambiguity and render reliable landmark localization. To overcome this, we
reformulate the 3D challenge as a \textbf{Parallel Probabilistic Landmark
Localization} task along the 1D axial dimension. We propose the
\textbf{Depth-Sequence Transformer (DST)}, a framework that processes
full-resolution CT volumes as sequences of 2D slices, learning to predict $N=6$
independent probability distributions that pinpoint key anatomical landmarks.
Our DST framework demonstrates exceptional accuracy and robustness. Evaluated
on a 100-patient clinical cohort with rigorous 5-fold cross-validation, it
achieves a Mean Absolute Error (MAE) of \textbf{0.1 slices}, with \textbf{96\%}
of predictions falling within a $\pm1$ slice tolerance. Furthermore, to
validate its architectural power, the DST backbone establishes the best result
on the public Clean-CC-CCII classification benchmark under an end-to-end
evaluation protocol. Our work delivers the first practical tool for automated
segment-specific ICAC analysis. The proposed framework provides a foundation
for further studies on the role of location-specific biomarkers in diagnosis,
prognosis, and procedural planning. Our code will be made publicly available.

</details>


### [249] [Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging Pretrained 2D Foundation Models](https://arxiv.org/abs/2507.08254)
*Ulzee An,Moonseong Jeong,Simon A. Lee,Aditya Gorla,Yuzhe Yang,Sriram Sankararaman*

Main category: eess.IV

TL;DR: Raptor是一种免训练方法，利用2D预训练模型为体积医学图像生成语义丰富嵌入，在多种任务上表现优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 开发体积成像数据（如MRI）基础模型面临挑战：高维训练的计算复杂性，以及难以获取足够大的体积数据集。

Method: 引入Raptor（随机平面张量归约），一种免训练方法。它利用一个在自然图像上预训练的冻结2D基础模型，从医学体积的单个横截面中提取视觉tokens，然后使用随机投影对这些tokens进行空间压缩，从而显著降低计算复杂性并保留语义信息。

Result: 在十个不同的医学体积任务上进行的大量实验验证了Raptor的卓越性能，优于现有最先进方法（包括专门在医学体积上预训练的模型），且完全无需昂贵的训练。

Conclusion: Raptor的有效性和多功能性使其成为推动医学体积深度学习方法发展的基础。

Abstract: Current challenges in developing foundational models for volumetric imaging
data, such as magnetic resonance imaging (MRI), stem from the computational
complexity of training state-of-the-art architectures in high dimensions and
curating sufficiently large datasets of volumes. To address these challenges,
we introduce Raptor (Random Planar Tensor Reduction), a train-free method for
generating semantically rich embeddings for volumetric data. Raptor leverages a
frozen 2D foundation model, pretrained on natural images, to extract visual
tokens from individual cross-sections of medical volumes. These tokens are then
spatially compressed using random projections, significantly reducing
computational complexity while retaining semantic information. Extensive
experiments on ten diverse medical volume tasks verify the superior performance
of Raptor over state-of-the-art methods, including those pretrained exclusively
on medical volumes (+3% SuPreM, +6% MISFM, +10% Merlin, +13% VoCo, and +14%
SLIViT), while entirely bypassing the need for costly training. Our results
highlight the effectiveness and versatility of Raptor as a foundation for
advancing deep learning-based methods for medical volumes.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [250] [Consciousness as a Jamming Phase](https://arxiv.org/abs/2507.08197)
*Kaichen Ouyang*

Main category: cond-mat.dis-nn

TL;DR: 论文提出“神经堵塞相图”，将大语言模型中的意识解释为高维无序系统中的临界堵塞现象。


<details>
  <summary>Details</summary>
Motivation: 旨在理解大型语言模型中意识的出现，并为人工智能中的经验性标度律提供统一的物理解释。

Method: 开发了“神经堵塞相图”，通过与颗粒物质等复杂系统的堵塞转变建立类比，识别出控制神经网络相行为的温度、体积分数和应力三个基本控制参数。

Result: 该理论为人工智能的经验性标度律提供了统一的物理解释，揭示了计算冷却、密度优化和噪声降低如何共同驱动系统达到临界堵塞表面，从而涌现出广义智能。研究发现，描述传统堵塞转变的热力学原理与神经网络中意识的出现共享相同的临界特征，包括发散的相关长度和标度指数。

Conclusion: 意识是一种堵塞相，通过长程关联内在连接知识组件，可以被理解为高维无序系统中的一种临界现象。

Abstract: This paper develops a neural jamming phase diagram that interprets the
emergence of consciousness in large language models as a critical phenomenon in
high-dimensional disordered systems.By establishing analogies with jamming
transitions in granular matter and other complex systems, we identify three
fundamental control parameters governing the phase behavior of neural networks:
temperature, volume fraction, and stress.The theory provides a unified physical
explanation for empirical scaling laws in artificial intelligence,
demonstrating how computational cooling, density optimization, and noise
reduction collectively drive systems toward a critical jamming surface where
generalized intelligence emerges. Remarkably, the same thermodynamic principles
that describe conventional jamming transitions appear to underlie the emergence
of consciousness in neural networks, evidenced by shared critical signatures
including divergent correlation lengths and scaling exponents.Our work explains
neural language models' critical scaling through jamming physics, suggesting
consciousness is a jamming phase that intrinsically connects knowledge
components via long-range correlations.

</details>
