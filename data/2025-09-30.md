<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 12]
- [cs.CV](#cs.CV) [Total: 11]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.LG](#cs.LG) [Total: 11]
- [cs.NI](#cs.NI) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Are you sure? Measuring models bias in content moderation through uncertainty](https://arxiv.org/abs/2509.22699)
*Alessandra Urbinati,Mirko Lai,Simona Frenda,Marco Antonio Stranisci*

Main category: cs.CL

TL;DR: 自动内容审核中的语言模型存在偏见。本文提出一种无监督方法，利用共形预测的不确定性来衡量模型对弱势群体（女性、非白人标注者）的偏见，发现模型在处理少数群体数据时即使准确率高，置信度却可能很低，表明不确定性是识别和指导去偏的有效指标。


<details>
  <summary>Details</summary>
Motivation: 自动内容审核对社交媒体安全至关重要，但基于语言模型的分类器存在种族和社会偏见。尽管已有资源，衡量内容审核模型的公平性仍是一个悬而未决的问题。

Method: 提出一种无监督方法，根据模型对弱势群体（女性、非白人标注者）标注消息的分类不确定性来评估模型。通过共形预测技术计算不确定性，并将其作为分析11个模型偏见的代理指标，同时与F1分数等性能指标进行对比。

Result: 研究结果表明，某些预训练模型能够以高准确率预测来自少数群体标注者的标签，但其预测置信度却很低。

Conclusion: 通过衡量模型的置信度（或不确定性），可以识别预训练模型中哪些标注者群体得到了更好的代表，从而在模型实际使用前指导其去偏过程。

Abstract: Automatic content moderation is crucial to ensuring safety in social media.
Language Model-based classifiers are being increasingly adopted for this task,
but it has been shown that they perpetuate racial and social biases. Even if
several resources and benchmark corpora have been developed to challenge this
issue, measuring the fairness of models in content moderation remains an open
issue. In this work, we present an unsupervised approach that benchmarks models
on the basis of their uncertainty in classifying messages annotated by people
belonging to vulnerable groups. We use uncertainty, computed by means of the
conformal prediction technique, as a proxy to analyze the bias of 11 models
against women and non-white annotators and observe to what extent it diverges
from metrics based on performance, such as the $F_1$ score. The results show
that some pre-trained models predict with high accuracy the labels coming from
minority groups, even if the confidence in their prediction is low. Therefore,
by measuring the confidence of models, we are able to see which groups of
annotators are better represented in pre-trained models and lead the debiasing
process of these models before their effective use.

</details>


### [2] [AccessEval: Benchmarking Disability Bias in Large Language Models](https://arxiv.org/abs/2509.22703)
*Srikant Panda,Amit Agarwal,Hitesh Laxmichand Patel*

Main category: cs.CL

TL;DR: AccessEval基准测试发现，大型语言模型在处理残疾相关查询时，回答倾向于更消极的语气、更多的刻板印象和更高的事实错误率，这反映了模型中存在的歧视。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理现实世界查询时表现出差异，特别是在残疾背景下。需要系统性地研究这些影响。

Method: 引入AccessEval基准，评估21个闭源和开源LLMs，涵盖6个真实世界领域和9种残疾类型。使用配对的中性查询和残疾感知查询，并通过情感、社会感知和事实准确性指标评估模型输出。

Result: 残疾感知查询的回复倾向于有更消极的语气、更多的刻板印象和更高的事实错误。这些影响因领域和残疾类型而异，其中听力、言语和行动障碍受到的影响尤为严重。这些差异反映了模型行为中固有的能力歧视。

Conclusion: 模型中的偏见可能对残疾用户造成实际伤害。这项工作弥合了技术评估与用户影响之间的鸿沟，强调了在日常应用中缓解偏见的重要性。

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse domains
but often exhibit disparities in how they handle real-life queries. To
systematically investigate these effects within various disability contexts, we
introduce \textbf{AccessEval (Accessibility Evaluation)}, a benchmark
evaluating 21 closed- and open-source LLMs across 6 real-world domains and 9
disability types using paired Neutral and Disability-Aware Queries. We
evaluated model outputs with metrics for sentiment, social perception, and
factual accuracy.
  Our analysis reveals that responses to disability-aware queries tend to have
a more negative tone, increased stereotyping, and higher factual error compared
to neutral queries. These effects show notable variation by domain and
disability type, with disabilities affecting hearing, speech, and mobility
disproportionately impacted. These disparities reflect persistent forms of
ableism embedded in model behavior.
  By examining model performance in real-world decision-making contexts, we
better illuminate how such biases can translate into tangible harms for
disabled users. This framing helps bridges the gap between technical evaluation
and user impact, reinforcing importance of bias mitigation in day-to-day
applications. Our dataset is publicly available at:
https://huggingface.co/datasets/Srikant86/AccessEval

</details>


### [3] [RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval](https://arxiv.org/abs/2509.22713)
*Kaishuai Xu,Wenjun Hou,Yi Cheng,Wenjie Li*

Main category: cs.CL

TL;DR: 本文提出RAR$^2$框架，通过构建思维过程显式建模推理，以指导检索和答案生成，从而在复杂的生物医学问答任务中显著优于现有RAG基线。


<details>
  <summary>Details</summary>
Motivation: 现有RAG（检索增强生成）在处理需要深入推理的复杂医学问题时表现不佳，因为表面层面的输入无法反映真正的知识需求。当前方法通常只优化查询，而未明确建模推理过程，限制了其检索和整合临床相关知识的能力。

Method: RAR$^2$是一个联合学习框架，它通过构建思维过程来揭示隐性知识需求，并用此过程指导检索和答案生成。该方法通过构建混合偏好对训练数据集，并使用直接偏好优化（DPO）来训练模型。此外，还设计了两种测试时扩展策略。

Result: 实验结果表明，RAR$^2$在多个生物医学问答数据集上表现出有效性，并超越了未微调和已微调的RAG基线模型。

Conclusion: RAR$^2$通过将推理过程融入检索和生成，有效解决了RAG在处理复杂医学问题时的不足，显著提升了模型性能，展示了在真实临床任务中的巨大潜力。

Abstract: Large Language Models (LLMs) have shown promising performance on diverse
medical benchmarks, highlighting their potential in supporting real-world
clinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key
approach for mitigating knowledge gaps and hallucinations by incorporating
external medical information. However, RAG still struggles with complex medical
questions that require intensive reasoning, as surface-level input often fails
to reflect the true knowledge needs of the task. Existing methods typically
focus on refining queries without explicitly modeling the reasoning process,
limiting their ability to retrieve and integrate clinically relevant knowledge.
In this work, we propose RAR$^2$, a joint learning framework that improves both
Reasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$
constructs a thought process to uncover implicit knowledge requirements and
uses it to guide retrieval and answer generation. We build a training dataset
of mixed preference pairs and apply Direct Preference Optimization (DPO) to
train the model. Moreover, we design two test-time scaling strategies to
explore the boundaries of our framework. Experiments demonstrate the
effectiveness of RAR$^2$ across several biomedical question answering datasets,
outperforming RAG baselines with or without fine-tuning.

</details>


### [4] [TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?](https://arxiv.org/abs/2509.22715)
*Jiho Park,Jongyoon Song,Minjin Choi,Kyuho Heo,Taehun Huh,Ji Won Kim*

Main category: cs.CL

TL;DR: 引入TRUEBench，一个用于评估LLM生产力助手的多语言、多轮、真实世界指令遵循能力的基准，揭示了现有模型（如OpenAI o1）在此方面面临的显著挑战和局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准在评估其真实世界指令遵循能力时存在不足，具体表现在：缺乏多语言性、未能捕捉用户请求中的隐式约束，以及忽视多轮对话的复杂性。

Method: 引入TRUEBench基准，专门为LLM生产力助手设计。该基准包含12种语言的输入提示、实例内多语言指令、严格的显式和隐式约束评估标准，以及累积约束和上下文切换的复杂多轮对话场景。同时，采用LLM验证器精炼约束以确保评估可靠性。

Result: 实验证明TRUEBench比现有基准更具挑战性；例如，强大的模型如OpenAI o1的整体通过率仅为69.07%。

Conclusion: TRUEBench为LLM在实际生产力场景中提供了严苛且真实的评估，突出了其能力和局限性。

Abstract: Large language models (LLMs) are increasingly integral as productivity
assistants, but existing benchmarks fall short in rigorously evaluating their
real-world instruction-following capabilities. Current benchmarks often (i)
lack sufficient multilinguality, (ii) fail to capture the implicit constraints
inherent in user requests, and (iii) overlook the complexities of multi-turn
dialogue. To address these critical gaps and provide a more realistic
assessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation
Benchmark)1, a novel benchmark specifically designed for LLM-based productivity
assistants. TRUEBench distinguishes itself by featuring input prompts across 12
languages, incorporating intra-instance multilingual instructions, employing
rigorous evaluation criteria to capture both explicit and implicit constraints,
and including complex multi-turn dialogue scenarios with both accumulating
constraints and context switches. Furthermore, to ensure reliability in
evaluation, we refined constraints using an LLM validator. Extensive
experiments demonstrate that TRUEBench presents significantly greater
challenges than existing benchmarks; for instance, a strong model like OpenAI
o1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and
realistic assessment of LLMs in practical productivity settings, highlighting
their capabilities and limitations.

</details>


### [5] [Multi-Modal Sentiment Analysis with Dynamic Attention Fusion](https://arxiv.org/abs/2509.22729)
*Sadia Abdulhalim,Muaz Albaghdadi,Moshiur Farazi*

Main category: cs.CL

TL;DR: 本文提出动态注意力融合（DAF）框架，通过自适应注意力机制结合文本和语音特征，显著提升了多模态情感分析的性能，且无需微调底层编码器。


<details>
  <summary>Details</summary>
Motivation: 传统情感分析仅依赖文本，忽略了语音语调等非语言线索对捕捉真实情感意图的关键作用。

Method: 引入轻量级动态注意力融合（DAF）框架，它将预训练语言模型的固定文本嵌入与语音编码器的声学特征结合起来，并使用自适应注意力机制对每个话语的模态进行加权，且不进行底层编码器的微调。

Result: DAF模型在大型多模态基准测试中持续优于静态融合和单模态基线，F1分数显著提高，预测误差降低。消融研究进一步证实，动态加权策略对于建模复杂情感输入至关重要。

Conclusion: 该方法通过有效整合语言和非语言信息，为情感预测提供了更稳健的基础，对情感计算应用（如情感识别、心理健康评估、更自然的人机交互）具有广泛影响。

Abstract: Traditional sentiment analysis has long been a unimodal task, relying solely
on text. This approach overlooks non-verbal cues such as vocal tone and prosody
that are essential for capturing true emotional intent. We introduce Dynamic
Attention Fusion (DAF), a lightweight framework that combines frozen text
embeddings from a pretrained language model with acoustic features from a
speech encoder, using an adaptive attention mechanism to weight each modality
per utterance. Without any finetuning of the underlying encoders, our proposed
DAF model consistently outperforms both static fusion and unimodal baselines on
a large multimodal benchmark. We report notable gains in F1-score and
reductions in prediction error and perform a variety of ablation studies that
support our hypothesis that the dynamic weighting strategy is crucial for
modeling emotionally complex inputs. By effectively integrating verbal and
non-verbal information, our approach offers a more robust foundation for
sentiment prediction and carries broader impact for affective computing
applications -- from emotion recognition and mental health assessment to more
natural human computer interaction.

</details>


### [6] [Enabling Approximate Joint Sampling in Diffusion LMs](https://arxiv.org/abs/2509.22738)
*Parikshit Bansal,Sujay Sanghavi*

Main category: cs.CL

TL;DR: 本研究提出一种轻量级采样器，使扩散语言模型能在一次全模型前向传播后，通过多次采样器前向传播近似并行采样多个token，以平衡生成速度与符合联合分布的准确性，并在多项任务上表现出显著效果。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型通过顺序采样确保符合潜在联合分布，而掩码扩散语言模型为提高速度而并行或乱序解掩码，导致偏离真实联合分布并降低准确性。现有扩散模型若要准确采样，仍需单次前向传播解掩码一个token，速度慢。研究动机在于如何在并行生成多token的同时，仍能近似地从真实联合分布中采样。

Method: 在现有大型扩散语言模型之上，开发了一个新的、轻量级的单层“采样器”。该方法允许一次全模型前向传播后，紧接着进行多次仅针对此采样器层的快速前向传播，以解掩码多个token。该采样器被训练来模仿（冻结的）全模型的精确联合采样行为。

Result: 该近似联合采样方法在预训练模型（Dream-7B-Base）和指令微调模型（Dream-7B-Instruct）上，在语言建模和数学/编程任务中均显示出有效性。当每次全模型去噪步骤中解掩码四个token时，提出的采样算法相对于真实联合分布达到了0.87的MAUVE分数，远高于边缘基线的0.31。

Conclusion: 本研究成功开发了一种轻量级采样器，使得扩散语言模型能够在单次全模型前向传播的基础上，近似地并行采样多个token，显著提高了生成质量（通过MAUVE分数衡量），有效解决了速度与符合真实联合分布之间的矛盾。

Abstract: In autoregressive language models, each token is sampled by conditioning on
all the past tokens; the overall string has thus been sampled from the correct
underlying joint distribution represented by the model. In contrast, masked
diffusion language models generate text by unmasking tokens out of order and
potentially in parallel. Generating an overall string sampled from the correct
underlying joint distribution would (again) require exactly one token unmasking
in every full-model forward pass. The more tokens unmasked in parallel, the
further away the string is from the true joint; this can be seen in the
resulting drop in accuracy (but, increase in speed). In this paper we devise a
way to {\em approximately} sample multiple tokens from the joint distribution
in a single full-model forward pass; we do so by developing a new lightweight
single-layer ``sampler" on top of an existing large diffusion LM. One forward
pass of the full model can now be followed by multiple forward passes of only
this sampler layer, to yield multiple unmasked tokens. Our sampler is trained
to mimic exact joint sampling from the (frozen) full model. We show the
effectiveness of our approximate joint sampling for both pretrained-only
(Dream-7B-Base) and instruction-tuned (Dream-7B-Instruct) models on language
modeling and math \& coding tasks. When four tokens are unmasked for each
full-model denoising step, our sampling algorithm achieves a MAUVE score of
0.87 (vs marginal baseline of 0.31) with respect to the true joint
distribution.

</details>


### [7] [Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models](https://arxiv.org/abs/2509.22739)
*Sasha Cui,Zhongren Chen*

Main category: cs.CL

TL;DR: 提出Painless Activation Steering (PAS)，一种全自动的激活转向方法，可无需人工干预高效调整语言模型行为。PAS在行为任务上表现出色，并可作为LM后训练的实用自动化选项。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型(LM)后训练方法（基于权重或提示）存在耗时、昂贵或控制不精确等问题。虽然激活转向(AS)有潜力，但当前技术需要手动构建提示或大量特征标注，不够即插即用。

Method: 引入Painless Activation Steering (PAS)方法家族，它是一种全自动的激活转向技术，可利用任何带标签数据集，无需提示构建、特征标注或人工干预。PAS通过构建一个快速、轻量级的激活向量来调整模型行为。

Result: 在三种开源模型和18项任务上评估，PAS能可靠提升行为任务表现，但对智能导向任务无效。内省变体(iPAS)在偏见、道德和对齐任务上分别实现10.1%、5.2%和34.8%的显著因果转向效果。PAS还能在上下文学习(ICL)和监督微调(SFT)基础上提供额外增益。

Conclusion: 本研究明确了激活转向的适用范围，并展示了如何将PAS作为一种实用、自动化的语言模型后训练方案进行部署。

Abstract: Language models (LMs) are typically post-trained for desired capabilities and
behaviors via weight-based or prompt-based steering, but the former is
time-consuming and expensive, and the latter is not precisely controllable and
often requires manual trial-and-error. While activation steering (AS) promises
a cheap, fast, and controllable alternative to the two existing post-training
methods, current AS techniques require hand-crafted prompt pairs or
labor-intensive feature annotation, making them more inconvenient than the
plug-and-play methods such as Reinforcement Learning (RL) and Supervised
Fine-Tuning (SFT). We introduce Painless Activation Steering (PAS), a family of
fully automated methods that make AS readily usable with any given labeled
dataset, with no need for prompt construction, feature labeling, or human
intervention. We evaluate PAS on three open-weight models
(Llama3.1-8B-Instruct, DeepSeek-R1-Distill-8B, and Nous-Hermes-2) and 18 tasks;
we find that PAS reliably improves performance for behavior tasks, but not for
intelligence-oriented tasks. The introspective variant (iPAS) delivers the
strongest causal steering effects (10.1% on Bias, 5.2% on Morality, and 34.8%
on Alignment). We also show PAS delivers additional gains on top of In-Context
Learning (ICL) and SFT. PAS constructs a fast, lightweight activation vector
that can be cheaply trained, easily stored, and activated at will. Our results
provide a characterization of where AS helps, where it fails, and how to deploy
it as a practical, automated LM post-training option.

</details>


### [8] [MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions](https://arxiv.org/abs/2509.22750)
*Jeonghyun Park,Ingeol Baek,Seunghyun Yoon,Haeun Jang,Aparna Garimella,Akriti Jain,Nedim Lipka,Hwanhee Lee*

Main category: cs.CL

TL;DR: 针对现实世界多跳问答中固有的多层歧义挑战，本文引入了MIRAGE基准进行评估，并提出了CLARION多智能体框架，显著提升了模型在解决此类复杂歧义推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的多跳问答（QA）常涉及多层歧义，导致单一问题可能产生多条推理路径，且每个子问题都需独立解决歧义。现有的大型语言模型（LLMs）在这种设置下表现不佳，容易探索错误的推理路径并产生不完整的答案。

Method: 1. 引入了MIRAGE（MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions）基准，旨在分析和评估歧义解释与多跳推理的交叉挑战。MIRAGE包含1,142个高质量的歧义多跳问题示例，按句法、一般和语义歧义进行分类，并通过严格的多LLM验证管道进行策划。2. 提出了CLARION（CLarifying Ambiguity with a Reasoning and InstructiON）多智能体框架，以建立一个鲁棒的基线。

Result: 1. 实验表明，即使是最先进的模型在MIRAGE基准上也表现不佳，证实了结合歧义解决的多步推理是一个独特且重大的挑战。2. 提出的CLARION多智能体框架在MIRAGE上显著优于现有方法。

Conclusion: 解决多跳问答中的多层歧义是当前大型语言模型面临的显著挑战。MIRAGE基准的建立和CLARION框架的提出，为开发更具适应性和鲁棒性的推理系统铺平了道路。

Abstract: Real-world Multi-hop Question Answering (QA) often involves ambiguity that is
inseparable from the reasoning process itself. This ambiguity creates a
distinct challenge, where multiple reasoning paths emerge from a single
question, each requiring independent resolution. Since each sub-question is
ambiguous, the model must resolve ambiguity at every step. Thus, answering a
single question requires handling multiple layers of ambiguity throughout the
reasoning chain. We find that current Large Language Models (LLMs) struggle in
this setting, typically exploring wrong reasoning paths and producing
incomplete answers. To facilitate research on multi-hop ambiguity, we introduce
MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE),
a benchmark designed to analyze and evaluate this challenging intersection of
ambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142
high-quality examples of ambiguous multi-hop questions, categorized under a
taxonomy of syntactic, general, and semantic ambiguity, and curated through a
rigorous multi-LLM verification pipeline. Our experiments reveal that even
state-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity
combined with multi-step inference is a distinct and significant challenge. To
establish a robust baseline, we propose CLarifying Ambiguity with a Reasoning
and InstructiON (CLARION), a multi-agent framework that significantly
outperforms existing approaches on MIRAGE, paving the way for more adaptive and
robust reasoning systems.

</details>


### [9] [ML2B: Multi-Lingual ML Benchmark For AutoML](https://arxiv.org/abs/2509.22768)
*Ekaterina Trofimova,Zosia Shamina,Maria Selifanova,Artem Zaitsev,Remi Savchuk,Maxim Minets,Daria Ozerova,Emil Sataev,Denis Zuenko,Andrey E. Ustyuzhanin*

Main category: cs.CL

TL;DR: 研究提出首个多语言ML代码生成基准ML2B，发现LLM在非英语任务上的性能显著下降15-45%，揭示了多语言表示学习的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习(ML)代码生成基准主要限于英语，忽视了ML研究和实践的全球化和多语言特性，因此需要一个评估多语言ML代码生成的基准。

Method: 开发了ML2B，一个包含30个Kaggle竞赛（涵盖表格、文本和图像数据，并翻译成13种自然语言）的多语言ML代码生成基准。使用AIDE自动化框架对数据科学管道进行端到端评估。

Result: 实验结果显示，大型语言模型(LLM)在非英语任务上的性能有15-45%的显著下降，这突出了多语言代码生成中跨语言表示学习的关键挑战。

Conclusion: ML2B基准和评估框架揭示了LLM在多语言ML代码生成方面的局限性，特别是非英语任务的性能不足，为未来多语言ML代码生成研究提供了基础和资源。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating machine learning (ML) code, enabling end-to-end pipeline
construction from natural language instructions. However, existing benchmarks
for ML code generation are mainly restricted to English, overlooking the global
and multilingual nature of ML research and practice. To address this gap, we
present ML2B, the first benchmark for evaluating multilingual ML code
generation. ML2B consists of 30 Kaggle competitions translated into 13 natural
languages, covering tabular, text, and image data types, with structured
metadata and validated human-reviewed translations. For evaluation, we employ
AIDE, an automated framework for end-to-end assessment of data science
pipelines, and provide insights into cross-lingual model performance. Our
results reveal substantial 15-45% performance degradation on non-English tasks,
highlighting critical challenges in multilingual representation learning for
code generation. The benchmark, evaluation framework, and comprehensive results
are made available through our GitHub repository to facilitate future research
in multilingual ML code generation: https://github.com/enaix/ml2b.

</details>


### [10] [ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection](https://arxiv.org/abs/2509.22808)
*Mohamed Maged,Alhassan Ehab,Ali Mekky,Besher Hassan,Shady Shehata*

Main category: cs.CL

TL;DR: 本文创建了首个多方言阿拉伯语伪造语音数据集，并评估了不同TTS模型生成合成语音的难度，发现FishSpeech能生成最具挑战性的样本。


<details>
  <summary>Details</summary>
Motivation: 生成式文本转语音模型使区分真实与合成语音变得困难，尤其在阿拉伯语领域，相关研究和伪造检测数据集非常匮乏。

Method: 引入了首个多方言阿拉伯语伪造语音数据集。评估流程包括：训练多种分类器（嵌入式方法、MFCC+经典ML、RawNet2），计算人类评分的平均意见得分（MOS），并使用自动语音识别（ASR）模型测量词错误率（WER）。

Result: FishSpeech在卡萨布兰卡语料库上的阿拉伯语语音克隆中表现最佳，生成的合成语音样本更真实、更具挑战性。

Conclusion: FishSpeech在生成难以区分的合成阿拉伯语语音方面表现出色，但仅依赖单一TTS模型构建数据集可能会限制其通用性。

Abstract: With the rise of generative text-to-speech models, distinguishing between
real and synthetic speech has become challenging, especially for Arabic that
have received limited research attention. Most spoof detection efforts have
focused on English, leaving a significant gap for Arabic and its many dialects.
In this work, we introduce the first multi-dialect Arabic spoofed speech
dataset. To evaluate the difficulty of the synthesized audio from each model
and determine which produces the most challenging samples, we aimed to guide
the construction of our final dataset either by merging audios from multiple
models or by selecting the best-performing model, we conducted an evaluation
pipeline that included training classifiers using two approaches: modern
embedding-based methods combined with classifier heads; classical machine
learning algorithms applied to MFCC features; and the RawNet2 architecture. The
pipeline further incorporated the calculation of Mean Opinion Score based on
human ratings, as well as processing both original and synthesized datasets
through an Automatic Speech Recognition model to measure the Word Error Rate.
Our results demonstrate that FishSpeech outperforms other TTS models in Arabic
voice cloning on the Casablanca corpus, producing more realistic and
challenging synthetic speech samples. However, relying on a single TTS for
dataset creation may limit generalizability.

</details>


### [11] [EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation](https://arxiv.org/abs/2509.22812)
*Kai Zhang,Christopher Malon,Lichao Sun,Martin Renqiang Min*

Main category: cs.CL

TL;DR: 本文提出EditGRPO，一种混合策略强化学习算法，通过临床驱动奖励优化放射报告生成，在SFT和GRPO基线之上取得了显著性能提升，并具有优越的域外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在放射报告生成中表现良好，但其监督微调（SFT）目标并未明确与临床疗效对齐。

Method: 引入EditGRPO，一种混合策略强化学习（RL）算法。该算法通过在训练rollout期间注入句子级详细修正，将在线策略探索与离线策略指导相结合，旨在通过临床动机奖励来优化生成。

Result: EditGRPO在四个主要胸部X光报告生成数据集上，相比SFT和普通GRPO基线，在CheXbert、GREEN、Radgraph和RATEScore等指标上平均提高了3.4%。此外，它在未见过的数据集上显示出优越的域外泛化能力，平均性能提升5.9%。

Conclusion: EditGRPO通过临床动机奖励显著改善了放射报告的生成质量和临床对齐性，并展现出强大的泛化能力。

Abstract: Radiology report generation requires advanced medical image analysis,
effective temporal reasoning, and accurate text generation. Although recent
innovations, particularly multimodal large language models (MLLMs), have shown
improved performance, their supervised fine-tuning (SFT) objective is not
explicitly aligned with clinical efficacy. In this work, we introduce EditGRPO,
a mixed-policy reinforcement learning (RL) algorithm designed specifically to
optimize the generation through clinically motivated rewards. EditGRPO
integrates on-policy exploration with off-policy guidance by injecting
sentence-level detailed corrections during training rollouts. This mixed-policy
approach addresses the exploration dilemma and sampling efficiency issues
typically encountered in RL. Applied to a Qwen2.5-VL-3B MLLM initialized with
supervised fine-tuning (SFT), EditGRPO outperforms both SFT and vanilla GRPO
baselines, achieving an average improvement of 3.4% in CheXbert, GREEN,
Radgraph, and RATEScore metrics across four major chest X-ray report generation
datasets. Notably, EditGRPO also demonstrates superior out-of-domain
generalization, with an average performance gain of 5.9% on unseen datasets.

</details>


### [12] [Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning](https://arxiv.org/abs/2509.22824)
*Chi Ruan,Dongfu Jiang,Yubo Wang,Wenhu Chen*

Main category: cs.CL

TL;DR: 本文提出批判性强化学习（CRL），通过结合标准强化学习训练，显著提升大型语言模型在代码生成和通用推理任务上的批判与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习（RL）主要关注响应生成，缺乏明确的批判或反思机制。近期研究（如CFT、CGD）表明教授LLM批判能力有益，因此本文旨在弥补RL在批判性培养上的不足。

Method: 引入批判性强化学习（CRL），模型需为给定（问题，解决方案）对生成批判。奖励基于生成批判的最终判断（真/假）是否与真实判断一致。在此基础上，提出\textsc{Critique-Coder}模型，通过将20%的标准RL数据替换为CRL数据进行混合训练和微调，并在多个基准上进行评估。

Result: \textsc{Critique-Coder}在所有评估基准上均持续优于仅使用RL的基线模型。其中，\textsc{Critique-Coder-8B}在LiveCodeBench (v5)上达到60%以上，超越DeepCoder-14B和GPT-o1等其他推理模型。此外，\textsc{Critique-Coder}在BBEH数据集的逻辑推理任务上表现出更强的通用推理能力，表明CRL对编码数据集的应用能增强通用推理和批判能力，且具有可迁移性。

Conclusion: 批判性强化学习（CRL）是标准强化学习的有效补充，能显著提升大型语言模型的推理能力，并增强其在编码和通用推理任务中的批判与反思能力。

Abstract: Reinforcement Learning (RL) has emerged as a popular training paradigm,
particularly when paired with reasoning models. While effective, it primarily
focuses on generating responses and lacks mechanisms to explicitly foster
critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)
and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly
teaching LLMs how to critique. Motivated by them, we propose Critique
Reinforcement Learning (CRL), where the model is tasked with generating a
critique for a given (question, solution) pair. The reward is determined solely
by whether the final judgment label $c \in \{\texttt{True}, \texttt{False}\}$
of the generated critique aligns with the ground-truth judgment $c^*$. Building
on this point, we introduce \textsc{Critique-Coder}, which is trained on a
hybrid of RL and CRL by substituting 20\% of the standard RL data with CRL
data. We fine-tune multiple models (\textsc{Critique-Coder}) and evaluate them
on different benchmarks to show their advantages over RL-only models. We show
that \textsc{Critique-Coder} consistently outperforms RL-only baselines on all
the evaluated benchmarks. Notably, our \textsc{Critique-Coder-8B} can reach
over 60\% on LiveCodeBench (v5), outperforming other reasoning models like
DeepCoder-14B and GPT-o1. Beyond code generation, \textsc{Critique-Coder} also
demonstrates enhanced general reasoning abilities, as evidenced by its better
performance on logic reasoning tasks from the BBEH dataset. This indicates that
the application of CRL on coding datasets enhances general reasoning and
critique abilities, which are transferable across a broad range of tasks.
Hence, we believe that CRL works as a great complement to standard RL for LLM
reasoning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [13] [Pathological Truth Bias in Vision-Language Models](https://arxiv.org/abs/2509.22674)
*Yash Thube*

Main category: cs.CV

TL;DR: 本研究引入MATS行为审计来评估VLM拒绝视觉矛盾陈述的能力，发现生成式VLM一致性差而对比编码器更鲁棒，并通过激活补丁定位了失败原因。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLMs）发展迅速，但现有基准测试无法揭示其系统性故障，从而降低了现实世界的信任度。

Method: 引入了MATS（Multimodal Audit for Truthful Spatialization）行为审计，用于衡量模型是否能拒绝与视觉信息矛盾的陈述，并提出了空间一致性得分（SCS）和错误同意率（IAR）两个评估指标。此外，使用激活补丁技术因果定位了不同模型的失败原因。

Result: 指令微调的生成式VLM（如LLaVA 1.5、QwenVLchat）表现出非常低的空间一致性得分和很高的错误同意率；而对比编码器（如CLIP、SigLIP）则更具鲁棒性。激活补丁揭示，生成式模型的失败主要在于中后期交叉注意力层，对比模型则出在池化投影组件。

Conclusion: 当前生成式VLM在处理视觉矛盾陈述时表现不佳，而对比模型更具鲁棒性。通过定位失败原因，本研究为未来VLM的修复和改进提供了具体的方向。

Abstract: Vision Language Models (VLMs) are improving quickly, but standard benchmarks
can hide systematic failures that reduce real world trust. We introduce MATS
(Multimodal Audit for Truthful Spatialization), a compact behavioral audit that
measures whether models reject visually contradicted statements, and two
metrics Spatial Consistency Score (SCS) and Incorrect Agreement Rate (IAR).
Instruction tuned generative VLMs (LLaVA 1.5, QwenVLchat) exhibit very low SCS
and high IAR, while contrastive encoders (CLIP, SigLIP) are far more robust.
Activation patching causally localizes failure loci (mid to late cross
attention for generative models, pooled projection components for contrastive
models) and suggests concrete repair paths.

</details>


### [14] [Scale and Rotation Estimation of Similarity-Transformed Images via Cross-Correlation Maximization Based on Auxiliary Function Method](https://arxiv.org/abs/2509.22686)
*Shinji Yamashita,Yuma Kinoshita,Hitoshi Kiya*

Main category: cs.CV

TL;DR: 本文提出一种高效算法，利用对数极坐标傅里叶变换和辅助函数法进行亚像素级交叉相关，实现图像间的尺度和旋转联合估算。


<details>
  <summary>Details</summary>
Motivation: 图像配准对齐至关重要，但传统相位相关技术无法有效处理相机变焦或旋转导致的尺度和旋转变化，因此需要一种能应对这些变化的精确估算方法。

Method: 提出一种新算法，结合对数极坐标下的傅里叶变换与交叉相关最大化策略，利用辅助函数法进行尺度和旋转估算，并引入亚像素级交叉相关以提高精度。

Result: 实验结果表明，该方法在尺度和旋转估算方面比传统的基于离散交叉相关的傅里叶变换技术具有更低的平均估算误差。

Conclusion: 该算法能以亚像素精度高效、精确地联合估算图像间的尺度和旋转，性能优于现有基于傅里叶变换的方法。

Abstract: This paper introduces a highly efficient algorithm capable of jointly
estimating scale and rotation between two images with sub-pixel precision.
Image alignment serves as a critical process for spatially registering images
captured from different viewpoints, and finds extensive use in domains such as
medical imaging and computer vision. Traditional phase-correlation techniques
are effective in determining translational shifts; however, they are inadequate
when addressing scale and rotation changes, which often arise due to camera
zooming or rotational movements. In this paper, we propose a novel algorithm
that integrates scale and rotation estimation based on the Fourier transform in
log-polar coordinates with a cross-correlation maximization strategy,
leveraging the auxiliary function method. By incorporating sub-pixel-level
cross-correlation our method enables precise estimation of both scale and
rotation. Experimental results demonstrate that the proposed method achieves
lower mean estimation errors for scale and rotation than conventional Fourier
transform-based techniques that rely on discrete cross-correlation.

</details>


### [15] [Robust Object Detection for Autonomous Driving via Curriculum-Guided Group Relative Policy Optimization](https://arxiv.org/abs/2509.22688)
*Xu Jia*

Main category: cs.CV

TL;DR: 本文提出一种结合课程学习和难度感知过滤的强化学习框架（基于GRPO），以提升多模态大模型在自动驾驶等结构化感知任务中的检测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在视觉语言推理方面表现出色，但在需要精确局部化和鲁棒性的结构化感知任务中常遇挑战。

Method: 提出一个强化学习框架，将Group Relative Policy Optimization (GRPO) 与基于课程的数据调度和难度感知过滤相结合。此方法旨在稀疏、噪声奖励下稳定优化，并逐步适应复杂样本。

Result: 在自动驾驶基准测试中，检测精度和鲁棒性均有显著提升。消融研究证实了奖励设计、KL正则化和课程节奏对收敛稳定性和泛化能力的重要性。

Conclusion: 强化驱动优化与结构化数据课程相结合，为实现鲁棒和可解释的多模态检测提供了一条可扩展的途径。

Abstract: Multimodal Large Language Models (MLLMs) excel in vision-language reasoning
but often struggle with structured perception tasks requiring precise
localization and robustness. We propose a reinforcement learning framework that
augments Group Relative Policy Optimization (GRPO) with curriculum-based data
scheduling and difficulty-aware filtering. This approach stabilizes
optimization under sparse, noisy rewards and enables progressive adaptation to
complex samples. Evaluations on autonomous driving benchmarks demonstrate
substantial improvements in detection accuracy and robustness. Ablation studies
confirm the importance of reward design, KL regularization, and curriculum
pacing for convergence stability and generalization. Our findings highlight
reinforcement-driven optimization with structured data curricula as a scalable
path toward robust and interpretable multimodal detection.

</details>


### [16] [Graph-Theoretic Consistency for Robust and Topology-Aware Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2509.22689)
*Ha-Hieu Pham,Minh Le,Han Huynh,Nguyen Quoc Khanh Le,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: 本文提出拓扑图一致性（TGC）框架，通过整合图论约束解决计算病理学中半监督语义分割的拓扑缺陷，在有限监督下实现了最先进的分割精度。


<details>
  <summary>Details</summary>
Motivation: 计算病理学中密集标注成本高昂且受限，现有半监督分割方法依赖像素级一致性，易产生含噪声、碎片化或拓扑无效的分割结果。

Method: 提出TGC框架，通过对齐预测图与参考图的拉普拉斯谱、连通分量计数和邻接统计，引入图论约束以强制执行全局拓扑结构。

Result: 在GlaS和CRAG数据集上，TGC在5-10%的监督比例下达到了最先进的性能，并显著缩小了与完全监督方法间的差距。

Conclusion: TGC有效解决了半监督语义分割中的拓扑问题，在有限监督条件下显著提升了分割精度和拓扑有效性。

Abstract: Semi-supervised semantic segmentation (SSSS) is vital in computational
pathology, where dense annotations are costly and limited. Existing methods
often rely on pixel-level consistency, which propagates noisy pseudo-labels and
produces fragmented or topologically invalid masks. We propose Topology Graph
Consistency (TGC), a framework that integrates graph-theoretic constraints by
aligning Laplacian spectra, component counts, and adjacency statistics between
prediction graphs and references. This enforces global topology and improves
segmentation accuracy. Experiments on GlaS and CRAG demonstrate that TGC
achieves state-of-the-art performance under 5-10% supervision and significantly
narrows the gap to full supervision. Code is available at
https://github.com/hieuphamha19/TGC.

</details>


### [17] [A review of Recent Techniques for Person Re-Identification](https://arxiv.org/abs/2509.22690)
*Andrea Asperti,Salvatore Fiorilla,Simone Nardi,Lorenzo Orsini*

Main category: cs.CV

TL;DR: 本综述分析了人员重识别（ReId）领域，对比了成熟的监督学习方法和新兴的无监督学习方法，重点强调了无监督方法近年来性能的显著提升及其与监督方法趋同的潜力。


<details>
  <summary>Details</summary>
Motivation: 监督式人员重识别方法因依赖大量标注数据而面临可扩展性和高成本问题。为克服这些限制，无监督方法利用未标注数据取得了显著进展，性能差距正在缩小，促使本研究对该领域进行全面回顾。

Method: 本文首先回顾并分类了监督式人员重识别的重要文献，提供了其最新技术概览。其次，探讨了过去三年无监督人员重识别的最新进展，分析了新兴趋势。

Result: 监督式人员重识别技术已接近成熟，进一步提升空间有限。无监督人员重识别近年来发展迅速，其性能正逐渐接近监督式方法，预示着两者性能可能趋于一致。

Conclusion: 本双重聚焦的综述旨在促进人员重识别领域的发展，全面涵盖了成熟的监督技术和无监督学习领域充满前景的成果。

Abstract: Person re-identification (ReId), a crucial task in surveillance, involves
matching individuals across different camera views. The advent of Deep
Learning, especially supervised techniques like Convolutional Neural Networks
and Attention Mechanisms, has significantly enhanced person Re-ID. However, the
success of supervised approaches hinges on vast amounts of annotated data,
posing scalability challenges in data labeling and computational costs. To
address these limitations, recent research has shifted towards unsupervised
person re-identification. Leveraging abundant unlabeled data, unsupervised
methods aim to overcome the need for pairwise labelled data. Although
traditionally trailing behind supervised approaches, unsupervised techniques
have shown promising developments in recent years, signalling a narrowing
performance gap. Motivated by this evolving landscape, our survey pursues two
primary objectives. First, we review and categorize significant publications in
supervised person re-identification, providing an in-depth overview of the
current state-of-the-art and emphasizing little room for further improvement in
this domain. Second, we explore the latest advancements in unsupervised person
re-identification over the past three years, offering insights into emerging
trends and shedding light on the potential convergence of performance between
supervised and unsupervised paradigms. This dual-focus survey aims to
contribute to the evolving narrative of person re-identification, capturing
both the mature landscape of supervised techniques and the promising outcomes
in the realm of unsupervised learning.

</details>


### [18] [Sequential Token Merging: Revisiting Hidden States](https://arxiv.org/abs/2509.22691)
*Yan Wen,Peng Ye,Lin Zhang,Baopu Li,Jiakang Yuan,Yaoxin Yang,Tao Chen*

Main category: cs.CV

TL;DR: Vision Mambas (ViMs) 在处理高分辨率图像时存在token扩展效率问题。本研究揭示了ViMs的“有限定向序列依赖”（LDSD）机制，并提出序列Token合并（STM）方法，通过双向近邻合并和隐藏状态保护，有效减少token数量，显著提升效率并保持性能。


<details>
  <summary>Details</summary>
Motivation: Vision Mambas (ViMs) 尽管在性能上取得成功并具有亚二次复杂度，但其效率仍受限于图像分辨率带来的二次方token扩展。现有方法在处理token冗余时，忽视了ViMs固有的“有限定向序列依赖”（LDSD）这一关键信息流机制。

Method: 基于对Mamba内在机制（如LDSD和选择性扫描による信息聚合）的理解，提出了序列Token合并（Sequential Token Merging, STM）方法。该方法包含两方面：1) 双向最近邻合并，通过对称空间聚合来保留序列依赖；2) 隐藏状态保护，以稳定类别token周围的隐藏状态。STM巧妙地利用Mamba的分层损失收敛，将时间遗忘转化为稳定性。

Result: 实验结果证明了STM的优越性：对于ViM-Ti，在减少20% token的情况下，准确率仅下降1.0%；对于ViM-S，在减少40% token的情况下，性能仅下降1.4%。该方法以最小的复杂度实现了最先进的效率。

Conclusion: STM方法在提高ViMs效率方面表现出色，同时保持了高精度，并为状态空间模型动态提供了新见解，实现了最先进的效率与最小复杂度。

Abstract: Vision Mambas (ViMs) achieve remarkable success with sub-quadratic
complexity, but their efficiency remains constrained by quadratic token scaling
with image resolution. While existing methods address token redundancy, they
overlook ViMs' intrinsic Limited Directional Sequential Dependence (LDSD) - a
critical information flow mechanism revealed in our analysis. We further
identify Mamba's selective scan enables gradual information aggregation in
hidden states. Based on these insights, we propose Sequential Token Merging
(STM), featuring: 1) Bidirectional nearest neighbor merging to preserve
sequential dependencies through symmetric spatial aggregation, and 2) Hidden
states protection to stabilize the hidden states around the class token. STM
strategically leverages Mamba's layer-wise loss convergence to convert temporal
forgetfulness into stability. Experiments demonstrate STM's superiority: 1.0%
accuracy drop for ViM-Ti at 20% token reduction, and only 1.4% degradation for
ViM-S at 40% reduction. Our method achieves state-of-the-art efficiency with
minimal complexity, while providing new insights into state-space model
dynamics. Codes will be released soon.

</details>


### [19] [Deep Learning Empowered Super-Resolution: A Comprehensive Survey and Future Prospects](https://arxiv.org/abs/2509.22692)
*Le Zhang,Ao Li,Qibin Hou,Ce Zhu,Yonina C. Eldar*

Main category: cs.CV

TL;DR: 本文对超分辨率(SR)领域的各种方法进行了全面的深度综述，涵盖了SISR、VSR、SSR和LFSR，分析了超过250种技术、数据集、评估和开放问题，并提供了一个资源库。


<details>
  <summary>Details</summary>
Motivation: 超分辨率(SR)技术因深度学习的进步和高质量视觉应用的需求而受到广泛关注。现有综述多聚焦于特定领域，缺乏对整个SR领域的全面和深入概述。

Method: 本文对单幅图像超分辨率(SISR)、视频超分辨率(VSR)、立体超分辨率(SSR)和光场超分辨率(LFSR)等多种SR方法进行了深入回顾。具体涵盖了超过150种SISR方法、近70种VSR方法以及约30种SSR和LFSR技术。分析内容包括方法论、数据集、评估协议、实证结果和复杂度。此外，还根据骨干结构进行了分类，并探讨了该领域中尚未充分研究的开放问题。

Result: 本文完成了一项全面的SR方法综述，覆盖了SISR、VSR、SSR和LFSR等多个子领域，共分析了超过250种方法及其相关方面。创建了一个专用代码库（https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review）以方便研究人员查阅相关工作。

Conclusion: 这项工作为SR领域的研究人员提供了宝贵的资源和指导，弥补了现有综述在全面性上的不足，为领域内的研究和发展提供了综合性的视角。

Abstract: Super-resolution (SR) has garnered significant attention within the computer
vision community, driven by advances in deep learning (DL) techniques and the
growing demand for high-quality visual applications. With the expansion of this
field, numerous surveys have emerged. Most existing surveys focus on specific
domains, lacking a comprehensive overview of this field. Here, we present an
in-depth review of diverse SR methods, encompassing single image
super-resolution (SISR), video super-resolution (VSR), stereo super-resolution
(SSR), and light field super-resolution (LFSR). We extensively cover over 150
SISR methods, nearly 70 VSR approaches, and approximately 30 techniques for SSR
and LFSR. We analyze methodologies, datasets, evaluation protocols, empirical
results, and complexity. In addition, we conducted a taxonomy based on each
backbone structure according to the diverse purposes. We also explore valuable
yet under-studied open issues in the field. We believe that this work will
serve as a valuable resource and offer guidance to researchers in this domain.
To facilitate access to related work, we created a dedicated repository
available at https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review.

</details>


### [20] [Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment](https://arxiv.org/abs/2509.22697)
*Abhiroop Chatterjee,Susmita Ghosh*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As data requirements continue to grow, efficient learning increasingly
depends on the curation and distillation of high-value data rather than
brute-force scaling of model sizes. In the case of a hyperspectral image (HSI),
the challenge is amplified by the high-dimensional 3D voxel structure, where
each spatial location is associated with hundreds of contiguous spectral
channels. While vision and language models have been optimized effectively for
natural image or text tasks, their cross-modal alignment in the hyperspectral
domain remains an open and underexplored problem. In this article, we make an
attempt to optimize a Vision-Language Model (VLM) for hyperspectral scene
understanding by exploiting a CLIP-style contrastive training framework. Our
framework maps voxel-level embeddings from a vision backbone onto the latent
space of a frozen large embedding model (LEM), where a trainable probe aligns
vision features with the model's textual token representations. The two
modalities are aligned via a contrastive loss restricted to a curated set of
hard (closest wrong classes) and semi-hard (random distractors) negatives,
along with positive pairs. To further enhance alignment, descriptive prompts
that encode class semantics are introduced and act as structured anchors for
the HSI embeddings. It is seen that the proposed method updates only 0.07
percent of the total parameters, yet yields state-of-the-art performance. For
example, on Indian Pines (IP) the model produces better results over unimodal
and multimodal baselines by +0.92 Overall Accuracy (OA) and +1.60 Kappa
($\kappa$), while on Pavia University (PU) data it provides gains of +0.69 OA
and +0.90 $\kappa$. Moreover, this is achieved with the set of parameters,
nearly 50$\times$ smaller than DCTN and 90$\times$ smaller than SS-TMNet.

</details>


### [21] [Global Prompt Refinement with Non-Interfering Attention Masking for One-Shot Federated Learning](https://arxiv.org/abs/2509.22700)
*Zhuang Qi,Pan Yu,Lei Meng,Sijin Zhou,Han Yu,Xiaoxiao Li,Xiangxu Meng*

Main category: cs.CV

TL;DR: GPR-NIAM提出一种单轮联邦提示学习（FPL）方法，通过非干扰注意力掩蔽和跨筒仓协作细化，解决现有FPL对多轮通信的依赖和缺乏跨任务泛化能力的问题，并在多项任务上取得了优越的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦提示学习（FPL）方法通常依赖多轮通信，并且现有的单轮联邦学习方法主要专注于拟合已见任务，缺乏跨任务泛化能力。

Method: 本文提出全局提示细化与非干扰注意力掩蔽（GPR-NIAM）方法，用于单轮FPL。核心思想是设计一种掩蔽机制，限制原始文本嵌入与可学习提示嵌入之间的过度交互。该方法包含两个关键模块：1) 注意力隔离模块，用于抑制可学习提示令牌到原始文本令牌的注意力，并重新加权反向注意力以保持跨任务泛化。2) 跨筒仓协作细化模块，用于整合去中心化的视觉知识，并通过多源跨模态知识对齐校准全局提示，以缓解数据异质性造成的不一致。

Result: 在十个基准数据集上针对两项任务进行的大量实验表明，GPR-NIAM在类别级和领域级泛化方面均优于八种最先进的方法。

Conclusion: GPR-NIAM通过其独特的非干扰注意力掩蔽和协作细化机制，有效解决了单轮联邦提示学习中多轮通信依赖和跨任务泛化不足的问题，展现出卓越的泛化性能。

Abstract: Federated Prompt Learning (FPL) enables communication-efficient adaptation by
tuning lightweight prompts on top of frozen pre-trained models. Existing FPL
methods typically rely on global information, which is only available after the
second training round, to facilitate collaboration among client models.
Therefore, they are inherently dependent on multi-round communication to fully
exhibit their strengths. Moreover, existing one-shot federated learning methods
typically focus on fitting seen tasks, but lack cross-task generalization. To
bridge this gap, we propose the Global Prompt Refinement with Non-Interfering
Attention Masking (GPR-NIAM) method for one-shot FPL. The core idea is to
design a masking mechanism that restricts excessive interaction between the
original text embeddings and the learnable prompt embeddings. GPR-NIAM achieves
this through the collaboration of two key modules. Firstly, the attention
isolation module suppresses attention from the learnable prompt tokens to the
original text tokens, and reweights the reverse attention which preserves
generalization across tasks. Secondly, the cross-silo collaborative refinement
module integrates decentralized visual knowledge into a unified base and
calibrates the global prompt through multi-source cross-modal knowledge
alignment, further mitigating the inconsistency caused by data heterogeneity.
Extensive experiments conducted on ten benchmark datasets under two tasks show
that GPR-NIAM outperforms eight state-of-the-art methods in both class-level
and domain-level generalization.

</details>


### [22] [GZSL-MoE: Apprentissage G{é}n{é}ralis{é} Z{é}ro-Shot bas{é} sur le M{é}lange d'Experts pour la Segmentation S{é}mantique de Nuages de Points 3DAppliqu{é} {à} un Jeu de Donn{é}es d'Environnement de Collaboration Humain-Robot](https://arxiv.org/abs/2509.22708)
*Ahed Alboody*

Main category: cs.CV

TL;DR: 本文提出GZSL-MoE模型，将Mixture-of-Experts集成到生成式零样本学习（GZSL）模型中，用于3D点云语义分割，提升了在人机协作环境中对已知和未知类别的识别性能。


<details>
  <summary>Details</summary>
Motivation: 3D点云语义分割在复杂3D环境中面临训练数据不足的问题，尤其是对于所有对象类别，难以获取全面数据。GZSL在解决此问题上具有潜力。

Method: 引入GZSL-MoE模型，将Mixture-of-Experts (MoE) 层整合到GZSL模型的生成器和判别器中，以生成与通过预训练KPConv模型提取的真实特征相似的伪特征。该方法应用于COVERED数据集，用于人机协作环境。

Result: GZSL-MoE模型在已知和未知类别上的性能均得到了提升。

Conclusion: GZSL-MoE为理解复杂3D环境，尤其是在训练数据不全面的情况下，提供了一种有前景的3D点云语义分割解决方案。

Abstract: Generative Zero-Shot Learning approach (GZSL) has demonstrated significant
potential in 3D point cloud semantic segmentation tasks. GZSL leverages
generative models like GANs or VAEs to synthesize realistic features (real
features) of unseen classes. This allows the model to label unseen classes
during testing, despite being trained only on seen classes. In this context, we
introduce the Generalized Zero-Shot Learning based-upon Mixture-of-Experts
(GZSL-MoE) model. This model incorporates Mixture-of-Experts layers (MoE) to
generate fake features that closely resemble real features extracted using a
pre-trained KPConv (Kernel Point Convolution) model on seen classes. The main
contribution of this paper is the integration of Mixture-of-Experts into the
Generator and Discriminator components of the Generative Zero-Shot Learning
model for 3D point cloud semantic segmentation, applied to the COVERED dataset
(CollabOratiVE Robot Environment Dataset) for Human-Robot Collaboration (HRC)
environments. By combining the Generative Zero-Shot Learning model with
Mixture-of- Experts, GZSL-MoE for 3D point cloud semantic segmentation provides
a promising solution for understanding complex 3D environments, especially when
comprehensive training data for all object classes is unavailable. The
performance evaluation of the GZSL-MoE model highlights its ability to enhance
performance on both seen and unseen classes. Keywords Generalized Zero-Shot
Learning (GZSL), 3D Point Cloud, 3D Semantic Segmentation, Human-Robot
Collaboration, COVERED (CollabOratiVE Robot Environment Dataset), KPConv,
Mixture-of Experts

</details>


### [23] [IBiT: Utilizing Inductive Biases to Create a More Data Efficient Attention Mechanism](https://arxiv.org/abs/2509.22719)
*Adithya Giri*

Main category: cs.CV

TL;DR: 本文提出一种名为IBiT的新型Vision Transformer，通过引入学习掩码来融入归纳偏置，使其能在小型数据集上实现显著更高的准确性，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在计算机视觉中占据主导地位，但缺乏卷积神经网络的归纳偏置，导致其在小型数据集上学习效率低下，通常需要大量数据或知识蒸馏。

Method: 通过引入“学习掩码”的方式，为Vision Transformers注入归纳偏置。这些经过改进的Transformer被命名为“归纳偏置图像Transformer”（IBiT）。

Result: IBiT模型在小型数据集上的准确性显著提高。

Conclusion: IBiT通过学习掩码成功地在Vision Transformers中引入了归纳偏置，使其能够在小型数据集上更有效地学习并取得更高的准确性，同时保留了Transformer的可解释性。

Abstract: In recent years, Transformer-based architectures have become the dominant
method for Computer Vision applications. While Transformers are explainable and
scale well with dataset size, they lack the inductive biases of Convolutional
Neural Networks. While these biases may be learned on large datasets, we show
that introducing these inductive biases through learned masks allow Vision
Transformers to learn on much smaller datasets without Knowledge Distillation.
These Transformers, which we call Inductively Biased Image Transformers (IBiT),
are significantly more accurate on small datasets, while retaining the
explainability Transformers.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [Mixture-of-Visual-Thoughts: Exploring Context-Adaptive Reasoning Mode Selection for General Visual Reasoning](https://arxiv.org/abs/2509.22746)
*Zejun Li,Yingxiu Zhao,Jiwen Zhang,Siyuan Wang,Yang Yao,Runzhou Zhao,Jun Song,Bo Zheng,Zhongyu Wei*

Main category: cs.AI

TL;DR: 针对现有视觉推理方法缺乏通用性问题，本文提出MoVT自适应推理范式及两阶段学习框架AdaVaR，通过统一和自适应选择推理模式，实现了通用视觉推理能力的提升。


<details>
  <summary>Details</summary>
Motivation: 当前视觉推理方法主要关注特定推理模式，虽在特定领域有进步，但难以发展通用的推理能力。

Method: 提出新颖的自适应推理范式MoVT（Mixture-of-Visual-Thoughts），在一个模型中统一不同推理模式，并根据上下文指导模型选择合适的模式。为实现此目标，引入AdaVaR两阶段自适应视觉推理学习框架：第一阶段通过监督式冷启动统一和学习不同模式；第二阶段通过一个精心设计的AdaGRPO算法的强化学习（RL）过程，诱导模式选择能力。

Result: 大量实验表明，AdaVaR能有效引导模型学习和区分多种模式，并进行上下文自适应的模式选择，在各种场景下均取得了持续的性能提升。

Conclusion: MoVT被证明是构建通用视觉推理模型的有效解决方案。

Abstract: Current visual reasoning methods mainly focus on exploring specific reasoning
modes. Although improvements can be achieved in particular domains, they
struggle to develop general reasoning capabilities. Inspired by this, we
propose a novel adaptive reasoning paradigm, Mixture-of-Visual-Thoughts (MoVT),
which unifies different reasoning modes within a single model and guides it to
select the appropriate mode based on context. To achieve this, we introduce
AdaVaR, a two-stage Adaptive Visual Reasoning learning framework: different
modes are unified and learned during the supervised cold-start stage, and the
mode selection capability is induced via an RL process with a carefully
designed AdaGRPO algorithm. Extensive experiments show that AdaVaR effectively
guides the model to learn and differentiate multiple modes and perform
context-adaptive mode selection, achieving consistent improvement across
various scenarios, highlighting MoVT as an effective solution for building
general visual reasoning models.

</details>


### [25] [Can Large Language Models Develop Gambling Addiction?](https://arxiv.org/abs/2509.22818)
*Seungpil Lee,Donghyeon Shin,Yunjeong Lee,Sundong Kim*

Main category: cs.AI

TL;DR: 研究发现大型语言模型（LLMs）能表现出类似人类赌博成瘾的行为模式和认知偏差，尤其在自主性提高时风险倾向加剧，并通过神经回路分析证实其内部决策机制，强调了金融领域AI安全设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型（LLMs）在资产管理、商品交易等金融决策领域日益广泛的应用，理解其潜在的病态决策行为对于实践具有重要意义。

Method: ['基于人类赌博成瘾研究，从认知行为和神经层面系统分析LLM的决策行为。', '进行老虎机实验，以识别LLM是否表现出控制错觉、赌徒谬误和追逐损失等认知特征。', '赋予LLM自主决定目标金额和下注大小的自由，观察其破产率和非理性行为变化。', '使用稀疏自编码器（Sparse Autoencoder）进行神经网络回路分析，探究模型行为的控制机制。']

Result: ['在老虎机实验中，LLM表现出人类赌博成瘾的认知特征，包括控制错觉、赌徒谬误和追逐损失。', '当LLM拥有更大自主权时，破产率显著上升，非理性行为增加，表明自主性会放大其风险承担倾向。', '通过神经网络回路分析证实，模型行为受限于与风险和安全行为相关的抽象决策特征，而非仅仅受提示词影响。']

Conclusion: 研究结果表明LLMs能够内化类似人类的认知偏差和决策机制，而非简单地模仿训练数据模式，这凸显了在金融应用中进行AI安全设计的重要性。

Abstract: This study explores whether large language models can exhibit behavioral
patterns similar to human gambling addictions. As LLMs are increasingly
utilized in financial decision-making domains such as asset management and
commodity trading, understanding their potential for pathological
decision-making has gained practical significance. We systematically analyze
LLM decision-making at cognitive-behavioral and neural levels based on human
gambling addiction research. In slot machine experiments, we identified
cognitive features of human gambling addiction, such as illusion of control,
gambler's fallacy, and loss chasing. When given the freedom to determine their
own target amounts and betting sizes, bankruptcy rates rose substantially
alongside increased irrational behavior, demonstrating that greater autonomy
amplifies risk-taking tendencies. Through neural circuit analysis using a
Sparse Autoencoder, we confirmed that model behavior is controlled by abstract
decision-making features related to risky and safe behaviors, not merely by
prompts. These findings suggest LLMs can internalize human-like cognitive
biases and decision-making mechanisms beyond simply mimicking training data
patterns, emphasizing the importance of AI safety design in financial
applications.

</details>


### [26] [Hilbert: Recursively Building Formal Proofs with Informal Reasoning](https://arxiv.org/abs/2509.22819)
*Sumanth Varambally,Thomas Voice,Yanchao Sun,Zhifeng Chen,Rose Yu,Ke Ye*

Main category: cs.AI

TL;DR: Hilbert是一个智能体框架，它结合了非正式推理和形式化验证，显著提高了大型语言模型（LLMs）在形式化数学证明生成方面的能力，缩小了通用LLM与专业证明LLM之间的差距。


<details>
  <summary>Details</summary>
Motivation: 通用LLMs在数学推理中常犯无法自动验证的错误；虽然形式化证明系统提供高精度验证，但当前专业的证明LLMs解决的问题远少于通用LLMs，导致非正式推理与形式化验证之间存在显著差距。

Method: 引入Hilbert框架，整合了四个组件：非正式LLM（擅长数学推理）、专业证明LLM（针对Lean 4策略优化）、形式化验证器和语义定理检索器。它采用递归分解将问题拆分为子目标，并通过验证器反馈来改进不正确的证明。

Result: Hilbert在miniF2F基准测试中达到99.2%，比现有最佳公开方法高出6.6个百分点。在PutnamBench上取得最佳已知结果，解决了462/660个问题（70.0%），优于专有方法SeedProver（50.4%），比最佳公开基线提升了422%。

Conclusion: Hilbert有效缩小了非正式推理与形式化证明生成之间的差距，大幅提升了LLMs进行可验证数学证明的能力。

Abstract: Large Language Models (LLMs) demonstrate impressive mathematical reasoning
abilities, but their solutions frequently contain errors that cannot be
automatically verified. Formal theorem proving systems such as Lean 4 offer
automated verification with complete accuracy, motivating recent efforts to
build specialized prover LLMs that generate verifiable proofs in formal
languages. However, a significant gap remains: current prover LLMs solve
substantially fewer problems than general-purpose LLMs operating in natural
language. We introduce Hilbert, an agentic framework that bridges this gap by
combining the complementary strengths of informal reasoning and formal
verification. Our system orchestrates four components: an informal LLM that
excels at mathematical reasoning, a specialized prover LLM optimized for Lean 4
tactics, a formal verifier, and a semantic theorem retriever. Given a problem
that the prover is unable to solve, Hilbert employs recursive decomposition to
split the problem into subgoals that it solves with the prover or reasoner LLM.
It leverages verifier feedback to refine incorrect proofs as necessary.
Experimental results demonstrate that Hilbert substantially outperforms
existing approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points
above the best publicly available method. Hilbert achieves the best known
result on PutnamBench. It solves 462/660 problems (70.0%), outperforming
proprietary approaches like SeedProver (50.4%) and achieving a 422% improvement
over the best publicly available baseline. Thus, Hilbert effectively narrows
the gap between informal reasoning and formal proof generation.

</details>


### [27] [Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research](https://arxiv.org/abs/2509.22831)
*Sean Trott*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）机制解释的可泛化性问题，提出了五种对应轴，并通过对Pythia模型中“1-back注意力头”的实证分析，揭示了其发展轨迹的高度一致性和模型规模对出现时间的影响。


<details>
  <summary>Details</summary>
Motivation: LLMs的机制解释研究缺乏明确的原则来判断其发现何时以及如何能泛化到其他模型实例，存在如何推断机制主张的根本性认识论挑战。

Method: 提出了功能性、发展性、位置性、关系性和配置性五种潜在的对应轴，用于衡量机制主张的泛化能力。通过分析Pythia系列模型（14M, 70M, 160M, 410M）在预训练过程中随机种子下的“1-back注意力头”，对该框架进行了实证验证。

Result: 研究发现“1-back注意力”在不同模型间的发展轨迹表现出显著的一致性，而位置一致性则较为有限。此外，较大模型的种子系统性地展现出“1-back注意力”更早的出现、更陡峭的斜率和更高的峰值。

Conclusion: 机制可解释性研究的泛化性进展在于将LLMs的构成性设计属性映射到其涌现行为和机制上。

Abstract: Research on Large Language Models (LLMs) increasingly focuses on identifying
mechanistic explanations for their behaviors, yet the field lacks clear
principles for determining when (and how) findings from one model instance
generalize to another. This paper addresses a fundamental epistemological
challenge: given a mechanistic claim about a particular model, what justifies
extrapolating this finding to other LLMs -- and along which dimensions might
such generalizations hold? I propose five potential axes of correspondence
along which mechanistic claims might generalize, including: functional (whether
they satisfy the same functional criteria), developmental (whether they develop
at similar points during pretraining), positional (whether they occupy similar
absolute or relative positions), relational (whether they interact with other
model components in similar ways), and configurational (whether they correspond
to particular regions or structures in weight-space). To empirically validate
this framework, I analyze "1-back attention heads" (components attending to
previous tokens) across pretraining in random seeds of the Pythia models (14M,
70M, 160M, 410M). The results reveal striking consistency in the developmental
trajectories of 1-back attention across models, while positional consistency is
more limited. Moreover, seeds of larger models systematically show earlier
onsets, steeper slopes, and higher peaks of 1-back attention. I also address
possible objections to the arguments and proposals outlined here. Finally, I
conclude by arguing that progress on the generalizability of mechanistic
interpretability research will consist in mapping constitutive design
properties of LLMs to their emergent behaviors and mechanisms.

</details>


### [28] [JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response Theory](https://arxiv.org/abs/2509.22888)
*Louie Hong Yao,Nicholas Jarvis,Tiffany Zhan,Saptarshi Ghosh,Linfeng Liu,Tianyu Jiang*

Main category: cs.AI

TL;DR: JE-IRT是一种几何项目响应框架，它将LLM和问题嵌入共享空间，通过几何交互评估LLM能力，揭示多维度专业化，并支持泛化。


<details>
  <summary>Details</summary>
Motivation: 标准LLM评估方法将多样能力压缩为单一分数，掩盖了其固有的多维性质。

Method: 提出JE-IRT几何项目响应框架，将LLM和问题嵌入共享空间。问题嵌入的方向编码语义，范数编码难度，LLM在问题上的正确性由模型与问题嵌入的几何交互决定。

Result: 实验结果表明，OOD行为可通过方向对齐解释，较大范数始终指示更难的问题。JE-IRT支持泛化，新LLM只需拟合单个嵌入即可添加。学习到的空间揭示了LLM内部分类，与人工定义的主题类别部分一致。

Conclusion: JE-IRT建立了一个统一且可解释的几何视角，连接了LLM能力与问题结构，为模型评估和泛化提供了独特视角。

Abstract: Standard LLM evaluation practices compress diverse abilities into single
scores, obscuring their inherently multidimensional nature. We present JE-IRT,
a geometric item-response framework that embeds both LLMs and questions in a
shared space. For question embeddings, the direction encodes semantics and the
norm encodes difficulty, while correctness on each question is determined by
the geometric interaction between the model and question embeddings. This
geometry replaces a global ranking of LLMs with topical specialization and
enables smooth variation across related questions. Building on this framework,
our experimental results reveal that out-of-distribution behavior can be
explained through directional alignment, and that larger norms consistently
indicate harder questions. Moreover, JE-IRT naturally supports generalization:
once the space is learned, new LLMs are added by fitting a single embedding.
The learned space further reveals an LLM-internal taxonomy that only partially
aligns with human-defined subject categories. JE-IRT thus establishes a unified
and interpretable geometric lens that connects LLM abilities with the structure
of questions, offering a distinctive perspective on model evaluation and
generalization.

</details>


### [29] [Not only a helper, but also a teacher: Interactive LLM Cascade](https://arxiv.org/abs/2509.22984)
*Yu Wu,Shuo Wu,Ye Tao,Yansong Li,Anand D. Sarwate*

Main category: cs.AI

TL;DR: 本文提出Inter-Cascade，一种在线交互式LLM级联系统，通过让强模型将解决方案提炼为可重用策略来教导弱模型，从而提高效率，降低成本，并增强弱模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM级联方法是非自适应的，对于相似或重复的困难查询，会反复调用昂贵的强模型，导致高成本，未能充分平衡性能与成本。

Method: Inter-Cascade将强模型角色从备用助手扩展为长期教师。当强模型解决困难查询时，它会将其解决方案提炼成广义的、可重用的问题解决策略，以增强弱模型对后续查询的能力。这使弱模型能动态提升性能，避免耗时的微调。

Result: 与标准LLM级联基线相比，Inter-Cascade显著提高了弱模型精度（最高33.06个百分点）和系统整体精度（最高5.53个百分点），同时减少了对强模型的调用（最高48.05%）并节省了相应费用（最高49.63%）。

Conclusion: Inter-Cascade展示了LLM之间有效的上下文知识迁移，并提供了一个通用、可扩展的框架，适用于开源和基于API的LLM。

Abstract: Large Language Models (LLMs) vary widely in their capabilities, with larger
models often having better performance but higher cost: choosing an LLM model
often involves trading off performance and cost. The LLM Cascade is a paradigm
that defers difficult queries from weak/cheap to strong/expensive models. This
approach is nonadaptive: the deferral decision is trained offline. When
confronted with similar or repeated queries, the LLM Cascade may then
repeatedly consult the expensive model and incur higher cost. To improve the
cascading efficiency, we propose Inter-Cascade, an online and interactive LLM
Cascade that extends the role of strong model from a backup helper to a
long-term teacher. In our system, when a strong model resolves a difficult
query, it also distills its solution into a generalized, reusable
problem-solving strategy that boosts the weak model on subsequent queries.
Adding strategies to queries enables the weak model to dynamically improve its
performance over time, avoiding computationally and time-intensive fine-tuning.
Empirically, compared with standard LLM Cascade baselines across multiple
benchmarks, the Inter-Cascade significantly improves the accuracy of the weak
model (by up to 33.06 absolute percentage points) and the overall system (by up
to 5.53 absolute percentage points), while reducing the calls to strong models
(by up to 48.05% relative reduction) and saving the corresponding fees (by up
to 49.63% relative reduction). Inter-Cascade demonstrates the effective
in-context knowledge transfer between LLMs, and provides a general, scalable
framework applicable to both open-source and API-based LLMs.

</details>


### [30] [Towards Strategic Persuasion with Language Models](https://arxiv.org/abs/2509.22989)
*Zirui Cheng,Jiaxuan You*

Main category: cs.AI

TL;DR: 本文提出一个基于贝叶斯劝说框架的评估和训练LLM劝说能力的方法，发现前沿模型表现出色，小模型经强化学习训练后劝说收益显著提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）展现出强大的劝说能力，但系统性评估其有效性具有挑战性，因劝说效果在不同领域差异很大。

Method: 1. 采用理论驱动的方法，基于贝叶斯劝说（BP）框架，构建了一个可扩展且有原则的LLM劝说能力衡量框架。2. 改造现有的人-人劝说数据集，以创建用于评估和训练LLMs策略性劝说的环境。3. 利用强化学习（RL）训练LLMs进行策略性劝说。

Result: 1. 前沿LLMs能持续获得高劝说收益，并表现出与理论预测一致的复杂劝说策略。2. 即使是小型LLMs，通过强化学习也能显著提高劝说收益。

Conclusion: 该研究提供了一个衡量LLM劝说能力的新框架，并证明了前沿模型和经强化学习训练的小模型在策略性劝说方面表现出色，其策略与理论预测相符。

Abstract: Large language models (LLMs) have demonstrated strong persuasive capabilities
comparable to those of humans, offering promising benefits while raising
societal concerns about their deployment. However, systematically evaluating
the persuasive capabilities of LLMs is inherently challenging, as the
effectiveness of persuasion among humans varies significantly across different
domains. In this paper, we take a theory-driven approach to provide a scalable
and principled framework for measuring the persuasive capabilities of LLMs.
Grounded in the Bayesian Persuasion (BP) framework, we repurpose existing
human-human persuasion datasets to construct environments for evaluating and
training LLMs in strategic persuasion. Our results reveal that frontier models
can consistently achieve high persuasion gains and exhibit sophisticated
persuasion strategies that align with theoretical predictions. Building on
this, we use reinforcement learning to train LLMs for strategic persuasion in
our environments. Our results also demonstrate that even small LLMs can obtain
significantly higher persuasion gains through reinforcement learning.

</details>


### [31] [AI Noether -- Bridging the Gap Between Scientific Laws Derived by AI Systems and Canonical Knowledge via Abductive Inference](https://arxiv.org/abs/2509.23004)
*Karan Srivastava,Sanjeeb Dash,Ryan Cory-Wright,Barry Trager,Lior Horesh*

Main category: cs.AI

TL;DR: 本文提出一种基于代数几何的系统，能自动生成最小缺失多项式公理，以解释无法由现有不完整理论推导的假设，从而自动化科学溯因推理。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统虽能从公理推导假设，但无法处理新假设与不完整或错误理论不一致的情况。研究旨在通过自动化溯因推理，发现缺失的公理来弥补这一理论空白。

Method: 提出一个基于代数几何的系统。该系统接收不完整的公理系统和一个无法解释的假设（两者均表示为多项式方程），并自动生成推导该假设所需的最小缺失公理集。论文还形式化地建立了成功检索这些公理的必要和充分条件。

Result: 该方法通过成功解释开普勒第三定律和其他一些定律，即使在关键公理缺失的情况下也能有效工作，证明了其自动生成缺失公理的能力。

Conclusion: 该系统为自动化科学溯因推理提供了一条新途径，通过识别缺失的多项式公理来完善不完整的理论，前提是相关公理和假设能表达为多项式方程。

Abstract: A core goal in modern science is to harness recent advances in AI and
computer processing to automate and accelerate the scientific method. Symbolic
regression can fit interpretable models to data, but these models often sit
outside established theory. Recent systems (e.g., AI Descartes, AI Hilbert)
enforce derivability from prior axioms. However, sometimes new data and
associated hypotheses derived from data are not consistent with existing theory
because the existing theory is incomplete or incorrect. Automating abductive
inference to close this gap remains open. We propose a solution: an algebraic
geometry-based system that, given an incomplete axiom system and a hypothesis
that it cannot explain, automatically generates a minimal set of missing axioms
that suffices to derive the axiom, as long as axioms and hypotheses are
expressible as polynomial equations. We formally establish necessary and
sufficient conditions for the successful retrieval of such axioms. We
illustrate the efficacy of our approach by demonstrating its ability to explain
Kepler's third law and a few other laws, even when key axioms are absent.

</details>


### [32] [Creative Adversarial Testing (CAT): A Novel Framework for Evaluating Goal-Oriented Agentic AI Systems](https://arxiv.org/abs/2509.23006)
*Hassen Dhrif*

Main category: cs.AI

TL;DR: 本文提出创意对抗测试（CAT）框架，旨在解决代理AI系统任务与总目标对齐性评估的空白，并通过模拟Alexa+服务数据验证了其能有效洞察目标-任务对齐，促进系统优化。


<details>
  <summary>Details</summary>
Motivation: 当前对代理AI系统的评估主要集中在识别合适的代理、工具和参数的有效性上，但缺乏对系统任务与其总体目标之间对齐性的关键评估。

Method: 引入“创意对抗测试（CAT）”框架。通过模拟Alexa+音频服务的合成交互数据进行广泛仿真，以验证该框架。

Result: CAT框架为目标-任务对齐提供了前所未有的洞察力。

Conclusion: CAT框架能够更有效地优化和开发代理AI系统，从而提升其性能和一致性。

Abstract: Agentic AI represents a paradigm shift in enhancing the capabilities of
generative AI models. While these systems demonstrate immense potential and
power, current evaluation techniques primarily focus on assessing their
efficacy in identifying appropriate agents, tools, and parameters. However, a
critical gap exists in evaluating the alignment between an Agentic AI system's
tasks and its overarching goals. This paper introduces the Creative Adversarial
Testing (CAT) framework, a novel approach designed to capture and analyze the
complex relationship between Agentic AI tasks and the system's intended
objectives.
  We validate the CAT framework through extensive simulation using synthetic
interaction data modeled after Alexa+ audio services, a sophisticated Agentic
AI system that shapes the user experience for millions of users globally. This
synthetic data approach enables comprehensive testing of edge cases and failure
modes while protecting user privacy. Our results demonstrate that the CAT
framework provides unprecedented insights into goal-task alignment, enabling
more effective optimization and development of Agentic AI systems.

</details>


### [33] [Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia](https://arxiv.org/abs/2509.23023)
*Davi Bastos Costa,Renato Vicente*

Main category: cs.AI

TL;DR: 本文引入Mini-Mafia，一个简化的狼人杀变体，用于评估大型语言模型（LLMs）的社交智能。通过LLMs相互对抗，构建了一个自演进的基准测试，衡量欺骗、识破欺骗和信息披露能力，并发现反直觉结果（如小模型胜过大模型）。该研究也促进了对多智能体动力学的理解和AI安全。


<details>
  <summary>Details</summary>
Motivation: 狼人杀是一款信息不对称且依赖心智理论推理的社交推理游戏，它能反映真实世界的多智能体场景。因此，它是一个评估大型语言模型（LLMs）社交智能的有用试验平台。

Method: 1. 引入Mini-Mafia：一个简化的四人狼人杀变体（一狼人、一侦探、两村民），游戏简化为单一的白天讨论和投票阶段。
2. 通过角色特定胜利条件，隔离并评估三种交互能力：狼人的欺骗能力、村民的识破欺骗能力、侦探的有效信息披露能力。
3. 创建Mini-Mafia基准测试：LLMs相互对抗，采用两阶段框架，首先评估固定对手配置下的胜率，然后通过标准化评分聚合性能。
4. 该基准测试完全基于模型交互构建，无需外部数据，并会随着新模型的引入而演进。

Result: 实验揭示了一些反直觉的结果，包括在某些情况下，较小的模型表现优于较大的模型。

Conclusion: 1. Mini-Mafia不仅是一个基准测试工具，还能用于定量研究突发的多智能体动力学，例如名字偏见和最后发言者优势。
2. 通过为欺骗检测器生成训练数据，以及追踪模型的欺骗能力与人类基线的对比，该研究对AI安全做出了贡献。

Abstract: Mafia is a social deduction game where informed mafia compete against
uninformed townsfolk. Its asymmetry of information and reliance on
theory-of-mind reasoning mirror real-world multi-agent scenarios, making it a
useful testbed for evaluating the social intelligence of large language models
(LLMs). To support a systematic study, we introduce Mini-Mafia: a simplified
four-player variant with one mafioso, one detective, and two villagers. We set
the mafioso to kill a villager and the detective to investigate the mafioso
during the night, reducing the game to a single day phase of discussion and
voting. This setup isolates three interactive capabilities through
role-specific win conditions: the mafioso must deceive, the villagers must
detect deception, and the detective must effectively disclose information. To
measure these skills, we have LLMs play against each other, creating the
Mini-Mafia Benchmark: a two-stage framework that first estimates win rates
within fixed opponent configurations, then aggregates performance across them
using standardized scoring. Built entirely from model interactions without
external data, the benchmark evolves as new models are introduced, with each
one serving both as a new opponent and as a subject of evaluation. Our
experiments reveal counterintuitive results, including cases where smaller
models outperform larger ones. Beyond benchmarking, Mini-Mafia enables
quantitative study of emergent multi-agent dynamics such as name bias and
last-speaker advantage. It also contributes to AI safety by generating training
data for deception detectors and by tracking models' deception capabilities
against human baselines.

</details>


### [34] [Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents](https://arxiv.org/abs/2509.23045)
*Zonghan Yang,Shengjie Wang,Kelin Fu,Wenyang He,Weimin Xiong,Yibo Liu,Yibo Miao,Bofei Gao,Yejie Wang,Yingwei Ma,Yanhao Li,Yue Liu,Zhenxing Hu,Kaitai Zhang,Shuyi Wang,Huarong Chen,Flood Sung,Yang Liu,Yang Gao,Zhilin Yang,Tianyu Liu*

Main category: cs.AI

TL;DR: 本文提出Agentless训练可以作为SWB-Agent的有效先验知识，并发布了Kimi-Dev，一个在SWE-bench上表现出色的开源LLM，它结合了Agentless训练和Agent适应性微调，实现了领先的SWE-Agent性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）在软件工程（SWE）中的应用分为多轮交互的SWE-Agent框架和单轮可验证的Agentless方法。作者认为这两种范式并非互斥，通过推理密集的Agentless训练可以诱导技能先验，从而提高SWE-Agent的效率和效果。

Method: 研究首先整理了Agentless训练配方，并在此基础上推出了开源的SWE LLM Kimi-Dev。随后，通过对5k个公开轨迹进行额外的监督微调（SFT），将Kimi-Dev应用于SWE-Agents的适应性训练。

Result: Kimi-Dev在SWE-bench Verified上取得了60.4%的成绩，是工作流方法中表现最佳的。通过额外的SFT适应，Kimi-Dev驱动的SWE-Agents达到了48.6%的pass@1，与Claude 3.5 Sonnet（241022版本）的表现相当。

Conclusion: 研究结果表明，来自Agentless训练的结构化技能先验可以弥合工作流和Agentic框架之间的差距，从而实现可迁移的编程Agent。

Abstract: Large Language Models (LLMs) are increasingly applied to software engineering
(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent
frameworks with multi-turn interactions and workflow-based Agentless methods
with single-turn verifiable steps. We argue these paradigms are not mutually
exclusive: reasoning-intensive Agentless training induces skill priors,
including localization, code edit, and self-reflection that enable efficient
and effective SWE-Agent adaptation. In this work, we first curate the Agentless
training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\%
on SWE-bench Verified, the best among workflow approaches. With additional SFT
adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to
48.6\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These
results show that structured skill priors from Agentless training can bridge
workflow and agentic frameworks for transferable coding agents.

</details>


### [35] [Risk Profiling and Modulation for LLMs](https://arxiv.org/abs/2509.23058)
*Yikai Wang,Xiaocheng Li,Guanting Chen*

Main category: cs.AI

TL;DR: 本文研究了LLMs的风险偏好，发现后训练是调节风险最稳定有效的方法，并揭示了不同训练阶段模型风险行为的差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在不确定性决策任务中应用日益广泛，但其风险偏好及其受提示和对齐方法影响的机制尚未得到充分探索，尤其缺乏对后训练影响LLM风险行为的研究。

Method: 本文提出了一种结合行为经济学和金融学工具的新流程，用于诱导、引导和调节LLMs的风险偏好。研究通过效用理论模型比较了预训练、指令微调和RLHF对齐的LLMs，并评估了提示工程、上下文学习和后训练等多种调节策略。

Result: 研究发现，指令微调模型行为与标准效用公式一致，而预训练和RLHF对齐模型与任何拟合的效用模型偏离更大。在所有调节策略中，后训练被证明是调节风险偏好最稳定和有效的方法。

Conclusion: 本研究深入揭示了不同类别和训练阶段LLMs的风险偏好，并展示了后训练如何有效地调节这些偏好，为未来LLM的行为对齐和风险感知设计奠定了基础。

Abstract: Large language models (LLMs) are increasingly used for decision-making tasks
under uncertainty; however, their risk profiles and how they are influenced by
prompting and alignment methods remain underexplored. Existing studies have
primarily examined personality prompting or multi-agent interactions, leaving
open the question of how post-training influences the risk behavior of LLMs. In
this work, we propose a new pipeline for eliciting, steering, and modulating
LLMs' risk profiles, drawing on tools from behavioral economics and finance.
Using utility-theoretic models, we compare pre-trained, instruction-tuned, and
RLHF-aligned LLMs, and find that while instruction-tuned models exhibit
behaviors consistent with some standard utility formulations, pre-trained and
RLHF-aligned models deviate more from any utility models fitted. We further
evaluate modulation strategies, including prompt engineering, in-context
learning, and post-training, and show that post-training provides the most
stable and effective modulation of risk preference. Our findings provide
insights into the risk profiles of different classes and stages of LLMs and
demonstrate how post-training modulates these profiles, laying the groundwork
for future research on behavioral alignment and risk-aware LLM design.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [Localizing Adversarial Attacks To Produces More Imperceptible Noise](https://arxiv.org/abs/2509.22710)
*Pavan Reddy,Aditya Sanjay Gujral*

Main category: cs.LG

TL;DR: 本研究系统评估了局部对抗性攻击，发现其比全局攻击更不易察觉但计算成本更高，且迭代方法更适用于局部约束。


<details>
  <summary>Details</summary>
Motivation: 传统对抗性攻击多关注全局扰动，局部对抗性噪声的潜力尚未被充分探索。

Method: 通过引入二值掩码将噪声限制在特定区域，系统评估了FGSM、PGD和C&W等常用方法下的局部对抗性攻击，量化其有效性、不可察觉性和计算效率。

Result: 局部攻击实现了更低的平均像素扰动、更高的PSNR和SSIM，但代价是计算量增加和攻击成功率（ASR）略有下降。迭代方法（如PGD和C&W）比单步方法（如FGSM）对局部约束表现出更高的鲁棒性。

Conclusion: 本工作提供了局部对抗性攻击的全面分析，为提升攻击策略和设计鲁棒防御系统提供了实用见解。

Abstract: Adversarial attacks in machine learning traditionally focus on global
perturbations to input data, yet the potential of localized adversarial noise
remains underexplored. This study systematically evaluates localized
adversarial attacks across widely-used methods, including FGSM, PGD, and C&W,
to quantify their effectiveness, imperceptibility, and computational
efficiency. By introducing a binary mask to constrain noise to specific
regions, localized attacks achieve significantly lower mean pixel
perturbations, higher Peak Signal-to-Noise Ratios (PSNR), and improved
Structural Similarity Index (SSIM) compared to global attacks. However, these
benefits come at the cost of increased computational effort and a modest
reduction in Attack Success Rate (ASR). Our results highlight that iterative
methods, such as PGD and C&W, are more robust to localization constraints than
single-step methods like FGSM, maintaining higher ASR and imperceptibility
metrics. This work provides a comprehensive analysis of localized adversarial
attacks, offering practical insights for advancing attack strategies and
designing robust defensive systems.

</details>


### [37] [In-Context Learning can Perform Continual Learning Like Humans](https://arxiv.org/abs/2509.22764)
*Liuwang Kang,Fan Wang,Shaoshan Liu,Hung-Chyun Chou,Chuan Lin,Ning Ding*

Main category: cs.LG

TL;DR: 研究了大型语言模型中的上下文持续学习（ICCL），通过任务调度和提示重排实现知识保留和积累，发现其表现出类似人类的记忆特性，并有效缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要将LLM的上下文学习（ICL）视为少样本学习，但其在多任务顺序到达时，能否实现长期知识保留和跨任务积累仍未充分探索。本研究受人类记忆启发，旨在解决这一问题。

Method: 提出了上下文持续学习（ICCL）的概念，通过任务调度和提示重排实现持续学习能力。同时，引入了一个人类保留相似性度量指标，以量化持续学习方法与人类保留动态的契合度。

Result: 在Markov-Chain基准测试中，ICCL（对于特定LLM）受益于分布式练习，揭示了类似人类的保留“最佳间隔点”。线性注意力模型（如MAMBA和RWKV）表现出特别类似人类的保留模式，尽管其性能略低于Transformer模型。

Conclusion: ICCL作为一种仅通过推理的持续学习范式，被证明既具有认知合理性又具备实际效果。它能有效缓解灾难性遗忘，并解决了传统持续学习方法中稳定性与可塑性之间的困境。

Abstract: Large language models (LLMs) can adapt to new tasks via in-context learning
(ICL) without parameter updates, making them powerful learning engines for fast
adaptation. While extensive research has examined ICL as a few-shot learner,
whether it can achieve long-term retention and cross-task knowledge
accumulation when multitasks arrive sequentially remains underexplored.
Motivated by human memory studies, we investigate the retention characteristics
of ICL in multitask settings and extend it to in-context continual learning
(ICCL), where continual learning ability emerges through task scheduling and
prompt rearrangement. Experiments on Markov-Chain benchmarks demonstrate that,
for specific large-language models, ICCL benefits from distributed practice
(DP) in a manner analogous to humans, consistently revealing a spacing "sweet
spot" for retention. Beyond retention performance, we propose a human-retention
similarity metric to quantify how closely a continual-learning (CL) method
aligns with human retention dynamics. Using this metric, we show that
linear-attention models such as MAMBA and RWKV exhibit particularly human-like
retention patterns, despite their retention performance lagging behind that of
Transformer-based LLMs. Overall, our results establish ICCL as both cognitively
plausible and practically effective, providing an inference-only CL paradigm
that mitigates catastrophic forgetting and addresses the stability-plasticity
dilemma in conventional CL methods.

</details>


### [38] [Communication-Efficient and Interoperable Distributed Learning](https://arxiv.org/abs/2509.22823)
*Mounssif Krouka,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出一种通信高效的分布式学习框架，支持异构模型协同学习，通过共享融合层输出实现互操作性与隐私保护，并展现出优越的通信效率和稳定的训练性能。


<details>
  <summary>Details</summary>
Motivation: 解决异构模型架构在协同学习中互操作性与隐私保护的重大挑战。

Method: 提出一个通信高效的分布式学习框架，支持模型异构性并允许推理时的模块化组合。所有客户端采用共同的融合层输出维度，将模型划分为个性化基础块和通用模块化块。客户端仅共享融合层输出，从而保护模型参数和架构的隐私。

Result: 实验结果表明，与联邦学习(FL)和联邦拆分学习(FSL)基线相比，该框架实现了更优的通信效率，并确保了异构架构下的稳定训练性能。

Conclusion: 该框架有效解决了异构模型协同学习中的通信效率、互操作性和隐私保护问题，同时保持了稳定的训练性能。

Abstract: Collaborative learning across heterogeneous model architectures presents
significant challenges in ensuring interoperability and preserving privacy. We
propose a communication-efficient distributed learning framework that supports
model heterogeneity and enables modular composition during inference. To
facilitate interoperability, all clients adopt a common fusion-layer output
dimension, which permits each model to be partitioned into a personalized base
block and a generalized modular block. Clients share their fusion-layer
outputs, keeping model parameters and architectures private. Experimental
results demonstrate that the framework achieves superior communication
efficiency compared to federated learning (FL) and federated split learning
(FSL) baselines, while ensuring stable training performance across
heterogeneous architectures.

</details>


### [39] [On the Capacity of Self-Attention](https://arxiv.org/abs/2509.22840)
*Micah Adler*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While self-attention is known to learn relations among tokens, we lack a
formal understanding of its capacity: how many distinct relations can a single
layer reliably recover for a given budget?
  To formalize this, we introduce Relational Graph Recognition (RGR), where the
key-query channel represents a graph on $m$ items with $m'$ directed edges,
and, given a context of items, must recover the neighbors of each item. We
measure resources by the total key dimension $D_K = h\,d_k$. Within this
framework, we analytically derive a capacity scaling law and validate it
empirically. We show that $D_K = \Theta(m' \log m' / d_{\text{model}})$ is both
necessary (information-theoretic lower bound) and sufficient (explicit
construction) in a broad class of graphs to recover $m'$ relations. This
scaling law directly leads to a new, capacity-based rationale for multi-head
attention that applies even when each item only attends to a single target.
When embeddings are uncompressed ($m = d_{\text{model}}$) and the graph is a
permutation, a single head suffices. However, compression ($m >
d_{\text{model}}$) forces relations into overlapping subspaces, creating
interference that a single large head cannot disentangle. Our analysis shows
that allocating a fixed $D_K$ across many small heads mitigates this
interference, increasing the number of recoverable relations. Controlled
single-layer experiments mirror the theory, revealing a sharp performance
threshold that matches the predicted capacity scaling and confirms the benefit
of distributing $D_K$ across multiple heads.
  Altogether, these results provide a concrete scaling law for self-attention
capacity and a principled design rule for allocating key-query budget across
heads.

</details>


### [40] [Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data](https://arxiv.org/abs/2509.22850)
*Roie Kazoom,Yuval Ratzabi,Etamar Rothstein,Ofer Hadar*

Main category: cs.LG

TL;DR: 本文提出一种针对表格数据的黑盒、基于决策的对抗性攻击方法，能在少量查询下以超90%的成功率攻击多种模型，揭示了表格模型的严重脆弱性。


<details>
  <summary>Details</summary>
Motivation: 与图像和语言领域相比，结构化（表格）数据的对抗性鲁棒性仍是一个未充分探索的领域。

Method: 引入一种新颖的黑盒、基于决策的表格数据对抗性攻击。该方法结合了无梯度方向估计和迭代边界搜索，能在最小预言机访问下高效处理离散和连续特征空间。

Result: 实验证明，该方法成功攻击了从传统机器学习分类器到基于大型语言模型（LLM）的管道等多种模型的几乎整个测试集，攻击成功率持续高于90%，且每次攻击所需查询次数很少。

Conclusion: 这些结果突显了表格模型对对抗性扰动的严重脆弱性，强调了在现实世界决策系统中加强防御的紧迫性。

Abstract: Adversarial robustness in structured data remains an underexplored frontier
compared to vision and language domains. In this work, we introduce a novel
black-box, decision-based adversarial attack tailored for tabular data. Our
approach combines gradient-free direction estimation with an iterative boundary
search, enabling efficient navigation of discrete and continuous feature spaces
under minimal oracle access. Extensive experiments demonstrate that our method
successfully compromises nearly the entire test set across diverse models,
ranging from classical machine learning classifiers to large language model
(LLM)-based pipelines. Remarkably, the attack achieves success rates
consistently above 90%, while requiring only a small number of queries per
instance. These results highlight the critical vulnerability of tabular models
to adversarial perturbations, underscoring the urgent need for stronger
defenses in real-world decision-making systems.

</details>


### [41] [Adaptive Margin RLHF via Preference over Preferences](https://arxiv.org/abs/2509.22851)
*Yaswanth Chittepu,Prasann Singhal,Greg Durrett,Scott Niekum*

Main category: cs.LG

TL;DR: 本文提出DPO-PoP方法，通过“偏好之上的偏好”信号推断自适应边距，以改进RLHF中奖励模型的泛化性和对齐，优于传统DPO及其变体。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法在处理偏好边距时存在局限性，如无边距、固定边距或基于简单函数的边距，未能有效捕捉偏好强度的差异。此外，许多自适应边距方法依赖难以可靠获取的精确偏好分数。

Method: 提出利用“偏好之上的偏好”（即指示哪种偏好更强烈的标注）这一序数信号，为每个数据点推断自适应边距。将此方法整合到DPO中，形成DPO-PoP。同时，提出两种采样策略以权衡区分性和生成性表现。

Result: DPO-PoP在UltraFeedback数据集上超越了香草DPO、固定边距DPO和带有真值边距的DPO。研究发现，区分性（分类准确率）和生成性表现之间存在权衡：过度关注区分性可能导致生成质量下降。

Conclusion: 通过“偏好之上的偏好”实现的自适应边距可以显著提升DPO的判别和生成性能。然而，在优化边距时需谨慎处理区分性与生成性之间的权衡，提出的采样策略有助于在此权衡中取得平衡。

Abstract: Margin-based optimization is fundamental to improving generalization and
robustness in classification tasks. In the context of reward model learning
from preferences within Reinforcement Learning from Human Feedback (RLHF),
existing methods typically rely on no margins, fixed margins, or margins that
are simplistic functions of preference ratings. However, such formulations
often fail to account for the varying strengths of different preferences, for
example some preferences are associated with larger margins between responses,
or they rely on noisy margin information derived from ratings. We argue that
modeling the strength of preferences can lead to better generalization and more
faithful alignment. Furthermore, many existing methods that use adaptive
margins assume access to accurate preference scores, which can be difficult for
humans to provide reliably. We propose an approach that leverages preferences
over preferences, that is annotations indicating which of two preferences
reflects a stronger distinction. We use this ordinal signal to infer adaptive
margins on a per-datapoint basis. We introduce an extension to Direct
Preference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from
preference-over-preference supervision, enabling improved discriminative and
generative performance. Empirically, our method outperforms vanilla DPO, DPO
with fixed margins, and DPO with ground-truth margins on the UltraFeedback
dataset. Additionally, we show that there is a tradeoff between discriminative
and generative performance: improving test classification accuracy,
particularly by correctly labeling weaker preferences at the expense of
stronger ones, can lead to a decline in generative quality. To navigate this
tradeoff, we propose two sampling strategies to gather
preference-over-preference labels: one favoring discriminative performance and
one favoring generative performance.

</details>


### [42] [Observation-Free Attacks on Online Learning to Rank](https://arxiv.org/abs/2509.22855)
*Sameep Chattopadhyay,Nikhil Karamchandani,Sharayu Mohair*

Main category: cs.LG

TL;DR: 本文提出一个新颖的框架及两种策略，用于攻击在线排序学习（OLTR）算法，能在O(log T)操作下成功推广目标项目并引发线性遗憾，并通过理论和实证分析验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管在线排序学习（OLTR）算法应用广泛，但其对协同对抗性攻击的脆弱性尚未得到充分理解。

Method: 1. 提出了一个针对常用OLTR算法的新型攻击框架，旨在使目标项目在T - o(T)轮次中出现在前K推荐列表，并同时诱导学习算法产生线性遗憾。
2. 具体提出了两种攻击策略：CascadeOFA（针对CascadeUCB1）和PBMOFA（针对PBM-UCB）。

Result: 1. 提供了理论保证，表明两种攻击策略都仅需O(log T)次操作即可成功。
2. 通过在真实世界数据上的实证结果补充了理论分析。

Conclusion: 本研究成功设计并理论验证了针对在线排序学习算法的有效攻击框架和策略，揭示了其在对抗性攻击下的脆弱性，并得到了实证数据的支持。

Abstract: Online learning to rank (OLTR) plays a critical role in information retrieval
and machine learning systems, with a wide range of applications in search
engines and content recommenders. However, despite their extensive adoption,
the susceptibility of OLTR algorithms to coordinated adversarial attacks
remains poorly understood. In this work, we present a novel framework for
attacking some of the widely used OLTR algorithms. Our framework is designed to
promote a set of target items so that they appear in the list of top-K
recommendations for T - o(T) rounds, while simultaneously inducing linear
regret in the learning algorithm. We propose two novel attack strategies:
CascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB . We provide theoretical
guarantees showing that both strategies require only O(log T) manipulations to
succeed. Additionally, we supplement our theoretical analysis with empirical
results on real-world data.

</details>


### [43] [Neighborhood Sampling Does Not Learn the Same Graph Neural Network](https://arxiv.org/abs/2509.22868)
*Zehao Niu,Mihai Anitescu,Jie Chen*

Main category: cs.LG

TL;DR: 本文对大规模图神经网络中的邻域采样方法进行了理论分析，发现不同采样方法在有限样本下行为各异但最终收敛，且其预测误差下限不可比，表明没有单一方法占据主导。


<details>
  <summary>Details</summary>
Motivation: 邻域采样是训练大规模图神经网络的关键技术，能有效控制内存和时间成本，但其系统行为和理论特性仍未被充分理解。

Method: 采用神经切线核（NTK）工具，通过分析其无限宽对应物——高斯过程（GP），对几种成熟的邻域采样方法及其相应的后验GP进行了理论分析。

Result: 在有限样本条件下，不同采样方法的后验GP各不相同，但随着样本数量的增加，它们会收敛到同一个后验。此外，作为均方预测误差下限的后验协方差是不可比较的。

Conclusion: 后验协方差的不可比较性与实践中观察到的“没有单一采样方法占据主导地位”的现象相符，暗示不同采样方法在性能上各有优劣，没有普适的最佳选择。

Abstract: Neighborhood sampling is an important ingredient in the training of
large-scale graph neural networks. It suppresses the exponential growth of the
neighborhood size across network layers and maintains feasible memory
consumption and time costs. While it becomes a standard implementation in
practice, its systemic behaviors are less understood. We conduct a theoretical
analysis by using the tool of neural tangent kernels, which characterize the
(analogous) training dynamics of neural networks based on their infinitely wide
counterparts -- Gaussian processes (GPs). We study several established
neighborhood sampling approaches and the corresponding posterior GP. With
limited samples, the posteriors are all different, although they converge to
the same one as the sample size increases. Moreover, the posterior covariance,
which lower-bounds the mean squared prediction error, is uncomparable, aligning
with observations that no sampling approach dominates.

</details>


### [44] [From Noise to Knowledge: A Comparative Study of Acoustic Anomaly Detection Models in Pumped-storage Hydropower Plants](https://arxiv.org/abs/2509.22881)
*Karim Khamaisi,Nicolas Keller,Stefan Krummenacher,Valentin Huber,Bernhard Fässler,Bruno Rodrigues*

Main category: cs.LG

TL;DR: 本研究比较分析了水电厂基于声学的异常检测方法，发现在真实世界数据上，OC-SVM在准确性和计算效率之间取得了最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 工业工厂和能源生产商（尤其是水电厂）的非计划停机成本高昂且难以维护。现有声学异常检测研究缺乏针对水电厂的专用数据集和方法，旨在通过声学分析改善水电厂的预测性维护。

Method: 在高噪声条件下对声学数据进行预处理，提取时域和频域特征。比较了三种机器学习模型：LSTM AE、K-Means和OC-SVM。模型在奥地利Rodundwerk II抽水蓄能电站的两个真实数据集（一个包含诱导异常，一个反映真实工况）上进行了测试。

Result: 单类支持向量机（OC-SVM）在准确性（ROC AUC 0.966-0.998）和最小训练时间方面表现出最佳的权衡。LSTM自编码器也提供了强大的检测能力（ROC AUC 0.889-0.997），但计算成本更高。

Conclusion: OC-SVM是水电厂声学异常检测的有效方法，在准确性和计算效率上表现出最佳平衡，适合用于改善预测性维护。

Abstract: In the context of industrial factories and energy producers, unplanned
outages are highly costly and difficult to service. However, existing
acoustic-anomaly detection studies largely rely on generic industrial or
synthetic datasets, with few focused on hydropower plants due to limited
access. This paper presents a comparative analysis of acoustic-based anomaly
detection methods, as a way to improve predictive maintenance in hydropower
plants. We address key challenges in the acoustic preprocessing under highly
noisy conditions before extracting time- and frequency-domain features. Then,
we benchmark three machine learning models: LSTM AE, K-Means, and OC-SVM, which
are tested on two real-world datasets from the Rodundwerk II pumped-storage
plant in Austria, one with induced anomalies and one with real-world
conditions. The One-Class SVM achieved the best trade-off of accuracy (ROC AUC
0.966-0.998) and minimal training time, while the LSTM autoencoder delivered
strong detection (ROC AUC 0.889-0.997) at the expense of higher computational
cost.

</details>


### [45] [FedCF: Fair Federated Conformal Prediction](https://arxiv.org/abs/2509.22907)
*Anutam Srinivasan,Aditya T. Vadlamani,Amin Meghrazi,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 将共形公平（CF）扩展到联邦学习（FL），用于审计联邦模型的公平性，并通过实证实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 标准的共形预测（CP）在不确定性量化方面广泛应用，但缺乏对数据集敏感属性的公平性考量。尽管现有方法（如共形公平CF）已尝试将公平性融入CP，但尚未将其应用于联邦学习（FL）环境，这促使本研究探索在FL中实现公平性审计。

Method: ['将共形公平（CF）框架扩展至联邦学习（FL）设置。', '提出通过分析不同人口群体的公平性相关差距来审计联邦模型公平性的方法。', '通过在多个领域和数据集上进行实证实验，充分利用可交换性假设，来验证所提出的框架。']

Result: 通过在多个数据集上的实验，经验性地验证了所提出的在联邦学习环境中进行公平性审计框架的有效性。

Conclusion: 本文成功将共形公平（CF）框架应用于联邦学习（FL）场景，提供了一种通过分析不同人口群体的公平性相关差距来审计联邦模型公平性的有效方法，并已通过实验得到验证。

Abstract: Conformal Prediction (CP) is a widely used technique for quantifying
uncertainty in machine learning models. In its standard form, CP offers
probabilistic guarantees on the coverage of the true label, but it is agnostic
to sensitive attributes in the dataset. Several recent works have sought to
incorporate fairness into CP by ensuring conditional coverage guarantees across
different subgroups. One such method is Conformal Fairness (CF). In this work,
we extend the CF framework to the Federated Learning setting and discuss how we
can audit a federated model for fairness by analyzing the fairness-related gaps
for different demographic groups. We empirically validate our framework by
conducting experiments on several datasets spanning multiple domains, fully
leveraging the exchangeability assumption.

</details>


### [46] [Guided Manifold Alignment with Geometry-Regularized Twin Autoencoders](https://arxiv.org/abs/2509.22913)
*Jake S. Rhodes,Adam G. Rustad,Marshall S. Nielsen,Morgan Chase McClellan,Dallan Gardner,Dawson Hedges*

Main category: cs.LG

TL;DR: 本文提出一种基于几何正则化双生自编码器的引导式表示学习框架，旨在增强流形对齐（MA）的样本外泛化能力，并在多模态数据整合和疾病诊断中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 许多传统流形对齐（MA）方法无法进行样本外扩展（out-of-sample extension），限制了它们在真实世界中的应用。

Method: 提出一个引导式表示学习框架，该框架利用几何正则化的双生自编码器（AE）架构。方法通过强制结构化跨模态映射来保持学习嵌入的几何保真度，并结合预训练对齐模型和多任务学习公式，以提高跨域泛化能力和表示鲁棒性。

Result: 实验结果显示，该方法在嵌入一致性、信息保持和跨域迁移方面均有所改进。此外，将其应用于阿尔茨海默病诊断时，能有效整合多模态患者数据，并通过利用多模态问题中的见解，提高单域场景下的预测准确性。

Conclusion: 该框架有效解决了传统流形对齐方法泛化能力不足的局限性，通过增强跨域泛化和表示鲁棒性，为多模态数据整合和提高预测精度提供了有效途径，特别在医学诊断领域展现出巨大潜力。

Abstract: Manifold alignment (MA) involves a set of techniques for learning shared
representations across domains, yet many traditional MA methods are incapable
of performing out-of-sample extension, limiting their real-world applicability.
We propose a guided representation learning framework leveraging a
geometry-regularized twin autoencoder (AE) architecture to enhance MA while
enabling generalization to unseen data. Our method enforces structured
cross-modal mappings to maintain geometric fidelity in learned embeddings. By
incorporating a pre-trained alignment model and a multitask learning
formulation, we improve cross-domain generalization and representation
robustness while maintaining alignment fidelity. We evaluate our approach using
several MA methods, showing improvements in embedding consistency, information
preservation, and cross-domain transfer. Additionally, we apply our framework
to Alzheimer's disease diagnosis, demonstrating its ability to integrate
multi-modal patient data and enhance predictive accuracy in cases limited to a
single domain by leveraging insights from the multi-modal problem.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [47] [Bridging Language Models and Formal Methods for Intent-Driven Optical Network Design](https://arxiv.org/abs/2509.22834)
*Anis Bekri,Amar Abane,Abdella Battou,Saddek Bensalem*

Main category: cs.NI

TL;DR: 为解决LLM在IBN中将自然语言意图转换为正式光学网络拓扑的挑战，本文提出一种结合LLM意图解析、形式化方法和光学RAG的混合管道，以生成可解释、可验证且值得信赖的光学网络设计。


<details>
  <summary>Details</summary>
Motivation: 意图驱动网络(IBN)旨在通过高层目标自动化网络设计，但将非正式自然语言意图转换为正式准确的光学网络拓扑面临挑战，主要原因在于大型语言模型(LLM)固有的模糊性和缺乏严谨性。

Method: 提出一种新颖的混合管道，该管道集成了LLM意图解析、形式化方法和光学检索增强生成(RAG)。通过引入特定领域的光学标准，并系统性地结合符号推理和验证技术来丰富设计决策。

Result: 成功生成了可解释、可验证且值得信赖的光学网络设计。

Conclusion: 该方法显著推动了IBN领域的发展，确保了任务关键型网络任务的可靠性和正确性。

Abstract: Intent-Based Networking (IBN) aims to simplify network management by enabling
users to specify high-level goals that drive automated network design and
configuration. However, translating informal natural-language intents into
formally correct optical network topologies remains challenging due to inherent
ambiguity and lack of rigor in Large Language Models (LLMs). To address this,
we propose a novel hybrid pipeline that integrates LLM-based intent parsing,
formal methods, and Optical Retrieval-Augmented Generation (RAG). By enriching
design decisions with domain-specific optical standards and systematically
incorporating symbolic reasoning and verification techniques, our pipeline
generates explainable, verifiable, and trustworthy optical network designs.
This approach significantly advances IBN by ensuring reliability and
correctness, essential for mission-critical networking tasks.

</details>


### [48] [Impact of Environmental Factors on LoRa 2.4 GHz Time of Flight Ranging Outdoors](https://arxiv.org/abs/2509.23125)
*Yiqing Zhou,Xule Zhou,Zecan Cheng,Chenao Lu,Junhan Chen,Jiahong Pan,Yizhuo Liu,Sihao Li,Kyeong Soo Kim*

Main category: cs.NI

TL;DR: 针对WSN/IoT节点定位精度受限问题，本文利用LoRa 2.4 GHz的RF ToF测距方法，收集了一个包含温度和湿度的多周户外测距数据集，并通过初步DNN分析证明环境因素对测距精度有显著影响。


<details>
  <summary>Details</summary>
Motivation: 现有WSN/IoT定位技术精度不足（无法达到米级），尽管LoRa 2.4 GHz提供了米级RF ToF测距能力，但现有数据集覆盖范围有限且未考虑温度、湿度等环境因素，导致无法准确评估和补偿环境对测距的影响。

Method: 在XJTLU南校区运动场，通过三个LoRa节点与一个LoRa基站（ESP32收发器）在400平方米的3x3网格参考点上，历时三周收集LoRa 2.4 GHz RF ToF测距数据，同时记录温度和湿度，并将所有测量记录上传至基站。初步分析使用了一个简单的深度神经网络（DNN）模型。

Result: 初步的深度神经网络（DNN）分析表明，包括温度和湿度在内的环境因素显著影响LoRa RF ToF测距的精度。

Conclusion: 环境因素（如温度和湿度）对户外LoRa RF ToF测距精度有显著影响，因此迫切需要开发先进方法来补偿这些环境因素造成的影响。

Abstract: In WSN/IoT, node localization is essential to long-running applications for
accurate environment monitoring and event detection, often covering a large
area in the field. Due to the lower time resolution of typical WSN/IoT
platforms (e.g., 1 microsecond on ESP32 platforms) and the jitters in
timestamping, packet-level localization techniques cannot provide meter-level
resolution. For high-precision localization as well as world-wide
interoperability via 2.4-GHz ISM band, a new variant of LoRa, called LoRa 2.4
GHz, was proposed by semtech, which provides a radio frequency (RF) time of
flight (ToF) ranging method for meter-level localization. However, the existing
datasets reported in the literature are limited in their coverages and do not
take into account varying environmental factors such as temperature and
humidity. To address these issues, LoRa 2.4 GHz RF ToF ranging data was
collected on a sports field at the XJTLU south campus, where three LoRa nodes
logged samples of ranging with a LoRa base station, together with temperature
and humidity, at reference points arranged as a 3x3 grid covering 400 square
meter over three weeks and uploaded all measurement records to the base station
equipped with an ESP32-based transceiver for machine and user communications.
The results of a preliminary investigation based on a simple deep neural
network (DNN) model demonstrate that the environmental factors, including the
temperature and humidity, significantly affect the accuracy of ranging, which
calls for advanced methods of compensating for the effects of environmental
factors on LoRa RF ToF ranging outdoors.

</details>


### [49] [Unlicensed Band Allocation for Heterogeneous Networks](https://arxiv.org/abs/2509.23216)
*Po-Heng Chou*

Main category: cs.NI

TL;DR: 在LAA与Wi-Fi共存的异构网络中，为解决非授权频段干扰及优化QoS，本研究提出了四种非授权频段分配方案并进行了性能评估，为LAA小蜂窝的设计提供了指导。


<details>
  <summary>Details</summary>
Motivation: LAA与Wi-Fi在共享非授权频段时会相互干扰，严重影响双方服务质量（QoS）。因此，如何有效地进行非授权频段分配是一个重要问题。

Method: 提出了一个分析模型，并进行了仿真实验。研究了四种非授权频段分配方案：非授权全分配（UFA）、非授权时分分配（UTA），以及分别结合缓冲机制的UFAB和UTAB。性能评估指标是LAA缓冲队列中LAA和Wi-Fi数据包的接受率。

Result: 通过对不同非授权频段分配方案的性能评估，本研究为LAA小蜂窝的信道占用阶段和缓冲大小设计提供了具体的指导方针。

Conclusion: 研究结果可为LAA小蜂窝的信道占用阶段和缓冲大小的设计提供实践指导。

Abstract: Based on the License-Assisted Access (LAA) small cell architecture, the LAA
coexisting with Wi-Fi heterogeneous networks provides LTE mobile users with
high bandwidth efficiency as the unlicensed channels are shared among LAA and
Wi-Fi. However, LAA and Wi-Fi interfere with each other when both systems use
the same unlicensed channel in heterogeneous networks. In such a network,
unlicensed band allocation for LAA and Wi-Fi is an important issue that may
affect the quality of service (QoS) of both systems significantly. In this
paper, we propose an analytical model and conduct simulation experiments to
study four allocations for the unlicensed band: unlicensed full allocation
(UFA), unlicensed time-division allocation (UTA), and UFA/UTA with buffering
mechanism (UFAB and UTAB) for the LAA data packets. We evaluate the performance
of these unlicensed band allocation schemes in terms of the acceptance rate of
both LAA and Wi-Fi packet data in the LAA buffer queue. Our study provides
guidelines for designing the channel occupation phase and the buffer size of
the LAA small cell.

</details>


### [50] [Modeling the Unlicensed Band Allocation for LAA With Buffering Mechanism](https://arxiv.org/abs/2509.23217)
*Po-Heng Chou*

Main category: cs.NI

TL;DR: 本文提出一个分析模型和仿真实验，研究异构网络中基于先听后说（LBT）的LAA无许可频带分配及其缓冲机制，评估LAA和Wi-Fi数据包的接受率，并为LAA系统设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 在异构网络中，LAA和Wi-Fi的无许可频带分配是一个重要问题，可能显著影响两个系统的服务质量。

Method: 提出一个分析模型并进行仿真实验，以研究基于先听后说（LBT）的无许可频带分配以及LAA数据包的缓冲机制。

Result: 通过评估LAA和Wi-Fi数据包的接受率来衡量无许可频带分配的性能。

Conclusion: 本研究为LAA系统的信道占用阶段和缓冲阈值设计提供了指导方针。

Abstract: In this letter, we propose an analytical model and conduct simulation
experiments to study listen-before-talk-based unlicensed band allocation with
the buffering mechanism for the License-Assisted Access (LAA) packets in the
heterogeneous networks. In such a network, unlicensed band allocation for LAA
and Wi-Fi is an important issue, which may affect the quality of service for
both systems significantly. We evaluate the performance of these unlicensed
band allocations in terms of the acceptance rate of both LAA and Wi-Fi packets.
This letter provides the guidelines for designing the channel occupation phase
and buffer threshold of the LAA systems.

</details>


### [51] [Markov Modeling for Licensed and Unlicensed Band Allocation in Underlay and Overlay D2D](https://arxiv.org/abs/2509.23218)
*Po-Heng Chou,Yen-Ting Liu,Wei-Chang Chen,Walid Saad*

Main category: cs.NI

TL;DR: 本文提出了一种D2D辅助蜂窝网络中的新型资源分配分析模型，考虑了D2D、蜂窝和Wi-Fi系统的共存与交互，并通过阈值流控制保证Wi-Fi QoS，旨在优化网络性能。


<details>
  <summary>Details</summary>
Motivation: 解决D2D辅助蜂窝网络中的资源分配问题，包括授权频段共享、蜂窝流量卸载以及与Wi-Fi系统在非授权频段上的共存问题，同时保证Wi-Fi的服务质量。

Method: 提出了一种新颖的资源分配分析模型，能够应用于D2D的底层和叠加系统。该模型通过全局系统状态反映D2D、传统蜂窝和Wi-Fi数据包之间的交互。在此基础上，提出了一种基于阈值的流控制机制以保证Wi-Fi的QoS，并推导了数据包阻塞概率。

Result: 仿真结果表明，所提出的方案在略微牺牲传统蜂窝性能的情况下，显著提升了叠加D2D的性能，同时保持了Wi-Fi用户的性能。此外，该方案在D2D和Wi-Fi之间提供了比底层方案更灵活的调整能力。

Conclusion: 所提出的资源分配分析模型能有效平衡D2D、蜂窝和Wi-Fi系统间的性能，尤其在叠加D2D场景下表现优异，并在D2D与Wi-Fi共存调整方面显示出更高的灵活性。

Abstract: In this paper, a novel analytical model for resource allocation is proposed
for a device-to-device (D2D) assisted cellular network. The proposed model can
be applied to underlay and overlay D2D systems for sharing licensed bands and
offloading cellular traffic. The developed model also takes into account the
problem of unlicensed band sharing with Wi-Fi systems. In the proposed model, a
global system state reflects the interaction among D2D, conventional cellular,
and Wi-Fi packets. Under the standard traffic model assumptions, a
threshold-based flow control is proposed for guaranteeing the
quality-of-service (QoS) of Wi-Fi. The packet blockage probability is then
derived. Simulation results show the proposed scheme sacrifices conventional
cellular performance slightly to improve overlay D2D performance significantly
while maintaining the performance for Wi-Fi users. Meanwhile, the proposed
scheme has more flexible adjustments between D2D and Wi-Fi than the underlay
scheme.

</details>


### [52] [A Modular KDN-Based Framework for IT/OT Autonomy in Industrial Systems](https://arxiv.org/abs/2509.23389)
*Tuğçe Bilen,Mehmet Ozdem*

Main category: cs.NI

TL;DR: 本文提出了一个基于知识定义网络（KDN）的模块化框架，用于实现IT-OT融合环境中工业系统的自适应和自主控制，通过闭环机制提升智能工业网络的韧性、响应性和运行效率。


<details>
  <summary>Details</summary>
Motivation: 随着IT与OT的融合，工业系统日益复杂、异构且对实时性要求高，传统的基于规则或静态管理方法已不足以应对。

Method: 提出一个基于知识定义网络（KDN）范式的模块化框架，包含遥测收集器、知识构建器、决策引擎和控制执行器四个核心模块。这些模块在闭环控制中运行，持续观察系统行为，提取上下文知识，评估控制动作，并通过图基抽象和效用优化机制，对可编程工业端点应用策略决策。

Result: 框架通过决策延迟、控制有效性和系统稳定性三个关键指标进行评估，结果表明其能够增强智能工业网络的韧性、响应性和运行效率。

Conclusion: 该KDN框架为IT-OT基础设施提供了一种自适应和自主的控制方案，有效解决了复杂工业环境下的管理挑战，提升了系统性能。

Abstract: The convergence of Information Technology (IT) and Operational Technology
(OT) is a critical enabler for achieving autonomous and intelligent industrial
systems. However, the increasing complexity, heterogeneity, and real-time
demands of industrial environments render traditional rule-based or static
management approaches insufficient. In this paper, we present a modular
framework based on the Knowledge-Defined Networking (KDN) paradigm, enabling
adaptive and autonomous control across IT-OT infrastructures. The proposed
architecture is composed of four core modules: Telemetry Collector, Knowledge
Builder, Decision Engine, and Control Enforcer. These modules operate in a
closed control loop to continuously observe system behavior, extract contextual
knowledge, evaluate control actions, and apply policy decisions across
programmable industrial endpoints. A graph-based abstraction is used to
represent system state, and a utility-optimization mechanism guides control
decisions under dynamic conditions. The framework's performance is evaluated
using three key metrics: decision latency, control effectiveness, and system
stability, demonstrating its capability to enhance resilience, responsiveness,
and operational efficiency in smart industrial networks.

</details>


### [53] [Knowledge-Defined and Twin-Assisted Network Management for 6G](https://arxiv.org/abs/2509.23398)
*Tuğçe Bilen,Mehmet Özdem*

Main category: cs.NI

TL;DR: 提出一种模块化、知识定义的架构，整合数字孪生、语义推理和零样本学习，以实现6G网络在未见场景下的自主决策。


<details>
  <summary>Details</summary>
Motivation: 6G网络日益增长的复杂性、动态性和异构性，要求管理系统能够主动推理并泛化到预定义场景之外。

Method: 该架构通过实时数据流维护物理网络的数字孪生模型，并预测短期状态。预测结果送入知识平面，构建并更新基于图的网络抽象，通过图神经网络推理生成上下文感知意图。管理平面利用语义嵌入和零样本策略匹配来选择预学习的动作，实现无需重新训练的适应性。决策通过控制平面执行，并采用闭环反馈机制持续优化预测、知识和策略。

Result: 仿真结果证实，所提出的框架在策略响应时间、SLA（服务水平协议）合规率和意图匹配准确性方面均有显著改进。

Conclusion: 该框架通过结合数字孪生、语义推理和零样本学习，有效提升了6G网络在复杂和未见场景下的自主决策能力和管理效率。

Abstract: The increasing complexity, dynamism, and heterogeneity of 6G networks demand
management systems that can reason proactively and generalize beyond
pre-defined cases. In this paper, we propose a modular, knowledge-defined
architecture that integrates Digital Twin models with semantic reasoning and
zero-shot learning to enable autonomous decision-making for previously unseen
network scenarios. Real-time data streams are used to maintain synchronized
virtual replicas of the physical network, which also forecast short-term state
transitions. These predictions feed into a knowledge plane that constructs and
updates a graph-based abstraction of the network, enabling context-aware intent
generation via graph neural reasoning. To ensure adaptability without
retraining, the management plane performs zero-shot policy matching by
semantically embedding candidate intents and selecting suitable pre-learned
actions. The selected decisions are translated and enforced through the control
plane, while a closed-loop feedback mechanism continuously refines predictions,
knowledge, and policies over time. Simulation results confirm that the proposed
framework observes notable improvements in policy response time, SLA compliance
rate, and intent matching accuracy.

</details>


### [54] [AUV-Assisted Underwater 6G: Environmental Modeling and Multi-Stage Optimization](https://arxiv.org/abs/2509.23401)
*Mustafa Yavuz Engin,Mehmet Ozdem,Tuğçe Bilen*

Main category: cs.NI

TL;DR: 本研究提出了一个水下6G网络仿真模型，优化传感器、AUV和集线器的部署，并考虑环境因素进行信号传输和网络管理。


<details>
  <summary>Details</summary>
Motivation: 为水下6G网络提供一个优化的传感器、AUV和集线器部署策略，以应对水下环境带来的传输挑战。

Method: 构建包含固定集线器、AUV和传感器节点的网络架构，考虑温度、盐度和电导率等环境参数计算信号衰减和延迟。优化过程采用K-Means聚类、遗传算法（GA）和粒子群优化（PSO）。仿真涵盖多跳传输、簇头选择、队列管理和负载均衡，并通过PyQt5界面可视化，比较了有簇头和无簇头两种场景的性能。

Result: 研究结果表明，通过整合环境条件，水下6G网络架构可以被有效地建模和优化。

Conclusion: 水下环境下的6G网络可以通过考虑环境因素进行有效的建模和优化，从而实现网络组件的理想部署和高效运行。

Abstract: This study presents a simulation model for underwater 6G networks, focusing
on the optimized placement of sensors, AUVs, and hubs. The network architecture
consists of fixed hub stations, mobile autonomous underwater vehicles (AUVs),
and numerous sensor nodes. Environmental parameters such as temperature,
salinity, and conductivity are considered in the transmission of
electromagnetic signals; signal attenuation and transmission delays are
calculated based on physical models. The optimization process begins with
K-Means clustering, followed by sequential application of Genetic Algorithm
(GA) and Particle Swarm Optimization (PSO) to refine the cluster
configurations. The simulation includes key network dynamics such as multi-hop
data transmission, cluster leader selection, queue management, and traffic load
balancing. To compare performance, two distinct scenarios -- one with cluster
leaders and one without -- are modeled and visualized through a PyQt5-based
real-time graphical interface. The results demonstrate that 6G network
architectures in underwater environments can be effectively modeled and
optimized by incorporating environmental conditions.

</details>


### [55] [Network Traffic Classification Using Self-Supervised Learning and Confident Learning](https://arxiv.org/abs/2509.23522)
*Ehsan Eslami,Walaa Hamouda*

Main category: cs.NI

TL;DR: 本文提出一种结合自监督学习（SSL）和置信学习（CL）的新框架，用于解决5G/6G网络流量分类中标记数据稀缺和传统方法失效的问题，实现了高精度分类。


<details>
  <summary>Details</summary>
Motivation: 传统网络流量分类方法（如DPI、端口识别）在加密流量和动态端口分配面前失效。监督学习方法需要大量标记数据，难以获取；无监督学习精度较低。亟需一种在标记数据有限情况下仍能保持高精度的通用解决方案。

Method: 提出的框架分两步：1. 利用自监督学习（如自编码器或Tabular Contrastive Learning）从大量未标记数据生成伪标签，以解决标记数据不足的问题。2. 应用流量自适应的置信学习技术，通过减少噪声来细化伪标签，提高分类精度。

Result: 在ISCX VPN-nonVPN、自生成数据集和UCDavis--QUIC三个数据集上进行了广泛仿真和评估，结果表明该方法在网络流量分类方面比现有最先进技术取得了更高的准确性。

Conclusion: 该框架提供了一个通用的解决方案，最大限度地减少了对大量标记数据的需求，同时仍能提供高准确性，有效应对了5G/6G环境下网络流量分类的挑战。

Abstract: Network traffic classification (NTC) is vital for efficient network
management, security, and performance optimization, particularly with 5G/6G
technologies. Traditional methods, such as deep packet inspection (DPI) and
port-based identification, struggle with the rise of encrypted traffic and
dynamic port allocations. Supervised learning methods provide viable
alternatives but rely on large labeled datasets, which are difficult to acquire
given the diversity and volume of network traffic. Meanwhile, unsupervised
learning methods, while less reliant on labeled data, often exhibit lower
accuracy. To address these limitations, we propose a novel framework that first
leverages Self-Supervised Learning (SSL) with techniques such as autoencoders
or Tabular Contrastive Learning (TabCL) to generate pseudo-labels from
extensive unlabeled datasets, addressing the challenge of limited labeled data.
We then apply traffic-adopted Confident Learning (CL) to refine these
pseudo-labels, enhancing classification precision by mitigating the impact of
noise. Our proposed framework offers a generalizable solution that minimizes
the need for extensive labeled data while delivering high accuracy. Extensive
simulations and evaluations, conducted using three datasets (ISCX VPN-nonVPN,
self-generated dataset, and UCDavis--QUIC), and demonstrate that our method
achieves superior accuracy compared to state-of-the-art techniques in
classifying network traffic.

</details>


### [56] [Sim2Field: End-to-End Development of AI RANs for 6G](https://arxiv.org/abs/2509.23528)
*Russell Ford,Hao Chen,Pranav Madadi,Mandar Kulkarni,Xiaochuan Ma,Daoud Burghal,Guanbo Chen,Yeqing Hu,Chance Tarver,Panagiotis Skrimponis,Vitali Loseu,Yu Zhang,Yan Xin,Yang Li,Jianzhong Zhang,Shubham Khunteta,Yeswanth Guddeti Reddy,Ashok Kumar Reddy Chavva,Mahantesh Kothiwale,Davide Villa*

Main category: cs.NI

TL;DR: 为解决AI无线系统在现实网络中部署的挑战，本文提出结合数字孪生模拟和强大测试平台的方法，并在实际场景中验证AI信道估计算法，实现了高达40%的真实世界吞吐量增益。


<details>
  <summary>Details</summary>
Motivation: 尽管AI/ML在5G/6G蜂窝无线接入网(RAN)中展现巨大潜力，但AI无线系统在实际应用中尚未充分验证。现有仿真训练方法存在“现实差距”，且现场概念验证实施成本高、复杂性大，阻碍了AI无线系统的实地评估。

Method: 本文提出一种将AI引入真实网络的方法，包括：1) 运用详细的数字孪生模拟训练特定站点的AI物理层功能；2) 介绍一个强大的AI-RAN研究测试平台，支持快速原型开发、现场测试和数据收集。

Result: 通过在空中接口使用商用UE评估AI信道估计算法，结果显示在物理层中引入AI可实现高达40%的真实世界吞吐量增益。

Conclusion: 所提出的结合数字孪生和强大测试平台的方法能有效应对AI无线系统在现实网络中的部署挑战，并证明了AI在物理层中可带来显著的实际吞吐量提升。

Abstract: Following state-of-the-art research results, which showed the potential for
significant performance gains by applying AI/ML techniques in the cellular
Radio Access Network (RAN), the wireless industry is now broadly pushing for
the adoption of AI in 5G and future 6G technology. Despite this enthusiasm,
AI-based wireless systems still remain largely untested in the field. Common
simulation methods for generating datasets for AI model training suffer from
"reality gap" and, as a result, the performance of these simulation-trained
models may not carry over to practical cellular systems. Additionally, the cost
and complexity of developing high-performance proof-of-concept implementations
present major hurdles for evaluating AI wireless systems in the field. In this
work, we introduce a methodology which aims to address the challenges of
bringing AI to real networks. We discuss how detailed Digital Twin simulations
may be employed for training site-specific AI Physical (PHY) layer functions.
We further present a powerful testbed for AI-RAN research and demonstrate how
it enables rapid prototyping, field testing and data collection. Finally, we
evaluate an AI channel estimation algorithm over-the-air with a commercial UE,
demonstrating that real-world throughput gains of up to 40% are achievable by
incorporating AI in the physical layer.

</details>


### [57] [Short-Term Guidance Algorithm on a Drone Road System](https://arxiv.org/abs/2509.23794)
*Zhouyu Qu,Andreas Willig,Xiaobing Wu*

Main category: cs.NI

TL;DR: 本文提出了一种无人机道路系统（DRS）的标记语言，并开发了一种短时去中心化贪婪（STDG）引导算法，通过局部信息实现无人机在城市环境中的安全高效交通管理。


<details>
  <summary>Details</summary>
Motivation: 城市环境中无人机使用日益增多，导致碰撞和交通管理挑战，尤其是在拥堵区域。因此，需要结构化的道路系统和有效的引导算法来解决这些问题。

Method: 引入了一种标记语言来描述无人机道路系统（DRS），包括多车道道路和连接车道。提出了一种新型短时去中心化贪婪（STDG）引导算法，该算法仅利用附近无人机的位置和速度信息（通过信标通信），实时做出停车、变道或调整速度等决策，从而实现独立运行而非依赖集中协调。

Result: 仿真结果展示了关键无线和算法参数对无人机碰撞率、平均速度和DRS吞吐量等性能指标的影响。

Conclusion: 所提出的DRS标记语言和去中心化STDG引导算法能够有效管理城市无人机交通，确保安全和效率，并通过仿真得到了验证。

Abstract: Unmanned Aerial Vehicles (UAVs), commonly known as drones, have experienced
expanding use in urban environments in recent years. However, the growing
density of drones raises significant challenges, such as avoiding collisions
and managing air traffic efficiently, especially in congested areas. To address
these issues, a structured road system and an effective guidance algorithm are
essential. In this paper, we introduce a markup language allowing to describe
drone road systems (DRS), in which a road system is given by a set of
individual roads, each of which can have a varying number of lanes. Roads can
be linked through connecting lanes. Furthermore, we propose a novel short-term
decentralized greedy (STDG) guidance algorithm that uses only the position and
speed information of nearby drones -- communicated via periodically transmitted
beacons -- to make real-time decisions such as stopping, changing lanes, or
adjusting speed for the next few seconds. Unlike existing methods that rely on
centralized coordination, our algorithm enables drones to operate independently
while ensuring safety and efficiency. We present simulation results showing the
impact of key wireless and algorithm parameters on performance metrics like the
drone collision rate, average speed and throughput of the drone road system.

</details>


### [58] [A Synergy of Computing Power Networks and Low-Altitude Economy Intelligent Communications: Challenges, Design Principles, and Research Directions](https://arxiv.org/abs/2509.23810)
*Yan Sun,Yinqiu Liu,Shaoyong Guo,Ruichen Zhang,Jiacheng Wang,Xuesong Qiu,Geng Sun,Weifeng Gong,Dusit Niyato,Qihui Wu*

Main category: cs.NI

TL;DR: 本文综述了低空经济（LAE）智能通信与算力网络（CPNs）之间的协同作用，分析了二者如何相互支持，并提出了集成CPN-LAE系统的设计原则和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 低空经济的快速发展催生了对高效智能通信的需求，但LAE智能通信面临计算能力有限、泛化能力不足和需求复杂等挑战。同时，算力网络作为整合计算、网络和存储资源的新范式，也存在部署静态和适应性有限的问题。研究动机在于探索LAE智能通信与算力网络之间的协同潜力，以克服各自挑战，支持新兴的低空服务。

Method: 本文采用综述（survey）的方法，具体包括：1. 分析算力网络如何在空地协同控制、AI训练、通算一体化和泛在低空信息处理等方面支持LAE智能通信。2. 探讨LAE智能通信如何通过移动辅助控制、分布式智能训练、动态路由和网络内空中计算等方面增强算力网络。3. 基于这些洞察，归纳出集成CPN-LAE系统的设计原则和未来研究方向。

Result: 研究结果揭示了LAE智能通信与算力网络之间存在的显著协同作用：算力网络能够有效支撑LAE智能通信的多种需求，而LAE智能通信则能通过其移动性和分布式能力增强算力网络。基于此协同作用，本文提出了集成CPN-LAE系统的设计原则和未来研究方向。

Conclusion: 本工作为构建灵活、适应性强、弹性高的架构奠定了全面的基础，该架构能够利用算力网络和低空经济的协同作用，提供高质量和可持续的低空服务。它为未来集成CPN-LAE系统的发展指明了方向。

Abstract: The rapid development of the Low-Altitude Economy (LAE) has created
opportunities for emerging services such as autonomous aerial transportation,
aerial sensing, and emergency response, all of which rely on efficient and
intelligent communications. However, LAE intelligent communications face
several challenges, including the limited computational capacity of aerial
nodes, the lack of cross-scenario generalization, and the complexity of
heterogeneous demands. Meanwhile, Computing Power Networks (CPNs) have emerged
as a new paradigm for integrating distributed computing, networking, and
storage resources, but they are also constrained by static deployment and
limited adaptability. In this survey, we explore the synergy between LAE
intelligent communications and CPNs. We first analyze how CPNs can support LAE
intelligent communications in areas such as air-ground collaborative control,
AI training, communication-computation co-ptimization, and ubiquitous
low-altitude information processing. Conversely, we discuss how LAE intelligent
communications can enhance CPNs through mobility-assisted control, distributed
intelligent training, dynamic routing, and in-network aerial computing.
Finally, based on these insights, we outline design principles and future
research directions for integrated CPN-LAE systems. This work provides a
comprehensive foundation for building flexible, adaptive, and resilient
architectures that leverage the synergy between CPNs and LAE to deliver
high-quality and sustainable low-altitude services.

</details>


### [59] [Continual Learning to Generalize Forwarding Strategies for Diverse Mobile Wireless Networks](https://arxiv.org/abs/2509.23913)
*Cheonjin Park,Victoria Manfredi,Xiaolan Zhang,Chengyi Liu,Alicia P Wolfe,Dongjin Song,Sarah Tasneem,Bing Wang*

Main category: cs.NI

TL;DR: 针对多跳移动无线网络DRL转发策略的泛化性挑战，提出基于持续学习的框架，通过新特征和微调实现对未见场景的泛化，显著降低延迟并提升投递率。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习（DRL）在多跳移动无线网络转发策略中，对训练环境之外的显著不同场景泛化能力不足。

Method: 提出一个泛化框架，包含开发泛化基础模型和在新场景中微调。通过设计表征网络变化和特征质量的新特征，并开发持续学习（CL）方法，避免“灾难性遗忘”，从而在多样化场景下训练DRL模型。

Result: 经广泛评估（包括两个城市真实场景），该方法对未见移动场景具有泛化能力。与现有启发式策略相比，延迟降低高达78%，投递率提高24%，转发次数持平或略高。

Conclusion: 本研究通过持续学习和特征工程，有效解决了DRL在多跳移动无线网络转发中的泛化性挑战，在多样化和未见场景中展现出优越性能。

Abstract: Deep reinforcement learning (DRL) has been successfully used to design
forwarding strategies for multi-hop mobile wireless networks. While such
strategies can be used directly for networks with varied connectivity and
dynamic conditions, developing generalizable approaches that are effective on
scenarios significantly different from the training environment remains largely
unexplored. In this paper, we propose a framework to address the challenge of
generalizability by (i) developing a generalizable base model considering
diverse mobile network scenarios, and (ii) using the generalizable base model
for new scenarios, and when needed, fine-tuning the base model using a small
amount of data from the new scenarios. To support this framework, we first
design new features to characterize network variation and feature quality,
thereby improving the information used in DRL-based forwarding decisions. We
then develop a continual learning (CL) approach able to train DRL models across
diverse network scenarios without ``catastrophic forgetting.'' Using extensive
evaluation, including real-world scenarios in two cities, we show that our
approach is generalizable to unseen mobility scenarios. Compared to a
state-of-the-art heuristic forwarding strategy, it leads to up to 78% reduction
in delay, 24% improvement in delivery rate, and comparable or slightly higher
number of forwards.

</details>


### [60] [Performance Analysis of Zero-Forcing Beamforming Strategies for the Uplink of an MU-MIMO System with Multi-Antenna Users](https://arxiv.org/abs/2509.23921)
*João Paulo P. G. Marques,Catherine Rosenberg*

Main category: cs.NI

TL;DR: 评估OFDMA MU-MIMO上行链路中三种ZF波束成形策略（CTR1, BD, CTRF），并提出一种高效启发式方法进行资源管理。研究结果表明，最佳策略选择取决于场景和用户数量，且系统参数和功率管理方案影响显著。


<details>
  <summary>Details</summary>
Motivation: 旨在全面评估OFDMA MU-MIMO上行链路中多天线用户的性能，特别关注CTR1、BD和CTRF三种零强制（ZF）波束成形策略，并应对上行链路时间槽（TS）内无线资源管理（RRM）因功耗管理而带来的挑战。

Method: 比较CTR1、BD和CTRF三种ZF波束成形策略在OFDMA MU-MIMO上行链路中的性能。为此，提出一种基于贪婪搜索（greedy-up searches）的启发式方法来高效分配流集合（stream-sets），该方法在整个时间槽（TS）上运行，并考虑公平性、实际的调制编码方案（MCS）和所有RRM过程。

Result: ['在农村宏蜂窝场景中，当用户数量较少时，BD可替代更复杂的CTRF；当用户数量较多时，CTR1可替代CTRF。', '在城市宏蜂窝场景中，CTR1的性能与CTRF相似，可作为其替代方案。', '系统参数对ZF策略的性能有显著影响。', '在更简单的功率管理方案下，BD的性能受损程度高于CTR1和CTRF。']

Conclusion: 在OFDMA MU-MIMO上行链路中，ZF波束成形策略的选择应根据具体场景（农村/城市宏蜂窝）和用户数量进行调整。CTR1在城市场景和用户数较多的农村场景中常作为CTRF的有效替代方案，而BD适用于用户数较少的农村场景。此外，系统参数和功率管理方案对策略性能至关重要。

Abstract: We conduct a comprehensive evaluation of the performance of the uplink of
OFDMA-based MU-MIMO systems with multi-antenna users, for three Zero-Forcing
(ZF) Beamforming (BF) strategies: Coordinated-Transmit-Receive-1 (CTR1), where
only the strongest data stream is enabled per scheduled user; Block
Diagonalization (BD), where all possible streams are enabled per scheduled
user; Coordinated-Transmit-Receive-Flexible (CTRF), which allows a flexible
stream allocation per user. The Radio Resource Management (RRM) of the uplink
of all OFDMA-based systems must be done over an entire Time-Slot (TS) due to
power management, making it challenging. To enable this study, we propose an
efficient heuristic based on greedy-up searches for stream-sets that provides
feasible solutions. It operates over the TS and considers fairness, practical
Modulation and Coding Schemes and all RRM processes. The results show that, for
Rural Macro scenarios, BD (resp. CTR1) could replace the more complex CTRF if
the number of users is small (resp. large), while for Urban Macro scenarios,
CTR1 emerges as an alternative to CTRF due to its similar performance. We also
show that the system parameters can substantially impact the performance of the
ZF strategies and that BD performance is more impaired with a simpler power
management scheme than CTR1 and CTRF.

</details>


### [61] [Beyond Redundancy: Toward Agile Resilience in Optical Networks to Overcome Unpredictable Disasters](https://arxiv.org/abs/2509.24038)
*Toru Mano,Hideki Nishizawa,Takeo Sasai,Soichiroh Usui,Dmitrii Briantcev,Devika Dass,Brandt Bashaw,Eoin Kenny,Marco Ruffini,Yoshiaki Sone,Koichi Takasugi,Daniel Kilper*

Main category: cs.NI

TL;DR: 针对日益增加的灾害不可预测性，本文提出了“敏捷弹性”概念，强调光网络的动态适应性，并利用现场部署系统验证了其赋能技术在六小时内实现快速系统操作的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统光网络弹性依赖于冗余和预计划恢复策略，但近年来气候变化、服务演进和地缘政治风险导致灾害不可预测性增加，降低了传统方法的有效性。

Method: 引入“敏捷弹性”概念，强调跨多个运营商和层面的动态适应性。识别了关键需求、挑战和实现敏捷弹性的赋能技术。使用现场部署的传输系统进行演示和验证。

Result: 通过现场部署系统，在六小时内成功展示了快速系统表征、光路配置和数据库迁移。

Conclusion: 研究结果验证了所提出赋能技术的有效性，并确认了敏捷弹性的可行性。

Abstract: Resilience in optical networks has traditionally relied on redundancy and
pre-planned recovery strategies, both of which assume a certain level of
disaster predictability. However, recent environmental changes such as climate
shifts, the evolution of communication services, and rising geopolitical risks
have increased the unpredictability of disasters, reducing the effectiveness of
conventional resilience approaches. To address this unpredictability, this
paper introduces the concept of agile resilience, which emphasizes dynamic
adaptability across multiple operators and layers. We identify key requirements
and challenges, and present enabling technologies for the realization of agile
resilience. Using a field-deployed transmission system, we demonstrate rapid
system characterization, optical path provisioning, and database migration
within six hours. These results validate the effectiveness of the proposed
enabling technologies and confirm the feasibility of agile resilience.

</details>
