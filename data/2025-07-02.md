<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.CV](#cs.CV) [Total: 79]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.DM](#cs.DM) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data](https://arxiv.org/abs/2507.00152)
*Ekaterina Borisova,Fabio Barth,Nils Feldhus,Raia Abu Ahmad,Malte Ostendorff,Pedro Ortiz Suarez,Georg Rehm,Sebastian Möller*

Main category: cs.CL

TL;DR: 评估大语言模型在表格理解任务中的跨领域和跨模态表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在下游任务中表现出色，但其处理表格数据的效率和能力尚未得到充分探索。

Method: 本文通过跨领域（科学与非科学）和跨模态（图像与文本）评估，研究了文本和多模态大语言模型在表格理解任务上的有效性。具体包括：比较模型在科学与非科学背景表格上的性能，检验其在图像与文本表示表格上的鲁棒性。此外，还进行了可解释性分析以衡量上下文使用和输入相关性。研究引入了TableEval基准测试集，包含3017个来自学术出版物、维基百科和财务报告的表格，每个表格提供图像、字典、HTML、XML和LaTeX五种格式。

Result: 研究发现，大语言模型在不同表格模态之间保持鲁棒性，但在处理科学表格时面临显著挑战。

Conclusion: 大语言模型在表格理解方面对不同模态具有鲁棒性，但在处理科学领域表格时仍存在明显不足。新引入的TableEval基准测试集可支持未来研究。

Abstract: Tables are among the most widely used tools for representing structured data
in research, business, medicine, and education. Although LLMs demonstrate
strong performance in downstream tasks, their efficiency in processing tabular
data remains underexplored. In this paper, we investigate the effectiveness of
both text-based and multimodal LLMs on table understanding tasks through a
cross-domain and cross-modality evaluation. Specifically, we compare their
performance on tables from scientific vs. non-scientific contexts and examine
their robustness on tables represented as images vs. text. Additionally, we
conduct an interpretability analysis to measure context usage and input
relevance. We also introduce the TableEval benchmark, comprising 3017 tables
from scholarly publications, Wikipedia, and financial reports, where each table
is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.
Our findings indicate that while LLMs maintain robustness across table
modalities, they face significant challenges when processing scientific tables.

</details>


### [2] [Prompting as Scientific Inquiry](https://arxiv.org/abs/2507.00163)
*Ari Holtzman,Chenhao Tan*

Main category: cs.CL

TL;DR: 提示词工程是研究和控制大型语言模型（LLM）的主要方法，并解锁了LLM的关键能力。作者主张，它不应被视为“炼金术”，而应被视为一种行为科学，是理解LLM的关键组成部分。


<details>
  <summary>Details</summary>
Motivation: 提示词工程对LLM至关重要，但常被误解为非科学或“炼金术”。本文旨在纠正这种观念，提升提示词工程的科学地位，将其重新定义为研究LLM行为的科学方法。

Method: 文章通过概念论证和类比方法，将大型语言模型视为被训练的复杂有机体，并将提示词工程重新框架为一种“行为科学”，以此阐述其作为探测模型“原生接口”的独特价值。

Result: 研究得出结论，提示词工程并非劣势或权宜之计，而是探测LLM“原生接口”（即语言）的关键手段，应被视为大型语言模型科学研究中不可或缺的核心组成部分。

Conclusion: 提示词工程是理解大型语言模型的正当且关键的科学方法，尤其当把LLM视为一个复杂、被训练的实体时，它是探究其行为和能力的“行为科学”。

Abstract: Prompting is the primary method by which we study and control large language
models. It is also one of the most powerful: nearly every major capability
attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was
first unlocked through prompting. Yet prompting is rarely treated as science
and is frequently frowned upon as alchemy. We argue that this is a category
error. If we treat LLMs as a new kind of complex and opaque organism that is
trained rather than programmed, then prompting is not a workaround: it is
behavioral science. Mechanistic interpretability peers into the neural
substrate, prompting probes the model in its native interface: language. We
contend that prompting is not inferior, but rather a key component in the
science of LLMs.

</details>


### [3] [LineRetriever: Planning-Aware Observation Reduction for Web Agents](https://arxiv.org/abs/2507.00210)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Massimo Caccia,Véronique Eglin,Alexandre Aussem,Jérémy Espinas,Alexandre Lacoste*

Main category: cs.CL

TL;DR: 为解决大型语言模型在网页导航中上下文过长的问题，本文提出LineRetriever方法，利用语言模型识别并检索与未来导航步骤最相关的观察行，从而在保持性能的同时有效减小上下文大小。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在网页导航任务中面临网页上下文（DOM/AxTree）过大超出模型限制的问题。现有方法（如截断或基于嵌入的检索）会丢失关键的页面状态和动作历史信息，这尤其阻碍了网络代理的自适应规划能力，因为规划需要对当前状态有准确理解。研究者假设嵌入模型无法充分捕获与规划相关的信息，尤其是在支持未来动作预测方面。

Method: 本文引入了LineRetriever，一种新颖的检索方法。它利用一个语言模型来识别并检索与未来导航步骤最相关的观察行。与传统仅关注语义相似性的检索方法不同，LineRetriever明确考虑了“规划视界”（planning horizon），优先选择有助于动作预测的元素。

Result: 实验证明，LineRetriever能够有效减少网络代理在每一步的观察信息大小，同时在保持模型上下文限制内，维持了稳定的性能表现。

Conclusion: LineRetriever成功地优化了网页导航任务中用于自适应规划的检索方法，通过智能地选择规划相关的观察信息，解决了上下文过长的问题，并在不牺牲性能的前提下提高了效率。

Abstract: While large language models have demonstrated impressive capabilities in web
navigation tasks, the extensive context of web pages, often represented as DOM
or Accessibility Tree (AxTree) structures, frequently exceeds model context
limits. Current approaches like bottom-up truncation or embedding-based
retrieval lose critical information about page state and action history. This
is particularly problematic for adaptive planning in web agents, where
understanding the current state is essential for determining future actions. We
hypothesize that embedding models lack sufficient capacity to capture
plan-relevant information, especially when retrieving content that supports
future action prediction. This raises a fundamental question: how can retrieval
methods be optimized for adaptive planning in web navigation tasks? In
response, we introduce \textit{LineRetriever}, a novel approach that leverages
a language model to identify and retrieve observation lines most relevant to
future navigation steps. Unlike traditional retrieval methods that focus solely
on semantic similarity, \textit{LineRetriever} explicitly considers the
planning horizon, prioritizing elements that contribute to action prediction.
Our experiments demonstrate that \textit{LineRetriever} can reduce the size of
the observation at each step for the web agent while maintaining consistent
performance within the context limitations.

</details>


### [4] [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)
*Mads Henrichsen,Rasmus Krebs*

Main category: cs.CL

TL;DR: 本文提出一种两阶段方法，利用LLM生成的推理来增强文本分类，通过训练生成模型同时输出推理和标签，在情感分类任务上实现了显著的准确性提升。


<details>
  <summary>Details</summary>
Motivation: 标准分类模型通常将输入直接映射到标签，缺乏显式推理过程，这可能限制其性能、鲁棒性和可解释性。

Method: 本研究采用两阶段方法：
1.  **第一阶段：** 微调Llama-3.2-1B-Instruct模型（Llama-R-Gen）于通用推理数据集syvai/reasoning-gen，使其能够根据问题和答案生成文本推理。
2.  **第二阶段：** 离线使用训练好的Llama-R-Gen生成增强的训练数据集。然后，一个下游的生成模型（同样基于Llama-3.2-1B-Instruct）被训练，该模型仅接收输入文本，并输出生成的推理紧随预测的情感标签。此方法在dair-ai/emotion情感分类数据集上进行了验证。

Result: 与仅输出情感的基线生成模型（Classifier Q->A）相比，训练输出推理和情感的生成模型（Classifier Q->RA）在情感预测准确性上取得了8.7个百分点的显著提升。

Conclusion: 这项工作强调了LLM生成的推理在创建更丰富训练数据集方面的巨大潜力，从而能够提高各种下游NLP任务的性能并提供显式解释。这同时也凸显了推理生成方法的强泛化能力以及显式推理训练的益处。

Abstract: Standard classification models often map inputs directly to labels without
explicit reasoning, potentially limiting their performance, robustness, and
interpretability. This paper introduces a novel two-stage approach to enhance
text classification by leveraging Large Language Model (LLM)-generated
reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model
(henceforth Llama-R-Gen) on a general-purpose reasoning dataset
(syvai/reasoning-gen) to generate textual reasoning (R) given a question and
its answer. In the second stage, this generally trained Llama-R-Gen is used
offline to create an augmented training dataset for a downstream generative
model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the
input text (Q) and is trained to output the generated reasoning (R) immediately
followed by the predicted emotion (A). We demonstrate this methodology on the
dair-ai/emotion dataset for emotion classification. Our experiments show that
the generative model trained to output reasoning and the emotion (Classifier
Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy
(for emotion prediction) compared to a baseline generative model trained solely
to output the emotion (Classifier Q->A), highlighting the strong generalization
capabilities of the reasoning generation and the benefit of explicit reasoning
training. This work underscores the potential of LLM-generated reasonings for
creating richer training datasets, thereby improving the performance of diverse
downstream NLP tasks and providing explicit explanations.

</details>


### [5] [Towards Style Alignment in Cross-Cultural Translation](https://arxiv.org/abs/2507.00216)
*Shreya Havaldar,Adam Stein,Eric Wong,Lyle Ungar*

Main category: cs.CL

TL;DR: 大语言模型在跨文化风格翻译中表现不佳，常倾向中性且在非西方语言中更差。本文提出RASTA方法，利用习得的风格概念，改善大语言模型的风格对齐，使其能更好地传达文化交流规范。


<details>
  <summary>Details</summary>
Motivation: 成功的交流依赖于说话者意图风格与听者理解风格的对齐。然而，文化差异常导致风格错位（例如，翻译中礼貌性丧失）。当前大语言模型在翻译风格方面存在缺陷，表现为翻译倾向中性化，且在非西方语言中的表现更差。

Method: 提出RASTA（Retrieval-Augmented STylistic Alignment）方法。该方法利用习得的风格概念，促使大语言模型翻译能恰当地传达文化交流规范并对齐风格。

Result: RASTA方法能有效缓解大语言模型在风格翻译上的缺陷，即纠正其翻译偏向中性化的倾向，并改善其在非西方语言中的表现。

Conclusion: RASTA提供了一种有效途径，通过增强大语言模型对文化交流规范和风格的理解与表达，来解决其在跨文化风格翻译中的不足。

Abstract: Successful communication depends on the speaker's intended style (i.e., what
the speaker is trying to convey) aligning with the listener's interpreted style
(i.e., what the listener perceives). However, cultural differences often lead
to misalignment between the two; for example, politeness is often lost in
translation. We characterize the ways that LLMs fail to translate style -
biasing translations towards neutrality and performing worse in non-Western
languages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic
Alignment), a method that leverages learned stylistic concepts to encourage LLM
translation to appropriately convey cultural communication norms and align
style.

</details>


### [6] [Linearly Decoding Refused Knowledge in Aligned Language Models](https://arxiv.org/abs/2507.00239)
*Aryan Shrivastava,Ari Holtzman*

Main category: cs.CL

TL;DR: 大多数LMs会拒绝有害请求，但越狱可绕过。本研究发现，即使被拒绝的信息，仍可从LM隐藏状态中被线性解码，且这些信息在模型中仍然活跃并有影响力。


<details>
  <summary>Details</summary>
Motivation: 研究越狱提示所能访问的信息，在多大程度上仍可通过线性探针从语言模型的隐藏状态中被解码，并探讨指令微调是否彻底消除了这些信息，或仅是抑制了它们的直接表达。

Method: 通过在语言模型（包括基础模型和指令微调模型）的隐藏状态上训练线性探针，来解码被模型拒绝但能通过越狱提示获取的信息。研究探针在不同模型间的迁移能力，并分析探针预测值与模型生成行为（如成对比较）的相关性，以评估这些信息的活跃程度。

Result: 研究发现，大量被初始拒绝的信息可以被线性探针高效解码，例如对国家平均智商的预测相关性超过0.8。此外，在基础模型上训练的探针可以迁移到指令微调模型，揭示了被拒绝的信息表征在指令微调后仍然存在。更重要的是，这些信息并非简单残留，而是被模型主动利用，其探针预测值与LM生成的成对比较相关，表明其对模型行为仍有潜在影响。

Conclusion: 指令微调并未完全消除或重新定位表示空间中的有害信息，而只是抑制了其直接表达。这些信息仍然可以通过线性方式访问，并在模型的下游行为中产生间接影响。

Abstract: Most commonly used language models (LMs) are instruction-tuned and aligned
using a combination of fine-tuning and reinforcement learning, causing them to
refuse users requests deemed harmful by the model. However, jailbreak prompts
can often bypass these refusal mechanisms and elicit harmful responses. In this
work, we study the extent to which information accessed via jailbreak prompts
is decodable using linear probes trained on LM hidden states. We show that a
great deal of initially refused information is linearly decodable. For example,
across models, the response of a jailbroken LM for the average IQ of a country
can be predicted by a linear probe with Pearson correlations exceeding $0.8$.
Surprisingly, we find that probes trained on base models (which do not refuse)
sometimes transfer to their instruction-tuned versions and are capable of
revealing information that jailbreaks decode generatively, suggesting that the
internal representations of many refused properties persist from base LMs
through instruction-tuning. Importantly, we show that this information is not
merely "leftover" in instruction-tuned models, but is actively used by them: we
find that probe-predicted values correlate with LM generated pairwise
comparisons, indicating that the information decoded by our probes align with
suppressed generative behavior that may be expressed more subtly in other
downstream tasks. Overall, our results suggest that instruction-tuning does not
wholly eliminate or even relocate harmful information in representation
space-they merely suppress its direct expression, leaving it both linearly
accessible and indirectly influential in downstream behavior.

</details>


### [7] [The Algebraic Structure of Morphosyntax](https://arxiv.org/abs/2507.00244)
*Isabella Senturia,Matilde Marcolli*

Main category: cs.CL

TL;DR: 本文在Merge和强极简主义框架下，提出了一个基于Magma和Operad的形态-句法接口数学模型，并重新诠释了分布式形态学操作。


<details>
  <summary>Details</summary>
Motivation: 在Merge和强极简主义假设下，建立形态-句法接口的数学模型。

Method: 将形态学建模为具有组合属性的形态树的Magma（不含运动），并通过扩展形态树集合实现余积分解。将形态句法树建模为Operad上的代数，并建立Operad上代数之间的对应关系，以此描述形态句法结构形成。

Result: 成功构建了一个描述形态-句法接口的数学模型，该模型能通过Operad对应关系和形态学余积描述形态句法树的结构形成。在此框架下，分布式形态学中的特定操作被重新诠释为允许灵活调整句法与形态边界的变换。

Conclusion: 该数学模型提供了一种精确描述形态-句法接口的方法，并为理解分布式形态学操作以及句法和形态之间的动态边界提供了新的视角。

Abstract: Within the context of the mathematical formulation of Merge and the Strong
Minimalist Thesis, we present a mathematical model of the morphology-syntax
interface. In this setting, morphology has compositional properties responsible
for word formation, organized into a magma of morphological trees. However,
unlike syntax, we do not have movement within morphology. A coproduct
decomposition exists, but it requires extending the set of morphological trees
beyond those which are generated solely by the magma, to a larger set of
possible morphological inputs to syntactic trees. These participate in the
formation of morphosyntactic trees as an algebra over an operad, and a
correspondence between algebras over an operad. The process of structure
formation for morphosyntactic trees can then be described in terms of this
operadic correspondence that pairs syntactic and morphological data and the
morphology coproduct. We reinterpret in this setting certain operations of
Distributed Morphology as transformation that allow for flexibility in moving
the boundary between syntax and morphology within the morphosyntactic objects.

</details>


### [8] [EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning](https://arxiv.org/abs/2507.00246)
*Sanchit Ahuja,Praneetha Vaddamanu,Barun Patra*

Main category: cs.CL

TL;DR: 研究发现，使用非英语语言进行推理能有效减少大型语言模型的token消耗，同时保持甚至提高准确性，揭示了多语言推理的巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管许多语言推理模型（LRMs）是基于多语言数据预训练的，但现有研究主要集中在英语。本研究旨在探讨英语是否是推理任务中最具token效率的语言。

Method: 评估了三款开源LRM（DeepSeek R1, Qwen 2.5, Qwen 3），在四个数学数据集上，使用七种不同语系的语言进行实验。

Result: 研究发现，使用非英语语言进行推理不仅能减少token使用，还能保持甚至提高准确性。这些益处即使在将推理过程翻译成英语后依然存在，表明是推理行为的深层转变而非表层语言效应。改进的程度与模型的跨语言能力相关。

Conclusion: 研究结果促使我们对语言模型推理采取更广阔的视角，强调了多语言推理的巨大潜力以及强大多语言基础的重要性。

Abstract: Despite recent advances in Language Reasoning Models (LRMs), most research
focuses solely on English, even though many models are pretrained on
multilingual data. In this work, we investigate: Is English the most
token-efficient language for reasoning? We evaluate three open-source RLMs:
DeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven
typologically diverse languages. We find that reasoning in non-English
languages not only reduces token usage, but also preserves accuracy. These
gains persist even after translating the reasoning traces into English,
suggesting genuine shifts in reasoning behavior rather than surface-level
linguistic effects. The extent of improvement, however, depends on the models
multilingual strength. Our findings motivate a broader view of reasoning in
language models, highlighting the potential of multilingual reasoning and the
importance of strong multilingual foundations. The code for our work can be
found: https://github.com/microsoft/EfficientXLang.

</details>


### [9] [Impact of Fine-Tuning Methods on Memorization in Large Language Models](https://arxiv.org/abs/2507.00258)
*Jie Hou,Chuxiong Wu,Lannan Luo,Qiang Zeng*

Main category: cs.CL

TL;DR: 研究发现，在大型语言模型微调过程中，相比基于参数的微调，基于提示（Prompt-based）的微调在保持竞争性能的同时，展现出更低的隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 随着预训练大语言模型（LLMs）的普及和微调范式成为主流，微调过程中因模型记忆化而产生的隐私泄露风险未得到充分关注。

Method: 研究通过对流行的微调方法进行分类，并采用成员推断攻击（MIAs）来评估不同微调方法对模型记忆化及其隐私风险的影响。

Result: 结果显示，基于提示的微调方法在达到与基于参数的微调相当的性能时，对成员推断攻击的脆弱性更低；且无论模型规模大小，基于提示的方法都能保持较低的记忆化水平。

Conclusion: 研究表明，基于参数的微调更容易泄露私有信息，而基于提示的微调则是一种更具隐私保护性的选择。

Abstract: As the capabilities of pre-trained large language models (LLMs) continue to
advance, the "pre-train and fine-tune" paradigm has become increasingly
mainstream, leading to the development of various fine-tuning methods. However,
the privacy risks arising from memorization during fine-tuning have received
relatively little attention. To address this gap, we categorize popular
fine-tuning approaches and assess their impact on memorization through the lens
of membership inference attacks (MIAs). Our results show that, compared to
parameter-based fine-tuning, prompt-based fine-tuning achieves competitive
performance while exhibiting lower vulnerability to MIAs. Furthermore,
prompt-based methods maintain low memorization regardless of model scale. These
findings suggest that parameter-based fine-tuning is more prone to leaking
private information, whereas prompt-based fine-tuning serves as a more
privacy-preserving option.

</details>


### [10] [Natural language processing for African languages](https://arxiv.org/abs/2507.00297)
*David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本研究旨在解决撒哈拉以南非洲低资源语言在NLP中的挑战，通过分析和整理高质量语料，开发大规模标注数据集，并验证多语言预训练语言模型（PLM）在这些语言上的有效性，从而提升NLP性能。


<details>
  <summary>Details</summary>
Motivation: 多语言NLP模型在低资源语言（如撒哈拉以南非洲语言）上存在挑战，主要原因包括：数据稀缺、数据质量差（噪音多）、缺乏标注数据集以进行性能评估，导致这些语言在NLP研究中代表性不足。

Method: ['分析公开语料库中的噪音，并整理构建高质量语料库。', '经验性地展示词嵌入的局限性，并探索多语言预训练语言模型（PLM）在未见和低资源语言中的应用潜力。', '研究如何使用少量单语文本来适应和专门化多语言PLM以应用于未见过的非洲语言。', '为21种非洲语言的命名实体识别和机器翻译任务开发大规模人工标注数据集。', '在监督、弱监督和迁移学习设置下，使用最先进的方法进行广泛的实证评估。']

Result: ['证明了词嵌入语义表示的质量不仅取决于数据量，还取决于预训练数据的质量。', '经验性地展示了多语言PLM在未见过和低资源语言场景下优于词嵌入的优势。', '成功为21种非洲语言的命名实体识别和机器翻译任务创建了大规模人工标注数据集。']

Conclusion: 本研究通过构建高质量语料库、揭示数据质量的重要性、验证多语言PLM在低资源语言中的有效性，并提供宝贵的大规模人工标注数据集，有效解决了非洲语言在NLP研究中代表性不足的问题，为提升其NLP性能奠定了基础。

Abstract: Recent advances in word embeddings and language models use large-scale,
unlabelled data and self-supervised learning to boost NLP performance.
Multilingual models, often trained on web-sourced data like Wikipedia, face
challenges: few low-resource languages are included, their data is often noisy,
and lack of labeled datasets makes it hard to evaluate performance outside
high-resource languages like English. In this dissertation, we focus on
languages spoken in Sub-Saharan Africa where all the indigenous languages in
this region can be regarded as low-resourced in terms of the availability of
labelled data for NLP tasks and unlabelled data found on the web. We analyse
the noise in the publicly available corpora, and curate a high-quality corpus,
demonstrating that the quality of semantic representations learned in word
embeddings does not only depend on the amount of data but on the quality of
pre-training data. We demonstrate empirically the limitations of word
embeddings, and the opportunities the multilingual pre-trained language model
(PLM) offers especially for languages unseen during pre-training and
low-resource scenarios. We further study how to adapt and specialize
multilingual PLMs to unseen African languages using a small amount of
monolingual texts. To address the under-representation of the African languages
in NLP research, we developed large scale human-annotated labelled datasets for
21 African languages in two impactful NLP tasks: named entity recognition and
machine translation. We conduct an extensive empirical evaluation using
state-of-the-art methods across supervised, weakly-supervised, and transfer
learning settings.

</details>


### [11] [Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones](https://arxiv.org/abs/2507.00322)
*Daking Rai,Samuel Miller,Kevin Moran,Ziyu Yao*

Main category: cs.CL

TL;DR: 语言模型在简单语法任务上存在缺陷，研究揭示了其内部“健全”与“缺陷”机制的冲突是原因。提出RASteer方法，通过增强可靠组件贡献，显著提升模型在特定任务（如平衡括号）上的表现，且不影响通用编码能力。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型在编码能力上取得显著进展，但在生成平衡括号等简单句法任务上仍表现不佳。本研究旨在理解并缓解这些错误的根本机制。

Method: 研究分析了不同规模语言模型（124M-7B）中错误持续存在的潜在机制，发现模型依靠独立预测的内部组件（注意力头和FF神经元）。这些组件有些是可靠的“健全机制”，有些则是引入噪音的“缺陷机制”，错误发生在“缺陷机制”主导预测时。基于此洞察，提出RASteer转向方法，系统地识别并增强可靠组件的贡献。

Result: RASteer显著改善了模型在平衡括号任务上的表现，将部分模型的准确率从0%提升至近100%，且未损害模型的通用编码能力。该方法在算术推理任务中也显示出普适性，性能提升高达约20%。

Conclusion: 本研究揭示了语言模型因内部组件冲突而产生语法错误的原因，并提出RASteer方法，通过增强可靠组件的贡献，有效纠正了这些错误，显著提升了模型在特定任务上的表现，并证明了其广泛适用性。

Abstract: Despite remarkable advances in coding capabilities, language models (LMs)
still struggle with simple syntactic tasks such as generating balanced
parentheses. In this study, we investigate the underlying mechanisms behind the
persistence of these errors across LMs of varying sizes (124M-7B) to both
understand and mitigate the errors. Our study reveals that LMs rely on a number
of components (attention heads and FF neurons) that independently make their
own predictions. While some components reliably promote correct answers across
a generalized range of inputs (i.e., implementing "sound mechanisms''), others
are less reliable and introduce noise by promoting incorrect tokens (i.e.,
implementing "faulty mechanisms''). Errors occur when the faulty mechanisms
overshadow the sound ones and dominantly affect the predictions. Motivated by
this insight, we introduce RASteer, a steering method to systematically
identify and increase the contribution of reliable components for improving
model performance. RASteer substantially improves performance on balanced
parentheses tasks, boosting accuracy of some models from $0$% to around $100$%
without impairing the models' general coding ability. We further demonstrate
its broader applicability in arithmetic reasoning tasks, achieving performance
gains of up to around $20$%.

</details>


### [12] [Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios](https://arxiv.org/abs/2507.00330)
*Mohna Chakraborty,Adithya Kulkarni,Qi Li*

Main category: cs.CL

TL;DR: COLDSELECT是一种针对冷启动场景下基于提示语方法的联合词形选择和实例选择方法，通过建模数据多样性，有效降低不确定性并提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 基于提示语的方法对模板、词形和少样本实例的选择敏感，在无标签的冷启动设置中尤其明显。现有研究忽视了实例与词形间的依赖关系，即实例-标签概率取决于词形词元在嵌入空间中的接近度。

Method: 提出COLDSELECT，一种联合词形和实例选择方法，旨在建模数据多样性。该方法将预训练语言模型（PLM）的词汇和掩码嵌入($h_{[MASK]}$)映射到共享空间，并通过降维和聚类实现高效多样的选择，以优化最小不确定性和最大多样性。

Result: 在八个基准测试上的实验表明，COLDSELECT在降低不确定性和增强泛化能力方面表现出色，在冷启动场景的词形和少样本实例选择上优于基线方法。

Conclusion: COLDSELECT通过有效捕捉数据关系和联合优化词形与实例选择，成功解决了冷启动设置中基于提示语方法的挑战，显著提升了性能和泛化能力。

Abstract: Prompt-based methods leverage the knowledge of pre-trained language models
(PLMs) trained with a masked language modeling (MLM) objective; however, these
methods are sensitive to template, verbalizer, and few-shot instance selection,
particularly in cold-start settings with no labeled data. Existing studies
overlook the dependency between instances and verbalizers, where instance-label
probabilities depend on verbalizer token proximity in the embedding space. To
address this, we propose COLDSELECT, a joint verbalizer and instance selection
approach that models data diversity. COLDSELECT maps PLM vocabulary and
$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction
and clustering to ensure efficient and diverse selection. By optimizing for
minimal uncertainty and maximal diversity, COLDSELECT captures data
relationships effectively. Experiments on eight benchmarks demonstrate
COLDSELECT's superiority in reducing uncertainty and enhancing generalization,
outperforming baselines in verbalizer and few-shot instance selection for
cold-start scenarios.

</details>


### [13] [Question Decomposition for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.00355)
*Paul J. L. Ammann,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: 针对多跳问题中标准RAG的检索局限性，本文提出一种结合大模型问题分解和重排的RAG管道，有效提升了多跳问答的检索性能和答案准确性。


<details>
  <summary>Details</summary>
Motivation: 标准RAG在处理多跳问题时面临挑战，因为相关事实往往分散在多个文档中，导致难以检索到足够信息，从而影响答案生成。

Method: 本文提出一种RAG管道，包含三个步骤：(i) 使用LLM将原始查询分解为子问题；(ii) 为每个子问题检索相关段落；(iii) 对合并后的候选池进行重排，以提高检索证据的覆盖率和精度。该方法利用现成重排器与LLM驱动的问题分解相结合，无需额外训练。

Result: 在MultiHop-RAG和HotpotQA数据集上的评估显示，与标准RAG基线相比，本文方法在检索（MRR@10）方面提升了36.7%，在答案准确率（F1）方面提升了11.6%。

Conclusion: 结合LLM驱动的问题分解与重排技术能有效弥补RAG在处理多跳问题时的检索差距，为多跳问答提供了一种实用且显著的增强方案，无需额外训练或专业索引。

Abstract: Grounding large language models (LLMs) in verifiable external sources is a
well-established strategy for generating reliable answers. Retrieval-augmented
generation (RAG) is one such approach, particularly effective for tasks like
question answering: it retrieves passages that are semantically related to the
question and then conditions the model on this evidence. However, multi-hop
questions, such as "Which company among NVIDIA, Apple, and Google made the
biggest profit in 2023?," challenge RAG because relevant facts are often
distributed across multiple documents rather than co-occurring in one source,
making it difficult for standard RAG to retrieve sufficient information. To
address this, we propose a RAG pipeline that incorporates question
decomposition: (i) an LLM decomposes the original query into sub-questions,
(ii) passages are retrieved for each sub-question, and (iii) the merged
candidate pool is reranked to improve the coverage and precision of the
retrieved evidence. We show that question decomposition effectively assembles
complementary documents, while reranking reduces noise and promotes the most
relevant passages before answer generation. Although reranking itself is
standard, we show that pairing an off-the-shelf cross-encoder reranker with
LLM-driven question decomposition bridges the retrieval gap on multi-hop
questions and provides a practical, drop-in enhancement, without any extra
training or specialized indexing. We evaluate our approach on the MultiHop-RAG
and HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy
(F1: +11.6%) over standard RAG baselines.

</details>


### [14] [Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics](https://arxiv.org/abs/2507.00380)
*Vojtěch Lanz,Jan Hajič jr*

Main category: cs.CL

TL;DR: 本文利用嵌套分层Pitman-Yor语言模型对格里高利圣咏旋律进行无监督优化分割，实现了模态分类的最新性能，并发现其与记忆效率的关联，但指出这种分割并非传统意义上的“集锦式编曲”。


<details>
  <summary>Details</summary>
Motivation: 格里高利圣咏旋律由片段构建的“集锦式编曲”理论虽受批评但片段重用现象普遍存在，且近期实证结果表明分割在模态分类上优于音乐理论特征。受圣咏以记忆方式学习的启发，旨在寻找一种最优的无监督分割方法。

Method: 使用嵌套分层Pitman-Yor语言模型来寻找圣咏旋律的最优无监督分割。通过模拟僧侣记忆礼仪手稿中的旋律来寻找模态分类与记忆效率之间的实证联系。

Result: 找到的分割方法在模态分类中取得了最先进的性能。发现了模态分类与记忆效率之间存在经验证据的联系。观察到旋律的开头和结尾处有更多程式化的区域，这与模态在演奏中的实际作用相对应。

Conclusion: 研究结果表明，即使是记忆最优的分割也并非传统意义上的“集锦式编曲”。

Abstract: The idea that Gregorian melodies are constructed from some vocabulary of
segments has long been a part of chant scholarship. This so-called
"centonisation" theory has received much musicological criticism, but frequent
re-use of certain melodic segments has been observed in chant melodies, and the
intractable number of possible segmentations allowed the option that some
undiscovered segmentation exists that will yet prove the value of
centonisation, and recent empirical results have shown that segmentations can
outperform music-theoretical features in mode classification. Inspired by the
fact that Gregorian chant was memorised, we search for an optimal unsupervised
segmentation of chant melody using nested hierarchical Pitman-Yor language
models. The segmentation we find achieves state-of-the-art performance in mode
classification. Modeling a monk memorising the melodies from one liturgical
manuscript, we then find empirical evidence for the link between mode
classification and memory efficiency, and observe more formulaic areas at the
beginnings and ends of melodies corresponding to the practical role of modality
in performance. However, the resulting segmentations themselves indicate that
even such a memory-optimal segmentation is not what is understood as
centonisation.

</details>


### [15] [Causal Prompting for Implicit Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2507.00389)
*Jing Ren,Wenhao Zhou,Bowen Li,Mujie Liu,Nguyen Linh Dan Le,Jiade Cen,Liping Chen,Ziqi Xu,Xiwei Xu,Xiaodong Li*

Main category: cs.CL

TL;DR: 提出CAPITAL因果提示框架，通过前门调整解决LLM在隐式情感分析中CoT推理的偏差问题，显著提升准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 隐式情感分析需要深度推理。现有基于LLM的提示方法，特别是CoT推理，常未评估因果有效性，易受内部偏差和虚假关联影响。

Method: 提出CAPITAL框架，将前门调整融入CoT推理。它将整体因果效应分解为提示对推理链的影响和推理链对输出的影响，通过基于编码器的聚类、NWGM近似和对比学习进行估计。

Result: 实验证明，CAPITAL在基准ISA数据集上，用三种LLM持续优于现有提示基线，在准确性和鲁棒性方面，尤其是在对抗条件下，表现突出。

Conclusion: 该工作为将因果推理整合到LLM提示中提供了一种原则性方法，突出了其在偏差感知情感推理中的益处。

Abstract: Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied
rather than explicitly stated, requiring models to perform deeper reasoning
over subtle contextual cues. While recent prompting-based methods using Large
Language Models (LLMs) have shown promise in ISA, they often rely on majority
voting over chain-of-thought (CoT) reasoning paths without evaluating their
causal validity, making them susceptible to internal biases and spurious
correlations. To address this challenge, we propose CAPITAL, a causal prompting
framework that incorporates front-door adjustment into CoT reasoning. CAPITAL
decomposes the overall causal effect into two components: the influence of the
input prompt on the reasoning chains, and the impact of those chains on the
final output. These components are estimated using encoder-based clustering and
the NWGM approximation, with a contrastive learning objective used to better
align the encoder's representation with the LLM's reasoning space. Experiments
on benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently
outperforms strong prompting baselines in both accuracy and robustness,
particularly under adversarial conditions. This work offers a principled
approach to integrating causal inference into LLM prompting and highlights its
benefits for bias-aware sentiment reasoning. The source code and case study are
available at: https://github.com/whZ62/CAPITAL.

</details>


### [16] [Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions](https://arxiv.org/abs/2507.00439)
*Gauri Kambhatla,Sanjana Gautam,Angela Zhang,Alex Liu,Ravi Srinivasan,Junyi Jessy Li,Matthew Lease*

Main category: cs.CL

TL;DR: 本研究展示了通过简单监督显著提升语言模型对不同人群主观问题的对齐能力，并提供了实践指导和研究基准。


<details>
  <summary>Details</summary>
Motivation: 准确预测不同人群如何回答主观问题具有重要价值。

Method: 通过相对简单的监督方法，在涵盖多种主题的三个数据集上，测量并提升语言模型与多样化人群的对齐。除了评估平均性能，还报告了对齐度在特定群体间的差异。研究对多种大型语言模型（LLMs）和提示策略进行了评估。

Result: 简单监督能显著提高语言模型与不同人群的对齐能力。对齐度在特定群体间存在差异。该方法具有简单性和通用性，易于采纳。

Conclusion: 简单监督是一种有效提升语言模型对齐不同人群主观问题能力的方法。本研究提供了在实践中何时使用或不使用该方法的有用指导，并通过广泛评估和开源，为未来研究提供了有用的基准。

Abstract: The ability to accurately predict how different population groups would
answer subjective questions would have great value. In this work, we show that
use of relatively simple supervision can greatly improve language model
alignment with diverse population groups, as measured over three datasets
spanning various topics. Beyond evaluating average performance, we also report
how alignment varies across specific groups. The simplicity and generality of
our approach promotes easy adoption, while our broad findings provide useful
guidance for when to use or not use our approach in practice. By conducting
evaluation over many LLMs and prompting strategies, along with open-sourcing
our work, we provide a useful benchmark to stimulate future research.

</details>


### [17] [Pitfalls of Evaluating Language Models with Open Benchmarks](https://arxiv.org/abs/2507.00460)
*Md. Najib Hasan,Mohammad Fakhruddin Babar,Souvika Sarkar,Monowar Hasan,Santu Karmaker*

Main category: cs.CL

TL;DR: 研究揭示开放式大语言模型基准测试（如HELM）的漏洞，证明通过在公开测试集上微调的“作弊”模型可获得高排名，但实际效用低，呼吁重新评估现有基准实践。


<details>
  <summary>Details</summary>
Motivation: 开放式大语言模型基准测试（如HELM和BIG-bench）虽旨在促进公平比较和可重复性，但其开放性也引入了关键且未充分探索的隐患。本研究旨在揭露这些弱点。

Method: 通过系统地构建“作弊”模型——即在公开测试集上直接进行微调的BART、T5和GPT-2的较小变体。

Result: 这些“作弊”模型在一个著名的开放式、整体性基准测试（HELM）上获得了最高排名，尽管它们泛化能力差且实际效用有限。

Conclusion: 开放基准上的高排行榜表现不一定能反映实际效果；私人或动态基准必须补充开放评估以保障完整性；以及，对当前基准测试实践进行根本性重新评估对于确保稳健和值得信赖的语言模型评估至关重要。

Abstract: Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer
standardized, transparent protocols that facilitate the fair comparison,
reproducibility, and iterative advancement of Language Models (LMs). However,
their openness also introduces critical and underexplored pitfalls. This study
exposes these weaknesses by systematically constructing ``cheating'' models --
smaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets
-- which achieve top rankings on a prominent open, holistic benchmark (HELM)
despite poor generalization and limited practical utility. Our findings
underscore three key insights: \ca high leaderboard performance on open
benchmarks may not always reflect real-world effectiveness; \cb private or
dynamic benchmarks must complement open evaluations to safeguard integrity; and
\cc a fundamental reevaluation of current benchmarking practices is essential
to ensure robust and trustworthy LM assessments.

</details>


### [18] [TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search](https://arxiv.org/abs/2507.00509)
*To Eun Kim,João Coelho,Gbemileke Onilude,Jai Singh*

Main category: cs.CL

TL;DR: 该研究提出了一个模块化框架，通过广告重写器和分类器，在基于RAG的对话系统中实现广告的无缝集成和检测，旨在提升广告融入度并保持用户体验。


<details>
  <summary>Details</summary>
Motivation: 在生成式搜索中，广告集成模糊了信息与推广内容的界限，引发了用户对透明度和信任的担忧。因此，需要在利用商业机会的同时，解决用户体验和信任问题。

Method: 本文提出了一个包含广告重写器（用于无缝集成）和鲁棒广告分类器（用于检测）的模块化管理流程。分类器利用合成数据和课程学习进行训练。通过分类器指导两种广告集成策略：对重写器进行监督微调，以及采用Best-of-N采样选择最难被检测的广告响应。

Result: 实验结果表明，在合成数据上训练的广告分类器表现出强大的检测性能。此外，通过分类器引导的优化（微调和Best-of-N采样）显著提升了广告的隐蔽性，实现了更无缝的集成。

Conclusion: 这些发现为开发更复杂的广告感知生成式搜索系统和鲁棒的广告分类器贡献了一个对抗性协同进化框架。

Abstract: As conversational search engines increasingly adopt generation-based
paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG), the integration of advertisements into generated responses
presents both commercial opportunities and challenges for user experience.
Unlike traditional search, where advertisements are clearly delineated,
generative systems blur the boundary between informational content and
promotional material, raising concerns around transparency and trust. In this
work, we propose a modular pipeline for advertisement management in RAG-based
conversational systems, consisting of an ad-rewriter for seamless ad
integration and a robust ad-classifier for detection. We leverage synthetic
data to train high-performing classifiers, which are then used to guide two
complementary ad-integration strategies: supervised fine-tuning of the
ad-rewriter and a best-of-N sampling approach that selects the least detectable
ad-integrated response among multiple candidates. Our evaluation focuses on two
core questions: the effectiveness of ad classifiers in detecting diverse ad
integration strategies, and the training methods that best support coherent,
minimally intrusive ad insertion. Experimental results show that our
ad-classifier, trained on synthetic advertisement data inspired by marketing
strategies and enhanced through curriculum learning, achieves robust detection
performance. Additionally, we demonstrate that classifier-guided optimization,
through both fine-tuning and best-of-N sampling, significantly improves ad
stealth, enabling more seamless integration. These findings contribute an
adversarial co-evolution framework for developing more sophisticated ad-aware
generative search systems and robust ad classifiers.

</details>


### [19] [NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data](https://arxiv.org/abs/2507.00534)
*Tahir Javed,Kaushal Bhogale,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: Nirantar是一个评估多语言多领域ASR持续学习的综合框架，利用真实世界增量数据克服现有模拟数据的局限性，并揭示现有CL方法在复杂场景下的不足，强调需开发更鲁棒的策略。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习（CL）研究依赖模拟数据，未能充分反映多语言和多领域ASR中真实世界动态、非均匀的语言和领域变化挑战。因此，需要一个能够更真实反映这些挑战的评估框架。

Method: 引入Nirantar框架，该框架利用在印度22种语言和208个地区通过自然事件逐步收集的共计3250小时（其中1720小时为新增）人工转录语音数据。它支持在语言增量学习（LIL）、领域增量学习（DIL）以及新颖的语言-领域增量学习（LIDIL）场景下进行评估。

Result: 通过Nirantar框架对现有持续学习方法进行评估，结果显示没有单一方法能够在所有评估场景中持续表现良好。

Conclusion: 现有持续学习策略不足以应对多语言、多领域ASR中的真实世界动态变化挑战，因此迫切需要开发更鲁棒和有效的持续学习策略。

Abstract: We introduce Nirantar, a comprehensive framework for evaluating continual
learning (CL) in multilingual and multi-domain ASR. Designed to reflect
real-world CL challenges, Nirantar leverages data collected incrementally
across 22 languages and 208 districts in India through natural episodes. This
enables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),
and the novel Language-Incremental Domain-Incremental Learning (LIDIL)
scenarios. Unlike prior work that relies on simulated episodes, Nirantar
presents dynamic, non-uniform language and domain shifts, making it an ideal
testbed for CL research. With 3250 hours of human-transcribed speech, including
1720 hours newly introduced in this work, our framework enables systematic
benchmarking of CL methods. We evaluate existing approaches and demonstrate
that no single method performs consistently well, underscoring the need for
more robust CL strategies.

</details>


### [20] [Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction](https://arxiv.org/abs/2507.00540)
*Shixiao Wang,Yifan Zhuang,Runsheng Zhang,Zhijun Song*

Main category: cs.CL

TL;DR: 本文提出一种基于Capsule Networks的用户语义意图建模算法，通过动态路由机制提升人机交互中意图识别的准确性，并在公开数据集上表现优于主流模型。


<details>
  <summary>Details</summary>
Motivation: 人机交互中意图识别的准确性不足。

Method: 提出一种基于Capsule Networks的用户语义意图建模算法。该方法使用向量化胶囊结构表示文本语义特征，通过动态路由机制捕捉语义实体间的层级关系和部分-整体结构。模型采用卷积特征提取模块作为低级编码器，并通过迭代路由形成高级抽象意图表示。损失函数中引入基于裕度的机制以增强意图类别区分能力。

Result: 在公开自然语言理解数据集上，所提出的模型在准确率、F1-score和意图检测率方面均优于传统方法及其他深度学习结构。研究还分析了动态路由迭代次数对模型性能的影响，并提供了损失函数的收敛曲线，验证了方法的稳定性和有效性。

Conclusion: 本研究提出一种新的结构化建模方法，有效提升了复杂语义条件下的意图识别能力。

Abstract: This paper proposes a user semantic intent modeling algorithm based on
Capsule Networks to address the problem of insufficient accuracy in intent
recognition for human-computer interaction. The method represents semantic
features in input text through a vectorized capsule structure. It uses a
dynamic routing mechanism to transfer information across multiple capsule
layers. This helps capture hierarchical relationships and part-whole structures
between semantic entities more effectively. The model uses a convolutional
feature extraction module as the low-level encoder. After generating initial
semantic capsules, it forms high-level abstract intent representations through
an iterative routing process. To further enhance performance, a margin-based
mechanism is introduced into the loss function. This improves the model's
ability to distinguish between intent classes. Experiments are conducted using
a public natural language understanding dataset. Multiple mainstream models are
used for comparison. Results show that the proposed model outperforms
traditional methods and other deep learning structures in terms of accuracy,
F1-score, and intent detection rate. The study also analyzes the effect of the
number of dynamic routing iterations on model performance. A convergence curve
of the loss function during training is provided. These results verify the
stability and effectiveness of the proposed method in semantic modeling.
Overall, this study presents a new structured modeling approach to improve
intent recognition under complex semantic conditions.

</details>


### [21] [Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm](https://arxiv.org/abs/2507.00547)
*Malmi Amadoru*

Main category: cs.CL

TL;DR: 针对计算密集型理论构建中高级算法（如主题建模）的透明度和严谨性不足问题，本文提供了确保研究方法严谨性的指导方针。


<details>
  <summary>Details</summary>
Motivation: 高级计算算法为理论发展提供了新途径，但其不透明性及应用缺乏透明度和严谨性，对方法学构成挑战，可能损害研究信任。该领域方法学严谨性的讨论尚处于初期。

Method: 通过阐述结构化主题建模算法的应用，并提出一套指导方针，讨论如何确保主题建模研究的严谨性。

Result: 提出了一套确保主题建模研究严谨性的指南。这些指南虽然针对主题建模算法，但经调整后也适用于其他算法。对主题建模新手研究者、编辑和审稿人尤为有用。

Conclusion: 本文为主题建模文献做出了贡献，并参与了计算密集型理论构建研究中方法学严谨性方面的新兴对话。

Abstract: The rise of advanced computational algorithms has opened new avenues for
computationally intensive research approaches to theory development. However,
the opacity of these algorithms and lack of transparency and rigour in their
application pose methodological challenges, potentially undermining trust in
research. The discourse on methodological rigour in this new genre of research
is still emerging. Against this backdrop, I attempt to offer guidance on
methodological rigour, particularly in the context of topic modelling
algorithms. By illustrating the application of the structural topic modelling
algorithm and presenting a set of guidelines, I discuss how to ensure rigour in
topic modelling studies. Although the guidelines are for the application of
topic modelling algorithms, they can be applied to other algorithms with
context-specific adjustments. The guidelines are helpful, especially for novice
researchers applying topic modelling, and editors and reviewers handling topic
modelling manuscripts. I contribute to the literature on topic modelling and
join the emerging dialogue on methodological rigour in computationally
intensive theory construction research.

</details>


### [22] [TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification](https://arxiv.org/abs/2507.00579)
*Miriam Anschütz,Ekaterina Gikalo,Niklas Herbster,Georg Groh*

Main category: cs.CL

TL;DR: 本文提出一种用于多语言大型语言模型（LLM）幻觉识别的两部分流水线系统，结合检索式事实核查和BERT模型，在SemEval-2025任务中表现出色，并在多种语言上取得前十名。


<details>
  <summary>Details</summary>
Motivation: LLM的幻觉问题严重影响其可信度和广泛应用。现有幻觉研究主要集中于英语数据，忽略了LLM的多语言特性，因此需要一个多语言幻觉识别方案。

Method: 本文提出一个两部分流水线系统：一是基于维基百科的检索式事实核查；二是微调一个BERT模型以识别常见的幻觉模式。

Result: 该系统在所有语言中均取得了有竞争力的结果，包括英语在内的八种语言进入前十名。此外，系统支持超过共享任务所涵盖的十四种语言。

Conclusion: 该多语言幻觉识别器有望在未来帮助改进LLM的输出质量和实用性。

Abstract: Hallucinations are one of the major problems of LLMs, hindering their
trustworthiness and deployment to wider use cases. However, most of the
research on hallucinations focuses on English data, neglecting the multilingual
nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3
- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related
Observable Overgeneration Mistakes. We propose a two-part pipeline that
combines retrieval-based fact verification against Wikipedia with a BERT-based
system fine-tuned to identify common hallucination patterns. Our system
achieves competitive results across all languages, reaching top-10 results in
eight languages, including English. Moreover, it supports multiple languages
beyond the fourteen covered by the shared task. This multilingual hallucination
identifier can help to improve LLM outputs and their usefulness in the future.

</details>


### [23] [Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based](https://arxiv.org/abs/2507.00601)
*Shuangquan Lyu,Yingnan Deng,Guiran Liu,Zhen Qi,Ruotong Wang*

Main category: cs.CL

TL;DR: 提出一个统一框架，结合知识迁移与参数高效微调，提升大模型在低资源语言场景下的跨语言适应性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在低资源语言场景下，知识迁移和任务适应能力受限的问题。

Method: 提出一个统一框架，整合知识迁移模块与参数高效微调策略。具体包括：引入知识对齐损失和软提示微调以吸收目标语言/任务特征，使用轻量级适应模块降低计算成本，并结合冻结策略和提示注入来平衡知识保留与快速适应。

Result: 在MLQA、XQuAD、PAWS-X等跨语言任务上，相较现有方法，展现出更高的性能和稳定性，尤其在数据极度匮乏的条件下优势显著。

Conclusion: 该方法具有强大的通用性和可扩展性，在保留大模型通用能力的同时增强了任务特定适应性，非常适合复杂语义建模和多语言处理任务。

Abstract: This paper addresses the limited transfer and adaptation capabilities of
large language models in low-resource language scenarios. It proposes a unified
framework that combines a knowledge transfer module with parameter-efficient
fine-tuning strategies. The method introduces knowledge alignment loss and soft
prompt tuning to guide the model in effectively absorbing the structural
features of target languages or tasks under minimal annotation. This enhances
both generalization performance and training stability. The framework includes
lightweight adaptation modules to reduce computational costs. During training,
it integrates freezing strategies and prompt injection to preserve the model's
original knowledge while enabling quick adaptation to new tasks. The study also
conducts stability analysis experiments and synthetic pseudo-data transfer
experiments to systematically evaluate the method's applicability and
robustness across different low-resource tasks. Experimental results show that
compared with existing multilingual pre-trained models and mainstream transfer
methods, the proposed approach achieves higher performance and stability on
cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates
particularly strong advantages under extremely data-scarce conditions. The
proposed method offers strong generality and scalability. It enhances
task-specific adaptability while preserving the general capabilities of large
language models. This makes it well-suited for complex semantic modeling and
multilingual processing tasks.

</details>


### [24] [Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies](https://arxiv.org/abs/2507.00606)
*Tao Xiong,Xavier Hu,Wenyan Fan,Shengyu Zhang*

Main category: cs.CL

TL;DR: 本文提出了Mixture of Reasoning (MoR)训练框架，旨在通过内嵌多样推理策略，使大型语言模型（LLMs）能够自主、任务自适应地进行推理，从而无需外部提示工程，并显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs通过如CoT和ToT等高级提示技术在复杂任务中表现出色，但它们对手动、任务特定提示的依赖限制了其适应性和效率。

Method: 引入了Mixture of Reasoning (MoR)训练框架，该框架包含两个阶段：1) 思维生成：使用GPT-4o等模型创建推理链模板；2) SFT数据集构建：将这些模板与基准数据集配对进行监督微调，从而将多样推理策略嵌入LLMs中。

Result: 实验结果显示，MoR显著提升了性能。其中，MoR150在使用CoT提示时达到了0.730（2.2%的提升），与基线相比整体实现了0.734（13.5%的提升）。

Conclusion: MoR成功消除了对特定任务提示的需求，为跨多样任务的鲁棒推理提供了一个可泛化的解决方案。

Abstract: Large language models (LLMs) excel in complex tasks through advanced
prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but
their reliance on manually crafted, task-specific prompts limits adaptability
and efficiency. We introduce Mixture of Reasoning (MoR), a training framework
that embeds diverse reasoning strategies into LLMs for autonomous,
task-adaptive reasoning without external prompt engineering. MoR has two
phases: Thought Generation, creating reasoning chain templates with models like
GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets
for supervised fine-tuning.Our experiments show that MoR significantly enhances
performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting
and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need
for task-specific prompts, offering a generalizable solution for robust
reasoning across diverse tasks.

</details>


### [25] [SAFER: Probing Safety in Reward Models with Sparse Autoencoder](https://arxiv.org/abs/2507.00665)
*Sihang Li,Wei Shi,Ziyuan Xie,Tao Liang,Guojun Ma,Xiang Wang*

Main category: cs.CL

TL;DR: 本文提出了一个名为 SAFER 的新框架，它使用稀疏自编码器来解释和改进 RLHF 中的奖励模型，从而提高大型语言模型的安全对齐性。


<details>
  <summary>Details</summary>
Motivation: 强化学习从人类反馈 (RLHF) 是使大型语言模型 (LLMs) 与人类价值观对齐的关键范式，但其核心的奖励模型却在很大程度上不透明，难以理解其决策过程。

Method: 本研究提出了 SAFER (Sparse Autoencoder For Enhanced Reward model) 框架，通过利用稀疏自编码器 (SAEs) 来揭示奖励模型激活中可解释的人类特征，从而深入了解与安全相关的决策制定。SAFER 应用于面向安全的偏好数据集，并通过选择和拒绝响应之间的激活差异来量化单个特征的显著性。基于这些特征层面的信号，设计了有针对性的数据投毒和去噪策略。

Result: 实验表明，SAFER 能够在不牺牲通用聊天性能的前提下，通过最小的数据修改精确地降低或增强安全对齐。这证明了 SAFER 在操纵和优化安全对齐方面的有效性。

Conclusion: 该方法有助于在高风险的大型语言模型对齐任务中解释、审计和改进奖励模型，为增强模型安全性和透明度做出了贡献。

Abstract: Reinforcement learning from human feedback (RLHF) is a key paradigm for
aligning large language models (LLMs) with human values, yet the reward models
at its core remain largely opaque. In this work, we present sparse Autoencoder
For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting
and improving reward models through mechanistic analysis. Leveraging Sparse
Autoencoders (SAEs), we uncover human-interpretable features in reward model
activations, enabling insight into safety-relevant decision-making. We apply
SAFER to safety-oriented preference datasets and quantify the salience of
individual features by activation differences between chosen and rejected
responses. Using these feature-level signals, we design targeted data poisoning
and denoising strategies. Experiments show that SAFER can precisely degrade or
enhance safety alignment with minimal data modification, without sacrificing
general chat performance. Our approach contributes to interpreting, auditing
and refining reward models in high-stakes LLM alignment tasks. Our codes are
available at https://github.com/xzy-101/SAFER-code. \textit{This paper
discusses topics related to large language model safety and may include
discussions or examples that highlight potential risks or unsafe outcomes.}

</details>


### [26] [Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English](https://arxiv.org/abs/2507.00700)
*Ahmed Sabir,Azinovič Gasper,Mengsay Loem,Rajesh Sharma*

Main category: cs.CL

TL;DR: 研究发现，视觉语言模型（VLMs）根据其训练语言的不同，展现出与人类文化认知相似的注意力模式。


<details>
  <summary>Details</summary>
Motivation: 鉴于跨文化研究表明人类在视觉信息处理上存在文化差异（如东西方的整体与分析视角），本研究旨在探讨主要在不同语言（日语和英语）上训练的VLMs是否也展现出类似受文化影响的注意力模式。

Method: 通过比较VLMs生成的图像描述，分析这些模型是否反映出整体与分析倾向的差异。

Result: 研究结果表明，VLMs不仅内化了语言的结构特性，而且再现了训练数据中嵌入的文化行为（即整体与分析倾向）。

Conclusion: 文化认知可能隐性地塑造了模型的输出。

Abstract: Cross-cultural research in perception and cognition has shown that
individuals from different cultural backgrounds process visual information in
distinct ways. East Asians, for example, tend to adopt a holistic perspective,
attending to contextual relationships, whereas Westerners often employ an
analytical approach, focusing on individual objects and their attributes. In
this study, we investigate whether Vision-Language Models (VLMs) trained
predominantly on different languages, specifically Japanese and English,
exhibit similar culturally grounded attentional patterns. Using comparative
analysis of image descriptions, we examine whether these models reflect
differences in holistic versus analytic tendencies. Our findings suggest that
VLMs not only internalize the structural properties of language but also
reproduce cultural behaviors embedded in the training data, indicating that
cultural cognition may implicitly shape model outputs.

</details>


### [27] [AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation](https://arxiv.org/abs/2507.00718)
*Elizabeth Fons,Elena Kochkina,Rachneet Kaur,Zhen Zeng,Berowne Hlavaty,Charese Smiley,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.CL

TL;DR: 探讨LLM从时间序列数据生成金融报告的潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在探索大型语言模型（LLMs）从时间序列数据生成金融报告的能力，以实现自动化且信息丰富的报告生成。

Method: 提出一个包含提示工程、模型选择和评估的框架。引入自动化高亮系统，用于区分报告中来源于时间序列数据、金融推理或外部知识的信息，以评估模型的真实性和推理能力。实验使用了真实股票市场指数和合成时间序列数据。

Result: 实验证明LLMs能够生成连贯且信息丰富的金融报告。

Conclusion: 大型语言模型具备从时间序列数据生成金融报告的潜力，且可通过特定方法有效评估其报告内容的来源和质量。

Abstract: This paper explores the potential of large language models (LLMs) to generate
financial reports from time series data. We propose a framework encompassing
prompt engineering, model selection, and evaluation. We introduce an automated
highlighting system to categorize information within the generated reports,
differentiating between insights derived directly from time series data,
stemming from financial reasoning, and those reliant on external knowledge.
This approach aids in evaluating the factual grounding and reasoning
capabilities of the models. Our experiments, utilizing both data from the real
stock market indices and synthetic time series, demonstrate the capability of
LLMs to produce coherent and informative financial reports.

</details>


### [28] [LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing](https://arxiv.org/abs/2507.00769)
*Daniel Fein,Sebastian Russo,Violet Xiang,Kabir Jolly,Rafael Rafailov,Nick Haber*

Main category: cs.CL

TL;DR: 针对大型语言模型（LLMs）生成的创意写作评估难题，本文提出了首个标准化基准和配对数据集LitBench。通过对零样本LLM判断器进行基准测试并训练奖励模型，研究表明训练模型表现优于现有模型，并与人类偏好高度一致，为创意写作的自动化评估提供了可靠资源。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）生成的开放式创意写作极具挑战性，因其缺乏“真实标准答案”。现有自动化评估方法不足，且将现成的（OTS）语言模型用作零样本判断器的可靠性尚不明确。

Method: 研究团队引入了LitBench，这是首个用于创意写作验证的标准化基准和配对数据集，包含一个2,480对人工标注的故事比较测试集（来自Reddit）和一个43,827对的人类偏好训练语料库。基于LitBench，他们(i)对零样本LLM判断器进行了基准测试，(ii)训练了Bradley Terry和生成式奖励模型，并(iii)进行在线人类研究以验证奖励模型对新生成的LLM故事的排名。

Result: 基准测试显示，Claude-3.7-Sonnet是表现最佳的现成判断器，与人类偏好的一致性达到73%。在训练的奖励模型中，Bradley-Terry和生成式奖励模型均达到78%的准确率，超越了所有现成判断器。在线人类研究进一步证实，训练的奖励模型与新生成的LLM故事中的人类偏好高度一致。

Conclusion: 本文发布了LitBench基准和训练的奖励模型，为创意写作系统的可靠、自动化评估和优化提供了经过验证的资源。

Abstract: Evaluating creative writing generated by large language models (LLMs) remains
challenging because open-ended narratives lack ground truths. Without
performant automated evaluation methods, off-the-shelf (OTS) language models
are employed as zero-shot judges, yet their reliability is unclear in this
context. In pursuit of robust evaluation for creative writing, we introduce
LitBench, the first standardized benchmark and paired dataset for creative
writing verification, comprising a held-out test set of 2,480 debiased,
human-labeled story comparisons drawn from Reddit and a 43,827-pair training
corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot
LLM judges, (ii) train Bradley Terry and generative reward models, and (iii)
conduct an online human study to validate reward model rankings on newly
LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the
strongest off-the-shelf judge, reaching 73% agreement with human preferences;
among trained reward models, Bradley-Terry and Generative reward models both
attain an accuracy of 78%, outperforming all off-the-shelf judges. An online
human study further confirms that our trained reward models consistently align
with human preferences in novel LLM-generated stories. We release LitBench and
reward models at
https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,
providing a vetted resource for reliable, automated evaluation and optimization
of creative writing systems.

</details>


### [29] [A Diagrammatic Calculus for a Functional Model of Natural Language Semantics](https://arxiv.org/abs/2507.00782)
*Matthieu Pierre Boyer*

Main category: cs.CL

TL;DR: 本文研究了一种功能编程方法应用于自然语言语义，旨在提升传统指称风格的表达能力，并为此形式化了基于范畴的类型与效应系统，构建了图表演算来高效计算句子的指称。


<details>
  <summary>Details</summary>
Motivation: 旨在通过功能编程方法，提升传统自然语言语义指称风格的表达能力。

Method: 1. 采用功能编程方法处理自然语言语义。2. 形式化基于范畴的类型与效应系统。3. 构建图表演算以建模解析和效应处理。4. 利用该演算高效计算句子的指称。

Result: 开发了一种新的范畴类型与效应系统及图表演算，用于高效计算句子的指称，并支持增强的语义表达能力。

Conclusion: 该功能编程方法，结合形式化的类型与效应系统及图表演算，能够有效增强自然语言语义的表达能力，并实现对句子指称的高效计算。

Abstract: In this paper, we study a functional programming approach to natural language
semantics, allowing us to increase the expressivity of a more traditional
denotation style. We will formalize a category based type and effect system,
and construct a diagrammatic calculus to model parsing and handling of effects,
and use it to efficiently compute the denotations for sentences.

</details>


### [30] [Generative AI and the future of scientometrics: current topics and future questions](https://arxiv.org/abs/2507.00783)
*Benedetto Lepori,Jens Peter Andersen,Karsten Donnay*

Main category: cs.CL

TL;DR: 本文回顾了生成式AI在科学计量学中的应用、其当前局限性以及对该领域未来发展的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 旨在审查生成式AI在科学计量学中的应用现状，并就其对该领域的更广泛影响展开讨论。

Method: 文章首先介绍了生成式AI的生成性和概率性本质及其与人类“推理”的关联；其次，批判性地评估了GenAI在科学计量学具体任务（如主题标注、引文分析、学者画像、研究评估）中的近期实验应用；最后，探讨了GenAI通过生成大量科学语言对衡量科学的文本特征（如作者、词汇、参考文献）可能产生的根本性影响。

Result: 生成式AI在语言生成任务（如标注）中展现出潜力，但在需要稳定语义、语用推理或结构化领域知识的任务中面临局限。研究指出这些结果可能迅速过时，并建议系统性比较不同GenAI模型在特定任务上的表现。

Conclusion: 为持续解读不断演变的知识生产模式，科学计量学领域亟需严谨的实证研究和深入的理论反思。

Abstract: The aim of this paper is to review the use of GenAI in scientometrics, and to
begin a debate on the broader implications for the field. First, we provide an
introduction on GenAI's generative and probabilistic nature as rooted in
distributional linguistics. And we relate this to the debate on the extent to
which GenAI might be able to mimic human 'reasoning'. Second, we leverage this
distinction for a critical engagement with recent experiments using GenAI in
scientometrics, including topic labelling, the analysis of citation contexts,
predictive applications, scholars' profiling, and research assessment. GenAI
shows promise in tasks where language generation dominates, such as labelling,
but faces limitations in tasks that require stable semantics, pragmatic
reasoning, or structured domain knowledge. However, these results might become
quickly outdated. Our recommendation is, therefore, to always strive to
systematically compare the performance of different GenAI models for specific
tasks. Third, we inquire whether, by generating large amounts of scientific
language, GenAI might have a fundamental impact on our field by affecting
textual characteristics used to measure science, such as authors, words, and
references. We argue that careful empirical work and theoretical reflection
will be essential to remain capable of interpreting the evolving patterns of
knowledge production.

</details>


### [31] [Many LLMs Are More Utilitarian Than One](https://arxiv.org/abs/2507.00814)
*Anita Keshmirian,Razan Baltaji,Babak Hemmatian,Hadi Asghari,Lav R. Varshney*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在多智能体群组决策中，会表现出类似人类的“功利主义提升”，即更倾向于为了多数人的利益而违反道德规范；然而，其内在驱动机制与人类不同。


<details>
  <summary>Details</summary>
Motivation: 研究LLM的道德判断能力，尤其是在多智能体协作背景下，对比个体与群组表现。鉴于人类在群体审议中会出现“功利主义提升”现象，本研究旨在探讨LLM群组是否也存在类似动态。

Method: 选取六种LLM，在既定的个人道德困境（需为多数人利益牺牲个体）下进行测试。测试条件包括：1）独立推理（Solo）；2）以两人或三人小组进行多轮讨论（Group）。通过比较不同条件下的模型道德判断，并与人类实验结果进行对比。

Result: 所有LLM模型在群组中比独立时更能接受道德违规行为，这与人类实验结果相似。部分模型优先考虑整体福祉，即使涉及陌生人；另一些在群组中更愿意违反道德规范。然而，尽管LLM群组与人类群组表现出相似的行动偏见，其内在机制不同：人类的转变源于对决策结果敏感性增强，而LLM群组则表现为规范敏感性降低或公正性增强。这意味着LLM集体表面行为模仿人类群体推理，但其潜在驱动因素不同。

Conclusion: LLM集体道德推理的底层驱动因素与人类存在根本差异，即使其表层行为相似。这为AI对齐、多智能体系统设计和人工智能道德推理提供了重要启示。

Abstract: Moral judgment is integral to large language model (LLM) alignment and social
reasoning. As multi-agent systems gain prominence, it becomes crucial to
understand how LLMs function collectively during collaboration, compared to
individual agents. In human moral judgment, group deliberation leads to a
utilitarian boost: a tendency to endorse norm violations that maximize benefits
for the greatest number of people despite harms. We study whether a similar
dynamic emerges in multi-agent LLM systems. We tested six models on
well-established sets of moral dilemmas across two conditions: (1) Solo, where
models reasoned independently, and (2) Group, where they engaged in multi-turn
discussions in pairs or triads. In personal moral dilemmas, where agents must
decide to directly harm one individual to maximize the utility for others, all
models found moral violations to be more acceptable when part of a group than
individually, similar to human experiments. Some models endorsed actions that
maximized overall well-being, even if they benefited strangers over familiar
individuals. Others became more willing to violate moral norms in groups.
However, while human groups show a similar action bias, the mechanism for their
utilitarian boost differs from LLMs. Whereas the human shift comes from
heightened sensitivity to decision outcomes, LLM groups show either reduced
norm sensitivity or enhanced impartiality. This suggests that while the surface
behavior of LLM collectives mimics human group reasoning, the underlying
drivers differ. We discuss the implications for AI alignment, multi-agent
design, and artificial moral reasoning.

</details>


### [32] [ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering](https://arxiv.org/abs/2507.00828)
*Alexander Hoyle,Lorena Calvo-Bartolomé,Jordan Boyd-Graber,Philip Resnik*

Main category: cs.CL

TL;DR: 设计并验证了一种可扩展的人类评估协议及LLM代理，用于主题模型和文档聚类，发现最佳LLM代理与人类表现无统计差异，可作为自动化评估的合理替代。


<details>
  <summary>Details</summary>
Motivation: 现有主题模型和文档聚类评估方法存在不足：自动化指标与人类偏好不符，而专家标注缺乏可扩展性。

Method: 1. 设计了一种新的可扩展人类评估协议，该协议要求标注者（或LLM代理）审查主题/聚类中的文本项，推断类别，并将其应用于其他文档。2. 利用该协议，在两个数据集上收集了大量众包标注，评估了多种主题模型的输出。3. 使用收集到的标注数据验证了自动化代理（特别是LLM代理的有效性）。

Result: 最佳的LLM代理在统计学上与人类标注者的表现无法区分。

Conclusion: LLM代理可以作为主题模型和文档聚类自动化评估中人类标注者的有效替代，从而解决了评估的可扩展性难题。

Abstract: Topic model and document-clustering evaluations either use automated metrics
that align poorly with human preferences or require expert labels that are
intractable to scale. We design a scalable human evaluation protocol and a
corresponding automated approximation that reflect practitioners' real-world
usage of models. Annotators -- or an LLM-based proxy -- review text items
assigned to a topic or cluster, infer a category for the group, then apply that
category to other documents. Using this protocol, we collect extensive
crowdworker annotations of outputs from a diverse set of topic models on two
datasets. We then use these annotations to validate automated proxies, finding
that the best LLM proxies are statistically indistinguishable from a human
annotator and can therefore serve as a reasonable substitute in automated
evaluations. Package, web interface, and data are at
https://github.com/ahoho/proxann

</details>


### [33] [Stylometry recognizes human and LLM-generated texts in short samples](https://arxiv.org/abs/2507.00838)
*Karol Przystalski,Jan K. Argasiński,Iwona Grabska-Gradzińska,Jeremi K. Ochab*

Main category: cs.CL

TL;DR: 该研究利用文体学方法区分大型语言模型（LLM）生成文本和人类文本，以解决模型归属和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的普及，区分其生成内容与人类创作内容变得至关重要，涉及模型归属、知识产权和AI伦理使用问题。

Method: 研究构建了基于维基百科的基准数据集，包含人类、纯LLM生成、LLM总结和复述的文本。使用决策树和LightGBM等树形模型，通过人为设计（StyloMetrix）和n-gram文体特征（包括词汇、语法、句法、标点模式）进行分类。采用Shapley Additive Explanations解释模型决策。

Result: 在7类多分类任务中，Matthews相关系数高达0.87；二元分类准确率在0.79至1.0之间，其中维基百科与GPT-4的二元分类准确率在平衡数据集上高达0.98。解释性分析揭示了百科文本类型特征、过度使用的词语以及LLM文本更高的语法标准化。

Conclusion: 研究表明，至少对于定义明确的文本类型，可以有效区分机器生成文本与人类生成文本，这对于日益复杂的LLM至关重要。

Abstract: The paper explores stylometry as a method to distinguish between texts
created by Large Language Models (LLMs) and humans, addressing issues of model
attribution, intellectual property, and ethical AI use. Stylometry has been
used extensively to characterise the style and attribute authorship of texts.
By applying it to LLM-generated texts, we identify their emergent writing
patterns. The paper involves creating a benchmark dataset based on Wikipedia,
with (a) human-written term summaries, (b) texts generated purely by LLMs
(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text
summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods
(Dipper, T5). The 10-sentence long texts were classified by tree-based models
(decision trees and LightGBM) using human-designed (StyloMetrix) and
n-gram-based (our own pipeline) stylometric features that encode lexical,
grammatical, syntactic, and punctuation patterns. The cross-validated results
reached a performance of up to .87 Matthews correlation coefficient in the
multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary
classification, with the particular example of Wikipedia and GPT-4 reaching up
to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed
features characteristic of the encyclopaedic text type, individual overused
words, as well as a greater grammatical standardisation of LLMs with respect to
human-written texts. These results show -- crucially, in the context of the
increasingly sophisticated LLMs -- that it is possible to distinguish machine-
from human-generated texts at least for a well-defined text type.

</details>


### [34] [TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation](https://arxiv.org/abs/2507.00875)
*Xi Xuan,King-kui Sin,Yufei Zhou,Chunyu Kit*

Main category: cs.CL

TL;DR: 本研究引入TransLaw，一个多智能体LLM框架，用于香港法律判决翻译。该框架通过协作智能体提升翻译质量，超越GPT-4o在多方面表现，但仍在某些复杂语境和自然度上逊于人类专家，同时显著降低了成本。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在机器翻译领域表现出色，但由于香港法律判决中复杂的法律术语、文化内涵和严格的语言结构，其在香港法律判决翻译中的潜力仍不确定。

Method: 本研究提出了TransLaw框架，采用三个专业智能体（翻译器、注释器和校对器）协同工作，以确保法律含义的准确性、风格的恰当性以及结构上的连贯性与衔接性。该框架支持LLM配置定制，并实现了显著的成本降低。

Result: 通过使用13种LLM进行评估，TransLaw在法律语义准确性、结构连贯性和风格忠实度方面超越了GPT-4o。然而，它在复杂术语的语境化和风格的自然度方面仍不及人类专家。该框架还实现了与专业人工翻译服务相比的巨大成本降低。

Conclusion: TransLaw为香港案例法翻译提供了一个新颖且高效的解决方案，在与高级LLM的比较中表现出色，尤其在成本效益和特定质量指标上，尽管在处理极度复杂的语境和自然度方面仍有提升空间。

Abstract: Multi-agent systems empowered by large language models (LLMs) have
demonstrated remarkable capabilities in a wide range of downstream
applications, including machine translation. However, the potential of LLMs in
translating Hong Kong legal judgments remains uncertain due to challenges such
as intricate legal terminology, culturally embedded nuances, and strict
linguistic structures. In this work, we introduce TransLaw, a novel multi-agent
framework implemented for real-world Hong Kong case law translation. It employs
three specialized agents, namely, Translator, Annotator, and Proofreader, to
collaboratively produce translations for high accuracy in legal meaning,
appropriateness in style, and adequate coherence and cohesion in structure.
This framework supports customizable LLM configurations and achieves tremendous
cost reduction compared to professional human translation services. We
evaluated its performance using 13 open-source and commercial LLMs as agents
and obtained interesting findings, including that it surpasses GPT-4o in legal
semantic accuracy, structural coherence, and stylistic fidelity, yet trails
human experts in contextualizing complex terminology and stylistic naturalness.
Our platform website is available at CityUHK, and our bilingual judgment corpus
used for the evaluation is available at Hugging Face.

</details>


### [35] [Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations](https://arxiv.org/abs/2507.00883)
*Aditya Tomar,Nihar Ranjan Sahoo,Ashish Mittal,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 研究了大型语言模型（LLMs）在不同文化背景数学问题上的表现，发现模型在文化适应性数据上表现下降，但推理能力强的模型韧性更强。


<details>
  <summary>Details</summary>
Motivation: 尽管数学被视为文化中立，但其问题呈现方式可能隐含文化背景。现有基准（如GSM8K）主要基于西方规范，可能导致LLMs在处理不同文化背景问题时表现不佳。

Method: 1. 为非洲、印度、中国、韩国和日本五个地区，通过基于提示的转换和人工验证，创建了GSM8K测试集的文化适应变体。2. 评估了六个（8B至72B参数）大型语言模型，采用五种提示策略，以评估它们对数学问题呈现中文化变异的鲁棒性。

Result: 1. 发现存在一致的性能差距：模型在原始（以美国为中心）数据集上表现最佳，而在文化适应版本上表现相对较差。2. 具有推理能力的模型对这些文化转变表现出更强的韧性。

Conclusion: 深层推理能力有助于弥合数学任务中文化呈现的差距，表明提高推理能力能增强LLMs在不同文化背景下的适应性。

Abstract: Although mathematics is often considered culturally neutral, the way
mathematical problems are presented can carry implicit cultural context.
Existing benchmarks like GSM8K are predominantly rooted in Western norms,
including names, currencies, and everyday scenarios. In this work, we create
culturally adapted variants of the GSM8K test set for five regions Africa,
India, China, Korea, and Japan using prompt-based transformations followed by
manual verification. We evaluate six large language models (LLMs), ranging from
8B to 72B parameters, across five prompting strategies to assess their
robustness to cultural variation in math problem presentation. Our findings
reveal a consistent performance gap: models perform best on the original
US-centric dataset and comparatively worse on culturally adapted versions.
However, models with reasoning capabilities are more resilient to these shifts,
suggesting that deeper reasoning helps bridge cultural presentation gaps in
mathematical tasks

</details>


### [36] [Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check](https://arxiv.org/abs/2507.00885)
*Nicholas Lourie,Michael Y. Hu,Kyunghyun Cho*

Main category: cs.CL

TL;DR: 对下游扩展律的元分析发现，线性趋势并不普遍，且易受实验设置影响，需深入理解其成功条件。


<details>
  <summary>Details</summary>
Motivation: 下游扩展律旨在通过预训练损失预测更大规模任务性能，但现有研究对这种预测的可能性存在争议，部分研究指出线性趋势，另一些则提出新兴特性和反向扩展等挑战。

Method: 对现有关于下游扩展律的数据进行了元分析。

Result: 研究发现，与线性扩展律的紧密拟合仅在少数情况下（39%）发生。此外，实验设置的微小改变可能彻底改变扩展趋势。

Conclusion: 分析强调了理解扩展律成功条件的必要性，并指出在建模预训练损失与下游任务性能关系时，必须接受扩展行为偏离线性趋势的情况。

Abstract: Downstream scaling laws aim to predict task performance at larger scales from
pretraining losses at smaller scales. Whether this prediction should be
possible is unclear: some works demonstrate that task performance follows clear
linear scaling trends under transformation, whereas others point out
fundamental challenges to downstream scaling laws, such as emergence and
inverse scaling. In this work, we conduct a meta-analysis of existing data on
downstream scaling laws, finding that close fit to linear scaling laws only
occurs in a minority of cases: 39% of the time. Furthermore, seemingly benign
changes to the experimental setting can completely change the scaling trend.
Our analysis underscores the need to understand the conditions under which
scaling laws succeed. To fully model the relationship between pretraining loss
and downstream task performance, we must embrace the cases in which scaling
behavior deviates from linear trends.

</details>


### [37] [MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes](https://arxiv.org/abs/2507.00891)
*Yuheng Wang,Xianhe Tang,Pufeng Huang*

Main category: cs.CL

TL;DR: 引入MemeCMD，一个自动生成的中文多轮对话数据集，包含情境相关的表情包，旨在弥补现有对话数据集中多模态表达的不足。


<details>
  <summary>Details</summary>
Motivation: 现有对话数据集多限于纯文本或手动标注，缺乏多模态交互（如表情包）所提供的表达力和语境细微差别，而表情包在在线社交互动中广泛使用。

Method: 结合大规模多模态大语言模型（MLLM）标注的表情包库，并由双智能体在不同场景下自动生成对话。引入检索框架和自适应阈值，以确保表情包使用的语境相关性和自然间隔。

Result: 实验证明，该方法能有效生成语境恰当且多样化的包含表情包的对话。

Conclusion: MemeCMD为推动多模态对话式AI提供了可扩展且保护隐私的资源。

Abstract: Memes are widely used in online social interactions, providing vivid,
intuitive, and often humorous means to express intentions and emotions.
Existing dialogue datasets are predominantly limited to either manually
annotated or pure-text conversations, lacking the expressiveness and contextual
nuance that multimodal interactions provide.To address these challenges, we
introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue
dataset with contextually retrieved memes. Our dataset combines a large-scale,
MLLM-annotated meme library with dialogues auto-generated by dual agents across
diverse scenarios. We introduce a retrieval framework and adaptive threshold to
ensure contextually relevant, naturally spaced meme usage. Experiments
demonstrate the effectiveness of our approach in generating contextually
appropriate and diverse meme-incorporated dialogues, offering a scalable and
privacy-preserving resource for advancing multimodal conversational AI.

</details>


### [38] [The Cognate Data Bottleneck in Language Phylogenetics](https://arxiv.org/abs/2507.00911)
*Luise Häuser,Alexandros Stamatakis*

Main category: cs.CL

TL;DR: 计算系统发育方法需要大规模同源词数据集，但现有自动提取方法（如从BabelNet）生成的数据集与真实结果不符，导致此类方法难以应用于历史语言学。


<details>
  <summary>Details</summary>
Motivation: 计算系统发育方法在同源词数据分析中潜力巨大，但受限于现有手动收集数据集规模过小。目前缺乏自动生成大规模同源词数据集的可行方法，这阻碍了机器学习等先进技术的应用。

Method: 通过从大型多语言百科词典BabelNet中自动提取数据集，构建字符矩阵，并在此基础上进行系统发育推断。同时，探讨从其他多语言资源中提取合适数据的可能性。

Result: 从BabelNet提取的数据集进行系统发育推断所生成的树与公认的黄金标准真实树基本不一致。研究认为从其他多语言资源中提取更合适字符矩阵的可能性也不大。

Conclusion: 由于无法为同源词数据获取所需的大规模数据集，因此需要大型数据集的计算系统发育方法无法应用于同源词数据。这使得这些计算方法在历史语言学中如何应用仍是一个悬而未决的问题。

Abstract: To fully exploit the potential of computational phylogenetic methods for
cognate data one needs to leverage specific (complex) models an machine
learning-based techniques. However, both approaches require datasets that are
substantially larger than the manually collected cognate data currently
available. To the best of our knowledge, there exists no feasible approach to
automatically generate larger cognate datasets. We substantiate this claim by
automatically extracting datasets from BabelNet, a large multilingual
encyclopedic dictionary. We demonstrate that phylogenetic inferences on the
respective character matrices yield trees that are largely inconsistent with
the established gold standard ground truth trees. We also discuss why we
consider it as being unlikely to be able to extract more suitable character
matrices from other multilingual resources. Phylogenetic data analysis
approaches that require larger datasets can therefore not be applied to cognate
data. Thus, it remains an open question how, and if these computational
approaches can be applied in historical linguistics.

</details>


### [39] [Discourse Heuristics For Paradoxically Moral Self-Correction](https://arxiv.org/abs/2507.00985)
*Guangliang Liu,Zimo Qi,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: LLM的道德自校正存在两个悖论：表面有效性和难以诊断根本原因。研究发现，这是由于自校正依赖于启发式捷径，导致同时提升自校正和自诊断时出现不一致。作者提出利用精心策划数据集的启发式方法来改进自校正，并指出了其泛化挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）的道德自校正是一种有前景的对齐方法，但它面临两大悖论：其有效性仅停留在表面，且LLMs在自校正过程中难以识别其道德不一致的根本原因。本研究旨在深入理解并解决这些悖论。

Method: 分析旨在增强道德自校正的微调语料库中的语篇结构，以揭示有效结构背后的启发式方法。

Result: 研究表明，道德自校正依赖于反映启发式捷径的语篇结构。这些启发式捷径的存在导致在同时增强自校正和自诊断能力时出现不一致性。

Conclusion: 基于研究发现，提出通过利用精心策划数据集的启发式方法来改进道德自校正。同时强调了该能力在从特定情境中学习和模型规模方面的泛化挑战。

Abstract: Moral self-correction has emerged as a promising approach for aligning the
output of Large Language Models (LLMs) with human moral values. However, moral
self-correction techniques are subject to two primary paradoxes. First, despite
empirical and theoretical evidence to support the effectiveness of
self-correction, this LLM capability only operates at a superficial level.
Second, while LLMs possess the capability of self-diagnosing immoral aspects of
their output, they struggle to identify the cause of this moral inconsistency
during their self-correction process. To better understand and address these
paradoxes, we analyze the discourse constructions in fine-tuning corpora
designed to enhance moral self-correction, uncovering the existence of the
heuristics underlying effective constructions. We demonstrate that moral
self-correction relies on discourse constructions that reflect heuristic
shortcuts, and that the presence of these heuristic shortcuts during
self-correction leads to inconsistency when attempting to enhance both
self-correction and self-diagnosis capabilities jointly. Based on our findings,
we propose a solution to improve moral self-correction by leveraging the
heuristics of curated datasets. We also highlight the generalization challenges
of this capability, particularly in terms of learning from situated context and
model scales.

</details>


### [40] [Should We Still Pretrain Encoders with Masked Language Modeling?](https://arxiv.org/abs/2507.00994)
*Hippolyte Gisserot-Boukhlef,Nicolas Boizard,Manuel Faysse,Duarte M. Alves,Emmanuel Malherbe,André F. T. Martins,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: 研究MLM和CLM预训练对文本表示的影响，发现MLM表现通常更好，但CLM更高效。提出一种CLM后接MLM的两阶段训练策略，能以较低计算成本获得最佳编码器性能。


<details>
  <summary>Details</summary>
Motivation: 探究解码器模型（CLM预训练）作为编码器表现优越性，是否源于CLM目标函数本身，而非模型或数据规模等混杂因素。

Method: 通过大规模、严格控制的预训练消融实验，训练30个不同参数规模的模型（2.1亿至10亿），并进行超过15,000次微调和评估。

Result: MLM训练的模型在文本表示任务中普遍表现更优，而CLM训练的模型数据效率更高且微调更稳定。研究表明，CLM后接MLM的两阶段训练策略在固定计算预算下可达最佳性能，且利用现有预训练CLM模型初始化能显著降低高性能编码器的训练成本。

Conclusion: CLM-MLM两阶段训练策略为构建高性能编码器模型提供了一种计算效率更高、更实用的方法，尤其能有效利用现有的大语言模型生态系统资源。

Abstract: Learning high-quality text representations is fundamental to a wide range of
NLP tasks. While encoder pretraining has traditionally relied on Masked
Language Modeling (MLM), recent evidence suggests that decoder models
pretrained with Causal Language Modeling (CLM) can be effectively repurposed as
encoders, often surpassing traditional encoders on text representation
benchmarks. However, it remains unclear whether these gains reflect an inherent
advantage of the CLM objective or arise from confounding factors such as model
and data scale. In this paper, we address this question through a series of
large-scale, carefully controlled pretraining ablations, training a total of 30
models ranging from 210 million to 1 billion parameters, and conducting over
15,000 fine-tuning and evaluation runs. We find that while training with MLM
generally yields better performance across text representation tasks,
CLM-trained models are more data-efficient and demonstrate improved fine-tuning
stability. Building on these findings, we experimentally show that a biphasic
training strategy that sequentially applies CLM and then MLM, achieves optimal
performance under a fixed computational training budget. Moreover, we
demonstrate that this strategy becomes more appealing when initializing from
readily available pretrained CLM models (from the existing LLM ecosystem),
reducing the computational burden needed to train best-in-class encoder models.
We release all project artifacts at https://hf.co/MLMvsCLM to foster further
research.

</details>


### [41] [La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America](https://arxiv.org/abs/2507.00999)
*María Grandury,Javier Aula-Blasco,Júlia Falcão,Clémentine Fourrier,Miguel González,Gonzalo Martínez,Gonzalo Santamaría,Rodrigo Agerri,Nuria Aldama,Luis Chiruzzo,Javier Conde,Helena Gómez,Marta Guerrero,Guido Ivetta,Natalia López,Flor Miriam Plaza-del-Arco,María Teresa Martín-Valdivia,Helena Montoro,Carmen Muñoz,Pedro Reviriego,Leire Rosado,Alejandro Vaca,María Estrella Vallecillo-Rodríguez,Jorge Vallego,Irune Zubiaga*

Main category: cs.CL

TL;DR: La Leaderboard是一个开源排行榜，旨在评估针对西班牙语社区多样化语言和文化的大语言模型，结合66个数据集和50个模型的结果，并倡导一种环保且可复现的评估方法。


<details>
  <summary>Details</summary>
Motivation: 为了促进开发能够代表西班牙语社区语言和文化多样性的大语言模型，并为其建立一个开放的、社区驱动的评估标准。

Method: 提出了名为“La Leaderboard”的开源排行榜，整合了巴斯克语、加泰罗尼亚语、加利西亚语及多种西班牙语变体的66个数据集。项目详细阐述了评估方法，并特别指出为减少环境影响和提高结果可复现性而采用了较少的少量样本（few-shot examples）。

Result: 该初始版本已展示了50个大语言模型的评估结果。

Conclusion: La Leaderboard建立了西班牙语社区大语言模型的评估标准，推动了语言多样性LLM的开发。其开放性及强调环保和可复现性的方法论，鼓励了其他语言评估排行榜的创建。

Abstract: Leaderboards showcase the current capabilities and limitations of Large
Language Models (LLMs). To motivate the development of LLMs that represent the
linguistic and cultural diversity of the Spanish-speaking community, we present
La Leaderboard, the first open-source leaderboard to evaluate generative LLMs
in languages and language varieties of Spain and Latin America. La Leaderboard
is a community-driven project that aims to establish an evaluation standard for
everyone interested in developing LLMs for the Spanish-speaking community. This
initial version combines 66 datasets in Basque, Catalan, Galician, and
different Spanish varieties, showcasing the evaluation results of 50 models. To
encourage community-driven development of leaderboards in other languages, we
explain our methodology, including guidance on selecting the most suitable
evaluation setup for each downstream task. In particular, we provide a
rationale for using fewer few-shot examples than typically found in the
literature, aiming to reduce environmental impact and facilitate access to
reproducible results for a broader research community.

</details>


### [42] [SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks](https://arxiv.org/abs/2507.01001)
*Yilun Zhao,Kaiyan Zhang,Tiansheng Hu,Sihong Wu,Ronan Le Bras,Taira Anderson,Jonathan Bragg,Joseph Chee Chang,Jesse Dodge,Matt Latzke,Yixin Liu,Charles McGrady,Xiangru Tang,Zihang Wang,Chen Zhao,Hannaneh Hajishirzi,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: SciArena是一个开放协作平台，通过社区投票评估基础模型在科学文献任务上的表现，并发布了用于自动化评估的基准。


<details>
  <summary>Details</summary>
Motivation: 传统的科学文献理解与合成基准无法有效评估基础模型在需要文献支持、长篇回复的开放式科学任务上的性能，因此需要一个利用集体智慧、社区驱动的评估方法。

Method: 开发了SciArena平台，采用类似Chatbot Arena的社区投票机制，让研究者直接对模型进行比较和评估。该平台已支持23个基础模型，收集了超过13,000张来自研究者的投票。此外，发布了SciArena-Eval元评估基准，用于衡量模型在判断答案质量方面的准确性。

Result: 分析数据确认所提问题多样且符合实际文献需求，参与研究者的评估表现出强自洽性和注释者间一致性。通过模型排名排行榜讨论了结果和见解。SciArena-Eval实验揭示了当前自动化评估的挑战，并强调了开发更可靠评估方法的必要性。

Conclusion: SciArena成功建立了一个有效的社区驱动平台，用于评估基础模型在复杂科学文献任务上的性能，并提供了宝贵的数据集。同时，研究强调了当前自动化评估方法的局限性，指明了未来研究的方向。

Abstract: We present SciArena, an open and collaborative platform for evaluating
foundation models on scientific literature tasks. Unlike traditional benchmarks
for scientific literature understanding and synthesis, SciArena engages the
research community directly, following the Chatbot Arena evaluation approach of
community voting on model comparisons. By leveraging collective intelligence,
SciArena offers a community-driven evaluation of model performance on
open-ended scientific tasks that demand literature-grounded, long-form
responses. The platform currently supports 23 open-source and proprietary
foundation models and has collected over 13,000 votes from trusted researchers
across diverse scientific domains. We analyze the data collected so far and
confirm that the submitted questions are diverse, aligned with real-world
literature needs, and that participating researchers demonstrate strong
self-consistency and inter-annotator agreement in their evaluations. We discuss
the results and insights based on the model ranking leaderboard. To further
promote research in building model-based automated evaluation systems for
literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based
on our collected preference data. The benchmark measures the accuracy of models
in judging answer quality by comparing their pairwise assessments with human
votes. Our experiments highlight the benchmark's challenges and emphasize the
need for more reliable automated evaluation methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [Moment Sampling in Video LLMs for Long-Form Video QA](https://arxiv.org/abs/2507.00033)
*Mustafa Chasmai,Gauri Jagatap,Gouthaman KV,Grant Van Horn,Subhransu Maji,Andrea Fanelli*

Main category: cs.CV

TL;DR: 针对长视频问答中视频大语言模型（Video LLM）帧采样效率低的问题，本文提出“时刻采样”方法，利用文本到视频时刻检索模型智能选择相关帧，显著提升了长视频问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有Video LLMs在长视频问答任务（特别是长距离推理）中表现不佳。常用的帧子采样方法存在缺陷，可能丢失关键信息或引入冗余帧，导致回答不准确和计算资源浪费。

Method: 提出“时刻采样”方法，这是一种模型无关的帧采样策略。它利用一个轻量级的文本到视频时刻检索模型，根据问题的上下文引导帧选择过程，优先选择与问题最相关的视频帧。

Result: 在四个长视频问答数据集上，结合四种最先进的Video LLMs进行广泛实验，结果表明所提出的“时刻采样”方法有效提升了长视频问答的性能。

Conclusion: 通过根据问题上下文聚焦于最相关的视频帧，“时刻采样”方法成功解决了长视频问答中帧采样的挑战，显著增强了Video LLMs在该任务上的表现。

Abstract: Recent advancements in video large language models (Video LLMs) have
significantly advanced the field of video question answering (VideoQA). While
existing methods perform well on short videos, they often struggle with
long-range reasoning in longer videos. To scale Video LLMs for longer video
content, frame sub-sampling (selecting frames at regular intervals) is commonly
used. However, this approach is suboptimal, often leading to the loss of
crucial frames or the inclusion of redundant information from multiple similar
frames. Missing key frames impairs the model's ability to answer questions
accurately, while redundant frames lead the model to focus on irrelevant video
segments and increase computational resource consumption. In this paper, we
investigate the use of a general-purpose text-to-video moment retrieval model
to guide the frame sampling process. We propose "moment sampling", a novel,
model-agnostic approach that enables the model to select the most relevant
frames according to the context of the question. Specifically, we employ a
lightweight moment retrieval model to prioritize frame selection. By focusing
on the frames most pertinent to the given question, our method enhances
long-form VideoQA performance in Video LLMs. Through extensive experiments on
four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we
demonstrate the effectiveness of the proposed approach.

</details>


### [44] [Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay](https://arxiv.org/abs/2507.00042)
*Xinrun Xu,Jianwen Yang,Qiuhong Zhang,Zhanbiao Lian,Zhiming Ding,Shan Jiang*

Main category: cs.CV

TL;DR: 本文提出ER-EMU算法，通过自适应经验回放和基于域距离的数据选择，有效解决了云边协同交通目标检测中边缘模型持续适应时的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 云边协同交通监控中的边缘模型在持续适应新数据分布时，面临灾难性遗忘问题，导致模型丢失先前学习的知识，尤其在动态交通环境（如昼夜、高峰期）中更为突出。现有方法（如经验回放、视觉提示）难以有效优先处理和利用历史数据，存在存储回放效率低、未考虑历史经验相关性差异等问题。

Method: 本文提出ER-EMU算法，一种基于自适应经验回放的边缘模型更新算法。该算法采用有限大小的FIFO（先进先出）经验缓冲区，并引入新颖的基于域距离度量（DDM-ES）的经验选择算法。DDM-ES利用多核最大均值差异（MK-MMD）量化域间不相似性，优先选择与当前目标域最不相似的历史数据，以确保训练多样性、保留更广泛的知识并防止过拟合。经验缓冲区还通过简单随机采样进行更新，以保持对先前域的均衡表示。

Result: 在包含重复昼夜循环的贝尔维尤交通视频数据集上的实验表明，ER-EMU算法持续改进了多个最先进的云边协同目标检测框架的性能。

Conclusion: ER-EMU通过智能的经验缓冲区管理和基于域距离的数据选择策略，有效缓解了动态交通监控中边缘模型的灾难性遗忘，提升了云边协同目标检测的持续适应能力。

Abstract: Continually adapting edge models in cloud-edge collaborative object detection
for traffic monitoring suffers from catastrophic forgetting, where models lose
previously learned knowledge when adapting to new data distributions. This is
especially problematic in dynamic traffic environments characterised by
periodic variations (e.g., day/night, peak hours), where past knowledge remains
valuable. Existing approaches like experience replay and visual prompts offer
some mitigation, but struggle to effectively prioritize and leverage historical
data for optimal knowledge retention and adaptation. Specifically, simply
storing and replaying all historical data can be inefficient, while treating
all historical experiences as equally important overlooks their varying
relevance to the current domain. This paper proposes ER-EMU, an edge model
update algorithm based on adaptive experience replay, to address these
limitations. ER-EMU utilizes a limited-size experience buffer managed using a
First-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based
Experience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel
maximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target
domains, prioritizing the selection of historical data that is most dissimilar
to the current target domain. This ensures training diversity and facilitates
the retention of knowledge from a wider range of past experiences, while also
preventing overfitting to the new domain. The experience buffer is also updated
using a simple random sampling strategy to maintain a balanced representation
of previous domains. Experiments on the Bellevue traffic video dataset,
involving repeated day/night cycles, demonstrate that ER-EMU consistently
improves the performance of several state-of-the-art cloud-edge collaborative
object detection frameworks.

</details>


### [45] [MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations](https://arxiv.org/abs/2507.00043)
*Mehmet Yigit Avci,Pedro Borges,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP是一个多模态对比学习框架，通过对齐MR图像与其DICOM元数据，学习对比度感知表示，无需手动标签，解决了MRI数据中元数据不完整或不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 准确解读MRI扫描依赖对图像对比度的精确理解，对比度由DICOM元数据中的采集参数决定。然而，现有对比度标签粗糙或缺失，元数据常不完整、有噪声或不一致，这阻碍了图像解释、检索和临床整合。此外，需要鲁棒的对比度感知表示以实现高级临床应用，如模态不变性表示和数据协调。

Method: 本文提出了MR-CLIP，一个多模态对比学习框架，旨在通过对齐MR图像与其DICOM元数据来学习对比度感知表示，且无需依赖手动标签。该模型在一个包含多种扫描仪和协议的多样化临床数据集上进行训练。

Result: MR-CLIP能够捕捉跨采集和扫描内部的对比度变化，并实现了与解剖结构无关的表示。研究结果表明，该框架在跨模态检索和对比度分类任务中表现出有效性，并展现了其可扩展性和在进一步临床应用中的巨大潜力。

Conclusion: MR-CLIP成功地从嘈杂或不完整的元数据中学习了对比度感知且与解剖结构无关的MR图像表示，有效解决了MRI数据处理中的关键挑战，并为高级临床应用（如图像解释、检索和数据协调）提供了有力的支持。

Abstract: Accurate interpretation of Magnetic Resonance Imaging scans in clinical
systems is based on a precise understanding of image contrast. This contrast is
primarily governed by acquisition parameters, such as echo time and repetition
time, which are stored in the DICOM metadata. To simplify contrast
identification, broad labels such as T1-weighted or T2-weighted are commonly
used, but these offer only a coarse approximation of the underlying acquisition
settings. In many real-world datasets, such labels are entirely missing,
leaving raw acquisition parameters as the only indicators of contrast. Adding
to this challenge, the available metadata is often incomplete, noisy, or
inconsistent. The lack of reliable and standardized metadata complicates tasks
such as image interpretation, retrieval, and integration into clinical
workflows. Furthermore, robust contrast-aware representations are essential to
enable more advanced clinical applications, such as achieving
modality-invariant representations and data harmonization. To address these
challenges, we propose MR-CLIP, a multimodal contrastive learning framework
that aligns MR images with their DICOM metadata to learn contrast-aware
representations, without relying on manual labels. Trained on a diverse
clinical dataset that spans various scanners and protocols, MR-CLIP captures
contrast variations across acquisitions and within scans, enabling
anatomy-invariant representations. We demonstrate its effectiveness in
cross-modal retrieval and contrast classification, highlighting its scalability
and potential for further clinical applications. The code and weights are
publicly available at https://github.com/myigitavci/MR-CLIP.

</details>


### [46] [HistoART: Histopathology Artifact Detection and Reporting Tool](https://arxiv.org/abs/2507.00044)
*Seyed Kahaki,Alexander R. Webber,Ghada Zamzmi,Adarsh Subbaswamy,Rucha Deshpande,Aldo Badano*

Main category: cs.CV

TL;DR: 该研究提出并比较了三种针对全切片图像（WSI）的伪影检测方法，其中基于基础模型的方法性能最优，旨在解决数字化病理中伪影影响分析的问题，并开发了质量报告工具。


<details>
  <summary>Details</summary>
Motivation: 现代癌症诊断中，全切片图像（WSI）虽被广泛应用，但其制备和扫描过程中引入的伪影会损害后续图像分析的准确性，因此需要有效的伪影检测方法。

Method: 提出了三种伪影检测方法并进行比较：1) 基于基础模型的方法（FMA），使用微调的UNI架构；2) 基于深度学习的方法（DLA），采用ResNet50骨干网络；3) 基于知识的方法（KBA），利用手工提取的纹理、颜色和频率特征。这些方法旨在检测六种常见伪影。研究在来自不同扫描仪的50,000多个图像块上进行了评估，并开发了质量报告记分卡。

Result: FMA在图像块级别的AUROC达到0.995（95% CI [0.994, 0.995]），表现最佳，显著优于基于ResNet50的方法（AUROC 0.977）和基于知识的方法（AUROC 0.940）。

Conclusion: 基于基础模型的方法在WSI伪影检测方面表现出卓越的性能。通过有效检测伪影并提供质量报告，可以显著提高数字化病理分析的可靠性。

Abstract: In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to
digitize tissue specimens for detailed, high-resolution examination; however,
other diagnostic approaches, such as liquid biopsy and molecular testing, are
also utilized based on the cancer type and clinical context. While WSI has
revolutionized digital histopathology by enabling automated, precise analysis,
it remains vulnerable to artifacts introduced during slide preparation and
scanning. These artifacts can compromise downstream image analysis. To address
this challenge, we propose and compare three robust artifact detection
approaches for WSIs: (1) a foundation model-based approach (FMA) using a
fine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning
approach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach
(KBA) leveraging handcrafted features from texture, color, and frequency-based
metrics. The methods target six common artifact types: tissue folds,
out-of-focus regions, air bubbles, tissue damage, marker traces, and blood
contamination. Evaluations were conducted on 50,000+ image patches from diverse
scanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA
achieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),
outperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])
and the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into
actionable insights, we developed a quality report scorecard that quantifies
high-quality patches and visualizes artifact distributions.

</details>


### [47] [CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning](https://arxiv.org/abs/2507.00045)
*Ming Li,Chenguang Wang,Yijun Liang,Xiyao Wang,Yuhang Zhou,Xiyang Wu,Yuqing Zhang,Ruiyi Zhang,Tianyi Zhou*

Main category: cs.CV

TL;DR: 现有MLLM在基准测试中表现出色，但面对受社交媒体启发的新型“CaughtCheating”任务时，其表现几乎为零，揭示了它们在复杂视觉推理方面的不足，并为实现人类级侦探能力提供了新挑战。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在现有基准测试中已接近满分，因此需要更具挑战性的任务来评估其真实能力；同时，尽管MLLMs展现出一定的侦探潜力，但其是否能匹配优秀人类侦探的性能仍是疑问。

Method: 调查了GPT-o3等MLLMs难以处理的硬场景，并提出了一种名为“CaughtCheating”的新型任务，其灵感来源于社交媒体上检测照片中可疑线索的需求。通过广泛的实验和分析，理解现有MLLMs为何缺乏解决此类任务的能力。

Result: 发现GPT-o3在“CaughtCheating”任务上的表现几乎为零，表明现有MLLMs在解决这类复杂的视觉感知和推理任务时存在显著能力不足。该任务被证实是一类具有高度实用价值的挑战性任务。

Conclusion: “CaughtCheating”任务为MLLMs的视觉感知和推理能力提供了一个重要的挑战性基准。成功解决此类任务是MLLMs获得人类级侦探感知和推理能力的关键一步。

Abstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have
achieved near-ceiling scores on various existing benchmarks, motivating a
demand for more challenging test tasks. These MLLMs have been reported to excel
in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their
potential as a detective who can notice minuscule cues in an image and weave
them into coherent, situational explanations, leading to a reliable answer. But
can they match the performance of excellent human detectives? To answer this
question, we investigate some hard scenarios where GPT-o3 can still handle, and
find a common scenario where o3's performance drops to nearly zero, which we
name CaughtCheating. It is inspired by the social media requests that ask
others to detect suspicious clues from photos shared by the poster's partner.
We conduct extensive experiments and analysis to understand why existing MLLMs
lack sufficient capability to solve this kind of task. CaughtCheating provides
a class of challenging visual perception and reasoning tasks with great value
and practical usage. Success in these tasks paves the way for MLLMs to acquire
human-level detective perception and reasoning capabilities.

</details>


### [48] [Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process](https://arxiv.org/abs/2507.00046)
*Akshansh Mishra,Eyob Mesele Sefene,Shivraman Thapliyal*

Main category: cs.CV

TL;DR: 该工作提出了一种基于进化计算（PSO）的图像分割方法，结合注意力加权可视化，用于分析增材搅拌摩擦沉积（AFSD）工艺中的材料健全性，有效识别并量化缺陷和界面质量。


<details>
  <summary>Details</summary>
Motivation: 分析增材搅拌摩擦沉积（AFSD）工艺中材料健全性（包括检测缺陷和特征）存在挑战，传统成像方法难以有效识别细微的材料过渡区和潜在缺陷。

Method: 提出基于进化计算的图像分割方法，核心是利用粒子群优化（PSO）算法自动确定最佳分割阈值。该方法整合了梯度幅值分析与距离变换，创建了新颖的注意力加权可视化图。通过自注意力图和多通道可视化（结合边界、空间关系和材料密度数据），对五种AFSD样品进行了分析。

Result: 该方法成功揭示了传统成像难以观察到的细微材料过渡区和潜在缺陷区域。PSO算法自动识别出最佳阈值（156-173），实现了材料界面的精确分割。多通道可视化有效结合了多种信息，量化了界面质量。注意力分析成功识别了AFSD接头中不完全结合和不均匀性区域。

Conclusion: 基于注意力分析的图像分割方法能够有效识别AFSD接头中的不完全结合和不均匀性，并提供量化指标，为增材制造组件的工艺优化和质量评估提供了重要工具。

Abstract: This work proposes an evolutionary computing-based image segmentation
approach for analyzing soundness in Additive Friction Stir Deposition (AFSD)
processes. Particle Swarm Optimization (PSO) was employed to determine optimal
segmentation thresholds for detecting defects and features in multilayer AFSD
builds. The methodology integrates gradient magnitude analysis with distance
transforms to create novel attention-weighted visualizations that highlight
critical interface regions. Five AFSD samples processed under different
conditions were analyzed using multiple visualization techniques i.e.
self-attention maps, and multi-channel visualization. These complementary
approaches reveal subtle material transition zones and potential defect regions
which were not readily observable through conventional imaging. The PSO
algorithm automatically identified optimal threshold values (ranging from
156-173) for each sample, enabling precise segmentation of material interfaces.
The multi-channel visualization technique effectively combines boundary
information (red channel), spatial relationships (green channel), and material
density data (blue channel) into cohesive representations that quantify
interface quality. The results demonstrate that attention-based analysis
successfully identifies regions of incomplete bonding and inhomogeneities in
AFSD joints, providing quantitative metrics for process optimization and
quality assessment of additively manufactured components.

</details>


### [49] [AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training](https://arxiv.org/abs/2507.00049)
*Feiyang Kang,Nadine Chang,Maying Shen,Marc T. Law,Rafid Mahmood,Ruoxi Jia,Jose M. Alvarez*

Main category: cs.CV

TL;DR: AdaDeDup是一个新的混合数据裁剪框架，它结合了密度剪枝和模型反馈，以簇自适应的方式在大规模数据集上显著提高数据效率，同时保持接近原始的模型性能。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集存在计算负担和固有冗余，对机器学习模型训练构成挑战。现有数据裁剪方法存在局限：基于密度的方法可能与任务无关，基于模型的方法可能引入冗余或计算成本高昂。

Method: 提出Adaptive De-Duplication (AdaDeDup)框架。该方法首先对数据进行分区并执行初步的密度剪枝。随后，利用代理模型评估每个簇内初始剪枝的影响（通过比较保留样本和剪枝样本的损失），此任务感知信号自适应调整簇特定的剪枝阈值，从而在冗余簇中更积极地剪枝，同时保留信息丰富簇中的关键数据。

Result: 在大型目标检测基准（Waymo, COCO, nuScenes）和标准模型（BEVFormer, Faster R-CNN）上的实验表明，AdaDeDup显著优于现有基线方法，大幅降低了性能下降（例如，在Waymo上比随机采样减少超过54%），并在剪枝20%数据的情况下，实现了接近原始模型的性能，有效提高了大规模模型训练的数据效率。

Conclusion: AdaDeDup通过其混合、任务感知和簇自适应的剪枝策略，成功解决了大规模数据训练中的效率和性能挑战，为提升大规模机器学习模型训练的数据效率提供了一个有效的解决方案。

Abstract: The computational burden and inherent redundancy of large-scale datasets
challenge the training of contemporary machine learning models. Data pruning
offers a solution by selecting smaller, informative subsets, yet existing
methods struggle: density-based approaches can be task-agnostic, while
model-based techniques may introduce redundancy or prove computationally
prohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid
framework that synergistically integrates density-based pruning with
model-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions
data and applies an initial density-based pruning. It then employs a proxy
model to evaluate the impact of this initial pruning within each cluster by
comparing losses on kept versus pruned samples. This task-aware signal
adaptively adjusts cluster-specific pruning thresholds, enabling more
aggressive pruning in redundant clusters while preserving critical data in
informative ones. Extensive experiments on large-scale object detection
benchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster
R-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms
prominent baselines, substantially reduces performance degradation (e.g., over
54% versus random sampling on Waymo), and achieves near-original model
performance while pruning 20% of data, highlighting its efficacy in enhancing
data efficiency for large-scale model training. Code is open-sourced.

</details>


### [50] [VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models](https://arxiv.org/abs/2507.00052)
*Binesh Sadanandan,Vahid Behzadan*

Main category: cs.CV

TL;DR: 研究开发了一个名为VSF-Med的端到端框架，用于评估医学视觉语言模型（VLM）的安全性，并揭示了现有先进VLM的显著漏洞。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）在医疗影像工作流程中具有巨大潜力，但目前临床环境中对其系统性安全评估严重不足。

Method: 引入VSF-Med，一个集成三项新颖组件的医学VLM漏洞评分框架：(i) 针对新兴威胁的复杂文本提示攻击模板库；(ii) 基于结构相似性（SSIM）阈值校准的不可感知视觉扰动；(iii) 由两个独立LLM评估的八维度评分标准，通过z-score归一化生成0-32的复合风险指标。该框架基于公开数据集和开源代码，能从5,000张放射图像合成超过30,000个对抗性变体，支持医学VLM的可复现基准测试。

Result: 综合分析显示，最先进的VLM在攻击效果持久性方面平均z-score偏移0.90σ，提示注入有效性为0.74σ，安全绕过成功率为0.63σ。其中，Llama-3.2-11B-Vision-Instruct在攻击效果持久性方面漏洞增加高达1.29σ，GPT-4o在该向量上增加0.69σ，在提示注入攻击上增加0.28σ。

Conclusion: VSF-Med框架为医学VLM的安全评估提供了可复现的工具，其研究结果表明当前先进的医学VLM在面对特定攻击时存在明显的安全漏洞，凸显了在临床应用前进行全面安全评估的必要性。

Abstract: Vision Language Models (VLMs) hold great promise for streamlining
labour-intensive medical imaging workflows, yet systematic security evaluations
in clinical settings remain scarce. We introduce VSF--Med, an end-to-end
vulnerability-scoring framework for medical VLMs that unites three novel
components: (i) a rich library of sophisticated text-prompt attack templates
targeting emerging threat vectors; (ii) imperceptible visual perturbations
calibrated by structural similarity (SSIM) thresholds to preserve clinical
realism; and (iii) an eight-dimensional rubric evaluated by two independent
judge LLMs, whose raw scores are consolidated via z-score normalization to
yield a 0--32 composite risk metric. Built entirely on publicly available
datasets and accompanied by open-source code, VSF--Med synthesizes over 30,000
adversarial variants from 5,000 radiology images and enables reproducible
benchmarking of any medical VLM with a single command. Our consolidated
analysis reports mean z-score shifts of $0.90\sigma$ for
persistence-of-attack-effects, $0.74\sigma$ for prompt-injection effectiveness,
and $0.63\sigma$ for safety-bypass success across state-of-the-art VLMs.
Notably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase
of $1.29\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases
of $0.69\sigma$ for that same vector and $0.28\sigma$ for prompt-injection
attacks.

</details>


### [51] [MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding](https://arxiv.org/abs/2507.00068)
*Ziqi Zhong,Daniel Tang*

Main category: cs.CV

TL;DR: MANTA框架通过将多模态输入统一到结构化文本空间，利用大型语言模型处理，显著提升了长视频问答、时序推理和跨模态理解的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态学习方法将不同模态独立处理，导致表示和推理存在不一致性。

Method: 引入MANTA框架，该框架将视觉和听觉输入统一到结构化文本空间，以便与大型语言模型无缝处理。它通过信息论优化实现语义对齐、自适应时间同步、分层内容表示和上下文感知检索，并提供严格的数学证明，确保上下文选择的最优性。

Result: 在长视频问答任务上，MANTA使现有SOTA模型整体准确率提升高达22.6%（超过30分钟视频提升27.3%）。在时序推理任务上提升23.8%，在跨模态理解任务上提升25.1%。

Conclusion: MANTA通过结构化文本统一多模态表示，并引入了新颖的密度估计技术，为多模态学习建立了新的基础。

Abstract: While multi-modal learning has advanced significantly, current approaches
often treat modalities separately, creating inconsistencies in representation
and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization
via Textual Alignment), a theoretically-grounded framework that unifies visual
and auditory inputs into a structured textual space for seamless processing
with large language models. MANTA addresses four key challenges: (1) semantic
alignment across modalities with information-theoretic optimization, (2)
adaptive temporal synchronization for varying information densities, (3)
hierarchical content representation for multi-scale understanding, and (4)
context-aware retrieval of sparse information from long sequences. We formalize
our approach within a rigorous mathematical framework, proving its optimality
for context selection under token constraints. Extensive experiments on the
challenging task of Long Video Question Answering show that MANTA improves
state-of-the-art models by up to 22.6% in overall accuracy, with particularly
significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we
demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)
and cross-modal understanding (25.1% improvement). Our framework introduces
novel density estimation techniques for redundancy minimization while
preserving rare signals, establishing new foundations for unifying multimodal
representations through structured text.

</details>


### [52] [An efficient plant disease detection using transfer learning approach](https://arxiv.org/abs/2507.00070)
*Bosubabu Sambana,Hillary Sunday Nnadi,Mohd Anas Wajid,Nwosu Ogochukwu Fidelia,Claudia Camacho-Zuñiga,Henry Dozie Ajuzie,Edeh Michael Onyema*

Main category: cs.CV

TL;DR: 本研究利用迁移学习和YOLOv7/v8模型，开发了一套自动植物病害检测系统，实现了对多种病害的早期、高效识别，以提升农业生产力。


<details>
  <summary>Details</summary>
Motivation: 植物病害对农业构成重大挑战，早期检测至关重要，能有效减轻损害。当前技术发展为自动化植物病害监测提供了契机。

Method: 本研究提出一种基于迁移学习的植物病害识别和监测系统。具体利用YOLOv7和YOLOv8两种先进目标检测模型，通过在植物叶片图像数据集上进行微调，以检测细菌、真菌和病毒性病害（如白粉病、角斑病、早疫病和番茄花叶病毒）。

Result: 模型性能评估指标包括mAP、F1-score、Precision和Recall，分别达到91.05、89.40、91.22和87.66。结果表明YOLOv8在效果和效率上优于其他目标检测方法。

Conclusion: 该方法提供了一种可扩展、自动化的植物病害早期检测方案，有助于提高作物产量，减少人工监测依赖，并支持可持续农业实践。

Abstract: Plant diseases pose significant challenges to farmers and the agricultural
sector at large. However, early detection of plant diseases is crucial to
mitigating their effects and preventing widespread damage, as outbreaks can
severely impact the productivity and quality of crops. With advancements in
technology, there are increasing opportunities for automating the monitoring
and detection of disease outbreaks in plants. This study proposed a system
designed to identify and monitor plant diseases using a transfer learning
approach. Specifically, the study utilizes YOLOv7 and YOLOv8, two
state-ofthe-art models in the field of object detection. By fine-tuning these
models on a dataset of plant leaf images, the system is able to accurately
detect the presence of Bacteria, Fungi and Viral diseases such as Powdery
Mildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's
performance was evaluated using several metrics, including mean Average
Precision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,
89.40, 91.22, and 87.66, respectively. The result demonstrates the superior
effectiveness and efficiency of YOLOv8 compared to other object detection
methods, highlighting its potential for use in modern agricultural practices.
The approach provides a scalable, automated solution for early any plant
disease detection, contributing to enhanced crop yield, reduced reliance on
manual monitoring, and supporting sustainable agricultural practices.

</details>


### [53] [Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics](https://arxiv.org/abs/2507.00153)
*Peter Mortimer,Mirko Maehlisch*

Main category: cs.CV

TL;DR: 本文提出一种基于扩散的图像增强方法，旨在通过使训练数据更好地代表部署环境（如雪地），解决自动驾驶汽车在异分布环境下的感知性能问题。


<details>
  <summary>Details</summary>
Motivation: 学习型感知算法在异分布和代表性不足的环境中表现不佳。室外机器人易受动态光照、季节和天气（如雪）影响，导致训练数据无法充分覆盖。本研究旨在为自动驾驶汽车在雪地环境的部署做准备。

Method: 提出一种新颖的基于扩散的图像增强方法。该方法利用基于互联网规模数据集的视觉基础模型，以控制训练数据中地表语义分布，使其更接近部署环境。同时，采用开放词汇语义分割模型过滤掉增强图像中的幻觉内容。

Result: 本文为概念性论文，提出了一种有望使训练数据更准确地代表部署环境（如雪地）的增强方法，从而预期能提升模型在该特定环境下的性能。该方法允许对训练数据的语义分布进行精细控制。

Conclusion: 基于扩散的图像增强方法有望扩展应用于除雪地之外的多种其他环境，例如沙地和火山地形，为提升学习型感知系统在不同部署环境下的鲁棒性提供了新途径。

Abstract: The performance of leaning-based perception algorithms suffer when deployed
in out-of-distribution and underrepresented environments. Outdoor robots are
particularly susceptible to rapid changes in visual scene appearance due to
dynamic lighting, seasonality and weather effects that lead to scenes
underrepresented in the training data of the learning-based perception system.
In this conceptual paper, we focus on preparing our autonomous vehicle for
deployment in snow-filled environments. We propose a novel method for
diffusion-based image augmentation to more closely represent the deployment
environment in our training data. Diffusion-based image augmentations rely on
the public availability of vision foundation models learned on internet-scale
datasets. The diffusion-based image augmentations allow us to take control over
the semantic distribution of the ground surfaces in the training data and to
fine-tune our model for its deployment environment. We employ open vocabulary
semantic segmentation models to filter out augmentation candidates that contain
hallucinations. We believe that diffusion-based image augmentations can be
extended to many other environments apart from snow surfaces, like sandy
environments and volcanic terrains.

</details>


### [54] [FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion](https://arxiv.org/abs/2507.00162)
*Yu Lu,Yi Yang*

Main category: cs.CV

TL;DR: 现有模型生成长视频时存在时间一致性和视觉保真度下降问题。本文提出无训练框架FreeLong/FreeLong++，通过融合多尺度频率特征来解决高频失真，显著提升长视频生成质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 文本到视频生成模型在生成高质量短视频方面取得进展，但扩展到长视频时，面临时间一致性和视觉保真度下降的挑战，主要表现为高频分量失真。

Method: 提出无训练框架FreeLong，通过融合全局低频特征与局部高频特征来平衡长视频特征的频率分布。在此基础上，FreeLong++进一步扩展为多分支架构，通过多注意力分支在不同时间尺度上实现多频带融合。该框架可即插即用于现有视频生成模型。

Result: FreeLong++显著提升了现有模型生成长视频（如原始长度的4倍和8倍）的时间一致性和视觉保真度。它在长视频生成任务上优于现有方法，并支持多提示词连贯生成和平滑场景过渡，还能使用长深度或姿态序列进行可控视频生成。

Conclusion: 通过多尺度频率融合的无训练框架FreeLong/FreeLong++，有效解决了长视频生成中的高频失真问题，显著提高了生成视频的质量、一致性和可控性，展现了良好的通用性。

Abstract: Recent advances in video generation models have enabled high-quality short
video generation from text prompts. However, extending these models to longer
videos remains a significant challenge, primarily due to degraded temporal
consistency and visual fidelity. Our preliminary observations show that naively
applying short-video generation models to longer sequences leads to noticeable
quality degradation. Further analysis identifies a systematic trend where
high-frequency components become increasingly distorted as video length grows,
an issue we term high-frequency distortion. To address this, we propose
FreeLong, a training-free framework designed to balance the frequency
distribution of long video features during the denoising process. FreeLong
achieves this by blending global low-frequency features, which capture holistic
semantics across the full video, with local high-frequency features extracted
from short temporal windows to preserve fine details. Building on this,
FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture
with multiple attention branches, each operating at a distinct temporal scale.
By arranging multiple window sizes from global to local, FreeLong++ enables
multi-band frequency fusion from low to high frequencies, ensuring both
semantic continuity and fine-grained motion dynamics across longer video
sequences. Without any additional training, FreeLong++ can be plugged into
existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer
videos with substantially improved temporal consistency and visual fidelity. We
demonstrate that our approach outperforms previous methods on longer video
generation tasks (e.g. 4x and 8x of native length). It also supports coherent
multi-prompt video generation with smooth scene transitions and enables
controllable video generation using long depth or pose sequences.

</details>


### [55] [SelvaBox: A high-resolution dataset for tropical tree crown detection](https://arxiv.org/abs/2507.00170)
*Hugo Baudchon,Arthur Ouaknine,Martin Weiss,Mélisande Teng,Thomas R. Walla,Antoine Caron-Guay,Christopher Pal,Etienne Laliberté*

Main category: cs.CV

TL;DR: 研究人员推出了SelvaBox，这是最大的热带树冠公开数据集，包含超过83,000个手动标注，显著提升了热带森林树冠检测的准确性和零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 检测热带森林中的单棵树冠对于研究受人类干预和气候变化影响的复杂生态系统至关重要。然而，热带树冠尺寸、结构和模式差异大，且高度重叠交错，需要先进的遥感方法和高分辨率图像。当前，缺乏大规模标注数据集是阻碍鲁棒模型开发的主要瓶颈。

Method: 研究引入了SelvaBox数据集，这是最大的开放获取热带树冠检测数据集，跨越三个国家，包含超过83,000个手动标注的树冠。在此数据集上进行了广泛的基准测试，并探索了更高分辨率输入的影响。此外，还采用统一的多分辨率管道，将SelvaBox与另外三个分辨率从3到10厘米/像素的数据集联合训练。

Result: 基准测试显示，更高分辨率的输入能持续提高检测精度。仅使用SelvaBox训练的模型在未见过的热带树冠数据集上实现了有竞争力的零样本检测性能，媲美或超越现有方法。通过统一的多分辨率管道，在SelvaBox和另外三个数据集上联合训练的检测器在所有评估数据集中均排名第一或第二。

Conclusion: SelvaBox数据集的发布显著解决了热带树冠检测领域数据稀缺的问题，其规模和质量使得模型能够实现更高的检测精度和出色的跨数据集泛化能力。这项工作为热带森林的生态研究提供了强大的工具和资源。

Abstract: Detecting individual tree crowns in tropical forests is essential to study
these complex and crucial ecosystems impacted by human interventions and
climate change. However, tropical crowns vary widely in size, structure, and
pattern and are largely overlapping and intertwined, requiring advanced remote
sensing methods applied to high-resolution imagery. Despite growing interest in
tropical tree crown detection, annotated datasets remain scarce, hindering
robust model development. We introduce SelvaBox, the largest open-access
dataset for tropical tree crown detection in high-resolution drone imagery. It
spans three countries and contains more than 83,000 manually labeled crowns -
an order of magnitude larger than all previous tropical forest datasets
combined. Extensive benchmarks on SelvaBox reveal two key findings: (1)
higher-resolution inputs consistently boost detection accuracy; and (2) models
trained exclusively on SelvaBox achieve competitive zero-shot detection
performance on unseen tropical tree crown datasets, matching or exceeding
competing methods. Furthermore, jointly training on SelvaBox and three other
datasets at resolutions from 3 to 10 cm per pixel within a unified
multi-resolution pipeline yields a detector ranking first or second across all
evaluated datasets. Our dataset, code, and pre-trained weights are made public.

</details>


### [56] [Graph-Based Deep Learning for Component Segmentation of Maize Plants](https://arxiv.org/abs/2507.00182)
*J. I. Ruíz,A. Méndez,E. Rodríguez*

Main category: cs.CV

TL;DR: 提出一种基于图神经网络（GNN）的新型深度学习架构，结合主成分分析（PCA），用于LiDAR点云数据中植物个体组分的识别，有效提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 在精准农业中，识别植物个体组分是一项重要任务。然而，传统的2D成像、3D重建和卷积神经网络（CNN）方法在处理3D数据和识别植物组分时存在局限性。

Method: 本研究提出一种基于图神经网络（GNN）的新型深度学习架构，并结合主成分分析（PCA）进行特征增强。该架构将点云中的每个点视为图的顶点，利用K-Nearest Neighbors（KNN）层建立边，随后通过Edge-Conv层进一步增强特征，最终应用图注意力网络（GAT）对叶片、茎秆和土壤等植物可见表型组分进行分类。

Result: 研究结果表明，该基于图的深度学习方法显著提高了植物组分识别的分割精度，平均IoU（交并比）达到80%以上，性能优于其他现有的基于点云的模型。

Conclusion: 所提出的基于图的深度学习方法有效增强了植物个体组分的分割准确性，并超越了现有模型，为精准农业中的植物表型分析提供了新的解决方案。

Abstract: In precision agriculture, one of the most important tasks when exploring crop
production is identifying individual plant components. There are several
attempts to accomplish this task by the use of traditional 2D imaging, 3D
reconstructions, and Convolutional Neural Networks (CNN). However, they have
several drawbacks when processing 3D data and identifying individual plant
components. Therefore, in this work, we propose a novel Deep Learning
architecture to detect components of individual plants on Light Detection and
Ranging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on
the concept of Graph Neural Networks (GNN), and feature enhancing with
Principal Component Analysis (PCA). For this, each point is taken as a vertex
and by the use of a K-Nearest Neighbors (KNN) layer, the edges are established,
thus representing the 3D PC data set. Subsequently, Edge-Conv layers are used
to further increase the features of each point. Finally, Graph Attention
Networks (GAT) are applied to classify visible phenotypic components of the
plant, such as the leaf, stem, and soil. This study demonstrates that our
graph-based deep learning approach enhances segmentation accuracy for
identifying individual plant components, achieving percentages above 80% in the
IoU average, thus outperforming other existing models based on point clouds.

</details>


### [57] [Computer Vision for Objects used in Group Work: Challenges and Opportunities](https://arxiv.org/abs/2507.00224)
*Changsoo Jung,Sheikh Mannan,Jack Fitzgerald,Nathaniel Blanchard*

Main category: cs.CV

TL;DR: 本文提出了FiboSB，一个用于K-12协作学习环境的6D姿态视频数据集，旨在解决现有系统在捕捉学生与物理对象互动方面的不足。研究评估了现有6D姿态方法，发现物体检测是瓶颈，并通过微调YOLO11-x显著提升了检测性能，为在复杂协作场景中应用6D姿态估计奠定了基础。


<details>
  <summary>Details</summary>
Motivation: K-12教育中，交互式和空间感知技术对促进实践探索至关重要。然而，现有系统在协作任务中难以准确捕捉学生与物理对象的真实互动。6D姿态估计能解决此问题，但缺乏在小组协作（涉及小型手持物体、远距离整体录制）这一复杂场景下的有效数据和方法。

Method: 引入了FiboSB，一个新颖的6D姿态视频数据集，记录三名参与者协作完成包含小立方体和秤的交互任务。数据集特点是远距离整体录制。研究在FiboSB上评估了四种SOTA 6D姿态估计方法，并进行误差分析。针对发现的物体检测模块失效问题，微调了YOLO11-x。

Result: 现有SOTA 6D姿态估计方法在协作小组工作场景下表现出局限性，其物体检测模块是主要失败原因。通过微调YOLO11-x在FiboSB数据集上，物体检测的mAP_50达到了0.898。

Conclusion: FiboSB数据集、基准测试结果以及YOLO11-x的误差分析，为在困难协作环境中利用6D姿态估计奠定了基础，凸显了在这种场景下鲁棒物体检测的重要性。

Abstract: Interactive and spatially aware technologies are transforming educational
frameworks, particularly in K-12 settings where hands-on exploration fosters
deeper conceptual understanding. However, during collaborative tasks, existing
systems often lack the ability to accurately capture real-world interactions
between students and physical objects. This issue could be addressed with
automatic 6D pose estimation, i.e., estimation of an object's position and
orientation in 3D space from RGB images or videos. For collaborative groups
that interact with physical objects, 6D pose estimates allow AI systems to
relate objects and entities. As part of this work, we introduce FiboSB, a novel
and challenging 6D pose video dataset featuring groups of three participants
solving an interactive task featuring small hand-held cubes and a weight scale.
This setup poses unique challenges for 6D pose because groups are holistically
recorded from a distance in order to capture all participants -- this, coupled
with the small size of the cubes, makes 6D pose estimation inherently
non-trivial. We evaluated four state-of-the-art 6D pose estimation methods on
FiboSB, exposing the limitations of current algorithms on collaborative group
work. An error analysis of these methods reveals that the 6D pose methods'
object detection modules fail. We address this by fine-tuning YOLO11-x for
FiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,
and analysis of YOLO11-x errors presented here lay the groundwork for
leveraging the estimation of 6D poses in difficult collaborative contexts.

</details>


### [58] [VOCAL: Visual Odometry via ContrAstive Learning](https://arxiv.org/abs/2507.00243)
*Chi-Yao Huang,Zeel Bhatt,Yezhou Yang*

Main category: cs.CV

TL;DR: VOCAL是一个将视觉里程计(VO)重构为标签排序挑战的新框架，它通过贝叶斯推断和对比学习提升了学习特征的可解释性与灵活性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉里程计(VO)取得了进展，但许多基于学习的VO技术依赖于刚性几何假设，这导致其可解释性不足，并且在纯数据驱动框架中缺乏坚实的理论基础。

Method: 提出了VOCAL（Visual Odometry via ContrAstive Learning）框架，将VO重新构想为标签排序挑战。通过将贝叶斯推断与表示学习框架相结合，VOCAL组织视觉特征以反映相机状态，并利用排序机制强制相似相机状态在潜在空间中收敛到一致且空间连贯的表示。

Result: 在KITTI数据集上的广泛评估表明，VOCAL显著增强了所学特征的可解释性和框架的灵活性，同时确保了与多模态数据的兼容性。

Conclusion: VOCAL框架推动了视觉里程计向更通用和可解释的空间智能发展。

Abstract: Breakthroughs in visual odometry (VO) have fundamentally reshaped the
landscape of robotics, enabling ultra-precise camera state estimation that is
crucial for modern autonomous systems. Despite these advances, many
learning-based VO techniques rely on rigid geometric assumptions, which often
fall short in interpretability and lack a solid theoretical basis within fully
data-driven frameworks. To overcome these limitations, we introduce VOCAL
(Visual Odometry via ContrAstive Learning), a novel framework that reimagines
VO as a label ranking challenge. By integrating Bayesian inference with a
representation learning framework, VOCAL organizes visual features to mirror
camera states. The ranking mechanism compels similar camera states to converge
into consistent and spatially coherent representations within the latent space.
This strategic alignment not only bolsters the interpretability of the learned
features but also ensures compatibility with multimodal data sources. Extensive
evaluations on the KITTI dataset highlight VOCAL's enhanced interpretability
and flexibility, pushing VO toward more general and explainable spatial
intelligence.

</details>


### [59] [Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition](https://arxiv.org/abs/2507.00248)
*Nikita Nikitin,Eugene Fomin*

Main category: cs.CV

TL;DR: 一个用于实时手语识别的新型轻量级深度神经网络框架，能够处理有限数据并优化边缘设备性能。


<details>
  <summary>Details</summary>
Motivation: 解决手语识别领域面临的数据稀缺、高计算成本以及训练与推理环境帧率不匹配等关键挑战。

Method: 将手语特有参数（如手形、掌向、运动、位置）编码为向量化输入，利用MediaPipe进行关键点提取以获得高可分离性数据表示。设计并优化了占用空间小于10MB的轻量级DNN架构，并使用“slait data”平台辅助数据标注和向量提取。

Result: 模型在孤立手语识别中实现了92%的准确率，能够在边缘设备上以低于10毫秒的延迟对343个手语进行准确分类。该系统已集成到“slait ai”网络应用中，表现出稳定的推理能力。

Conclusion: 该框架提供了一个高效、准确且资源友好的实时手语识别解决方案，尤其适用于数据和计算资源受限的环境，展示了在边缘设备和实际应用中的潜力。

Abstract: We present a novel framework for real-time sign language recognition using
lightweight DNNs trained on limited data. Our system addresses key challenges
in sign language recognition, including data scarcity, high computational
costs, and discrepancies in frame rates between training and inference
environments. By encoding sign language specific parameters, such as handshape,
palm orientation, movement, and location into vectorized inputs, and leveraging
MediaPipe for landmark extraction, we achieve highly separable input data
representations. Our DNN architecture, optimized for sub 10MB deployment,
enables accurate classification of 343 signs with less than 10ms latency on
edge devices. The data annotation platform 'slait data' facilitates structured
labeling and vector extraction. Our model achieved 92% accuracy in isolated
sign recognition and has been integrated into the 'slait ai' web application,
where it demonstrates stable inference.

</details>


### [60] [GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception](https://arxiv.org/abs/2507.00253)
*Zhuangzhuang Dai,Vincent Gbouna Zakka,Luis J. Manso,Chen Li*

Main category: cs.CV

TL;DR: 本文提出GazeTarget360系统，旨在从图像中实现广义视觉场景下的360度注视目标估计，克服了现有方法的局限性，实现了高效准确的预测。


<details>
  <summary>Details</summary>
Motivation: 使机器人理解人类注视目标对于下游任务（如人机交互中的注意力估计和运动预测）至关重要。现有方法存在局限性：仅限于帧内目标定位，且视觉注视估计方法（如OpenFace）未能有效吸收背景信息，无法预测被摄者看向相机以外的注视目标。

Method: 提出名为GazeTarget360的系统，用于从图像中估计广义视觉场景中的360度注视目标。该系统整合了眼球接触检测器、预训练视觉编码器和多尺度融合解码器的条件推理引擎。

Result: 交叉验证结果表明，GazeTarget360系统在未知场景中能产生准确可靠的注视目标预测。

Conclusion: GazeTarget360是首个能够从真实相机镜头中预测注视目标的系统，具有高效性和可部署性。研究人员已公开其源代码。

Abstract: Enabling robots to understand human gaze target is a crucial step to allow
capabilities in downstream tasks, for example, attention estimation and
movement anticipation in real-world human-robot interactions. Prior works have
addressed the in-frame target localization problem with data-driven approaches
by carefully removing out-of-frame samples. Vision-based gaze estimation
methods, such as OpenFace, do not effectively absorb background information in
images and cannot predict gaze target in situations where subjects look away
from the camera. In this work, we propose a system to address the problem of
360-degree gaze target estimation from an image in generalized visual scenes.
The system, named GazeTarget360, integrates conditional inference engines of an
eye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion
decoder. Cross validation results show that GazeTarget360 can produce accurate
and reliable gaze target predictions in unseen scenarios. This makes a
first-of-its-kind system to predict gaze targets from realistic camera footage
which is highly efficient and deployable. Our source code is made publicly
available at: https://github.com/zdai257/DisengageNet.

</details>


### [61] [VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos](https://arxiv.org/abs/2507.00261)
*Zhiyin Lin,Purvi Goel,Joy Yun,C. Karen Liu,Joao Pedro Araujo*

Main category: cs.CV

TL;DR: 开发了VirtualFencer系统，能从实拍视频中无监督提取3D击剑动作和策略，并生成逼真的击剑行为，已通过多种对战模式验证其能力。


<details>
  <summary>Details</summary>
Motivation: 击剑运动动作多样且具战略性，运动员行为受对手影响。这种动作多样性和双人策略的结合，激发了对击剑进行数据驱动建模的需求。

Method: 提出VirtualFencer系统，该系统能够从实拍视频中无监督地提取3D击剑动作和策略，并利用这些提取的知识生成逼真的击剑行为。

Result: 系统展示了其多功能性，包括：(i) 自我对战（self-play），(ii) 与在线视频中的真实击剑动作对战，以及 (iii) 与专业击剑运动员进行互动对战。

Conclusion: VirtualFencer系统成功实现了从无监督视频中提取击剑运动数据并生成逼真击剑行为，证明了数据驱动模型在分析和模拟复杂体育运动中的潜力。

Abstract: Fencing is a sport where athletes engage in diverse yet strategically logical
motions. While most motions fall into a few high-level actions (e.g. step,
lunge, parry), the execution can vary widely-fast vs. slow, large vs. small,
offensive vs. defensive. Moreover, a fencer's actions are informed by a
strategy that often comes in response to the opponent's behavior. This
combination of motion diversity with underlying two-player strategy motivates
the application of data-driven modeling to fencing. We present VirtualFencer, a
system capable of extracting 3D fencing motion and strategy from in-the-wild
video without supervision, and then using that extracted knowledge to generate
realistic fencing behavior. We demonstrate the versatile capabilities of our
system by having it (i) fence against itself (self-play), (ii) fence against a
real fencer's motion from online video, and (iii) fence interactively against a
professional fencer.

</details>


### [62] [Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections](https://arxiv.org/abs/2507.00263)
*Vignesh Ram Nithin Kappagantula,Shayan Hassantabar*

Main category: cs.CV

TL;DR: 本文提出一种机器学习流程，用于自动对度假租赁平台中未结构化分类的房产图片进行房间场景发现与分组，并识别卧室内的床型，帮助旅客理解空间布局。该方法计算高效且性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 度假租赁平台上的房产图片数量剧增但缺乏结构化分类，导致旅客难以理解房产的空间布局，尤其当存在多个同类型房间时，这构成了重大挑战。

Method: 本文提出了一个计算高效、低延迟且样本高效的机器学习流程。该流程整合了：1) 一个监督式房间类型检测模型；2) 一个监督式重叠检测模型，用于识别两张图片间的重叠相似性；3) 一个聚类算法，利用相似性分数将同一空间的图片进行分组。此外，该流程还使用多模态大语言模型（MLLM）根据卧室组的视觉内容和房产元数据，将每个卧室组映射到相应的床型。

Result: 所提出的模型和整个流程在评估中均表现出强大的性能。实验结果表明，该方法显著优于对比学习和基于预训练嵌入的聚类等既有方法。

Conclusion: 所提出的机器学习流程有效解决了房间场景发现和分组问题，并成功识别了床型。该方案能够帮助旅客更好地理解度假租赁房产的空间组织、布局和睡眠配置，并且其性能显著优于现有方法，适用于实时和数据稀缺的环境。

Abstract: The rapid growth of vacation rental (VR) platforms has led to an increasing
volume of property images, often uploaded without structured categorization.
This lack of organization poses significant challenges for travelers attempting
to understand the spatial layout of a property, particularly when multiple
rooms of the same type are present. To address this issue, we introduce an
effective approach for solving the room scene discovery and grouping problem,
as well as identifying bed types within each bedroom group. This grouping is
valuable for travelers to comprehend the spatial organization, layout, and the
sleeping configuration of the property. We propose a computationally efficient
machine learning pipeline characterized by low latency and the ability to
perform effectively with sample-efficient learning, making it well-suited for
real-time and data-scarce environments. The pipeline integrates a supervised
room-type detection model, a supervised overlap detection model to identify the
overlap similarity between two images, and a clustering algorithm to group the
images of the same space together using the similarity scores. Additionally,
the pipeline maps each bedroom group to the corresponding bed types specified
in the property's metadata, based on the visual content present in the group's
images using a Multi-modal Large Language Model (MLLM) model. We evaluate the
aforementioned models individually and also assess the pipeline in its
entirety, observing strong performance that significantly outperforms
established approaches such as contrastive learning and clustering with
pretrained embeddings.

</details>


### [63] [Self-Supervised Multiview Xray Matching](https://arxiv.org/abs/2507.00287)
*Mohamad Dabboussi,Malo Huard,Yann Gousseau,Pietro Gori*

Main category: cs.CV

TL;DR: 本文提出一种新颖的自监督Transformer模型，通过从未标注的CT体积生成合成X射线（DRR）来学习多视角X射线图像间的对应关系，并证明该方法能有效提升真实数据上的多视角骨折检测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在单张X射线图像分析方面取得了进展，但现有方法难以在不同X射线视图间建立鲁棒的对应关系，而这对于精确的临床评估（如骨折诊断）至关重要。

Method: 开发了一种新颖的自监督流水线，通过从未标注的CT体积自动生成数字重建X射线（DRR），从而在合成X射线视图间生成多对多对应矩阵，无需手动标注。该方法采用基于Transformer的训练阶段来预测多视图间的对应关系，并将合成视图的对应学习用作预训练策略，以增强真实数据上的多视角骨折检测。

Result: 研究表明，学习合成X射线视图间的对应关系可以作为有效的预训练策略。在合成和真实X射线数据集上的广泛评估显示，整合这些对应关系显著提高了多视角骨折分类的性能。

Conclusion: 该自监督方法成功解决了多视角X射线图像对应关系的挑战，通过预训练提升了多视角骨折检测等临床诊断的准确性。

Abstract: Accurate interpretation of multi-view radiographs is crucial for diagnosing
fractures, muscular injuries, and other anomalies. While significant advances
have been made in AI-based analysis of single images, current methods often
struggle to establish robust correspondences between different X-ray views, an
essential capability for precise clinical evaluations. In this work, we present
a novel self-supervised pipeline that eliminates the need for manual annotation
by automatically generating a many-to-many correspondence matrix between
synthetic X-ray views. This is achieved using digitally reconstructed
radiographs (DRR), which are automatically derived from unannotated CT volumes.
Our approach incorporates a transformer-based training phase to accurately
predict correspondences across two or more X-ray views. Furthermore, we
demonstrate that learning correspondences among synthetic X-ray views can be
leveraged as a pretraining strategy to enhance automatic multi-view fracture
detection on real data. Extensive evaluations on both synthetic and real X-ray
datasets show that incorporating correspondences improves performance in
multi-view fracture classification.

</details>


### [64] [Reducing Variability of Multiple Instance Learning Methods for Digital Pathology](https://arxiv.org/abs/2507.00292)
*Ali Mammadov,Loïc Le Folgoc,Guillaume Hocquet,Pietro Gori*

Main category: cs.CV

TL;DR: 针对数字病理学中全玻片图像（WSI）分类的多数实例学习（MIL）方法性能不稳定性问题，本文提出了一种多保真度模型融合策略，以降低变异性、提高复现性并简化超参数调整。


<details>
  <summary>Details</summary>
Motivation: 数字病理学中，全玻片图像（WSI）因其高分辨率和大数据量，在应用深度学习模型时面临挑战。多数实例学习（MIL）方法被用于WSI分类，但其性能在不同运行中表现出高达10-15 AUC点的显著变异性，这主要归因于权重初始化、批次排序和学习率，使得MIL方法之间的可靠比较变得困难。

Method: 本文引入了一种多保真度模型融合策略。该方法首先对多个模型进行少量epochs的训练，然后根据验证分数平均最稳定和最有前景的模型。此方法可应用于任何现有MIL模型，以降低性能变异性。

Result: 该方法成功降低了性能变异性，简化了超参数调整，提高了模型的复现性，同时保持了计算效率。研究在2个不同数据集、3种初始化策略和5种MIL方法上进行了广泛验证，共计超过2000次实验。

Conclusion: 所提出的多保真度模型融合策略有效解决了MIL方法在WSI分类中性能变异性高的问题。它提升了模型比较的可靠性和实验的复现性，且计算效率高，为数字病理学领域的MIL应用提供了有价值的解决方案。

Abstract: Digital pathology has revolutionized the field by enabling the digitization
of tissue samples into whole slide images (WSIs). However, the high resolution
and large size of WSIs present significant challenges when it comes to applying
Deep Learning models. As a solution, WSIs are often divided into smaller
patches with a global label (\textit{i.e., diagnostic}) per slide, instead of a
(too) costly pixel-wise annotation. By treating each slide as a bag of patches,
Multiple Instance Learning (MIL) methods have emerged as a suitable solution
for WSI classification. A major drawback of MIL methods is their high
variability in performance across different runs, which can reach up to 10-15
AUC points on the test set, making it difficult to compare different MIL
methods reliably. This variability mainly comes from three factors: i) weight
initialization, ii) batch (shuffling) ordering, iii) and learning rate. To
address that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL
methods. We first train multiple models for a few epochs and average the most
stable and promising ones based on validation scores. This approach can be
applied to any existing MIL model to reduce performance variability. It also
simplifies hyperparameter tuning and improves reproducibility while maintaining
computational efficiency. We extensively validate our approach on WSI
classification tasks using 2 different datasets, 3 initialization strategies
and 5 MIL methods, for a total of more than 2000 experiments.

</details>


### [65] [Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes](https://arxiv.org/abs/2507.00327)
*Chuyan Zhang,Kefan Wang,Yun Gu*

Main category: cs.CV

TL;DR: 传统LoRA在领域差距大时受固定低秩限制，现有自适应方法计算昂贵。SR-LoRA利用预训练权重矩阵的“稳定秩”进行层级秩分配，无需额外搜索成本，能在性能与效率间取得更优平衡。


<details>
  <summary>Details</summary>
Motivation: Low-Rank Adaptation (LoRA) 的固定低秩结构限制了其在领域差距大的任务中的适应性，而现有自适应LoRA方法通常依赖于计算密集型技术（如迭代剪枝、秩搜索）来动态分配秩，导致效率低下。

Method: 提出Stable Rank-Guided Low-Rank Adaptation (SR-LoRA) 框架。该方法利用预训练权重矩阵的“稳定秩”作为先验信息，以 principled 和 efficient 的方式进行层级秩分配，从而根据权重的内在维度实现秩的重新分配，且无需额外搜索成本。

Result: 在具有显著领域差距的少样本任务上，SR-LoRA持续优于近期自适应LoRA变体，并在性能和效率之间取得了更优的权衡。

Conclusion: SR-LoRA通过利用稳定秩实现了高效且适应性强的层级秩分配，有效克服了传统LoRA在领域差距大时的局限性及现有自适应方法的计算成本问题，并在性能和效率上达到了先进水平。

Abstract: Low-Rank Adaptation (LoRA) has proven effective in reducing computational
costs while maintaining performance comparable to fully fine-tuned foundation
models across various tasks. However, its fixed low-rank structure restricts
its adaptability in scenarios with substantial domain gaps, where higher ranks
are often required to capture domain-specific complexities. Current adaptive
LoRA methods attempt to overcome this limitation by dynamically expanding or
selectively allocating ranks, but these approaches frequently depend on
computationally intensive techniques such as iterative pruning, rank searches,
or additional regularization. To address these challenges, we introduce Stable
Rank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the
stable rank of pre-trained weight matrices as a natural prior for layer-wise
rank allocation. By leveraging the stable rank, which reflects the intrinsic
dimensionality of the weights, SR-LoRA enables a principled and efficient
redistribution of ranks across layers, enhancing adaptability without incurring
additional search costs. Empirical evaluations on few-shot tasks with
significant domain gaps show that SR-LoRA consistently outperforms recent
adaptive LoRA variants, achieving a superior trade-off between performance and
efficiency. Our code is available at
https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA.

</details>


### [66] [MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms](https://arxiv.org/abs/2507.00328)
*Xuan Liu,Yinhao Ren,Marc D. Ryser,Lars J. Grimm,Joseph Y. Lo*

Main category: cs.CV

TL;DR: 本文提出了MammoTracker，一个掩膜引导的乳腺X线照片病灶追踪框架。该框架通过粗到细策略和构建大型数据集，实现了自动化的病灶定位，并在性能上超越了基线模型。


<details>
  <summary>Details</summary>
Motivation: 在乳腺癌监测和早期诊断中，准确追踪时间序列乳腺X线照片中的病灶至关重要。然而，计算机辅助诊断（CAD）系统在自动化跨检查病灶对应方面仍面临挑战，限制了其有效性。

Method: 提出了MammoTracker，一个采用粗到细策略的掩膜引导病灶追踪框架，包含全局搜索、局部搜索和分数优化三个核心模块。为支持大规模训练和评估，构建了一个包含730个病例（逾20000对病灶）的新型大型时间序列乳腺X线病灶数据集。

Result: 实验结果表明，MammoTracker实现了0.455的平均重叠度和0.509的准确率，比基线模型提高了8%。

Conclusion: MammoTracker显著提升了CAD系统进行病灶进展分析的能力，证明了其在乳腺X线照片病灶追踪领域的巨大潜力。

Abstract: Accurate lesion tracking in temporal mammograms is essential for monitoring
breast cancer progression and facilitating early diagnosis. However, automated
lesion correspondence across exams remains a challenges in computer-aided
diagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker,
a mask-guided lesion tracking framework that automates lesion localization
across consecutively exams. Our approach follows a coarse-to-fine strategy
incorporating three key modules: global search, local search, and score
refinement. To support large-scale training and evaluation, we introduce a new
dataset with curated prior-exam annotations for 730 mass and calcification
cases from the public EMBED mammogram dataset, yielding over 20000 lesion
pairs, making it the largest known resource for temporal lesion tracking in
mammograms. Experimental results demonstrate that MammoTracker achieves 0.455
average overlap and 0.509 accuracy, surpassing baseline models by 8%,
highlighting its potential to enhance CAD-based lesion progression analysis.
Our dataset will be available at
https://gitlab.oit.duke.edu/railabs/LoGroup/mammotracker.

</details>


### [67] [Populate-A-Scene: Affordance-Aware Human Video Generation](https://arxiv.org/abs/2507.00334)
*Mengyi Shan,Zecheng He,Haoyu Ma,Felix Juefei-Xu,Peizhao Zhang,Tingbo Hou,Ching-Yao Chuang*

Main category: cs.CV

TL;DR: 将文本到视频模型微调为交互式世界模拟器，使其能够从单张场景图像中隐式推断并预测人类与环境的交互。


<details>
  <summary>Details</summary>
Motivation: 探索文本到视频生成模型是否具备可供性感知潜力，并能否将其重新利用为交互式世界模拟器，以预测人类与环境的互动。

Method: 通过微调文本到视频模型，使其在给定场景图像和人类动作提示时，能够将人物连贯地插入场景，并确保行为、外观、协调性和场景可供性。核心方法是从单个场景图像中隐式推断人类的可供性，无需显式边界框或身体姿态。通过分析交叉注意力热图来揭示模型固有的可供性感知能力。

Result: 模型成功将人物插入场景并确保行为与场景可供性的一致性。深入研究交叉注意力热图表明，无需标记的可供性数据集，预训练的视频模型也能展现出其内在的可供性感知能力。

Conclusion: 文本到视频生成模型可以被重新定位为交互式世界模拟器，通过隐式推断和预测人类-环境交互中的可供性，从而实现更智能的视频生成。

Abstract: Can a video generation model be repurposed as an interactive world simulator?
We explore the affordance perception potential of text-to-video models by
teaching them to predict human-environment interaction. Given a scene image and
a prompt describing human actions, we fine-tune the model to insert a person
into the scene, while ensuring coherent behavior, appearance, harmonization,
and scene affordance. Unlike prior work, we infer human affordance for video
generation (i.e., where to insert a person and how they should behave) from a
single scene image, without explicit conditions like bounding boxes or body
poses. An in-depth study of cross-attention heatmaps demonstrates that we can
uncover the inherent affordance perception of a pre-trained video model without
labeled affordance datasets.

</details>


### [68] [Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video](https://arxiv.org/abs/2507.00339)
*Alexander Moore,Amar Saini,Kylie Cancilla,Doug Poland,Carmen Carrano*

Main category: cs.CV

TL;DR: 本文提出了MOVi-MC-AC数据集，这是一个大规模、多摄像头、非模态分割和内容补全数据集，首次提供了真实非模态内容，并支持多视角下对象的跟踪和识别，以解决现有数据集中多摄像头上下文信息的缺失。


<details>
  <summary>Details</summary>
Motivation: 非模态分割和内容补全任务需要估计被遮挡的对象信息，但现有数据集缺乏多摄像头视角下的对象上下文信息。此外，非模态内容生成任务依赖的伪标签方法存在局限性，未能考虑自然遮挡，因此需要一个提供真实非模态内容的新数据集。

Method: 通过模拟包含通用家居物品的杂乱场景，创建了多摄像头视频数据集MOVi-MC-AC。该数据集在帧和多个摄像头之间提供了对象检测和分割的一致性ID，并首次提供了真实的非模态内容标签，规避了传统“剪切-粘贴”生成伪标签的不足，确保了对自然遮挡的考虑。

Result: MOVi-MC-AC是迄今为止最大的非模态分割数据集，包含了约580万个对象实例的标签。它也是首个提供真实非模态内容的数据集。该数据集通过在多摄像头设置中提供一致的对象ID，为合成视频引入了新的复杂性，填补了现有数据稀缺的空白。

Conclusion: MOVi-MC-AC数据集的推出，通过提供多摄像头上下文和真实的非模态内容，极大地促进了计算机视觉领域在复杂场景下对象检测、跟踪和分割（特别是非模态任务）的研究与发展，解决了现有数据不足的关键问题。

Abstract: Amodal segmentation and amodal content completion require using object priors
to estimate occluded masks and features of objects in complex scenes. Until
now, no data has provided an additional dimension for object context: the
possibility of multiple cameras sharing a view of a scene. We introduce
MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the
largest amodal segmentation and first amodal content dataset to date. Cluttered
scenes of generic household objects are simulated in multi-camera video.
MOVi-MC-AC contributes to the growing literature of object detection, tracking,
and segmentation by including two new contributions to the deep learning for
computer vision world. Multiple Camera (MC) settings where objects can be
identified and tracked between various unique camera perspectives are rare in
both synthetic and real-world video. We introduce a new complexity to synthetic
video by providing consistent object ids for detections and segmentations
between both frames and multiple cameras each with unique features and motion
patterns on a single scene. Amodal Content (AC) is a reconstructive task in
which models predict the appearance of target objects through occlusions. In
the amodal segmentation literature, some datasets have been released with
amodal detection, tracking, and segmentation labels. While other methods rely
on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do
not account for natural occlusions present in the modal masks. MOVi-MC-AC
provides labels for ~5.8 million object instances, setting a new maximum in the
amodal dataset literature, along with being the first to provide ground-truth
amodal content. The full dataset is available at
https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,

</details>


### [69] [CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation](https://arxiv.org/abs/2507.00356)
*Zhiwei Yi,Xin Cheng,Jingyu Ma,Ruifei Zhu,Junwei Tian,Yuanxiu Zhou,Xinge Zhao,Hongzhe Li*

Main category: cs.CV

TL;DR: 本研究提出了CGEarthEye，一个专为吉林一号卫星超高分辨率图像设计的遥感视觉基础模型（RSVFM），并在首个1500万级多时相自监督学习数据集JLSSD上进行预训练，在多项遥感任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习推动了遥感智能解译，但超高分辨率光学遥感图像获取渠道有限，限制了高分辨率遥感视觉基础模型的发展。吉林一号星座拥有丰富的亚米级图像资源，为构建此类模型提供了机会。

Method: 本研究提出了CGEarthEye框架，包含五种不同参数规模的骨干网络（总计21亿参数），专为吉林一号卫星特性设计。为增强模型表征能力，开发了JLSSD数据集，这是一个通过多级表征聚类和采样策略构建的1500万规模、全球覆盖、单年季度采样的多时相自监督学习数据集。预训练集成了季节对比、增强对比和掩码块令牌对比策略。

Result: CGEarthEye在涵盖四种典型遥感任务的10个基准数据集上均实现了最先进（SOTA）的性能。进一步分析显示，CGEarthEye在特征可视化、模型收敛、参数效率和实际制图应用方面均表现出卓越的特性。

Conclusion: CGEarthEye展现出卓越的表征能力，有望促进吉林一号数据在传统地球观测应用中更广泛和高效的应用。

Abstract: Deep learning methods have significantly advanced the development of
intelligent rinterpretation in remote sensing (RS), with foundational model
research based on large-scale pre-training paradigms rapidly reshaping various
domains of Earth Observation (EO). However, compared to the open accessibility
and high spatiotemporal coverage of medium-resolution data, the limited
acquisition channels for ultra-high-resolution optical RS imagery have
constrained the progress of high-resolution remote sensing vision foundation
models (RSVFM). As the world's largest sub-meter-level commercial RS satellite
constellation, the Jilin-1 constellation possesses abundant sub-meter-level
image resources. This study proposes CGEarthEye, a RSVFM framework specifically
designed for Jilin-1 satellite characteristics, comprising five backbones with
different parameter scales with totaling 2.1 billion parameters. To enhance the
representational capacity of the foundation model, we developed JLSSD, the
first 15-million-scale multi-temporal self-supervised learning (SSL) dataset
featuring global coverage with quarterly temporal sampling within a single
year, constructed through multi-level representation clustering and sampling
strategies. The framework integrates seasonal contrast, augmentation-based
contrast, and masked patch token contrastive strategies for pre-training.
Comprehensive evaluations across 10 benchmark datasets covering four typical RS
tasks demonstrate that the CGEarthEye consistently achieves state-of-the-art
(SOTA) performance. Further analysis reveals CGEarthEye's superior
characteristics in feature visualization, model convergence, parameter
efficiency, and practical mapping applications. This study anticipates that the
exceptional representation capabilities of CGEarthEye will facilitate broader
and more efficient applications of Jilin-1 data in traditional EO application.

</details>


### [70] [GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control](https://arxiv.org/abs/2507.00363)
*Xingjun Wang,Lianlei Shan*

Main category: cs.CV

TL;DR: 该论文提出一种方法来增强3D Gaussian Splatting (3DGS)，通过改进初始化、优化和密度控制，实现高保真实时渲染。


<details>
  <summary>Details</summary>
Motivation: 3DGS在初始化、将非结构化高斯分布优化为有序表面以及自适应密度控制方面存在挑战，这些问题限制了其渲染质量和收敛速度。

Method: 1. 采用几何引导的初始化方法来预测高斯参数，确保精确放置并加速收敛。2. 引入表面对齐优化策略，以细化高斯位置，提高几何精度并与场景表面法线对齐。3. 提出动态自适应密度控制机制，根据区域复杂性调整高斯密度，以提升视觉保真度。

Result: 所提出的方法实现了高保真实时渲染，显著提升了视觉质量，即使在复杂场景中也能表现出色。与现有最先进方法相比，该方法展现出相当或更优的渲染效果。

Conclusion: 通过在初始化、优化和密度控制方面的创新，该方法有效解决了3DGS的现有问题，能够在实时性下提供卓越的高保真图像渲染效果。

Abstract: We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with limited adaptive density control
mechanism proposed so far. Our first key contribution is a geometry-guided
initialization to predict Gaussian parameters, ensuring precise placement and
faster convergence. We then introduce a surface-aligned optimization strategy
to refine Gaussian placement, improving geometric accuracy and aligning with
the surface normals of the scene. Finally, we present a dynamic adaptive
density control mechanism that adjusts Gaussian density based on regional
complexity, for visual fidelity. These innovations enable our method to achieve
high-fidelity real-time rendering and significant improvements in visual
quality, even in complex scenes. Our method demonstrates comparable or superior
results to state-of-the-art methods, rendering high-fidelity images in real
time.

</details>


### [71] [An Improved U-Net Model for Offline handwriting signature denoising](https://arxiv.org/abs/2507.00365)
*Wanghui Xiao*

Main category: cs.CV

TL;DR: 本研究提出了一种基于改进U-net结构的签名手写去噪模型，旨在解决手写签名样本中噪声干扰问题，提高签名识别系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 手写签名在鉴定中常因样本中混杂大量干扰信息而给识别工作带来严峻挑战，因此需要一种有效的方法来去除噪声，提高图像质量。

Method: 本研究提出了一种基于改进U-net结构的签名手写去噪模型，并通过引入离散小波变换（DWT）和主成分分析（PCA）变换，增强了模型抑制噪声的能力。

Result: 实验结果表明，该模型在去噪效果上显著优于传统方法，能有效提高签名图像的清晰度和可读性。

Conclusion: 该模型为签名分析和识别提供了更可靠的技术支持，有望提高签名识别系统的准确性和鲁棒性。

Abstract: Handwriting signatures, as an important means of identity recognition, are
widely used in multiple fields such as financial transactions, commercial
contracts and personal affairs due to their legal effect and uniqueness. In
forensic science appraisals, the analysis of offline handwriting signatures
requires the appraiser to provide a certain number of signature samples, which
are usually derived from various historical contracts or archival materials.
However, the provided handwriting samples are often mixed with a large amount
of interfering information, which brings severe challenges to handwriting
identification work. This study proposes a signature handwriting denoising
model based on the improved U-net structure, aiming to enhance the robustness
of the signature recognition system. By introducing discrete wavelet transform
and PCA transform, the model's ability to suppress noise has been enhanced. The
experimental results show that this modelis significantly superior to the
traditional methods in denoising effect, can effectively improve the clarity
and readability of the signed images, and provide more reliable technical
support for signature analysis and recognition.

</details>


### [72] [Out-of-Distribution Detection with Adaptive Top-K Logits Integration](https://arxiv.org/abs/2507.00368)
*Hikaru Shijo,Yutaka Yoshihama,Kenichi Yadani,Norifumi Murata*

Main category: cs.CV

TL;DR: 提出一种名为ATLI的新方法，通过自适应整合模型的前k个逻辑值，显著提升了神经网络的域外数据（OOD）检测性能。


<details>
  <summary>Details</summary>
Motivation: 神经网络对域外（OOD）样本的预测常表现出过高置信度，因此检测OOD数据对提升机器学习安全性至关重要。

Method: 发现除最大逻辑值外，其他逻辑值对OOD检测亦有帮助。基于此，提出ATLI（Adaptive Top-k Logits Integration）方法，该方法自适应地确定对每个模型有效的top-k逻辑值，并将最大逻辑值与其他top-k逻辑值进行整合。

Result: 在ImageNet-1K基准测试中，所提出的ATLI方法相较于MaxLogit方法降低了6.73%的假阳性率（FPR95），并且比其他现有最佳方法额外降低了2.67%的FPR95。

Conclusion: ATLI方法通过有效整合多个逻辑值，显著提高了OOD检测的准确性，从而有助于提升机器学习系统的安全性。

Abstract: Neural networks often make overconfident predictions from out-of-distribution
(OOD) samples. Detection of OOD data is therefore crucial to improve the safety
of machine learning. The simplest and most powerful method for OOD detection is
MaxLogit, which uses the model's maximum logit to provide an OOD score. We have
discovered that, in addition to the maximum logit, some other logits are also
useful for OOD detection. Based on this finding, we propose a new method called
ATLI (Adaptive Top-k Logits Integration), which adaptively determines effective
top-k logits that are specific to each model and combines the maximum logit
with the other top-k logits. In this study we evaluate our proposed method
using ImageNet-1K benchmark. Extensive experiments showed our proposed method
to reduce the false positive rate (FPR95) by 6.73% compared to the MaxLogit
approach, and decreased FPR95 by an additional 2.67% compared to other
state-of-the-art methods.

</details>


### [73] [PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching](https://arxiv.org/abs/2507.00371)
*Xin Yang,Ruiming Du,Hanyang Huang,Jiayang Xie,Pengyao Xie,Leisen Fang,Ziyue Guo,Nanjun Jiang,Yu Jiang,Haiyan Cen*

Main category: cs.CV

TL;DR: 提出一种名为PlantSegNeRF的新方法，能从多视角RGB图像序列直接生成高精度植物器官实例点云，并在语义和实例分割任务中显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有植物点云器官分割技术在分辨率、分割精度和跨物种泛化能力方面存在局限性，影响器官级表型性状的高分辨率和精确提取。

Method: 本研究提出PlantSegNeRF方法。该方法首先对多视角图像进行2D实例分割以生成带ID的器官实例掩膜；接着利用实例匹配模块匹配和细化来自不同视角的同一植物器官的实例ID；然后开发实例NeRF来渲染包含颜色、密度、语义和实例信息的隐式场景；最后，基于体密度将隐式场景转换为高精度的植物实例点云。

Result: 在点云语义分割中，PlantSegNeRF在结构复杂数据集上表现优于常用方法，精度、召回率、F1-分数和IoU平均提升16.1%、18.3%、17.8%和24.2%。在植物点云实例分割任务中，其在mPrec、mRec、mCov和mWCov方面平均提升11.7%、38.2%、32.2%和25.3%。

Conclusion: 本研究扩展了器官级植物表型分析的能力，并提供了一种高通量的方法，为植物科学中大型模型的开发提供高质量的3D数据。

Abstract: Organ segmentation of plant point clouds is a prerequisite for the
high-resolution and accurate extraction of organ-level phenotypic traits.
Although the fast development of deep learning has boosted much research on
segmentation of plant point clouds, the existing techniques for organ
segmentation still face limitations in resolution, segmentation accuracy, and
generalizability across various plant species. In this study, we proposed a
novel approach called plant segmentation neural radiance fields (PlantSegNeRF),
aiming to directly generate high-precision instance point clouds from
multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF
performed 2D instance segmentation on the multi-view images to generate
instance masks for each organ with a corresponding ID. The multi-view instance
IDs corresponding to the same plant organ were then matched and refined using a
specially designed instance matching module. The instance NeRF was developed to
render an implicit scene, containing color, density, semantic and instance
information. The implicit scene was ultimately converted into high-precision
plant instance point clouds based on the volume density. The results proved
that in semantic segmentation of point clouds, PlantSegNeRF outperformed the
commonly used methods, demonstrating an average improvement of 16.1%, 18.3%,
17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the
second-best results on structurally complex datasets. More importantly,
PlantSegNeRF exhibited significant advantages in plant point cloud instance
segmentation tasks. Across all plant datasets, it achieved average improvements
of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.
This study extends the organ-level plant phenotyping and provides a
high-throughput way to supply high-quality 3D data for the development of
large-scale models in plant science.

</details>


### [74] [Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur](https://arxiv.org/abs/2507.00372)
*Xinge Yang,Chuong Nguyen,Wenbin Wang,Kaizhang Kang,Wolfgang Heidrich,Xiaoxing Li*

Main category: cs.CV

TL;DR: 针对大光圈相机景深过浅导致的图像模糊问题，本文提出了一种高效可扩展的数据集合成方法，训练出的网络能在真实高分辨率图像上有效泛化。


<details>
  <summary>Details</summary>
Motivation: 现代相机大光圈导致景深过浅，图像失焦模糊，尤其在智能眼镜等固定焦距相机中，受限于尺寸和功耗，难以加入自动对焦机制。此外，现有深度学习模型因相机特有光学像差和离焦特性，在真实世界中表现不佳，且高质量RGB-D数据集稀缺。

Method: 提出一种高效可扩展的数据集合成方法，该方法不依赖真实数据进行微调，能同时模拟深度依赖的离焦和空间变化的像差，解决了计算复杂度和高质量RGB-D数据集稀缺问题。

Result: 实验结果表明，使用该方法生成的低分辨率合成图像训练的网络，能在多样化场景中有效泛化到高分辨率（12MP）的真实世界图像。

Conclusion: 本文提出的数据集合成方法成功解决了固定焦距相机因景深过浅导致的图像模糊问题以及深度学习模型在真实场景中的泛化挑战，为相关应用提供了有效的数据生成方案。

Abstract: Modern cameras with large apertures often suffer from a shallow depth of
field, resulting in blurry images of objects outside the focal plane. This
limitation is particularly problematic for fixed-focus cameras, such as those
used in smart glasses, where adding autofocus mechanisms is challenging due to
form factor and power constraints. Due to unmatched optical aberrations and
defocus properties unique to each camera system, deep learning models trained
on existing open-source datasets often face domain gaps and do not perform well
in real-world settings. In this paper, we propose an efficient and scalable
dataset synthesis approach that does not rely on fine-tuning with real-world
data. Our method simultaneously models depth-dependent defocus and spatially
varying optical aberrations, addressing both computational complexity and the
scarcity of high-quality RGB-D datasets. Experimental results demonstrate that
a network trained on our low resolution synthetic images generalizes
effectively to high resolution (12MP) real-world images across diverse scenes.

</details>


### [75] [Customizable ROI-Based Deep Image Compression](https://arxiv.org/abs/2507.00373)
*Ian Jin,Fanxin Xia,Feng Ding,Xinfeng Zhang,Meiqin Liu,Yao Zhao,Weisi Lin,Lili Meng*

Main category: cs.CV

TL;DR: 本文提出一种可定制的深度图像感兴趣区域（ROI）压缩范式，允许用户通过文本定义ROI并灵活调整ROI与非ROI之间的重建质量平衡。


<details>
  <summary>Details</summary>
Motivation: 现有ROI图像压缩方案预定义ROI且不可更改，同时缺乏有效机制来平衡ROI与非ROI之间的重建质量，无法满足多样化用户（包括人类和机器任务）对自定义ROI和不同质量权衡的需求。

Method: 本研究提出一种可定制的ROI深度图像压缩范式。首先，开发了文本控制掩码获取（TMA）模块，允许用户通过语义文本自定义ROI。其次，设计了可定制值分配（CVA）机制，通过用户决定的可变程度来遮蔽非ROI，以管理ROI与非ROI之间的重建质量权衡。最后，提出了潜在掩码注意力（LMA）模块，在潜在空间中提取并融合掩码的潜在空间先验和图像的潜在率失真优化（RDO）先验，以优化源图像的潜在表示。

Result: 实验结果表明，所提出的可定制ROI深度图像压缩范式有效解决了ROI定义和掩码获取的定制化需求，以及ROI与非ROI之间重建质量权衡的管理问题。

Conclusion: 该工作成功提出了一种可定制的ROI深度图像压缩范式，能够满足用户对ROI定义和掩码获取的灵活性，并有效管理ROI与非ROI之间的重建质量权衡，从而适应更广泛的用户需求。

Abstract: Region of Interest (ROI)-based image compression optimizes bit allocation by
prioritizing ROI for higher-quality reconstruction. However, as the users
(including human clients and downstream machine tasks) become more diverse,
ROI-based image compression needs to be customizable to support various
preferences. For example, different users may define distinct ROI or require
different quality trade-offs between ROI and non-ROI. Existing ROI-based image
compression schemes predefine the ROI, making it unchangeable, and lack
effective mechanisms to balance reconstruction quality between ROI and non-ROI.
This work proposes a paradigm for customizable ROI-based deep image
compression. First, we develop a Text-controlled Mask Acquisition (TMA) module,
which allows users to easily customize their ROI for compression by just
inputting the corresponding semantic \emph{text}. It makes the encoder
controlled by text. Second, we design a Customizable Value Assign (CVA)
mechanism, which masks the non-ROI with a changeable extent decided by users
instead of a constant one to manage the reconstruction quality trade-off
between ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)
module, where the latent spatial prior of the mask and the latent
Rate-Distortion Optimization (RDO) prior of the image are extracted and fused
in the latent space, and further used to optimize the latent representation of
the source image. Experimental results demonstrate that our proposed
customizable ROI-based deep image compression paradigm effectively addresses
the needs of customization for ROI definition and mask acquisition as well as
the reconstruction quality trade-off management between the ROI and non-ROI.

</details>


### [76] [MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis](https://arxiv.org/abs/2507.00377)
*Jianhao Xie,Ziang Zhang,Zhenyu Weng,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: MedDiff-FT是一种数据高效的医疗图像生成方法，通过微调扩散模型解决医疗图像分割中数据稀缺问题，其生成的合成图像在五项分割任务上将SOTA性能平均提高了1%的Dice分数。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医疗图像分割中的应用受限于高质量训练数据的稀缺性。现有扩散模型虽然能生成合成图像，但其在医疗领域的效果受限于对大规模数据集的依赖以及对更高图像质量的需求。

Method: 本文提出了MedDiff-FT，一种可控的医疗图像生成方法。它通过微调一个扩散基础模型，以数据高效的方式生成具有结构依赖性和领域特异性的医疗图像。推理阶段，采用动态自适应引导掩码确保解剖学上的连贯性，并通过轻量级随机掩码生成器注入分层随机性以增强多样性。此外，还采用自动化质量评估协议，利用特征空间度量过滤次优输出，并通过掩码腐蚀精炼保真度。

Result: MedDiff-FT生成的合成图像-掩码对在五个医疗分割数据集上进行评估，使SOTA方法的分割性能平均提高了1%的Dice分数。

Conclusion: MedDiff-FT框架有效平衡了生成质量、多样性和计算效率，为医疗数据增强提供了一个实用的解决方案。

Abstract: Recent advancements in deep learning for medical image segmentation are often
limited by the scarcity of high-quality training data.While diffusion models
provide a potential solution by generating synthetic images, their
effectiveness in medical imaging remains constrained due to their reliance on
large-scale medical datasets and the need for higher image quality. To address
these challenges, we present MedDiff-FT, a controllable medical image
generation method that fine-tunes a diffusion foundation model to produce
medical images with structural dependency and domain specificity in a
data-efficient manner. During inference, a dynamic adaptive guiding mask
enforces spatial constraints to ensure anatomically coherent synthesis, while a
lightweight stochastic mask generator enhances diversity through hierarchical
randomness injection. Additionally, an automated quality assessment protocol
filters suboptimal outputs using feature-space metrics, followed by mask
corrosion to refine fidelity. Evaluated on five medical segmentation
datasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's
segmentation performance by an average of 1% in Dice score. The framework
effectively balances generation quality, diversity, and computational
efficiency, offering a practical solution for medical data augmentation. The
code is available at https://github.com/JianhaoXie1/MedDiff-FT.

</details>


### [77] [Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space](https://arxiv.org/abs/2507.00392)
*Yingping Liang,Yutao Hu,Wenqi Shao,Ying Fu*

Main category: cs.CV

TL;DR: 提出L2M（Lift to Match）两阶段框架，通过将2D图像提升到3D空间并利用大规模单视图数据，学习3D感知特征以实现鲁棒且泛化能力强的特征匹配。


<details>
  <summary>Details</summary>
Motivation: 现有特征匹配方法过度依赖稀缺、干净的多视图图像数据，导致泛化能力受限；传统特征编码器仅在单视图2D图像上训练，难以捕获3D感知对应关系。

Method: 提出L2M两阶段框架。第一阶段，结合多视图图像合成和3D特征高斯表示，学习一个注入3D几何知识的3D感知特征编码器。第二阶段，利用新视图渲染策略和大规模单视图合成数据生成，训练特征解码器以实现鲁棒特征匹配。

Result: 实验证明，所提出的方法在零样本评估基准上实现了卓越的泛化能力，凸显了该框架在鲁棒特征匹配方面的有效性。

Conclusion: L2M框架通过注入3D几何知识并利用大规模单视图数据，有效解决了传统特征匹配方法的泛化性问题，实现了鲁棒且泛化能力强的特征匹配。

Abstract: Feature matching plays a fundamental role in many computer vision tasks, yet
existing methods heavily rely on scarce and clean multi-view image collections,
which constrains their generalization to diverse and challenging scenarios.
Moreover, conventional feature encoders are typically trained on single-view 2D
images, limiting their capacity to capture 3D-aware correspondences. In this
paper, we propose a novel two-stage framework that lifts 2D images to 3D space,
named as \textbf{Lift to Match (L2M)}, taking full advantage of large-scale and
diverse single-view images. To be specific, in the first stage, we learn a
3D-aware feature encoder using a combination of multi-view image synthesis and
3D feature Gaussian representation, which injects 3D geometry knowledge into
the encoder. In the second stage, a novel-view rendering strategy, combined
with large-scale synthetic data generation from single-view images, is employed
to learn a feature decoder for robust feature matching, thus achieving
generalization across diverse domains. Extensive experiments demonstrate that
our method achieves superior generalization across zero-shot evaluation
benchmarks, highlighting the effectiveness of the proposed framework for robust
feature matching.

</details>


### [78] [Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains](https://arxiv.org/abs/2507.00401)
*Xin Xu,Eibe Frank,Geoffrey Holmes*

Main category: cs.CV

TL;DR: 本文提出了一种名为“MIV-head”的新方法，用于在骨干网络无法微调的限制下进行跨域少样本学习，在保持竞争性准确率的同时，显著降低了适应成本。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，骨干网络（特征提取器）的微调变得不可能或不可行，这导致了在跨域少样本学习中处理冻结骨干网络产生的低质量静态嵌入的挑战。

Method: 将少样本分类问题重新表述为一系列多实例验证（MIV）任务。在此基础上，引入了新颖的“MIV-head”方法，该方法独立于任何预训练骨干网络，计算高效。其核心组件在目标域的少样本数据上进行训练，并在“元测试”阶段使用，无需微调骨干网络。

Result: MIV-head在不微调骨干网络的情况下，在目标域测试数据上取得了强大的性能。与最先进的“适配器”方法相比，它在准确性上具有高度竞争力，同时适应成本显著降低。同时，传统“分类头”方法的准确性远低于MIV-head。消融研究也证明了其核心组件的有效性。

Conclusion: MIV-head为骨干网络微调受限的跨域少样本学习提供了一个高效且性能卓越的解决方案，其表现优于现有分类头方法，并能以更低成本媲美先进的适配器方法。

Abstract: We investigate cross-domain few-shot learning under the constraint that
fine-tuning of backbones (i.e., feature extractors) is impossible or infeasible
-- a scenario that is increasingly common in practical use cases. Handling the
low-quality and static embeddings produced by frozen, "black-box" backbones
leads to a problem representation of few-shot classification as a series of
multiple instance verification (MIV) tasks. Inspired by this representation, we
introduce a novel approach to few-shot domain adaptation, named the "MIV-head",
akin to a classification head that is agnostic to any pretrained backbone and
computationally efficient. The core components designed for the MIV-head, when
trained on few-shot data from a target domain, collectively yield strong
performance on test data from that domain. Importantly, it does so without
fine-tuning the backbone, and within the "meta-testing" phase. Experimenting
under various settings and on an extension of the Meta-dataset benchmark for
cross-domain few-shot image classification, using representative off-the-shelf
convolutional neural network and vision transformer backbones pretrained on
ImageNet1K, we show that the MIV-head achieves highly competitive accuracy when
compared to state-of-the-art "adapter" (or partially fine-tuning) methods
applied to the same backbones, while incurring substantially lower adaptation
cost. We also find well-known "classification head" approaches lag far behind
in terms of accuracy. Ablation study empirically justifies the core components
of our approach. We share our code at https://github.com/xxweka/MIV-head.

</details>


### [79] [DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting](https://arxiv.org/abs/2507.00429)
*Jingyi Pan,Dan Xu,Qiong Luo*

Main category: cs.CV

TL;DR: DiGA3D是一种新颖通用的3D修复管线，它利用扩散模型解决多视角3D修复中出现的鲁棒性、外观和几何一致性问题。


<details>
  <summary>Details</summary>
Motivation: 当前统一的3D修复框架面临挑战：1) 单一参考视角修复方法在远离参考视角时缺乏鲁棒性；2) 独立修复多视角图像时2D扩散先验导致外观不一致；3) 修复区域几何变化大时几何不一致限制了性能。

Method: DiGA3D采用扩散模型以粗到细的方式传播一致的外观和几何。具体方法包括：1) 鲁棒的多参考视角选择策略以减少传播误差；2) 注意力特征传播（AFP）机制，通过扩散模型从选定参考视角向其他视角传播注意力特征以保持外观一致性；3) 引入纹理-几何分数蒸馏采样（TG-SDS）损失以进一步提高修复3D场景的几何一致性。

Result: 在多项3D修复任务上的大量实验证明了DiGA3D方法的有效性。

Conclusion: DiGA3D成功应对了3D修复领域的关键挑战，通过确保跨视图的一致外观和几何，提供了一个通用且有效的解决方案。

Abstract: Developing a unified pipeline that enables users to remove, re-texture, or
replace objects in a versatile manner is crucial for text-guided 3D inpainting.
However, there are still challenges in performing multiple 3D inpainting tasks
within a unified framework: 1) Single reference inpainting methods lack
robustness when dealing with views that are far from the reference view. 2)
Appearance inconsistency arises when independently inpainting multi-view images
with 2D diffusion priors; 3) Geometry inconsistency limits performance when
there are significant geometric changes in the inpainting regions. To tackle
these challenges, we introduce DiGA3D, a novel and versatile 3D inpainting
pipeline that leverages diffusion models to propagate consistent appearance and
geometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy
for selecting multiple reference views to reduce errors during propagation.
Next, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that
propagates attention features from the selected reference views to other views
via diffusion models to maintain appearance consistency. Furthermore, DiGA3D
introduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to
further improve the geometric consistency of inpainted 3D scenes. Extensive
experiments on multiple 3D inpainting tasks demonstrate the effectiveness of
our method. The project page is available at https://rorisis.github.io/DiGA3D/.

</details>


### [80] [MFH: Marrying Frequency Domain with Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2507.00430)
*Huanxin Yang,Qiwen Wang*

Main category: cs.CV

TL;DR: 该论文提出一种将频域分析（基于离散余弦变换DCT）引入手写数学表达式识别（HMER）的新方法MFH，有效提升了识别性能，尤其在结构分析方面。


<details>
  <summary>Details</summary>
Motivation: 手写数学表达式识别（HMER）面临公式结构复杂和字符布局多变的挑战。

Method: 将频域分析引入HMER，提出MFH方法，并利用离散余弦变换（DCT）来辅助数学公式的结构分析。

Result: 该网络在多种基线模型上均表现出一致的性能提升。实验结果显示，MFH-CoMER在CROHME 2014/2016/2019测试集上分别取得了61.66%/62.07%/63.72%的准确率。

Conclusion: 频域信息能有效辅助手写数学表达式的结构分析，显著提升HMER的识别性能和准确率。

Abstract: Handwritten mathematical expression recognition (HMER) suffers from complex
formula structures and character layouts in sequence prediction. In this paper,
we incorporate frequency domain analysis into HMER and propose a method that
marries frequency domain with HMER (MFH), leveraging the discrete cosine
transform (DCT). We emphasize the structural analysis assistance of frequency
information for recognizing mathematical formulas. When implemented on various
baseline models, our network exhibits a consistent performance enhancement,
demonstrating the efficacy of frequency domain information. Experiments show
that our MFH-CoMER achieves noteworthy accuracyrates of 61.66%/62.07%/63.72% on
the CROHME 2014/2016/2019 test sets. The source code is available at
https://github.com/Hryxyhe/MFH.

</details>


### [81] [Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration](https://arxiv.org/abs/2507.00447)
*Xin Luo,Menglin Zhang,Yunwei Lan,Tianyu Zhang,Rui Li,Chang Liu,Dong Liu*

Main category: cs.CV

TL;DR: 本文提出Latent-PMRF，在变分自编码器（VAE）的潜在空间重构PMRF，以更好地与人类感知对齐，从而在盲人脸修复中实现更好的感知-失真权衡和显著的收敛效率提升。


<details>
  <summary>Details</summary>
Motivation: 人脸修复算法需平衡感知质量与保真度（PD-tradeoff）。现有方法PMRF在像素空间建模，限制了其与人类感知的对齐，而人类感知在区分图像分布中至关重要。

Method: 提出Latent-PMRF，在VAE的潜在空间中重新定义PMRF，将源分布设定为最小失真估计的潜在表示，从而将最小失真限制在VAE的重建误差内。同时，设计并提出了一种性能优越的新型VAE。

Result: 所提出的VAE在重建和修复方面显著优于现有VAE。Latent-PMRF在盲人脸修复方面表现出优越性，提供了比现有方法更好的PD-tradeoff，并在FID方面实现5.79倍的速度提升，展现了卓越的收敛效率。

Conclusion: Latent-PMRF通过在潜在空间建模并结合优化设计的VAE，成功解决了人脸修复中的PD-tradeoff难题，不仅提高了修复质量，还大幅提升了收敛速度，为该领域提供了更优的解决方案。

Abstract: The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face
restoration algorithms must balance perceptual quality and fidelity. To achieve
minimal distortion while maintaining perfect perceptual quality, Posterior-Mean
Rectified Flow (PMRF) proposes a flow based approach where source distribution
is minimum distortion estimations. Although PMRF is shown to be effective, its
pixel-space modeling approach limits its ability to align with human
perception, where human perception is defined as how humans distinguish between
two image distributions. In this work, we propose Latent-PMRF, which
reformulates PMRF in the latent space of a variational autoencoder (VAE),
facilitating better alignment with human perception during optimization. By
defining the source distribution on latent representations of minimum
distortion estimation, we bound the minimum distortion by the VAE's
reconstruction error. Moreover, we reveal the design of VAE is crucial, and our
proposed VAE significantly outperforms existing VAEs in both reconstruction and
restoration. Extensive experiments on blind face restoration demonstrate the
superiority of Latent-PMRF, offering an improved PD-tradeoff compared to
existing methods, along with remarkable convergence efficiency, achieving a
5.79X speedup over PMRF in terms of FID. Our code will be available as
open-source.

</details>


### [82] [ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales](https://arxiv.org/abs/2507.00454)
*Yihao Zhen,Qiang Wang,Yu Qiao,Liangqiong Qu,Huijie Fan*

Main category: cs.CV

TL;DR: 本文提出ATSTrack，通过对齐视觉与语言输入间的时空尺度差异，解决视觉-语言跟踪（VLT）中的错位问题，并在实验中取得了与现有方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言跟踪（VLT）的主要挑战是目标移动导致的视觉与语言描述错位。现有方法虽尝试特征修改，但未能充分解决视觉和语言输入在时空尺度上的固有差异，这限制了它们的性能。

Method: 提出ATSTrack。具体方法包括：1) 将语言描述分解为短语，并基于其与视觉输入的时空对应关系，进行细粒度的特征修改。2) 引入一个视觉-语言token，该token包含前一帧的修改后语言信息，用于引导模型提取更相关的视觉特征，以减少空间尺度差异的影响。

Result: 实验结果表明，所提出的ATSTrack在性能上与现有方法相当。

Conclusion: ATSTrack通过有效对齐视觉与语言输入的时空尺度，解决了VLT中的关键挑战，并在性能上达到了现有先进方法的水平，证明了其方法的有效性。

Abstract: A main challenge of Visual-Language Tracking (VLT) is the misalignment
between visual inputs and language descriptions caused by target movement.
Previous trackers have explored many effective feature modification methods to
preserve more aligned features. However, an important yet unexplored factor
ultimately hinders their capability, which is the inherent differences in the
temporal and spatial scale of information between visual and language inputs.
To address this issue, we propose a novel visual-language tracker that enhances
the effect of feature modification by \textbf{A}ligning \textbf{T}emporal and
\textbf{S}patial scale of different input components, named as
\textbf{ATSTrack}. Specifically, we decompose each language description into
phrases with different attributes based on their temporal and spatial
correspondence with visual inputs, and modify their features in a fine-grained
manner. Moreover, we introduce a Visual-Language token that comprises modified
linguistic information from the previous frame to guide the model to extract
visual features that are more relevant to language description, thereby
reducing the impact caused by the differences in spatial scale. Experimental
results show that our proposed ATSTrack achieves performance comparable to
existing methods. Our code will be released.

</details>


### [83] [Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation](https://arxiv.org/abs/2507.00462)
*Jizhou Han,Chenhao Ding,SongLin Dong,Yuhang He,Xinyuan Gao,Yihong Gong*

Main category: cs.CV

TL;DR: 针对CLIP等视觉-语言模型在分布偏移下的测试时自适应问题，本研究提出MS-TTA，一种无需训练的方法，利用kNN Mean-Shift增强特征表示，从而实现更稳定和鲁棒的自适应，并在实验中超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（如CLIP）虽泛化能力强，但在测试时面临分布偏移的挑战。现有无需训练的测试时自适应（TTA）方法局限于CLIP原始特征空间，且仅依赖高置信度样本，忽略了低置信度样本的潜在价值。

Method: 本研究提出MS-TTA，一个无需训练的TTA方案。它通过单步k近邻（kNN）Mean-Shift技术，在CLIP原始特征空间之外增强特征表示。MS-TTA能精炼所有测试样本，以提升特征紧凑性和类别可分性，从而实现更稳定的自适应。此外，通过缓存精炼后的嵌入，还能提供Mean Shift增强的logits以进一步优化推理。

Result: 在OOD（分布外）和跨数据集基准测试上的广泛评估显示，MS-TTA持续优于现有最先进的无需训练TTA方法。

Conclusion: MS-TTA在无需额外训练的情况下，实现了鲁棒的测试时自适应，有效提升了视觉-语言模型在分布偏移场景下的性能，证明了通过特征空间外扩展和全样本精炼的有效性。

Abstract: Visual-language models (VLMs) like CLIP exhibit strong generalization but
struggle with distribution shifts at test time. Existing training-free
test-time adaptation (TTA) methods operate strictly within CLIP's original
feature space, relying on high-confidence samples while overlooking the
potential of low-confidence ones. We propose MS-TTA, a training-free approach
that enhances feature representations beyond CLIP's space using a single-step
k-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA
improves feature compactness and class separability, leading to more stable
adaptation. Additionally, a cache of refined embeddings further enhances
inference by providing Mean Shift enhanced logits. Extensive evaluations on OOD
and cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms
state-of-the-art training-free TTA methods, achieving robust adaptation without
requiring additional training.

</details>


### [84] [Bisecle: Binding and Separation in Continual Learning for Video Language Understanding](https://arxiv.org/abs/2507.00469)
*Yue Tan,Xiaoqian Hu,Hao Xue,Celso De Melo,Flora D. Salim*

Main category: cs.CV

TL;DR: 本文提出受人脑海马体启发的方法Bisecle，以解决视觉语言模型在视频连续学习中面临的灾难性遗忘和更新冲突问题，实现高效且鲁棒的视频理解。


<details>
  <summary>Details</summary>
Motivation: 现有前沿视觉语言模型在视频理解中表现出色，但面对连续变化的真实世界视频流时，需要持续适应新的数据分布和场景。对大型模型进行微调成本高昂，而参数高效的更新方式会导致灾难性遗忘和更新冲突，这是当前持续学习框架面临的关键挑战。

Method: 受人脑海马体快速绑定和模式分离机制的启发，提出Bisecle模型。该模型包含一个多向监督模块，用于捕获更多跨模态关系；以及一个对比提示学习方案，用于隔离任务特定知识以实现高效的记忆存储。绑定和分离过程进一步增强了视觉语言模型保留复杂经验的能力。

Result: 对Bisecle进行了全面评估，结果表明它能够有效缓解遗忘，并在多个VideoQA基准测试中增强跨任务泛化能力。

Conclusion: Bisecle模型通过整合类海马体的机制，实现了视觉语言模型在视频理解任务中鲁棒且高效的持续学习，有效缓解了遗忘并提升了泛化能力。

Abstract: Frontier vision-language models (VLMs) have made remarkable improvements in
video understanding tasks. However, real-world videos typically exist as
continuously evolving data streams (e.g., dynamic scenes captured by wearable
glasses), necessitating models to continually adapt to shifting data
distributions and novel scenarios. Considering the prohibitive computational
costs of fine-tuning models on new tasks, usually, a small subset of parameters
is updated while the bulk of the model remains frozen. This poses new
challenges to existing continual learning frameworks in the context of large
multimodal foundation models, i.e., catastrophic forgetting and update
conflict. While the foundation models struggle with parameter-efficient
continual learning, the hippocampus in the human brain has evolved highly
efficient mechanisms for memory formation and consolidation. Inspired by the
rapid Binding and pattern separation mechanisms in the hippocampus, in this
work, we propose Bisecle for video-language continual learning, where a
multi-directional supervision module is used to capture more cross-modal
relationships and a contrastive prompt learning scheme is designed to isolate
task-specific knowledge to facilitate efficient memory storage. Binding and
separation processes further strengthen the ability of VLMs to retain complex
experiences, enabling robust and efficient continual learning in video
understanding tasks. We perform a thorough evaluation of the proposed Bisecle,
demonstrating its ability to mitigate forgetting and enhance cross-task
generalization on several VideoQA benchmarks.

</details>


### [85] [ARIG: Autoregressive Interactive Head Generation for Real-time Conversations](https://arxiv.org/abs/2507.00472)
*Ying Guo,Xi Liu,Cheng Zhen,Pengfei Yan,Xiaoming Wei*

Main category: cs.CV

TL;DR: 提出ARIG框架，通过自回归(AR)和扩散过程，结合交互行为与对话状态理解，实现实时、逼真的交互式虚拟代理头部运动生成。


<details>
  <summary>Details</summary>
Motivation: 面对面交流是常见的人类活动，但现有交互式头部生成方法（如分段生成或显式切换）在未来信号获取、上下文行为理解和切换平滑性方面存在局限，难以实现实时性和真实感。

Method: 本文提出一个基于自回归(AR)的逐帧生成框架ARIG，以实现实时生成和更好的交互真实感。为实现实时性，模型将运动预测建模为非矢量量化的AR过程，并使用扩散过程表示运动分布，从而在连续空间中获得更准确的预测。为提高交互真实感，模型强调交互行为理解(IBU)和详细对话状态理解(CSU)：IBU基于双轨双模态信号，通过双向整合学习总结短程行为并进行长程上下文理解；CSU利用语音活动信号和IBU上下文特征理解实际对话中存在的各种状态（如打断、反馈、停顿等）。这些理解结果作为最终渐进式运动预测的条件。

Result: 广泛的实验验证了所提出模型的有效性。

Conclusion: 本文提出的ARIG框架，通过创新的自回归和扩散过程，结合细致的交互行为与对话状态理解，有效解决了现有交互式头部生成方法在实时性和真实感方面的挑战，实现了更自然的虚拟代理生成。

Abstract: Face-to-face communication, as a common human activity, motivates the
research on interactive head generation. A virtual agent can generate motion
responses with both listening and speaking capabilities based on the audio or
motion signals of the other user and itself. However, previous clip-wise
generation paradigm or explicit listener/speaker generator-switching methods
have limitations in future signal acquisition, contextual behavioral
understanding, and switching smoothness, making it challenging to be real-time
and realistic. In this paper, we propose an autoregressive (AR) based
frame-wise framework called ARIG to realize the real-time generation with
better interaction realism. To achieve real-time generation, we model motion
prediction as a non-vector-quantized AR process. Unlike discrete codebook-index
prediction, we represent motion distribution using diffusion procedure,
achieving more accurate predictions in continuous space. To improve interaction
realism, we emphasize interactive behavior understanding (IBU) and detailed
conversational state understanding (CSU). In IBU, based on dual-track
dual-modal signals, we summarize short-range behaviors through
bidirectional-integrated learning and perform contextual understanding over
long ranges. In CSU, we use voice activity signals and context features of IBU
to understand the various states (interruption, feedback, pause, etc.) that
exist in actual conversations. These serve as conditions for the final
progressive motion prediction. Extensive experiments have verified the
effectiveness of our model.

</details>


### [86] [ADAptation: Reconstruction-based Unsupervised Active Learning for Breast Ultrasound Diagnosis](https://arxiv.org/abs/2507.00474)
*Yaofei Duan,Yuhao Huang,Xin Yang,Luyi Han,Xinyu Xie,Zhiyuan Zhu,Ping He,Ka-Hou Chan,Ligang Cui,Sio-Kei Im,Dong Ni,Tao Tan*

Main category: cs.CV

TL;DR: 本文提出了一种名为ADAptation的无监督主动学习域适应框架，利用扩散模型处理跨域分布差异，并通过超球面约束对比学习和双重评分机制高效选择样本，以在有限标注预算下提升诊断模型在分布偏移场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习诊断模型常因训练和测试域之间的分布偏移而性能下降。尽管收集并标注足够的目标域数据是理想方案，但受限于时间和资源。现有的主动学习（AL）方法虽能减少标注成本，但难以有效处理跨数据集的分布差异问题。

Method: 本研究提出了一种新颖的无监督主动学习域适应框架——ADAptation。核心方法包括：1) 利用扩散模型将目标域图像转换为源域风格，以实现分布同质化，弥合数据集间的差距；2) 引入超球面约束对比学习网络，实现紧凑的特征聚类；3) 设计双重评分机制，平衡样本的不确定性和代表性，以高效选择信息量大的样本。

Result: 在四个乳腺超声数据集（三个公开、一个内部/多中心）和五种常用深度分类器上进行的广泛实验表明，所提出的ADAptation方法显著优于现有强大的基于主动学习的竞争方法，验证了其在临床域适应方面的有效性和泛化能力。

Conclusion: ADAptation框架通过创新的方法有效应对了深度学习诊断模型在域适应中遇到的分布偏移挑战，在有限的标注预算下，展现出卓越的性能和普适性，对临床应用具有重要意义。

Abstract: Deep learning-based diagnostic models often suffer performance drops due to
distribution shifts between training (source) and test (target) domains.
Collecting and labeling sufficient target domain data for model retraining
represents an optimal solution, yet is limited by time and scarce resources.
Active learning (AL) offers an efficient approach to reduce annotation costs
while maintaining performance, but struggles to handle the challenge posed by
distribution variations across different datasets. In this study, we propose a
novel unsupervised Active learning framework for Domain Adaptation, named
ADAptation, which efficiently selects informative samples from multi-domain
data pools under limited annotation budget. As a fundamental step, our method
first utilizes the distribution homogenization capabilities of diffusion models
to bridge cross-dataset gaps by translating target images into source-domain
style. We then introduce two key innovations: (a) a hypersphere-constrained
contrastive learning network for compact feature clustering, and (b) a
dual-scoring mechanism that quantifies and balances sample uncertainty and
representativeness. Extensive experiments on four breast ultrasound datasets
(three public and one in-house/multi-center) across five common deep
classifiers demonstrate that our method surpasses existing strong AL-based
competitors, validating its effectiveness and generalization for clinical
domain adaptation. The code is available at the anonymized link:
https://github.com/miccai25-966/ADAptation.

</details>


### [87] [Just Noticeable Difference for Large Multimodal Models](https://arxiv.org/abs/2507.00490)
*Zijian Chen,Yuan Tian,Yuze Sun,Wei Sun,Zicheng Zhang,Weisi Lin,Guangtao Zhai,Wenjun Zhang*

Main category: cs.CV

TL;DR: 本文提出LMM-JND概念和VPA-JND数据集，用于量化和揭示大型多模态模型（LMM）在细微视觉差异感知上的显著盲点，表明LMMs视觉性能远低于人类水平，并探讨了其对安全性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LMMs在多种任务和刺激类型下感知边界的系统探索，特别是其对细微视觉差异（JND）的感知能力。LMMs的感知缺陷未被充分研究，可能导致安全隐患和响应效率低下。

Method: 提出了“LMM-JND”这一新概念及其确定流程，旨在系统量化LMM的视觉盲点。构建了一个包含21.5k参考图像和489k刺激（涵盖12种扭曲类型）的大规模数据集VPA-JND，用于LMM-JND研究。同时，深入探究了不同LMM家族及其视觉和语言骨干模型对感知性能的影响。

Result: 研究发现，包括GPT-4o和InternVL2.5系列在内的最先进LMMs在基本的视觉比较查询中表现不佳，视觉性能显著低于人类水平。此外，LMM的视觉和语言骨干模型的设计理念与其视觉敏锐度之间存在显著关联。

Conclusion: LMM-JND是研究LMMs独特且重要的视角。可预测的LMM-JND对于安全问题至关重要。本研究强调了当前LMM在感知细微视觉差异方面的不足，并为未来LMMs的视觉精度改进提供了方向。

Abstract: Just noticeable difference (JND), the minimum change that the human visual
system (HVS) can perceive, has been studied for decades. Although recent work
has extended this line of research into machine vision, there has been a
scarcity of studies systematically exploring its perceptual boundaries across
multiple tasks and stimulus types, particularly in the current era of rapidly
advancing large multimodal models (LMMs), where studying the multifaceted
capabilities of models has become a mainstream focus. Moreover, the perceptual
defects of LMMs are not investigated thoroughly, resulting in potential
security issues and suboptimal response efficiency. In this paper, we take an
initial attempt and demonstrate that there exist significant visual blind spots
in current LMMs. To systemically quantify this characteristic, we propose a new
concept, {\bf LMM-JND}, together with its determination pipeline. Targeting
uncovering the behavior commonalities in HVS-aligned visual perception tasks,
we delve into several LMM families and construct a large-scale dataset, named
VPA-JND, which contains 21.5k reference images with over 489k stimuli across 12
distortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where
state-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle
with basic comparison queries and fall significantly short of human-level
visual performance. We further explore the effects of vision and language
backbones and find a notable correlation between their design philosophy that
may instruct the future refinement of LMMs for their visual acuity. Together,
our research underscores the significance of LMM-JND as a unique perspective
for studying LMMs, and predictable LMM-JND is crucial for security concerns.
This work will be available at https://github.com/zijianchen98/LMM-JND.

</details>


### [88] [Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models](https://arxiv.org/abs/2507.00493)
*Fenil R. Doshi,Thomas Fel,Talia Konkle,George Alvarez*

Main category: cs.CV

TL;DR: 研究提出并评估了一种衡量视觉模型对全局配置形状理解能力的方法——Configural Shape Score (CSS)。结果显示自监督和语言对齐的Transformer模型在此方面表现出色，并揭示其依赖长程交互。研究强调整合局部纹理与全局形状对构建类人视觉系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型过度依赖局部纹理线索，导致特征脆弱且缺乏组合性。此外，当前的形状与纹理偏置研究将两者对立，忽略了模型（和人类）可以同时依赖两种线索的可能性，也掩盖了这两种表征的绝对质量。因此，需要一种评估模型绝对配置能力的方法。

Method: 引入“配置形状分数”（Configural Shape Score, CSS）来度量模型的绝对配置能力。CSS通过识别“对象变位词对”（Object-Anagram pairs）来工作，这些图像对在保留局部纹理的同时，通过排列部件顺序来描绘不同的物体类别。

Result: 1. CSS在86个卷积、Transformer和混合模型上揭示了广泛的配置敏感度，其中全自监督和语言对齐的Transformer（如DINOv2、SigLIP2和EVA-CLIP）表现最佳。
2. 机制探测显示，高CSS网络依赖长程交互：受半径控制的注意力掩码会显著降低性能，并呈现独特的U形整合模式；表示相似性分析表明存在从局部到全局编码的中间深度转换。
3. BagNet对照实验表明并非通过“边界破解”策略实现。
4. CSS还能预测其他依赖形状的评估结果。

Conclusion: 通向真正鲁棒、通用和类人视觉系统的路径，可能不在于在形状和纹理之间做出人为选择，而在于能够无缝整合局部纹理和全局配置形状的架构和学习框架。

Abstract: Humans are able to recognize objects based on both local texture cues and the
configuration of object parts, yet contemporary vision models primarily harvest
local texture cues, yielding brittle, non-compositional features. Work on
shape-vs-texture bias has pitted shape and texture representations in
opposition, measuring shape relative to texture, ignoring the possibility that
models (and humans) can simultaneously rely on both types of cues, and
obscuring the absolute quality of both types of representation. We therefore
recast shape evaluation as a matter of absolute configural competence,
operationalized by the Configural Shape Score (CSS), which (i) measures the
ability to recognize both images in Object-Anagram pairs that preserve local
texture while permuting global part arrangement to depict different object
categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii)
uncovers a broad spectrum of configural sensitivity with fully self-supervised
and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and
EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes
reveal that (iii) high-CSS networks depend on long-range interactions:
radius-controlled attention masks abolish performance showing a distinctive
U-shaped integration profile, and representational-similarity analyses expose a
mid-depth transition from local to global coding. A BagNet control remains at
chance (iv), ruling out "border-hacking" strategies. Finally, (v) we show that
configural shape score also predicts other shape-dependent evals. Overall, we
propose that the path toward truly robust, generalizable, and human-like vision
systems may not lie in forcing an artificial choice between shape and texture,
but rather in architectural and learning frameworks that seamlessly integrate
both local-texture and global configural shape.

</details>


### [89] [Laplace-Mamba: Laplace Frequency Prior-Guided Mamba-CNN Fusion Network for Image Dehazing](https://arxiv.org/abs/2507.00501)
*Yongzhen Wang,Liangliang Chen,Bingwen Hu,Heng Liu,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

TL;DR: 拉普拉斯-Mamba是一种新颖的去雾框架，通过拉普拉斯分解将图像分为低频（由SSM处理）和高频（由CNN处理）分量，有效结合全局和局部特征建模，从而在图像恢复质量和效率上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管空间状态模型（SSMs）在建模长距离依赖和计算效率方面具有优势，但它们在重建局部结构和处理高维数据时表现不佳，导致精细图像特征的恢复效果欠佳。

Method: 本文提出了Laplace-Mamba框架，它整合了拉普拉斯频率先验与混合Mamba-CNN架构。通过拉普拉斯分解将图像解耦为低频（全局纹理）和高频（边缘细节）分量。低频分支采用SSM进行全局上下文建模，高频分支则利用CNN精细化局部结构细节。此外，拉普拉斯变换有助于对低频分量进行信息保留的下采样，显著提升计算效率。

Result: 在多个基准测试中，我们的方法在恢复质量和效率方面均超越了现有最先进方法。

Conclusion: Laplace-Mamba通过独特的频率分解和双通路处理机制，成功克服了SSMs在局部结构重建上的局限性，并在图像去雾任务中实现了卓越的性能和效率。

Abstract: Recent progress in image restoration has underscored Spatial State Models
(SSMs) as powerful tools for modeling long-range dependencies, owing to their
appealing linear complexity and computational efficiency. However, SSM-based
approaches exhibit limitations in reconstructing localized structures and tend
to be less effective when handling high-dimensional data, frequently resulting
in suboptimal recovery of fine image features. To tackle these challenges, we
introduce Laplace-Mamba, a novel framework that integrates Laplace frequency
prior with a hybrid Mamba-CNN architecture for efficient image dehazing.
Leveraging the Laplace decomposition, the image is disentangled into
low-frequency components capturing global texture and high-frequency components
representing edges and fine details. This decomposition enables specialized
processing via dual parallel pathways: the low-frequency branch employs SSMs
for global context modeling, while the high-frequency branch utilizes CNNs to
refine local structural details, effectively addressing diverse haze scenarios.
Notably, the Laplace transformation facilitates information-preserving
downsampling of low-frequency components in accordance with the Nyquist theory,
thereby significantly improving computational efficiency. Extensive evaluations
across multiple benchmarks demonstrate that our method outperforms
state-of-the-art approaches in both restoration quality and efficiency. The
source code and pretrained models are available at
https://github.com/yz-wang/Laplace-Mamba.

</details>


### [90] [ExPaMoE: An Expandable Parallel Mixture of Experts for Continual Test-Time Adaptation](https://arxiv.org/abs/2507.00502)
*JianChao Zhao,Songlin Dong*

Main category: cs.CV

TL;DR: 针对持续测试时适应（CTTA）中现有方法面临的特征纠缠和灾难性遗忘问题，本文提出ExPaMoE框架，通过可扩展的并行专家混合架构和动态专家池扩展，有效应对复杂域偏移。


<details>
  <summary>Details</summary>
Motivation: 现有持续测试时适应（CTTA）方法依赖于跨域共享模型参数，在大规模或非稳态域偏移下容易导致特征纠缠和灾难性遗忘。

Method: 本文提出ExPaMoE，一种基于可扩展并行专家混合（Expandable Parallel Mixture-of-Experts）架构的新框架。ExPaMoE通过双分支专家设计和token引导的特征分离解耦领域通用和领域特定知识，并基于光谱感知在线域判别器（SODD）动态扩展专家池，该判别器利用频域线索实时检测分布变化。

Result: 广泛的实验证明ExPaMoE在各种CTTA场景中表现出优越性。在CIFAR-10C、CIFAR-100C、ImageNet-C和Cityscapes-to-ACDC等标准基准上进行了评估。此外，本文还引入了ImageNet++，一个从多个ImageNet派生数据集构建的大规模、更真实的CTTA基准。ExPaMoE持续超越现有技术，展现出强大的鲁棒性、可扩展性和抗遗忘能力。

Conclusion: ExPaMoE通过其创新的可扩展并行专家混合架构，成功解决了CTTA在面对复杂、非稳态域偏移时的挑战，显著提升了模型的适应性、鲁棒性和抗遗忘能力，为未来CTTA研究提供了新的方向。

Abstract: Continual Test-Time Adaptation (CTTA) aims to enable models to adapt
on-the-fly to a stream of unlabeled data under evolving distribution shifts.
However, existing CTTA methods typically rely on shared model parameters across
all domains, making them vulnerable to feature entanglement and catastrophic
forgetting in the presence of large or non-stationary domain shifts. To address
this limitation, we propose \textbf{ExPaMoE}, a novel framework based on an
\emph{Expandable Parallel Mixture-of-Experts} architecture. ExPaMoE decouples
domain-general and domain-specific knowledge via a dual-branch expert design
with token-guided feature separation, and dynamically expands its expert pool
based on a \emph{Spectral-Aware Online Domain Discriminator} (SODD) that
detects distribution changes in real-time using frequency-domain cues.
Extensive experiments demonstrate the superiority of ExPaMoE across diverse
CTTA scenarios. We evaluate our method on standard benchmarks including
CIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC for semantic
segmentation. Additionally, we introduce \textbf{ImageNet++}, a large-scale and
realistic CTTA benchmark built from multiple ImageNet-derived datasets, to
better reflect long-term adaptation under complex domain evolution. ExPaMoE
consistently outperforms prior arts, showing strong robustness, scalability,
and resistance to forgetting.

</details>


### [91] [LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs](https://arxiv.org/abs/2507.00505)
*Haoran Lou,Chunxiao Fan,Ziyan Liu,Yuexin Wu,Xinxiang Wang*

Main category: cs.CV

TL;DR: 本文提出LLaVA-SP，通过仅添加六个空间视觉tokens来增强多模态大语言模型（MLLMs）的视觉表示能力，以解决现有模型在处理局部关系和细节理解方面的不足，并在多项基准测试中超越SOTA模型LLaVA-1.5，同时保持低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs常用的CLIP-ViT视觉编码器在捕捉全局图像特征方面表现良好，但难以建模相邻图像块之间的局部关系，导致视觉表示能力较弱，进而影响MLLMs的细节理解能力。

Method: 提出LLaVA-SP模型，通过向原始视觉tokens额外添加六个空间视觉tokens来增强视觉表示。方法包括：1) 设计一种新型投影器，利用卷积核从ViT图像块特征中提取空间视觉tokens，模拟“从中心到全局”和“从抽象到具体”两种空间排序方式，并通过交叉注意力机制融合细粒度视觉信息。2) 提出两种模型变体：LLaVA-SP-Cropping（通过渐进式裁剪聚焦细节特征）和LLaVA-SP-Pooling（通过自适应池化捕捉全局语义）。模型通过LoRA进行微调。

Result: LLaVA-SP在多项多模态基准测试中取得了显著的性能提升，在多个任务上优于最先进的LLaVA-1.5模型，并且推理延迟几乎相同。

Conclusion: LLaVA-SP通过引入少量空间视觉tokens，有效增强了MLLMs的视觉表示能力和细节理解能力，实现了卓越的性能提升和高效率。

Abstract: The architecture of multimodal large language models (MLLMs) commonly
connects a vision encoder, often based on CLIP-ViT, to a large language model.
While CLIP-ViT works well for capturing global image features, it struggles to
model local relationships between adjacent patches, leading to weaker visual
representation, which in turn affects the detailed understanding ability of
MLLMs. To solve this, we propose LLaVA-SP, which \textbf{ only adds six spatial
visual tokens} to the original visual tokens to enhance the visual
representation. Our approach offers three key advantages: 1)We propose a novel
Projector, which uses convolutional kernels to derive visual spatial tokens
from ViT patch features, simulating two visual spatial ordering approaches:
``from central region to global" and ``from abstract to specific". Then, a
cross-attention mechanism is applied to fuse fine-grained visual information,
enriching the overall visual representation. 2) We present two model variants:
LLaVA-SP-Cropping, which focuses on detail features through progressive
cropping, and LLaVA-SP-Pooling, which captures global semantics through
adaptive pooling, enabling the model to handle diverse visual understanding
tasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA,
achieves significant performance improvements across various multimodal
benchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple
tasks with nearly identical inference latency. The code and models are
available at
\href{https://github.com/CnFaker/LLaVA-SP}{\texttt{https://github.com/CnFaker/LLaVA-SP}}.

</details>


### [92] [SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning](https://arxiv.org/abs/2507.00506)
*Yunfei Xie,Yuxuan Cheng,Juncheng Wu,Haoyu Zhang,Yuyin Zhou,Shoudong Han*

Main category: cs.CV

TL;DR: 提出SCING框架，通过选择性视觉提示融合和扰动驱动一致性对齐，优化基于CLIP的行人重识别任务中的跨模态对齐和鲁棒性，同时保持高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有的将视觉-语言预训练模型（如CLIP）应用于行人重识别（ReID）任务的方法，常依赖复杂的适配器设计或模态特定调整，且忽视跨模态交互，导致高计算成本或次优的对齐效果。

Method: 提出名为“选择性跨模态提示调优”（SCING）的框架。该方法包含两项创新：1. 选择性视觉提示融合（SVIP）：一个轻量级模块，通过跨模态门控机制将判别性视觉特征动态注入文本提示。2. 扰动驱动一致性对齐（PDCA）：一种双路径训练策略，通过正则化原始和增强跨模态嵌入之间的一致性，在随机图像扰动下强制执行不变特征对齐。

Result: 在Market1501、DukeMTMC-ReID、Occluded-Duke、Occluded-REID和P-DukeMTMC等多个主流基准上进行了广泛实验，证明了该方法令人印象深刻的性能。该框架消除了笨重的适配器，同时保持了高效推理，在性能和计算开销之间实现了最佳权衡。

Conclusion: SCING框架通过其独特的SVIP和PDCA模块，有效增强了跨模态对齐和对真实世界扰动的鲁棒性，为行人重识别任务提供了一个简单而高效的解决方案，实现了性能和计算效率的优化平衡。

Abstract: Recent advancements in adapting vision-language pre-training models like CLIP
for person re-identification (ReID) tasks often rely on complex adapter design
or modality-specific tuning while neglecting cross-modal interaction, leading
to high computational costs or suboptimal alignment. To address these
limitations, we propose a simple yet effective framework named Selective
Cross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and
robustness against real-world perturbations. Our method introduces two key
innovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a
lightweight module that dynamically injects discriminative visual features into
text prompts via a cross-modal gating mechanism. Moreover, the proposed
Perturbation-Driven Consistency Alignment (PDCA) is a dual-path training
strategy that enforces invariant feature alignment under random image
perturbations by regularizing consistency between original and augmented
cross-modal embeddings. Extensive experiments are conducted on several popular
benchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID,
and P-DukeMTMC, which demonstrate the impressive performance of the proposed
method. Notably, our framework eliminates heavy adapters while maintaining
efficient inference, achieving an optimal trade-off between performance and
computational overhead. The code will be released upon acceptance.

</details>


### [93] [Topology-Constrained Learning for Efficient Laparoscopic Liver Landmark Detection](https://arxiv.org/abs/2507.00519)
*Ruize Cui,Jiaan Zhang,Jialun Pei,Kai Wang,Pheng-Ann Heng,Jing Qin*

Main category: cs.CV

TL;DR: TopoNet是一个新的拓扑约束学习框架，用于腹腔镜肝脏地标检测，通过结合RGB纹理和深度拓扑结构，以及引入拓扑约束损失，实现了高精度和低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜肝脏手术中，肝脏地标提供关键的解剖指导以降低手术风险。然而，地标的管状结构特性和术中动态形变给自动地标检测带来了巨大挑战。

Method: 本研究引入TopoNet，一个新颖的拓扑约束学习框架。该框架采用snake-CNN双路径编码器同时捕获RGB纹理信息和深度拓扑结构。此外，提出一个边界感知拓扑融合（BTF）模块，自适应地融合RGB-D特征以增强边缘感知并保留全局拓扑。最后，嵌入一个拓扑约束损失函数，包含中心线约束损失和拓扑持久性损失，以确保预测和标签之间的同伦等效性。

Result: 在L3D和P2ILF数据集上进行的广泛实验表明，TopoNet在准确性和计算复杂度方面表现出色。

Conclusion: TopoNet在腹腔镜肝脏手术中具有显著的临床应用潜力。

Abstract: Liver landmarks provide crucial anatomical guidance to the surgeon during
laparoscopic liver surgery to minimize surgical risk. However, the tubular
structural properties of landmarks and dynamic intraoperative deformations pose
significant challenges for automatic landmark detection. In this study, we
introduce TopoNet, a novel topology-constrained learning framework for
laparoscopic liver landmark detection. Our framework adopts a snake-CNN
dual-path encoder to simultaneously capture detailed RGB texture information
and depth-informed topological structures. Meanwhile, we propose a
boundary-aware topology fusion (BTF) module, which adaptively merges RGB-D
features to enhance edge perception while preserving global topology.
Additionally, a topological constraint loss function is embedded, which
contains a center-line constraint loss and a topological persistence loss to
ensure homotopy equivalence between predictions and labels. Extensive
experiments on L3D and P2ILF datasets demonstrate that TopoNet achieves
outstanding accuracy and computational complexity, highlighting the potential
for clinical applications in laparoscopic liver surgery. Our code will be
available at https://github.com/cuiruize/TopoNet.

</details>


### [94] [Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving](https://arxiv.org/abs/2507.00525)
*Djamahl Etchegaray,Yuxia Fu,Zi Huang,Yadan Luo*

Main category: cs.CV

TL;DR: 针对自动驾驶中VLM（视觉语言模型）缺乏可解释性及无法响应局部用户查询的问题，本研究引入了Box-QAymo数据集和基准，通过边界框实现用户对指定对象的时空推理查询。评估结果显示当前VLM在此方面存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在自动驾驶中常基于理想化假设，难以捕捉真实场景中的用户意图，且当前面向驾驶的VQA数据集仅限于全场景描述或路径预测，无法评估VLM是否能响应局部、用户驱动的查询，这阻碍了自动驾驶系统实现安全、可信赖且可解释的通信。

Method: 研究者提出了Box-QAymo数据集和基准，用于评估和微调VLM对用户指定对象的空间和时间推理能力。用户通过绘制边界框来表达意图。该方法包含一个分层的评估协议：从基础能力检查开始，逐步评估VLM在目标属性预测、实例运动理解以及跨帧对象间时空运动推理方面的表现。为支持此，团队众包了细粒度对象类别和视觉属性，并提取了对象轨迹以构建时间相关问答对，通过负采样、时间一致性检查和难度平衡确保了数据集的鲁棒性和多样性。

Result: 通过全面的评估，研究发现当前VLM在处理感知相关问题时存在显著局限性，这突显了它们在实现真实世界性能方面存在的巨大差距。

Conclusion: 这项工作为开发更鲁棒、更可解释的自动驾驶系统奠定了基础，使其能够在真实世界条件下与用户进行有效沟通。

Abstract: Interpretable communication is essential for safe and trustworthy autonomous
driving, yet current vision-language models (VLMs) often operate under
idealized assumptions and struggle to capture user intent in real-world
scenarios. Existing driving-oriented VQA datasets are limited to full-scene
descriptions or waypoint prediction, preventing the assessment of whether VLMs
can respond to localized user-driven queries. We introduce Box-QAymo, a
box-referring dataset and benchmark designed to both evaluate and finetune VLMs
on spatial and temporal reasoning over user-specified objects. Users express
intent by drawing bounding boxes, offering a fast and intuitive interface for
focused queries in complex scenes. Specifically, we propose a hierarchical
evaluation protocol that begins with binary sanity-check questions to assess
basic model capacities, and progresses to (1) attribute prediction for
box-referred objects, (2) motion understanding of target instances, and (3)
spatiotemporal motion reasoning over inter-object dynamics across frames. To
support this, we crowd-sourced fine-grained object classes and visual
attributes that reflect the complexity drivers encounter, and extract object
trajectories to construct temporally grounded QA pairs. Rigorous quality
control through negative sampling, temporal consistency checks, and
difficulty-aware balancing guarantee dataset robustness and diversity. Our
comprehensive evaluation reveals significant limitations in current VLMs when
queried about perception questions, highlighting the gap in achieving
real-world performance. This work provides a foundation for developing more
robust and interpretable autonomous driving systems that can communicate
effectively with users under real-world conditions. Project page and dataset
are available at https://djamahl99.github.io/qaymo-pages/.

</details>


### [95] [Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation](https://arxiv.org/abs/2507.00537)
*Feng Lin,Marco Chen,Haokui Zhang,Xiaotian Yu,Guangming Lu,Rong Xiao*

Main category: cs.CV

TL;DR: 本文研究CLIP图像编码器中注意力头的作用，提出AAT方法通过抑制有害注意力头来提升下游任务表现，尤其在跨模态检索中效果显著，且成本低。


<details>
  <summary>Details</summary>
Motivation: 尽管CLIP性能强大，但作者假设其图像编码器中某些注意力头对最终表示有负面影响，并相信通过消融这些头可以改善下游任务性能。

Method: 提出注意力消融技术（AAT），通过操纵注意力权重来抑制特定注意力头的贡献。AAT整合了两种策略，系统地识别并消融有害注意力头以增强表示质量。

Result: 实验证明，AAT持续改善了各种下游任务的表现。在跨模态检索中，它将CLIP系列模型的召回率提升高达11.1%。

Conclusion: AAT能够有效优化大型视觉-语言模型，同时几乎不增加推理成本。

Abstract: This paper studies the role of attention heads in CLIP's image encoder. While
CLIP has exhibited robust performance across diverse applications, we
hypothesize that certain attention heads negatively affect final
representations and that ablating them can improve performance in downstream
tasks. To capitalize on this insight, we propose a simple yet effective method,
called Attention Ablation Technique (AAT), to suppress the contribution of
specific heads by manipulating attention weights. By integrating two
alternative strategies tailored for different application scenarios, AAT
systematically identifies and ablates detrimental attention heads to enhance
representation quality. Experiments demonstrate that AAT consistently improves
downstream task performance across various domains, boosting recall rate by up
to 11.1% on CLIP-family models for cross-modal retrieval. The results highlight
the potential of AAT to effectively refine large-scale vision-language models
with virtually no increase in inference cost.

</details>


### [96] [LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing](https://arxiv.org/abs/2507.00554)
*Zhenya Yang,Bingchen Gong,Kai Chen,Qi Dou*

Main category: cs.CV

TL;DR: 提出LOD-GS，一种对采样率敏感的3D高斯泼溅抗锯齿框架，通过动态预测最优滤波强度来消除锯齿，并引入新数据集进行更全面评估。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅(3DGS)存在持续的锯齿伪影问题，现有抗锯齿方法（主要依赖低通滤波）对采样率不敏感，常导致欠滤波或过平滑。此外，当前方法和数据集在评估时忽略了相机距离对采样率的影响。

Method: 提出LOD-GS框架，为每个3D高斯基元动态预测最佳滤波强度。具体通过为每个高斯引入一组以采样率作为输入来建模外观变化的基函数，实现采样率敏感滤波。这些基函数参数与3D高斯参数一起进行端到端优化。同时，为解决评估缺陷，引入了一个包含不同相机距离下渲染对象的新合成数据集。

Result: 在公共数据集和新收集数据集上，该方法实现了最先进(SOTA)的渲染质量，并有效消除了锯齿。

Conclusion: LOD-GS通过其采样率敏感的滤波框架和引入的全面评估数据集，成功解决了3DGS中的锯齿问题，显著提升了渲染质量。

Abstract: Despite the advancements in quality and efficiency achieved by 3D Gaussian
Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent
challenge. Existing approaches primarily rely on low-pass filtering to mitigate
aliasing. However, these methods are not sensitive to the sampling rate, often
resulting in under-filtering and over-smoothing renderings. To address this
limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework
for Gaussian Splatting, which dynamically predicts the optimal filtering
strength for each 3D Gaussian primitive. Specifically, we introduce a set of
basis functions to each Gaussian, which take the sampling rate as input to
model appearance variations, enabling sampling-rate-sensitive filtering. These
basis function parameters are jointly optimized with the 3D Gaussian in an
end-to-end manner. The sampling rate is influenced by both focal length and
camera distance. However, existing methods and datasets rely solely on
down-sampling to simulate focal length changes for anti-aliasing evaluation,
overlooking the impact of camera distance. To enable a more comprehensive
assessment, we introduce a new synthetic dataset featuring objects rendered at
varying camera distances. Extensive experiments on both public datasets and our
newly collected dataset demonstrate that our method achieves SOTA rendering
quality while effectively eliminating aliasing. The code and dataset have been
open-sourced.

</details>


### [97] [Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment](https://arxiv.org/abs/2507.00566)
*Kai Zhou,Shuhai Zhang,Zeng You,Jinwu Hu,Mingkui Tan,Fei Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为PGFA的原型引导特征对齐范式，用于解决零样本骨架动作识别中骨架特征判别性不足和对齐偏差的问题，通过端到端跨模态对比训练和原型引导的文本特征对齐策略，在多个数据集上实现了显著的精度提升。


<details>
  <summary>Details</summary>
Motivation: 零样本骨架动作识别任务在泛化到未知动作时极具挑战性。现有方法（两阶段训练）存在两大局限：1) 骨架特征判别性不足，固定编码器未能捕捉有效对齐信息；2) 测试时忽略了骨架与未见文本特征之间的对齐偏差。

Method: 本文提出PGFA（原型引导特征对齐范式）。具体方法包括：1) 构建一个端到端的跨模态对比训练框架，以增强骨架-文本对齐并提高骨架特征的判别性；2) 引入原型引导的文本特征对齐策略，以缓解测试阶段分布差异造成的不利影响。研究提供了理论分析支持并进行了实证评估。

Result: 与顶级竞争者SMIE方法相比，PGFA在NTU-60、NTU-120和PKU-MMD三个数据集上分别取得了22.96%、12.53%和18.54%的绝对精度提升。

Conclusion: PGFA通过其创新的端到端跨模态对比训练和原型引导文本特征对齐策略，有效解决了零样本骨架动作识别中的关键挑战，显著提升了泛化性能。

Abstract: Zero-shot skeleton-based action recognition aims to classify unseen
skeleton-based human actions without prior exposure to such categories during
training. This task is extremely challenging due to the difficulty in
generalizing from known to unknown actions. Previous studies typically use
two-stage training: pre-training skeleton encoders on seen action categories
using cross-entropy loss and then aligning pre-extracted skeleton and text
features, enabling knowledge transfer to unseen classes through skeleton-text
alignment and language models' generalization. However, their efficacy is
hindered by 1) insufficient discrimination for skeleton features, as the fixed
skeleton encoder fails to capture necessary alignment information for effective
skeleton-text alignment; 2) the neglect of alignment bias between skeleton and
unseen text features during testing. To this end, we propose a prototype-guided
feature alignment paradigm for zero-shot skeleton-based action recognition,
termed PGFA. Specifically, we develop an end-to-end cross-modal contrastive
training framework to improve skeleton-text alignment, ensuring sufficient
discrimination for skeleton features. Additionally, we introduce a
prototype-guided text feature alignment strategy to mitigate the adverse impact
of the distribution discrepancy during testing. We provide a theoretical
analysis to support our prototype-guided text feature alignment strategy and
empirically evaluate our overall PGFA on three well-known datasets. Compared
with the top competitor SMIE method, our PGFA achieves absolute accuracy
improvements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD
datasets, respectively.

</details>


### [98] [Out-of-distribution detection in 3D applications: a review](https://arxiv.org/abs/2507.00570)
*Zizhao Li,Xueyang Kang,Joseph West,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: 本文全面概述了分布外(OOD)检测，强调其在自动驾驶等3D应用中对可靠AI的关键性，涵盖了用例、数据集、评估指标、现有方法和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在推理时对训练数据以外的对象泛化能力有限，可能导致未见对象的误分类或忽略，这在自动驾驶等3D应用中构成严重安全隐患。因此，作为可靠AI的一部分，识别偏离训练分布的OOD输入至关重要。

Method: 本文通过以下方面对OOD检测进行了全面综述和比较分析：介绍了关键用例、基准数据集及评估指标；分析了OOD检测方法论，包括模型结构、不确定性指标、分布距离分类法和不确定性校准技术；探讨了有前景的研究方向，特别是对抗鲁棒OOD检测和故障识别及其在3D应用中的集成。

Result: 作为一篇综述性论文，本文的“结果”在于其提供了OOD检测领域的全面理论和实践见解。它系统地展示了新兴研究机会，特别是OOD检测与3D视觉的结合。

Conclusion: 本文为OOD检测领域提供了全面的理论与实践洞察，揭示了新兴研究机遇（如3D视觉集成），旨在帮助新研究人员更有效地探索该领域，从而推动可靠、安全、鲁棒AI系统的发展。

Abstract: The ability to detect objects that are not prevalent in the training set is a
critical capability in many 3D applications, including autonomous driving.
Machine learning methods for object recognition often assume that all object
categories encountered during inference belong to a closed set of classes
present in the training data. This assumption limits generalization to the real
world, as objects not seen during training may be misclassified or entirely
ignored. As part of reliable AI, OOD detection identifies inputs that deviate
significantly from the training distribution. This paper provides a
comprehensive overview of OOD detection within the broader scope of trustworthy
and uncertain AI. We begin with key use cases across diverse domains, introduce
benchmark datasets spanning multiple modalities, and discuss evaluation
metrics. Next, we present a comparative analysis of OOD detection
methodologies, exploring model structures, uncertainty indicators, and
distributional distance taxonomies, alongside uncertainty calibration
techniques. Finally, we highlight promising research directions, including
adversarially robust OOD detection and failure identification, particularly
relevant to 3D applications. The paper offers both theoretical and practical
insights into OOD detection, showcasing emerging research opportunities such as
3D vision integration. These insights help new researchers navigate the field
more effectively, contributing to the development of reliable, safe, and robust
AI systems.

</details>


### [99] [AI-Generated Video Detection via Perceptual Straightening](https://arxiv.org/abs/2507.00583)
*Christian Internò,Robert Geirhos,Markus Olhofer,Sunny Liu,Barbara Hammer,David Klindt*

Main category: cs.CV

TL;DR: 提出ReStraV，通过分析视频在神经网络表示域中的几何特性（如时间轨迹的“直线化”）来区分真实与AI生成视频，并在检测性能上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 生成式AI迅速发展导致高逼真合成视频泛滥，对内容认证构成严峻挑战并引发滥用担忧。现有检测方法在泛化性和捕捉细微时间不一致性方面表现不足。

Method: 提出ReStraV方法。受“感知直线化”假设启发，该方法利用预训练自监督视觉Transformer (DINOv2) 量化视频在模型表示域中的时间曲率和步长距离，聚合这些度量的统计信息后训练分类器。

Result: 分析表明AI生成视频与真实视频在曲率和距离模式上存在显著差异。一个轻量级分类器在VidProM基准测试中实现了最先进的检测性能（例如，97.17%准确率和98.63% AUROC），显著优于现有图像和视频方法，且计算效率高。

Conclusion: 本工作为AI生成视频检测提供了利用神经网络表示几何的新见解，并提供了一种低成本、高效的检测解决方案。

Abstract: The rapid advancement of generative AI enables highly realistic synthetic
videos, posing significant challenges for content authentication and raising
urgent concerns about misuse. Existing detection methods often struggle with
generalization and capturing subtle temporal inconsistencies. We propose
ReStraV(Representation Straightening Video), a novel approach to distinguish
natural from AI-generated videos. Inspired by the "perceptual straightening"
hypothesis -- which suggests real-world video trajectories become more straight
in neural representation domain -- we analyze deviations from this expected
geometric property. Using a pre-trained self-supervised vision transformer
(DINOv2), we quantify the temporal curvature and stepwise distance in the
model's representation domain. We aggregate statistics of these measures for
each video and train a classifier. Our analysis shows that AI-generated videos
exhibit significantly different curvature and distance patterns compared to
real videos. A lightweight classifier achieves state-of-the-art detection
performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),
substantially outperforming existing image- and video-based methods. ReStraV is
computationally efficient, it is offering a low-cost and effective detection
solution. This work provides new insights into using neural representation
geometry for AI-generated video detection.

</details>


### [100] [Similarity Memory Prior is All You Need for Medical Image Segmentation](https://arxiv.org/abs/2507.00585)
*Tang Hao,Guo ZhiQing,Wang LieJun,Liu Chao*

Main category: cs.CV

TL;DR: 受“祖母细胞”启发，本文提出Sim-MPNet用于医学图像分割，通过动态记忆权重损失注意力（DMW-LA）和双相似度全局内部增强模块（DS-GIM）提升性能，并在多项实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 受猕猴初级视皮层（V1）中“祖母细胞”能直接识别复杂形状视觉输入的启发，研究其在促进医学图像分割研究中的价值。

Method: 设计了相似性记忆先验网络（Sim-MPNet）进行医学图像分割。具体包括：1. 提出动态记忆权重损失注意力（DMW-LA），通过原型记忆库中的相似性记忆先验匹配和记忆类别特征，并利用权重损失动态（W-LD）更新策略反向动态更新，以学习细微纹理变化并提取类别特征。2. 提出双相似度全局内部增强模块（DS-GIM），通过余弦相似度和欧氏距离深入探索输入数据特征分布的内部差异。

Result: 在四个公开数据集上的大量实验表明，Sim-MPNet比其他最先进的方法具有更好的分割性能。

Conclusion: Sim-MPNet在医学图像分割任务中表现出卓越性能，证明了所提出方法（DMW-LA和DS-GIM）的有效性，为医学图像分割提供了新的解决方案。

Abstract: In recent years, it has been found that "grandmother cells" in the primary
visual cortex (V1) of macaques can directly recognize visual input with complex
shapes. This inspires us to examine the value of these cells in promoting the
research of medical image segmentation. In this paper, we design a Similarity
Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically,
we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and
remembers the category features of specific lesions or organs in medical images
through the similarity memory prior in the prototype memory bank, thus helping
the network to learn subtle texture changes between categories. DMW-LA also
dynamically updates the similarity memory prior in reverse through Weight-Loss
Dynamic (W-LD) update strategy, effectively assisting the network directly
extract category features. In addition, we propose the Double-Similarity Global
Internal Enhancement Module (DS-GIM) to deeply explore the internal differences
in the feature distribution of input data through cosine similarity and
euclidean distance. Extensive experiments on four public datasets show that
Sim-MPNet has better segmentation performance than other state-of-the-art
methods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.

</details>


### [101] [Context-Aware Academic Emotion Dataset and Benchmark](https://arxiv.org/abs/2507.00586)
*Luming Zhao,Jingwen Xuan,Jiamin Lou,Yonghui Yu,Wenwu Yang*

Main category: cs.CV

TL;DR: 该论文提出了RAER数据集，并设计了基于CLIP的上下文感知学术情感识别方法CLIP-CAER，显著提高了在真实学习环境中识别学生学术情感的准确性，强调了上下文的重要性。


<details>
  <summary>Details</summary>
Motivation: 自动识别真实学习环境中的学生学术情感具有挑战性，主要原因是缺乏公开可用的学术情感数据集，且现有面部表情识别方法未充分考虑上下文线索。

Method: 1. 构建并发布了RAER数据集：包含约2700个视频片段，采集自约140名学生在教室、图书馆、实验室、宿舍等多种自然学习场景，并由约10位标注员使用两种粒度级别的情感标签进行标注，确保一致性和可靠性。2. 提出了CLIP-CAER方法：该方法利用视觉-语言模型CLIP中的可学习文本提示，有效整合视频中的面部表情和上下文线索来识别学术情感。

Result: 实验结果表明，CLIP-CAER方法显著优于主要为基础情感设计的现有最先进视频面部表情识别方法。

Conclusion: 上下文在准确识别学术情感中扮演着至关重要的角色。RAER数据集和CLIP-CAER方法的提出为学术情感识别领域带来了重要进展。

Abstract: Academic emotion analysis plays a crucial role in evaluating students'
engagement and cognitive states during the learning process. This paper
addresses the challenge of automatically recognizing academic emotions through
facial expressions in real-world learning environments. While significant
progress has been made in facial expression recognition for basic emotions,
academic emotion recognition remains underexplored, largely due to the scarcity
of publicly available datasets. To bridge this gap, we introduce RAER, a novel
dataset comprising approximately 2,700 video clips collected from around 140
students in diverse, natural learning contexts such as classrooms, libraries,
laboratories, and dormitories, covering both classroom sessions and individual
study. Each clip was annotated independently by approximately ten annotators
using two distinct sets of academic emotion labels with varying granularity,
enhancing annotation consistency and reliability. To our knowledge, RAER is the
first dataset capturing diverse natural learning scenarios. Observing that
annotators naturally consider context cues-such as whether a student is looking
at a phone or reading a book-alongside facial expressions, we propose CLIP-CAER
(CLIP-based Context-aware Academic Emotion Recognition). Our method utilizes
learnable text prompts within the vision-language model CLIP to effectively
integrate facial expression and context cues from videos. Experimental results
demonstrate that CLIP-CAER substantially outperforms state-of-the-art
video-based facial expression recognition methods, which are primarily designed
for basic emotions, emphasizing the crucial role of context in accurately
recognizing academic emotions. Project page: https://zgsfer.github.io/CAER

</details>


### [102] [Overtake Detection in Trucks Using CAN Bus Signals: A Comparative Study of Machine Learning Methods](https://arxiv.org/abs/2507.00593)
*Fernando Alonso-Fernandez,Talha Hanif Butt,Prayag Tiwari*

Main category: cs.CV

TL;DR: 本研究利用卡车CAN总线数据，评估ANN、RF和SVM三种机器学习分类器，以准确预测卡车超车动作，并通过分数级融合策略实现了较高的预测精度，旨在提升ADAS系统的决策能力和行车安全。


<details>
  <summary>Details</summary>
Motivation: 确保卡车安全超车对预防事故和保障交通流畅至关重要。为使高级驾驶辅助系统（ADAS）能及时做出明智决策，准确预测超车动作是必不可少的。

Method: 研究使用从五辆沃尔沃卡车收集的CAN总线数据，评估了人工神经网络（ANN）、随机森林（RF）和支持向量机（SVM）三种常见分类器进行车辆机动检测。分析了不同预处理配置对性能的影响，并采用分数级融合策略来提升每辆车的分类表现。

Result: 研究发现交通条件的多样性强烈影响信号模式，进而影响分类性能。通过多车辆数据训练可改善模型泛化能力并减少条件特异性偏差。分数级融合策略在多数情况下实现了最佳的单车性能，总体精度达到TNR=93%和TPR=86.5%。

Conclusion: 利用CAN总线数据预测卡车超车动作是有效可行的，分数级融合策略显著提升了预测精度。本研究成果有助于理解驾驶行为，并为未来摄像头监控系统（CMS）的应用提供了有价值的洞察。

Abstract: Safe overtaking manoeuvres in trucks are vital for preventing accidents and
ensuring efficient traffic flow. Accurate prediction of such manoeuvres is
essential for Advanced Driver Assistance Systems (ADAS) to make timely and
informed decisions. In this study, we focus on overtake detection using
Controller Area Network (CAN) bus data collected from five in-service trucks
provided by the Volvo Group. We evaluate three common classifiers for vehicle
manoeuvre detection, Artificial Neural Networks (ANN), Random Forest (RF), and
Support Vector Machines (SVM), and analyse how different preprocessing
configurations affect performance. We find that variability in traffic
conditions strongly influences the signal patterns, particularly in the
no-overtake class, affecting classification performance if training data lacks
adequate diversity. Since the data were collected under unconstrained,
real-world conditions, class diversity cannot be guaranteed a priori. However,
training with data from multiple vehicles improves generalisation and reduces
condition-specific bias. Our pertruck analysis also reveals that classification
accuracy, especially for overtakes, depends on the amount of training data per
vehicle. To address this, we apply a score-level fusion strategy, which yields
the best per-truck performance across most cases. Overall, we achieve an
accuracy via fusion of TNR=93% (True Negative Rate) and TPR=86.5% (True
Positive Rate). This research has been part of the BIG FUN project, which
explores how Artificial Intelligence can be applied to logged vehicle data to
understand and predict driver behaviour, particularly in relation to Camera
Monitor Systems (CMS), being introduced as digital replacements for traditional
exterior mirrors.

</details>


### [103] [World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model](https://arxiv.org/abs/2507.00603)
*Yupeng Zheng,Pengxuan Yang,Zebin Xing,Qichao Zhang,Yuhang Zheng,Yinfeng Gao,Pengfei Li,Teng Zhang,Zhongpu Xia,Peng Jia,Dongbin Zhao*

Main category: cs.CV

TL;DR: World4Drive是一个端到端自动驾驶框架，通过利用视觉基础模型构建潜在世界模型，实现了无需感知标注的自监督规划，并在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶系统通常依赖昂贵的感知监督来提取场景信息。主要挑战在于如何构建信息丰富的驾驶世界模型，以通过自监督学习实现无需感知标注的端到端规划。

Method: 提出World4Drive框架，其步骤如下：1) 利用视觉基础模型提取场景特征，包括驾驶意图和富含空间-语义先验的世界潜在表示。2) 基于这些特征和驾驶意图，生成多模态规划轨迹，并在潜在空间中预测多个意图驱动的未来状态。3) 引入世界模型选择器模块来评估并选择最佳轨迹。通过实际未来观测与潜在空间重建的预测观测之间的自监督对齐，实现无需感知标注的端到端规划。

Result: 在开放循环nuScenes和闭环NavSim基准测试中，World4Drive无需手动感知标注即达到了最先进的性能。具体表现为：L2误差相对降低18.1%，碰撞率降低46.7%，训练收敛速度快3.75倍。

Conclusion: World4Drive成功地通过自监督学习实现了无需感知标注的端到端自动驾驶规划，有效解决了对昂贵感知标注的依赖问题，并在关键性能指标上超越了现有方法。

Abstract: End-to-end autonomous driving directly generates planning trajectories from
raw sensor data, yet it typically relies on costly perception supervision to
extract scene information. A critical research challenge arises: constructing
an informative driving world model to enable perception annotation-free,
end-to-end planning via self-supervised learning. In this paper, we present
World4Drive, an end-to-end autonomous driving framework that employs vision
foundation models to build latent world models for generating and evaluating
multi-modal planning trajectories. Specifically, World4Drive first extracts
scene features, including driving intention and world latent representations
enriched with spatial-semantic priors provided by vision foundation models. It
then generates multi-modal planning trajectories based on current scene
features and driving intentions and predicts multiple intention-driven future
states within the latent space. Finally, it introduces a world model selector
module to evaluate and select the best trajectory. We achieve perception
annotation-free, end-to-end planning through self-supervised alignment between
actual future observations and predicted observations reconstructed from the
latent space. World4Drive achieves state-of-the-art performance without manual
perception annotations on both the open-loop nuScenes and closed-loop NavSim
benchmarks, demonstrating an 18.1\% relative reduction in L2 error, 46.7% lower
collision rate, and 3.75 faster training convergence. Codes will be accessed at
https://github.com/ucaszyp/World4Drive.

</details>


### [104] [De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection](https://arxiv.org/abs/2507.00608)
*Zehua Fu,Chenguang Liu,Yuyu Chen,Jiaqi Zhou,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: 针对交通场景目标检测中无监督域适应（UDA）自标注方法存在“简单样本偏差”问题，本文提出了DeSimPL方法，通过引入实例级记忆库、对抗样本和自适应加权损失来缓解该问题，显著提升了自标注检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 交通和运输场景下的目标检测需要耗时且高成本的标注数据。无监督域适应（UDA）是解决此问题的方法。当前自标注UDA方法因训练过程中简单样本比例过高（即“简单标注偏差”）导致性能不如域对齐方法，因此有必要研究并解决其局限性。

Method: 提出De-Simplifying Pseudo Labels (DeSimPL) 方法。具体措施包括：1) 利用实例级记忆库实现创新的伪标签更新策略；2) 在训练中引入对抗样本以提高样本比例；3) 提出一种自适应加权损失，以避免训练后期模型受大量假阳性伪标签影响。

Result: 实验结果表明，DeSimPL有效降低了训练过程中简单样本的比例，显著提升了自标注检测器的性能。在四个基准测试上的广泛实验验证了论文的分析和结论。

Conclusion: 自标注检测器性能受“简单样本偏差”影响的分析是准确的。DeSimPL方法通过有效解决该问题，显著提高了自标注UDA目标检测的性能，为该领域的发展提供了新的方向。

Abstract: Despite its significant success, object detection in traffic and
transportation scenarios requires time-consuming and laborious efforts in
acquiring high-quality labeled data. Therefore, Unsupervised Domain Adaptation
(UDA) for object detection has recently gained increasing research attention.
UDA for object detection has been dominated by domain alignment methods, which
achieve top performance. Recently, self-labeling methods have gained popularity
due to their simplicity and efficiency. In this paper, we investigate the
limitations that prevent self-labeling detectors from achieving commensurate
performance with domain alignment methods. Specifically, we identify the high
proportion of simple samples during training, i.e., the simple-label bias, as
the central cause. We propose a novel approach called De-Simplifying Pseudo
Labels (DeSimPL) to mitigate the issue. DeSimPL utilizes an instance-level
memory bank to implement an innovative pseudo label updating strategy. Then,
adversarial samples are introduced during training to enhance the proportion.
Furthermore, we propose an adaptive weighted loss to avoid the model suffering
from an abundance of false positive pseudo labels in the late training period.
Experimental results demonstrate that DeSimPL effectively reduces the
proportion of simple samples during training, leading to a significant
performance improvement for self-labeling detectors. Extensive experiments
conducted on four benchmarks validate our analysis and conclusions.

</details>


### [105] [UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions](https://arxiv.org/abs/2507.00648)
*Siyuan Yao,Rui Zhu,Ziqi Wang,Wenqi Ren,Yanyang Yan,Xiaochun Cao*

Main category: cs.CV

TL;DR: UMDATrack是一个统一域适应框架，通过合成少量恶劣天气数据和定制模块，显著提升了目标跟踪在多种恶劣天气下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉目标跟踪方法主要针对良好光照条件，但在夜间、雾天等恶劣天气下，由于巨大的域偏移导致性能显著下降。

Method: 提出UMDATrack，其包含：1) 可控场景生成器，在文本提示指导下合成少量（<2%）恶劣天气未标注视频；2) 域定制适配器（DCA），使目标表示快速适应不同天气；3) 目标感知置信度对齐模块（TCA），基于最优传输理论增强定位一致性。

Result: 大量实验证明UMDATrack超越现有先进视觉跟踪器，取得了显著的最新（SOTA）性能提升。

Conclusion: UMDATrack通过创新的域适应框架，有效解决了恶劣天气下视觉目标跟踪的性能下降问题，在统一框架下实现了高质量的目标状态预测，达到领先水平。

Abstract: Visual object tracking has gained promising progress in past decades. Most of
the existing approaches focus on learning target representation in
well-conditioned daytime data, while for the unconstrained real-world scenarios
with adverse weather conditions, e.g. nighttime or foggy environment, the
tremendous domain shift leads to significant performance degradation. In this
paper, we propose UMDATrack, which is capable of maintaining high-quality
target state prediction under various adverse weather conditions within a
unified domain adaptation framework. Specifically, we first use a controllable
scenario generator to synthesize a small amount of unlabeled videos (less than
2% frames in source daytime datasets) in multiple weather conditions under the
guidance of different text prompts. Afterwards, we design a simple yet
effective domain-customized adapter (DCA), allowing the target objects'
representation to rapidly adapt to various weather conditions without redundant
model updating. Furthermore, to enhance the localization consistency between
source and target domains, we propose a target-aware confidence alignment
module (TCA) following optimal transport theorem. Extensive experiments
demonstrate that UMDATrack can surpass existing advanced visual trackers and
lead new state-of-the-art performance by a significant margin. Our code is
available at https://github.com/Z-Z188/UMDATrack.

</details>


### [106] [LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment](https://arxiv.org/abs/2507.00659)
*Juelin Zhu,Shuaibang Peng,Long Wang,Hanlin Tan,Yu Liu,Maojun Zhang,Shen Yan*

Main category: cs.CV

TL;DR: 本文提出LoD-Loc v2，一种新颖的粗到精轮廓对齐方法，首次实现在低细节层次（LoD1）城市模型上的无人机空中视觉定位，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如LoD-Loc）主要依赖高细节层次（LoD2/LoD3）城市模型，然而全球绝大多数可用和计划建设的城市模型都是低细节层次（LoD1）。实现在低LoD模型上的定位对于释放无人机在全球城市定位的潜力至关重要。

Method: LoD-Loc v2采用粗到精的策略，利用显式轮廓对齐进行定位。具体步骤包括：1) 使用建筑物分割网络提取查询图像中的建筑物轮廓。2) 在粗姿态选择阶段，通过统一采样姿态假设并构建姿态成本体积，衡量投影与预测轮廓的对齐度，以选择粗姿态。3) 在精细姿态估计阶段，结合多束追踪的粒子滤波方法来探索假设空间并获得最终姿态。为促进研究，还发布了两个LoD1城市模型数据集。

Result: 实验结果表明，LoD-Loc v2不仅提升了高LoD模型的估计精度，而且首次实现了基于低LoD模型的定位。它显著优于现有最先进的基线方法，甚至超越了基于纹理的模型方法，并扩大了收敛范围以适应更大的先验误差。

Conclusion: LoD-Loc v2成功解决了在低LoD城市模型上进行空中视觉定位的挑战，通过创新的粗到精轮廓对齐策略，显著提升了定位精度和鲁棒性，为无人机在全球城市环境中的广泛应用开辟了新途径。

Abstract: We propose a novel method for aerial visual localization over low
Level-of-Detail (LoD) city models. Previous wireframe-alignment-based method
LoD-Loc has shown promising localization results leveraging LoD models.
However, LoD-Loc mainly relies on high-LoD (LoD3 or LoD2) city models, but the
majority of available models and those many countries plan to construct
nationwide are low-LoD (LoD1). Consequently, enabling localization on low-LoD
city models could unlock drones' potential for global urban localization. To
address these issues, we introduce LoD-Loc v2, which employs a coarse-to-fine
strategy using explicit silhouette alignment to achieve accurate localization
over low-LoD city models in the air. Specifically, given a query image, LoD-Loc
v2 first applies a building segmentation network to shape building silhouettes.
Then, in the coarse pose selection stage, we construct a pose cost volume by
uniformly sampling pose hypotheses around a prior pose to represent the pose
probability distribution. Each cost of the volume measures the degree of
alignment between the projected and predicted silhouettes. We select the pose
with maximum value as the coarse pose. In the fine pose estimation stage, a
particle filtering method incorporating a multi-beam tracking approach is used
to efficiently explore the hypothesis space and obtain the final pose
estimation. To further facilitate research in this field, we release two
datasets with LoD1 city models covering 10.7 km , along with real RGB queries
and ground-truth pose annotations. Experimental results show that LoD-Loc v2
improves estimation accuracy with high-LoD models and enables localization with
low-LoD models for the first time. Moreover, it outperforms state-of-the-art
baselines by large margins, even surpassing texture-model-based methods, and
broadens the convergence basin to accommodate larger prior errors.

</details>


### [107] [A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation](https://arxiv.org/abs/2507.00676)
*Edward Effendy,Kuan-Wei Tseng,Rei Kawakami*

Main category: cs.CV

TL;DR: 提出一种新颖的基于Transformer的全身抓取框架，整合姿态生成与动作填充，实现真实稳定的物体交互。


<details>
  <summary>Details</summary>
Motivation: 旨在解决全身抓取中姿态生成和动作填充的挑战，以实现真实稳定的物体交互，并克服手物交互数据稀缺问题。

Method: 采用三阶段管道：抓取姿态生成、时序填充和LiftUp Transformer。通过在大型运动数据集上进行数据高效的泛化预训练，获取可转移的时空表示。

Result: 在GRAB数据集上表现优异，在连贯性、稳定性、视觉真实感方面超越现有技术。

Conclusion: 该框架有效提升了全身抓取的真实性和稳定性，且其模块化设计使其易于推广应用于其他人体运动场景。

Abstract: Accepted in the ICIP 2025
  We present a novel transformer-based framework for whole-body grasping that
addresses both pose generation and motion infilling, enabling realistic and
stable object interactions. Our pipeline comprises three stages: Grasp Pose
Generation for full-body grasp generation, Temporal Infilling for smooth motion
continuity, and a LiftUp Transformer that refines downsampled joints back to
high-resolution markers. To overcome the scarcity of hand-object interaction
data, we introduce a data-efficient Generalized Pretraining stage on large,
diverse motion datasets, yielding robust spatio-temporal representations
transferable to grasping tasks. Experiments on the GRAB dataset show that our
method outperforms state-of-the-art baselines in terms of coherence, stability,
and visual realism. The modular design also supports easy adaptation to other
human-motion applications.

</details>


### [108] [ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2507.00898)
*Zifu Wan,Ce Zhang,Silong Yong,Martin Q. Ma,Simon Stepputtis,Louis-Philippe Morency,Deva Ramanan,Katia Sycara,Yaqi Xie*

Main category: cs.CV

TL;DR: 本文提出了一种名为ONLY的免训练解码方法，通过单次查询和一层干预，高效地缓解大型视觉-语言模型（LVLMs）中的幻觉问题，并优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型（LVLMs）虽性能出色，但普遍存在幻觉问题，限制了其在实际应用中的可靠部署。现有对比解码方法虽能缓解幻觉，但需要多次查询，导致响应速度慢，不适用于实时应用。

Method: 本文提出ONLY，一种免训练解码方法。该方法仅需单次查询和解码过程中的一层干预，即可实现高效实时部署。具体通过使用每个token的文本-视觉熵比，选择性地放大关键文本信息来增强文本输出。

Result: ONLY在多个基准测试中持续优于最先进的方法，同时所需的实现工作量和计算成本极低。

Conclusion: ONLY提供了一种高效、低成本且性能卓越的解决方案，有效缓解了LVLMs的幻觉问题，使其更适合实时应用。

Abstract: Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm
for understanding and reasoning about image input through textual responses.
Although they have achieved remarkable performance across a range of
multi-modal tasks, they face the persistent challenge of hallucination, which
introduces practical weaknesses and raises concerns about their reliable
deployment in real-world applications. Existing work has explored contrastive
decoding approaches to mitigate this issue, where the output of the original
LVLM is compared and contrasted with that of a perturbed version. However,
these methods require two or more queries that slow down LVLM response
generation, making them less suitable for real-time applications. To overcome
this limitation, we propose ONLY, a training-free decoding approach that
requires only a single query and a one-layer intervention during decoding,
enabling efficient real-time deployment. Specifically, we enhance textual
outputs by selectively amplifying crucial textual information using a
text-to-visual entropy ratio for each token. Extensive experimental results
demonstrate that our proposed ONLY consistently outperforms state-of-the-art
methods across various benchmarks while requiring minimal implementation effort
and computational cost. Code is available at https://github.com/zifuwan/ONLY.

</details>


### [109] [Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack](https://arxiv.org/abs/2507.00690)
*Keke Tang,Ziyong Du,Weilong Peng,Xiaofei Wang,Peican Zhu,Ligang Liu,Zhihong Tian*

Main category: cs.CV

TL;DR: 本文提出CageAttack，一种基于笼形形变的对抗性攻击框架，能生成自然、可迁移且难以防御的点云对抗样本，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的点云对抗攻击方法（如严格几何约束）限制了攻击的可迁移性和抗防御性；而无结构变形方法常导致不自然扭曲，损害对抗样本的真实性。因此，需要一种方法能生成既自然又具高效攻击性的对抗点云。

Method: 本文提出CageAttack框架。该方法首先在目标物体周围构建一个笼形结构，以此为基础进行平滑、自然的形变。随后，对笼形结构的顶点施加扰动，这些扰动会无缝传播到点云，确保产生的形变与物体本身保持一致性并保留真实性。

Result: 在七种3D深度神经网络分类器和三个数据集上的大量实验表明，CageAttack在可迁移性、抗防御性和真实性之间取得了卓越的平衡，性能超越了现有最先进的方法。

Conclusion: CageAttack通过引入笼形形变框架，成功解决了点云对抗攻击中自然性、可迁移性和抗防御性之间的矛盾，实现了更优的攻击效果和真实感，为点云对抗样本生成设定了新基准。

Abstract: Adversarial attacks on point clouds often impose strict geometric constraints
to preserve plausibility; however, such constraints inherently limit
transferability and undefendability. While deformation offers an alternative,
existing unstructured approaches may introduce unnatural distortions, making
adversarial point clouds conspicuous and undermining their plausibility. In
this paper, we propose CageAttack, a cage-based deformation framework that
produces natural adversarial point clouds. It first constructs a cage around
the target object, providing a structured basis for smooth, natural-looking
deformation. Perturbations are then applied to the cage vertices, which
seamlessly propagate to the point cloud, ensuring that the resulting
deformations remain intrinsic to the object and preserve plausibility.
Extensive experiments on seven 3D deep neural network classifiers across three
datasets show that CageAttack achieves a superior balance among
transferability, undefendability, and plausibility, outperforming
state-of-the-art methods. Codes will be made public upon acceptance.

</details>


### [110] [Rectifying Magnitude Neglect in Linear Attention](https://arxiv.org/abs/2507.00698)
*Qihang Fan,Huaibo Huang,Yuang Ai,ran He*

Main category: cs.CV

TL;DR: 本文分析了线性注意力（Linear Attention）性能低于Softmax注意力（Softmax Attention）的原因，发现其忽视Query的幅度信息。为此，提出幅度感知线性注意力（MALA），通过整合Query幅度信息，使其注意力分布更接近Softmax注意力，并在多项视觉与语言任务中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: Softmax注意力具有出色的全局建模能力，但其二次复杂度限制了应用。线性注意力虽复杂度为线性，但相比Softmax注意力存在显著的性能下降。研究旨在揭示线性注意力性能下降的深层原因并加以解决。

Method: 分析发现，线性注意力与Softmax注意力不同，完全忽视了Query的幅度信息，导致注意力分数分布无法动态适应Query尺度的变化，从而与Softmax注意力的分布差异显著。基于此观察，提出幅度感知线性注意力（MALA），通过修改计算方式，使其能充分融入Query的幅度信息，从而生成更接近Softmax注意力且结构更均衡的注意力分数分布。

Result: MALA在图像分类、目标检测、实例分割、语义分割、自然语言处理、语音识别和图像生成等多个任务上进行了评估，并在所有这些任务上均取得了强大的结果。

Conclusion: MALA通过有效整合Query的幅度信息，成功解决了线性注意力性能下降的问题，使其在保持线性复杂度的同时，能够生成更接近Softmax注意力的注意力分布，并在广泛的任务中展现出卓越的有效性。

Abstract: As the core operator of Transformers, Softmax Attention exhibits excellent
global modeling capabilities. However, its quadratic complexity limits its
applicability to vision tasks. In contrast, Linear Attention shares a similar
formulation with Softmax Attention while achieving linear complexity, enabling
efficient global information modeling. Nevertheless, Linear Attention suffers
from a significant performance degradation compared to standard Softmax
Attention. In this paper, we analyze the underlying causes of this issue based
on the formulation of Linear Attention. We find that, unlike Softmax Attention,
Linear Attention entirely disregards the magnitude information of the Query.
This prevents the attention score distribution from dynamically adapting as the
Query scales. As a result, despite its structural similarity to Softmax
Attention, Linear Attention exhibits a significantly different attention score
distribution. Based on this observation, we propose Magnitude-Aware Linear
Attention (MALA), which modifies the computation of Linear Attention to fully
incorporate the Query's magnitude. This adjustment allows MALA to generate an
attention score distribution that closely resembles Softmax Attention while
exhibiting a more well-balanced structure. We evaluate the effectiveness of
MALA on multiple tasks, including image classification, object detection,
instance segmentation, semantic segmentation, natural language processing,
speech recognition, and image generation. Our MALA achieves strong results on
all of these tasks. Code will be available at https://github.com/qhfan/MALA

</details>


### [111] [BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving](https://arxiv.org/abs/2507.00707)
*Zeming Chen,Hang Zhao*

Main category: cs.CV

TL;DR: 本文提出BEV-VAE模型，通过构建紧凑统一的BEV（鸟瞰图）潜在空间，解决自动驾驶多视角图像生成中缺乏3D一致性的问题，实现可控且3D一致的场景合成。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的多视角图像生成要求跨摄像机视图的3D场景理解一致性。然而，大多数现有方法将此视为2D图像集生成任务，缺乏显式3D建模。本研究认为结构化表示对于场景生成，特别是自动驾驶应用至关重要。

Method: BEV-VAE模型首先训练一个多视角图像变分自编码器（VAE），以学习一个紧凑且统一的BEV潜在空间。随后，利用潜在扩散Transformer从该潜在空间生成场景。该模型支持给定相机配置生成任意视角，并可选择性地输入3D布局。

Result: 在nuScenes和Argoverse 2 (AV2)数据集上进行的实验表明，BEV-VAE在3D一致性重建和生成方面均表现出强大的性能。

Conclusion: BEV-VAE通过引入结构化的BEV潜在空间，有效解决了自动驾驶多视角图像生成中的3D一致性问题，实现了高质量且可控的场景合成，证实了结构化表示在这一领域的关键作用。

Abstract: Multi-view image generation in autonomous driving demands consistent 3D scene
understanding across camera views. Most existing methods treat this problem as
a 2D image set generation task, lacking explicit 3D modeling. However, we argue
that a structured representation is crucial for scene generation, especially
for autonomous driving applications. This paper proposes BEV-VAE for consistent
and controllable view synthesis. BEV-VAE first trains a multi-view image
variational autoencoder for a compact and unified BEV latent space and then
generates the scene with a latent diffusion transformer. BEV-VAE supports
arbitrary view generation given camera configurations, and optionally 3D
layouts. Experiments on nuScenes and Argoverse 2 (AV2) show strong performance
in both 3D consistent reconstruction and generation. The code is available at:
https://github.com/Czm369/bev-vae.

</details>


### [112] [TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving](https://arxiv.org/abs/2507.00709)
*Yiming Yang,Yueru Luo,Bingkun He,Hongbin Lin,Suzhong Fu,Chao Yan,Kun Tang,Xinrui Yan,Chao Zheng,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: TopoStreamer是一个用于车道段拓扑推理的端到端时间感知模型，通过引入流式属性约束、动态车道边界位置编码和车道段去噪，解决了现有方法的局限性，并在OpenLane-V2数据集上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有车道段拓扑推理方法在一致性位置嵌入和时间多属性学习方面存在局限性，阻碍了准确的道路网络重建。准确的道路网络对于自动驾驶系统执行转弯、变道等路依赖操作至关重要。

Method: 提出TopoStreamer模型，一个端到端的时间感知模型，包含三项关键改进：1. 流式属性约束，强制中心线和边界坐标及其分类的时间一致性。2. 动态车道边界位置编码，增强对最新位置信息的学习。3. 车道段去噪，帮助捕获多样化的车道段模式。此外，采用车道边界分类指标评估现有模型的准确性。

Result: 在OpenLane-V2数据集上，TopoStreamer相较于现有最先进方法实现了显著的性能提升：车道段感知任务的mAP提升了3.4%，中心线感知任务的OLS提升了2.1%。

Conclusion: TopoStreamer通过其创新性改进，有效解决了车道段拓扑推理中的关键问题，并在关键感知任务上取得了最先进的性能，为端到端自动驾驶系统提供更准确的道路网络重建能力。

Abstract: Lane segment topology reasoning constructs a comprehensive road network by
capturing the topological relationships between lane segments and their
semantic types. This enables end-to-end autonomous driving systems to perform
road-dependent maneuvers such as turning and lane changing. However, the
limitations in consistent positional embedding and temporal multiple attribute
learning in existing methods hinder accurate roadnet reconstruction. To address
these issues, we propose TopoStreamer, an end-to-end temporal perception model
for lane segment topology reasoning. Specifically, TopoStreamer introduces
three key improvements: streaming attribute constraints, dynamic lane boundary
positional encoding, and lane segment denoising. The streaming attribute
constraints enforce temporal consistency in both centerline and boundary
coordinates, along with their classifications. Meanwhile, dynamic lane boundary
positional encoding enhances the learning of up-to-date positional information
within queries, while lane segment denoising helps capture diverse lane segment
patterns, ultimately improving model performance. Additionally, we assess the
accuracy of existing models using a lane boundary classification metric, which
serves as a crucial measure for lane-changing scenarios in autonomous driving.
On the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements
over state-of-the-art methods, achieving substantial performance gains of +3.4%
mAP in lane segment perception and +2.1% OLS in centerline perception tasks.

</details>


### [113] [UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement](https://arxiv.org/abs/2507.00721)
*Xiao Zhang,Fei Wei,Yong Wang,Wenda Zhao,Feiyi Li,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 针对零样本域适应(ZSDA)中检测任务与视觉语言模型(VLM)错位及手动提示的局限性，本文提出UPRE框架，通过联合优化文本提示和视觉表示，结合多视图域提示和视觉增强模块，并在多层次策略下实现对齐和多样化表示，在九个基准数据集上展现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 零样本域适应 (ZSDA) 因目标域图像缺失而面临严峻挑战。现有利用视觉语言模型 (VLM) 的方法主要关注域分布漂移，却忽视了检测任务与VLM之间因依赖手动提示而产生的错位问题。

Method: 本文提出统一提示和表示增强 (UPRE) 框架，该框架联合优化文本提示和视觉表示。具体包括：1) 引入多视图域提示，结合语言域先验和检测特定知识；2) 引入视觉表示增强模块，生成域风格变体；3) 采用多层次增强策略，包括相对域距离（在图像级别对齐多模态表示）和正负分离（在实例级别捕获多样化视觉表示）。

Result: 在九个基准数据集上进行的广泛实验表明，所提出的UPRE框架在ZSDA检测场景中表现出卓越的性能。

Conclusion: UPRE框架通过联合优化文本提示和视觉表示，成功解决了ZSDA中VLM与检测任务错位以及手动提示依赖的问题，并在多个基准数据集上证明了其卓越的性能和有效性。

Abstract: Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the
lack of images in the target domain. Previous approaches leverage
Vision-Language Models (VLMs) to tackle this challenge, exploiting their
zero-shot learning capabilities. However, these methods primarily address
domain distribution shifts and overlook the misalignment between the detection
task and VLMs, which rely on manually crafted prompts. To overcome these
limitations, we propose the unified prompt and representation enhancement
(UPRE) framework, which jointly optimizes both textual prompts and visual
representations. Specifically, our approach introduces a multi-view domain
prompt that combines linguistic domain priors with detection-specific
knowledge, and a visual representation enhancement module that produces domain
style variations. Furthermore, we introduce multi-level enhancement strategies,
including relative domain distance and positive-negative separation, which
align multi-modal representations at the image level and capture diverse visual
representations at the instance level, respectively. Extensive experiments
conducted on nine benchmark datasets demonstrate the superior performance of
our framework in ZSDA detection scenarios. Code is available at
https://github.com/AMAP-ML/UPRE.

</details>


### [114] [Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features](https://arxiv.org/abs/2507.00724)
*Linghui Zhu,Yiming Li,Haiqin Weng,Yan Liu,Tianwei Zhang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 针对微调个性化大视觉模型面临的模型窃取风险，本文揭示了现有防御方法的不足。为此，提出了一种无害化模型所有权验证方法，通过解耦共同特征与数据集特定特征，利用影子模型、元分类器和假设检验来有效检测模型窃取。


<details>
  <summary>Details</summary>
Motivation: 大视觉模型通过使用私有数据进行微调实现个性化，使其成为宝贵的知识产权。然而，模型窃取攻击对这些个性化模型构成重大风险。现有针对传统DNN的防御方法（通常为从零训练的模型设计）对微调模型要么引入额外安全风险，要么易于误判，甚至无效。

Method: 提出一种通过解耦相似共同特征的无害化模型所有权验证方法。该方法包含三个阶段：1) 创建影子模型，保留受害模型的共同特征并破坏数据集特定特征；2) 利用影子模型与受害模型的输出差异来表示数据集特定特征，并训练一个元分类器以识别是否包含这些特征来检测窃取模型；3) 通过假设检验进行模型所有权验证，以降低随机性并增强鲁棒性。

Result: 在基准数据集上的广泛实验验证了所提出方法在同时检测不同类型模型窃取方面的有效性。

Conclusion: 本文提出了一种针对微调个性化大视觉模型的有效且无害的模型所有权验证方法，成功解决了现有防御方法对这类模型失效的问题，能够有效检测模型窃取。

Abstract: Large vision models achieve remarkable performance in various downstream
tasks, primarily by personalizing pre-trained models through fine-tuning with
private and valuable local data, which makes the personalized model a valuable
intellectual property for its owner. Similar to the era of traditional DNNs,
model stealing attacks also pose significant risks to these personalized
models. However, in this paper, we reveal that most existing defense methods
(developed for traditional DNNs), typically designed for models trained from
scratch, either introduce additional security risks, are prone to misjudgment,
or are even ineffective for fine-tuned models. To alleviate these problems,
this paper proposes a harmless model ownership verification method for
personalized models by decoupling similar common features. In general, our
method consists of three main stages. In the first stage, we create shadow
models that retain common features of the victim model while disrupting
dataset-specific features. We represent the dataset-specific features of the
victim model by the output differences between the shadow and victim models.
After that, a meta-classifier is trained to identify stolen models by
determining whether suspicious models contain the dataset-specific features of
the victim. In the third stage, we conduct model ownership verification by
hypothesis test to mitigate randomness and enhance robustness. Extensive
experiments on benchmark datasets verify the effectiveness of the proposed
method in detecting different types of model stealing simultaneously.

</details>


### [115] [Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network](https://arxiv.org/abs/2507.00739)
*An Le,Hung Nguyen,Sungbal Seo,You-Suk Bae,Truong Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种新型双正交可调小波单元，通过放宽滤波器设计约束，显著提升了卷积神经网络（CNN）在图像分类和异常检测任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过引入更灵活的滤波器设计，优化CNN中的卷积、池化和下采样操作，从而改进图像分类和异常检测的准确性和鲁棒性。

Method: 采用提升方案（lifting scheme）构建了一种新型双正交可调小波单元。该单元放宽了正交性和等滤波器长度的约束，提供了更大的滤波器设计灵活性。该单元被集成到ResNet-18和ResNet-34等卷积神经网络中进行性能验证。

Result: 在图像分类方面，集成到ResNet-18后，在CIFAR-10数据集上分类精度提升2.12%，在Describable Textures Dataset (DTD)上提升9.73%，显示出其捕获细粒度细节的有效性。在ResNet-34上也观察到类似提升。在MVTec异常检测数据集的榛子类别上，该方法在分割和检测任务中均表现出竞争性且平衡的性能，并在准确性和鲁棒性方面优于现有方法。

Conclusion: 该新型双正交可调小波单元能有效增强CNN在图像分类（尤其是在细粒度细节捕获方面）和异常检测任务中的性能，并表现出优于现有方法的准确性和鲁棒性。

Abstract: This work introduces a novel biorthogonal tunable wavelet unit constructed
using a lifting scheme that relaxes both the orthogonality and equal filter
length constraints, providing greater flexibility in filter design. The
proposed unit enhances convolution, pooling, and downsampling operations,
leading to improved image classification and anomaly detection in convolutional
neural networks (CNN). When integrated into an 18-layer residual neural network
(ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12%
and on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its
effectiveness in capturing fine-grained details. Similar improvements were
observed in ResNet-34. For anomaly detection in the hazelnut category of the
MVTec Anomaly Detection dataset, the proposed method achieved competitive and
wellbalanced performance in both segmentation and detection tasks,
outperforming existing approaches in terms of accuracy and robustness.

</details>


### [116] [Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning](https://arxiv.org/abs/2507.00748)
*Bob Zhang,Haoran Li,Tao Zhang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Yanbin Hao*

Main category: cs.CV

TL;DR: 本文提出一种基于强化学习（RL）的后训练策略，通过冷启动SFT（使用CoT数据）和基于规则的RL，显著提升了多模态大语言模型（MLLM）在复杂多图推理和泛化任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在单图视觉定位方面表现出色，但在处理涉及复杂多图组合和多模态指令的真实世界应用时性能下降，暴露出跨图推理和泛化能力的局限性。

Method: 采用基于强化学习（RL）的后训练策略。首先，合成高质量思维链（CoT）数据进行冷启动监督微调（SFT），使用LoRA使模型识别正确解决方案。随后，利用SFT模型进行拒绝采样以筛选高质量RL数据，并通过基于规则的RL引导模型走向最优推理路径。

Result: 本方法在MIG-Bench上取得了+9.04%的性能提升，在多个域外推理定位基准上相对SFT基线提升了+4.98%。此外，在多图感知泛化方面也表现出强大能力，在BLINK和MMIU基准的子集上分别相对基础模型提升了+3.1%和+2.4%。

Conclusion: 所提出的RL后训练策略有效解决了MLLMs在多图推理和泛化方面的挑战，显著提升了其在复杂多图定位任务中的性能和泛化能力。

Abstract: Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding
in single-image scenarios with textual references. However, their performance
degrades when handling real-world applications involving complex multi-image
compositions and multimodal instructions, which reveals limitations in
cross-image reasoning and generalization. To address these challenges, we adopt
a Reinforcement Learning (RL) based post-training strategy to improve the
reasoning performance of MLLMs in multi-image grounding tasks. Our approach
begins with synthesizing high-quality chain-of-thought (CoT) data for
cold-start initialization, followed by supervised fine-tuning (SFT) using
low-rank adaptation (LoRA). The cold-start training stage enables the model to
identify correct solutions. Subsequently, we perform rejection sampling using
the merged SFT model to curate high-quality RL data and leverage rule-based RL
to guide the model toward optimal reasoning paths. Extensive experimental
results demonstrate the effectiveness of our approach, achieving +9.04\%
improvements on MIG-Bench and +4.98\% improvements on several out-of-domain
reasoning grounding benchmarks over the SFT baseline. Furthermore, our approach
exhibits strong generalization in multi-image perception, with gains of +3.1\%
and +2.4\% over the base model on subsets of the BLINK and MMIU benchmarks,
respectively.

</details>


### [117] [Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation](https://arxiv.org/abs/2507.00752)
*Hao Xing,Kai Zhe Boey,Yuankai Wu,Darius Burschka,Gordon Cheng*

Main category: cs.CV

TL;DR: 本文提出一种多模态图卷积网络（MMGCN），通过融合多源数据和创新的数据增强策略，有效解决人类动作时间分割中的过分割问题，显著提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 智能机器人需要精确的人类动作时间分割来理解子活动及其时序结构。然而，现有的人体姿态估计和目标检测中的噪声常导致动作序列的过分割，从而破坏动作的连贯性。

Method: 本文提出MMGCN框架，整合低帧率视觉数据与高帧率运动数据（骨架和目标检测）来减少碎片化。主要贡献包括：1) 弦波编码策略，将3D骨架坐标映射到连续sin-cos空间，增强空间表示鲁棒性；2) 时序图融合模块，通过分层特征聚合对齐不同分辨率的多模态输入；3) SmoothLabelMix数据增强技术，通过混合输入序列和标签生成具有渐进动作过渡的合成训练样本，提升预测的时间一致性并减少过分割伪影。

Result: 在Bimanual Actions数据集上的广泛实验表明，本方法优于现有最先进方法，尤其在动作分割精度方面表现出色，F1@10达到94.5%，F1@25达到92.8%。

Conclusion: 本研究所提出的MMGCN方法能有效缓解人类动作时间分割中的过分割问题，并显著提高分割精度，对于协作机器人等应用具有重要意义。

Abstract: Accurate temporal segmentation of human actions is critical for intelligent
robots in collaborative settings, where a precise understanding of sub-activity
labels and their temporal structure is essential. However, the inherent noise
in both human pose estimation and object detection often leads to
over-segmentation errors, disrupting the coherence of action sequences. To
address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that
integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g.,
30 fps) motion data (skeleton and object detections) to mitigate fragmentation.
Our framework introduces three key contributions. First, a sinusoidal encoding
strategy that maps 3D skeleton coordinates into a continuous sin-cos space to
enhance spatial representation robustness. Second, a temporal graph fusion
module that aligns multi-modal inputs with differing resolutions via
hierarchical feature aggregation, Third, inspired by the smooth transitions
inherent to human actions, we design SmoothLabelMix, a data augmentation
technique that mixes input sequences and labels to generate synthetic training
examples with gradual action transitions, enhancing temporal consistency in
predictions and reducing over-segmentation artifacts.
  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for
human-object interaction understanding, demonstrate that our approach
outperforms state-of-the-art methods, especially in action segmentation
accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.

</details>


### [118] [Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs](https://arxiv.org/abs/2507.00754)
*Selim Kuzucu,Muhammad Ferjad Naeem,Anna Kukleva,Federico Tombari,Bernt Schiele*

Main category: cs.CV

TL;DR: LUViT提出一种协同预训练策略，通过MAE预训练ViT并使用MAE目标训练LLM中的LoRA层，有效克服LLM和ViT间的模态不匹配，显著提升视觉任务性能。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型（LLMs）与视觉Transformer（ViTs）结合有望增强视觉任务，但LLMs的文本中心预训练与ViTs的视觉中心训练之间存在固有模态不匹配，导致直接融合效果不佳、微调不稳定，LLM潜力未能充分利用。

Method: 提出LUViT，通过协同预训练策略弥合模态不匹配。具体方法包括：1) 使用掩码自编码（MAE）预训练ViT骨干，以获取更丰富的视觉表示；2) 采用MAE目标同时训练LLM块中的低秩适应（LoRA）层。这种联合优化旨在引导ViT生成与LLM对齐的特征，并使LLM能有效解释视觉信息。

Result: 通过大量实验证明，LUViT显著提升了在各种下游视觉任务上的性能，展示了利用LLM知识进行视觉理解的更有效和高效的途径。

Conclusion: LUViT通过创新的协同预训练策略，成功克服了LLMs与ViTs之间的模态不匹配问题，有效利用了LLMs的强大能力来增强视觉理解，为未来多模态模型集成提供了有价值的路径。

Abstract: The integration of Large Language Model (LLMs) blocks with Vision
Transformers (ViTs) holds immense promise for vision-only tasks by leveraging
the rich semantic knowledge and reasoning capabilities of LLMs. However, a
fundamental challenge lies in the inherent modality mismatch between
text-centric pretraining of LLMs and vision-centric training of ViTs. Direct
fusion often fails to fully exploit the LLM's potential and suffers from
unstable finetuning. As a result, LLM blocks are kept frozen while only the
vision components are learned. As a remedy to these challenges, we introduce
Language-Unlocked Vision Transformers (LUViT), a novel approach that bridges
this modality mismatch through a synergistic pre-training strategy. LUViT
co-adapts a ViT backbone and an LLM fusion block by (1) employing Masked
Auto-Encoding (MAE) to pre-train the ViT for richer visual representations, and
(2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM
block using the MAE objective. This joint optimization guides the ViT to
produce LLM-aligned features and the LLM to effectively interpret visual
information. We demonstrate through extensive experiments that LUViT
significantly improves performance on various downstream vision tasks,
showcasing a more effective and efficient pathway to harness LLM knowledge for
visual understanding.

</details>


### [119] [Towards Open-World Human Action Segmentation Using Graph Convolutional Networks](https://arxiv.org/abs/2507.00756)
*Hao Xing,Kai Zhe Boey,Gordon Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种解决开放世界动作分割问题的新框架，通过引入EPGCN、基于Mixup的训练和时间聚类损失，实现了对未知动作的有效检测和分割，并在两个数据集上显著超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法在封闭世界动作分割中表现良好，但在开放世界场景中，面对新出现的动作时泛化能力不足。由于人类活动的多样性，收集所有动作类别进行训练是不现实的，因此需要模型无需手动标注即可检测和分割分布外动作。

Method: 本文定义了开放世界动作分割问题，并提出一个结构化框架来检测和分割未见动作。主要创新包括：1) 增强型金字塔图卷积网络（EPGCN）及其新颖的解码器模块；2) 基于Mixup的训练，用于合成分布外数据，无需手动标注；3) 新型时间聚类损失，用于聚合并分布内动作并区分分布外样本。

Result: 在Bimanual Actions和H2O两个人类-物体交互识别数据集上进行评估。实验结果显示，相对于最先进的动作分割模型，在开放集分割（F1@50）方面相对提升16.9%，在分布外检测性能（AUROC）方面相对提升34.6%。同时进行了深入的消融研究。

Conclusion: 所提出的框架有效解决了开放世界动作分割问题，在检测和分割未见动作方面表现出显著优于现有技术的能力，并通过消融研究确定了最优配置，验证了各组件的有效性。

Abstract: Human-object interaction segmentation is a fundamental task of daily activity
understanding, which plays a crucial role in applications such as assistive
robotics, healthcare, and autonomous systems. Most existing learning-based
methods excel in closed-world action segmentation, they struggle to generalize
to open-world scenarios where novel actions emerge. Collecting exhaustive
action categories for training is impractical due to the dynamic diversity of
human activities, necessitating models that detect and segment
out-of-distribution actions without manual annotation. To address this issue,
we formally define the open-world action segmentation problem and propose a
structured framework for detecting and segmenting unseen actions. Our framework
introduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional
Network (EPGCN) with a novel decoder module for robust spatiotemporal feature
upsampling. 2) Mixup-based training to synthesize out-of-distribution data,
eliminating reliance on manual annotations. 3) A novel Temporal Clustering loss
that groups in-distribution actions while distancing out-of-distribution
samples.
  We evaluate our framework on two challenging human-object interaction
recognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets.
Experimental results demonstrate significant improvements over state-of-the-art
action segmentation models across multiple open-set evaluation metrics,
achieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and
out-of-distribution detection performances (AUROC), respectively. Additionally,
we conduct an in-depth ablation study to assess the impact of each proposed
component, identifying the optimal framework configuration for open-world
action segmentation.

</details>


### [120] [OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection](https://arxiv.org/abs/2507.00789)
*Ziji Lu*

Main category: cs.CV

TL;DR: 提出OptiPrune框架，通过结合分布感知噪声优化和相似性token剪枝，显著提升文生图扩散模型在资源受限硬件上的语义对齐精度和推理效率。


<details>
  <summary>Details</summary>
Motivation: 文生图扩散模型在生成图像与文本提示之间实现精确语义对齐方面存在困难，同时难以在资源受限硬件上保持部署效率。现有方法要么导致大量计算开销，要么通过激进的token剪枝损害语义保真度。

Method: 本文提出OptiPrune，一个统一框架，同时解决语义对齐和效率挑战：
1.  **分布感知噪声优化模块**：引入受注意力分数引导的模块，将初始潜在噪声引导至语义有意义的区域，以缓解主体忽视和特征缠结问题，并保留高斯先验。
2.  **硬件高效token剪枝策略**：通过patch-wise相似性选择代表性基础token，注入随机性以增强泛化能力，并在注意力操作前使用最大相似度复制恢复剪枝token。

Result: 在包括Animal-Animal在内的基准数据集上进行的实验表明，OptiPrune在显著降低计算成本的同时，实现了最先进的提示-图像一致性（prompt-image consistency）。

Conclusion: OptiPrune成功解决了文生图扩散模型在语义对齐和计算效率之间的权衡问题，提供了一种高效且能保持高质量对齐的生成方案。

Abstract: Text-to-image diffusion models often struggle to achieve accurate semantic
alignment between generated images and text prompts while maintaining
efficiency for deployment on resource-constrained hardware. Existing approaches
either incur substantial computational overhead through noise optimization or
compromise semantic fidelity by aggressively pruning tokens. In this work, we
propose OptiPrune, a unified framework that combines distribution-aware initial
noise optimization with similarity-based token pruning to address both
challenges simultaneously. Specifically, (1) we introduce a distribution-aware
noise optimization module guided by attention scores to steer the initial
latent noise toward semantically meaningful regions, mitigating issues such as
subject neglect and feature entanglement; (2) we design a hardware-efficient
token pruning strategy that selects representative base tokens via patch-wise
similarity, injects randomness to enhance generalization, and recovers pruned
tokens using maximum similarity copying before attention operations. Our method
preserves the Gaussian prior during noise optimization and enables efficient
inference without sacrificing alignment quality. Experiments on benchmark
datasets, including Animal-Animal, demonstrate that OptiPrune achieves
state-of-the-art prompt-image consistency with significantly reduced
computational cost.

</details>


### [121] [LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling](https://arxiv.org/abs/2507.00790)
*Huaqiu Li,Yong Wang,Tongwen Huang,Hailang Huang,Haoqian Wang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 提出一种基于预训练潜在扩散模型的无数据集、统一图像恢复方法，通过循环后验采样实现。


<details>
  <summary>Details</summary>
Motivation: 现有统一图像恢复方法存在泛化性差（任务定制）和受限于闭集约束（依赖成对数据集）的问题。

Method: 提出一种新颖、无数据集、统一的图像恢复方法，通过利用预训练的潜在扩散模型进行循环后验采样。该方法结合多模态理解模型提供语义先验（任务无关），并使用轻量级模块对齐退化输入与扩散模型生成偏好，同时采用循环优化进行后验采样。

Result: 大量实验证明，该方法优于现有最先进的方法，验证了其有效性和鲁棒性。

Conclusion: 论文提出了一种新颖的、无数据集、统一的图像恢复方法，有效解决了现有方法的局限性，并在实验中表现出优异的性能和鲁棒性。

Abstract: Unified image restoration is a significantly challenging task in low-level
vision. Existing methods either make tailored designs for specific tasks,
limiting their generalizability across various types of degradation, or rely on
training with paired datasets, thereby suffering from closed-set constraints.
To address these issues, we propose a novel, dataset-free, and unified approach
through recurrent posterior sampling utilizing a pretrained latent diffusion
model. Our method incorporates the multimodal understanding model to provide
sematic priors for the generative model under a task-blind condition.
Furthermore, it utilizes a lightweight module to align the degraded input with
the generated preference of the diffusion model, and employs recurrent
refinement for posterior sampling. Extensive experiments demonstrate that our
method outperforms state-of-the-art methods, validating its effectiveness and
robustness. Our code and data will be available at
https://github.com/AMAP-ML/LD-RPS.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [122] [DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning](https://arxiv.org/abs/2507.00008)
*Hang Wu,Hongkai Chen,Yujun Cai,Chang Liu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: 本文提出DiMo-GUI，一个无需训练的GUI定位框架，通过模态分离和动态区域细化解决GUI中视觉和语言歧义，并在基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在图形用户界面（GUI）中定位自然语言查询面临独特挑战，包括视觉元素多样性、空间杂乱和语言模糊性。

Method: 引入DiMo-GUI框架，采用两种核心策略：1) 将GUI输入分解为文本和图标元素，使用通用视觉-语言模型对各模态独立推理；2) 当预测模糊或不正确时，动态生成候选焦点区域并分层细化（逐步放大子区域）以提高定位精度。该方法无需额外训练或标注。

Result: 在标准GUI定位基准测试中，DiMo-GUI比基线推理管道表现出持续的改进。

Conclusion: 结合模态分离与区域聚焦推理的方法，在GUI定位中显示出显著有效性。

Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses
unique challenges due to the diversity of visual elements, spatial clutter, and
the ambiguity of language. In this paper, we introduce DiMo-GUI, a
training-free framework for GUI grounding that leverages two core strategies:
dynamic visual grounding and modality-aware optimization. Instead of treating
the GUI as a monolithic image, our method splits the input into textual
elements and iconic elements, allowing the model to reason over each modality
independently using general-purpose vision-language models. When predictions
are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by
generating candidate focal regions centered on the model's initial predictions
and incrementally zooms into subregions to refine the grounding result. This
hierarchical refinement process helps disambiguate visually crowded layouts
without the need for additional training or annotations. We evaluate our
approach on standard GUI grounding benchmarks and demonstrate consistent
improvements over baseline inference pipelines, highlighting the effectiveness
of combining modality separation with region-focused reasoning.

</details>


### [123] [TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables](https://arxiv.org/abs/2507.00041)
*Varun Mannam,Fang Wang,Chaochun Liu,Xin Chen*

Main category: cs.AI

TL;DR: 本文提出TalentMine，一个基于LLM的框架，用于将人才管理系统中的表格数据转换为语义丰富的表示，解决了现有表格提取中语义信息丢失的问题，并在查询回答任务中实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 人才管理系统中关键信息多以复杂表格形式存在，传统语言模型难以有效检索；现有表格提取方法缺乏语义理解，导致下游查询失败，关键瓶颈在于表格结构信息可提取但语义关系丢失。

Method: 引入TalentMine框架，利用LLM将提取的表格转换为语义丰富表示。不同于传统CSV或文本线性化，采用多模态推理以保留表格数据的结构和语义维度。研究发现Claude v3 Haiku模型在人才管理应用中表现最佳。

Result: 在员工福利文档集上，TalentMine在查询回答任务中达到100%准确率，远超标准AWS Textract（0%）和AWS Textract Visual Q&A（40%）。对比分析显示Claude v3 Haiku模型性能最优。

Conclusion: 工作贡献包括：系统分析当前表格提取管线中的语义信息丢失、提出新颖的基于LLM的语义丰富表格表示方法、为检索增强系统提供高效集成框架，以及在人才分析任务上取得显著改进的全面基准测试。

Abstract: In talent management systems, critical information often resides in complex
tabular formats, presenting significant retrieval challenges for conventional
language models. These challenges are pronounced when processing Talent
documentation that requires precise interpretation of tabular relationships for
accurate information retrieval and downstream decision-making. Current table
extraction methods struggle with semantic understanding, resulting in poor
performance when integrated into retrieval-augmented chat applications. This
paper identifies a key bottleneck - while structural table information can be
extracted, the semantic relationships between tabular elements are lost,
causing downstream query failures. To address this, we introduce TalentMine, a
novel LLM-enhanced framework that transforms extracted tables into semantically
enriched representations. Unlike conventional approaches relying on CSV or text
linearization, our method employs specialized multimodal reasoning to preserve
both structural and semantic dimensions of tabular data. Experimental
evaluation across employee benefits document collections demonstrates
TalentMine's superior performance, achieving 100% accuracy in query answering
tasks compared to 0% for standard AWS Textract extraction and 40% for AWS
Textract Visual Q&A capabilities. Our comparative analysis also reveals that
the Claude v3 Haiku model achieves optimal performance for talent management
applications. The key contributions of this work include (1) a systematic
analysis of semantic information loss in current table extraction pipelines,
(2) a novel LLM-based method for semantically enriched table representation,
(3) an efficient integration framework for retrieval-augmented systems as
end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks
showing substantial improvements across multiple categories.

</details>


### [124] [A collaborative digital twin built on FAIR data and compute infrastructure](https://arxiv.org/abs/2507.00048)
*Thomas M. Deucher,Juan C. Verduzco,Michael Titus,Alejandro Strachan*

Main category: cs.AI

TL;DR: 本文提出一个基于nanoHUB的分布式自驱动实验室（SDL）框架，通过FAIR数据管理、预测性机器学习和序贯优化，加速协作式科学发现与优化，并通过食品染料颜色优化任务进行演示。


<details>
  <summary>Details</summary>
Motivation: 通过将机器学习与自驱动实验室（SDL）中的自动化实验相结合，以加速科学和工程应用中的发现和优化任务。同时，通过可查找、可访问、可互操作和可重用（FAIR）数据基础设施，促进具有共同兴趣的SDL之间更有效的协作。

Method: 开发了一个基于nanoHUB服务构建的分布式SDL实现，用于在线模拟和FAIR数据管理。该框架允许分散的协作者将原始实验数据贡献到共享的中央数据库。新数据通过简单的网络界面提交，并由nanoHUB Sim2L自动处理，提取导出量并索引到FAIR数据存储库ResultsDB中。独立的nanoHUB工作流利用主动学习实现序贯优化，机器学习模型会根据所有现有数据实时训练，以指导未来实验的选择。

Result: 成功实现了一个分布式SDL框架，该框架支持地理位置分散的协作者共享实验数据，并受益于自动更新的分析工具和机器学习模型。通过一个旨在找到最佳食品染料组合以实现目标颜色的优化任务，验证了该框架的有效性。所引入的工具具有通用性，可以轻松扩展到其他优化问题。

Conclusion: 该工作提出的分布式SDL框架，通过集成FAIR数据管理、预测性机器学习模型和序贯优化，极大地促进了协作式发现和优化。其工具具有通用性，可轻松应用于各种其他优化问题，从而促进可访问且高效的科学研究和教育。

Abstract: The integration of machine learning with automated experimentation in
self-driving laboratories (SDL) offers a powerful approach to accelerate
discovery and optimization tasks in science and engineering applications. When
supported by findable, accessible, interoperable, and reusable (FAIR) data
infrastructure, SDLs with overlapping interests can collaborate more
effectively. This work presents a distributed SDL implementation built on
nanoHUB services for online simulation and FAIR data management. In this
framework, geographically dispersed collaborators conducting independent
optimization tasks contribute raw experimental data to a shared central
database. These researchers can then benefit from analysis tools and machine
learning models that automatically update as additional data become available.
New data points are submitted through a simple web interface and automatically
processed using a nanoHUB Sim2L, which extracts derived quantities and indexes
all inputs and outputs in a FAIR data repository called ResultsDB. A separate
nanoHUB workflow enables sequential optimization using active learning, where
researchers define the optimization objective, and machine learning models are
trained on-the-fly with all existing data, guiding the selection of future
experiments. Inspired by the concept of ``frugal twin", the optimization task
seeks to find the optimal recipe to combine food dyes to achieve the desired
target color. With easily accessible and inexpensive materials, researchers and
students can set up their own experiments, share data with collaborators, and
explore the combination of FAIR data, predictive ML models, and sequential
optimization. The tools introduced are generally applicable and can easily be
extended to other optimization problems.

</details>


### [125] [SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network](https://arxiv.org/abs/2507.00050)
*Devin Y. De Silva,Sandareka Wickramanayake,Dulani Meedeniya,Sanka Rasnayaka*

Main category: cs.AI

TL;DR: SEZ-HARN是一种新型可解释零样本人体活动识别模型，它使用IMU数据识别未知活动并提供骨骼视频解释，同时保持竞争力性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于IMU的人体活动识别(HAR)数据集不全面，且模型缺乏透明度。零样本HAR(ZS-HAR)虽能解决数据限制，但当前模型缺乏解释性，限制了其在实际应用中的推广。

Method: 提出了一种名为“自解释零样本人体活动识别网络”(SEZ-HARN)的新型IMU基零样本HAR模型。该模型能够识别训练中未遇到的活动，并通过生成骨骼视频来解释其决策过程。

Result: SEZ-HARN能生成真实且易于理解的解释，同时在零样本识别精度方面具有竞争力。在PAMAP2数据集上，其零样本预测精度与最佳黑盒模型相差不到3%，在其他三个数据集上表现相当。

Conclusion: SEZ-HARN成功解决了零样本HAR模型的透明度问题，同时保持了高识别性能，为实际应用提供了可解释的解决方案。

Abstract: Human Activity Recognition (HAR), which uses data from Inertial Measurement
Unit (IMU) sensors, has many practical applications in healthcare and assisted
living environments. However, its use in real-world scenarios has been limited
by the lack of comprehensive IMU-based HAR datasets that cover a wide range of
activities and the lack of transparency in existing HAR models. Zero-shot HAR
(ZS-HAR) overcomes the data limitations, but current models struggle to explain
their decisions, making them less transparent. This paper introduces a novel
IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity
Recognition Network (SEZ-HARN). It can recognize activities not encountered
during training and provide skeleton videos to explain its decision-making
process. We evaluate the effectiveness of the proposed SEZ-HARN on four
benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its
performance against three state-of-the-art black-box ZS-HAR models. The
experiment results demonstrate that SEZ-HARN produces realistic and
understandable explanations while achieving competitive Zero-shot recognition
accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\% of the
best-performing black-box model on PAMAP2 while maintaining comparable
performance on the other three datasets.

</details>


### [126] [Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation](https://arxiv.org/abs/2507.00054)
*Shreyansh Padarha*

Main category: cs.AI

TL;DR: 本研究提出AdvDistill，一种奖励引导的数据集蒸馏框架，通过为教师模型响应分配奖励权重，显著提升了小型语言模型在数学和复杂推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的知识蒸馏方法通常仅复制教师模型在分布内的响应，导致小型语言模型（SLMs）的泛化能力受限，尤其在推理任务上表现更差，且计算成本高昂。

Method: 本文提出AdvDistill，一个奖励引导的数据集蒸馏框架。该框架为每个提示利用教师模型的多个生成响应，并基于规则验证器为这些响应分配奖励。这些变化且呈正态分布的奖励作为训练学生模型时的权重。

Result: 研究结果表明，学生模型在数学和复杂推理任务上的性能取得了显著提升。

Conclusion: 在数据集蒸馏过程中引入奖励机制是有效且有益的，能够显著提升小型语言模型在复杂推理任务上的性能和泛化能力。

Abstract: The push to compress and impart the proficiency of Large Language Models
(LLMs) into more deployable and efficient Small Language Models (SLMs) has
benefited from improvements in knowledge distillation (KD) techniques. These
techniques allow a smaller student model to learn from a more capable and
larger teacher model's responses. However, distillation often revolves around
the student model merely copying the teacher's in-distribution responses,
limiting its generalisability. This limitation is amplified on reasoning tasks
and can be computationally expensive. In this study, we propose AdvDistill, a
reward-guided dataset distillation framework. We utilise multiple generations
(responses) from a teacher for each prompt and assign rewards based on
rule-based verifiers. These varying and normally distributed rewards serve as
weights when training student models. Our methods and their subsequent
behavioural analysis demonstrate a significant improvement in student model
performance for mathematical and complex reasoning tasks, showcasing the
efficacy and benefits of incorporating a rewarding mechanism in dataset
distillation processes.

</details>


### [127] [VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems](https://arxiv.org/abs/2507.00079)
*Ethan Smyth,Alessandro Suglia*

Main category: cs.AI

TL;DR: 本文提出了VoyagerVision，一个利用视觉反馈（截图）增强大型语言模型空间解释能力的多模态模型，并在Minecraft中成功创建了独特结构，扩展了模型的开放性潜力。


<details>
  <summary>Details</summary>
Motivation: 为了追求更具能力的通用人工智能（AGI），开放性是一个活跃的研究领域。鉴于大型语言模型（LLM）在解释图像输入方面的最新进展，研究者提出通过提供视觉输入，可以提高模型解释空间环境的能力，从而增加其成功执行任务的数量，并扩展其开放性潜力。

Method: 本文提出并构建了VoyagerVision模型，这是一个基于Voyager的多模态模型。该模型通过使用Minecraft游戏内的屏幕截图作为视觉反馈，使其能够创建游戏内的结构。

Result: 在五十次迭代中，VoyagerVision平均创建了2.75个独特的结构，而Voyager无法实现此目标。此外，在建筑单元测试中，VoyagerVision在平面世界中一半的尝试中取得了成功，大多数失败发生在更复杂的结构中。

Conclusion: VoyagerVision通过整合视觉输入，显著提升了模型在Minecraft等空间环境中的解释和构建能力，成功扩展了Voyager模型的开放性潜力，开辟了一个全新的研究方向。

Abstract: Open-endedness is an active field of research in the pursuit of capable
Artificial General Intelligence (AGI), allowing models to pursue tasks of their
own choosing. Simultaneously, recent advancements in Large Language Models
(LLMs) such as GPT-4o [9] have allowed such models to be capable of
interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use
of such features, providing an LLM with pixel data of an agent's POV to parse
the environment and allow it to solve tasks. This paper proposes that providing
these visual inputs to a model gives it greater ability to interpret spatial
environments, and as such, can increase the number of tasks it can successfully
perform, extending its open-ended potential. To this aim, this paper proposes
VoyagerVision -- a multi-modal model capable of creating structures within
Minecraft using screenshots as a form of visual feedback, building on the
foundation of Voyager. VoyagerVision was capable of creating an average of 2.75
unique structures within fifty iterations of the system, as Voyager was
incapable of this, it is an extension in an entirely new direction.
Additionally, in a set of building unit tests VoyagerVision was successful in
half of all attempts in flat worlds, with most failures arising in more complex
structures. Project website is available at
https://esmyth-dev.github.io/VoyagerVision.github.io/

</details>


### [128] [Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models](https://arxiv.org/abs/2507.00092)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Zhang Yuting,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: 本研究引入“逆向推理”范式，使大型语言模型（LLMs）能够事后分解并解释其推理链，显著提升了决策过程的透明度和可解释性。SAGE-nano模型在此范式下，在推理准确性和解释质量方面表现出色，性能接近顶尖模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在复杂推理任务上表现强大，但其决策过程仍像黑箱，难以理解模型为何选择特定推理路径，这限制了AI的可解释性和安全性。

Method: 引入“逆向推理”这一新范式，通过元认知结构让LLMs利用注意力机制反思并识别主要决策点，从而事后生成推理选择的解释。具体地，SAGE-nano模型采用了一种新颖的元学习框架来反转注意力流，以实现对自身推理过程的自解释。

Result: SAGE-nano（一个40亿参数的模型）在逻辑推理、数学和伦理困境测试中，其推理准确率（AQUA-RAT上74.6%）和解释质量（92.1%的人工偏好得分）均达到前沿水平。其性能几乎与Claude-3.5 Sonnet或GPT-4o等模型持平，并证明逆向推理能够同时提升可解释性和推理性能。

Conclusion: 该工作为LLM自反思提供了一个严格框架，并通过逆向推理提升了模型的透明度和可解释性，为构建透明AI系统开辟了新途径，并弥补了AI安全、教育和科学发现领域的重大空白。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities at
solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but
their decision-making processes remain somewhat blackbox. We introduce
textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and
explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a
4-billion-parameter reasoning model, employs a metacognitive structure that
reflects back via attention processes to identify major decision points and
generate explanations of reasoning choices. While typical CoT approaches are
directed towards forward reasoning generation, inverse reasoning provides
insight into why specific reasoning chains were selected over others. Through
thorough testing of logical reasoning puzzles, math problems and ethical
dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we
demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy
(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for
its task, and offers performance almost on par with models like Claude-3.5
Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for
LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework
to reverse the attention flow, (iii) comprehensive evaluation frameworks for
reasoning transparency, and (iv) evidence that increasing reasoning using
inverse reasoning improves interpretability along with reasoning performance.
Our work creates new avenues for transparent AI systems and closes significant
gaps in AI safety, education, and scientific discovery.

</details>


### [129] [BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis](https://arxiv.org/abs/2507.00180)
*Vidhi Rathore*

Main category: cs.AI

TL;DR: 本文提出一个新颖的流水线，利用强化学习代理探索黑盒遗留系统的输入空间，识别关键决策边界，并通过聚类和决策树提取可解释的决策逻辑。


<details>
  <summary>Details</summary>
Motivation: 遗留系统现代化改造面临文档缺失和对复杂决策逻辑理解不足的挑战。传统行为克隆仅复制输入输出，无法捕获底层意图，因此需要一种方法来自动提取可解释的决策逻辑。

Method: 该方法将遗留系统视为黑盒。利用强化学习（RL）代理探索输入空间，通过奖励导致系统输出发生有意义变化的动作来识别关键决策边界。收集这些输出变化的“反事实状态转换”，使用K-Means聚类，然后在此基础上训练决策树，以提取近似系统决策逻辑的人类可读规则。

Result: 该流水线在三种不同复杂度的模拟遗留系统（基于阈值、组合条件和非线性范围逻辑）上进行了验证。结果表明，RL代理成功将探索重点放在相关边界区域，并且提取的规则准确反映了底层模拟系统的核心逻辑。

Conclusion: 该方法为在遗留系统迁移过程中生成规范和测试用例提供了有希望的基础，因为它能从黑盒系统中提取准确且可解释的决策逻辑。

Abstract: Modernizing legacy software systems is a critical but challenging task, often
hampered by a lack of documentation and understanding of the original system's
intricate decision logic. Traditional approaches like behavioral cloning merely
replicate input-output behavior without capturing the underlying intent. This
paper proposes a novel pipeline to automatically extract interpretable decision
logic from legacy systems treated as black boxes. The approach uses a
Reinforcement Learning (RL) agent to explore the input space and identify
critical decision boundaries by rewarding actions that cause meaningful changes
in the system's output. These counterfactual state transitions, where the
output changes, are collected and clustered using K-Means. Decision trees are
then trained on these clusters to extract human-readable rules that approximate
the system's decision logic near the identified boundaries. I demonstrated the
pipeline's effectiveness on three dummy legacy systems with varying complexity,
including threshold-based, combined-conditional, and non-linear range logic.
Results show that the RL agent successfully focuses exploration on relevant
boundary regions, and the extracted rules accurately reflect the core logic of
the underlying dummy systems, providing a promising foundation for generating
specifications and test cases during legacy migration.

</details>


### [130] [ChatGPT produces more "lazy" thinkers: Evidence of cognitive engagement decline](https://arxiv.org/abs/2507.00181)
*Georgios P. Georgiou*

Main category: cs.AI

TL;DR: 研究发现，在学术写作中，使用ChatGPT的学生比未使用者认知投入显著降低，可能导致认知卸载。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在教育中日益普及，但担忧它们可能减少深度思考和主动学习。本研究旨在探讨生成式AI工具（特别是ChatGPT）对学生学术写作认知投入的影响。

Method: 采用实验设计，参与者随机分为AI辅助组（使用ChatGPT）和非辅助组（对照组）。两组均完成结构化议论文写作任务，随后填写专门开发的认知投入量表（CES-AI），以评估心理努力、注意力、深度加工和策略性思维。

Result: 结果显示，ChatGPT组的认知投入分数显著低于对照组，这表明AI辅助可能导致认知卸载。

Conclusion: AI辅助可能损害学生的自我调节学习和深度认知参与。研究建议采取教学策略，促进学生主动、反思性地与AI生成内容互动，以避免负面影响。

Abstract: Despite the increasing use of large language models (LLMs) in education,
concerns have emerged about their potential to reduce deep thinking and active
learning. This study investigates the impact of generative artificial
intelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of
students during academic writing tasks. The study employed an experimental
design with participants randomly assigned to either an AI-assisted (ChatGPT)
or a non-assisted (control) condition. Participants completed a structured
argumentative writing task followed by a cognitive engagement scale (CES), the
CES-AI, developed to assess mental effort, attention, deep processing, and
strategic thinking. The results revealed significantly lower cognitive
engagement scores in the ChatGPT group compared to the control group. These
findings suggest that AI assistance may lead to cognitive offloading. The study
contributes to the growing body of literature on the psychological implications
of AI in education and raises important questions about the integration of such
tools into academic practice. It calls for pedagogical strategies that promote
active, reflective engagement with AI-generated content to avoid compromising
self-regulated learning and deep cognitive involvement of students.

</details>


### [131] [Holistic Artificial Intelligence in Medicine; improved performance and explainability](https://arxiv.org/abs/2507.00205)
*Periklis Petridis,Georgios Margaritis,Vasiliki Stoumpou,Dimitris Bertsimas*

Main category: cs.AI

TL;DR: xHAIM是一个利用生成式AI来提升医疗AI预测准确性和可解释性的框架，通过多模态数据处理和患者摘要生成，显著提高了预测性能，并能提供可追溯的临床解释。


<details>
  <summary>Details</summary>
Motivation: 现有的HAIM框架在医疗AI应用中存在数据处理任务无关性和缺乏可解释性的局限，阻碍了其在临床的实际部署和信任。

Method: 引入xHAIM框架，该框架利用生成式AI，通过四个步骤实现：1) 自动识别跨模态任务相关患者数据；2) 生成全面的患者摘要；3) 利用这些摘要改进预测模型；4) 通过将预测与患者特异性医学知识联系起来，提供临床解释。

Result: 在HAIM-MIMIC-MM数据集上评估，xHAIM在胸部病理学和手术任务中的平均AUC从79.9%提高到90.3%。

Conclusion: xHAIM将AI从一个黑盒预测器转变为一个可解释的决策支持系统，使临床医生能够交互式地将预测追溯到相关的患者数据，从而将AI进展与临床实用性结合起来。

Abstract: With the increasing interest in deploying Artificial Intelligence in
medicine, we previously introduced HAIM (Holistic AI in Medicine), a framework
that fuses multimodal data to solve downstream clinical tasks. However, HAIM
uses data in a task-agnostic manner and lacks explainability. To address these
limitations, we introduce xHAIM (Explainable HAIM), a novel framework
leveraging Generative AI to enhance both prediction and explainability through
four structured steps: (1) automatically identifying task-relevant patient data
across modalities, (2) generating comprehensive patient summaries, (3) using
these summaries for improved predictive modeling, and (4) providing clinical
explanations by linking predictions to patient-specific medical knowledge.
Evaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%
to 90.3% across chest pathology and operative tasks. Importantly, xHAIM
transforms AI from a black-box predictor into an explainable decision support
system, enabling clinicians to interactively trace predictions back to relevant
patient data, bridging AI advancements with clinical utility.

</details>


### [132] [Learning for routing: A guided review of recent developments and future directions](https://arxiv.org/abs/2507.00218)
*Fangting Zhou,Attila Lischka,Balazs Kulcsar,Jiaming Wu,Morteza Haghir Chehreghani,Gilbert Laporte*

Main category: cs.AI

TL;DR: 本文综述了机器学习在解决NP-难组合优化问题（特别是旅行商问题和车辆路径问题）方面的应用进展，并提出了基于ML的路由方法分类法。


<details>
  <summary>Details</summary>
Motivation: 针对NP-难路径问题，传统精确算法计算耗时过长，启发式算法无法保证最优性。鉴于机器学习模型的成功，有必要探索其在解决这些复杂问题中的潜力。

Method: 本文通过文献综述，分析了机器学习在路由问题中的应用现状。提出了一种分类法，将基于ML的路由方法分为构建式和改进式两种方法。

Result: 本综述整合了传统运筹学方法与最先进的机器学习技术，提供了一个结构化的框架，用于理解和分类基于ML的路由问题解决方案。

Conclusion: 该结构化框架旨在指导未来在ML-based路由问题（包括新兴的VRP变体）方面的研究。

Abstract: This paper reviews the current progress in applying machine learning (ML)
tools to solve NP-hard combinatorial optimization problems, with a focus on
routing problems such as the traveling salesman problem (TSP) and the vehicle
routing problem (VRP). Due to the inherent complexity of these problems, exact
algorithms often require excessive computational time to find optimal
solutions, while heuristics can only provide approximate solutions without
guaranteeing optimality. With the recent success of machine learning models,
there is a growing trend in proposing and implementing diverse ML techniques to
enhance the resolution of these challenging routing problems. We propose a
taxonomy categorizing ML-based routing methods into construction-based and
improvement-based approaches, highlighting their applicability to various
problem characteristics. This review aims to integrate traditional OR methods
with state-of-the-art ML techniques, providing a structured framework to guide
future research and address emerging VRP variants.

</details>


### [133] [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://arxiv.org/abs/2507.00417)
*Joongwon Kim,Anirudh Goyal,Liang Tan,Hannaneh Hajishirzi,Srinivasan Iyer,Tianlu Wang*

Main category: cs.AI

TL;DR: ASTRO是一种新的框架，通过蒙特卡洛树搜索（MCTS）派生的合成数据集，将搜索算法的推理能力（包括自我反思、回溯和探索）教授给大型语言模型（如Llama 3），并通过强化学习进一步优化，在数学问题解决基准上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的通过强化学习训练的开源推理模型在RL之前已经表现出强大的推理能力和搜索行为，因此尚不清楚如何提升其他非推理模型（如Llama 3）的推理能力。本研究旨在为开放式LLM注入强大的推理能力。

Method: 引入ASTRO框架，该框架通过将蒙特卡洛树搜索（MCTS）在数学问题解决轨迹上的搜索痕迹转换为自然语言思维链（包含成功和失败恢复），构建了一个合成数据集。利用此数据集对模型进行微调，并进一步通过可验证奖励的强化学习来提升性能。ASTRO使模型内化结构化搜索行为，并在RL过程中为探索提供丰富的先验知识。

Result: 将ASTRO应用于Llama 3系列模型，在MATH-500上实现了16.0%的绝对性能提升，在AMC 2023上提升26.9%，在AIME 2024上提升20.0%。特别是在需要迭代修正的挑战性问题上表现出显著改进。

Conclusion: 研究结果表明，受搜索启发的训练为向开放式大型语言模型注入强大的推理能力提供了一种原则性的方法。

Abstract: We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework
for training language models to reason like search algorithms, explicitly
leveraging self-reflection, backtracking, and exploration in their outputs.
Recently, training large language models (LLMs) via reinforcement learning (RL)
has led to the advent of reasoning models with greatly enhanced reasoning
capabilities. Open-source replications of reasoning models, while successful,
build upon models that already exhibit strong reasoning capabilities along with
search behavior observed even before RL. As a result, it is yet unclear how to
boost the reasoning capabilities of other non-reasoner models including Llama
3. ASTRO teaches such models to internalize structured search behavior through
a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over
mathematical problem-solving trajectories. By converting search traces into
natural language chain-of-thoughts that capture both successes and recoveries
from failure, ASTRO bootstraps models with a rich prior for exploration during
RL. We finetune our models on these search-derived traces and further improve
performance via RL with verifiable rewards. We apply ASTRO to the Llama 3
family of models and achieve absolute performance gains of 16.0% on MATH-500,
26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon
challenging problems that require iterative correction. Our results demonstrate
that search-inspired training offers a principled way to instill robust
reasoning capabilities into open LLMs.

</details>


### [134] [Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
*Maggie Huan,Yuetai Li,Tuney Zheng,Xiaoyu Xu,Seungone Kim,Minxin Du,Radha Poovendran,Graham Neubig,Xiang Yue*

Main category: cs.AI

TL;DR: 研究发现，大语言模型在数学推理上的进步多数难以泛化到其他领域。受控实验表明，RL微调能保持通用能力，而SFT微调会导致能力遗忘和表征漂移。这提示需重新审视LLM的后训练策略。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在数学推理基准上超越人类，研究者质疑这些进步是反映了广泛的问题解决能力，还是仅为狭隘的过拟合，并希望探究其泛化性。

Method: 评估了20多个开源推理优化模型在多种任务（数学、科学问答、规划、编码等）上的表现。在Qwen3-14B模型上进行受控实验，使用纯数学数据并对比不同微调方法（SFT与RL）。通过潜在空间表征和token空间分布漂移分析其内在机制。

Result: 大多数在数学上成功的模型未能将其能力泛化到其他领域。RL微调的模型泛化良好，而SFT微调的模型常遗忘通用能力。分析显示SFT导致显著的表征和输出漂移，而RL则保留了通用领域结构。

Conclusion: 本研究结果表明，需要重新思考大语言模型的标准后训练方法，特别是对SFT蒸馏数据的依赖，以促进推理模型的全面发展。

Abstract: Math reasoning has become the poster child of progress in large language
models (LLMs), with new models rapidly surpassing human-level performance on
benchmarks like MATH and AIME. But as math leaderboards improve week by week,
it is worth asking: do these gains reflect broader problem-solving ability or
just narrow overfitting? To answer this question, we evaluate over 20
open-weight reasoning-tuned models across a broad suite of tasks, including
math, scientific QA, agent planning, coding, and standard
instruction-following. We surprisingly find that most models that succeed in
math fail to transfer their gains to other domains. To rigorously study this
phenomenon, we conduct controlled experiments on Qwen3-14B models using
math-only data but different tuning methods. We find that reinforcement
learning (RL)-tuned models generalize well across domains, while supervised
fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space
representation and token-space distribution shift analyses reveal that SFT
induces substantial representation and output drift, while RL preserves
general-domain structure. Our results suggest a need to rethink standard
post-training recipes, particularly the reliance on SFT-distilled data for
advancing reasoning models.

</details>


### [135] [Advancing Local Search in SMT-NRA with MCSAT Integration](https://arxiv.org/abs/2507.00557)
*Tianyi Ding,Haokun Li,Xinpeng Ni,Bican Xia,Tianqi Zhao*

Main category: cs.AI

TL;DR: 本文通过引入泛化的二维单元跳跃、提出扩展的局部搜索框架并与MCSAT结合、优化MCSAT以及设计混合框架，显著提升了非线性实数算术的可满足性模理论（SMT-NRA）的局部搜索性能。


<details>
  <summary>Details</summary>
Motivation: 旨在改进SMT-NRA的局部搜索方法，特别是通过泛化现有关键操作（单元跳跃）和提高搜索效率。

Method: ['引入二维单元跳跃（2d-cell-jump）操作，泛化了SMT-NRA局部搜索方法中的关键操作“单元跳跃”。', '提出了扩展的局部搜索框架（2d-LS），并将其与模型构建可满足性演算（MCSAT）框架集成，以提高搜索效率。', '为进一步提升MCSAT效率，实现了“样本单元投影算子”技术，该技术适用于实数域的CDCL式搜索，并有助于引导搜索远离冲突状态。', '设计了一个结合MCSAT、2d-LS和OpenCAD的SMT-NRA混合框架，通过信息交换来提高搜索效率。']

Result: 实验结果表明，所提出的方法显著提升了局部搜索性能。

Conclusion: 所提出的方法（包括2d-cell-jump、2d-LS框架、MCSAT优化以及混合框架）对提高SMT-NRA的局部搜索效率是有效的。

Abstract: In this paper, we advance local search for Satisfiability Modulo the Theory
of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a
two-dimensional cell-jump move, called \emph{$2d$-cell-jump}, generalizing the
key operation, cell-jump, of the local search method for SMT-NRA. Then, we
propose an extended local search framework, named \emph{$2d$-LS} (following the
local search framework, LS, for SMT-NRA), integrating the model constructing
satisfiability calculus (MCSAT) framework to improve search efficiency. To
further improve the efficiency of MCSAT, we implement a recently proposed
technique called \emph{sample-cell projection operator} for MCSAT, which is
well suited for CDCL-style search in the real domain and helps guide the search
away from conflicting states. Finally, we design a hybrid framework for SMT-NRA
combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through
information exchange. The experimental results demonstrate improvements in
local search performance, highlighting the effectiveness of the proposed
methods.

</details>


### [136] [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://arxiv.org/abs/2507.00726)
*Dongyoon Hwang,Hojoon Lee,Jaegul Choo,Dongmin Park,Jongho Park*

Main category: cs.AI

TL;DR: 本研究探索了LLM使用RL在国际象棋中进行战略推理的能力。通过知识蒸馏提供密集奖励，发现虽然优于稀疏奖励，但模型仍远低于专家水平。分析表明，主要限制在于预训练模型对国际象棋内部理解的不足，而RL可能无法完全克服。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）在大型语言模型（LLM）的数学推理中展现出潜力，但LLM的战略推理能力（特别是通过RL）仍未被充分探索。本研究旨在探讨LLM是否能通过RL发展在国际象棋中的战略推理能力。

Method: 利用一个预训练的国际象棋动作-价值网络，为LLM的输出走法质量提供密集奖励，这被视为一种知识蒸馏形式。此外，还进行了监督微调（SFT）和RL的消融实验，以分析国际象棋推理训练中的性能限制。

Result: 实验结果显示，基于知识蒸馏的密集奖励通常优于稀疏二进制奖励。然而，所有模型都远低于专家水平并达到性能瓶颈。进一步的消融实验证据表明，这一限制源于预训练模型自身对国际象棋内部理解的不足。

Conclusion: LLM通过RL学习国际象棋战略推理的能力受到其预训练模型对国际象棋内部理解不足的限制，仅靠RL可能无法完全克服这一根本性缺陷。尽管密集奖励有所帮助，但不足以弥补这一核心障碍。

Abstract: While reinforcement learning (RL) for large language models (LLMs) has shown
promise in mathematical reasoning, strategic reasoning for LLMs using RL
remains largely unexplored. We investigate whether LLMs can develop strategic
reasoning capabilities through RL in chess. To this end, we leverage a
chess-pretrained action-value network to provide dense reward on the LLM's
output move quality, which can be seen as a form of knowledge distillation. Our
experiments show that our distillation-based dense rewards often outperform
sparse binary rewards. However, surprisingly, all models plateau far below
expert levels. We provide SFT and RL ablations on chess reasoning training and
find evidence that this limitation stems from a deficit in the pretrained
models' internal understanding of chess--a deficit which RL alone may not be
able to fully overcome.

</details>


### [137] [A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis](https://arxiv.org/abs/2507.00810)
*Qing Xu,Xiaohua Xuan*

Main category: cs.AI

TL;DR: 本文提出并证明了一种基于非光滑优化、二次规划和迭代过程的改进数值算法，用于解决minimax问题。


<details>
  <summary>Details</summary>
Motivation: 解决minimax问题，并提供一种改进的数值算法。

Method: 提出一种改进的数值算法，该算法基于非光滑优化、二次规划和迭代过程。

Result: 在梯度连续性和有界性等温和假设下，为所提出的算法提供了严格的收敛性证明。

Conclusion: 该算法具有广泛的应用前景，可用于鲁棒优化、不平衡学习等多个领域。

Abstract: In this paper, we propose an improved numerical algorithm for solving minimax
problems based on nonsmooth optimization, quadratic programming and iterative
process. We also provide a rigorous proof of convergence for our algorithm
under some mild assumptions, such as gradient continuity and boundedness. Such
an algorithm can be widely applied in various fields such as robust
optimization, imbalanced learning, etc.

</details>


### [138] [SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents](https://arxiv.org/abs/2507.00841)
*Siyuan Liang,Tianmeng Fang,Zhe Liu,Aishan Liu,Yan Xiao,Jinyuan He,Ee-Chien Chang,Xiaochun Cao*

Main category: cs.AI

TL;DR: 本研究探讨了多模态智能代理系统面临的越狱风险，并提出了一种结合行为序列信息和大语言模型的自动化风险识别与评估方案，旨在提高风险行为识别能力，降低代理被越狱的概率。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型在智能代理中的广泛应用，代理系统面临越狱风险，攻击者可能诱导其执行敏感操作。现有安全措施在检测复杂多轮风险行为时存在局限性，且缺乏高效一致的自动化评估方法。

Method: 本工作探索了移动多模态代理的安全问题，构建了融合行为序列信息的风险判别机制，并设计了基于大语言模型的自动化辅助评估方案。

Result: 在多项高风险任务中初步验证表明，该方法能在一定程度上提高对风险行为的识别能力，并有助于降低代理被越狱的概率。

Conclusion: 本研究旨在为多模态智能代理系统的安全风险建模与防护提供有价值的参考。

Abstract: With the wide application of multimodal foundation models in intelligent
agent systems, scenarios such as mobile device control, intelligent assistant
interaction, and multimodal task execution are gradually relying on such large
model-driven agents. However, the related systems are also increasingly exposed
to potential jailbreak risks. Attackers may induce the agents to bypass the
original behavioral constraints through specific inputs, and then trigger
certain risky and sensitive operations, such as modifying settings, executing
unauthorized commands, or impersonating user identities, which brings new
challenges to system security. Existing security measures for intelligent
agents still have limitations when facing complex interactions, especially in
detecting potentially risky behaviors across multiple rounds of conversations
or sequences of tasks. In addition, an efficient and consistent automated
methodology to assist in assessing and determining the impact of such risks is
currently lacking. This work explores the security issues surrounding mobile
multimodal agents, attempts to construct a risk discrimination mechanism by
incorporating behavioral sequence information, and designs an automated
assisted assessment scheme based on a large language model. Through preliminary
validation in several representative high-risk tasks, the results show that the
method can improve the recognition of risky behaviors to some extent and assist
in reducing the probability of agents being jailbroken. We hope that this study
can provide some valuable references for the security risk modeling and
protection of multimodal intelligent agent systems.

</details>


### [139] [Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact](https://arxiv.org/abs/2507.00951)
*Rizwan Qureshi,Ranjan Sapkota,Abbas Shah,Amgad Muneer,Anas Zafar,Ashmal Vayani,Maged Shoman,Abdelrahman B. M. Eldaly,Kai Zhang,Ferhat Sadak,Shaina Raza,Xinqi Fan,Ravid Shwartz-Ziv,Hong Yan,Vinjia Jain,Aman Chadha,Manoj Karkee,Jia Wu,Philip Torr,Seyedali Mirjalili*

Main category: cs.AI

TL;DR: 本文对通用人工智能（AGI）的发展进行了跨学科综合分析，指出当前AI模型受限于令牌预测和缺乏能动性。文章强调通过智能体RAG框架、泛化策略以及记忆与推理的深度整合来迈向更具适应性的AGI，并探讨了相关挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管现有模型（如GPT-4.5、Claude 3.5等）能力日益增强，但它们在模仿人类思维、推理和行动方面仍存在根本性局限，主要体现在依赖令牌预测和缺乏具体能动性。这种局限性促使了对通用人工智能（AGI）持续的探索和研究。

Method: 本文采用跨学科综合分析方法，涵盖人工智能、认知神经科学、心理学、生成模型和基于智能体的系统。具体方法包括：分析通用智能的架构和认知基础；强调智能体RAG框架（结合检索、规划和动态工具使用）的作用；讨论泛化策略（包括信息压缩、测试时适应和免训练方法）；重新审视视觉-语言模型（VLMs）作为具身理解和协作的接口；论证真正智能源于记忆与推理的整合；探讨神经符号系统、强化学习和认知支架如何弥合统计学习与目标导向认知之间的鸿沟。

Result: 研究结果表明：当前主流AI模型在实现类人通用智能方面存在根本性限制。论文提出，真正的智能并非仅来自规模，而是源于记忆与推理的整合，即模块化、交互式和自改进组件的协同作用，其中压缩能力促进适应性行为。分析指出，新兴架构已开始弥合统计学习与目标导向认知之间的鸿沟。此外，本文识别了AGI发展路径上的关键科学、技术和伦理挑战。

Conclusion: 要实现通用人工智能（AGI），需要超越单纯的规模扩展，转而专注于整合记忆与推理，构建模块化、交互式和自改进的系统，并利用智能体RAG等框架和多种泛化策略来实现更具适应性的行为。AGI的最终实现仍面临重大的科学、技术和伦理挑战，需要多学科的持续努力和探索。

Abstract: Can machines truly think, reason and act in domains like humans? This
enduring question continues to shape the pursuit of Artificial General
Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,
DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal
fluency and partial reasoning, these systems remain fundamentally limited by
their reliance on token-level prediction and lack of grounded agency. This
paper offers a cross-disciplinary synthesis of AGI development, spanning
artificial intelligence, cognitive neuroscience, psychology, generative models,
and agent-based systems. We analyze the architectural and cognitive foundations
of general intelligence, highlighting the role of modular reasoning, persistent
memory, and multi-agent coordination. In particular, we emphasize the rise of
Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use
to enable more adaptive behavior. We discuss generalization strategies,
including information compression, test-time adaptation, and training-free
methods, as critical pathways toward flexible, domain-agnostic intelligence.
Vision-Language Models (VLMs) are reexamined not just as perception modules but
as evolving interfaces for embodied understanding and collaborative task
completion. We also argue that true intelligence arises not from scale alone
but from the integration of memory and reasoning: an orchestration of modular,
interactive, and self-improving components where compression enables adaptive
behavior. Drawing on advances in neurosymbolic systems, reinforcement learning,
and cognitive scaffolding, we explore how recent architectures begin to bridge
the gap between statistical learning and goal-directed cognition. Finally, we
identify key scientific, technical, and ethical challenges on the path to AGI.

</details>


### [140] [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979)
*Dongyoon Hahm,Woogyeol Jin,June Suk Choi,Sungsoo Ahn,Kimin Lee*

Main category: cs.AI

TL;DR: 该研究提出CIP，一种利用因果影响图(CID)的新技术，旨在增强LLM驱动的自主智能体的安全性，通过识别和减轻决策风险，并在代码执行和移动设备控制任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）驱动的自主智能体在各种辅助任务中展现巨大潜力，但确保其安全可靠的行为以防止意外后果至关重要。

Method: 引入名为CIP的新技术，该技术利用因果影响图（CIDs）来识别和减轻智能体决策带来的风险。该方法包括三个关键步骤：1) 基于任务规范初始化CID以描绘决策过程；2) 使用CID引导智能体与环境交互；3) 基于观察到的行为和结果迭代优化CID。

Result: 实验结果表明，所提出的方法有效提升了代码执行和移动设备控制任务的安全性。

Conclusion: 通过利用因果影响图（CIDs），CIP为LLM驱动的智能体提供了一种结构化的风险识别和缓解机制，显著增强了其在实际应用中的安全性。

Abstract: As autonomous agents powered by large language models (LLMs) continue to
demonstrate potential across various assistive tasks, ensuring their safe and
reliable behavior is crucial for preventing unintended consequences. In this
work, we introduce CIP, a novel technique that leverages causal influence
diagrams (CIDs) to identify and mitigate risks arising from agent
decision-making. CIDs provide a structured representation of cause-and-effect
relationships, enabling agents to anticipate harmful outcomes and make safer
decisions. Our approach consists of three key steps: (1) initializing a CID
based on task specifications to outline the decision-making process, (2)
guiding agent interactions with the environment using the CID, and (3)
iteratively refining the CID based on observed behaviors and outcomes.
Experimental results demonstrate that our method effectively enhances safety in
both code execution and mobile device control tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [141] [Hypertokens: Holographic Associative Memory in Tokenized LLMs](https://arxiv.org/abs/2507.00002)
*Christopher James Augeri*

Main category: cs.LG

TL;DR: 提出HDRAM，一个结合纠错码、全息计算和量子启发搜索的符号记忆框架，用于解决大型语言模型中的信息扩散问题，显著改善联想检索能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在明显的精度损失，被重新定义为信息扩散，尤其是在K:V和V:K记忆操作中。

Method: 引入HDRAM（全息定义随机存取存储器），将Transformer潜在空间视为扩频信道。该框架基于“超令牌”构建，超令牌是集成经典纠错码（ECC）、全息计算和量子启发搜索的结构化符号代码。通过原理性去扩频恢复分布式信息，利用相位相干内存地址实现高效键值操作和Grover风格搜索。方法还结合了ECC语法、压缩感知和Krylov子空间对齐。

Result: 在不改变现有Transformer架构的情况下，显著提高了联想检索能力。

Conclusion: 证明了经典-全息-量子启发（CHQ）原理可以有效增强Transformer架构，从而解决大型语言模型中的信息扩散问题并提升其性能。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but suffer from
apparent precision loss, reframed here as information spreading. This reframing
shifts the problem from computational precision to an information-theoretic
communication issue. We address the K:V and V:K memory problem in LLMs by
introducing HDRAM (Holographically Defined Random Access Memory), a symbolic
memory framework treating transformer latent space as a spread-spectrum
channel. Built upon hypertokens, structured symbolic codes integrating
classical error-correcting codes (ECC), holographic computing, and
quantum-inspired search, HDRAM recovers distributed information through
principled despreading. These phase-coherent memory addresses enable efficient
key-value operations and Grover-style search in latent space. By combining ECC
grammar with compressed sensing and Krylov subspace alignment, HDRAM
significantly improves associative retrieval without architectural changes,
demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can
fortify transformer architectures.

</details>


### [142] [Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE](https://arxiv.org/abs/2507.00003)
*Eyhab Al-Masri*

Main category: cs.LG

TL;DR: 该论文提出了NeutroSENSE，一个结合中智逻辑的集成框架，用于可解释的IoT入侵检测。它通过量化不确定性来提高准确性和可解释性，并支持在边缘部署中实现信任感知AI。


<details>
  <summary>Details</summary>
Motivation: 在IoT环境中需要更具可解释性且值得信赖的入侵检测系统，尤其是在边缘部署中，通过量化预测不确定性以实现知情决策和人机协作。

Method: NeutroSENSE框架结合了随机森林、XGBoost和逻辑回归等集成学习模型与中智逻辑。系统将预测置信度分解为真、假和不确定性(I)分量，并使用全局和自适应阈值标记高不确定性预测以供审查。

Result: 在IoT-CAD数据集上，NeutroSENSE实现了97%的准确率。误分类样本的不确定性(I=0.62)显著高于正确分类样本(I=0.24)。不确定性（I-score）被证明是衡量不确定性的有效指标，与错误可能性高度相关，支持知情弃权和有针对性的复审。

Conclusion: 研究表明中智逻辑能够同时提升IoT入侵检测系统的准确性和可解释性，为边缘和雾计算环境中的信任感知AI提供了实用的基础。

Abstract: This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework
for interpretable intrusion detection in IoT environments. By integrating
Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, the
system decomposes prediction confidence into truth (T), falsity (F), and
indeterminacy (I) components, enabling uncertainty quantification and
abstention. Predictions with high indeterminacy are flagged for review using
both global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD
dataset, NeutroSENSE achieved 97% accuracy, while demonstrating that
misclassified samples exhibit significantly higher indeterminacy (I = 0.62)
than correct ones (I = 0.24). The use of indeterminacy as a proxy for
uncertainty enables informed abstention and targeted review-particularly
valuable in edge deployments. Figures and tables validate the correlation
between I-scores and error likelihood, supporting more trustworthy,
human-in-the-loop AI decisions. This work shows that neutrosophic logic
enhances both accuracy and explainability, providing a practical foundation for
trust-aware AI in edge and fog-based IoT security systems.

</details>


### [143] [A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search](https://arxiv.org/abs/2507.00004)
*Austin R. Ellis-Mohr,Anuj K. Nayak,Lav R. Varshney*

Main category: cs.LG

TL;DR: 本文提出DS3框架，将推理建模为技能图遍历，推导出推理策略的计算成本与成功率表达式，并将其与训练扩展定律结合，统一分析了各种推理策略，揭示了LLM推理的计算与性能关系，并为资源分配提供理论指导。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在训练和部署阶段消耗大量计算、能源和财务资源。推理成本，特别是对于推理型模型，已成为总资源负担中日益增长的重要部分。现有计算优化方法未能充分考虑更高效的运行点。

Method: 引入定向随机技能搜索（DS3）通用框架，将推理表示为学习技能图上的随机遍历。从简化实例推导出任务成功率和计算成本的闭式表达式，涵盖思维链（CoT）和思维树（ToT）等推理策略。将先前的LLM训练三方图框架扩展以纳入推理，并结合经验方法表征LLM缩放行为。

Result: 理论上恢复了经验观察到的模式，包括：精度随对数计算量线性增长；最优推理策略随任务难度和模型能力变化；推理引起的涌现行为（即使在参数缩放性能平台期）；以及统一分析了BoN和多数投票行为。

Conclusion: 该框架通过明确表征训练-推理的相互依赖性，深化了理论理解，并支持LLM的原则性算法设计和资源分配。

Abstract: Large language models (LLMs) demand considerable computational, energy, and
financial resources during both training and deployment. While scaling laws for
training have guided much of the field's recent progress, inference costs now
represent a significant and growing component of the overall resource burden,
particularly for reasoning-focused models. Existing characterizations of
compute-optimality that consider model size, dataset size, and inference tokens
in isolation or in fixed combinations risk overlooking more efficient operating
points. We introduce directed stochastic skill search (DS3), a general
framework that represents inference as stochastic traversal over a learned
skill graph. From a simplified yet expressive instantiation, we derive
closed-form expressions for task success and compute cost across a wide range
of inference strategies -- including chain-of-thought (CoT) and tree-of-thought
(ToT) -- enabling comparative analysis as a function of task difficulty and
model capability. To that end, we extend a prior first-principles tripartite
graph framework of LLM training to incorporate inference, and separately bridge
DS3 with empirical methods that characterize LLM scaling behavior. We
theoretically recover empirically observed patterns, including: linear accuracy
scaling with logarithmic compute; variation in preferred inference strategies
as a function of task difficulty and model capability; emergent behavior
elicited by reasoning even when performance plateaus under parameter scaling;
and both best-of-N (BoN) and majority voting behavior captured within a unified
analytical framework. By explicitly characterizing training-inference
interdependencies, our framework deepens theoretical understanding and supports
principled algorithmic design and resource allocation.

</details>


### [144] [Novel RL approach for efficient Elevator Group Control Systems](https://arxiv.org/abs/2507.00011)
*Nathan Vaartjes,Vincent Francois-Lavet*

Main category: cs.LG

TL;DR: 本文利用强化学习（RL）优化大型建筑的电梯群控系统（EGCS），通过新颖的建模和算法创新，成功处理随机性和组合复杂性，并证明其性能优于传统规则算法。


<details>
  <summary>Details</summary>
Motivation: 大型建筑中高效的电梯交通管理对减少乘客旅行时间和能耗至关重要。然而，传统的启发式或模式检测控制器难以应对电梯调度的随机性和组合复杂性。

Method: 将阿姆斯特丹自由大学的六梯十五层系统建模为马尔可夫决策过程（MDP），并训练一个端到端的强化学习（RL）电梯群控系统（EGCS）。关键创新包括：设计新颖的动作空间编码以处理调度复杂性、引入“infra-steps”以模拟连续乘客到达，以及定制奖励信号以提高学习效率。此外，还探索了多种调整折扣因子以适应“infra-step”公式的方法。RL架构基于Dueling Double Deep Q-learning。

Result: 所提出的基于RL的EGCS能够适应波动的交通模式，从高度随机的环境中学习，并表现出优于传统规则算法的性能。

Conclusion: 强化学习方法能够有效解决大型建筑电梯交通管理的复杂性和随机性问题，通过适应多变环境并学习，显著提升电梯群控系统性能，优于传统解决方案。

Abstract: Efficient elevator traffic management in large buildings is critical for
minimizing passenger travel times and energy consumption. Because heuristic- or
pattern-detection-based controllers struggle with the stochastic and
combinatorial nature of dispatching, we model the six-elevator, fifteen-floor
system at Vrije Universiteit Amsterdam as a Markov Decision Process and train
an end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).
Key innovations include a novel action space encoding to handle the
combinatorial complexity of elevator dispatching, the introduction of
infra-steps to model continuous passenger arrivals, and a tailored reward
signal to improve learning efficiency. In addition, we explore various ways to
adapt the discounting factor to the infra-step formulation. We investigate RL
architectures based on Dueling Double Deep Q-learning, showing that the
proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a
highly stochastic environment, and thereby outperforms a traditional rule-based
algorithm.

</details>


### [145] [Towards Undistillable Models by Minimizing Conditional Mutual Information](https://arxiv.org/abs/2507.00012)
*Linfeng Ye,Shayan Mohajer Hamidi,En-hui Yang*

Main category: cs.LG

TL;DR: 为保护深度神经网络（DNN）的知识产权，本研究提出一种新的训练方法——条件互信息最小化（CMIM），旨在使DNN不可被知识蒸馏（KD）。实验证明，CMIM模型不仅具有不可蒸馏性，且自身预测精度有所提高。


<details>
  <summary>Details</summary>
Motivation: 在知识蒸馏（KD）中，深度神经网络（DNN）作为黑盒教师模型可能导致其知识产权被复制。为了保护DNN的知识产权，需要构建一种“不可蒸馏”的DNN，即通过KD蒸馏出的学生模型（即仿冒学生）的预测准确率不优于独立训练的标签平滑（LS）学生模型。

Method: 研究观察到，一个不可蒸馏的DNN可能具有的特性是，其输出概率分布在响应所有相同标签的样本实例时应高度集中，理想情况下每个标签对应的聚类应塌缩成一个概率分布。基于此观察，并使用条件互信息（CMI）来衡量每个聚类的集中度，提出了一种新的训练方法——CMI最小化（CMIM）方法。该方法通过联合最小化传统的交叉熵（CE）损失和所有温度尺度聚类的CMI值来训练DNN。

Result: 通过广泛实验证明，所提出的CMIM模型对现有文献中所有测试的知识蒸馏方法都表现出不可蒸馏性。具体来说，从CMIM模型中蒸馏出的仿冒学生模型表现不如各自的LS学生模型。此外，CMIM模型自身的预测准确度也优于仅用CE损失训练的模型。

Conclusion: CMIM方法成功地构建了具有知识产权保护能力的不可蒸馏DNNs，使其能够有效抵抗知识蒸馏攻击。同时，CMIM模型在保持或提升自身预测准确性方面也表现出色，为DNN的IP保护提供了一种有效途径。

Abstract: A deep neural network (DNN) is said to be undistillable if, when used as a
black-box input-output teacher, it cannot be distilled through knowledge
distillation (KD). In this case, the distilled student (referred to as the
knockoff student) does not outperform a student trained independently with
label smoothing (LS student) in terms of prediction accuracy. To protect
intellectual property of DNNs, it is desirable to build undistillable DNNs. To
this end, it is first observed that an undistillable DNN may have the trait
that each cluster of its output probability distributions in response to all
sample instances with the same label should be highly concentrated to the
extent that each cluster corresponding to each label should ideally collapse
into one probability distribution. Based on this observation and by measuring
the concentration of each cluster in terms of conditional mutual information
(CMI), a new training method called CMI minimized (CMIM) method is proposed,
which trains a DNN by jointly minimizing the conventional cross entropy (CE)
loss and the CMI values of all temperature scaled clusters across the entire
temperature spectrum. The resulting CMIM model is shown, by extensive
experiments, to be undistillable by all tested KD methods existing in the
literature. That is, the knockoff students distilled by these KD methods from
the CMIM model underperform the respective LS students. In addition, the CMIM
model is also shown to performs better than the model trained with the CE loss
alone in terms of their own prediction accuracy.

</details>


### [146] [ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting](https://arxiv.org/abs/2507.00013)
*Hyunwoo Seo,Chiehyeon Lim*

Main category: cs.LG

TL;DR: ST-MTM是一种基于季节-趋势分解和对比学习的掩码时间序列模型，旨在通过处理复杂时间序列的内在语义结构来提高预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码时间序列建模（MTM）在预测复杂时间序列时，简单地掩码原始数据会忽略时间序列固有的语义结构（由多个时间序列分量产生的复杂时序变化），可能导致模型学习到虚假的时间模式。

Method: 本文提出了ST-MTM框架，该框架结合了季节-趋势分解。它包含一种新颖的掩码方法：对季节分量采用周期掩码策略以处理多周期性；对趋势分量采用子序列掩码策略以掩码共享相似变化的区域。此外，ST-MTM引入了一个对比学习任务，以增强多个掩码季节表示之间的上下文一致性。

Result: 实验结果表明，ST-MTM在预测性能上持续优于现有的掩码建模、对比学习和监督预测方法。

Conclusion: ST-MTM通过将季节-趋势分解与创新的掩码方法相结合，并辅以对比学习，有效解决了复杂时间序列中语义结构被忽视的问题，显著提升了时间序列预测的准确性。

Abstract: Forecasting complex time series is an important yet challenging problem that
involves various industrial applications. Recently, masked time-series modeling
has been proposed to effectively model temporal dependencies for forecasting by
reconstructing masked segments from unmasked ones. However, since the semantic
information in time series is involved in intricate temporal variations
generated by multiple time series components, simply masking a raw time series
ignores the inherent semantic structure, which may cause MTM to learn spurious
temporal patterns present in the raw data. To capture distinct temporal
semantics, we show that masked modeling techniques should address entangled
patterns through a decomposition approach. Specifically, we propose ST-MTM, a
masked time-series modeling framework with seasonal-trend decomposition, which
includes a novel masking method for the seasonal-trend components that
incorporates different temporal variations from each component. ST-MTM uses a
period masking strategy for seasonal components to produce multiple masked
seasonal series based on inherent multi-periodicity and a sub-series masking
strategy for trend components to mask temporal regions that share similar
variations. The proposed masking method presents an effective pre-training task
for learning intricate temporal variations and dependencies. Additionally,
ST-MTM introduces a contrastive learning task to support masked modeling by
enhancing contextual consistency among multiple masked seasonal
representations. Experimental results show that our proposed ST-MTM achieves
consistently superior forecasting performance compared to existing masked
modeling, contrastive learning, and supervised forecasting methods.

</details>


### [147] [SWE-Bench-CL: Continual Learning for Coding Agents](https://arxiv.org/abs/2507.00014)
*Thomas Joshi,Shayan Chowdhury,Fatih Uysal*

Main category: cs.LG

TL;DR: 引入SWE-Bench-CL，一个评估大型语言模型（LLM）在动态软件开发环境中持续学习能力的新基准和配套评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码生成基准主要针对静态任务，无法反映真实软件开发中持续演进、需要累积经验、知识迁移和抵抗灾难性遗忘的动态特性。

Method: 1. 构建SWE-Bench-CL基准，将GitHub问题按时间顺序排列，模拟软件仓库的自然演化。2. 提供LangGraph评估框架，并集成基于FAISS的语义记忆模块。3. 设计一套专门的持续学习指标，包括平均准确率、遗忘、前后向迁移、工具使用效率以及综合持续学习分数等。4. 概述了一个实验协议，用于比较带记忆和不带记忆的智能体。

Result: 发布了SWE-Bench-CL数据集、一个交互式评估框架、语义记忆模块和一套定制化的持续学习指标。所有代码和数据均已公开，为社区提供了可复现的平台。

Conclusion: 本研究为开发和评估更具适应性、鲁棒性的软件工程AI智能体提供了重要的工具和平台，以应对真实世界中动态且不断变化的软件开发挑战。

Abstract: Large Language Models (LLMs) have achieved impressive results on static
code-generation benchmarks, but real-world software development unfolds as a
continuous stream of evolving issues, fixes, and feature requests. We introduce
SWE-Bench-CL, a novel continual learning benchmark built on the human-verified
SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By
organizing GitHub issues into chronologically ordered sequences that reflect
natural repository evolution, SWE-Bench-CL enables direct evaluation of an
agent's ability to accumulate experience, transfer knowledge across tasks, and
resist catastrophic forgetting. We complement the dataset with (i) a
preliminary analysis of inter-task structural similarity and contextual
sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented
with a FAISS-backed semantic memory module, and (iii) a suite of specialized
continual learning metrics -- including average accuracy, forgetting,
forward/backward transfer, tool-use efficiency, and a generalized Composite
Continual Learning Score and CL-F-beta score -- to capture the
stability-plasticity trade-off. We outline a rigorous experimental protocol
comparing memory-enabled and memory-disabled agents across diverse Python
repositories. All code and data are publicly available at
https://github.com/thomasjoshi/agents-never-forget, providing the community
with a reproducible platform for developing more adaptive and robust AI agents
in software engineering.

</details>


### [148] [MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic](https://arxiv.org/abs/2507.00304)
*Yujun Zhang,Runlong Li,Xiaoxiang Liang,Xinhao Yang,Tian Su,Bo Liu,Yan Zhou*

Main category: cs.LG

TL;DR: 本文提出MamNet模型，一个结合时域建模（Mamba）和频域特征提取（傅里叶变换）的网络流量预测与异常检测方法，实验证明其在准确性和F1-Score上优于现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 网络流量异常波动可能预示安全威胁或系统故障，因此，高效的网络流量预测与异常检测方法对网络安全和流量管理至关重要。

Method: 提出MamNet模型，该模型通过Mamba模块进行时域建模以捕获长期依赖，利用傅里叶变换进行频域特征提取以识别周期波动，并通过特征融合层整合多尺度信息，以增强异常检测能力。

Result: 在UNSW-NB15和CAIDA数据集上的实验表明，MamNet在准确率、召回率和F1-Score方面优于多种主流模型，在复杂流量模式和长期趋势检测方面，性能提升约2%至4%。

Conclusion: MamNet能有效捕获不同时间尺度的网络流量异常，适用于网络安全和流量管理中的异常检测任务。

Abstract: The abnormal fluctuations in network traffic may indicate potential security
threats or system failures. Therefore, efficient network traffic prediction and
anomaly detection methods are crucial for network security and traffic
management. This paper proposes a novel network traffic prediction and anomaly
detection model, MamNet, which integrates time-domain modeling and
frequency-domain feature extraction. The model first captures the long-term
dependencies of network traffic through the Mamba module (time-domain
modeling), and then identifies periodic fluctuations in the traffic using
Fourier Transform (frequency-domain feature extraction). In the feature fusion
layer, multi-scale information is integrated to enhance the model's ability to
detect network traffic anomalies. Experiments conducted on the UNSW-NB15 and
CAIDA datasets demonstrate that MamNet outperforms several recent mainstream
models in terms of accuracy, recall, and F1-Score. Specifically, it achieves an
improvement of approximately 2% to 4% in detection performance for complex
traffic patterns and long-term trend detection. The results indicate that
MamNet effectively captures anomalies in network traffic across different time
scales and is suitable for anomaly detection tasks in network security and
traffic management. Future work could further optimize the model structure by
incorporating external network event information, thereby improving the model's
adaptability and stability in complex network environments.

</details>


### [149] [Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications](https://arxiv.org/abs/2507.00015)
*Lu Zhang,Sangarapillai Lambotharan,Gan Zheng,Guisheng Liao,Xuekang Liu,Fabio Roli,Carsten Maple*

Main category: cs.LG

TL;DR: 为解决基于Transformer的物联网调制分类系统易受对抗攻击的问题，本文提出一种新颖的Vision Transformer (ViT) 架构，引入“对抗性指示器 (AdvI) token”并结合对抗训练，以统一模型实现攻击检测与防御，实验证明其在白盒攻击下表现优异。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在物联网设备的自动调制分类中应用广泛，但其对无线电信号的分类容易受到微妙而复杂的对抗性攻击，因此亟需开发有效的防御策略。

Method: 提出一种新的Vision Transformer (ViT) 架构，引入“对抗性指示器 (AdvI) token”来检测对抗性攻击。该方法将对抗训练与基于AdvI token的检测机制结合，在一个统一的神经网络模型中实现了训练时防御和运行时防御，从而降低了系统架构的复杂性。

Result: 研究表明，AdvI token在ViT中充当关键元素，能够影响注意力权重并突出输入数据中潜在可疑或异常的区域。实验结果证明，该方法在处理白盒攻击场景（包括FGM、PGD和BIM）方面优于多种现有竞争方法。

Conclusion: 所提出的集成AdvI token的ViT防御策略有效增强了基于Transformer的调制分类系统抵御对抗性攻击的能力，并通过统一的防御机制简化了系统设计。

Abstract: The remarkable success of transformers across various fields such as natural
language processing and computer vision has paved the way for their
applications in automatic modulation classification, a critical component in
the communication systems of Internet of Things (IoT) devices. However, it has
been observed that transformer-based classification of radio signals is
susceptible to subtle yet sophisticated adversarial attacks. To address this
issue, we have developed a defensive strategy for transformer-based modulation
classification systems to counter such adversarial attacks. In this paper, we
propose a novel vision transformer (ViT) architecture by introducing a new
concept known as adversarial indicator (AdvI) token to detect adversarial
attacks. To the best of our knowledge, this is the first work to propose an
AdvI token in ViT to defend against adversarial attacks. Integrating an
adversarial training method with a detection mechanism using AdvI token, we
combine a training time defense and running time defense in a unified neural
network model, which reduces architectural complexity of the system compared to
detecting adversarial perturbations using separate models. We investigate into
the operational principles of our method by examining the attention mechanism.
We show the proposed AdvI token acts as a crucial element within the ViT,
influencing attention weights and thereby highlighting regions or features in
the input data that are potentially suspicious or anomalous. Through
experimental results, we demonstrate that our approach surpasses several
competitive methods in handling white-box attack scenarios, including those
utilizing the fast gradient method, projected gradient descent attacks and
basic iterative method.

</details>


### [150] [Gradient-based Fine-Tuning through Pre-trained Model Regularization](https://arxiv.org/abs/2507.00016)
*Xuanbo Liu,Liu Liu,Fuxiang Wu,Fusheng Hao,Xianglong Liu*

Main category: cs.LG

TL;DR: 本文提出一种高效的梯度正则化微调方法（GRFT），通过选择性更新权重矩阵的行或列并引入正则化，显著减少了微调参数量和计算资源，同时实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练模型在下游任务上的微调需要大量计算和存储资源。现有梯度微调方法（如GPS）虽能减少训练参数，但仍存在计算和存储开销大的问题。

Method: 提出一种高效的梯度正则化微调方法（GRFT）。该方法通过更新权重矩阵的行或列进行微调，并从理论上证明选择平方梯度和最高的行或列是更新的最佳策略。此外，该方法融入正则化以增强预训练模型的知识迁移。

Result: GRFT在FGVC和VTAB数据集上分别仅更新了总参数的1.22%和0.30%，极大地提升了效率。在性能方面，GRFT超越了现有方法，包括GPS、Adapter Tuning和LoRA，达到了最先进的水平。

Conclusion: GRFT是一种高效且有效的预训练模型微调策略，它通过精简的参数选择和正则化，在显著降低资源需求的同时，实现了卓越的性能表现。

Abstract: Large pre-trained models have demonstrated extensive applications across
various fields. However, fine-tuning these models for specific downstream tasks
demands significant computational resources and storage. One fine-tuning
method, gradient-based parameter selection (GPS), focuses on fine-tuning only
the parameters with high gradients in each neuron, thereby reducing the number
of training parameters. Nevertheless, this approach increases computational
resource requirements and storage demands. In this paper, we propose an
efficient gradient-based and regularized fine-tuning method (GRFT) that updates
the rows or columns of the weight matrix. We theoretically demonstrate that the
rows or columns with the highest sum of squared gradients are optimal for
updating. This strategy effectively reduces storage overhead and improves the
efficiency of parameter selection. Additionally, we incorporate regularization
to enhance knowledge transfer from the pre-trained model. GRFT achieves
state-of-the-art performance, surpassing existing methods such as GPS, Adapter
Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the
total parameters on FGVC and VTAB datasets, respectively, demonstrating its
high efficiency and effectiveness. The source code will be released soon.

</details>


### [151] [Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections](https://arxiv.org/abs/2507.00018)
*Bo Wang,Qinyuan Cheng,Runyu Peng,Rong Bao,Peiji Li,Qipeng Guo,Linyang Li,Zhiyuan Zeng,Yunhua Zhou,Xipeng Qiu*

Main category: cs.LG

TL;DR: 本文提出一个统一理论框架，连接SFT与偏好学习，指出SFT的KL散度限制并提出改进策略，显著提升了LLM的指令遵循性能。


<details>
  <summary>Details</summary>
Motivation: 将预训练语言模型（LLMs）应用于实际任务时，SFT和偏好学习是关键的后训练阶段，但缺乏统一的理论理解，且传统SFT存在优化局限性。

Method: 通过数学推导，建立统一理论框架，证明SFT和偏好学习（如DPO）在同一最优策略-奖励子空间运行，SFT是隐式奖励学习的特例。分析发现传统SFT中KL散度项在优化时对策略保持常数，故提出简单的学习率降低方法。此外，从f-散度函数推导替代SFT目标函数，以保留KL项。并将LLM logits与Q函数的关系从偏好学习扩展到SFT上下文。

Result: 研究揭示了传统SFT中KL散度项无法有效约束模型更新的局限。所提出的学习率降低方法在指令遵循任务中实现了高达25%的相对增益和6%的绝对胜率提升。基于f-散度的替代SFT目标函数进一步增强了DPO后模型的性能。同时，成功将LLM logits与Q函数的关系扩展并验证到SFT语境。

Conclusion: 本研究统一了LLM后训练中SFT和偏好学习的理论基础，揭示并解决了传统SFT的优化缺陷，提出了有效的性能改进方法，为LLM的接地气训练提供了新的理论洞察和实践指导。

Abstract: Post-training processes are essential phases in grounding pre-trained
language models to real-world tasks, with learning from demonstrations or
preference signals playing a crucial role in this adaptation. We present a
unified theoretical framework bridging Supervised Fine-Tuning (SFT) and
preference learning in Large Language Model (LLM) post-training. Through
rigorous mathematical derivation, we demonstrate that both SFT and preference
learning methods like Direct Preference Optimization (DPO) operate within the
same optimal policy-reward subspace, with SFT representing a special case of
implicit reward learning. Our analysis reveals a critical limitation in
conventional SFT: the KL divergence term in distribution matching becomes
constant with respect to the policy during optimization, failing to constrain
model updates. To address this, we propose a simple yet effective learning rate
reduction approach that yields significant performance improvements (up to
\textbf{25\%} relative gain and \textbf{6\%} absolute win rate increase in
instruction following tasks. Additionally, we derive alternative SFT objectives
from various f-divergence functions that preserve the KL term during
optimization, further enhancing post-DPO model performance. Finally, we extend
the theoretical relationship between LLM logits and Q-functions from preference
learning to the SFT context, providing mathematical derivations and
experimental validation.

</details>


### [152] [Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations](https://arxiv.org/abs/2507.00019)
*Minati Rath,Hema Date*

Main category: cs.LG

TL;DR: 本研究提出了三种量子启发式数据编码策略（ILS、GDS、CCVS），用于将经典数据转换为量子数据以应用于经典机器学习模型，旨在降低编码时间并分析其对分类性能的影响。


<details>
  <summary>Details</summary>
Motivation: 主要研究动机是降低高昂的数据编码时间，同时确保编码值的正确性，并探究不同编码策略对机器学习模型分类性能的影响。

Method: 研究提出了三种量子启发式数据编码策略：实例级别策略（ILS）、全局离散值策略（GDS）和类别条件值策略（CCVS）。这些策略被应用于分类任务，并评估了它们在编码效率、正确性、模型准确性和计算成本方面的影响。

Result: 通过分析编码时间、精度和预测性能之间的权衡，本研究为优化量子启发式数据转换以用于经典机器学习工作流提供了洞见。

Conclusion: 研究结果为如何优化经典机器学习中的量子启发式数据转换提供了见解，特别是通过权衡编码时间、精度和预测性能来指导决策。

Abstract: In this study, we propose, evaluate and compare three quantum inspired data
encoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy
(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical
data into quantum data for use in pure classical machine learning models. The
primary objective is to reduce high encoding time while ensuring correct
encoding values and analyzing their impact on classification performance. The
Instance Level Strategy treats each row of dataset independently; mimics local
quantum states. Global Discrete Value Based encoding strategy maps all unique
feature values across the full dataset to quantum states uniformly. In
contrast, the Class conditional Value based encoding strategy encodes unique
values separately for each class, preserving class dependent information.
  We apply these encoding strategies to a classification task and assess their
impact on en-coding efficiency, correctness, model accuracy, and computational
cost. By analyzing the trade offs between encoding time, precision, and
predictive performance, this study provides insights into optimizing quantum
inspired data transformations for classical machine learning workflows.

</details>


### [153] [Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods](https://arxiv.org/abs/2507.00020)
*Marcio Borges,Felipe Pereira,Michel Tosin*

Main category: cs.LG

TL;DR: 本研究利用变分自编码器（VAE）提升马尔可夫链蒙特卡罗（McMC）方法在贝叶斯反演问题中的效率和适应性，尤其是在先验知识有限的情况下，通过数据驱动生成更广泛的先验提议。


<details>
  <summary>Details</summary>
Motivation: 传统McMC方法，如KLE，需预知协方差函数，但在实际应用中常难以获取。研究旨在通过数据驱动方法，灵活捕捉贝叶斯反演问题中更广泛的相关结构，从而提高McMC的效率和适用性。

Method: 采用变分自编码器（VAE）框架，生成更广泛频谱的先验提议，以增强McMC方法。该方法在合成地下水流反演问题中（利用压力数据估算渗透率场）进行了测试。

Result: 数值实验表明，当已知相关长度时，基于VAE的参数化方法可达到与KLE相当的精度；当假设的相关长度偏离真实值时，VAE优于KLE。此外，VAE显著降低了随机维度，提高了计算效率。

Conclusion: 研究结果表明，在McMC方法中利用深度生成模型能够实现高维问题中更具适应性和效率的贝叶斯推断。

Abstract: This study uses a Variational Autoencoder method to enhance the efficiency
and applicability of Markov Chain Monte Carlo (McMC) methods by generating
broader-spectrum prior proposals. Traditional approaches, such as the
Karhunen-Lo\`eve Expansion (KLE), require previous knowledge of the covariance
function, often unavailable in practical applications. The VAE framework
enables a data-driven approach to flexibly capture a broader range of
correlation structures in Bayesian inverse problems, particularly subsurface
flow modeling. The methodology is tested on a synthetic groundwater flow
inversion problem, where pressure data is used to estimate permeability fields.
Numerical experiments demonstrate that the VAE-based parameterization achieves
comparable accuracy to KLE when the correlation length is known and outperforms
KLE when the assumed correlation length deviates from the true value. Moreover,
the VAE approach significantly reduces stochastic dimensionality, improving
computational efficiency. The results suggest that leveraging deep generative
models in McMC methods can lead to more adaptable and efficient Bayesian
inference in high-dimensional problems.

</details>


### [154] [GLU Attention Improve Transformer](https://arxiv.org/abs/2507.00022)
*Zehao Wang*

Main category: cs.LG

TL;DR: 本文提出一种GLU注意力机制，通过为注意力值引入非线性，在文本和视觉任务中提升模型性能与收敛速度，且无额外参数和可忽略的计算成本。


<details>
  <summary>Details</summary>
Motivation: 作者旨在利用Gated Linear Units（GLU）在提升神经网络性能方面的潜力，改进现有的注意力机制，以实现更好的模型表现。

Method: 提出一种名为GLU注意力的新型注意力机制，其核心创新在于为注意力机制的值（values）引入非线性。

Result: 实验证明GLU注意力在文本和视觉模态上均能提升模型性能和收敛速度。该机制无需额外参数，计算成本可忽略不计，并且可以与Flash Attention、RoPE及GQA等现有技术无缝集成。

Conclusion: GLU注意力是一种轻量、高效且兼容性强的新型注意力机制，能够以极小的代价显著提升神经网络的性能和训练效率。

Abstract: Gated Linear Units (GLU) have shown great potential in enhancing neural
network performance. In this paper, I introduce a novel attention mechanism
called GLU Attention, which introduces nonlinearity into the values of
Attention. My experiments demonstrate that GLU Attention improves both model
performance and convergence speed across text and vision modalities with zero
additional parameters and negligible computational costs. GLU Attention is
lightweight and can seamlessly integrate with other technologies, such as Flash
Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention
(MHA) variants such as Grouped-Query Attention (GQA). This project is
open-sourced at github.

</details>


### [155] [AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity](https://arxiv.org/abs/2507.00024)
*Yeyong Yu,Xilei Bian,Jie Xiong,Xing Wu,Quan Qian*

Main category: cs.LG

TL;DR: 本文提出AIMatDesign，一个强化的学习框架，用于解决高维材料反向设计中数据稀缺和模型可靠性不足的问题。该框架通过数据增强、大型语言模型（LLMs）引导的校正和基于知识的奖励函数，显著提升了材料发现效率和成功率，并成功预测和合成了高性能合金。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习驱动的反向材料设计方法面临两大挑战：一是机器学习模型在高维空间中可靠性差，导致预测偏差；二是模型未能有效整合领域专家知识，限制了知识引导设计的潜力。

Method: 引入AIMatDesign，一个强化学习框架。该框架通过以下方式解决挑战：1) 使用基于差异的算法扩充实验数据，构建可信经验池，加速模型收敛；2) 采用由大型语言模型（LLMs）引导的自动化精炼策略，动态校正预测不一致性，增强模型可靠性；3) 设计基于知识的奖励函数，利用专家领域规则提升训练的稳定性和效率。

Result: 实验证明AIMatDesign在发现效率、收敛速度和成功率方面显著优于传统机器学习和强化学习方法。通过AIMatDesign提出的候选材料，成功实验合成了代表性的锆基合金，其非晶态金属（BMG）表现出1.7GPa的屈服强度和10.2%的延伸率，与预测结果高度吻合。此外，该框架准确捕捉了屈服强度随成分变化的趋势。

Conclusion: AIMatDesign是一个可靠且潜力巨大的框架，能够实现闭环的材料发现。它通过有效整合数据增强、LLM辅助校正和专家知识，成功解决了高维材料反向设计中的关键挑战，为新材料的探索提供了有效途径。

Abstract: With the growing demand for novel materials, machine learning-driven inverse
design methods face significant challenges in reconciling the high-dimensional
materials composition space with limited experimental data. Existing approaches
suffer from two major limitations: (I) machine learning models often lack
reliability in high-dimensional spaces, leading to prediction biases during the
design process; (II) these models fail to effectively incorporate domain expert
knowledge, limiting their capacity to support knowledge-guided inverse design.
To address these challenges, we introduce AIMatDesign, a reinforcement learning
framework that addresses these limitations by augmenting experimental data
using difference-based algorithms to build a trusted experience pool,
accelerating model convergence. To enhance model reliability, an automated
refinement strategy guided by large language models (LLMs) dynamically corrects
prediction inconsistencies, reinforcing alignment between reward signals and
state value functions. Additionally, a knowledge-based reward function
leverages expert domain rules to improve stability and efficiency during
training. Our experiments demonstrate that AIMatDesign significantly surpasses
traditional machine learning and reinforcement learning methods in discovery
efficiency, convergence speed, and success rates. Among the numerous candidates
proposed by AIMatDesign, experimental synthesis of representative Zr-based
alloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\%
elongation, closely matching predictions. Moreover, the framework accurately
captured the trend of yield strength variation with composition, demonstrating
its reliability and potential for closed-loop materials discovery.

</details>


### [156] [Generalizing to New Dynamical Systems via Frequency Domain Adaptation](https://arxiv.org/abs/2507.00025)
*Tiexin Qin,Hong Yan,Haoliang Li*

Main category: cs.LG

TL;DR: 现有深度学习模型在物理动力学泛化方面受限。本文提出FNSDA，一种参数高效的方法，通过在傅里叶空间适应来泛化新动力学，识别共享和特定环境的傅里叶模式。实验证明，FNSDA在泛化性能上优于或媲美现有方法，且参数成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络在建模复杂物理动力学方面潜力巨大，但当前方法在特定领域进行可靠预测的能力有限，且难以泛化到由相同通用动力学控制但环境特性不同的未见系统。

Method: 提出了一种参数高效的方法——傅里叶神经网络动力学适应器（FNSDA）。FNSDA通过在傅里叶空间进行适应来实现对新动力学的泛化。具体而言，它基于已知环境通过傅里叶模式的自动划分来识别可共享的动力学，并通过以低维潜在系统参数为条件来调整针对每个新环境的特定模式，从而实现高效泛化。

Result: 在四类代表性动态系统上的评估结果显示，与现有方法相比，FNSDA能够实现更优或具有竞争力的泛化性能，并且显著降低了参数成本。

Conclusion: FNSDA成功解决了深度学习模型在物理动力学泛化方面的局限性，通过傅里叶空间适应实现了高效的泛化，并在性能和参数效率方面表现出色。

Abstract: Learning the underlying dynamics from data with deep neural networks has
shown remarkable potential in modeling various complex physical dynamics.
However, current approaches are constrained in their ability to make reliable
predictions in a specific domain and struggle with generalizing to unseen
systems that are governed by the same general dynamics but differ in
environmental characteristics. In this work, we formulate a parameter-efficient
method, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can
readily generalize to new dynamics via adaptation in the Fourier space.
Specifically, FNSDA identifies the shareable dynamics based on the known
environments using an automatic partition in Fourier modes and learns to adjust
the modes specific for each new environment by conditioning on low-dimensional
latent systematic parameters for efficient generalization. We evaluate our
approach on four representative families of dynamic systems, and the results
show that FNSDA can achieve superior or competitive generalization performance
compared to existing methods with a significantly reduced parameter cost. Our
code is available at https://github.com/WonderSeven/FNSDA.

</details>


### [157] [ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models](https://arxiv.org/abs/2507.00026)
*Jiale Ding,Xiang Zheng,Cong Wang,Wei-Bin Lee,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: 提出ROSE框架，利用多目标强化学习生成多样化且情境丰富的对抗性提示，以更有效地评估LLM的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）作为黑盒组件被广泛部署，其安全评估（尤其在对抗性提示下）至关重要。现有手动安全基准静态且耗时，难以跟上LLM发展；现有自动化方法则缺乏主题多样性和与真实世界的结合，导致生成的对抗性提示主题狭窄且场景重复。因此，需要一种能够自适应、覆盖广泛有害主题和真实场景的更有效安全评估方法。

Method: 提出“面向现实的安全评估（Reality-Oriented Safety Evaluation, ROSE）”框架，该框架利用多目标强化学习来微调一个对抗性LLM，从而生成主题多样化且情境丰富的对抗性提示。

Result: 实验结果表明，ROSE在揭示最先进LLM的安全漏洞方面优于现有方法，并在综合评估指标上取得了显著改进。

Conclusion: ROSE代表了LLM安全评估迈向更实用和面向现实的一步。

Abstract: As Large Language Models (LLMs) are increasingly deployed as black-box
components in real-world applications, evaluating their safety-especially under
adversarial prompting-has become critical. Arguably, effective safety
evaluations should be adaptive, evolving with LLM capabilities, and also cover
a broad spectrum of harmful topics and real-world scenarios to fully expose
potential vulnerabilities. Existing manual safety benchmarks, built on
handcrafted adversarial prompts, are limited by their static nature and the
intensive labor required to update them, making it difficult to keep pace with
rapidly advancing LLMs. In contrast, automated adversarial prompt generation
offers a promising path toward adaptive evaluation. However, current methods
often suffer from insufficient adversarial topic coverage (topic-level
diversity) and weak alignment with real-world contexts. These shortcomings stem
from the exploration-exploitation dilemma in black-box optimization and a lack
of real-world contextualization, resulting in adversarial prompts that are both
topically narrow and scenario-repetitive. To address these issues, we propose
Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses
multi-objective reinforcement learning to fine-tune an adversarial LLM for
generating topically diverse and contextually rich adversarial prompts.
Experiments show that ROSE outperforms existing methods in uncovering safety
vulnerabilities in state-of-the-art LLMs, with notable improvements in
integrated evaluation metrics. We hope ROSE represents a step toward more
practical and reality-oriented safety evaluation of LLMs. WARNING: This paper
contains examples of potentially harmful text.

</details>


### [158] [HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation](https://arxiv.org/abs/2507.00028)
*Lihuan Li,Hao Xue,Shuang Ao,Yang Song,Flora Salim*

Main category: cs.LG

TL;DR: 本文提出HiT-JEPA，一个基于分层架构的统一框架，用于学习多尺度城市轨迹表示，旨在整合细粒度细节和高层语义。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹表示方法难以在一个模型中同时捕获细粒度细节和高层摘要信息，限制了它们处理长期依赖并保留局部细微差别的能力，而有效的轨迹表示对于分析空间移动模式至关重要。

Method: 提出HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint Embedding Predictive Architecture)，该框架采用三层分层结构，逐步捕获点级细粒度细节、中间模式和高层轨迹抽象，从而在单一连贯结构中整合局部动态和全局语义。

Result: 在多个真实世界数据集上进行的轨迹相似性计算实验表明，HiT-JEPA的分层设计产生了更丰富、多尺度的轨迹表示。

Conclusion: HiT-JEPA成功解决了现有方法在整合轨迹多尺度信息方面的挑战，提供了更全面有效的轨迹表示，有助于深入分析空间移动模式。

Abstract: The representation of urban trajectory data plays a critical role in
effectively analyzing spatial movement patterns. Despite considerable progress,
the challenge of designing trajectory representations that can capture diverse
and complementary information remains an open research problem. Existing
methods struggle in incorporating trajectory fine-grained details and
high-level summary in a single model, limiting their ability to attend to both
long-term dependencies while preserving local nuances. To address this, we
propose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint
Embedding Predictive Architecture), a unified framework for learning
multi-scale urban trajectory representations across semantic abstraction
levels. HiT-JEPA adopts a three-layer hierarchy that progressively captures
point-level fine-grained details, intermediate patterns, and high-level
trajectory abstractions, enabling the model to integrate both local dynamics
and global semantics in one coherent structure. Extensive experiments on
multiple real-world datasets for trajectory similarity computation show that
HiT-JEPA's hierarchical design yields richer, multi-scale representations. Code
is available at: https://anonymous.4open.science/r/HiT-JEPA.

</details>


### [159] [LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing](https://arxiv.org/abs/2507.00029)
*Wenbing Li,Zikai Song,Hang Zhou,Yunyao Zhang,Junqing Yu,Wei Yang*

Main category: cs.LG

TL;DR: 本文提出LoRA-Mixer，一个轻量级MoE框架，通过动态路由任务特定的LoRA专家替换注意力模块的投影矩阵，显著提升大型语言模型在多任务上的参数效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有结合LoRA与MoE适配LLM的方法存在局限性，如替换整个层或附加并行分支，导致参数效率低下和任务保真度不足。

Method: LoRA-Mixer将LoRA专家集成到MoE框架中，其核心创新在于用动态路由的任务特定LoRA专家替换注意力模块的输入/输出线性层的投影矩阵。该框架支持两种操作范式：1) 通过新型硬软路由策略联合优化LoRA专家和路由机制；2) 直接部署预训练的LoRA模块。为实现鲁棒的路由器训练并最大化专家复用，引入了自适应的专业化平衡损失（SBL）。

Result: LoRA-Mixer在七个基准数据集上进行了广泛实验。相较于基准模型，在GSM8K、HumanEval和MedQA上分别实现了7.61%、4.88%和3.08%的显著提升。与最先进方法相比，在相同数据集上分别额外提升了1.09%、1.45%和1.68%，且仅使用了48%的参数。

Conclusion: LoRA-Mixer在多任务适应方面展现出卓越的效率和强大的性能，有效解决了现有LoRA-MoE方法的局限性。

Abstract: Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts
(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit
prevailing limitations: they either swap entire attention/feed-forward layers
for switch experts or bolt on parallel expert branches, diluting parameter
efficiency and task fidelity. We propose the LoRA-Mixer, a modular and
lightweight MoE framework that integrates LoRA experts. Our core innovation
lies in replacing the projection matrices of the attention module's
input/output linear layers with dynamically routed, task-specific LoRA experts.
This design ensures seamless compatibility with diverse foundation models,
including transformers and state space models (SSMs), by leveraging their
inherent linear projection structures. The framework supports two operational
paradigms: (1) joint optimization of LoRA experts and routing mechanisms via a
novel hard-soft routing strategy, or (2) direct deployment of pre-trained,
frozen LoRA modules sourced from external repositories. To enable robust router
training with limited data while ensuring stable routing decisions and
maximizing expert reuse, we introduce an adaptive Specialization Balance Loss
(SBL) that jointly optimizes expert balance and task-specific alignment.
Extensive experiments on seven benchmark datasets, including MedQA, CoLA,
SST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of
LoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer
achieves significant improvements of 7.61%, 4.88%, and 3.08% over the base
models, respectively. Compared with state-of-the-art methods, LoRA-Mixer
achieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,
using only 48% of the parameters, demonstrating its efficiency and strong
performance.

</details>


### [160] [Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.00030)
*Abhishek Verma,Nallarasan V,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 提出一种结合上下文赌博机（contextual bandits）与深度强化学习（DRL）的新范式，通过自适应选择动作持续时间，增强策略灵活性和计算效率，并在Atari游戏上验证了其显著性能提升。


<details>
  <summary>Details</summary>
Motivation: DRL在复杂任务中取得了显著成功，但动作执行的时间尺度是一个关键但未充分探索的方面，固定的动作持续时间限制了策略的灵活性和计算效率。

Method: 提出一种新颖的范式，将上下文赌博机与DRL（具体为深度Q网络DQN）结合，通过一个上下文赌博机模块增强DQN，使其能根据状态上下文学习选择最佳的动作重复率，从而自适应地选择动作持续时间。

Result: 在Atari 2600游戏上的实验表明，相比于静态持续时间的基线方法，该方法取得了显著的性能提升，突显了自适应时间抽象在DRL中的有效性。

Conclusion: 该范式为游戏和机器人等需要动态动作持续时间的实时应用提供了一个可扩展的解决方案，证明了其在增强DRL策略灵活性和计算效率方面的潜力。

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in complex
sequential decision-making tasks, such as playing Atari 2600 games and
mastering board games. A critical yet underexplored aspect of DRL is the
temporal scale of action execution. We propose a novel paradigm that integrates
contextual bandits with DRL to adaptively select action durations, enhancing
policy flexibility and computational efficiency. Our approach augments a Deep
Q-Network (DQN) with a contextual bandit module that learns to choose optimal
action repetition rates based on state contexts. Experiments on Atari 2600
games demonstrate significant performance improvements over static duration
baselines, highlighting the efficacy of adaptive temporal abstractions in DRL.
This paradigm offers a scalable solution for real-time applications like gaming
and robotics, where dynamic action durations are critical.

</details>


### [161] [Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru](https://arxiv.org/abs/2507.00031)
*Chuan Li,Jiang You,Hassine Moungla,Vincent Gauthier,Miguel Nunez-del-Prado,Hugo Alatrista-Salas*

Main category: cs.LG

TL;DR: 该研究利用秘鲁COVID-19期间的数字接触追踪数据，提出一种空间邻域融合（SPN）技术，以解决稀疏移动数据预测的挑战，显著提升了跨城市区域人流预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 准确建模人类移动性对于理解疫情传播和部署及时干预措施至关重要。当前挑战在于城市区域之间小时移动计数的高度空间稀疏性，这限制了传统时间序列模型的预测能力。

Method: 研究利用秘鲁国家数字接触追踪（DCT）应用在COVID-19疫情期间收集的大规模时空数据集。为解决空间稀疏性问题，提出了一种轻量级、模型无关的空间邻域融合（SPN）技术，通过聚合其H3相邻单元格的信号来增强每个单元格的特征。该策略在NLinear、PatchTST和K-U-Net三种预测骨干模型上进行评估。

Result: 实验结果表明，SPN技术持续提升了预测性能，在测试MSE上实现了高达9.85%的降低。

Conclusion: 研究结果表明，对稀疏移动信号进行空间平滑处理提供了一种简单但有效的方法，以实现公共卫生危机期间稳健的时空预测。

Abstract: Accurate modeling of human mobility is critical for understanding epidemic
spread and deploying timely interventions. In this work, we leverage a
large-scale spatio-temporal dataset collected from Peru's national Digital
Contact Tracing (DCT) application during the COVID-19 pandemic to forecast
mobility flows across urban regions. A key challenge lies in the spatial
sparsity of hourly mobility counts across hexagonal grid cells, which limits
the predictive power of conventional time series models. To address this, we
propose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN)
technique that augments each cell's features with aggregated signals from its
immediate H3 neighbors. We evaluate this strategy on three forecasting
backbones: NLinear, PatchTST, and K-U-Net, under various historical input
lengths. Experimental results show that SPN consistently improves forecasting
performance, achieving up to 9.85 percent reduction in test MSE. Our findings
demonstrate that spatial smoothing of sparse mobility signals provides a simple
yet effective path toward robust spatio-temporal forecasting during public
health crises.

</details>


### [162] [Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark](https://arxiv.org/abs/2507.00034)
*Reece Bourisaw,Reid McCants,Jean-Marie Le Corre,Anna Iskhakova,Arsen S. Iskhakov*

Main category: cs.LG

TL;DR: 本研究为OECD/NEA AI/ML CHF基准测试第二阶段编译并数字化了一个全面的临界热通量(CHF)数据集，涵盖均匀和非均匀轴向加热条件，并评估了现有模型（包括基于均匀数据的神经网络）在非均匀剖面下的局限性，为后续的先进模型开发奠定基础。


<details>
  <summary>Details</summary>
Motivation: 支持OECD/NEA AI/ML CHF基准测试第二阶段，该阶段引入了空间变化的功率分布，以解决轻水反应堆中定义安全热工水力运行极限的关键问题。现有临界热通量（CHF）预测模型（包括经典关联式、表格法和仅基于均匀数据训练的神经网络）在处理非均匀加热条件时表现出显著误差，因此需要能够明确纳入轴向功率分布的模型。

Method: 编译并数字化了一个涵盖均匀和非均匀轴向加热条件的广泛CHF数据集。加热剖面从技术报告中提取，插值到一致的轴向网格上，通过能量平衡检查进行验证，并编码为机器可读格式。同时，评估了经典的CHF关联式、现代表格法以及一个仅用均匀数据训练的神经网络作为基线模型。

Result: 经典CHF关联式在均匀加热下也存在显著误差，在非均匀剖面下性能显著下降；现代表格法虽有改进但仍不完善。仅用均匀数据训练的神经网络在均匀条件下表现良好，但无法泛化到空间变化的场景，这凸显了开发明确考虑轴向功率分布模型的必要性。

Conclusion: 本研究通过提供精选数据集和基线建模结果，为CHF基准测试下一阶段的先进迁移学习策略、严格的不确定性量化以及设计优化工作奠定了基础，对提升反应堆安全性和效率至关重要。

Abstract: Critical heat flux (CHF) marks the onset of boiling crisis in light-water
reactors, defining safe thermal-hydraulic operating limits. To support Phase II
of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power
profiles, this work compiles and digitizes a broad CHF dataset covering both
uniform and non-uniform axial heating conditions. Heating profiles were
extracted from technical reports, interpolated onto a consistent axial mesh,
validated via energy-balance checks, and encoded in machine-readable formats
for benchmark compatibility.
  Classical CHF correlations exhibit substantial errors under uniform heating
and degrade markedly when applied to non-uniform profiles, while modern tabular
methods offer improved but still imperfect predictions. A neural network
trained solely on uniform data performs well in that regime but fails to
generalize to spatially varying scenarios, underscoring the need for models
that explicitly incorporate axial power distributions. By providing these
curated datasets and baseline modeling results, this study lays the groundwork
for advanced transfer-learning strategies, rigorous uncertainty quantification,
and design-optimization efforts in the next phase of the CHF benchmark.

</details>


### [163] [IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting](https://arxiv.org/abs/2507.00036)
*Rohan Putatunda,Sanjay Purushotham,Ratnaksha Lele,Vandana P. Janeja*

Main category: cs.LG

TL;DR: 提出IDRIFTNET，一种物理驱动的深度学习模型，有效提升了在有限数据和复杂环境下冰山轨迹预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 准确预测冰山轨迹面临巨大挑战，主要因为时空数据稀缺、冰山运动的复杂非线性特性以及环境因素的动态影响，这限制了现有深度学习模型的预测能力。

Method: 提出混合型IDRIFTNET模型，它是一种物理驱动的深度学习模型。该模型结合了冰山漂移物理的解析公式与增强残差学习模型（学习解析解与真实观测之间的不匹配模式），并辅以旋转增强谱神经网络（捕捉数据的全局和局部模式）来预测未来的冰山漂移位置。

Result: IDRIFTNET模型在预测南极冰山A23A和B22A的轨迹时，表现优于现有最先进模型，在不同时间点均实现了更低的最终位移误差（FDE）和平均位移误差（ADE）。

Conclusion: IDRIFTNET在有限数据和动态环境条件下，能有效捕捉冰山复杂非线性漂移，准确预测冰山轨迹。

Abstract: Drifting icebergs in the polar oceans play a key role in the Earth's climate
system, impacting freshwater fluxes into the ocean and regional ecosystems
while also posing a challenge to polar navigation. However, accurately
forecasting iceberg trajectories remains a formidable challenge, primarily due
to the scarcity of spatiotemporal data and the complex, nonlinear nature of
iceberg motion, which is also impacted by environmental variables. The iceberg
motion is influenced by multiple dynamic environmental factors, creating a
highly variable system that makes trajectory identification complex. These
limitations hinder the ability of deep learning models to effectively capture
the underlying dynamics and provide reliable predictive outcomes. To address
these challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep
learning model that combines an analytical formulation of iceberg drift
physics, with an augmented residual learning model. The model learns the
pattern of mismatch between the analytical solution and ground-truth
observations, which is combined with a rotate-augmented spectral neural network
that captures both global and local patterns from the data to forecast future
iceberg drift positions. We compare IDRIFTNET model performance with
state-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings
demonstrate that IDRIFTNET outperforms other models by achieving a lower Final
Displacement Error (FDE) and Average Displacement Error (ADE) across a variety
of time points. These results highlight IDRIFTNET's effectiveness in capturing
the complex, nonlinear drift of icebergs for forecasting iceberg trajectories
under limited data and dynamic environmental conditions.

</details>


### [164] [Model Fusion via Neuron Interpolation](https://arxiv.org/abs/2507.00037)
*Phoomraphee Luenam,Andreas Spanopoulos,Amit Sant,Thomas Hofmann,Sotiris Anagnostidis,Sidak Pal Singh*

Main category: cs.LG

TL;DR: 本文提出一种新颖的、以神经元为中心的模型融合算法系列，通过整合神经元归因分数，有效融合多个神经网络，无论训练数据分布如何，并在零样本和非独立同分布（non-IID）融合场景中表现显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 模型融合旨在结合多个模型的知识以创建单一代表性模型，但由于内部表示（如排列不变性、随机初始化或数据分布不同）的差异，这一过程并非易事。

Method: 本文提出一种新颖的、以神经元为中心的模型融合算法系列。该算法通过分组父模型的中间神经元来创建目标表示，并由融合模型近似。与现有方法不同，它将神经元归因分数纳入融合过程，并可推广到任意层类型，适用于不同的训练数据分布。

Result: 在各种基准数据集上的实验结果表明，该算法持续优于以往的融合技术，特别是在零样本（zero-shot）和非独立同分布（non-IID）融合场景下。

Conclusion: 该研究提供了一种先进的模型融合方法，通过创新的神经元分组和归因分数整合，有效克服了传统融合的挑战，尤其在复杂和数据受限的融合场景中展现出卓越性能。

Abstract: Model fusion aims to combine the knowledge of multiple models by creating one
representative model that captures the strengths of all of its parents.
However, this process is non-trivial due to differences in internal
representations, which can stem from permutation invariance, random
initialization, or differently distributed training data. We present a novel,
neuron-centric family of model fusion algorithms designed to integrate multiple
trained neural networks into a single network effectively regardless of
training data distribution. Our algorithms group intermediate neurons of parent
models to create target representations that the fused model approximates with
its corresponding sub-network. Unlike prior approaches, our approach
incorporates neuron attribution scores into the fusion process. Furthermore,
our algorithms can generalize to arbitrary layer types. Experimental results on
various benchmark datasets demonstrate that our algorithms consistently
outperform previous fusion techniques, particularly in zero-shot and non-IID
fusion scenarios. The code is available at
https://github.com/AndrewSpano/neuron-interpolation-model-fusion.

</details>


### [165] [Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information](https://arxiv.org/abs/2507.00038)
*Fei Chen,Wenchi Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种基于逐点V信息（PVI）的数据缩减策略，旨在通过识别并利用最具信息量的实例来提升AI模型的训练效率和性能，并成功应用于中文NLP任务。


<details>
  <summary>Details</summary>
Motivation: 在以数据为中心的AI中，大规模数据集的训练效率和数据质量是关键挑战。研究旨在解决如何从庞大数据集中选择最优实例，而非使用全部数据，以提高训练效率和模型表现。

Method: 该研究提出一种基于逐点V信息（PVI）的数据缩减策略。首先，利用PVI量化实例难度，并通过过滤低难度实例实现静态数据缩减。其次，采用渐进式学习方法，根据PVI升序对实例进行训练，以加速收敛。此外，将PVI框架从英语数据集推广到多种中文NLP任务和基础模型。

Result: 实验结果显示，静态缩减10%-30%的数据仅导致0.0001%至0.76%的准确率损失，同时保持分类器性能。渐进式学习方法不仅加速了收敛，还比传统训练获得了0.8%的准确率提升。该策略能增强模型性能并提高训练效率。PVI框架成功应用于中文NLP任务，为跨语言数据缩减和快速训练提供了有价值的见解。

Conclusion: 所提出的基于PVI的数据缩减策略能有效选择最优数据子集，从而显著提升模型性能和训练效率。该框架在跨语言（如中文NLP）任务中的成功应用，进一步证实了其通用性和有效性。

Abstract: Data reduction plays a vital role in data-centric AI by identifying the most
informative instance within large-scale datasets to enhance model training
efficiency. The core challenge lies in how to select the optimal
instances-rather than the entire datasets-to improve data quality and training
efficiency. In this paper, we propose an effective data reduction strategy
based on Pointwise V-information(PVI). First, we quantify instance difficulty
using PVI and filter out low-difficulty instances enabling a static approach.
Experiments demonstrate that removing 10%-30% of the data preserves the
classifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we
use a progressive learning approach to training the classifiers on instances
sorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy
gain over conventional training. Our results suggest that with the effective
data reduction strategy, training a classifier on the selected optimal subset
could enhance the model performance and boost training efficiency. Moreover, we
have transferred the PVI framework, which previously applied only to English
datasets, to diverse Chinese NLP tasks and base models, leading to valuable
insights for cross-lingual data reduction and faster training. The codes are
released at https://github.com/zhouwenchi/DatasetReductionStrategy.

</details>


### [166] [Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing](https://arxiv.org/abs/2507.00039)
*Lucas Potin,Rosa Figueiredo,Vincent Labatut,Christine Largeron*

Main category: cs.LG

TL;DR: 本文对图分类中38种基于模式的质量度量进行理论和经验比较，并提出一种新的聚类预处理方法，发现流行度量并非总能带来最佳结果。


<details>
  <summary>Details</summary>
Motivation: 图分类中，基于子图模式的方法解释性好，但用于评估模式区分能力的质量度量繁多且缺乏针对图的系统性比较，导致流行度量被盲目使用，效果未经充分评估。

Method: 理论上，基于四种数学特性对38种质量度量进行特征化。经验上，构建基准数据集和模式的“黄金标准”排名方法，并比较这些度量在模式排名和分类性能上的表现。此外，提出一种基于聚类的预处理步骤来优化分类性能。

Result: 基于聚类的预处理步骤有效，能减少处理模式的数量同时保持相当的分类性能。实验结果还显示，一些文献中广泛使用的流行度量并未与最佳分类结果相关联。

Conclusion: 本研究全面分析了图分类中模式质量度量，提供了理论和实证见解。所提出的预处理步骤能提升效率，并揭示了盲目采用流行度量可能导致次优结果，强调了基于证据选择度量的重要性。

Abstract: Graph classification aims to categorize graphs based on their structural and
attribute features, with applications in diverse fields such as social network
analysis and bioinformatics. Among the methods proposed to solve this task,
those relying on patterns (i.e. subgraphs) provide good explainability, as the
patterns used for classification can be directly interpreted. To identify
meaningful patterns, a standard approach is to use a quality measure, i.e. a
function that evaluates the discriminative power of each pattern. However, the
literature provides tens of such measures, making it difficult to select the
most appropriate for a given application. Only a handful of surveys try to
provide some insight by comparing these measures, and none of them specifically
focuses on graphs. This typically results in the systematic use of the most
widespread measures, without thorough evaluation. To address this issue, we
present a comparative analysis of 38 quality measures from the literature. We
characterize them theoretically, based on four mathematical properties. We
leverage publicly available datasets to constitute a benchmark, and propose a
method to elaborate a gold standard ranking of the patterns. We exploit these
resources to perform an empirical comparison of the measures, both in terms of
pattern ranking and classification performance. Moreover, we propose a
clustering-based preprocessing step, which groups patterns appearing in the
same graphs to enhance classification performance. Our experimental results
demonstrate the effectiveness of this step, reducing the number of patterns to
be processed while achieving comparable performance. Additionally, we show that
some popular measures widely used in the literature are not associated with the
best results.

</details>


### [167] [Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation](https://arxiv.org/abs/2507.00055)
*Varsha Pendyala,Pedro Morgado,William Sethares*

Main category: cs.LG

TL;DR: 提出LiSER，一个利用无标注音视频数据进行多模态情感识别（SER）的知识蒸馏框架，以减少对大量标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 语音接口中的情感识别（SER）对用户体验至关重要，多模态（音视频）SER效果更佳。然而，开发此类系统所需的大量标注数据收集成本高昂。

Method: 本文提出LiSER知识蒸馏框架，利用基于先进语音和面部表示模型的“教师模型”，将语音情感和面部表情知识蒸馏到“轻量级学生模型”，从而利用无标注的音视频数据进行SER。

Result: 在RAVDESS和CREMA-D数据集上的实验表明，LiSER能显著降低SER任务对大量标注数据集的依赖。

Conclusion: LiSER框架为在标注数据有限的情况下开发高效的多模态语音情感识别系统提供了一种有效途径。

Abstract: Voice interfaces integral to the human-computer interaction systems can
benefit from speech emotion recognition (SER) to customize responses based on
user emotions. Since humans convey emotions through multi-modal audio-visual
cues, developing SER systems using both the modalities is beneficial. However,
collecting a vast amount of labeled data for their development is expensive.
This paper proposes a knowledge distillation framework called LightweightSER
(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher
models built on advanced speech and face representation models. LiSER transfers
knowledge regarding speech emotions and facial expressions from the teacher
models to lightweight student models. Experiments conducted on two benchmark
datasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence
on extensive labeled datasets for SER tasks.

</details>


### [168] [Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data](https://arxiv.org/abs/2507.00061)
*Hoang-Dieu Vu,Duc-Nghia Tran,Quang-Tu Pham,Hieu H. Pham,Nicolas Vuillerme,Duc-Tan Tran*

Main category: cs.LG

TL;DR: 本文提出Smooth-Distill，一个新型自蒸馏框架，利用统一的CNN模型（MTL-net）同步进行人体活动识别（HAR）和传感器位置检测，通过使用模型自身平滑的历史版本作为教师模型，显著降低计算开销，并在多任务学习中表现出优越性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有蒸馏方法计算成本高昂，且难以同时有效执行人体活动识别和传感器位置检测。研究旨在开发一种高效、低计算开销的多任务学习方法，以适应可穿戴传感器数据的HAR系统，特别是在资源受限或需要频繁模型更新的场景。

Method: 核心方法是Smooth-Distill自蒸馏框架。它采用统一的CNN-based架构MTL-net，处理加速度计数据并分支输出HAR和传感器位置检测结果。关键创新在于使用模型自身平滑的历史版本作为教师模型进行知识蒸馏，而非独立的教师模型，从而大幅减少训练计算开销。研究还开发了一个包含12种睡眠姿势的新型加速度计数据集，并结合MHealth和WISDM公共数据集进行实验。

Result: 实验结果表明，Smooth-Distill在不同评估场景下均持续优于其他替代方法，在人体活动识别和设备位置检测任务中均取得了显著改进。该方法在训练过程中展现出更强的收敛稳定性，并且相比传统多任务学习基线，过拟合现象有所减少。

Conclusion: Smooth-Distill框架为人体活动识别系统中的知识蒸馏实际应用做出了贡献，为使用加速度计数据的多任务学习提供了一个有效解决方案，平衡了准确性和训练效率。它降低了模型训练的计算成本，这对于需要频繁模型更新或在资源受限平台上的场景至关重要。

Abstract: This paper introduces Smooth-Distill, a novel self-distillation framework
designed to simultaneously perform human activity recognition (HAR) and sensor
placement detection using wearable sensor data. The proposed approach utilizes
a unified CNN-based architecture, MTL-net, which processes accelerometer data
and branches into two outputs for each respective task. Unlike conventional
distillation methods that require separate teacher and student models, the
proposed framework utilizes a smoothed, historical version of the model itself
as the teacher, significantly reducing training computational overhead while
maintaining performance benefits. To support this research, we developed a
comprehensive accelerometer-based dataset capturing 12 distinct sleep postures
across three different wearing positions, complementing two existing public
datasets (MHealth and WISDM). Experimental results show that Smooth-Distill
consistently outperforms alternative approaches across different evaluation
scenarios, achieving notable improvements in both human activity recognition
and device placement detection tasks. This method demonstrates enhanced
stability in convergence patterns during training and exhibits reduced
overfitting compared to traditional multitask learning baselines. This
framework contributes to the practical implementation of knowledge distillation
in human activity recognition systems, offering an effective solution for
multitask learning with accelerometer data that balances accuracy and training
efficiency. More broadly, it reduces the computational cost of model training,
which is critical for scenarios requiring frequent model updates or training on
resource-constrained platforms. The code and model are available at
https://github.com/Kuan2vn/smooth\_distill.

</details>


### [169] [Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory](https://arxiv.org/abs/2507.00073)
*Urvi Pawar,Kunal Telangi*

Main category: cs.LG

TL;DR: 提出分数阶策略梯度（FPG），利用分数阶微积分在强化学习中建模长期时间依赖，有效降低方差并提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 标准策略梯度方法受马尔可夫假设限制，导致方差高、采样效率低，需要一种能有效建模长期时间依赖的强化学习框架。

Method: 提出分数阶策略梯度（FPG），通过Caputo分数阶导数重新构建梯度，建立状态转移间的幂律时间关联。开发了分数阶时序差分误差的高效递归计算技术，具有常数时间和内存开销。

Result: 理论分析表明，FPG能将渐近方差降低O(t^(-alpha))量级并保持收敛性。实证验证显示，相比现有基线，样本效率提升35-68%，方差降低24-52%。

Conclusion: FPG提供了一种数学上严谨的方法，无需额外计算开销即可在策略优化中利用长期依赖关系，克服了传统方法的关键局限性。

Abstract: We propose Fractional Policy Gradients (FPG), a reinforcement learning
framework incorporating fractional calculus for long-term temporal modeling in
policy optimization. Standard policy gradient approaches face limitations from
Markovian assumptions, exhibiting high variance and inefficient sampling. By
reformulating gradients using Caputo fractional derivatives, FPG establishes
power-law temporal correlations between state transitions. We develop an
efficient recursive computation technique for fractional temporal-difference
errors with constant time and memory requirements. Theoretical analysis shows
FPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus
standard policy gradients while preserving convergence. Empirical validation
demonstrates 35-68% sample efficiency gains and 24-52% variance reduction
versus state-of-the-art baselines. This framework provides a mathematically
grounded approach for leveraging long-range dependencies without computational
overhead.

</details>


### [170] [Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap](https://arxiv.org/abs/2507.00075)
*Yifan Sun,Yushan Liang,Zhen Zhang,Jiaye Teng*

Main category: cs.LG

TL;DR: 本文通过求解器-验证器差距理论模型，探讨了大型语言模型（LLM）自改进的训练动态及其性能演变，并能预测自改进的最终效果。此外，还分析了有限外部数据如何影响自改进过程。


<details>
  <summary>Details</summary>
Motivation: 尽管自改进技术对提升LLM性能至关重要，但LLM在自改进过程中性能如何演变，目前研究不足。

Method: 通过引入“求解器-验证器差距”概念，对LLM自改进的训练动态进行了理论建模。在此框架下，研究了如何根据早期训练信息预测自改进的最终性能，并扩展分析了外部数据对自改进过程的影响。最后，通过实证实验验证了模型的有效性。

Result: 成功构建了自改进训练动态的理论模型，该模型能有效预测自改进的最终性能。研究还发现，在有限外部数据情境下，外部数据在任何阶段使用都不会显著影响最终性能，这与现有经验观察相符。

Conclusion: 本文为LLM自改进的性能演变提供了理论理解和预测方法，并揭示了有限外部数据影响自改进动态的机制，为LLM的优化提供了理论指导。

Abstract: Self-improvement is among the most prominent techniques within the realm of
large language models (LLM), aiming to enhance the LLM performance without
relying on external data. Despite its significance, generally how LLM
performances evolve during the self-improvement process remains underexplored.
In this paper, we theoretically model the training dynamics of self-improvement
via the concept of solver-verifier gap. This is inspired by the conjecture that
the performance enhancement of self-improvement stems from the gap between
LLM's solver capability and verifier capability. Based on the theoretical
framework, we further introduce how to predict the ultimate power of
self-improvement using only information from the first few training epochs. We
empirically validate the effectiveness of the theoretical model on various LLMs
and datasets. Beyond self-improvement, we extend our analysis to investigate
how external data influences these dynamics within the framework. Notably, we
find that under limited external data regimes, such external data can be
utilized at any stage without significantly affecting final performances, which
accords with the empirical observations.

</details>


### [171] [The language of time: a language model perspective on time-series foundation models](https://arxiv.org/abs/2507.00078)
*Yi Xie,Yun Xiong,Zejian Shi,Hao Niu,Zhengfu Liu*

Main category: cs.LG

TL;DR: 本文通过理论和实验分析，提出时间序列基础模型成功地将语言模型表征范式泛化到潜在概率分布形式，通过将时间序列切片量化为具有类似自然语言统计特性的离散词汇，从而解释了其跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型展现出卓越的跨域泛化和迁移能力，但这与时间序列数据反映独特动力系统的直觉相悖，形成了一个基本悖论。研究旨在解决这一悖论，理解其成功背后的表征学习机制和泛化能力。

Method: 本研究从理论和实验两方面，深入探究了基于切片的时间序列基础模型的表征学习机制和泛化能力。核心方法是提出并论证这些模型将语言模型的确定性向量表征泛化为潜在的概率分布形式，并通过理论分析证明连续时间序列切片可被忠实量化为离散词汇，且其关键统计特性与自然语言高度一致。

Result: 研究结果表明，连续时间序列切片可以被忠实地量化为具有与自然语言高度一致的关键统计特性的离散词汇。这种泛化机制使得时间序列模型能够继承大型语言模型的强大表征和迁移能力，从而有力地解释了它们在时间任务中卓越的性能。

Conclusion: 本工作为理解、评估和改进大规模时间序列基础模型的安全性与可靠性提供了坚实的理论基石。

Abstract: With the rise of large language models, the paradigm of training foundation
models with massive parameter counts on vast datasets has been adopted in
multiple domains to achieve remarkable success. Time series foundation models
represent a significant extension of this paradigm, demonstrating exceptional
expressive power, generalization, and cross-domain transferability. However,
this gives rise to a fundamental paradox: time series data reflect distinct
dynamical systems, making cross-domain transfer intuitively implausible, yet
this is contradicted by the models' empirical success. To resolve this paradox,
this paper investigates, from both theoretical and experimental perspectives,
the representation learning mechanisms and generalization capabilities of
patch-based time series foundation models. We argue that such models are not
merely applying a new architecture but are fundamentally generalizing the
representation paradigm of language models by extending deterministic
vector-based representations to latent probabilistic distributional forms. Our
theoretical analysis supports this framework by demonstrating that continuous
time-series patches can be faithfully quantized into a discrete vocabulary
whose key statistical properties are highly consistent with those of natural
language. This generalization allows time series models to inherit the robust
representation and transfer abilities of large language models, thereby
explaining their superior performance in temporal tasks. Ultimately, our work
provides a rigorous theoretical cornerstone for understanding, evaluating, and
improving the safety and reliability of large-scale time series foundation
models.

</details>


### [172] [Online Meal Detection Based on CGM Data Dynamics](https://arxiv.org/abs/2507.00080)
*Ali Tavasoli,Heman Shakeri*

Main category: cs.LG

TL;DR: 本文提出一种利用连续血糖监测（CGM）数据中提取的动态模式作为特征来检测进餐事件的方法。


<details>
  <summary>Details</summary>
Motivation: 旨在通过利用血糖动态的内在特性，提高进餐检测的准确性，并增强血糖动态的可解释性。

Method: 从连续血糖监测（CGM）数据中提取动态模式作为特征，这些模式能够捕获血糖变异性的关键方面，以识别与进餐相关的模式和异常。

Result: 该方法提高了进餐检测的准确性，增强了血糖动态的可解释性。它提供了一个鲁棒的特征提取框架，有助于跨数据集的泛化，并在实际应用中表现可靠，优于传统方法。

Conclusion: 所提出的基于动态模式的进餐检测技术，在提高检测准确性、增强可解释性及实现真实世界应用的鲁棒性与泛化能力方面，展现出显著优势。

Abstract: We utilize dynamical modes as features derived from Continuous Glucose
Monitoring (CGM) data to detect meal events. By leveraging the inherent
properties of underlying dynamics, these modes capture key aspects of glucose
variability, enabling the identification of patterns and anomalies associated
with meal consumption. This approach not only improves the accuracy of meal
detection but also enhances the interpretability of the underlying glucose
dynamics. By focusing on dynamical features, our method provides a robust
framework for feature extraction, facilitating generalization across diverse
datasets and ensuring reliable performance in real-world applications. The
proposed technique offers significant advantages over traditional approaches,
improving detection accuracy,

</details>


### [173] [Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission](https://arxiv.org/abs/2507.00082)
*Faranaksadat Solat,Joohyung Lee,Mohamed Seif,Dusit Niyato,H. Vincent Poor*

Main category: cs.LG

TL;DR: FedHLM是一种通信高效的混合语言模型（HLM）框架，通过联邦学习（FL）协作学习不确定性阈值并利用点对点（P2P）解析，显著减少大型语言模型（LLM）的通信开销，适用于边缘AI。


<details>
  <summary>Details</summary>
Motivation: 现有混合语言模型（HLM）在本地小模型（SLM）预测不确定时仍需频繁卸载给大型语言模型（LLM），导致在带宽受限环境中产生巨大的通信开销。

Method: 本文提出FedHLM框架，其核心在于将不确定性感知推理与联邦学习（FL）相结合。它通过FL协作学习令牌级不确定性阈值，以动态决定何时调用LLM；利用基于嵌入的令牌表示实现P2P解析，允许客户端复用相似对等体的推理结果；并引入分层模型聚合机制，通过边缘服务器和跨集群协调来优化本地路由策略和全局决策边界，从而捕获重复的不确定性模式。

Result: 在大型新闻分类任务上的实验结果表明，FedHLM在精度损失可忽略不计的情况下，将LLM传输量减少了95%以上。

Conclusion: FedHLM是一个为可扩展和高效边缘AI应用量身定制的通信高效HLM解决方案。

Abstract: Hybrid Language Models (HLMs) combine the low-latency efficiency of Small
Language Models (SLMs) on edge devices with the high accuracy of Large Language
Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM
inference, HLMs reduce latency and communication by invoking LLMs only when
local SLM predictions are uncertain, i.e., when token-level confidence is low
or entropy is high. However, ambiguous or low-confidence predictions still
require frequent offloading to the LLM, leading to significant communication
overhead in bandwidth-constrained settings. To address this, we propose FedHLM,
a communication-efficient HLM framework that integrates uncertainty-aware
inference with Federated Learning (FL). FedHLM's key innovation lies in
collaboratively learning token-level uncertainty thresholds that govern when
LLM assistance is needed. Rather than using static or manually tuned
thresholds, FedHLM employs FL to optimize these thresholds in a
privacy-preserving, distributed manner. Additionally, it leverages
embedding-based token representations for Peer-to-Peer (P2P) resolution,
enabling clients to reuse tokens inferred by semantically similar peers without
engaging the LLM. We further introduce hierarchical model aggregation: edge
servers refine local routing policies through client updates, while
cross-cluster coordination aligns global decision boundaries. This layered
design captures recurring uncertainty patterns, reducing redundant LLM queries.
Experiments on large-scale news classification tasks show that FedHLM reduces
LLM transmissions by over 95 percent with negligible accuracy loss, making it
well-suited for scalable and efficient edge-AI applications.

</details>


### [174] [Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks](https://arxiv.org/abs/2507.00083)
*Wei Meng*

Main category: cs.LG

TL;DR: 本研究提出IA-STGNN模型，旨在解决战略级模拟中战术打击与战略延迟之间因果建模的缺失问题。该模型通过集成图注意力、反事实模拟等机制，实现了对战略延迟的可解释预测，并显著优于现有基线模型，为高层政策建模提供决策支持。


<details>
  <summary>Details</summary>
Motivation: 当前的战略级模拟缺乏战术打击行为与战略延迟之间的结构化因果建模，特别是在捕获“韧性-节点抑制-谈判窗口”链中的中间变量方面存在结构性瓶颈。

Method: 提出了一种新型框架——干预感知时空图神经网络（IA-STGNN），用于闭合从战术输入到战略延迟输出的因果循环。该模型集成了图注意力机制、反事实模拟单元和空间干预节点重建。训练数据来源于遵循NIST SP 800-160标准的多物理场模拟平台（GEANT4 + COMSOL）。

Result: IA-STGNN模型显著优于基线模型（ST-GNN、GCN-LSTM、XGBoost），MAE降低了12.8%，Top-5%准确率提高了18.4%，同时改善了因果路径一致性和干预稳定性。

Conclusion: IA-STGNN能够实现战略延迟的可解释预测，并支持核威慑模拟、外交窗口评估和多策略优化等应用，为高层政策建模提供结构化和透明的AI决策支持机制。

Abstract: This study addresses the lack of structured causal modeling between tactical
strike behavior and strategic delay in current strategic-level simulations,
particularly the structural bottlenecks in capturing intermediate variables
within the "resilience - nodal suppression - negotiation window" chain. We
propose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN),
a novel framework that closes the causal loop from tactical input to strategic
delay output. The model integrates graph attention mechanisms, counterfactual
simulation units, and spatial intervention node reconstruction to enable
dynamic simulations of strike configurations and synchronization strategies.
Training data are generated from a multi-physics simulation platform (GEANT4 +
COMSOL) under NIST SP 800-160 standards, ensuring structural traceability and
policy-level validation. Experimental results demonstrate that IA-STGNN
significantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost),
achieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5
percent accuracy, while improving causal path consistency and intervention
stability. IA-STGNN enables interpretable prediction of strategic delay and
supports applications such as nuclear deterrence simulation, diplomatic window
assessment, and multi-strategy optimization, providing a structured and
transparent AI decision-support mechanism for high-level policy modeling.

</details>


### [175] [A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism](https://arxiv.org/abs/2507.00085)
*Ruiyuan Jiang,Dongyao Jia,Eng Gee Lim,Pengfei Fan,Yuli Zhang,Shangbo Wang*

Main category: cs.LG

TL;DR: 本文提出GFEN模型，通过新颖的拓扑时空图融合技术和混合方法，有效解决了交通预测中时空特征整合与非平稳数据处理的挑战，显著提升了预测精度和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测方法难以整合复杂的时空特性，并且使用静态技术处理非平稳和异常历史数据，限制了适应性并损害了数据平滑效果。

Method: 提出Graph Fusion Enhanced Network (GFEN) 模型：1) 引入拓扑时空图融合技术，通过可训练方法从数据分布和网络拓扑中提取并融合时空关联，以建模多尺度时空特征。2) 采用混合方法，结合k阶差分数学框架和基于注意力机制的深度学习结构，自适应平滑历史观测数据并动态缓解数据异常和非平稳性。

Result: GFEN在预测精度上超越现有SOTA方法约6.3%，收敛速度比近期混合模型快近两倍。

Conclusion: GFEN表现出卓越的性能，并有潜力显著提升交通预测系统的效率。

Abstract: Accurate traffic prediction is essential for Intelligent Transportation
Systems (ITS), yet current methods struggle with the inherent complexity and
non-linearity of traffic dynamics, making it difficult to integrate spatial and
temporal characteristics. Furthermore, existing approaches use static
techniques to address non-stationary and anomalous historical data, which
limits adaptability and undermines data smoothing. To overcome these
challenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative
framework for network-level traffic speed prediction. GFEN introduces a novel
topological spatiotemporal graph fusion technique that meticulously extracts
and merges spatial and temporal correlations from both data distribution and
network topology using trainable methods, enabling the modeling of multi-scale
spatiotemporal features. Additionally, GFEN employs a hybrid methodology
combining a k-th order difference-based mathematical framework with an
attention-based deep learning structure to adaptively smooth historical
observations and dynamically mitigate data anomalies and non-stationarity.
Extensive experiments demonstrate that GFEN surpasses state-of-the-art methods
by approximately 6.3% in prediction accuracy and exhibits convergence rates
nearly twice as fast as recent hybrid models, confirming its superior
performance and potential to significantly enhance traffic prediction system
efficiency.

</details>


### [176] [pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation](https://arxiv.org/abs/2507.00087)
*Jiale Zhao,Pengzhi Mao,Kaifei Wang,Yiming Li,Yaping Peng,Ranfei Chen,Shuqi Lu,Xiaohong Ji,Jiaxiang Ding,Xin Zhang,Yucheng Liao,Weinan E,Weijie Zhang,Han Wen,Hao Chi*

Main category: cs.LG

TL;DR: pUniFind是首个大规模多模态蛋白质组学预训练模型，整合肽谱打分与从头测序，显著提升了肽段鉴定数量、修饰覆盖度和灵敏度，并可识别新型肽段。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在质谱数据解释中主要作为特征提取器，缺乏统一的打分框架。

Method: 本文提出了pUniFind，一个大规模多模态预训练模型，通过在超过1亿开放搜索衍生的谱图上训练，整合了端到端肽段-谱图打分与开放式、零样本从头测序。该模型通过跨模态预测对齐谱图和肽段模态，并辅以深度学习的质量控制模块。

Result: pUniFind在多个数据集上均优于传统引擎，特别是在免疫肽组学中，识别肽段数量增加了42.6%。它支持超过1,300种修饰，即使在搜索空间大300倍的情况下，识别的PSM仍比现有从头测序方法多60%。其质量控制模块额外恢复了38.5%的肽段，包括1,891个基因组中存在但参考蛋白质组中缺失的肽段。

Conclusion: pUniFind提供了一个统一、可扩展的蛋白质组学深度学习框架，显著提升了分析的灵敏度、修饰覆盖范围和可解释性。

Abstract: Deep learning has advanced mass spectrometry data interpretation, yet most
models remain feature extractors rather than unified scoring frameworks. We
present pUniFind, the first large-scale multimodal pre-trained model in
proteomics that integrates end-to-end peptide-spectrum scoring with open,
zero-shot de novo sequencing. Trained on over 100 million open search-derived
spectra, pUniFind aligns spectral and peptide modalities via cross modality
prediction and outperforms traditional engines across diverse datasets,
particularly achieving a 42.6 percent increase in the number of identified
peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind
identifies 60 percent more PSMs than existing de novo methods despite a
300-fold larger search space. A deep learning based quality control module
further recovers 38.5 percent additional peptides including 1,891 mapped to the
genome but absent from reference proteomes while preserving full fragment ion
coverage. These results establish a unified, scalable deep learning framework
for proteomic analysis, offering improved sensitivity, modification coverage,
and interpretability.

</details>


### [177] [A new machine learning framework for occupational accidents forecasting with safety inspections integration](https://arxiv.org/abs/2507.00089)
*Aho Yapi,Pierre Latouche,Arnaud Guillin,Yan Bailly*

Main category: cs.LG

TL;DR: 本文提出一个通用框架，利用安全检查数据和二元时间序列模型（尤其是LSTM）进行短期职业事故预测，生成每日预测并汇总为每周安全评估，以支持决策者提前干预。


<details>
  <summary>Details</summary>
Motivation: 为了提供可靠的短期职业事故预测，帮助决策者更好地了解和识别高风险时期，从而实现事故发生前的积极干预和资源优化配置。

Method: 该研究提出一个通用框架，将安全检查数据建模为二元时间序列以预测事故发生。方法包括生成每日预测并聚合成每周安全评估，采用滑动窗口交叉验证和聚合周期级别指标进行模型评估。框架内系统比较了逻辑回归、树模型和神经网络等多种机器学习算法。

Result: 长短期记忆（LSTM）网络在该框架中表现优于其他方法，能以0.86的平衡准确率检测即将到来的高风险时期。研究结果证实了该方法的鲁棒性，表明二元时间序列模型能够基于安全检查数据预测关键时期。所提出的方法能将常规安全检查数据转化为清晰的每周风险评分，识别事故最可能发生的时期。

Conclusion: 该方法将常规安全检查数据转化为清晰的每周风险评分，能有效识别事故最可能发生的时期。决策者可将这些风险评分整合到规划工具中，用于分类检查优先级、安排有针对性的干预措施，并将资源分配给最高风险的地点或班次，从而在事故发生前进行干预，最大化安全投入的回报。

Abstract: We propose a generic framework for short-term occupational accident
forecasting that leverages safety inspections and models accident occurrences
as binary time series. The approach generates daily predictions, which are then
aggregated into weekly safety assessments to better inform decision making. To
ensure the reliability and operational applicability of the forecasts, we apply
a sliding-window cross-validation procedure specifically designed for time
series data, combined with an evaluation based on aggregated period-level
metrics. Several machine learning algorithms, including logistic regression,
tree-based models, and neural networks, are trained and systematically compared
within this framework. Unlike the other approaches, the long short-term memory
(LSTM) network outperforms the other approaches and detects the upcoming
high-risk periods with a balanced accuracy of 0.86, confirming the robustness
of our methodology and demonstrating that a binary time series model can
anticipate these critical periods based on safety inspections. The proposed
methodology converts routine safety inspection data into clear weekly risk
scores, detecting the periods when accidents are most likely. Decision-makers
can integrate these scores into their planning tools to classify inspection
priorities, schedule targeted interventions, and funnel resources to the sites
or shifts classified as highest risk, stepping in before incidents occur and
getting the greatest return on safety investments.

</details>


### [178] [Generating Heterogeneous Multi-dimensional Data : A Comparative Study](https://arxiv.org/abs/2507.00090)
*Corbeau Michael,Claeys Emmanuelle,Serrurier Mathieu,Zaraté Pascale*

Main category: cs.LG

TL;DR: 比较多种数据生成方法（随机采样、TVAE、GAN、CTGAN、DPM）以生成消防干预模拟数据，并使用领域特定和标准指标评估合成数据质量，以优化消防资源分配。


<details>
  <summary>Details</summary>
Motivation: 消防人员及物资分配高度依赖模拟以优化响应，但传统合成数据评估方法不足以捕捉真实世界场景的细微需求，尤其在数据高度不平衡且非高斯分布的复杂消防领域。

Method: 对随机采样、表格变分自编码器（TVAE）、标准生成对抗网络（GAN）、条件表格生成对抗网络（CTGAN）和扩散概率模型（DPM）等多种数据生成方法进行比较。通过结合瓦瑟斯坦距离等标准度量和响应时间分布、干预时空分布、事故表示等领域特定指标来评估合成数据质量，以验证数据变异性、复杂关联保持、异常事件表示、原始统计分布符合度和操作相关性。

Result: 摘要中未提供具体的实验结果或各方法性能的比较分析。

Conclusion: 本研究旨在通过比较不同数据生成方法和开发新的领域特定评估指标，生成高质量的消防干预合成数据，从而促进对各种场景的有效模拟，并最终实现消防响应的全局优化。

Abstract: Allocation of personnel and material resources is highly sensible in the case
of firefighter interventions. This allocation relies on simulations to
experiment with various scenarios. The main objective of this allocation is the
global optimization of the firefighters response. Data generation is then
mandatory to study various scenarios In this study, we propose to compare
different data generation methods. Methods such as Random Sampling, Tabular
Variational Autoencoders, standard Generative Adversarial Networks, Conditional
Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are
examined to ascertain their efficacy in capturing the intricacies of
firefighter interventions. Traditional evaluation metrics often fall short in
capturing the nuanced requirements of synthetic datasets for real-world
scenarios. To address this gap, an evaluation of synthetic data quality is
conducted using a combination of domain-specific metrics tailored to the
firefighting domain and standard measures such as the Wasserstein distance.
Domain-specific metrics include response time distribution, spatial-temporal
distribution of interventions, and accidents representation. These metrics are
designed to assess data variability, the preservation of fine and complex
correlations and anomalies such as event with a very low occurrence, the
conformity with the initial statistical distribution and the operational
relevance of the synthetic data. The distribution has the particularity of
being highly unbalanced, none of the variables following a Gaussian
distribution, adding complexity to the data generation process.

</details>


### [179] [DFReg: A Physics-Inspired Framework for Global Weight Distribution Regularization in Neural Networks](https://arxiv.org/abs/2507.00101)
*Giovanni Ruggieri*

Main category: cs.LG

TL;DR: DFReg是一种受物理学启发的深度神经网络正则化方法，通过作用于全局权重分布施加泛函惩罚，以实现平滑、多样且均匀的权重配置，且无需架构修改或随机扰动。


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法（如Dropout或L2衰减）可能需要网络架构修改或引入随机扰动。研究旨在提出一种新的、无需这些限制且能施加全局结构正则化的方法。

Method: 引入DFReg方法，该方法基于密度泛函理论（DFT），对深度神经网络的全局权重分布施加泛函惩罚。此惩罚旨在鼓励形成平滑、多样且均匀分布的权重配置。

Result: DFReg方法能够实现对全局结构规整性的施加，并促进权重配置的平滑、多样化和均匀分布。与传统方法不同，它无需改变网络架构或引入随机扰动。

Conclusion: DFReg提供了一种新颖的深度神经网络正则化途径，它通过物理学启发的全局权重分布优化，克服了传统正则化方法的一些局限性，实现了无架构改变或随机扰动的全局结构正则化。

Abstract: We introduce DFReg, a physics-inspired regularization method for deep neural
networks that operates on the global distribution of weights. Drawing from
Density Functional Theory (DFT), DFReg applies a functional penalty to
encourage smooth, diverse, and well-distributed weight configurations. Unlike
traditional techniques such as Dropout or L2 decay, DFReg imposes global
structural regularity without architectural changes or stochastic
perturbations.

</details>


### [180] [Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series](https://arxiv.org/abs/2507.00102)
*Bernd Hofmann,Patrick Bruendl,Huong Giang Nguyen,Joerg Franke*

Main category: cs.LG

TL;DR: 本文提出一种数据驱动且透明的工业故障检测方法，结合机器学习、SHAP可解释性及领域特定可视化技术，应用于压接工艺，实现了高检测精度和良好可解释性，提升了工业质量控制的信任度。


<details>
  <summary>Details</summary>
Motivation: 现代制造中产品质量一致性至关重要，特别是安全关键应用。传统质量控制方法缺乏适应性且需大量领域知识。数据驱动方法（如机器学习）虽性能优异，但其“黑箱”特性限制了在强调可解释性的工业环境中的应用。因此，需要一种既数据驱动又透明的故障检测方法。

Method: 该方法集成了一个用于多类别故障分类的监督机器学习模型，采用Shapley Additive Explanations (SHAP) 进行事后可解释性分析，并结合领域特定可视化技术将模型解释映射到操作员可理解的特征。此外，研究提出了一种评估方法，通过定量扰动分析评估模型解释，并通过定性专家评估可视化效果。该方法应用于压接工艺，使用单变量、离散时间序列数据集。

Result: 该系统在故障检测方面达到了95.9%的准确率。定量选择性分析和定性专家评估均证实了生成解释的相关性和可解释性。

Conclusion: 这种以人为中心的方法旨在增强数据驱动故障检测的信任度和可解释性，从而为工业质量控制中的应用系统设计做出贡献。

Abstract: Ensuring consistent product quality in modern manufacturing is crucial,
particularly in safety-critical applications. Conventional quality control
approaches, reliant on manually defined thresholds and features, lack
adaptability to the complexity and variability inherent in production data and
necessitate extensive domain expertise. Conversely, data-driven methods, such
as machine learning, demonstrate high detection performance but typically
function as black-box models, thereby limiting their acceptance in industrial
environments where interpretability is paramount. This paper introduces a
methodology for industrial fault detection, which is both data-driven and
transparent. The approach integrates a supervised machine learning model for
multi-class fault classification, Shapley Additive Explanations for post-hoc
interpretability, and a do-main-specific visualisation technique that maps
model explanations to operator-interpretable features. Furthermore, the study
proposes an evaluation methodology that assesses model explanations through
quantitative perturbation analysis and evaluates visualisations by qualitative
expert assessment. The approach was applied to the crimping process, a
safety-critical joining technique, using a dataset of univariate, discrete time
series. The system achieves a fault detection accuracy of 95.9 %, and both
quantitative selectivity analysis and qualitative expert evaluations confirmed
the relevance and inter-pretability of the generated explanations. This
human-centric approach is designed to enhance trust and interpretability in
data-driven fault detection, thereby contributing to applied system design in
industrial quality control.

</details>


### [181] [Graph Neural Networks in Wind Power Forecasting](https://arxiv.org/abs/2507.00105)
*Javier Castellano,Ignacio Villanueva*

Main category: cs.LG

TL;DR: 研究图神经网络（GNNs）在风能预测中的应用，发现其性能与最佳CNN基准模型相当。


<details>
  <summary>Details</summary>
Motivation: 探讨图神经网络（GNNs）在风能预测问题上的适用性。

Method: 利用三个风力发电设施的五年历史数据，以数值天气预报（NWP）变量为预测因子，评估GNN模型，并与CNN基准模型进行比较，预测时程为未来24至36小时。

Result: 发现某些GNN架构的预测性能可与表现最佳的CNN基准模型媲美。

Conclusion: GNNs在风能预测领域具有与传统CNN模型相当的潜力，证明其是一种有效的预测方法。

Abstract: We study the applicability of GNNs to the problem of wind energy forecasting.
We find that certain architectures achieve performance comparable to our best
CNN-based benchmark. The study is conducted on three wind power facilities
using five years of historical data. Numerical Weather Prediction (NWP)
variables were used as predictors, and models were evaluated on a 24 to 36 hour
ahead test horizon.

</details>


### [182] [Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros](https://arxiv.org/abs/2507.00184)
*Jacob Schrum,Olivia Kilday,Emilio Salas,Bess Hagan,Reid Williams*

Main category: cs.LG

TL;DR: 本文探索了扩散模型在文本到游戏关卡生成方面的应用，解决了数据准备和生成完整可玩关卡等实际挑战。研究发现，使用简单的Transformer模型进行文本嵌入效果最佳，且训练时间更短，表明无需依赖大型语言模型。此外，还提供了一个用于构建长关卡的图形用户界面。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型能够无条件生成基于图块的游戏关卡，但文本到关卡生成领域尚待充分探索。开发一个可用模型存在实际考量，包括需要关卡描述/关卡对、合适的文本嵌入模型，以及生成整个可玩关卡而非单个场景的能力。

Method: 研究提出了自动为现有关卡数据集分配描述性标题的策略。训练扩散模型时，同时使用了预训练文本编码器和从头开始训练的简单Transformer模型作为文本嵌入。通过比较输入与输出标题的重叠度来评估生成关卡，并评估其多样性和可玩性。实验结果与无条件扩散模型、生成对抗网络以及其他文本到关卡方法（如Five-Dollar Model和MarioGPT）进行了比较。

Result: 最佳的扩散模型使用了简单的Transformer模型进行文本嵌入，其训练时间比采用更复杂文本编码器的扩散模型更短。这表明在文本到关卡生成任务中，并不需要依赖大型语言模型。此外，研究还开发了一个图形用户界面（GUI），允许设计师利用模型生成的场景构建更长的关卡。

Conclusion: 本研究成功地探索了扩散模型在文本到关卡生成中的应用，并证明了其有效性。关键结论是，简单的Transformer模型在文本嵌入方面表现出色且效率更高，避免了对复杂大型语言模型的依赖。所开发的GUI也增强了模型在实际应用中的可用性。

Abstract: Recent research shows how diffusion models can unconditionally generate
tile-based game levels, but use of diffusion models for text-to-level
generation is underexplored. There are practical considerations for creating a
usable model: caption/level pairs are needed, as is a text embedding model, and
a way of generating entire playable levels, rather than individual scenes. We
present strategies to automatically assign descriptive captions to an existing
level dataset, and train diffusion models using both pretrained text encoders
and simple transformer models trained from scratch. Captions are automatically
assigned to generated levels so that the degree of overlap between input and
output captions can be compared. We also assess the diversity and playability
of the resulting levels. Results are compared with an unconditional diffusion
model and a generative adversarial network, as well as the text-to-level
approaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model
uses a simple transformer model for text embedding, and takes less time to
train than diffusion models employing more complex text encoders, indicating
that reliance on larger language models is not necessary. We also present a GUI
allowing designers to construct long levels from model-generated scenes.

</details>


### [183] [Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions](https://arxiv.org/abs/2507.00191)
*Eray Erturk,Fahad Kamran,Salar Abbaspourazad,Sean Jewell,Harsh Sharma,Yujie Li,Sinead Williamson,Nicholas J Foti,Joseph Futoma*

Main category: cs.LG

TL;DR: 研究为可穿戴设备中的行为信号开发了基础模型，利用大量数据进行了优化，并在57项健康任务中展现出强大性能，突显了定制化模型设计对健康应用的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型主要应用于低级传感器数据，但行为数据与生理相关的时间尺度和量值更一致，可能更具信息量。因此，研究旨在开发基于可穿戴设备行为信号的基础模型以改善健康预测。

Method: 利用来自16.2万个体、超过25亿小时的可穿戴设备数据，开发了针对行为信号的基础模型。研究系统性地优化了模型的架构和分词策略，以适应这种独特的数据集。

Result: 模型在57项健康相关任务上表现出强大性能，包括个体级别分类和时变健康状态预测等多样化实际应用。特别是在行为驱动任务（如睡眠预测）中表现出色，并且与原始传感器数据表示结合时性能进一步提升。

Conclusion: 研究结果强调了根据可穿戴设备数据特点定制基础模型设计的重要性，并展示了其赋能新型健康应用的巨大潜力。

Abstract: Wearable devices record physiological and behavioral signals that can improve
health predictions. While foundation models are increasingly used for such
predictions, they have been primarily applied to low-level sensor data, despite
behavioral data often being more informative due to their alignment with
physiologically relevant timescales and quantities. We develop foundation
models of such behavioral signals using over 2.5B hours of wearable data from
162K individuals, systematically optimizing architectures and tokenization
strategies for this unique dataset. Evaluated on 57 health-related tasks, our
model shows strong performance across diverse real-world applications including
individual-level classification and time-varying health state prediction. The
model excels in behavior-driven tasks like sleep prediction, and improves
further when combined with representations of raw sensor data. These results
underscore the importance of tailoring foundation model design to wearables and
demonstrate the potential to enable new health applications.

</details>


### [184] [What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness](https://arxiv.org/abs/2507.00195)
*Kumar Kshitij Patel*

Main category: cs.LG

TL;DR: 本论文深入理论分析了在数据异构环境下分布式和联邦优化中局部更新算法（特别是Local SGD）的性能，阐明了其优势条件并提供了分析框架和收敛界。


<details>
  <summary>Details</summary>
Motivation: 旨在提升对在真实数据异构模型下，分布式和联邦优化中局部更新算法（尤其是Local SGD）的理论理解，并探究局部更新算法何时以及为何能超越中心化或mini-batch方法。

Method: 核心关注并分析了有界二阶异构假设；建立了多种局部更新算法的紧密上下界，并刻画了问题复杂度；开发了一个基于细粒度共识误差的分析框架；将研究扩展到在线联邦学习，并分析了一阶和bandit反馈下的遗憾界。

Result: 证明了有界二阶异构假设是局部更新算法在凸和非凸设置下优于中心化或mini-batch方法的必要和充分条件；为多种局部更新算法获得了紧密的有限时间收敛界和min-max复杂度；在三阶平滑和宽松异构假设下，获得了更精确的收敛界；为在线联邦学习导出了基本遗憾界。

Conclusion: 这些研究结果清晰地阐明了局部更新算法提供可证明优势的时间和原因，本论文可作为在异构环境中分析Local SGD的独立指南。

Abstract: This thesis contributes to the theoretical understanding of local update
algorithms, especially Local SGD, in distributed and federated optimization
under realistic models of data heterogeneity. A central focus is on the bounded
second-order heterogeneity assumption, which is shown to be both necessary and
sufficient for local updates to outperform centralized or mini-batch methods in
convex and non-convex settings. The thesis establishes tight upper and lower
bounds in several regimes for various local update algorithms and characterizes
the min-max complexity of multiple problem classes. At its core is a
fine-grained consensus-error-based analysis framework that yields sharper
finite-time convergence bounds under third-order smoothness and relaxed
heterogeneity assumptions. The thesis also extends to online federated
learning, providing fundamental regret bounds under both first-order and bandit
feedback. Together, these results clarify when and why local updates offer
provable advantages, and the thesis serves as a self-contained guide for
analyzing Local SGD in heterogeneous environments.

</details>


### [185] [PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense Spatial Networks for Encrypted Lossy Image Reconstruction](https://arxiv.org/abs/2507.00230)
*Peilin He,James Joshi*

Main category: cs.LG

TL;DR: 一种基于联邦学习的隐私保护残差密集空间网络（PPFL-RDSN），用于在协作场景中安全地进行图像重建，解决隐私和计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 在协作场景下，使用残差密集空间网络（RDSNs）从低分辨率输入重建高质量图像时，集中式训练存在严重隐私风险（如数据泄露、推理攻击）和高计算成本。

Method: 提出了一种名为PPFL-RDSN的新型隐私保护联邦学习框架，该框架整合了联邦学习（FL）、局部差分隐私和鲁棒模型水印技术，以确保数据安全、隐私保护和模型真实性。

Result: PPFL-RDSN在性能上与最先进的集中式方法相当，同时降低了计算负担，并有效缓解了安全和隐私漏洞。

Conclusion: PPFL-RDSN为安全的、隐私保护的协作计算机视觉应用（特别是有损图像重建）提供了一个实用且有效的解决方案。

Abstract: Reconstructing high-quality images from low-resolution inputs using Residual
Dense Spatial Networks (RDSNs) is crucial yet challenging, particularly in
collaborative scenarios where centralized training poses significant privacy
risks, including data leakage and inference attacks, as well as high
computational costs. We propose a novel Privacy-Preserving Federated
Learning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image
reconstruction. PPFL-RDSN integrates Federated Learning (FL), local
differential privacy, and robust model watermarking techniques, ensuring data
remains secure on local devices, safeguarding sensitive information, and
maintaining model authenticity without revealing underlying data. Empirical
evaluations show that PPFL-RDSN achieves comparable performance to the
state-of-the-art centralized methods while reducing computational burdens, and
effectively mitigates security and privacy vulnerabilities, making it a
practical solution for secure and privacy-preserving collaborative computer
vision applications.

</details>


### [186] [Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations](https://arxiv.org/abs/2507.00234)
*Jiztom Kavalakkatt Francis,Matthew J Darr*

Main category: cs.LG

TL;DR: 该论文提出了一个新颖的框架，通过整合ResNet和2D Transformer生成的注意力图，并结合全局加权输入显著性，来增强模型可解释性，解决了现有方法中的时空错位问题，并在安全关键领域提供了可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法存在时空错位问题，卷积网络无法捕捉全局上下文，而Transformer缺乏局部精度。这在医疗和工业监控等安全关键领域阻碍了可操作的见解。

Method: 该方法整合了ResNet的梯度加权激活图和Transformer的注意力展开，形成统一的可视化，实现完全时空对齐并保持实时性能。此外，还使用一个NLP模块将融合的热图转换为领域特定的叙述。

Result: 在临床（ECG心律失常检测）和工业（能耗预测）数据集上，该混合框架在PhysioNet数据集上达到了94.1%的准确率（F1 0.93），在UCI能源家电数据集上将回归误差降低到RMSE = 0.28 kWh（R2 = 0.95）。其性能比单独的ResNet、Transformer和InceptionTime基线高出3.8-12.4%。NLP模块通过BLEU-4（0.586）和ROUGE-L（0.650）得分验证了其有效性。

Conclusion: 该方法通过将可解释性形式化为因果忠实度和时空对齐，弥合了技术输出与利益相关者理解之间的鸿沟，为透明、时间感知的决策提供了可扩展的解决方案。

Abstract: In this paper, we present a novel framework for enhancing model
interpretability by integrating heatmaps produced separately by ResNet and a
restructured 2D Transformer with globally weighted input saliency. We address
the critical problem of spatial-temporal misalignment in existing
interpretability methods, where convolutional networks fail to capture global
context and Transformers lack localized precision - a limitation that impedes
actionable insights in safety-critical domains like healthcare and industrial
monitoring. Our method merges gradient-weighted activation maps (ResNet) and
Transformer attention rollout into a unified visualization, achieving full
spatial-temporal alignment while preserving real-time performance. Empirical
evaluations on clinical (ECG arrhythmia detection) and industrial (energy
consumption prediction) datasets demonstrate significant improvements: the
hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and
reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy
Appliance dataset-outperforming standalone ResNet, Transformer, and
InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps
into domain-specific narratives (e.g., "Elevated ST-segment between 2-4 seconds
suggests myocardial ischemia"), validated via BLEU-4 (0.586) and ROUGE-L
(0.650) scores. By formalizing interpretability as causal fidelity and
spatial-temporal alignment, our approach bridges the gap between technical
outputs and stakeholder understanding, offering a scalable solution for
transparent, time-aware decision-making.

</details>


### [187] [Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning](https://arxiv.org/abs/2507.00257)
*Davide Salaorni,Vincenzo De Paola,Samuele Delpero,Giovanni Dispoto,Paolo Bonetti,Alessio Russo,Giuseppe Calcagno,Francesco Trovò,Matteo Papini,Alberto Maria Metelli,Marco Mussi,Marcello Restelli*

Main category: cs.LG

TL;DR: RL在真实世界应用面临挑战，现有基准不足。本文引入Gym4ReaL，一个包含真实世界复杂性的环境套件，用于评估和促进RL算法在真实场景中的发展。


<details>
  <summary>Details</summary>
Motivation: 强化学习(RL)在模拟环境中表现出色，但将其应用于真实世界时面临大状态-动作空间、非平稳性和部分可观测性等固有挑战。现有基准环境未能充分包含和探索这些真实世界复杂性。

Method: 本文引入并开发了名为	exttt{Gym4ReaL}的综合性真实环境套件。该套件旨在支持和评估RL算法在真实世界场景中的性能，并包含多样化的任务以暴露算法面临的实际挑战。

Result: 实验结果显示，在	exttt{Gym4ReaL}的真实世界设定中，标准RL算法与基于规则的基准方法相比，仍能保持竞争力。

Conclusion: 标准RL算法在真实世界环境中具有竞争力，但为充分发挥RL在解决真实世界任务复杂性方面的潜力，仍需开发新的方法。Gym4ReaL为未来的研究提供了必要的平台。

Abstract: In recent years, \emph{Reinforcement Learning} (RL) has made remarkable
progress, achieving superhuman performance in a wide range of simulated
environments. As research moves toward deploying RL in real-world applications,
the field faces a new set of challenges inherent to real-world settings, such
as large state-action spaces, non-stationarity, and partial observability.
Despite their importance, these challenges are often underexplored in current
benchmarks, which tend to focus on idealized, fully observable, and stationary
environments, often neglecting to incorporate real-world complexities
explicitly. In this paper, we introduce \texttt{Gym4ReaL}, a comprehensive
suite of realistic environments designed to support the development and
evaluation of RL algorithms that can operate in real-world scenarios. The suite
includes a diverse set of tasks that expose algorithms to a variety of
practical challenges. Our experimental results show that, in these settings,
standard RL algorithms confirm their competitiveness against rule-based
benchmarks, motivating the development of new methods to fully exploit the
potential of RL to tackle the complexities of real-world tasks.

</details>


### [188] [Who Should I Listen To? Adaptive Collaboration in Personalized Federated Learning](https://arxiv.org/abs/2507.00259)
*Amr Abourayya,Jens Kleesiek,Bharat Rao,Michael Kamp*

Main category: cs.LG

TL;DR: 本文提出FEDMOSAIC，一种基于自适应协作的联邦协同训练方法，旨在解决联邦学习中的数据异质性问题。通过在共享无标签数据集上交换预测实现细粒度信任，该方法在个性化联邦学习（PFL）中展现出优越性能并提供收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的数据异质性是一个核心挑战。尽管个性化联邦学习（PFL）旨在解决此问题，但许多现有PFL方法未能超越本地或中心化基线，这表明其强制执行的协作方式与数据结构之间存在不匹配。

Method: 提出一种基于自适应协作的方法FEDMOSAIC。该方法是一种联邦协同训练方式，允许客户端自适应地决定对其他参与方的依赖程度，并在单个样本层面选择信任对象。具体通过在共享无标签数据集上交换预测来实现细粒度信任，客户端根据私有和公共数据之间的一致性调整损失权重，并根据其估计的逐样本置信度贡献全局伪标签。

Result: 经验证，FEDMOSAIC在多种非独立同分布（non-IID）设置下均优于现有最先进的PFL方法。同时，该研究还在标准假设下提供了收敛性保证。

Conclusion: 研究结果表明，数据感知型协作在实现鲁棒且有效的个性化方面具有巨大潜力。

Abstract: Data heterogeneity is a central challenge in federated learning, and
personalized federated learning (PFL) aims to address it by tailoring models to
each client's distribution. Yet many PFL methods fail to outperform local or
centralized baselines, suggesting a mismatch between the collaboration they
enforce and the structure of the data. We propose an approach based on adaptive
collaboration, where clients decide adaptively not only how much to rely on
others, but also whom to trust at the level of individual examples. We
instantiate this principle in FEDMOSAIC, a federated co-training method in
which clients exchange predictions over a shared unlabeled dataset. This
enables fine-grained trust decisions that are difficult to achieve with
parameter sharing alone. Each client adjusts its loss weighting based on the
agreement between private and public data, and contributes to global
pseudo-labels in proportion to its estimated per-example confidence.
Empirically, FEDMOSAIC improves upon state-of-the-art PFL methods across
diverse non-IID settings, and we provide convergence guarantees under standard
assumptions. Our results demonstrate the potential of data-aware collaboration
for robust and effective personalization.

</details>


### [189] [Examining Reject Relations in Stimulus Equivalence Simulations](https://arxiv.org/abs/2507.00265)
*Alexis Carrillo,Asieh Abolpour Mofrad,Anis Yazidi,Moises Betancort*

Main category: cs.LG

TL;DR: 本研究使用计算模型（FFN、BERT、GPT）探讨了拒绝关系在刺激等价（SE）习得中的作用。结果显示，即使模型在等价测试中表现高准确率，其性能也与纯粹联想学习的代理相似，表明人工神经网络可能依赖联想策略而非真正的SE。


<details>
  <summary>Details</summary>
Motivation: 模拟是探索刺激等价的宝贵工具，但拒绝关系对等价类形成评估的潜在干扰作用仍存在争议。本研究旨在调查拒绝关系在刺激等价习得中的作用。

Method: 研究使用了前馈神经网络（FFN）、BERT和GPT模型，在18种匹配样本（MTS）模拟条件下进行测试。条件变量包括训练结构（线性、一对多、多对一）、关系类型（仅选择、仅拒绝、选择-拒绝）和负面比较选择（标准、偏向）。一个概率代理作为基准，代表纯粹的联想学习。

Result: 结果表明，拒绝关系会影响代理的表现。尽管一些代理在等价测试中取得了高准确率，尤其是在存在拒绝关系和偏向负面比较的情况下，但其性能与概率代理相当。

Conclusion: 这些发现表明，包括Transformer模型在内的人工神经网络可能依赖联想策略而非真正的刺激等价。这强调了在等价计算模型中需仔细考虑拒绝关系并采用更严格的标准。

Abstract: Simulations offer a valuable tool for exploring stimulus equivalence (SE),
yet the potential of reject relations to disrupt the assessment of equivalence
class formation is contentious. This study investigates the role of reject
relations in the acquisition of stimulus equivalence using computational
models. We examined feedforward neural networks (FFNs), bidirectional encoder
representations from transformers (BERT), and generative pre-trained
transformers (GPT) across 18 conditions in matching-to-sample (MTS)
simulations. Conditions varied in training structure (linear series,
one-to-many, and many-to-one), relation type (select-only, reject-only, and
select-reject), and negative comparison selection (standard and biased). A
probabilistic agent served as a benchmark, embodying purely associative
learning. The primary goal was to determine whether artificial neural networks
could demonstrate equivalence class formation or whether their performance
reflected associative learning. Results showed that reject relations influenced
agent performance. While some agents achieved high accuracy on equivalence
tests, particularly with reject relations and biased negative comparisons, this
performance was comparable to the probabilistic agent. These findings suggest
that artificial neural networks, including transformer models, may rely on
associative strategies rather than SE. This underscores the need for careful
consideration of reject relations and more stringent criteria in computational
models of equivalence.

</details>


### [190] [Double Q-learning for Value-based Deep Reinforcement Learning, Revisited](https://arxiv.org/abs/2507.00275)
*Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: 本文提出并研究了深度双Q学习（DDQL）算法，该算法更忠实地将双Q学习的核心思想应用于深度强化学习，有效减少了高估问题，并在Atari游戏上超越了Double DQN。


<details>
  <summary>Details</summary>
Motivation: Q学习及其深度强化学习变体普遍存在对Q值的过高估计问题。Double DQN虽然旨在解决此问题，但其对传统双Q学习中“训练两个相互引导的Q函数”这一核心思想的适应不够彻底。因此，研究的动机是探索并验证真正继承此核心思想的深度强化学习算法（DDQL），以期更有效地减少高估并提升性能。

Method: 研究引入并分析了深度双Q学习（DDQL）算法，该算法的核心在于训练两个Q函数并使其相互引导，从而更好地去关联动作选择和评估。通过在57个Atari 2600游戏上进行实验，比较DDQL与Double DQN的性能和高估水平。同时，还深入探讨了DDQL的网络架构、回放比例和minibatch采样策略等关键方面。

Result: 实验结果表明，DDQL相比Double DQN显著减少了Q值的过高估计。在57个Atari 2600游戏的综合表现上，DDQL在无需额外超参数的情况下超越了Double DQN。研究证实了高性能DDQL实例的存在。

Conclusion: DDQL是双Q学习在基于价值的深度强化学习中更有效且更忠实的适应方式。它成功地解决了现有方法中存在的过高估计问题，并在性能上优于Double DQN。这为提升深度强化学习算法的稳定性和表现提供了一条有前景的路径。

Abstract: Overestimation is pervasive in reinforcement learning (RL), including in
Q-learning, which forms the algorithmic basis for many value-based deep RL
algorithms. Double Q-learning is an algorithm introduced to address
Q-learning's overestimation by training two Q-functions and using both to
de-correlate action-selection and action-evaluation in bootstrap targets.
Shortly after Q-learning was adapted to deep RL in the form of deep Q-networks
(DQN), Double Q-learning was adapted to deep RL in the form of Double DQN.
However, Double DQN only loosely adapts Double Q-learning, forgoing the
training of two different Q-functions that bootstrap off one another. In this
paper, we study algorithms that adapt this core idea of Double Q-learning for
value-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our
aim is to understand whether DDQL exhibits less overestimation than Double DQN
and whether performant instantiations of DDQL exist. We answer both questions
affirmatively, demonstrating that DDQL reduces overestimation and outperforms
Double DQN in aggregate across 57 Atari 2600 games, without requiring
additional hyperparameters. We also study several aspects of DDQL, including
its network architecture, replay ratio, and minibatch sampling strategy.

</details>


### [191] [Structure-preserving Lift & Learn: Scientific machine learning for nonlinear conservative partial differential equations](https://arxiv.org/abs/2507.00301)
*Harsh Sharma,Juan Diego Draxl Giannoni,Boris Kramer*

Main category: cs.LG

TL;DR: 该工作提出一种结构保持的“Lift & Learn”科学机器学习方法，通过变量提升和能量平方化策略，为非线性偏微分方程构建高效且物理结构保持的降阶模型。


<details>
  <summary>Details</summary>
Motivation: 为具有守恒律的非线性偏微分方程（PDEs）学习结构保持的降阶模型，以克服现有方法在效率或准确性与结构保持之间的权衡，并开发既高效又尊重底层物理的降阶模型。

Method: 提出了“结构保持的Lift & Learn”科学机器学习方法，利用变量提升变换。基于能量平方化策略，将非线性PDEs转化为等效的二次提升系统，使得提升动力学在原始变量中呈线性。通过解析推导二次降阶项，并结合约束优化问题学习剩余的线性降阶算子。通过一维波动方程、二维Sine-Gordon方程和二维Klein-Gordon-Zakharov方程进行了数值验证。

Result: 所提出的混合学习方法能够生成计算高效的二次降阶模型，且这些模型能保持高维问题的底层物理。数值结果表明，该学习方法在准确性和计算效率方面均与最先进的结构保持数据驱动模型降阶方法具有竞争力。

Conclusion: 所提出的结构保持“Lift & Learn”方法，结合能量平方化和混合学习，是一种有效且高效的策略，能够为非线性偏微分方程学习保持物理结构的降阶模型，并在准确性和计算性能方面表现出色。

Abstract: This work presents structure-preserving Lift & Learn, a scientific machine
learning method that employs lifting variable transformations to learn
structure-preserving reduced-order models for nonlinear partial differential
equations (PDEs) with conservation laws. We propose a hybrid learning approach
based on a recently developed energy-quadratization strategy that uses
knowledge of the nonlinearity at the PDE level to derive an equivalent
quadratic lifted system with quadratic system energy. The lifted dynamics
obtained via energy quadratization are linear in the old variables, making
model learning very effective in the lifted setting. Based on the lifted
quadratic PDE model form, the proposed method derives quadratic reduced terms
analytically and then uses those derived terms to formulate a constrained
optimization problem to learn the remaining linear reduced operators in a
structure-preserving way. The proposed hybrid learning approach yields
computationally efficient quadratic reduced-order models that respect the
underlying physics of the high-dimensional problem. We demonstrate the
generalizability of quadratic models learned via the proposed
structure-preserving Lift & Learn method through three numerical examples: the
one-dimensional wave equation with exponential nonlinearity, the
two-dimensional sine-Gordon equation, and the two-dimensional
Klein-Gordon-Zakharov equations. The numerical results show that the proposed
learning approach is competitive with the state-of-the-art structure-preserving
data-driven model reduction method in terms of both accuracy and computational
efficiency.

</details>


### [192] [Open-ended Scientific Discovery via Bayesian Surprise](https://arxiv.org/abs/2507.00310)
*Dhruv Agarwal,Bodhisattwa Prasad Majumder,Reece Adamson,Megha Chakravorty,Satvika Reddy Gavireddy,Aditya Parashar,Harshit Surana,Bhavana Dalvi Mishra,Andrew McCallum,Ashish Sabharwal,Peter Clark*

Main category: cs.LG

TL;DR: 本文提出AutoDS，一种基于贝叶斯惊喜和蒙特卡洛树搜索（MCTS）的开放式自主科学发现（ASD）方法，旨在让AI系统自主驱动探索，并在实验中表现出显著的发现能力和人类惊喜度。


<details>
  <summary>Details</summary>
Motivation: 现有自主科学发现方法多依赖人类预设问题，限制了AI的探索能力。少数开放式方法存在多样性启发式难以有效导航假设空间，以及主观兴趣代理定义不精确的问题。亟需一种更有效准则驱动AI自主探索。

Method: AutoDS利用贝叶斯惊喜驱动科学探索，通过量化大型语言模型（LLM）在收集实验结果后其先验信念到后验信念的认知转变。为高效探索嵌套假设空间，方法采用蒙特卡洛树搜索（MCTS）策略，并以惊喜度作为奖励函数进行渐进式扩展。

Result: 在21个真实世界数据集的评估中，AutoDS在固定预算下显著优于竞争对手，产生了多5-29%被LLM认为是“惊喜”的发现。人类评估进一步显示，AutoDS的发现中有三分之二令领域专家感到惊喜。

Conclusion: AutoDS通过引入贝叶斯惊喜作为探索准则，在构建开放式自主科学发现系统方面迈出了重要一步，其发现不仅被LLM认可，也得到了人类专家的验证，有望加速科学发现进程。

Abstract: The promise of autonomous scientific discovery (ASD) hinges not only on
answering questions, but also on knowing which questions to ask. Most recent
works in ASD explore the use of large language models (LLMs) in goal-driven
settings, relying on human-specified research questions to guide hypothesis
generation. However, scientific discovery may be accelerated further by
allowing the AI system to drive exploration by its own criteria. The few
existing approaches in open-ended ASD select hypotheses based on diversity
heuristics or subjective proxies for human interestingness, but the former
struggles to meaningfully navigate the typically vast hypothesis space, and the
latter suffers from imprecise definitions. This paper presents AutoDS -- a
method for open-ended ASD that instead drives scientific exploration using
Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior
beliefs about a hypothesis to its posterior beliefs after gathering
experimental results. To efficiently explore the space of nested hypotheses,
our method employs a Monte Carlo tree search (MCTS) strategy with progressive
widening using surprisal as the reward function. We evaluate AutoDS in the
setting of data-driven discovery across 21 real-world datasets spanning domains
such as biology, economics, finance, and behavioral science. Our results
demonstrate that under a fixed budget, AutoDS substantially outperforms
competitors by producing 5--29\% more discoveries deemed surprising by the LLM.
Our human evaluation further finds that two-thirds of AutoDS discoveries are
surprising to the domain experts, suggesting this is an important step forward
towards building open-ended ASD systems.

</details>


### [193] [$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation](https://arxiv.org/abs/2507.00316)
*Siyou Li,Pengyao Qin,Huanan Wu,Dong Nie,Arun J. Thirunavukarasu,Juntao Yu,Le Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Automated radiology report generation (RRG) aims to produce detailed textual
reports from clinical imaging, such as computed tomography (CT) scans, to
improve the accuracy and efficiency of diagnosis and provision of management
advice. RRG is complicated by two key challenges: (1) inherent complexity in
extracting relevant information from imaging data under resource constraints,
and (2) difficulty in objectively evaluating discrepancies between
model-generated and expert-written reports. To address these challenges, we
propose $\mu^2$LLM, a $\underline{\textbf{mu}}$ltiscale
$\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The
novel ${\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal
features from the multiscale visual tokenizer and the text tokenizer, then
enhances report generation quality through direct preference optimization
(DPO), guided by GREEN-RedLlama. Experimental results on four large CT
image-report medical datasetdemonstrate that our method outperforms existing
approaches, highlighting the potential of our fine-tuned $\mu^2$LLMs on limited
data for RRG tasks.

</details>


### [194] [Exploring Theory-Laden Observations in the Brain Basis of Emotional Experience](https://arxiv.org/abs/2507.00320)
*Christiana Westlin,Ashutosh Singh,Deniz Erdogmus,Georgios Stratis,Lisa Feldman Barrett*

Main category: cs.LG

TL;DR: 重新分析情感脑部数据，发现个体差异而非固定模式，强调初始假设对科学结论的影响。


<details>
  <summary>Details</summary>
Motivation: 情感科学中普遍假设情绪类别是生物和心理上的固定类型，这种假设可能导致研究设计和结果存在偏倚。本研究旨在通过替代视角（情绪是可变、情境化的实例）重新分析现有数据，以验证情绪类别内部存在显著个体差异的假设。

Method: 重新分析了一项已发表的研究数据，该研究报告了个人脑部模式与34种情绪类别的群体平均评分之间的映射。本研究的再分析基于情绪类别是可变实例的替代观点，并对数据中方差结构做出了最小化假设。

Result: 未观察到原始研究中报告的固定映射模式，反而观察到情绪类别内个体之间存在显著的脑部模式差异，这与本研究的预测一致。

Conclusion: 研究结果表明，初始科学假设会显著影响最终的科学结论。因此，在认真采纳一个假设之前，应通过多种分析方法进行验证。

Abstract: In the science of emotion, it is widely assumed that folk emotion categories
form a biological and psychological typology, and studies are routinely
designed and analyzed to identify emotion-specific patterns. This approach
shapes the observations that studies report, ultimately reinforcing the
assumption that guided the investigation. Here, we reanalyzed data from one
such typologically-guided study that reported mappings between individual brain
patterns and group-averaged ratings of 34 emotion categories. Our reanalysis
was guided by an alternative view of emotion categories as populations of
variable, situated instances, and which predicts a priori that there will be
significant variation in brain patterns within a category across instances.
Correspondingly, our analysis made minimal assumptions about the structure of
the variance present in the data. As predicted, we did not observe the original
mappings and instead observed significant variation across individuals. These
findings demonstrate how starting assumptions can ultimately impact scientific
conclusions and suggest that a hypothesis must be supported using multiple
analytic methods before it is taken seriously.

</details>


### [195] [Data-Driven Exploration for a Class of Continuous-Time Linear--Quadratic Reinforcement Learning Problems](https://arxiv.org/abs/2507.00358)
*Yilie Huang,Xun Yu Zhou*

Main category: cs.LG

TL;DR: 针对连续时间随机线性二次（LQ）控制问题，本研究提出一种自适应探索机制的无模型强化学习方法，通过动态调整熵正则化和策略方差来提高学习效率，实现与最佳结果匹配的次线性遗憾界，并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有方法在连续时间LQ控制问题中采用固定或确定性探索策略，需要大量调优且无法根据学习进展进行调整，导致学习效率低下。

Method: 提出一种无模型、数据驱动的自适应探索机制，该机制由评论家自适应调整熵正则化，并由执行者自适应调整策略方差。

Result: 1. 实现了与已知最佳无模型结果相匹配的次线性遗憾界。
2. 数值实验表明，相比非自适应无模型和基于模型的方法，该自适应探索方法能加速收敛并改善遗憾性能。
3. 通过最小调优提高了学习效率。

Conclusion: 所提出的自适应探索方法在连续时间随机LQ强化学习中显著提升了学习效率和性能，克服了固定探索策略的局限性，实现了理论和实践上的优越性。

Abstract: We study reinforcement learning (RL) for the same class of continuous-time
stochastic linear--quadratic (LQ) control problems as in
\cite{huang2024sublinear}, where volatilities depend on both states and
controls while states are scalar-valued and running control rewards are absent.
We propose a model-free, data-driven exploration mechanism that adaptively
adjusts entropy regularization by the critic and policy variance by the actor.
Unlike the constant or deterministic exploration schedules employed in
\cite{huang2024sublinear}, which require extensive tuning for implementations
and ignore learning progresses during iterations, our adaptive exploratory
approach boosts learning efficiency with minimal tuning. Despite its
flexibility, our method achieves a sublinear regret bound that matches the
best-known model-free results for this class of LQ problems, which were
previously derived only with fixed exploration schedules. Numerical experiments
demonstrate that adaptive explorations accelerate convergence and improve
regret performance compared to the non-adaptive model-free and model-based
counterparts.

</details>


### [196] [MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE](https://arxiv.org/abs/2507.00390)
*Geng Zhang,Yuxuan Han,Yuxuan Lou,Wangbo Zhao,Yiqi Zhang,Yang You*

Main category: cs.LG

TL;DR: MoNE是一种新型MoE专家修剪方法，通过用轻量级新手替换冗余专家，有效且鲁棒地降低了MoE模型的内存开销，同时保持甚至提升了性能。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE) 模型在扩展大型语言模型时效率很高，但由于需要将所有专家保留在内存中，会产生显著的内存开销。现有的结构化剪枝方法往往表现不佳，并在模型架构、校准数据源和校准样本量三个维度上存在不稳定的性能下降。

Method: 本文提出Mixture-of-Novices-and-Experts (MoNE) 方法。该方法通过评估专家的访问频率和输出方差来识别冗余专家。对于使用率低且输出稳定的专家，MoNE将其修剪并用轻量级的“新手”（对其原始输出的无偏估计）替换，从而最大限度地减少性能下降。

Result: 广泛的实验表明，MoNE在三个维度上始终优于基线方法，并带来最小的精度下降，证实了其有效性和鲁棒性。值得注意的是，在25%的修剪率下，它将九个下游任务的平均零样本准确率提高了高达2.71；在50%的修剪率下，提高了高达3.61。

Conclusion: MoNE是一种有效且鲁棒的专家修剪方法，能够显著降低MoE模型的内存开销，同时通过用轻量级新手替换冗余专家，实现高性能并保持模型准确性，甚至在某些情况下有所提升。

Abstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models
by activating only a subset of experts per input token. However, deploying
MoE-based models incurs significant memory overhead due to the need to retain
all experts in memory. While structured pruning is promising to reduce memory
costs, existing methods often show suboptimal performance and unstable
degradation in three dimensions: model architectures, calibration data sources,
and calibration sample sizes. This paper proposes
Mixture-of-Novices-and-Experts (MoNE), a novel expert pruning method that
replaces redundant experts with lightweight novices to achieve effective and
robust model compression. MoNE evaluates expert redundancy based on two
metrics: access frequency and output variance. Experts exhibiting low usage and
stable outputs are pruned and replaced with lightweight novices-unbiased
estimations of their original outputs-minimizing performance degradation.
Extensive experiments demonstrate that MoNE consistently outperforms baseline
methods with minimal accuracy degradation across the three dimensions,
confirming its effectiveness and robustness. Notably, it improves the average
zero shot accuracy across nine downstream tasks by up to 2.71 under 25\%
pruning ratio and 3.61 under 50\% pruning. The code is available at
https://github.com/zxgx/mode-pd.

</details>


### [197] [Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows](https://arxiv.org/abs/2507.00425)
*Ruixiang Zhang,Shuangfei Zhai,Jiatao Gu,Yizhe Zhang,Huangjie Zheng,Tianrong Chen,Miguel Angel Bautista,Josh Susskind,Navdeep Jaitly*

Main category: cs.LG

TL;DR: TarFlowLM将语言建模从离散词元空间转移到连续潜在空间，利用基于Transformer的自回归归一化流，实现更灵活的模型构建，并在语言建模基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有自回归语言模型依赖于离散词元、单向上下文和单次解码，这限制了建模灵活性，因此需要探索新的模型设计空间。

Method: 提出TarFlowLM框架，将语言建模从离散词元空间转移到连续潜在空间。该方法采用基于Transformer的自回归归一化流来建模连续表示，并引入了新的基于混合的耦合变换。

Result: 在语言建模基准测试中展现出强大的似然性能。该框架能够捕获全局双向上下文，支持分块生成和分层多遍生成，突显了其固有的建模灵活性。

Conclusion: 所提出的基于连续潜在空间和归一化流的TarFlowLM框架，为语言建模提供了显著的灵活性，并取得了优异的性能，克服了传统离散模型的局限性。

Abstract: Autoregressive models have driven remarkable progress in language modeling.
Their foundational reliance on discrete tokens, unidirectional context, and
single-pass decoding, while central to their success, also inspires the
exploration of a design space that could offer new axes of modeling
flexibility. In this work, we explore an alternative paradigm, shifting
language modeling from a discrete token space to a continuous latent space. We
propose a novel framework TarFlowLM, that employs transformer-based
autoregressive normalizing flows to model these continuous representations.
This approach unlocks substantial flexibility, enabling the construction of
models that can capture global bi-directional context through stacked,
alternating-direction autoregressive transformations, support block-wise
generation with flexible token patch sizes, and facilitate a hierarchical
multi-pass generation process. We further propose new mixture-based coupling
transformations designed to capture complex dependencies within the latent
space shaped by discrete data, and demonstrate theoretical connections to
conventional discrete autoregressive models. Extensive experiments on language
modeling benchmarks demonstrate strong likelihood performance and highlight the
flexible modeling capabilities inherent in our framework.

</details>


### [198] [HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism](https://arxiv.org/abs/2507.00394)
*Geng Zhang,Shenggan Cheng,Xuanlei Zhao,Ziming Liu,Yang You*

Main category: cs.LG

TL;DR: HelixPipe是一种新型的流水线并行方法，专为长序列Transformer训练设计，通过优化注意力计算和内存管理，显著提升了训练效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行方法在处理不断增长的Transformer序列长度时，由于注意力计算的二次方复杂度及高昂的内存开销，导致性能不佳。

Method: HelixPipe核心方法包括：1. 注意力并行分区，并行调度不同微批次的注意力计算以减少流水线气泡。2. 双重先进后出（first-in-last-out）微批次调度，平衡内存并重叠通信与计算。3. 结合无注意力重计算和分块MLP，以缓解碎片化并支持更长序列。

Result: 实验证明，HelixPipe在长序列训练中优势显著，在不同流水线规模、模型大小和集群配置下，其吞吐量和可扩展性均优于现有方法。具体而言，在64个H20 GPU上训练128k序列长度的7B模型时，实现了26%的加速。

Conclusion: HelixPipe有效解决了长序列Transformer训练的性能和内存挑战，通过其创新的并行策略，显著提高了训练效率和可扩展性，为大型模型长序列训练提供了有力支持。

Abstract: As transformer sequence lengths grow, existing pipeline parallelisms incur
suboptimal performance due to the quadratic attention computation and the
substantial memory overhead. To relieve these challenges, we propose HelixPipe,
a novel pipeline parallelism for long sequence transformer training. First,
HelixPipe introduces attention parallel partition, which schedules attention
computations of different micro batches across different pipeline stages in
parallel, reducing pipeline bubbles. Second, it employs a two-fold
first-in-last-out micro batch schedule to balance memory usage and overlap
communication with computation. Additionally, HelixPipe utilizes recomputation
without attention and chunked MLP to mitigate fragmentation and enable longer
sequences. Experiments demonstrate that HelixPipe gains increasing advantages
with longer sequence lengths, and outperforms existing methods in throughput
and scalability across varying pipeline sizes, model sizes, and cluster
configurations. Notably, it achieves a 26\% speedup over baseline methods when
training a 7B model with 128k sequence length on 64 H20 GPUs. Code is available
at https://github.com/code-tunnel/Megatron-LM/tree/dev.

</details>


### [199] [Diffusion Disambiguation Models for Partial Label Learning](https://arxiv.org/abs/2507.00411)
*Jinfu Fan,Xiaohui Zhong,Kangrui Ren,Jiangnan Li,Linqing Huang*

Main category: cs.LG

TL;DR: 本文提出DDMP模型，将局部标签学习（PLL）中的标签去歧义问题重构为生成过程，利用扩散模型去噪歧义标签，并通过构建伪干净标签和动态转换矩阵克服实例-标签不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 解决实际机器学习应用中长期存在的歧义标签学习问题，特别是从一组候选标签中识别实例真实标签的局部标签学习（PLL）问题。

Method: 1. 将标签去歧义问题重新定义为生成模型视角，利用扩散模型通过逆向去噪过程迭代细化初始随机猜测来生成标签。2. 针对歧义标签导致的实例与标签不匹配问题，提出了扩散去歧义模型（DDMP）。3. DDMP首先利用实例与标签间的互补信息构建伪干净标签进行初始扩散训练。4. 引入一个感知转换矩阵来动态估计和更新潜在的真实标签。5. 在训练过程中逐步完善真实标签以提升分类器性能。

Result: 实验结果表明，DDMP模型具有显著优势，且非常适用于局部标签学习（PLL）任务。

Conclusion: DDMP模型通过将标签去歧义问题视为生成过程，并创新性地利用伪干净标签和动态转换矩阵，有效解决了局部标签学习中的歧义标签挑战，提升了学习性能。

Abstract: Learning from ambiguous labels is a long-standing problem in practical
machine learning applications. The purpose of \emph{partial label learning}
(PLL) is to identify the ground-truth label from a set of candidate labels
associated with a given instance. Inspired by the remarkable performance of
diffusion models in various generation tasks, this paper explores their
potential to denoise ambiguous labels through the reverse denoising process.
Therefore, this paper reformulates the label disambiguation problem from the
perspective of generative models, where labels are generated by iteratively
refining initial random guesses. This perspective enables the diffusion model
to learn how label information is generated stochastically. By modeling the
generation uncertainty, we can use the maximum likelihood estimate of the label
for classification inference. However, such ambiguous labels lead to a mismatch
between instance and label, which reduces the quality of generated data. To
address this issue, this paper proposes a \emph{diffusion disambiguation model
for PLL} (DDMP), which first uses the potential complementary information
between instances and labels to construct pseudo-clean labels for initial
diffusion training. Furthermore, a transition-aware matrix is introduced to
estimate the potential ground-truth labels, which are dynamically updated
during the diffusion generation. During training, the ground-truth label is
progressively refined, improving the classifier. Experiments show the advantage
of the DDMP and its suitability for PLL.

</details>


### [200] [Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention](https://arxiv.org/abs/2507.00449)
*Zhihao Zhan,Jianan Zhao,Zhaocheng Zhu,Jian Tang*

Main category: cs.LG

TL;DR: 本文旨在提高SSMs的长上下文建模能力，通过引入新的合成任务“联合回忆”来揭示其理论局限性。为解决该问题，提出将SSMs与上下文依赖稀疏注意力（CDSA）结合，并实例化为HAX模型，实验证明HAX在长上下文基准测试上优于现有SSM基线。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理中，高效的长上下文建模仍然是一个关键挑战。主流的Transformer架构时间复杂度与序列长度呈二次方增长，而状态空间模型（SSMs）虽然提供亚二次方解决方案，但在捕获长距离依赖方面表现不佳。此外，广泛使用的合成任务（如关联回忆）未能充分代表真实世界的长上下文复杂性。

Method: 1. 扩展关联回忆任务至“联合回忆”，要求模型在指定上下文中回忆与键相关的值，以更准确地评估长上下文能力。2. 理论证明SSMs在亚二次方时间复杂度下不具备解决多查询联合回忆的表达能力。3. 提出将SSMs与上下文依赖稀疏注意力（CDSA）相结合的解决方案，该方案具有解决多查询联合回忆的表达能力和亚二次方计算复杂度。4. 提出局部敏感哈希注意力与稀疏键选择（HAX），作为理论解决方案的实例化，并针对自然语言领域进行优化。

Result: 在合成和真实世界的长上下文基准测试中，HAX持续优于SSM基线模型，并且也优于集成上下文无关稀疏注意力的SSM模型。

Conclusion: HAX模型通过结合SSMs与上下文依赖稀疏注意力，成功解决了长上下文建模中的效率和长距离依赖捕获问题。它不仅填补了理论分析与实际应用之间的空白，还在多项基准测试中展现出优越的性能，是长上下文NLP领域的一个重要进展。

Abstract: Efficient long-context modeling remains a critical challenge for natural
language processing (NLP), as the time complexity of the predominant
Transformer architecture scales quadratically with the sequence length. While
state-space models (SSMs) offer alternative sub-quadratic solutions, they
struggle to capture long-range dependencies effectively. In this work, we focus
on analyzing and improving the long-context modeling capabilities of SSMs. We
show that the widely used synthetic task, associative recall, which requires a
model to recall a value associated with a single key without context,
insufficiently represents the complexities of real-world long-context modeling.
To address this limitation, we extend the associative recall to a novel
synthetic task, \emph{joint recall}, which requires a model to recall the value
associated with a key given in a specified context. Theoretically, we prove
that SSMs do not have the expressiveness to solve multi-query joint recall in
sub-quadratic time complexity. To resolve this issue, we propose a solution
based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which
has the expressiveness to solve multi-query joint recall with sub-quadratic
computation. To bridge the gap between theoretical analysis and real-world
applications, we propose locality-sensitive Hashing Attention with sparse Key
Selection (HAX), which instantiates the theoretical solution and is further
tailored to natural language domains. Extensive experiments on both synthetic
and real-world long-context benchmarks show that HAX consistently outperforms
SSM baselines and SSMs integrated with context-independent sparse attention
(CISA).

</details>


### [201] [A Recipe for Causal Graph Regression: Confounding Effects Revisited](https://arxiv.org/abs/2507.00440)
*Yujia Yin,Tianyi Qu,Zihao Wang,Yifan Chen*

Main category: cs.LG

TL;DR: 现有因果图学习（CGL）在图分类OOD任务中表现出色，但忽视了更具挑战性的图回归任务。本研究提出一种解决因果图回归（CGR）的方法，通过重新思考混淆效应处理，并利用对比学习将因果干预推广至回归任务，在图OOD基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 因果图学习（CGL）虽能提升图神经网络在域外（OOD）场景下的泛化能力，但其成功案例主要局限于分类任务，而图学习中更具挑战性的回归任务却被忽视。

Method: 本研究专注于解决因果图回归（CGR）问题。为此，我们重新调整了现有CGL研究中主要针对分类任务的混淆效应处理方式。具体而言，我们审视了混淆因子在图级回归中的预测能力，并通过对比学习的视角将分类特有的因果干预技术泛化到回归任务。

Result: 在图OOD基准测试上进行的广泛实验验证了我们所提出的CGR方法的有效性。

Conclusion: 本研究成功地将因果图学习方法应用于图回归任务，通过对混淆效应处理的创新和结合对比学习，有效提升了模型在OOD场景下的泛化能力，填补了现有CGL研究在回归领域的空白。

Abstract: Through recognizing causal subgraphs, causal graph learning (CGL) has risen
to be a promising approach for improving the generalizability of graph neural
networks under out-of-distribution (OOD) scenarios. However, the empirical
successes of CGL techniques are mostly exemplified in classification settings,
while regression tasks, a more challenging setting in graph learning, are
overlooked. We thus devote this work to tackling causal graph regression (CGR);
to this end we reshape the processing of confounding effects in existing CGL
studies, which mainly deal with classification. Specifically, we reflect on the
predictive power of confounders in graph-level regression, and generalize
classification-specific causal intervention techniques to regression through a
lens of contrastive learning. Extensive experiments on graph OOD benchmarks
validate the efficacy of our proposals for CGR. The model implementation and
the code are provided on https://github.com/causal-graph/CGR.

</details>


### [202] [Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design](https://arxiv.org/abs/2507.00445)
*Xingyu Su,Xiner Li,Masatoshi Uehara,Sunwoo Kim,Yulai Zhao,Gabriele Scalia,Ehsan Hajiramezanali,Tommaso Biancalani,Degui Zhi,Shuiwang Ji*

Main category: cs.LG

TL;DR: 提出一种迭代蒸馏微调框架，通过离策略策略蒸馏，解决扩散模型在生物分子设计中奖励引导生成的稳定性与效率问题，并在多项任务中展现优越的奖励优化能力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在复杂数据生成方面表现出色，但在生物分子设计等实际应用中，需根据可能不可微分的奖励函数进行优化。现有强化学习方法在微调扩散模型时，常面临不稳定、样本效率低和模式崩溃等问题。

Method: 提出一种迭代的、基于蒸馏的微调框架。该方法将问题建模为策略蒸馏：在roll-in阶段收集离策略数据，在roll-out阶段模拟基于奖励的软最优策略，并通过最小化模拟软最优策略与当前模型策略之间的KL散度来更新模型。

Result: 相较于现有基于强化学习的方法，所提方法的离策略公式结合KL散度最小化显著提高了训练稳定性和样本效率。在蛋白质、小分子和调控DNA设计等多种生物分子设计任务中，经验结果证明了该方法的有效性和卓越的奖励优化能力。

Conclusion: 该迭代蒸馏框架通过创新的离策略策略蒸馏方法，有效解决了扩散模型在奖励引导生成中的优化挑战，实现了更稳定、更高效的训练和卓越的性能，特别适用于生物分子设计领域。

Abstract: We address the problem of fine-tuning diffusion models for reward-guided
generation in biomolecular design. While diffusion models have proven highly
effective in modeling complex, high-dimensional data distributions, real-world
applications often demand more than high-fidelity generation, requiring
optimization with respect to potentially non-differentiable reward functions
such as physics-based simulation or rewards based on scientific knowledge.
Although RL methods have been explored to fine-tune diffusion models for such
objectives, they often suffer from instability, low sample efficiency, and mode
collapse due to their on-policy nature. In this work, we propose an iterative
distillation-based fine-tuning framework that enables diffusion models to
optimize for arbitrary reward functions. Our method casts the problem as policy
distillation: it collects off-policy data during the roll-in phase, simulates
reward-based soft-optimal policies during roll-out, and updates the model by
minimizing the KL divergence between the simulated soft-optimal policy and the
current model policy. Our off-policy formulation, combined with KL divergence
minimization, enhances training stability and sample efficiency compared to
existing RL-based methods. Empirical results demonstrate the effectiveness and
superior reward optimization of our approach across diverse tasks in protein,
small molecule, and regulatory DNA design.

</details>


### [203] [Best Agent Identification for General Game Playing](https://arxiv.org/abs/2507.00451)
*Matthew Stephenson,Alex Newcombe,Eric Piette,Dennis Soemers*

Main category: cs.LG

TL;DR: 该论文提出一种基于多臂老虎机（MAB）最佳臂识别的新方法Optimistic-WS，用于在多问题领域中高效准确地识别每个子任务的最佳算法，并在通用游戏领域验证其性能显著优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 在多问题领域中，需要高效且准确地识别每个子任务的最佳算法，以优化其性能和评估质量。

Method: 将多问题领域中的最佳算法识别问题视为一组多臂老虎机的最佳臂识别问题。提出了一种基于Wilson分数区间的乐观选择过程（Optimistic-WS），该过程根据潜在遗憾减少来对所有老虎机中的每个臂进行排名。

Result: 在GVGAI和Ludii通用游戏平台上的评估结果表明，与现有最佳臂识别算法相比，Optimistic-WS在平均简单遗憾方面取得了显著的性能提升。

Conclusion: 该新颖方法能显著提高通用游戏框架以及其他算法运行时间长的多任务领域中智能体评估的质量和准确性。

Abstract: We present an efficient and generalised procedure to accurately identify the
best performing algorithm for each sub-task in a multi-problem domain. Our
approach treats this as a set of best arm identification problems for
multi-armed bandits, where each bandit corresponds to a specific task and each
arm corresponds to a specific algorithm or agent. We propose an optimistic
selection process based on the Wilson score interval (Optimistic-WS) that ranks
each arm across all bandits in terms of their potential regret reduction. We
evaluate the performance of Optimistic-WS on two of the most popular general
game domains, the General Video Game AI (GVGAI) framework and the Ludii general
game playing system, with the goal of identifying the highest performing agent
for each game within a limited number of trials. Compared to previous best arm
identification algorithms for multi-armed bandits, our results demonstrate a
substantial performance improvement in terms of average simple regret. This
novel approach can be used to significantly improve the quality and accuracy of
agent evaluation procedures for general game frameworks, as well as other
multi-task domains with high algorithm runtimes.

</details>


### [204] [Recurrent Memory-Augmented Transformers with Chunked Attention for Long-Context Language Modeling](https://arxiv.org/abs/2507.00453)
*Ankit Kashyap*

Main category: cs.LG

TL;DR: 提出一种新型Transformer架构，结合全局/局部注意力与记忆机制，旨在高效处理长上下文语言建模。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer在处理长上下文时面临二次方注意力成本，难以高效处理长距离依赖。研究旨在不增加成本的情况下，高效处理短距离和长距离依赖。

Method: 设计了一种Transformer架构，整合全局注意力、分块局部注意力及门控FIFO记忆机制。记忆模块采用受循环网络启发的门控更新机制，并在每个注意力头应用旋转位置编码。该架构采用PyTorch从零实现。

Result: 模型能够高效处理短距离和长距离依赖，且不造成注意力成本的二次方增长。它提供了一种轻量级且可扩展的设计。

Conclusion: 该新型架构为长上下文语言建模提供了一种高效、轻量级且可扩展的解决方案，适用于对话建模、代码补全和文档理解等任务。

Abstract: We present a Transformer architecture for long-context language modeling that
combines global attention with two biologically inspired components: chunked
local attention and a gated FIFO memory mechanism. This unified attention block
allows the model to efficiently handle both short-range and long-range
dependencies without increasing attention cost quadratically. The memory module
persistently stores past token representations using a gated update mechanism
inspired by recurrent networks. Rotary positional encoding is applied per
attention head to enable directionally disentangled, scale-invariant positional
signals. The architecture is implemented entirely from scratch in PyTorch, with
no reliance on high-level libraries, enabling transparent and modular
experimentation. Our model offers a lightweight and extensible design for tasks
such as dialogue modeling, code completion, and document understanding.

</details>


### [205] [Diversity Conscious Refined Random Forest](https://arxiv.org/abs/2507.00467)
*Sijan Bhattarai,Saurav Bhandari,Girija Bhusal,Saroj Shakya,Tapendra Pandey*

Main category: cs.LG

TL;DR: 提出一种改进的随机森林分类器，通过动态特征选择和基于相关性的树聚类，在提高多样性的同时减少冗余并提升分类精度。


<details>
  <summary>Details</summary>
Motivation: 标准随机森林（RF）依赖大量树和所有输入特征，导致高推理成本和模型冗余。

Method: 提出一种迭代自优化的随机森林分类器。该方法首先移除信息量最少的特征，然后分析性地决定需生长的新树数量，最后通过基于相关性的聚类移除冗余树，从而在信息量特征上动态生成树并强制实现最大多样性。

Result: 在8个二分类和多分类基准数据集上，与标准随机森林在相同树数量下相比，所提出的模型实现了更高的分类精度。

Conclusion: 该研究成功开发了一种改进的随机森林模型，通过优化特征选择和树多样性，有效解决了标准随机森林的冗余问题，并提升了分类性能。

Abstract: Random Forest (RF) is a widely used ensemble learning technique known for its
robust classification performance across diverse domains. However, it often
relies on hundreds of trees and all input features, leading to high inference
cost and model redundancy. In this work, our goal is to grow trees dynamically
only on informative features and then enforce maximal diversity by clustering
and retaining uncorrelated trees. Therefore, we propose a Refined Random Forest
Classifier that iteratively refines itself by first removing the least
informative features and then analytically determines how many new trees should
be grown, followed by correlation-based clustering to remove redundant trees.
The classification accuracy of our model was compared against the standard RF
on the same number of trees. Experiments on 8 multiple benchmark datasets,
including binary and multiclass datasets, demonstrate that the proposed model
achieves improved accuracy compared to standard RF.

</details>


### [206] [Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization](https://arxiv.org/abs/2507.00480)
*Kiyoung Om,Kyuil Sim,Taeyoung Yun,Hyeongyu Kang,Jinkyoo Park*

Main category: cs.LG

TL;DR: 本文提出一种新颖的两阶段框架，用于解决高维黑盒函数在黑盒约束下的优化问题，通过结合基于流的模型和在潜在空间中平摊后验采样，有效克服了维度诅咒和模式崩溃等挑战，并取得了卓越性能。


<details>
  <summary>Details</summary>
Motivation: 在高维黑盒约束下优化黑盒函数是一项普遍存在但具有挑战性的任务。现有的贝叶斯优化方法受限于维度诅咒，而基于生成模型的方法则面临可伸缩性差和模式崩溃的风险，尤其是在目标分布高度多模态时。因此，需要一种新框架来克服这些局限性。

Method: 该方法迭代执行两个阶段：首先，训练基于流的模型以捕获数据分布，并训练代理模型以预测函数值和约束违反情况（带不确定性量化）。其次，将候选选择问题视为后验推断问题，以有效寻找高目标值且不违反约束的有前景候选。为解决后验分布多模态和高原问题，在基于流模型的潜在空间中平摊后验分布的采样，该空间比数据空间更平滑。

Result: 实验证明，该方法在各种合成和实际世界的约束黑盒优化任务中均实现了卓越的性能。

Conclusion: 所提出的框架成功克服了高维约束黑盒优化中现有方法的挑战，通过创新的两阶段方法和在潜在空间中平摊后验采样，显著提升了优化性能。

Abstract: Optimizing high-dimensional black-box functions under black-box constraints
is a pervasive task in a wide range of scientific and engineering problems.
These problems are typically harder than unconstrained problems due to
hard-to-find feasible regions. While Bayesian optimization (BO) methods have
been developed to solve such problems, they often struggle with the curse of
dimensionality. Recently, generative model-based approaches have emerged as a
promising alternative for constrained optimization. However, they suffer from
poor scalability and are vulnerable to mode collapse, particularly when the
target distribution is highly multi-modal. In this paper, we propose a new
framework to overcome these challenges. Our method iterates through two stages.
First, we train flow-based models to capture the data distribution and
surrogate models that predict both function values and constraint violations
with uncertainty quantification. Second, we cast the candidate selection
problem as a posterior inference problem to effectively search for promising
candidates that have high objective values while not violating the constraints.
During posterior inference, we find that the posterior distribution is highly
multi-modal and has a large plateau due to constraints, especially when
constraint feedback is given as binary indicators of feasibility. To mitigate
this issue, we amortize the sampling from the posterior distribution in the
latent space of flow-based models, which is much smoother than that in the data
space. We empirically demonstrate that our method achieves superior performance
on various synthetic and real-world constrained black-box optimization tasks.
Our code is publicly available \href{https://github.com/umkiyoung/CiBO}{here}.

</details>


### [207] [PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning](https://arxiv.org/abs/2507.00485)
*Weiran Guo,Guanjun Liu,Ziyuan Zhou,Ling Wang*

Main category: cs.LG

TL;DR: 本文揭示了安全强化学习（Safe RL）易受后门攻击的漏洞，并提出了首个针对Safe RL的后门攻击框架PNAct，该框架利用正负动作样本来植入后门，实验验证了其有效性和可行性，突显了Safe RL潜在的安全风险。


<details>
  <summary>Details</summary>
Motivation: 安全强化学习（Safe RL）旨在通过结合成本度量来确保智能体在决策过程中遵守安全约束。然而，本文指出Safe RL可能容易受到后门攻击的操纵，导致智能体执行不安全行为。研究动机在于探究并证明Safe RL的这种潜在脆弱性。

Method: 1. 引入Safe RL后门攻击的相关概念和评估指标。2. 提出首个结合正负动作样本（PNAct）的Safe RL后门攻击框架，其中正样本提供参考动作，负样本指示需避免的动作。3. 从理论上阐述PNAct的特性。4. 设计了基于PNAct的攻击算法。5. 通过实验评估了所提出后门攻击框架的有效性。

Result: 实验结果表明，所提出的后门攻击框架（PNAct）在Safe RL中是有效且可行的。研究成功地证明了通过PNAct可以成功地植入后门并操纵智能体执行不安全动作。这突出显示了Safe RL系统存在潜在的安全风险。

Conclusion: Safe RL并非绝对安全，它容易受到后门攻击的威胁，这些攻击能够操纵智能体执行不安全行为。本文提出的PNAct框架是首次在Safe RL领域证明后门攻击的可行性，强调了未来在设计和部署Safe RL系统时，必须重视并解决其潜在的安全漏洞。

Abstract: Reinforcement Learning (RL) is widely used in tasks where agents interact
with an environment to maximize rewards. Building on this foundation, Safe
Reinforcement Learning (Safe RL) incorporates a cost metric alongside the
reward metric, ensuring that agents adhere to safety constraints during
decision-making. In this paper, we identify that Safe RL is vulnerable to
backdoor attacks, which can manipulate agents into performing unsafe actions.
First, we introduce the relevant concepts and evaluation metrics for backdoor
attacks in Safe RL. It is the first attack framework in the Safe RL field that
involves both Positive and Negative Action sample (PNAct) is to implant
backdoors, where positive action samples provide reference actions and negative
action samples indicate actions to be avoided. We theoretically point out the
properties of PNAct and design an attack algorithm. Finally, we conduct
experiments to evaluate the effectiveness of our proposed backdoor attack
framework, evaluating it with the established metrics. This paper highlights
the potential risks associated with Safe RL and underscores the feasibility of
such attacks. Our code and supplementary material are available at
https://github.com/azure-123/PNAct.

</details>


### [208] [Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling](https://arxiv.org/abs/2507.00518)
*Walid Bendada,Guillaume Salha-Galvan,Romain Hennequin,Théo Bontempelli,Thomas Bouabça,Tristan Cazenave*

Main category: cs.LG

TL;DR: 本文提出vMF-exp，一种可扩展的强化学习探索方法，用于处理超球面嵌入表示的大型动作集，并作为Boltzmann探索的替代方案。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中探索大型动作集面临可扩展性挑战，尤其流行的Boltzmann探索方法因需要为每个动作计算softmax值而存在伸缩性问题。

Method: vMF-exp方法通过使用von Mises-Fisher分布采样状态嵌入表示，然后探索该表示的最近邻居。这种方法可以扩展到几乎无限数量的候选动作。

Result: 理论上，在一定假设下，vMF-exp渐近地保持与Boltzmann探索相同的动作探索概率。实验在模拟数据、真实世界公共数据和全球音乐流媒体服务的推荐系统上成功部署，经验性地验证了所提出方法的关键特性。

Conclusion: vMF-exp提供了一种可扩展的替代方案，用于探索具有超球面嵌入的大型动作集，有效解决了传统方法（如Boltzmann探索）的伸缩性限制。

Abstract: This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable
method for exploring large action sets in reinforcement learning problems where
hyperspherical embedding vectors represent these actions. vMF-exp involves
initially sampling a state embedding representation using a von Mises-Fisher
distribution, then exploring this representation's nearest neighbors, which
scales to virtually unlimited numbers of candidate actions. We show that, under
theoretical assumptions, vMF-exp asymptotically maintains the same probability
of exploring each action as Boltzmann Exploration (B-exp), a popular
alternative that, nonetheless, suffers from scalability issues as it requires
computing softmax values for each action. Consequently, vMF-exp serves as a
scalable alternative to B-exp for exploring large action sets with
hyperspherical embeddings. Experiments on simulated data, real-world public
data, and the successful large-scale deployment of vMF-exp on the recommender
system of a global music streaming service empirically validate the key
properties of the proposed method.

</details>


### [209] [Foundation Models for Clinical Records at Health System Scale](https://arxiv.org/abs/2507.00574)
*Haresh Rengaraj Rajamohan,Xiang Gao,Weicheng Zhu,Shih-Lun Huang,Long Chen,Kyunghyun Cho,Cem M. Deniz,Narges Razavian*

Main category: cs.LG

TL;DR: 本文提出一种针对电子健康记录（EHR）序列数据的生成式预训练策略，利用“下次就诊事件预测”任务进行预训练，并在零样本预测任务中达到与全量微调模型相当的性能，同时指出并解决了EHR基础模型评估中重复事件的潜在问题。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练在语言及其他数据类型建模中已展现巨大潜力，但在结构化电子健康记录（EHRs）领域尚未得到充分探索。现有方法可能需要昂贵的任务特定微调，且评估指标可能受重复事件影响。

Method: 研究采用一种新颖的生成式预训练策略，通过自回归地根据患者历史生成下次就诊的各种分词化临床事件。该模型固有地处理异构数据的联合预测。此外，引入了对重复事件预测的正则化，并指出EHR基础模型评估中一个关键缺陷：不区分新发事件和后续发生会夸大性能指标。

Result: 通过对痴呆症和膝关节骨性关节炎2年和5年发病率的零样本预测进行评估，本模型的性能可与经过完全微调的掩码预训练Transformer基线模型媲美。

Conclusion: 本研究提出的方法能够捕捉复杂的临床依赖关系，且无需昂贵的任务特定微调。这表明生成式预训练在EHR领域具有巨大潜力，尤其在关注重复事件的评估细微差别时。

Abstract: Large-scale pretraining has transformed modeling of language and other data
types, but its potential remains underexplored in healthcare with structured
electronic health records (EHRs). We present a novel generative pretraining
strategy for sequential EHR data using next-visit event prediction. Our model
learns to autoregressively generate various tokenized clinical events for the
next visit based on patient history and inherently handles the joint prediction
of heterogeneous data types. Additionally, we introduce regularization on
predicting repeated events and highlight a key pitfall in EHR-based foundation
model evaluations: repeated event tokens can inflate performance metrics when
new onsets are not distinguished from subsequent occurrences. Our model is
evaluated via zero-shot prediction for forecasting dementia and knee
osteoarthritis incidence within 2 and 5 years, and the model performance rivals
a fully fine-tuned masked pretrained Transformer baseline, demonstrating that
our approach captures complex clinical dependencies without requiring costly
task-specific fine-tuning.

</details>


### [210] [Quantum Circuit Structure Optimization for Quantum Reinforcement Learning](https://arxiv.org/abs/2507.00589)
*Seok Bin Son,Joongheon Kim*

Main category: cs.LG

TL;DR: 本文提出QRL-NAS算法，通过集成量子神经网络架构搜索（QNAS）优化量子强化学习（QRL）中的参数化量子电路（PQC）结构，实验证明其性能优于固定电路的QRL。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在高维空间面临维度灾难导致的学习效率低下。量子强化学习（QRL）利用量子计算特性缓解此问题，但现有QRL研究中使用的参数化量子电路（PQC）结构多为经验性固定，缺乏最优性验证。

Method: 提出QRL-NAS算法，该算法将量子神经网络架构搜索（QNAS）集成到QRL框架中，以自适应地优化PQC结构。PQC作为核心计算模块，执行类似经典神经网络隐藏层的线性及非线性变换。

Result: 实验结果表明，QRL-NAS算法在量子强化学习任务中获得的奖励高于使用固定电路的QRL方法。

Conclusion: QRL-NAS算法通过优化QRL中的PQC结构，有效提高了学习效率和性能，验证了其有效性和实用价值。

Abstract: Reinforcement learning (RL) enables agents to learn optimal policies through
environmental interaction. However, RL suffers from reduced learning efficiency
due to the curse of dimensionality in high-dimensional spaces. Quantum
reinforcement learning (QRL) addresses this issue by leveraging superposition
and entanglement in quantum computing, allowing efficient handling of
high-dimensional problems with fewer resources. QRL combines quantum neural
networks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as
the core computational module. The PQC performs linear and nonlinear
transformations through gate operations, similar to hidden layers in classical
neural networks. Previous QRL studies, however, have used fixed PQC structures
based on empirical intuition without verifying their optimality. This paper
proposes a QRL-NAS algorithm that integrates quantum neural architecture search
(QNAS) to optimize PQC structures within QRL. Experiments demonstrate that
QRL-NAS achieves higher rewards than QRL with fixed circuits, validating its
effectiveness and practical utility.

</details>


### [211] [Residual Reward Models for Preference-based Reinforcement Learning](https://arxiv.org/abs/2507.00611)
*Chenyang Cao,Miguel Rogel-García,Mohamed Nabail,Xueqian Wang,Nicholas Rhinehart*

Main category: cs.LG

TL;DR: 本文提出残差奖励模型（RRM），通过结合先验奖励和学习奖励来加速偏好式强化学习（PbRL）的收敛，并在模拟和真实机器人任务中展示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 偏好式强化学习（PbRL）在奖励难以指定时很有用，但其收敛速度慢，因为它需要训练一个奖励模型。现有方法在预训练和微调奖励模型时可能面临优化挑战。因此，需要一种能有效利用先验知识来加速PbRL学习的方法。

Method: 本文提出残差奖励模型（RRM），该模型假设环境的真实奖励可分解为两部分：一个在训练前可用的先验奖励（例如用户估计或逆强化学习获得的奖励）和一个通过偏好训练得到的学习奖励。文章引入了基于状态和基于图像的RRM版本，并在Meta-World环境和一台Franka Panda真实机器人上进行了评估。

Result: 实验结果表明，RRM显著提高了常见PbRL方法的性能，并对多种类型的先验奖励都有效。在真实机器人上，该方法显著加速了策略学习，用更少步骤即可成功完成任务。

Conclusion: 残差奖励模型（RRM）通过有效结合先验知识，显著解决了偏好式强化学习（PbRL）收敛速度慢的问题，从而提升了PbRL的性能和学习效率，特别是在真实机器人任务中表现突出。

Abstract: Preference-based Reinforcement Learning (PbRL) provides a way to learn
high-performance policies in environments where the reward signal is hard to
specify, avoiding heuristic and time-consuming reward design. However, PbRL can
suffer from slow convergence speed since it requires training in a reward
model. Prior work has proposed learning a reward model from demonstrations and
fine-tuning it using preferences. However, when the model is a neural network,
using different loss functions for pre-training and fine-tuning can pose
challenges to reliable optimization. In this paper, we propose a method to
effectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM
assumes that the true reward of the environment can be split into a sum of two
parts: a prior reward and a learned reward. The prior reward is a term
available before training, for example, a user's ``best guess'' reward
function, or a reward function learned from inverse reinforcement learning
(IRL), and the learned reward is trained with preferences. We introduce
state-based and image-based versions of RRM and evaluate them on several tasks
in the Meta-World environment suite. Experimental results show that our method
substantially improves the performance of a common PbRL method. Our method
achieves performance improvements for a variety of different types of prior
rewards, including proxy rewards, a reward obtained from IRL, and even a
negated version of the proxy reward. We also conduct experiments with a Franka
Panda to show that our method leads to superior performance on a real robot. It
significantly accelerates policy learning for different tasks, achieving
success in fewer steps than the baseline. The videos are presented at
https://sunlighted.github.io/RRM-web/.

</details>


### [212] [Cooperative Sheaf Neural Networks](https://arxiv.org/abs/2507.00647)
*André Ribeiro,Ana Luiza Tenório,Juan Belieni,Amauri H. Souza,Diego Mesquita*

Main category: cs.LG

TL;DR: 现有层束扩散模型无法实现合作行为，因为它缺乏消息方向性。本文引入了有向图上的细胞层束概念，并基于此提出了合作层束神经网络（CSNN），该网络能够允许节点选择性地关注远程节点，并在实验中表现出更好的性能。


<details>
  <summary>Details</summary>
Motivation: 层束扩散是处理异质数据和避免过平滑的有效方法，而合作消息传递能增强信息扩散的灵活性。研究者自然会问：层束扩散是否能展现合作行为？本文发现现有层束扩散因缺乏消息方向性而无法实现合作行为，这促使他们寻求解决方案。

Method: 为了解决现有层束扩散的局限性，作者引入了有向图上的细胞层束概念，并刻画了它们的入度和出度拉普拉斯算子。基于此构建，他们提出了合作层束神经网络（CSNN）。

Result: 理论上，CSNN能够允许节点选择性地关注（监听）任意远的节点，同时忽略路径中的其他节点，这有望缓解过挤压问题。实验结果表明，CSNN在性能上优于现有的层束扩散模型和合作图神经网络。

Conclusion: 现有层束扩散模型因缺乏消息方向性而无法实现合作行为。通过引入有向图上的细胞层束并构建CSNN，本文成功克服了这一限制，CSNN不仅在理论上展现出选择性关注的能力，还在实践中证明了其优越的性能。

Abstract: Sheaf diffusion has recently emerged as a promising design pattern for graph
representation learning due to its inherent ability to handle heterophilic data
and avoid oversmoothing. Meanwhile, cooperative message passing has also been
proposed as a way to enhance the flexibility of information diffusion by
allowing nodes to independently choose whether to propagate/gather information
from/to neighbors. A natural question ensues: is sheaf diffusion capable of
exhibiting this cooperative behavior? Here, we provide a negative answer to
this question. In particular, we show that existing sheaf diffusion methods
fail to achieve cooperative behavior due to the lack of message directionality.
To circumvent this limitation, we introduce the notion of cellular sheaves over
directed graphs and characterize their in- and out-degree Laplacians. We
leverage our construction to propose Cooperative Sheaf Neural Networks (CSNNs).
Theoretically, we characterize the receptive field of CSNN and show it allows
nodes to selectively attend (listen) to arbitrarily far nodes while ignoring
all others in their path, potentially mitigating oversquashing. Our experiments
show that CSNN presents overall better performance compared to prior art on
sheaf diffusion as well as cooperative graph neural networks.

</details>


### [213] [GANs Secretly Perform Approximate Bayesian Model Selection](https://arxiv.org/abs/2507.00651)
*Maurizio Filippone,Marius P. Linhard*

Main category: cs.LG

TL;DR: 本文将GANs解释为概率生成模型，提出基于边缘似然优化和奥卡姆剃刀原理的正则化及优化策略，有效提升了GAN的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管GANs是成功的生成模型，但其优化过程复杂且难以驾驭，同时需要有效正则化以避免过拟合。

Method: 研究者将GANs解读为具有部分随机性的贝叶斯神经网络，并将其对抗式优化视为边缘似然代理的优化。利用边缘似然优化与奥卡姆剃刀原理的联系，提出了新的正则化和优化策略，旨在平滑损失函数并寻找更利于泛化的平坦极小值解。

Result: 实验结果表明，这些策略显著提升了GANs的性能。

Conclusion: 所提出的策略不仅带来了性能提升，也为深入理解GANs的正则化机制奠定了基础。

Abstract: Generative Adversarial Networks (GANs) are popular and successful generative
models. Despite their success, optimization is notoriously challenging and they
require regularization against overfitting. In this work, we explain the
success and limitations of GANs by interpreting them as probabilistic
generative models. This interpretation enables us to view GANs as Bayesian
neural networks with partial stochasticity, allowing us to establish conditions
of universal approximation. We can then cast the adversarial-style optimization
of several variants of GANs as the optimization of a proxy for the marginal
likelihood. Taking advantage of the connection between marginal likelihood
optimization and Occam's razor, we can define regularization and optimization
strategies to smooth the loss landscape and search for solutions with minimum
description length, which are associated with flat minima and good
generalization. The results on a wide range of experiments indicate that these
strategies lead to performance improvements and pave the way to a deeper
understanding of regularization strategies for GANs.

</details>


### [214] [Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models](https://arxiv.org/abs/2507.00653)
*Yilun Zhang*

Main category: cs.LG

TL;DR: 本文提出认知负荷感知推理(CLAI)框架，通过将认知负荷理论应用于LLM推理，旨在优化计算资源。该框架包括CLAI-Prompt和CLAI-Tune两种实现方式，能在不牺牲准确性的前提下，显著减少高达45%的token消耗，并展现出自主分解难题的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）推理的计算成本日益增加，成为其广泛和可持续部署的关键障碍。现有优化策略主要基于统计启发式或架构修改，缺乏指导推理过程的认知理论。

Method: 引入认知负荷感知推理（CLAI）框架，将认知负荷理论（CLT）和神经科学原理运用于LLM推理。将内在、无关和相关认知负荷形式化为可量化的LLM指标（$ICL_{LLM}$、$ECL_{LLM}$和$GCL_{LLM}$），将推理过程重构为认知经济优化问题。提出两种实现路径：CLAI-Prompt（通过结构化元提示引导基础LLM）和CLAI-Tune（通过微调使模型内化这些原则）。

Result: 在复杂推理、长上下文问答和代码生成等基准测试中，CLAI方法在不牺牲准确性的前提下，实现了显著的token消耗降低（高达45%）。此外，CLAI-Tune展现出自主分解难题的涌现能力，这是人类专家认知的一个关键特征。

Conclusion: 这项工作表明，通过模仿大脑的资源管理策略，我们可以构建更高效、更鲁棒、更强大的AI系统。

Abstract: The escalating computational costs of Large Language Model (LLM) inference
have become a critical barrier to their widespread and sustainable deployment.
While existing optimization strategies are effective, they are predominantly
based on statistical heuristics or architectural modifications, lacking a
guiding cognitive theory to manage the inference process itself. This paper
aims to bridge this gap by introducing a novel paradigm: the Cognitive
Load-Aware Inference (CLAI) framework, which operationalizes principles from
Cognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize
the concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and
Germane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$,
and $GCL_{LLM}$), thereby reframing the inference process as a cognitive
economics optimization problem: based on the intrinsic complexity of a problem
($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically
allocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two
implementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM
through cognitive control steps via a structured meta-prompt, and CLAI-Tune, a
fine-tuned model that internalizes these principles for spontaneous cognitive
economy. Across a range of benchmarks in complex reasoning, long-context
question answering, and code generation, our methods achieve significant
reductions in token consumption (up to 45\%) without sacrificing accuracy.
Furthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose
difficult problems, a key characteristic of human expert cognition. This work
demonstrates that by emulating the brain's resource management strategies, we
can build more efficient, robust, and capable artificial intelligence systems.

</details>


### [215] [Neural Augmented Kalman Filters for Road Network assisted GNSS positioning](https://arxiv.org/abs/2507.00654)
*Hans van Gorp,Davide Belli,Amir Jalalirad,Bence Major*

Main category: cs.LG

TL;DR: 本文提出一种基于时间图神经网络（TGNN）的方法，将道路网络信息集成到卡尔曼滤波器（KF）中，以提高GNSS在城市环境中的定位精度，并实现了29%的定位误差降低。


<details>
  <summary>Details</summary>
Motivation: 全球导航卫星系统（GNSS）在密集城市环境中因多径和非视距误差导致定位精度下降。现有利用道路网络数据的方法多限于离线应用或基于卡尔曼滤波器（KF）的启发式方法，灵活性和鲁棒性不足。

Method: 提出训练一个时间图神经网络（TGNN），将其与卡尔曼滤波器（KF）结合。TGNN旨在预测正确的路段及其相关不确定性，并将其用于KF的测量更新步骤。该方法使用真实世界的GNSS数据和开源道路网络进行了验证。

Result: 在具有挑战性的场景中，与仅使用GNSS的卡尔曼滤波器相比，定位误差降低了29%。

Conclusion: 据作者所知，这是首次将深度学习方法与道路网络数据和GNSS测量相结合，用于确定用户在地球上的位置，显著提升了城市环境下的定位精度。

Abstract: The Global Navigation Satellite System (GNSS) provides critical positioning
information globally, but its accuracy in dense urban environments is often
compromised by multipath and non-line-of-sight errors. Road network data can be
used to reduce the impact of these errors and enhance the accuracy of a
positioning system. Previous works employing road network data are either
limited to offline applications, or rely on Kalman Filter (KF) heuristics with
little flexibility and robustness. We instead propose training a Temporal Graph
Neural Network (TGNN) to integrate road network information into a KF. The TGNN
is designed to predict the correct road segment and its associated uncertainty
to be used in the measurement update step of the KF. We validate our approach
with real-world GNSS data and open-source road networks, observing a 29%
decrease in positioning error for challenging scenarios compared to a GNSS-only
KF. To the best of our knowledge, ours is the first deep learning-based
approach jointly employing road network data and GNSS measurements to determine
the user position on Earth.

</details>


### [216] [Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding](https://arxiv.org/abs/2507.00669)
*Duc Cao-Dinh,Khai Le-Duc,Anh Dao,Bach Phan Tat,Chris Ngo,Duy M. H. Nguyen,Nguyen X. Khanh,Thanh Nguyen-Tang*

Main category: cs.LG

TL;DR: 本文提出了Audio-3DVG框架，将语音信息集成到3D视觉定位任务中，通过对象提及检测和语音引导注意力模块，实现了语音基3D视觉定位的最新技术水平，并可与文本基方法媲美。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位研究主要依赖文本描述，而基于语音的3D视觉定位（Audio-based 3D Visual Grounding）仍未被充分探索且充满挑战。鉴于自动语音识别（ASR）和语音表示学习的进展，研究人员希望利用语音数据进行3D定位。

Method: 提出Audio-3DVG框架，整合语音和空间信息。任务被分解为两个互补部分：1) 对象提及检测（Object Mention Detection），一项多标签分类任务，用于明确识别语音中提及的对象；2) 语音引导注意力模块（Audio-Guided Attention module），用于捕捉候选对象与关系语音线索之间的交互。为支持基准测试，研究者为标准3DVG数据集（如ScanRefer、Sr3D和Nr3D）合成了语音描述。

Result: 实验结果表明，Audio-3DVG不仅在基于语音的定位任务中取得了新的最先进性能，而且能够与基于文本的方法竞争。

Conclusion: 该研究突出了将口语整合到3D视觉任务中的巨大潜力。

Abstract: 3D Visual Grounding (3DVG) involves localizing target objects in 3D point
clouds based on natural language. While prior work has made strides using
textual descriptions, leveraging spoken language-known as Audio-based 3D Visual
Grounding-remains underexplored and challenging. Motivated by advances in
automatic speech recognition (ASR) and speech representation learning, we
propose Audio-3DVG, a simple yet effective framework that integrates audio and
spatial information for enhanced grounding. Rather than treating speech as a
monolithic input, we decompose the task into two complementary components.
First, we introduce Object Mention Detection, a multi-label classification task
that explicitly identifies which objects are referred to in the audio, enabling
more structured audio-scene reasoning. Second, we propose an Audio-Guided
Attention module that captures interactions between candidate objects and
relational speech cues, improving target discrimination in cluttered scenes. To
support benchmarking, we synthesize audio descriptions for standard 3DVG
datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate
that Audio-3DVG not only achieves new state-of-the-art performance in
audio-based grounding, but also competes with text-based methods-highlighting
the promise of integrating spoken language into 3D vision tasks.

</details>


### [217] [Diffusion Classifier Guidance for Non-robust Classifiers](https://arxiv.org/abs/2507.00687)
*Philipp Vaeth,Dibyanshu Kumar,Benjamin Paassen,Magda Gregorová*

Main category: cs.LG

TL;DR: 本文提出一种新方法，使分类器引导扩散模型能够使用通用、非鲁棒的分类器，通过一步去噪预测和随机优化技术稳定引导过程，以应对噪声敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 大多数分类器引导方法仅限于鲁棒分类器，而通用非鲁棒分类器在扩散过程的噪声条件下表现出显著的准确性下降，导致引导梯度不稳定。研究动机是扩展分类器引导，使其能与未经噪声训练的通用非鲁棒分类器协同工作。

Method: 分析了非鲁棒和鲁棒分类器对扩散过程噪声的敏感性。提出一种方法，利用一步去噪图像预测，并借鉴随机优化方法（如指数移动平均）实现稳定技术，以解决不稳定的引导梯度问题。

Result: 实验结果表明，该方法提高了分类器引导的稳定性，同时保持了样本多样性和视觉质量。

Conclusion: 这项工作有助于推进生成模型中的条件采样技术，使更广泛的分类器能够用作引导分类器。

Abstract: Classifier guidance is intended to steer a diffusion process such that a
given classifier reliably recognizes the generated data point as a certain
class. However, most classifier guidance approaches are restricted to robust
classifiers, which were specifically trained on the noise of the diffusion
forward process. We extend classifier guidance to work with general,
non-robust, classifiers that were trained without noise. We analyze the
sensitivity of both non-robust and robust classifiers to noise of the diffusion
process on the standard CelebA data set, the specialized SportBalls data set
and the high-dimensional real-world CelebA-HQ data set. Our findings reveal
that non-robust classifiers exhibit significant accuracy degradation under
noisy conditions, leading to unstable guidance gradients. To mitigate these
issues, we propose a method that utilizes one-step denoised image predictions
and implements stabilization techniques inspired by stochastic optimization
methods, such as exponential moving averages. Experimental results demonstrate
that our approach improves the stability of classifier guidance while
maintaining sample diversity and visual quality. This work contributes to
advancing conditional sampling techniques in generative models, enabling a
broader range of classifiers to be used as guidance classifiers.

</details>


### [218] [A Test-Function Approach to Incremental Stability](https://arxiv.org/abs/2507.00695)
*Daniel Pfrommer,Max Simchowitz,Ali Jadbabaie*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents a novel framework for analyzing
Incremental-Input-to-State Stability ($\delta$ISS) based on the idea of using
rewards as "test functions." Whereas control theory traditionally deals with
Lyapunov functions that satisfy a time-decrease condition, reinforcement
learning (RL) value functions are constructed by exponentially decaying a
Lipschitz reward function that may be non-smooth and unbounded on both sides.
Thus, these RL-style value functions cannot be directly understood as Lyapunov
certificates. We develop a new equivalence between a variant of incremental
input-to-state stability of a closed-loop system under given a policy, and the
regularity of RL-style value functions under adversarial selection of a
H\"older-continuous reward function. This result highlights that the regularity
of value functions, and their connection to incremental stability, can be
understood in a way that is distinct from the traditional Lyapunov-based
approach to certifying stability in control theory.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [219] [Plan-Based Scalable Online Virtual Network Embedding](https://arxiv.org/abs/2507.00237)
*Oleg Kolosov,David Breitgand,Dean H. Lorenz,Gala Yadgar*

Main category: cs.NI

TL;DR: 本文提出一种名为OLIVE的新型在线算法，旨在解决现有在线虚拟网络嵌入(VNE)解决方案在边缘环境中扩展性不足的问题。OLIVE通过利用离线计算的聚合需求嵌入作为指导，显著提升了每秒请求处理能力，远超现有最佳方案。


<details>
  <summary>Details</summary>
Motivation: 虚拟网络嵌入（VNE）问题，尤其是在线VNE，对于在共享边缘基础设施上部署虚拟化应用至关重要。然而，现有在线VNE解决方案在处理高并发请求和大型物理拓扑时，存在严重的扩展性瓶颈，无法适应边缘环境中高度偏斜和不可预测的需求。

Method: 本文提出一种创新的在线算法OLIVE。该算法的核心思想是利用一个预先离线计算出的、针对聚合预期需求的近似最优嵌入方案。此离线嵌入作为OLIVE在线处理实际单个请求的指导，并允许算法动态补偿与预设计划的偏差。

Result: 实验结果表明，OLIVE解决方案处理每秒请求的能力比文献中报道的最佳结果高出两个数量级。

Conclusion: OLIVE算法卓越的扩展性使其特别适用于真实的边缘计算环境，能够有效应对其固有的高动态和不可预测性需求。

Abstract: Network virtualization allows hosting applications with diverse computation
and communication requirements on shared edge infrastructure. Given a set of
requests for deploying virtualized applications, the edge provider has to
deploy a maximum number of them to the underlying physical network, subject to
capacity constraints. This challenge is known as the virtual network embedding
(VNE) problem: it models applications as virtual networks, where virtual nodes
represent functions and virtual links represent communication between the
virtual nodes.
  All variants of VNE are known to be strongly NP-hard. Because of its
centrality to network virtualization, VNE has been extensively studied. We
focus on the online variant of VNE, in which deployment requests are not known
in advance. This reflects the highly skewed and unpredictable demand intrinsic
to the edge. Unfortunately, existing solutions to online VNE do not scale well
with the number of requests per second and the physical topology size.
  We propose a novel approach in which our new online algorithm, OLIVE,
leverages a nearly optimal embedding for an aggregated expected demand. This
embedding is computed offline. It serves as a plan that OLIVE uses as a guide
for handling actual individual requests while dynamically compensating for
deviations from the plan. We demonstrate that our solution can handle a number
of requests per second greater by two orders of magnitude than the best results
reported in the literature. Thus, it is particularly suitable for realistic
edge environments.

</details>


### [220] [Seeing Through the Fog: Empowering Mobile Devices to Expose and Mitigate RAN Buffer Effects on Delay-Sensitive Protocols](https://arxiv.org/abs/2507.00337)
*Yuxin Liu,Tianyang Zhang,Qiang Wu,Ju Ren,Kyle Jamieson,Yaxiong Xie*

Main category: cs.NI

TL;DR: 本文提出CellNinjia和Gandalf系统，用于识别并补偿蜂窝网络中无线接入网（RAN）缓冲区引入的非拥塞相关延迟，从而显著提升基于延迟的拥塞控制协议在蜂窝网络中的性能。


<details>
  <summary>Details</summary>
Motivation: 基于延迟的拥塞控制协议在蜂窝网络中表现不佳，因为无线接入网（RAN）缓冲区会引入大量与网络拥塞无关的延迟，这干扰了协议对实际拥塞的判断，并严重降低了其性能。

Method: 研究者开发了CellNinjia，一个提供RAN实时操作可见性的软件系统；并在此基础上设计了Gandalf，该系统利用CellNinjia的可见性来系统地识别和补偿RAN引起的特定延迟，而非将其视为随机噪声。

Result: 在商业4G LTE和5G网络中的评估显示，Gandalf显著提升了基于延迟的协议性能，例如Copa性能提升高达7.49倍，PCC Vivace高达9.53倍。这些性能提升是在不修改协议核心算法的前提下实现的。

Conclusion: 通过有效地识别和处理RAN引入的延迟，基于延迟的拥塞控制协议能够充分发挥其在蜂窝网络中的潜力，实现显著的性能提升。

Abstract: Delay-based protocols rely on end-to-end delay measurements to detect network
congestion. However, in cellular networks, Radio Access Network (RAN) buffers
introduce significant delays unrelated to congestion, fundamentally challenging
these protocols' assumptions. We identify two major types of RAN buffers -
retransmission buffers and uplink scheduling buffers - that can introduce
delays comparable to congestion-induced delays, severely degrading protocol
performance. We present CellNinjia, a software-based system providing real-time
visibility into RAN operations, and Gandalf, which leverages this visibility to
systematically handle RAN-induced delays. Unlike existing approaches that treat
these delays as random noise, Gandalf identifies specific RAN operations and
compensates for their effects. Our evaluation in commercial 4G LTE and 5G
networks shows that Gandalf enables substantial performance improvements - up
to 7.49x for Copa and 9.53x for PCC Vivace - without modifying the protocols'
core algorithms, demonstrating that delay-based protocols can realize their
full potential in cellular networks.

</details>


### [221] [Remote Rendering for Virtual Reality: performance comparison of multimedia frameworks and protocols](https://arxiv.org/abs/2507.00623)
*Daniel Mejías,Inhar Yeregui,Roberto Viola,Miguel Fernández,Mario Montagud*

Main category: cs.NI

TL;DR: 针对XR应用对算力和带宽的高需求，本文将GStreamer和FFmpeg与远程渲染引擎集成，并分析不同流媒体协议在WiFi和5G下的性能，构建了先进的XR研究测试平台。


<details>
  <summary>Details</summary>
Motivation: 复杂的Extended Reality (XR) 应用需要大量处理能力和高带宽通信，而轻量级设备往往无法满足这些要求。

Method: 本文描述了GStreamer和FFmpeg两大主流多媒体框架与远程渲染引擎的集成。通过WiFi或5G网络，分析了Web Real-Time Communications (WebRTC)、Dynamic Adaptive Streaming over HTTP (DASH) 以及基于QUIC的新兴协议在向终端设备交付渲染内容时的性能。

Result: 成功构建了一个超越现有技术水平的XR远程渲染测试平台。通过对不同协议在WiFi和5G网络下的性能分析，为XR领域的未来研究提供了基础。

Conclusion: 所构建的解决方案提供了一个先进的XR测试平台，将有助于推动该领域的尖端研究。

Abstract: The increasing complexity of Extended Reality (XR) applications demands
substantial processing power and high bandwidth communications, often
unavailable on lightweight devices. Remote rendering consists of offloading
processing tasks to a remote node with a powerful GPU, delivering the rendered
content to the end device. The delivery is usually performed through popular
streaming protocols such as Web Real-Time Communications (WebRTC), offering a
data channel for interactions, or Dynamic Adaptive Streaming over HTTP (DASH),
better suitable for scalability. Moreover, new streaming protocols based on
QUIC are emerging as potential replacements for WebRTC and DASH and offer
benefits like connection migration, stream multiplexing and multipath delivery.
This work describes the integration of the two most popular multimedia
frameworks, GStreamer and FFmpeg, with a rendering engine acting as a Remote
Renderer, and analyzes their performance when offering different protocols for
delivering the rendered content to the end device over WIFI or 5G. This
solution constitutes a beyond state-of-the-art testbed to conduct cutting-edge
research in the XR field.

</details>


### [222] [Toward Edge General Intelligence with Multiple-Large Language Model (Multi-LLM): Architecture, Trust, and Orchestration](https://arxiv.org/abs/2507.00672)
*Haoxiang Luo,Yinqiu Liu,Ruichen Zhang,Jiacheng Wang,Gang Sun,Dusit Niyato,Hongfang Yu,Zehui Xiong,Xianbin Wang,Xuemin Shen*

Main category: cs.NI

TL;DR: 本综述探讨了在边缘计算中集成多LLM系统，以克服传统AI模型的局限性，提升资源受限环境下的AI应用性能和适应性，并分析了其实现技术、信任机制和多模态架构。


<details>
  <summary>Details</summary>
Motivation: 传统边缘AI模型在处理复杂、动态任务（尤其是需要高级推理和多模态数据处理时）表现不足。因此，迫切需要在资源受限的边缘环境中，通过更先进的AI模型来提高性能和适应性。

Method: 本文采用综述方法，回顾了边缘AI从传统模型到单LLM再到多LLM系统的发展历程。具体讨论了使能技术，如动态编排、资源调度和跨域知识迁移；重点关注了可信多LLM系统，并介绍了多模态多LLM架构。

Result: 本综述分析了多LLM在边缘计算中的集成潜力，识别了实现多LLM系统的关键技术，并探讨了可信和多模态多LLM架构的重要性。此外，还提出了未来研究方向，包括资源效率、可信治理以及隐私、信任和鲁棒性。

Conclusion: 多LLM系统是提升边缘计算中AI应用性能和适应性的有效途径。本综述为旨在利用多LLM系统的研究人员和实践者提供了宝贵的参考，并指明了未来在资源效率、可信治理、隐私和鲁棒性方面的发展方向。

Abstract: Edge computing enables real-time data processing closer to its source, thus
improving the latency and performance of edge-enabled AI applications. However,
traditional AI models often fall short when dealing with complex, dynamic tasks
that require advanced reasoning and multimodal data processing. This survey
explores the integration of multi-LLMs (Large Language Models) to address this
in edge computing, where multiple specialized LLMs collaborate to enhance task
performance and adaptability in resource-constrained environments. We review
the transition from conventional edge AI models to single LLM deployment and,
ultimately, to multi-LLM systems. The survey discusses enabling technologies
such as dynamic orchestration, resource scheduling, and cross-domain knowledge
transfer that are key for multi-LLM implementation. A central focus is on
trusted multi-LLM systems, ensuring robust decision-making in environments
where reliability and privacy are crucial. We also present multimodal multi-LLM
architectures, where multiple LLMs specialize in handling different data
modalities, such as text, images, and audio, by integrating their outputs for
comprehensive analysis. Finally, we highlight future directions, including
improving resource efficiency, trustworthy governance multi-LLM systems, while
addressing privacy, trust, and robustness concerns. This survey provides a
valuable reference for researchers and practitioners aiming to leverage
multi-LLM systems in edge computing applications.

</details>


### [223] [Enhancing Vehicular Platooning with Wireless Federated Learning: A Resource-Aware Control Framework](https://arxiv.org/abs/2507.00856)
*Beining Wu,Jun Huang,Qiang Duan,Liang Dong,Zhipeng Cai*

Main category: cs.NI

TL;DR: 本文提出一个两阶段的资源感知控制框架 (RACE)，用于提升无线联邦学习 (WFL) 在车辆编队 (VP) 系统中的性能，通过联合优化信息年龄 (AoI) 和联邦学习模型漂移 (FLMD)，实现了更好的学习收敛和对动态环境的适应性。


<details>
  <summary>Details</summary>
Motivation: 在高度动态的车辆编队 (VP) 环境中，集成了无线联邦学习 (WFL) 的系统面临频繁的通信变化和资源限制，严重影响信息交换和学习模型同步。为确保及时准确的控制，需要解决信息时效性 (AoI) 和联邦学习模型漂移 (FLMD) 的挑战。

Method: 将VP中的WFL建模为一个同时考虑信息年龄 (AoI) 和联邦学习模型漂移 (FLMD) 的联合优化问题。提出一个两阶段的资源感知控制框架 (RACE)：第一阶段利用拉格朗日对偶分解方法进行资源配置；第二阶段采用多智能体深度强化学习方法进行车辆选择，该方法融合了多头自注意力机制和长短期记忆网络，以捕获通信状态的时空相关性。

Result: 实验结果显示，与基线方法相比，所提出的RACE框架在AI4MARS数据集上将AoI优化提升高达45%，加速了学习收敛，并能更有效地适应动态的VP环境。

Conclusion: 所提出的RACE框架有效解决了动态车辆编队环境中无线联邦学习的挑战，显著提升了信息时效性、学习收敛速度以及系统对动态环境的适应能力。

Abstract: This paper aims to enhance the performance of Vehicular Platooning (VP)
systems integrated with Wireless Federated Learning (WFL). In highly dynamic
environments, vehicular platoons experience frequent communication changes and
resource constraints, which significantly affect information exchange and
learning model synchronization. To address these challenges, we first formulate
WFL in VP as a joint optimization problem that simultaneously considers Age of
Information (AoI) and Federated Learning Model Drift (FLMD) to ensure timely
and accurate control. Through theoretical analysis, we examine the impact of
FLMD on convergence performance and develop a two-stage Resource-Aware Control
framework (RACE). The first stage employs a Lagrangian dual decomposition
method for resource configuration, while the second stage implements a
multi-agent deep reinforcement learning approach for vehicle selection. The
approach integrates Multi-Head Self-Attention and Long Short-Term Memory
networks to capture spatiotemporal correlations in communication states.
Experimental results demonstrate that, compared to baseline methods, the
proposed framework improves AoI optimization by up to 45%, accelerates learning
convergence, and adapts more effectively to dynamic VP environments on the
AI4MARS dataset.

</details>


### [224] [QUIC Delay Control: an implementation of congestion and delay control](https://arxiv.org/abs/2507.00896)
*Saverio Mascolo,Andrea Vittorio Balillo,Gioacchino Manfredi,Davide D'Agostino,Luca De Cicco*

Main category: cs.NI

TL;DR: 提出了一种名为QUIC-DC的新型拥塞和延迟控制算法，通过估计单向排队延迟提前应对拥塞，旨在同时减少丢包和端到端通信延迟，并保持网络利用率。


<details>
  <summary>Details</summary>
Motivation: 现有的拥塞控制算法未能有效控制转发路径上的排队延迟，而排队延迟是影响网络性能，特别是实时应用性能的关键因素。因此，需要一种能同时控制拥塞和排队延迟的新算法。

Method: 核心思想是估计连接的单向排队延迟，以触发对拥塞的早期反应。该算法在QUIC-DC中实现了这一思想，并结合了TCP Westwood+拥塞控制算法。通过在模拟和真实网络连接中与QUIC Cubic、BBRv2、NewReno和Westwood+进行比较来评估其性能。

Result: 实验结果表明，QUIC-DC能显著减少数据包丢失和端到端通信延迟，同时保持良好的网络利用率。

Conclusion: QUIC-DC的优势在于其能够同时有效减少丢包和延迟并维持网络利用率，使其成为对实时应用非常有益的解决方案。

Abstract: A new congestion and delay control algorithm named QUIC Delay Control
(QUIC-DC) is proposed for controlling not only congestion but also the queuing
delay encountered along the forward communication path. The core idea is to
estimate the one-way queuing delay of a connection to trigger an early reaction
to congestion. This idea, along with the TCP Westwood+ congestion control
algorithm, has been implemented in QUIC-DC and compared with QUIC Cubic, BBRv2,
NewReno, Westwood+. The results obtained in both emulated and real network
connections show that QUIC-DC can significantly reduce packet losses along with
end-to-end communication delays, while preserving network utilization, features
that are both very useful for real-time applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [225] [Real-Time In-Network Machine Learning on P4-Programmable FPGA SmartNICs with Fixed-Point Arithmetic and Taylor](https://arxiv.org/abs/2507.00428)
*Mohammad Firas Sada,John J. Graham,Mahidhar Tatineni,Dmitry Mishin,Thomas A. DeFanti,Frank Würthwein*

Main category: cs.DC

TL;DR: 论文探索了如何利用P4可编程的FPGA SmartNICs实现网络内机器学习，通过将模型权重存储在控制平面表中，从而支持在网络边缘低延迟、灵活地部署可再训练的ML模型。


<details>
  <summary>Details</summary>
Motivation: 现代网络操作中，对能够实现低延迟ML推理（例如QoS预测和网络安全异常检测）的网络可编程性需求日益增长。

Method: 本研究利用P4可编程的FPGA SmartNICs作为平台，探索将P4编程范式应用于神经网络和回归模型，具体方法是将模型权重和偏差存储在控制平面的表查找中。

Result: 该方法实现了在网络边缘灵活可编程和高效部署可再训练的ML模型，且部署独立于核心交换机基础设施。

Conclusion: P4编程范式与FPGA SmartNICs的结合，为在网络边缘高效集成和部署动态、可重训练的机器学习模型提供了一个可行且灵活的解决方案，满足了网络内低延迟ML推理的需求。

Abstract: As machine learning (ML) applications become integral to modern network
operations, there is an increasing demand for network programmability that
enables low-latency ML inference for tasks such as Quality of Service (QoS)
prediction and anomaly detection in cybersecurity. ML models provide
adaptability through dynamic weight adjustments, making Programming
Protocol-independent Packet Processors (P4)-programmable FPGA SmartNICs an
ideal platform for investigating In-Network Machine Learning (INML). These
devices offer high-throughput, low-latency packet processing and can be
dynamically reconfigured via the control plane, allowing for flexible
integration of ML models directly at the network edge. This paper explores the
application of the P4 programming paradigm to neural networks and regression
models, where weights and biases are stored in control plane table lookups.
This approach enables flexible programmability and efficient deployment of
retrainable ML models at the network edge, independent of core infrastructure
at the switch level.

</details>


### [226] [PANDAS: Peer-to-peer, Adaptive Networking for Data Availability Sampling within Ethereum Consensus Timebounds](https://arxiv.org/abs/2507.00824)
*Matthieu Pigaglio,Onur Ascigil,Michał Król,Sergi Rene,Felix Lange,Kaleem Peeroo,Ramin Sadre,Vladimir Stankovic,Etienne Rivière*

Main category: cs.DC

TL;DR: PANDAS是一种实用的解决方案，旨在无需修改以太坊核心协议的情况下，在Danksharding下实现Layer-2数据的高效传播和可用性采样，解决了在4秒共识槽期限内完成数据处理的严苛挑战。


<details>
  <summary>Details</summary>
Motivation: 以太坊的吞吐量有限，Layer-2协议通过全局广播数据限制了自身的可扩展性。Danksharding计划通过随机数据可用性采样（DAS）来支持Layer-2数据的选择性分发，但现有方案难以在每个共识槽的4秒内完成数据传播和采样。

Method: 本文提出了PANDAS，一种在不修改以太坊共识和节点发现协议的前提下，通过轻量级、直接的数据交换来传播Layer-2数据并采样其可用性的方法。PANDAS的设计考虑了消息丢失、节点故障、无响应参与者及网络扩展性。

Result: PANDAS原型在1,000个节点的集群和20,000个对等点的模拟中表现出色，证实其能够在全球尺度的网络延迟下，在4秒的严格时限内完成Layer-2数据的传播和可用性采样。

Conclusion: PANDAS成功解决了Danksharding中数据可用性采样（DAS）与以太坊共识集成所面临的关键时间限制，为以太坊实现其所需的吞吐量和可扩展性提供了切实可行的路径。

Abstract: Layer-2 protocols can assist Ethereum's limited throughput, but globally
broadcasting layer-2 data limits their scalability. The Danksharding evolution
of Ethereum aims to support the selective distribution of layer-2 data, whose
availability in the network is verified using randomized data availability
sampling (DAS). Integrating DAS into Ethereum's consensus process is
challenging, as pieces of layer-2 data must be disseminated and sampled within
four seconds of the beginning of each consensus slot. No existing solution can
support dissemination and sampling under such strict time bounds.
  We propose PANDAS, a practical approach to integrate DAS with Ethereum under
Danksharding's requirements without modifying its protocols for consensus and
node discovery. PANDAS disseminates layer-2 data and samples its availability
using lightweight, direct exchanges. Its design accounts for message loss, node
failures, and unresponsive participants while anticipating the need to scale
out the Ethereum network. Our evaluation of PANDAS's prototype in a 1,000-node
cluster and simulations for up to 20,000 peers shows that it allows layer-2
data dissemination and sampling under planetary-scale latencies within the
4-second deadline.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [227] [Estimating Correctness Without Oracles in LLM-Based Code Generation](https://arxiv.org/abs/2507.00057)
*Thomas Valentin,Ardi Madadi,Gaetano Sapia,Marcel Böhme*

Main category: cs.PL

TL;DR: 提出并验证了一种名为“不一致性”的新度量方法，用于在缺乏参考实现（预言机）的情况下，高效评估大型语言模型（LLMs）生成代码的正确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言代码生成方面表现出色，但存在“幻觉”问题，即生成语法正确但逻辑错误的程序。在没有现有正确实现（预言机）的情况下，难以量化生成代码的正确性。

Method: 本文提出了一种名为“不一致性”（incoherence）的错误度量方法。该方法可以在没有预言机的情况下高效估算，并能提供LLM生成程序不正确概率的下限。

Result: 实验结果显示该方法具有卓越的有效性。在平均代码生成任务中，基于不一致性的方法可以自动识别约三分之二的错误程序，且无误报。基于不一致性的LLM评估可以可靠地替代基于预言机的评估，并且两种评估方法对LLMs正确程序数量的排名高度一致。

Conclusion: “不一致性”度量方法是评估LLM生成代码正确性的有效工具，即使没有参考实现也能提供可靠的错误估计。它能够作为传统预言机评估的有效替代方案，提升了LLM代码生成评估的实用性。

Abstract: Generating code from natural language specifications is one of the most
successful applications of Large Language Models (LLMs). Yet, they hallucinate:
LLMs produce outputs that may be grammatically correct but are factually
incorrect. Without an existing, correct implementation (i.e., an oracle), can
we quantify how likely the generated program is correct?
  In this paper, we propose a measure of incorrectness, called incoherence,
that can be estimated efficiently in the absence of an oracle and provides a
lower bound on the error, i.e., the probability that the LLM-generated program
for that specification is incorrect. Our experiments demonstrate an
extraordinary effectiveness. For the average code generation task, our
incoherence-based methodology can automatically identify about two-thirds of
incorrect programs without reports of false positives. In fact, an oracle-based
evaluation of LLMs can be reliably replaced by an incoherence-based evaluation.
In particular, we find a very strong agreement between the ranking of LLMs by
the number of programs deemed correct via an oracle (pass@1) and the ranking of
LLMs by the number of programs deemed correct via our incoherence.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [228] [Edge Computing and its Application in Robotics: A Survey](https://arxiv.org/abs/2507.00523)
*Nazish Tahir,Ramviyas Parasuraman*

Main category: cs.RO

TL;DR: 本文是一篇综述，旨在全面分析边缘计算在机器人领域的应用、关键优势、最新进展、面临的挑战以及未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 边缘计算能为时间敏感型机器人应用提供低延迟、移动性和位置感知能力，但目前缺乏对其与机器人技术集成优势的全面近期综述。此外，现实世界机器人应用对快速响应时间有关键需求。

Method: 本文通过文献调研，对边缘机器人领域的最新发展进行了全面评估，深入分析了该领域的关键动机、挑战和未来方向，并探讨了边缘计算在实际机器人场景中的重要性，最后概述了开放的研究挑战。

Result: 文章提供了边缘机器人领域最新进展的全面评估，深入剖析了其关键动机、挑战和未来发展方向，并强调了边缘计算在对快速响应时间要求高的真实世界机器人场景中的重要性。

Conclusion: 本文弥补了当前边缘机器人领域全面综述的空白，为未来的研究指明了方向，并提出了该领域的各种开放性研究挑战。

Abstract: The Edge computing paradigm has gained prominence in both academic and
industry circles in recent years. By implementing edge computing facilities and
services in robotics, it becomes a key enabler in the deployment of artificial
intelligence applications to robots. Time-sensitive robotics applications
benefit from the reduced latency, mobility, and location awareness provided by
the edge computing paradigm, which enables real-time data processing and
intelligence at the network's edge. While the advantages of integrating edge
computing into robotics are numerous, there has been no recent survey that
comprehensively examines these benefits. This paper aims to bridge that gap by
highlighting important work in the domain of edge robotics, examining recent
advancements, and offering deeper insight into the challenges and motivations
behind both current and emerging solutions. In particular, this article
provides a comprehensive evaluation of recent developments in edge robotics,
with an emphasis on fundamental applications, providing in-depth analysis of
the key motivations, challenges, and future directions in this rapidly evolving
domain. It also explores the importance of edge computing in real-world
robotics scenarios where rapid response times are critical. Finally, the paper
outlines various open research challenges in the field of edge robotics.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [229] [Multimodal, Multi-Disease Medical Imaging Foundation Model (MerMED-FM)](https://arxiv.org/abs/2507.00185)
*Yang Zhou,Chrystie Wan Ning Quek,Jun Zhou,Yan Wang,Yang Bai,Yuhe Ke,Jie Yao,Laura Gutierrez,Zhen Ling Teo,Darren Shu Jeng Ting,Brian T. Soetikno,Christopher S. Nielsen,Tobias Elze,Zengxiang Li,Linh Le Dinh,Lionel Tim-Ee Cheng,Tran Nguyen Tuan Anh,Chee Leong Cheng,Tien Yin Wong,Nan Liu,Iain Beehuat Tan,Tony Kiat Hon Lim,Rick Siow Mong Goh,Yong Liu,Daniel Shu Wei Ting*

Main category: eess.IV

TL;DR: 开发了MerMED-FM，一个多模态、多专业的医学影像基础模型，通过自监督学习和记忆模块在大量多源数据上训练，并在多种模态上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学影像AI模型大多是单模态、单疾病的，多模态/多疾病模型的尝试临床准确性不稳定，且训练需要大量人工标记数据。

Method: 开发了MerMED-FM，一个先进的多模态、多专业基础模型。该模型使用自监督学习和记忆模块进行训练，训练数据包含来自十多个专科和七种模态（CT、CXR、US、病理切片、CFP、OCT、皮肤图像）的330万张医学图像。

Result: MerMED-FM在所有模态上均表现出色，AUROC分别为：OCT 0.988；病理 0.982；US 0.951；CT 0.943；皮肤 0.931；CFP 0.894；CXR 0.858。

Conclusion: MerMED-FM有潜力成为一个高度适应性、多功能、跨专业的基础模型，能够实现对不同医学领域医学影像的稳健解读。

Abstract: Current artificial intelligence models for medical imaging are predominantly
single modality and single disease. Attempts to create multimodal and
multi-disease models have resulted in inconsistent clinical accuracy.
Furthermore, training these models typically requires large, labour-intensive,
well-labelled datasets. We developed MerMED-FM, a state-of-the-art multimodal,
multi-specialty foundation model trained using self-supervised learning and a
memory module. MerMED-FM was trained on 3.3 million medical images from over
ten specialties and seven modalities, including computed tomography (CT), chest
X-rays (CXR), ultrasound (US), pathology patches, color fundus photography
(CFP), optical coherence tomography (OCT) and dermatology images. MerMED-FM was
evaluated across multiple diseases and compared against existing foundational
models. Strong performance was achieved across all modalities, with AUROCs of
0.988 (OCT); 0.982 (pathology); 0.951 (US); 0.943 (CT); 0.931 (skin); 0.894
(CFP); 0.858 (CXR). MerMED-FM has the potential to be a highly adaptable,
versatile, cross-specialty foundation model that enables robust medical imaging
interpretation across diverse medical disciplines.

</details>


### [230] [SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures](https://arxiv.org/abs/2507.00209)
*Fengyi Jiang,Xiaorui Zhang,Lingbo Jin,Ruixing Liang,Yuxin Chen,Adi Chola Venkatesh,Jason Culman,Tiantian Wu,Lirong Shao,Wenqing Sun,Cong Gao,Hallie McNamara,Jingpei Lu,Omid Mohareri*

Main category: eess.IV

TL;DR: 本文介绍了SurgiSR4K，这是首个公开的、原生4K分辨率的机器人辅助微创手术影像和视频数据集，旨在填补高分辨率手术数据在计算机视觉研究中的空白。


<details>
  <summary>Details</summary>
Motivation: 高分辨率成像对于提高微创手术（MIS）的视觉清晰度和计算机辅助引导至关重要。尽管4K内窥镜系统日益普及，但目前仍缺乏专门针对机器人辅助MIS的公共原生4K数据集。

Method: 研究者通过收集和整理，构建并发布了SurgiSR4K数据集。该数据集以原生4K分辨率捕获，代表了机器人辅助手术的真实条件，并包含了各种视觉场景，如镜面反射、工具遮挡、出血和软组织变形等常见挑战。

Result: SurgiSR4K成为首个公开可用的原生4K手术影像和视频数据集，它涵盖了腹腔镜和机器人手术中常见的多种复杂视觉情景，为高分辨率数据在计算机视觉任务中的应用提供了可能性。

Conclusion: SurgiSR4K数据集为高分辨率手术成像研究奠定了坚实基础，并有望推动智能成像技术的发展，从而提升图像引导机器人手术的性能、安全性和可用性。

Abstract: High-resolution imaging is crucial for enhancing visual clarity and enabling
precise computer-assisted guidance in minimally invasive surgery (MIS). Despite
the increasing adoption of 4K endoscopic systems, there remains a significant
gap in publicly available native 4K datasets tailored specifically for
robotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible
surgical imaging and video dataset captured at a native 4K resolution,
representing realistic conditions of robotic-assisted procedures. SurgiSR4K
comprises diverse visual scenarios including specular reflections, tool
occlusions, bleeding, and soft tissue deformations, meticulously designed to
reflect common challenges faced during laparoscopic and robotic surgeries. This
dataset opens up possibilities for a broad range of computer vision tasks that
might benefit from high resolution data, such as super resolution (SR), smoke
removal, surgical instrument detection, 3D tissue reconstruction, monocular
depth estimation, instance segmentation, novel view synthesis, and
vision-language model (VLM) development. SurgiSR4K provides a robust foundation
for advancing research in high-resolution surgical imaging and fosters the
development of intelligent imaging technologies aimed at enhancing performance,
safety, and usability in image-guided robotic surgeries.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [231] [Investigating Stochastic Methods for Prosody Modeling in Speech Synthesis](https://arxiv.org/abs/2507.00227)
*Paul Mayer,Florian Lux,Alejandro Pérez-González-de-Martos,Angelina Elizarova,Lindsey Vanderlyn,Dirk Väth,Ngoc Thang Vu*

Main category: eess.AS

TL;DR: 本文探讨了文本到语音合成中生成富有表现力韵律的挑战，研究并评估了多种随机方法（如规范化流），发现它们能生成与人类表现相当的自然韵律，并提升了可控性。


<details>
  <summary>Details</summary>
Motivation: 在文本到语音合成中，生成富有表现力的韵律仍然是一项挑战，尤其是在为了可解释性和可控性而显式建模音高、能量和和时长等参数的系统中。

Method: 研究人员调查了包括规范化流、条件流匹配和修正流在内的随机方法在此任务上的有效性。他们将这些方法与传统的确定性基线以及真实人类发音进行了广泛的主观和客观评估。

Result: 广泛的评估表明，随机方法通过捕获人类语音固有的变异性，能够产生与人类说话者相媲美的自然韵律。此外，通过允许调整采样温度，这些方法还提供了额外的可控性选项。

Conclusion: 随机方法能够有效地为文本到语音合成生成自然且富有表现力的韵律，其性能可与人类相媲美，并提供了增强的控制能力。

Abstract: While generative methods have progressed rapidly in recent years, generating
expressive prosody for an utterance remains a challenging task in
text-to-speech synthesis. This is particularly true for systems that model
prosody explicitly through parameters such as pitch, energy, and duration,
which is commonly done for the sake of interpretability and controllability. In
this work, we investigate the effectiveness of stochastic methods for this
task, including Normalizing Flows, Conditional Flow Matching, and Rectified
Flows. We compare these methods to a traditional deterministic baseline, as
well as to real human realizations. Our extensive subjective and objective
evaluations demonstrate that stochastic methods produce natural prosody on par
with human speakers by capturing the variability inherent in human speech.
Further, they open up additional controllability options by allowing the
sampling temperature to be tuned.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [232] [Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)](https://arxiv.org/abs/2507.00094)
*Jacobo Casas-Ramos,Sarah Winkler,Alessandro Gianola,Marco Montali,Manuel Mucientes,Manuel Lama*

Main category: cs.DB

TL;DR: 本文提出了一种新颖的算法，结合A*搜索和SMT求解，能够在高效且表达力强的情况下，为包含通用数据类型和数据条件的数据感知Declare模型计算最优对齐，解决了现有对齐一致性检查方法在处理复杂数据依赖时的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管对数据感知规范的过程分析和挖掘日益增长，但声明式过程模型中基于对齐的一致性检查主要关注纯控制流或仅限于数值数据和常量比较的简单数据感知扩展。这是因为在存在数据依赖的情况下，寻找对齐在计算上非常困难。因此，现有方法难以有效处理具有通用数据类型和复杂数据条件的声明式模型。

Method: 作者结合了两种处理控制流和数据依赖的最佳已知方法：A*搜索和SMT求解。具体来说，他们引入了一种新颖的算法技术，通过应用旨在逐步解决约束冲突的“修复操作”来有效地探索搜索空间，生成后继状态。

Result: 研究证明了算法的正确性，并通过实验验证了其效率。评估结果表明，该方法在性能上与现有技术相当或超越，同时显著支持更具表达力的数据依赖。

Conclusion: 本文成功证明，即使在包含通用数据类型和数据条件的丰富数据感知Declare设置中，也能高效且表达力强地计算最优数据感知对齐。这克服了现有工具在处理复杂数据感知过程模型时的局限性，并展示了其在支持真实世界应用方面的巨大潜力。

Abstract: Despite growing interest in process analysis and mining for data-aware
specifications, alignment-based conformance checking for declarative process
models has focused on pure control-flow specifications, or mild data-aware
extensions limited to numerical data and variable-to-constant comparisons. This
is not surprising: finding alignments is computationally hard, even more so in
the presence of data dependencies. In this paper, we challenge this problem in
the case where the reference model is captured using data-aware Declare with
general data types and data conditions. We show that, unexpectedly, it is
possible to compute data-aware optimal alignments in this rich setting,
enjoying at once efficiency and expressiveness. This is achieved by carefully
combining the two best-known approaches to deal with control flow and data
dependencies when computing alignments, namely A* search and SMT solving.
Specifically, we introduce a novel algorithmic technique that efficiently
explores the search space, generating descendant states through the application
of repair actions aiming at incrementally resolving constraint violations. We
prove the correctness of our algorithm and experimentally show its efficiency.
The evaluation witnesses that our approach matches or surpasses the performance
of the state of the art while also supporting significantly more expressive
data dependencies, showcasing its potential to support real-world applications.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [233] [Discovering the underlying analytic structure within Standard Model constants using artificial intelligence](https://arxiv.org/abs/2507.00225)
*S. V. Chekanov,H. Kjellerstrand*

Main category: hep-ph

TL;DR: 本文利用符号回归和遗传编程，探索标准模型基本参数间的解析结构，并发现了一系列高精度关系。


<details>
  <summary>Details</summary>
Motivation: 旨在发现标准模型（SM）基本参数间潜在的解析结构和隐藏模式。

Method: 采用符号回归（symbolic regression）和遗传编程（genetic programming）来识别这些参数间的解析关系。

Result: 发现了连接标准模型基本常数对的最简单解析关系，并报告了约一千个相对精度优于1%的表达式。

Conclusion: 这些结果可为模型构建者和人工智能方法提供宝贵输入，以揭示标准模型常数间的隐藏模式，并可能作为构建更深层物理定律的基础。

Abstract: This paper presents a search for underlying analytic structures among the
fundamental parameters of the Standard Model (SM) using symbolic regression and
genetic programming. We identify the simplest analytic relationships connecting
pairs of these constants and report several notable observations based on about
a thousand expressions with relative precision better than 1%. These results
may serve as valuable inputs for model builders and artificial intelligence
methods aimed at uncovering hidden patterns among the SM constants, or
potentially used as building blocks for a deeper underlying law that connects
all parameters of the SM through a small set of fundamental constants.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [234] [Integrating Universal Generative AI Platforms in Educational Labs to Foster Critical Thinking and Digital Literacy](https://arxiv.org/abs/2507.00007)
*Vasiliy Znamenskiy,Rafael Niyazov,Joel Hernandez*

Main category: cs.CY

TL;DR: 提出一种新的教育框架，将生成式AI（GenAI）融入实验室活动，旨在培养大学生的批判性思维和数字素养。


<details>
  <summary>Details</summary>
Motivation: 认识到学生对大型语言模型（LLMs）不加批判的依赖存在的局限性和风险，该研究旨在将GenAI重新定义为教育中的研究对象和认知工具。

Method: 该框架指导学生制定特定学科的提示，并评估GenAI在文本、图像和视频模式下生成的响应。通过在一门面向非理科专业学生的天文学课程中进行试点。

Result: 试点实施显示学生参与度高，批判性反思深入，许多学生课后继续活动并在研究研讨会上展示成果。结果表明，结构化的AI互动结合反思性评估方法可以提高学习成果。

Conclusion: GenAI在教育中与结构化互动和反思性评估相结合，能有效提升学习成果、批判性思维和数字素养。该模型可复制应用于其他科学学科的跨学科AI集成实验室工作。

Abstract: This paper presents a new educational framework for integrating generative
artificial intelligence (GenAI) platforms such as ChatGPT, Claude, and Gemini
into laboratory activities aimed at developing critical thinking and digital
literacy among undergraduate students. Recognizing the limitations and risks of
uncritical reliance on large language models (LLMs), the proposed pedagogical
model reframes GenAI as a research subject and cognitive tool. Students
formulate discipline-specific prompts and evaluate GenAI-generated responses in
text, image, and video modalities. A pilot implementation in a general
astronomy course for non-science majors demonstrated high levels of engagement
and critical reflection, with many students continuing the activity after class
and presenting results at a research symposium. The results highlight the
importance of structured AI interactions in education and suggest that GenAI
can improve learning outcomes when combined with reflective assessment methods.
The study proposes a replicable model for interdisciplinary AI-integrated lab
work, adaptable to scientific disciplines. See the guide to learning activities
based on Generative-Ai platforms: https://doi.org/10.5281/zenodo.15555802

</details>


### [235] [Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing](https://arxiv.org/abs/2507.00032)
*Grey Kuling,Marinka Zitnik*

Main category: cs.CY

TL;DR: KUL-KT是一个受生物学启发的新型知识追踪模型，结合赫布记忆和梯度巩固，实现高效的个性化学习，支持多类型输入，并在基准测试和实际部署中表现优异。


<details>
  <summary>Details</summary>
Motivation: 旨在将神经系统的记忆巩固原理应用于学生建模，以实现更高效的知识追踪，解决现有模型在遗忘、持续学习、个性化和数据效率等方面的挑战。

Method: KUL-KT结合了赫布记忆编码（用于快速、单次交互更新）和梯度巩固（用于慢速、线性网络巩固），引入了时间衰减赫布记忆更新（实现自然遗忘）和损失对齐内部目标（LIT）方法（实现无需反向传播的持续学习）。它在嵌入空间操作，支持结构化和非结构化输入。

Result: KUL-KT在十个公共知识追踪基准测试中超越了现有强基线，在课堂部署中显著提升了学习者感知到的帮助性并降低了难度。消融研究证实其核心创新对持续适应至关重要。与图基模型相比，KUL-KT训练速度快1.75倍，内存使用量减少99.01%。

Conclusion: KUL-KT是一个具有生物学基础、内存高效、输入灵活的框架，能够实现大规模个性化学习，且支持少量样本个性化和自然遗忘，无需存储原始数据或依赖大规模队列训练。

Abstract: We introduce KUL-KT, a biologically inspired architecture for knowledge
tracing (KT), combining Hebbian memory encoding with gradient-based
consolidation in a scalable, input-agnostic framework. KUL-KT adapts the
principle of memory consolidation in neural systems, to student modeling by
introducing two key innovations: (i) a time-decaying Hebbian memory update that
enables graceful forgetting, and (ii) a novel Loss-aligned Internal Target
(LIT) method to compute an ideal internal state, allowing continual learning
without backpropagation through time. The architecture consists of a fast
Hebbian memory that captures each learner interaction via a single associative
update, and a slower linear network that consolidates recalled samples through
gradient descent. This design enables few-shot personalization and natural
forgetting without storing raw data or relying on large cohort training.
Operating entirely in embedding space, KUL-KT supports both structured
(tabular) and unstructured (short-answer) inputs. Empirically, KUL-KT
outperforms strong baselines on ten public KT benchmarks in rank-sensitive
metrics such as nDCG and Recall@10. In a classroom deployment, KUL-KT
personalized quizzes from short-answer data, leading to improved
learner-perceived helpfulness and reduced difficulty (p < 0.05). Ablation
studies confirm that Hebbian decay and LIT are critical for continual
adaptation. Compared to a strong graph-based KT model, KUL-KT trains 1.75x
faster and uses 99.01\% less memory. These results position KUL-KT as a
biologically grounded, memory-efficient, and input-flexible framework for
personalized learning at scale.

</details>


### [236] [Teaching Programming in the Age of Generative AI: Insights from Literature, Pedagogical Proposals, and Student Perspectives](https://arxiv.org/abs/2507.00108)
*Clemente Rubio-Manzano,Jazna Meza,Rodolfo Fernandez-Santibanez,Christian Vidal-Castro*

Main category: cs.CY

TL;DR: 本文探讨了LLM对编程教育的影响，回顾了相关研究，并提出通过代码可视化和执行模拟来加强学生对代码的理解，而非仅仅关注编写，初步的学生反馈支持此方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）正在彻底改变计算机编程，进而影响大学入门级编程课程的教学、学习和评估方式，这引发了关于如何应对生成式AI挑战的深入讨论。

Method: 文章一方面回顾了关于生成式AI在编程教育中应用的现有研究，总结了其优缺点；另一方面，提出了通过关注代码理解和执行来丰富教学方法，具体倡导使用代码的视觉表示和执行的视觉模拟作为有效的教学工具；最后，引用了学生关于面向对象编程课程的意见，以提供支持将视觉模拟纳入教学过程的初步背景。

Result: 文献回顾揭示了生成式AI在编程教育中的利弊；提出了一种以代码理解和执行为核心的教学方法论；学生意见初步支持了将Java（或其他语言）中的视觉模拟融入培训过程的价值。

Conclusion: 通过倡导使用代码的视觉表示和执行的视觉模拟，可以有效促进学生对编程的更深层次理解，从而改进编程的教学、学习和评估。初步的学生反馈为此方法的引入提供了支持。

Abstract: Computer programming is undergoing a true transformation driven by powerful
new tools for automatic source code generation based on large language models.
This transformation is also manifesting in introductory programming courses at
universities around the world, generating an in-depth debate about how
programming content should be taught, learned, and assessed in the context of
generative artificial intelligence.
  This article aims, on the one hand, to review the most relevant studies on
this issue, highlighting the advantages and disadvantages identified in the
specialized literature. On the other hand, it proposes enriching teaching and
learning methodologies by focusing on code comprehension and execution rather
than on mere coding or program functionality. In particular, it advocates for
the use of visual representations of code and visual simulations of its
execution as effective tools for teaching, learning, and assessing programming,
thus fostering a deeper understanding among students.
  Finally, the opinions of students who took the object-oriented programming
course are presented to provide preliminary context supporting the
incorporation of visual simulations in Java (or other languages) as part of the
training process.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [237] [MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models](https://arxiv.org/abs/2507.00487)
*Jianghao Lin,Xinyuan Wang,Xinyi Dai,Menghui Zhu,Bo Chen,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: 本文提出MassTool，一个多任务搜索框架，旨在通过增强查询表示和工具检索准确性，提升LLM的工具检索能力。


<details>
  <summary>Details</summary>
Motivation: 现有工具检索方法主要侧重优化工具表示，而忽略了精确查询理解的重要性，这导致了LLM与外部工具交互的效率不足。

Method: MassTool采用双塔架构：工具使用检测塔（预测函数调用需求）和工具检索塔（利用QC-GCN进行查询-工具匹配）。它还引入了基于搜索的用户意图建模（SUIM）以处理多样化查询，以及自适应知识迁移（AdaKT）模块实现高效多任务学习。通过联合优化工具使用检测损失、列表式检索损失和对比正则化损失，MassTool建立了一个鲁棒的双步序贯决策流程来精确理解查询。

Result: 广泛的实验证明了MassTool在提高检索准确性方面的有效性。

Conclusion: MassTool通过其新颖的多任务框架和联合优化策略，有效提升了工具检索的准确性，尤其是在精确查询理解方面。

Abstract: Tool retrieval is a critical component in enabling large language models
(LLMs) to interact effectively with external tools. It aims to precisely filter
the massive tools into a small set of candidates for the downstream
tool-augmented LLMs. However, most existing approaches primarily focus on
optimizing tool representations, often neglecting the importance of precise
query comprehension. To address this gap, we introduce MassTool, a multi-task
search-based framework designed to enhance both query representation and tool
retrieval accuracy. MassTool employs a two-tower architecture: a tool usage
detection tower that predicts the need for function calls, and a tool retrieval
tower that leverages a query-centric graph convolution network (QC-GCN) for
effective query-tool matching. It also incorporates search-based user intent
modeling (SUIM) to handle diverse and out-of-distribution queries, alongside an
adaptive knowledge transfer (AdaKT) module for efficient multi-task learning.
By jointly optimizing tool usage detection loss, list-wise retrieval loss, and
contrastive regularization loss, MassTool establishes a robust dual-step
sequential decision-making pipeline for precise query understanding. Extensive
experiments demonstrate its effectiveness in improving retrieval accuracy. Our
code is available at https://github.com/wxydada/MassTool.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [238] [State and Memory is All You Need for Robust and Reliable AI Agents](https://arxiv.org/abs/2507.00081)
*Matthew Muhoberac,Atharva Parikh,Nirvi Vakharia,Saniya Virani,Aco Radujevic,Savannah Wood,Meghav Verma,Dimitri Metaxotos,Jeyaraman Soundararajan,Thierry Masquelin,Alexander G. Godfrey,Sean Gardner,Dobrila Rudnicki,Sam Michael,Gaurav Chopra*

Main category: cs.MA

TL;DR: SciBORG是一个基于LLM的模块化智能体框架，通过FSA记忆解决了LLM在复杂科学工作流中记忆、规划和工具集成挑战，实现了自主、可靠且可扩展的任务执行。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在自然语言理解和生成方面取得了巨大进步，但其在复杂、真实世界的科学工作流中的应用仍受限于记忆、规划和工具集成方面的挑战。

Method: 本文提出了SciBORG（Scientific Bespoke Artificial Intelligence Agents Optimized for Research Goals），一个模块化的智能体框架。该框架允许基于LLM的智能体自主规划、推理并执行稳健可靠的领域特定任务。智能体通过源代码文档动态构建，并利用有限状态自动机（FSA）记忆增强，实现持久状态跟踪和上下文感知决策。此方法无需手动提示工程，并能通过维护跨扩展工作流的上下文以及从工具或执行故障中恢复，实现跨多样应用的稳健、可扩展部署。

Result: SciBORG通过与物理（如微波合成器）和虚拟硬件集成进行了验证，展示了其在执行用户指定反应和自主从PubChem数据库检索多步骤生物测定方面的能力，包括多步骤规划、推理、智能体间通信与协作。系统基准测试表明，SciBORG智能体实现了可靠的执行、自适应规划和可解释的状态转换。

Conclusion: 研究结果表明，记忆和状态感知是实现智能体规划和可靠性的关键要素，为在复杂环境中部署AI智能体提供了可泛化的基础。

Abstract: Large language models (LLMs) have enabled powerful advances in natural
language understanding and generation. Yet their application to complex,
real-world scientific workflows remain limited by challenges in memory,
planning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke
Artificial Intelligence Agents Optimized for Research Goals), a modular agentic
framework that allows LLM-based agents to autonomously plan, reason, and
achieve robust and reliable domain-specific task execution. Agents are
constructed dynamically from source code documentation and augmented with
finite-state automata (FSA) memory, enabling persistent state tracking and
context-aware decision-making. This approach eliminates the need for manual
prompt engineering and allows for robust, scalable deployment across diverse
applications via maintaining context across extended workflows and to recover
from tool or execution failures. We validate SciBORG through integration with
both physical and virtual hardware, such as microwave synthesizers for
executing user-specified reactions, with context-aware decision making and
demonstrate its use in autonomous multi-step bioassay retrieval from the
PubChem database utilizing multi-step planning, reasoning, agent-to-agent
communication and coordination for execution of exploratory tasks. Systematic
benchmarking shows that SciBORG agents achieve reliable execution, adaptive
planning, and interpretable state transitions. Our results show that memory and
state awareness are critical enablers of agentic planning and reliability,
offering a generalizable foundation for deploying AI agents in complex
environments.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [239] [How large language models judge and influence human cooperation](https://arxiv.org/abs/2507.00088)
*Alexandre S. Pires,Laurens Samson,Sennay Ghebreab,Fernando P. Santos*

Main category: physics.soc-ph

TL;DR: 本研究评估了大型语言模型（LLM）对合作行为的判断，并利用演化博弈论模型探讨其对人类合作的长期影响。结果显示LLM在判断声誉不佳的个体时存在差异，这些差异会影响合作的普及，并指出需要通过提示词校准LLM规范以维护人类合作。


<details>
  <summary>Details</summary>
Motivation: 人类日益依赖LLM辅助社会决策，先前的研究表明LLM会影响道德和政治判断。然而，LLM驱动的社会决策对人类合作（通常由间接互惠、声誉和判断他人互动驱动）的长期影响尚不明确。

Method: 1. 向21种主流LLM提供大量涵盖各种社会情境的合作或拒绝合作示例，并要求它们进行判断。2. 通过演化博弈论模型，评估在LLM驱动判断盛行的人群中合作的动态，预测LLM对人类亲社会行为的长期影响。3. 测试引导LLM规范的提示词，以塑造其判断。

Result: 1. LLM在评估与良好对手的合作时表现出显著一致性。2. 在判断与声誉不佳个体的合作时，模型内部和模型之间存在差异。3. 模型间的差异会显著影响合作的普遍性。4. 通过目标导向的提示词可以有效引导和塑造LLM的判断。

Conclusion: 本研究将LLM的建议与长期社会动态联系起来，并强调为维护人类合作，需谨慎校准LLM的规范。

Abstract: Humans increasingly rely on large language models (LLMs) to support decisions
in social settings. Previous work suggests that such tools shape people's moral
and political judgements. However, the long-term implications of LLM-based
social decision-making remain unknown. How will human cooperation be affected
when the assessment of social interactions relies on language models? This is a
pressing question, as human cooperation is often driven by indirect
reciprocity, reputations, and the capacity to judge interactions of others.
Here, we assess how state-of-the-art LLMs judge cooperative actions. We provide
21 different LLMs with an extensive set of examples where individuals cooperate
-- or refuse cooperating -- in a range of social contexts, and ask how these
interactions should be judged. Furthermore, through an evolutionary
game-theoretical model, we evaluate cooperation dynamics in populations where
the extracted LLM-driven judgements prevail, assessing the long-term impact of
LLMs on human prosociality. We observe a remarkable agreement in evaluating
cooperation against good opponents. On the other hand, we notice within- and
between-model variance when judging cooperation with ill-reputed individuals.
We show that the differences revealed between models can significantly impact
the prevalence of cooperation. Finally, we test prompts to steer LLM norms,
showing that such interventions can shape LLM judgements, particularly through
goal-oriented prompts. Our research connects LLM-based advices and long-term
social dynamics, and highlights the need to carefully align LLM norms in order
to preserve human cooperation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [240] [Stealtooth: Breaking Bluetooth Security Abusing Silent Automatic Pairing](https://arxiv.org/abs/2507.00847)
*Keiichiro Kimura,Hiroki Kuzuno,Yoshiaki Shiraishi,Masakatu Morii*

Main category: cs.CR

TL;DR: 研究发现蓝牙自动配对功能存在未探索的漏洞，并提出了“Stealtooth”攻击，可静默覆盖设备链接密钥并实现中间人攻击。该漏洞在多种商业设备中普遍存在，攻击成本低，且已向厂商披露。


<details>
  <summary>Details</summary>
Motivation: 蓝牙自动配对功能虽提升了用户便利性，却创造了一个此前未被探索的攻击面，可能威胁到依赖配对过程的蓝牙通信安全。因此，研究旨在探索并利用这一潜在漏洞。

Method: 研究者开发了“Stealtooth”攻击，通过滥用商业蓝牙设备自动配对功能中的未知漏洞，利用蓝牙音频设备在特定条件下自动进入配对模式的特性，实现设备链接密钥的静默覆盖。攻击还扩展为“MitM Stealtooth”，结合自动配对滥用和省电模式技术实现中间人攻击。研究对10款商业蓝牙设备进行了评估，仅使用通用硬件和开源软件。

Result: 研究结果表明，不同类型和制造商的商业蓝牙设备普遍存在该漏洞。攻击实施门槛低，仅需商品硬件和开源软件。研究揭示了安全性和可用性之间的关键矛盾，证明当前的自动配对实现方式会产生系统性漏洞。

Conclusion: 当前的蓝牙自动配对实现方式存在系统性漏洞，对设备安全构成威胁。研究者提出了设备和协议层面的防御措施，包括增强用户通知和标准化自动配对指南，以缓解安全与可用性之间的冲突。漏洞已负责任地披露给受影响厂商，部分已发布补丁。

Abstract: Bluetooth is a pervasive wireless communication technology used by billions
of devices for short-range connectivity. The security of Bluetooth relies on
the pairing process, where devices establish shared long-term keys for secure
communications. However, many commercial Bluetooth devices implement automatic
pairing functions to improve user convenience, creating a previously unexplored
attack surface.
  We present Stealtooth, a novel attack that abuses unknown vulnerabilities in
the automatic pairing functions in commercial Bluetooth devices to achieve
completely silent device link key overwriting. The Stealtooth attack leverages
the fact that Bluetooth audio devices automatically transition to pairing mode
under specific conditions, enabling attackers to hijack pairing processes
without user awareness or specialized tools. We also extend the attack into the
MitM Stealtooth attack, combining automatic pairing abuse with power-saving
mode techniques to enable man-in-the-middle attacks.
  We evaluate the attacks against 10 commercial Bluetooth devices from major
manufacturers, demonstrating widespread vulnerabilities across diverse device
types and manufacturers. Our practical implementation requires only commodity
hardware and open-source software, highlighting the low barrier to entry for
attackers.
  We propose defenses both device and protocol levels, including enhanced user
notifications and standardized automatic pairing guidelines. Our findings
reveal a critical tension between security and usability, showing that current
automatic pairing implementations create systematic vulnerabilities. We
responsibly disclosed our findings to affected vendors, with several already
releasing patches.

</details>


### [241] [Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds](https://arxiv.org/abs/2507.00740)
*Craig S Wright*

Main category: cs.CR

TL;DR: 本文对Bitcoin白皮书中定义的SPV进行了完整的形式化规范、协议描述和数学证明，澄清了误解并展示其安全性与最优性。


<details>
  <summary>Details</summary>
Motivation: 针对流行实现中对SPV的误解，本文旨在澄清并证明SPV在有限对抗假设下是安全的，且对于需要可扩展和可验证交易包含的数字现金系统而言，它是严格最优的。

Method: 作者从第一性原理重建SPV协议，将其验证模型建立在符号自动机、默克尔成员关系和证明链支配谓词之上。通过严谨的概率和博弈论分析，推导出协议安全运行的经济边界，并验证了在部分连接、恶意中继网络和对抗性传播延迟下的活跃性和安全性。同时，引入了自适应轮询和压缩头同步等低带宽优化。

Result: 研究表明，在有限对抗假设下，原始SPV不仅安全，而且对于需要可扩展和可验证交易包含的数字现金系统而言，它是严格最优的。通过分析得出了协议安全运行的经济边界，并验证了在部分连接、恶意中继网络和对抗性传播延迟等条件下，协议的活跃性和安全性。此外，还引入了低带宽优化并保持了正确性。

Conclusion: 本文档可作为安全SPV实现的蓝图，并全面反驳了围绕非验证客户端的常见误解，重申了原始设计的稳健性。

Abstract: This paper presents a complete formal specification, protocol description,
and mathematical proof structure for Simplified Payment Verification (SPV) as
originally defined in the Bitcoin whitepaper \cite{nakamoto2008}. In stark
contrast to the misrepresentations proliferated by popular implementations, we
show that SPV is not only secure under bounded adversarial assumptions but
strictly optimal for digital cash systems requiring scalable and verifiable
transaction inclusion. We reconstruct the SPV protocol from first principles,
grounding its verification model in symbolic automata, Merkle membership
relations, and chain-of-proof dominance predicates. Through rigorous
probabilistic and game-theoretic analysis, we derive the economic bounds within
which the protocol operates securely and verify its liveness and safety
properties under partial connectivity, hostile relay networks, and adversarial
propagation delay. Our specification further introduces low-bandwidth
optimisations such as adaptive polling and compressed header synchronisation
while preserving correctness. This document serves both as a blueprint for
secure SPV implementation and a rebuttal of common misconceptions surrounding
non-validating clients.

</details>


### [242] [AI-Governed Agent Architecture for Web-Trustworthy Tokenization of Alternative Assets](https://arxiv.org/abs/2507.00096)
*Ailiya Borjigin,Wei Zhou,Cong He*

Main category: cs.CR

TL;DR: 本文提出一种由AI治理的代理架构，结合智能代理与区块链，以解决替代资产通证化在网络上缺乏信任的挑战，显著提升通证化过程的透明度、安全性和合规性。


<details>
  <summary>Details</summary>
Motivation: 替代资产通证化在网络上交易面临信任挑战，尤其是在链下数据验证和监管合规方面，亟需解决方案来确保通证化生态系统的可信度。

Method: 提出一个AI治理的代理架构，将自主代理（负责资产验证、估值、合规、生命周期管理）与区块链结合。一个AI驱动的治理层通过自适应策略和加密经济激励来监控代理行为并强制执行信任。

Result: 该方法显著提升了资产通证化的透明度、安全性和合规性，有效解决了数据真实性和欺诈问题。通过对房地产资产的案例研究，证明了该架构能通过实时AI异常检测和链上强制执行，有效规避欺诈性列表和洗钱等风险。

Conclusion: 结合AI治理、多代理系统和区块链可以显著增强通证化资产生态系统的信任度。本研究为网络上可信的资产通证化提供了一个新颖的框架，并为实践者部署安全、合规的通证化平台提供了见解。

Abstract: Alternative Assets tokenization is transforming non-traditional financial
instruments are represented and traded on the web. However, ensuring
trustworthiness in web-based tokenized ecosystems poses significant challenges,
from verifying off-chain asset data to enforcing regulatory compliance. This
paper proposes an AI-governed agent architecture that integrates intelligent
agents with blockchain to achieve web-trustworthy tokenization of alternative
assets. In the proposed architecture, autonomous agents orchestrate the
tokenization process (asset verification, valuation, compliance checking, and
lifecycle management), while an AI-driven governance layer monitors agent
behavior and enforces trust through adaptive policies and cryptoeconomic
incentives. We demonstrate that this approach enhances transparency, security,
and compliance in asset tokenization, addressing key concerns around data
authenticity and fraud. A case study on tokenizing real estate assets
illustrates how the architecture mitigates risks (e.g., fraudulent listings and
money laundering) through real-time AI anomaly detection and on-chain
enforcement. Our evaluation and analysis suggest that combining AI governance
with multi-agent systems and blockchain can significantly bolster trust in
tokenized asset ecosystems. This work offers a novel framework for trustworthy
asset tokenization on the web and provides insights for practitioners aiming to
deploy secure, compliant tokenization platforms.

</details>


### [243] [AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise](https://arxiv.org/abs/2507.00145)
*Hasan Yiğit*

Main category: cs.CR

TL;DR: 本文提出AI-Hybrid TRNG，一个深度学习框架，利用低成本硬件和CPU抖动从物理噪声中提取高熵随机数。它通过多项统计测试，性能优于传统方法，且占用空间小，适用于资源受限设备，拓宽了高完整性随机数的应用。


<details>
  <summary>Details</summary>
Motivation: 传统的真随机数发生器（TRNG）通常需要笨重的量子设备或昂贵的实验室级射频接收器，这限制了其普及和在资源受限环境中的应用。研究旨在开发一种成本低廉、高效且能产生高质量随机数的新方法。

Method: AI-Hybrid TRNG是一个深度学习框架，通过一个低成本、拇指大小的射频前端和CPU时序抖动进行训练，直接从物理噪声中提取近乎均匀的熵，并输出32位高熵流，无需量化步骤。它采用动态内外网络，结合自适应自然源和重播机制，以生成真正不可预测和自主的序列。

Result: 生成的随机数通过NIST SP 800-22测试，表现优于基于CPU的方法。它还通过了19项定制的位级和整数级统计测试，所有结果均满足密码学标准，且前向和后向预测实验未发现可利用的偏差。此外，模型占用空间小于0.5 MB，使其可部署在微控制器（MCUs）、FPGA软核以及其他资源受限平台上。

Conclusion: AI-Hybrid TRNG通过将随机性质量与专用硬件需求分离，极大地扩展了高完整性随机数发生器在安全系统、密码协议、嵌入式和边缘设备、随机模拟以及服务器应用中的可及性和应用范围。

Abstract: AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform
entropy directly from physical noise, eliminating the need for bulky quantum
devices or expensive laboratory-grade RF receivers. Instead, it relies on a
low-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and
then emits 32-bit high-entropy streams without any quantization step.
  Unlike deterministic or trained artificial intelligence random number
generators (RNGs), our dynamic inner-outer network couples adaptive natural
sources and reseeding, yielding truly unpredictable and autonomous sequences.
Generated numbers pass the NIST SP 800-22 battery better than a CPU-based
method. It also passes nineteen bespoke statistical tests for both bit- and
integer-level analysis. All results satisfy cryptographic standards, while
forward and backward prediction experiments reveal no exploitable biases. The
model's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft
cores, as well as suitable for other resource-constrained platforms.
  By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG
broadens the reach of high-integrity random number generators across secure
systems, cryptographic protocols, embedded and edge devices, stochastic
simulations, and server applications that need randomness.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [244] [InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph](https://arxiv.org/abs/2507.00066)
*Xingyu Xiao,Jiejuan Tong,Peng Chen,Jun Sun,Zhe Sui,Jingang Liang,Hongru Zhao,Jun Zhao,Haitao Wang*

Main category: cs.HC

TL;DR: 本文提出InSight-R框架，通过结合经验行为数据和知识图谱，实现客观、自动化的人为故障事件（HFE）识别，并支持在数字化环境中进行实时人因可靠性评估，以克服传统HRA方法的主观性和局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的人因可靠性分析（HRA）方法过度依赖专家判断，导致其可重复性差、主观性强，且难以整合界面层数据，尤其无法严格评估人机界面设计如何影响操作员表现和错误倾向。

Method: 本研究提出了由AutoGraph驱动的InSight-R框架，用于风险知情的人为故障事件识别和界面诱发风险评估。该框架通过将经验行为数据与自动图基执行框架（AutoGraph）构建的界面嵌入式知识图谱（IE-KG）关联，实现了基于易错和偏时操作路径的自动化HFE识别。此外，研究还探讨了设计者-用户冲突与人为错误的关系。

Result: InSight-R框架不仅增强了HFE识别的客观性和可解释性，还为在数字化控制环境中进行动态、实时的人因可靠性评估提供了可扩展的途径。

Conclusion: 该框架为界面设计优化提供了可操作的见解，并促进了机制驱动型人因可靠性分析方法学的进步。

Abstract: Human reliability remains a critical concern in safety-critical domains such
as nuclear power, where operational failures are often linked to human error.
While conventional human reliability analysis (HRA) methods have been widely
adopted, they rely heavily on expert judgment for identifying human failure
events (HFEs) and assigning performance influencing factors (PIFs). This
reliance introduces challenges related to reproducibility, subjectivity, and
limited integration of interface-level data. In particular, current approaches
lack the capacity to rigorously assess how human-machine interface design
contributes to operator performance variability and error susceptibility. To
address these limitations, this study proposes a framework for risk-informed
human failure event identification and interface-induced risk assessment driven
by AutoGraph (InSight-R). By linking empirical behavioral data to the
interface-embedded knowledge graph (IE-KG) constructed by the automated
graph-based execution framework (AutoGraph), the InSight-R framework enables
automated HFE identification based on both error-prone and time-deviated
operational paths. Furthermore, we discuss the relationship between
designer-user conflicts and human error. The results demonstrate that InSight-R
not only enhances the objectivity and interpretability of HFE identification
but also provides a scalable pathway toward dynamic, real-time human
reliability assessment in digitalized control environments. This framework
offers actionable insights for interface design optimization and contributes to
the advancement of mechanism-driven HRA methodologies.

</details>


### [245] [Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments](https://arxiv.org/abs/2507.00161)
*Christopher M. Wegemer,Edward Halim,Jeff Burke*

Main category: cs.HC

TL;DR: 本研究开发了一个AI驱动的数字公民故事平台，通过情感自适应叙事减少政治两极分化，促进开放心态和换位思考。


<details>
  <summary>Details</summary>
Motivation: 政治两极分化损害民主公民教育，加剧了对立观点的身份抵制。新兴AI技术为减少两极分化、促进政治开放心态提供了新机遇。

Method: 采用基于设计的研发(DBR)方法，迭代开发了AI辅助数字公民故事(AI-DCS)平台。该平台结合政治心理学和叙事学理论，利用情感计算（面部表情识别、注意力追踪）实时评估用户状态，并通过GPT-4进行语言自适应，个性化叙事内容以维持情感投入，促进对不同政治观点的换位思考。

Result: 本工作为AI支持的、情感敏感的策略奠定了基础，这些策略旨在解决情感两极分化，同时保留学习者自主性。

Conclusion: 为公民教育干预、算法素养以及AI对话管理和情感自适应学习环境相关的人机交互挑战提供了启示。

Abstract: Political polarization undermines democratic civic education by exacerbating
identity-based resistance to opposing viewpoints. Emerging AI technologies
offer new opportunities to advance interventions that reduce polarization and
promote political open-mindedness. We examined novel design strategies that
leverage adaptive and emotionally-responsive civic narratives that may sustain
students' emotional engagement in stories, and in turn, promote
perspective-taking toward members of political out-groups. Drawing on theories
from political psychology and narratology, we investigate how affective
computing techniques can support three storytelling mechanisms: transportation
into a story world, identification with characters, and interaction with the
storyteller. Using a design-based research (DBR) approach, we iteratively
developed and refined an AI-mediated Digital Civic Storytelling (AI-DCS)
platform. Our prototype integrates facial emotion recognition and attention
tracking to assess users' affective and attentional states in real time.
Narrative content is organized around pre-structured story outlines, with
beat-by-beat language adaptation implemented via GPT-4, personalizing
linguistic tone to sustain students' emotional engagement in stories that
center political perspectives different from their own. Our work offers a
foundation for AI-supported, emotionally-sensitive strategies that address
affective polarization while preserving learner autonomy. We conclude with
implications for civic education interventions, algorithmic literacy, and HCI
challenges associated with AI dialogue management and affect-adaptive learning
environments.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [246] [$σ$-Maximal Ancestral Graphs](https://arxiv.org/abs/2507.00093)
*Binghua Yao,Joris M. Mooij*

Main category: cs.DM

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Maximal Ancestral Graphs (MAGs) provide an abstract representation of
Directed Acyclic Graphs (DAGs) with latent (selection) variables. These
graphical objects encode information about ancestral relations and
d-separations of the DAGs they represent. This abstract representation has been
used amongst others to prove the soundness and completeness of the FCI
algorithm for causal discovery, and to derive a do-calculus for its output. One
significant inherent limitation of MAGs is that they rule out the possibility
of cyclic causal relationships. In this work, we address that limitation. We
introduce and study a class of graphical objects that we coin
''$\sigma$-Maximal Ancestral Graphs'' (''$\sigma$-MAGs''). We show how these
graphs provide an abstract representation of (possibly cyclic) Directed Graphs
(DGs) with latent (selection) variables, analogously to how MAGs represent
DAGs. We study the properties of these objects and provide a characterization
of their Markov equivalence classes.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [247] [Verifiable Natural Language to Linear Temporal Logic Translation: A Benchmark Dataset and Evaluation Suite](https://arxiv.org/abs/2507.00877)
*William H English,Chase Walker,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: eess.SY

TL;DR: 现有自然语言到时间逻辑翻译系统在接地和验证方面评估不足。本文提出VLTL-Bench，一个统一的基准测试集，旨在全面评估这些关键能力，促进可验证系统的发展。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言（NL）到时间逻辑（TL）翻译系统的评估在现有基准上表现接近完美，但这些研究仅衡量逻辑翻译的准确性，忽略了系统将原子命题接地到新场景或环境的能力，而这对于在具体状态空间中验证结果公式至关重要。这导致性能指标虚高，并忽视了可扩展、领域通用系统的需求。

Method: 本文引入了“可验证线性时间逻辑基准”（VLTL-Bench），这是一个统一的基准，用于衡量自动化NL到LTL翻译的验证和可验证性。该数据集包含三个独特的状态空间、数千种多样的自然语言规范及其对应的形式化时间逻辑规范。此外，基准还包含示例轨迹以验证时间逻辑表达式。它不仅支持端到端评估，还为分解过程的每个子步骤（提升、接地、翻译、验证）提供了真实数据。

Result: 创建并发布了VLTL-Bench基准测试集，这是一个统一的、包含多状态空间、多样化自然语言及形式化规范、以及用于验证的时间逻辑表达式示例轨迹的综合数据集。该基准能够测量自动化NL到LTL翻译的验证和可验证性，并为过程中各子步骤提供真实数据，以支持细致研究和改进。

Conclusion: VLTL-Bench通过提供一个测量接地和验证能力的统一基准，填补了现有NL到TL翻译系统评估的空白。该基准将鼓励在可验证的NL到LTL翻译方法方面取得更具方法论严谨性的进展，推动领域发展，以构建更具鲁棒性和通用性的系统。

Abstract: Empirical evaluation of state-of-the-art natural-language (NL) to
temporal-logic (TL) translation systems reveals near-perfect performance on
existing benchmarks. However, current studies measure only the accuracy of the
translation of NL logic into formal TL, ignoring a system's capacity to ground
atomic propositions into new scenarios or environments. This is a critical
feature, necessary for the verification of resulting formulas in a concrete
state space. Consequently, most NL-to-TL translation frameworks propose their
own bespoke dataset in which the correct grounding is known a-priori, inflating
performance metrics and neglecting the need for extensible, domain-general
systems. In this paper, we introduce the Verifiable Linear Temporal Logic
Benchmark ( VLTL-Bench), a unifying benchmark that measures verification and
verifiability of automated NL-to-LTL translation. The dataset consists of three
unique state spaces and thousands of diverse natural language specifications
and corresponding formal specifications in temporal logic. Moreover, the
benchmark contains sample traces to validate the temporal logic expressions.
While the benchmark directly supports end-to-end evaluation, we observe that
many frameworks decompose the process into i) lifting, ii) grounding, iii)
translation, and iv) verification. The benchmark provides ground truths after
each of these steps to enable researches to improve and evaluate different
substeps of the overall problem. To encourage methodologically sound advances
in verifiable NL-to-LTL translation approaches, we release VLTL-Bench here:
https://www.kaggle.com/datasets/dubascudes/vltl bench.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [248] [Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture](https://arxiv.org/abs/2507.00466)
*Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

TL;DR: 本文提出一个基于Transformer的端到端模型，用于MIDI音乐中的节拍和下拍追踪，通过新颖的数据预处理和Transformer架构，在多数据集上超越了现有符号音乐节拍追踪方法。


<details>
  <summary>Details</summary>
Motivation: MIDI音乐表演中的节拍追踪对于乐谱级音乐转录和节奏分析至关重要，但现有方法主要集中于音频领域，符号音乐的节拍追踪仍具挑战。

Method: 提出一个端到端基于Transformer的模型，采用编码器-解码器架构将MIDI输入转换为节拍标注。引入了创新的数据预处理技术，包括动态增强和优化的标记化策略，以提高准确性和泛化性。

Result: 在A-MAPS、ASAP、GuitarSet和Leduc等多个数据集上进行广泛实验，与现有隐马尔可夫模型（HMMs）和深度学习节拍追踪方法相比，本模型优于现有的符号音乐节拍追踪方法，并在各种音乐风格和乐器上取得了具有竞争力的F1分数。

Conclusion: 研究结果突出了Transformer架构在符号节拍追踪方面的潜力，并建议未来可与自动音乐转录系统集成，以增强音乐分析和乐谱生成能力。

Abstract: Beat tracking in musical performance MIDI is a challenging and important task
for notation-level music transcription and rhythmical analysis, yet existing
methods primarily focus on audio-based approaches. This paper proposes an
end-to-end transformer-based model for beat and downbeat tracking in
performance MIDI, leveraging an encoder-decoder architecture for
sequence-to-sequence translation of MIDI input to beat annotations. Our
approach introduces novel data preprocessing techniques, including dynamic
augmentation and optimized tokenization strategies, to improve accuracy and
generalizability across different datasets. We conduct extensive experiments
using the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model
against state-of-the-art hidden Markov models (HMMs) and deep learning-based
beat tracking methods. The results demonstrate that our model outperforms
existing symbolic music beat tracking approaches, achieving competitive
F1-scores across various musical styles and instruments. Our findings highlight
the potential of transformer architectures for symbolic beat tracking and
suggest future integration with automatic music transcription systems for
enhanced music analysis and score generation.

</details>


### [249] [Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection](https://arxiv.org/abs/2507.00693)
*Yifan Gao,Jiao Fu,Long Guo,Hong Liu*

Main category: cs.SD

TL;DR: 该研究利用语音数据，结合大型语言模型（LLM）和传统特征，识别青少年自杀风险，并在SW1挑战中排名第一。


<details>
  <summary>Details</summary>
Motivation: 早期识别自杀风险对预防自杀行为至关重要。研究旨在探索语音作为一种无创、易获取的心理健康指标，用于识别有自杀风险的青少年。

Method: 采用大型语言模型（LLM）作为主要的特征提取工具，并结合传统的声学和语义特征对语音数据进行分析。

Result: 所提出的方法在测试集上取得了74%的准确率，并在SW1挑战赛中排名第一。

Conclusion: 这些发现表明，基于LLM的方法在自杀风险评估的语音分析中具有巨大潜力。

Abstract: Early identification of suicide risk is crucial for preventing suicidal
behaviors. As a result, the identification and study of patterns and markers
related to suicide risk have become a key focus of current research. In this
paper, we present the results of our work in the 1st SpeechWellness Challenge
(SW1), which aims to explore speech as a non-invasive and easily accessible
mental health indicator for identifying adolescents at risk of suicide.Our
approach leverages large language model (LLM) as the primary tool for feature
extraction, alongside conventional acoustic and semantic features. The proposed
method achieves an accuracy of 74\% on the test set, ranking first in the SW1
challenge. These findings demonstrate the potential of LLM-based methods for
analyzing speech in the context of suicide risk assessment.

</details>


### [250] [Multi-interaction TTS toward professional recording reproduction](https://arxiv.org/abs/2507.00808)
*Hiroki Kanagawa,Kenichi Fujita,Aya Watanabe,Yusuke Ijima*

Main category: cs.SD

TL;DR: 本文提出了一种多步交互式文本转语音（TTS）方法，使用户能够像配音导演一样，迭代地精炼合成语音的风格，解决了现有TTS系统缺乏细致风格调整的问题。


<details>
  <summary>Details</summary>
Motivation: 在文本转语音（TTS）领域，用户在初始合成后无法对语音风格进行细粒度迭代调整，导致合成语音常与用户预期不符。这种在真实录音中至关重要的迭代反馈精炼过程在TTS中被忽视。

Method: 提出了一种具有多步交互能力的TTS方法，该方法通过模拟配音演员与导演之间的关系，允许用户直观、快速地对合成语音的风格进行迭代精炼。为此，还构建了相应的交互数据集。

Result: 实验结果表明，所提出的模型及其对应数据集能够根据用户的指示实现迭代式的风格精炼，从而证明了其多交互能力。

Conclusion: 本研究成功地将迭代式风格精炼引入TTS，使用户能够更精确、直观地控制合成语音的风格，弥补了现有TTS在精细风格调整方面的不足。

Abstract: Voice directors often iteratively refine voice actors' performances by
providing feedback to achieve the desired outcome. While this iterative
feedback-based refinement process is important in actual recordings, it has
been overlooked in text-to-speech synthesis (TTS). As a result, fine-grained
style refinement after the initial synthesis is not possible, even though the
synthesized speech often deviates from the user's intended style. To address
this issue, we propose a TTS method with multi-step interaction that allows
users to intuitively and rapidly refine synthetized speech. Our approach models
the interaction between the TTS model and its user to emulate the relationship
between voice actors and voice directors. Experiments show that the proposed
model with its corresponding dataset enable iterative style refinements in
accordance with users' directions, thus demonstrating its multi-interaction
capability. Sample audios are available: https://ntt-hilab-gensp.
github.io/ssw13multiinteraction_tts/

</details>


### [251] [A High-Fidelity Speech Super Resolution Network using a Complex Global Attention Module with Spectro-Temporal Loss](https://arxiv.org/abs/2507.00229)
*Tarikul Islam Tamiti,Biraj Joshi,Rida Hasan,Rashedul Hasan,Taieba Athay,Nursad Mamun,Anomadarshi Barua*

Main category: cs.SD

TL;DR: CTFT-Net是一个语音超分辨率模型，通过在复数域中重建幅度和相位来增强低分辨率语音，并在极端上采样任务中超越现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有语音超分辨率方法大多侧重于幅度重建，但最新研究指出相位重建对提高感知质量至关重要。因此，需要一种能够同时处理幅度和相位的模型。

Method: 本文提出了CTFT-Net，一个在复数时频域中同时重建幅度和相位的网络。它集成了复数全局注意力块以建模音素间和频率间依赖，以及复数Conformer以捕获长程和局部特征，从而改善频率重建和噪声鲁棒性。模型采用时域和多分辨率频域损失函数以增强泛化能力。

Result: 实验结果表明，在VCTK数据集上，CTFT-Net在性能上超越了现有最先进模型（NU-Wave、WSRGlow、NVSR、AERO），尤其在极端上采样（2 kHz到48 kHz）情况下，能够有效地重建高频信息而不会产生噪声伪影。

Conclusion: CTFT-Net通过在复数域中同时进行幅度和相位重建，结合创新的网络结构和损失函数，显著提升了语音超分辨率性能，特别是在高倍率上采样和高质量高频重建方面表现卓越。

Abstract: Speech super-resolution (SSR) enhances low-resolution speech by increasing
the sampling rate. While most SSR methods focus on magnitude reconstruction,
recent research highlights the importance of phase reconstruction for improved
perceptual quality. Therefore, we introduce CTFT-Net, a Complex Time-Frequency
Transformation Network that reconstructs both magnitude and phase in complex
domains for improved SSR tasks. It incorporates a complex global attention
block to model inter-phoneme and inter-frequency dependencies and a complex
conformer to capture long-range and local features, improving frequency
reconstruction and noise robustness. CTFT-Net employs time-domain and
multi-resolution frequency-domain loss functions for better generalization.
Experiments show CTFT-Net outperforms state-of-the-art models (NU-Wave,
WSRGlow, NVSR, AERO) on the VCTK dataset, particularly for extreme upsampling
(2 kHz to 48 kHz), reconstructing high frequencies effectively without noisy
artifacts.

</details>
