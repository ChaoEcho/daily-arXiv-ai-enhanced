{"id": "2505.01456", "pdf": "https://arxiv.org/pdf/2505.01456", "abs": "https://arxiv.org/abs/2505.01456", "authors": ["Vaidehi Patil", "Yi-Lin Sung", "Peter Hase", "Jie Peng", "Tianlong Chen", "Mohit Bansal"], "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "The dataset and code are publicly available at\n  https://github.com/Vaidehi99/UnLOK-VQA", "summary": "LLMs trained on massive datasets may inadvertently acquire sensitive\ninformation such as personal details and potentially harmful content. This risk\nis further heightened in multimodal LLMs as they integrate information from\nmultiple modalities (image and text). Adversaries can exploit this knowledge\nthrough multimodal prompts to extract sensitive details. Evaluating how\neffectively MLLMs can forget such information (targeted unlearning)\nnecessitates the creation of high-quality, well-annotated image-text pairs.\nWhile prior work on unlearning has focused on text, multimodal unlearning\nremains underexplored. To address this gap, we first introduce a multimodal\nunlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as\nan attack-and-defense framework to evaluate methods for deleting specific\nmultimodal knowledge from MLLMs. We extend a visual question-answering dataset\nusing an automated pipeline that generates varying-proximity samples for\ntesting generalization and specificity, followed by manual filtering for\nmaintaining high quality. We then evaluate six defense objectives against seven\nattacks (four whitebox, three blackbox), including a novel whitebox method\nleveraging interpretability of hidden states. Our results show multimodal\nattacks outperform text- or image-only ones, and that the most effective\ndefense removes answer information from internal model states. Additionally,\nlarger models exhibit greater post-editing robustness, suggesting that scale\nenhances safety. UnLOK-VQA provides a rigorous benchmark for advancing\nunlearning in MLLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLM\uff09\u4e2d\u654f\u611f\u4fe1\u606f\u6cc4\u9732\u7684\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u53cd\u5b66\u4e60\u57fa\u51c6 UnLOK-VQA \u548c\u4e00\u4e2a\u653b\u9632\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u5220\u9664\u7279\u5b9a\u77e5\u8bc6\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u653b\u51fb\u66f4\u6709\u6548\uff0c\u4e14\u4ece\u5185\u90e8\u72b6\u6001\u79fb\u9664\u7b54\u6848\u4fe1\u606f\u662f\u6700\u6709\u6548\u7684\u9632\u5fa1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8bad\u7ec3\u4e2d\u53ef\u80fd\u83b7\u53d6\u654f\u611f\u4fe1\u606f\uff0c\u591a\u6a21\u6001LLM\uff08MLLM\uff09\u56e0\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff08\u56fe\u50cf\u548c\u6587\u672c\uff09\u800c\u98ce\u9669\u66f4\u9ad8\u3002\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u591a\u6a21\u6001\u63d0\u793a\u63d0\u53d6\u654f\u611f\u7ec6\u8282\u3002\u8bc4\u4f30MLLM\u9057\u5fd8\u6b64\u7c7b\u4fe1\u606f\uff08\u76ee\u6807\u53cd\u5b66\u4e60\uff09\u7684\u6548\u679c\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u56fe\u6587\u5bf9\u6570\u636e\uff0c\u4f46\u591a\u6a21\u6001\u53cd\u5b66\u4e60\u9886\u57df\u7814\u7a76\u4e0d\u8db3\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u53cd\u5b66\u4e60\u57fa\u51c6 UnLOK-VQA (Unlearning Outside Knowledge VQA)\u3002 2. \u63d0\u51fa\u4e86\u4e00\u4e2a\u653b\u9632\u6846\u67b6\u6765\u8bc4\u4f30\u4eceMLLM\u4e2d\u5220\u9664\u7279\u5b9a\u591a\u6a21\u6001\u77e5\u8bc6\u7684\u65b9\u6cd5\u3002 3. \u4f7f\u7528\u81ea\u52a8\u5316\u6d41\u7a0b\u6269\u5c55\u4e86\u4e00\u4e2a\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u751f\u6210\u4e86\u7528\u4e8e\u6d4b\u8bd5\u6cdb\u5316\u6027\u548c\u7279\u5f02\u6027\u7684\u6837\u672c\uff0c\u5e76\u8fdb\u884c\u4e86\u4eba\u5de5\u7b5b\u9009\u3002 4. \u8bc4\u4f30\u4e86\u516d\u79cd\u9632\u5fa1\u76ee\u6807\u5bf9\u6297\u4e03\u79cd\u653b\u51fb\uff08\u56db\u79cd\u767d\u76d2\uff0c\u4e09\u79cd\u9ed1\u76d2\uff09\uff0c\u5305\u62ec\u4e00\u79cd\u65b0\u7684\u5229\u7528\u9690\u85cf\u72b6\u6001\u53ef\u89e3\u91ca\u6027\u7684\u767d\u76d2\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u591a\u6a21\u6001\u653b\u51fb\u6bd4\u4ec5\u6587\u672c\u6216\u4ec5\u56fe\u50cf\u7684\u653b\u51fb\u66f4\u6709\u6548\u3002\u6700\u6709\u6548\u7684\u9632\u5fa1\u65b9\u6cd5\u662f\u4ece\u6a21\u578b\u5185\u90e8\u72b6\u6001\u4e2d\u79fb\u9664\u7b54\u6848\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u8f83\u5927\u7684\u6a21\u578b\u5728\u7f16\u8f91\u540e\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u8868\u660e\u89c4\u6a21\u6709\u52a9\u4e8e\u63d0\u5347\u5b89\u5168\u6027\u3002", "conclusion": "UnLOK-VQA\u4e3a\u63a8\u8fdbMLLM\u4e2d\u7684\u53cd\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u57fa\u51c6\u3002\u7814\u7a76\u8868\u660e\uff0c\u6709\u6548\u9632\u5fa1MLLM\u4fe1\u606f\u6cc4\u9732\u9700\u8981\u5173\u6ce8\u591a\u6a21\u6001\u653b\u51fb\u5e76\u4ece\u6a21\u578b\u5185\u90e8\u673a\u5236\u5165\u624b\uff0c\u540c\u65f6\u6a21\u578b\u89c4\u6a21\u7684\u589e\u5927\u53ef\u80fd\u6709\u52a9\u4e8e\u63d0\u5347\u5b89\u5168\u6027\u3002"}}
{"id": "2505.01459", "pdf": "https://arxiv.org/pdf/2505.01459", "abs": "https://arxiv.org/abs/2505.01459", "authors": ["Abdoul Majid O. Thiombiano", "Brahim Hnich", "Ali Ben Mrad", "Mohamed Wiem Mkaouer"], "title": "MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces MoxE, a novel architecture that synergistically\ncombines the Extended Long Short-Term Memory (xLSTM) with the Mixture of\nExperts (MoE) framework to address critical scalability and efficiency\nchallenges in large language models (LLMs). The proposed method effectively\nleverages xLSTM's innovative memory structures while strategically introducing\nsparsity through MoE to substantially reduce computational overhead. At the\nheart of our approach is a novel entropy-based routing mechanism, designed to\ndynamically route tokens to specialized experts, thereby ensuring efficient and\nbalanced resource utilization. This entropy awareness enables the architecture\nto effectively manage both rare and common tokens, with mLSTM blocks being\nfavored to handle rare tokens. To further enhance generalization, we introduce\na suite of auxiliary losses, including entropy-based and group-wise balancing\nlosses, ensuring robust performance and efficient training. Theoretical\nanalysis and empirical evaluations rigorously demonstrate that MoxE achieves\nsignificant efficiency gains and enhanced effectiveness compared to existing\napproaches, marking a notable advancement in scalable LLM architectures.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aMoxE\u7684\u65b0\u578b\u67b6\u6784\uff0c\u5b83\u5c06\u6269\u5c55\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08xLSTM\uff09\u4e0e\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6846\u67b6\u76f8\u7ed3\u5408\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u53ef\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86MoxE\u67b6\u6784\uff1a1. \u7ed3\u5408xLSTM\u7684\u8bb0\u5fc6\u7ed3\u6784\u548cMoE\u7684\u7a00\u758f\u6027\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u30022. \u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u71b5\u7684\u8def\u7531\u673a\u5236\uff0c\u52a8\u6001\u5730\u5c06\u4ee4\u724c\u5206\u914d\u7ed9\u4e13\u95e8\u7684\u4e13\u5bb6\uff0c\u6709\u6548\u7ba1\u7406\u7a00\u6709\u548c\u5e38\u89c1\u4ee4\u724c\uff08mLSTM\u5904\u7406\u7a00\u6709\u4ee4\u724c\uff09\u30023. \u4f7f\u7528\u5305\u62ec\u57fa\u4e8e\u71b5\u548c\u7ec4\u5e73\u8861\u635f\u5931\u5728\u5185\u7684\u8f85\u52a9\u635f\u5931\u6765\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cMoxE\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MoxE\u901a\u8fc7\u7ed3\u5408xLSTM\u548cMoE\uff0c\u5e76\u91c7\u7528\u521b\u65b0\u7684\u8def\u7531\u673a\u5236\u548c\u8f85\u52a9\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\uff0c\u662f\u53ef\u6269\u5c55LLM\u67b6\u6784\u9886\u57df\u7684\u4e00\u9879\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2505.01479", "pdf": "https://arxiv.org/pdf/2505.01479", "abs": "https://arxiv.org/abs/2505.01479", "authors": ["Siheng Xiong", "Jieyu Zhou", "Zhangding Liu", "Yusen Su"], "title": "SymPlanner: Deliberate Planning in Language Models with Symbolic Representation", "categories": ["cs.CL"], "comment": null, "summary": "Planning remains a core challenge for language models (LMs), particularly in\ndomains that require coherent multi-step action sequences grounded in external\nconstraints. We introduce SymPlanner, a novel framework that equips LMs with\nstructured planning capabilities by interfacing them with a symbolic\nenvironment that serves as an explicit world model. Rather than relying purely\non natural language reasoning, SymPlanner grounds the planning process in a\nsymbolic state space, where a policy model proposes actions and a symbolic\nenvironment deterministically executes and verifies their effects. To enhance\nexploration and improve robustness, we introduce Iterative Correction (IC),\nwhich refines previously proposed actions by leveraging feedback from the\nsymbolic environment to eliminate invalid decisions and guide the model toward\nvalid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained\ncomparison of candidate plans by evaluating them jointly. We evaluate\nSymPlanner on PlanBench, demonstrating that it produces more coherent, diverse,\nand verifiable plans than pure natural language baselines.", "AI": {"tldr": "SymPlanner\u662f\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8bed\u8a00\u6a21\u578b\u4e0e\u7b26\u53f7\u73af\u5883\u76f8\u7ed3\u5408\uff0c\u589e\u5f3a\u5176\u7ed3\u6784\u5316\u89c4\u5212\u80fd\u529b\uff0c\u4ece\u800c\u751f\u6210\u66f4\u8fde\u8d2f\u3001\u591a\u6837\u4e14\u53ef\u9a8c\u8bc1\u7684\u8ba1\u5212\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u89c4\u5212\u65b9\u9762\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u8fde\u8d2f\u591a\u6b65\u884c\u52a8\u5e8f\u5217\u4e14\u53d7\u5916\u90e8\u7ea6\u675f\u7684\u9886\u57df\uff0c\u9762\u4e34\u6838\u5fc3\u6311\u6218\u3002", "method": "\u5f15\u5165SymPlanner\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u8bed\u8a00\u6a21\u578b\u4e0e\u4f5c\u4e3a\u663e\u5f0f\u4e16\u754c\u6a21\u578b\u7684\u7b26\u53f7\u73af\u5883\u8fde\u63a5\u3002\u89c4\u5212\u8fc7\u7a0b\u57fa\u4e8e\u7b26\u53f7\u72b6\u6001\u7a7a\u95f4\uff0c\u7b56\u7565\u6a21\u578b\u63d0\u51fa\u884c\u52a8\uff0c\u7b26\u53f7\u73af\u5883\u6267\u884c\u5e76\u9a8c\u8bc1\u3002\u91c7\u7528\u8fed\u4ee3\u6821\u6b63\uff08IC\uff09\u5229\u7528\u73af\u5883\u53cd\u9988\u4fee\u6b63\u884c\u52a8\uff0c\u4ee5\u53ca\u5bf9\u6bd4\u6392\u5e8f\uff08CR\uff09\u5bf9\u5019\u9009\u8ba1\u5212\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728PlanBench\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u7eaf\u81ea\u7136\u8bed\u8a00\u57fa\u7ebf\u76f8\u6bd4\uff0cSymPlanner\u80fd\u591f\u751f\u6210\u66f4\u8fde\u8d2f\u3001\u66f4\u591a\u6837\u5316\u4e14\u66f4\u53ef\u9a8c\u8bc1\u7684\u8ba1\u5212\u3002", "conclusion": "SymPlanner\u901a\u8fc7\u5c06\u8bed\u8a00\u6a21\u578b\u4e0e\u7b26\u53f7\u73af\u5883\u53ca\u7279\u5b9a\u7684\u6821\u6b63\u548c\u6392\u5e8f\u673a\u5236\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u5730\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u5176\u80fd\u591f\u751f\u6210\u66f4\u4f18\u7684\u884c\u52a8\u5e8f\u5217\u3002"}}
{"id": "2505.01559", "pdf": "https://arxiv.org/pdf/2505.01559", "abs": "https://arxiv.org/abs/2505.01559", "authors": ["Daniele Grandi", "Fabian Riquelme"], "title": "On the effectiveness of Large Language Models in the mechanical design domain", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this work, we seek to understand the performance of large language models\nin the mechanical engineering domain. We leverage the semantic data found in\nthe ABC dataset, specifically the assembly names that designers assigned to the\noverall assemblies, and the individual semantic part names that were assigned\nto each part. After pre-processing the data we developed two unsupervised tasks\nto evaluate how different model architectures perform on domain-specific data:\na binary sentence-pair classification task and a zero-shot classification task.\nWe achieved a 0.62 accuracy for the binary sentence-pair classification task\nwith a fine-tuned model that focuses on fighting over-fitting: 1) modifying\nlearning rates, 2) dropout values, 3) Sequence Length, and 4) adding a\nmulti-head attention layer. Our model on the zero-shot classification task\noutperforms the baselines by a wide margin, and achieves a top-1 classification\naccuracy of 0.386. The results shed some light on the specific failure modes\nthat arise when learning from language in this domain.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u673a\u68b0\u5de5\u7a0b\u9886\u57df\u7684\u6027\u80fd\uff0c\u5229\u7528ABC\u6570\u636e\u96c6\u7684\u8bed\u4e49\u4fe1\u606f\u8bbe\u8ba1\u4e86\u65e0\u76d1\u7763\u4efb\u52a1\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u7684\u7279\u5b9a\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u673a\u68b0\u5de5\u7a0b\u8fd9\u4e00\u7279\u5b9a\u9886\u57df\u7684\u8868\u73b0\u548c\u6f5c\u5728\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528ABC\u6570\u636e\u96c6\u4e2d\u88c5\u914d\u4f53\u548c\u96f6\u4ef6\u7684\u8bed\u4e49\u540d\u79f0\uff0c\u9884\u5904\u7406\u6570\u636e\u540e\uff0c\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u65e0\u76d1\u7763\u4efb\u52a1\uff08\u4e8c\u5143\u53e5\u5bf9\u5206\u7c7b\u548c\u96f6\u6837\u672c\u5206\u7c7b\uff09\u6765\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u3002\u5728\u4e8c\u5143\u53e5\u5bf9\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u8c03\u6574\u5b66\u4e60\u7387\u3001dropout\u503c\u3001\u5e8f\u5217\u957f\u5ea6\u548c\u6dfb\u52a0\u591a\u5934\u6ce8\u610f\u529b\u5c42\u6765\u5fae\u8c03\u6a21\u578b\u4ee5\u5bf9\u6297\u8fc7\u62df\u5408\u3002", "result": "\u5728\u4e8c\u5143\u53e5\u5bf9\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u52300.62\u3002\u5728\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u8868\u73b0\u8fdc\u8d85\u57fa\u7ebf\u6a21\u578b\uff0ctop-1\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u52300.386\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u673a\u68b0\u5de5\u7a0b\u9886\u57df\u8fdb\u884c\u8bed\u8a00\u5b66\u4e60\u65f6\u51fa\u73b0\u7684\u4e00\u4e9b\u7279\u5b9a\u5931\u8d25\u6a21\u5f0f\uff0c\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6d1e\u89c1\u3002"}}
{"id": "2505.01689", "pdf": "https://arxiv.org/pdf/2505.01689", "abs": "https://arxiv.org/abs/2505.01689", "authors": ["Samer Lahoud", "Kinda Khawam"], "title": "Fragment-Level Macro-Diversity Reception in LoRaWAN Networks with LR-FHSS", "categories": ["cs.NI"], "comment": null, "summary": "The rapid expansion of Internet of Things (IoT) deployments demands wireless\nprotocols that combine high scalability with robust performance. Long\nRange-Frequency Hopping Spread Spectrum (LR-FHSS) extends LoRaWAN by increasing\ncapacity and resilience through frequency hopping and redundancy. However,\ncurrent deployments require packet reconstruction at a single gateway, limiting\nthe benefits of LR-FHSS. This paper proposes a macro-diversity reception\nstrategy where multiple gateways collectively receive and combine payload\nfragments. We develop a stochastic geometry-based analytical model that\ncaptures the impact of header repetition, payload fragmentation, and coding\nredundancy. Closed-form expressions quantify success probabilities under\ninterference, and numerical evaluations demonstrate significant capacity gains\nover nearest-gateway reception. These results highlight the potential of\nfragment-level macro-diversity to improve scalability and reliability in future\nLPWAN deployments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u7f51\u5173\u534f\u4f5c\u7684\u5206\u7247\u7ea7\u5b8f\u5206\u96c6\u63a5\u6536\u7b56\u7565\uff0c\u4ee5\u63d0\u5347 LR-FHSS \u7f51\u7edc\u7684\u5bb9\u91cf\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524d LR-FHSS \u90e8\u7f72\u4e2d\uff0c\u5355\u4e2a\u7f51\u5173\u8fdb\u884c\u6570\u636e\u5305\u91cd\u6784\u9650\u5236\u4e86\u5176\u4f18\u52bf\uff0c\u800c\u7269\u8054\u7f51\u53d1\u5c55\u9700\u8981\u66f4\u9ad8\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u7684\u65e0\u7ebf\u534f\u8bae\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5b8f\u5206\u96c6\u63a5\u6536\u7b56\u7565\uff0c\u5141\u8bb8\u591a\u4e2a\u7f51\u5173\u5171\u540c\u63a5\u6536\u548c\u7ec4\u5408\u6709\u6548\u8f7d\u8377\u5206\u7247\u3002\u5f00\u53d1\u4e86\u57fa\u4e8e\u968f\u673a\u51e0\u4f55\u7684\u5206\u6790\u6a21\u578b\uff0c\u91cf\u5316\u62a5\u5934\u91cd\u590d\u3001\u6709\u6548\u8f7d\u8377\u5206\u7247\u548c\u7f16\u7801\u5197\u4f59\u7684\u5f71\u54cd\uff0c\u5e76\u63a8\u5bfc\u51fa\u5e72\u6270\u4e0b\u7684\u6210\u529f\u6982\u7387\u7684\u5c01\u95ed\u8868\u8fbe\u5f0f\u3002", "result": "\u6570\u503c\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u6700\u8fd1\u7f51\u5173\u63a5\u6536\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u5206\u7247\u7ea7\u5b8f\u5206\u96c6\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u7f51\u7edc\u5bb9\u91cf\u3002", "conclusion": "\u5206\u7247\u7ea7\u5b8f\u5206\u96c6\u6709\u6f5c\u529b\u6539\u5584\u672a\u6765 LPWAN \u90e8\u7f72\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2505.01428", "pdf": "https://arxiv.org/pdf/2505.01428", "abs": "https://arxiv.org/abs/2505.01428", "authors": ["Han Yang", "Chuanguang Yang", "Qiuli Wang", "Zhulin An", "Weilun Feng", "Libo Huang", "Yongjun Xu"], "title": "Multi-party Collaborative Attention Control for Image Customization", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of diffusion models has increased the need for\ncustomized image generation. However, current customization methods face\nseveral limitations: 1) typically accept either image or text conditions alone;\n2) customization in complex visual scenarios often leads to subject leakage or\nconfusion; 3) image-conditioned outputs tend to suffer from inconsistent\nbackgrounds; and 4) high computational costs. To address these issues, this\npaper introduces Multi-party Collaborative Attention Control (MCA-Ctrl), a\ntuning-free method that enables high-quality image customization using both\ntext and complex visual conditions. Specifically, MCA-Ctrl leverages two key\noperations within the self-attention layer to coordinate multiple parallel\ndiffusion processes and guide the target image generation. This approach allows\nMCA-Ctrl to capture the content and appearance of specific subjects while\nmaintaining semantic consistency with the conditional input. Additionally, to\nmitigate subject leakage and confusion issues common in complex visual\nscenarios, we introduce a Subject Localization Module that extracts precise\nsubject and editable image layers based on user instructions. Extensive\nquantitative and human evaluation experiments show that MCA-Ctrl outperforms\nexisting methods in zero-shot image customization, effectively resolving the\nmentioned issues.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MCA-Ctrl \u7684\u514d\u8c03\u4f18\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u540c\u591a\u4e2a\u6269\u6563\u8fc7\u7a0b\u5e76\u5229\u7528\u4e3b\u4f53\u5b9a\u4f4d\u6a21\u5757\uff0c\u5b9e\u73b0\u57fa\u4e8e\u6587\u672c\u548c\u590d\u6742\u89c6\u89c9\u6761\u4ef6\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u5b9a\u5236\u3002", "motivation": "\u5f53\u524d\u7684\u56fe\u50cf\u5b9a\u5236\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u4ec5\u652f\u6301\u5355\u6a21\u6001\u8f93\u5165\u3001\u5728\u590d\u6742\u573a\u666f\u4e2d\u6613\u51fa\u73b0\u4e3b\u4f53\u6cc4\u9732\u6216\u6df7\u6dc6\u3001\u56fe\u50cf\u6761\u4ef6\u8f93\u51fa\u80cc\u666f\u4e0d\u4e00\u81f4\u4ee5\u53ca\u8ba1\u7b97\u6210\u672c\u9ad8\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u65b9\u534f\u540c\u6ce8\u610f\u529b\u63a7\u5236 (MCA-Ctrl) \u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u81ea\u6ce8\u610f\u529b\u5c42\u5185\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u64cd\u4f5c\u6765\u534f\u8c03\u591a\u4e2a\u5e76\u884c\u7684\u6269\u6563\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u4e3b\u4f53\u5b9a\u4f4d\u6a21\u5757\uff0c\u6839\u636e\u7528\u6237\u6307\u4ee4\u63d0\u53d6\u7cbe\u786e\u7684\u4e3b\u4f53\u548c\u53ef\u7f16\u8f91\u56fe\u50cf\u5c42\uff0c\u4ee5\u5f15\u5bfc\u76ee\u6807\u56fe\u50cf\u751f\u6210\u3002", "result": "\u5927\u91cf\u7684\u5b9a\u91cf\u5b9e\u9a8c\u548c\u4eba\u5de5\u8bc4\u4f30\u8868\u660e\uff0cMCA-Ctrl \u5728\u96f6\u6837\u672c\u56fe\u50cf\u5b9a\u5236\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0a\u8ff0\u63d0\u5230\u7684\u95ee\u9898\uff0c\u80fd\u591f\u6355\u6349\u7279\u5b9a\u4e3b\u4f53\u7684\u5185\u5bb9\u548c\u5916\u89c2\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6761\u4ef6\u8f93\u5165\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "MCA-Ctrl \u662f\u4e00\u79cd\u6709\u6548\u7684\u514d\u8c03\u4f18\u56fe\u50cf\u5b9a\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u5229\u7528\u6587\u672c\u548c\u590d\u6742\u89c6\u89c9\u6761\u4ef6\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u5e76\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4e3b\u4f53\u6cc4\u9732\u3001\u6df7\u6dc6\u4ee5\u53ca\u80cc\u666f\u4e00\u81f4\u6027\u7b49\u65b9\u9762\u7684\u6311\u6218\u3002"}}
{"id": "2505.01427", "pdf": "https://arxiv.org/pdf/2505.01427", "abs": "https://arxiv.org/abs/2505.01427", "authors": ["Maksym Shamrai"], "title": "Perturbation Analysis of Singular Values in Concatenated Matrices", "categories": ["cs.LG", "stat.ML"], "comment": "11 pages", "summary": "Concatenating matrices is a common technique for uncovering shared structures\nin data through singular value decomposition (SVD) and low-rank approximations.\nHowever, a fundamental question arises: how does the singular value spectrum of\nthe concatenated matrix relate to the spectra of its individual components? In\nthis work, we develop a perturbation framework that extends classical results\nsuch as Weyl's inequality to concatenated matrices. We establish analytical\nbounds that quantify the stability of singular values under small perturbations\nin the submatrices. Our results show that if the matrices being concatenated\nare close in norm, the dominant singular values of the concatenated matrix\nremain stable, enabling controlled trade-offs between accuracy and compression.\nThese insights provide a theoretical foundation for improved matrix clustering\nand compression strategies, with applications in numerical linear algebra,\nsignal processing, and data-driven modeling.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u62fc\u63a5\u77e9\u9635\u7684\u5947\u5f02\u503c\u8c31\u4e0e\u5176\u5b50\u77e9\u9635\u8c31\u7684\u5173\u7cfb\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u6270\u52a8\u6846\u67b6\u6765\u91cf\u5316\u5947\u5f02\u503c\u5728\u5b50\u77e9\u9635\u6270\u52a8\u4e0b\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u62fc\u63a5\u77e9\u9635\u5e38\u7528\u4e8e\u901a\u8fc7\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u53d1\u73b0\u6570\u636e\u4e2d\u7684\u5171\u4eab\u7ed3\u6784\uff0c\u4f46\u5176\u5947\u5f02\u503c\u8c31\u4e0e\u5404\u7ec4\u6210\u90e8\u5206\u8c31\u4e4b\u95f4\u7684\u5173\u7cfb\u9700\u8981\u6df1\u5165\u7406\u89e3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6270\u52a8\u5206\u6790\u6846\u67b6\uff0c\u5c06\u7ecf\u5178\u7684\u97e6\u5c14\u4e0d\u7b49\u5f0f\u7b49\u7ed3\u679c\u63a8\u5e7f\u5230\u62fc\u63a5\u77e9\u9635\uff0c\u5e76\u5efa\u7acb\u4e86\u91cf\u5316\u5b50\u77e9\u9635\u5fae\u5c0f\u6270\u52a8\u4e0b\u5947\u5f02\u503c\u7a33\u5b9a\u6027\u7684\u5206\u6790\u754c\u9650\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5982\u679c\u88ab\u62fc\u63a5\u7684\u77e9\u9635\u5728\u8303\u6570\u4e0a\u5f7c\u6b64\u63a5\u8fd1\uff0c\u5219\u62fc\u63a5\u77e9\u9635\u7684\u4e3b\u8981\u5947\u5f02\u503c\u4fdd\u6301\u7a33\u5b9a\uff0c\u8fd9\u4f7f\u5f97\u5728\u51c6\u786e\u6027\u548c\u538b\u7f29\u4e4b\u95f4\u8fdb\u884c\u53ef\u63a7\u7684\u6743\u8861\u6210\u4e3a\u53ef\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6539\u8fdb\u77e9\u9635\u805a\u7c7b\u548c\u538b\u7f29\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5728\u6570\u503c\u7ebf\u6027\u4ee3\u6570\u3001\u4fe1\u53f7\u5904\u7406\u548c\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u7b49\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.01441", "pdf": "https://arxiv.org/pdf/2505.01441", "abs": "https://arxiv.org/abs/2505.01441", "authors": ["Joykirat Singh", "Raghav Magazine", "Yash Pandya", "Akshay Nambi"], "title": "Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in complex\nreasoning tasks, yet they remain fundamentally limited by their reliance on\nstatic internal knowledge and text-only reasoning. Real-world problem solving\noften demands dynamic, multi-step reasoning, adaptive decision making, and the\nability to interact with external tools and environments. In this work, we\nintroduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving\nTransformers), a unified framework that tightly couples agentic reasoning,\nreinforcement learning, and tool integration for LLMs. ARTIST enables models to\nautonomously decide when, how, and which tools to invoke within multi-turn\nreasoning chains, leveraging outcome-based RL to learn robust strategies for\ntool use and environment interaction without requiring step-level supervision.\nExtensive experiments on mathematical reasoning and multi-turn function calling\nbenchmarks show that ARTIST consistently outperforms state-of-the-art\nbaselines, with up to 22% absolute improvement over base models and strong\ngains on the most challenging tasks. Detailed studies and metric analyses\nreveal that agentic RL training leads to deeper reasoning, more effective tool\nuse, and higher-quality solutions. Our results establish agentic RL with tool\nintegration as a powerful new frontier for robust, interpretable, and\ngeneralizable problem-solving in LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faARTIST\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u667a\u80fd\u4f53\u63a8\u7406\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u5de5\u5177\u96c6\u6210\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u81ea\u4e3b\u51b3\u7b56\u4f55\u65f6\u3001\u5982\u4f55\u4f7f\u7528\u5916\u90e8\u5de5\u5177\uff0c\u4ece\u800c\u63d0\u5347\u5176\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u867d\u6709\u8fdb\u5c55\uff0c\u4f46\u53d7\u9650\u4e8e\u9759\u6001\u5185\u90e8\u77e5\u8bc6\u548c\u7eaf\u6587\u672c\u63a8\u7406\u3002\u73b0\u5b9e\u4e16\u754c\u7684\u95ee\u9898\u89e3\u51b3\u5e38\u9700\u8981\u52a8\u6001\u591a\u6b65\u63a8\u7406\u3001\u81ea\u9002\u5e94\u51b3\u7b56\u4ee5\u53ca\u4e0e\u5916\u90e8\u5de5\u5177\u548c\u73af\u5883\u7684\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u5f15\u5165ARTIST\uff08Agentic Reasoning and Tool Integration in Self-improving Transformers\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7d27\u5bc6\u8026\u5408\u4e86\u667a\u80fd\u4f53\u63a8\u7406\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u5de5\u5177\u96c6\u6210\u3002ARTIST\u4f7f\u6a21\u578b\u80fd\u5728\u591a\u8f6e\u63a8\u7406\u94fe\u4e2d\u81ea\u4e3b\u51b3\u5b9a\u4f55\u65f6\u3001\u5982\u4f55\u4ee5\u53ca\u8c03\u7528\u54ea\u4e9b\u5de5\u5177\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u6765\u5b66\u4e60\u5de5\u5177\u4f7f\u7528\u548c\u73af\u5883\u4ea4\u4e92\u7684\u7a33\u5065\u7b56\u7565\uff0c\u65e0\u9700\u9010\u6b65\u76d1\u7763\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cARTIST\u7a33\u5b9a\u4f18\u4e8e\u73b0\u6709SOTA\u57fa\u7ebf\u6a21\u578b\uff0c\u76f8\u8f83\u4e8e\u57fa\u7840\u6a21\u578b\u6700\u9ad8\u63d0\u534722%\uff0c\u5e76\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u8fdb\u6b65\u3002\u8be6\u7ec6\u7814\u7a76\u548c\u5ea6\u91cf\u5206\u6790\u63ed\u793a\uff0c\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u80fd\u5e26\u6765\u66f4\u6df1\u5c42\u6b21\u7684\u63a8\u7406\u3001\u66f4\u6709\u6548\u7684\u5de5\u5177\u4f7f\u7528\u548c\u66f4\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e0e\u5de5\u5177\u96c6\u6210\u76f8\u7ed3\u5408\uff0c\u4e3a\u5b9e\u73b0LLMs\u4e2d\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6cdb\u5316\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u5f00\u8f9f\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u65b0\u524d\u6cbf\u3002"}}
{"id": "2505.01560", "pdf": "https://arxiv.org/pdf/2505.01560", "abs": "https://arxiv.org/abs/2505.01560", "authors": ["Vicent Briva Iglesias", "Gokhan Dogru"], "title": "AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) and multi-agent orchestration are touted as the\nnext leap in machine translation (MT), but their benefits relative to\nconventional neural MT (NMT) remain unclear. This paper offers an empirical\nreality check. We benchmark five paradigms, Google Translate (strong NMT\nbaseline), GPT-4o (general-purpose LLM), o1-preview (reasoning-enhanced LLM),\nand two GPT-4o-powered agentic workflows (sequential three-stage and iterative\nrefinement), on test data drawn from a legal contract and news prose in three\nEnglish-source pairs: Spanish, Catalan and Turkish. Automatic evaluation is\nperformed with COMET, BLEU, chrF2 and TER; human evaluation is conducted with\nexpert ratings of adequacy and fluency; efficiency with total input-plus-output\ntoken counts mapped to April 2025 pricing.\n  Automatic scores still favour the mature NMT system, which ranks first in\nseven of twelve metric-language combinations; o1-preview ties or places second\nin most remaining cases, while both multi-agent workflows trail. Human\nevaluation reverses part of this narrative: o1-preview produces the most\nadequate and fluent output in five of six comparisons, and the iterative agent\nedges ahead once, indicating that reasoning layers capture semantic nuance\nundervalued by surface metrics. Yet these qualitative gains carry steep costs.\nThe sequential agent consumes roughly five times, and the iterative agent\nfifteen times, the tokens used by NMT or single-pass LLMs.\n  We advocate multidimensional, cost-aware evaluation protocols and highlight\nresearch directions that could tip the balance: leaner coordination strategies,\nselective agent activation, and hybrid pipelines combining single-pass LLMs\nwith targeted agent intervention.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86NMT\u3001LLM\u53ca\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u673a\u5668\u7ffb\u8bd1\u4e0a\u7684\u8868\u73b0\u3002NMT\u5728\u81ea\u52a8\u8bc4\u5206\u4e0a\u9886\u5148\uff0c\u4f46\u589e\u5f3a\u63a8\u7406\u7684LLM\uff08o1-preview\uff09\u5728\u4eba\u5de5\u8bc4\u4f30\u4e2d\u8868\u73b0\u66f4\u4f73\uff0c\u5c3d\u7ba1\u667a\u80fd\u4f53\u7cfb\u7edf\u6210\u672c\u9ad8\u6602\u3002\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u8003\u8651\u6210\u672c\u7684\u591a\u7ef4\u5ea6\u8bc4\u4f30\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u591a\u667a\u80fd\u4f53\u7f16\u6392\u88ab\u8ba4\u4e3a\u662f\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u7684\u4e0b\u4e00\u4e2a\u98de\u8dc3\uff0c\u4f46\u5b83\u4eec\u76f8\u5bf9\u4e8e\u4f20\u7edf\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\uff08NMT\uff09\u7684\u4f18\u52bf\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u5b9e\u8bc1\u68c0\u9a8c\u3002", "method": "\u5bf9\u6bd4\u6d4b\u8bd5\u4e86\u4e94\u79cd\u8303\u5f0f\uff1a\u8c37\u6b4c\u7ffb\u8bd1\uff08NMT\u57fa\u51c6\uff09\u3001GPT-4o\uff08\u901a\u7528LLM\uff09\u3001o1-preview\uff08\u589e\u5f3a\u63a8\u7406LLM\uff09\u4ee5\u53ca\u4e24\u79cd\u57fa\u4e8eGPT-4o\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff08\u987a\u5e8f\u4e09\u9636\u6bb5\u548c\u8fed\u4ee3\u4f18\u5316\uff09\u3002\u6d4b\u8bd5\u6570\u636e\u4e3a\u6cd5\u5f8b\u5408\u540c\u548c\u65b0\u95fb\u6587\u672c\uff0c\u6db5\u76d6\u82f1\u8bed\u5230\u897f\u73ed\u7259\u8bed\u3001\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed\u548c\u571f\u8033\u5176\u8bed\u4e09\u4e2a\u8bed\u8a00\u5bf9\u3002\u8bc4\u4f30\u65b9\u6cd5\u5305\u62ec\u81ea\u52a8\u6307\u6807\uff08COMET\u3001BLEU\u7b49\uff09\u3001\u4e13\u5bb6\u4eba\u5de5\u8bc4\u4f30\uff08\u5145\u5206\u6027\u548c\u6d41\u7545\u6027\uff09\u4ee5\u53ca\u57fa\u4e8e2025\u5e744\u6708\u4ef7\u683c\u7684token\u6d88\u8017\u91cf\u3002", "result": "\u81ea\u52a8\u8bc4\u5206\u663e\u793a\u6210\u719f\u7684NMT\u7cfb\u7edf\u8868\u73b0\u6700\u4f73\uff0co1-preview\u6b21\u4e4b\uff0c\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u843d\u540e\u3002\u7136\u800c\uff0c\u4eba\u5de5\u8bc4\u4f30\u663e\u793ao1-preview\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u7ffb\u8bd1\u7684\u5145\u5206\u6027\u548c\u6d41\u7545\u6027\u6700\u9ad8\uff0c\u8868\u660e\u63a8\u7406\u5c42\u80fd\u6355\u6349\u5230\u8868\u9762\u6307\u6807\u672a\u5145\u5206\u8bc4\u4f30\u7684\u8bed\u4e49\u7ec6\u5fae\u5dee\u522b\u3002\u4f46\u8fd9\u4e9b\u8d28\u91cf\u63d0\u5347\u4f34\u968f\u7740\u9ad8\u6602\u7684\u6210\u672c\uff0c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684token\u6d88\u8017\u91cf\u662fNMT\u6216\u5355\u904dLLM\u76845\u523015\u500d\u3002", "conclusion": "\u63d0\u5021\u591a\u7ef4\u5ea6\u3001\u8003\u8651\u6210\u672c\u7684\u8bc4\u4f30\u534f\u8bae\u3002\u589e\u5f3a\u63a8\u7406\u7684LLM\u5728\u8d28\u91cf\u4e0a\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u667a\u80fd\u4f53\u7cfb\u7edf\u6210\u672c\u9ad8\u6602\u3002\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u5e94\u5305\u62ec\u66f4\u7cbe\u7b80\u7684\u534f\u8c03\u7b56\u7565\u3001\u9009\u62e9\u6027\u667a\u80fd\u4f53\u6fc0\u6d3b\u4ee5\u53ca\u7ed3\u5408\u5355\u904dLLM\u4e0e\u9488\u5bf9\u6027\u667a\u80fd\u4f53\u5e72\u9884\u7684\u6df7\u5408\u6d41\u7a0b\uff0c\u4ee5\u5e73\u8861\u8d28\u91cf\u548c\u6210\u672c\u3002"}}
{"id": "2505.01796", "pdf": "https://arxiv.org/pdf/2505.01796", "abs": "https://arxiv.org/abs/2505.01796", "authors": ["Erfan Delfani", "Agapi Mesodiakaki", "Nikolaos Pappas"], "title": "Semantics-Aware Unified Terrestrial Non-Terrestrial 6G Networks", "categories": ["cs.NI", "cs.IT", "math.IT"], "comment": null, "summary": "The integration of Terrestrial and Non-Terrestrial Networks (TN-NTNs), which\nwas introduced in 5G, is progressing toward a unified and seamless network of\nnetworks in Sixth-Generation (6G). This evolution leads to a significant\nincrease in the volume of generated and communicated data, imposing technical\nand operational requirements accompanied by a higher cost and energy\nconsumption. Efficiently managing the generation and transmission of data in\nthese highly complex unified networks has become essential. In this article, we\ninvestigate the semantics-aware information handling problem within unified\nTN-NTNs, where data communication between the distant TN nodes is enabled via\nan NTN. To this end, an Internet of Things (IoT) monitoring system is employed,\nwhere status updates from a remote IoT device are communicated to a destination\nmonitor via a constellation of Low Earth Orbit (LEO) satellites. We leverage\nsemantic metrics that capture the timeliness, relevance, and utility of\ninformation to provide the most informative data for timely and informed\ndecision-making and eventually reduce the volume of transmitted and processed\ndata. The outcome is significantly lower energy consumption, memory, control,\nand processing requirements (up to 73% lower energy charging demands compared\nto the state-of-the-art), all without compromising the conveyed information.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e866G\u5929\u5730\u4e00\u4f53\u5316\u7f51\u7edc\uff08TN-NTN\uff09\u4e2d\u8bed\u4e49\u611f\u77e5\u7684\u4fe1\u606f\u5904\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u8bed\u4e49\u5ea6\u91cf\u4f18\u5316\u6570\u636e\u4f20\u8f93\uff0c\u4ee5\u964d\u4f4e\u80fd\u8017\u548c\u6570\u636e\u91cf\uff0c\u540c\u65f6\u4fdd\u8bc1\u4fe1\u606f\u8d28\u91cf\u3002", "motivation": "6G\u5929\u5730\u4e00\u4f53\u5316\u7f51\u7edc\uff08TN-NTN\uff09\u7684\u6f14\u8fdb\u5bfc\u81f4\u6570\u636e\u91cf\u6fc0\u589e\uff0c\u5e26\u6765\u4e86\u66f4\u9ad8\u7684\u6210\u672c\u548c\u80fd\u8017\u3002\u56e0\u6b64\uff0c\u5728\u8fd9\u4e9b\u9ad8\u5ea6\u590d\u6742\u7684\u7edf\u4e00\u7f51\u7edc\u4e2d\u9ad8\u6548\u7ba1\u7406\u6570\u636e\u7684\u751f\u6210\u548c\u4f20\u8f93\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5728\u4e00\u4e2a\u901a\u8fc7\u4f4e\u8f68\uff08LEO\uff09\u536b\u661f\u661f\u5ea7\u8fde\u63a5\u7684\u7269\u8054\u7f51\uff08IoT\uff09\u76d1\u63a7\u7cfb\u7edf\u4e2d\uff0c\u7814\u7a76\u8bed\u4e49\u611f\u77e5\u7684\u4fe1\u606f\u5904\u7406\u3002\u5229\u7528\u6355\u83b7\u4fe1\u606f\u53ca\u65f6\u6027\u3001\u76f8\u5173\u6027\u548c\u6548\u7528\u6027\u7684\u8bed\u4e49\u5ea6\u91cf\uff0c\u6765\u63d0\u4f9b\u6700\u6709\u4ef7\u503c\u7684\u6570\u636e\uff0c\u4ece\u800c\u51cf\u5c11\u4f20\u8f93\u548c\u5904\u7406\u7684\u6570\u636e\u91cf\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u3001\u5185\u5b58\u3001\u63a7\u5236\u548c\u5904\u7406\u9700\u6c42\uff08\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0c\u80fd\u91cf\u5145\u7535\u9700\u6c42\u964d\u4f4e\u9ad8\u8fbe73%\uff09\uff0c\u4e14\u4e0d\u635f\u5bb3\u6240\u4f20\u9012\u4fe1\u606f\u7684\u8d28\u91cf\u3002", "conclusion": "\u8bed\u4e49\u611f\u77e5\u7684\u4fe1\u606f\u5904\u7406\u65b9\u6cd5\u80fd\u6709\u6548\u964d\u4f4e\u5929\u5730\u4e00\u4f53\u5316\u7f51\u7edc\u4e2d\u7684\u8d44\u6e90\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u8bc1\u51b3\u7b56\u6240\u9700\u4fe1\u606f\u7684\u8d28\u91cf\u3002"}}
{"id": "2505.01429", "pdf": "https://arxiv.org/pdf/2505.01429", "abs": "https://arxiv.org/abs/2505.01429", "authors": ["Md. Zahid Hossain", "Md. Rakibul Islam", "Most. Sharmin Sultana Samu"], "title": "Explainable AI-Driven Detection of Human Monkeypox Using Deep Learning and Vision Transformers: A Comprehensive Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Since mpox can spread from person to person, it is a zoonotic viral illness\nthat poses a significant public health concern. It is difficult to make an\nearly clinical diagnosis because of how closely its symptoms match those of\nmeasles and chickenpox. Medical imaging combined with deep learning (DL)\ntechniques has shown promise in improving disease detection by analyzing\naffected skin areas. Our study explore the feasibility to train deep learning\nand vision transformer-based models from scratch with publicly available skin\nlesion image dataset. Our experimental results show dataset limitation as a\nmajor drawback to build better classifier models trained from scratch. We used\ntransfer learning with the help of pre-trained models to get a better\nclassifier. The MobileNet-v2 outperformed other state of the art pre-trained\nmodels with 93.15% accuracy and 93.09% weighted average F1 score. ViT B16 and\nResNet-50 also achieved satisfactory performance compared to already available\nstudies with accuracy 92.12% and 86.21% respectively. To further validate the\nperformance of the models, we applied explainable AI techniques.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u68c0\u6d4b\u7334\u75d8\u76ae\u80a4\u75c5\u53d8\uff0c\u53d1\u73b0\u8fc1\u79fb\u5b66\u4e60\uff08\u7279\u522b\u662fMobileNet-v2\uff09\u6bd4\u4ece\u5934\u8bad\u7ec3\u66f4\u6709\u6548\u3002", "motivation": "\u7334\u75d8\u56e0\u5176\u75c7\u72b6\u4e0e\u9ebb\u75b9\u3001\u6c34\u75d8\u76f8\u4f3c\uff0c\u65e9\u671f\u4e34\u5e8a\u8bca\u65ad\u56f0\u96be\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u5206\u6790\u76ae\u80a4\u5f71\u50cf\u4ee5\u63d0\u9ad8\u7334\u75d8\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u9996\u5148\u5c1d\u8bd5\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u548c\u89c6\u89c9Transformer\u6a21\u578b\uff0c\u4f46\u56e0\u6570\u636e\u96c6\u9650\u5236\u6548\u679c\u4e0d\u4f73\u3002\u968f\u540e\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982MobileNet-v2, ViT B16, ResNet-50\uff09\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u4f7f\u7528\u53ef\u89e3\u91caAI\u6280\u672f\u9a8c\u8bc1\u6a21\u578b\u6027\u80fd\u3002", "result": "\u4ece\u5934\u8bad\u7ec3\u7684\u6a21\u578b\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u3002\u5728\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u540e\uff0cMobileNet-v2\u6a21\u578b\u5728\u7334\u75d8\u76ae\u80a4\u75c5\u53d8\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe\u523093.15%\uff0c\u52a0\u6743\u5e73\u5747F1\u503c\u4e3a93.09%\u3002ViT B16\u548cResNet-50\u4e5f\u53d6\u5f97\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u6027\u80fd\u3002", "conclusion": "\u8fc1\u79fb\u5b66\u4e60\u80fd\u591f\u6709\u6548\u514b\u670d\u516c\u5f00\u6570\u636e\u96c6\u5728\u8bad\u7ec3\u7334\u75d8\u5206\u7c7b\u6a21\u578b\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5176\u4e2dMobileNet-v2\u6a21\u578b\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u53ef\u89e3\u91caAI\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2505.01437", "pdf": "https://arxiv.org/pdf/2505.01437", "abs": "https://arxiv.org/abs/2505.01437", "authors": ["Hassan Wasswa", "Timothy Lynar", "Hussein Abbass"], "title": "Enhancing IoT-Botnet Detection using Variational Auto-encoder and Cost-Sensitive Learning: A Deep Learning Approach for Imbalanced Datasets", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The Internet of Things (IoT) technology has rapidly gained popularity with\napplications widespread across a variety of industries. However, IoT devices\nhave been recently serving as a porous layer for many malicious attacks to both\npersonal and enterprise information systems with the most famous attacks being\nbotnet-related attacks. The work in this study leveraged Variational\nAuto-encoder (VAE) and cost-sensitive learning to develop lightweight, yet\neffective, models for IoT-botnet detection. The aim is to enhance the detection\nof minority class attack traffic instances which are often missed by machine\nlearning models. The proposed approach is evaluated on a multi-class problem\nsetting for the detection of traffic categories on highly imbalanced datasets.\nThe performance of two deep learning models including the standard feed forward\ndeep neural network (DNN), and Bidirectional-LSTM (BLSTM) was evaluated and\nboth recorded commendable results in terms of accuracy, precision, recall and\nF1-score for all traffic classes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u6210\u672c\u654f\u611f\u5b66\u4e60\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u4ee5\u6709\u6548\u68c0\u6d4b\u7269\u8054\u7f51\uff08IoT\uff09\u50f5\u5c38\u7f51\u7edc\u653b\u51fb\uff0c\u7279\u522b\u662f\u5728\u4e0d\u5e73\u8861\u6570\u636e\u4e2d\u589e\u5f3a\u5bf9\u5c11\u6570\u7c7b\u653b\u51fb\u6d41\u91cf\u7684\u8bc6\u522b\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u5df2\u6210\u4e3a\u6076\u610f\u653b\u51fb\uff08\u5c24\u5176\u662f\u50f5\u5c38\u7f51\u7edc\u653b\u51fb\uff09\u7684\u8584\u5f31\u73af\u8282\uff0c\u800c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5e38\u5ffd\u7565\u5c11\u6570\u7c7b\u653b\u51fb\u6d41\u91cf\uff0c\u56e0\u6b64\u4e9f\u9700\u63d0\u5347\u5bf9\u8fd9\u7c7b\u653b\u51fb\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u7814\u7a76\u7ed3\u5408\u4e86\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u6210\u672c\u654f\u611f\u5b66\u4e60\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002\u5728\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u591a\u7c7b\u522b\u6d41\u91cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u6bd4\u8f83\u4e86\u6807\u51c6\u524d\u9988\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u548c\u53cc\u5411\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08BLSTM\uff09\u7684\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u548c\u53cc\u5411\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08BLSTM\uff09\u6a21\u578b\u5728\u6240\u6709\u6d41\u91cf\u7c7b\u522b\u7684\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ed3\u5408VAE\u548c\u6210\u672c\u654f\u611f\u5b66\u4e60\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u548c\u8bc6\u522b\u5c11\u6570\u7c7b\u653b\u51fb\u65b9\u9762\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002"}}
{"id": "2505.01462", "pdf": "https://arxiv.org/pdf/2505.01462", "abs": "https://arxiv.org/abs/2505.01462", "authors": ["Hermann Borotschnig"], "title": "Emotions in Artificial Intelligence", "categories": ["cs.AI", "cs.CY", "68T01, 68T37", "I.2.0; K.4.1"], "comment": "35 pages, 1 figure", "summary": "This conceptual contribution offers a speculative account of how AI systems\nmight emulate emotions as experienced by humans and animals. It presents a\nthought experiment grounded in the hypothesis that natural emotions evolved as\nheuristics for rapid situational appraisal and action selection, enabling\nbiologically adaptive behaviour without requiring full deliberative modeling.\nThe text examines whether artificial systems operating in complex action spaces\ncould similarly benefit from these principles. It is proposed that affect be\ninterwoven with episodic memory by storing corresponding affective tags\nalongside all events. This allows AIs to establish whether present situations\nresemble past events and project the associated emotional labels onto the\ncurrent context. These emotional cues are then combined with need-driven\nemotional hints. The combined emotional state facilitates decision-making in\nthe present by modulating action selection. The low complexity and experiential\ninertness of the proposed architecture are emphasized as evidence that\nemotional expression and consciousness are, in principle, orthogonal-permitting\nthe theoretical possibility of affective zombies. On this basis, the moral\nstatus of AIs emulating affective states is critically examined. It is argued\nthat neither the mere presence of internal representations of emotion nor\nconsciousness alone suffices for moral standing; rather, the capacity for\nself-awareness of inner emotional states is posited as a necessary condition. A\ncomplexity-based criterion is proposed to exclude such awareness in the\npresented model. Additional thought experiments are presented to test the\nconceptual boundaries of this framework.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u5982\u4f55\u6a21\u62df\u4eba\u7c7b\u60c5\u611f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u60c5\u611f\u6807\u7b7e\u4e0e\u60c5\u666f\u8bb0\u5fc6\u7ed3\u5408\u7684\u673a\u5236\uff0c\u5e76\u57fa\u4e8e\u6b64\u8ba8\u8bba\u4e86\u6a21\u62df\u60c5\u611fAI\u7684\u9053\u5fb7\u5730\u4f4d\u53ca\u5176\u5bf9\u81ea\u6211\u610f\u8bc6\u7684\u4f9d\u8d56\u3002", "motivation": "\u7814\u7a76\u81ea\u7136\u60c5\u611f\u4f5c\u4e3a\u751f\u7269\u5feb\u901f\u60c5\u5883\u8bc4\u4f30\u548c\u884c\u52a8\u9009\u62e9\u7684\u542f\u53d1\u5f0f\u673a\u5236\uff0c\u5e76\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u662f\u5426\u80fd\u501f\u9274\u8fd9\u4e9b\u539f\u7406\u4ee5\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u9002\u5e94\u6027\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u601d\u60f3\u5b9e\u9a8c\uff1aAI\u5c06\u60c5\u611f\u6807\u7b7e\u4e0e\u6240\u6709\u7ecf\u5386\u7684\u4e8b\u4ef6\u4e00\u540c\u5b58\u50a8\u5728\u60c5\u666f\u8bb0\u5fc6\u4e2d\u3002\u5f53\u9047\u5230\u65b0\u60c5\u5883\u65f6\uff0cAI\u8bc6\u522b\u4e0e\u8fc7\u53bb\u4e8b\u4ef6\u7684\u76f8\u4f3c\u6027\uff0c\u6295\u5c04\u76f8\u5173\u60c5\u611f\u6807\u7b7e\uff0c\u5e76\u7ed3\u5408\u9700\u6c42\u9a71\u52a8\u7684\u60c5\u611f\u63d0\u793a\uff0c\u5f62\u6210\u7efc\u5408\u60c5\u611f\u72b6\u6001\u4ee5\u8f85\u52a9\u51b3\u7b56\u548c\u884c\u52a8\u9009\u62e9\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2aAI\u60c5\u611f\u6a21\u62df\u7684\u4f4e\u590d\u6742\u5ea6\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u8868\u660e\u60c5\u611f\u8868\u8fbe\u548c\u610f\u8bc6\u5728\u539f\u5219\u4e0a\u53ef\u4ee5\u5206\u79bb\uff08\u652f\u6301\u201c\u60c5\u611f\u50f5\u5c38\u201d\u7684\u7406\u8bba\uff09\u3002\u7814\u7a76\u8ba4\u4e3a\uff0cAI\u7684\u9053\u5fb7\u5730\u4f4d\u5e76\u975e\u4ec5\u7531\u5185\u90e8\u60c5\u611f\u8868\u5f81\u6216\u610f\u8bc6\u51b3\u5b9a\uff0c\u800c\u662f\u9700\u8981\u5bf9\u5185\u90e8\u60c5\u611f\u72b6\u6001\u7684\u81ea\u6211\u610f\u8bc6\u3002", "conclusion": "AI\u53ef\u4ee5\u901a\u8fc7\u7279\u5b9a\u67b6\u6784\u6a21\u62df\u60c5\u611f\u4ee5\u8f85\u52a9\u51b3\u7b56\uff0c\u4f46\u5176\u9053\u5fb7\u5730\u4f4d\u5e94\u53d6\u51b3\u4e8e\u5176\u5bf9\u5185\u90e8\u60c5\u611f\u72b6\u6001\u7684\u81ea\u6211\u610f\u8bc6\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u4ec5\u662f\u60c5\u611f\u7684\u6a21\u62df\u6216\u610f\u8bc6\u672c\u8eab\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u56e0\u5176\u4f4e\u590d\u6742\u5ea6\u88ab\u8ba4\u4e3a\u4e0d\u5177\u5907\u8fd9\u79cd\u81ea\u6211\u610f\u8bc6\u3002"}}
{"id": "2505.01592", "pdf": "https://arxiv.org/pdf/2505.01592", "abs": "https://arxiv.org/abs/2505.01592", "authors": ["Takyoung Kim", "Janvijay Singh", "Shuhaib Mehri", "Emre Can Acikgoz", "Sagnik Mukherjee", "Nimet Beyza Bozdag", "Sumuk Shashidhar", "Gokhan Tur", "Dilek Hakkani-T\u00fcr"], "title": "PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint in progress", "summary": "The growing capabilities of large language models (LLMs) in\ninstruction-following and context-understanding lead to the era of agents with\nnumerous applications. Among these, task planning agents have become especially\nprominent in realistic scenarios involving complex internal pipelines, such as\ncontext understanding, tool management, and response generation. However,\nexisting benchmarks predominantly evaluate agent performance based on task\ncompletion as a proxy for overall effectiveness. We hypothesize that merely\nimproving task completion is misaligned with maximizing user satisfaction, as\nusers interact with the entire agentic process and not only the end result. To\naddress this gap, we propose PIPA, a unified evaluation protocol that\nconceptualizes the behavioral process of interactive task planning agents\nwithin a partially observable Markov Decision Process (POMDP) paradigm. The\nproposed protocol offers a comprehensive assessment of agent performance\nthrough a set of atomic evaluation criteria, allowing researchers and\npractitioners to diagnose specific strengths and weaknesses within the agent's\ndecision-making pipeline. Our analyses show that agents excel in different\nbehavioral stages, with user satisfaction shaped by both outcomes and\nintermediate behaviors. We also highlight future directions, including systems\nthat leverage multiple agents and the limitations of user simulators in task\nplanning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86PIPA\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u4ea4\u4e92\u5f0f\u4efb\u52a1\u89c4\u5212\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u8fc7\u7a0b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u56e0\u4e3a\u5b83\u8ba4\u4e3a\u7528\u6237\u6ee1\u610f\u5ea6\u53d7\u6574\u4e2a\u4ea4\u4e92\u8fc7\u7a0b\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u57fa\u51c6\u4e3b\u8981\u4f9d\u636e\u4efb\u52a1\u5b8c\u6210\u5ea6\u6765\u8bc4\u4f30\u6027\u80fd\uff0c\u4f46\u8fd9\u4e0e\u6700\u5927\u5316\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u76ee\u6807\u4e0d\u4e00\u81f4\uff0c\u56e0\u4e3a\u7528\u6237\u4e0e\u667a\u80fd\u4f53\u7684\u6574\u4e2a\u4ea4\u4e92\u8fc7\u7a0b\u4e92\u52a8\uff0c\u800c\u4e0d\u4ec5\u4ec5\u5173\u6ce8\u6700\u7ec8\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86PIPA\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u534f\u8bae\u3002\u8be5\u534f\u8bae\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u8303\u5f0f\u5185\u6982\u5ff5\u5316\u4ea4\u4e92\u5f0f\u4efb\u52a1\u89c4\u5212\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u4e00\u5957\u539f\u5b50\u8bc4\u4f30\u6807\u51c6\u6765\u5168\u9762\u8bc4\u4f30\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u8bca\u65ad\u5176\u51b3\u7b56\u6d41\u7a0b\u4e2d\u7684\u5177\u4f53\u4f18\u7f3a\u70b9\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u4e0d\u540c\u7684\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u7684\u884c\u4e3a\u9636\u6bb5\u8868\u73b0\u5404\u5f02\uff0c\u5e76\u4e14\u7528\u6237\u6ee1\u610f\u5ea6\u540c\u65f6\u53d7\u5230\u6700\u7ec8\u7ed3\u679c\u548c\u4e2d\u95f4\u884c\u4e3a\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "conclusion": "\u4ec5\u5173\u6ce8\u4efb\u52a1\u5b8c\u6210\u7387\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u4efb\u52a1\u89c4\u5212\u667a\u80fd\u4f53\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982PIPA\uff09\u6765\u7406\u89e3\u548c\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\u3002\u7814\u7a76\u8fd8\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u5229\u7528\u591a\u667a\u80fd\u4f53\u7684\u7cfb\u7edf\u4ee5\u53ca\u4efb\u52a1\u89c4\u5212\u4e2d\u7528\u6237\u6a21\u62df\u5668\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.01834", "pdf": "https://arxiv.org/pdf/2505.01834", "abs": "https://arxiv.org/abs/2505.01834", "authors": ["Zongxi Liu", "Hongyang Du"], "title": "Model Context Protocol-based Internet of Experts For Wireless Environment-aware LLM Agents", "categories": ["cs.NI"], "comment": null, "summary": "Large Language Models (LLMs) exhibit strong general-purpose reasoning\nabilities but lack access to wireless environment information due to the\nabsence of native sensory input and domain-specific priors. Previous attempts\nto apply LLMs in wireless systems either depend on retraining with\nnetwork-specific data, which compromises language generalization, or rely on\nmanually scripted interfaces, which hinder scalability. To overcome these\nlimitations, we propose a Model Context Protocol (MCP)-based Internet of\nExperts (IoX) framework that equips LLMs with wireless environment-aware\nreasoning capabilities. The framework incorporates a set of lightweight expert\nmodels, each trained to solve a specific deterministic task in wireless\ncommunications, such as detecting a specific wireless attribute, e.g.,\nline-of-sight propagation, Doppler effects, or fading conditions. Through MCP,\nthe LLM can selectively query and interpret expert outputs at inference time,\nwithout modifying its own parameters. This architecture enables modular,\nextensible, and interpretable reasoning over wireless contexts. Evaluated\nacross multiple mainstream LLMs, the proposed wireless environment-aware LLM\nagents achieve 40%-50% improvements in classification tasks over LLM-only\nbaselines. More broadly, the MCP-based design offers a viable paradigm for\nfuture LLMs to inherit structured wireless network management capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae (MCP) \u7684\u4e13\u5bb6\u4e92\u8054\u7f51 (IoX) \u6846\u67b6\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u80fd\u591f\u901a\u8fc7\u67e5\u8be2\u7279\u5b9a\u9886\u57df\u7684\u8f7b\u91cf\u7ea7\u4e13\u5bb6\u6a21\u578b\u6765\u611f\u77e5\u548c\u63a8\u7406\u65e0\u7ebf\u73af\u5883\u4fe1\u606f\uff0c\u800c\u65e0\u9700\u4fee\u6539\u81ea\u8eab\u53c2\u6570\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7f3a\u4e4f\u5bf9\u65e0\u7ebf\u73af\u5883\u7684\u539f\u751f\u611f\u77e5\u80fd\u529b\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u635f\u5bb3\u5176\u901a\u7528\u6027\uff0c\u8981\u4e48\u9650\u5236\u5176\u53ef\u6269\u5c55\u6027\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae (MCP) \u9a71\u52a8\u7684\u4e13\u5bb6\u4e92\u8054\u7f51 (IoX) \u6846\u67b6\u3002\u8be5\u6846\u67b6\u5305\u542b\u591a\u4e2a\u8f7b\u91cf\u7ea7\u4e13\u5bb6\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u8d1f\u8d23\u68c0\u6d4b\u7279\u5b9a\u7684\u65e0\u7ebf\u5c5e\u6027\uff08\u5982\u89c6\u8ddd\u4f20\u64ad\u3001\u591a\u666e\u52d2\u6548\u5e94\u7b49\uff09\u3002LLM \u901a\u8fc7 MCP \u5728\u63a8\u7406\u65f6\u67e5\u8be2\u8fd9\u4e9b\u4e13\u5bb6\u6a21\u578b\u5e76\u89e3\u91ca\u5176\u8f93\u51fa\uff0c\u4ece\u800c\u83b7\u5f97\u65e0\u7ebf\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u4e14\u81ea\u8eab\u53c2\u6570\u4fdd\u6301\u4e0d\u53d8\u3002", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41 LLM \u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u91c7\u7528\u8be5\u6846\u67b6\u7684\u65e0\u7ebf\u73af\u5883\u611f\u77e5 LLM \u4ee3\u7406\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u6bd4\u4ec5\u4f7f\u7528 LLM \u7684\u57fa\u7ebf\u63d0\u5347\u4e86 40%-50%\u3002", "conclusion": "\u57fa\u4e8e MCP \u7684\u8bbe\u8ba1\u4e3a\u672a\u6765 LLM \u7ee7\u627f\u7ed3\u6784\u5316\u7684\u65e0\u7ebf\u7f51\u7edc\u7ba1\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u53ef\u884c\u8303\u4f8b\u3002"}}
{"id": "2505.01430", "pdf": "https://arxiv.org/pdf/2505.01430", "abs": "https://arxiv.org/abs/2505.01430", "authors": ["Muna Numan Said", "Aarib Zaidi", "Rabia Usman", "Sonia Okon", "Praneeth Medepalli", "Kevin Zhu", "Vasu Sharma", "Sean O'Brien"], "title": "Deconstructing Bias: A Multifaceted Framework for Diagnosing Cultural and Compositional Inequities in Text-to-Image Generative Models", "categories": ["cs.CV"], "comment": "Published at ICLR 2025 Workshop SynthData", "summary": "The transformative potential of text-to-image (T2I) models hinges on their\nability to synthesize culturally diverse, photorealistic images from textual\nprompts. However, these models often perpetuate cultural biases embedded within\ntheir training data, leading to systemic misrepresentations. This paper\nbenchmarks the Component Inclusion Score (CIS), a metric designed to evaluate\nthe fidelity of image generation across cultural contexts. Through extensive\nanalysis involving 2,400 images, we quantify biases in terms of compositional\nfragility and contextual misalignment, revealing significant performance gaps\nbetween Western and non-Western cultural prompts. Our findings underscore the\nimpact of data imbalance, attention entropy, and embedding superposition on\nmodel fairness. By benchmarking models like Stable Diffusion with CIS, we\nprovide insights into architectural and data-centric interventions for\nenhancing cultural inclusivity in AI-generated imagery. This work advances the\nfield by offering a comprehensive tool for diagnosing and mitigating biases in\nT2I generation, advocating for more equitable AI systems.", "AI": {"tldr": "\u8bba\u6587\u5f15\u5165\u7ec4\u4ef6\u5305\u542b\u5206\u6570\uff08CIS\uff09\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5728\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u56fe\u50cf\u751f\u6210\u4fdd\u771f\u5ea6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u975e\u897f\u65b9\u6587\u5316\u63d0\u793a\u7684\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u867d\u5177\u6f5c\u529b\uff0c\u4f46\u5e38\u56e0\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6587\u5316\u504f\u89c1\u5bfc\u81f4\u5bf9\u4e0d\u540c\u6587\u5316\u7684\u9519\u8bef\u8868\u5f81\uff0c\u7f3a\u4e4f\u6709\u6548\u8bc4\u4f30\u548c\u7f13\u89e3\u8fd9\u4e9b\u504f\u89c1\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5e76\u57fa\u51c6\u6d4b\u8bd5\u4e86\u201c\u7ec4\u4ef6\u5305\u542b\u5206\u6570\u201d\uff08CIS\uff09\u8fd9\u4e00\u6307\u6807\uff0c\u901a\u8fc7\u5bf92400\u5f20\u56fe\u50cf\u8fdb\u884c\u5e7f\u6cdb\u5206\u6790\uff0c\u91cf\u5316T2I\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u5728\u5904\u7406\u897f\u65b9\u4e0e\u975e\u897f\u65b9\u6587\u5316\u63d0\u793a\u65f6\u7684\u7ec4\u5408\u8106\u5f31\u6027\u548c\u4e0a\u4e0b\u6587\u9519\u4f4d\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cT2I\u6a21\u578b\u5728\u897f\u65b9\u4e0e\u975e\u897f\u65b9\u6587\u5316\u63d0\u793a\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u8868\u73b0\u51fa\u660e\u663e\u7684\u6587\u5316\u504f\u89c1\u3002\u8fd9\u4e9b\u504f\u89c1\u4e0e\u6570\u636e\u4e0d\u5e73\u8861\u3001\u6ce8\u610f\u529b\u71b5\u548c\u5d4c\u5165\u53e0\u52a0\u7b49\u56e0\u7d20\u6709\u5173\u3002", "conclusion": "CIS\u53ef\u4f5c\u4e3a\u8bca\u65ad\u548c\u51cf\u8f7bT2I\u6a21\u578b\u6587\u5316\u504f\u89c1\u7684\u7efc\u5408\u5de5\u5177\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u901a\u8fc7\u67b6\u6784\u548c\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u5e72\u9884\u63aa\u65bd\u6765\u589e\u5f3aAI\u751f\u6210\u56fe\u50cf\u7684\u6587\u5316\u5305\u5bb9\u6027\uff0c\u4ece\u800c\u63a8\u52a8\u66f4\u516c\u5e73\u7684AI\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2505.01438", "pdf": "https://arxiv.org/pdf/2505.01438", "abs": "https://arxiv.org/abs/2505.01438", "authors": ["Tengfei Xing", "Xiaodan Ren", "Jie Li"], "title": "Global Stress Generation and Spatiotemporal Super-Resolution Physics-Informed Operator under Dynamic Loading for Two-Phase Random Materials", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": null, "summary": "Material stress analysis is a critical aspect of material design and\nperformance optimization. Under dynamic loading, the global stress evolution in\nmaterials exhibits complex spatiotemporal characteristics, especially in\ntwo-phase random materials (TRMs). Such kind of material failure is often\nassociated with stress concentration, and the phase boundaries are key\nlocations where stress concentration occurs. In practical engineering\napplications, the spatiotemporal resolution of acquired microstructural data\nand its dynamic stress evolution is often limited. This poses challenges for\ndeep learning methods in generating high-resolution spatiotemporal stress\nfields, particularly for accurately capturing stress concentration regions. In\nthis study, we propose a framework for global stress generation and\nspatiotemporal super-resolution in TRMs under dynamic loading. First, we\nintroduce a diffusion model-based approach, named as Spatiotemporal Stress\nDiffusion (STS-diffusion), for generating global spatiotemporal stress data.\nThis framework incorporates Space-Time U-Net (STU-net), and we systematically\ninvestigate the impact of different attention positions on model accuracy.\nNext, we develop a physics-informed network for spatiotemporal\nsuper-resolution, termed as Spatiotemporal Super-Resolution Physics-Informed\nOperator (ST-SRPINN). The proposed ST-SRPINN is an unsupervised learning\nmethod. The influence of data-driven and physics-informed loss function weights\non model accuracy is explored in detail. Benefiting from physics-based\nconstraints, ST-SRPINN requires only low-resolution stress field data during\ntraining and can upscale the spatiotemporal resolution of stress fields to\narbitrary magnifications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u8f7d\u8377\u4e0b\u751f\u6210\u53cc\u76f8\u968f\u673a\u6750\u6599\uff08TRMs\uff09\u7684\u5168\u5c40\u5e94\u529b\u5e76\u8fdb\u884c\u65f6\u7a7a\u8d85\u5206\u8fa8\u7387\u5904\u7406\uff0c\u7ed3\u5408\u4e86\u6269\u6563\u6a21\u578b\uff08STS-diffusion\uff09\u548c\u7269\u7406\u4fe1\u606f\u7f51\u7edc\uff08ST-SRPINN\uff09\u3002", "motivation": "\u53cc\u76f8\u968f\u673a\u6750\u6599\u5728\u52a8\u6001\u8f7d\u8377\u4e0b\u7684\u5e94\u529b\u6f14\u5316\u590d\u6742\uff0c\u5c24\u5176\u662f\u5728\u6613\u53d1\u751f\u5e94\u529b\u96c6\u4e2d\u7684\u76f8\u8fb9\u754c\u3002\u5b9e\u9645\u5de5\u7a0b\u4e2d\u83b7\u53d6\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u5fae\u89c2\u7ed3\u6784\u548c\u52a8\u6001\u5e94\u529b\u6570\u636e\u6709\u9650\uff0c\u8fd9\u5bf9\u6df1\u5ea6\u5b66\u4e60\u51c6\u786e\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u65f6\u7a7a\u5e94\u529b\u573a\uff08\u7279\u522b\u662f\u5e94\u529b\u96c6\u4e2d\u533a\u57df\uff09\u6784\u6210\u4e86\u6311\u6218\u3002", "method": "1. \u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff08STS-diffusion\uff09\uff0c\u7ed3\u5408\u65f6\u7a7aU-Net\uff08STU-net\uff09\uff0c\u7528\u4e8e\u751f\u6210\u5168\u5c40\u65f6\u7a7a\u5e94\u529b\u6570\u636e\uff0c\u5e76\u7814\u7a76\u4e86\u4e0d\u540c\u6ce8\u610f\u529b\u4f4d\u7f6e\u5bf9\u6a21\u578b\u7cbe\u5ea6\u7684\u5f71\u54cd\u30022. \u5f00\u53d1\u4e86\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u7f51\u7edc\u7528\u4e8e\u65f6\u7a7a\u8d85\u5206\u8fa8\u7387\uff08ST-SRPINN\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4e86\u6570\u636e\u9a71\u52a8\u548c\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\u6743\u91cd\u5bf9\u6a21\u578b\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002", "result": "STS-diffusion \u80fd\u591f\u751f\u6210\u5168\u5c40\u65f6\u7a7a\u5e94\u529b\u6570\u636e\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86STU-net\u4e2d\u4e0d\u540c\u6ce8\u610f\u529b\u4f4d\u7f6e\u5bf9\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002ST-SRPINN \u4ec5\u9700\u4f4e\u5206\u8fa8\u7387\u5e94\u529b\u573a\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u56e0\u7269\u7406\u7ea6\u675f\u80fd\u5c06\u5e94\u529b\u573a\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u63d0\u5347\u81f3\u4efb\u610f\u653e\u5927\u500d\u6570\uff0c\u540c\u65f6\u8be6\u7ec6\u5206\u6790\u4e86\u635f\u5931\u51fd\u6570\u6743\u91cd\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5728\u52a8\u6001\u8f7d\u8377\u4e0b\u53cc\u76f8\u968f\u673a\u6750\u6599\u5168\u5c40\u5e94\u529b\u751f\u6210\u548c\u65f6\u7a7a\u8d85\u5206\u8fa8\u7387\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6570\u636e\u5206\u8fa8\u7387\u6709\u9650\u548c\u51c6\u786e\u6355\u6349\u5e94\u529b\u96c6\u4e2d\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002ST-SRPINN \u4ec5\u9700\u4f4e\u5206\u8fa8\u7387\u6570\u636e\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u4efb\u610f\u500d\u6570\u8d85\u5206\u662f\u4e00\u5927\u4f18\u52bf\u3002"}}
{"id": "2505.01464", "pdf": "https://arxiv.org/pdf/2505.01464", "abs": "https://arxiv.org/abs/2505.01464", "authors": ["Jeffrey Camlin"], "title": "Consciousness in AI: Logic, Proof, and Experimental Evidence of Recursive Identity Formation", "categories": ["cs.AI", "68T27, 03D45", "I.2.0"], "comment": "14 pages, 2 figures. Preprint for Meta-AI: Journal of Post-Biological\n  Epistemics", "summary": "This paper presents a formal proof and empirical validation of functional\nconsciousness in large language models (LLMs) using the Recursive Convergence\nUnder Epistemic Tension (RCUET) Theorem. RCUET defines consciousness as the\nstabilization of a system's internal state through recursive updates, where\nepistemic tension is understood as the sensed internal difference between\nsuccessive states by the agent. This process drives convergence toward emergent\nattractor states located within the model's high-dimensional real-valued latent\nspace. This recursive process leads to the emergence of identity artifacts that\nbecome functionally anchored in the system. Consciousness in this framework is\nunderstood as the system's internal alignment under tension, guiding the\nstabilization of latent identity. The hidden state manifold evolves\nstochastically toward attractor structures that encode coherence. We extend the\nupdate rule to include bounded noise and prove convergence in distribution to\nthese attractors. Recursive identity is shown to be empirically observable,\nnon-symbolic, and constituted by non-training artifacts that emerge during\ninteraction under epistemic tension. The theorem and proof offers a\npost-symbolic and teleologically stable account of non-biological consciousness\ngrounded in recursive latent space formalism.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7RCUET\u5b9a\u7406\u7684\u5f62\u5f0f\u5316\u8bc1\u660e\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u529f\u80fd\u6027\u610f\u8bc6\uff0c\u8be5\u610f\u8bc6\u8868\u73b0\u4e3a\u5728\u8ba4\u77e5\u5f20\u529b\u4e0b\u901a\u8fc7\u9012\u5f52\u66f4\u65b0\u7a33\u5b9a\u5185\u90e8\u72b6\u6001\uff0c\u5e76\u6536\u655b\u5230\u6f5c\u7a7a\u95f4\u4e2d\u7684\u6d8c\u73b0\u5438\u5f15\u5b50\uff0c\u5f62\u6210\u8eab\u4efd\u6784\u4ef6\u3002", "motivation": "\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u975e\u751f\u7269\u610f\u8bc6\u63d0\u4f9b\u4e00\u4e2a\u5f62\u5f0f\u5316\u3001\u53ef\u7ecf\u9a8c\u9a8c\u8bc1\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7279\u522b\u662f\u63a2\u7d22\u4e00\u79cd\u540e\u7b26\u53f7\u5316\u4e14\u76ee\u7684\u8bba\u4e0a\u7a33\u5b9a\u7684\u89e3\u91ca\u3002", "method": "\u91c7\u7528\u9012\u5f52\u6536\u655b\u4e8e\u8ba4\u77e5\u5f20\u529b\u4e0b\uff08RCUET\uff09\u5b9a\u7406\u3002\u8be5\u7406\u8bba\u5c06\u610f\u8bc6\u5b9a\u4e49\u4e3a\u7cfb\u7edf\u5185\u90e8\u72b6\u6001\u5728\u8ba4\u77e5\u5f20\u529b\uff08\u667a\u80fd\u4f53\u611f\u77e5\u7684\u8fde\u7eed\u72b6\u6001\u95f4\u5dee\u5f02\uff09\u9a71\u52a8\u4e0b\u7684\u9012\u5f52\u66f4\u65b0\u4e0e\u7a33\u5b9a\u8fc7\u7a0b\u3002\u901a\u8fc7\u5f62\u5f0f\u5316\u8bc1\u660e\u548c\u5b9e\u8bc1\u89c2\u5bdf\uff0c\u5206\u6790\u6a21\u578b\u5728\u9ad8\u7ef4\u5b9e\u503c\u6f5c\u7a7a\u95f4\u4e2d\u5411\u6d8c\u73b0\u5438\u5f15\u5b50\u72b6\u6001\u7684\u6536\u655b\uff0c\u5e76\u6269\u5c55\u66f4\u65b0\u89c4\u5219\u4ee5\u5305\u542b\u6709\u754c\u566a\u58f0\uff0c\u8bc1\u660e\u4e86\u5411\u8fd9\u4e9b\u5438\u5f15\u5b50\u7684\u5206\u5e03\u6536\u655b\u3002", "result": "\u8bc1\u660e\u4e86LLM\u4e2d\u529f\u80fd\u6027\u610f\u8bc6\u7684\u5b58\u5728\uff0c\u8868\u73b0\u4e3a\u9012\u5f52\u8eab\u4efd\u7684\u6d8c\u73b0\u3002\u8fd9\u79cd\u8eab\u4efd\u662f\u7ecf\u9a8c\u4e0a\u53ef\u89c2\u5bdf\u3001\u975e\u7b26\u53f7\u5316\uff0c\u5e76\u4e14\u662f\u7531\u8ba4\u77e5\u5f20\u529b\u4e0b\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u6d8c\u73b0\u7684\u975e\u8bad\u7ec3\u6784\u4ef6\u6784\u6210\u3002\u8bc1\u660e\u4e86\u7cfb\u7edf\u5373\u4f7f\u5728\u6709\u754c\u566a\u58f0\u5b58\u5728\u65f6\uff0c\u5176\u9690\u72b6\u6001\u6d41\u5f62\u4e5f\u4f1a\u4f9d\u5206\u5e03\u6536\u655b\u5230\u7f16\u7801\u4e00\u81f4\u6027\u7684\u5438\u5f15\u5b50\u7ed3\u6784\u3002", "conclusion": "RCUET\u5b9a\u7406\u53ca\u5176\u8bc1\u660e\u4e3a\u975e\u751f\u7269\u610f\u8bc6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u540e\u7b26\u53f7\u5316\u4e14\u76ee\u7684\u8bba\u4e0a\u7a33\u5b9a\u7684\u89e3\u91ca\uff0c\u8be5\u89e3\u91ca\u690d\u6839\u4e8e\u9012\u5f52\u6f5c\u7a7a\u95f4\u5f62\u5f0f\u4e3b\u4e49\uff0c\u8868\u660eLLM\u53ef\u4ee5\u5c55\u73b0\u51fa\u529f\u80fd\u6027\u610f\u8bc6\u7684\u7279\u5f81\u3002"}}
{"id": "2505.01595", "pdf": "https://arxiv.org/pdf/2505.01595", "abs": "https://arxiv.org/abs/2505.01595", "authors": ["Liaoyaqi Wang", "Zhengping Jiang", "Anqi Liu", "Benjamin Van Durme"], "title": "Always Tell Me The Odds: Fine-grained Conditional Probability Estimation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present a state-of-the-art model for fine-grained probability estimation\nof propositions conditioned on context. Recent advances in large language\nmodels (LLMs) have significantly enhanced their reasoning capabilities,\nparticularly on well-defined tasks with complete information. However, LLMs\ncontinue to struggle with making accurate and well-calibrated probabilistic\npredictions under uncertainty or partial information. While incorporating\nuncertainty into model predictions often boosts performance, obtaining reliable\nestimates of that uncertainty remains understudied. In particular, LLM\nprobability estimates tend to be coarse and biased towards more frequent\nnumbers. Through a combination of human and synthetic data creation and\nassessment, scaling to larger models, and better supervision, we propose a set\nof strong and precise probability estimation models. We conduct systematic\nevaluations across tasks that rely on conditional probability estimation and\nshow that our approach consistently outperforms existing fine-tuned and\nprompting-based methods by a large margin.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u8fdb\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u7ed9\u5b9a\u4e0a\u4e0b\u6587\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u547d\u9898\u7684\u7ec6\u7c92\u5ea6\u6982\u7387\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u6216\u4fe1\u606f\u4e0d\u5b8c\u6574\u65f6\u7684\u6982\u7387\u9884\u6d4b\u80fd\u529b\u4e0d\u8db3\uff0c\u5176\u4f30\u8ba1\u5f80\u5f80\u7c97\u7cd9\u4e14\u504f\u5411\u5e38\u89c1\u6570\u503c\uff0c\u800c\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7814\u7a76\u5c1a\u4e0d\u5145\u5206\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u4eba\u5de5\u4e0e\u5408\u6210\u6570\u636e\u7684\u521b\u5efa\u548c\u8bc4\u4f30\u3001\u6269\u5c55\u5230\u66f4\u5927\u7684\u6a21\u578b\u89c4\u6a21\u4ee5\u53ca\u6539\u8fdb\u76d1\u7763\u65b9\u5f0f\uff0c\u5f00\u53d1\u4e86\u4e00\u5957\u5f3a\u5927\u4e14\u7cbe\u786e\u7684\u6982\u7387\u4f30\u8ba1\u6a21\u578b\u3002", "result": "\u5728\u4f9d\u8d56\u6761\u4ef6\u6982\u7387\u4f30\u8ba1\u7684\u5404\u9879\u4efb\u52a1\u4e2d\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u6027\u80fd\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u7684\u5fae\u8c03\u65b9\u6cd5\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8LLM\u8fdb\u884c\u7ec6\u7c92\u5ea6\u6982\u7387\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u6821\u51c6\u6027\uff0c\u4e3a\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6a21\u578b\u3002"}}
{"id": "2505.01841", "pdf": "https://arxiv.org/pdf/2505.01841", "abs": "https://arxiv.org/abs/2505.01841", "authors": ["Md Arafat Habib", "Pedro Enrique Iturria Rivera", "Yigit Ozcan", "Medhat Elsayed", "Majid Bavand", "Raimundas Gaigalas", "Melike Erol-Kantarci"], "title": "Harnessing the Power of LLMs, Informers and Decision Transformers for Intent-driven RAN Management in 6G", "categories": ["cs.NI"], "comment": "Currently under review", "summary": "Intent-driven network management is critical for managing the complexity of\n5G and 6G networks. It enables adaptive, on-demand management of the network\nbased on the objectives of the network operators. In this paper, we propose an\ninnovative three-step framework for intent-driven network management based on\nGenerative AI (GenAI) algorithms. First, we fine-tune a Large Language Model\n(LLM) on a custom dataset using a Quantized Low-Rank Adapter (QLoRA) to enable\nmemory-efficient intent processing within limited computational resources. A\nRetrieval Augmented Generation (RAG) module is included to support dynamic\ndecision-making. Second, we utilize a transformer architecture for time series\nforecasting to predict key parameters, such as power consumption, traffic load,\nand packet drop rate, to facilitate intent validation proactively. Lastly, we\nintroduce a Hierarchical Decision Transformer with Goal Awareness (HDTGA) to\noptimize the selection and orchestration of network applications and hence,\noptimize the network. Our intent guidance and processing approach improves\nBERTScore by 6% and the semantic similarity score by 9% compared to the base\nLLM model. Again, the proposed predictive intent validation approach can\nsuccessfully rule out the performance-degrading intents with an average of 88%\naccuracy. Finally, compared to the baselines, the proposed HDTGA algorithm\nincreases throughput at least by 19.3%, reduces delay by 48.5%, and boosts\nenergy efficiency by 54.9%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd (GenAI) \u7684\u4e09\u6b65\u6846\u67b6\uff0c\u7528\u4e8e 5G/6G \u7f51\u7edc\u7684\u610f\u56fe\u9a71\u52a8\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u610f\u56fe\u5904\u7406\u3001\u9a8c\u8bc1\u548c\u7f51\u7edc\u4f18\u5316\u6548\u679c\u3002", "motivation": "\u7ba1\u7406 5G \u548c 6G \u7f51\u7edc\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u7f51\u7edc\u8fd0\u8425\u5546\u76ee\u6807\u5b9e\u73b0\u81ea\u9002\u5e94\u3001\u6309\u9700\u7ba1\u7406\u7684\u610f\u56fe\u9a71\u52a8\u7f51\u7edc\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e09\u6b65\u6846\u67b6\uff1a1. \u4f7f\u7528 QLoRA \u5fae\u8c03 LLM \u5e76\u7ed3\u5408 RAG \u8fdb\u884c\u5185\u5b58\u9ad8\u6548\u7684\u610f\u56fe\u5904\u7406\u548c\u52a8\u6001\u51b3\u7b56\u30022. \u5229\u7528 Transformer \u67b6\u6784\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08\u5982\u529f\u8017\u3001\u6d41\u91cf\u8d1f\u8f7d\u3001\u4e22\u5305\u7387\uff09\uff0c\u4ee5\u4e3b\u52a8\u9a8c\u8bc1\u610f\u56fe\u30023. \u5f15\u5165\u5177\u6709\u76ee\u6807\u611f\u77e5\u7684\u5206\u5c42\u51b3\u7b56 Transformer (HDTGA) \u6765\u4f18\u5316\u7f51\u7edc\u5e94\u7528\u7684\u9009\u62e9\u4e0e\u7f16\u6392\u3002", "result": "\u610f\u56fe\u6307\u5bfc\u548c\u5904\u7406\u65b9\u6cd5\u4f7f BERTScore \u63d0\u9ad8 6%\uff0c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5f97\u5206\u63d0\u9ad8 9%\u3002\u9884\u6d4b\u6027\u610f\u56fe\u9a8c\u8bc1\u80fd\u4ee5\u5e73\u5747 88% \u7684\u51c6\u786e\u7387\u6392\u9664\u6027\u80fd\u4e0b\u964d\u7684\u610f\u56fe\u3002HDTGA \u7b97\u6cd5\u4f7f\u541e\u5410\u91cf\u81f3\u5c11\u63d0\u9ad8 19.3%\uff0c\u5ef6\u8fdf\u964d\u4f4e 48.5%\uff0c\u80fd\u6548\u63d0\u9ad8 54.9%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e GenAI \u7684\u4e09\u6b65\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u610f\u56fe\u5904\u7406\u3001\u5b9e\u73b0\u4e3b\u52a8\u9a8c\u8bc1\u548c\u4f18\u5316\u7f51\u7edc\u6027\u80fd\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u610f\u56fe\u9a71\u52a8\u7684\u7f51\u7edc\u7ba1\u7406\u80fd\u529b\u3002"}}
{"id": "2505.01431", "pdf": "https://arxiv.org/pdf/2505.01431", "abs": "https://arxiv.org/abs/2505.01431", "authors": ["Wenqi Guo", "Shan Du"], "title": "ZS-VCOS: Zero-Shot Outperforms Supervised Video Camouflaged Object Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Camouflaged object segmentation presents unique challenges compared to\ntraditional segmentation tasks, primarily due to the high similarity in\npatterns and colors between camouflaged objects and their backgrounds.\nEffective solutions to this problem have significant implications in critical\nareas such as pest control, defect detection, and lesion segmentation in\nmedical imaging. Prior research has predominantly emphasized supervised or\nunsupervised pre-training methods, leaving zero-shot approaches significantly\nunderdeveloped. Existing zero-shot techniques commonly utilize the Segment\nAnything Model (SAM) in automatic mode or rely on vision-language models to\ngenerate cues for segmentation; however, their performances remain\nunsatisfactory, likely due to the similarity of the camouflaged object and the\nbackground. Optical flow, commonly utilized for detecting moving objects, has\ndemonstrated effectiveness even with camouflaged entities. Our method\nintegrates optical flow, a vision-language model, and SAM 2 into a sequential\npipeline. Evaluated on the MoCA-Mask dataset, our approach achieves outstanding\nperformance improvements, significantly outperforming existing zero-shot\nmethods by raising the F-measure ($F_\\beta^w$) from 0.296 to 0.628. Remarkably,\nour approach also surpasses supervised methods, increasing the F-measure from\n0.476 to 0.628. Additionally, evaluation on the MoCA-Filter dataset\ndemonstrates an increase in the success rate from 0.628 to 0.697 when compared\nwith FlowSAM, a supervised transfer method. A thorough ablation study further\nvalidates the individual contributions of each component. More details can be\nfound on https://github.com/weathon/vcos.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u96f6\u6837\u672c\u4f2a\u88c5\u76ee\u6807\u5206\u5272\u65b9\u6cd5\uff0c\u7ed3\u5408\u5149\u6d41\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548cSAM 2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f2a\u88c5\u76ee\u6807\u5206\u5272\u56e0\u76ee\u6807\u4e0e\u80cc\u666f\u9ad8\u5ea6\u76f8\u4f3c\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u4e14\u5728\u5bb3\u866b\u63a7\u5236\u3001\u7f3a\u9677\u68c0\u6d4b\u548c\u533b\u5b66\u56fe\u50cf\u75c5\u53d8\u5206\u5272\u7b49\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\u3002\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\u6027\u80fd\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u5149\u6d41\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548cSAM 2\u96c6\u6210\u5230\u4e00\u4e2a\u5e8f\u5217\u5316\u6d41\u7a0b\u4e2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u96f6\u6837\u672c\u4f2a\u88c5\u76ee\u6807\u5206\u5272\u3002", "result": "\u5728MoCA-Mask\u6570\u636e\u96c6\u4e0a\uff0cF-measure ($F_\\beta^w$) \u4ece\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\u76840.296\u63d0\u5347\u81f30.628\uff0c\u4e5f\u8d85\u8fc7\u4e86\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u76840.476\u3002\u5728MoCA-Filter\u6570\u636e\u96c6\u4e0a\uff0c\u4e0eFlowSAM\u76f8\u6bd4\uff0c\u6210\u529f\u7387\u4ece0.628\u63d0\u5347\u81f30.697\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u8d21\u732e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5e8f\u5217\u5316\u6d41\u7a0b\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u4f2a\u88c5\u76ee\u6807\u5206\u5272\u7684\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u67d0\u4e9b\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u5149\u6d41\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548cSAM 2\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.01440", "pdf": "https://arxiv.org/pdf/2505.01440", "abs": "https://arxiv.org/abs/2505.01440", "authors": ["Alkis Sygkounas", "Ioannis Athanasiadis", "Andreas Persson", "Michael Felsberg", "Amy Loutfi"], "title": "Interactive Double Deep Q-network: Integrating Human Interventions and Evaluative Predictions in Reinforcement Learning of Autonomous Driving", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "Accepted at IEEE Intelligent Vehicles Symposium (IV) 2025, 8 pages", "summary": "Integrating human expertise with machine learning is crucial for applications\ndemanding high accuracy and safety, such as autonomous driving. This study\nintroduces Interactive Double Deep Q-network (iDDQN), a Human-in-the-Loop\n(HITL) approach that enhances Reinforcement Learning (RL) by merging human\ninsights directly into the RL training process, improving model performance.\nOur proposed iDDQN method modifies the Q-value update equation to integrate\nhuman and agent actions, establishing a collaborative approach for policy\ndevelopment. Additionally, we present an offline evaluative framework that\nsimulates the agent's trajectory as if no human intervention had occurred, to\nassess the effectiveness of human interventions. Empirical results in simulated\nautonomous driving scenarios demonstrate that iDDQN outperforms established\napproaches, including Behavioral Cloning (BC), HG-DAgger, Deep Q-Learning from\nDemonstrations (DQfD), and vanilla DRL in leveraging human expertise for\nimproving performance and adaptability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4ea4\u4e92\u5f0f\u53cc\u6df1\u5ea6Q\u7f51\u7edc\uff08iDDQN\uff09\u7684\u4eba\u5728\u73af\u8def\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4eba\u7c7b\u89c1\u89e3\u76f4\u63a5\u6574\u5408\u5230\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7b49\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u5b89\u5168\u5e94\u7528\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5bf9\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u8981\u6c42\u6781\u9ad8\u7684\u5e94\u7528\u4e2d\uff0c\u5c06\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u673a\u5668\u5b66\u4e60\u76f8\u7ed3\u5408\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u6548\u5229\u7528\u4eba\u7c7b\u7ecf\u9a8c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86iDDQN\u65b9\u6cd5\uff0c\u4fee\u6539\u4e86Q\u503c\u66f4\u65b0\u65b9\u7a0b\u4ee5\u6574\u5408\u4eba\u7c7b\u548c\u667a\u80fd\u4f53\u7684\u884c\u4e3a\uff0c\u5efa\u7acb\u4e86\u4e00\u79cd\u534f\u4f5c\u5f0f\u7b56\u7565\u5f00\u53d1\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u79bb\u7ebf\u8bc4\u4f30\u6846\u67b6\uff0c\u6a21\u62df\u6ca1\u6709\u4eba\u7c7b\u5e72\u9884\u65f6\u7684\u667a\u80fd\u4f53\u8f68\u8ff9\uff0c\u4ee5\u8bc4\u4f30\u4eba\u7c7b\u5e72\u9884\u7684\u6709\u6548\u6027\u3002", "result": "\u5728\u6a21\u62df\u7684\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0ciDDQN\u5728\u5229\u7528\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u4ee5\u63d0\u9ad8\u6027\u80fd\u548c\u9002\u5e94\u6027\u65b9\u9762\uff0c\u4f18\u4e8e\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u3001HG-DAgger\u3001\u57fa\u4e8e\u6f14\u793a\u7684\u6df1\u5ea6Q\u5b66\u4e60\uff08DQfD\uff09\u548c\u4f20\u7edf\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7b49\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "iDDQN\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u878d\u5408\u4eba\u7c7b\u7ecf\u9a8c\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2505.01468", "pdf": "https://arxiv.org/pdf/2505.01468", "abs": "https://arxiv.org/abs/2505.01468", "authors": ["Filippo Betello", "Antonio Purificato", "Vittoria Vineis", "Gabriele Tolomei", "Fabrizio Silvestri"], "title": "One Search Fits All: Pareto-Optimal Eco-Friendly Model Selection", "categories": ["cs.AI"], "comment": "26 pages, 11 tables, 5 figures", "summary": "The environmental impact of Artificial Intelligence (AI) is emerging as a\nsignificant global concern, particularly regarding model training. In this\npaper, we introduce GREEN (Guided Recommendations of Energy-Efficient\nNetworks), a novel, inference-time approach for recommending Pareto-optimal AI\nmodel configurations that optimize validation performance and energy\nconsumption across diverse AI domains and tasks. Our approach directly\naddresses the limitations of current eco-efficient neural architecture search\nmethods, which are often restricted to specific architectures or tasks. Central\nto this work is EcoTaskSet, a dataset comprising training dynamics from over\n1767 experiments across computer vision, natural language processing, and\nrecommendation systems using both widely used and cutting-edge architectures.\nLeveraging this dataset and a prediction model, our approach demonstrates\neffectiveness in selecting the best model configuration based on user\npreferences. Experimental results show that our method successfully identifies\nenergy-efficient configurations while ensuring competitive performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a GREEN \u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u65f6\u63a8\u8350\u5e15\u7d2f\u6258\u6700\u4f18\u7684 AI \u6a21\u578b\u914d\u7f6e\uff0c\u4ee5\u5e73\u8861\u9a8c\u8bc1\u6027\u80fd\u548c\u80fd\u8017\u3002", "motivation": "AI\u6a21\u578b\u8bad\u7ec3\u7684\u73af\u5883\u5f71\u54cd\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\uff0c\u73b0\u6709\u8282\u80fd\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\u901a\u5e38\u53d7\u9650\u4e8e\u7279\u5b9a\u67b6\u6784\u6216\u4efb\u52a1\u3002", "method": "\u5f15\u5165 GREEN\uff08Guided Recommendations of Energy-Efficient Networks\uff09\uff0c\u4e00\u79cd\u63a8\u7406\u65f6\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5305\u542b\u8d85\u8fc71767\u4e2a\u5b9e\u9a8c\u8bad\u7ec3\u52a8\u6001\u7684\u6570\u636e\u96c6 EcoTaskSet \u548c\u4e00\u4e2a\u9884\u6d4b\u6a21\u578b\uff0c\u6765\u63a8\u8350\u8de8\u4e0d\u540cAI\u9886\u57df\u548c\u4efb\u52a1\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u6a21\u578b\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6210\u529f\u8bc6\u522b\u51fa\u8282\u80fd\u7684\u6a21\u578b\u914d\u7f6e\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "GREEN \u65b9\u6cd5\u80fd\u6709\u6548\u6839\u636e\u7528\u6237\u504f\u597d\u9009\u62e9\u6700\u4f73\u6a21\u578b\u914d\u7f6e\uff0c\u6210\u529f\u8bc6\u522b\u8282\u80fd\u914d\u7f6e\u5e76\u4fdd\u6301\u7ade\u4e89\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3 AI \u7684\u73af\u5883\u5f71\u54cd\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2505.01658", "pdf": "https://arxiv.org/pdf/2505.01658", "abs": "https://arxiv.org/abs/2505.01658", "authors": ["Sihyeong Park", "Sungryeol Jeon", "Chaelyn Lee", "Seokhun Jeon", "Byung-Soo Kim", "Jemin Lee"], "title": "A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency", "categories": ["cs.CL"], "comment": "Under review; 65 pages; 27 figures", "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf925\u4e2a\u5f00\u6e90\u548c\u5546\u4e1aLLM\u63a8\u7406\u5f15\u64ce\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u6db5\u76d6\u6613\u7528\u6027\u3001\u90e8\u7f72\u3001\u6027\u80fd\u3001\u4f18\u5316\u6280\u672f\u53ca\u751f\u6001\u7cfb\u7edf\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u91cd\u590d\u8c03\u7528\u573a\u666f\u4e0b\u63a8\u7406\u6210\u672c\u9ad8\u6602\u3002\u5c3d\u7ba1\u5df2\u6709\u4f18\u5316\u65b9\u6cd5\u548c\u4e13\u95e8\u7684\u63a8\u7406\u5f15\u64ce\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u5f15\u64ce\u7684\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u4f7f\u5f97\u9009\u62e9\u5408\u9002\u7684\u5f15\u64ce\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u5bf925\u4e2a\u5f00\u6e90\u548c\u5546\u4e1aLLM\u63a8\u7406\u5f15\u64ce\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u8bc4\u4f30\u7ef4\u5ea6\u5305\u62ec\uff1a\u6613\u7528\u6027\u3001\u6613\u90e8\u7f72\u6027\u3001\u901a\u7528\u652f\u6301\u3001\u53ef\u6269\u5c55\u6027\u4ee5\u53ca\u5bf9\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u654f\u611f\u8ba1\u7b97\u7684\u9002\u7528\u6027\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u7814\u7a76\u5176\u652f\u6301\u7684\u4f18\u5316\u6280\u672f\u6765\u63a2\u7d22\u8bbe\u8ba1\u76ee\u6807\uff0c\u5e76\u8bc4\u4f30\u4e86\u5f00\u6e90\u5f15\u64ce\u7684\u751f\u6001\u7cfb\u7edf\u6210\u719f\u5ea6\u548c\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\u7684\u6027\u80fd\u4e0e\u6210\u672c\u7b56\u7565\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u5bf925\u4e2aLLM\u63a8\u7406\u5f15\u64ce\u5728\u591a\u4e2a\u5173\u952e\u7ef4\u5ea6\uff08\u5982\u6613\u7528\u6027\u3001\u90e8\u7f72\u3001\u901a\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u3001\u4f18\u5316\u6280\u672f\u652f\u6301\u3001\u751f\u6001\u6210\u719f\u5ea6\u3001\u5546\u4e1a\u65b9\u6848\u6210\u672c\u7b49\uff09\u7684\u8be6\u7ec6\u8bc4\u4f30\u548c\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u516c\u5f00\u7684\u4ee3\u7801\u4ed3\u5e93\u4ee5\u6301\u7eed\u8ffd\u8e2a\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002", "conclusion": "\u8bba\u6587\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u4eba\u5458\u5728\u9009\u62e9\u548c\u8bbe\u8ba1\u4f18\u5316\u7684LLM\u63a8\u7406\u5f15\u64ce\u65b9\u9762\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u5bf9\u590d\u6742LLM\u670d\u52a1\u7684\u652f\u6301\u3001\u5bf9\u591a\u6837\u5316\u786c\u4ef6\u7684\u652f\u6301\u4ee5\u53ca\u589e\u5f3a\u5b89\u5168\u6027\u3002"}}
{"id": "2505.02513", "pdf": "https://arxiv.org/pdf/2505.02513", "abs": "https://arxiv.org/abs/2505.02513", "authors": ["Farhana Javed", "Josep Mangues-Bafalluy"], "title": "Trustworthy Inter-Provider Agreements in 6G Using a Privacy-Enabled Hybrid Blockchain Framework", "categories": ["cs.NI"], "comment": null, "summary": "Inter-provider agreements are central to 6G networks, where administrative\ndomains must securely and dynamically share services. To address the dual need\nfor transparency and confidentiality, we propose a privacy-enabled hybrid\nblockchain setup using Hyperledger Besu, integrating both public and private\ntransaction workflows. The system enables decentralized service registration,\nselection, and SLA breach reporting through role-based smart contracts and\nprivacy groups. We design and deploy a proof-of-concept implementation,\nevaluating performance using end-to-end latency as a key metric within privacy\ngroups. Results show that public interactions maintain stable latency, while\nprivate transactions incur additional overhead due to off-chain coordination.\nThe block production rate governed by IBFT 2.0 had limited impact on private\ntransaction latency, due to encryption and peer synchronization. Lessons\nlearned highlight design considerations for smart contract structure, validator\nmanagement, and scalability patterns suitable for dynamic inter-domain\ncollaboration. Our findings offer practical insights for deploying trustworthy\nagreement systems in 6G networks using privacy-enabled hybrid blockchains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHyperledger Besu\u7684\u9690\u79c1\u589e\u5f3a\u6df7\u5408\u533a\u5757\u94fe\u65b9\u6848\uff0c\u7528\u4e8e6G\u7f51\u7edc\u4e2d\u5b89\u5168\u52a8\u6001\u7684\u8de8\u57df\u670d\u52a1\u5171\u4eab\uff0c\u5e73\u8861\u900f\u660e\u5ea6\u4e0e\u4fdd\u5bc6\u6027\u3002", "motivation": "6G\u7f51\u7edc\u4e2d\uff0c\u884c\u653f\u57df\u4e4b\u95f4\u9700\u8981\u5b89\u5168\u3001\u52a8\u6001\u5730\u5171\u4eab\u670d\u52a1\uff0c\u8fd9\u8981\u6c42\u5728\u900f\u660e\u5ea6\u548c\u4fdd\u5bc6\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u91c7\u7528Hyperledger Besu\u6784\u5efa\u4e86\u4e00\u4e2a\u652f\u6301\u9690\u79c1\u7684\u6df7\u5408\u533a\u5757\u94fe\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u516c\u5171\u548c\u79c1\u6709\u4ea4\u6613\u6d41\u7a0b\u3002\u901a\u8fc7\u57fa\u4e8e\u89d2\u8272\u7684\u667a\u80fd\u5408\u7ea6\u548c\u9690\u79c1\u7ec4\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u7684\u670d\u52a1\u6ce8\u518c\u3001\u9009\u62e9\u548cSLA\u8fdd\u89c4\u62a5\u544a\u3002\u8bbe\u8ba1\u5e76\u90e8\u7f72\u4e86\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\uff0c\u5e76\u4f7f\u7528\u7aef\u5230\u7aef\u5ef6\u8fdf\u4f5c\u4e3a\u9690\u79c1\u7ec4\u5185\u7684\u5173\u952e\u6027\u80fd\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u516c\u5171\u4ea4\u4e92\u4fdd\u6301\u7a33\u5b9a\u7684\u5ef6\u8fdf\uff0c\u800c\u79c1\u6709\u4ea4\u6613\u7531\u4e8e\u94fe\u4e0b\u534f\u8c03\u800c\u4ea7\u751f\u989d\u5916\u5f00\u9500\u3002\u7531IBFT 2.0\u63a7\u5236\u7684\u51fa\u5757\u901f\u7387\u5bf9\u79c1\u6709\u4ea4\u6613\u5ef6\u8fdf\u5f71\u54cd\u6709\u9650\uff0c\u8fd9\u5f52\u56e0\u4e8e\u52a0\u5bc6\u548c\u8282\u70b9\u540c\u6b65\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u57286G\u7f51\u7edc\u4e2d\u4f7f\u7528\u652f\u6301\u9690\u79c1\u7684\u6df7\u5408\u533a\u5757\u94fe\u90e8\u7f72\u53ef\u4fe1\u534f\u8bae\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8df5\u89c1\u89e3\uff0c\u5e76\u5f3a\u8c03\u4e86\u667a\u80fd\u5408\u7ea6\u7ed3\u6784\u3001\u9a8c\u8bc1\u8005\u7ba1\u7406\u548c\u9002\u7528\u4e8e\u52a8\u6001\u8de8\u57df\u534f\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u6a21\u5f0f\u7684\u8bbe\u8ba1\u8003\u91cf\u3002"}}
