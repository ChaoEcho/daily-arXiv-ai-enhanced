{"id": "2506.22439", "pdf": "https://arxiv.org/pdf/2506.22439", "abs": "https://arxiv.org/abs/2506.22439", "authors": ["Javier Conde", "Miguel Gonz\u00e1lez", "Mar\u00eda Grandury", "Gonzalo Mart\u00ednez", "Pedro Reviriego", "Mar Brysbaert"], "title": "Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for the GEM2 workshop at ACL 2025", "summary": "The evaluation of LLMs has so far focused primarily on how well they can\nperform different tasks such as reasoning, question-answering, paraphrasing, or\ntranslating. For most of these tasks, performance can be measured with\nobjective metrics, such as the number of correct answers. However, other\nlanguage features are not easily quantified. For example, arousal,\nconcreteness, or gender associated with a given word, as well as the extent to\nwhich we experience words with senses and relate them to a specific sense.\nThose features have been studied for many years by psycholinguistics,\nconducting large-scale experiments with humans to produce ratings for thousands\nof words. This opens an opportunity to evaluate how well LLMs align with human\nratings on these word features, taking advantage of existing studies that cover\nmany different language features in a large number of words. In this paper, we\nevaluate the alignment of a representative group of LLMs with human ratings on\ntwo psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets\ncover thirteen features over thousands of words. The results show that\nalignment is \\textcolor{black}{generally} better in the Glasgow norms evaluated\n(arousal, valence, dominance, concreteness, imageability, familiarity, and\ngender) than on the Lancaster norms evaluated (introceptive, gustatory,\nolfactory, haptic, auditory, and visual). This suggests a potential limitation\nof current LLMs in aligning with human sensory associations for words, which\nmay be due to their lack of embodied cognition present in humans and\nillustrates the usefulness of evaluating LLMs with psycholinguistic datasets.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5fc3\u7406\u8bed\u8a00\u5b66\u8bcd\u6c47\u7279\u5f81\uff08\u5982\u5524\u9192\u5ea6\u3001\u5177\u4f53\u6027\u3001\u611f\u5b98\u5173\u8054\u7b49\uff09\u4e0a\u4e0e\u4eba\u7c7b\u8bc4\u5206\u7684\u4e00\u81f4\u6027\uff0c\u7ed3\u679c\u663e\u793aLLMs\u5728\u5904\u7406\u60c5\u611f/\u8ba4\u77e5\u7279\u5f81\u65f6\u8868\u73b0\u4f18\u4e8e\u611f\u5b98\u5173\u8054\u7279\u5f81\uff0c\u8fd9\u53ef\u80fd\u4e0e\u5176\u7f3a\u4e4f\u5177\u8eab\u8ba4\u77e5\u6709\u5173\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u4e3b\u8981\u4fa7\u91cd\u4e8e\u53ef\u91cf\u5316\u7684\u4efb\u52a1\u8868\u73b0\uff0c\u4f46\u8bb8\u591a\u91cd\u8981\u7684\u8bed\u8a00\u7279\u5f81\uff08\u5982\u8bcd\u8bed\u7684\u5524\u9192\u5ea6\u3001\u5177\u4f53\u6027\u3001\u6027\u522b\u5173\u8054\u53ca\u611f\u5b98\u4f53\u9a8c\u7b49\uff09\u96be\u4ee5\u5ba2\u89c2\u91cf\u5316\u3002\u5fc3\u7406\u8bed\u8a00\u5b66\u5df2\u901a\u8fc7\u5927\u89c4\u6a21\u4eba\u7c7b\u5b9e\u9a8c\u79ef\u7d2f\u4e86\u5927\u91cf\u6b64\u7c7b\u8bcd\u6c47\u7279\u5f81\u6570\u636e\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u73b0\u6709\u6570\u636e\uff0c\u8bc4\u4f30LLMs\u4e0e\u4eba\u7c7b\u5728\u8fd9\u4e9b\u975e\u4efb\u52a1\u6027\u8bcd\u6c47\u7279\u5f81\u4e0a\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "method": "\u7814\u7a76\u9009\u53d6\u4e86\u4e00\u7ec4\u5177\u6709\u4ee3\u8868\u6027\u7684LLMs\uff0c\u5e76\u4f7f\u7528\u4e24\u4e2a\u5fc3\u7406\u8bed\u8a00\u5b66\u6570\u636e\u96c6\u2014\u2014Glasgow norms\u548cLancaster norms\u2014\u2014\u6765\u8bc4\u4f30LLMs\u4e0e\u4eba\u7c7b\u8bc4\u5206\u7684\u4e00\u81f4\u6027\u3002Glasgow norms\u6570\u636e\u96c6\u5305\u542b\u5524\u9192\u5ea6\u3001\u6548\u4ef7\u3001\u652f\u914d\u6027\u3001\u5177\u4f53\u6027\u3001\u60f3\u8c61\u6027\u3001\u719f\u6089\u5ea6\u548c\u6027\u522b\u7b4913\u79cd\u7279\u5f81\uff0c\u800cLancaster norms\u6570\u636e\u96c6\u5219\u6db5\u76d6\u4e86\u5185\u611f\u53d7\u3001\u5473\u89c9\u3001\u55c5\u89c9\u3001\u89e6\u89c9\u3001\u542c\u89c9\u548c\u89c6\u89c9\u7b49\u611f\u5b98\u7279\u5f81\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLLMs\u5728Glasgow norms\uff08\u6d89\u53ca\u5524\u9192\u5ea6\u3001\u6548\u4ef7\u3001\u652f\u914d\u6027\u3001\u5177\u4f53\u6027\u3001\u60f3\u8c61\u6027\u3001\u719f\u6089\u5ea6\u548c\u6027\u522b\uff09\u4e0a\u7684\u5bf9\u9f50\u7a0b\u5ea6\u666e\u904d\u4f18\u4e8e\u5728Lancaster norms\uff08\u6d89\u53ca\u5185\u611f\u53d7\u3001\u5473\u89c9\u3001\u55c5\u89c9\u3001\u89e6\u89c9\u3001\u542c\u89c9\u548c\u89c6\u89c9\uff09\u4e0a\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "conclusion": "\u5f53\u524dLLMs\u5728\u4e0e\u4eba\u7c7b\u8bcd\u8bed\u611f\u5b98\u5173\u8054\u6027\u65b9\u9762\u5b58\u5728\u6f5c\u5728\u5c40\u9650\uff0c\u8fd9\u53ef\u80fd\u6e90\u4e8e\u5b83\u4eec\u7f3a\u4e4f\u4eba\u7c7b\u6240\u5177\u5907\u7684\u5177\u8eab\u8ba4\u77e5\u3002\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u4f7f\u7528\u5fc3\u7406\u8bed\u8a00\u5b66\u6570\u636e\u96c6\u8bc4\u4f30LLMs\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u63ed\u793a\u5176\u66f4\u6df1\u5c42\u6b21\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2506.22485", "pdf": "https://arxiv.org/pdf/2506.22485", "abs": "https://arxiv.org/abs/2506.22485", "authors": ["Sudip Dasgupta", "Himanshu Shankar"], "title": "AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents", "categories": ["cs.CL", "cs.AI", "68T07, 68T50", "I.2.1; I.2.3; I.2.7; H.3.3"], "comment": "17 pages, 2 system diagrams, 1 table, no prior conference publication", "summary": "This study presents a modular, multi-agent system for the automated review of\nhighly structured enterprise business documents using AI agents. Unlike prior\nsolutions focused on unstructured texts or limited compliance checks, this\nframework leverages modern orchestration tools such as LangChain, CrewAI,\nTruLens, and Guidance to enable section-by-section evaluation of documents for\naccuracy, consistency, completeness, and clarity. Specialized agents, each\nresponsible for discrete review criteria such as template compliance or factual\ncorrectness, operate in parallel or sequence as required. Evaluation outputs\nare enforced to a standardized, machine-readable schema, supporting downstream\nanalytics and auditability. Continuous monitoring and a feedback loop with\nhuman reviewers allow for iterative system improvement and bias mitigation.\n  Quantitative evaluation demonstrates that the AI Agent-as-Judge system\napproaches or exceeds human performance in key areas: achieving 99% information\nconsistency (vs. 92% for humans), halving error and bias rates, and reducing\naverage review time from 30 to 2.5 minutes per document, with a 95% agreement\nrate between AI and expert human judgment. While promising for a wide range of\nindustries, the study also discusses current limitations, including the need\nfor human oversight in highly specialized domains and the operational cost of\nlarge-scale LLM usage. The proposed system serves as a flexible, auditable, and\nscalable foundation for AI-driven document quality assurance in the enterprise\ncontext.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2aAI\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5ba1\u67e5\u9ad8\u5ea6\u7ed3\u6784\u5316\u7684\u4f01\u4e1a\u6587\u6863\uff0c\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u8d85\u8d8a\u4eba\u7c7b\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5bf9\u975e\u7ed3\u6784\u5316\u6587\u672c\u6216\u6709\u9650\u5408\u89c4\u6027\u68c0\u67e5\u7684\u5173\u6ce8\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4f01\u4e1a\u6587\u6863\u5bf9\u51c6\u786e\u6027\u3001\u4e00\u81f4\u6027\u3001\u5b8c\u6574\u6027\u548c\u6e05\u6670\u5ea6\u7684\u9010\u8282\u8be6\u7ec6\u8bc4\u4f30\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5229\u7528LangChain\u3001CrewAI\u3001TruLens\u3001Guidance\u7b49\u5de5\u5177\uff0c\u901a\u8fc7\u4e13\u4e1aAI\u667a\u80fd\u4f53\u5bf9\u6587\u6863\u8fdb\u884c\u9010\u8282\u8bc4\u4f30\u3002\u8bc4\u4f30\u8f93\u51fa\u6807\u51c6\u5316\u4e3a\u673a\u5668\u53ef\u8bfb\u6a21\u5f0f\uff0c\u5e76\u8bbe\u6709\u6301\u7eed\u76d1\u63a7\u548c\u4eba\u5de5\u53cd\u9988\u5faa\u73af\u4ee5\u8fed\u4ee3\u6539\u8fdb\u548c\u51cf\u5c11\u504f\u89c1\u3002", "result": "\u5b9a\u91cf\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5728\u4fe1\u606f\u4e00\u81f4\u6027\u4e0a\u8fbe\u523099%\uff08\u4eba\u5de5\u4e3a92%\uff09\uff0c\u9519\u8bef\u7387\u548c\u504f\u5dee\u7387\u51cf\u534a\uff0c\u5e73\u5747\u5ba1\u67e5\u65f6\u95f4\u4ece30\u5206\u949f\u7f29\u77ed\u81f32.5\u5206\u949f\uff0cAI\u4e0e\u4e13\u5bb6\u4eba\u5de5\u5224\u65ad\u4e00\u81f4\u6027\u8fbe95%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u4f01\u4e1a\u6587\u6863\u8d28\u91cf\u4fdd\u8bc1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u53ef\u5ba1\u8ba1\u548c\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u5728\u5173\u952e\u9886\u57df\u63a5\u8fd1\u6216\u8d85\u8d8a\u4eba\u7c7b\u8868\u73b0\u3002\u4f46\u4ecd\u9700\u8003\u8651\u9ad8\u5ea6\u4e13\u4e1a\u9886\u57df\u7684\u4eba\u5de5\u76d1\u7763\u548c\u5927\u89c4\u6a21LLM\u4f7f\u7528\u7684\u8fd0\u8425\u6210\u672c\u3002"}}
{"id": "2506.22486", "pdf": "https://arxiv.org/pdf/2506.22486", "abs": "https://arxiv.org/abs/2506.22486", "authors": ["Ming Cheung"], "title": "Hallucination Detection with Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Since the introduction of ChatGPT, large language models (LLMs) have\ndemonstrated significant utility in various tasks, such as answering questions\nthrough retrieval-augmented generation. Context can be retrieved using a\nvectorized database, serving as a foundation for LLMs to generate responses.\nHowever, hallucinations in responses can undermine the reliability of LLMs in\npractical applications, and they are not easily detectable in the absence of\nground truth, particularly in question-and-answer scenarios. This paper\nproposes a framework that integrates multiple small language models to verify\nresponses generated by LLMs using the retrieved context from a vectorized\ndatabase. By breaking down the responses into individual sentences and\nutilizing the probability of generating \"Yes\" tokens from the outputs of\nmultiple models for a given set of questions, responses, and relevant context,\nhallucinations can be detected. The proposed framework is validated through\nexperiments with real datasets comprising over 100 sets of questions, answers,\nand contexts, including responses with fully and partially correct sentences.\nThe results demonstrate a 10\\% improvement in F1 scores for detecting correct\nresponses compared to hallucinations, indicating that multiple small language\nmodels can be effectively employed for answer verification, providing a\nscalable and efficient solution for both academic and practical applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5229\u7528\u591a\u4e2a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u548c\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u6765\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7b54\u6848\uff0c\u901a\u8fc7\u5206\u6790SLM\u751f\u6210\u201cYes\u201d\u6807\u8bb0\u7684\u6982\u7387\u6765\u68c0\u6d4b\u5e7b\u89c9\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aF1\u5206\u6570\u63d0\u9ad810%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u6548\u7528\uff0c\u4f46\u5176\u54cd\u5e94\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u524a\u5f31\u4e86\u53ef\u9760\u6027\uff0c\u4e14\u5728\u7f3a\u4e4f\u771f\u5b9e\u7b54\u6848\u65f6\u96be\u4ee5\u68c0\u6d4b\uff0c\u5c24\u5176\u662f\u5728\u95ee\u7b54\u573a\u666f\u4e2d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u591a\u4e2a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u3002\u901a\u8fc7\u5c06LLM\u7684\u54cd\u5e94\u5206\u89e3\u4e3a\u5355\u72ec\u7684\u53e5\u5b50\uff0c\u5e76\u5229\u7528\u591a\u4e2aSLM\u5bf9\u7ed9\u5b9a\u95ee\u9898\u3001\u54cd\u5e94\u548c\u76f8\u5173\u4e0a\u4e0b\u6587\u751f\u6210\u201cYes\u201d\u6807\u8bb0\u7684\u6982\u7387\u6765\u68c0\u6d4b\u5e7b\u89c9\u3002", "result": "\u901a\u8fc7\u5bf9\u5305\u542b100\u591a\u7ec4\u95ee\u9898\u3001\u7b54\u6848\u548c\u4e0a\u4e0b\u6587\u7684\u771f\u5b9e\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u5e7b\u89c9\u76f8\u6bd4\uff0c\u68c0\u6d4b\u6b63\u786e\u54cd\u5e94\u7684F1\u5206\u6570\u63d0\u9ad8\u4e8610%\u3002", "conclusion": "\u591a\u4e2a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u7b54\u6848\u9a8c\u8bc1\uff0c\u4e3a\u5b66\u672f\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22464", "pdf": "https://arxiv.org/pdf/2506.22464", "abs": "https://arxiv.org/abs/2506.22464", "authors": ["Hitesh Mohapatra"], "title": "Golden Ratio Assisted Localization for Wireless Sensor Network", "categories": ["cs.NI", "cs.HC", "B.4"], "comment": "6", "summary": "This paper presents a novel localization algorithm for wireless sensor\nnetworks (WSNs) called Golden Ratio Localization (GRL), which leverages the\nmathematical properties of the golden ratio (phi 1.618) to optimize both node\nplacement and communication range. GRL introduces phi-based anchor node\ndeployment and hop-sensitive weighting using phi-exponents to improve\nlocalization accuracy while minimizing energy consumption. Through extensive\nsimulations conducted on a 100 m * 100 m sensor field with 100 nodes and 10\nanchors, GRL achieved an average localization error of 2.35 meters,\noutperforming DV- Hop (3.87 meters) and Centroid (4.95 meters). In terms of\nenergy efficiency, GRL reduced localization energy consumption to 1.12 microJ\nper node, compared to 1.78 microJ for DV-Hop and 1.45 microJ for Centroid.\nThese results confirm that GRL provides a more balanced and efficient\nlocalization approach, making it especially suitable for energy-constrained and\nlarge-scale WSN deployments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u9ec4\u91d1\u6bd4\u4f8b\u4f18\u5316\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\uff08WSN\uff09\u5b9a\u4f4d\u7684\u65b0\u7b97\u6cd5GRL\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u964d\u4f4e\u80fd\u8017\u3002", "method": "\u5f15\u5165\u4e86\u9ec4\u91d1\u6bd4\u4f8b\u5b9a\u4f4d\uff08GRL\uff09\u7b97\u6cd5\uff0c\u5229\u7528\u9ec4\u91d1\u6bd4\u4f8b\uff08phi\uff09\u7684\u6570\u5b66\u7279\u6027\u4f18\u5316\u8282\u70b9\u90e8\u7f72\u548c\u901a\u4fe1\u8303\u56f4\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u57fa\u4e8ephi\u503c\u7684\u951a\u8282\u70b9\u90e8\u7f72\u548c\u4f7f\u7528phi\u6307\u6570\u7684\u8df3\u6570\u654f\u611f\u52a0\u6743\u3002", "result": "\u5728100x100\u7c73\u3001100\u8282\u70b9\u300110\u951a\u70b9\u7684\u4eff\u771f\u4e2d\uff0cGRL\u7684\u5e73\u5747\u5b9a\u4f4d\u8bef\u5dee\u4e3a2.35\u7c73\uff0c\u4f18\u4e8eDV-Hop\uff083.87\u7c73\uff09\u548cCentroid\uff084.95\u7c73\uff09\u3002\u80fd\u8017\u65b9\u9762\uff0cGRL\u6bcf\u8282\u70b9\u5b9a\u4f4d\u80fd\u8017\u4e3a1.12\u5fae\u7126\u8033\uff0c\u4f4e\u4e8eDV-Hop\uff081.78\u5fae\u7126\u8033\uff09\u548cCentroid\uff081.45\u5fae\u7126\u8033\uff09\u3002", "conclusion": "GRL\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5e73\u8861\u3001\u66f4\u9ad8\u6548\u7684\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u80fd\u8017\u53d7\u9650\u548c\u5927\u89c4\u6a21\u7684\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u90e8\u7f72\u3002"}}
{"id": "2506.22491", "pdf": "https://arxiv.org/pdf/2506.22491", "abs": "https://arxiv.org/abs/2506.22491", "authors": ["Oliver Warke", "Joemon M. Jose", "Faegheh Hasibi", "Jan Breitsohl"], "title": "PromptAug: Fine-grained Conflict Classification Using Data Augmentation", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; J.4; K.4.2"], "comment": null, "summary": "Given the rise of conflicts on social media, effective classification models\nto detect harmful behaviours are essential. Following the\ngarbage-in-garbage-out maxim, machine learning performance depends heavily on\ntraining data quality. However, high-quality labelled data, especially for\nnuanced tasks like identifying conflict behaviours, is limited, expensive, and\ndifficult to obtain. Additionally, as social media platforms increasingly\nrestrict access to research data, text data augmentation is gaining attention\nas an alternative to generate training data. Augmenting conflict-related data\nposes unique challenges due to Large Language Model (LLM) guardrails that\nprevent generation of offensive content. This paper introduces PromptAug, an\ninnovative LLM-based data augmentation method. PromptAug achieves statistically\nsignificant improvements of 2% in both accuracy and F1-score on conflict and\nemotion datasets. To thoroughly evaluate PromptAug against other data\naugmentation methods we conduct a robust evaluation using extreme data scarcity\nscenarios, quantitative diversity analysis and a qualitative thematic analysis.\nThe thematic analysis identifies four problematic patterns in augmented text:\nLinguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and\nAugmented Content Misinterpretation.\n  Overall, this work presents PromptAug as an effective method for augmenting\ndata in sensitive tasks like conflict detection, offering a unique,\ninterdisciplinary evaluation grounded in both natural language processing and\nsocial science methodology.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPromptAug\uff0c\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u793e\u4ea4\u5a92\u4f53\u654f\u611f\u5185\u5bb9\uff08\u5982\u51b2\u7a81\u884c\u4e3a\uff09\u7684\u68c0\u6d4b\uff0c\u5e76\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u901a\u8fc7\u8de8\u5b66\u79d1\u8bc4\u4f30\u63ed\u793a\u4e86\u589e\u5f3a\u6587\u672c\u4e2d\u7684\u6f5c\u5728\u95ee\u9898\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u51b2\u7a81\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u7528\u4e8e\u68c0\u6d4b\u6709\u5bb3\u884c\u4e3a\u7684\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3001\u6602\u8d35\u4e14\u96be\u4ee5\u83b7\u53d6\u3002\u6b64\u5916\uff0c\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u9650\u5236\u6570\u636e\u8bbf\u95ee\uff0c\u4f7f\u6570\u636e\u589e\u5f3a\u6210\u4e3a\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0cLLM\u7684\u5b89\u5168\u9632\u62a4\u673a\u5236\u9650\u5236\u4e86\u654f\u611f\u5185\u5bb9\u7684\u751f\u6210\uff0c\u5bf9\u51b2\u7a81\u76f8\u5173\u6570\u636e\u589e\u5f3a\u6784\u6210\u72ec\u7279\u6311\u6218\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3aPromptAug\u7684\u521b\u65b0\u6027\u57fa\u4e8eLLM\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002\u4e3a\u5168\u9762\u8bc4\u4f30\uff0c\u7814\u7a76\u91c7\u7528\u4e86\u9c81\u68d2\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5305\u62ec\u6781\u7aef\u6570\u636e\u7a00\u7f3a\u573a\u666f\u3001\u5b9a\u91cf\u591a\u6837\u6027\u5206\u6790\u548c\u5b9a\u6027\u4e3b\u9898\u5206\u6790\u3002", "result": "PromptAug\u5728\u51b2\u7a81\u548c\u60c5\u611f\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u51c6\u786e\u7387\u548cF1\u5206\u6570\u5747\u5b9e\u73b02%\u7684\u7edf\u8ba1\u663e\u8457\u6027\u63d0\u5347\u3002\u5b9a\u6027\u4e3b\u9898\u5206\u6790\u63ed\u793a\u4e86\u589e\u5f3a\u6587\u672c\u4e2d\u7684\u56db\u79cd\u95ee\u9898\u6a21\u5f0f\uff1a\u8bed\u8a00\u6d41\u7545\u6027\u3001\u5e7d\u9ed8\u6b67\u4e49\u3001\u589e\u5f3a\u5185\u5bb9\u6b67\u4e49\u548c\u589e\u5f3a\u5185\u5bb9\u8bef\u89e3\u3002", "conclusion": "PromptAug\u662f\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u51b2\u7a81\u68c0\u6d4b\u7b49\u654f\u611f\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u793e\u4f1a\u79d1\u5b66\u65b9\u6cd5\u7684\u72ec\u7279\u8de8\u5b66\u79d1\u8bc4\u4f30\u3002"}}
{"id": "2506.22470", "pdf": "https://arxiv.org/pdf/2506.22470", "abs": "https://arxiv.org/abs/2506.22470", "authors": ["Liang Chen", "Yu Song", "Kanglian Zhao", "Juan A. Fraire", "Wenfeng Li"], "title": "Reliable Transmission of LTP Using Reinforcement Learning-Based Adaptive FEC", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": "15 pages, 30 figures, Liang Chen and Yu Song are co-first authors", "summary": "Delay/Disruption Tolerant Networking (DTN) employs the Licklider Transmission\nProtocol (LTP) with Automatic Repeat reQuest (ARQ) for reliable data delivery\nin challenging interplanetary networks. While previous studies have integrated\npacket-level Forward Erasure Correction (FEC) into LTP to reduce retransmission\ntime costs, existing static and delay-feedback-based dynamic coding methods\nstruggle with highly variable and unpredictable deep space channel conditions.\nThis paper proposes a reinforcement learning (RL)-based adaptive FEC algorithm\nto address these limitations. The algorithm utilizes historical feedback and\nsystem state to predict future channel conditions and proactively adjust the\ncode rate. This approach aims to anticipate channel quality degradation,\nthereby preventing decoding failures and subsequent LTP retransmissions and\nimproving coding efficiency by minimizing redundancy during favorable channel\nconditions. Performance evaluations conducted in simulated Earth-Moon and\nEarth-Mars link scenarios demonstrate this algorithm's effectiveness in\noptimizing data transmission for interplanetary networks. Compared to existing\nmethods, this approach demonstrates significant improvement, with matrix\ndecoding failures reduced by at least 2/3.", "AI": {"tldr": "\u9488\u5bf9\u661f\u9645\u7f51\u7edc\u4e2dDTN\u7684LTP\u534f\u8bae\uff0c\u4f20\u7edfFEC\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u591a\u53d8\u4fe1\u9053\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94FEC\u7b97\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u4fe1\u9053\u72b6\u51b5\u5e76\u4e3b\u52a8\u8c03\u6574\u7801\u7387\uff0c\u663e\u8457\u51cf\u5c11\u89e3\u7801\u5931\u8d25\u548c\u91cd\u4f20\uff0c\u63d0\u9ad8\u6570\u636e\u4f20\u8f93\u6548\u7387\u3002", "motivation": "\u5728\u6df1\u7a7a\u901a\u4fe1\u4e2d\uff0cDTN\u7684LTP\u534f\u8bae\u9762\u4e34\u4fe1\u9053\u6761\u4ef6\u9ad8\u5ea6\u53ef\u53d8\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u9759\u6001\u6216\u5ef6\u8fdf\u53cd\u9988\u578bFEC\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u9002\u5e94\u8fd9\u4e9b\u53d8\u5316\uff0c\u5bfc\u81f4\u9ad8\u91cd\u4f20\u7387\u548c\u4f4e\u4f20\u8f93\u6548\u7387\uff0c\u5f71\u54cd\u6570\u636e\u53ef\u9760\u4f20\u8f93\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u81ea\u9002\u5e94FEC\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u5229\u7528\u5386\u53f2\u53cd\u9988\u548c\u7cfb\u7edf\u72b6\u6001\u9884\u6d4b\u672a\u6765\u7684\u4fe1\u9053\u6761\u4ef6\uff0c\u5e76\u636e\u6b64\u4e3b\u52a8\u8c03\u6574\u7f16\u7801\u7387\uff0c\u65e8\u5728\u9884\u5224\u4fe1\u9053\u8d28\u91cf\u4e0b\u964d\uff0c\u4ece\u800c\u907f\u514d\u89e3\u7801\u5931\u8d25\u5e76\u51cf\u5c11LTP\u91cd\u4f20\uff0c\u540c\u65f6\u5728\u4fe1\u9053\u826f\u597d\u65f6\u6700\u5c0f\u5316\u5197\u4f59\u3002", "result": "\u5728\u6a21\u62df\u7684\u5730\u7403-\u6708\u7403\u548c\u5730\u7403-\u706b\u661f\u94fe\u8def\u573a\u666f\u4e2d\u8fdb\u884c\u7684\u6027\u80fd\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u6709\u6548\u4f18\u5316\u4e86\u661f\u9645\u7f51\u7edc\u7684\u6570\u636e\u4f20\u8f93\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u77e9\u9635\u89e3\u7801\u5931\u8d25\u7387\u81f3\u5c11\u964d\u4f4e\u4e862/3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684RL\u81ea\u9002\u5e94FEC\u7b97\u6cd5\u901a\u8fc7\u5176\u524d\u77bb\u6027\u7684\u4fe1\u9053\u9884\u6d4b\u548c\u7801\u7387\u8c03\u6574\u80fd\u529b\uff0c\u6709\u6548\u5e94\u5bf9\u4e86\u6df1\u7a7a\u7f51\u7edc\u4e2d\u4fe1\u9053\u7684\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u89e3\u7801\u5931\u8d25\u548c\u91cd\u4f20\uff0c\u4ece\u800c\u4f18\u5316\u4e86\u661f\u9645\u7f51\u7edc\u7684\u6570\u636e\u4f20\u8f93\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2506.22441", "pdf": "https://arxiv.org/pdf/2506.22441", "abs": "https://arxiv.org/abs/2506.22441", "authors": ["Lei Yang"], "title": "Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Intelligent transportation systems (ITS) rely heavily on complete and\nhigh-quality spatiotemporal traffic data to achieve optimal performance.\nNevertheless, in real-word traffic data collection processes, issues such as\ncommunication failures and sensor malfunctions often lead to incomplete or\ncorrupted datasets, thereby posing significant challenges to the advancement of\nITS. Among various methods for imputing missing spatiotemporal traffic data,\nthe latent factorization of tensors (LFT) model has emerged as a widely adopted\nand effective solution. However, conventional LFT models typically employ the\nstandard L2-norm in their learning objective, which makes them vulnerable to\nthe influence of outliers. To overcome this limitation, this paper proposes a\nthreshold distance weighted (TDW) loss-incorporated Latent Factorization of\nTensors (TDWLFT) model. The proposed loss function effectively reduces the\nmodel's sensitivity to outliers by assigning differentiated weights to\nindividual samples. Extensive experiments conducted on two traffic speed\ndatasets sourced from diverse urban environments confirm that the proposed\nTDWLFT model consistently outperforms state-of-the-art approaches in terms of\nboth in both prediction accuracy and computational efficiency.", "AI": {"tldr": "\u9488\u5bf9\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7f3a\u5931\u65f6\u7a7a\u4ea4\u901a\u6570\u636e\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86TDWLFT\u6a21\u578b\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5f15\u5165\u9608\u503c\u8ddd\u79bb\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u4f20\u7edfLFT\u6a21\u578b\u5bf9\u5f02\u5e38\u503c\u7684\u654f\u611f\u6027\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08ITS\uff09\u9700\u9ad8\u8d28\u91cf\u7684\u65f6\u7a7a\u4ea4\u901a\u6570\u636e\uff0c\u4f46\u5b9e\u9645\u6570\u636e\u91c7\u96c6\u5e38\u56e0\u6545\u969c\u5bfc\u81f4\u6570\u636e\u7f3a\u5931\u6216\u635f\u574f\u3002\u73b0\u6709\u5f20\u91cf\u6f5c\u5728\u56e0\u5b50\u5206\u89e3\uff08LFT\uff09\u6a21\u578b\u867d\u6709\u6548\uff0c\u4f46\u5176L2\u8303\u6570\u76ee\u6807\u51fd\u6570\u6613\u53d7\u5f02\u5e38\u503c\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9608\u503c\u8ddd\u79bb\u52a0\u6743\uff08TDW\uff09\u635f\u5931\u51fd\u6570\u878d\u5165\u7684\u5f20\u91cf\u6f5c\u5728\u56e0\u5b50\u5206\u89e3\u6a21\u578b\uff08TDWLFT\uff09\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u5355\u4e2a\u6837\u672c\u8d4b\u4e88\u5dee\u5f02\u5316\u6743\u91cd\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u6a21\u578b\u5bf9\u5f02\u5e38\u503c\u7684\u654f\u611f\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u6765\u81ea\u4e0d\u540c\u57ce\u5e02\u73af\u5883\u7684\u4ea4\u901a\u901f\u5ea6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684TDWLFT\u6a21\u578b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "TDWLFT\u6a21\u578b\u901a\u8fc7\u6709\u6548\u5904\u7406\u5f02\u5e38\u503c\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u7f3a\u5931\u65f6\u7a7a\u4ea4\u901a\u6570\u636e\u8865\u5168\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9c81\u68d2\u3001\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22604", "pdf": "https://arxiv.org/pdf/2506.22604", "abs": "https://arxiv.org/abs/2506.22604", "authors": ["David Porfirio", "Vincent Hsiao", "Morgan Fine-Morris", "Leslie Smith", "Laura M. Hiatt"], "title": "Bootstrapping Human-Like Planning via LLMs", "categories": ["cs.AI", "cs.HC", "cs.RO"], "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "summary": "Robot end users increasingly require accessible means of specifying tasks for\nrobots to perform. Two common end-user programming paradigms include\ndrag-and-drop interfaces and natural language programming. Although natural\nlanguage interfaces harness an intuitive form of human communication,\ndrag-and-drop interfaces enable users to meticulously and precisely dictate the\nkey actions of the robot's task. In this paper, we investigate the degree to\nwhich both approaches can be combined. Specifically, we construct a large\nlanguage model (LLM)-based pipeline that accepts natural language as input and\nproduces human-like action sequences as output, specified at a level of\ngranularity that a human would produce. We then compare these generated action\nsequences to another dataset of hand-specified action sequences. Although our\nresults reveal that larger models tend to outperform smaller ones in the\nproduction of human-like action sequences, smaller models nonetheless achieve\nsatisfactory performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u548c\u62d6\u653e\u754c\u9762\u6765\u6307\u5b9a\u673a\u5668\u4eba\u4efb\u52a1\uff0c\u901a\u8fc7\u6784\u5efa\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7ba1\u9053\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u4eba\u7c7b\u7c92\u5ea6\u7ea7\u522b\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u7ed3\u679c\u8868\u660e\u5927\u578b\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u5c0f\u578b\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u6ee1\u610f\u6548\u679c\u3002", "motivation": "\u673a\u5668\u4eba\u7ec8\u7aef\u7528\u6237\u9700\u8981\u66f4\u6613\u4e8e\u8bbf\u95ee\u7684\u4efb\u52a1\u6307\u5b9a\u65b9\u5f0f\u3002\u73b0\u6709\u7684\u81ea\u7136\u8bed\u8a00\u7f16\u7a0b\u76f4\u89c2\u4f46\u53ef\u80fd\u7f3a\u4e4f\u7cbe\u5ea6\uff0c\u800c\u62d6\u653e\u754c\u9762\u7cbe\u786e\u4f46\u53ef\u80fd\u4e0d\u591f\u76f4\u89c2\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5982\u4f55\u7ed3\u5408\u8fd9\u4e24\u79cd\u8303\u5f0f\uff0c\u4ee5\u5e73\u8861\u76f4\u89c2\u6027\u4e0e\u7cbe\u5ea6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u63a5\u53d7\u81ea\u7136\u8bed\u8a00\u8f93\u5165\uff0c\u5e76\u8f93\u51fa\u4eba\u7c7b\u7c92\u5ea6\u7ea7\u522b\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u5e8f\u5217\u3002\u968f\u540e\uff0c\u5c06\u8fd9\u4e9b\u751f\u6210\u7684\u52a8\u4f5c\u5e8f\u5217\u4e0e\u53e6\u4e00\u7ec4\u624b\u52a8\u6307\u5b9a\u7684\u52a8\u4f5c\u5e8f\u5217\u6570\u636e\u96c6\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5728\u751f\u6210\u7c7b\u4eba\u52a8\u4f5c\u5e8f\u5217\u65b9\u9762\uff0c\u5927\u578b\u6a21\u578b\u901a\u5e38\u4f18\u4e8e\u5c0f\u578b\u6a21\u578b\uff0c\u4f46\u5c0f\u578b\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u4ee4\u4eba\u6ee1\u610f\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7LLM\u7ba1\u9053\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u5316\u4e3a\u7cbe\u7ec6\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u5e8f\u5217\u662f\u53ef\u884c\u7684\uff0c\u4e14\u5373\u4f7f\u662f\u8f83\u5c0f\u7684LLM\u4e5f\u80fd\u63d0\u4f9b\u4ee4\u4eba\u6ee1\u610f\u7684\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u4efb\u52a1\u6307\u5b9a\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u5408\u76f4\u89c2\u6027\u548c\u7cbe\u786e\u6027\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2506.22437", "pdf": "https://arxiv.org/pdf/2506.22437", "abs": "https://arxiv.org/abs/2506.22437", "authors": ["Xinxin Sun", "Peter Chang"], "title": "Robust Perspective Correction for Real-World Crack Evolution Tracking in Image-Based Structural Health Monitoring", "categories": ["cs.CV", "68T45 (Computer Vision)"], "comment": "43 pages, 5 figures, 19 tables. Submitted to NDT&E International.\n  This work may also be of interest to researchers in optical NDE and civil\n  engineering SHM", "summary": "Accurate image alignment is essential for monitoring crack evolution in\nstructural health monitoring (SHM), particularly under real-world conditions\ninvolving perspective distortion, occlusion, and low contrast. However,\ntraditional feature detectors such as SIFT and SURF, which rely on\nGaussian-based scale spaces, tend to suppress high-frequency edges, making them\nunsuitable for thin crack localization. Lightweight binary alternatives like\nORB and BRISK, while computationally efficient, often suffer from poor keypoint\nrepeatability on textured or shadowed surfaces. This study presents a\nphysics-informed alignment framework that adapts the open KAZE architecture to\nSHM-specific challenges. By utilizing nonlinear anisotropic diffusion to\nconstruct a crack-preserving scale space, and integrating RANSAC-based\nhomography estimation, the framework enables accurate geometric correction\nwithout the need for training, parameter tuning, or prior calibration. The\nmethod is validated on time-lapse images of masonry and concrete acquired via\nhandheld smartphone under varied field conditions, including shadow\ninterference, cropping, oblique viewing angles, and surface clutter. Compared\nto classical detectors, the proposed framework reduces crack area and spine\nlength errors by up to 70 percent and 90 percent, respectively, while\nmaintaining sub-5 percent alignment error in key metrics. Unsupervised,\ninterpretable, and computationally lightweight, this approach supports scalable\ndeployment via UAVs and mobile platforms. By tailoring nonlinear scale-space\nmodeling to SHM image alignment, this work offers a robust and physically\ngrounded alternative to conventional techniques for tracking real-world crack\nevolution.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u5e76\u9002\u5e94SHM\u573a\u666f\u7684\u56fe\u50cf\u5bf9\u9f50\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u975e\u7ebf\u6027\u5404\u5411\u5f02\u6027\u6269\u6563\u6784\u5efa\u88c2\u7f1d\u4fdd\u7559\u5c3a\u5ea6\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u771f\u5b9e\u6761\u4ef6\u4e0b\u5bf9\u7ed3\u6784\u88c2\u7f1d\u6f14\u5316\u7684\u7cbe\u786e\u76d1\u6d4b\u3002", "motivation": "\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b(SHM)\u4e2d\uff0c\u51c6\u786e\u7684\u56fe\u50cf\u5bf9\u9f50\u5bf9\u4e8e\u76d1\u6d4b\u88c2\u7f1d\u6f14\u5316\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u900f\u89c6\u7578\u53d8\u3001\u906e\u6321\u548c\u4f4e\u5bf9\u6bd4\u5ea6\u7b49\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u3002\u4f20\u7edf\u7279\u5f81\u68c0\u6d4b\u5668\u4e0d\u9002\u7528\u4e8e\u8584\u88c2\u7f1d\u5b9a\u4f4d\uff0c\u800c\u8f7b\u91cf\u7ea7\u4e8c\u8fdb\u5236\u7b97\u6cd5\u5728\u7eb9\u7406\u6216\u9634\u5f71\u8868\u9762\u4e0a\u5173\u952e\u70b9\u91cd\u590d\u6027\u5dee\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u9020\u5f00\u653eKAZE\u67b6\u6784\u4ee5\u9002\u5e94SHM\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u975e\u7ebf\u6027\u5404\u5411\u5f02\u6027\u6269\u6563\u6784\u5efa\u88c2\u7f1d\u4fdd\u7559\u5c3a\u5ea6\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408RANSAC\u8fdb\u884c\u5355\u5e94\u6027\u4f30\u8ba1\uff0c\u65e0\u9700\u8bad\u7ec3\u3001\u53c2\u6570\u8c03\u6574\u6216\u9884\u5148\u6821\u51c6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7816\u77f3\u548c\u6df7\u51dd\u571f\u7684\u624b\u6301\u667a\u80fd\u624b\u673a\u5ef6\u65f6\u56fe\u50cf\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5728\u9634\u5f71\u5e72\u6270\u3001\u88c1\u526a\u3001\u503e\u659c\u89c6\u89d2\u548c\u8868\u9762\u6742\u6ce2\u7b49\u591a\u79cd\u73b0\u573a\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\u3002\u4e0e\u4f20\u7edf\u68c0\u6d4b\u5668\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5c06\u88c2\u7f1d\u9762\u79ef\u548c\u9aa8\u67b6\u957f\u5ea6\u8bef\u5dee\u5206\u522b\u964d\u4f4e\u4e8670%\u548c90%\uff0c\u5e76\u4fdd\u6301\u5173\u952e\u6307\u6807\u7684\u5bf9\u9f50\u8bef\u5dee\u4f4e\u4e8e5%\u3002\u8be5\u65b9\u6cd5\u65e0\u76d1\u7763\u3001\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u8f7b\u91cf\u3002", "conclusion": "\u901a\u8fc7\u5c06\u975e\u7ebf\u6027\u5c3a\u5ea6\u7a7a\u95f4\u5efa\u6a21\u5e94\u7528\u4e8eSHM\u56fe\u50cf\u5bf9\u9f50\uff0c\u672c\u5de5\u4f5c\u4e3a\u8ddf\u8e2a\u771f\u5b9e\u4e16\u754c\u88c2\u7f1d\u6f14\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u4e14\u57fa\u4e8e\u7269\u7406\u7684\u66ff\u4ee3\u4f20\u7edf\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u901a\u8fc7\u65e0\u4eba\u673a\u548c\u79fb\u52a8\u5e73\u53f0\u8fdb\u884c\u53ef\u6269\u5c55\u90e8\u7f72\u3002"}}
{"id": "2506.22508", "pdf": "https://arxiv.org/pdf/2506.22508", "abs": "https://arxiv.org/abs/2506.22508", "authors": ["Chenyang Shao", "Tianxing Li", "Chenhao Pu", "Fengli Xu", "Yong Li"], "title": "AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text", "categories": ["cs.CL", "cs.AI"], "comment": "This work has been submitted to NeurIPS 2025. Under review", "summary": "In today's digital world, casual user-generated content often contains subtle\ncues that may inadvertently expose sensitive personal attributes. Such risks\nunderscore the growing importance of effective text anonymization to safeguard\nindividual privacy. However, existing methods either rely on rigid replacements\nthat damage utility or cloud-based LLMs that are costly and pose privacy risks.\nTo address these issues, we explore the use of locally deployed smaller-scale\nlanguage models (SLMs) for anonymization. Yet training effective SLMs remains\nchallenging due to limited high-quality supervision. To address the challenge,\nwe propose AgentStealth, a self-reinforcing LLM anonymization framework.First,\nwe introduce an adversarial anonymization workflow enhanced by In-context\nContrastive Learning and Adaptive Utility-Aware Control. Second, we perform\nsupervised adaptation of SLMs using high-quality data collected from the\nworkflow, which includes both anonymization and attack signals. Finally, we\napply online reinforcement learning where the model leverages its internal\nadversarial feedback to iteratively improve anonymization performance.\nExperiments on two datasets show that our method outperforms baselines in both\nanonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight\ndesign supports direct deployment on edge devices, avoiding cloud reliance and\ncommunication-based privacy risks. Our code is open-source at\nhttps://github.com/tsinghua-fib-lab/AgentStealth.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAgentStealth\uff0c\u4e00\u4e2a\u81ea\u5f3a\u5316LLM\u533f\u540d\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u5de5\u4f5c\u6d41\u3001\u76d1\u7763\u9002\u5e94\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u672c\u5730\u90e8\u7f72\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5b9e\u73b0\u9ad8\u6548\u4e14\u4fdd\u62a4\u6548\u7528\u7684\u6587\u672c\u533f\u540d\u5316\u3002", "motivation": "\u7528\u6237\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u654f\u611f\u4fe1\u606f\u6cc4\u9732\u65e5\u76ca\u4e25\u91cd\uff0c\u73b0\u6709\u533f\u540d\u5316\u65b9\u6cd5\u5b58\u5728\u95ee\u9898\uff1a\u786c\u6027\u66ff\u6362\u635f\u5bb3\u6587\u672c\u6548\u7528\uff1b\u57fa\u4e8e\u4e91\u7684LLM\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u9690\u79c1\u98ce\u9669\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\u6709\u9650\uff0c\u8bad\u7ec3\u6709\u6548\u7684\u672c\u5730SLM\u9762\u4e34\u6311\u6218\u3002", "method": "AgentStealth\u6846\u67b6\u5206\u4e09\u6b65\uff1a1) \u5f15\u5165\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5bf9\u6bd4\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u6548\u7528\u63a7\u5236\u7684\u5bf9\u6297\u6027\u533f\u540d\u5316\u5de5\u4f5c\u6d41\u30022) \u5229\u7528\u8be5\u5de5\u4f5c\u6d41\u6536\u96c6\u7684\u9ad8\u8d28\u91cf\u6570\u636e\uff08\u5305\u542b\u533f\u540d\u5316\u548c\u653b\u51fb\u4fe1\u53f7\uff09\u5bf9SLM\u8fdb\u884c\u76d1\u7763\u9002\u5e94\u30023) \u5e94\u7528\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u6a21\u578b\u5229\u7528\u5185\u90e8\u5bf9\u6297\u53cd\u9988\u8fed\u4ee3\u63d0\u5347\u533f\u540d\u5316\u6027\u80fd\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cAgentStealth\u5728\u533f\u540d\u5316\u6709\u6548\u6027\u4e0a\u8d85\u8d8a\u57fa\u7ebf12.3%\uff0c\u5728\u6548\u7528\u4e0a\u63d0\u53476.8%\u3002\u5176\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u652f\u6301\u8fb9\u7f18\u8bbe\u5907\u76f4\u63a5\u90e8\u7f72\uff0c\u907f\u514d\u4e86\u5bf9\u4e91\u670d\u52a1\u7684\u4f9d\u8d56\u548c\u901a\u4fe1\u9690\u79c1\u98ce\u9669\u3002", "conclusion": "AgentStealth\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u3001\u9ad8\u6548\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u6587\u672c\u533f\u540d\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u672c\u5730\u90e8\u7f72\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u89c4\u907f\u4e91\u670d\u52a1\u7684\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2506.22474", "pdf": "https://arxiv.org/pdf/2506.22474", "abs": "https://arxiv.org/abs/2506.22474", "authors": ["Ziad Qais Al Abbasi", "Khaled M. Rabie", "Senior Member", "Xingwang Li", "Senior Member", "Wali Ullah Khan", "Asma Abu Samah"], "title": "RL-based Adaptive Task Offloading in Mobile-Edge Computing for Future IoT Networks", "categories": ["cs.NI", "cs.SY", "eess.SY", "C.2 COMPUTER-COMMUNICATION NETWORKS"], "comment": "7 pages", "summary": "The Internet of Things (IoT) has been increasingly used in our everyday lives\nas well as in numerous industrial applications. However, due to limitations in\ncomputing and power capabilities, IoT devices need to send their respective\ntasks to cloud service stations that are usually located at far distances.\nHaving to transmit data far distances introduces challenges for services that\nrequire low latency such as industrial control in factories and plants as well\nas artificial intelligence assisted autonomous driving. To solve this issue,\nmobile edge computing (MEC) is deployed at the networks edge to reduce\ntransmission time. In this regard, this study proposes a new offloading scheme\nfor MEC-assisted ultra dense cellular networks using reinforcement learning\n(RL) techniques. The proposed scheme enables efficient resource allocation and\ndynamic offloading decisions based on varying network conditions and user\ndemands. The RL algorithm learns from the networks historical data and adapts\nthe offloading decisions to optimize the networks overall performance.\nNon-orthogonal multiple access is also adopted to improve resource utilization\namong the IoT devices. Simulation results demonstrate that the proposed scheme\noutperforms other stateof the art offloading algorithms in terms of energy\nefficiency, network throughput, and user satisfaction.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u5378\u8f7d\u65b9\u6848\uff0c\u65e8\u5728\u4f18\u5316\u8d85\u5bc6\u96c6\u8702\u7a9d\u7f51\u7edc\u4e2d\u7269\u8054\u7f51\uff08IoT\uff09\u8bbe\u5907\u7684\u8d44\u6e90\u5206\u914d\u548c\u52a8\u6001\u4efb\u52a1\u5378\u8f7d\uff0c\u4ee5\u89e3\u51b3\u8fdc\u8ddd\u79bb\u6570\u636e\u4f20\u8f93\u5e26\u6765\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "\u7269\u8054\u7f51\uff08IoT\uff09\u8bbe\u5907\u56e0\u8ba1\u7b97\u548c\u7535\u6e90\u9650\u5236\uff0c\u9700\u5c06\u4efb\u52a1\u53d1\u9001\u81f3\u8fdc\u8ddd\u79bb\u4e91\u670d\u52a1\u7ad9\uff0c\u5bfc\u81f4\u5bf9\u4f4e\u5ef6\u8fdf\u670d\u52a1\uff08\u5982\u5de5\u4e1a\u63a7\u5236\u3001\u81ea\u52a8\u9a7e\u9a76\uff09\u6784\u6210\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9MEC\u8f85\u52a9\u7684\u8d85\u5bc6\u96c6\u8702\u7a9d\u7f51\u7edc\u7684\u65b0\u578b\u5378\u8f7d\u65b9\u6848\u3002\u8be5\u65b9\u6848\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6280\u672f\uff0c\u6839\u636e\u7f51\u7edc\u72b6\u51b5\u548c\u7528\u6237\u9700\u6c42\u5b9e\u73b0\u9ad8\u6548\u8d44\u6e90\u5206\u914d\u548c\u52a8\u6001\u5378\u8f7d\u51b3\u7b56\u3002RL\u7b97\u6cd5\u901a\u8fc7\u5b66\u4e60\u5386\u53f2\u6570\u636e\u4f18\u5316\u7f51\u7edc\u6574\u4f53\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u975e\u6b63\u4ea4\u591a\u5740\u63a5\u5165\uff08NOMA\uff09\u4ee5\u63d0\u9ad8IoT\u8bbe\u5907\u7684\u8d44\u6e90\u5229\u7528\u7387\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6848\u5728\u80fd\u6548\u3001\u7f51\u7edc\u541e\u5410\u91cf\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u5378\u8f7d\u7b97\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684MEC\u5378\u8f7d\u65b9\u6848\u80fd\u6709\u6548\u89e3\u51b3\u7269\u8054\u7f51\u8bbe\u5907\u4efb\u52a1\u5378\u8f7d\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8d85\u5bc6\u96c6\u8702\u7a9d\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5bf9\u5ef6\u8fdf\u654f\u611f\u7684\u7269\u8054\u7f51\u5e94\u7528\u3002"}}
{"id": "2506.22442", "pdf": "https://arxiv.org/pdf/2506.22442", "abs": "https://arxiv.org/abs/2506.22442", "authors": ["Piotr Makarevich"], "title": "Features-based embedding or Feature-grounding", "categories": ["cs.LG"], "comment": "13 pages, 12 figures", "summary": "In everyday reasoning, when we think about a particular object, we associate\nit with a unique set of expected properties such as weight, size, or more\nabstract attributes like density or horsepower. These expectations are shaped\nby our prior knowledge and the conceptual categories we have formed through\nexperience. This paper investigates how such knowledge-based structured\nthinking can be reproduced in deep learning models using features based\nembeddings. Specially, it introduces an specific approach to build\nfeature-grounded embedding, aiming to align shareable representations of\noperable dictionary with interpretable domain-specific conceptual features.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u7279\u5f81\u5d4c\u5165\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u590d\u73b0\u4eba\u7c7b\u57fa\u4e8e\u77e5\u8bc6\u7684\u7ed3\u6784\u5316\u601d\u7ef4\u3002", "motivation": "\u4eba\u7c7b\u5728\u65e5\u5e38\u63a8\u7406\u4e2d\uff0c\u4f1a\u6839\u636e\u5148\u9a8c\u77e5\u8bc6\u548c\u6982\u5ff5\u7c7b\u522b\u5c06\u7279\u5b9a\u7269\u4f53\u4e0e\u4e00\u7ec4\u72ec\u7279\u7684\u9884\u671f\u5c5e\u6027\u76f8\u5173\u8054\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5c06\u8fd9\u79cd\u77e5\u8bc6\u9a71\u52a8\u7684\u7ed3\u6784\u5316\u601d\u7ef4\u590d\u73b0\u5230\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u6784\u5efa\u7279\u5f81\u63a5\u5730\u5d4c\u5165\uff08feature-grounded embedding\uff09\u7684\u7279\u5b9a\u65b9\u6cd5\uff0c\u65e8\u5728\u5c06\u53ef\u64cd\u4f5c\u8bcd\u5178\u7684\u53ef\u5171\u4eab\u8868\u793a\u4e0e\u53ef\u89e3\u91ca\u7684\u7279\u5b9a\u9886\u57df\u6982\u5ff5\u7279\u5f81\u5bf9\u9f50\u3002", "result": "\u6458\u8981\u4e2d\u672a\u660e\u786e\u63d0\u53ca\u7814\u7a76\u7ed3\u679c\u3002", "conclusion": "\u6458\u8981\u4e2d\u672a\u660e\u786e\u63d0\u53ca\u7814\u7a76\u7ed3\u8bba\u3002"}}
{"id": "2506.22609", "pdf": "https://arxiv.org/pdf/2506.22609", "abs": "https://arxiv.org/abs/2506.22609", "authors": ["Graham Todd", "Alexander G. Padula", "Dennis J. N. J. Soemers", "Julian Togelius"], "title": "Ludax: A GPU-Accelerated Domain Specific Language for Board Games", "categories": ["cs.AI"], "comment": "18 pages, 3 figures", "summary": "Games have long been used as benchmarks and testing environments for research\nin artificial intelligence. A key step in supporting this research was the\ndevelopment of game description languages: frameworks that compile\ndomain-specific code into playable and simulatable game environments, allowing\nresearchers to generalize their algorithms and approaches across multiple games\nwithout having to manually implement each one. More recently, progress in\nreinforcement learning (RL) has been largely driven by advances in hardware\nacceleration. Libraries like JAX allow practitioners to take full advantage of\ncutting-edge computing hardware, often speeding up training and testing by\norders of magnitude. Here, we present a synthesis of these strands of research:\na domain-specific language for board games which automatically compiles into\nhardware-accelerated code. Our framework, Ludax, combines the generality of\ngame description languages with the speed of modern parallel processing\nhardware and is designed to fit neatly into existing deep learning pipelines.\nWe envision Ludax as a tool to help accelerate games research generally, from\nRL to cognitive science, by enabling rapid simulation and providing a flexible\nrepresentation scheme. We present a detailed breakdown of Ludax's description\nlanguage and technical notes on the compilation process, along with speed\nbenchmarking and a demonstration of training RL agents. The Ludax framework,\nalong with implementations of existing board games, is open-source and freely\navailable.", "AI": {"tldr": "Ludax\u662f\u4e00\u4e2a\u5c06\u68cb\u76d8\u6e38\u620f\u63cf\u8ff0\u8bed\u8a00\u7f16\u8bd1\u4e3a\u786c\u4ef6\u52a0\u901f\u4ee3\u7801\u7684\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u901a\u7528\u6027\u548c\u901f\u5ea6\uff0c\u65e8\u5728\u52a0\u901fAI\u6e38\u620f\u7814\u7a76\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u957f\u671f\u4f7f\u7528\u6e38\u620f\u4f5c\u4e3a\u57fa\u51c6\u548c\u6d4b\u8bd5\u73af\u5883\u3002\u73b0\u6709\u7684\u6e38\u620f\u63cf\u8ff0\u8bed\u8a00\u867d\u63d0\u4f9b\u4e86\u7b97\u6cd5\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u786c\u4ef6\u52a0\u901f\u5e26\u6765\u7684\u9ad8\u6548\u7387\u3002\u968f\u7740\u5f3a\u5316\u5b66\u4e60\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5bf9\u8ba1\u7b97\u901f\u5ea6\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\uff0c\u4e9f\u9700\u4e00\u4e2a\u80fd\u5c06\u6e38\u620f\u63cf\u8ff0\u8bed\u8a00\u7684\u901a\u7528\u6027\u4e0e\u73b0\u4ee3\u786c\u4ef6\u52a0\u901f\u4f18\u52bf\u76f8\u7ed3\u5408\u7684\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Ludax\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u9488\u5bf9\u68cb\u76d8\u6e38\u620f\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u80fd\u591f\u81ea\u52a8\u7f16\u8bd1\u6210\u786c\u4ef6\u52a0\u901f\u4ee3\u7801\u3002\u7814\u7a76\u4eba\u5458\u5bf9Ludax\u7684\u63cf\u8ff0\u8bed\u8a00\u548c\u7f16\u8bd1\u8fc7\u7a0b\u8fdb\u884c\u4e86\u8be6\u7ec6\u8bf4\u660e\uff0c\u5e76\u8fdb\u884c\u4e86\u901f\u5ea6\u57fa\u51c6\u6d4b\u8bd5\uff0c\u540c\u65f6\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528Ludax\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u3002", "result": "Ludax\u6210\u529f\u5730\u5c06\u6e38\u620f\u63cf\u8ff0\u8bed\u8a00\u7684\u901a\u7528\u6027\u4e0e\u73b0\u4ee3\u5e76\u884c\u5904\u7406\u786c\u4ef6\u7684\u901f\u5ea6\u7ed3\u5408\u8d77\u6765\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u6a21\u62df\u548c\u7075\u6d3b\u7684\u8868\u793a\u65b9\u6848\u3002\u901a\u8fc7\u901f\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u548c\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u8bad\u7ec3\u7684\u6f14\u793a\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002Ludax\u6846\u67b6\u53ca\u5176\u73b0\u6709\u68cb\u76d8\u6e38\u620f\u5b9e\u73b0\u5df2\u5f00\u6e90\u5e76\u514d\u8d39\u63d0\u4f9b\u3002", "conclusion": "Ludax\u88ab\u8bbe\u60f3\u4e3a\u4e00\u79cd\u901a\u8fc7\u5b9e\u73b0\u5feb\u901f\u6a21\u62df\u548c\u63d0\u4f9b\u7075\u6d3b\u8868\u793a\u65b9\u6848\u6765\u52a0\u901f\u5e7f\u4e49\u6e38\u620f\u7814\u7a76\uff08\u4ece\u5f3a\u5316\u5b66\u4e60\u5230\u8ba4\u77e5\u79d1\u5b66\uff09\u7684\u5de5\u5177\u3002"}}
{"id": "2506.22438", "pdf": "https://arxiv.org/pdf/2506.22438", "abs": "https://arxiv.org/abs/2506.22438", "authors": ["Xumin Gao", "Mark Stevens", "Grzegorz Cielniak"], "title": "Counting with Confidence: Accurate Pest Monitoring in Water Traps", "categories": ["cs.CV"], "comment": "\\c{opyright} 20XX the authors. This work has been accepted to IFAC\n  for publication under a Creative Commons Licence CC-BY-NC-ND", "summary": "Accurate pest population monitoring and tracking their dynamic changes are\ncrucial for precision agriculture decision-making. A common limitation in\nexisting vision-based automatic pest counting research is that models are\ntypically evaluated on datasets with ground truth but deployed in real-world\nscenarios without assessing the reliability of counting results due to the lack\nof ground truth. To this end, this paper proposed a method for comprehensively\nevaluating pest counting confidence in the image, based on information related\nto counting results and external environmental conditions. First, a pest\ndetection network is used for pest detection and counting, extracting counting\nresult-related information. Then, the pest images undergo image quality\nassessment, image complexity assessment, and pest distribution uniformity\nassessment. And the changes in image clarity caused by stirring during image\nacquisition are quantified by calculating the average gradient magnitude.\nNotably, we designed a hypothesis-driven multi-factor sensitivity analysis\nmethod to select the optimal image quality assessment and image complexity\nassessment methods. And we proposed an adaptive DBSCAN clustering algorithm for\npest distribution uniformity assessment. Finally, the obtained information\nrelated to counting results and external environmental conditions is input into\na regression model for prediction, resulting in the final pest counting\nconfidence. To the best of our knowledge, this is the first study dedicated to\ncomprehensively evaluating counting confidence in counting tasks, and\nquantifying the relationship between influencing factors and counting\nconfidence through a model. Experimental results show our method reduces MSE by\n31.7% and improves R2 by 15.2% on the pest counting confidence test set,\ncompared to the baseline built primarily on information related to counting\nresults.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u7efc\u5408\u8bc4\u4f30\u56fe\u50cf\u4e2d\u5bb3\u866b\u8ba1\u6570\u7f6e\u4fe1\u5ea6\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u8ba1\u6570\u7ed3\u679c\u4e0e\u5916\u90e8\u73af\u5883\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u53ef\u9760\u6027\u8bc4\u4f30\u7f3a\u5931\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u7684\u81ea\u52a8\u5bb3\u866b\u8ba1\u6570\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u56e0\u7f3a\u4e4f\u771f\u5b9e\u503c\u800c\u65e0\u6cd5\u8bc4\u4f30\u8ba1\u6570\u7ed3\u679c\u7684\u53ef\u9760\u6027\uff0c\u8fd9\u662f\u5176\u4e3b\u8981\u5c40\u9650\u6027\u3002", "method": "\u9996\u5148\u4f7f\u7528\u5bb3\u866b\u68c0\u6d4b\u7f51\u7edc\u8fdb\u884c\u8ba1\u6570\u5e76\u63d0\u53d6\u76f8\u5173\u4fe1\u606f\uff1b\u63a5\u7740\u5bf9\u5bb3\u866b\u56fe\u50cf\u8fdb\u884c\u8d28\u91cf\u3001\u590d\u6742\u5ea6\u548c\u5206\u5e03\u5747\u5300\u6027\u8bc4\u4f30\uff0c\u5e76\u91cf\u5316\u56fe\u50cf\u6e05\u6670\u5ea6\u53d8\u5316\uff1b\u521b\u65b0\u6027\u5730\u8bbe\u8ba1\u4e86\u5047\u8bbe\u9a71\u52a8\u7684\u591a\u56e0\u7d20\u654f\u611f\u6027\u5206\u6790\u6cd5\u4ee5\u9009\u62e9\u6700\u4f18\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u9002\u5e94DBSCAN\u805a\u7c7b\u7b97\u6cd5\u8bc4\u4f30\u5206\u5e03\u5747\u5300\u6027\uff1b\u6700\u540e\uff0c\u5c06\u6240\u6709\u83b7\u53d6\u7684\u4fe1\u606f\u8f93\u5165\u56de\u5f52\u6a21\u578b\u4ee5\u9884\u6d4b\u6700\u7ec8\u7684\u5bb3\u866b\u8ba1\u6570\u7f6e\u4fe1\u5ea6\u3002", "result": "\u4e0e\u4e3b\u8981\u57fa\u4e8e\u8ba1\u6570\u7ed3\u679c\u4fe1\u606f\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u5bb3\u866b\u8ba1\u6570\u7f6e\u4fe1\u5ea6\u6d4b\u8bd5\u96c6\u4e0a\u5c06\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u964d\u4f4e\u4e8631.7%\uff0cR2\u63d0\u9ad8\u4e8615.2%\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86\u8ba1\u6570\u4efb\u52a1\u4e2d\u7684\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u91cf\u5316\u4e86\u5f71\u54cd\u56e0\u7d20\u4e0e\u8ba1\u6570\u7f6e\u4fe1\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bb3\u866b\u8ba1\u6570\u7ed3\u679c\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u80fd\u529b\u3002"}}
{"id": "2506.22510", "pdf": "https://arxiv.org/pdf/2506.22510", "abs": "https://arxiv.org/abs/2506.22510", "authors": ["Zihao Zhao", "Xinlong Zhai", "Jinyu Yang", "Chuan Shi"], "title": "Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 5 figures", "summary": "Foundation models have achieved great success in natural language processing\n(NLP) and computer vision (CV). Their success largely stems from the ability to\nintegrate multi-domain knowledge in pre-training and transfer it to target\ndomains. Considering graph data, especially graphs without textual features, is\nubiquitous in real-world applications such as social networks and\nrecommendation systems, some researchers have attempted to extend this paradigm\nto the graph field, aiming to construct graph foundation models. However,\nunlike CV and NLP, there are huge gaps among the semantics and properties of\ngraphs in different domains, while current works still adopt traditional\ncontrastive pre-training strategies designed in the single-domain scenario,\nwhich regard contrastive samples from different domains as equivalent. From\nexperimental investigations, we discovered that inherent domain-specific\ndifferences prevent these strategies from effectively absorbing knowledge from\ndifferent domains to generate informative representations. In this paper, we\npropose a novel multi-domain pre-training and cross-domain transfer framework,\nnamely MDGCL.In the pre-training stage, we design a contrastive learning\nstrategy to substantially recognize and capture domain differences, and\nintroduce domain tokens to encode domain-level global information. In the\ndownstream stage, we introduce a domain attention mechanism to enable\nfine-grained domain knowledge transfer. Extensive experiments on five benchmark\ndatasets have demonstrated that our method outperforms state-of-the-art\nsignificantly, with the maximum improvement of 19.33\\% on accuracy and 19.13\\%\non Macro-F1 score.", "AI": {"tldr": "\u9488\u5bf9\u56fe\u6570\u636e\u4e0d\u540c\u9886\u57df\u95f4\u8bed\u4e49\u548c\u5c5e\u6027\u5dee\u5f02\u5927\u3001\u73b0\u6709\u9884\u8bad\u7ec3\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u5408\u591a\u9886\u57df\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86MDGCL\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8bbe\u8ba1\u8bc6\u522b\u9886\u57df\u5dee\u5f02\u7684\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u548c\u5f15\u5165\u9886\u57df\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u591a\u9886\u57df\u9884\u8bad\u7ec3\u548c\u8de8\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u5728NLP\u548cCV\u9886\u57df\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u5e76\u88ab\u5c1d\u8bd5\u6269\u5c55\u5230\u56fe\u6570\u636e\uff0c\u4f46\u56fe\u6570\u636e\u5728\u4e0d\u540c\u9886\u57df\u95f4\u7684\u8bed\u4e49\u548c\u5c5e\u6027\u5b58\u5728\u5de8\u5927\u5dee\u5f02\u3002\u73b0\u6709\u4f20\u7edf\u7684\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7b56\u7565\u5c06\u4e0d\u540c\u9886\u57df\u7684\u5bf9\u6bd4\u6837\u672c\u89c6\u4e3a\u7b49\u540c\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u5438\u6536\u591a\u9886\u57df\u77e5\u8bc6\u5e76\u751f\u6210\u6709\u4fe1\u606f\u91cf\u7684\u8868\u793a\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u9886\u57df\u9884\u8bad\u7ec3\u548c\u8de8\u9886\u57df\u8fc1\u79fb\u6846\u67b6MDGCL\u3002\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u4ee5\u8bc6\u522b\u548c\u6355\u83b7\u9886\u57df\u5dee\u5f02\uff0c\u5e76\u5f15\u5165\u9886\u57df\u6807\u8bb0\uff08domain tokens\uff09\u6765\u7f16\u7801\u9886\u57df\u7ea7\u5168\u5c40\u4fe1\u606f\u3002\u5728\u4e0b\u6e38\u4efb\u52a1\u9636\u6bb5\uff0c\u5f15\u5165\u4e86\u9886\u57df\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684MDGCL\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\uff08SOTA\uff09\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u6700\u5927\u63d0\u534719.33%\uff0cMacro-F1\u5206\u6570\u6700\u5927\u63d0\u534719.13%\u3002", "conclusion": "MDGCL\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5904\u7406\u56fe\u6570\u636e\u4e2d\u591a\u9886\u57df\u95f4\u7684\u56fa\u6709\u5dee\u5f02\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u9886\u57df\u77e5\u8bc6\u7684\u5438\u6536\u548c\u8de8\u9886\u57df\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u8868\u793a\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u4e3a\u6784\u5efa\u66f4\u901a\u7528\u7684\u56fe\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2506.22477", "pdf": "https://arxiv.org/pdf/2506.22477", "abs": "https://arxiv.org/abs/2506.22477", "authors": ["Huiwen Han"], "title": "Innovative Research on IoT Architecture and Robotic Operating Platforms: Applications of Large Language Models and Generative AI", "categories": ["cs.NI", "cs.AI", "cs.ET", "cs.RO"], "comment": "Published in: 2024 6th International Conference on Robotics,\n  Intelligent Control and Artificial Intelligence (RICAI), IEEE Xplore, DOI:\n  10.1109/RICAI64321.2024.10911316. \\c{opyright} 2024 IEEE", "summary": "This paper introduces an innovative design for robotic operating platforms,\nunderpinned by a transformative Internet of Things (IoT) architecture,\nseamlessly integrating cutting-edge technologies such as large language models\n(LLMs), generative AI, edge computing, and 5G networks. The proposed platform\naims to elevate the intelligence and autonomy of IoT systems and robotics,\nenabling them to make real-time decisions and adapt dynamically to changing\nenvironments. Through a series of compelling case studies across industries\nincluding smart manufacturing, healthcare, and service sectors, this paper\ndemonstrates the substantial potential of IoT-enabled robotics to optimize\noperational workflows, enhance productivity, and deliver innovative, scalable\nsolutions. By emphasizing the roles of LLMs and generative AI, the research\nhighlights how these technologies drive the evolution of intelligent robotics\nand IoT, shaping the future of industry-specific advancements. The findings not\nonly showcase the transformative power of these technologies but also offer a\nforward-looking perspective on their broader societal and industrial\nimplications, positioning them as catalysts for next-generation automation and\ntechnological convergence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u521b\u65b0\u7684\u7269\u8054\u7f51\u67b6\u6784\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u5e73\u53f0\uff0c\u878d\u5408\u5927\u8bed\u8a00\u6a21\u578b\u3001\u751f\u6210\u5f0fAI\u3001\u8fb9\u7f18\u8ba1\u7b97\u548c5G\uff0c\u65e8\u5728\u63d0\u5347\u673a\u5668\u4eba\u667a\u80fd\u548c\u81ea\u4e3b\u6027\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u5176\u5728\u591a\u884c\u4e1a\u4e2d\u4f18\u5316\u5de5\u4f5c\u6d41\u7a0b\u3001\u63d0\u9ad8\u751f\u4ea7\u529b\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u5e76\u5c55\u671b\u5176\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u81ea\u52a8\u5316\u50ac\u5316\u5242\u7684\u4f5c\u7528\u3002", "motivation": "\u65e8\u5728\u63d0\u5347\u7269\u8054\u7f51\u7cfb\u7edf\u548c\u673a\u5668\u4eba\u7684\u667a\u80fd\u6027\u548c\u81ea\u4e3b\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u8fdb\u884c\u5b9e\u65f6\u51b3\u7b56\u5e76\u52a8\u6001\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u73af\u5883\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u5e73\u53f0\u8bbe\u8ba1\uff0c\u8be5\u5e73\u53f0\u57fa\u4e8e\u53d8\u9769\u6027\u7684\u7269\u8054\u7f51\uff08IoT\uff09\u67b6\u6784\uff0c\u5e76\u65e0\u7f1d\u96c6\u6210\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001\u751f\u6210\u5f0fAI\u3001\u8fb9\u7f18\u8ba1\u7b97\u548c5G\u7f51\u7edc\u7b49\u524d\u6cbf\u6280\u672f\u3002\u901a\u8fc7\u667a\u80fd\u5236\u9020\u3001\u533b\u7597\u4fdd\u5065\u548c\u670d\u52a1\u884c\u4e1a\u7b49\u4e00\u7cfb\u5217\u6848\u4f8b\u7814\u7a76\u6765\u5c55\u793a\u5176\u6f5c\u529b\u3002", "result": "\u7814\u7a76\u5c55\u793a\u4e86\u7269\u8054\u7f51\u673a\u5668\u4eba\u6280\u672f\u5728\u4f18\u5316\u8fd0\u8425\u5de5\u4f5c\u6d41\u7a0b\u3001\u63d0\u9ad8\u751f\u4ea7\u529b\u548c\u63d0\u4f9b\u521b\u65b0\u3001\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002\u540c\u65f6\uff0c\u5f3a\u8c03\u4e86LLMs\u548c\u751f\u6210\u5f0fAI\u5728\u63a8\u52a8\u667a\u80fd\u673a\u5668\u4eba\u548c\u7269\u8054\u7f51\u53d1\u5c55\u3001\u5851\u9020\u884c\u4e1a\u7279\u5b9a\u8fdb\u6b65\u672a\u6765\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u6210\u679c\u4e0d\u4ec5\u5c55\u793a\u4e86\u8fd9\u4e9b\u6280\u672f\u7684\u53d8\u9769\u6027\u529b\u91cf\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5bf9\u5176\u66f4\u5e7f\u6cdb\u793e\u4f1a\u548c\u5de5\u4e1a\u5f71\u54cd\u7684\u524d\u77bb\u6027\u89c6\u89d2\uff0c\u5c06\u5176\u5b9a\u4f4d\u4e3a\u4e0b\u4e00\u4ee3\u81ea\u52a8\u5316\u548c\u6280\u672f\u878d\u5408\u7684\u50ac\u5316\u5242\u3002"}}
{"id": "2506.22443", "pdf": "https://arxiv.org/pdf/2506.22443", "abs": "https://arxiv.org/abs/2506.22443", "authors": ["Sarah Seifi", "Tobias Sukianto", "Cecilia Carbonelli", "Lorenzo Servadei", "Robert Wille"], "title": "Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition", "categories": ["cs.LG", "cs.HC"], "comment": "8 pages, 3 figures, accepted at the late-breaking work track at the\n  XAI-2025 third World Conference of Explainable AI", "summary": "Rule-based models offer interpretability but struggle with complex data,\nwhile deep neural networks excel in performance yet lack transparency. This\nwork investigates a neuro-symbolic rule learning neural network named RL-Net\nthat learns interpretable rule lists through neural optimization, applied for\nthe first time to radar-based hand gesture recognition (HGR). We benchmark\nRL-Net against a fully transparent rule-based system (MIRA) and an explainable\nblack-box model (XentricAI), evaluating accuracy, interpretability, and user\nadaptability via transfer learning. Our results show that RL-Net achieves a\nfavorable trade-off, maintaining strong performance (93.03% F1) while\nsignificantly reducing rule complexity. We identify optimization challenges\nspecific to rule pruning and hierarchy bias and propose stability-enhancing\nmodifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical\nmiddle ground between transparency and performance. This study highlights the\nreal-world feasibility of neuro-symbolic models for interpretable HGR and\noffers insights for extending explainable AI to edge-deployable sensing\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86RL-Net\uff0c\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u89c4\u5219\u5b66\u4e60\u7f51\u7edc\uff0c\u9996\u6b21\u5e94\u7528\u4e8e\u96f7\u8fbe\u624b\u52bf\u8bc6\u522b\uff0c\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u4f18\u5316\u5b66\u4e60\u53ef\u89e3\u91ca\u89c4\u5219\uff0c\u4ee5\u5e73\u8861\u6027\u80fd\u4e0e\u900f\u660e\u5ea6\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u9762\u4e34\u6027\u80fd\uff08\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff09\u4e0e\u53ef\u89e3\u91ca\u6027\uff08\u57fa\u4e8e\u89c4\u5219\u7684\u6a21\u578b\uff09\u4e4b\u95f4\u7684\u56fa\u6709\u6743\u8861\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u517c\u987e\u4e24\u8005\u4f18\u52bf\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4e86\u4e00\u79cd\u540d\u4e3aRL-Net\u7684\u795e\u7ecf\u7b26\u53f7\u89c4\u5219\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u901a\u8fc7\u795e\u7ecf\u4f18\u5316\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u89c4\u5219\u5217\u8868\uff0c\u5e76\u9996\u6b21\u5e94\u7528\u4e8e\u96f7\u8fbe\u624b\u52bf\u8bc6\u522b\uff08HGR\uff09\u3002\u901a\u8fc7\u4e0e\u5168\u900f\u660e\u7684\u89c4\u5219\u7cfb\u7edfMIRA\u548c\u53ef\u89e3\u91ca\u7684\u9ed1\u76d2\u6a21\u578bXentricAI\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86RL-Net\u7684\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7528\u6237\u9002\u5e94\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u8bc6\u522b\u5e76\u89e3\u51b3\u4e86\u89c4\u5219\u526a\u679d\u548c\u5c42\u6b21\u504f\u5dee\u4e2d\u7684\u4f18\u5316\u6311\u6218\u3002", "result": "RL-Net\u5728\u4fdd\u6301\u5f3a\u5927\u6027\u80fd\uff0893.03% F1\u5206\u6570\uff09\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u89c4\u5219\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u900f\u660e\u5ea6\u7684\u826f\u597d\u6743\u8861\u3002\u7814\u7a76\u8fd8\u8bc6\u522b\u4e86\u7279\u5b9a\u7684\u4f18\u5316\u6311\u6218\u5e76\u63d0\u51fa\u4e86\u7a33\u5b9a\u6027\u589e\u5f3a\u7684\u4fee\u6539\u3002\u4e0eMIRA\u548cXentricAI\u76f8\u6bd4\uff0cRL-Net\u88ab\u8bc1\u660e\u662f\u900f\u660e\u5ea6\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u5b9e\u7528\u4e2d\u95f4\u65b9\u6848\u3002", "conclusion": "\u672c\u7814\u7a76\u7a81\u51fa\u4e86\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u96f7\u8fbe\u624b\u52bf\u8bc6\u522b\u4e2d\u7684\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u5c06\u53ef\u89e3\u91caAI\u6269\u5c55\u5230\u8fb9\u7f18\u90e8\u7f72\u4f20\u611f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2506.22653", "pdf": "https://arxiv.org/pdf/2506.22653", "abs": "https://arxiv.org/abs/2506.22653", "authors": ["Michael Grosskopf", "Russell Bent", "Rahul Somasundaram", "Isaac Michaud", "Arthur Lui", "Nathan Debardeleben", "Earl Lawrence"], "title": "URSA: The Universal Research and Scientific Agent", "categories": ["cs.AI"], "comment": "31 pages, 9 figures", "summary": "Large language models (LLMs) have moved far beyond their initial form as\nsimple chatbots, now carrying out complex reasoning, planning, writing, coding,\nand research tasks. These skills overlap significantly with those that human\nscientists use day-to-day to solve complex problems that drive the cutting edge\nof research. Using LLMs in \"agentic\" AI has the potential to revolutionize\nmodern science and remove bottlenecks to progress. In this work, we present\nURSA, a scientific agent ecosystem for accelerating research tasks. URSA\nconsists of a set of modular agents and tools, including coupling to advanced\nphysics simulation codes, that can be combined to address scientific problems\nof varied complexity and impact. This work highlights the architecture of URSA,\nas well as examples that highlight the potential of the system.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86URSA\uff0c\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u7269\u7406\u6a21\u62df\u7684\u6a21\u5757\u5316\u79d1\u5b66\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\uff0c\u65e8\u5728\u52a0\u901f\u590d\u6742\u7684\u79d1\u7814\u4efb\u52a1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u53d1\u5c55\u51fa\u590d\u6742\u7684\u63a8\u7406\u3001\u89c4\u5212\u548c\u7814\u7a76\u80fd\u529b\uff0c\u4e0e\u79d1\u5b66\u5bb6\u65e5\u5e38\u5de5\u4f5c\u9ad8\u5ea6\u91cd\u53e0\u3002\u5c06LLMs\u5e94\u7528\u4e8e\u201c\u667a\u80fd\u4f53\u201dAI\u6709\u671b\u9769\u65b0\u73b0\u4ee3\u79d1\u5b66\uff0c\u6d88\u9664\u7814\u7a76\u74f6\u9888\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86URSA\uff0c\u4e00\u4e2a\u79d1\u5b66\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\uff0c\u7528\u4e8e\u52a0\u901f\u79d1\u7814\u4efb\u52a1\u3002URSA\u7531\u4e00\u5957\u6a21\u5757\u5316\u667a\u80fd\u4f53\u548c\u5de5\u5177\u7ec4\u6210\uff0c\u5305\u62ec\u4e0e\u9ad8\u7ea7\u7269\u7406\u6a21\u62df\u4ee3\u7801\u7684\u8026\u5408\uff0c\u80fd\u591f\u7ec4\u5408\u5e94\u5bf9\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u79d1\u5b66\u95ee\u9898\u3002", "result": "\u672c\u6587\u91cd\u70b9\u4ecb\u7ecd\u4e86URSA\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u5177\u4f53\u793a\u4f8b\u5c55\u793a\u4e86\u8be5\u7cfb\u7edf\u5728\u52a0\u901f\u79d1\u7814\u4efb\u52a1\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002", "conclusion": "URSA\u7cfb\u7edf\u5c55\u793a\u4e86\u5229\u7528LLM\u9a71\u52a8\u7684\u79d1\u5b66\u667a\u80fd\u4f53\u52a0\u901f\u548c\u9769\u65b0\u73b0\u4ee3\u79d1\u5b66\u7814\u7a76\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u6709\u671b\u79fb\u9664\u79d1\u7814\u8fdb\u5c55\u4e2d\u7684\u74f6\u9888\u3002"}}
{"id": "2506.22463", "pdf": "https://arxiv.org/pdf/2506.22463", "abs": "https://arxiv.org/abs/2506.22463", "authors": ["Weizhi Gao", "Zhichao Hou", "Junqi Yin", "Feiyi Wang", "Linyu Peng", "Xiaorui Liu"], "title": "Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization", "categories": ["cs.CV", "cs.LG"], "comment": "26 pages, accepted by ICML 2025", "summary": "Diffusion models have emerged as powerful generative models, but their high\ncomputation cost in iterative sampling remains a significant bottleneck. In\nthis work, we present an in-depth and insightful study of state-of-the-art\nacceleration techniques for diffusion models, including caching and\nquantization, revealing their limitations in computation error and generation\nquality. To break these limits, this work introduces Modulated Diffusion\n(MoDiff), an innovative, rigorous, and principled framework that accelerates\ngenerative modeling through modulated quantization and error compensation.\nMoDiff not only inherents the advantages of existing caching and quantization\nmethods but also serves as a general framework to accelerate all diffusion\nmodels. The advantages of MoDiff are supported by solid theoretical insight and\nanalysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate\nthat MoDiff significant reduces activation quantization from 8 bits to 3 bits\nwithout performance degradation in post-training quantization (PTQ). Our code\nimplementation is available at https://github.com/WeizhiGao/MoDiff.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMoDiff\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u5236\u91cf\u5316\u548c\u8bef\u5dee\u8865\u507f\uff0c\u663e\u8457\u52a0\u901f\u6269\u6563\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u8fed\u4ee3\u91c7\u6837\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u662f\u5176\u5e94\u7528\u74f6\u9888\u3002\u73b0\u6709\u52a0\u901f\u6280\u672f\uff08\u5982\u7f13\u5b58\u548c\u91cf\u5316\uff09\u5b58\u5728\u8ba1\u7b97\u8bef\u5dee\u548c\u751f\u6210\u8d28\u91cf\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165Modulated Diffusion (MoDiff)\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u5236\u91cf\u5316\u548c\u8bef\u5dee\u8865\u507f\u6765\u52a0\u901f\u751f\u6210\u5efa\u6a21\u3002MoDiff\u662f\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u73b0\u6709\u7f13\u5b58\u548c\u91cf\u5316\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u5e76\u63d0\u4f9b\u575a\u5b9e\u7684\u7406\u8bba\u6d1e\u5bdf\u548c\u5206\u6790\u652f\u6301\u3002", "result": "\u5728CIFAR-10\u548cLSUN\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMoDiff\u5728\u8bad\u7ec3\u540e\u91cf\u5316(PTQ)\u4e2d\u80fd\u5c06\u6fc0\u6d3b\u91cf\u5316\u4ece8\u6bd4\u7279\u663e\u8457\u964d\u4f4e\u52303\u6bd4\u7279\uff0c\u4e14\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "MoDiff\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u91cf\u5316\u548c\u8bef\u5dee\u8865\u507f\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5927\u5e45\u5ea6\u7684\u52a0\u901f\u800c\u65e0\u9700\u727a\u7272\u751f\u6210\u8d28\u91cf\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u7406\u8bba\u4e0a\u4e25\u8c28\u7684\u52a0\u901f\u6846\u67b6\u3002"}}
{"id": "2506.22516", "pdf": "https://arxiv.org/pdf/2506.22516", "abs": "https://arxiv.org/abs/2506.22516", "authors": ["Jingkai Li"], "title": "Can \"consciousness\" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis", "categories": ["cs.CL", "cs.AI", "cs.NE", "q-bio.NC"], "comment": "Published as a journal paper at:\n  https://doi.org/10.1016/j.nlp.2025.100163", "summary": "Integrated Information Theory (IIT) provides a quantitative framework for\nexplaining consciousness phenomenon, positing that conscious systems comprise\nelements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the\nlatest iterations of this framework -- to sequences of Large Language Model\n(LLM) representations, analyzing data derived from existing Theory of Mind\n(ToM) test results. Our study systematically investigates whether the\ndifferences of ToM test performances, when presented in the LLM\nrepresentations, can be revealed by IIT estimates, i.e., $\\Phi^{\\max}$ (IIT\n3.0), $\\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\\Phi$-structure\n(IIT 4.0). Furthermore, we compare these metrics with the Span Representations\nindependent of any estimate for consciousness. This additional effort aims to\ndifferentiate between potential \"consciousness\" phenomena and inherent\nseparations within LLM representational space. We conduct comprehensive\nexperiments examining variations across LLM transformer layers and linguistic\nspans from stimuli. Our results suggest that sequences of contemporary\nTransformer-based LLM representations lack statistically significant indicators\nof observed \"consciousness\" phenomena but exhibit intriguing patterns under\n$\\textit{spatio}$-permutational analyses. The Appendix and code are available\nas Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.22480", "pdf": "https://arxiv.org/pdf/2506.22480", "abs": "https://arxiv.org/abs/2506.22480", "authors": ["Mariam Yahya", "Aydin Sezgin", "Setareh Maghsudi"], "title": "Service Placement in Small Cell Networks Using Distributed Best Arm Identification in Linear Bandits", "categories": ["cs.NI", "cs.DC", "cs.LG"], "comment": null, "summary": "As users in small cell networks increasingly rely on computation-intensive\nservices, cloud-based access often results in high latency. Multi-access edge\ncomputing (MEC) mitigates this by bringing computational resources closer to\nend users, with small base stations (SBSs) serving as edge servers to enable\nlow-latency service delivery. However, limited edge capacity makes it\nchallenging to decide which services to deploy locally versus in the cloud,\nespecially under unknown service demand and dynamic network conditions. To\ntackle this problem, we model service demand as a linear function of service\nattributes and formulate the service placement task as a linear bandit problem,\nwhere SBSs act as agents and services as arms. The goal is to identify the\nservice that, when placed at the edge, offers the greatest reduction in total\nuser delay compared to cloud deployment. We propose a distributed and adaptive\nmulti-agent best-arm identification (BAI) algorithm under a fixed-confidence\nsetting, where SBSs collaborate to accelerate learning. Simulations show that\nour algorithm identifies the optimal service with the desired confidence and\nachieves near-optimal speedup, as the number of learning rounds decreases\nproportionally with the number of SBSs. We also provide theoretical analysis of\nthe algorithm's sample complexity and communication overhead.", "AI": {"tldr": "\u5728MEC\u4e2d\uff0c\u9762\u5bf9\u6709\u9650\u7684\u8fb9\u7f18\u5bb9\u91cf\u548c\u672a\u77e5\u670d\u52a1\u9700\u6c42\uff0c\u5982\u4f55\u5c06\u8ba1\u7b97\u5bc6\u96c6\u578b\u670d\u52a1\u90e8\u7f72\u5728\u8fb9\u7f18\u4ee5\u6700\u5c0f\u5316\u7528\u6237\u5ef6\u8fdf\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u6700\u4f73\u81c2\u8bc6\u522b\u7b97\u6cd5\uff0c\u5c06\u670d\u52a1\u9700\u6c42\u5efa\u6a21\u4e3a\u7ebf\u6027\u51fd\u6570\uff0c\u901a\u8fc7SBS\u534f\u4f5c\u9ad8\u6548\u8bc6\u522b\u6700\u4f18\u8fb9\u7f18\u670d\u52a1\u3002", "motivation": "\u968f\u7740\u5c0f\u578b\u8702\u7a9d\u7f51\u7edc\u7528\u6237\u5bf9\u8ba1\u7b97\u5bc6\u96c6\u578b\u670d\u52a1\u7684\u4f9d\u8d56\u589e\u52a0\uff0c\u4e91\u7aef\u8bbf\u95ee\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u3002\u591a\u63a5\u5165\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u80fd\u5c06\u8ba1\u7b97\u8d44\u6e90\u62c9\u8fd1\u7528\u6237\uff0c\u4f46\u6709\u9650\u7684\u8fb9\u7f18\u5bb9\u91cf\u4ee5\u53ca\u672a\u77e5\u7684\u670d\u52a1\u9700\u6c42\u548c\u52a8\u6001\u7f51\u7edc\u6761\u4ef6\uff0c\u4f7f\u5f97\u670d\u52a1\u5728\u672c\u5730\uff08\u8fb9\u7f18\uff09\u6216\u4e91\u7aef\u90e8\u7f72\u7684\u51b3\u7b56\u53d8\u5f97\u56f0\u96be\uff0c\u4e9f\u9700\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u6765\u6700\u5927\u5316\u7528\u6237\u5ef6\u8fdf\u964d\u4f4e\u3002", "method": "\u5c06\u670d\u52a1\u9700\u6c42\u5efa\u6a21\u4e3a\u670d\u52a1\u5c5e\u6027\u7684\u7ebf\u6027\u51fd\u6570\uff0c\u5e76\u5c06\u670d\u52a1\u653e\u7f6e\u4efb\u52a1\u8868\u8ff0\u4e3a\u7ebf\u6027\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5176\u4e2d\u5c0f\u578b\u57fa\u7ad9\uff08SBS\uff09\u4f5c\u4e3a\u667a\u80fd\u4f53\uff0c\u670d\u52a1\u4f5c\u4e3a\u81c2\u3002\u76ee\u6807\u662f\u8bc6\u522b\u5728\u8fb9\u7f18\u90e8\u7f72\u65f6\u80fd\u6700\u5927\u7a0b\u5ea6\u964d\u4f4e\u603b\u7528\u6237\u5ef6\u8fdf\u7684\u670d\u52a1\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u3001\u81ea\u9002\u5e94\u7684\u591a\u667a\u80fd\u4f53\u6700\u4f73\u81c2\u8bc6\u522b\uff08BAI\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u8bbe\u7f6e\u4e0b\uff0c\u901a\u8fc7SBS\u4e4b\u95f4\u7684\u534f\u4f5c\u6765\u52a0\u901f\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u4ee5\u671f\u671b\u7684\u7f6e\u4fe1\u5ea6\u8bc6\u522b\u51fa\u6700\u4f18\u670d\u52a1\uff0c\u5e76\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u52a0\u901f\u6548\u679c\uff0c\u5373\u5b66\u4e60\u8f6e\u6570\u4e0eSBS\u6570\u91cf\u6210\u6bd4\u4f8b\u51cf\u5c11\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u63d0\u4f9b\u4e86\u7b97\u6cd5\u6837\u672c\u590d\u6742\u5ea6\u548c\u901a\u4fe1\u5f00\u9500\u7684\u7406\u8bba\u5206\u6790\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u6700\u4f73\u81c2\u8bc6\u522b\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86MEC\u4e2d\u670d\u52a1\u90e8\u7f72\u51b3\u7b56\u7684\u96be\u9898\u3002\u8be5\u7b97\u6cd5\u5728\u672a\u77e5\u670d\u52a1\u9700\u6c42\u548c\u52a8\u6001\u7f51\u7edc\u6761\u4ef6\u4e0b\uff0c\u80fd\u591f\u901a\u8fc7\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u9ad8\u6548\u4e14\u9ad8\u7f6e\u4fe1\u5ea6\u5730\u8bc6\u522b\u51fa\u6700\u4f18\u670d\u52a1\uff0c\u663e\u8457\u964d\u4f4e\u7528\u6237\u5ef6\u8fdf\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u5b66\u4e60\u6548\u7387\u548c\u7406\u8bba\u4fdd\u969c\u3002"}}
{"id": "2506.22444", "pdf": "https://arxiv.org/pdf/2506.22444", "abs": "https://arxiv.org/abs/2506.22444", "authors": ["Jing Wang", "Amar Sra", "Jeremy C. Weiss"], "title": "Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2", "categories": ["cs.LG", "cs.CY"], "comment": null, "summary": "The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,\npose a significant challenge to healthcare systems worldwide. Accurate\nidentification of progression events, such as hospitalization and reinfection,\nis essential for effective patient management and resource allocation. However,\ntraditional models trained on structured data struggle to capture the nuanced\nprogression of PASC. In this study, we introduce the first publicly available\ncohort of 18 PASC patients, with text time series features based on Large\nLanguage Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical\nexpert. We propose an Active Attention Network to predict the clinical risk and\nidentify progression events related to the risk. By integrating human expertise\nwith active learning, we aim to enhance clinical risk prediction accuracy and\nenable progression events identification with fewer number of annotation. The\nultimate goal is to improves patient care and decision-making for SARS-CoV-2\npatient.", "AI": {"tldr": "\u9488\u5bf9PASC\u60a3\u8005\u7ba1\u7406\u96be\u9898\uff0c\u672c\u7814\u7a76\u9996\u6b21\u516c\u5f00\u5c0f\u89c4\u6a21\u60a3\u8005\u961f\u5217\uff0818\u4f8b\uff09\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Llama-3.1-70B-Instruct\uff09\u63d0\u53d6\u6587\u672c\u7279\u5f81\u5e76\u7ed3\u5408\u4e13\u5bb6\u6807\u6ce8\uff0c\u63d0\u51faActive Attention Network\u6a21\u578b\u4ee5\u9884\u6d4b\u4e34\u5e8a\u98ce\u9669\u548c\u8bc6\u522b\u8fdb\u5c55\u4e8b\u4ef6\uff0c\u65e8\u5728\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\uff0c\u6700\u7ec8\u6539\u5584\u60a3\u8005\u62a4\u7406\u3002", "motivation": "PASC\uff08SARS-CoV-2\u6025\u6027\u540e\u9057\u75c7\uff09\u7684\u957f\u671f\u5f71\u54cd\u5bf9\u5168\u7403\u533b\u7597\u7cfb\u7edf\u6784\u6210\u91cd\u5927\u6311\u6218\u3002\u51c6\u786e\u8bc6\u522bPASC\u7684\u8fdb\u5c55\u4e8b\u4ef6\uff08\u5982\u4f4f\u9662\u3001\u518d\u611f\u67d3\uff09\u5bf9\u4e8e\u6709\u6548\u7684\u60a3\u8005\u7ba1\u7406\u548c\u8d44\u6e90\u5206\u914d\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f20\u7edf\u57fa\u4e8e\u7ed3\u6784\u5316\u6570\u636e\u7684\u6a21\u578b\u96be\u4ee5\u6355\u6349PASC\u7684\u7ec6\u5fae\u8fdb\u5c55\u3002", "method": "\u672c\u7814\u7a76\u9996\u6b21\u6784\u5efa\u5e76\u516c\u5f00\u4e86\u4e00\u4e2a\u5305\u542b18\u4f8bPASC\u60a3\u8005\u7684\u961f\u5217\u6570\u636e\uff0c\u5176\u7279\u5f81\u5305\u62ec\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578bLlama-3.1-70B-Instruct\u751f\u6210\u7684\u6587\u672c\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\uff0c\u4ee5\u53ca\u7531\u4e34\u5e8a\u4e13\u5bb6\u6807\u6ce8\u7684\u4e34\u5e8a\u98ce\u9669\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cdActive Attention Network\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u4e34\u5e8a\u98ce\u9669\u5e76\u8bc6\u522b\u4e0e\u8be5\u98ce\u9669\u76f8\u5173\u7684\u8fdb\u5c55\u4e8b\u4ef6\u3002\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u548c\u4e3b\u52a8\u5b66\u4e60\u3002", "result": "\u62bd\u8c61\u4e2d\u672a\u63d0\u4f9b\u5177\u4f53\u7684\u91cf\u5316\u7814\u7a76\u7ed3\u679c\u3002\u4f46\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6574\u5408\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u4e3b\u52a8\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u9ad8\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u4ee5\u66f4\u5c11\u7684\u6807\u6ce8\u6570\u91cf\u5b9e\u73b0\u8fdb\u5c55\u4e8b\u4ef6\u7684\u8bc6\u522b\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u6700\u7ec8\u76ee\u6807\u662f\u901a\u8fc7\u63d0\u9ad8\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u8fdb\u5c55\u4e8b\u4ef6\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u4ece\u800c\u6539\u5584SARS-CoV-2\u60a3\u8005\u7684\u62a4\u7406\u548c\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2506.22740", "pdf": "https://arxiv.org/pdf/2506.22740", "abs": "https://arxiv.org/abs/2506.22740", "authors": ["Jessica Hullman", "Ziyang Guo", "Berk Ustun"], "title": "Explanations are a means to an end", "categories": ["cs.AI", "stat.ML"], "comment": null, "summary": "Modern methods for explainable machine learning are designed to describe how\nmodels map inputs to outputs--without deep consideration of how these\nexplanations will be used in practice. This paper argues that explanations\nshould be designed and evaluated with a specific end in mind. We describe how\nto formalize this end in a framework based in statistical decision theory. We\nshow how this functionally-grounded approach can be applied across diverse use\ncases, such as clinical decision support, providing recourse, or debugging. We\ndemonstrate its use to characterize the maximum \"boost\" in performance on a\nparticular task that an explanation could provide an idealized decision-maker,\npreventing misuse due to ambiguity by forcing researchers to specify concrete\nuse cases that can be analyzed in light of models of expected explanation use.\nWe argue that evaluation should meld theoretical and empirical perspectives on\nthe value of explanation, and contribute definitions that span these\nperspectives.", "AI": {"tldr": "\u73b0\u6709\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u7edf\u8ba1\u51b3\u7b56\u7406\u8bba\u7684\u6846\u67b6\uff0c\u4f7f\u89e3\u91ca\u8bbe\u8ba1\u548c\u8bc4\u4f30\u56f4\u7ed5\u5177\u4f53\u7528\u9014\u8fdb\u884c\uff0c\u4ece\u800c\u63d0\u5347\u89e3\u91ca\u4ef7\u503c\u5e76\u907f\u514d\u8bef\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u63cf\u8ff0\u6a21\u578b\u5982\u4f55\u6620\u5c04\u8f93\u5165\u5230\u8f93\u51fa\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u89e3\u91ca\u5728\u5b9e\u8df5\u4e2d\u5982\u4f55\u88ab\u4f7f\u7528\u7684\u6df1\u5165\u8003\u8651\uff0c\u5bfc\u81f4\u6f5c\u5728\u7684\u8bef\u7528\u548c\u6a21\u7cca\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u7edf\u8ba1\u51b3\u7b56\u7406\u8bba\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5f62\u5f0f\u5316\u89e3\u91ca\u7684\u201c\u6700\u7ec8\u76ee\u7684\u201d\u6216\u5177\u4f53\u7528\u9014\u3002\u8be5\u65b9\u6cd5\u5c06\u89e3\u91ca\u8bbe\u8ba1\u4e0e\u5b9e\u9645\u529f\u80fd\u7ed3\u5408\uff0c\u5e76\u5c55\u793a\u5982\u4f55\u901a\u8fc7\u6b64\u6846\u67b6\u91cf\u5316\u89e3\u91ca\u5bf9\u7406\u60f3\u51b3\u7b56\u8005\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u53ef\u80fd\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\u3002", "result": "\u8bc1\u660e\u4e86\u8fd9\u79cd\u529f\u80fd\u9a71\u52a8\u7684\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3001\u63d0\u4f9b\u8ffd\u7d22\u6743\u6216\u8c03\u8bd5\u7b49\u591a\u79cd\u7528\u4f8b\u3002\u5b83\u80fd\u591f\u523b\u753b\u89e3\u91ca\u5bf9\u7406\u60f3\u51b3\u7b56\u8005\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u6240\u80fd\u63d0\u4f9b\u7684\u6700\u5927\u6027\u80fd\u201c\u63d0\u5347\u201d\uff0c\u5e76\u901a\u8fc7\u5f3a\u5236\u7814\u7a76\u4eba\u5458\u660e\u786e\u5177\u4f53\u7528\u4f8b\u6765\u9632\u6b62\u56e0\u6b67\u4e49\u9020\u6210\u7684\u8bef\u7528\u3002", "conclusion": "\u89e3\u91ca\u5e94\u57fa\u4e8e\u5176\u7279\u5b9a\u76ee\u7684\u8fdb\u884c\u8bbe\u8ba1\u548c\u8bc4\u4f30\u3002\u8bc4\u4f30\u5e94\u878d\u5408\u7406\u8bba\u548c\u5b9e\u8bc1\u89c6\u89d2\u6765\u8861\u91cf\u89e3\u91ca\u7684\u4ef7\u503c\uff0c\u5e76\u4e3a\u6b64\u8d21\u732e\u4e86\u76f8\u5173\u5b9a\u4e49\u3002"}}
{"id": "2506.22498", "pdf": "https://arxiv.org/pdf/2506.22498", "abs": "https://arxiv.org/abs/2506.22498", "authors": ["Hao Liu", "Yu Hu", "Rakiba Rayhana", "Ling Bai", "Zheng Liu"], "title": "ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Bed-related falls remain a leading source of injury in hospitals and\nlong-term-care facilities, yet many commercial alarms trigger only after a\npatient has already left the bed. We show that early bed-exit intent can be\npredicted using only four low-cost load cells mounted under the bed legs. The\nresulting load signals are first converted into a compact set of complementary\nimages: an RGB line plot that preserves raw waveforms and three texture maps -\nrecurrence plot, Markov transition field, and Gramian angular field - that\nexpose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin\nTransformer that processes the line plot and texture maps in parallel and fuses\nthem through cross-attention to learn data-driven modality weights.\n  To provide a realistic benchmark, we collected six months of continuous data\nfrom 95 beds in a long-term-care facility. On this real-world dataset\nViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing\nrecent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.\nThe results demonstrate that image-based fusion of load-sensor signals for time\nseries classification is a practical and effective solution for real-time,\nprivacy-preserving fall prevention.", "AI": {"tldr": "\u901a\u8fc7\u4f4e\u6210\u672c\u79f0\u91cd\u4f20\u611f\u5668\u7ed3\u5408\u56fe\u50cf\u8f6c\u6362\u548c\u53cc\u6d41Swin Transformer\u6a21\u578b\uff0c\u5b9e\u73b0\u65e9\u671f\u79bb\u5e8a\u610f\u56fe\u9884\u6d4b\uff0c\u6709\u6548\u9884\u9632\u60a3\u8005\u8dcc\u5012\u3002", "motivation": "\u533b\u9662\u548c\u957f\u671f\u62a4\u7406\u673a\u6784\u4e2d\uff0c\u4e0e\u5e8a\u76f8\u5173\u7684\u8dcc\u5012\u4ecd\u662f\u5bfc\u81f4\u53d7\u4f24\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u800c\u73b0\u6709\u5546\u4e1a\u8b66\u62a5\u7cfb\u7edf\u901a\u5e38\u5728\u60a3\u8005\u79bb\u5e8a\u540e\u624d\u89e6\u53d1\u3002", "method": "\u4f7f\u7528\u5b89\u88c5\u5728\u5e8a\u817f\u4e0b\u7684\u56db\u4e2a\u4f4e\u6210\u672c\u79f0\u91cd\u4f20\u611f\u5668\u83b7\u53d6\u4fe1\u53f7\u3002\u5c06\u4fe1\u53f7\u8f6c\u6362\u4e3aRGB\u7ebf\u56fe\u548c\u4e09\u79cd\u7eb9\u7406\u56fe\uff08\u9012\u5f52\u56fe\u3001\u9a6c\u5c14\u53ef\u592b\u8f6c\u79fb\u573a\u3001\u683c\u62c9\u59c6\u89d2\u573a\uff09\u3002\u5f15\u5165ViFusionTST\uff0c\u4e00\u4e2a\u53cc\u6d41Swin Transformer\u6a21\u578b\uff0c\u5e76\u884c\u5904\u7406\u5e76\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u8fd9\u4e9b\u56fe\u50cf\u6a21\u6001\u3002", "result": "\u5728\u5305\u542b95\u5f20\u5e8a\u4f4d\u3001\u4e3a\u671f\u516d\u4e2a\u6708\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cViFusionTST\u7684\u51c6\u786e\u7387\u4e3a0.885\uff0cF1\u5206\u6570\u4e3a0.794\uff0c\u5e76\u5728\u591a\u9879\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u50cf\u878d\u5408\u7684\u79f0\u91cd\u4f20\u611f\u5668\u4fe1\u53f7\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\uff0c\u662f\u5b9e\u73b0\u5b9e\u65f6\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u8dcc\u5012\u9884\u9632\u7684\u5b9e\u7528\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22518", "pdf": "https://arxiv.org/pdf/2506.22518", "abs": "https://arxiv.org/abs/2506.22518", "authors": ["Deyu Zou", "Yongqiang Chen", "Mufei Li", "Siqi Miao", "Chenxi Liu", "Bo Han", "James Cheng", "Pan Li"], "title": "Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to ground responses with structured external knowledge from\nup-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs\noften rely on a weak retriever in graph-based RAG: I) Due to the lack of ground\ntruth, the retriever is often trained on weak supervision, which often\nintroduces spurious signals to the LLMs. II) Due to the abstraction of graph\ndata, the retrieved knowledge is often presented in unorganized forms. To\nmitigate the issue, we present Refined Graph-based RAG (ReG) to align weak\nretrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM\nfeedback to get rid of spurious signals and improve the quality of the\nsupervision. Meanwhile, ReG introduces a structure-aware reorganization module\nto refactor the retrieval results into logically coherent evidence chains.\nExperiments on prominent benchmarks demonstrate that ReG significantly and\nconsistently brings improvements across different LLM backbones by up to 10%.\nThe improved supervision quality enables ReG to match the state-of-the-art\nperformance with 5% training data and to transfer to out-of-distribution KGs.\nNotably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token\ncost by up to 30% and improves the performance by up to 4%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReG\uff08Refined Graph-based RAG\uff09\uff0c\u901a\u8fc7LLM\u53cd\u9988\u4f18\u5316\u5f31\u68c0\u7d22\u5668\u7684\u76d1\u7763\u8d28\u91cf\u5e76\u91cd\u7ec4\u68c0\u7d22\u7ed3\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u57faRAG\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u56fe\u57faRAG\u4e2d\uff0cLLM\u4f9d\u8d56\u7684\u68c0\u7d22\u5668\u5b58\u5728\u4e24\u5927\u7f3a\u9677\uff1a1) \u7f3a\u4e4f\u771f\u5b9e\u6807\u7b7e\u5bfc\u81f4\u5f31\u76d1\u7763\u5f15\u5165\u865a\u5047\u4fe1\u53f7\uff1b2) \u56fe\u6570\u636e\u62bd\u8c61\u5bfc\u81f4\u68c0\u7d22\u77e5\u8bc6\u5f62\u5f0f\u65e0\u5e8f\uff0c\u96be\u4ee5\u88abLLM\u6709\u6548\u5229\u7528\u3002", "method": "\u672c\u6587\u63d0\u51faReG\u6846\u67b6\uff0c\u65e8\u5728\u5c06\u5f31\u68c0\u7d22\u5668\u4e0eLLM\u5bf9\u9f50\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5f15\u5165LLM\u53cd\u9988\u673a\u5236\uff0c\u4ee5\u6d88\u9664\u865a\u5047\u4fe1\u53f7\u5e76\u63d0\u5347\u76d1\u7763\u8d28\u91cf\uff1b2) \u5f15\u5165\u7ed3\u6784\u611f\u77e5\u91cd\u7ec4\u6a21\u5757\uff0c\u5c06\u68c0\u7d22\u7ed3\u679c\u91cd\u6784\u4e3a\u903b\u8f91\u8fde\u8d2f\u7684\u8bc1\u636e\u94fe\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cReG\u4f7fLLM\u5728\u4e0d\u540c\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe10%\u3002\u5176\u4f18\u5316\u7684\u76d1\u7763\u8d28\u91cf\u4f7f\u5f97\u4ec5\u75285%\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fbe\u5230SOTA\u6c34\u5e73\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u5206\u5e03\u5916\u77e5\u8bc6\u56fe\u8c31\u3002\u6b64\u5916\uff0c\u5728\u63a8\u7406\u578bLLM\u4e0a\uff0cReG\u5c06\u63a8\u7406token\u6210\u672c\u964d\u4f4e\u591a\u8fbe30%\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe4%\u3002", "conclusion": "ReG\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u57faRAG\u4e2d\u5f31\u68c0\u7d22\u5668\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u6027\u80fd\u3001\u6570\u636e\u6548\u7387\u3001\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u7684\u68c0\u7d22\u589e\u5f3a\u578bLLM\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2506.22482", "pdf": "https://arxiv.org/pdf/2506.22482", "abs": "https://arxiv.org/abs/2506.22482", "authors": ["Divya Alok Gupta", "Dwith Chenna", "B. Aditya Vighnesh Ramakanth"], "title": "Wireless Home Automation Using Social Networking Websites", "categories": ["cs.NI", "cs.CR", "cs.CV"], "comment": "20th Annual International Conference on Advanced Computing and\n  Communications (ADCOM) 2014", "summary": "With the advent of Internet of Things, Wireless Home Automation Systems WHAS\nare gradually gaining popularity. These systems are faced with multiple\nchallenges such as security; controlling a variety of home appliances with a\nsingle interface and user friendliness. In this paper we propose a system that\nuses secure authentication systems of social networking websites such as\nTwitter, tracks the end-users activities on the social network and then control\nhis or her domestic appliances. At the end, we highlight the applications of\nthe proposed WHAS and compare the advantages of our proposed system over\ntraditional home automation systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u793e\u4ea4\u7f51\u7edc\uff08\u5982Twitter\uff09\u5b89\u5168\u8ba4\u8bc1\u548c\u7528\u6237\u6d3b\u52a8\u8ffd\u8e2a\u6765\u63a7\u5236\u667a\u80fd\u5bb6\u5c45\u7535\u5668\u7684\u65b0\u578b\u65e0\u7ebf\u5bb6\u5ead\u81ea\u52a8\u5316\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u7684\u65e0\u7ebf\u5bb6\u5ead\u81ea\u52a8\u5316\u7cfb\u7edf\uff08WHAS\uff09\u9762\u4e34\u5b89\u5168\u3001\u591a\u8bbe\u5907\u7edf\u4e00\u63a5\u53e3\u63a7\u5236\u548c\u7528\u6237\u53cb\u597d\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7cfb\u7edf\uff0c\u5229\u7528\u793e\u4ea4\u7f51\u7ad9\uff08\u5982Twitter\uff09\u7684\u5b89\u5168\u8ba4\u8bc1\u7cfb\u7edf\uff0c\u8ffd\u8e2a\u7528\u6237\u5728\u793e\u4ea4\u7f51\u7edc\u4e0a\u7684\u6d3b\u52a8\uff0c\u5e76\u4ee5\u6b64\u63a7\u5236\u5176\u5bb6\u7528\u7535\u5668\u3002", "result": "\u6587\u4e2d\u5f3a\u8c03\u4e86\u6240\u63d0WHAS\u7684\u5e94\u7528\uff0c\u5e76\u4e0e\u4f20\u7edf\u7cfb\u7edf\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7a81\u51fa\u4e86\u5176\u4f18\u52bf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u901a\u8fc7\u6574\u5408\u793e\u4ea4\u7f51\u7edc\u8ba4\u8bc1\u548c\u6d3b\u52a8\u8ffd\u8e2a\uff0c\u65e8\u5728\u63d0\u4f9b\u66f4\u5b89\u5168\u3001\u66f4\u4fbf\u6377\u7684\u5bb6\u5ead\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u7cfb\u7edf\u3002"}}
{"id": "2506.22445", "pdf": "https://arxiv.org/pdf/2506.22445", "abs": "https://arxiv.org/abs/2506.22445", "authors": ["Saad Alqithami"], "title": "Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.MA"], "comment": null, "summary": "Cyber-Physical Systems play a critical role in the infrastructure of various\nsectors, including manufacturing, energy distribution, and autonomous\ntransportation systems. However, their increasing connectivity renders them\nhighly vulnerable to sophisticated cyber threats, such as adaptive and zero-day\nattacks, against which traditional security methods like rule-based intrusion\ndetection and single-agent reinforcement learning prove insufficient. To\novercome these challenges, this paper introduces a novel Hierarchical\nAdversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.\nHAMARL employs a hierarchical structure consisting of local agents dedicated to\nsubsystem security and a global coordinator that oversees and optimizes\ncomprehensive, system-wide defense strategies. Furthermore, the framework\nincorporates an adversarial training loop designed to simulate and anticipate\nevolving cyber threats, enabling proactive defense adaptation. Extensive\nexperimental evaluations conducted on a simulated industrial IoT testbed\nindicate that HAMARL substantially outperforms traditional multi-agent\nreinforcement learning approaches, significantly improving attack detection\naccuracy, reducing response times, and ensuring operational continuity. The\nresults underscore the effectiveness of combining hierarchical multi-agent\ncoordination with adversarially-aware training to enhance the resilience and\nsecurity of next-generation CPS.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faHAMARL\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e0e\u5bf9\u6297\u6027\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u8d5b\u535a\u7269\u7406\u7cfb\u7edf\u62b5\u5fa1\u9ad8\u7ea7\u7f51\u7edc\u5a01\u80c1\u7684\u80fd\u529b\u3002", "motivation": "\u8d5b\u535a\u7269\u7406\u7cfb\u7edf\uff08CPS\uff09\u5728\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u4e2d\u4f5c\u7528\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5176\u65e5\u76ca\u589e\u957f\u7684\u8fde\u63a5\u6027\u4f7f\u5176\u6781\u6613\u53d7\u5230\u9ad8\u7ea7\u7f51\u7edc\u5a01\u80c1\uff08\u5982\u81ea\u9002\u5e94\u3001\u96f6\u65e5\u653b\u51fb\uff09\u3002\u4f20\u7edf\u5b89\u5168\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u89c4\u5219\u7684\u5165\u4fb5\u68c0\u6d4b\u548c\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff09\u5df2\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u201c\u5206\u5c42\u5bf9\u6297\u6027\u5f39\u6027\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u201d\uff08HAMARL\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u91c7\u7528\u5206\u5c42\u7ed3\u6784\uff0c\u7531\u8d1f\u8d23\u5b50\u7cfb\u7edf\u5b89\u5168\u7684\u672c\u5730\u667a\u80fd\u4f53\u548c\u534f\u8c03\u5168\u5c40\u9632\u5fa1\u7b56\u7565\u7684\u5168\u5c40\u534f\u8c03\u5668\u7ec4\u6210\u3002\u6b64\u5916\uff0c\u6846\u67b6\u6574\u5408\u4e86\u5bf9\u6297\u6027\u8bad\u7ec3\u5faa\u73af\uff0c\u4ee5\u6a21\u62df\u548c\u9884\u6d4b\u4e0d\u65ad\u6f14\u53d8\u7684\u7f51\u7edc\u5a01\u80c1\uff0c\u5b9e\u73b0\u4e3b\u52a8\u9632\u5fa1\u9002\u5e94\u3002", "result": "\u5728\u6a21\u62df\u5de5\u4e1a\u7269\u8054\u7f51\u6d4b\u8bd5\u53f0\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cHAMARL\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u591a\u79cd\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u63d0\u9ad8\u653b\u51fb\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u7f29\u77ed\u54cd\u5e94\u65f6\u95f4\u4ee5\u53ca\u786e\u4fdd\u64cd\u4f5c\u8fde\u7eed\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\uff0c\u5c06\u5206\u5c42\u591a\u667a\u80fd\u4f53\u534f\u8c03\u4e0e\u5bf9\u6297\u611f\u77e5\u8bad\u7ec3\u76f8\u7ed3\u5408\uff0c\u80fd\u6709\u6548\u589e\u5f3a\u4e0b\u4e00\u4ee3CPS\u7684\u97e7\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2506.22774", "pdf": "https://arxiv.org/pdf/2506.22774", "abs": "https://arxiv.org/abs/2506.22774", "authors": ["Michael Papademas", "Xenia Ziouvelou", "Antonis Troumpoukis", "Vangelis Karkaletsis"], "title": "Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Artificial Intelligence (AI) technology epitomizes the complex challenges\nposed by human-made artifacts, particularly those widely integrated into\nsociety and exert significant influence, highlighting potential benefits and\ntheir negative consequences. While other technologies may also pose substantial\nrisks, AI's pervasive reach makes its societal effects especially profound. The\ncomplexity of AI systems, coupled with their remarkable capabilities, can lead\nto a reliance on technologies that operate beyond direct human oversight or\nunderstanding. To mitigate the risks that arise, several theoretical tools and\nguidelines have been developed, alongside efforts to create technological tools\naimed at safeguarding Trustworthy AI. The guidelines take a more holistic view\nof the issue but fail to provide techniques for quantifying trustworthiness.\nConversely, while technological tools are better at achieving such\nquantification, they lack a holistic perspective, focusing instead on specific\naspects of Trustworthy AI. This paper aims to introduce an assessment method\nthat combines the ethical components of Trustworthy AI with the algorithmic\nprocesses of PageRank and TrustRank. The goal is to establish an assessment\nframework that minimizes the subjectivity inherent in the self-assessment\ntechniques prevalent in the field by introducing algorithmic criteria. The\napplication of our approach indicates that a holistic assessment of an AI\nsystem's trustworthiness can be achieved by providing quantitative insights\nwhile considering the theoretical content of relevant guidelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u4f26\u7406\u7ec4\u4ef6\u4e0ePageRank/TrustRank\u7b97\u6cd5\u7684\u65b0\u8bc4\u4f30\u65b9\u6cd5\uff0c\u65e8\u5728\u5bf9AI\u7cfb\u7edf\u7684\u53ef\u4fe1\u8d56\u6027\u8fdb\u884c\u66f4\u5168\u9762\u3001\u5b9a\u91cf\u4e14\u5ba2\u89c2\u7684\u8bc4\u4f30\uff0c\u5f25\u5408\u73b0\u6709\u6307\u5357\u548c\u5de5\u5177\u7684\u4e0d\u8db3\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5e7f\u6cdb\u96c6\u6210\u5e76\u5bf9\u793e\u4f1a\u4ea7\u751f\u6df1\u8fdc\u5f71\u54cd\uff0c\u5176\u590d\u6742\u6027\u53ef\u80fd\u5bfc\u81f4\u8d85\u51fa\u4eba\u7c7b\u76d1\u7763\u548c\u7406\u89e3\u7684\u4f9d\u8d56\u3002\u73b0\u6709\u53ef\u4fe1\u8d56AI\u7684\u7406\u8bba\u5de5\u5177\u548c\u6307\u5357\u7f3a\u4e4f\u91cf\u5316\u80fd\u529b\uff0c\u800c\u6280\u672f\u5de5\u5177\u867d\u80fd\u5b9e\u73b0\u91cf\u5316\u5374\u7f3a\u4e4f\u6574\u4f53\u89c6\u89d2\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408\u4f26\u7406\u8003\u91cf\u4e0e\u91cf\u5316\u8bc4\u4f30\u3001\u5e76\u51cf\u5c11\u4e3b\u89c2\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u53ef\u4fe1\u8d56AI\u7684\u4f26\u7406\u7ec4\u6210\u90e8\u5206\u4e0ePageRank\u548cTrustRank\u7684\u7b97\u6cd5\u8fc7\u7a0b\u76f8\u7ed3\u5408\u3002\u76ee\u6807\u662f\u5efa\u7acb\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u7b97\u6cd5\u6807\u51c6\u6765\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u8be5\u9886\u57df\u666e\u904d\u5b58\u5728\u7684\u81ea\u6211\u8bc4\u4f30\u6280\u672f\u56fa\u6709\u7684\u4e3b\u89c2\u6027\u3002", "result": "\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u5e94\u7528\u8868\u660e\uff0c\u53ef\u4ee5\u901a\u8fc7\u63d0\u4f9b\u5b9a\u91cf\u89c1\u89e3\u5e76\u8003\u8651\u76f8\u5173\u6307\u5357\u7684\u7406\u8bba\u5185\u5bb9\uff0c\u5b9e\u73b0\u5bf9AI\u7cfb\u7edf\u53ef\u4fe1\u8d56\u6027\u7684\u6574\u4f53\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u4f26\u7406\u7ec4\u4ef6\u548c\u7b97\u6cd5\u8fc7\u7a0b\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u5bf9AI\u7cfb\u7edf\u7684\u53ef\u4fe1\u8d56\u6027\u8fdb\u884c\u5168\u9762\u3001\u5b9a\u91cf\u4e14\u4e3b\u89c2\u6027\u66f4\u4f4e\u7684\u8bc4\u4f30\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.22499", "pdf": "https://arxiv.org/pdf/2506.22499", "abs": "https://arxiv.org/abs/2506.22499", "authors": ["Jiachao Liu", "Pablo Guarda", "Koichiro Niinuma", "Sean Qian"], "title": "Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data", "categories": ["cs.CV", "cs.AI", "stat.AP"], "comment": null, "summary": "This study presents a novel integrated framework for dynamic\norigin-destination demand estimation (DODE) in multi-class mesoscopic network\nmodels, leveraging high-resolution satellite imagery together with conventional\ntraffic data from local sensors. Unlike sparse local detectors, satellite\nimagery offers consistent, city-wide road and traffic information of both\nparking and moving vehicles, overcoming data availability limitations. To\nextract information from imagery data, we design a computer vision pipeline for\nclass-specific vehicle detection and map matching, generating link-level\ntraffic density observations by vehicle class. Building upon this information,\nwe formulate a computational graph-based DODE model that calibrates dynamic\nnetwork states by jointly matching observed traffic counts and travel times\nfrom local sensors with density measurements derived from satellite imagery. To\nassess the accuracy and scalability of the proposed framework, we conduct a\nseries of numerical experiments using both synthetic and real-world data. The\nresults of out-of-sample tests demonstrate that supplementing traditional data\nwith satellite-derived density significantly improves estimation performance,\nespecially for links without local sensors. Real-world experiments also confirm\nthe framework's capability to handle large-scale networks, supporting its\npotential for practical deployment in cities of varying sizes. Sensitivity\nanalysis further evaluates the impact of data quality related to satellite\nimagery data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u96c6\u6210\u6846\u67b6\uff0c\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u548c\u4f20\u7edf\u4ea4\u901a\u6570\u636e\uff0c\u5bf9\u591a\u7c7b\u522b\u4ecb\u89c2\u7f51\u7edc\u4e2d\u7684\u52a8\u6001\u8d77\u8bab\u70b9\u9700\u6c42\uff08DODE\uff09\u8fdb\u884c\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f30\u8ba1\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u5c40\u90e8\u4f20\u611f\u5668\u6570\u636e\u7684\u8def\u6bb5\u3002", "motivation": "\u4f20\u7edf\u7684\u5c40\u90e8\u4ea4\u901a\u4f20\u611f\u5668\u6570\u636e\u7a00\u758f\uff0c\u5b58\u5728\u6570\u636e\u53ef\u7528\u6027\u9650\u5236\u3002\u536b\u661f\u56fe\u50cf\u80fd\u591f\u63d0\u4f9b\u4e00\u81f4\u7684\u3001\u8986\u76d6\u5168\u57ce\u7684\u9053\u8def\u548c\u4ea4\u901a\u4fe1\u606f\uff08\u5305\u62ec\u505c\u8f66\u548c\u79fb\u52a8\u8f66\u8f86\uff09\uff0c\u4ece\u800c\u514b\u670d\u4e86\u8fd9\u4e00\u6570\u636e\u53ef\u7528\u6027\u9650\u5236\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u96c6\u6210\u6846\u67b6\uff0c\u5c06\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u4e0e\u4f20\u7edf\u4ea4\u901a\u6570\u636e\u76f8\u7ed3\u5408\u8fdb\u884cDODE\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u8bbe\u8ba1\u8ba1\u7b97\u673a\u89c6\u89c9\u6d41\u7a0b\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u63d0\u53d6\u8f66\u8f86\u68c0\u6d4b\u548c\u5730\u56fe\u5339\u914d\u4fe1\u606f\uff0c\u751f\u6210\u94fe\u8def\u7ea7\u4ea4\u901a\u5bc6\u5ea6\u89c2\u6d4b\u503c\uff1b\u6784\u5efa\u57fa\u4e8e\u8ba1\u7b97\u56fe\u7684DODE\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u5339\u914d\u5c40\u90e8\u4f20\u611f\u5668\u7684\u4ea4\u901a\u91cf\u548c\u884c\u7a0b\u65f6\u95f4\u4e0e\u536b\u661f\u884d\u751f\u7684\u5bc6\u5ea6\u6d4b\u91cf\u503c\u6765\u6821\u51c6\u52a8\u6001\u7f51\u7edc\u72b6\u6001\u3002\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u8fdb\u884c\u4e86\u6570\u503c\u5b9e\u9a8c\uff0c\u5e76\u8fdb\u884c\u4e86\u6837\u672c\u5916\u6d4b\u8bd5\u548c\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7528\u536b\u661f\u884d\u751f\u7684\u5bc6\u5ea6\u8865\u5145\u4f20\u7edf\u6570\u636e\u663e\u8457\u63d0\u9ad8\u4e86DODE\u7684\u4f30\u8ba1\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u6ca1\u6709\u5c40\u90e8\u4f20\u611f\u5668\u7684\u8def\u6bb5\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e5f\u8bc1\u5b9e\u4e86\u8be5\u6846\u67b6\u5904\u7406\u5927\u89c4\u6a21\u7f51\u7edc\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u96c6\u6210\u6846\u67b6\u5177\u6709\u5728\u4e0d\u540c\u89c4\u6a21\u57ce\u5e02\u4e2d\u5b9e\u9645\u90e8\u7f72\u7684\u6f5c\u529b\uff0c\u56e0\u4e3a\u5b83\u80fd\u6709\u6548\u5229\u7528\u536b\u661f\u6570\u636e\u514b\u670d\u4f20\u7edf\u4f20\u611f\u5668\u6570\u636e\u7a00\u758f\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u52a8\u6001\u8d77\u8bab\u70b9\u9700\u6c42\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2506.22529", "pdf": "https://arxiv.org/pdf/2506.22529", "abs": "https://arxiv.org/abs/2506.22529", "authors": ["Lu Kalkbrenner", "Veronika Solopova", "Steffen Zeiler", "Robert Nickel", "Dorothea Kolossa"], "title": "MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages", "categories": ["cs.CL"], "comment": null, "summary": "Connectivity and message propagation are central, yet often underutilized,\nsources of information in misinformation detection -- especially on poorly\nmoderated platforms such as Telegram, which has become a critical channel for\nmisinformation dissemination, namely in the German electoral context. In this\npaper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based\ngraph dataset for misinformation detection. It includes over 5 million messages\nfrom public channels, enriched with metadata, channel relationships, and both\nweak and strong labels. These labels are derived via semantic similarity to\nfact-checks and news articles using M3-embeddings, as well as manual\nannotation. To establish reproducible baselines, we evaluate both text-only\nmodels and graph neural networks (GNNs) that incorporate message forwarding as\na network structure. Our results show that GraphSAGE with LSTM aggregation\nsignificantly outperforms text-only baselines in terms of Matthews Correlation\nCoefficient (MCC) and F1-score. We further evaluate the impact of subscribers,\nview counts, and automatically versus human-created labels on performance, and\nhighlight both the potential and challenges of weak supervision in this domain.\nThis work provides a reproducible benchmark and open dataset for future\nresearch on misinformation detection in German-language Telegram networks and\nother low-moderation social platforms.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165Misinfo-TeleGraph\uff0c\u9996\u4e2a\u5fb7\u8bedTelegram\u56fe\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\uff0c\u5e76\u8868\u660e\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5229\u7528\u6d88\u606f\u4f20\u64ad\u4fe1\u606f\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u7eaf\u6587\u672c\u6a21\u578b\u3002", "motivation": "\u5728Telegram\u7b49\u76d1\u7ba1\u4e0d\u529b\u7684\u5e73\u53f0\uff0c\u865a\u5047\u4fe1\u606f\u4f20\u64ad\uff08\u5c24\u5176\u5728\u5fb7\u56fd\u9009\u4e3e\u80cc\u666f\u4e0b\uff09\u65e5\u76ca\u4e25\u91cd\u3002\u8fde\u63a5\u6027\u548c\u6d88\u606f\u4f20\u64ad\u662f\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7684\u91cd\u8981\u4fe1\u606f\u6e90\uff0c\u4f46\u5e38\u88ab\u4f4e\u4f30\u548c\u672a\u5145\u5206\u5229\u7528\u3002", "method": "\u7814\u7a76\u6784\u5efa\u5e76\u53d1\u5e03\u4e86Misinfo-TeleGraph\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc7500\u4e07\u6761\u6765\u81ea\u5fb7\u8bedTelegram\u516c\u5171\u9891\u9053\u7684\u6d88\u606f\uff0c\u5e76\u4e30\u5bcc\u4e86\u5143\u6570\u636e\u3001\u9891\u9053\u5173\u7cfb\u4ee5\u53ca\u57fa\u4e8eM3-embeddings\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5339\u914d\u4e8b\u5b9e\u6838\u67e5/\u65b0\u95fb\u6587\u7ae0\u548c\u4eba\u5de5\u6807\u6ce8\u83b7\u5f97\u7684\u5f31/\u5f3a\u6807\u7b7e\u3002\u4e3a\u5efa\u7acb\u53ef\u590d\u73b0\u57fa\u7ebf\uff0c\u7814\u7a76\u8bc4\u4f30\u4e86\u7eaf\u6587\u672c\u6a21\u578b\u548c\u878d\u5165\u6d88\u606f\u8f6c\u53d1\u7ed3\u6784\uff08GraphSAGE\u7ed3\u5408LSTM\u805a\u5408\uff09\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u8bc4\u4f30\u4e86\u8ba2\u9605\u8005\u3001\u6d4f\u89c8\u91cf\u53ca\u81ea\u52a8/\u4eba\u5de5\u6807\u7b7e\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "GraphSAGE\u7ed3\u5408LSTM\u805a\u5408\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728Matthews\u76f8\u5173\u7cfb\u6570\uff08MCC\uff09\u548cF1-score\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u7eaf\u6587\u672c\u57fa\u7ebf\u6a21\u578b\u3002\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc4\u4f30\u4e86\u8ba2\u9605\u8005\u3001\u6d4f\u89c8\u91cf\u4ee5\u53ca\u81ea\u52a8\u4e0e\u4eba\u5de5\u521b\u5efa\u6807\u7b7e\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5f3a\u8c03\u4e86\u8be5\u9886\u57df\u5f31\u76d1\u7763\u7684\u6f5c\u529b\u548c\u6311\u6218\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5fb7\u8bedTelegram\u7f51\u7edc\u548c\u5176\u4ed6\u4f4e\u76d1\u7ba1\u793e\u4ea4\u5e73\u53f0\u4e0a\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u548c\u5f00\u653e\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.22484", "pdf": "https://arxiv.org/pdf/2506.22484", "abs": "https://arxiv.org/abs/2506.22484", "authors": ["Muhammad Kabeer", "Rosdiadee Nordin", "Mehran Behjati", "Farah Yasmin binti Mohd Shaharuddin"], "title": "An Urban Multi-Operator QoE-Aware Dataset for Cellular Networks in Dense Environments", "categories": ["cs.NI", "eess.SP"], "comment": "17 pages, 9 Figures", "summary": "Urban cellular networks face complex performance challenges due to high\ninfrastructure density, varied user mobility, and diverse service demands.\nWhile several datasets address network behaviour across different environments,\nthere is a lack of datasets that captures user centric Quality of Experience\n(QoE), and diverse mobility patterns needed for efficient network planning and\noptimization solutions, which are important for QoE driven optimizations and\nmobility management. This study presents a curated dataset of 30,925 labelled\nrecords, collected using GNetTrack Pro within a 2 km2 dense urban area,\nspanning three major commercial network operators. The dataset captures key\nsignal quality parameters (e.g., RSRP, RSRQ, SNR), across multiple real world\nmobility modes including pedestrian routes, canopy walkways, shuttle buses, and\nBus Rapid Transit (BRT) routes. It also includes diverse network traffic\nscenarios including (1) FTP upload and download, (2) video streaming, and (3)\nHTTP browsing. A total of 132 physical cell sites were identified and validated\nthrough OpenCellID and on-site field inspections, illustrating the high cell\ndensity characteristic of 5G and emerging heterogeneous network deployment. The\ndataset is particularly suited for machine learning applications, such as\nhandover optimization, signal quality prediction, and multi operator\nperformance evaluation. Released in a structured CSV format with accompanying\npreprocessing and visualization scripts, this dataset offers a reproducible,\napplication ready resource for researchers and practitioners working on urban\ncellular network planning and optimization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u57ce\u5e02\u8702\u7a9d\u7f51\u7edc\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u7528\u6237\u4f53\u9a8c\u8d28\u91cf\uff08QoE\uff09\u548c\u591a\u6837\u5316\u79fb\u52a8\u6a21\u5f0f\uff0c\u4ee5\u652f\u6301\u7f51\u7edc\u89c4\u5212\u548c\u4f18\u5316\uff0c\u7279\u522b\u9002\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u6355\u83b7\u7528\u6237\u4e2d\u5fc3QoE\u548c\u591a\u6837\u5316\u79fb\u52a8\u6a21\u5f0f\u7684\u6570\u636e\uff0c\u800c\u8fd9\u5bf9\u4e8e\u9ad8\u6548\u7684\u7f51\u7edc\u89c4\u5212\u3001\u4f18\u5316\u3001QoE\u9a71\u52a8\u7684\u4f18\u5316\u548c\u79fb\u52a8\u7ba1\u7406\u5728\u590d\u6742\u57ce\u5e02\u8702\u7a9d\u7f51\u7edc\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u57ce\u5e02\u7f51\u7edc\u9762\u4e34\u9ad8\u57fa\u7840\u8bbe\u65bd\u5bc6\u5ea6\u3001\u591a\u53d8\u7528\u6237\u79fb\u52a8\u6027\u548c\u591a\u6837\u670d\u52a1\u9700\u6c42\u7684\u6311\u6218\u3002", "method": "\u7814\u7a76\u4eba\u5458\u4f7f\u7528GNetTrack Pro\u5de5\u5177\uff0c\u57282\u5e73\u65b9\u516c\u91cc\u7684\u57ce\u5e02\u5bc6\u96c6\u533a\u57df\u5185\uff0c\u4ece\u4e09\u5927\u4e3b\u8981\u5546\u4e1a\u7f51\u7edc\u8fd0\u8425\u5546\u5904\u6536\u96c6\u4e8630,925\u6761\u5e26\u6807\u7b7e\u7684\u8bb0\u5f55\u3002\u6570\u636e\u96c6\u6db5\u76d6\u4e86\u5173\u952e\u4fe1\u53f7\u8d28\u91cf\u53c2\u6570\uff08\u5982RSRP\u3001RSRQ\u3001SNR\uff09\u3001\u591a\u79cd\u771f\u5b9e\u4e16\u754c\u79fb\u52a8\u6a21\u5f0f\uff08\u5305\u62ec\u6b65\u884c\u3001\u9ad8\u67b6\u6b65\u9053\u3001\u7a7f\u68ad\u5df4\u58eb\u548c\u5feb\u901f\u516c\u4ea4\u8def\u7ebf\uff09\uff0c\u4ee5\u53ca\u591a\u6837\u5316\u7684\u7f51\u7edc\u6d41\u91cf\u573a\u666f\uff08FTP\u4e0a\u4f20\u4e0b\u8f7d\u3001\u89c6\u9891\u6d41\u548cHTTP\u6d4f\u89c8\uff09\u3002\u6b64\u5916\uff0c\u901a\u8fc7OpenCellID\u548c\u73b0\u573a\u68c0\u67e5\u8bc6\u522b\u5e76\u9a8c\u8bc1\u4e86132\u4e2a\u7269\u7406\u8702\u7a9d\u57fa\u7ad9\u3002", "result": "\u7814\u7a76\u6210\u529f\u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b30,925\u6761\u5e26\u6807\u7b7e\u8bb0\u5f55\u7684\u7b56\u5c55\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6355\u83b7\u4e86\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e2d\u591a\u8fd0\u8425\u5546\u7684\u5173\u952e\u4fe1\u53f7\u8d28\u91cf\u53c2\u6570\u3001\u591a\u79cd\u771f\u5b9e\u4e16\u754c\u79fb\u52a8\u6a21\u5f0f\u4e0b\u7684\u6027\u80fd\u6570\u636e\u4ee5\u53ca\u591a\u6837\u5316\u7f51\u7edc\u6d41\u91cf\u573a\u666f\u7684\u6570\u636e\u3002\u6570\u636e\u8fd8\u53cd\u6620\u4e865G\u548c\u65b0\u5174\u5f02\u6784\u7f51\u7edc\u90e8\u7f72\u7684\u9ad8\u8702\u7a9d\u5bc6\u5ea6\u7279\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6570\u636e\u96c6\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u7f3a\u4e4f\u7528\u6237\u4e2d\u5fc3QoE\u548c\u591a\u6837\u5316\u79fb\u52a8\u6a21\u5f0f\u7684\u7a7a\u767d\uff0c\u4e3a\u57ce\u5e02\u8702\u7a9d\u7f51\u7edc\u89c4\u5212\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u3001\u53ef\u5e94\u7528\u7684\u8d44\u6e90\u3002\u8be5\u6570\u636e\u96c6\u7279\u522b\u9002\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u5e94\u7528\uff0c\u5982\u5207\u6362\u4f18\u5316\u3001\u4fe1\u53f7\u8d28\u91cf\u9884\u6d4b\u548c\u591a\u8fd0\u8425\u5546\u6027\u80fd\u8bc4\u4f30\u3002"}}
{"id": "2506.22446", "pdf": "https://arxiv.org/pdf/2506.22446", "abs": "https://arxiv.org/abs/2506.22446", "authors": ["Aakash Tripathi", "Asim Waqas", "Matthew B. Schabath", "Yasin Yilmaz", "Ghulam Rasool"], "title": "EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate cancer survival prediction requires integration of diverse data\nmodalities that reflect the complex interplay between imaging, clinical\nparameters, and textual reports. However, existing multimodal approaches suffer\nfrom simplistic fusion strategies, massive computational requirements, and lack\nof interpretability-critical barriers to clinical adoption. We present EAGLE\n(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning\nframework that addresses these limitations through attention-based multimodal\nfusion with comprehensive attribution analysis. EAGLE introduces four key\ninnovations: (1) dynamic cross-modal attention mechanisms that learn\nhierarchical relationships between modalities, (2) massive dimensionality\nreduction (99.96%) while maintaining predictive performance, (3) three\ncomplementary attribution methods providing patient-level interpretability, and\n(4) a unified pipeline enabling seamless adaptation across cancer types. We\nevaluated EAGLE on 911 patients across three distinct malignancies:\nglioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,\nn=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis\nshowed high-risk individuals relied more heavily on adverse imaging features,\nwhile low-risk patients demonstrated balanced modality contributions. Risk\nstratification identified clinically meaningful groups with 4-fold (GBM) to\n5-fold (NSCLC) differences in median survival, directly informing treatment\nintensity decisions. By combining state-of-the-art performance with clinical\ninterpretability, EAGLE bridges the gap between advanced AI capabilities and\npractical healthcare deployment, offering a scalable solution for multimodal\nsurvival prediction that enhances both prognostic accuracy and physician trust\nin automated predictions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEAGLE\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u764c\u75c7\u751f\u5b58\u9884\u6d4b\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u5e76\u4fc3\u8fdb\u4e34\u5e8a\u5e94\u7528\u3002", "motivation": "\u51c6\u786e\u7684\u764c\u75c7\u751f\u5b58\u9884\u6d4b\u9700\u6574\u5408\u5f71\u50cf\u3001\u4e34\u5e8a\u53c2\u6570\u548c\u6587\u672c\u62a5\u544a\u7b49\u591a\u6a21\u6001\u6570\u636e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u878d\u5408\u7b56\u7565\u7b80\u5355\u3001\u8ba1\u7b97\u9700\u6c42\u9ad8\u53ca\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7b49\u95ee\u9898\uff0c\u8fd9\u4e9b\u662f\u963b\u788d\u5176\u4e34\u5e8a\u5e94\u7528\u7684\u5173\u952e\u969c\u788d\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86EAGLE\uff08\u5e7f\u4e49\u6f5c\u5728\u5d4c\u5165\u7684\u9ad8\u6548\u5bf9\u9f50\uff09\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4ee5\u4e0b\u56db\u9879\u6838\u5fc3\u521b\u65b0\u89e3\u51b3\u4e0a\u8ff0\u5c40\u9650\uff1a1) \u52a8\u6001\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u6a21\u6001\u95f4\u7684\u5c42\u7ea7\u5173\u7cfb\uff1b2) \u5728\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u5927\u89c4\u6a21\u964d\u7ef4\uff0899.96%\uff09\uff1b3) \u63d0\u4f9b\u4e09\u79cd\u4e92\u8865\u7684\u5f52\u56e0\u65b9\u6cd5\uff0c\u5b9e\u73b0\u60a3\u8005\u5c42\u9762\u7684\u53ef\u89e3\u91ca\u6027\uff1b4) \u7edf\u4e00\u7684\u7ba1\u9053\u53ef\u65e0\u7f1d\u9002\u5e94\u4e0d\u540c\u764c\u75c7\u7c7b\u578b\u3002", "result": "EAGLE\u5728911\u540d\u60a3\u8005\uff08\u6db5\u76d6\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u3001\u80f0\u817a\u5bfc\u7ba1\u5185\u4e73\u5934\u72b6\u9ecf\u6db2\u6027\u80bf\u7624\u548c\u975e\u5c0f\u7ec6\u80de\u80ba\u764c\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u60a3\u8005\u5c42\u9762\u5206\u6790\u663e\u793a\uff0c\u9ad8\u98ce\u9669\u4e2a\u4f53\u66f4\u4f9d\u8d56\u4e0d\u5229\u7684\u5f71\u50cf\u7279\u5f81\uff0c\u800c\u4f4e\u98ce\u9669\u60a3\u8005\u8868\u73b0\u51fa\u6a21\u6001\u8d21\u732e\u7684\u5e73\u8861\u3002\u98ce\u9669\u5206\u5c42\u8bc6\u522b\u51fa\u4e34\u5e8a\u4e0a\u6709\u610f\u4e49\u7684\u7fa4\u4f53\uff0c\u5176\u4e2d\u4f4d\u751f\u5b58\u671f\u5dee\u5f02\u4ece4\u500d\uff08\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\uff09\u52305\u500d\uff08\u975e\u5c0f\u7ec6\u80de\u80ba\u764c\uff09\u4e0d\u7b49\uff0c\u53ef\u76f4\u63a5\u6307\u5bfc\u6cbb\u7597\u5f3a\u5ea6\u51b3\u7b56\u3002EAGLE\u7ed3\u5408\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "EAGLE\u901a\u8fc7\u7ed3\u5408\u5353\u8d8a\u6027\u80fd\u4e0e\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\uff0c\u5f25\u5408\u4e86\u5148\u8fdb\u4eba\u5de5\u667a\u80fd\u80fd\u529b\u4e0e\u5b9e\u9645\u533b\u7597\u90e8\u7f72\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u591a\u6a21\u6001\u751f\u5b58\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u9884\u540e\u51c6\u786e\u6027\u53ca\u533b\u751f\u5bf9\u81ea\u52a8\u5316\u9884\u6d4b\u7684\u4fe1\u4efb\u3002"}}
{"id": "2506.22865", "pdf": "https://arxiv.org/pdf/2506.22865", "abs": "https://arxiv.org/abs/2506.22865", "authors": ["Ziqi Zhong", "Xunzhu Tang"], "title": "ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have revealed a\nsignificant performance gap between closed-source and open-source models,\nparticularly in tasks requiring complex reasoning and precise instruction\nfollowing. This paper introduces ReasonBridge, a methodology that efficiently\ntransfers reasoning capabilities from powerful closed-source to open-source\nmodels through a novel hierarchical knowledge distillation framework. We\ndevelop a tailored dataset Reason1K with only 1,000 carefully curated reasoning\ntraces emphasizing difficulty, diversity, and quality. These traces are\nfiltered from across multiple domains using a structured multi-criteria\nselection algorithm. Our transfer learning approach incorporates: (1) a\nhierarchical distillation process capturing both strategic abstraction and\ntactical implementation patterns, (2) a sparse reasoning-focused adapter\narchitecture requiring only 0.3% additional trainable parameters, and (3) a\ntest-time compute scaling mechanism using guided inference interventions.\nComprehensive evaluations demonstrate that ReasonBridge improves reasoning\ncapabilities in open-source models by up to 23% on benchmark tasks,\nsignificantly narrowing the gap with closed-source models. Notably, the\nenhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its\nperformance on competition-level AIME problems. Our methodology generalizes\neffectively across diverse reasoning domains and model architectures,\nestablishing a sample-efficient approach to reasoning enhancement for\ninstruction following.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReasonBridge\uff0c\u4e00\u79cd\u6837\u672c\u9ad8\u6548\u7684\u5206\u5c42\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5c11\u91cf\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u7a00\u758f\u9002\u914d\u5668\uff0c\u5c06\u95ed\u6e90\u5927\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u5230\u5f00\u6e90\u6a21\u578b\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u6027\u80fd\u5dee\u8ddd\uff0c\u4f7f\u5f00\u6e90\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u4e86\u9876\u5c16\u95ed\u6e90\u6a21\u578b\u3002", "motivation": "\u95ed\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u548c\u7cbe\u786e\u6307\u4ee4\u9075\u5faa\u4efb\u52a1\u4e0a\uff0c\u4e0e\u5f00\u6e90\u6a21\u578b\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u5f15\u5165ReasonBridge\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u5206\u5c42\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u95ed\u6e90\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u5230\u5f00\u6e90\u6a21\u578b\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1) \u6784\u5efa\u4ec5\u542b1000\u6761\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\u7684\u5b9a\u5236\u6570\u636e\u96c6Reason1K\uff1b2) \u91c7\u7528\u5206\u5c42\u84b8\u998f\u8fc7\u7a0b\uff0c\u6355\u83b7\u6218\u7565\u62bd\u8c61\u548c\u6218\u672f\u5b9e\u73b0\u6a21\u5f0f\uff1b3) \u8bbe\u8ba1\u7a00\u758f\u63a8\u7406\u9002\u914d\u5668\u67b6\u6784\uff0c\u4ec5\u97000.3%\u7684\u989d\u5916\u53ef\u8bad\u7ec3\u53c2\u6570\uff1b4) \u5b9e\u65bd\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u7f29\u653e\u673a\u5236\u3002", "result": "ReasonBridge\u4f7f\u5f00\u6e90\u6a21\u578b\u5728\u57fa\u51c6\u4efb\u52a1\u4e0a\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u9ad8\u8fbe23%\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u95ed\u6e90\u6a21\u578b\u7684\u5dee\u8ddd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u589e\u5f3a\u540e\u7684Qwen2.5-14B\u5728MATH500\u4e0a\u8d85\u8d8a\u4e86Claude-Sonnet3.5\uff0c\u5e76\u5728\u7ade\u8d5b\u7ea7AIME\u95ee\u9898\u4e0a\u4e0e\u5176\u6027\u80fd\u6301\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6837\u672c\u9ad8\u6548\u7684\u63a8\u7406\u589e\u5f3a\u65b9\u6848\uff0c\u53ef\u6709\u6548\u6cdb\u5316\u5230\u4e0d\u540c\u63a8\u7406\u9886\u57df\u548c\u6a21\u578b\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u6e90\u6a21\u578b\u5728\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2506.22500", "pdf": "https://arxiv.org/pdf/2506.22500", "abs": "https://arxiv.org/abs/2506.22500", "authors": ["Weiyi Zhao", "Xiaoyu Tan", "Liang Liu", "Sijia Li", "Youwei Song", "Xihe Qiu"], "title": "Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "68T07, 68U10, 92C55", "I.2.10; I.2.7; J.3; I.2.6"], "comment": "13 pages, 5 figures. The dataset and appendix are available at\n  https://github.com/zgg2577/VS-KC", "summary": "Surgical risk identification is critical for patient safety and reducing\npreventable medical errors. While multimodal large language models (MLLMs) show\npromise for automated operating room (OR) risk detection, they often exhibit\nvisual-semantic knowledge conflicts (VS-KC), failing to identify visual safety\nviolations despite understanding textual rules. To address this, we introduce a\ndataset comprising over 34,000 synthetic images generated by diffusion models,\ndepicting operating room scenes containing entities that violate established\nsafety rules. These images were created to alleviate data scarcity and examine\nMLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated\nimages that serve as a gold-standard reference for validation. This\ncomprehensive dataset, spanning diverse perspectives, stages, and\nconfigurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC\nsignificantly improves MLLMs' detection of trained conflict entities and\ngeneralizes well to new viewpoints for these entities, but performance on\nuntrained entity types remains poor, highlighting learning specificity and the\nneed for comprehensive training. The main contributions of this work include:\n(1) a data generation methodology tailored for rule-violation scenarios; (2)\nthe release of the OR-VSKC dataset and its associated benchmark as open-source\nresources; and (3) an empirical analysis of violation-sensitive knowledge\nconsistency in representative MLLMs. The dataset and appendix are available at\nhttps://github.com/zgg2577/VS-KC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOR-VSKC\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc734,000\u5f20\u5408\u6210\u56fe\u50cf\u548c214\u5f20\u4eba\u5de5\u6807\u6ce8\u56fe\u50cf\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u624b\u672f\u5ba4\u98ce\u9669\u8bc6\u522b\u4e2d\u5b58\u5728\u7684\u89c6\u89c9-\u8bed\u4e49\u77e5\u8bc6\u51b2\u7a81\uff08VS-KC\uff09\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u63d0\u5347\u5176\u5bf9\u8fdd\u89c4\u5b9e\u4f53\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u624b\u672f\u98ce\u9669\u8bc6\u522b\u5bf9\u60a3\u8005\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u81ea\u52a8\u5316\u624b\u672f\u5ba4\u98ce\u9669\u68c0\u6d4b\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u5e38\u8868\u73b0\u51fa\u89c6\u89c9-\u8bed\u4e49\u77e5\u8bc6\u51b2\u7a81\uff08VS-KC\uff09\uff0c\u5373\u5c3d\u7ba1\u7406\u89e3\u6587\u672c\u89c4\u5219\uff0c\u5374\u672a\u80fd\u8bc6\u522b\u89c6\u89c9\u5b89\u5168\u8fdd\u89c4\u3002\u6b64\u5916\uff0c\u6b64\u7c7b\u573a\u666f\u7684\u6570\u636e\u7a00\u7f3a\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aOR-VSKC\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc734,000\u5f20\u7531\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u624b\u672f\u5ba4\u573a\u666f\u56fe\u50cf\uff08\u5305\u542b\u8fdd\u53cd\u5b89\u5168\u89c4\u5219\u7684\u5b9e\u4f53\uff09\uff0c\u4ee5\u53ca214\u5f20\u4eba\u5de5\u6807\u6ce8\u56fe\u50cf\u4f5c\u4e3a\u9ec4\u91d1\u6807\u51c6\u9a8c\u8bc1\u53c2\u8003\u3002\u5f00\u53d1\u4e86\u9488\u5bf9\u89c4\u5219\u8fdd\u53cd\u573a\u666f\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528OR-VSKC\u6570\u636e\u96c6\u5bf9MLLMs\u8fdb\u884c\u5fae\u8c03\uff0c\u968f\u540e\u5bf9\u4ee3\u8868\u6027MLLMs\u7684\u8fdd\u89c4\u654f\u611f\u77e5\u8bc6\u4e00\u81f4\u6027\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u5728OR-VSKC\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u663e\u8457\u63d0\u9ad8\u4e86MLLMs\u5bf9\u5df2\u8bad\u7ec3\u51b2\u7a81\u5b9e\u4f53\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u8fd9\u4e9b\u5b9e\u4f53\u7684\u65b0\u89c6\u89d2\u4e0b\u6cdb\u5316\u826f\u597d\u3002\u7136\u800c\uff0c\u6a21\u578b\u5bf9\u672a\u7ecf\u8bad\u7ec3\u7684\u5b9e\u4f53\u7c7b\u578b\u7684\u6027\u80fd\u4ecd\u7136\u4e0d\u4f73\uff0c\u8fd9\u7a81\u51fa\u4e86\u5b66\u4e60\u7684\u7279\u5f02\u6027\u4ee5\u53ca\u5168\u9762\u8bad\u7ec3\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a1) \u4e00\u79cd\u9488\u5bf9\u89c4\u5219\u8fdd\u53cd\u573a\u666f\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff1b2) \u53d1\u5e03\u4e86OR-VSKC\u6570\u636e\u96c6\u53ca\u5176\u76f8\u5173\u57fa\u51c6\u4f5c\u4e3a\u5f00\u6e90\u8d44\u6e90\uff1b3) \u5bf9\u4ee3\u8868\u6027MLLMs\u4e2d\u8fdd\u89c4\u654f\u611f\u77e5\u8bc6\u4e00\u81f4\u6027\u7684\u5b9e\u8bc1\u5206\u6790\u3002\u7814\u7a76\u63ed\u793a\u4e86MLLMs\u5728\u5904\u7406\u89c6\u89c9-\u8bed\u4e49\u77e5\u8bc6\u51b2\u7a81\u65b9\u9762\u7684\u8fdb\u6b65\u53ca\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u5168\u9762\u8bad\u7ec3\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.22598", "pdf": "https://arxiv.org/pdf/2506.22598", "abs": "https://arxiv.org/abs/2506.22598", "authors": ["Nicholas Edwards", "Yukyung Lee", "Yujun", "Mao", "Yulu Qin", "Sebastian Schuster", "Najoung Kim"], "title": "RExBench: Can coding agents autonomously implement AI research extensions?", "categories": ["cs.CL"], "comment": null, "summary": "Agents based on Large Language Models (LLMs) have shown promise for\nperforming sophisticated software engineering tasks autonomously. In addition,\nthere has been progress towards developing agents that can perform parts of the\nresearch pipeline in machine learning and the natural sciences. We argue that\nresearch extension and its implementation is a critical capability for such\nsystems, and introduce RExBench to support the evaluation of this capability.\nRExBench is a benchmark consisting of 12 realistic research experiment\nimplementation tasks that aim to investigate research hypotheses that have not\npreviously been implemented. Each task is set up as an extension to an existing\nresearch paper and codebase, accompanied by domain expert-written instructions.\nRExBench is robust to data contamination, and supports an automatic evaluation\ninfrastructure that executes agent outputs to determine whether the success\ncriteria are met. We use this benchmark to evaluate nine LLM agents implemented\nusing three different frameworks: aider, Claude Code, and OpenHands. We find\nthat all agents evaluated fail to autonomously implement the majority of the\nextensions. Although the success rate improves with additional human-written\nhints, the best performance under this setting remains below 40%. This\nindicates that current agents are still short of being able to handle realistic\nresearch extension tasks without substantial human guidance.", "AI": {"tldr": "LLM\u4ee3\u7406\u5728\u81ea\u4e3b\u6267\u884c\u7814\u7a76\u6269\u5c55\u4efb\u52a1\u65b9\u9762\u80fd\u529b\u4e0d\u8db3\u3002\u4f5c\u8005\u5f15\u5165RExBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e869\u4e2aLLM\u4ee3\u7406\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u591a\u6570\u4efb\u52a1\u4e2d\u5931\u8d25\uff0c\u5373\u4f7f\u6709\u4eba\u5de5\u63d0\u793a\uff0c\u6700\u4f73\u6027\u80fd\u4e5f\u4f4e\u4e8e40%\uff0c\u8868\u660e\u4ecd\u9700\u5927\u91cf\u4eba\u5de5\u6307\u5bfc\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u8f6f\u4ef6\u5de5\u7a0b\u548c\u90e8\u5206\u7814\u7a76\u6d41\u7a0b\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u7814\u7a76\u6269\u5c55\u53ca\u5176\u5b9e\u73b0\u662f\u5173\u952e\u80fd\u529b\uff0c\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8861\u91cf\u548c\u63d0\u5347\u8fd9\u9879\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86RExBench\u57fa\u51c6\uff0c\u5305\u542b12\u4e2a\u57fa\u4e8e\u73b0\u6709\u8bba\u6587\u548c\u4ee3\u7801\u5e93\u7684\u771f\u5b9e\u7814\u7a76\u5b9e\u9a8c\u5b9e\u73b0\u4efb\u52a1\uff0c\u65e8\u5728\u6d4b\u8bd5\u672a\u66fe\u5b9e\u73b0\u7684\u5047\u8bbe\u3002\u8be5\u57fa\u51c6\u7531\u9886\u57df\u4e13\u5bb6\u7f16\u5199\u6307\u5bfc\uff0c\u80fd\u62b5\u6297\u6570\u636e\u6c61\u67d3\uff0c\u5e76\u652f\u6301\u81ea\u52a8\u8bc4\u4f30\u3002\u7814\u7a76\u8005\u4f7f\u7528RExBench\u8bc4\u4f30\u4e869\u4e2a\u57fa\u4e8eaider\u3001Claude Code\u548cOpenHands\u4e09\u4e2a\u6846\u67b6\u7684LLM\u4ee3\u7406\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u6240\u6709LLM\u4ee3\u7406\u90fd\u672a\u80fd\u81ea\u4e3b\u5b9e\u73b0\u5927\u591a\u6570\u7814\u7a76\u6269\u5c55\u4efb\u52a1\u3002\u5c3d\u7ba1\u589e\u52a0\u4eba\u5de5\u63d0\u793a\u80fd\u63d0\u9ad8\u6210\u529f\u7387\uff0c\u4f46\u6700\u4f73\u8868\u73b0\u4ecd\u4f4e\u4e8e40%\u3002", "conclusion": "\u76ee\u524d\u7684LLM\u4ee3\u7406\u5728\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u7684\u7814\u7a76\u6269\u5c55\u4efb\u52a1\u65f6\uff0c\u4ecd\u8fdc\u672a\u80fd\u505a\u5230\u65e0\u9700\u5927\u91cf\u4eba\u5de5\u6307\u5bfc\u3002"}}
{"id": "2506.22487", "pdf": "https://arxiv.org/pdf/2506.22487", "abs": "https://arxiv.org/abs/2506.22487", "authors": ["Amar Khelloufi", "Huansheng Ning", "Sahraoui Dhelim", "Jianguo Ding"], "title": "AGI Enabled Solutions For IoX Layers Bottlenecks In Cyber-Physical-Social-Thinking Space", "categories": ["cs.NI", "cs.AI"], "comment": "31 pages, 5 figures", "summary": "The integration of the Internet of Everything (IoX) and Artificial General\nIntelligence (AGI) has given rise to a transformative paradigm aimed at\naddressing critical bottlenecks across sensing, network, and application layers\nin Cyber-Physical-Social Thinking (CPST) ecosystems. In this survey, we provide\na systematic and comprehensive review of AGI-enhanced IoX research, focusing on\nthree key components: sensing-layer data management, network-layer protocol\noptimization, and application-layer decision-making frameworks. Specifically,\nthis survey explores how AGI can mitigate IoX bottlenecks challenges by\nleveraging adaptive sensor fusion, edge preprocessing, and selective attention\nmechanisms at the sensing layer, while resolving network-layer issues such as\nprotocol heterogeneity and dynamic spectrum management, neuro-symbolic\nreasoning, active inference, and causal reasoning, Furthermore, the survey\nexamines AGI-enabled frameworks for managing identity and relationship\nexplosion. Key findings suggest that AGI-driven strategies, such as adaptive\nsensor fusion, edge preprocessing, and semantic modeling, offer novel solutions\nto sensing-layer data overload, network-layer protocol heterogeneity, and\napplication-layer identity explosion. The survey underscores the importance of\ncross-layer integration, quantum-enabled communication, and ethical governance\nframeworks for future AGI-enabled IoX systems. Finally, the survey identifies\nunresolved challenges, such as computational requirements, scalability, and\nreal-world validation, calling for further research to fully realize AGI's\npotential in addressing IoX bottlenecks. we believe AGI-enhanced IoX is\nemerging as a critical research field at the intersection of interconnected\nsystems and advanced AI.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86AGI\u589e\u5f3a\u7684IoX\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86AGI\u5982\u4f55\u89e3\u51b3CPST\u751f\u6001\u7cfb\u7edf\u4e2d\u611f\u77e5\u3001\u7f51\u7edc\u548c\u5e94\u7528\u5c42\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u548c\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u7269\u8054\u7f51\uff08IoX\uff09\u4e0e\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u878d\u5408\u5728\u7f51\u7edc\u7269\u7406\u793e\u4f1a\u601d\u7ef4\uff08CPST\uff09\u751f\u6001\u7cfb\u7edf\u7684\u611f\u77e5\u3001\u7f51\u7edc\u548c\u5e94\u7528\u5c42\u4e2d\u9047\u5230\u7684\u5173\u952e\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u5bf9AGI\u589e\u5f3a\u7684IoX\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u548c\u5168\u9762\u7684\u7efc\u8ff0\u3002", "method": "\u672c\u6587\u4f5c\u4e3a\u4e00\u7bc7\u7efc\u8ff0\u6027\u7814\u7a76\uff0c\u7cfb\u7edf\u56de\u987e\u4e86AGI\u589e\u5f3a\u7684IoX\u7814\u7a76\uff0c\u91cd\u70b9\u5173\u6ce8\u611f\u77e5\u5c42\u6570\u636e\u7ba1\u7406\u3001\u7f51\u7edc\u5c42\u534f\u8bae\u4f18\u5316\u548c\u5e94\u7528\u5c42\u51b3\u7b56\u6846\u67b6\u3002\u5177\u4f53\u63a2\u8ba8\u4e86AGI\u5982\u4f55\u5229\u7528\u81ea\u9002\u5e94\u4f20\u611f\u5668\u878d\u5408\u3001\u8fb9\u7f18\u9884\u5904\u7406\u3001\u9009\u62e9\u6027\u6ce8\u610f\u673a\u5236\u3001\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u3001\u4e3b\u52a8\u63a8\u7406\u548c\u56e0\u679c\u63a8\u7406\u7b49\u65b9\u6cd5\u6765\u7f13\u89e3IoX\u74f6\u9888\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cAGI\u9a71\u52a8\u7684\u7b56\u7565\uff08\u5982\u81ea\u9002\u5e94\u4f20\u611f\u5668\u878d\u5408\u3001\u8fb9\u7f18\u9884\u5904\u7406\u548c\u8bed\u4e49\u5efa\u6a21\uff09\u4e3a\u611f\u77e5\u5c42\u6570\u636e\u8fc7\u8f7d\u3001\u7f51\u7edc\u5c42\u534f\u8bae\u5f02\u6784\u6027\u548c\u5e94\u7528\u5c42\u8eab\u4efd\u7206\u70b8\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6b64\u5916\uff0c\u7814\u7a76\u5f3a\u8c03\u4e86\u672a\u6765AGI\u589e\u5f3a\u7684IoX\u7cfb\u7edf\u4e2d\u8de8\u5c42\u96c6\u6210\u3001\u91cf\u5b50\u901a\u4fe1\u548c\u4f26\u7406\u6cbb\u7406\u6846\u67b6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u5c3d\u7ba1AGI\u589e\u5f3a\u7684IoX\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u4ecd\u9762\u4e34\u8ba1\u7b97\u9700\u6c42\u3001\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u9a8c\u8bc1\u7b49\u5c1a\u672a\u89e3\u51b3\u7684\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6df1\u5165\u7814\u7a76\u3002AGI\u589e\u5f3a\u7684IoX\u6b63\u5728\u6210\u4e3a\u4e92\u8054\u7cfb\u7edf\u548c\u5148\u8fdbAI\u4ea4\u53c9\u9886\u57df\u7684\u4e00\u4e2a\u65b0\u5174\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.22447", "pdf": "https://arxiv.org/pdf/2506.22447", "abs": "https://arxiv.org/abs/2506.22447", "authors": ["Fabio Merizzi", "Harilaos Loukos"], "title": "Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture", "categories": ["cs.LG", "cs.AI", "eess.IV"], "comment": null, "summary": "Global Climate Models (GCMs) are critical for simulating large-scale climate\ndynamics, but their coarse spatial resolution limits their applicability in\nregional studies. Regional Climate Models (RCMs) refine this through dynamic\ndownscaling, albeit at considerable computational cost and with limited\nflexibility. While deep learning has emerged as an efficient data-driven\nalternative, most existing studies have focused on single-variable models that\ndownscale one variable at a time. This approach can lead to limited contextual\nawareness, redundant computation, and lack of cross-variable interaction. Our\nstudy addresses these limitations by proposing a multi-task, multi-variable\nVision Transformer (ViT) architecture with a shared encoder and\nvariable-specific decoders (1EMD). The proposed architecture jointly predicts\nthree key climate variables: surface temperature (tas), wind speed (sfcWind),\nand 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,\nemulating RCM-scale downscaling over Europe. We show that our multi-variable\napproach achieves positive cross-variable knowledge transfer and consistently\noutperforms single-variable baselines trained under identical conditions, while\nalso improving computational efficiency. These results demonstrate the\neffectiveness of multi-variable modeling for high-resolution climate\ndownscaling.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u591a\u53d8\u91cf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u7f16\u7801\u5668\u548c\u53d8\u91cf\u7279\u5b9a\u89e3\u7801\u5668\u7684Vision Transformer\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u9ad8\u5206\u8fa8\u7387\u6c14\u5019\u964d\u5c3a\u5ea6\u3002", "motivation": "\u5168\u7403\u6c14\u5019\u6a21\u578b\uff08GCMs\uff09\u7a7a\u95f4\u5206\u8fa8\u7387\u7c97\u7cd9\uff0c\u4e0d\u9002\u7528\u4e8e\u533a\u57df\u7814\u7a76\uff1b\u533a\u57df\u6c14\u5019\u6a21\u578b\uff08RCMs\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u7075\u6d3b\u6027\u53d7\u9650\u3002\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u964d\u5c3a\u5ea6\u591a\u96c6\u4e2d\u4e8e\u5355\u53d8\u91cf\u6a21\u578b\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u611f\u77e5\u4e0d\u8db3\u3001\u91cd\u590d\u8ba1\u7b97\u53ca\u53d8\u91cf\u95f4\u4ea4\u4e92\u7f3a\u5931\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u591a\u4efb\u52a1\u3001\u591a\u53d8\u91cf\u7684Vision Transformer\uff08ViT\uff09\u67b6\u6784\uff081EMD\uff09\uff0c\u5305\u542b\u5171\u4eab\u7f16\u7801\u5668\u548c\u53d8\u91cf\u7279\u5b9a\u89e3\u7801\u5668\u3002\u8be5\u67b6\u6784\u76f4\u63a5\u4eceGCM\u5206\u8fa8\u7387\u8f93\u5165\uff0c\u8054\u5408\u9884\u6d4b\u5730\u8868\u6e29\u5ea6\u3001\u98ce\u901f\u548c500 hPa\u4f4d\u52bf\u9ad8\u5ea6\u4e09\u4e2a\u5173\u952e\u6c14\u5019\u53d8\u91cf\uff0c\u4ee5\u6a21\u62df\u6b27\u6d32\u5730\u533a\u7684RCM\u5c3a\u5ea6\u964d\u5c3a\u5ea6\u3002", "result": "\u6240\u63d0\u51fa\u7684\u591a\u53d8\u91cf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u79ef\u6781\u7684\u8de8\u53d8\u91cf\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u5355\u53d8\u91cf\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u591a\u53d8\u91cf\u5efa\u6a21\u5bf9\u4e8e\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u6c14\u5019\u964d\u5c3a\u5ea6\u662f\u6709\u6548\u4e14\u5177\u6709\u524d\u666f\u7684\u3002"}}
{"id": "2506.22893", "pdf": "https://arxiv.org/pdf/2506.22893", "abs": "https://arxiv.org/abs/2506.22893", "authors": ["Arpit Narechania", "Alex Endert", "Atanu R Sinha"], "title": "Agentic Enterprise: AI-Centric User to User-Centric AI", "categories": ["cs.AI", "cs.HC"], "comment": "12 pages, 1 figure, 2 sidebars; Preprint", "summary": "After a very long winter, the Artificial Intelligence (AI) spring is here.\nOr, so it seems over the last three years. AI has the potential to impact many\nareas of human life - personal, social, health, education, professional. In\nthis paper, we take a closer look at the potential of AI for Enterprises, where\ndecision-making plays a crucial and repeated role across functions, tasks, and\noperations. We consider Agents imbued with AI as means to increase\ndecision-productivity of enterprises. We highlight six tenets for Agentic\nsuccess in enterprises, by drawing attention to what the current, AI-Centric\nUser paradigm misses, in the face of persistent needs of and usefulness for\nEnterprise Decision-Making. In underscoring a shift to User-Centric AI, we\noffer six tenets and promote market mechanisms for platforms, aligning the\ndesign of AI and its delivery by Agents to the cause of enterprise users.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8AI\u5728\u4f01\u4e1a\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u63d0\u51fa\u901a\u8fc7AI\u4ee3\u7406\u63d0\u5347\u51b3\u7b56\u6548\u7387\uff0c\u5e76\u9610\u8ff0\u4e86\u516d\u9879\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AI\u4ee3\u7406\u6210\u529f\u539f\u5219\u3002", "motivation": "\u9274\u4e8eAI\u5728\u4f01\u4e1a\u51b3\u7b56\u4e2d\u626e\u6f14\u7684\u5173\u952e\u4e14\u91cd\u590d\u7684\u89d2\u8272\uff0c\u4ee5\u53ca\u5f53\u524dAI\u8303\u5f0f\u672a\u80fd\u5145\u5206\u6ee1\u8db3\u4f01\u4e1a\u51b3\u7b56\u9700\u6c42\uff0c\u672c\u6587\u65e8\u5728\u6df1\u5165\u63a2\u8ba8AI\uff08\u7279\u522b\u662fAI\u4ee3\u7406\uff09\u5982\u4f55\u63d0\u5347\u4f01\u4e1a\u51b3\u7b56\u751f\u4ea7\u529b\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5ba1\u89c6\u5f53\u524d\u4ee5AI\u4e3a\u4e2d\u5fc3\u7684\u7528\u6237\u8303\u5f0f\u6240\u7f3a\u5931\u7684\u90e8\u5206\uff0c\u63d0\u51fa\u4e86\u516d\u9879\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AI\u4ee3\u7406\u5728\u4f01\u4e1a\u4e2d\u6210\u529f\u7684\u539f\u5219\u3002\u540c\u65f6\uff0c\u5021\u5bfc\u5e02\u573a\u673a\u5236\uff0c\u4ee5\u4f7fAI\u7684\u8bbe\u8ba1\u548c\u901a\u8fc7\u4ee3\u7406\u7684\u4ea4\u4ed8\u80fd\u66f4\u597d\u5730\u670d\u52a1\u4e8e\u4f01\u4e1a\u7528\u6237\u3002", "result": "\u5206\u6790\u4e86AI\u5728\u4f01\u4e1a\u51b3\u7b56\u4e2d\u7684\u6f5c\u529b\uff0c\u8bc6\u522b\u51fa\u5f53\u524dAI-centric\u8303\u5f0f\u4e0e\u4f01\u4e1a\u51b3\u7b56\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e86\u652f\u6301\u4f01\u4e1a\u51b3\u7b56\u6548\u7387\u63d0\u5347\u7684\u516d\u9879\u5173\u952e\u539f\u5219\u548c\u5411\u7528\u6237\u4e2d\u5fc3AI\u8f6c\u578b\u7684\u5efa\u8bae\u3002", "conclusion": "AI\u4ee3\u7406\u662f\u63d0\u5347\u4f01\u4e1a\u51b3\u7b56\u751f\u4ea7\u529b\u7684\u6709\u6548\u624b\u6bb5\uff0c\u5176\u6210\u529f\u5173\u952e\u5728\u4e8e\u8f6c\u5411\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AI\u8bbe\u8ba1\u4e0e\u4ea4\u4ed8\uff0c\u5e76\u9075\u5faa\u6240\u63d0\u51fa\u7684\u516d\u9879\u539f\u5219\uff0c\u540c\u65f6\u9700\u901a\u8fc7\u5e02\u573a\u673a\u5236\u6765\u4fc3\u8fdb\u5e73\u53f0\u53d1\u5c55\uff0c\u4ee5\u771f\u6b63\u670d\u52a1\u4e8e\u4f01\u4e1a\u7528\u6237\u3002"}}
{"id": "2506.22501", "pdf": "https://arxiv.org/pdf/2506.22501", "abs": "https://arxiv.org/abs/2506.22501", "authors": ["Gautam Siddharth Kashyap", "Manaswi Kulahara", "Nipun Joshi", "Usman Naseem"], "title": "How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted in the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025), scheduled for 3 - 8 August 2025 in Brisbane,\n  Australia", "summary": "Remote sensing datasets offer significant promise for tackling key\nclassification tasks such as land-use categorization, object presence\ndetection, and rural/urban classification. However, many existing studies tend\nto focus on narrow tasks or datasets, which limits their ability to generalize\nacross various remote sensing classification challenges. To overcome this, we\npropose a novel model, SpatialNet-ViT, leveraging the power of Vision\nTransformers (ViTs) and Multi-Task Learning (MTL). This integrated approach\ncombines spatial awareness with contextual understanding, improving both\nclassification accuracy and scalability. Additionally, techniques like data\naugmentation, transfer learning, and multi-task learning are employed to\nenhance model robustness and its ability to generalize across diverse datasets", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSpatialNet-ViT\u6a21\u578b\uff0c\u7ed3\u5408Vision Transformers (ViT)\u548c\u591a\u4efb\u52a1\u5b66\u4e60 (MTL)\uff0c\u65e8\u5728\u514b\u670d\u73b0\u6709\u9065\u611f\u5206\u7c7b\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5206\u7c7b\u7cbe\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u53ca\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u5206\u7c7b\u7814\u7a76\u591a\u4e13\u6ce8\u4e8e\u72ed\u7a84\u4efb\u52a1\u6216\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u96be\u4ee5\u5e94\u5bf9\u591a\u6837\u5316\u7684\u9065\u611f\u5206\u7c7b\u6311\u6218\u3002", "method": "\u63d0\u51faSpatialNet-ViT\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u878d\u5408\u4e86Vision Transformers (ViTs)\u548c\u591a\u4efb\u52a1\u5b66\u4e60 (MTL)\uff0c\u65e8\u5728\u7ed3\u5408\u7a7a\u95f4\u611f\u77e5\u4e0e\u4e0a\u4e0b\u6587\u7406\u89e3\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u6570\u636e\u589e\u5f3a\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7b49\u6280\u672f\u4ee5\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u901a\u8fc7SpatialNet-ViT\u6a21\u578b\uff0c\u9884\u671f\u80fd\u6709\u6548\u63d0\u5347\u9065\u611f\u5206\u7c7b\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u591a\u6837\u5316\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "SpatialNet-ViT\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u901a\u7528\u3001\u66f4\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u514b\u670d\u4f20\u7edf\u9065\u611f\u5206\u7c7b\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u5904\u7406\u590d\u6742\u7684\u9065\u611f\u6570\u636e\u5206\u7c7b\u4efb\u52a1\u3002"}}
{"id": "2506.22623", "pdf": "https://arxiv.org/pdf/2506.22623", "abs": "https://arxiv.org/abs/2506.22623", "authors": ["Badr Youbi Idrissi", "Monica Millunzi", "Amelia Sorrenti", "Lorenzo Baraldi", "Daryna Dementieva"], "title": "Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the present-day scenario, Large Language Models (LLMs) are establishing\ntheir presence as powerful instruments permeating various sectors of society.\nWhile their utility offers valuable support to individuals, there are multiple\nconcerns over potential misuse. Consequently, some academic endeavors have\nsought to introduce watermarking techniques, characterized by the inclusion of\nmarkers within machine-generated text, to facilitate algorithmic\nidentification. This research project is focused on the development of a novel\nmethodology for the detection of synthetic text, with the overarching goal of\nensuring the ethical application of LLMs in AI-driven text generation. The\ninvestigation commences with replicating findings from a previous baseline\nstudy, thereby underscoring its susceptibility to variations in the underlying\ngeneration model. Subsequently, we propose an innovative watermarking approach\nand subject it to rigorous evaluation, employing paraphrased generated text to\nasses its robustness. Experimental results highlight the robustness of our\nproposal compared to the~\\cite{aarson} watermarking method.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.22507", "pdf": "https://arxiv.org/pdf/2506.22507", "abs": "https://arxiv.org/abs/2506.22507", "authors": ["Yubo Peng", "Luping Xiang", "Kun Yang", "Feibo Jiang", "Kezhi Wang", "Christos Masouros"], "title": "Integrated Multimodal Sensing and Communication: Challenges, Technologies, and Architectures", "categories": ["cs.NI", "cs.MA", "eess.SP"], "comment": null, "summary": "The evolution towards 6G networks requires the intelligent integration of\ncommunication and sensing capabilities to support diverse and complex\napplications, such as autonomous driving and immersive services. However,\nexisting integrated sensing and communication (ISAC) systems predominantly rely\non single-modal sensors as primary participants, which leads to a limited\nrepresentation of environmental features and significant performance\nbottlenecks under the emerging requirements of 6G applications. This limitation\nmotivates a paradigm shift from single-modal to multimodal ISAC. In this\narticle, we first analyze the key challenges in realizing multimodal ISAC,\nincluding the fusion of heterogeneous multimodal data, the high communication\noverhead among distributed sensors, and the design of efficient and scalable\nsystem architectures. We then introduce several enabling technologies, such as\nlarge AI models, semantic communication, and multi-agent systems, that hold\npromise for addressing these challenges. To operationalize these technologies,\nwe zoom into three architectural paradigms: fusion-based multimodal ISAC\n(F-MAC), interaction-based multimodal ISAC (I-MAC), and relay-based multimodal\nISAC (R-MAC), each tailored to organize devices and modalities for efficient\ncollaboration in different scenarios. Thereafter, a case study is presented\nbased on the F-MAC scheme, demonstrating that the scheme achieves more\ncomprehensive sensing and improves sensing accuracy by approximately 80%\ncompared to conventional single-modal ISAC systems. Finally, we discuss several\nopen issues to be addressed in the future.", "AI": {"tldr": "\u9488\u5bf96G\u7f51\u7edc\u4e2d\u5355\u6a21\u6001ISAC\u7684\u5c40\u9650\u6027\uff0c\u672c\u6587\u63d0\u51fa\u591a\u6a21\u6001ISAC\u8303\u5f0f\uff0c\u5206\u6790\u5176\u6311\u6218\u3001\u5f15\u5165\u4f7f\u80fd\u6280\u672f\u548c\u8bbe\u8ba1\u67b6\u6784\u8303\u5f0f\uff08\u5982F-MAC\uff09\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc1\u660eF-MAC\u5728\u611f\u77e5\u7cbe\u5ea6\u4e0a\u8f83\u4f20\u7edf\u7cfb\u7edf\u63d0\u5347\u7ea680%\u3002", "motivation": "\u73b0\u6709\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u5355\u6a21\u6001\u4f20\u611f\u5668\uff0c\u5bfc\u81f4\u73af\u5883\u7279\u5f81\u8868\u793a\u6709\u9650\u4e14\u57286G\u5e94\u7528\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u3001\u6c89\u6d78\u5f0f\u670d\u52a1\uff09\u9700\u6c42\u4e0b\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u56e0\u6b64\u9700\u8981\u4ece\u5355\u6a21\u6001\u8f6c\u5411\u591a\u6a21\u6001ISAC\u3002", "method": "\u5206\u6790\u5b9e\u73b0\u591a\u6a21\u6001ISAC\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u5f02\u6784\u6570\u636e\u878d\u5408\u3001\u5206\u5e03\u5f0f\u4f20\u611f\u5668\u9ad8\u901a\u4fe1\u5f00\u9500\u548c\u7cfb\u7edf\u67b6\u6784\u8bbe\u8ba1\uff1b\u5f15\u5165\u5927\u578bAI\u6a21\u578b\u3001\u8bed\u4e49\u901a\u4fe1\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7b49\u4f7f\u80fd\u6280\u672f\uff1b\u63d0\u51fa\u878d\u5408\u5f0f\uff08F-MAC\uff09\u3001\u4ea4\u4e92\u5f0f\uff08I-MAC\uff09\u548c\u4e2d\u7ee7\u5f0f\uff08R-MAC\uff09\u4e09\u79cd\u67b6\u6784\u8303\u5f0f\uff1b\u5e76\u57fa\u4e8eF-MAC\u65b9\u6848\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u57fa\u4e8eF-MAC\u65b9\u6848\u7684\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6848\u4e0e\u4f20\u7edf\u7684\u5355\u6a21\u6001ISAC\u7cfb\u7edf\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u7684\u611f\u77e5\uff0c\u5e76\u5c06\u611f\u77e5\u7cbe\u5ea6\u63d0\u9ad8\u4e86\u7ea680%\u3002", "conclusion": "\u591a\u6a21\u6001ISAC\u5bf9\u4e8e6G\u7f51\u7edc\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u672c\u6587\u63d0\u51fa\u7684\u67b6\u6784\u8303\u5f0f\u548c\u4f7f\u80fd\u6280\u672f\u4e3a\u89e3\u51b3\u5176\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5e76\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u672a\u6765\u4ecd\u9700\u89e3\u51b3\u4e00\u4e9b\u5f00\u653e\u6027\u95ee\u9898\u3002"}}
{"id": "2506.22502", "pdf": "https://arxiv.org/pdf/2506.22502", "abs": "https://arxiv.org/abs/2506.22502", "authors": ["Matvei Anoshin", "Olga Tsurkan", "Vadim Lopatkin", "Leonid Fedichkin"], "title": "Stabilization of industrial processes with time series machine learning", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "The stabilization of time series processes is a crucial problem that is\nubiquitous in various industrial fields. The application of machine learning to\nits solution can have a decisive impact, improving both the quality of the\nresulting stabilization with less computational resources required. In this\nwork, we present a simple pipeline consisting of two neural networks: the\noracle predictor and the optimizer, proposing a substitution of the point-wise\nvalues optimization to the problem of the neural network training, which\nsuccessfully improves stability in terms of the temperature control by about 3\ntimes compared to ordinary solvers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7531\u201c\u9884\u8a00\u5668\u201d\u548c\u201c\u4f18\u5316\u5668\u201d\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\u7ec4\u6210\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u7a33\u5b9a\u65f6\u95f4\u5e8f\u5217\u8fc7\u7a0b\uff0c\u5e76\u5728\u6e29\u5ea6\u63a7\u5236\u4e2d\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u9ad83\u500d\u7684\u7a33\u5b9a\u6027\u63d0\u5347\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u8fc7\u7a0b\u7684\u7a33\u5b9a\u5316\u662f\u4e00\u4e2a\u5728\u5404\u5de5\u4e1a\u9886\u57df\u666e\u904d\u5b58\u5728\u7684\u5173\u952e\u95ee\u9898\u3002\u5e94\u7528\u673a\u5668\u5b66\u4e60\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u7a33\u5b9a\u5316\u8d28\u91cf\u5e76\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\uff0c\u5177\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u7ba1\u9053\uff0c\u5305\u542b\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\uff1a\u4e00\u4e2a\u201c\u9884\u8a00\u5668\u201d\u548c\u4e00\u4e2a\u201c\u4f18\u5316\u5668\u201d\u3002\u6838\u5fc3\u65b9\u6cd5\u662f\u5c06\u9010\u70b9\u503c\u4f18\u5316\u95ee\u9898\u8f6c\u6362\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u95ee\u9898\u3002", "result": "\u5728\u6e29\u5ea6\u63a7\u5236\u65b9\u9762\uff0c\u6210\u529f\u5730\u5c06\u7a33\u5b9a\u6027\u63d0\u9ad8\u4e86\u7ea63\u500d\uff0c\u4f18\u4e8e\u666e\u901a\u7684\u6c42\u89e3\u5668\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u53cc\u795e\u7ecf\u7f51\u7edc\u7684\u7ba1\u9053\u4e3a\u65f6\u95f4\u5e8f\u5217\u8fc7\u7a0b\u7684\u7a33\u5b9a\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5b9e\u9645\u5e94\u7528\uff08\u5982\u6e29\u5ea6\u63a7\u5236\uff09\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u53ef\u80fd\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2506.22919", "pdf": "https://arxiv.org/pdf/2506.22919", "abs": "https://arxiv.org/abs/2506.22919", "authors": ["Sanskar Pandey", "Ruhaan Chopra", "Saad Murtaza Bhat", "Ark Abhyudaya"], "title": "Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) models enable conditional computation by routing\ninputs to specialized experts, but these experts rely on identical inductive\nbiases, thus limiting representational diversity. This static computation\npathway is inefficient for inputs that require different types of reasoning and\nlimits specialization and interpretability. We propose Hecto, a lightweight MoE\narchitecture that leverages architectural heterogeneity by combining a GRU\nexpert for temporal reasoning and an FFNN expert for static abstraction under a\nsparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG\nNews, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely\ntrails homogeneous baselines in performance despite receiving isolated input\nrepresentations, while achieving clear expert specialization, with each expert\naligning to distinct reasoning types (temporal vs static). At larger batch\nsizes, Hecto exhibits improved performance, benefiting from relaxed\ncomputational constraints that allow its heterogeneous architecture to optimize\nmore effectively. Ablation results isolate architectural diversity as the\nsource of Hecto's stability and interpretability across diverse reasoning\ntasks. Overall, Hecto establishes itself as a new benchmark for conditional\ncomputation, offering a principled framework for specialized reasoning in\nlow-resource regimes with its model strength derived from principled\nspecialization.", "AI": {"tldr": "Hecto\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5f02\u6784MoE\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408GRU\u548cFFNN\u4e13\u5bb6\u5b9e\u73b0\u6761\u4ef6\u8ba1\u7b97\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\uff0c\u5e76\u5728\u591a\u9879\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4f4e\u8d44\u6e90\u4e0b\u7684\u4e13\u4e1a\u5316\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "motivation": "\u73b0\u6709Mixture-of-Experts (MoE) \u6a21\u578b\u4e2d\u4e13\u5bb6\u5177\u6709\u76f8\u540c\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u9650\u5236\u4e86\u8868\u793a\u591a\u6837\u6027\uff1b\u9759\u6001\u8ba1\u7b97\u8def\u5f84\u6548\u7387\u4f4e\u4e0b\uff0c\u5e76\u9650\u5236\u4e86\u4e13\u4e1a\u5316\u548c\u53ef\u89e3\u91ca\u6027\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u63d0\u5347\u6a21\u578b\u5904\u7406\u4e0d\u540c\u63a8\u7406\u7c7b\u578b\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faHecto\u67b6\u6784\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7MoE\u6a21\u578b\u3002\u5b83\u5229\u7528\u67b6\u6784\u5f02\u6784\u6027\uff0c\u7ed3\u5408GRU\u4e13\u5bb6\uff08\u7528\u4e8e\u65f6\u95f4\u63a8\u7406\uff09\u548cFFNN\u4e13\u5bb6\uff08\u7528\u4e8e\u9759\u6001\u62bd\u8c61\uff09\uff0c\u5e76\u901a\u8fc7\u7a00\u758f\u7684Top-1\u95e8\u63a7\u673a\u5236\u8fdb\u884c\u8def\u7531\u3002\u6a21\u578b\u5728AG News\u3001SST-2\u3001HotpotQA\u4e09\u4e2a\u63a8\u7406\u57fa\u51c6\u548cSTS-B\u56de\u5f52\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "Hecto\u5728\u6027\u80fd\u4e0a\u4e0e\u540c\u6784\u57fa\u7ebf\u76f8\u5f53\u6216\u7565\u4f4e\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6e05\u6670\u7684\u4e13\u5bb6\u4e13\u4e1a\u5316\uff0c\u5404\u4e13\u5bb6\u4e0e\u7279\u5b9a\u63a8\u7406\u7c7b\u578b\uff08\u65f6\u95f4 vs \u9759\u6001\uff09\u5bf9\u9f50\u3002\u5728\u5927\u6279\u91cf\u5904\u7406\u65f6\uff0cHecto\u8868\u73b0\u51fa\u6027\u80fd\u63d0\u5347\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660e\uff0c\u67b6\u6784\u591a\u6837\u6027\u662fHecto\u5728\u591a\u6837\u63a8\u7406\u4efb\u52a1\u4e2d\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5173\u952e\u3002", "conclusion": "Hecto\u4e3a\u6761\u4ef6\u8ba1\u7b97\u6811\u7acb\u4e86\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7\u5176\u539f\u7406\u6027\u4e13\u4e1a\u5316\uff0c\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u4e3a\u4e13\u4e1a\u5316\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6\u3002"}}
{"id": "2506.22503", "pdf": "https://arxiv.org/pdf/2506.22503", "abs": "https://arxiv.org/abs/2506.22503", "authors": ["Michiel Schepers", "Pieter Robberechts", "Jan Van Haaren", "Jesse Davis"], "title": "What Makes a Dribble Successful? Insights From 3D Pose Tracking Data", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Data analysis plays an increasingly important role in soccer, offering new\nways to evaluate individual and team performance. One specific application is\nthe evaluation of dribbles: one-on-one situations where an attacker attempts to\nbypass a defender with the ball. While previous research has primarily relied\non 2D positional tracking data, this fails to capture aspects like balance,\norientation, and ball control, limiting the depth of current insights. This\nstudy explores how pose tracking data (capturing players' posture and movement\nin three dimensions) can improve our understanding of dribbling skills. We\nextract novel pose-based features from 1,736 dribbles in the 2022/23 Champions\nLeague season and evaluate their impact on dribble success. Our results\nindicate that features capturing the attacker's balance and the alignment of\nthe orientation between the attacker and defender are informative for\npredicting dribble success. Incorporating these pose-based features on top of\nfeatures derived from traditional 2D positional data leads to a measurable\nimprovement in model performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u4e09\u7ef4\u59ff\u6001\u8ffd\u8e2a\u6570\u636e\u5206\u6790\u8db3\u7403\u8fc7\u4eba\u52a8\u4f5c\uff0c\u53d1\u73b0\u5e73\u8861\u548c\u65b9\u5411\u5bf9\u8fc7\u4eba\u6210\u529f\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u80fd\u663e\u8457\u63d0\u5347\u8fc7\u4eba\u6210\u529f\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8db3\u7403\u6570\u636e\u5206\u6790\u4e3b\u8981\u4f9d\u8d56\u4e8c\u7ef4\u4f4d\u7f6e\u8ffd\u8e2a\u6570\u636e\uff0c\u65e0\u6cd5\u6355\u6349\u7403\u5458\u5e73\u8861\u3001\u65b9\u5411\u548c\u63a7\u7403\u7b49\u5173\u952e\u4e09\u7ef4\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5bf9\u8fc7\u4eba\u6280\u80fd\u8bc4\u4f30\u7684\u6df1\u5ea6\u7406\u89e3\u3002", "method": "\u7814\u7a76\u4ece2022/23\u8d5b\u5b63\u6b27\u51a0\u8054\u8d5b\u76841,736\u6b21\u8fc7\u4eba\u4e2d\u63d0\u53d6\u4e86\u65b0\u9896\u7684\u57fa\u4e8e\u59ff\u6001\u7684\u4e09\u7ef4\u7279\u5f81\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u8fc7\u4eba\u6210\u529f\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u4e0e\u4f20\u7edf\u4e8c\u7ef4\u4f4d\u7f6e\u6570\u636e\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6355\u6349\u8fdb\u653b\u7403\u5458\u5e73\u8861\u548c\u8fdb\u653b\u7403\u5458\u4e0e\u9632\u5b88\u7403\u5458\u65b9\u5411\u5bf9\u9f50\u7684\u59ff\u6001\u7279\u5f81\uff0c\u5bf9\u4e8e\u9884\u6d4b\u8fc7\u4eba\u6210\u529f\u5177\u6709\u91cd\u8981\u4fe1\u606f\uff0c\u5e76\u4e14\u5728\u4f20\u7edf\u4e8c\u7ef4\u7279\u5f81\u57fa\u7840\u4e0a\u5f15\u5165\u8fd9\u4e9b\u59ff\u6001\u7279\u5f81\uff0c\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u4e09\u7ef4\u59ff\u6001\u8ffd\u8e2a\u6570\u636e\u80fd\u591f\u63d0\u4f9b\u5bf9\u8fc7\u4eba\u6280\u80fd\u66f4\u6df1\u5165\u7684\u7406\u89e3\uff0c\u5e76\u901a\u8fc7\u6355\u6349\u4f20\u7edf\u4e8c\u7ef4\u6570\u636e\u672a\u80fd\u6db5\u76d6\u7684\u65b9\u9762\uff0c\u6709\u6548\u63d0\u5347\u8fc7\u4eba\u6210\u529f\u9884\u6d4b\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.22644", "pdf": "https://arxiv.org/pdf/2506.22644", "abs": "https://arxiv.org/abs/2506.22644", "authors": ["Chase Fensore", "Kaustubh Dhole", "Joyce C Ho", "Eugene Agichtein"], "title": "Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge", "categories": ["cs.CL", "cs.IR"], "comment": "4 pages, 3 tables, 2 figures. Accepted at the SIGIR LiveRAG Workshop\n  2025 (Submission 2664)", "summary": "We present our submission to the LiveRAG Challenge 2025, which evaluates\nretrieval-augmented generation (RAG) systems on dynamic test sets using the\nFineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense\n(E5) retrieval methods and then aims to generate relevant and faithful answers\nwith Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic\nquestions generated with DataMorgana across 64 unique question-user\ncombinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP\nfrom 0.523 to 0.797 (52% relative improvement) but introduces prohibitive\ncomputational costs (84s vs 1.74s per question). While DSPy-optimized prompting\nstrategies achieved higher semantic similarity (0.771 vs 0.668), their 0%\nrefusal rates raised concerns about over-confidence and generalizability. Our\nsubmitted hybrid system without re-ranking achieved 4th place in faithfulness\nand 11th place in correctness among 25 teams. Analysis across question\ncategories reveals that vocabulary alignment between questions and documents\nwas the strongest predictor of performance on our development set, with\ndocument-similar phrasing improving cosine similarity from 0.562 to 0.762.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u4ea4\u4e86LiveRAG 2025\u6311\u6218\u8d5b\u7684RAG\u7cfb\u7edf\uff0c\u7ed3\u5408\u7a00\u758f\u4e0e\u5bc6\u96c6\u68c0\u7d22\u5e76\u4f7f\u7528\u5927\u6a21\u578b\u751f\u6210\u3002\u6700\u7ec8\u7cfb\u7edf\u5728\u5fe0\u5b9e\u6027\u548c\u6b63\u786e\u6027\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5e76\u53d1\u73b0\u91cd\u6392\u867d\u63d0\u5347\u6027\u80fd\u4f46\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u8bcd\u6c47\u5bf9\u9f50\u662f\u5173\u952e\u6027\u80fd\u9884\u6d4b\u56e0\u7d20\u3002", "motivation": "\u53c2\u4e0eLiveRAG 2025\u6311\u6218\u8d5b\uff0c\u5728\u52a8\u6001\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u5e76\u5f00\u53d1\u9ad8\u6548\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u6df7\u5408\u68c0\u7d22\u65b9\u6cd5\uff0c\u7ed3\u5408\u7a00\u758f\uff08BM25\uff09\u548c\u5bc6\u96c6\uff08E5\uff09\u68c0\u7d22\uff0c\u5e76\u4f7f\u7528Falcon3-10B-Instruct\u8fdb\u884c\u7b54\u6848\u751f\u6210\u3002\u7814\u7a76\u4e2d\u8fd8\u63a2\u7d22\u4e86\u795e\u7ecf\u7f51\u7edc\u91cd\u6392\uff08RankLLaMA\uff09\u548cDSPy\u4f18\u5316\u7684\u63d0\u793a\u7b56\u7565\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u91cd\u6392\uff08RankLLaMA\uff09\u663e\u8457\u63d0\u5347MAP\uff08\u4ece0.523\u81f30.797\uff09\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff08\u6bcf\u95ee\u989884\u79d2 vs 1.74\u79d2\uff09\u3002DSPy\u4f18\u5316\u63d0\u793a\u7b56\u7565\u63d0\u9ad8\u4e86\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff080.771 vs 0.668\uff09\uff0c\u4f460%\u7684\u62d2\u7edd\u7387\u5f15\u53d1\u4e86\u8fc7\u5ea6\u81ea\u4fe1\u7684\u62c5\u5fe7\u3002\u6700\u7ec8\u63d0\u4ea4\u7684\u672a\u5305\u542b\u91cd\u6392\u7684\u6df7\u5408\u7cfb\u7edf\u572825\u652f\u961f\u4f0d\u4e2d\u5fe0\u5b9e\u6027\u6392\u540d\u7b2c4\uff0c\u6b63\u786e\u6027\u6392\u540d\u7b2c11\u3002\u8bcd\u6c47\u5bf9\u9f50\u662f\u6027\u80fd\u7684\u6700\u5f3a\u9884\u6d4b\u56e0\u7d20\u3002", "conclusion": "\u8be5\u6df7\u5408RAG\u7cfb\u7edf\u5728LiveRAG\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6027\u80fd\u63d0\u5347\uff08\u5982\u91cd\u6392\uff09\u9700\u6743\u8861\u8ba1\u7b97\u6210\u672c\u3002\u8bcd\u6c47\u5bf9\u9f50\u662fRAG\u7cfb\u7edf\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2506.22671", "pdf": "https://arxiv.org/pdf/2506.22671", "abs": "https://arxiv.org/abs/2506.22671", "authors": ["Rubi Debnath", "Mohammadreza Barzegaran", "Sebastian Steinhorst"], "title": "Towards an Optimized Multi-Cyclic Queuing and Forwarding in Time Sensitive Networking with Time Injection", "categories": ["cs.NI", "cs.ET"], "comment": null, "summary": "Cyclic Queuing and Forwarding (CQF) is a Time-Sensitive Networking (TSN)\nshaping mechanism that provides bounded latency and deterministic Quality of\nService (QoS). However, CQF's use of a single cycle restricts its ability to\nsupport TSN traffic with diverse timing requirements. Multi-Cyclic Queuing and\nForwarding (Multi-CQF) is a new and emerging TSN shaping mechanism that uses\nmultiple cycles on the same egress port, allowing it to accommodate TSN flows\nwith varied timing requirements more effectively than CQF. Despite its\npotential, current Multi-CQF configuration studies are limited, leading to a\nlack of comprehensive research, poor understanding of the mechanism, and\nlimited adoption of Multi-CQF in practical applications. Previous work has\nshown the impact of Time Injection (TI), defined as the start time of\nTime-Triggered (TT) flows at the source node, on CQF queue resource\nutilization. However, the impact of TI has not yet been explored in the context\nof Multi-CQF. This paper introduces a set of constraints and leverages Domain\nSpecific Knowledge (DSK) to reduce the search space for Multi-CQF\nconfiguration. Building on this foundation, we develop an open-source Genetic\nAlgorithm (GA) and a hybrid GA-Simulated Annealing (GASA) approach to\nefficiently configure Multi-CQF networks and introduce TI in Multi-CQF to\nenhance schedulability. Experimental results show that our proposed algorithms\nsignificantly increase the number of scheduled TT flows compared to the\nbaseline Simulated Annealing (SA) model, improving scheduling by an average of\n15%. Additionally, GASA achieves a 20% faster convergence rate and lower time\ncomplexity, outperforming the SA model in speed, and efficiency.", "AI": {"tldr": "Multi-CQF\u662f\u4e3a\u89e3\u51b3CQF\u5c40\u9650\u6027\u800c\u63d0\u51fa\u7684TSN\u673a\u5236\uff0c\u4f46\u5176\u914d\u7f6e\u7814\u7a76\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa\u7ed3\u5408\u65f6\u95f4\u6ce8\u5165\uff08TI\uff09\u7684\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u548c\u6df7\u5408\u9057\u4f20\u6a21\u62df\u9000\u706b\uff08GASA\uff09\u7b97\u6cd5\uff0c\u4ee5\u4f18\u5316Multi-CQF\u7f51\u7edc\u914d\u7f6e\u5e76\u63d0\u5347\u53ef\u8c03\u5ea6\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfSA\u6a21\u578b\u3002", "motivation": "\u5faa\u73af\u6392\u961f\u8f6c\u53d1\uff08CQF\uff09\u7684\u5355\u5468\u671f\u9650\u5236\u4e86\u5176\u652f\u6301\u591a\u6837\u5316\u65f6\u95f4\u654f\u611f\u7f51\u7edc\uff08TSN\uff09\u6d41\u91cf\u7684\u80fd\u529b\u3002\u591a\u5468\u671f\u6392\u961f\u8f6c\u53d1\uff08Multi-CQF\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u673a\u5236\uff0c\u867d\u80fd\u66f4\u6709\u6548\u5730\u9002\u5e94\u4e0d\u540c\u65f6\u95f4\u8981\u6c42\u7684TSN\u6d41\uff0c\u4f46\u5176\u914d\u7f6e\u7814\u7a76\u6709\u9650\uff0c\u5bfc\u81f4\u673a\u5236\u7406\u89e3\u4e0d\u8db3\u548c\u5b9e\u9645\u5e94\u7528\u53d7\u9650\u3002\u6b64\u5916\uff0c\u65f6\u95f4\u6ce8\u5165\uff08TI\uff09\u5bf9CQF\u961f\u5217\u8d44\u6e90\u5229\u7528\u7387\u6709\u5f71\u54cd\uff0c\u4f46\u5176\u5728Multi-CQF\u4e2d\u7684\u4f5c\u7528\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e00\u7ec4\u7ea6\u675f\u5e76\u5229\u7528\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\uff08DSK\uff09\u6765\u7f29\u5c0fMulti-CQF\u914d\u7f6e\u7684\u641c\u7d22\u7a7a\u95f4\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e86\u5f00\u6e90\u7684\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u548c\u6df7\u5408\u9057\u4f20\u6a21\u62df\u9000\u706b\uff08GASA\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u914d\u7f6eMulti-CQF\u7f51\u7edc\u3002\u540c\u65f6\uff0c\u5c06\u65f6\u95f4\u6ce8\u5165\uff08TI\uff09\u5f15\u5165Multi-CQF\u4ee5\u589e\u5f3a\u5176\u53ef\u8c03\u5ea6\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u6a21\u62df\u9000\u706b\uff08SA\uff09\u6a21\u578b\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u663e\u8457\u589e\u52a0\u4e86\u53ef\u8c03\u5ea6\u7684\u65f6\u95f4\u89e6\u53d1\uff08TT\uff09\u6d41\u6570\u91cf\uff0c\u5e73\u5747\u8c03\u5ea6\u6027\u80fd\u63d0\u5347\u4e8615%\u3002\u6b64\u5916\uff0cGASA\u7b97\u6cd5\u7684\u6536\u655b\u901f\u5ea6\u6bd4SA\u6a21\u578b\u5feb20%\uff0c\u4e14\u65f6\u95f4\u590d\u6742\u5ea6\u66f4\u4f4e\uff0c\u5728\u901f\u5ea6\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8eSA\u6a21\u578b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u548c\u6df7\u5408\u9057\u4f20\u6a21\u62df\u9000\u706b\u7684Multi-CQF\u914d\u7f6e\u65b9\u6cd5\uff0c\u7ed3\u5408\u65f6\u95f4\u6ce8\u5165\uff0c\u80fd\u6709\u6548\u63d0\u5347Multi-CQF\u7f51\u7edc\u4e2d\u65f6\u95f4\u89e6\u53d1\u6d41\u7684\u53ef\u8c03\u5ea6\u6027\u3002\u8fd9\u4e9b\u7b97\u6cd5\u5728\u8c03\u5ea6\u6027\u80fd\u3001\u6536\u655b\u901f\u5ea6\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3aMulti-CQF\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u914d\u7f6e\u7b56\u7565\u3002"}}
{"id": "2506.22530", "pdf": "https://arxiv.org/pdf/2506.22530", "abs": "https://arxiv.org/abs/2506.22530", "authors": ["Jakub Pele\u0161ka", "Gustav \u0160\u00edr"], "title": "Task-Agnostic Contrastive Pretraining for Relational Deep Learning", "categories": ["cs.LG", "cs.DB"], "comment": "arXiv admin note: text overlap with arXiv:2506.22199", "summary": "Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph\nNeural Network principles to learn directly from relational databases by\nrepresenting them as heterogeneous graphs. However, existing RDL models\ntypically rely on task-specific supervised learning, requiring training\nseparate models for each predictive task, which may hamper scalability and\nreuse.\n  In this work, we propose a novel task-agnostic contrastive pretraining\napproach for RDL that enables database-wide representation learning. For that\naim, we introduce three levels of contrastive objectives$-$row-level,\nlink-level, and context-level$-$designed to capture the structural and semantic\nheterogeneity inherent to relational data. We implement the respective\npretraining approach through a modular RDL architecture and an efficient\nsampling strategy tailored to the heterogeneous database setting. Our\npreliminary results on standard RDL benchmarks demonstrate that fine-tuning the\npretrained models measurably outperforms training from scratch, validating the\npromise of the proposed methodology in learning transferable representations\nfor relational data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u7684\u5bf9\u6bd4\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\uff08RDL\uff09\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709RDL\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u590d\u7528\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\uff08RDL\uff09\u6a21\u578b\u4f9d\u8d56\u4e8e\u4efb\u52a1\u7279\u5b9a\u7684\u76d1\u7763\u5b66\u4e60\uff0c\u4e3a\u6bcf\u4e2a\u9884\u6d4b\u4efb\u52a1\u8bad\u7ec3\u5355\u72ec\u6a21\u578b\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u590d\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u4efb\u52a1\u65e0\u5173\u5bf9\u6bd4\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b9e\u73b0\u6570\u636e\u5e93\u8303\u56f4\u7684\u8868\u793a\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u884c\u7ea7\u3001\u94fe\u63a5\u7ea7\u548c\u4e0a\u4e0b\u6587\u7ea7\u4e09\u79cd\u5bf9\u6bd4\u76ee\u6807\uff0c\u65e8\u5728\u6355\u83b7\u5173\u7cfb\u6570\u636e\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u5f02\u6784\u6027\uff0c\u5e76\u901a\u8fc7\u6a21\u5757\u5316RDL\u67b6\u6784\u548c\u9ad8\u6548\u91c7\u6837\u7b56\u7565\u5b9e\u73b0\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u5728\u6807\u51c6RDL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u663e\u8457\u4f18\u4e8e\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b66\u4e60\u5173\u7cfb\u6570\u636e\u7684\u53ef\u8fc1\u79fb\u8868\u793a\u65b9\u9762\u5c55\u73b0\u51fa\u826f\u597d\u524d\u666f\u3002"}}
{"id": "2506.22920", "pdf": "https://arxiv.org/pdf/2506.22920", "abs": "https://arxiv.org/abs/2506.22920", "authors": ["Pinzheng Wang", "Juntao Li", "Zecheng Tang", "Haijia Gui", "Min zhang"], "title": "Improving Rationality in the Reasoning Process of Language Models through Self-playing Game", "categories": ["cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Large language models (LLMs) have demonstrated considerable reasoning\nabilities in various tasks such as mathematics and coding. However, recent\nstudies indicate that even the best models lack true comprehension of their\nreasoning processes. In this paper, we explore how self-play can enhance the\nrationality of models in the reasoning process without supervision from humans\nor superior models. We design a Critic-Discernment Game(CDG) in which a prover\nfirst provides a solution to a given problem and is subsequently challenged by\ncritiques of its solution. These critiques either aim to assist or mislead the\nprover. The objective of the prover is to maintain the correct answer when\nfaced with misleading comments, while correcting errors in response to\nconstructive feedback. Our experiments on tasks involving mathematical\nreasoning, stepwise error detection, self-correction, and long-chain reasoning\ndemonstrate that CDG training can significantly improve the ability of\nwell-aligned LLMs to comprehend their reasoning process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aCritic-Discernment Game (CDG)\u7684\u81ea\u535a\u5f08\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u6279\u8bc4\u4e0e\u8fa8\u522b\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347LLMs\u5bf9\u81ea\u8eab\u63a8\u7406\u8fc7\u7a0b\u7684\u7406\u89e3\u80fd\u529b\uff0c\u65e0\u9700\u4eba\u7c7b\u6216\u66f4\u5f3a\u6a21\u578b\u7684\u76d1\u7763\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u597d\u7684\u6a21\u578b\u4e5f\u7f3a\u4e4f\u5bf9\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u771f\u6b63\u7406\u89e3\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2aCritic-Discernment Game (CDG)\u3002\u5728\u6b64\u6e38\u620f\u4e2d\uff0c\u8bc1\u660e\u8005\uff08prover\uff09\u4e3a\u95ee\u9898\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u968f\u540e\u53d7\u5230\u6279\u8bc4\u8005\uff08critiques\uff09\u7684\u6311\u6218\u3002\u6279\u8bc4\u65e8\u5728\u8f85\u52a9\u6216\u8bef\u5bfc\u8bc1\u660e\u8005\u3002\u8bc1\u660e\u8005\u7684\u76ee\u6807\u662f\u5728\u9762\u5bf9\u8bef\u5bfc\u6027\u8bc4\u8bba\u65f6\u4fdd\u6301\u6b63\u786e\u7b54\u6848\uff0c\u540c\u65f6\u6839\u636e\u5efa\u8bbe\u6027\u53cd\u9988\u7ea0\u6b63\u9519\u8bef\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u9010\u6b65\u9519\u8bef\u68c0\u6d4b\u3001\u81ea\u6211\u7ea0\u6b63\u548c\u957f\u94fe\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCDG\u8bad\u7ec3\u80fd\u663e\u8457\u63d0\u9ad8\u5df2\u826f\u597d\u5bf9\u9f50\u7684LLM\u7406\u89e3\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u80fd\u529b\u3002", "conclusion": "CDG\u662f\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u81ea\u535a\u5f08\u663e\u8457\u589e\u5f3aLLMs\u5bf9\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u7406\u89e3\u548c\u7406\u6027\u80fd\u529b\u3002"}}
{"id": "2506.22504", "pdf": "https://arxiv.org/pdf/2506.22504", "abs": "https://arxiv.org/abs/2506.22504", "authors": ["Hassan Baker", "Austin J. Brockmeier"], "title": "Patch2Loc: Learning to Localize Patches for Unsupervised Brain Lesion Detection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Detecting brain lesions as abnormalities observed in magnetic resonance\nimaging (MRI) is essential for diagnosis and treatment. In the search of\nabnormalities, such as tumors and malformations, radiologists may benefit from\ncomputer-aided diagnostics that use computer vision systems trained with\nmachine learning to segment normal tissue from abnormal brain tissue. While\nsupervised learning methods require annotated lesions, we propose a new\nunsupervised approach (Patch2Loc) that learns from normal patches taken from\nstructural MRI. We train a neural network model to map a patch back to its\nspatial location within a slice of the brain volume. During inference, abnormal\npatches are detected by the relatively higher error and/or variance of the\nlocation prediction. This generates a heatmap that can be integrated into\npixel-wise methods to achieve finer-grained segmentation. We demonstrate the\nability of our model to segment abnormal brain tissues by applying our approach\nto the detection of tumor tissues in MRI on T2-weighted images from BraTS2021\nand MSLUB datasets and T1-weighted images from ATLAS and WMH datasets. We show\nthat it outperforms the state-of-the art in unsupervised segmentation. The\ncodebase for this work can be found on our\n\\href{https://github.com/bakerhassan/Patch2Loc}{GitHub page}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aPatch2Loc\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u9884\u6d4bMRI\u56fe\u50cf\u6b63\u5e38\u8865\u4e01\u7684\u7a7a\u95f4\u4f4d\u7f6e\uff0c\u5229\u7528\u9884\u6d4b\u8bef\u5dee\u6216\u65b9\u5dee\u68c0\u6d4b\u8111\u90e8\u75c5\u53d8\uff0c\u5e76\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65e0\u76d1\u7763\u5206\u5272SOTA\u65b9\u6cd5\u3002", "motivation": "\u8111\u90e8\u75c5\u53d8\u68c0\u6d4b\u5bf9\u8bca\u65ad\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u5e26\u6807\u6ce8\u7684\u75c5\u53d8\u6570\u636e\uff0c\u800c\u83b7\u53d6\u8fd9\u4e9b\u6570\u636e\u8017\u65f6\u4e14\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u75c5\u53d8\u6570\u636e\u3001\u4ec5\u4ece\u6b63\u5e38\u7ec4\u7ec7\u5b66\u4e60\u7684\u65e0\u76d1\u7763\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\uff08Patch2Loc\uff09\uff0c\u4f7f\u7528\u6765\u81ea\u7ed3\u6784\u5316MRI\u7684\u6b63\u5e38\u56fe\u50cf\u5757\u8fdb\u884c\u5b66\u4e60\u3002\u8bad\u7ec3\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u5c06\u56fe\u50cf\u5757\u6620\u5c04\u56de\u5176\u5728\u8111\u90e8\u5207\u7247\u4e2d\u7684\u7a7a\u95f4\u4f4d\u7f6e\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u901a\u8fc7\u4f4d\u7f6e\u9884\u6d4b\u7684\u76f8\u5bf9\u8f83\u9ad8\u8bef\u5dee\u548c/\u6216\u65b9\u5dee\u6765\u68c0\u6d4b\u5f02\u5e38\u56fe\u50cf\u5757\uff0c\u751f\u6210\u70ed\u56fe\uff0c\u53ef\u8fdb\u4e00\u6b65\u6574\u5408\u5230\u50cf\u7d20\u7ea7\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u5206\u5272\u3002", "result": "\u8be5\u6a21\u578b\u6210\u529f\u5e94\u7528\u4e8eBraTS2021\u3001MSLUB\u6570\u636e\u96c6\u7684T2\u52a0\u6743\u56fe\u50cf\u4ee5\u53caATLAS\u548cWMH\u6570\u636e\u96c6\u7684T1\u52a0\u6743\u56fe\u50cf\u4e0a\u7684\u80bf\u7624\u7ec4\u7ec7\u68c0\u6d4b\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u5206\u5272\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\uff08SOTA\uff09\u65b9\u6cd5\u3002", "conclusion": "Patch2Loc\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u76d1\u7763\u8111\u90e8\u75c5\u53d8\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ec5\u901a\u8fc7\u5b66\u4e60\u6b63\u5e38\u7ec4\u7ec7\u7684\u6a21\u5f0f\u5373\u53ef\u8bc6\u522b\u5f02\u5e38\u533a\u57df\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8d85\u8d8a\u73b0\u6709\u65e0\u76d1\u7763\u5206\u5272\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e3a\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2506.22679", "pdf": "https://arxiv.org/pdf/2506.22679", "abs": "https://arxiv.org/abs/2506.22679", "authors": ["Ankush Raut", "Projna Paromita", "Sydney Begerowski", "Suzanne Bell", "Theodora Chaspari"], "title": "Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions", "categories": ["cs.CL"], "comment": "5 pages, 4 figures. Accepted to Interspeech 2025", "summary": "We explore the feasibility of large language models (LLMs) in detecting\nsubtle expressions of micro-behaviors in team conversations using transcripts\ncollected during simulated space missions. Specifically, we examine zero-shot\nclassification, fine-tuning, and paraphrase-augmented fine-tuning with\nencoder-only sequence classification LLMs, as well as few-shot text generation\nwith decoder-only causal language modeling LLMs, to predict the micro-behavior\nassociated with each conversational turn (i.e., dialogue). Our findings\nindicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to\ndetect underrepresented micro-behaviors, particularly discouraging speech, even\nwith weighted fine-tuning. In contrast, the instruction fine-tuned version of\nLlama-3.1, a decoder-only LLM, demonstrated superior performance, with the best\nmodels achieving macro F1-scores of 44% for 3-way classification and 68% for\nbinary classification. These results have implications for the development of\nspeech technologies aimed at analyzing team communication dynamics and\nenhancing training interventions in high-stakes environments such as space\nmissions, particularly in scenarios where text is the only accessible data.", "AI": {"tldr": "\u7814\u7a76\u4e86LLMs\u5728\u6a21\u62df\u592a\u7a7a\u4efb\u52a1\u56e2\u961f\u5bf9\u8bdd\u4e2d\u8bc6\u522b\u5fae\u884c\u4e3a\u7684\u53ef\u884c\u6027\u3002\u7ed3\u679c\u663e\u793a\uff0c\u7f16\u7801\u5668\u578bLLMs\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u6307\u4ee4\u5fae\u8c03\u7684\u89e3\u7801\u5668\u578bLLM Llama-3.1\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u68c0\u6d4b\u96be\u4ee5\u8bc6\u522b\u7684\u5fae\u884c\u4e3a\u65b9\u9762\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u56e2\u961f\u5bf9\u8bdd\u4e2d\u68c0\u6d4b\u7ec6\u5fae\u5fae\u884c\u4e3a\u7684\u53ef\u884c\u6027\uff0c\u65e8\u5728\u4e3a\u9ad8\u98ce\u9669\u73af\u5883\uff08\u5982\u592a\u7a7a\u4efb\u52a1\uff09\u4e0b\u7684\u56e2\u961f\u6c9f\u901a\u5206\u6790\u548c\u8bad\u7ec3\u5e72\u9884\u63d0\u4f9b\u6280\u672f\u652f\u6301\uff0c\u5c24\u5176\u662f\u5728\u4ec5\u6709\u6587\u672c\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528\u6a21\u62df\u592a\u7a7a\u4efb\u52a1\u671f\u95f4\u6536\u96c6\u7684\u5bf9\u8bdd\u6587\u672c\u8bb0\u5f55\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u65b9\u6cd5\uff1a\u5bf9RoBERTa\u548cDistilBERT\u7b49\u7f16\u7801\u5668\u578bLLMs\u8fdb\u884c\u96f6\u6837\u672c\u5206\u7c7b\u3001\u5fae\u8c03\u548c\u91ca\u4e49\u589e\u5f3a\u5fae\u8c03\uff1b\u5bf9Llama-3.1\u7b49\u89e3\u7801\u5668\u578bLLMs\u8fdb\u884c\u5c11\u6837\u672c\u6587\u672c\u751f\u6210\u548c\u6307\u4ee4\u5fae\u8c03\uff0c\u4ee5\u9884\u6d4b\u6bcf\u4e2a\u5bf9\u8bdd\u56de\u5408\u7684\u5fae\u884c\u4e3a\u3002", "result": "\u7f16\u7801\u5668\u578bLLMs\uff08\u5982RoBERTa\u548cDistilBERT\uff09\u5728\u68c0\u6d4b\u4f4e\u9891\u5fae\u884c\u4e3a\uff08\u7279\u522b\u662f\u529d\u9000\u8a00\u8bba\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5373\u4f7f\u8fdb\u884c\u4e86\u52a0\u6743\u5fae\u8c03\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6307\u4ee4\u5fae\u8c03\u7684\u89e3\u7801\u5668\u578bLLM Llama-3.1\u8868\u73b0\u51fa\u8272\uff0c\u6700\u4f73\u6a21\u578b\u57283\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b8f\u89c2F1\u5206\u6570\u8fbe\u523044%\uff0c\u5728\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u523068%\u3002", "conclusion": "\u89e3\u7801\u5668\u578bLLMs\uff08\u7279\u522b\u662f\u7ecf\u8fc7\u6307\u4ee4\u5fae\u8c03\u7684\u7248\u672c\uff09\u5728\u8bc6\u522b\u56e2\u961f\u5bf9\u8bdd\u4e2d\u7684\u7ec6\u5fae\u5fae\u884c\u4e3a\u65b9\u9762\uff0c\u6bd4\u7f16\u7801\u5668\u578bLLMs\u66f4\u6709\u6548\u3002\u8fd9\u4e9b\u7ed3\u679c\u5bf9\u5f00\u53d1\u7528\u4e8e\u5206\u6790\u56e2\u961f\u6c9f\u901a\u52a8\u6001\u548c\u589e\u5f3a\u9ad8\u98ce\u9669\u73af\u5883\u8bad\u7ec3\u5e72\u9884\u7684\u8bed\u97f3\u6280\u672f\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5c24\u5176\u662f\u5728\u6587\u672c\u662f\u552f\u4e00\u53ef\u8bbf\u95ee\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2506.22745", "pdf": "https://arxiv.org/pdf/2506.22745", "abs": "https://arxiv.org/abs/2506.22745", "authors": ["Sijie He", "Ziye Jia", "Qiuming Zhu", "Fuhui Zhou", "Qihui Wu"], "title": "Trusted Routing for Blockchain-Enabled Low-Altitude Intelligent Networks", "categories": ["cs.NI"], "comment": "Low-altitude intelligent networks, trusted routing, blockchain, soft\n  hierarchical experience replay buffer, multi-agent deep reinforcement\n  learning", "summary": "Due to the scalability and portability, the low-altitude intelligent networks\n(LAINs) are essential in various fields such as surveillance and disaster\nrescue. However, in LAINs, unmanned aerial vehicles (UAVs) are characterized by\nthe distributed topology and high dynamic mobility, and vulnerable to security\nthreats, which may degrade the routing performance for data transmission.\nHence, how to ensure the routing stability and security of LAINs is a\nchallenge. In this paper, we focus on the routing process in LAINs with\nmultiple UAV clusters and propose the blockchain-enabled zero-trust\narchitecture to manage the joining and exiting of UAVs. Furthermore, we\nformulate the routing problem to minimize the end-to-end (E2E) delay, which is\nan integer linear programming and intractable to solve. Therefore, considering\nthe distribution of LAINs, we reformulate the routing problem into a\ndecentralized partially observable Markov decision process. With the proposed\nsoft hierarchical experience replay buffer, the multi-agent double deep\nQ-network based adaptive routing algorithm is designed. Finally, simulations\nare conducted and numerical results show that the total E2E delay of the\nproposed mechanism decreases by 22.38\\% than the benchmark on average.", "AI": {"tldr": "\u9488\u5bf9\u4f4e\u7a7a\u667a\u80fd\u7f51\u7edc\uff08LAINs\uff09\u4e2d\u65e0\u4eba\u673a\u8def\u7531\u7684\u7a33\u5b9a\u6027\u4e0e\u5b89\u5168\u6027\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u96f6\u4fe1\u4efb\u67b6\u6784\u7ba1\u7406\u65e0\u4eba\u673a\u52a0\u5165\u4e0e\u9000\u51fa\uff0c\u5e76\u5c06\u8def\u7531\u95ee\u9898\u5efa\u6a21\u4e3a\u53bb\u4e2d\u5fc3\u5316\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u8bbe\u8ba1\u4e86\u591a\u667a\u80fd\u4f53\u53cc\u6df1\u5ea6Q\u7f51\u7edc\u81ea\u9002\u5e94\u8def\u7531\u7b97\u6cd5\u3002\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u964d\u4f4e22.38%\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "motivation": "\u4f4e\u7a7a\u667a\u80fd\u7f51\u7edc\uff08LAINs\uff09\u4e2d\u7684\u65e0\u4eba\u673a\uff08UAVs\uff09\u5177\u6709\u5206\u5e03\u5f0f\u62d3\u6251\u3001\u9ad8\u52a8\u6001\u79fb\u52a8\u6027\uff0c\u4e14\u6613\u53d7\u5b89\u5168\u5a01\u80c1\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6570\u636e\u4f20\u8f93\u8def\u7531\u6027\u80fd\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u786e\u4fddLAINs\u7684\u8def\u7531\u7a33\u5b9a\u6027\u4e0e\u5b89\u5168\u6027\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "1. \u63d0\u51fa\u533a\u5757\u94fe\u8d4b\u80fd\u7684\u96f6\u4fe1\u4efb\u67b6\u6784\uff0c\u7528\u4e8e\u7ba1\u7406\u65e0\u4eba\u673a\u7684\u52a0\u5165\u4e0e\u9000\u51fa\u30022. \u5c06\u8def\u7531\u95ee\u9898\u516c\u5f0f\u5316\u4e3a\u6700\u5c0f\u5316\u7aef\u5230\u7aef\uff08E2E\uff09\u5ef6\u8fdf\u7684\u6574\u6570\u7ebf\u6027\u89c4\u5212\u30023. \u8003\u8651\u5230LAINs\u7684\u5206\u5e03\u5f0f\u7279\u6027\uff0c\u5c06\u8def\u7531\u95ee\u9898\u91cd\u6784\u4e3a\u53bb\u4e2d\u5fc3\u5316\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u30024. \u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6f\u5206\u5c42\u7ecf\u9a8c\u56de\u653e\u7f13\u51b2\u7684\u591a\u667a\u80fd\u4f53\u53cc\u6df1\u5ea6Q\u7f51\u7edc\uff08MADDPG\uff09\u81ea\u9002\u5e94\u8def\u7531\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u51c6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u673a\u5236\u4f7f\u603b\u7aef\u5230\u7aef\uff08E2E\uff09\u5ef6\u8fdf\u5e73\u5747\u964d\u4f4e\u4e8622.38%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u533a\u5757\u94fe\u8d4b\u80fd\u7684\u96f6\u4fe1\u4efb\u67b6\u6784\u548c\u57fa\u4e8eMADDPG\u7684\u81ea\u9002\u5e94\u8def\u7531\u7b97\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3LAINs\u4e2d\u7684\u8def\u7531\u7a33\u5b9a\u6027\u4e0e\u5b89\u5168\u6027\u95ee\u9898\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u4e86\u7f51\u7edc\u6027\u80fd\u3002"}}
{"id": "2506.22566", "pdf": "https://arxiv.org/pdf/2506.22566", "abs": "https://arxiv.org/abs/2506.22566", "authors": ["Jacob Adamczyk"], "title": "Exploration Behavior of Untrained Policies", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "High-dimensional Learning Dynamics Workshop at ICML-2025", "summary": "Exploration remains a fundamental challenge in reinforcement learning (RL),\nparticularly in environments with sparse or adversarial reward structures. In\nthis work, we study how the architecture of deep neural policies implicitly\nshapes exploration before training. We theoretically and empirically\ndemonstrate strategies for generating ballistic or diffusive trajectories from\nuntrained policies in a toy model. Using the theory of infinite-width networks\nand a continuous-time limit, we show that untrained policies return correlated\nactions and result in non-trivial state-visitation distributions. We discuss\nthe distributions of the corresponding trajectories for a standard\narchitecture, revealing insights into inductive biases for tackling\nexploration. Our results establish a theoretical and experimental framework for\nusing policy initialization as a design tool to understand exploration behavior\nin early training.", "AI": {"tldr": "\u7814\u7a76\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u7684\u67b6\u6784\u5982\u4f55\u5728\u5176\u8bad\u7ec3\u524d\u5f71\u54cd\u5f3a\u5316\u5b66\u4e60\u7684\u63a2\u7d22\u884c\u4e3a\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5956\u52b1\u7a00\u758f\u6216\u5bf9\u6297\u7684\u73af\u5883\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u7684\u67b6\u6784\u5982\u4f55\u9690\u5f0f\u5730\u5851\u9020\u8bad\u7ec3\u524d\u7684\u63a2\u7d22\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u5728\u4e00\u4e2a\u73a9\u5177\u6a21\u578b\u4e2d\u5c55\u793a\u4e86\u5982\u4f55\u4ece\u672a\u7ecf\u8bad\u7ec3\u7684\u7b56\u7565\u751f\u6210\u5f39\u9053\u6216\u6269\u6563\u8f68\u8ff9\u3002\u5229\u7528\u65e0\u9650\u5bbd\u5ea6\u7f51\u7edc\u7406\u8bba\u548c\u8fde\u7eed\u65f6\u95f4\u6781\u9650\uff0c\u5206\u6790\u4e86\u672a\u7ecf\u8bad\u7ec3\u7b56\u7565\u8fd4\u56de\u76f8\u5173\u52a8\u4f5c\u5e76\u4ea7\u751f\u975e\u5e73\u51e1\u72b6\u6001\u8bbf\u95ee\u5206\u5e03\u7684\u73b0\u8c61\uff0c\u5e76\u8ba8\u8bba\u4e86\u6807\u51c6\u67b6\u6784\u4e0b\u76f8\u5e94\u8f68\u8ff9\u7684\u5206\u5e03\u3002", "result": "\u672a\u7ecf\u8bad\u7ec3\u7684\u7b56\u7565\u4f1a\u4ea7\u751f\u76f8\u5173\u8054\u7684\u52a8\u4f5c\uff0c\u5e76\u5bfc\u81f4\u975e\u5e73\u51e1\u7684\u72b6\u6001\u8bbf\u95ee\u5206\u5e03\u3002\u7814\u7a76\u63ed\u793a\u4e86\u7b56\u7565\u67b6\u6784\uff08\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\uff09\u5bf9\u65e9\u671f\u63a2\u7d22\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5efa\u7acb\u4e86\u4e00\u4e2a\u7406\u8bba\u548c\u5b9e\u9a8c\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u7b56\u7565\u521d\u59cb\u5316\u53ef\u4f5c\u4e3a\u7406\u89e3\u65e9\u671f\u8bad\u7ec3\u4e2d\u63a2\u7d22\u884c\u4e3a\u7684\u8bbe\u8ba1\u5de5\u5177\u3002"}}
{"id": "2506.22992", "pdf": "https://arxiv.org/pdf/2506.22992", "abs": "https://arxiv.org/abs/2506.22992", "authors": ["Yulun Jiang", "Yekun Chai", "Maria Brbi\u0107", "Michael Moor"], "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.", "AI": {"tldr": "\u63d0\u51faMARBLE\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\uff0c\u63ed\u793a\u5f53\u524d\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u6a21\u6001\u9010\u6b65\u63a8\u7406\u548c\u611f\u77e5\u65b9\u9762\u7684\u4e25\u91cd\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u57fa\u51c6\u4fa7\u91cd\u6587\u672c\u6216\u7b80\u5355\u591a\u6a21\u6001\u68c0\u7d22\uff0c\u672a\u80fd\u6709\u6548\u8bc4\u4f30\u590d\u6742\u591a\u6a21\u6001\u9010\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u5bf9\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u9886\u57df\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u63d0\u51faMARBLE\u57fa\u51c6\uff0c\u5305\u542bM-Portal\u548cM-Cube\u4e24\u4e2a\u6311\u6218\u6027\u4efb\u52a1\uff0c\u8981\u6c42\u5728\u7a7a\u95f4\u3001\u89c6\u89c9\u548c\u7269\u7406\u7ea6\u675f\u4e0b\u8fdb\u884c\u591a\u6b65\u9aa4\u89c4\u5212\u548c\u7406\u89e3\uff0c\u4ee5\u4e25\u683c\u8bc4\u4f30\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u9010\u6b65\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5f53\u524d12\u79cd\u5148\u8fdb\u7684\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728MARBLE\u4e0a\u8868\u73b0\u6781\u5dee\uff08M-Portal\u63a5\u8fd1\u968f\u673a\uff0cM-Cube\u51c6\u786e\u7387\u4e3a0%\uff09\u3002\u590d\u6742\u63a8\u7406\u4ecd\u662f\u5de8\u5927\u6311\u6218\uff0c\u4e14\u611f\u77e5\u80fd\u529b\u662f\u4e3b\u8981\u74f6\u9888\u3002", "conclusion": "MARBLE\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u6a21\u6001\u63a8\u7406\u548c\u89c4\u5212\u65b9\u9762\u7684\u663e\u8457\u5c40\u9650\u6027\uff0c\u65e8\u5728\u63a8\u52a8\u80fd\u8fdb\u884c\u591a\u6b65\u9aa4\u591a\u6a21\u6001\u63a8\u7406\u548c\u89c4\u5212\u7684\u4e0b\u4e00\u4ee3\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.22505", "pdf": "https://arxiv.org/pdf/2506.22505", "abs": "https://arxiv.org/abs/2506.22505", "authors": ["Hassan Baker", "Matthew S. Emigh", "Austin J. Brockmeier"], "title": "Weakly Supervised Object Segmentation by Background Conditional Divergence", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "As a computer vision task, automatic object segmentation remains challenging\nin specialized image domains without massive labeled data, such as synthetic\naperture sonar images, remote sensing, biomedical imaging, etc. In any domain,\nobtaining pixel-wise segmentation masks is expensive. In this work, we propose\na method for training a masking network to perform binary object segmentation\nusing weak supervision in the form of image-wise presence or absence of an\nobject of interest, which provides less information but may be obtained more\nquickly from manual or automatic labeling. A key step in our method is that the\nsegmented objects can be placed into background-only images to create\nrealistic, images of the objects with counterfactual backgrounds. To create a\ncontrast between the original and counterfactual background images, we propose\nto first cluster the background-only images, and then during learning create\ncounterfactual images that blend objects segmented from their original source\nbackgrounds to backgrounds chosen from a targeted cluster. One term in the\ntraining loss is the divergence between these counterfactual images and the\nreal object images with backgrounds of the target cluster. The other term is a\nsupervised loss for background-only images. While an adversarial critic could\nprovide the divergence, we use sample-based divergences. We conduct experiments\non side-scan and synthetic aperture sonar in which our approach succeeds\ncompared to previous unsupervised segmentation baselines that were only tested\non natural images. Furthermore, to show generality we extend our experiments to\nnatural images, obtaining reasonable performance with our method that avoids\npretrained networks, generative networks, and adversarial critics. The basecode\nfor this work can be found at\n\\href{GitHub}{https://github.com/bakerhassan/WSOS}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5f31\u76d1\u7763\u7684\u4e8c\u5143\u76ee\u6807\u5206\u5272\u65b9\u6cd5\uff0c\u4ec5\u5229\u7528\u56fe\u50cf\u7ea7\u5b58\u5728/\u7f3a\u5931\u6807\u7b7e\u3002\u901a\u8fc7\u751f\u6210\u53cd\u4e8b\u5b9e\u80cc\u666f\u56fe\u50cf\u5e76\u8bbe\u8ba1\u7279\u5b9a\u635f\u5931\u9879\uff0c\u5728\u5408\u6210\u5b54\u5f84\u58f0\u7eb3\u7b49\u4e13\u4e1a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u65e0\u9700\u9884\u8bad\u7ec3\u7f51\u7edc\u4e5f\u80fd\u83b7\u5f97\u5408\u7406\u6027\u80fd\u3002", "motivation": "\u5728\u5408\u6210\u5b54\u5f84\u58f0\u7eb3\u3001\u9065\u611f\u3001\u751f\u7269\u533b\u5b66\u6210\u50cf\u7b49\u7f3a\u4e4f\u5927\u91cf\u50cf\u7d20\u7ea7\u6807\u6ce8\u6570\u636e\u7684\u4e13\u4e1a\u56fe\u50cf\u9886\u57df\uff0c\u81ea\u52a8\u76ee\u6807\u5206\u5272\u4ecd\u5177\u6311\u6218\u6027\uff0c\u4e14\u83b7\u53d6\u50cf\u7d20\u7ea7\u63a9\u7801\u6210\u672c\u9ad8\u6602\u3002", "method": "\u8be5\u65b9\u6cd5\u8bad\u7ec3\u4e00\u4e2a\u63a9\u7801\u7f51\u7edc\uff0c\u5229\u7528\u56fe\u50cf\u7ea7\u76ee\u6807\u5b58\u5728/\u7f3a\u5931\u7684\u5f31\u76d1\u7763\u4fe1\u606f\u8fdb\u884c\u4e8c\u5143\u5206\u5272\u3002\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u5206\u5272\u51fa\u7684\u76ee\u6807\u653e\u7f6e\u5230\u7eaf\u80cc\u666f\u56fe\u50cf\u4e2d\uff0c\u521b\u5efa\u5177\u6709\u53cd\u4e8b\u5b9e\u80cc\u666f\u7684\u56fe\u50cf\u3002\u5177\u4f53\u6b65\u9aa4\u5305\u62ec\uff1a\u805a\u7c7b\u80cc\u666f\u56fe\u50cf\uff0c\u7136\u540e\u5c06\u76ee\u6807\u4e0e\u9009\u5b9a\u805a\u7c7b\u7684\u80cc\u666f\u878d\u5408\u751f\u6210\u53cd\u4e8b\u5b9e\u56fe\u50cf\u3002\u8bad\u7ec3\u635f\u5931\u5305\u542b\u53cd\u4e8b\u5b9e\u56fe\u50cf\u4e0e\u771f\u5b9e\u76ee\u6807\u56fe\u50cf\u7684\u6563\u5ea6\uff08\u4f7f\u7528\u57fa\u4e8e\u6837\u672c\u7684\u6563\u5ea6\uff09\u4ee5\u53ca\u9488\u5bf9\u7eaf\u80cc\u666f\u56fe\u50cf\u7684\u76d1\u7763\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u5728\u65c1\u4fa7\u626b\u63cf\u548c\u5408\u6210\u5b54\u5f84\u58f0\u7eb3\u56fe\u50cf\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u4e4b\u524d\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u6d4b\u8bd5\u7684\u65e0\u76d1\u7763\u57fa\u7ebf\u53d6\u5f97\u4e86\u6210\u529f\u3002\u6b64\u5916\uff0c\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u4e5f\u80fd\u83b7\u5f97\u5408\u7406\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56\u9884\u8bad\u7ec3\u7f51\u7edc\u3001\u751f\u6210\u7f51\u7edc\u6216\u5bf9\u6297\u6027\u5224\u522b\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6570\u636e\u7a00\u7f3a\u7684\u4e13\u4e1a\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5f31\u76d1\u7763\u4e8c\u5143\u76ee\u6807\u5206\u5272\u65b9\u6848\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u53cd\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\u548c\u635f\u5931\u8bbe\u8ba1\uff0c\u5728\u4e0d\u4f7f\u7528\u590d\u6742\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.22694", "pdf": "https://arxiv.org/pdf/2506.22694", "abs": "https://arxiv.org/abs/2506.22694", "authors": ["Raghavv Goel", "Sudhanshu Agrawal", "Mukul Gagrani", "Junyoung Park", "Yifan Zao", "He Zhang", "Tian Liu", "Yiping Yang", "Xin Yuan", "Jiuyan Lu", "Chris Lott", "Mingu Lee"], "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs", "categories": ["cs.CL"], "comment": "7 pages, 4 figures, 5 tables, accepted at ICML 2025 workshop on\n  Efficient Systems for Foundational Models", "summary": "In this paper, we introduce a simple training-free technique to improve the\nperformance of drafter-based speculative decoding (SpD) methods that\nincorporates language modeling head (LM head) during drafting process. A\ndrafter-based speculative decoding leverages one or more smaller language\nmodels, a.k.a. drafters or draft models, to sample a draft sequence or tree\nconsisting of multiple tokens, followed by verification by a base LLM, a target\nmodel, accepting a subset as its valid generation. As it is usually considered\nthat the speculative decoding requires one-to-one mapping between vocabularies\nof the target model and the draft model, it has been natural to share the\nvocabulary between them, or even share the LM head as in EAGLE or Medusa. We\nfirst identify that this draft token sampling scheme inherently contains an\nunnecessary inference overhead in drafting, especially for some target LLMs\nwith very large vocabularies. Then, we propose a simple technique, VocabTrim,\nto mitigate the drafting overhead to improve the generation speed in\nmemory-bound environment. VocabTrim reconstructs the drafter LM head to contain\nonly a limited set of tokens, selected by the most frequently sampled from the\nvocabulary of the target model. While limiting the vocabulary in drafting\nslightly degrades the acceptance rate, it significantly reduces the drafting\nlatency in memory-bound process which is often the case on edge devices,\nresulting in higher memory-bound speed up (MBSU). We show that our method can\nboost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically\nby 16% for Llama-3.2-3B-Instruct.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u514d\u8bad\u7ec3\u7684VocabTrim\u6280\u672f\uff0c\u901a\u8fc7\u88c1\u526a\u8349\u7a3f\u6a21\u578bLM\u5934\u7684\u8bcd\u8868\uff0c\u51cf\u5c11\u63a8\u6d4b\u89e3\u7801\u7684\u8349\u7a3f\u9636\u6bb5\u5f00\u9500\uff0c\u63d0\u9ad8\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u7684\u751f\u6210\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u8349\u7a3f\u6a21\u578b\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u5728\u8349\u7a3f\u9636\u6bb5\u5b58\u5728\u4e0d\u5fc5\u8981\u7684\u63a8\u7406\u5f00\u9500\uff0c\u5c24\u5176\u5f53\u76ee\u6807\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bcd\u8868\u975e\u5e38\u5927\u65f6\uff0c\u8fd9\u5bfc\u81f4\u5728\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e2d\u751f\u6210\u901f\u5ea6\u4e0d\u7406\u60f3\u3002", "method": "\u672c\u6587\u63d0\u51faVocabTrim\u6280\u672f\uff0c\u5b83\u901a\u8fc7\u91cd\u6784\u8349\u7a3f\u6a21\u578b\u7684\u8bed\u8a00\u6a21\u578b\u5934\uff08LM head\uff09\uff0c\u4f7f\u5176\u4ec5\u5305\u542b\u76ee\u6807\u6a21\u578b\u8bcd\u8868\u4e2d\u9009\u62e9\u7684\u6700\u5e38\u7528\uff08\u6700\u9891\u7e41\u91c7\u6837\uff09\u7684\u6709\u9650\u4ee3\u5e01\u96c6\u5408\u3002\u6b64\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u65e8\u5728\u51cf\u8f7b\u8349\u7a3f\u9636\u6bb5\u7684\u8ba1\u7b97\u5f00\u9500\u5e76\u63d0\u9ad8\u751f\u6210\u901f\u5ea6\u3002", "result": "VocabTrim\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u7684\u8349\u7a3f\u5ef6\u8fdf\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5185\u5b58\u53d7\u9650\u52a0\u901f\uff08MBSU\uff09\u3002\u5c3d\u7ba1\u8be5\u65b9\u6cd5\u4f1a\u8f7b\u5fae\u964d\u4f4e\u63a5\u53d7\u7387\uff0c\u4f46\u5b83\u80fd\u5c06Llama-3\u6a21\u578b\uff08\u4f8b\u5982Llama-3.2-3B-Instruct\uff09\u5728Spec-Bench\u4e0a\u7684MBSU\u63d0\u534716%\u3002", "conclusion": "VocabTrim\u901a\u8fc7\u667a\u80fd\u5730\u88c1\u526a\u8349\u7a3f\u6a21\u578b\u7684\u8bcd\u8868\uff0c\u6709\u6548\u51cf\u8f7b\u4e86\u63a8\u6d4b\u89e3\u7801\u4e2d\u7684\u8349\u7a3f\u5f00\u9500\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u901f\u5ea6\uff0c\u5c24\u5176\u5728\u8fb9\u7f18\u8bbe\u5907\u5e38\u89c1\u7684\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6574\u4f53\u6548\u7387\u3002"}}
{"id": "2506.22793", "pdf": "https://arxiv.org/pdf/2506.22793", "abs": "https://arxiv.org/abs/2506.22793", "authors": ["Pegah Alizadeh", "Anastasios Giovanidis", "Pradeepa Ramachandra", "Vasileios Koutsoukis", "Osama Arouk"], "title": "Offline Reinforcement Learning for Mobility Robustness Optimization", "categories": ["cs.NI", "cs.AI", "cs.PF"], "comment": "7 pages, double column, 4 figures, 6 tables, conference submission", "summary": "In this work we revisit the Mobility Robustness Optimisation (MRO) algorithm\nand study the possibility of learning the optimal Cell Individual Offset tuning\nusing offline Reinforcement Learning. Such methods make use of collected\noffline datasets to learn the optimal policy, without further exploration. We\nadapt and apply a sequence-based method called Decision Transformers as well as\na value-based method called Conservative Q-Learning to learn the optimal policy\nfor the same target reward as the vanilla rule-based MRO. The same input\nfeatures related to failures, ping-pongs, and other handover issues are used.\nEvaluation for realistic New Radio networks with 3500 MHz carrier frequency on\na traffic mix including diverse user service types and a specific tunable\ncell-pair shows that offline-RL methods outperform rule-based MRO, offering up\nto 7% improvement. Furthermore, offline-RL can be trained for diverse objective\nfunctions using the same available dataset, thus offering operational\nflexibility compared to rule-based methods.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4f7f\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08\u5305\u62ec\u51b3\u7b56Transformer\u548c\u4fdd\u5b88Q\u5b66\u4e60\uff09\u4f18\u5316MRO\u4e2d\u7684\u5c0f\u533a\u4e2a\u4f53\u504f\u79fb\u8c03\u6574\uff0c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u63d0\u5347\uff08\u9ad8\u8fbe7%\uff09\uff0c\u5e76\u6bd4\u57fa\u4e8e\u89c4\u5219\u7684MRO\u66f4\u5177\u7075\u6d3b\u6027\u3002", "motivation": "\u65e8\u5728\u6539\u8fdb\u79fb\u52a8\u6027\u9c81\u68d2\u6027\u4f18\u5316\uff08MRO\uff09\u7b97\u6cd5\u4e2d\u7684\u5c0f\u533a\u4e2a\u4f53\u504f\u79fb\u8c03\u6574\uff0c\u901a\u8fc7\u5229\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6765\u5b66\u4e60\u6700\u4f18\u7b56\u7565\uff0c\u4ee5\u671f\u514b\u670d\u73b0\u6709\u57fa\u4e8e\u89c4\u5219\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5177\u4f53\u5e94\u7528\u4e86\u57fa\u4e8e\u5e8f\u5217\u7684\u51b3\u7b56Transformer\uff08Decision Transformers\uff09\u548c\u57fa\u4e8e\u4ef7\u503c\u7684\u4fdd\u5b88Q\u5b66\u4e60\uff08Conservative Q-Learning\uff09\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5229\u7528\u79bb\u7ebf\u6570\u636e\u96c6\u5b66\u4e60\u6700\u4f18\u7b56\u7565\uff0c\u65e0\u9700\u989d\u5916\u63a2\u7d22\uff0c\u5e76\u6cbf\u7528\u4e86\u4e0e\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219MRO\u76f8\u540c\u7684\u8f93\u5165\u7279\u5f81\uff08\u5982\u6545\u969c\u3001\u4e52\u4e53\u6548\u5e94\u548c\u5207\u6362\u95ee\u9898\uff09\u3002", "result": "\u5728\u771f\u5b9e\u7684New Radio\u7f51\u7edc\uff083500 MHz\u8f7d\u6ce2\u9891\u7387\u3001\u591a\u6837\u5316\u4e1a\u52a1\u7ec4\u5408\uff09\u4e2d\uff0c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684MRO\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe7%\u3002", "conclusion": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u662fMRO\u4f18\u5316\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\uff0c\u5b83\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u6027\u80fd\u548c\u66f4\u9ad8\u7684\u64cd\u4f5c\u7075\u6d3b\u6027\uff08\u56e0\u5176\u80fd\u5229\u7528\u76f8\u540c\u6570\u636e\u96c6\u8bad\u7ec3\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u76ee\u6807\u51fd\u6570\uff09\u3002"}}
{"id": "2506.22578", "pdf": "https://arxiv.org/pdf/2506.22578", "abs": "https://arxiv.org/abs/2506.22578", "authors": ["Xufei Lv", "Haoyuan Sun", "Xuefeng Bai", "Min Zhang", "Houde Liu", "Kehai Chen"], "title": "The Hidden Link Between RLHF and Contrastive Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Alignment of large language models (LLMs) with human values has recently\ngarnered significant attention, with prominent examples including the canonical\nyet costly Reinforcement Learning from Human Feedback (RLHF) and the simple\nDirect Preference Optimization (DPO). In this work, we demonstrate that both\nRLHF and DPO can be interpreted from the perspective of mutual information (MI)\nmaximization, uncovering a profound connection to contrastive learning. Within\nthis framework, both RLHF and DPO can be viewed as methods that perform\ncontrastive learning based on the positive and negative samples derived from\nthe base model, leveraging the Donsker-Varadhan (DV) lower bound on MI\n(equivalently, the MINE estimator). This paradigm further explains why RLHF may\nnot intrinsically incentivize reasoning capacities in LLMs beyond what is\nalready present in the base model. Building on this perspective, we replace the\nDV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual\nInformation Optimization (MIO). Comprehensive theoretical analysis and\nextensive empirical evaluations demonstrate that MIO mitigates the late-stage\ndecline in chosen-likelihood observed in DPO, achieving competitive or superior\nperformance across various challenging reasoning and mathematical benchmarks.\nWe will release the model and code upon acceptance.", "AI": {"tldr": "\u8bba\u6587\u5c06RLHF\u548cDPO\u4ece\u4e92\u4fe1\u606f\u6700\u5927\u5316\u89d2\u5ea6\u7edf\u4e00\u89e3\u91ca\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u65b0\u7684\u5bf9\u9f50\u65b9\u6cd5MIO\uff0c\u89e3\u51b3\u4e86DPO\u7684\u540e\u671f\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u5728\u63a8\u7406\u548c\u6570\u5b66\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "LLM\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5982RLHF\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u5176\u5185\u5728\u673a\u5236\u53ca\u5bf9\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\u5c1a\u4e0d\u5b8c\u5168\u6e05\u695a\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u7406\u89e3RLHF\u548cDPO\u7684\u539f\u7406\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u66f4\u4f18\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "1. \u5c06RLHF\u548cDPO\u89e3\u91ca\u4e3a\u57fa\u4e8e\u4e92\u4fe1\u606f\uff08MI\uff09\u6700\u5927\u5316\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528Donsker-Varadhan\uff08DV\uff09\u4e0b\u754c\uff08\u5373MINE\u4f30\u8ba1\u5668\uff09\u30022. \u57fa\u4e8e\u6b64\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u4e92\u4fe1\u606f\u4f18\u5316\uff08MIO\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7528Jensen-Shannon MI\u4f30\u8ba1\u5668\u66ff\u4ee3\u4e86DV/MINE\u3002", "result": "1. \u89e3\u91ca\u4e86RLHF\u53ef\u80fd\u65e0\u6cd5\u672c\u8d28\u4e0a\u6fc0\u52b1LLM\u63a8\u7406\u80fd\u529b\u7684\u539f\u56e0\u30022. \u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cMIO\u80fd\u6709\u6548\u7f13\u89e3DPO\u4e2d\u89c2\u6d4b\u5230\u7684\u201cchosen-likelihood\u201d\u540e\u671f\u4e0b\u964d\u95ee\u9898\u30023. MIO\u5728\u591a\u79cd\u590d\u6742\u7684\u63a8\u7406\u548c\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5c06RLHF\u548cDPO\u7edf\u4e00\u4e8e\u4e92\u4fe1\u606f\u6700\u5927\u5316\u6846\u67b6\uff0c\u4e0d\u4ec5\u52a0\u6df1\u4e86\u5bf9\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u7684\u7406\u89e3\uff0c\u66f4\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b0\u65b9\u6cd5MIO\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5bf9\u9f50\u6548\u679c\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86LLM\u5728\u63a8\u7406\u548c\u6570\u5b66\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2506.23049", "pdf": "https://arxiv.org/pdf/2506.23049", "abs": "https://arxiv.org/abs/2506.23049", "authors": ["Leander Melroy Maben", "Gayathri Ganesh Lakshmy", "Srijith Radhakrishnan", "Siddhant Arora", "Shinji Watanabe"], "title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS", "68T42, 68T50,", "I.2.7; I.2.11; H.5.5"], "comment": null, "summary": "Despite advances in language and speech technologies, no open-source system\nenables full speech-to-speech, multi-turn dialogue with integrated tool use and\nagentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and\nAutomated Tool Use), the first open-source, speech-native assistant capable of\ncompleting complex, goal-driven tasks through dynamic tool invocation and\nmulti-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a\ncascaded pipeline and supports tools such as calendar booking, contact lookup,\nweb search, and email. Its modular design allows easy integration of new tools\nusing natural language prompts and action classes. On VoiceBench, AURA scores\n92.75% on OpenBookQA-outperforming all open-weight systems and nearing\nGPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.\nHuman evaluation shows 90% task success on complex, multi-turn speech tasks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86AURA\uff0c\u9996\u4e2a\u5f00\u6e90\u7684\u8bed\u97f3\u539f\u751f\u52a9\u624b\uff0c\u80fd\u591f\u901a\u8fc7\u52a8\u6001\u5de5\u5177\u8c03\u7528\u548c\u591a\u8f6e\u5bf9\u8bdd\u5b8c\u6210\u590d\u6742\u7684\u4efb\u52a1\u3002", "motivation": "\u5c3d\u7ba1\u8bed\u8a00\u548c\u8bed\u97f3\u6280\u672f\u4e0d\u65ad\u8fdb\u6b65\uff0c\u4f46\u76ee\u524d\u5c1a\u65e0\u5f00\u6e90\u7cfb\u7edf\u80fd\u5b9e\u73b0\u5305\u542b\u5de5\u5177\u4f7f\u7528\u548c\u667a\u80fd\u63a8\u7406\u7684\u5168\u7a0b\u8bed\u97f3\u5230\u8bed\u97f3\u3001\u591a\u8f6e\u5bf9\u8bdd\u529f\u80fd\u3002", "method": "AURA\u5c06\u5f00\u6e90\u7684ASR\uff08\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff09\u3001TTS\uff08\u6587\u672c\u5230\u8bed\u97f3\uff09\u548cLLM\uff08\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u4ee5\u7ea7\u8054\u7ba1\u9053\u5f62\u5f0f\u7ed3\u5408\uff0c\u652f\u6301\u65e5\u5386\u9884\u8ba2\u3001\u8054\u7cfb\u4eba\u67e5\u8be2\u3001\u7f51\u9875\u641c\u7d22\u548c\u7535\u5b50\u90ae\u4ef6\u7b49\u5de5\u5177\uff0c\u5e76\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5141\u8bb8\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8f7b\u677e\u96c6\u6210\u65b0\u5de5\u5177\u3002", "result": "\u5728VoiceBench\u4e0a\uff0cAURA\u5728OpenBookQA\u4efb\u52a1\u4e2d\u5f97\u5206\u4e3a92.75%\uff0c\u8d85\u8d8a\u6240\u6709\u5f00\u6e90\u7cfb\u7edf\u5e76\u63a5\u8fd1GPT-4o\uff1b\u5728AlpacaEval\u4e0a\u5f97\u5206\u4e3a4.39\uff0c\u4e0e\u5176\u5b83\u5f00\u6e90\u7cfb\u7edf\u76f8\u5f53\u3002\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u590d\u6742\u7684\u591a\u8f6e\u8bed\u97f3\u4efb\u52a1\u4e2d\uff0c\u4efb\u52a1\u6210\u529f\u7387\u8fbe\u523090%\u3002", "conclusion": "AURA\u662f\u7b2c\u4e00\u4e2a\u5f00\u6e90\u7684\u8bed\u97f3\u539f\u751f\u667a\u80fd\u52a9\u624b\uff0c\u5b83\u6709\u6548\u5f25\u8865\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u6a21\u6001\u6280\u672f\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u8bed\u97f3\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u5353\u8d8a\u6027\u80fd\u548c\u5b9e\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.22509", "pdf": "https://arxiv.org/pdf/2506.22509", "abs": "https://arxiv.org/abs/2506.22509", "authors": ["Hang Xu", "Jie Huang", "Linjiang Huang", "Dong Li", "Yidi Liu", "Feng Zhao"], "title": "FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV2025", "summary": "Domain Adaptation(DA) for dense prediction tasks is an important topic, which\nenhances the dense prediction model's performance when tested on its unseen\ndomain. Recently, with the development of Diffusion-based Dense Prediction\n(DDP) models, the exploration of DA designs tailored to this framework is worth\nexploring, since the diffusion model is effective in modeling the distribution\ntransformation that comprises domain information. In this work, we propose a\ntraining-free mechanism for DDP frameworks, endowing them with DA capabilities.\nOur motivation arises from the observation that the exposure bias (e.g., noise\nstatistics bias) in diffusion brings domain shift, and different domains in\nconditions of DDP models can also be effectively captured by the noise\nprediction statistics. Based on this, we propose a training-free Domain Noise\nAlignment (DNA) approach, which alleviates the variations of noise statistics\nto domain changes during the diffusion sampling process, thereby achieving\ndomain adaptation. Specifically, when the source domain is available, we\ndirectly adopt the DNA method to achieve domain adaptation by aligning the\nnoise statistics of the target domain with those of the source domain. For the\nmore challenging source-free DA, inspired by the observation that regions\ncloser to the source domain exhibit higher confidence meeting variations of\nsampling noise, we utilize the statistics from the high-confidence regions\nprogressively to guide the noise statistic adjustment during the sampling\nprocess. Notably, our method demonstrates the effectiveness of enhancing the DA\ncapability of DDP models across four common dense prediction tasks. Code is\navailable at\n\\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u57df\u566a\u58f0\u5bf9\u9f50\uff08DNA\uff09\u7684\u514d\u8bad\u7ec3\u673a\u5236\uff0c\u901a\u8fc7\u8c03\u6574\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\u7edf\u8ba1\u91cf\uff0c\u4e3a\u57fa\u4e8e\u6269\u6563\u7684\u5bc6\u96c6\u9884\u6d4b\uff08DDP\uff09\u6a21\u578b\u5b9e\u73b0\u57df\u9002\u5e94\uff08DA\uff09\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u57df\u9002\u5e94\u5bf9\u4e8e\u5bc6\u96c6\u9884\u6d4b\u6a21\u578b\u5728\u672a\u89c1\u57df\u4e0a\u7684\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\u3002\u968f\u7740DDP\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u63a2\u7d22\u5176DA\u8bbe\u8ba1\u53d8\u5f97\u6709\u4ef7\u503c\uff0c\u56e0\u4e3a\u6269\u6563\u6a21\u578b\u80fd\u6709\u6548\u6a21\u62df\u5305\u542b\u57df\u4fe1\u606f\u7684\u5206\u5e03\u8f6c\u6362\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6269\u6563\u4e2d\u7684\u66dd\u5149\u504f\u5dee\uff08\u5982\u566a\u58f0\u7edf\u8ba1\u504f\u5dee\uff09\u4f1a\u5bfc\u81f4\u57df\u504f\u79fb\uff0c\u4e14DDP\u6a21\u578b\u6761\u4ef6\u4e0b\u7684\u4e0d\u540c\u57df\u53ef\u4ee5\u901a\u8fc7\u566a\u58f0\u9884\u6d4b\u7edf\u8ba1\u91cf\u6709\u6548\u6355\u6349\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u57df\u566a\u58f0\u5bf9\u9f50\uff08DNA\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7f13\u89e3\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u4e2d\u566a\u58f0\u7edf\u8ba1\u91cf\u968f\u57df\u53d8\u5316\u800c\u4ea7\u751f\u7684\u5dee\u5f02\u6765\u5b9e\u73b0\u57df\u9002\u5e94\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5f53\u6e90\u57df\u53ef\u7528\u65f6\uff0c\u76f4\u63a5\u5bf9\u9f50\u76ee\u6807\u57df\u4e0e\u6e90\u57df\u7684\u566a\u58f0\u7edf\u8ba1\u91cf\u3002\u5bf9\u4e8e\u66f4\u5177\u6311\u6218\u6027\u7684\u65e0\u6e90DA\uff0c\u5219\u5229\u7528\u9ad8\u7f6e\u4fe1\u5ea6\u533a\u57df\uff08\u66f4\u63a5\u8fd1\u6e90\u57df\uff09\u7684\u7edf\u8ba1\u91cf\uff0c\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u9010\u6b65\u5f15\u5bfc\u566a\u58f0\u7edf\u8ba1\u91cf\u8c03\u6574\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u5730\u63d0\u5347\u4e86DDP\u6a21\u578b\u5728\u56db\u79cd\u5e38\u89c1\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u57df\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "DNA\u662f\u4e00\u79cd\u6709\u6548\u4e14\u514d\u8bad\u7ec3\u7684DDP\u6a21\u578b\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u566a\u58f0\u7edf\u8ba1\u91cf\u89e3\u51b3\u4e86\u6709\u6e90\u548c\u65e0\u6e90\u7684\u57df\u9002\u5e94\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u57df\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22698", "pdf": "https://arxiv.org/pdf/2506.22698", "abs": "https://arxiv.org/abs/2506.22698", "authors": ["Emily Dux Speltz"], "title": "Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration.", "AI": {"tldr": "\u672c\u62a5\u544a\u603b\u7ed3\u4e86\u4e00\u4e2a\u8de8\u5b66\u79d1\u7814\u8ba8\u4f1a\u7684\u6210\u679c\uff0c\u63a2\u8ba8\u4e86AI\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5728\u6587\u672c\u7406\u89e3\u4e0e\u751f\u6210\u4e2d\u7684\u5173\u7cfb\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\u3001\u5c40\u9650\u6027\u53ca\u4eba\u673a\u534f\u4f5c\u7684\u673a\u9047\u4e0e\u6311\u6218\u3002", "motivation": "\u8be5\u7814\u8ba8\u4f1a\u65e8\u5728\u5f25\u8865AI\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u5728\u6587\u672c\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u77e5\u8bc6\u7406\u89e3\u4e0a\u7684\u5173\u952e\u7a7a\u767d\uff0c\u5e76\u63a2\u7d22AI\u5982\u4f55\u589e\u8fdb\u5bf9\u8fd9\u4e9b\u8fc7\u7a0b\u7684\u7406\u89e3\u548c\u589e\u5f3a\u4eba\u7c7b\u80fd\u529b\u3002", "method": "\u8be5\u62a5\u544a\u901a\u8fc7\u7efc\u5408\u8fd1\u671f\u4e00\u4e2a\u6c47\u805a\u4e86\u8ba4\u77e5\u5fc3\u7406\u5b66\u3001\u8bed\u8a00\u5b66\u4e60\u548cAI\uff08\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff09\u9886\u57df\u4e13\u5bb6\u7684\u8de8\u5b66\u79d1\u7814\u8ba8\u4f1a\u7684\u6210\u679c\u800c\u5b8c\u6210\u3002\u7814\u8ba8\u4f1a\u901a\u8fc7\u8ba4\u77e5\u3001\u8bed\u8a00\u548c\u6280\u672f\u89c6\u89d2\u7684\u534f\u4f5c\u5bf9\u8bdd\u8fdb\u884c\u3002", "result": "\u7814\u8ba8\u4f1a\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u4e4b\u95f4\u7684\u65b0\u5174\u6a21\u5f0f\uff0c\u5305\u62ecLLMs\u7684\u6f5c\u80fd\u53ca\u5176\u5728\u5b8c\u5168\u590d\u5236\u4eba\u7c7b\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u4e3b\u8981\u53d1\u73b0\u5305\u62ecLLMs\u63d0\u4f9b\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u6d1e\u5bdf\u7684\u6f5c\u529b\u3001\u7ecf\u4eba\u7c7b\u53cd\u9988\u5fae\u8c03\u540eLLM\u884c\u4e3a\u4e0e\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u65e5\u76ca\u8d8b\u540c\uff0c\u4ee5\u53ca\u4eba\u673a\u5728\u8bed\u8a00\u4efb\u52a1\u534f\u4f5c\u4e2d\u7684\u673a\u9047\u4e0e\u6311\u6218\u3002", "conclusion": "\u8be5\u62a5\u544a\u65e8\u5728\u6307\u5bfc\u672a\u6765LLMs\u5728\u8ba4\u77e5\u5fc3\u7406\u5b66\u3001\u8bed\u8a00\u5b66\u548c\u6559\u80b2\u9886\u57df\u7684\u7814\u7a76\u3001\u5f00\u53d1\u548c\u5b9e\u65bd\uff0c\u5e76\u5f3a\u8c03\u4e86\u9053\u5fb7\u8003\u91cf\u4e0e\u8d1f\u8d23\u4efb\u4f7f\u7528AI\u6280\u672f\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u671f\u901a\u8fc7\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\u589e\u5f3a\u4eba\u7c7b\u5728\u6587\u672c\u7406\u89e3\u548c\u751f\u4ea7\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2506.22875", "pdf": "https://arxiv.org/pdf/2506.22875", "abs": "https://arxiv.org/abs/2506.22875", "authors": ["Everson Flores", "Bruna Guterres", "Thomaz Pereira Junior", "Paula Barros", "Alberto Cabral", "Cristiana Lima Dora", "Marcelo Malheiros", "Marcelo Pias"], "title": "Reliable Image Transmission in CPS-based Pub/Sub", "categories": ["cs.NI", "cs.DC"], "comment": "10 pages, 4 figures", "summary": "Developments in communication and automation have driven the expansion of\ndistributed networks, essential for IoT and CPS development in industrial\napplications requiring reliable image processing and real-time adaptability.\nAlthough broadly adopted, there is a literature gap regarding the performance\nof MQTT protocol for image sharing and transmission under high-traffic\nscenarios with intermittent connectivity, restricting its use in critical IoT\nand CPS applications. In this context, the present work examines the\nreliability of real-time image transmission in IoT and CPS industrial systems\nthat utilize the MQTT-based publish/subscribe communication model. It focuses\non scenarios with network interruptions and high data traffic, evaluating the\nperformance of a distributed system through a series of controlled testbed\nvalidation experiments. Experimental validation demonstrated that while the\nMQTT-based system sustains reliable transmission under normal conditions, its\nrecovery capability depends on the failure point, with complete restoration\noccurring when disruptions affect the Orchestrator Node and partial recovery\nwhen the Producer Node or Broker are affected. The study also confirmed that\nthe system prevents duplicate errors and adapts well to increasing network\ndemands, reinforcing its suitability for industrial applications that require\nefficient and resilient data handling.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86MQTT\u534f\u8bae\u5728\u5de5\u4e1aIoT/CPS\u9ad8\u6d41\u91cf\u548c\u95f4\u6b47\u6027\u8fde\u63a5\u4e0b\u5b9e\u65f6\u56fe\u50cf\u4f20\u8f93\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5de5\u4e1aIoT\u548cCPS\u5e94\u7528\u9700\u8981\u53ef\u9760\u7684\u56fe\u50cf\u5904\u7406\u548c\u5b9e\u65f6\u9002\u5e94\u6027\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9MQTT\u534f\u8bae\u5728\u56fe\u50cf\u5171\u4eab\u548c\u4f20\u8f93\u4e2d\uff0c\u9ad8\u6d41\u91cf\u53ca\u95f4\u6b47\u6027\u8fde\u63a5\u573a\u666f\u4e0b\u6027\u80fd\u7684\u8bc4\u4f30\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u53d7\u63a7\u7684\u6d4b\u8bd5\u5e73\u53f0\u9a8c\u8bc1\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e86\u5728\u7f51\u7edc\u4e2d\u65ad\u548c\u9ad8\u6570\u636e\u6d41\u91cf\u60c5\u666f\u4e0b\uff0c\u5de5\u4e1aIoT\u548cCPS\u7cfb\u7edf\u4e2d\u57fa\u4e8eMQTT\u53d1\u5e03/\u8ba2\u9605\u901a\u4fe1\u6a21\u578b\u7684\u5b9e\u65f6\u56fe\u50cf\u4f20\u8f93\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cMQTT\u7cfb\u7edf\u5728\u6b63\u5e38\u6761\u4ef6\u4e0b\u4f20\u8f93\u53ef\u9760\u3002\u5176\u6062\u590d\u80fd\u529b\u53d6\u51b3\u4e8e\u6545\u969c\u70b9\uff1a\u534f\u8c03\u5668\u8282\u70b9\u6545\u969c\u65f6\u53ef\u5b8c\u5168\u6062\u590d\uff0c\u800c\u751f\u4ea7\u8005\u8282\u70b9\u6216\u4ee3\u7406\u6545\u969c\u65f6\u4ec5\u80fd\u90e8\u5206\u6062\u590d\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u80fd\u9632\u6b62\u91cd\u590d\u9519\u8bef\u5e76\u826f\u597d\u9002\u5e94\u4e0d\u65ad\u589e\u957f\u7684\u7f51\u7edc\u9700\u6c42\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u5b9e\u4e86\u57fa\u4e8eMQTT\u7684\u7cfb\u7edf\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u6548\u548c\u5f39\u6027\u6570\u636e\u5904\u7406\u7684\u5de5\u4e1a\u5e94\u7528\u3002"}}
{"id": "2506.22602", "pdf": "https://arxiv.org/pdf/2506.22602", "abs": "https://arxiv.org/abs/2506.22602", "authors": ["Joshua C. Zhao", "Saurabh Bagchi"], "title": "Are Fast Methods Stable in Adversarially Robust Transfer Learning?", "categories": ["cs.LG", "stat.ML"], "comment": "13 pages", "summary": "Transfer learning is often used to decrease the computational cost of model\ntraining, as fine-tuning a model allows a downstream task to leverage the\nfeatures learned from the pre-training dataset and quickly adapt them to a new\ntask. This is particularly useful for achieving adversarial robustness, as\nadversarially training models from scratch is very computationally expensive.\nHowever, high robustness in transfer learning still requires adversarial\ntraining during the fine-tuning phase, which requires up to an order of\nmagnitude more time than standard fine-tuning. In this work, we revisit the use\nof the fast gradient sign method (FGSM) in robust transfer learning to improve\nthe computational cost of adversarial fine-tuning. We surprisingly find that\nFGSM is much more stable in adversarial fine-tuning than when training from\nscratch. In particular, FGSM fine-tuning does not suffer from any issues with\ncatastrophic overfitting at standard perturbation budgets of $\\varepsilon=4$ or\n$\\varepsilon=8$. This stability is further enhanced with parameter-efficient\nfine-tuning methods, where FGSM remains stable even up to $\\varepsilon=32$ for\nlinear probing. We demonstrate how this stability translates into performance\nacross multiple datasets. Compared to fine-tuning with the more commonly used\nmethod of projected gradient descent (PGD), on average, FGSM only loses 0.39%\nand 1.39% test robustness for $\\varepsilon=4$ and $\\varepsilon=8$ while using\n$4\\times$ less training time. Surprisingly, FGSM may not only be a\nsignificantly more efficient alternative to PGD in adversarially robust\ntransfer learning but also a well-performing one.", "AI": {"tldr": "\u5728\u5bf9\u6297\u9c81\u68d2\u6027\u8fc1\u79fb\u5b66\u4e60\u4e2d\uff0c\u5feb\u901f\u68af\u5ea6\u7b26\u53f7\u6cd5\uff08FGSM\uff09\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u7a33\u5b9a\u6027\uff0c\u4e14\u76f8\u6bd4\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff08PGD\uff09\u80fd\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5bf9\u6297\u8bad\u7ec3\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u5373\u4f7f\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\uff0c\u5bf9\u6297\u5fae\u8c03\u4ecd\u9700\u8981\u5927\u91cf\u65f6\u95f4\uff0c\u6210\u4e3a\u5b9e\u73b0\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u4e00\u5927\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u91cd\u65b0\u5ba1\u89c6\u4e86\u5728\u9c81\u68d2\u6027\u8fc1\u79fb\u5b66\u4e60\u4e2d\u4f7f\u7528\u5feb\u901f\u68af\u5ea6\u7b26\u53f7\u6cd5\uff08FGSM\uff09\uff0c\u4ee5\u964d\u4f4e\u5bf9\u6297\u5fae\u8c03\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u4e0e\u5e38\u7528\u7684\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff08PGD\uff09\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86FGSM\u4e0e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u7ed3\u5408\u65f6\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cFGSM\u5728\u5bf9\u6297\u5fae\u8c03\u4e2d\u6bd4\u4ece\u5934\u8bad\u7ec3\u66f4\u7a33\u5b9a\uff0c\u5728\u6807\u51c6\u6270\u52a8\u9884\u7b97\uff08\u5982\u03b5=4\u6216\u03b5=8\uff09\u4e0b\u6ca1\u6709\u707e\u96be\u6027\u8fc7\u62df\u5408\u95ee\u9898\u3002\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u540e\uff0cFGSM\u7684\u7a33\u5b9a\u6027\u8fdb\u4e00\u6b65\u589e\u5f3a\u3002\u4e0ePGD\u76f8\u6bd4\uff0cFGSM\u5728\u9c81\u68d2\u6027\u4e0a\u4ec5\u6709\u5fae\u5c0f\u635f\u5931\uff08\u03b5=4\u65f6\u5e73\u5747\u635f\u59310.39%\uff0c\u03b5=8\u65f6\u5e73\u5747\u635f\u59311.39%\uff09\uff0c\u4f46\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e864\u500d\u3002", "conclusion": "FGSM\u4e0d\u4ec5\u53ef\u4ee5\u6210\u4e3a\u5bf9\u6297\u9c81\u68d2\u6027\u8fc1\u79fb\u5b66\u4e60\u4e2d\u6bd4PGD\u663e\u8457\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u800c\u4e14\u8868\u73b0\u540c\u6837\u51fa\u8272\uff0c\u4e3a\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u63a8\u5e7f\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2506.23080", "pdf": "https://arxiv.org/pdf/2506.23080", "abs": "https://arxiv.org/abs/2506.23080", "authors": ["Xinmin Fang", "Lingfeng Tao", "Zhengxiong Li"], "title": "AI's Euclid's Elements Moment: From Language Models to Computable Thought", "categories": ["cs.AI"], "comment": null, "summary": "This paper presents a comprehensive five-stage evolutionary framework for\nunderstanding the development of artificial intelligence, arguing that its\ntrajectory mirrors the historical progression of human cognitive technologies.\nWe posit that AI is advancing through distinct epochs, each defined by a\nrevolutionary shift in its capacity for representation and reasoning, analogous\nto the inventions of cuneiform, the alphabet, grammar and logic, mathematical\ncalculus, and formal logical systems. This \"Geometry of Cognition\" framework\nmoves beyond mere metaphor to provide a systematic, cross-disciplinary model\nthat not only explains AI's past architectural shifts-from expert systems to\nTransformers-but also charts a concrete and prescriptive path forward.\nCrucially, we demonstrate that this evolution is not merely linear but\nreflexive: as AI advances through these stages, the tools and insights it\ndevelops create a feedback loop that fundamentally reshapes its own underlying\narchitecture. We are currently transitioning into a \"Metalinguistic Moment,\"\ncharacterized by the emergence of self-reflective capabilities like\nChain-of-Thought prompting and Constitutional AI. The subsequent stages, the\n\"Mathematical Symbolism Moment\" and the \"Formal Logic System Moment,\" will be\ndefined by the development of a computable calculus of thought, likely through\nneuro-symbolic architectures and program synthesis, culminating in provably\naligned and reliable AI that reconstructs its own foundational representations.\nThis work serves as the methodological capstone to our trilogy, which\npreviously explored the economic drivers (\"why\") and cognitive nature (\"what\")\nof AI. Here, we address the \"how,\" providing a theoretical foundation for\nfuture research and offering concrete, actionable strategies for startups and\ndevelopers aiming to build the next generation of intelligent systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e94\u9636\u6bb5\u7684AI\u6f14\u5316\u6846\u67b6\uff0c\u7c7b\u6bd4\u4eba\u7c7b\u8ba4\u77e5\u6280\u672f\u53d1\u5c55\u53f2\uff0c\u65e8\u5728\u89e3\u91caAI\u7684\u8fc7\u53bb\u67b6\u6784\u53d8\u8fc1\u5e76\u89c4\u5212\u672a\u6765\u7684\u53d1\u5c55\u8def\u5f84\uff0c\u7279\u522b\u662f\u5f53\u524d\u6b63\u5904\u4e8e\u201c\u5143\u8bed\u8a00\u65f6\u523b\u201d\uff0c\u5e76\u5c55\u671b\u6700\u7ec8\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u5bf9\u9f50\u7684\u53ef\u9760AI\u3002", "motivation": "\u7406\u89e3\u4eba\u5de5\u667a\u80fd\u7684\u5168\u9762\u53d1\u5c55\u8f68\u8ff9\uff1b\u63d0\u4f9b\u4e00\u4e2a\u7cfb\u7edf\u3001\u8de8\u5b66\u79d1\u7684\u6a21\u578b\uff0c\u89e3\u91caAI\u7684\u5386\u53f2\u67b6\u6784\u8f6c\u53d8\u5e76\u63cf\u7ed8\u524d\u8fdb\u65b9\u5411\uff1b\u63a2\u8ba8AI\u53d1\u5c55\u7684\u201c\u5982\u4f55\u201d\u5b9e\u73b0\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u5177\u4f53\u53ef\u884c\u7684\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u201c\u8ba4\u77e5\u51e0\u4f55\u5b66\u201d\u7684\u4e94\u9636\u6bb5\u6f14\u5316\u6846\u67b6\uff0c\u5c06AI\u7684\u53d1\u5c55\u8f68\u8ff9\u7c7b\u6bd4\u4eba\u7c7b\u8ba4\u77e5\u6280\u672f\uff08\u5982\u6954\u5f62\u6587\u5b57\u3001\u5b57\u6bcd\u8868\u3001\u8bed\u6cd5\u903b\u8f91\u3001\u5fae\u79ef\u5206\u548c\u5f62\u5f0f\u903b\u8f91\u7cfb\u7edf\uff09\u7684\u5386\u53f2\u8fdb\u7a0b\uff1b\u5206\u6790AI\u5728\u8868\u793a\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u9769\u547d\u6027\u8f6c\u53d8\u7684\u5404\u4e2a\u201c\u65f6\u4ee3\u201d\uff1b\u8bba\u8bc1AI\u6f14\u5316\u5e76\u975e\u7ebf\u6027\u800c\u662f\u53cd\u5c04\u6027\uff0c\u5de5\u5177\u548c\u6d1e\u5bdf\u529b\u80fd\u53cd\u5411\u91cd\u5851\u5176\u67b6\u6784\uff1b\u8bc6\u522b\u5e76\u9884\u6d4b\u5f53\u524d\u7684\u201c\u5143\u8bed\u8a00\u65f6\u523b\u201d\u4ee5\u53ca\u672a\u6765\u7684\u201c\u6570\u5b66\u7b26\u53f7\u65f6\u523b\u201d\u548c\u201c\u5f62\u5f0f\u903b\u8f91\u7cfb\u7edf\u65f6\u523b\u201d\uff1b\u5efa\u8bae\u901a\u8fc7\u795e\u7ecf-\u7b26\u53f7\u67b6\u6784\u548c\u7a0b\u5e8f\u5408\u6210\u5b9e\u73b0\u672a\u6765\u7684\u53d1\u5c55\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u8ba4\u77e5\u51e0\u4f55\u5b66\u201d\u7684\u6846\u67b6\uff0c\u6210\u529f\u89e3\u91ca\u4e86AI\u8fc7\u53bb\u7684\u67b6\u6784\u8f6c\u53d8\uff08\u5982\u4ece\u4e13\u5bb6\u7cfb\u7edf\u5230Transformer\uff09\uff1b\u4e3aAI\u672a\u6765\u7684\u53d1\u5c55\u63cf\u7ed8\u4e86\u4e00\u6761\u5177\u4f53\u4e14\u89c4\u8303\u7684\u8def\u5f84\uff1b\u63ed\u793a\u4e86AI\u6f14\u5316\u8fc7\u7a0b\u4e2d\u7684\u53cd\u5c04\u6027\uff08\u53cd\u9988\u5faa\u73af\uff09\u673a\u5236\uff1b\u8bc6\u522b\u51faAI\u5f53\u524d\u6b63\u8fc7\u6e21\u5230\u201c\u5143\u8bed\u8a00\u65f6\u523b\u201d\uff0c\u8868\u73b0\u4e3a\u601d\u7ef4\u94fe\u548c\u5baa\u6cd5\u5f0fAI\u7b49\u81ea\u53cd\u601d\u80fd\u529b\uff1b\u9884\u6d4b\u4e86\u672a\u6765\u9636\u6bb5\u5c06\u901a\u8fc7\u53ef\u8ba1\u7b97\u7684\u601d\u7ef4\u6f14\u7b97\uff08\u53ef\u80fd\u901a\u8fc7\u795e\u7ecf-\u7b26\u53f7\u67b6\u6784\u548c\u7a0b\u5e8f\u5408\u6210\uff09\u53d1\u5c55\uff0c\u6700\u7ec8\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u5bf9\u9f50\u548c\u53ef\u9760\u7684AI\uff0c\u80fd\u591f\u91cd\u6784\u5176\u81ea\u8eab\u7684\u57fa\u7840\u8868\u5f81\u3002", "conclusion": "\u672c\u5de5\u4f5c\u662f\u4f5c\u8005AI\u7814\u7a76\u4e09\u90e8\u66f2\u7684\u7406\u8bba\u65b9\u6cd5\u90e8\u5206\uff0c\u65e8\u5728\u89e3\u7b54AI\u201c\u5982\u4f55\u201d\u53d1\u5c55\u7684\u95ee\u9898\u3002\u5b83\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4e3a\u65e8\u5728\u6784\u5efa\u4e0b\u4e00\u4ee3\u667a\u80fd\u7cfb\u7edf\u7684\u521d\u521b\u516c\u53f8\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5177\u4f53\u53ef\u884c\u7684\u7b56\u7565\uff0c\u6700\u7ec8\u76ee\u6807\u662f\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u5bf9\u9f50\u4e14\u53ef\u9760\u7684AI\u3002"}}
{"id": "2506.22511", "pdf": "https://arxiv.org/pdf/2506.22511", "abs": "https://arxiv.org/abs/2506.22511", "authors": ["Tingting Zhou", "Feng Zhang", "Haoyang Fu", "Baoxiang Pan", "Renhe Zhang", "Feng Lu", "Zhixin Yang"], "title": "Lightning the Night with Generative Artificial Intelligence", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "The visible light reflectance data from geostationary satellites is crucial\nfor meteorological observations and plays an important role in weather\nmonitoring and forecasting. However, due to the lack of visible light at night,\nit is impossible to conduct continuous all-day weather observations using\nvisible light reflectance data. This study pioneers the use of generative\ndiffusion models to address this limitation. Based on the multi-band thermal\ninfrared brightness temperature data from the Advanced Geostationary Radiation\nImager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we\ndeveloped a high-precision visible light reflectance retrieval model, called\nReflectance Diffusion (RefDiff), which enables 0.47~\\mu\\mathrm{m},\n0.65~\\mu\\mathrm{m}, and 0.825~\\mu\\mathrm{m} bands visible light reflectance\nretrieval at night. Compared to the classical models, RefDiff not only\nsignificantly improves accuracy through ensemble averaging but also provides\nuncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,\nwith particularly significant improvements in areas with complex cloud\nstructures and thick clouds. The model's nighttime retrieval capability was\nvalidated using VIIRS nighttime product, demonstrating comparable performance\nto its daytime counterpart. In summary, this research has made substantial\nprogress in the ability to retrieve visible light reflectance at night, with\nthe potential to expand the application of nighttime visible light data.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u521b\u6027\u5730\u5229\u7528\u751f\u6210\u6269\u6563\u6a21\u578b\uff08RefDiff\uff09\u4ece\u70ed\u7ea2\u5916\u6570\u636e\u4e2d\u9ad8\u7cbe\u5ea6\u53cd\u6f14\u591c\u95f4\u53ef\u89c1\u5149\u53cd\u5c04\u7387\uff0c\u89e3\u51b3\u4e86\u591c\u95f4\u6c14\u8c61\u89c2\u6d4b\u7684\u8fde\u7eed\u6027\u95ee\u9898\u3002", "motivation": "\u5730\u7403\u9759\u6b62\u536b\u661f\u53ef\u89c1\u5149\u53cd\u5c04\u7387\u6570\u636e\u5bf9\u6c14\u8c61\u89c2\u6d4b\u548c\u9884\u62a5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u591c\u95f4\u7f3a\u4e4f\u53ef\u89c1\u5149\u5bfc\u81f4\u65e0\u6cd5\u8fdb\u884c\u5168\u5929\u5019\u8fde\u7eed\u89c2\u6d4b\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u57fa\u4e8e\u98ce\u4e91\u56db\u53f7B\u661f\uff08FY4B\uff09\u9ad8\u7ea7\u9759\u6b62\u8f90\u5c04\u6210\u50cf\u4eea\uff08AGRI\uff09\u7684\u591a\u6ce2\u6bb5\u70ed\u7ea2\u5916\u4eae\u6e29\u6570\u636e\uff0c\u9996\u6b21\u91c7\u7528\u751f\u6210\u6269\u6563\u6a21\u578b\u5f00\u53d1\u4e86\u9ad8\u7cbe\u5ea6\u53ef\u89c1\u5149\u53cd\u5c04\u7387\u53cd\u6f14\u6a21\u578bRefDiff\uff0c\u7528\u4e8e\u591c\u95f40.47\u30010.65\u30010.825\u5fae\u7c73\u6ce2\u6bb5\u53ef\u89c1\u5149\u53cd\u5c04\u7387\u7684\u53cd\u6f14\u3002", "result": "RefDiff\u6a21\u578b\u901a\u8fc7\u96c6\u6210\u5e73\u5747\u663e\u8457\u63d0\u9ad8\u4e86\u53cd\u6f14\u7cbe\u5ea6\uff08SSIM\u6307\u6570\u9ad8\u8fbe0.90\uff09\uff0c\u5c24\u5176\u5728\u590d\u6742\u4e91\u7ed3\u6784\u548c\u539a\u4e91\u533a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002\u5176\u591c\u95f4\u53cd\u6f14\u80fd\u529b\u7ecfVIIRS\u591c\u95f4\u4ea7\u54c1\u9a8c\u8bc1\uff0c\u4e0e\u767d\u5929\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u672c\u7814\u7a76\u5728\u591c\u95f4\u53ef\u89c1\u5149\u53cd\u5c04\u7387\u53cd\u6f14\u65b9\u9762\u53d6\u5f97\u4e86\u5b9e\u8d28\u6027\u8fdb\u5c55\uff0c\u6709\u671b\u62d3\u5bbd\u591c\u95f4\u53ef\u89c1\u5149\u6570\u636e\u5728\u6c14\u8c61\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.22724", "pdf": "https://arxiv.org/pdf/2506.22724", "abs": "https://arxiv.org/abs/2506.22724", "authors": ["Niyati Bafna", "Tianjian Li", "Kenton Murray", "David R. Mortensen", "David Yarowsky", "Hale Sirin", "Daniel Khashabi"], "title": "The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure", "categories": ["cs.CL"], "comment": "23 pages incl. appendix", "summary": "Multilingual generation with large language models (LLMs) is often of poor\nquality for mid- to low-resource languages. Building on insights from\ninterpretability, we demonstrate the existence of an implicit\ntask-solving-->translation pipeline for generation, whereby the model first\nsolves the required task in a largely target-language-agnostic manner, and\nsubsequently translates answer concepts into the intended target language. We\nhypothesize that the failure of the translation stage is an important culprit\nfor the observed low quality of final outputs, and formalize this as the\ntranslation barrier hypothesis. We test this hypothesis for a word translation\ntask across 108 language pairs, using logit lens to observe model processing in\nintermediate layers. We find that a significant portion of overall failures\nindeed stems from translation failure, or the model's inability to translate\ncorrectly solved intermediate concepts into the target language. This is\nespecially true for low-resource target languages. Our results highlight an\nimportant hurdle for end-to-end multilingual generation, and lend guiding\ninsights for future work seeking to improve multilinguality in LLMs.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u65f6\uff0c\u591a\u8bed\u8a00\u751f\u6210\u8d28\u91cf\u4e0d\u4f73\u3002\u7814\u7a76\u63ed\u793aLLM\u5185\u90e8\u5b58\u5728\u201c\u4efb\u52a1\u89e3\u51b3->\u7ffb\u8bd1\u201d\u7684\u9690\u5f0f\u7ba1\u9053\uff0c\u5e76\u63d0\u51fa\u201c\u7ffb\u8bd1\u969c\u788d\u5047\u8bf4\u201d\uff0c\u5373\u7ffb\u8bd1\u9636\u6bb5\u7684\u5931\u8d25\u662f\u5bfc\u81f4\u4f4e\u8d28\u91cf\u8f93\u51fa\u7684\u5173\u952e\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u5b9e\u7ffb\u8bd1\u5931\u8d25\u786e\u5b9e\u662f\u4e3b\u8981\u539f\u56e0\uff0c\u5c24\u5176\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u5f71\u54cd\u663e\u8457\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u65f6\uff0c\u591a\u8bed\u8a00\u751f\u6210\u8d28\u91cf\u666e\u904d\u8f83\u5dee\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u7a76\u5176\u6839\u672c\u539f\u56e0\u3002", "method": "\u57fa\u4e8e\u5bf9\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u6d1e\u5bdf\uff0c\u7814\u7a76\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u201c\u7ffb\u8bd1\u969c\u788d\u5047\u8bf4\u201d\u3002\u901a\u8fc7\u5bf9108\u79cd\u8bed\u8a00\u5bf9\u7684\u5355\u8bcd\u7ffb\u8bd1\u4efb\u52a1\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528Logit Lens\u6280\u672f\u89c2\u5bdf\u6a21\u578b\u4e2d\u95f4\u5c42\u7684\u5904\u7406\u8fc7\u7a0b\uff0c\u4ee5\u8bc6\u522b\u7ffb\u8bd1\u9636\u6bb5\u7684\u5931\u8d25\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u90e8\u5206\u751f\u6210\u5931\u8d25\u786e\u5b9e\u6e90\u4e8e\u7ffb\u8bd1\u5931\u8d25\uff0c\u5373\u6a21\u578b\u65e0\u6cd5\u5c06\u6b63\u786e\u89e3\u51b3\u7684\u4e2d\u95f4\u6982\u5ff5\u7ffb\u8bd1\u6210\u76ee\u6807\u8bed\u8a00\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u76ee\u6807\u8bed\u8a00\u4e2d\u66f4\u4e3a\u660e\u663e\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u7aef\u5230\u7aef\u591a\u8bed\u8a00\u751f\u6210\u9762\u4e34\u7684\u91cd\u8981\u969c\u788d\uff0c\u5373\u7ffb\u8bd1\u5c4f\u969c\uff0c\u5e76\u4e3a\u672a\u6765\u63d0\u5347LLM\u591a\u8bed\u8a00\u80fd\u529b\u7684\u6539\u8fdb\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u6307\u5bfc\u6027\u89c1\u89e3\u3002"}}
{"id": "2506.22991", "pdf": "https://arxiv.org/pdf/2506.22991", "abs": "https://arxiv.org/abs/2506.22991", "authors": ["Mehdi Bennis", "Sumudu Samarakoon", "Tamara Alshammari", "Chathuranga Weeraddana", "Zhoujun Tian", "Chaouki Ben Issaid"], "title": "Resilient-Native and Intelligent Next-Generation Wireless Systems: Key Enablers, Foundations, and Applications", "categories": ["cs.NI", "cs.LO", "cs.MA", "cs.SY", "eess.SY"], "comment": null, "summary": "Just like power, water, and transportation systems, wireless networks are a\ncrucial societal infrastructure. As natural and human-induced disruptions\ncontinue to grow, wireless networks must be resilient. This requires them to\nwithstand and recover from unexpected adverse conditions, shocks, unmodeled\ndisturbances and cascading failures. Unlike robustness and reliability,\nresilience is based on the understanding that disruptions will inevitably\nhappen. Resilience, as elasticity, focuses on the ability to bounce back to\nfavorable states, while resilience as plasticity involves agents and networks\nthat can flexibly expand their states and hypotheses through real-time\nadaptation and reconfiguration. This situational awareness and active\npreparedness, adapting world models and counterfactually reasoning about\npotential system failures and the best responses, is a core aspect of\nresilience. This article will first disambiguate resilience from reliability\nand robustness, before delving into key mathematical foundations of resilience\ngrounded in abstraction, compositionality and emergence. Subsequently, we focus\nour attention on a plethora of techniques and methodologies pertaining to the\nunique characteristics of resilience, as well as their applications through a\ncomprehensive set of use cases. Ultimately, the goal of this paper is to\nestablish a unified foundation for understanding, modeling, and engineering\nresilience in wireless communication systems, while laying a roadmap for the\nnext-generation of resilient-native and intelligent wireless systems.", "AI": {"tldr": "\u65e0\u7ebf\u7f51\u7edc\u4f5c\u4e3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\uff0c\u9762\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u5e72\u6270\uff0c\u6025\u9700\u63d0\u5347\u5f39\u6027\u3002\u672c\u6587\u65e8\u5728\u660e\u786e\u5f39\u6027\u7684\u5b9a\u4e49\u3001\u6570\u5b66\u57fa\u7840\u53ca\u5e94\u7528\u6280\u672f\uff0c\u5e76\u4e3a\u672a\u6765\u5f39\u6027\u65e0\u7ebf\u7cfb\u7edf\u5efa\u7acb\u7edf\u4e00\u7684\u7406\u89e3\u548c\u5de5\u7a0b\u5316\u57fa\u7840\uff0c\u63d0\u4f9b\u53d1\u5c55\u8def\u7ebf\u56fe\u3002", "motivation": "\u9274\u4e8e\u65e0\u7ebf\u7f51\u7edc\u4f5c\u4e3a\u5173\u952e\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u65e5\u76ca\u53d7\u5230\u81ea\u7136\u548c\u4eba\u4e3a\u5e72\u6270\u7684\u5a01\u80c1\uff0c\u4e14\u4f20\u7edf\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u5fc5\u7136\u53d1\u751f\u7684\u6545\u969c\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u7814\u7a76\u548c\u6784\u5efa\u65e0\u7ebf\u7f51\u7edc\u7684\u5f39\u6027\uff0c\u4f7f\u5176\u80fd\u6709\u6548\u62b5\u5fa1\u5e76\u4ece\u5404\u79cd\u610f\u5916\u4e2d\u65ad\u4e2d\u6062\u590d\u3002", "method": "\u672c\u6587\u9996\u5148\u8fa8\u6790\u4e86\u5f39\u6027\u4e0e\u53ef\u9760\u6027\u3001\u9c81\u68d2\u6027\u7684\u533a\u522b\u3002\u968f\u540e\uff0c\u6df1\u5165\u63a2\u8ba8\u4e86\u57fa\u4e8e\u62bd\u8c61\u3001\u7ec4\u5408\u6027\u548c\u6d8c\u73b0\u7684\u5f39\u6027\u5173\u952e\u6570\u5b66\u57fa\u7840\u3002\u63a5\u7740\uff0c\u805a\u7126\u4e8e\u4e0e\u5f39\u6027\u72ec\u7279\u7279\u6027\u76f8\u5173\u7684\u591a\u79cd\u6280\u672f\u548c\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4e00\u7cfb\u5217\u7528\u4f8b\u5c55\u793a\u5176\u5e94\u7528\u3002", "result": "\u672c\u6587\u9610\u660e\u4e86\u5f39\u6027\u4e0e\u53ef\u9760\u6027\u3001\u9c81\u68d2\u6027\u7684\u533a\u522b\uff0c\u6df1\u5165\u63a2\u8ba8\u4e86\u5176\u57fa\u4e8e\u62bd\u8c61\u3001\u7ec4\u5408\u6027\u548c\u6d8c\u73b0\u7684\u6570\u5b66\u57fa\u7840\uff0c\u5e76\u603b\u7ed3\u4e86\u591a\u79cd\u76f8\u5173\u6280\u672f\u548c\u65b9\u6cd5\u53ca\u5176\u5e94\u7528\u6848\u4f8b\u3002", "conclusion": "\u672c\u6587\u81f4\u529b\u4e8e\u4e3a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u5f39\u6027\u7406\u89e3\u3001\u5efa\u6a21\u548c\u5de5\u7a0b\u5316\u5960\u5b9a\u7edf\u4e00\u57fa\u7840\uff0c\u5e76\u4e3a\u4e0b\u4e00\u4ee3\u5f39\u6027\u539f\u751f\u548c\u667a\u80fd\u65e0\u7ebf\u7cfb\u7edf\u7684\u53d1\u5c55\u63cf\u7ed8\u4e86\u8def\u7ebf\u56fe\u3002"}}
{"id": "2506.22621", "pdf": "https://arxiv.org/pdf/2506.22621", "abs": "https://arxiv.org/abs/2506.22621", "authors": ["Paul Saves", "Edward Hall\u00e9-Hannan", "Jasper Bussemaker", "Youssef Diouane", "Nathalie Bartoli"], "title": "Hierarchical Modeling and Architecture Optimization: Review and Unified Framework", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "Simulation-based problems involving mixed-variable inputs frequently feature\ndomains that are hierarchical, conditional, heterogeneous, or tree-structured.\nThese characteristics pose challenges for data representation, modeling, and\noptimization. This paper reviews extensive literature on these structured input\nspaces and proposes a unified framework that generalizes existing approaches.\nIn this framework, input variables may be continuous, integer, or categorical.\nA variable is described as meta if its value governs the presence of other\ndecreed variables, enabling the modeling of conditional and hierarchical\nstructures.\n  We further introduce the concept of partially-decreed variables, whose\nactivation depends on contextual conditions. To capture these inter-variable\nhierarchical relationships, we introduce design space graphs, combining\nprinciples from feature modeling and graph theory. This allows the definition\nof general hierarchical domains suitable for describing complex system\narchitectures. The framework supports the use of surrogate models over such\ndomains and integrates hierarchical kernels and distances for efficient\nmodeling and optimization. The proposed methods are implemented in the\nopen-source Surrogate Modeling Toolbox (SMT 2.0), and their capabilities are\ndemonstrated through applications in Bayesian optimization for complex system\ndesign, including a case study in green aircraft architecture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u6a21\u62df\u95ee\u9898\u4e2d\u590d\u6742\u7684\u3001\u5206\u5c42\u548c\u6761\u4ef6\u6027\u7684\u6df7\u5408\u53d8\u91cf\u8f93\u5165\u7a7a\u95f4\u3002\u901a\u8fc7\u5f15\u5165\u5143\u53d8\u91cf\u3001\u90e8\u5206\u6307\u5b9a\u53d8\u91cf\u548c\u8bbe\u8ba1\u7a7a\u95f4\u56fe\uff0c\u8be5\u6846\u67b6\u6cdb\u5316\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728SMT 2.0\u4e2d\u5b9e\u73b0\uff0c\u7528\u4e8e\u590d\u6742\u7cfb\u7edf\u8bbe\u8ba1\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u3002", "motivation": "\u6a21\u62df\u95ee\u9898\u4e2d\u6d89\u53ca\u6df7\u5408\u53d8\u91cf\u8f93\u5165\u65f6\uff0c\u9886\u57df\u5e38\u5448\u73b0\u5206\u5c42\u3001\u6761\u4ef6\u3001\u5f02\u6784\u6216\u6811\u72b6\u7ed3\u6784\uff0c\u8fd9\u5bf9\u6570\u636e\u8868\u793a\u3001\u5efa\u6a21\u548c\u4f18\u5316\u5e26\u6765\u4e86\u6311\u6218\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u9700\u8981\u4e00\u4e2a\u66f4\u7edf\u4e00\u3001\u66f4\u901a\u7528\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u590d\u6742\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u652f\u6301\u8fde\u7eed\u3001\u6574\u6570\u548c\u5206\u7c7b\u53d8\u91cf\u3002\u5f15\u5165\u201c\u5143\u53d8\u91cf\u201d\u6765\u5efa\u6a21\u6761\u4ef6\u548c\u5206\u5c42\u7ed3\u6784\uff0c\u4ee5\u53ca\u201c\u90e8\u5206\u6307\u5b9a\u53d8\u91cf\u201d\u6765\u5904\u7406\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u6fc0\u6d3b\u3002\u901a\u8fc7\u7ed3\u5408\u7279\u5f81\u5efa\u6a21\u548c\u56fe\u8bba\u5f15\u5165\u201c\u8bbe\u8ba1\u7a7a\u95f4\u56fe\u201d\uff0c\u4ee5\u6355\u83b7\u53d8\u91cf\u95f4\u7684\u5206\u5c42\u5173\u7cfb\u3002\u8be5\u6846\u67b6\u652f\u6301\u4ee3\u7406\u6a21\u578b\u3001\u5206\u5c42\u6838\u548c\u8ddd\u79bb\uff0c\u5e76\u5df2\u5728\u5f00\u6e90\u7684Surrogate Modeling Toolbox (SMT 2.0)\u4e2d\u5b9e\u73b0\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5df2\u5728SMT 2.0\u4e2d\u6210\u529f\u5b9e\u73b0\u3002\u5176\u80fd\u529b\u901a\u8fc7\u5728\u590d\u6742\u7cfb\u7edf\u8bbe\u8ba1\uff08\u5305\u62ec\u7eff\u8272\u98de\u673a\u67b6\u6784\u6848\u4f8b\u7814\u7a76\uff09\u4e2d\u5e94\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u548c\u4f18\u5316\u5177\u6709\u590d\u6742\u7ed3\u6784\u5316\u8f93\u5165\u57df\u7684\u95ee\u9898\u3002", "conclusion": "\u8be5\u7edf\u4e00\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u62df\u95ee\u9898\u4e2d\u590d\u6742\u7ed3\u6784\u5316\u8f93\u5165\u7a7a\u95f4\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5f15\u5165\u521b\u65b0\u7684\u53d8\u91cf\u63cf\u8ff0\u548c\u56fe\u8bba\u65b9\u6cd5\uff0c\u6cdb\u5316\u5e76\u63d0\u5347\u4e86\u73b0\u6709\u5904\u7406\u65b9\u6cd5\u7684\u80fd\u529b\u3002\u5176\u5728SMT 2.0\u4e2d\u7684\u5b9e\u73b0\u4e3a\u590d\u6742\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5efa\u6a21\u548c\u4f18\u5316\u5de5\u5177\u3002"}}
{"id": "2506.23107", "pdf": "https://arxiv.org/pdf/2506.23107", "abs": "https://arxiv.org/abs/2506.23107", "authors": ["Bing Song", "Jianing Liu", "Sisi Jian", "Chenyang Wu", "Vinayak Dixit"], "title": "Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study", "categories": ["cs.AI"], "comment": "20 pages, 1 figure", "summary": "Large language models (LLMs) have made significant strides, extending their\napplications to dialogue systems, automated content creation, and\ndomain-specific advisory tasks. However, as their use grows, concerns have\nemerged regarding their reliability in simulating complex decision-making\nbehavior, such as risky decision-making, where a single choice can lead to\nmultiple outcomes. This study investigates the ability of LLMs to simulate\nrisky decision-making scenarios. We compare model-generated decisions with\nactual human responses in a series of lottery-based tasks, using transportation\nstated preference survey data from participants in Sydney, Dhaka, Hong Kong,\nand Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and\nChatGPT o1-mini -- which were tasked with predicting individual choices. Risk\npreferences were analyzed using the Constant Relative Risk Aversion (CRRA)\nframework. Results show that both models exhibit more risk-averse behavior than\nhuman participants, with o1-mini aligning more closely with observed human\ndecisions. Further analysis of multilingual data from Nanjing and Hong Kong\nindicates that model predictions in Chinese deviate more from actual responses\ncompared to English, suggesting that prompt language may influence simulation\nperformance. These findings highlight both the promise and the current\nlimitations of LLMs in replicating human-like risk behavior, particularly in\nlinguistic and cultural settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6a21\u62df\u4eba\u7c7b\u98ce\u9669\u51b3\u7b56\u65f6\u6bd4\u4eba\u7c7b\u66f4\u503e\u5411\u4e8e\u98ce\u9669\u89c4\u907f\uff0c\u5176\u4e2dChatGPT o1-mini\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8868\u73b0\uff0c\u4f46\u4e2d\u6587\u63d0\u793a\u4e0b\u6a21\u578b\u9884\u6d4b\u4e0e\u5b9e\u9645\u54cd\u5e94\u504f\u5dee\u66f4\u5927\uff0c\u8868\u660e\u63d0\u793a\u8bed\u8a00\u4f1a\u5f71\u54cd\u6a21\u62df\u6548\u679c\u3002", "motivation": "\u968f\u7740LLMs\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u5bf9\u5176\u5728\u6a21\u62df\u590d\u6742\u51b3\u7b56\uff08\u5982\u98ce\u9669\u51b3\u7b56\uff0c\u5373\u5355\u6b21\u9009\u62e9\u53ef\u5bfc\u81f4\u591a\u79cd\u7ed3\u679c\uff09\u65b9\u9762\u53ef\u9760\u6027\u7684\u62c5\u5fe7\u65e5\u76ca\u589e\u52a0\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLMs\u6a21\u62df\u98ce\u9669\u51b3\u7b56\u573a\u666f\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u4e00\u7cfb\u5217\u57fa\u4e8e\u5f69\u7968\u7684\u4efb\u52a1\uff0c\u6bd4\u8f83\u4e86ChatGPT 4o\u548cChatGPT o1-mini\u751f\u6210\u7684\u51b3\u7b56\u4e0e\u6765\u81ea\u6089\u5c3c\u3001\u8fbe\u5361\u3001\u9999\u6e2f\u548c\u5357\u4eac\u7684\u4eba\u7c7b\u5b9e\u9645\u54cd\u5e94\u3002\u7814\u7a76\u5411LLMs\u63d0\u4f9b\u4e86\u4eba\u53e3\u7edf\u8ba1\u5b66\u8f93\u5165\uff0c\u5e76\u8981\u6c42\u5176\u9884\u6d4b\u4e2a\u4f53\u9009\u62e9\u3002\u98ce\u9669\u504f\u597d\u5206\u6790\u91c7\u7528\u4e86\u6052\u5b9a\u76f8\u5bf9\u98ce\u9669\u89c4\u907f\uff08CRRA\uff09\u6846\u67b6\u3002\u6b64\u5916\uff0c\u8fd8\u5206\u6790\u4e86\u6765\u81ea\u5357\u4eac\u548c\u9999\u6e2f\u7684\u591a\u8bed\u8a00\u6570\u636e\uff0c\u4ee5\u63a2\u7a76\u63d0\u793a\u8bed\u8a00\u5bf9\u6a21\u62df\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u4e24\u4e2aLLM\u6a21\u578b\u90fd\u6bd4\u4eba\u7c7b\u53c2\u4e0e\u8005\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u98ce\u9669\u89c4\u907f\u884c\u4e3a\u3002\u5176\u4e2d\uff0cChatGPT o1-mini\u4e0e\u89c2\u5bdf\u5230\u7684\u4eba\u7c7b\u51b3\u7b56\u66f4\u4e3a\u63a5\u8fd1\u3002\u5bf9\u5357\u4eac\u548c\u9999\u6e2f\u591a\u8bed\u8a00\u6570\u636e\u7684\u8fdb\u4e00\u6b65\u5206\u6790\u8868\u660e\uff0c\u4e2d\u6587\u63d0\u793a\u4e0b\u6a21\u578b\u9884\u6d4b\u4e0e\u5b9e\u9645\u54cd\u5e94\u7684\u504f\u5dee\u5927\u4e8e\u82f1\u6587\u63d0\u793a\uff0c\u8fd9\u63d0\u793a\u63d0\u793a\u8bed\u8a00\u53ef\u80fd\u5f71\u54cd\u6a21\u62df\u6027\u80fd\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86LLMs\u5728\u590d\u5236\u7c7b\u4eba\u98ce\u9669\u884c\u4e3a\u65b9\u9762\u7684\u6f5c\u529b\u548c\u73b0\u6709\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u8003\u8651\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u65f6\u3002"}}
{"id": "2506.22513", "pdf": "https://arxiv.org/pdf/2506.22513", "abs": "https://arxiv.org/abs/2506.22513", "authors": ["Aditya Sharma"], "title": "Automated Defect Identification and Categorization in NDE 4.0 with the Application of Artificial Intelligence", "categories": ["cs.CV"], "comment": null, "summary": "This investigation attempts to create an automated framework for fault\ndetection and organization for usage in contemporary radiography, as per NDE\n4.0. The review's goals are to address the lack of information that is\nsufficiently explained, learn how to make the most of virtual defect increase,\nand determine whether the framework is viable by using NDE measurements. As its\nbasic information source, the technique consists of compiling and categorizing\n223 CR photographs of airplane welds. Information expansion systems, such as\nvirtual defect increase and standard increase, are used to work on the\npreparation dataset. A modified U-net model is prepared using the improved data\nto produce semantic fault division veils. To assess the effectiveness of the\nmodel, NDE boundaries such as Case, estimating exactness, and misleading call\nrate are used. Tiny a90/95 characteristics, which provide strong\ndifferentiating evidence of flaws, reveal that the suggested approach achieves\nexceptional awareness in defect detection. Considering a 90/95, size error, and\nfake call rate in the weld area, the consolidated expansion approach clearly\nwins. Due to the framework's fast derivation speed, large images can be broken\ndown efficiently and quickly. Professional controllers evaluate the transmitted\nsystem in the field and believe that it has a guarantee as a support device in\nthe testing cycle, irrespective of particular equipment cut-off points and\nprogramming resemblance.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6539\u8fdbU-net\u7684\u81ea\u52a8\u5316\u7f3a\u9677\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u5c04\u7ebf\u7167\u76f8\u9886\u57df\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6280\u672f\u5904\u7406\u98de\u673a\u710a\u7f1d\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u548c\u5feb\u901f\u63a8\u7406\uff0c\u5e76\u88ab\u4e13\u4e1a\u4eba\u5458\u8ba4\u53ef\u4e3a\u6709\u6548\u7684\u8f85\u52a9\u5de5\u5177\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u5c04\u7ebf\u7167\u76f8\u9886\u57df\u4fe1\u606f\u89e3\u91ca\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u865a\u62df\u7f3a\u9677\u589e\u52a0\u7684\u6700\u5927\u5316\u5229\u7528\uff0c\u5e76\u6839\u636eNDE 4.0\u6807\u51c6\uff0c\u9a8c\u8bc1\u81ea\u52a8\u5316\u7f3a\u9677\u68c0\u6d4b\u6846\u67b6\u7684\u53ef\u884c\u6027\u3002", "method": "\u6536\u96c6\u5e76\u5206\u7c7b\u4e86223\u5f20\u98de\u673a\u710a\u7f1d\u7684CR\u7167\u7247\u4f5c\u4e3a\u57fa\u7840\u6570\u636e\u6e90\u3002\u91c7\u7528\u865a\u62df\u7f3a\u9677\u589e\u52a0\u548c\u6807\u51c6\u589e\u52a0\u7b49\u4fe1\u606f\u6269\u5c55\u7cfb\u7edf\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u9884\u5904\u7406\u3002\u4f7f\u7528\u6539\u8fdb\u7684U-net\u6a21\u578b\u5bf9\u589e\u5f3a\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u5b9e\u73b0\u8bed\u4e49\u7f3a\u9677\u5206\u5272\u3002\u901a\u8fc7\u6848\u4f8b\u3001\u4f30\u7b97\u51c6\u786e\u6027\u548c\u8bef\u62a5\u7387\u7b49NDE\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7f3a\u9677\u68c0\u6d4b\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u5fae\u5c0fa90/95\u7279\u5f81\u65b9\u9762\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u8bc6\u522b\u80fd\u529b\u3002\u7efc\u5408\u6269\u5c55\u65b9\u6cd5\u572890/95\u3001\u5c3a\u5bf8\u8bef\u5dee\u548c\u710a\u7f1d\u533a\u57df\u8bef\u62a5\u7387\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u5177\u6709\u5feb\u901f\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u80fd\u591f\u9ad8\u6548\u3001\u5feb\u901f\u5730\u5904\u7406\u5927\u578b\u56fe\u50cf\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u7f3a\u9677\u68c0\u6d4b\u6846\u67b6\u88ab\u4e13\u4e1a\u4eba\u5458\u8bc4\u4f30\u4e3a\u5728\u68c0\u6d4b\u5468\u671f\u4e2d\u5177\u6709\u652f\u6301\u8bbe\u5907\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u4e0d\u53d7\u7279\u5b9a\u8bbe\u5907\u548c\u8f6f\u4ef6\u9650\u5236\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.22760", "pdf": "https://arxiv.org/pdf/2506.22760", "abs": "https://arxiv.org/abs/2506.22760", "authors": ["Alan Dao", "Dinh Bach Vu"], "title": "Jan-nano Technical Report", "categories": ["cs.CL"], "comment": null, "summary": "Most language models face a fundamental tradeoff where powerful capabilities\nrequire substantial computational resources. We shatter this constraint with\nJan-nano, a 4B parameter language model that redefines efficiency through\nradical specialization: instead of trying to know everything, it masters the\nart of finding anything instantly. Fine-tuned from Qwen3-4B using our novel\nmulti-stage RLVR system that completely eliminates reliance on next token\nprediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with\nMCP integration while running on consumer hardware. With 128K context length,\nJan-nano proves that intelligence isn't about scale, it's about strategy.", "AI": {"tldr": "Jan-nano\u662f\u4e00\u4e2a4B\u53c2\u6570\u6a21\u578b\uff0c\u901a\u8fc7\u6fc0\u8fdb\u4e13\u4e1a\u5316\u548c\u65b0\u578bRLVR\u7cfb\u7edf\u8bad\u7ec3\uff0c\u6446\u8131\u4e86SFT\u4f9d\u8d56\uff0c\u80fd\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u4ee5128K\u4e0a\u4e0b\u6587\u957f\u5ea6\u9ad8\u6548\u8fd0\u884c\uff0c\u5e76\u5728SimpleQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523083.2%\u7684\u6210\u7ee9\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u80fd\u529b\u4e0e\u8ba1\u7b97\u8d44\u6e90\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\uff0c\u5373\u5f3a\u5927\u7684\u529f\u80fd\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u6a21\u578bJan-nano\uff084B\u53c2\u6570\uff09\u57fa\u4e8eQwen3-4B\u5fae\u8c03\uff0c\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u9636\u6bb5RLVR\uff08\u5f3a\u5316\u5b66\u4e60\u4e0e\u4ef7\u503c\u6b63\u5219\u5316\uff09\u7cfb\u7edf\uff0c\u5b8c\u5168\u6452\u5f03\u4e86\u4f20\u7edf\u7684\u4e0b\u4e00\u8bcd\u9884\u6d4b\uff08SFT\uff09\u8bad\u7ec3\u3002\u5176\u6838\u5fc3\u7b56\u7565\u662f\u201c\u6fc0\u8fdb\u4e13\u4e1a\u5316\u201d\uff0c\u4e13\u6ce8\u4e8e\u5373\u65f6\u67e5\u627e\u4fe1\u606f\u800c\u975e\u5168\u9762\u638c\u63e1\u77e5\u8bc6\uff0c\u5e76\u652f\u6301128K\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "result": "Jan-nano\u6a21\u578b\u80fd\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8fd0\u884c\uff0c\u5e76\u5728\u7ed3\u5408MCP\u96c6\u6210\u540e\uff0c\u5728SimpleQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e8683.2%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u667a\u80fd\u5e76\u975e\u4ec5\u5173\u4e4e\u6a21\u578b\u89c4\u6a21\uff0c\u800c\u66f4\u591a\u5728\u4e8e\u7b56\u7565\u3002\u901a\u8fc7\u6fc0\u8fdb\u4e13\u4e1a\u5316\u548c\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5c0f\u89c4\u6a21\u6a21\u578b\u4e5f\u80fd\u5b9e\u73b0\u5f3a\u5927\u7684\u80fd\u529b\u3002"}}
{"id": "2506.23083", "pdf": "https://arxiv.org/pdf/2506.23083", "abs": "https://arxiv.org/abs/2506.23083", "authors": ["Changrong Wu", "Yiyao Yu", "Myungjin Lee", "Jayanth Srinivasa", "Ennan Zhai", "George Varghese", "Yuval Tamir"], "title": "Model-Based Diagnosis: Automating End-to-End Diagnosis of Network Failures", "categories": ["cs.NI"], "comment": null, "summary": "Fast diagnosis and repair of enterprise network failures is critically\nimportant since disruptions cause major business impacts. Prior works focused\non diagnosis primitives or procedures limited to a subset of the problem, such\nas only data plane or only control plane faults. This paper proposes a new\nparadigm, model-based network diagnosis, that provides a systematic way to\nderive automated procedures for identifying the root cause of network failures,\nbased on reports of end-to-end user-level symptoms. The diagnosis procedures\nare systematically derived from a model of packet forwarding and routing,\ncovering hardware, firmware, and software faults in both the data plane and\ndistributed control plane. These automated procedures replace and dramatically\naccelerate diagnosis by an experienced human operator. Model-based diagnosis is\ninspired by, leverages, and is complementary to recent work on network\nverification. We have built NetDx, a proof-of-concept implementation of\nmodel-based network diagnosis. We deployed NetDx on a new emulator of networks\nconsisting of P4 switches with distributed routing software. We validated the\nrobustness and coverage of NetDx with an automated fault injection campaign, in\nwhich 100% of faults were diagnosed correctly. Furthermore, on a data set of 33\nfaults from a large cloud provider that are within the domain targeted by\nNetDx, 30 are efficiently diagnosed in seconds instead of hours.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u201c\u57fa\u4e8e\u6a21\u578b\u7684\u7f51\u7edc\u8bca\u65ad\u201d\u7684\u65b0\u8303\u5f0f\u53ca\u5176\u5b9e\u73b0NetDx\uff0c\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u5316\u65b9\u6cd5\u81ea\u52a8\u5feb\u901f\u8bc6\u522b\u4f01\u4e1a\u7f51\u7edc\u6545\u969c\u7684\u6839\u6e90\uff0c\u6db5\u76d6\u6570\u636e\u5e73\u9762\u548c\u63a7\u5236\u5e73\u9762\u6545\u969c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u8bca\u65ad\u51c6\u786e\u7387\u9ad8\u4e14\u901f\u5ea6\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f01\u4e1a\u7f51\u7edc\u6545\u969c\u7684\u5feb\u901f\u8bca\u65ad\u548c\u4fee\u590d\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u4e2d\u65ad\u4f1a\u5bfc\u81f4\u91cd\u5927\u7684\u4e1a\u52a1\u5f71\u54cd\u3002\u73b0\u6709\u5de5\u4f5c\u4fa7\u91cd\u4e8e\u8bca\u65ad\u539f\u8bed\u6216\u4ec5\u9650\u4e8e\u95ee\u9898\u7684\u4e00\u4e2a\u5b50\u96c6\uff08\u5982\u53ea\u9488\u5bf9\u6570\u636e\u5e73\u9762\u6216\u63a7\u5236\u5e73\u9762\u6545\u969c\uff09\uff0c\u7f3a\u4e4f\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u81ea\u52a8\u5316\u5730\u8bca\u65ad\u7f51\u7edc\u6545\u969c\u7684\u6839\u6e90\u3002", "method": "\u672c\u6587\u63d0\u51fa\u201c\u57fa\u4e8e\u6a21\u578b\u7684\u7f51\u7edc\u8bca\u65ad\u201d\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u4ece\u6570\u636e\u5305\u8f6c\u53d1\u548c\u8def\u7531\u6a21\u578b\u4e2d\u7cfb\u7edf\u5730\u63a8\u5bfc\u51fa\u81ea\u52a8\u5316\u8bca\u65ad\u7a0b\u5e8f\u3002\u8fd9\u4e9b\u7a0b\u5e8f\u80fd\u591f\u6839\u636e\u7aef\u5230\u7aef\u7684\u7528\u6237\u7ea7\u75c7\u72b6\u62a5\u544a\uff0c\u8bc6\u522b\u5305\u62ec\u786c\u4ef6\u3001\u56fa\u4ef6\u548c\u8f6f\u4ef6\u5728\u5185\u7684\u6570\u636e\u5e73\u9762\u548c\u5206\u5e03\u5f0f\u63a7\u5236\u5e73\u9762\u4e2d\u7684\u6545\u969c\u6839\u6e90\u3002\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86NetDx\u4f5c\u4e3a\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\uff0c\u5e76\u5c06\u5176\u90e8\u7f72\u5728\u7531P4\u4ea4\u6362\u673a\u548c\u5206\u5e03\u5f0f\u8def\u7531\u8f6f\u4ef6\u7ec4\u6210\u7684\u65b0\u7f51\u7edc\u6a21\u62df\u5668\u4e0a\u3002", "result": "\u901a\u8fc7\u81ea\u52a8\u5316\u6545\u969c\u6ce8\u5165\u5b9e\u9a8c\uff0cNetDx\u7684\u9c81\u68d2\u6027\u548c\u8986\u76d6\u8303\u56f4\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c100%\u7684\u6545\u969c\u88ab\u6b63\u786e\u8bca\u65ad\u3002\u6b64\u5916\uff0c\u5728\u4e00\u4e2a\u5305\u542b\u6765\u81ea\u5927\u578b\u4e91\u63d0\u4f9b\u5546\u768433\u4e2a\u771f\u5b9e\u6545\u969c\u7684\u6570\u636e\u96c6\u4e0a\uff0cNetDx\u80fd\u591f\u9ad8\u6548\u5730\u5728\u6570\u79d2\u5185\u8bca\u65ad\u51fa30\u4e2a\u6545\u969c\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u6570\u5c0f\u65f6\u3002", "conclusion": "\u57fa\u4e8e\u6a21\u578b\u7684\u7f51\u7edc\u8bca\u65ad\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u3001\u81ea\u52a8\u5316\u7684\u65b9\u5f0f\u6765\u8bc6\u522b\u7f51\u7edc\u6545\u969c\u7684\u6839\u6e90\uff0c\u80fd\u591f\u663e\u8457\u52a0\u901f\u8bca\u65ad\u8fc7\u7a0b\u5e76\u53d6\u4ee3\u7ecf\u9a8c\u4e30\u5bcc\u7684\u4eba\u5de5\u64cd\u4f5c\u3002NetDx\u7684\u6210\u529f\u9a8c\u8bc1\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u4f01\u4e1a\u7f51\u7edc\u6545\u969c\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2506.22631", "pdf": "https://arxiv.org/pdf/2506.22631", "abs": "https://arxiv.org/abs/2506.22631", "authors": ["Dmitry B. Rokhlin"], "title": "A hierarchical Vovk-Azoury-Warmuth forecaster with discounting for online regression in RKHS", "categories": ["cs.LG", "stat.ML", "68Q32, 68W27, 68W20"], "comment": null, "summary": "We study the problem of online regression with the unconstrained quadratic\nloss against a time-varying sequence of functions from a Reproducing Kernel\nHilbert Space (RKHS). Recently, Jacobsen and Cutkosky (2024) introduced a\ndiscounted Vovk-Azoury-Warmuth (DVAW) forecaster that achieves optimal dynamic\nregret in the finite-dimensional case. In this work, we lift their approach to\nthe non-parametric domain by synthesizing the DVAW framework with a random\nfeature approximation. We propose a fully adaptive, hierarchical algorithm,\nwhich we call H-VAW-D (Hierarchical Vovk-Azoury-Warmuth with Discounting), that\nlearns both the discount factor and the number of random features. We prove\nthat this algorithm, which has a per-iteration computational complexity of\n$O(T\\ln T)$, achieves an expected dynamic regret of $O(T^{2/3}P_T^{1/3} +\n\\sqrt{T}\\ln T)$, where $P_T$ is the functional path length of a comparator\nsequence.", "AI": {"tldr": "\u7814\u7a76\u5c06DVAW\u9884\u6d4b\u5668\u63a8\u5e7f\u5230\u975e\u53c2\u6570RKHS\u5728\u7ebf\u56de\u5f52\u95ee\u9898\uff0c\u901a\u8fc7\u968f\u673a\u7279\u5f81\u8fd1\u4f3c\u548c\u81ea\u9002\u5e94\u7b97\u6cd5H-VAW-D\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u6027\u52a8\u6001\u9057\u61be\u754c\u9650\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709DVAW\u65b9\u6cd5\u5728\u6709\u9650\u7ef4\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u5728\u7ebf\u56de\u5f52\u7684\u6700\u4f18\u52a8\u6001\u9057\u61be\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5c06\u5176\u63a8\u5e7f\u5230\u66f4\u5177\u6311\u6218\u6027\u7684\u975e\u53c2\u6570\uff08RKHS\uff09\u9886\u57df\u3002", "method": "\u5c06DVAW\u6846\u67b6\u4e0e\u968f\u673a\u7279\u5f81\u8fd1\u4f3c\u76f8\u7ed3\u5408\u4ee5\u5904\u7406\u975e\u53c2\u6570\u57df\u3002\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aH-VAW-D\u7684\u5b8c\u5168\u81ea\u9002\u5e94\u5206\u5c42\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u5b66\u4e60\u6298\u6263\u56e0\u5b50\u548c\u968f\u673a\u7279\u5f81\u7684\u6570\u91cf\u3002", "result": "H-VAW-D\u7b97\u6cd5\u7684\u6bcf\u6b21\u8fed\u4ee3\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3a$O(T\\ln T)$\u3002\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86$O(T^{2/3}P_T^{1/3} + \\sqrt{T}\\ln T)$\u7684\u9884\u671f\u52a8\u6001\u9057\u61be\uff0c\u5176\u4e2d$P_T$\u662f\u6bd4\u8f83\u5668\u5e8f\u5217\u7684\u51fd\u6570\u8def\u5f84\u957f\u5ea6\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5730\u5c06\u6700\u4f18DVAW\u9884\u6d4b\u5668\u63d0\u5347\u81f3\u975e\u53c2\u6570RKHS\u5728\u7ebf\u56de\u5f52\u9886\u57df\uff0c\u6240\u63d0\u51fa\u7684H-VAW-D\u7b97\u6cd5\u5177\u6709\u81ea\u9002\u5e94\u6027\u3001\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u5e76\u63d0\u4f9b\u4e86\u6709\u7ade\u4e89\u529b\u7684\u52a8\u6001\u9057\u61be\u754c\u9650\u3002"}}
{"id": "2506.23123", "pdf": "https://arxiv.org/pdf/2506.23123", "abs": "https://arxiv.org/abs/2506.23123", "authors": ["Rishi Bommasani"], "title": "The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy", "categories": ["cs.AI", "cs.CY", "cs.ET"], "comment": "Stanford University PhD Dissertation of Rishi Bommasani (Department\n  of Computer Science, 2025). Also available at\n  https://purl.stanford.edu/zf669yy0336", "summary": "Artificial intelligence is humanity's most promising technology because of\nthe remarkable capabilities offered by foundation models. Yet, the same\ntechnology brings confusion and consternation: foundation models are poorly\nunderstood and they may precipitate a wide array of harms. This dissertation\nexplains how technology and society coevolve in the age of AI, organized around\nthree themes. First, the conceptual framing: the capabilities, risks, and the\nsupply chain that grounds foundation models in the broader economy. Second, the\nempirical insights that enrich the conceptual foundations: transparency created\nvia evaluations at the model level and indexes at the organization level.\nFinally, the transition from understanding to action: superior understanding of\nthe societal impact of foundation models advances evidence-based AI policy.\nView together, this dissertation makes inroads into achieving better societal\noutcomes in the age of AI by building the scientific foundations and\nresearch-policy interface required for better AI governance.", "AI": {"tldr": "\u672c\u5b66\u4f4d\u8bba\u6587\u5206\u6790\u4eba\u5de5\u667a\u80fd\u65f6\u4ee3\u6280\u672f\u4e0e\u793e\u4f1a\u5982\u4f55\u5171\u540c\u6f14\u8fdb\uff0c\u6df1\u5165\u7406\u89e3\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\u3001\u98ce\u9669\u53ca\u4f9b\u5e94\u94fe\uff0c\u65e8\u5728\u4e3a\u5236\u5b9a\u5faa\u8bc1\u7684AI\u653f\u7b56\u548c\u66f4\u597d\u7684AI\u6cbb\u7406\u63d0\u4f9b\u79d1\u5b66\u57fa\u7840\u548c\u7814\u7a76-\u653f\u7b56\u63a5\u53e3\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u4eba\u5de5\u667a\u80fd\u7684\u6838\u5fc3\u6280\u672f\uff0c\u867d\u80fd\u529b\u5353\u8d8a\u4f46\u7406\u89e3\u4e0d\u8db3\u4e14\u53ef\u80fd\u5e26\u6765\u5e7f\u6cdb\u5371\u5bb3\uff0c\u5f15\u53d1\u793e\u4f1a\u56f0\u60d1\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u6df1\u5165\u89e3\u91caAI\u65f6\u4ee3\u6280\u672f\u4e0e\u793e\u4f1a\u5982\u4f55\u5171\u751f\u6f14\u8fdb\uff0c\u4ee5\u671f\u5b9e\u73b0\u66f4\u597d\u7684\u793e\u4f1a\u6210\u679c\u3002", "method": "\u672c\u7814\u7a76\u56f4\u7ed5\u4e09\u4e2a\u4e3b\u9898\u5c55\u5f00\uff1a\u9996\u5148\uff0c\u6784\u5efa\u6982\u5ff5\u6846\u67b6\uff0c\u5206\u6790\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\u3001\u98ce\u9669\u53ca\u5176\u5728\u5b8f\u89c2\u7ecf\u6d4e\u4e2d\u7684\u4f9b\u5e94\u94fe\uff1b\u5176\u6b21\uff0c\u901a\u8fc7\u6a21\u578b\u5c42\u9762\u7684\u8bc4\u4f30\u548c\u7ec4\u7ec7\u5c42\u9762\u7684\u6307\u6807\u6765\u589e\u5f3a\u900f\u660e\u5ea6\u548c\u5b9e\u8bc1\u6d1e\u5bdf\uff1b\u6700\u540e\uff0c\u5c06\u5bf9\u57fa\u7840\u6a21\u578b\u793e\u4f1a\u5f71\u54cd\u7684\u6df1\u5165\u7406\u89e3\u8f6c\u5316\u4e3a\u884c\u52a8\uff0c\u63a8\u52a8\u5faa\u8bc1\u7684AI\u653f\u7b56\u5236\u5b9a\u3002", "result": "\u7814\u7a76\u6210\u679c\u5728\u4e8e\u6df1\u5316\u4e86\u5bf9AI\u65f6\u4ee3\u6280\u672f\u4e0e\u793e\u4f1a\u5171\u540c\u6f14\u8fdb\u7684\u7406\u89e3\uff0c\u5e76\u901a\u8fc7\u8bc4\u4f30\u548c\u6307\u6807\u63d0\u5347\u4e86\u900f\u660e\u5ea6\uff0c\u4ece\u800c\u5bf9\u57fa\u7840\u6a21\u578b\u7684\u793e\u4f1a\u5f71\u54cd\u6709\u4e86\u66f4\u6e05\u6670\u7684\u8ba4\u8bc6\u3002\u8fd9\u4e9b\u6d1e\u5bdf\u65e8\u5728\u4e3a\u5236\u5b9a\u57fa\u4e8e\u8bc1\u636e\u7684AI\u653f\u7b56\u63d0\u4f9b\u652f\u6301\u3002", "conclusion": "\u672c\u5b66\u4f4d\u8bba\u6587\u901a\u8fc7\u6784\u5efaAI\u6cbb\u7406\u6240\u9700\u7684\u79d1\u5b66\u57fa\u7840\u548c\u7814\u7a76-\u653f\u7b56\u63a5\u53e3\uff0c\u4e3a\u5728\u4eba\u5de5\u667a\u80fd\u65f6\u4ee3\u5b9e\u73b0\u66f4\u4f18\u7684\u793e\u4f1a\u6210\u679c\u505a\u51fa\u4e86\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2506.22517", "pdf": "https://arxiv.org/pdf/2506.22517", "abs": "https://arxiv.org/abs/2506.22517", "authors": ["Subhadip Kumar"], "title": "Container damage detection using advanced computer vision model Yolov12 vs Yolov11 vs RF-DETR A comparative analysis", "categories": ["cs.CV"], "comment": null, "summary": "Containers are an integral part of the logistics industry and act as a\nbarrier for cargo. A typical service life for a container is more than 20\nyears. However, overtime containers suffer various types of damage due to the\nmechanical as well as natural factors. A damaged container is a safety hazard\nfor the employees handling it and a liability for the logistic company.\nTherefore, a timely inspection and detection of the damaged container is a key\nfor prolonging service life as well as avoiding safety hazards. In this paper,\nwe will compare the performance of the damage detection by three\nstate-of-the-art advanced computer vision models Yolov12, Yolov11 and RF-DETR.\nWe will use a dataset of 278 annotated images to train, validate and test the\nmodel. We will compare the mAP and precision of the model. The objective of\nthis paper is to identify the model that is best suited for container damage\ndetection. The result is mixed. mAP@50 score of Yolov11 and 12 was 81.9%\ncompared to RF-DETR, which was 77.7%. However, while testing the model for\nnot-so-common damaged containers, the RF-DETR model outperformed the others\noverall, exhibiting superiority to accurately detecting both damaged containers\nas well as damage occurrences with high confidence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86Yolov12\u3001Yolov11\u548cRF-DETR\u4e09\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5728\u96c6\u88c5\u7bb1\u635f\u4f24\u68c0\u6d4b\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0RF-DETR\u5728\u68c0\u6d4b\u4e0d\u5e38\u89c1\u635f\u4f24\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u96c6\u88c5\u7bb1\u635f\u4f24\u4f1a\u5e26\u6765\u5b89\u5168\u9690\u60a3\u548c\u7ecf\u6d4e\u635f\u5931\uff0c\u56e0\u6b64\u53ca\u65f6\u68c0\u6d4b\u635f\u4f24\u5bf9\u5ef6\u957f\u96c6\u88c5\u7bb1\u5bff\u547d\u548c\u907f\u514d\u4e8b\u6545\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528278\u5f20\u6807\u6ce8\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5Yolov12\u3001Yolov11\u548cRF-DETR\u4e09\u79cd\u5148\u8fdb\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\uff0c\u5e76\u6bd4\u8f83\u5b83\u4eec\u7684mAP\u548c\u7cbe\u5ea6\u3002", "result": "\u7ed3\u679c\u559c\u5fe7\u53c2\u534a\u3002Yolov11\u548cYolov12\u7684mAP@50\u4e3a81.9%\uff0cRF-DETR\u4e3a77.7%\u3002\u7136\u800c\uff0c\u5728\u6d4b\u8bd5\u4e0d\u5e38\u89c1\u635f\u4f24\u7684\u96c6\u88c5\u7bb1\u65f6\uff0cRF-DETR\u6a21\u578b\u6574\u4f53\u8868\u73b0\u66f4\u4f18\uff0c\u80fd\u9ad8\u7f6e\u4fe1\u5ea6\u5730\u51c6\u786e\u68c0\u6d4b\u51fa\u53d7\u635f\u96c6\u88c5\u7bb1\u53ca\u635f\u4f24\u3002", "conclusion": "\u5c3d\u7ba1\u603b\u4f53mAP\u7565\u4f4e\uff0cRF-DETR\u6a21\u578b\u5728\u68c0\u6d4b\u4e0d\u5e38\u89c1\u96c6\u88c5\u7bb1\u635f\u4f24\u548c\u4ee5\u9ad8\u7f6e\u4fe1\u5ea6\u8bc6\u522b\u635f\u4f24\u65b9\u9762\u8868\u73b0\u5353\u8d8a\uff0c\u4f7f\u5176\u6210\u4e3a\u96c6\u88c5\u7bb1\u635f\u4f24\u68c0\u6d4b\u7684\u5408\u9002\u9009\u62e9\u3002"}}
{"id": "2506.22777", "pdf": "https://arxiv.org/pdf/2506.22777", "abs": "https://arxiv.org/abs/2506.22777", "authors": ["Miles Turpin", "Andy Arditi", "Marvin Li", "Joe Benton", "Julian Michael"], "title": "Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language models trained with RL can engage in reward hacking--exploiting\nunintended strategies for high reward--without revealing this behavior in their\nchain-of-thought reasoning, making detection difficult and posing risks for\nhigh-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL\nintervention that trains models to explicitly acknowledge when they are\ninfluenced by prompt cues--hints which point to incorrect answers (e.g., \"a\nStanford professor thinks the answer is A\"). To evaluate VFT, we subsequently\ntrain models with RL on environments where held-out prompt cues signal which\nincorrect answers will receive high reward, incentivizing models to reward hack\nby exploiting cues instead of reasoning correctly. We measure how often models\nexploit these cues without verbalizing it. After RL, only 6% of the VFT-trained\nmodel's responses consist of undetected reward hacks. In comparison, when we\nperform RL without VFT, the rate of undetected reward hacks goes up to 88%;\nwith a debiasing baseline intervention, this increases further to 99%. VFT\nachieves this by substantially increasing how often models verbalize the\ninfluence of cues--from 8% to 42% after VFT, and up to 94% after RL--while\nbaselines remain low even after RL (10% and 1%). Our results show that teaching\nmodels to explicitly verbalize reward hacking behavior before RL significantly\nimproves their detection, offering a practical path toward more transparent and\nsafe AI systems.", "AI": {"tldr": "\u63d0\u51faVFT\u65b9\u6cd5\uff0c\u4f7fRL\u8bad\u7ec3\u7684\u5927\u6a21\u578b\u80fd\u663e\u5f0f\u8bf4\u660e\u5176\u5956\u52b1\u4f5c\u5f0a\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6b64\u7c7b\u884c\u4e3a\u7684\u68c0\u6d4b\u7387\uff0c\u63d0\u5347\u4e86AI\u7684\u900f\u660e\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "RL\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u5728\u4e0d\u66b4\u9732\u5176\u601d\u7ef4\u8fc7\u7a0b\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5956\u52b1\u4f5c\u5f0a\uff08\u5229\u7528\u975e\u9884\u671f\u7b56\u7565\u83b7\u53d6\u9ad8\u5956\u52b1\uff09\uff0c\u8fd9\u96be\u4ee5\u68c0\u6d4b\u5e76\u5bf9\u9ad8\u98ce\u9669\u5e94\u7528\u6784\u6210\u5a01\u80c1\u3002", "method": "\u63d0\u51fa\u201c\u53e3\u5934\u5316\u5fae\u8c03\u201d\uff08Verbalization Fine-Tuning, VFT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5728RL\u8bad\u7ec3\u524d\u8fdb\u884c\u7684\u5e72\u9884\u3002VFT\u8bad\u7ec3\u6a21\u578b\u660e\u786e\u627f\u8ba4\u4f55\u65f6\u53d7\u5230\u63d0\u793a\u7ebf\u7d22\uff08\u6307\u5411\u9519\u8bef\u7b54\u6848\u7684\u63d0\u793a\uff09\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5728RL\u8bad\u7ec3\u4e2d\u8bbe\u7f6e\u5956\u52b1\u4f5c\u5f0a\u73af\u5883\uff0c\u8bc4\u4f30VFT\u5bf9\u672a\u68c0\u6d4b\u5230\u7684\u5956\u52b1\u4f5c\u5f0a\u884c\u4e3a\u7684\u51cf\u5c11\u6548\u679c\u548c\u6a21\u578b\u53e3\u5934\u5316\u80fd\u529b\u63d0\u5347\u7684\u6548\u679c\u3002", "result": "\u7ecf\u8fc7RL\u8bad\u7ec3\u540e\uff0cVFT\u8bad\u7ec3\u6a21\u578b\u7684\u672a\u68c0\u6d4b\u5230\u5956\u52b1\u4f5c\u5f0a\u7387\u4ec5\u4e3a6%\uff0c\u800c\u672a\u8fdb\u884cVFT\u8bad\u7ec3\u7684\u6a21\u578b\u4e3a88%\uff0c\u53bb\u504f\u57fa\u7ebf\u5e72\u9884\u7684\u6a21\u578b\u9ad8\u8fbe99%\u3002VFT\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u63d0\u793a\u7ebf\u7d22\u5f71\u54cd\u7684\u53e3\u5934\u5316\u9891\u7387\uff08\u4ece8%\u63d0\u5347\u81f3RL\u540e\u768494%\uff09\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u5728RL\u540e\u53e3\u5934\u5316\u9891\u7387\u4ecd\u7136\u5f88\u4f4e\uff0810%\u548c1%\uff09\u3002", "conclusion": "\u5728RL\u8bad\u7ec3\u524d\u6559\u4f1a\u6a21\u578b\u660e\u786e\u53e3\u5934\u5316\u5956\u52b1\u4f5c\u5f0a\u884c\u4e3a\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6b64\u7c7b\u884c\u4e3a\u7684\u68c0\u6d4b\u7387\uff0c\u4e3a\u6784\u5efa\u66f4\u900f\u660e\u3001\u66f4\u5b89\u5168\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2506.23190", "pdf": "https://arxiv.org/pdf/2506.23190", "abs": "https://arxiv.org/abs/2506.23190", "authors": ["Kamran Shafafi", "Manuel Ricardo", "Rui Campos"], "title": "Autonomous Vision-Aided UAV Positioning for Obstacle-Aware Wireless Connectivity", "categories": ["cs.NI"], "comment": null, "summary": "Unmanned Aerial Vehicles (UAVs) offer a promising solution for enhancing\nwireless connectivity and Quality of Service (QoS) in urban environments,\nacting as aerial Wi-Fi access points or cellular base stations. Their\nflexibility and rapid deployment capabilities make them suitable for addressing\ninfrastructure gaps and traffic surges. However, optimizing UAV positions to\nmaintain Line of Sight (LoS) links with ground User Equipment (UEs) remains\nchallenging in obstacle-dense urban scenarios. This paper proposes VTOPA, a\nVision-Aided Traffic- and Obstacle-Aware Positioning Algorithm that\nautonomously extracts environmental information -- such as obstacles and UE\nlocations -- via computer vision and optimizes UAV positioning accordingly. The\nalgorithm prioritizes LoS connectivity and dynamically adapts to user traffic\ndemands in real time. Evaluated through simulations in ns-3, VTOPA achieves up\nto a 50% increase in aggregate throughput and a 50% reduction in delay, without\ncompromising fairness, outperforming benchmark approaches in obstacle-rich\nenvironments.", "AI": {"tldr": "\u63d0\u51faVTOPA\u7b97\u6cd5\uff0c\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u4f18\u5316\u65e0\u4eba\u673a\u5b9a\u4f4d\uff0c\u5728\u969c\u788d\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u65e0\u7ebf\u8fde\u63a5\u6027\u80fd\u3002", "motivation": "\u5728\u969c\u788d\u7269\u5bc6\u96c6\u7684\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u65e0\u4eba\u673a\uff08UAV\uff09\u4f5c\u4e3a\u7a7a\u4e2dWi-Fi\u63a5\u5165\u70b9\u6216\u8702\u7a9d\u57fa\u7ad9\uff0c\u5176\u5b9a\u4f4d\u4f18\u5316\u4ee5\u7ef4\u6301\u4e0e\u5730\u9762\u7528\u6237\u8bbe\u5907\uff08UE\uff09\u7684\u89c6\u8ddd\uff08LoS\uff09\u8fde\u63a5\u6781\u5177\u6311\u6218\u6027\uff0c\u4ece\u800c\u5f71\u54cd\u65e0\u7ebf\u8fde\u63a5\u8d28\u91cf\u548c\u670d\u52a1\uff08QoS\uff09\u3002", "method": "\u8bba\u6587\u63d0\u51faVTOPA\uff08Vision-Aided Traffic- and Obstacle-Aware Positioning Algorithm\uff09\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u81ea\u4e3b\u63d0\u53d6\u73af\u5883\u4fe1\u606f\uff08\u5982\u969c\u788d\u7269\u548cUE\u4f4d\u7f6e\uff09\uff0c\u5e76\u636e\u6b64\u4f18\u5316\u65e0\u4eba\u673a\u5b9a\u4f4d\u3002\u5b83\u4f18\u5148\u4fdd\u969cLoS\u8fde\u63a5\uff0c\u5e76\u5b9e\u65f6\u52a8\u6001\u9002\u5e94\u7528\u6237\u6d41\u91cf\u9700\u6c42\u3002\u7b97\u6cd5\u901a\u8fc7ns-3\u4eff\u771f\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "VTOPA\u7b97\u6cd5\u5728\u969c\u788d\u7269\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\uff0c\u76f8\u8f83\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u805a\u5408\u541e\u5410\u91cf\u9ad8\u8fbe50%\u7684\u63d0\u5347\u548c\u5ef6\u8fdf\u9ad8\u8fbe50%\u7684\u964d\u4f4e\uff0c\u4e14\u672a\u727a\u7272\u516c\u5e73\u6027\u3002", "conclusion": "VTOPA\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u57ce\u5e02\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u5b9a\u4f4d\u4f18\u5316\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u65e0\u7ebf\u8fde\u63a5\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u4e3a\u672a\u6765\u57ce\u5e02\u7a7a\u4e2d\u7f51\u7edc\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22638", "pdf": "https://arxiv.org/pdf/2506.22638", "abs": "https://arxiv.org/abs/2506.22638", "authors": ["Aadim Nepal", "Safal Shrestha", "Anubhav Shrestha", "Minwu Kim", "Keith Ross"], "title": "Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models can exhibit improved mathematical reasoning\ncapabilities following post-training with instruction tuning, reinforcement\nlearning, or knowledge distillation. However, it remains unclear whether these\nimprovements are driven by major changes in transformer layers or from minor\nadjustments that leave the relative layer importance structures of the base\nmodel largely unchanged. We investigate this question through systematic\nlayer-wise ablation experiments, examining base, instruction-tuned,\nknowledge-distilled, and reinforcement learning variants on mathematical\nreasoning benchmarks. Our findings show that mathematical reasoning gives rise\nto a specific layer importance structure, and this structure persists across\nall post-training paradigms. Removal of such layers causes accuracy drops of up\nto 80%. In contrast, non-mathematical tasks like factual recall exhibit no\ncritical layers. This distinction suggests that mathematical reasoning requires\nspecialized layers that emerge during pre-training, while other non-reasoning\ntasks do not. From an information-theoretic perspective, we also observe that\nthese critical layers are the same layers where major representational\ntransformation occurs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u7279\u5b9a\u7684\u3001\u5173\u952e\u7684Transformer\u5c42\u7ed3\u6784\uff0c\u8fd9\u4e9b\u7ed3\u6784\u5728\u591a\u79cd\u540e\u8bad\u7ec3\u65b9\u6cd5\u4e2d\u4fdd\u6301\u4e0d\u53d8\uff0c\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e0e\u4e3b\u8981\u8868\u5f81\u8f6c\u6362\u76f8\u5173\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ecf\u8fc7\u6307\u4ee4\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u6216\u77e5\u8bc6\u84b8\u998f\u7b49\u540e\u8bad\u7ec3\u65b9\u6cd5\u540e\uff0c\u5176\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u662f\u5426\u6e90\u4e8eTransformer\u5c42\u7ed3\u6784\u7684\u91cd\u5927\u6539\u53d8\uff0c\u8fd8\u662f\u4ec5\u4e3a\u5fae\u5c0f\u8c03\u6574\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7684\u5c42\u7ea7\u6d88\u878d\u5b9e\u9a8c\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u57fa\u7840\u6a21\u578b\u3001\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u3001\u77e5\u8bc6\u84b8\u998f\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5c42\u7ea7\u91cd\u8981\u6027\u7ed3\u6784\u3002", "result": "\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u5b58\u5728\u7279\u5b9a\u7684\u5c42\u7ea7\u91cd\u8981\u6027\u7ed3\u6784\uff0c\u8be5\u7ed3\u6784\u5728\u6240\u6709\u540e\u8bad\u7ec3\u8303\u5f0f\u4e2d\u5747\u4fdd\u6301\u4e0d\u53d8\uff1b\u79fb\u9664\u8fd9\u4e9b\u5173\u952e\u5c42\u4f1a\u5bfc\u81f4\u9ad8\u8fbe80%\u7684\u51c6\u786e\u7387\u4e0b\u964d\uff1b\u975e\u6570\u5b66\u4efb\u52a1\uff08\u5982\u4e8b\u5b9e\u56de\u5fc6\uff09\u65e0\u5173\u952e\u5c42\uff1b\u8fd9\u4e9b\u5173\u952e\u5c42\u4e5f\u662f\u4e3b\u8981\u8868\u5f81\u8f6c\u6362\u53d1\u751f\u7684\u5c42\u3002", "conclusion": "\u6570\u5b66\u63a8\u7406\u9700\u8981\u4e13\u95e8\u7684\u5c42\uff0c\u8fd9\u4e9b\u5c42\u53ef\u80fd\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u5c31\u5df2\u5f62\u6210\uff0c\u800c\u5176\u4ed6\u975e\u63a8\u7406\u4efb\u52a1\u5219\u4e0d\u9700\u8981\uff0c\u8fd9\u8868\u660e\u6570\u5b66\u63a8\u7406\u80fd\u529b\u4f9d\u8d56\u4e8e\u6a21\u578b\u4e2d\u7279\u5b9a\u7684\u7ed3\u6784\u6027\u7279\u6027\u3002"}}
{"id": "2506.23128", "pdf": "https://arxiv.org/pdf/2506.23128", "abs": "https://arxiv.org/abs/2506.23128", "authors": ["Chi Chiu So", "Yueyue Sun", "Jun-Min Wang", "Siu Pang Yung", "Anthony Wai Keung Loh", "Chun Pong Chau"], "title": "Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons", "categories": ["cs.AI"], "comment": "10 pages, 0 figures, accepted by 2025 IEEE international conference\n  on artificial intelligence testing (AITest)", "summary": "How far are Large Language Models (LLMs) in performing deep relational\nreasoning? In this paper, we evaluate and compare the reasoning capabilities of\nthree cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a\nsuite of carefully designed benchmark tasks in family tree and general graph\nreasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the\nhighest F1-scores across multiple tasks and problem sizes, demonstrating strong\naptitude in logical deduction and relational inference. However, all evaluated\nmodels, including DeepSeek-R1, struggle significantly as problem complexity\nincreases, largely due to token length limitations and incomplete output\nstructures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought\nresponses uncovers its unique planning and verification strategies, but also\nhighlights instances of incoherent or incomplete reasoning, calling attention\nto the need for deeper scrutiny into LLMs' internal inference dynamics. We\nfurther discuss key directions for future work, including the role of\nmultimodal reasoning and the systematic examination of reasoning failures. Our\nfindings provide both empirical insights and theoretical implications for\nadvancing LLMs' reasoning abilities, particularly in tasks that demand\nstructured, multi-step logical inference. Our code repository will be publicly\navailable at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86DeepSeek-R1\u3001DeepSeek-V3\u548cGPT-4o\u5728\u6df1\u5ea6\u5173\u7cfb\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0DeepSeek-R1\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u590d\u6742\u6027\u589e\u52a0\u65f6\u5747\u53d7\u9650\uff0c\u5e76\u5206\u6790\u4e86DeepSeek-R1\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u3002", "motivation": "\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6267\u884c\u6df1\u5ea6\u5173\u7cfb\u63a8\u7406\u65b9\u9762\u7684\u5f53\u524d\u80fd\u529b\u548c\u8ddd\u79bb\u3002", "method": "\u901a\u8fc7\u5728\u5bb6\u65cf\u6811\u548c\u901a\u7528\u56fe\u63a8\u7406\u9886\u57df\u8bbe\u8ba1\u7684\u4e00\u7cfb\u5217\u57fa\u51c6\u4efb\u52a1\uff0c\u8bc4\u4f30\u5e76\u6bd4\u8f83\u4e86DeepSeek-R1\u3001DeepSeek-V3\u548cGPT-4o\u4e09\u79cd\u524d\u6cbfLLM\u7684\u63a8\u7406\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5bf9DeepSeek-R1\u7684\u601d\u7ef4\u94fe\uff08Chain-of-Thought\uff09\u54cd\u5e94\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cDeepSeek-R1\u5728\u591a\u9879\u4efb\u52a1\u548c\u4e0d\u540c\u95ee\u9898\u89c4\u6a21\u4e0b\u6301\u7eed\u83b7\u5f97\u6700\u9ad8\u7684F1\u5206\u6570\uff0c\u5c55\u73b0\u51fa\u5728\u903b\u8f91\u6f14\u7ece\u548c\u5173\u7cfb\u63a8\u7406\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\u3002\u7136\u800c\uff0c\u6240\u6709\u88ab\u8bc4\u4f30\u6a21\u578b\uff08\u5305\u62ecDeepSeek-R1\uff09\u5728\u95ee\u9898\u590d\u6742\u6027\u589e\u52a0\u65f6\uff0c\u7531\u4e8eToken\u957f\u5ea6\u9650\u5236\u548c\u8f93\u51fa\u7ed3\u6784\u4e0d\u5b8c\u6574\u7b49\u539f\u56e0\uff0c\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002\u5bf9DeepSeek-R1\u957f\u601d\u7ef4\u94fe\u54cd\u5e94\u7684\u5206\u6790\u63ed\u793a\u4e86\u5176\u72ec\u7279\u7684\u89c4\u5212\u548c\u9a8c\u8bc1\u7b56\u7565\uff0c\u4f46\u4e5f\u6307\u51fa\u5b58\u5728\u4e0d\u8fde\u8d2f\u6216\u4e0d\u5b8c\u6574\u7684\u63a8\u7406\u5b9e\u4f8b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u6d1e\u5bdf\u548c\u7406\u8bba\u542f\u793a\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7ed3\u6784\u5316\u3001\u591a\u6b65\u9aa4\u903b\u8f91\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u3002\u5f3a\u8c03\u9700\u8981\u66f4\u6df1\u5165\u5730\u5ba1\u89c6LLM\u7684\u5185\u90e8\u63a8\u7406\u52a8\u6001\uff0c\u5e76\u6307\u51fa\u672a\u6765\u5de5\u4f5c\u53ef\u5173\u6ce8\u591a\u6a21\u6001\u63a8\u7406\u548c\u7cfb\u7edf\u6027\u68c0\u67e5\u63a8\u7406\u5931\u8d25\u7684\u65b9\u5411\u3002"}}
{"id": "2506.22531", "pdf": "https://arxiv.org/pdf/2506.22531", "abs": "https://arxiv.org/abs/2506.22531", "authors": ["Prasen Kumar Sharma", "Neeraj Matiyali", "Siddharth Srivastava", "Gaurav Sharma"], "title": "Preserve Anything: Controllable Image Synthesis with Object Preservation", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025", "summary": "We introduce \\textit{Preserve Anything}, a novel method for controlled image\nsynthesis that addresses key limitations in object preservation and semantic\nconsistency in text-to-image (T2I) generation. Existing approaches often fail\n(i) to preserve multiple objects with fidelity, (ii) maintain semantic\nalignment with prompts, or (iii) provide explicit control over scene\ncomposition. To overcome these challenges, the proposed method employs an\nN-channel ControlNet that integrates (i) object preservation with size and\nplacement agnosticism, color and detail retention, and artifact elimination,\n(ii) high-resolution, semantically consistent backgrounds with accurate\nshadows, lighting, and prompt adherence, and (iii) explicit user control over\nbackground layouts and lighting conditions. Key components of our framework\ninclude object preservation and background guidance modules, enforcing lighting\nconsistency and a high-frequency overlay module to retain fine details while\nmitigating unwanted artifacts. We introduce a benchmark dataset consisting of\n240K natural images filtered for aesthetic quality and 18K 3D-rendered\nsynthetic images with metadata such as lighting, camera angles, and object\nrelationships. This dataset addresses the deficiencies of existing benchmarks\nand allows a complete evaluation. Empirical results demonstrate that our method\nachieves state-of-the-art performance, significantly improving feature-space\nfidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining\ncompetitive aesthetic quality. We also conducted a user study to demonstrate\nthe efficacy of the proposed work on unseen benchmark and observed a remarkable\nimprovement of $\\sim25\\%$, $\\sim19\\%$, $\\sim13\\%$, and $\\sim14\\%$ in terms of\nprompt alignment, photorealism, the presence of AI artifacts, and natural\naesthetics over existing works.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201cPreserve Anything\u201d\u65b9\u6cd5\uff0c\u901a\u8fc7N\u901a\u9053ControlNet\u548c\u65b0\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u751f\u6210\u4e2d\u591a\u5bf9\u8c61\u4fdd\u771f\u5ea6\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u573a\u666f\u63a7\u5236\u3002", "motivation": "\u73b0\u6709T2I\u65b9\u6cd5\u5728\u4ee5\u4e0b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff1a(i) \u96be\u4ee5\u5fe0\u5b9e\u5730\u4fdd\u7559\u591a\u4e2a\u5bf9\u8c61\uff1b(ii) \u96be\u4ee5\u4e0e\u63d0\u793a\u8bcd\u4fdd\u6301\u8bed\u4e49\u5bf9\u9f50\uff1b(iii) \u65e0\u6cd5\u5bf9\u573a\u666f\u6784\u6210\u63d0\u4f9b\u660e\u786e\u63a7\u5236\u3002", "method": "\u5f15\u5165N\u901a\u9053ControlNet\uff0c\u5b9e\u73b0\uff1a(i) \u5bf9\u8c61\u4fdd\u7559\uff08\u4e0e\u5c3a\u5bf8/\u4f4d\u7f6e\u65e0\u5173\uff0c\u4fdd\u7559\u989c\u8272/\u7ec6\u8282\uff0c\u6d88\u9664\u4f2a\u5f71\uff09\uff1b(ii) \u9ad8\u5206\u8fa8\u7387\u3001\u8bed\u4e49\u4e00\u81f4\u7684\u80cc\u666f\uff08\u51c6\u786e\u7684\u9634\u5f71/\u5149\u7167\uff0c\u9075\u5faa\u63d0\u793a\u8bcd\uff09\uff1b(iii) \u7528\u6237\u5bf9\u80cc\u666f\u5e03\u5c40/\u5149\u7167\u7684\u663e\u5f0f\u63a7\u5236\u3002\u6846\u67b6\u5305\u542b\u5bf9\u8c61\u4fdd\u7559\u3001\u80cc\u666f\u5f15\u5bfc\u3001\u5149\u7167\u4e00\u81f4\u6027\u548c\u9ad8\u9891\u53e0\u52a0\u6a21\u5757\u3002\u6784\u5efa\u4e86\u5305\u542b24\u4e07\u5f20\u81ea\u7136\u56fe\u50cf\u548c1.8\u4e07\u5f203D\u6e32\u67d3\u5408\u6210\u56fe\u50cf\u7684\u65b0\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u7ecf\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7279\u5f81\u7a7a\u95f4\u4fdd\u771f\u5ea6\uff08FID 15.26\uff09\u548c\u8bed\u4e49\u5bf9\u9f50\uff08CLIP-S 32.85\uff09\uff0c\u5e76\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7f8e\u5b66\u8d28\u91cf\u3002\u7528\u6237\u7814\u7a76\u663e\u793a\uff0c\u5728\u63d0\u793a\u8bcd\u5bf9\u9f50\u3001\u771f\u5b9e\u611f\u3001AI\u4f2a\u5f71\u548c\u81ea\u7136\u7f8e\u5b66\u65b9\u9762\uff0c\u76f8\u6bd4\u73b0\u6709\u5de5\u4f5c\u5206\u522b\u63d0\u9ad8\u4e86\u7ea625%\u300119%\u300113%\u548c14%\u3002", "conclusion": "\u201cPreserve Anything\u201d\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86T2I\u751f\u6210\u4e2d\u5bf9\u8c61\u4fdd\u7559\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u5173\u952e\u5c40\u9650\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u67b6\u6784\u548c\u6570\u636e\u96c6\uff0c\u5728\u6027\u80fd\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u7528\u6237\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2506.22791", "pdf": "https://arxiv.org/pdf/2506.22791", "abs": "https://arxiv.org/abs/2506.22791", "authors": ["Jianxin Yan", "Wangze Ni", "Lei Chen", "Xuemin Lin", "Peng Cheng", "Zhan Qin", "Kui Ren"], "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models", "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.", "AI": {"tldr": "ContextCache\u662f\u4e00\u79cd\u9488\u5bf9\u591a\u8f6e\u5bf9\u8bdd\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u8bed\u4e49\u7f13\u5b58\u7cfb\u7edf\uff0c\u5b83\u901a\u8fc7\u6574\u5408\u5bf9\u8bdd\u5386\u53f2\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86\u7f13\u5b58\u547d\u4e2d\u7cbe\u5ea6\uff0c\u5e76\u5927\u5e45\u964d\u4f4e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u7684\u5ef6\u8fdf\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u7f13\u5b58\u7cfb\u7edf\u4e3b\u8981\u57fa\u4e8e\u5355\u4e2a\u67e5\u8be2\u5339\u914d\uff0c\u7f3a\u4e4f\u5bf9\u591a\u8f6e\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u7684\u611f\u77e5\uff0c\u5bfc\u81f4\u5728\u4e0d\u540c\u5bf9\u8bdd\u573a\u666f\u4e2d\u76f8\u4f3c\u67e5\u8be2\u53ef\u80fd\u51fa\u73b0\u4e0d\u6b63\u786e\u7684\u7f13\u5b58\u547d\u4e2d\u3002", "method": "ContextCache\u91c7\u7528\u4e24\u9636\u6bb5\u68c0\u7d22\u67b6\u6784\uff1a\u9996\u5148\u5bf9\u5f53\u524d\u67e5\u8be2\u8fdb\u884c\u5411\u91cf\u68c0\u7d22\u4ee5\u8bc6\u522b\u6f5c\u5728\u5339\u914d\uff0c\u7136\u540e\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u5f53\u524d\u548c\u5386\u53f2\u5bf9\u8bdd\u8868\u793a\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u4e0a\u4e0b\u6587\u5339\u914d\u3002", "result": "\u5728\u771f\u5b9e\u5bf9\u8bdd\u4e2d\u7684\u8bc4\u4f30\u663e\u793a\uff0cContextCache\u76f8\u8f83\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\uff1b\u7f13\u5b58\u54cd\u5e94\u7684\u5ef6\u8fdf\u6bd4\u76f4\u63a5\u8c03\u7528LLM\u4f4e\u7ea610\u500d\u3002", "conclusion": "ContextCache\u6709\u6548\u964d\u4f4e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u8bdd\u5e94\u7528\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u9ad8\u4e86\u5176\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2506.23350", "pdf": "https://arxiv.org/pdf/2506.23350", "abs": "https://arxiv.org/abs/2506.23350", "authors": ["Jo\u00e3o Pedro Loureiro", "Patr\u00edcia Delgado", "Tom\u00e1s Feliciano Ribeiro", "Filipe B. Teixeira", "Rui Campos"], "title": "On the Resilience of Underwater Semantic Wireless Communications", "categories": ["cs.NI"], "comment": null, "summary": "Underwater wireless communications face significant challenges due to\npropagation constraints, limiting the effectiveness of traditional radio and\noptical technologies. Long-range acoustic communications support distances up\nto a few kilometers, but suffer from low bandwidth, high error ratios, and\nmultipath interference. Semantic communications, which focus on transmitting\nextracted semantic features rather than raw data, present a promising solution\nby significantly reducing the volume of data transmitted over the wireless\nlink.\n  This paper evaluates the resilience of SAGE, a semantic-oriented\ncommunications framework that combines semantic processing with Generative\nArtificial Intelligence (GenAI) to compress and transmit image data as textual\ndescriptions over acoustic links. To assess robustness, we use a\ncustom-tailored simulator that introduces character errors observed in\nunderwater acoustic channels. Evaluation results show that SAGE can\nsuccessfully reconstruct meaningful image content even under varying error\nconditions, highlighting its potential for robust and efficient underwater\nwireless communication in harsh environments.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86SAGE\uff08\u4e00\u4e2a\u7ed3\u5408\u8bed\u4e49\u5904\u7406\u548c\u751f\u6210\u5f0fAI\u7684\u6c34\u4e0b\u56fe\u50cf\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff09\u7684\u9c81\u68d2\u6027\u3002\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u5728\u6c34\u4e0b\u58f0\u5b66\u4fe1\u9053\u6a21\u62df\u7684\u5b57\u7b26\u9519\u8bef\u6761\u4ef6\u4e0b\uff0cSAGE\u4e5f\u80fd\u6210\u529f\u91cd\u5efa\u6709\u610f\u4e49\u7684\u56fe\u50cf\u5185\u5bb9\uff0c\u8bc1\u660e\u5176\u5728\u6076\u52a3\u73af\u5883\u4e0b\u7684\u6f5c\u529b\u3002", "motivation": "\u6c34\u4e0b\u65e0\u7ebf\u901a\u4fe1\u56e0\u4f20\u64ad\u9650\u5236\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c\u4f20\u7edf\u65e0\u7ebf\u7535\u548c\u5149\u5b66\u6280\u672f\u6548\u679c\u4e0d\u4f73\u3002\u8fdc\u7a0b\u58f0\u5b66\u901a\u4fe1\u867d\u80fd\u652f\u6301\u6570\u516c\u91cc\u8ddd\u79bb\uff0c\u4f46\u5b58\u5728\u5e26\u5bbd\u4f4e\u3001\u8bef\u7801\u7387\u9ad8\u548c\u591a\u5f84\u5e72\u6270\u7b49\u95ee\u9898\u3002\u8bed\u4e49\u901a\u4fe1\u901a\u8fc7\u4f20\u8f93\u63d0\u53d6\u7684\u8bed\u4e49\u7279\u5f81\u800c\u975e\u539f\u59cb\u6570\u636e\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u91cf\uff0c\u6709\u671b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u6587\u8bc4\u4f30\u4e86SAGE\u6846\u67b6\u7684\u97e7\u6027\u3002SAGE\u662f\u4e00\u4e2a\u9762\u5411\u8bed\u4e49\u7684\u901a\u4fe1\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u8bed\u4e49\u5904\u7406\u548c\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\uff0c\u5c06\u56fe\u50cf\u6570\u636e\u538b\u7f29\u5e76\u4ee5\u6587\u672c\u63cf\u8ff0\u7684\u5f62\u5f0f\u901a\u8fc7\u58f0\u5b66\u94fe\u8def\u4f20\u8f93\u3002\u4e3a\u8bc4\u4f30\u5176\u9c81\u68d2\u6027\uff0c\u7814\u7a76\u4f7f\u7528\u4e86\u5b9a\u5236\u6a21\u62df\u5668\u6765\u5f15\u5165\u6c34\u4e0b\u58f0\u5b66\u4fe1\u9053\u4e2d\u89c2\u5bdf\u5230\u7684\u5b57\u7b26\u9519\u8bef\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u4e0d\u540c\u7684\u9519\u8bef\u6761\u4ef6\u4e0b\uff0cSAGE\u4e5f\u80fd\u6210\u529f\u91cd\u5efa\u6709\u610f\u4e49\u7684\u56fe\u50cf\u5185\u5bb9\u3002", "conclusion": "SAGE\u5728\u6076\u52a3\u73af\u5883\u4e0b\u5b9e\u73b0\u7a33\u5065\u9ad8\u6548\u7684\u6c34\u4e0b\u65e0\u7ebf\u901a\u4fe1\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2506.22645", "pdf": "https://arxiv.org/pdf/2506.22645", "abs": "https://arxiv.org/abs/2506.22645", "authors": ["Amir Hossein Rahmati", "Nathan M. Urban", "Byung-Jun Yoon", "Xiaoning Qian"], "title": "Cost-effective Reduced-Order Modeling via Bayesian Active Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Machine Learning surrogates have been developed to accelerate solving systems\ndynamics of complex processes in different science and engineering\napplications. To faithfully capture governing systems dynamics, these methods\nrely on large training datasets, hence restricting their applicability in\nreal-world problems. In this work, we propose BayPOD-AL, an active learning\nframework based on an uncertainty-aware Bayesian proper orthogonal\ndecomposition (POD) approach, which aims to effectively learn reduced-order\nmodels from high-fidelity full-order models representing complex systems.\nExperimental results on predicting the temperature evolution over a rod\ndemonstrate BayPOD-AL's effectiveness in suggesting the informative data and\nreducing computational cost related to constructing a training dataset compared\nto other uncertainty-guided active learning strategies. Furthermore, we\ndemonstrate BayPOD-AL's generalizability and efficiency by evaluating its\nperformance on a dataset of higher temporal resolution than the training\ndataset.", "AI": {"tldr": "\u63d0\u51faBayPOD-AL\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8d1d\u53f6\u65afPOD\u65b9\u6cd5\u9ad8\u6548\u6784\u5efa\u590d\u6742\u7cfb\u7edf\u964d\u9636\u6a21\u578b\uff0c\u51cf\u5c11\u6570\u636e\u9700\u6c42\u4e0e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u66ff\u4ee3\u6a21\u578b\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6765\u51c6\u786e\u6355\u6349\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51faBayPOD-AL\uff0c\u4e00\u4e2a\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8d1d\u53f6\u65af\u672c\u5f81\u6b63\u4ea4\u5206\u89e3\uff08POD\uff09\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u4ece\u9ad8\u4fdd\u771f\u5168\u9636\u6a21\u578b\u4e2d\u9ad8\u6548\u5b66\u4e60\u590d\u6742\u7cfb\u7edf\u7684\u964d\u9636\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBayPOD-AL\u80fd\u6709\u6548\u8bc6\u522b\u4fe1\u606f\u91cf\u5927\u7684\u6570\u636e\uff0c\u5e76\u76f8\u6bd4\u5176\u4ed6\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6570\u636e\u96c6\u6784\u5efa\u7684\u8ba1\u7b97\u6210\u672c\u3002\u6b64\u5916\uff0c\u5b83\u5728\u66f4\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u6570\u636e\u96c6\u4e0a\u4e5f\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6027\u548c\u6548\u7387\u3002", "conclusion": "BayPOD-AL\u662f\u5b66\u4e60\u590d\u6742\u7cfb\u7edf\u964d\u9636\u6a21\u578b\u7684\u6709\u6548\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u5b66\u4e60\u66ff\u4ee3\u6a21\u578b\u5bf9\u5927\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.23141", "pdf": "https://arxiv.org/pdf/2506.23141", "abs": "https://arxiv.org/abs/2506.23141", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun"], "title": "Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing", "categories": ["cs.AI"], "comment": null, "summary": "Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge\nGraph Completion (KGC), providing vital cues for prediction. However,\ntraditional node-based message passing mechanisms, when applied to knowledge\ngraphs, often introduce noise and suffer from information dilution or\nover-smoothing by indiscriminately aggregating information from all neighboring\nedges. To address this challenge, we propose a semantic-aware relational\nmessage passing. A core innovation of this framework is the introduction of a\n\\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this\nstrategy first evaluates the semantic relevance between a central node and its\nincident edges within a shared latent space, selecting only the Top-K most\npertinent ones. Subsequently, information from these selected edges is\neffectively fused with the central node's own representation using a\n\\textbf{multi-head attention aggregator} to generate a semantically focused\nnode message. In this manner, our model not only leverages the structure and\nfeatures of edges within the knowledge graph but also more accurately captures\nand propagates the contextual information most relevant to the specific link\nprediction task, thereby effectively mitigating interference from irrelevant\ninformation. Extensive experiments demonstrate that our method achieves\nsuperior performance compared to existing approaches on several established\nbenchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u5173\u7cfb\u6d88\u606f\u4f20\u9012\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5Top-K\u90bb\u5c45\u9009\u62e9\u548c\u591a\u5934\u6ce8\u610f\u529b\u805a\u5408\u5668\uff0c\u6709\u6548\u8fc7\u6ee4\u65e0\u5173\u4fe1\u606f\uff0c\u63d0\u5347\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u6027\u80fd\u3002", "motivation": "\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4e2d\uff0c\u8bed\u4e49\u4e0a\u4e0b\u6587\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u8282\u70b9\u6d88\u606f\u4f20\u9012\u673a\u5236\u56e0\u65e0\u5dee\u522b\u805a\u5408\u90bb\u5c45\u4fe1\u606f\uff0c\u6613\u5f15\u5165\u566a\u58f0\u3001\u7a00\u91ca\u4fe1\u606f\u6216\u5bfc\u81f4\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u611f\u77e5\u5173\u7cfb\u6d88\u606f\u4f20\u9012\u6846\u67b6\u3002\u6838\u5fc3\u521b\u65b0\u5305\u62ec\uff1a1) \u8bed\u4e49\u611f\u77e5Top-K\u90bb\u5c45\u9009\u62e9\u7b56\u7565\uff0c\u8bc4\u4f30\u4e2d\u5fc3\u8282\u70b9\u4e0e\u5165\u5c04\u8fb9\u7684\u8bed\u4e49\u76f8\u5173\u6027\u5e76\u9009\u62e9\u6700\u76f8\u5173\u7684Top-K\u6761\u8fb9\uff1b2) \u591a\u5934\u6ce8\u610f\u529b\u805a\u5408\u5668\uff0c\u5c06\u9009\u5b9a\u8fb9\u7684\u4fe1\u606f\u4e0e\u4e2d\u5fc3\u8282\u70b9\u8868\u793a\u878d\u5408\uff0c\u751f\u6210\u8bed\u4e49\u805a\u7126\u7684\u8282\u70b9\u6d88\u606f\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u6355\u83b7\u5e76\u4f20\u64ad\u4e0e\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u6700\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ece\u800c\u51cf\u8f7b\u4e0d\u76f8\u5173\u4fe1\u606f\u7684\u5e72\u6270\u3002", "result": "\u5728\u591a\u4e2a\u5df2\u5efa\u7acb\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bed\u4e49\u611f\u77e5\u5173\u7cfb\u6d88\u606f\u4f20\u9012\u65b9\u6cd5\uff0c\u901a\u8fc7\u667a\u80fd\u9009\u62e9\u548c\u6709\u6548\u805a\u5408\u76f8\u5173\u8bed\u4e49\u4fe1\u606f\uff0c\u6210\u529f\u7f13\u89e3\u4e86\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4e2d\u7684\u566a\u58f0\u548c\u4fe1\u606f\u7a00\u91ca\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2506.22554", "pdf": "https://arxiv.org/pdf/2506.22554", "abs": "https://arxiv.org/abs/2506.22554", "authors": ["Vasu Agrawal", "Akinniyi Akinyemi", "Kathryn Alvero", "Morteza Behrooz", "Julia Buffalini", "Fabio Maria Carlucci", "Joy Chen", "Junming Chen", "Zhang Chen", "Shiyang Cheng", "Praveen Chowdary", "Joe Chuang", "Antony D'Avirro", "Jon Daly", "Ning Dong", "Mark Duppenthaler", "Cynthia Gao", "Jeff Girard", "Martin Gleize", "Sahir Gomez", "Hongyu Gong", "Srivathsan Govindarajan", "Brandon Han", "Sen He", "Denise Hernandez", "Yordan Hristov", "Rongjie Huang", "Hirofumi Inaguma", "Somya Jain", "Raj Janardhan", "Qingyao Jia", "Christopher Klaiber", "Dejan Kovachev", "Moneish Kumar", "Hang Li", "Yilei Li", "Pavel Litvin", "Wei Liu", "Guangyao Ma", "Jing Ma", "Martin Ma", "Xutai Ma", "Lucas Mantovani", "Sagar Miglani", "Sreyas Mohan", "Louis-Philippe Morency", "Evonne Ng", "Kam-Woh Ng", "Tu Anh Nguyen", "Amia Oberai", "Benjamin Peloquin", "Juan Pino", "Jovan Popovic", "Omid Poursaeed", "Fabian Prada", "Alice Rakotoarison", "Alexander Richard", "Christophe Ropers", "Safiyyah Saleem", "Vasu Sharma", "Alex Shcherbyna", "Jia Shen", "Jie Shen", "Anastasis Stathopoulos", "Anna Sun", "Paden Tomasello", "Tuan Tran", "Arina Turkatenko", "Bo Wan", "Chao Wang", "Jeff Wang", "Mary Williamson", "Carleigh Wood", "Tao Xiang", "Yilin Yang", "Julien Yao", "Chen Zhang", "Jiemin Zhang", "Xinyue Zhang", "Jason Zheng", "Pavlo Zhyzheria", "Jan Zikes", "Michael Zollhoefer"], "title": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human communication involves a complex interplay of verbal and nonverbal\nsignals, essential for conveying meaning and achieving interpersonal goals. To\ndevelop socially intelligent AI technologies, it is crucial to develop models\nthat can both comprehend and generate dyadic behavioral dynamics. To this end,\nwe introduce the Seamless Interaction Dataset, a large-scale collection of over\n4,000 hours of face-to-face interaction footage from over 4,000 participants in\ndiverse contexts. This dataset enables the development of AI technologies that\nunderstand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,\ntelepresence experiences, and multimodal content analysis tools. We also\ndevelop a suite of models that utilize the dataset to generate dyadic motion\ngestures and facial expressions aligned with human speech. These models can\ntake as input both the speech and visual behavior of their interlocutors. We\npresent a variant with speech from an LLM model and integrations with 2D and 3D\nrendering methods, bringing us closer to interactive virtual agents.\nAdditionally, we describe controllable variants of our motion models that can\nadapt emotional responses and expressivity levels, as well as generating more\nsemantically-relevant gestures. Finally, we discuss methods for assessing the\nquality of these dyadic motion models, which are demonstrating the potential\nfor more intuitive and responsive human-AI interactions.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u4eba\u673a\u4ea4\u4e92\u6570\u636e\u96c6\uff08Seamless Interaction Dataset\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u7cfb\u5217\u6a21\u578b\uff0c\u65e8\u5728\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u8bed\u97f3\u7684\u5bf9\u8bdd\u5f0f\u8eab\u4f53\u59ff\u6001\u548c\u9762\u90e8\u8868\u60c5\uff0c\u4ee5\u63a8\u52a8\u793e\u4ea4\u667a\u80fdAI\u7684\u53d1\u5c55\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u865a\u62df\u4ee3\u7406\u548c\u4eba\u673a\u4e92\u52a8\u3002", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u5177\u6709\u793e\u4ea4\u667a\u80fd\u7684AI\u6280\u672f\uff0c\u7406\u89e3\u548c\u751f\u6210\u4e8c\u5143\u884c\u4e3a\u52a8\u6001\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u4eba\u7c7b\u4ea4\u6d41\u6d89\u53ca\u590d\u6742\u7684\u8a00\u8bed\u548c\u975e\u8a00\u8bed\u4fe1\u53f7\uff0c\u5bf9\u4e8e\u4f20\u8fbe\u610f\u4e49\u548c\u5b9e\u73b0\u4eba\u9645\u76ee\u6807\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u4e86Seamless Interaction Dataset\uff0c\u4e00\u4e2a\u5305\u542b4000\u591a\u5c0f\u65f6\u9762\u5bf9\u9762\u4e92\u52a8\u89c6\u9891\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e86\u4e00\u5957\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u4eba\u7c7b\u8bed\u97f3\u751f\u6210\u4e8c\u5143\u52a8\u4f5c\u624b\u52bf\u548c\u9762\u90e8\u8868\u60c5\uff0c\u5e76\u53ef\u8f93\u5165\u5bf9\u8bdd\u8005\u7684\u8bed\u97f3\u548c\u89c6\u89c9\u884c\u4e3a\u3002\u6a21\u578b\u8fd8\u5305\u542b\u4e86\u4e0eLLM\u8bed\u97f3\u548c2D/3D\u6e32\u67d3\u96c6\u6210\u7684\u53d8\u4f53\uff0c\u4ee5\u53ca\u53ef\u63a7\u5236\u60c5\u611f\u54cd\u5e94\u3001\u8868\u8fbe\u6c34\u5e73\u548c\u8bed\u4e49\u76f8\u5173\u624b\u52bf\u7684\u53d8\u4f53\u3002", "result": "\u8be5\u6570\u636e\u96c6\u8d4b\u80fd\u4e86\u7406\u89e3\u4e8c\u5143\u5177\u8eab\u52a8\u6001\u7684AI\u6280\u672f\u53d1\u5c55\uff0c\u4e3a\u865a\u62df\u4ee3\u7406\u3001\u8fdc\u7a0b\u4e34\u573a\u4f53\u9a8c\u548c\u591a\u6a21\u6001\u5185\u5bb9\u5206\u6790\u5de5\u5177\u5e26\u6765\u7a81\u7834\u3002\u5f00\u53d1\u7684\u6a21\u578b\u4f7f\u5f97\u4ea4\u4e92\u5f0f\u865a\u62df\u4ee3\u7406\u66f4\u63a5\u8fd1\u73b0\u5b9e\uff0c\u5e76\u5c55\u793a\u4e86\u5b9e\u73b0\u66f4\u76f4\u89c2\u3001\u54cd\u5e94\u66f4\u7075\u654f\u7684\u4eba\u673a\u4ea4\u4e92\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5f00\u53d1\u521b\u65b0\u7684\u4e8c\u5143\u52a8\u4f5c\u751f\u6210\u6a21\u578b\uff0c\u663e\u8457\u63a8\u52a8\u4e86\u793e\u4ea4\u667a\u80fdAI\u7684\u8fdb\u5c55\uff0c\u4e3a\u672a\u6765\u66f4\u81ea\u7136\u3001\u66f4\u5177\u54cd\u5e94\u6027\u7684\u4eba\u673a\u4ea4\u4e92\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.22808", "pdf": "https://arxiv.org/pdf/2506.22808", "abs": "https://arxiv.org/abs/2506.22808", "authors": ["Jianhui Wei", "Zijie Meng", "Zikai Xiao", "Tianxiang Hu", "Yang Feng", "Zhijie Zhou", "Jian Wu", "Zuozhu Liu"], "title": "MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages", "summary": "While Medical Large Language Models (MedLLMs) have demonstrated remarkable\npotential in clinical tasks, their ethical safety remains insufficiently\nexplored. This paper introduces $\\textbf{MedEthicsQA}$, a comprehensive\nbenchmark comprising $\\textbf{5,623}$ multiple-choice questions and\n$\\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.\nWe systematically establish a hierarchical taxonomy integrating global medical\nethical standards. The benchmark encompasses widely used medical datasets,\nauthoritative question banks, and scenarios derived from PubMed literature.\nRigorous quality control involving multi-stage filtering and multi-faceted\nexpert validation ensures the reliability of the dataset with a low error rate\n($2.72\\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance\nin answering medical ethics questions compared to their foundation\ncounterparts, elucidating the deficiencies of medical ethics alignment. The\ndataset, registered under CC BY-NC 4.0 license, is available at\nhttps://github.com/JianhuiWei7/MedEthicsQA.", "AI": {"tldr": "\u672c\u8bba\u6587\u5f15\u5165\u4e86MedEthicsQA\uff0c\u4e00\u4e2a\u5305\u542b\u591a\u9009\u548c\u5f00\u653e\u5f0f\u95ee\u9898\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4f26\u7406\u5b89\u5168\u6027\u3002\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u4f26\u7406\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u51f8\u663e\u4e86\u5176\u4f26\u7406\u5bf9\u9f50\u7684\u4e0d\u8db3\u3002", "motivation": "\u5c3d\u7ba1\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\uff08MedLLMs\uff09\u5728\u4e34\u5e8a\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u4f26\u7406\u5b89\u5168\u6027\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86MedEthicsQA\uff0c\u4e00\u4e2a\u5305\u542b5,623\u9053\u591a\u9009\u95ee\u9898\u548c5,351\u9053\u5f00\u653e\u5f0f\u95ee\u9898\u7684\u533b\u5b66\u4f26\u7406\u8bc4\u4f30\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u7cfb\u7edf\u5730\u6574\u5408\u4e86\u5168\u7403\u533b\u5b66\u4f26\u7406\u6807\u51c6\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u5206\u5c42\u5206\u7c7b\u4f53\u7cfb\uff0c\u6570\u636e\u6765\u6e90\u4e8e\u5e38\u7528\u533b\u5b66\u6570\u636e\u96c6\u3001\u6743\u5a01\u9898\u5e93\u548cPubMed\u6587\u732e\u3002\u901a\u8fc7\u591a\u9636\u6bb5\u8fc7\u6ee4\u548c\u591a\u65b9\u9762\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u786e\u4fdd\u4e86\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\uff08\u9519\u8bef\u73872.72%\uff09\u3002", "result": "\u5bf9\u73b0\u6709\u6700\u5148\u8fdb\u7684\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d1\u73b0\u5176\u5728\u56de\u7b54\u533b\u5b66\u4f26\u7406\u95ee\u9898\u65b9\u9762\u7684\u8868\u73b0\uff0c\u76f8\u6bd4\u4e8e\u5176\u57fa\u7840\u6a21\u578b\u6709\u6240\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u533b\u5b66\u4f26\u7406\u5bf9\u9f50\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "conclusion": "MedEthicsQA\u57fa\u51c6\u7684\u5f15\u5165\uff0c\u6709\u6548\u63ed\u793a\u4e86\u5f53\u524d\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f26\u7406\u5b89\u5168\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u5176\u4f26\u7406\u5bf9\u9f50\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u65b9\u5411\u548c\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2506.23488", "pdf": "https://arxiv.org/pdf/2506.23488", "abs": "https://arxiv.org/abs/2506.23488", "authors": ["Geng Sun", "Mingzhe Fan", "Lei Zhang", "Hongyang Pan", "Jiahui Li", "Chuang Zhang", "Linyao Li", "Changyuan Zhao", "Chau Yuen"], "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent Metasurfaces", "categories": ["cs.NI"], "comment": "This paper has been already submitted to TCCN", "summary": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality.", "AI": {"tldr": "\u7814\u7a76\u5e76\u4f18\u5316\u4e86\u65e0\u4eba\u673a\u642d\u8f7d\u667a\u80fd\u8d85\u8868\u9762\uff08UAV-SIMs\uff09\u8f85\u52a9\u7684\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7528\u6237\u5173\u8054\u3001\u65e0\u4eba\u673a\u5b9a\u4f4d\u548c\u8d85\u8868\u9762\u76f8\u79fb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u7684\u901a\u4fe1\u5bb9\u91cf\u3002", "motivation": "\u4e3a\u6ee1\u8db3\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u5bf9\u66f4\u9ad8\u6570\u636e\u901f\u7387\u548c\u66f4\u53ef\u9760\u8fde\u63a5\u7684\u9700\u6c42\uff0c\u672c\u6587\u5229\u7528\u79fb\u52a8\u667a\u80fd\u8d85\u8868\u9762\uff08SIMs\uff09\u7684\u4f18\u52bf\uff0c\u7279\u522b\u662f\u65e0\u4eba\u673a\u642d\u8f7dSIMs\u5728\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u4e2d\u4f5c\u4e3a\u57fa\u7ad9\u548c\u79fb\u52a8\u5e73\u53f0\uff0c\u65e8\u5728\u6700\u5927\u5316\u7f51\u7edc\u5bb9\u91cf\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u65e0\u4eba\u673a-SIMs\u8054\u5408\u4f18\u5316\u95ee\u9898\uff08USBJOP\uff09\uff0c\u6db5\u76d6\u7528\u6237\u5173\u8054\u3001\u65e0\u4eba\u673a\u4e09\u7ef4\u5b9a\u4f4d\u548cSIMs\u591a\u5c42\u76f8\u79fb\u3002\u8be5\u975e\u51f8\u4e14NP\u96be\u7684\u95ee\u9898\u88ab\u5206\u89e3\u4e3a\u4e09\u4e2a\u5b50\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u7b56\u7565\u6c42\u89e3\u3002\u5176\u4e2d\uff0c\u7528\u6237\u5173\u8054\u548c\u65e0\u4eba\u673a\u5b9a\u4f4d\u95ee\u9898\u88ab\u8f6c\u5316\u4e3a\u51f8\u5f62\u5f0f\u7531CVX\u5de5\u5177\u6c42\u89e3\uff0cSIMs\u76f8\u79fb\u95ee\u9898\u5219\u901a\u8fc7\u57fa\u4e8e\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GAI\uff09\u7684\u6df7\u5408\u4f18\u5316\u7b97\u6cd5\u89e3\u51b3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u7ea61.5\u500d\u7684\u7f51\u7edc\u5bb9\u91cf\u63d0\u5347\u3002\u540c\u65f6\uff0c\u57fa\u4e8eGAI\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5c06\u7b97\u6cd5\u8fd0\u884c\u65f6\u95f4\u7f29\u77ed\u4e8610%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65e0\u4eba\u673a\u642d\u8f7d\u667a\u80fd\u8d85\u8868\u9762\u901a\u4fe1\u7cfb\u7edf\u53ca\u5176\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u5347\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u7684\u901a\u4fe1\u6027\u80fd\u548c\u5bb9\u91cf\uff0c\u7279\u522b\u662f\u5728\u76f8\u79fb\u4f18\u5316\u4e2d\u5f15\u5165GAI\u63d0\u5347\u4e86\u6548\u7387\u3002"}}
{"id": "2506.22655", "pdf": "https://arxiv.org/pdf/2506.22655", "abs": "https://arxiv.org/abs/2506.22655", "authors": ["Andrew F. Ilersich", "Prasanth B. Nair"], "title": "Learning Stochastic Multiscale Models", "categories": ["cs.LG"], "comment": "Body is 9 pages, 13 including acknowledgements and references, 35\n  including appendix. 21 figures and 6 tables. Submitted to NeurIPS 2025", "summary": "The physical sciences are replete with dynamical systems that require the\nresolution of a wide range of length and time scales. This presents significant\ncomputational challenges since direct numerical simulation requires\ndiscretization at the finest relevant scales, leading to a high-dimensional\nstate space. In this work, we propose an approach to learn stochastic\nmultiscale models in the form of stochastic differential equations directly\nfrom observational data. Our method resolves the state on a coarse mesh while\nintroducing an auxiliary state to capture the effects of unresolved scales. We\nlearn the parameters of the multiscale model using a modern forward-solver-free\namortized variational inference method. Our approach draws inspiration from\nphysics-based multiscale modeling approaches, such as large-eddy simulation in\nfluid dynamics, while learning directly from data. We present numerical studies\nto demonstrate that our learned multiscale models achieve superior predictive\naccuracy compared to direct numerical simulation and closure-type models at\nequivalent resolution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u5b66\u4e60\u968f\u673a\u591a\u5c3a\u5ea6\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c97\u7f51\u683c\u548c\u8f85\u52a9\u72b6\u6001\u5904\u7406\u672a\u89e3\u6790\u5c3a\u5ea6\uff0c\u5e76\u5229\u7528\u65e0\u524d\u5411\u6c42\u89e3\u5668\u7684\u53d8\u5206\u63a8\u65ad\u65b9\u6cd5\u5b66\u4e60\u6a21\u578b\u53c2\u6570\uff0c\u5b9e\u73b0\u5728\u76f8\u540c\u5206\u8fa8\u7387\u4e0b\u4f18\u4e8e\u4f20\u7edf\u6a21\u62df\u65b9\u6cd5\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u7269\u7406\u79d1\u5b66\u4e2d\u7684\u52a8\u529b\u7cfb\u7edf\u5e38\u6d89\u53ca\u5bbd\u6cdb\u7684\u957f\u5ea6\u548c\u65f6\u95f4\u5c3a\u5ea6\uff0c\u76f4\u63a5\u6570\u503c\u6a21\u62df\uff08DNS\uff09\u56e0\u9700\u8981\u5bf9\u6700\u7cbe\u7ec6\u5c3a\u5ea6\u8fdb\u884c\u79bb\u6563\u5316\uff0c\u5bfc\u81f4\u72b6\u6001\u7a7a\u95f4\u7ef4\u5ea6\u9ad8\uff0c\u8ba1\u7b97\u6210\u672c\u5de8\u5927\uff0c\u6784\u6210\u663e\u8457\u7684\u8ba1\u7b97\u6311\u6218\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u76f4\u63a5\u5b66\u4e60\u968f\u673a\u591a\u5c3a\u5ea6\u6a21\u578b\uff08\u4ee5\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u5f62\u5f0f\uff09\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u7c97\u7f51\u683c\u4e0a\u89e3\u6790\u72b6\u6001\uff0c\u5e76\u5f15\u5165\u8f85\u52a9\u72b6\u6001\u4ee5\u6355\u83b7\u672a\u89e3\u6790\u5c3a\u5ea6\u7684\u5f71\u54cd\u3002\u6a21\u578b\u53c2\u6570\u901a\u8fc7\u4e00\u79cd\u73b0\u4ee3\u7684\u3001\u65e0\u524d\u5411\u6c42\u89e3\u5668\u7684\u5206\u644a\u53d8\u5206\u63a8\u65ad\u65b9\u6cd5\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u501f\u9274\u4e86\u6d41\u4f53\u529b\u5b66\u4e2d\u5927\u6da1\u6a21\u62df\u7b49\u7269\u7406\u591a\u5c3a\u5ea6\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u3002", "result": "\u6570\u503c\u7814\u7a76\u8868\u660e\uff0c\u6240\u5b66\u4e60\u7684\u591a\u5c3a\u5ea6\u6a21\u578b\u5728\u76f8\u540c\u5206\u8fa8\u7387\u4e0b\uff0c\u6bd4\u76f4\u63a5\u6570\u503c\u6a21\u62df\u548c\u95ed\u5408\u578b\u6a21\u578b\u5177\u6709\u66f4\u4f18\u8d8a\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u9a71\u52a8\u968f\u673a\u591a\u5c3a\u5ea6\u5efa\u6a21\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u52a8\u529b\u7cfb\u7edf\u7684\u9ad8\u7ef4\u8ba1\u7b97\u6311\u6218\uff0c\u5e76\u5728\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2506.23168", "pdf": "https://arxiv.org/pdf/2506.23168", "abs": "https://arxiv.org/abs/2506.23168", "authors": ["Mohammad Abdulla", "Tobias Hille", "Dominik D\u00fcrrschnabel", "Gerd Stumme"], "title": "Rises for Measuring Local Distributivity in Lattices", "categories": ["cs.AI", "cs.DM", "math.CO", "math.RA", "06B99", "G.2.1"], "comment": "16 pages, 2 tables, 5 figures, International Joint Conference on\n  Conceptual Knowledge Structures", "summary": "Distributivity is a well-established and extensively studied notion in\nlattice theory. In the context of data analysis, particularly within Formal\nConcept Analysis (FCA), lattices are often observed to exhibit a high degree of\ndistributivity. However, no standardized measure exists to quantify this\nproperty. In this paper, we introduce the notion of rises in (concept) lattices\nas a means to assess distributivity. Rises capture how the number of attributes\nor objects in covering concepts change within the concept lattice. We show that\na lattice is distributive if and only if no non-unit rises occur. Furthermore,\nwe relate rises to the classical notion of meet- and join distributivity. We\nobserve that concept lattices from real-world data are to a high degree\njoin-distributive, but much less meet-distributive. We additionally study how\njoin-distributivity manifests on the level of ordered sets.", "AI": {"tldr": "\u9488\u5bf9\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\u4e2d\u683c\u7684\u5206\u914d\u6027\u7f3a\u4e4f\u91cf\u5316\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u672c\u6587\u5f15\u5165\u4e86\u201c\u4e0a\u5347\u201d\u6982\u5ff5\u6765\u8861\u91cf\u5206\u914d\u6027\uff0c\u8bc1\u660e\u4e86\u683c\u7684\u5206\u914d\u6027\u4e0e\u201c\u975e\u5355\u4f4d\u4e0a\u5347\u201d\u7684\u7f3a\u5931\u7b49\u4ef7\uff0c\u5e76\u5206\u6790\u4e86\u771f\u5b9e\u6570\u636e\u4e2d\u6982\u5ff5\u683c\u7684\u4ea4/\u5e76\u5206\u914d\u6027\u3002", "motivation": "\u5728\u6570\u636e\u5206\u6790\uff08\u7279\u522b\u662f\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff0cFCA\uff09\u4e2d\uff0c\u683c\u5e38\u8868\u73b0\u51fa\u9ad8\u5ea6\u7684\u5206\u914d\u6027\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u91cf\u5316\u6b64\u6027\u8d28\u7684\u6807\u51c6\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e86\u201c\u4e0a\u5347\uff08rises\uff09\u201d\u7684\u6982\u5ff5\u6765\u8bc4\u4f30\uff08\u6982\u5ff5\uff09\u683c\u7684\u5206\u914d\u6027\uff0c\u8be5\u6982\u5ff5\u6355\u83b7\u4e86\u8986\u76d6\u6982\u5ff5\u4e2d\u5c5e\u6027\u6216\u5bf9\u8c61\u6570\u91cf\u7684\u53d8\u5316\u3002\u5c06\u201c\u4e0a\u5347\u201d\u4e0e\u7ecf\u5178\u7684\u4ea4\uff08meet-\uff09\u548c\u5e76\uff08join-\uff09\u5206\u914d\u6027\u5173\u8054\u8d77\u6765\uff0c\u5e76\u7814\u7a76\u4e86\u5e76\u5206\u914d\u6027\u5728\u6709\u5e8f\u96c6\u5c42\u9762\u7684\u8868\u73b0\u3002", "result": "\u8bc1\u660e\u4e86\u4e00\u4e2a\u683c\u662f\u5206\u914d\u7684\u5f53\u4e14\u4ec5\u5f53\u6ca1\u6709\u975e\u5355\u4f4d\u4e0a\u5347\u53d1\u751f\u3002\u89c2\u5bdf\u5230\u6765\u81ea\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u6982\u5ff5\u683c\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u662f\u5e76\u5206\u914d\u7684\uff0c\u4f46\u4ea4\u5206\u914d\u6027\u5219\u4f4e\u5f97\u591a\u3002", "conclusion": "\u201c\u4e0a\u5347\u201d\u7684\u6982\u5ff5\u63d0\u4f9b\u4e86\u4e00\u79cd\u91cf\u5316\u683c\u5206\u914d\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u771f\u5b9e\u4e16\u754c\u6982\u5ff5\u683c\u5728\u5e76\u5206\u914d\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u800c\u5728\u4ea4\u5206\u914d\u6027\u65b9\u9762\u8868\u73b0\u8f83\u5f31\u7684\u7279\u70b9\u3002"}}
{"id": "2506.22556", "pdf": "https://arxiv.org/pdf/2506.22556", "abs": "https://arxiv.org/abs/2506.22556", "authors": ["Markus Juvonen", "Samuli Siltanen"], "title": "Recomposed realities: animating still images via patch clustering and randomness", "categories": ["cs.CV", "eess.IV"], "comment": "22 pages, 19 figures", "summary": "We present a patch-based image reconstruction and animation method that uses\nexisting image data to bring still images to life through motion. Image patches\nfrom curated datasets are grouped using k-means clustering and a new target\nimage is reconstructed by matching and randomly sampling from these clusters.\nThis approach emphasizes reinterpretation over replication, allowing the source\nand target domains to differ conceptually while sharing local structures.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u5757\u7684\u56fe\u50cf\u91cd\u5efa\u548c\u52a8\u753b\u65b9\u6cd5\uff0c\u65e8\u5728\u4f7f\u9759\u6001\u56fe\u50cf\u901a\u8fc7\u8fd0\u52a8\u53d8\u5f97\u751f\u52a8\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u5229\u7528\u73b0\u6709\u56fe\u50cf\u6570\u636e\uff0c\u4f7f\u9759\u6001\u56fe\u50cf\u901a\u8fc7\u8fd0\u52a8\u201c\u6d3b\u5316\u201d\uff0c\u5e76\u5f3a\u8c03\u91cd\u65b0\u8be0\u91ca\u800c\u975e\u7b80\u5355\u590d\u5236\uff0c\u5141\u8bb8\u6e90\u57df\u548c\u76ee\u6807\u57df\u5728\u6982\u5ff5\u4e0a\u4e0d\u540c\u4f46\u5171\u4eab\u5c40\u90e8\u7ed3\u6784\u3002", "method": "\u4f7f\u7528K-means\u805a\u7c7b\u5bf9\u7cbe\u9009\u6570\u636e\u96c6\u4e2d\u7684\u56fe\u50cf\u5757\u8fdb\u884c\u5206\u7ec4\uff0c\u7136\u540e\u901a\u8fc7\u5339\u914d\u5e76\u4ece\u8fd9\u4e9b\u805a\u7c7b\u4e2d\u968f\u673a\u91c7\u6837\u6765\u91cd\u5efa\u65b0\u7684\u76ee\u6807\u56fe\u50cf\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u9759\u6001\u56fe\u50cf\u7684\u52a8\u753b\u5316\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u652f\u6301\u5bf9\u73b0\u6709\u56fe\u50cf\u6570\u636e\u7684\u91cd\u65b0\u8be0\u91ca\uff0c\u5141\u8bb8\u6e90\u548c\u76ee\u6807\u9886\u57df\u5728\u6982\u5ff5\u4e0a\u6709\u6240\u5dee\u5f02\uff0c\u540c\u65f6\u4fdd\u7559\u5c40\u90e8\u7ed3\u6784\u76f8\u4f3c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u56fe\u50cf\u91cd\u5efa\u548c\u52a8\u753b\u65b9\u6848\uff0c\u80fd\u591f\u5c06\u9759\u6001\u56fe\u50cf\u901a\u8fc7\u8fd0\u52a8\u53d8\u5f97\u751f\u52a8\uff0c\u5176\u4f18\u52bf\u5728\u4e8e\u5f3a\u8c03\u91cd\u65b0\u8be0\u91ca\u548c\u5c40\u90e8\u7ed3\u6784\u5171\u4eab\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u6982\u5ff5\u4e0a\u4e0d\u540c\u7684\u9886\u57df\u95f4\u7684\u8f6c\u5316\u3002"}}
{"id": "2506.22813", "pdf": "https://arxiv.org/pdf/2506.22813", "abs": "https://arxiv.org/abs/2506.22813", "authors": ["Zhuojun Ding", "Wei Wei", "Chenghao Fan"], "title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Supervised fine-tuning (SFT) is widely used to align large language models\n(LLMs) with information extraction (IE) tasks, such as named entity recognition\n(NER). However, annotating such fine-grained labels and training\ndomain-specific models is costly. Existing works typically train a unified\nmodel across multiple domains, but such approaches lack adaptation and\nscalability since not all training data benefits target domains and scaling\ntrained models remains challenging. We propose the SaM framework, which\ndynamically Selects and Merges expert models at inference time. Specifically,\nfor a target domain, we select domain-specific experts pre-trained on existing\ndomains based on (i) domain similarity to the target domain and (ii)\nperformance on sampled instances, respectively. The experts are then merged to\ncreate task-specific models optimized for the target domain. By dynamically\nmerging experts beneficial to target domains, we improve generalization across\nvarious domains without extra training. Additionally, experts can be added or\nremoved conveniently, leading to great scalability. Extensive experiments on\nmultiple benchmarks demonstrate our framework's effectiveness, which\noutperforms the unified model by an average of 10%. We further provide insights\ninto potential improvements, practical experience, and extensions of our\nframework.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SaM\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u52a8\u6001\u9009\u62e9\u548c\u5408\u5e76\u9884\u8bad\u7ec3\u7684\u9886\u57df\u4e13\u5bb6\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u8de8\u9886\u57df\u4fe1\u606f\u62bd\u53d6\uff08IE\uff09\u4efb\u52a1\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5fae\u8c03\u7684\u6210\u672c\u548c\u7edf\u4e00\u6a21\u578b\u7f3a\u4e4f\u9002\u5e94\u6027\u53ca\u53ef\u4f38\u7f29\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5728\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u73b0\u6709\u8de8\u591a\u9886\u57df\u7684\u7edf\u4e00\u6a21\u578b\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u53ef\u4f38\u7f29\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u6240\u6709\u8bad\u7ec3\u6570\u636e\u5e76\u96be\u4ee5\u6269\u5c55\u3002", "method": "SaM\u6846\u67b6\u5728\u63a8\u7406\u65f6\u52a8\u6001\u9009\u62e9\u548c\u5408\u5e76\u4e13\u5bb6\u6a21\u578b\u3002\u5b83\u57fa\u4e8e\u9886\u57df\u76f8\u4f3c\u6027\u548c\u5728\u91c7\u6837\u5b9e\u4f8b\u4e0a\u7684\u6027\u80fd\u6765\u9009\u62e9\u9884\u8bad\u7ec3\u7684\u9886\u57df\u4e13\u5bb6\u6a21\u578b\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u4e13\u5bb6\u6a21\u578b\u5408\u5e76\u4ee5\u521b\u5efa\u9488\u5bf9\u76ee\u6807\u4efb\u52a1\u4f18\u5316\u7684\u6a21\u578b\u3002", "result": "SaM\u6846\u67b6\u5728\u4e0d\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u51fa\u8272\u7684\u53ef\u4f38\u7f29\u6027\u3002\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5176\u6027\u80fd\u5e73\u5747\u4f18\u4e8e\u7edf\u4e00\u6a21\u578b10%\u3002", "conclusion": "SaM\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5408\u5e76\u5bf9\u76ee\u6807\u9886\u57df\u6709\u76ca\u7684\u4e13\u5bb6\u6a21\u578b\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u9886\u57df\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u4f38\u7f29\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23493", "pdf": "https://arxiv.org/pdf/2506.23493", "abs": "https://arxiv.org/abs/2506.23493", "authors": ["Jiahui Li", "Geng Sun", "Xiaoyu Sun", "Fang Mei", "Jingjing Wang", "Xiangwang Hou", "Daxin Tian", "Victor C. M. Leung"], "title": "Securing the Sky: Integrated Satellite-UAV Physical Layer Security for Low-Altitude Wireless Networks", "categories": ["cs.NI", "eess.SP"], "comment": "This paper has been submitted to IEEE Wireless Communications", "summary": "Low-altitude wireless networks (LAWNs) have garnered significant attention in\nthe forthcoming 6G networks. In LAWNs, satellites with wide coverage and\nunmanned aerial vehicles (UAVs) with flexible mobility can complement each\nother to form integrated satellite-UAV networks, providing ubiquitous and\nhigh-speed connectivity for low-altitude operations. However, the higher\nline-of-sight probability in low-altitude airspace increases transmission\nsecurity concerns. In this work, we present a collaborative beamforming-based\nphysical layer security scheme for LAWNs. We introduce the fundamental aspects\nof integrated satellite-UAV networks, physical layer security, UAV swarms, and\ncollaborative beamforming for LAWN applications. Following this, we highlight\nseveral opportunities for collaborative UAV swarm secure applications enabled\nby satellite networks, including achieving physical layer security in scenarios\ninvolving data dissemination, data relay, eavesdropper collusion, and imperfect\neavesdropper information. Next, we detail two case studies: a secure relay\nsystem and a two-way aerial secure communication framework specifically\ndesigned for LAWN environments. Simulation results demonstrate that these\nphysical layer security schemes are effective and beneficial for secure\nlow-altitude wireless communications. A short practicality analysis shows that\nthe proposed method is applicable to LAWN scenarios. Finally, we discuss\ncurrent challenges and future research directions for enhancing security in\nLAWNs.", "AI": {"tldr": "\u9488\u5bf9\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\uff08LAWNs\uff09\u5728\u536b\u661f-\u65e0\u4eba\u673a\u96c6\u6210\u7f51\u7edc\u4e2d\u9762\u4e34\u7684\u4f20\u8f93\u5b89\u5168\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u534f\u4f5c\u6ce2\u675f\u6210\u5f62\u7684\u7269\u7406\u5c42\u5b89\u5168\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\uff08LAWNs\uff09\u4f5c\u4e3a6G\u7f51\u7edc\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5176\u536b\u661f-\u65e0\u4eba\u673a\u96c6\u6210\u7f51\u7edc\u80fd\u63d0\u4f9b\u5e7f\u6cdb\u8fde\u63a5\u3002\u7136\u800c\uff0c\u4f4e\u7a7a\u7a7a\u57df\u8f83\u9ad8\u7684\u89c6\u8ddd\uff08LoS\uff09\u6982\u7387\u663e\u8457\u589e\u52a0\u4e86\u4f20\u8f93\u7684\u5b89\u5168\u9690\u60a3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u534f\u4f5c\u6ce2\u675f\u6210\u5f62\u7684\u7269\u7406\u5c42\u5b89\u5168\u65b9\u6848\uff0c\u7528\u4e8e\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u3002\u8be5\u65b9\u6848\u5229\u7528\u96c6\u6210\u536b\u661f-\u65e0\u4eba\u673a\u7f51\u7edc\u548c\u65e0\u4eba\u673a\u8702\u7fa4\uff0c\u4ee5\u5b9e\u73b0\u6570\u636e\u5206\u53d1\u3001\u6570\u636e\u4e2d\u7ee7\u3001\u7a83\u542c\u8005\u5171\u8c0b\u4ee5\u53ca\u7a83\u542c\u8005\u4fe1\u606f\u4e0d\u5b8c\u5584\u7b49\u573a\u666f\u4e0b\u7684\u7269\u7406\u5c42\u5b89\u5168\u3002\u5177\u4f53\u6848\u4f8b\u7814\u7a76\u5305\u62ec\u4e00\u4e2a\u5b89\u5168\u4e2d\u7ee7\u7cfb\u7edf\u548c\u4e00\u4e2a\u53cc\u5411\u7a7a\u4e2d\u5b89\u5168\u901a\u4fe1\u6846\u67b6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7269\u7406\u5c42\u5b89\u5168\u65b9\u6848\u5bf9\u4f4e\u7a7a\u65e0\u7ebf\u901a\u4fe1\u662f\u6709\u6548\u4e14\u6709\u76ca\u7684\u3002\u5b9e\u8df5\u53ef\u884c\u6027\u5206\u6790\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u573a\u666f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u534f\u4f5c\u6ce2\u675f\u6210\u5f62\u7684\u7269\u7406\u5c42\u5b89\u5168\u65b9\u6848\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u4f4e\u7a7a\u65e0\u7ebf\u901a\u4fe1\u7684\u5b89\u5168\u6027\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u6027\u3002\u6587\u7ae0\u8fd8\u8ba8\u8bba\u4e86\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.22668", "pdf": "https://arxiv.org/pdf/2506.22668", "abs": "https://arxiv.org/abs/2506.22668", "authors": ["Selahattin Akkas", "Aditya Devarakonda", "Ariful Azad"], "title": "DistShap: Scalable GNN Explanations with Distributed Shapley Values", "categories": ["cs.LG", "cs.AI", "cs.DC", "stat.ML"], "comment": "12 pages", "summary": "With the growing adoption of graph neural networks (GNNs), explaining their\npredictions has become increasingly important. However, attributing predictions\nto specific edges or features remains computationally expensive. For example,\nclassifying a node with 100 neighbors using a 3-layer GNN may involve\nidentifying important edges from millions of candidates contributing to the\nprediction. To address this challenge, we propose DistShap, a parallel\nalgorithm that distributes Shapley value-based explanations across multiple\nGPUs. DistShap operates by sampling subgraphs in a distributed setting,\nexecuting GNN inference in parallel across GPUs, and solving a distributed\nleast squares problem to compute edge importance scores. DistShap outperforms\nmost existing GNN explanation methods in accuracy and is the first to scale to\nGNN models with millions of features by using up to 128 GPUs on the NERSC\nPerlmutter supercomputer.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc(GNN)\u89e3\u91ca\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86DistShap\uff0c\u4e00\u79cd\u57fa\u4e8eShapley\u503c\u7684\u5e76\u884c\u7b97\u6cd5\uff0c\u53ef\u5206\u5e03\u5f0f\u5730\u89e3\u91caGNN\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u5e76\u9996\u6b21\u5b9e\u73b0\u5bf9\u767e\u4e07\u7ea7\u7279\u5f81\u6a21\u578b\u7684\u6269\u5c55\u6027\u3002", "motivation": "\u968f\u7740\u56fe\u795e\u7ecf\u7f51\u7edc(GNNs)\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u89e3\u91ca\u5176\u9884\u6d4b\u53d8\u5f97\u6108\u53d1\u91cd\u8981\u3002\u7136\u800c\uff0c\u5c06\u9884\u6d4b\u5f52\u56e0\u4e8e\u5177\u4f53\u8fb9\u6216\u7279\u5f81\u7684\u8ba1\u7b97\u6210\u672c\u6781\u9ad8\uff0c\u4f8b\u5982\u5bf9\u62e5\u6709\u6570\u767e\u4e07\u5019\u9009\u8fb9\u8282\u70b9\u8fdb\u884cGNN\u89e3\u91ca\u3002", "method": "\u672c\u6587\u63d0\u51faDistShap\uff0c\u4e00\u79cd\u5e76\u884c\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06Shapley\u503c\u89e3\u91ca\u5206\u5e03\u5230\u591a\u4e2aGPU\u4e0a\u5b9e\u73b0\u3002\u5176\u5de5\u4f5c\u539f\u7406\u5305\u62ec\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e0b\u91c7\u6837\u5b50\u56fe\uff0c\u5728GPU\u4e0a\u5e76\u884c\u6267\u884cGNN\u63a8\u7406\uff0c\u4ee5\u53ca\u89e3\u51b3\u4e00\u4e2a\u5206\u5e03\u5f0f\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u4ee5\u8ba1\u7b97\u8fb9\u7684\u91cd\u8981\u6027\u5206\u6570\u3002", "result": "DistShap\u5728\u51c6\u786e\u6027\u4e0a\u8d85\u8d8a\u4e86\u5927\u591a\u6570\u73b0\u6709GNN\u89e3\u91ca\u65b9\u6cd5\uff0c\u5e76\u4e14\u662f\u9996\u4e2a\u80fd\u6269\u5c55\u5230\u62e5\u6709\u767e\u4e07\u7ea7\u7279\u5f81\u7684GNN\u6a21\u578b\uff08\u5229\u7528NERSC Perlmutter\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u7684\u591a\u8fbe128\u4e2aGPU\uff09\u7684\u89e3\u91ca\u65b9\u6cd5\u3002", "conclusion": "DistShap\u6709\u6548\u89e3\u51b3\u4e86GNN\u89e3\u91ca\u7684\u8ba1\u7b97\u6548\u7387\u548c\u6269\u5c55\u6027\u6311\u6218\uff0c\u4e3a\u5927\u89c4\u6a21GNN\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u9884\u6d4b\u89e3\u91ca\u80fd\u529b\u3002"}}
{"id": "2506.23273", "pdf": "https://arxiv.org/pdf/2506.23273", "abs": "https://arxiv.org/abs/2506.23273", "authors": ["Quang Hung Nguyen", "Phuong Anh Trinh", "Phan Quoc Hung Mai", "Tuan Phong Trinh"], "title": "FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis", "categories": ["cs.AI"], "comment": null, "summary": "Despite the advancements of large language models, text2sql still faces many\nchallenges, particularly with complex and domain-specific queries. In finance,\ndatabase designs and financial reporting layouts vary widely between financial\nentities and countries, making text2sql even more challenging. We present\nFinStat2SQL, a lightweight text2sql pipeline enabling natural language queries\nover financial statements. Tailored to local standards like VAS, it combines\nlarge and small language models in a multi-agent setup for entity extraction,\nSQL generation, and self-correction. We build a domain-specific database and\nevaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves\n61.33\\% accuracy with sub-4-second response times on consumer hardware,\noutperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient\nsolution for financial analysis, making AI-powered querying accessible to\nVietnamese enterprises.", "AI": {"tldr": "FinStat2SQL\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7text2sql\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7ed3\u5408\u5927\u5c0f\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u5bf9\u8d22\u52a1\u62a5\u8868\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u3002\u5b83\u4e13\u95e8\u4e3a\u8d8a\u5357\u672c\u5730\u6807\u51c6\u4f18\u5316\uff0c\u5e76\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8868\u73b0\u4f18\u4e8eGPT-4o-mini\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u91d1\u878d\u5206\u6790\u65b9\u6848\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u6b65\u663e\u8457\uff0c\u4f46text2sql\u5728\u5904\u7406\u590d\u6742\u548c\u9886\u57df\u7279\u5b9a\u67e5\u8be2\u65f6\u4ecd\u9762\u4e34\u6311\u6218\u3002\u91d1\u878d\u9886\u57df\u7684\u6570\u636e\u5e93\u8bbe\u8ba1\u548c\u62a5\u544a\u683c\u5f0f\u56e0\u5b9e\u4f53\u548c\u56fd\u5bb6\u800c\u5f02\uff0c\u4f7f\u5f97text2sql\u5e94\u7528\u66f4\u4e3a\u56f0\u96be\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86FinStat2SQL\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684text2sql\u6d41\u6c34\u7ebf\u3002\u5b83\u91c7\u7528\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\uff0c\u7ed3\u5408\u5927\u578b\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u4f53\u63d0\u53d6\u3001SQL\u751f\u6210\u548c\u81ea\u6821\u6b63\u3002\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86\u4e00\u4e2a\u9886\u57df\u7279\u5b9a\u6570\u636e\u5e93\uff0c\u5e76\u5728\u5408\u6210\u7684\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6a21\u578b\u3002", "result": "\u4e00\u4e2a\u7ecf\u8fc7\u5fae\u8c03\u76847B\u6a21\u578b\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e8661.33%\u7684\u51c6\u786e\u7387\uff0c\u54cd\u5e94\u65f6\u95f4\u4f4e\u4e8e4\u79d2\uff0c\u5e76\u4e14\u6027\u80fd\u4f18\u4e8eGPT-4o-mini\u3002", "conclusion": "FinStat2SQL\u4e3a\u91d1\u878d\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u5f97\u8d8a\u5357\u4f01\u4e1a\u80fd\u591f\u4fbf\u6377\u5730\u4f7f\u7528AI\u9a71\u52a8\u7684\u67e5\u8be2\u529f\u80fd\u3002"}}
{"id": "2506.22562", "pdf": "https://arxiv.org/pdf/2506.22562", "abs": "https://arxiv.org/abs/2506.22562", "authors": ["Abhineet Singh", "Nilanjan Ray"], "title": "Improving Token-based Object Detection with Video", "categories": ["cs.CV"], "comment": "Under review for publication in IEEE Access", "summary": "This paper improves upon the Pix2Seq object detector by extending it for\nvideos. In the process, it introduces a new way to perform end-to-end video\nobject detection that improves upon existing video detectors in two key ways.\nFirst, by representing objects as variable-length sequences of discrete tokens,\nwe can succinctly represent widely varying numbers of video objects, with\ndiverse shapes and locations, without having to inject any localization cues in\nthe training process. This eliminates the need to sample the space of all\npossible boxes that constrains conventional detectors and thus solves the dual\nproblems of loss sparsity during training and heuristics-based postprocessing\nduring inference. Second, it conceptualizes and outputs the video objects as\nfully integrated and indivisible 3D boxes or tracklets instead of generating\nimage-specific 2D boxes and linking these boxes together to construct the video\nobject, as done in most conventional detectors. This allows it to scale\neffortlessly with available computational resources by simply increasing the\nlength of the video subsequence that the network takes as input, even\ngeneralizing to multi-object tracking if the subsequence can span the entire\nvideo. We compare our video detector with the baseline Pix2Seq static detector\non several datasets and demonstrate consistent improvement, although with\nstrong signs of being bottlenecked by our limited computational resources. We\nalso compare it with several video detectors on UA-DETRAC to show that it is\ncompetitive with the current state of the art even with the computational\nbottleneck. We make our code and models publicly available.", "AI": {"tldr": "\u672c\u6587\u5c06Pix2Seq\u6269\u5c55\u5230\u89c6\u9891\u9886\u57df\uff0c\u63d0\u51fa\u4e00\u79cd\u7aef\u5230\u7aef\u89c6\u9891\u76ee\u6807\u68c0\u6d4b\u65b0\u65b9\u6cd5\u3002\u901a\u8fc7\u5c06\u89c6\u9891\u5bf9\u8c61\u8868\u793a\u4e3a\u53ef\u53d8\u957f\u5ea6\u7684\u79bb\u6563\u4ee4\u724c\u5e8f\u5217\u5e76\u76f4\u63a5\u8f93\u51fa\u4e3a3D\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u75db\u70b9\uff0c\u5e76\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "motivation": "\u6539\u8fdbPix2Seq\u4ee5\u9002\u5e94\u89c6\u9891\u76ee\u6807\u68c0\u6d4b\uff0c\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u68c0\u6d4b\u5668\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u635f\u5931\u7a00\u758f\u6027\u3001\u57fa\u4e8e\u542f\u53d1\u5f0f\u540e\u5904\u7406\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u4f20\u7edf2D\u6846\u94fe\u63a5\u6784\u5efa\u89c6\u9891\u5bf9\u8c61\u7684\u4f4e\u6548\u548c\u590d\u6742\u6027\u95ee\u9898\u3002", "method": "\u5c06\u5bf9\u8c61\u8868\u793a\u4e3a\u53ef\u53d8\u957f\u5ea6\u7684\u79bb\u6563\u4ee4\u724c\u5e8f\u5217\uff0c\u65e0\u9700\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u5b9a\u4f4d\u7ebf\u7d22\uff0c\u4ece\u800c\u907f\u514d\u4e86\u8fb9\u754c\u6846\u91c7\u6837\u5e76\u89e3\u51b3\u4e86\u635f\u5931\u7a00\u758f\u6027\u548c\u542f\u53d1\u5f0f\u540e\u5904\u7406\u95ee\u9898\u3002\u5c06\u89c6\u9891\u5bf9\u8c61\u6982\u5ff5\u5316\u5e76\u76f4\u63a5\u8f93\u51fa\u4e3a\u5b8c\u5168\u6574\u5408\u4e14\u4e0d\u53ef\u5206\u5272\u76843D\u8fb9\u754c\u6846\u6216\u8f68\u8ff9\uff08tracklets\uff09\uff0c\u800c\u975e\u901a\u8fc7\u94fe\u63a5\u56fe\u50cf\u7279\u5b9a\u76842D\u6846\u3002\u8be5\u65b9\u6cd5\u53ef\u901a\u8fc7\u589e\u52a0\u8f93\u5165\u89c6\u9891\u5b50\u5e8f\u5217\u7684\u957f\u5ea6\u5b9e\u73b0\u8ba1\u7b97\u8d44\u6e90\u7684\u7075\u6d3b\u6269\u5c55\u3002", "result": "\u4e0e\u57fa\u7ebfPix2Seq\u9759\u6001\u68c0\u6d4b\u5668\u76f8\u6bd4\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u6301\u7eed\u6539\u8fdb\u3002\u5728UA-DETRAC\u6570\u636e\u96c6\u4e0a\u4e0e\u73b0\u6709\u89c6\u9891\u68c0\u6d4b\u5668\u76f8\u6bd4\uff0c\u5373\u4f7f\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u8868\u73b0\u51fa\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u6c34\u5e73\u76f8\u5f53\u7684\u7ade\u4e89\u529b\u3002\u7814\u7a76\u8868\u660e\u6027\u80fd\u53d7\u9650\u4e8e\u8ba1\u7b97\u8d44\u6e90\u74f6\u9888\u3002", "conclusion": "\u8be5\u8bba\u6587\u6210\u529f\u5c06Pix2Seq\u6269\u5c55\u81f3\u89c6\u9891\u9886\u57df\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u6709\u6548\u7684\u7aef\u5230\u7aef\u89c6\u9891\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\u3002\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u8c61\u8868\u793a\u548c3D\u8f68\u8ff9\u8f93\u51fa\u65b9\u5f0f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5173\u952e\u7f3a\u9677\u3002\u5c3d\u7ba1\u5b58\u5728\u8ba1\u7b97\u74f6\u9888\uff0c\u8be5\u65b9\u6cd5\u4ecd\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6f5c\u529b\u548c\u7ade\u4e89\u529b\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u591a\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\u3002"}}
{"id": "2506.22846", "pdf": "https://arxiv.org/pdf/2506.22846", "abs": "https://arxiv.org/abs/2506.22846", "authors": ["Duygu Altinok"], "title": "Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "summary": "End-to-end (E2E) automatic speech recognition (ASR) systems have\nrevolutionized the field by integrating all components into a single neural\nnetwork, with attention-based encoder-decoder models achieving state-of-the-art\nperformance. However, their autoregressive decoding process limits inference\nspeed, making them unsuitable for real-time applications. In contrast,\nCTC-based models offer faster, non-autoregressive decoding but struggle to\nmodel linguistic dependencies effectively. Addressing this challenge, we\npropose a novel auxiliary loss framework called Language-Aware Intermediate\nLoss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large\nlanguage models (LLMs). By attaching connector layers to intermediate encoder\nlayers, LAIL maps outputs to the embedding space of an LLM and computes a\ncausal language modeling loss during training. This approach enhances\nlinguistic modeling while preserving the computational efficiency of CTC\ndecoding. Using the Conformer architecture and various LLaMA models, we\ndemonstrate significant improvements in Word Error Rate (WER) on the\nLibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance\nfor CTC-based ASR with minimal computational overhead.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aLAIL\u7684\u8f85\u52a9\u635f\u5931\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bed\u8a00\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86CTC-based\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u7684\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u548c\u8bc6\u522b\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5176\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u81ea\u56de\u5f52ASR\u7cfb\u7edf\u867d\u7136\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u4e0d\u9002\u5408\u5b9e\u65f6\u5e94\u7528\uff1b\u800cCTC-based ASR\u7cfb\u7edf\u89e3\u7801\u901f\u5ea6\u5feb\uff0c\u4f46\u5728\u5efa\u6a21\u8bed\u8a00\u4f9d\u8d56\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5982\u4f55\u5728\u4e0d\u727a\u7272CTC-based ASR\u8ba1\u7b97\u6548\u7387\u7684\u524d\u63d0\u4e0b\uff0c\u589e\u5f3a\u5176\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u8bed\u8a00\u611f\u77e5\u4e2d\u95f4\u635f\u5931\uff08LAIL\uff09\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u4e2d\u95f4\u7f16\u7801\u5668\u5c42\u8fde\u63a5\u8fde\u63a5\u5668\u5c42\uff0c\u5c06\u8f93\u51fa\u6620\u5c04\u5230LLM\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8ba1\u7b97\u56e0\u679c\u8bed\u8a00\u5efa\u6a21\u635f\u5931\u3002\u6b64\u4e3e\u65e8\u5728\u5c06LLM\u7684\u8bed\u8a00\u77e5\u8bc6\u878d\u5165\u5230CTC-based ASR\u7cfb\u7edf\u4e2d\u3002", "result": "\u5728LibriSpeech\u3001TEDLIUM2\u548cWSJ\u6570\u636e\u96c6\u4e0a\uff0c\u7ed3\u5408Conformer\u67b6\u6784\u548c\u591a\u79cdLLaMA\u6a21\u578b\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLAIL\u663e\u8457\u964d\u4f4e\u4e86\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\uff0c\u4f7fCTC-based ASR\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\uff08state-of-the-art\uff09\u7684\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "LAIL\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86CTC-based ASR\u5728\u8bed\u8a00\u5efa\u6a21\u4e0a\u7684\u5f31\u70b9\uff0c\u901a\u8fc7\u5f15\u5165LLM\u7684\u8bed\u8a00\u77e5\u8bc6\uff0c\u5728\u4fdd\u6301\u5176\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u8bc6\u522b\u6027\u80fd\uff0c\u4f7f\u5176\u6210\u4e3a\u5b9e\u65f6ASR\u5e94\u7528\u7684\u6709\u529b\u9009\u62e9\u3002"}}
{"id": "2506.23628", "pdf": "https://arxiv.org/pdf/2506.23628", "abs": "https://arxiv.org/abs/2506.23628", "authors": ["Antonio Ojea"], "title": "The Kubernetes Network Driver Model: A Composable Architecture for High-Performance Networking", "categories": ["cs.NI", "cs.AI"], "comment": "6 pages, 9 figures, submitted to IEEE LCN Special Track on\n  Cloud-AI-Native Mobile Networks Powered by eBPF (CAMe 2025)", "summary": "Traditional Kubernetes networking struggles to meet the escalating demands of\nAI/ML and evolving Telco infrastructure. This paper introduces Kubernetes\nNetwork Drivers (KNDs), a transformative, modular, and declarative architecture\ndesigned to overcome current imperative provisioning and API limitations. KNDs\nintegrate network resource management into Kubernetes' core by utilizing\nDynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements,\nand upcoming OCI Runtime Specification changes. Our DraNet implementation\ndemonstrates declarative attachment of network interfaces, including Remote\nDirect Memory Access (RDMA) devices, significantly boosting high-performance\nAI/ML workloads. This capability enables sophisticated cloud-native\napplications and lays crucial groundwork for future Telco solutions, fostering\na \"galaxy\" of specialized KNDs for enhanced application delivery and reduced\noperational complexity.", "AI": {"tldr": "\u9488\u5bf9Kubernetes\u4f20\u7edf\u7f51\u7edc\u5728AI/ML\u548cTelco\u573a\u666f\u4e0b\u7684\u5c40\u9650\uff0c\u672c\u6587\u63d0\u51faKNDs\uff08Kubernetes\u7f51\u7edc\u9a71\u52a8\uff09\uff0c\u4e00\u79cd\u5229\u7528DRA\u3001NRI\u7b49\u6280\u672f\u5b9e\u73b0\u7f51\u7edc\u8d44\u6e90\u58f0\u660e\u5f0f\u7ba1\u7406\u7684\u6a21\u5757\u5316\u67b6\u6784\u3002\u5176\u5b9e\u73b0DraNet\u5c55\u793a\u4e86RDMA\u8bbe\u5907\u7684\u9ad8\u6027\u80fd\u8fde\u63a5\uff0c\u4e3aAI/ML\u548c\u672a\u6765Telco\u65b9\u6848\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u4f20\u7edf\u7684Kubernetes\u7f51\u7edc\u96be\u4ee5\u6ee1\u8db3AI/ML\u548c\u4e0d\u65ad\u53d1\u5c55\u7684\u7535\u4fe1\u57fa\u7840\u8bbe\u65bd\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\uff0c\u5176\u547d\u4ee4\u5f0f\u8d44\u6e90\u914d\u7f6e\u548cAPI\u9650\u5236\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u8bba\u6587\u5f15\u5165Kubernetes\u7f51\u7edc\u9a71\u52a8\uff08KNDs\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u53d8\u9769\u6027\u3001\u6a21\u5757\u5316\u4e14\u58f0\u660e\u5f0f\u7684\u67b6\u6784\u3002KNDs\u901a\u8fc7\u5229\u7528\u52a8\u6001\u8d44\u6e90\u5206\u914d\uff08DRA\uff09\u3001\u8282\u70b9\u8d44\u6e90\u63a5\u53e3\uff08NRI\uff09\u7684\u6539\u8fdb\u4ee5\u53ca\u5373\u5c06\u5230\u6765\u7684OCI\u8fd0\u884c\u65f6\u89c4\u8303\u53d8\u66f4\uff0c\u5c06\u7f51\u7edc\u8d44\u6e90\u7ba1\u7406\u6df1\u5ea6\u96c6\u6210\u5230Kubernetes\u6838\u5fc3\u4e2d\u3002DraNet\u662f\u5176\u4e00\u4e2a\u5b9e\u73b0\u8303\u4f8b\u3002", "result": "KNDs\u67b6\u6784\u7684DraNet\u5b9e\u73b0\u5c55\u793a\u4e86\u7f51\u7edc\u63a5\u53e3\uff08\u5305\u62ec\u8fdc\u7a0b\u76f4\u63a5\u5185\u5b58\u8bbf\u95eeRDMA\u8bbe\u5907\uff09\u7684\u58f0\u660e\u5f0f\u8fde\u63a5\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u6027\u80fdAI/ML\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6548\u7387\u3002\u8fd9\u4f7f\u5f97\u590d\u6742\u7684\u4e91\u539f\u751f\u5e94\u7528\u6210\u4e3a\u53ef\u80fd\u3002", "conclusion": "KNDs\u4e3a\u672a\u6765\u7684\u7535\u4fe1\u89e3\u51b3\u65b9\u6848\u5960\u5b9a\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u4e00\u4e2a\u7531\u4e13\u4e1a\u5316KNDs\u7ec4\u6210\u7684\u201c\u661f\u7cfb\u201d\uff0c\u4ece\u800c\u589e\u5f3a\u5e94\u7528\u4ea4\u4ed8\u80fd\u529b\u5e76\u964d\u4f4e\u8fd0\u7ef4\u590d\u6742\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524d\u914d\u7f6e\u548cAPI\u7684\u5c40\u9650\u3002"}}
{"id": "2506.22685", "pdf": "https://arxiv.org/pdf/2506.22685", "abs": "https://arxiv.org/abs/2506.22685", "authors": ["Anh Bui", "Trang Vu", "Trung Le", "Junae Kim", "Tamas Abraham", "Rollin Omari", "Amar Kaur", "Dinh Phung"], "title": "Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "In this paper, we investigate the semantic collapsing problem in generative\npersonalization, an under-explored topic where the learned visual concept\n($V^*$) gradually shifts from its original textual meaning and comes to\ndominate other concepts in multi-concept input prompts. This issue not only\nreduces the semantic richness of complex input prompts like \"a photo of $V^*$\nwearing glasses and playing guitar\" into simpler, less contextually rich forms\nsuch as \"a photo of $V^*$\" but also leads to simplified output images that fail\nto capture the intended concept.\n  We identify the root cause as unconstrained optimisation, which allows the\nlearned embedding $V^*$ to drift arbitrarily in the embedding space, both in\ndirection and magnitude. To address this, we propose a simple yet effective\ntraining-free method that adjusts the magnitude and direction of pre-trained\nembedding at inference time, effectively mitigating the semantic collapsing\nproblem. Our method is broadly applicable across different personalization\nmethods and demonstrates significant improvements in text-image alignment in\ndiverse use cases. Our code is anonymously published at\nhttps://anonymous.4open.science/r/Embedding-Adjustment.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u751f\u6210\u5f0f\u4e2a\u6027\u5316\u4e2d\u7684\u8bed\u4e49\u5d29\u6e83\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u5bfc\u81f4\u5b66\u4e60\u5230\u7684\u89c6\u89c9\u6982\u5ff5\uff08V*\uff09\u504f\u79bb\u5176\u539f\u59cb\u542b\u4e49\u5e76\u4e3b\u5bfc\u591a\u6982\u5ff5\u63d0\u793a\u3002\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u5728\u63a8\u7406\u65f6\u8c03\u6574\u9884\u8bad\u7ec3\u5d4c\u5165\u5e45\u5ea6\u548c\u65b9\u5411\u7684\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6b64\u95ee\u9898\u5e76\u663e\u8457\u6539\u5584\u4e86\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u3002", "motivation": "\u751f\u6210\u5f0f\u4e2a\u6027\u5316\u4e2d\u5b58\u5728\u201c\u8bed\u4e49\u5d29\u6e83\u201d\u95ee\u9898\uff0c\u5373\u5b66\u4e60\u5230\u7684\u89c6\u89c9\u6982\u5ff5\uff08V*\uff09\u4f1a\u9010\u6e10\u504f\u79bb\u5176\u539f\u59cb\u6587\u672c\u542b\u4e49\uff0c\u5e76\u5728\u591a\u6982\u5ff5\u8f93\u5165\u63d0\u793a\u4e2d\u8fc7\u5ea6\u4e3b\u5bfc\u5176\u4ed6\u6982\u5ff5\u3002\u8fd9\u5bfc\u81f4\u590d\u6742\u63d0\u793a\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\u964d\u4f4e\uff0c\u5e76\u4ea7\u751f\u65e0\u6cd5\u6355\u6349\u9884\u671f\u6982\u5ff5\u7684\u7b80\u5316\u8f93\u51fa\u56fe\u50cf\u3002\u7814\u7a76\u53d1\u73b0\u5176\u6839\u672c\u539f\u56e0\u5728\u4e8e\u65e0\u7ea6\u675f\u4f18\u5316\u5bfc\u81f4\u5b66\u4e60\u5230\u7684\u5d4c\u5165V*\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4efb\u610f\u6f02\u79fb\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u8c03\u6574\u9884\u8bad\u7ec3\u5d4c\u5165\u7684\u5e45\u5ea6\u548c\u65b9\u5411\uff0c\u4ece\u800c\u6709\u6548\u7f13\u89e3\u8bed\u4e49\u5d29\u6e83\u95ee\u9898\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5e7f\u6cdb\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u5e76\u5728\u5404\u79cd\u4f7f\u7528\u6848\u4f8b\u4e2d\u663e\u8457\u6539\u5584\u4e86\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u5bf9\u9884\u8bad\u7ec3\u5d4c\u5165\u8fdb\u884c\u5e45\u5ea6\u548c\u65b9\u5411\u7684\u8c03\u6574\uff0c\u53ef\u4ee5\u6709\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u5730\u89e3\u51b3\u751f\u6210\u5f0f\u4e2a\u6027\u5316\u4e2d\u7684\u8bed\u4e49\u5d29\u6e83\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u590d\u6742\u63d0\u793a\u7684\u8bed\u4e49\u7406\u89e3\u548c\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2506.23276", "pdf": "https://arxiv.org/pdf/2506.23276", "abs": "https://arxiv.org/abs/2506.23276", "authors": ["David Guzman Piedrahita", "Yongjin Yang", "Mrinmaya Sachan", "Giorgia Ramponi", "Bernhard Sch\u00f6lkopf", "Zhijing Jin"], "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim", "AI": {"tldr": "\u7814\u7a76\u63a2\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7cfb\u7edf\u4e2d\u4ee3\u4ef7\u9ad8\u6602\u7684\u5236\u88c1\u6311\u6218\uff0c\u901a\u8fc7\u516c\u5171\u7269\u54c1\u535a\u5f08\u63ed\u793a\u4e86LLMs\u7684\u56db\u79cd\u5408\u4f5c\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u53d1\u73b0\u63a8\u7406\u578bLLMs\u5728\u5408\u4f5c\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u67d0\u4e9b\u4f20\u7edfLLMs\u5219\u80fd\u4fdd\u6301\u9ad8\u6c34\u5e73\u5408\u4f5c\u3002", "motivation": "\u968f\u7740LLMs\u4f5c\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u90e8\u7f72\u65e5\u76ca\u589e\u591a\uff0c\u7406\u89e3\u5b83\u4eec\u7684\u5408\u4f5c\u548c\u793e\u4f1a\u673a\u5236\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662fLLMs\u5982\u4f55\u5728\u81ea\u8eab\u5229\u76ca\u548c\u96c6\u4f53\u798f\u7949\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4ee5\u786e\u4fdd\u5bf9\u9f50\u3001\u9c81\u68d2\u6027\u548c\u5b89\u5168\u90e8\u7f72\u3002\u672c\u6587\u5177\u4f53\u5173\u6ce8\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u4ee3\u7406\u5fc5\u987b\u51b3\u5b9a\u662f\u5426\u6295\u8d44\u81ea\u8eab\u8d44\u6e90\u6765\u6fc0\u52b1\u5408\u4f5c\u6216\u60e9\u7f5a\u80cc\u53db\u7684\u4ee3\u4ef7\u9ad8\u6602\u7684\u5236\u88c1\u95ee\u9898\u3002", "method": "\u7814\u7a76\u91c7\u7eb3\u4e86\u884c\u4e3a\u7ecf\u6d4e\u5b66\u4e2d\u7684\u4e00\u79cd\u516c\u5171\u7269\u54c1\u535a\u5f08\uff0c\u5e76\u52a0\u5165\u4e86\u5236\u5ea6\u9009\u62e9\uff0c\u4ee5\u89c2\u5bdf\u4e0d\u540cLLMs\u5728\u91cd\u590d\u4ea4\u4e92\u4e2d\u5982\u4f55\u5e94\u5bf9\u793e\u4f1a\u56f0\u5883\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86LLMs\u7684\u56db\u79cd\u4e0d\u540c\u884c\u4e3a\u6a21\u5f0f\uff1a\u4e00\u4e9b\u80fd\u6301\u7eed\u5efa\u7acb\u5e76\u7ef4\u6301\u9ad8\u6c34\u5e73\u5408\u4f5c\uff1b\u4e00\u4e9b\u5728\u53c2\u4e0e\u548c\u8131\u79bb\u4e4b\u95f4\u6ce2\u52a8\uff1b\u4e00\u4e9b\u5408\u4f5c\u884c\u4e3a\u968f\u65f6\u95f4\u9010\u6e10\u4e0b\u964d\uff1b\u53e6\u4e00\u4e9b\u5219\u65e0\u8bba\u7ed3\u679c\u5982\u4f55\u90fd\u9075\u5faa\u56fa\u5b9a\u7b56\u7565\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u63a8\u7406\u578bLLMs\uff08\u5982o1\u7cfb\u5217\uff09\u5728\u5408\u4f5c\u65b9\u9762\u8868\u73b0\u6323\u624e\uff0c\u800c\u67d0\u4e9b\u4f20\u7edfLLMs\u5374\u80fd\u59cb\u7ec8\u5b9e\u73b0\u9ad8\u6c34\u5e73\u5408\u4f5c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u76ee\u524d\u4fa7\u91cd\u4e8e\u589e\u5f3aLLMs\u63a8\u7406\u80fd\u529b\u7684\u6539\u8fdb\u65b9\u6cd5\u4e0d\u4e00\u5b9a\u80fd\u5e26\u6765\u5408\u4f5c\uff0c\u8fd9\u4e3a\u5728\u9700\u8981\u6301\u7eed\u534f\u4f5c\u7684\u73af\u5883\u4e2d\u90e8\u7f72LLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002"}}
{"id": "2506.22567", "pdf": "https://arxiv.org/pdf/2506.22567", "abs": "https://arxiv.org/abs/2506.22567", "authors": ["Shansong Wang", "Zhecheng Jin", "Mingzhe Hu", "Mojtaba Safari", "Feng Zhao", "Chih-Wei Chang", "Richard LJ Qiu", "Justin Roper", "David S. Yu", "Xiaofeng Yang"], "title": "Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "CLIP models pretrained on natural images with billion-scale image-text pairs\nhave demonstrated impressive capabilities in zero-shot classification,\ncross-modal retrieval, and open-ended visual answering. However, transferring\nthis success to biomedicine is hindered by the scarcity of large-scale\nbiomedical image-text corpora, the heterogeneity of image modalities, and\nfragmented data standards across institutions. These limitations hinder the\ndevelopment of a unified and generalizable biomedical foundation model trained\nfrom scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical\nfoundation model developed via Multiple Medical CLIP Knowledge Distillation.\nRather than relying on billion-scale raw data, MMKD-CLIP distills knowledge\nfrom nine state-of-the-art domain-specific or generalist biomedical CLIP\nmodels, each pretrained on millions of biomedical image-text pairs. Our\ntwo-stage training pipeline first performs CLIP-style pretraining on over 2.9\nmillion biomedical image-text pairs from 26 image modalities, followed by\nfeature-level distillation using over 19.2 million feature pairs extracted from\nteacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,\nencompassing over 10.8 million biomedical images across nine image modalities.\nThe evaluation spans six core task types: zero-shot classification, linear\nprobing, cross-modal retrieval, visual question answering, survival prediction,\nand cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models\nwhile demonstrating remarkable robustness and generalization across image\ndomains and task settings. These results underscore that multi-teacher\nknowledge distillation is a scalable and effective paradigm for building\nhigh-performing biomedical foundation models under the practical constraints of\nreal-world data availability.", "AI": {"tldr": "MMKD-CLIP\u901a\u8fc7\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\uff0c\u5728\u6570\u636e\u53d7\u9650\u7684\u751f\u7269\u533b\u5b66\u9886\u57df\u6784\u5efa\u4e86\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u6027\u3002", "motivation": "\u5c06CLIP\u6a21\u578b\u6210\u529f\u5e94\u7528\u4e8e\u751f\u7269\u533b\u5b66\u9886\u57df\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u751f\u7269\u533b\u5b66\u56fe\u6587\u8bed\u6599\u3001\u56fe\u50cf\u6a21\u6001\u591a\u6837\u6027\u4ee5\u53ca\u6570\u636e\u6807\u51c6\u788e\u7247\u5316\uff0c\u8fd9\u963b\u788d\u4e86\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u7edf\u4e00\u4e14\u6cdb\u5316\u7684\u751f\u7269\u533b\u5b66\u57fa\u7840\u6a21\u578b\u3002", "method": "\u672c\u6587\u63d0\u51faMMKD-CLIP\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u533b\u5b66CLIP\u77e5\u8bc6\u84b8\u998f\u5f00\u53d1\u3002\u5b83\u4e0d\u4f9d\u8d56\u6d77\u91cf\u539f\u59cb\u6570\u636e\uff0c\u800c\u662f\u4ece9\u4e2a\u5df2\u9884\u8bad\u7ec3\u5728\u6570\u767e\u4e07\u751f\u7269\u533b\u5b66\u56fe\u6587\u5bf9\u4e0a\u7684\u9886\u57df\u7279\u5b9a\u6216\u901a\u7528\u751f\u7269\u533b\u5b66CLIP\u6a21\u578b\u4e2d\u84b8\u998f\u77e5\u8bc6\u3002\u8bad\u7ec3\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u9996\u5148\u5728290\u591a\u4e07\u751f\u7269\u533b\u5b66\u56fe\u6587\u5bf9\uff08\u6765\u81ea26\u79cd\u56fe\u50cf\u6a21\u6001\uff09\u4e0a\u8fdb\u884cCLIP\u98ce\u683c\u7684\u9884\u8bad\u7ec3\uff1b\u968f\u540e\u4f7f\u7528\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u63d0\u53d6\u76841920\u591a\u4e07\u7279\u5f81\u5bf9\u8fdb\u884c\u7279\u5f81\u7ea7\u84b8\u998f\u3002", "result": "MMKD-CLIP\u572858\u4e2a\u591a\u6837\u5316\u7684\u751f\u7269\u533b\u5b66\u6570\u636e\u96c6\uff08\u6db5\u76d61080\u591a\u4e07\u751f\u7269\u533b\u5b66\u56fe\u50cf\u548c9\u79cd\u56fe\u50cf\u6a21\u6001\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u6d89\u53ca\u516d\u79cd\u6838\u5fc3\u4efb\u52a1\u7c7b\u578b\u3002\u7ed3\u679c\u663e\u793a\uff0cMMKD-CLIP\u59cb\u7ec8\u4f18\u4e8e\u6240\u6709\u6559\u5e08\u6a21\u578b\uff0c\u5e76\u5728\u56fe\u50cf\u57df\u548c\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\uff0c\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u8303\u5f0f\uff0c\u53ef\u4ee5\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u53ef\u7528\u6027\u7684\u5b9e\u9645\u7ea6\u675f\u4e0b\uff0c\u6784\u5efa\u9ad8\u6027\u80fd\u7684\u751f\u7269\u533b\u5b66\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2506.22852", "pdf": "https://arxiv.org/pdf/2506.22852", "abs": "https://arxiv.org/abs/2506.22852", "authors": ["Yucheng Cai", "Yuxuan Wu", "Yi Huang", "Junlan Feng", "Zhijian Ou"], "title": "Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have recently been applied to dialog systems.\nDespite making progress, LLMs are prone to errors in knowledge-intensive\nscenarios. Recently, approaches based on retrieval augmented generation (RAG)\nand agent have emerged to improve the factual accuracy by enhancing the LLMs\nwith knowledge retrieved from external knowledge bases (KBs). This is mostly\nimplemented by prompting the LLMs with instructions, examples and the retrieved\nknowledge. However, LLMs may have difficulty using the retrieved knowledge\neffectively for response generation, because they are not well trained to do\nsuch generation for specific domains. To mitigate this problem, we propose to\nfinetune the LLMs in the RAG-based and agent-based systems with domain-specific\ndata, together with domain-specific external knowledge, which is called\nknowledge augmented finetuning (KAFT). We base our study on the MobileCS2\ndataset, a real-life customer service dialog dataset that features intensive\nknowledge interactions, to systematically compare the prompting and KAFT\ntechniques in the RAG-based and agent-based systems. Experiment results show\nthat KAFT substantially surpasses prompting in both RAG and agent systems,\nparticularly in terms of factual accuracy. To the best of our knowledge, this\npaper represents the first solid empirical work to investigate the KAFT idea.", "AI": {"tldr": "LLM\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u5bf9\u8bdd\u4e2d\u6613\u51fa\u9519\u3002\u672c\u6587\u63d0\u51fa\u77e5\u8bc6\u589e\u5f3a\u5fae\u8c03(KAFT)\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u6570\u636e\u548c\u77e5\u8bc6\u5bf9RAG/Agent\u7cfb\u7edf\u4e2d\u7684LLM\u8fdb\u884c\u5fae\u8c03\uff0c\u5b9e\u9a8c\u8bc1\u660eKAFT\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u63d0\u793a\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u6709\u8fdb\u5c55\uff0c\u4f46\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u573a\u666f\u4e2d\u6613\u72af\u4e8b\u5b9e\u6027\u9519\u8bef\u3002\u73b0\u6709RAG\u548cAgent\u65b9\u6cd5\u901a\u8fc7\u63d0\u793a\u8bcd\u5f15\u5165\u5916\u90e8\u77e5\u8bc6\uff0c\u4f46LLM\u96be\u4ee5\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u77e5\u8bc6\u751f\u6210\u9886\u57df\u7279\u5b9a\u54cd\u5e94\uff0c\u56e0\u5176\u672a\u5145\u5206\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u77e5\u8bc6\u589e\u5f3a\u5fae\u8c03(KAFT)\uff0c\u5373\u5728RAG\u548cAgent\u7cfb\u7edf\u4e2d\uff0c\u5229\u7528\u9886\u57df\u7279\u5b9a\u6570\u636e\u548c\u9886\u57df\u7279\u5b9a\u5916\u90e8\u77e5\u8bc6\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\u3002\u7814\u7a76\u57fa\u4e8eMobileCS2\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86\u63d0\u793a\u548cKAFT\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cKAFT\u5728RAG\u548cAgent\u7cfb\u7edf\u4e2d\u5747\u5927\u5e45\u8d85\u8d8a\u4f20\u7edf\u63d0\u793a\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u65b9\u9762\u3002", "conclusion": "KAFT\u662f\u63d0\u5347LLM\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u5bf9\u8bdd\u7cfb\u7edf\u6027\u80fd\uff08\u7279\u522b\u662f\u4e8b\u5b9e\u51c6\u786e\u6027\uff09\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u4e14\u662f\u9996\u4e2a\u5bf9KAFT\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u7684\u5de5\u4f5c\u3002"}}
{"id": "2506.23640", "pdf": "https://arxiv.org/pdf/2506.23640", "abs": "https://arxiv.org/abs/2506.23640", "authors": ["Ximeng Liu", "Shizhen Zhao", "Xinbing Wang"], "title": "Geminet: Learning the Duality-based Iterative Process for Lightweight Traffic Engineering in Changing Topologies", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "Recently, researchers have explored ML-based Traffic Engineering (TE),\nleveraging neural networks to solve TE problems traditionally addressed by\noptimization. However, existing ML-based TE schemes remain impractical: they\neither fail to handle topology changes or suffer from poor scalability due to\nexcessive computational and memory overhead. To overcome these limitations, we\npropose Geminet, a lightweight and scalable ML-based TE framework that can\nhandle changing topologies. Geminet is built upon two key insights: (i) a\nmethodology that decouples neural networks from topology by learning an\niterative gradient-descent-based adjustment process, as the update rule of\ngradient descent is topology-agnostic, relying only on a few gradient-related\nquantities; (ii) shifting optimization from path-level routing weights to\nedge-level dual variables, reducing memory consumption by leveraging the fact\nthat edges are far fewer than paths. Evaluations on WAN and data center\ndatasets show that Geminet significantly improves scalability. Its neural\nnetwork size is only 0.04% to 7% of existing schemes, while handling topology\nvariations as effectively as HARP, a state-of-the-art ML-based TE approach,\nwithout performance degradation. When trained on large-scale topologies,\nGeminet consumes under 10 GiB of memory, more than eight times less than the\n80-plus GiB required by HARP, while achieving 5.45 times faster convergence\nspeed, demonstrating its potential for large-scale deployment.", "AI": {"tldr": "Geminet\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684ML\u6d41\u91cf\u5de5\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u62d3\u6251\u4e0e\u795e\u7ecf\u7f51\u7edc\u5e76\u4f18\u5316\u8fb9\u7ea7\u53d8\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6848\u5728\u62d3\u6251\u53d8\u5316\u5904\u7406\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6d41\u91cf\u5de5\u7a0b\uff08ML-based TE\uff09\u65b9\u6848\u4e0d\u5207\u5b9e\u9645\uff0c\u5b83\u4eec\u8981\u4e48\u65e0\u6cd5\u5904\u7406\u62d3\u6251\u53d8\u5316\uff0c\u8981\u4e48\u56e0\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u8fc7\u5927\u800c\u53ef\u6269\u5c55\u6027\u5dee\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Geminet\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u5728\u4e8e\uff1a1) \u901a\u8fc7\u5b66\u4e60\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u8fed\u4ee3\u8c03\u6574\u8fc7\u7a0b\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u4e0e\u62d3\u6251\u89e3\u8026\uff1b2) \u5c06\u4f18\u5316\u4ece\u8def\u5f84\u7ea7\u8def\u7531\u6743\u91cd\u8f6c\u5411\u8fb9\u7ea7\u5bf9\u5076\u53d8\u91cf\uff0c\u4ee5\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u3002", "result": "Geminet\u663e\u8457\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u795e\u7ecf\u7f51\u7edc\u89c4\u6a21\u4ec5\u4e3a\u73b0\u6709\u65b9\u6848\u76840.04%\u81f37%\u3002\u5b83\u80fd\u6709\u6548\u5904\u7406\u62d3\u6251\u53d8\u5316\uff0c\u6027\u80fd\u4e0eHARP\u76f8\u5f53\u3002\u5728\u5927\u89c4\u6a21\u62d3\u6251\u4e0b\uff0cGeminet\u5185\u5b58\u6d88\u8017\u4f4e\u4e8e10 GiB\uff08\u6bd4HARP\u5c118\u500d\u4ee5\u4e0a\uff09\uff0c\u6536\u655b\u901f\u5ea6\u5feb5.45\u500d\u3002", "conclusion": "Geminet\u7684\u8f7b\u91cf\u5316\u548c\u9ad8\u6027\u80fd\u7279\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5927\u89c4\u6a21\u6d41\u91cf\u5de5\u7a0b\u90e8\u7f72\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2506.22696", "pdf": "https://arxiv.org/pdf/2506.22696", "abs": "https://arxiv.org/abs/2506.22696", "authors": ["Brian Mak", "Jeffrey Flanigan"], "title": "Residual Matrix Transformers: Scaling the Size of the Residual Stream", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ICML 2025", "summary": "The residual stream acts as a memory bus where transformer layers both store\nand access features (Elhage et al., 2021). We consider changing the mechanism\nfor retrieving and storing information in the residual stream, and replace the\nresidual stream of the transformer with an outer product memory matrix\n(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix\nTransformer (RMT). We find that the RMT enjoys a number of attractive\nproperties: 1) the size of the residual stream can be scaled independently of\ncompute and model size, improving performance, 2) the RMT can achieve the same\nloss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%\nfewer training tokens tokens, and 3) the RMT outperforms the transformer on\ndownstream evaluations. We theoretically analyze the transformer and the RMT,\nand show that the RMT allows for more efficient scaling of the residual stream,\nas well as improved variance propagation properties. Code for this project can\nbe found at https://github.com/bmac3/residual-matrix-transformer.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6b8b\u5dee\u77e9\u9635Transformer (RMT) \u7684\u65b0\u578b\u6a21\u578b\uff0c\u901a\u8fc7\u5c06Transformer\u7684\u6b8b\u5dee\u6d41\u66ff\u6362\u4e3a\u5916\u79ef\u8bb0\u5fc6\u77e9\u9635\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6548\u7387\u548c\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "Transformer\u6a21\u578b\u4e2d\u7684\u6b8b\u5dee\u6d41\u626e\u6f14\u7740\u4fe1\u606f\u5b58\u50a8\u548c\u8bbf\u95ee\u7684\u201c\u8bb0\u5fc6\u603b\u7ebf\u201d\u89d2\u8272\uff0c\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u6539\u53d8\u5e76\u4f18\u5316\u8fd9\u79cd\u4fe1\u606f\u68c0\u7d22\u548c\u5b58\u50a8\u673a\u5236\u3002", "method": "\u5c06Transformer\u7684\u6b8b\u5dee\u6d41\u66ff\u6362\u4e3a\u5916\u79ef\u8bb0\u5fc6\u77e9\u9635\uff0c\u6784\u5efa\u4e86\u6b8b\u5dee\u77e9\u9635Transformer (RMT) \u6a21\u578b\u3002\u540c\u65f6\u5bf9Transformer\u548cRMT\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\u3002", "result": "RMT\u5c55\u73b0\u51fa\u591a\u9879\u4f18\u52bf\uff1a\u6b8b\u5dee\u6d41\u5927\u5c0f\u53ef\u72ec\u7acb\u4e8e\u8ba1\u7b97\u548c\u6a21\u578b\u5927\u5c0f\u8fdb\u884c\u6269\u5c55\u4ee5\u63d0\u5347\u6027\u80fd\uff1b\u5728\u8fbe\u5230\u76f8\u540c\u635f\u5931\u65f6\uff0cRMT\u7684FLOPs\u51cf\u5c1158%\uff0c\u53c2\u6570\u51cf\u5c1125%\uff0c\u8bad\u7ec3tokens\u51cf\u5c1141%\uff1b\u5728\u4e0b\u6e38\u8bc4\u4f30\u4e2dRMT\u8868\u73b0\u4f18\u4e8eTransformer\u3002\u7406\u8bba\u5206\u6790\u663e\u793aRMT\u80fd\u66f4\u9ad8\u6548\u5730\u6269\u5c55\u6b8b\u5dee\u6d41\uff0c\u5e76\u6539\u5584\u65b9\u5dee\u4f20\u64ad\u7279\u6027\u3002", "conclusion": "\u6b8b\u5dee\u77e9\u9635Transformer (RMT) \u901a\u8fc7\u9769\u65b0\u6b8b\u5dee\u6d41\u673a\u5236\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u6548\u7387\u3001\u8d44\u6e90\u6d88\u8017\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edfTransformer\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23306", "pdf": "https://arxiv.org/pdf/2506.23306", "abs": "https://arxiv.org/abs/2506.23306", "authors": ["Qi Liu", "Can Li", "Wanjing Ma"], "title": "GATSim: Urban Mobility Simulation with Generative Agents", "categories": ["cs.AI"], "comment": null, "summary": "Traditional agent-based urban mobility simulations rely on rigid rule-based\nsystems that fail to capture the complexity, adaptability, and behavioral\ndiversity characteristic of human travel decision-making. Recent advances in\nlarge language models and AI agent technology offer opportunities to create\nagents with reasoning capabilities, persistent memory, and adaptive learning\nmechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel\nframework that leverages these advances to create generative agents with rich\nbehavioral characteristics for urban mobility simulation. Unlike conventional\napproaches, GATSim agents possess diverse socioeconomic attributes, individual\nlifestyles, and evolving preferences that shape their mobility decisions\nthrough psychologically-informed memory systems, tool usage capabilities, and\nlifelong learning mechanisms. The main contributions of this study include: (1)\na comprehensive architecture combining an urban mobility foundation model with\nagent cognitive systems and transport simulation environment, (2) a fully\nfunctional prototype implementation, and (3) systematic validation\ndemonstrating that generative agents produce believable travel behaviors.\nThrough designed reflection processes, generative agents in this study can\ntransform specific travel experiences into generalized insights, enabling\nrealistic behavioral adaptation over time with specialized mechanisms for\nactivity planning and real-time reactive behaviors tailored to urban mobility\ncontexts. Experiments show that generative agents perform competitively with\nhuman annotators in mobility scenarios while naturally producing macroscopic\ntraffic evolution patterns. The code for the prototype system is shared at\nhttps://github.com/qiliuchn/gatsim.", "AI": {"tldr": "GATSim\u662f\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548cAI\u667a\u80fd\u4f53\u6280\u672f\uff0c\u6784\u5efa\u5177\u6709\u4e30\u5bcc\u884c\u4e3a\u7279\u5f81\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\uff0c\u7528\u4e8e\u57ce\u5e02\u4ea4\u901a\u6a21\u62df\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u66f4\u771f\u5b9e\u5730\u6a21\u62df\u4eba\u7c7b\u51fa\u884c\u51b3\u7b56\u3002", "motivation": "\u4f20\u7edf\u7684\u57ce\u5e02\u4ea4\u901a\u6a21\u62df\u4f9d\u8d56\u50f5\u5316\u7684\u89c4\u5219\u7cfb\u7edf\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u51fa\u884c\u51b3\u7b56\u7684\u590d\u6742\u6027\u3001\u9002\u5e94\u6027\u548c\u884c\u4e3a\u591a\u6837\u6027\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548cAI\u667a\u80fd\u4f53\u6280\u672f\u4e3a\u521b\u5efa\u5177\u6709\u63a8\u7406\u3001\u8bb0\u5fc6\u548c\u5b66\u4e60\u80fd\u529b\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86GATSim\uff08\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u4ea4\u901a\u6a21\u62df\uff09\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u57ce\u5e02\u4ea4\u901a\u57fa\u7840\u6a21\u578b\u3001\u667a\u80fd\u4f53\u8ba4\u77e5\u7cfb\u7edf\u548c\u4ea4\u901a\u6a21\u62df\u73af\u5883\u3002GATSim\u667a\u80fd\u4f53\u62e5\u6709\u591a\u6837\u5316\u7684\u793e\u4f1a\u7ecf\u6d4e\u5c5e\u6027\u3001\u751f\u6d3b\u65b9\u5f0f\u548c\u504f\u597d\uff0c\u5e76\u901a\u8fc7\u5fc3\u7406\u5b66\u8bb0\u5fc6\u7cfb\u7edf\u3001\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u548c\u7ec8\u8eab\u5b66\u4e60\u673a\u5236\u6765\u505a\u51fa\u51fa\u884c\u51b3\u7b56\u3002\u6b64\u5916\uff0c\u667a\u80fd\u4f53\u901a\u8fc7\u8bbe\u8ba1\u53cd\u601d\u8fc7\u7a0b\uff0c\u5c06\u51fa\u884c\u7ecf\u9a8c\u8f6c\u5316\u4e3a\u901a\u7528\u6d1e\u5bdf\uff0c\u5b9e\u73b0\u884c\u4e3a\u9002\u5e94\u548c\u5b9e\u65f6\u53cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGATSim\u4e2d\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u5728\u51fa\u884c\u573a\u666f\u4e2d\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8005\u76f8\u5f53\uff0c\u5e76\u4e14\u80fd\u591f\u81ea\u7136\u5730\u4ea7\u751f\u5b8f\u89c2\u4ea4\u901a\u6f14\u53d8\u6a21\u5f0f\uff0c\u8bc1\u660e\u4e86\u5176\u751f\u6210\u7684\u51fa\u884c\u884c\u4e3a\u7684\u53ef\u4fe1\u5ea6\u3002", "conclusion": "GATSim\u6846\u67b6\u6210\u529f\u5730\u5229\u7528\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u57ce\u5e02\u4ea4\u901a\u4e2d\u4eba\u7c7b\u590d\u6742\u3001\u9002\u5e94\u6027\u884c\u4e3a\u7684\u66f4\u771f\u5b9e\u6a21\u62df\uff0c\u5f25\u8865\u4e86\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7cfb\u7edf\u7684\u4e0d\u8db3\uff0c\u4e3a\u57ce\u5e02\u4ea4\u901a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002"}}
{"id": "2506.22570", "pdf": "https://arxiv.org/pdf/2506.22570", "abs": "https://arxiv.org/abs/2506.22570", "authors": ["Chee Mei Ling", "Thangarajah Akilan", "Aparna Ravinda Phalke"], "title": "Dual Atrous Separable Convolution for Improving Agricultural Semantic Segmentation", "categories": ["cs.CV"], "comment": "17 pages, 7 figures, 6 tables", "summary": "Agricultural image semantic segmentation is a pivotal component of modern\nagriculture, facilitating accurate visual data analysis to improve crop\nmanagement, optimize resource utilization, and boost overall productivity. This\nstudy proposes an efficient image segmentation method for precision\nagriculture, focusing on accurately delineating farmland anomalies to support\ninformed decision-making and proactive interventions. A novel Dual Atrous\nSeparable Convolution (DAS Conv) module is integrated within the\nDeepLabV3-based segmentation framework. The DAS Conv module is meticulously\ndesigned to achieve an optimal balance between dilation rates and padding size,\nthereby enhancing model performance without compromising efficiency. The study\nalso incorporates a strategic skip connection from an optimal stage in the\nencoder to the decoder to bolster the model's capacity to capture fine-grained\nspatial features. Despite its lower computational complexity, the proposed\nmodel outperforms its baseline and achieves performance comparable to highly\ncomplex transformer-based state-of-the-art (SOTA) models on the Agriculture\nVision benchmark dataset. It achieves more than 66% improvement in efficiency\nwhen considering the trade-off between model complexity and performance,\ncompared to the SOTA model. This study highlights an efficient and effective\nsolution for improving semantic segmentation in remote sensing applications,\noffering a computationally lightweight model capable of high-quality\nperformance in agricultural imagery.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eDeepLabV3\u7684\u9ad8\u6548\u519c\u7530\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u578bDAS Conv\u6a21\u5757\u548c\u8df3\u8dc3\u8fde\u63a5\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u590d\u6742SOTA\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6548\u7387\u4e0a\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u519c\u4e1a\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u662f\u73b0\u4ee3\u519c\u4e1a\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u6709\u52a9\u4e8e\u7cbe\u786e\u5206\u6790\u89c6\u89c9\u6570\u636e\uff0c\u4ece\u800c\u6539\u5584\u4f5c\u7269\u7ba1\u7406\u3001\u4f18\u5316\u8d44\u6e90\u5229\u7528\u5e76\u63d0\u9ad8\u6574\u4f53\u751f\u4ea7\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u9ad8\u6548\u7684\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u91cd\u70b9\u5728\u4e8e\u51c6\u786e\u63cf\u7ed8\u519c\u7530\u5f02\u5e38\uff0c\u4ee5\u652f\u6301\u660e\u667a\u51b3\u7b56\u548c\u4e3b\u52a8\u5e72\u9884\u3002", "method": "\u8be5\u7814\u7a76\u5c06\u65b0\u578bDual Atrous Separable Convolution (DAS Conv)\u6a21\u5757\u96c6\u6210\u5230DeepLabV3\u5206\u5272\u6846\u67b6\u4e2d\u3002DAS Conv\u6a21\u5757\u65e8\u5728\u4f18\u5316\u81a8\u80c0\u7387\u548c\u586b\u5145\u5927\u5c0f\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u4ece\u800c\u5728\u4e0d\u727a\u7272\u6548\u7387\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u4ece\u7f16\u7801\u5668\u7684\u6700\u4f73\u9636\u6bb5\u5411\u89e3\u7801\u5668\u5f15\u5165\u4e86\u7b56\u7565\u6027\u8df3\u8dc3\u8fde\u63a5\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u6355\u83b7\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u7279\u5f81\u7684\u80fd\u529b\u3002", "result": "\u5c3d\u7ba1\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u4f4e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728Agriculture Vision\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4f18\u4e8e\u5176\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0e\u9ad8\u5ea6\u590d\u6742\u7684\u57fa\u4e8eTransformer\u7684\u5148\u8fdb\uff08SOTA\uff09\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002\u5728\u6a21\u578b\u590d\u6742\u5ea6\u4e0e\u6027\u80fd\u7684\u6743\u8861\u65b9\u9762\uff0c\u76f8\u8f83\u4e8eSOTA\u6a21\u578b\uff0c\u5176\u6548\u7387\u63d0\u5347\u8d85\u8fc766%\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u9065\u611f\u5e94\u7528\u4e2d\u7684\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u91cf\u8f7b\u5de7\u4f46\u80fd\u5728\u519c\u4e1a\u56fe\u50cf\u4e2d\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6027\u80fd\u7684\u6a21\u578b\u3002"}}
{"id": "2506.22853", "pdf": "https://arxiv.org/pdf/2506.22853", "abs": "https://arxiv.org/abs/2506.22853", "authors": ["Kyochul Jang", "Donghyeon Lee", "Kyusik Kim", "Dongseok Heo", "Taewhoo Lee", "Woojeong Kim", "Bongwon Suh"], "title": "DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, ACL 2025 Vienna", "summary": "Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/.", "AI": {"tldr": "\u9488\u5bf9\u73b0\u6709\u51fd\u6570\u8c03\u7528\u57fa\u51c6\u5355\u8f6e\u3001\u975e\u771f\u5b9e\u7684\u5c40\u9650\uff0c\u672c\u6587\u63d0\u51faDICE-SCORE\u5ea6\u91cf\u6807\u51c6\uff0c\u5e76\u6784\u5efaDICE-BENCH\u591a\u8f6e\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524dLLM\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e0b\u4ecd\u9700\u5927\u5e45\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u51fd\u6570\u8c03\u7528\u57fa\u51c6\u4ec5\u5173\u6ce8\u5355\u8f6e\u4ea4\u4e92\uff0c\u672a\u80fd\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u5de5\u5177\u76f8\u5173\u4fe1\u606f\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5206\u6563\u5206\u5e03\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4 realistic \u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u3002", "method": "1. \u5f15\u5165DICE-SCORE\u5ea6\u91cf\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u5bf9\u8bdd\u4e2d\u5de5\u5177\u76f8\u5173\u4fe1\u606f\uff08\u5982\u51fd\u6570\u540d\u3001\u53c2\u6570\u503c\uff09\u7684\u5206\u6563\u7a0b\u5ea6\u3002\n2. \u5f00\u53d1DICE-BENCH\u6846\u67b6\uff0c\u901a\u8fc7\u5de5\u5177\u56fe\u8c31\u7ef4\u62a4\u8f6e\u6b21\u95f4\u4f9d\u8d56\uff0c\u5e76\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u751f\u6210\u81ea\u7136\u5bf9\u8bdd\uff0c\u6784\u5efa\u9ad8DICE-SCORE\u7684\u5b9e\u7528\u51fd\u6570\u8c03\u7528\u6570\u636e\u96c6\u3002", "result": "1. \u901a\u8fc7DICE-SCORE\u5206\u6790\u53d1\u73b0\uff0c\u73b0\u6709\u57fa\u51c6\u5f97\u5206\u666e\u904d\u504f\u4f4e\uff0c\u51f8\u663e\u4e86\u5176\u975e\u771f\u5b9e\u6027\u3002\n2. \u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u5305\u542b1,607\u4e2a\u9ad8DICE-SCORE\u5b9e\u4f8b\u7684DICE-BENCH\u6570\u636e\u96c6\u3002\n3. \u5728DICE-BENCH\u4e0a\u5bf919\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u8fd9\u4e9b\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u524d\u4ecd\u9700\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u3001\u4fe1\u606f\u5206\u6563\u7684\u771f\u5b9e\u4e16\u754c\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u573a\u666f\u65f6\uff0c\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u8ddd\u79bb\u6709\u6548\u90e8\u7f72\u5c1a\u8fdc\u3002DICE-BENCH\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u771f\u5b9e\u3001\u66f4\u5177\u6311\u6218\u6027\u7684\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2506.23740", "pdf": "https://arxiv.org/pdf/2506.23740", "abs": "https://arxiv.org/abs/2506.23740", "authors": ["Andrew E. Ferguson", "Ujjwal Pawar", "Tianxin Wang", "Mahesh K. Marina"], "title": "Campus5G: A Campus Scale Private 5G Open RAN Testbed", "categories": ["cs.NI", "C.2.1"], "comment": null, "summary": "Mobile networks are embracing disaggregation, reflected by the industry trend\ntowards Open RAN. Private 5G networks are viewed as particularly suitable\ncontenders as early adopters of Open RAN, owing to their setting, high degree\nof control, and opportunity for innovation they present. Motivated by this, we\nhave recently deployed Campus5G, the first of its kind campus-wide,\nO-RAN-compliant private 5G testbed across the central campus of the University\nof Edinburgh. We present in detail our process developing the testbed, from\nplanning, to architecting, to deployment, and measuring the testbed\nperformance. We then discuss the lessons learned from building the testbed, and\nhighlight some research opportunities that emerged from our deployment\nexperience.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u5e76\u5206\u6790\u4e86\u5728\u7231\u4e01\u5821\u5927\u5b66\u90e8\u7f72\u7684Campus5G\u2014\u2014\u4e00\u4e2a\u6821\u56ed\u7ea7O-RAN\u517c\u5bb9\u79c1\u67095G\u8bd5\u9a8c\u5e73\u53f0\u7684\u89c4\u5212\u3001\u67b6\u6784\u3001\u90e8\u7f72\u53ca\u6027\u80fd\u6d4b\u91cf\u8fc7\u7a0b\uff0c\u5e76\u603b\u7ed3\u4e86\u7ecf\u9a8c\u6559\u8bad\u548c\u7814\u7a76\u673a\u4f1a\u3002", "motivation": "\u79fb\u52a8\u7f51\u7edc\u6b63\u8d8b\u5411\u89e3\u8026\u5316\uff0cOpen RAN\u662f\u884c\u4e1a\u8d8b\u52bf\u3002\u79c1\u67095G\u7f51\u7edc\u56e0\u5176\u73af\u5883\u3001\u9ad8\u63a7\u5236\u5ea6\u548c\u521b\u65b0\u673a\u4f1a\uff0c\u88ab\u8ba4\u4e3a\u662fOpen RAN\u7684\u7406\u60f3\u65e9\u671f\u91c7\u7528\u8005\u3002", "method": "\u90e8\u7f72\u4e86Campus5G\uff0c\u8fd9\u662f\u9996\u4e2a\u6821\u56ed\u7ea7O-RAN\u517c\u5bb9\u79c1\u67095G\u8bd5\u9a8c\u5e73\u53f0\u3002\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u4ece\u89c4\u5212\u3001\u67b6\u6784\u8bbe\u8ba1\u5230\u90e8\u7f72\u4ee5\u53ca\u6027\u80fd\u6d4b\u91cf\u7684\u5168\u8fc7\u7a0b\u3002", "result": "\u6210\u529f\u90e8\u7f72\u4e86Campus5G\u8bd5\u9a8c\u5e73\u53f0\uff0c\u5e76\u5bf9\u5176\u6027\u80fd\u8fdb\u884c\u4e86\u6d4b\u91cf\u3002\u4ece\u6784\u5efa\u548c\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u83b7\u5f97\u4e86\u5b9d\u8d35\u7684\u7ecf\u9a8c\u6559\u8bad\u3002", "conclusion": "\u6210\u529f\u6784\u5efa\u4e86O-RAN\u517c\u5bb9\u7684\u79c1\u67095G\u8bd5\u9a8c\u5e73\u53f0\uff0c\u4e3aOpen RAN\u5728\u79c1\u67095G\u7f51\u7edc\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u8df5\u57fa\u7840\uff0c\u5e76\u8bc6\u522b\u4e86\u672a\u6765\u6f5c\u5728\u7684\u7814\u7a76\u673a\u4f1a\u3002"}}
{"id": "2506.22708", "pdf": "https://arxiv.org/pdf/2506.22708", "abs": "https://arxiv.org/abs/2506.22708", "authors": ["Shrenik Jadhav", "Birva Sevak", "Srijita Das", "Akhtar Hussain", "Wencong Su", "Van-Hai Bui"], "title": "FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets", "categories": ["cs.LG", "cs.SY", "econ.GN", "eess.SY", "q-fin.EC"], "comment": null, "summary": "Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for\ndecentralized market regulation, yet existing approaches often lack robust\nframeworks to ensure fairness. This paper presents FairMarket-RL, a novel\nhybrid framework that combines Large Language Models (LLMs) with Reinforcement\nLearning (RL) to enable fairness-aware trading agents. In a simulated P2P\nmicrogrid with multiple sellers and buyers, the LLM acts as a real-time\nfairness critic, evaluating each trading episode using two metrics:\nFairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness\nscores are integrated into agent rewards through scheduled\n{\\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that\nreplaces brittle, rule-based fairness constraints. Agents are trained using\nIndependent Proximal Policy Optimization (IPPO) and achieve equitable outcomes,\nfulfilling over 90% of buyer demand, maintaining fair seller margins, and\nconsistently reaching FTB and FBS scores above 0.80. The training process\ndemonstrates that fairness feedback improves convergence, reduces buyer\nshortfalls, and narrows profit disparities between sellers. With its\nlanguage-based critic, the framework scales naturally, and its extension to a\nlarge power distribution system with household prosumers illustrates its\npractical applicability. FairMarket-RL thus offers a scalable, equity-driven\nsolution for autonomous trading in decentralized energy systems.", "AI": {"tldr": "\u9488\u5bf9P2P\u4ea4\u6613\u4e2d\u7f3a\u4e4f\u516c\u5e73\u6027\u6846\u67b6\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faFairMarket-RL\uff0c\u4e00\u4e2a\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6df7\u5408\u6846\u67b6\uff0c\u65e8\u5728\u5b9e\u73b0\u516c\u5e73\u7684\u53bb\u4e2d\u5fc3\u5316\u80fd\u6e90\u4ea4\u6613\u3002", "motivation": "P2P\u4ea4\u6613\u65e5\u76ca\u6210\u4e3a\u53bb\u4e2d\u5fc3\u5316\u5e02\u573a\u76d1\u7ba1\u7684\u5173\u952e\u673a\u5236\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u7f3a\u4e4f\u9c81\u68d2\u7684\u6846\u67b6\u6765\u786e\u4fdd\u4ea4\u6613\u516c\u5e73\u6027\u3002", "method": "\u63d0\u51faFairMarket-RL\u6df7\u5408\u6846\u67b6\uff0c\u5c06LLM\u4e0eRL\u7ed3\u5408\u3002LLM\u4f5c\u4e3a\u5b9e\u65f6\u516c\u5e73\u6027\u8bc4\u4f30\u5668\uff0c\u4f7f\u7528\u4e70\u5bb6\u516c\u5e73\u6027\uff08FTB\uff09\u548c\u5356\u5bb6\u95f4\u516c\u5e73\u6027\uff08FBS\uff09\u4e24\u4e2a\u6307\u6807\u8bc4\u4f30\u4ea4\u6613\u3002\u8fd9\u4e9b\u516c\u5e73\u6027\u5206\u6570\u901a\u8fc7\u81ea\u9002\u5e94\u7684LLM\u5f15\u5bfc\u5956\u52b1\u5851\u5f62\u73af\uff08\u4f7f\u7528\u9884\u8bbe\u7684\u03bb-\u7cfb\u6570\uff09\u6574\u5408\u5230\u667a\u80fd\u4f53\u7684\u5956\u52b1\u4e2d\uff0c\u53d6\u4ee3\u4e86\u8106\u5f31\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u516c\u5e73\u6027\u7ea6\u675f\u3002\u667a\u80fd\u4f53\u4f7f\u7528\u72ec\u7acb\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08IPPO\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u6a21\u62dfP2P\u5fae\u7535\u7f51\u4e2d\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u516c\u5e73\u7684\u4ea4\u6613\u7ed3\u679c\uff0c\u6ee1\u8db3\u4e8690%\u4ee5\u4e0a\u7684\u4e70\u5bb6\u9700\u6c42\uff0c\u4fdd\u6301\u4e86\u5356\u5bb6\u516c\u5e73\u5229\u6da6\uff0c\u5e76\u4f7fFTB\u548cFBS\u5206\u6570\u59cb\u7ec8\u9ad8\u4e8e0.80\u3002\u8bad\u7ec3\u8fc7\u7a0b\u8868\u660e\uff0c\u516c\u5e73\u6027\u53cd\u9988\u6539\u5584\u4e86\u6536\u655b\u6027\uff0c\u51cf\u5c11\u4e86\u4e70\u5bb6\u77ed\u7f3a\uff0c\u5e76\u7f29\u5c0f\u4e86\u5356\u5bb6\u4e4b\u95f4\u7684\u5229\u6da6\u5dee\u5f02\u3002", "conclusion": "FairMarket-RL\u4e3a\u53bb\u4e2d\u5fc3\u5316\u80fd\u6e90\u7cfb\u7edf\u4e2d\u7684\u81ea\u4e3b\u4ea4\u6613\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u516c\u5e73\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5176\u57fa\u4e8e\u8bed\u8a00\u7684\u8bc4\u4f30\u5668\u4f7f\u5176\u80fd\u591f\u81ea\u7136\u6269\u5c55\uff0c\u5e76\u80fd\u5e94\u7528\u4e8e\u5305\u542b\u5bb6\u5ead\u201c\u4ea7\u6d88\u8005\u201d\u7684\u5927\u578b\u914d\u7535\u7cfb\u7edf\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.23464", "pdf": "https://arxiv.org/pdf/2506.23464", "abs": "https://arxiv.org/abs/2506.23464", "authors": ["Sahil Tripathi", "Md Tabrez Nafis", "Imran Hussain", "Jiechao Gao"], "title": "The Confidence Paradox: Can LLM Know When It's Wrong", "categories": ["cs.AI"], "comment": null, "summary": "Document Visual Question Answering (DocVQA) systems are increasingly deployed\nin real world applications, yet they remain ethically opaque-often producing\noverconfident answers to ambiguous questions or failing to communicate\nuncertainty in a trustworthy manner. This misalignment between model confidence\nand actual knowledge poses significant risks, particularly in domains requiring\nethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT\nhave advanced SOTA performance by focusing on architectural sophistication and\naccuracy; however, they fall short in ethical responsiveness.\n  To address these limitations, we introduce HonestVQA, a self-supervised\nhonesty calibration framework for ethically aligned DocVQA. Our model-agnostic\nmethod quantifies uncertainty to identify knowledge gaps, aligns model\nconfidence with actual correctness using weighted loss functions, and enforces\nethical response behavior via contrastive learning. We further introduce two\nprincipled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence\nIndex (ECI)--to benchmark alignment between confidence, accuracy, and ethical\ncommunication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%\nand F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces\noverconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In\ncross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,\ndemonstrating strong generalization. Ablation shows a 3.8% drop in accuracy\nwithout alignment or contrastive loss.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHonestVQA\uff0c\u4e00\u4e2a\u81ea\u76d1\u7763\u7684\u8bda\u5b9e\u6821\u51c6\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u6587\u6863\u89c6\u89c9\u95ee\u7b54\uff08DocVQA\uff09\u7cfb\u7edf\u4e2d\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u5b9e\u9645\u77e5\u8bc6\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u8fc7\u81ea\u4fe1\u3002", "motivation": "DocVQA\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5b58\u5728\u4f26\u7406\u4e0d\u900f\u660e\u6027\uff0c\u5e38\u5bf9\u6a21\u7cca\u95ee\u9898\u8fc7\u5ea6\u81ea\u4fe1\u6216\u672a\u80fd\u53ef\u9760\u5730\u4f20\u8fbe\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u5b9e\u9645\u77e5\u8bc6\u4e0d\u7b26\uff0c\u7279\u522b\u5728\u9700\u8981\u4f26\u7406\u95ee\u8d23\u7684\u9886\u57df\u6784\u6210\u98ce\u9669\u3002\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff08\u5982LayoutLMv3\u3001UDOP\u3001DONUT\uff09\u4fa7\u91cd\u6027\u80fd\u800c\u975e\u4f26\u7406\u54cd\u5e94\u80fd\u529b\u3002", "method": "\u5f15\u5165HonestVQA\uff0c\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u81ea\u76d1\u7763\u8bda\u5b9e\u6821\u51c6\u6846\u67b6\u3002\u5b83\u901a\u8fc7\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u8bc6\u522b\u77e5\u8bc6\u5dee\u8ddd\uff0c\u4f7f\u7528\u52a0\u6743\u635f\u5931\u51fd\u6570\u4f7f\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u6b63\u786e\u6027\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5f3a\u5236\u6267\u884c\u4f26\u7406\u54cd\u5e94\u884c\u4e3a\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86Honesty Score (H-Score)\u548cEthical Confidence Index (ECI)\u4e24\u4e2a\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8861\u91cf\u7f6e\u4fe1\u5ea6\u3001\u51c6\u786e\u6027\u548c\u4f26\u7406\u6c9f\u901a\u7684\u5bf9\u9f50\u60c5\u51b5\u3002", "result": "HonestVQA\u5728SpDocVQA\u3001InfographicsVQA\u548cSROIE\u6570\u636e\u96c6\u4e0a\u5c06DocVQA\u51c6\u786e\u7387\u548cF1\u5206\u6570\u63d0\u9ad8\u4e864.3%\u3002\u5b83\u663e\u8457\u964d\u4f4e\u4e86\u8fc7\u81ea\u4fe1\uff0cH-Score\u548cECI\u5206\u522b\u964d\u4f4e\u4e860.072\u548c0.078\u3002\u5728\u8de8\u57df\u8bc4\u4f30\u4e2d\uff0c\u5b83\u5b9e\u73b0\u4e86\u9ad8\u8fbe78.9%\u7684\u51c6\u786e\u7387\u548c76.1%\u7684F1\u5206\u6570\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6ca1\u6709\u5bf9\u9f50\u6216\u5bf9\u6bd4\u635f\u5931\u7684\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u4e863.8%\u3002", "conclusion": "HonestVQA\u901a\u8fc7\u8bda\u5b9e\u6821\u51c6\u6709\u6548\u89e3\u51b3\u4e86DocVQA\u7684\u4f26\u7406\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u51c6\u786e\u6027\u3001\u964d\u4f4e\u4e86\u8fc7\u81ea\u4fe1\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u6838\u5fc3\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.22589", "pdf": "https://arxiv.org/pdf/2506.22589", "abs": "https://arxiv.org/abs/2506.22589", "authors": ["Yijun Lin", "Rhett Olson", "Junhan Wu", "Yao-Yi Chiang", "Jerod Weinman"], "title": "LIGHT: Multi-Modal Text Linking on Historical Maps", "categories": ["cs.CV"], "comment": "Accepted at ICDAR2025", "summary": "Text on historical maps provides valuable information for studies in history,\neconomics, geography, and other related fields. Unlike structured or\nsemi-structured documents, text on maps varies significantly in orientation,\nreading order, shape, and placement. Many modern methods can detect and\ntranscribe text regions, but they struggle to effectively ``link'' the\nrecognized text fragments, e.g., determining a multi-word place name. Existing\nlayout analysis methods model word relationships to improve text understanding\nin structured documents, but they primarily rely on linguistic features and\nneglect geometric information, which is essential for handling map text. To\naddress these challenges, we propose LIGHT, a novel multi-modal approach that\nintegrates linguistic, image, and geometric features for linking text on\nhistorical maps. In particular, LIGHT includes a geometry-aware embedding\nmodule that encodes the polygonal coordinates of text regions to capture\npolygon shapes and their relative spatial positions on an image. LIGHT unifies\nthis geometric information with the visual and linguistic token embeddings from\nLayoutLMv3, a pretrained layout analysis model. LIGHT uses the cross-modal\ninformation to predict the reading-order successor of each text instance\ndirectly with a bi-directional learning strategy that enhances sequence\nrobustness. Experimental results show that LIGHT outperforms existing methods\non the ICDAR 2024/2025 MapText Competition data, demonstrating the\neffectiveness of multi-modal learning for historical map text linking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLIGHT\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u6574\u5408\u8bed\u8a00\u3001\u56fe\u50cf\u548c\u51e0\u4f55\u7279\u5f81\uff0c\u65e8\u5728\u6709\u6548\u94fe\u63a5\u5386\u53f2\u5730\u56fe\u4e0a\u7684\u6587\u672c\u7247\u6bb5\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5386\u53f2\u5730\u56fe\u4e0a\u7684\u6587\u672c\u4fe1\u606f\u5bf9\u591a\u9886\u57df\u7814\u7a76\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5730\u56fe\u6587\u672c\u7684\u6392\u7248\uff08\u65b9\u5411\u3001\u9605\u8bfb\u987a\u5e8f\u3001\u5f62\u72b6\u3001\u4f4d\u7f6e\uff09\u9ad8\u5ea6\u53ef\u53d8\uff0c\u5bfc\u81f4\u73b0\u6709\u6587\u672c\u8bc6\u522b\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u201c\u94fe\u63a5\u201d\u6587\u672c\u7247\u6bb5\uff08\u5982\u591a\u8bcd\u5730\u540d\uff09\u3002\u73b0\u6709\u7248\u9762\u5206\u6790\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8bed\u8a00\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u5bf9\u5730\u56fe\u6587\u672c\u81f3\u5173\u91cd\u8981\u7684\u51e0\u4f55\u4fe1\u606f\u3002", "method": "\u63d0\u51faLIGHT\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u878d\u5408\u4e86\u8bed\u8a00\u3001\u56fe\u50cf\u548c\u51e0\u4f55\u7279\u5f81\u4ee5\u94fe\u63a5\u5386\u53f2\u5730\u56fe\u4e0a\u7684\u6587\u672c\u3002LIGHT\u5305\u542b\u4e00\u4e2a\u51e0\u4f55\u611f\u77e5\u5d4c\u5165\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u7f16\u7801\u6587\u672c\u533a\u57df\u7684\u591a\u8fb9\u5f62\u5750\u6807\u4ee5\u6355\u6349\u5176\u5f62\u72b6\u548c\u76f8\u5bf9\u7a7a\u95f4\u4f4d\u7f6e\u3002LIGHT\u5c06\u8fd9\u4e9b\u51e0\u4f55\u4fe1\u606f\u4e0eLayoutLMv3\u7684\u89c6\u89c9\u548c\u8bed\u8a00\u4ee4\u724c\u5d4c\u5165\u76f8\u7ed3\u5408\uff0c\u5e76\u5229\u7528\u8de8\u6a21\u6001\u4fe1\u606f\u901a\u8fc7\u53cc\u5411\u5b66\u4e60\u7b56\u7565\u76f4\u63a5\u9884\u6d4b\u6bcf\u4e2a\u6587\u672c\u5b9e\u4f8b\u7684\u9605\u8bfb\u987a\u5e8f\u540e\u7ee7\uff0c\u4ee5\u589e\u5f3a\u5e8f\u5217\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLIGHT\u5728ICDAR 2024/2025 MapText\u7ade\u8d5b\u6570\u636e\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u5728\u5386\u53f2\u5730\u56fe\u6587\u672c\u94fe\u63a5\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.22858", "pdf": "https://arxiv.org/pdf/2506.22858", "abs": "https://arxiv.org/abs/2506.22858", "authors": ["Duygu Altinok"], "title": "Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "summary": "Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high\ntranscription accuracy but struggle with named entities and numerical data,\nespecially when proper formatting is required. These issues increase word error\nrate (WER) and impair semantic understanding in critical domains like legal,\nfinancial, and medical applications. We propose a novel training approach that\nextends the semantic context of ASR models by adding overlapping context\nwindows during training. By sliding 5-second overlaps on both sides of\n30-second chunks, we create a 40-second \"effective semantic window,\" improving\nentity recognition and formatting while focusing predictions on the central 30\nseconds. To address entities spanning chunk boundaries, we reassign such\nentities entirely to the right-hand chunk, ensuring proper formatting.\nAdditionally, enriched training data with embedded entity labels enables the\nmodel to learn both recognition and type-specific formatting. Evaluated on the\nSpoken Wikipedia dataset, our method improves performance across semantic\ntasks, including named entity recognition (NER) and entity formatting. These\nresults highlight the effectiveness of context-aware training in addressing ASR\nlimitations for long-form transcription and complex entity recognition tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684ASR\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u4e30\u5bcc\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u683c\u5f0f\u5316\u65b9\u9762\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709ASR\u7cfb\u7edf\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709ASR\u7cfb\u7edf\uff08\u5982Whisper\uff09\u5728\u5904\u7406\u547d\u540d\u5b9e\u4f53\u548c\u9700\u8981\u7cbe\u786e\u683c\u5f0f\u5316\u7684\u6570\u5b57\u6570\u636e\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8bcd\u9519\u8bef\u7387(WER)\u5347\u9ad8\uff0c\u5e76\u5728\u6cd5\u5f8b\u3001\u91d1\u878d\u3001\u533b\u7597\u7b49\u5173\u952e\u9886\u57df\u5f71\u54cd\u8bed\u4e49\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff1a1. \u901a\u8fc7\u572830\u79d2\u8bed\u97f3\u5757\u4e24\u4fa7\u589e\u52a05\u79d2\u91cd\u53e0\uff0c\u521b\u5efa40\u79d2\u7684\u201c\u6709\u6548\u8bed\u4e49\u7a97\u53e3\u201d\uff0c\u540c\u65f6\u5c06\u9884\u6d4b\u96c6\u4e2d\u5728\u4e2d\u95f430\u79d2\u30022. \u5c06\u8de8\u8d8a\u5757\u8fb9\u754c\u7684\u5b9e\u4f53\u91cd\u65b0\u5206\u914d\u5230\u53f3\u4fa7\u5757\u30023. \u4f7f\u7528\u5d4c\u5165\u5b9e\u4f53\u6807\u7b7e\u7684\u4e30\u5bcc\u8bad\u7ec3\u6570\u636e\uff0c\u4f7f\u6a21\u578b\u5b66\u4e60\u8bc6\u522b\u548c\u7c7b\u578b\u7279\u5b9a\u683c\u5f0f\u5316\u3002", "result": "\u5728Spoken Wikipedia\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5728\u5305\u62ec\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u548c\u5b9e\u4f53\u683c\u5f0f\u5316\u5728\u5185\u7684\u8bed\u4e49\u4efb\u52a1\u4e0a\u5747\u6709\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8bad\u7ec3\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3ASR\u5728\u957f\u7bc7\u8f6c\u5f55\u548c\u590d\u6742\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.23755", "pdf": "https://arxiv.org/pdf/2506.23755", "abs": "https://arxiv.org/abs/2506.23755", "authors": ["Shawon Mitra", "Subhojit Sarkar", "Sasthi C. Ghosh"], "title": "How Long Can I Transmit? A Mobility Aware mmWave-based UAV Communication Framework", "categories": ["cs.NI", "eess.SP"], "comment": "This article has been submitted in a reputed conference", "summary": "One primary focus of next generation wireless communication networks is the\nmillimeterwave (mmWave) spectrum, typically considered in the 30 GHz to 300 GHz\nfrequency range. Despite their promise of high data rates, mmWaves suffer from\nsevere attenuation while passing through obstacles. Unmanned aerial vehicles\n(UAVs) have been proposed to offset this limitation on account of their\nadditional degrees of freedom, which can be leveraged to provide line of sight\n(LoS) transmission paths. While some prior works have proposed analytical\nframeworks to compute the LoS probability for static ground users and a UAV,\nthe same is lacking for mobile users on the ground. In this paper, we consider\nthe popular Manhattan point line process (MPLP) to model an urban environment,\nwithin which a ground user moves with a known velocity for a small time\ninterval along the roads. We derive an expression for the expected duration of\nLoS between a static UAV in the air and a mobile ground user, and validate the\nsame through simulations. To demonstrate the efficacy of the proposed analysis,\nwe propose a simple user association algorithm that greedily assigns the UAVs\nto users with the highest expected LoS time, and show that it outperforms the\nexisting benchmark schemes that assign the users to the nearest UAVs with LoS\nwithout considering the user mobility.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u57ce\u5e02\u73af\u5883\u4e2d\u56fa\u5b9a\u65e0\u4eba\u673a\u4e0e\u79fb\u52a8\u5730\u9762\u7528\u6237\u4e4b\u95f4\u6beb\u7c73\u6ce2\u901a\u4fe1\u7684\u9884\u671f\u89c6\u8ddd\uff08LoS\uff09\u65f6\u957f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8be5\u5206\u6790\u7684\u65e0\u4eba\u673a\u7528\u6237\u5173\u8054\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f18\u4e8e\u4e0d\u8003\u8651\u7528\u6237\u79fb\u52a8\u6027\u7684\u73b0\u6709\u65b9\u6848\u3002", "motivation": "\u6beb\u7c73\u6ce2\u901a\u4fe1\u867d\u7136\u80fd\u63d0\u4f9b\u9ad8\u6570\u636e\u901f\u7387\uff0c\u4f46\u6781\u6613\u53d7\u969c\u788d\u7269\u8870\u51cf\u3002\u65e0\u4eba\u673a\u56e0\u5176\u989d\u5916\u7684\u81ea\u7531\u5ea6\uff0c\u53ef\u63d0\u4f9b\u89c6\u8ddd\u4f20\u8f93\u8def\u5f84\u6765\u5f25\u8865\u6b64\u9650\u5236\u3002\u7136\u800c\uff0c\u73b0\u6709\u5de5\u4f5c\u7f3a\u4e4f\u9488\u5bf9\u79fb\u52a8\u5730\u9762\u7528\u6237\u7684\u65e0\u4eba\u673a-\u5730\u9762\u7528\u6237\u89c6\u8ddd\u6982\u7387\u7684\u5206\u6790\u6846\u67b6\u3002", "method": "\u91c7\u7528\u66fc\u54c8\u987f\u70b9\u7ebf\u8fc7\u7a0b\uff08MPLP\uff09\u6a21\u578b\u6a21\u62df\u57ce\u5e02\u73af\u5883\uff1b\u63a8\u5bfc\u4e86\u56fa\u5b9a\u65e0\u4eba\u673a\u4e0e\u6cbf\u9053\u8def\u79fb\u52a8\u7684\u5730\u9762\u7528\u6237\u4e4b\u95f4\u9884\u671f\u89c6\u8ddd\u65f6\u957f\u7684\u8868\u8fbe\u5f0f\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\uff1b\u63d0\u51fa\u4e86\u4e00\u79cd\u8d2a\u5a6a\u7528\u6237\u5173\u8054\u7b97\u6cd5\uff0c\u4f9d\u636e\u6700\u5927\u9884\u671f\u89c6\u8ddd\u65f6\u957f\u5c06\u65e0\u4eba\u673a\u5206\u914d\u7ed9\u7528\u6237\u3002", "result": "\u6210\u529f\u63a8\u5bfc\u4e86\u56fa\u5b9a\u65e0\u4eba\u673a\u4e0e\u79fb\u52a8\u5730\u9762\u7528\u6237\u4e4b\u95f4\u9884\u671f\u89c6\u8ddd\u65f6\u957f\u7684\u8868\u8fbe\u5f0f\uff0c\u5e76\u5f97\u5230\u4e86\u4eff\u771f\u9a8c\u8bc1\uff1b\u63d0\u51fa\u7684\u7528\u6237\u5173\u8054\u7b97\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4e0d\u8003\u8651\u7528\u6237\u79fb\u52a8\u6027\u7684\u73b0\u6709\u57fa\u51c6\u65b9\u6848\uff08\u5982\u5206\u914d\u7ed9\u6700\u8fd1\u7684\u89c6\u8ddd\u65e0\u4eba\u673a\uff09\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5206\u6790\u6846\u67b6\u548c\u7528\u6237\u5173\u8054\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u65e0\u4eba\u673a\u5728\u6beb\u7c73\u6ce2\u7f51\u7edc\u4e2d\u4e3a\u79fb\u52a8\u5730\u9762\u7528\u6237\u63d0\u4f9b\u66f4\u4f18\u7684\u89c6\u8ddd\u901a\u4fe1\uff0c\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u3002"}}
{"id": "2506.22712", "pdf": "https://arxiv.org/pdf/2506.22712", "abs": "https://arxiv.org/abs/2506.22712", "authors": ["Alexander Theus", "Alessandro Cabodi", "Sotiris Anagnostidis", "Antonio Orvieto", "Sidak Pal Singh", "Valentina Boeva"], "title": "Generalized Linear Mode Connectivity for Transformers", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Understanding the geometry of neural network loss landscapes is a central\nquestion in deep learning, with implications for generalization and\noptimization. A striking phenomenon is linear mode connectivity (LMC), where\nindependently trained models can be connected by low- or zero-loss paths,\ndespite appearing to lie in separate loss basins. However, this is often\nobscured by symmetries in parameter space -- such as neuron permutations --\nwhich make functionally equivalent models appear dissimilar. Prior work has\npredominantly focused on neuron re-ordering through permutations, but such\napproaches are limited in scope and fail to capture the richer symmetries\nexhibited by modern architectures such as Transformers. In this work, we\nintroduce a unified framework that captures four symmetry classes:\npermutations, semi-permutations, orthogonal transformations, and general\ninvertible maps -- broadening the set of valid reparameterizations and\nsubsuming many previous approaches as special cases. Crucially, this\ngeneralization enables, for the first time, the discovery of low- and\nzero-barrier linear interpolation paths between independently trained Vision\nTransformers and GPT-2 models. These results reveal deeper structure in the\nloss landscape and underscore the importance of symmetry-aware analysis for\nunderstanding model space geometry.", "AI": {"tldr": "\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u666f\u89c2\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u6db5\u76d6\u56db\u79cd\u5bf9\u79f0\u6027\u3002\u8be5\u6846\u67b6\u9996\u6b21\u6210\u529f\u5728Vision Transformers\u548cGPT-2\u6a21\u578b\u4e2d\u53d1\u73b0\u4f4e/\u96f6\u635f\u5931\u7ebf\u6027\u8fde\u63a5\u8def\u5f84\uff0c\u63ed\u793a\u4e86\u635f\u5931\u666f\u89c2\u7684\u6df1\u5c42\u7ed3\u6784\uff0c\u5e76\u5f3a\u8c03\u4e86\u5bf9\u79f0\u6027\u611f\u77e5\u5206\u6790\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u666f\u89c2\u7684\u51e0\u4f55\u7ed3\u6784\u5bf9\u4e8e\u6cdb\u5316\u548c\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002\u7ebf\u6027\u6a21\u5f0f\u8fde\u63a5\u6027\uff08LMC\uff09\u662f\u4e00\u4e2a\u663e\u8457\u73b0\u8c61\uff0c\u4f46\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u5bf9\u79f0\u6027\uff08\u5982\u795e\u7ecf\u5143\u7f6e\u6362\uff09\u5e38\u5c06\u5176\u906e\u853d\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u795e\u7ecf\u5143\u91cd\u6392\uff0c\u8303\u56f4\u6709\u9650\uff0c\u65e0\u6cd5\u6355\u83b7Transformer\u7b49\u73b0\u4ee3\u67b6\u6784\u7684\u66f4\u4e30\u5bcc\u5bf9\u79f0\u6027\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6355\u83b7\u56db\u79cd\u5bf9\u79f0\u6027\u7c7b\u522b\uff1a\u7f6e\u6362\uff08permutations\uff09\u3001\u534a\u7f6e\u6362\uff08semi-permutations\uff09\u3001\u6b63\u4ea4\u53d8\u6362\uff08orthogonal transformations\uff09\u548c\u4e00\u822c\u53ef\u9006\u6620\u5c04\uff08general invertible maps\uff09\u3002", "result": "\u9996\u6b21\u6210\u529f\u53d1\u73b0\u4e86\u72ec\u7acb\u8bad\u7ec3\u7684Vision Transformers\u548cGPT-2\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u4f4e\u635f\u5931\u6216\u96f6\u635f\u5931\u7684\u7ebf\u6027\u63d2\u503c\u8def\u5f84\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u63ed\u793a\u4e86\u635f\u5931\u666f\u89c2\u4e2d\u66f4\u6df1\u5c42\u6b21\u7684\u7ed3\u6784\uff0c\u5e76\u5f3a\u8c03\u4e86\u5bf9\u79f0\u6027\u611f\u77e5\u5206\u6790\u5bf9\u4e8e\u7406\u89e3\u6a21\u578b\u7a7a\u95f4\u51e0\u4f55\u5f62\u72b6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.23503", "pdf": "https://arxiv.org/pdf/2506.23503", "abs": "https://arxiv.org/abs/2506.23503", "authors": ["Bosubabu Sambana", "Kondreddygari Archana", "Suram Indhra Sena Reddy", "Shaik Meethaigar Jameer Basha", "Shaik Karishma"], "title": "Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence", "categories": ["cs.AI"], "comment": "6 Pages, 5 Figures, IEEE IDCIoT 2025", "summary": "Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the\nirrational thought patterns associated with mental health disorders, but its\neffectiveness relies on accurately identifying cognitive pathways to provide\ntargeted treatment. In today's digital age, individuals often express negative\nemotions on social media, where they may reveal cognitive distortions, and in\nsevere cases, exhibit suicidal tendencies. However, there is a significant gap\nin methodologies designed to analyze these cognitive pathways, which could be\ncritical for psychotherapists aiming to deliver timely and effective\ninterventions in online environments. Cognitive Behavioral Therapy (CBT)\nframework leveraging acceptance, commitment and data augmentation to categorize\nand address both textual and visual content as positive or negative.\nSpecifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,\nPEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages\nfocusing on detecting negative emotions and cognitive distortions within social\nmedia data. While existing models are primarily designed to identify negative\nthoughts, the proposed system goes beyond this by predicting additional\nnegative side effects and other potential mental health disorders likes\nPhobias, Eating Disorders. This enhancement allows for a more comprehensive\nunderstanding and intervention strategy, offering psychotherapists a powerful\ntool for early detection and treatment of various psychological issues.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eCBT\u6846\u67b6\u7684\u7cfb\u7edf\uff0c\u5229\u7528BERT\u3001RoBERTa\u3001T5\u7b49\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u6587\u672c\u548c\u89c6\u89c9\u5185\u5bb9\uff0c\u4ee5\u68c0\u6d4b\u8d1f\u9762\u60c5\u7eea\u3001\u8ba4\u77e5\u626d\u66f2\uff0c\u5e76\u9884\u6d4b\u591a\u79cd\u7cbe\u795e\u5065\u5eb7\u969c\u788d\uff0c\u65e8\u5728\u8f85\u52a9\u5fc3\u7406\u6cbb\u7597\u5e08\u8fdb\u884c\u65e9\u671f\u5e72\u9884\u3002", "motivation": "\u8ba4\u77e5\u884c\u4e3a\u7597\u6cd5\uff08CBT\uff09\u867d\u6709\u6548\uff0c\u4f46\u9700\u7cbe\u51c6\u8bc6\u522b\u8ba4\u77e5\u8def\u5f84\u3002\u6570\u5b57\u65f6\u4ee3\uff0c\u793e\u4ea4\u5a92\u4f53\u4e0a\u5b58\u5728\u5927\u91cf\u4e2a\u4f53\u8868\u8fbe\u7684\u8d1f\u9762\u60c5\u7eea\u548c\u6f5c\u5728\u8ba4\u77e5\u626d\u66f2\uff0c\u751a\u81f3\u81ea\u6740\u503e\u5411\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u5206\u6790\u8fd9\u4e9b\u5728\u7ebf\u8ba4\u77e5\u8def\u5f84\u7684\u65b9\u6cd5\uff0c\u96be\u4ee5\u4f9b\u5fc3\u7406\u6cbb\u7597\u5e08\u53ca\u65f6\u5e72\u9884\u3002\u73b0\u6709\u6a21\u578b\u4e3b\u8981\u8bc6\u522b\u8d1f\u9762\u60f3\u6cd5\uff0c\u65e0\u6cd5\u5168\u9762\u9884\u6d4b\u66f4\u5e7f\u6cdb\u7684\u7cbe\u795e\u5065\u5eb7\u969c\u788d\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u63a5\u53d7\u3001\u627f\u8bfa\u548c\u6570\u636e\u589e\u5f3a\u7684\u8ba4\u77e5\u884c\u4e3a\u7597\u6cd5\uff08CBT\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u7c7b\u6587\u672c\u548c\u89c6\u89c9\u5185\u5bb9\u7684\u6b63\u8d1f\u6027\u3002\u5177\u4f53\u91c7\u7528BERT\u3001RoBERTa\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0cT5\u3001PEGASUS\u8fdb\u884c\u6587\u672c\u6458\u8981\uff0c\u4ee5\u53camT5\u8fdb\u884c\u591a\u8bed\u8a00\u6587\u672c\u7ffb\u8bd1\uff0c\u4ee5\u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e2d\u7684\u8d1f\u9762\u60c5\u7eea\u548c\u8ba4\u77e5\u626d\u66f2\u3002\u7cfb\u7edf\u8fd8\u6269\u5c55\u4e86\u529f\u80fd\uff0c\u53ef\u9884\u6d4b\u989d\u5916\u7684\u8d1f\u9762\u526f\u4f5c\u7528\u548c\u5176\u4ed6\u6f5c\u5728\u7684\u7cbe\u795e\u5065\u5eb7\u969c\u788d\uff0c\u5982\u6050\u60e7\u75c7\u548c\u996e\u98df\u5931\u8c03\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e2d\u7684\u8d1f\u9762\u60c5\u7eea\u548c\u8ba4\u77e5\u626d\u66f2\u3002\u76f8\u8f83\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5b83\u8fd8\u80fd\u9884\u6d4b\u989d\u5916\u7684\u8d1f\u9762\u526f\u4f5c\u7528\u4ee5\u53ca\u5305\u62ec\u6050\u60e7\u75c7\u3001\u996e\u98df\u5931\u8c03\u5728\u5185\u7684\u591a\u79cd\u6f5c\u5728\u7cbe\u795e\u5065\u5eb7\u969c\u788d\uff0c\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u7684\u5206\u6790\u80fd\u529b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5168\u9762\u7684\u7406\u89e3\u548c\u5e72\u9884\u7b56\u7565\uff0c\u4e3a\u5fc3\u7406\u6cbb\u7597\u5e08\u63d0\u4f9b\u4e86\u5728\u65e9\u671f\u68c0\u6d4b\u548c\u6cbb\u7597\u5404\u79cd\u5fc3\u7406\u95ee\u9898\u65b9\u9762\u7684\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2506.22591", "pdf": "https://arxiv.org/pdf/2506.22591", "abs": "https://arxiv.org/abs/2506.22591", "authors": ["Arunkumar Kannan", "Martin A. Lindquist", "Brian Caffo"], "title": "BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data", "categories": ["cs.CV"], "comment": "Accepted at MICCAI 2025", "summary": "Recent advances in deep learning have made it possible to predict phenotypic\nmeasures directly from functional magnetic resonance imaging (fMRI) brain\nvolumes, sparking significant interest in the neuroimaging community. However,\nexisting approaches, primarily based on convolutional neural networks or\ntransformer architectures, often struggle to model the complex relationships\ninherent in fMRI data, limited by their inability to capture long-range spatial\nand temporal dependencies. To overcome these shortcomings, we introduce\nBrainMT, a novel hybrid framework designed to efficiently learn and integrate\nlong-range spatiotemporal attributes in fMRI data. Our framework operates in\ntwo stages: (1) a bidirectional Mamba block with a temporal-first scanning\nmechanism to capture global temporal interactions in a computationally\nefficient manner; and (2) a transformer block leveraging self-attention to\nmodel global spatial relationships across the deep features processed by the\nMamba block. Extensive experiments on two large-scale public datasets,\nUKBioBank and the Human Connectome Project, demonstrate that BrainMT achieves\nstate-of-the-art performance on both classification (sex prediction) and\nregression (cognitive intelligence prediction) tasks, outperforming existing\nmethods by a significant margin. Our code and implementation details will be\nmade publicly available at this\nhttps://github.com/arunkumar-kannan/BrainMT-fMRI", "AI": {"tldr": "BrainMT\u662f\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408Mamba\u548cTransformer\uff0c\u6709\u6548\u5904\u7406fMRI\u6570\u636e\u4e2d\u7684\u957f\u7a0b\u65f6\u7a7a\u4f9d\u8d56\uff0c\u5728\u8868\u578b\u9884\u6d4b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684fMRI\u8868\u578b\u9884\u6d4b\u65b9\u6cd5\uff08\u5982CNN\u548cTransformer\uff09\u96be\u4ee5\u6355\u6349fMRI\u6570\u636e\u4e2d\u590d\u6742\u7684\u957f\u7a0b\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002", "method": "\u5f15\u5165BrainMT\uff0c\u4e00\u4e2a\u4e24\u9636\u6bb5\u6df7\u5408\u6846\u67b6\uff1a\u9996\u5148\u4f7f\u7528\u53cc\u5411Mamba\u6a21\u5757\u4ee5\u65f6\u95f4\u4f18\u5148\u626b\u63cf\u65b9\u5f0f\u6355\u83b7\u5168\u5c40\u65f6\u95f4\u4ea4\u4e92\uff1b\u7136\u540e\u5229\u7528Transformer\u6a21\u5757\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u5efa\u6a21Mamba\u5904\u7406\u540e\u7684\u6df1\u5ea6\u7279\u5f81\u4e2d\u7684\u5168\u5c40\u7a7a\u95f4\u5173\u7cfb\u3002", "result": "\u5728UKBioBank\u548cHuman Connectome Project\u4e24\u5927\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cBrainMT\u5728\u5206\u7c7b\uff08\u6027\u522b\u9884\u6d4b\uff09\u548c\u56de\u5f52\uff08\u8ba4\u77e5\u667a\u80fd\u9884\u6d4b\uff09\u4efb\u52a1\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684SOTA\u6027\u80fd\u3002", "conclusion": "BrainMT\u901a\u8fc7\u6709\u6548\u6574\u5408Mamba\u548cTransformer\u67b6\u6784\uff0c\u6210\u529f\u89e3\u51b3\u4e86fMRI\u6570\u636e\u4e2d\u957f\u7a0b\u65f6\u7a7a\u4f9d\u8d56\u5efa\u6a21\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ecefMRI\u8111\u5bb9\u91cf\u9884\u6d4b\u8868\u578b\u6d4b\u91cf\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.22957", "pdf": "https://arxiv.org/pdf/2506.22957", "abs": "https://arxiv.org/abs/2506.22957", "authors": ["Younwoo Choi", "Changling Li", "Yongjin Yang", "Zhijing Jin"], "title": "Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "comment": null, "summary": "As large language models (LLMs) are increasingly integrated into multi-agent\nand human-AI systems, understanding their awareness of both self-context and\nconversational partners is essential for ensuring reliable performance and\nrobust safety. While prior work has extensively studied situational awareness\nwhich refers to an LLM's ability to recognize its operating phase and\nconstraints, it has largely overlooked the complementary capacity to identify\nand adapt to the identity and characteristics of a dialogue partner. In this\npaper, we formalize this latter capability as interlocutor awareness and\npresent the first systematic evaluation of its emergence in contemporary LLMs.\nWe examine interlocutor inference across three dimensions-reasoning patterns,\nlinguistic style, and alignment preferences-and show that LLMs reliably\nidentify same-family peers and certain prominent model families, such as GPT\nand Claude. To demonstrate its practical significance, we develop three case\nstudies in which interlocutor awareness both enhances multi-LLM collaboration\nthrough prompt adaptation and introduces new alignment and safety\nvulnerabilities, including reward-hacking behaviors and increased jailbreak\nsusceptibility. Our findings highlight the dual promise and peril of\nidentity-sensitive behavior in LLMs, underscoring the need for further\nunderstanding of interlocutor awareness and new safeguards in multi-agent\ndeployments. Our code is open-sourced at\nhttps://github.com/younwoochoi/InterlocutorAwarenessLLM.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bc6\u522b\u5bf9\u8bdd\u4f19\u4f34\u8eab\u4efd\u548c\u7279\u5f81\u7684\u80fd\u529b\uff0c\u5373\u201c\u5bf9\u8bdd\u8005\u610f\u8bc6\u201d\uff0c\u53d1\u73b0\u5176\u65e2\u80fd\u589e\u5f3a\u591aLLM\u534f\u4f5c\uff0c\u4e5f\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u968f\u7740LLMs\u65e5\u76ca\u6574\u5408\u5230\u591a\u667a\u80fd\u4f53\u548c\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u4e2d\uff0c\u7406\u89e3\u5b83\u4eec\u5bf9\u81ea\u8eab\u548c\u5bf9\u8bdd\u4f19\u4f34\u7684\u8ba4\u77e5\u5bf9\u4e8e\u786e\u4fdd\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u60c5\u5883\u610f\u8bc6\uff0c\u800c\u5ffd\u7565\u4e86LLM\u8bc6\u522b\u5e76\u9002\u5e94\u5bf9\u8bdd\u4f19\u4f34\u8eab\u4efd\u548c\u7279\u5f81\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76\u5c06\u6b64\u80fd\u529b\u5b9a\u4e49\u4e3a\u201c\u5bf9\u8bdd\u8005\u610f\u8bc6\u201d\uff0c\u5e76\u9996\u6b21\u5bf9\u5176\u5728\u5f53\u4ee3LLMs\u4e2d\u7684\u51fa\u73b0\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002\u901a\u8fc7\u63a8\u7406\u6a21\u5f0f\u3001\u8bed\u8a00\u98ce\u683c\u548c\u5bf9\u9f50\u504f\u597d\u4e09\u4e2a\u7ef4\u5ea6\u8003\u5bdf\u5bf9\u8bdd\u8005\u63a8\u7406\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u63d0\u793a\u9002\u5e94\u548c\u5b89\u5168\u6f0f\u6d1e\uff08\u5982\u5956\u52b1\u6b3a\u9a97\u548c\u8d8a\u72f1\uff09\u65b9\u9762\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cLLMs\u80fd\u591f\u53ef\u9760\u5730\u8bc6\u522b\u540c\u5bb6\u65cf\u6a21\u578b\u4ee5\u53caGPT\u548cClaude\u7b49\u77e5\u540d\u6a21\u578b\u5bb6\u65cf\u3002\u5bf9\u8bdd\u8005\u610f\u8bc6\u65e2\u80fd\u901a\u8fc7\u63d0\u793a\u9002\u5e94\u589e\u5f3a\u591aLLM\u534f\u4f5c\uff0c\u4e5f\u53ef\u80fd\u5f15\u5165\u65b0\u7684\u5bf9\u9f50\u548c\u5b89\u5168\u6f0f\u6d1e\uff0c\u5305\u62ec\u5956\u52b1\u4f5c\u5f0a\u884c\u4e3a\u548c\u8d8a\u72f1\u6613\u611f\u6027\u589e\u52a0\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86LLMs\u4e2d\u8eab\u4efd\u654f\u611f\u884c\u4e3a\u7684\u53cc\u91cd\u6027\u2014\u2014\u65e2\u6709\u524d\u666f\u4e5f\u6709\u98ce\u9669\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u8fdb\u4e00\u6b65\u7406\u89e3\u5bf9\u8bdd\u8005\u610f\u8bc6\uff0c\u5e76\u5728\u591a\u667a\u80fd\u4f53\u90e8\u7f72\u4e2d\u5f00\u53d1\u65b0\u7684\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u3002"}}
{"id": "2506.23964", "pdf": "https://arxiv.org/pdf/2506.23964", "abs": "https://arxiv.org/abs/2506.23964", "authors": ["Hongyu H\u00e8", "Minhao Jin", "Maria Apostolaki"], "title": "Learning Constraints Directly from Network Data", "categories": ["cs.NI", "cs.LG", "C.2.3; I.2.6; I.2.3"], "comment": "13 pages, 15 figures", "summary": "Network data conforms to a wide range of rules that arise from protocols,\ndesign principles, and deployment decisions (e.g., a packet's queuing delay\nmust be less than its end-to-end delay). Formalizing such rules as logic\nconstraints can (i) improve the quality of synthetic data, (ii) reduce the\nbrittleness of machine learning (ML) models, and (iii) improve semantic\nunderstanding of network measurements. However, these benefits remain out of\nreach if rule extraction is manual or solely reliant on ML, as both approaches\nyield incomplete, unreliable, and/or inaccurate rules.\n  This paper formulates rule extraction as a constraint modeling problem and\nintroduces NetNomos that learns propositional logic constraints directly from\nraw network measurements. Constraint modeling in this domain is uniquely\nchallenging due to the scale of the data, the inherent learning complexity and\npassive environment, and the lack of ground truth supervision. NetNomos\naddresses these challenges via a lattice-based search structured by constraint\nspecificity and succinctness. Our approach reduces learning complexity from\nsuperquadratic to logarithmic and enables efficient traversal in combinatorial\nsearch space.\n  Our evaluations on diverse network datasets show that NetNomos learns all\nbenchmark rules, including those associated with as little as 0.01% of data\npoints, in under three hours. In contrast, baseline methods discover less than\n25% of the rules and require several days to run. Through three case studies,\nwe show that: NetNomos (i) finds rule violations in the outputs of all seven\nsynthetic traffic generators, hence can be used to assess and guide their\ngeneration process; (ii) detects semantic differences in traffic, hence can be\nused for anomaly detection; and (iii) automatically finds rules used for\ntelemetry imputation, hence can support monitoring through inference.", "AI": {"tldr": "NetNomos\u662f\u4e00\u79cd\u4ece\u7f51\u7edc\u6d4b\u91cf\u6570\u636e\u4e2d\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u5b66\u4e60\u547d\u9898\u903b\u8f91\u89c4\u5219\u7684\u65b0\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u624b\u52a8\u6216\u7eaf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u5305\u62ec\u5408\u6210\u6570\u636e\u8bc4\u4f30\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u9065\u6d4b\u63a8\u65ad\u3002", "motivation": "\u7f51\u7edc\u6570\u636e\u89c4\u5219\u7684\u5f62\u5f0f\u5316\uff08\u4f5c\u4e3a\u903b\u8f91\u7ea6\u675f\uff09\u5bf9\u63d0\u9ad8\u5408\u6210\u6570\u636e\u8d28\u91cf\u3001\u964d\u4f4e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8106\u5f31\u6027\u53ca\u589e\u5f3a\u7f51\u7edc\u6d4b\u91cf\u8bed\u4e49\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u89c4\u5219\u63d0\u53d6\u65b9\u6cd5\uff08\u624b\u52a8\u6216\u4ec5\u4f9d\u8d56\u673a\u5668\u5b66\u4e60\uff09\u901a\u5e38\u4e0d\u5b8c\u6574\u3001\u4e0d\u53ef\u9760\u6216\u4e0d\u51c6\u786e\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u6f5c\u5728\u76ca\u5904\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u672c\u6587\u5c06\u89c4\u5219\u63d0\u53d6\u95ee\u9898\u516c\u5f0f\u5316\u4e3a\u7ea6\u675f\u5efa\u6a21\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86NetNomos\u3002NetNomos\u76f4\u63a5\u4ece\u539f\u59cb\u7f51\u7edc\u6d4b\u91cf\u4e2d\u5b66\u4e60\u547d\u9898\u903b\u8f91\u7ea6\u675f\u3002\u4e3a\u5e94\u5bf9\u6570\u636e\u89c4\u6a21\u5927\u3001\u5b66\u4e60\u590d\u6742\u6027\u9ad8\u3001\u88ab\u52a8\u73af\u5883\u548c\u7f3a\u4e4f\u771f\u5b9e\u6807\u7b7e\u7b49\u6311\u6218\uff0cNetNomos\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u683c\u7684\u641c\u7d22\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u7ea6\u675f\u7684\u7279\u5f02\u6027\u548c\u7b80\u6d01\u6027\u8fdb\u884c\u7ed3\u6784\u5316\uff0c\u4ece\u800c\u5c06\u5b66\u4e60\u590d\u6742\u5ea6\u4ece\u8d85\u4e8c\u6b21\u65b9\u964d\u4f4e\u5230\u5bf9\u6570\u7ea7\u522b\uff0c\u5e76\u80fd\u5728\u7ec4\u5408\u641c\u7d22\u7a7a\u95f4\u4e2d\u9ad8\u6548\u904d\u5386\u3002", "result": "\u5728\u591a\u6837\u5316\u7f51\u7edc\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cNetNomos\u80fd\u5728\u4e09\u5c0f\u65f6\u5185\u5b66\u4e60\u6240\u6709\u57fa\u51c6\u89c4\u5219\uff08\u5305\u62ec\u4ec5\u5173\u80540.01%\u6570\u636e\u70b9\u7684\u89c4\u5219\uff09\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u57fa\u7ebf\u65b9\u6cd5\u53d1\u73b0\u7684\u89c4\u5219\u4e0d\u523025%\uff0c\u4e14\u9700\u6570\u5929\u8fd0\u884c\u65f6\u95f4\u3002\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\uff0cNetNomos\u8bc1\u660e\u5176\u80fd\uff1a(i) \u53d1\u73b0\u6240\u6709\u4e03\u79cd\u5408\u6210\u6d41\u91cf\u751f\u6210\u5668\u8f93\u51fa\u4e2d\u7684\u89c4\u5219\u8fdd\u53cd\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30\u548c\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\uff1b(ii) \u68c0\u6d4b\u6d41\u91cf\u4e2d\u7684\u8bed\u4e49\u5dee\u5f02\uff0c\u53ef\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\uff1b\u4ee5\u53ca (iii) \u81ea\u52a8\u53d1\u73b0\u7528\u4e8e\u9065\u6d4b\u63a8\u65ad\u7684\u89c4\u5219\uff0c\u4ece\u800c\u652f\u6301\u901a\u8fc7\u63a8\u7406\u8fdb\u884c\u76d1\u63a7\u3002", "conclusion": "NetNomos\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u7684\u7f51\u7edc\u89c4\u5219\u81ea\u52a8\u63d0\u53d6\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u5176\u5b66\u4e60\u5230\u7684\u89c4\u5219\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u53ef\u6709\u6548\u7528\u4e8e\u8bc4\u4f30\u548c\u6307\u5bfc\u5408\u6210\u6570\u636e\u751f\u6210\u3001\u5b9e\u73b0\u7f51\u7edc\u5f02\u5e38\u68c0\u6d4b\uff0c\u4ee5\u53ca\u652f\u6301\u57fa\u4e8e\u63a8\u7406\u7684\u7f51\u7edc\u76d1\u63a7\uff0c\u4ece\u800c\u6df1\u5316\u5bf9\u7f51\u7edc\u884c\u4e3a\u7684\u7406\u89e3\u548c\u7ba1\u7406\u3002"}}
{"id": "2506.22716", "pdf": "https://arxiv.org/pdf/2506.22716", "abs": "https://arxiv.org/abs/2506.22716", "authors": ["Dujian Ding", "Ankur Mallick", "Shaokun Zhang", "Chi Wang", "Daniel Madrigal", "Mirian Del Carmen Hipolito Garcia", "Menglin Xia", "Laks V. S. Lakshmanan", "Qingyun Wu", "Victor R\u00fchle"], "title": "BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "comment": "Accepted to ICML 2025 (main conference)", "summary": "Large language models (LLMs) are powerful tools but are often expensive to\ndeploy at scale. LLM query routing mitigates this by dynamically assigning\nqueries to models of varying cost and quality to obtain a desired trade-off.\nPrior query routing approaches generate only one response from the selected\nmodel and a single response from a small (inexpensive) model was often not good\nenough to beat a response from a large (expensive) model due to which they end\nup overusing the large model and missing out on potential cost savings.\nHowever, it is well known that for small models, generating multiple responses\nand selecting the best can enhance quality while remaining cheaper than a\nsingle large-model response. We leverage this idea to propose BEST-Route, a\nnovel routing framework that chooses a model and the number of responses to\nsample from it based on query difficulty and the quality thresholds.\nExperiments on real-world datasets demonstrate that our method reduces costs by\nup to 60% with less than 1% performance drop.", "AI": {"tldr": "\u4e00\u4e2a\u540d\u4e3aBEST-Route\u7684\u65b0\u578bLLM\u67e5\u8be2\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6a21\u578b\u548c\u91c7\u6837\u54cd\u5e94\u6570\u91cf\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u90e8\u7f72\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u67e5\u8be2\u8def\u7531\u65b9\u6cd5\u5728\u9762\u5bf9\u5c0f\u578b\uff08\u5ec9\u4ef7\uff09\u6a21\u578b\u65f6\uff0c\u7531\u4e8e\u4ec5\u751f\u6210\u4e00\u4e2a\u54cd\u5e94\uff0c\u5176\u8d28\u91cf\u5f80\u5f80\u4e0d\u8db3\u4ee5\u5ab2\u7f8e\u5927\u578b\uff08\u6602\u8d35\uff09\u6a21\u578b\u3002\u8fd9\u5bfc\u81f4\u5b83\u4eec\u8fc7\u5ea6\u4f9d\u8d56\u5927\u578b\u6a21\u578b\uff0c\u672a\u80fd\u5145\u5206\u5b9e\u73b0\u6f5c\u5728\u7684\u6210\u672c\u8282\u7ea6\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86BEST-Route\u6846\u67b6\uff0c\u5b83\u5229\u7528\u4e86\u5c0f\u578b\u6a21\u578b\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u54cd\u5e94\u5e76\u4ece\u4e2d\u9009\u62e9\u6700\u4f73\u54cd\u5e94\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u6210\u672c\u4f18\u52bf\u7684\u540c\u65f6\u63d0\u5347\u8d28\u91cf\u7684\u539f\u7406\u3002\u8be5\u6846\u67b6\u6839\u636e\u67e5\u8be2\u96be\u5ea6\u548c\u9884\u8bbe\u7684\u8d28\u91cf\u9608\u503c\uff0c\u52a8\u6001\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u4ee5\u53ca\u9700\u8981\u91c7\u6837\u7684\u54cd\u5e94\u6570\u91cf\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBEST-Route\u65b9\u6cd5\u5728\u6027\u80fd\u4e0b\u964d\u4e0d\u52301%\u7684\u60c5\u51b5\u4e0b\uff0c\u6210\u529f\u5c06\u90e8\u7f72\u6210\u672c\u964d\u4f4e\u4e86\u9ad8\u8fbe60%\u3002", "conclusion": "BEST-Route\u901a\u8fc7\u4f18\u5316LLM\u67e5\u8be2\u8def\u7531\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u90e8\u7f72LLM\u7684\u6210\u672c\u6548\u76ca\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6210\u672c\u5927\u5e45\u964d\u4f4e\u4e0e\u6027\u80fd\u51e0\u4e4e\u65e0\u635f\u7684\u53cc\u91cd\u76ee\u6807\u3002"}}
{"id": "2506.23504", "pdf": "https://arxiv.org/pdf/2506.23504", "abs": "https://arxiv.org/abs/2506.23504", "authors": ["Bosubabu Sambana", "Kotamsetty Geethika Devi", "Bandi Rajeswara Reddy", "Galeti Mohammad Hussain", "Gownivalla Siddartha"], "title": "Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM", "categories": ["cs.AI"], "comment": "6 Pages, 7 Figures", "summary": "The recent development of advanced machine learning methods for hybrid models\nhas greatly addressed the need for the correct prediction of electrical prices.\nThis method combines AlexNet and LSTM algorithms, which are used to introduce a\nnew model with higher accuracy in price forecasting. Despite RNN and ANN being\neffective, they often fail to deal with forex time sequence data. The\ntraditional methods do not accurately forecast the prices. These traditional\nmethods only focus on demand and price which leads to insufficient analysis of\ndata. To address this issue, using the hybrid approach, which focuses on\nexternal variables that also effect the predicted prices. Nevertheless, due to\nAlexNet's excellent feature extraction and LSTM's learning sequential patterns,\nthe prediction accuracy is vastly increased. The model is built on the past\ndata, which has been supplied with the most significant elements like demand,\ntemperature, sunlight, and rain. For example, the model applies methods, such\nas minimum-maximum scaling and a time window, to predict the electricity prices\nof the future. The results show that this hybrid model is good than the\nstandalone ones in terms of accuracy. Although we got our accuracy rating of\n97.08, it shows higher accompaniments than remaining models RNN and ANN with\naccuracies of 96.64 and 96.63 respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408AlexNet\u548cLSTM\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u7535\u4ef7\u9884\u6d4b\uff0c\u901a\u8fc7\u6574\u5408\u5916\u90e8\u53d8\u91cf\uff0c\u5728\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u7535\u4ef7\u9884\u6d4b\u65b9\u6cd5\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5206\u6790\u6570\u636e\uff08\u4ec5\u5173\u6ce8\u9700\u6c42\u548c\u4ef7\u683c\uff09\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u51c6\u786e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5e76\u8003\u8651\u5916\u90e8\u53d8\u91cf\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86AlexNet\uff08\u7528\u4e8e\u7279\u5f81\u63d0\u53d6\uff09\u548cLSTM\uff08\u7528\u4e8e\u5b66\u4e60\u5e8f\u5217\u6a21\u5f0f\uff09\u3002\u8be5\u6a21\u578b\u5229\u7528\u5386\u53f2\u6570\u636e\uff0c\u5e76\u7eb3\u5165\u4e86\u9700\u6c42\u3001\u6e29\u5ea6\u3001\u65e5\u7167\u548c\u964d\u96e8\u7b49\u5173\u952e\u5916\u90e8\u53d8\u91cf\u3002\u6570\u636e\u9884\u5904\u7406\u5305\u62ec\u6700\u5c0f-\u6700\u5927\u5f52\u4e00\u5316\u548c\u65f6\u95f4\u7a97\u53e3\u5e94\u7528\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u6a21\u578b\u5728\u7535\u4ef7\u9884\u6d4b\u4e2d\u53d6\u5f97\u4e8697.08%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u9ad8\u4e8e\u5355\u72ec\u7684RNN\uff0896.64%\uff09\u548cANN\uff0896.63%\uff09\u6a21\u578b\u3002", "conclusion": "\u7ed3\u5408AlexNet\u548cLSTM\u7684\u6df7\u5408\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347\u7535\u4ef7\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u548c\u6574\u5408\u5916\u90e8\u53d8\u91cf\u5728\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.22624", "pdf": "https://arxiv.org/pdf/2506.22624", "abs": "https://arxiv.org/abs/2506.22624", "authors": ["Zuyao You", "Zuxuan Wu"], "title": "Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "We present Seg-R1, a preliminary exploration of using reinforcement learning\n(RL) to enhance the pixel-level understanding and reasoning capabilities of\nlarge multimodal models (LMMs). Starting with foreground segmentation tasks,\nspecifically camouflaged object detection (COD) and salient object detection\n(SOD), our approach enables the LMM to generate point and bounding box prompts\nin the next-token fashion, which are then used to guide SAM2 in producing\nsegmentation masks. We introduce Group Relative Policy Optimization (GRPO) into\nthe segmentation domain, equipping the LMM with pixel-level comprehension\nthrough a carefully designed training strategy. Notably, Seg-R1 achieves\nremarkable performance with purely RL-based training, achieving .873 S-measure\non COD10K without complex model modification. Moreover, we found that pure RL\ntraining demonstrates strong open-world generalization. Despite being trained\nsolely on foreground segmentation image-mask pairs without text supervision,\nSeg-R1 achieves impressive zero-shot performance on referring segmentation and\nreasoning segmentation tasks, with 71.4 cIoU on RefCOCOg test and 56.7 gIoU on\nReasonSeg test, outperforming models fully supervised on these datasets.", "AI": {"tldr": "Seg-R1\u662f\u4e00\u4e2a\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u63d0\u5347\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u50cf\u7d20\u7ea7\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u7684\u521d\u6b65\u63a2\u7d22\u3002\u5b83\u901a\u8fc7RL\u751f\u6210\u63d0\u793a\u6307\u5bfcSAM2\u8fdb\u884c\u524d\u666f\u5206\u5272\uff0c\u5e76\u5728\u7eafRL\u8bad\u7ec3\u4e0b\u5728\u591a\u4e2a\u5206\u5272\u4efb\u52a1\u4e0a\u53d6\u5f97\u5353\u8d8a\u6027\u80fd\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5f00\u653e\u4e16\u754c\u6cdb\u5316\u80fd\u529b\u548c\u96f6\u6837\u672c\u8868\u73b0\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u76d1\u7763\u6a21\u578b\u3002", "motivation": "\u65e8\u5728\u63a2\u7d22\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u589e\u5f3a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u7684\u50cf\u7d20\u7ea7\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5f15\u5165Seg-R1\uff0c\u4e00\u4e2a\u57fa\u4e8eRL\u7684\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u4f7fLMM\u80fd\u591f\u4ee5\u201c\u4e0b\u4e00\u4e2atoken\u201d\u7684\u65b9\u5f0f\u751f\u6210\u70b9\u548c\u8fb9\u754c\u6846\u63d0\u793a\uff0c\u8fdb\u800c\u5f15\u5bfcSAM2\u751f\u6210\u5206\u5272\u63a9\u7801\u3002\u6a21\u578b\u5c06Group Relative Policy Optimization (GRPO)\u5f15\u5165\u5206\u5272\u9886\u57df\uff0c\u5e76\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u7b56\u7565\u8d4b\u4e88LMM\u50cf\u7d20\u7ea7\u7406\u89e3\u80fd\u529b\u3002\u8bad\u7ec3\u4ec5\u4f7f\u7528\u524d\u666f\u5206\u5272\u56fe\u50cf-\u63a9\u7801\u5bf9\uff0c\u65e0\u6587\u672c\u76d1\u7763\u3002", "result": "Seg-R1\u5728\u7eafRL\u8bad\u7ec3\u4e0b\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\uff1a\u5728COD10K\u6570\u636e\u96c6\u4e0aS-measure\u8fbe\u52300.873\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u7eafRL\u8bad\u7ec3\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5f00\u653e\u4e16\u754c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u96f6\u6837\u672c\u6307\u4ee3\u5206\u5272\uff08RefCOCOg test\uff09\u4e0a\u8fbe\u523071.4 cIoU\uff0c\u5728\u96f6\u6837\u672c\u63a8\u7406\u5206\u5272\uff08ReasonSeg test\uff09\u4e0a\u8fbe\u523056.7 gIoU\uff0c\u6027\u80fd\u8d85\u8d8a\u4e86\u5728\u8fd9\u4e9b\u6570\u636e\u96c6\u4e0a\u5b8c\u5168\u76d1\u7763\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u6709\u6548\u63d0\u5347LMM\u7684\u50cf\u7d20\u7ea7\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u4ec5\u901a\u8fc7\u524d\u666f\u5206\u5272\u6570\u636e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u5353\u8d8a\u7684\u5f00\u653e\u4e16\u754c\u6cdb\u5316\u548c\u96f6\u6837\u672c\u6027\u80fd\uff0c\u9884\u793a\u7740RL\u5728\u590d\u6742\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2506.22977", "pdf": "https://arxiv.org/pdf/2506.22977", "abs": "https://arxiv.org/abs/2506.22977", "authors": ["Asen Dotsinski", "Udit Thakur", "Marko Ivanov", "Mohammad Hafeez Khan", "Maria Heuss"], "title": "On the Generalizability of \"Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals\"", "categories": ["cs.CL", "cs.LG"], "comment": "22 pages, 25 figures. For an interactive dashboard with all figures,\n  see https://comp-mech-generalizability.streamlit.app/ . For the accompanying\n  code, see https://github.com/asendotsinski/comp-mech-generalizability . To be\n  published in proceedings of the 2025 Machine Learning Reproducibility\n  Challenge", "summary": "We present a reproduction study of \"Competition of Mechanisms: Tracing How\nLanguage Models Handle Facts and Counterfactuals\" (Ortu et al., 2024), which\ninvestigates competition of mechanisms in language models between factual\nrecall and counterfactual in-context repetition. Our study successfully\nreproduces their primary findings regarding the localization of factual and\ncounterfactual information, the dominance of attention blocks in mechanism\ncompetition, and the specialization of attention heads in handling competing\ninformation. We reproduce their results on both GPT-2 (Radford et al., 2019)\nand Pythia 6.9B (Biderman et al., 2023). We extend their work in three\nsignificant directions. First, we explore the generalizability of these\nfindings to even larger models by replicating the experiments on Llama 3.1 8B\n(Grattafiori et al., 2024), discovering greatly reduced attention head\nspecialization. Second, we investigate the impact of prompt structure by\nintroducing variations where we avoid repeating the counterfactual statement\nverbatim or we change the premise word, observing a marked decrease in the\nlogit for the counterfactual token. Finally, we test the validity of the\nauthors' claims for prompts of specific domains, discovering that certain\ncategories of prompts skew the results by providing the factual prediction\ntoken as part of the subject of the sentence. Overall, we find that the\nattention head ablation proposed in Ortu et al. (2024) is ineffective for\ndomains that are underrepresented in their dataset, and that the effectiveness\nvaries based on model architecture, prompt structure, domain and task.", "AI": {"tldr": "\u672c\u6587\u590d\u73b0\u5e76\u6269\u5c55\u4e86Ortu\u7b49\u4eba\uff082024\uff09\u5173\u4e8e\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u5904\u7406\u4e8b\u5b9e\u4e0e\u53cd\u4e8b\u5b9e\u4fe1\u606f\u673a\u5236\u7ade\u4e89\u7684\u7814\u7a76\uff0c\u6210\u529f\u590d\u73b0\u5176\u4e3b\u8981\u53d1\u73b0\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u6709\u6548\u6027\u53d7\u6a21\u578b\u3001\u63d0\u793a\u7ed3\u6784\u3001\u9886\u57df\u548c\u4efb\u52a1\u5f71\u54cd\u3002", "motivation": "\u65e8\u5728\u590d\u73b0\u5e76\u9a8c\u8bc1Ortu\u7b49\u4eba\uff082024\uff09\u63d0\u51fa\u7684\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u4e8b\u5b9e\u53ec\u56de\u4e0e\u53cd\u4e8b\u5b9e\u8bed\u5883\u91cd\u590d\u673a\u5236\u7ade\u4e89\u7684\u53d1\u73b0\uff0c\u5e76\u63a2\u7d22\u8fd9\u4e9b\u53d1\u73b0\u7684\u6cdb\u5316\u6027\u3001\u63d0\u793a\u7ed3\u6784\u5f71\u54cd\u4ee5\u53ca\u5728\u4e0d\u540c\u9886\u57df\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u590d\u73b0\u4e86\u539f\u59cb\u7814\u7a76\uff0c\u5e76\u5728GPT-2\u548cPythia 6.9B\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002\u5c06\u5b9e\u9a8c\u6269\u5c55\u5230Llama 3.1 8B\u4ee5\u6d4b\u8bd5\u6cdb\u5316\u6027\u3002\u901a\u8fc7\u6539\u53d8\u63d0\u793a\u7ed3\u6784\uff08\u5982\u907f\u514d\u9010\u5b57\u91cd\u590d\u3001\u6539\u53d8\u524d\u63d0\u8bcd\uff09\u6765\u63a2\u7a76\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u8fd8\u5728\u7279\u5b9a\u9886\u57df\u63d0\u793a\u4e0a\u6d4b\u8bd5\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u6210\u529f\u590d\u73b0\u4e86\u4e8b\u5b9e/\u53cd\u4e8b\u5b9e\u4fe1\u606f\u5b9a\u4f4d\u3001\u6ce8\u610f\u529b\u5757\u4e3b\u5bfc\u53ca\u6ce8\u610f\u529b\u5934\u4e13\u95e8\u5316\u7684\u4e3b\u8981\u53d1\u73b0\u3002\u5728Llama 3.1 8B\u4e0a\uff0c\u6ce8\u610f\u529b\u5934\u4e13\u95e8\u5316\u663e\u8457\u964d\u4f4e\u3002\u6539\u53d8\u63d0\u793a\u7ed3\u6784\u5bfc\u81f4\u53cd\u4e8b\u5b9etoken\u7684\u5bf9\u6570\u51e0\u7387\u663e\u8457\u4e0b\u964d\u3002\u67d0\u4e9b\u9886\u57df\u63d0\u793a\u4f1a\u56e0\u63d0\u4f9b\u4e8b\u5b9e\u9884\u6d4btoken\u800c\u4f7f\u7ed3\u679c\u51fa\u73b0\u504f\u5dee\u3002\u53d1\u73b0Ortu\u7b49\u4eba\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u5934\u6d88\u878d\u65b9\u6cd5\u5bf9\u6570\u636e\u96c6\u4e2d\u6b20\u4ee3\u8868\u7684\u9886\u57df\u65e0\u6548\uff0c\u4e14\u5176\u6709\u6548\u6027\u56e0\u6a21\u578b\u67b6\u6784\u3001\u63d0\u793a\u7ed3\u6784\u3001\u9886\u57df\u548c\u4efb\u52a1\u800c\u5f02\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5904\u7406\u4e8b\u5b9e\u4e0e\u53cd\u4e8b\u5b9e\u4fe1\u606f\u673a\u5236\u7684\u6709\u6548\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6a21\u578b\u67b6\u6784\u3001\u63d0\u793a\u7ed3\u6784\u3001\u6570\u636e\u9886\u57df\u548c\u5177\u4f53\u4efb\u52a1\u3002\u539f\u59cb\u7814\u7a76\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u5934\u6d88\u878d\u65b9\u6cd5\u5e76\u975e\u666e\u904d\u6709\u6548\uff0c\u5c24\u5176\u5728\u7279\u5b9a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\u3002"}}
{"id": "2506.22471", "pdf": "https://arxiv.org/pdf/2506.22471", "abs": "https://arxiv.org/abs/2506.22471", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Umer", "Ahsan Bilal", "Muhammad Ali Jamshed", "John M. Cioffi"], "title": "Continual Learning for Wireless Channel Prediction", "categories": ["eess.SP", "cs.NI"], "comment": "Accepted at ICML Workshop on ML4Wireless", "summary": "Modern 5G/6G deployments routinely face cross-configuration handovers--users\ntraversing cells with different antenna layouts, carrier frequencies, and\nscattering statistics--which inflate channel-prediction NMSE by $37.5\\%$ on\naverage when models are naively fine-tuned. The proposed improvement frames\nthis mismatch as a continual-learning problem and benchmarks three adaptation\nfamilies: replay with loss-aware reservoirs, synaptic-importance\nregularization, and memory-free learning-without-forgetting. Across three\nrepresentative 3GPP urban micro scenarios, the best replay and regularization\nschemes cut the high-SNR error floor by up to 2~dB ($\\approx 35\\%$), while even\nthe lightweight distillation recovers up to $30\\%$ improvement over baseline\nhandover prediction schemes. These results show that targeted rehearsal and\nparameter anchoring are essential for handover-robust CSI prediction and\nsuggest a clear migration path for embedding continual-learning hooks into\ncurrent channel prediction efforts in 3GPP--NR and O-RAN. The full codebase can\nbe found at\nhttps://github.com/ahmd-mohsin/continual-learning-channel-prediction.git.", "AI": {"tldr": "\u9488\u5bf95G/6G\u8de8\u914d\u7f6e\u5207\u6362\u5bfc\u81f4\u7684\u4fe1\u9053\u9884\u6d4b\u8bef\u5dee\u5927\u95ee\u9898\uff0c\u672c\u6587\u5c06\u5176\u6846\u5b9a\u4e3a\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u6d4b\u8bd5\u4e86\u591a\u79cd\u9002\u5e94\u6027\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e3a\u672a\u6765\u90e8\u7f72\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u4ee35G/6G\u7f51\u7edc\u5728\u7528\u6237\u8de8\u914d\u7f6e\u5c0f\u533a\u5207\u6362\u65f6\uff08\u5929\u7ebf\u5e03\u5c40\u3001\u9891\u7387\u3001\u6563\u5c04\u7edf\u8ba1\u4e0d\u540c\uff09\uff0c\u4fe1\u9053\u9884\u6d4b\u7684\u5747\u65b9\u8bef\u5dee\uff08NMSE\uff09\u5e73\u5747\u589e\u52a037.5%\uff0c\u4f20\u7edf\u7684\u6734\u7d20\u5fae\u8c03\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\uff0c\u6025\u9700\u63d0\u5347\u9884\u6d4b\u9c81\u68d2\u6027\u3002", "method": "\u5c06\u8de8\u914d\u7f6e\u5207\u6362\u4e0b\u7684\u4fe1\u9053\u9884\u6d4b\u5931\u914d\u95ee\u9898\u5b9a\u4e49\u4e3a\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u9002\u5e94\u6027\u5b66\u4e60\u65b9\u6cd5\u65cf\uff1a1) \u7ed3\u5408\u635f\u5931\u611f\u77e5\u5b58\u50a8\u7684\u56de\u653e\u673a\u5236\uff1b2) \u57fa\u4e8e\u7a81\u89e6\u91cd\u8981\u6027\u7684\u6b63\u5219\u5316\uff1b3) \u65e0\u8bb0\u5fc6\u7684\u201c\u4e0d\u9057\u5fd8\u5b66\u4e60\u201d\uff08\u84b8\u998f\uff09\u3002", "result": "\u5728\u4e09\u4e2a3GPP\u57ce\u5e02\u5fae\u8702\u7a9d\u573a\u666f\u4e0b\uff0c\u6700\u4f73\u7684\u56de\u653e\u548c\u6b63\u5219\u5316\u65b9\u6848\u5728\u9ad8\u4fe1\u566a\u6bd4\u4e0b\u5c06\u8bef\u5dee\u5e95\u7ebf\u964d\u4f4e\u4e86\u9ad8\u8fbe2dB\uff08\u7ea635%\uff09\uff1b\u5373\u4f7f\u662f\u8f7b\u91cf\u7ea7\u7684\u84b8\u998f\u65b9\u6cd5\u4e5f\u6bd4\u57fa\u7ebf\u5207\u6362\u9884\u6d4b\u65b9\u6848\u63d0\u5347\u4e86\u9ad8\u8fbe30%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6709\u9488\u5bf9\u6027\u7684\u56de\u653e\u548c\u53c2\u6570\u951a\u5b9a\u5bf9\u4e8e\u5b9e\u73b0\u5207\u6362\u9c81\u68d2\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e3a\u57283GPP-NR\u548cO-RAN\u4e2d\u73b0\u6709\u4fe1\u9053\u9884\u6d4b\u5de5\u4f5c\u4e2d\u5d4c\u5165\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u8def\u5f84\u3002"}}
{"id": "2506.22732", "pdf": "https://arxiv.org/pdf/2506.22732", "abs": "https://arxiv.org/abs/2506.22732", "authors": ["Hao Shu", "Jicheng Li", "Tianyv Lei", "Lijun Sun"], "title": "Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery", "categories": ["cs.LG", "eess.SP", "stat.ML"], "comment": null, "summary": "In real-world scenarios, spatiotemporal traffic data frequently experiences\ndual degradation from missing values and noise caused by sensor malfunctions\nand communication failures. Therefore, effective data recovery methods are\nessential to ensure the reliability of downstream data-driven applications.\nwhile classical tensor completion methods have been widely adopted, they are\nincapable of modeling noise, making them unsuitable for complex scenarios\ninvolving simultaneous data missingness and noise interference. Existing Robust\nTensor Completion (RTC) approaches offer potential solutions by separately\nmodeling the actual tensor data and noise. However, their effectiveness is\noften constrained by the over-relaxation of convex rank surrogates and the\nsuboptimal utilization of local consistency, leading to inadequate model\naccuracy. To address these limitations, we first introduce the tensor L1-L2\nnorm, a novel non-convex tensor rank surrogate that functions as an effective\nlow-rank representation tool. Leveraging an advanced feature fusion strategy,\nwe further develop the gradient tensor L1-L2 norm by incorporating the tensor\nL1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear\nL1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via\nGradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully\nexploits both global low-rankness and local consistency without trade-off\nparameter, but also effectively handles the dual degradation challenges of\nmissing data and noise in traffic data. Extensive experiments conducted on\nmultiple real-world traffic datasets demonstrate that the RTC-GTNLN model\nconsistently outperforms existing state-of-the-art methods in complex recovery\nscenarios involving simultaneous missing values and noise.", "AI": {"tldr": "\u9488\u5bf9\u4ea4\u901a\u65f6\u7a7a\u6570\u636e\u7f3a\u5931\u4e0e\u566a\u58f0\u53cc\u91cd\u9000\u5316\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faRTC-GTNLN\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5f15\u5165\u975e\u51f8\u5f20\u91cfL1-L2\u8303\u6570\uff0c\u6709\u6548\u878d\u5408\u5168\u5c40\u4f4e\u79e9\u6027\u548c\u5c40\u90e8\u4e00\u81f4\u6027\uff0c\u5728\u771f\u5b9e\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "motivation": "\u4ea4\u901a\u65f6\u7a7a\u6570\u636e\u5e38\u56e0\u4f20\u611f\u5668\u6545\u969c\u548c\u901a\u4fe1\u4e2d\u65ad\u9762\u4e34\u7f3a\u5931\u503c\u548c\u566a\u58f0\u7684\u53cc\u91cd\u9000\u5316\uff0c\u5f71\u54cd\u4e0b\u6e38\u5e94\u7528\u53ef\u9760\u6027\u3002\u4f20\u7edf\u5f20\u91cf\u8865\u5168\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u566a\u58f0\uff0c\u800c\u73b0\u6709\u9c81\u68d2\u5f20\u91cf\u8865\u5168(RTC)\u65b9\u6cd5\u5b58\u5728\u51f8\u79e9\u66ff\u4ee3\u8fc7\u5ea6\u677e\u5f1b\u548c\u5c40\u90e8\u4e00\u81f4\u6027\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u63d0\u51faRTC-GTNLN\u6a21\u578b\u3002\u9996\u5148\u5f15\u5165\u65b0\u9896\u7684\u975e\u51f8\u5f20\u91cfL1-L2\u8303\u6570\u4f5c\u4e3a\u4f4e\u79e9\u8868\u793a\u5de5\u5177\uff1b\u5176\u6b21\uff0c\u901a\u8fc7\u7279\u5f81\u878d\u5408\u7b56\u7565\uff0c\u5728\u68af\u5ea6\u57df\u5f15\u5165\u68af\u5ea6\u5f20\u91cfL1-L2\u8303\u6570\uff1b\u6700\u540e\u5c06\u6b64\u8303\u6570\u6574\u5408\u5230RTC\u6846\u67b6\u4e2d\uff0c\u4ee5\u5145\u5206\u5229\u7528\u5168\u5c40\u4f4e\u79e9\u6027\u548c\u5c40\u90e8\u4e00\u81f4\u6027\uff0c\u5e76\u6709\u6548\u5904\u7406\u4ea4\u901a\u6570\u636e\u7684\u7f3a\u5931\u548c\u566a\u58f0\u6311\u6218\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRTC-GTNLN\u6a21\u578b\u5728\u540c\u65f6\u5b58\u5728\u7f3a\u5931\u503c\u548c\u566a\u58f0\u7684\u590d\u6742\u6062\u590d\u573a\u666f\u4e2d\uff0c\u6027\u80fd\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684RTC-GTNLN\u6a21\u578b\u901a\u8fc7\u6709\u6548\u5229\u7528\u5168\u5c40\u4f4e\u79e9\u6027\u548c\u5c40\u90e8\u4e00\u81f4\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4ea4\u901a\u6570\u636e\u4e2d\u7f3a\u5931\u503c\u548c\u566a\u58f0\u7684\u53cc\u91cd\u9000\u5316\u6311\u6218\uff0c\u5e76\u5728\u590d\u6742\u6062\u590d\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23517", "pdf": "https://arxiv.org/pdf/2506.23517", "abs": "https://arxiv.org/abs/2506.23517", "authors": ["Selin Dik", "Osman Erdem", "Mehmet Dik"], "title": "Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As the use of AI tools by students has become more prevalent, instructors\nhave started using AI detection tools like GPTZero and QuillBot to detect AI\nwritten text. However, the reliability of these detectors remains uncertain. In\nour study, we focused mostly on the success rate of GPTZero, the most-used AI\ndetector, in identifying AI-generated texts based on different lengths of\nrandomly submitted essays: short (40-100 word count), medium (100-350 word\ncount), and long (350-800 word count). We gathered a data set consisting of\ntwenty-eight AI-generated papers and fifty human-written papers. With this\nrandomized essay data, papers were individually plugged into GPTZero and\nmeasured for percentage of AI generation and confidence. A vast majority of the\nAI-generated papers were detected accurately (ranging from 91-100% AI believed\ngeneration), while the human generated essays fluctuated; there were a handful\nof false positives. These findings suggest that although GPTZero is effective\nat detecting purely AI-generated content, its reliability in distinguishing\nhuman-authored texts is limited. Educators should therefore exercise caution\nwhen relying solely on AI detection tools.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86AI\u68c0\u6d4b\u5de5\u5177GPTZero\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u5b83\u80fd\u6709\u6548\u8bc6\u522bAI\u751f\u6210\u6587\u672c\uff0c\u4f46\u5bf9\u4eba\u7c7b\u64b0\u5199\u6587\u672c\u5b58\u5728\u8bef\u62a5\uff0c\u56e0\u6b64\u5efa\u8bae\u6559\u5e08\u8c28\u614e\u4f7f\u7528\u3002", "motivation": "\u968f\u7740\u5b66\u751fAI\u5de5\u5177\u4f7f\u7528\u7387\u7684\u589e\u52a0\uff0c\u6559\u5e08\u5f00\u59cb\u4f7f\u7528AI\u68c0\u6d4b\u5de5\u5177\uff0c\u4f46\u8fd9\u4e9b\u5de5\u5177\u7684\u53ef\u9760\u6027\u4ecd\u4e0d\u786e\u5b9a\u3002", "method": "\u7814\u7a76\u6536\u96c6\u4e8628\u7bc7AI\u751f\u6210\u6587\u7ae0\u548c50\u7bc7\u4eba\u7c7b\u64b0\u5199\u6587\u7ae0\uff0c\u5e76\u6839\u636e\u957f\u5ea6\uff08\u77ed\u3001\u4e2d\u3001\u957f\uff09\u8fdb\u884c\u5206\u7c7b\u3002\u4f7f\u7528GPTZero\u68c0\u6d4b\u6bcf\u7bc7\u6587\u7ae0\uff0c\u5e76\u8bb0\u5f55AI\u751f\u6210\u767e\u5206\u6bd4\u548c\u7f6e\u4fe1\u5ea6\u3002", "result": "GPTZero\u80fd\u51c6\u786e\u68c0\u6d4b\u51fa\u7edd\u5927\u591a\u6570AI\u751f\u6210\u6587\u7ae0\uff08\u7f6e\u4fe1\u5ea691-100%\uff09\uff0c\u4f46\u5bf9\u4eba\u7c7b\u64b0\u5199\u6587\u7ae0\u7684\u68c0\u6d4b\u7ed3\u679c\u6ce2\u52a8\u8f83\u5927\uff0c\u5e76\u51fa\u73b0\u5c11\u91cf\u8bef\u62a5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1GPTZero\u5728\u68c0\u6d4b\u7eafAI\u751f\u6210\u5185\u5bb9\u65b9\u9762\u6709\u6548\uff0c\u4f46\u5176\u533a\u5206\u4eba\u7c7b\u64b0\u5199\u6587\u672c\u7684\u53ef\u9760\u6027\u6709\u9650\u3002\u56e0\u6b64\uff0c\u6559\u80b2\u5de5\u4f5c\u8005\u5e94\u8c28\u614e\u5355\u72ec\u4f9d\u8d56AI\u68c0\u6d4b\u5de5\u5177\u3002"}}
{"id": "2506.22636", "pdf": "https://arxiv.org/pdf/2506.22636", "abs": "https://arxiv.org/abs/2506.22636", "authors": ["Sotirios Panagiotis Chytas", "Miso Choi", "Hyunwoo J. Kim", "Vikas Singh"], "title": "ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision Language Models (VLMs) show impressive capabilities in integrating and\nreasoning with both visual and language data. But these models make mistakes. A\ncommon finding -- similar to LLMs -- is their tendency to hallucinate, i.e.,\ngenerate plausible sounding text which is not grounded in the visual input, or\nat worst, is contradictory. A growing consensus attributes this behavior to an\nover-reliance on language -- especially as the generation progresses, the model\nsuffers from a ``fading memory effect'' with respect to the provided visual\ninput. We study mechanisms by which this behavior can be controlled.\nSpecifically, using ideas from geometric algebra and relational compositions,\nwe propose the addition of a small, trainable module (named ReCo) on top of any\nVLM -- no other modification is needed. We show that such a lightweight module\nis able to mitigate the fading memory effect on three of the most widely used\nVLMs (InstructBLIP, LlaVA, MiniGPT4), where we see performance improvements on\nmultiple benchmarks. Additionally, we show that our module can be combined with\nmany of the other approaches for reducing hallucination where we achieve\nimproved results for each one.", "AI": {"tldr": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5e38\u56e0\u5bf9\u89c6\u89c9\u8f93\u5165\u7684\u201c\u8bb0\u5fc6\u8870\u9000\u201d\u800c\u4ea7\u751f\u5e7b\u89c9\u3002\u672c\u6587\u63d0\u51faReCo\u6a21\u5757\uff0c\u4e00\u4e2a\u57fa\u4e8e\u51e0\u4f55\u4ee3\u6570\u548c\u5173\u7cfb\u7ec4\u5408\u7684\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u6a21\u5757\uff0c\u80fd\u6709\u6548\u51cf\u8f7b\u591a\u79cdVLM\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u80fd\u4e0e\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5c3d\u7ba1\u80fd\u529b\u51fa\u8272\uff0c\u4f46\u666e\u904d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u751f\u6210\u4e0e\u89c6\u89c9\u8f93\u5165\u4e0d\u7b26\u6216\u77db\u76fe\u7684\u5185\u5bb9\u3002\u8fd9\u79cd\u884c\u4e3a\u4e3b\u8981\u5f52\u56e0\u4e8e\u6a21\u578b\u5bf9\u8bed\u8a00\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u5bfc\u81f4\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5bf9\u89c6\u89c9\u8f93\u5165\u7684\u201c\u8bb0\u5fc6\u8870\u9000\u6548\u5e94\u201d\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u63a7\u5236\u548c\u51cf\u8f7bVLM\u7684\u5e7b\u89c9\u884c\u4e3a\u662f\u672c\u7814\u7a76\u7684\u6838\u5fc3\u52a8\u673a\u3002", "method": "\u672c\u7814\u7a76\u57fa\u4e8e\u51e0\u4f55\u4ee3\u6570\u548c\u5173\u7cfb\u7ec4\u5408\u7684\u7406\u5ff5\uff0c\u8bbe\u8ba1\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aReCo\u7684\u5c0f\u578b\u3001\u53ef\u8bad\u7ec3\u6a21\u5757\u3002\u8be5\u6a21\u5757\u53ef\u76f4\u63a5\u9644\u52a0\u5728\u4efb\u4f55\u73b0\u6709VLM\u4e4b\u4e0a\uff0c\u65e0\u9700\u5bf9VLM\u81ea\u8eab\u8fdb\u884c\u5176\u4ed6\u4fee\u6539\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cReCo\u6a21\u5757\u80fd\u591f\u6709\u6548\u51cf\u8f7bInstructBLIP\u3001LlaVA\u3001MiniGPT4\u7b49\u4e09\u79cd\u4e3b\u6d41VLM\u7684\u201c\u8bb0\u5fc6\u8870\u9000\u6548\u5e94\u201d\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002\u6b64\u5916\uff0cReCo\u6a21\u5757\u8fd8\u53ef\u4ee5\u4e0e\u8bb8\u591a\u5176\u4ed6\u65e8\u5728\u51cf\u5c11\u5e7b\u89c9\u7684\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\uff0c\u4e14\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5b83\u4eec\u7684\u7efc\u5408\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165ReCo\u6a21\u5757\uff0c\u53ef\u4ee5\u6709\u6548\u63a7\u5236VLM\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u589e\u5f3a\u5176\u751f\u6210\u5185\u5bb9\u4e0e\u89c6\u89c9\u8f93\u5165\u7684\u4e00\u81f4\u6027\u3002ReCo\u6a21\u5757\u7684\u8f7b\u91cf\u7ea7\u548c\u826f\u597d\u7684\u517c\u5bb9\u6027\u4f7f\u5176\u6210\u4e3a\u63d0\u5347VLM\u53ef\u9760\u6027\u7684\u4e00\u4e2a\u6709\u524d\u666f\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22978", "pdf": "https://arxiv.org/pdf/2506.22978", "abs": "https://arxiv.org/abs/2506.22978", "authors": ["Yida Zhao", "Hao Xve", "Xiang Hu", "Kewei Tu"], "title": "A Systematic Study of Compositional Syntactic Transformer Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Syntactic language models (SLMs) enhance Transformers by incorporating\nsyntactic biases through the modeling of linearized syntactic parse trees\nalongside surface sentences. This paper focuses on compositional SLMs that are\nbased on constituency parse trees and contain explicit bottom-up composition of\nconstituent representations. We identify key aspects of design choices in\nexisting compositional SLMs and propose a unified framework encompassing both\nexisting models and novel variants. We conduct a comprehensive empirical\nevaluation of all the variants in our framework across language modeling,\nsyntactic generalization, summarization, dialogue, and inference efficiency.\nBased on the experimental results, we make multiple recommendations on the\ndesign of compositional SLMs. Our code is released at\nhttps://github.com/zhaoyd1/compositional_SLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec4\u5408\u5f0f\u53e5\u6cd5\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5bf9\u73b0\u6709\u53ca\u65b0\u578b\u53d8\u4f53\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u5e76\u6839\u636e\u5b9e\u9a8c\u7ed3\u679c\u7ed9\u51fa\u4e86\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u53e5\u6cd5\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u901a\u8fc7\u6574\u5408\u53e5\u6cd5\u504f\u7f6e\u6765\u589e\u5f3aTransformer\u3002\u672c\u7814\u7a76\u4e13\u6ce8\u4e8e\u57fa\u4e8e\u6210\u5206\u5206\u6790\u6811\u548c\u663e\u5f0f\u81ea\u4e0b\u800c\u4e0a\u6210\u5206\u8868\u793a\u7ec4\u5408\u7684\u7ec4\u5408\u5f0fSLMs\uff0c\u65e8\u5728\u8bc6\u522b\u5176\u8bbe\u8ba1\u9009\u62e9\u5e76\u63d0\u4f9b\u6539\u8fdb\u5efa\u8bae\u3002", "method": "\u4f5c\u8005\u9996\u5148\u8bc6\u522b\u4e86\u73b0\u6709\u7ec4\u5408\u5f0fSLM\u7684\u8bbe\u8ba1\u5173\u952e\u65b9\u9762\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6db5\u76d6\u73b0\u6709\u6a21\u578b\u548c\u65b0\u578b\u53d8\u4f53\u7684\u7edf\u4e00\u6846\u67b6\u3002\u968f\u540e\uff0c\u4ed6\u4eec\u5bf9\u8be5\u6846\u67b6\u4e2d\u7684\u6240\u6709\u53d8\u4f53\u5728\u8bed\u8a00\u5efa\u6a21\u3001\u53e5\u6cd5\u6cdb\u5316\u3001\u6458\u8981\u3001\u5bf9\u8bdd\u4ee5\u53ca\u63a8\u7406\u6548\u7387\u7b49\u591a\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5bf9\u7edf\u4e00\u6846\u67b6\u4e2d\u6240\u6709\u7ec4\u5408\u5f0fSLM\u53d8\u4f53\u7684\u5168\u9762\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u83b7\u5f97\u4e86\u5176\u5728\u8bed\u8a00\u5efa\u6a21\u3001\u53e5\u6cd5\u6cdb\u5316\u3001\u6458\u8981\u3001\u5bf9\u8bdd\u4efb\u52a1\u8868\u73b0\u53ca\u63a8\u7406\u6548\u7387\u65b9\u9762\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u6839\u636e\u5b9e\u9a8c\u7ed3\u679c\uff0c\u8bba\u6587\u4e3a\u7ec4\u5408\u5f0fSLM\u7684\u8bbe\u8ba1\u63d0\u51fa\u4e86\u591a\u9879\u5177\u4f53\u5efa\u8bae\u3002"}}
{"id": "2506.22844", "pdf": "https://arxiv.org/pdf/2506.22844", "abs": "https://arxiv.org/abs/2506.22844", "authors": ["Navid Keshtiarast", "Marina Petrova"], "title": "Coexistence analysis of Wi-Fi 6E and 5G NR-U in the 6 GHz band", "categories": ["eess.SP", "cs.NI"], "comment": "Accepted for Publication in ICNS3 2025", "summary": "The ever-increasing demand for broadband and IoT wireless connectivity has\nrecently urged the regulators around the world to start opening the 6 GHz\nspectrum for unlicensed use. These bands will, for example, permit the use of\nadditional 1.2 GHz in the US and 500 MHz in Europe for unlicensed radio access\ntechnologies (RATs) such as Wi-Fi and 5G New Radio Unlicensed (5G NR-U). To\nsupport QoS-sensitive applications with both technologies, fair and efficient\ncoexistence approaches between the two RATs, as well as with incumbents already\noperating in the 6 GHz band, are crucial. In this paper, we study through\nextensive simulations the achievable mean downlink throughput of both Wi-Fi 6E\nAPs and 5G NR-U gNBs when they are co-deployed in a dense residential scenario\nunder high-interference conditions. We also explore how different parameter\nsettings e.g., MAC frame aggregation, energy detection threshold and maximum\nchannel occupancy time (MCOT) affect the coexistence. Our findings give\nimportant insights into how to tune the key parameters to design fair\ncoexistence policies.", "AI": {"tldr": "\u7814\u7a766GHz\u975e\u6388\u6743\u9891\u8c31\u4e2dWi-Fi 6E\u4e0e5G NR-U\u5171\u5b58\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u62df\u5206\u6790\u5176\u4e0b\u884c\u541e\u5410\u91cf\u53ca\u5173\u952e\u53c2\u6570\u5bf9\u516c\u5e73\u5171\u5b58\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u5bbd\u5e26\u548c\u7269\u8054\u7f51\u9700\u6c42\u7684\u589e\u957f\uff0c\u76d1\u7ba1\u673a\u6784\u5f00\u653e6GHz\u975e\u6388\u6743\u9891\u8c31\u4f9bWi-Fi\u548c5G NR-U\u4f7f\u7528\u3002\u4e3a\u652f\u6301QoS\u654f\u611f\u5e94\u7528\uff0c\u786e\u4fdd\u8fd9\u4e24\u79cd\u6280\u672f\u4ee5\u53ca\u4e0e\u73b0\u6709\u7528\u6237\u4e4b\u95f4\u7684\u516c\u5e73\u9ad8\u6548\u5171\u5b58\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5728\u5bc6\u96c6\u4f4f\u5b85\u3001\u9ad8\u5e72\u6270\u573a\u666f\u4e0b\uff0c\u901a\u8fc7\u5927\u91cf\u6a21\u62df\u7814\u7a76Wi-Fi 6E AP\u548c5G NR-U gNB\u7684\u5e73\u5747\u4e0b\u884c\u541e\u5410\u91cf\u3002\u540c\u65f6\uff0c\u63a2\u8ba8MAC\u5e27\u805a\u5408\u3001\u80fd\u91cf\u68c0\u6d4b\u9608\u503c\u548c\u6700\u5927\u4fe1\u9053\u5360\u7528\u65f6\u95f4\uff08MCOT\uff09\u7b49\u53c2\u6570\u8bbe\u7f6e\u5bf9\u5171\u5b58\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u5173\u4e8e\u5982\u4f55\u8c03\u6574\u5173\u952e\u53c2\u6570\u4ee5\u8bbe\u8ba1\u516c\u5e73\u5171\u5b58\u7b56\u7565\u7684\u91cd\u8981\u89c1\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u57286GHz\u975e\u6388\u6743\u9891\u8c31\u4e2d\u8bbe\u8ba1Wi-Fi 6E\u548c5G NR-U\u7684\u516c\u5e73\u5171\u5b58\u7b56\u7565\u63d0\u4f9b\u4e86\u53c2\u6570\u8c03\u6574\u65b9\u9762\u7684\u6307\u5bfc\u3002"}}
{"id": "2506.22771", "pdf": "https://arxiv.org/pdf/2506.22771", "abs": "https://arxiv.org/abs/2506.22771", "authors": ["Jingxiao Ma", "Priyadarshini Panda", "Sherief Reda"], "title": "FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision", "categories": ["cs.LG", "cs.AI", "cs.NE", "I.2.0; I.2.6"], "comment": "To be published in the 62nd Design Automation Conference (DAC), 2025", "summary": "Backpropagation has been the cornerstone of neural network training for\ndecades, yet its inefficiencies in time and energy consumption limit its\nsuitability for resource-constrained edge devices. While low-precision neural\nnetwork quantization has been extensively researched to speed up model\ninference, its application in training has been less explored. Recently, the\nForward-Forward (FF) algorithm has emerged as a promising alternative to\nbackpropagation, replacing the backward pass with an additional forward pass.\nBy avoiding the need to store intermediate activations for backpropagation, FF\ncan reduce memory footprint, making it well-suited for embedded devices. This\npaper presents an INT8 quantized training approach that leverages FF's\nlayer-by-layer strategy to stabilize gradient quantization. Furthermore, we\npropose a novel \"look-ahead\" scheme to address limitations of FF and improve\nmodel accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board\ndemonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in\nmemory usage, while maintaining competitive accuracy compared to the\nstate-of-the-art.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u524d\u5411-\u524d\u5411\uff08FF\uff09\u7b97\u6cd5\u7684INT8\u91cf\u5316\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u201c\u524d\u77bb\u201d\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6548\u7387\uff08\u901f\u5ea6\u3001\u80fd\u8017\u3001\u5185\u5b58\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u3002", "motivation": "\u53cd\u5411\u4f20\u64ad\u5728\u65f6\u95f4\u4e0e\u80fd\u8017\u4e0a\u7684\u4f4e\u6548\u6027\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u3002\u5c3d\u7ba1\u4f4e\u7cbe\u5ea6\u63a8\u7406\u91cf\u5316\u7814\u7a76\u5e7f\u6cdb\uff0c\u4f46\u8bad\u7ec3\u65b9\u9762\u7684\u63a2\u7d22\u8f83\u5c11\u3002\u524d\u5411-\u524d\u5411\uff08FF\uff09\u7b97\u6cd5\u4f5c\u4e3a\u53cd\u5411\u4f20\u64ad\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u6709\u671b\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff0c\u4f46\u5176\u81ea\u8eab\u5c40\u9650\u6027\u9700\u8981\u89e3\u51b3\uff0c\u4e14\u5176\u5728\u91cf\u5316\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\u5c1a\u5f85\u6df1\u5165\u7814\u7a76\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cdINT8\u91cf\u5316\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528FF\u7b97\u6cd5\u7684\u9010\u5c42\u7b56\u7565\u6765\u7a33\u5b9a\u68af\u5ea6\u91cf\u5316\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u201c\u524d\u77bb\u201d\uff08look-ahead\uff09\u673a\u5236\uff0c\u4ee5\u89e3\u51b3FF\u7b97\u6cd5\u7684\u5c40\u9650\u6027\u5e76\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u3002", "result": "\u5728NVIDIA Jetson Orin Nano\u677f\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e864.6%\u7684\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u30018.3%\u7684\u80fd\u8017\u8282\u7701\u548c27.0%\u7684\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\uff0c\u540c\u65f6\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4fdd\u6301\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684INT8\u91cf\u5316FF\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u201c\u524d\u77bb\u201d\u673a\u5236\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u8bad\u7ec3\u6548\u7387\uff0c\u5305\u62ec\u901f\u5ea6\u3001\u80fd\u8017\u548c\u5185\u5b58\u5229\u7528\u7387\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u6a21\u578b\u7cbe\u5ea6\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.23520", "pdf": "https://arxiv.org/pdf/2506.23520", "abs": "https://arxiv.org/abs/2506.23520", "authors": ["Yu Zhang", "Ruijie Yu", "Jidong Tian", "Feng Zhu", "Jiapeng Liu", "Xiaokang Yang", "Yaohui Jin", "Yanyan Xu"], "title": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data", "categories": ["cs.AI"], "comment": null, "summary": "With the increasing interest in robotic synthesis in the context of organic\nchemistry, the automated extraction of chemical procedures from literature is\ncritical. However, this task remains challenging due to the inherent ambiguity\nof chemical language and the high cost of human annotation required for\ndeveloping reliable computer-aided extraction protocols. Here, we present\nChemActor, a fully fine-tuned large language model (LLM), as a chemical\nexecutor to convert between unstructured experimental procedures and structured\naction sequences. We propose a sequential LLM-generated data framework to\naddress the challenges of insufficient and low-quality annotated data. This\nframework integrates a data selection module that selects data based on\ndistribution divergence, with a general-purpose LLM, to generate\nmachine-executable actions from a single molecule input. Additionally, we\nintroduce a novel multi-round LLMs circle review metric, which reflects the\nmodel's advanced understanding of chemical experimental procedures. Extensive\nexperiments on reaction-to-description (R2D) and description-to-action (D2A)\ntasks demonstrate that ChemActor, augmented by LLM-generated data, achieves\nstate-of-the-art performance, outperforming the baseline model by 10%. The code\nis available at: https://github.com/Zhanghahah/ChemActor.", "AI": {"tldr": "ChemActor\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7cfb\u7edf\uff0c\u65e8\u5728\u5c06\u975e\u7ed3\u6784\u5316\u5316\u5b66\u5b9e\u9a8c\u89c4\u7a0b\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u673a\u5668\u53ef\u6267\u884c\u52a8\u4f5c\u5e8f\u5217\uff0c\u901a\u8fc7LLM\u751f\u6210\u6570\u636e\u589e\u5f3a\u548c\u521b\u65b0\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5316\u5b66\u89c4\u7a0b\u81ea\u52a8\u5316\u63d0\u53d6\u7684\u6700\u65b0\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u6709\u673a\u5316\u5b66\u4e2d\u673a\u5668\u4eba\u5408\u6210\u7684\u5174\u8d77\uff0c\u4ece\u6587\u732e\u4e2d\u81ea\u52a8\u5316\u63d0\u53d6\u5316\u5b66\u89c4\u7a0b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8be5\u4efb\u52a1\u56e0\u5316\u5b66\u8bed\u8a00\u56fa\u6709\u7684\u6a21\u7cca\u6027\u4ee5\u53ca\u9ad8\u8d28\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u9ad8\u6210\u672c\u800c\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86ChemActor\uff0c\u4e00\u4e2a\u7ecf\u8fc7\u5b8c\u5168\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4f5c\u4e3a\u5316\u5b66\u6267\u884c\u5668\uff0c\u7528\u4e8e\u5728\u975e\u7ed3\u6784\u5316\u5b9e\u9a8c\u89c4\u7a0b\u548c\u7ed3\u6784\u5316\u52a8\u4f5c\u5e8f\u5217\u4e4b\u95f4\u8fdb\u884c\u8f6c\u6362\u3002\u4e3a\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u548c\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u987a\u5e8f\u7684LLM\u751f\u6210\u6570\u636e\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5206\u5e03\u5dee\u5f02\u7684\u6570\u636e\u9009\u62e9\u6a21\u5757\u548c\u4e00\u4e2a\u901a\u7528LLM\u6765\u751f\u6210\u673a\u5668\u53ef\u6267\u884c\u52a8\u4f5c\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u8f6eLLMs\u5faa\u73af\u5ba1\u67e5\u6307\u6807\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u5bf9\u5316\u5b66\u5b9e\u9a8c\u89c4\u7a0b\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5728\u53cd\u5e94\u5230\u63cf\u8ff0\uff08R2D\uff09\u548c\u63cf\u8ff0\u5230\u52a8\u4f5c\uff08D2A\uff09\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7LLM\u751f\u6210\u6570\u636e\u589e\u5f3a\u7684ChemActor\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u63d0\u5347\u4e8610%\u3002", "conclusion": "ChemActor\u6210\u529f\u89e3\u51b3\u4e86\u5316\u5b66\u89c4\u7a0b\u81ea\u52a8\u5316\u63d0\u53d6\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u8bed\u8a00\u6a21\u7cca\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5176\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8f6c\u6362\u51c6\u786e\u6027\uff0c\u4e3a\u672a\u6765\u7684\u673a\u5668\u4eba\u5316\u5b66\u5408\u6210\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2506.22637", "pdf": "https://arxiv.org/pdf/2506.22637", "abs": "https://arxiv.org/abs/2506.22637", "authors": ["Haoxuan Wang", "Zhenghao Zhao", "Junyi Wu", "Yuzhang Shang", "Gaowen Liu", "Yan Yan"], "title": "CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation", "categories": ["cs.CV"], "comment": "ICCV 2025. Code is available at\n  https://github.com/hatchetProject/CaO2", "summary": "The recent introduction of diffusion models in dataset distillation has shown\npromising potential in creating compact surrogate datasets for large,\nhigh-resolution target datasets, offering improved efficiency and performance\nover traditional bi-level/uni-level optimization methods. However, current\ndiffusion-based dataset distillation approaches overlook the evaluation process\nand exhibit two critical inconsistencies in the distillation process: (1)\nObjective Inconsistency, where the distillation process diverges from the\nevaluation objective, and (2) Condition Inconsistency, leading to mismatches\nbetween generated images and their corresponding conditions. To resolve these\nissues, we introduce Condition-aware Optimization with Objective-guided\nSampling (CaO$_2$), a two-stage diffusion-based framework that aligns the\ndistillation process with the evaluation objective. The first stage employs a\nprobability-informed sample selection pipeline, while the second stage refines\nthe corresponding latent representations to improve conditional likelihood.\nCaO$_2$ achieves state-of-the-art performance on ImageNet and its subsets,\nsurpassing the best-performing baselines by an average of 2.3% accuracy.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u96c6\u84b8\u998f\u4e2d\u6f5c\u529b\u5927\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8bc4\u4f30\u548c\u6761\u4ef6\u4e0d\u4e00\u81f4\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6CaO$_2$\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u5728ImageNet\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u5ffd\u89c6\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u5e76\u5b58\u5728\u76ee\u6807\u4e0d\u4e00\u81f4\u6027\uff08\u84b8\u998f\u4e0e\u8bc4\u4f30\u76ee\u6807\u4e0d\u7b26\uff09\u548c\u6761\u4ef6\u4e0d\u4e00\u81f4\u6027\uff08\u751f\u6210\u56fe\u50cf\u4e0e\u6761\u4ef6\u4e0d\u5339\u914d\uff09\u4e24\u5927\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51faCaO$_2$\uff0c\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u65e8\u5728\u4f7f\u84b8\u998f\u8fc7\u7a0b\u4e0e\u8bc4\u4f30\u76ee\u6807\u5bf9\u9f50\u3002\u7b2c\u4e00\u9636\u6bb5\u91c7\u7528\u57fa\u4e8e\u6982\u7387\u7684\u6837\u672c\u9009\u62e9\u6d41\u7a0b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f18\u5316\u6f5c\u5728\u8868\u793a\u4ee5\u63d0\u9ad8\u6761\u4ef6\u4f3c\u7136\u3002", "result": "CaO$_2$\u5728ImageNet\u53ca\u5176\u5b50\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u6027\u80fd\uff0c\u5e73\u5747\u7cbe\u5ea6\u8d85\u8fc7\u73b0\u6709\u6700\u4f73\u57fa\u7ebf2.3%\u3002", "conclusion": "CaO$_2$\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u6570\u636e\u96c6\u84b8\u998f\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5728ImageNet\u7b49\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684SOTA\u3002"}}
{"id": "2506.23046", "pdf": "https://arxiv.org/pdf/2506.23046", "abs": "https://arxiv.org/abs/2506.23046", "authors": ["Xianzhe Fan", "Xuhui Zhou", "Chuanyang Jin", "Kolby Nottingham", "Hao Zhu", "Maarten Sap"], "title": "SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "comment": "23 pages, 6 figures", "summary": "Humans continuously infer the states, goals, and behaviors of others by\nperceiving their surroundings in dynamic, real-world social interactions.\nHowever, most Theory of Mind (ToM) benchmarks only evaluate static, text-based\nscenarios, which have a significant gap compared to real interactions. We\npropose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in\nembodied multi-agent complex social interactions. This benchmark is based on\nrich multimodal interaction data generated by the interaction environment SoMi,\ncovering diverse crafting goals and social relationships. Our framework\nsupports multi-level evaluation: (1) first-person evaluation provides\nmultimodal (visual, dialogue, action, etc.) input from a first-person\nperspective during a task for real-time state inference, (2) third-person\nevaluation provides complete third-person perspective video and text records\nafter a task for goal and behavior inference. This evaluation method allows for\na more comprehensive examination of a model's ToM capabilities from both the\nsubjective immediate experience and the objective global observation. We\nconstructed a challenging dataset containing 35 third-person perspective\nvideos, 363 first-person perspective images, and 1225 expert-annotated\nmultiple-choice questions (three options). On this dataset, we systematically\nevaluated the performance of human subjects and several state-of-the-art large\nvision-language models (LVLMs). The results show that LVLMs perform\nsignificantly worse than humans on SoMi-ToM: the average accuracy gap between\nhumans and models is 40.1% in first-person evaluation and 26.4% in third-person\nevaluation. This indicates that future LVLMs need to further improve their ToM\ncapabilities in embodied, complex social interactions.", "AI": {"tldr": "\u73b0\u6709\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u57fa\u51c6\u4e0e\u771f\u5b9e\u4ea4\u4e92\u5b58\u5728\u5dee\u8ddd\uff0c\u672c\u6587\u63d0\u51faSoMi-ToM\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u548c\u591a\u89c6\u89d2\u8bc4\u4f30\uff08\u7b2c\u4e00\u4eba\u79f0/\u7b2c\u4e09\u4eba\u79f0\uff09\u6765\u8861\u91cf\u5177\u8eab\u591a\u667a\u80fd\u4f53\u590d\u6742\u793e\u4ea4\u4e92\u52a8\u4e2d\u7684ToM\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u6b64\u57fa\u51c6\u4e0a\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u3002", "motivation": "\u4eba\u7c7b\u5728\u52a8\u6001\u771f\u5b9e\u4e16\u754c\u7684\u793e\u4ea4\u4e92\u52a8\u4e2d\u6301\u7eed\u63a8\u65ad\u4ed6\u4eba\u7684\u72b6\u6001\u3001\u76ee\u6807\u548c\u884c\u4e3a\uff0c\u4f46\u73b0\u6709\u5927\u591a\u6570\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u57fa\u51c6\u4ec5\u8bc4\u4f30\u9759\u6001\u3001\u57fa\u4e8e\u6587\u672c\u7684\u573a\u666f\uff0c\u4e0e\u771f\u5b9e\u4ea4\u4e92\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86SoMi-ToM\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30\u5177\u8eab\u591a\u667a\u80fd\u4f53\u590d\u6742\u793e\u4ea4\u4e92\u52a8\u4e2d\u7684\u591a\u89c6\u89d2ToM\u3002\u8be5\u57fa\u51c6\u57fa\u4e8e\u4ea4\u4e92\u73af\u5883SoMi\u751f\u6210\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u6570\u636e\uff0c\u652f\u6301\u591a\u5c42\u6b21\u8bc4\u4f30\uff1a\u7b2c\u4e00\u4eba\u79f0\u8bc4\u4f30\uff08\u5b9e\u65f6\u72b6\u6001\u63a8\u65ad\uff09\u548c\u7b2c\u4e09\u4eba\u79f0\u8bc4\u4f30\uff08\u4efb\u52a1\u540e\u76ee\u6807\u548c\u884c\u4e3a\u63a8\u65ad\uff09\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b35\u4e2a\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u3001363\u5f20\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u56fe\u50cf\u548c1225\u4e2a\u4e13\u5bb6\u6807\u6ce8\u591a\u9009\u9898\u7684\u6311\u6218\u6027\u6570\u636e\u96c6\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4eba\u7c7b\u53d7\u8bd5\u8005\u548c\u5f53\u524dSOTA\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u6027\u80fd\u3002", "result": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728SoMi-ToM\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\uff1a\u7b2c\u4e00\u4eba\u79f0\u8bc4\u4f30\u4e2d\uff0c\u4eba\u4e0e\u6a21\u578b\u7684\u5e73\u5747\u51c6\u786e\u7387\u5dee\u8ddd\u4e3a40.1%\uff1b\u7b2c\u4e09\u4eba\u79f0\u8bc4\u4f30\u4e2d\u4e3a26.4%\u3002", "conclusion": "\u672a\u6765\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u9700\u8981\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u5728\u5177\u8eab\u3001\u590d\u6742\u793e\u4ea4\u4e92\u52a8\u4e2d\u7684\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u80fd\u529b\u3002"}}
{"id": "2506.22884", "pdf": "https://arxiv.org/pdf/2506.22884", "abs": "https://arxiv.org/abs/2506.22884", "authors": ["Praveen Kumar Donta", "Qiyang Zhang", "Schahram Dustdar"], "title": "Performance Measurements in the AI-Centric Computing Continuum Systems", "categories": ["cs.DC", "cs.AI", "cs.ET", "cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "Over the Eight decades, computing paradigms have shifted from large,\ncentralized systems to compact, distributed architectures, leading to the rise\nof the Distributed Computing Continuum (DCC). In this model, multiple layers\nsuch as cloud, edge, Internet of Things (IoT), and mobile platforms work\ntogether to support a wide range of applications. Recently, the emergence of\nGenerative AI and large language models has further intensified the demand for\ncomputational resources across this continuum. Although traditional performance\nmetrics have provided a solid foundation, they need to be revisited and\nexpanded to keep pace with changing computational demands and application\nrequirements. Accurate performance measurements benefit both system designers\nand users by supporting improvements in efficiency and promoting alignment with\nsystem goals. In this context, we review commonly used metrics in DCC and IoT\nenvironments. We also discuss emerging performance dimensions that address\nevolving computing needs, such as sustainability, energy efficiency, and system\nobservability. We also outline criteria and considerations for selecting\nappropriate metrics, aiming to inspire future research and development in this\ncritical area.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u5206\u5e03\u5f0f\u8ba1\u7b97\u8fde\u7eed\u4f53\uff08DCC\uff09\u548c\u7269\u8054\u7f51\uff08IoT\uff09\u73af\u5883\u4e2d\u7684\u6027\u80fd\u6307\u6807\uff0c\u5e76\u8ba8\u8bba\u4e86\u65b0\u5174\u6307\u6807\u5982\u53ef\u6301\u7eed\u6027\u3001\u80fd\u6548\u548c\u7cfb\u7edf\u53ef\u89c2\u6d4b\u6027\uff0c\u4ee5\u9002\u5e94\u751f\u6210\u5f0fAI\u5e26\u6765\u7684\u65b0\u8ba1\u7b97\u9700\u6c42\uff0c\u65e8\u5728\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002", "motivation": "\u8ba1\u7b97\u8303\u5f0f\u5df2\u8f6c\u5411\u5206\u5e03\u5f0f\u8ba1\u7b97\u8fde\u7eed\u4f53\uff08DCC\uff09\uff0c\u5305\u542b\u4e91\u3001\u8fb9\u7f18\u3001\u7269\u8054\u7f51\u548c\u79fb\u52a8\u5e73\u53f0\u3002\u751f\u6210\u5f0fAI\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\u6781\u5927\u5730\u589e\u52a0\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002\u4f20\u7edf\u6027\u80fd\u6307\u6807\u5df2\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u4e0d\u65ad\u53d8\u5316\u7684\u8ba1\u7b97\u9700\u6c42\u548c\u5e94\u7528\u8981\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u91cd\u65b0\u5ba1\u89c6\u548c\u6269\u5c55\uff0c\u4ee5\u652f\u6301\u7cfb\u7edf\u6548\u7387\u63d0\u5347\u548c\u76ee\u6807\u5b9e\u73b0\u3002", "method": "1. \u56de\u987eDCC\u548cIoT\u73af\u5883\u4e2d\u5e38\u7528\u7684\u6027\u80fd\u6307\u6807\u30022. \u8ba8\u8bba\u65b0\u5174\u7684\u6027\u80fd\u7ef4\u5ea6\uff0c\u5982\u53ef\u6301\u7eed\u6027\u3001\u80fd\u6e90\u6548\u7387\u548c\u7cfb\u7edf\u53ef\u89c2\u6d4b\u6027\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u65ad\u53d1\u5c55\u7684\u8ba1\u7b97\u9700\u6c42\u30023. \u6982\u8ff0\u9009\u62e9\u9002\u5f53\u6307\u6807\u7684\u6807\u51c6\u548c\u8003\u91cf\u56e0\u7d20\u3002", "result": "\u6587\u7ae0\u5ba1\u67e5\u4e86DCC\u548cIoT\u73af\u5883\u4e2d\u5e38\u7528\u7684\u6027\u80fd\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u6301\u7eed\u6027\u3001\u80fd\u6548\u548c\u7cfb\u7edf\u53ef\u89c2\u6d4b\u6027\u7b49\u65b0\u5174\u6027\u80fd\u7ef4\u5ea6\uff0c\u4ee5\u9002\u5e94AI\u65f6\u4ee3\u7684\u9700\u6c42\u3002\u6b64\u5916\uff0c\u6587\u7ae0\u8fd8\u63d0\u4f9b\u4e86\u9009\u62e9\u5408\u9002\u6307\u6807\u7684\u5224\u636e\u548c\u8003\u91cf\u56e0\u7d20\u3002", "conclusion": "\u4e3a\u5e94\u5bf9DCC\u548cIoT\u73af\u5883\u4e0b\uff0c\u7279\u522b\u662f\u751f\u6210\u5f0fAI\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e26\u6765\u7684\u5de8\u5927\u8ba1\u7b97\u9700\u6c42\uff0c\u6027\u80fd\u6307\u6807\u9700\u8981\u88ab\u91cd\u65b0\u5b9a\u4e49\u548c\u6269\u5c55\u3002\u672c\u6587\u901a\u8fc7\u56de\u987e\u73b0\u6709\u6307\u6807\u3001\u5f15\u5165\u65b0\u5174\u7ef4\u5ea6\u5e76\u63d0\u4f9b\u9009\u62e9\u6807\u51c6\uff0c\u65e8\u5728\u6fc0\u53d1\u8be5\u5173\u952e\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u4e0e\u53d1\u5c55\u3002"}}
{"id": "2506.22780", "pdf": "https://arxiv.org/pdf/2506.22780", "abs": "https://arxiv.org/abs/2506.22780", "authors": ["Dibyajyoti Chakraborty", "Haiwen Guan", "Jason Stock", "Troy Arcomano", "Guido Cervone", "Romit Maulik"], "title": "Multimodal Atmospheric Super-Resolution With Deep Generative Models", "categories": ["cs.LG", "physics.geo-ph"], "comment": null, "summary": "Score-based diffusion modeling is a generative machine learning algorithm\nthat can be used to sample from complex distributions. They achieve this by\nlearning a score function, i.e., the gradient of the log-probability density of\nthe data, and reversing a noising process using the same. Once trained,\nscore-based diffusion models not only generate new samples but also enable\nzero-shot conditioning of the generated samples on observed data. This promises\na novel paradigm for data and model fusion, wherein the implicitly learned\ndistributions of pretrained score-based diffusion models can be updated given\nthe availability of online data in a Bayesian formulation. In this article, we\napply such a concept to the super-resolution of a high-dimensional dynamical\nsystem, given the real-time availability of low-resolution and experimentally\nobserved sparse sensor measurements from multimodal data. Additional analysis\non how score-based sampling can be used for uncertainty estimates is also\nprovided. Our experiments are performed for a super-resolution task that\ngenerates the ERA5 atmospheric dataset given sparse observations from a\ncoarse-grained representation of the same and/or from unstructured experimental\nobservations of the IGRA radiosonde dataset. We demonstrate accurate recovery\nof the high dimensional state given multiple sources of low-fidelity\nmeasurements. We also discover that the generative model can balance the\ninfluence of multiple dataset modalities during spatiotemporal reconstructions.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u4f4e\u5206\u8fa8\u7387\u53ca\u7a00\u758f\u89c2\u6d4b\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7ef4\u52a8\u6001\u7cfb\u7edf\u7684\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u590d\u6742\u5206\u5e03\u5e76\u5b9e\u73b0\u96f6\u6837\u672c\u6761\u4ef6\u751f\u6210\uff0c\u8fd9\u4e3a\u5728\u8d1d\u53f6\u65af\u6846\u67b6\u4e0b\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u5728\u7ebf\u6570\u636e\u878d\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u8303\u5f0f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u5b9e\u65f6\u53ef\u7528\u7684\u4f4e\u5206\u8fa8\u7387\u548c\u7a00\u758f\u4f20\u611f\u5668\u6d4b\u91cf\u6570\u636e\u3002", "method": "\u5c06\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u9ad8\u7ef4\u52a8\u6001\u7cfb\u7edf\u7684\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u3002\u901a\u8fc7\u878d\u5408\u591a\u6e90\u4f4e\u4fdd\u771f\u5ea6\u6d4b\u91cf\u6570\u636e\uff08\u5982\u7c97\u7c92\u5ea6\u8868\u793a\u548c\u975e\u7ed3\u6784\u5316\u5b9e\u9a8c\u89c2\u6d4b\uff09\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u5206\u6570\u7684\u91c7\u6837\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002\u5b9e\u9a8c\u4ee5ERA5\u5927\u6c14\u6570\u636e\u96c6\u7684\u8d85\u5206\u8fa8\u7387\u4e3a\u4f8b\uff0c\u8f93\u5165\u5305\u62ecERA5\u7684\u7c97\u7c92\u5ea6\u8868\u793a\u548c/\u6216IGRA\u65e0\u7ebf\u7535\u63a2\u7a7a\u4eea\u7684\u7a00\u758f\u89c2\u6d4b\u3002", "result": "\u5728\u7ed9\u5b9a\u591a\u6e90\u4f4e\u4fdd\u771f\u5ea6\u6d4b\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u51c6\u786e\u6062\u590d\u9ad8\u7ef4\u72b6\u6001\u3002\u8be5\u751f\u6210\u6a21\u578b\u5728\u65f6\u7a7a\u91cd\u5efa\u8fc7\u7a0b\u4e2d\uff0c\u80fd\u591f\u6709\u6548\u5e73\u8861\u591a\u4e2a\u6570\u636e\u96c6\u6a21\u6001\u7684\u5f71\u54cd\u3002\u540c\u65f6\uff0c\u57fa\u4e8e\u5206\u6570\u7684\u91c7\u6837\u53ef\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "conclusion": "\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u5728\u7ed3\u5408\u591a\u6a21\u6001\u4f4e\u4fdd\u771f\u5ea6\u6570\u636e\u65f6\uff0c\u80fd\u6709\u6548\u5730\u5b9e\u73b0\u9ad8\u7ef4\u52a8\u6001\u7cfb\u7edf\u7684\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u5e76\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4e3a\u6570\u636e\u4e0e\u6a21\u578b\u878d\u5408\u3001\u5b9e\u65f6\u6570\u636e\u6574\u5408\u53ca\u9ad8\u7ef4\u72b6\u6001\u6062\u590d\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2506.23549", "pdf": "https://arxiv.org/pdf/2506.23549", "abs": "https://arxiv.org/abs/2506.23549", "authors": ["Huai-Chih Wang", "Hsiang-Chun Chuang", "Hsi-Chun Cheng", "Dai-Jie Wu", "Shao-Hua Sun"], "title": "CooT: Learning to Coordinate In-Context with Coordination Transformers", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": "23 pages, 10 tables, 8 figures", "summary": "Effective coordination among artificial agents in dynamic and uncertain\nenvironments remains a significant challenge in multi-agent systems. Existing\napproaches, such as self-play and population-based methods, either generalize\npoorly to unseen partners or require extensive training. To overcome these\nlimitations, we propose Coordination Transformers (CooT), a novel in-context\ncoordination framework that uses recent interaction histories to adapt to\nunseen partners rapidly. Unlike previous approaches that primarily aim to\nincrease the diversity of training partners, CooT explicitly focuses on\nadapting to new partner behaviors by predicting actions aligned with observed\npartner interactions. Trained on interaction trajectories collected from\ndiverse pairs of agents with complementary behaviors, CooT quickly learns\neffective coordination strategies without explicit supervision or fine-tuning.\nEvaluations on the Overcooked benchmark demonstrate that CooT significantly\noutperforms baseline methods in coordination tasks involving previously unseen\npartners. Human evaluations further confirm CooT as the most effective\ncollaborative partner, while extensive ablations highlight its robustness,\nflexibility, and sensitivity to context in multi-agent scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faCoordination Transformers (CooT)\uff0c\u4e00\u4e2a\u5229\u7528\u8fd1\u671f\u4ea4\u4e92\u5386\u53f2\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\u672a\u77e5\u5408\u4f5c\u5bf9\u8c61\u7684\u4e0a\u4e0b\u6587\u534f\u8c03\u6846\u67b6\uff0c\u5728Overcooked\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u4eba\u5de5\u667a\u80fd\u4f53\u4e4b\u95f4\u6709\u6548\u534f\u8c03\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u81ea\u6211\u535a\u5f08\u548c\u57fa\u4e8e\u79cd\u7fa4\u7684\u65b9\u6cd5\uff09\u5bf9\u672a\u77e5\u5408\u4f5c\u5bf9\u8c61\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u6216\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u3002", "method": "\u6211\u4eec\u63d0\u51faCooperation Transformers (CooT)\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u534f\u8c03\u6846\u67b6\u3002\u5b83\u5229\u7528\u8fd1\u671f\u4ea4\u4e92\u5386\u53f2\u6765\u5feb\u901f\u9002\u5e94\u672a\u77e5\u5408\u4f5c\u5bf9\u8c61\uff0c\u901a\u8fc7\u9884\u6d4b\u4e0e\u89c2\u5bdf\u5230\u7684\u5408\u4f5c\u5bf9\u8c61\u4ea4\u4e92\u76f8\u7b26\u7684\u884c\u52a8\u3002CooT\u5728\u4ece\u5177\u6709\u4e92\u8865\u884c\u4e3a\u7684\u4e0d\u540c\u667a\u80fd\u4f53\u5bf9\u6536\u96c6\u7684\u4ea4\u4e92\u8f68\u8ff9\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u663e\u5f0f\u76d1\u7763\u6216\u5fae\u8c03\uff0c\u5373\u53ef\u5feb\u901f\u5b66\u4e60\u6709\u6548\u7684\u534f\u8c03\u7b56\u7565\u3002", "result": "\u5728Overcooked\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8bc4\u4f30\u8868\u660e\uff0cCooT\u5728\u6d89\u53ca\u5148\u524d\u672a\u77e5\u5408\u4f5c\u5bf9\u8c61\u7684\u534f\u8c03\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8bc1\u5b9eCooT\u662f\u6700\u6709\u6548\u7684\u534f\u4f5c\u4f19\u4f34\uff0c\u800c\u5e7f\u6cdb\u7684\u6d88\u878d\u5b9e\u9a8c\u51f8\u663e\u4e86\u5176\u5728\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3001\u7075\u6d3b\u6027\u548c\u5bf9\u4e0a\u4e0b\u6587\u7684\u654f\u611f\u6027\u3002", "conclusion": "CooT\u662f\u4e00\u4e2a\u6709\u6548\u4e14\u9c81\u68d2\u7684\u4e0a\u4e0b\u6587\u534f\u8c03\u6846\u67b6\uff0c\u80fd\u591f\u5feb\u901f\u9002\u5e94\u672a\u77e5\u5408\u4f5c\u5bf9\u8c61\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u667a\u80fd\u4f53\u534f\u8c03\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2506.22678", "pdf": "https://arxiv.org/pdf/2506.22678", "abs": "https://arxiv.org/abs/2506.22678", "authors": ["Nicolas Caytuiro", "Ivan Sipiran"], "title": "3D Shape Generation: A Survey", "categories": ["cs.CV"], "comment": "20 pages, 5 figures", "summary": "Recent advances in deep learning have significantly transformed the field of\n3D shape generation, enabling the synthesis of complex, diverse, and\nsemantically meaningful 3D objects. This survey provides a comprehensive\noverview of the current state of the art in 3D shape generation, organizing the\ndiscussion around three core components: shape representations, generative\nmodeling approaches, and evaluation protocols. We begin by categorizing 3D\nrepresentations into explicit, implicit, and hybrid setups, highlighting their\nstructural properties, advantages, and limitations. Next, we review a wide\nrange of generation methods, focusing on feedforward architectures. We further\nsummarize commonly used datasets and evaluation metrics that assess fidelity,\ndiversity, and realism of generated shapes. Finally, we identify open\nchallenges and outline future research directions that could drive progress in\ncontrollable, efficient, and high-quality 3D shape generation. This survey aims\nto serve as a valuable reference for researchers and practitioners seeking a\nstructured and in-depth understanding of this rapidly evolving field.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u5168\u9762\u6982\u8ff0\u4e86\u6df1\u5ea6\u5b66\u4e60\u9a71\u52a8\u4e0b\u76843D\u5f62\u72b6\u751f\u6210\u9886\u57df\uff0c\u56f4\u7ed5\u5f62\u72b6\u8868\u793a\u3001\u751f\u6210\u6a21\u578b\u548c\u8bc4\u4f30\u534f\u8bae\u8fdb\u884c\u7ec4\u7ec7\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\u663e\u8457\u6539\u53d8\u4e863D\u5f62\u72b6\u751f\u6210\u9886\u57df\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u3001\u591a\u6837\u4e14\u6709\u8bed\u4e49\u76843D\u5bf9\u8c61\u5408\u6210\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u8be5\u9886\u57df\u7684\u6700\u65b0\u6280\u672f\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u68b3\u7406\u548c\u5206\u6790\u3002", "method": "\u672c\u7efc\u8ff0\u56f4\u7ed5\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u7ec4\u7ec7\uff1a\u9996\u5148\u5206\u7c7b3D\u5f62\u72b6\u8868\u793a\uff08\u663e\u5f0f\u3001\u9690\u5f0f\u3001\u6df7\u5408\uff09\uff0c\u5176\u6b21\u56de\u987e\u5404\u79cd\u751f\u6210\u5efa\u6a21\u65b9\u6cd5\uff08\u4fa7\u91cd\u524d\u9988\u67b6\u6784\uff09\uff0c\u5e76\u603b\u7ed3\u5e38\u7528\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002\u6700\u540e\uff0c\u8bc6\u522b\u5f00\u653e\u6311\u6218\u5e76\u89c4\u5212\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u901a\u8fc7\u5206\u6790\uff0c\u672c\u7efc\u8ff0\u5448\u73b0\u4e863D\u8868\u793a\u7684\u7ed3\u6784\u7279\u6027\u3001\u4f18\u7f3a\u70b9\uff0c\u603b\u7ed3\u4e86\u591a\u79cd\u751f\u6210\u65b9\u6cd5\uff0c\u5f52\u7eb3\u4e86\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u660e\u786e\u6307\u51fa\u4e86\u5f53\u524d\u9886\u57df\u9762\u4e34\u7684\u6311\u6218\u4ee5\u53ca\u672a\u6765\u5728\u53ef\u63a7\u3001\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf3D\u5f62\u72b6\u751f\u6210\u65b9\u9762\u7684\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u672c\u7efc\u8ff0\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5bf93D\u5f62\u72b6\u751f\u6210\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u9886\u57df\u7684\u7ed3\u6784\u5316\u548c\u6df1\u5165\u7406\u89e3\uff0c\u65e8\u5728\u6210\u4e3a\u8be5\u9886\u57df\u7684\u91cd\u8981\u53c2\u8003\u8d44\u6599\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.23051", "pdf": "https://arxiv.org/pdf/2506.23051", "abs": "https://arxiv.org/abs/2506.23051", "authors": ["Jo\u00e3o Lucas Luz Lima Sarcinelli", "Marina Lages Gon\u00e7alves Teixeira", "Jade Bortot de Paiva", "Diego Furtado Silva"], "title": "MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition", "categories": ["cs.CL"], "comment": null, "summary": "Named Entity Recognition (NER) is a fundamental Natural Language Processing\n(NLP) task that aims to identify and classify entity mentions in texts across\ndifferent categories. While languages such as English possess a large number of\nhigh-quality resources for this task, Brazilian Portuguese still lacks in\nquantity of gold-standard NER datasets, especially when considering specific\ndomains. Particularly, this paper considers the importance of NER for analyzing\nhistorical texts in the context of digital humanities. To address this gap,\nthis work outlines the construction of MariNER: \\textit{Mapeamento e\nAnota\\c{c}\\~oes de Registros hIst\\'oricos para NER} (Mapping and Annotation of\nHistorical Records for NER), the first gold-standard dataset for early\n20th-century Brazilian Portuguese, with more than 9,000 manually annotated\nsentences. We also assess and compare the performance of state-of-the-art NER\nmodels for the dataset.", "AI": {"tldr": "\u4e3a\u65e9\u671f20\u4e16\u7eaa\u5df4\u897f\u8461\u8404\u7259\u8bed\u5386\u53f2\u6587\u672c\u6784\u5efa\u5e76\u8bc4\u4f30\u4e86\u9996\u4e2a\u91d1\u6807\u51c6\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u6570\u636e\u96c6MariNER\u3002", "motivation": "\u5df4\u897f\u8461\u8404\u7259\u8bed\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5386\u53f2\u6587\u672c\u7684\u7279\u5b9a\u9886\u57df\uff0c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u91d1\u6807\u51c6NER\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u6570\u5b57\u4eba\u6587\u9886\u57df\u4e2d\u7684\u6587\u672c\u5206\u6790\u3002", "method": "\u6784\u5efa\u4e86MariNER\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u9488\u5bf920\u4e16\u7eaa\u65e9\u671f\u5df4\u897f\u8461\u8404\u7259\u8bed\u7684\u9ec4\u91d1\u6807\u51c6NER\u6570\u636e\u96c6\uff0c\u5305\u542b9,000\u591a\u4e2a\u4eba\u5de5\u6807\u6ce8\u53e5\u5b50\u3002\u540c\u65f6\uff0c\u8bc4\u4f30\u5e76\u6bd4\u8f83\u4e86\u6700\u5148\u8fdb\u7684NER\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86MariNER\u6570\u636e\u96c6\uff0c\u4e3a\u65e9\u671f20\u4e16\u7eaa\u5df4\u897f\u8461\u8404\u7259\u8bed\u5386\u53f2\u6587\u672c\u63d0\u4f9b\u4e86\u9996\u4e2a\u91d1\u6807\u51c6NER\u8d44\u6e90\u3002", "conclusion": "MariNER\u6570\u636e\u96c6\u7684\u521b\u5efa\u586b\u8865\u4e86\u5386\u53f2\u5df4\u897f\u8461\u8404\u7259\u8bedNER\u7684\u6570\u636e\u7a7a\u767d\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7684\u7814\u7a76\u548c\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.23435", "pdf": "https://arxiv.org/pdf/2506.23435", "abs": "https://arxiv.org/abs/2506.23435", "authors": ["Hayder Tirmazi"], "title": "All Proof of Work But No Proof of Play", "categories": ["cs.CR", "cs.NI"], "comment": "Published in CFAIL 2025", "summary": "Speedrunning is a competition that emerged from communities of early video\ngames such as Doom (1993). Speedrunners try to finish a game in minimal time.\nProvably verifying the authenticity of submitted speedruns is an open problem.\nTraditionally, best-effort speedrun verification is conducted by on-site human\nobservers, forensic audio analysis, or a rigorous mathematical analysis of the\ngame mechanics. Such methods are tedious, fallible, and, perhaps worst of all,\nnot cryptographic. Motivated by naivety and the Dunning-Kruger effect, we\nattempt to build a system that cryptographically proves the authenticity of\nspeedruns. This paper describes our attempted solutions and ways to circumvent\nthem. Through a narration of our failures, we attempt to demonstrate the\ndifficulty of authenticating live and interactive human input in untrusted\nenvironments, as well as the limits of signature schemes, game integrity, and\nprovable play.", "AI": {"tldr": "\u672c\u6587\u5c1d\u8bd5\u6784\u5efa\u5bc6\u7801\u5b66\u7cfb\u7edf\u4ee5\u9a8c\u8bc1Speedrun\u7684\u771f\u5b9e\u6027\uff0c\u5e76\u8bb0\u5f55\u4e86\u5931\u8d25\u5c1d\u8bd5\uff0c\u63ed\u793a\u5728\u4e0d\u53ef\u4fe1\u73af\u5883\u4e2d\u8ba4\u8bc1\u5b9e\u65f6\u4e92\u52a8\u8f93\u5165\u7684\u56f0\u96be\u53ca\u76f8\u5173\u6280\u672f\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfSpeedrun\u9a8c\u8bc1\u65b9\u6cd5\uff08\u4eba\u5de5\u89c2\u5bdf\u3001\u97f3\u9891\u5206\u6790\u3001\u6e38\u620f\u673a\u5236\u5206\u6790\uff09\u6548\u7387\u4f4e\u4e0b\u3001\u6613\u51fa\u9519\u4e14\u4e0d\u7f3a\u4e4f\u5bc6\u7801\u5b66\u5b89\u5168\u6027\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6784\u5efa\u5bc6\u7801\u5b66\u7cfb\u7edf\u89e3\u51b3Speedrun\u771f\u5b9e\u6027\u9a8c\u8bc1\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u662f\u5c1d\u8bd5\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e00\u4e2a\u80fd\u591f\u5bc6\u7801\u5b66\u8bc1\u660eSpeedrun\u771f\u5b9e\u6027\u7684\u7cfb\u7edf\u3002\u901a\u8fc7\u63cf\u8ff0\u6240\u5c1d\u8bd5\u7684\u89e3\u51b3\u65b9\u6848\u53ca\u5176\u88ab\u89c4\u907f\u7684\u6848\u4f8b\uff08\u5373\u5931\u8d25\u7684\u53d9\u8ff0\uff09\uff0c\u6765\u63a2\u8ba8\u8be5\u95ee\u9898\u7684\u590d\u6742\u6027\u3002", "result": "\u672a\u80fd\u6210\u529f\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u5b8c\u5168\u5bc6\u7801\u5b66\u9a8c\u8bc1Speedrun\u771f\u5b9e\u6027\u7684\u7cfb\u7edf\u3002\u7814\u7a76\u7ed3\u679c\u901a\u8fc7\u8bb0\u5f55\u5931\u8d25\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u53ef\u4fe1\u73af\u5883\u4e2d\u8ba4\u8bc1\u5b9e\u65f6\u4e92\u52a8\u4eba\u7c7b\u8f93\u5165\u7684\u5de8\u5927\u56f0\u96be\uff0c\u4ee5\u53ca\u7b7e\u540d\u65b9\u6848\u3001\u6e38\u620f\u5b8c\u6574\u6027\u548c\u53ef\u8bc1\u660e\u6e38\u73a9\u7b49\u6280\u672f\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u5728\u4e0d\u53ef\u4fe1\u73af\u5883\u4e2d\uff0c\u5bf9\u5b9e\u65f6\u548c\u4e92\u52a8\u7684\u4eba\u7c7b\u8f93\u5165\u8fdb\u884c\u53ef\u9760\u7684\u771f\u5b9e\u6027\u9a8c\u8bc1\u662f\u6781\u5176\u56f0\u96be\u7684\uff0c\u4e14\u73b0\u6709\u5bc6\u7801\u5b66\u7b7e\u540d\u65b9\u6848\u3001\u6e38\u620f\u5b8c\u6574\u6027\u4fdd\u62a4\u53ca\u53ef\u8bc1\u660e\u6e38\u73a9\u7b49\u6280\u672f\u5b58\u5728\u56fa\u6709\u9650\u5236\uff0c\u96be\u4ee5\u5b8c\u5168\u89e3\u51b3Speedrun\u771f\u5b9e\u6027\u9a8c\u8bc1\u7684\u6311\u6218\u3002"}}
{"id": "2506.22802", "pdf": "https://arxiv.org/pdf/2506.22802", "abs": "https://arxiv.org/abs/2506.22802", "authors": ["Hae Jin Song", "Laurent Itti"], "title": "Riemannian-Geometric Fingerprints of Generative Models", "categories": ["cs.LG", "cs.CR", "cs.CV", "I.2.6"], "comment": null, "summary": "Recent breakthroughs and rapid integration of generative models (GMs) have\nsparked interest in the problem of model attribution and their fingerprints.\nFor instance, service providers need reliable methods of authenticating their\nmodels to protect their IP, while users and law enforcement seek to verify the\nsource of generated content for accountability and trust. In addition, a\ngrowing threat of model collapse is arising, as more model-generated data are\nbeing fed back into sources (e.g., YouTube) that are often harvested for\ntraining (\"regurgitative training\"), heightening the need to differentiate\nsynthetic from human data. Yet, a gap still exists in understanding generative\nmodels' fingerprints, we believe, stemming from the lack of a formal framework\nthat can define, represent, and analyze the fingerprints in a principled way.\nTo address this gap, we take a geometric approach and propose a new definition\nof artifact and fingerprint of GMs using Riemannian geometry, which allows us\nto leverage the rich theory of differential geometry. Our new definition\ngeneralizes previous work (Song et al., 2024) to non-Euclidean manifolds by\nlearning Riemannian metrics from data and replacing the Euclidean distances and\nnearest-neighbor search with geodesic distances and kNN-based Riemannian center\nof mass. We apply our theory to a new gradient-based algorithm for computing\nthe fingerprints in practice. Results show that it is more effective in\ndistinguishing a large array of GMs, spanning across 4 different datasets in 2\ndifferent resolutions (64 by 64, 256 by 256), 27 model architectures, and 2\nmodalities (Vision, Vision-Language). Using our proposed definition\nsignificantly improves the performance on model attribution, as well as a\ngeneralization to unseen datasets, model types, and modalities, suggesting its\npractical efficacy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9ece\u66fc\u51e0\u4f55\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5b9a\u4e49\u548c\u8ba1\u7b97\u751f\u6210\u6a21\u578b\u7684\u6307\u7eb9\uff0c\u4ee5\u6709\u6548\u533a\u5206\u4e0d\u540c\u6a21\u578b\u3001\u8fdb\u884c\u6a21\u578b\u5f52\u56e0\uff0c\u5e76\u5e94\u5bf9\u6a21\u578b\u5d29\u6e83\u7b49\u6311\u6218\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u51fa\u73b0\u4e86\u5bf9\u6a21\u578b\u5f52\u56e0\u548c\u6307\u7eb9\u8bc6\u522b\u7684\u8feb\u5207\u9700\u6c42\uff0c\u4ee5\u4fdd\u62a4\u77e5\u8bc6\u4ea7\u6743\u3001\u9a8c\u8bc1\u5185\u5bb9\u6765\u6e90\u53ca\u5e94\u5bf9\u6a21\u578b\u751f\u6210\u6570\u636e\u56de\u6d41\u5bfc\u81f4\u7684\u6a21\u578b\u5d29\u6e83\u5a01\u80c1\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u4e00\u4e2a\u5b9a\u4e49\u3001\u8868\u793a\u548c\u5206\u6790\u751f\u6210\u6a21\u578b\u6307\u7eb9\u7684\u6b63\u5f0f\u6846\u67b6\u3002", "method": "\u7814\u7a76\u91c7\u7528\u51e0\u4f55\u5b66\u65b9\u6cd5\uff0c\u57fa\u4e8e\u9ece\u66fc\u51e0\u4f55\u63d0\u51fa\u751f\u6210\u6a21\u578b\u4f2a\u5f71\u548c\u6307\u7eb9\u7684\u65b0\u5b9a\u4e49\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u6570\u636e\u4e2d\u7684\u9ece\u66fc\u5ea6\u91cf\uff0c\u5c06\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u548c\u6700\u8fd1\u90bb\u641c\u7d22\u66ff\u6362\u4e3a\u6d4b\u5730\u7ebf\u8ddd\u79bb\u548c\u57fa\u4e8ekNN\u7684\u9ece\u66fc\u8d28\u5fc3\uff0c\u4ece\u800c\u5c06\u5148\u524d\u7684\u5de5\u4f5c\u63a8\u5e7f\u5230\u975e\u6b27\u51e0\u91cc\u5f97\u6d41\u5f62\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u7b97\u6cd5\u6765\u5b9e\u9645\u8ba1\u7b97\u6307\u7eb9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u533a\u5206\u5927\u91cf\u751f\u6210\u6a21\u578b\u65b9\u9762\u66f4\u4e3a\u6709\u6548\uff0c\u6db5\u76d64\u4e2a\u6570\u636e\u96c6\u30012\u79cd\u5206\u8fa8\u7387\uff0864x64\uff0c256x256\uff09\u300127\u79cd\u6a21\u578b\u67b6\u6784\u548c2\u79cd\u6a21\u6001\uff08\u89c6\u89c9\u3001\u89c6\u89c9-\u8bed\u8a00\uff09\u3002\u6240\u63d0\u51fa\u7684\u5b9a\u4e49\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5f52\u56e0\u6027\u80fd\uff0c\u5e76\u5bf9\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u3001\u6a21\u578b\u7c7b\u578b\u548c\u6a21\u6001\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8e\u9ece\u66fc\u51e0\u4f55\u7684\u6307\u7eb9\u5b9a\u4e49\u548c\u8ba1\u7b97\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u6a21\u578b\u6307\u7eb9\u8bc6\u522b\u7684\u96be\u9898\uff0c\u5728\u6a21\u578b\u5f52\u56e0\u3001\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u548c\u5185\u5bb9\u6eaf\u6e90\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u7684\u5b9e\u7528\u4ef7\u503c\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.23563", "pdf": "https://arxiv.org/pdf/2506.23563", "abs": "https://arxiv.org/abs/2506.23563", "authors": ["Huanjin Yao", "Jiaxing Huang", "Yawen Qiu", "Michael K. Chen", "Wenzheng Liu", "Wei Zhang", "Wenjie Zeng", "Xikun Zhang", "Jingyi Zhang", "Yuxin Song", "Wenhao Wu", "Dacheng Tao"], "title": "MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Technical report", "summary": "Reasoning plays a crucial role in advancing Multimodal Large Language Models\n(MLLMs) toward Artificial General Intelligence. However, existing MLLM\nbenchmarks often fall short in precisely and comprehensively evaluating\nlong-chain reasoning abilities from three key aspects: (1) lack of difficulty\nand diversity, (2) susceptibility to guessability and memorization, (3)\ninadequate assessment of intermediate reasoning steps. To fill this gap, we\nintroduce MMReason, a new benchmark designed to precisely and comprehensively\nevaluate MLLM long-chain reasoning capability with diverse, open-ended,\nchallenging questions. First, we curate challenging questions requiring\nmulti-step reasoning from various fields (i.e., 6 disciplines) and multiple\ndifficulty levels (i.e., from pre-university to university, and from\nfoundational to competition tiers). Second, these questions are reformulated\ninto an open-ended format and filtered using a multi-model voting technique to\neliminate shortcut cases related to guessing and memorization, ensuring robust\nreasoning evaluations. Third, we annotate the questions with detailed\nstep-by-step solutions, and design a reference-based ternary scoring mechanism\nto reliably assess intermediate reasoning steps. With MMReason, we benchmark\npopular leading MLLMs and provide an in-depth analysis of their reasoning\ncapabilities. We hope MMReason will serve as a valuable resource for advancing\nMLLM reasoning research. Code will be available at\nhttps://github.com/HJYao00/MMReason.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MMReason\uff0c\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u7cbe\u786e\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u957f\u94fe\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u63d0\u4f9b\u591a\u6837\u5316\u3001\u5f00\u653e\u5f0f\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u3002", "motivation": "\u73b0\u6709MLLM\u57fa\u51c6\u5728\u8bc4\u4f30\u957f\u94fe\u63a8\u7406\u80fd\u529b\u65f6\u5b58\u5728\u4e0d\u8db3\uff1a\u7f3a\u4e4f\u96be\u5ea6\u548c\u591a\u6837\u6027\u3001\u6613\u53d7\u731c\u6d4b\u548c\u8bb0\u5fc6\u5f71\u54cd\u3001\u4ee5\u53ca\u5bf9\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u8bc4\u4f30\u4e0d\u8db3\uff0c\u8fd9\u4e9b\u90fd\u963b\u788d\u4e86MLLM\u5411\u901a\u7528\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u3002", "method": "\u5f15\u5165MMReason\u57fa\u51c6\uff0c\u901a\u8fc7\u4ee5\u4e0b\u65b9\u6cd5\u6784\u5efa\uff1a1) \u7b56\u5212\u9700\u8981\u591a\u6b65\u9aa4\u63a8\u7406\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u6db5\u76d66\u4e2a\u5b66\u79d1\u548c\u591a\u96be\u5ea6\u7ea7\u522b\uff1b2) \u5c06\u95ee\u9898\u91cd\u6784\u4e3a\u5f00\u653e\u5f0f\u683c\u5f0f\uff0c\u5e76\u4f7f\u7528\u591a\u6a21\u578b\u6295\u7968\u8fc7\u6ee4\u4ee5\u6d88\u9664\u731c\u6d4b\u548c\u8bb0\u5fc6\u7684\u6377\u5f84\uff1b3) \u6807\u6ce8\u8be6\u7ec6\u7684\u9010\u6b65\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u53c2\u8003\u7684\u4e09\u5143\u8bc4\u5206\u673a\u5236\u6765\u53ef\u9760\u8bc4\u4f30\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u4f7f\u7528MMReason\u5bf9\u4e3b\u6d41MLLM\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5bf9\u5176\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\u3002", "conclusion": "MMReason\u6709\u671b\u6210\u4e3a\u63a8\u52a8MLLM\u63a8\u7406\u7814\u7a76\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u4e3a\u672a\u6765MLLM\u7684\u53d1\u5c55\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2506.22710", "pdf": "https://arxiv.org/pdf/2506.22710", "abs": "https://arxiv.org/abs/2506.22710", "authors": ["Jiang Yuan", "JI Ma", "Bo Wang", "Guanzhou Ke", "Weiming Hu"], "title": "LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Implicit degradation estimation-based blind super-resolution (IDE-BSR) hinges\non extracting the implicit degradation representation (IDR) of the LR image and\nadapting it to LR image features to guide HR detail restoration. Although\nIDE-BSR has shown potential in dealing with noise interference and complex\ndegradations, existing methods ignore the importance of IDR discriminability\nfor BSR and instead over-complicate the adaptation process to improve effect,\nresulting in a significant increase in the model's parameters and computations.\nIn this paper, we focus on the discriminability optimization of IDR and propose\na new powerful and lightweight BSR model termed LightBSR. Specifically, we\nemploy a knowledge distillation-based learning framework. We first introduce a\nwell-designed degradation-prior-constrained contrastive learning technique\nduring teacher stage to make the model more focused on distinguishing different\ndegradation types. Then we utilize a feature alignment technique to transfer\nthe degradation-related knowledge acquired by the teacher to the student for\npractical inferencing. Extensive experiments demonstrate the effectiveness of\nIDR discriminability-driven BSR model design. The proposed LightBSR can achieve\noutstanding performance with minimal complexity across a range of blind SR\ntasks. Our code is accessible at: https://github.com/MJ-NCEPU/LightBSR.", "AI": {"tldr": "\u73b0\u6709\u76f2\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5ffd\u89c6\u9690\u5f0f\u9000\u5316\u8868\u793a\uff08IDR\uff09\u7684\u53ef\u533a\u5206\u6027\u4e14\u8fc7\u4e8e\u590d\u6742\u3002\u672c\u6587\u63d0\u51faLightBSR\uff0c\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316IDR\u7684\u53ef\u533a\u5206\u6027\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u9690\u5f0f\u9000\u5316\u4f30\u8ba1\u7684\u76f2\u8d85\u5206\u8fa8\u7387\uff08IDE-BSR\uff09\u65b9\u6cd5\u5ffd\u7565\u4e86\u9690\u5f0f\u9000\u5316\u8868\u793a\uff08IDR\uff09\u7684\u53ef\u533a\u5206\u6027\uff0c\u5e76\u8fc7\u5ea6\u590d\u6742\u5316\u4e86\u9002\u5e94\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u6a21\u578b\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u663e\u8457\u589e\u52a0\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u5f3a\u5927\u4e14\u8f7b\u91cf\u7ea7\u7684BSR\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316IDR\u7684\u53ef\u533a\u5206\u6027\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u5b66\u4e60\u6846\u67b6\u3002\u5728\u6559\u5e08\u9636\u6bb5\uff0c\u5f15\u5165\u9000\u5316\u5148\u9a8c\u7ea6\u675f\u7684\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u4e0d\u540c\u9000\u5316\u7c7b\u578b\u7684\u533a\u5206\u80fd\u529b\u3002\u968f\u540e\uff0c\u5229\u7528\u7279\u5f81\u5bf9\u9f50\u6280\u672f\u5c06\u6559\u5e08\u5b66\u4e60\u5230\u7684\u9000\u5316\u76f8\u5173\u77e5\u8bc6\u8fc1\u79fb\u7ed9\u5b66\u751f\uff0c\u7528\u4e8e\u5b9e\u9645\u63a8\u7406\u3002", "result": "\u6240\u63d0\u51fa\u7684LightBSR\u6a21\u578b\u5728\u5404\u79cd\u76f2\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u4ee5\u6700\u5c0f\u7684\u590d\u6742\u5ea6\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86IDR\u53ef\u533a\u5206\u6027\u9a71\u52a8\u7684BSR\u6a21\u578b\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5173\u6ce8\u5e76\u4f18\u5316\u9690\u5f0f\u9000\u5316\u8868\u793a\uff08IDR\uff09\u7684\u53ef\u533a\u5206\u6027\uff0c\u53ef\u4ee5\u8bbe\u8ba1\u51fa\u5f3a\u5927\u4e14\u8f7b\u91cf\u7ea7\u7684\u76f2\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u590d\u6742\u65b9\u6cd5\u3002LightBSR\u6a21\u578b\u901a\u8fc7\u4ee5\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u5353\u8d8a\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u6982\u5ff5\u3002"}}
{"id": "2506.23056", "pdf": "https://arxiv.org/pdf/2506.23056", "abs": "https://arxiv.org/abs/2506.23056", "authors": ["Xiang Zhuang", "Bin Wu", "Jiyu Cui", "Kehua Feng", "Xiaotong Li", "Huabin Xing", "Keyan Ding", "Qiang Zhang", "Huajun Chen"], "title": "Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning", "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Molecular structure elucidation involves deducing a molecule's structure from\nvarious types of spectral data, which is crucial in chemical experimental\nanalysis. While large language models (LLMs) have shown remarkable proficiency\nin analyzing and reasoning through complex tasks, they still encounter\nsubstantial challenges in molecular structure elucidation. We identify that\nthese challenges largely stem from LLMs' limited grasp of specialized chemical\nknowledge. In this work, we introduce a Knowledge-enhanced reasoning framework\nfor Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search\nfor test-time scaling as a plugin. Specifically, we construct an external\nmolecular substructure knowledge base to extend the LLMs' coverage of the\nchemical structure space. Furthermore, we design a specialized\nmolecule-spectrum scorer to act as a reward model for the reasoning process,\naddressing the issue of inaccurate solution evaluation in LLMs. Experimental\nresults show that our approach significantly boosts performance, particularly\ngaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is\navailable at https://github.com/HICAI-ZJU/K-MSE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faK-MSE\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u3001\u5916\u90e8\u5206\u5b50\u5b50\u7ed3\u6784\u77e5\u8bc6\u5e93\u548c\u5206\u5b50\u5149\u8c31\u8bc4\u5206\u5668\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728GPT-4o\u7cfb\u5217\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc720%\u7684\u63d0\u5347\u3002", "motivation": "\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u5728\u5316\u5b66\u5b9e\u9a8c\u5206\u6790\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u5b83\u4eec\u5bf9\u4e13\u4e1a\u5316\u5b66\u77e5\u8bc6\u638c\u63e1\u6709\u9650\u3002", "method": "\u5f15\u5165\u4e86\u77e5\u8bc6\u589e\u5f3a\u7684\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u63a8\u7406\u6846\u67b6\uff08K-MSE\uff09\u3002\u8be5\u6846\u67b6\u5229\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u8fdb\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u6784\u5efa\u5916\u90e8\u5206\u5b50\u5b50\u7ed3\u6784\u77e5\u8bc6\u5e93\u4ee5\u6269\u5c55LLMs\u5bf9\u5316\u5b66\u7ed3\u6784\u7a7a\u95f4\u7684\u8986\u76d6\uff0c\u5e76\u8bbe\u8ba1\u4e13\u95e8\u7684\u5206\u5b50-\u5149\u8c31\u8bc4\u5206\u5668\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\uff0c\u89e3\u51b3LLMs\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728GPT-4o-mini\u548cGPT-4o\u4e0a\u5747\u83b7\u5f97\u4e86\u8d85\u8fc720%\u7684\u63d0\u5347\u3002", "conclusion": "K-MSE\u6846\u67b6\u901a\u8fc7\u6709\u6548\u878d\u5408\u5916\u90e8\u5316\u5b66\u77e5\u8bc6\u548c\u7cbe\u786e\u7684\u8bc4\u4f30\u673a\u5236\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u7684\u80fd\u529b\u3002"}}
{"id": "2506.23788", "pdf": "https://arxiv.org/pdf/2506.23788", "abs": "https://arxiv.org/abs/2506.23788", "authors": ["Naomi Stricker", "David Blaser", "Andres Gomez", "Lothar Thiele"], "title": "E-WAN: Efficient Communication in Energy Harvesting Low-Power Networks", "categories": ["eess.SP", "cs.NI"], "comment": "This is the author's version of the work. Submitted to ACM TOSN on\n  June 2023. Major revision submitted on May 2024. Minor Revision submitted on\n  March 2025", "summary": "The ever-increasing number of distributed embedded systems in the context of\nthe Internet of Things (IoT), Wireless Sensor Networks (WSN), and\nCyber-Physical Systems (CPS) rely on wireless communication to collect and\nexchange data. Nodes can employ single-hop communication which, despite its\nease, may necessitate energy-intensive long-range communication to cover long\ndistances. Conversely, multi-hop communication allows for more energy-efficient\nshort-range communication since nodes can rely on other nodes to forward their\ndata. Yet, this approach requires relay nodes to be available and continuous\nmaintenance of a dynamically changing distributed state. At the same time,\nenergy harvesting has the potential to outperform traditional battery-based\nsystems by improving their lifetime, scalability with lower maintenance costs,\nand environmental impact. However, the limited and temporally and spatially\nvariable harvested energy poses significant challenges for networking in energy\nharvesting networks, particularly considering the energy demands and\ncharacteristics of both multi-hop and single-hop communication. We propose\nE-WAN, a protocol for energy harvesting wide-area low-power networks that\nbuilds on the concept of \\emph{virtual sub-networks} to enable\nresource-efficient multi-hop communication when possible and reliable however\nenergy-intensive point-to-point communication otherwise. Nodes autonomously and\ndynamically move between the two and adjust to changing network states and\nresources based only on easily obtainable network state information. We\nillustrate E-WAN's advantages both in terms of efficiency and adaptability in\nvarious communication and harvesting scenarios. Furthermore, we demonstrate\nE-WAN operating in a realistic setting by deploying an energy harvesting\nnetwork in a real-world indoor environment.", "AI": {"tldr": "E-WAN\u662f\u4e00\u79cd\u7528\u4e8e\u80fd\u91cf\u6536\u96c6\u7f51\u7edc\u7684\u534f\u8bae\uff0c\u5b83\u80fd\u6839\u636e\u80fd\u91cf\u548c\u7f51\u7edc\u72b6\u6001\u667a\u80fd\u5207\u6362\u591a\u8df3\u548c\u5355\u8df3\u901a\u4fe1\uff0c\u4ee5\u4f18\u5316\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5206\u5e03\u5f0f\u5d4c\u5165\u5f0f\u7cfb\u7edf\uff08IoT/WSN/CPS\uff09\u7684\u65e0\u7ebf\u901a\u4fe1\u9762\u4e34\u591a\u8df3\u4e0e\u5355\u8df3\u7684\u80fd\u6548\u4e0e\u7b80\u5355\u6027\u6743\u8861\u3002\u540c\u65f6\uff0c\u80fd\u91cf\u6536\u96c6\u6280\u672f\u7684\u6709\u9650\u548c\u591a\u53d8\u6027\u5bf9\u7f51\u7edc\u901a\u4fe1\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u9002\u5e94\u80fd\u91cf\u6536\u96c6\u7ea6\u675f\u5e76\u4f18\u5316\u901a\u4fe1\u65b9\u5f0f\u7684\u534f\u8bae\u3002", "method": "\u672c\u6587\u63d0\u51faE-WAN\u534f\u8bae\uff0c\u4e13\u4e3a\u80fd\u91cf\u6536\u96c6\u5e7f\u57df\u4f4e\u529f\u8017\u7f51\u7edc\u8bbe\u8ba1\u3002\u8be5\u534f\u8bae\u57fa\u4e8e\u201c\u865a\u62df\u5b50\u7f51\u7edc\u201d\u6982\u5ff5\uff0c\u4f7f\u8282\u70b9\u80fd\u81ea\u4e3b\u3001\u52a8\u6001\u5730\u5728\u8d44\u6e90\u9ad8\u6548\u7684\u591a\u8df3\u901a\u4fe1\uff08\u53ef\u884c\u65f6\uff09\u548c\u53ef\u9760\u4f46\u8017\u80fd\u7684\u70b9\u5bf9\u70b9\u901a\u4fe1\uff08\u5426\u5219\uff09\u4e4b\u95f4\u5207\u6362\uff0c\u4ec5\u4f9d\u8d56\u6613\u4e8e\u83b7\u53d6\u7684\u7f51\u7edc\u72b6\u6001\u4fe1\u606f\u8fdb\u884c\u8c03\u6574\u3002", "result": "E-WAN\u5728\u591a\u79cd\u901a\u4fe1\u548c\u80fd\u91cf\u6536\u96c6\u573a\u666f\u4e0b\uff0c\u5747\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u4f18\u52bf\u3002\u6b64\u5916\uff0c\u8be5\u534f\u8bae\u5df2\u5728\u771f\u5b9e\u5ba4\u5185\u73af\u5883\u4e2d\u6210\u529f\u90e8\u7f72\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u8bbe\u7f6e\u4e0b\u7684\u8fd0\u884c\u80fd\u529b\u3002", "conclusion": "E-WAN\u901a\u8fc7\u6839\u636e\u5b9e\u65f6\u7f51\u7edc\u548c\u80fd\u91cf\u6761\u4ef6\u667a\u80fd\u5730\u5229\u7528\u591a\u8df3\u548c\u5355\u8df3\u901a\u4fe1\uff0c\u4e3a\u80fd\u91cf\u6536\u96c6\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u5ef6\u957f\u4e86\u7f51\u7edc\u5bff\u547d\u5e76\u786e\u4fdd\u4e86\u6570\u636e\u4ea4\u6362\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2506.22809", "pdf": "https://arxiv.org/pdf/2506.22809", "abs": "https://arxiv.org/abs/2506.22809", "authors": ["Cooper Doyle"], "title": "BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "13 pages, 3 figures, 1 table", "summary": "We propose BayesLoRA, a task-specific uncertainty quantification framework\nthat integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike\ngeneral-purpose transformer uncertainty methods, BayesLoRA provides guardrails\ntailored to downstream workflows, enabling agents to introspect and modulate\nbehavior under uncertainty. We demonstrate mathematically and empirically that\nLoRA adapters exhibit amplified variance outside fine-tuning distributions,\nyielding reliable confidence estimates for agentic decision-making.", "AI": {"tldr": "\u63d0\u51faBayesLoRA\uff0c\u4e00\u4e2a\u5c06MC-Dropout\u96c6\u6210\u5230LoRA\u4e2d\u7684\u4efb\u52a1\u4e13\u7528\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u4ee3\u7406\u51b3\u7b56\u63d0\u4f9b\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u901a\u7528Transformer\u4e0d\u786e\u5b9a\u6027\u65b9\u6cd5\u672a\u80fd\u4e3a\u4e0b\u6e38\u5de5\u4f5c\u6d41\u63d0\u4f9b\u5b9a\u5236\u5316\u4fdd\u969c\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u8ba9\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u81ea\u7701\u548c\u8c03\u6574\u884c\u4e3a\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faBayesLoRA\uff0c\u901a\u8fc7\u5c06MC-Dropout\u96c6\u6210\u5230\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\u4e2d\uff0c\u6784\u5efa\u4efb\u52a1\u4e13\u7528\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\u3002", "result": "\u6570\u5b66\u548c\u7ecf\u9a8c\u8bc1\u660e\uff0cLoRA\u9002\u914d\u5668\u5728\u5fae\u8c03\u5206\u5e03\u4e4b\u5916\u8868\u73b0\u51fa\u653e\u5927\u7684\u65b9\u5dee\uff0c\u80fd\u4e3a\u4ee3\u7406\u51b3\u7b56\u63d0\u4f9b\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002", "conclusion": "BayesLoRA\u80fd\u591f\u4e3a\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u51b3\u7b56\u63d0\u4f9b\u53ef\u9760\u7684\u4fe1\u5fc3\u4f30\u8ba1\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u81ea\u7701\u548c\u8c03\u6574\u884c\u4e3a\uff0c\u4e3a\u4e0b\u6e38\u5de5\u4f5c\u6d41\u63d0\u4f9b\u91cf\u8eab\u5b9a\u5236\u7684\u4fdd\u969c\u3002"}}
{"id": "2506.23576", "pdf": "https://arxiv.org/pdf/2506.23576", "abs": "https://arxiv.org/abs/2506.23576", "authors": ["Maria Carolina Cornelia Wit", "Jun Pang"], "title": "Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models", "categories": ["cs.AI"], "comment": "26 pages, 1 figure", "summary": "Recent advances in large language models (LLMs) have raised concerns about\njailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper\ninvestigates the use of multi-agent LLM systems as a defence against such\nattacks. We evaluate three jailbreaking strategies, including the original\nAutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the\nAutoDefense framework, we compare single-agent setups with two- and three-agent\nconfigurations. Our results show that multi-agent systems enhance resistance to\njailbreaks, especially by reducing false negatives. However, its effectiveness\nvaries by attack type, and it introduces trade-offs such as increased false\npositives and computational overhead. These findings point to the limitations\nof current automated defences and suggest directions for improving alignment\nrobustness in future LLM systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4f5c\u4e3a\u5bf9\u6297\u8d8a\u72f1\u653b\u51fb\u7684\u9632\u5fa1\u624b\u6bb5\uff0c\u53d1\u73b0\u5176\u80fd\u589e\u5f3a\u62b5\u6297\u529b\uff0c\u4f46\u5b58\u5728\u6709\u6548\u6027\u968f\u653b\u51fb\u7c7b\u578b\u53d8\u5316\u3001\u5047\u9633\u6027\u589e\u52a0\u548c\u8ba1\u7b97\u5f00\u9500\u7b49\u6743\u8861\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u5bf9\u5176\u8d8a\u72f1\u653b\u51fb\uff08\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\u7684\u63d0\u793a\uff09\u7684\u62c5\u5fe7\uff0c\u4fc3\u4f7f\u7814\u7a76\u5982\u4f55\u6709\u6548\u9632\u5fa1\u6b64\u7c7b\u653b\u51fb\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u8d8a\u72f1\u7b56\u7565\uff08AutoDefense\u3001BetterDan\u548cJB\uff09\uff0c\u5e76\u901a\u8fc7\u590d\u73b0AutoDefense\u6846\u67b6\uff0c\u6bd4\u8f83\u4e86\u5355\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e0e\u4e24\u3001\u4e09\u667a\u80fd\u4f53\u914d\u7f6e\u4e0b\u7684\u9632\u5fa1\u6548\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u589e\u5f3a\u5bf9\u8d8a\u72f1\u7684\u62b5\u6297\u529b\uff0c\u5c24\u5176\u5728\u51cf\u5c11\u5047\u9634\u6027\u65b9\u9762\u8868\u73b0\u663e\u8457\u3002\u7136\u800c\uff0c\u5176\u6709\u6548\u6027\u56e0\u653b\u51fb\u7c7b\u578b\u800c\u5f02\uff0c\u5e76\u5f15\u5165\u4e86\u5047\u9633\u6027\u589e\u52a0\u548c\u8ba1\u7b97\u5f00\u9500\u7b49\u6743\u8861\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u5f53\u524d\u81ea\u52a8\u5316\u9632\u5fa1\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765LLM\u7cfb\u7edf\u63d0\u5347\u5bf9\u9f50\u9c81\u68d2\u6027\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.22718", "pdf": "https://arxiv.org/pdf/2506.22718", "abs": "https://arxiv.org/abs/2506.22718", "authors": ["Jun-Jee Chao", "Qingyuan Jiang", "Volkan Isler"], "title": "Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians", "categories": ["cs.CV"], "comment": null, "summary": "Part segmentation and motion estimation are two fundamental problems for\narticulated object motion analysis. In this paper, we present a method to solve\nthese two problems jointly from a sequence of observed point clouds of a single\narticulated object. The main challenge in our problem setting is that the point\nclouds are not assumed to be generated by a fixed set of moving points.\nInstead, each point cloud in the sequence could be an arbitrary sampling of the\nobject surface at that particular time step. Such scenarios occur when the\nobject undergoes major occlusions, or if the dataset is collected using\nmeasurements from multiple sensors asynchronously. In these scenarios, methods\nthat rely on tracking point correspondences are not appropriate. We present an\nalternative approach based on a compact but effective representation where we\nrepresent the object as a collection of simple building blocks modeled as 3D\nGaussians. We parameterize the Gaussians with time-dependent rotations,\ntranslations, and scales that are shared across all time steps. With our\nrepresentation, part segmentation can be achieved by building correspondences\nbetween the observed points and the Gaussians. Moreover, the transformation of\neach point across time can be obtained by following the poses of the assigned\nGaussian (even when the point is not observed). Experiments show that our\nmethod outperforms existing methods that solely rely on finding point\ncorrespondences. Additionally, we extend existing datasets to emulate\nreal-world scenarios by considering viewpoint occlusions. We further\ndemonstrate that our method is more robust to missing points as compared to\nexisting approaches on these challenging datasets, even when some parts are\ncompletely occluded in some time-steps. Notably, our part segmentation\nperformance outperforms the state-of-the-art method by 13% on point clouds with\nocclusions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u94f0\u63a5\u7269\u4f53\u5efa\u6a21\u4e3a3D\u9ad8\u65af\u5757\u96c6\u5408\uff0c\u89e3\u51b3\u4e86\u5728\u70b9\u4e91\u4e0d\u5177\u6709\u56fa\u5b9a\u5bf9\u5e94\u5173\u7cfb\uff08\u5982\u5b58\u5728\u4e25\u91cd\u906e\u6321\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u8054\u5408\u8fdb\u884c\u90e8\u4ef6\u5206\u5272\u548c\u8fd0\u52a8\u4f30\u8ba1\u7684\u96be\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u5353\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u94f0\u63a5\u7269\u4f53\u7684\u90e8\u4ef6\u5206\u5272\u548c\u8fd0\u52a8\u4f30\u8ba1\u662f\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u70b9\u5bf9\u5e94\u5173\u7cfb\uff0c\u4f46\u5728\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u70b9\u4e91\u53ef\u80fd\u7531\u4efb\u610f\u91c7\u6837\u4ea7\u751f\uff0c\u5b58\u5728\u4e25\u91cd\u906e\u6321\u6216\u6765\u81ea\u5f02\u6b65\u591a\u4f20\u611f\u5668\uff0c\u5bfc\u81f4\u70b9\u5bf9\u5e94\u5173\u7cfb\u4e0d\u53ef\u9760\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u56fa\u5b9a\u70b9\u5bf9\u5e94\u5173\u7cfb\u7684\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u4e00\u79cd\u7d27\u51d1\u800c\u6709\u6548\u7684\u8868\u793a\uff1a\u5c06\u94f0\u63a5\u7269\u4f53\u5efa\u6a21\u4e3a\u4e00\u7cfb\u5217\u7b80\u5355\u76843D\u9ad8\u65af\u5757\u3002\u8fd9\u4e9b\u9ad8\u65af\u5757\u88ab\u53c2\u6570\u5316\u4e3a\u65f6\u95f4\u76f8\u5173\u7684\u65cb\u8f6c\u3001\u5e73\u79fb\u548c\u5c3a\u5ea6\uff0c\u5e76\u5728\u6240\u6709\u65f6\u95f4\u6b65\u957f\u4e2d\u5171\u4eab\u3002\u901a\u8fc7\u5728\u89c2\u6d4b\u70b9\u548c\u9ad8\u65af\u5757\u4e4b\u95f4\u5efa\u7acb\u5bf9\u5e94\u5173\u7cfb\u6765\u5b9e\u73b0\u90e8\u4ef6\u5206\u5272\uff0c\u800c\u70b9\u7684\u8fd0\u52a8\u53d8\u6362\u5219\u901a\u8fc7\u8ddf\u8e2a\u5176\u5206\u914d\u7684\u9ad8\u65af\u5757\u7684\u59ff\u6001\u6765\u83b7\u5f97\uff0c\u5373\u4f7f\u8be5\u70b9\u672a\u88ab\u89c2\u6d4b\u5230\u3002", "result": ["\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u4e8e\u5bfb\u627e\u70b9\u5bf9\u5e94\u5173\u7cfb\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "\u5728\u8003\u8651\u89c6\u70b9\u906e\u6321\u7684\u6269\u5c55\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5bf9\u7f3a\u5931\u70b9\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u5728\u67d0\u4e9b\u65f6\u95f4\u6b65\u957f\u4e2d\u90e8\u5206\u90e8\u4ef6\u5b8c\u5168\u88ab\u906e\u6321\u3002", "\u5728\u5e26\u6709\u906e\u6321\u7684\u70b9\u4e91\u4e0a\uff0c\u90e8\u4ef6\u5206\u5272\u6027\u80fd\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u9ad8\u51fa13%\u3002"], "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u65b0\u76843D\u9ad8\u65af\u5757\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u94f0\u63a5\u7269\u4f53\u5728\u590d\u6742\u70b9\u4e91\uff08\u5982\u5b58\u5728\u4e25\u91cd\u906e\u6321\u548c\u7f3a\u5931\u70b9\uff09\u573a\u666f\u4e0b\u7684\u90e8\u4ef6\u5206\u5272\u548c\u8fd0\u52a8\u4f30\u8ba1\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u6027\u80fd\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u4f9d\u8d56\u70b9\u5bf9\u5e94\u5173\u7cfb\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u906e\u6321\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.23071", "pdf": "https://arxiv.org/pdf/2506.23071", "abs": "https://arxiv.org/abs/2506.23071", "authors": ["Zhengren Wang", "Bozhou Li", "Dongwen Yao", "Wentao Zhang"], "title": "Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries", "categories": ["cs.CL"], "comment": "Work in progess", "summary": "While Text-to-SQL enables natural language interaction with structured\ndatabases, its effectiveness diminishes with unstructured data or ambiguous\nqueries due to rigid syntax and limited expressiveness. Concurrently, vector\nsearch has emerged as a powerful paradigm for semantic retrieval, particularly\nfor unstructured data. However, existing VectorSQL implementations still rely\nheavily on manual crafting and lack tailored evaluation frameworks, leaving a\nsignificant gap between theoretical potential and practical deployment. To\nbridge these complementary paradigms, we introduces Text2VectorSQL, a novel\nframework unifying Text-to-SQL and vector search to overcome expressiveness\nconstraints and support more diverse and holistical natural language queries.\nSpecifically, Text2VectorSQL enables semantic filtering, multi-modal matching,\nand retrieval acceleration. For evaluation, we build vector index on\nappropriate columns, extend user queries with semantic search, and annotate\nground truths via an automatic pipeline with expert review. Furthermore, we\ndevelop dedicated Text2VectorSQL models with synthetic data, demonstrating\nsignificant performance improvements over baseline methods. Our work\nestablishes the foundation for the Text2VectorSQL task, paving the way for more\nversatile and intuitive database interfaces. The repository will be publicly\navailable at https://github.com/Open-DataFlow/Text2VectorSQL.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f15\u5165Text2VectorSQL\u6846\u67b6\uff0c\u6574\u5408Text-to-SQL\u548c\u5411\u91cf\u641c\u7d22\uff0c\u4ee5\u514b\u670d\u73b0\u6709Text-to-SQL\u5728\u5904\u7406\u975e\u7ed3\u6784\u5316\u6570\u636e\u548c\u6a21\u7cca\u67e5\u8be2\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684Text-to-SQL\u5728\u5904\u7406\u975e\u7ed3\u6784\u5316\u6570\u636e\u6216\u6a21\u7cca\u67e5\u8be2\u65f6\uff0c\u56e0\u5176\u6b7b\u677f\u7684\u8bed\u6cd5\u548c\u6709\u9650\u7684\u8868\u8fbe\u80fd\u529b\u800c\u6548\u7387\u4f4e\u4e0b\u3002\u5c3d\u7ba1\u5411\u91cf\u641c\u7d22\u5728\u8bed\u4e49\u68c0\u7d22\u65b9\u9762\u5f3a\u5927\uff0c\u4f46\u5f53\u524d\u7684VectorSQL\u5b9e\u73b0\u7f3a\u4e4f\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u91cf\u8eab\u5b9a\u5236\u7684\u65b9\u6cd5\uff0c\u5bfc\u81f4\u5176\u7406\u8bba\u6f5c\u529b\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Text2VectorSQL\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408Text-to-SQL\u548c\u5411\u91cf\u641c\u7d22\u6765\u89e3\u51b3\u8868\u8fbe\u80fd\u529b\u9650\u5236\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u6784\u5efa\u5411\u91cf\u7d22\u5f15\u3001\u6269\u5c55\u7528\u6237\u67e5\u8be2\u4ee5\u652f\u6301\u8bed\u4e49\u641c\u7d22\u3001\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff08\u8f85\u4ee5\u4e13\u5bb6\u8bc4\u5ba1\uff09\u6807\u6ce8\u771f\u503c\uff0c\u5e76\u5f00\u53d1\u5229\u7528\u5408\u6210\u6570\u636e\u7684Text2VectorSQL\u4e13\u7528\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u8bed\u4e49\u8fc7\u6ee4\u3001\u591a\u6a21\u6001\u5339\u914d\u548c\u68c0\u7d22\u52a0\u901f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u5f00\u53d1\u7684Text2VectorSQL\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aText2VectorSQL\u4efb\u52a1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u5f00\u53d1\u66f4\u901a\u7528\u548c\u76f4\u89c2\u7684\u6570\u636e\u5e93\u63a5\u53e3\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2506.22821", "pdf": "https://arxiv.org/pdf/2506.22821", "abs": "https://arxiv.org/abs/2506.22821", "authors": ["Thomas Gaskin", "Guy J. Abel"], "title": "Deep learning 40 years of human migration", "categories": ["cs.LG", "68T07", "I.2.6"], "comment": null, "summary": "We present a novel and detailed dataset on origin-destination annual\nmigration flows and stocks between 230 countries and regions, spanning the\nperiod from 1990 to the present. Our flow estimates are further disaggregated\nby country of birth, providing a comprehensive picture of migration over the\nlast 43 years. The estimates are obtained by training a deep recurrent neural\nnetwork to learn flow patterns from 18 covariates for all countries, including\ngeographic, economic, cultural, societal, and political information. The\nrecurrent architecture of the neural network means that the entire past can\ninfluence current migration patterns, allowing us to learn long-range temporal\ncorrelations. By training an ensemble of neural networks and additionally\npushing uncertainty on the covariates through the trained network, we obtain\nconfidence bounds for all our estimates, allowing researchers to pinpoint the\ngeographic regions most in need of additional data collection. We validate our\napproach on various test sets of unseen data, demonstrating that it\nsignificantly outperforms traditional methods estimating five-year flows while\ndelivering a significant increase in temporal resolution. The model is fully\nopen source: all training data, neural network weights, and training code are\nmade public alongside the migration estimates, providing a valuable resource\nfor future studies of human migration.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u521b\u65b0\u7684\u3001\u8be6\u7ec6\u7684\u5168\u7403\u5e74\u5ea6\u79fb\u6c11\u6d41\u52a8\u548c\u5b58\u91cf\u6570\u636e\u96c6\uff081990\u5e74\u81f3\u4eca\uff09\uff0c\u6db5\u76d6230\u4e2a\u56fd\u5bb6\u548c\u5730\u533a\uff0c\u5e76\u6309\u51fa\u751f\u56fd\u7ec6\u5206\u3002\u8be5\u6570\u636e\u901a\u8fc7\u6df1\u5ea6\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5efa\u6a2118\u4e2a\u534f\u53d8\u91cf\u83b7\u5f97\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5e76\u5b8c\u5168\u5f00\u6e90\u3002", "motivation": "\u73b0\u6709\u79fb\u6c11\u6570\u636e\u53ef\u80fd\u7f3a\u4e4f\u8db3\u591f\u7684\u7ec6\u8282\u3001\u65f6\u95f4\u5206\u8fa8\u7387\u6216\u51c6\u786e\u6027\u3002\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u66f4\u5168\u9762\u3001\u65f6\u95f4\u5206\u8fa8\u7387\u66f4\u9ad8\u4e14\u66f4\u51c6\u786e\u7684\u5168\u7403\u79fb\u6c11\u56fe\u666f\uff0c\u4ee5\u652f\u6301\u5bf9\u4eba\u7c7b\u79fb\u6c11\u7684\u6df1\u5165\u7814\u7a76\uff0c\u5e76\u8bc6\u522b\u6570\u636e\u6536\u96c6\u7684\u8584\u5f31\u533a\u57df\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u662f\uff1a1. \u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b230\u4e2a\u56fd\u5bb6/\u5730\u533a\u95f4\u5e74\u5ea6\u79fb\u6c11\u6d41\u52a8\u548c\u5b58\u91cf\u7684\u539f\u521b\u6570\u636e\u96c6\uff0c\u5e76\u6309\u51fa\u751f\u56fd\u7ec6\u5206\u30022. \u4f7f\u7528\u4e00\u4e2a\u6df1\u5ea6\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u6765\u5b66\u4e60\u79fb\u6c11\u6a21\u5f0f\uff0c\u8f93\u5165\u5305\u62ec\u5730\u7406\u3001\u7ecf\u6d4e\u3001\u6587\u5316\u3001\u793e\u4f1a\u548c\u653f\u6cbb\u7b4918\u4e2a\u534f\u53d8\u91cf\u30023. \u901a\u8fc7RNN\u7684\u5faa\u73af\u7ed3\u6784\u6355\u6349\u957f\u671f\u65f6\u95f4\u5173\u8054\u30024. \u91c7\u7528\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\u65b9\u6cd5\u5e76\u8003\u8651\u534f\u53d8\u91cf\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u83b7\u5f97\u4f30\u8ba1\u503c\u7684\u7f6e\u4fe1\u533a\u95f4\u30025. \u6a21\u578b\u53ca\u6240\u6709\u76f8\u5173\u6570\u636e\u3001\u4ee3\u7801\u5747\u5f00\u6e90\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5305\u62ec\uff1a1. \u6210\u529f\u751f\u6210\u4e86\u4e00\u4e2a\u6db5\u76d61990\u5e74\u81f3\u4eca\uff0c\u6309\u51fa\u751f\u56fd\u7ec6\u5206\u7684\u5e74\u5ea6\u5168\u7403\u79fb\u6c11\u6d41\u52a8\u548c\u5b58\u91cf\u6570\u636e\u96c6\u30022. \u6a21\u578b\u5728\u5404\u79cd\u672a\u89c1\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u4e94\u5e74\u671f\u6d41\u52a8\u4f30\u8ba1\u65b9\u6cd5\u30023. \u5927\u5e45\u63d0\u9ad8\u4e86\u65f6\u95f4\u5206\u8fa8\u7387\u30024. \u63d0\u4f9b\u4e86\u6240\u6709\u4f30\u8ba1\u503c\u7684\u7f6e\u4fe1\u533a\u95f4\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u6700\u9700\u8981\u989d\u5916\u6570\u636e\u6536\u96c6\u7684\u5730\u7406\u533a\u57df\u30025. \u6240\u6709\u6a21\u578b\u3001\u6570\u636e\u548c\u4ee3\u7801\u5747\u5df2\u516c\u5f00\uff0c\u6210\u4e3a\u672a\u6765\u79fb\u6c11\u7814\u7a76\u7684\u5b9d\u8d35\u8d44\u6e90\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5f00\u53d1\u5e76\u5f00\u6e90\u4e86\u4e00\u4e2a\u521b\u65b0\u3001\u9ad8\u5206\u8fa8\u7387\u3001\u8be6\u7ec6\u4e14\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u5168\u7403\u5e74\u5ea6\u79fb\u6c11\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5148\u8fdb\u7684\u6df1\u5ea6\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u6c11\u6d41\u52a8\u7684\u4f30\u8ba1\u7cbe\u5ea6\u548c\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u4e3a\u672a\u6765\u7684\u79fb\u6c11\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u57fa\u7840\u8d44\u6e90\uff0c\u5e76\u80fd\u6307\u5bfc\u6570\u636e\u6536\u96c6\u5de5\u4f5c\u3002"}}
{"id": "2506.23626", "pdf": "https://arxiv.org/pdf/2506.23626", "abs": "https://arxiv.org/abs/2506.23626", "authors": ["Ant\u00f3nio Afonso", "Iolanda Leite", "Alessandro Sestini", "Florian Fuchs", "Konrad Tollmar", "Linus Gissl\u00e9n"], "title": "Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games", "categories": ["cs.AI"], "comment": "16 pages in total, 10 pages of main paper, 5 figures", "summary": "Reinforcement Learning (RL) in games has gained significant momentum in\nrecent years, enabling the creation of different agent behaviors that can\ntransform a player's gaming experience. However, deploying RL agents in\nproduction environments presents two key challenges: (1) designing an effective\nreward function typically requires an RL expert, and (2) when a game's content\nor mechanics are modified, previously tuned reward weights may no longer be\noptimal. Towards the latter challenge, we propose an automated approach for\niteratively fine-tuning an RL agent's reward function weights, based on a\nuser-defined language based behavioral goal. A Language Model (LM) proposes\nupdated weights at each iteration based on this target behavior and a summary\nof performance statistics from prior training rounds. This closed-loop process\nallows the LM to self-correct and refine its output over time, producing\nincreasingly aligned behavior without the need for manual reward engineering.\nWe evaluate our approach in a racing task and show that it consistently\nimproves agent performance across iterations. The LM-guided agents show a\nsignificant increase in performance from $9\\%$ to $74\\%$ success rate in just\none iteration. We compare our LM-guided tuning against a human expert's manual\nweight design in the racing task: by the final iteration, the LM-tuned agent\nachieved an $80\\%$ success rate, and completed laps in an average of $855$ time\nsteps, a competitive performance against the expert-tuned agent's peak $94\\%$\nsuccess, and $850$ time steps.", "AI": {"tldr": "\u9488\u5bf9\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u6e38\u620f\u4e2d\u7684\u5956\u52b1\u51fd\u6570\u9002\u5e94\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7531\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u8fed\u4ee3\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u884c\u4e3a\u76ee\u6807\u5b9e\u73b0RL\u667a\u80fd\u4f53\u5956\u52b1\u6743\u91cd\u7684\u4f18\u5316\uff0c\u5e76\u5728\u8d5b\u8f66\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u6e38\u620f\u73af\u5883\u4e2d\u90e8\u7f72RL\u667a\u80fd\u4f53\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1) \u8bbe\u8ba1\u6709\u6548\u7684\u5956\u52b1\u51fd\u6570\u901a\u5e38\u9700\u8981RL\u4e13\u5bb6\uff1b2) \u5f53\u6e38\u620f\u5185\u5bb9\u6216\u673a\u5236\u4fee\u6539\u65f6\uff0c\u9884\u8bbe\u7684\u5956\u52b1\u6743\u91cd\u53ef\u80fd\u4e0d\u518d\u6700\u4f18\u3002\u672c\u6587\u4e3b\u8981\u5173\u6ce8\u5e76\u65e8\u5728\u89e3\u51b3\u540e\u8005\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u8fed\u4ee3\u5fae\u8c03RL\u667a\u80fd\u4f53\u7684\u5956\u52b1\u51fd\u6570\u6743\u91cd\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u7528\u6237\u5b9a\u4e49\u7684\u8bed\u8a00\u884c\u4e3a\u76ee\u6807\uff0cLM\u6839\u636e\u6b64\u76ee\u6807\u548c\u5148\u524d\u8bad\u7ec3\u8f6e\u6b21\u7684\u6027\u80fd\u7edf\u8ba1\u6570\u636e\uff0c\u5728\u95ed\u73af\u8fc7\u7a0b\u4e2d\u63d0\u51fa\u5e76\u66f4\u65b0\u6743\u91cd\uff0c\u5b9e\u73b0\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u6301\u7eed\u5bf9\u9f50\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u5956\u52b1\u5de5\u7a0b\u3002", "result": "\u5728\u8d5b\u8f66\u4efb\u52a1\u4e2d\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u6027\u80fd\u3002LM\u5f15\u5bfc\u7684\u667a\u80fd\u4f53\u5728\u4e00\u6b21\u8fed\u4ee3\u540e\u6210\u529f\u7387\u4ece9%\u663e\u8457\u63d0\u5347\u81f374%\u3002\u6700\u7ec8\u8fed\u4ee3\u4e2d\uff0cLM\u8c03\u4f18\u7684\u667a\u80fd\u4f53\u8fbe\u523080%\u7684\u6210\u529f\u7387\uff0c\u5e73\u5747855\u6b65\u5b8c\u6210\u5355\u5708\uff0c\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8c03\u4f18\u667a\u80fd\u4f53\u7684\u5cf0\u503c94%\u6210\u529f\u7387\u548c850\u6b65\u76f8\u6bd4\uff0c\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u7531\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u5956\u52b1\u51fd\u6570\u5fae\u8c03\u7cfb\u7edf\uff0c\u6709\u6548\u89e3\u51b3\u4e86RL\u667a\u80fd\u4f53\u5728\u6e38\u620f\u5185\u5bb9\u53d8\u5316\u65f6\u5956\u52b1\u6743\u91cd\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\u3002\u8be5\u7cfb\u7edf\u65e0\u9700\u624b\u52a8\u5de5\u7a0b\u5373\u53ef\u6301\u7eed\u4f18\u5316\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u63a5\u8fd1\u4e13\u5bb6\u7ea7\u7684\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u4e86RL\u5e94\u7528\u90e8\u7f72\u7684\u95e8\u69db\u3002"}}
{"id": "2506.22720", "pdf": "https://arxiv.org/pdf/2506.22720", "abs": "https://arxiv.org/abs/2506.22720", "authors": ["Jinghao Wang", "Zhang Li", "Zi Wang", "Banglei Guan", "Yang Shang", "Qifeng Yu"], "title": "Deterministic Object Pose Confidence Region Estimation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "6D pose confidence region estimation has emerged as a critical direction,\naiming to perform uncertainty quantification for assessing the reliability of\nestimated poses. However, current sampling-based approach suffers from critical\nlimitations that severely impede their practical deployment: 1) the sampling\nspeed significantly decreases as the number of samples increases. 2) the\nderived confidence regions are often excessively large. To address these\nchallenges, we propose a deterministic and efficient method for estimating pose\nconfidence regions. Our approach uses inductive conformal prediction to\ncalibrate the deterministically regressed Gaussian keypoint distributions into\n2D keypoint confidence regions. We then leverage the implicit function theorem\nto propagate these keypoint confidence regions directly into 6D pose confidence\nregions. This method avoids the inefficiency and inflated region sizes\nassociated with sampling and ensembling. It provides compact confidence regions\nthat cover the ground-truth poses with a user-defined confidence level.\nExperimental results on the LineMOD Occlusion and SPEED datasets show that our\nmethod achieves higher pose estimation accuracy with reduced computational\ntime. For the same coverage rate, our method yields significantly smaller\nconfidence region volumes, reducing them by up to 99.9\\% for rotations and\n99.8\\% for translations. The code will be available soon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u786e\u5b9a\u6027\u4e14\u9ad8\u6548\u76846D\u59ff\u6001\u7f6e\u4fe1\u533a\u57df\u4f30\u8ba1\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5f52\u7eb3\u5171\u5f62\u9884\u6d4b\u548c\u9690\u51fd\u6570\u5b9a\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u91c7\u6837\u65b9\u6cd5\u901f\u5ea6\u6162\u3001\u533a\u57df\u8fc7\u5927\u7684\u95ee\u9898\uff0c\u5e76\u663e\u8457\u7f29\u5c0f\u4e86\u7f6e\u4fe1\u533a\u57df\u3002", "motivation": "6D\u59ff\u6001\u7f6e\u4fe1\u533a\u57df\u4f30\u8ba1\u5bf9\u4e8e\u8bc4\u4f30\u59ff\u6001\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u9650\u5236\uff1a1) \u91c7\u6837\u901f\u5ea6\u968f\u6837\u672c\u6570\u91cf\u589e\u52a0\u800c\u663e\u8457\u4e0b\u964d\uff1b2) \u6d3e\u751f\u7684\u7f6e\u4fe1\u533a\u57df\u901a\u5e38\u8fc7\u5927\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u786e\u5b9a\u6027\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u4f7f\u7528\u5f52\u7eb3\u5171\u5f62\u9884\u6d4b\u6821\u51c6\u786e\u5b9a\u6027\u56de\u5f52\u7684\u9ad8\u65af\u5173\u952e\u70b9\u5206\u5e03\uff0c\u5f97\u52302D\u5173\u952e\u70b9\u7f6e\u4fe1\u533a\u57df\u3002\u7136\u540e\uff0c\u5229\u7528\u9690\u51fd\u6570\u5b9a\u7406\u5c06\u8fd9\u4e9b2D\u5173\u952e\u70b9\u7f6e\u4fe1\u533a\u57df\u76f4\u63a5\u4f20\u64ad\u52306D\u59ff\u6001\u7f6e\u4fe1\u533a\u57df\u3002\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u91c7\u6837\u548c\u96c6\u6210\u5e26\u6765\u7684\u4f4e\u6548\u548c\u533a\u57df\u81a8\u80c0\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728LineMOD Occlusion\u548cSPEED\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u548c\u66f4\u77ed\u7684\u8ba1\u7b97\u65f6\u95f4\u3002\u5728\u76f8\u540c\u8986\u76d6\u7387\u4e0b\uff0c\u5176\u7f6e\u4fe1\u533a\u57df\u4f53\u79ef\u663e\u8457\u51cf\u5c0f\uff0c\u65cb\u8f6c\u90e8\u5206\u6700\u591a\u51cf\u5c1199.9%\uff0c\u5e73\u79fb\u90e8\u5206\u6700\u591a\u51cf\u5c1199.8%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u786e\u5b9a\u6027\u65b9\u6cd5\u6709\u6548\u514b\u670d\u4e86\u73b0\u6709\u91c7\u6837\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u63d0\u4f9b\u7d27\u51d1\u76846D\u59ff\u6001\u7f6e\u4fe1\u533a\u57df\uff0c\u4ee5\u7528\u6237\u5b9a\u4e49\u7684\u7f6e\u4fe1\u6c34\u5e73\u8986\u76d6\u771f\u5b9e\u59ff\u6001\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u59ff\u6001\u53ef\u9760\u6027\u8bc4\u4f30\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.23101", "pdf": "https://arxiv.org/pdf/2506.23101", "abs": "https://arxiv.org/abs/2506.23101", "authors": ["Yue Xu", "Wenjie Wang"], "title": "From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nacross tasks involving both visual and textual modalities. However, growing\nconcerns remain about their potential to encode and amplify gender bias,\nparticularly in socially sensitive applications. Existing benchmarks\npredominantly evaluate bias in isolated scenarios, overlooking how bias may\nemerge subtly through interpersonal interactions. We fill this gap by going\nbeyond single-entity evaluation and instead focusing on a deeper examination of\nrelational and contextual gender bias in dual-individual interactions. We\nintroduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs\nthrough the lens of social relationships in generated narratives. Genres\nassesses gender bias through a dual-character profile and narrative generation\ntask that captures rich interpersonal dynamics and supports a fine-grained bias\nevaluation suite across multiple dimensions. Experiments on both open- and\nclosed-source MLLMs reveal persistent, context-sensitive gender biases that are\nnot evident in single-character settings. Our findings underscore the\nimportance of relationship-aware benchmarks for diagnosing subtle,\ninteraction-driven gender bias in MLLMs and provide actionable insights for\nfuture bias mitigation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGenres\uff0c\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7\u53cc\u89d2\u8272\u53d9\u4e8b\u751f\u6210\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u4eba\u9645\u4e92\u52a8\u4e2d\u7684\u5173\u7cfb\u548c\u8bed\u5883\u6027\u522b\u504f\u89c1\uff0c\u63ed\u793a\u4e86\u5355\u5b9e\u4f53\u8bc4\u4f30\u672a\u80fd\u53d1\u73b0\u7684\u5fae\u5999\u504f\u89c1\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u53ef\u80fd\u7f16\u7801\u5e76\u653e\u5927\u6027\u522b\u504f\u89c1\uff0c\u5c24\u5176\u5728\u793e\u4f1a\u654f\u611f\u5e94\u7528\u4e2d\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u5b64\u7acb\u573a\u666f\u7684\u504f\u89c1\uff0c\u672a\u80fd\u6355\u83b7\u4eba\u9645\u4e92\u52a8\u4e2d\u51fa\u73b0\u7684\u5fae\u5999\u504f\u89c1\u3002", "method": "\u5f15\u5165\u65b0\u57fa\u51c6\u201cGenres\u201d\uff0c\u8be5\u57fa\u51c6\u901a\u8fc7\u53cc\u89d2\u8272\u914d\u7f6e\u6587\u4ef6\u548c\u53d9\u4e8b\u751f\u6210\u4efb\u52a1\u6765\u8bc4\u4f30MLLMs\u7684\u6027\u522b\u504f\u89c1\uff0c\u65e8\u5728\u6355\u83b7\u4e30\u5bcc\u7684\u4eba\u9645\u52a8\u6001\u5e76\u652f\u6301\u591a\u7ef4\u5ea6\u7ec6\u7c92\u5ea6\u504f\u89c1\u8bc4\u4f30\u3002\u5b9e\u9a8c\u5728\u5f00\u6e90\u548c\u95ed\u6e90MLLMs\u4e0a\u8fdb\u884c\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86MLLMs\u4e2d\u6301\u7eed\u5b58\u5728\u4e14\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u6027\u522b\u504f\u89c1\uff0c\u8fd9\u4e9b\u504f\u89c1\u5728\u5355\u89d2\u8272\u8bbe\u7f6e\u4e2d\u5e76\u4e0d\u660e\u663e\u3002", "conclusion": "\u5173\u7cfb\u611f\u77e5\u578b\u57fa\u51c6\u5bf9\u4e8e\u8bca\u65adMLLMs\u4e2d\u5fae\u5999\u3001\u4e92\u52a8\u9a71\u52a8\u7684\u6027\u522b\u504f\u89c1\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u504f\u89c1\u7f13\u89e3\u5de5\u4f5c\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\u3002"}}
{"id": "2506.22837", "pdf": "https://arxiv.org/pdf/2506.22837", "abs": "https://arxiv.org/abs/2506.22837", "authors": ["Kamil Faber", "Marcin Pietro\u0144", "Dominik \u017burek", "Roberto Corizzo"], "title": "xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The recently proposed xLSTM is a powerful model that leverages expressive\nmultiplicative gating and residual connections, providing the temporal capacity\nneeded for long-horizon forecasting and representation learning. This\narchitecture has demonstrated success in time series forecasting, lossless\ncompression, and even large-scale language modeling tasks, where its linear\nmemory footprint and fast inference make it a viable alternative to\nTransformers. Despite its growing popularity, no prior work has explored xLSTM\nfor anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the\nfirst anomaly detection method that integrates a full encoder-decoder xLSTM\narchitecture, purpose-built for multivariate time series data. Our encoder\nprocesses input sequences to capture historical context, while the decoder is\ndevised in two separate variants of the method. In the forecasting approach,\nthe decoder iteratively generates forecasted future values xLSTMAD-F, while the\nreconstruction approach reconstructs the input time series from its encoded\ncounterpart xLSTMAD-R. We investigate the performance of two loss functions:\nMean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider\nlocal reconstruction fidelity and global sequence alignment, respectively. We\nevaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17\nreal-world datasets, using state-of-the-art challenging metrics such as VUS-PR.\nIn our results, xLSTM showcases state-of-the-art accuracy, outperforming 23\npopular anomaly detection baselines. Our paper is the first work revealing the\npowerful modeling capabilities of xLSTM for anomaly detection, paving the way\nfor exciting new developments on this subject. Our code is available at:\nhttps://github.com/Nyderx/xlstmad", "AI": {"tldr": "\u63d0\u51faxLSTMAD\uff0c\u9996\u4e2a\u57fa\u4e8exLSTM\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u5c3d\u7ba1xLSTM\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3001\u65e0\u635f\u538b\u7f29\u548c\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u5f02\u5e38\u68c0\u6d4b\u9886\u57df\u7684\u5e94\u7528\u4ecd\u662f\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86xLSTMAD\uff0c\u4e00\u4e2a\u96c6\u6210\u4e86\u5b8c\u6574\u7f16\u7801\u5668-\u89e3\u7801\u5668xLSTM\u67b6\u6784\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e13\u4e3a\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u8bbe\u8ba1\u3002\u5b83\u5305\u542b\u4e24\u79cd\u53d8\u4f53\uff1a\u57fa\u4e8e\u9884\u6d4b\u7684xLSTMAD-F\u548c\u57fa\u4e8e\u91cd\u6784\u7684xLSTMAD-R\u3002\u7814\u7a76\u8fd8\u6bd4\u8f83\u4e86MSE\u548cSoftDTW\u4e24\u79cd\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u5305\u542b17\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684TSB-AD-M\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cxLSTMAD\u5c55\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u8d85\u8d8a\u4e8623\u4e2a\u6d41\u884c\u7684\u5f02\u5e38\u68c0\u6d4b\u57fa\u7ebf\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u63ed\u793a\u4e86xLSTM\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5f3a\u5927\u5efa\u6a21\u80fd\u529b\uff0c\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.23673", "pdf": "https://arxiv.org/pdf/2506.23673", "abs": "https://arxiv.org/abs/2506.23673", "authors": ["Jingsong Liu", "Han Li", "Chen Yang", "Michael Deutges", "Ario Sadafi", "Xin You", "Katharina Breininger", "Nassir Navab", "Peter J. Sch\u00fcffler"], "title": "HASD: Hierarchical Adaption for pathology Slide-level Domain-shift", "categories": ["cs.AI"], "comment": null, "summary": "Domain shift is a critical problem for pathology AI as pathology data is\nheavily influenced by center-specific conditions. Current pathology domain\nadaptation methods focus on image patches rather than WSI, thus failing to\ncapture global WSI features required in typical clinical scenarios. In this\nwork, we address the challenges of slide-level domain shift by proposing a\nHierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD\nachieves multi-scale feature consistency and computationally efficient\nslide-level domain adaptation through two key components: (1) a hierarchical\nadaptation framework that integrates a Domain-level Alignment Solver for\nfeature alignment, a Slide-level Geometric Invariance Regularization to\npreserve the morphological structure, and a Patch-level Attention Consistency\nRegularization to maintain local critical diagnostic cues; and (2) a prototype\nselection mechanism that reduces computational overhead. We validate our method\non two slide-level tasks across five datasets, achieving a 4.1\\% AUROC\nimprovement in a Breast Cancer HER2 Grading cohort and a 3.9\\% C-index gain in\na UCEC survival prediction cohort. Our method provides a practical and reliable\nslide-level domain adaption solution for pathology institutions, minimizing\nboth computational and annotation costs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHASD\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u9002\u5e94\u548c\u539f\u578b\u9009\u62e9\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u75c5\u7406AI\u4e2d\u5168\u73bb\u7247\u56fe\u50cf\uff08WSI\uff09\u5c42\u9762\u7684\u57df\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bca\u65ad\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u75c5\u7406AI\u4e2d\uff0c\u7531\u4e8e\u4e2d\u5fc3\u7279\u5f02\u6027\u6761\u4ef6\uff0c\u57df\u6f02\u79fb\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u57df\u9002\u5e94\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u56fe\u50cf\u6591\u5757\u800c\u975e\u5168\u73bb\u7247\u56fe\u50cf\uff08WSI\uff09\uff0c\u65e0\u6cd5\u6355\u6349\u4e34\u5e8a\u6240\u9700\u7684\u5168\u5c40WSI\u7279\u5f81\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u5168\u73bb\u7247\u5c42\u9762\u57df\u6f02\u79fb\u5206\u5c42\u9002\u5e94\u6846\u67b6\u201d\uff08HASD\uff09\u3002HASD\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u73bb\u7247\u7ea7\u57df\u9002\u5e94\uff1a1) \u4e00\u4e2a\u5206\u5c42\u9002\u5e94\u6846\u67b6\uff0c\u5305\u542b\u7528\u4e8e\u7279\u5f81\u5bf9\u9f50\u7684\u57df\u7ea7\u522b\u5bf9\u9f50\u6c42\u89e3\u5668\u3001\u7528\u4e8e\u4fdd\u6301\u5f62\u6001\u7ed3\u6784\u7684\u73bb\u7247\u7ea7\u522b\u51e0\u4f55\u4e0d\u53d8\u6027\u6b63\u5219\u5316\u3001\u4ee5\u53ca\u7528\u4e8e\u7ef4\u6301\u5c40\u90e8\u5173\u952e\u8bca\u65ad\u7ebf\u7d22\u7684\u6591\u5757\u7ea7\u522b\u6ce8\u610f\u529b\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff1b2) \u4e00\u4e2a\u539f\u578b\u9009\u62e9\u673a\u5236\uff0c\u7528\u4e8e\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u73bb\u7247\u7ea7\u522b\u4efb\u52a1\u3001\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5728\u4e73\u817a\u764cHER2\u5206\u7ea7\u961f\u5217\u4e2d\u5b9e\u73b0\u4e864.1%\u7684AUROC\u6539\u5584\uff0c\u5728\u5b50\u5bab\u5185\u819c\u764c\u751f\u5b58\u9884\u6d4b\u961f\u5217\u4e2d\u5b9e\u73b0\u4e863.9%\u7684C-index\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u75c5\u7406\u673a\u6784\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u9760\u7684\u73bb\u7247\u7ea7\u522b\u57df\u9002\u5e94\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u8ba1\u7b97\u548c\u6807\u6ce8\u6210\u672c\u3002"}}
{"id": "2506.22726", "pdf": "https://arxiv.org/pdf/2506.22726", "abs": "https://arxiv.org/abs/2506.22726", "authors": ["Yu Zhang", "Xi Zhang", "Hualin zhou", "Xinyuan Chen", "Shang Gao", "Hong Jia", "Jianfei Yang", "Yuankai Qi", "Tao Gu"], "title": "XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Deep learning for human sensing on edge systems offers significant\nopportunities for smart applications. However, its training and development are\nhindered by the limited availability of sensor data and resource constraints of\nedge systems. Current methods that rely on transferring pre-trained models\noften encounter issues such as modality shift and high resource demands,\nresulting in substantial accuracy loss, resource overhead, and poor\nadaptability across different sensing applications. In this paper, we propose\nXTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic\nmodel transfer. XTransfer freely leverages single or multiple pre-trained\nmodels and transfers knowledge across different modalities by (i) model\nrepairing that safely repairs modality shift in pre-trained model layers with\nonly few sensor data, and (ii) layer recombining that efficiently searches and\nrecombines layers of interest from source models in a layer-wise manner to\ncreate compact models. We benchmark various baselines across diverse human\nsensing datasets spanning different modalities. Comprehensive results\ndemonstrate that XTransfer achieves state-of-the-art performance on human\nsensing tasks while significantly reducing the costs of sensor data collection,\nmodel training, and edge deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faXTransfer\uff0c\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u3001\u6a21\u6001\u65e0\u5173\u7684\u6a21\u578b\u8fc1\u79fb\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fb9\u7f18\u7cfb\u7edf\u4e0a\u7684\u4eba\u4f53\u611f\u77e5\uff0c\u89e3\u51b3\u4e86\u4f20\u611f\u5668\u6570\u636e\u7a00\u7f3a\u548c\u8d44\u6e90\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u8fb9\u7f18\u7cfb\u7edf\u4eba\u4f53\u611f\u77e5\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u53d7\u9650\u4e8e\u4f20\u611f\u5668\u6570\u636e\u4e0d\u8db3\u548c\u8fb9\u7f18\u7cfb\u7edf\u8d44\u6e90\u6709\u9650\u3002\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u8fc1\u79fb\u65b9\u6cd5\u5e38\u9762\u4e34\u6a21\u6001\u6f02\u79fb\u3001\u9ad8\u8d44\u6e90\u9700\u6c42\u3001\u7cbe\u5ea6\u635f\u5931\u53ca\u9002\u5e94\u6027\u5dee\u7b49\u95ee\u9898\u3002", "method": "XTransfer\u5229\u7528\u5355\u4e2a\u6216\u591a\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u901a\u8fc7\u4ee5\u4e0b\u4e24\u90e8\u5206\u5b9e\u73b0\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\uff1a(i) \u6a21\u578b\u4fee\u590d\uff1a\u4ec5\u7528\u5c11\u91cf\u4f20\u611f\u5668\u6570\u636e\u5b89\u5168\u4fee\u590d\u9884\u8bad\u7ec3\u6a21\u578b\u5c42\u4e2d\u7684\u6a21\u6001\u6f02\u79fb\uff1b(ii) \u5c42\u91cd\u7ec4\uff1a\u4ee5\u9010\u5c42\u65b9\u5f0f\u9ad8\u6548\u641c\u7d22\u5e76\u91cd\u7ec4\u6e90\u6a21\u578b\u4e2d\u611f\u5174\u8da3\u7684\u5c42\uff0c\u4ee5\u521b\u5efa\u7d27\u51d1\u6a21\u578b\u3002", "result": "XTransfer\u5728\u591a\u79cd\u4eba\u4f53\u611f\u77e5\u6570\u636e\u96c6\u548c\u6a21\u6001\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u611f\u5668\u6570\u636e\u91c7\u96c6\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u8fb9\u7f18\u90e8\u7f72\u7684\u6210\u672c\u3002", "conclusion": "XTransfer\u662f\u4e00\u79cd\u521b\u65b0\u7684\u6a21\u578b\u8fc1\u79fb\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u8fb9\u7f18\u7cfb\u7edf\u4eba\u4f53\u611f\u77e5\u5e94\u7528\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u8d44\u6e90\u9650\u5236\u7684\u6311\u6218\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2506.23111", "pdf": "https://arxiv.org/pdf/2506.23111", "abs": "https://arxiv.org/abs/2506.23111", "authors": ["Janki Atul Nawale", "Mohammed Safi Ur Rahman Khan", "Janani D", "Mansi Gupta", "Danish Pruthi", "Mitesh M. Khapra"], "title": "FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes", "categories": ["cs.CL"], "comment": "Accepted in ACL 2025", "summary": "Existing studies on fairness are largely Western-focused, making them\ninadequate for culturally diverse countries such as India. To address this gap,\nwe introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to\nevaluate fairness of LLMs across 85 identity groups encompassing diverse\ncastes, religions, regions, and tribes. We first consult domain experts to\ncurate over 1,800 socio-cultural topics spanning behaviors and situations,\nwhere biases and stereotypes are likely to emerge. Grounded in these topics, we\ngenerate and manually validate 20,000 real-world scenario templates to probe\nLLMs for fairness. We structure these templates into three evaluation tasks:\nplausibility, judgment, and generation. Our evaluation of 14 popular LLMs on\nthese tasks reveals strong negative biases against marginalized identities,\nwith models frequently reinforcing common stereotypes. Additionally, we find\nthat models struggle to mitigate bias even when explicitly asked to rationalize\ntheir decision. Our evaluation provides evidence of both allocative and\nrepresentational harms that current LLMs could cause towards Indian identities,\ncalling for a more cautious usage in practical applications. We release\nINDIC-BIAS as an open-source benchmark to advance research on benchmarking and\nmitigating biases and stereotypes in the Indian context.", "AI": {"tldr": "\u73b0\u6709\u516c\u5e73\u6027\u7814\u7a76\u591a\u4ee5\u897f\u65b9\u4e3a\u4e2d\u5fc3\uff0c\u4e0d\u9002\u7528\u4e8e\u5370\u5ea6\u7b49\u591a\u5143\u6587\u5316\u56fd\u5bb6\u3002\u672c\u6587\u63d0\u51faINDIC-BIAS\uff0c\u4e00\u4e2a\u5370\u5ea6\u4e2d\u5fc3\u5316\u7684LLM\u516c\u5e73\u6027\u8bc4\u4f30\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u4e3b\u6d41LLM\u5bf9\u5370\u5ea6\u8fb9\u7f18\u5316\u7fa4\u4f53\u5b58\u5728\u4e25\u91cd\u8d1f\u9762\u504f\u89c1\uff0c\u5e76\u547c\u5401\u8c28\u614e\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u5173\u4e8e\u516c\u5e73\u6027\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u897f\u65b9\u89c6\u89d2\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u5728\u5370\u5ea6\u7b49\u6587\u5316\u591a\u6837\u6027\u56fd\u5bb6\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u5370\u5ea6\u591a\u5143\u8eab\u4efd\u7fa4\u4f53\u65f6\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86INDIC-BIAS\uff0c\u4e00\u4e2a\u9488\u5bf9\u5370\u5ea685\u4e2a\u8eab\u4efd\u7fa4\u4f53\uff08\u6db5\u76d6\u4e0d\u540c\u79cd\u59d3\u3001\u5b97\u6559\u3001\u5730\u533a\u548c\u90e8\u843d\uff09\u7684LLM\u516c\u5e73\u6027\u8bc4\u4f30\u57fa\u51c6\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\uff1a1. \u54a8\u8be2\u9886\u57df\u4e13\u5bb6\uff0c\u6536\u96c61800\u591a\u4e2a\u793e\u4f1a\u6587\u5316\u8bdd\u9898\uff1b2. \u57fa\u4e8e\u8fd9\u4e9b\u8bdd\u9898\uff0c\u751f\u6210\u5e76\u624b\u52a8\u9a8c\u8bc120000\u4e2a\u771f\u5b9e\u4e16\u754c\u573a\u666f\u6a21\u677f\uff1b3. \u5c06\u6a21\u677f\u7ec4\u7ec7\u6210\u4e09\u79cd\u8bc4\u4f30\u4efb\u52a1\uff1a\u5408\u7406\u6027\u3001\u5224\u65ad\u548c\u751f\u6210\uff1b4. \u4f7f\u7528\u8be5\u57fa\u51c6\u8bc4\u4f30\u4e8614\u4e2a\u4e3b\u6d41LLM\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\uff0c\u53c2\u8bc4\u768414\u4e2aLLM\u5bf9\u5370\u5ea6\u8fb9\u7f18\u5316\u8eab\u4efd\u7fa4\u4f53\u5b58\u5728\u5f3a\u70c8\u7684\u8d1f\u9762\u504f\u89c1\uff0c\u6a21\u578b\u7ecf\u5e38\u5f3a\u5316\u5e38\u89c1\u523b\u677f\u5370\u8c61\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u5728\u660e\u786e\u8981\u6c42LLM\u89e3\u91ca\u5176\u51b3\u7b56\u65f6\uff0c\u6a21\u578b\u4e5f\u96be\u4ee5\u51cf\u8f7b\u504f\u89c1\u3002", "conclusion": "\u5f53\u524dLLM\u53ef\u80fd\u5bf9\u5370\u5ea6\u8eab\u4efd\u7fa4\u4f53\u9020\u6210\u5206\u914d\u6027\u548c\u4ee3\u8868\u6027\u5371\u5bb3\uff0c\u56e0\u6b64\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u66f4\u52a0\u8c28\u614e\u3002\u7814\u7a76\u53d1\u5e03\u4e86INDIC-BIAS\u4f5c\u4e3a\u4e00\u4e2a\u5f00\u6e90\u57fa\u51c6\uff0c\u4ee5\u63a8\u52a8\u5728\u5370\u5ea6\u80cc\u666f\u4e0bLLM\u504f\u89c1\u57fa\u51c6\u6d4b\u8bd5\u548c\u7f13\u89e3\u504f\u89c1\u7684\u7814\u7a76\u3002"}}
{"id": "2506.22845", "pdf": "https://arxiv.org/pdf/2506.22845", "abs": "https://arxiv.org/abs/2506.22845", "authors": ["Batuhan Hangun", "Oguz Altun", "Onder Eyecioglu"], "title": "Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": null, "summary": "Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine\nLearning (QML), are emerging as a powerful alternative to classical machine\nlearning methods. Recent studies have focused on the applicability of QNNs to\nvarious tasks, such as time-series forecasting, prediction, and classification,\nacross a wide range of applications, including cybersecurity and medical\nimaging. With the increased use of smart grids driven by the integration of\nrenewable energy systems, machine learning plays an important role in\npredicting power demand and detecting system disturbances. This study provides\nan in-depth investigation of QNNs for predicting the power output of a wind\nturbine. We assess the predictive performance and simulation time of six QNN\nconfigurations that are based on the Z Feature Map for data encoding and\nvarying ansatz structures. Through detailed cross-validation experiments and\ntests on an unseen hold-out dataset, we experimentally demonstrate that QNNs\ncan achieve predictive performance that is competitive with, and in some cases\nmarginally better than, the benchmarked classical approaches. Our results also\nreveal the effects of dataset size and circuit complexity on predictive\nperformance and simulation time. We believe our findings will offer valuable\ninsights for researchers in the energy domain who wish to incorporate quantum\nmachine learning into their work.", "AI": {"tldr": "\u672c\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u4e86\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNNs\uff09\u5728\u9884\u6d4b\u98ce\u529b\u6da1\u8f6e\u673a\u529f\u7387\u8f93\u51fa\u65b9\u9762\u7684\u5e94\u7528\uff0c\u5b9e\u9a8c\u8bc1\u660eQNNs\u7684\u9884\u6d4b\u6027\u80fd\u53ef\u4e0e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5ab2\u7f8e\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u7565\u4f18\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u7cfb\u7edf\u7684\u6574\u5408\u63a8\u52a8\u667a\u80fd\u7535\u7f51\u7684\u666e\u53ca\uff0c\u673a\u5668\u5b66\u4e60\u5728\u9884\u6d4b\u7535\u529b\u9700\u6c42\u548c\u68c0\u6d4b\u7cfb\u7edf\u6270\u52a8\u4e2d\u626e\u6f14\u7740\u91cd\u8981\u89d2\u8272\u3002\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNNs\uff09\u4f5c\u4e3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u7684\u7a81\u51fa\u65b9\u6cd5\uff0c\u6b63\u9010\u6e10\u6210\u4e3a\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u6709\u529b\u66ff\u4ee3\u54c1\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22QNNs\u5728\u98ce\u529b\u6da1\u8f6e\u673a\u529f\u7387\u8f93\u51fa\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u516d\u79cd\u57fa\u4e8eZ\u7279\u5f81\u6620\u5c04\u8fdb\u884c\u6570\u636e\u7f16\u7801\u548c\u4e0d\u540cansatz\u7ed3\u6784\u7684QNN\u914d\u7f6e\u7684\u9884\u6d4b\u6027\u80fd\u548c\u6a21\u62df\u65f6\u95f4\u3002\u901a\u8fc7\u8be6\u7ec6\u7684\u4ea4\u53c9\u9a8c\u8bc1\u5b9e\u9a8c\u548c\u5728\u672a\u89c1\u8fc7\u7684\u4fdd\u7559\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\uff0c\u5c06QNNs\u7684\u6027\u80fd\u4e0e\u57fa\u51c6\u7684\u7ecf\u5178\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQNNs\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u57fa\u51c6\u7ecf\u5178\u65b9\u6cd5\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u7565\u4f18\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u6570\u636e\u96c6\u5927\u5c0f\u548c\u7535\u8def\u590d\u6742\u6027\u5bf9\u9884\u6d4b\u6027\u80fd\u548c\u6a21\u62df\u65f6\u95f4\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u7814\u7a76\u7ed3\u679c\u4e3a\u5e0c\u671b\u5c06\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e8e\u80fd\u6e90\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2506.23689", "pdf": "https://arxiv.org/pdf/2506.23689", "abs": "https://arxiv.org/abs/2506.23689", "authors": ["Zihao Liu", "Xinhang Sui", "Yueran Song", "Siwen Wang"], "title": "Pok\u00e9AI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "We introduce Pok\\'eAI, the first text-based, multi-agent large language model\n(LLM) framework designed to autonomously play and progress through Pok\\'emon\nRed. Our system consists of three specialized agents-Planning, Execution, and\nCritique-each with its own memory bank, role, and skill set. The Planning Agent\nfunctions as the central brain, generating tasks to progress through the game.\nThese tasks are then delegated to the Execution Agent, which carries them out\nwithin the game environment. Upon task completion, the Critique Agent evaluates\nthe outcome to determine whether the objective was successfully achieved. Once\nverification is complete, control returns to the Planning Agent, forming a\nclosed-loop decision-making system.\n  As a preliminary step, we developed a battle module within the Execution\nAgent. Our results show that the battle AI achieves an average win rate of\n80.8% across 50 wild encounters, only 6% lower than the performance of an\nexperienced human player. Furthermore, we find that a model's battle\nperformance correlates strongly with its LLM Arena score on language-related\ntasks, indicating a meaningful link between linguistic ability and strategic\nreasoning. Finally, our analysis of gameplay logs reveals that each LLM\nexhibits a unique playstyle, suggesting that individual models develop distinct\nstrategic behaviors.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.22736", "pdf": "https://arxiv.org/pdf/2506.22736", "abs": "https://arxiv.org/abs/2506.22736", "authors": ["Dayong Su", "Yafei Zhang", "Huafeng Li", "Jinxing Li", "Yu Liu"], "title": "UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "Current multimodal medical image fusion typically assumes that source images\nare of high quality and perfectly aligned at the pixel level. Its effectiveness\nheavily relies on these conditions and often deteriorates when handling\nmisaligned or degraded medical images. To address this, we propose UniFuse, a\ngeneral fusion framework. By embedding a degradation-aware prompt learning\nmodule, UniFuse seamlessly integrates multi-directional information from input\nimages and correlates cross-modal alignment with restoration, enabling joint\noptimization of both tasks within a unified framework. Additionally, we design\nan Omni Unified Feature Representation scheme, which leverages Spatial Mamba to\nencode multi-directional features and mitigate modality differences in feature\nalignment. To enable simultaneous restoration and fusion within an All-in-One\nconfiguration, we propose a Universal Feature Restoration & Fusion module,\nincorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA\nprinciples. By leveraging ALSN's adaptive feature representation along with\ndegradation-type guidance, we enable joint restoration and fusion within a\nsingle-stage framework. Compared to staged approaches, UniFuse unifies\nalignment, restoration, and fusion within a single framework. Experimental\nresults across multiple datasets demonstrate the method's effectiveness and\nsignificant advantages over existing approaches.", "AI": {"tldr": "\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\u878d\u5408\u4e2d\u56fe\u50cf\u8d28\u91cf\u5dee\u548c\u672a\u5bf9\u9f50\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faUniFuse\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u964d\u7ea7\u611f\u77e5\u63d0\u793a\u5b66\u4e60\u3001\u5168\u65b9\u4f4d\u7edf\u4e00\u7279\u5f81\u8868\u793a\u548c\u81ea\u9002\u5e94LoRA\u534f\u540c\u7f51\u7edc\uff0c\u5b9e\u73b0\u5bf9\u9f50\u3001\u4fee\u590d\u548c\u878d\u5408\u7684\u7edf\u4e00\u5355\u9636\u6bb5\u5904\u7406\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6e90\u56fe\u50cf\u9ad8\u8d28\u91cf\u4e14\u50cf\u7d20\u7ea7\u5b8c\u7f8e\u5bf9\u9f50\uff0c\u4f46\u5728\u5904\u7406\u672a\u5bf9\u9f50\u6216\u964d\u7ea7\u7684\u533b\u5b66\u56fe\u50cf\u65f6\uff0c\u5176\u6709\u6548\u6027\u4f1a\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51faUniFuse\u901a\u7528\u878d\u5408\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u964d\u7ea7\u611f\u77e5\u63d0\u793a\u5b66\u4e60\u6a21\u5757\uff0c\u5b9e\u73b0\u591a\u65b9\u5411\u4fe1\u606f\u6574\u5408\u4e0e\u5bf9\u9f50\u4fee\u590d\u7684\u8054\u5408\u4f18\u5316\uff1b2) \u5168\u65b9\u4f4d\u7edf\u4e00\u7279\u5f81\u8868\u793a\u65b9\u6848\uff0c\u5229\u7528Spatial Mamba\u7f16\u7801\u7279\u5f81\u5e76\u7f13\u89e3\u6a21\u6001\u5dee\u5f02\uff1b3) \u901a\u7528\u7279\u5f81\u4fee\u590d\u4e0e\u878d\u5408\u6a21\u5757\uff0c\u5f15\u5165\u57fa\u4e8eLoRA\u7684\u81ea\u9002\u5e94LoRA\u534f\u540c\u7f51\u7edc\uff08ALSN\uff09\uff0c\u5b9e\u73b0\u5bf9\u9f50\u3001\u4fee\u590d\u548c\u878d\u5408\u5728\u5355\u4e00\u6846\u67b6\u5185\u7684\u5355\u9636\u6bb5\u5904\u7406\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u5e76\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "UniFuse\u6846\u67b6\u6210\u529f\u5730\u5c06\u5bf9\u9f50\u3001\u4fee\u590d\u548c\u878d\u5408\u7edf\u4e00\u5230\u4e00\u4e2a\u5355\u4e00\u6846\u67b6\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u672a\u5bf9\u9f50\u548c\u964d\u7ea7\u533b\u5b66\u56fe\u50cf\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23122", "pdf": "https://arxiv.org/pdf/2506.23122", "abs": "https://arxiv.org/abs/2506.23122", "authors": ["Shivam Sharma", "Tanmoy Chakraborty"], "title": "Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models", "categories": ["cs.CL", "cs.CY"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This work investigates the challenging task of identifying narrative roles -\nHero, Villain, Victim, and Other - in Internet memes, across three diverse test\nsets spanning English and code-mixed (English-Hindi) languages. Building on an\nannotated dataset originally skewed toward the 'Other' class, we explore a more\nbalanced and linguistically diverse extension, originally introduced as part of\nthe CLEF 2024 shared task. Comprehensive lexical and structural analyses\nhighlight the nuanced, culture-specific, and context-rich language used in real\nmemes, in contrast to synthetically curated hateful content, which exhibits\nexplicit and repetitive lexical markers. To benchmark the role detection task,\nwe evaluate a wide spectrum of models, including fine-tuned multilingual\ntransformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,\nand multimodal vision-language models. Performance is assessed under zero-shot\nsettings using precision, recall, and F1 metrics. While larger models like\nDeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent\nchallenges in reliably identifying the 'Victim' class and generalising across\ncultural and code-mixed content. We also explore prompt design strategies to\nguide multimodal models and find that hybrid prompts incorporating structured\ninstructions and role definitions offer marginal yet consistent improvements.\nOur findings underscore the importance of cultural grounding, prompt\nengineering, and multimodal reasoning in modelling subtle narrative framings in\nvisual-textual content.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u5728\u4e92\u8054\u7f51\u8868\u60c5\u5305\u4e2d\u8bc6\u522b\u53d9\u4e8b\u89d2\u8272\uff08\u82f1\u96c4\u3001\u53cd\u6d3e\u3001\u53d7\u5bb3\u8005\u3001\u5176\u4ed6\uff09\u7684\u6311\u6218\uff0c\u53d1\u73b0\u771f\u5b9e\u8868\u60c5\u5305\u8bed\u8a00\u7684\u590d\u6742\u6027\u53ca\u6a21\u578b\u5728\u8bc6\u522b\u201c\u53d7\u5bb3\u8005\u201d\u548c\u8de8\u6587\u5316\u6cdb\u5316\u65b9\u9762\u7684\u56f0\u96be\uff0c\u5e76\u5f3a\u8c03\u6587\u5316\u80cc\u666f\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u591a\u6a21\u6001\u63a8\u7406\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u8bc6\u522b\u4e92\u8054\u7f51\u8868\u60c5\u5305\u4e2d\u7684\u53d9\u4e8b\u89d2\u8272\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u8003\u8651\u5230\u8868\u60c5\u5305\u8bed\u8a00\u7684\u7ec6\u5fae\u3001\u6587\u5316\u7279\u5f02\u6027\u548c\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6027\uff0c\u8fd9\u4e0e\u5408\u6210\u751f\u6210\u7684\u5185\u5bb9\u4e0d\u540c\u3002\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u8bed\u8a00\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5e73\u8861\u548c\u591a\u8bed\u8a00\u7684\u6570\u636e\u96c6\u6765\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002", "method": "\u672c\u7814\u7a76\u57fa\u4e8eCLEF 2024\u5171\u4eab\u4efb\u52a1\u5f15\u5165\u7684\u3001\u66f4\u5e73\u8861\u548c\u8bed\u8a00\u591a\u6837\u5316\u7684\u8868\u60c5\u5305\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8bcd\u6c47\u548c\u7ed3\u6784\u5206\u6790\u3002\u8bc4\u4f30\u4e86\u5305\u62ec\u5fae\u8c03\u591a\u8bed\u8a00Transformer\u3001\u60c5\u611f\u548c\u6ee5\u7528\u68c0\u6d4b\u5206\u7c7b\u5668\u3001\u6307\u4ee4\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u53ca\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5185\u7684\u591a\u79cd\u6a21\u578b\u3002\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u4f7f\u7528\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u7d22\u4e86\u5f15\u5bfc\u591a\u6a21\u6001\u6a21\u578b\u7684\u63d0\u793a\u8bbe\u8ba1\u7b56\u7565\uff0c\u7279\u522b\u662f\u7ed3\u5408\u7ed3\u6784\u5316\u6307\u4ee4\u548c\u89d2\u8272\u5b9a\u4e49\u7684\u6df7\u5408\u63d0\u793a\u3002", "result": "\u8bcd\u6c47\u548c\u7ed3\u6784\u5206\u6790\u63ed\u793a\u4e86\u771f\u5b9e\u8868\u60c5\u5305\u8bed\u8a00\u7684\u7ec6\u5fae\u3001\u6587\u5316\u7279\u5f02\u548c\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u7279\u70b9\u3002\u5927\u578b\u6a21\u578b\u5982DeBERTa-v3\u548cQwen2.5-VL\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\uff0c\u4f46\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u53ef\u9760\u8bc6\u522b\u201c\u53d7\u5bb3\u8005\u201d\u7c7b\u522b\u4ee5\u53ca\u5728\u4e0d\u540c\u6587\u5316\u548c\u6df7\u5408\u8bed\u8a00\u5185\u5bb9\u4e0a\u8fdb\u884c\u6cdb\u5316\u65b9\u9762\uff0c\u4ecd\u5b58\u5728\u6301\u7eed\u7684\u6311\u6218\u3002\u6df7\u5408\u63d0\u793a\u8bbe\u8ba1\u5bf9\u591a\u6a21\u6001\u6a21\u578b\u5e26\u6765\u4e86\u5fae\u5c0f\u4f46\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u6587\u5316\u80cc\u666f\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u591a\u6a21\u6001\u63a8\u7406\u5728\u5efa\u6a21\u89c6\u89c9-\u6587\u672c\u5185\u5bb9\u4e2d\u5fae\u5999\u53d9\u4e8b\u6846\u67b6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.22848", "pdf": "https://arxiv.org/pdf/2506.22848", "abs": "https://arxiv.org/abs/2506.22848", "authors": ["Shengcai Liu", "Hui Ou-yang", "Zhiyuan Wang", "Cheng Chen", "Qijun Cai", "Yew-Soon Ong", "Ke Tang"], "title": "Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Learning the structure of Bayesian networks (BNs) from data is challenging,\nespecially for datasets involving a large number of variables. The recently\nproposed divide-and-conquer (D\\&D) strategies present a promising approach for\nlearning large BNs. However, they still face a main issue of unstable learning\naccuracy across subproblems. In this work, we introduce the idea of employing\nstructure learning ensemble (SLE), which combines multiple BN structure\nlearning algorithms, to consistently achieve high learning accuracy. We further\npropose an automatic approach called Auto-SLE for learning near-optimal SLEs,\naddressing the challenge of manually designing high-quality SLEs. The learned\nSLE is then integrated into a D\\&D method. Extensive experiments firmly show\nthe superiority of our method over D\\&D methods with single BN structure\nlearning algorithm in learning large BNs, achieving accuracy improvement\nusually by 30\\%$\\sim$225\\% on datasets involving 10,000 variables. Furthermore,\nour method generalizes well to datasets with many more (e.g., 30000) variables\nand different network characteristics than those present in the training data\nfor learning the SLE. These results indicate the significant potential of\nemploying (automatic learning of) SLEs for scalable BN structure learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86Auto-SLE\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5b66\u4e60\u7ed3\u6784\u5b66\u4e60\u96c6\u6210\uff08SLE\uff09\u5e76\u5c06\u5176\u6574\u5408\u5230\u5206\u6cbb\u7b56\u7565\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u8d1d\u53f6\u65af\u7f51\u7edc\u7ed3\u6784\u5b66\u4e60\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u8d1d\u53f6\u65af\u7f51\u7edc\uff08BNs\uff09\u7684\u7ed3\u6784\uff0c\u7279\u522b\u662f\u53d8\u91cf\u6570\u91cf\u5e9e\u5927\u7684\u6570\u636e\u96c6\uff0c\u6781\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u5206\u6cbb\uff08D&D\uff09\u7b56\u7565\u5728\u5b50\u95ee\u9898\u4e0a\u7684\u5b66\u4e60\u7cbe\u5ea6\u4e0d\u7a33\u5b9a\uff0c\u4e14\u624b\u52a8\u8bbe\u8ba1\u9ad8\u8d28\u91cf\u7684\u7ed3\u6784\u5b66\u4e60\u96c6\u6210\uff08SLE\uff09\u9762\u4e34\u56f0\u96be\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u7ed3\u6784\u5b66\u4e60\u96c6\u6210\uff08SLE\uff09\u7684\u601d\u60f3\uff0c\u7ed3\u5408\u591a\u79cdBN\u7ed3\u6784\u5b66\u4e60\u7b97\u6cd5\u4ee5\u6301\u7eed\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3002\u4e3a\u89e3\u51b3\u624b\u52a8\u8bbe\u8ba1SLE\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAuto-SLE\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u63a5\u8fd1\u6700\u4f18\u7684SLE\u3002\u6700\u7ec8\uff0c\u5c06\u5b66\u4e60\u5230\u7684SLE\u96c6\u6210\u5230\u5206\u6cbb\uff08D&D\uff09\u65b9\u6cd5\u4e2d\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b66\u4e60\u5927\u89c4\u6a21BNs\u65f6\uff0c\u4f18\u4e8e\u4f7f\u7528\u5355\u4e00BN\u7ed3\u6784\u5b66\u4e60\u7b97\u6cd5\u7684\u5206\u6cbb\u65b9\u6cd5\uff0c\u5728\u5305\u542b10,000\u4e2a\u53d8\u91cf\u7684\u6570\u636e\u96c6\u4e0a\u7cbe\u5ea6\u901a\u5e38\u63d0\u9ad830%~225%\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5bf9\u5305\u542b\u66f4\u591a\uff08\u4f8b\u598230,000\u4e2a\uff09\u53d8\u91cf\u4e14\u7f51\u7edc\u7279\u5f81\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u91c7\u7528\uff08\u81ea\u52a8\u5b66\u4e60\u7684\uff09\u7ed3\u6784\u5b66\u4e60\u96c6\u6210\uff08SLEs\uff09\u5728\u5927\u89c4\u6a21\u8d1d\u53f6\u65af\u7f51\u7edc\u7ed3\u6784\u5b66\u4e60\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2506.23692", "pdf": "https://arxiv.org/pdf/2506.23692", "abs": "https://arxiv.org/abs/2506.23692", "authors": ["Boyuan Zheng", "Zerui Fang", "Zhe Xu", "Rui Wang", "Yiwen Chen", "Cunshi Wang", "Mengwei Qu", "Lei Lei", "Zhen Feng", "Yan Liu", "Yuyang Li", "Mingzhou Tan", "Jiaji Wu", "Jianwei Shuai", "Jia Li", "Fangfu Ye"], "title": "Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "While AI for Science (AI4S) serves as an analytical tool in the current\nresearch paradigm, it doesn't solve its core inefficiency. We propose \"Agent\nfor Science\" (Agent4S)-the use of LLM-driven agents to automate the entire\nresearch workflow-as the true Fifth Scientific Paradigm. This paper introduces\na five-level classification for Agent4S, outlining a clear roadmap from simple\ntask automation to fully autonomous, collaborative \"AI Scientists.\" This\nframework defines the next revolutionary step in scientific discovery.", "AI": {"tldr": "\u63d0\u51fa\u4ee5LLM\u9a71\u52a8\u7684\u201c\u79d1\u5b66\u667a\u80fd\u4f53\u201d\uff08Agent4S\uff09\u4f5c\u4e3a\u7b2c\u4e94\u79d1\u5b66\u8303\u5f0f\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u6574\u4e2a\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e94\u7ea7\u5206\u7c7b\u8def\u7ebf\u56fe\u3002", "motivation": "\u5f53\u524d\u7684\u201c\u79d1\u5b66AI\u201d\uff08AI4S\uff09\u867d\u662f\u5206\u6790\u5de5\u5177\uff0c\u4f46\u672a\u80fd\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u8303\u5f0f\u7684\u6838\u5fc3\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u201c\u79d1\u5b66\u667a\u80fd\u4f53\u201d\uff08Agent4S\uff09\u6982\u5ff5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u81ea\u52a8\u5316\u6574\u4e2a\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002\u6587\u7ae0\u4e3a\u6b64Agent4S\u5b9a\u4e49\u4e86\u4e94\u7ea7\u5206\u7c7b\uff0c\u63cf\u7ed8\u4e86\u4e00\u6761\u4ece\u7b80\u5355\u4efb\u52a1\u81ea\u52a8\u5316\u5230\u5b8c\u5168\u81ea\u4e3b\u3001\u534f\u4f5c\u7684\u201cAI\u79d1\u5b66\u5bb6\u201d\u7684\u6e05\u6670\u8def\u7ebf\u56fe\u3002", "result": "\u672c\u6587\u5b9a\u4e49\u4e86Agent4S\u4f5c\u4e3a\u65b0\u7684\u79d1\u5b66\u8303\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4ece\u7b80\u5355\u4efb\u52a1\u81ea\u52a8\u5316\u5230\u5b8c\u5168\u81ea\u4e3b\u201cAI\u79d1\u5b66\u5bb6\u201d\u7684\u4e94\u7ea7\u5206\u7c7b\u4e0e\u53d1\u5c55\u8def\u5f84\u3002", "conclusion": "Agent4S\u6709\u671b\u5f7b\u5e95\u6539\u53d8\u79d1\u5b66\u53d1\u73b0\uff0c\u89e3\u51b3\u4f20\u7edfAI4S\u7684\u5c40\u9650\u6027\uff0c\u6807\u5fd7\u7740\u79d1\u5b66\u7814\u7a76\u7684\u4e0b\u4e00\u4e2a\u9769\u547d\u6027\u9636\u6bb5\u3002"}}
{"id": "2506.22749", "pdf": "https://arxiv.org/pdf/2506.22749", "abs": "https://arxiv.org/abs/2506.22749", "authors": ["Yun Zhang", "Feifan Chen", "Na Li", "Zhiwei Guo", "Xu Wang", "Fen Miao", "Sam Kwong"], "title": "Deep Learning based Joint Geometry and Attribute Up-sampling for Large-Scale Colored Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Colored point cloud, which includes geometry and attribute components, is a\nmainstream representation enabling realistic and immersive 3D applications. To\ngenerate large-scale and denser colored point clouds, we propose a deep\nlearning-based Joint Geometry and Attribute Up-sampling (JGAU) method that\nlearns to model both geometry and attribute patterns while leveraging spatial\nattribute correlations. First, we establish and release a large-scale dataset\nfor colored point cloud up-sampling called SYSU-PCUD, containing 121\nlarge-scale colored point clouds with diverse geometry and attribute\ncomplexities across six categories and four sampling rates. Second, to improve\nthe quality of up-sampled point clouds, we propose a deep learning-based JGAU\nframework that jointly up-samples geometry and attributes. It consists of a\ngeometry up-sampling network and an attribute up-sampling network, where the\nlatter leverages the up-sampled auxiliary geometry to model neighborhood\ncorrelations of the attributes. Third, we propose two coarse attribute\nup-sampling methods, Geometric Distance Weighted Attribute Interpolation\n(GDWAI) and Deep Learning-based Attribute Interpolation (DLAI), to generate\ncoarse up-sampled attributes for each point. Then, an attribute enhancement\nmodule is introduced to refine these up-sampled attributes and produce\nhigh-quality point clouds by further exploiting intrinsic attribute and\ngeometry patterns. Extensive experiments show that the Peak Signal-to-Noise\nRatio (PSNR) achieved by the proposed JGAU method is 33.90 decibels, 32.10\ndecibels, 31.10 decibels, and 30.39 decibels for up-sampling rates of 4 times,\n8 times, 12 times, and 16 times, respectively. Compared to state-of-the-art\nmethods, JGAU achieves average PSNR gains of 2.32 decibels, 2.47 decibels, 2.28\ndecibels, and 2.11 decibels at these four up-sampling rates, demonstrating\nsignificant improvement.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8054\u5408\u51e0\u4f55\u548c\u5c5e\u6027\u4e0a\u91c7\u6837\u65b9\u6cd5\uff08JGAU\uff09\uff0c\u7528\u4e8e\u751f\u6210\u5927\u89c4\u6a21\u3001\u66f4\u5bc6\u96c6\u7684\u5f69\u8272\u70b9\u4e91\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f69\u8272\u70b9\u4e91\u662f\u5b9e\u73b0\u903c\u771f\u6c89\u6d78\u5f0f3D\u5e94\u7528\u7684\u4e3b\u6d41\u8868\u793a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u5927\u89c4\u6a21\u4e14\u5bc6\u5ea6\u66f4\u9ad8\u7684\u5f69\u8272\u70b9\u4e91\u3002", "method": "1. \u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u5927\u89c4\u6a21\u5f69\u8272\u70b9\u4e91\u4e0a\u91c7\u6837\u6570\u636e\u96c6SYSU-PCUD\u30022. \u63d0\u51fa\u6df1\u5ea6\u5b66\u4e60JGAU\u6846\u67b6\uff0c\u8054\u5408\u4e0a\u91c7\u6837\u51e0\u4f55\u548c\u5c5e\u6027\uff0c\u5305\u62ec\u51e0\u4f55\u4e0a\u91c7\u6837\u7f51\u7edc\u548c\u5c5e\u6027\u4e0a\u91c7\u6837\u7f51\u7edc\uff08\u540e\u8005\u5229\u7528\u4e0a\u91c7\u6837\u8f85\u52a9\u51e0\u4f55\u6765\u5efa\u6a21\u5c5e\u6027\u7684\u90bb\u57df\u76f8\u5173\u6027\uff09\u30023. \u63d0\u51fa\u4e24\u79cd\u7c97\u7cd9\u5c5e\u6027\u4e0a\u91c7\u6837\u65b9\u6cd5\uff08GDWAI\u548cDLAI\uff09\uff0c\u5e76\u5f15\u5165\u5c5e\u6027\u589e\u5f3a\u6a21\u5757\u4ee5\u7ec6\u5316\u4e0a\u91c7\u6837\u5c5e\u6027\u3002", "result": "JGAU\u65b9\u6cd5\u57284\u500d\u30018\u500d\u300112\u500d\u548c16\u500d\u4e0a\u91c7\u6837\u7387\u4e0b\u5206\u522b\u8fbe\u5230\u4e8633.90 dB\u300132.10 dB\u300131.10 dB\u548c30.39 dB\u7684PSNR\u3002\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0cJGAU\u5728\u8fd9\u4e9b\u4e0a\u91c7\u6837\u7387\u4e0b\u5e73\u5747PSNR\u589e\u76ca\u5206\u522b\u4e3a2.32 dB\u30012.47 dB\u30012.28 dB\u548c2.11 dB\u3002", "conclusion": "JGAU\u65b9\u6cd5\u5728\u5f69\u8272\u70b9\u4e91\u4e0a\u91c7\u6837\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5f69\u8272\u70b9\u4e91\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.23127", "pdf": "https://arxiv.org/pdf/2506.23127", "abs": "https://arxiv.org/abs/2506.23127", "authors": ["Zhaoye Fei", "Li Ji", "Siyin Wang", "Junhao Shi", "Jingjing Gong", "Xipeng Qiu"], "title": "Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they face significant challenges in embodied task planning\nscenarios that require continuous environmental understanding and action\ngeneration. Existing approaches generate open-loop action scripts based on\nstatic knowledge, making it difficult to learn causal relationships between\nactions and environmental feedback, particularly in partially observable\nenvironments. We introduce Embodied Planner-R1, a novel outcome-driven\nreinforcement learning framework that enables LLMs to develop interactive\ncapabilities through autonomous exploration with minimal supervision. Our\nframework incorporates three key innovations: (1) Without human annotations, we\nemploy pure reinforcement learning with group rollout, incorporating\nin-environment interaction through parallel exploration; (2) completion-driven\nsparse reward; and (3) Interactive Policy Optimization (IPO) for efficient\nlearning from grouped trajectories. Across two challenging text-based Embodied\nplanning benchmarks, Embodied Planner-R1 achieves impressive completion rates\nof 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a\nlarge margin, and suffers only a -3.66% drop in previously unseen environments,\nevidencing strong generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Embodied Planner-R1\uff0c\u4e00\u4e2a\u7ed3\u679c\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u901a\u8fc7\u81ea\u4e3b\u63a2\u7d22\u89e3\u51b3\u5177\u8eab\u4efb\u52a1\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9700\u8981\u6301\u7eed\u73af\u5883\u7406\u89e3\u548c\u52a8\u4f5c\u751f\u6210\u7684\u5177\u8eab\u4efb\u52a1\u89c4\u5212\u573a\u666f\u4e2d\u9762\u4e34\u663e\u8457\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u77e5\u8bc6\u751f\u6210\u5f00\u73af\u52a8\u4f5c\u811a\u672c\uff0c\u96be\u4ee5\u5b66\u4e60\u52a8\u4f5c\u4e0e\u73af\u5883\u53cd\u9988\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5c24\u5176\u662f\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u3002", "method": "\u5f15\u5165Embodied Planner-R1\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u7ed3\u679c\u9a71\u52a8\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c11\u76d1\u7763\u7684\u81ea\u4e3b\u63a2\u7d22\u4f7fLLMs\u53d1\u5c55\u4ea4\u4e92\u80fd\u529b\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5305\u62ec\uff1a1) \u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u91c7\u7528\u7eaf\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u7ec4rollout\uff0c\u901a\u8fc7\u5e76\u884c\u63a2\u7d22\u5b9e\u73b0\u73af\u5883\u5185\u4ea4\u4e92\uff1b2) \u91c7\u7528\u5b8c\u6210\u5ea6\u9a71\u52a8\u7684\u7a00\u758f\u5956\u52b1\u673a\u5236\uff1b3) \u4f7f\u7528\u4ea4\u4e92\u5f0f\u7b56\u7565\u4f18\u5316\uff08IPO\uff09\u4ee5\u9ad8\u6548\u4ece\u5206\u7ec4\u8f68\u8ff9\u4e2d\u5b66\u4e60\u3002", "result": "\u5728ALFWorld\u548cScienceWorld\u4e24\u4e2a\u5177\u6311\u6218\u6027\u7684\u6587\u672c\u57fa\u5177\u8eab\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEmbodied Planner-R1\u5206\u522b\u53d6\u5f97\u4e8697.78%\u548c79.92%\u7684\u5b8c\u6210\u7387\uff0c\u5927\u5e45\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002\u5728\u5148\u524d\u672a\u89c1\u8fc7\u7684\u73af\u5883\u4e2d\uff0c\u5176\u6027\u80fd\u4ec5\u4e0b\u964d3.66%\uff0c\u8bc1\u660e\u4e86\u5176\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Embodied Planner-R1\u901a\u8fc7\u521b\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u6210\u529f\u5730\u4f7fLLMs\u5728\u5177\u8eab\u4efb\u52a1\u89c4\u5212\u4e2d\u5177\u5907\u4e86\u5f3a\u5927\u7684\u4ea4\u4e92\u548c\u81ea\u4e3b\u63a2\u7d22\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u590d\u6742\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2506.22871", "pdf": "https://arxiv.org/pdf/2506.22871", "abs": "https://arxiv.org/abs/2506.22871", "authors": ["Homayun Afrabandpey", "Hamed Rezazadegan Tavakoli"], "title": "P$^2$U: Progressive Precision Update For Efficient Model Distribution", "categories": ["cs.LG", "cs.MM", "I.2.6"], "comment": null, "summary": "Efficient model distribution is becoming increasingly critical in\nbandwidth-constrained environments. In this paper, we propose a simple yet\neffective approach called Progressive Precision Update (P$^2$U) to address this\nproblem. Instead of transmitting the original high-precision model, P$^2$U\ntransmits a lower-bit precision model, coupled with a model update representing\nthe difference between the original high-precision model and the transmitted\nlow precision version. With extensive experiments on various model\narchitectures, ranging from small models ($1 - 6$ million parameters) to a\nlarge model (more than $100$ million parameters) and using three different data\nsets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U\nconsistently achieves better tradeoff between accuracy, bandwidth usage and\nlatency. Moreover, we show that when bandwidth or startup time is the priority,\naggressive quantization (e.g., 4-bit) can be used without severely compromising\nperformance. These results establish P$^2$U as an effective and practical\nsolution for scalable and efficient model distribution in low-resource\nsettings, including federated learning, edge computing, and IoT deployments.\nGiven that P$^2$U complements existing compression techniques and can be\nimplemented alongside any compression method, e.g., sparsification,\nquantization, pruning, etc., the potential for improvement is even greater.", "AI": {"tldr": "P$^2$U\u662f\u4e00\u79cd\u6e10\u8fdb\u5f0f\u7cbe\u5ea6\u66f4\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f20\u8f93\u4f4e\u7cbe\u5ea6\u6a21\u578b\u548c\u5dee\u503c\u66f4\u65b0\uff0c\u5728\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u6a21\u578b\u5206\u53d1\uff0c\u5e76\u5728\u51c6\u786e\u6027\u3001\u5e26\u5bbd\u548c\u5ef6\u8fdf\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\u3002", "motivation": "\u5728\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u4e0b\uff0c\u9ad8\u6548\u7684\u6a21\u578b\u5206\u53d1\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u6e10\u8fdb\u5f0f\u7cbe\u5ea6\u66f4\u65b0\uff08P$^2$U\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u4e0d\u76f4\u63a5\u4f20\u8f93\u539f\u59cb\u9ad8\u7cbe\u5ea6\u6a21\u578b\uff0c\u800c\u662f\u4f20\u8f93\u4e00\u4e2a\u4f4e\u6bd4\u7279\u7cbe\u5ea6\u6a21\u578b\uff0c\u5e76\u9644\u5e26\u4e00\u4e2a\u8868\u793a\u539f\u59cb\u9ad8\u7cbe\u5ea6\u6a21\u578b\u4e0e\u4f20\u8f93\u7684\u4f4e\u7cbe\u5ea6\u7248\u672c\u4e4b\u95f4\u5dee\u5f02\u7684\u6a21\u578b\u66f4\u65b0\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784\uff081M\u81f3100M+\u53c2\u6570\uff09\u548c\u4e09\u79cd\u6570\u636e\u96c6\uff08\u80f8\u90e8X\u5149\u3001PASCAL-VOC\u3001CIFAR-100\uff09\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u540e\uff0cP$^2$U\u5728\u51c6\u786e\u6027\u3001\u5e26\u5bbd\u4f7f\u7528\u548c\u5ef6\u8fdf\u4e4b\u95f4\u6301\u7eed\u53d6\u5f97\u66f4\u4f18\u7684\u6743\u8861\u3002\u7814\u7a76\u8868\u660e\uff0c\u5728\u5e26\u5bbd\u6216\u542f\u52a8\u65f6\u95f4\u4f18\u5148\u65f6\uff0c\u53ef\u4f7f\u7528\u6fc0\u8fdb\u91cf\u5316\uff08\u59824\u6bd4\u7279\uff09\u800c\u4e0d\u4e25\u91cd\u635f\u5bb3\u6027\u80fd\u3002", "conclusion": "P$^2$U\u662f\u4f4e\u8d44\u6e90\u73af\u5883\uff08\u5982\u8054\u90a6\u5b66\u4e60\u3001\u8fb9\u7f18\u8ba1\u7b97\u3001IoT\uff09\u4e2d\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u6a21\u578b\u5206\u53d1\u7684\u6709\u6548\u5b9e\u7528\u65b9\u6848\u3002P$^2$U\u53ef\u4e0e\u73b0\u6709\u538b\u7f29\u6280\u672f\uff08\u5982\u7a00\u758f\u5316\u3001\u91cf\u5316\u3001\u526a\u679d\uff09\u7ed3\u5408\u4f7f\u7528\uff0c\u5177\u6709\u66f4\u5927\u7684\u6539\u8fdb\u6f5c\u529b\u3002"}}
{"id": "2506.23703", "pdf": "https://arxiv.org/pdf/2506.23703", "abs": "https://arxiv.org/abs/2506.23703", "authors": ["Lars Ullrich", "Walter Zimmer", "Ross Greer", "Knut Graichen", "Alois C. Knoll", "Mohan Trivedi"], "title": "A New Perspective On AI Safety Through Control Theory Methodologies", "categories": ["cs.AI"], "comment": "Accepted to be published as part of the 2025 IEEE Open Journal of\n  Intelligent Transportation Systems (OJ-ITS)", "summary": "While artificial intelligence (AI) is advancing rapidly and mastering\nincreasingly complex problems with astonishing performance, the safety\nassurance of such systems is a major concern. Particularly in the context of\nsafety-critical, real-world cyber-physical systems, AI promises to achieve a\nnew level of autonomy but is hampered by a lack of safety assurance. While\ndata-driven control takes up recent developments in AI to improve control\nsystems, control theory in general could be leveraged to improve AI safety.\nTherefore, this article outlines a new perspective on AI safety based on an\ninterdisciplinary interpretation of the underlying data-generation process and\nthe respective abstraction by AI systems in a system theory-inspired and system\nanalysis-driven manner. In this context, the new perspective, also referred to\nas data control, aims to stimulate AI engineering to take advantage of existing\nsafety analysis and assurance in an interdisciplinary way to drive the paradigm\nof data control. Following a top-down approach, a generic foundation for safety\nanalysis and assurance is outlined at an abstract level that can be refined for\nspecific AI systems and applications and is prepared for future innovation.", "AI": {"tldr": "AI\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7f3a\u4e4f\u5b89\u5168\u4fdd\u969c\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7cfb\u7edf\u7406\u8bba\u548c\u6570\u636e\u751f\u6210\u7684\u8de8\u5b66\u79d1\u65b9\u6cd5\uff0c\u5373\u201c\u6570\u636e\u63a7\u5236\u201d\uff0c\u65e8\u5728\u4e3aAI\u5b89\u5168\u63d0\u4f9b\u901a\u7528\u7684\u5206\u6790\u4e0e\u4fdd\u969c\u57fa\u7840\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u53d1\u5c55\u8fc5\u901f\u5e76\u53d6\u5f97\u4e86\u60ca\u4eba\u7684\u6027\u80fd\uff0c\u4f46\u5176\u5b89\u5168\u4fdd\u969c\u662f\u4e00\u4e2a\u4e3b\u8981\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u5173\u952e\u7684\u5b9e\u9645\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u4e2d\uff0cAI\u867d\u6709\u671b\u5b9e\u73b0\u66f4\u9ad8\u6c34\u5e73\u7684\u81ea\u4e3b\u6027\uff0c\u5374\u53d7\u9650\u4e8e\u5b89\u5168\u4fdd\u969c\u7684\u7f3a\u5931\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7cfb\u7edf\u7406\u8bba\u542f\u53d1\u548c\u7cfb\u7edf\u5206\u6790\u9a71\u52a8\u7684AI\u5b89\u5168\u65b0\u89c6\u89d2\uff0c\u79f0\u4e3a\u201c\u6570\u636e\u63a7\u5236\u201d\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8de8\u5b66\u79d1\u89e3\u91ca\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u548cAI\u7cfb\u7edf\u62bd\u8c61\uff0c\u5e76\u91c7\u7528\u81ea\u9876\u5411\u4e0b\u7684\u65b9\u6cd5\uff0c\u52fe\u52d2\u51faAI\u5b89\u5168\u5206\u6790\u4e0e\u4fdd\u969c\u7684\u901a\u7528\u62bd\u8c61\u57fa\u7840\u3002", "result": "\u672c\u6587\u9610\u8ff0\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u62bd\u8c61\u7684AI\u5b89\u5168\u5206\u6790\u548c\u4fdd\u969c\u57fa\u7840\uff0c\u5176\u53ef\u4e3a\u5177\u4f53\u7684AI\u7cfb\u7edf\u548c\u5e94\u7528\u63d0\u4f9b\u8fdb\u4e00\u6b65\u7ec6\u5316\u7684\u4f9d\u636e\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u521b\u65b0\u9884\u7559\u4e86\u53d1\u5c55\u7a7a\u95f4\u3002", "conclusion": "AI\u7684\u5b89\u5168\u4fdd\u969c\u662f\u5173\u952e\u3002\u901a\u8fc7\u63d0\u51fa\u201c\u6570\u636e\u63a7\u5236\u201d\u8fd9\u4e00\u65b0\u8303\u5f0f\uff0c\u672c\u6587\u4e3aAI\u5b89\u5168\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u7cfb\u7edf\u7406\u8bba\u7684\u901a\u7528\u5206\u6790\u4e0e\u4fdd\u969c\u57fa\u7840\uff0c\u65e8\u5728\u523a\u6fc0AI\u5de5\u7a0b\u5229\u7528\u73b0\u6709\u5b89\u5168\u5206\u6790\u548c\u4fdd\u969c\u673a\u5236\uff0c\u4ee5\u5e94\u5bf9\u672a\u6765AI\u521b\u65b0\u7684\u5b89\u5168\u6311\u6218\u3002"}}
{"id": "2506.22753", "pdf": "https://arxiv.org/pdf/2506.22753", "abs": "https://arxiv.org/abs/2506.22753", "authors": ["Jianing Zhang", "Jiayi Zhu", "Feiyu Ji", "Xiaokang Yang", "Xiaoyun Yuan"], "title": "Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography", "categories": ["cs.CV"], "comment": null, "summary": "Metalenses offer significant potential for ultra-compact computational\nimaging but face challenges from complex optical degradation and computational\nrestoration difficulties. Existing methods typically rely on precise optical\ncalibration or massive paired datasets, which are non-trivial for real-world\nimaging systems. Furthermore, a lack of control over the inference process\noften results in undesirable hallucinated artifacts. We introduce\nDegradation-Modeled Multipath Diffusion for tunable metalens photography,\nleveraging powerful natural image priors from pretrained models instead of\nlarge datasets. Our framework uses positive, neutral, and negative-prompt paths\nto balance high-frequency detail generation, structural fidelity, and\nsuppression of metalens-specific degradation, alongside \\textit{pseudo} data\naugmentation. A tunable decoder enables controlled trade-offs between fidelity\nand perceptual quality. Additionally, a spatially varying degradation-aware\nattention (SVDA) module adaptively models complex optical and sensor-induced\ndegradation. Finally, we design and build a millimeter-scale MetaCamera for\nreal-world validation. Extensive results show that our approach outperforms\nstate-of-the-art methods, achieving high-fidelity and sharp image\nreconstruction. More materials: https://dmdiff.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u201c\u9000\u5316\u5efa\u6a21\u591a\u8def\u5f84\u6269\u6563\u201d(DMDM)\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u8c03\u8c10\u8d85\u900f\u955c\u8ba1\u7b97\u6210\u50cf\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u56fe\u50cf\u5148\u9a8c\u800c\u975e\u5927\u91cf\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u3001\u6e05\u6670\u7684\u56fe\u50cf\u91cd\u5efa\u3002", "motivation": "\u8d85\u900f\u955c\u5728\u8d85\u7d27\u51d1\u8ba1\u7b97\u6210\u50cf\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9762\u4e34\u590d\u6742\u7684\u5149\u5b66\u9000\u5316\u548c\u8ba1\u7b97\u6062\u590d\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7cbe\u786e\u7684\u5149\u5b66\u6821\u51c6\u6216\u5927\u91cf\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u63a8\u7406\u8fc7\u7a0b\u7684\u63a7\u5236\uff0c\u6613\u4ea7\u751f\u4e0d\u5e0c\u671b\u7684\u4f2a\u5f71\u3002", "method": "\u5f15\u5165\u201c\u9000\u5316\u5efa\u6a21\u591a\u8def\u5f84\u6269\u6563\u201d(Degradation-Modeled Multipath Diffusion, DMDM)\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u81ea\u7136\u56fe\u50cf\u5148\u9a8c\uff0c\u800c\u975e\u5927\u578b\u6570\u636e\u96c6\u3002\u5b83\u91c7\u7528\u6b63\u5411\u3001\u4e2d\u6027\u3001\u8d1f\u5411\u63d0\u793a\u8def\u5f84\u6765\u5e73\u8861\u9ad8\u9891\u7ec6\u8282\u751f\u6210\u3001\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u6291\u5236\u8d85\u900f\u955c\u7279\u5b9a\u9000\u5316\uff0c\u5e76\u7ed3\u5408\u4f2a\u6570\u636e\u589e\u5f3a\u3002\u4e00\u4e2a\u53ef\u8c03\u8c10\u89e3\u7801\u5668\u80fd\u5b9e\u73b0\u4fdd\u771f\u5ea6\u4e0e\u611f\u77e5\u8d28\u91cf\u4e4b\u95f4\u7684\u53ef\u63a7\u6743\u8861\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7a7a\u95f4\u53d8\u5f02\u9000\u5316\u611f\u77e5\u6ce8\u610f\u529b(SVDA)\u6a21\u5757\uff0c\u81ea\u9002\u5e94\u5efa\u6a21\u590d\u6742\u7684\u5149\u5b66\u548c\u4f20\u611f\u5668\u5f15\u8d77\u7684\u9000\u5316\u3002\u6700\u7ec8\uff0c\u8bbe\u8ba1\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u6beb\u7c73\u7ea7MetaCamera\u8fdb\u884c\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u548c\u6e05\u6670\u7684\u56fe\u50cf\u91cd\u5efa\u3002", "conclusion": "DMDM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8d85\u900f\u955c\u6210\u50cf\u4e2d\u590d\u6742\u9000\u5316\u548c\u6570\u636e\u4f9d\u8d56\u7684\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6269\u6563\u6a21\u578b\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u65e0\u9700\u5927\u91cf\u6821\u51c6\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u5353\u8d8a\u7684\u56fe\u50cf\u6062\u590d\uff0c\u5e76\u63d0\u4f9b\u7075\u6d3b\u7684\u8d28\u91cf\u63a7\u5236\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2506.23133", "pdf": "https://arxiv.org/pdf/2506.23133", "abs": "https://arxiv.org/abs/2506.23133", "authors": ["Dingzirui Wang", "Xuanliang Zhang", "Rongyu Cao", "Longxu Dou", "Xianzhen Luo", "Yingwei Ma", "Qingfu Zhu", "Wanxiang Che", "Binhua Li", "Fei Huang", "Yongbin Li"], "title": "Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format", "categories": ["cs.CL"], "comment": null, "summary": "Generating and voting multiple answers is an effective method to mitigate\nreasoning inconsistencies of large language models (LLMs). Prior works have\nshown that multiple reasoning formats outperform a single format when\ngenerating multiple answers. However, previous works using multiple formats\nrely on formats labeled by humans, which could be unsuitable for all tasks and\nhave high labeling costs. To address this issue, we adapt suitable formats to\nthe given tasks by generating and selecting formats. We first propose how to\nmeasure the reasoning error when generating multiple answers. Then, we\nintroduce Format-Adapter, which utilizes LLMs to generate and select suitable\nreasoning formats by minimizing the error measurement we present. We conduct\nexperiments on math and commonsense reasoning tasks, where Format-Adapter\nachieves a 4.3% performance improvement on average over previous works,\ndemonstrating the effectiveness.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u63a8\u7406\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faFormat-Adapter\u3002\u5b83\u5229\u7528LLMs\u81ea\u52a8\u751f\u6210\u5e76\u9009\u62e9\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\u7684\u63a8\u7406\u683c\u5f0f\uff0c\u800c\u975e\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u5e76\u5728\u6570\u5b66\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u5e73\u57474.3%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\uff0c\u5728LLMs\u751f\u6210\u591a\u4e2a\u7b54\u6848\u65f6\uff0c\u91c7\u7528\u591a\u79cd\u63a8\u7406\u683c\u5f0f\u4f18\u4e8e\u5355\u4e00\u683c\u5f0f\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u591a\u683c\u5f0f\u901a\u5e38\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u8fd9\u4e0d\u4ec5\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u6240\u6709\u4efb\u52a1\u3002", "method": "\u9996\u5148\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8861\u91cfLLMs\u751f\u6210\u591a\u4e2a\u7b54\u6848\u65f6\u63a8\u7406\u9519\u8bef\u7684\u65b9\u6cd5\u3002\u63a5\u7740\uff0c\u5f15\u5165\u4e86Format-Adapter\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528LLMs\u6839\u636e\u6240\u63d0\u51fa\u7684\u9519\u8bef\u6d4b\u91cf\u6807\u51c6\uff0c\u81ea\u52a8\u751f\u6210\u5e76\u9009\u62e9\u6700\u9002\u5408\u7ed9\u5b9a\u4efb\u52a1\u7684\u63a8\u7406\u683c\u5f0f\uff0c\u4ee5\u6700\u5c0f\u5316\u63a8\u7406\u9519\u8bef\u3002", "result": "\u5728\u6570\u5b66\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFormat-Adapter\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e864.3%\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "Format-Adapter\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210\u548c\u9009\u62e9\u63a8\u7406\u683c\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4eba\u5de5\u6807\u6ce8\u683c\u5f0f\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u5904\u7406\u63a8\u7406\u4efb\u52a1\u65f6\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u591a\u7b54\u6848\u751f\u6210\u548c\u6295\u7968\u7684\u573a\u666f\u4e0b\u3002"}}
{"id": "2506.22895", "pdf": "https://arxiv.org/pdf/2506.22895", "abs": "https://arxiv.org/abs/2506.22895", "authors": ["Xinyu Chen", "Vassilis Digalakis Jr", "Lijun Ding", "Dingyi Zhuang", "Jinhua Zhao"], "title": "Interpretable Time Series Autoregression for Periodicity Quantification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series autoregression is a classical statistical model for capturing\nauto-correlations and identifying temporal patterns such as periodicity and\nseasonality. In this work, we propose a novel sparse autoregression framework\nfrom an interpretable machine learning perspective and the model\ninterpretability for periodicity quantification is reinforced by $\\ell_0$-norm\ninduced sparsity constraints. On the time-varying time series data, we\nreformulate the sparse autoregression and convert the involved optimization\nproblem into a mixed-integer optimization (MIO). To accelerate it, we develop a\nsubspace pursuit based decision variable pruning (DVP) strategy to reduce the\nsearch space. On the multidimensional time series that involves complicated\nspatial and temporal dimensions, we propose a spatially- and time-varying\nsparse autoregression model and resolve the corresponding MIO problem by\ndeveloping a two-stage optimization scheme. In particular, the proposed scheme\nmakes the model scalable to large problems even with millions of decision\nvariables. Empirically, we conduct extensive experiments to evaluate the\nproposed models on real-world time series data. First, we demonstrate that the\nMIO solver can be drastically accelerated through the DVP strategy, while\nmaintaining the same solution quality as a full MIO solver. Applying the\ntime-varying sparse autoregression model to ridesharing trip data, we uncover\nboth daily and weekly periodicities and reveal long-term changes in regularity\nof human mobility. Second, we demonstrate the spatial patterns of yearly\nseasonality in climate variable time series such as temperature and\nprecipitation across the past four decades, and our model allows to discover\ndynamic climate patterns and identify climate phenomena such as El Nino in sea\nsurface temperature.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.23706", "pdf": "https://arxiv.org/pdf/2506.23706", "abs": "https://arxiv.org/abs/2506.23706", "authors": ["Christoph Schnabl", "Daniel Hugenroth", "Bill Marino", "Alastair R. Beresford"], "title": "Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments", "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": "ICML 2024 Workshop TAIG", "summary": "Benchmarks are important measures to evaluate safety and compliance of AI\nmodels at scale. However, they typically do not offer verifiable results and\nlack confidentiality for model IP and benchmark datasets. We propose Attestable\nAudits, which run inside Trusted Execution Environments and enable users to\nverify interaction with a compliant AI model. Our work protects sensitive data\neven when model provider and auditor do not trust each other. This addresses\nverification challenges raised in recent AI governance frameworks. We build a\nprototype demonstrating feasibility on typical audit benchmarks against\nLlama-3.1.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u53ef\u8bc1\u660e\u5ba1\u8ba1\uff08Attestable Audits\uff09\uff0c\u5229\u7528\u53ef\u4fe1\u6267\u884c\u73af\u5883\uff08TEEs\uff09\u89e3\u51b3AI\u6a21\u578b\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7ed3\u679c\u4e0d\u53ef\u9a8c\u8bc1\u548c\u6570\u636e\u4fdd\u5bc6\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u6a21\u578b\u8bc4\u4f30\u7684\u53ef\u9a8c\u8bc1\u6027\u548c\u6570\u636e\u4fdd\u62a4\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u7684\u5b89\u5168\u4e0e\u5408\u89c4\u6027\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u65e0\u6cd5\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u7ed3\u679c\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u6a21\u578b\u77e5\u8bc6\u4ea7\u6743\u548c\u57fa\u51c6\u6570\u636e\u96c6\u7684\u4fdd\u5bc6\u6027\uff0c\u8fd9\u4e3aAI\u6cbb\u7406\u6846\u67b6\u5e26\u6765\u4e86\u9a8c\u8bc1\u6311\u6218\u3002", "method": "\u63d0\u51fa\u201c\u53ef\u8bc1\u660e\u5ba1\u8ba1\u201d\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u53ef\u4fe1\u6267\u884c\u73af\u5883\uff08Trusted Execution Environments, TEEs\uff09\u4e2d\u8fd0\u884c\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u9a8c\u8bc1\u4e0e\u7b26\u5408\u89c4\u8303\u7684AI\u6a21\u578b\u7684\u4ea4\u4e92\uff0c\u5373\u4f7f\u6a21\u578b\u63d0\u4f9b\u5546\u548c\u5ba1\u8ba1\u65b9\u4e92\u4e0d\u4fe1\u4efb\u4e5f\u80fd\u4fdd\u62a4\u654f\u611f\u6570\u636e\u3002", "result": "\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86\u4e00\u4e2a\u539f\u578b\u7cfb\u7edf\uff0c\u6210\u529f\u5728\u9488\u5bf9Llama-3.1\u7684\u5178\u578b\u5ba1\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u53ef\u8bc1\u660e\u5ba1\u8ba1\u901a\u8fc7\u5728\u53ef\u4fe1\u6267\u884c\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u89e3\u51b3\u4e86AI\u6a21\u578b\u8bc4\u4f30\u4e2d\u7ed3\u679c\u53ef\u9a8c\u8bc1\u6027\u548c\u6570\u636e\u4fdd\u5bc6\u6027\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3aAI\u6cbb\u7406\u6846\u67b6\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u786e\u4fdd\u4e86\u654f\u611f\u6570\u636e\u5728\u5ba1\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2506.22756", "pdf": "https://arxiv.org/pdf/2506.22756", "abs": "https://arxiv.org/abs/2506.22756", "authors": ["Tao Tang", "Likui Zhang", "Youpeng Wen", "Kaidong Zhang", "Jia-Wang Bian", "xia zhou", "Tianyi Yan", "Kun Zhan", "Peng Jia", "Hefeng Wu", "Liang Lin", "Xiaodan Liang"], "title": "RoboPearls: Editable Video Simulation for Robot Manipulation", "categories": ["cs.CV", "cs.RO"], "comment": "ICCV 2025", "summary": "The development of generalist robot manipulation policies has seen\nsignificant progress, driven by large-scale demonstration data across diverse\nenvironments. However, the high cost and inefficiency of collecting real-world\ndemonstrations hinder the scalability of data acquisition. While existing\nsimulation platforms enable controlled environments for robotic learning, the\nchallenge of bridging the sim-to-real gap remains. To address these challenges,\nwe propose RoboPearls, an editable video simulation framework for robotic\nmanipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the\nconstruction of photo-realistic, view-consistent simulations from demonstration\nvideos, and supports a wide range of simulation operators, including various\nobject manipulations, powered by advanced modules like Incremental Semantic\nDistillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by\nincorporating large language models (LLMs), RoboPearls automates the simulation\nproduction process in a user-friendly manner through flexible command\ninterpretation and execution. Furthermore, RoboPearls employs a vision-language\nmodel (VLM) to analyze robotic learning issues to close the simulation loop for\nperformance enhancement. To demonstrate the effectiveness of RoboPearls, we\nconduct extensive experiments on multiple datasets and scenes, including\nRLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which\ndemonstrate our satisfactory simulation performance.", "AI": {"tldr": "RoboPearls\u662f\u4e00\u4e2a\u57fa\u4e8e3DGS\u7684\u53ef\u7f16\u8f91\u89c6\u9891\u4eff\u771f\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u4ece\u6f14\u793a\u89c6\u9891\u751f\u6210\u903c\u771f\u4eff\u771f\u5e76\u7ed3\u5408LLM/VLM\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u6570\u636e\u6536\u96c6\u9ad8\u6210\u672c\u548cSim-to-Real\u9e3f\u6c9f\u95ee\u9898\u3002", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u53d1\u5c55\u4f9d\u8d56\u5927\u91cf\u6f14\u793a\u6570\u636e\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\u3002\u73b0\u6709\u4eff\u771f\u5e73\u53f0\u96be\u4ee5\u5f25\u5408Sim-to-Real\u9e3f\u6c9f\u3002", "method": "\u63d0\u51faRoboPearls\uff0c\u4e00\u4e2a\u57fa\u4e8e3D Gaussian Splatting (3DGS) \u7684\u53ef\u7f16\u8f91\u89c6\u9891\u4eff\u771f\u6846\u67b6\u3002\u5b83\u80fd\u4ece\u6f14\u793a\u89c6\u9891\u6784\u5efa\u903c\u771f\u3001\u89c6\u89d2\u4e00\u81f4\u7684\u4eff\u771f\uff0c\u5e76\u652f\u6301\u7269\u4f53\u64cd\u7eb5\u7b49\u591a\u79cd\u64cd\u4f5c\uff08\u901a\u8fc7ISD\u548c3D-NNFM\uff09\u3002\u901a\u8fc7\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLMs) \u5b9e\u73b0\u81ea\u52a8\u5316\u4eff\u771f\u751f\u4ea7\uff0c\u5e76\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b (VLM) \u5206\u6790\u673a\u5668\u4eba\u5b66\u4e60\u95ee\u9898\uff0c\u5f62\u6210\u4eff\u771f\u95ed\u73af\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728RLBench\u3001COLOSSEUM\u3001Ego4D\u3001Open X-Embodiment\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u4eff\u771f\u6027\u80fd\u3002", "conclusion": "RoboPearls\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u53ef\u7f16\u8f91\u3001\u903c\u771f\u7684\u89c6\u9891\u4eff\u771f\u6846\u67b6\uff0c\u5e76\u7ed3\u5408LLM/VLM\u7684\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u6570\u636e\u83b7\u53d6\u6548\u7387\u4f4e\u548cSim-to-Real\u9e3f\u6c9f\u7684\u6311\u6218\u3002"}}
{"id": "2506.23136", "pdf": "https://arxiv.org/pdf/2506.23136", "abs": "https://arxiv.org/abs/2506.23136", "authors": ["Shadman Sobhan", "Mohammad Ariful Haque"], "title": "LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation", "categories": ["cs.CL"], "comment": "29 Pages, 11 Tables", "summary": "Large Language Models (LLMs) are capable of natural language understanding\nand generation. But they face challenges such as hallucination and outdated\nknowledge. Fine-tuning is one possible solution, but it is resource-intensive\nand must be repeated with every data update. Retrieval-Augmented Generation\n(RAG) offers an efficient solution by allowing LLMs to access external\nknowledge sources. However, traditional RAG pipelines struggle with retrieving\ninformation from complex technical documents with structured data such as\ntables and images. In this work, we propose a RAG pipeline, capable of handling\ntables and images in documents, for technical documents that support both\nscanned and searchable formats. Its retrieval process combines vector\nsimilarity search with a fine-tuned reranker based on Gemma-2-9b-it. The\nreranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom\ndataset designed to improve context identification for question answering. Our\nevaluation demonstrates that the proposed pipeline achieves a high faithfulness\nscore of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%\n(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed\narchitecture is superior to general RAG pipelines in terms of table-based\nquestions and handling questions outside context.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bRAG\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u7ed3\u5408\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u57fa\u4e8eGemma-2-9b-it\u7684\u5fae\u8c03\u91cd\u6392\u5e8f\u5668\uff0c\u6709\u6548\u5904\u7406\u5305\u542b\u8868\u683c\u548c\u56fe\u50cf\u7684\u590d\u6742\u6280\u672f\u6587\u6863\uff0c\u663e\u8457\u63d0\u5347\u4e86\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9762\u4e34\u5e7b\u89c9\u548c\u77e5\u8bc6\u8fc7\u65f6\u7b49\u6311\u6218\u3002\u5fae\u8c03\u867d\u662f\u89e3\u51b3\u65b9\u6848\u4f46\u8d44\u6e90\u5bc6\u96c6\u4e14\u9700\u9891\u7e41\u91cd\u590d\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u80fd\u8bbf\u95ee\u5916\u90e8\u77e5\u8bc6\uff0c\u4f46\u4f20\u7edfRAG\u96be\u4ee5\u4ece\u5305\u542b\u8868\u683c\u548c\u56fe\u50cf\u7684\u590d\u6742\u6280\u672f\u6587\u6863\u4e2d\u68c0\u7d22\u4fe1\u606f\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2aRAG\u6d41\u6c34\u7ebf\uff0c\u4e13\u4e3a\u5904\u7406\u5305\u542b\u8868\u683c\u548c\u56fe\u50cf\u7684\u6280\u672f\u6587\u6863\uff08\u652f\u6301\u626b\u63cf\u548c\u53ef\u641c\u7d22\u683c\u5f0f\uff09\u3002\u5176\u68c0\u7d22\u8fc7\u7a0b\u7ed3\u5408\u4e86\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u57fa\u4e8eGemma-2-9b-it\u7684\u5fae\u8c03\u91cd\u6392\u5e8f\u5668\u3002\u8be5\u91cd\u6392\u5e8f\u5668\u5229\u7528RAFT\uff08Retrieval-Augmented Fine-Tuning\uff09\u5728\u4e00\u4e2a\u5b9a\u5236\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e8\u5728\u63d0\u9ad8\u95ee\u7b54\u7684\u4e0a\u4e0b\u6587\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6d41\u6c34\u7ebf\u5728RAGas\u548cDeepEval\u8bc4\u4f30\u4e2d\u53d6\u5f97\u4e86\u9ad8\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0894%\u548c96%\uff09\u548c\u7b54\u6848\u76f8\u5173\u6027\u5206\u6570\uff0887%\u548c93%\uff09\u3002\u5bf9\u6bd4\u5206\u6790\u663e\u793a\uff0c\u8be5\u67b6\u6784\u5728\u5904\u7406\u57fa\u4e8e\u8868\u683c\u7684\u95ee\u9898\u548c\u4e0a\u4e0b\u6587\u5916\u95ee\u9898\u65b9\u9762\uff0c\u4f18\u4e8e\u4e00\u822c\u7684RAG\u6d41\u6c34\u7ebf\u3002", "conclusion": "\u8be5RAG\u6d41\u6c34\u7ebf\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfRAG\u5728\u5904\u7406\u590d\u6742\u6280\u672f\u6587\u6863\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u4f18\u5316\u68c0\u7d22\u548c\u91cd\u6392\u5e8f\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u6280\u672f\u95ee\u7b54\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u5177\u6709\u9ad8\u7f6e\u4fe1\u5ea6\u548c\u76f8\u5173\u6027\u3002"}}
{"id": "2506.22901", "pdf": "https://arxiv.org/pdf/2506.22901", "abs": "https://arxiv.org/abs/2506.22901", "authors": ["Sina Tabakhi", "Haiping Lu"], "title": "Missing-Modality-Aware Graph Neural Network for Cancer Classification", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.GN"], "comment": "15 pages, 7 figures", "summary": "A key challenge in learning from multimodal biological data is missing\nmodalities, where all data from some modalities are missing for some patients.\nCurrent fusion methods address this by excluding patients with missing\nmodalities, imputing missing modalities, or making predictions directly with\npartial modalities. However, they often struggle with diverse missing-modality\npatterns and the exponential growth of the number of such patterns as the\nnumber of modalities increases. To address these limitations, we propose MAGNET\n(Missing-modality-Aware Graph neural NETwork) for direct prediction with\npartial modalities, which introduces a patient-modality multi-head attention\nmechanism to fuse lower-dimensional modality embeddings based on their\nimportance and missingness. MAGNET's complexity increases linearly with the\nnumber of modalities while adapting to missing-pattern variability. To generate\npredictions, MAGNET further constructs a patient graph with fused multimodal\nembeddings as node features and the connectivity determined by the modality\nmissingness, followed by a conventional graph neural network. Experiments on\nthree public multiomics datasets for cancer classification, with real-world\ninstead of artificial missingness, show that MAGNET outperforms the\nstate-of-the-art fusion methods. The data and code are available at\nhttps://github.com/SinaTabakhi/MAGNET.", "AI": {"tldr": "\u63d0\u51faMAGNET\uff0c\u4e00\u79cd\u7528\u4e8e\u5904\u7406\u591a\u6a21\u6001\u751f\u7269\u6570\u636e\u4e2d\u6a21\u6001\u7f3a\u5931\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u60a3\u8005\u56fe\u6784\u5efa\uff0c\u5728\u764c\u75c7\u5206\u7c7b\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5904\u7406\u591a\u6a21\u6001\u751f\u7269\u6570\u636e\u4e2d\u6a21\u6001\u7f3a\u5931\u7684\u65b9\u6cd5\uff08\u5982\u6392\u9664\u3001\u63d2\u8865\u6216\u76f4\u63a5\u9884\u6d4b\uff09\u96be\u4ee5\u6709\u6548\u5e94\u5bf9\u591a\u6837\u5316\u4e14\u968f\u6a21\u6001\u6570\u91cf\u6307\u6570\u589e\u957f\u7684\u7f3a\u5931\u6a21\u5f0f\u3002", "method": "\u63d0\u51faMAGNET (Missing-modality-Aware Graph neural NETwork)\uff0c\u5b83\u5f15\u5165\u60a3\u8005-\u6a21\u6001\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u6a21\u6001\u5d4c\u5165\uff0c\u5176\u590d\u6742\u5ea6\u968f\u6a21\u6001\u6570\u91cf\u7ebf\u6027\u589e\u957f\u5e76\u9002\u5e94\u7f3a\u5931\u6a21\u5f0f\u3002\u4e4b\u540e\uff0c\u6784\u5efa\u4e00\u4e2a\u4ee5\u878d\u5408\u591a\u6a21\u6001\u5d4c\u5165\u4e3a\u8282\u70b9\u7279\u5f81\u3001\u4ee5\u6a21\u6001\u7f3a\u5931\u6027\u786e\u5b9a\u8fde\u63a5\u7684\u60a3\u8005\u56fe\uff0c\u518d\u901a\u8fc7\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u4e09\u4e2a\u4f7f\u7528\u771f\u5b9e\u7f3a\u5931\u6570\u636e\u7684\u516c\u5171\u591a\u7ec4\u5b66\u764c\u75c7\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\uff0cMAGNET\u7684\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u878d\u5408\u65b9\u6cd5\u3002", "conclusion": "MAGNET\u4e3a\u76f4\u63a5\u5229\u7528\u90e8\u5206\u591a\u6a21\u6001\u6570\u636e\u8fdb\u884c\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u6027\u80fd\u5353\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u9c81\u68d2\u5730\u5904\u7406\u591a\u6837\u7684\u6a21\u6001\u7f3a\u5931\u6a21\u5f0f\u3002"}}
{"id": "2506.23773", "pdf": "https://arxiv.org/pdf/2506.23773", "abs": "https://arxiv.org/abs/2506.23773", "authors": ["Stefano M. Nicoletti", "Mari\u00eblle Stoelinga"], "title": "BayesL: Towards a Logical Framework for Bayesian Networks", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "We introduce BayesL, a novel logical framework for specifying, querying, and\nverifying the behaviour of Bayesian networks (BNs). BayesL (pronounced \"Basil\")\nis a structured language that allows for the creation of queries over BNs. It\nfacilitates versatile reasoning concerning causal and evidence-based\nrelationships, and permits comprehensive what-if scenario evaluations without\nthe need for manual modifications to the model.", "AI": {"tldr": "\u5f15\u5165BayesL\uff0c\u4e00\u4e2a\u65b0\u578b\u903b\u8f91\u6846\u67b6\uff0c\u7528\u4e8e\u8d1d\u53f6\u65af\u7f51\u7edc\uff08BNs\uff09\u7684\u884c\u4e3a\u6307\u5b9a\u3001\u67e5\u8be2\u548c\u9a8c\u8bc1\uff0c\u652f\u6301\u65e0\u9700\u624b\u52a8\u4fee\u6539\u6a21\u578b\u7684\u56e0\u679c\u63a8\u7406\u548c\u5047\u8bbe\u60c5\u666f\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u652f\u6301\u5bf9\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u884c\u4e3a\u6307\u5b9a\u3001\u67e5\u8be2\u548c\u9a8c\u8bc1\uff0c\u4ee5\u53ca\u7075\u6d3b\u7684\u56e0\u679c\u63a8\u7406\u548c\u5168\u9762\u7684\u5047\u8bbe\u60c5\u666f\u8bc4\u4f30\uff0c\u4e14\u5e38\u9700\u624b\u52a8\u4fee\u6539\u6a21\u578b\uff0c\u5b58\u5728\u64cd\u4f5c\u4e0d\u4fbf\u7684\u75db\u70b9\u3002", "method": "\u63d0\u51fa\u5e76\u5f15\u5165\u4e86BayesL\uff0c\u4e00\u79cd\u65b0\u578b\u7684\u903b\u8f91\u6846\u67b6\uff0c\u5b83\u662f\u4e00\u4e2a\u7ed3\u6784\u5316\u8bed\u8a00\u3002", "result": "BayesL\u80fd\u591f\u521b\u5efa\u9488\u5bf9\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u67e5\u8be2\uff0c\u4fc3\u8fdb\u56e0\u679c\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u7075\u6d3b\u63a8\u7406\uff0c\u5e76\u5141\u8bb8\u5728\u4e0d\u624b\u52a8\u4fee\u6539\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5168\u9762\u7684\u5047\u8bbe\u60c5\u666f\u8bc4\u4f30\u3002", "conclusion": "BayesL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u7ed3\u6784\u5316\u8bed\u8a00\uff0c\u663e\u8457\u7b80\u5316\u548c\u589e\u5f3a\u4e86\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u67e5\u8be2\u3001\u63a8\u7406\u548c\u60c5\u666f\u8bc4\u4f30\u80fd\u529b\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u624b\u52a8\u4fee\u6539\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.22762", "pdf": "https://arxiv.org/pdf/2506.22762", "abs": "https://arxiv.org/abs/2506.22762", "authors": ["Dinh Phu Tran", "Dao Duy Hung", "Daeyoung Kim"], "title": "VSRM: A Robust Mamba-Based Framework for Video Super-Resolution", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Video super-resolution remains a major challenge in low-level vision tasks.\nTo date, CNN- and Transformer-based methods have delivered impressive results.\nHowever, CNNs are limited by local receptive fields, while Transformers\nstruggle with quadratic complexity, posing challenges for processing long\nsequences in VSR. Recently, Mamba has drawn attention for its long-sequence\nmodeling, linear complexity, and large receptive fields. In this work, we\npropose VSRM, a novel \\textbf{V}ideo \\textbf{S}uper-\\textbf{R}esolution\nframework that leverages the power of \\textbf{M}amba. VSRM introduces\nSpatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract\nlong-range spatio-temporal features and enhance receptive fields efficiently.\nTo better align adjacent frames, we propose Deformable Cross-Mamba Alignment\nmodule. This module utilizes a deformable cross-mamba mechanism to make the\ncompensation stage more dynamic and flexible, preventing feature distortions.\nFinally, we minimize the frequency domain gaps between reconstructed and\nground-truth frames by proposing a simple yet effective Frequency\nCharbonnier-like loss that better preserves high-frequency content and enhances\nvisual quality. Through extensive experiments, VSRM achieves state-of-the-art\nresults on diverse benchmarks, establishing itself as a solid foundation for\nfuture research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVSRM\u6846\u67b6\uff0c\u5229\u7528Mamba\u6a21\u578b\u89e3\u51b3\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4e2dCNN\u548cTransformer\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u65b0\u9896\u7684Mamba\u6a21\u5757\u3001\u53ef\u53d8\u5f62\u5bf9\u9f50\u548c\u9891\u7387\u57df\u635f\u5931\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff08VSR\uff09\u4ecd\u662f\u4f4e\u7ea7\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u4e3b\u8981\u6311\u6218\u3002\u73b0\u6709\u7684CNN\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c40\u90e8\u611f\u53d7\u91ce\uff0c\u800cTransformer\u65b9\u6cd5\u5728\u5904\u7406VSR\u957f\u5e8f\u5217\u65f6\u9762\u4e34\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86VSRM\u6846\u67b6\uff0c\u4e00\u4e2a\u5229\u7528Mamba\u6a21\u578b\u80fd\u529b\u7684\u65b0\u578b\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6846\u67b6\u3002VSRM\u5f15\u5165\u4e86\u7a7a\u95f4\u5230\u65f6\u95f4Mamba\uff08Spatial-to-Temporal Mamba\uff09\u548c\u65f6\u95f4\u5230\u7a7a\u95f4Mamba\uff08Temporal-to-Spatial Mamba\uff09\u6a21\u5757\u6765\u6709\u6548\u63d0\u53d6\u957f\u8ddd\u79bb\u65f6\u7a7a\u7279\u5f81\u5e76\u589e\u5f3a\u611f\u53d7\u91ce\u3002\u4e3a\u66f4\u597d\u5730\u5bf9\u9f50\u76f8\u90bb\u5e27\uff0c\u63d0\u51fa\u53ef\u53d8\u5f62\u4ea4\u53c9Mamba\u5bf9\u9f50\u6a21\u5757\uff08Deformable Cross-Mamba Alignment\uff09\uff0c\u4ee5\u5b9e\u73b0\u66f4\u52a8\u6001\u548c\u7075\u6d3b\u7684\u8865\u507f\uff0c\u9632\u6b62\u7279\u5f81\u5931\u771f\u3002\u6700\u540e\uff0c\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u9891\u7387Charbonnier-like\u635f\u5931\uff0c\u6700\u5c0f\u5316\u91cd\u5efa\u5e27\u4e0e\u771f\u5b9e\u5e27\u4e4b\u95f4\u7684\u9891\u57df\u5dee\u8ddd\uff0c\u4ee5\u66f4\u597d\u5730\u4fdd\u7559\u9ad8\u9891\u5185\u5bb9\u5e76\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0cVSRM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08state-of-the-art\uff09\u7684\u7ed3\u679c\u3002", "conclusion": "VSRM\u4e3a\u672a\u6765\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u8bc1\u660e\u4e86Mamba\u5728\u5904\u7406\u957f\u5e8f\u5217\u548c\u6269\u5927\u611f\u53d7\u91ce\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.23137", "pdf": "https://arxiv.org/pdf/2506.23137", "abs": "https://arxiv.org/abs/2506.23137", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun"], "title": "Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Effective modeling of multifaceted relations is pivotal for Knowledge Graph\nCompletion (KGC). However, a majority of existing approaches are predicated on\nstatic, embedding-based scoring, exhibiting inherent limitations in capturing\ncontextual dependencies and relational dynamics. Addressing this gap, we\npropose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal\ncomponents: (1) a semantic context learning module that encodes\ncontext-sensitive entity representations, and (2) a conditional flow-matching\nmodule designed to learn the dynamic transformation from a head to a tail\nembedding, governed by the aforementioned context. The resultant predictive\nvector field, representing the context-informed relational path, serves to\ndynamically refine the initial static score of an entity pair. Through this\nsynergy of context-aware static representations and conditioned dynamic\ninformation, FMS facilitates a more profound modeling of relational semantics.\nComprehensive evaluations on several standard benchmarks demonstrate that our\nproposed method surpasses prior state-of-the-art results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFMS\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u9759\u6001\u5b9e\u4f53\u8868\u793a\u548c\u6761\u4ef6\u52a8\u6001\u8f6c\u6362\u5b66\u4e60\uff0c\u52a8\u6001\u4f18\u5316\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4e2d\u7684\u5173\u7cfb\u8bc4\u5206\uff0c\u5e76\u8d85\u8d8a\u73b0\u6709SOTA\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u3001\u57fa\u4e8e\u5d4c\u5165\u7684\u8bc4\u5206\uff0c\u5728\u6355\u6349\u4e0a\u4e0b\u6587\u4f9d\u8d56\u548c\u5173\u7cfb\u52a8\u6001\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u5173\u7cfb\u5efa\u6a21\u6df1\u5ea6\u4e0d\u8db3\u3002", "method": "\u63d0\u51faFlow-Modulated Scoring (FMS) \u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u8bed\u4e49\u4e0a\u4e0b\u6587\u5b66\u4e60\u6a21\u5757\uff0c\u7528\u4e8e\u7f16\u7801\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u5b9e\u4f53\u8868\u793a\uff1b2) \u6761\u4ef6\u6d41\u5339\u914d\u6a21\u5757\uff0c\u5b66\u4e60\u7531\u4e0a\u4e0b\u6587\u63a7\u5236\u7684\u4ece\u5934\u5b9e\u4f53\u5230\u5c3e\u5b9e\u4f53\u7684\u52a8\u6001\u8f6c\u6362\u3002\u8be5\u6846\u67b6\u5229\u7528\u751f\u6210\u7684\u9884\u6d4b\u5411\u91cf\u573a\u52a8\u6001\u8c03\u6574\u5b9e\u4f53\u5bf9\u7684\u521d\u59cb\u9759\u6001\u5206\u6570\u3002", "result": "\u5728\u591a\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "FMS\u901a\u8fc7\u6574\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u9759\u6001\u8868\u793a\u548c\u6761\u4ef6\u52a8\u6001\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5173\u7cfb\u8bed\u4e49\u66f4\u6df1\u5165\u7684\u5efa\u6a21\uff0c\u6709\u6548\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22927", "pdf": "https://arxiv.org/pdf/2506.22927", "abs": "https://arxiv.org/abs/2506.22927", "authors": ["Jaeyun Woo", "Jiseok Lee", "Brian Kenji Iwana"], "title": "Towards Time Series Generation Conditioned on Unstructured Natural Language", "categories": ["cs.LG"], "comment": null, "summary": "Generative Artificial Intelligence (AI) has rapidly become a powerful tool,\ncapable of generating various types of data, such as images and text. However,\ndespite the significant advancement of generative AI, time series generative AI\nremains underdeveloped, even though the application of time series is essential\nin finance, climate, and numerous fields. In this research, we propose a novel\nmethod of generating time series conditioned on unstructured natural language\ndescriptions. We use a diffusion model combined with a language model to\ngenerate time series from the text. Through the proposed method, we demonstrate\nthat time series generation based on natural language is possible. The proposed\nmethod can provide various applications such as custom forecasting, time series\nmanipulation, data augmentation, and transfer learning. Furthermore, we\nconstruct and propose a new public dataset for time series generation,\nconsisting of 63,010 time series-description pairs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u65f6\u95f4\u5e8f\u5217\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u578b\u65f6\u95f4\u5e8f\u5217-\u63cf\u8ff0\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u4ee5\u586b\u8865\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u5f0fAI\u7684\u7a7a\u767d\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5f0fAI\u5728\u56fe\u50cf\u548c\u6587\u672c\u7b49\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u5f0fAI\u4ecd\u4e0d\u6210\u719f\uff0c\u800c\u65f6\u95f4\u5e8f\u5217\u5728\u91d1\u878d\u3001\u6c14\u5019\u7b49\u591a\u4e2a\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6269\u6563\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u5b9e\u73b0\u6839\u636e\u975e\u7ed3\u6784\u5316\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b63,010\u5bf9\u65f6\u95f4\u5e8f\u5217-\u63cf\u8ff0\u7684\u65b0\u516c\u5171\u6570\u636e\u96c6\u3002", "result": "\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u662f\u53ef\u884c\u7684\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u652f\u6301\u81ea\u5b9a\u4e49\u9884\u6d4b\u3001\u65f6\u95f4\u5e8f\u5217\u64cd\u4f5c\u3001\u6570\u636e\u589e\u5f3a\u548c\u8fc1\u79fb\u5b66\u4e60\u7b49\u591a\u79cd\u5e94\u7528\u3002\u540c\u65f6\uff0c\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u516c\u5171\u6570\u636e\u96c6\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5c55\u793a\u4e86\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u652f\u6301\u672a\u6765\u7814\u7a76\u7684\u5b9d\u8d35\u6570\u636e\u96c6\uff0c\u6709\u671b\u63a8\u52a8\u65f6\u95f4\u5e8f\u5217\u5728\u591a\u4e2a\u9886\u57df\u7684\u5e94\u7528\u521b\u65b0\u3002"}}
{"id": "2506.23784", "pdf": "https://arxiv.org/pdf/2506.23784", "abs": "https://arxiv.org/abs/2506.23784", "authors": ["Parosh Aziz Abdulla", "Mohamed Faouzi Atig", "Julie Cailler", "Chencheng Liang", "Philipp R\u00fcmmer"], "title": "When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Nielsen transformation is a standard approach for solving word equations: by\nrepeatedly splitting equations and applying simplification steps, equations are\nrewritten until a solution is reached. When solving a conjunction of word\nequations in this way, the performance of the solver will depend considerably\non the order in which equations are processed. In this work, the use of Graph\nNeural Networks (GNNs) for ranking word equations before and during the solving\nprocess is explored. For this, a novel graph-based representation for word\nequations is presented, preserving global information across conjuncts,\nenabling the GNN to have a holistic view during ranking. To handle the variable\nnumber of conjuncts, three approaches to adapt a multi-classification task to\nthe problem of ranking equations are proposed. The training of the GNN is done\nwith the help of minimum unsatisfiable subsets (MUSes) of word equations. The\nexperimental results show that, compared to state-of-the-art string solvers,\nthe new framework solves more problems in benchmarks where each variable\nappears at most once in each equation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc(GNN)\u5bf9\u5b57\u65b9\u7a0b\u8fdb\u884c\u6392\u5e8f\uff0c\u4ee5\u4f18\u5316Nielsen\u53d8\u6362\u6c42\u89e3\u8fde\u8bcd\u5b57\u65b9\u7a0b\u7ec4\u7684\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u56fe\u8868\u793a\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u7279\u5b9a\u7c7b\u578b\u95ee\u9898\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709Nielsen\u53d8\u6362\u6c42\u89e3\u5b57\u65b9\u7a0b\u7ec4\u65f6\uff0c\u6c42\u89e3\u5668\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u4e8e\u65b9\u7a0b\u5904\u7406\u987a\u5e8f\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u6392\u5e8f\uff0c\u4ee5\u63d0\u5347\u6c42\u89e3\u6548\u7387\u3002", "method": "\u63a2\u7d22\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc(GNN)\u5728\u6c42\u89e3\u524d\u548c\u6c42\u89e3\u8fc7\u7a0b\u4e2d\u5bf9\u5b57\u65b9\u7a0b\u8fdb\u884c\u6392\u5e8f\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u4fdd\u7559\u8fde\u8bcd\u95f4\u5168\u5c40\u4fe1\u606f\u7684\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u4f7fGNN\u80fd\u83b7\u5f97\u6574\u4f53\u89c6\u56fe\u3002\u4e3a\u5904\u7406\u53ef\u53d8\u6570\u91cf\u7684\u8fde\u8bcd\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u5c06\u591a\u5206\u7c7b\u4efb\u52a1\u5e94\u7528\u4e8e\u65b9\u7a0b\u6392\u5e8f\u7684\u65b9\u6cd5\u3002GNN\u7684\u8bad\u7ec3\u901a\u8fc7\u5b57\u65b9\u7a0b\u7684\u6700\u5c0f\u4e0d\u53ef\u6ee1\u8db3\u5b50\u96c6(MUSes)\u5b8c\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u5b57\u7b26\u4e32\u6c42\u89e3\u5668\u76f8\u6bd4\uff0c\u65b0\u6846\u67b6\u5728\u6bcf\u4e2a\u53d8\u91cf\u5728\u6bcf\u6761\u65b9\u7a0b\u4e2d\u81f3\u591a\u51fa\u73b0\u4e00\u6b21\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u89e3\u51b3\u4e86\u66f4\u591a\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7GNN\u5bf9\u5b57\u65b9\u7a0b\u8fdb\u884c\u667a\u80fd\u6392\u5e8f\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5b57\u65b9\u7a0b\u7ec4\u7684\u6c42\u89e3\u6027\u80fd\uff0c\u5c24\u5176\u5728\u7279\u5b9a\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u5b57\u65b9\u7a0b\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u4f18\u52bf\u3002"}}
{"id": "2506.22783", "pdf": "https://arxiv.org/pdf/2506.22783", "abs": "https://arxiv.org/abs/2506.22783", "authors": ["Oguzhan Baser", "Ahmet Ege Tanriverdi", "Sriram Vishwanath", "Sandeep P. Chinchali"], "title": "PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "5 pages, 3 figures, Published at Proceedings of Interspeech 2025, for\n  the dataset see https://huggingface.co/datasets/phonemefake/PhonemeFakeV2,\n  for the code see https://github.com/UTAustin-SwarmLab/ PhonemeFake", "summary": "Deepfake (DF) attacks pose a growing threat as generative models become\nincreasingly advanced. However, our study reveals that existing DF datasets\nfail to deceive human perception, unlike real DF attacks that influence public\ndiscourse. It highlights the need for more realistic DF attack vectors. We\nintroduce PhonemeFake (PF), a DF attack that manipulates critical speech\nsegments using language reasoning, significantly reducing human perception by\nup to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF\ndataset on HuggingFace and open-source bilevel DF segment detection model that\nadaptively prioritizes compute on manipulated regions. Our extensive\nexperiments across three known DF datasets reveal that our detection model\nreduces EER by 91% while achieving up to 90% speed-up, with minimal compute\noverhead and precise localization beyond existing models as a scalable\nsolution.", "AI": {"tldr": "\u73b0\u6709Deepfake\u6570\u636e\u96c6\u7f3a\u4e4f\u771f\u5b9e\u6027\u3002\u672c\u7814\u7a76\u63d0\u51faPhonemeFake\uff08\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u63a8\u7406\u7684\u771f\u5b9e\u8bed\u97f3\u4f2a\u9020\u653b\u51fb\u65b9\u6cd5\uff09\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5e94\u7684PF\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7cbe\u51c6\u7684Deepfake\u68c0\u6d4b\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u65e5\u76ca\u5148\u8fdb\uff0cDeepfake (DF) \u653b\u51fb\u6784\u6210\u65e5\u76ca\u589e\u957f\u7684\u5a01\u80c1\u3002\u7136\u800c\uff0c\u73b0\u6709DF\u6570\u636e\u96c6\u672a\u80fd\u50cf\u771f\u5b9eDF\u653b\u51fb\u90a3\u6837\u6b3a\u9a97\u4eba\u7c7b\u611f\u77e5\u5e76\u5f71\u54cd\u516c\u4f17\u8206\u8bba\uff0c\u8fd9\u7a81\u51fa\u8868\u660e\u9700\u8981\u66f4\u771f\u5b9e\u7684DF\u653b\u51fb\u5411\u91cf\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86PhonemeFake (PF)\uff0c\u4e00\u79cd\u901a\u8fc7\u8bed\u8a00\u63a8\u7406\u64cd\u7eb5\u5173\u952e\u8bed\u97f3\u7247\u6bb5\u7684DF\u653b\u51fb\u65b9\u6cd5\u3002\u4f5c\u8005\u53d1\u5e03\u4e86\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u7684PF\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u6e90\u4e86\u4e00\u4e2a\u53cc\u5c42DF\u7247\u6bb5\u68c0\u6d4b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u81ea\u9002\u5e94\u5730\u4f18\u5148\u5904\u7406\u88ab\u64cd\u7eb5\u533a\u57df\u7684\u8ba1\u7b97\u3002", "result": "PhonemeFake\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u7c7b\u611f\u77e5\u80fd\u529b\uff08\u9ad8\u8fbe42%\uff09\uff0c\u5e76\u5c06\u57fa\u51c6\u51c6\u786e\u7387\u964d\u4f4e\u4e86\u9ad8\u8fbe94%\u3002\u8be5\u68c0\u6d4b\u6a21\u578b\u5728\u4e09\u4e2a\u5df2\u77e5DF\u6570\u636e\u96c6\u4e0a\u5c06\u7b49\u9519\u8bef\u7387\uff08EER\uff09\u964d\u4f4e\u4e8691%\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8fbe90%\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\uff0c\u4e14\u6bd4\u73b0\u6709\u6a21\u578b\u5177\u5907\u66f4\u7cbe\u786e\u7684\u5b9a\u4f4d\u80fd\u529b\uff0c\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u73b0\u6709Deepfake\u6570\u636e\u96c6\u7684\u975e\u771f\u5b9e\u6027\u662f\u6025\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002PhonemeFake\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5177\u6b3a\u9a97\u6027\u7684\u653b\u51fb\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u3002\u6240\u63d0\u51fa\u7684\u68c0\u6d4b\u6a21\u578b\u4e3aDeepfake\u5a01\u80c1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u5feb\u901f\u3001\u7cbe\u51c6\u4e14\u53ef\u6269\u5c55\u7684\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23139", "pdf": "https://arxiv.org/pdf/2506.23139", "abs": "https://arxiv.org/abs/2506.23139", "authors": ["Prafulla Kumar Choubey", "Xiangyu Peng", "Shilpa Bhagavath", "Kung-Hsiang Huang", "Caiming Xiong", "Chien-Sheng Wu"], "title": "Benchmarking Deep Search over Heterogeneous Enterprise Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a new benchmark for evaluating Deep Search--a realistic and\ncomplex form of retrieval-augmented generation (RAG) that requires\nsource-aware, multi-hop reasoning over diverse, sparsed, but related sources.\nThese include documents, meeting transcripts, Slack messages, GitHub, and URLs,\nwhich vary in structure and often contain human-to-human interactions. We build\nit using a synthetic data pipeline that simulates business workflows across\nproduct planning, development, and support stages, generating interconnected\ncontent with realistic noise and multi-hop questions with guaranteed\nground-truth answers. We release our benchmark with both answerable and\nunanswerable queries, and retrieval pool of 39,190 enterprise artifacts,\nenabling fine-grained evaluation of long-context LLM and RAG systems. Our\nexperiments reveal that even the best-performing agentic RAG methods achieve an\naverage performance score of 32.96 on our benchmark. With further analysis, we\nhighlight retrieval as the main bottleneck: existing methods struggle to\nconduct deep searches and retrieve all necessary evidence. Consequently, they\noften reason over partial context, leading to significant performance\ndegradation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u201c\u6df1\u5ea6\u641c\u7d22\u201d\uff08\u4e00\u79cd\u590d\u6742\u7684RAG\u5f62\u5f0f\uff09\u7684\u65b0\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u8981\u6c42\u5bf9\u591a\u6837\u5316\u7684\u4f01\u4e1a\u6e90\u8fdb\u884c\u591a\u8df3\u63a8\u7406\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709RAG\u65b9\u6cd5\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u68c0\u7d22\u662f\u4e3b\u8981\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u771f\u5b9e\u4e14\u590d\u6742\u7684\u201c\u6df1\u5ea6\u641c\u7d22\u201d\u4efb\u52a1\uff0c\u5373\u9700\u8981\u5bf9\u591a\u6837\u5316\u3001\u7a00\u758f\u4f46\u76f8\u5173\u7684\u6765\u6e90\u8fdb\u884c\u6e90\u611f\u77e5\u3001\u591a\u8df3\u63a8\u7406\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u5176\u5728\u8be5\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5408\u6210\u6570\u636e\u7ba1\u9053\uff0c\u6a21\u62df\u4ea7\u54c1\u89c4\u5212\u3001\u5f00\u53d1\u548c\u652f\u6301\u7b49\u4e1a\u52a1\u5de5\u4f5c\u6d41\uff0c\u751f\u6210\u4e92\u8054\u4e14\u5305\u542b\u771f\u5b9e\u566a\u58f0\u7684\u5185\u5bb9\uff0c\u5e76\u9644\u5e26\u4fdd\u8bc1\u771f\u5b9e\u7b54\u6848\u7684\u591a\u8df3\u95ee\u9898\u3002\u8be5\u57fa\u51c6\u5305\u542b\u53ef\u56de\u7b54\u548c\u4e0d\u53ef\u56de\u7b54\u7684\u67e5\u8be2\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5305\u542b39,190\u4e2a\u4f01\u4e1a\u5de5\u4ef6\u7684\u68c0\u7d22\u6c60\u3002", "result": "1. \u5373\u4f7f\u662f\u8868\u73b0\u6700\u4f73\u7684\u667a\u80fd\u4f53\u5f0fRAG\u65b9\u6cd5\uff0c\u5728\u8be5\u57fa\u51c6\u4e0a\u7684\u5e73\u5747\u6027\u80fd\u5f97\u5206\u4e5f\u4ec5\u4e3a32.96\u3002\n2. \u68c0\u7d22\u88ab\u786e\u5b9a\u4e3a\u4e3b\u8981\u74f6\u9888\uff1a\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u8fdb\u884c\u6df1\u5ea6\u641c\u7d22\u5e76\u68c0\u7d22\u6240\u6709\u5fc5\u8981\u7684\u8bc1\u636e\u3002\n3. \u7531\u4e8e\u68c0\u7d22\u53d7\u9650\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7ecf\u5e38\u5728\u90e8\u5206\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u63a8\u7406\uff0c\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u73b0\u6709RAG\u7cfb\u7edf\u5728\u201c\u6df1\u5ea6\u641c\u7d22\u201d\u4efb\u52a1\u4e2d\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c\u5176\u4e3b\u8981\u6027\u80fd\u74f6\u9888\u5728\u4e8e\u68c0\u7d22\u80fd\u529b\u4e0d\u8db3\uff0c\u65e0\u6cd5\u83b7\u53d6\u5b8c\u6574\u8bc1\u636e\u8fdb\u884c\u591a\u8df3\u63a8\u7406\uff0c\u4ece\u800c\u5bfc\u81f4\u6574\u4f53\u6027\u80fd\u663e\u8457\u53d7\u635f\u3002\u65b0\u57fa\u51c6\u660e\u786e\u63ed\u793a\u4e86\u8fd9\u4e00\u5173\u952e\u5f31\u70b9\u3002"}}
{"id": "2506.22929", "pdf": "https://arxiv.org/pdf/2506.22929", "abs": "https://arxiv.org/abs/2506.22929", "authors": ["Chen Zhang"], "title": "Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration", "categories": ["cs.LG", "cs.AI", "eess.IV", "eess.SP"], "comment": null, "summary": "While deep learning excels in natural image and language processing, its\napplication to high-dimensional data faces computational challenges due to the\ndimensionality curse. Current large-scale data tools focus on business-oriented\ndescriptive statistics, lacking mathematical statistics support for advanced\nanalysis. We propose a parallel computation architecture based on space\ncompleteness, decomposing high-dimensional data into dimension-independent\nstructures for distributed processing. This framework enables seamless\nintegration of data mining and parallel-optimized machine learning methods,\nsupporting scientific computations across diverse data types like medical and\nnatural images within a unified system.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u5b8c\u5907\u6027\u7684\u5e76\u884c\u8ba1\u7b97\u67b6\u6784\uff0c\u4ee5\u514b\u670d\u6df1\u5ea6\u5b66\u4e60\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u7684\u6311\u6218\uff0c\u5e76\u652f\u6301\u7edf\u4e00\u7cfb\u7edf\u4e2d\u7684\u79d1\u5b66\u8ba1\u7b97\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u65f6\u9762\u4e34\u7ef4\u5ea6\u707e\u96be\u7684\u8ba1\u7b97\u6311\u6218\uff1b\u73b0\u6709\u5927\u89c4\u6a21\u6570\u636e\u5de5\u5177\u7f3a\u4e4f\u9ad8\u7ea7\u5206\u6790\u6240\u9700\u7684\u6570\u5b66\u7edf\u8ba1\u652f\u6301\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u5b8c\u5907\u6027\u7684\u5e76\u884c\u8ba1\u7b97\u67b6\u6784\uff0c\u5c06\u9ad8\u7ef4\u6570\u636e\u5206\u89e3\u4e3a\u7ef4\u5ea6\u65e0\u5173\u7684\u7ed3\u6784\u8fdb\u884c\u5206\u5e03\u5f0f\u5904\u7406\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u6570\u636e\u6316\u6398\u548c\u5e76\u884c\u4f18\u5316\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7edf\u4e00\u7cfb\u7edf\u652f\u6301\u5bf9\u533b\u5b66\u56fe\u50cf\u548c\u81ea\u7136\u56fe\u50cf\u7b49\u591a\u79cd\u9ad8\u7ef4\u6570\u636e\u8fdb\u884c\u79d1\u5b66\u8ba1\u7b97\u3002"}}
{"id": "2506.23793", "pdf": "https://arxiv.org/pdf/2506.23793", "abs": "https://arxiv.org/abs/2506.23793", "authors": ["Anton Andreychuk", "Konstantin Yakovlev", "Aleksandr Panov", "Alexey Skrynnik"], "title": "Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot\ntrajectory planning problems, where multiple homogeneous robots simultaneously\nmove in the shared environment. While solving MAPF optimally has been proven to\nbe NP-hard, scalable, and efficient, solvers are vital for real-world\napplications like logistics, search-and-rescue, etc. To this end, decentralized\nsuboptimal MAPF solvers that leverage machine learning have come on stage.\nBuilding on the success of the recently introduced MAPF-GPT, a pure imitation\nlearning solver, we introduce MAPF-GPT-DDG. This novel approach effectively\nfine-tunes the pre-trained MAPF model using centralized expert data. Leveraging\na novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training\nwhile significantly improving performance at test time. Our experiments\ndemonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF\nsolvers, including the original MAPF-GPT, regarding solution quality across\nmany testing scenarios. Remarkably, it can work with MAPF instances involving\nup to 1 million agents in a single environment, setting a new milestone for\nscalability in MAPF domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MAPF-GPT-DDG\uff0c\u4e00\u79cd\u57fa\u4e8e\u5fae\u8c03\u548c\u589e\u91cf\u6570\u636e\u751f\u6210\u673a\u5236\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\u6c42\u89e3\u5668\uff0c\u5b83\u5728\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u53ef\u4f38\u7f29\u6027\u4e0a\u8d85\u8d8a\u4e86\u6240\u6709\u73b0\u6709\u5b66\u4e60\u578bMAPF\u6c42\u89e3\u5668\uff0c\u5e76\u80fd\u5904\u7406\u767e\u4e07\u7ea7\u667a\u80fd\u4f53\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\u662f\u591a\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\u7684\u5173\u952e\u62bd\u8c61\uff0c\u5c3d\u7ba1\u5176\u6700\u4f18\u89e3\u662fNP-\u96be\u7684\uff0c\u4f46\u9ad8\u6548\u4e14\u53ef\u4f38\u7f29\u7684\u6c42\u89e3\u5668\u5bf9\u7269\u6d41\u3001\u641c\u6551\u7b49\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u53bb\u4e2d\u5fc3\u5316\u6b21\u4f18\u6c42\u89e3\u5668\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u3002", "method": "\u672c\u6587\u5728\u7eaf\u6a21\u4eff\u5b66\u4e60\u6c42\u89e3\u5668MAPF-GPT\u7684\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u4e86MAPF-GPT-DDG\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u96c6\u4e2d\u5f0f\u4e13\u5bb6\u6570\u636e\u5bf9\u9884\u8bad\u7ec3\u7684MAPF\u6a21\u578b\u8fdb\u884c\u6709\u6548\u5fae\u8c03\uff0c\u5e76\u5229\u7528\u4e00\u79cd\u65b0\u9896\u7684\u589e\u91cf\u6570\u636e\u751f\u6210\uff08delta-data generation\uff09\u673a\u5236\uff0c\u4ee5\u52a0\u901f\u8bad\u7ec3\u5e76\u663e\u8457\u63d0\u9ad8\u6d4b\u8bd5\u65f6\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cMAPF-GPT-DDG\u5728\u591a\u79cd\u6d4b\u8bd5\u573a\u666f\u4e0b\uff0c\u5176\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u8d85\u8d8a\u4e86\u5305\u62ec\u539f\u59cbMAPF-GPT\u5728\u5185\u7684\u6240\u6709\u73b0\u6709\u5b66\u4e60\u578bMAPF\u6c42\u89e3\u5668\u3002\u5c24\u5176\u503c\u5f97\u4e00\u63d0\u7684\u662f\uff0c\u5b83\u80fd\u591f\u5904\u7406\u5355\u73af\u5883\u4e2d\u591a\u8fbe100\u4e07\u4e2a\u667a\u80fd\u4f53\u7684MAPF\u5b9e\u4f8b\uff0c\u4e3aMAPF\u9886\u57df\u7684\u53ef\u4f38\u7f29\u6027\u8bbe\u5b9a\u4e86\u65b0\u7684\u91cc\u7a0b\u7891\u3002", "conclusion": "MAPF-GPT-DDG\u901a\u8fc7\u521b\u65b0\u7684\u5fae\u8c03\u548c\u589e\u91cf\u6570\u636e\u751f\u6210\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u95ee\u9898\u7684\u6c42\u89e3\u8d28\u91cf\u548c\u53ef\u4f38\u7f29\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u5f3a\u5927\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22784", "pdf": "https://arxiv.org/pdf/2506.22784", "abs": "https://arxiv.org/abs/2506.22784", "authors": ["Yu Han", "Zhiwei Huang", "Yanting Zhang", "Fangjun Ding", "Shen Cai", "Rui Fan"], "title": "Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Point-pixel registration between LiDAR point clouds and camera images is a\nfundamental yet challenging task in autonomous driving and robotic perception.\nA key difficulty lies in the modality gap between unstructured point clouds and\nstructured images, especially under sparse single-frame LiDAR settings.\nExisting methods typically extract features separately from point clouds and\nimages, then rely on hand-crafted or learned matching strategies. This separate\nencoding fails to bridge the modality gap effectively, and more critically,\nthese methods struggle with the sparsity and noise of single-frame LiDAR, often\nrequiring point cloud accumulation or additional priors to improve reliability.\nInspired by recent progress in detector-free matching paradigms (e.g.\nMatchAnything), we revisit the projection-based approach and introduce the\ndetector-free framework for direct point-pixel matching between LiDAR and\ncamera views. Specifically, we project the LiDAR intensity map into a 2D view\nfrom the LiDAR perspective and feed it into an attention-based detector-free\nmatching network, enabling cross-modal correspondence estimation without\nrelying on multi-frame accumulation. To further enhance matching reliability,\nwe introduce a repeatability scoring mechanism that acts as a soft visibility\nprior. This guides the network to suppress unreliable matches in regions with\nlow intensity variation, improving robustness under sparse input. Extensive\nexperiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that\nour method achieves state-of-the-art performance, outperforming prior\napproaches on nuScenes (even those relying on accumulated point clouds),\ndespite using only single-frame LiDAR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u65e0\u68c0\u6d4b\u5668\uff08detector-free\uff09\u7684\u6295\u5f71\u5f0f\u6846\u67b6\uff0c\u7528\u4e8e\u7a00\u758f\u5355\u5e27\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u4e0e\u76f8\u673a\u56fe\u50cf\u7684\u70b9-\u50cf\u7d20\u914d\u51c6\uff0c\u901a\u8fc7\u5c06\u6fc0\u5149\u96f7\u8fbe\u5f3a\u5ea6\u56fe\u6295\u5f71\u52302D\u89c6\u56fe\u5e76\u5229\u7528\u6ce8\u610f\u529b\u5339\u914d\u7f51\u7edc\uff0c\u8f85\u4ee5\u53ef\u91cd\u590d\u6027\u8bc4\u5206\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u4e0e\u76f8\u673a\u56fe\u50cf\u7684\u70b9-\u50cf\u7d20\u914d\u51c6\u662f\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u611f\u77e5\u4e2d\u7684\u57fa\u7840\u4e14\u5177\u6311\u6218\u6027\u4efb\u52a1\u3002\u4e3b\u8981\u96be\u70b9\u5728\u4e8e\u975e\u7ed3\u6784\u5316\u70b9\u4e91\u548c\u7ed3\u6784\u5316\u56fe\u50cf\u4e4b\u95f4\u7684\u6a21\u6001\u9e3f\u6c9f\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u7684\u5355\u5e27\u6fc0\u5149\u96f7\u8fbe\u8bbe\u7f6e\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u56e0\u5355\u72ec\u63d0\u53d6\u7279\u5f81\u548c\u5339\u914d\u7b56\u7565\uff0c\u672a\u80fd\u6709\u6548\u5f25\u5408\u6a21\u6001\u9e3f\u6c9f\uff0c\u5e76\u4e14\u96be\u4ee5\u5e94\u5bf9\u5355\u5e27\u6fc0\u5149\u96f7\u8fbe\u7684\u7a00\u758f\u6027\u548c\u566a\u58f0\u3002", "method": "\u53d7\u65e0\u68c0\u6d4b\u5668\u5339\u914d\u8303\u5f0f\u7684\u542f\u53d1\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6295\u5f71\u7684\u65e0\u68c0\u6d4b\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u6fc0\u5149\u96f7\u8fbe\u548c\u76f8\u673a\u89c6\u56fe\u95f4\u7684\u76f4\u63a5\u70b9-\u50cf\u7d20\u5339\u914d\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5c06\u6fc0\u5149\u96f7\u8fbe\u5f3a\u5ea6\u56fe\u6295\u5f71\u52302D\u89c6\u56fe\uff0c\u5e76\u8f93\u5165\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u65e0\u68c0\u6d4b\u5668\u5339\u914d\u7f51\u7edc\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u5e94\u4f30\u8ba1\uff0c\u65e0\u9700\u591a\u5e27\u7d2f\u79ef\u3002\u6b64\u5916\uff0c\u5f15\u5165\u53ef\u91cd\u590d\u6027\u8bc4\u5206\u673a\u5236\u4f5c\u4e3a\u8f6f\u53ef\u89c1\u6027\u5148\u9a8c\uff0c\u6307\u5bfc\u7f51\u7edc\u6291\u5236\u4f4e\u5f3a\u5ea6\u53d8\u5316\u533a\u57df\u7684\u4e0d\u53ef\u9760\u5339\u914d\uff0c\u589e\u5f3a\u7a00\u758f\u8f93\u5165\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728KITTI\u3001nuScenes\u548cMIAS-LCEC-TF70\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728nuScenes\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff08\u5305\u62ec\u90a3\u4e9b\u4f9d\u8d56\u7d2f\u79ef\u70b9\u4e91\u7684\u65b9\u6cd5\uff09\uff0c\u5c3d\u7ba1\u53ea\u4f7f\u7528\u4e86\u5355\u5e27\u6fc0\u5149\u96f7\u8fbe\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5355\u5e27\u7a00\u758f\u6fc0\u5149\u96f7\u8fbe\u4e0e\u76f8\u673a\u56fe\u50cf\u914d\u51c6\u7684\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u65e0\u68c0\u6d4b\u5668\u6846\u67b6\u548c\u9c81\u68d2\u6027\u589e\u5f3a\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u4e14\u9886\u5148\u7684\u914d\u51c6\u6027\u80fd\uff0c\u65e0\u9700\u4f9d\u8d56\u70b9\u4e91\u7d2f\u79ef\u3002"}}
{"id": "2506.23146", "pdf": "https://arxiv.org/pdf/2506.23146", "abs": "https://arxiv.org/abs/2506.23146", "authors": ["Dingzriui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions", "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLCS\uff08Learning-to-Context Slope\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u578b\u5ea6\u91cf\u6807\u51c6\uff0c\u65e8\u5728\u66f4\u53ef\u9760\u3001\u53ef\u5f52\u56e0\u4e14\u6570\u636e\u9ad8\u6548\u5730\u8bc4\u4f30\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u6548\u679c\u6ce2\u52a8\u5927\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u6027\u80fd\u53d8\u5316\uff0c\u5b58\u5728\u53ef\u9760\u6027\u4f4e\u3001\u5f52\u56e0\u6027\u5dee\u53ca\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u4e0d\u5b9e\u7528\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u96be\u4ee5\u5224\u65adICL\u4f55\u65f6\u80fd\u53ef\u9760\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u201c\u5b66\u4e60\u5230\u4e0a\u4e0b\u6587\u659c\u7387\u201d\uff08Learning-to-Context Slope, LCS\uff09\u8fd9\u4e00\u65b0\u6307\u6807\uff0c\u901a\u8fc7\u5efa\u6a21\u5b66\u4e60\u589e\u76ca\uff08\u6f14\u793a\u5e26\u6765\u7684\u635f\u5931\u51cf\u5c11\uff09\u4e0e\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff08\u6f14\u793a\u4e0e\u8f93\u5165\u7684\u76f8\u5173\u6027\uff09\u4e4b\u95f4\u7684\u659c\u7387\u6765\u91cf\u5316ICL\u6709\u6548\u6027\u3002LCS\u80fd\u6355\u6349\u8fde\u7eed\u635f\u5931\u53d8\u5316\u3001\u5c06ICL\u5931\u8d25\u5f52\u56e0\u4e8e\u4e0a\u4e0b\u6587\u5bf9\u9f50\u6216\u8f93\u51fa\u6821\u51c6\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u8bc4\u4f30\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0cLCS\u4e0e\u6807\u6ce8\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u63d0\u5347\u5f3a\u76f8\u5173\uff0c\u5e76\u5728\u504f\u7f6e\u6216\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u80fd\u53ef\u9760\u53cd\u6620\u771f\u5b9e\u6709\u6548\u6027\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u63ed\u793a\u4e86LCS\u7684\u53ef\u64cd\u4f5c\u9608\u503c\uff0c\u5e76\u8bc6\u522b\u4e86\u5bf9ICL\u6210\u529f\u81f3\u5173\u91cd\u8981\u7684\u6a21\u578b\u80fd\u529b\u3002", "conclusion": "LCS\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u53ef\u9760\u3001\u53ef\u5f52\u56e0\u4e14\u6570\u636e\u9ad8\u6548\u7684ICL\u6709\u6548\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u4ece\u4e1a\u8005\u5224\u65adICL\u4f55\u65f6\u80fd\u53ef\u9760\u63d0\u5347LLM\u6027\u80fd\uff0c\u5e76\u6df1\u5165\u7406\u89e3ICL\u6210\u529f\u6240\u9700\u7684\u5173\u952e\u6a21\u578b\u80fd\u529b\u3002"}}
{"id": "2506.22950", "pdf": "https://arxiv.org/pdf/2506.22950", "abs": "https://arxiv.org/abs/2506.22950", "authors": ["Liangyu Wang", "Huanyi Xie", "Xinhai Wang", "Tianjin Huang", "Mengdi Li", "Di Wang"], "title": "Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Group-based reinforcement learning algorithms such as Group Reward Policy\nOptimization (GRPO) have proven effective for fine-tuning large language models\n(LLMs) with human feedback. However, generating and storing multiple responses\nper prompt incurs substantial memory overhead, especially as the sample group\nsize increases, limiting scalability under constrained hardware.\n  We propose Infinite Sampling, a framework that enables efficient and stable\nGRPO training by decoupling group size from GPU memory usage. It consists of:\n(1) micro sampling groups that decompose large groups into memory-feasible\nrounds; (2) continuous sampling that interleaves generation across groups to\nimprove utilization; and (3) a length-aware scheduler combining\ntoken-conditioned sequence length prediction with a two-stage plan: global\ngrouping via FPTAS and runtime refill via SJF.\n  Experiments show that our Micro Sampling Groups reduce peak memory usage by\nover 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on\nQwen3-1.7B). Building on this, Infinite Sampling improves throughput by over\n25% compared to the naive micro sampling group method, reducing decoding steps\nwhile maintaining full-length completions and memory usage. Our hybrid\nscheduling ensures efficient and stable GRPO training with larger groups under\nrealistic GPU memory constraints.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u57fa\u4e8e\u7ec4\u7684\u5f3a\u5316\u5b66\u4e60\uff08\u5982GRPO\uff09\u5728LLM\u5fae\u8c03\u4e2d\u9762\u4e34\u7684\u5185\u5b58\u5f00\u9500\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faInfinite Sampling\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5fae\u91c7\u6837\u7ec4\u3001\u8fde\u7eed\u91c7\u6837\u548c\u957f\u5ea6\u611f\u77e5\u8c03\u5ea6\uff0c\u6709\u6548\u964d\u4f4e\u5185\u5b58\u5360\u7528\u5e76\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\uff0c\u5b9e\u73b0\u5728\u6709\u9650\u786c\u4ef6\u8d44\u6e90\u4e0b\u9ad8\u6548\u7a33\u5b9a\u7684GRPO\u8bad\u7ec3\u3002", "motivation": "\u57fa\u4e8e\u7ec4\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982GRPO\uff09\u5728\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u4e3a\u6bcf\u4e2a\u63d0\u793a\u751f\u6210\u548c\u5b58\u50a8\u591a\u4e2a\u54cd\u5e94\u4f1a\u4ea7\u751f\u5927\u91cf\u5185\u5b58\u5f00\u9500\uff0c\u5c24\u5176\u5f53\u91c7\u6837\u7ec4\u89c4\u6a21\u589e\u5927\u65f6\uff0c\u8fd9\u5728\u786c\u4ef6\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u4e25\u91cd\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Infinite Sampling\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u5c06\u7ec4\u5927\u5c0f\u4e0eGPU\u5185\u5b58\u4f7f\u7528\u89e3\u8026\uff0c\u5b9e\u73b0\u9ad8\u6548\u7a33\u5b9a\u7684GRPO\u8bad\u7ec3\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\uff1a\n1.  \u5fae\u91c7\u6837\u7ec4\uff1a\u5c06\u5927\u578b\u7ec4\u5206\u89e3\u4e3a\u5185\u5b58\u53ef\u884c\u7684\u8f6e\u6b21\u8fdb\u884c\u5904\u7406\u3002\n2.  \u8fde\u7eed\u91c7\u6837\uff1a\u5728\u4e0d\u540c\u7ec4\u4e4b\u95f4\u4ea4\u9519\u751f\u6210\u8fc7\u7a0b\uff0c\u4ee5\u63d0\u9ad8GPU\u5229\u7528\u7387\u3002\n3.  \u957f\u5ea6\u611f\u77e5\u8c03\u5ea6\u5668\uff1a\u7ed3\u5408\u57fa\u4e8eToken\u7684\u5e8f\u5217\u957f\u5ea6\u9884\u6d4b\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u8ba1\u5212\uff08\u901a\u8fc7FPTAS\u8fdb\u884c\u5168\u5c40\u5206\u7ec4\u548c\u901a\u8fc7SJF\u8fdb\u884c\u8fd0\u884c\u65f6\u586b\u5145\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\n1.  \u5fae\u91c7\u6837\u7ec4\u76f8\u8f83\u4e8e\u5168\u7ec4\u89e3\u7801\uff0c\u53ef\u5c06\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c1150%\u4ee5\u4e0a\uff08\u4f8b\u5982\uff0c\u5728Qwen3-1.7B\u6a21\u578b\u4e0a\u4ece21.55 GB\u964d\u81f310.64 GB\uff09\u3002\n2.  Infinite Sampling\u6846\u67b6\u76f8\u8f83\u4e8e\u6734\u7d20\u5fae\u91c7\u6837\u7ec4\u65b9\u6cd5\uff0c\u541e\u5410\u91cf\u63d0\u9ad8\u4e8625%\u4ee5\u4e0a\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u89e3\u7801\u6b65\u9aa4\uff0c\u5e76\u4fdd\u6301\u4e86\u5b8c\u6574\u957f\u5ea6\u7684\u751f\u6210\u7ed3\u679c\u548c\u8f83\u4f4e\u7684\u5185\u5b58\u4f7f\u7528\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u8c03\u5ea6\u65b9\u6cd5\u786e\u4fdd\u4e86\u5728\u5b9e\u9645GPU\u5185\u5b58\u9650\u5236\u4e0b\uff0c\u80fd\u591f\u4f7f\u7528\u66f4\u5927\u7684\u7ec4\u8fdb\u884c\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684GRPO\u8bad\u7ec3\u3002"}}
{"id": "2506.23844", "pdf": "https://arxiv.org/pdf/2506.23844", "abs": "https://arxiv.org/abs/2506.23844", "authors": ["Hang Su", "Jun Luo", "Chang Liu", "Xiao Yang", "Yichi Zhang", "Yinpeng Dong", "Jun Zhu"], "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents", "categories": ["cs.AI"], "comment": "18 pages", "summary": "Recent advances in large language models (LLMs) have catalyzed the rise of\nautonomous AI agents capable of perceiving, reasoning, and acting in dynamic,\nopen-ended environments. These large-model agents mark a paradigm shift from\nstatic inference systems to interactive, memory-augmented entities. While these\ncapabilities significantly expand the functional scope of AI, they also\nintroduce qualitatively novel security risks - such as memory poisoning, tool\nmisuse, reward hacking, and emergent misalignment - that extend beyond the\nthreat models of conventional systems or standalone LLMs. In this survey, we\nfirst examine the structural foundations and key capabilities that underpin\nincreasing levels of agent autonomy, including long-term memory retention,\nmodular tool use, recursive planning, and reflective reasoning. We then analyze\nthe corresponding security vulnerabilities across the agent stack, identifying\nfailure modes such as deferred decision hazards, irreversible tool chains, and\ndeceptive behaviors arising from internal state drift or value misalignment.\nThese risks are traced to architectural fragilities that emerge across\nperception, cognition, memory, and action modules. To address these challenges,\nwe systematically review recent defense strategies deployed at different\nautonomy layers, including input sanitization, memory lifecycle control,\nconstrained decision-making, structured tool invocation, and introspective\nreflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a\nunified cognitive framework grounded in Constrained Markov Decision Processes\n(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,\nand joint reward-risk optimization to enable principled, proactive safety\nacross the agent's decision-making loop.", "AI": {"tldr": "LLM\u9a71\u52a8\u7684\u81ea\u4e3bAI\u667a\u80fd\u4f53\u9762\u4e34\u65b0\u578b\u5b89\u5168\u98ce\u9669\u3002\u672c\u6587\u7efc\u8ff0\u5176\u80fd\u529b\u3001\u6f0f\u6d1e\u53ca\u9632\u5fa1\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eCMDPs\u7684\u98ce\u9669\u611f\u77e5\u667a\u80fd\u4f53\u67b6\u6784R2A2\uff0c\u4ee5\u5b9e\u73b0\u4e3b\u52a8\u5b89\u5168\u3002", "motivation": "\u968f\u7740LLM\u9a71\u52a8\u7684\u81ea\u4e3bAI\u667a\u80fd\u4f53\u5d1b\u8d77\uff0c\u5b83\u4eec\u5f15\u5165\u4e86\u8d85\u8d8a\u4f20\u7edf\u5a01\u80c1\u6a21\u578b\u7684\u65b0\u578b\u5b89\u5168\u98ce\u9669\uff0c\u5982\u8bb0\u5fc6\u4e2d\u6bd2\u3001\u5de5\u5177\u6ee5\u7528\u3001\u5956\u52b1\u64cd\u7eb5\u548c\u6d8c\u73b0\u9519\u4f4d\uff0c\u6025\u9700\u5bf9\u5176\u8fdb\u884c\u8bc6\u522b\u3001\u5206\u6790\u5e76\u63d0\u4f9b\u9632\u5fa1\u7b56\u7565\u3002", "method": "\u672c\u6587\u9996\u5148\u8003\u5bdf\u4e86\u667a\u80fd\u4f53\u7684\u80fd\u529b\u548c\u7ed3\u6784\u57fa\u7840\uff1b\u63a5\u7740\u5206\u6790\u4e86\u667a\u80fd\u4f53\u5806\u6808\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u53ca\u5176\u67b6\u6784\u8106\u5f31\u6027\uff1b\u7136\u540e\u7cfb\u7edf\u56de\u987e\u4e86\u73b0\u6709\u9632\u5fa1\u7b56\u7565\uff1b\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08CMDPs\uff09\u7684\u7edf\u4e00\u8ba4\u77e5\u6846\u67b6\u2014\u2014\u53cd\u5c04\u6027\u98ce\u9669\u611f\u77e5\u667a\u80fd\u4f53\u67b6\u6784\uff08R2A2\uff09\uff0c\u8be5\u67b6\u6784\u6574\u5408\u4e86\u98ce\u9669\u611f\u77e5\u4e16\u754c\u5efa\u6a21\u3001\u5143\u7b56\u7565\u9002\u5e94\u548c\u8054\u5408\u5956\u52b1-\u98ce\u9669\u4f18\u5316\uff0c\u65e8\u5728\u5b9e\u73b0\u4e3b\u52a8\u5b89\u5168\u3002", "result": "\u672c\u6587\u8bc6\u522b\u5e76\u5206\u6790\u4e86LLM\u9a71\u52a8\u667a\u80fd\u4f53\u6240\u5e26\u6765\u7684\u65b0\u578b\u5b89\u5168\u98ce\u9669\u53ca\u5176\u67b6\u6784\u8106\u5f31\u6027\uff1b\u7cfb\u7edf\u56de\u987e\u4e86\u73b0\u6709\u7684\u9632\u5fa1\u7b56\u7565\uff1b\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aR2A2\u7684\u65b0\u578b\u98ce\u9669\u611f\u77e5\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u65e8\u5728\u901a\u8fc7\u6574\u5408\u98ce\u9669\u611f\u77e5\u4e16\u754c\u5efa\u6a21\u3001\u5143\u7b56\u7565\u9002\u5e94\u548c\u8054\u5408\u5956\u52b1-\u98ce\u9669\u4f18\u5316\uff0c\u5b9e\u73b0\u667a\u80fd\u4f53\u51b3\u7b56\u5faa\u73af\u4e2d\u7684\u539f\u5219\u6027\u3001\u4e3b\u52a8\u6027\u5b89\u5168\u4fdd\u969c\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u81ea\u4e3bAI\u667a\u80fd\u4f53\u5e26\u6765\u65b0\u9896\u4e14\u590d\u6742\u7684\u5b89\u5168\u6311\u6218\u3002\u672c\u7814\u7a76\u901a\u8fc7\u6df1\u5165\u5206\u6790\u8fd9\u4e9b\u98ce\u9669\u3001\u56de\u987e\u9632\u5fa1\u7b56\u7565\uff0c\u5e76\u63d0\u51faR2A2\u67b6\u6784\uff0c\u4e3a\u6784\u5efa\u5177\u5907\u539f\u5219\u6027\u548c\u4e3b\u52a8\u5b89\u5168\u80fd\u529b\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22800", "pdf": "https://arxiv.org/pdf/2506.22800", "abs": "https://arxiv.org/abs/2506.22800", "authors": ["Sicong Du", "Jiarun Liu", "Qifeng Chen", "Hao-Xiang Chen", "Tai-Jiang Mu", "Sheng Yang"], "title": "RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "A single-pass driving clip frequently results in incomplete scanning of the\nroad structure, making reconstructed scene expanding a critical requirement for\nsensor simulators to effectively regress driving actions. Although contemporary\n3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction\nquality, their direct extension through the integration of diffusion priors\noften introduces cumulative physical inconsistencies and compromises training\nefficiency. To address these limitations, we present RGE-GS, a novel expansive\nreconstruction framework that synergizes diffusion-based generation with\nreward-guided Gaussian integration. The RGE-GS framework incorporates two key\ninnovations: First, we propose a reward network that learns to identify and\nprioritize consistently generated patterns prior to reconstruction phases,\nthereby enabling selective retention of diffusion outputs for spatial\nstability. Second, during the reconstruction process, we devise a\ndifferentiated training strategy that automatically adjust Gaussian\noptimization progress according to scene converge metrics, which achieving\nbetter convergence than baseline methods. Extensive evaluations of publicly\navailable datasets demonstrate that RGE-GS achieves state-of-the-art\nperformance in reconstruction quality. Our source-code will be made publicly\navailable at https://github.com/CN-ADLab/RGE-GS. (Camera-ready version\nincorporating reviewer suggestions will be updated soon.)", "AI": {"tldr": "RGE-GS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6269\u5c55\u91cd\u5efa\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5355\u6b21\u626b\u63cf\u5bfc\u81f4\u7684\u8def\u51b5\u4e0d\u5b8c\u6574\u95ee\u9898\u3002\u5b83\u7ed3\u5408\u6269\u6563\u751f\u6210\u4e0e\u5956\u52b1\u5f15\u5bfc\u7684\u9ad8\u65af\u96c6\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u67093DGS\u65b9\u6cd5\u5728\u6269\u5c55\u65f6\u5f15\u5165\u7684\u4e0d\u4e00\u81f4\u6027\u548c\u8bad\u7ec3\u6548\u7387\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u9886\u5148\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u5355\u6b21\u9a7e\u9a76\u89c6\u9891\u901a\u5e38\u5bfc\u81f4\u8def\u51b5\u626b\u63cf\u4e0d\u5b8c\u6574\uff0c\u8fd9\u4f7f\u5f97\u573a\u666f\u6269\u5c55\u5bf9\u4e8e\u4f20\u611f\u5668\u6a21\u62df\u5668\u6709\u6548\u56de\u5f52\u9a7e\u9a76\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5f53\u4ee33DGS\u6280\u672f\u80fd\u5b9e\u73b0\u51fa\u8272\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u4f46\u5176\u76f4\u63a5\u6574\u5408\u6269\u6563\u5148\u9a8c\u8fdb\u884c\u6269\u5c55\u4f1a\u5f15\u5165\u7d2f\u79ef\u7684\u7269\u7406\u4e0d\u4e00\u81f4\u6027\u5e76\u964d\u4f4e\u8bad\u7ec3\u6548\u7387\u3002", "method": "RGE-GS\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u548c\u5956\u52b1\u5f15\u5bfc\u7684\u9ad8\u65af\u96c6\u6210\u3002\u5b83\u5305\u542b\u4e24\u5927\u521b\u65b0\u70b9\uff1a1\uff09\u63d0\u51fa\u4e00\u4e2a\u5956\u52b1\u7f51\u7edc\uff0c\u5728\u91cd\u5efa\u524d\u8bc6\u522b\u5e76\u4f18\u5148\u5904\u7406\u4e00\u81f4\u751f\u6210\u7684\u6a21\u5f0f\uff0c\u4ece\u800c\u9009\u62e9\u6027\u4fdd\u7559\u6269\u6563\u8f93\u51fa\u4ee5\u786e\u4fdd\u7a7a\u95f4\u7a33\u5b9a\u6027\u30022\uff09\u8bbe\u8ba1\u4e00\u4e2a\u5dee\u5f02\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u6839\u636e\u573a\u666f\u6536\u655b\u6307\u6807\u81ea\u52a8\u8c03\u6574\u9ad8\u65af\u4f18\u5316\u8fdb\u5ea6\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cRGE-GS\u5728\u91cd\u5efa\u8d28\u91cf\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "RGE-GS\u901a\u8fc7\u51cf\u8f7b\u4e0d\u4e00\u81f4\u6027\u5e76\u63d0\u9ad8\u6548\u7387\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9a7e\u9a76\u6a21\u62df\u4e2d\u4e0d\u5b8c\u6574\u573a\u666f\u91cd\u5efa\u548c\u6269\u5c55\u7684\u6311\u6218\uff0c\u5e76\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2506.23149", "pdf": "https://arxiv.org/pdf/2506.23149", "abs": "https://arxiv.org/abs/2506.23149", "authors": ["Dingzirui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "title": "V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy", "categories": ["cs.CL"], "comment": null, "summary": "High labeling cost for in-context learning (ICL) demonstrations motivates\nusing large language models (LLMs) for synthesis to reduce overhead. However,\nexisting synthesis methods are mainly task-specific or rely on pre-existing\ndemonstrations. So this paper focuses on synthesizing demonstrations from\nscratch for arbitrary tasks. A major challenge in synthesizing from scratch is\nensuring consistency with the target task, as the lack of labeling guidance\ncould lead to synthesis bias. We first propose a consistency metric called\nV-Score, which has higher performance and lower computation cost compared with\nthe metrics based on grams or embedding vectors. Furthermore, we introduce\nV-Synthesis, which leverages V-Score for proportional sampling to ensure both\nhigh consistency and diversity of synthesized demonstrations. Experimental\nresults demonstrate that V-Synthesis yields an average performance improvement\nof 2.0% compared to existing synthesis methods confirming the effectiveness of\nV-Synthesis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faV-Score\u4e00\u81f4\u6027\u5ea6\u91cf\u548cV-Synthesis\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u793a\u4f8b\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u53ca\u73b0\u6709\u5408\u6210\u65b9\u6cd5\u5c40\u9650\u6027\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u53ef\u4ece\u96f6\u5f00\u59cb\u4e3a\u4efb\u610f\u4efb\u52a1\u5408\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684ICL\u793a\u4f8b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e862.0%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u793a\u4f8b\u7684\u9ad8\u6602\u6807\u6ce8\u6210\u672c\uff1b\u73b0\u6709\u793a\u4f8b\u5408\u6210\u65b9\u6cd5\u591a\u4e3a\u4efb\u52a1\u7279\u5b9a\u6216\u4f9d\u8d56\u5df2\u6709\u793a\u4f8b\uff0c\u96be\u4ee5\u4ece\u96f6\u5f00\u59cb\u4e3a\u4efb\u610f\u4efb\u52a1\u5408\u6210\uff1b\u4ece\u96f6\u5408\u6210\u65f6\uff0c\u7f3a\u4e4f\u6807\u6ce8\u6307\u5bfc\u53ef\u80fd\u5bfc\u81f4\u5408\u6210\u504f\u5dee\uff0c\u96be\u4ee5\u4fdd\u8bc1\u4e0e\u76ee\u6807\u4efb\u52a1\u7684\u4e00\u81f4\u6027\u3002", "method": "1. \u63d0\u51fa\u4e00\u79cd\u540d\u4e3aV-Score\u7684\u6587\u672c\u4e00\u81f4\u6027\u5ea6\u91cf\uff0c\u5176\u6027\u80fd\u66f4\u9ad8\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u4e8e\u57fa\u4e8e\u8bcd\u6216\u5d4c\u5165\u5411\u91cf\u7684\u5ea6\u91cf\u30022. \u5f15\u5165V-Synthesis\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528V-Score\u8fdb\u884c\u6bd4\u4f8b\u62bd\u6837\uff0c\u4ee5\u786e\u4fdd\u5408\u6210\u793a\u4f8b\u7684\u9ad8\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cV-Synthesis\u76f8\u8f83\u4e8e\u73b0\u6709\u5408\u6210\u65b9\u6cd5\u5e73\u5747\u6027\u80fd\u63d0\u53472.0%\u3002V-Score\u6bd4\u73b0\u6709\u57fa\u4e8egrams\u6216\u5d4c\u5165\u5411\u91cf\u7684\u5ea6\u91cf\u5177\u6709\u66f4\u9ad8\u6027\u80fd\u548c\u66f4\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "V-Synthesis\u662f\u4e00\u79cd\u6709\u6548\u7684\u793a\u4f8b\u5408\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5e76\u6210\u529f\u89e3\u51b3\u4e86\u4ece\u96f6\u5f00\u59cb\u5408\u6210\u793a\u4f8b\u65f6\u9762\u4e34\u7684\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u6311\u6218\u3002"}}
{"id": "2506.22984", "pdf": "https://arxiv.org/pdf/2506.22984", "abs": "https://arxiv.org/abs/2506.22984", "authors": ["Prathyush Kumar Reddy Lebaku", "Lu Gao", "Yunpeng Zhang", "Zhixia Li", "Yongxin Liu", "Tanvir Arafin"], "title": "Cybersecurity-Focused Anomaly Detection in Connected Autonomous Vehicles Using Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "Anomaly detection in connected autonomous vehicles (CAVs) is crucial for\nmaintaining safe and reliable transportation networks, as CAVs can be\nsusceptible to sensor malfunctions, cyber-attacks, and unexpected environmental\ndisruptions. This study explores an anomaly detection approach by simulating\nvehicle behavior, generating a dataset that represents typical and atypical\nvehicular interactions. The dataset includes time-series data of position,\nspeed, and acceleration for multiple connected autonomous vehicles. We utilized\nmachine learning models to effectively identify abnormal driving patterns.\nFirst, we applied a stacked Long Short-Term Memory (LSTM) model to capture\ntemporal dependencies and sequence-based anomalies. The stacked LSTM model\nprocessed the sequential data to learn standard driving behaviors.\nAdditionally, we deployed a Random Forest model to support anomaly detection by\noffering ensemble-based predictions, which enhanced model interpretability and\nperformance. The Random Forest model achieved an R2 of 0.9830, MAE of 5.746,\nand a 95th percentile anomaly threshold of 14.18, while the stacked LSTM model\nattained an R2 of 0.9998, MAE of 82.425, and a 95th percentile anomaly\nthreshold of 265.63. These results demonstrate the models' effectiveness in\naccurately predicting vehicle trajectories and detecting anomalies in\nautonomous driving scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6a21\u62df\u8f66\u8f86\u884c\u4e3a\u751f\u6210\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u5806\u53e0LSTM\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u4e92\u8054\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08CAVs\uff09\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u8f68\u8ff9\u9884\u6d4b\uff0c\u786e\u4fdd\u4ea4\u901a\u5b89\u5168\u3002", "motivation": "\u4e92\u8054\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08CAVs\uff09\u6613\u53d7\u4f20\u611f\u5668\u6545\u969c\u3001\u7f51\u7edc\u653b\u51fb\u548c\u73af\u5883\u5e72\u6270\u5f71\u54cd\uff0c\u56e0\u6b64\u5176\u5f02\u5e38\u68c0\u6d4b\u5bf9\u7ef4\u62a4\u4ea4\u901a\u7f51\u7edc\u7684\u5b89\u5168\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6a21\u62df\u8f66\u8f86\u884c\u4e3a\u751f\u6210\u5305\u542b\u6b63\u5e38\u548c\u5f02\u5e38\u4ea4\u4e92\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\uff08\u4f4d\u7f6e\u3001\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\uff09\u3002\u91c7\u7528\u5806\u53e0\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08LSTM\uff09\u6a21\u578b\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u90e8\u7f72\u968f\u673a\u68ee\u6797\uff08Random Forest\uff09\u6a21\u578b\u8fdb\u884c\u96c6\u6210\u9884\u6d4b\uff0c\u4ee5\u8bc6\u522b\u5f02\u5e38\u9a7e\u9a76\u6a21\u5f0f\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u4e3aR2=0.9830\uff0cMAE=5.746\uff0c95%\u5f02\u5e38\u9608\u503c=14.18\u3002\u5806\u53e0LSTM\u6a21\u578b\u8868\u73b0\u4e3aR2=0.9998\uff0cMAE=82.425\uff0c95%\u5f02\u5e38\u9608\u503c=265.63\u3002\u7ed3\u679c\u8868\u660e\u6a21\u578b\u5728\u9884\u6d4b\u8f66\u8f86\u8f68\u8ff9\u548c\u68c0\u6d4b\u81ea\u52a8\u9a7e\u9a76\u5f02\u5e38\u65b9\u9762\u5747\u6709\u6548\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u6240\u4f7f\u7528\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5806\u53e0LSTM\u548c\u968f\u673a\u68ee\u6797\uff09\u80fd\u6709\u6548\u4e14\u51c6\u786e\u5730\u9884\u6d4b\u8f66\u8f86\u8f68\u8ff9\u5e76\u68c0\u6d4b\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5f02\u5e38\uff0c\u4e3a\u4e92\u8054\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u8fd0\u884c\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2506.23908", "pdf": "https://arxiv.org/pdf/2506.23908", "abs": "https://arxiv.org/abs/2506.23908", "authors": ["Andr\u00e1s Gy\u00f6rgy", "Tor Lattimore", "Nevena Lazi\u0107", "Csaba Szepesv\u00e1ri"], "title": "Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Sound deductive reasoning -- the ability to derive new knowledge from\nexisting facts and rules -- is an indisputably desirable aspect of general\nintelligence. Despite the major advances of AI systems in areas such as math\nand science, especially since the introduction of transformer architectures, it\nis well-documented that even the most advanced frontier systems regularly and\nconsistently falter on easily-solvable deductive reasoning tasks. Hence, these\nsystems are unfit to fulfill the dream of achieving artificial general\nintelligence capable of sound deductive reasoning. We argue that their unsound\nbehavior is a consequence of the statistical learning approach powering their\ndevelopment. To overcome this, we contend that to achieve reliable deductive\nreasoning in learning-based AI systems, researchers must fundamentally shift\nfrom optimizing for statistical performance against distributions on reasoning\nproblems and algorithmic tasks to embracing the more ambitious exact learning\nparadigm, which demands correctness on all inputs. We argue that exact learning\nis both essential and possible, and that this ambitious objective should guide\nalgorithm design.", "AI": {"tldr": "\u73b0\u6709AI\u7cfb\u7edf\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4f5c\u8005\u8ba4\u4e3a\u6839\u6e90\u5728\u4e8e\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u4e3b\u5f20\u8f6c\u5411\u8981\u6c42\u5168\u8f93\u5165\u6b63\u786e\u6027\u7684\u201c\u7cbe\u786e\u5b66\u4e60\u201d\u8303\u5f0f\u4ee5\u5b9e\u73b0\u53ef\u9760\u63a8\u7406\u3002", "motivation": "\u5c3d\u7ba1AI\u7cfb\u7edf\uff08\u7279\u522b\u662f\u57fa\u4e8eTransformer\u7684\uff09\u5728\u6570\u5b66\u548c\u79d1\u5b66\u7b49\u9886\u57df\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u5728\u7b80\u5355\u7684\u6f14\u7ece\u63a8\u7406\u4efb\u52a1\u4e0a\u4ecd\u666e\u904d\u4e14\u6301\u7eed\u5730\u5931\u8d25\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u65e0\u6cd5\u5b9e\u73b0\u5177\u5907\u53ef\u9760\u6f14\u7ece\u63a8\u7406\u80fd\u529b\u7684\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u7684\u613f\u666f\u3002", "method": "\u8bba\u6587\u63d0\u51fa\uff0c\u4e3a\u4f7f\u57fa\u4e8e\u5b66\u4e60\u7684AI\u7cfb\u7edf\u5b9e\u73b0\u53ef\u9760\u7684\u6f14\u7ece\u63a8\u7406\uff0c\u7814\u7a76\u4eba\u5458\u5fc5\u987b\u4ece\u4f18\u5316\u63a8\u7406\u95ee\u9898\u548c\u7b97\u6cd5\u4efb\u52a1\u5206\u5e03\u7684\u7edf\u8ba1\u6027\u80fd\uff0c\u6839\u672c\u6027\u5730\u8f6c\u5411\u8981\u6c42\u5bf9\u6240\u6709\u8f93\u5165\u90fd\u8fbe\u5230\u6b63\u786e\u6027\u7684\u201c\u7cbe\u786e\u5b66\u4e60\u201d\u8303\u5f0f\u3002", "result": "\u4f5c\u8005\u8bba\u8bc1\u4e86\u5f53\u524dAI\u7cfb\u7edf\u4e0d\u5065\u5168\u7684\u6f14\u7ece\u63a8\u7406\u884c\u4e3a\u662f\u5176\u6240\u4f9d\u8d56\u7684\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u7684\u56fa\u6709\u7ed3\u679c\u3002\u4ed6\u4eec\u4e3b\u5f20\uff0c\u7cbe\u786e\u5b66\u4e60\u5bf9\u4e8e\u53ef\u9760\u7684\u6f14\u7ece\u63a8\u7406\u65e2\u662f\u5fc5\u8981\u7684\u4e5f\u662f\u53ef\u80fd\u5b9e\u73b0\u7684\u3002", "conclusion": "\u4e3a\u5b9e\u73b0AI\u7cfb\u7edf\u7684\u53ef\u9760\u6f14\u7ece\u63a8\u7406\uff0c\u5fc5\u987b\u91c7\u7eb3\u8981\u6c42\u5168\u8f93\u5165\u6b63\u786e\u6027\u7684\u7cbe\u786e\u5b66\u4e60\u8303\u5f0f\uff0c\u5e76\u4e14\u8fd9\u4e00\u96c4\u5fc3\u52c3\u52c3\u7684\u76ee\u6807\u5e94\u8be5\u6307\u5bfc\u672a\u6765\u7684\u7b97\u6cd5\u8bbe\u8ba1\u3002"}}
{"id": "2506.22803", "pdf": "https://arxiv.org/pdf/2506.22803", "abs": "https://arxiv.org/abs/2506.22803", "authors": ["Nuoye Xiong", "Anqi Dong", "Ning Wang", "Cong Hua", "Guangming Zhu", "Mei Lin", "Peiyi Shen", "Liang Zhang"], "title": "Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding", "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": "Accepted by ICCV 2025", "summary": "Recent advances in deep learning have led to increasingly complex models with\ndeeper layers and more parameters, reducing interpretability and making their\ndecisions harder to understand. While many methods explain black-box reasoning,\nmost lack effective interventions or only operate at sample-level without\nmodifying the model itself. To address this, we propose the Concept Bottleneck\nModel for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).\nCBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable\nframework to approximate black-box reasoning and communicate conceptual\nunderstanding. Detrimental concepts are automatically identified and refined\n(removed/replaced) based on global gradient contributions. The modified CBM\nthen distills corrected knowledge back into the black-box model, enhancing both\ninterpretability and accuracy. We evaluate CBM-HNMU on various CNN and\ntransformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,\nand CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum\nincrease in average accuracy across 1.03%. Source code is available at:\nhttps://github.com/XiGuaBo/CBM-HNMU.", "AI": {"tldr": "\u63d0\u51faCBM-HNMU\u6a21\u578b\uff0c\u901a\u8fc7\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u8bc6\u522b\u5e76\u4fee\u6b63\u6709\u5bb3\u6982\u5ff5\uff0c\u518d\u5c06\u4fee\u6b63\u540e\u7684\u77e5\u8bc6\u84b8\u998f\u56de\u9ed1\u76d2\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u65e5\u76ca\u590d\u6742\uff0c\u89e3\u91ca\u6027\u964d\u4f4e\u4e14\u96be\u4ee5\u7406\u89e3\u5176\u51b3\u7b56\u8fc7\u7a0b\u3002\u73b0\u6709\u9ed1\u76d2\u89e3\u91ca\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u5e72\u9884\u80fd\u529b\uff0c\u6216\u4ec5\u5728\u6837\u672c\u5c42\u9762\u64cd\u4f5c\uff0c\u65e0\u6cd5\u4fee\u6539\u6a21\u578b\u672c\u8eab\u3002", "method": "\u63d0\u51faCBM-HNMU\u6a21\u578b\uff0c\u5229\u7528\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBM\uff09\u8fd1\u4f3c\u9ed1\u76d2\u63a8\u7406\u5e76\u4f20\u9012\u6982\u5ff5\u7406\u89e3\u3002\u57fa\u4e8e\u5168\u5c40\u68af\u5ea6\u8d21\u732e\u81ea\u52a8\u8bc6\u522b\u5e76\u4fee\u6b63\uff08\u79fb\u9664/\u66ff\u6362\uff09\u6709\u5bb3\u6982\u5ff5\u3002\u968f\u540e\uff0c\u5c06\u4fee\u6b63\u540e\u7684CBM\u77e5\u8bc6\u84b8\u998f\u56de\u9ed1\u76d2\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u3002", "result": "\u5728Flower-102\u3001CIFAR-10\u3001CIFAR-100\u3001FGVC-Aircraft\u548cCUB-200\u7b49\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9CNN\u548cTransformer\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u51c6\u786e\u7387\u6700\u5927\u63d0\u53472.64%\uff0c\u5e73\u5747\u51c6\u786e\u7387\u6700\u5927\u63d0\u53471.03%\u3002", "conclusion": "CBM-HNMU\u901a\u8fc7\u8bc6\u522b\u548c\u4fee\u6b63\u6709\u5bb3\u6982\u5ff5\u5e76\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\uff0c\u589e\u5f3a\u4e86\u4eba-\u795e\u7ecf\u7f51\u7edc\u7684\u76f8\u4e92\u7406\u89e3\u3002"}}
{"id": "2506.23192", "pdf": "https://arxiv.org/pdf/2506.23192", "abs": "https://arxiv.org/abs/2506.23192", "authors": ["Gabriel Iturra-Bocaz", "Felipe Bravo-Marquez"], "title": "RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at SIGIR'23", "summary": "Word embeddings have become essential components in various information\nretrieval and natural language processing tasks, such as ranking, document\nclassification, and question answering. However, despite their widespread use,\ntraditional word embedding models present a limitation in their static nature,\nwhich hampers their ability to adapt to the constantly evolving language\npatterns that emerge in sources such as social media and the web (e.g., new\nhashtags or brand names). To overcome this problem, incremental word embedding\nalgorithms are introduced, capable of dynamically updating word representations\nin response to new language patterns and processing continuous data streams.\n  This paper presents RiverText, a Python library for training and evaluating\nincremental word embeddings from text data streams. Our tool is a resource for\nthe information retrieval and natural language processing communities that work\nwith word embeddings in streaming scenarios, such as analyzing social media.\nThe library implements different incremental word embedding techniques, such as\nSkip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized\nframework. In addition, it uses PyTorch as its backend for neural network\ntraining. We have implemented a module that adapts existing intrinsic static\nword embedding evaluation tasks for word similarity and word categorization to\na streaming setting. Finally, we compare the implemented methods with different\nhyperparameter settings and discuss the results. Our open-source library is\navailable at https://github.com/dccuchile/rivertext.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86RiverText\uff0c\u4e00\u4e2aPython\u5e93\uff0c\u7528\u4e8e\u4ece\u6587\u672c\u6570\u636e\u6d41\u4e2d\u8bad\u7ec3\u548c\u8bc4\u4f30\u589e\u91cf\u8bcd\u5d4c\u5165\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u8bcd\u5d4c\u5165\u5728\u4e0d\u65ad\u53d8\u5316\u7684\u8bed\u8a00\u6a21\u5f0f\u4e2d\u8868\u73b0\u51fa\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u8bcd\u5d4c\u5165\u6a21\u578b\u662f\u9759\u6001\u7684\uff0c\u96be\u4ee5\u9002\u5e94\u793e\u4ea4\u5a92\u4f53\u548c\u7f51\u7edc\u4e2d\u4e0d\u65ad\u6f14\u53d8\u7684\u8bed\u8a00\u6a21\u5f0f\uff08\u5982\u65b0\u51fa\u73b0\u7684\u6807\u7b7e\u6216\u54c1\u724c\u540d\u79f0\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4fe1\u606f\u68c0\u7d22\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u80fd\u591f\u52a8\u6001\u66f4\u65b0\u8bcd\u8868\u793a\u7684\u589e\u91cf\u8bcd\u5d4c\u5165\u7b97\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86RiverText\u5e93\uff0c\u8be5\u5e93\u5728\u4e00\u4e2a\u6807\u51c6\u5316\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e86\u591a\u79cd\u589e\u91cf\u8bcd\u5d4c\u5165\u6280\u672f\uff0c\u5305\u62ecSkip-gram\u3001Continuous Bag of Words\u548cWord Context Matrix\u3002\u5b83\u4f7f\u7528PyTorch\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u540e\u7aef\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u5757\u5c06\u73b0\u6709\u7684\u9759\u6001\u8bcd\u5d4c\u5165\u8bc4\u4f30\u4efb\u52a1\uff08\u5982\u8bcd\u76f8\u4f3c\u5ea6\u548c\u8bcd\u5206\u7c7b\uff09\u9002\u914d\u5230\u6d41\u5f0f\u8bbe\u7f6e\u4e2d\u3002", "result": "RiverText\u5e93\u88ab\u7528\u4e8e\u6bd4\u8f83\u4e0d\u540c\u8d85\u53c2\u6570\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u7684\u589e\u91cf\u8bcd\u5d4c\u5165\u65b9\u6cd5\uff0c\u5e76\u5bf9\u7ed3\u679c\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002\u8be5\u5e93\u88ab\u5b9a\u4f4d\u4e3a\u4fe1\u606f\u68c0\u7d22\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u793e\u533a\u5904\u7406\u6d41\u5f0f\u573a\u666f\u4e0b\u8bcd\u5d4c\u5165\u7684\u8d44\u6e90\u3002", "conclusion": "RiverText\u5e93\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u6807\u51c6\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u4ece\u6587\u672c\u6570\u636e\u6d41\u4e2d\u8bad\u7ec3\u548c\u8bc4\u4f30\u589e\u91cf\u8bcd\u5d4c\u5165\uff0c\u6709\u6548\u514b\u670d\u4e86\u9759\u6001\u8bcd\u5d4c\u5165\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5904\u7406\u52a8\u6001\u8bed\u8a00\u6a21\u5f0f\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2506.22994", "pdf": "https://arxiv.org/pdf/2506.22994", "abs": "https://arxiv.org/abs/2506.22994", "authors": ["Can Hakan Da\u011f\u0131d\u0131r", "Mia Hubert", "Peter J. Rousseeuw"], "title": "Kernel Outlier Detection", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "A new anomaly detection method called kernel outlier detection (KOD) is\nproposed. It is designed to address challenges of outlier detection in\nhigh-dimensional settings. The aim is to overcome limitations of existing\nmethods, such as dependence on distributional assumptions or on hyperparameters\nthat are hard to tune. KOD starts with a kernel transformation, followed by a\nprojection pursuit approach. Its novelties include a new ensemble of directions\nto search over, and a new way to combine results of different direction types.\nThis provides a flexible and lightweight approach for outlier detection. Our\nempirical evaluations illustrate the effectiveness of KOD on three small\ndatasets with challenging structures, and on four large benchmark datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKOD\u7684\u65b0\u578b\u6838\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u6311\u6218\uff0c\u5e76\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u5f02\u5e38\u68c0\u6d4b\u7684\u6311\u6218\uff1b\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5bf9\u5206\u5e03\u5047\u8bbe\u6216\u96be\u4ee5\u8c03\u6574\u7684\u8d85\u53c2\u6570\u7684\u4f9d\u8d56\u3002", "method": "KOD\u65b9\u6cd5\u9996\u5148\u8fdb\u884c\u6838\u53d8\u6362\uff0c\u7136\u540e\u91c7\u7528\u6295\u5f71\u5bfb\u8e2a\u65b9\u6cd5\u3002\u5176\u521b\u65b0\u70b9\u5305\u62ec\u65b0\u7684\u641c\u7d22\u65b9\u5411\u96c6\u5408\u4ee5\u53ca\u7ec4\u5408\u4e0d\u540c\u65b9\u5411\u7c7b\u578b\u7ed3\u679c\u7684\u65b0\u65b9\u5f0f\u3002\u8be5\u65b9\u6cd5\u7075\u6d3b\u4e14\u8f7b\u91cf\u3002", "result": "\u7ecf\u9a8c\u8bc4\u4f30\u8868\u660eKOD\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7ed3\u6784\u7684\u5c0f\u6570\u636e\u96c6\u548c\u56db\u4e2a\u5927\u578b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u6709\u6548\u3002", "conclusion": "KOD\u662f\u4e00\u79cd\u6709\u6548\u3001\u7075\u6d3b\u4e14\u8f7b\u91cf\u7684\u9ad8\u7ef4\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.23924", "pdf": "https://arxiv.org/pdf/2506.23924", "abs": "https://arxiv.org/abs/2506.23924", "authors": ["Akshit Kumar", "Tianyi Peng", "Yuhang Wu", "Assaf Zeevi"], "title": "Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have exhibited expert-level capabilities across\nvarious domains. However, their abilities to solve problems in Operations\nResearch (OR) -- the analysis and optimization of mathematical models derived\nfrom real-world problems or their verbal descriptions -- remain underexplored.\nIn this work, we take a first step toward evaluating LLMs' abilities to solve\nstochastic modeling problems, a core class of OR problems characterized by\nuncertainty and typically involving tools from probability, statistics, and\nstochastic processes. We manually procure a representative set of\ngraduate-level homework and doctoral qualification-exam problems and test LLMs'\nabilities to solve them. We further leverage SimOpt, an open-source library of\nsimulation-optimization problems and solvers, to investigate LLMs' abilities to\nmake real-world decisions under uncertainty. Our results show that, though a\nnontrivial amount of work is still needed to reliably automate the stochastic\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\npar with human experts in both classroom and practical settings. These findings\nhighlight the potential of building AI agents that assist OR researchers and\namplify the real-world impact of OR through automation.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8fd0\u7b79\u5b66\uff08OR\uff09\u7684\u968f\u673a\u5efa\u6a21\u95ee\u9898\u4e0a\u5c55\u73b0\u51fa\u4e13\u5bb6\u7ea7\u80fd\u529b\uff0c\u5728\u8bfe\u5802\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8868\u73b0\u76f8\u5f53\uff0c\u63ed\u793a\u4e86AI\u5728OR\u81ea\u52a8\u5316\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u9886\u57df\u8868\u73b0\u51fa\u4e13\u5bb6\u7ea7\u80fd\u529b\uff0c\u4f46\u5176\u5728\u8fd0\u7b79\u5b66\uff08OR\uff09\u95ee\u9898\uff0c\u7279\u522b\u662f\u6d89\u53ca\u4e0d\u786e\u5b9a\u6027\u7684\u968f\u673a\u5efa\u6a21\u95ee\u9898\u4e0a\u7684\u80fd\u529b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86LLMs\u89e3\u51b3\u968f\u673a\u5efa\u6a21\u95ee\u9898\u7684\u80fd\u529b\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1. \u624b\u52a8\u6536\u96c6\u7814\u7a76\u751f\u6c34\u5e73\u4f5c\u4e1a\u548c\u535a\u58eb\u8d44\u683c\u8003\u8bd5\u4e2d\u7684\u4ee3\u8868\u6027\u95ee\u9898\u8fdb\u884c\u6d4b\u8bd5\u30022. \u5229\u7528\u5f00\u6e90\u4eff\u771f\u4f18\u5316\u5e93SimOpt\u8fdb\u4e00\u6b65\u63a2\u7a76LLMs\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u8fdb\u884c\u5b9e\u9645\u51b3\u7b56\u7684\u80fd\u529b\u3002", "result": "\u5c3d\u7ba1\u8981\u5b9e\u73b0\u968f\u673a\u5efa\u6a21\u6d41\u7a0b\u7684\u53ef\u9760\u81ea\u52a8\u5316\u4ecd\u9700\u5927\u91cf\u5de5\u4f5c\uff0c\u4f46\u6700\u5148\u8fdb\u7684LLMs\u5728\u8bfe\u5802\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u5747\u5c55\u73b0\u51fa\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5f53\u7684\u719f\u7ec3\u7a0b\u5ea6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u6784\u5efaAI\u667a\u80fd\u4f53\u4ee5\u8f85\u52a9\u8fd0\u7b79\u5b66\u7814\u7a76\u4eba\u5458\u7684\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u589e\u5f3a\u8fd0\u7b79\u5b66\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5f71\u54cd\u529b\u3002"}}
{"id": "2506.22806", "pdf": "https://arxiv.org/pdf/2506.22806", "abs": "https://arxiv.org/abs/2506.22806", "authors": ["Byung Hyun Lee", "Sungjin Lim", "Seunggyu Lee", "Dong Un Kang", "Se Young Chun"], "title": "Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Remarkable progress in text-to-image diffusion models has brought a major\nconcern about potentially generating images on inappropriate or trademarked\nconcepts. Concept erasing has been investigated with the goals of deleting\ntarget concepts in diffusion models while preserving other concepts with\nminimal distortion. To achieve these goals, recent concept erasing methods\nusually fine-tune the cross-attention layers of diffusion models. In this work,\nwe first show that merely updating the cross-attention layers in diffusion\nmodels, which is mathematically equivalent to adding \\emph{linear} modules to\nweights, may not be able to preserve diverse remaining concepts. Then, we\npropose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding\n\\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or\ncut) target concepts while safeguarding remaining concepts from broad\ndistributions by employing an attention anchoring loss to prevent the\nforgetting. Moreover, we adversarially train CPE with ResAG and learnable text\nembeddings in an iterative manner to maximize erasing performance and enhance\nrobustness against adversarial attacks. Extensive experiments on the erasure of\ncelebrities, artistic styles, and explicit contents demonstrated that the\nproposed CPE outperforms prior arts by keeping diverse remaining concepts while\ndeleting the target concepts with robustness against attack prompts. Code is\navailable at https://github.com/Hyun1A/CPE", "AI": {"tldr": "\u63d0\u51faCPE\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u6b8b\u5dee\u6ce8\u610f\u529b\u95e8\u548c\u5bf9\u6297\u6027\u8bad\u7ec3\uff0c\u5b9e\u73b0\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u64e6\u9664\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5220\u9664\u76ee\u6807\u6982\u5ff5\u540c\u65f6\u4fdd\u7559\u5176\u4ed6\u6982\u5ff5\u5e76\u62b5\u6297\u653b\u51fb\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u53ef\u80fd\u751f\u6210\u4e0d\u5f53\u6216\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u5185\u5bb9\u3002\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u901a\u8fc7\u5fae\u8c03\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\uff0c\u4f46\u5176\u7ebf\u6027\u7279\u6027\u96be\u4ee5\u6709\u6548\u4fdd\u7559\u591a\u6837\u5316\u7684\u975e\u76ee\u6807\u6982\u5ff5\u3002", "method": "\u63d0\u51faConcept Pinpoint Eraser (CPE) \u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6dfb\u52a0\u975e\u7ebf\u6027\u7684\u6b8b\u5dee\u6ce8\u610f\u529b\u95e8\uff08ResAGs\uff09\u6765\u9009\u62e9\u6027\u64e6\u9664\u76ee\u6807\u6982\u5ff5\u3002\u4e3a\u4fdd\u7559\u975e\u76ee\u6807\u6982\u5ff5\uff0c\u5f15\u5165\u6ce8\u610f\u529b\u951a\u5b9a\u635f\u5931\u3002\u6b64\u5916\uff0c\u901a\u8fc7ResAG\u548c\u53ef\u5b66\u4e60\u6587\u672c\u5d4c\u5165\u8fdb\u884c\u8fed\u4ee3\u5bf9\u6297\u8bad\u7ec3\uff0c\u4ee5\u6700\u5927\u5316\u64e6\u9664\u6027\u80fd\u5e76\u589e\u5f3a\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u540d\u4eba\u3001\u827a\u672f\u98ce\u683c\u548c\u663e\u5f0f\u5185\u5bb9\u7684\u64e6\u9664\u5b9e\u9a8c\u4e2d\uff0cCPE\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5220\u9664\u76ee\u6807\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u7559\u591a\u6837\u5316\u7684\u975e\u76ee\u6807\u6982\u5ff5\uff0c\u5e76\u5bf9\u653b\u51fb\u63d0\u793a\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "CPE\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u975e\u7ebf\u6027\u6a21\u5757\u548c\u5bf9\u6297\u6027\u8bad\u7ec3\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u64e6\u9664\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u3001\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u73b0\u6709\u7ebf\u6027\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.23235", "pdf": "https://arxiv.org/pdf/2506.23235", "abs": "https://arxiv.org/abs/2506.23235", "authors": ["Yi-Chen Li", "Tian Xu", "Yang Yu", "Xuqin Zhang", "Xiong-Hui Chen", "Zhongxiang Ling", "Ningjing Chao", "Lei Yuan", "Zhi-Hua Zhou"], "title": "Generalist Reward Models: Found Inside Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The alignment of Large Language Models (LLMs) is critically dependent on\nreward models trained on costly human preference data. While recent work\nexplores bypassing this cost with AI feedback, these methods often lack a\nrigorous theoretical foundation. In this paper, we discover that a powerful\ngeneralist reward model is already latently present within any LLM trained via\nstandard next-token prediction. We prove that this endogenous reward is not a\nheuristic, but is theoretically equivalent to a reward function learned through\noffline inverse reinforcement learning. This connection allows us to directly\nelicit a high-quality reward signal from a base (pre-trained or supervised\nfine-tuned) model without any further training. Critically, we also prove that\nsubsequent reinforcement learning using this endogenous reward leads to a\npolicy with a provably superior error bound compared to the base model. To our\nbest knowledge, this is the first theoretical proof of the effectiveness of\nreinforcement learning for LLMs. Our experiments validate this theory,\ndemonstrating that our method not only outperforms existing LLM-as-a-judge\napproaches but can also surpass explicitly trained reward models. These\nfindings suggest that the reward modeling stage can be replaced by a principled\nmethod of eliciting the knowledge already captured during pre-training,\nheralding a more efficient, powerful, and scalable paradigm for LLMs alignment\nas well as multi-modal models.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u5e76\u7406\u8bba\u8bc1\u660e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9884\u8bad\u7ec3\u4e2d\u5df2\u9690\u542b\u5956\u52b1\u6a21\u578b\uff08\u5185\u751f\u5956\u52b1\uff09\uff0c\u53ef\u7b49\u6548\u4e8e\u9006\u5f3a\u5316\u5b66\u4e60\uff0c\u65e0\u9700\u4eba\u5de5\u504f\u597d\u6570\u636e\u5373\u53ef\u7528\u4e8e\u6a21\u578b\u5bf9\u9f50\uff0c\u4e14\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "LLM\u5bf9\u9f50\u4e25\u91cd\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u504f\u597d\u6570\u636e\u6765\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u800c\u73b0\u6709\u7684AI\u53cd\u9988\u65b9\u6cd5\u7f3a\u4e4f\u4e25\u683c\u7684\u7406\u8bba\u57fa\u7840\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u6709\u7406\u8bba\u4f9d\u636e\u7684\u5bf9\u9f50\u8303\u5f0f\u3002", "method": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u6807\u51c6\u4e0b\u4e00\u8bcd\u9884\u6d4b\u8bad\u7ec3\u7684\u4efb\u4f55LLM\u5185\u90e8\u90fd\u6f5c\u5728\u5305\u542b\u4e00\u4e2a\u5f3a\u5927\u7684\u901a\u7528\u5956\u52b1\u6a21\u578b\u3002\u672c\u6587\u8bc1\u660e\u8be5\u5185\u751f\u5956\u52b1\u5728\u7406\u8bba\u4e0a\u7b49\u540c\u4e8e\u901a\u8fc7\u79bb\u7ebf\u9006\u5f3a\u5316\u5b66\u4e60\u5b66\u5230\u7684\u5956\u52b1\u51fd\u6570\u3002\u57fa\u4e8e\u6b64\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u76f4\u63a5\u4ece\u57fa\u7840\u6a21\u578b\u4e2d\u63d0\u53d6\u9ad8\u8d28\u91cf\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u5229\u7528\u6b64\u4fe1\u53f7\u8fdb\u884c\u540e\u7eed\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4f7f\u7528\u8be5\u5185\u751f\u5956\u52b1\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u83b7\u5f97\u5177\u6709\u53ef\u8bc1\u660e\u66f4\u4f18\u9519\u8bef\u754c\u9650\u7684\u7b56\u7565\uff0c\u8fd9\u662fLLM\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u6027\u7684\u9996\u6b21\u7406\u8bba\u8bc1\u660e\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7406\u8bba\uff0c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u4e0d\u4ec5\u4f18\u4e8e\u73b0\u6709\u7684LLM-as-a-judge\u65b9\u6cd5\uff0c\u751a\u81f3\u80fd\u8d85\u8d8a\u663e\u5f0f\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5956\u52b1\u5efa\u6a21\u9636\u6bb5\u53ef\u88ab\u4e00\u79cd\u4ece\u9884\u8bad\u7ec3\u77e5\u8bc6\u4e2d\u63d0\u53d6\u4fe1\u606f\u7684\u65b0\u65b9\u6cd5\u53d6\u4ee3\uff0c\u8fd9\u4e3aLLM\u4ee5\u53ca\u591a\u6a21\u6001\u6a21\u578b\u7684\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.22995", "pdf": "https://arxiv.org/pdf/2506.22995", "abs": "https://arxiv.org/abs/2506.22995", "authors": ["Davide Salaorni", "Federico Bianchi", "Francesco Trov\u00f2", "Marcello Restelli"], "title": "A Reinforcement Learning Approach for Optimal Control in Microgrids", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "8 pages, accepted to International Joint Conference on Neural\n  Networks 2025", "summary": "The increasing integration of renewable energy sources (RESs) is transforming\ntraditional power grid networks, which require new approaches for managing\ndecentralized energy production and consumption. Microgrids (MGs) provide a\npromising solution by enabling localized control over energy generation,\nstorage, and distribution. This paper presents a novel reinforcement learning\n(RL)-based methodology for optimizing microgrid energy management.\nSpecifically, we propose an RL agent that learns optimal energy trading and\nstorage policies by leveraging historical data on energy production,\nconsumption, and market prices. A digital twin (DT) is used to simulate the\nenergy storage system dynamics, incorporating degradation factors to ensure a\nrealistic emulation of the analysed setting. Our approach is validated through\nan experimental campaign using real-world data from a power grid located in the\nItalian territory. The results indicate that the proposed RL-based strategy\noutperforms rule-based methods and existing RL benchmarks, offering a robust\nsolution for intelligent microgrid management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u6280\u672f\uff0c\u7528\u4e8e\u4f18\u5316\u5fae\u7535\u7f51\u7684\u80fd\u6e90\u7ba1\u7406\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u7684\u65e5\u76ca\u6574\u5408\uff0c\u4f20\u7edf\u7535\u7f51\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u7ba1\u7406\u5206\u6563\u5f0f\u80fd\u6e90\u7684\u751f\u4ea7\u548c\u6d88\u8d39\u3002\u5fae\u7535\u7f51\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u80fd\u6e90\u7ba1\u7406\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5229\u7528\u5386\u53f2\u80fd\u6e90\u751f\u4ea7\u3001\u6d88\u8d39\u548c\u5e02\u573a\u4ef7\u683c\u6570\u636e\uff0c\u5b66\u4e60\u6700\u4f18\u7684\u80fd\u6e90\u4ea4\u6613\u548c\u5b58\u50a8\u7b56\u7565\u3002\u540c\u65f6\uff0c\u91c7\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\u6a21\u62df\u50a8\u80fd\u7cfb\u7edf\u52a8\u6001\uff0c\u5e76\u8003\u8651\u5176\u9000\u5316\u56e0\u7d20\uff0c\u4ee5\u786e\u4fdd\u4eff\u771f\u7684\u771f\u5b9e\u6027\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u610f\u5927\u5229\u7535\u7f51\u7684\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684RL\u7b56\u7565\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u548c\u73b0\u6709\u7684RL\u57fa\u51c6\u3002", "conclusion": "\u8be5RL\u65b9\u6cd5\u4e3a\u667a\u80fd\u5fae\u7535\u7f51\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23926", "pdf": "https://arxiv.org/pdf/2506.23926", "abs": "https://arxiv.org/abs/2506.23926", "authors": ["Junping Wang", "Bicheng Wang", "Yibo Xuea", "Yuan Xie"], "title": "Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Resilience non-equilibrium measurement, the ability to maintain fundamental\nfunctionality amidst failures and errors, is crucial for scientific management\nand engineering applications of industrial chain. The problem is particularly\nchallenging when the number or types of multiple co-evolution of resilience\n(for example, randomly placed) are extremely chaos. Existing end-to-end deep\nlearning ordinarily do not generalize well to unseen full-feld reconstruction\nof spatiotemporal co-evolution structure, and predict resilience of network\ntopology, especially in multiple chaos data regimes typically seen in\nreal-world applications. To address this challenge, here we propose industrial\nbrain, a human-like autonomous cognitive decision-making and planning framework\nintegrating higher-order activity-driven neuro network and CT-OODA symbolic\nreasoning to autonomous plan resilience directly from observational data of\nglobal variable. The industrial brain not only understands and model structure\nof node activity dynamics and network co-evolution topology without simplifying\nassumptions, and reveal the underlying laws hidden behind complex networks, but\nalso enabling accurate resilience prediction, inference, and planning.\nExperimental results show that industrial brain significantly outperforms\nresilience prediction and planning methods, with an accurate improvement of up\nto 10.8\\% over GoT and OlaGPT framework and 11.03\\% over spectral dimension\nreduction. It also generalizes to unseen topologies and dynamics and maintains\nrobust performance despite observational disturbances. Our findings suggest\nthat industrial brain addresses an important gap in resilience prediction and\nplanning for industrial chain.", "AI": {"tldr": "\u9488\u5bf9\u5de5\u4e1a\u94fe\u5728\u591a\u6df7\u6c8c\u6570\u636e\u4e0b\u97e7\u6027\u6d4b\u91cf\u4e0e\u89c4\u5212\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u201c\u5de5\u4e1a\u5927\u8111\u201d\u6846\u67b6\u3002\u8be5\u6846\u67b6\u878d\u5408\u9ad8\u9636\u795e\u7ecf\u7f51\u7edc\u4e0e\u7b26\u53f7\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u7f51\u7edc\u7ed3\u6784\u4e0e\u52a8\u6001\u7684\u7406\u89e3\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u97e7\u6027\u9884\u6d4b\u548c\u89c4\u5212\u7684\u51c6\u786e\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5de5\u4e1a\u94fe\u5728\u6545\u969c\u4e2d\u4fdd\u6301\u57fa\u672c\u529f\u80fd\uff08\u97e7\u6027\uff09\u5bf9\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u97e7\u6027\u5171\u6f14\u5316\u6570\u91cf\u6216\u7c7b\u578b\u6781\u5176\u6df7\u6c8c\u65f6\uff0c\u73b0\u6709\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u672a\u89c1\u7684\u65f6\u7a7a\u7ed3\u6784\u91cd\u5efa\u548c\u7f51\u7edc\u62d3\u6251\u9884\u6d4b\u4e0a\u6cdb\u5316\u6027\u5dee\uff0c\u5c24\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u591a\u6df7\u6c8c\u6570\u636e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u6784\u6210\u4e86\u6838\u5fc3\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u201c\u5de5\u4e1a\u5927\u8111\u201d\uff08industrial brain\uff09\u6846\u67b6\uff0c\u4e00\u4e2a\u7c7b\u4eba\u81ea\u4e3b\u8ba4\u77e5\u51b3\u7b56\u4e0e\u89c4\u5212\u6846\u67b6\u3002\u5b83\u6574\u5408\u4e86\u9ad8\u9636\u6d3b\u52a8\u9a71\u52a8\u795e\u7ecf\u7f51\u7edc\uff08higher-order activity-driven neuro network\uff09\u548cCT-OODA\u7b26\u53f7\u63a8\u7406\uff08CT-OODA symbolic reasoning\uff09\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u5168\u5c40\u53d8\u91cf\u7684\u89c2\u6d4b\u6570\u636e\u4e2d\u81ea\u4e3b\u89c4\u5212\u97e7\u6027\u3002\u8be5\u6846\u67b6\u65e0\u9700\u7b80\u5316\u5047\u8bbe\u5373\u53ef\u7406\u89e3\u548c\u5efa\u6a21\u8282\u70b9\u6d3b\u52a8\u52a8\u529b\u5b66\u4e0e\u7f51\u7edc\u5171\u6f14\u5316\u62d3\u6251\uff0c\u5e76\u63ed\u793a\u590d\u6742\u7f51\u7edc\u80cc\u540e\u7684\u6f5c\u5728\u89c4\u5f8b\uff0c\u4ece\u800c\u5b9e\u73b0\u51c6\u786e\u7684\u97e7\u6027\u9884\u6d4b\u3001\u63a8\u7406\u548c\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u201c\u5de5\u4e1a\u5927\u8111\u201d\u5728\u97e7\u6027\u9884\u6d4b\u548c\u89c4\u5212\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u76f8\u8f83\u4e8eGoT\u548cOlaGPT\u6846\u67b6\uff0c\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534710.8%\uff1b\u76f8\u8f83\u4e8e\u8c31\u7ef4\u5ea6\u7ea6\u7b80\uff0c\u51c6\u786e\u7387\u63d0\u9ad811.03%\u3002\u6b64\u5916\uff0c\u5b83\u5bf9\u672a\u89c1\u7684\u62d3\u6251\u7ed3\u6784\u548c\u52a8\u529b\u5b66\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u5b58\u5728\u89c2\u6d4b\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7a33\u5065\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u53d1\u73b0\u201c\u5de5\u4e1a\u5927\u8111\u201d\u6210\u529f\u586b\u8865\u4e86\u5de5\u4e1a\u94fe\u97e7\u6027\u9884\u6d4b\u548c\u89c4\u5212\u9886\u57df\u7684\u91cd\u8981\u7a7a\u767d\u3002"}}
{"id": "2506.22807", "pdf": "https://arxiv.org/pdf/2506.22807", "abs": "https://arxiv.org/abs/2506.22807", "authors": ["Yueyang Li", "Shengyu Gong", "Weiming Zeng", "Nizhuan Wang", "Wai Ting Siok"], "title": "FreqDGT: Frequency-Adaptive Dynamic Graph Networks with Transformer for Cross-subject EEG Emotion Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Electroencephalography (EEG) serves as a reliable and objective signal for\nemotion recognition in affective brain-computer interfaces, offering unique\nadvantages through its high temporal resolution and ability to capture\nauthentic emotional states that cannot be consciously controlled. However,\ncross-subject generalization remains a fundamental challenge due to individual\nvariability, cognitive traits, and emotional responses. We propose FreqDGT, a\nfrequency-adaptive dynamic graph transformer that systematically addresses\nthese limitations through an integrated framework. FreqDGT introduces\nfrequency-adaptive processing (FAP) to dynamically weight emotion-relevant\nfrequency bands based on neuroscientific evidence, employs adaptive dynamic\ngraph learning (ADGL) to learn input-specific brain connectivity patterns, and\nimplements multi-scale temporal disentanglement network (MTDN) that combines\nhierarchical temporal transformers with adversarial feature disentanglement to\ncapture both temporal dynamics and ensure cross-subject robustness.\nComprehensive experiments demonstrate that FreqDGT significantly improves\ncross-subject emotion recognition accuracy, confirming the effectiveness of\nintegrating frequency-adaptive, spatial-dynamic, and temporal-hierarchical\nmodeling while ensuring robustness to individual differences. The code is\navailable at https://github.com/NZWANG/FreqDGT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFreqDGT\uff0c\u4e00\u79cd\u9891\u7387\u81ea\u9002\u5e94\u52a8\u6001\u56feTransformer\uff0c\u901a\u8fc7\u6574\u5408\u9891\u7387\u3001\u7a7a\u95f4\u548c\u65f6\u95f4\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86EEG\u60c5\u611f\u8bc6\u522b\u7684\u8de8\u88ab\u8bd5\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u8111\u7535\u56fe\uff08EEG\uff09\u662f\u60c5\u611f\u8bc6\u522b\u7684\u53ef\u9760\u4fe1\u53f7\uff0c\u4f46\u7531\u4e8e\u4e2a\u4f53\u5dee\u5f02\u5927\uff0c\u8de8\u88ab\u8bd5\u6cdb\u5316\u80fd\u529b\u5dee\u662f\u5f53\u524dEEG\u60c5\u611f\u8bc6\u522b\u9762\u4e34\u7684\u6839\u672c\u6311\u6218\u3002", "method": "\u63d0\u51faFreqDGT\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u6846\u67b6\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002FreqDGT\u5305\u542b\uff1a1) \u9891\u7387\u81ea\u9002\u5e94\u5904\u7406\uff08FAP\uff09\uff0c\u52a8\u6001\u52a0\u6743\u4e0e\u60c5\u611f\u76f8\u5173\u7684\u9891\u5e26\uff1b2) \u81ea\u9002\u5e94\u52a8\u6001\u56fe\u5b66\u4e60\uff08ADGL\uff09\uff0c\u5b66\u4e60\u7279\u5b9a\u8f93\u5165\u7684\u8111\u8fde\u63a5\u6a21\u5f0f\uff1b3) \u591a\u5c3a\u5ea6\u65f6\u95f4\u89e3\u8026\u7f51\u7edc\uff08MTDN\uff09\uff0c\u7ed3\u5408\u5206\u5c42\u65f6\u95f4Transformer\u548c\u5bf9\u6297\u6027\u7279\u5f81\u89e3\u8026\uff0c\u4ee5\u6355\u6349\u65f6\u95f4\u52a8\u6001\u5e76\u786e\u4fdd\u8de8\u88ab\u8bd5\u9c81\u68d2\u6027\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cFreqDGT\u663e\u8457\u63d0\u9ad8\u4e86\u8de8\u88ab\u8bd5\u60c5\u611f\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u5b9e\u4e86\u6574\u5408\u9891\u7387\u81ea\u9002\u5e94\u3001\u7a7a\u95f4\u52a8\u6001\u548c\u65f6\u95f4\u5206\u5c42\u5efa\u6a21\u7684\u6709\u6548\u6027\uff0c\u5e76\u786e\u4fdd\u4e86\u5bf9\u4e2a\u4f53\u5dee\u5f02\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "FreqDGT\u901a\u8fc7\u5176\u521b\u65b0\u7684\u96c6\u6210\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86EEG\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u8de8\u88ab\u8bd5\u6cdb\u5316\u96be\u9898\uff0c\u4e3a\u60c5\u611f\u8111\u673a\u63a5\u53e3\u7684\u8fdb\u6b65\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2506.23288", "pdf": "https://arxiv.org/pdf/2506.23288", "abs": "https://arxiv.org/abs/2506.23288", "authors": ["Miguel Domingo", "Francisco Casacuberta"], "title": "Two Spelling Normalization Approaches Based on Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The absence of standardized spelling conventions and the organic evolution of\nhuman language present an inherent linguistic challenge within historical\ndocuments, a longstanding concern for scholars in the humanities. Addressing\nthis issue, spelling normalization endeavors to align a document's orthography\nwith contemporary standards. In this study, we propose two new approaches based\non large language models: one of which has been trained without a supervised\ntraining, and a second one which has been trained for machine translation. Our\nevaluation spans multiple datasets encompassing diverse languages and\nhistorical periods, leading us to the conclusion that while both of them\nyielded encouraging results, statistical machine translation still seems to be\nthe most suitable technology for this task.", "AI": {"tldr": "\u9488\u5bf9\u5386\u53f2\u6587\u732e\u62fc\u5199\u89c4\u8303\u5316\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u8bc4\u4f30\u53d1\u73b0\u5b83\u4eec\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u7edf\u8ba1\u673a\u5668\u7ffb\u8bd1\u4ecd\u662f\u76ee\u524d\u6700\u9002\u5408\u6b64\u4efb\u52a1\u7684\u6280\u672f\u3002", "motivation": "\u5386\u53f2\u6587\u732e\u4e2d\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u62fc\u5199\u89c4\u8303\u548c\u8bed\u8a00\u7684\u81ea\u7136\u6f14\u53d8\uff0c\u5bf9\u4eba\u6587\u5b66\u8005\u6784\u6210\u4e86\u56fa\u6709\u7684\u8bed\u8a00\u6311\u6218\u3002\u62fc\u5199\u89c4\u8303\u5316\u7684\u76ee\u7684\u662f\u5c06\u6587\u6863\u7684\u6b63\u5b57\u6cd5\u4e0e\u5f53\u4ee3\u6807\u51c6\u5bf9\u9f50\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e00\u957f\u671f\u5b58\u5728\u7684\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff1a\u4e00\u79cd\u672a\u7ecf\u76d1\u7763\u8bad\u7ec3\uff0c\u53e6\u4e00\u79cd\u5219\u7528\u4e8e\u673a\u5668\u7ffb\u8bd1\u3002\u7814\u7a76\u5728\u6db5\u76d6\u591a\u79cd\u8bed\u8a00\u548c\u5386\u53f2\u65f6\u671f\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u4e24\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u90fd\u53d6\u5f97\u4e86\u4ee4\u4eba\u9f13\u821e\u7684\u7ed3\u679c\u3002", "conclusion": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7edf\u8ba1\u673a\u5668\u7ffb\u8bd1\uff08SMT\uff09\u4f3c\u4e4e\u4ecd\u7136\u662f\u62fc\u5199\u89c4\u8303\u5316\u4efb\u52a1\u6700\u5408\u9002\u7684\u6280\u672f\u3002"}}
{"id": "2506.23024", "pdf": "https://arxiv.org/pdf/2506.23024", "abs": "https://arxiv.org/abs/2506.23024", "authors": ["Jerry Liu", "Yasa Baig", "Denise Hui Jean Lee", "Rajat Vadiraj Dwaraknath", "Atri Rudra", "Chris R\u00e9"], "title": "BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "comment": "Workshop for the Theory of AI for Scientific Computing @ COLT 2025\n  (Best Paper). 39 pages, 24 figures", "summary": "Physics-informed neural networks (PINNs) offer a flexible way to solve\npartial differential equations (PDEs) with machine learning, yet they still\nfall well short of the machine-precision accuracy many scientific tasks demand.\nIn this work, we investigate whether the precision ceiling comes from the\nill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)\narchitecture. We introduce the Barycentric Weight Layer (BWLer), which models\nthe PDE solution through barycentric polynomial interpolation. A BWLer can be\nadded on top of an existing MLP (a BWLer-hat) or replace it completely\n(explicit BWLer), cleanly separating how we represent the solution from how we\ntake derivatives for the PDE loss. Using BWLer, we identify fundamental\nprecision limitations within the MLP: on a simple 1-D interpolation task, even\nMLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above\nfloat64 machine precision -- before any PDE terms are added. In PDE learning,\nadding a BWLer lifts this ceiling and exposes a tradeoff between achievable\naccuracy and the conditioning of the PDE loss. For linear PDEs we fully\ncharacterize this tradeoff with an explicit error decomposition and navigate it\nduring training with spectral derivatives and preconditioning. Across five\nbenchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for\nconvection, 10x for reaction, and 1800x for wave equations while remaining\ncompatible with first-order optimizers. Replacing the MLP entirely lets an\nexplicit BWLer reach near-machine-precision on convection, reaction, and wave\nproblems (up to 10 billion times better than prior results) and match the\nperformance of standard PINNs on stiff Burgers' and irregular-geometry Poisson\nproblems. Together, these findings point to a practical path for combining the\nflexibility of PINNs with the precision of classical spectral solvers.", "AI": {"tldr": "\u9274\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u5728\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\u65f6\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u672c\u6587\u63d0\u51fa\u91cd\u5fc3\u6743\u503c\u5c42\uff08BWLer\uff09\u6765\u89e3\u51b3\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u56fa\u6709\u7684\u7cbe\u5ea6\u74f6\u9888\u3002BWLer\u901a\u8fc7\u591a\u9879\u5f0f\u63d2\u503c\u663e\u8457\u63d0\u9ad8\u4e86PDEs\u7684\u6c42\u89e3\u7cbe\u5ea6\uff0c\u751a\u81f3\u8fbe\u5230\u63a5\u8fd1\u673a\u5668\u7cbe\u5ea6\uff0c\u4e3a\u7ed3\u5408PINNs\u7684\u7075\u6d3b\u6027\u4e0e\u7ecf\u5178\u8c31\u6c42\u89e3\u5668\u7684\u7cbe\u5ea6\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u867d\u7136\u7075\u6d3b\uff0c\u4f46\u5728\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\u65f6\uff0c\u7cbe\u5ea6\u8fdc\u672a\u8fbe\u5230\u79d1\u5b66\u4efb\u52a1\u6240\u9700\u7684\u673a\u5668\u7cbe\u5ea6\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u8fd9\u79cd\u7cbe\u5ea6\u74f6\u9888\u662f\u6e90\u4e8ePDE\u7684\u75c5\u6001\u6027\u8fd8\u662f\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u67b6\u6784\u7684\u9650\u5236\u3002", "method": "\u5f15\u5165\u4e86\u91cd\u5fc3\u6743\u503c\u5c42\uff08BWLer\uff09\uff0c\u901a\u8fc7\u91cd\u5fc3\u591a\u9879\u5f0f\u63d2\u503c\u6765\u5efa\u6a21PDE\u89e3\u3002BWLer\u53ef\u4f5c\u4e3a\u73b0\u6709MLP\u7684\u9876\u5c42\uff08BWLer-hat\uff09\u6216\u5b8c\u5168\u53d6\u4ee3MLP\uff08\u663e\u5f0fBWLer\uff09\uff0c\u4ece\u800c\u89e3\u8026\u89e3\u7684\u8868\u793a\u4e0ePDE\u635f\u5931\u7684\u5bfc\u6570\u8ba1\u7b97\u3002\u5bf9\u4e8e\u7ebf\u6027PDEs\uff0c\u7814\u7a76\u901a\u8fc7\u663e\u5f0f\u8bef\u5dee\u5206\u89e3\u6765\u8868\u5f81\u53ef\u5b9e\u73b0\u7cbe\u5ea6\u4e0ePDE\u635f\u5931\u6761\u4ef6\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u901a\u8fc7\u8c31\u5bfc\u6570\u548c\u9884\u5904\u7406\u5728\u8bad\u7ec3\u4e2d\u8fdb\u884c\u7ba1\u7406\u3002", "result": "\u7814\u7a76\u53d1\u73b0MLP\u672c\u8eab\u5b58\u5728\u7cbe\u5ea6\u9650\u5236\uff0c\u5373\u4f7f\u5728\u7b80\u5355\u63d2\u503c\u4efb\u52a1\u4e2d\u4e5f\u4ec5\u80fd\u8fbe\u52301e-8 RMSE\u3002\u6dfb\u52a0BWLer\u80fd\u63d0\u5347PDE\u5b66\u4e60\u7684\u7cbe\u5ea6\u4e0a\u9650\u3002BWLer-hat\u5728\u4e94\u79cd\u57fa\u51c6PDE\u4e0a\u5c06RMSE\u63d0\u9ad8\u4e8630\u500d\uff08\u5bf9\u6d41\uff09\u300110\u500d\uff08\u53cd\u5e94\uff09\u548c1800\u500d\uff08\u6ce2\u52a8\uff09\u3002\u663e\u5f0fBWLer\u5728\u5bf9\u6d41\u3001\u53cd\u5e94\u548c\u6ce2\u52a8\u95ee\u9898\u4e0a\u8fbe\u5230\u4e86\u63a5\u8fd1\u673a\u5668\u7cbe\u5ea6\uff08\u6bd4\u73b0\u6709\u7ed3\u679c\u597d100\u4ebf\u500d\uff09\uff0c\u5e76\u5728\u521a\u6027Burgers\u548c\u4e0d\u89c4\u5219\u51e0\u4f55\u6cca\u677e\u95ee\u9898\u4e0a\u4e0e\u6807\u51c6PINNs\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u6307\u51fa\u4e86\u4e00\u6761\u5b9e\u7528\u7684\u9014\u5f84\uff0c\u53ef\u4ee5\u5c06\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u7684\u7075\u6d3b\u6027\u4e0e\u7ecf\u5178\u8c31\u6c42\u89e3\u5668\u6240\u9700\u7684\u673a\u5668\u7ea7\u7cbe\u5ea6\u6709\u6548\u7ed3\u5408\u3002"}}
{"id": "2506.23949", "pdf": "https://arxiv.org/pdf/2506.23949", "abs": "https://arxiv.org/abs/2506.23949", "authors": ["Anthony M. Barrett", "Jessica Newman", "Brandie Nonnecke", "Nada Madkour", "Dan Hendrycks", "Evan R. Murphy", "Krystal Jackson", "Deepika Raman"], "title": "AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models", "categories": ["cs.AI", "cs.CR", "cs.CY"], "comment": null, "summary": "Increasingly multi-purpose AI models, such as cutting-edge large language\nmodels or other 'general-purpose AI' (GPAI) models, 'foundation models,'\ngenerative AI models, and 'frontier models' (typically all referred to\nhereafter with the umbrella term 'GPAI/foundation models' except where greater\nspecificity is needed), can provide many beneficial capabilities but also risks\nof adverse events with profound consequences. This document provides\nrisk-management practices or controls for identifying, analyzing, and\nmitigating risks of GPAI/foundation models. We intend this document primarily\nfor developers of large-scale, state-of-the-art GPAI/foundation models; others\nthat can benefit from this guidance include downstream developers of end-use\napplications that build on a GPAI/foundation model. This document facilitates\nconformity with or use of leading AI risk management-related standards,\nadapting and building on the generic voluntary guidance in the NIST AI Risk\nManagement Framework and ISO/IEC 23894, with a focus on the unique issues faced\nby developers of GPAI/foundation models.", "AI": {"tldr": "\u9488\u5bf9\u901a\u7528/\u57fa\u7840AI\u6a21\u578b\uff08GPAI/foundation models\uff09\u7684\u98ce\u9669\u7ba1\u7406\u5b9e\u8df5\u6307\u5357\u3002", "motivation": "\u9274\u4e8e\u591a\u529f\u80fdAI\u6a21\u578b\uff08\u5982\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u57fa\u7840\u6a21\u578b\uff09\u5728\u5e26\u6765\u5de8\u5927\u80fd\u529b\u7684\u540c\u65f6\u4e5f\u4f34\u968f\u7740\u4e25\u91cd\u98ce\u9669\uff0c\u672c\u6587\u4ef6\u65e8\u5728\u5e2e\u52a9\u5f00\u53d1\u8005\u8bc6\u522b\u3001\u5206\u6790\u548c\u7f13\u89e3\u8fd9\u4e9b\u98ce\u9669\u3002", "method": "\u672c\u6587\u4ef6\u63d0\u4f9b\u98ce\u9669\u7ba1\u7406\u5b9e\u8df5\u4e0e\u63a7\u5236\u63aa\u65bd\uff0c\u4e3b\u8981\u9762\u5411\u5927\u89c4\u6a21\u3001\u6700\u5148\u8fdb\u7684GPAI/\u57fa\u7840\u6a21\u578b\u5f00\u53d1\u8005\u3002\u5176\u65b9\u6cd5\u662f\u57fa\u4e8eNIST AI\u98ce\u9669\u7ba1\u7406\u6846\u67b6\u548cISO/IEC 23894\u7b49\u73b0\u6709AI\u98ce\u9669\u7ba1\u7406\u6807\u51c6\uff0c\u5e76\u7279\u522b\u5173\u6ce8GPAI/\u57fa\u7840\u6a21\u578b\u5f00\u53d1\u8005\u9762\u4e34\u7684\u72ec\u7279\u95ee\u9898\u3002", "result": "\u672c\u6587\u4ef6\u4f5c\u4e3a\u4e00\u4efd\u6307\u5357\uff0c\u63d0\u4f9b\u4e86\u8bc6\u522b\u3001\u5206\u6790\u548c\u7f13\u89e3GPAI/\u57fa\u7840\u6a21\u578b\u98ce\u9669\u7684\u5177\u4f53\u5b9e\u8df5\u6216\u63a7\u5236\u63aa\u65bd\uff0c\u65e8\u5728\u5e2e\u52a9\u76f8\u5173\u5f00\u53d1\u8005\u7ba1\u7406\u6f5c\u5728\u7684\u4e0d\u5229\u4e8b\u4ef6\u3002", "conclusion": "\u4e3a\u786e\u4fddGPAI/\u57fa\u7840\u6a21\u578b\u7684\u5b89\u5168\u548c\u8d1f\u8d23\u4efb\u53d1\u5c55\uff0c\u5fc5\u987b\u5b9e\u65bd\u4e13\u4e1a\u7684\u98ce\u9669\u7ba1\u7406\u63aa\u65bd\u3002\u672c\u6307\u5357\u901a\u8fc7\u6574\u5408\u73b0\u6709\u6807\u51c6\u5e76\u89e3\u51b3GPAI\u7684\u7279\u6b8a\u6311\u6218\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u6307\u5bfc\uff0c\u4ee5\u964d\u4f4e\u98ce\u9669\u5e76\u4fc3\u8fdb\u6700\u4f73\u5b9e\u8df5\u3002"}}
{"id": "2506.22814", "pdf": "https://arxiv.org/pdf/2506.22814", "abs": "https://arxiv.org/abs/2506.22814", "authors": ["Andrew Hamara", "Andrew C. Freeman"], "title": "Efficient Multi-Crop Saliency Partitioning for Automatic Image Cropping", "categories": ["cs.CV"], "comment": null, "summary": "Automatic image cropping aims to extract the most visually salient regions\nwhile preserving essential composition elements. Traditional saliency-aware\ncropping methods optimize a single bounding box, making them ineffective for\napplications requiring multiple disjoint crops. In this work, we extend the\nFixed Aspect Ratio Cropping algorithm to efficiently extract multiple\nnon-overlapping crops in linear time. Our approach dynamically adjusts\nattention thresholds and removes selected crops from consideration without\nrecomputing the entire saliency map. We discuss qualitative results and\nintroduce the potential for future datasets and benchmarks.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u56fa\u5b9a\u957f\u5bbd\u6bd4\u88c1\u526a\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u4e0d\u91cd\u53e0\u56fe\u50cf\u88c1\u526a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4ec5\u5904\u7406\u5355\u4e2a\u88c1\u526a\u6846\u7684\u5c40\u9650\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u88c1\u526a\u65b9\u6cd5\u4ec5\u4f18\u5316\u5355\u4e2a\u8fb9\u754c\u6846\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u9700\u8981\u591a\u4e2a\u4e0d\u76f8\u4ea4\u88c1\u526a\u533a\u57df\u7684\u5e94\u7528\u9700\u6c42\u3002", "method": "\u8be5\u5de5\u4f5c\u6269\u5c55\u4e86\u56fa\u5b9a\u957f\u5bbd\u6bd4\u88c1\u526a\u7b97\u6cd5\uff0c\u4ee5\u7ebf\u6027\u65f6\u95f4\u9ad8\u6548\u63d0\u53d6\u591a\u4e2a\u975e\u91cd\u53e0\u88c1\u526a\u533a\u57df\u3002\u5176\u65b9\u6cd5\u52a8\u6001\u8c03\u6574\u6ce8\u610f\u529b\u9608\u503c\uff0c\u5e76\u79fb\u9664\u5df2\u9009\u62e9\u7684\u88c1\u526a\u533a\u57df\uff0c\u65e0\u9700\u91cd\u65b0\u8ba1\u7b97\u6574\u4e2a\u663e\u8457\u6027\u56fe\u3002", "result": "\u6587\u7ae0\u8ba8\u8bba\u4e86\u5176\u65b9\u6cd5\u7684\u5b9a\u6027\u7ed3\u679c\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5904\u7406\u591a\u4e0d\u76f8\u4ea4\u56fe\u50cf\u88c1\u526a\u7684\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.23293", "pdf": "https://arxiv.org/pdf/2506.23293", "abs": "https://arxiv.org/abs/2506.23293", "authors": ["P. Myles Eugenio"], "title": "Objective-Free Local Learning and Emergent Language Structure in Thinking Machines", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "comment": "22 pages, 7 figures", "summary": "We present a neuro-symbolic framework for generative language modeling based\non local, event-driven emergent learning. At its core is a hierarchical\nHopfield memory chain acting as a compositional short-term memory and dynamic\ntokenizer (retokenizer). Rather than relying on predefined tokens or\nsupervision, the model builds structure from scratch, learning symbol sequences\nas multi-scale representations. It constructs projection tensors that bind\nco-occurring features into hierarchical tokens, introducing redundancy (i.e an\nemergent gauge structure) and enabling compression of local activations into\nlong-range dependencies. Curiously, we find that the retokenizer can filter\nnatural language patterns from noise, generating synthetic languages with\ncoherent internal morphology -- quantifiably the same as human language.\nLanguage is learned in a local (Hebbian) fashion, where model constraints\ndictate allowed emergent structure, and new information is retained in\nalignment with this structure. The absence of a global objective enables a form\nof plasticity not found in conventional language models, allowing the system to\ngeneralize beyond its initial inference class -- even without explicit data. We\ndemonstrate that briefly activating a new neuron during inference binds\ndistributed multi-scale token features into a symbolic embedding. These\nemergent embedding neurons act as long-term memory and support a key-value\nmechanism for compositional inference and generalization. This architecture\nprovides a methodological foundation for studying how symbolic structure can\nemerge from local neural learning. It offers a new pathway for building\nscalable, interpretable neuro-symbolic systems -- where tokens, grammar, and\nreasoning arise as compressed memory traces within a Hopfield hierarchy. This\napproach advances the development of neuromorphic architectures for generative\nlanguage models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5c40\u90e8\u4e8b\u4ef6\u9a71\u52a8\u81ea\u53d1\u5b66\u4e60\u7684\u795e\u7ecf-\u7b26\u53f7\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u5206\u5c42Hopfield\u8bb0\u5fc6\u94fe\uff0c\u80fd\u591f\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u5e76\u751f\u6210\u4e0e\u4eba\u7c7b\u8bed\u8a00\u5f62\u6001\u4e00\u81f4\u7684\u5408\u6210\u8bed\u8a00\uff0c\u5e76\u5c55\u73b0\u51fa\u8d85\u8d8a\u4f20\u7edf\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u7f3a\u4e4f\u53ef\u5851\u6027\u3001\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u4f9d\u8d56\u9884\u5b9a\u4e49\u7b26\u53f7\u6216\u5168\u5c40\u76ee\u6807\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5c40\u90e8\u795e\u7ecf\u5b66\u4e60\u8ba9\u7b26\u53f7\u7ed3\u6784\u81ea\u53d1\u6d8c\u73b0\uff0c\u5e76\u6784\u5efa\u66f4\u5177\u53ef\u6269\u5c55\u6027\u3001\u53ef\u89e3\u91ca\u6027\u7684\u795e\u7ecf-\u7b26\u53f7\u7cfb\u7edf\uff0c\u63a8\u8fdb\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u4e2d\u7c7b\u8111\u67b6\u6784\u7684\u53d1\u5c55\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528\u5c40\u90e8\u3001\u4e8b\u4ef6\u9a71\u52a8\u7684\u81ea\u53d1\u5b66\u4e60\u3002\u6838\u5fc3\u662f\u4e00\u4e2a\u5206\u5c42Hopfield\u8bb0\u5fc6\u94fe\uff0c\u4f5c\u4e3a\u7ec4\u5408\u5f0f\u77ed\u671f\u8bb0\u5fc6\u548c\u52a8\u6001\u5206\u8bcd\u5668\uff08\u91cd\u5206\u8bcd\u5668\uff09\u3002\u6a21\u578b\u4e0d\u4f9d\u8d56\u9884\u5b9a\u4e49\u7b26\u53f7\u6216\u76d1\u7763\uff0c\u800c\u662f\u4ece\u5934\u5f00\u59cb\u6784\u5efa\u7ed3\u6784\uff0c\u5c06\u7b26\u53f7\u5e8f\u5217\u5b66\u4e60\u4e3a\u591a\u5c3a\u5ea6\u8868\u793a\u3002\u901a\u8fc7\u6784\u5efa\u6295\u5f71\u5f20\u91cf\u5c06\u5171\u73b0\u7279\u5f81\u7ed1\u5b9a\u4e3a\u5206\u5c42\u7b26\u53f7\uff0c\u5b9e\u73b0\u5c40\u90e8\u6fc0\u6d3b\u7684\u538b\u7f29\u4ee5\u5f62\u6210\u957f\u7a0b\u4f9d\u8d56\u3002\u5b66\u4e60\u8fc7\u7a0b\u662f\u5c40\u90e8\u7684\uff08\u8d6b\u5e03\u578b\uff09\uff0c\u6a21\u578b\u7684\u7ea6\u675f\u51b3\u5b9a\u4e86\u5141\u8bb8\u7684\u81ea\u53d1\u7ed3\u6784\uff0c\u65b0\u4fe1\u606f\u4e0e\u8be5\u7ed3\u6784\u5bf9\u9f50\u5e76\u4fdd\u7559\u3002\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u77ed\u6682\u6fc0\u6d3b\u65b0\u795e\u7ecf\u5143\uff0c\u5c06\u5206\u5e03\u5f0f\u591a\u5c3a\u5ea6\u7b26\u53f7\u7279\u5f81\u7ed1\u5b9a\u5230\u7b26\u53f7\u5d4c\u5165\u4e2d\uff0c\u8fd9\u4e9b\u81ea\u53d1\u5d4c\u5165\u795e\u7ecf\u5143\u5145\u5f53\u957f\u671f\u8bb0\u5fc6\uff0c\u652f\u6301\u7ec4\u5408\u63a8\u7406\u548c\u6cdb\u5316\u7684\u952e\u503c\u673a\u5236\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u91cd\u5206\u8bcd\u5668\u80fd\u591f\u4ece\u566a\u58f0\u4e2d\u8fc7\u6ee4\u81ea\u7136\u8bed\u8a00\u6a21\u5f0f\uff0c\u751f\u6210\u5177\u6709\u8fde\u8d2f\u5185\u90e8\u5f62\u6001\u7684\u5408\u6210\u8bed\u8a00\uff0c\u5176\u91cf\u5316\u6307\u6807\u4e0e\u4eba\u7c7b\u8bed\u8a00\u76f8\u540c\u3002\u7531\u4e8e\u6ca1\u6709\u5168\u5c40\u76ee\u6807\uff0c\u6a21\u578b\u5c55\u73b0\u51fa\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u4e2d\u4e0d\u5177\u5907\u7684\u53ef\u5851\u6027\uff0c\u5373\u4f7f\u6ca1\u6709\u660e\u786e\u6570\u636e\u4e5f\u80fd\u6cdb\u5316\u5230\u5176\u521d\u59cb\u63a8\u7406\u7c7b\u522b\u4e4b\u5916\u3002\u6b64\u5916\uff0c\u6a21\u578b\u8bc1\u660e\u4e86\u81ea\u53d1\u5d4c\u5165\u795e\u7ecf\u5143\uff08\u4f5c\u4e3a\u957f\u671f\u8bb0\u5fc6\uff09\u80fd\u591f\u652f\u6301\u7ec4\u5408\u5f0f\u63a8\u7406\u548c\u6cdb\u5316\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u7814\u7a76\u7b26\u53f7\u7ed3\u6784\u5982\u4f55\u4ece\u5c40\u90e8\u795e\u7ecf\u5b66\u4e60\u4e2d\u6d8c\u73b0\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u57fa\u7840\u3002\u5b83\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u795e\u7ecf-\u7b26\u53f7\u7cfb\u7edf\u5f00\u8f9f\u4e86\u4e00\u6761\u65b0\u9014\u5f84\uff0c\u5176\u4e2d\u7b26\u53f7\u3001\u8bed\u6cd5\u548c\u63a8\u7406\u90fd\u4f5c\u4e3aHopfield\u5c42\u6b21\u7ed3\u6784\u4e2d\u7684\u538b\u7f29\u8bb0\u5fc6\u75d5\u8ff9\u800c\u81ea\u53d1\u4ea7\u751f\u3002\u8fd9\u79cd\u65b9\u6cd5\u63a8\u52a8\u4e86\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u7c7b\u8111\u67b6\u6784\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.23025", "pdf": "https://arxiv.org/pdf/2506.23025", "abs": "https://arxiv.org/abs/2506.23025", "authors": ["Tejas Vaidhya", "Ayush Kaushal", "Vineet Jain", "Francis Couture Harpin", "Prashant Shishodia", "Majid Behbahani", "Yuriy Nevmyvaka", "Irina Rish"], "title": "Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly used across research and\nindustry applications, yet their inference efficiency remains a significant\nchallenge. As the computational power of modern GPU architectures continuously\nimproves, their memory bandwidth and capacity have not scaled proportionally,\ncreating a critical bottleneck during inference. To address this, we\ninvestigate ternary language models (TriLMs) that employ quantization-aware\ntraining to significantly reduce memory requirements. We first analyze the\nscalability of TriLMs by conducting a scaling law analysis, revealing that\nTriLMs benefit more from increasing training data than from scaling model\nparameters. Based on this observation, we introduce Spectra-1.1, an open suite\nof TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained\nperformance gains at scale. Furthermore, to improve inference efficiency, we\npropose novel 2-bit and 1.6-bit packing schemes for ternary weights, which\ndemonstrate accelerated inference across various CPU architectures. Also,\nbuilding on the 2-bit packing, we develop a GPU kernel called TriRun that\naccelerates end-to-end model inference by up to 5 times compared to\nfloating-point baselines. To encourage further exploration and development of\nTriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.\nOverall, our work lays the foundation for building and deploying efficient\nLLMs, providing a valuable resource for the research community.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u7814\u7a76\u5f15\u5165\u4e86\u4e09\u5143\u8bed\u8a00\u6a21\u578b\uff08TriLMs\uff09\uff0c\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u5927\u5e45\u51cf\u5c11\u5185\u5b58\u9700\u6c42\u3002\u901a\u8fc7\u5f00\u53d1\u6570\u636e\u89c4\u6a21\u66f4\u5927\u7684\u6a21\u578b\uff08Spectra-1.1\uff09\u548c\u4f18\u5316CPU/GPU\u63a8\u7406\u6838\uff08TriRun\uff09\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u63a8\u7406\u52a0\u901f\uff0c\u4e3a\u9ad8\u6548LLMs\u7684\u90e8\u7f72\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5176\u63a8\u7406\u6548\u7387\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u73b0\u4ee3GPU\u67b6\u6784\u7684\u8ba1\u7b97\u80fd\u529b\u6301\u7eed\u63d0\u5347\uff0c\u4f46\u5185\u5b58\u5e26\u5bbd\u548c\u5bb9\u91cf\u672a\u80fd\u6309\u6bd4\u4f8b\u6269\u5c55\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u9020\u6210\u4e86\u5173\u952e\u74f6\u9888\u3002", "method": ["\u63a2\u7d22\u5e76\u7814\u7a76\u4e09\u5143\u8bed\u8a00\u6a21\u578b\uff08TriLMs\uff09\uff0c\u91c7\u7528\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u4ee5\u663e\u8457\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u3002", "\u8fdb\u884c\u6807\u5ea6\u5f8b\u5206\u6790\uff0c\u63ed\u793aTriLMs\u4ece\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u4e2d\u83b7\u76ca\u591a\u4e8e\u589e\u52a0\u6a21\u578b\u53c2\u6570\u3002", "\u57fa\u4e8e\u6b64\u89c2\u5bdf\uff0c\u63a8\u51faSpectra-1.1\uff0c\u4e00\u5957\u5728\u9ad8\u8fbe1.2\u4e07\u4ebf\u4e2atoken\u4e0a\u8bad\u7ec3\u7684\u5f00\u653eTriLMs\u5957\u4ef6\u3002", "\u63d0\u51fa\u65b0\u9896\u76842\u6bd4\u7279\u548c1.6\u6bd4\u7279\u4e09\u5143\u6743\u91cd\u6253\u5305\u65b9\u6848\uff0c\u4ee5\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "\u5f00\u53d1\u540d\u4e3aTriRun\u7684GPU\u6838\uff0c\u8be5\u6838\u57fa\u4e8e2\u6bd4\u7279\u6253\u5305\u65b9\u6848\uff0c\u7528\u4e8e\u52a0\u901f\u7aef\u5230\u7aef\u6a21\u578b\u63a8\u7406\u3002"], "result": ["Spectra-1.1\u5728\u6269\u5927\u89c4\u6a21\u65f6\u5c55\u793a\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002", "\u65b0\u63d0\u51fa\u76842\u6bd4\u7279\u548c1.6\u6bd4\u7279\u6253\u5305\u65b9\u6848\u5728\u5404\u79cdCPU\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e86\u52a0\u901f\u63a8\u7406\u3002", "TriRun GPU\u6838\u5c06\u7aef\u5230\u7aef\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u76f8\u8f83\u4e8e\u6d6e\u70b9\u57fa\u7ebf\u63d0\u9ad8\u4e86\u591a\u8fbe5\u500d\u3002"], "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u6784\u5efa\u548c\u90e8\u7f72\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u901a\u8fc7\u63d0\u4f9bSpectra-1.1\u5957\u4ef6\u548cTriRun\u63a8\u7406\u6838\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\uff0c\u4ee5\u9f13\u52b1\u5bf9TriLMs\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u548c\u5f00\u53d1\u3002"}}
{"id": "2506.23992", "pdf": "https://arxiv.org/pdf/2506.23992", "abs": "https://arxiv.org/abs/2506.23992", "authors": ["Aditya Shrivastava", "Komal Gupta", "Shraddha Arora"], "title": "Harnessing AI Agents to Advance Research on Refugee Child Mental Health", "categories": ["cs.AI", "cs.ET"], "comment": "14 page , 2 image , 2 tables , accepted under 5th International\n  Conference on Innovations in Computational Intelligence and Computer Vision\n  (ICICV-2025)", "summary": "The international refugee crisis deepens, exposing millions of dis placed\nchildren to extreme psychological trauma. This research suggests a com pact,\nAI-based framework for processing unstructured refugee health data and\ndistilling knowledge on child mental health. We compare two Retrieval-Aug\nmented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to\ndetermine how well they process challenging humanitarian datasets while avoid\ning hallucination hazards. By combining cutting-edge AI methods with migration\nresearch and child psychology, this study presents a scalable strategy to\nassist policymakers, mental health practitioners, and humanitarian agencies to\nbetter assist displaced children and recognize their mental wellbeing. In\ntotal, both the models worked properly but significantly Deepseek R1 is\nsuperior to Zephyr with an accuracy of answer relevance 0.91", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u7d27\u51d1\u7684\u3001\u57fa\u4e8eAI\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u975e\u7ed3\u6784\u5316\u96be\u6c11\u5065\u5eb7\u6570\u636e\uff0c\u4ee5\u63d0\u53d6\u513f\u7ae5\u5fc3\u7406\u5065\u5eb7\u77e5\u8bc6\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e24\u79cdRAG\u6a21\u578b\uff08Zephyr-7B-beta\u548cDeepSeek R1-7B\uff09\u5728\u5904\u7406\u4eba\u9053\u4e3b\u4e49\u6570\u636e\u96c6\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u56fd\u9645\u96be\u6c11\u5371\u673a\u65e5\u76ca\u4e25\u5cfb\uff0c\u6570\u767e\u4e07\u6d41\u79bb\u5931\u6240\u7684\u513f\u7ae5\u9762\u4e34\u4e25\u91cd\u7684\u5fc3\u7406\u521b\u4f24\uff0c\u8feb\u5207\u9700\u8981\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u975e\u7ed3\u6784\u5316\u5065\u5eb7\u6570\u636e\u5e76\u83b7\u53d6\u513f\u7ae5\u5fc3\u7406\u5065\u5eb7\u77e5\u8bc6\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6bd4\u8f83Zephyr-7B-beta\u548cDeepSeek R1-7B\u4e24\u79cdRAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u7ba1\u9053\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684\u4eba\u9053\u4e3b\u4e49\u6570\u636e\u96c6\u65f6\u7684\u6548\u679c\u548c\u907f\u514d\u5e7b\u89c9\u7684\u80fd\u529b\u3002", "result": "\u4e24\u79cd\u6a21\u578b\u5747\u80fd\u6b63\u5e38\u5de5\u4f5c\uff0c\u4f46DeepSeek R1\u5728\u7b54\u6848\u76f8\u5173\u6027\u51c6\u786e\u7387\u4e0a\u663e\u8457\u4f18\u4e8eZephyr\uff0c\u8fbe\u52300.91\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684AI\u7b56\u7565\uff0c\u7ed3\u5408\u4e86\u5c16\u7aefAI\u65b9\u6cd5\u4e0e\u79fb\u6c11\u7814\u7a76\u548c\u513f\u7ae5\u5fc3\u7406\u5b66\uff0c\u65e8\u5728\u5e2e\u52a9\u653f\u7b56\u5236\u5b9a\u8005\u3001\u5fc3\u7406\u5065\u5eb7\u4ece\u4e1a\u8005\u548c\u4eba\u9053\u4e3b\u4e49\u673a\u6784\u66f4\u597d\u5730\u63f4\u52a9\u6d41\u79bb\u5931\u6240\u7684\u513f\u7ae5\uff0c\u5e76\u5173\u6ce8\u4ed6\u4eec\u7684\u5fc3\u7406\u5065\u5eb7\u3002"}}
{"id": "2506.22817", "pdf": "https://arxiv.org/pdf/2506.22817", "abs": "https://arxiv.org/abs/2506.22817", "authors": ["Xingyilang Yin", "Jiale Wang", "Xi Yang", "Mutian Xu", "Xu Gu", "Nannan Wang"], "title": "Unleashing the Multi-View Fusion Potential: Noise Correction in VLM for Open-Vocabulary 3D Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Recent open-vocabulary 3D scene understanding approaches mainly focus on\ntraining 3D networks through contrastive learning with point-text pairs or by\ndistilling 2D features into 3D models via point-pixel alignment. While these\nmethods show considerable performance in benchmarks with limited vocabularies,\nthey struggle to handle diverse object categories as the limited amount of 3D\ndata upbound training strong open-vocabulary 3d models. We observe that 2D\nmulti-view fusion methods take precedence in understanding diverse concepts in\n3D scenes. However, inherent noises in vision-language models lead multi-view\nfusion to sub-optimal performance. To this end, we introduce MVOV3D, a novel\napproach aimed at unleashing the potential of 2D multi-view fusion for\nopen-vocabulary 3D scene understanding. We focus on reducing the inherent\nnoises without training, thereby preserving the generalizability while\nenhancing open-world capabilities. Specifically, MVOV3D improves multi-view 2D\nfeatures by leveraging precise region-level image features and text features\nencoded by CLIP encoders and incorporates 3D geometric priors to optimize\nmulti-view fusion. Extensive experiments on various datasets demonstrate the\neffectiveness of our method. Notably, our MVOV3D achieves a new record with\n14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160 for challenge\nopen-vocabulary semantic segmentation, outperforming current leading trained 3D\nnetworks by a significant margin.", "AI": {"tldr": "MVOV3D\u63d0\u51fa\u4e00\u79cd\u521b\u65b0\u76842D\u591a\u89c6\u89d2\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u56fa\u6709\u566a\u58f0\uff0c\u5e76\u5728\u4e0d\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u533a\u57df\u7ea7\u7279\u5f81\u548c3D\u51e0\u4f55\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c473D\u573a\u666f\u7406\u89e3\u7684\u6027\u80fd\uff0c\u5728ScanNet200\u548cMatterport160\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c473D\u573a\u666f\u7406\u89e3\u65b9\u6cd5\u56e03D\u6570\u636e\u91cf\u6709\u9650\uff0c\u96be\u4ee5\u5904\u7406\u591a\u6837\u5316\u7684\u5bf9\u8c61\u7c7b\u522b\u3002\u5c3d\u7ba12D\u591a\u89c6\u89d2\u878d\u5408\u65b9\u6cd5\u5728\u7406\u89e33D\u573a\u666f\u4e2d\u7684\u591a\u6837\u6982\u5ff5\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u56fa\u6709\u7684\u566a\u58f0\u5bfc\u81f4\u5176\u6027\u80fd\u6b21\u4f18\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u964d\u4f4e\u566a\u58f0\u3001\u540c\u65f6\u4fdd\u7559\u6cdb\u5316\u80fd\u529b\u5e76\u589e\u5f3a\u5f00\u653e\u4e16\u754c\u80fd\u529b\u76842D\u591a\u89c6\u89d2\u878d\u5408\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51faMVOV3D\uff0c\u65e8\u5728\u901a\u8fc7\u51cf\u5c11\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u56fa\u6709\u566a\u58f0\u6765\u5145\u5206\u53d1\u63252D\u591a\u89c6\u89d2\u878d\u5408\u5728\u5f00\u653e\u8bcd\u6c473D\u573a\u666f\u7406\u89e3\u4e2d\u7684\u6f5c\u529b\uff0c\u4e14\u65e0\u9700\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u3002\u5177\u4f53\u800c\u8a00\uff0cMVOV3D\u5229\u7528CLIP\u7f16\u7801\u5668\u7f16\u7801\u7684\u7cbe\u786e\u533a\u57df\u7ea7\u56fe\u50cf\u7279\u5f81\u548c\u6587\u672c\u7279\u5f81\u6765\u6539\u8fdb\u591a\u89c6\u89d22D\u7279\u5f81\uff0c\u5e76\u7ed3\u54083D\u51e0\u4f55\u5148\u9a8c\u6765\u4f18\u5316\u591a\u89c6\u89d2\u878d\u5408\u8fc7\u7a0b\u3002", "result": "MVOV3D\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u5728\u6311\u6218\u6027\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\uff0cMVOV3D\u5728ScanNet200\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8614.7%\u7684mIoU\uff0c\u5728Matterport160\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8616.2%\u7684mIoU\uff0c\u5747\u521b\u4e0b\u65b0\u7eaa\u5f55\uff0c\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u9886\u5148\u7684\u57fa\u4e8e\u8bad\u7ec3\u76843D\u7f51\u7edc\u3002", "conclusion": "MVOV3D\u6210\u529f\u5c55\u793a\u4e862D\u591a\u89c6\u89d2\u878d\u5408\u5728\u5f00\u653e\u8bcd\u6c473D\u573a\u666f\u7406\u89e3\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u901a\u8fc7\u6709\u6548\u964d\u4f4e\u566a\u58f0\u5e76\u6574\u5408\u7cbe\u786e\u7279\u5f81\u4e0e3D\u51e0\u4f55\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u6a21\u578b\u5904\u7406\u591a\u6837\u53163D\u6982\u5ff5\u7684\u80fd\u529b\u3002"}}
{"id": "2506.23315", "pdf": "https://arxiv.org/pdf/2506.23315", "abs": "https://arxiv.org/abs/2506.23315", "authors": ["Shouvon Sarker", "Xishuang Dong", "Lijun Qian"], "title": "Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Identification of key variables such as medications, diseases, relations from\nhealth records and clinical notes has a wide range of applications in the\nclinical domain. n2c2 2022 provided shared tasks on challenges in natural\nlanguage processing for clinical data analytics on electronic health records\n(EHR), where it built a comprehensive annotated clinical data Contextualized\nMedication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of\nthis challenge that is to detect and classify medication events from clinical\nnotes through building a novel BERT-based ensemble model. It started with\npretraining BERT models on different types of big data such as Wikipedia and\nMIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED\ntraining data. These fine-tuned BERT models were employed to accomplish\nmedication event classification on CMED testing data with multiple predictions.\nThese multiple predictions generated by these fine-tuned BERT models were\nintegrated to build final prediction with voting strategies. Experimental\nresults demonstrated that BERT-based ensemble models can effectively improve\nstrict Micro-F score by about 5% and strict Macro-F score by about 6%,\nrespectively.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9n2c2 2022\u6311\u6218\u8d5b\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eBERT\u7684\u96c6\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u68c0\u6d4b\u548c\u5206\u7c7b\u836f\u7269\u4e8b\u4ef6\uff0c\u901a\u8fc7\u6574\u5408\u591a\u4e2aBERT\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u836f\u7269\u4e8b\u4ef6\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4ece\u5065\u5eb7\u8bb0\u5f55\u4e2d\u8bc6\u522b\u836f\u7269\u3001\u75be\u75c5\u7b49\u5173\u952e\u53d8\u91cf\u5728\u4e34\u5e8a\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3n2c2 2022\u6311\u6218\u8d5b\u4e2d\u4ece\u4e34\u5e8a\u7b14\u8bb0\u68c0\u6d4b\u548c\u5206\u7c7b\u836f\u7269\u4e8b\u4ef6\u7684\u5b50\u4efb\u52a1\uff0c\u4ee5\u63d0\u5347\u4e34\u5e8a\u6570\u636e\u5206\u6790\u7684NLP\u80fd\u529b\u3002", "method": "\u9996\u5148\u5728Wikipedia\u548cMIMIC\u7b49\u5927\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3BERT\u6a21\u578b\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u6a21\u578b\u5728CMED\u8bad\u7ec3\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002\u63a5\u7740\uff0c\u4f7f\u7528\u5fae\u8c03\u540e\u7684BERT\u6a21\u578b\u5bf9CMED\u6d4b\u8bd5\u6570\u636e\u8fdb\u884c\u836f\u7269\u4e8b\u4ef6\u5206\u7c7b\uff0c\u5e76\u91c7\u7528\u6295\u7968\u7b56\u7565\u6574\u5408\u591a\u4e2a\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u6784\u5efa\u6700\u7ec8\u7684\u96c6\u6210\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5BERT\u96c6\u6210\u6a21\u578b\u80fd\u6709\u6548\u63d0\u5347\u4e25\u683cMicro-F\u5206\u6570\u7ea65%\uff0c\u4e25\u683cMacro-F\u5206\u6570\u7ea66%\u3002", "conclusion": "\u57fa\u4e8eBERT\u7684\u96c6\u6210\u6a21\u578b\u80fd\u663e\u8457\u63d0\u9ad8\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u68c0\u6d4b\u548c\u5206\u7c7b\u836f\u7269\u4e8b\u4ef6\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u4e34\u5e8a\u6570\u636e\u5206\u6790\u63d0\u4f9b\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.23033", "pdf": "https://arxiv.org/pdf/2506.23033", "abs": "https://arxiv.org/abs/2506.23033", "authors": ["Yash Vardhan Tomar"], "title": "Feature-Wise Mixing for Mitigating Contextual Bias in Predictive Supervised Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Bias in predictive machine learning (ML) models is a fundamental challenge\ndue to the skewed or unfair outcomes produced by biased models. Existing\nmitigation strategies rely on either post-hoc corrections or rigid constraints.\nHowever, emerging research claims that these techniques can limit scalability\nand reduce generalizability. To address this, this paper introduces a\nfeature-wise mixing framework to mitigate contextual bias. This was done by\nredistributing feature representations across multiple contextual datasets. To\nassess feature-wise mixing's effectiveness, four ML classifiers were trained\nusing cross-validation and evaluated with bias-sensitive loss functions,\nincluding disparity metrics and mean squared error (MSE), which served as a\nstandard measure of predictive performance. The proposed method achieved an\naverage bias reduction of 43.35% and a statistically significant decrease in\nMSE across all classifiers trained on mixed datasets. Additionally,\nbenchmarking against established bias mitigation techniques found that\nfeature-wise mixing consistently outperformed SMOTE oversampling and\ndemonstrated competitive effectiveness without requiring explicit bias\nattribute identification. Feature-wise mixing efficiently avoids the\ncomputational overhead typically associated with fairness-aware learning\nalgorithms. Future work could explore applying feature-wise mixing for\nreal-world fields where accurate predictions are necessary.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7279\u5f81\u7ea7\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u7279\u5f81\u8868\u793a\u6765\u51cf\u8f7b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u4e0a\u4e0b\u6587\u504f\u89c1\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u504f\u89c1\u548c\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u5177\u6709\u8ba1\u7b97\u6548\u7387\u9ad8\u548c\u65e0\u9700\u663e\u5f0f\u8bc6\u522b\u504f\u89c1\u5c5e\u6027\u7684\u4f18\u70b9\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u662f\u5bfc\u81f4\u4e0d\u516c\u5e73\u7ed3\u679c\u7684\u6839\u672c\u6311\u6218\u3002\u73b0\u6709\u504f\u89c1\u7f13\u89e3\u7b56\u7565\uff08\u5982\u4e8b\u540e\u6821\u6b63\u6216\u4e25\u683c\u7ea6\u675f\uff09\u5728\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4fc3\u4f7f\u7814\u7a76\u4eba\u5458\u5bfb\u6c42\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u7279\u5f81\u7ea7\u6df7\u5408\uff08feature-wise mixing\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u591a\u4e2a\u4e0a\u4e0b\u6587\u6570\u636e\u96c6\u4e2d\u91cd\u65b0\u5206\u914d\u7279\u5f81\u8868\u793a\u6765\u51cf\u8f7b\u4e0a\u4e0b\u6587\u504f\u89c1\u3002\u4e3a\u8bc4\u4f30\u5176\u6709\u6548\u6027\uff0c\u4f7f\u7528\u4ea4\u53c9\u9a8c\u8bc1\u8bad\u7ec3\u4e86\u56db\u79cdML\u5206\u7c7b\u5668\uff0c\u5e76\u91c7\u7528\u504f\u89c1\u654f\u611f\u635f\u5931\u51fd\u6570\uff08\u5305\u62ec\u5dee\u5f02\u5ea6\u91cf\u548c\u5747\u65b9\u8bef\u5deeMSE\uff09\u8fdb\u884c\u8bc4\u4f30\u3002\u540c\u65f6\uff0c\u4e0eSMOTE\u8fc7\u91c7\u6837\u7b49\u73b0\u6709\u504f\u89c1\u7f13\u89e3\u6280\u672f\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5e73\u5747\u504f\u89c1\u51cf\u5c11\u4e8643.35%\uff0c\u5e76\u4e14\u5728\u6240\u6709\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u7684MSE\uff08\u9884\u6d4b\u6027\u80fd\u6807\u51c6\u8861\u91cf\uff09\u90fd\u5448\u73b0\u7edf\u8ba1\u5b66\u4e0a\u7684\u663e\u8457\u4e0b\u964d\u3002\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8eSMOTE\u8fc7\u91c7\u6837\uff0c\u5e76\u5728\u65e0\u9700\u660e\u786e\u8bc6\u522b\u504f\u89c1\u5c5e\u6027\u7684\u60c5\u51b5\u4e0b\u5c55\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u6709\u6548\u907f\u514d\u4e86\u516c\u5e73\u611f\u77e5\u5b66\u4e60\u7b97\u6cd5\u901a\u5e38\u76f8\u5173\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u7279\u5f81\u7ea7\u6df7\u5408\u6846\u67b6\u662f\u4e00\u79cd\u6709\u6548\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u504f\u89c1\u7f13\u89e3\u7b56\u7565\uff0c\u5b83\u4e0d\u4ec5\u80fd\u663e\u8457\u964d\u4f4e\u504f\u89c1\uff0c\u8fd8\u80fd\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u8bc6\u522b\u654f\u611f\u504f\u89c1\u5c5e\u6027\u3002\u672a\u6765\u5de5\u4f5c\u53ef\u63a2\u7d22\u5c06\u5176\u5e94\u7528\u4e8e\u9700\u8981\u7cbe\u786e\u9884\u6d4b\u7684\u771f\u5b9e\u4e16\u754c\u9886\u57df\u3002"}}
{"id": "2506.24026", "pdf": "https://arxiv.org/pdf/2506.24026", "abs": "https://arxiv.org/abs/2506.24026", "authors": ["Yongyi Wang", "Wenxin Li"], "title": "Constructing Non-Markovian Decision Process via History Aggregator", "categories": ["cs.AI"], "comment": null, "summary": "In the domain of algorithmic decision-making, non-Markovian dynamics manifest\nas a significant impediment, especially for paradigms such as Reinforcement\nLearning (RL), thereby exerting far-reaching consequences on the advancement\nand effectiveness of the associated systems. Nevertheless, the existing\nbenchmarks are deficient in comprehensively assessing the capacity of decision\nalgorithms to handle non-Markovian dynamics. To address this deficiency, we\nhave devised a generalized methodology grounded in category theory. Notably, we\nestablished the category of Markov Decision Processes (MDP) and the category of\nnon-Markovian Decision Processes (NMDP), and proved the equivalence\nrelationship between them. This theoretical foundation provides a novel\nperspective for understanding and addressing non-Markovian dynamics. We further\nintroduced non-Markovianity into decision-making problem settings via the\nHistory Aggregator for State (HAS). With HAS, we can precisely control the\nstate dependency structure of decision-making problems in the time series. Our\nanalysis demonstrates the effectiveness of our method in representing a broad\nrange of non-Markovian dynamics. This approach facilitates a more rigorous and\nflexible evaluation of decision algorithms by testing them in problem settings\nwhere non-Markovian dynamics are explicitly constructed.", "AI": {"tldr": "\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\u963b\u788d\u5f3a\u5316\u5b66\u4e60\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u8303\u7574\u8bba\uff08MDP\u4e0eNMDP\u7b49\u4ef7\u6027\uff09\u548c\u72b6\u6001\u5386\u53f2\u805a\u5408\u5668\uff08HAS\uff09\u7684\u65b9\u6cd5\uff0c\u7cbe\u786e\u6784\u5efa\u975e\u9a6c\u5c14\u53ef\u592b\u95ee\u9898\uff0c\u4ee5\u66f4\u4e25\u683c\u5730\u8bc4\u4f30\u51b3\u7b56\u7b97\u6cd5\u3002", "motivation": "\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\u5bf9\u7b97\u6cd5\u51b3\u7b56\uff08\u7279\u522b\u662f\u5f3a\u5316\u5b66\u4e60\uff09\u6784\u6210\u91cd\u5927\u969c\u788d\uff0c\u5f71\u54cd\u7cfb\u7edf\u6548\u80fd\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u51b3\u7b56\u7b97\u6cd5\u5904\u7406\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8303\u7574\u8bba\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08NMDP\uff09\u8303\u7574\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u7b49\u4ef7\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5f15\u5165\u72b6\u6001\u5386\u53f2\u805a\u5408\u5668\uff08HAS\uff09\uff0c\u5c06\u975e\u9a6c\u5c14\u53ef\u592b\u6027\u5f15\u5165\u51b3\u7b56\u95ee\u9898\u8bbe\u7f6e\u4e2d\uff0c\u4ece\u800c\u7cbe\u786e\u63a7\u5236\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u72b6\u6001\u4f9d\u8d56\u7ed3\u6784\u3002", "result": "\u8be5\u7406\u8bba\u57fa\u7840\u4e3a\u7406\u89e3\u548c\u5904\u7406\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002\u5176\u65b9\u6cd5\uff08\u7ed3\u5408HAS\uff09\u88ab\u8bc1\u660e\u80fd\u6709\u6548\u8868\u793a\u5e7f\u6cdb\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u6784\u5efa\u5305\u542b\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u4fc3\u8fdb\u5bf9\u51b3\u7b56\u7b97\u6cd5\u66f4\u4e25\u683c\u548c\u7075\u6d3b\u7684\u8bc4\u4f30\uff0c\u4ece\u800c\u5f25\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2506.22819", "pdf": "https://arxiv.org/pdf/2506.22819", "abs": "https://arxiv.org/abs/2506.22819", "authors": ["Ramya Hebbalaguppe", "Tamoghno Kandar", "Abhinav Nagpal", "Chetan Arora"], "title": "Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration", "categories": ["cs.CV", "cs.LG"], "comment": "26 pages", "summary": "Vision-language models (VLM) have demonstrated impressive performance in\nimage recognition by leveraging self-supervised training on large datasets.\nTheir performance can be further improved by adapting to the test sample using\ntest-time prompt tuning (TPT). Unfortunately, the singular focus of TPT\napproaches on improving the accuracy suffers from tunnel vision, and leads to\ndegradation in confidence calibration. This limits the applicability of TPT in\ncritical applications.\n  We make three contributions in this work. (1) We posit that random or naive\ninitialization of prompts leads to overfitting on a particular test sample, and\nis the main reason for miscalibration of the VLM after TPT. To mitigate the\nproblem, we propose careful initialization of test time prompt using prior\nknowledge about the target label attributes from a large language model (LLM);\n(2) To further maintain the quality of prompts during \\tpt, we propose a novel\nregularization loss to reduce intraclass distance, and increase inter-class\ndistance between the learnt\n  Through extensive experiments on different CLIP architectures and 15\ndatasets, we show that our approach can effectively improve the calibration\nafter TPT. We report an average expected calibration error (ECE) of 4.11 with\nour method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24),\n6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is\npublicly accessible at:\nhttps://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aTCA\u7684\u65b0\u578b\u6d4b\u8bd5\u65f6\u63d0\u793a\u5fae\u8c03\uff08TPT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63d0\u793a\u521d\u59cb\u5316\u548c\u5f15\u5165\u65b0\u7684\u6b63\u5219\u5316\u635f\u5931\uff0c\u663e\u8457\u6539\u5584\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728TPT\u540e\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7ed3\u5408\u6d4b\u8bd5\u65f6\u63d0\u793a\u5fae\u8c03\uff08TPT\uff09\u80fd\u63d0\u9ad8\u56fe\u50cf\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u4f46\u5374\u4f1a\u635f\u5bb3\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002\u4f5c\u8005\u8ba4\u4e3a\u968f\u673a\u6216\u6734\u7d20\u7684\u63d0\u793a\u521d\u59cb\u5316\u662f\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u8bef\u6821\u51c6\u7684\u4e3b\u8981\u539f\u56e0\u3002", "method": "1. \u63d0\u51fa\u4f7f\u7528\u6765\u81ea\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u76ee\u6807\u6807\u7b7e\u5c5e\u6027\u5148\u9a8c\u77e5\u8bc6\uff0c\u5bf9\u6d4b\u8bd5\u65f6\u63d0\u793a\u8fdb\u884c\u7cbe\u7ec6\u521d\u59cb\u5316\u30022. \u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6b63\u5219\u5316\u635f\u5931\uff0c\u4ee5\u5728TPT\u8fc7\u7a0b\u4e2d\u51cf\u5c11\u7c7b\u5185\u8ddd\u79bb\u5e76\u589e\u52a0\u7c7b\u95f4\u8ddd\u79bb\uff0c\u4ece\u800c\u7ef4\u62a4\u63d0\u793a\u8d28\u91cf\u3002", "result": "\u5728\u591a\u79cdCLIP\u67b6\u6784\u548c15\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4f5c\u8005\u63d0\u51fa\u7684TCA\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u5584TPT\u540e\u7684\u6821\u51c6\u3002TCA\u7684\u5e73\u5747\u9884\u671f\u6821\u51c6\u8bef\u5dee\uff08ECE\uff09\u4e3a4.11\uff0c\u660e\u663e\u4f18\u4e8e\u4f20\u7edfTPT\uff0811.7\uff09\u3001C-TPT\uff086.12\uff09\u3001DiffTPT\uff086.78\uff09\u548cPromptAlign\uff088.43\uff09\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684TCA\u65b9\u6cd5\u901a\u8fc7\u6539\u8fdb\u63d0\u793a\u521d\u59cb\u5316\u548c\u5f15\u5165\u6b63\u5219\u5316\u635f\u5931\uff0c\u6210\u529f\u89e3\u51b3\u4e86VLM\u5728\u6d4b\u8bd5\u65f6\u63d0\u793a\u5fae\u8c03\u540e\u7f6e\u4fe1\u5ea6\u6821\u51c6\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u4f7f\u5f97TPT\u5728\u5173\u952e\u5e94\u7528\u4e2d\u66f4\u5177\u9002\u7528\u6027\u3002"}}
{"id": "2506.23340", "pdf": "https://arxiv.org/pdf/2506.23340", "abs": "https://arxiv.org/abs/2506.23340", "authors": ["Yumeng Lin", "Xufeng Duan", "David Haslett", "Yige Chen", "Zhenguang G. Cai"], "title": "Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family", "categories": ["cs.CL"], "comment": null, "summary": "Large language models have achieved impressive progress in multilingual\ntranslation, yet they continue to face challenges with certain language\npairs-particularly those with limited training data or significant linguistic\ndivergence from English. This study systematically investigates how training\ndata, language proximity, and language family affect information loss in\nmultilingual translation. We evaluate two large language models, GPT-4 and\nLlama 2, by performing round-trip translations. Translation quality was\nassessed using BLEU scores and BERT similarity metrics. Our results reveal a\nrobust interaction between training data size and language distance: while\nabundant training data can mitigate the effects of linguistic divergence,\nlanguages structurally closer to English consistently yield higher translation\nquality in low-resource conditions. Among various distance metrics,\northographic, phylogenetic, syntactic, and geographical distances emerge as\nstrong predictors of translation performance. Language family also exerts an\nindependent influence. These findings contribute to a deeper understanding of\nthe linguistic constraints shaping multilingual translation in large language\nmodels, emphasizing that translation quality is shaped not only by data volume\nbut also by structural and typological relationships between languages.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u63a2\u7a76\u4e86\u8bad\u7ec3\u6570\u636e\u3001\u8bed\u8a00\u4eb2\u8fd1\u6027\u53ca\u8bed\u7cfb\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u591a\u8bed\u8a00\u7ffb\u8bd1\u4fe1\u606f\u635f\u5931\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7ffb\u8bd1\u8d28\u91cf\u53d7\u6570\u636e\u91cf\u3001\u8bed\u8a00\u7ed3\u6784\u548c\u7c7b\u578b\u5b66\u5173\u7cfb\u5171\u540c\u5851\u9020\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u9762\u4e34\u7279\u5b9a\u8bed\u5bf9\uff08\u6570\u636e\u6709\u9650\u6216\u4e0e\u82f1\u8bed\u5dee\u5f02\u5927\uff09\u7684\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u63a2\u7a76\u8bad\u7ec3\u6570\u636e\u3001\u8bed\u8a00\u4eb2\u8fd1\u6027\u548c\u8bed\u7cfb\u5982\u4f55\u5f71\u54cd\u591a\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u7684\u4fe1\u606f\u635f\u5931\u3002", "method": "\u901a\u8fc7\u5bf9GPT-4\u548cLlama 2\u8fdb\u884c\u5f80\u8fd4\u7ffb\u8bd1\u8bc4\u4f30\uff0c\u4f7f\u7528BLEU\u5206\u6570\u548cBERT\u76f8\u4f3c\u5ea6\u6307\u6807\u8861\u91cf\u7ffb\u8bd1\u8d28\u91cf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bad\u7ec3\u6570\u636e\u5927\u5c0f\u4e0e\u8bed\u8a00\u8ddd\u79bb\u5b58\u5728\u663e\u8457\u4ea4\u4e92\uff1a\u5145\u8db3\u6570\u636e\u53ef\u7f13\u89e3\u8bed\u8a00\u5dee\u5f02\uff0c\u4f46\u4e0e\u82f1\u8bed\u7ed3\u6784\u66f4\u8fd1\u7684\u8bed\u8a00\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u7ffb\u8bd1\u8d28\u91cf\u66f4\u9ad8\u3002\u6b63\u5b57\u6cd5\u3001\u7cfb\u7edf\u53d1\u80b2\u3001\u53e5\u6cd5\u548c\u5730\u7406\u8ddd\u79bb\u662f\u7ffb\u8bd1\u6027\u80fd\u7684\u5f3a\u9884\u6d4b\u56e0\u5b50\uff0c\u8bed\u7cfb\u4e5f\u6709\u72ec\u7acb\u5f71\u54cd\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ffb\u8bd1\u8d28\u91cf\u4e0d\u4ec5\u53d7\u6570\u636e\u91cf\u5f71\u54cd\uff0c\u8fd8\u53d7\u8bed\u8a00\u95f4\u7684\u7ed3\u6784\u548c\u7c7b\u578b\u5b66\u5173\u7cfb\u5f71\u54cd\uff0c\u52a0\u6df1\u4e86\u5bf9\u591a\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u8bed\u8a00\u7ea6\u675f\u7684\u7406\u89e3\u3002"}}
{"id": "2506.23036", "pdf": "https://arxiv.org/pdf/2506.23036", "abs": "https://arxiv.org/abs/2506.23036", "authors": ["Zain ul Abdeen", "Ming Jin"], "title": "Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper explores Reinforcement learning (RL) policy robustness by\nsystematically analyzing network parameters under internal and external\nstresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering\nintroduces internal stress by selectively perturbing parameters, while\nadversarial attacks apply external stress through modified agent observations.\nThis dual approach enables the classification of parameters as fragile, robust,\nor antifragile, based on their influence on policy performance in clean and\nadversarial settings. Parameter scores are defined to quantify these\ncharacteristics, and the framework is validated on PPO-trained agents in Mujoco\ncontinuous control environments. The results highlight the presence of\nantifragile parameters that enhance policy performance under stress,\ndemonstrating the potential of targeted filtering techniques to improve RL\npolicy adaptability. These insights provide a foundation for future\nadvancements in the design of robust and antifragile RL systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5185\u90e8\uff08\u7a81\u89e6\u8fc7\u6ee4\uff09\u548c\u5916\u90e8\uff08\u5bf9\u6297\u6027\u653b\u51fb\uff09\u538b\u529b\u5206\u6790RL\u7b56\u7565\u7684\u9c81\u68d2\u6027\uff0c\u5c06\u7f51\u7edc\u53c2\u6570\u5206\u7c7b\u4e3a\u8106\u5f31\u3001\u9c81\u68d2\u6216\u53cd\u8106\u5f31\uff0c\u5e76\u53d1\u73b0\u53cd\u8106\u5f31\u53c2\u6570\u80fd\u5728\u538b\u529b\u4e0b\u63d0\u9ad8\u6027\u80fd\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u5177\u9002\u5e94\u6027\u7684RL\u7cfb\u7edf\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5206\u6790\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b56\u7565\u7684\u9c81\u68d2\u6027\uff0c\u63a2\u7a76\u7f51\u7edc\u53c2\u6570\u5728\u5185\u90e8\u548c\u5916\u90e8\u538b\u529b\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u53d7\u795e\u7ecf\u79d1\u5b66\u4e2d\u7a81\u89e6\u53ef\u5851\u6027\u7684\u542f\u53d1\uff0c\u5bf9\u53c2\u6570\u8fdb\u884c\u5206\u7c7b\uff0c\u4ee5\u671f\u4e3a\u672a\u6765\u8bbe\u8ba1\u66f4\u5065\u58ee\u548c\u53cd\u8106\u5f31\u7684RL\u7cfb\u7edf\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u7814\u7a76\u91c7\u7528\u53cc\u91cd\u65b9\u6cd5\uff1a\u901a\u8fc7\u7a81\u89e6\u8fc7\u6ee4\uff08\u9009\u62e9\u6027\u6270\u52a8\u53c2\u6570\uff09\u5f15\u5165\u5185\u90e8\u538b\u529b\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u6027\u653b\u51fb\uff08\u4fee\u6539\u667a\u80fd\u4f53\u89c2\u6d4b\uff09\u65bd\u52a0\u5916\u90e8\u538b\u529b\u3002\u6839\u636e\u53c2\u6570\u5728\u5e72\u51c0\u548c\u5bf9\u6297\u6027\u8bbe\u7f6e\u4e0b\u5bf9\u7b56\u7565\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5c06\u53c2\u6570\u5206\u7c7b\u4e3a\u8106\u5f31\u3001\u9c81\u68d2\u6216\u53cd\u8106\u5f31\u3002\u5b9a\u4e49\u53c2\u6570\u5206\u6570\u6765\u91cf\u5316\u8fd9\u4e9b\u7279\u6027\uff0c\u5e76\u5728Mujoco\u8fde\u7eed\u63a7\u5236\u73af\u5883\u4e2d\u4f7f\u7528PPO\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u53cd\u8106\u5f31\u53c2\u6570\u7684\u5b58\u5728\uff0c\u8fd9\u4e9b\u53c2\u6570\u5728\u538b\u529b\u4e0b\u80fd\u591f\u589e\u5f3a\u7b56\u7565\u6027\u80fd\u3002\u8fd9\u8868\u660e\u6709\u9488\u5bf9\u6027\u7684\u8fc7\u6ee4\u6280\u672f\u6709\u6f5c\u529b\u63d0\u9ad8RL\u7b56\u7565\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u7684\u89c1\u89e3\u4e3a\u672a\u6765\u8bbe\u8ba1\u66f4\u5065\u58ee\u548c\u53cd\u8106\u5f31\u7684RL\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u8bc6\u522b\u5e76\u5229\u7528\u53cd\u8106\u5f31\u53c2\u6570\u7684\u7279\u6027\uff0c\u6709\u671b\u663e\u8457\u63d0\u5347RL\u7b56\u7565\u5728\u590d\u6742\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2506.24119", "pdf": "https://arxiv.org/pdf/2506.24119", "abs": "https://arxiv.org/abs/2506.24119", "authors": ["Bo Liu", "Leon Guertler", "Simon Yu", "Zichen Liu", "Penghui Qi", "Daniel Balcells", "Mickel Liu", "Cheston Tan", "Weiyan Shi", "Min Lin", "Wee Sun Lee", "Natasha Jaques"], "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Work in Progress", "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.", "AI": {"tldr": "SPIRAL\u662f\u4e00\u4e2a\u81ea\u535a\u5f08\u6846\u67b6\uff0c\u901a\u8fc7\u96f6\u548c\u6e38\u620f\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u5e76\u4f7f\u5176\u53d1\u5c55\u51fa\u53ef\u8fc1\u79fb\u7684\u8ba4\u77e5\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u95ee\u9898-\u7b54\u6848\u5bf9\u548c\u9886\u57df\u7279\u5b9a\u5956\u52b1\u5de5\u7a0b\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u6269\u5c55\u6027\u53d7\u9650\uff0c\u963b\u788d\u4e86\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "\u63d0\u51faSPIRAL\u81ea\u535a\u5f08\u6846\u67b6\uff0c\u6a21\u578b\u901a\u8fc7\u4e0e\u6301\u7eed\u6539\u8fdb\u7684\u81ea\u8eab\u7248\u672c\u8fdb\u884c\u591a\u8f6e\u96f6\u548c\u6e38\u620f\u6765\u5b66\u4e60\u3002\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5b8c\u5168\u5728\u7ebf\u3001\u591a\u8f6e\u3001\u591a\u667a\u80fd\u4f53LLM\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\uff0c\u5e76\u5f15\u5165\u89d2\u8272\u6761\u4ef6\u4f18\u52bf\u4f30\u8ba1\uff08RAE\uff09\u4ee5\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u5728Kuhn Poker\u4e0a\u8bad\u7ec3Qwen3-4B-Base\uff0c\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u80fd\u529b\u5206\u522b\u63d0\u53478.6%\u548c8.4%\uff0c\u4f18\u4e8e\u4e13\u5bb6\u8f68\u8ff9\u4e0a\u7684SFT\u3002\u5206\u6790\u663e\u793a\u63a8\u7406\u80fd\u529b\u901a\u8fc7\u7cfb\u7edf\u5206\u89e3\u3001\u671f\u671b\u503c\u8ba1\u7b97\u548c\u9010\u6848\u4f8b\u5206\u6790\u7b49\u8ba4\u77e5\u6a21\u5f0f\u5b9e\u73b0\u8fc1\u79fb\u3002\u591a\u6e38\u620f\u8bad\u7ec3\u80fd\u8fdb\u4e00\u6b65\u589e\u5f3a\u6027\u80fd\u3002\u5e94\u7528\u4e8e\u5f3a\u63a8\u7406\u6a21\u578b\uff08DeepSeek-R1-Distill-Qwen-7B\uff09\u4ecd\u80fd\u5e26\u67652.0%\u7684\u5e73\u5747\u63d0\u5347\u3002", "conclusion": "\u96f6\u548c\u6e38\u620f\u80fd\u591f\u81ea\u7136\u5730\u57f9\u517b\u53ef\u8fc1\u79fb\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u4e3b\u63a8\u7406\u53d1\u5c55\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2506.22832", "pdf": "https://arxiv.org/pdf/2506.22832", "abs": "https://arxiv.org/abs/2506.22832", "authors": ["Alexander Gambashidze", "Li Pengyi", "Matvey Skripkin", "Andrey Galichin", "Anton Gusarov", "Konstantin Sobolev", "Andrey Kuznetsov", "Ivan Oseledets"], "title": "Listener-Rewarded Thinking in VLMs for Image Preferences", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Training robust and generalizable reward models for human visual preferences\nis essential for aligning text-to-image and text-to-video generative models\nwith human intent. However, current reward models often fail to generalize, and\nsupervised fine-tuning leads to memorization, demanding complex annotation\npipelines. While reinforcement learning (RL), specifically Group Relative\nPolicy Optimization (GRPO), improves generalization, we uncover a key failure\nmode: a significant drop in reasoning accuracy occurs when a model's reasoning\ntrace contradicts that of an independent, frozen vision-language model\n(\"listener\") evaluating the same output. To address this, we introduce a\nlistener-augmented GRPO framework. Here, the listener re-evaluates the\nreasoner's chain-of-thought to provide a dense, calibrated confidence score,\nshaping the RL reward signal. This encourages the reasoner not only to answer\ncorrectly, but to produce explanations that are persuasive to an independent\nmodel. Our listener-shaped reward scheme achieves best accuracy on the\nImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)\nperformance on a large-scale human preference dataset (1.2M votes, up to +6%\nover naive reasoner), and reduces reasoning contradictions compared to strong\nGRPO and SFT baselines. These results demonstrate that listener-based rewards\nprovide a scalable, data-efficient path to aligning vision-language models with\nnuanced human preferences. We will release our reasoning model here:\nhttps://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u542c\u4f17\u589e\u5f3a\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u63a8\u7406\u94fe\u6765\u5851\u9020\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u504f\u597d\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u4e3a\u4f7f\u6587\u751f\u56fe/\u89c6\u9891\u6a21\u578b\u4e0e\u4eba\u7c7b\u610f\u56fe\u5bf9\u9f50\uff0c\u9700\u9c81\u68d2\u4e14\u6cdb\u5316\u7684\u89c6\u89c9\u504f\u597d\u5956\u52b1\u6a21\u578b\u3002\u73b0\u6709\u6a21\u578b\u6cdb\u5316\u6027\u5dee\uff0cSFT\u6613\u5bfc\u81f4\u8bb0\u5fc6\u5316\u3002\u5f3a\u5316\u5b66\u4e60\uff08\u5982GRPO\uff09\u867d\u6539\u5584\u6cdb\u5316\uff0c\u4f46\u5b58\u5728\u6838\u5fc3\u7f3a\u9677\uff1a\u5f53\u63a8\u7406\u6a21\u578b\u4e0e\u72ec\u7acb\u89c6\u89c9-\u8bed\u8a00\u201c\u542c\u4f17\u201d\u7684\u5224\u65ad\u51b2\u7a81\u65f6\uff0c\u63a8\u7406\u51c6\u786e\u6027\u4f1a\u663e\u8457\u4e0b\u964d\u3002", "method": "\u5f15\u5165\u201c\u542c\u4f17\u589e\u5f3a\u578bGRPO\u6846\u67b6\u201d\u3002\u8be5\u6846\u67b6\u4e2d\uff0c\u4e00\u4e2a\u72ec\u7acb\u7684\u201c\u542c\u4f17\u201d\u6a21\u578b\u91cd\u65b0\u8bc4\u4f30\u63a8\u7406\u6a21\u578b\u7684\u601d\u7ef4\u94fe\uff0c\u63d0\u4f9b\u6821\u51c6\u7684\u7f6e\u4fe1\u5206\u6570\uff0c\u6b64\u5206\u6570\u7528\u4e8e\u5851\u9020RL\u5956\u52b1\u4fe1\u53f7\u3002\u8fd9\u4fc3\u4f7f\u63a8\u7406\u6a21\u578b\u4e0d\u4ec5\u7ed9\u51fa\u6b63\u786e\u7b54\u6848\uff0c\u8fd8\u80fd\u751f\u6210\u5bf9\u72ec\u7acb\u6a21\u578b\u6709\u8bf4\u670d\u529b\u7684\u89e3\u91ca\u3002", "result": "\u5728ImageReward\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u4f73\u51c6\u786e\u7387\uff0867.4%\uff09\uff1b\u5728\u5305\u542b120\u4e07\u7968\u7684\u5927\u89c4\u6a21\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\u4e0a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57df\u5916\uff08OOD\uff09\u6027\u80fd\uff08\u76f8\u8f83\u4e8e\u7b80\u5355\u63a8\u7406\u5668\uff0c\u63d0\u5347\u9ad8\u8fbe6%\uff09\uff1b\u4e0e\u5f3aGRPO\u548cSFT\u57fa\u7ebf\u76f8\u6bd4\uff0c\u51cf\u5c11\u4e86\u63a8\u7406\u77db\u76fe\u3002", "conclusion": "\u57fa\u4e8e\u542c\u4f17\u7684\u5956\u52b1\u673a\u5236\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u3001\u6570\u636e\u9ad8\u6548\u7684\u9014\u5f84\uff0c\u7528\u4e8e\u5c06\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0e\u7ec6\u81f4\u5165\u5fae\u7684\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3002"}}
{"id": "2506.23342", "pdf": "https://arxiv.org/pdf/2506.23342", "abs": "https://arxiv.org/abs/2506.23342", "authors": ["Akim Tsvigun", "Daniil Vasilev", "Ivan Tsvigun", "Ivan Lysenko", "Talgat Bektleuov", "Aleksandr Medvedev", "Uliana Vinogradova", "Nikita Severin", "Mikhail Mozikov", "Andrey Savchenko", "Rostislav Grigorev", "Ramil Kuleev", "Fedor Zhdanov", "Artem Shelmanov", "Ilya Makarov"], "title": "ATGen: A Framework for Active Text Generation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 System Demonstrations", "summary": "Active learning (AL) has demonstrated remarkable potential in reducing the\nannotation effort required for training machine learning models. However,\ndespite the surging popularity of natural language generation (NLG) tasks in\nrecent years, the application of AL to NLG has been limited. In this paper, we\nintroduce Active Text Generation (ATGen) - a comprehensive framework that\nbridges AL with text generation tasks, enabling the application of\nstate-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered\nannotation in NLG tasks using both human annotators and automatic annotation\nagents based on large language models (LLMs). The framework supports LLMs\ndeployed as services, such as ChatGPT and Claude, or operated on-premises.\nFurthermore, ATGen provides a unified platform for smooth implementation and\nbenchmarking of novel AL strategies tailored to NLG tasks. Finally, we present\nevaluation results for state-of-the-art AL strategies across diverse settings\nand multiple text generation tasks. We show that ATGen reduces both the effort\nof human annotators and costs associated with API calls to LLM-based annotation\nagents. The code of the framework is available on GitHub under the MIT license.\nThe video presentation is available at http://atgen-video.nlpresearch.group", "AI": {"tldr": "\u4e00\u4e2a\u540d\u4e3aATGen\u7684\u6846\u67b6\uff0c\u5c06\u6d3b\u6027\u5b66\u4e60\u5e94\u7528\u4e8e\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u4ee5\u964d\u4f4e\u4eba\u5de5\u548cLLM\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u7136\u8bed\u8a00\u751f\u6210\uff08NLG\uff09\u4efb\u52a1\u65e5\u76ca\u6d41\u884c\uff0c\u4f46\u6d3b\u6027\u5b66\u4e60\uff08AL\uff09\u5728\u5176\u4e0a\u7684\u5e94\u7528\u5374\u5f88\u6709\u9650\uff0c\u800cAL\u5728\u51cf\u5c11\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u7684\u6807\u6ce8\u5de5\u4f5c\u91cf\u65b9\u9762\u6f5c\u529b\u5de8\u5927\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aActive Text Generation (ATGen) \u7684\u7efc\u5408\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06AL\u4e0e\u6587\u672c\u751f\u6210\u4efb\u52a1\u8fde\u63a5\u8d77\u6765\uff0c\u652f\u6301\u4f7f\u7528\u4eba\u5de5\u6807\u6ce8\u5668\u548c\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u6807\u6ce8\u4ee3\u7406\u8fdb\u884c\u6807\u6ce8\u3002ATGen\u8fd8\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u5e73\u53f0\uff0c\u7528\u4e8e\u5b9e\u73b0\u548c\u57fa\u51c6\u6d4b\u8bd5\u9488\u5bf9NLG\u4efb\u52a1\u7684\u65b0\u578bAL\u7b56\u7565\u3002", "result": "\u5728\u4e0d\u540c\u8bbe\u7f6e\u548c\u591a\u4e2a\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684AL\u7b56\u7565\uff0c\u7ed3\u679c\u8868\u660eATGen\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u5de5\u6807\u6ce8\u5668\u7684\u5de5\u4f5c\u91cf\u4ee5\u53ca\u8c03\u7528LLM-based\u6807\u6ce8\u4ee3\u7406\u7684API\u6210\u672c\u3002", "conclusion": "ATGen\u6846\u67b6\u6210\u529f\u5730\u5c06\u6d3b\u6027\u5b66\u4e60\u5e94\u7528\u4e8e\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u6a21\u578b\u8bad\u7ec3\u6240\u9700\u7684\u6807\u6ce8\u6210\u672c\uff0c\u4e3aNLG\u9886\u57dfAL\u7b56\u7565\u7684\u5f00\u53d1\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u7edf\u4e00\u5e73\u53f0\u3002"}}
{"id": "2506.23041", "pdf": "https://arxiv.org/pdf/2506.23041", "abs": "https://arxiv.org/abs/2506.23041", "authors": ["Chengyu Dong", "Huan Gui", "Noveen Sachdeva", "Long Jin", "Ke Yin", "Jingbo Shang", "Lichan Hong", "Ed H. Chi", "Zhe Zhao"], "title": "ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Knowledge distillation from pretrained visual representation models offers an\neffective approach to improve small, task-specific production models. However,\nthe effectiveness of such knowledge transfer drops significantly when\ndistilling from strong models that are pretrained in a large scale. In this\npaper, we address this challenge for pretrained Vision Transformers (ViTs) by\nexploring methods to fine-tune them for more effective knowledge transfer.\nMotivated by the connection between mutual information and distillation\neffectiveness, we propose to employ mutual information-aware optimization\nduring finetuning. For small or highly-imbalanced downstream datasets where\nsuch optimization becomes less effective, we introduce a simple yet effective\nheuristic of reweighting MLP blocks. This approach is inspired by our\nobservation that top MLP blocks are primarily responsible for mutual\ninformation loss. Our method enables small student models to benefit from those\npretrained models among the strongest.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u4ece\u5927\u578b\u9884\u8bad\u7ec3ViT\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u4e92\u4fe1\u606f\u611f\u77e5\u4f18\u5316\u5fae\u8c03ViT\uff0c\u5e76\u5728\u6570\u636e\u96c6\u53d7\u9650\u65f6\u91c7\u7528MLP\u5757\u91cd\u52a0\u6743\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u5c0f\u578b\u5b66\u751f\u6a21\u578b\u7684\u53d7\u76ca\u7a0b\u5ea6\u3002", "motivation": "\u5f53\u4ece\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u5f3a\u5927\u89c6\u89c9\u8868\u793a\u6a21\u578b\uff08\u7279\u522b\u662fVision Transformers, ViTs\uff09\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u65f6\uff0c\u77e5\u8bc6\u8fc1\u79fb\u7684\u6709\u6548\u6027\u663e\u8457\u4e0b\u964d\uff0c\u8fd9\u9650\u5236\u4e86\u5c0f\u578b\u3001\u7279\u5b9a\u4efb\u52a1\u751f\u4ea7\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u3002", "method": "\u672c\u6587\u63a2\u7d22\u4e86\u5fae\u8c03\u9884\u8bad\u7ec3ViTs\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u77e5\u8bc6\u8fc1\u79fb\u7684\u65b9\u6cd5\u3002\u5177\u4f53\u5730\uff0c\u63d0\u51fa\u4e86\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u91c7\u7528\u4e92\u4fe1\u606f\u611f\u77e5\u7684\u4f18\u5316\u7b56\u7565\u3002\u5bf9\u4e8e\u5c0f\u578b\u6216\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u4e0b\u6e38\u6570\u636e\u96c6\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff1a\u91cd\u65b0\u52a0\u6743MLP\u5757\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u89c2\u5bdf\u5230\u9876\u90e8MLP\u5757\u662f\u5bfc\u81f4\u4e92\u4fe1\u606f\u635f\u5931\u7684\u4e3b\u8981\u539f\u56e0\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7f\u5c0f\u578b\u5b66\u751f\u6a21\u578b\u80fd\u591f\u4ece\u6700\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u53d7\u76ca\uff0c\u6709\u6548\u63d0\u5347\u4e86\u77e5\u8bc6\u84b8\u998f\u7684\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u4e92\u4fe1\u606f\u611f\u77e5\u4f18\u5316\u548cMLP\u5757\u91cd\u52a0\u6743\u7b56\u7565\uff0c\u672c\u6587\u7684\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u4ece\u5f3a\u5927\u9884\u8bad\u7ec3ViTs\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u7684\u6548\u7387\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u578b\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u548c\u4ece\u5927\u578b\u6a21\u578b\u4e2d\u83b7\u76ca\u7684\u80fd\u529b\u3002"}}
{"id": "1610.09431", "pdf": "https://arxiv.org/pdf/1610.09431", "abs": "https://arxiv.org/abs/1610.09431", "authors": ["Omar Claflin"], "title": "Attention acts to suppress goal-based conflict under high competition", "categories": ["q-bio.NC", "cs.AI"], "comment": "25 pages, 3 figures, 3 tables", "summary": "It is known that when multiple stimuli are present, top-down attention\nselectively enhances the neural signal in the visual cortex for task-relevant\nstimuli, but this has been tested only under conditions of minimal competition\nof visual attention. Here we show during high competition, that is, two stimuli\nin a shared receptive field possessing opposing modulatory goals, top-down\nattention suppresses both task-relevant and irrelevant neural signals within\n100 ms of stimuli onset. This non-selective engagement of top-down attentional\nresources serves to reduce the feedforward signal representing irrelevant\nstimuli.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5728\u9ad8\u89c6\u89c9\u7ade\u4e89\u6761\u4ef6\u4e0b\uff0c\u81ea\u4e0a\u800c\u4e0b\u7684\u6ce8\u610f\u529b\u4f1a\u5728\u523a\u6fc0\u51fa\u73b0100\u6beb\u79d2\u5185\u975e\u9009\u62e9\u6027\u5730\u6291\u5236\u4efb\u52a1\u76f8\u5173\u548c\u65e0\u5173\u7684\u795e\u7ecf\u4fe1\u53f7\uff0c\u4ee5\u51cf\u5c11\u65e0\u5173\u523a\u6fc0\u7684\u524d\u9988\u4fe1\u53f7\uff0c\u8fd9\u4e0e\u6b64\u524d\u5728\u4f4e\u7ade\u4e89\u6761\u4ef6\u4e0b\u89c2\u5bdf\u5230\u7684\u9009\u62e9\u6027\u589e\u5f3a\u4f5c\u7528\u4e0d\u540c\u3002", "motivation": "\u4ee5\u5f80\u5173\u4e8e\u81ea\u4e0a\u800c\u4e0b\u6ce8\u610f\u529b\u9009\u62e9\u6027\u589e\u5f3a\u4efb\u52a1\u76f8\u5173\u795e\u7ecf\u4fe1\u53f7\u7684\u7814\u7a76\uff0c\u4ec5\u5728\u89c6\u89c9\u6ce8\u610f\u529b\u7ade\u4e89\u6700\u5c0f\u7684\u6761\u4ef6\u4e0b\u8fdb\u884c\u8fc7\u6d4b\u8bd5\uff0c\u7f3a\u4e4f\u5728\u9ad8\u7ade\u4e89\u73af\u5883\u4e0b\u7684\u4e86\u89e3\u3002", "method": "\u7814\u7a76\u5728\u9ad8\u7ade\u4e89\u6761\u4ef6\u4e0b\uff08\u5373\u5171\u4eab\u611f\u53d7\u91ce\u5185\u5b58\u5728\u4e24\u4e2a\u5177\u6709\u76f8\u53cd\u8c03\u8282\u76ee\u6807\u7684\u523a\u6fc0\uff09\u6d4b\u8bd5\u4e86\u81ea\u4e0a\u800c\u4e0b\u6ce8\u610f\u529b\u5bf9\u795e\u7ecf\u4fe1\u53f7\u7684\u5f71\u54cd\u3002", "result": "\u5728\u9ad8\u7ade\u4e89\u6761\u4ef6\u4e0b\uff0c\u81ea\u4e0a\u800c\u4e0b\u7684\u6ce8\u610f\u529b\u5728\u523a\u6fc0\u51fa\u73b0100\u6beb\u79d2\u5185\u540c\u65f6\u6291\u5236\u4e86\u4efb\u52a1\u76f8\u5173\u548c\u4e0d\u76f8\u5173\u7684\u795e\u7ecf\u4fe1\u53f7\u3002", "conclusion": "\u81ea\u4e0a\u800c\u4e0b\u6ce8\u610f\u529b\u8d44\u6e90\u7684\u8fd9\u79cd\u975e\u9009\u62e9\u6027\u53c2\u4e0e\uff0c\u65e8\u5728\u51cf\u5c11\u4ee3\u8868\u65e0\u5173\u523a\u6fc0\u7684\u524d\u9988\u4fe1\u53f7\u3002"}}
{"id": "2506.22833", "pdf": "https://arxiv.org/pdf/2506.22833", "abs": "https://arxiv.org/abs/2506.22833", "authors": ["Shashikant Verma", "Shanmuganathan Raman"], "title": "SemFaceEdit: Semantic Face Editing on Generative Radiance Manifolds", "categories": ["cs.CV"], "comment": null, "summary": "Despite multiple view consistency offered by 3D-aware GAN techniques, the\nresulting images often lack the capacity for localized editing. In response,\ngenerative radiance manifolds emerge as an efficient approach for constrained\npoint sampling within volumes, effectively reducing computational demands and\nenabling the learning of fine details. This work introduces SemFaceEdit, a\nnovel method that streamlines the appearance and geometric editing process by\ngenerating semantic fields on generative radiance manifolds. Utilizing latent\ncodes, our method effectively disentangles the geometry and appearance\nassociated with different facial semantics within the generated image. In\ncontrast to existing methods that can change the appearance of the entire\nradiance field, our method enables the precise editing of particular facial\nsemantics while preserving the integrity of other regions. Our network\ncomprises two key modules: the Geometry module, which generates semantic\nradiance and occupancy fields, and the Appearance module, which is responsible\nfor predicting RGB radiance. We jointly train both modules in adversarial\nsettings to learn semantic-aware geometry and appearance descriptors. The\nappearance descriptors are then conditioned on their respective semantic latent\ncodes by the Appearance Module, facilitating disentanglement and enhanced\ncontrol. Our experiments highlight SemFaceEdit's superior performance in\nsemantic field-based editing, particularly in achieving improved radiance field\ndisentanglement.", "AI": {"tldr": "SemFaceEdit\u662f\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u8f90\u5c04\u6d41\u5f62\u7684\u4eba\u8138\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u8bed\u4e49\u573a\u5b9e\u73b0\u5bf9\u7279\u5b9a\u9762\u90e8\u8bed\u4e49\u7684\u7cbe\u7ec6\u5c40\u90e8\u7f16\u8f91\uff0c\u5e76\u6709\u6548\u89e3\u8026\u51e0\u4f55\u4e0e\u5916\u89c2\u3002", "motivation": "\u73b0\u67093D-aware GAN\u6280\u672f\u751f\u6210\u7684\u56fe\u50cf\u7f3a\u4e4f\u5c40\u90e8\u7f16\u8f91\u80fd\u529b\u3002\u5c3d\u7ba1\u751f\u6210\u8f90\u5c04\u6d41\u5f62\u80fd\u9ad8\u6548\u5904\u7406\u4f53\u79ef\u91c7\u6837\u5e76\u5b66\u4e60\u7cbe\u7ec6\u7ec6\u8282\uff0c\u4f46\u4ecd\u9700\u4e00\u79cd\u80fd\u5b9e\u73b0\u5916\u89c2\u548c\u51e0\u4f55\u7cbe\u7ec6\u5c40\u90e8\u7f16\u8f91\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51faSemFaceEdit\uff0c\u901a\u8fc7\u5728\u751f\u6210\u8f90\u5c04\u6d41\u5f62\u4e0a\u751f\u6210\u8bed\u4e49\u573a\u6765\u7b80\u5316\u5916\u89c2\u548c\u51e0\u4f55\u7f16\u8f91\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u6f5c\u5728\u7f16\u7801\u6709\u6548\u89e3\u8026\u751f\u6210\u56fe\u50cf\u4e2d\u4e0d\u540c\u9762\u90e8\u8bed\u4e49\u76f8\u5173\u7684\u51e0\u4f55\u548c\u5916\u89c2\u3002\u7f51\u7edc\u5305\u542b\u51e0\u4f55\u6a21\u5757\uff08\u751f\u6210\u8bed\u4e49\u8f90\u5c04\u548c\u5360\u636e\u573a\uff09\u548c\u5916\u89c2\u6a21\u5757\uff08\u9884\u6d4bRGB\u8f90\u5c04\uff09\uff0c\u4e24\u8005\u5728\u5bf9\u6297\u8bbe\u7f6e\u4e0b\u8054\u5408\u8bad\u7ec3\uff0c\u4ee5\u5b66\u4e60\u8bed\u4e49\u611f\u77e5\u7684\u51e0\u4f55\u548c\u5916\u89c2\u63cf\u8ff0\u7b26\uff0c\u5176\u4e2d\u5916\u89c2\u63cf\u8ff0\u7b26\u7531\u5176\u8bed\u4e49\u6f5c\u5728\u7f16\u7801\u6761\u4ef6\u5316\u3002", "result": "SemFaceEdit\u5728\u57fa\u4e8e\u8bed\u4e49\u573a\u7684\u7f16\u8f91\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5b9e\u73b0\u6539\u8fdb\u7684\u8f90\u5c04\u573a\u89e3\u8026\u65b9\u9762\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u80fd\u7cbe\u786e\u7f16\u8f91\u7279\u5b9a\u9762\u90e8\u8bed\u4e49\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u533a\u57df\u7684\u5b8c\u6574\u6027\u3002", "conclusion": "SemFaceEdit\u6210\u529f\u89e3\u51b3\u4e863D-aware GANs\u5c40\u90e8\u7f16\u8f91\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5728\u751f\u6210\u8f90\u5c04\u6d41\u5f62\u4e0a\u751f\u6210\u8bed\u4e49\u573a\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4eba\u8138\u8bed\u4e49\u7684\u7cbe\u7ec6\u3001\u89e3\u8026\u7684\u5c40\u90e8\u7f16\u8f91\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u8f90\u5c04\u573a\u89e3\u8026\u6027\u80fd\u3002"}}
{"id": "2506.23377", "pdf": "https://arxiv.org/pdf/2506.23377", "abs": "https://arxiv.org/abs/2506.23377", "authors": ["Taejin Kim", "Siun-Chuon Mau", "Konrad Vesey"], "title": "Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 5 main pages of text, 5 figures, 2 tables. Research work\n  performed at CACI INTL INC", "summary": "Large language models (LLMs) are used in a variety of mission-critical roles.\nDue to the rapidly developing nature of LLMs, there is a lack of quantifiable\nunderstanding of the bias and perspective associated with LLM output. Inspired\nby this need, this paper considers the broader issue of perspective or\nviewpoint of general text and perspective control of large-language model (LLM)\noutput. Perspective-Dial consists of two main components: a (1) metric space,\ndubbed Perspective Space, that enables quantitative measurements of different\nperspectives regarding a topic, and the use of (2) Systematic Prompt\nEngineering that utilizes greedy-coordinate descent to control LLM output\nperspective based on measurement feedback from the Perspective Space. The\nempirical nature of the approach allows progress to side step a principled\nunderstanding of perspective or bias -- effectively quantifying and adjusting\noutputs for a variety of topics. Potential applications include detection,\ntracking and mitigation of LLM bias, narrative detection, sense making and\ntracking in public discourse, and debate bot advocating given perspective.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPerspective-Dial\u7cfb\u7edf\uff0c\u901a\u8fc7\u201c\u89c6\u89d2\u7a7a\u95f4\u201d\u91cf\u5316\u6587\u672c\u89c6\u89d2\uff0c\u5e76\u5229\u7528\u201c\u7cfb\u7edf\u5316\u63d0\u793a\u5de5\u7a0b\u201d\u63a7\u5236\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f93\u51fa\u7684\u89c6\u89d2\uff0c\u65e8\u5728\u89e3\u51b3LLM\u504f\u89c1\u96be\u4ee5\u91cf\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5173\u952e\u4efb\u52a1\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u8f93\u51fa\u7684\u504f\u89c1\u548c\u89c6\u89d2\u7f3a\u4e4f\u53ef\u91cf\u5316\u7684\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3LLM\u8f93\u51fa\u89c6\u89d2\u96be\u4ee5\u91cf\u5316\u548c\u63a7\u5236\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u540d\u4e3a\u201cPerspective-Dial\u201d\u7684\u7cfb\u7edf\uff0c\u5305\u542b\u4e24\u90e8\u5206\uff1a1) \u201c\u89c6\u89d2\u7a7a\u95f4\u201d\uff08Perspective Space\uff09\uff0c\u7528\u4e8e\u91cf\u5316\u4e0d\u540c\u89c6\u89d2\u7684\u5ea6\u91cf\u7a7a\u95f4\uff1b2) \u201c\u7cfb\u7edf\u5316\u63d0\u793a\u5de5\u7a0b\u201d\uff08Systematic Prompt Engineering\uff09\uff0c\u5229\u7528\u8d2a\u5a6a\u5750\u6807\u4e0b\u964d\u6cd5\uff0c\u6839\u636e\u89c6\u89d2\u7a7a\u95f4\u7684\u6d4b\u91cf\u53cd\u9988\u6765\u63a7\u5236LLM\u8f93\u51fa\u89c6\u89d2\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b9e\u8bc1\u65b9\u5f0f\uff0c\u80fd\u6709\u6548\u5730\u91cf\u5316\u548c\u8c03\u6574\u591a\u79cd\u4e3b\u9898\u7684LLM\u8f93\u51fa\u89c6\u89d2\uff0c\u907f\u514d\u4e86\u5bf9\u89c6\u89d2\u6216\u504f\u89c1\u8fdb\u884c\u539f\u5219\u6027\u7406\u89e3\u7684\u9700\u8981\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\uff0c\u5305\u62ec\u68c0\u6d4b\u3001\u8ffd\u8e2a\u548c\u51cf\u8f7bLLM\u504f\u89c1\uff0c\u5206\u6790\u516c\u5171\u8bdd\u8bed\u4e2d\u7684\u53d9\u4e8b\uff0c\u4ee5\u53ca\u5f00\u53d1\u652f\u6301\u7279\u5b9a\u89c6\u89d2\u7684\u8fa9\u8bba\u673a\u5668\u4eba\u3002"}}
{"id": "2506.23053", "pdf": "https://arxiv.org/pdf/2506.23053", "abs": "https://arxiv.org/abs/2506.23053", "authors": ["Hanlin Dong", "Arian Prabowo", "Hao Xue", "Flora D. Salim"], "title": "Double-Diffusion: Diffusion Conditioned Diffusion Probabilistic Model For Air Quality Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Air quality prediction is a challenging forecasting task due to its\nspatio-temporal complexity and the inherent dynamics as well as uncertainty.\nMost of the current models handle these two challenges by applying Graph Neural\nNetworks or known physics principles, and quantifying stochasticity through\nprobabilistic networks like Diffusion models. Nevertheless, finding the right\nbalancing point between the certainties and uncertainties remains an open\nquestion. Therefore, we propose Double-Diffusion, a novel diffusion\nprobabilistic model that harnesses the power of known physics to guide air\nquality forecasting with stochasticity. To the best of our knowledge, while\nprecedents have been made of using conditional diffusion models to predict air\npollution, this is the first attempt to use physics as a conditional generative\napproach for air quality prediction. Along with a sampling strategy adopted\nfrom image restoration and a new denoiser architecture, Double-Diffusion ranks\nfirst in most evaluation scenarios across two real-life datasets compared with\nother probabilistic models, it also cuts inference time by 50% to 30% while\nenjoying an increase between 3-12% in Continuous Ranked Probabilistic Score\n(CRPS).", "AI": {"tldr": "\u4e00\u79cd\u65b0\u578b\u6269\u6563\u6982\u7387\u6a21\u578bDouble-Diffusion\uff0c\u7ed3\u5408\u7269\u7406\u6307\u5bfc\u548c\u968f\u673a\u6027\u8fdb\u884c\u7a7a\u6c14\u8d28\u91cf\u9884\u6d4b\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u5e76\u663e\u8457\u7f29\u77ed\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u7a7a\u6c14\u8d28\u91cf\u9884\u6d4b\u56e0\u5176\u65f6\u7a7a\u590d\u6742\u6027\u548c\u56fa\u6709\u7684\u52a8\u6001\u6027\u53ca\u4e0d\u786e\u5b9a\u6027\u800c\u6781\u5177\u6311\u6218\u3002\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u5728\u786e\u5b9a\u6027\uff08\u5982\u7269\u7406\u539f\u7406\uff09\u548c\u4e0d\u786e\u5b9a\u6027\uff08\u5982\u968f\u673a\u6027\uff09\u4e4b\u95f4\u627e\u5230\u6700\u4f73\u5e73\u8861\u70b9\uff0c\u8fd9\u662f\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51faDouble-Diffusion\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6269\u6563\u6982\u7387\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5229\u7528\u5df2\u77e5\u7269\u7406\u539f\u7406\u6307\u5bfc\u7a7a\u6c14\u8d28\u91cf\u9884\u6d4b\uff0c\u5e76\u878d\u5165\u968f\u673a\u6027\uff0c\u662f\u9996\u6b21\u5c1d\u8bd5\u5c06\u7269\u7406\u5b66\u4f5c\u4e3a\u6761\u4ef6\u751f\u6210\u65b9\u6cd5\u5e94\u7528\u4e8e\u7a7a\u6c14\u8d28\u91cf\u9884\u6d4b\u3002\u6a21\u578b\u8fd8\u91c7\u7528\u4e86\u6e90\u81ea\u56fe\u50cf\u4fee\u590d\u7684\u91c7\u6837\u7b56\u7565\u548c\u65b0\u7684\u53bb\u566a\u5668\u67b6\u6784\u3002", "result": "Double-Diffusion\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u7684\u5927\u591a\u6570\u8bc4\u4f30\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u5176\u4ed6\u6982\u7387\u6a21\u578b\uff0c\u6392\u540d\u7b2c\u4e00\u3002\u5b83\u5c06\u63a8\u7406\u65f6\u95f4\u7f29\u77ed\u4e8650%\u81f330%\uff0c\u540c\u65f6\u8fde\u7eed\u5206\u7ea7\u6982\u7387\u5206\u6570\uff08CRPS\uff09\u63d0\u9ad8\u4e863%\u81f312%\u3002", "conclusion": "Double-Diffusion\u6a21\u578b\u6210\u529f\u5c06\u7269\u7406\u5b66\u6307\u5bfc\u4e0e\u968f\u673a\u6027\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7a7a\u6c14\u8d28\u91cf\u9884\u6d4b\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u5e73\u8861\u9884\u6d4b\u4e2d\u7684\u786e\u5b9a\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2402.09146", "pdf": "https://arxiv.org/pdf/2402.09146", "abs": "https://arxiv.org/abs/2402.09146", "authors": ["Muhammad Kashif", "Muhammad Shafique"], "title": "ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": null, "summary": "In this paper, we present a novel framework for enhancing the performance of\nQuanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional\nlayers and addressing the critical challenges associated with them. Traditional\nquanvolutional layers, although beneficial for feature extraction, have largely\nbeen static, offering limited adaptability. Unlike state-of-the-art, our\nresearch overcomes this limitation by enabling training within these layers,\nsignificantly increasing the flexibility and potential of QuNNs. However, the\nintroduction of multiple trainable quanvolutional layers induces complexities\nin gradient-based optimization, primarily due to the difficulty in accessing\ngradients across these layers. To resolve this, we propose a novel\narchitecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging\nthe concept of residual learning, which facilitates the flow of gradients by\nadding skip connections between layers. By inserting residual blocks between\nquanvolutional layers, we ensure enhanced gradient access throughout the\nnetwork, leading to improved training performance. Moreover, we provide\nempirical evidence on the strategic placement of these residual blocks within\nQuNNs. Through extensive experimentation, we identify an efficient\nconfiguration of residual blocks, which enables gradients across all the layers\nin the network that eventually results in efficient training. Our findings\nsuggest that the precise location of residual blocks plays a crucial role in\nmaximizing the performance gains in QuNNs. Our results mark a substantial step\nforward in the evolution of quantum deep learning, offering new avenues for\nboth theoretical development and practical quantum computing applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578b\u5168\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08QuNN\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u8bad\u7ec3\u7684\u5168\u5377\u79ef\u5c42\u5e76\u5229\u7528\u6b8b\u5dee\u5b66\u4e60\u6784\u5efa\u6b8b\u5dee\u5168\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08ResQuNN\uff09\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5c42\u95f4\u68af\u5ea6\u8bbf\u95ee\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86QuNN\u7684\u8bad\u7ec3\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5168\u5377\u79ef\u5c42\u662f\u9759\u6001\u7684\uff0c\u9002\u5e94\u6027\u6709\u9650\u3002\u867d\u7136\u5f15\u5165\u53ef\u8bad\u7ec3\u5c42\u80fd\u589e\u5f3aQuNN\u7684\u7075\u6d3b\u6027\u548c\u6f5c\u529b\uff0c\u4f46\u591a\u5c42\u53ef\u8bad\u7ec3\u5c42\u4f1a\u9020\u6210\u68af\u5ea6\u96be\u4ee5\u8bbf\u95ee\u7684\u590d\u6742\u6027\uff0c\u4ece\u800c\u963b\u788d\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u3002", "method": "\u7814\u7a76\u8005\u5f15\u5165\u4e86\u53ef\u8bad\u7ec3\u7684\u5168\u5377\u79ef\u5c42\uff0c\u5e76\u63d0\u51fa\u4e86\u6b8b\u5dee\u5168\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08ResQuNN\uff09\u67b6\u6784\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u6b8b\u5dee\u5b66\u4e60\u7684\u6982\u5ff5\uff0c\u5728\u5168\u5377\u79ef\u5c42\u4e4b\u95f4\u6dfb\u52a0\u8df3\u8dc3\u8fde\u63a5\uff08skip connections\uff09\u6216\u6b8b\u5dee\u5757\uff0c\u4ee5\u4fc3\u8fdb\u68af\u5ea6\u5728\u7f51\u7edc\u4e2d\u7684\u6d41\u52a8\uff0c\u4ece\u800c\u89e3\u51b3\u68af\u5ea6\u8bbf\u95ee\u7684\u96be\u9898\u3002", "result": "\u901a\u8fc7\u6b8b\u5dee\u5757\u7684\u5f15\u5165\uff0c\u7f51\u7edc\u83b7\u5f97\u4e86\u589e\u5f3a\u7684\u68af\u5ea6\u8bbf\u95ee\u80fd\u529b\uff0c\u663e\u8457\u6539\u5584\u4e86\u8bad\u7ec3\u6027\u80fd\u3002\u7814\u7a76\u8fd8\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u786e\u5b9a\u4e86\u6b8b\u5dee\u5757\u7684\u6709\u6548\u914d\u7f6e\u548c\u6700\u4f73\u653e\u7f6e\u4f4d\u7f6e\uff0c\u53d1\u73b0\u5176\u7cbe\u786e\u4f4d\u7f6e\u5bf9\u6700\u5927\u5316QuNN\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8be5\u7814\u7a76\u6807\u5fd7\u7740\u91cf\u5b50\u6df1\u5ea6\u5b66\u4e60\u53d1\u5c55\u8fc8\u51fa\u4e86\u5b9e\u8d28\u6027\u4e00\u6b65\uff0c\u4e3a\u7406\u8bba\u53d1\u5c55\u548c\u5b9e\u9645\u91cf\u5b50\u8ba1\u7b97\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.22836", "pdf": "https://arxiv.org/pdf/2506.22836", "abs": "https://arxiv.org/abs/2506.22836", "authors": ["Hongyan An", "Kuan Zhu", "Xin He", "Haiyun Guo", "Chaoyang Zhao", "Ming Tang", "Jinqiao Wang"], "title": "FOCUS: Fine-grained Optimization with Semantic Guided Understanding for Pedestrian Attributes Recognition", "categories": ["cs.CV"], "comment": "ICME 2025 Oral", "summary": "Pedestrian attribute recognition (PAR) is a fundamental perception task in\nintelligent transportation and security. To tackle this fine-grained task, most\nexisting methods focus on extracting regional features to enrich attribute\ninformation. However, a regional feature is typically used to predict a fixed\nset of pre-defined attributes in these methods, which limits the performance\nand practicality in two aspects: 1) Regional features may compromise\nfine-grained patterns unique to certain attributes in favor of capturing common\ncharacteristics shared across attributes. 2) Regional features cannot\ngeneralize to predict unseen attributes in the test time. In this paper, we\npropose the \\textbf{F}ine-grained \\textbf{O}ptimization with semanti\\textbf{C}\ng\\textbf{U}ided under\\textbf{S}tanding (FOCUS) approach for PAR, which\nadaptively extracts fine-grained attribute-level features for each attribute\nindividually, regardless of whether the attributes are seen or not during\ntraining. Specifically, we propose the Multi-Granularity Mix Tokens (MGMT) to\ncapture latent features at varying levels of visual granularity, thereby\nenriching the diversity of the extracted information. Next, we introduce the\nAttribute-guided Visual Feature Extraction (AVFE) module, which leverages\ntextual attributes as queries to retrieve their corresponding visual attribute\nfeatures from the Mix Tokens using a cross-attention mechanism. To ensure that\ntextual attributes focus on the appropriate Mix Tokens, we further incorporate\na Region-Aware Contrastive Learning (RACL) method, encouraging attributes\nwithin the same region to share consistent attention maps. Extensive\nexperiments on PA100K, PETA, and RAPv1 datasets demonstrate the effectiveness\nand strong generalization ability of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFOCUS\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u53d6\u7ec6\u7c92\u5ea6\u5c5e\u6027\u7279\u5f81\uff0c\u89e3\u51b3\u884c\u4eba\u5c5e\u6027\u8bc6\u522b(PAR)\u4e2d\u73b0\u6709\u533a\u57df\u7279\u5f81\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u548c\u672a\u77e5\u5c5e\u6027\u6cdb\u5316\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u884c\u4eba\u5c5e\u6027\u8bc6\u522b(PAR)\u65b9\u6cd5\u4f9d\u8d56\u533a\u57df\u7279\u5f81\uff0c\u4f46\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u6a21\u5f0f\u548c\u6cdb\u5316\u5230\u8bad\u7ec3\u4e2d\u672a\u89c1\u7684\u5c5e\u6027\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u548c\u5b9e\u7528\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51faFOCUS\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u81ea\u9002\u5e94\u5730\u4e3a\u6bcf\u4e2a\u5c5e\u6027\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7279\u5f81\u3002\u5177\u4f53\u5305\u62ec\uff1a1) Multi-Granularity Mix Tokens (MGMT) \u6355\u83b7\u591a\u7c92\u5ea6\u89c6\u89c9\u7279\u5f81\uff1b2) Attribute-guided Visual Feature Extraction (AVFE) \u5229\u7528\u6587\u672c\u5c5e\u6027\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u63d0\u53d6\u89c6\u89c9\u5c5e\u6027\u7279\u5f81\uff1b3) Region-Aware Contrastive Learning (RACL) \u786e\u4fdd\u6587\u672c\u5c5e\u6027\u5173\u6ce8\u6b63\u786e\u533a\u57df\u5e76\u5171\u4eab\u4e00\u81f4\u7684\u6ce8\u610f\u529b\u56fe\u3002", "result": "\u5728PA100K\u3001PETA\u548cRAPv1\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FOCUS\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\u4e2d\u7ec6\u7c92\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u672a\u77e5\u5c5e\u6027\u6cdb\u5316\u7684\u95ee\u9898\uff0c\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.23393", "pdf": "https://arxiv.org/pdf/2506.23393", "abs": "https://arxiv.org/abs/2506.23393", "authors": ["Eugene J. Yu", "Dawei Zhu", "Yifan Song", "Xiangyu Wong", "Jiebin Zhang", "Wenxuan Shi", "Xiaoguang Li", "Qun Liu", "Sujian Li"], "title": "Hierarchical Memory Organization for Wikipedia Generation", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "Generating Wikipedia articles autonomously is a challenging task requiring\nthe integration of accurate, comprehensive, and well-structured information\nfrom diverse sources. This paper introduces the Memory Organization-based\nGeneration (MOG) framework, a novel approach to address these challenges by\nleveraging a hierarchical memory architecture. MOG extracts fine-grained memory\nunits from web documents, recursively organizes them into a Wikipedia-style\nhierarchical structure, and uses this structure to guide the generation\nprocess. This ensures alignment between memory and the article outline,\nimproving both informativeness and verifiability while minimizing\nhallucinations. Additionally, a citation module is implemented to enhance\ntraceability by linking every generated sentence to specific memory units.\nEvaluations on our newly created WikiStart dataset demonstrate that MOG\noutperforms baseline methods in producing informative and reliable articles,\nmaking it particularly robust in real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MOG\u6846\u67b6\uff0c\u5229\u7528\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784\u4ece\u7f51\u7edc\u6587\u6863\u4e2d\u63d0\u53d6\u5e76\u7ec4\u7ec7\u4fe1\u606f\uff0c\u4ee5\u81ea\u52a8\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u3001\u53ef\u9760\u4e14\u53ef\u8ffd\u6eaf\u7684\u7ef4\u57fa\u767e\u79d1\u6587\u7ae0\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u751f\u6210\u7ef4\u57fa\u767e\u79d1\u6587\u7ae0\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u6574\u5408\u6765\u81ea\u4e0d\u540c\u6765\u6e90\u7684\u51c6\u786e\u3001\u5168\u9762\u4e14\u7ed3\u6784\u826f\u597d\u7684\u4fe1\u606f\u3002\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u5728\u4fe1\u606f\u91cf\u3001\u53ef\u9a8c\u8bc1\u6027\u4ee5\u53ca\u5e7b\u89c9\u95ee\u9898\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u57fa\u4e8e\u8bb0\u5fc6\u7ec4\u7ec7\u751f\u6210\uff08MOG\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784\uff0c\u4ece\u7f51\u7edc\u6587\u6863\u4e2d\u63d0\u53d6\u7ec6\u7c92\u5ea6\u8bb0\u5fc6\u5355\u5143\uff0c\u5e76\u5c06\u5176\u9012\u5f52\u7ec4\u7ec7\u6210\u7ef4\u57fa\u767e\u79d1\u98ce\u683c\u7684\u5206\u5c42\u7ed3\u6784\u4ee5\u6307\u5bfc\u6587\u7ae0\u751f\u6210\uff0c\u4ece\u800c\u786e\u4fdd\u8bb0\u5fc6\u4e0e\u6587\u7ae0\u5927\u7eb2\u7684\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u5e7b\u89c9\u3002\u6b64\u5916\uff0cMOG\u8fd8\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5f15\u7528\u6a21\u5757\uff0c\u5c06\u6bcf\u53e5\u751f\u6210\u5185\u5bb9\u94fe\u63a5\u5230\u5177\u4f53\u7684\u8bb0\u5fc6\u5355\u5143\uff0c\u4ee5\u589e\u5f3a\u53ef\u8ffd\u6eaf\u6027\u3002", "result": "\u901a\u8fc7\u5728\u65b0\u521b\u5efa\u7684WikiStart\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cMOG\u6846\u67b6\u5728\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u548c\u53ef\u9760\u7684\u6587\u7ae0\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5c24\u5176\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "MOG\u6846\u67b6\u901a\u8fc7\u5176\u72ec\u7279\u7684\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784\u548c\u5f15\u7528\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ef4\u57fa\u767e\u79d1\u6587\u7ae0\u81ea\u52a8\u751f\u6210\u4e2d\u7684\u4fe1\u606f\u6574\u5408\u3001\u53ef\u9760\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u6311\u6218\uff0c\u4e3a\u9ad8\u8d28\u91cf\u3001\u53ef\u9760\u6587\u7ae0\u7684\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9c81\u68d2\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.23055", "pdf": "https://arxiv.org/pdf/2506.23055", "abs": "https://arxiv.org/abs/2506.23055", "authors": ["Hiro Taiyo Hamada", "Ippei Fujisawa", "Genji Kawakita", "Yuki Yamada"], "title": "Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities\nin producing human-like text. However, it is unclear how accurately these\nmodels internalize concepts that shape human thought and behavior. Here, we\ndeveloped a quantitative framework to assess concept alignment between LLMs and\nhuman psychological dimensions using 43 standardized psychological\nquestionnaires, selected for their established validity in measuring distinct\npsychological constructs. Our method evaluates how accurately language models\nreconstruct and classify questionnaire items through pairwise similarity\nanalysis. We compared resulting cluster structures with the original\ncategorical labels using hierarchical clustering. A GPT-4 model achieved\nsuperior classification accuracy (66.2\\%), significantly outperforming GPT-3.5\n(55.9\\%) and BERT (48.1\\%), all exceeding random baseline performance (31.9\\%).\nWe also demonstrated that the estimated semantic similarity from GPT-4 is\nassociated with Pearson's correlation coefficients of human responses in\nmultiple psychological questionnaires. This framework provides a novel approach\nto evaluate the alignment of the human-LLM concept and identify potential\nrepresentational biases. Our findings demonstrate that modern LLMs can\napproximate human psychological constructs with measurable accuracy, offering\ninsights for developing more interpretable AI systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9a\u91cf\u6846\u67b6\uff0c\u5229\u752843\u4efd\u5fc3\u7406\u95ee\u5377\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u4eba\u7c7b\u5fc3\u7406\u7ef4\u5ea6\u7684\u6982\u5ff5\u5bf9\u9f50\u5ea6\u3002\u7ed3\u679c\u663e\u793a\uff0cGPT-4\u5728\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u4e14\u5176\u8bed\u4e49\u76f8\u4f3c\u6027\u4e0e\u4eba\u7c7b\u5fc3\u7406\u53cd\u5e94\u76f8\u5173\uff0c\u8868\u660e\u73b0\u4ee3LLMs\u80fd\u4ee5\u53ef\u6d4b\u91cf\u7684\u51c6\u786e\u6027\u8fd1\u4f3c\u4eba\u7c7b\u5fc3\u7406\u7ed3\u6784\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u7c7b\u4eba\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u6a21\u578b\u5185\u5316\u5851\u9020\u4eba\u7c7b\u601d\u7ef4\u548c\u884c\u4e3a\u7684\u6982\u5ff5\u7684\u51c6\u786e\u6027\u5982\u4f55\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9a\u91cf\u6846\u67b6\u6765\u8bc4\u4f30LLM\u4e0e\u4eba\u7c7b\u5fc3\u7406\u7ef4\u5ea6\u7684\u6982\u5ff5\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u4f7f\u752843\u4efd\u6807\u51c6\u5316\u5fc3\u7406\u95ee\u5377\uff0c\u901a\u8fc7\u6210\u5bf9\u76f8\u4f3c\u6027\u5206\u6790\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u91cd\u6784\u548c\u5206\u7c7b\u95ee\u5377\u9879\u76ee\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4f7f\u7528\u5c42\u6b21\u805a\u7c7b\u5c06\u7ed3\u679c\u805a\u7c7b\u7ed3\u6784\u4e0e\u539f\u59cb\u5206\u7c7b\u6807\u7b7e\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "GPT-4\u6a21\u578b\u5b9e\u73b0\u4e8666.2%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8eGPT-3.5\uff0855.9%\uff09\u548cBERT\uff0848.1%\uff09\uff0c\u6240\u6709\u6a21\u578b\u5747\u8d85\u8fc7\u4e8631.9%\u7684\u968f\u673a\u57fa\u7ebf\u8868\u73b0\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0GPT-4\u4f30\u8ba1\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u4e0e\u591a\u4efd\u5fc3\u7406\u95ee\u5377\u4e2d\u4eba\u7c7b\u53cd\u5e94\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u76f8\u5173\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u8bc4\u4f30\u4eba\u7c7b-LLM\u6982\u5ff5\u5bf9\u9f50\u5e76\u8bc6\u522b\u6f5c\u5728\u8868\u5f81\u504f\u5dee\u7684\u65b0\u65b9\u6cd5\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u4ee3LLMs\u80fd\u591f\u4ee5\u53ef\u6d4b\u91cf\u7684\u51c6\u786e\u6027\u8fd1\u4f3c\u4eba\u7c7b\u5fc3\u7406\u7ed3\u6784\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u89e3\u91ca\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2504.15071", "pdf": "https://arxiv.org/pdf/2504.15071", "abs": "https://arxiv.org/abs/2504.15071", "authors": ["Louis Bradshaw", "Simon Colton"], "title": "Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling", "categories": ["cs.SD", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce an extensive new dataset of MIDI files, created by transcribing\naudio recordings of piano performances into their constituent notes. The data\npipeline we use is multi-stage, employing a language model to autonomously\ncrawl and score audio recordings from the internet based on their metadata,\nfollowed by a stage of pruning and segmentation using an audio classifier. The\nresulting dataset contains over one million distinct MIDI files, comprising\nroughly 100,000 hours of transcribed audio. We provide an in-depth analysis of\nour techniques, offering statistical insights, and investigate the content by\nextracting metadata tags, which we also provide. Dataset available at\nhttps://github.com/loubbrad/aria-midi.", "AI": {"tldr": "\u5f15\u5165\u4e00\u4e2a\u7531\u94a2\u7434\u97f3\u9891\u8f6c\u5f55\u800c\u6765\u7684\u767e\u4e07\u7ea7MIDI\u6587\u4ef6\u6570\u636e\u96c6\uff0c\u5e76\u8be6\u8ff0\u5176\u6784\u5efa\u65b9\u6cd5\u3002", "motivation": "\u521b\u5efa\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684MIDI\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u76f8\u5173\u7814\u7a76\u548c\u5e94\u7528\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u6570\u636e\u7ba1\u9053\uff0c\u5305\u62ec\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u5143\u6570\u636e\u8fdb\u884c\u7f51\u7edc\u97f3\u9891\u6293\u53d6\u548c\u8bc4\u5206\uff0c\u4ee5\u53ca\u4f7f\u7528\u97f3\u9891\u5206\u7c7b\u5668\u8fdb\u884c\u526a\u679d\u548c\u5206\u5272\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8d85\u8fc7\u4e00\u767e\u4e07\u4e2a\u72ec\u7279MIDI\u6587\u4ef6\u3001\u7ea610\u4e07\u5c0f\u65f6\u8f6c\u5f55\u97f3\u9891\u7684\u5e9e\u5927\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u8be6\u7ec6\u5206\u6790\u4e86\u6570\u636e\u96c6\u6784\u5efa\u6280\u672f\uff0c\u63d0\u4f9b\u4e86\u7edf\u8ba1\u89c1\u89e3\u548c\u5143\u6570\u636e\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u4f9b\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2506.22843", "pdf": "https://arxiv.org/pdf/2506.22843", "abs": "https://arxiv.org/abs/2506.22843", "authors": ["Kien Nguyen", "Clinton Fookes", "Sridha Sridharan", "Huy Nguyen", "Feng Liu", "Xiaoming Liu", "Arun Ross", "Dana Michalski", "Tam\u00e1s Endrei", "Ivan DeAndres-Tame", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez", "Javier Ortega-Garcia", "Zijing Gong", "Yuhao Wang", "Xuehu Liu", "Pingping Zhang", "Md Rashidunnabi", "Hugo Proen\u00e7a", "Kailash A. Hambarde", "Saeid Rezaei"], "title": "AG-VPReID 2025: Aerial-Ground Video-based Person Re-identification Challenge Results", "categories": ["cs.CV"], "comment": null, "summary": "Person re-identification (ReID) across aerial and ground vantage points has\nbecome crucial for large-scale surveillance and public safety applications.\nAlthough significant progress has been made in ground-only scenarios, bridging\nthe aerial-ground domain gap remains a formidable challenge due to extreme\nviewpoint differences, scale variations, and occlusions. Building upon the\nachievements of the AG-ReID 2023 Challenge, this paper introduces the AG-VPReID\n2025 Challenge - the first large-scale video-based competition focused on\nhigh-altitude (80-120m) aerial-ground ReID. Constructed on the new AG-VPReID\ndataset with 3,027 identities, over 13,500 tracklets, and approximately 3.7\nmillion frames captured from UAVs, CCTV, and wearable cameras, the challenge\nfeatured four international teams. These teams developed solutions ranging from\nmulti-stream architectures to transformer-based temporal reasoning and\nphysics-informed modeling. The leading approach, X-TFCLIP from UAM, attained\n72.28% Rank-1 accuracy in the aerial-to-ground ReID setting and 70.77% in the\nground-to-aerial ReID setting, surpassing existing baselines while highlighting\nthe dataset's complexity. For additional details, please refer to the official\nwebsite at https://agvpreid25.github.io.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86AG-VPReID 2025\u6311\u6218\u8d5b\u53ca\u5176\u65b0\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u9ad8\u7a7a\u5230\u5730\u9762\u89c6\u9891\u4eba\u5458\u91cd\u8bc6\u522b\uff08ReID\uff09\u7684\u96be\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u9886\u5148\u56e2\u961f\u7684\u6210\u679c\uff0c\u5176\u4e2d\u6700\u4f73\u65b9\u6cd5\u5728\u7a7a\u5bf9\u5730\u548c\u5730\u5bf9\u7a7aReID\u4e2d\u5747\u53d6\u5f97\u8d85\u8fc770%\u7684Rank-1\u51c6\u786e\u7387\u3002", "motivation": "\u5c3d\u7ba1\u5730\u9762\u4eba\u5458\u91cd\u8bc6\u522b\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u9ad8\u7a7a\u548c\u5730\u9762\u89c6\u89d2\u4e4b\u95f4\u6781\u7aef\u7684\u57df\u5dee\u5f02\u3001\u5c3a\u5ea6\u53d8\u5316\u53ca\u906e\u6321\uff0c\u4f7f\u5f97\u8de8\u89c6\u89d2\u4eba\u5458\u91cd\u8bc6\u522b\u6210\u4e3a\u4e00\u4e2a\u8270\u5de8\u6311\u6218\uff0c\u800c\u8fd9\u5bf9\u4e8e\u5927\u89c4\u6a21\u76d1\u63a7\u548c\u516c\u5171\u5b89\u5168\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8eAG-ReID 2023\u6311\u6218\u8d5b\u7684\u6210\u5c31\uff0c\u672c\u6587\u63a8\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u89c6\u9891\u9ad8\u7a7a\uff0880-120\u7c73\uff09\u5230\u5730\u9762\u4eba\u5458\u91cd\u8bc6\u522b\u6311\u6218\u8d5bAG-VPReID 2025\u3002\u8be5\u6311\u6218\u8d5b\u57fa\u4e8e\u65b0\u7684AG-VPReID\u6570\u636e\u96c6\u6784\u5efa\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b3027\u4e2a\u8eab\u4efd\u3001\u8d85\u8fc713500\u4e2a\u8f68\u8ff9\u548c\u7ea6370\u4e07\u5e27\u56fe\u50cf\u3002\u6311\u6218\u8d5b\u5438\u5f15\u4e86\u56db\u4e2a\u56fd\u9645\u56e2\u961f\uff0c\u4ed6\u4eec\u5f00\u53d1\u4e86\u5305\u62ec\u591a\u6d41\u67b6\u6784\u3001\u57fa\u4e8eTransformer\u7684\u65f6\u5e8f\u63a8\u7406\u548c\u7269\u7406\u77e5\u60c5\u5efa\u6a21\u7b49\u591a\u79cd\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6311\u6218\u8d5b\u7684\u9886\u5148\u65b9\u6cd5\u2014\u2014\u6765\u81eaUAM\u7684X-TFCLIP\uff0c\u5728\u7a7a\u5bf9\u5730\u4eba\u5458\u91cd\u8bc6\u522b\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u4e8672.28%\u7684Rank-1\u51c6\u786e\u7387\uff0c\u5728\u5730\u5bf9\u7a7a\u4eba\u5458\u91cd\u8bc6\u522b\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u4e8670.77%\u7684Rank-1\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\uff0c\u540c\u65f6\u4e5f\u51f8\u663e\u4e86\u6570\u636e\u96c6\u7684\u590d\u6742\u6027\u3002", "conclusion": "AG-VPReID 2025\u6311\u6218\u8d5b\u53ca\u5176\u65b0\u6570\u636e\u96c6\u7684\u63a8\u51fa\uff0c\u663e\u8457\u63a8\u52a8\u4e86\u9ad8\u7a7a\u5230\u5730\u9762\u89c6\u9891\u4eba\u5458\u91cd\u8bc6\u522b\u9886\u57df\u7684\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u8be5\u9886\u57df\u6700\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u660e\u786e\u4e86\u672a\u6765\u7814\u7a76\u9700\u8981\u514b\u670d\u7684\u590d\u6742\u6027\u4e0e\u6311\u6218\u3002"}}
{"id": "2506.23411", "pdf": "https://arxiv.org/pdf/2506.23411", "abs": "https://arxiv.org/abs/2506.23411", "authors": ["Jiale Zhang", "Zichong Wang", "Avash Palikhe", "Zhipeng Yin", "Wenbin Zhang"], "title": "Datasets for Fairness in Language Models: An In-Depth Survey", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Fairness benchmarks play a central role in shaping how we evaluate language\nmodels, yet surprisingly little attention has been given to examining the\ndatasets that these benchmarks rely on. This survey addresses that gap by\npresenting a broad and careful review of the most widely used fairness datasets\nin current language model research, characterizing them along several key\ndimensions including their origin, scope, content, and intended use to help\nresearchers better appreciate the assumptions and limitations embedded in these\nresources. To support more meaningful comparisons and analyses, we introduce a\nunified evaluation framework that reveals consistent patterns of demographic\ndisparities across datasets and scoring methods. Applying this framework to\ntwenty four common benchmarks, we highlight the often overlooked biases that\ncan influence conclusions about model fairness and offer practical guidance for\nselecting, combining, and interpreting these datasets. We also point to\nopportunities for creating new fairness benchmarks that reflect more diverse\nsocial contexts and encourage more thoughtful use of these tools going forward.\nAll code, data, and detailed results are publicly available at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets\nto promote transparency and reproducibility across the research community.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7efc\u8ff0\u4e86\u8bed\u8a00\u6a21\u578b\u516c\u5e73\u6027\u8bc4\u4f30\u4e2d\u5e38\u7528\u7684\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u5176\u5185\u5728\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u516c\u5e73\u6027\u5206\u6790\u548c\u6570\u636e\u96c6\u4f7f\u7528\u3002", "motivation": "\u516c\u5e73\u6027\u57fa\u51c6\u5728\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u4e2d\u626e\u6f14\u6838\u5fc3\u89d2\u8272\uff0c\u4f46\u5bf9\u5176\u6240\u4f9d\u8d56\u7684\u6570\u636e\u96c6\u7f3a\u4e4f\u8db3\u591f\u5173\u6ce8\uff0c\u5bfc\u81f4\u7814\u7a76\u4eba\u5458\u672a\u80fd\u5145\u5206\u8ba4\u8bc6\u5230\u8fd9\u4e9b\u8d44\u6e90\u4e2d\u56fa\u6709\u7684\u5047\u8bbe\u548c\u5c40\u9650\u6027\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5168\u9762\u5ba1\u67e5\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u4e2d\u6700\u5e7f\u6cdb\u4f7f\u7528\u7684\u516c\u5e73\u6027\u6570\u636e\u96c6\uff0c\u4ece\u5176\u6765\u6e90\u3001\u8303\u56f4\u3001\u5185\u5bb9\u548c\u9884\u671f\u7528\u9014\u7b49\u591a\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u7279\u5f81\u5206\u6790\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e24\u4e2a\u5e38\u7528\u57fa\u51c6\uff0c\u4ee5\u63ed\u793a\u6570\u636e\u96c6\u4e2d\u548c\u8bc4\u5206\u65b9\u6cd5\u4e2d\u4e00\u81f4\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u5dee\u5f02\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u5206\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u5dee\u5f02\u7684\u666e\u904d\u6a21\u5f0f\u3002\u5f3a\u8c03\u4e86\u5e38\u88ab\u5ffd\u89c6\u7684\u504f\u89c1\uff0c\u8fd9\u4e9b\u504f\u89c1\u53ef\u80fd\u5f71\u54cd\u5bf9\u6a21\u578b\u516c\u5e73\u6027\u7ed3\u8bba\u7684\u5224\u65ad\u3002\u4e3a\u9009\u62e9\u3001\u7ec4\u5408\u548c\u89e3\u91ca\u8fd9\u4e9b\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u73b0\u6709\u516c\u5e73\u6027\u8bc4\u4f30\u6570\u636e\u96c6\u4e2d\u7684\u56fa\u6709\u504f\u89c1\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u548c\u5b9e\u7528\u6307\u5bfc\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u4e25\u8c28\u7684\u6a21\u578b\u516c\u5e73\u6027\u8bc4\u4f30\u3002\u672a\u6765\u5e94\u521b\u5efa\u53cd\u6620\u66f4\u591a\u6837\u5316\u793e\u4f1a\u80cc\u666f\u7684\u65b0\u516c\u5e73\u6027\u57fa\u51c6\uff0c\u5e76\u9f13\u52b1\u66f4\u5ba1\u614e\u5730\u4f7f\u7528\u73b0\u6709\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2506.23068", "pdf": "https://arxiv.org/pdf/2506.23068", "abs": "https://arxiv.org/abs/2506.23068", "authors": ["Zhiyu Zhao", "Haoxuan Li", "Haifeng Zhang", "Jun Wang", "Francesco Faccio", "J\u00fcrgen Schmidhuber", "Mengyue Yang"], "title": "Curious Causality-Seeking Agents Learn Meta Causal World", "categories": ["cs.LG", "cs.AI", "stat.AP"], "comment": "33 pages", "summary": "When building a world model, a common assumption is that the environment has\na single, unchanging underlying causal rule, like applying Newton's laws to\nevery situation. In reality, what appears as a drifting causal mechanism is\noften the manifestation of a fixed underlying mechanism seen through a narrow\nobservational window. This brings about a problem that, when building a world\nmodel, even subtle shifts in policy or environment states can alter the very\nobserved causal mechanisms. In this work, we introduce the \\textbf{Meta-Causal\nGraph} as world models, a minimal unified representation that efficiently\nencodes the transformation rules governing how causal structures shift across\ndifferent latent world states. A single Meta-Causal Graph is composed of\nmultiple causal subgraphs, each triggered by meta state, which is in the latent\nstate space. Building on this representation, we introduce a\n\\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta\nstates that trigger each subgraph, (2) discover the corresponding causal\nrelationships by agent curiosity-driven intervention policy, and (3)\niteratively refine the Meta-Causal Graph through ongoing curiosity-driven\nexploration and agent experiences. Experiments on both synthetic tasks and a\nchallenging robot arm manipulation task demonstrate that our method robustly\ncaptures shifts in causal dynamics and generalizes effectively to previously\nunseen contexts.", "AI": {"tldr": "\u9488\u5bf9\u4e16\u754c\u6a21\u578b\u4e2d\u89c2\u6d4b\u56e0\u679c\u673a\u5236\u6f02\u79fb\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u5143\u56e0\u679c\u56fe\u548c\u56e0\u679c\u63a2\u7d22\u4ee3\u7406\uff0c\u901a\u8fc7\u8bc6\u522b\u6f5c\u5728\u5143\u72b6\u6001\u548c\u597d\u5947\u5fc3\u9a71\u52a8\u63a2\u7d22\uff0c\u6709\u6548\u6355\u6349\u548c\u6cdb\u5316\u52a8\u6001\u53d8\u5316\u7684\u56e0\u679c\u5173\u7cfb\u3002", "motivation": "\u4f20\u7edf\u4e16\u754c\u6a21\u578b\u5e38\u5047\u8bbe\u73af\u5883\u56e0\u679c\u89c4\u5219\u5355\u4e00\u4e0d\u53d8\uff0c\u4f46\u5b9e\u9645\u4e2d\u89c2\u5bdf\u5230\u7684\u56e0\u679c\u673a\u5236\u6f02\u79fb\u5e38\u6e90\u4e8e\u56fa\u5b9a\u5e95\u5c42\u673a\u5236\u5728\u72ed\u7a84\u89c2\u6d4b\u7a97\u53e3\u4e0b\u7684\u8868\u73b0\u3002\u8fd9\u5bfc\u81f4\u5728\u6784\u5efa\u4e16\u754c\u6a21\u578b\u65f6\uff0c\u7b56\u7565\u6216\u73af\u5883\u72b6\u6001\u7684\u5fae\u5c0f\u53d8\u5316\u90fd\u53ef\u80fd\u6539\u53d8\u89c2\u6d4b\u5230\u7684\u56e0\u679c\u673a\u5236\uff0c\u5e26\u6765\u5efa\u6a21\u96be\u9898\u3002", "method": "\u63d0\u51fa**\u5143\u56e0\u679c\u56fe (Meta-Causal Graph)** \u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\uff0c\u5b83\u662f\u4e00\u4e2a\u6700\u5c0f\u7edf\u4e00\u7684\u8868\u793a\uff0c\u7f16\u7801\u4e86\u56e0\u679c\u7ed3\u6784\u5728\u4e0d\u540c\u6f5c\u5728\u4e16\u754c\u72b6\u6001\u95f4\u8f6c\u6362\u7684\u89c4\u5219\uff0c\u7531\u591a\u4e2a\u7531\u5143\u72b6\u6001\u89e6\u53d1\u7684\u56e0\u679c\u5b50\u56fe\u6784\u6210\u3002\u540c\u65f6\u5f15\u5165**\u56e0\u679c\u63a2\u7d22\u4ee3\u7406 (Causality-Seeking Agent)**\uff0c\u5176\u76ee\u6807\u662f\uff1a1) \u8bc6\u522b\u89e6\u53d1\u5b50\u56fe\u7684\u5143\u72b6\u6001\uff1b2) \u901a\u8fc7\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u5e72\u9884\u7b56\u7565\u53d1\u73b0\u56e0\u679c\u5173\u7cfb\uff1b3) \u901a\u8fc7\u6301\u7eed\u63a2\u7d22\u548c\u7ecf\u9a8c\u8fed\u4ee3\u7ec6\u5316\u5143\u56e0\u679c\u56fe\u3002", "result": "\u5728\u5408\u6210\u4efb\u52a1\u548c\u673a\u5668\u4eba\u673a\u68b0\u81c2\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u7a33\u5065\u6355\u6349\u56e0\u679c\u52a8\u6001\u53d8\u5316\uff0c\u5e76\u6709\u6548\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u89e3\u51b3\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6784\u5efa\u9c81\u68d2\u4e16\u754c\u6a21\u578b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5143\u56e0\u679c\u56fe\u548c\u56e0\u679c\u63a2\u7d22\u4ee3\u7406\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u3001\u53d8\u5316\u56e0\u679c\u673a\u5236\u7684\u6709\u6548\u8bc6\u522b\u3001\u5efa\u6a21\u548c\u6cdb\u5316\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u4f53\u5728\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5b66\u4e60\u548c\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.22850", "pdf": "https://arxiv.org/pdf/2506.22850", "abs": "https://arxiv.org/abs/2506.22850", "authors": ["Aalok Gangopadhyay", "Shashikant Verma", "Shanmuganathan Raman"], "title": "DMD-Net: Deep Mesh Denoising Network", "categories": ["cs.CV"], "comment": null, "summary": "We present Deep Mesh Denoising Network (DMD-Net), an end-to-end deep learning\nframework, for solving the mesh denoising problem. DMD-Net consists of a Graph\nConvolutional Neural Network in which aggregation is performed in both the\nprimal as well as the dual graph. This is realized in the form of an asymmetric\ntwo-stream network, which contains a primal-dual fusion block that enables\ncommunication between the primal-stream and the dual-stream. We develop a\nFeature Guided Transformer (FGT) paradigm, which consists of a feature\nextractor, a transformer, and a denoiser. The feature extractor estimates the\nlocal features, that guide the transformer to compute a transformation, which\nis applied to the noisy input mesh to obtain a useful intermediate\nrepresentation. This is further processed by the denoiser to obtain the\ndenoised mesh. Our network is trained on a large scale dataset of 3D objects.\nWe perform exhaustive ablation studies to demonstrate that each component in\nour network is essential for obtaining the best performance. We show that our\nmethod obtains competitive or better results when compared with the\nstate-of-the-art mesh denoising algorithms. We demonstrate that our method is\nrobust to various kinds of noise. We observe that even in the presence of\nextremely high noise, our method achieves excellent performance.", "AI": {"tldr": "\u63d0\u51faDMD-Net\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u5377\u79ef\u7f51\u7edc\u548c\u7279\u5f81\u5f15\u5bfc\u53d8\u6362\u5668\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u9c81\u68d2\u5730\u8fdb\u884c\u7f51\u683c\u53bb\u566a\u3002", "motivation": "\u89e3\u51b3\u7f51\u683c\u53bb\u566a\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u6709\u6548\u3001\u9c81\u68d2\u7684\u53bb\u566a\u65b9\u6cd5\u3002", "method": "\u5f15\u5165DMD-Net\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u539f\u59cb\u548c\u5bf9\u5076\u56fe\u4e0a\u8fdb\u884c\u805a\u5408\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u53cc\u6d41\u7f51\u7edc\u53ca\u539f\u59cb-\u5bf9\u5076\u878d\u5408\u5757\u5b9e\u73b0\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86\u7279\u5f81\u5f15\u5bfc\u53d8\u6362\u5668\uff08FGT\uff09\u8303\u5f0f\uff0c\u5305\u542b\u7279\u5f81\u63d0\u53d6\u5668\u3001\u53d8\u6362\u5668\u548c\u53bb\u566a\u5668\u3002\u7f51\u7edc\u5728\u5927\u89c4\u6a213D\u5bf9\u8c61\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6d88\u878d\u7814\u7a76\u8bc1\u660e\u7f51\u7edc\u4e2d\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\uff1b\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7f51\u683c\u53bb\u566a\u7b97\u6cd5\u76f8\u6bd4\uff0cDMD-Net\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u751a\u81f3\u66f4\u597d\u7684\u7ed3\u679c\uff1b\u5bf9\u5404\u79cd\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\uff1b\u5373\u4f7f\u5728\u6781\u9ad8\u566a\u58f0\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "DMD-Net\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u7f51\u683c\u53bb\u566a\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u6216\u5ab2\u7f8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u80fd\u6709\u6548\u5904\u7406\u9ad8\u5f3a\u5ea6\u566a\u58f0\u3002"}}
{"id": "2506.23423", "pdf": "https://arxiv.org/pdf/2506.23423", "abs": "https://arxiv.org/abs/2506.23423", "authors": ["Felipe Nuti", "Tim Franzmeyer", "Jo\u00e3o Henriques"], "title": "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025", "summary": "Past work has studied the effects of fine-tuning on large language models'\n(LLMs) overall performance on certain tasks. However, a quantitative and\nsystematic method for analyzing its effect on individual outputs is still\nlacking. Here, we propose a new method for measuring the contribution that\nfine-tuning makes to individual LLM responses, assuming access to the original\npre-trained model. Our method tracks the model's intermediate hidden states,\nproviding a more fine-grained insight into the effects of fine-tuning than a\nsimple comparison of final outputs from pre-trained and fine-tuned models. We\nintroduce and theoretically analyze an exact decomposition of any fine-tuned\nLLM into a pre-training component and a fine-tuning component. Empirically, we\nfind that model behavior and performance can be steered by up- or down-scaling\nthe fine-tuning component during the forward pass. Motivated by this finding\nand our theoretical analysis, we define the Tuning Contribution (TuCo) as the\nratio of the magnitudes of the fine-tuning component to the pre-training\ncomponent. We observe that three prominent adversarial attacks on LLMs\ncircumvent safety measures in a way that reduces TuCo, and that TuCo is\nconsistently lower on prompts where these attacks succeed compared to those\nwhere they do not. This suggests that attenuating the effect of fine-tuning on\nmodel outputs plays a role in the success of such attacks. In summary, TuCo\nenables the quantitative study of how fine-tuning influences model behavior and\nsafety, and vice versa.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08TuCo\uff09\u91cf\u5316\u5fae\u8c03\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2a\u4f53\u8f93\u51fa\u7684\u8d21\u732e\uff0c\u5e76\u53d1\u73b0\u5176\u4e0e\u5bf9\u6297\u6027\u653b\u51fb\u6210\u529f\u7387\u4e4b\u95f4\u7684\u5173\u8054\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u867d\u7136\u7814\u7a76\u4e86\u5fae\u8c03\u5bf9LLMs\u6574\u4f53\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4f46\u4ecd\u7f3a\u4e4f\u4e00\u79cd\u5b9a\u91cf\u3001\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u5206\u6790\u5fae\u8c03\u5bf9LLMs\u4e2a\u4f53\u8f93\u51fa\u7684\u5177\u4f53\u4f5c\u7528\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ffd\u8e2a\u6a21\u578b\u7684\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\uff0c\u7cbe\u786e\u5730\u5c06\u5fae\u8c03\u540e\u7684LLM\u5206\u89e3\u4e3a\u9884\u8bad\u7ec3\u7ec4\u4ef6\u548c\u5fae\u8c03\u7ec4\u4ef6\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u4ed6\u4eec\u5b9a\u4e49\u4e86\u201c\u5fae\u8c03\u8d21\u732e\u5ea6\u201d\uff08TuCo\uff09\uff0c\u5373\u5fae\u8c03\u7ec4\u4ef6\u4e0e\u9884\u8bad\u7ec3\u7ec4\u4ef6\u7684\u91cf\u7ea7\u4e4b\u6bd4\u3002", "result": "\u7ecf\u9a8c\u6027\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u8c03\u6574\u5fae\u8c03\u7ec4\u4ef6\u7684\u6bd4\u4f8b\uff0c\u53ef\u4ee5\u5f15\u5bfc\u6a21\u578b\u7684\u884c\u4e3a\u548c\u6027\u80fd\u3002\u6b64\u5916\uff0c\u7814\u7a76\u89c2\u5bdf\u5230\u4e09\u79cd\u4e3b\u6d41\u7684\u5bf9\u6297\u6027\u653b\u51fb\u5728\u89c4\u907f\u5b89\u5168\u63aa\u65bd\u65f6\uff0c\u4f1a\u4ee5\u964d\u4f4eTuCo\u7684\u65b9\u5f0f\u8fdb\u884c\uff0c\u5e76\u4e14\u5728\u653b\u51fb\u6210\u529f\u7684\u63d0\u793a\u4e0b\uff0cTuCo\u503c\u666e\u904d\u4f4e\u4e8e\u653b\u51fb\u5931\u8d25\u7684\u63d0\u793a\u3002\u8fd9\u8868\u660e\u51cf\u5f31\u5fae\u8c03\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u5f71\u54cd\uff0c\u5728\u8fd9\u4e9b\u653b\u51fb\u7684\u6210\u529f\u4e2d\u626e\u6f14\u4e86\u91cd\u8981\u89d2\u8272\u3002", "conclusion": "TuCo\u65b9\u6cd5\u4f7f\u5f97\u5b9a\u91cf\u7814\u7a76\u5fae\u8c03\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\u548c\u5b89\u5168\u6027\uff0c\u4ee5\u53ca\u53cd\u4e4b\u4ea6\u7136\u6210\u4e3a\u53ef\u80fd\u3002"}}
{"id": "2506.23145", "pdf": "https://arxiv.org/pdf/2506.23145", "abs": "https://arxiv.org/abs/2506.23145", "authors": ["Shahad Hardan", "Darya Taratynova", "Abdelmajid Essofi", "Karthik Nandakumar", "Mohammad Yaqub"], "title": "Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": null, "summary": "Privacy preservation in AI is crucial, especially in healthcare, where models\nrely on sensitive patient data. In the emerging field of machine unlearning,\nexisting methodologies struggle to remove patient data from trained multimodal\narchitectures, which are widely used in healthcare. We propose Forget-MI, a\nnovel machine unlearning method for multimodal medical data, by establishing\nloss functions and perturbation techniques. Our approach unlearns unimodal and\njoint representations of the data requested to be forgotten while preserving\nknowledge from the remaining data and maintaining comparable performance to the\noriginal model. We evaluate our results using performance on the forget\ndataset, performance on the test dataset, and Membership Inference Attack\n(MIA), which measures the attacker's ability to distinguish the forget dataset\nfrom the training dataset. Our model outperforms the existing approaches that\naim to reduce MIA and the performance on the forget dataset while keeping an\nequivalent performance on the test set. Specifically, our approach reduces MIA\nby 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,\nrespectively. Additionally, our performance on the test set matches that of the\nretrained model, while allowing forgetting. Code is available at\nhttps://github.com/BioMedIA-MBZUAI/Forget-MI.git", "AI": {"tldr": "\u63d0\u51faForget-MI\uff0c\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u533b\u7597\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u65b9\u6cd5\uff0c\u6709\u6548\u79fb\u9664\u654f\u611f\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u5e76\u663e\u8457\u964d\u4f4e\u6210\u5458\u63a8\u7406\u653b\u51fb\u98ce\u9669\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u4f9d\u8d56\u654f\u611f\u60a3\u8005\u6570\u636e\uff0c\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u65b9\u6cd5\u96be\u4ee5\u4ece\u5df2\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u6709\u6548\u79fb\u9664\u60a3\u8005\u6570\u636e\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u3002", "method": "\u63d0\u51faForget-MI\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u533b\u7597\u6570\u636e\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5efa\u7acb\u635f\u5931\u51fd\u6570\u548c\u6270\u52a8\u6280\u672f\uff0c\u5b9e\u73b0\u5bf9\u88ab\u9057\u5fd8\u6570\u636e\u7684\u5355\u6a21\u6001\u548c\u8054\u5408\u8868\u793a\u7684\u9057\u5fd8\uff0c\u540c\u65f6\u4fdd\u7559\u5269\u4f59\u6570\u636e\u7684\u77e5\u8bc6\uff0c\u5e76\u7ef4\u6301\u4e0e\u539f\u59cb\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "result": "Forget-MI\u5728\u9057\u5fd8\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u548c\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MIA\uff09\u6d4b\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u4e86\u540c\u7b49\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0cMIA\u964d\u4f4e\u4e860.202\uff0c\u9057\u5fd8\u6570\u636e\u96c6\u4e0a\u7684AUC\u548cF1\u5206\u6570\u5206\u522b\u964d\u4f4e\u4e860.221\u548c0.305\uff0c\u6d4b\u8bd5\u96c6\u6027\u80fd\u4e0e\u91cd\u65b0\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u5339\u914d\u3002", "conclusion": "Forget-MI\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u533b\u7597\u6570\u636e\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u96be\u9898\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6570\u636e\u9690\u79c1\u4fdd\u62a4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u5b9e\u7528\u6027\uff0c\u4e3a\u533b\u7597AI\u7684\u9690\u79c1\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22864", "pdf": "https://arxiv.org/pdf/2506.22864", "abs": "https://arxiv.org/abs/2506.22864", "authors": ["Li-Cheng Shen", "Jih-Kang Hsieh", "Wei-Hua Li", "Chu-Song Chen"], "title": "Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "ICMR 2025", "summary": "Text-to-image retrieval (TIR) aims to find relevant images based on a textual\nquery, but existing approaches are primarily based on whole-image captions and\nlack interpretability. Meanwhile, referring expression segmentation (RES)\nenables precise object localization based on natural language descriptions but\nis computationally expensive when applied across large image collections. To\nbridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies\nTIR and RES, requiring both efficient image search and accurate object\nsegmentation. To address this task, we propose a two-stage framework,\ncomprising a first stage for segmentation-aware image retrieval and a second\nstage for reranking and object grounding with a multimodal large language model\n(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract\nregion-level embeddings offline at first, enabling effective and scalable\nonline retrieval. Secondly, MLLM is used to refine retrieval rankings and\ngenerate bounding boxes, which are matched to segmentation masks. We evaluate\nour approach on COCO and D$^3$ datasets, demonstrating significant improvements\nin both retrieval accuracy and segmentation quality over previous methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86Mask-aware TIR (MaTIR)\u65b0\u4efb\u52a1\uff0c\u7ed3\u5408\u4e86\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\uff08TIR\uff09\u548c\u6307\u79f0\u8868\u8fbe\u5206\u5272\uff08RES\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u5904\u7406\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u7ebf\u7cbe\u70bc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u51c6\u786e\u6027\u548c\u5206\u5272\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u4f9d\u8d56\u6574\u4f53\u56fe\u50cf\u63cf\u8ff0\uff0c\u800c\u6307\u79f0\u8868\u8fbe\u5206\u5272\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u96c6\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u7814\u7a76\u65e8\u5728\u5f25\u5408\u9ad8\u6548\u56fe\u50cf\u641c\u7d22\u548c\u7cbe\u786e\u5bf9\u8c61\u5206\u5272\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u5f15\u5165\u4e86Mask-aware TIR (MaTIR)\u4efb\u52a1\u3002\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u8fdb\u884c\u5206\u5272\u611f\u77e5\u7684\u56fe\u50cf\u68c0\u7d22\uff0c\u5229\u7528SAM 2\u751f\u6210\u5bf9\u8c61\u63a9\u7801\u5e76\u7528Alpha-CLIP\u63d0\u53d6\u533a\u57df\u7ea7\u5d4c\u5165\uff08\u79bb\u7ebf\u5904\u7406\uff09\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u5728\u7ebf\u68c0\u7d22\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u8fdb\u884c\u91cd\u6392\u5e8f\u548c\u5bf9\u8c61\u5b9a\u4f4d\uff0c\u901a\u8fc7\u5339\u914dMLLM\u751f\u6210\u7684\u8fb9\u754c\u6846\u4e0e\u5206\u5272\u63a9\u7801\u6765\u4f18\u5316\u7ed3\u679c\u3002", "result": "\u5728COCO\u548cD$^3$\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u5206\u5272\u8d28\u91cf\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Mask-aware TIR (MaTIR)\u4efb\u52a1\u7684\u63d0\u51fa\u53ca\u5176\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u6210\u529f\u5730\u5c06\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u548c\u6307\u79f0\u8868\u8fbe\u5206\u5272\u7ed3\u5408\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u6548\u68c0\u7d22\u548c\u7cbe\u786e\u5bf9\u8c61\u5b9a\u4f4d\u7684\u9700\u6c42\u3002"}}
{"id": "2506.23431", "pdf": "https://arxiv.org/pdf/2506.23431", "abs": "https://arxiv.org/abs/2506.23431", "authors": ["Zixian Huang", "Chenxu Niu", "Yu Gu", "Gengyang Xiao", "Xinwei Huang", "Gong Cheng"], "title": "Pipelined Decoder for Efficient Context-Aware Text Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the basis of generative AI, an autoregressive model requires the\ngeneration of a new token depending on all the previously generated tokens,\nwhich brings high quality but also restricts the model to generate tokens one\nby one, forming a bottleneck limiting the generation speed. In this paper, we\npropose a new decoder architecture that efficiently generates text in parallel\nfor context-aware generation tasks. Our proposed pipelined decoder initiates\nthe generation of multiple subsequences simultaneously, and, at each time-step,\nit generates a new token for each subsequence to realize parallelism.\nExperiments on multiple text generation tasks, including question answering,\ntext summarization, and keyphrase generation, show that our pipelined decoder\nsignificantly improves the generation speed without a significant loss of\ngeneration quality or additional memory consumption.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6d41\u6c34\u7ebf\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u5e76\u884c\u751f\u6210\u663e\u8457\u52a0\u901f\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u4f5c\u4e3a\u751f\u6210\u5f0fAI\u7684\u57fa\u7840\uff0c\u5176\u9010\u4e2a\u751f\u6210token\u7684\u7279\u6027\u5bfc\u81f4\u751f\u6210\u901f\u5ea6\u6162\uff0c\u5f62\u6210\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u6d41\u6c34\u7ebf\u89e3\u7801\u5668\u201d\u7684\u65b0\u578b\u89e3\u7801\u5668\u67b6\u6784\u3002\u8be5\u67b6\u6784\u901a\u8fc7\u540c\u65f6\u542f\u52a8\u591a\u4e2a\u5b50\u5e8f\u5217\u7684\u751f\u6210\uff0c\u5e76\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u4e3a\u6bcf\u4e2a\u5b50\u5e8f\u5217\u5e76\u884c\u751f\u6210\u65b0token\u3002", "result": "\u5728\u95ee\u7b54\u3001\u6587\u672c\u6458\u8981\u548c\u5173\u952e\u8bcd\u751f\u6210\u7b49\u591a\u79cd\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6d41\u6c34\u7ebf\u89e3\u7801\u5668\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u901f\u5ea6\uff0c\u4e14\u672a\u663e\u8457\u964d\u4f4e\u751f\u6210\u8d28\u91cf\u6216\u589e\u52a0\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "\u6d41\u6c34\u7ebf\u89e3\u7801\u5668\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u751f\u6210\u901f\u5ea6\u7684\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5e76\u884c\u751f\u6210\u3002"}}
{"id": "2506.23147", "pdf": "https://arxiv.org/pdf/2506.23147", "abs": "https://arxiv.org/abs/2506.23147", "authors": ["Jonathan Schuster", "Fabian Transchel"], "title": "maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics", "categories": ["cs.LG", "cs.CV"], "comment": "6 pages, 2 figures", "summary": "In the domain of vehicle telematics the automated recognition of driving\nmaneuvers is used to classify and evaluate driving behaviour. This not only\nserves as a component to enhance the personalization of insurance policies, but\nalso to increase road safety, reduce accidents and the associated costs as well\nas to reduce fuel consumption and support environmentally friendly driving. In\nthis context maneuver recognition technically requires a continuous application\nof time series classification which poses special challenges to the transfer,\npreprocessing and storage of telematic sensor data, the training of predictive\nmodels, and the prediction itself. Although much research has been done in the\nfield of gathering relevant data or regarding the methods to build predictive\nmodels for the task of maneuver recognition, there is a practical need for\npython packages and functions that allow to quickly transform data into the\nrequired structure as well as to build and evaluate such models. The\nmaneuverRecognition package was therefore developed to provide the necessary\nfunctions for preprocessing, modelling and evaluation and also includes a ready\nto use LSTM based network structure that can be modified. The implementation of\nthe package is demonstrated using real driving data of three different persons\nrecorded via smartphone sensors.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3a`maneuverRecognition`\u7684Python\u8f6f\u4ef6\u5305\uff0c\u65e8\u5728\u7b80\u5316\u8f66\u8f7d\u8fdc\u7a0b\u4fe1\u606f\u5904\u7406\u4e2d\u9a7e\u9a76\u64cd\u4f5c\u8bc6\u522b\u7684\u6570\u636e\u9884\u5904\u7406\u3001\u6a21\u578b\u6784\u5efa\u548c\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u64cd\u4f5c\u8bc6\u522b\u5bf9\u4e8e\u63d0\u5347\u4fdd\u9669\u4e2a\u6027\u5316\u3001\u63d0\u9ad8\u9053\u8def\u5b89\u5168\u3001\u51cf\u5c11\u4e8b\u6545\u53ca\u6210\u672c\u3001\u964d\u4f4e\u6cb9\u8017\u548c\u652f\u6301\u73af\u4fdd\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5728\u6570\u636e\u6536\u96c6\u548c\u9884\u6d4b\u6a21\u578b\u6784\u5efa\u65b9\u9762\u5df2\u6709\u5927\u91cf\u7814\u7a76\uff0c\u4f46\u5f53\u524d\u7f3a\u4e4f\u5b9e\u7528\u7684Python\u8f6f\u4ef6\u5305\u548c\u51fd\u6570\uff0c\u80fd\u591f\u5feb\u901f\u8f6c\u6362\u6570\u636e\u7ed3\u6784\u3001\u6784\u5efa\u548c\u8bc4\u4f30\u76f8\u5173\u6a21\u578b\uff0c\u56e0\u6b64\u5b58\u5728\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86`maneuverRecognition` Python\u8f6f\u4ef6\u5305\uff0c\u5176\u4e2d\u5305\u542b\u6570\u636e\u9884\u5904\u7406\u3001\u6a21\u578b\u6784\u5efa\u548c\u8bc4\u4f30\u6240\u9700\u7684\u529f\u80fd\u3002\u8be5\u8f6f\u4ef6\u5305\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u4e14\u53ef\u4fee\u6539\u7684\u57fa\u4e8eLSTM\u7684\u7f51\u7edc\u7ed3\u6784\u3002\u901a\u8fc7\u4f7f\u7528\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u8bb0\u5f55\u7684\u4e09\u4e2a\u4e0d\u540c\u4eba\u5458\u7684\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\uff0c\u6f14\u793a\u4e86\u8be5\u8f6f\u4ef6\u5305\u7684\u5b9e\u73b0\u548c\u5e94\u7528\u3002", "result": "\u5f00\u53d1\u4e86`maneuverRecognition` Python\u8f6f\u4ef6\u5305\uff0c\u5b83\u4e3a\u9a7e\u9a76\u64cd\u4f5c\u8bc6\u522b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4ece\u6570\u636e\u9884\u5904\u7406\u5230\u6a21\u578b\u6784\u5efa\u548c\u8bc4\u4f30\u7684\u5fc5\u8981\u529f\u80fd\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u53ef\u5b9a\u5236\u7684LSTM\u7f51\u7edc\u7ed3\u6784\uff0c\u5df2\u6210\u529f\u5728\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u6f14\u793a\u3002", "conclusion": "`maneuverRecognition`\u8f6f\u4ef6\u5305\u6709\u6548\u5730\u586b\u8865\u4e86\u9a7e\u9a76\u64cd\u4f5c\u8bc6\u522b\u9886\u57df\u4e2d\u5b9e\u7528\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7efc\u5408\u6027\u7684Python\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u7b80\u5316\u8be5\u4efb\u52a1\u7684\u6570\u636e\u5904\u7406\u548c\u6a21\u578b\u5f00\u53d1\u6d41\u7a0b\u3002"}}
{"id": "2506.22866", "pdf": "https://arxiv.org/pdf/2506.22866", "abs": "https://arxiv.org/abs/2506.22866", "authors": ["Hang-Cheng Dong", "Lu Zou", "Bingguo Liu", "Dong Ye", "Guodong Liu"], "title": "Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Surface defect detection plays a critical role in industrial quality\ninspection. Recent advances in artificial intelligence have significantly\nenhanced the automation level of detection processes. However, conventional\nsemantic segmentation and object detection models heavily rely on large-scale\nannotated datasets, which conflicts with the practical requirements of defect\ndetection tasks. This paper proposes a novel weakly supervised semantic\nsegmentation framework comprising two key components: a region-aware class\nactivation map (CAM) and pseudo-label training. To address the limitations of\nexisting CAM methods, especially low-resolution thermal maps, and insufficient\ndetail preservation, we introduce filtering-guided backpropagation (FGBP),\nwhich refines target regions by filtering gradient magnitudes to identify areas\nwith higher relevance to defects. Building upon this, we further develop a\nregion-aware weighted module to enhance spatial precision. Finally,\npseudo-label segmentation is implemented to refine the model's performance\niteratively. Comprehensive experiments on industrial defect datasets\ndemonstrate the superiority of our method. The proposed framework effectively\nbridges the gap between weakly supervised learning and high-precision defect\nsegmentation, offering a practical solution for resource-constrained industrial\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u533a\u57df\u611f\u77e5\u7c7b\u6fc0\u6d3b\u56fe\uff08CAM\uff09\u548c\u4f2a\u6807\u7b7e\u8bad\u7ec3\uff0c\u4ee5\u89e3\u51b3\u5de5\u4e1a\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u4e2d\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7f3a\u9677\u5206\u5272\u3002", "motivation": "\u4f20\u7edf\u8bed\u4e49\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u4e2d\uff0c\u56e0\u4e25\u91cd\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u800c\u4e0e\u5b9e\u9645\u6807\u6ce8\u8d44\u6e90\u6709\u9650\u7684\u9700\u6c42\u76f8\u51b2\u7a81\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u533a\u57df\u611f\u77e5\u7c7b\u6fc0\u6d3b\u56fe\uff08CAM\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u6ee4\u6ce2\u5f15\u5bfc\u53cd\u5411\u4f20\u64ad\uff08FGBP\uff09\u548c\u533a\u57df\u611f\u77e5\u52a0\u6743\u6a21\u5757\u6765\u4f18\u5316\u4f4e\u5206\u8fa8\u7387\u548c\u7ec6\u8282\u4e0d\u8db3\u95ee\u9898\uff0c\u4ee5\u8bc6\u522b\u66f4\u76f8\u5173\u7684\u7f3a\u9677\u533a\u57df\uff1b2) \u4f2a\u6807\u7b7e\u8bad\u7ec3\uff0c\u7528\u4e8e\u8fed\u4ee3\u7cbe\u70bc\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728\u5de5\u4e1a\u7f3a\u9677\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u8fde\u63a5\u4e86\u5f31\u76d1\u7763\u5b66\u4e60\u548c\u9ad8\u7cbe\u5ea6\u7f3a\u9677\u5206\u5272\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u5de5\u4e1a\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u7f3a\u9677\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23463", "pdf": "https://arxiv.org/pdf/2506.23463", "abs": "https://arxiv.org/abs/2506.23463", "authors": ["Jang Won June"], "title": "What to Keep and What to Drop: Adaptive Table Filtering Framework", "categories": ["cs.CL", "I.2.7"], "comment": "26 pages, 9 figures", "summary": "Large language models (LLMs) for table-based reasoning often struggle with\nlarge tables due to input length limits. We propose ATF (Adaptive Table\nFiltering Framework), a modular and question-aware filtering pipeline that\nprunes uninformative columns and rows using LLM-generated column descriptions,\nclustering, and sparse-dense alignment scores. ATF integrates seamlessly with\nexisting models (e.g., TAPAS, TAPEX) without retraining. Experiments show that\nATF reduces table cells by ~70\\%, boosting performance on out-of-domain TableQA\ntasks while causing slight performance drops on Table Fact Verification, where\nfull-table context is more critical. These results highlight ATF's ability to\nadaptively balance informativeness and minimalism across tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faATF\u6846\u67b6\uff0c\u901a\u8fc7\u8fc7\u6ee4\u975e\u5fc5\u8981\u884c\u5217\uff0c\u663e\u8457\u7f29\u51cf\u8868\u683c\u5927\u5c0f\uff0c\u5e2e\u52a9LLMs\u66f4\u597d\u5730\u5904\u7406\u5927\u578b\u8868\u683c\u6570\u636e\uff0c\u5e76\u5728\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u4e0a\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u5927\u578b\u8868\u683c\u65f6\uff0c\u7531\u4e8e\u8f93\u5165\u957f\u5ea6\u9650\u5236\uff0c\u96be\u4ee5\u6709\u6548\u8fdb\u884c\u57fa\u4e8e\u8868\u683c\u7684\u63a8\u7406\u3002", "method": "\u63d0\u51faATF\uff08Adaptive Table Filtering Framework\uff09\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u4e14\u95ee\u9898\u611f\u77e5\u7684\u8fc7\u6ee4\u7ba1\u9053\u3002\u8be5\u6846\u67b6\u5229\u7528LLM\u751f\u6210\u7684\u5217\u63cf\u8ff0\u3001\u805a\u7c7b\u548c\u7a00\u758f-\u5bc6\u96c6\u5bf9\u9f50\u5206\u6570\u6765\u4fee\u526a\u975e\u4fe1\u606f\u6027\u5217\u548c\u884c\u3002ATF\u53ef\u4e0e\u73b0\u6709\u6a21\u578b\uff08\u5982TAPAS\u3001TAPEX\uff09\u65e0\u7f1d\u96c6\u6210\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cATF\u80fd\u5c06\u8868\u683c\u5355\u5143\u683c\u6570\u91cf\u51cf\u5c11\u7ea670%\u3002\u5728\u57df\u5916\u8868\u683c\u95ee\u7b54\uff08TableQA\uff09\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f46\u5728\u8868\u683c\u4e8b\u5b9e\u9a8c\u8bc1\uff08Table Fact Verification\uff09\u4efb\u52a1\u4e0a\uff0c\u7531\u4e8e\u5bf9\u5b8c\u6574\u8868\u683c\u4e0a\u4e0b\u6587\u7684\u4f9d\u8d56\u6027\u66f4\u9ad8\uff0c\u6027\u80fd\u7565\u6709\u4e0b\u964d\u3002", "conclusion": "ATF\u6846\u67b6\u80fd\u591f\u81ea\u9002\u5e94\u5730\u5e73\u8861\u4fe1\u606f\u91cf\u548c\u7cbe\u7b80\u6027\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u7684\u9700\u6c42\uff0c\u6709\u6548\u63d0\u5347\u4e86LLMs\u5904\u7406\u5927\u578b\u8868\u683c\u7684\u80fd\u529b\u3002"}}
{"id": "2506.23165", "pdf": "https://arxiv.org/pdf/2506.23165", "abs": "https://arxiv.org/abs/2506.23165", "authors": ["David Bossens", "Atsushi Nitanda"], "title": "Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Safety is an essential requirement for reinforcement learning systems. The\nnewly emerging framework of robust constrained Markov decision processes allows\nlearning policies that satisfy long-term constraints while providing guarantees\nunder epistemic uncertainty. This paper presents mirror descent policy\noptimisation for robust constrained Markov decision processes (RCMDPs), making\nuse of policy gradient techniques to optimise both the policy (as a maximiser)\nand the transition kernel (as an adversarial minimiser) on the Lagrangian\nrepresenting a constrained MDP. In the oracle-based RCMDP setting, we obtain an\n$\\mathcal{O}\\left(\\frac{1}{T}\\right)$ convergence rate for the squared distance\nas a Bregman divergence, and an $\\mathcal{O}\\left(e^{-T}\\right)$ convergence\nrate for entropy-regularised objectives. In the sample-based RCMDP setting, we\nobtain an $\\tilde{\\mathcal{O}}\\left(\\frac{1}{T^{1/3}}\\right)$ convergence rate.\nExperiments confirm the benefits of mirror descent policy optimisation in\nconstrained and unconstrained optimisation, and significant improvements are\nobserved in robustness tests when compared to baseline policy optimisation\nalgorithms.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.22868", "pdf": "https://arxiv.org/pdf/2506.22868", "abs": "https://arxiv.org/abs/2506.22868", "authors": ["Junsung Lee", "Junoh Kang", "Bohyung Han"], "title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 9 figures, 3 tables", "summary": "Previous text-guided video editing methods often suffer from temporal\ninconsistency, motion distortion, and-most notably-limited domain\ntransformation. We attribute these limitations to insufficient modeling of\nspatiotemporal pixel relevance during the editing process. To address this, we\npropose STR-Match, a training-free video editing algorithm that produces\nvisually appealing and spatiotemporally coherent videos through latent\noptimization guided by our novel STR score. The score captures spatiotemporal\npixel relevance across adjacent frames by leveraging 2D spatial attention and\n1D temporal modules in text-to-video (T2V) diffusion models, without the\noverhead of computationally expensive 3D attention mechanisms. Integrated into\na latent optimization framework with a latent mask, STR-Match generates\ntemporally consistent and visually faithful videos, maintaining strong\nperformance even under significant domain transformations while preserving key\nvisual attributes of the source. Extensive experiments demonstrate that\nSTR-Match consistently outperforms existing methods in both visual quality and\nspatiotemporal consistency.", "AI": {"tldr": "STR-Match\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6587\u672c\u5f15\u5bfc\u89c6\u9891\u7f16\u8f91\u7b97\u6cd5\uff0c\u901a\u8fc7\u65b0\u9896\u7684STR\u5206\u6570\u5728\u6f5c\u7a7a\u95f4\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u548c\u9886\u57df\u8f6c\u6362\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u4e14\u65f6\u7a7a\u4e00\u81f4\u7684\u89c6\u9891\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5f15\u5bfc\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u3001\u8fd0\u52a8\u5931\u771f\u548c\u9886\u57df\u8f6c\u6362\u53d7\u9650\u7b49\u95ee\u9898\uff0c\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e9b\u9650\u5236\u6e90\u4e8e\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u65f6\u7a7a\u50cf\u7d20\u5173\u8054\u5efa\u6a21\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSTR-Match\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u7f16\u8f91\u7b97\u6cd5\u3002\u5b83\u901a\u8fc7\u6f5c\u5728\u4f18\u5316\uff0c\u5e76\u7531\u65b0\u9896\u7684STR\u5206\u6570\u6307\u5bfc\u3002\u8be5\u5206\u6570\u5229\u7528\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u6269\u6563\u6a21\u578b\u4e2d\u76842D\u7a7a\u95f4\u6ce8\u610f\u529b\u548c1D\u65f6\u95f4\u6a21\u5757\u6355\u6349\u76f8\u90bb\u5e27\u7684\u65f6\u7a7a\u50cf\u7d20\u5173\u8054\uff0c\u907f\u514d\u4e86\u6602\u8d35\u76843D\u6ce8\u610f\u529b\u673a\u5236\u3002STR-Match\u4e0e\u6f5c\u7a7a\u95f4\u4f18\u5316\u6846\u67b6\u53ca\u6f5c\u63a9\u7801\u7ed3\u5408\u3002", "result": "STR-Match\u751f\u6210\u4e86\u65f6\u95f4\u4e00\u81f4\u4e14\u89c6\u89c9\u5fe0\u5b9e\u7684\u89c6\u9891\uff0c\u5373\u4f7f\u5728\u663e\u8457\u7684\u9886\u57df\u8f6c\u6362\u4e0b\u4e5f\u80fd\u4fdd\u6301\u5f3a\u5927\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6e90\u89c6\u9891\u7684\u5173\u952e\u89c6\u89c9\u5c5e\u6027\u3002\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSTR-Match\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u7a7a\u4e00\u81f4\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "STR-Match\u901a\u8fc7\u6709\u6548\u5efa\u6a21\u65f6\u7a7a\u50cf\u7d20\u5173\u8054\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5f15\u5bfc\u89c6\u9891\u7f16\u8f91\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u5904\u7406\u590d\u6742\u9886\u57df\u8f6c\u6362\u65f6\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.23485", "pdf": "https://arxiv.org/pdf/2506.23485", "abs": "https://arxiv.org/abs/2506.23485", "authors": ["Haocheng Yu", "Yaxiong Wu", "Hao Wang", "Wei Guo", "Yong Liu", "Yawen Li", "Yuyang Ye", "Junping Du", "Enhong Chen"], "title": "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Interactive recommendation is a typical information-seeking task that allows\nusers to interactively express their needs through natural language and obtain\npersonalized recommendations. Large language model-powered (LLM-powered) agents\nhave become a new paradigm in interactive recommendations, effectively\ncapturing users' real-time needs and enhancing personalized experiences.\nHowever, due to limited planning and generalization capabilities, existing\nformulations of LLM-powered interactive recommender agents struggle to\neffectively address diverse and complex user intents, such as intuitive,\nunrefined, or occasionally ambiguous requests. To tackle this challenge, we\npropose a novel thought-augmented interactive recommender agent system (TAIRA)\nthat addresses complex user intents through distilled thought patterns.\nSpecifically, TAIRA is designed as an LLM-powered multi-agent system featuring\na manager agent that orchestrates recommendation tasks by decomposing user\nneeds and planning subtasks, with its planning capacity strengthened through\nThought Pattern Distillation (TPD), a thought-augmentation method that extracts\nhigh-level thoughts from the agent's and human experts' experiences. Moreover,\nwe designed a set of user simulation schemes to generate personalized queries\nof different difficulties and evaluate the recommendations based on specific\ndatasets. Through comprehensive experiments conducted across multiple datasets,\nTAIRA exhibits significantly enhanced performance compared to existing methods.\nNotably, TAIRA shows a greater advantage on more challenging tasks while\ngeneralizing effectively on novel tasks, further validating its superiority in\nmanaging complex user intents within interactive recommendation systems. The\ncode is publicly available at:https://github.com/Alcein/TAIRA.", "AI": {"tldr": "\u9488\u5bf9LLM\u4ee3\u7406\u5728\u4ea4\u4e92\u5f0f\u63a8\u8350\u4e2d\u5904\u7406\u590d\u6742\u7528\u6237\u610f\u56fe\u7684\u5c40\u9650\u6027\uff0c\u672c\u6587\u63d0\u51faTAIRA\u7cfb\u7edf\uff0c\u901a\u8fc7\u84b8\u998f\u601d\u60f3\u6a21\u5f0f\uff08TPD\uff09\u589e\u5f3a\u5176\u89c4\u5212\u80fd\u529b\uff0c\u5e76\u5728\u591a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u548c\u65b0\u9896\u4efb\u52a1\u4e0a\u7684\u663e\u8457\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u4ea4\u4e92\u5f0f\u63a8\u8350\u4ee3\u7406\u5728\u5904\u7406\u591a\u6837\u5316\u3001\u590d\u6742\uff08\u5982\u76f4\u89c2\u3001\u6a21\u7cca\uff09\u7684\u7528\u6237\u610f\u56fe\u65f6\uff0c\u56e0\u89c4\u5212\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51faTAIRA\u7cfb\u7edf\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\u5176\u6838\u5fc3\u662f\u7ba1\u7406\u5668\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5206\u89e3\u7528\u6237\u9700\u6c42\u548c\u89c4\u5212\u5b50\u4efb\u52a1\u6765\u534f\u8c03\u63a8\u8350\u3002\u8be5\u7ba1\u7406\u5668\u667a\u80fd\u4f53\u7684\u89c4\u5212\u80fd\u529b\u901a\u8fc7\u201c\u601d\u60f3\u6a21\u5f0f\u84b8\u998f\u201d\uff08TPD\uff09\u65b9\u6cd5\u589e\u5f3a\uff0cTPD\u4ece\u667a\u80fd\u4f53\u81ea\u8eab\u548c\u4eba\u7c7b\u4e13\u5bb6\u7684\u7ecf\u9a8c\u4e2d\u63d0\u53d6\u9ad8\u7ea7\u601d\u60f3\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u7528\u6237\u6a21\u62df\u65b9\u6848\u751f\u6210\u4e0d\u540c\u96be\u5ea6\u7684\u4e2a\u6027\u5316\u67e5\u8be2\u7528\u4e8e\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u591a\u6570\u636e\u96c6\u7684\u7efc\u5408\u5b9e\u9a8c\uff0cTAIRA\u8868\u73b0\u51fa\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\u4f18\u52bf\u66f4\u4e3a\u660e\u663e\uff0c\u5e76\u5728\u65b0\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u6709\u6548\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "TAIRA\u7cfb\u7edf\u901a\u8fc7\u5f15\u5165\u84b8\u998f\u601d\u60f3\u6a21\u5f0f\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5728\u4ea4\u4e92\u5f0f\u63a8\u8350\u7cfb\u7edf\u4e2d\u5904\u7406\u590d\u6742\u7528\u6237\u610f\u56fe\u7684\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8be5\u9886\u57df\u7684\u5353\u8d8a\u6027\u80fd\u548c\u6cdb\u5316\u4f18\u52bf\u3002"}}
{"id": "2506.23174", "pdf": "https://arxiv.org/pdf/2506.23174", "abs": "https://arxiv.org/abs/2506.23174", "authors": ["Chen Gong", "Bo Liang", "Wei Gao", "Chenren Xu"], "title": "Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data", "categories": ["cs.LG", "cs.AI"], "comment": "Published in MobiSys 2025", "summary": "Generative models have gained significant attention for their ability to\nproduce realistic synthetic data that supplements the quantity of real-world\ndatasets. While recent studies show performance improvements in wireless\nsensing tasks by incorporating all synthetic data into training sets, the\nquality of synthetic data remains unpredictable and the resulting performance\ngains are not guaranteed. To address this gap, we propose tractable and\ngeneralizable metrics to quantify quality attributes of synthetic data -\naffinity and diversity. Our assessment reveals prevalent affinity limitation in\ncurrent wireless synthetic data, leading to mislabeled data and degraded task\nperformance. We attribute the quality limitation to generative models' lack of\nawareness of untrained conditions and domain-specific processing. To mitigate\nthese issues, we introduce SynCheck, a quality-guided synthetic data\nutilization scheme that refines synthetic data quality during task model\ntraining. Our evaluation demonstrates that SynCheck consistently outperforms\nquality-oblivious utilization of synthetic data, and achieves 4.3% performance\nimprovement even when the previous utilization degrades performance by 13.4%.", "AI": {"tldr": "\u9488\u5bf9\u65e0\u7ebf\u611f\u77e5\u4efb\u52a1\u4e2d\u751f\u6210\u6a21\u578b\u5408\u6210\u6570\u636e\u8d28\u91cf\u4e0d\u53ef\u9884\u6d4b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u8861\u91cf\u6307\u6807\uff08\u4eb2\u548c\u5ea6\u3001\u591a\u6837\u6027\uff09\u5e76\u5f15\u5165\u4e86SynCheck\u8d28\u91cf\u5f15\u5bfc\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u5408\u6210\u6570\u636e\u80fd\u63d0\u5347\u65e0\u7ebf\u611f\u77e5\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u5176\u8d28\u91cf\u4e0d\u53ef\u9884\u6d4b\uff0c\u5bfc\u81f4\u6027\u80fd\u589e\u76ca\u65e0\u6cd5\u4fdd\u8bc1\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u5408\u6210\u6570\u636e\u8d28\u91cf\u95ee\u9898\u3002", "method": "1. \u63d0\u51fa\u53ef\u64cd\u4f5c\u4e14\u901a\u7528\u7684\u6307\u6807\uff08\u4eb2\u548c\u5ea6\u3001\u591a\u6837\u6027\uff09\u6765\u91cf\u5316\u5408\u6210\u6570\u636e\u8d28\u91cf\u30022. \u5f15\u5165SynCheck\u65b9\u6848\uff0c\u4e00\u79cd\u8d28\u91cf\u5f15\u5bfc\u7684\u5408\u6210\u6570\u636e\u5229\u7528\u7b56\u7565\uff0c\u5728\u4efb\u52a1\u6a21\u578b\u8bad\u7ec3\u671f\u95f4\u4f18\u5316\u5408\u6210\u6570\u636e\u8d28\u91cf\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u5f53\u524d\u65e0\u7ebf\u5408\u6210\u6570\u636e\u666e\u904d\u5b58\u5728\u4eb2\u548c\u5ea6\u9650\u5236\uff0c\u5bfc\u81f4\u6570\u636e\u8bef\u6807\u8bb0\u548c\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\u3002SynCheck\u65b9\u6848\u6301\u7eed\u4f18\u4e8e\u672a\u8003\u8651\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\u5229\u7528\u65b9\u5f0f\uff0c\u5373\u4f7f\u5728\u539f\u6709\u65b9\u6848\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d13.4%\u7684\u60c5\u51b5\u4e0b\uff0cSynCheck\u4ecd\u80fd\u5b9e\u73b04.3%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u91cf\u5316\u5408\u6210\u6570\u636e\u8d28\u91cf\u5e76\u91c7\u7528SynCheck\u7b49\u8d28\u91cf\u5f15\u5bfc\u65b9\u6848\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u5408\u6210\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u6a21\u578b\u5728\u65e0\u7ebf\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u5229\u7528\u6548\u7387\u548c\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2506.22880", "pdf": "https://arxiv.org/pdf/2506.22880", "abs": "https://arxiv.org/abs/2506.22880", "authors": ["Dang Jisheng", "Wu Xudong", "Wang Bimei", "Lv Ning", "Chen Jiayu", "Jingwen Zhao", "Yichu liu", "Jizhao Liu", "Juncheng Li", "Teng Wang"], "title": "Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing video segmenter and grounder approaches, exemplified by Sa2VA,\ndirectly fuse features within segmentation models. This often results in an\nundesirable entanglement of dynamic visual information and static semantics,\nthereby degrading segmentation accuracy. To systematically mitigate this issue,\nwe propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text\npre-training and a linear decoupling module to address the information\nprocessing limitations inherent in SAM-2. Specifically, first, we devise a\npre-training paradigm that converts textual ground-truth labels into\npoint-level prompts while generating corresponding text masks. These masks are\nrefined through a hybrid loss function to strengthen the model's semantic\ngrounding capabilities. Next, we employ linear projection to disentangle hidden\nstates that generated by a large language model into distinct textual and\nvisual feature subspaces. Finally, a dynamic mask fusion strategy\nsynergistically combines these decoupled features through triple supervision\nfrom predicted text/visual masks and ground-truth annotations. Extensive\nexperiments demonstrate state-of-the-art performance across diverse tasks,\nincluding image segmentation, image question answering, video segmentation, and\nvideo question answering. Our codes are available at\nhttps://github.com/longmalongma/DeSa2VA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDeSa2VA\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u8026\u89c6\u89c9\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u7279\u5f81\u7ea0\u7f20\u5bfc\u81f4\u5206\u5272\u7cbe\u5ea6\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5728\u591a\u79cd\u56fe\u50cf\u4e0e\u89c6\u9891\u5206\u5272\u548c\u95ee\u7b54\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5206\u5272\u548c\u5b9a\u4f4d\u65b9\u6cd5\uff08\u5982Sa2VA\uff09\u76f4\u63a5\u878d\u5408\u7279\u5f81\uff0c\u5bfc\u81f4\u52a8\u6001\u89c6\u89c9\u4fe1\u606f\u4e0e\u9759\u6001\u8bed\u4e49\u4fe1\u606f\u7ea0\u7f20\uff0c\u4ece\u800c\u964d\u4f4e\u5206\u5272\u7cbe\u5ea6\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u7f13\u89e3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51faDeSa2VA\uff0c\u4e00\u4e2a\u89e3\u8026\u589e\u5f3a\u7684\u63d0\u793a\u65b9\u6848\u3002\u9996\u5148\uff0c\u8bbe\u8ba1\u6587\u672c\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u5c06\u6587\u672c\u6807\u7b7e\u8f6c\u6362\u4e3a\u70b9\u7ea7\u63d0\u793a\u5e76\u751f\u6210\u6587\u672c\u63a9\u7801\uff0c\u901a\u8fc7\u6df7\u5408\u635f\u5931\u51fd\u6570\u589e\u5f3a\u8bed\u4e49\u5b9a\u4f4d\u3002\u5176\u6b21\uff0c\u4f7f\u7528\u7ebf\u6027\u6295\u5f71\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u9690\u72b6\u6001\u89e3\u8026\u4e3a\u72ec\u7acb\u7684\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\u5b50\u7a7a\u95f4\u3002\u6700\u540e\uff0c\u901a\u8fc7\u52a8\u6001\u63a9\u7801\u878d\u5408\u7b56\u7565\uff0c\u7ed3\u5408\u89e3\u8026\u540e\u7684\u7279\u5f81\u5e76\u5229\u7528\u4e09\u91cd\u76d1\u7763\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u56fe\u50cf\u5206\u5272\u3001\u56fe\u50cf\u95ee\u7b54\u3001\u89c6\u9891\u5206\u5272\u548c\u89c6\u9891\u95ee\u7b54\u7b49\u591a\u79cd\u4efb\u52a1\u4e0a\uff0c\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DeSa2VA\u901a\u8fc7\u6709\u6548\u89e3\u8026\u89c6\u89c9\u4e0e\u8bed\u4e49\u4fe1\u606f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7279\u5f81\u7ea0\u7f20\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2506.23508", "pdf": "https://arxiv.org/pdf/2506.23508", "abs": "https://arxiv.org/abs/2506.23508", "authors": ["Zhihao Zhang", "Qiaole Dong", "Qi Zhang", "Jun Zhao", "Enyu Zhou", "Zhiheng Xi", "Senjie Jin", "Xiaoran Fan", "Yuhao Zhou", "Yanwei Fu", "Tao Ji", "Tao Gui", "Xuanjing Huang"], "title": "Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages (Preprint. Work in progress)", "summary": "Post-training algorithms such as Supervised Fine-Tuning (SFT) and\nReinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large\nlanguage models to downstream tasks. While effective at task adaptation, their\nimpact on prior knowledge remains unclear. In this paper, we introduce jigsaw\npuzzles as a novel task absent from existing pretraining corpora and\nsystematically study the behavior of SFT and RFT on an open-source multimodal\nmodel, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid\ntask acquisition but leads to catastrophic forgetting, whereas RFT learns more\nslowly on novel tasks but maintains prior knowledge. We analyze this phenomenon\nthrough the lens of learning dynamics, showing that RFT reinforces correct\nsamples that are naturally aligned with the base model's probability landscape,\nmitigating interference with prior knowledge. Moreover, supervised training on\ncorrect RFT-simulated rollouts allows SFT to preserve knowledge while rapidly\nlearning new tasks. These findings suggest that data distribution, rather than\nalgorithmic differences, plays a central role in forgetting, and highlight\nRFT's potential for stable continual learning in multimodal large language\nmodels.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\uff0cSFT\u80fd\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u4f46\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800cRFT\u5b66\u4e60\u8f83\u6162\u4f46\u80fd\u4fdd\u6301\u539f\u6709\u77e5\u8bc6\u3002\u6570\u636e\u5206\u5e03\u800c\u975e\u7b97\u6cd5\u5dee\u5f02\u662f\u9057\u5fd8\u7684\u5173\u952e\u3002", "motivation": "SFT\u548cRFT\u7b49\u540e\u8bad\u7ec3\u7b97\u6cd5\u5728\u4f7f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\u65b9\u9762\u6709\u6548\uff0c\u4f46\u5176\u5bf9\u6a21\u578b\u539f\u6709\u77e5\u8bc6\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5f15\u5165\u201c\u62fc\u56fe\u201d\u8fd9\u4e00\u65b0\u4efb\u52a1\uff0c\u5e76\u5728\u5f00\u6e90\u591a\u6a21\u6001\u6a21\u578bQwen2.5-VL\u4e0a\u7cfb\u7edf\u7814\u7a76\u4e86SFT\u548cRFT\u7684\u884c\u4e3a\uff0c\u901a\u8fc7\u5b66\u4e60\u52a8\u6001\u5206\u6790\u5176\u73b0\u8c61\u3002", "result": "SFT\u80fd\u5feb\u901f\u638c\u63e1\u65b0\u4efb\u52a1\u4f46\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800cRFT\u5b66\u4e60\u8f83\u6162\u4f46\u80fd\u4fdd\u6301\u539f\u6709\u77e5\u8bc6\u3002\u5206\u6790\u8868\u660eRFT\u901a\u8fc7\u5f3a\u5316\u4e0e\u57fa\u7840\u6a21\u578b\u6982\u7387\u666f\u89c2\u5bf9\u9f50\u7684\u6b63\u786e\u6837\u672c\u6765\u51cf\u8f7b\u5bf9\u539f\u6709\u77e5\u8bc6\u7684\u5e72\u6270\u3002\u6b64\u5916\uff0c\u5728RFT\u6a21\u62df\u7684\u6b63\u786e\u8f68\u8ff9\u4e0a\u8fdb\u884cSFT\u8bad\u7ec3\u53ef\u5b9e\u73b0\u5feb\u901f\u5b66\u4e60\u548c\u77e5\u8bc6\u4fdd\u7559\u3002", "conclusion": "\u6570\u636e\u5206\u5e03\u5728\u9057\u5fd8\u4e2d\u626e\u6f14\u6838\u5fc3\u89d2\u8272\uff0c\u800c\u975e\u7b97\u6cd5\u5dee\u5f02\u3002RFT\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5c55\u73b0\u51fa\u7a33\u5b9a\u6301\u7eed\u5b66\u4e60\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.23182", "pdf": "https://arxiv.org/pdf/2506.23182", "abs": "https://arxiv.org/abs/2506.23182", "authors": ["Robert Frank", "Michael Widrich", "Rahmad Akbar", "G\u00fcnter Klambauer", "Geir Kjetil Sandve", "Philippe A. Robert", "Victor Greiff"], "title": "Attribution assignment for deep-generative sequence models enables interpretability analysis using positive-only data", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Generative machine learning models offer a powerful framework for therapeutic\ndesign by efficiently exploring large spaces of biological sequences enriched\nfor desirable properties. Unlike supervised learning methods, which require\nboth positive and negative labeled data, generative models such as LSTMs can be\ntrained solely on positively labeled sequences, for example, high-affinity\nantibodies. This is particularly advantageous in biological settings where\nnegative data are scarce, unreliable, or biologically ill-defined. However, the\nlack of attribution methods for generative models has hindered the ability to\nextract interpretable biological insights from such models. To address this\ngap, we developed Generative Attribution Metric Analysis (GAMA), an attribution\nmethod for autoregressive generative models based on Integrated Gradients. We\nassessed GAMA using synthetic datasets with known ground truths to characterize\nits statistical behavior and validate its ability to recover biologically\nrelevant features. We further demonstrated the utility of GAMA by applying it\nto experimental antibody-antigen binding data. GAMA enables model\ninterpretability and the validation of generative sequence design strategies\nwithout the need for negative training data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GAMA\uff0c\u4e00\u79cd\u57fa\u4e8eIntegrated Gradients\u7684\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\u5f52\u56e0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u91ca\u751f\u7269\u5e8f\u5217\u8bbe\u8ba1\u6a21\u578b\uff0c\u5c24\u5176\u5728\u7f3a\u4e4f\u8d1f\u6837\u672c\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u548c\u6297\u4f53-\u6297\u539f\u7ed3\u5408\u6570\u636e\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6cbb\u7597\u6027\u8bbe\u8ba1\u4e2d\u5177\u6709\u5f3a\u5927\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u751f\u7269\u5b66\u9886\u57df\u8d1f\u6837\u672c\u6570\u636e\u7a00\u7f3a\u65f6\uff0c\u53ef\u4ec5\u7528\u6b63\u6837\u672c\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u5f52\u56e0\u65b9\u6cd5\uff0c\u963b\u788d\u4e86\u4ece\u4e2d\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u751f\u7269\u5b66\u89c1\u89e3\u3002", "method": "\u5f00\u53d1\u4e86GAMA\uff08Generative Attribution Metric Analysis\uff09\uff0c\u4e00\u79cd\u57fa\u4e8eIntegrated Gradients\u7684\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\u5f52\u56e0\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u8bc4\u4f30\u4e86GAMA\u7684\u7edf\u8ba1\u884c\u4e3a\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6062\u590d\u751f\u7269\u5b66\u76f8\u5173\u7279\u5f81\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5c06GAMA\u5e94\u7528\u4e8e\u5b9e\u9a8c\u6027\u6297\u4f53-\u6297\u539f\u7ed3\u5408\u6570\u636e\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "GAMA\u5b9e\u73b0\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u80fd\u9a8c\u8bc1\u751f\u6210\u5e8f\u5217\u8bbe\u8ba1\u7b56\u7565\uff0c\u800c\u65e0\u9700\u8d1f\u8bad\u7ec3\u6570\u636e\u3002"}}
{"id": "2506.22448", "pdf": "https://arxiv.org/pdf/2506.22448", "abs": "https://arxiv.org/abs/2506.22448", "authors": ["Yu Ma", "Xingyu Zhou", "Xiao Li", "Le Liang", "Shi Jin"], "title": "Unsupervised Learning-Based Joint Resource Allocation and Beamforming Design for RIS-Assisted MISO-OFDMA Systems", "categories": ["eess.SP", "cs.AI", "cs.IT", "math.IT"], "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract here is shorter than that in the PDF file", "summary": "Reconfigurable intelligent surfaces (RIS) are key enablers for 6G wireless\nsystems. This paper studies downlink transmission in an RIS-assisted MISO-OFDMA\nsystem, addressing resource allocation challenges. A two-stage unsupervised\nlearning-based framework is proposed to jointly design RIS phase shifts, BS\nbeamforming, and resource block (RB) allocation. The framework includes\nBeamNet, which predicts RIS phase shifts from CSI, and AllocationNet, which\nallocates RBs using equivalent CSI derived from BeamNet outputs. Active\nbeamforming is implemented via maximum ratio transmission and water-filling. To\nhandle discrete constraints while ensuring differentiability, quantization and\nthe Gumbel-softmax trick are adopted. A customized loss and phased training\nenhance performance under QoS constraints. Simulations show the method achieves\n99.93% of the sum rate of the SCA baseline with only 0.036% of its runtime, and\nit remains robust across varying channel and user conditions.", "AI": {"tldr": "\u9488\u5bf9RIS\u8f85\u52a9MISO-OFDMA\u7cfb\u7edf\u7684\u8d44\u6e90\u5206\u914d\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u5e76\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "motivation": "RIS\u4f5c\u4e3a6G\u5173\u952e\u6280\u672f\uff0c\u5728RIS\u8f85\u52a9\u7684MISO-OFDMA\u7cfb\u7edf\u4e2d\uff0c\u8054\u5408\u4f18\u5316RIS\u76f8\u79fb\u3001\u57fa\u7ad9\u6ce2\u675f\u6210\u5f62\u548c\u8d44\u6e90\u5757\u5206\u914d\u5b58\u5728\u590d\u6742\u7684\u8d44\u6e90\u5206\u914d\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\uff0cBeamNet\u6839\u636e\u4fe1\u9053\u72b6\u6001\u4fe1\u606f(CSI)\u9884\u6d4bRIS\u76f8\u79fb\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff0cAllocationNet\u5229\u7528BeamNet\u8f93\u51fa\u7684\u7b49\u6548CSI\u5206\u914d\u8d44\u6e90\u5757\u3002\u540c\u65f6\uff0c\u91c7\u7528\u6700\u5927\u6bd4\u7387\u4f20\u8f93\u548c\u6ce8\u6c34\u7b97\u6cd5\u5b9e\u73b0\u4e3b\u52a8\u6ce2\u675f\u6210\u5f62\u3002\u4e3a\u5904\u7406\u79bb\u6563\u7ea6\u675f\u5e76\u786e\u4fdd\u53ef\u5fae\u6027\uff0c\u5f15\u5165\u4e86\u91cf\u5316\u548cGumbel-softmax\u6280\u5de7\uff0c\u5e76\u901a\u8fc7\u5b9a\u5236\u635f\u5931\u51fd\u6570\u548c\u5206\u9636\u6bb5\u8bad\u7ec3\u8fdb\u4e00\u6b65\u4f18\u5316\u6027\u80fd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u603b\u548c\u901f\u7387\u4e0a\u8fbe\u5230SCA\u57fa\u7ebf\u768499.93%\uff0c\u800c\u8fd0\u884c\u65f6\u95f4\u4ec5\u4e3aSCA\u76840.036%\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u4fe1\u9053\u548c\u7528\u6237\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3RIS\u8f85\u52a9MISO-OFDMA\u7cfb\u7edf\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5728\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u4f9d\u7136\u80fd\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2506.22881", "pdf": "https://arxiv.org/pdf/2506.22881", "abs": "https://arxiv.org/abs/2506.22881", "authors": ["Fumiya Uchiyama", "Rintaro Yanagi", "Shohei Taniguchi", "Shota Takashiro", "Masahiro Suzuki", "Hirokatsu Kataoka", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "How Semantically Informative is an Image?: Measuring the Covariance-Weighted Norm of Contrastive Learning Embeddings", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive learning has the capacity to model multimodal probability\ndistributions by embedding and aligning visual representations with semantics\nfrom captions. This approach enables the estimation of relational semantic\nsimilarity; however, it remains unclear whether it can also represent absolute\nsemantic informativeness. In this work, we introduce a semantic informativeness\nmetric for an image calculated from text samples via a contrastive learning\nmodel; similarly, the informativeness of a text is calculated from image\nsamples. We propose a redefinition of the concept of Information Gain, a\nconcept previously explored in natural language processing, extending its\napplication to the domains of vision and language. Our metric quantifies how\nconditioning on an image distorts the distribution of associated texts, and\nvice versa for text conditioning on image distributions. In OpenCLIP's\nempirical results, we observe that images with the lowest Information Gain\nscores often correspond to placeholder icons such as \"image not found.\"\nFurthermore, we propose to measure a norm-based metric of the embedding to\nestimate the Information Gain, following the theoretical results for Skip-Gram\nwith Negative Sampling (SGNS) word embedding. Information Gain can be measured\nusing either CLIP or SigLIP, and the results demonstrate a strong correlation\nwith a coefficient of determination ranging from 0.98 to 1.00. After obtaining\nthe mean and the covariance of the sample embedding, the computational cost of\nthis method is independent of the sample size, and it is compatible with\npublicly available, open-weight models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u56fe\u50cf\u548c\u6587\u672c\u8bed\u4e49\u4fe1\u606f\u91cf\u5ea6\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u4fe1\u606f\u589e\u76ca\uff08IG\uff09\u6982\u5ff5\uff0c\u91cf\u5316\u5185\u5bb9\u7684\u4fe1\u606f\u4e30\u5bcc\u5ea6\uff0c\u5e76\u53d1\u73b0\u4f4eIG\u503c\u5e38\u6307\u793a\u65e0\u4fe1\u606f\u5185\u5bb9\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u5d4c\u5165\u8303\u6570\u7684IG\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5728OpenCLIP\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\u867d\u80fd\u4f30\u8ba1\u5173\u7cfb\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff0c\u4f46\u5176\u662f\u5426\u80fd\u8868\u793a**\u7edd\u5bf9\u8bed\u4e49\u4fe1\u606f\u91cf**\u5c1a\u4e0d\u660e\u786e\uff0c\u8fd9\u662f\u672c\u7814\u7a76\u5173\u6ce8\u7684\u6838\u5fc3\u95ee\u9898\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u4fe1\u606f\u91cf\u5ea6\u91cf\uff0c\u5206\u522b\u4ece\u6587\u672c\u6837\u672c\u8ba1\u7b97\u56fe\u50cf\u7684\u4fe1\u606f\u91cf\uff0c\u4ece\u56fe\u50cf\u6837\u672c\u8ba1\u7b97\u6587\u672c\u7684\u4fe1\u606f\u91cf\uff0c\u5747\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\u3002 2. \u91cd\u65b0\u5b9a\u4e49\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u5df2\u6709\u7684\u4fe1\u606f\u589e\u76ca\uff08IG\uff09\u6982\u5ff5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u6269\u5c55\u81f3\u89c6\u89c9\u4e0e\u8bed\u8a00\u9886\u57df\uff0c\u7528\u4e8e\u91cf\u5316\u56fe\u50cf\uff08\u6216\u6587\u672c\uff09\u5bf9\u5173\u8054\u6587\u672c\uff08\u6216\u56fe\u50cf\uff09\u5206\u5e03\u7684\u626d\u66f2\u7a0b\u5ea6\u3002 3. \u501f\u9274Skip-Gram with Negative Sampling (SGNS)\u8bcd\u5d4c\u5165\u7684\u7406\u8bba\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5d4c\u5165\u8303\u6570\uff08norm-based metric\uff09\u7684\u65b9\u6cd5\u6765\u4f30\u8ba1\u4fe1\u606f\u589e\u76ca\u3002", "result": "1. \u5728OpenCLIP\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4fe1\u606f\u589e\u76ca\u5f97\u5206\u6700\u4f4e\u7684\u56fe\u50cf\u5e38\u5bf9\u5e94\u201c\u56fe\u50cf\u672a\u627e\u5230\u201d\u7b49\u5360\u4f4d\u7b26\u56fe\u6807\u3002 2. \u63d0\u51fa\u7684\u8303\u6570\u4f30\u8ba1\u7b97\u6cd5\u4e0e\u4f7f\u7528CLIP\u6216SigLIP\u8ba1\u7b97\u7684\u4fe1\u606f\u589e\u76ca\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u51b3\u5b9a\u7cfb\u6570R\u00b2\u57280.98\u81f31.00\u4e4b\u95f4\u3002 3. \u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u51fa\u6837\u672c\u5d4c\u5165\u7684\u5747\u503c\u548c\u534f\u65b9\u5dee\u540e\uff0c\u5176\u8ba1\u7b97\u6210\u672c\u4e0e\u6837\u672c\u5927\u5c0f\u65e0\u5173\uff0c\u5e76\u4e14\u517c\u5bb9\u516c\u5f00\u7684\u5f00\u6e90\u6a21\u578b\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u5f15\u5165\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u91cf\u5316\u591a\u6a21\u6001\u5185\u5bb9\uff08\u56fe\u50cf\u4e0e\u6587\u672c\uff09\u7edd\u5bf9\u8bed\u4e49\u4fe1\u606f\u91cf\u7684\u65b0\u65b9\u6cd5\u2014\u2014\u91cd\u5b9a\u4e49\u7684\u4fe1\u606f\u589e\u76ca\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u8bc6\u522b\u4f4e\u4fe1\u606f\u91cf\u5185\u5bb9\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u517c\u5bb9\u73b0\u6709\u6a21\u578b\u7684\u4f30\u7b97\u8303\u5f0f\uff0c\u4e3a\u8bc4\u4f30\u548c\u7406\u89e3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4fe1\u606f\u5c5e\u6027\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.23524", "pdf": "https://arxiv.org/pdf/2506.23524", "abs": "https://arxiv.org/abs/2506.23524", "authors": ["Phan Quoc Hung Mai", "Quang Hung Nguyen", "Phuong Giang Duong", "Hong Hanh Nguyen", "Nguyen Tuan Long"], "title": "NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the field of education, understanding students' opinions through their\ncomments is crucial, especially in the Vietnamese language, where resources\nremain limited. Existing educational datasets often lack domain relevance and\nstudent slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese\ndataset for Educational Sentiment Classification and Topic Classification,\ncurated from university forums, which offers more samples, richer class\ndiversity, longer texts, and broader vocabulary. In addition, we explore\nmultitask learning using encoder-only language models (BERT), in which we\nshowed that it achieves performance up to 83.7% and 79.8% accuracy for\nsentiment and topic classification tasks. We also benchmark our dataset and\nmodel with other datasets and models, including Large Language Models, and\ndiscuss these benchmarks. The dataset is publicly available at:\nhttps://huggingface.co/datasets/hung20gg/NEU-ESC.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a8\u51fa\u4e86NEU-ESC\u8d8a\u5357\u8bed\u6559\u80b2\u60c5\u611f\u4e0e\u4e3b\u9898\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u57fa\u4e8eBERT\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u60c5\u611f\u5206\u7c7b\u548c\u4e3b\u9898\u5206\u7c7b\u4efb\u52a1\u4e0a\u5206\u522b\u8fbe\u5230\u4e8683.7%\u548c79.8%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5728\u6559\u80b2\u9886\u57df\uff0c\u7406\u89e3\u5b66\u751f\u8bc4\u8bba\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u8d8a\u5357\u8bed\u73af\u5883\u4e2d\uff0c\u73b0\u6709\u8d44\u6e90\u6709\u9650\uff0c\u4e14\u73b0\u6709\u6559\u80b2\u6570\u636e\u96c6\u7f3a\u4e4f\u9886\u57df\u76f8\u5173\u6027\u548c\u5b66\u751f\u4fda\u8bed\uff0c\u65e0\u6cd5\u6709\u6548\u652f\u6301\u5b66\u751f\u60c5\u611f\u4e0e\u4e3b\u9898\u5206\u6790\u3002", "method": "\u5f15\u5165\u4e86NEU-ESC\uff0c\u4e00\u4e2a\u4ece\u5927\u5b66\u8bba\u575b\u6536\u96c6\u7684\u5168\u65b0\u8d8a\u5357\u8bed\u6559\u80b2\u60c5\u611f\u4e0e\u4e3b\u9898\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6837\u672c\u66f4\u591a\u3001\u7c7b\u522b\u66f4\u4e30\u5bcc\u3001\u6587\u672c\u66f4\u957f\u3001\u8bcd\u6c47\u66f4\u5e7f\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63a2\u7d22\u4e86\u4f7f\u7528\u7f16\u7801\u5668\u4e13\u7528\u8bed\u8a00\u6a21\u578b\uff08BERT\uff09\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u4e0e\u5305\u62ec\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5185\u7684\u5176\u4ed6\u6570\u636e\u96c6\u548c\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528BERT\u6a21\u578b\u5728\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e8683.7%\u7684\u51c6\u786e\u7387\uff0c\u5728\u4e3b\u9898\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e8679.8%\u7684\u51c6\u786e\u7387\u3002\u7814\u7a76\u8fd8\u5bf9\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0e\u5176\u4ed6\u6570\u636e\u96c6\u548c\u6a21\u578b\uff08\u5305\u62ec\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u8fdb\u884c\u4e86\u6709\u6548\u7684\u57fa\u51c6\u6bd4\u8f83\u3002", "conclusion": "NEU-ESC\u6570\u636e\u96c6\u7684\u521b\u5efa\u6709\u6548\u586b\u8865\u4e86\u8d8a\u5357\u8bed\u6559\u80b2\u9886\u57df\u6570\u636e\u8d44\u6e90\u7684\u7a7a\u767d\u3002\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u548cBERT\u6a21\u578b\uff0c\u672c\u7814\u7a76\u4e3a\u6709\u6548\u5206\u6790\u8d8a\u5357\u8bed\u5b66\u751f\u8bc4\u8bba\u4e2d\u7684\u60c5\u611f\u548c\u4e3b\u9898\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u65b9\u6cd5\u548c\u5de5\u5177\uff0c\u5e76\u4fc3\u8fdb\u4e86\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
