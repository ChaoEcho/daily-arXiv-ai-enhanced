{"id": "2509.08097", "pdf": "https://arxiv.org/pdf/2509.08097", "abs": "https://arxiv.org/abs/2509.08097", "authors": ["Stephen Jasina", "Loqman Salamatian", "Joshua Mathews", "Scott Anderson", "Paul Barford", "Mark Crovella", "Walter Willinger"], "title": "Matisse: Visualizing Measured Internet Latencies as Manifolds", "categories": ["cs.NI", "C.2.1"], "comment": "16 pages, 14 figures", "summary": "Manifolds are complex topological spaces that can be used to represent\ndatasets of real-world measurements. Visualizing such manifolds can help with\nillustrating their topological characteristics (e.g., curvature) and providing\ninsights into important properties of the underlying data (e.g., anomalies in\nthe measurements). In this paper, we describe a new methodology and system for\ngenerating and visualizing manifolds that are inferred from actual Internet\nlatency measurements between different cities and are projected over a 2D\nEuclidean space (e.g., a geographic map). Our method leverages a series of\ngraphs that capture critical information contained in the data, including\nwell-defined locations (for vertices) and Ricci curvature information (for\nedges). Our visualization approach then generates a curved surface (manifold)\nin which (a) geographical locations of vertices are maintained and (b) the\nRicci curvature values of the graph edges determine the curvature properties of\nthe manifold. The resulting manifold highlights areas of critical connectivity\nand defines an instance of \"Internet delay space\" where latency measurements\nmanifest as geodesics. We describe details of our method and its implementation\nin a tool, which we call Matisse, for generating, visualizing and manipulating\nmanifolds projected onto a base map. We illustrate Matisse with two case\nstudies: a simple example to demonstrate key concepts, and visualizations of\nthe US public Internet to show Matisse's utility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Matisse \u7684\u65b0\u65b9\u6cd5\u548c\u7cfb\u7edf\uff0c\u7528\u4e8e\u5c06\u4ece\u4e92\u8054\u7f51\u5ef6\u8fdf\u6d4b\u91cf\u4e2d\u63a8\u65ad\u51fa\u7684\u6d41\u5f62\u53ef\u89c6\u5316\u5e76\u6295\u5f71\u5230\u5730\u7406\u5730\u56fe\u4e0a\uff0c\u4ee5\u63ed\u793a\u7f51\u7edc\u62d3\u6251\u7279\u6027\u548c\u5173\u952e\u8fde\u63a5\u533a\u57df\u3002", "motivation": "\u6d41\u5f62\u80fd\u591f\u8868\u793a\u590d\u6742\u6570\u636e\u96c6\u5e76\u53ef\u89c6\u5316\u5176\u62d3\u6251\u7279\u6027\uff08\u5982\u66f2\u7387\uff09\u548c\u6570\u636e\u5c5e\u6027\uff08\u5982\u5f02\u5e38\uff09\u3002\u5c06\u4e92\u8054\u7f51\u5ef6\u8fdf\u6d4b\u91cf\u6570\u636e\u8f6c\u6362\u4e3a\u6d41\u5f62\u5e76\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u7f51\u7edc\u7ed3\u6784\u548c\u6027\u80fd\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e00\u7cfb\u5217\u56fe\u6765\u6355\u83b7\u6570\u636e\u5173\u952e\u4fe1\u606f\uff08\u9876\u70b9\u5730\u7406\u4f4d\u7f6e\u548c\u8fb9\u7684Ricci\u66f2\u7387\uff09\uff0c\u4ece\u57ce\u5e02\u95f4\u7684\u4e92\u8054\u7f51\u5ef6\u8fdf\u6d4b\u91cf\u4e2d\u63a8\u65ad\u51fa\u6d41\u5f62\u3002\u53ef\u89c6\u5316\u8fc7\u7a0b\u5c06\u6d41\u5f62\u6295\u5f71\u52302D\u5730\u7406\u7a7a\u95f4\uff0c\u4fdd\u6301\u9876\u70b9\u7684\u5730\u7406\u4f4d\u7f6e\uff0c\u5e76\u6839\u636e\u56fe\u8fb9\u4e0a\u7684Ricci\u66f2\u7387\u503c\u786e\u5b9a\u6d41\u5f62\u7684\u66f2\u7387\u7279\u6027\u3002\u8be5\u7cfb\u7edf\u540d\u4e3aMatisse\u3002", "result": "\u751f\u6210\u7684\u6d41\u5f62\u7a81\u51fa\u4e86\u5173\u952e\u8fde\u63a5\u533a\u57df\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e00\u4e2a\u201c\u4e92\u8054\u7f51\u5ef6\u8fdf\u7a7a\u95f4\u201d\uff0c\u5176\u4e2d\u5ef6\u8fdf\u6d4b\u91cf\u8868\u73b0\u4e3a\u6d4b\u5730\u7ebf\u3002Matisse\u5de5\u5177\u6210\u529f\u5b9e\u73b0\u4e86\u6d41\u5f62\u7684\u751f\u6210\u3001\u53ef\u89c6\u5316\u548c\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\uff08\u5305\u62ec\u7f8e\u56fd\u516c\u5171\u4e92\u8054\u7f51\u7684\u53ef\u89c6\u5316\uff09\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86Matisse\u5de5\u5177\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u56fe\u7684\u6d41\u5f62\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u4e92\u8054\u7f51\u5ef6\u8fdf\u6570\u636e\u3002\u901a\u8fc7\u5c06\u5730\u7406\u4f4d\u7f6e\u4e0eRicci\u66f2\u7387\u76f8\u7ed3\u5408\uff0cMatisse\u80fd\u6709\u6548\u63ed\u793a\u7f51\u7edc\u5173\u952e\u8fde\u63a5\u548c\u5ef6\u8fdf\u7a7a\u95f4\u7279\u6027\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.08124", "pdf": "https://arxiv.org/pdf/2509.08124", "abs": "https://arxiv.org/abs/2509.08124", "authors": ["Ian Jessen"], "title": "UTM Performance Under Stressing Scenarios", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "Proliferation of new classes of airspace participants, including uncrewed and\nadvanced aerial mobility vehicles, necessitates the development and deployment\nof novel airspace management solutions, such as the Unmanned Traffic Management\n(UTM) system and the Provider of Services to UAM (PSU) Network. The efficacy of\nsuch systems has been demonstrated on multiple occasions via real-world\ndeployments in limited test environments, however exploration of system\nbehavior under stressing conditions requires the development of appropriate\nmodeling and simulation (M&S) environments. Autonomy Networks for Advanced\nMobility at Lincoln Laboratory (ANAMLL) is a virtual Systems Integration\nLaboratory (SIL) designed to host federated autonomy networks, such as a UTM or\nPSU Network, and to enable test and validation at scales not available in\nreal-world deployments. As an example of ANAMLL's utility, we explore the\nperformance of a representative UTM network during a stressing demand scenario.\nIn a close examination of the demand scenario, ANAMLL demonstrates a UTM system\ndemand point at which in-flight replanning can no longer be accomplished within\nan allowable time window. In a second analysis of the same scenario, ANAMLL\ndemonstrates the impact of network connectivity performance on end-user\nairspace access.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u865a\u62df\u4eff\u771f\u73af\u5883ANAMLL\uff0c\u7528\u4e8e\u5728\u538b\u529b\u6761\u4ef6\u4e0b\u6d4b\u8bd5UTM\uff08\u65e0\u4eba\u4ea4\u901a\u7ba1\u7406\uff09\u7f51\u7edc\uff0c\u63ed\u793a\u4e86\u98de\u884c\u4e2d\u91cd\u89c4\u5212\u7684\u5c40\u9650\u6027\u4ee5\u53ca\u7f51\u7edc\u8fde\u63a5\u5bf9\u7a7a\u57df\u8bbf\u95ee\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u65b0\u578b\u7a7a\u57df\u53c2\u4e0e\u8005\uff08\u5982\u65e0\u4eba\u673a\u548c\u5148\u8fdb\u7a7a\u4e2d\u4ea4\u901a\u5de5\u5177\uff09\u7684\u6fc0\u589e\uff0c\u6025\u9700\u5f00\u53d1\u65b0\u578b\u7a7a\u57df\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff08\u5982UTM\u548cPSU\u7f51\u7edc\uff09\u3002\u73b0\u6709\u7cfb\u7edf\u5728\u6709\u9650\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u9a8c\u8bc1\u4e0d\u8db3\u4ee5\u63a2\u7d22\u5176\u5728\u538b\u529b\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u5408\u9002\u7684\u5efa\u6a21\u4e0e\u4eff\u771f\u73af\u5883\u3002", "method": "\u672c\u6587\u5229\u7528\u6797\u80af\u5b9e\u9a8c\u5ba4\u7684ANAMLL\uff08Autonomy Networks for Advanced Mobility at Lincoln Laboratory\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u865a\u62df\u7cfb\u7edf\u96c6\u6210\u5b9e\u9a8c\u5ba4\uff08SIL\uff09\uff0c\u7528\u4e8e\u6258\u7ba1\u548c\u6d4b\u8bd5\u8054\u90a6\u81ea\u4e3b\u7f51\u7edc\uff0c\u5e76\u652f\u6301\u5728\u771f\u5b9e\u90e8\u7f72\u65e0\u6cd5\u5b9e\u73b0\u7684\u89c4\u6a21\u4e0b\u8fdb\u884c\u6d4b\u8bd5\u548c\u9a8c\u8bc1\u3002\u7814\u7a76\u901a\u8fc7ANAMLL\u63a2\u7d22\u4e86\u4e00\u4e2a\u4ee3\u8868\u6027UTM\u7f51\u7edc\u5728\u6781\u7aef\u9700\u6c42\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5728\u4e00\u6b21\u9700\u6c42\u573a\u666f\u7684\u8be6\u7ec6\u68c0\u67e5\u4e2d\uff0cANAMLL\u5c55\u793a\u4e86UTM\u7cfb\u7edf\u7684\u4e00\u4e2a\u9700\u6c42\u70b9\uff0c\u5728\u8be5\u70b9\u4e0a\uff0c\u98de\u884c\u4e2d\u91cd\u65b0\u89c4\u5212\u65e0\u6cd5\u5728\u5141\u8bb8\u7684\u65f6\u95f4\u7a97\u53e3\u5185\u5b8c\u6210\u3002\u5728\u5bf9\u540c\u4e00\u573a\u666f\u7684\u7b2c\u4e8c\u6b21\u5206\u6790\u4e2d\uff0cANAMLL\u5c55\u793a\u4e86\u7f51\u7edc\u8fde\u63a5\u6027\u80fd\u5bf9\u6700\u7ec8\u7528\u6237\u7a7a\u57df\u8bbf\u95ee\u7684\u5f71\u54cd\u3002", "conclusion": "ANAMLL\u4f5c\u4e3a\u4e00\u4e2a\u865a\u62df\u4eff\u771f\u5e73\u53f0\uff0c\u6709\u6548\u8bc1\u660e\u4e86\u5176\u5728\u6d4b\u8bd5\u548c\u9a8c\u8bc1\u5927\u89c4\u6a21\u7a7a\u57df\u7ba1\u7406\u7cfb\u7edf\uff08\u5982UTM\u7f51\u7edc\uff09\u5728\u538b\u529b\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u548c\u5c40\u9650\u6027\u65b9\u9762\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u7684\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2509.08274", "pdf": "https://arxiv.org/pdf/2509.08274", "abs": "https://arxiv.org/abs/2509.08274", "authors": ["Abdul Razaque", "Abitkhanova Zhadyra Abitkhanovna"], "title": "Enhancing 6G Network Security and Incident Response through Integrated VNF and SDN Technologies", "categories": ["cs.NI"], "comment": "14 page, 10 Figures", "summary": "Low-speed internet can negatively affect incident response in a number of\nways, including decreased teamwork, delayed detection, inefficient action, and\nelevated risk. Delayed data acquisition and processing may result from\ninadequate internet connectivity, hindering security teams' ability to obtain\nthe necessary information for timely and effective responses. Each of these\nfactors may augment the organization's susceptibility to security incidents and\ntheir subsequent ramifications. This article establishes a virtual network\nfunction service delivery network (VNFSDN) through the integration of virtual\nnetwork function (VNF) and software-defined networking (SDN) technologies. The\nVNFSDN approach enhances network security effectiveness and efficiency while\nreducing the danger of breaches. This method assists security services in\nrapidly assessing vast quantities of data generated by 6G networks. VNFSDN\nadapts dynamically to changing safety requirements and connection conditions\nthrough the use of SDN and VNF. This flexibility enables enterprises to\nmitigate or halt the impact of cyberattacks by swiftly identifying and\naddressing security threats. The VNFSDN enhances network resilience, allowing\noperators to proactively mitigate possible security attacks and minimize\ndowntime. The incorporation of machine learning and artificial intelligence\ninto VNFSDN can significantly improve network security and threat detection\ncapabilities. The VNFSDN integrates VNF and SDN technologies to deliver\nsecurity services that analyze vast quantities of 6G data in real time. As\nsecurity requirements and network conditions evolve, it adapts dynamically to\nenhance network resilience and facilitate proactive threat detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVNFSDN\uff08\u865a\u62df\u7f51\u7edc\u529f\u80fd\u670d\u52a1\u4ea4\u4ed8\u7f51\u7edc\uff09\uff0c\u901a\u8fc7\u6574\u5408VNF\u548cSDN\u6280\u672f\u5e76\u7ed3\u5408\u673a\u5668\u5b66\u4e60/\u4eba\u5de5\u667a\u80fd\uff0c\u65e8\u5728\u89e3\u51b3\u4f4e\u901f\u4e92\u8054\u7f51\u4e0b\u4e8b\u4ef6\u54cd\u5e94\u6548\u7387\u4f4e\u4e0b\u4ee5\u53ca6G\u7f51\u7edc\u6d77\u91cf\u6570\u636e\u5904\u7406\u7684\u6311\u6218\uff0c\u4ece\u800c\u589e\u5f3a\u7f51\u7edc\u5b89\u5168\u3001\u97e7\u6027\u548c\u5a01\u80c1\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u4f4e\u901f\u4e92\u8054\u7f51\u4e25\u91cd\u5f71\u54cd\u4e8b\u4ef6\u54cd\u5e94\uff08\u5982\u68c0\u6d4b\u5ef6\u8fdf\u3001\u884c\u52a8\u4f4e\u6548\uff09\uff0c\u589e\u52a0\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u5b89\u5168\u56e2\u961f\u96be\u4ee5\u6709\u6548\u83b7\u53d6\u548c\u5904\u7406\u5fc5\u8981\u4fe1\u606f\uff0c\u7279\u522b\u662f\u672a\u67656G\u7f51\u7edc\u5c06\u4ea7\u751f\u6d77\u91cf\u6570\u636e\uff0c\u4f20\u7edf\u65b9\u5f0f\u96be\u4ee5\u5e94\u5bf9\uff0c\u5bfc\u81f4\u7ec4\u7ec7\u66f4\u6613\u53d7\u653b\u51fb\u3002", "method": "\u901a\u8fc7\u6574\u5408\u865a\u62df\u7f51\u7edc\u529f\u80fd\uff08VNF\uff09\u548c\u8f6f\u4ef6\u5b9a\u4e49\u7f51\u7edc\uff08SDN\uff09\u6280\u672f\uff0c\u6784\u5efa\u865a\u62df\u7f51\u7edc\u529f\u80fd\u670d\u52a1\u4ea4\u4ed8\u7f51\u7edc\uff08VNFSDN\uff09\u3002\u8be5\u65b9\u6cd5\u8fd8\u5efa\u8bae\u878d\u5165\u673a\u5668\u5b66\u4e60\u548c\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u7f51\u7edc\u5b89\u5168\u548c\u5a01\u80c1\u68c0\u6d4b\u80fd\u529b\u3002", "result": "VNFSDN\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u5b89\u5168\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u5b89\u5168\u6f0f\u6d1e\u98ce\u9669\u3002\u5b83\u80fd\u534f\u52a9\u5b89\u5168\u670d\u52a1\u5feb\u901f\u8bc4\u4f306G\u7f51\u7edc\u4ea7\u751f\u7684\u6d77\u91cf\u6570\u636e\uff0c\u5e76\u6839\u636e\u5b89\u5168\u9700\u6c42\u548c\u8fde\u63a5\u6761\u4ef6\u52a8\u6001\u8c03\u6574\u3002\u8fd9\u4f7f\u5f97\u4f01\u4e1a\u80fd\u591f\u8fc5\u901f\u8bc6\u522b\u548c\u5e94\u5bf9\u5b89\u5168\u5a01\u80c1\uff0c\u4ece\u800c\u51cf\u8f7b\u6216\u963b\u6b62\u7f51\u7edc\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u589e\u5f3a\u7f51\u7edc\u97e7\u6027\uff0c\u5e76\u5b9e\u73b0\u4e3b\u52a8\u5b89\u5168\u653b\u51fb\u7f13\u89e3\u548c\u505c\u673a\u65f6\u95f4\u6700\u5c0f\u5316\u3002", "conclusion": "VNFSDN\u901a\u8fc7\u96c6\u6210VNF\u548cSDN\u6280\u672f\uff08\u5e76\u53ef\u7ed3\u5408ML/AI\uff09\uff0c\u4e3a\u89e3\u51b3\u4f4e\u901f\u4e92\u8054\u7f51\u5bf9\u4e8b\u4ef6\u54cd\u5e94\u7684\u8d1f\u9762\u5f71\u54cd\u53ca6G\u7f51\u7edc\u6d77\u91cf\u6570\u636e\u7684\u5b89\u5168\u5206\u6790\u9700\u6c42\u63d0\u4f9b\u4e86\u4e00\u4e2a\u52a8\u6001\u3001\u9ad8\u6548\u4e14\u6709\u5f39\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u5b89\u5168\u548c\u5a01\u80c1\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2509.08400", "pdf": "https://arxiv.org/pdf/2509.08400", "abs": "https://arxiv.org/abs/2509.08400", "authors": ["Xingkun Yin", "Feiran You", "Hongyang Du", "Kaibin Huang"], "title": "Ubiquitous Intelligence Via Wireless Network-Driven LLMs Evolution", "categories": ["cs.NI"], "comment": null, "summary": "We introduce ubiquitous intelligence as a paradigm where Large Language\nModels (LLMs) evolve within wireless network-driven ecosystems. Unlike static\nmodel deployments, this approach enables scalable and continuous intelligence\nascension through coordination between networks and LLMs. Wireless networks\nsupport system-orchestrated lifelong learning, while LLMs drive the\nnext-generation network development that is more adaptive and responsive. This\nco-evolution highlights a shift toward self-improving systems, sustaining\ncapability growth across diverse and resource-constrained environments.", "AI": {"tldr": "\u5f15\u5165\u666e\u9002\u667a\u80fd\u8303\u5f0f\uff0c\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u65e0\u7ebf\u7f51\u7edc\u751f\u6001\u7cfb\u7edf\u7684\u534f\u540c\u6f14\u8fdb\uff0c\u4ee5\u8fbe\u5230\u53ef\u4f38\u7f29\u3001\u6301\u7eed\u3001\u81ea\u8fdb\u5316\u7684\u667a\u80fd\u63d0\u5347\u3002", "motivation": "\u65e8\u5728\u514b\u670dLLMs\u9759\u6001\u90e8\u7f72\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u7f51\u7edc\u4e0eLLMs\u7684\u534f\u540c\uff0c\u5b9e\u73b0\u667a\u80fd\u5728\u591a\u6837\u5316\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u6301\u7eed\u3001\u53ef\u4f38\u7f29\u589e\u957f\u3002", "method": "\u63d0\u51faLLMs\u4e0e\u65e0\u7ebf\u7f51\u7edc\u534f\u540c\u6f14\u8fdb\u7684\u8303\u5f0f\uff1a\u65e0\u7ebf\u7f51\u7edc\u652f\u6301\u7cfb\u7edf\u7f16\u6392\u7684\u7ec8\u8eab\u5b66\u4e60\uff0c\u800cLLMs\u5219\u63a8\u52a8\u4e0b\u4e00\u4ee3\u7f51\u7edc\u7684\u81ea\u9002\u5e94\u548c\u54cd\u5e94\u5f0f\u53d1\u5c55\u3002", "result": "\u5b9e\u73b0\u4e86\u81ea\u6539\u8fdb\u7684\u7cfb\u7edf\uff0c\u4f7f\u5176\u80fd\u529b\u5728\u591a\u6837\u5316\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u6301\u7eed\u589e\u957f\u3002", "conclusion": "\u666e\u9002\u667a\u80fd\u901a\u8fc7LLMs\u4e0e\u65e0\u7ebf\u7f51\u7edc\u7684\u5171\u751f\u53d1\u5c55\uff0c\u5f00\u521b\u4e86\u667a\u80fd\u7cfb\u7edf\u7684\u65b0\u7bc7\u7ae0\uff0c\u5177\u5907\u5728\u590d\u6742\u73af\u5883\u4e0b\u6301\u7eed\u81ea\u6211\u63d0\u5347\u7684\u80fd\u529b\u3002"}}
{"id": "2509.07998", "pdf": "https://arxiv.org/pdf/2509.07998", "abs": "https://arxiv.org/abs/2509.07998", "authors": ["Mesay Gemeda Yigezu", "Girma Yohannis Bade", "Atnafu Lambebo Tonja", "Olga Kolesnikova", "Grigori Sidorov", "Alexander Gelbukh"], "title": "Bilingual Word Level Language Identification for Omotic Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language identification is the task of determining the languages for a given\ntext. In many real world scenarios, text may contain more than one language,\nparticularly in multilingual communities. Bilingual Language Identification\n(BLID) is the task of identifying and distinguishing between two languages in a\ngiven text. This paper presents BLID for languages spoken in the southern part\nof Ethiopia, namely Wolaita and Gofa. The presence of words similarities and\ndifferences between the two languages makes the language identification task\nchallenging. To overcome this challenge, we employed various experiments on\nvarious approaches. Then, the combination of the BERT based pretrained language\nmodel and LSTM approach performed better, with an F1 score of 0.72 on the test\nset. As a result, the work will be effective in tackling unwanted social media\nissues and providing a foundation for further research in this area.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u57c3\u585e\u4fc4\u6bd4\u4e9a\u5357\u90e8Wolaita\u548cGofa\u4e24\u79cd\u8bed\u8a00\u7684\u53cc\u8bed\u8bc6\u522b\uff08BLID\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u65b9\u6cd5\u3002\u7ed3\u5408BERT\u9884\u8bad\u7ec3\u6a21\u578b\u548cLSTM\u7684\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0aF1\u5206\u6570\u4e3a0.72\u3002", "motivation": "\u5728\u591a\u8bed\u8a00\u793e\u533a\u4e2d\uff0c\u6587\u672c\u5e38\u5305\u542b\u591a\u79cd\u8bed\u8a00\u3002\u7531\u4e8eWolaita\u548cGofa\u8fd9\u4e24\u79cd\u8bed\u8a00\u4e4b\u95f4\u8bcd\u6c47\u5b58\u5728\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u6027\uff0c\u4f7f\u5f97\u5176\u53cc\u8bed\u8bc6\u522b\uff08BLID\uff09\u4efb\u52a1\u6781\u5177\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u6709\u6548\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e86\u591a\u79cd\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\u3002\u5176\u4e2d\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8eBERT\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548cLSTM\u7684\u65b9\u6cd5\u88ab\u91c7\u7528\u5e76\u8bc4\u4f30\u3002", "result": "\u7ed3\u5408BERT\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548cLSTM\u7684\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e860.72\u7684F1\u5206\u6570\u3002", "conclusion": "\u672c\u7814\u7a76\u5de5\u4f5c\u5c06\u6709\u6548\u89e3\u51b3\u793e\u4ea4\u5a92\u4f53\u4e2d\u4e0d\u5fc5\u8981\u7684\u8bed\u8a00\u95ee\u9898\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2509.07996", "pdf": "https://arxiv.org/pdf/2509.07996", "abs": "https://arxiv.org/abs/2509.07996", "authors": ["Lingdong Kong", "Wesley Yang", "Jianbiao Mei", "Youquan Liu", "Ao Liang", "Dekai Zhu", "Dongyue Lu", "Wei Yin", "Xiaotao Hu", "Mingkai Jia", "Junyuan Deng", "Kaiwen Zhang", "Yang Wu", "Tianyi Yan", "Shenyuan Gao", "Song Wang", "Linfeng Li", "Liang Pan", "Yong Liu", "Jianke Zhu", "Wei Tsang Ooi", "Steven C. H. Hoi", "Ziwei Liu"], "title": "3D and 4D World Modeling: A Survey", "categories": ["cs.CV", "cs.RO"], "comment": "Survey; 34 pages, 10 figures, 14 tables; GitHub Repo at\n  https://github.com/worldbench/survey", "summary": "World modeling has become a cornerstone in AI research, enabling agents to\nunderstand, represent, and predict the dynamic environments they inhabit. While\nprior work largely emphasizes generative methods for 2D image and video data,\nthey overlook the rapidly growing body of work that leverages native 3D and 4D\nrepresentations such as RGB-D imagery, occupancy grids, and LiDAR point clouds\nfor large-scale scene modeling. At the same time, the absence of a standardized\ndefinition and taxonomy for ``world models'' has led to fragmented and\nsometimes inconsistent claims in the literature. This survey addresses these\ngaps by presenting the first comprehensive review explicitly dedicated to 3D\nand 4D world modeling and generation. We establish precise definitions,\nintroduce a structured taxonomy spanning video-based (VideoGen),\noccupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and\nsystematically summarize datasets and evaluation metrics tailored to 3D/4D\nsettings. We further discuss practical applications, identify open challenges,\nand highlight promising research directions, aiming to provide a coherent and\nfoundational reference for advancing the field. A systematic summary of\nexisting literature is available at https://github.com/worldbench/survey", "AI": {"tldr": "\u672c\u7efc\u8ff0\u9996\u6b21\u5168\u9762\u5ba1\u67e5\u4e863D\u548c4D\u4e16\u754c\u5efa\u6a21\u4e0e\u751f\u6210\uff0c\u65e8\u5728\u5f25\u8865\u73b0\u6709\u7814\u7a76\u5bf92D\u65b9\u6cd5\u4fa7\u91cd\u800c\u5ffd\u89c63D/4D\u7684\u4e0d\u8db3\uff0c\u5e76\u89e3\u51b3\u4e16\u754c\u6a21\u578b\u5b9a\u4e49\u548c\u5206\u7c7b\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u4e3a\u9886\u57df\u63d0\u4f9b\u57fa\u7840\u6027\u53c2\u8003\u3002", "motivation": "\u73b0\u6709\u4e16\u754c\u5efa\u6a21\u7814\u7a76\u4e3b\u8981\u5173\u6ce82D\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\uff0c\u5ffd\u89c6\u4e86\u5feb\u901f\u589e\u957f\u76843D\u548c4D\u8868\u793a\u65b9\u6cd5\uff1b\u6b64\u5916\uff0c\u7f3a\u4e4f\u5bf9\u201c\u4e16\u754c\u6a21\u578b\u201d\u7684\u6807\u51c6\u5316\u5b9a\u4e49\u548c\u5206\u7c7b\uff0c\u5bfc\u81f4\u9886\u57df\u5185\u4e3b\u5f20\u788e\u7247\u5316\u4e14\u4e0d\u4e00\u81f4\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5bf93D\u548c4D\u4e16\u754c\u5efa\u6a21\u4e0e\u751f\u6210\u8fdb\u884c\u9996\u6b21\u5168\u9762\u7efc\u8ff0\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u5efa\u7acb\u7cbe\u786e\u5b9a\u4e49\uff0c\u5f15\u5165\u7ed3\u6784\u5316\u5206\u7c7b\u6cd5\uff08\u6db5\u76d6\u57fa\u4e8e\u89c6\u9891\u3001\u5360\u7528\u548cLiDAR\u7684\u65b9\u6cd5\uff09\uff0c\u7cfb\u7edf\u603b\u7ed3\u9002\u7528\u4e8e3D/4D\u8bbe\u7f6e\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u8ba8\u8bba\u5b9e\u9645\u5e94\u7528\u3001\u5f00\u653e\u6311\u6218\u53ca\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002", "result": "\u5efa\u7acb\u4e863D\u548c4D\u4e16\u754c\u6a21\u578b\u7684\u7cbe\u786e\u5b9a\u4e49\u548c\u7ed3\u6784\u5316\u5206\u7c7b\u6cd5\uff08VideoGen, OccGen, LiDARGen\uff09\uff1b\u7cfb\u7edf\u603b\u7ed3\u4e863D/4D\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff1b\u8ba8\u8bba\u4e86\u5b9e\u9645\u5e94\u7528\u3001\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u7efc\u8ff0\u65e8\u5728\u4e3a\u63a8\u52a83D\u548c4D\u4e16\u754c\u5efa\u6a21\u9886\u57df\u7684\u53d1\u5c55\u63d0\u4f9b\u4e00\u4e2a\u8fde\u8d2f\u4e14\u57fa\u7840\u6027\u7684\u53c2\u8003\u4f9d\u636e\u3002"}}
{"id": "2509.07997", "pdf": "https://arxiv.org/pdf/2509.07997", "abs": "https://arxiv.org/abs/2509.07997", "authors": ["Abigail Breitfeld", "Alberto Candela", "Juan Delfa", "Akseli Kangaslahti", "Itai Zilberstein", "Steve Chien", "David Wettergreen"], "title": "Learning-Based Planning for Improving Science Return of Earth Observation Satellites", "categories": ["cs.AI", "cs.RO"], "comment": "International Symposium on Artificial Intelligence, Robotics and\n  Automation in Space, November 2024", "summary": "Earth observing satellites are powerful tools for collecting scientific\ninformation about our planet, however they have limitations: they cannot easily\ndeviate from their orbital trajectories, their sensors have a limited field of\nview, and pointing and operating these sensors can take a large amount of the\nspacecraft's resources. It is important for these satellites to optimize the\ndata they collect and include only the most important or informative\nmeasurements. Dynamic targeting is an emerging concept in which satellite\nresources and data from a lookahead instrument are used to intelligently\nreconfigure and point a primary instrument. Simulation studies have shown that\ndynamic targeting increases the amount of scientific information gathered\nversus conventional sampling strategies. In this work, we present two different\nlearning-based approaches to dynamic targeting, using reinforcement and\nimitation learning, respectively. These learning methods build on a dynamic\nprogramming solution to plan a sequence of sampling locations. We evaluate our\napproaches against existing heuristic methods for dynamic targeting, showing\nthe benefits of using learning for this application. Imitation learning\nperforms on average 10.0\\% better than the best heuristic method, while\nreinforcement learning performs on average 13.7\\% better. We also show that\nboth learning methods can be trained effectively with relatively small amounts\nof data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u7684\u4e24\u79cd\u52a8\u6001\u76ee\u6807\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u5730\u7403\u89c2\u6d4b\u536b\u661f\u7684\u6570\u636e\u91c7\u96c6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8fd9\u4e9b\u65b9\u6cd5\u6bd4\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u6536\u96c6\u79d1\u5b66\u4fe1\u606f\uff0c\u4e14\u5bf9\u6570\u636e\u91cf\u8981\u6c42\u4e0d\u9ad8\u3002", "motivation": "\u5730\u7403\u89c2\u6d4b\u536b\u661f\u5728\u8f68\u9053\u3001\u89c6\u573a\u548c\u8d44\u6e90\u6d88\u8017\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4f18\u5316\u6570\u636e\u91c7\u96c6\u4ee5\u4ec5\u83b7\u53d6\u6700\u91cd\u8981\u7684\u4fe1\u606f\u3002\u52a8\u6001\u76ee\u6807\u5b9a\u4f4d\u662f\u65b0\u5174\u6982\u5ff5\uff0c\u65e8\u5728\u667a\u80fd\u5730\u91cd\u65b0\u914d\u7f6e\u548c\u6307\u5411\u4e3b\u4eea\u5668\u4ee5\u589e\u52a0\u79d1\u5b66\u4fe1\u606f\u91cf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff1a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u6a21\u4eff\u5b66\u4e60\uff08IL\uff09\u3002\u8fd9\u4e9b\u65b9\u6cd5\u57fa\u4e8e\u52a8\u6001\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u6765\u89c4\u5212\u91c7\u6837\u4f4d\u7f6e\u5e8f\u5217\u3002\u901a\u8fc7\u4e0e\u73b0\u6709\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u6765\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "\u6a21\u4eff\u5b66\u4e60\u5e73\u5747\u6bd4\u6700\u4f73\u542f\u53d1\u5f0f\u65b9\u6cd5\u6027\u80fd\u63d0\u9ad810.0%\uff0c\u5f3a\u5316\u5b66\u4e60\u5e73\u5747\u63d0\u9ad813.7%\u3002\u4e24\u79cd\u5b66\u4e60\u65b9\u6cd5\u90fd\u80fd\u7528\u76f8\u5bf9\u5c11\u91cf\u7684\u6570\u636e\u8fdb\u884c\u6709\u6548\u8bad\u7ec3\u3002", "conclusion": "\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff08\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\uff09\u5728\u5730\u7403\u89c2\u6d4b\u536b\u661f\u7684\u52a8\u6001\u76ee\u6807\u5b9a\u4f4d\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u79d1\u5b66\u4fe1\u606f\u6536\u96c6\u91cf\uff0c\u5e76\u4e14\u8bad\u7ec3\u6548\u7387\u9ad8\u3002"}}
{"id": "2509.07993", "pdf": "https://arxiv.org/pdf/2509.07993", "abs": "https://arxiv.org/abs/2509.07993", "authors": ["Federico Fontana", "Anxhelo Diko", "Romeo Lanzino", "Marco Raoul Marini", "Bachir Kaddar", "Gian Luca Foresti", "Luigi Cinque"], "title": "Revisiting Deepfake Detection: Chronological Continual Learning and the Limits of Generalization", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.GR"], "comment": null, "summary": "The rapid evolution of deepfake generation technologies poses critical\nchallenges for detection systems, as non-continual learning methods demand\nfrequent and expensive retraining. We reframe deepfake detection (DFD) as a\nContinual Learning (CL) problem, proposing an efficient framework that\nincrementally adapts to emerging visual manipulation techniques while retaining\nknowledge of past generators. Our framework, unlike prior approaches that rely\non unreal simulation sequences, simulates the real-world chronological\nevolution of deepfake technologies in extended periods across 7 years.\nSimultaneously, our framework builds upon lightweight visual backbones to allow\nfor the real-time performance of DFD systems. Additionally, we contribute two\nnovel metrics: Continual AUC (C-AUC) for historical performance and Forward\nTransfer AUC (FWT-AUC) for future generalization. Through extensive\nexperimentation (over 600 simulations), we empirically demonstrate that while\nefficient adaptation (+155 times faster than full retraining) and robust\nretention of historical knowledge is possible, the generalization of current\napproaches to future generators without additional training remains near-random\n(FWT-AUC $\\approx$ 0.5) due to the unique imprint characterizing each existing\ngenerator. Such observations are the foundation of our newly proposed\nNon-Universal Deepfake Distribution Hypothesis.\n  \\textbf{Code will be released upon acceptance.}", "AI": {"tldr": "\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u9762\u4e34\u6280\u672f\u5feb\u901f\u6f14\u8fdb\u6311\u6218\u3002\u672c\u6587\u5c06\u68c0\u6d4b\u91cd\u6784\u4e3a\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u4e2a\u9ad8\u6548\u6846\u67b6\uff0c\u80fd\u589e\u91cf\u9002\u5e94\u65b0\u4f5c\u5f0a\u6280\u672f\u5e76\u4fdd\u7559\u5386\u53f2\u77e5\u8bc6\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136\u80fd\u9ad8\u6548\u9002\u5e94\u5386\u53f2\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5bf9\u672a\u6765\u751f\u6210\u5668\u7684\u6cdb\u5316\u80fd\u529b\u63a5\u8fd1\u968f\u673a\uff0c\u5e76\u636e\u6b64\u63d0\u51fa\u201c\u975e\u666e\u9002\u6027\u6df1\u5ea6\u4f2a\u9020\u5206\u5e03\u5047\u8bbe\u201d\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u6280\u672f\u53d1\u5c55\u8fc5\u901f\uff0c\u5bfc\u81f4\u73b0\u6709\u975e\u6301\u7eed\u5b66\u4e60\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u9891\u7e41\u4e14\u6602\u8d35\u7684\u518d\u8bad\u7ec3\uff0c\u96be\u4ee5\u6709\u6548\u9002\u5e94\u4e0d\u65ad\u6d8c\u73b0\u7684\u65b0\u4f2a\u9020\u6280\u672f\uff0c\u5bf9\u68c0\u6d4b\u7cfb\u7edf\u6784\u6210\u4e25\u5cfb\u6311\u6218\u3002", "method": "\u5c06\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff08DFD\uff09\u91cd\u6784\u4e3a\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u9ad8\u6548\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u957f\u8fbe7\u5e74\u7684\u771f\u5b9e\u4e16\u754c\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u6f14\u53d8\uff0c\u800c\u975e\u4f9d\u8d56\u865a\u5047\u6a21\u62df\u5e8f\u5217\uff0c\u4ee5\u589e\u91cf\u9002\u5e94\u65b0\u5174\u89c6\u89c9\u64cd\u4f5c\u6280\u672f\u5e76\u4fdd\u7559\u5386\u53f2\u77e5\u8bc6\u3002\u540c\u65f6\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u4ee5\u652f\u6301DFD\u7cfb\u7edf\u7684\u5b9e\u65f6\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8d21\u732e\u4e86Continual AUC (C-AUC) \u548cForward Transfer AUC (FWT-AUC) \u4e24\u4e2a\u65b0\u6307\u6807\uff0c\u5206\u522b\u7528\u4e8e\u8bc4\u4f30\u5386\u53f2\u6027\u80fd\u548c\u672a\u6765\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u901a\u8fc7600\u591a\u6b21\u6a21\u62df\u5b9e\u9a8c\u8bc1\u660e\uff1a1) \u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u9002\u5e94\uff08\u6bd4\u5b8c\u5168\u518d\u8bad\u7ec3\u5feb155\u500d\uff09\u5e76\u7a33\u5065\u4fdd\u7559\u5386\u53f2\u77e5\u8bc6\u30022) \u7136\u800c\uff0c\u7531\u4e8e\u6bcf\u4e2a\u73b0\u6709\u751f\u6210\u5668\u72ec\u7279\u7684\u7279\u5f81\u5370\u8bb0\uff0c\u5f53\u524d\u65b9\u6cd5\u5728\u672a\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u672a\u6765\u751f\u6210\u5668\u7684\u6cdb\u5316\u80fd\u529b\u63a5\u8fd1\u968f\u673a\uff08FWT-AUC \u2248 0.5\uff09\u3002", "conclusion": "\u57fa\u4e8e\u5b9e\u9a8c\u89c2\u5bdf\uff0c\u63d0\u51fa\u4e86\u201c\u975e\u666e\u9002\u6027\u6df1\u5ea6\u4f2a\u9020\u5206\u5e03\u5047\u8bbe (Non-Universal Deepfake Distribution Hypothesis)\u201d\uff0c\u6307\u51fa\u5f53\u524d\u65b9\u6cd5\u5728\u672a\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u96be\u4ee5\u6709\u6548\u6cdb\u5316\u5230\u672a\u6765\u7684\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u5668\u3002"}}
{"id": "2509.08455", "pdf": "https://arxiv.org/pdf/2509.08455", "abs": "https://arxiv.org/abs/2509.08455", "authors": ["Wanja de Sombre", "Arash Asadi", "Debopam Bhattacherjee", "Deepak Vasisht", "Andrea Ortiz"], "title": "SKYLINK: Scalable and Resilient Link Management in LEO Satellite Network", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The rapid growth of space-based services has established LEO satellite\nnetworks as a promising option for global broadband connectivity.\nNext-generation LEO networks leverage inter-satellite links (ISLs) to provide\nfaster and more reliable communications compared to traditional bent-pipe\narchitectures, even in remote regions. However, the high mobility of\nsatellites, dynamic traffic patterns, and potential link failures pose\nsignificant challenges for efficient and resilient routing. To address these\nchallenges, we model the LEO satellite network as a time-varying graph\ncomprising a constellation of satellites and ground stations. Our objective is\nto minimize a weighted sum of average delay and packet drop rate. Each\nsatellite independently decides how to distribute its incoming traffic to\nneighboring nodes in real time. Given the infeasibility of finding optimal\nsolutions at scale, due to the exponential growth of routing options and\nuncertainties in link capacities, we propose SKYLINK, a novel fully distributed\nlearning strategy for link management in LEO satellite networks. SKYLINK\nenables each satellite to adapt to the time-varying network conditions,\nensuring real-time responsiveness, scalability to millions of users, and\nresilience to network failures, while maintaining low communication overhead\nand computational complexity. To support the evaluation of SKYLINK at global\nscale, we develop a new simulator for large-scale LEO satellite networks. For\n25.4 million users, SKYLINK reduces the weighted sum of average delay and drop\nrate by 29% compared to the bent-pipe approach, and by 92% compared to\nDijkstra. It lowers drop rates by 95% relative to k-shortest paths, 99%\nrelative to Dijkstra, and 74% compared to the bent-pipe baseline, while\nachieving up to 46% higher throughput. At the same time, SKYLINK maintains\nconstant computational complexity with respect to constellation size.", "AI": {"tldr": "\u63d0\u51faSKYLINK\uff0c\u4e00\u79cd\u9488\u5bf9LEO\u536b\u661f\u7f51\u7edc\u7684\u5168\u5206\u5e03\u5f0f\u5b66\u4e60\u8def\u7531\u7b56\u7565\uff0c\u6709\u6548\u5e94\u5bf9\u9ad8\u52a8\u6001\u6027\u6311\u6218\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u548c\u4e22\u5305\u7387\uff0c\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u5e76\u5177\u5907\u9ad8\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u65f6\u54cd\u5e94\u80fd\u529b\u3002", "motivation": "LEO\u536b\u661f\u7f51\u7edc\u6709\u671b\u63d0\u4f9b\u5168\u7403\u5bbd\u5e26\u8fde\u63a5\uff0c\u5c24\u5176\u901a\u8fc7\u661f\u95f4\u94fe\u8def\uff08ISL\uff09\u5b9e\u73b0\u66f4\u5feb\u66f4\u53ef\u9760\u7684\u901a\u4fe1\u3002\u7136\u800c\uff0c\u536b\u661f\u9ad8\u79fb\u52a8\u6027\u3001\u52a8\u6001\u6d41\u91cf\u6a21\u5f0f\u548c\u6f5c\u5728\u94fe\u8def\u6545\u969c\u7ed9\u9ad8\u6548\u5f39\u6027\u8def\u7531\u5e26\u6765\u4e86\u5de8\u5927\u6311\u6218\uff0c\u4e14\u5728\u5927\u89c4\u6a21\u7f51\u7edc\u4e2d\u96be\u4ee5\u627e\u5230\u6700\u4f18\u89e3\u3002\u672c\u7814\u7a76\u65e8\u5728\u6700\u5c0f\u5316\u5e73\u5747\u5ef6\u8fdf\u548c\u4e22\u5305\u7387\u7684\u52a0\u6743\u548c\u3002", "method": "\u5c06LEO\u536b\u661f\u7f51\u7edc\u5efa\u6a21\u4e3a\u5305\u542b\u536b\u661f\u548c\u5730\u9762\u7ad9\u7684\u65f6\u53d8\u56fe\u3002\u6bcf\u4e2a\u536b\u661f\u5b9e\u65f6\u72ec\u7acb\u51b3\u5b9a\u5982\u4f55\u5206\u914d\u4f20\u5165\u6d41\u91cf\u3002\u63d0\u51faSKYLINK\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u5168\u5206\u5e03\u5f0f\u5b66\u4e60\u7b56\u7565\u7528\u4e8e\u94fe\u8def\u7ba1\u7406\uff0c\u4f7f\u5176\u80fd\u9002\u5e94\u65f6\u53d8\u7f51\u7edc\u6761\u4ef6\uff0c\u786e\u4fdd\u5b9e\u65f6\u54cd\u5e94\u3001\u53ef\u6269\u5c55\u6027\u548c\u7f51\u7edc\u6545\u969c\u5f39\u6027\u3002\u4e3a\u652f\u6301\u5168\u7403\u89c4\u6a21\u7684\u8bc4\u4f30\uff0c\u5f00\u53d1\u4e86\u65b0\u7684\u5927\u89c4\u6a21LEO\u536b\u661f\u7f51\u7edc\u6a21\u62df\u5668\u3002", "result": "\u5bf9\u4e8e2540\u4e07\u7528\u6237\uff0cSKYLINK\u76f8\u6bd4bent-pipe\u5c06\u5e73\u5747\u5ef6\u8fdf\u548c\u4e22\u5305\u7387\u7684\u52a0\u6743\u548c\u964d\u4f4e29%\uff0c\u76f8\u6bd4Dijkstra\u964d\u4f4e92%\u3002\u5728\u4e22\u5305\u7387\u65b9\u9762\uff0c\u6bd4k-shortest paths\u4f4e95%\uff0c\u6bd4Dijkstra\u4f4e99%\uff0c\u6bd4bent-pipe\u4f4e74%\u3002\u540c\u65f6\uff0c\u541e\u5410\u91cf\u63d0\u9ad8\u9ad8\u8fbe46%\u3002\u6b64\u5916\uff0cSKYLINK\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u661f\u5ea7\u89c4\u6a21\u4fdd\u6301\u6052\u5b9a\u3002", "conclusion": "SKYLINK\u4f5c\u4e3a\u4e00\u79cd\u5168\u5206\u5e03\u5f0f\u5b66\u4e60\u7b56\u7565\uff0c\u80fd\u6709\u6548\u89e3\u51b3LEO\u536b\u661f\u7f51\u7edc\u7684\u590d\u6742\u8def\u7531\u6311\u6218\uff0c\u5728\u5ef6\u8fdf\u3001\u4e22\u5305\u7387\u548c\u541e\u5410\u91cf\u65b9\u9762\u5747\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u4e14\u5177\u5907\u5b9e\u65f6\u54cd\u5e94\u3001\u53ef\u6269\u5c55\u6027\u3001\u97e7\u6027\u3001\u4f4e\u901a\u4fe1\u5f00\u9500\u548c\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u4e3a\u4e0b\u4e00\u4ee3LEO\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08000", "pdf": "https://arxiv.org/pdf/2509.08000", "abs": "https://arxiv.org/abs/2509.08000", "authors": ["Debdeep Sanyal", "Manodeep Ray", "Murari Mandal"], "title": "AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs", "categories": ["cs.CL"], "comment": "19 pages", "summary": "The release of open-weight large language models (LLMs) creates a tension\nbetween advancing accessible research and preventing misuse, such as malicious\nfine-tuning to elicit harmful content. Current safety measures struggle to\npreserve the general capabilities of the LLM while resisting a determined\nadversary with full access to the model's weights and architecture, who can use\nfull-parameter fine-tuning to erase existing safeguards. To address this, we\nintroduce AntiDote, a bi-level optimization procedure for training LLMs to be\nresistant to such tampering. AntiDote involves an auxiliary adversary\nhypernetwork that learns to generate malicious Low-Rank Adaptation (LoRA)\nweights conditioned on the defender model's internal activations. The defender\nLLM is then trained with an objective to nullify the effect of these\nadversarial weight additions, forcing it to maintain its safety alignment. We\nvalidate this approach against a diverse suite of 52 red-teaming attacks,\nincluding jailbreak prompting, latent space manipulation, and direct\nweight-space attacks. AntiDote is upto 27.4\\% more robust against adversarial\nattacks compared to both tamper-resistance and unlearning baselines. Crucially,\nthis robustness is achieved with a minimal trade-off in utility, incurring a\nperformance degradation of upto less than 0.5\\% across capability benchmarks\nincluding MMLU, HellaSwag, and GSM8K. Our work offers a practical and compute\nefficient methodology for building open-weight models where safety is a more\nintegral and resilient property.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAntiDote\uff0c\u4e00\u79cd\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\uff0c\u65e8\u5728\u4f7f\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u62b5\u6297\u6076\u610f\u5fae\u8c03\u653b\u51fb\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\uff0c\u6709\u6548\u9632\u6b62\u5b89\u5168\u9632\u62a4\u88ab\u64e6\u9664\u3002", "motivation": "\u5f00\u6e90LLMs\u5728\u63a8\u52a8\u7814\u7a76\u7684\u540c\u65f6\uff0c\u4e5f\u5e26\u6765\u4e86\u88ab\u6076\u610f\u5fae\u8c03\u4ee5\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u98ce\u9669\u3002\u73b0\u6709\u5b89\u5168\u63aa\u65bd\u96be\u4ee5\u5728\u4fdd\u7559\u6a21\u578b\u901a\u7528\u80fd\u529b\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u62b5\u6297\u62e5\u6709\u5b8c\u5168\u6a21\u578b\u8bbf\u95ee\u6743\u9650\u7684\u5bf9\u624b\u901a\u8fc7\u53c2\u6570\u5fae\u8c03\u64e6\u9664\u5b89\u5168\u9632\u62a4\u3002", "method": "\u5f15\u5165AntiDote\uff0c\u4e00\u4e2a\u53cc\u5c42\u4f18\u5316\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u6d89\u53ca\u4e00\u4e2a\u8f85\u52a9\u5bf9\u6297\u6027\u8d85\u7f51\u7edc\uff0c\u5b83\u6839\u636e\u9632\u5fa1\u6a21\u578b\u7684\u5185\u90e8\u6fc0\u6d3b\u5b66\u4e60\u751f\u6210\u6076\u610f\u7684LoRA\u6743\u91cd\u3002\u9632\u5fa1\u578bLLM\u968f\u540e\u88ab\u8bad\u7ec3\u4ee5\u62b5\u6d88\u8fd9\u4e9b\u5bf9\u6297\u6027\u6743\u91cd\u6dfb\u52a0\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u5f3a\u5236\u6a21\u578b\u7ef4\u6301\u5176\u5b89\u5168\u5bf9\u9f50\u3002", "result": "AntiDote\u572852\u79cd\u7ea2\u961f\u653b\u51fb\uff08\u5305\u62ec\u8d8a\u72f1\u63d0\u793a\u3001\u6f5c\u5728\u7a7a\u95f4\u64cd\u7eb5\u548c\u76f4\u63a5\u6743\u91cd\u7a7a\u95f4\u653b\u51fb\uff09\u4e2d\u88ab\u9a8c\u8bc1\uff0c\u5176\u9c81\u68d2\u6027\u6bd4\u9632\u7be1\u6539\u548c\u9057\u5fd8\u57fa\u7ebf\u63d0\u9ad8\u4e86\u9ad8\u8fbe27.4%\u3002\u540c\u65f6\uff0c\u5b83\u5bf9\u6a21\u578b\u6548\u7528\u5f71\u54cd\u6781\u5c0f\uff0c\u5728MMLU\u3001HellaSwag\u548cGSM8K\u7b49\u80fd\u529b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u4e0b\u964d\u4e0d\u52300.5%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u5b89\u5168\u6027\u66f4\u5f3a\u3001\u66f4\u5177\u97e7\u6027\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u4f7f\u5176\u5b89\u5168\u6027\u6210\u4e3a\u4e00\u4e2a\u66f4\u5185\u5728\u548c\u66f4\u5177\u5f39\u6027\u7684\u7279\u6027\u3002"}}
{"id": "2509.08003", "pdf": "https://arxiv.org/pdf/2509.08003", "abs": "https://arxiv.org/abs/2509.08003", "authors": ["Shahid Shafi Dar", "Bharat Kaurav", "Arnav Jain", "Chandravardhan Singh Raghaw", "Mohammad Zia Ur Rehman", "Nagendra Kumar"], "title": "An Explainable Deep Neural Network with Frequency-Aware Channel and Spatial Refinement for Flood Prediction in Sustainable Cities", "categories": ["cs.CV"], "comment": null, "summary": "In an era of escalating climate change, urban flooding has emerged as a\ncritical challenge for sustainable cities, threatening lives, infrastructure,\nand ecosystems. Traditional flood detection methods are constrained by their\nreliance on unimodal data and static rule-based systems, which fail to capture\nthe dynamic, non-linear relationships inherent in flood events. Furthermore,\nexisting attention mechanisms and ensemble learning approaches exhibit\nlimitations in hierarchical refinement, cross-modal feature integration, and\nadaptability to noisy or unstructured environments, resulting in suboptimal\nflood classification performance. To address these challenges, we present\nXFloodNet, a novel framework that redefines urban flood classification through\nadvanced deep-learning techniques. XFloodNet integrates three novel components:\n(1) a Hierarchical Cross-Modal Gated Attention mechanism that dynamically\naligns visual and textual features, enabling precise multi-granularity\ninteractions and resolving contextual ambiguities; (2) a Heterogeneous\nConvolutional Adaptive Multi-Scale Attention module, which leverages\nfrequency-enhanced channel attention and frequency-modulated spatial attention\nto extract and prioritize discriminative flood-related features across spectral\nand spatial domains; and (3) a Cascading Convolutional Transformer Feature\nRefinement technique that harmonizes hierarchical features through adaptive\nscaling and cascading operations, ensuring robust and noise-resistant flood\ndetection. We evaluate our proposed method on three benchmark datasets, such as\nChennai Floods, Rhine18 Floods, and Harz17 Floods, XFloodNet achieves\nstate-of-the-art F1-scores of 93.33%, 82.24%, and 88.60%, respectively,\nsurpassing existing methods by significant margins.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86XFloodNet\uff0c\u4e00\u4e2a\u5229\u7528\u5148\u8fdb\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5e94\u5bf9\u57ce\u5e02\u6d2a\u6d9d\u5206\u7c7b\u6311\u6218\u7684\u6846\u67b6\u3002\u901a\u8fc7\u5f15\u5165\u5206\u5c42\u8de8\u6a21\u6001\u95e8\u63a7\u6ce8\u610f\u529b\u3001\u5f02\u6784\u5377\u79ef\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u53ca\u7ea7\u8054\u5377\u79efTransformer\u7279\u5f81\u7ec6\u5316\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u548c\u590d\u6742\u73af\u5883\u9002\u5e94\u6027\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6d2a\u6d9d\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5728\u6c14\u5019\u53d8\u5316\u52a0\u5267\u7684\u65f6\u4ee3\uff0c\u57ce\u5e02\u6d2a\u6d9d\u5bf9\u53ef\u6301\u7eed\u57ce\u5e02\u6784\u6210\u4e25\u5cfb\u6311\u6218\u3002\u4f20\u7edf\u6d2a\u6d9d\u68c0\u6d4b\u65b9\u6cd5\u53d7\u9650\u4e8e\u5355\u6a21\u6001\u6570\u636e\u548c\u9759\u6001\u89c4\u5219\uff0c\u65e0\u6cd5\u6355\u6349\u6d2a\u6d9d\u4e8b\u4ef6\u7684\u52a8\u6001\u975e\u7ebf\u6027\u5173\u7cfb\u3002\u73b0\u6709\u6ce8\u610f\u529b\u673a\u5236\u548c\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u5728\u5206\u5c42\u7ec6\u5316\u3001\u8de8\u6a21\u6001\u7279\u5f81\u6574\u5408\u53ca\u566a\u58f0\u73af\u5883\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5bfc\u81f4\u6d2a\u6d9d\u5206\u7c7b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86XFloodNet\u6846\u67b6\uff0c\u901a\u8fc7\u4ee5\u4e0b\u4e09\u4e2a\u65b0\u9896\u7ec4\u4ef6\u91cd\u65b0\u5b9a\u4e49\u57ce\u5e02\u6d2a\u6d9d\u5206\u7c7b\uff1a\n1.  **\u5206\u5c42\u8de8\u6a21\u6001\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\uff1a** \u52a8\u6001\u5bf9\u9f50\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u591a\u7c92\u5ea6\u4ea4\u4e92\u5e76\u89e3\u51b3\u4e0a\u4e0b\u6587\u6a21\u7cca\u6027\u3002\n2.  **\u5f02\u6784\u5377\u79ef\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u6a21\u5757\uff1a** \u5229\u7528\u9891\u7387\u589e\u5f3a\u901a\u9053\u6ce8\u610f\u529b\u548c\u9891\u7387\u8c03\u5236\u7a7a\u95f4\u6ce8\u610f\u529b\uff0c\u4ece\u9891\u8c31\u548c\u7a7a\u95f4\u57df\u63d0\u53d6\u5e76\u4f18\u5148\u5904\u7406\u5224\u522b\u6027\u6d2a\u6d9d\u76f8\u5173\u7279\u5f81\u3002\n3.  **\u7ea7\u8054\u5377\u79efTransformer\u7279\u5f81\u7ec6\u5316\u6280\u672f\uff1a** \u901a\u8fc7\u81ea\u9002\u5e94\u7f29\u653e\u548c\u7ea7\u8054\u64cd\u4f5c\u534f\u8c03\u5206\u5c42\u7279\u5f81\uff0c\u786e\u4fdd\u9c81\u68d2\u4e14\u6297\u566a\u58f0\u7684\u6d2a\u6d9d\u68c0\u6d4b\u3002", "result": "\u5728Chennai Floods\u3001Rhine18 Floods\u548cHarz17 Floods\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cXFloodNet\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684F1\u5206\u6570\uff0c\u5206\u522b\u4e3a93.33%\u300182.24%\u548c88.60%\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "XFloodNet\u901a\u8fc7\u5176\u521b\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u7ec4\u4ef6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u57ce\u5e02\u6d2a\u6d9d\u5206\u7c7b\u4e2d\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u7684\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u3001\u7279\u5f81\u63d0\u53d6\u548c\u566a\u58f0\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u6781\u5927\u5730\u63d0\u5347\u4e86\u6d2a\u6d9d\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4e3a\u57ce\u5e02\u6d2a\u6d9d\u5206\u7c7b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08088", "pdf": "https://arxiv.org/pdf/2509.08088", "abs": "https://arxiv.org/abs/2509.08088", "authors": ["Linyao Chen", "Zimian Peng", "Yingxuan Yang", "Yikun Wang", "Wenzheng Tom Tang", "Hiroki H. Kobayashi", "Weinan Zhang"], "title": "EnvX: Agentize Everything with Agentic AI", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "The widespread availability of open-source repositories has led to a vast\ncollection of reusable software components, yet their utilization remains\nmanual, error-prone, and disconnected. Developers must navigate documentation,\nunderstand APIs, and write integration code, creating significant barriers to\nefficient software reuse. To address this, we present EnvX, a framework that\nleverages Agentic AI to agentize GitHub repositories, transforming them into\nintelligent, autonomous agents capable of natural language interaction and\ninter-agent collaboration. Unlike existing approaches that treat repositories\nas static code resources, EnvX reimagines them as active agents through a\nthree-phase process: (1) TODO-guided environment initialization, which sets up\nthe necessary dependencies, data, and validation datasets; (2) human-aligned\nagentic automation, allowing repository-specific agents to autonomously perform\nreal-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple\nagents to collaborate. By combining large language model capabilities with\nstructured tool integration, EnvX automates not just code generation, but the\nentire process of understanding, initializing, and operationalizing repository\nfunctionality. We evaluate EnvX on the GitTaskBench benchmark, using 18\nrepositories across domains such as image processing, speech recognition,\ndocument analysis, and video manipulation. Our results show that EnvX achieves\na 74.07% execution completion rate and 51.85% task pass rate, outperforming\nexisting frameworks. Case studies further demonstrate EnvX's ability to enable\nmulti-repository collaboration via the A2A protocol. This work marks a shift\nfrom treating repositories as passive code resources to intelligent,\ninteractive agents, fostering greater accessibility and collaboration within\nthe open-source ecosystem.", "AI": {"tldr": "EnvX\u6846\u67b6\u5229\u7528Agentic AI\u5c06GitHub\u4ed3\u5e93\u8f6c\u5316\u4e3a\u667a\u80fd\u3001\u81ea\u6cbb\u7684\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u4ee3\u7406\u95f4\u534f\u4f5c\uff0c\u81ea\u52a8\u5316\u4e86\u5f00\u6e90\u7ec4\u4ef6\u7684\u7406\u89e3\u3001\u521d\u59cb\u5316\u548c\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8f6f\u4ef6\u590d\u7528\u6548\u7387\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u5f00\u6e90\u7ec4\u4ef6\u4f17\u591a\uff0c\u4f46\u5176\u5229\u7528\u8fc7\u7a0b\u4ecd\u662f\u624b\u52a8\u3001\u6613\u9519\u4e14\u5206\u6563\u7684\u3002\u5f00\u53d1\u8005\u9700\u624b\u52a8\u7406\u89e3\u6587\u6863\u3001API\u5e76\u7f16\u5199\u96c6\u6210\u4ee3\u7801\uff0c\u8fd9\u4e25\u91cd\u963b\u788d\u4e86\u9ad8\u6548\u7684\u8f6f\u4ef6\u590d\u7528\u3002", "method": "\u63d0\u51faEnvX\u6846\u67b6\uff0c\u901a\u8fc7Agentic AI\u5c06GitHub\u4ed3\u5e93\u201c\u4ee3\u7406\u5316\u201d\uff0c\u4f7f\u5176\u6210\u4e3a\u667a\u80fd\u3001\u81ea\u6cbb\u7684\u4ee3\u7406\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1) TODO\u5f15\u5bfc\u7684\u73af\u5883\u521d\u59cb\u5316\uff1b2) \u4eba\u7c7b\u5bf9\u9f50\u7684\u4ee3\u7406\u81ea\u52a8\u5316\uff0c\u6267\u884c\u5b9e\u9645\u4efb\u52a1\uff1b3) \u4ee3\u7406\u95f4(A2A)\u534f\u8bae\uff0c\u5b9e\u73b0\u591a\u4ee3\u7406\u534f\u4f5c\u3002\u5b83\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u4e0e\u7ed3\u6784\u5316\u5de5\u5177\u96c6\u6210\uff0c\u81ea\u52a8\u5316\u4e86\u4ed3\u5e93\u529f\u80fd\u7684\u7406\u89e3\u3001\u521d\u59cb\u5316\u548c\u64cd\u4f5c\u5168\u8fc7\u7a0b\u3002", "result": "EnvX\u5728GitTaskBench\u57fa\u51c6\u6d4b\u8bd5\uff08\u6db5\u76d618\u4e2a\u4ed3\u5e93\uff09\u4e2d\uff0c\u5b9e\u73b0\u4e8674.07%\u7684\u6267\u884c\u5b8c\u6210\u7387\u548c51.85%\u7684\u4efb\u52a1\u901a\u8fc7\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u3002\u6848\u4f8b\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86EnvX\u901a\u8fc7A2A\u534f\u8bae\u5b9e\u73b0\u591a\u4ed3\u5e93\u534f\u4f5c\u7684\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6807\u5fd7\u7740\u4ece\u5c06\u4ed3\u5e93\u89c6\u4e3a\u88ab\u52a8\u4ee3\u7801\u8d44\u6e90\u8f6c\u53d8\u4e3a\u667a\u80fd\u3001\u4ea4\u4e92\u5f0f\u4ee3\u7406\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4fc3\u8fdb\u4e86\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\u4e2d\u66f4\u9ad8\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u534f\u4f5c\u6027\u3002"}}
{"id": "2509.08058", "pdf": "https://arxiv.org/pdf/2509.08058", "abs": "https://arxiv.org/abs/2509.08058", "authors": ["Kai Ye", "Liangcai Su", "Chenxiong Qian"], "title": "How Far Are We from True Unlearnability?", "categories": ["cs.LG", "cs.AI"], "comment": "This paper has been accepted by ICLR 2025", "summary": "High-quality data plays an indispensable role in the era of large models, but\nthe use of unauthorized data for model training greatly damages the interests\nof data owners. To overcome this threat, several unlearnable methods have been\nproposed, which generate unlearnable examples (UEs) by compromising the\ntraining availability of data. Clearly, due to unknown training purposes and\nthe powerful representation learning capabilities of existing models, these\ndata are expected to be unlearnable for models across multiple tasks, i.e.,\nthey will not help improve the model's performance. However, unexpectedly, we\nfind that on the multi-task dataset Taskonomy, UEs still perform well in tasks\nsuch as semantic segmentation, failing to exhibit cross-task unlearnability.\nThis phenomenon leads us to question: How far are we from attaining truly\nunlearnable examples? We attempt to answer this question from the perspective\nof model optimization. To this end, we observe the difference in the\nconvergence process between clean and poisoned models using a simple model\narchitecture. Subsequently, from the loss landscape we find that only a part of\nthe critical parameter optimization paths show significant differences,\nimplying a close relationship between the loss landscape and unlearnability.\nConsequently, we employ the loss landscape to explain the underlying reasons\nfor UEs and propose Sharpness-Aware Learnability (SAL) to quantify the\nunlearnability of parameters based on this explanation. Furthermore, we propose\nan Unlearnable Distance (UD) to measure the unlearnability of data based on the\nSAL distribution of parameters in clean and poisoned models. Finally, we\nconduct benchmark tests on mainstream unlearnable methods using the proposed\nUD, aiming to promote community awareness of the capability boundaries of\nexisting unlearnable methods.", "AI": {"tldr": "\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u793a\u4f8b\uff08UEs\uff09\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u672a\u80fd\u5b9e\u73b0\u8de8\u4efb\u52a1\u4e0d\u53ef\u5b66\u4e60\u6027\u3002\u672c\u6587\u4ece\u6a21\u578b\u4f18\u5316\u89d2\u5ea6\u63a2\u7a76\u539f\u56e0\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u635f\u5931\u5e73\u9762\u7684Sharpness-Aware Learnability (SAL) \u548c Unlearnable Distance (UD) \u6765\u91cf\u5316\u53c2\u6570\u548c\u6570\u636e\u4e0d\u53ef\u5b66\u4e60\u6027\uff0c\u5e76\u4ee5\u6b64\u57fa\u51c6\u6d4b\u8bd5\u4e86\u4e3b\u6d41\u4e0d\u53ef\u5b66\u4e60\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5176\u80fd\u529b\u8fb9\u754c\u3002", "motivation": "\u5927\u6a21\u578b\u65f6\u4ee3\u9ad8\u8d28\u91cf\u6570\u636e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u672a\u7ecf\u6388\u6743\u7684\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u635f\u5bb3\u4e86\u6570\u636e\u6240\u6709\u8005\u5229\u76ca\u3002\u4e3a\u5e94\u5bf9\u6b64\u5a01\u80c1\uff0c\u5df2\u63d0\u51fa\u4e86\u591a\u79cd\u4e0d\u53ef\u5b66\u4e60\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u7814\u7a76\u53d1\u73b0\uff0c\u5728Taskonomy\u7b49\u591a\u4efb\u52a1\u6570\u636e\u96c6\u4e0a\uff0c\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u793a\u4f8b\uff08UEs\uff09\u5728\u8bed\u4e49\u5206\u5272\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u672a\u80fd\u5c55\u73b0\u8de8\u4efb\u52a1\u4e0d\u53ef\u5b66\u4e60\u6027\u3002\u8fd9\u5f15\u53d1\u4e86\u7591\u95ee\uff1a\u6211\u4eec\u8ddd\u79bb\u5b9e\u73b0\u771f\u6b63\u4e0d\u53ef\u5b66\u4e60\u7684\u793a\u4f8b\u8fd8\u6709\u591a\u8fdc\uff1f", "method": "\u672c\u6587\u4ece\u6a21\u578b\u4f18\u5316\u89d2\u5ea6\u5206\u6790\uff0c\u89c2\u5bdf\u5e72\u51c0\u6a21\u578b\u548c\u53d7\u6c61\u67d3\u6a21\u578b\u5728\u7b80\u5355\u67b6\u6784\u4e0b\u7684\u6536\u655b\u8fc7\u7a0b\u5dee\u5f02\u3002\u4ece\u635f\u5931\u5e73\u9762\u4e2d\u53d1\u73b0\uff0c\u53ea\u6709\u90e8\u5206\u5173\u952e\u53c2\u6570\u4f18\u5316\u8def\u5f84\u663e\u793a\u663e\u8457\u5dee\u5f02\uff0c\u6697\u793a\u635f\u5931\u5e73\u9762\u4e0e\u4e0d\u53ef\u5b66\u4e60\u6027\u7d27\u5bc6\u76f8\u5173\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86Sharpness-Aware Learnability (SAL) \u6765\u91cf\u5316\u53c2\u6570\u7684\u4e0d\u53ef\u5b66\u4e60\u6027\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86Unlearnable Distance (UD) \u6765\u8861\u91cf\u6570\u636e\u7684\u4e0d\u53ef\u5b66\u4e60\u6027\uff08\u57fa\u4e8eSAL\u5206\u5e03\uff09\u3002\u6700\u540e\uff0c\u5229\u7528UD\u5bf9\u4e3b\u6d41\u4e0d\u53ef\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728Taskonomy\u7b49\u591a\u4efb\u52a1\u6570\u636e\u96c6\u4e0a\uff0c\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u793a\u4f8b\uff08UEs\uff09\u672a\u80fd\u5c55\u73b0\u51fa\u9884\u671f\u7684\u8de8\u4efb\u52a1\u4e0d\u53ef\u5b66\u4e60\u6027\uff0c\u5728\u90e8\u5206\u4efb\u52a1\u4e2d\u4ecd\u8868\u73b0\u826f\u597d\u3002\u901a\u8fc7\u6a21\u578b\u4f18\u5316\u5206\u6790\uff0c\u8bc1\u5b9e\u4e86\u635f\u5931\u5e73\u9762\u4e0e\u4e0d\u53ef\u5b66\u4e60\u6027\u4e4b\u95f4\u7684\u7d27\u5bc6\u5173\u7cfb\uff0c\u4e14\u5e72\u51c0\u6a21\u578b\u4e0e\u53d7\u6c61\u67d3\u6a21\u578b\u5728\u5173\u952e\u53c2\u6570\u4f18\u5316\u8def\u5f84\u4e0a\u4ec5\u90e8\u5206\u663e\u793a\u663e\u8457\u5dee\u5f02\u3002\u672c\u6587\u6210\u529f\u63d0\u51fa\u4e86SAL\u7528\u4e8e\u91cf\u5316\u53c2\u6570\u4e0d\u53ef\u5b66\u4e60\u6027\uff0c\u5e76\u63d0\u51fa\u4e86UD\u6765\u8861\u91cf\u6570\u636e\u4e0d\u53ef\u5b66\u4e60\u6027\u3002", "conclusion": "\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u65b9\u6cd5\u5728\u5b9e\u73b0\u771f\u6b63\u8de8\u4efb\u52a1\u4e0d\u53ef\u5b66\u4e60\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u6587\u901a\u8fc7\u635f\u5931\u5e73\u9762\u5206\u6790\uff0c\u63d0\u51faSAL\u548cUD\u91cf\u5316\u4e0d\u53ef\u5b66\u4e60\u6027\uff0c\u65e8\u5728\u52a0\u6df1\u793e\u533a\u5bf9\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u65b9\u6cd5\u80fd\u529b\u8fb9\u754c\u7684\u7406\u89e3\u548c\u8ba4\u8bc6\uff0c\u4ee5\u671f\u63a8\u52a8\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.08488", "pdf": "https://arxiv.org/pdf/2509.08488", "abs": "https://arxiv.org/abs/2509.08488", "authors": ["Hasan Albinsaid", "Bodhibrata Mukhopadhyay", "Mohamed-Slim Alouini"], "title": "Design and Development of a Scalable and Energy-Efficient Localization Framework Leveraging LoRa Ranging-Capable Transceivers", "categories": ["cs.NI"], "comment": null, "summary": "Precise and energy-efficient localization is a critical requirement in many\nInternet of Things (IoT) applications, particularly in large-scale deployments\nsuch as asset tagging, agriculture, and smart cities, where long battery life\nand cost-effectiveness are crucial. The Semtech SX1280 LoRa transceiver\npresents a promising solution for IoT localization. It combines low cost, low\npower, and precise ranging capability over distances of up to 1 km. However,\nthe ranging process requires two devices to be simultaneously active, one\ninitiating the ranging request and the other responding to it, which can lead\nto significant energy expenditure if not properly managed. Despite the\ntransceiver's excellent performance, no existing system-level framework\neffectively manages sleep-wake coordination and role assignment needed for\nenergy-efficient operation. This paper presents a coordination framework that\nsignificantly reduces power consumption while maintaining the inherent precise\nranging capability of the chip. The framework schedules short, synchronized\nwake-up windows between the initiator and the responder, allowing devices to\nremain in deep sleep for most of their duty cycle. This scheduling strategy\nminimizes reliance on precise continuous timing and mitigates drift in low-cost\noscillators. To validate the framework, we designed and developed custom nodes\nthat are compliant with the framework's protocol. Experimental results show\nthat the proposed approach allows a node to stay in ultra-low power mode and\nwake periodically to check for instructions. The node can remain in standby\nmode for up to nine months on a single coin cell battery and can perform\nranging operations on demand in near real-time, all while maintaining a\nlocalization accuracy within five meters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u534f\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u6b65\u77ed\u5524\u9192\u7a97\u53e3\u663e\u8457\u964d\u4f4eLoRa SX1280\u5728IoT\u5b9a\u4f4d\u4e2d\u7684\u529f\u8017\uff0c\u5b9e\u73b0\u4e86\u957f\u7535\u6c60\u5bff\u547d\uff089\u4e2a\u6708\uff09\u548c\u9ad8\u7cbe\u5ea6\uff085\u7c73\u5185\uff09\u7684\u6309\u9700\u6d4b\u8ddd\u3002", "motivation": "\u5927\u89c4\u6a21IoT\u5e94\u7528\u5bf9\u7cbe\u786e\u548c\u8282\u80fd\u5b9a\u4f4d\u6709\u5173\u952e\u9700\u6c42\uff0cSemtech SX1280 LoRa\u6536\u53d1\u5668\u867d\u5177\u4f4e\u6210\u672c\u3001\u4f4e\u529f\u8017\u548c\u7cbe\u51c6\u6d4b\u8ddd\u80fd\u529b\uff0c\u4f46\u5176\u6d4b\u8ddd\u8fc7\u7a0b\u9700\u4e24\u8bbe\u5907\u540c\u65f6\u6d3b\u8dc3\uff0c\u5bfc\u81f4\u9ad8\u80fd\u8017\u3002\u73b0\u6709\u7cfb\u7edf\u7ea7\u6846\u67b6\u672a\u80fd\u6709\u6548\u7ba1\u7406\u4f11\u7720-\u5524\u9192\u534f\u8c03\u53ca\u89d2\u8272\u5206\u914d\u4ee5\u5b9e\u73b0\u8282\u80fd\u8fd0\u884c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u534f\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u5ea6\u53d1\u8d77\u8005\u548c\u54cd\u5e94\u8005\u4e4b\u95f4\u540c\u6b65\u7684\u77ed\u5524\u9192\u7a97\u53e3\uff0c\u4f7f\u8bbe\u5907\u5927\u90e8\u5206\u65f6\u95f4\u5904\u4e8e\u6df1\u5ea6\u7761\u7720\u6a21\u5f0f\u3002\u8be5\u7b56\u7565\u65e8\u5728\u6700\u5c0f\u5316\u5bf9\u8fde\u7eed\u7cbe\u786e\u8ba1\u65f6\u7684\u4f9d\u8d56\uff0c\u5e76\u51cf\u8f7b\u4f4e\u6210\u672c\u632f\u8361\u5668\u7684\u6f02\u79fb\u3002\u4e3a\u9a8c\u8bc1\u6846\u67b6\uff0c\u8bbe\u8ba1\u5e76\u5f00\u53d1\u4e86\u7b26\u5408\u534f\u8bae\u7684\u5b9a\u5236\u8282\u70b9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f7f\u8282\u70b9\u80fd\u4fdd\u6301\u8d85\u4f4e\u529f\u8017\u6a21\u5f0f\u5e76\u5468\u671f\u6027\u5524\u9192\u68c0\u67e5\u6307\u4ee4\u3002\u8282\u70b9\u5728\u5355\u9897\u7ebd\u6263\u7535\u6c60\u4e0b\u53ef\u5f85\u673a\u957f\u8fbe\u4e5d\u4e2a\u6708\uff0c\u5e76\u80fd\u6309\u9700\u8fd1\u5b9e\u65f6\u6267\u884c\u6d4b\u8ddd\u64cd\u4f5c\uff0c\u540c\u65f6\u4fdd\u6301\u4e94\u7c73\u4ee5\u5185\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u534f\u8c03\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LoRa SX1280\u5728\u8282\u80fd\u4e0e\u7cbe\u786e\u6d4b\u8ddd\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u4e3aIoT\u5e94\u7528\u4e2d\u7684\u80fd\u6e90\u9ad8\u6548\u3001\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u7cfb\u7edf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08022", "pdf": "https://arxiv.org/pdf/2509.08022", "abs": "https://arxiv.org/abs/2509.08022", "authors": ["Yao Liang", "Dongcheng Zhao", "Feifei Zhao", "Guobin Shen", "Yuwei Wang", "Dongqi Liang", "Yi Zeng"], "title": "MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The alignment of large language models (LLMs) with human values is critical\nfor their safe and effective deployment across diverse user populations.\nHowever, existing benchmarks often neglect cultural and demographic diversity,\nleading to limited understanding of how value alignment generalizes globally.\nIn this work, we introduce MVPBench, a novel benchmark that systematically\nevaluates LLMs' alignment with multi-dimensional human value preferences across\n75 countries. MVPBench contains 24,020 high-quality instances annotated with\nfine-grained value labels, personalized questions, and rich demographic\nmetadata, making it the most comprehensive resource of its kind to date. Using\nMVPBench, we conduct an in-depth analysis of several state-of-the-art LLMs,\nrevealing substantial disparities in alignment performance across geographic\nand demographic lines. We further demonstrate that lightweight fine-tuning\nmethods, such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization\n(DPO), can significantly enhance value alignment in both in-domain and\nout-of-domain settings. Our findings underscore the necessity for\npopulation-aware alignment evaluation and provide actionable insights for\nbuilding culturally adaptive and value-sensitive LLMs. MVPBench serves as a\npractical foundation for future research on global alignment, personalized\nvalue modeling, and equitable AI development.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165MVPBench\uff0c\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u572875\u4e2a\u56fd\u5bb6\u591a\u7ef4\u5ea6\u4eba\u7c7b\u4ef7\u503c\u504f\u597d\u5bf9\u9f50\u7684\u65b0\u57fa\u51c6\u3002\u53d1\u73b0\u73b0\u6709LLM\u5b58\u5728\u663e\u8457\u7684\u5730\u57df\u548c\u4eba\u53e3\u5bf9\u9f50\u5dee\u5f02\uff0c\u5e76\u8bc1\u660e\u8f7b\u91cf\u7ea7\u5fae\u8c03\u65b9\u6cd5\u53ef\u6709\u6548\u63d0\u5347\u5bf9\u9f50\u6027\u80fd\u3002", "motivation": "LLM\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u5bf9\u5176\u5b89\u5168\u6709\u6548\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5bf9\u9f50\u57fa\u51c6\u5ffd\u89c6\u6587\u5316\u548c\u4eba\u53e3\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u5bf9LLM\u4ef7\u503c\u5bf9\u9f50\u7684\u5168\u7403\u6cdb\u5316\u80fd\u529b\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u4e86MVPBench\uff0c\u4e00\u4e2a\u5305\u542b24,020\u4e2a\u9ad8\u8d28\u91cf\u5b9e\u4f8b\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LLM\u572875\u4e2a\u56fd\u5bb6\u7684\u591a\u7ef4\u5ea6\u4eba\u7c7b\u4ef7\u503c\u504f\u597d\u5bf9\u9f50\u3002\u8be5\u57fa\u51c6\u5305\u542b\u7ec6\u7c92\u5ea6\u4ef7\u503c\u6807\u7b7e\u3001\u4e2a\u6027\u5316\u95ee\u9898\u548c\u4e30\u5bcc\u7684\u4eba\u53e3\u7edf\u8ba1\u5143\u6570\u636e\u3002\u4f7f\u7528MVPBench\u5206\u6790\u4e86SOTA LLM\uff0c\u5e76\u63a2\u7d22\u4e86LoRA\u548cDPO\u7b49\u8f7b\u91cf\u7ea7\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86SOTA LLM\u5728\u5730\u57df\u548c\u4eba\u53e3\u5c42\u9762\u5b58\u5728\u663e\u8457\u7684\u4ef7\u503c\u5bf9\u9f50\u6027\u80fd\u5dee\u5f02\u3002\u540c\u65f6\uff0c\u8f7b\u91cf\u7ea7\u5fae\u8c03\u65b9\u6cd5\uff08\u5982LoRA\u548cDPO\uff09\u80fd\u591f\u663e\u8457\u589e\u5f3aLLM\u5728\u57df\u5185\u548c\u57df\u5916\u8bbe\u7f6e\u4e2d\u7684\u4ef7\u503c\u5bf9\u9f50\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8fdb\u884c\u4eba\u53e3\u611f\u77e5\u5bf9\u9f50\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u4e3a\u6784\u5efa\u6587\u5316\u9002\u5e94\u6027\u5f3a\u3001\u4ef7\u503c\u654f\u611f\u7684LLM\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002MVPBench\u4e3a\u5168\u7403\u5bf9\u9f50\u3001\u4e2a\u6027\u5316\u4ef7\u503c\u5efa\u6a21\u548c\u516c\u5e73AI\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.08016", "pdf": "https://arxiv.org/pdf/2509.08016", "abs": "https://arxiv.org/abs/2509.08016", "authors": ["Hyungjin Chung", "Hyelin Nam", "Jiyeon Kim", "Hyojun Go", "Byeongjun Park", "Junho Kim", "Joonseok Lee", "Seongsu Ha", "Byung-Hoon Kim"], "title": "Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs", "categories": ["cs.CV", "cs.LG"], "comment": "https://github.com/hyungjin-chung/VPS", "summary": "Video Large Language Models (VideoLLMs) face a critical bottleneck:\nincreasing the number of input frames to capture fine-grained temporal detail\nleads to prohibitive computational costs and performance degradation from long\ncontext lengths. We introduce Video Parallel Scaling (VPS), an inference-time\nmethod that expands a model's perceptual bandwidth without increasing its\ncontext window. VPS operates by running multiple parallel inference streams,\neach processing a unique, disjoint subset of the video's frames. By aggregating\nthe output probabilities from these complementary streams, VPS integrates a\nricher set of visual information than is possible with a single pass. We\ntheoretically show that this approach effectively contracts the Chinchilla\nscaling law by leveraging uncorrelated visual evidence, thereby improving\nperformance without additional training. Extensive experiments across various\nmodel architectures and scales (2B-32B) on benchmarks such as Video-MME and\nEventHallusion demonstrate that VPS consistently and significantly improves\nperformance. It scales more favorably than other parallel alternatives (e.g.\nSelf-consistency) and is complementary to other decoding strategies, offering a\nmemory-efficient and robust framework for enhancing the temporal reasoning\ncapabilities of VideoLLMs.", "AI": {"tldr": "\u9488\u5bf9VideoLLM\u5904\u7406\u957f\u89c6\u9891\u65f6\u7684\u8ba1\u7b97\u4e0e\u6027\u80fd\u74f6\u9888\uff0c\u672c\u6587\u63d0\u51fa\u63a8\u7406\u65f6\u65b9\u6cd5Video Parallel Scaling (VPS)\u3002VPS\u901a\u8fc7\u5e76\u884c\u5904\u7406\u89c6\u9891\u5b50\u96c6\u5e76\u805a\u5408\u7ed3\u679c\uff0c\u5728\u4e0d\u589e\u52a0\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u611f\u77e5\u5e26\u5bbd\u548c\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08VideoLLMs\uff09\u5728\u6355\u83b7\u7ec6\u7c92\u5ea6\u65f6\u95f4\u7ec6\u8282\u65f6\uff0c\u56e0\u8f93\u5165\u5e27\u6570\u589e\u52a0\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u548c\u957f\u4e0a\u4e0b\u6587\u957f\u5ea6\u5f15\u8d77\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u9762\u4e34\u5173\u952e\u74f6\u9888\u3002", "method": "\u672c\u6587\u5f15\u5165\u89c6\u9891\u5e76\u884c\u6269\u5c55\uff08VPS\uff09\u65b9\u6cd5\u3002\u8fd9\u662f\u4e00\u79cd\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd0\u884c\u591a\u4e2a\u5e76\u884c\u63a8\u7406\u6d41\u6765\u6269\u5c55\u6a21\u578b\u611f\u77e5\u5e26\u5bbd\uff0c\u6bcf\u4e2a\u6d41\u5904\u7406\u89c6\u9891\u5e27\u7684\u72ec\u7279\u4e14\u4e0d\u76f8\u4ea4\u7684\u5b50\u96c6\u3002VPS\u901a\u8fc7\u805a\u5408\u8fd9\u4e9b\u4e92\u8865\u6d41\u7684\u8f93\u51fa\u6982\u7387\u6765\u6574\u5408\u66f4\u4e30\u5bcc\u7684\u89c6\u89c9\u4fe1\u606f\u3002\u7406\u8bba\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e0d\u76f8\u5173\u7684\u89c6\u89c9\u8bc1\u636e\u6709\u6548\u5730\u6536\u7f29\u4e86Chinchilla\u7f29\u653e\u5b9a\u5f8b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u9ad8\u6027\u80fd\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cVPS\u5728\u5404\u79cd\u6a21\u578b\u67b6\u6784\u548c\u89c4\u6a21\uff082B-32B\uff09\u4e0a\uff0c\u4e8eVideo-MME\u548cEventHallusion\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002\u5b83\u6bd4\u5176\u4ed6\u5e76\u884c\u66ff\u4ee3\u65b9\u6848\uff08\u5982Self-consistency\uff09\u66f4\u5177\u6269\u5c55\u6027\uff0c\u5e76\u4e0e\u73b0\u6709\u89e3\u7801\u7b56\u7565\u4e92\u8865\u3002", "conclusion": "VPS\u4e3a\u589e\u5f3aVideoLLMs\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5185\u5b58\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u9762\u4e34\u7684\u8ba1\u7b97\u548c\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2509.08151", "pdf": "https://arxiv.org/pdf/2509.08151", "abs": "https://arxiv.org/abs/2509.08151", "authors": ["Botao Zhu", "Jeslyn Wang", "Dusit Niyato", "Xianbin Wang"], "title": "Trust Semantics Distillation for Collaborator Selection via Memory-Augmented Agentic AI", "categories": ["cs.AI"], "comment": null, "summary": "Accurate trustworthiness evaluation of potential collaborating devices is\nessential for the effective execution of complex computing tasks. This\nevaluation process involves collecting diverse trust-related data from\npotential collaborators, including historical performance and available\nresources, for collaborator selection. However, when each task owner\nindependently assesses all collaborators' trustworthiness, frequent data\nexchange, complex reasoning, and dynamic situation changes can result in\nsignificant overhead and deteriorated trust evaluation. To overcome these\nchallenges, we propose a task-specific trust semantics distillation (2TSD)\nmodel based on a large AI model (LAM)-driven teacher-student agent\narchitecture. The teacher agent is deployed on a server with powerful\ncomputational capabilities and an augmented memory module dedicated to\nmultidimensional trust-related data collection, task-specific trust semantics\nextraction, and task-collaborator matching analysis. Upon receiving\ntask-specific requests from device-side student agents, the teacher agent\ntransfers the trust semantics of potential collaborators to the student agents,\nenabling rapid and accurate collaborator selection. Experimental results\ndemonstrate that the proposed 2TSD model can reduce collaborator evaluation\ntime, decrease device resource consumption, and improve the accuracy of\ncollaborator selection.", "AI": {"tldr": "\u9488\u5bf9\u5206\u5e03\u5f0f\u8ba1\u7b97\u4efb\u52a1\u4e2d\u8bbe\u5907\u4fe1\u4efb\u8bc4\u4f30\u6548\u7387\u4f4e\u548c\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u5927\u578bAI\u6a21\u578b\u9a71\u52a8\u76842TSD\u5e08\u751f\u4ee3\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u4fe1\u4efb\u8bed\u4e49\u84b8\u998f\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u7684\u534f\u4f5c\u8bbe\u5907\u9009\u62e9\u3002", "motivation": "\u5f53\u6bcf\u4e2a\u4efb\u52a1\u6240\u6709\u8005\u72ec\u7acb\u8bc4\u4f30\u6240\u6709\u534f\u4f5c\u8bbe\u5907\u7684\u4fe1\u4efb\u5ea6\u65f6\uff0c\u9891\u7e41\u7684\u6570\u636e\u4ea4\u6362\u3001\u590d\u6742\u7684\u63a8\u7406\u548c\u52a8\u6001\u60c5\u5883\u53d8\u5316\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u5f00\u9500\u548c\u4fe1\u4efb\u8bc4\u4f30\u7684\u9000\u5316\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u7279\u5b9a\u4fe1\u4efb\u8bed\u4e49\u84b8\u998f\uff082TSD\uff09\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u57fa\u4e8e\u5927\u578bAI\u6a21\u578b\uff08LAM\uff09\u9a71\u52a8\u7684\u5e08\u751f\u4ee3\u7406\u67b6\u6784\u3002\u6559\u5e08\u4ee3\u7406\u90e8\u7f72\u5728\u670d\u52a1\u5668\u7aef\uff0c\u8d1f\u8d23\u6536\u96c6\u591a\u7ef4\u4fe1\u4efb\u6570\u636e\u3001\u63d0\u53d6\u4efb\u52a1\u7279\u5b9a\u4fe1\u4efb\u8bed\u4e49\u5e76\u8fdb\u884c\u5339\u914d\u5206\u6790\uff1b\u5b66\u751f\u4ee3\u7406\u5728\u8bbe\u5907\u7aef\u63a5\u6536\u6559\u5e08\u4ee3\u7406\u4f20\u9012\u7684\u4fe1\u4efb\u8bed\u4e49\uff0c\u4ece\u800c\u5feb\u901f\u51c6\u786e\u5730\u9009\u62e9\u534f\u4f5c\u8bbe\u5907\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u76842TSD\u6a21\u578b\u80fd\u591f\u51cf\u5c11\u534f\u4f5c\u8bbe\u5907\u8bc4\u4f30\u65f6\u95f4\uff0c\u964d\u4f4e\u8bbe\u5907\u8d44\u6e90\u6d88\u8017\uff0c\u5e76\u63d0\u9ad8\u534f\u4f5c\u8bbe\u5907\u9009\u62e9\u7684\u51c6\u786e\u6027\u3002", "conclusion": "2TSD\u6a21\u578b\u901a\u8fc7\u5176\u521b\u65b0\u7684\u5e08\u751f\u4ee3\u7406\u67b6\u6784\u548c\u4fe1\u4efb\u8bed\u4e49\u84b8\u998f\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u4efb\u52a1\u534f\u4f5c\u4e2d\u8bbe\u5907\u4fe1\u4efb\u8bc4\u4f30\u7684\u6548\u7387\u3001\u5f00\u9500\u548c\u51c6\u786e\u6027\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6267\u884c\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.08086", "pdf": "https://arxiv.org/pdf/2509.08086", "abs": "https://arxiv.org/abs/2509.08086", "authors": ["Michael Kishelev", "Pranab Bhadani", "Wanying Ding", "Vinay Chaudhri"], "title": "JEL: A Novel Model Linking Knowledge Graph entities to News Mentions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We present JEL, a novel computationally efficient end-to-end multi-neural\nnetwork based entity linking model, which beats current state-of-art model.\nKnowledge Graphs have emerged as a compelling abstraction for capturing\ncritical relationships among the entities of interest and integrating data from\nmultiple heterogeneous sources. A core problem in leveraging a knowledge graph\nis linking its entities to the mentions (e.g., people, company names) that are\nencountered in textual sources (e.g., news, blogs., etc) correctly, since there\nare thousands of entities to consider for each mention. This task of linking\nmentions and entities is referred as Entity Linking (EL). It is a fundamental\ntask in natural language processing and is beneficial in various uses cases,\nsuch as building a New Analytics platform. News Analytics, in JPMorgan, is an\nessential task that benefits multiple groups across the firm. According to a\nsurvey conducted by the Innovation Digital team 1 , around 25 teams across the\nfirm are actively looking for news analytics solutions, and more than \\$2\nmillion is being spent annually on external vendor costs. Entity linking is\ncritical for bridging unstructured news text with knowledge graphs, enabling\nusers access to vast amounts of curated data in a knowledge graph and\ndramatically facilitating their daily work.", "AI": {"tldr": "JEL\u662f\u4e00\u79cd\u65b0\u578b\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u591a\u795e\u7ecf\u7f51\u7edc\u5b9e\u4f53\u94fe\u63a5\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u4f73\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u5c06\u6587\u672c\u63d0\u53ca\u94fe\u63a5\u5230\u77e5\u8bc6\u56fe\u8c31\u7684\u5173\u952e\u95ee\u9898\uff0c\u5c24\u5176\u5728\u65b0\u95fb\u5206\u6790\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u7684\u6838\u5fc3\u95ee\u9898\u662f\u5c06\u6587\u672c\u4e2d\u7684\u63d0\u53ca\u6b63\u786e\u94fe\u63a5\u5230\u5b9e\u4f53\uff0c\u8fd9\u9879\u4efb\u52a1\uff08\u5b9e\u4f53\u94fe\u63a5\uff0cEL\uff09\u5bf9\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u65b0\u95fb\u5206\u6790\u5e73\u53f0\u7b49\u591a\u79cd\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u5728\u6469\u6839\u5927\u901a\uff0c\u65b0\u95fb\u5206\u6790\u662f\u4e00\u9879\u5173\u952e\u4efb\u52a1\uff0c\u9762\u4e34\u5de8\u5927\u7684\u5e02\u573a\u9700\u6c42\uff0825\u4e2a\u56e2\u961f\u5bfb\u6c42\u89e3\u51b3\u65b9\u6848\uff09\u548c\u9ad8\u6602\u7684\u5916\u90e8\u4f9b\u5e94\u5546\u6210\u672c\uff08\u6bcf\u5e74\u8d85\u8fc7200\u4e07\u7f8e\u5143\uff09\u3002\u6709\u6548\u7684\u5b9e\u4f53\u94fe\u63a5\u5bf9\u4e8e\u5c06\u975e\u7ed3\u6784\u5316\u65b0\u95fb\u6587\u672c\u4e0e\u77e5\u8bc6\u56fe\u8c31\u8fde\u63a5\uff0c\u4ece\u800c\u4f7f\u7528\u6237\u8bbf\u95ee\u5927\u91cf\u7cbe\u9009\u6570\u636e\u5e76\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u5e76\u5f00\u53d1\u4e86JEL\u6a21\u578b\u3002JEL\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u591a\u795e\u7ecf\u7f51\u7edc\u5b9e\u4f53\u94fe\u63a5\u6a21\u578b\u3002", "result": "JEL\u6a21\u578b\u5728\u5b9e\u4f53\u94fe\u63a5\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002", "conclusion": "JEL\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6027\u80fd\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u5b9e\u4f53\u94fe\u63a5\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u6865\u63a5\u975e\u7ed3\u6784\u5316\u65b0\u95fb\u6587\u672c\u4e0e\u77e5\u8bc6\u56fe\u8c31\uff0c\u89e3\u51b3\u4e86\u5728\u91d1\u878d\u65b0\u95fb\u5206\u6790\u7b49\u9886\u57df\u4e2d\u9762\u4e34\u7684\u5b9e\u9645\u4e1a\u52a1\u75db\u70b9\uff0c\u5e76\u80fd\u663e\u8457\u63d0\u5347\u4f01\u4e1a\u7684\u6570\u636e\u5229\u7528\u6548\u7387\u3002"}}
{"id": "2509.08595", "pdf": "https://arxiv.org/pdf/2509.08595", "abs": "https://arxiv.org/abs/2509.08595", "authors": ["Daniel Meyer", "Lisandro Z Granville", "Leandro M. Bertholdo"], "title": "The Role of Legacy Mobile Networks in Infrastructure Resilience: Evidence from the Southern Brazil Flood", "categories": ["cs.NI", "cs.CY", "cs.ET", "C.2.1; C.2.3"], "comment": "6 pages, 4 figures. To appear in IEEE GLOBECOM 2025 (preprint, before\n  peer review)", "summary": "This paper investigates the resilience of mobile communication networks\nduring the extreme flooding that affected Rio Grande do Sul, Brazil, in May\n2024. Based on regulatory data and technical insights from operators, the study\nidentifies the leading causes of mobile network disruptions, primarily related\nto flooding and prolonged power outages. The results reveal the significant\nvulnerability of modern networks (4G/5G) during the event and the essential\nrole played by legacy technologies (2G/3G) in sustaining basic connectivity\nunder adverse conditions. The findings underscore the necessity of\ndisaster-aware infrastructure planning, taking into account the ongoing\nsignificance of legacy systems, diversified power supply strategies, and\nresilient network designs to enhance service continuity during future crises.", "AI": {"tldr": "\u5206\u6790\u4e862024\u5e74\u5df4\u897f\u6d2a\u707e\u671f\u95f4\u79fb\u52a8\u7f51\u7edc\u7684\u5f39\u6027\uff0c\u53d1\u73b04G/5G\u6613\u53d7\u5f71\u54cd\uff0c2G/3G\u5728\u707e\u96be\u4e2d\u53d1\u6325\u5173\u952e\u4f5c\u7528\uff0c\u5f3a\u8c03\u9700\u89c4\u5212\u66f4\u5177\u97e7\u6027\u7684\u57fa\u7840\u8bbe\u65bd\u3002", "motivation": "\u8c03\u67e52024\u5e745\u6708\u5df4\u897f\u91cc\u5965\u683c\u5170\u5fb7\u5dde\u6781\u7aef\u6d2a\u707e\u671f\u95f4\u79fb\u52a8\u901a\u4fe1\u7f51\u7edc\u7684\u97e7\u6027\uff0c\u5e76\u8bc6\u522b\u5bfc\u81f4\u7f51\u7edc\u4e2d\u65ad\u7684\u4e3b\u8981\u539f\u56e0\u3002", "method": "\u57fa\u4e8e\u76d1\u7ba1\u6570\u636e\u548c\u8fd0\u8425\u5546\u63d0\u4f9b\u7684\u6280\u672f\u6d1e\u5bdf\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u7f51\u7edc\u4e2d\u65ad\u4e3b\u8981\u7531\u6d2a\u707e\u548c\u957f\u65f6\u95f4\u505c\u7535\u5f15\u8d77\uff1b\u73b0\u4ee3\u7f51\u7edc\uff084G/5G\uff09\u5728\u6b64\u6b21\u4e8b\u4ef6\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u8106\u5f31\u6027\uff1b\u4f20\u7edf\u6280\u672f\uff082G/3G\uff09\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u5bf9\u7ef4\u6301\u57fa\u672c\u8fde\u63a5\u53d1\u6325\u4e86\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "\u672a\u6765\u5371\u673a\u4e2d\uff0c\u6709\u5fc5\u8981\u8fdb\u884c\u707e\u5bb3\u611f\u77e5\u7684\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\uff0c\u8003\u8651\u4f20\u7edf\u7cfb\u7edf\u7684\u6301\u7eed\u91cd\u8981\u6027\uff0c\u91c7\u53d6\u591a\u6837\u5316\u7684\u4f9b\u7535\u7b56\u7565\uff0c\u5e76\u8bbe\u8ba1\u66f4\u5177\u97e7\u6027\u7684\u7f51\u7edc\uff0c\u4ee5\u589e\u5f3a\u670d\u52a1\u8fde\u7eed\u6027\u3002"}}
{"id": "2509.08025", "pdf": "https://arxiv.org/pdf/2509.08025", "abs": "https://arxiv.org/abs/2509.08025", "authors": ["Hoang-Trung Nguyen", "Tan-Minh Nguyen", "Xuan-Bach Le", "Tuan-Kiet Le", "Khanh-Huyen Nguyen", "Ha-Thanh Nguyen", "Thi-Hai-Yen Vuong", "Le-Minh Nguyen"], "title": "NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents the methodologies and results of the NOWJ team's\nparticipation across all five tasks at the COLIEE 2025 competition, emphasizing\nadvancements in the Legal Case Entailment task (Task 2). Our comprehensive\napproach systematically integrates pre-ranking models (BM25, BERT, monoT5),\nembedding-based semantic representations (BGE-m3, LLM2Vec), and advanced Large\nLanguage Models (Qwen-2, QwQ-32B, DeepSeek-V3) for summarization, relevance\nscoring, and contextual re-ranking. Specifically, in Task 2, our two-stage\nretrieval system combined lexical-semantic filtering with contextualized LLM\nanalysis, achieving first place with an F1 score of 0.3195. Additionally, in\nother tasks--including Legal Case Retrieval, Statute Law Retrieval, Legal\nTextual Entailment, and Legal Judgment Prediction--we demonstrated robust\nperformance through carefully engineered ensembles and effective prompt-based\nreasoning strategies. Our findings highlight the potential of hybrid models\nintegrating traditional IR techniques with contemporary generative models,\nproviding a valuable reference for future advancements in legal information\nprocessing.", "AI": {"tldr": "NOWJ\u56e2\u961f\u5728COLIEE 2025\u6bd4\u8d5b\u4e2d\u63d0\u4ea4\u4e86\u6240\u6709\u4e94\u9879\u4efb\u52a1\u7684\u65b9\u6cd5\u548c\u7ed3\u679c\uff0c\u5c24\u5176\u5728\u6cd5\u5f8b\u6848\u4f8b\u8574\u542b\u4efb\u52a1\uff08Task 2\uff09\u4e2d\u901a\u8fc7\u6df7\u5408\u6a21\u578b\u83b7\u5f97\u4e86\u7b2c\u4e00\u540d\u3002", "motivation": "\u53c2\u4e0eCOLIEE 2025\u6bd4\u8d5b\u5e76\u5c55\u793a\u6cd5\u5f8b\u4fe1\u606f\u5904\u7406\u65b9\u9762\u7684\u8fdb\u6b65\uff0c\u7279\u522b\u662f\u6cd5\u5f8b\u6848\u4f8b\u8574\u542b\u4efb\u52a1\u3002", "method": "\u7efc\u5408\u65b9\u6cd5\u7ed3\u5408\u4e86\u9884\u6392\u5e8f\u6a21\u578b\uff08BM25, BERT, monoT5\uff09\u3001\u57fa\u4e8e\u5d4c\u5165\u7684\u8bed\u4e49\u8868\u793a\uff08BGE-m3, LLM2Vec\uff09\u4ee5\u53ca\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Qwen-2, QwQ-32B, DeepSeek-V3\uff09\u8fdb\u884c\u6458\u8981\u3001\u76f8\u5173\u6027\u8bc4\u5206\u548c\u4e0a\u4e0b\u6587\u91cd\u6392\u5e8f\u3002\u5728Task 2\u4e2d\uff0c\u91c7\u7528\u4e86\u8bcd\u6c47\u8bed\u4e49\u8fc7\u6ee4\u4e0e\u4e0a\u4e0b\u6587LLM\u5206\u6790\u76f8\u7ed3\u5408\u7684\u4e24\u9636\u6bb5\u68c0\u7d22\u7cfb\u7edf\u3002\u5176\u4ed6\u4efb\u52a1\u5219\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u96c6\u6210\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u63a8\u7406\u7b56\u7565\u5b9e\u73b0\u3002", "result": "\u5728\u6cd5\u5f8b\u6848\u4f8b\u8574\u542b\u4efb\u52a1\uff08Task 2\uff09\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0cF1\u5206\u6570\u4e3a0.3195\u3002\u5728\u6cd5\u5f8b\u6848\u4f8b\u68c0\u7d22\u3001\u6cd5\u89c4\u68c0\u7d22\u3001\u6cd5\u5f8b\u6587\u672c\u8574\u542b\u548c\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u7b49\u5176\u4ed6\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5c06\u4f20\u7edf\u4fe1\u606f\u68c0\u7d22\u6280\u672f\u4e0e\u5f53\u4ee3\u751f\u6210\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u6df7\u5408\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u6cd5\u5f8b\u4fe1\u606f\u5904\u7406\u7684\u8fdb\u6b65\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2509.08024", "pdf": "https://arxiv.org/pdf/2509.08024", "abs": "https://arxiv.org/abs/2509.08024", "authors": ["Lata Pangtey", "Omkar Kabde", "Shahid Shafi Dar", "Nagendra Kumar"], "title": "Two Stage Context Learning with Large Language Models for Multimodal Stance Detection on Climate Change", "categories": ["cs.CV", "cs.CY"], "comment": null, "summary": "With the rapid proliferation of information across digital platforms, stance\ndetection has emerged as a pivotal challenge in social media analysis. While\nmost of the existing approaches focus solely on textual data, real-world social\nmedia content increasingly combines text with visual elements creating a need\nfor advanced multimodal methods. To address this gap, we propose a multimodal\nstance detection framework that integrates textual and visual information\nthrough a hierarchical fusion approach. Our method first employs a Large\nLanguage Model to retrieve stance-relevant summaries from source text, while a\ndomain-aware image caption generator interprets visual content in the context\nof the target topic. These modalities are then jointly modeled along with the\nreply text, through a specialized transformer module that captures interactions\nbetween the texts and images. The proposed modality fusion framework integrates\ndiverse modalities to facilitate robust stance classification. We evaluate our\napproach on the MultiClimate dataset, a benchmark for climate change-related\nstance detection containing aligned video frames and transcripts. We achieve\naccuracy of 76.2%, precision of 76.3%, recall of 76.2% and F1-score of 76.2%,\nrespectively, outperforming existing state-of-the-art approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u878d\u5408\u7684\u591a\u6a21\u6001\u7acb\u573a\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7LLM\u5904\u7406\u6587\u672c\u3001\u56fe\u50cf\u5b57\u5e55\u751f\u6210\u5668\u5904\u7406\u89c6\u89c9\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u4e13\u95e8\u7684Transformer\u6a21\u5757\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u5728MultiClimate\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u8d85\u8d8aSOTA\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7acb\u573a\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u6570\u636e\uff0c\u4f46\u5b9e\u9645\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u65e5\u76ca\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u5143\u7d20\uff0c\u8fd9\u8981\u6c42\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5206\u5c42\u878d\u5408\u7684\u591a\u6a21\u6001\u7acb\u573a\u68c0\u6d4b\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u6e90\u6587\u672c\u4e2d\u63d0\u53d6\u7acb\u573a\u76f8\u5173\u6458\u8981\uff0c\u540c\u65f6\u5229\u7528\u9886\u57df\u611f\u77e5\u56fe\u50cf\u5b57\u5e55\u751f\u6210\u5668\u89e3\u91ca\u89c6\u89c9\u5185\u5bb9\u3002\u7136\u540e\uff0c\u901a\u8fc7\u4e00\u4e2a\u4e13\u95e8\u7684Transformer\u6a21\u5757\uff0c\u5c06\u8fd9\u4e9b\u6a21\u6001\uff08\u4ee5\u53ca\u56de\u590d\u6587\u672c\uff09\u8054\u5408\u5efa\u6a21\uff0c\u4ee5\u6355\u6349\u6587\u672c\u548c\u56fe\u50cf\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4ece\u800c\u5b9e\u73b0\u9c81\u68d2\u7684\u7acb\u573a\u5206\u7c7b\u3002", "result": "\u5728MultiClimate\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d6\u5f97\u4e8676.2%\u7684\u51c6\u786e\u7387\u300176.3%\u7684\u7cbe\u786e\u7387\u300176.2%\u7684\u53ec\u56de\u7387\u548c76.2%\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u7acb\u573a\u68c0\u6d4b\u6846\u67b6\u901a\u8fc7\u6709\u6548\u6574\u5408\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u7acb\u573a\u5206\u7c7b\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u793e\u4ea4\u5a92\u4f53\u4e2d\u591a\u6a21\u6001\u5185\u5bb9\u7acb\u573a\u68c0\u6d4b\u7684\u6311\u6218\u3002"}}
{"id": "2509.08222", "pdf": "https://arxiv.org/pdf/2509.08222", "abs": "https://arxiv.org/abs/2509.08222", "authors": ["Minjong Yoo", "Jinwoo Jang", "Wei-jin Park", "Honguk Woo"], "title": "Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following", "categories": ["cs.AI"], "comment": "21 pages. NeurIPS 2024", "summary": "This study presents an Exploratory Retrieval-Augmented Planning (ExRAP)\nframework, designed to tackle continual instruction following tasks of embodied\nagents in dynamic, non-stationary environments. The framework enhances Large\nLanguage Models' (LLMs) embodied reasoning capabilities by efficiently\nexploring the physical environment and establishing the environmental context\nmemory, thereby effectively grounding the task planning process in time-varying\nenvironment contexts. In ExRAP, given multiple continual instruction following\ntasks, each instruction is decomposed into queries on the environmental context\nmemory and task executions conditioned on the query results. To efficiently\nhandle these multiple tasks that are performed continuously and simultaneously,\nwe implement an exploration-integrated task planning scheme by incorporating\nthe {information-based exploration} into the LLM-based planning process.\nCombined with memory-augmented query evaluation, this integrated scheme not\nonly allows for a better balance between the validity of the environmental\ncontext memory and the load of environment exploration, but also improves\noverall task performance. Furthermore, we devise a {temporal consistency\nrefinement} scheme for query evaluation to address the inherent decay of\nknowledge in the memory. Through experiments with VirtualHome, ALFRED, and\nCARLA, our approach demonstrates robustness against a variety of embodied\ninstruction following scenarios involving different instruction scales and\ntypes, and non-stationarity degrees, and it consistently outperforms other\nstate-of-the-art LLM-based task planning approaches in terms of both goal\nsuccess rate and execution efficiency.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faExRAP\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u63a2\u7d22\u73af\u5883\u548c\u5efa\u7acb\u4e0a\u4e0b\u6587\u8bb0\u5fc6\uff0c\u589e\u5f3aLLM\u5728\u52a8\u6001\u975e\u9759\u6001\u73af\u5883\u4e2d\u6301\u7eed\u6307\u4ee4\u9075\u5faa\u4efb\u52a1\u7684\u5177\u8eab\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u6574\u5408\u4fe1\u606f\u63a2\u7d22\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4f18\u5316\uff0c\u5728\u591a\u9879\u5177\u8eab\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eSOTA\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5177\u8eab\u667a\u80fd\u4f53\u5728\u52a8\u6001\u3001\u975e\u9759\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u6301\u7eed\u6307\u4ee4\u9075\u5faa\u4efb\u52a1\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5177\u8eab\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u6709\u6548\u5229\u7528\u65f6\u53d8\u73af\u5883\u4e0a\u4e0b\u6587\u8fdb\u884c\u4efb\u52a1\u89c4\u5212\u3002", "method": "1. \u63d0\u51fa**Exploratory Retrieval-Augmented Planning (ExRAP)**\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u63a2\u7d22\u7269\u7406\u73af\u5883\u5e76\u5efa\u7acb\u73af\u5883\u4e0a\u4e0b\u6587\u8bb0\u5fc6\uff0c\u5c06\u4efb\u52a1\u89c4\u5212\u4e0e\u65f6\u53d8\u73af\u5883\u4e0a\u4e0b\u6587\u6709\u6548\u7ed3\u5408\u30022. \u5c06\u6301\u7eed\u6307\u4ee4\u5206\u89e3\u4e3a\u5bf9\u73af\u5883\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u7684\u67e5\u8be2\u548c\u57fa\u4e8e\u67e5\u8be2\u7ed3\u679c\u7684\u4efb\u52a1\u6267\u884c\u30023. \u5b9e\u73b0**\u63a2\u7d22\u96c6\u6210\u4efb\u52a1\u89c4\u5212\u65b9\u6848**\uff0c\u5c06**\u4fe1\u606f\u63a2\u7d22**\u6574\u5408\u5230LLM\u7684\u89c4\u5212\u8fc7\u7a0b\u4e2d\u30024. \u7ed3\u5408**\u8bb0\u5fc6\u589e\u5f3a\u67e5\u8be2\u8bc4\u4f30**\uff0c\u5e73\u8861\u73af\u5883\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u7684\u6709\u6548\u6027\u548c\u63a2\u7d22\u8d1f\u8f7d\uff0c\u540c\u65f6\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u30025. \u8bbe\u8ba1**\u65f6\u95f4\u4e00\u81f4\u6027\u7ec6\u5316\u65b9\u6848**\u7528\u4e8e\u67e5\u8be2\u8bc4\u4f30\uff0c\u4ee5\u89e3\u51b3\u8bb0\u5fc6\u4e2d\u77e5\u8bc6\u7684\u56fa\u6709\u8870\u51cf\u95ee\u9898\u3002", "result": "1. \u901a\u8fc7\u5728VirtualHome\u3001ALFRED\u548cCARLA\u4e0a\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5177\u8eab\u6307\u4ee4\u9075\u5faa\u573a\u666f\uff08\u5305\u62ec\u4e0d\u540c\u6307\u4ee4\u89c4\u6a21\u3001\u7c7b\u578b\u548c\u975e\u9759\u6001\u7a0b\u5ea6\uff09\u4e0b\u7684\u9c81\u68d2\u6027\u30022. \u5728**\u76ee\u6807\u6210\u529f\u7387**\u548c**\u6267\u884c\u6548\u7387**\u65b9\u9762\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u57fa\u4e8eLLM\u7684\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\u3002", "conclusion": "ExRAP\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5730\u63a2\u7d22\u73af\u5883\u3001\u5efa\u7acb\u5e76\u7ef4\u62a4\u4e0a\u4e0b\u6587\u8bb0\u5fc6\uff08\u5305\u62ec\u65f6\u95f4\u4e00\u81f4\u6027\u4f18\u5316\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u52a8\u6001\u975e\u9759\u6001\u73af\u5883\u4e2d\u5904\u7406\u6301\u7eed\u5177\u8eab\u6307\u4ee4\u9075\u5faa\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u5e76\u5728\u5173\u952e\u6027\u80fd\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5b9e\u7528\u4ef7\u503c\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.08087", "pdf": "https://arxiv.org/pdf/2509.08087", "abs": "https://arxiv.org/abs/2509.08087", "authors": ["Victor Garcia", "Mariia Sidulova", "Aldo Badano"], "title": "Performance Assessment Strategies for Generative AI Applications in Healthcare", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Generative artificial intelligence (GenAI) represent an emerging paradigm\nwithin artificial intelligence, with applications throughout the medical\nenterprise. Assessing GenAI applications necessitates a comprehensive\nunderstanding of the clinical task and awareness of the variability in\nperformance when implemented in actual clinical environments. Presently, a\nprevalent method for evaluating the performance of generative models relies on\nquantitative benchmarks. Such benchmarks have limitations and may suffer from\ntrain-to-the-test overfitting, optimizing performance for a specified test set\nat the cost of generalizability across other task and data distributions.\nEvaluation strategies leveraging human expertise and utilizing cost-effective\ncomputational models as evaluators are gaining interest. We discuss current\nstate-of-the-art methodologies for assessing the performance of GenAI\napplications in healthcare and medical devices.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u8bc4\u4f30\u533b\u7597\u9886\u57df\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u5e94\u7528\u7684\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0c\u6307\u51fa\u5f53\u524d\u5b9a\u91cf\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u7ed3\u5408\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u548c\u8ba1\u7b97\u6a21\u578b\u7684\u8bc4\u4f30\u7b56\u7565\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u533b\u7597\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u8bc4\u4f30\u9762\u4e34\u6311\u6218\u3002\u5f53\u524d\u666e\u904d\u4f7f\u7528\u7684\u5b9a\u91cf\u57fa\u51c6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u201c\u8bad\u7ec3\u8fc7\u5ea6\u62df\u5408\u6d4b\u8bd5\u96c6\u201d\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u5168\u9762\u548c\u6709\u6548\u7684\u8bc4\u4f30\u7b56\u7565\u3002", "method": "\u672c\u6587\u91c7\u7528\u8ba8\u8bba\u548c\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u63a2\u8ba8\u5e76\u603b\u7ed3\u4e86\u8bc4\u4f30\u533b\u7597\u4fdd\u5065\u548c\u533b\u7597\u8bbe\u5907\u4e2dGenAI\u5e94\u7528\u6027\u80fd\u7684\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u524d\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u7684\u5b9a\u91cf\u57fa\u51c6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u6613\u53d1\u751f\u8bad\u7ec3\u96c6\u8fc7\u5ea6\u62df\u5408\u6d4b\u8bd5\u96c6\u3001\u727a\u7272\u6cdb\u5316\u80fd\u529b\u7b49\u95ee\u9898\u3002\u540c\u65f6\uff0c\u5229\u7528\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u548c\u7ecf\u6d4e\u9ad8\u6548\u7684\u8ba1\u7b97\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u8005\u7684\u7b56\u7565\u6b63\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002", "conclusion": "\u8bc4\u4f30\u533b\u7597\u9886\u57df\u7684GenAI\u5e94\u7528\u9700\u8981\u6df1\u5165\u7406\u89e3\u4e34\u5e8a\u4efb\u52a1\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u6027\u80fd\u53d8\u5f02\u6027\u3002\u4e3a\u514b\u670d\u73b0\u6709\u5b9a\u91cf\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u672a\u6765\u5e94\u66f4\u591a\u5730\u91c7\u7528\u7ed3\u5408\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u548c\u8ba1\u7b97\u6a21\u578b\u7684\u8bc4\u4f30\u7b56\u7565\uff0c\u4ee5\u786e\u4fddGenAI\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.08248", "pdf": "https://arxiv.org/pdf/2509.08248", "abs": "https://arxiv.org/abs/2509.08248", "authors": ["Arin Upadhyay"], "title": "EFPIX: A zero-trust encrypted flood protocol", "categories": ["cs.CR", "cs.NI"], "comment": null, "summary": "We propose a flood-based relay communication protocol that achieves\nend-to-end encryption, plausible deniability for users, and untraceable\nmessages. It is resistant to changes in topology and infrastructure failures.\nIt is also designed to hide metadata, such as sender and receiver, from those\nnot involved.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6d2a\u6cdb\u7684\u4e2d\u7ee7\u901a\u4fe1\u534f\u8bae\uff0c\u65e8\u5728\u5b9e\u73b0\u7aef\u5230\u7aef\u52a0\u5bc6\u3001\u7528\u6237\u53ef\u5426\u8ba4\u6027\u53ca\u6d88\u606f\u4e0d\u53ef\u8ffd\u6eaf\u6027\uff0c\u5e76\u80fd\u9690\u85cf\u5143\u6570\u636e\uff0c\u540c\u65f6\u62b5\u6297\u62d3\u6251\u53d8\u5316\u548c\u57fa\u7840\u8bbe\u65bd\u6545\u969c\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u901a\u4fe1\u534f\u8bae\u5728\u5b89\u5168\u6027\u3001\u9690\u79c1\u6027\uff08\u5982\u7aef\u5230\u7aef\u52a0\u5bc6\u3001\u7528\u6237\u53ef\u5426\u8ba4\u6027\u3001\u6d88\u606f\u4e0d\u53ef\u8ffd\u6eaf\u6027\u53ca\u5143\u6570\u636e\u9690\u85cf\uff09\u548c\u9c81\u68d2\u6027\uff08\u62b5\u6297\u62d3\u6251\u53d8\u5316\u548c\u57fa\u7840\u8bbe\u65bd\u6545\u969c\uff09\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d2a\u6cdb\uff08flood-based\uff09\u7684\u4e2d\u7ee7\u901a\u4fe1\u534f\u8bae\u3002", "result": "\u8be5\u534f\u8bae\u6210\u529f\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u52a0\u5bc6\u3001\u7528\u6237\u53ef\u5426\u8ba4\u6027\u3001\u6d88\u606f\u4e0d\u53ef\u8ffd\u6eaf\u6027\uff0c\u80fd\u591f\u62b5\u6297\u62d3\u6251\u53d8\u5316\u548c\u57fa\u7840\u8bbe\u65bd\u6545\u969c\uff0c\u5e76\u80fd\u5411\u65e0\u5173\u65b9\u9690\u85cf\u53d1\u9001\u65b9\u548c\u63a5\u6536\u65b9\u7b49\u5143\u6570\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u5ea6\u5b89\u5168\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u5bf9\u73af\u5883\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u7684\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08032", "pdf": "https://arxiv.org/pdf/2509.08032", "abs": "https://arxiv.org/abs/2509.08032", "authors": ["Fengyu She", "Nan Wang", "Hongfei Wu", "Ziyi Wan", "Jingmian Wang", "Chang Wang"], "title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery", "categories": ["cs.CL"], "comment": null, "summary": "Scientific literature is growing exponentially, creating a critical\nbottleneck for researchers to efficiently synthesize knowledge. While\ngeneral-purpose Large Language Models (LLMs) show potential in text processing,\nthey often fail to capture scientific domain-specific nuances (e.g., technical\njargon, methodological rigor) and struggle with complex scientific tasks,\nlimiting their utility for interdisciplinary research. To address these gaps,\nthis paper presents SciGPT, a domain-adapted foundation model for scientific\nliterature understanding and ScienceBench, an open source benchmark tailored to\nevaluate scientific LLMs.\n  Built on the Qwen3 architecture, SciGPT incorporates three key innovations:\n(1) low-cost domain distillation via a two-stage pipeline to balance\nperformance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention\nmechanism that cuts memory consumption by 55\\% for 32,000-token long-document\nreasoning; and (3) knowledge-aware adaptation integrating domain ontologies to\nbridge interdisciplinary knowledge gaps.\n  Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in\ncore scientific tasks including sequence labeling, generation, and inference.\nIt also exhibits strong robustness in unseen scientific tasks, validating its\npotential to facilitate AI-augmented scientific discovery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSciGPT\uff0c\u4e00\u4e2a\u57fa\u4e8eQwen3\u67b6\u6784\u7684\u9886\u57df\u9002\u5e94\u6027\u57fa\u7840\u6a21\u578b\uff0c\u4e13\u4e3a\u79d1\u5b66\u6587\u732e\u7406\u89e3\u8bbe\u8ba1\uff0c\u5e76\u5f15\u5165ScienceBench\u4f5c\u4e3a\u8bc4\u4f30\u57fa\u51c6\u3002SciGPT\u901a\u8fc7\u4f4e\u6210\u672c\u9886\u57df\u84b8\u998f\u3001\u7a00\u758fMoE\u6ce8\u610f\u529b\u673a\u5236\u548c\u77e5\u8bc6\u611f\u77e5\u9002\u5e94\u6027\u521b\u65b0\uff0c\u5728\u6838\u5fc3\u79d1\u5b66\u4efb\u52a1\u4e0a\u8d85\u8d8aGPT-4o\uff0c\u5e76\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u5bfc\u81f4\u7814\u7a76\u4eba\u5458\u9ad8\u6548\u5408\u6210\u77e5\u8bc6\u9762\u4e34\u74f6\u9888\u3002\u901a\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u6355\u6349\u79d1\u5b66\u9886\u57df\u7279\u5b9a\u7ec6\u5fae\u5dee\u522b\uff08\u5982\u4e13\u4e1a\u672f\u8bed\u3001\u65b9\u6cd5\u4e25\u8c28\u6027\uff09\u5e76\u5904\u7406\u590d\u6742\u79d1\u5b66\u4efb\u52a1\uff0c\u9650\u5236\u4e86\u5176\u5728\u8de8\u5b66\u79d1\u7814\u7a76\u4e2d\u7684\u6548\u7528\u3002", "method": "\u63d0\u51faSciGPT\uff0c\u4e00\u4e2a\u57fa\u4e8eQwen3\u67b6\u6784\u7684\u9886\u57df\u9002\u5e94\u6027\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u79d1\u5b66\u6587\u732e\u7406\u89e3\u3002\u6838\u5fc3\u521b\u65b0\u5305\u62ec\uff1a1) \u901a\u8fc7\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebf\u5b9e\u73b0\u4f4e\u6210\u672c\u9886\u57df\u84b8\u998f\uff1b2) \u91c7\u7528\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\uff08SMoE\uff09\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c0632,000-token\u957f\u6587\u6863\u63a8\u7406\u7684\u5185\u5b58\u6d88\u8017\u964d\u4f4e55%\uff1b3) \u6574\u5408\u9886\u57df\u672c\u4f53\u7684\u77e5\u8bc6\u611f\u77e5\u9002\u5e94\u6027\u3002\u540c\u65f6\uff0c\u8fd8\u53d1\u5e03\u4e86ScienceBench\uff0c\u4e00\u4e2a\u8bc4\u4f30\u79d1\u5b66LLM\u7684\u5f00\u6e90\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728ScienceBench\u4e0a\uff0cSciGPT\u5728\u5305\u62ec\u5e8f\u5217\u6807\u6ce8\u3001\u751f\u6210\u548c\u63a8\u7406\u5728\u5185\u7684\u6838\u5fc3\u79d1\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eGPT-4o\u3002\u6b64\u5916\uff0c\u5b83\u5728\u672a\u89c1\u79d1\u5b66\u4efb\u52a1\u4e2d\u4e5f\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SciGPT\u9a8c\u8bc1\u4e86\u5176\u4fc3\u8fdbAI\u589e\u5f3a\u578b\u79d1\u5b66\u53d1\u73b0\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.08026", "pdf": "https://arxiv.org/pdf/2509.08026", "abs": "https://arxiv.org/abs/2509.08026", "authors": ["Zeinab Ghasemi Darehnaei", "Mohammad Shokouhifar", "Hossein Yazdanjouei", "S. M. J. Rastegar Fatemi"], "title": "Two-Stage Swarm Intelligence Ensemble Deep Transfer Learning (SI-EDTL) for Vehicle Detection Using Unmanned Aerial Vehicles", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper introduces SI-EDTL, a two-stage swarm intelligence ensemble deep\ntransfer learning model for detecting multiple vehicles in UAV images. It\ncombines three pre-trained Faster R-CNN feature extractor models (InceptionV3,\nResNet50, GoogLeNet) with five transfer classifiers (KNN, SVM, MLP, C4.5,\nNa\\\"ive Bayes), resulting in 15 different base learners. These are aggregated\nvia weighted averaging to classify regions as Car, Van, Truck, Bus, or\nbackground. Hyperparameters are optimized with the whale optimization algorithm\nto balance accuracy, precision, and recall. Implemented in MATLAB R2020b with\nparallel processing, SI-EDTL outperforms existing methods on the AU-AIR UAV\ndataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSI-EDTL\uff0c\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u7fa4\u667a\u80fd\u96c6\u6210\u6df1\u5ea6\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u7684\u591a\u8f66\u8f86\u68c0\u6d4b\u3002", "motivation": "\u63d0\u5347\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u591a\u7c7b\u578b\u8f66\u8f86\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "SI-EDTL\u6a21\u578b\u7ed3\u5408\u4e09\u4e2a\u9884\u8bad\u7ec3\u7684Faster R-CNN\u7279\u5f81\u63d0\u53d6\u5668\uff08InceptionV3, ResNet50, GoogLeNet\uff09\u548c\u4e94\u4e2a\u8fc1\u79fb\u5206\u7c7b\u5668\uff08KNN, SVM, MLP, C4.5, Naive Bayes\uff09\uff0c\u6784\u5efa15\u4e2a\u57fa\u7840\u5b66\u4e60\u5668\u3002\u8fd9\u4e9b\u5b66\u4e60\u5668\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u8fdb\u884c\u805a\u5408\uff0c\u7528\u4e8e\u5206\u7c7b\u6c7d\u8f66\u3001\u8d27\u8f66\u3001\u5361\u8f66\u3001\u5df4\u58eb\u6216\u80cc\u666f\u3002\u8d85\u53c2\u6570\u4f7f\u7528\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5\uff08WOA\uff09\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u5e73\u8861\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u3002\u6a21\u578b\u5728MATLAB R2020b\u4e2d\u5e76\u884c\u5b9e\u73b0\u3002", "result": "SI-EDTL\u6a21\u578b\u5728AU-AIR\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SI-EDTL\u4e3a\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u7684\u591a\u8f66\u8f86\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u5353\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08282", "pdf": "https://arxiv.org/pdf/2509.08282", "abs": "https://arxiv.org/abs/2509.08282", "authors": ["Seonghyeon Go"], "title": "Real-world Music Plagiarism Detection With Music Segment Transcription System", "categories": ["cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted in APSIPA 2025 but not published yet(will be published in 2\n  month..), Arxiv preprint ready for references in future-works", "summary": "As a result of continuous advances in Music Information Retrieval (MIR)\ntechnology, generating and distributing music has become more diverse and\naccessible. In this context, interest in music intellectual property protection\nis increasing to safeguard individual music copyrights. In this work, we\npropose a system for detecting music plagiarism by combining various MIR\ntechnologies. We developed a music segment transcription system that extracts\nmusically meaningful segments from audio recordings to detect plagiarism across\ndifferent musical formats. With this system, we compute similarity scores based\non multiple musical features that can be evaluated through comprehensive\nmusical analysis. Our approach demonstrated promising results in music\nplagiarism detection experiments, and the proposed method can be applied to\nreal-world music scenarios. We also collected a Similar Music Pair (SMP)\ndataset for musical similarity research using real-world cases. The dataset are\npublicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u79cd\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\uff08MIR\uff09\u6280\u672f\u7684\u97f3\u4e50\u6284\u88ad\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u63d0\u53d6\u97f3\u4e50\u7247\u6bb5\u5e76\u57fa\u4e8e\u591a\u7ef4\u7279\u5f81\u8ba1\u7b97\u76f8\u4f3c\u5ea6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002\u540c\u65f6\uff0c\u516c\u5f00\u4e86\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u6848\u4f8b\u7684\u76f8\u4f3c\u97f3\u4e50\u5bf9\uff08SMP\uff09\u6570\u636e\u96c6\u3002", "motivation": "\u968f\u7740MIR\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u97f3\u4e50\u7684\u751f\u6210\u548c\u5206\u53d1\u53d8\u5f97\u66f4\u52a0\u591a\u6837\u548c\u4fbf\u6377\uff0c\u56e0\u6b64\u5bf9\u4fdd\u62a4\u4e2a\u4eba\u97f3\u4e50\u7248\u6743\u548c\u77e5\u8bc6\u4ea7\u6743\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u97f3\u4e50\u7247\u6bb5\u8f6c\u5f55\u7cfb\u7edf\uff0c\u4ece\u97f3\u9891\u8bb0\u5f55\u4e2d\u63d0\u53d6\u5177\u6709\u97f3\u4e50\u610f\u4e49\u7684\u7247\u6bb5\uff0c\u4ee5\u5b9e\u73b0\u8de8\u4e0d\u540c\u97f3\u4e50\u683c\u5f0f\u7684\u6284\u88ad\u68c0\u6d4b\u3002\u968f\u540e\uff0c\u5229\u7528\u591a\u79cd\u97f3\u4e50\u7279\u5f81\u8ba1\u7b97\u76f8\u4f3c\u5ea6\u5206\u6570\uff0c\u5e76\u901a\u8fc7\u5168\u9762\u7684\u97f3\u4e50\u5206\u6790\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u97f3\u4e50\u6284\u88ad\u68c0\u6d4b\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u524d\u666f\uff0c\u5e76\u53ef\u5e94\u7528\u4e8e\u5b9e\u9645\u97f3\u4e50\u573a\u666f\u3002\u6b64\u5916\uff0c\u6536\u96c6\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u6848\u4f8b\u7684\u76f8\u4f3c\u97f3\u4e50\u5bf9\uff08SMP\uff09\u6570\u636e\u96c6\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5229\u7528MIR\u6280\u672f\u6709\u6548\u68c0\u6d4b\u97f3\u4e50\u6284\u88ad\u7684\u7cfb\u7edf\uff0c\u4e3a\u97f3\u4e50\u7248\u6743\u4fdd\u62a4\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u6709\u52a9\u4e8e\u97f3\u4e50\u76f8\u4f3c\u6027\u7814\u7a76\u7684SMP\u6570\u636e\u96c6\u3002"}}
{"id": "2509.08089", "pdf": "https://arxiv.org/pdf/2509.08089", "abs": "https://arxiv.org/abs/2509.08089", "authors": ["Lucas Fenaux", "Zheng Wang", "Jacob Yan", "Nathan Chung", "Florian Kerschbaum"], "title": "Hammer and Anvil: A Principled Defense Against Backdoors in Federated Learning", "categories": ["cs.LG", "cs.CR", "68T99"], "comment": null, "summary": "Federated Learning is a distributed learning technique in which multiple\nclients cooperate to train a machine learning model. Distributed settings\nfacilitate backdoor attacks by malicious clients, who can embed malicious\nbehaviors into the model during their participation in the training process.\nThese malicious behaviors are activated during inference by a specific trigger.\nNo defense against backdoor attacks has stood the test of time, especially\nagainst adaptive attackers, a powerful but not fully explored category of\nattackers. In this work, we first devise a new adaptive adversary that\nsurpasses existing adversaries in capabilities, yielding attacks that only\nrequire one or two malicious clients out of 20 to break existing\nstate-of-the-art defenses. Then, we present Hammer and Anvil, a principled\ndefense approach that combines two defenses orthogonal in their underlying\nprinciple to produce a combined defense that, given the right set of\nparameters, must succeed against any attack. We show that our best combined\ndefense, Krum+, is successful against our new adaptive adversary and\nstate-of-the-art attacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u81ea\u9002\u5e94\u653b\u51fb\uff0c\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u4ec5\u9700\u5c11\u91cf\u6076\u610f\u5ba2\u6237\u7aef\u5373\u53ef\u653b\u7834\u73b0\u6709SOTA\u540e\u95e8\u9632\u5fa1\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u201cHammer and Anvil\u201d\u7ec4\u5408\u9632\u5fa1\u6846\u67b6\uff0c\u5176\u4e2dKrum+\u80fd\u6709\u6548\u62b5\u5fa1\u8fd9\u79cd\u65b0\u578b\u653b\u51fb\u548c\u73b0\u6709SOTA\u653b\u51fb\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u5f0f\u73af\u5883\u5bb9\u6613\u53d7\u5230\u6076\u610f\u5ba2\u6237\u7aef\u7684\u540e\u95e8\u653b\u51fb\uff0c\u800c\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u5f3a\u5927\u7684\u81ea\u9002\u5e94\u653b\u51fb\u8005\u65f6\uff0c\u672a\u80fd\u901a\u8fc7\u65f6\u95f4\u8003\u9a8c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u653b\u51fb\u6765\u66b4\u9732\u73b0\u6709\u9632\u5fa1\u7684\u4e0d\u8db3\uff0c\u5e76\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u7b56\u7565\u3002", "method": "\u9996\u5148\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8d85\u8d8a\u73b0\u6709\u5bf9\u624b\u80fd\u529b\u7684\u65b0\u578b\u81ea\u9002\u5e94\u653b\u51fb\u8005\uff0c\u8be5\u653b\u51fb\u8005\u53ea\u970020\u4e2a\u5ba2\u6237\u7aef\u4e2d\u76841-2\u4e2a\u6076\u610f\u5ba2\u6237\u7aef\u5373\u53ef\u653b\u7834\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u3002\u5176\u6b21\uff0c\u63d0\u51fa\u201cHammer and Anvil\u201d\u8fd9\u4e00\u539f\u5219\u6027\u9632\u5fa1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e24\u79cd\u57fa\u4e8e\u6b63\u4ea4\u539f\u7406\u7684\u9632\u5fa1\u673a\u5236\uff0c\u65e8\u5728\u5f62\u6210\u4e00\u4e2a\u5728\u6b63\u786e\u53c2\u6570\u8bbe\u7f6e\u4e0b\u80fd\u6210\u529f\u62b5\u5fa1\u4efb\u4f55\u653b\u51fb\u7684\u7ec4\u5408\u9632\u5fa1\u3002\u5176\u4e2d\uff0cKrum+\u662f\u6700\u4f73\u7684\u7ec4\u5408\u9632\u5fa1\u3002", "result": "\u6240\u8bbe\u8ba1\u7684\u65b0\u578b\u81ea\u9002\u5e94\u653b\u51fb\u8005\u80fd\u591f\u4ee5\u6781\u4f4e\u7684\u6076\u610f\u5ba2\u6237\u7aef\u6bd4\u4f8b\uff081-2/20\uff09\u6210\u529f\u653b\u7834\u73b0\u6709\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6700\u4f73\u7ec4\u5408\u9632\u5fa1Krum+\u80fd\u6709\u6548\u62b5\u5fa1\u8fd9\u79cd\u65b0\u578b\u81ea\u9002\u5e94\u653b\u51fb\u8005\u4ee5\u53ca\u73b0\u6709\u7684\u6700\u5148\u8fdb\u653b\u51fb\u3002", "conclusion": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u540e\u95e8\u9632\u5fa1\u5bf9\u81ea\u9002\u5e94\u653b\u51fb\u8005\u6548\u679c\u4e0d\u4f73\u3002\u672c\u6587\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u5f3a\u5927\u7684\u65b0\u578b\u81ea\u9002\u5e94\u653b\u51fb\u8005\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u9632\u5fa1\u7684\u8106\u5f31\u6027\uff0c\u5e76\u6210\u529f\u5f00\u53d1\u4e86\u201cHammer and Anvil\u201d\u9632\u5fa1\u6846\u67b6\u4e0b\u7684Krum+\u7ec4\u5408\u9632\u5fa1\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2d\u62b5\u5fa1\u590d\u6742\u540e\u95e8\u653b\u51fb\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u539f\u5219\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08384", "pdf": "https://arxiv.org/pdf/2509.08384", "abs": "https://arxiv.org/abs/2509.08384", "authors": ["Mateo M. Blanco", "Manuel Fern\u00e1ndez-Veiga", "Ana Fern\u00e1ndez-Vilas", "Rebeca P. D\u00edaz-Redondo"], "title": "From Physical to Logical: Graph-State-Based Connectivity in Quantum Networks", "categories": ["quant-ph", "cs.NI"], "comment": null, "summary": "Entanglement is a key resource in quantum communication, but bipartite\nschemes are often insufficient for advanced protocols like quantum secret\nsharing or distributed computing. Graph states offer a flexible way to\nrepresent and manage multipartite entanglement in quantum networks, enabling\nlogical connectivity through local operations and classical communication\n(LOCC). In this work, we extend existing approaches based on bi-star\nconfigurations to more complex multi-star topologies. We analyze the maximum\nconnectivity that can be achieved in networks of $m$ switches, each connected\nto $n$ clients, including asymmetric cases where the number of clients varies\nper switch. We also propose methods to enable logical communication between\ndistant nodes. Our results support the development of scalable quantum networks\nwith rich connectivity beyond traditional bipartite structures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6269\u5c55\u4e86\u57fa\u4e8e\u53cc\u661f\u914d\u7f6e\u7684\u56fe\u6001\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u591a\u661f\u62d3\u6251\u4e2d\u7684\u591a\u65b9\u7ea0\u7f20\u7ba1\u7406\uff0c\u65e8\u5728\u63d0\u9ad8\u91cf\u5b50\u7f51\u7edc\u7684\u8fde\u63a5\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u91cf\u5b50\u901a\u4fe1\u4e2d\u7684\u53cc\u5411\u7ea0\u7f20\u65b9\u6848\u4e0d\u8db3\u4ee5\u652f\u6301\u9ad8\u7ea7\u534f\u8bae\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u591a\u65b9\u7ea0\u7f20\u7ba1\u7406\u65b9\u5f0f\u3002\u56fe\u6001\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u914d\u7f6e\uff08\u5982\u53cc\u661f\uff09\u5728\u5b9e\u73b0\u4e30\u5bcc\u8fde\u63a5\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5c06\u73b0\u6709\u57fa\u4e8e\u53cc\u661f\u914d\u7f6e\u7684\u65b9\u6cd5\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u591a\u661f\u62d3\u6251\u7ed3\u6784\u3002\u5206\u6790\u7531$m$\u4e2a\u4ea4\u6362\u673a\uff08\u6bcf\u4e2a\u8fde\u63a5$n$\u4e2a\u5ba2\u6237\u7aef\uff0c\u5305\u62ec\u4e0d\u5bf9\u79f0\u60c5\u51b5\uff09\u7ec4\u6210\u7684\u7f51\u7edc\u4e2d\u53ef\u5b9e\u73b0\u7684\u6700\u5927\u8fde\u63a5\u6027\uff0c\u5e76\u63d0\u51fa\u8fdc\u8ddd\u79bb\u8282\u70b9\u95f4\u903b\u8f91\u901a\u4fe1\u7684\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u786e\u5b9a\u4e86\u5728\u591a\u661f\u62d3\u6251\u7f51\u7edc\u4e2d\u53ef\u5b9e\u73b0\u7684\u6700\u5927\u8fde\u63a5\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u73b0\u8fdc\u8ddd\u79bb\u8282\u70b9\u95f4\u903b\u8f91\u901a\u4fe1\u7684\u65b9\u6cd5\u3002\u8fd9\u4e9b\u7ed3\u679c\u652f\u6301\u5f00\u53d1\u8d85\u8d8a\u4f20\u7edf\u53cc\u5411\u7ed3\u6784\u7684\u53ef\u6269\u5c55\u91cf\u5b50\u7f51\u7edc\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u591a\u661f\u62d3\u6251\u6269\u5c55\u4e86\u56fe\u6001\u65b9\u6cd5\uff0c\u4e3a\u6784\u5efa\u5177\u6709\u9ad8\u8fde\u63a5\u6027\u548c\u53ef\u6269\u5c55\u6027\u3001\u80fd\u591f\u652f\u6301\u9ad8\u7ea7\u91cf\u5b50\u534f\u8bae\u7684\u91cf\u5b50\u7f51\u7edc\u63d0\u4f9b\u4e86\u57fa\u7840\u548c\u65b9\u6cd5\u652f\u6301\u3002"}}
{"id": "2509.08075", "pdf": "https://arxiv.org/pdf/2509.08075", "abs": "https://arxiv.org/abs/2509.08075", "authors": ["Flor Miriam Plaza-del-Arco", "Paul R\u00f6ttger", "Nino Scherrer", "Emanuele Borgonovo", "Elmar Plischke", "Dirk Hovy"], "title": "No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly integrated into our daily lives\nand personalized. However, LLM personalization might also increase unintended\nside effects. Recent work suggests that persona prompting can lead models to\nfalsely refuse user requests. However, no work has fully quantified the extent\nof this issue. To address this gap, we measure the impact of 15\nsociodemographic personas (based on gender, race, religion, and disability) on\nfalse refusal. To control for other factors, we also test 16 different models,\n3 tasks (Natural Language Inference, politeness, and offensiveness\nclassification), and nine prompt paraphrases. We propose a Monte Carlo-based\nmethod to quantify this issue in a sample-efficient manner. Our results show\nthat as models become more capable, personas impact the refusal rate less and\nless. Certain sociodemographic personas increase false refusal in some models,\nwhich suggests underlying biases in the alignment strategies or safety\nmechanisms. However, we find that the model choice and task significantly\ninfluence false refusals, especially in sensitive content tasks. Our findings\nsuggest that persona effects have been overestimated, and might be due to other\nfactors.", "AI": {"tldr": "\u672c\u7814\u7a76\u91cf\u5316\u4e86\u793e\u4f1a\u4eba\u53e3\u5b66\u89d2\u8272\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bef\u62d2\u7528\u6237\u8bf7\u6c42\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6a21\u578b\u80fd\u529b\u3001\u6a21\u578b\u9009\u62e9\u548c\u4efb\u52a1\u7c7b\u578b\u662f\u5f71\u54cd\u8bef\u62d2\u7387\u7684\u5173\u952e\u56e0\u7d20\uff0c\u800c\u89d2\u8272\u6548\u5e94\u53ef\u80fd\u88ab\u9ad8\u4f30\u3002", "motivation": "LLM\u4e2a\u6027\u5316\u53ef\u80fd\u589e\u52a0\u610f\u60f3\u4e0d\u5230\u7684\u526f\u4f5c\u7528\uff0c\u7279\u522b\u662f\u89d2\u8272\u63d0\u793a\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u8bef\u62d2\u7528\u6237\u8bf7\u6c42\u3002\u6b64\u524d\u7684\u5de5\u4f5c\u672a\u80fd\u5145\u5206\u91cf\u5316\u8fd9\u4e00\u95ee\u9898\u7684\u7a0b\u5ea6\u3002", "method": "\u7814\u7a76\u6d4b\u91cf\u4e8615\u79cd\u793e\u4f1a\u4eba\u53e3\u5b66\u89d2\u8272\uff08\u57fa\u4e8e\u6027\u522b\u3001\u79cd\u65cf\u3001\u5b97\u6559\u548c\u6b8b\u75be\uff09\u5bf9\u8bef\u62d2\u7684\u5f71\u54cd\u3002\u4e3a\u63a7\u5236\u5176\u4ed6\u56e0\u7d20\uff0c\u6d4b\u8bd5\u4e8616\u79cd\u4e0d\u540c\u6a21\u578b\u30013\u79cd\u4efb\u52a1\uff08\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u3001\u793c\u8c8c\u6027\u3001\u5192\u72af\u6027\u5206\u7c7b\uff09\u548c9\u79cd\u63d0\u793a\u8bcd\u53d8\u4f53\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u7684\u65b9\u6cd5\u4ee5\u6837\u672c\u9ad8\u6548\u7684\u65b9\u5f0f\u8fdb\u884c\u91cf\u5316\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u968f\u7740\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u89d2\u8272\u5bf9\u62d2\u7edd\u7387\u7684\u5f71\u54cd\u8d8a\u6765\u8d8a\u5c0f\u3002\u67d0\u4e9b\u793e\u4f1a\u4eba\u53e3\u5b66\u89d2\u8272\u5728\u90e8\u5206\u6a21\u578b\u4e2d\u786e\u5b9e\u589e\u52a0\u4e86\u8bef\u62d2\uff0c\u6697\u793a\u5bf9\u9f50\u7b56\u7565\u6216\u5b89\u5168\u673a\u5236\u5b58\u5728\u6f5c\u5728\u504f\u89c1\u3002\u7136\u800c\uff0c\u6a21\u578b\u9009\u62e9\u548c\u4efb\u52a1\u7c7b\u578b\u663e\u8457\u5f71\u54cd\u8bef\u62d2\uff0c\u5c24\u5176\u662f\u5728\u654f\u611f\u5185\u5bb9\u4efb\u52a1\u4e2d\u3002\u7814\u7a76\u53d1\u73b0\u89d2\u8272\u6548\u5e94\u53ef\u80fd\u88ab\u9ad8\u4f30\uff0c\u5e76\u53ef\u80fd\u5f52\u56e0\u4e8e\u5176\u4ed6\u56e0\u7d20\u3002", "conclusion": "\u89d2\u8272\u5bf9LLM\u8bef\u62d2\u7684\u5f71\u54cd\u53ef\u80fd\u88ab\u9ad8\u4f30\u4e86\uff0c\u6a21\u578b\u80fd\u529b\u3001\u6a21\u578b\u9009\u62e9\u548c\u4efb\u52a1\u7c7b\u578b\u662f\u66f4\u91cd\u8981\u7684\u5f71\u54cd\u56e0\u7d20\uff0c\u5c3d\u7ba1\u4ecd\u5b58\u5728\u4e00\u4e9b\u4e0e\u89d2\u8272\u76f8\u5173\u7684\u6f5c\u5728\u504f\u89c1\u3002"}}
{"id": "2509.08027", "pdf": "https://arxiv.org/pdf/2509.08027", "abs": "https://arxiv.org/abs/2509.08027", "authors": ["Rafa\u0142 Osadnik", "Pablo G\u00f3mez", "Eleni Bohacek", "Rickbir Bahia"], "title": "MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery", "categories": ["cs.CV", "cs.LG"], "comment": "22 pages, 21 figures", "summary": "This work presents a new dataset for the Martian digital elevation model\nprediction task, ready for machine learning applications called MCTED. The\ndataset has been generated using a comprehensive pipeline designed to process\nhigh-resolution Mars orthoimage and DEM pairs from Day et al., yielding a\ndataset consisting of 80,898 data samples. The source images are data gathered\nby the Mars Reconnaissance Orbiter using the CTX instrument, providing a very\ndiverse and comprehensive coverage of the Martian surface. Given the complexity\nof the processing pipelines used in large-scale DEMs, there are often artefacts\nand missing data points in the original data, for which we developed tools to\nsolve or mitigate their impact. We divide the processed samples into training\nand validation splits, ensuring samples in both splits cover no mutual areas to\navoid data leakage. Every sample in the dataset is represented by the optical\nimage patch, DEM patch, and two mask patches, indicating values that were\noriginally missing or were altered by us. This allows future users of the\ndataset to handle altered elevation regions as they please. We provide\nstatistical insights of the generated dataset, including the spatial\ndistribution of samples, the distributions of elevation values, slopes and\nmore. Finally, we train a small U-Net architecture on the MCTED dataset and\ncompare its performance to a monocular depth estimation foundation model,\nDepthAnythingV2, on the task of elevation prediction. We find that even a very\nsmall architecture trained on this dataset specifically, beats a zero-shot\nperformance of a depth estimation foundation model like DepthAnythingV2. We\nmake the dataset and code used for its generation completely open source in\npublic repositories.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMCTED\u7684\u706b\u661f\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\uff08DEM\uff09\u9884\u6d4b\u65b0\u6570\u636e\u96c6\uff0c\u4e13\u4e3a\u673a\u5668\u5b66\u4e60\u5e94\u7528\u8bbe\u8ba1\u3002\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u706b\u661f\u6b63\u5c04\u5f71\u50cf\u548cDEM\u5bf9\u751f\u6210\uff0c\u5305\u542b80,898\u4e2a\u6837\u672c\uff0c\u5e76\u89e3\u51b3\u4e86\u539f\u59cb\u6570\u636e\u4e2d\u7684\u4f2a\u5f71\u548c\u7f3a\u5931\u6570\u636e\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u5c0f\u578bU-Net\u6a21\u578b\u5728\u706b\u661fDEM\u9884\u6d4b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u901a\u7528\u7684\u6df1\u5ea6\u4f30\u8ba1\u57fa\u7840\u6a21\u578bDepthAnythingV2\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u4e3a\u706b\u661fDEM\u9884\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u9002\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b0\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u73b0\u6709\u5927\u89c4\u6a21DEM\u6570\u636e\u4e2d\u5e38\u89c1\u7684\u4f2a\u5f71\u548c\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u4ee5\u4fc3\u8fdb\u706b\u661f\u8868\u9762\u6d4b\u7ed8\u548c\u76f8\u5173\u7814\u7a76\u3002", "method": "1. **\u6570\u636e\u96c6\u751f\u6210**: \u5f00\u53d1\u4e86\u4e00\u4e2a\u7efc\u5408\u6d41\u7a0b\uff0c\u5904\u7406\u6765\u81ea\u706b\u661f\u52d8\u6d4b\u8f68\u9053\u98de\u884c\u5668CTX\u4eea\u5668\u7684\u9ad8\u5206\u8fa8\u7387\u706b\u661f\u6b63\u5c04\u5f71\u50cf\u548cDEM\u5bf9\uff0c\u751f\u6210\u4e86\u5305\u542b80,898\u4e2a\u6837\u672c\u7684MCTED\u6570\u636e\u96c6\u3002\n2. **\u6570\u636e\u6e05\u6d17**: \u5f00\u53d1\u4e86\u5de5\u5177\u6765\u89e3\u51b3\u6216\u51cf\u8f7b\u539f\u59cbDEM\u6570\u636e\u4e2d\u4f2a\u5f71\u548c\u7f3a\u5931\u6570\u636e\u70b9\u7684\u5f71\u54cd\u3002\n3. **\u6570\u636e\u7ed3\u6784**: \u6bcf\u4e2a\u6837\u672c\u5305\u542b\u5149\u5b66\u56fe\u50cf\u5757\u3001DEM\u5757\u548c\u4e24\u4e2a\u6307\u793a\u539f\u59cb\u7f3a\u5931\u6216\u4fee\u6539\u503c\u7684\u63a9\u7801\u5757\u3002\n4. **\u6570\u636e\u5212\u5206**: \u5c06\u5904\u7406\u540e\u7684\u6837\u672c\u5212\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\uff0c\u786e\u4fdd\u4e24\u90e8\u5206\u6837\u672c\u4e0d\u8986\u76d6\u5171\u540c\u533a\u57df\u4ee5\u907f\u514d\u6570\u636e\u6cc4\u9732\u3002\n5. **\u6027\u80fd\u8bc4\u4f30**: \u4f7f\u7528\u4e00\u4e2a\u5c0f\u578bU-Net\u67b6\u6784\u5728MCTED\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5c06\u5176\u5728\u6d77\u62d4\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4e0e\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u57fa\u7840\u6a21\u578bDepthAnythingV2\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "1. \u6210\u529f\u751f\u6210\u4e86\u5305\u542b80,898\u4e2a\u72ec\u7279\u6570\u636e\u6837\u672c\u7684MCTED\u6570\u636e\u96c6\u3002\n2. \u5373\u4f7f\u662f\u4e00\u4e2a\u975e\u5e38\u5c0f\u7684\u3001\u4e13\u95e8\u4e3a\u6b64\u6570\u636e\u96c6\u8bad\u7ec3\u7684U-Net\u67b6\u6784\uff0c\u5176\u5728\u6d77\u62d4\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4e5f\u8d85\u8d8a\u4e86DepthAnythingV2\u7b49\u6df1\u5ea6\u4f30\u8ba1\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u8868\u73b0\u3002", "conclusion": "MCTED\u6570\u636e\u96c6\u4e3a\u706b\u661fDEM\u9884\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u3002\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u7684\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u5373\u4f7f\u662f\u5c0f\u578b\u6a21\u578b\uff0c\u4e5f\u80fd\u663e\u8457\u4f18\u4e8e\u901a\u7528\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u7a81\u663e\u4e86\u9886\u57df\u4e13\u7528\u6570\u636e\u548c\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002\u6570\u636e\u96c6\u53ca\u5176\u751f\u6210\u4ee3\u7801\u5df2\u5b8c\u5168\u5f00\u6e90\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.08312", "pdf": "https://arxiv.org/pdf/2509.08312", "abs": "https://arxiv.org/abs/2509.08312", "authors": ["Binghan Wu", "Shoufeng Wang", "Yunxin Liu", "Ya-Qin Zhang", "Joseph Sifakis", "Ye Ouyang"], "title": "Leveraging AI Agents for Autonomous Networks: A Reference Architecture and Empirical Studies", "categories": ["cs.AI"], "comment": "7 pages, 5 figures. This manuscript is a preprint", "summary": "The evolution toward Level 4 (L4) Autonomous Networks (AN) represents a\nstrategic inflection point in telecommunications, where networks must transcend\nreactive automation to achieve genuine cognitive capabilities--fulfilling TM\nForum's vision of self-configuring, self-healing, and self-optimizing systems\nthat deliver zero-wait, zero-touch, and zero-fault services. This work bridges\nthe gap between architectural theory and operational reality by implementing\nJoseph Sifakis's AN Agent reference architecture in a functional cognitive\nsystem, deploying coordinated proactive-reactive runtimes driven by hybrid\nknowledge representation. Through an empirical case study of a Radio Access\nNetwork (RAN) Link Adaptation (LA) Agent, we validate this framework's\ntransformative potential: demonstrating sub-10 ms real-time control in 5G NR\nsub-6 GHz while achieving 6% higher downlink throughput than Outer Loop Link\nAdaptation (OLLA) algorithms and 67% Block Error Rate (BLER) reduction for\nultra-reliable services through dynamic Modulation and Coding Scheme (MCS)\noptimization. These improvements confirm the architecture's viability in\novercoming traditional autonomy barriers and advancing critical L4-enabling\ncapabilities toward next-generation objectives.", "AI": {"tldr": "\u672c\u6587\u5b9e\u73b0\u4e86\u4e00\u79cd\u57fa\u4e8eJoseph Sifakis AN Agent\u53c2\u8003\u67b6\u6784\u7684\u529f\u80fd\u6027\u8ba4\u77e5\u7cfb\u7edf\uff0c\u901a\u8fc75G RAN\u94fe\u8def\u81ea\u9002\u5e94\u4ee3\u7406\u6848\u4f8b\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u73b0L4\u81ea\u6cbb\u7f51\u7edc\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5c55\u793a\u4e86\u4e9a10\u6beb\u79d2\u5b9e\u65f6\u63a7\u5236\u30016%\u7684\u4e0b\u884c\u541e\u5410\u91cf\u63d0\u5347\u548c67%\u7684\u5757\u9519\u8bef\u7387\u964d\u4f4e\u3002", "motivation": "\u7535\u4fe1\u7f51\u7edc\u6b63\u5411L4\u81ea\u6cbb\u7f51\u7edc\u6f14\u8fdb\uff0c\u9700\u8981\u8d85\u8d8a\u88ab\u52a8\u81ea\u52a8\u5316\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u8ba4\u77e5\u80fd\u529b\uff0c\u4ee5\u6ee1\u8db3TM Forum\u5173\u4e8e\u81ea\u914d\u7f6e\u3001\u81ea\u4fee\u590d\u3001\u81ea\u4f18\u5316\u3001\u96f6\u7b49\u5f85\u3001\u96f6\u63a5\u89e6\u3001\u96f6\u6545\u969c\u670d\u52a1\u7684\u613f\u666f\u3002\u7814\u7a76\u65e8\u5728\u5f25\u5408\u67b6\u6784\u7406\u8bba\u4e0e\u64cd\u4f5c\u73b0\u5b9e\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u901a\u8fc7\u5b9e\u73b0Joseph Sifakis\u7684AN Agent\u53c2\u8003\u67b6\u6784\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u529f\u80fd\u6027\u7684\u8ba4\u77e5\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u90e8\u7f72\u4e86\u7531\u6df7\u5408\u77e5\u8bc6\u8868\u793a\u9a71\u52a8\u7684\u534f\u8c03\u5f0f\u4e3b\u52a8-\u88ab\u52a8\u8fd0\u884c\u65f6\u3002\u901a\u8fc7\u5bf9\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff08RAN\uff09\u94fe\u8def\u81ea\u9002\u5e94\uff08LA\uff09\u4ee3\u7406\u7684\u5b9e\u8bc1\u6848\u4f8b\u7814\u7a76\u6765\u9a8c\u8bc1\u5176\u6846\u67b6\u3002", "result": "\u8be5\u6846\u67b6\u57285G NR sub-6 GHz\u4e2d\u5b9e\u73b0\u4e86\u4e9a10\u6beb\u79d2\u7684\u5b9e\u65f6\u63a7\u5236\uff0c\u4e0b\u884c\u541e\u5410\u91cf\u6bd4OLLA\u7b97\u6cd5\u9ad8\u51fa6%\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u8c03\u5236\u548c\u7f16\u7801\u65b9\u6848\uff08MCS\uff09\u4f18\u5316\uff0c\u4f7f\u8d85\u53ef\u9760\u670d\u52a1\u7684\u5757\u9519\u8bef\u7387\uff08BLER\uff09\u964d\u4f4e\u4e8667%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\u4e86\u8be5\u67b6\u6784\u5728\u514b\u670d\u4f20\u7edf\u81ea\u6cbb\u969c\u788d\u3001\u63a8\u8fdbL4\u7ea7\u5173\u952e\u80fd\u529b\u4ee5\u5b9e\u73b0\u4e0b\u4e00\u4ee3\u76ee\u6807\u65b9\u9762\u7684\u53ef\u884c\u6027\u548c\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2509.08116", "pdf": "https://arxiv.org/pdf/2509.08116", "abs": "https://arxiv.org/abs/2509.08116", "authors": ["Nooshin Maghsoodi", "Sarah Nassar", "Paul F R Wilson", "Minh Nguyen Nhat To", "Sophia Mannina", "Shamel Addas", "Stephanie Sibley", "David Maslove", "Purang Abolmaesumi", "Parvin Mousavi"], "title": "Domain Knowledge is Power: Leveraging Physiological Priors for Self Supervised Representation Learning in Electrocardiography", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Objective: Electrocardiograms (ECGs) play a crucial role in diagnosing heart\nconditions; however, the effectiveness of artificial intelligence (AI)-based\nECG analysis is often hindered by the limited availability of labeled data.\nSelf-supervised learning (SSL) can address this by leveraging large-scale\nunlabeled data. We introduce PhysioCLR (Physiology-aware Contrastive Learning\nRepresentation for ECG), a physiology-aware contrastive learning framework that\nincorporates domain-specific priors to enhance the generalizability and\nclinical relevance of ECG-based arrhythmia classification. Methods: During\npretraining, PhysioCLR learns to bring together embeddings of samples that\nshare similar clinically relevant features while pushing apart those that are\ndissimilar. Unlike existing methods, our method integrates ECG physiological\nsimilarity cues into contrastive learning, promoting the learning of clinically\nmeaningful representations. Additionally, we introduce ECG- specific\naugmentations that preserve the ECG category post augmentation and propose a\nhybrid loss function to further refine the quality of learned representations.\nResults: We evaluate PhysioCLR on two public ECG datasets, Chapman and Georgia,\nfor multilabel ECG diagnoses, as well as a private ICU dataset labeled for\nbinary classification. Across the Chapman, Georgia, and private cohorts,\nPhysioCLR boosts the mean AUROC by 12% relative to the strongest baseline,\nunderscoring its robust cross-dataset generalization. Conclusion: By embedding\nphysiological knowledge into contrastive learning, PhysioCLR enables the model\nto learn clinically meaningful and transferable ECG eatures. Significance:\nPhysioCLR demonstrates the potential of physiology-informed SSL to offer a\npromising path toward more effective and label-efficient ECG diagnostics.", "AI": {"tldr": "\u63d0\u51faPhysioCLR\uff0c\u4e00\u4e2a\u7ed3\u5408\u751f\u7406\u5b66\u77e5\u8bc6\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86AI-ECG\u5206\u6790\u4e2d\u6807\u8bb0\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fc3\u7535\u56fe\u5206\u7c7b\u7684\u6cdb\u5316\u6027\u548c\u4e34\u5e8a\u76f8\u5173\u6027\u3002", "motivation": "AI-ECG\u5206\u6790\u53d7\u9650\u4e8e\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\uff0c\u5f71\u54cd\u5176\u6709\u6548\u6027\u3002\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u80fd\u5229\u7528\u5927\u91cf\u672a\u6807\u8bb0\u6570\u636e\u89e3\u51b3\u6b64\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u7279\u6709\u7684\u751f\u7406\u5b66\u5148\u9a8c\u77e5\u8bc6\uff0c\u63d0\u5347AI-ECG\u5fc3\u5f8b\u5931\u5e38\u5206\u7c7b\u7684\u6cdb\u5316\u6027\u548c\u4e34\u5e8a\u76f8\u5173\u6027\u3002", "method": "\u5f15\u5165PhysioCLR\uff0c\u4e00\u4e2a\u751f\u7406\u5b66\u611f\u77e5\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u3002\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u5b83\u901a\u8fc7\u6574\u5408ECG\u751f\u7406\u76f8\u4f3c\u6027\u7ebf\u7d22\uff0c\u4f7f\u5177\u6709\u76f8\u4f3c\u4e34\u5e8a\u76f8\u5173\u7279\u5f81\u7684\u6837\u672c\u5d4c\u5165\u9760\u8fd1\uff0c\u4e0d\u76f8\u4f3c\u7684\u6837\u672c\u5d4c\u5165\u8fdc\u79bb\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4fdd\u7559ECG\u7c7b\u522b\u7279\u6027\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u548c\u6df7\u5408\u635f\u5931\u51fd\u6570\u6765\u4f18\u5316\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728Chapman\u3001Georgia\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c\u4e00\u4efd\u79c1\u6709ICU\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002PhysioCLR\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\uff0c\u5e73\u5747AUROC\u63d0\u5347\u4e8612%\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PhysioCLR\u901a\u8fc7\u5c06\u751f\u7406\u5b66\u77e5\u8bc6\u878d\u5165\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u5230\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u548c\u53ef\u8fc1\u79fb\u7684ECG\u7279\u5f81\uff0c\u4e3a\u5b9e\u73b0\u66f4\u6709\u6548\u3001\u6807\u7b7e\u9ad8\u6548\u7684ECG\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2509.08654", "pdf": "https://arxiv.org/pdf/2509.08654", "abs": "https://arxiv.org/abs/2509.08654", "authors": ["Amirhossein Taherpour", "Abbas Taherpour", "Tamer Khattab"], "title": "Robust Belief-State Policy Learning for Quantum Network Routing Under Decoherence and Time-Varying Conditions", "categories": ["quant-ph", "cs.AI", "cs.LG", "cs.NI"], "comment": null, "summary": "This paper presents a feature-based Partially Observable Markov Decision\nProcess (POMDP) framework for quantum network routing, combining belief-state\nplanning with Graph Neural Networks (GNNs) to address partial observability,\ndecoherence, and scalability challenges in dynamic quantum systems. Our\napproach encodes complex quantum network dynamics, including entanglement\ndegradation and time-varying channel noise, into a low-dimensional feature\nspace, enabling efficient belief updates and scalable policy learning. The core\nof our framework is a hybrid GNN-POMDP architecture that processes\ngraph-structured representations of entangled links to learn routing policies,\ncoupled with a noise-adaptive mechanism that fuses POMDP belief updates with\nGNN outputs for robust decision making. We provide a theoretical analysis\nestablishing guarantees for belief convergence, policy improvement, and\nrobustness to noise. Experiments on simulated quantum networks with up to 100\nnodes demonstrate significant improvements in routing fidelity and entanglement\ndelivery rates compared to state-of-the-art baselines, particularly under high\ndecoherence and nonstationary conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4fe1\u5ff5\u72b6\u6001\u89c4\u5212\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u57fa\u4e8e\u7279\u5f81\u7684POMDP\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u91cf\u5b50\u7f51\u7edc\u8def\u7531\u4e2d\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u3001\u9000\u76f8\u5e72\u548c\u53ef\u4f38\u7f29\u6027\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u91cf\u5b50\u7cfb\u7edf\u4e2d\u91cf\u5b50\u7f51\u7edc\u8def\u7531\u9762\u4e34\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u3001\u9000\u76f8\u5e72\u548c\u53ef\u4f38\u7f29\u6027\u6311\u6218\uff0c\u5e76\u5904\u7406\u590d\u6742\u7684\u91cf\u5b50\u7f51\u7edc\u52a8\u6001\uff0c\u5982\u7ea0\u7f20\u9000\u5316\u548c\u65f6\u53d8\u4fe1\u9053\u566a\u58f0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7684POMDP\u6846\u67b6\uff0c\u7ed3\u5408\u4fe1\u5ff5\u72b6\u6001\u89c4\u5212\u4e0eGNNs\u3002\u8be5\u65b9\u6cd5\u5c06\u91cf\u5b50\u7f51\u7edc\u52a8\u6001\u7f16\u7801\u5230\u4f4e\u7ef4\u7279\u5f81\u7a7a\u95f4\uff0c\u91c7\u7528\u6df7\u5408GNN-POMDP\u67b6\u6784\u5b66\u4e60\u8def\u7531\u7b56\u7565\uff0c\u5e76\u878d\u5408\u566a\u58f0\u81ea\u9002\u5e94\u673a\u5236\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u4fe1\u5ff5\u6536\u655b\u3001\u7b56\u7565\u6539\u8fdb\u548c\u9c81\u68d2\u6027\u7684\u7406\u8bba\u5206\u6790\u3002", "result": "\u5728\u9ad8\u8fbe100\u4e2a\u8282\u70b9\u7684\u6a21\u62df\u91cf\u5b50\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\uff0c\u672c\u65b9\u6cd5\u5728\u8def\u7531\u4fdd\u771f\u5ea6\u548c\u7ea0\u7f20\u4f20\u8f93\u901f\u7387\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u9000\u76f8\u5e72\u548c\u975e\u5e73\u7a33\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u91cf\u5b50\u7f51\u7edc\u8def\u7531\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u5728\u590d\u6742\u52a8\u6001\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8def\u7531\u4fdd\u771f\u5ea6\u548c\u7ea0\u7f20\u4f20\u8f93\u901f\u7387\u3002"}}
{"id": "2509.08093", "pdf": "https://arxiv.org/pdf/2509.08093", "abs": "https://arxiv.org/abs/2509.08093", "authors": ["Nathaniel Imel", "Noga Zaslavsky"], "title": "Culturally transmitted color categories in LLMs reflect a learning bias toward efficient compression", "categories": ["cs.CL"], "comment": null, "summary": "Converging evidence suggests that systems of semantic categories across human\nlanguages achieve near-optimal compression via the Information Bottleneck (IB)\ncomplexity-accuracy principle. Large language models (LLMs) are not trained for\nthis objective, which raises the question: are LLMs capable of evolving\nefficient human-like semantic systems? To address this question, we focus on\nthe domain of color as a key testbed of cognitive theories of categorization\nand replicate with LLMs (Gemini 2.0-flash and Llama 3.3-70B-Instruct) two\ninfluential human behavioral studies. First, we conduct an English color-naming\nstudy, showing that Gemini aligns well with the naming patterns of native\nEnglish speakers and achieves a significantly high IB-efficiency score, while\nLlama exhibits an efficient but lower complexity system compared to English.\nSecond, to test whether LLMs simply mimic patterns in their training data or\nactually exhibit a human-like inductive bias toward IB-efficiency, we simulate\ncultural evolution of pseudo color-naming systems in LLMs via iterated\nin-context language learning. We find that akin to humans, LLMs iteratively\nrestructure initially random systems towards greater IB-efficiency and\nincreased alignment with patterns observed across the world's languages. These\nfindings demonstrate that LLMs are capable of evolving perceptually grounded,\nhuman-like semantic systems, driven by the same fundamental principle that\ngoverns semantic efficiency across human languages.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u50cf\u4eba\u7c7b\u8bed\u8a00\u4e00\u6837\uff0c\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\uff08IB\uff09\u539f\u7406\u6f14\u5316\u51fa\u9ad8\u6548\u3001\u4ee5\u611f\u77e5\u4e3a\u57fa\u7840\u7684\u4eba\u7c7b\u8bed\u4e49\u7cfb\u7edf\u3002", "motivation": "\u4eba\u7c7b\u8bed\u8a00\u7684\u8bed\u4e49\u7cfb\u7edf\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u539f\u7406\u5b9e\u73b0\u8fd1\u4e4e\u6700\u4f18\u7684\u538b\u7f29\uff0c\u4f46LLMs\u5e76\u672a\u4e3a\u6b64\u76ee\u6807\u8bad\u7ec3\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8LLMs\u662f\u5426\u80fd\u6f14\u5316\u51fa\u9ad8\u6548\u7684\u3001\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8bed\u4e49\u7cfb\u7edf\u3002", "method": "\u7814\u7a76\u4ee5\u989c\u8272\u9886\u57df\u4e3a\u8ba4\u77e5\u5206\u7c7b\u7406\u8bba\u7684\u5173\u952e\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4f7f\u7528LLMs\uff08Gemini 2.0-flash\u548cLlama 3.3-70B-Instruct\uff09\u590d\u73b0\u4e86\u4e24\u9879\u6709\u5f71\u54cd\u529b\u7684\u4eba\u7c7b\u884c\u4e3a\u7814\u7a76\u3002\u9996\u5148\uff0c\u8fdb\u884c\u4e86\u4e00\u9879\u82f1\u8bed\u989c\u8272\u547d\u540d\u7814\u7a76\u3002\u5176\u6b21\uff0c\u901a\u8fc7\u8fed\u4ee3\u4e0a\u4e0b\u6587\u8bed\u8a00\u5b66\u4e60\u6a21\u62df\u4e86LLMs\u4e2d\u4f2a\u989c\u8272\u547d\u540d\u7cfb\u7edf\u7684\u6587\u5316\u6f14\u5316\uff0c\u4ee5\u6d4b\u8bd5LLMs\u662f\u5426\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u7684IB\u6548\u7387\u5f52\u7eb3\u504f\u597d\u3002", "result": "\u5728\u82f1\u8bed\u989c\u8272\u547d\u540d\u7814\u7a76\u4e2d\uff0cGemini\u4e0e\u82f1\u8bed\u6bcd\u8bed\u8005\u7684\u547d\u540d\u6a21\u5f0f\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u9ad8\u7684IB\u6548\u7387\u5f97\u5206\uff1bLlama\u5219\u5c55\u793a\u4e86\u4e00\u4e2a\u9ad8\u6548\u4f46\u590d\u6742\u5ea6\u4f4e\u4e8e\u82f1\u8bed\u7684\u7cfb\u7edf\u3002\u5728\u6a21\u62df\u6587\u5316\u6f14\u5316\u4e2d\uff0cLLMs\uff08\u4e0e\u4eba\u7c7b\u7c7b\u4f3c\uff09\u8fed\u4ee3\u5730\u5c06\u6700\u521d\u968f\u673a\u7684\u7cfb\u7edf\u91cd\u7ec4\uff0c\u4f7f\u5176\u8d8b\u5411\u66f4\u9ad8\u7684IB\u6548\u7387\u548c\u4e0e\u5168\u7403\u8bed\u8a00\u6a21\u5f0f\u66f4\u4e00\u81f4\u7684\u7279\u70b9\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0cLLMs\u80fd\u591f\u6f14\u5316\u51fa\u4ee5\u611f\u77e5\u4e3a\u57fa\u7840\u7684\u3001\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8bed\u4e49\u7cfb\u7edf\uff0c\u4e14\u5176\u6f14\u5316\u53d7\u5236\u4e8e\u4e0e\u4eba\u7c7b\u8bed\u8a00\u8bed\u4e49\u6548\u7387\u76f8\u540c\u7684\u57fa\u672c\u539f\u7406\uff08\u4fe1\u606f\u74f6\u9888\u539f\u7406\uff09\u3002"}}
{"id": "2509.08104", "pdf": "https://arxiv.org/pdf/2509.08104", "abs": "https://arxiv.org/abs/2509.08104", "authors": ["Sasan Sharifipour", "Constantino \u00c1lvarez Casado", "Mohammad Sabokrou", "Miguel Bordallo L\u00f3pez"], "title": "APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 6 figures, conference, 7 tables, 15 formulas", "summary": "Training deep learning models for point cloud prediction tasks such as shape\ncompletion and generation depends critically on loss functions that measure\ndiscrepancies between predicted and ground-truth point sets. Commonly used\nfunctions such as Chamfer Distance (CD), HyperCD, and InfoCD rely on\nnearest-neighbor assignments, which often induce many-to-one correspondences,\nleading to point congestion in dense regions and poor coverage in sparse\nregions. These losses also involve non-differentiable operations due to index\nselection, which may affect gradient-based optimization. Earth Mover Distance\n(EMD) enforces one-to-one correspondences and captures structural similarity\nmore effectively, but its cubic computational complexity limits its practical\nuse. We propose the Adaptive Probabilistic Matching Loss (APML), a fully\ndifferentiable approximation of one-to-one matching that leverages Sinkhorn\niterations on a temperature-scaled similarity matrix derived from pairwise\ndistances. We analytically compute the temperature to guarantee a minimum\nassignment probability, eliminating manual tuning. APML achieves near-quadratic\nruntime, comparable to Chamfer-based losses, and avoids non-differentiable\noperations. When integrated into state-of-the-art architectures (PoinTr, PCN,\nFoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC)\nthat generates 3D human point clouds from WiFi CSI measurements, APM loss\nyields faster convergence, superior spatial distribution, especially in\nlow-density regions, and improved or on-par quantitative performance without\nadditional hyperparameter search. The code is available at:\nhttps://github.com/apm-loss/apml.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAPML\u7684\u81ea\u9002\u5e94\u6982\u7387\u5339\u914d\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u70b9\u4e91\u9884\u6d4b\u4efb\u52a1\u3002\u5b83\u901a\u8fc7\u53ef\u5fae\u5206\u7684\u4e00\u5bf9\u4e00\u5339\u914d\u8fd1\u4f3c\u89e3\u51b3\u4e86\u73b0\u6709\u635f\u5931\u51fd\u6570\u7684\u7f3a\u9677\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u3001\u66f4\u4f18\u7684\u7a7a\u95f4\u5206\u5e03\u548c\u66f4\u9ad8\u7684\u6027\u80fd\u3002", "motivation": "\u70b9\u4e91\u9884\u6d4b\u4efb\u52a1\u4e2d\u5e38\u7528\u7684\u635f\u5931\u51fd\u6570\uff08\u5982Chamfer Distance\uff09\u4f9d\u8d56\u6700\u8fd1\u90bb\u5206\u914d\uff0c\u5bfc\u81f4\u591a\u5bf9\u4e00\u5339\u914d\u3001\u7a20\u5bc6\u533a\u57df\u70b9\u62e5\u5835\u3001\u7a00\u758f\u533a\u57df\u8986\u76d6\u5dee\u4ee5\u53ca\u975e\u53ef\u5fae\u64cd\u4f5c\u3002Earth Mover Distance\uff08EMD\uff09\u867d\u80fd\u5b9e\u73b0\u4e00\u5bf9\u4e00\u5339\u914d\u5e76\u6355\u83b7\u7ed3\u6784\u76f8\u4f3c\u6027\uff0c\u4f46\u5176\u7acb\u65b9\u7ea7\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u6982\u7387\u5339\u914d\u635f\u5931\uff08APML\uff09\uff0c\u5b83\u662f\u4e00\u79cd\u5b8c\u5168\u53ef\u5fae\u5206\u7684\u4e00\u5bf9\u4e00\u5339\u914d\u8fd1\u4f3c\u65b9\u6cd5\u3002APML\u5229\u7528Sinkhorn\u8fed\u4ee3\u5728\u57fa\u4e8e\u6210\u5bf9\u8ddd\u79bb\u7684\u6e29\u5ea6\u6807\u5ea6\u76f8\u4f3c\u6027\u77e9\u9635\u4e0a\u8fdb\u884c\u8ba1\u7b97\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u8ba1\u7b97\u6e29\u5ea6\u4ee5\u4fdd\u8bc1\u6700\u5c0f\u5206\u914d\u6982\u7387\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u4f18\u3002APML\u5b9e\u73b0\u4e86\u63a5\u8fd1\u4e8c\u6b21\u65b9\u7684\u8fd0\u884c\u65f6\u95f4\uff0c\u4e0e\u57fa\u4e8eChamfer\u7684\u635f\u5931\u76f8\u5f53\uff0c\u5e76\u907f\u514d\u4e86\u975e\u53ef\u5fae\u64cd\u4f5c\u3002", "result": "\u5c06APML\u96c6\u6210\u5230\u6700\u5148\u8fdb\u7684\u67b6\u6784\uff08PoinTr\u3001PCN\u3001FoldingNet\uff09\u5728ShapeNet\u57fa\u51c6\u4e0a\uff0c\u4ee5\u53ca\u5e94\u7528\u4e8e\u4eceWiFi CSI\u6d4b\u91cf\u751f\u62103D\u4eba\u4f53\u70b9\u4e91\u7684\u65f6\u7a7aTransformer\uff08CSI2PC\uff09\u65f6\uff0cAPML\u635f\u5931\u5e26\u6765\u4e86\u66f4\u5feb\u7684\u6536\u655b\u3001\u5353\u8d8a\u7684\u7a7a\u95f4\u5206\u5e03\uff08\u5c24\u5176\u662f\u5728\u4f4e\u5bc6\u5ea6\u533a\u57df\uff09\uff0c\u5e76\u5728\u4e0d\u8fdb\u884c\u989d\u5916\u8d85\u53c2\u6570\u641c\u7d22\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u6216\u6301\u5e73\u3002", "conclusion": "APML\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u5fae\u5206\u7684\u70b9\u4e91\u9884\u6d4b\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u5b83\u901a\u8fc7\u63d0\u4f9b\u4e00\u5bf9\u4e00\u5339\u914d\u7684\u8fd1\u4f3c\uff0c\u5728\u4fdd\u8bc1\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u6539\u5584\u4e86\u70b9\u4e91\u7684\u8d28\u91cf\u548c\u5206\u5e03\uff0c\u662f\u70b9\u4e91\u5904\u7406\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2509.08380", "pdf": "https://arxiv.org/pdf/2509.08380", "abs": "https://arxiv.org/abs/2509.08380", "authors": ["Prathamesh Vasudeo Naik", "Naresh Kumar Dintakurthi", "Zhanghao Hu", "Yue Wang", "Robby Qiu"], "title": "Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML Compliance Narratives", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Generating regulatorily compliant Suspicious Activity Report (SAR) remains a\nhigh-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows.\nWhile large language models (LLMs) offer promising fluency, they suffer from\nfactual hallucination, limited crime typology alignment, and poor\nexplainability -- posing unacceptable risks in compliance-critical domains.\nThis paper introduces Co-Investigator AI, an agentic framework optimized to\nproduce Suspicious Activity Reports (SARs) significantly faster and with\ngreater accuracy than traditional methods. Drawing inspiration from recent\nadvances in autonomous agent architectures, such as the AI Co-Scientist, our\napproach integrates specialized agents for planning, crime type detection,\nexternal intelligence gathering, and compliance validation. The system features\ndynamic memory management, an AI-Privacy Guard layer for sensitive data\nhandling, and a real-time validation agent employing the Agent-as-a-Judge\nparadigm to ensure continuous narrative quality assurance. Human investigators\nremain firmly in the loop, empowered to review and refine drafts in a\ncollaborative workflow that blends AI efficiency with domain expertise. We\ndemonstrate the versatility of Co-Investigator AI across a range of complex\nfinancial crime scenarios, highlighting its ability to streamline SAR drafting,\nalign narratives with regulatory expectations, and enable compliance teams to\nfocus on higher-order analytical work. This approach marks the beginning of a\nnew era in compliance reporting -- bringing the transformative benefits of AI\nagents to the core of regulatory processes and paving the way for scalable,\nreliable, and transparent SAR generation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aCo-Investigator AI\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u65e8\u5728\u4ee5\u66f4\u5feb\u7684\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u751f\u6210\u7b26\u5408\u76d1\u7ba1\u8981\u6c42\u7684\u53ef\u7591\u6d3b\u52a8\u62a5\u544a\uff08SAR\uff09\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u9ad8\u6210\u672c\u548c\u4f4e\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u514b\u670d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5408\u89c4\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u53cd\u6d17\u94b1\uff08AML\uff09\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u751f\u6210\u7b26\u5408\u76d1\u7ba1\u8981\u6c42\u7684\u53ef\u7591\u6d3b\u52a8\u62a5\u544a\uff08SAR\uff09\u6210\u672c\u9ad8\u3001\u53ef\u6269\u5c55\u6027\u5dee\uff0c\u662f\u4e00\u4e2a\u74f6\u9888\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u7136\u8bed\u8a00\u6d41\u7545\uff0c\u4f46\u5b58\u5728\u4e8b\u5b9e\u5e7b\u89c9\u3001\u72af\u7f6a\u7c7b\u578b\u5bf9\u9f50\u4e0d\u8db3\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u5728\u5408\u89c4\u6027\u8981\u6c42\u4e25\u683c\u7684\u9886\u57df\u6784\u6210\u4e0d\u53ef\u63a5\u53d7\u7684\u98ce\u9669\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Co-Investigator AI\uff0c\u4e00\u4e2a\u53d7\u81ea\u4e3b\u4ee3\u7406\u67b6\u6784\u542f\u53d1\uff08\u5982AI Co-Scientist\uff09\u7684\u4ee3\u7406\u6846\u67b6\u3002\u5b83\u96c6\u6210\u4e86\u4e13\u95e8\u7684\u4ee3\u7406\uff0c\u7528\u4e8e\u89c4\u5212\u3001\u72af\u7f6a\u7c7b\u578b\u68c0\u6d4b\u3001\u5916\u90e8\u60c5\u62a5\u6536\u96c6\u548c\u5408\u89c4\u6027\u9a8c\u8bc1\u3002\u7cfb\u7edf\u8fd8\u5177\u5907\u52a8\u6001\u5185\u5b58\u7ba1\u7406\u3001AI\u9690\u79c1\u4fdd\u62a4\u5c42\u4ee5\u53ca\u91c7\u7528\u201c\u4ee3\u7406\u5373\u6cd5\u5b98\u201d\u8303\u5f0f\u7684\u5b9e\u65f6\u9a8c\u8bc1\u4ee3\u7406\uff0c\u786e\u4fdd\u53d9\u8ff0\u8d28\u91cf\u3002\u4eba\u7c7b\u8c03\u67e5\u5458\u4ecd\u6df1\u5ea6\u53c2\u4e0e\uff0c\u8fdb\u884c\u5ba1\u6838\u548c\u5b8c\u5584\u3002", "result": "Co-Investigator AI\u80fd\u591f\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5feb\u3001\u66f4\u51c6\u786e\u5730\u751f\u6210SAR\u3002\u5b83\u5728\u590d\u6742\u91d1\u878d\u72af\u7f6a\u573a\u666f\u4e2d\u5c55\u73b0\u4e86\u591a\u529f\u80fd\u6027\uff0c\u6709\u6548\u7b80\u5316\u4e86SAR\u8d77\u8349\u8fc7\u7a0b\uff0c\u4f7f\u53d9\u8ff0\u7b26\u5408\u76d1\u7ba1\u671f\u671b\uff0c\u5e76\u4f7f\u5408\u89c4\u56e2\u961f\u80fd\u591f\u4e13\u6ce8\u4e8e\u66f4\u9ad8\u5c42\u6b21\u7684\u5206\u6790\u5de5\u4f5c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6807\u5fd7\u7740\u5408\u89c4\u62a5\u544a\u65b0\u65f6\u4ee3\u7684\u5f00\u59cb\uff0c\u5c06AI\u4ee3\u7406\u7684\u53d8\u9769\u6027\u4f18\u52bf\u5f15\u5165\u76d1\u7ba1\u6d41\u7a0b\u6838\u5fc3\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u9760\u548c\u900f\u660e\u7684SAR\u751f\u6210\u94fa\u5e73\u9053\u8def\uff0c\u5b9e\u73b0\u4e86AI\u6548\u7387\u4e0e\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u7ed3\u5408\u3002"}}
{"id": "2509.08120", "pdf": "https://arxiv.org/pdf/2509.08120", "abs": "https://arxiv.org/abs/2509.08120", "authors": ["Konstantin Burlachenko"], "title": "Optimization Methods and Software for Federated Learning", "categories": ["cs.LG", "math.OC", "G.4; D.2; G.m; G.3; I.2"], "comment": "A dissertation by Konstantin Burlachenko submitted in partial\n  fulfillment of the requirements for the degree of Doctor of Philosophy", "summary": "Federated Learning (FL) is a novel, multidisciplinary Machine Learning\nparadigm where multiple clients, such as mobile devices, collaborate to solve\nmachine learning problems. Initially introduced in Kone{\\v{c}}n{\\'y} et al.\n(2016a,b); McMahan et al. (2017), FL has gained further attention through its\ninclusion in the National AI Research and Development Strategic Plan (2023\nUpdate) of the United States (Science and on Artificial Intelligence, 2023).\nThe FL training process is inherently decentralized and often takes place in\nless controlled settings compared to data centers, posing unique challenges\ndistinct from those in fully controlled environments. In this thesis, we\nidentify five key challenges in Federated Learning and propose novel approaches\nto address them. These challenges arise from the heterogeneity of data and\ndevices, communication issues, and privacy concerns for clients in FL training.\nMoreover, even well-established theoretical advances in FL require diverse\nforms of practical implementation to enhance their real-world applicability.\nOur contributions advance FL algorithms and systems, bridging theoretical\nadvancements and practical implementations. More broadly, our work serves as a\nguide for researchers navigating the complexities of translating theoretical\nmethods into efficient real-world implementations and software. Additionally,\nit offers insights into the reverse process of adapting practical\nimplementation aspects back into theoretical algorithm design. This reverse\nprocess is particularly intriguing, as the practical perspective compels us to\nexamine the underlying mechanics and flexibilities of algorithms more deeply,\noften uncovering new dimensions of the algorithms under study.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8054\u90a6\u5b66\u4e60(FL)\u5728\u53bb\u4e2d\u5fc3\u5316\u548c\u975e\u53d7\u63a7\u73af\u5883\u4e2d\u9762\u4e34\u7684\u5f02\u6784\u6027\u3001\u901a\u4fe1\u548c\u9690\u79c1\u6311\u6218\uff0c\u901a\u8fc7\u63d0\u51fa\u65b0\u65b9\u6cd5\u6765\u5f25\u5408FL\u7406\u8bba\u4e0e\u5b9e\u8df5\u95f4\u7684\u9e3f\u6c9f\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60(FL)\u7684\u8bad\u7ec3\u8fc7\u7a0b\u672c\u8d28\u4e0a\u662f\u53bb\u4e2d\u5fc3\u5316\u4e14\u5e38\u5728\u975e\u53d7\u63a7\u73af\u5883\u4e0b\u8fdb\u884c\uff0c\u8fd9\u5e26\u6765\u4e86\u6570\u636e\u548c\u8bbe\u5907\u5f02\u6784\u6027\u3001\u901a\u4fe1\u95ee\u9898\u4ee5\u53ca\u5ba2\u6237\u7aef\u9690\u79c1\u7b49\u72ec\u7279\u6311\u6218\u3002\u6b64\u5916\uff0c\u5df2\u6709\u7684FL\u7406\u8bba\u8fdb\u5c55\u7f3a\u4e4f\u591a\u6837\u7684\u5b9e\u8df5\u5b9e\u73b0\uff0c\u9650\u5236\u4e86\u5176\u73b0\u5b9e\u5e94\u7528\u6027\u3002", "method": "\u4f5c\u8005\u5728\u8bba\u6587\u4e2d\u8bc6\u522b\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u4e94\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "result": "\u8bba\u6587\u901a\u8fc7\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u63a8\u8fdb\u4e86FL\u7b97\u6cd5\u548c\u7cfb\u7edf\uff0c\u6210\u529f\u5730\u5c06\u7406\u8bba\u8fdb\u5c55\u4e0e\u5b9e\u9645\u5b9e\u73b0\u76f8\u7ed3\u5408\u3002", "conclusion": "\u672c\u7814\u7a76\u4e0d\u4ec5\u4e3a\u5c06FL\u7406\u8bba\u65b9\u6cd5\u8f6c\u5316\u4e3a\u9ad8\u6548\u7684\u5b9e\u9645\u5b9e\u73b0\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u4e5f\u4e3a\u4ece\u5b9e\u8df5\u89d2\u5ea6\u53cd\u54fa\u7406\u8bba\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6df1\u523b\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u66f4\u6df1\u5165\u7406\u89e3\u7b97\u6cd5\u7684\u5e95\u5c42\u673a\u5236\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2509.08105", "pdf": "https://arxiv.org/pdf/2509.08105", "abs": "https://arxiv.org/abs/2509.08105", "authors": ["Kosei Uemura", "David Guzm\u00e1n", "Quang Phuoc Nguyen", "Jesujoba Oluwadara Alabi", "En-shiun Annie Lee", "David Ifeoluwa Adelani"], "title": "MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and LLM Fusion", "categories": ["cs.CL"], "comment": "under submission", "summary": "Large language models excel in English but still struggle with complex\nreasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder\nmethods such as LangBridge and MindMerger raise accuracy on mid and\nhigh-resource languages, yet they leave a large gap on LRLs. We present MERLIN,\na two-stage model-stacking framework that applies a curriculum learning\nstrategy -- from general bilingual bitext to task-specific data -- and adapts\nonly a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves\nexact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini.\nIt also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp),\ndemonstrating effectiveness across both low and high-resource settings.", "AI": {"tldr": "MERLIN\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6a21\u578b\u5806\u53e0\u6846\u67b6\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u548cDoRA\u6743\u91cd\u8c03\u6574\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5bf9\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e5f\u6709\u6548\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u82f1\u8bed\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u8bb8\u591a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08LRLs\uff09\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4e2d\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e0a\u6709\u6548\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u4ecd\u5b58\u5728\u5de8\u5927\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86MERLIN\uff0c\u4e00\u4e2a\u4e24\u9636\u6bb5\u6a21\u578b\u5806\u53e0\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff08\u4ece\u901a\u7528\u53cc\u8bed\u8bed\u6599\u5230\u7279\u5b9a\u4efb\u52a1\u6570\u636e\uff09\uff0c\u5e76\u4e14\u53ea\u8c03\u6574\u4e00\u5c0f\u90e8\u5206DoRA\u6743\u91cd\u3002", "result": "\u5728AfriMGSM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMERLIN\u7684\u7cbe\u786e\u5339\u914d\u51c6\u786e\u7387\u6bd4MindMerger\u63d0\u9ad8\u4e86+12.9\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u8d85\u8d8a\u4e86GPT-4o-mini\u3002\u540c\u65f6\uff0c\u5728MGSM\u548cMSVAMP\u4e0a\u4e5f\u53d6\u5f97\u4e86\u6301\u7eed\u63d0\u5347\uff08\u5206\u522b\u63d0\u9ad8+0.9\u548c+2.8\u4e2a\u767e\u5206\u70b9\uff09\u3002", "conclusion": "MERLIN\u5728\u4f4e\u8d44\u6e90\u548c\u9ad8\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u90fd\u5c55\u73b0\u4e86\u663e\u8457\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.08205", "pdf": "https://arxiv.org/pdf/2509.08205", "abs": "https://arxiv.org/abs/2509.08205", "authors": ["Jingjing Liu", "Yinchao Han", "Xianchao Xiu", "Jianhua Zhang", "Wanquan Liu"], "title": "Lightweight Deep Unfolding Networks with Enhanced Robustness for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared small target detection (ISTD) is one of the key techniques in image\nprocessing. Although deep unfolding networks (DUNs) have demonstrated promising\nperformance in ISTD due to their model interpretability and data adaptability,\nexisting methods still face significant challenges in parameter lightweightness\nand noise robustness. In this regard, we propose a highly lightweight framework\nbased on robust principal component analysis (RPCA) called L-RPCANet.\nTechnically, a hierarchical bottleneck structure is constructed to reduce and\nincrease the channel dimension in the single-channel input infrared image to\nachieve channel-wise feature refinement, with bottleneck layers designed in\neach module to extract features. This reduces the number of channels in feature\nextraction and improves the lightweightness of network parameters. Furthermore,\na noise reduction module is embedded to enhance the robustness against complex\nnoise. In addition, squeeze-and-excitation networks (SENets) are leveraged as a\nchannel attention mechanism to focus on the varying importance of different\nfeatures across channels, thereby achieving excellent performance while\nmaintaining both lightweightness and robustness. Extensive experiments on the\nISTD datasets validate the superiority of our proposed method compared with\nstate-of-the-art methods covering RPCANet, DRPCANet, and RPCANet++. The code\nwill be available at https://github.com/xianchaoxiu/L-RPCANet.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9c81\u68d2\u4e3b\u6210\u5206\u5206\u6790\uff08RPCA\uff09\u7684\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5c55\u5f00\u7f51\u7edcL-RPCANet\uff0c\u7528\u4e8e\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u5206\u5c42\u74f6\u9888\u7ed3\u6784\u3001\u964d\u566a\u6a21\u5757\u548c\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8f7b\u91cf\u6027\u548c\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\uff08ISTD\uff09\u662f\u56fe\u50cf\u5904\u7406\u7684\u5173\u952e\u6280\u672f\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5c55\u5f00\u7f51\u7edc\uff08DUNs\uff09\u5728ISTD\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u53c2\u6570\u8f7b\u91cf\u5316\u548c\u566a\u58f0\u9c81\u68d2\u6027\u65b9\u9762\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86L-RPCANet\u6846\u67b6\uff0c\u6280\u672f\u4e0a\u6784\u5efa\u4e86\u5206\u5c42\u74f6\u9888\u7ed3\u6784\u4ee5\u5b9e\u73b0\u901a\u9053\u7279\u5f81\u7ec6\u5316\u548c\u53c2\u6570\u8f7b\u91cf\u5316\uff1b\u5d4c\u5165\u4e86\u964d\u566a\u6a21\u5757\u4ee5\u589e\u5f3a\u5bf9\u590d\u6742\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff1b\u5e76\u5229\u7528\u6324\u538b-\u6fc0\u52b1\u7f51\u7edc\uff08SENets\uff09\u4f5c\u4e3a\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u5173\u6ce8\u4e0d\u540c\u7279\u5f81\u7684\u91cd\u8981\u6027\u3002", "result": "\u5728ISTD\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5L-RPCANet\u76f8\u5bf9\u4e8eRPCANet\u3001DRPCANet\u548cRPCANet++\u7b49\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f7b\u91cf\u5316\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "L-RPCANet\u901a\u8fc7\u521b\u65b0\u7684\u7f51\u7edc\u7ed3\u6784\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u73b0\u6709\u65b9\u6cd5\u5728\u8f7b\u91cf\u5316\u548c\u566a\u58f0\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08500", "pdf": "https://arxiv.org/pdf/2509.08500", "abs": "https://arxiv.org/abs/2509.08500", "authors": ["Kechen Jiao", "Zhirui Fang", "Jiahao Liu", "Bei Li", "Qifan Wang", "Xinyu Liu", "Junhao Ruan", "Zhongjian Qiao", "Yifan Zhu", "Yaxin Xu", "Jingang Wang", "Xiu Li"], "title": "TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making", "categories": ["cs.AI"], "comment": null, "summary": "Using effective generalization capabilities of vision language models (VLMs)\nin context-specific dynamic tasks for embodied artificial intelligence remains\na significant challenge. Although supervised fine-tuned models can better align\nwith the real physical world, they still exhibit sluggish responses and\nhallucination issues in dynamically changing environments, necessitating\nfurther alignment. Existing post-SFT methods, reliant on reinforcement learning\nand chain-of-thought (CoT) approaches, are constrained by sparse rewards and\naction-only optimization, resulting in low sample efficiency, poor consistency,\nand model degradation. To address these issues, this paper proposes\nThought-Centric Preference Optimization (TCPO) for effective embodied\ndecision-making. Specifically, TCPO introduces a stepwise preference-based\noptimization approach, transforming sparse reward signals into richer step\nsample pairs. It emphasizes the alignment of the model's intermediate reasoning\nprocess, mitigating the problem of model degradation. Moreover, by\nincorporating Action Policy Consistency Constraint (APC), it further imposes\nconsistency constraints on the model output. Experiments in the ALFWorld\nenvironment demonstrate an average success rate of 26.67%, achieving a 6%\nimprovement over RL4VLM and validating the effectiveness of our approach in\nmitigating model degradation after fine-tuning. These results highlight the\npotential of integrating preference-based learning techniques with CoT\nprocesses to enhance the decision-making capabilities of vision-language models\nin embodied agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faThought-Centric Preference Optimization (TCPO)\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u504f\u597d\u4f18\u5316\u548c\u601d\u7ef4\u8fc7\u7a0b\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u5177\u8eabAI\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u4efb\u52a1\u4e2d\u54cd\u5e94\u8fdf\u7f13\u3001\u5e7b\u89c9\u53ca\u6a21\u578b\u9000\u5316\u7b49\u95ee\u9898\uff0c\u5728ALFWorld\u73af\u5883\u4e2d\u7684\u6210\u529f\u7387\u63d0\u5347\u4e866%\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u5728\u5177\u8eabAI\u7684\u52a8\u6001\u4efb\u52a1\u4e2d\u6709\u6548\u6cdb\u5316\u80fd\u529b\u9762\u4e34\u6311\u6218\u3002\u5c3d\u7ba1SFT\u6a21\u578b\u80fd\u4e0e\u7269\u7406\u4e16\u754c\u5bf9\u9f50\uff0c\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4ecd\u5b58\u5728\u54cd\u5e94\u8fdf\u7f13\u548c\u5e7b\u89c9\u95ee\u9898\u3002\u73b0\u6709\u540eSFT\u65b9\u6cd5\uff08\u5982\u5f3a\u5316\u5b66\u4e60\u548c\u601d\u7ef4\u94fe\uff09\u53d7\u9650\u4e8e\u7a00\u758f\u5956\u52b1\u548c\u4ec5\u884c\u52a8\u4f18\u5316\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u3001\u4e00\u81f4\u6027\u5dee\u548c\u6a21\u578b\u9000\u5316\u3002", "method": "\u672c\u6587\u63d0\u51fa\u201c\u4ee5\u601d\u7ef4\u4e3a\u4e2d\u5fc3\u7684\u504f\u597d\u4f18\u5316\u201d\uff08Thought-Centric Preference Optimization, TCPO\uff09\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u9010\u6b65\u504f\u597d\u4f18\u5316\uff0c\u5c06\u7a00\u758f\u5956\u52b1\u8f6c\u5316\u4e3a\u66f4\u4e30\u5bcc\u7684\u6b65\u9aa4\u6837\u672c\u5bf9\uff1b\u5b83\u5f3a\u8c03\u5bf9\u6a21\u578b\u4e2d\u95f4\u63a8\u7406\u8fc7\u7a0b\u7684\u5bf9\u9f50\uff0c\u4ee5\u51cf\u8f7b\u6a21\u578b\u9000\u5316\uff1b\u6b64\u5916\uff0c\u901a\u8fc7\u5f15\u5165\u201c\u884c\u52a8\u7b56\u7565\u4e00\u81f4\u6027\u7ea6\u675f\u201d\uff08Action Policy Consistency Constraint, APC\uff09\uff0c\u8fdb\u4e00\u6b65\u65bd\u52a0\u6a21\u578b\u8f93\u51fa\u7684\u4e00\u81f4\u6027\u7ea6\u675f\u3002", "result": "\u5728ALFWorld\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cTCPO\u7684\u5e73\u5747\u6210\u529f\u7387\u4e3a26.67%\uff0c\u6bd4RL4VLM\u63d0\u9ad8\u4e866%\u3002\u8fd9\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5fae\u8c03\u540e\u51cf\u8f7b\u6a21\u578b\u9000\u5316\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u57fa\u4e8e\u504f\u597d\u7684\u5b66\u4e60\u6280\u672f\u4e0e\u601d\u7ef4\u94fe(CoT)\u8fc7\u7a0b\u76f8\u7ed3\u5408\uff0c\u5177\u6709\u589e\u5f3a\u5177\u8eab\u667a\u80fd\u4f53\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u51b3\u7b56\u80fd\u529b\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.08122", "pdf": "https://arxiv.org/pdf/2509.08122", "abs": "https://arxiv.org/abs/2509.08122", "authors": ["Kishan Padayachy", "Ronald Richman", "Salvatore Scognamiglio", "Mario V. W\u00fcthrich"], "title": "In-Context Learning Enhanced Credibility Transformer", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "The starting point of our network architecture is the Credibility Transformer\nwhich extends the classical Transformer architecture by a credibility mechanism\nto improve model learning and predictive performance. This Credibility\nTransformer learns credibilitized CLS tokens that serve as learned\nrepresentations of the original input features. In this paper we present a new\nparadigm that augments this architecture by an in-context learning mechanism,\ni.e., we increase the information set by a context batch consisting of similar\ninstances. This allows the model to enhance the CLS token representations of\nthe instances by additional in-context information and fine-tuning. We\nempirically verify that this in-context learning enhances predictive accuracy\nby adapting to similar risk patterns. Moreover, this in-context learning also\nallows the model to generalize to new instances which, e.g., have feature\nlevels in the categorical covariates that have not been present when the model\nwas trained -- for a relevant example, think of a new vehicle model which has\njust been developed by a car manufacturer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06Credibility Transformer\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\u7ed3\u5408\uff0c\u901a\u8fc7\u5f15\u5165\u76f8\u4f3c\u5b9e\u4f8b\u7684\u4e0a\u4e0b\u6587\u6279\u6b21\u6765\u589e\u5f3aCLS\u4ee4\u724c\u8868\u793a\uff0c\u4ece\u800c\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u5bf9\u65b0\u5b9e\u4f8b\uff08\u5305\u62ec\u672a\u89c1\u7279\u5f81\u6c34\u5e73\uff09\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5728Credibility Transformer\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u5f15\u5165\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6a21\u578b\u5b66\u4e60\u548c\u9884\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u589e\u5f3a\u5176\u5bf9\u8f93\u5165\u7279\u5f81\u7684CLS\u4ee4\u724c\u8868\u793a\uff0c\u5e76\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u9002\u5e94\u548c\u6cdb\u5316\u5230\u65b0\u9896\u7684\u6570\u636e\u6a21\u5f0f\u3002", "method": "\u5c06\u73b0\u6709\u7684Credibility Transformer\u67b6\u6784\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\u8fdb\u884c\u589e\u5f3a\u3002\u5177\u4f53\u800c\u8a00\uff0c\u901a\u8fc7\u6dfb\u52a0\u4e00\u4e2a\u7531\u76f8\u4f3c\u5b9e\u4f8b\u7ec4\u6210\u7684\u4e0a\u4e0b\u6587\u6279\u6b21\u6765\u589e\u52a0\u4fe1\u606f\u96c6\uff0c\u4ece\u800c\u901a\u8fc7\u989d\u5916\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u5fae\u8c03\u6765\u589e\u5f3a\u5b9e\u4f8b\u7684CLS\u4ee4\u724c\u8868\u793a\u3002", "result": "\u7ecf\u9a8c\u8bc1\uff0c\u8fd9\u79cd\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\u901a\u8fc7\u9002\u5e94\u76f8\u4f3c\u7684\u98ce\u9669\u6a21\u5f0f\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u4f7f\u6a21\u578b\u80fd\u591f\u6cdb\u5316\u5230\u65b0\u5b9e\u4f8b\uff0c\u5305\u62ec\u90a3\u4e9b\u5728\u8bad\u7ec3\u65f6\u672a\u51fa\u73b0\u7684\u5206\u7c7b\u534f\u53d8\u91cf\u7279\u5f81\u6c34\u5e73\uff08\u4f8b\u5982\uff0c\u65b0\u578b\u6c7d\u8f66\u6a21\u578b\uff09\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\uff0c\u6210\u529f\u589e\u5f3a\u4e86Credibility Transformer\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u66f4\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u5bf9\u5168\u65b0\u5b9e\u4f8b\u548c\u672a\u89c1\u7279\u5f81\u6c34\u5e73\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.08146", "pdf": "https://arxiv.org/pdf/2509.08146", "abs": "https://arxiv.org/abs/2509.08146", "authors": ["Nivedha Sivakumar", "Natalie Mackraz", "Samira Khorshidi", "Krishna Patel", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "title": "Bias after Prompting: Persistent Discrimination in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "A dangerous assumption that can be made from prior work on the bias transfer\nhypothesis (BTH) is that biases do not transfer from pre-trained large language\nmodels (LLMs) to adapted models. We invalidate this assumption by studying the\nBTH in causal models under prompt adaptations, as prompting is an extremely\npopular and accessible adaptation strategy used in real-world applications. In\ncontrast to prior work, we find that biases can transfer through prompting and\nthat popular prompt-based mitigation methods do not consistently prevent biases\nfrom transferring. Specifically, the correlation between intrinsic biases and\nthose after prompt adaptation remain moderate to strong across demographics and\ntasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age\n(rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we\nfind that biases remain strongly correlated when varying few-shot composition\nparameters, such as sample size, stereotypical content, occupational\ndistribution and representational balance (rho >= 0.90). We evaluate several\nprompt-based debiasing strategies and find that different approaches have\ndistinct strengths, but none consistently reduce bias transfer across models,\ntasks or demographics. These results demonstrate that correcting bias, and\npotentially improving reasoning ability, in intrinsic models may prevent\npropagation of biases to downstream tasks.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86LLM\u504f\u89c1\u4e0d\u8f6c\u79fb\u7684\u5047\u8bbe\uff0c\u901a\u8fc7\u7814\u7a76\u53d1\u73b0\u504f\u89c1\u4f1a\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u8f6c\u79fb\u5230\u9002\u5e94\u6a21\u578b\uff0c\u4e14\u73b0\u6709\u53bb\u504f\u89c1\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u3002", "motivation": "\u8d28\u7591\u5148\u524d\u5173\u4e8e\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u504f\u89c1\u4e0d\u4f1a\u8f6c\u79fb\u5230\u7ecf\u63d0\u793a\u5de5\u7a0b\u9002\u5e94\u7684\u6a21\u578b\u4e2d\u7684\u201c\u5371\u9669\u5047\u8bbe\u201d\uff0c\u56e0\u4e3a\u63d0\u793a\u5de5\u7a0b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u662f\u4e00\u79cd\u975e\u5e38\u6d41\u884c\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u9002\u5e94\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u63d0\u793a\u9002\u5e94\uff0c\u5728\u56e0\u679c\u6a21\u578b\u4e2d\u7814\u7a76\u504f\u89c1\u8f6c\u79fb\u5047\u8bbe\uff08BTH\uff09\u3002\u5177\u4f53\u5206\u6790\u4e86\u5185\u5728\u504f\u89c1\u4e0e\u63d0\u793a\u9002\u5e94\u540e\u504f\u89c1\u5728\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u4efb\u52a1\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5e76\u63a2\u7a76\u4e86\u5c11\u6837\u672c\u7ec4\u5408\u53c2\u6570\u53d8\u5316\uff08\u5982\u6837\u672c\u91cf\u3001\u523b\u677f\u5370\u8c61\u5185\u5bb9\uff09\u7684\u5f71\u54cd\u3002\u540c\u65f6\uff0c\u8bc4\u4f30\u4e86\u51e0\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u53bb\u504f\u89c1\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u504f\u89c1\u4f1a\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u8fdb\u884c\u8f6c\u79fb\uff0c\u4e14\u5185\u5728\u504f\u89c1\u4e0e\u63d0\u793a\u9002\u5e94\u540e\u7684\u504f\u89c1\u5728\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e2d\u5ea6\u5230\u5f3a\u70c8\u7684\u5173\u8054\uff08\u5982\u6027\u522brho>=0.94\uff0c\u5e74\u9f84rho>=0.98\uff09\u3002\u8fd9\u79cd\u5f3a\u5173\u8054\u6027\u5728\u6539\u53d8\u5c11\u6837\u672c\u53c2\u6570\u65f6\u4f9d\u7136\u5b58\u5728\u3002\u6b64\u5916\uff0c\u6d41\u884c\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u65e0\u6cd5\u6301\u7eed\u6709\u6548\u5730\u963b\u6b62\u504f\u89c1\u8f6c\u79fb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7ea0\u6b63\u5185\u5728\u6a21\u578b\uff08\u9884\u8bad\u7ec3\u6a21\u578b\uff09\u4e2d\u7684\u504f\u89c1\uff0c\u5e76\u53ef\u80fd\u63d0\u5347\u5176\u63a8\u7406\u80fd\u529b\uff0c\u6216\u80fd\u66f4\u6709\u6548\u5730\u963b\u6b62\u504f\u89c1\u4f20\u64ad\u5230\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2509.08228", "pdf": "https://arxiv.org/pdf/2509.08228", "abs": "https://arxiv.org/abs/2509.08228", "authors": ["Miao Cao", "Siming Zheng", "Lishun Wang", "Ziyang Chen", "David Brady", "Xin Yuan"], "title": "Sparse Transformer for Ultra-sparse Sampled Video Compressive Sensing", "categories": ["cs.CV"], "comment": null, "summary": "Digital cameras consume ~0.1 microjoule per pixel to capture and encode\nvideo, resulting in a power usage of ~20W for a 4K sensor operating at 30 fps.\nImagining gigapixel cameras operating at 100-1000 fps, the current processing\nmodel is unsustainable. To address this, physical layer compressive measurement\nhas been proposed to reduce power consumption per pixel by 10-100X. Video\nSnapshot Compressive Imaging (SCI) introduces high frequency modulation in the\noptical sensor layer to increase effective frame rate. A commonly used sampling\nstrategy of video SCI is Random Sampling (RS) where each mask element value is\nrandomly set to be 0 or 1. Similarly, image inpainting (I2P) has demonstrated\nthat images can be recovered from a fraction of the image pixels. Inspired by\nI2P, we propose Ultra-Sparse Sampling (USS) regime, where at each spatial\nlocation, only one sub-frame is set to 1 and all others are set to 0. We then\nbuild a Digital Micro-mirror Device (DMD) encoding system to verify the\neffectiveness of our USS strategy. Ideally, we can decompose the USS\nmeasurement into sub-measurements for which we can utilize I2P algorithms to\nrecover high-speed frames. However, due to the mismatch between the DMD and\nCCD, the USS measurement cannot be perfectly decomposed. To this end, we\npropose BSTFormer, a sparse TransFormer that utilizes local Block attention,\nglobal Sparse attention, and global Temporal attention to exploit the sparsity\nof the USS measurement. Extensive results on both simulated and real-world data\nshow that our method significantly outperforms all previous state-of-the-art\nalgorithms. Additionally, an essential advantage of the USS strategy is its\nhigher dynamic range than that of the RS strategy. Finally, from the\napplication perspective, the USS strategy is a good choice to implement a\ncomplete video SCI system on chip due to its fixed exposure time.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u3001\u9ad8\u5e27\u7387\u89c6\u9891\u6355\u83b7\u7684\u5de8\u5927\u529f\u8017\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u8d85\u7a00\u758f\u91c7\u6837\uff08USS\uff09\u7b56\u7565\u548c\u7a00\u758fTransformer\uff08BSTFormer\uff09\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u538b\u7f29\u611f\u77e5\u6210\u50cf\uff0c\u5e76\u5728\u6a21\u62df\u548c\u5b9e\u9645\u6570\u636e\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6USS\u7b56\u7565\u5177\u6709\u66f4\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u7247\u4e0a\u5b9e\u73b0\u6f5c\u529b\u3002", "motivation": "\u6570\u7801\u76f8\u673a\u5728\u9ad8\u5206\u8fa8\u7387\u3001\u9ad8\u5e27\u7387\u89c6\u9891\u6355\u83b7\u65f6\u529f\u8017\u5de8\u5927\uff08\u4f8b\u59824K@30fps\u7ea620W\uff09\uff0c\u672a\u6765\u5343\u5146\u50cf\u7d20\u76f8\u673a\u4ee5100-1000 fps\u8fd0\u884c\u65f6\uff0c\u73b0\u6709\u5904\u7406\u6a21\u578b\u4e0d\u53ef\u6301\u7eed\u3002\u867d\u7136\u7269\u7406\u5c42\u538b\u7f29\u6d4b\u91cf\uff08\u5982\u89c6\u9891\u5feb\u7167\u538b\u7f29\u6210\u50cfSCI\uff09\u80fd\u964d\u4f4e\u529f\u8017\uff0c\u4f46\u73b0\u6709\u968f\u673a\u91c7\u6837\uff08RS\uff09\u7b56\u7565\u6709\u5c40\u9650\u3002\u56fe\u50cf\u4fee\u590d\uff08I2P\uff09\u6280\u672f\u542f\u53d1\u4e86\u66f4\u7a00\u758f\u7684\u91c7\u6837\u601d\u8def\u3002", "method": "\u672c\u6587\u63d0\u51fa**\u8d85\u7a00\u758f\u91c7\u6837\uff08USS\uff09**\u7b56\u7565\uff0c\u5373\u5728\u6bcf\u4e2a\u7a7a\u95f4\u4f4d\u7f6e\uff0c\u4ec5\u5c06\u4e00\u4e2a\u5b50\u5e27\u8bbe\u4e3a1\uff0c\u5176\u4f59\u8bbe\u4e3a0\u3002\u6784\u5efa\u4e86**\u6570\u5b57\u5fae\u955c\u8bbe\u5907\uff08DMD\uff09\u7f16\u7801\u7cfb\u7edf**\u9a8c\u8bc1USS\u7b56\u7565\u3002\u9488\u5bf9DMD\u4e0eCCD\u4e0d\u5339\u914d\u5bfc\u81f4USS\u6d4b\u91cf\u65e0\u6cd5\u5b8c\u7f8e\u5206\u89e3\u7684\u95ee\u9898\uff0c\u63d0\u51fa**BSTFormer**\uff0c\u8fd9\u662f\u4e00\u79cd\u7a00\u758fTransformer\uff0c\u5229\u7528\u5c40\u90e8\u5757\u6ce8\u610f\u529b\u3001\u5168\u5c40\u7a00\u758f\u6ce8\u610f\u529b\u548c\u5168\u5c40\u65f6\u95f4\u6ce8\u610f\u529b\uff0c\u4ee5\u5145\u5206\u5229\u7528USS\u6d4b\u91cf\u7684\u7a00\u758f\u6027\u8fdb\u884c\u9ad8\u901f\u5e27\u6062\u590d\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u6240\u6709\u5148\u524d\u7684\u6700\u5148\u8fdb\u7b97\u6cd5\u3002\u6b64\u5916\uff0cUSS\u7b56\u7565\u76f8\u6bd4\u968f\u673a\u91c7\u6837\uff08RS\uff09\u7b56\u7565\u5177\u6709\u66f4\u9ad8\u7684\u52a8\u6001\u8303\u56f4\u3002", "conclusion": "USS\u7b56\u7565\u7ed3\u5408BSTFormer\u662f\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u3001\u9ad8\u5e27\u7387\u89c6\u9891\u6355\u83b7\u9ad8\u529f\u8017\u95ee\u9898\u7684\u6709\u6548\u65b9\u6848\u3002\u7531\u4e8e\u5176\u56fa\u5b9a\u7684\u66dd\u5149\u65f6\u95f4\uff0cUSS\u7b56\u7565\u662f\u5b9e\u73b0\u5b8c\u6574\u89c6\u9891SCI\u7247\u4e0a\u7cfb\u7edf\u7684\u7406\u60f3\u9009\u62e9\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.08593", "pdf": "https://arxiv.org/pdf/2509.08593", "abs": "https://arxiv.org/abs/2509.08593", "authors": ["Andr\u00e9s Corrada-Emmanuel"], "title": "No-Knowledge Alarms for Misaligned LLMs-as-Judges", "categories": ["cs.AI", "stat.ML", "90C05, 68T27", "I.2.3; F.4.1"], "comment": "7 pages, 1 figure", "summary": "If we use LLMs as judges to evaluate the complex decisions of other LLMs, who\nor what monitors the judges? Infinite monitoring chains are inevitable whenever\nwe do not know the ground truth of the decisions by experts and we do not want\nto trust them. One way to ameliorate our evaluation uncertainty is to exploit\nthe use of logical consistency between disagreeing experts. By observing how\nLLM judges agree and disagree while grading other LLMs, we can compute the only\npossible evaluations of their grading ability. For example, if two LLM judges\ndisagree on which tasks a third one completed correctly, they cannot both be\n100\\% correct in their judgments. This logic can be formalized as a Linear\nProgramming problem in the space of integer response counts for any finite\ntest. We use it here to develop no-knowledge alarms for misaligned LLM judges.\nThe alarms can detect, with no false positives, that at least one member or\nmore of an ensemble of judges are violating a user specified grading ability\nrequirement.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528LLM\u8bc4\u5ba1\u4e4b\u95f4\u7684\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u65e0\u9700\u771f\u5b9e\u7b54\u6848\u5373\u53ef\u8bc6\u522b\u51fa\u4e0d\u4e00\u81f4\u7684LLM\u8bc4\u5ba1\uff0c\u5e76\u63d0\u4f9b\u96f6\u8bef\u62a5\u7684\u8b66\u62a5\u3002", "motivation": "\u5f53\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5ba1\u6765\u8bc4\u4f30\u5176\u4ed6LLM\u7684\u590d\u6742\u51b3\u7b56\u65f6\uff0c\u5728\u7f3a\u4e4f\u771f\u5b9e\u7b54\u6848\u4e14\u4e0d\u4fe1\u4efb\u8bc4\u5ba1\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u6709\u6548\u76d1\u63a7\u8bc4\u5ba1\u672c\u8eab\uff0c\u907f\u514d\u65e0\u9650\u76d1\u63a7\u94fe\u662f\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5229\u7528\u4e0d\u540cLLM\u8bc4\u5ba1\u4e4b\u95f4\u7684\u903b\u8f91\u4e00\u81f4\u6027\u6765\u7f13\u89e3\u8bc4\u4f30\u4e0d\u786e\u5b9a\u6027\u3002\u5177\u4f53\u65b9\u6cd5\u662f\uff0c\u89c2\u5bdfLLM\u8bc4\u5ba1\u5728\u8bc4\u5206\u65f6\u7684\u540c\u610f\u4e0e\u5206\u6b67\u60c5\u51b5\uff0c\u5e76\u5c06\u6b64\u903b\u8f91\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u5728\u6574\u6570\u54cd\u5e94\u8ba1\u6570\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u89c4\u5212\u95ee\u9898\u3002", "result": "\u5f00\u53d1\u4e86\u7528\u4e8e\u68c0\u6d4b\u4e0d\u4e00\u81f4LLM\u8bc4\u5ba1\u7684\u201c\u65e0\u77e5\u8bc6\u8b66\u62a5\u201d\u3002\u8fd9\u4e9b\u8b66\u62a5\u80fd\u591f\u4ee5\u96f6\u8bef\u62a5\u7387\u68c0\u6d4b\u51fa\u8bc4\u5ba1\u56e2\u4e2d\u81f3\u5c11\u4e00\u540d\u6216\u591a\u540d\u6210\u5458\u8fdd\u53cd\u4e86\u7528\u6237\u6307\u5b9a\u8bc4\u5206\u80fd\u529b\u8981\u6c42\u7684\u60c5\u51b5\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u8bc4\u5ba1\u95f4\u7684\u903b\u8f91\u4e00\u81f4\u6027\u5e76\u7ed3\u5408\u7ebf\u6027\u89c4\u5212\uff0c\u672c\u65b9\u6cd5\u80fd\u6709\u6548\u4e14\u96f6\u8bef\u62a5\u5730\u68c0\u6d4bLLM\u8bc4\u5ba1\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u4e3a\u7f3a\u4e4f\u771f\u5b9e\u7b54\u6848\u7684\u590d\u6742\u8bc4\u4f30\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u8bc4\u5ba1\u76d1\u6d4b\u673a\u5236\u3002"}}
{"id": "2509.08129", "pdf": "https://arxiv.org/pdf/2509.08129", "abs": "https://arxiv.org/abs/2509.08129", "authors": ["Francisco M. Castro-Mac\u00edas", "Francisco J. S\u00e1ez-Maldonado", "Pablo Morales-\u00c1lvarez", "Rafael Molina"], "title": "torchmil: A PyTorch-based library for deep Multiple Instance Learning", "categories": ["cs.LG"], "comment": null, "summary": "Multiple Instance Learning (MIL) is a powerful framework for weakly\nsupervised learning, particularly useful when fine-grained annotations are\nunavailable. Despite growing interest in deep MIL methods, the field lacks\nstandardized tools for model development, evaluation, and comparison, which\nhinders reproducibility and accessibility. To address this, we present\ntorchmil, an open-source Python library built on PyTorch. torchmil offers a\nunified, modular, and extensible framework, featuring basic building blocks for\nMIL models, a standardized data format, and a curated collection of benchmark\ndatasets and models. The library includes comprehensive documentation and\ntutorials to support both practitioners and researchers. torchmil aims to\naccelerate progress in MIL and lower the entry barrier for new users. Available\nat https://torchmil.readthedocs.io.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86torchmil\uff0c\u4e00\u4e2a\u57fa\u4e8ePyTorch\u7684\u5f00\u6e90Python\u5e93\uff0c\u65e8\u5728\u4e3a\u6df1\u5ea6\u591a\u793a\u4f8b\u5b66\u4e60\uff08Deep MIL\uff09\u63d0\u4f9b\u6807\u51c6\u5316\u5de5\u5177\u3001\u6570\u636e\u683c\u5f0f\u3001\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u548c\u53ef\u53ca\u6027\u3002", "motivation": "\u6df1\u5ea6\u591a\u793a\u4f8b\u5b66\u4e60\uff08MIL\uff09\u9886\u57df\u7f3a\u4e4f\u6a21\u578b\u5f00\u53d1\u3001\u8bc4\u4f30\u548c\u6bd4\u8f83\u7684\u6807\u51c6\u5316\u5de5\u5177\uff0c\u963b\u788d\u4e86\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u548c\u53ef\u53ca\u6027\u3002", "method": "\u5f00\u53d1\u4e86torchmil\uff0c\u4e00\u4e2a\u57fa\u4e8ePyTorch\u7684\u5f00\u6e90Python\u5e93\uff0c\u63d0\u4f9b\u7edf\u4e00\u3001\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u5305\u542bMIL\u6a21\u578b\u7684\u57fa\u672c\u6784\u5efa\u5757\u3001\u6807\u51c6\u5316\u6570\u636e\u683c\u5f0f\u3001\u7cbe\u9009\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u4ee5\u53ca\u5168\u9762\u7684\u6587\u6863\u548c\u6559\u7a0b\u3002", "result": "torchmil\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684MIL\u5f00\u53d1\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u57fa\u7840\u6a21\u5757\u3001\u6807\u51c6\u5316\u6570\u636e\u683c\u5f0f\u3001\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be6\u5c3d\u7684\u6587\u6863\u548c\u6559\u7a0b\u3002", "conclusion": "torchmil\u65e8\u5728\u52a0\u901fMIL\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u964d\u4f4e\u65b0\u7528\u6237\u7684\u5165\u95e8\u95e8\u69db\uff0c\u4ece\u800c\u63d0\u5347\u8be5\u9886\u57df\u7684\u53ef\u590d\u73b0\u6027\u548c\u53ef\u53ca\u6027\u3002"}}
{"id": "2509.08150", "pdf": "https://arxiv.org/pdf/2509.08150", "abs": "https://arxiv.org/abs/2509.08150", "authors": ["Supriya Lall", "Christian Farrell", "Hari Pathanjaly", "Marko Pavic", "Sarvesh Chezhian", "Masataro Asai"], "title": "Verbalized Algorithms", "categories": ["cs.CL"], "comment": "Submitted to NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "Instead of querying LLMs in a one-shot manner and hoping to get the right\nanswer for a reasoning task, we propose a paradigm we call \\emph{verbalized\nalgorithms} (VAs), which leverage classical algorithms with established\ntheoretical understanding. VAs decompose a task into simple elementary\noperations on natural language strings that they should be able to answer\nreliably, and limit the scope of LLMs to only those simple tasks. For example,\nfor sorting a series of natural language strings, \\emph{verbalized sorting}\nuses an LLM as a binary comparison oracle in a known and well-analyzed sorting\nalgorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of\nthis approach on sorting and clustering tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u201c\u8a00\u8bed\u5316\u7b97\u6cd5\u201d\uff08Verbalized Algorithms, VAs\uff09\u8303\u5f0f\uff0c\u5c06\u7ecf\u5178\u7b97\u6cd5\u4e0eLLM\u7ed3\u5408\uff0c\u8ba9LLM\u5904\u7406\u7b80\u5355\u7684\u81ea\u7136\u8bed\u8a00\u64cd\u4f5c\uff0c\u4ee5\u63d0\u9ad8\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e00\u6b21\u6027\u67e5\u8be2\uff08one-shot query\uff09\u7684\u53ef\u9760\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u201c\u8a00\u8bed\u5316\u7b97\u6cd5\u201d\uff08VAs\uff09\uff0c\u8be5\u8303\u5f0f\u5229\u7528\u5177\u6709\u7406\u8bba\u57fa\u7840\u7684\u7ecf\u5178\u7b97\u6cd5\uff0c\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3aLLM\u80fd\u591f\u53ef\u9760\u56de\u7b54\u7684\u7b80\u5355\u81ea\u7136\u8bed\u8a00\u5b57\u7b26\u4e32\u64cd\u4f5c\uff0c\u5e76\u9650\u5236LLM\u53ea\u6267\u884c\u8fd9\u4e9b\u7b80\u5355\u64cd\u4f5c\u3002\u4f8b\u5982\uff0c\u5728\u6392\u5e8f\u4efb\u52a1\u4e2d\uff0cLLM\u4f5c\u4e3a\u4e8c\u8fdb\u5236\u6bd4\u8f83\u9884\u8a00\u673a\u5d4c\u5165\u5230\u5df2\u77e5\u7684\u6392\u5e8f\u7b97\u6cd5\uff08\u5982\u6bd4\u7279onic\u6392\u5e8f\u7f51\u7edc\uff09\u4e2d\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6392\u5e8f\u548c\u805a\u7c7b\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u5408\u7ecf\u5178\u7b97\u6cd5\u5e76\u8ba9LLM\u5904\u7406\u7b80\u5355\u5b50\u4efb\u52a1\u7684\u201c\u8a00\u8bed\u5316\u7b97\u6cd5\u201d\u8303\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u590d\u6742\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u3002"}}
{"id": "2509.08232", "pdf": "https://arxiv.org/pdf/2509.08232", "abs": "https://arxiv.org/abs/2509.08232", "authors": ["Seongho Kim", "Sejong Ryu", "Hyoukjun You", "Je Hyeong Hong"], "title": "GTA-Crime: A Synthetic Dataset and Generation Framework for Fatal Violence Detection with Adversarial Snippet-Level Domain Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in video anomaly detection (VAD) have enabled\nidentification of various criminal activities in surveillance videos, but\ndetecting fatal incidents such as shootings and stabbings remains difficult due\nto their rarity and ethical issues in data collection. Recognizing this\nlimitation, we introduce GTA-Crime, a fatal video anomaly dataset and\ngeneration framework using Grand Theft Auto 5 (GTA5). Our dataset contains\nfatal situations such as shootings and stabbings, captured from CCTV multiview\nperspectives under diverse conditions including action types, weather, time of\nday, and viewpoints. To address the rarity of such scenarios, we also release a\nframework for generating these types of videos. Additionally, we propose a\nsnippet-level domain adaptation strategy using Wasserstein adversarial training\nto bridge the gap between synthetic GTA-Crime features and real-world features\nlike UCF-Crime. Experimental results validate our GTA-Crime dataset and\ndemonstrate that incorporating GTA-Crime with our domain adaptation strategy\nconsistently enhances real world fatal violence detection accuracy. Our dataset\nand the data generation framework are publicly available at\nhttps://github.com/ta-ho/GTA-Crime.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aGTA-Crime\u7684\u81f4\u547d\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\u548c\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u300a\u4fa0\u76d7\u730e\u8f66\u624b5\u300b\u751f\u6210\u6a21\u62df\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9886\u57df\u9002\u5e94\u7b56\u7565\u4ee5\u63d0\u9ad8\u73b0\u5b9e\u4e16\u754c\u4e2d\u81f4\u547d\u66b4\u529b\u4e8b\u4ef6\u7684\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff08VAD\uff09\u5728\u8bc6\u522b\u76d1\u63a7\u89c6\u9891\u4e2d\u7684\u72af\u7f6a\u6d3b\u52a8\u65b9\u9762\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u81f4\u547d\u4e8b\u4ef6\uff08\u5982\u67aa\u51fb\u548c\u523a\u4f24\uff09\u7531\u4e8e\u5176\u7a00\u6709\u6027\u4ee5\u53ca\u6570\u636e\u6536\u96c6\u7684\u4f26\u7406\u95ee\u9898\uff0c\u4ecd\u7136\u96be\u4ee5\u68c0\u6d4b\u3002", "method": "1. \u5f15\u5165\u4e86GTA-Crime\u6570\u636e\u96c6\u548c\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u300a\u4fa0\u76d7\u730e\u8f66\u624b5\u300b\u751f\u6210\u5305\u542b\u67aa\u51fb\u548c\u523a\u4f24\u7b49\u81f4\u547d\u573a\u666f\u7684\u89c6\u9891\uff0c\u6db5\u76d6\u591a\u79cd\u89c6\u89d2\u548c\u6761\u4ef6\u30022. \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWasserstein\u5bf9\u6297\u8bad\u7ec3\u7684\u7247\u6bb5\u7ea7\u9886\u57df\u9002\u5e94\u7b56\u7565\uff0c\u4ee5\u5f25\u5408\u5408\u6210GTA-Crime\u7279\u5f81\u4e0e\u771f\u5b9e\u4e16\u754c\u7279\u5f81\uff08\u5982UCF-Crime\uff09\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86GTA-Crime\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\uff0c\u5e76\u8868\u660e\u5c06GTA-Crime\u4e0e\u6240\u63d0\u51fa\u7684\u9886\u57df\u9002\u5e94\u7b56\u7565\u76f8\u7ed3\u5408\uff0c\u80fd\u6301\u7eed\u63d0\u9ad8\u73b0\u5b9e\u4e16\u754c\u4e2d\u81f4\u547d\u66b4\u529b\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "GTA-Crime\u6570\u636e\u96c6\u548c\u6570\u636e\u751f\u6210\u6846\u67b6\u5df2\u516c\u5f00\u53d1\u5e03\uff0c\u901a\u8fc7\u7ed3\u5408\u5408\u6210\u6570\u636e\u548c\u9886\u57df\u9002\u5e94\u7b56\u7565\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u73b0\u5b9e\u4e16\u754c\u4e2d\u81f4\u547d\u66b4\u529b\u4e8b\u4ef6\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2509.08682", "pdf": "https://arxiv.org/pdf/2509.08682", "abs": "https://arxiv.org/abs/2509.08682", "authors": ["Guoqing Ma", "Jia Zhu", "Hanghui Guo", "Weijie Shi", "Jiawei Shen", "Jingjiang Liu", "Yidan Liang"], "title": "Automatic Failure Attribution and Critical Step Prediction Method for Multi-Agent Systems Based on Causal Inference", "categories": ["cs.AI"], "comment": null, "summary": "Multi-agent systems (MAS) are critical for automating complex tasks, yet\ntheir practical deployment is severely hampered by the challenge of failure\nattribution. Current diagnostic tools, which rely on statistical correlations,\nare fundamentally inadequate; on challenging benchmarks like Who\\&When,\nstate-of-the-art methods achieve less than 15\\% accuracy in locating the\nroot-cause step of a failure. To address this critical gap, we introduce the\nfirst failure attribution framework for MAS grounded in multi-granularity\ncausal inference. Our approach makes two key technical contributions: (1) a\nperformance causal inversion principle, which correctly models performance\ndependencies by reversing the data flow in execution logs, combined with\nShapley values to accurately assign agent-level blame; (2) a novel causal\ndiscovery algorithm, CDC-MAS, that robustly identifies critical failure steps\nby tackling the non-stationary nature of MAS interaction data. The framework's\nattribution results directly fuel an automated optimization loop, generating\ntargeted suggestions whose efficacy is validated via counterfactual\nsimulations. Evaluations on the Who\\&When and TRAIL benchmarks demonstrate a\nsignificant leap in performance. Our method achieves up to 36.2\\% step-level\naccuracy. Crucially, the generated optimizations boost overall task success\nrates by an average of 22.4\\%. This work provides a principled and effective\nsolution for debugging complex agent interactions, paving the way for more\nreliable and interpretable multi-agent systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u7c92\u5ea6\u56e0\u679c\u63a8\u65ad\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6545\u969c\u5f52\u56e0\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6545\u969c\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u8bca\u65ad\u5de5\u5177\u7684\u4e0d\u8db3\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9762\u4e34\u4e25\u91cd\u7684\u6545\u969c\u5f52\u56e0\u6311\u6218\uff0c\u73b0\u6709\u57fa\u4e8e\u7edf\u8ba1\u5173\u8054\u7684\u8bca\u65ad\u5de5\u5177\u6548\u679c\u4e0d\u4f73\uff08\u5982\u5728Who&When\u57fa\u51c6\u4e0a\u51c6\u786e\u7387\u4f4e\u4e8e15%\uff09\uff0c\u6025\u9700\u4e00\u4e2a\u80fd\u51c6\u786e\u5b9a\u4f4d\u6545\u969c\u6839\u672c\u539f\u56e0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u4e86\u57fa\u4e8e\u591a\u7c92\u5ea6\u56e0\u679c\u63a8\u65ad\u7684\u6545\u969c\u5f52\u56e0\u6846\u67b6\u3002\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a1) \u6027\u80fd\u56e0\u679c\u53cd\u8f6c\u539f\u7406\u7ed3\u5408Shapley\u503c\uff0c\u901a\u8fc7\u53cd\u8f6c\u6267\u884c\u65e5\u5fd7\u4e2d\u7684\u6570\u636e\u6d41\u6765\u51c6\u786e\u5206\u914d\u667a\u80fd\u4f53\u7ea7\u522b\u7684\u8d23\u4efb\uff1b2) \u65b0\u9896\u7684\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5CDC-MAS\uff0c\u7528\u4e8e\u5904\u7406MAS\u4ea4\u4e92\u6570\u636e\u7684\u975e\u5e73\u7a33\u6027\uff0c\u4ee5\u8bc6\u522b\u5173\u952e\u6545\u969c\u6b65\u9aa4\u3002\u5f52\u56e0\u7ed3\u679c\u76f4\u63a5\u9a71\u52a8\u81ea\u52a8\u5316\u4f18\u5316\u5faa\u73af\uff0c\u5e76\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6a21\u62df\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5728Who&When\u548cTRAIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\u3002\u6b65\u9aa4\u7ea7\u51c6\u786e\u7387\u6700\u9ad8\u8fbe\u523036.2%\u3002\u751f\u6210\u7684\u4f18\u5316\u5efa\u8bae\u5e73\u5747\u5c06\u6574\u4f53\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e8622.4%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8c03\u8bd5\u590d\u6742\u667a\u80fd\u4f53\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u539f\u5219\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u66f4\u53ef\u89e3\u91ca\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.08140", "pdf": "https://arxiv.org/pdf/2509.08140", "abs": "https://arxiv.org/abs/2509.08140", "authors": ["Mihir Kumar", "Aaron Ontoyin Yin", "Zakari Salifu", "Kelvin Amoaba", "Afriyie Kwesi Samuel", "Fuat Alican", "Yigit Ihlamur"], "title": "From Limited Data to Rare-event Prediction: LLM-powered Feature Engineering and Multi-model Learning in Venture Capital", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 3 figures", "summary": "This paper presents a framework for predicting rare, high-impact outcomes by\nintegrating large language models (LLMs) with a multi-model machine learning\n(ML) architecture. The approach combines the predictive strength of black-box\nmodels with the interpretability required for reliable decision-making. We use\nLLM-powered feature engineering to extract and synthesize complex signals from\nunstructured data, which are then processed within a layered ensemble of models\nincluding XGBoost, Random Forest, and Linear Regression. The ensemble first\nproduces a continuous estimate of success likelihood, which is then thresholded\nto produce a binary rare-event prediction. We apply this framework to the\ndomain of Venture Capital (VC), where investors must evaluate startups with\nlimited and noisy early-stage data. The empirical results show strong\nperformance: the model achieves precision between 9.8X and 11.1X the random\nclassifier baseline in three independent test subsets. Feature sensitivity\nanalysis further reveals interpretable success drivers: the startup's category\nlist accounts for 15.6% of predictive influence, followed by the number of\nfounders, while education level and domain expertise contribute smaller yet\nconsistent effects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u6a21\u578b\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u67b6\u6784\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u7f55\u89c1\u3001\u9ad8\u5f71\u54cd\u4e8b\u4ef6\u3002\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u98ce\u9669\u6295\u8d44\uff08VC\uff09\u9886\u57df\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u53ef\u89e3\u91ca\u7684\u6210\u529f\u9a71\u52a8\u56e0\u7d20\u3002", "motivation": "\u9884\u6d4b\u7f55\u89c1\u3001\u9ad8\u5f71\u54cd\u4e8b\u4ef6\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u98ce\u9669\u6295\u8d44\u7b49\u9886\u57df\uff0c\u6295\u8d44\u8005\u9700\u8981\u8bc4\u4f30\u6570\u636e\u6709\u9650\u4e14\u5608\u6742\u7684\u65e9\u671f\u521b\u4e1a\u516c\u53f8\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u7ed3\u5408\u9ed1\u76d2\u6a21\u578b\u9884\u6d4b\u80fd\u529b\u4e0e\u53ef\u9760\u51b3\u7b56\u6240\u9700\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528LLM\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b\uff0c\u4ece\u975e\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u63d0\u53d6\u548c\u7efc\u5408\u590d\u6742\u4fe1\u53f7\u3002\u8fd9\u4e9b\u4fe1\u53f7\u968f\u540e\u7531\u4e00\u4e2a\u5206\u5c42\u6a21\u578b\u96c6\u6210\u8fdb\u884c\u5904\u7406\uff0c\u5305\u62ecXGBoost\u3001\u968f\u673a\u68ee\u6797\u548c\u7ebf\u6027\u56de\u5f52\u3002\u8be5\u96c6\u6210\u6a21\u578b\u9996\u5148\u751f\u6210\u6210\u529f\u53ef\u80fd\u6027\u7684\u8fde\u7eed\u4f30\u8ba1\uff0c\u7136\u540e\u901a\u8fc7\u9608\u503c\u5904\u7406\u751f\u6210\u4e8c\u5143\u7f55\u89c1\u4e8b\u4ef6\u9884\u6d4b\u3002", "result": "\u5728\u98ce\u9669\u6295\u8d44\u9886\u57df\u7684\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff1a\u5728\u4e09\u4e2a\u72ec\u7acb\u7684\u6d4b\u8bd5\u5b50\u96c6\u4e2d\uff0c\u6a21\u578b\u7684\u7cbe\u786e\u5ea6\u6bd4\u968f\u673a\u5206\u7c7b\u5668\u57fa\u7ebf\u9ad89.8\u523011.1\u500d\u3002\u7279\u5f81\u654f\u611f\u6027\u5206\u6790\u63ed\u793a\u4e86\u53ef\u89e3\u91ca\u7684\u6210\u529f\u9a71\u52a8\u56e0\u7d20\uff1a\u521b\u4e1a\u516c\u53f8\u7684\u7c7b\u522b\u5217\u8868\u8d21\u732e\u4e8615.6%\u7684\u9884\u6d4b\u5f71\u54cd\uff0c\u5176\u6b21\u662f\u521b\u59cb\u4eba\u6570\u91cf\uff0c\u800c\u6559\u80b2\u6c34\u5e73\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e5f\u63d0\u4f9b\u4e86\u8f83\u5c0f\u4f46\u4e00\u81f4\u7684\u5f71\u54cd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u9884\u6d4b\u7f55\u89c1\u3001\u9ad8\u5f71\u54cd\u4e8b\u4ef6\uff0c\u5e76\u5728\u98ce\u9669\u6295\u8d44\u9886\u57df\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002\u901a\u8fc7LLM\u9a71\u52a8\u7684\u7279\u5f81\u5de5\u7a0b\u548c\u591a\u6a21\u578b\u96c6\u6210\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u8fd8\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6d1e\u5bdf\uff0c\u63ed\u793a\u4e86\u5173\u952e\u7684\u6210\u529f\u9a71\u52a8\u56e0\u7d20\u3002"}}
{"id": "2509.08217", "pdf": "https://arxiv.org/pdf/2509.08217", "abs": "https://arxiv.org/abs/2509.08217", "authors": ["Eve Fleisig", "Matthias Orlikowski", "Philipp Cimiano", "Dan Klein"], "title": "Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "For machine learning datasets to accurately represent diverse opinions in a\npopulation, they must preserve variation in data labels while filtering out\nspam or low-quality responses. How can we balance annotator reliability and\nrepresentation? We empirically evaluate how a range of heuristics for annotator\nfiltering affect the preservation of variation on subjective tasks. We find\nthat these methods, designed for contexts in which variation from a single\nground-truth label is considered noise, often remove annotators who disagree\ninstead of spam annotators, introducing suboptimal tradeoffs between accuracy\nand label diversity. We find that conservative settings for annotator removal\n(<5%) are best, after which all tested methods increase the mean absolute error\nfrom the true average label. We analyze performance on synthetic spam to\nobserve that these methods often assume spam annotators are less random than\nreal spammers tend to be: most spammers are distributionally indistinguishable\nfrom real annotators, and the minority that are distinguishable tend to give\nfixed answers, not random ones. Thus, tasks requiring the preservation of\nvariation reverse the intuition of existing spam filtering methods: spammers\ntend to be less random than non-spammers, so metrics that assume variation is\nspam fare worse. These results highlight the need for spam removal methods that\naccount for label diversity.", "AI": {"tldr": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6570\u636e\u96c6\u7684\u5783\u573e\u6807\u6ce8\u8fc7\u6ee4\u65b9\u6cd5\u5728\u5904\u7406\u4e3b\u89c2\u4efb\u52a1\u65f6\uff0c\u503e\u5411\u4e8e\u79fb\u9664\u5408\u6cd5\u7684\u591a\u6837\u6027\u800c\u975e\u5783\u573e\u6807\u6ce8\uff0c\u5bfc\u81f4\u6807\u7b7e\u591a\u6837\u6027\u635f\u5931\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5bf9\u5783\u573e\u6807\u6ce8\u8005\u7684\u968f\u673a\u6027\u5047\u8bbe\u6709\u8bef\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u517c\u987e\u6807\u7b7e\u591a\u6837\u6027\u7684\u65b0\u8fc7\u6ee4\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4f7f\u673a\u5668\u5b66\u4e60\u6570\u636e\u96c6\u80fd\u51c6\u786e\u53cd\u6620\u4eba\u7fa4\u4e2d\u7684\u591a\u6837\u5316\u89c2\u70b9\uff0c\u9700\u5728\u8fc7\u6ee4\u5783\u573e\u6216\u4f4e\u8d28\u91cf\u54cd\u5e94\u7684\u540c\u65f6\uff0c\u4fdd\u7559\u6570\u636e\u6807\u7b7e\u4e2d\u7684\u56fa\u6709\u53d8\u5f02\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u5e73\u8861\u6807\u6ce8\u8005\u53ef\u9760\u6027\uff08\u53bb\u9664\u5783\u573e\uff09\u4e0e\u6807\u7b7e\u591a\u6837\u6027\uff08\u4fdd\u7559\u89c2\u70b9\uff09\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "method": "\u7ecf\u9a8c\u6027\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u7528\u4e8e\u6807\u6ce8\u8005\u8fc7\u6ee4\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u89c2\u5bdf\u5b83\u4eec\u5728\u4e3b\u89c2\u4efb\u52a1\u4e2d\u5982\u4f55\u5f71\u54cd\u53d8\u5f02\u6027\u7684\u4fdd\u7559\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5206\u6790\u5408\u6210\u5783\u573e\u6570\u636e\u6765\u68c0\u9a8c\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6027\u80fd\u548c\u5176\u5bf9\u5783\u573e\u6807\u6ce8\u8005\u7684\u5047\u8bbe\u3002", "result": "1. \u73b0\u6709\u8fc7\u6ee4\u65b9\u6cd5\uff08\u8bbe\u8ba1\u7528\u4e8e\u5355\u4e00\u201c\u771f\u5b9e\u201d\u6807\u7b7e\u573a\u666f\uff09\u5e38\u79fb\u9664\u6301\u5f02\u8bae\u7684\u6807\u6ce8\u8005\u800c\u975e\u5783\u573e\u6807\u6ce8\u8005\uff0c\u5bfc\u81f4\u51c6\u786e\u6027\u548c\u6807\u7b7e\u591a\u6837\u6027\u4e4b\u95f4\u51fa\u73b0\u6b21\u4f18\u6743\u8861\u3002 2. \u6807\u6ce8\u8005\u79fb\u9664\u7684\u4fdd\u5b88\u8bbe\u7f6e\uff08<5%\uff09\u6548\u679c\u6700\u4f73\uff0c\u8d85\u8fc7\u6b64\u9608\u503c\u540e\uff0c\u6240\u6709\u6d4b\u8bd5\u65b9\u6cd5\u90fd\u4f1a\u589e\u52a0\u4e0e\u771f\u5b9e\u5e73\u5747\u6807\u7b7e\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u3002 3. \u73b0\u6709\u65b9\u6cd5\u5e38\u5047\u8bbe\u5783\u573e\u6807\u6ce8\u8005\u4e0d\u5982\u771f\u5b9e\u6807\u6ce8\u8005\u968f\u673a\uff0c\u4f46\u5927\u591a\u6570\u5783\u573e\u6807\u6ce8\u8005\u4e0e\u771f\u5b9e\u6807\u6ce8\u8005\u5728\u5206\u5e03\u4e0a\u96be\u4ee5\u533a\u5206\uff0c\u5c11\u6570\u53ef\u533a\u5206\u7684\u503e\u5411\u4e8e\u7ed9\u51fa\u56fa\u5b9a\u800c\u975e\u968f\u673a\u7684\u7b54\u6848\u3002 4. \u9700\u8981\u4fdd\u7559\u53d8\u5f02\u6027\u7684\u4efb\u52a1\u4e0e\u73b0\u6709\u5783\u573e\u8fc7\u6ee4\u65b9\u6cd5\u7684\u76f4\u89c9\u76f8\u53cd\uff1a\u5783\u573e\u6807\u6ce8\u8005\u5f80\u5f80\u4e0d\u5982\u975e\u5783\u573e\u6807\u6ce8\u8005\u968f\u673a\uff0c\u56e0\u6b64\u5047\u8bbe\u53d8\u5f02\u662f\u5783\u573e\u7684\u5ea6\u91cf\u65b9\u6cd5\u6548\u679c\u66f4\u5dee\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\uff0c\u6025\u9700\u5f00\u53d1\u80fd\u591f\u5145\u5206\u8003\u8651\u5e76\u4fdd\u7559\u6807\u7b7e\u591a\u6837\u6027\u7684\u5783\u573e\u6807\u6ce8\u79fb\u9664\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u4e3b\u89c2\u4efb\u52a1\u4e2d\u73b0\u6709\u8fc7\u6ee4\u673a\u5236\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.08234", "pdf": "https://arxiv.org/pdf/2509.08234", "abs": "https://arxiv.org/abs/2509.08234", "authors": ["Faisal Ahmed"], "title": "RepViT-CXR: A Channel Replication Strategy for Vision Transformers in Chest X-ray Tuberculosis and Pneumonia Classification", "categories": ["cs.CV", "cs.LG", "F.2.2; I.2.7"], "comment": "10 pages, 5 figures", "summary": "Chest X-ray (CXR) imaging remains one of the most widely used diagnostic\ntools for detecting pulmonary diseases such as tuberculosis (TB) and pneumonia.\nRecent advances in deep learning, particularly Vision Transformers (ViTs), have\nshown strong potential for automated medical image analysis. However, most ViT\narchitectures are pretrained on natural images and require three-channel\ninputs, while CXR scans are inherently grayscale. To address this gap, we\npropose RepViT-CXR, a channel replication strategy that adapts single-channel\nCXR images into a ViT-compatible format without introducing additional\ninformation loss. We evaluate RepViT-CXR on three benchmark datasets. On the\nTB-CXR dataset,our method achieved an accuracy of 99.9% and an AUC of 99.9%,\nsurpassing prior state-of-the-art methods such as Topo-CXR (99.3% accuracy,\n99.8% AUC). For the Pediatric Pneumonia dataset, RepViT-CXR obtained 99.0%\naccuracy, with 99.2% recall, 99.3% precision, and an AUC of 99.0%,\noutperforming strong baselines including DCNN and VGG16. On the Shenzhen TB\ndataset, our approach achieved 91.1% accuracy and an AUC of 91.2%, marking a\nperformance improvement over previously reported CNN-based methods. These\nresults demonstrate that a simple yet effective channel replication strategy\nallows ViTs to fully leverage their representational power on grayscale medical\nimaging tasks. RepViT-CXR establishes a new state of the art for TB and\npneumonia detection from chest X-rays, showing strong potential for deployment\nin real-world clinical screening systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRepViT-CXR\uff0c\u4e00\u79cd\u901a\u9053\u590d\u5236\u7b56\u7565\uff0c\u4f7fVision Transformers\uff08ViTs\uff09\u80fd\u6709\u6548\u5904\u7406\u7070\u5ea6\u80f8\u90e8X\u5c04\u7ebf\uff08CXR\uff09\u56fe\u50cf\u3002\u8be5\u65b9\u6cd5\u5728\u7ed3\u6838\u75c5\u548c\u80ba\u708e\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u6027\u80fd\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002", "motivation": "Vision Transformers (ViTs) \u5728\u81ea\u52a8\u5316\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u9700\u8981\u4e09\u901a\u9053\u8f93\u5165\u4e14\u9884\u8bad\u7ec3\u4e8e\u81ea\u7136\u56fe\u50cf\u3002\u80f8\u90e8X\u5c04\u7ebf (CXR) \u56fe\u50cf\u672c\u8d28\u4e0a\u662f\u7070\u5ea6\u7684\u5355\u901a\u9053\u56fe\u50cf\uff0c\u8fd9\u9650\u5236\u4e86ViTs\u5728\u5176\u4e0a\u7684\u76f4\u63a5\u5e94\u7528\uff0c\u5b58\u5728\u4e00\u4e2a\u901a\u9053\u4e0d\u5339\u914d\u7684\u7a7a\u767d\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3\u7070\u5ea6CXR\u4e0eViT\u4e09\u901a\u9053\u8f93\u5165\u8981\u6c42\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\uff0c\u4f5c\u8005\u63d0\u51faRepViT-CXR\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u9053\u590d\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u7b80\u5355\u5730\u590d\u5236\u5355\u901a\u9053CXR\u56fe\u50cf\u4ee5\u751f\u6210ViT\u517c\u5bb9\u7684\u4e09\u901a\u9053\u683c\u5f0f\uff0c\u540c\u65f6\u907f\u514d\u5f15\u5165\u989d\u5916\u7684\u4fe1\u606f\u635f\u5931\u3002", "result": "RepViT-CXR\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff1a\u5728TB-CXR\u6570\u636e\u96c6\u4e0a\uff0c\u51c6\u786e\u738799.9%\uff0cAUC 99.9%\uff0c\u8d85\u8d8a\u4e86Topo-CXR\u7b49\u73b0\u6709SOTA\u65b9\u6cd5\u3002\u5728\u513f\u7ae5\u80ba\u708e\u6570\u636e\u96c6\u4e0a\uff0c\u51c6\u786e\u738799.0%\uff0cAUC 99.0%\uff0c\u4f18\u4e8eDCNN\u548cVGG16\u7b49\u5f3a\u57fa\u7ebf\u3002\u5728\u6df1\u5733TB\u6570\u636e\u96c6\u4e0a\uff0c\u51c6\u786e\u738791.1%\uff0cAUC 91.2%\uff0c\u6027\u80fd\u4f18\u4e8e\u6b64\u524d\u62a5\u9053\u7684CNN\u65b9\u6cd5\u3002", "conclusion": "RepViT-CXR\u8bc1\u660e\u4e86\u7b80\u5355\u800c\u6709\u6548\u7684\u901a\u9053\u590d\u5236\u7b56\u7565\u80fd\u591f\u4f7fViTs\u5145\u5206\u53d1\u6325\u5176\u5728\u7070\u5ea6\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e2d\u7684\u8868\u793a\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u80f8\u90e8X\u5c04\u7ebf\u7ed3\u6838\u75c5\u548c\u80ba\u708e\u68c0\u6d4b\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684SOTA\uff0c\u5e76\u5177\u6709\u5728\u5b9e\u9645\u4e34\u5e8a\u7b5b\u67e5\u7cfb\u7edf\u4e2d\u90e8\u7f72\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.08705", "pdf": "https://arxiv.org/pdf/2509.08705", "abs": "https://arxiv.org/abs/2509.08705", "authors": ["Shalima Binta Manir", "Tim Oates"], "title": "One Model, Two Minds: A Context-Gated Graph Learner that Recreates Human Biases", "categories": ["cs.AI"], "comment": "9 pages, 7 figures, 2 tables", "summary": "We introduce a novel Theory of Mind (ToM) framework inspired by dual-process\ntheories from cognitive science, integrating a fast, habitual graph-based\nreasoning system (System 1), implemented via graph convolutional networks\n(GCNs), and a slower, context-sensitive meta-adaptive learning system (System\n2), driven by meta-learning techniques. Our model dynamically balances\nintuitive and deliberative reasoning through a learned context gate mechanism.\nWe validate our architecture on canonical false-belief tasks and systematically\nexplore its capacity to replicate hallmark cognitive biases associated with\ndual-process theory, including anchoring, cognitive-load fatigue, framing\neffects, and priming effects. Experimental results demonstrate that our\ndual-process approach closely mirrors human adaptive behavior, achieves robust\ngeneralization to unseen contexts, and elucidates cognitive mechanisms\nunderlying reasoning biases. This work bridges artificial intelligence and\ncognitive theory, paving the way for AI systems exhibiting nuanced, human-like\nsocial cognition and adaptive decision-making capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u53d7\u8ba4\u77e5\u79d1\u5b66\u53cc\u52a0\u5de5\u7406\u8bba\u542f\u53d1\u7684\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u548c\u5143\u5b66\u4e60\uff0c\u80fd\u52a8\u6001\u5e73\u8861\u76f4\u89c9\u4e0e\u5ba1\u614e\u63a8\u7406\uff0c\u6210\u529f\u590d\u5236\u4eba\u7c7b\u8ba4\u77e5\u504f\u5dee\uff0c\u5e76\u5b9e\u73b0\u9c81\u68d2\u6cdb\u5316\uff0c\u4e3a\u7c7b\u4ebaAI\u793e\u4ea4\u8ba4\u77e5\u4e0e\u51b3\u7b56\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u53d7\u4eba\u7c7b\u8ba4\u77e5\u79d1\u5b66\u53cc\u52a0\u5de5\u7406\u8bba\u542f\u53d1\u7684\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u6846\u67b6\uff0c\u4ee5\u6a21\u62df\u4eba\u7c7b\u590d\u6742\u7684\u793e\u4ea4\u8ba4\u77e5\u548c\u81ea\u9002\u5e94\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u7406\u89e3\u8ba4\u77e5\u504f\u5dee\u7684\u673a\u5236\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u53cc\u52a0\u5de5ToM\u6846\u67b6\uff0c\u5176\u4e2dSystem 1\u901a\u8fc7\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u5b9e\u73b0\u5feb\u901f\u3001\u4e60\u60ef\u6027\u63a8\u7406\uff0cSystem 2\u901a\u8fc7\u5143\u5b66\u4e60\u6280\u672f\u9a71\u52a8\u6162\u901f\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u5143\u9002\u5e94\u6027\u5b66\u4e60\u3002\u6a21\u578b\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u4e0a\u4e0b\u6587\u95e8\u673a\u5236\u52a8\u6001\u5e73\u8861\u8fd9\u4e24\u79cd\u63a8\u7406\u65b9\u5f0f\u3002\u5728\u7ecf\u5178\u9519\u8bef\u4fe1\u5ff5\u4efb\u52a1\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5e76\u7cfb\u7edf\u6027\u63a2\u7d22\u5176\u590d\u5236\u951a\u5b9a\u6548\u5e94\u3001\u8ba4\u77e5\u8d1f\u8377\u75b2\u52b3\u3001\u6846\u67b6\u6548\u5e94\u548c\u542f\u52a8\u6548\u5e94\u7b49\u8ba4\u77e5\u504f\u5dee\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u53cc\u52a0\u5de5\u65b9\u6cd5\u80fd\u591f\u5bc6\u5207\u6a21\u62df\u4eba\u7c7b\u7684\u9002\u5e94\u6027\u884c\u4e3a\uff0c\u5b9e\u73b0\u5bf9\u672a\u89c1\u4e0a\u4e0b\u6587\u7684\u9c81\u68d2\u6cdb\u5316\uff0c\u5e76\u9610\u660e\u4e86\u63a8\u7406\u504f\u5dee\u80cc\u540e\u7684\u8ba4\u77e5\u673a\u5236\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f25\u5408\u4e86\u4eba\u5de5\u667a\u80fd\u548c\u8ba4\u77e5\u7406\u8bba\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u5f00\u53d1\u5177\u6709\u7ec6\u81f4\u7c7b\u4eba\u793e\u4ea4\u8ba4\u77e5\u548c\u9002\u5e94\u6027\u51b3\u7b56\u80fd\u529b\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.08156", "pdf": "https://arxiv.org/pdf/2509.08156", "abs": "https://arxiv.org/abs/2509.08156", "authors": ["Swati Swati", "Arjun Roy", "Emmanouil Panagiotou", "Eirini Ntoutsi"], "title": "MMM-fair: An Interactive Toolkit for Exploring and Operationalizing Multi-Fairness Trade-offs", "categories": ["cs.LG", "cs.CY"], "comment": "Accepted to be published in the Proceedings of the 34th ACM\n  International Conference on Information and Knowledge Management, November\n  10--14, 2025, Seoul, Republic of Korea", "summary": "Fairness-aware classification requires balancing performance and fairness,\noften intensified by intersectional biases. Conflicting fairness definitions\nfurther complicate the task, making it difficult to identify universally fair\nsolutions. Despite growing regulatory and societal demands for equitable AI,\npopular toolkits offer limited support for exploring multi-dimensional fairness\nand related trade-offs. To address this, we present mmm-fair, an open-source\ntoolkit leveraging boosting-based ensemble approaches that dynamically\noptimizes model weights to jointly minimize classification errors and diverse\nfairness violations, enabling flexible multi-objective optimization. The system\nempowers users to deploy models that align with their context-specific needs\nwhile reliably uncovering intersectional biases often missed by\nstate-of-the-art methods. In a nutshell, mmm-fair uniquely combines in-depth\nmulti-attribute fairness, multi-objective optimization, a no-code, chat-based\ninterface, LLM-powered explanations, interactive Pareto exploration for model\nselection, custom fairness constraint definition, and deployment-ready models\nin a single open-source toolkit, a combination rarely found in existing\nfairness tools. Demo walkthrough available at: https://youtu.be/_rcpjlXFqkw.", "AI": {"tldr": "mmm-fair\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u57fa\u4e8eboosting\u7684\u96c6\u6210\u65b9\u6cd5\u548c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u52a8\u6001\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u591a\u7ef4\u5ea6\uff08\u5305\u62ec\u4ea4\u53c9\u504f\u89c1\uff09\u516c\u5e73\u6027\uff0c\u5e76\u63d0\u4f9b\u7528\u6237\u53cb\u597d\u7684\u754c\u9762\u548c\u9ad8\u7ea7\u529f\u80fd\uff0c\u4ee5\u89e3\u51b3\u516c\u5e73\u6027\u611f\u77e5\u5206\u7c7b\u4e2d\u7684\u73b0\u6709\u6311\u6218\u3002", "motivation": "\u516c\u5e73\u6027\u611f\u77e5\u5206\u7c7b\u9700\u5e73\u8861\u6027\u80fd\u4e0e\u516c\u5e73\u6027\uff0c\u4f46\u4ea4\u53c9\u504f\u89c1\u548c\u51b2\u7a81\u7684\u516c\u5e73\u6027\u5b9a\u4e49\u4f7f\u5176\u590d\u6742\u5316\u3002\u5c3d\u7ba1\u5bf9\u516c\u5e73AI\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u73b0\u6709\u5de5\u5177\u5305\u5728\u63a2\u7d22\u591a\u7ef4\u5ea6\u516c\u5e73\u6027\u548c\u6743\u8861\u65b9\u9762\u652f\u6301\u6709\u9650\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86mmm-fair\uff0c\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u5229\u7528\u57fa\u4e8eboosting\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u52a8\u6001\u4f18\u5316\u6a21\u578b\u6743\u91cd\uff0c\u4ee5\u5171\u540c\u6700\u5c0f\u5316\u5206\u7c7b\u8bef\u5dee\u548c\u591a\u79cd\u516c\u5e73\u6027\u8fdd\u89c4\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u591a\u76ee\u6807\u4f18\u5316\u3002\u5b83\u8fd8\u63d0\u4f9b\u65e0\u4ee3\u7801\u3001\u57fa\u4e8e\u804a\u5929\u7684\u754c\u9762\uff0cLLM\u9a71\u52a8\u7684\u89e3\u91ca\uff0c\u4ea4\u4e92\u5f0f\u5e15\u7d2f\u6258\u63a2\u7d22\uff0c\u81ea\u5b9a\u4e49\u516c\u5e73\u6027\u7ea6\u675f\u5b9a\u4e49\uff0c\u5e76\u751f\u6210\u53ef\u90e8\u7f72\u6a21\u578b\u3002", "result": "mmm-fair\u4f7f\u7528\u6237\u80fd\u591f\u90e8\u7f72\u7b26\u5408\u5176\u7279\u5b9a\u4e0a\u4e0b\u6587\u9700\u6c42\u7684\u6a21\u578b\uff0c\u5e76\u53ef\u9760\u5730\u53d1\u73b0\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u5e38\u9057\u6f0f\u7684\u4ea4\u53c9\u504f\u89c1\u3002\u5b83\u72ec\u7279\u5730\u7ed3\u5408\u4e86\u6df1\u5ea6\u591a\u5c5e\u6027\u516c\u5e73\u6027\u3001\u591a\u76ee\u6807\u4f18\u5316\u3001\u65e0\u4ee3\u7801\u754c\u9762\u3001LLM\u89e3\u91ca\u3001\u4ea4\u4e92\u5f0f\u5e15\u7d2f\u6258\u63a2\u7d22\u3001\u81ea\u5b9a\u4e49\u516c\u5e73\u6027\u7ea6\u675f\u5b9a\u4e49\u548c\u53ef\u90e8\u7f72\u6a21\u578b\u4e8e\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\u4e2d\uff0c\u8fd9\u662f\u73b0\u6709\u516c\u5e73\u6027\u5de5\u5177\u4e2d\u7f55\u89c1\u7684\u7ec4\u5408\u3002", "conclusion": "mmm-fair\u662f\u4e00\u4e2a\u5168\u9762\u4e14\u521b\u65b0\u7684\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u5176\u5148\u8fdb\u7684\u591a\u7ef4\u5ea6\u516c\u5e73\u6027\u5206\u6790\u3001\u591a\u76ee\u6807\u4f18\u5316\u548c\u7528\u6237\u53cb\u597d\u7684\u7279\u6027\uff0c\u5f25\u8865\u4e86\u516c\u5e73\u6027\u611f\u77e5AI\u4e2d\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u66f4\u516c\u5e73\u548c\u7b26\u5408\u4e0a\u4e0b\u6587\u7684AI\u90e8\u7f72\u3002"}}
{"id": "2509.08304", "pdf": "https://arxiv.org/pdf/2509.08304", "abs": "https://arxiv.org/abs/2509.08304", "authors": ["Yehudit Aperstein", "Alon Gottlib", "Gal Benita", "Alexander Apartsin"], "title": "Towards Knowledge-Aware Document Systems: Modeling Semantic Coverage Relations via Answerability Detection", "categories": ["cs.CL"], "comment": "27 pages, 1 figure", "summary": "Understanding how information is shared across documents, regardless of the\nformat in which it is expressed, is critical for tasks such as information\nretrieval, summarization, and content alignment. In this work, we introduce a\nnovel framework for modelling Semantic Coverage Relations (SCR), which\nclassifies document pairs based on how their informational content aligns. We\ndefine three core relation types: equivalence, where both texts convey the same\ninformation using different textual forms or styles; inclusion, where one\ndocument fully contains the information of another and adds more; and semantic\noverlap, where each document presents partially overlapping content. To capture\nthese relations, we adopt a question answering (QA)-based approach, using the\nanswerability of shared questions across documents as an indicator of semantic\ncoverage. We construct a synthetic dataset derived from the SQuAD corpus by\nparaphrasing source passages and selectively omitting information, enabling\nprecise control over content overlap. This dataset allows us to benchmark\ngenerative language models and train transformer-based classifiers for SCR\nprediction. Our findings demonstrate that discriminative models significantly\noutperform generative approaches, with the RoBERTa-base model achieving the\nhighest accuracy of 61.4% and the Random Forest-based model showing the best\nbalance with a macro-F1 score of 52.9%. The results show that QA provides an\neffective lens for assessing semantic relations across stylistically diverse\ntexts, offering insights into the capacity of current models to reason about\ninformation beyond surface similarity. The dataset and code developed in this\nstudy are publicly available to support reproducibility.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u4e49\u8986\u76d6\u5173\u7cfb\uff08SCR\uff09\u5efa\u6a21\u6846\u67b6\uff0c\u5b9a\u4e49\u4e86\u7b49\u4ef7\u3001\u5305\u542b\u548c\u8bed\u4e49\u91cd\u53e0\u4e09\u79cd\u5173\u7cfb\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u95ee\u7b54\uff08QA\uff09\u7684\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\u3002\u901a\u8fc7\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u5224\u522b\u5f0f\u6a21\u578b\uff08\u5982RoBERTa\uff09\u5728SCR\u9884\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8e\u751f\u6210\u5f0f\u6a21\u578b\u3002", "motivation": "\u7406\u89e3\u6587\u6863\u95f4\u4fe1\u606f\u5171\u4eab\u6a21\u5f0f\u5bf9\u4fe1\u606f\u68c0\u7d22\u3001\u6458\u8981\u548c\u5185\u5bb9\u5bf9\u9f50\u7b49\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u5f15\u5165\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u6587\u6863\u5bf9\u7684\u4fe1\u606f\u5185\u5bb9\u5bf9\u9f50\u65b9\u5f0f\u8fdb\u884c\u5206\u7c7b\uff0c\u5373\u8bed\u4e49\u8986\u76d6\u5173\u7cfb\uff08SCR\uff09\u3002", "method": "\u5b9a\u4e49\u4e86\u7b49\u4ef7\u3001\u5305\u542b\u548c\u8bed\u4e49\u91cd\u53e0\u4e09\u79cd\u6838\u5fc3\u8bed\u4e49\u8986\u76d6\u5173\u7cfb\u3002\u91c7\u7528\u57fa\u4e8e\u95ee\u7b54\uff08QA\uff09\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5171\u4eab\u95ee\u9898\u7684\u53ef\u56de\u7b54\u6027\u4f5c\u4e3a\u8bed\u4e49\u8986\u76d6\u7684\u6307\u6807\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u4eceSQuAD\u8bed\u6599\u5e93\u6d3e\u751f\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6539\u5199\u6e90\u6587\u672c\u548c\u9009\u62e9\u6027\u7701\u7565\u4fe1\u606f\u6765\u7cbe\u786e\u63a7\u5236\u5185\u5bb9\u91cd\u53e0\u3002\u5229\u7528\u8be5\u6570\u636e\u96c6\u6765\u57fa\u51c6\u6d4b\u8bd5\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u5e76\u8bad\u7ec3\u57fa\u4e8eTransformer\u7684\u5206\u7c7b\u5668\u8fdb\u884cSCR\u9884\u6d4b\u3002", "result": "\u5224\u522b\u5f0f\u6a21\u578b\u5728SCR\u9884\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8e\u751f\u6210\u5f0f\u65b9\u6cd5\u3002\u5176\u4e2d\uff0cRoBERTa-base\u6a21\u578b\u8fbe\u5230\u4e8661.4%\u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u800c\u57fa\u4e8eRandom Forest\u7684\u6a21\u578b\u5728\u5b8fF1\u5206\u6570\u4e0a\u53d6\u5f9752.9%\uff0c\u8868\u73b0\u51fa\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u95ee\u7b54\uff08QA\uff09\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u8bc4\u4f30\u98ce\u683c\u591a\u6837\u6587\u672c\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u5f53\u524d\u6a21\u578b\u8d85\u8d8a\u8868\u9762\u76f8\u4f3c\u6027\u8fdb\u884c\u4fe1\u606f\u63a8\u7406\u80fd\u529b\u7684\u89c1\u89e3\u3002\u4e3a\u652f\u6301\u7814\u7a76\u7684\u53ef\u91cd\u73b0\u6027\uff0c\u672c\u7814\u7a76\u7684\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2509.08243", "pdf": "https://arxiv.org/pdf/2509.08243", "abs": "https://arxiv.org/abs/2509.08243", "authors": ["Zheng Yang", "Yanteng Zhang", "Xupeng Kou", "Yang Liu", "Chao Ren"], "title": "Symmetry Interactive Transformer with CNN Framework for Diagnosis of Alzheimer's Disease Using Structural MRI", "categories": ["cs.CV"], "comment": null, "summary": "Structural magnetic resonance imaging (sMRI) combined with deep learning has\nachieved remarkable progress in the prediction and diagnosis of Alzheimer's\ndisease (AD). Existing studies have used CNN and transformer to build a\nwell-performing network, but most of them are based on pretraining or ignoring\nthe asymmetrical character caused by brain disorders. We propose an end-to-end\nnetwork for the detection of disease-based asymmetric induced by left and right\nbrain atrophy which consist of 3D CNN Encoder and Symmetry Interactive\nTransformer (SIT). Following the inter-equal grid block fetch operation, the\ncorresponding left and right hemisphere features are aligned and subsequently\nfed into the SIT for diagnostic analysis. SIT can help the model focus more on\nthe regions of asymmetry caused by structural changes, thus improving\ndiagnostic performance. We evaluated our method based on the ADNI dataset, and\nthe results show that the method achieves better diagnostic accuracy (92.5\\%)\ncompared to several CNN methods and CNNs combined with a general transformer.\nThe visualization results show that our network pays more attention in regions\nof brain atrophy, especially for the asymmetric pathological characteristics\ninduced by AD, demonstrating the interpretability and effectiveness of the\nmethod.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u7ed3\u54083D CNN\u7f16\u7801\u5668\u548c\u5bf9\u79f0\u4ea4\u4e92\u5f0fTransformer (SIT) \u7684\u7aef\u5230\u7aef\u7f51\u7edc\uff0c\u7528\u4e8e\u68c0\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u5f15\u8d77\u7684\u8111\u90e8\u4e0d\u5bf9\u79f0\u6027\uff0c\u5e76\u5728ADNI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8692.5%\u7684\u8bca\u65ad\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684AD\u8bca\u65ad\u65b9\u6cd5\u591a\u4f9d\u8d56\u9884\u8bad\u7ec3\u6216\u5ffd\u7565\u4e86\u8111\u90e8\u75be\u75c5\u5bfc\u81f4\u7684\u4e0d\u5bf9\u79f0\u7279\u5f81\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u5173\u6ce8\u5de6\u53f3\u8111\u840e\u7f29\u5f15\u8d77\u7684\u4e0d\u5bf9\u79f0\u6027\u6765\u63d0\u9ad8\u8bca\u65ad\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u7f51\u7edc\uff0c\u75313D CNN\u7f16\u7801\u5668\u548c\u5bf9\u79f0\u4ea4\u4e92\u5f0fTransformer (SIT) \u7ec4\u6210\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u201cinter-equal grid block fetch\u201d\u64cd\u4f5c\u5bf9\u9f50\u5de6\u53f3\u534a\u7403\u7279\u5f81\uff0c\u7136\u540e\u5c06\u5176\u8f93\u5165SIT\u8fdb\u884c\u8bca\u65ad\u5206\u6790\u3002SIT\u65e8\u5728\u4f7f\u6a21\u578b\u66f4\u5173\u6ce8\u7ed3\u6784\u53d8\u5316\u5f15\u8d77\u7684\u4e0d\u5bf9\u79f0\u533a\u57df\u3002", "result": "\u5728ADNI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e8692.5%\u7684\u8bca\u65ad\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u51e0\u79cdCNN\u65b9\u6cd5\u548c\u7ed3\u5408\u901a\u7528Transformer\u7684CNN\u65b9\u6cd5\u3002\u53ef\u89c6\u5316\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7f51\u7edc\u66f4\u5173\u6ce8\u8111\u840e\u7f29\u533a\u57df\uff0c\u7279\u522b\u662fAD\u5f15\u8d77\u7684\u4e0d\u5bf9\u79f0\u75c5\u7406\u7279\u5f81\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u5730\u6355\u6349AD\u5f15\u8d77\u7684\u5de6\u53f3\u8111\u4e0d\u5bf9\u79f0\u7279\u5f81\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728AD\u8bca\u65ad\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.08713", "pdf": "https://arxiv.org/pdf/2509.08713", "abs": "https://arxiv.org/abs/2509.08713", "authors": ["Ziming Luo", "Atoosa Kasirzadeh", "Nihar B. Shah"], "title": "The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems", "categories": ["cs.AI", "cs.DL"], "comment": null, "summary": "AI scientist systems, capable of autonomously executing the full research\nworkflow from hypothesis generation and experimentation to paper writing, hold\nsignificant potential for accelerating scientific discovery. However, the\ninternal workflow of these systems have not been closely examined. This lack of\nscrutiny poses a risk of introducing flaws that could undermine the integrity,\nreliability, and trustworthiness of their research outputs. In this paper, we\nidentify four potential failure modes in contemporary AI scientist systems:\ninappropriate benchmark selection, data leakage, metric misuse, and post-hoc\nselection bias. To examine these risks, we design controlled experiments that\nisolate each failure mode while addressing challenges unique to evaluating AI\nscientist systems. Our assessment of two prominent open-source AI scientist\nsystems reveals the presence of several failures, across a spectrum of\nseverity, which can be easily overlooked in practice. Finally, we demonstrate\nthat access to trace logs and code from the full automated workflow enables far\nmore effective detection of such failures than examining the final paper alone.\nWe thus recommend journals and conferences evaluating AI-generated research to\nmandate submission of these artifacts alongside the paper to ensure\ntransparency, accountability, and reproducibility.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86AI\u79d1\u5b66\u5bb6\u7cfb\u7edf\u5185\u90e8\u5de5\u4f5c\u6d41\u7a0b\u7684\u6f5c\u5728\u7f3a\u9677\uff0c\u8bc6\u522b\u4e86\u56db\u79cd\u4e3b\u8981\u6545\u969c\u6a21\u5f0f\uff08\u5982\u4e0d\u5f53\u57fa\u51c6\u9009\u62e9\u3001\u6570\u636e\u6cc4\u9732\u7b49\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5728\u73b0\u6709\u5f00\u6e90\u7cfb\u7edf\u4e2d\u53d1\u73b0\u4e86\u8fd9\u4e9b\u7f3a\u9677\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u63d0\u4ea4AI\u751f\u6210\u7814\u7a76\u7684\u5b8c\u6574\u65e5\u5fd7\u548c\u4ee3\u7801\u4ee5\u786e\u4fdd\u900f\u660e\u5ea6\u3001\u53ef\u4fe1\u5ea6\u548c\u53ef\u590d\u73b0\u6027\u7684\u91cd\u8981\u6027\u3002", "motivation": "AI\u79d1\u5b66\u5bb6\u7cfb\u7edf\u867d\u6709\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5185\u90e8\u5de5\u4f5c\u6d41\u7a0b\u7f3a\u4e4f\u5ba1\u67e5\u3002\u8fd9\u79cd\u5ba1\u67e5\u7f3a\u5931\u53ef\u80fd\u5f15\u5165\u7f3a\u9677\uff0c\u635f\u5bb3\u5176\u7814\u7a76\u8f93\u51fa\u7684\u5b8c\u6574\u6027\u3001\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u8bc6\u522b\u5e76\u68c0\u67e5\u8fd9\u4e9b\u7cfb\u7edf\u4e2d\u7684\u6f5c\u5728\u6545\u969c\u6a21\u5f0f\u3002", "method": "\u7814\u7a76\u8bc6\u522b\u4e86\u5f53\u4ee3AI\u79d1\u5b66\u5bb6\u7cfb\u7edf\u7684\u56db\u79cd\u6f5c\u5728\u6545\u969c\u6a21\u5f0f\uff1a\u4e0d\u5f53\u7684\u57fa\u51c6\u9009\u62e9\u3001\u6570\u636e\u6cc4\u9732\u3001\u5ea6\u91cf\u6ee5\u7528\u548c\u4e8b\u540e\u9009\u62e9\u504f\u5dee\u3002\u4e3a\u68c0\u9a8c\u8fd9\u4e9b\u98ce\u9669\uff0c\u8bbe\u8ba1\u4e86\u53d7\u63a7\u5b9e\u9a8c\u4ee5\u9694\u79bb\u6bcf\u79cd\u6545\u969c\u6a21\u5f0f\uff0c\u5e76\u89e3\u51b3\u4e86\u8bc4\u4f30AI\u79d1\u5b66\u5bb6\u7cfb\u7edf\u6240\u7279\u6709\u7684\u6311\u6218\u3002\u5bf9\u4e24\u4e2a\u77e5\u540d\u7684\u5f00\u6e90AI\u79d1\u5b66\u5bb6\u7cfb\u7edf\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5229\u7528\u5b8c\u6574\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\u7684\u8ddf\u8e2a\u65e5\u5fd7\u548c\u4ee3\u7801\u6765\u6709\u6548\u68c0\u6d4b\u6545\u969c\u3002", "result": "\u5bf9\u4e24\u4e2a\u77e5\u540d\u5f00\u6e90AI\u79d1\u5b66\u5bb6\u7cfb\u7edf\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u5b58\u5728\u4e00\u7cfb\u5217\u4e25\u91cd\u7a0b\u5ea6\u4e0d\u4e00\u7684\u5931\u8d25\uff0c\u8fd9\u4e9b\u5931\u8d25\u5728\u5b9e\u8df5\u4e2d\u5f88\u5bb9\u6613\u88ab\u5ffd\u89c6\u3002\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u8bbf\u95ee\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\u7684\u5b8c\u6574\u8ddf\u8e2a\u65e5\u5fd7\u548c\u4ee3\u7801\u6bd4\u4ec5\u68c0\u67e5\u6700\u7ec8\u8bba\u6587\u80fd\u66f4\u6709\u6548\u5730\u68c0\u6d4b\u6b64\u7c7b\u6545\u969c\u3002", "conclusion": "\u4e3a\u786e\u4fdd\u900f\u660e\u5ea6\u3001\u95ee\u8d23\u5236\u548c\u53ef\u590d\u73b0\u6027\uff0c\u5efa\u8bae\u671f\u520a\u548c\u4f1a\u8bae\u5728\u8bc4\u4f30AI\u751f\u6210\u7684\u7814\u7a76\u65f6\uff0c\u5f3a\u5236\u8981\u6c42\u63d0\u4ea4\u8bba\u6587\u7684\u540c\u65f6\u9644\u5e26\u5b8c\u6574\u7684\u8ddf\u8e2a\u65e5\u5fd7\u548c\u4ee3\u7801\u7b49\u4eba\u5de5\u5236\u54c1\u3002"}}
{"id": "2509.08163", "pdf": "https://arxiv.org/pdf/2509.08163", "abs": "https://arxiv.org/abs/2509.08163", "authors": ["Ho Ming Lee", "Katrien Antonio", "Benjamin Avanzi", "Lorenzo Marchi", "Rui Zhou"], "title": "Machine Learning with Multitype Protected Attributes: Intersectional Fairness through Regularisation", "categories": ["cs.LG", "q-fin.RM", "stat.AP", "stat.ML", "90B50, 62P05, 62H20, 68T07"], "comment": null, "summary": "Ensuring equitable treatment (fairness) across protected attributes (such as\ngender or ethnicity) is a critical issue in machine learning. Most existing\nliterature focuses on binary classification, but achieving fairness in\nregression tasks-such as insurance pricing or hiring score assessments-is\nequally important. Moreover, anti-discrimination laws also apply to continuous\nattributes, such as age, for which many existing methods are not applicable. In\npractice, multiple protected attributes can exist simultaneously; however,\nmethods targeting fairness across several attributes often overlook so-called\n\"fairness gerrymandering\", thereby ignoring disparities among intersectional\nsubgroups (e.g., African-American women or Hispanic men). In this paper, we\npropose a distance covariance regularisation framework that mitigates the\nassociation between model predictions and protected attributes, in line with\nthe fairness definition of demographic parity, and that captures both linear\nand nonlinear dependencies. To enhance applicability in the presence of\nmultiple protected attributes, we extend our framework by incorporating two\nmultivariate dependence measures based on distance covariance: the previously\nproposed joint distance covariance (JdCov) and our novel concatenated distance\ncovariance (CCdCov), which effectively address fairness gerrymandering in both\nregression and classification tasks involving protected attributes of various\ntypes. We discuss and illustrate how to calibrate regularisation strength,\nincluding a method based on Jensen-Shannon divergence, which quantifies\ndissimilarities in prediction distributions across groups. We apply our\nframework to the COMPAS recidivism dataset and a large motor insurance claims\ndataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8ddd\u79bb\u534f\u65b9\u5dee\u7684\u6b63\u5219\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u4e2d\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u591a\u91cd\u53d7\u4fdd\u62a4\u5c5e\u6027\u548c\u4ea4\u53c9\u5b50\u7fa4\u4f53\u5dee\u5f02\uff08\u5373\u201c\u516c\u5e73\u6027\u9009\u533a\u5212\u5206\u201d\uff09\u65b9\u9762\uff0c\u901a\u8fc7\u5f15\u5165\u8054\u5408\u8ddd\u79bb\u534f\u65b9\u5dee\uff08JdCov\uff09\u548c\u8fde\u63a5\u8ddd\u79bb\u534f\u65b9\u5dee\uff08CCdCov\uff09\u6765\u63d0\u5347\u9002\u7528\u6027\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\uff08\u5c24\u5176\u662f\u5728\u53d7\u4fdd\u62a4\u5c5e\u6027\u4e0a\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u4e8c\u5143\u5206\u7c7b\uff0c\u5ffd\u89c6\u4e86\u56de\u5f52\u4efb\u52a1\uff08\u5982\u4fdd\u9669\u5b9a\u4ef7\u3001\u62db\u8058\u8bc4\u4f30\uff09\u4e2d\u7684\u516c\u5e73\u6027\u9700\u6c42\u3002\u540c\u65f6\uff0c\u8bb8\u591a\u73b0\u6709\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u5e74\u9f84\u7b49\u8fde\u7eed\u53d7\u4fdd\u62a4\u5c5e\u6027\uff0c\u4e14\u5728\u5904\u7406\u591a\u91cd\u53d7\u4fdd\u62a4\u5c5e\u6027\u65f6\uff0c\u5e38\u5ffd\u7565\u201c\u516c\u5e73\u6027\u9009\u533a\u5212\u5206\u201d\u95ee\u9898\uff0c\u5bfc\u81f4\u4ea4\u53c9\u5b50\u7fa4\u4f53\uff08\u5982\u7279\u5b9a\u6c11\u65cf\u5973\u6027\uff09\u95f4\u7684\u5dee\u5f02\u88ab\u5ffd\u89c6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8ddd\u79bb\u534f\u65b9\u5dee\u6b63\u5219\u5316\u6846\u67b6\uff0c\u65e8\u5728\u6839\u636e\u4eba\u53e3\u7edf\u8ba1\u5b66\u5e73\u7b49\u539f\u5219\uff0c\u51cf\u5f31\u6a21\u578b\u9884\u6d4b\u4e0e\u53d7\u4fdd\u62a4\u5c5e\u6027\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u5e76\u80fd\u6355\u83b7\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\u3002\u4e3a\u5904\u7406\u591a\u91cd\u53d7\u4fdd\u62a4\u5c5e\u6027\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u4e24\u79cd\u57fa\u4e8e\u8ddd\u79bb\u534f\u65b9\u5dee\u7684\u591a\u5143\u4f9d\u8d56\u6027\u5ea6\u91cf\u8fdb\u884c\u6269\u5c55\uff1a\u5df2\u6709\u7684\u8054\u5408\u8ddd\u79bb\u534f\u65b9\u5dee\uff08JdCov\uff09\u548c\u65b0\u63d0\u51fa\u7684\u8fde\u63a5\u8ddd\u79bb\u534f\u65b9\u5dee\uff08CCdCov\uff09\u3002\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u6d89\u53ca\u591a\u79cd\u7c7b\u578b\u53d7\u4fdd\u62a4\u5c5e\u6027\u7684\u201c\u516c\u5e73\u6027\u9009\u533a\u5212\u5206\u201d\u95ee\u9898\u3002\u540c\u65f6\uff0c\u8ba8\u8bba\u4e86\u5305\u62ec\u57fa\u4e8eJensen-Shannon\u6563\u5ea6\u7684\u65b9\u6cd5\u5728\u5185\u7684\u6b63\u5219\u5316\u5f3a\u5ea6\u6821\u51c6\u6280\u672f\u3002", "result": "\u5c06\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5e94\u7528\u4e8eCOMPAS\u7d2f\u72af\u6570\u636e\u96c6\u548c\u5927\u578b\u6c7d\u8f66\u4fdd\u9669\u7d22\u8d54\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8ddd\u79bb\u534f\u65b9\u5dee\u6b63\u5219\u5316\u53ca\u6539\u8fdb\u7684\u591a\u5143\u4f9d\u8d56\u6027\u5ea6\u91cf\uff08JdCov\u548cCCdCov\uff09\uff0c\u80fd\u591f\u6709\u6548\u51cf\u8f7b\u6a21\u578b\u9884\u6d4b\u4e0e\u53d7\u4fdd\u62a4\u5c5e\u6027\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u5904\u7406\u591a\u91cd\u5c5e\u6027\uff0c\u5e76\u89e3\u51b3\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u201c\u516c\u5e73\u6027\u9009\u533a\u5212\u5206\u201d\u5bfc\u81f4\u7684\u4ea4\u53c9\u5b50\u7fa4\u4f53\u5dee\u5f02\u95ee\u9898\u3002"}}
{"id": "2509.08345", "pdf": "https://arxiv.org/pdf/2509.08345", "abs": "https://arxiv.org/abs/2509.08345", "authors": ["Alejandro Andrade-Lotero", "Lee Becker", "Joshua Southerland", "Scott Hellman"], "title": "Toward Subtrait-Level Model Explainability in Automated Writing Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to National Council on Measurement in Education (NCME) 2025\n  Annual Meeting", "summary": "Subtrait (latent-trait components) assessment presents a promising path\ntoward enhancing transparency of automated writing scores. We prototype\nexplainability and subtrait scoring with generative language models and show\nmodest correlation between human subtrait and trait scores, and between\nautomated and human subtrait scores. Our approach provides details to demystify\nscores for educators and students.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b50\u7279\u8d28\u8bc4\u4f30\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u5199\u4f5c\u8bc4\u5206\u7684\u900f\u660e\u5ea6\uff0c\u5e76\u53d1\u73b0\u81ea\u52a8\u5316\u4e0e\u4eba\u5de5\u8bc4\u5206\u4e4b\u95f4\u5b58\u5728\u9002\u5ea6\u76f8\u5173\u6027\u3002", "motivation": "\u589e\u5f3a\u81ea\u52a8\u5199\u4f5c\u5206\u6570\u7684\u900f\u660e\u5ea6\uff0c\u4f7f\u8bc4\u5206\u8fc7\u7a0b\u66f4\u6613\u4e8e\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u548c\u5b50\u7279\u8d28\u8bc4\u5206\u7684\u539f\u578b\u5f00\u53d1\u3002", "result": "\u5728\u4eba\u7c7b\u5b50\u7279\u8d28\u4e0e\u7279\u8d28\u5206\u6570\u4e4b\u95f4\uff0c\u4ee5\u53ca\u81ea\u52a8\u5316\u4e0e\u4eba\u7c7b\u5b50\u7279\u8d28\u5206\u6570\u4e4b\u95f4\uff0c\u53d1\u73b0\u4e86\u9002\u5ea6\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u63d0\u4f9b\u8be6\u7ec6\u4fe1\u606f\uff0c\u6709\u52a9\u4e8e\u6d88\u9664\u6559\u80b2\u8005\u548c\u5b66\u751f\u5bf9\u81ea\u52a8\u5199\u4f5c\u5206\u6570\u7684\u56f0\u60d1\uff0c\u63d0\u5347\u5176\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.08260", "pdf": "https://arxiv.org/pdf/2509.08260", "abs": "https://arxiv.org/abs/2509.08260", "authors": ["Chi Zhang", "Xiang Zhang", "Chenxu Jiang", "Gui-Song Xia", "Lei Yu"], "title": "EVDI++: Event-based Video Deblurring and Interpolation via Self-Supervised Learning", "categories": ["cs.CV"], "comment": "18 pages", "summary": "Frame-based cameras with extended exposure times often produce perceptible\nvisual blurring and information loss between frames, significantly degrading\nvideo quality. To address this challenge, we introduce EVDI++, a unified\nself-supervised framework for Event-based Video Deblurring and Interpolation\nthat leverages the high temporal resolution of event cameras to mitigate motion\nblur and enable intermediate frame prediction. Specifically, the Learnable\nDouble Integral (LDI) network is designed to estimate the mapping relation\nbetween reference frames and sharp latent images. Then, we refine the coarse\nresults and optimize overall training efficiency by introducing a\nlearning-based division reconstruction module, enabling images to be converted\nwith varying exposure intervals. We devise an adaptive parameter-free fusion\nstrategy to obtain the final results, utilizing the confidence embedded in the\nLDI outputs of concurrent events. A self-supervised learning framework is\nproposed to enable network training with real-world blurry videos and events by\nexploring the mutual constraints among blurry frames, latent images, and event\nstreams. We further construct a dataset with real-world blurry images and\nevents using a DAVIS346c camera, demonstrating the generalizability of the\nproposed EVDI++ in real-world scenarios. Extensive experiments on both\nsynthetic and real-world datasets show that our method achieves\nstate-of-the-art performance in video deblurring and interpolation tasks.", "AI": {"tldr": "EVDI++\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u89e3\u51b3\u4f20\u7edf\u5e27\u76f8\u673a\u56e0\u957f\u65f6\u95f4\u66dd\u5149\u5bfc\u81f4\u7684\u89c6\u9891\u6a21\u7cca\u548c\u5e27\u95f4\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5b9e\u73b0\u89c6\u9891\u53bb\u6a21\u7cca\u548c\u63d2\u5e27\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5e27\u76f8\u673a\u957f\u65f6\u95f4\u66dd\u5149\u4f1a\u5bfc\u81f4\u611f\u77e5\u4e0a\u7684\u89c6\u89c9\u6a21\u7cca\u548c\u5e27\u95f4\u4fe1\u606f\u635f\u5931\uff0c\u4e25\u91cd\u964d\u4f4e\u89c6\u9891\u8d28\u91cf\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u7f13\u89e3\u8fd0\u52a8\u6a21\u7cca\u5e76\u5b9e\u73b0\u4e2d\u95f4\u5e27\u9884\u6d4b\uff0c\u4e9f\u9700\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165EVDI++\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u5305\u542b\uff1a1. \u53ef\u5b66\u4e60\u53cc\u79ef\u5206(LDI)\u7f51\u7edc\uff0c\u7528\u4e8e\u4f30\u8ba1\u53c2\u8003\u5e27\u548c\u6e05\u6670\u6f5c\u5728\u56fe\u50cf\u7684\u6620\u5c04\u5173\u7cfb\uff1b2. \u57fa\u4e8e\u5b66\u4e60\u7684\u5206\u533a\u91cd\u5efa\u6a21\u5757\uff0c\u7cbe\u70bc\u7ed3\u679c\u5e76\u4f18\u5316\u8bad\u7ec3\u6548\u7387\uff0c\u652f\u6301\u4e0d\u540c\u66dd\u5149\u95f4\u9694\u8f6c\u6362\uff1b3. \u81ea\u9002\u5e94\u65e0\u53c2\u6570\u878d\u5408\u7b56\u7565\uff0c\u5229\u7528\u5e76\u53d1\u4e8b\u4ef6LDI\u8f93\u51fa\u7684\u7f6e\u4fe1\u5ea6\u83b7\u53d6\u6700\u7ec8\u7ed3\u679c\uff1b4. \u901a\u8fc7\u63a2\u7d22\u6a21\u7cca\u5e27\u3001\u6f5c\u5728\u56fe\u50cf\u548c\u4e8b\u4ef6\u6d41\u4e4b\u95f4\u7684\u76f8\u4e92\u7ea6\u675f\uff0c\u63d0\u51fa\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4ee5\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u8bad\u7ec3\uff1b5. \u6784\u5efa\u4e86\u5305\u542b\u771f\u5b9e\u4e16\u754c\u6a21\u7cca\u56fe\u50cf\u548c\u4e8b\u4ef6\u7684\u6570\u636e\u96c6\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEVDI++\u5728\u89c6\u9891\u53bb\u6a21\u7cca\u548c\u63d2\u5e27\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "EVDI++\u6210\u529f\u5730\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u89e3\u51b3\u4e86\u89c6\u9891\u6a21\u7cca\u548c\u63d2\u5e27\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5176\u521b\u65b0\u7684\u81ea\u76d1\u7763\u6846\u67b6\u548c\u6a21\u5757\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u53bb\u6a21\u7cca\u548c\u63d2\u5e27\u6548\u679c\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.08785", "pdf": "https://arxiv.org/pdf/2509.08785", "abs": "https://arxiv.org/abs/2509.08785", "authors": ["Anup Tuladhar", "Araz Minhas", "Adam Kirton", "Eli Kinney-Lang"], "title": "Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making", "categories": ["cs.AI", "cs.MA", "stat.ML"], "comment": "Extended Abstract for RLDM 2025", "summary": "We present a preliminary experimental platform that explores how narrative\nelements might shape AI decision-making by combining reinforcement learning\n(RL) with language model reasoning. While AI systems can now both make\ndecisions and engage in narrative reasoning, these capabilities have mostly\nbeen studied separately. Our platform attempts to bridge this gap using a\ndual-system architecture to examine how narrative frameworks could influence\nreward-based learning. The system comprises a reinforcement learning policy\nthat suggests actions based on past experience, and a language model that\nprocesses these suggestions through different narrative frameworks to guide\ndecisions. This setup enables initial experimentation with narrative elements\nwhile maintaining consistent environment and reward structures. We implement\nthis architecture in a configurable gridworld environment, where agents receive\nboth policy suggestions and information about their surroundings. The\nplatform's modular design facilitates controlled testing of environmental\ncomplexity, narrative parameters, and the interaction between reinforcement\nlearning and narrative-based decisions. Our logging system captures basic\ndecision metrics, from RL policy values to language model reasoning to action\nselection patterns. While preliminary, this implementation provides a\nfoundation for studying how different narrative frameworks might affect\nreward-based decisions and exploring potential interactions between\noptimization-based learning and symbolic reasoning in AI systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u521d\u6b65\u5b9e\u9a8c\u5e73\u53f0\uff0c\u65e8\u5728\u63a2\u7d22\u53d9\u4e8b\u5143\u7d20\u5982\u4f55\u5f71\u54cdAI\u51b3\u7b56\u3002", "motivation": "AI\u7684\u51b3\u7b56\u80fd\u529b\u548c\u53d9\u4e8b\u63a8\u7406\u80fd\u529b\u901a\u5e38\u88ab\u5206\u5f00\u7814\u7a76\uff0c\u672c\u5e73\u53f0\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u9e3f\u6c9f\uff0c\u63a2\u7d22\u53d9\u4e8b\u6846\u67b6\u5982\u4f55\u5f71\u54cd\u57fa\u4e8e\u5956\u52b1\u7684\u5b66\u4e60\u3002", "method": "\u91c7\u7528\u53cc\u7cfb\u7edf\u67b6\u6784\uff0c\u5305\u542b\u4e00\u4e2a\u57fa\u4e8e\u8fc7\u5f80\u7ecf\u9a8c\u5efa\u8bae\u884c\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u548c\u4e00\u4e2a\u901a\u8fc7\u4e0d\u540c\u53d9\u4e8b\u6846\u67b6\u5904\u7406\u8fd9\u4e9b\u5efa\u8bae\u4ee5\u6307\u5bfc\u51b3\u7b56\u7684\u8bed\u8a00\u6a21\u578b\u3002\u8be5\u67b6\u6784\u5728\u4e00\u4e2a\u53ef\u914d\u7f6e\u7684\u7f51\u683c\u4e16\u754c\u73af\u5883\u4e2d\u5b9e\u73b0\uff0c\u5e76\u5177\u5907\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u8be6\u7ec6\u7684\u65e5\u5fd7\u7cfb\u7edf\uff0c\u7528\u4e8e\u6355\u83b7\u51b3\u7b56\u6307\u6807\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u521d\u6b65\u7684\u5b9e\u9a8c\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u80fd\u591f\u5b9e\u73b0\u53d9\u4e8b\u5143\u7d20\u4e0e\u5956\u52b1\u7ed3\u6784\u4fdd\u6301\u4e00\u81f4\u7684\u521d\u6b65\u5b9e\u9a8c\uff0c\u5e76\u80fd\u8fdb\u884c\u73af\u5883\u590d\u6742\u5ea6\u3001\u53d9\u4e8b\u53c2\u6570\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u4e0e\u53d9\u4e8b\u51b3\u7b56\u4e4b\u95f4\u4ea4\u4e92\u7684\u53d7\u63a7\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u521d\u6b65\u5b9e\u65bd\u4e3a\u7814\u7a76\u4e0d\u540c\u53d9\u4e8b\u6846\u67b6\u5982\u4f55\u5f71\u54cd\u57fa\u4e8e\u5956\u52b1\u7684\u51b3\u7b56\uff0c\u4ee5\u53ca\u63a2\u7d22AI\u7cfb\u7edf\u4e2d\u57fa\u4e8e\u4f18\u5316\u7684\u5b66\u4e60\u4e0e\u7b26\u53f7\u63a8\u7406\u4e4b\u95f4\u7684\u6f5c\u5728\u4e92\u52a8\uff0c\u5960\u5b9a\u4e86\u91cd\u8981\u7684\u57fa\u7840\u3002"}}
{"id": "2509.08176", "pdf": "https://arxiv.org/pdf/2509.08176", "abs": "https://arxiv.org/abs/2509.08176", "authors": ["Honghui Du", "Leandro Minku", "Huiyu Zhou"], "title": "MARLINE: Multi-Source Mapping Transfer Learning for Non-Stationary Environments", "categories": ["cs.LG", "cs.AI"], "comment": "Published in the 2020 IEEE International Conference on Data Mining\n  (ICDM)", "summary": "Concept drift is a major problem in online learning due to its impact on the\npredictive performance of data stream mining systems. Recent studies have\nstarted exploring data streams from different sources as a strategy to tackle\nconcept drift in a given target domain. These approaches make the assumption\nthat at least one of the source models represents a concept similar to the\ntarget concept, which may not hold in many real-world scenarios. In this paper,\nwe propose a novel approach called Multi-source mApping with tRansfer LearnIng\nfor Non-stationary Environments (MARLINE). MARLINE can benefit from knowledge\nfrom multiple data sources in non-stationary environments even when source and\ntarget concepts do not match. This is achieved by projecting the target concept\nto the space of each source concept, enabling multiple source sub-classifiers\nto contribute towards the prediction of the target concept as part of an\nensemble. Experiments on several synthetic and real-world datasets show that\nMARLINE was more accurate than several state-of-the-art data stream learning\napproaches.", "AI": {"tldr": "\u63d0\u51faMARLINE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u76ee\u6807\u6982\u5ff5\u6295\u5f71\u5230\u6e90\u6982\u5ff5\u7a7a\u95f4\uff0c\u5373\u4f7f\u6e90\u4e0e\u76ee\u6807\u6982\u5ff5\u4e0d\u5339\u914d\uff0c\u4e5f\u80fd\u5728\u6982\u5ff5\u6f02\u79fb\u73af\u5883\u4e0b\u5229\u7528\u591a\u6e90\u77e5\u8bc6\u63d0\u5347\u6570\u636e\u6d41\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u6982\u5ff5\u6f02\u79fb\u4e25\u91cd\u5f71\u54cd\u5728\u7ebf\u5b66\u4e60\u7684\u9884\u6d4b\u6027\u80fd\u3002\u73b0\u6709\u5229\u7528\u591a\u6e90\u6570\u636e\u6d41\u89e3\u51b3\u6982\u5ff5\u6f02\u79fb\u7684\u65b9\u6cd5\uff0c\u5047\u8bbe\u6e90\u6982\u5ff5\u4e0e\u76ee\u6807\u6982\u5ff5\u76f8\u4f3c\uff0c\u8fd9\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2d\u5f80\u5f80\u4e0d\u6210\u7acb\u3002", "method": "\u63d0\u51faMARLINE\uff08Multi-source mApping with tRansfer LearnIng for Non-stationary Environments\uff09\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u76ee\u6807\u6982\u5ff5\u6295\u5f71\u5230\u6bcf\u4e2a\u6e90\u6982\u5ff5\u7684\u7a7a\u95f4\uff0c\u4f7f\u591a\u4e2a\u6e90\u5b50\u5206\u7c7b\u5668\u80fd\u4ee5\u96c6\u6210\u65b9\u5f0f\u534f\u540c\u9884\u6d4b\u76ee\u6807\u6982\u5ff5\uff0c\u4ece\u800c\u5728\u6e90\u4e0e\u76ee\u6807\u6982\u5ff5\u4e0d\u5339\u914d\u65f6\u4e5f\u80fd\u4ece\u591a\u6e90\u77e5\u8bc6\u4e2d\u83b7\u76ca\u3002", "result": "\u5728\u591a\u4e2a\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMARLINE\u6bd4\u51e0\u79cd\u6700\u5148\u8fdb\u7684\u6570\u636e\u6d41\u5b66\u4e60\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "MARLINE\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6e90\u6570\u636e\u6d41\u5b66\u4e60\u5728\u6982\u5ff5\u6f02\u79fb\u4e2d\u6e90\u4e0e\u76ee\u6807\u6982\u5ff5\u4e0d\u5339\u914d\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u7ebf\u5b66\u4e60\u7cfb\u7edf\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.08355", "pdf": "https://arxiv.org/pdf/2509.08355", "abs": "https://arxiv.org/abs/2509.08355", "authors": ["Yashad Samant", "Lee Becker", "Scott Hellman", "Bradley Behan", "Sarah Hughes", "Joshua Southerland"], "title": "Automatic Detection of Inauthentic Templated Responses in English Language Assessments", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to National Council on Measurement in Education (NCME) 2025\n  Annual Meeting", "summary": "In high-stakes English Language Assessments, low-skill test takers may employ\nmemorized materials called ``templates'' on essay questions to ``game'' or fool\nthe automated scoring system. In this study, we introduce the automated\ndetection of inauthentic, templated responses (AuDITR) task, describe a machine\nlearning-based approach to this task and illustrate the importance of regularly\nupdating these models in production.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u81ea\u52a8\u5316\u68c0\u6d4b\u82f1\u8bed\u8bed\u8a00\u8bc4\u4f30\u4e2d\u6a21\u677f\u5316\u4f5c\u5f0a\u7b54\u6848\u7684\u4efb\u52a1\uff08AuDITR\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u4e86\u6a21\u578b\u5b9a\u671f\u66f4\u65b0\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u82f1\u8bed\u8bed\u8a00\u8bc4\u4f30\u4e2d\uff0c\u4f4e\u6280\u80fd\u8003\u751f\u53ef\u80fd\u4f7f\u7528\u8bb0\u5fc6\u6a21\u677f\u6765\u6b3a\u9a97\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u81ea\u52a8\u5316\u68c0\u6d4b\u4e0d\u771f\u5b9e\u3001\u6a21\u677f\u5316\u7b54\u6848\uff08AuDITR\uff09\u7684\u4efb\u52a1\uff0c\u5e76\u63cf\u8ff0\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u6b64\u4efb\u52a1\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u5b9a\u671f\u66f4\u65b0\u8fd9\u4e9b\u68c0\u6d4b\u6a21\u578b\u5bf9\u4e8e\u5176\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u4e3a\u4e86\u6709\u6548\u5e94\u5bf9\u6a21\u677f\u5316\u4f5c\u5f0a\uff0c\u5f00\u53d1\u548c\u90e8\u7f72\u7684\u673a\u5668\u5b66\u4e60\u68c0\u6d4b\u6a21\u578b\u9700\u8981\u6301\u7eed\u7684\u66f4\u65b0\u548c\u7ef4\u62a4\u3002"}}
{"id": "2509.08265", "pdf": "https://arxiv.org/pdf/2509.08265", "abs": "https://arxiv.org/abs/2509.08265", "authors": ["Long Gao", "Yunhe Zhang", "Yan Jiang", "Weiying Xie", "Yunsong Li"], "title": "Hyperspectral Mamba for Hyperspectral Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral object tracking holds great promise due to the rich spectral\ninformation and fine-grained material distinctions in hyperspectral images,\nwhich are beneficial in challenging scenarios. While existing hyperspectral\ntrackers have made progress by either transforming hyperspectral data into\nfalse-color images or incorporating modality fusion strategies, they often fail\nto capture the intrinsic spectral information, temporal dependencies, and\ncross-depth interactions. To address these limitations, a new hyperspectral\nobject tracking network equipped with Mamba (HyMamba), is proposed. It unifies\nspectral, cross-depth, and temporal modeling through state space modules\n(SSMs). The core of HyMamba lies in the Spectral State Integration (SSI)\nmodule, which enables progressive refinement and propagation of spectral\nfeatures with cross-depth and temporal spectral information. Embedded within\neach SSI, the Hyperspectral Mamba (HSM) module is introduced to learn spatial\nand spectral information synchronously via three directional scanning SSMs.\nBased on SSI and HSM, HyMamba constructs joint features from false-color and\nhyperspectral inputs, and enhances them through interaction with original\nspectral features extracted from raw hyperspectral images. Extensive\nexperiments conducted on seven benchmark datasets demonstrate that HyMamba\nachieves state-of-the-art performance. For instance, it achieves 73.0\\% of the\nAUC score and 96.3\\% of the DP@20 score on the HOTC2020 dataset. The code will\nbe released at https://github.com/lgao001/HyMamba.", "AI": {"tldr": "\u63d0\u51faHyMamba\uff0c\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u8d85\u5149\u8c31\u76ee\u6807\u8ddf\u8e2a\u7f51\u7edc\uff0c\u901a\u8fc7\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\uff08SSMs\uff09\u7edf\u4e00\u5efa\u6a21\u5149\u8c31\u3001\u8de8\u6df1\u5ea6\u548c\u65f6\u95f4\u4fe1\u606f\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8d85\u5149\u8c31\u8ddf\u8e2a\u5668\u672a\u80fd\u6709\u6548\u6355\u83b7\u5185\u5728\u5149\u8c31\u4fe1\u606f\u3001\u65f6\u95f4\u4f9d\u8d56\u6027\u53ca\u8de8\u6df1\u5ea6\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u672c\u6587\u63d0\u51faHyMamba\u8d85\u5149\u8c31\u76ee\u6807\u8ddf\u8e2a\u7f51\u7edc\uff0c\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\uff08SSMs\uff09\u7edf\u4e00\u5149\u8c31\u3001\u8de8\u6df1\u5ea6\u548c\u65f6\u95f4\u5efa\u6a21\u3002\u5176\u6838\u5fc3\u662f\u5149\u8c31\u72b6\u6001\u96c6\u6210\uff08SSI\uff09\u6a21\u5757\uff0c\u5b9e\u73b0\u5149\u8c31\u7279\u5f81\u7684\u6e10\u8fdb\u5f0f\u63d0\u70bc\u548c\u4f20\u64ad\uff1bSSI\u5185\u5d4c\u8d85\u5149\u8c31Mamba\uff08HSM\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u4e09\u5411\u626b\u63cfSSMs\u540c\u6b65\u5b66\u4e60\u7a7a\u95f4\u548c\u5149\u8c31\u4fe1\u606f\u3002HyMamba\u901a\u8fc7\u4ea4\u4e92\u4f5c\u7528\uff0c\u7ed3\u5408\u4f2a\u5f69\u8272\u548c\u8d85\u5149\u8c31\u8f93\u5165\u6784\u5efa\u8054\u5408\u7279\u5f81\uff0c\u5e76\u5229\u7528\u539f\u59cb\u5149\u8c31\u7279\u5f81\u8fdb\u884c\u589e\u5f3a\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eHyMamba\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728HOTC2020\u6570\u636e\u96c6\u4e0a\uff0cAUC\u5206\u6570\u4e3a73.0%\uff0cDP@20\u5206\u6570\u4e3a96.3%\u3002", "conclusion": "HyMamba\u901a\u8fc7\u6709\u6548\u5efa\u6a21\u5149\u8c31\u3001\u8de8\u6df1\u5ea6\u548c\u65f6\u95f4\u4fe1\u606f\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u8d85\u5149\u8c31\u8ddf\u8e2a\u5668\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d85\u5149\u8c31\u76ee\u6807\u8ddf\u8e2a\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10946", "pdf": "https://arxiv.org/pdf/2505.10946", "abs": "https://arxiv.org/abs/2505.10946", "authors": ["Li Qiao", "Mahdi Boloursaz Mashhadi", "Zhen Gao", "Robert Schober", "Deniz G\u00fcnd\u00fcz"], "title": "ToDMA: Large Model-Driven Token-Domain Multiple Access for Semantic Communications", "categories": ["cs.IT", "cs.AI", "cs.LG", "eess.SP", "math.IT"], "comment": "Submitted to IEEE journals", "summary": "Token communications (TokCom) is an emerging generative semantic\ncommunication concept that reduces transmission rates by using context and\nmultimodal large language model (MLLM)-based token processing, with tokens\nserving as universal semantic units across modalities. In this paper, we\npropose a semantic multiple access scheme in the token domain, referred to as\ntoken domain multiple access (ToDMA), where a large number of devices share a\ntoken codebook and a modulation codebook for source and channel coding,\nrespectively. Specifically, each transmitter first tokenizes its source signal\nand modulate each token to a codeword. At the receiver, compressed sensing is\nemployed first to detect active tokens and the corresponding channel state\ninformation (CSI) from the superposed signals. Then, the source token sequences\nare reconstructed by clustering the token-associated CSI across multiple time\nslots. In case of token collisions, some active tokens cannot be assigned and\nsome positions in the reconstructed token sequences are empty. We propose to\nuse pre-trained MLLMs to leverage the context, predict masked tokens, and thus\nmitigate token collisions. Simulation results demonstrate the effectiveness of\nthe proposed ToDMA framework for both text and image transmission tasks,\nachieving significantly lower latency compared to context-unaware orthogonal\ncommunication schemes, while also delivering superior distortion and perceptual\nquality compared to state-of-the-art context-unaware non-orthogonal\ncommunication methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ee4\u724c\u57df\u591a\u5740\u63a5\u5165\uff08ToDMA\uff09\u65b9\u6848\uff0c\u5229\u7528MLLM\u548c\u538b\u7f29\u611f\u77e5\u5b9e\u73b0\u9ad8\u6548\u7684\u751f\u6210\u5f0f\u8bed\u4e49\u901a\u4fe1\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u6587\u672c\u548c\u56fe\u50cf\u4f20\u8f93\u8d28\u91cf\uff0c\u5e76\u80fd\u6709\u6548\u7f13\u89e3\u4ee4\u724c\u51b2\u7a81\u3002", "motivation": "\u4ee4\u724c\u901a\u4fe1\uff08TokCom\uff09\u901a\u8fc7\u4e0a\u4e0b\u6587\u548cMLLM\u51cf\u5c11\u4f20\u8f93\u901f\u7387\uff0c\u4f5c\u4e3a\u65b0\u5174\u7684\u751f\u6210\u5f0f\u8bed\u4e49\u901a\u4fe1\u6982\u5ff5\u3002\u672c\u6587\u65e8\u5728\u4e3a\u5927\u91cf\u8bbe\u5907\u63d0\u4f9b\u4e00\u79cd\u5728\u4ee4\u724c\u57df\u5185\u9ad8\u6548\u5171\u4eab\u8d44\u6e90\u5e76\u8fdb\u884c\u8bed\u4e49\u591a\u5740\u63a5\u5165\u7684\u65b9\u6848\u3002", "method": "\u53d1\u9001\u7aef\u5c06\u6e90\u4fe1\u53f7\u4ee4\u724c\u5316\u5e76\u8c03\u5236\u4e3a\u7801\u5b57\u3002\u63a5\u6536\u7aef\u9996\u5148\u5229\u7528\u538b\u7f29\u611f\u77e5\u68c0\u6d4b\u53e0\u52a0\u4fe1\u53f7\u4e2d\u7684\u6d3b\u8dc3\u4ee4\u724c\u548c\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\uff0c\u7136\u540e\u901a\u8fc7\u805a\u7c7b\u591a\u65f6\u9699\u7684\u4ee4\u724c\u76f8\u5173CSI\u91cd\u5efa\u6e90\u4ee4\u724c\u5e8f\u5217\u3002\u4e3a\u7f13\u89e3\u4ee4\u724c\u51b2\u7a81\uff0c\u91c7\u7528\u9884\u8bad\u7ec3\u7684MLLM\u5229\u7528\u4e0a\u4e0b\u6587\u9884\u6d4b\u88ab\u63a9\u76d6\u7684\u4ee4\u724c\u3002", "result": "ToDMA\u6846\u67b6\u5728\u6587\u672c\u548c\u56fe\u50cf\u4f20\u8f93\u4efb\u52a1\u4e2d\u5747\u6709\u6548\u3002\u4e0e\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u6b63\u4ea4\u901a\u4fe1\u65b9\u6848\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002\u4e0e\u73b0\u6709\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u975e\u6b63\u4ea4\u901a\u4fe1\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63d0\u4f9b\u4e86\u66f4\u4f18\u8d8a\u7684\u5931\u771f\u548c\u611f\u77e5\u8d28\u91cf\u3002", "conclusion": "ToDMA\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u8bed\u4e49\u591a\u5740\u63a5\u5165\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4ee4\u724c\u5904\u7406\u3001\u538b\u7f29\u611f\u77e5\u548cMLLM\u7684\u4e0a\u4e0b\u6587\u9884\u6d4b\u80fd\u529b\uff0c\u5728\u751f\u6210\u5f0f\u8bed\u4e49\u901a\u4fe1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4f20\u8f93\u6548\u7387\u548c\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u964d\u4f4e\u5ef6\u8fdf\u548c\u6539\u5584\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.08180", "pdf": "https://arxiv.org/pdf/2509.08180", "abs": "https://arxiv.org/abs/2509.08180", "authors": ["Paul Curry"], "title": "The Domain Mixed Unit: A New Neural Arithmetic Layer", "categories": ["cs.LG", "68T07", "I.2.6"], "comment": "7 pages, 5 tables, includes results on the NALM benchmark", "summary": "The Domain Mixed Unit (DMU) is a new neural arithmetic unit that learns a\nsingle parameter gate that mixes between log-space and linear-space\nrepresentations while performing either addition (DMU add) or subtraction (DMU\nsub). Two initializations are proposed for the DMU: one covering addition and\nmultiplication, and another covering subtraction and division. The DMU achieves\nstate-of-the-art performance on the NALM Benchmark, a dataset designed to test\nthe ability of neural arithmetic units to generalize arithmetic operations,\nspecifically performing with the highest percentage solved over all seeds on\nmultiplication and division. The DMU will be submitted as a pull request to the\nopen-source NALM benchmark, and its code is available on GitHub at\nhttps://github.com/marict?tab=repositories", "AI": {"tldr": "Domain Mixed Unit (DMU) \u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7b97\u672f\u5355\u5143\uff0c\u901a\u8fc7\u6df7\u5408\u5bf9\u6570\u548c\u7ebf\u6027\u7a7a\u95f4\u8868\u793a\u6765\u6267\u884c\u52a0\u51cf\u6cd5\u3002\u5b83\u5728NALM\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e58\u6cd5\u548c\u9664\u6cd5\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7b97\u672f\u5355\u5143\uff0c\u4ee5\u63d0\u9ad8\u795e\u7ecf\u7f51\u7edc\u5728\u7b97\u672f\u64cd\u4f5c\uff08\u7279\u522b\u662f\u4e58\u6cd5\u548c\u9664\u6cd5\uff09\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "method": "DMU\u662f\u4e00\u4e2a\u65b0\u7684\u795e\u7ecf\u7b97\u672f\u5355\u5143\uff0c\u5b83\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u5355\u53c2\u6570\u95e8\u63a7\u6765\u6df7\u5408\u5bf9\u6570\u7a7a\u95f4\u548c\u7ebf\u6027\u7a7a\u95f4\u8868\u793a\uff0c\u4ece\u800c\u6267\u884c\u52a0\u6cd5\uff08DMU add\uff09\u6216\u51cf\u6cd5\uff08DMU sub\uff09\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cdDMU\u521d\u59cb\u5316\u65b9\u6848\uff1a\u4e00\u79cd\u7528\u4e8e\u52a0\u6cd5\u548c\u4e58\u6cd5\uff0c\u53e6\u4e00\u79cd\u7528\u4e8e\u51cf\u6cd5\u548c\u9664\u6cd5\u3002", "result": "DMU\u5728NALM\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4e58\u6cd5\u548c\u9664\u6cd5\u4efb\u52a1\u4e2d\uff0c\u5176\u5728\u6240\u6709\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u90fd\u8fbe\u5230\u4e86\u6700\u9ad8\u7684\u89e3\u51b3\u767e\u5206\u6bd4\u3002", "conclusion": "DMU\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b0\u578b\u795e\u7ecf\u7b97\u672f\u5355\u5143\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u5728\u4e58\u6cd5\u548c\u9664\u6cd5\u7b49\u7b97\u672f\u64cd\u4f5c\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u7814\u7a76\u7684\u4ee3\u7801\u5c06\u5f00\u6e90\u5e76\u63d0\u4ea4\u81f3NALM\u57fa\u51c6\u6d4b\u8bd5\u9879\u76ee\u3002"}}
{"id": "2509.08358", "pdf": "https://arxiv.org/pdf/2509.08358", "abs": "https://arxiv.org/abs/2509.08358", "authors": ["Sergey Pletenev", "Daniil Moskovskiy", "Alexander Panchenko"], "title": "<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern Large Language Models (LLMs) are excellent at generating synthetic\ndata. However, their performance in sensitive domains such as text\ndetoxification has not received proper attention from the scientific community.\nThis paper explores the possibility of using LLM-generated synthetic toxic data\nas an alternative to human-generated data for training models for\ndetoxification. Using Llama 3 and Qwen activation-patched models, we generated\nsynthetic toxic counterparts for neutral texts from ParaDetox and SST-2\ndatasets. Our experiments show that models fine-tuned on synthetic data\nconsistently perform worse than those trained on human data, with a drop in\nperformance of up to 30% in joint metrics. The root cause is identified as a\ncritical lexical diversity gap: LLMs generate toxic content using a small,\nrepetitive vocabulary of insults that fails to capture the nuances and variety\nof human toxicity. These findings highlight the limitations of current LLMs in\nthis domain and emphasize the continued importance of diverse, human-annotated\ndata for building robust detoxification systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76LLM\u751f\u6210\u5408\u6210\u6bd2\u6027\u6570\u636e\u7528\u4e8e\u8bad\u7ec3\u6587\u672c\u89e3\u6bd2\u6a21\u578b\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u5408\u6210\u6570\u636e\u7f3a\u4e4f\u8bcd\u6c47\u591a\u6837\u6027\u3002", "motivation": "LLM\u5728\u751f\u6210\u5408\u6210\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6587\u672c\u89e3\u6bd2\u7b49\u654f\u611f\u9886\u57df\uff0c\u5176\u751f\u6210\u5408\u6210\u6570\u636e\u7684\u6027\u80fd\u53ca\u5e94\u7528\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u5173\u6ce8\u3002\u7814\u7a76\u65e8\u5728\u63a2\u8ba8LLM\u751f\u6210\u7684\u5408\u6210\u6bd2\u6027\u6570\u636e\u662f\u5426\u80fd\u6709\u6548\u66ff\u4ee3\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u6765\u8bad\u7ec3\u6587\u672c\u89e3\u6bd2\u6a21\u578b\u3002", "method": "\u4f7f\u7528Llama 3\u548cQwen\uff08\u6fc0\u6d3b\u8865\u4e01\u6a21\u578b\uff09\u4eceParaDetox\u548cSST-2\u6570\u636e\u96c6\u4e2d\u6027\u6587\u672c\u751f\u6210\u4e86\u5408\u6210\u7684\u6bd2\u6027\u5bf9\u5e94\u6587\u672c\u3002\u968f\u540e\uff0c\u5c06\u89e3\u6bd2\u6a21\u578b\u5728\u8fd9\u4e9b\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u4e0e\u5728\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5728\u5408\u6210\u6570\u636e\u4e0a\u5fae\u8c03\u7684\u6a21\u578b\u6027\u80fd\u59cb\u7ec8\u4e0d\u5982\u5728\u4eba\u7c7b\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5728\u7efc\u5408\u6307\u6807\u4e0a\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe30%\u3002\u5206\u6790\u8868\u660e\uff0c\u6839\u672c\u539f\u56e0\u5728\u4e8e\u5408\u6210\u6bd2\u6027\u6570\u636e\u5b58\u5728\u5173\u952e\u7684\u8bcd\u6c47\u591a\u6837\u6027\u9e3f\u6c9f\uff1aLLM\u751f\u6210\u7684\u6bd2\u6027\u5185\u5bb9\u8bcd\u6c47\u91cf\u5c0f\u4e14\u91cd\u590d\uff0c\u672a\u80fd\u6355\u6349\u4eba\u7c7b\u6bd2\u6027\u8868\u8fbe\u7684\u7ec6\u5fae\u5dee\u522b\u548c\u591a\u6837\u6027\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6bd2\u6027\u6570\u636e\u4ee5\u7528\u4e8e\u8bad\u7ec3\u6587\u672c\u89e3\u6bd2\u7cfb\u7edf\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u6784\u5efa\u5065\u58ee\u7684\u6587\u672c\u89e3\u6bd2\u7cfb\u7edf\u4ecd\u9700\u4f9d\u8d56\u591a\u6837\u5316\u3001\u4eba\u5de5\u6807\u6ce8\u7684\u6570\u636e\u3002"}}
{"id": "2509.08266", "pdf": "https://arxiv.org/pdf/2509.08266", "abs": "https://arxiv.org/abs/2509.08266", "authors": ["Saurav Sengupta", "Nazanin Moradinasab", "Jiebei Liu", "Donald E. Brown"], "title": "Examining Vision Language Models through Multi-dimensional Experiments with Vision and Text Features", "categories": ["cs.CV"], "comment": null, "summary": "Recent research on Vision Language Models (VLMs) suggests that they rely on\ninherent biases learned during training to respond to questions about visual\nproperties of an image. These biases are exacerbated when VLMs are asked highly\nspecific questions that require focusing on specific areas of the image. For\nexample, a VLM tasked with counting stars on a modified American flag (e.g.,\nwith more than 50 stars) will often disregard the visual evidence and fail to\nanswer accurately. We build upon this research and develop a multi-dimensional\nexamination framework to systematically determine which characteristics of the\ninput data, including both the image and the accompanying prompt, lead to such\ndifferences in performance. Using open-source VLMs, we further examine how\nattention values fluctuate with varying input parameters (e.g., image size,\nnumber of objects in the image, background color, prompt specificity). This\nresearch aims to learn how the behavior of vision language models changes and\nto explore methods for characterizing such changes. Our results suggest, among\nother things, that even minor modifications in image characteristics and prompt\nspecificity can lead to large changes in how a VLM formulates its answer and,\nsubsequently, its overall performance.", "AI": {"tldr": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u5728\u5904\u7406\u5177\u4f53\u89c6\u89c9\u95ee\u9898\u65f6\u6613\u53d7\u504f\u5dee\u5f71\u54cd\u3002\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u7ef4\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u6790\u56fe\u50cf\u548c\u63d0\u793a\u7279\u5f81\u5982\u4f55\u5f71\u54cdVLM\u6027\u80fd\u548c\u6ce8\u610f\u529b\uff0c\u53d1\u73b0\u5fae\u5c0f\u8f93\u5165\u53d8\u5316\u53ef\u5bfc\u81f4VLM\u884c\u4e3a\u53ca\u6027\u80fd\u7684\u663e\u8457\u6ce2\u52a8\u3002", "motivation": "VLMs\u5728\u56de\u7b54\u56fe\u50cf\u7279\u5b9a\u89c6\u89c9\u5c5e\u6027\u95ee\u9898\u65f6\uff0c\u4f1a\u4f9d\u8d56\u8bad\u7ec3\u504f\u89c1\uff0c\u5c24\u5176\u5728\u9762\u5bf9\u9ad8\u5ea6\u5177\u4f53\u7684\u95ee\u9898\u65f6\uff0c\u8fd9\u79cd\u504f\u89c1\u4f1a\u52a0\u5267\u5e76\u5bfc\u81f4\u4e0d\u51c6\u786e\u3002\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5730\u63a2\u7a76\u5bfc\u81f4VLM\u6027\u80fd\u5dee\u5f02\u7684\u8f93\u5165\u6570\u636e\uff08\u56fe\u50cf\u548c\u63d0\u793a\uff09\u7279\u6027\uff0c\u5e76\u4e86\u89e3\u5176\u884c\u4e3a\u5982\u4f55\u53d8\u5316\u3002", "method": "1. \u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u7ef4\u68c0\u67e5\u6846\u67b6\uff0c\u4ee5\u7cfb\u7edf\u6027\u5730\u786e\u5b9a\u56fe\u50cf\u548c\u4f34\u968f\u63d0\u793a\u7684\u54ea\u4e9b\u7279\u5f81\u5bfc\u81f4VLM\u7684\u6027\u80fd\u5dee\u5f02\u30022. \u4f7f\u7528\u5f00\u6e90VLM\uff0c\u8fdb\u4e00\u6b65\u68c0\u67e5\u6ce8\u610f\u529b\u503c\u5982\u4f55\u968f\u4e0d\u540c\u7684\u8f93\u5165\u53c2\u6570\uff08\u4f8b\u5982\u56fe\u50cf\u5927\u5c0f\u3001\u56fe\u50cf\u4e2d\u7269\u4f53\u6570\u91cf\u3001\u80cc\u666f\u989c\u8272\u3001\u63d0\u793a\u7279\u5f02\u6027\uff09\u800c\u6ce2\u52a8\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u56fe\u50cf\u7279\u5f81\u548c\u63d0\u793a\u7279\u5f02\u6027\u7684\u5fae\u5c0f\u4fee\u6539\uff0c\u4e5f\u53ef\u80fd\u5bfc\u81f4VLM\u5f62\u6210\u7b54\u6848\u7684\u65b9\u5f0f\u53ca\u5176\u6574\u4f53\u6027\u80fd\u7684\u5de8\u5927\u53d8\u5316\u3002", "conclusion": "VLMs\u5bf9\u8f93\u5165\u6570\u636e\uff08\u56fe\u50cf\u7279\u6027\u548c\u63d0\u793a\u7279\u5f02\u6027\uff09\u7684\u5fae\u5c0f\u53d8\u5316\u8868\u73b0\u51fa\u9ad8\u5ea6\u654f\u611f\u6027\uff0c\u8fd9\u4f1a\u663e\u8457\u5f71\u54cd\u5b83\u4eec\u7684\u884c\u4e3a\u548c\u6700\u7ec8\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u7406\u89e3\u5e76\u8868\u5f81\u8fd9\u4e9b\u53d8\u5316\u5bf9\u4e8e\u6539\u8fdbVLM\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.07990", "pdf": "https://arxiv.org/pdf/2509.07990", "abs": "https://arxiv.org/abs/2509.07990", "authors": ["Charan Gajjala Chenchu", "Kinam Kim", "Gao Lu", "Zia Ud Din"], "title": "Signals vs. Videos: Advancing Motion Intention Recognition for Human-Robot Collaboration in Construction", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Human-robot collaboration (HRC) in the construction industry depends on\nprecise and prompt recognition of human motion intentions and actions by robots\nto maximize safety and workflow efficiency. There is a research gap in\ncomparing data modalities, specifically signals and videos, for motion\nintention recognition. To address this, the study leverages deep learning to\nassess two different modalities in recognizing workers' motion intention at the\nearly stage of movement in drywall installation tasks. The Convolutional Neural\nNetwork - Long Short-Term Memory (CNN-LSTM) model utilizing surface\nelectromyography (sEMG) data achieved an accuracy of around 87% with an average\ntime of 0.04 seconds to perform prediction on a sample input. Meanwhile, the\npre-trained Video Swin Transformer combined with transfer learning harnessed\nvideo sequences as input to recognize motion intention and attained an accuracy\nof 94% but with a longer average time of 0.15 seconds for a similar prediction.\nThis study emphasizes the unique strengths and trade-offs of both data formats,\ndirecting their systematic deployments to enhance HRC in real-world\nconstruction projects.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6bd4\u8f83\u8868\u9762\u808c\u7535\u4fe1\u53f7(sEMG)\u548c\u89c6\u9891\u4e24\u79cd\u6a21\u6001\u5728\u5efa\u7b51\u4eba\u673a\u534f\u4f5c\u4e2d\u65e9\u671f\u8bc6\u522b\u4eba\u7c7b\u8fd0\u52a8\u610f\u56fe\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u89c6\u9891\u6a21\u6001\u51c6\u786e\u7387\u66f4\u9ad8\u4f46\u9884\u6d4b\u65f6\u95f4\u66f4\u957f\u3002", "motivation": "\u5efa\u7b51\u4e1a\u4eba\u673a\u534f\u4f5c(HRC)\u9700\u8981\u673a\u5668\u4eba\u7cbe\u786e\u53ca\u65f6\u8bc6\u522b\u5de5\u4eba\u8fd0\u52a8\u610f\u56fe\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u4f46\u5728\u8fd0\u52a8\u610f\u56fe\u8bc6\u522b\u65b9\u9762\uff0c\u5bf9\u6bd4\u4fe1\u53f7\u548c\u89c6\u9891\u7b49\u4e0d\u540c\u6570\u636e\u6a21\u6001\u7684\u7814\u7a76\u5b58\u5728\u7a7a\u767d\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u8bc4\u4f30\u4e24\u79cd\u6570\u636e\u6a21\u6001\uff08\u8868\u9762\u808c\u7535\u4fe1\u53f7sEMG\u548c\u89c6\u9891\uff09\u5728\u77f3\u818f\u677f\u5b89\u88c5\u4efb\u52a1\u8fd0\u52a8\u65e9\u671f\u8bc6\u522b\u5de5\u4eba\u8fd0\u52a8\u610f\u56fe\u3002\u5177\u4f53\u65b9\u6cd5\u662f\uff1a\u5bf9sEMG\u6570\u636e\u91c7\u7528CNN-LSTM\u6a21\u578b\uff1b\u5bf9\u89c6\u9891\u5e8f\u5217\u91c7\u7528\u9884\u8bad\u7ec3\u7684Video Swin Transformer\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u3002", "result": "\u4f7f\u7528sEMG\u6570\u636e\u7684CNN-LSTM\u6a21\u578b\u5b9e\u73b0\u4e86\u7ea687%\u7684\u51c6\u786e\u7387\uff0c\u5e73\u5747\u9884\u6d4b\u65f6\u95f4\u4e3a0.04\u79d2\u3002\u4f7f\u7528\u89c6\u9891\u5e8f\u5217\u7684Video Swin Transformer\u6a21\u578b\u5b9e\u73b0\u4e8694%\u7684\u51c6\u786e\u7387\uff0c\u4f46\u5e73\u5747\u9884\u6d4b\u65f6\u95f4\u4e3a0.15\u79d2\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86sEMG\u548c\u89c6\u9891\u4e24\u79cd\u6570\u636e\u683c\u5f0f\u5728\u8fd0\u52a8\u610f\u56fe\u8bc6\u522b\u65b9\u9762\u7684\u72ec\u7279\u4f18\u52bf\u548c\u6743\u8861\uff0c\u4e3a\u5728\u5b9e\u9645\u5efa\u7b51\u9879\u76ee\u4e2d\u7cfb\u7edf\u90e8\u7f72\u5b83\u4eec\u4ee5\u589e\u5f3a\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.08181", "pdf": "https://arxiv.org/pdf/2509.08181", "abs": "https://arxiv.org/abs/2509.08181", "authors": ["Honghui Du", "Leandro Minku", "Aonghus Lawlor", "Huiyu Zhou"], "title": "Multi-Label Transfer Learning in Non-Stationary Data Streams", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at IEEE International Conference on Data Mining (ICDM) 2025", "summary": "Label concepts in multi-label data streams often experience drift in\nnon-stationary environments, either independently or in relation to other\nlabels. Transferring knowledge between related labels can accelerate\nadaptation, yet research on multi-label transfer learning for data streams\nremains limited. To address this, we propose two novel transfer learning\nmethods: BR-MARLENE leverages knowledge from different labels in both source\nand target streams for multi-label classification; BRPW-MARLENE builds on this\nby explicitly modelling and transferring pairwise label dependencies to enhance\nlearning performance. Comprehensive experiments show that both methods\noutperform state-of-the-art multi-label stream approaches in non-stationary\nenvironments, demonstrating the effectiveness of inter-label knowledge transfer\nfor improved predictive performance.", "AI": {"tldr": "\u9488\u5bf9\u591a\u6807\u7b7e\u6570\u636e\u6d41\u4e2d\u7684\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5BR-MARLENE\u548cBRPW-MARLENE\uff0c\u901a\u8fc7\u6807\u7b7e\u95f4\u77e5\u8bc6\u8fc1\u79fb\u663e\u8457\u63d0\u9ad8\u4e86\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u591a\u6807\u7b7e\u6570\u636e\u6d41\u4e2d\u7684\u6982\u5ff5\u5e38\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u53d1\u751f\u6f02\u79fb\uff0c\u4e14\u6807\u7b7e\u95f4\u53ef\u80fd\u76f8\u4e92\u5173\u8054\u3002\u5c3d\u7ba1\u76f8\u5173\u6807\u7b7e\u95f4\u7684\u77e5\u8bc6\u8fc1\u79fb\u80fd\u52a0\u901f\u9002\u5e94\uff0c\u4f46\u9488\u5bf9\u6570\u636e\u6d41\u7684\u591a\u6807\u7b7e\u8fc1\u79fb\u5b66\u4e60\u7814\u7a76\u4ecd\u7136\u6709\u9650\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u9896\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff1a1. BR-MARLENE\u5229\u7528\u6e90\u548c\u76ee\u6807\u6570\u636e\u6d41\u4e2d\u4e0d\u540c\u6807\u7b7e\u7684\u77e5\u8bc6\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\uff1b2. BRPW-MARLENE\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u548c\u8fc1\u79fb\u6210\u5bf9\u6807\u7b7e\u4f9d\u8d56\u5173\u7cfb\u6765\u589e\u5f3a\u5b66\u4e60\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5168\u9762\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u4e24\u79cd\u65b9\u6cd5\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u591a\u6807\u7b7e\u6570\u636e\u6d41\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6807\u7b7e\u95f4\u77e5\u8bc6\u8fc1\u79fb\u80fd\u6709\u6548\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5904\u7406\u591a\u6807\u7b7e\u6570\u636e\u6d41\u7684\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.08381", "pdf": "https://arxiv.org/pdf/2509.08381", "abs": "https://arxiv.org/abs/2509.08381", "authors": ["Yu Cheng Chih", "Yong Hao Hou"], "title": "Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 8 figures, includes experiments on JSON extraction,\n  knowledge graph extraction, and NER", "summary": "Deploying large language models (LLMs) for structured data extraction in\ndomains such as financial compliance reporting, legal document analytics, and\nmultilingual knowledge base construction is often impractical for smaller teams\ndue to the high cost of running large architectures and the difficulty of\npreparing large, high-quality datasets. Most recent instruction-tuning studies\nfocus on seven-billion-parameter or larger models, leaving limited evidence on\nwhether much smaller models can work reliably under low-resource, multi-task\nconditions. This work presents ETLCH, a billion-parameter LLaMA-based model\nfine-tuned with low-rank adaptation on only a few hundred to one thousand\nsamples per task for JSON extraction, knowledge graph extraction, and named\nentity recognition. Despite its small scale, ETLCH outperforms strong baselines\nacross most evaluation metrics, with substantial gains observed even at the\nlowest data scale. These findings demonstrate that well-tuned small models can\ndeliver stable and accurate structured outputs at a fraction of the\ncomputational cost, enabling cost-effective and reliable information extraction\npipelines in resource-constrained environments.", "AI": {"tldr": "\u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff0c\u672c\u7814\u7a76\u63d0\u51faETLCH\uff0c\u4e00\u4e2a\u5341\u4ebf\u53c2\u6570\u7684LLaMA\u6a21\u578b\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u5c11\u91cf\u6837\u672c\uff0c\u5728\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\u4efb\u52a1\u4e2d\u8d85\u8d8a\u5927\u6a21\u578b\u57fa\u7ebf\uff0c\u8bc1\u660e\u5c0f\u6a21\u578b\u4e5f\u80fd\u63d0\u4f9b\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\u5bf9\u5c0f\u578b\u56e2\u961f\u800c\u8a00\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u51c6\u5907\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u73b0\u6709\u6307\u4ee4\u5fae\u8c03\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5927\u578b\u6a21\u578b\uff0c\u7f3a\u4e4f\u5173\u4e8e\u5c0f\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u3001\u591a\u4efb\u52a1\u6761\u4ef6\u4e0b\u53ef\u9760\u6027\u7684\u8bc1\u636e\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86ETLCH\uff0c\u4e00\u4e2a\u57fa\u4e8eLLaMA\u7684\u5341\u4ebf\u53c2\u6570\u6a21\u578b\uff0c\u4f7f\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\u8fdb\u884c\u5fae\u8c03\u3002\u6bcf\u4e2a\u4efb\u52a1\uff08\u5982JSON\u63d0\u53d6\u3001\u77e5\u8bc6\u56fe\u8c31\u63d0\u53d6\u548c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff09\u4ec5\u4f7f\u7528\u4e86\u6570\u767e\u5230\u4e00\u5343\u4e2a\u6837\u672c\u3002", "result": "\u5c3d\u7ba1ETLCH\u89c4\u6a21\u8f83\u5c0f\uff0c\u4f46\u5728\u5927\u591a\u6570\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5373\u4f7f\u5728\u6700\u4f4e\u6570\u636e\u91cf\u6761\u4ef6\u4e0b\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7ecf\u8fc7\u7cbe\u5fc3\u5fae\u8c03\u7684\u5c0f\u6a21\u578b\u80fd\u591f\u4ee5\u6781\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u63d0\u4f9b\u7a33\u5b9a\u4e14\u51c6\u786e\u7684\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u7ecf\u6d4e\u9ad8\u6548\u3001\u53ef\u9760\u7684\u4fe1\u606f\u63d0\u53d6\u7ba1\u9053\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2509.08280", "pdf": "https://arxiv.org/pdf/2509.08280", "abs": "https://arxiv.org/abs/2509.08280", "authors": ["Hyeonseok Kim", "Byeongkeun Kang", "Yeejin Lee"], "title": "Generalized Zero-Shot Learning for Point Cloud Segmentation with Evidence-Based Dynamic Calibration", "categories": ["cs.CV"], "comment": "20 pages, 12 figures, AAAI 2025", "summary": "Generalized zero-shot semantic segmentation of 3D point clouds aims to\nclassify each point into both seen and unseen classes. A significant challenge\nwith these models is their tendency to make biased predictions, often favoring\nthe classes encountered during training. This problem is more pronounced in 3D\napplications, where the scale of the training data is typically smaller than in\nimage-based tasks. To address this problem, we propose a novel method called\nE3DPC-GZSL, which reduces overconfident predictions towards seen classes\nwithout relying on separate classifiers for seen and unseen data. E3DPC-GZSL\ntackles the overconfidence problem by integrating an evidence-based uncertainty\nestimator into a classifier. This estimator is then used to adjust prediction\nprobabilities using a dynamic calibrated stacking factor that accounts for\npointwise prediction uncertainty. In addition, E3DPC-GZSL introduces a novel\ntraining strategy that improves uncertainty estimation by refining the semantic\nspace. This is achieved by merging learnable parameters with text-derived\nfeatures, thereby improving model optimization for unseen data. Extensive\nexperiments demonstrate that the proposed approach achieves state-of-the-art\nperformance on generalized zero-shot semantic segmentation datasets, including\nScanNet v2 and S3DIS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faE3DPC-GZSL\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u57fa\u4e8e\u8bc1\u636e\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5668\u5e76\u4f18\u5316\u8bed\u4e49\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e863D\u70b9\u4e91\u5e7f\u4e49\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u4e2d\u6a21\u578b\u5bf9\u5df2\u77e5\u7c7b\u522b\u8fc7\u62df\u5408\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "3D\u70b9\u4e91\u7684\u5e7f\u4e49\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u9762\u4e34\u6a21\u578b\u9884\u6d4b\u504f\u5411\u4e8e\u8bad\u7ec3\u4e2d\u89c1\u8fc7\u7684\u7c7b\u522b\uff08seen classes\uff09\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u57283D\u6570\u636e\u91cf\u76f8\u5bf9\u8f83\u5c0f\u7684\u60c5\u51b5\u4e0b\u3002\u8fd9\u5bfc\u81f4\u5bf9\u672a\u89c1\u8fc7\u7c7b\u522b\uff08unseen classes\uff09\u7684\u9884\u6d4b\u4e0d\u51c6\u786e\u3002", "method": "\u672c\u6587\u63d0\u51faE3DPC-GZSL\u65b9\u6cd5\uff0c\u4e3b\u8981\u5305\u62ec\uff1a1) \u5c06\u57fa\u4e8e\u8bc1\u636e\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5668\u6574\u5408\u5230\u5206\u7c7b\u5668\u4e2d\uff0c\u4ee5\u51cf\u5c11\u5bf9\u5df2\u77e5\u7c7b\u522b\u7684\u8fc7\u81ea\u4fe1\u9884\u6d4b\uff1b2) \u4f7f\u7528\u52a8\u6001\u6821\u51c6\u5806\u53e0\u56e0\u5b50\uff0c\u6839\u636e\u9010\u70b9\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u8c03\u6574\u9884\u6d4b\u6982\u7387\uff1b3) \u5f15\u5165\u65b0\u9896\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u53ef\u5b66\u4e60\u53c2\u6570\u4e0e\u6587\u672c\u6d3e\u751f\u7279\u5f81\u878d\u5408\u6765\u7ec6\u5316\u8bed\u4e49\u7a7a\u95f4\uff0c\u4ece\u800c\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5e76\u4f18\u5316\u6a21\u578b\u5bf9\u672a\u89c1\u8fc7\u6570\u636e\u7684\u5b66\u4e60\u3002", "result": "E3DPC-GZSL\u65b9\u6cd5\u5728ScanNet v2\u548cS3DIS\u7b49\u5e7f\u4e49\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\u4e0a\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "E3DPC-GZSL\u901a\u8fc7\u6709\u6548\u89e3\u51b3\u5df2\u77e5\u7c7b\u522b\u504f\u7f6e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u70b9\u4e91\u5e7f\u4e49\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\uff0c\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07991", "pdf": "https://arxiv.org/pdf/2509.07991", "abs": "https://arxiv.org/abs/2509.07991", "authors": ["Jingyuan Wang", "Junhua Li"], "title": "DLGE: Dual Local-Global Encoding for Generalizable Cross-BCI-Paradigm", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep learning models have been frequently used to decode a single\nbrain-computer interface (BCI) paradigm based on electroencephalography (EEG).\nIt is challenging to decode multiple BCI paradigms using one model due to\ndiverse barriers, such as different channel configurations and disparate\ntask-related representations. In this study, we propose Dual Local-Global\nEncoder (DLGE), enabling the classification across different BCI paradigms. To\naddress the heterogeneity in EEG channel configurations across paradigms, we\nemploy an anatomically inspired brain-region partitioning and padding strategy\nto standardize EEG channel configuration. In the proposed model, the local\nencoder is designed to learn shared features across BCI paradigms within each\nbrain region based on time-frequency information, which integrates temporal\nattention on individual channels with spatial attention among channels for each\nbrain region. These shared features are subsequently aggregated in the global\nencoder to form respective paradigm-specific feature representations. Three BCI\nparadigms (motor imagery, resting state, and driving fatigue) were used to\nevaluate the proposed model. The results demonstrate that our model is capable\nof processing diverse BCI paradigms without retraining and retuning, achieving\naverage macro precision, recall, and F1-score of 60.16\\%, 59.88\\%, and 59.56\\%,\nrespectively. We made an initial attempt to develop a general model for\ncross-BCI-paradigm classification, avoiding retraining or redevelopment for\neach paradigm. This study paves the way for the development of an effective but\nsimple model for cross-BCI-paradigm decoding, which might benefit the design of\nportable devices for universal BCI decoding.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c40\u90e8-\u5168\u5c40\u7f16\u7801\u5668\uff08DLGE\uff09\u6a21\u578b\uff0c\u65e8\u5728\u901a\u8fc7\u6807\u51c6\u5316EEG\u901a\u9053\u914d\u7f6e\u548c\u5b66\u4e60\u5171\u4eab/\u7279\u5b9a\u8303\u5f0f\u7279\u5f81\uff0c\u5b9e\u73b0\u8de8\u4e0d\u540c\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u8303\u5f0f\u7684\u901a\u7528\u5206\u7c7b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u7528\u5355\u4e00\u6a21\u578b\u89e3\u7801\u591a\u4e2aBCI\u8303\u5f0f\uff0c\u539f\u56e0\u5728\u4e8e\u901a\u9053\u914d\u7f6e\u548c\u4efb\u52a1\u76f8\u5173\u8868\u5f81\u7684\u5f02\u8d28\u6027\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u591a\u79cdBCI\u8303\u5f0f\u7684\u901a\u7528\u6a21\u578b\u3002", "method": "\u63d0\u51faDLGE\u6a21\u578b\u3002\u901a\u8fc7\u89e3\u5256\u5b66\u542f\u53d1\u7684\u5927\u8111\u533a\u57df\u5212\u5206\u548c\u586b\u5145\u7b56\u7565\u6807\u51c6\u5316EEG\u901a\u9053\u914d\u7f6e\u3002\u5c40\u90e8\u7f16\u7801\u5668\u5b66\u4e60\u6bcf\u4e2a\u5927\u8111\u533a\u57df\u5185\u57fa\u4e8e\u65f6\u9891\u4fe1\u606f\u7684\u5171\u4eab\u7279\u5f81\uff0c\u6574\u5408\u901a\u9053\u5185\u65f6\u95f4\u6ce8\u610f\u529b\u548c\u901a\u9053\u95f4\u7a7a\u95f4\u6ce8\u610f\u529b\u3002\u5168\u5c40\u7f16\u7801\u5668\u805a\u5408\u8fd9\u4e9b\u5171\u4eab\u7279\u5f81\u5f62\u6210\u8303\u5f0f\u7279\u5b9a\u7684\u7279\u5f81\u8868\u793a\u3002\u4f7f\u7528\u8fd0\u52a8\u60f3\u8c61\u3001\u9759\u606f\u72b6\u6001\u548c\u9a7e\u9a76\u75b2\u52b3\u4e09\u79cdBCI\u8303\u5f0f\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "DLGE\u6a21\u578b\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u548c\u8c03\u4f18\u5373\u53ef\u5904\u7406\u591a\u79cdBCI\u8303\u5f0f\uff0c\u5e73\u5747\u5b8f\u89c2\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5206\u522b\u8fbe\u523060.16%\u300159.88%\u548c59.56%\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5c1d\u8bd5\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u8de8BCI\u8303\u5f0f\u5206\u7c7b\u7684\u901a\u7528\u6a21\u578b\uff0c\u907f\u514d\u4e86\u4e3a\u6bcf\u4e2a\u8303\u5f0f\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\u6216\u91cd\u65b0\u5f00\u53d1\u3002\u8fd9\u4e3a\u5f00\u53d1\u6709\u6548\u4e14\u7b80\u5355\u7684\u901a\u7528BCI\u89e3\u7801\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u5229\u4e8e\u4fbf\u643a\u5f0f\u8bbe\u5907\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2509.08184", "pdf": "https://arxiv.org/pdf/2509.08184", "abs": "https://arxiv.org/abs/2509.08184", "authors": ["Francesco D'Angelo", "Francesco Croce", "Nicolas Flammarion"], "title": "Selective Induction Heads: How Transformers Select Causal Structures In Context", "categories": ["cs.LG", "stat.ME"], "comment": null, "summary": "Transformers have exhibited exceptional capabilities in sequence modeling\ntasks, leveraging self-attention and in-context learning. Critical to this\nsuccess are induction heads, attention circuits that enable copying tokens\nbased on their previous occurrences. In this work, we introduce a novel\nframework that showcases transformers' ability to dynamically handle causal\nstructures. Existing works rely on Markov Chains to study the formation of\ninduction heads, revealing how transformers capture causal dependencies and\nlearn transition probabilities in-context. However, they rely on a fixed causal\nstructure that fails to capture the complexity of natural languages, where the\nrelationship between tokens dynamically changes with context. To this end, our\nframework varies the causal structure through interleaved Markov chains with\ndifferent lags while keeping the transition probabilities fixed. This setting\nunveils the formation of Selective Induction Heads, a new circuit that endows\ntransformers with the ability to select the correct causal structure\nin-context. We empirically demonstrate that transformers learn this mechanism\nto predict the next token by identifying the correct lag and copying the\ncorresponding token from the past. We provide a detailed construction of a\n3-layer transformer to implement the selective induction head, and a\ntheoretical analysis proving that this mechanism asymptotically converges to\nthe maximum likelihood solution. Our findings advance the understanding of how\ntransformers select causal structures, providing new insights into their\nfunctioning and interpretability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86Transformers\u5982\u4f55\u901a\u8fc7\u201c\u9009\u62e9\u6027\u5f52\u7eb3\u5934\u201d\u52a8\u6001\u5904\u7406\u56e0\u679c\u7ed3\u6784\uff0c\u8be5\u673a\u5236\u4f7f\u5176\u80fd\u591f\u6839\u636e\u4e0a\u4e0b\u6587\u9009\u62e9\u6b63\u786e\u7684\u56e0\u679c\u6ede\u540e\u5e76\u590d\u5236\u76f8\u5e94\u6807\u8bb0\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4f9d\u8d56\u56fa\u5b9a\u56e0\u679c\u7ed3\u6784\uff08\u5982\u9a6c\u5c14\u53ef\u592b\u94fe\uff09\u6765\u5206\u6790Transformers\u7684\u5f52\u7eb3\u5934\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u81ea\u7136\u8bed\u8a00\u4e2d\u52a8\u6001\u53d8\u5316\u7684\u6807\u8bb0\u5173\u7cfb\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u5904\u7406\u52a8\u6001\u56e0\u679c\u7ed3\u6784\u7684\u65b0\u6846\u67b6\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u9519\u5177\u6709\u4e0d\u540c\u6ede\u540e\u7684\u9a6c\u5c14\u53ef\u592b\u94fe\u6765\u52a8\u6001\u6539\u53d8\u56e0\u679c\u7ed3\u6784\uff0c\u540c\u65f6\u56fa\u5b9a\u8f6c\u79fb\u6982\u7387\u3002\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a3\u5c42Transformer\u6765\u6f14\u793a\u9009\u62e9\u6027\u5f52\u7eb3\u5934\u7684\u5b9e\u73b0\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\u53ca\u7406\u8bba\u5206\u6790\u3002", "result": "\u6211\u4eec\u63ed\u793a\u4e86\u201c\u9009\u62e9\u6027\u5f52\u7eb3\u5934\u201d\u7684\u5f62\u6210\uff0c\u8fd9\u662f\u4e00\u79cd\u4f7fTransformers\u80fd\u591f\u5728\u4e0a\u4e0b\u6587\u4e2d\u9009\u62e9\u6b63\u786e\u56e0\u679c\u7ed3\u6784\u7684\u65b0\u7535\u8def\u3002\u5b9e\u8bc1\u8868\u660e\uff0cTransformers\u5b66\u4e60\u4e86\u901a\u8fc7\u8bc6\u522b\u6b63\u786e\u6ede\u540e\u5e76\u590d\u5236\u8fc7\u53bb\u6807\u8bb0\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u6807\u8bb0\u7684\u673a\u5236\uff0c\u7406\u8bba\u5206\u6790\u8bc1\u660e\u8be5\u673a\u5236\u6e10\u8fd1\u6536\u655b\u5230\u6700\u5927\u4f3c\u7136\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u589e\u8fdb\u4e86\u5bf9Transformers\u5982\u4f55\u9009\u62e9\u56e0\u679c\u7ed3\u6784\u7684\u7406\u89e3\uff0c\u4e3a\u6df1\u5165\u8ba4\u8bc6\u5176\u5de5\u4f5c\u539f\u7406\u548c\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2509.08438", "pdf": "https://arxiv.org/pdf/2509.08438", "abs": "https://arxiv.org/abs/2509.08438", "authors": ["Jinzhong Ning", "Paerhati Tulajiang", "Yingying Le", "Yijia Zhang", "Yuanyuan Sun", "Hongfei Lin", "Haifeng Liu"], "title": "CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework", "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Speech Relation Extraction (SpeechRE) aims to extract relation triplets\ndirectly from speech. However, existing benchmark datasets rely heavily on\nsynthetic data, lacking sufficient quantity and diversity of real human speech.\nMoreover, existing models also suffer from rigid single-order generation\ntemplates and weak semantic alignment, substantially limiting their\nperformance. To address these challenges, we introduce CommonVoice-SpeechRE, a\nlarge-scale dataset comprising nearly 20,000 real-human speech samples from\ndiverse speakers, establishing a new benchmark for SpeechRE research.\nFurthermore, we propose the Relation Prompt-Guided Multi-Order Generative\nEnsemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet\ngeneration ensemble strategy, leveraging data diversity through diverse element\norders during both training and inference, and (2) CNN-based latent relation\nprediction heads that generate explicit relation prompts to guide cross-modal\nalignment and accurate triplet generation. Experiments show our approach\noutperforms state-of-the-art methods, providing both a benchmark dataset and an\neffective solution for real-world SpeechRE. The source code and dataset are\npublicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CommonVoice-SpeechRE\uff0c\u4e00\u4e2a\u5927\u578b\u771f\u5b9e\u8bed\u97f3\u5173\u7cfb\u62bd\u53d6\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u4e86RPG-MoGe\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5e8f\u751f\u6210\u7b56\u7565\u548c\u5173\u7cfb\u63d0\u793a\u5f15\u5bfc\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u51c6\u786e\u7684\u4e09\u5143\u7ec4\u751f\u6210\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5173\u7cfb\u62bd\u53d6(SpeechRE)\u57fa\u51c6\u6570\u636e\u96c6\u8fc7\u5ea6\u4f9d\u8d56\u5408\u6210\u6570\u636e\uff0c\u7f3a\u4e4f\u771f\u5b9e\u4eba\u58f0\u7684\u591a\u6837\u6027\u548c\u6570\u91cf\uff1b\u73b0\u6709\u6a21\u578b\u5b58\u5728\u50f5\u786c\u7684\u5355\u5e8f\u751f\u6210\u6a21\u677f\u548c\u8bed\u4e49\u5bf9\u9f50\u5f31\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "1. \u6784\u5efa\u4e86CommonVoice-SpeechRE\uff0c\u4e00\u4e2a\u5305\u542b\u8fd120,000\u4e2a\u771f\u5b9e\u4eba\u58f0\u6837\u672c\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u30022. \u63d0\u51fa\u4e86Relation Prompt-Guided Multi-Order Generative Ensemble (RPG-MoGe)\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\uff1a(1) \u4e00\u79cd\u591a\u5e8f\u4e09\u5143\u7ec4\u751f\u6210\u96c6\u6210\u7b56\u7565\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5229\u7528\u4e0d\u540c\u5143\u7d20\u987a\u5e8f\u6765\u589e\u52a0\u6570\u636e\u591a\u6837\u6027\uff1b(2) \u57fa\u4e8eCNN\u7684\u6f5c\u5728\u5173\u7cfb\u9884\u6d4b\u5934\uff0c\u751f\u6210\u663e\u5f0f\u5173\u7cfb\u63d0\u793a\u4ee5\u6307\u5bfc\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u51c6\u786e\u7684\u4e09\u5143\u7ec4\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u4e3aSpeechRE\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684SpeechRE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08289", "pdf": "https://arxiv.org/pdf/2509.08289", "abs": "https://arxiv.org/abs/2509.08289", "authors": ["Yuelin Guo", "Haoyu He", "Zhiyuan Chen", "Zitong Huang", "Renhao Lu", "Lu Shi", "Zejun Wang", "Weizhe Zhang"], "title": "Dual-Thresholding Heatmaps to Cluster Proposals for Weakly Supervised Object Detection", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Weakly supervised object detection (WSOD) has attracted significant attention\nin recent years, as it does not require box-level annotations. State-of-the-art\nmethods generally adopt a multi-module network, which employs WSDDN as the\nmultiple instance detection network module and multiple instance refinement\nmodules to refine performance. However, these approaches suffer from three key\nlimitations. First, existing methods tend to generate pseudo GT boxes that\neither focus only on discriminative parts, failing to capture the whole object,\nor cover the entire object but fail to distinguish between adjacent intra-class\ninstances. Second, the foundational WSDDN architecture lacks a crucial\nbackground class representation for each proposal and exhibits a large semantic\ngap between its branches. Third, prior methods discard ignored proposals during\noptimization, leading to slow convergence. To address these challenges, we\nfirst design a heatmap-guided proposal selector (HGPS) algorithm, which\nutilizes dual thresholds on heatmaps to pre-select proposals, enabling pseudo\nGT boxes to both capture the full object extent and distinguish between\nadjacent intra-class instances. We then present a weakly supervised basic\ndetection network (WSBDN), which augments each proposal with a background class\nrepresentation and uses heatmaps for pre-supervision to bridge the semantic gap\nbetween matrices. At last, we introduce a negative certainty supervision loss\non ignored proposals to accelerate convergence. Extensive experiments on the\nchallenging PASCAL VOC 2007 and 2012 datasets demonstrate the effectiveness of\nour framework. We achieve mAP/mCorLoc scores of 58.5%/81.8% on VOC 2007 and\n55.6%/80.5% on VOC 2012, performing favorably against the state-of-the-art WSOD\nmethods. Our code is publicly available at\nhttps://github.com/gyl2565309278/DTH-CP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f31\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\uff08WSOD\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u70ed\u56fe\u5f15\u5bfc\u7684\u5efa\u8bae\u533a\u57df\u9009\u62e9\u5668\u3001\u5f31\u76d1\u7763\u57fa\u7840\u68c0\u6d4b\u7f51\u7edc\u548c\u8d1f\u786e\u5b9a\u6027\u76d1\u7763\u635f\u5931\uff0c\u89e3\u51b3\u4e86\u73b0\u6709WSOD\u65b9\u6cd5\u5728\u4f2a\u771f\u503c\u751f\u6210\u3001\u80cc\u666f\u8868\u793a\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8fbe\u5230\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709WSOD\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u5c40\u9650\u6027\uff1a1. \u4f2a\u771f\u503c\u6846\u751f\u6210\u4e0d\u51c6\u786e\uff0c\u8981\u4e48\u53ea\u5173\u6ce8\u5224\u522b\u6027\u90e8\u5206\u5bfc\u81f4\u76ee\u6807\u4e0d\u5b8c\u6574\uff0c\u8981\u4e48\u65e0\u6cd5\u533a\u5206\u76f8\u90bb\u540c\u7c7b\u5b9e\u4f8b\uff1b2. WSDDN\u67b6\u6784\u7f3a\u4e4f\u80cc\u666f\u7c7b\u8868\u793a\uff0c\u4e14\u5206\u652f\u95f4\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\uff1b3. \u4f18\u5316\u8fc7\u7a0b\u4e2d\u4e22\u5f03\u88ab\u5ffd\u7565\u7684\u5efa\u8bae\u533a\u57df\uff0c\u5bfc\u81f4\u6536\u655b\u7f13\u6162\u3002", "method": "1. \u8bbe\u8ba1\u4e86\u70ed\u56fe\u5f15\u5bfc\u7684\u5efa\u8bae\u533a\u57df\u9009\u62e9\u5668\uff08HGPS\uff09\u7b97\u6cd5\uff0c\u5229\u7528\u70ed\u56fe\u53cc\u9608\u503c\u9884\u9009\u5efa\u8bae\u533a\u57df\uff0c\u4ee5\u6355\u6349\u5b8c\u6574\u76ee\u6807\u5e76\u533a\u5206\u76f8\u90bb\u5b9e\u4f8b\u30022. \u63d0\u51fa\u4e86\u5f31\u76d1\u7763\u57fa\u7840\u68c0\u6d4b\u7f51\u7edc\uff08WSBDN\uff09\uff0c\u4e3a\u6bcf\u4e2a\u5efa\u8bae\u533a\u57df\u589e\u52a0\u80cc\u666f\u7c7b\u8868\u793a\uff0c\u5e76\u5229\u7528\u70ed\u56fe\u8fdb\u884c\u9884\u76d1\u7763\u4ee5\u5f25\u5408\u8bed\u4e49\u9e3f\u6c9f\u30023. \u5f15\u5165\u4e86\u5bf9\u88ab\u5ffd\u7565\u5efa\u8bae\u533a\u57df\u7684\u8d1f\u786e\u5b9a\u6027\u76d1\u7763\u635f\u5931\uff0c\u4ee5\u52a0\u901f\u6536\u655b\u3002", "result": "\u5728PASCAL VOC 2007\u6570\u636e\u96c6\u4e0a\uff0cmAP/mCorLoc\u5206\u6570\u8fbe\u523058.5%/81.8%\uff1b\u5728PASCAL VOC 2012\u6570\u636e\u96c6\u4e0a\uff0cmAP/mCorLoc\u5206\u6570\u8fbe\u523055.6%/80.5%\u3002\u8fd9\u4e9b\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u7684WSOD\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5f31\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u6539\u8fdb\u4f2a\u771f\u503c\u751f\u6210\u3001\u589e\u5f3a\u7f51\u7edc\u67b6\u6784\u548c\u52a0\u901f\u6536\u655b\uff0c\u663e\u8457\u63d0\u5347\u4e86WSOD\u6027\u80fd\uff0c\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002"}}
{"id": "2509.08188", "pdf": "https://arxiv.org/pdf/2509.08188", "abs": "https://arxiv.org/abs/2509.08188", "authors": ["Hritik Arasu", "Faisal R Jahangiri"], "title": "ArtifactGen: Benchmarking WGAN-GP vs Diffusion for Label-Aware EEG Artifact Synthesis", "categories": ["cs.LG", "cs.NE", "q-bio.NC"], "comment": "16 Pages, 6 figures", "summary": "Artifacts in electroencephalography (EEG) -- muscle, eye movement, electrode,\nchewing, and shiver -- confound automated analysis yet are costly to label at\nscale. We study whether modern generative models can synthesize realistic,\nlabel-aware artifact segments suitable for augmentation and stress-testing.\nUsing the TUH EEG Artifact (TUAR) corpus, we curate subject-wise splits and\nfixed-length multi-channel windows (e.g., 250 samples) with preprocessing\ntailored to each model (per-window min-max for adversarial training;\nper-recording/channel $z$-score for diffusion). We compare a conditional\nWGAN-GP with a projection discriminator to a 1D denoising diffusion model with\nclassifier-free guidance, and evaluate along three axes: (i) fidelity via Welch\nband-power deltas ($\\Delta\\delta,\\ \\Delta\\theta,\\ \\Delta\\alpha,\\ \\Delta\\beta$),\nchannel-covariance Frobenius distance, autocorrelation $L_2$, and\ndistributional metrics (MMD/PRD); (ii) specificity via class-conditional\nrecovery with lightweight $k$NN/classifiers; and (iii) utility via augmentation\neffects on artifact recognition. In our setting, WGAN-GP achieves closer\nspectral alignment and lower MMD to real data, while both models exhibit weak\nclass-conditional recovery, limiting immediate augmentation gains and revealing\nopportunities for stronger conditioning and coverage. We release a reproducible\npipeline -- data manifests, training configurations, and evaluation scripts --\nto establish a baseline for EEG artifact synthesis and to surface actionable\nfailure modes for future work.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528\u751f\u6210\u6a21\u578b\uff08WGAN-GP\u548c1D\u6269\u6563\u6a21\u578b\uff09\u5408\u6210EEG\u4f2a\u5f71\u6570\u636e\u3002WGAN-GP\u5728\u9891\u8c31\u4fdd\u771f\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u4e24\u4e2a\u6a21\u578b\u5728\u7c7b\u522b\u6761\u4ef6\u6062\u590d\u65b9\u9762\u5747\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u76f4\u63a5\u7684\u6570\u636e\u589e\u5f3a\u6548\u679c\u3002\u7814\u7a76\u53d1\u5e03\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7ebf\u7ba1\u9053\u3002", "motivation": "EEG\u4f2a\u5f71\uff08\u5982\u808c\u8089\u3001\u773c\u52a8\u3001\u7535\u6781\u7b49\uff09\u4f1a\u4e25\u91cd\u6df7\u6dc6\u81ea\u52a8\u5316\u5206\u6790\uff0c\u4e14\u5927\u89c4\u6a21\u624b\u52a8\u6807\u6ce8\u6210\u672c\u6781\u9ad8\u3002\u56e0\u6b64\uff0c\u9700\u8981\u63a2\u7d22\u4f7f\u7528\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u5408\u6210\u903c\u771f\u3001\u6807\u7b7e\u611f\u77e5\u7684\u4f2a\u5f71\u6570\u636e\uff0c\u4ee5\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u548c\u7cfb\u7edf\u538b\u529b\u6d4b\u8bd5\u3002", "method": "\u4f7f\u7528TUH EEG Artifact (TUAR) \u8bed\u6599\u5e93\uff0c\u8fdb\u884c\u57fa\u4e8e\u53d7\u8bd5\u8005\u7684\u5206\u5272\u548c\u56fa\u5b9a\u957f\u5ea6\u591a\u901a\u9053\u7a97\u5904\u7406\uff0c\u5e76\u6839\u636e\u6a21\u578b\uff08WGAN-GP\u6216\u6269\u6563\u6a21\u578b\uff09\u5b9a\u5236\u9884\u5904\u7406\u65b9\u6cd5\u3002\u6bd4\u8f83\u4e86\u5e26\u6295\u5f71\u5224\u522b\u5668\u7684\u6761\u4ef6WGAN-GP\u548c\u5e26\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u76841D\u53bb\u566a\u6269\u6563\u6a21\u578b\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\uff1a(i) \u771f\u5b9e\u6027\uff08\u901a\u8fc7Welch\u5e26\u529f\u7387\u5dee\u503c\u3001\u901a\u9053\u534f\u65b9\u5deeFrobenius\u8ddd\u79bb\u3001\u81ea\u76f8\u5173$L_2$\u548c\u5206\u5e03\u5ea6\u91cfMMD/PRD\uff09\uff1b(ii) \u7279\u5f02\u6027\uff08\u901a\u8fc7\u8f7b\u91cf\u7ea7kNN/\u5206\u7c7b\u5668\u7684\u7c7b\u522b\u6761\u4ef6\u6062\u590d\u80fd\u529b\uff09\uff1b(iii) \u5b9e\u7528\u6027\uff08\u901a\u8fc7\u5bf9\u4f2a\u5f71\u8bc6\u522b\u7684\u589e\u5f3a\u6548\u679c\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cWGAN-GP\u5728\u9891\u8c31\u5bf9\u9f50\u548cMMD\u503c\u65b9\u9762\u66f4\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\u3002\u7136\u800c\uff0c\u4e24\u79cd\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u8f83\u5f31\u7684\u7c7b\u522b\u6761\u4ef6\u6062\u590d\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5373\u65f6\u6570\u636e\u589e\u5f3a\u65b9\u9762\u7684\u6536\u76ca\uff0c\u5e76\u63ed\u793a\u4e86\u5728\u66f4\u5f3a\u6761\u4ef6\u5316\u548c\u8986\u76d6\u8303\u56f4\u65b9\u9762\u7684\u6539\u8fdb\u673a\u4f1a\u3002", "conclusion": "\u751f\u6210\u6a21\u578b\u5408\u6210EEG\u4f2a\u5f71\u5177\u6709\u4e00\u5b9a\u6f5c\u529b\uff0c\u5c24\u5176WGAN-GP\u5728\u9891\u8c31\u4fdd\u771f\u5ea6\u4e0a\u8868\u73b0\u826f\u597d\u3002\u4f46\u76ee\u524d\u6a21\u578b\u5728\u7c7b\u522b\u6761\u4ef6\u6062\u590d\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u4f5c\u4e3a\u5373\u65f6\u6570\u636e\u589e\u5f3a\u5de5\u5177\u7684\u6709\u6548\u6027\u3002\u672c\u7814\u7a76\u53d1\u5e03\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u7ba1\u9053\uff0c\u4e3aEEG\u4f2a\u5f71\u5408\u6210\u5efa\u7acb\u4e86\u57fa\u7ebf\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5de5\u4f5c\u53ef\u6539\u8fdb\u7684\u5177\u6311\u6218\u6027\u9886\u57df\u3002"}}
{"id": "2509.08463", "pdf": "https://arxiv.org/pdf/2509.08463", "abs": "https://arxiv.org/abs/2509.08463", "authors": ["Fanzhen Liu", "Alsharif Abuadbba", "Kristen Moore", "Surya Nepal", "Cecile Paris", "Jia Wu", "Jian Yang", "Quan Z. Sheng"], "title": "Adversarial Attacks Against Automated Fact-Checking: A Survey", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "Accepted to the Main Conference of EMNLP 2025. Resources are\n  available at\n  https://github.com/FanzhenLiu/Awesome-Automated-Fact-Checking-Attacks", "summary": "In an era where misinformation spreads freely, fact-checking (FC) plays a\ncrucial role in verifying claims and promoting reliable information. While\nautomated fact-checking (AFC) has advanced significantly, existing systems\nremain vulnerable to adversarial attacks that manipulate or generate claims,\nevidence, or claim-evidence pairs. These attacks can distort the truth, mislead\ndecision-makers, and ultimately undermine the reliability of FC models. Despite\ngrowing research interest in adversarial attacks against AFC systems, a\ncomprehensive, holistic overview of key challenges remains lacking. These\nchallenges include understanding attack strategies, assessing the resilience of\ncurrent models, and identifying ways to enhance robustness. This survey\nprovides the first in-depth review of adversarial attacks targeting FC,\ncategorizing existing attack methodologies and evaluating their impact on AFC\nsystems. Additionally, we examine recent advancements in adversary-aware\ndefenses and highlight open research questions that require further\nexploration. Our findings underscore the urgent need for resilient FC\nframeworks capable of withstanding adversarial manipulations in pursuit of\npreserving high verification accuracy.", "AI": {"tldr": "\u672c\u6587\u5bf9\u9488\u5bf9\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\uff08AFC\uff09\u7cfb\u7edf\u7684\u5bf9\u6297\u6027\u653b\u51fb\u8fdb\u884c\u4e86\u9996\u6b21\u6df1\u5165\u7efc\u8ff0\u3002\u5b83\u5206\u7c7b\u4e86\u653b\u51fb\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u5176\u5f71\u54cd\uff0c\u5ba1\u67e5\u4e86\u5bf9\u6297\u6027\u9632\u5fa1\u8fdb\u5c55\uff0c\u5e76\u6307\u51fa\u4e86\u5f00\u653e\u7814\u7a76\u95ee\u9898\uff0c\u5f3a\u8c03\u4e86\u6784\u5efa\u5f39\u6027AFC\u6846\u67b6\u7684\u7d27\u8feb\u6027\u3002", "motivation": "\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\uff08AFC\uff09\u7cfb\u7edf\u5728\u6253\u51fb\u9519\u8bef\u4fe1\u606f\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\uff0c\u4f46\u5b83\u4eec\u6613\u53d7\u5bf9\u6297\u6027\u653b\u51fb\u5f71\u54cd\uff0c\u8fd9\u4e9b\u653b\u51fb\u4f1a\u64cd\u7eb5\u4fe1\u606f\u5e76\u635f\u5bb3\u6a21\u578b\u53ef\u9760\u6027\u3002\u5c3d\u7ba1\u7814\u7a76\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u653b\u51fb\u6311\u6218\u7684\u5168\u9762\u3001\u6574\u4f53\u6982\u8ff0\uff0c\u5305\u62ec\u7406\u89e3\u653b\u51fb\u7b56\u7565\u3001\u8bc4\u4f30\u6a21\u578b\u97e7\u6027\u53ca\u589e\u5f3a\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u9879\u6df1\u5165\u7684\u7efc\u8ff0\uff0c\u5bf9\u9488\u5bf9\u4e8b\u5b9e\u6838\u67e5\uff08FC\uff09\u7684\u5bf9\u6297\u6027\u653b\u51fb\u8fdb\u884c\u4e86\u9996\u6b21\u5168\u9762\u5ba1\u89c6\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u5bf9\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\u3001\u8bc4\u4f30\u5176\u5bf9AFC\u7cfb\u7edf\u7684\u5f71\u54cd\u3001\u5ba1\u67e5\u5bf9\u6297\u6027\u9632\u5fa1\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u63d0\u51fa\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u7684\u5f00\u653e\u7814\u7a76\u95ee\u9898\u3002", "result": "\u8be5\u7efc\u8ff0\u63ed\u793a\u4e86\u9488\u5bf9AFC\u7cfb\u7edf\u7684\u5404\u79cd\u5bf9\u6297\u6027\u653b\u51fb\u7c7b\u578b\u53ca\u5176\u5f71\u54cd\uff0c\u5e76\u5ba1\u89c6\u4e86\u5bf9\u6297\u6027\u9632\u5fa1\u7684\u5f53\u524d\u8fdb\u5c55\u3002\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u73b0\u6709\u7cfb\u7edf\u5bf9\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5f53\u524d\u7814\u7a76\u5728\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u65b9\u9762\u7684\u72b6\u51b5\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u5177\u6709\u97e7\u6027\u7684\u4e8b\u5b9e\u6838\u67e5\u6846\u67b6\uff0c\u4ee5\u6709\u6548\u62b5\u5fa1\u5bf9\u6297\u6027\u64cd\u7eb5\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u9ad8\u6838\u67e5\u51c6\u786e\u6027\u65b9\u9762\u53d1\u6325\u4f5c\u7528\u3002"}}
{"id": "2509.08303", "pdf": "https://arxiv.org/pdf/2509.08303", "abs": "https://arxiv.org/abs/2509.08303", "authors": ["M. Warizmi Wafiq", "Peter Cutter", "Ate Poortinga", "Daniel Marc G. dela Torre", "Karis Tenneson", "Vanna Teck", "Enikoe Bihari", "Chanarun Saisaward", "Weraphong Suaruang", "Andrea McMahon", "Andi Vika Faradiba Muin", "Karno B. Batiran", "Chairil A", "Nurul Qomar", "Arya Arismaya Metananda", "David Ganz", "David Saah"], "title": "An Open Benchmark Dataset for GeoAI Foundation Models for Oil Palm Mapping in Indonesia", "categories": ["cs.CV"], "comment": null, "summary": "Oil palm cultivation remains one of the leading causes of deforestation in\nIndonesia. To better track and address this challenge, detailed and reliable\nmapping is needed to support sustainability efforts and emerging regulatory\nframeworks. We present an open-access geospatial dataset of oil palm\nplantations and related land cover types in Indonesia, produced through expert\nlabeling of high-resolution satellite imagery from 2020 to 2024. The dataset\nprovides polygon-based, wall-to-wall annotations across a range of\nagro-ecological zones and includes a hierarchical typology that distinguishes\noil palm planting stages as well as similar perennial crops. Quality was\nensured through multi-interpreter consensus and field validation. The dataset\nwas created using wall-to-wall digitization over large grids, making it\nsuitable for training and benchmarking both conventional convolutional neural\nnetworks and newer geospatial foundation models. Released under a CC-BY\nlicense, it fills a key gap in training data for remote sensing and aims to\nimprove the accuracy of land cover types mapping. By supporting transparent\nmonitoring of oil palm expansion, the resource contributes to global\ndeforestation reduction goals and follows FAIR data principles.", "AI": {"tldr": "\u672c\u6587\u53d1\u5e03\u4e86\u4e00\u4e2a\u5f00\u653e\u83b7\u53d6\u7684\u5370\u5ea6\u5c3c\u897f\u4e9a\u6cb9\u68d5\u79cd\u690d\u56ed\u53ca\u5176\u76f8\u5173\u571f\u5730\u8986\u76d6\u7c7b\u578b\u7684\u5730\u7406\u7a7a\u95f4\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u521b\u5efa\uff0c\u65e8\u5728\u63d0\u9ad8\u571f\u5730\u8986\u76d6\u6d4b\u7ed8\u7cbe\u5ea6\u5e76\u652f\u6301\u68ee\u6797\u780d\u4f10\u76d1\u6d4b\u3002", "motivation": "\u5370\u5ea6\u5c3c\u897f\u4e9a\u7684\u6cb9\u68d5\u79cd\u690d\u662f\u68ee\u6797\u780d\u4f10\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\u3002\u4e3a\u6709\u6548\u8ffd\u8e2a\u5e76\u5e94\u5bf9\u6b64\u6311\u6218\uff0c\u6025\u9700\u8be6\u7ec6\u53ef\u9760\u7684\u5730\u56fe\u6570\u636e\u4ee5\u652f\u6301\u53ef\u6301\u7eed\u53d1\u5c55\u5de5\u4f5c\u548c\u65b0\u5174\u76d1\u7ba1\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce82020\u5e74\u81f32024\u5e74\u7684\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\uff0c\u751f\u6210\u4e86\u4e00\u4e2a\u5f00\u653e\u83b7\u53d6\u7684\u3001\u57fa\u4e8e\u591a\u8fb9\u5f62\u7684\u3001\u5168\u8986\u76d6\u7684\u5730\u7406\u7a7a\u95f4\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u533a\u5206\u6cb9\u68d5\u79cd\u690d\u9636\u6bb5\u53ca\u7c7b\u4f3c\u591a\u5e74\u751f\u4f5c\u7269\u7684\u5206\u5c42\u7c7b\u578b\u5b66\u3002\u6570\u636e\u8d28\u91cf\u901a\u8fc7\u591a\u89e3\u91ca\u5668\u5171\u8bc6\u548c\u5b9e\u5730\u9a8c\u8bc1\u786e\u4fdd\uff0c\u5e76\u91c7\u7528\u5927\u7f51\u683c\u4e0a\u7684\u5168\u8986\u76d6\u6570\u5b57\u5316\u6280\u672f\u3002", "result": "\u8be5\u6570\u636e\u96c6\u9002\u5408\u8bad\u7ec3\u548c\u57fa\u51c6\u6d4b\u8bd5\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u53ca\u65b0\u578b\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u3002\u5b83\u586b\u8865\u4e86\u9065\u611f\u8bad\u7ec3\u6570\u636e\u7684\u5173\u952e\u7a7a\u767d\uff0c\u65e8\u5728\u663e\u8457\u63d0\u9ad8\u571f\u5730\u8986\u76d6\u7c7b\u578b\u6d4b\u7ed8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u652f\u6301\u5bf9\u6cb9\u68d5\u6269\u5f20\u7684\u900f\u660e\u76d1\u6d4b\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u5168\u7403\u51cf\u5c11\u68ee\u6797\u780d\u4f10\u7684\u76ee\u6807\uff0c\u5e76\u9075\u5faaFAIR\u6570\u636e\u539f\u5219\uff0c\u5bf9\u53ef\u6301\u7eed\u53d1\u5c55\u548c\u73af\u5883\u6cbb\u7406\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.08191", "pdf": "https://arxiv.org/pdf/2509.08191", "abs": "https://arxiv.org/abs/2509.08191", "authors": ["Robert Stephany", "Youngsoo Choi"], "title": "Rollout-LaSDI: Enhancing the long-term accuracy of Latent Space Dynamics", "categories": ["cs.LG"], "comment": "6 pages, 2 figures", "summary": "Solving complex partial differential equations is vital in the physical\nsciences, but often requires computationally expensive numerical methods.\nReduced-order models (ROMs) address this by exploiting dimensionality reduction\nto create fast approximations. While modern ROMs can solve parameterized\nfamilies of PDEs, their predictive power degrades over long time horizons. We\naddress this by (1) introducing a flexible, high-order, yet inexpensive\nfinite-difference scheme and (2) proposing a Rollout loss that trains ROMs to\nmake accurate predictions over arbitrary time horizons. We demonstrate our\napproach on the 2D Burgers equation.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u964d\u9636\u6a21\u578b\uff08ROMs\uff09\u5728\u957f\u65f6\u95f4\u9884\u6d4b\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\u65f6\u51c6\u786e\u6027\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6709\u9650\u5dee\u5206\u65b9\u6848\u548cRollout\u635f\u5931\uff0c\u4ee5\u63d0\u9ad8ROMs\u5728\u4efb\u610f\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u57282D Burgers\u65b9\u7a0b\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u590d\u6742\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u6c42\u89e3\u5728\u7269\u7406\u79d1\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u964d\u9636\u6a21\u578b\u867d\u80fd\u63d0\u4f9b\u5feb\u901f\u8fd1\u4f3c\uff0c\u4f46\u5728\u957f\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u5176\u9884\u6d4b\u80fd\u529b\u4f1a\u663e\u8457\u4e0b\u964d\u3002", "method": ["\u5f15\u5165\u4e00\u79cd\u7075\u6d3b\u3001\u9ad8\u9636\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u7684\u6709\u9650\u5dee\u5206\u65b9\u6848\u3002", "\u63d0\u51fa\u4e00\u79cd\u201cRollout\u635f\u5931\u201d\uff08Rollout loss\uff09\uff0c\u7528\u4e8e\u8bad\u7ec3\u964d\u9636\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u5728\u4efb\u610f\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8fdb\u884c\u51c6\u786e\u9884\u6d4b\u3002"], "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e8c\u7ef4Burgers\u65b9\u7a0b\u4e0a\u5f97\u5230\u4e86\u6709\u6548\u9a8c\u8bc1\u3002", "conclusion": "\u7ed3\u5408\u65b0\u7684\u6709\u9650\u5dee\u5206\u65b9\u6848\u548cRollout\u635f\u5931\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u964d\u9636\u6a21\u578b\u5728\u4efb\u610f\u957f\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u5bf9\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2509.08480", "pdf": "https://arxiv.org/pdf/2509.08480", "abs": "https://arxiv.org/abs/2509.08480", "authors": ["Daniel Braun"], "title": "Acquiescence Bias in Large Language Models", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Acquiescence bias, i.e. the tendency of humans to agree with statements in\nsurveys, independent of their actual beliefs, is well researched and\ndocumented. Since Large Language Models (LLMs) have been shown to be very\ninfluenceable by relatively small changes in input and are trained on\nhuman-generated data, it is reasonable to assume that they could show a similar\ntendency. We present a study investigating the presence of acquiescence bias in\nLLMs across different models, tasks, and languages (English, German, and\nPolish). Our results indicate that, contrary to humans, LLMs display a bias\ntowards answering no, regardless of whether it indicates agreement or\ndisagreement.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0d\u663e\u793a\u4eba\u7c7b\u7684\u9ed8\u8bb8\u504f\u89c1\uff0c\u53cd\u800c\u503e\u5411\u4e8e\u56de\u7b54\u201c\u5426\u201d\u3002", "motivation": "\u9ed8\u8bb8\u504f\u89c1\u5728\u4eba\u7c7b\u8c03\u67e5\u4e2d\u666e\u904d\u5b58\u5728\u3002\u9274\u4e8eLLMs\u6613\u53d7\u8f93\u5165\u5f71\u54cd\u4e14\u57fa\u4e8e\u4eba\u7c7b\u6570\u636e\u8bad\u7ec3\uff0c\u7814\u7a76\u5047\u8bbe\u5b83\u4eec\u53ef\u80fd\u4e5f\u5b58\u5728\u7c7b\u4f3c\u504f\u89c1\u3002", "method": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u4e0d\u540c\u6a21\u578b\u3001\u4efb\u52a1\u548c\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u5fb7\u8bed\u548c\u6ce2\u5170\u8bed\uff09\u4e2dLLMs\u7684\u9ed8\u8bb8\u504f\u89c1\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4eba\u7c7b\u76f8\u53cd\uff0cLLMs\u8868\u73b0\u51fa\u4e00\u79cd\u504f\u5411\u56de\u7b54\u201c\u5426\u201d\u7684\u504f\u89c1\uff0c\u65e0\u8bba\u201c\u5426\u201d\u662f\u8868\u793a\u540c\u610f\u8fd8\u662f\u4e0d\u540c\u610f\u3002", "conclusion": "LLMs\u5e76\u672a\u663e\u793a\u51fa\u4eba\u7c7b\u7684\u9ed8\u8bb8\u504f\u89c1\uff0c\u53cd\u800c\u5448\u73b0\u51fa\u4e00\u79cd\u72ec\u7279\u7684\u201c\u5426\u201d\u503e\u5411\u3002"}}
{"id": "2509.08311", "pdf": "https://arxiv.org/pdf/2509.08311", "abs": "https://arxiv.org/abs/2509.08311", "authors": ["Rongsheng Wang", "Fenghe Tang", "Qingsong Yao", "Rui Yan", "Xu Zhang", "Zhen Huang", "Haoran Lai", "Zhiyang He", "Xiaodong Tao", "Zihang Jiang", "Shaohua Kevin Zhou"], "title": "SimCroP: Radiograph Representation Learning with Similarity-driven Cross-granularity Pre-training", "categories": ["cs.CV"], "comment": "Accepted by MICCAI 2025", "summary": "Medical vision-language pre-training shows great potential in learning\nrepresentative features from massive paired radiographs and reports. However,\nin computed tomography (CT) scans, the distribution of lesions which contain\nintricate structures is characterized by spatial sparsity. Besides, the complex\nand implicit relationships between different pathological descriptions in each\nsentence of the report and their corresponding sub-regions in radiographs pose\nadditional challenges. In this paper, we propose a Similarity-Driven\nCross-Granularity Pre-training (SimCroP) framework on chest CTs, which combines\nsimilarity-driven alignment and cross-granularity fusion to improve radiograph\ninterpretation. We first leverage multi-modal masked modeling to optimize the\nencoder for understanding precise low-level semantics from radiographs. Then,\nsimilarity-driven alignment is designed to pre-train the encoder to adaptively\nselect and align the correct patches corresponding to each sentence in reports.\nThe cross-granularity fusion module integrates multimodal information across\ninstance level and word-patch level, which helps the model better capture key\npathology structures in sparse radiographs, resulting in improved performance\nfor multi-scale downstream tasks. SimCroP is pre-trained on a large-scale\npaired CT-reports dataset and validated on image classification and\nsegmentation tasks across five public datasets. Experimental results\ndemonstrate that SimCroP outperforms both cutting-edge medical self-supervised\nlearning methods and medical vision-language pre-training methods. Codes and\nmodels are available at https://github.com/ToniChopp/SimCroP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSimCroP\u6846\u67b6\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u9a71\u52a8\u5bf9\u9f50\u548c\u8de8\u7c92\u5ea6\u878d\u5408\uff0c\u89e3\u51b3\u4e86CT\u5f71\u50cf\u4e2d\u75c5\u7076\u7a00\u758f\u6027\u548c\u62a5\u544a-\u56fe\u50cf\u590d\u6742\u5bf9\u5e94\u5173\u7cfb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653e\u5c04\u5f71\u50cf\u89e3\u8bfb\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "motivation": "\u5728CT\u626b\u63cf\u4e2d\uff0c\u75c5\u7076\u5206\u5e03\u5177\u6709\u7a7a\u95f4\u7a00\u758f\u6027\u4e14\u7ed3\u6784\u590d\u6742\u3002\u6b64\u5916\uff0c\u62a5\u544a\u4e2d\u4e0d\u540c\u75c5\u7406\u63cf\u8ff0\u4e0eCT\u56fe\u50cf\u5bf9\u5e94\u5b50\u533a\u57df\u4e4b\u95f4\u7684\u5173\u7cfb\u590d\u6742\u4e14\u9690\u853d\uff0c\u7ed9\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u5e26\u6765\u4e86\u989d\u5916\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u540d\u4e3aSimCroP\uff08Similarity-Driven Cross-Granularity Pre-training\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u76f8\u4f3c\u6027\u9a71\u52a8\u5bf9\u9f50\u548c\u8de8\u7c92\u5ea6\u878d\u5408\uff1a1. \u5229\u7528\u591a\u6a21\u6001\u63a9\u7801\u5efa\u6a21\u4f18\u5316\u7f16\u7801\u5668\u4ee5\u7406\u89e3\u4f4e\u7ea7\u8bed\u4e49\u30022. \u8bbe\u8ba1\u76f8\u4f3c\u6027\u9a71\u52a8\u5bf9\u9f50\u6765\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u4ee5\u81ea\u9002\u5e94\u5730\u9009\u62e9\u5e76\u5bf9\u9f50\u62a5\u544a\u4e2d\u6bcf\u4e2a\u53e5\u5b50\u5bf9\u5e94\u7684\u56fe\u50cf\u5757\u30023. \u5f15\u5165\u8de8\u7c92\u5ea6\u878d\u5408\u6a21\u5757\uff0c\u6574\u5408\u5b9e\u4f8b\u7ea7\u522b\u548c\u8bcd-\u56fe\u50cf\u5757\u7ea7\u522b\u7684\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u83b7\u7a00\u758f\u5f71\u50cf\u4e2d\u7684\u5173\u952e\u75c5\u7406\u7ed3\u6784\u3002", "result": "SimCroP\u5728\u5927\u89c4\u6a21CT-\u62a5\u544a\u914d\u5bf9\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSimCroP\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u533b\u5b66\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u548c\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "SimCroP\u6846\u67b6\u901a\u8fc7\u6709\u6548\u7ed3\u5408\u76f8\u4f3c\u6027\u9a71\u52a8\u5bf9\u9f50\u548c\u8de8\u7c92\u5ea6\u878d\u5408\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u83b7\u7a00\u758f\u653e\u5c04\u5f71\u50cf\u4e2d\u7684\u5173\u952e\u75c5\u7406\u7ed3\u6784\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u591a\u5c3a\u5ea6\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u6539\u5584\u4e86\u653e\u5c04\u5f71\u50cf\u89e3\u8bfb\u6548\u679c\u3002"}}
