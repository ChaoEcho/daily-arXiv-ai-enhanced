{"id": "2508.06495", "pdf": "https://arxiv.org/pdf/2508.06495", "abs": "https://arxiv.org/abs/2508.06495", "authors": ["Juliana Resplande Sant'anna Gomes", "Arlindo Rodrigues Galv\u00e3o Filho"], "title": "Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Master Thesis in Computer Science at Federal University on Goias\n  (UFG). Written in Portuguese", "summary": "The accelerated dissemination of disinformation often outpaces the capacity\nfor manual fact-checking, highlighting the urgent need for Semi-Automated\nFact-Checking (SAFC) systems. Within the Portuguese language context, there is\na noted scarcity of publicly available datasets that integrate external\nevidence, an essential component for developing robust AFC systems, as many\nexisting resources focus solely on classification based on intrinsic text\nfeatures. This dissertation addresses this gap by developing, applying, and\nanalyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,\nMuMiN-PT) with external evidence. The approach simulates a user's verification\nprocess, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)\nto extract the main claim from texts and search engine APIs (Google Search API,\nGoogle FactCheck Claims Search API) to retrieve relevant external documents\n(evidence). Additionally, a data validation and preprocessing framework,\nincluding near-duplicate detection, is introduced to enhance the quality of the\nbase corpora.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u8461\u8404\u7259\u8bed\u534a\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\uff08SAFC\uff09\u7cfb\u7edf\u7f3a\u4e4f\u5916\u90e8\u8bc1\u636e\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u641c\u7d22\u5f15\u64ceAPI\u4e30\u5bcc\u73b0\u6709\u65b0\u95fb\u8bed\u6599\u5e93\uff08Fake.Br, COVID19.BR, MuMiN-PT\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u6570\u636e\u9a8c\u8bc1\u6846\u67b6\u3002", "motivation": "\u5047\u4fe1\u606f\u8fc5\u901f\u4f20\u64ad\u8d85\u51fa\u4e86\u4eba\u5de5\u6838\u67e5\u80fd\u529b\uff0c\u6025\u9700\u534a\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u3002\u7136\u800c\uff0c\u5728\u8461\u8404\u7259\u8bed\u73af\u5883\u4e2d\uff0c\u7f3a\u4e4f\u6574\u5408\u5916\u90e8\u8bc1\u636e\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u73b0\u6709\u8d44\u6e90\u591a\u4f9d\u8d56\u6587\u672c\u5185\u90e8\u7279\u5f81\uff0c\u8fd9\u963b\u788d\u4e86\u5065\u58ee\u7684\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\uff08AFC\uff09\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u5f00\u53d1\u5e76\u5e94\u7528\u4e86\u4e00\u5957\u65b9\u6cd5\u8bba\uff0c\u65e8\u5728\u5229\u7528\u5916\u90e8\u8bc1\u636e\u4e30\u5bcc\u8461\u8404\u7259\u8bed\u65b0\u95fb\u8bed\u6599\u5e93\u3002\u8be5\u65b9\u6cd5\u6a21\u62df\u7528\u6237\u9a8c\u8bc1\u8fc7\u7a0b\uff1a\u4f7f\u7528LLM\uff08Gemini 1.5 Flash\uff09\u63d0\u53d6\u6587\u672c\u4e3b\u5f20\uff0c\u5e76\u5229\u7528\u641c\u7d22\u5f15\u64ceAPI\uff08Google Search API, Google FactCheck Claims Search API\uff09\u68c0\u7d22\u76f8\u5173\u5916\u90e8\u8bc1\u636e\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u5305\u542b\u8fd1\u91cd\u590d\u68c0\u6d4b\u7684\u6570\u636e\u9a8c\u8bc1\u548c\u9884\u5904\u7406\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u8bed\u6599\u5e93\u8d28\u91cf\u3002", "result": "\u6210\u529f\u5f00\u53d1\u5e76\u5e94\u7528\u4e86\u4e00\u5957\u5229\u7528\u5916\u90e8\u8bc1\u636e\u4e30\u5bcc\u8461\u8404\u7259\u8bed\u65b0\u95fb\u8bed\u6599\u5e93\u7684\u65b9\u6cd5\u8bba\uff0c\u6709\u6548\u586b\u8865\u4e86\u8be5\u9886\u57df\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u63d0\u5347\u8bed\u6599\u5e93\u8d28\u91cf\u7684\u6570\u636e\u9a8c\u8bc1\u4e0e\u9884\u5904\u7406\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f00\u53d1\u5e76\u5e94\u7528\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8461\u8404\u7259\u8bed\u73af\u5883\u4e2d\u7f3a\u4e4f\u6574\u5408\u5916\u90e8\u8bc1\u636e\u7684\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\u7684\u6311\u6218\uff0c\u4e3a\u672a\u6765\u66f4\u9c81\u68d2\u7684\u534a\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06504", "pdf": "https://arxiv.org/pdf/2508.06504", "abs": "https://arxiv.org/abs/2508.06504", "authors": ["Yao Ge", "Sudeshna Das", "Yuting Guo", "Abeed Sarker"], "title": "Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models", "categories": ["cs.CL", "cs.AI"], "comment": "31 pages, 4 figures, 15 tables", "summary": "Biomedical named entity recognition (NER) is a high-utility natural language\nprocessing (NLP) task, and large language models (LLMs) show promise\nparticularly in few-shot settings (i.e., limited training data). In this\narticle, we address the performance challenges of LLMs for few-shot biomedical\nNER by investigating a dynamic prompting strategy involving retrieval-augmented\ngeneration (RAG). In our approach, the annotated in-context learning examples\nare selected based on their similarities with the input texts, and the prompt\nis dynamically updated for each instance during inference. We implemented and\noptimized static and dynamic prompt engineering techniques and evaluated them\non five biomedical NER datasets. Static prompting with structured components\nincreased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA\n3-70B, relative to basic static prompting. Dynamic prompting further improved\nperformance, with TF-IDF and SBERT retrieval methods yielding the best results,\nimproving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,\nrespectively. These findings highlight the utility of contextually adaptive\nprompts via RAG for biomedical NER.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u52a8\u6001\u63d0\u793a\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5c11\u6837\u672c\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u4e2d\u5b58\u5728\u7684\u6027\u80fd\u6311\u6218\uff0c\u5c3d\u7ba1LLM\u5728\u6b64\u9886\u57df\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u52a8\u6001\u63d0\u793a\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u6839\u636e\u8f93\u5165\u6587\u672c\u52a8\u6001\u9009\u62e9\u5e26\u6ce8\u91ca\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u793a\u4f8b\u5e76\u66f4\u65b0\u63d0\u793a\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u8fd8\u5b9e\u65bd\u5e76\u4f18\u5316\u4e86\u9759\u6001\u548c\u52a8\u6001\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u5e76\u5728\u4e94\u4e2a\u751f\u7269\u533b\u5b66NER\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5176\u4e2d\u68c0\u7d22\u65b9\u6cd5\u5305\u62ecTF-IDF\u548cSBERT\u3002", "result": "\u7ed3\u6784\u5316\u9759\u6001\u63d0\u793a\u4f7fGPT-4\u7684\u5e73\u5747F1\u5206\u6570\u76f8\u5bf9\u57fa\u672c\u9759\u6001\u63d0\u793a\u63d0\u9ad8\u4e8612%\uff0cGPT-3.5\u548cLLaMA 3-70B\u63d0\u9ad8\u4e8611%\u3002\u52a8\u6001\u63d0\u793a\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5176\u4e2dTF-IDF\u548cSBERT\u68c0\u7d22\u65b9\u6cd5\u57285\u6837\u672c\u548c10\u6837\u672c\u8bbe\u7f6e\u4e0b\u5206\u522b\u4f7f\u5e73\u5747F1\u5206\u6570\u63d0\u9ad8\u4e867.3%\u548c5.6%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u901a\u8fc7RAG\u5b9e\u73b0\u7684\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u63d0\u793a\u5728\u751f\u7269\u533b\u5b66NER\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.06524", "pdf": "https://arxiv.org/pdf/2508.06524", "abs": "https://arxiv.org/abs/2508.06524", "authors": ["Lei Jiang", "Fan Chen"], "title": "CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DC", "cs.LG"], "comment": "8 pages", "summary": "Neural scaling laws have driven the development of increasingly large\nlanguage models (LLMs) by linking accuracy improvements to growth in parameter\ncount, dataset size, and compute. However, these laws overlook the carbon\nemissions that scale exponentially with LLM size. This paper presents\n\\textit{CarbonScaling}, an analytical framework that extends neural scaling\nlaws to incorporate both operational and embodied carbon in LLM training. By\nintegrating models for neural scaling, GPU hardware evolution, parallelism\noptimization, and carbon estimation, \\textit{CarbonScaling} quantitatively\nconnects model accuracy to carbon footprint. Results show that while a\npower-law relationship between accuracy and carbon holds, real-world\ninefficiencies significantly increase the scaling factor. Hardware technology\nscaling reduces carbon emissions for small to mid-sized models, but offers\ndiminishing returns for extremely large LLMs due to communication overhead and\nunderutilized GPUs. Training optimizations-especially aggressive critical batch\nsize scaling-help alleviate this inefficiency. \\textit{CarbonScaling} offers\nkey insights for training more sustainable and carbon-efficient LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCarbonScaling\u6846\u67b6\uff0c\u91cf\u5316\u5927\u6a21\u578b\u7cbe\u5ea6\u4e0e\u78b3\u8db3\u8ff9\u5173\u7cfb\uff0c\u6307\u51fa\u78b3\u6392\u653e\u968f\u6a21\u578b\u89c4\u6a21\u5448\u5e42\u5f8b\u589e\u957f\uff0c\u5e76\u63d0\u4f9b\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u4ec5\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u7cbe\u5ea6\u4e0e\u53c2\u6570\u3001\u6570\u636e\u3001\u8ba1\u7b97\u91cf\u7684\u5173\u7cfb\uff0c\u4f46\u5ffd\u7565\u4e86LLM\u89c4\u6a21\u6307\u6570\u7ea7\u589e\u957f\u6240\u5e26\u6765\u7684\u78b3\u6392\u653e\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51faCarbonScaling\u5206\u6790\u6846\u67b6\uff0c\u5c06\u64cd\u4f5c\u548c\u9690\u542b\u78b3\u6392\u653e\u7eb3\u5165LLM\u8bad\u7ec3\u7684\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u3002\u901a\u8fc7\u6574\u5408\u795e\u7ecf\u7f29\u653e\u3001GPU\u786c\u4ef6\u6f14\u8fdb\u3001\u5e76\u884c\u4f18\u5316\u548c\u78b3\u6392\u653e\u4f30\u7b97\u6a21\u578b\uff0cCarbonScaling\u5b9a\u91cf\u8fde\u63a5\u6a21\u578b\u7cbe\u5ea6\u4e0e\u78b3\u8db3\u8ff9\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u7cbe\u5ea6\u4e0e\u78b3\u6392\u653e\u4e4b\u95f4\u5b58\u5728\u5e42\u5f8b\u5173\u7cfb\uff0c\u4f46\u5b9e\u9645\u4f4e\u6548\u7387\u663e\u8457\u589e\u52a0\u4e86\u7f29\u653e\u56e0\u5b50\u3002\u786c\u4ef6\u6280\u672f\u7f29\u653e\u53ef\u964d\u4f4e\u4e2d\u5c0f\u6a21\u578b\u78b3\u6392\u653e\uff0c\u4f46\u5bf9\u8d85\u5927\u578bLLM\u7531\u4e8e\u901a\u4fe1\u5f00\u9500\u548cGPU\u672a\u5145\u5206\u5229\u7528\u800c\u6536\u76ca\u9012\u51cf\u3002\u8bad\u7ec3\u4f18\u5316\uff0c\u7279\u522b\u662f\u6fc0\u8fdb\u7684\u5173\u952e\u6279\u6b21\u5927\u5c0f\u7f29\u653e\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u4f4e\u6548\u7387\u3002", "conclusion": "CarbonScaling\u6846\u67b6\u4e3a\u8bad\u7ec3\u66f4\u53ef\u6301\u7eed\u548c\u78b3\u6548\u7387\u66f4\u9ad8\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2508.06615", "pdf": "https://arxiv.org/pdf/2508.06615", "abs": "https://arxiv.org/abs/2508.06615", "authors": ["Ryan Erik Landvater", "Navin Kathawa", "Mustafa Yousif MD", "Ulysses Balis MD"], "title": "Iris RESTful Server and IrisTileSource: An Iris implementation for existing OpenSeaDragon viewers", "categories": ["cs.NI"], "comment": "10 pages, 4 figures, 1 table", "summary": "The Iris File Extension (IFE) is a low overhead performance-oriented whole\nslide image (WSI) file format designed to improve the image rendering\nexperience for pathologists and simplify image management for system\nadministrators. However, static hypertext transfer protocol (HTTP) file servers\ncannot natively stream subregions of high-resolution image files, such as the\nIFE. The majority of contemporary WSI viewer systems are designed as\nbrowser-based web applications and leverage OpenSeaDragon as the tile-based\nrendering framework. These systems convert WSI files to Deep Zoom Images (DZI)\nfor compatibility with simple static HTTP file servers. In order to address\nthis limitation, we have developed the Iris RESTful Server, a low-overhead HTTP\nserver with a RESTful API that is natively compatible with the DICOMweb WADO-RS\nAPI. Written in C++ with Boost Beast HTTP and Asio networking libraries atop\nthe public IFE libraries, the server offers both security and high performance.\nTesting shows that a single instance can handle over 5000 tile requests per\nsecond with a median latency of 21 ms on a private network. We also developed\nand merged a new OpenSeaDragon TileSource, compatible with the Iris RESTful\nAPI, into the next OpenSeaDragon release, enabling simple and immediate drop-in\nreplacement of DZI images within WSI viewer stacks. Designed as a secure\ncross-origin resource sharing microservice, this architecture includes detailed\ndeployment instructions for new or existing WSI workflows, and the public\nexamples.restful.irisdigitialpathology.org subdomain is provided as a\ndevelopment tool to accelerate WSI web viewer development.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aIris RESTful Server\u7684\u4f4e\u5f00\u9500HTTP\u670d\u52a1\u5668\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u5b89\u5168\u5730\u6d41\u5f0f\u4f20\u8f93Iris\u6587\u4ef6\u6269\u5c55\uff08IFE\uff09\u683c\u5f0f\u7684\u5168\u73bb\u7247\u56fe\u50cf\uff08WSI\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9759\u6001HTTP\u670d\u52a1\u5668\u65e0\u6cd5\u76f4\u63a5\u6d41\u5f0f\u4f20\u8f93\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5b50\u533a\u57df\u7684\u95ee\u9898\uff0c\u5e76\u517c\u5bb9DICOMweb\u548cOpenSeaDragon\u3002", "motivation": "Iris\u6587\u4ef6\u6269\u5c55\uff08IFE\uff09\u662f\u4e00\u79cd\u4f4e\u5f00\u9500\u3001\u9ad8\u6027\u80fd\u7684WSI\u683c\u5f0f\uff0c\u4f46\u4f20\u7edf\u7684\u9759\u6001HTTP\u6587\u4ef6\u670d\u52a1\u5668\u65e0\u6cd5\u539f\u751f\u6d41\u5f0f\u4f20\u8f93\u5176\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u5b50\u533a\u57df\u3002\u5f53\u524d\u7684WSI\u67e5\u770b\u5668\u7cfb\u7edf\u591a\u5c06WSI\u6587\u4ef6\u8f6c\u6362\u4e3aDZI\u683c\u5f0f\u4ee5\u517c\u5bb9\u7b80\u5355\u7684\u9759\u6001HTTP\u670d\u52a1\u5668\uff0c\u8fd9\u662f\u4e00\u79cd\u5c40\u9650\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u76f4\u63a5\u3001\u9ad8\u6548\u3001\u5b89\u5168\u5730\u6d41\u5f0f\u4f20\u8f93IFE\u6587\u4ef6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86Iris RESTful Server\uff0c\u4e00\u4e2a\u57fa\u4e8eC++\u3001\u4f7f\u7528Boost Beast HTTP\u548cAsio\u7f51\u7edc\u5e93\u6784\u5efa\u7684\u4f4e\u5f00\u9500HTTP\u670d\u52a1\u5668\u3002\u8be5\u670d\u52a1\u5668\u5177\u6709\u4e0eDICOMweb WADO-RS API\u539f\u751f\u517c\u5bb9\u7684RESTful API\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u5e76\u5408\u5e76\u4e86\u4e00\u4e2a\u65b0\u7684OpenSeaDragon TileSource\uff0c\u4f7f\u5176\u517c\u5bb9Iris RESTful API\uff0c\u5e76\u8bbe\u8ba1\u4e3a\u5b89\u5168\u7684\u8de8\u57df\u8d44\u6e90\u5171\u4eab\u5fae\u670d\u52a1\u3002", "result": "\u6d4b\u8bd5\u8868\u660e\uff0c\u5355\u4e2a\u670d\u52a1\u5668\u5b9e\u4f8b\u5728\u79c1\u6709\u7f51\u7edc\u4e2d\u6bcf\u79d2\u53ef\u5904\u7406\u8d85\u8fc75000\u4e2a\u74e6\u7247\u8bf7\u6c42\uff0c\u4e2d\u4f4d\u6570\u5ef6\u8fdf\u4e3a21\u6beb\u79d2\u3002\u65b0\u7684OpenSeaDragon TileSource\u4f7fWSI\u67e5\u770b\u5668\u7cfb\u7edf\u80fd\u591f\u7b80\u5355\u3001\u5373\u65f6\u5730\u5c06DZI\u56fe\u50cf\u66ff\u6362\u4e3aIFE\u56fe\u50cf\u3002", "conclusion": "Iris RESTful Server\u6210\u529f\u89e3\u51b3\u4e86IFE\u6587\u4ef6\u7684\u6d41\u5f0f\u4f20\u8f93\u9650\u5236\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u548c\u9ad8\u5b89\u5168\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709OpenSeaDragon\u751f\u6001\u7cfb\u7edf\u7684\u65e0\u7f1d\u96c6\u6210\u3002\u5b83\u901a\u8fc7\u6d88\u9664DZI\u8f6c\u6362\u7684\u9700\u8981\uff0c\u7b80\u5316\u4e86\u56fe\u50cf\u7ba1\u7406\uff0c\u5e76\u6539\u5584\u4e86\u75c5\u7406\u5b66\u5bb6\u7684\u56fe\u50cf\u6e32\u67d3\u4f53\u9a8c\uff0c\u4ece\u800c\u52a0\u901f\u4e86WSI\u7f51\u7edc\u67e5\u770b\u5668\u7684\u5f00\u53d1\u3002"}}
{"id": "2508.06533", "pdf": "https://arxiv.org/pdf/2508.06533", "abs": "https://arxiv.org/abs/2508.06533", "authors": ["Aamod Thakur", "Ajay Nagpal", "Atharva Savarkar", "Kundeshwar Pundalik", "Siddhesh Dosi", "Piyush Sawarkar", "Viraj Thakur", "Rohit Saluja", "Maunendra Sankar Desarkar", "Ganesh Ramakrishnan"], "title": "The Art of Breaking Words: Rethinking Multilingual Tokenizer Design", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While model architecture and training objectives are well-studied,\ntokenization, particularly in multilingual contexts, remains a relatively\nneglected aspect of Large Language Model (LLM) development. Existing tokenizers\noften exhibit high token-to-word ratios, inefficient use of context length, and\nslower inference. We present a systematic study that links vocabulary size,\npre-tokenization rules, and training-corpus composition to both token-to-word\nefficiency and model quality. To ground our analysis in a linguistically\ndiverse context, we conduct extensive experiments on Indic scripts, which\npresent unique challenges due to their high script diversity and orthographic\ncomplexity. Drawing on the insights from these analyses, we propose a novel\nalgorithm for data composition that balances multilingual data for tokenizer\ntraining. Our observations on pretokenization strategies significantly improve\nmodel performance, and our data composition algorithm reduces the average\ntoken-to-word ratio by approximately 6% with respect to the conventional data\nrandomization approach. Our tokenizer achieves more than 40% improvement on\naverage token-to-word ratio against stateof-the-art multilingual Indic models.\nThis improvement yields measurable gains in both model performance and\ninference speed. This highlights tokenization alongside architecture and\ntraining objectives as a critical lever for building efficient, scalable\nmultilingual LLMs", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u63a2\u8ba8\u4e86\u591a\u8bed\u8a00LLM\u4e2d\u5206\u8bcd\u5bf9\u6548\u7387\u548c\u6a21\u578b\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5206\u6790\u548c\u4f18\u5316\u5370\u5ea6\u8bed\u5206\u8bcd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u7ec4\u6210\u7b97\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86token-to-word\u6bd4\u7387\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u5f3a\u8c03\u4e86\u5206\u8bcd\u5728\u9ad8\u6548\u591a\u8bed\u8a00LLM\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u591a\u8bed\u8a00\u73af\u5883\u4e0bLLM\u7684\u5206\u8bcd\u5f00\u53d1\u88ab\u76f8\u5bf9\u5ffd\u89c6\uff0c\u73b0\u6709\u5206\u8bcd\u5668\u5b58\u5728\u9ad8token-to-word\u6bd4\u7387\u3001\u4e0a\u4e0b\u6587\u5229\u7528\u7387\u4f4e\u548c\u63a8\u7406\u6162\u7b49\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u8bcd\u6c47\u91cf\u5927\u5c0f\u3001\u9884\u5206\u8bcd\u89c4\u5219\u548c\u8bad\u7ec3\u8bed\u6599\u7ec4\u6210\u5bf9\u5206\u8bcd\u6548\u7387\u548c\u6a21\u578b\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u5728\u5370\u5ea6\u8bed\u811a\u672c\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u7ec4\u6210\u7b97\u6cd5\uff0c\u4ee5\u5e73\u8861\u591a\u8bed\u8a00\u6570\u636e\u8fdb\u884c\u5206\u8bcd\u5668\u8bad\u7ec3\u3002", "result": "\u9884\u5206\u8bcd\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff1b\u6570\u636e\u7ec4\u6210\u7b97\u6cd5\u4f7f\u5e73\u5747token-to-word\u6bd4\u7387\u6bd4\u4f20\u7edf\u65b9\u6cd5\u964d\u4f4e\u7ea66%\uff1b\u672c\u7814\u7a76\u7684\u5206\u8bcd\u5668\u5728\u5e73\u5747token-to-word\u6bd4\u7387\u4e0a\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u5370\u5ea6\u8bed\u6a21\u578b\u63d0\u5347\u8d85\u8fc740%\uff1b\u8fd9\u4e9b\u6539\u8fdb\u5e26\u6765\u4e86\u6a21\u578b\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\u7684\u53ef\u8861\u91cf\u589e\u76ca\u3002", "conclusion": "\u5206\u8bcd\u4e0e\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u76ee\u6807\u4e00\u6837\uff0c\u662f\u6784\u5efa\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u591a\u8bed\u8a00LLM\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2508.06616", "pdf": "https://arxiv.org/pdf/2508.06616", "abs": "https://arxiv.org/abs/2508.06616", "authors": ["Md Arafat Habib", "Medhat Elsayed", "Yigit Ozcan", "Pedro Enrique Iturria-Rivera", "Majid Bavand", "Melike Erol-Kantarci"], "title": "Generative AI for Intent-Driven Network Management in 6G: A Case Study on Hierarchical Learning Approach", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "With the emergence of 6G, mobile networks are becoming increasingly\nheterogeneous and dynamic, necessitating advanced automation for efficient\nmanagement. Intent-Driven Networks (IDNs) address this by translating\nhigh-level intents into optimization policies. Large Language Models (LLMs) can\nenhance this process by understanding complex human instructions to enable\nadaptive, intelligent automation. Given the rapid advancements in Generative AI\n(GenAI), a comprehensive survey of LLM-based IDN architectures in disaggregated\nRadio Access Network (RAN) environments is both timely and critical. This\narticle provides such a survey, along with a case study on a hierarchical\nlearning-enabled IDN architecture that integrates GenAI across three key\nstages: intent processing, intent validation, and intent execution. Unlike most\nexisting approaches that apply GenAI in the form of LLMs for intent processing\nonly, we propose a hierarchical framework that introduces GenAI across all\nthree stages of IDN. To demonstrate the effectiveness of the proposed IDN\nmanagement architecture, we present a case study based on the latest GenAI\narchitecture named Mamba. The case study shows how the proposed GenAI-driven\narchitecture enhances network performance through intelligent automation,\nsurpassing the performance of the conventional IDN architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5f0f\u610f\u56fe\u9a71\u52a8\u7f51\u7edc\uff08IDN\uff09\u67b6\u6784\uff0c\u5c06\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u6574\u5408\u5230\u610f\u56fe\u5904\u7406\u3001\u9a8c\u8bc1\u548c\u6267\u884c\u7684\u5404\u4e2a\u9636\u6bb5\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eMamba\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u8bc1\u660e\u5176\u57286G\u5f02\u6784\u7f51\u7edc\u4e2d\u4f18\u4e8e\u4f20\u7edfIDN\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "6G\u79fb\u52a8\u7f51\u7edc\u65e5\u76ca\u590d\u6742\uff0c\u9700\u8981\u9ad8\u7ea7\u81ea\u52a8\u5316\u7ba1\u7406\u3002\u610f\u56fe\u9a71\u52a8\u7f51\u7edc\uff08IDN\uff09\u80fd\u5b9e\u73b0\u9ad8\u5c42\u610f\u56fe\u5230\u7b56\u7565\u7684\u8f6c\u6362\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u589e\u5f3a\u5176\u667a\u80fd\u81ea\u52a8\u5316\u80fd\u529b\u3002\u9274\u4e8e\u73b0\u6709GenAI\u5728IDN\u4e2d\u591a\u4ec5\u7528\u4e8e\u610f\u56fe\u5904\u7406\uff0c\u4e9f\u9700\u63a2\u7d22\u5176\u5728IDN\u5168\u6d41\u7a0b\u7684\u6574\u5408\u5e94\u7528\u3002", "method": "\u672c\u6587\u9996\u5148\u5bf9\u57fa\u4e8eLLM\u7684IDN\u67b6\u6784\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5b66\u4e60\u4f7f\u80fd\u7684IDN\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5c06\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u6574\u5408\u5230\u610f\u56fe\u5904\u7406\u3001\u610f\u56fe\u9a8c\u8bc1\u548c\u610f\u56fe\u6267\u884c\u4e09\u4e2a\u5173\u952e\u9636\u6bb5\u3002\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8e\u6700\u65b0GenAI\u67b6\u6784Mamba\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u6f14\u793a\u4e86\u6240\u63d0\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684GenAI\u9a71\u52a8\u5206\u5c42IDN\u67b6\u6784\u901a\u8fc7\u667a\u80fd\u81ea\u52a8\u5316\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u6027\u80fd\uff0c\u5e76\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u610f\u56fe\u9a71\u52a8\u7f51\u7edc\u67b6\u6784\u3002", "conclusion": "\u7ed3\u5408GenAI\u7684\u5206\u5c42IDN\u67b6\u6784\u80fd\u591f\u6709\u6548\u589e\u5f3a6G\u7f51\u7edc\u7684\u7ba1\u7406\uff0c\u5b9e\u73b0\u66f4\u667a\u80fd\u3001\u66f4\u9ad8\u6548\u7684\u81ea\u52a8\u5316\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709IDN\u65b9\u6cd5\u3002"}}
{"id": "2508.06539", "pdf": "https://arxiv.org/pdf/2508.06539", "abs": "https://arxiv.org/abs/2508.06539", "authors": ["Atahan Karagoz"], "title": "Self-Organizing Survival Manifolds: A Theory for Unsupervised Discovery of Prognostic Structures in Biological Systems", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Survival is traditionally modeled as a supervised learning task, reliant on\ncurated outcome labels and fixed covariates. This work rejects that premise. It\nproposes that survival is not an externally annotated target but a geometric\nconsequence: an emergent property of the curvature and flow inherent in\nbiological state space. We develop a theory of Self-Organizing Survival\nManifolds (SOSM), in which survival-relevant dynamics arise from low-curvature\ngeodesic flows on latent manifolds shaped by internal biological constraints. A\nsurvival energy functional based on geodesic curvature minimization is\nintroduced and shown to induce structures where prognosis aligns with geometric\nflow stability. We derive discrete and continuous formulations of the objective\nand prove theoretical results demonstrating the emergence and convergence of\nsurvival-aligned trajectories under biologically plausible conditions. The\nframework draws connections to thermodynamic efficiency, entropy flow, Ricci\ncurvature, and optimal transport, grounding survival modeling in physical law.\nHealth, disease, aging, and death are reframed as geometric phase transitions\nin the manifold's structure. This theory offers a universal, label-free\nfoundation for modeling survival as a property of form, not annotation-bridging\nmachine learning, biophysics, and the geometry of life itself.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u81ea\u7ec4\u7ec7\u751f\u5b58\u6d41\u5f62\uff08SOSM\uff09\u7406\u8bba\uff0c\u5c06\u751f\u5b58\u5efa\u6a21\u4e3a\u751f\u7269\u72b6\u6001\u7a7a\u95f4\u4e2d\u5185\u5728\u7684\u51e0\u4f55\u6027\u8d28\uff0c\u800c\u975e\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\uff0c\u901a\u8fc7\u6d4b\u5730\u7ebf\u66f2\u7387\u6700\u5c0f\u5316\u4f7f\u9884\u540e\u4e0e\u51e0\u4f55\u6d41\u7a33\u5b9a\u6027\u5bf9\u9f50\u3002", "motivation": "\u4f20\u7edf\u7684\u751f\u5b58\u5efa\u6a21\u4f9d\u8d56\u4e8e\u9884\u8bbe\u7684\u6807\u7b7e\u548c\u56fa\u5b9a\u534f\u53d8\u91cf\uff0c\u662f\u4e00\u79cd\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u3002\u672c\u7814\u7a76\u62d2\u7edd\u8fd9\u4e00\u524d\u63d0\uff0c\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u5c06\u751f\u5b58\u89c6\u4e3a\u751f\u7269\u72b6\u6001\u7a7a\u95f4\u4e2d\u66f2\u7387\u548c\u6d41\u7684\u51e0\u4f55\u7ed3\u679c\uff0c\u5373\u4e00\u79cd\u5185\u5728 emergent \u5c5e\u6027\u7684\u65b0\u8303\u5f0f\u3002", "method": "\u5f00\u53d1\u4e86\u81ea\u7ec4\u7ec7\u751f\u5b58\u6d41\u5f62\uff08SOSM\uff09\u7406\u8bba\uff0c\u5176\u4e2d\u751f\u5b58\u76f8\u5173\u52a8\u6001\u6e90\u4e8e\u53d7\u5185\u90e8\u751f\u7269\u7ea6\u675f\u5851\u9020\u7684\u6f5c\u5728\u6d41\u5f62\u4e0a\u7684\u4f4e\u66f2\u7387\u6d4b\u5730\u7ebf\u6d41\u3002\u5f15\u5165\u4e86\u57fa\u4e8e\u6d4b\u5730\u7ebf\u66f2\u7387\u6700\u5c0f\u5316\u7684\u751f\u5b58\u80fd\u91cf\u51fd\u6570\uff0c\u5e76\u63a8\u5bfc\u4e86\u76ee\u6807\u51fd\u6570\u7684\u79bb\u6563\u548c\u8fde\u7eed\u516c\u5f0f\u3002\u8be5\u6846\u67b6\u5c06\u751f\u5b58\u5efa\u6a21\u4e0e\u70ed\u529b\u5b66\u6548\u7387\u3001\u71b5\u6d41\u3001Ricci\u66f2\u7387\u548c\u6700\u4f18\u4f20\u8f93\u7b49\u7269\u7406\u5b9a\u5f8b\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6240\u5f15\u5165\u7684\u751f\u5b58\u80fd\u91cf\u51fd\u6570\u80fd\u591f\u8bf1\u5bfc\u7ed3\u6784\uff0c\u4f7f\u9884\u540e\u4e0e\u51e0\u4f55\u6d41\u7684\u7a33\u5b9a\u6027\u5bf9\u9f50\u3002\u5728\u751f\u7269\u5b66\u5408\u7406\u6761\u4ef6\u4e0b\uff0c\u7406\u8bba\u8bc1\u660e\u4e86\u751f\u5b58\u5bf9\u9f50\u8f68\u8ff9\u7684\u51fa\u73b0\u548c\u6536\u655b\u6027\u3002\u5065\u5eb7\u3001\u75be\u75c5\u3001\u8870\u8001\u548c\u6b7b\u4ea1\u88ab\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6d41\u5f62\u7ed3\u6784\u4e2d\u7684\u51e0\u4f55\u76f8\u53d8\u3002", "conclusion": "\u8be5\u7406\u8bba\u4e3a\u751f\u5b58\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u3001\u65e0\u6807\u7b7e\u7684\u57fa\u7840\uff0c\u5c06\u751f\u5b58\u89c6\u4e3a\u5f62\u5f0f\u7684\u5c5e\u6027\u800c\u975e\u5916\u90e8\u6807\u6ce8\uff0c\u4ece\u800c\u6210\u529f\u8fde\u63a5\u4e86\u673a\u5668\u5b66\u4e60\u3001\u751f\u7269\u7269\u7406\u5b66\u548c\u751f\u547d\u51e0\u4f55\u5b66\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u5b9a\u5f8b\u7684\u5168\u65b0\u751f\u5b58\u5206\u6790\u89c6\u89d2\u3002"}}
{"id": "2508.06559", "pdf": "https://arxiv.org/pdf/2508.06559", "abs": "https://arxiv.org/abs/2508.06559", "authors": ["Sina Baghal"], "title": "Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization", "categories": ["cs.AI", "cs.GT", "cs.LG"], "comment": null, "summary": "Pasur is a fishing card game played over six rounds and is played similarly\nto games such as Cassino and Scopa, and Bastra. This paper introduces a\nCUDA-accelerated computational framework for simulating Pasur, emphasizing\nefficient memory management. We use our framework to compute near-Nash\nequilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm\nfor solving large imperfect-information games.\n  Solving Pasur presents unique challenges due to its intricate rules and the\nlarge size of its game tree. We handle rule complexity using PyTorch CUDA\ntensors and to address the memory-intensive nature of the game, we decompose\nthe game tree into two key components: (1) actual game states, and (2)\ninherited scores from previous rounds. We construct the Full Game Tree by\npairing card states with accumulated scores in the Unfolding Process. This\ndesign reduces memory overhead by storing only essential strategy values and\nnode connections. To further manage computational complexity, we apply a\nround-by-round backward training strategy, starting from the final round and\nrecursively propagating average utilities to earlier stages. Our approach\nconstructs the complete game tree, which on average consists of over $10^9$\nnodes. We provide detailed implementation snippets.\n  After computing a near-Nash equilibrium strategy, we train a tree-based model\nto predict these strategies for use during gameplay. We then estimate the fair\nvalue of each deck through large-scale self-play between equilibrium strategies\nby simulating, for instance, 10,000 games per matchup, executed in parallel\nusing GPU acceleration.\n  Similar frameworks can be extended to other reinforcement learning algorithms\nwhere the action tree naturally decomposes into multiple rounds such as\nturn-based strategy games or sequential trading decisions in financial markets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2aCUDA\u52a0\u901f\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u5229\u7528\u9ad8\u6548\u5185\u5b58\u7ba1\u7406\u548cCFR\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u7eb8\u724c\u6e38\u620fPasur\u7684\u7eb3\u4ec0\u5747\u8861\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u81ea\u535a\u5f08\u8bc4\u4f30\u724c\u7ec4\u4ef7\u503c\u3002", "motivation": "Pasur\u6e38\u620f\u89c4\u5219\u590d\u6742\u4e14\u535a\u5f08\u6811\u5e9e\u5927\uff0c\u7ed9\u8ba1\u7b97\u5176\u8fd1\u7eb3\u4ec0\u5747\u8861\u5e26\u6765\u4e86\u72ec\u7279\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aCUDA\u52a0\u901f\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u5f3a\u8c03\u9ad8\u6548\u5185\u5b58\u7ba1\u7406\u3002\u91c7\u7528\u53cd\u4e8b\u5b9e\u540e\u6094\u6700\u5c0f\u5316\uff08CFR\uff09\u7b97\u6cd5\u6765\u6c42\u89e3\u8fd1\u7eb3\u4ec0\u5747\u8861\u3002\u901a\u8fc7PyTorch CUDA\u5f20\u91cf\u5904\u7406\u89c4\u5219\u590d\u6742\u5ea6\u3002\u5c06\u535a\u5f08\u6811\u5206\u89e3\u4e3a\u201c\u5b9e\u9645\u6e38\u620f\u72b6\u6001\u201d\u548c\u201c\u524d\u51e0\u8f6e\u7ee7\u627f\u5f97\u5206\u201d\u4e24\u90e8\u5206\uff0c\u901a\u8fc7\u5c55\u5f00\u8fc7\u7a0b\u6784\u5efa\u5b8c\u6574\u535a\u5f08\u6811\u4ee5\u51cf\u5c11\u5185\u5b58\u5f00\u9500\u3002\u4f7f\u7528\u9010\u8f6e\u5411\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u4ece\u6700\u7ec8\u8f6e\u5f00\u59cb\u9012\u5f52\u4f20\u64ad\u5e73\u5747\u6548\u7528\u3002\u5728\u8ba1\u7b97\u51fa\u8fd1\u7eb3\u4ec0\u5747\u8861\u7b56\u7565\u540e\uff0c\u8bad\u7ec3\u4e00\u4e2a\u6811\u72b6\u6a21\u578b\u9884\u6d4b\u8fd9\u4e9b\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u81ea\u535a\u5f08\uff08\u4f8b\u5982\u6bcf\u573a\u5bf9\u51b3\u6a21\u62df10,000\u6b21\uff0c\u5e76\u4f7f\u7528GPU\u5e76\u884c\u52a0\u901f\uff09\u8bc4\u4f30\u6bcf\u526f\u724c\u7684\u516c\u5e73\u4ef7\u503c\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u5e73\u5747\u8282\u70b9\u6570\u8d85\u8fc7$10^9$\u7684\u5b8c\u6574\u6e38\u620f\u6811\uff0c\u5e76\u8ba1\u7b97\u51fa\u4e86Pasur\u7684\u8fd1\u7eb3\u4ec0\u5747\u8861\u7b56\u7565\u3002\u5b9e\u73b0\u4e86\u5728GPU\u52a0\u901f\u4e0b\u9ad8\u6548\u7684\u7b56\u7565\u9884\u6d4b\u548c\u724c\u7ec4\u4ef7\u503c\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u590d\u6742\u4e0d\u5b8c\u5168\u4fe1\u606f\u6e38\u620f\uff08\u5982Pasur\uff09\u7eb3\u4ec0\u5747\u8861\u7684\u65b9\u6848\u3002\u7c7b\u4f3c\u6846\u67b6\u53ef\u6269\u5c55\u5e94\u7528\u4e8e\u5176\u4ed6\u884c\u52a8\u6811\u81ea\u7136\u5206\u89e3\u4e3a\u591a\u8f6e\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f8b\u5982\u56de\u5408\u5236\u7b56\u7565\u6e38\u620f\u6216\u91d1\u878d\u5e02\u573a\u4e2d\u7684\u5e8f\u5217\u4ea4\u6613\u51b3\u7b56\u3002"}}
{"id": "2508.06496", "pdf": "https://arxiv.org/pdf/2508.06496", "abs": "https://arxiv.org/abs/2508.06496", "authors": ["Rakesh Raj Madavan", "Akshat Kaimal", "Hashim Faisal", "Chandrakala S"], "title": "Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG", "categories": ["cs.CV", "cs.MA"], "comment": null, "summary": "An ensemble of trained multimodal encoders and vision-language models (VLMs)\nhas become a standard approach for visual question answering (VQA) tasks.\nHowever, such models often fail to produce responses with the detailed\nprecision necessary for complex, domain-specific applications such as medical\nVQA. Our representation model, BIND: BLIVA Integrated with Dense Encoding,\nextends prior multimodal work by refining the joint embedding space through\ndense, query-token-based encodings inspired by contrastive pretraining\ntechniques. This refined encoder powers Med-GRIM, a model designed for medical\nVQA tasks that leverages graph-based retrieval and prompt engineering to\nintegrate domain-specific knowledge. Rather than relying on compute-heavy\nfine-tuning of vision and language models on specific datasets, Med-GRIM\napplies a low-compute, modular workflow with small language models (SLMs) for\nefficiency. Med-GRIM employs prompt-based retrieval to dynamically inject\nrelevant knowledge, ensuring both accuracy and robustness in its responses. By\nassigning distinct roles to each agent within the VQA system, Med-GRIM achieves\nlarge language model performance at a fraction of the computational cost.\nAdditionally, to support scalable research in zero-shot multimodal medical\napplications, we introduce DermaGraph, a novel Graph-RAG dataset comprising\ndiverse dermatological conditions. This dataset facilitates both multimodal and\nunimodal querying. The code and dataset are available at:\nhttps://github.com/Rakesh-123-cryp/Med-GRIM.git", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMed-GRIM\u6a21\u578b\u548cDermaGraph\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7a20\u5bc6\u7f16\u7801\u548c\u56fe\u68c0\u7d22\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u9ad8\u7cbe\u5ea6\u7684\u533b\u5b66VQA\uff0c\u6027\u80fd\u5ab2\u7f8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u73b0\u6709VQA\u6a21\u578b\u5728\u533b\u5b66\u7b49\u590d\u6742\u3001\u9886\u57df\u7279\u5b9a\u5e94\u7528\u4e2d\u7f3a\u4e4f\u7ec6\u8282\u7cbe\u5ea6\uff0c\u4e14\u4f9d\u8d56\u8ba1\u7b97\u91cf\u5927\u7684\u5fae\u8c03\uff0c\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u63d0\u51faBIND\u6a21\u578b\uff0c\u901a\u8fc7\u53d7\u5bf9\u6bd4\u9884\u8bad\u7ec3\u542f\u53d1\u7684\u7a20\u5bc6\u3001\u57fa\u4e8e\u67e5\u8be2\u7684\u7f16\u7801\u6765\u4f18\u5316\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u3002\u57fa\u4e8eBIND\uff0c\u5f00\u53d1Med-GRIM\u533b\u5b66VQA\u6a21\u578b\uff0c\u5229\u7528\u56fe\u68c0\u7d22\u548c\u63d0\u793a\u5de5\u7a0b\u6574\u5408\u9886\u57df\u77e5\u8bc6\uff0c\u91c7\u7528\u4f4e\u8ba1\u7b97\u91cf\u3001\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u7528\u4e8e\u96f6\u6837\u672c\u591a\u6a21\u6001\u533b\u5b66\u5e94\u7528\u7684DermaGraph Graph-RAG\u6570\u636e\u96c6\u3002", "result": "Med-GRIM\u5728\u8ba1\u7b97\u6210\u672c\u5927\u5927\u964d\u4f4e\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u4e86\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u786e\u4fdd\u4e86\u54cd\u5e94\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002DermaGraph\u6570\u636e\u96c6\u652f\u6301\u53ef\u6269\u5c55\u7684\u96f6\u6837\u672c\u591a\u6a21\u6001\u533b\u5b66\u7814\u7a76\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u7684\u8868\u793a\u6a21\u578b\u3001\u9ad8\u6548\u7684\u68c0\u7d22\u673a\u5236\u548c\u4e13\u95e8\u7684\u533b\u5b66\u6570\u636e\u96c6\uff0cMed-GRIM\u4e3a\u533b\u5b66VQA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u6027\u80fd\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9886\u57df\u7279\u5b9a\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u7684\u6311\u6218\u3002"}}
{"id": "2508.06548", "pdf": "https://arxiv.org/pdf/2508.06548", "abs": "https://arxiv.org/abs/2508.06548", "authors": ["Zhanye Luo", "Yuefeng Han", "Xiufan Yu"], "title": "Factor Augmented Supervised Learning with Text Embeddings", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) generate text embeddings from text data,\nproducing vector representations that capture the semantic meaning and\ncontextual relationships of words. However, the high dimensionality of these\nembeddings often impedes efficiency and drives up computational cost in\ndownstream tasks. To address this, we propose AutoEncoder-Augmented Learning\nwith Text (AEALT), a supervised, factor-augmented framework that incorporates\ndimension reduction directly into pre-trained LLM workflows. First, we extract\nembeddings from text documents; next, we pass them through a supervised\naugmented autoencoder to learn low-dimensional, task-relevant latent factors.\nBy modeling the nonlinear structure of complex embeddings, AEALT outperforms\nconventional deep-learning approaches that rely on raw embeddings. We validate\nits broad applicability with extensive experiments on classification, anomaly\ndetection, and prediction tasks using multiple real-world public datasets.\nNumerical results demonstrate that AEALT yields substantial gains over both\nvanilla embeddings and several standard dimension reduction methods.", "AI": {"tldr": "\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u9ad8\u7ef4\u6587\u672c\u5d4c\u5165\u5bfc\u81f4\u4e0b\u6e38\u4efb\u52a1\u6548\u7387\u4f4e\u4e0b\u548c\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86AEALT\uff08AutoEncoder-Augmented Learning with Text\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6709\u76d1\u7763\u7684\u589e\u5f3a\u578b\u81ea\u7f16\u7801\u5668\u5c06\u964d\u7ef4\u76f4\u63a5\u96c6\u6210\u5230LLM\u5de5\u4f5c\u6d41\u4e2d\uff0c\u5b66\u4e60\u4f4e\u7ef4\u3001\u4efb\u52a1\u76f8\u5173\u7684\u6f5c\u5728\u56e0\u5b50\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u5d4c\u5165\u548c\u4f20\u7edf\u964d\u7ef4\u65b9\u6cd5\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u6587\u672c\u5d4c\u5165\u7ef4\u5ea6\u9ad8\uff0c\u5bfc\u81f4\u4e0b\u6e38\u4efb\u52a1\u7684\u6548\u7387\u4f4e\u4e0b\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51faAutoEncoder-Augmented Learning with Text (AEALT) \u6846\u67b6\u3002\u8be5\u6846\u67b6\u662f\u4e00\u4e2a\u6709\u76d1\u7763\u7684\u3001\u56e0\u5b50\u589e\u5f3a\u7684\u964d\u7ef4\u65b9\u6cd5\uff0c\u76f4\u63a5\u6574\u5408\u5230\u9884\u8bad\u7ec3LLM\u5de5\u4f5c\u6d41\u4e2d\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u9996\u5148\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u5d4c\u5165\uff0c\u7136\u540e\u901a\u8fc7\u4e00\u4e2a\u6709\u76d1\u7763\u7684\u589e\u5f3a\u578b\u81ea\u7f16\u7801\u5668\u6765\u5b66\u4e60\u4f4e\u7ef4\u3001\u4efb\u52a1\u76f8\u5173\u7684\u6f5c\u5728\u56e0\u5b50\uff0c\u4ece\u800c\u5efa\u6a21\u590d\u6742\u5d4c\u5165\u7684\u975e\u7ebf\u6027\u7ed3\u6784\u3002", "result": "AEALT\u5728\u5206\u7c7b\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u9884\u6d4b\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u516c\u5171\u6570\u636e\u96c6\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u4f9d\u8d56\u539f\u59cb\u5d4c\u5165\u7684\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0cAEALT\u76f8\u5bf9\u4e8e\u539f\u59cb\u5d4c\u5165\u548c\u51e0\u79cd\u6807\u51c6\u7684\u964d\u7ef4\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "AEALT\u662f\u4e00\u4e2a\u6709\u6548\u7684\u6587\u672c\u5d4c\u5165\u964d\u7ef4\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e14\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u901a\u8fc7\u5b66\u4e60\u4f4e\u7ef4\u3001\u4efb\u52a1\u76f8\u5173\u7684\u6f5c\u5728\u56e0\u5b50\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ef4\u5d4c\u5165\u5e26\u6765\u7684\u8ba1\u7b97\u548c\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2508.06975", "pdf": "https://arxiv.org/pdf/2508.06975", "abs": "https://arxiv.org/abs/2508.06975", "authors": ["Zhengying Lou", "Baha Eddine Youcef Belmekki", "Mohamed-Slim Alouini"], "title": "THz/RF Multi-Hop Routing Throughput: Performance, Optimization, and Application", "categories": ["cs.NI"], "comment": null, "summary": "Terahertz (THz) communication offers a promising solution for high-throughput\nwireless systems. However, the severe path loss of THz signals raises concerns\nabout its effectiveness compared to radio frequency (RF) communication. In this\narticle, we establish the first stochastic geometry (SG)-based analytical\nframework for routing in THz systems. We develop a stepwise optimization\napproach to maximize throughput, including power allocation, relay selection,\nand number of hops design. Analytical expressions for throughput and coverage\nprobability are derived under the SG framework, enabling low complexity and\nscalable performance evaluation. Numerical results show that the proposed\nstepwise-optimal routing strategies not only outperform existing SG-based\nmethods but also approach the ideal upper bound. Moreover, we compare the\nthroughput and coverage performance of THz and RF routing and demonstrate the\napplications of the proposed analytical framework and routing strategies in\nsystem parameter design and unmanned aerial vehicle networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u968f\u673a\u51e0\u4f55\uff08SG\uff09\u7684\u592a\u8d6b\u5179\uff08THz\uff09\u7cfb\u7edf\u8def\u7531\u5206\u6790\u6846\u67b6\uff0c\u5e76\u5f00\u53d1\u4e86\u5206\u6b65\u4f18\u5316\u65b9\u6cd5\u4ee5\u6700\u5927\u5316\u541e\u5410\u91cf\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u63a5\u8fd1\u7406\u8bba\u4e0a\u9650\u3002", "motivation": "\u592a\u8d6b\u5179\u901a\u4fe1\u867d\u5177\u9ad8\u541e\u5410\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u4e25\u91cd\u7684\u8def\u5f84\u635f\u8017\u95ee\u9898\uff0c\u5176\u6709\u6548\u6027\u53d7\u8d28\u7591\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5efa\u7acb\u6709\u6548\u7684\u5206\u6790\u6846\u67b6\u548c\u8def\u7531\u7b56\u7565\u6765\u514b\u670d\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5efa\u7acb\u9996\u4e2a\u57fa\u4e8e\u968f\u673a\u51e0\u4f55\uff08SG\uff09\u7684\u592a\u8d6b\u5179\u7cfb\u7edf\u8def\u7531\u5206\u6790\u6846\u67b6\uff1b2) \u5f00\u53d1\u5206\u6b65\u4f18\u5316\u65b9\u6cd5\u4ee5\u6700\u5927\u5316\u541e\u5410\u91cf\uff0c\u6db5\u76d6\u529f\u7387\u5206\u914d\u3001\u4e2d\u7ee7\u9009\u62e9\u548c\u8df3\u6570\u8bbe\u8ba1\uff1b3) \u5728SG\u6846\u67b6\u4e0b\u63a8\u5bfc\u51fa\u541e\u5410\u91cf\u548c\u8986\u76d6\u6982\u7387\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff1a1) \u6240\u63d0\u51fa\u7684\u5206\u6b65\u4f18\u5316\u8def\u7531\u7b56\u7565\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8eSG\u7684\u65b9\u6cd5\uff0c\u5e76\u63a5\u8fd1\u7406\u60f3\u4e0a\u9650\uff1b2) \u5bf9\u592a\u8d6b\u5179\u548c\u5c04\u9891\u8def\u7531\u7684\u541e\u5410\u91cf\u53ca\u8986\u76d6\u6027\u80fd\u8fdb\u884c\u4e86\u6bd4\u8f83\uff1b3) \u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u5206\u6790\u6846\u67b6\u548c\u8def\u7531\u7b56\u7565\u5728\u7cfb\u7edf\u53c2\u6570\u8bbe\u8ba1\u548c\u65e0\u4eba\u673a\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u968f\u673a\u51e0\u4f55\u7684\u5206\u6790\u6846\u67b6\u548c\u5206\u6b65\u4f18\u5316\u8def\u7531\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u592a\u8d6b\u5179\u7cfb\u7edf\u7684\u541e\u5410\u91cf\u548c\u8986\u76d6\u6027\u80fd\uff0c\u514b\u670d\u8def\u5f84\u635f\u8017\u95ee\u9898\uff0c\u5e76\u4e3a\u592a\u8d6b\u5179\u7cfb\u7edf\u8bbe\u8ba1\u53ca\u65e0\u4eba\u673a\u7f51\u7edc\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u548c\u89c1\u89e3\u3002"}}
{"id": "2508.06574", "pdf": "https://arxiv.org/pdf/2508.06574", "abs": "https://arxiv.org/abs/2508.06574", "authors": ["Fatemeh Moradi", "Mehran Tarif", "Mohammadhossein Homaei"], "title": "Semi-Supervised Supply Chain Fraud Detection with Unsupervised Pre-Filtering", "categories": ["cs.LG", "cs.CR"], "comment": "Six Pages, two Figures and six Tables", "summary": "Detecting fraud in modern supply chains is a growing challenge, driven by the\ncomplexity of global networks and the scarcity of labeled data. Traditional\ndetection methods often struggle with class imbalance and limited supervision,\nreducing their effectiveness in real-world applications. This paper proposes a\nnovel two-phase learning framework to address these challenges. In the first\nphase, the Isolation Forest algorithm performs unsupervised anomaly detection\nto identify potential fraud cases and reduce the volume of data requiring\nfurther analysis. In the second phase, a self-training Support Vector Machine\n(SVM) refines the predictions using both labeled and high-confidence\npseudo-labeled samples, enabling robust semi-supervised learning. The proposed\nmethod is evaluated on the DataCo Smart Supply Chain Dataset, a comprehensive\nreal-world supply chain dataset with fraud indicators. It achieves an F1-score\nof 0.817 while maintaining a false positive rate below 3.0%. These results\ndemonstrate the effectiveness and efficiency of combining unsupervised\npre-filtering with semi-supervised refinement for supply chain fraud detection\nunder real-world constraints, though we acknowledge limitations regarding\nconcept drift and the need for comparison with deep learning approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u548c\u534a\u76d1\u7763\u652f\u6301\u5411\u91cf\u673a\uff0c\u6709\u6548\u5e94\u5bf9\u73b0\u4ee3\u4f9b\u5e94\u94fe\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u6570\u636e\u7a00\u758f\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6311\u6218\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u9ad8F1\u5206\u6570\u3002", "motivation": "\u73b0\u4ee3\u4f9b\u5e94\u94fe\u6b3a\u8bc8\u68c0\u6d4b\u9762\u4e34\u5168\u7403\u7f51\u7edc\u590d\u6742\u6027\u3001\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u4ee5\u53ca\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u76d1\u7763\u53d7\u9650\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\uff0c\u4f7f\u7528Isolation Forest\u7b97\u6cd5\u8fdb\u884c\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\uff0c\u8bc6\u522b\u6f5c\u5728\u6b3a\u8bc8\u5e76\u51cf\u5c11\u5206\u6790\u6570\u636e\u91cf\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff0c\u4f7f\u7528\u81ea\u8bad\u7ec3\u652f\u6301\u5411\u91cf\u673a(SVM)\u7ed3\u5408\u6709\u6807\u7b7e\u548c\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u6837\u672c\u8fdb\u884c\u534a\u76d1\u7763\u5b66\u4e60\uff0c\u4ee5\u7cbe\u70bc\u9884\u6d4b\u3002", "result": "\u5728DataCo\u667a\u80fd\u4f9b\u5e94\u94fe\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e860.817\u7684F1\u5206\u6570\uff0c\u540c\u65f6\u5c06\u8bef\u62a5\u7387\u63a7\u5236\u57283.0%\u4ee5\u4e0b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u65e0\u76d1\u7763\u9884\u8fc7\u6ee4\u548c\u534a\u76d1\u7763\u7cbe\u70bc\u7684\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u7ea6\u675f\u4e0b\uff0c\u5bf9\u4e8e\u4f9b\u5e94\u94fe\u6b3a\u8bc8\u68c0\u6d4b\u662f\u6709\u6548\u4e14\u9ad8\u6548\u7684\uff0c\u4f46\u4ecd\u9700\u6ce8\u610f\u6982\u5ff5\u6f02\u79fb\u5e76\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002"}}
{"id": "2508.06569", "pdf": "https://arxiv.org/pdf/2508.06569", "abs": "https://arxiv.org/abs/2508.06569", "authors": ["Lance Yao", "Suman Samantray", "Ayana Ghosh", "Kevin Roccapriore", "Libor Kovarik", "Sarah Allec", "Maxim Ziatdinov"], "title": "Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "comment": null, "summary": "The history of science is punctuated by serendipitous discoveries, where\nunexpected observations, rather than targeted hypotheses, opened new fields of\ninquiry. While modern autonomous laboratories excel at accelerating hypothesis\ntesting, their optimization for efficiency risks overlooking these crucial,\nunplanned findings. To address this gap, we introduce SciLink, an open-source,\nmulti-agent artificial intelligence framework designed to operationalize\nserendipity in materials research by creating a direct, automated link between\nexperimental observation, novelty assessment, and theoretical simulations. The\nframework employs a hybrid AI strategy where specialized machine learning\nmodels perform quantitative analysis of experimental data, while large language\nmodels handle higher-level reasoning. These agents autonomously convert raw\ndata from materials characterization techniques into falsifiable scientific\nclaims, which are then quantitatively scored for novelty against the published\nliterature. We demonstrate the framework's versatility across diverse research\nscenarios, showcasing its application to atomic-resolution and hyperspectral\ndata, its capacity to integrate real-time human expert guidance, and its\nability to close the research loop by proposing targeted follow-up experiments.\nBy systematically analyzing all observations and contextualizing them, SciLink\nprovides a practical framework for AI-driven materials research that not only\nenhances efficiency but also actively cultivates an environment ripe for\nserendipitous discoveries, thereby bridging the gap between automated\nexperimentation and open-ended scientific exploration.", "AI": {"tldr": "SciLink\u662f\u4e00\u4e2aAI\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u89c2\u5bdf\u5206\u6790\u3001\u65b0\u9896\u6027\u8bc4\u4f30\u548c\u7406\u8bba\u6a21\u62df\uff0c\u5728\u6750\u6599\u7814\u7a76\u4e2d\u4fc3\u8fdb\u610f\u5916\u53d1\u73b0\uff0c\u5f25\u8865\u4e86\u81ea\u52a8\u5316\u5b9e\u9a8c\u5728\u63a2\u7d22\u6027\u7814\u7a76\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u79d1\u5b66\u53f2\u5145\u6ee1\u4e86\u610f\u5916\u53d1\u73b0\uff0c\u4f46\u73b0\u4ee3\u81ea\u52a8\u5316\u5b9e\u9a8c\u5ba4\u867d\u7136\u64c5\u957f\u52a0\u901f\u5047\u8bbe\u68c0\u9a8c\uff0c\u5374\u53ef\u80fd\u56e0\u6548\u7387\u4f18\u5316\u800c\u5ffd\u89c6\u8fd9\u4e9b\u91cd\u8981\u7684\u3001\u8ba1\u5212\u5916\u7684\u53d1\u73b0\uff0c\u4ece\u800c\u5bfc\u81f4\u79d1\u5b66\u63a2\u7d22\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165SciLink\uff0c\u4e00\u4e2a\u5f00\u6e90\u3001\u591a\u667a\u80fd\u4f53AI\u6846\u67b6\u3002\u8be5\u6846\u67b6\u91c7\u7528\u6df7\u5408AI\u7b56\u7565\uff0c\u7ed3\u5408\u4e13\u4e1a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u6570\u636e\u5b9a\u91cf\u5206\u6790\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u7ea7\u63a8\u7406\u3002\u5b83\u80fd\u591f\u81ea\u4e3b\u5c06\u6750\u6599\u8868\u5f81\u7684\u539f\u59cb\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u8bc1\u4f2a\u7684\u79d1\u5b66\u4e3b\u5f20\uff0c\u5e76\u6839\u636e\u5df2\u53d1\u8868\u6587\u732e\u5bf9\u5176\u65b0\u9896\u6027\u8fdb\u884c\u91cf\u5316\u8bc4\u5206\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u9a8c\u89c2\u5bdf\u3001\u65b0\u9896\u6027\u8bc4\u4f30\u548c\u7406\u8bba\u6a21\u62df\u4e4b\u95f4\u7684\u76f4\u63a5\u81ea\u52a8\u5316\u8fde\u63a5\u3002", "result": "\u5c55\u793a\u4e86SciLink\u5728\u591a\u79cd\u7814\u7a76\u573a\u666f\u4e2d\u7684\u591a\u529f\u80fd\u6027\uff0c\u5305\u62ec\u5904\u7406\u539f\u5b50\u5206\u8fa8\u7387\u548c\u9ad8\u5149\u8c31\u6570\u636e\u3001\u6574\u5408\u5b9e\u65f6\u4eba\u7c7b\u4e13\u5bb6\u6307\u5bfc\uff0c\u4ee5\u53ca\u901a\u8fc7\u63d0\u51fa\u6709\u9488\u5bf9\u6027\u7684\u540e\u7eed\u5b9e\u9a8c\u6765\u5b8c\u6210\u7814\u7a76\u95ed\u73af\u3002", "conclusion": "SciLink\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u548c\u8bed\u5883\u5316\u6240\u6709\u89c2\u5bdf\u7ed3\u679c\uff0c\u4e3aAI\u9a71\u52a8\u7684\u6750\u6599\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u6846\u67b6\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u8fd8\u79ef\u6781\u57f9\u517b\u4e86\u6709\u5229\u4e8e\u610f\u5916\u53d1\u73b0\u7684\u73af\u5883\uff0c\u4ece\u800c\u5f25\u5408\u4e86\u81ea\u52a8\u5316\u5b9e\u9a8c\u4e0e\u5f00\u653e\u5f0f\u79d1\u5b66\u63a2\u7d22\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002"}}
{"id": "2508.06511", "pdf": "https://arxiv.org/pdf/2508.06511", "abs": "https://arxiv.org/abs/2508.06511", "authors": ["He Feng", "Yongjia Ma", "Donglin Di", "Lei Fan", "Tonghua Su", "Xiangqian Wu"], "title": "DiTalker: A Unified DiT-based Framework for High-Quality and Speaking Styles Controllable Portrait Animation", "categories": ["cs.CV"], "comment": null, "summary": "Portrait animation aims to synthesize talking videos from a static reference\nface, conditioned on audio and style frame cues (e.g., emotion and head poses),\nwhile ensuring precise lip synchronization and faithful reproduction of\nspeaking styles. Existing diffusion-based portrait animation methods primarily\nfocus on lip synchronization or static emotion transformation, often\noverlooking dynamic styles such as head movements. Moreover, most of these\nmethods rely on a dual U-Net architecture, which preserves identity consistency\nbut incurs additional computational overhead. To this end, we propose DiTalker,\na unified DiT-based framework for speaking style-controllable portrait\nanimation. We design a Style-Emotion Encoding Module that employs two separate\nbranches: a style branch extracting identity-specific style information (e.g.,\nhead poses and movements), and an emotion branch extracting identity-agnostic\nemotion features. We further introduce an Audio-Style Fusion Module that\ndecouples audio and speaking styles via two parallel cross-attention layers,\nusing these features to guide the animation process. To enhance the quality of\nresults, we adopt and modify two optimization constraints: one to improve lip\nsynchronization and the other to preserve fine-grained identity and background\ndetails. Extensive experiments demonstrate the superiority of DiTalker in terms\nof lip synchronization and speaking style controllability. Project Page:\nhttps://thenameishope.github.io/DiTalker/", "AI": {"tldr": "DiTalker\u662f\u4e00\u4e2a\u57fa\u4e8eDiT\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u8bf4\u8bdd\u98ce\u683c\u53ef\u63a7\u7684\u8096\u50cf\u52a8\u753b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u98ce\u683c\uff08\u5982\u5934\u90e8\u8fd0\u52a8\uff09\u63a7\u5236\u4e0a\u7684\u4e0d\u8db3\u53ca\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5e76\u5728\u53e3\u578b\u540c\u6b65\u548c\u98ce\u683c\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8096\u50cf\u52a8\u753b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u53e3\u578b\u540c\u6b65\u6216\u9759\u6001\u60c5\u611f\u8f6c\u6362\uff0c\u5e38\u5ffd\u7565\u5934\u90e8\u8fd0\u52a8\u7b49\u52a8\u6001\u98ce\u683c\uff1b\u540c\u65f6\uff0c\u591a\u6570\u65b9\u6cd5\u91c7\u7528\u53ccU-Net\u67b6\u6784\uff0c\u867d\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u63d0\u51faDiTalker\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u4e8eDiT\u7684\u8096\u50cf\u52a8\u753b\u6846\u67b6\u3002\u8bbe\u8ba1\u4e86\u6837\u5f0f-\u60c5\u611f\u7f16\u7801\u6a21\u5757\uff0c\u5206\u79bb\u63d0\u53d6\u8eab\u4efd\u7279\u5b9a\u6837\u5f0f\uff08\u5982\u5934\u90e8\u59ff\u52bf\u548c\u8fd0\u52a8\uff09\u548c\u8eab\u4efd\u65e0\u5173\u60c5\u611f\u7279\u5f81\uff1b\u5f15\u5165\u4e86\u97f3\u9891-\u6837\u5f0f\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u5e76\u884c\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u89e3\u8026\u97f3\u9891\u548c\u8bf4\u8bdd\u98ce\u683c\u3002\u6b64\u5916\uff0c\u91c7\u7528\u4e86\u4e24\u9879\u4f18\u5316\u7684\u7ea6\u675f\uff0c\u4ee5\u63d0\u5347\u53e3\u578b\u540c\u6b65\u548c\u4fdd\u6301\u8eab\u4efd\u53ca\u80cc\u666f\u7ec6\u8282\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eDiTalker\u5728\u53e3\u578b\u540c\u6b65\u548c\u8bf4\u8bdd\u98ce\u683c\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "DiTalker\u6210\u529f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u9ad8\u5ea6\u53ef\u63a7\u7684\u8096\u50cf\u52a8\u753b\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u5728\u52a8\u6001\u98ce\u683c\u751f\u6210\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u9ad8\u8d28\u91cf\u8bf4\u8bdd\u4eba\u50cf\u89c6\u9891\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.06583", "pdf": "https://arxiv.org/pdf/2508.06583", "abs": "https://arxiv.org/abs/2508.06583", "authors": ["Ying Liu", "Can Li", "Ting Zhang", "Mei Wang", "Qiannan Zhu", "Jian Li", "Hua Huang"], "title": "Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The conversational capabilities of large language models hold significant\npromise for enabling scalable and interactive tutoring. While prior research\nhas primarily examined their capacity for Socratic questioning, it often\noverlooks a critical dimension: adaptively guiding learners based on their\ncognitive states. This study shifts focus from mere question generation to the\nbroader instructional guidance capability. We ask: Can LLMs emulate expert\ntutors who dynamically adjust strategies in response to learners'\nunderstanding? To investigate this, we propose GuideEval, a benchmark grounded\nin authentic educational dialogues that evaluates pedagogical guidance through\na three-phase behavioral framework: (1) Perception, inferring learner states;\n(2) Orchestration, adapting instructional strategies; and (3) Elicitation,\nstimulating proper reflections. Empirical findings reveal that existing LLMs\nfrequently fail to provide effective adaptive scaffolding when learners exhibit\nconfusion or require redirection. Furthermore, we introduce a behavior-guided\nfinetuning strategy that leverages behavior-prompted instructional dialogues,\nsignificantly enhancing guidance performance. By shifting the focus from\nisolated content evaluation to learner-centered interaction, our work advocates\na more dialogic paradigm for evaluating Socratic LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u5173\u6ce8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6559\u5b66\u4e2d\u9002\u5e94\u6027\u5f15\u5bfc\u5b66\u751f\u7684\u80fd\u529b\uff0c\u63d0\u51faGuideEval\u57fa\u51c6\u6765\u8bc4\u4f30\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u6587\u7ae0\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u4e00\u79cd\u884c\u4e3a\u5f15\u5bfc\u7684\u5fae\u8c03\u7b56\u7565\u4ee5\u63d0\u5347\u6027\u80fd\uff0c\u5021\u5bfc\u4ee5\u5b66\u4e60\u8005\u4e3a\u4e2d\u5fc3\u7684\u5bf9\u8bdd\u5f0f\u8bc4\u4f30\u8303\u5f0f\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u82cf\u683c\u62c9\u5e95\u5f0f\u63d0\u95ee\u80fd\u529b\uff0c\u4f46\u5ffd\u89c6\u4e86\u6839\u636e\u5b66\u4e60\u8005\u8ba4\u77e5\u72b6\u6001\u8fdb\u884c\u9002\u5e94\u6027\u5f15\u5bfc\u7684\u5173\u952e\u7ef4\u5ea6\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8LLMs\u662f\u5426\u80fd\u50cf\u4e13\u5bb6\u5bfc\u5e08\u4e00\u6837\uff0c\u6839\u636e\u5b66\u4e60\u8005\u7684\u7406\u89e3\u52a8\u6001\u8c03\u6574\u6559\u5b66\u7b56\u7565\u3002", "method": "\u63d0\u51faGuideEval\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u57fa\u4e8e\u771f\u5b9e\u7684\u6559\u80b2\u5bf9\u8bdd\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u884c\u4e3a\u6846\u67b6\uff08\u611f\u77e5\u5b66\u4e60\u8005\u72b6\u6001\u3001\u7f16\u6392\u6559\u5b66\u7b56\u7565\u3001\u542f\u53d1\u53cd\u601d\uff09\u8bc4\u4f30\u6559\u5b66\u5f15\u5bfc\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u5229\u7528\u884c\u4e3a\u63d0\u793a\u6559\u5b66\u5bf9\u8bdd\u7684\u884c\u4e3a\u5f15\u5bfc\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u7ecf\u9a8c\u53d1\u73b0\uff0c\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u4e60\u8005\u56f0\u60d1\u6216\u9700\u8981\u7ea0\u6b63\u65f6\uff0c\u5f80\u5f80\u65e0\u6cd5\u63d0\u4f9b\u6709\u6548\u7684\u9002\u5e94\u6027\u652f\u67b6\u5f0f\u5f15\u5bfc\u3002\u800c\u65b0\u5f15\u5165\u7684\u884c\u4e3a\u5f15\u5bfc\u5fae\u8c03\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5f15\u5bfc\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u5c06\u8bc4\u4f30\u91cd\u5fc3\u4ece\u5b64\u7acb\u7684\u5185\u5bb9\u8bc4\u4f30\u8f6c\u5411\u4ee5\u5b66\u4e60\u8005\u4e3a\u4e2d\u5fc3\u7684\u4ea4\u4e92\uff0c\u5021\u5bfc\u91c7\u7528\u66f4\u5177\u5bf9\u8bdd\u6027\u7684\u8303\u5f0f\u6765\u8bc4\u4f30\u82cf\u683c\u62c9\u5e95\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u8bc1\u660e\u901a\u8fc7\u884c\u4e3a\u5f15\u5bfc\u5fae\u8c03\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5176\u9002\u5e94\u6027\u5f15\u5bfc\u80fd\u529b\u3002"}}
{"id": "2508.07001", "pdf": "https://arxiv.org/pdf/2508.07001", "abs": "https://arxiv.org/abs/2508.07001", "authors": ["Myeung Suk Oh", "Zhiyao Zhang", "FNU Hairi", "Alvaro Velasquez", "Jia Liu"], "title": "Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": "This paper has been accepted in ACM International Symposium on\n  Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and\n  Mobile Computing (MobiHoc) 2025", "summary": "With wireless devices increasingly forming a unified smart network for\nseamless, user-friendly operations, random access (RA) medium access control\n(MAC) design is considered a key solution for handling unpredictable data\ntraffic from multiple terminals. However, it remains challenging to design an\neffective RA-based MAC protocol to minimize collisions and ensure transmission\nfairness across the devices. While existing multi-agent reinforcement learning\n(MARL) approaches with centralized training and decentralized execution (CTDE)\nhave been proposed to optimize RA performance, their reliance on centralized\ntraining and the significant overhead required for information collection can\nmake real-world applications unrealistic. In this work, we adopt a fully\ndecentralized MARL architecture, where policy learning does not rely on\ncentralized tasks but leverages consensus-based information exchanges across\ndevices. We design our MARL algorithm over an actor-critic (AC) network and\npropose exchanging only local rewards to minimize communication overhead.\nFurthermore, we provide a theoretical proof of global convergence for our\napproach. Numerical experiments show that our proposed MARL algorithm can\nsignificantly improve RA network performance compared to other baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u968f\u673a\u63a5\u5165\uff08RA\uff09MAC\u534f\u8bae\uff0c\u901a\u8fc7\u8bbe\u5907\u95f4\u5171\u8bc6\u4fe1\u606f\u4ea4\u6362\u548c\u4ec5\u4ea4\u6362\u5c40\u90e8\u5956\u52b1\u7684\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "\u667a\u80fd\u7f51\u7edc\u4e2d\u968f\u673a\u63a5\u5165MAC\u8bbe\u8ba1\u9762\u4e34\u9ad8\u51b2\u7a81\u548c\u516c\u5e73\u6027\u6311\u6218\u3002\u73b0\u6709\u96c6\u4e2d\u8bad\u7ec3\u3001\u53bb\u4e2d\u5fc3\u5316\u6267\u884c\uff08CTDE\uff09\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u65b9\u6cd5\u56e0\u96c6\u4e2d\u8bad\u7ec3\u4f9d\u8d56\u548c\u4fe1\u606f\u6536\u96c6\u5f00\u9500\u5927\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684MARL\u67b6\u6784\uff0c\u7b56\u7565\u5b66\u4e60\u901a\u8fc7\u8bbe\u5907\u95f4\u7684\u5171\u8bc6\u4fe1\u606f\u4ea4\u6362\u5b9e\u73b0\uff0c\u800c\u975e\u4f9d\u8d56\u96c6\u4e2d\u4efb\u52a1\u3002\u7b97\u6cd5\u57fa\u4e8eActor-Critic (AC) \u7f51\u7edc\u8bbe\u8ba1\uff0c\u4ec5\u4ea4\u6362\u5c40\u90e8\u5956\u52b1\u4ee5\u6700\u5c0f\u5316\u901a\u4fe1\u5f00\u9500\u3002\u6b64\u5916\uff0c\u63d0\u4f9b\u4e86\u5168\u5c40\u6536\u655b\u7684\u7406\u8bba\u8bc1\u660e\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684MARL\u7b97\u6cd5\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u968f\u673a\u63a5\u5165\u7f51\u7edc\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u53bb\u4e2d\u5fc3\u5316MARL\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4fe1\u606f\u4ea4\u6362\u673a\u5236\u548c\u7406\u8bba\u6536\u655b\u6027\u4fdd\u8bc1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u968f\u673a\u63a5\u5165MAC\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06576", "pdf": "https://arxiv.org/pdf/2508.06576", "abs": "https://arxiv.org/abs/2508.06576", "authors": ["Azmine Toushik Wasi"], "title": "GFlowNets for Learning Better Drug-Drug Interaction Representations", "categories": ["cs.LG", "q-bio.BM", "q-bio.MN"], "comment": "Accepted to ICANN 2025:AIDD", "summary": "Drug-drug interactions pose a significant challenge in clinical pharmacology,\nwith severe class imbalance among interaction types limiting the effectiveness\nof predictive models. Common interactions dominate datasets, while rare but\ncritical interactions remain underrepresented, leading to poor model\nperformance on infrequent cases. Existing methods often treat DDI prediction as\na binary problem, ignoring class-specific nuances and exacerbating bias toward\nfrequent interactions. To address this, we propose a framework combining\nGenerative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)\nto generate synthetic samples for rare classes, improving model balance and\ngenerate effective and novel DDI pairs. Our approach enhances predictive\nperformance across interaction types, ensuring better clinical reliability.", "AI": {"tldr": "\u9488\u5bf9\u836f\u7269-\u836f\u7269\u76f8\u4e92\u4f5c\u7528\uff08DDI\uff09\u9884\u6d4b\u4e2d\u7f55\u89c1\u7c7b\u522b\u7684\u4e25\u91cd\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408GFlowNet\u548cVGAE\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6837\u672c\u6765\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u548c\u4e34\u5e8a\u53ef\u9760\u6027\u3002", "motivation": "\u836f\u7269-\u836f\u7269\u76f8\u4e92\u4f5c\u7528\uff08DDI\uff09\u9884\u6d4b\u5b58\u5728\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u5bfc\u81f4\u73b0\u6709\u6a21\u578b\u5728\u7f55\u89c1\u4f46\u5173\u952e\u7684\u76f8\u4e92\u4f5c\u7528\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1b\u73b0\u6709\u65b9\u6cd5\u5e38\u5c06\u5176\u89c6\u4e3a\u4e8c\u5143\u95ee\u9898\uff0c\u5ffd\u89c6\u7c7b\u522b\u7279\u5f02\u6027\uff0c\u52a0\u5267\u4e86\u5bf9\u5e38\u89c1\u76f8\u4e92\u4f5c\u7528\u7684\u504f\u501a\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u751f\u6210\u6d41\u7f51\u7edc\uff08Generative Flow Networks, GFlowNet\uff09\u4e0e\u53d8\u5206\u56fe\u81ea\u7f16\u7801\u5668\uff08Variational Graph Autoencoders, VGAE\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u7f55\u89c1DDI\u7c7b\u522b\u751f\u6210\u5408\u6210\u6837\u672c\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u5bf9\u4e0d\u540c\u76f8\u4e92\u4f5c\u7528\u7c7b\u578b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u751f\u6210\u5408\u6210\u6837\u672c\uff0c\u8be5\u6846\u67b6\u6539\u5584\u4e86\u6a21\u578b\u5e73\u8861\u6027\uff0c\u751f\u6210\u4e86\u6709\u6548\u4e14\u65b0\u9896\u7684DDI\u5bf9\uff0c\u4ece\u800c\u786e\u4fdd\u4e86\u66f4\u597d\u7684\u4e34\u5e8a\u53ef\u9760\u6027\u3002"}}
{"id": "2508.06571", "pdf": "https://arxiv.org/pdf/2508.06571", "abs": "https://arxiv.org/abs/2508.06571", "authors": ["Anqing Jiang", "Yu Gao", "Yiru Wang", "Zhigang Sun", "Shuo Wang", "Yuwen Heng", "Hao Sun", "Shichen Tang", "Lijuan Zhu", "Jinhao Chai", "Jijun Wang", "Zichong Gu", "Hao Jiang", "Li Sun"], "title": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": "9 pagres, 2 figures", "summary": "Vision-Language-Action (VLA) models have demonstrated potential in autonomous\ndriving. However, two critical challenges hinder their development: (1)\nExisting VLA architectures are typically based on imitation learning in\nopen-loop setup which tends to capture the recorded behaviors in the dataset,\nleading to suboptimal and constrained performance, (2) Close-loop training\nrelies heavily on high-fidelity sensor simulation, where domain gaps and\ncomputational inefficiencies pose significant barriers. In this paper, we\nintroduce IRL-VLA, a novel close-loop Reinforcement Learning via\n\\textbf{I}nverse \\textbf{R}einforcement \\textbf{L}earning reward world model\nwith a self-built VLA approach. Our framework proceeds in a three-stage\nparadigm: In the first stage, we propose a VLA architecture and pretrain the\nVLA policy via imitation learning. In the second stage, we construct a\nlightweight reward world model via inverse reinforcement learning to enable\nefficient close-loop reward computation. To further enhance planning\nperformance, finally, we design specialized reward world model guidence\nreinforcement learning via PPO(Proximal Policy Optimization) to effectively\nbalance the safety incidents, comfortable driving, and traffic efficiency. Our\napproach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving\nbenchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that\nour framework will accelerate VLA research in close-loop autonomous driving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIRL-VLA\uff0c\u4e00\u79cd\u7ed3\u5408\u9006\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u5956\u52b1\u4e16\u754c\u6a21\u578b\u7684\u95ed\u73af\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u514b\u670d\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76VLA\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u591a\u91c7\u7528\u5f00\u73af\u6a21\u4eff\u5b66\u4e60\uff0c\u5bfc\u81f4\u6027\u80fd\u6b21\u4f18\u4e14\u53d7\u9650\uff1b\u540c\u65f6\uff0c\u95ed\u73af\u8bad\u7ec3\u4f9d\u8d56\u9ad8\u4fdd\u771f\u4f20\u611f\u5668\u4eff\u771f\uff0c\u5b58\u5728\u57df\u5dee\u8ddd\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "IRL-VLA\u6846\u67b6\u5206\u4e3a\u4e09\u9636\u6bb5\uff1a1. \u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u9884\u8bad\u7ec3VLA\u7b56\u7565\u30022. \u6784\u5efa\u8f7b\u91cf\u7ea7\u9006\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u4e16\u754c\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u95ed\u73af\u5956\u52b1\u8ba1\u7b97\u30023. \u5229\u7528\u5956\u52b1\u4e16\u754c\u6a21\u578b\u5f15\u5bfc\u7684PPO\u5f3a\u5316\u5b66\u4e60\uff0c\u4f18\u5316\u9a7e\u9a76\u884c\u4e3a\uff0c\u5e73\u8861\u5b89\u5168\u6027\u3001\u8212\u9002\u6027\u548c\u4ea4\u901a\u6548\u7387\u3002", "result": "\u8be5\u65b9\u6cd5\u5728NAVSIM v2\u7aef\u5230\u7aef\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u6027\u80fd\uff0c\u5e76\u5728CVPR2025\u81ea\u52a8\u9a7e\u9a76\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u4e9a\u519b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684IRL-VLA\u6846\u67b6\u6709\u671b\u52a0\u901f\u95ed\u73af\u81ea\u52a8\u9a7e\u9a76\u9886\u57dfVLA\u7814\u7a76\u7684\u8fdb\u5c55\u3002"}}
{"id": "2508.06515", "pdf": "https://arxiv.org/pdf/2508.06515", "abs": "https://arxiv.org/abs/2508.06515", "authors": ["Minh Duc Chu", "Kshitij Pawar", "Zihao He", "Roxanna Sharifi", "Ross Sonnenblick", "Magdalayna Curry", "Laura D'Adamo", "Lindsay Young", "Stuart B Murray", "Kristina Lerman"], "title": "BigTokDetect: A Clinically-Informed Vision-Language Model Framework for Detecting Pro-Bigorexia Videos on TikTok", "categories": ["cs.CV"], "comment": null, "summary": "Social media platforms increasingly struggle to detect harmful content that\npromotes muscle dysmorphic behaviors, particularly pro-bigorexia content that\ndisproportionately affects adolescent males. Unlike traditional eating disorder\ndetection focused on the \"thin ideal,\" pro-bigorexia material masquerades as\nlegitimate fitness content through complex multimodal combinations of visual\ndisplays, coded language, and motivational messaging that evade text-based\ndetection systems. We address this challenge by developing BigTokDetect, a\nclinically-informed detection framework for identifying pro-bigorexia content\non TikTok. We introduce BigTok, the first expert-annotated multimodal dataset\nof over 2,200 TikTok videos labeled by clinical psychologists and psychiatrists\nacross five primary categories spanning body image, nutrition, exercise,\nsupplements, and masculinity. Through a comprehensive evaluation of\nstate-of-the-art vision language models, we achieve 0.829% accuracy on primary\ncategory classification and 0.690% on subcategory detection via domain-specific\nfinetuning. Our ablation studies demonstrate that multimodal fusion improves\nperformance by 5-10% over text-only approaches, with video features providing\nthe most discriminative signals. These findings establish new benchmarks for\nmultimodal harmful content detection and provide both the computational tools\nand methodological framework needed for scalable content moderation in\nspecialized mental health domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u591a\u6a21\u6001\u68c0\u6d4b\u6846\u67b6BigTokDetect\u53ca\u5176\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6BigTok\uff0c\u65e8\u5728\u8bc6\u522bTikTok\u4e0a\u5ba3\u4f20\u808c\u8089\u53d8\u5f62\u75c7\uff08\u5927\u808c\u75c7\uff09\u7684\u6709\u5bb3\u5185\u5bb9\uff0c\u5e76\u8bc1\u660e\u591a\u6a21\u6001\u878d\u5408\u80fd\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u6709\u5bb3\u5185\u5bb9\u5ba1\u6838\u8bbe\u7acb\u65b0\u57fa\u51c6\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u96be\u4ee5\u6709\u6548\u68c0\u6d4b\u63a8\u5e7f\u808c\u8089\u53d8\u5f62\u75c7\u7684\u6709\u5bb3\u5185\u5bb9\uff0c\u7279\u522b\u662f\u9488\u5bf9\u9752\u5c11\u5e74\u7537\u6027\u7684\u201c\u5927\u808c\u75c7\u201d\u4fe1\u606f\u3002\u8fd9\u7c7b\u5185\u5bb9\u5e38\u4ee5\u5408\u6cd5\u5065\u8eab\u5f62\u5f0f\u51fa\u73b0\uff0c\u901a\u8fc7\u590d\u6742\u7684\u89c6\u89c9\u3001\u7f16\u7801\u8bed\u8a00\u548c\u6fc0\u52b1\u4fe1\u606f\u7ec4\u5408\u89c4\u907f\u4f20\u7edf\u6587\u672c\u68c0\u6d4b\u7cfb\u7edf\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u591a\u6a21\u6001\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e34\u5e8a\u77e5\u60c5\u7684BigTokDetect\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522bTikTok\u4e0a\u7684\u5927\u808c\u75c7\u5185\u5bb9\u3002\u6784\u5efa\u4e86\u9996\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6BigTok\uff0c\u5305\u542b2200\u591a\u4e2aTikTok\u89c6\u9891\uff0c\u7531\u4e34\u5e8a\u5fc3\u7406\u5b66\u5bb6\u548c\u7cbe\u795e\u75c5\u5b66\u5bb6\u5728\u8eab\u4f53\u5f62\u8c61\u3001\u8425\u517b\u3001\u953b\u70bc\u3001\u8865\u5145\u5242\u548c\u7537\u5b50\u6c14\u6982\u7b49\u4e94\u4e2a\u4e3b\u8981\u7c7b\u522b\u4e0b\u8fdb\u884c\u6807\u6ce8\u3002\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff0c\u5168\u9762\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u4e3b\u8981\u7c7b\u522b\u5206\u7c7b\u4e0a\u5b9e\u73b0\u4e860.829\u7684\u51c6\u786e\u7387\uff0c\u5728\u5b50\u7c7b\u522b\u68c0\u6d4b\u4e0a\u5b9e\u73b0\u4e860.690\u7684\u51c6\u786e\u7387\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u591a\u6a21\u6001\u878d\u5408\u6bd4\u7eaf\u6587\u672c\u65b9\u6cd5\u6027\u80fd\u63d0\u53475-10%\uff0c\u5176\u4e2d\u89c6\u9891\u7279\u5f81\u63d0\u4f9b\u4e86\u6700\u5177\u533a\u5206\u6027\u7684\u4fe1\u53f7\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u591a\u6a21\u6001\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u8ba1\u7b97\u5de5\u5177\u548c\u65b9\u6cd5\u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u4e13\u4e1a\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u53ef\u6269\u5c55\u5185\u5bb9\u5ba1\u6838\u3002"}}
{"id": "2508.06595", "pdf": "https://arxiv.org/pdf/2508.06595", "abs": "https://arxiv.org/abs/2508.06595", "authors": ["Xiaoyuan Zhu", "Muru Zhang", "Ollie Liu", "Robin Jia", "Willie Neiswanger"], "title": "LLM Unlearning Without an Expert Curated Dataset", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern large language models often encode sensitive, harmful, or copyrighted\nknowledge, raising the need for post-hoc unlearning-the ability to remove\nspecific domains of knowledge from a model without full retraining. A major\nbottleneck in current unlearning pipelines is constructing effective forget\nsets-datasets that approximate the target domain and guide the model to forget\nit. In this work, we introduce a scalable, automated approach to generate\nhigh-quality forget sets using language models themselves. Our method\nsynthesizes textbook-style data through a structured prompting pipeline,\nrequiring only a domain name as input. Through experiments on unlearning\nbiosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic\ndatasets consistently outperform the baseline synthetic alternatives and are\ncomparable to the expert-curated ones. Additionally, ablation studies reveal\nthat the multi-step generation pipeline significantly boosts data diversity,\nwhich in turn improves unlearning utility. Overall, our findings suggest that\nsynthetic datasets offer a promising path toward practical, scalable unlearning\nfor a wide range of emerging domains without the need for manual intervention.\nWe release our code and dataset at\nhttps://github.com/xyzhu123/Synthetic_Textbook.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u201c\u9057\u5fd8\u6570\u636e\u96c6\u201d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u77e5\u8bc6\u7684\u9057\u5fd8\uff0c\u5176\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u5408\u6210\u6570\u636e\u5e76\u5ab2\u7f8e\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e38\u5305\u542b\u654f\u611f\u3001\u6709\u5bb3\u6216\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u77e5\u8bc6\uff0c\u9700\u8981\u4e8b\u540e\u9057\u5fd8\u3002\u73b0\u6709\u9057\u5fd8\u6d41\u7a0b\u7684\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u6784\u5efa\u6709\u6548\u7684\u201c\u9057\u5fd8\u6570\u636e\u96c6\u201d\uff0c\u4ee5\u6307\u5bfc\u6a21\u578b\u79fb\u9664\u7279\u5b9a\u77e5\u8bc6\u9886\u57df\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u672c\u8eab\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u9057\u5fd8\u6570\u636e\u96c6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u7ba1\u9053\u5408\u6210\u201c\u6559\u79d1\u4e66\u5f0f\u201d\u6570\u636e\uff0c\u4ec5\u9700\u63d0\u4f9b\u4e00\u4e2a\u9886\u57df\u540d\u79f0\u4f5c\u4e3a\u8f93\u5165\u3002", "result": "\u5728\u751f\u7269\u5b89\u5168\u3001\u7f51\u7edc\u5b89\u5168\u548c\u300a\u54c8\u5229\u00b7\u6ce2\u7279\u300b\u5c0f\u8bf4\u7684\u9057\u5fd8\u5b9e\u9a8c\u4e2d\uff0c\u4f5c\u8005\u7684\u5408\u6210\u6570\u636e\u96c6\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u5408\u6210\u66ff\u4ee3\u54c1\uff0c\u5e76\u4e0e\u4e13\u5bb6\u7b56\u5212\u7684\u6570\u636e\u96c6\u6548\u679c\u76f8\u5f53\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u591a\u6b65\u9aa4\u751f\u6210\u7ba1\u9053\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u591a\u6837\u6027\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u9057\u5fd8\u6548\u7528\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u96c6\u4e3a\u5404\u79cd\u65b0\u5174\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u3001\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u77e5\u8bc6\u9057\u5fd8\u9014\u5f84\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002"}}
{"id": "2508.07194", "pdf": "https://arxiv.org/pdf/2508.07194", "abs": "https://arxiv.org/abs/2508.07194", "authors": ["Jack Wampler", "Hammas Bin Tanveer", "Rishab Nithyanand", "Eric Wustrow"], "title": "ProtoScan: Measuring censorship in IPv6", "categories": ["cs.NI"], "comment": "10 pages, 2 figures, 2 tables", "summary": "Internet censorship continues to impact billions of people worldwide, and\nmeasurement of it remains an important focus of research. However, most\nInternet censorship measurements have focused solely on the IPv4 Internet\ninfrastructure. Yet, more clients and servers are available over IPv6:\nAccording to Google, over a third of their users now have native IPv6 access.\nGiven the slow-but-steady rate of IPv6 adoption, it is important to understand\nits impact on censorship. In this paper, we measure and analyze how censorship\ndiffers over IPv6 compared to the well-studied IPv4 censorship systems in use\ntoday. We perform a comprehensive global study of censorship across an array of\ncommonly censored protocols, including HTTP, DNS, and TLS, on both IPv4 and\nIPv6, and compare the results. We find that there are several differences in\nhow countries censor IPv6 traffic, both in terms of IPv6 resources, and in\nwhere and what blocklists or technologies are deployed on IPv6 networks. Many\nof these differences are not all-or-nothing: we find that most censors have\nsome capacity to block in IPv6, but are less comprehensive or less reliable\ncompared to their IPv4 censorship systems. Our results suggest that IPv6 offers\nnew areas for censorship circumvention researchers to explore, providing\npotentially new ways to evade censors. As more users gain access to IPv6\naddresses and networks, there will be a need for tools that take advantage of\nIPv6 techniques and infrastructure to bypass censorship.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cIPv6\u5ba1\u67e5\u4e0eIPv4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u901a\u5e38\u4e0d\u5982IPv4\u5168\u9762\u6216\u53ef\u9760\uff0c\u4e3a\u89c4\u907f\u5ba1\u67e5\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u4e92\u8054\u7f51\u5ba1\u67e5\u5f71\u54cd\u6570\u5341\u4ebf\u4eba\uff0c\u4f46\u73b0\u6709\u6d4b\u91cf\u591a\u96c6\u4e2d\u4e8eIPv4\u3002\u9274\u4e8eIPv6\u7528\u6237\u4e0d\u65ad\u589e\u957f\uff08\u8c37\u6b4c\u6570\u636e\u663e\u793a\u8d85\u8fc7\u4e09\u5206\u4e4b\u4e00\u7528\u6237\u62e5\u6709\u539f\u751fIPv6\u8bbf\u95ee\uff09\uff0c\u7406\u89e3IPv6\u5bf9\u5ba1\u67e5\u7684\u5f71\u54cd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5bf9IPv4\u548cIPv6\u4e0a\u5e38\u89c1\u7684\u5ba1\u67e5\u534f\u8bae\uff08\u5305\u62ecHTTP\u3001DNS\u548cTLS\uff09\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5168\u7403\u5ba1\u67e5\u6d4b\u91cf\uff0c\u5e76\u6bd4\u8f83\u4e86\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5404\u56fd\u5728\u5ba1\u67e5IPv6\u6d41\u91cf\u65b9\u9762\u5b58\u5728\u591a\u9879\u5dee\u5f02\uff0c\u5305\u62ec\u5bf9IPv6\u8d44\u6e90\u7684\u5ba1\u67e5\u65b9\u5f0f\u4ee5\u53ca\u5728IPv6\u7f51\u7edc\u4e0a\u90e8\u7f72\u7684\u9ed1\u540d\u5355\u6216\u6280\u672f\u3002\u591a\u6570\u5ba1\u67e5\u8005\u5177\u5907\u4e00\u5b9a\u7684IPv6\u5c01\u9501\u80fd\u529b\uff0c\u4f46\u4e0eIPv4\u5ba1\u67e5\u7cfb\u7edf\u76f8\u6bd4\uff0c\u5176\u5168\u9762\u6027\u548c\u53ef\u9760\u6027\u8f83\u4f4e\u3002", "conclusion": "IPv6\u4e3a\u5ba1\u67e5\u89c4\u907f\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u63a2\u7d22\u9886\u57df\uff0c\u53ef\u80fd\u5e26\u6765\u89c4\u907f\u5ba1\u67e5\u7684\u65b0\u65b9\u6cd5\u3002\u968f\u7740\u66f4\u591a\u7528\u6237\u83b7\u5f97IPv6\u5730\u5740\u548c\u7f51\u7edc\uff0c\u5c06\u9700\u8981\u5229\u7528IPv6\u6280\u672f\u548c\u57fa\u7840\u8bbe\u65bd\u6765\u7ed5\u8fc7\u5ba1\u67e5\u7684\u5de5\u5177\u3002"}}
{"id": "2508.06587", "pdf": "https://arxiv.org/pdf/2508.06587", "abs": "https://arxiv.org/abs/2508.06587", "authors": ["A. Quadir", "M. Tanveer"], "title": "Hypergraph Neural Network with State Space Models for Node Classification", "categories": ["cs.LG"], "comment": null, "summary": "In recent years, graph neural networks (GNNs) have gained significant\nattention for node classification tasks on graph-structured data. However,\ntraditional GNNs primarily focus on adjacency relationships between nodes,\noften overlooking the rich role-based characteristics that are crucial for\nlearning more expressive node representations. Existing methods for capturing\nrole-based features are largely unsupervised and fail to achieve optimal\nperformance in downstream tasks. To address these limitations, we propose a\nnovel hypergraph neural network with state space model (HGMN) that effectively\nintegrates role-aware representations into GNNs and the state space model. HGMN\nutilizes hypergraph construction techniques to model higher-order relationships\nand combines role-based and adjacency-based representations through a learnable\nmamba transformer mechanism. By leveraging two distinct hypergraph construction\nmethods-based on node degree and neighborhood levels, it strengthens the\nconnections among nodes with similar roles, enhancing the model's\nrepresentational power. Additionally, the inclusion of hypergraph convolution\nlayers enables the model to capture complex dependencies within hypergraph\nstructures. To mitigate the over-smoothing problem inherent in deep GNNs, we\nincorporate a residual network, ensuring improved stability and better feature\npropagation across layers. Extensive experiments conducted on one newly\nintroduced dataset and four benchmark datasets demonstrate the superiority of\nHGMN. The model achieves significant performance improvements on node\nclassification tasks compared to state-of-the-art GNN methods. These results\nhighlight HGMN's ability to provide enriched node representations by\neffectively embedding role-based features alongside adjacency information,\nmaking it a versatile and powerful tool for a variety of graph-based learning\napplications.", "AI": {"tldr": "\u63d0\u51faHGMN\uff0c\u4e00\u79cd\u878d\u5408\u4e86\u89d2\u8272\u548c\u90bb\u63a5\u7279\u5f81\u7684\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u6355\u6349\u9ad8\u9636\u5173\u7cfb\u548c\u7f13\u89e3\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8282\u70b9\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u4e3b\u8981\u5173\u6ce8\u90bb\u63a5\u5173\u7cfb\uff0c\u5ffd\u7565\u4e86\u5bf9\u8282\u70b9\u8868\u793a\u81f3\u5173\u91cd\u8981\u7684\u4e30\u5bcc\u89d2\u8272\u7279\u5f81\u3002\u73b0\u6709\u6355\u83b7\u89d2\u8272\u7279\u5f81\u7684\u65b9\u6cd5\u5927\u591a\u662f\u65e0\u76d1\u7763\u7684\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faHGMN\uff08\u5e26\u6709\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\uff0c\u901a\u8fc7\u8d85\u56fe\u6784\u5efa\u6280\u672f\uff08\u57fa\u4e8e\u8282\u70b9\u5ea6\u6570\u548c\u90bb\u57df\u7ea7\u522b\uff09\u5efa\u6a21\u9ad8\u9636\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u53ef\u5b66\u4e60\u7684Mamba Transformer\u673a\u5236\u7ed3\u5408\u89d2\u8272\u4e0e\u90bb\u63a5\u8868\u793a\u3002\u6a21\u578b\u5305\u542b\u8d85\u56fe\u5377\u79ef\u5c42\u4ee5\u6355\u83b7\u590d\u6742\u4f9d\u8d56\uff0c\u5e76\u5f15\u5165\u6b8b\u5dee\u7f51\u7edc\u4ee5\u7f13\u89e3\u6df1\u5ea6GNN\u4e2d\u7684\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002", "result": "\u5728\u65b0\u5efa\u6570\u636e\u96c6\u548c\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHGMN\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684GNN\u65b9\u6cd5\uff0c\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "HGMN\u901a\u8fc7\u6709\u6548\u5d4c\u5165\u89d2\u8272\u7279\u5f81\u548c\u90bb\u63a5\u4fe1\u606f\uff0c\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u8282\u70b9\u8868\u793a\uff0c\u4f7f\u5176\u6210\u4e3a\u5404\u79cd\u57fa\u4e8e\u56fe\u7684\u5b66\u4e60\u5e94\u7528\u7684\u901a\u7528\u800c\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2508.06585", "pdf": "https://arxiv.org/pdf/2508.06585", "abs": "https://arxiv.org/abs/2508.06585", "authors": ["Jayant Sravan Tamarapalli", "Rynaa Grover", "Nilay Pande", "Sahiti Yerramilli"], "title": "CountQA: How Well Do MLLMs Count in the Wild?", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in\nunderstanding visual scenes, yet they exhibit a critical lack in a fundamental\ncognitive skill: object counting. This blind spot severely limits their\nreliability in real-world applications. To date, this capability has been\nlargely unevaluated in complex scenarios, as existing benchmarks either feature\nsparse object densities or are confined to specific visual domains, failing to\ntest models under realistic conditions. Addressing this gap, we introduce\nCountQA, a challenging new benchmark designed to probe this deficiency.\nComprising over 1,500 question-answer pairs, CountQA features real-world images\nwith high object density, clutter, and occlusion. We investigate this weakness\nby evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the\ntop-performing model achieves a mere 42.9% accuracy, with performance declining\nas object counts rise. By providing a dedicated benchmark to diagnose and\nrectify this core weakness, CountQA paves the way for a new generation of MLLMs\nthat are not only descriptively fluent but also numerically grounded and\nspatially aware. We will open-source the dataset and code upon paper acceptance\nto foster further research.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u7269\u4f53\u8ba1\u6570\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e2d\u3002\u672c\u6587\u5f15\u5165\u4e86CountQA\u57fa\u51c6\u6765\u8bc4\u4f30\u8fd9\u4e00\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u9876\u7ea7MLLMs\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdc\u4f4e\u4e8e\u9884\u671f\u3002", "motivation": "MLLMs\u867d\u7136\u64c5\u957f\u7406\u89e3\u89c6\u89c9\u573a\u666f\uff0c\u4f46\u5728\u7269\u4f53\u8ba1\u6570\u8fd9\u4e00\u57fa\u672c\u8ba4\u77e5\u6280\u80fd\u4e0a\u4e25\u91cd\u4e0d\u8db3\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u672a\u80fd\u6709\u6548\u6d4b\u8bd5\u6a21\u578b\u5728\u590d\u6742\u3001\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u8ba1\u6570\u80fd\u529b\uff0c\u5b58\u5728\u8bc4\u4f30\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u4e86CountQA\uff0c\u4e00\u4e2a\u5305\u542b1500\u591a\u5bf9\u95ee\u7b54\u7684\u65b0\u6311\u6218\u6027\u57fa\u51c6\uff0c\u5176\u7279\u70b9\u662f\u4f7f\u7528\u5177\u6709\u9ad8\u7269\u4f53\u5bc6\u5ea6\u3001\u6742\u4e71\u548c\u906e\u6321\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u3002\u901a\u8fc7\u5728CountQA\u57fa\u51c6\u4e0a\u8bc4\u4f3015\u4e2a\u4e3b\u6d41MLLMs\u6765\u63ed\u793a\u5b83\u4eec\u7684\u8ba1\u6570\u5f31\u70b9\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6027\u80fd\u6700\u4f73\u7684MLLM\u5728CountQA\u4e0a\u7684\u51c6\u786e\u7387\u4e5f\u4ec5\u4e3a42.9%\uff0c\u4e14\u968f\u7740\u7269\u4f53\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5176\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "CountQA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bca\u65ad\u548c\u7ea0\u6b63MLLMs\u5728\u6570\u503c\u7406\u89e3\u548c\u7a7a\u95f4\u611f\u77e5\u65b9\u9762\u7684\u6838\u5fc3\u5f31\u70b9\u3002\u5b83\u65e8\u5728\u63a8\u52a8\u65b0\u4e00\u4ee3MLLMs\u7684\u53d1\u5c55\uff0c\u4f7f\u5176\u4e0d\u4ec5\u5728\u63cf\u8ff0\u4e0a\u6d41\u7545\uff0c\u800c\u4e14\u5728\u6570\u503c\u4e0a\u66f4\u52a0\u51c6\u786e\u53ef\u9760\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2508.06517", "pdf": "https://arxiv.org/pdf/2508.06517", "abs": "https://arxiv.org/abs/2508.06517", "authors": ["Haoran Xi", "Chen Liu", "Xiaolin Li"], "title": "Frequency Prior Guided Matching: A Data Augmentation Approach for Generalizable Semi-Supervised Polyp Segmentation", "categories": ["cs.CV"], "comment": "19 pages, 8 figures, 6 tables", "summary": "Automated polyp segmentation is essential for early diagnosis of colorectal\ncancer, yet developing robust models remains challenging due to limited\nannotated data and significant performance degradation under domain shift.\nAlthough semi-supervised learning (SSL) reduces annotation requirements,\nexisting methods rely on generic augmentations that ignore polyp-specific\nstructural properties, resulting in poor generalization to new imaging centers\nand devices. To address this, we introduce Frequency Prior Guided Matching\n(FPGM), a novel augmentation framework built on a key discovery: polyp edges\nexhibit a remarkably consistent frequency signature across diverse datasets.\nFPGM leverages this intrinsic regularity in a two-stage process. It first\nlearns a domain-invariant frequency prior from the edge regions of labeled\npolyps. Then, it performs principled spectral perturbations on unlabeled\nimages, aligning their amplitude spectra with this learned prior while\npreserving phase information to maintain structural integrity. This targeted\nalignment normalizes domain-specific textural variations, thereby compelling\nthe model to learn the underlying, generalizable anatomical structure.\nValidated on six public datasets, FPGM establishes a new state-of-the-art\nagainst ten competing methods. It demonstrates exceptional zero-shot\ngeneralization capabilities, achieving over 10% absolute gain in Dice score in\ndata-scarce scenarios. By significantly enhancing cross-domain robustness, FPGM\npresents a powerful solution for clinically deployable polyp segmentation under\nlimited supervision.", "AI": {"tldr": "\u9488\u5bf9\u7ed3\u80a0\u606f\u8089\u5206\u5272\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u57df\u6f02\u79fb\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u9891\u7387\u5148\u9a8c\u5f15\u5bfc\u5339\u914d\uff08FPGM\uff09\u6846\u67b6\u3002FPGM\u5229\u7528\u606f\u8089\u8fb9\u7f18\u4e00\u81f4\u7684\u9891\u7387\u7279\u5f81\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8de8\u57df\u548c\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u591a\u9879\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u7ed3\u80a0\u764c\u7684\u65e9\u671f\u8bca\u65ad\u9700\u8981\u7cbe\u786e\u7684\u81ea\u52a8\u5316\u606f\u8089\u5206\u5272\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u9762\u4e34\u6807\u6ce8\u6570\u636e\u6709\u9650\u548c\u57df\u6f02\u79fb\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u6311\u6218\u3002\u534a\u76d1\u7763\u5b66\u4e60\u867d\u80fd\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\uff0c\u4f46\u5176\u901a\u7528\u589e\u5f3a\u65b9\u5f0f\u5ffd\u7565\u4e86\u606f\u8089\u7279\u6709\u7684\u7ed3\u6784\u5c5e\u6027\uff0c\u5bfc\u81f4\u5728\u65b0\u6210\u50cf\u4e2d\u5fc3\u548c\u8bbe\u5907\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u6838\u5fc3\u53d1\u73b0\u662f\u606f\u8089\u8fb9\u7f18\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e2d\u5177\u6709\u9ad8\u5ea6\u4e00\u81f4\u7684\u9891\u7387\u7279\u5f81\u3002FPGM\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u589e\u5f3a\u6846\u67b6\uff1a\u9996\u5148\uff0c\u4ece\u5df2\u6807\u6ce8\u606f\u8089\u7684\u8fb9\u7f18\u533a\u57df\u5b66\u4e60\u57df\u4e0d\u53d8\u7684\u9891\u7387\u5148\u9a8c\uff1b\u7136\u540e\uff0c\u5bf9\u672a\u6807\u6ce8\u56fe\u50cf\u6267\u884c\u9891\u8c31\u6270\u52a8\uff0c\u4f7f\u5176\u5e45\u5ea6\u8c31\u4e0e\u5b66\u4e60\u5230\u7684\u5148\u9a8c\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u7559\u76f8\u4f4d\u4fe1\u606f\u4ee5\u7ef4\u6301\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u4ece\u800c\u6807\u51c6\u5316\u57df\u7279\u6709\u7684\u7eb9\u7406\u53d8\u5316\uff0c\u4fc3\u4f7f\u6a21\u578b\u5b66\u4e60\u901a\u7528\u89e3\u5256\u7ed3\u6784\u3002", "result": "FPGM\u5728\u516d\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8d85\u8d8a\u4e86\u5341\u79cd\u73b0\u6709\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6280\u672f\u6c34\u5e73\u3002\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\uff0c\u5176\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0cDice\u5206\u6570\u7edd\u5bf9\u589e\u76ca\u8d85\u8fc710%\u3002", "conclusion": "FPGM\u663e\u8457\u589e\u5f3a\u4e86\u606f\u8089\u5206\u5272\u7684\u8de8\u57df\u9c81\u68d2\u6027\uff0c\u4e3a\u5728\u6709\u9650\u76d1\u7763\u4e0b\u4e34\u5e8a\u90e8\u7f72\u7684\u606f\u8089\u5206\u5272\u63d0\u4f9b\u4e86\u5f3a\u5927\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06600", "pdf": "https://arxiv.org/pdf/2508.06600", "abs": "https://arxiv.org/abs/2508.06600", "authors": ["Zijian Chen", "Xueguang Ma", "Shengyao Zhuang", "Ping Nie", "Kai Zou", "Andrew Liu", "Joshua Green", "Kshama Patel", "Ruoxi Meng", "Mingyi Su", "Sahel Sharifymoghaddam", "Yanxi Li", "Haoran Hong", "Xinyu Shi", "Xuye Liu", "Nandan Thakur", "Crystina Zhang", "Luyu Gao", "Wenhu Chen", "Jimmy Lin"], "title": "BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Deep-Research agents, which integrate large language models (LLMs) with\nsearch tools, have shown success in improving the effectiveness of handling\ncomplex queries that require iterative search planning and reasoning over\nsearch results. Evaluations on current benchmarks like BrowseComp relies on\nblack-box live web search APIs, have notable limitations in (1) fairness:\ndynamic and opaque web APIs hinder fair comparisons and reproducibility of deep\nresearch methods; (2) transparency: lack of control over the document corpus\nmakes it difficult to isolate retriever contributions. In other words, the\ncurrent evaluations may compare a complete deep research system at a given\ntime, but they do not foster well-controlled experiments to provide insights\ninto the capability of underlying deep research LLMs. To address these\nchallenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,\nemploying a fixed, carefully curated corpus. Each query in BrowseComp-Plus\nincludes human-verified supporting documents and mined challenging negatives,\nenabling controlled experimentation. The benchmark is shown to be effective in\ndistinguishing the performance of deep research systems. For instance, the\nopen-source model Search-R1, when paired with the BM25 retriever, achieves\n3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with\nthe Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with\nfewer search calls. This benchmark allows comprehensive evaluation and\ndisentangled analysis of deep research agents and retrieval methods, fostering\ninsights into retrieval effectiveness, citation accuracy, and context\nengineering in Deep-Research system.", "AI": {"tldr": "\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u516c\u5e73\u6027\u4e0e\u900f\u660e\u5ea6\u95ee\u9898\u3002\u672c\u6587\u63d0\u51faBrowseComp-Plus\uff0c\u4e00\u4e2a\u57fa\u4e8e\u56fa\u5b9a\u8bed\u6599\u5e93\u7684\u57fa\u51c6\uff0c\u80fd\u5b9e\u73b0\u53d7\u63a7\u5b9e\u9a8c\uff0c\u6709\u6548\u533a\u5206\u4e0d\u540c\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5e76\u652f\u6301\u5bf9\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u548c\u68c0\u7d22\u65b9\u6cd5\u8fdb\u884c\u89e3\u8026\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u5bf9\u96c6\u6210LLM\u4e0e\u641c\u7d22\u5de5\u5177\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u8bc4\u4f30\u57fa\u51c6\uff08\u5982BrowseComp\uff09\u4f9d\u8d56\u4e8e\u52a8\u6001\u3001\u4e0d\u900f\u660e\u7684\u5b9e\u65f6\u7f51\u7edc\u641c\u7d22API\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7f3a\u4e4f\u516c\u5e73\u6027\u548c\u53ef\u590d\u73b0\u6027\uff0c\u96be\u4ee5\u5206\u79bb\u68c0\u7d22\u5668\u7684\u8d21\u732e\uff0c\u65e0\u6cd5\u4e3a\u5e95\u5c42LLM\u7684\u80fd\u529b\u63d0\u4f9b\u53d7\u63a7\u5b9e\u9a8c\u7684\u6d1e\u5bdf\u3002", "method": "\u5f15\u5165BrowseComp-Plus\uff0c\u8fd9\u662f\u4e00\u4e2a\u4eceBrowseComp\u6d3e\u751f\u51fa\u7684\u65b0\u57fa\u51c6\uff0c\u91c7\u7528\u56fa\u5b9a\u4e14\u7cbe\u5fc3\u7b56\u5212\u7684\u8bed\u6599\u5e93\u3002\u6bcf\u4e2a\u67e5\u8be2\u90fd\u5305\u542b\u4eba\u5de5\u9a8c\u8bc1\u7684\u652f\u6301\u6587\u6863\u548c\u6316\u6398\u51fa\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u8d1f\u6837\u672c\uff0c\u4ece\u800c\u5b9e\u73b0\u53d7\u63a7\u5b9e\u9a8c\u3002", "result": "BrowseComp-Plus\u57fa\u51c6\u80fd\u6709\u6548\u533a\u5206\u4e0d\u540c\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u4f8b\u5982\uff0cSearch-R1\u6a21\u578b\uff08\u7ed3\u5408BM25\u68c0\u7d22\u5668\uff09\u51c6\u786e\u73873.86%\uff0c\u800cGPT-5\u51c6\u786e\u7387\u8fbe55.9%\u3002\u5c06GPT-5\u4e0eQwen3-Embedding-8B\u68c0\u7d22\u5668\u96c6\u6210\u540e\uff0c\u5176\u51c6\u786e\u7387\u8fdb\u4e00\u6b65\u63d0\u5347\u81f370.1%\uff0c\u540c\u65f6\u641c\u7d22\u8c03\u7528\u6b21\u6570\u66f4\u5c11\u3002", "conclusion": "BrowseComp-Plus\u57fa\u51c6\u80fd\u591f\u5bf9\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u548c\u68c0\u7d22\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u4e14\u89e3\u8026\u7684\u8bc4\u4f30\u5206\u6790\uff0c\u4ece\u800c\u6df1\u5165\u6d1e\u5bdf\u68c0\u7d22\u6709\u6548\u6027\u3001\u5f15\u7528\u51c6\u786e\u6027\u4ee5\u53ca\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u4e2d\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u3002"}}
{"id": "2508.07197", "pdf": "https://arxiv.org/pdf/2508.07197", "abs": "https://arxiv.org/abs/2508.07197", "authors": ["Ian Martiny", "Hammas Bin Tanveer", "Jack Wampler", "Rishab Nithyanand", "Eric Wustrow"], "title": "Mind the IP Gap: Measuring the impact of IPv6 on DNS censorship", "categories": ["cs.NI"], "comment": "19 pages, 6 tables", "summary": "Internet censorship impacts large segments of the Internet, but so far, prior\nwork has focused almost exclusively on performing measurements using IPv4. As\nthe Internet grows, and more users connect, IPv6 is increasingly supported and\navailable to users and servers alike. But despite this steady growth, it\nremains unclear if the information control systems that implement censorship\n(firewalls, deep packet inspection, DNS injection, etc) are as effective with\nIPv6 traffic as they are with IPv4. In this paper, we perform the first global\nmeasurement of DNS censorship on the IPv6 Internet. Leveraging a recent\ntechnique that allows us to discover IPv6-capable open resolvers (along with\ntheir corresponding IPv4 address), we send over 20 million A and AAAA DNS\nrequests to DNS resolvers worldwide, and measure the rate at which they block,\nat the resolver, network, and country level as well examine the characteristics\nof blocked domains. We observe that while nearly all censors support blocking\nIPv6, their policies are inconsistent with and frequently less effective than\ntheir IPv4 censorship infrastructure. Our results suggest that supporting IPv6\ncensorship is not all-or-nothing: many censors support it, but poorly. As a\nresult, these censors may have to expend additional resources to bring IPv6\ncensorship up to parity with IPv4. In the meantime, this affords censorship\ncircumvention researchers a new opportunity to exploit these differences to\nevade detection and blocking.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5168\u7403IPv6 DNS\u5ba1\u67e5\u666e\u904d\u5b58\u5728\uff0c\u4f46\u5176\u7b56\u7565\u4e0d\u5982IPv4\u5ba1\u67e5\u6709\u6548\u548c\u4e00\u81f4\uff0c\u4e3a\u89c4\u907f\u5ba1\u67e5\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "motivation": "\u4ee5\u5f80\u7684\u5ba1\u67e5\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728IPv4\u4e0a\uff0c\u4f46\u968f\u7740IPv6\u7684\u666e\u53ca\uff0c\u5176\u4fe1\u606f\u63a7\u5236\u7cfb\u7edf\u5bf9IPv6\u6d41\u91cf\u7684\u6709\u6548\u6027\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u5bf9IPv6\u4e92\u8054\u7f51\u4e0a\u7684DNS\u5ba1\u67e5\u8fdb\u884c\u9996\u6b21\u5168\u7403\u6027\u6d4b\u91cf\u3002", "method": "\u5229\u7528\u4e00\u79cd\u65b0\u6280\u672f\u53d1\u73b0\u652f\u6301IPv6\u7684\u5f00\u653e\u89e3\u6790\u5668\uff08\u53ca\u5176\u5bf9\u5e94IPv4\u5730\u5740\uff09\uff0c\u5411\u5168\u7403DNS\u89e3\u6790\u5668\u53d1\u9001\u8d85\u8fc72000\u4e07\u6b21A\u548cAAAA DNS\u8bf7\u6c42\uff0c\u6d4b\u91cf\u89e3\u6790\u5668\u3001\u7f51\u7edc\u548c\u56fd\u5bb6\u5c42\u9762\u7684\u5c4f\u853d\u7387\uff0c\u5e76\u5206\u6790\u88ab\u5c4f\u853d\u57df\u540d\u7684\u7279\u6027\u3002", "result": "\u89c2\u5bdf\u5230\u51e0\u4e4e\u6240\u6709\u5ba1\u67e5\u8005\u90fd\u652f\u6301IPv6\u5c4f\u853d\uff0c\u4f46\u5176\u7b56\u7565\u4e0eIPv4\u4e0d\u4e00\u81f4\uff0c\u4e14\u6548\u679c\u666e\u904d\u4e0d\u5982\u5176IPv4\u5ba1\u67e5\u57fa\u7840\u8bbe\u65bd\u3002\u8bb8\u591a\u5ba1\u67e5\u8005\u5bf9IPv6\u5ba1\u67e5\u7684\u652f\u6301\u4e0d\u5230\u4f4d\u3002", "conclusion": "\u5ba1\u67e5\u8005\u53ef\u80fd\u9700\u8981\u6295\u5165\u989d\u5916\u8d44\u6e90\u4ee5\u4f7fIPv6\u5ba1\u67e5\u8fbe\u5230\u4e0eIPv4\u76f8\u540c\u7684\u6c34\u5e73\u3002\u540c\u65f6\uff0c\u8fd9\u4e3a\u5ba1\u67e5\u89c4\u907f\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5229\u7528\u8fd9\u4e9b\u5dee\u5f02\u6765\u89c4\u907f\u68c0\u6d4b\u548c\u5c4f\u853d\u7684\u65b0\u673a\u4f1a\u3002"}}
{"id": "2508.06588", "pdf": "https://arxiv.org/pdf/2508.06588", "abs": "https://arxiv.org/abs/2508.06588", "authors": ["Zian Zhai", "Fan Li", "Xingyu Tan", "Xiaoyang Wang", "Wenjie Zhang"], "title": "Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Vector Quantization (VQ) has recently emerged as a promising approach for\nlearning discrete representations of graph-structured data. However, a\nfundamental challenge, i.e., codebook collapse, remains underexplored in the\ngraph domain, significantly limiting the expressiveness and generalization of\ngraph tokens.In this paper, we present the first empirical study showing that\ncodebook collapse consistently occurs when applying VQ to graph data, even with\nmitigation strategies proposed in vision or language domains. To understand why\ngraph VQ is particularly vulnerable to collapse, we provide a theoretical\nanalysis and identify two key factors: early assignment imbalances caused by\nredundancy in graph features and structural patterns, and self-reinforcing\noptimization loops in deterministic VQ. To address these issues, we propose\nRGVQ, a novel framework that integrates graph topology and feature similarity\nas explicit regularization signals to enhance codebook utilization and promote\ntoken diversity. RGVQ introduces soft assignments via Gumbel-Softmax\nreparameterization, ensuring that all codewords receive gradient updates. In\naddition, RGVQ incorporates a structure-aware contrastive regularization to\npenalize the token co-assignments among similar node pairs. Extensive\nexperiments demonstrate that RGVQ substantially improves codebook utilization\nand consistently boosts the performance of state-of-the-art graph VQ backbones\nacross multiple downstream tasks, enabling more expressive and transferable\ngraph token representations.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u63ed\u793a\u5e76\u5206\u6790\u4e86\u56fe\u6570\u636e\u4e0a\u5411\u91cf\u91cf\u5316\uff08VQ\uff09\u4e2d\u7684\u7801\u672c\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86RGVQ\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u56fe\u62d3\u6251\u548c\u7279\u5f81\u76f8\u4f3c\u6027\uff0c\u7ed3\u5408Gumbel-Softmax\u548c\u7ed3\u6784\u611f\u77e5\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7801\u672c\u5229\u7528\u7387\u548c\u56fe\u4ee4\u724c\u8868\u793a\u6027\u80fd\u3002", "motivation": "\u5411\u91cf\u91cf\u5316\uff08VQ\uff09\u5728\u56fe\u6570\u636e\u79bb\u6563\u8868\u793a\u4e2d\u5f88\u6709\u524d\u666f\uff0c\u4f46\u56fe\u9886\u57df\u4e2d\u7684\u201c\u7801\u672c\u5d29\u6e83\u201d\u95ee\u9898\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u56fe\u4ee4\u724c\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6cdb\u5316\u6027\u3002\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u5728\u56fe\u6570\u636e\u4e0a\u4ecd\u65e0\u6548\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u5176\u539f\u56e0\u5e76\u63d0\u51fa\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u9996\u5148\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc1\u660e\u7801\u672c\u5d29\u6e83\u5728\u56feVQ\u4e2d\u666e\u904d\u5b58\u5728\u3002\u63a5\u7740\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u65e9\u671f\u5206\u914d\u4e0d\u5e73\u8861\uff08\u6e90\u4e8e\u56fe\u7279\u5f81\u548c\u7ed3\u6784\u6a21\u5f0f\u7684\u5197\u4f59\uff09\u548c\u786e\u5b9a\u6027VQ\u4e2d\u81ea\u6211\u5f3a\u5316\u4f18\u5316\u5faa\u73af\u662f\u5bfc\u81f4\u5d29\u6e83\u7684\u5173\u952e\u56e0\u7d20\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86RGVQ\u6846\u67b6\uff0c\u8be5\u6846\u67b6\uff1a1. \u901a\u8fc7Gumbel-Softmax\u91cd\u53c2\u6570\u5316\u5f15\u5165\u8f6f\u5206\u914d\uff0c\u786e\u4fdd\u6240\u6709\u7801\u5b57\u63a5\u6536\u68af\u5ea6\u66f4\u65b0\uff0c\u589e\u5f3a\u7801\u672c\u5229\u7528\u7387\u30022. \u5f15\u5165\u7ed3\u6784\u611f\u77e5\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u60e9\u7f5a\u76f8\u4f3c\u8282\u70b9\u5bf9\u4e4b\u95f4\u7684\u4ee4\u724c\u5171\u5206\u914d\uff0c\u4fc3\u8fdb\u4ee4\u724c\u591a\u6837\u6027\u3002", "result": "RGVQ\u663e\u8457\u63d0\u9ad8\u4e86\u7801\u672c\u5229\u7528\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6301\u7eed\u63d0\u5347\u4e86\u6700\u5148\u8fdb\u56feVQ\u9aa8\u5e72\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u5177\u8868\u8fbe\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u7684\u56fe\u4ee4\u724c\u8868\u793a\u3002", "conclusion": "RGVQ\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u6570\u636e\u4e2dVQ\u7684\u7801\u672c\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u6574\u5408\u56fe\u62d3\u6251\u548c\u7279\u5f81\u76f8\u4f3c\u6027\u4f5c\u4e3a\u663e\u5f0f\u6b63\u5219\u5316\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u8f6f\u5206\u914d\u548c\u7ed3\u6784\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u56fe\u4ee4\u724c\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6cdb\u5316\u6027\uff0c\u4e3a\u56fe\u6570\u636e\u7684\u79bb\u6563\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2508.06668", "pdf": "https://arxiv.org/pdf/2508.06668", "abs": "https://arxiv.org/abs/2508.06668", "authors": ["Jessie Galasso"], "title": "Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis", "categories": ["cs.AI", "cs.IR", "cs.SE"], "comment": null, "summary": "Formal Concept Analysis (FCA) is a mathematical framework for knowledge\nrepresentation and discovery. It performs a hierarchical clustering over a set\nof objects described by attributes, resulting in conceptual structures in which\nobjects are organized depending on the attributes they share. These conceptual\nstructures naturally highlight commonalities and variabilities among similar\nobjects by categorizing them into groups which are then arranged by similarity,\nmaking it particularly appropriate for variability extraction and analysis.\nDespite the potential of FCA, determining which of its properties can be\nleveraged for variability-related tasks (and how) is not always\nstraightforward, partly due to the mathematical orientation of its foundational\nliterature. This paper attempts to bridge part of this gap by gathering a\nselection of properties of the framework which are essential to variability\nanalysis, and how they can be used to interpret diverse variability information\nwithin the resulting conceptual structures.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u9610\u660e\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff08FCA\uff09\u5bf9\u53d8\u5f02\u6027\u5206\u6790\u7684\u5173\u952e\u7279\u6027\u53ca\u5176\u5e94\u7528\uff0c\u4ee5\u5f25\u8865\u5176\u6570\u5b66\u57fa\u7840\u4e0e\u5b9e\u9645\u5e94\u7528\u95f4\u7684\u9e3f\u6c9f\u3002", "motivation": "\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff08FCA\uff09\u5728\u77e5\u8bc6\u8868\u793a\u548c\u53d8\u5f02\u6027\u5206\u6790\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7531\u4e8e\u5176\u6df1\u5965\u7684\u6570\u5b66\u6027\u8d28\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528\u5176\u7279\u6027\u8fdb\u884c\u53d8\u5f02\u6027\u76f8\u5173\u4efb\u52a1\u5e76\u4e0d\u76f4\u89c2\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6536\u96c6\u5bf9\u53d8\u5f02\u6027\u5206\u6790\u81f3\u5173\u91cd\u8981\u7684FCA\u7279\u6027\uff0c\u5e76\u89e3\u91ca\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u7279\u6027\u6765\u89e3\u8bfb\u6240\u751f\u6210\u6982\u5ff5\u7ed3\u6784\u4e2d\u7684\u591a\u6837\u5316\u53d8\u5f02\u6027\u4fe1\u606f\u3002", "result": "\u62bd\u8c61\u4e2d\u672a\u63d0\u4f9b\u5177\u4f53\u7684\u7814\u7a76\u7ed3\u679c\uff0c\u4f46\u8bba\u6587\u7684\u6210\u679c\u5728\u4e8e\u6c47\u96c6\u5e76\u9610\u8ff0\u4e86FCA\u4e2d\u5bf9\u53d8\u5f02\u6027\u5206\u6790\u81f3\u5173\u91cd\u8981\u7684\u5c5e\u6027\u53ca\u5176\u89e3\u8bfb\u53d8\u5f02\u6027\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5730\u9610\u8ff0FCA\u7684\u7279\u5b9a\u5c5e\u6027\u53ca\u5176\u5728\u53d8\u5f02\u6027\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u8bba\u6587\u4e3a\u7406\u89e3\u548c\u5229\u7528FCA\u8fdb\u884c\u53d8\u5f02\u6027\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u6307\u5bfc\uff0c\u4ece\u800c\u5f25\u8865\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.06525", "pdf": "https://arxiv.org/pdf/2508.06525", "abs": "https://arxiv.org/abs/2508.06525", "authors": ["Guoyuan An", "JaeYoon Kim", "SungEui Yoon"], "title": "Large Language Models Facilitate Vision Reflection in Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents several novel findings on the explainability of vision\nreflection in large multimodal models (LMMs). First, we show that prompting an\nLMM to verify the prediction of a specialized vision model can improve\nrecognition accuracy, even on benchmarks like ImageNet, despite prior evidence\nthat LMMs typically underperform dedicated vision encoders. Second, we analyze\nthe internal behavior of vision reflection and find that the vision-language\nconnector maps visual features into explicit textual concepts, allowing the\nlanguage model to reason about prediction plausibility using commonsense\nknowledge. We further observe that replacing a large number of vision tokens\nwith only a few text tokens still enables LLaVA to generate similar answers,\nsuggesting that LMMs may rely primarily on a compact set of distilled textual\nrepresentations rather than raw vision features. Third, we show that a\ntraining-free connector can enhance LMM performance in fine-grained recognition\ntasks, without extensive feature-alignment training. Together, these findings\noffer new insights into the explainability of vision-language models and\nsuggest that vision reflection is a promising strategy for achieving robust and\ninterpretable visual recognition.", "AI": {"tldr": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u901a\u8fc7\u201c\u89c6\u89c9\u53cd\u601d\u201d\uff08\u9a8c\u8bc1\u89c6\u89c9\u6a21\u578b\u9884\u6d4b\uff09\u53ef\u63d0\u9ad8\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5373\u4f7f\u5728ImageNet\u4e0a\u3002\u5176\u5185\u90e8\u673a\u5236\u662f\u5c06\u89c6\u89c9\u7279\u5f81\u8f6c\u5316\u4e3a\u7d27\u51d1\u6587\u672c\u6982\u5ff5\u4f9b\u5e38\u8bc6\u63a8\u7406\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u8fde\u63a5\u5668\u4e5f\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u8868\u660e\u89c6\u89c9\u53cd\u601d\u662f\u5b9e\u73b0\u7a33\u5065\u53ef\u89e3\u91ca\u89c6\u89c9\u8bc6\u522b\u7684\u6709\u6548\u7b56\u7565\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u4e2d\u201c\u89c6\u89c9\u53cd\u601d\u201d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5229\u7528LMMs\u7684\u80fd\u529b\u63d0\u5347\u89c6\u89c9\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u514b\u670d\u5176\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u901a\u5e38\u4e0d\u5982\u4e13\u7528\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "method": "1. \u63d0\u793aLMMs\u9a8c\u8bc1\u4e13\u7528\u89c6\u89c9\u6a21\u578b\u7684\u9884\u6d4b\u30022. \u5206\u6790\u89c6\u89c9-\u8bed\u8a00\u8fde\u63a5\u5668\u5c06\u89c6\u89c9\u7279\u5f81\u6620\u5c04\u4e3a\u6587\u672c\u6982\u5ff5\u7684\u5185\u90e8\u884c\u4e3a\uff0c\u5e76\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5229\u7528\u5e38\u8bc6\u8fdb\u884c\u63a8\u7406\u30023. \u5b9e\u9a8c\u7528\u5c11\u91cf\u6587\u672ctoken\u66ff\u6362\u5927\u91cf\u89c6\u89c9token\u5bf9LMMs\uff08\u5982LLaVA\uff09\u884c\u4e3a\u7684\u5f71\u54cd\u30024. \u5f15\u5165\u5e76\u6d4b\u8bd5\u65e0\u9700\u8bad\u7ec3\u7684\u8fde\u63a5\u5668\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6548\u80fd\u3002", "result": "1. LMMs\u901a\u8fc7\u9a8c\u8bc1\u4e13\u95e8\u89c6\u89c9\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u80fd\u63d0\u9ad8\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5305\u62ec\u5728ImageNet\u7b49\u57fa\u51c6\u4e0a\u30022. \u89c6\u89c9-\u8bed\u8a00\u8fde\u63a5\u5668\u80fd\u5c06\u89c6\u89c9\u7279\u5f81\u8f6c\u5316\u4e3a\u663e\u5f0f\u6587\u672c\u6982\u5ff5\uff0c\u4f7f\u8bed\u8a00\u6a21\u578b\u80fd\u901a\u8fc7\u5e38\u8bc6\u63a8\u7406\u5224\u65ad\u9884\u6d4b\u7684\u5408\u7406\u6027\u30023. LMMs\u53ef\u80fd\u4e3b\u8981\u4f9d\u8d56\u5c11\u91cf\u7cbe\u70bc\u7684\u6587\u672c\u8868\u793a\uff0c\u800c\u975e\u539f\u59cb\u89c6\u89c9\u7279\u5f81\u30024. \u65e0\u9700\u8bad\u7ec3\u7684\u8fde\u63a5\u5668\u53ef\u5728\u4e0d\u8fdb\u884c\u5927\u91cf\u7279\u5f81\u5bf9\u9f50\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u5347LMM\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63d0\u4f9b\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u65b0\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u201c\u89c6\u89c9\u53cd\u601d\u201d\u662f\u5b9e\u73b0\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u89c6\u89c9\u8bc6\u522b\u7684\u4e00\u79cd\u6709\u524d\u666f\u7684\u7b56\u7565\u3002"}}
{"id": "2508.06621", "pdf": "https://arxiv.org/pdf/2508.06621", "abs": "https://arxiv.org/abs/2508.06621", "authors": ["Tomohiro Sawada", "Kartik Goyal"], "title": "Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models", "categories": ["cs.CL"], "comment": "Submitted to EMNLP", "summary": "Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a\nlearned token vocabulary with a detailed merge list. Recent work has shown that\nthis merge list exposes a potential attack surface for extracting information\nabout language model's training data. In this paper, we explore the downstream\nimpact of BPE inference algorithms that do not rely on this merge list at all,\nand hence differ from the encoding process during BPE training. To address this\nquestion, we investigate two broad classes of BPE inference schemes that differ\nfrom BPE application during training: a) targeted deviation from merge-lists\nincluding random merge orders, and various corruptions of merge list involving\ndeletion/truncation, and b) non-targeted BPE inference algorithms that do not\ndepend on the merge list but focus on compressing the text either greedily or\nexactly. Extensive experiments across diverse language modeling tasks like\naccuracy-based QA benchmarks, machine translation, and open-ended generation\nreveal that while targeted deviation from the merge lists exhibits significant\ndegradation in language model performance, the non-targeted merge-list-free\ninference algorithms result in minimal impact on downstream performance that is\noften much smaller than expected. These findings pave way for simpler and\npotentially more privacy-preserving tokenization schemes that do not\ncatastrophically compromise model performance.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4e0d\u4f9d\u8d56\u5408\u5e76\u5217\u8868\u7684\u60c5\u51b5\u4e0b\u8fdb\u884cBPE\u63a8\u7406\uff08\u5c24\u5176\u662f\u975e\u5b9a\u5411\u538b\u7f29\uff09\u5bf9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u5fae\u4e4e\u5176\u5fae\uff0c\u4e3a\u66f4\u7b80\u5355\u3001\u66f4\u6ce8\u91cd\u9690\u79c1\u7684\u6807\u8bb0\u5316\u65b9\u6848\u94fa\u5e73\u9053\u8def\u3002", "motivation": "\u6807\u51c6BPE\u6807\u8bb0\u5316\u65b9\u6848\u7684\u5408\u5e76\u5217\u8868\u53ef\u80fd\u66b4\u9732\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6570\u636e\uff0c\u6784\u6210\u6f5c\u5728\u7684\u4fe1\u606f\u63d0\u53d6\u653b\u51fb\u9762\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u4e0d\u4f9d\u8d56\u5408\u5e76\u5217\u8868\u7684BPE\u63a8\u7406\u7b97\u6cd5\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u63a2\u7d22\u66f4\u5b89\u5168\u7684\u6807\u8bb0\u5316\u65b9\u6848\u3002", "method": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e24\u7c7b\u4e0d\u540c\u4e8eBPE\u8bad\u7ec3\u7f16\u7801\u8fc7\u7a0b\u7684BPE\u63a8\u7406\u65b9\u6848\uff1aa) \u6709\u9488\u5bf9\u6027\u5730\u504f\u79bb\u5408\u5e76\u5217\u8868\uff08\u5982\u968f\u673a\u5408\u5e76\u987a\u5e8f\u3001\u5220\u9664/\u622a\u65ad\u5408\u5e76\u5217\u8868\uff09\uff1bb) \u4e0d\u4f9d\u8d56\u5408\u5e76\u5217\u8868\u4f46\u4e13\u6ce8\u4e8e\u6587\u672c\u538b\u7f29\uff08\u8d2a\u5a6a\u6216\u7cbe\u786e\uff09\u7684\u975e\u5b9a\u5411BPE\u63a8\u7406\u7b97\u6cd5\u3002\u901a\u8fc7\u5728\u95ee\u7b54\u3001\u673a\u5668\u7ffb\u8bd1\u548c\u5f00\u653e\u5f0f\u751f\u6210\u7b49\u591a\u79cd\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6848\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6709\u9488\u5bf9\u6027\u5730\u504f\u79bb\u5408\u5e76\u5217\u8868\u4f1a\u5bfc\u81f4\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u7136\u800c\uff0c\u4e0d\u4f9d\u8d56\u5408\u5e76\u5217\u8868\u7684\u975e\u5b9a\u5411\u63a8\u7406\u7b97\u6cd5\u5bf9\u4e0b\u6e38\u6027\u80fd\u7684\u5f71\u54cd\u6781\u5c0f\uff0c\u4e14\u901a\u5e38\u8fdc\u4f4e\u4e8e\u9884\u671f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f00\u53d1\u66f4\u7b80\u5355\u3001\u53ef\u80fd\u66f4\u6ce8\u91cd\u9690\u79c1\u7684\u6807\u8bb0\u5316\u65b9\u6848\u662f\u53ef\u884c\u7684\uff0c\u8fd9\u4e9b\u65b9\u6848\u5728\u4e0d\u4e25\u91cd\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u80fd\u591f\u89c4\u907f\u5408\u5e76\u5217\u8868\u5e26\u6765\u7684\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2508.07394", "pdf": "https://arxiv.org/pdf/2508.07394", "abs": "https://arxiv.org/abs/2508.07394", "authors": ["Luca Lusvarghi", "Javier Gozalvez", "Baldomero Coll-Perales", "Mohammad Irfan Khan", "Miguel Sepulcre", "Seyhan Ucar", "Onur Altintas"], "title": "The Search for Relevance: A Context-Aware Paradigm Shift in Semantic and Task-Oriented V2X Communications", "categories": ["cs.NI"], "comment": null, "summary": "The design of communication systems has traditionally focused on the reliable\nand timely delivery of data. However, the scalability challenges faced by the\nevolution to a 6G-driven society demand new communication paradigms that\ncarefully curate the content being transmitted. This paper envisions a joint\nsemantic and task-oriented communication paradigm where Connected and\nAutonomous Vehicles (CAVs) transmit only the information necessary to convey\nthe desired meaning that is relevant to the intended receivers based on the\ncommunication context. The V2X domain offers a unique environment for the\ndevelopment of the envisioned semantic and task-oriented communications\nparadigm, as CAVs are native semantic devices, and the V2X domain is rich in\ncontextual information. This contextual information can be leveraged to\nestimate the relevance that information may have for the intended receivers. We\nillustrate and quantitatively evaluate the potential benefits of semantic and\ntask-oriented V2X communications. Numerical results show that by focusing on\nthe transmission of the most relevant information for the intended receivers,\nsemantic and task-oriented V2X communications can achieve a two-fold\nimprovement in communication efficiency, which will significantly benefit the\nscalability of V2X networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u4e0e\u4efb\u52a1\u5bfc\u5411\u7684V2X\u901a\u4fe1\u8303\u5f0f\uff0c\u901a\u8fc7\u4ec5\u4f20\u8f93\u76f8\u5173\u4fe1\u606f\uff0c\u5c06\u901a\u4fe1\u6548\u7387\u63d0\u5347\u4e00\u500d\uff0c\u4ee5\u5e94\u5bf96G\u65f6\u4ee3\u7684\u6269\u5c55\u6027\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u901a\u4fe1\u7cfb\u7edf\u4fa7\u91cd\u6570\u636e\u53ef\u9760\u53ca\u65f6\u4f20\u8f93\uff0c\u4f466G\u9a71\u52a8\u7684\u793e\u4f1a\u9762\u4e34\u6269\u5c55\u6027\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u901a\u4fe1\u8303\u5f0f\uff0c\u7b5b\u9009\u4f20\u8f93\u5185\u5bb9\u3002\u8f66\u8054\u7f51\uff08V2X\uff09\u73af\u5883\u5929\u7136\u9002\u5408\u5f00\u53d1\u8fd9\u79cd\u901a\u4fe1\u8303\u5f0f\uff0c\u56e0\u4e3aCAVs\u662f\u539f\u751f\u8bed\u4e49\u8bbe\u5907\u4e14\u60c5\u5883\u4fe1\u606f\u4e30\u5bcc\u3002", "method": "\u672c\u6587\u8bbe\u60f3\u4e86\u4e00\u79cd\u8054\u5408\u8bed\u4e49\u4e0e\u4efb\u52a1\u5bfc\u5411\u7684\u901a\u4fe1\u8303\u5f0f\uff0c\u5176\u4e2d\u4e92\u8054\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08CAVs\uff09\u4ec5\u4f20\u8f93\u5bf9\u63a5\u6536\u65b9\u6709\u610f\u4e49\u4e14\u57fa\u4e8e\u901a\u4fe1\u4e0a\u4e0b\u6587\u5fc5\u8981\u7684\u4fe1\u606f\u3002\u5229\u7528V2X\u9886\u57df\u7684\u60c5\u5883\u4fe1\u606f\u6765\u4f30\u8ba1\u4fe1\u606f\u5bf9\u63a5\u6536\u65b9\u7684\u76f8\u5173\u6027\u3002\u901a\u8fc7\u6570\u503c\u7ed3\u679c\u5b9a\u91cf\u8bc4\u4f30\u5176\u6f5c\u5728\u76ca\u5904\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u4e13\u6ce8\u4e8e\u4f20\u8f93\u5bf9\u76ee\u6807\u63a5\u6536\u65b9\u6700\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u8bed\u4e49\u4e0e\u4efb\u52a1\u5bfc\u5411\u7684V2X\u901a\u4fe1\u53ef\u4ee5\u5b9e\u73b0\u901a\u4fe1\u6548\u7387\u7684\u4e24\u500d\u63d0\u5347\u3002", "conclusion": "\u8bed\u4e49\u4e0e\u4efb\u52a1\u5bfc\u5411\u7684V2X\u901a\u4fe1\u80fd\u663e\u8457\u63d0\u9ad8V2X\u7f51\u7edc\u7684\u6269\u5c55\u6027\uff0c\u4e3a\u672a\u67656G\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316\u4fe1\u606f\u4f20\u8f93\u6765\u5e94\u5bf9\u9ad8\u5e76\u53d1\u6311\u6218\u3002"}}
{"id": "2508.06589", "pdf": "https://arxiv.org/pdf/2508.06589", "abs": "https://arxiv.org/abs/2508.06589", "authors": ["Xinglin Zhao", "Yanwen Wang", "Xiaobo Liu", "Yanrong Hao", "Rui Cao", "Xin Wen"], "title": "A Federated Learning Framework for Handling Subtype Confounding and Heterogeneity in Large-Scale Neuroimaging Diagnosis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Computer-aided diagnosis (CAD) systems play a crucial role in analyzing\nneuroimaging data for neurological and psychiatric disorders. However,\nsmall-sample studies suffer from low reproducibility, while large-scale\ndatasets introduce confounding heterogeneity due to multiple disease subtypes\nbeing labeled under a single category. To address these challenges, we propose\na novel federated learning framework tailored for neuroimaging CAD systems. Our\napproach includes a dynamic navigation module that routes samples to the most\nsuitable local models based on latent subtype representations, and a\nmeta-integration module that combines predictions from heterogeneous local\nmodels into a unified diagnostic output. We evaluated our framework using a\ncomprehensive dataset comprising fMRI data from over 1300 MDD patients and 1100\nhealthy controls across multiple study cohorts. Experimental results\ndemonstrate significant improvements in diagnostic accuracy and robustness\ncompared to traditional methods. Specifically, our framework achieved an\naverage accuracy of 74.06\\% across all tested sites, showcasing its\neffectiveness in handling subtype heterogeneity and enhancing model\ngeneralizability. Ablation studies further confirmed the importance of both the\ndynamic navigation and meta-integration modules in improving performance. By\naddressing data heterogeneity and subtype confounding, our framework advances\nreliable and reproducible neuroimaging CAD systems, offering significant\npotential for personalized medicine and clinical decision-making in neurology\nand psychiatry.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u795e\u7ecf\u5f71\u50cf\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\uff08CAD\uff09\u7cfb\u7edf\u4e2d\u6570\u636e\u5f02\u8d28\u6027\u548c\u4e9a\u578b\u6df7\u6dc6\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5bfc\u822a\u548c\u5143\u96c6\u6210\u6a21\u5757\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u5f71\u50cfCAD\u7cfb\u7edf\u9762\u4e34\u6311\u6218\uff1a\u5c0f\u6837\u672c\u7814\u7a76\u590d\u73b0\u6027\u4f4e\uff0c\u800c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u56e0\u591a\u91cd\u75be\u75c5\u4e9a\u578b\u88ab\u5355\u4e00\u6807\u7b7e\u6807\u6ce8\u800c\u5f15\u5165\u6df7\u6dc6\u5f02\u8d28\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u6709\u6548\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\u548c\u4e9a\u578b\u6df7\u6dc6\uff0c\u4ee5\u63d0\u5347\u8bca\u65ad\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u590d\u73b0\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u4e3a\u795e\u7ecf\u5f71\u50cfCAD\u7cfb\u7edf\u8bbe\u8ba1\u3002\u8be5\u6846\u67b6\u5305\u542b\uff1a1) \u52a8\u6001\u5bfc\u822a\u6a21\u5757\uff0c\u6839\u636e\u6f5c\u5728\u4e9a\u578b\u8868\u793a\u5c06\u6837\u672c\u8def\u7531\u81f3\u6700\u9002\u5408\u7684\u672c\u5730\u6a21\u578b\uff1b2) \u5143\u96c6\u6210\u6a21\u5757\uff0c\u5c06\u5f02\u6784\u672c\u5730\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u6574\u5408\u6210\u7edf\u4e00\u7684\u8bca\u65ad\u8f93\u51fa\u3002", "result": "\u5728\u5305\u542b\u8d85\u8fc71300\u540d\u6291\u90c1\u75c7\u60a3\u8005\u548c1100\u540d\u5065\u5eb7\u5bf9\u7167\u7684fMRI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u8bca\u65ad\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523074.06%\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u52a8\u6001\u5bfc\u822a\u548c\u5143\u96c6\u6210\u6a21\u5757\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6709\u6548\u89e3\u51b3\u6570\u636e\u5f02\u8d28\u6027\u548c\u4e9a\u578b\u6df7\u6dc6\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u795e\u7ecf\u5f71\u50cfCAD\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u53ef\u590d\u73b0\u6027\uff0c\u5728\u4e2a\u6027\u5316\u533b\u7597\u548c\u795e\u7ecf\u7cbe\u795e\u79d1\u4e34\u5e8a\u51b3\u7b56\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.06674", "pdf": "https://arxiv.org/pdf/2508.06674", "abs": "https://arxiv.org/abs/2508.06674", "authors": ["Weijie Shi", "Yue Cui", "Hao Chen", "Jiaming Li", "Mengze Li", "Jia Zhu", "Jiajie Xu", "Xiaofang Zhou"], "title": "Zero-Shot Cellular Trajectory Map Matching", "categories": ["cs.AI"], "comment": null, "summary": "Cellular Trajectory Map-Matching (CTMM) aims to align cellular location\nsequences to road networks, which is a necessary preprocessing in\nlocation-based services on web platforms like Google Maps, including navigation\nand route optimization. Current approaches mainly rely on ID-based features and\nregion-specific data to learn correlations between cell towers and roads,\nlimiting their adaptability to unexplored areas. To enable high-accuracy CTMM\nwithout additional training in target regions, Zero-shot CTMM requires to\nextract not only region-adaptive features, but also sequential and location\nuncertainty to alleviate positioning errors in cellular data. In this paper, we\npropose a pixel-based trajectory calibration assistant for zero-shot CTMM,\nwhich takes advantage of transferable geospatial knowledge to calibrate\npixelated trajectory, and then guide the path-finding process at the road\nnetwork level. To enhance knowledge sharing across similar regions, a Gaussian\nmixture model is incorporated into VAE, enabling the identification of\nscenario-adaptive experts through soft clustering. To mitigate high positioning\nerrors, a spatial-temporal awareness module is designed to capture sequential\nfeatures and location uncertainty, thereby facilitating the inference of\napproximate user positions. Finally, a constrained path-finding algorithm is\nemployed to reconstruct the road ID sequence, ensuring topological validity\nwithin the road network. This process is guided by the calibrated trajectory\nwhile optimizing for the shortest feasible path, thus minimizing unnecessary\ndetours. Extensive experiments demonstrate that our model outperforms existing\nmethods in zero-shot CTMM by 16.8\\%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u8702\u7a9d\u8f68\u8ff9\u5730\u56fe\u5339\u914d\uff08CTMM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u8f68\u8ff9\u6821\u51c6\u3001\u7ed3\u5408\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u8fdb\u884c\u77e5\u8bc6\u5171\u4eab\u3001\u65f6\u7a7a\u611f\u77e5\u6a21\u5757\u5904\u7406\u5b9a\u4f4d\u8bef\u5dee\u4ee5\u53ca\u7ea6\u675f\u8def\u5f84\u67e5\u627e\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u672a\u63a2\u7d22\u533a\u57df\u7684\u5339\u914d\u7cbe\u5ea6\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u8702\u7a9d\u8f68\u8ff9\u5730\u56fe\u5339\u914d\uff08CTMM\uff09\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u57fa\u4e8eID\u548c\u533a\u57df\u7279\u5b9a\u7684\u6570\u636e\uff0c\u5bfc\u81f4\u5176\u5728\u672a\u63a2\u7d22\u533a\u57df\u9002\u5e94\u6027\u5dee\u3002\u4e3a\u4e86\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u96f6\u6837\u672cCTMM\uff0c\u9700\u8981\u63d0\u53d6\u533a\u57df\u81ea\u9002\u5e94\u7279\u5f81\uff0c\u5e76\u8003\u8651\u5e8f\u5217\u4fe1\u606f\u548c\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u6027\u4ee5\u7f13\u89e3\u8702\u7a9d\u6570\u636e\u4e2d\u7684\u5b9a\u4f4d\u8bef\u5dee\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u50cf\u7d20\u7ea7\u8f68\u8ff9\u6821\u51c6\u52a9\u624b\uff0c\u5229\u7528\u53ef\u8fc1\u79fb\u7684\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u6821\u51c6\u50cf\u7d20\u5316\u8f68\u8ff9\uff0c\u5e76\u6307\u5bfc\u9053\u8def\u7f51\u7edc\u5c42\u9762\u7684\u8def\u5f84\u67e5\u627e\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5c06\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u6574\u5408\u5230VAE\u4e2d\uff0c\u901a\u8fc7\u8f6f\u805a\u7c7b\u8bc6\u522b\u573a\u666f\u81ea\u9002\u5e94\u4e13\u5bb6\uff0c\u4ee5\u589e\u5f3a\u7c7b\u4f3c\u533a\u57df\u7684\u77e5\u8bc6\u5171\u4eab\uff1b2) \u8bbe\u8ba1\u4e00\u4e2a\u65f6\u7a7a\u611f\u77e5\u6a21\u5757\u4ee5\u6355\u6349\u5e8f\u5217\u7279\u5f81\u548c\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u63a8\u65ad\u8fd1\u4f3c\u7528\u6237\u4f4d\u7f6e\uff0c\u7f13\u89e3\u9ad8\u5b9a\u4f4d\u8bef\u5dee\uff1b3) \u91c7\u7528\u7ea6\u675f\u8def\u5f84\u67e5\u627e\u7b97\u6cd5\u91cd\u5efa\u9053\u8defID\u5e8f\u5217\uff0c\u786e\u4fdd\u9053\u8def\u7f51\u7edc\u7684\u62d3\u6251\u6709\u6548\u6027\uff0c\u5e76\u4f18\u5316\u6700\u77ed\u53ef\u884c\u8def\u5f84\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u96f6\u6837\u672cCTMM\u65b9\u9762\u7684\u6027\u80fd\u63d0\u9ad8\u4e8616.8%\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u89e3\u51b3\u4e86\u96f6\u6837\u672cCTMM\u7684\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u50cf\u7d20\u7ea7\u6821\u51c6\u548c\u591a\u6a21\u5757\u96c6\u6210\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u672a\u63a2\u7d22\u533a\u57df\u7684\u9ad8\u7cbe\u5ea6\u8f68\u8ff9\u5339\u914d\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.06528", "pdf": "https://arxiv.org/pdf/2508.06528", "abs": "https://arxiv.org/abs/2508.06528", "authors": ["Xiuliang Zhang", "Tadiwa Elisha Nyamasvisva", "Chuntao Liu"], "title": "A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages,6 figures", "summary": "Video-based behavior recognition is essential in fields such as public\nsafety, intelligent surveillance, and human-computer interaction. Traditional\n3D Convolutional Neural Network (3D CNN) effectively capture local\nspatiotemporal features but struggle with modeling long-range dependencies.\nConversely, Transformers excel at learning global contextual information but\nface challenges with high computational costs. To address these limitations, we\npropose a hybrid framework combining 3D CNN and Transformer architectures. The\n3D CNN module extracts low-level spatiotemporal features, while the Transformer\nmodule captures long-range temporal dependencies, with a fusion mechanism\nintegrating both representations. Evaluated on benchmark datasets, the proposed\nmodel outperforms traditional 3D CNN and standalone Transformers, achieving\nhigher recognition accuracy with manageable complexity. Ablation studies\nfurther validate the complementary strengths of the two modules. This hybrid\nframework offers an effective and scalable solution for video-based behavior\nrecognition.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u54083D CNN\u548cTransformer\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u884c\u4e3a\u8bc6\u522b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89c6\u9891\u884c\u4e3a\u8bc6\u522b\u5728\u591a\u4e2a\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf3D CNN\u96be\u4ee5\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\uff0c\u800cTransformer\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u6548\u7387\u4e0e\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u54083D CNN\u548cTransformer\u67b6\u6784\u30023D CNN\u6a21\u5757\u8d1f\u8d23\u63d0\u53d6\u4f4e\u7ea7\u65f6\u7a7a\u7279\u5f81\uff0cTransformer\u6a21\u5757\u6355\u6349\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u878d\u5408\u673a\u5236\u6574\u5408\u4e24\u79cd\u8868\u793a\u3002", "result": "\u8be5\u6a21\u578b\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf3D CNN\u548c\u72ec\u7acbTransformer\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8bc6\u522b\u7cbe\u5ea6\u4e14\u590d\u6742\u5ea6\u53ef\u63a7\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u4e24\u4e2a\u6a21\u5757\u7684\u4e92\u8865\u4f18\u52bf\u3002", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u4e3a\u89c6\u9891\u884c\u4e3a\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06649", "pdf": "https://arxiv.org/pdf/2508.06649", "abs": "https://arxiv.org/abs/2508.06649", "authors": ["Daniel Wang", "Eli Brignac", "Minjia Mao", "Xiao Fang"], "title": "Measuring Stereotype and Deviation Biases in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are widely applied across diverse domains,\nraising concerns about their limitations and potential risks. In this study, we\ninvestigate two types of bias that LLMs may display: stereotype bias and\ndeviation bias. Stereotype bias refers to when LLMs consistently associate\nspecific traits with a particular demographic group. Deviation bias reflects\nthe disparity between the demographic distributions extracted from\nLLM-generated content and real-world demographic distributions. By asking four\nadvanced LLMs to generate profiles of individuals, we examine the associations\nbetween each demographic group and attributes such as political affiliation,\nreligion, and sexual orientation. Our experimental results show that all\nexamined LLMs exhibit both significant stereotype bias and deviation bias\ntowards multiple groups. Our findings uncover the biases that occur when LLMs\ninfer user attributes and shed light on the potential harms of LLM-generated\noutputs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u6240\u6709\u6d4b\u8bd5\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u4e2a\u4eba\u6863\u6848\u65f6\uff0c\u90fd\u663e\u8457\u5b58\u5728\u523b\u677f\u5370\u8c61\u504f\u89c1\u548c\u504f\u5dee\u504f\u89c1\uff0c\u63ed\u793a\u4e86\u5176\u8f93\u51fa\u5185\u5bb9\u7684\u6f5c\u5728\u5371\u5bb3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u5c40\u9650\u6027\u548c\u6f5c\u5728\u98ce\u9669\u4ee4\u4eba\u62c5\u5fe7\u3002\u672c\u7814\u7a76\u65e8\u5728\u8c03\u67e5LLMs\u53ef\u80fd\u8868\u73b0\u51fa\u7684\u4e24\u79cd\u504f\u89c1\uff1a\u523b\u677f\u5370\u8c61\u504f\u89c1\uff08\u5c06\u7279\u5b9a\u7279\u8d28\u4e0e\u7279\u5b9a\u4eba\u53e3\u7fa4\u4f53\u5173\u8054\uff09\u548c\u504f\u5dee\u504f\u89c1\uff08LLM\u751f\u6210\u5185\u5bb9\u7684\u4eba\u53e3\u5206\u5e03\u4e0e\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u7684\u5dee\u5f02\uff09\u3002", "method": "\u901a\u8fc7\u8981\u6c42\u56db\u79cd\u5148\u8fdb\u7684LLMs\u751f\u6210\u4e2a\u4eba\u6863\u6848\uff0c\u7814\u7a76\u4eba\u5458\u68c0\u67e5\u4e86\u6bcf\u4e2a\u4eba\u53e3\u7fa4\u4f53\u4e0e\u653f\u6cbb\u7acb\u573a\u3001\u5b97\u6559\u548c\u6027\u53d6\u5411\u7b49\u5c5e\u6027\u4e4b\u95f4\u7684\u5173\u8054\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u6709\u53d7\u68c0\u9a8c\u7684LLMs\u90fd\u5bf9\u591a\u4e2a\u7fa4\u4f53\u8868\u73b0\u51fa\u663e\u8457\u7684\u523b\u677f\u5370\u8c61\u504f\u89c1\u548c\u504f\u5dee\u504f\u89c1\u3002", "conclusion": "\u672c\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86LLMs\u5728\u63a8\u65ad\u7528\u6237\u5c5e\u6027\u65f6\u51fa\u73b0\u7684\u504f\u89c1\uff0c\u5e76\u9610\u660e\u4e86LLM\u751f\u6210\u5185\u5bb9\u6f5c\u5728\u7684\u5371\u5bb3\u3002"}}
{"id": "2508.07506", "pdf": "https://arxiv.org/pdf/2508.07506", "abs": "https://arxiv.org/abs/2508.07506", "authors": ["Hammas Bin Tanveer", "Wai Sun Chan", "Ricky K. P. Mok", "Sebastian Kappes", "Philipp Richter", "Oliver Gasser", "John Ronan", "Arthur Berger", "kc Claffy"], "title": "Unveiling IPv6 Scanning Dynamics: A Longitudinal Study Using Large Scale Proactive and Passive IPv6 Telescopes", "categories": ["cs.NI"], "comment": "24 pages, 16 figures, 8 tables, Accepted at ACM CoNEXT 2025 for\n  publication in the Proceedings of the ACM on Networking", "summary": "We introduce new tools and vantage points to develop and integrate proactive\ntechniques to attract IPv6 scan traffic, thus enabling its analysis. By\ndeploying the largest-ever IPv6 proactive telescope in a production ISP\nnetwork, we collected over 600M packets of unsolicited traffic from 1.9k\nAutonomous Systems in 10 months. We characterized the sources of unsolicited\ntraffic, evaluated the effectiveness of five major features across the network\nstack, and inferred scanners' sources of target addresses and their strategies.", "AI": {"tldr": "\u901a\u8fc7\u90e8\u7f72\u5927\u578bIPv6\u4e3b\u52a8\u671b\u8fdc\u955c\uff0c\u8be5\u7814\u7a76\u6536\u96c6\u5e76\u5206\u6790\u4e86\u5927\u89c4\u6a21IPv6\u626b\u63cf\u6d41\u91cf\u7684\u6765\u6e90\u3001\u7279\u5f81\u548c\u7b56\u7565\u3002", "motivation": "\u5f00\u53d1\u548c\u6574\u5408\u4e3b\u52a8\u6280\u672f\u4ee5\u5438\u5f15IPv6\u626b\u63cf\u6d41\u91cf\uff0c\u8fdb\u800c\u5bf9\u5176\u8fdb\u884c\u5206\u6790\u3002", "method": "\u5728\u4e00\u4e2a\u751f\u4ea7ISP\u7f51\u7edc\u4e2d\u90e8\u7f72\u4e86\u6709\u53f2\u4ee5\u6765\u6700\u5927\u7684IPv6\u4e3b\u52a8\u671b\u8fdc\u955c\u3002", "result": "\u572810\u4e2a\u6708\u5185\uff0c\u4ece1.9k\u81ea\u6cbb\u7cfb\u7edf\u6536\u96c6\u4e86\u8d85\u8fc76\u4ebf\u4e2a\u672a\u8bf7\u6c42\u6570\u636e\u5305\u3002\u8be5\u7814\u7a76\u8868\u5f81\u4e86\u672a\u8bf7\u6c42\u6d41\u91cf\u7684\u6765\u6e90\uff0c\u8bc4\u4f30\u4e86\u7f51\u7edc\u5806\u6808\u4e2d\u4e94\u4e2a\u4e3b\u8981\u7279\u5f81\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a8\u65ad\u4e86\u626b\u63cf\u4eea\u76ee\u6807\u5730\u5740\u7684\u6765\u6e90\u53ca\u5176\u7b56\u7565\u3002", "conclusion": "\u6210\u529f\u5438\u5f15\u5e76\u5206\u6790\u4e86\u5927\u89c4\u6a21IPv6\u626b\u63cf\u6d41\u91cf\uff0c\u63ed\u793a\u4e86\u626b\u63cf\u6e90\u7684\u7279\u70b9\u53ca\u626b\u63cf\u7b56\u7565\uff0c\u4e3a\u6df1\u5165\u7406\u89e3\u548c\u9632\u5fa1IPv6\u626b\u63cf\u884c\u4e3a\u63d0\u4f9b\u4e86\u5b9d\u8d35\u6d1e\u5bdf\u3002"}}
{"id": "2508.06591", "pdf": "https://arxiv.org/pdf/2508.06591", "abs": "https://arxiv.org/abs/2508.06591", "authors": ["Rachel K. Luu", "Jingyu Deng", "Mohammed Shahrudin Ibrahim", "Nam-Joon Cho", "Ming Dao", "Subra Suresh", "Markus J. Buehler"], "title": "Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.mtrl-sci", "cond-mat.other", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have reshaped the research landscape by enabling\nnew approaches to knowledge retrieval and creative ideation. Yet their\napplication in discipline-specific experimental science, particularly in highly\nmulti-disciplinary domains like materials science, remains limited. We present\na first-of-its-kind framework that integrates generative AI with literature\nfrom hitherto-unconnected fields such as plant science, biomimetics, and\nmaterials engineering to extract insights and design experiments for materials.\nWe focus on humidity-responsive systems such as pollen-based materials and\nRhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and\nadaptive performance. Using a suite of AI tools, including a fine-tuned model\n(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a\nHierarchical Sampling strategy, we extract structure-property relationships and\ntranslate them into new classes of bioinspired materials. Structured inference\nprotocols generate and evaluate hundreds of hypotheses from a single query,\nsurfacing novel and experimentally tractable ideas. We validate our approach\nthrough real-world implementation: LLM-generated procedures, materials designs,\nand mechanical predictions were tested in the laboratory, culminating in the\nfabrication of a novel pollen-based adhesive with tunable morphology and\nmeasured shear strength, establishing a foundation for future plant-derived\nadhesive design. This work demonstrates how AI-assisted ideation can drive\nreal-world materials design and enable effective human-AI collaboration.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u751f\u6210\u5f0fAI\u4e0e\u8de8\u5b66\u79d1\u77e5\u8bc6\uff08\u5982\u690d\u7269\u79d1\u5b66\u3001\u4eff\u751f\u5b66\u3001\u6750\u6599\u5de5\u7a0b\uff09\u7ed3\u5408\uff0c\u5229\u7528BioinspiredLLM\u3001RAG\u7b49AI\u5de5\u5177\uff0c\u7528\u4e8e\u8bbe\u8ba1\u6e7f\u5ea6\u54cd\u5e94\u7684\u751f\u7269\u542f\u53d1\u6750\u6599\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6210\u529f\u5236\u9020\u51faLLM\u8bbe\u8ba1\u7684\u65b0\u578b\u82b1\u7c89\u57fa\u7c98\u5408\u5242\uff0c\u8bc1\u660e\u4e86AI\u5728\u63a8\u52a8\u5b9e\u9645\u6750\u6599\u8bbe\u8ba1\u548c\u5b9e\u73b0\u4eba\u673a\u534f\u4f5c\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u77e5\u8bc6\u68c0\u7d22\u548c\u521b\u610f\u6784\u601d\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u6750\u6599\u79d1\u5b66\u7b49\u9ad8\u5ea6\u591a\u5b66\u79d1\u7684\u5b9e\u9a8c\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\u4ecd\u53d7\u5230\u9650\u5236\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6574\u5408\u751f\u6210\u5f0fAI\u4e0e\u8de8\u5b66\u79d1\u6587\u732e\uff08\u5982\u690d\u7269\u79d1\u5b66\u3001\u4eff\u751f\u5b66\u3001\u6750\u6599\u5de5\u7a0b\uff09\u7684\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u53d6\u89c1\u89e3\u5e76\u8bbe\u8ba1\u6750\u6599\u5b9e\u9a8c\u3002\u7814\u7a76\u805a\u7126\u4e8e\u82b1\u7c89\u57fa\u6750\u6599\u548c\u68d5\u6988\u53f6\u7b49\u6e7f\u5ea6\u54cd\u5e94\u7cfb\u7edf\u3002\u91c7\u7528\u4e86\u4e00\u7cfb\u5217AI\u5de5\u5177\uff0c\u5305\u62ec\u5fae\u8c03\u6a21\u578b\uff08BioinspiredLLM\uff09\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3001Agentic\u7cfb\u7edf\u548c\u5206\u5c42\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u63d0\u53d6\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u534f\u8bae\u751f\u6210\u548c\u8bc4\u4f30\u6570\u767e\u4e2a\u5047\u8bbe\u3002", "result": "\u7814\u7a76\u6210\u529f\u63d0\u53d6\u4e86\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u65b0\u578b\u751f\u7269\u542f\u53d1\u6750\u6599\u3002\u901a\u8fc7\u5b9e\u9a8c\u5ba4\u9a8c\u8bc1\uff0cLLM\u751f\u6210\u7684\u5b9e\u9a8c\u7a0b\u5e8f\u3001\u6750\u6599\u8bbe\u8ba1\u548c\u529b\u5b66\u9884\u6d4b\u5f97\u4ee5\u5b9e\u73b0\uff0c\u6700\u7ec8\u6210\u529f\u5236\u9020\u51fa\u4e00\u79cd\u5177\u6709\u53ef\u8c03\u5f62\u6001\u548c\u53ef\u6d4b\u91cf\u526a\u5207\u5f3a\u5ea6\u7684\u65b0\u578b\u82b1\u7c89\u57fa\u7c98\u5408\u5242\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u672a\u6765\u7684\u690d\u7269\u6e90\u7c98\u5408\u5242\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u5b83\u5c55\u793a\u4e86AI\u8f85\u52a9\u6784\u601d\u5982\u4f55\u6709\u6548\u63a8\u52a8\u73b0\u5b9e\u4e16\u754c\u7684\u6750\u6599\u8bbe\u8ba1\uff0c\u5e76\u5b9e\u73b0\u4eba\u673a\u4e4b\u95f4\u7684\u6709\u6548\u534f\u4f5c\u3002"}}
{"id": "2508.06706", "pdf": "https://arxiv.org/pdf/2508.06706", "abs": "https://arxiv.org/abs/2508.06706", "authors": ["Jaikrishna Manojkumar Patil", "Nathaniel Lee", "Al Mehdi Saadat Chowdhury", "YooJung Choi", "Paulo Shakarian"], "title": "Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "Rule-based methods for knowledge graph completion provide explainable results\nbut often require a significantly large number of rules to achieve competitive\nperformance. This can hinder explainability due to overwhelmingly large rule\nsets. We discover rule contexts (meaningful subsets of rules that work\ntogether) from training data and use learned probability distribution (i.e.\nprobabilistic circuits) over these rule contexts to more rapidly achieve\nperformance of the full rule set. Our approach achieves a 70-96% reduction in\nnumber of rules used while outperforming baseline by up to 31$\\times$ when\nusing equivalent minimal number of rules and preserves 91% of peak baseline\nperformance even when comparing our minimal rule sets against baseline's full\nrule sets. We show that our framework is grounded in well-known semantics of\nprobabilistic logic, does not require independence assumptions, and that our\ntractable inference procedure provides both approximate lower bounds and exact\nprobability of a given query. The efficacy of our method is validated by\nempirical studies on 8 standard benchmark datasets where we show competitive\nperformance by using only a fraction of the rules required by AnyBURL's\nstandard inference method, the current state-of-the-art for rule-based\nknowledge graph completion. This work may have further implications for general\nprobabilistic reasoning over learned sets of rules.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u6982\u7387\u7535\u8def\u548c\u201c\u89c4\u5219\u4e0a\u4e0b\u6587\u201d\u7684\u65b9\u6cd5\uff0c\u5927\u5e45\u51cf\u5c11\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4e2d\u57fa\u4e8e\u89c4\u5219\u65b9\u6cd5\u7684\u89c4\u5219\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u57fa\u4e8e\u89c4\u5219\u7684\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u65b9\u6cd5\u867d\u7136\u53ef\u89e3\u91ca\uff0c\u4f46\u4e3a\u8fbe\u5230\u7ade\u4e89\u529b\u80fd\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u89c4\u5219\uff0c\u8fd9\u53cd\u800c\u4f1a\u56e0\u4e3a\u89c4\u5219\u96c6\u8fc7\u5927\u800c\u635f\u5bb3\u5176\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u53d1\u73b0\u201c\u89c4\u5219\u4e0a\u4e0b\u6587\u201d\uff08\u534f\u540c\u5de5\u4f5c\u7684\u89c4\u5219\u5b50\u96c6\uff09\uff0c\u5e76\u5229\u7528\u5728\u8fd9\u4e9b\u89c4\u5219\u4e0a\u4e0b\u6587\u4e0a\u5b66\u4e60\u5230\u7684\u6982\u7387\u5206\u5e03\uff08\u5373\u6982\u7387\u7535\u8def\uff09\u8fdb\u884c\u63a8\u7406\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u6982\u7387\u903b\u8f91\uff0c\u4e0d\u8981\u6c42\u72ec\u7acb\u6027\u5047\u8bbe\uff0c\u4e14\u5176\u53ef\u5904\u7406\u7684\u63a8\u7406\u8fc7\u7a0b\u80fd\u63d0\u4f9b\u8fd1\u4f3c\u4e0b\u754c\u548c\u7cbe\u786e\u7684\u67e5\u8be2\u6982\u7387\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6240\u7528\u89c4\u5219\u6570\u91cf\u4e0a\u5b9e\u73b0\u4e8670-96%\u7684\u524a\u51cf\uff0c\u5728\u4f7f\u7528\u540c\u7b49\u6700\u5c11\u89c4\u5219\u65f6\uff0c\u6027\u80fd\u8d85\u8d8a\u57fa\u7ebf\u8fbe31\u500d\u3002\u5373\u4f7f\u5c06\u672c\u6587\u7684\u6700\u5c11\u89c4\u5219\u96c6\u4e0e\u57fa\u7ebf\u7684\u5b8c\u6574\u89c4\u5219\u96c6\u76f8\u6bd4\uff0c\u4e5f\u80fd\u4fdd\u630191%\u7684\u5cf0\u503c\u57fa\u7ebf\u6027\u80fd\u3002\u57288\u4e2a\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u7814\u7a76\u8bc1\u5b9e\u4e86\u5176\u6709\u6548\u6027\uff0c\u4ec5\u4f7f\u7528AnyBURL\uff08\u5f53\u524d\u57fa\u4e8e\u89c4\u5219\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684SOTA\uff09\u6807\u51c6\u63a8\u7406\u65b9\u6cd5\u6240\u9700\u89c4\u5219\u7684\u4e00\u5c0f\u90e8\u5206\uff0c\u5373\u8fbe\u5230\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u51cf\u5c11\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u6240\u9700\u7684\u89c4\u5219\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u4e3a\u6cdb\u5316\u5230\u5b66\u4e60\u89c4\u5219\u96c6\u4e0a\u7684\u6982\u7387\u63a8\u7406\u63d0\u4f9b\u4e86\u8fdb\u4e00\u6b65\u7684\u542f\u793a\u3002"}}
{"id": "2508.06529", "pdf": "https://arxiv.org/pdf/2508.06529", "abs": "https://arxiv.org/abs/2508.06529", "authors": ["Jiayuan Wang", "Q. M. Jonathan Wu", "Katsuya Suto", "Ning Zhang"], "title": "RMT-PPAD: Real-time Multi-task Learning for Panoptic Perception in Autonomous Driving", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Autonomous driving systems rely on panoptic driving perception that requires\nboth precision and real-time performance. In this work, we propose RMT-PPAD, a\nreal-time, transformer-based multi-task model that jointly performs object\ndetection, drivable area segmentation, and lane line segmentation. We introduce\na lightweight module, a gate control with an adapter to adaptively fuse shared\nand task-specific features, effectively alleviating negative transfer between\ntasks. Additionally, we design an adaptive segmentation decoder to learn the\nweights over multi-scale features automatically during the training stage. This\navoids the manual design of task-specific structures for different segmentation\ntasks. We also identify and resolve the inconsistency between training and\ntesting labels in lane line segmentation. This allows fairer evaluation.\nExperiments on the BDD100K dataset demonstrate that RMT-PPAD achieves\nstate-of-the-art results with mAP50 of 84.9% and Recall of 95.4% for object\ndetection, mIoU of 92.6% for drivable area segmentation, and IoU of 56.8% and\naccuracy of 84.7% for lane line segmentation. The inference speed reaches 32.6\nFPS. Moreover, we introduce real-world scenarios to evaluate RMT-PPAD\nperformance in practice. The results show that RMT-PPAD consistently delivers\nstable performance. The source codes and pre-trained models are released at\nhttps://github.com/JiayuanWang-JW/RMT-PPAD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRMT-PPAD\uff0c\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u5b9e\u65f6\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u3001\u53ef\u884c\u9a76\u533a\u57df\u5206\u5272\u548c\u8f66\u9053\u7ebf\u5206\u5272\uff0c\u5e76\u5728BDD100K\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9700\u8981\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u7684\u5168\u666f\u611f\u77e5\u80fd\u529b\u3002\u73b0\u6709\u7684\u591a\u4efb\u52a1\u611f\u77e5\u6a21\u578b\u9762\u4e34\u4efb\u52a1\u95f4\u8d1f\u8fc1\u79fb\u3001\u5206\u5272\u4efb\u52a1\u89e3\u7801\u5668\u624b\u52a8\u8bbe\u8ba1\u590d\u6742\u4ee5\u53ca\u8f66\u9053\u7ebf\u6807\u7b7e\u8bad\u7ec3\u6d4b\u8bd5\u4e0d\u4e00\u81f4\u7b49\u6311\u6218\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u6548\u7387\u3001\u6cdb\u5316\u6027\u548c\u516c\u5e73\u8bc4\u4f30\u3002", "method": "RMT-PPAD\u6a21\u578b\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u3002\u6838\u5fc3\u521b\u65b0\u5305\u62ec\uff1a1) \u5f15\u5165\u8f7b\u91cf\u7ea7\u95e8\u63a7\u9002\u914d\u5668\u6a21\u5757\uff0c\u81ea\u9002\u5e94\u878d\u5408\u5171\u4eab\u548c\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\uff0c\u6709\u6548\u7f13\u89e3\u8d1f\u8fc1\u79fb\uff1b2) \u8bbe\u8ba1\u81ea\u9002\u5e94\u5206\u5272\u89e3\u7801\u5668\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u81ea\u52a8\u5b66\u4e60\u591a\u5c3a\u5ea6\u7279\u5f81\u6743\u91cd\uff0c\u907f\u514d\u624b\u52a8\u8bbe\u8ba1\uff1b3) \u8bc6\u522b\u5e76\u89e3\u51b3\u8f66\u9053\u7ebf\u5206\u5272\u4e2d\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6807\u7b7e\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u4ee5\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u8bc4\u4f30\u3002", "result": "\u5728BDD100K\u6570\u636e\u96c6\u4e0a\uff0cRMT-PPAD\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\uff1a\u76ee\u6807\u68c0\u6d4bmAP50 84.9%\uff0cRecall 95.4%\uff1b\u53ef\u884c\u9a76\u533a\u57df\u5206\u5272mIoU 92.6%\uff1b\u8f66\u9053\u7ebf\u5206\u5272IoU 56.8%\uff0c\u7cbe\u5ea684.7%\u3002\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u8fbe\u523032.6 FPS\u3002\u6b64\u5916\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8bc4\u4f30\u8868\u660eRMT-PPAD\u80fd\u6301\u7eed\u63d0\u4f9b\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "conclusion": "RMT-PPAD\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u591a\u4efb\u52a1\u611f\u77e5\u6a21\u578b\uff0c\u80fd\u591f\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5bf9\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u7684\u53cc\u91cd\u9700\u6c42\u3002\u5b83\u901a\u8fc7\u521b\u65b0\u6027\u7684\u6a21\u5757\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.06665", "pdf": "https://arxiv.org/pdf/2508.06665", "abs": "https://arxiv.org/abs/2508.06665", "authors": ["Jonathan Shaw", "Dillon Mee", "Timothy Khouw", "Zackary Leech", "Daniel Wilson"], "title": "Testing the Limits of Machine Translation from One Book", "categories": ["cs.CL"], "comment": null, "summary": "Current state-of-the-art models demonstrate capacity to leverage in-context\nlearning to translate into previously unseen language contexts. Tanzer et al.\n[2024] utilize language materials (e.g. a grammar) to improve translation\nquality for Kalamang using large language models (LLMs). We focus on Kanuri, a\nlanguage that, despite having substantial speaker population, has minimal\ndigital resources. We design two datasets for evaluation: one focused on health\nand humanitarian terms, and another containing generalized terminology,\ninvestigating how domain-specific tasks impact LLM translation quality.\n  By providing different combinations of language resources (grammar,\ndictionary, and parallel sentences), we measure LLM translation effectiveness,\ncomparing results to native speaker translations and human linguist\nperformance. We evaluate using both automatic metrics and native speaker\nassessments of fluency and accuracy.\n  Results demonstrate that parallel sentences remain the most effective data\nsource, outperforming other methods in human evaluations and automatic metrics.\nWhile incorporating grammar improves over zero-shot translation, it fails as an\neffective standalone data source. Human evaluations reveal that LLMs achieve\naccuracy (meaning) more effectively than fluency (grammaticality).\n  These findings suggest LLM translation evaluation benefits from\nmultidimensional assessment beyond simple accuracy metrics, and that grammar\nalone, without parallel sentences, does not provide sufficient context for\neffective domain-specific translation.", "AI": {"tldr": "\u7814\u7a76\u4e86LLM\u5728\u6570\u5b57\u8d44\u6e90\u532e\u4e4f\u7684\u5361\u52aa\u91cc\u8bed\u4e0a\u7684\u7ffb\u8bd1\u80fd\u529b\uff0c\u53d1\u73b0\u5e76\u884c\u53e5\u662f\u6700\u6709\u6548\u7684\u8d44\u6e90\uff0c\u800c\u8bed\u6cd5\u8d44\u6e90\u5355\u72ec\u4f7f\u7528\u6548\u679c\u4e0d\u4f73\uff0c\u4e14LLM\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u6d41\u5229\u6027\u3002", "motivation": "\u867d\u7136\u73b0\u6709LLM\u80fd\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fdb\u884c\u8bed\u8a00\u7ffb\u8bd1\uff0c\u4f46\u5bf9\u4e8e\u50cf\u5361\u52aa\u91cc\u8bed\u8fd9\u79cd\u62e5\u6709\u5927\u91cf\u4f7f\u7528\u8005\u5374\u6570\u5b57\u8d44\u6e90\u7a00\u7f3a\u7684\u8bed\u8a00\uff0c\u5176\u7ffb\u8bd1\u8868\u73b0\u4ee5\u53ca\u9886\u57df\u7279\u5f02\u6027\u4efb\u52a1\u5bf9LLM\u7ffb\u8bd1\u8d28\u91cf\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u672c\u7814\u7a76\u4e3a\u8bc4\u4f30LLM\u7684\u7ffb\u8bd1\u80fd\u529b\uff0c\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u6570\u636e\u96c6\uff08\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u5065\u5eb7\u548c\u4eba\u9053\u4e3b\u4e49\u672f\u8bed\uff0c\u53e6\u4e00\u4e2a\u5305\u542b\u901a\u7528\u672f\u8bed\uff09\u3002\u901a\u8fc7\u63d0\u4f9b\u4e0d\u540c\u7ec4\u5408\u7684\u8bed\u8a00\u8d44\u6e90\uff08\u8bed\u6cd5\u3001\u8bcd\u5178\u548c\u5e76\u884c\u53e5\uff09\uff0c\u6d4b\u91cfLLM\u7684\u7ffb\u8bd1\u6709\u6548\u6027\u3002\u8bc4\u4f30\u6807\u51c6\u5305\u62ec\u81ea\u52a8\u6307\u6807\u4ee5\u53ca\u6bcd\u8bed\u4f7f\u7528\u8005\u5bf9\u8bd1\u6587\u6d41\u5229\u5ea6\u548c\u51c6\u786e\u6027\u7684\u8bc4\u4f30\uff0c\u5e76\u4e0e\u6bcd\u8bed\u4f7f\u7528\u8005\u548c\u4eba\u7c7b\u8bed\u8a00\u5b66\u5bb6\u7684\u7ffb\u8bd1\u8868\u73b0\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5e76\u884c\u53e5\u4ecd\u7136\u662fLLM\u7ffb\u8bd1\u6700\u6709\u6548\u7684\u6570\u636e\u6e90\uff0c\u5728\u4eba\u7c7b\u8bc4\u4f30\u548c\u81ea\u52a8\u6307\u6807\u4e2d\u5747\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u867d\u7136\u6574\u5408\u8bed\u6cd5\u80fd\u6539\u5584\u96f6\u6837\u672c\u7ffb\u8bd1\uff0c\u4f46\u5355\u72ec\u7684\u8bed\u6cd5\u8d44\u6e90\u5e76\u975e\u6709\u6548\u7684\u72ec\u7acb\u6570\u636e\u6e90\u3002\u4eba\u7c7b\u8bc4\u4f30\u63ed\u793aLLM\u5728\u7ffb\u8bd1\u51c6\u786e\u6027\uff08\u610f\u4e49\uff09\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u6d41\u5229\u6027\uff08\u8bed\u6cd5\uff09\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0LLM\u7ffb\u8bd1\u8bc4\u4f30\u5e94\u91c7\u7528\u591a\u7ef4\u5ea6\u65b9\u6cd5\uff0c\u800c\u975e\u4ec5\u9650\u4e8e\u7b80\u5355\u7684\u51c6\u786e\u6027\u6307\u6807\u3002\u6b64\u5916\uff0c\u5355\u72ec\u7684\u8bed\u6cd5\u4fe1\u606f\u5728\u7f3a\u4e4f\u5e76\u884c\u53e5\u7684\u60c5\u51b5\u4e0b\uff0c\u4e0d\u8db3\u4ee5\u63d0\u4f9b\u6709\u6548\u9886\u57df\u7279\u5b9a\u7ffb\u8bd1\u6240\u9700\u7684\u5145\u5206\u4e0a\u4e0b\u6587\u3002"}}
{"id": "2508.07578", "pdf": "https://arxiv.org/pdf/2508.07578", "abs": "https://arxiv.org/abs/2508.07578", "authors": ["Yu Gou", "Tong Zhang", "Jun Liu", "Tingting Yang", "Shanshan Song", "Jun-Hong Cui"], "title": "Achieving Fair-Effective Communications and Robustness in Underwater Acoustic Sensor Networks: A Semi-Cooperative Approach", "categories": ["cs.NI"], "comment": null, "summary": "This paper investigates the fair-effective communication and robustness in\nimperfect and energy-constrained underwater acoustic sensor networks\n(IC-UASNs). Specifically, we investigate the impact of unexpected node\nmalfunctions on the network performance under the time-varying acoustic\nchannels. Each node is expected to satisfy Quality of Service (QoS)\nrequirements. However, achieving individual QoS requirements may interfere with\nother concurrent communications. Underwater nodes rely excessively on the\nrationality of other underwater nodes when guided by fully cooperative\napproaches, making it difficult to seek a trade-off between individual QoS and\nglobal fair-effective communications under imperfect conditions. Therefore,\nthis paper presents a SEmi-COoperative Power Allocation approach (SECOPA) that\nachieves fair-effective communication and robustness in IC-UASNs. The approach\nis distributed multi-agent reinforcement learning (MARL)-based, and the\nobjectives are twofold. On the one hand, each intelligent node individually\ndecides the transmission power to simultaneously optimize individual and global\nperformance. On the other hand, advanced training algorithms are developed to\nprovide imperfect environments for training robust models that can adapt to the\ntime-varying acoustic channels and handle unexpected node failures in the\nnetwork. Numerical results are presented to validate our proposed approach.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u975e\u5b8c\u7f8e\u3001\u80fd\u91cf\u53d7\u9650\u6c34\u58f0\u4f20\u611f\u5668\u7f51\u7edc\uff08IC-UASNs\uff09\u4e2d\u7684\u516c\u5e73\u6709\u6548\u901a\u4fe1\u4e0e\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u534a\u534f\u4f5c\u529f\u7387\u5206\u914d\u65b9\u6cd5\uff08SECOPA\uff09\uff0c\u4ee5\u5728\u5e94\u5bf9\u8282\u70b9\u6545\u969c\u548c\u65f6\u53d8\u4fe1\u9053\u7684\u540c\u65f6\uff0c\u4f18\u5316\u4e2a\u4f53\u4e0e\u5168\u5c40\u6027\u80fd\u3002", "motivation": "\u5728\u975e\u5b8c\u7f8e\u3001\u80fd\u91cf\u53d7\u9650\u6c34\u58f0\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\uff0c\u5b58\u5728\u610f\u5916\u8282\u70b9\u6545\u969c\u548c\u65f6\u53d8\u58f0\u5b66\u4fe1\u9053\uff0c\u4e14\u8282\u70b9\u9700\u6ee1\u8db3QoS\u8981\u6c42\u3002\u7136\u800c\uff0c\u5b9e\u73b0\u4e2a\u4f53QoS\u53ef\u80fd\u5e72\u6270\u5e76\u53d1\u901a\u4fe1\uff0c\u5bfc\u81f4\u4e2a\u4f53QoS\u4e0e\u5168\u5c40\u516c\u5e73\u6709\u6548\u901a\u4fe1\u96be\u4ee5\u6743\u8861\u3002\u4f20\u7edf\u7684\u5b8c\u5168\u534f\u4f5c\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5176\u4ed6\u8282\u70b9\u7684\u7406\u6027\uff0c\u5728\u975e\u5b8c\u7f8e\u6761\u4ef6\u4e0b\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u534a\u534f\u4f5c\u529f\u7387\u5206\u914d\u65b9\u6cd5\uff08SECOPA\uff09\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u3002\u8be5\u65b9\u6cd5\u4f7f\u6bcf\u4e2a\u667a\u80fd\u8282\u70b9\u80fd\u591f\u72ec\u7acb\u51b3\u5b9a\u4f20\u8f93\u529f\u7387\uff0c\u4ece\u800c\u540c\u65f6\u4f18\u5316\u4e2a\u4f53\u548c\u5168\u5c40\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86\u9ad8\u7ea7\u8bad\u7ec3\u7b97\u6cd5\uff0c\u5728\u975e\u5b8c\u7f8e\u73af\u5883\u4e2d\u8bad\u7ec3\u51fa\u9c81\u68d2\u6a21\u578b\uff0c\u4ee5\u9002\u5e94\u65f6\u53d8\u58f0\u5b66\u4fe1\u9053\u5e76\u5904\u7406\u610f\u5916\u8282\u70b9\u6545\u969c\u3002", "result": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51faSECOPA\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684SECOPA\u65b9\u6cd5\u80fd\u591f\u5728\u975e\u5b8c\u7f8e\u3001\u80fd\u91cf\u53d7\u9650\u6c34\u58f0\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u5b9e\u73b0\u516c\u5e73\u6709\u6548\u7684\u901a\u4fe1\u548c\u9c81\u68d2\u6027\uff0c\u6210\u529f\u5e94\u5bf9\u65f6\u53d8\u4fe1\u9053\u548c\u610f\u5916\u8282\u70b9\u6545\u969c\uff0c\u5e76\u6709\u6548\u5e73\u8861\u4e2a\u4f53\u4e0e\u5168\u5c40\u6027\u80fd\u76ee\u6807\u3002"}}
{"id": "2508.06601", "pdf": "https://arxiv.org/pdf/2508.06601", "abs": "https://arxiv.org/abs/2508.06601", "authors": ["Kyle O'Brien", "Stephen Casper", "Quentin Anthony", "Tomek Korbak", "Robert Kirk", "Xander Davies", "Ishan Mishra", "Geoffrey Irving", "Yarin Gal", "Stella Biderman"], "title": "Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "https://deepignorance.ai/", "summary": "Open-weight AI systems offer unique benefits, including enhanced\ntransparency, open research, and decentralized access. However, they are\nvulnerable to tampering attacks which can efficiently elicit harmful behaviors\nby modifying weights or activations. Currently, there is not yet a robust\nscience of open-weight model risk management. Existing safety fine-tuning\nmethods and other post-training techniques have struggled to make LLMs\nresistant to more than a few dozen steps of adversarial fine-tuning. In this\npaper, we investigate whether filtering text about dual-use topics from\ntraining data can prevent unwanted capabilities and serve as a more\ntamper-resistant safeguard. We introduce a multi-stage pipeline for scalable\ndata filtering and show that it offers a tractable and effective method for\nminimizing biothreat proxy knowledge in LLMs. We pretrain multiple\n6.9B-parameter models from scratch and find that they exhibit substantial\nresistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M\ntokens of biothreat-related text -- outperforming existing post-training\nbaselines by over an order of magnitude -- with no observed degradation to\nunrelated capabilities. However, while filtered models lack internalized\ndangerous knowledge, we find that they can still leverage such information when\nit is provided in context (e.g., via search tool augmentation), demonstrating a\nneed for a defense-in-depth approach. Overall, these findings help to establish\npretraining data curation as a promising layer of defense for open-weight AI\nsystems.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u901a\u8fc7\u9884\u8bad\u7ec3\u6570\u636e\u8fc7\u6ee4\u53cc\u7528\u9014\u5185\u5bb9\uff0c\u53ef\u663e\u8457\u63d0\u9ad8\u5f00\u6e90AI\u6a21\u578b\u5bf9\u6076\u610f\u7be1\u6539\u7684\u62b5\u6297\u529b\uff0c\u4f46\u4ecd\u9700\u591a\u5c42\u9632\u5fa1\u3002", "motivation": "\u5f00\u6e90AI\u7cfb\u7edf\u867d\u6709\u4f18\u52bf\uff0c\u4f46\u6613\u53d7\u7be1\u6539\u653b\u51fb\uff0c\u73b0\u6709\u5fae\u8c03\u7b49\u540e\u8bad\u7ec3\u5b89\u5168\u65b9\u6cd5\u5bf9\u5bf9\u6297\u6027\u5fae\u8c03\u62b5\u6297\u529b\u5f31\uff0c\u7f3a\u4e4f\u7a33\u5065\u7684\u98ce\u9669\u7ba1\u7406\u79d1\u5b66\u3002", "method": "\u5f15\u5165\u591a\u9636\u6bb5\u6570\u636e\u8fc7\u6ee4\u7ba1\u9053\uff0c\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u7b5b\u9009\u51fa\u53cc\u7528\u9014\uff08\u5982\u751f\u7269\u5a01\u80c1\uff09\u76f8\u5173\u6587\u672c\uff0c\u5e76\u7528\u6b64\u8fc7\u6ee4\u6570\u636e\u4ece\u5934\u9884\u8bad\u7ec3\u591a\u4e2a6.9B\u53c2\u6570\u6a21\u578b\uff0c\u4ee5\u8bc4\u4f30\u5176\u6297\u7be1\u6539\u80fd\u529b\u3002", "result": "\u8fc7\u6ee4\u540e\u7684\u6a21\u578b\u5bf9\u5bf9\u6297\u6027\u5fae\u8c03\u653b\u51fb\u5c55\u73b0\u51fa\u663e\u8457\u62b5\u6297\u529b\uff08\u9ad8\u8fbe10,000\u6b65\u548c300M\u751f\u7269\u5a01\u80c1\u76f8\u5173\u6587\u672c\uff09\uff0c\u8fdc\u8d85\u73b0\u6709\u540e\u8bad\u7ec3\u57fa\u7ebf\uff0c\u4e14\u672a\u5f71\u54cd\u65e0\u5173\u80fd\u529b\u3002\u4f46\u53d1\u73b0\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u4e2d\u83b7\u5f97\u5371\u9669\u4fe1\u606f\u65f6\u4ecd\u80fd\u5229\u7528\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6570\u636e\u7b5b\u9009\u662f\u5f00\u6e90AI\u7cfb\u7edf\u6709\u524d\u666f\u7684\u9632\u5fa1\u5c42\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u8010\u7be1\u6539\u7684\u6a21\u578b\uff1b\u4f46\u9274\u4e8e\u6a21\u578b\u4ecd\u80fd\u5229\u7528\u5916\u90e8\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ecd\u9700\u91c7\u7528\u6df1\u5ea6\u9632\u5fa1\u7b56\u7565\u3002"}}
{"id": "2508.06716", "pdf": "https://arxiv.org/pdf/2508.06716", "abs": "https://arxiv.org/abs/2508.06716", "authors": ["Blair Johnson", "Clayton Kerce", "Faramarz Fekri"], "title": "GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning", "categories": ["cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Differentiable inductive logic programming (ILP) techniques have proven\neffective at finding approximate rule-based solutions to link prediction and\nnode classification problems on knowledge graphs; however, the common\nassumption of chain-like rule structure can hamper the performance and\ninterpretability of existing approaches. We introduce GLIDR, a differentiable\nrule learning method that models the inference of logic rules with more\nexpressive syntax than previous methods. GLIDR uses a differentiable message\npassing inference algorithm that generalizes previous chain-like rule learning\nmethods to allow rules with features like branches and cycles. GLIDR has a\nsimple and expressive rule search space which is parameterized by a limit on\nthe maximum number of free variables that may be included in a rule. Explicit\nlogic rules can be extracted from the weights of a GLIDR model for use with\nsymbolic solvers. We demonstrate that GLIDR can significantly outperform\nexisting rule learning methods on knowledge graph completion tasks and even\ncompete with embedding methods despite the inherent disadvantage of being a\nstructure-only prediction method. We show that rules extracted from GLIDR\nretain significant predictive performance, and that GLIDR is highly robust to\ntraining data noise. Finally, we demonstrate that GLIDR can be chained with\ndeep neural networks and optimized end-to-end for rule learning on arbitrary\ndata modalities.", "AI": {"tldr": "GLIDR\u662f\u4e00\u79cd\u65b0\u7684\u53ef\u5fae\u5206\u89c4\u5219\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u652f\u6301\u66f4\u590d\u6742\u7684\u89c4\u5219\u7ed3\u6784\uff08\u5982\u5206\u652f\u548c\u5faa\u73af\uff09\u6765\u89e3\u51b3\u73b0\u6709\u94fe\u5f0f\u89c4\u5219\u7684\u5c40\u9650\u6027\uff0c\u4ece\u800c\u5728\u77e5\u8bc6\u56fe\u8c31\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u89c4\u5219\u3002", "motivation": "\u73b0\u6709\u53ef\u5fae\u5206\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\uff08ILP\uff09\u6280\u672f\u5728\u77e5\u8bc6\u56fe\u8c31\u94fe\u63a5\u9884\u6d4b\u548c\u8282\u70b9\u5206\u7c7b\u4e2d\uff0c\u666e\u904d\u5047\u8bbe\u94fe\u5f0f\u89c4\u5219\u7ed3\u6784\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u672c\u6587\u63d0\u51faGLIDR\uff0c\u4e00\u79cd\u53ef\u5fae\u5206\u89c4\u5219\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u53ef\u5fae\u5206\u6d88\u606f\u4f20\u9012\u63a8\u7406\u7b97\u6cd5\u6765\u5efa\u6a21\u5177\u6709\u66f4\u4e30\u5bcc\u8bed\u6cd5\u7684\u903b\u8f91\u89c4\u5219\uff08\u5305\u62ec\u5206\u652f\u548c\u5faa\u73af\uff09\u3002\u5176\u89c4\u5219\u641c\u7d22\u7a7a\u95f4\u901a\u8fc7\u6700\u5927\u81ea\u7531\u53d8\u91cf\u6570\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u5e76\u80fd\u4ece\u6a21\u578b\u6743\u91cd\u4e2d\u63d0\u53d6\u663e\u5f0f\u903b\u8f91\u89c4\u5219\u3002", "result": "GLIDR\u5728\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89c4\u5219\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u80fd\u4e0e\u5d4c\u5165\u65b9\u6cd5\u7ade\u4e89\u3002\u4eceGLIDR\u63d0\u53d6\u7684\u89c4\u5219\u4fdd\u6301\u4e86\u663e\u8457\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u4e14GLIDR\u5bf9\u8bad\u7ec3\u6570\u636e\u566a\u58f0\u9ad8\u5ea6\u9c81\u68d2\u3002\u6b64\u5916\uff0cGLIDR\u53ef\u4e0e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\u3002", "conclusion": "GLIDR\u901a\u8fc7\u652f\u6301\u66f4\u5177\u8868\u73b0\u529b\u7684\u89c4\u5219\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u89c4\u5219\u5b66\u4e60\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u89c4\u5219\u3002\u5176\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u7ed3\u5408\u6269\u5c55\u4e86\u5176\u5728\u591a\u79cd\u6570\u636e\u6a21\u6001\u4e0a\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.06530", "pdf": "https://arxiv.org/pdf/2508.06530", "abs": "https://arxiv.org/abs/2508.06530", "authors": ["Ming-Kun Xie", "Jia-Hao Xiao", "Gang Niu", "Lei Feng", "Zhiqiang Kou", "Min-Ling Zhang", "Masashi Sugiyama"], "title": "What Makes \"Good\" Distractors for Object Hallucination Evaluation in Large Vision-Language Models?", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Large Vision-Language Models (LVLMs), empowered by the success of Large\nLanguage Models (LLMs), have achieved impressive performance across domains.\nDespite the great advances in LVLMs, they still suffer from the unavailable\nobject hallucination issue, which tends to generate objects inconsistent with\nthe image content. The most commonly used Polling-based Object Probing\nEvaluation (POPE) benchmark evaluates this issue by sampling negative\ncategories according to category-level statistics, \\textit{e.g.}, category\nfrequencies and co-occurrence. However, with the continuous advancement of\nLVLMs, the POPE benchmark has shown diminishing effectiveness in assessing\nobject hallucination, as it employs a simplistic sampling strategy that\noverlooks image-specific information and restricts distractors to negative\nobject categories only. In this paper, we introduce the Hallucination\nsearching-based Object Probing Evaluation (HOPE) benchmark, aiming to generate\nthe most misleading distractors (\\textit{i.e.}, non-existent objects or\nincorrect image descriptions) that can trigger hallucination in LVLMs, which\nserves as a means to more rigorously assess their immunity to hallucination. To\nexplore the image-specific information, the content-aware hallucination\nsearching leverages Contrastive Language-Image Pre-Training (CLIP) to\napproximate the predictive behavior of LVLMs by selecting negative objects with\nthe highest predicted likelihood as distractors. To expand the scope of\nhallucination assessment, the description-based hallucination searching\nconstructs highly misleading distractors by pairing true objects with false\ndescriptions. Experimental results show that HOPE leads to a precision drop of\nat least 9\\% and up to 23\\% across various state-of-the-art LVLMs,\nsignificantly outperforming POPE in exposing hallucination vulnerabilities. The\ncode is available at https://github.com/xiemk/HOPE.", "AI": {"tldr": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5b58\u5728\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6POPE\u6548\u679c\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51faHOPE\u57fa\u51c6\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u5ea6\u8bef\u5bfc\u6027\u7684\u5e72\u6270\u9879\uff08\u5305\u62ec\u56fe\u50cf\u7279\u5b9a\u548c\u63cf\u8ff0\u7c7b\u5e72\u6270\uff09\uff0c\u66f4\u4e25\u683c\u5730\u8bc4\u4f30LVLMs\u7684\u6297\u5e7b\u89c9\u80fd\u529b\uff0c\u5e76\u8bc1\u660e\u5176\u80fd\u663e\u8457\u66b4\u9732LVLMs\u7684\u5e7b\u89c9\u6f0f\u6d1e\u3002", "motivation": "\u5c3d\u7ba1LVLMs\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u4ecd\u53d7\u56f0\u4e8e\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u751f\u6210\u4e0e\u56fe\u50cf\u5185\u5bb9\u4e0d\u7b26\u7684\u7269\u4f53\u3002\u6700\u5e38\u7528\u7684POPE\u8bc4\u4f30\u57fa\u51c6\u56e0\u5176\u91c7\u6837\u7b56\u7565\u8fc7\u4e8e\u7b80\u5355\u3001\u5ffd\u7565\u56fe\u50cf\u7279\u5b9a\u4fe1\u606f\u4e14\u9650\u5236\u5e72\u6270\u9879\uff0c\u5728\u8bc4\u4f30LVLMs\u7684\u7269\u4f53\u5e7b\u89c9\u65b9\u9762\u6548\u679c\u9010\u6e10\u51cf\u5f31\u3002", "method": "\u672c\u6587\u63d0\u51faHOPE\uff08Hallucination searching-based Object Probing Evaluation\uff09\u57fa\u51c6\uff0c\u65e8\u5728\u751f\u6210\u6700\u5177\u8bef\u5bfc\u6027\u7684\u5e72\u6270\u9879\uff08\u4e0d\u5b58\u5728\u7684\u7269\u4f53\u6216\u4e0d\u6b63\u786e\u7684\u56fe\u50cf\u63cf\u8ff0\uff09\uff0c\u4ee5\u66f4\u4e25\u683c\u5730\u8bc4\u4f30LVLMs\u7684\u6297\u5e7b\u89c9\u80fd\u529b\u3002\n1. **\u5185\u5bb9\u611f\u77e5\u5e7b\u89c9\u641c\u7d22**\uff1a\u5229\u7528CLIP\u8fd1\u4f3cLVLMs\u7684\u9884\u6d4b\u884c\u4e3a\uff0c\u9009\u62e9\u9884\u6d4b\u53ef\u80fd\u6027\u6700\u9ad8\u7684\u8d1f\u9762\u5bf9\u8c61\u4f5c\u4e3a\u5e72\u6270\u9879\uff0c\u4ee5\u63a2\u7d22\u56fe\u50cf\u7279\u5b9a\u4fe1\u606f\u3002\n2. **\u57fa\u4e8e\u63cf\u8ff0\u7684\u5e7b\u89c9\u641c\u7d22**\uff1a\u901a\u8fc7\u5c06\u771f\u5b9e\u5bf9\u8c61\u4e0e\u865a\u5047\u63cf\u8ff0\u914d\u5bf9\u6765\u6784\u5efa\u9ad8\u5ea6\u8bef\u5bfc\u6027\u7684\u5e72\u6270\u9879\uff0c\u4ee5\u6269\u5927\u5e7b\u89c9\u8bc4\u4f30\u7684\u8303\u56f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHOPE\u5728\u5404\u79cd\u5148\u8fdb\u7684LVLMs\u4e0a\u5bfc\u81f4\u81f3\u5c119%\u523023%\u7684\u7cbe\u5ea6\u4e0b\u964d\uff0c\u663e\u8457\u4f18\u4e8ePOPE\u5728\u66b4\u9732\u5e7b\u89c9\u6f0f\u6d1e\u65b9\u9762\u7684\u8868\u73b0\u3002", "conclusion": "HOPE\u57fa\u51c6\u901a\u8fc7\u5f15\u5165\u66f4\u5177\u6311\u6218\u6027\u548c\u8bef\u5bfc\u6027\u7684\u5e72\u6270\u9879\uff0c\u80fd\u66f4\u4e25\u683c\u3001\u6709\u6548\u5730\u8bc4\u4f30LVLMs\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u66b4\u9732\u4e86\u5f53\u524d\u6700\u5148\u8fdbLVLMs\u7684\u5e7b\u89c9\u8106\u5f31\u6027\u3002"}}
{"id": "2508.06671", "pdf": "https://arxiv.org/pdf/2508.06671", "abs": "https://arxiv.org/abs/2508.06671", "authors": ["Swati Rajwal", "Shivank Garg", "Reem Abdel-Salam", "Abdelrahman Zayed"], "title": "Do Biased Models Have Biased Thoughts?", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Accepted at main track of the Second Conference on Language Modeling\n  (COLM 2025)", "summary": "The impressive performance of language models is undeniable. However, the\npresence of biases based on gender, race, socio-economic status, physical\nappearance, and sexual orientation makes the deployment of language models\nchallenging. This paper studies the effect of chain-of-thought prompting, a\nrecent approach that studies the steps followed by the model before it\nresponds, on fairness. More specifically, we ask the following question:\n\\textit{Do biased models have biased thoughts}? To answer our question, we\nconduct experiments on $5$ popular large language models using fairness metrics\nto quantify $11$ different biases in the model's thoughts and output. Our\nresults show that the bias in the thinking steps is not highly correlated with\nthe output bias (less than $0.6$ correlation with a $p$-value smaller than\n$0.001$ in most cases). In other words, unlike human beings, the tested models\nwith biased decisions do not always possess biased thoughts.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bed\u8a00\u6a21\u578b\u7684\u601d\u7ef4\u8fc7\u7a0b\uff08chain-of-thought\uff09\u4e2d\u7684\u504f\u89c1\u4e0e\u6700\u7ec8\u8f93\u51fa\u7684\u504f\u89c1\u76f8\u5173\u6027\u4e0d\u9ad8\uff0c\u610f\u5473\u7740\u6709\u504f\u89c1\u7684\u8f93\u51fa\u4e0d\u4e00\u5b9a\u6e90\u4e8e\u6709\u504f\u89c1\u7684\u201c\u601d\u8003\u201d\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u5353\u8d8a\uff0c\u4f46\u5176\u56fa\u6709\u7684\u6027\u522b\u3001\u79cd\u65cf\u3001\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u7b49\u504f\u89c1\u963b\u788d\u4e86\u5176\u5e7f\u6cdb\u90e8\u7f72\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u94fe\u5f0f\u601d\u8003\uff08chain-of-thought\uff09\u63d0\u793a\u5bf9\u6a21\u578b\u516c\u5e73\u6027\u7684\u5f71\u54cd\u3002", "method": "\u5bf95\u4e2a\u6d41\u884c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4f7f\u7528\u516c\u5e73\u6027\u6307\u6807\u91cf\u5316\u5176\u601d\u7ef4\u8fc7\u7a0b\u548c\u6700\u7ec8\u8f93\u51fa\u4e2d\u768411\u79cd\u4e0d\u540c\u504f\u89c1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u601d\u7ef4\u6b65\u9aa4\u4e2d\u7684\u504f\u89c1\u4e0e\u8f93\u51fa\u504f\u89c1\u76f8\u5173\u6027\u4e0d\u9ad8\uff08\u591a\u6570\u60c5\u51b5\u4e0b\u76f8\u5173\u6027\u5c0f\u4e8e0.6\uff0cp\u503c\u5c0f\u4e8e0.001\uff09\u3002", "conclusion": "\u4e0e\u4eba\u7c7b\u4e0d\u540c\uff0c\u6d4b\u8bd5\u6a21\u578b\u5373\u4f7f\u6709\u504f\u89c1\u51b3\u7b56\uff0c\u5176\u5185\u90e8\u601d\u7ef4\u8fc7\u7a0b\u4e0d\u4e00\u5b9a\u4e5f\u5b58\u5728\u504f\u89c1\u3002"}}
{"id": "2508.07604", "pdf": "https://arxiv.org/pdf/2508.07604", "abs": "https://arxiv.org/abs/2508.07604", "authors": ["Maryam Abbasalizadeh", "Sashank Narain"], "title": "Joint Scheduling and Resource Allocation in mmWave IAB Networks Using Deep RL", "categories": ["cs.NI"], "comment": "Accepted at MILCOM 2025 (IEEE Military Communications Conference)", "summary": "Integrated Access and Backhaul (IAB) is critical for dense 5G and beyond\ndeployments, especially in mmWave bands where fiber backhaul is infeasible. We\npropose a novel Deep Reinforcement Learning (DRL) framework for joint link\nscheduling and resource slicing in dynamic, interference-prone IAB networks.\nOur method integrates a greedy Double Deep Q-Network (DDQN) scheduler to\nactivate access and backhaul links based on traffic and topology, with a\nmulti-agent DDQN allocator for bandwidth and antenna assignment across network\nslices. This decentralized approach respects strict antenna constraints and\nsupports concurrent scheduling across heterogeneous links. Evaluations across\n96 dynamic topologies show 99.84 percent scheduling accuracy and 20.90 percent\nthroughput improvement over baselines. The framework's efficient operation and\nadaptability make it suitable for dynamic and resource-constrained deployments,\nwhere fast link scheduling and autonomous backhaul coordination are vital.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u3001\u6613\u53d7\u5e72\u6270\u7684\u7efc\u5408\u63a5\u5165\u548c\u56de\u4f20\uff08IAB\uff09\u7f51\u7edc\u4e2d\u8fdb\u884c\u8054\u5408\u94fe\u8def\u8c03\u5ea6\u548c\u8d44\u6e90\u5207\u7247\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8c03\u5ea6\u7cbe\u5ea6\u548c\u541e\u5410\u91cf\u3002", "motivation": "IAB\u5bf9\u5bc6\u96c65G\u53ca\u672a\u6765\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5149\u7ea4\u56de\u4f20\u4e0d\u53ef\u884c\u7684\u6beb\u7c73\u6ce2\u9891\u6bb5\u3002\u5f53\u524d\u6311\u6218\u5728\u4e8e\u5982\u4f55\u6709\u6548\u7ba1\u7406\u52a8\u6001\u4e14\u6613\u53d7\u5e72\u6270\u7684IAB\u7f51\u7edc\u4e2d\u7684\u94fe\u8def\u8c03\u5ea6\u4e0e\u8d44\u6e90\u5206\u914d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u8d2a\u5a6a\u7684\u53cc\u6df1\u5ea6Q\u7f51\u7edc\uff08DDQN\uff09\u8c03\u5ea6\u5668\uff08\u7528\u4e8e\u6839\u636e\u6d41\u91cf\u548c\u62d3\u6251\u6fc0\u6d3b\u63a5\u5165\u548c\u56de\u4f20\u94fe\u8def\uff09\u548c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53DDQN\u5206\u914d\u5668\uff08\u7528\u4e8e\u8de8\u7f51\u7edc\u5207\u7247\u7684\u5e26\u5bbd\u548c\u5929\u7ebf\u5206\u914d\uff09\u3002\u6b64\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\u80fd\u6ee1\u8db3\u4e25\u683c\u7684\u5929\u7ebf\u7ea6\u675f\u5e76\u652f\u6301\u5f02\u6784\u94fe\u8def\u7684\u5e76\u53d1\u8c03\u5ea6\u3002", "result": "\u572896\u79cd\u52a8\u6001\u62d3\u6251\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6846\u67b6\u7684\u8c03\u5ea6\u51c6\u786e\u7387\u8fbe\u523099.84%\uff0c\u541e\u5410\u91cf\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e8620.90%\u3002", "conclusion": "\u8be5\u6846\u67b6\u8fd0\u884c\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\uff0c\u975e\u5e38\u9002\u5408\u9700\u8981\u5feb\u901f\u94fe\u8def\u8c03\u5ea6\u548c\u81ea\u4e3b\u56de\u4f20\u534f\u8c03\u7684\u52a8\u6001\u3001\u8d44\u6e90\u53d7\u9650\u7684\u90e8\u7f72\u73af\u5883\u3002"}}
{"id": "2508.06614", "pdf": "https://arxiv.org/pdf/2508.06614", "abs": "https://arxiv.org/abs/2508.06614", "authors": ["Fangjun Hu", "Guangkuo Liu", "Yifan Zhang", "Xun Gao"], "title": "Local Diffusion Models and Phases of Data Distributions", "categories": ["cs.LG", "cond-mat.stat-mech", "quant-ph"], "comment": "8+22 pages, 4+3 figures", "summary": "As a class of generative artificial intelligence frameworks inspired by\nstatistical physics, diffusion models have shown extraordinary performance in\nsynthesizing complicated data distributions through a denoising process\ngradually guided by score functions. Real-life data, like images, is often\nspatially structured in low-dimensional spaces. However, ordinary diffusion\nmodels ignore this local structure and learn spatially global score functions,\nwhich are often computationally expensive. In this work, we introduce a new\nperspective on the phases of data distributions, which provides insight into\nconstructing local denoisers with reduced computational costs. We define two\ndistributions as belonging to the same data distribution phase if they can be\nmutually connected via spatially local operations such as local denoisers.\nThen, we show that the reverse denoising process consists of an early trivial\nphase and a late data phase, sandwiching a rapid phase transition where local\ndenoisers must fail. To diagnose such phase transitions, we prove an\ninformation-theoretic bound on the fidelity of local denoisers based on\nconditional mutual information, and conduct numerical experiments in a\nreal-world dataset. This work suggests simpler and more efficient architectures\nof diffusion models: far from the phase transition point, we can use small\nlocal neural networks to compute the score function; global neural networks are\nonly necessary around the narrow time interval of phase transitions. This\nresult also opens up new directions for studying phases of data distributions,\nthe broader science of generative artificial intelligence, and guiding the\ndesign of neural networks inspired by physics concepts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6570\u636e\u5206\u5e03\u76f8\u4f4d\u7684\u6982\u5ff5\uff0c\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5b58\u5728\u7684\u76f8\u53d8\u73b0\u8c61\uff0c\u5e76\u636e\u6b64\u5efa\u8bae\u5728\u4e0d\u540c\u9636\u6bb5\u4f7f\u7528\u5c40\u90e8\u6216\u5168\u5c40\u795e\u7ecf\u7f51\u7edc\u6765\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff0c\u4ece\u800c\u964d\u4f4e\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u666e\u901a\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u5177\u6709\u5c40\u90e8\u7a7a\u95f4\u7ed3\u6784\u7684\u6570\u636e\u65f6\uff0c\u5b66\u4e60\u5168\u5c40\u5206\u6570\u51fd\u6570\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u5f15\u5165\u201c\u6570\u636e\u5206\u5e03\u76f8\u4f4d\u201d\u65b0\u89c6\u89d2\uff0c\u5b9a\u4e49\u901a\u8fc7\u5c40\u90e8\u64cd\u4f5c\u53ef\u76f8\u4e92\u8fde\u63a5\u7684\u5206\u5e03\u5c5e\u4e8e\u540c\u4e00\u76f8\u4f4d\u3002\u8bc1\u660e\u4e86\u9006\u5411\u53bb\u566a\u8fc7\u7a0b\u5305\u542b\u65e9\u671f\u5e73\u51e1\u9636\u6bb5\u3001\u665a\u671f\u6570\u636e\u9636\u6bb5\u4ee5\u53ca\u4e2d\u95f4\u7684\u5feb\u901f\u76f8\u53d8\u9636\u6bb5\uff08\u6b64\u65f6\u5c40\u90e8\u53bb\u566a\u5668\u5931\u6548\uff09\u3002\u901a\u8fc7\u57fa\u4e8e\u6761\u4ef6\u4e92\u4fe1\u606f\u7684\u4fe1\u606f\u8bba\u754c\u9650\u548c\u6570\u503c\u5b9e\u9a8c\u6765\u8bca\u65ad\u8fd9\u4e9b\u76f8\u53d8\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u53bb\u566a\u8fc7\u7a0b\u5b58\u5728\u76f8\u53d8\u70b9\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6269\u6563\u6a21\u578b\u67b6\u6784\uff1a\u5728\u8fdc\u79bb\u76f8\u53d8\u70b9\u65f6\uff0c\u53ef\u4f7f\u7528\u5c0f\u578b\u5c40\u90e8\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u5206\u6570\u51fd\u6570\uff1b\u4ec5\u5728\u76f8\u53d8\u7a84\u65f6\u95f4\u95f4\u9694\u5185\u9700\u8981\u5168\u5c40\u795e\u7ecf\u7f51\u7edc\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6570\u636e\u5206\u5e03\u76f8\u4f4d\u3001\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4ee5\u53ca\u53d7\u7269\u7406\u6982\u5ff5\u542f\u53d1\u8bbe\u8ba1\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.06736", "pdf": "https://arxiv.org/pdf/2508.06736", "abs": "https://arxiv.org/abs/2508.06736", "authors": ["Alican Yilmaz", "Junyang Cai", "Serdar Kadioglu", "Bistra Dilkina"], "title": "ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Solving Mixed-Integer Programming (MIP) problems often requires substantial\ncomputational resources due to their combinatorial nature. Parallelization has\nemerged as a critical strategy to accelerate solution times and enhance\nscalability to tackle large, complex instances. This paper investigates the\nparallelization capabilities of Balans, a recently proposed multi-armed\nbandits-based adaptive large neighborhood search for MIPs. While Balans's\nmodular architecture inherently supports parallel exploration of diverse\nparameter configurations, this potential has not been thoroughly examined. To\naddress this gap, we introduce ParBalans, an extension that leverages both\nsolver-level and algorithmic-level parallelism to improve performance on\nchallenging MIP instances. Our experimental results demonstrate that ParBalans\nexhibits competitive performance compared to the state-of-the-art commercial\nsolver Gurobi, particularly on hard optimization benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faParBalans\uff0c\u4e00\u4e2a\u5229\u7528\u6c42\u89e3\u5668\u548c\u7b97\u6cd5\u5c42\u9762\u5e76\u884c\u5316\u7684Balans\u6269\u5c55\uff0c\u65e8\u5728\u52a0\u901f\u6c42\u89e3\u6df7\u5408\u6574\u6570\u89c4\u5212(MIP)\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4e0e\u5546\u4e1a\u6c42\u89e3\u5668Gurobi\u7ade\u4e89\u3002", "motivation": "\u6df7\u5408\u6574\u6570\u89c4\u5212(MIP)\u95ee\u9898\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u5927\uff0c\u5e76\u884c\u5316\u662f\u52a0\u901f\u6c42\u89e3\u548c\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u7684\u5173\u952e\u7b56\u7565\u3002\u5df2\u6709\u7684Balans\u6c42\u89e3\u5668\u867d\u5177\u6709\u6a21\u5757\u5316\u67b6\u6784\uff0c\u7406\u8bba\u4e0a\u652f\u6301\u5e76\u884c\u63a2\u7d22\uff0c\u4f46\u5176\u5e76\u884c\u5316\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u548c\u5229\u7528\u3002", "method": "\u5f15\u5165ParBalans\uff0c\u5b83\u662fBalans\u7684\u6269\u5c55\uff0c\u901a\u8fc7\u5229\u7528\u6c42\u89e3\u5668\u5c42\u9762\uff08solver-level\uff09\u548c\u7b97\u6cd5\u5c42\u9762\uff08algorithmic-level\uff09\u7684\u53cc\u91cd\u5e76\u884c\u5316\u6765\u63d0\u5347\u5728\u590d\u6742MIP\u5b9e\u4f8b\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cParBalans\u5728\u89e3\u51b3\u6311\u6218\u6027MIP\u5b9e\u4f8b\u65f6\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u96be\u5ea6\u4f18\u5316\u57fa\u51c6\u4e0a\uff0c\u5176\u8868\u73b0\u53ef\u4e0e\u6700\u5148\u8fdb\u7684\u5546\u4e1a\u6c42\u89e3\u5668Gurobi\u76f8\u5ab2\u7f8e\u3002", "conclusion": "ParBalans\u6210\u529f\u5730\u5c06\u5e76\u884c\u5316\u80fd\u529b\u878d\u5165Balans\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5176MIP\u6c42\u89e3\u6548\u7387\u548c\u5e94\u5bf9\u590d\u6742\u95ee\u9898\u7684\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0e\u9876\u7ea7\u5546\u4e1a\u6c42\u89e3\u5668\u76f8\u5f53\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.06535", "pdf": "https://arxiv.org/pdf/2508.06535", "abs": "https://arxiv.org/abs/2508.06535", "authors": ["Faisal Ahmed"], "title": "Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification", "categories": ["eess.IV", "cs.CV", "cs.LG", "F.2.2; I.2.7"], "comment": "8 pages, 1 figure", "summary": "Accurate classification of Acute Lymphoblastic Leukemia (ALL) from peripheral\nblood smear images is essential for early diagnosis and effective treatment\nplanning. This study investigates the use of transfer learning with pretrained\nconvolutional neural networks (CNNs) to improve diagnostic performance. To\naddress the class imbalance in the dataset of 3,631 Hematologic and 7,644 ALL\nimages, we applied extensive data augmentation techniques to create a balanced\ntraining set of 10,000 images per class. We evaluated several models, including\nResNet50, ResNet101, and EfficientNet variants B0, B1, and B3. EfficientNet-B3\nachieved the best results, with an F1-score of 94.30%, accuracy of 92.02%,\nandAUCof94.79%,outperformingpreviouslyreported methods in the C-NMCChallenge.\nThesefindings demonstrate the effectiveness of combining data augmentation with\nadvanced transfer learning models, particularly EfficientNet-B3, in developing\naccurate and robust diagnostic tools for hematologic malignancy detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3CNN\u6a21\u578b\uff08\u7279\u522b\u662fEfficientNet-B3\uff09\u5b9e\u73b0\u4e86\u5bf9\u6025\u6027\u6dcb\u5df4\u7ec6\u80de\u767d\u8840\u75c5\u8840\u6d82\u7247\u56fe\u50cf\u7684\u9ad8\u7cbe\u5ea6\u5206\u7c7b\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4ece\u5916\u5468\u8840\u6d82\u7247\u56fe\u50cf\u4e2d\u51c6\u786e\u5206\u7c7b\u6025\u6027\u6dcb\u5df4\u7ec6\u80de\u767d\u8840\u75c5\uff08ALL\uff09\u5bf9\u4e8e\u65e9\u671f\u8bca\u65ad\u548c\u6709\u6548\u6cbb\u7597\u8ba1\u5212\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u91c7\u7528\u9884\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u5e76\u5bf9\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u8fdb\u884c\u5e7f\u6cdb\u6570\u636e\u589e\u5f3a\u4ee5\u521b\u5efa\u5e73\u8861\u8bad\u7ec3\u96c6\u3002\u8bc4\u4f30\u4e86\u5305\u62ecResNet50\u3001ResNet101\u548cEfficientNet\u53d8\u4f53\uff08B0\u3001B1\u3001B3\uff09\u5728\u5185\u7684\u591a\u4e2a\u6a21\u578b\u3002", "result": "EfficientNet-B3\u8868\u73b0\u6700\u4f73\uff0cF1-score\u8fbe\u523094.30%\uff0c\u51c6\u786e\u7387\u4e3a92.02%\uff0cAUC\u4e3a94.79%\uff0c\u4f18\u4e8eC-NMC\u6311\u6218\u8d5b\u4e2d\u5148\u524d\u62a5\u544a\u7684\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548c\u5148\u8fdb\u7684\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662fEfficientNet-B3\uff09\uff0c\u80fd\u6709\u6548\u5f00\u53d1\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u8840\u6db2\u6076\u6027\u80bf\u7624\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2508.06709", "pdf": "https://arxiv.org/pdf/2508.06709", "abs": "https://arxiv.org/abs/2508.06709", "authors": ["Evangelia Spiliopoulou", "Riccardo Fogliato", "Hanna Burnsky", "Tamer Soliman", "Jie Ma", "Graham Horwood", "Miguel Ballesteros"], "title": "Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) can serve as judges that offer rapid and\nreliable assessments of other LLM outputs. However, models may systematically\nassign overly favorable ratings to their own outputs, a phenomenon known as\nself-bias, which can distort evaluations of true model performance. Previous\nstudies often conflate genuine differences in model quality with bias or\nincorrectly assume that evaluations from LLMs and humans follow the same rating\ndistributions. In this work, we present a statistical framework that explicitly\nformalizes assumptions under which self-bias can be identified and estimated.\nOur method models the difference in the scoring distribution that\nLLM-as-a-judge assigns to its own completions compared to other models, while\naccounting for the underlying quality of the completions provided by an\nindependent, third-party judge (e.g., humans). Our method reliably isolates and\nquantifies self-bias, even when models vary in ability, ensuring that genuine\nperformance differences are not mistaken for self-bias. We conduct an empirical\nanalysis of self-bias on a large dataset (>5000 prompt-completion pairs)\nconsisting of expert human annotations and judgments from nine different LLM\njudges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,\nsystematically assign higher scores to their own outputs. These models also\ndisplay family-bias; systematically assigning higher ratings to outputs\nproduced by other models of the same family. Our findings highlight potential\npitfalls of using LLM judges and offer practical guidance to mitigate biases\nwhen interpreting automated evaluations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u7edf\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u91cf\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u8bc4\u5224\u8005\u65f6\u7684\u201c\u81ea\u504f\u89c1\u201d\u548c\u201c\u5bb6\u65cf\u504f\u89c1\u201d\uff0c\u5b9e\u8bc1\u53d1\u73b0GPT-4o\u548cClaude 3.5 Sonnet\u7b49\u6a21\u578b\u5b58\u5728\u8fd9\u4e9b\u504f\u89c1\uff0c\u5e76\u63d0\u4f9b\u4e86\u7f13\u89e3\u504f\u89c1\u7684\u5efa\u8bae\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u7528\u4f5c\u8bc4\u5224\u8005\uff0c\u4f46\u5b83\u4eec\u53ef\u80fd\u5bf9\u5176\u81ea\u8eab\u8f93\u51fa\u7ed9\u4e88\u8fc7\u9ad8\u8bc4\u4ef7\uff08\u81ea\u504f\u89c1\uff09\uff0c\u4ece\u800c\u626d\u66f2\u771f\u5b9e\u7684\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u3002\u4ee5\u5f80\u7814\u7a76\u5e38\u6df7\u6dc6\u6a21\u578b\u8d28\u91cf\u5dee\u5f02\u4e0e\u504f\u89c1\uff0c\u6216\u9519\u8bef\u5047\u8bbeLLM\u548c\u4eba\u7c7b\u8bc4\u5224\u5206\u5e03\u76f8\u540c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u660e\u786e\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u548c\u91cf\u5316\u8fd9\u79cd\u81ea\u504f\u89c1\uff0c\u4ee5\u83b7\u5f97\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u8ba1\u6846\u67b6\uff0c\u660e\u786e\u5f62\u5f0f\u5316\u4e86\u8bc6\u522b\u548c\u4f30\u8ba1\u81ea\u504f\u89c1\u7684\u5047\u8bbe\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5efa\u6a21LLM\u8bc4\u5224\u5668\u5bf9\u5176\u81ea\u8eab\u5b8c\u6210\u5ea6\u8bc4\u5206\u5206\u5e03\u4e0e\u5bf9\u5176\u4ed6\u6a21\u578b\u5b8c\u6210\u5ea6\u8bc4\u5206\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u91cf\u5316\u504f\u89c1\uff0c\u540c\u65f6\u4f1a\u8003\u8651\u72ec\u7acb\u7b2c\u4e09\u65b9\uff08\u5982\u4eba\u7c7b\uff09\u8bc4\u5224\u63d0\u4f9b\u7684\u5e95\u5c42\u771f\u5b9e\u5b8c\u6210\u5ea6\u8d28\u91cf\u3002\u7814\u7a76\u5728\u4e00\u4e2a\u5305\u542b\u8d85\u8fc75000\u4e2a\u63d0\u793a-\u5b8c\u6210\u5bf9\u7684\u5927\u578b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\uff0c\u8be5\u6570\u636e\u96c6\u7ed3\u5408\u4e86\u4eba\u7c7b\u4e13\u5bb6\u6807\u6ce8\u548c\u6765\u81ea\u4e5d\u4e2a\u4e0d\u540cLLM\u8bc4\u5224\u5668\u7684\u5224\u65ad\u3002", "result": "\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0\uff0c\u67d0\u4e9b\u6a21\u578b\uff08\u5982GPT-4o\u548cClaude 3.5 Sonnet\uff09\u7cfb\u7edf\u6027\u5730\u7ed9\u81ea\u5df1\u7684\u8f93\u51fa\u5206\u914d\u66f4\u9ad8\u7684\u5206\u6570\uff0c\u8868\u73b0\u51fa\u81ea\u504f\u89c1\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u6a21\u578b\u8fd8\u8868\u73b0\u51fa\u201c\u5bb6\u65cf\u504f\u89c1\u201d\uff0c\u5373\u7cfb\u7edf\u6027\u5730\u7ed9\u540c\u4e00\u5bb6\u65cf\u5176\u4ed6\u6a21\u578b\u4ea7\u751f\u7684\u8f93\u51fa\u5206\u914d\u66f4\u9ad8\u7684\u8bc4\u5206\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u5668\u65f6\u6f5c\u5728\u7684\u9677\u9631\uff0c\u5e76\u4e3a\u5728\u89e3\u91ca\u81ea\u52a8\u5316\u8bc4\u4f30\u65f6\u51cf\u8f7b\u504f\u89c1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2508.07679", "pdf": "https://arxiv.org/pdf/2508.07679", "abs": "https://arxiv.org/abs/2508.07679", "authors": ["Tong Zhang", "Yu Gou", "Jun Liu", "Shanshan Song", "Tingting Yang", "Jun-Hong Cui"], "title": "Joint link scheduling and power allocation in imperfect and energy-constrained underwater wireless sensor networks", "categories": ["cs.NI"], "comment": "Accepted by IEEE Transactions on Mobile Computing", "summary": "Underwater wireless sensor networks (UWSNs) stand as promising technologies\nfacilitating diverse underwater applications. However, the major design issues\nof the considered system are the severely limited energy supply and unexpected\nnode malfunctions. This paper aims to provide fair, efficient, and reliable\n(FER) communication to the imperfect and energy-constrained UWSNs (IC-UWSNs).\nTherefore, we formulate a FER-communication optimization problem (FERCOP) and\npropose ICRL-JSA to solve the formulated problem. ICRL-JSA is a deep\nmulti-agent reinforcement learning (MARL)-based optimizer for IC-UWSNs through\njoint link scheduling and power allocation, which automatically learns\nscheduling algorithms without human intervention. However, conventional RL\nmethods are unable to address the challenges posed by underwater environments\nand IC-UWSNs. To construct ICRL-JSA, we integrate deep Q-network into IC-UWSNs\nand propose an advanced training mechanism to deal with complex acoustic\nchannels, limited energy supplies, and unexpected node malfunctions. Simulation\nresults demonstrate the superiority of the proposed ICRL-JSA scheme with an\nadvanced training mechanism compared to various benchmark algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faICRL-JSA\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u94fe\u8def\u8c03\u5ea6\u548c\u529f\u7387\u5206\u914d\uff0c\u89e3\u51b3\u6c34\u4e0b\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\uff08UWSNs\uff09\u4e2d\u80fd\u91cf\u53d7\u9650\u548c\u8282\u70b9\u6545\u969c\u5bfc\u81f4\u7684\u516c\u5e73\u3001\u9ad8\u6548\u3001\u53ef\u9760\u901a\u4fe1\u95ee\u9898\u3002", "motivation": "\u6c34\u4e0b\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\uff08UWSNs\uff09\u9762\u4e34\u4e25\u91cd\u7684\u80fd\u91cf\u4f9b\u5e94\u9650\u5236\u548c\u610f\u60f3\u4e0d\u5230\u7684\u8282\u70b9\u6545\u969c\uff0c\u5bfc\u81f4\u901a\u4fe1\u96be\u4ee5\u5b9e\u73b0\u516c\u5e73\u3001\u9ad8\u6548\u548c\u53ef\u9760\uff08FER\uff09\u3002", "method": "\u672c\u6587\u5c06FER\u901a\u4fe1\u95ee\u9898\u5efa\u6a21\u4e3aFERCOP\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51faICRL-JSA\u6765\u89e3\u51b3\u3002ICRL-JSA\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u8054\u5408\u94fe\u8def\u8c03\u5ea6\u548c\u529f\u7387\u5206\u914d\u5b9e\u73b0\u81ea\u52a8\u5316\u5b66\u4e60\u3002\u5b83\u5c06\u6df1\u5ea6Q\u7f51\u7edc\u878d\u5165\u5230\u6c34\u4e0b\u73af\u5883\u4e2d\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u5148\u8fdb\u7684\u8bad\u7ec3\u673a\u5236\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u58f0\u5b66\u4fe1\u9053\u3001\u6709\u9650\u80fd\u91cf\u4f9b\u5e94\u548c\u8282\u70b9\u6545\u969c\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684ICRL-JSA\u65b9\u6848\u53ca\u5176\u5148\u8fdb\u7684\u8bad\u7ec3\u673a\u5236\uff0c\u76f8\u6bd4\u4e8e\u5404\u79cd\u57fa\u51c6\u7b97\u6cd5\uff0c\u5728\u6027\u80fd\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "ICRL-JSA\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u80fd\u91cf\u53d7\u9650\u4e14\u5b58\u5728\u8282\u70b9\u6545\u969c\u7684\u6c34\u4e0b\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u7684\u516c\u5e73\u3001\u9ad8\u6548\u548c\u53ef\u9760\u901a\u4fe1\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.06617", "pdf": "https://arxiv.org/pdf/2508.06617", "abs": "https://arxiv.org/abs/2508.06617", "authors": ["Md Arafat Hossain", "Xingfu Wu", "Valerie Taylor", "Ali Jannesari"], "title": "Generalizing Scaling Laws for Dense and Sparse Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": "8 pages, 8 figures", "summary": "Over the past few years, the size of language models has grown exponentially,\nas has the computational cost to train these large models. This rapid growth\nhas motivated researchers to develop new techniques aimed at enhancing the\nefficiency of the training process. Despite these advancements, optimally\npredicting the model size or allocating optimal resources remains a challenge.\nSeveral efforts have addressed the challenge by proposing different scaling\nlaws, but almost all of them are architecture-specific (dense or sparse). In\nthis work we revisit existing scaling laws and propose a generalized scaling\nlaw to provide a unified framework that is applicable to both dense and sparse\nlarge language models. We evaluate and compare our proposed scaling law with\nexisting scaling laws to demonstrate its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u8bed\u8a00\u6a21\u578b\u7f29\u653e\u5b9a\u5f8b\uff0c\u9002\u7528\u4e8e\u7a20\u5bc6\u548c\u7a00\u758f\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u7279\u5b9a\u67b6\u6784\u7f29\u653e\u5b9a\u5f8b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u6210\u672c\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u73b0\u6709\u7f29\u653e\u5b9a\u5f8b\u591a\u4e3a\u7279\u5b9a\u67b6\u6784\uff08\u7a20\u5bc6\u6216\u7a00\u758f\uff09\uff0c\u96be\u4ee5\u7edf\u4e00\u9884\u6d4b\u6a21\u578b\u5927\u5c0f\u548c\u4f18\u5316\u8d44\u6e90\u5206\u914d\u3002", "method": "\u91cd\u65b0\u5ba1\u89c6\u73b0\u6709\u7f29\u653e\u5b9a\u5f8b\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u5e7f\u4e49\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u4ee5\u63d0\u4f9b\u9002\u7528\u4e8e\u7a20\u5bc6\u548c\u7a00\u758f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u5bf9\u6240\u63d0\u51fa\u7684\u5e7f\u4e49\u7f29\u653e\u5b9a\u5f8b\u4e0e\u73b0\u6709\u7f29\u653e\u5b9a\u5f8b\u8fdb\u884c\u4e86\u8bc4\u4f30\u548c\u6bd4\u8f83\uff0c\u4ee5\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u5e7f\u4e49\u7f29\u653e\u5b9a\u5f8b\uff0c\u4e3a\u7a20\u5bc6\u548c\u7a00\u758f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u9884\u6d4b\u548c\u8d44\u6e90\u5206\u914d\u6846\u67b6\uff0c\u6709\u6548\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u67b6\u6784\u7279\u5f02\u6027\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.06746", "pdf": "https://arxiv.org/pdf/2508.06746", "abs": "https://arxiv.org/abs/2508.06746", "authors": ["Xin Tang", "Qian Chen", "Fengshun Li", "Youchun Gong", "Yinqiu Liu", "Wen Tian", "Shaowen Qin", "Xiaohuan Li"], "title": "Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism", "categories": ["cs.AI"], "comment": null, "summary": "With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in\nsensitive applications, such as urban monitoring, emergency response, and\nsecure sensing, ensuring reliable connectivity and covert communication has\nbecome increasingly vital. However, dynamic mobility and exposure risks pose\nsignificant challenges. To tackle these challenges, this paper proposes a\nself-organizing UAV network framework combining Graph Diffusion-based Policy\nOptimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The\nGDPO method uses generative AI to dynamically generate sparse but\nwell-connected topologies, enabling flexible adaptation to changing node\ndistributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game\n(SG)-based incentive mechanism guides self-interested UAVs to choose relay\nbehaviors and neighbor links that support cooperation and enhance covert\ncommunication. Extensive experiments are conducted to validate the\neffectiveness of the proposed framework in terms of model convergence, topology\ngeneration quality, and enhancement of covert communication performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408GDPO\u548cStackelberg\u535a\u5f08\u7684\u81ea\u7ec4\u7ec7\u65e0\u4eba\u673a\u7f51\u7edc\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u654f\u611f\u5e94\u7528\u4e2d\u65e0\u4eba\u673a\u7f51\u7edc\u9762\u4e34\u7684\u52a8\u6001\u79fb\u52a8\u6027\u548c\u66b4\u9732\u98ce\u9669\uff0c\u4ee5\u5b9e\u73b0\u53ef\u9760\u8fde\u63a5\u548c\u9690\u853d\u901a\u4fe1\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u7f51\u7edc\u5728\u654f\u611f\u5e94\u7528\uff08\u5982\u57ce\u5e02\u76d1\u63a7\u3001\u5e94\u6025\u54cd\u5e94\u3001\u5b89\u5168\u611f\u77e5\uff09\u4e2d\u7684\u9700\u6c42\u589e\u957f\uff0c\u786e\u4fdd\u53ef\u9760\u8fde\u63a5\u548c\u9690\u853d\u901a\u4fe1\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u52a8\u6001\u79fb\u52a8\u6027\u548c\u66b4\u9732\u98ce\u9669\u5e26\u6765\u4e86\u4e25\u5cfb\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u81ea\u7ec4\u7ec7\u65e0\u4eba\u673a\u7f51\u7edc\u6846\u67b6\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u56fe\u6269\u6563\u7684\u7b56\u7565\u4f18\u5316\uff08GDPO\uff09\u548c\u57fa\u4e8eStackelberg\u535a\u5f08\uff08SG\uff09\u7684\u6fc0\u52b1\u673a\u5236\u3002GDPO\u5229\u7528\u751f\u6210\u5f0fAI\u52a8\u6001\u751f\u6210\u7a00\u758f\u4f46\u8fde\u63a5\u826f\u597d\u7684\u62d3\u6251\u7ed3\u6784\uff0c\u4ee5\u9002\u5e94\u8282\u70b9\u5206\u5e03\u548c\u5730\u9762\u7528\u6237\uff08GU\uff09\u9700\u6c42\uff1bSG\u6fc0\u52b1\u673a\u5236\u5219\u5f15\u5bfc\u81ea\u5229\u65e0\u4eba\u673a\u9009\u62e9\u652f\u6301\u5408\u4f5c\u5e76\u589e\u5f3a\u9690\u853d\u901a\u4fe1\u7684\u4e2d\u7ee7\u884c\u4e3a\u548c\u90bb\u5c45\u94fe\u8def\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u5728\u6a21\u578b\u6536\u655b\u6027\u3001\u62d3\u6251\u751f\u6210\u8d28\u91cf\u548c\u9690\u853d\u901a\u4fe1\u6027\u80fd\u589e\u5f3a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u65e0\u4eba\u673a\u7f51\u7edc\u5728\u52a8\u6001\u4e14\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u53ef\u9760\u8fde\u63a5\u548c\u9690\u853d\u901a\u4fe1\u80fd\u529b\u3002"}}
{"id": "2508.06537", "pdf": "https://arxiv.org/pdf/2508.06537", "abs": "https://arxiv.org/abs/2508.06537", "authors": ["Shantanusinh Parmar"], "title": "Benchmarking Deep Learning-Based Object Detection Models on Feature Deficient Astrophotography Imagery Dataset", "categories": ["cs.CV", "astro-ph.IM"], "comment": null, "summary": "Object detection models are typically trained on datasets like ImageNet,\nCOCO, and PASCAL VOC, which focus on everyday objects. However, these lack\nsignal sparsity found in non-commercial domains. MobilTelesco, a\nsmartphone-based astrophotography dataset, addresses this by providing sparse\nnight-sky images. We benchmark several detection models on it, highlighting\nchallenges under feature-deficient conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMobilTelesco\u7684\u667a\u80fd\u624b\u673a\u5929\u6587\u6444\u5f71\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u76ee\u6807\u68c0\u6d4b\u6570\u636e\u96c6\u7f3a\u4e4f\u4fe1\u53f7\u7a00\u758f\u6027\u7684\u95ee\u9898\uff0c\u5e76\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u591a\u79cd\u68c0\u6d4b\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5728\u7279\u5f81\u4e0d\u8db3\u6761\u4ef6\u4e0b\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u65e5\u5e38\u7269\u4f53\u6570\u636e\u96c6\uff08\u5982ImageNet, COCO, PASCAL VOC\uff09\u8bad\u7ec3\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u7f3a\u4e4f\u975e\u5546\u4e1a\u9886\u57df\uff08\u5982\u5929\u6587\u6444\u5f71\uff09\u4e2d\u5b58\u5728\u7684\u4fe1\u53f7\u7a00\u758f\u6027\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684\u5929\u6587\u6444\u5f71\u6570\u636e\u96c6MobilTelesco\uff0c\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u7a00\u758f\u7684\u591c\u7a7a\u56fe\u50cf\u3002\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5bf9\u591a\u4e2a\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u63ed\u793a\u4e86\u5728\u7279\u5f81\u7a00\u758f\uff08\u4fe1\u53f7\u4e0d\u8db3\uff09\u6761\u4ef6\u4e0b\uff0c\u73b0\u6709\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u6240\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "\u5f53\u524d\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u5904\u7406\u5177\u6709\u4fe1\u53f7\u7a00\u758f\u6027\u7684\u975e\u5546\u4e1a\u9886\u57df\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0cMobilTelesco\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u51f8\u663e\u8fd9\u4e9b\u6311\u6218\u5e76\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2508.06729", "pdf": "https://arxiv.org/pdf/2508.06729", "abs": "https://arxiv.org/abs/2508.06729", "authors": ["Komala Subramanyam Cherukuri", "Pranav Abishai Moses", "Aisa Sakata", "Jiangping Chen", "Haihua Chen"], "title": "Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Oral histories are vital records of lived experience, particularly within\ncommunities affected by systemic injustice and historical erasure. Effective\nand efficient analysis of their oral history archives can promote access and\nunderstanding of the oral histories. However, Large-scale analysis of these\narchives remains limited due to their unstructured format, emotional\ncomplexity, and high annotation costs. This paper presents a scalable framework\nto automate semantic and sentiment annotation for Japanese American\nIncarceration Oral History. Using LLMs, we construct a high-quality dataset,\nevaluate multiple models, and test prompt engineering strategies in\nhistorically sensitive contexts. Our multiphase approach combines expert\nannotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We\nlabeled 558 sentences from 15 narrators for sentiment and semantic\nclassification, then evaluated zero-shot, few-shot, and RAG strategies. For\nsemantic classification, ChatGPT achieved the highest F1 score (88.71%),\nfollowed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama\nslightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models\nshowing comparable results. The best prompt configurations were used to\nannotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our\nfindings show that LLMs can effectively perform semantic and sentiment\nannotation across large oral history collections when guided by well-designed\nprompts. This study provides a reusable annotation pipeline and practical\nguidance for applying LLMs in culturally sensitive archival analysis. By\nbridging archival ethics with scalable NLP techniques, this work lays the\ngroundwork for responsible use of artificial intelligence in digital humanities\nand preservation of collective memory. GitHub:\nhttps://github.com/kc6699c/LLM4OralHistoryAnalysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u6807\u6ce8\u5927\u89c4\u6a21\u53e3\u8ff0\u5386\u53f2\u6863\u6848\uff08\u7279\u6307\u65e5\u672c\u88d4\u7f8e\u56fd\u4eba\u56da\u7981\u53e3\u8ff0\u5386\u53f2\uff09\u4e2d\u7684\u8bed\u4e49\u548c\u60c5\u611f\u4fe1\u606f\u3002", "motivation": "\u53e3\u8ff0\u5386\u53f2\u662f\u91cd\u8981\u4e14\u5b9d\u8d35\u7684\u8bb0\u5f55\uff0c\u4f46\u7531\u4e8e\u5176\u975e\u7ed3\u6784\u5316\u683c\u5f0f\u3001\u60c5\u611f\u590d\u6742\u6027\u548c\u9ad8\u6602\u7684\u6807\u6ce8\u6210\u672c\uff0c\u5927\u89c4\u6a21\u5206\u6790\u9762\u4e34\u6311\u6218\uff0c\u8fd9\u9650\u5236\u4e86\u5bf9\u5176\u5185\u5bb9\u7684\u6709\u6548\u8bbf\u95ee\u548c\u7406\u89e3\u3002", "method": "\u7814\u7a76\u91c7\u7528\u591a\u9636\u6bb5\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e13\u5bb6\u6807\u6ce8\u3001\u63d0\u793a\u5de5\u7a0b\u8bbe\u8ba1\u548cLLM\uff08ChatGPT\u3001Llama\u3001Qwen\uff09\u8bc4\u4f30\u3002\u9996\u5148\uff0c\u6807\u6ce8\u4e86558\u53e5\u8bdd\u7528\u4e8e\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548cRAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u7b56\u7565\u3002\u968f\u540e\uff0c\u5229\u7528\u6700\u4f73\u63d0\u793a\u914d\u7f6e\uff0c\u5bf9\u6765\u81ea1002\u6b21\u8bbf\u8c08\u768492,191\u53e5\u8bdd\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u6807\u6ce8\u3002", "result": "\u5728\u8bed\u4e49\u5206\u7c7b\u4e2d\uff0cChatGPT\u8868\u73b0\u6700\u4f73\uff08F1\u5206\u657088.71%\uff09\uff0cLlama\u548cQwen\u7d27\u968f\u5176\u540e\u3002\u5728\u60c5\u611f\u5206\u6790\u4e2d\uff0cLlama\u7565\u4f18\u4e8eQwen\u548cChatGPT\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5747\u8868\u73b0\u51fa\u53ef\u6bd4\u8f83\u7684\u7ed3\u679c\u3002\u7814\u7a76\u8868\u660e\uff0c\u5728\u826f\u597d\u8bbe\u8ba1\u7684\u63d0\u793a\u5f15\u5bfc\u4e0b\uff0cLLMs\u80fd\u6709\u6548\u5730\u5bf9\u5927\u89c4\u6a21\u53e3\u8ff0\u5386\u53f2\u96c6\u5408\u8fdb\u884c\u8bed\u4e49\u548c\u60c5\u611f\u6807\u6ce8\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u91cd\u7528\u7684\u6807\u6ce8\u6d41\u7a0b\u548c\u5b9e\u9645\u6307\u5bfc\uff0c\u7528\u4e8e\u5728\u6587\u5316\u654f\u611f\u7684\u6863\u6848\u5206\u6790\u4e2d\u5e94\u7528LLMs\u3002\u901a\u8fc7\u5c06\u6863\u6848\u4f26\u7406\u4e0e\u53ef\u6269\u5c55\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u4e3a\u6570\u5b57\u4eba\u6587\u548c\u96c6\u4f53\u8bb0\u5fc6\u4fdd\u5b58\u4e2d\u8d1f\u8d23\u4efb\u5730\u4f7f\u7528\u4eba\u5de5\u667a\u80fd\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.07778", "pdf": "https://arxiv.org/pdf/2508.07778", "abs": "https://arxiv.org/abs/2508.07778", "authors": ["Farhad Rezazadeh", "Raymond Zhao", "Jiongyu Dai", "Amir Ashtari Gargari", "Hatim Chergui", "Lingjia Liu"], "title": "An Experimental Reservoir-Augmented Foundation Model: 6G O-RAN Case Study", "categories": ["cs.NI"], "comment": "5 pages, 2 figures", "summary": "Next-generation open radio access networks (O-RAN) continuously stream tens\nof key performance indicators (KPIs) together with raw in-phase/quadrature (IQ)\nsamples, yielding ultra-high-dimensional, non-stationary time series that\noverwhelm conventional transformer architectures. We introduce a\nreservoir-augmented masked autoencoding transformer (RA-MAT). This time series\nfoundation model employs echo state network (ESN) computing with masked\nautoencoding to satisfy the stringent latency, energy efficiency, and\nscalability requirements of 6G O-RAN testing. A fixed, randomly initialized ESN\nrapidly projects each temporal patch into a rich dynamical embedding without\nbackpropagation through time, converting the quadratic self-attention\nbottleneck into a lightweight linear operation. These embeddings drive a\npatch-wise masked autoencoder that reconstructs 30% randomly masked patches,\ncompelling the encoder to capture both local dynamics and long-range structure\nfrom unlabeled data. After self-supervised pre-training, RA-MAT is fine-tuned\nwith a shallow task head while keeping the reservoir and most transformer\nlayers frozen, enabling low-footprint adaptation to diverse downstream tasks\nsuch as O-RAN KPI forecasting. In a comprehensive O-RAN KPI case study, RA-MAT\nachieved sub-0.06 mean squared error (MSE) on several continuous and discrete\nKPIs. This work positions RA-MAT as a practical pathway toward real-time,\nfoundation-level analytics in future 6G networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRA-MAT\u6a21\u578b\uff0c\u4e00\u79cd\u7ed3\u5408\u56de\u58f0\u72b6\u6001\u7f51\u7edc\uff08ESN\uff09\u548c\u63a9\u7801\u81ea\u7f16\u7801\u7684Transformer\uff0c\u65e8\u5728\u9ad8\u6548\u5904\u74066G O-RAN\u4ea7\u751f\u7684\u9ad8\u7ef4\u3001\u975e\u5e73\u7a33\u65f6\u5e8f\u6570\u636e\uff0c\u5e76\u5b9e\u73b0\u4f18\u5f02\u7684KPI\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u5f00\u653e\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff08O-RAN\uff09\u6301\u7eed\u4ea7\u751f\u8d85\u9ad8\u7ef4\u3001\u975e\u5e73\u7a33\u7684\u65f6\u5e8f\u6570\u636e\uff08\u5305\u62ecKPI\u548cIQ\u6837\u672c\uff09\uff0c\u4f20\u7edfTransformer\u67b6\u6784\u96be\u4ee5\u6709\u6548\u5904\u7406\uff0c\u9762\u4e34\u4e25\u82db\u7684\u5ef6\u8fdf\u3001\u80fd\u6548\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u540d\u4e3aRA-MAT\u7684\u50a8\u5c42\u589e\u5f3a\u63a9\u7801\u81ea\u7f16\u7801Transformer\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u56de\u58f0\u72b6\u6001\u7f51\u7edc\uff08ESN\uff09\u8ba1\u7b97\u548c\u63a9\u7801\u81ea\u7f16\u7801\uff0c\u901a\u8fc7\u56fa\u5b9a\u3001\u968f\u673a\u521d\u59cb\u5316\u7684ESN\u5c06\u65f6\u95f4\u7247\u5feb\u901f\u6295\u5f71\u4e3a\u52a8\u6001\u5d4c\u5165\uff0c\u5c06Transformer\u7684\u4e8c\u6b21\u81ea\u6ce8\u610f\u529b\u74f6\u9888\u8f6c\u5316\u4e3a\u8f7b\u91cf\u7ea7\u7ebf\u6027\u64cd\u4f5c\u3002\u8fd9\u4e9b\u5d4c\u5165\u9a71\u52a8\u4e00\u4e2a\u9010\u7247\u63a9\u7801\u81ea\u7f16\u7801\u5668\u91cd\u678430%\u7684\u968f\u673a\u63a9\u7801\u7247\uff0c\u4fc3\u4f7f\u7f16\u7801\u5668\u4ece\u65e0\u6807\u7b7e\u6570\u636e\u4e2d\u6355\u83b7\u5c40\u90e8\u52a8\u6001\u548c\u957f\u7a0b\u7ed3\u6784\u3002\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u540e\uff0cRA-MAT\u901a\u8fc7\u6d45\u5c42\u4efb\u52a1\u5934\u8fdb\u884c\u5fae\u8c03\uff0c\u540c\u65f6\u51bb\u7ed3\u50a8\u5c42\u548c\u5927\u90e8\u5206Transformer\u5c42\uff0c\u4ee5\u4f4e\u5f00\u9500\u9002\u5e94O-RAN KPI\u9884\u6d4b\u7b49\u591a\u6837\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "\u5728\u5168\u9762\u7684O-RAN KPI\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cRA-MAT\u5728\u591a\u4e2a\u8fde\u7eed\u548c\u79bb\u6563KPI\u4e0a\u5747\u5b9e\u73b0\u4e86\u4f4e\u4e8e0.06\u7684\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4f7fRA-MAT\u6210\u4e3a\u672a\u67656G\u7f51\u7edc\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u3001\u57fa\u7840\u7ea7\u5206\u6790\u7684\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2508.06622", "pdf": "https://arxiv.org/pdf/2508.06622", "abs": "https://arxiv.org/abs/2508.06622", "authors": ["Jeremiah Birrell", "Reza Ebrahimi"], "title": "Learning to Forget with Information Divergence Reweighted Objectives for Noisy Labels", "categories": ["cs.LG", "stat.ML"], "comment": "25 pages, 2 figures", "summary": "We introduce ANTIDOTE, a new class of objectives for learning under noisy\nlabels which are defined in terms of a relaxation over an\ninformation-divergence neighborhood. Using convex duality, we provide a\nreformulation as an adversarial training method that has similar computational\ncost to training with standard cross-entropy loss. We show that our approach\nadaptively reduces the influence of the samples with noisy labels during\nlearning, exhibiting a behavior that is analogous to forgetting those samples.\nANTIDOTE is effective in practical environments where label noise is inherent\nin the training data or where an adversary can alter the training labels.\nExtensive empirical evaluations on different levels of symmetric, asymmetric,\nhuman annotation, and real-world label noise show that ANTIDOTE outperforms\nleading comparable losses in the field and enjoys a time complexity that is\nvery close to that of the standard cross entropy loss.", "AI": {"tldr": "ANTIDOTE\u662f\u4e00\u79cd\u5904\u7406\u6807\u7b7e\u566a\u58f0\u7684\u65b0\u578b\u76ee\u6807\u51fd\u6570\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u5b9e\u73b0\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "motivation": "\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8bad\u7ec3\u6570\u636e\u4e2d\u666e\u904d\u5b58\u5728\u56fa\u6709\u6216\u5bf9\u6297\u6027\u5f15\u5165\u7684\u6807\u7b7e\u566a\u58f0\uff0c\u8fd9\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u5b66\u4e60\u6548\u679c\u3002", "method": "\u5f15\u5165ANTIDOTE\uff0c\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u6563\u5ea6\u90bb\u57df\u677e\u5f1b\u7684\u65b0\u578b\u76ee\u6807\u51fd\u6570\uff0c\u7528\u4e8e\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u3002\u901a\u8fc7\u51f8\u5bf9\u5076\u6027\uff0c\u5c06\u5176\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e00\u79cd\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "ANTIDOTE\u80fd\u81ea\u9002\u5e94\u5730\u964d\u4f4e\u566a\u58f0\u6807\u7b7e\u6837\u672c\u7684\u5f71\u54cd\uff0c\u8868\u73b0\u51fa\u201c\u9057\u5fd8\u201d\u884c\u4e3a\u3002\u5728\u591a\u79cd\u6807\u7b7e\u566a\u58f0\u7c7b\u578b\uff08\u5bf9\u79f0\u3001\u975e\u5bf9\u79f0\u3001\u4eba\u5de5\u6807\u6ce8\u3001\u771f\u5b9e\u4e16\u754c\uff09\u4e0b\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4e14\u65f6\u95f4\u590d\u6742\u5ea6\u63a5\u8fd1\u6807\u51c6\u4ea4\u53c9\u71b5\u635f\u5931\u3002", "conclusion": "ANTIDOTE\u4e3a\u6709\u566a\u58f0\u6807\u7b7e\u7684\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5e94\u5bf9\u56fa\u6709\u7684\u6216\u5bf9\u6297\u6027\u5f15\u5165\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u3002"}}
{"id": "2508.06753", "pdf": "https://arxiv.org/pdf/2508.06753", "abs": "https://arxiv.org/abs/2508.06753", "authors": ["Evangelos Georganas", "Dhiraj Kalamkar", "Alexander Heinecke"], "title": "Pushing the Envelope of LLM Inference on AI-PC", "categories": ["cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the\nperplexity and end-task performance of their full-precision counterparts using\nthe same model size, is ushering in a new era of LLM inference for\nresource-constrained environments such as edge devices and AI PCs. While these\nquantization advances promise models that are more cost-effective in terms of\nlatency, memory, throughput, and energy consumption, the computational\nefficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)\nused to deploy them remains underexplored. In this work, we take a bottom-up\napproach: we first design and implement 1-bit and 2-bit microkernels optimized\nfor modern CPUs, achieving peak computational efficiency across a variety of\nCPU platforms. We integrate these microkernels into a state-of-the-art LLM\ninference framework, namely PyTorch-TPP, and present end-to-end inference\nresults with 2-bit models that outperform the current SOTA runtime bitnet.cpp\nby up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model\ninference. Our optimized runtime advances the state of LLM inference on AI PCs\nand edge devices, paving the way for efficient deployment of ultra-low-bit LLM\nmodels.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4e3a\u73b0\u4ee3CPU\u8bbe\u8ba1\u548c\u5b9e\u73b0\u4f18\u5316\u76841/2\u6bd4\u7279\u5fae\u6838\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230PyTorch-TPP\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d85\u4f4e\u6bd4\u7279LLM\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u6700\u9ad8\u8fbe\u5230\u73b0\u6709SOTA\u8fd0\u884c\u65f6\u76842.2\u500d\u548c16\u6bd4\u7279\u6a21\u578b\u76847\u500d\u3002", "motivation": "\u5c3d\u7ba1\u8d85\u4f4e\u6bd4\u7279LLM\u6a21\u578b\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u6700\u5148\u8fdb\u7684\u63a8\u7406\u8fd0\u884c\u65f6\uff08\u5982bitnet.cpp\uff09\u5728\u90e8\u7f72\u8fd9\u4e9b\u6a21\u578b\u65f6\u7684\u8ba1\u7b97\u6548\u7387\u5c1a\u672a\u5f97\u5230\u5145\u5206\u4f18\u5316\u3002", "method": "\u7814\u7a76\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u6cd5\uff0c\u9996\u5148\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u9488\u5bf9\u73b0\u4ee3CPU\u4f18\u5316\u76841\u6bd4\u7279\u548c2\u6bd4\u7279\u5fae\u6838\uff0c\u4ee5\u5b9e\u73b0\u5cf0\u503c\u8ba1\u7b97\u6548\u7387\uff1b\u7136\u540e\u5c06\u8fd9\u4e9b\u5fae\u6838\u96c6\u6210\u5230PyTorch-TPP\u8fd9\u4e00LLM\u63a8\u7406\u6846\u67b6\u4e2d\u3002", "result": "\u901a\u8fc72\u6bd4\u7279\u6a21\u578b\u8fdb\u884c\u7aef\u5230\u7aef\u63a8\u7406\uff0c\u6027\u80fd\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8fd0\u884c\u65f6bitnet.cpp\u63d0\u9ad8\u4e86\u9ad8\u8fbe2.2\u500d\uff0c\u5e76\u4e14\u4e0e16\u6bd4\u7279\u6a21\u578b\u63a8\u7406\u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe7\u500d\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u4f18\u5316\u8fd0\u884c\u65f6\u63d0\u5347\u4e86AI PC\u548c\u8fb9\u7f18\u8bbe\u5907\u4e0aLLM\u63a8\u7406\u7684\u6c34\u5e73\uff0c\u4e3a\u8d85\u4f4e\u6bd4\u7279LLM\u6a21\u578b\u7684\u6709\u6548\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06543", "pdf": "https://arxiv.org/pdf/2508.06543", "abs": "https://arxiv.org/abs/2508.06543", "authors": ["Jinghan Yu", "Zhiyuan Ma", "Yue Ma", "Kaiqi Liu", "Yuhan Wang", "Jianjun Li"], "title": "MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing", "categories": ["cs.CV"], "comment": null, "summary": "Recent years have witnessed the success of diffusion models in\nimage-customized tasks. Prior works have achieved notable progress on\nhuman-oriented erasing using explicit mask guidance and semantic-aware\ninpainting. However, they struggle under complex multi-IP scenarios involving\nhuman-human occlusions, human-object entanglements, and background\ninterferences. These challenges are mainly due to: 1) Dataset limitations, as\nexisting datasets rarely cover dense occlusions, camouflaged backgrounds, and\ndiverse interactions; 2) Lack of spatial decoupling, where foreground instances\ncannot be effectively disentangled, limiting clean background restoration. In\nthis work, we introduce a high-quality multi-IP human erasing dataset with\ndiverse pose variations and complex backgrounds. We then propose Multi-Layer\nDiffusion (MILD), a novel strategy that decomposes generation into semantically\nseparated pathways for each instance and the background. To enhance\nhuman-centric understanding, we introduce Human Morphology Guidance,\nintegrating pose, parsing, and spatial relations. We further present\nSpatially-Modulated Attention to better guide attention flow. Extensive\nexperiments show that MILD outperforms state-of-the-art methods on challenging\nhuman erasing benchmarks.", "AI": {"tldr": "\u9488\u5bf9\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u591a\u4eba\u7269\u56fe\u50cf\u64e6\u9664\u4e2d\u7684\u4e0d\u8db3\uff0c\u672c\u6587\u63d0\u51faMILD\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u65b0\u6570\u636e\u96c6\u3001\u591a\u5c42\u5206\u89e3\u751f\u6210\u548c\u5f15\u5165\u4eba\u4f53\u5f62\u6001/\u7a7a\u95f4\u5f15\u5bfc\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u4f53\u64e6\u9664\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u591a\u4eba\u7269\uff08\u6d89\u53ca\u4eba\u4f53-\u4eba\u4f53\u906e\u6321\u3001\u4eba\u4f53-\u7269\u4f53\u7f20\u7ed5\u53ca\u80cc\u666f\u5e72\u6270\uff09\u4eba\u4f53\u64e6\u9664\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\uff1a1) \u6570\u636e\u96c6\u7f3a\u4e4f\u5bf9\u5bc6\u96c6\u906e\u6321\u3001\u4f2a\u88c5\u80cc\u666f\u548c\u591a\u6837\u4e92\u52a8\u7684\u8986\u76d6\uff1b2) \u7f3a\u4e4f\u6709\u6548\u7684\u7a7a\u95f4\u89e3\u8026\u80fd\u529b\uff0c\u65e0\u6cd5\u5e72\u51c0\u5730\u5206\u79bb\u524d\u666f\u5b9e\u4f8b\u5e76\u6062\u590d\u80cc\u666f\u3002", "method": "1. \u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u591a\u4eba\u7269\u4eba\u4f53\u64e6\u9664\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6837\u59ff\u6001\u548c\u590d\u6742\u80cc\u666f\u3002\n2. \u63d0\u51fa\u4e86\u591a\u5c42\u6269\u6563\uff08Multi-Layer Diffusion, MILD\uff09\u7b56\u7565\uff0c\u5c06\u56fe\u50cf\u751f\u6210\u5206\u89e3\u4e3a\u8bed\u4e49\u4e0a\u72ec\u7acb\u7684\u4eba\u4f53\u5b9e\u4f8b\u548c\u80cc\u666f\u8def\u5f84\u3002\n3. \u5f15\u5165\u4e86\u4eba\u4f53\u5f62\u6001\u5f15\u5bfc\uff08Human Morphology Guidance\uff09\uff0c\u7ed3\u5408\u59ff\u6001\u3001\u89e3\u6790\u548c\u7a7a\u95f4\u5173\u7cfb\u4ee5\u589e\u5f3a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u7406\u89e3\u3002\n4. \u63d0\u51fa\u4e86\u7a7a\u95f4\u8c03\u5236\u6ce8\u610f\u529b\uff08Spatially-Modulated Attention\uff09\uff0c\u4ee5\u66f4\u597d\u5730\u5f15\u5bfc\u6ce8\u610f\u529b\u6d41\u3002", "result": "\u5728\u6311\u6218\u6027\u4eba\u4f53\u64e6\u9664\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMILD\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\uff08SOTA\uff09\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u521b\u65b0\u6027\u5730\u6784\u5efa\u6570\u636e\u96c6\u3001\u63d0\u51faMILD\u591a\u5c42\u5206\u89e3\u7b56\u7565\u5e76\u5f15\u5165\u7279\u5b9a\u5f15\u5bfc\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u590d\u6742\u591a\u4eba\u7269\u573a\u666f\u4e0b\u7684\u4eba\u4f53\u64e6\u9664\u96be\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2508.06755", "pdf": "https://arxiv.org/pdf/2508.06755", "abs": "https://arxiv.org/abs/2508.06755", "authors": ["Xianjun Yang", "Liqiang Xiao", "Shiyang Li", "Faisal Ladhak", "Hyokun Yun", "Linda Ruth Petzold", "Yi Xu", "William Yang Wang"], "title": "Many-Turn Jailbreaking", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current jailbreaking work on large language models (LLMs) aims to elicit\nunsafe outputs from given prompts. However, it only focuses on single-turn\njailbreaking targeting one specific query. On the contrary, the advanced LLMs\nare designed to handle extremely long contexts and can thus conduct multi-turn\nconversations. So, we propose exploring multi-turn jailbreaking, in which the\njailbroken LLMs are continuously tested on more than the first-turn\nconversation or a single target query. This is an even more serious threat\nbecause 1) it is common for users to continue asking relevant follow-up\nquestions to clarify certain jailbroken details, and 2) it is also possible\nthat the initial round of jailbreaking causes the LLMs to respond to additional\nirrelevant questions consistently. As the first step (First draft done at June\n2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak\nBenchmark (MTJ-Bench) for benchmarking this setting on a series of open- and\nclosed-source models and provide novel insights into this new safety threat. By\nrevealing this new vulnerability, we aim to call for community efforts to build\nsafer LLMs and pave the way for a more in-depth understanding of jailbreaking\nLLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u8f6e\u8d8a\u72f1\u6982\u5ff5\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u9488\u5bf9\u6b64\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6MTJ-Bench\uff0c\u4ee5\u63ed\u793a\u5176\u6f5c\u5728\u5a01\u80c1\u3002", "motivation": "\u73b0\u6709LLM\u8d8a\u72f1\u7814\u7a76\u4ec5\u5173\u6ce8\u5355\u8f6e\u4ea4\u4e92\uff0c\u800c\u5148\u8fdbLLM\u652f\u6301\u591a\u8f6e\u5bf9\u8bdd\u3002\u591a\u8f6e\u8d8a\u72f1\u6784\u6210\u66f4\u4e25\u91cd\u5a01\u80c1\uff0c\u56e0\u4e3a\u5b83\u5141\u8bb8\u7528\u6237\u6301\u7eed\u8ffd\u95ee\u8d8a\u72f1\u7ec6\u8282\uff0c\u6216\u5bfc\u81f4LLM\u5728\u521d\u59cb\u8d8a\u72f1\u540e\u5bf9\u65e0\u5173\u95ee\u9898\u4e5f\u6301\u7eed\u63d0\u4f9b\u4e0d\u5b89\u5168\u54cd\u5e94\u3002", "method": "\u4f5c\u4e3a\u63a2\u7d22\u591a\u8f6e\u8d8a\u72f1\u7684\u7b2c\u4e00\u6b65\uff0c\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u8f6e\u8d8a\u72f1\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff08MTJ-Bench\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e00\u7cfb\u5217\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u5728\u8be5\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u65b0\u578b\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "conclusion": "\u901a\u8fc7\u63ed\u793a\u8fd9\u4e00\u65b0\u6f0f\u6d1e\uff0c\u65e8\u5728\u547c\u5401\u793e\u533a\u5171\u540c\u52aa\u529b\uff0c\u6784\u5efa\u66f4\u5b89\u5168\u7684LLM\uff0c\u5e76\u4e3a\u66f4\u6df1\u5165\u5730\u7406\u89e3\u8d8a\u72f1\u673a\u5236\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2508.07882", "pdf": "https://arxiv.org/pdf/2508.07882", "abs": "https://arxiv.org/abs/2508.07882", "authors": ["Conor Muldoon"], "title": "Scalable and Energy-Efficient Predictive Data Collection in Wireless Sensor Networks with Constructive Interference", "categories": ["cs.NI"], "comment": null, "summary": "A new class of Wireless Sensor Network has emerged whereby multiple nodes\ntransmit data simultaneously, exploiting constructive interference to enable\ndata collection frameworks with low energy usage and latency. This paper\npresents STAIR (Spatio-Temporal Activation for Intelligent Relaying), a\nscalable, resilient framework for Wireless Sensor Networks that leverages\nconstructive interference and operates effectively under stringent resource\nconstraints. Using constructive interference requires all nodes to transmit the\nsame packet at the same time, thus, only one source node can send data per time\nslot. STAIR uses coarse-grained topology information to flood a selected subset\nof the network, relaying sensor readings from individual nodes during their\nallocated time slots. A submodular optimisation algorithm with proven quality\nbounds determines near-optimal sensor activation locations and times, aiming to\nminimise the sum of mean squared prediction errors from a multiple multivariate\nlinear regression model, which is used to estimate values at unselected\nlocations and times. This framework has been extensively validated on a\nreal-world testbed deployment.", "AI": {"tldr": "STAIR\u662f\u4e00\u4e2a\u5229\u7528\u5efa\u8bbe\u6027\u5e72\u6270\u7684\u65b0\u578b\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\uff08WSN\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6b21\u6a21\u4f18\u5316\u7b97\u6cd5\u9009\u62e9\u6027\u6fc0\u6d3b\u8282\u70b9\uff0c\u4ee5\u6700\u5c0f\u5316\u6570\u636e\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u5728\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u65b0\u5174\u7684\u5efa\u8bbe\u6027\u5e72\u6270\u6280\u672f\u4e3a\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u63d0\u4f9b\u4e86\u4f4e\u80fd\u8017\u3001\u4f4e\u5ef6\u8fdf\u7684\u6570\u636e\u6536\u96c6\u6f5c\u529b\u3002\u9700\u8981\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u5f39\u6027\u4e14\u5728\u8d44\u6e90\u53d7\u9650\u4e0b\u9ad8\u6548\u5229\u7528\u6b64\u6280\u672f\u7684\u6570\u636e\u6536\u96c6\u6846\u67b6\uff0c\u5e76\u80fd\u6709\u6548\u4f30\u8ba1\u672a\u76d1\u6d4b\u533a\u57df\u7684\u503c\u3002", "result": "STAIR\u6846\u67b6\u5728\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u5e73\u53f0\u90e8\u7f72\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u9a8c\u8bc1\uff0c\u8bc1\u660e\u5176\u5728\u5229\u7528\u5efa\u8bbe\u6027\u5e72\u6270\u5b9e\u73b0\u6570\u636e\u6536\u96c6\u548c\u503c\u4f30\u8ba1\u65b9\u9762\u7684\u6709\u6548\u6027\u3001\u53ef\u4f38\u7f29\u6027\u548c\u5f39\u6027\u3002", "method": "1. \u63d0\u51faSTAIR\uff08Spatio-Temporal Activation for Intelligent Relaying\uff09\u6846\u67b6\uff0c\u5229\u7528\u5efa\u8bbe\u6027\u5e72\u6270\u5b9e\u73b0\u591a\u8282\u70b9\u540c\u6b65\u6570\u636e\u4e2d\u7ee7\u30022. \u901a\u8fc7\u7c97\u7c92\u5ea6\u62d3\u6251\u4fe1\u606f\u6cdb\u6d2a\u7f51\u7edc\u5b50\u96c6\uff0c\u5728\u5206\u914d\u65f6\u9699\u4e2d\u4e2d\u7ee7\u4f20\u611f\u5668\u8bfb\u6570\u30023. \u91c7\u7528\u5177\u6709\u8d28\u91cf\u4fdd\u8bc1\u7684\u6b21\u6a21\u4f18\u5316\u7b97\u6cd5\uff0c\u786e\u5b9a\u8fd1\u4e4e\u6700\u4f18\u7684\u4f20\u611f\u5668\u6fc0\u6d3b\u4f4d\u7f6e\u548c\u65f6\u95f4\u30024. \u4f18\u5316\u76ee\u6807\u662f\u6700\u5c0f\u5316\u591a\u5143\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff08\u7528\u4e8e\u4f30\u8ba1\u672a\u9009\u62e9\u4f4d\u7f6e\u548c\u65f6\u95f4\u7684\u503c\uff09\u7684\u5747\u65b9\u9884\u6d4b\u8bef\u5dee\u603b\u548c\u3002", "conclusion": "STAIR\u6846\u67b6\u6210\u529f\u5730\u4e3a\u5229\u7528\u5efa\u8bbe\u6027\u5e72\u6270\u7684\u65b0\u578b\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u5f39\u6027\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u6570\u636e\u6536\u96c6\u548c\u503c\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u90e8\u7f72\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2508.06627", "pdf": "https://arxiv.org/pdf/2508.06627", "abs": "https://arxiv.org/abs/2508.06627", "authors": ["Mosbah Aouad", "Anirudh Choudhary", "Awais Farooq", "Steven Nevers", "Lusine Demirkhanyan", "Bhrandon Harris", "Suguna Pappu", "Christopher Gondi", "Ravishankar Iyer"], "title": "Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Record", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and\nearly detection remains a major clinical challenge due to the absence of\nspecific symptoms and reliable biomarkers. In this work, we propose a new\nmultimodal approach that integrates longitudinal diagnosis code histories and\nroutinely collected laboratory measurements from electronic health records to\ndetect PDAC up to one year prior to clinical diagnosis. Our method combines\nneural controlled differential equations to model irregular lab time series,\npretrained language models and recurrent networks to learn diagnosis code\ntrajectory representations, and cross-attention mechanisms to capture\ninteractions between the two modalities. We develop and evaluate our approach\non a real-world dataset of nearly 4,700 patients and achieve significant\nimprovements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods.\nFurthermore, our model identifies diagnosis codes and laboratory panels\nassociated with elevated PDAC risk, including both established and new\nbiomarkers. Our code is available at\nhttps://github.com/MosbahAouad/EarlyPDAC-MML.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u8bca\u65ad\u4ee3\u7801\u5386\u53f2\u548c\u5b9e\u9a8c\u5ba4\u6d4b\u91cf\u6570\u636e\uff0c\u5b9e\u73b0\u80f0\u817a\u5bfc\u7ba1\u817a\u764c\uff08PDAC\uff09\u7684\u65e9\u671f\u68c0\u6d4b\u3002", "motivation": "\u80f0\u817a\u5bfc\u7ba1\u817a\u764c\uff08PDAC\uff09\u662f\u6700\u81f4\u547d\u7684\u764c\u75c7\u4e4b\u4e00\uff0c\u7531\u4e8e\u7f3a\u4e4f\u7279\u5f02\u6027\u75c7\u72b6\u548c\u53ef\u9760\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u65e9\u671f\u53d1\u73b0\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u7684\u4e34\u5e8a\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u591a\u79cd\u6280\u672f\uff1a\u4f7f\u7528\u795e\u7ecf\u53d7\u63a7\u5fae\u5206\u65b9\u7a0b\u5efa\u6a21\u4e0d\u89c4\u5219\u7684\u5b9e\u9a8c\u5ba4\u65f6\u95f4\u5e8f\u5217\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u5faa\u73af\u7f51\u7edc\u5b66\u4e60\u8bca\u65ad\u4ee3\u7801\u8f68\u8ff9\u8868\u793a\uff0c\u5e76\u91c7\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u4e24\u79cd\u6a21\u6001\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u7eb5\u5411\u8bca\u65ad\u4ee3\u7801\u5386\u53f2\u548c\u5e38\u89c4\u6536\u96c6\u7684\u5b9e\u9a8c\u5ba4\u6d4b\u91cf\u6570\u636e\u3002", "result": "\u5728\u8fd14700\u540d\u60a3\u8005\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728AUC\u65b9\u9762\u6bd4\u73b0\u6709\u6700\u65b0\u65b9\u6cd5\u63d0\u9ad8\u4e866.5%\u81f315.5%\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u8bc6\u522b\u51fa\u4e0ePDAC\u98ce\u9669\u5347\u9ad8\u76f8\u5173\u7684\u8bca\u65ad\u4ee3\u7801\u548c\u5b9e\u9a8c\u5ba4\u6307\u6807\uff0c\u5305\u62ec\u5df2\u77e5\u7684\u548c\u65b0\u7684\u751f\u7269\u6807\u5fd7\u7269\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u80f0\u817a\u5bfc\u7ba1\u817a\u764c\u7684\u65e9\u671f\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u8bc6\u522b\u51fa\u6f5c\u5728\u7684\u98ce\u9669\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4e3a\u4e34\u5e8a\u65e9\u671f\u5e72\u9884\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.06754", "pdf": "https://arxiv.org/pdf/2508.06754", "abs": "https://arxiv.org/abs/2508.06754", "authors": ["Vanessa Figueiredo"], "title": "A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks", "categories": ["cs.AI", "I.2.7"], "comment": null, "summary": "We introduce a modular prompting framework that supports safer and more\nadaptive use of large language models (LLMs) across dynamic, user-centered\ntasks. Grounded in human learning theory, particularly the Zone of Proximal\nDevelopment (ZPD), our method combines a natural language boundary prompt with\na control schema encoded with fuzzy scaffolding logic and adaptation rules.\nThis architecture enables LLMs to modulate behavior in response to user state\nwithout requiring fine-tuning or external orchestration. In a simulated\nintelligent tutoring setting, the framework improves scaffolding quality,\nadaptivity, and instructional alignment across multiple models, outperforming\nstandard prompting baselines. Evaluation is conducted using rubric-based LLM\ngraders at scale. While initially developed for education, the framework has\nshown promise in other interaction-heavy domains, such as procedural content\ngeneration for games. Designed for safe deployment, it provides a reusable\nmethodology for structuring interpretable, goal-aligned LLM behavior in\nuncertain or evolving contexts.", "AI": {"tldr": "\u5f15\u5165\u4e00\u4e2a\u6a21\u5757\u5316\u63d0\u793a\u6846\u67b6\uff0c\u57fa\u4e8e\u8fd1\u4fa7\u53d1\u5c55\u533a\uff08ZPD\uff09\u7406\u8bba\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7528\u6237\u4e2d\u5fc3\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u81ea\u9002\u5e94\u7684\u884c\u4e3a\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3LLMs\u5728\u52a8\u6001\u3001\u7528\u6237\u4e2d\u5fc3\u4efb\u52a1\u4e2d\u5b89\u5168\u548c\u81ea\u9002\u5e94\u5e94\u7528\u7684\u95ee\u9898\uff0c\u4f7f\u5176\u884c\u4e3a\u80fd\u591f\u6839\u636e\u7528\u6237\u72b6\u6001\u8fdb\u884c\u8c03\u6574\uff0c\u540c\u65f6\u65e0\u9700\u8fdb\u884c\u5fae\u8c03\u6216\u5916\u90e8\u534f\u8c03\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u3001\u76ee\u6807\u5bf9\u9f50\u7684\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u5757\u5316\u63d0\u793a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u81ea\u7136\u8bed\u8a00\u8fb9\u754c\u63d0\u793a\u548c\u4e00\u4e2a\u7f16\u7801\u6709\u6a21\u7cca\u811a\u624b\u67b6\u903b\u8f91\u548c\u81ea\u9002\u5e94\u89c4\u5219\u7684\u63a7\u5236\u6a21\u5f0f\uff0c\u5176\u7406\u8bba\u57fa\u7840\u662f\u4eba\u7c7b\u5b66\u4e60\u7406\u8bba\u4e2d\u7684\u8fd1\u4fa7\u53d1\u5c55\u533a\uff08ZPD\uff09\u3002", "result": "\u5728\u6a21\u62df\u667a\u80fd\u8f85\u5bfc\u573a\u666f\u4e2d\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2a\u6a21\u578b\u7684\u811a\u624b\u67b6\u8d28\u91cf\u3001\u81ea\u9002\u5e94\u6027\u548c\u6559\u5b66\u4e00\u81f4\u6027\uff0c\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u57fa\u7ebf\u3002\u8bc4\u4f30\u91c7\u7528\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684LLM\u8bc4\u5206\u5668\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u5728\u6e38\u620f\u7a0b\u5e8f\u5185\u5bb9\u751f\u6210\u7b49\u5176\u4ed6\u4ea4\u4e92\u5bc6\u96c6\u578b\u9886\u57df\u4e5f\u663e\u793a\u51fa\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u91cd\u7528\u7684\u65b9\u6cd5\u8bba\uff0c\u7528\u4e8e\u5728\u4e0d\u786e\u5b9a\u6216\u6f14\u53d8\u7684\u73af\u5883\u4e2d\u6784\u5efa\u53ef\u89e3\u91ca\u3001\u76ee\u6807\u5bf9\u9f50\u7684LLM\u884c\u4e3a\uff0c\u8bbe\u8ba1\u4e0a\u6ce8\u91cd\u5b89\u5168\u90e8\u7f72\uff0c\u5e76\u5177\u6709\u8d85\u8d8a\u6559\u80b2\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2508.06546", "pdf": "https://arxiv.org/pdf/2508.06546", "abs": "https://arxiv.org/abs/2508.06546", "authors": ["Qi Xun Yeo", "Yanyan Li", "Gim Hee Lee"], "title": "Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images", "categories": ["cs.CV", "eess.IV"], "comment": "This paper has been accepted in ICCV 25", "summary": "Modern 3D semantic scene graph estimation methods utilize ground truth 3D\nannotations to accurately predict target objects, predicates, and\nrelationships. In the absence of given 3D ground truth representations, we\nexplore leveraging only multi-view RGB images to tackle this task. To attain\nrobust features for accurate scene graph estimation, we must overcome the noisy\nreconstructed pseudo point-based geometry from predicted depth maps and reduce\nthe amount of background noise present in multi-view image features. The key is\nto enrich node and edge features with accurate semantic and spatial information\nand through neighboring relations. We obtain semantic masks to guide feature\naggregation to filter background features and design a novel method to\nincorporate neighboring node information to aid robustness of our scene graph\nestimates. Furthermore, we leverage on explicit statistical priors calculated\nfrom the training summary statistics to refine node and edge predictions based\non their one-hop neighborhood. Our experiments show that our method outperforms\ncurrent methods purely using multi-view images as the initial input. Our\nproject page is available at https://qixun1.github.io/projects/SCRSSG.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u4ec5\u5229\u7528\u591a\u89c6\u89d2RGB\u56fe\u50cf\u8fdb\u884c3D\u8bed\u4e49\u573a\u666f\u56fe\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u63a9\u7801\u3001\u90bb\u5c45\u4fe1\u606f\u6574\u5408\u548c\u7edf\u8ba1\u5148\u9a8c\u6765\u514b\u670d\u566a\u58f0\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u540c\u7c7b\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d3D\u8bed\u4e49\u573a\u666f\u56fe\u4f30\u8ba1\u4f9d\u8d56\u4e8e3D\u771f\u503c\u6807\u6ce8\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5728\u7f3a\u4e4f3D\u771f\u503c\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u5229\u7528\u591a\u89c6\u89d2RGB\u56fe\u50cf\u8fdb\u884c\u6b64\u4efb\u52a1\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u8981\u514b\u670d\u9884\u6d4b\u6df1\u5ea6\u56fe\u5bfc\u81f4\u7684\u566a\u58f0\u51e0\u4f55\u4ee5\u53ca\u591a\u89c6\u89d2\u56fe\u50cf\u7279\u5f81\u4e2d\u7684\u80cc\u666f\u566a\u58f0\u3002", "method": "\u6838\u5fc3\u662f\u5229\u7528\u51c6\u786e\u7684\u8bed\u4e49\u548c\u7a7a\u95f4\u4fe1\u606f\u4ee5\u53ca\u90bb\u5c45\u5173\u7cfb\u6765\u4e30\u5bcc\u8282\u70b9\u548c\u8fb9\u7f18\u7279\u5f81\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u83b7\u53d6\u8bed\u4e49\u63a9\u7801\u4ee5\u5f15\u5bfc\u7279\u5f81\u805a\u5408\u5e76\u8fc7\u6ee4\u80cc\u666f\uff1b\u8bbe\u8ba1\u65b0\u65b9\u6cd5\u6574\u5408\u90bb\u5c45\u8282\u70b9\u4fe1\u606f\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\uff1b\u5229\u7528\u8bad\u7ec3\u7edf\u8ba1\u6570\u636e\u8ba1\u7b97\u7684\u663e\u5f0f\u7edf\u8ba1\u5148\u9a8c\uff0c\u57fa\u4e8e\u4e00\u8df3\u90bb\u5c45\u5173\u7cfb\u7cbe\u70bc\u8282\u70b9\u548c\u8fb9\u7f18\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528\u591a\u89c6\u89d2\u56fe\u50cf\u4f5c\u4e3a\u521d\u59cb\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u540c\u7c7b\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u65e03D\u771f\u503c\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u4ece\u591a\u89c6\u89d2RGB\u56fe\u50cf\u4f30\u8ba13D\u8bed\u4e49\u573a\u666f\u56fe\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7279\u5f81\u589e\u5f3a\u548c\u566a\u58f0\u8fc7\u6ee4\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.06803", "pdf": "https://arxiv.org/pdf/2508.06803", "abs": "https://arxiv.org/abs/2508.06803", "authors": ["Ziqi Liu", "Yangbin Chen", "Ziyang Zhou", "Yilin Li", "Mingxuan Hu", "Yushan Pan", "Zhijie Xu"], "title": "SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Sarcasm detection is a crucial yet challenging Natural Language Processing\ntask. Existing Large Language Model methods are often limited by\nsingle-perspective analysis, static reasoning pathways, and a susceptibility to\nhallucination when processing complex ironic rhetoric, which impacts their\naccuracy and reliability. To address these challenges, we propose **SEVADE**, a\nnovel **S**elf-**Ev**olving multi-agent **A**nalysis framework with\n**D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The\ncore of our framework is a Dynamic Agentive Reasoning Engine (DARE), which\nutilizes a team of specialized agents grounded in linguistic theory to perform\na multifaceted deconstruction of the text and generate a structured reasoning\nchain. Subsequently, a separate lightweight rationale adjudicator (RA) performs\nthe final classification based solely on this reasoning chain. This decoupled\narchitecture is designed to mitigate the risk of hallucination by separating\ncomplex reasoning from the final judgment. Extensive experiments on four\nbenchmark datasets demonstrate that our framework achieves state-of-the-art\nperformance, with average improvements of **6.75%** in Accuracy and **6.29%**\nin Macro-F1 score.", "AI": {"tldr": "\u63d0\u51faSEVADE\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u6f14\u5316\u591a\u667a\u80fd\u4f53\u5206\u6790\u6846\u67b6\uff0c\u91c7\u7528\u89e3\u8026\u8bc4\u4f30\uff0c\u4ee5\u5b9e\u73b0\u6297\u5e7b\u89c9\u7684\u8bbd\u523a\u68c0\u6d4b\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8bbd\u523a\u68c0\u6d4b\u4e2d\u5b58\u5728\u5355\u89c6\u89d2\u5206\u6790\u3001\u9759\u6001\u63a8\u7406\u8def\u5f84\u53ca\u6613\u53d7\u5e7b\u89c9\u5f71\u54cd\u7684\u95ee\u9898\uff0c\u635f\u5bb3\u4e86\u5176\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faSEVADE\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u52a8\u6001\u667a\u80fd\u4f53\u63a8\u7406\u5f15\u64ce\uff08DARE\uff09\uff0c\u5229\u7528\u4e00\u7ec4\u57fa\u4e8e\u8bed\u8a00\u5b66\u7406\u8bba\u7684\u4e13\u4e1a\u667a\u80fd\u4f53\u8fdb\u884c\u6587\u672c\u591a\u65b9\u9762\u89e3\u6784\u5e76\u751f\u6210\u7ed3\u6784\u5316\u63a8\u7406\u94fe\u3002\u968f\u540e\uff0c\u72ec\u7acb\u7684\u8f7b\u91cf\u7ea7\u7406\u7531\u88c1\u51b3\u5668\uff08RA\uff09\u4ec5\u57fa\u4e8e\u8be5\u63a8\u7406\u94fe\u8fdb\u884c\u6700\u7ec8\u5206\u7c7b\uff0c\u901a\u8fc7\u89e3\u8026\u63a8\u7406\u4e0e\u5224\u65ad\u6765\u964d\u4f4e\u5e7b\u89c9\u98ce\u9669\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSEVADE\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e866.75%\uff0cMacro-F1\u5206\u6570\u63d0\u9ad8\u4e866.29%\u3002", "conclusion": "SEVADE\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u591a\u667a\u80fd\u4f53\u548c\u89e3\u8026\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709LLM\u5728\u8bbd\u523a\u68c0\u6d4b\u4e2d\u6613\u53d7\u5e7b\u89c9\u5f71\u54cd\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.07978", "pdf": "https://arxiv.org/pdf/2508.07978", "abs": "https://arxiv.org/abs/2508.07978", "authors": ["Hamidreza Mazandarani", "Mohammad Farhoudi", "Masoud Shokrnezhad", "Tarik Taleb"], "title": "Adaptive Multiple Access and Service Placement for Generative Diffusion Models", "categories": ["cs.NI"], "comment": "This manuscript has been accepted for presentation at IEEE GLOBECOM\n  2025. You can use this material personally. Reprinting or republishing this\n  material for the purpose of advertising or promotion, etc., must adhere to\n  IEEE policy. The DOI will be supplied as soon as it becomes available", "summary": "Generative Diffusion Models (GDMs) have emerged as key components of\nGenerative Artificial Intelligence (GenAI), offering unparalleled\nexpressiveness and controllability for complex data generation tasks. However,\ntheir deployment in real-time and mobile environments remains challenging due\nto the iterative and resource-intensive nature of the inference process.\nAddressing these challenges, this paper introduces a unified optimization\nframework that jointly tackles service placement and multiple access control\nfor GDMs in mobile edge networks. We propose LEARN-GDM, a Deep Reinforcement\nLearning-based algorithm that dynamically partitions denoising blocks across\nheterogeneous edge nodes, while accounting for latent transmission costs and\nenabling adaptive reduction of inference steps. Our approach integrates a\ngreedy multiple access scheme with a Double and Dueling Deep Q-Learning\n(D3QL)-based service placement, allowing for scalable, adaptable, and\nresource-efficient operation under stringent quality of service requirements.\nSimulations demonstrate the superior performance of the proposed framework in\nterms of scalability and latency resilience compared to conventional monolithic\nand fixed chain-length placement strategies. This work advances the state of\nthe art in edge-enabled GenAI by offering an adaptable solution for GDM\nservices orchestration, paving the way for future extensions toward semantic\nnetworking and co-inference across distributed environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u4e00\u4f18\u5316\u6846\u67b6LEARN-GDM\uff0c\u65e8\u5728\u89e3\u51b3\u751f\u6210\u6269\u6563\u6a21\u578b\u5728\u79fb\u52a8\u8fb9\u7f18\u7f51\u7edc\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u8d44\u6e90\u5bc6\u96c6\u548c\u5b9e\u65f6\u6027\u6311\u6218\uff0c\u901a\u8fc7\u52a8\u6001\u5757\u5212\u5206\u548c\u591a\u8def\u8bbf\u95ee\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u548c\u4f4e\u5ef6\u8fdf\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u6269\u6563\u6a21\u578b(GDMs)\u5728\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd(GenAI)\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8868\u8fbe\u548c\u63a7\u5236\u80fd\u529b\uff0c\u4f46\u5176\u8fed\u4ee3\u548c\u8d44\u6e90\u5bc6\u96c6\u578b\u63a8\u7406\u8fc7\u7a0b\u4f7f\u5176\u96be\u4ee5\u5728\u5b9e\u65f6\u548c\u79fb\u52a8\u73af\u5883\u4e2d\u90e8\u7f72\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u4f18\u5316\u6846\u67b6\uff0c\u8054\u5408\u5904\u7406\u79fb\u52a8\u8fb9\u7f18\u7f51\u7edc\u4e2dGDM\u7684\u670d\u52a1\u653e\u7f6e\u548c\u591a\u8def\u8bbf\u95ee\u63a7\u5236\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60(DRL)\u7684\u7b97\u6cd5LEARN-GDM\uff0c\u8be5\u7b97\u6cd5\u80fd\u52a8\u6001\u5730\u5c06\u53bb\u566a\u5757\u5206\u914d\u5230\u5f02\u6784\u8fb9\u7f18\u8282\u70b9\uff0c\u8003\u8651\u6f5c\u5728\u4f20\u8f93\u6210\u672c\u5e76\u81ea\u9002\u5e94\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u4e86\u8d2a\u5a6a\u591a\u8def\u8bbf\u95ee\u65b9\u6848\u548c\u57fa\u4e8eD3QL(Double and Dueling Deep Q-Learning)\u7684\u670d\u52a1\u653e\u7f6e\u7b56\u7565\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u5355\u4e00\u548c\u56fa\u5b9a\u94fe\u957f\u653e\u7f6e\u7b56\u7565\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u53ef\u6269\u5c55\u6027\u548c\u5ef6\u8fdf\u5f39\u6027\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u4e3aGDM\u670d\u52a1\u7f16\u6392\u63d0\u4f9b\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u8fb9\u7f18\u8d4b\u80fdGenAI\u7684\u73b0\u6709\u6280\u672f\u6c34\u5e73\uff0c\u5e76\u4e3a\u672a\u6765\u5411\u8bed\u4e49\u7f51\u7edc\u548c\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u7684\u534f\u540c\u63a8\u7406\u6269\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06635", "pdf": "https://arxiv.org/pdf/2508.06635", "abs": "https://arxiv.org/abs/2508.06635", "authors": ["Yewon Byun", "Shantanu Gupta", "Zachary C. Lipton", "Rachel Leah Childers", "Bryan Wilder"], "title": "Using Imperfect Synthetic Data in Downstream Inference Tasks", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Predictions and generations from large language models are increasingly being\nexplored as an aid to computational social science and human subject research\nin limited data regimes. While previous technical work has explored the\npotential to use model-predicted labels for unlabeled data in a principled\nmanner, there is increasing interest in using large language models to generate\nentirely new synthetic samples (also termed as synthetic simulations), such as\nin responses to surveys. However, it is not immediately clear by what means\npractitioners can combine such data with real data and yet produce\nstatistically valid conclusions upon them. In this work, we introduce a new\nestimator based on generalized method of moments, providing a\nhyperparameter-free solution with strong theoretical guarantees to address the\nchallenge at hand. Surprisingly, we find that interactions between the moment\nresiduals of synthetic data and those of real data can improve estimates of the\ntarget parameter. We empirically validate the finite-sample performance of our\nestimator across different regression tasks in computational social science\napplications, demonstrating large empirical gains.", "AI": {"tldr": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u7ed3\u5408\u7684\u7edf\u8ba1\u6709\u6548\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u77e9\u65b9\u6cd5\u7684\u65b0\u578b\u4f30\u8ba1\u5668\uff0c\u5e76\u53d1\u73b0\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u77e9\u6b8b\u5dee\u4ea4\u4e92\u53ef\u663e\u8457\u63d0\u5347\u53c2\u6570\u4f30\u8ba1\u6548\u679c\u3002", "motivation": "\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u548c\u4eba\u7c7b\u53d7\u8bd5\u8005\u7814\u7a76\u8d8a\u6765\u8d8a\u591a\u5730\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u5408\u6210\u6837\u672c\u3002\u4f46\u5982\u4f55\u5c06\u8fd9\u4e9b\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u6709\u6548\u7ed3\u5408\uff0c\u5e76\u786e\u4fdd\u7edf\u8ba1\u7ed3\u8bba\u7684\u6709\u6548\u6027\uff0c\u662f\u4e00\u4e2a\u5f85\u89e3\u51b3\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u77e9\u65b9\u6cd5\uff08Generalized Method of Moments, GMM\uff09\u7684\u65b0\u578b\u4f30\u8ba1\u5668\u3002\u8be5\u4f30\u8ba1\u5668\u65e0\u9700\u8d85\u53c2\u6570\uff0c\u5e76\u5177\u6709\u5f3a\u5927\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "1. \u53d1\u73b0\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u77e9\u6b8b\u5dee\u4e4b\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u53ef\u4ee5\u63d0\u5347\u76ee\u6807\u53c2\u6570\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002\n2. \u5728\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u5e94\u7528\u4e2d\u7684\u591a\u79cd\u56de\u5f52\u4efb\u52a1\u4e0a\uff0c\u7ecf\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u4f30\u8ba1\u5668\u5177\u6709\u51fa\u8272\u7684\u6709\u9650\u6837\u672c\u6027\u80fd\uff0c\u5e76\u5e26\u6765\u4e86\u663e\u8457\u7684\u7ecf\u9a8c\u589e\u76ca\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8eGMM\u7684\u4f30\u8ba1\u5668\uff0c\u4e3a\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4ee5\u5f97\u51fa\u7edf\u8ba1\u6709\u6548\u7ed3\u8bba\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u95f4\u610f\u60f3\u4e0d\u5230\u7684\u4ea4\u4e92\u4f5c\u7528\u63d0\u5347\u4e86\u53c2\u6570\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2508.06823", "pdf": "https://arxiv.org/pdf/2508.06823", "abs": "https://arxiv.org/abs/2508.06823", "authors": ["Xuan Zhao", "Jun Tao"], "title": "Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation", "categories": ["cs.AI"], "comment": "Accepted by IEEE VIS 2025", "summary": "Exploring volumetric data is crucial for interpreting scientific datasets.\nHowever, selecting optimal viewpoints for effective navigation can be\nchallenging, particularly for users without extensive domain expertise or\nfamiliarity with 3D navigation. In this paper, we propose a novel framework\nthat leverages natural language interaction to enhance volumetric data\nexploration. Our approach encodes volumetric blocks to capture and\ndifferentiate underlying structures. It further incorporates a CLIP Score\nmechanism, which provides semantic information to the blocks to guide\nnavigation. The navigation is empowered by a reinforcement learning framework\nthat leverage these semantic cues to efficiently search for and identify\ndesired viewpoints that align with the user's intent. The selected viewpoints\nare evaluated using CLIP Score to ensure that they best reflect the user\nqueries. By automating viewpoint selection, our method improves the efficiency\nof volumetric data navigation and enhances the interpretability of complex\nscientific phenomena.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u81ea\u52a8\u5316\u4f53\u6570\u636e\u89c6\u70b9\u9009\u62e9\uff0c\u4ee5\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u63a2\u7d22\u4f53\u6570\u636e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9009\u62e9\u6700\u4f73\u89c6\u70b9\u5bf9\u975e\u4e13\u4e1a\u7528\u6237\u800c\u8a00\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5229\u7528\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7f16\u7801\u4f53\u6570\u636e\u5757\u5e76\u7ed3\u5408CLIP Score\u63d0\u4f9b\u8bed\u4e49\u4fe1\u606f\u3002\u8be5\u6846\u67b6\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u8bed\u4e49\u7ebf\u7d22\u9ad8\u6548\u641c\u7d22\u548c\u8bc6\u522b\u7528\u6237\u610f\u56fe\u7684\u89c6\u70b9\uff0c\u5e76\u7528CLIP Score\u8bc4\u4f30\u9009\u5b9a\u89c6\u70b9\u3002", "result": "\u8be5\u65b9\u6cd5\u81ea\u52a8\u5316\u4e86\u89c6\u70b9\u9009\u62e9\uff0c\u63d0\u5347\u4e86\u4f53\u6570\u636e\u5bfc\u822a\u6548\u7387\uff0c\u5e76\u589e\u5f3a\u4e86\u590d\u6742\u79d1\u5b66\u73b0\u8c61\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u4f53\u6570\u636e\u5bfc\u822a\u7684\u6548\u7387\u548c\u590d\u6742\u79d1\u5b66\u73b0\u8c61\u7684\u89e3\u91ca\u6027\uff0c\u964d\u4f4e\u4e86\u7528\u6237\u63a2\u7d22\u96be\u5ea6\u3002"}}
{"id": "2508.06551", "pdf": "https://arxiv.org/pdf/2508.06551", "abs": "https://arxiv.org/abs/2508.06551", "authors": ["Ye Tao"], "title": "Slice or the Whole Pie? Utility Control for AI Models", "categories": ["cs.CV"], "comment": null, "summary": "Training deep neural networks (DNNs) has become an increasingly\nresource-intensive task, requiring large volumes of labeled data, substantial\ncomputational power, and considerable fine-tuning efforts to achieve optimal\nperformance across diverse use cases. Although pre-trained models offer a\nuseful starting point, adapting them to meet specific user needs often demands\nextensive customization, and infrastructure overhead. This challenge grows when\na single model must support diverse appli-cations with differing requirements\nfor performance. Traditional solutions often involve training multiple model\nversions to meet varying requirements, which can be inefficient and difficult\nto maintain. In order to overcome this challenge, we propose NNObfuscator, a\nnovel utility control mechanism that enables AI models to dynamically modify\ntheir performance according to predefined conditions. It is different from\ntraditional methods that need separate models for each user. Instead,\nNNObfuscator allows a single model to be adapted in real time, giving you\ncontrolled access to multiple levels of performance. This mechanism enables\nmodel owners set up tiered access, ensuring that free-tier users receive a\nbaseline level of performance while premium users benefit from enhanced\ncapabilities. The approach improves resource allocation, reduces unnecessary\ncomputation, and supports sustainable business models in AI deployment. To\nvalidate our approach, we conducted experiments on multiple tasks, including\nimage classification, semantic segmentation, and text to image generation,\nusing well-established models such as ResNet, DeepLab, VGG16, FCN and Stable\nDiffusion. Experimental results show that NNObfuscator successfully makes model\nmore adaptable, so that a single trained model can handle a broad range of\ntasks without requiring a lot of changes.", "AI": {"tldr": "NNObfuscator\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5b9e\u7528\u63a7\u5236\u673a\u5236\uff0c\u5141\u8bb8\u5355\u4e2aAI\u6a21\u578b\u6839\u636e\u9884\u5b9a\u4e49\u6761\u4ef6\u52a8\u6001\u8c03\u6574\u6027\u80fd\uff0c\u4ece\u800c\u5b9e\u73b0\u5206\u5c42\u8bbf\u95ee\u5e76\u63d0\u9ad8\u8d44\u6e90\u6548\u7387\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u548c\u6a21\u578b\u9002\u5e94\u6027\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u96be\u4ee5\u7528\u5355\u4e00\u6a21\u578b\u6ee1\u8db3\u4e0d\u540c\u5e94\u7528\u7684\u591a\u6837\u5316\u6027\u80fd\u9700\u6c42\uff0c\u4f20\u7edf\u591a\u6a21\u578b\u7248\u672c\u89e3\u51b3\u65b9\u6848\u6548\u7387\u4f4e\u4e0b\u3001\u96be\u4ee5\u7ef4\u62a4\u3002", "method": "\u63d0\u51faNNObfuscator\uff0c\u4e00\u79cd\u5b9e\u7528\u63a7\u5236\u673a\u5236\uff0c\u4f7f\u5355\u4e2aAI\u6a21\u578b\u80fd\u591f\u6839\u636e\u9884\u5b9a\u4e49\u6761\u4ef6\u52a8\u6001\u8c03\u6574\u5176\u6027\u80fd\uff0c\u5b9e\u73b0\u5b9e\u65f6\u9002\u5e94\u548c\u5206\u5c42\u8bbf\u95ee\uff08\u4f8b\u5982\uff0c\u514d\u8d39\u7528\u6237\u4e0e\u9ad8\u7ea7\u7528\u6237\uff09\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u8bed\u4e49\u5206\u5272\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528ResNet\u3001DeepLab\u3001VGG16\u3001FCN\u548cStable Diffusion\u7b49\u6a21\u578b\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNNObfuscator\u6210\u529f\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9002\u5e94\u6027\uff0c\u4f7f\u5355\u4e2a\u8bad\u7ec3\u6a21\u578b\u65e0\u9700\u5927\u91cf\u4fee\u6539\u5373\u53ef\u5904\u7406\u5e7f\u6cdb\u7684\u4efb\u52a1\u3002", "conclusion": "NNObfuscator\u4e3aAI\u6a21\u578b\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5141\u8bb8\u5355\u4e2a\u6a21\u578b\u52a8\u6001\u8c03\u6574\u6027\u80fd\uff0c\u4f18\u5316\u4e86\u8d44\u6e90\u5206\u914d\uff0c\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\uff0c\u5e76\u652f\u6301\u4e86\u53ef\u6301\u7eed\u7684\u4e1a\u52a1\u6a21\u578b\u3002"}}
{"id": "2508.06810", "pdf": "https://arxiv.org/pdf/2508.06810", "abs": "https://arxiv.org/abs/2508.06810", "authors": ["Steven Coyne", "Diana Galvan-Sosa", "Ryan Spring", "Cam\u00e9lia Guerraoui", "Michael Zock", "Keisuke Sakaguchi", "Kentaro Inui"], "title": "Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems", "categories": ["cs.CL"], "comment": "Pre-review version of DOI 10.1007/978-3-031-98459-4_21, presented at\n  AIED 2025. All content is as of submission time except for de-anonymization,\n  ensuing layout fixes, use of the current code repository link, and BibTeX\n  fixes. Readers are encouraged to refer to the published version", "summary": "Recent advances in natural language processing (NLP) have contributed to the\ndevelopment of automated writing evaluation (AWE) systems that can correct\ngrammatical errors. However, while these systems are effective at improving\ntext, they are not optimally designed for language learning. They favor direct\nrevisions, often with a click-to-fix functionality that can be applied without\nconsidering the reason for the correction. Meanwhile, depending on the error\ntype, learners may benefit most from simple explanations and strategically\nindirect hints, especially on generalizable grammatical rules. To support the\ngeneration of such feedback, we introduce an annotation framework that models\neach error's error type and generalizability. For error type classification, we\nintroduce a typology focused on inferring learners' knowledge gaps by\nconnecting their errors to specific grammatical patterns. Following this\nframework, we collect a dataset of annotated learner errors and corresponding\nhuman-written feedback comments, each labeled as a direct correction or hint.\nWith this data, we evaluate keyword-guided, keyword-free, and template-guided\nmethods of generating feedback using large language models (LLMs). Human\nteachers examined each system's outputs, assessing them on grounds including\nrelevance, factuality, and comprehensibility. We report on the development of\nthe dataset and the comparative performance of the systems investigated.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u6559\u5b66\u6027\u8bed\u6cd5\u53cd\u9988\u7684\u6807\u6ce8\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6b64\u7c7b\u53cd\u9988\u7684\u4e0d\u540c\u65b9\u6cd5\uff0c\u65e8\u5728\u6539\u8fdb\u73b0\u6709AWE\u7cfb\u7edf\u5bf9\u8bed\u8a00\u5b66\u4e60\u7684\u652f\u6301\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u5199\u4f5c\u8bc4\u4f30\uff08AWE\uff09\u7cfb\u7edf\u503e\u5411\u4e8e\u76f4\u63a5\u4fee\u6b63\u8bed\u6cd5\u9519\u8bef\uff0c\u800c\u7f3a\u4e4f\u5bf9\u5b66\u4e60\u8005\u6709\u76ca\u7684\u89e3\u91ca\u548c\u95f4\u63a5\u63d0\u793a\uff0c\u8fd9\u4e0d\u5229\u4e8e\u8bed\u8a00\u5b66\u4e60\u8005\u7406\u89e3\u9519\u8bef\u539f\u56e0\u548c\u6cdb\u5316\u8bed\u6cd5\u89c4\u5219\u3002", "method": "1. \u5f15\u5165\u4e00\u4e2a\u6807\u6ce8\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u9519\u8bef\u7c7b\u578b\u548c\u5176\u53ef\u6cdb\u5316\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u9519\u8bef\u7c7b\u578b\u5206\u7c7b\u6cd5\u4ee5\u8bc6\u522b\u5b66\u4e60\u8005\u7684\u77e5\u8bc6\u7f3a\u9677\u30022. \u57fa\u4e8e\u6b64\u6846\u67b6\u6536\u96c6\u4e86\u4e00\u4e2a\u5305\u542b\u5df2\u6807\u6ce8\u5b66\u4e60\u8005\u9519\u8bef\u53ca\u76f8\u5e94\u4eba\u5de5\u53cd\u9988\uff08\u76f4\u63a5\u4fee\u6b63\u6216\u63d0\u793a\uff09\u7684\u6570\u636e\u96c6\u30023. \u5229\u7528\u8be5\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5173\u952e\u8bcd\u5f15\u5bfc\u3001\u65e0\u5173\u952e\u8bcd\u548c\u6a21\u677f\u5f15\u5bfc\u7b49\u53cd\u9988\u751f\u6210\u65b9\u6cd5\u30024. \u7531\u4eba\u7c7b\u6559\u5e08\u5bf9\u5404\u7cfb\u7edf\u8f93\u51fa\u7684\u76f8\u5173\u6027\u3001\u4e8b\u5b9e\u6027\u548c\u53ef\u7406\u89e3\u6027\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bba\u6587\u62a5\u544a\u4e86\u6240\u5f00\u53d1\u6570\u636e\u96c6\u7684\u6784\u5efa\u8fc7\u7a0b\u4ee5\u53ca\u6240\u7814\u7a76\u7684\u5404\u7c7b\u53cd\u9988\u751f\u6210\u7cfb\u7edf\u7684\u6bd4\u8f83\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u63d0\u51fa\u521b\u65b0\u6846\u67b6\u3001\u6784\u5efa\u4e13\u4e1a\u6570\u636e\u96c6\u5e76\u8bc4\u4f30\u57fa\u4e8eLLMs\u7684\u53cd\u9988\u751f\u6210\u65b9\u6cd5\uff0c\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u5730\u652f\u6301\u8bed\u8a00\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u5199\u4f5c\u8bc4\u4f30\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u548c\u89c1\u89e3\u3002"}}
{"id": "2508.08225", "pdf": "https://arxiv.org/pdf/2508.08225", "abs": "https://arxiv.org/abs/2508.08225", "authors": ["Mansoor Shafi", "Erik G. Larsson", "Xingqin Lin", "Dorin Panaitopol", "Stefan Parkvall", "Flavien Ronteix-Jacquet", "Antti Toskala"], "title": "Industrial Viewpoints on RAN Technologies for 6G", "categories": ["cs.NI", "cs.IT", "eess.SP", "math.IT"], "comment": "submitted to the Proceedings of the IEEE", "summary": "6G standardization is to start imminently, with commercial deployments\nexpected before 2030. Its technical components and performance requirements are\nthe focus of this article. Our emphasis is on the 6G radio access, especially\nMIMO, AI, waveforms, coding, signal constellations and integration with\nnon-terrestrial networks. Whilst standardization has not yet formally started,\nthe scope of the 6G study items has been defined. Our predictions in this paper\nare speculative as there are no results of the study yet, but our views are\nguided by implementation and deployment aspects. We expect that the views here\nwill guide researchers and industry practitioners.", "AI": {"tldr": "\u672c\u6587\u5c55\u671b6G\u6807\u51c6\u5316\u548c\u90e8\u7f72\uff0c\u91cd\u70b9\u5206\u6790\u5176\u6280\u672f\u7ec4\u6210\u548c\u6027\u80fd\u8981\u6c42\uff0c\u7279\u522b\u662f\u65e0\u7ebf\u63a5\u5165\u6280\u672f\u3002", "motivation": "6G\u6807\u51c6\u5316\u5373\u5c06\u542f\u52a8\uff0c\u6587\u7ae0\u65e8\u5728\u63a2\u8ba8\u5176\u6280\u672f\u7ec4\u6210\u548c\u6027\u80fd\u8981\u6c42\uff0c\u4ee5\u5e94\u5bf9\u672a\u6765\u5546\u4e1a\u90e8\u7f72\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5f3a\u8c03\u548c\u63a2\u8ba86G\u65e0\u7ebf\u63a5\u5165\u6280\u672f\uff08\u5982MIMO\u3001AI\u3001\u6ce2\u5f62\u3001\u7f16\u7801\u3001\u4fe1\u53f7\u661f\u5ea7\u548c\u4e0e\u975e\u5730\u9762\u7f51\u7edc\u7684\u96c6\u6210\uff09\uff0c\u5e76\u7ed3\u5408\u5bf9\u5b9e\u65bd\u548c\u90e8\u7f72\u65b9\u9762\u7684\u8003\u91cf\uff0c\u63d0\u51fa\u9884\u6d4b\u6027\u89c2\u70b9\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u5173\u4e8e6G\u6280\u672f\u7684\u63a8\u6d4b\u6027\u9884\u6d4b\u548c\u89c2\u70b9\uff0c\u8fd9\u4e9b\u89c2\u70b9\u57fa\u4e8e\u5bf9\u672a\u6765\u5b9e\u65bd\u548c\u90e8\u7f72\u7684\u8003\u91cf\uff0c\u800c\u975e\u73b0\u6709\u7814\u7a76\u7684\u6b63\u5f0f\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u7684\u89c2\u70b9\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u884c\u4e1a\u4ece\u4e1a\u8005\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2508.06638", "pdf": "https://arxiv.org/pdf/2508.06638", "abs": "https://arxiv.org/abs/2508.06638", "authors": ["Muyan Anna Li", "Aditi Gautam"], "title": "Segmented Confidence Sequences and Multi-Scale Adaptive Confidence Segments for Anomaly Detection in Nonstationary Time Series", "categories": ["cs.LG", "cs.AI", "14J60 (Primary) 14F05, 14J26 (Secondary)", "F.2.2; I.2.0"], "comment": "20 pages, 11 figures", "summary": "As time series data become increasingly prevalent in domains such as\nmanufacturing, IT, and infrastructure monitoring, anomaly detection must adapt\nto nonstationary environments where statistical properties shift over time.\nTraditional static thresholds are easily rendered obsolete by regime shifts,\nconcept drift, or multi-scale changes. To address these challenges, we\nintroduce and empirically evaluate two novel adaptive thresholding frameworks:\nSegmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence\nSegments (MACS). Both leverage statistical online learning and segmentation\nprinciples for local, contextually sensitive adaptation, maintaining guarantees\non false alarm rates even under evolving distributions. Our experiments across\nWafer Manufacturing benchmark datasets show significant F1-score improvement\ncompared to traditional percentile and rolling quantile approaches. This work\ndemonstrates that robust, statistically principled adaptive thresholds enable\nreliable, interpretable, and timely detection of diverse real-world anomalies.", "AI": {"tldr": "\u9488\u5bf9\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u578b\u81ea\u9002\u5e94\u9608\u503c\u6846\u67b6SCS\u548cMACS\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u548c\u5206\u6bb5\u539f\u7406\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5728\u5236\u9020\u4e1a\u3001IT\u548c\u57fa\u7840\u8bbe\u65bd\u76d1\u63a7\u4e2d\u65e5\u76ca\u666e\u904d\uff0c\u4f46\u5176\u975e\u5e73\u7a33\u7279\u6027\u5bfc\u81f4\u4f20\u7edf\u9759\u6001\u9608\u503c\u5728\u9762\u5bf9\u72b6\u6001\u53d8\u5316\u3001\u6982\u5ff5\u6f02\u79fb\u6216\u591a\u5c3a\u5ea6\u53d8\u5316\u65f6\u5931\u6548\uff0c\u4e9f\u9700\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u5e76\u5b9e\u8bc1\u8bc4\u4f30\u4e86\u4e24\u79cd\u65b0\u578b\u81ea\u9002\u5e94\u9608\u503c\u6846\u67b6\uff1a\u5206\u6bb5\u7f6e\u4fe1\u5e8f\u5217\uff08Segmented Confidence Sequences, SCS\uff09\u548c\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u7f6e\u4fe1\u5206\u6bb5\uff08Multi-Scale Adaptive Confidence Segments, MACS\uff09\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5747\u5229\u7528\u7edf\u8ba1\u5728\u7ebf\u5b66\u4e60\u548c\u5206\u6bb5\u539f\u7406\u5b9e\u73b0\u5c40\u90e8\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u81ea\u9002\u5e94\uff0c\u5373\u4f7f\u5728\u6570\u636e\u5206\u5e03\u6f14\u53d8\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u6301\u8bef\u62a5\u7387\u7684\u4fdd\u8bc1\u3002", "result": "\u5728\u6676\u5706\u5236\u9020\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u767e\u5206\u4f4d\u548c\u6eda\u52a8\u5206\u4f4d\u6570\u65b9\u6cd5\u76f8\u6bd4\uff0cSCS\u548cMACS\u5728F1\u5206\u6570\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9c81\u68d2\u4e14\u57fa\u4e8e\u7edf\u8ba1\u539f\u7406\u7684\u81ea\u9002\u5e94\u9608\u503c\u80fd\u591f\u5b9e\u73b0\u5bf9\u5404\u7c7b\u771f\u5b9e\u4e16\u754c\u5f02\u5e38\u7684\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u548c\u53ca\u65f6\u68c0\u6d4b\u3002"}}
{"id": "2508.06832", "pdf": "https://arxiv.org/pdf/2508.06832", "abs": "https://arxiv.org/abs/2508.06832", "authors": ["Haifeng Li", "Wang Guo", "Haiyang Wu", "Mengwei Wu", "Jipeng Zhang", "Qing Zhu", "Yu Liu", "Xin Huang", "Chao Tao"], "title": "Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges", "categories": ["cs.AI"], "comment": null, "summary": "The mainstream paradigm of remote sensing image interpretation has long been\ndominated by vision-centered models, which rely on visual features for semantic\nunderstanding. However, these models face inherent limitations in handling\nmulti-modal reasoning, semantic abstraction, and interactive decision-making.\nWhile recent advances have introduced Large Language Models (LLMs) into remote\nsensing workflows, existing studies primarily focus on downstream applications,\nlacking a unified theoretical framework that explains the cognitive role of\nlanguage. This review advocates a paradigm shift from vision-centered to\nlanguage-centered remote sensing interpretation. Drawing inspiration from the\nGlobal Workspace Theory (GWT) of human cognition, We propose a\nlanguage-centered framework for remote sensing interpretation that treats LLMs\nas the cognitive central hub integrating perceptual, task, knowledge and action\nspaces to enable unified understanding, reasoning, and decision-making. We\nfirst explore the potential of LLMs as the central cognitive component in\nremote sensing interpretation, and then summarize core technical challenges,\nincluding unified multimodal representation, knowledge association, and\nreasoning and decision-making. Furthermore, we construct a global\nworkspace-driven interpretation mechanism and review how language-centered\nsolutions address each challenge. Finally, we outline future research\ndirections from four perspectives: adaptive alignment of multimodal data, task\nunderstanding under dynamic knowledge constraints, trustworthy reasoning, and\nautonomous interaction. This work aims to provide a conceptual foundation for\nthe next generation of remote sensing interpretation systems and establish a\nroadmap toward cognition-driven intelligent geospatial analysis.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u63d0\u51fa\u4e00\u79cd\u8bed\u8a00\u4e2d\u5fc3\u5316\u7684\u9065\u611f\u56fe\u50cf\u89e3\u8bd1\u65b0\u8303\u5f0f\uff0c\u501f\u9274\u4eba\u7c7b\u8ba4\u77e5\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u7406\u8bba\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u8ba4\u77e5\u4e2d\u5fc3\u67a2\u7ebd\uff0c\u65e8\u5728\u514b\u670d\u4f20\u7edf\u89c6\u89c9\u4e2d\u5fc3\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u76f8\u5173\u6280\u672f\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4e3b\u6d41\u7684\u9065\u611f\u56fe\u50cf\u89e3\u8bd1\u8303\u5f0f\u4ee5\u89c6\u89c9\u4e2d\u5fc3\u6a21\u578b\u4e3a\u4e3b\uff0c\u4f46\u5728\u5904\u7406\u591a\u6a21\u6001\u63a8\u7406\u3001\u8bed\u4e49\u62bd\u8c61\u548c\u4ea4\u4e92\u5f0f\u51b3\u7b56\u65b9\u9762\u5b58\u5728\u56fa\u6709\u9650\u5236\u3002\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u5e94\u7528\u4e8e\u9065\u611f\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u89e3\u91ca\u8bed\u8a00\u8ba4\u77e5\u4f5c\u7528\u7684\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u3002", "method": "\u5021\u5bfc\u4ece\u89c6\u89c9\u4e2d\u5fc3\u5411\u8bed\u8a00\u4e2d\u5fc3\u7684\u9065\u611f\u89e3\u8bd1\u8303\u5f0f\u8f6c\u53d8\u3002\u63d0\u51fa\u4e00\u4e2a\u4ee5LLMs\u4e3a\u8ba4\u77e5\u4e2d\u5fc3\u67a2\u7ebd\u7684\u8bed\u8a00\u4e2d\u5fc3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u611f\u77e5\u3001\u4efb\u52a1\u3001\u77e5\u8bc6\u548c\u884c\u52a8\u7a7a\u95f4\uff0c\u53d7\u4eba\u7c7b\u8ba4\u77e5\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u7406\u8bba\u542f\u53d1\u3002\u6587\u7ae0\u63a2\u8ba8\u4e86LLMs\u4f5c\u4e3a\u6838\u5fc3\u8ba4\u77e5\u7ec4\u4ef6\u7684\u6f5c\u529b\uff0c\u603b\u7ed3\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u8868\u793a\u3001\u77e5\u8bc6\u5173\u8054\u3001\u63a8\u7406\u4e0e\u51b3\u7b56\u7b49\u6280\u672f\u6311\u6218\uff0c\u5e76\u6784\u5efa\u4e86\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u9a71\u52a8\u7684\u89e3\u8bd1\u673a\u5236\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "result": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8bed\u8a00\u4e2d\u5fc3\u5316\u9065\u611f\u89e3\u8bd1\u6846\u67b6\uff0c\u603b\u7ed3\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u6838\u5fc3\u6280\u672f\u6311\u6218\uff0c\u5e76\u56de\u987e\u4e86\u8bed\u8a00\u4e2d\u5fc3\u5316\u89e3\u51b3\u65b9\u6848\u5982\u4f55\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002\u5b83\u4e3a\u672a\u6765\u7684\u9065\u611f\u89e3\u8bd1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\u3002", "conclusion": "\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u4e0b\u4e00\u4ee3\u9065\u611f\u89e3\u8bd1\u7cfb\u7edf\u63d0\u4f9b\u6982\u5ff5\u57fa\u7840\uff0c\u5e76\u4e3a\u8ba4\u77e5\u9a71\u52a8\u7684\u667a\u80fd\u5730\u7406\u7a7a\u95f4\u5206\u6790\u5efa\u7acb\u8def\u7ebf\u56fe\u3002\u63d0\u51fa\u4e86\u56db\u4e2a\u672a\u6765\u7814\u7a76\u65b9\u5411\uff1a\u591a\u6a21\u6001\u6570\u636e\u81ea\u9002\u5e94\u5bf9\u9f50\u3001\u52a8\u6001\u77e5\u8bc6\u7ea6\u675f\u4e0b\u7684\u4efb\u52a1\u7406\u89e3\u3001\u53ef\u4fe1\u63a8\u7406\u548c\u81ea\u4e3b\u4ea4\u4e92\u3002"}}
{"id": "2508.06552", "pdf": "https://arxiv.org/pdf/2508.06552", "abs": "https://arxiv.org/abs/2508.06552", "authors": ["Unisha Joshi"], "title": "Age-Diverse Deepfake Dataset: Bridging the Age Gap in Deepfake Detection", "categories": ["cs.CV", "cs.LG"], "comment": "11 pages, 4 figures, and 7 tables", "summary": "The challenges associated with deepfake detection are increasing\nsignificantly with the latest advancements in technology and the growing\npopularity of deepfake videos and images. Despite the presence of numerous\ndetection models, demographic bias in the deepfake dataset remains largely\nunaddressed. This paper focuses on the mitigation of age-specific bias in the\ndeepfake dataset by introducing an age-diverse deepfake dataset that will\nimprove fairness across age groups. The dataset is constructed through a\nmodular pipeline incorporating the existing deepfake datasets Celeb-DF,\nFaceForensics++, and UTKFace datasets, and the creation of synthetic data to\nfill the age distribution gaps. The effectiveness and generalizability of this\ndataset are evaluated using three deepfake detection models: XceptionNet,\nEfficientNet, and LipForensics. Evaluation metrics, including AUC, pAUC, and\nEER, revealed that models trained on the age-diverse dataset demonstrated\nfairer performance across age groups, improved overall accuracy, and higher\ngeneralization across datasets. This study contributes a reproducible,\nfairness-aware deepfake dataset and model pipeline that can serve as a\nfoundation for future research in fairer deepfake detection. The complete\ndataset and implementation code are available at\nhttps://github.com/unishajoshi/age-diverse-deepfake-detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u5e74\u9f84\u591a\u6837\u5316\u7684\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u5e74\u9f84\u7279\u5f02\u6027\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u7684\u516c\u5e73\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u53d1\u5c55\uff0c\u5176\u68c0\u6d4b\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u666e\u904d\u5b58\u5728\u6570\u636e\u96c6\u4e2d\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u5dee\uff0c\u7279\u522b\u662f\u5e74\u9f84\u7279\u5f02\u6027\u504f\u5dee\uff0c\u8fd9\u5bfc\u81f4\u68c0\u6d4b\u516c\u5e73\u6027\u4e0d\u8db3\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5e74\u9f84\u591a\u6837\u5316\u7684\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u6574\u5408\u4e86Celeb-DF\u3001FaceForensics++\u548cUTKFace\u7b49\u73b0\u6709\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u6765\u586b\u8865\u5e74\u9f84\u5206\u5e03\u7684\u7a7a\u767d\u3002\u4f7f\u7528XceptionNet\u3001EfficientNet\u548cLipForensics\u4e09\u79cd\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7AUC\u3001pAUC\u548cEER\u7b49\u6307\u6807\u8bc4\u4f30\u4e86\u8be5\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5e74\u9f84\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5728\u4e0d\u540c\u5e74\u9f84\u7ec4\u95f4\u8868\u73b0\u51fa\u66f4\u516c\u5e73\u7684\u6027\u80fd\uff0c\u4e14\u6574\u4f53\u51c6\u786e\u6027\u66f4\u9ad8\uff0c\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u4e5f\u66f4\u5f3a\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u3001\u5177\u5907\u516c\u5e73\u6027\u610f\u8bc6\u7684\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\u548c\u6a21\u578b\u7ba1\u9053\uff0c\u4e3a\u672a\u6765\u66f4\u516c\u5e73\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06870", "pdf": "https://arxiv.org/pdf/2508.06870", "abs": "https://arxiv.org/abs/2508.06870", "authors": ["Gangular Singh Irengbam", "Nirvash Singh Wahengbam", "Lanthoiba Meitei Khumanthem", "Paikhomba Oinam"], "title": "Text to Speech System for Meitei Mayek Script", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "This paper presents the development of a Text-to-Speech (TTS) system for the\nManipuri language using the Meitei Mayek script. Leveraging Tacotron 2 and\nHiFi-GAN, we introduce a neural TTS architecture adapted to support tonal\nphonology and under-resourced linguistic environments. We develop a phoneme\nmapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and\ndemonstrate intelligible and natural speech synthesis, validated through\nsubjective and objective metrics. This system lays the groundwork for\nlinguistic preservation and technological inclusion of Manipuri.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u4e3a\u6885\u6cf0\u6587\uff08Meitei Mayek\uff09\u4e66\u5199\u7684\u66fc\u5c3c\u666e\u5c14\u8bed\uff08Manipuri\uff09\u5f00\u53d1\u7684\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u7cfb\u7edf\uff0c\u5229\u7528Tacotron 2\u548cHiFi-GAN\u5b9e\u73b0\u4e86\u6e05\u6670\u81ea\u7136\u7684\u8bed\u97f3\u5408\u6210\u3002", "motivation": "\u4e3a\u652f\u6301\u5177\u6709\u58f0\u8c03\u97f3\u7cfb\u7684\u8d44\u6e90\u7a00\u7f3a\u8bed\u8a00\u73af\u5883\uff0c\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5b9e\u73b0\u8bed\u97f3\u4fdd\u5b58\u548c\u6280\u672f\u5305\u5bb9\u7684\u66fc\u5c3c\u666e\u5c14\u8bedTTS\u7cfb\u7edf\u3002", "method": "\u5229\u7528Tacotron 2\u548cHiFi-GAN\u6784\u5efa\u4e86\u4e00\u4e2a\u795e\u7ecfTTS\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u9002\u5e94\u58f0\u8c03\u97f3\u7cfb\u548c\u8d44\u6e90\u7a00\u7f3a\u73af\u5883\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u5f00\u53d1\u6885\u6cf0\u6587\u5230ARPAbet\u7684\u97f3\u7d20\u6620\u5c04\uff0c\u5e76\u6574\u7406\u4e86\u4e00\u4e2a\u5355\u8bf4\u8bdd\u4eba\u6570\u636e\u96c6\u3002", "result": "\u6210\u529f\u5c55\u793a\u4e86\u6e05\u6670\u81ea\u7136\u7684\u8bed\u97f3\u5408\u6210\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u4e3b\u89c2\u548c\u5ba2\u89c2\u6307\u6807\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u66fc\u5c3c\u666e\u5c14\u8bed\u7684\u8bed\u8a00\u4fdd\u5b58\u548c\u6280\u672f\u5305\u5bb9\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.07555", "pdf": "https://arxiv.org/pdf/2508.07555", "abs": "https://arxiv.org/abs/2508.07555", "authors": ["Keyuan Zhang", "Yin Sun", "Bo Ji"], "title": "Multimodal Remote Inference", "categories": ["cs.LG", "cs.IT", "cs.NI", "math.IT"], "comment": "Accepted by The 22nd IEEE International Conference on Mobile Ad-Hoc\n  and Smart Systems (MASS 2025)", "summary": "We consider a remote inference system with multiple modalities, where a\nmultimodal machine learning (ML) model performs real-time inference using\nfeatures collected from remote sensors. As sensor observations may change\ndynamically over time, fresh features are critical for inference tasks.\nHowever, timely delivering features from all modalities is often infeasible due\nto limited network resources. To this end, we study a two-modality scheduling\nproblem to minimize the ML model's inference error, which is expressed as a\npenalty function of AoI for both modalities. We develop an index-based\nthreshold policy and prove its optimality. Specifically, the scheduler switches\nmodalities when the current modality's index function exceeds a threshold. We\nshow that the two modalities share the same threshold, and both the index\nfunctions and the threshold can be computed efficiently. The optimality of our\npolicy holds for (i) general AoI functions that are \\emph{non-monotonic} and\n\\emph{non-additive} and (ii) \\emph{heterogeneous} transmission times. Numerical\nresults show that our policy reduces inference error by up to 55% compared to\nround-robin and uniform random policies, which are oblivious to the AoI-based\ninference error function. Our results shed light on how to improve remote\ninference accuracy by optimizing task-oriented AoI functions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6700\u4f18\u7684\u57fa\u4e8e\u7d22\u5f15\u9608\u503c\u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u4ee5\u6700\u5c0f\u5316\u591a\u6a21\u6001\u8fdc\u7a0b\u63a8\u7406\u7cfb\u7edf\u4e2d\u56e0\u7f51\u7edc\u8d44\u6e90\u53d7\u9650\u5bfc\u81f4\u7684\u6570\u636e\u4e0d\u65b0\u9c9c\u9020\u6210\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63a8\u7406\u8bef\u5dee\uff0c\u5e76\u5728\u5e7f\u6cdb\u6761\u4ef6\u4e0b\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u591a\u6a21\u6001\u8fdc\u7a0b\u63a8\u7406\u7cfb\u7edf\u4e2d\uff0c\u5b9e\u65f6\u63a8\u7406\u4f9d\u8d56\u4e8e\u4ece\u8fdc\u7a0b\u4f20\u611f\u5668\u6536\u96c6\u7684\u7279\u5f81\u3002\u7531\u4e8e\u4f20\u611f\u5668\u6570\u636e\u52a8\u6001\u53d8\u5316\uff0c\u7279\u5f81\u7684\u65b0\u9c9c\u5ea6\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u6709\u9650\u7684\u7f51\u7edc\u8d44\u6e90\u4f7f\u5f97\u53ca\u65f6\u4f20\u8f93\u6240\u6709\u6a21\u6001\u7684\u7279\u5f81\u53d8\u5f97\u4e0d\u53ef\u884c\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4ea7\u751f\u63a8\u7406\u8bef\u5dee\u3002", "method": "\u7814\u7a76\u4e86\u4e00\u4e2a\u53cc\u6a21\u6001\u8c03\u5ea6\u95ee\u9898\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u63a8\u7406\u8bef\u5dee\uff0c\u8be5\u8bef\u5dee\u88ab\u8868\u8fbe\u4e3a\u4e24\u79cd\u6a21\u6001AoI\uff08\u4fe1\u606f\u65b0\u9c9c\u5ea6\uff09\u7684\u60e9\u7f5a\u51fd\u6570\u3002\u4e3a\u6b64\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7d22\u5f15\u7684\u9608\u503c\u7b56\u7565\u5e76\u8bc1\u660e\u4e86\u5176\u6700\u4f18\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8c03\u5ea6\u5668\u5728\u5f53\u524d\u6a21\u6001\u7684\u7d22\u5f15\u51fd\u6570\u8d85\u8fc7\u67d0\u4e2a\u9608\u503c\u65f6\u5207\u6362\u6a21\u6001\u3002\u8be5\u7b56\u7565\u9002\u7528\u4e8e\u975e\u5355\u8c03\u3001\u975e\u53ef\u52a0\u7684\u4e00\u822cAoI\u51fd\u6570\u4ee5\u53ca\u5f02\u6784\u4f20\u8f93\u65f6\u95f4\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4e0d\u8003\u8651AoI\u63a8\u7406\u8bef\u5dee\u51fd\u6570\u7684\u8f6e\u8be2\u548c\u5747\u5300\u968f\u673a\u7b56\u7565\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u7b56\u7565\u80fd\u591f\u5c06\u63a8\u7406\u8bef\u5dee\u964d\u4f4e\u9ad8\u8fbe55%\u3002", "conclusion": "\u672c\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u9762\u5411\u4efb\u52a1\u7684AoI\u51fd\u6570\u6765\u63d0\u9ad8\u8fdc\u7a0b\u63a8\u7406\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u89e3\u51b3\u8fdc\u7a0b\u63a8\u7406\u4e2d\u7684\u6570\u636e\u65b0\u9c9c\u5ea6\u4e0e\u51c6\u786e\u6027\u6743\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.06641", "pdf": "https://arxiv.org/pdf/2508.06641", "abs": "https://arxiv.org/abs/2508.06641", "authors": ["Jonas S Almeida", "Daniel E Russ", "Susana Vinga", "Ines Duarte", "Lee Mason", "Praphulla Bhawsar", "Aaron Ge", "Arlindo Oliveira", "Jeya Balaji Balasubramanian"], "title": "Fractal Language Modelling by Universal Sequence Maps (USM)", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "q-bio.QM"], "comment": "16 pages, 8 figures", "summary": "Motivation: With the advent of Language Models using Transformers,\npopularized by ChatGPT, there is a renewed interest in exploring encoding\nprocedures that numerically represent symbolic sequences at multiple scales and\nembedding dimensions. The challenge that encoding addresses is the need for\nmechanisms that uniquely retain contextual information about the succession of\nindividual symbols, which can then be modeled by nonlinear formulations such as\nneural networks.\n  Context: Universal Sequence Maps(USM) are iterated functions that bijectively\nencode symbolic sequences onto embedded numerical spaces. USM is composed of\ntwo Chaos Game Representations (CGR), iterated forwardly and backwardly, that\ncan be projected into the frequency domain (FCGR). The corresponding USM\ncoordinates can be used to compute a Chebyshev distance metric as well as k-mer\nfrequencies, without having to recompute the embedded numeric coordinates, and,\nparadoxically, allowing for non-integers values of k.\n  Results: This report advances the bijective fractal encoding by Universal\nSequence Maps (USM) by resolving seeding biases affecting the iterated process.\nThe resolution had two results, the first expected, the second an intriguing\noutcome: 1) full reconciliation of numeric positioning with sequence identity;\nand 2) uncovering the nature of USM as an efficient numeric process converging\ntowards a steady state sequence embedding solution. We illustrate these results\nfor genomic sequences because of the convenience of a planar representation\ndefined by an alphabet with only 4 tokens (the 4 nucleotides). Nevertheless,\nthe application to alphabet of arbitrary cardinality was found to be\nstraightforward.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u89e3\u51b3\u901a\u7528\u5e8f\u5217\u56fe\uff08USM\uff09\u7f16\u7801\u4e2d\u7684\u79cd\u5b50\u504f\u5dee\uff0c\u63d0\u5347\u4e86\u5176\u5bf9\u7b26\u53f7\u5e8f\u5217\u7684\u751f\u7269\u5c04\u9891\u5206\u5f62\u7f16\u7801\u80fd\u529b\uff0c\u63ed\u793a\u4e86USM\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u3001\u6536\u655b\u7684\u6570\u503c\u8fc7\u7a0b\uff0c\u53ef\u5b9e\u73b0\u6570\u503c\u5b9a\u4f4d\u4e0e\u5e8f\u5217\u8eab\u4efd\u7684\u5b8c\u5168\u4e00\u81f4\u3002", "motivation": "\u9274\u4e8e\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\uff09\u7684\u5174\u8d77\uff0c\u4eba\u4eec\u5bf9\u80fd\u4ee5\u591a\u5c3a\u5ea6\u548c\u591a\u5d4c\u5165\u7ef4\u5ea6\u5bf9\u7b26\u53f7\u5e8f\u5217\u8fdb\u884c\u6570\u503c\u8868\u793a\u7684\u7f16\u7801\u7a0b\u5e8f\u91cd\u65b0\u4ea7\u751f\u4e86\u5174\u8da3\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u9700\u8981\u4e00\u79cd\u80fd\u72ec\u7279\u4fdd\u7559\u5355\u4e2a\u7b26\u53f7\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u673a\u5236\uff0c\u4ee5\u4fbf\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u7b49\u975e\u7ebf\u6027\u6a21\u578b\u8fdb\u884c\u5efa\u6a21\u3002", "method": "\u672c\u6587\u57fa\u4e8e\u901a\u7528\u5e8f\u5217\u56fe\uff08USM\uff09\u8fdb\u884c\u7814\u7a76\uff0cUSM\u662f\u4e00\u79cd\u8fed\u4ee3\u51fd\u6570\uff0c\u901a\u8fc7\u524d\u540e\u5411\u6df7\u6c8c\u6e38\u620f\u8868\u793a\uff08CGR\uff09\u5c06\u7b26\u53f7\u5e8f\u5217\u53cc\u5c04\u7f16\u7801\u5230\u6570\u503c\u7a7a\u95f4\u3002\u672c\u62a5\u544a\u7684\u6838\u5fc3\u65b9\u6cd5\u662f\u89e3\u51b3\u5f71\u54cdUSM\u8fed\u4ee3\u8fc7\u7a0b\u7684\u79cd\u5b50\u504f\u5dee\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u5176\u7f16\u7801\u51c6\u786e\u6027\u3002", "result": "\u89e3\u51b3\u4e86USM\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u7684\u79cd\u5b50\u504f\u5dee\u95ee\u9898\uff0c\u83b7\u5f97\u4e86\u4e24\u9879\u4e3b\u8981\u7ed3\u679c\uff1a1\uff09\u5b9e\u73b0\u4e86\u6570\u503c\u5b9a\u4f4d\u4e0e\u5e8f\u5217\u8eab\u4efd\u7684\u5b8c\u5168\u4e00\u81f4\uff1b2\uff09\u63ed\u793a\u4e86USM\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u503c\u8fc7\u7a0b\uff0c\u53ef\u6536\u655b\u5230\u7a33\u6001\u5e8f\u5217\u5d4c\u5165\u89e3\u7684\u672c\u8d28\u3002\u8fd9\u4e9b\u7ed3\u679c\u5728\u57fa\u56e0\u7ec4\u5e8f\u5217\u4e0a\u8fdb\u884c\u4e86\u6f14\u793a\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5bf9\u4efb\u610f\u57fa\u6570\u5b57\u6bcd\u8868\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u89e3\u51b3\u79cd\u5b50\u504f\u5dee\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901a\u7528\u5e8f\u5217\u56fe\uff08USM\uff09\u7684\u7f16\u7801\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002USM\u73b0\u5728\u80fd\u591f\u5c06\u6570\u503c\u5b9a\u4f4d\u4e0e\u5e8f\u5217\u8eab\u4efd\u5b8c\u7f8e\u5339\u914d\uff0c\u5e76\u88ab\u8bc1\u660e\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u6536\u655b\u7684\u7b26\u53f7\u5e8f\u5217\u5d4c\u5165\u65b9\u6cd5\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5904\u7406\u7b26\u53f7\u5e8f\u5217\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u8868\u793a\u3002"}}
{"id": "2508.06836", "pdf": "https://arxiv.org/pdf/2508.06836", "abs": "https://arxiv.org/abs/2508.06836", "authors": ["Xutong Zhao", "Yaqi Xie"], "title": "Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning", "categories": ["cs.AI"], "comment": "Accepted at AISTATS 2025", "summary": "Cooperative multi-agent reinforcement learning (MARL) aims to coordinate\nmultiple agents to achieve a common goal. A key challenge in MARL is credit\nassignment, which involves assessing each agent's contribution to the shared\nreward. Given the diversity of tasks, agents may perform different types of\ncoordination, with rewards attributed to diverse and often overlapping agent\nsubsets. In this work, we formalize the credit assignment level as the number\nof agents cooperating to obtain a reward, and address scenarios with multiple\ncoexisting levels. We introduce a multi-level advantage formulation that\nperforms explicit counterfactual reasoning to infer credits across distinct\nlevels. Our method, Multi-level Advantage Credit Assignment (MACA), captures\nagent contributions at multiple levels by integrating advantage functions that\nreason about individual, joint, and correlated actions. Utilizing an\nattention-based framework, MACA identifies correlated agent relationships and\nconstructs multi-level advantages to guide policy learning. Comprehensive\nexperiments on challenging Starcraft v1\\&v2 tasks demonstrate MACA's superior\nperformance, underscoring its efficacy in complex credit assignment scenarios.", "AI": {"tldr": "\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5f15\u5165MACA\u65b9\u6cd5\u901a\u8fc7\u591a\u7ea7\u4f18\u52bf\u51fd\u6570\u548c\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u4fe1\u7528\u5206\u914d\u96be\u9898\uff0c\u6709\u6548\u8bc6\u522b\u548c\u8bc4\u4f30\u4e0d\u540c\u534f\u4f5c\u5c42\u7ea7\u7684\u8d21\u732e\u3002", "motivation": "\u5728\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u51c6\u786e\u8bc4\u4f30\u6bcf\u4e2a\u667a\u80fd\u4f53\u5bf9\u5171\u4eab\u5956\u52b1\u7684\u8d21\u732e\uff08\u4fe1\u7528\u5206\u914d\uff09\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u5f53\u534f\u4f5c\u7c7b\u578b\u591a\u6837\u4e14\u5956\u52b1\u5f52\u56e0\u4e8e\u4e0d\u540c\u6216\u91cd\u53e0\u7684\u667a\u80fd\u4f53\u5b50\u96c6\u65f6\u3002", "method": "\u63d0\u51faMACA\uff08Multi-level Advantage Credit Assignment\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f62\u5f0f\u5316\u4fe1\u7528\u5206\u914d\u5c42\u7ea7\uff08\u534f\u4f5c\u667a\u80fd\u4f53\u6570\u91cf\uff09\uff0c\u5f15\u5165\u591a\u7ea7\u4f18\u52bf\u51fd\u6570\u8fdb\u884c\u660e\u786e\u7684\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u4ee5\u63a8\u65ad\u4e0d\u540c\u5c42\u7ea7\u7684\u8d21\u732e\u3002MACA\u6574\u5408\u4e86\u5bf9\u4e2a\u4f53\u3001\u8054\u5408\u548c\u5173\u8054\u884c\u4e3a\u7684\u4f18\u52bf\u51fd\u6570\uff0c\u5e76\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u8bc6\u522b\u5173\u8054\u667a\u80fd\u4f53\u5173\u7cfb\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684Starcraft v1\u548cv2\u4efb\u52a1\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cMACA\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MACA\u5728\u590d\u6742\u4fe1\u7528\u5206\u914d\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2508.06553", "pdf": "https://arxiv.org/pdf/2508.06553", "abs": "https://arxiv.org/abs/2508.06553", "authors": ["Jiahao Xiao", "Jianbo Zhang", "BoWen Yan", "Shengyu Guo", "Tongrui Ye", "Kaiwei Zhang", "Zicheng Zhang", "Xiaohong Liu", "Zhengxue Cheng", "Lei Fan", "Chuyi Li", "Guangtao Zhai"], "title": "Static and Plugged: Make Embodied Evaluation Simple", "categories": ["cs.CV"], "comment": null, "summary": "Embodied intelligence is advancing rapidly, driving the need for efficient\nevaluation. Current benchmarks typically rely on interactive simulated\nenvironments or real-world setups, which are costly, fragmented, and hard to\nscale. To address this, we introduce StaticEmbodiedBench, a plug-and-play\nbenchmark that enables unified evaluation using static scene representations.\nCovering 42 diverse scenarios and 8 core dimensions, it supports scalable and\ncomprehensive assessment through a simple interface. Furthermore, we evaluate\n19 Vision-Language Models (VLMs) and 11 Vision-Language-Action models (VLAs),\nestablishing the first unified static leaderboard for Embodied intelligence.\nMoreover, we release a subset of 200 samples from our benchmark to accelerate\nthe development of embodied intelligence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faStaticEmbodiedBench\uff0c\u4e00\u4e2a\u57fa\u4e8e\u9759\u6001\u573a\u666f\u8868\u793a\u7684\u5373\u63d2\u5373\u7528\u57fa\u51c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5177\u8eab\u667a\u80fd\u8bc4\u4f30\u6210\u672c\u9ad8\u3001\u788e\u7247\u5316\u548c\u96be\u4ee5\u6269\u5c55\u7684\u95ee\u9898\uff0c\u5e76\u5efa\u7acb\u4e86\u9996\u4e2a\u7edf\u4e00\u7684\u9759\u6001\u6392\u884c\u699c\u3002", "motivation": "\u5f53\u524d\u7684\u5177\u8eab\u667a\u80fd\u8bc4\u4f30\u57fa\u51c6\u4f9d\u8d56\u4e8e\u6602\u8d35\u3001\u788e\u7247\u5316\u4e14\u96be\u4ee5\u6269\u5c55\u7684\u4ea4\u4e92\u5f0f\u6a21\u62df\u6216\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\uff0c\u5bfc\u81f4\u5bf9\u9ad8\u6548\u8bc4\u4f30\u7684\u9700\u6c42\u3002", "method": "\u5f15\u5165\u4e86StaticEmbodiedBench\uff0c\u4e00\u4e2a\u5229\u7528\u9759\u6001\u573a\u666f\u8868\u793a\u8fdb\u884c\u7edf\u4e00\u8bc4\u4f30\u7684\u5373\u63d2\u5373\u7528\u57fa\u51c6\u3002\u5b83\u6db5\u76d642\u4e2a\u591a\u6837\u5316\u573a\u666f\u548c8\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff0c\u901a\u8fc7\u7b80\u5355\u754c\u9762\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u5168\u9762\u7684\u8bc4\u4f30\u3002", "result": "\u4f7f\u7528StaticEmbodiedBench\u8bc4\u4f30\u4e8619\u4e2a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c11\u4e2a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLAs\uff09\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u5177\u8eab\u667a\u80fd\u7684\u7edf\u4e00\u9759\u6001\u6392\u884c\u699c\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u5e03\u4e86\u57fa\u51c6\u7684200\u4e2a\u6837\u672c\u5b50\u96c6\uff0c\u4ee5\u52a0\u901f\u5177\u8eab\u667a\u80fd\u7684\u53d1\u5c55\u3002", "conclusion": "StaticEmbodiedBench\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528\u9759\u6001\u573a\u666f\u8868\u793a\uff0c\u663e\u8457\u63a8\u8fdb\u4e86\u5177\u8eab\u667a\u80fd\u7684\u8bc4\u4f30\u548c\u53d1\u5c55\u3002"}}
{"id": "2508.06877", "pdf": "https://arxiv.org/pdf/2508.06877", "abs": "https://arxiv.org/abs/2508.06877", "authors": ["Xiaobo Zhang", "Congqing He", "Ying He", "Jian Peng", "Dajie Fu", "Tien-Ping Tan"], "title": "ESNERA: Empirical and semantic named entity alignment for named entity dataset merging", "categories": ["cs.CL", "cs.AI"], "comment": "30 pages, 12 figures", "summary": "Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing. It remains a research hotspot due to its wide applicability across\ndomains. Although recent advances in deep learning have significantly improved\nNER performance, they rely heavily on large, high-quality annotated datasets.\nHowever, building these datasets is expensive and time-consuming, posing a\nmajor bottleneck for further research. Current dataset merging approaches\nmainly focus on strategies like manual label mapping or constructing label\ngraphs, which lack interpretability and scalability. To address this, we\npropose an automatic label alignment method based on label similarity. The\nmethod combines empirical and semantic similarities, using a greedy pairwise\nmerging strategy to unify label spaces across different datasets. Experiments\nare conducted in two stages: first, merging three existing NER datasets into a\nunified corpus with minimal impact on NER performance; second, integrating this\ncorpus with a small-scale, self-built dataset in the financial domain. The\nresults show that our method enables effective dataset merging and enhances NER\nperformance in the low-resource financial domain. This study presents an\nefficient, interpretable, and scalable solution for integrating multi-source\nNER corpora.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6807\u7b7e\u76f8\u4f3c\u5ea6\u7684\u81ea\u52a8\u5bf9\u9f50\u65b9\u6cd5\uff0c\u65e8\u5728\u9ad8\u6548\u6574\u5408\u591a\u6e90\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u6570\u636e\u96c6\uff0c\u4ee5\u514b\u670d\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u5e76\u5728\u8d44\u6e90\u7a00\u7f3a\u9886\u57df\u63d0\u5347NER\u6027\u80fd\u3002", "motivation": "\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u5bf9\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u6027\u5f3a\uff0c\u4f46\u5176\u6784\u5efa\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\u3002\u73b0\u6709\u6570\u636e\u96c6\u5408\u5e76\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u6574\u5408\u591a\u6e90NER\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6807\u7b7e\u76f8\u4f3c\u5ea6\u7684\u81ea\u52a8\u6807\u7b7e\u5bf9\u9f50\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u7ecf\u9a8c\u76f8\u4f3c\u5ea6\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff0c\u91c7\u7528\u8d2a\u5a6a\u914d\u5bf9\u5408\u5e76\u7b56\u7565\u6765\u7edf\u4e00\u4e0d\u540c\u6570\u636e\u96c6\u7684\u6807\u7b7e\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5408\u5e76NER\u6570\u636e\u96c6\uff0c\u5e76\u5728\u4f4e\u8d44\u6e90\u91d1\u878d\u9886\u57df\u663e\u8457\u63d0\u5347\u4e86NER\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u6574\u5408\u591a\u6e90NER\u8bed\u6599\u5e93\u3002"}}
{"id": "2508.07586", "pdf": "https://arxiv.org/pdf/2508.07586", "abs": "https://arxiv.org/abs/2508.07586", "authors": ["Wenjing Zhang", "Ye Hu", "Tao Luo", "Zhilong Zhang", "Mingzhe Chen"], "title": "Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method", "categories": ["cs.AI", "cs.NI"], "comment": null, "summary": "In this paper, a novel covert semantic communication framework is\ninvestigated. Within this framework, a server extracts and transmits the\nsemantic information, i.e., the meaning of image data, to a user over several\ntime slots. An attacker seeks to detect and eavesdrop the semantic transmission\nto acquire details of the original image. To avoid data meaning being\neavesdropped by an attacker, a friendly jammer is deployed to transmit jamming\nsignals to interfere the attacker so as to hide the transmitted semantic\ninformation. Meanwhile, the server will strategically select time slots for\nsemantic information transmission. Due to limited energy, the jammer will not\ncommunicate with the server and hence the server does not know the transmit\npower of the jammer. Therefore, the server must jointly optimize the semantic\ninformation transmitted at each time slot and the corresponding transmit power\nto maximize the privacy and the semantic information transmission quality of\nthe user. To solve this problem, we propose a prioritised sampling assisted\ntwin delayed deep deterministic policy gradient algorithm to jointly determine\nthe transmitted semantic information and the transmit power per time slot\nwithout the communications between the server and the jammer. Compared to\nstandard reinforcement learning methods, the propose method uses an additional\nQ network to estimate Q values such that the agent can select the action with a\nlower Q value from the two Q networks thus avoiding local optimal action\nselection and estimation bias of Q values. Simulation results show that the\nproposed algorithm can improve the privacy and the semantic information\ntransmission quality by up to 77.8% and 14.3% compared to the traditional\nreinforcement learning methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9690\u853d\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u670d\u52a1\u5668\u5411\u7528\u6237\u4f20\u8f93\u56fe\u50cf\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u90e8\u7f72\u5e72\u6270\u5668\u5bf9\u6297\u7a83\u542c\u8005\u3002\u9488\u5bf9\u670d\u52a1\u5668\u65e0\u6cd5\u611f\u77e5\u5e72\u6270\u5668\u529f\u7387\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5148\u91c7\u6837\u53cc\u5ef6\u8fdf\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08DDPG\uff09\u7b97\u6cd5\uff0c\u7528\u4e8e\u8054\u5408\u4f18\u5316\u8bed\u4e49\u4fe1\u606f\u4f20\u8f93\u548c\u529f\u7387\u5206\u914d\uff0c\u4ee5\u6700\u5927\u5316\u7528\u6237\u9690\u79c1\u548c\u4f20\u8f93\u8d28\u91cf\u3002\u4eff\u771f\u7ed3\u679c\u663e\u793a\u8be5\u7b97\u6cd5\u5728\u9690\u79c1\u548c\u4f20\u8f93\u8d28\u91cf\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5728\u9690\u853d\u8bed\u4e49\u901a\u4fe1\u4e2d\uff0c\u670d\u52a1\u5668\u5411\u7528\u6237\u4f20\u8f93\u56fe\u50cf\u8bed\u4e49\u4fe1\u606f\u65f6\u9762\u4e34\u7a83\u542c\u5a01\u80c1\u3002\u4e3a\u786e\u4fdd\u4f20\u8f93\u7684\u9690\u853d\u6027\u548c\u7528\u6237\u9690\u79c1\uff0c\u540c\u65f6\u514b\u670d\u670d\u52a1\u5668\u65e0\u6cd5\u611f\u77e5\u53cb\u597d\u5e72\u6270\u5668\u529f\u7387\u7684\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u4f18\u5316\u7b56\u7565\u6765\u534f\u540c\u6700\u5927\u5316\u9690\u79c1\u548c\u8bed\u4e49\u4fe1\u606f\u4f20\u8f93\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u4f18\u5148\u91c7\u6837\u8f85\u52a9\u53cc\u5ef6\u8fdf\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08prioritised sampling assisted twin delayed DDPG\uff09\u201d\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u5728\u670d\u52a1\u5668\u4e0e\u5e72\u6270\u5668\u65e0\u901a\u4fe1\u7684\u60c5\u51b5\u4e0b\uff0c\u8054\u5408\u51b3\u5b9a\u6bcf\u4e2a\u65f6\u9699\u7684\u8bed\u4e49\u4fe1\u606f\u4f20\u8f93\u5185\u5bb9\u548c\u76f8\u5e94\u7684\u53d1\u5c04\u529f\u7387\u3002\u76f8\u8f83\u4e8e\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u5f15\u5165\u989d\u5916\u7684Q\u7f51\u7edc\uff0c\u907f\u514d\u4e86\u5c40\u90e8\u6700\u4f18\u52a8\u4f5c\u9009\u62e9\u548cQ\u503c\u4f30\u8ba1\u504f\u5dee\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u5c06\u7528\u6237\u9690\u79c1\u63d0\u9ad8\u9ad8\u8fbe77.8%\uff0c\u5e76\u5c06\u8bed\u4e49\u4fe1\u606f\u4f20\u8f93\u8d28\u91cf\u63d0\u9ad8\u9ad8\u8fbe14.3%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u4f18\u5148\u91c7\u6837\u53cc\u5ef6\u8fdf\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u7684\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u670d\u52a1\u5668\u65e0\u6cd5\u611f\u77e5\u5e72\u6270\u5668\u529f\u7387\u4e0b\u7684\u9690\u853d\u8bed\u4e49\u901a\u4fe1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u9690\u79c1\u548c\u8bed\u4e49\u4fe1\u606f\u4f20\u8f93\u8d28\u91cf\u3002"}}
{"id": "2508.06647", "pdf": "https://arxiv.org/pdf/2508.06647", "abs": "https://arxiv.org/abs/2508.06647", "authors": ["Andrey Sidorenko", "Paul Tiwald"], "title": "Privacy-Preserving Tabular Synthetic Data Generation Using TabularARGN", "categories": ["cs.LG"], "comment": null, "summary": "Synthetic data generation has become essential for securely sharing and\nanalyzing sensitive data sets. Traditional anonymization techniques, however,\noften fail to adequately preserve privacy. We introduce the Tabular\nAuto-Regressive Generative Network (TabularARGN), a neural network architecture\nspecifically designed for generating high-quality synthetic tabular data. Using\na discretization-based auto-regressive approach, TabularARGN achieves high data\nfidelity while remaining computationally efficient. We evaluate TabularARGN\nagainst existing synthetic data generation methods, showing competitive results\nin statistical similarity, machine learning utility, and detection robustness.\nWe further perform an in-depth privacy evaluation using systematic\nmembership-inference attacks, highlighting the robustness and effective\nprivacy-utility balance of our approach.", "AI": {"tldr": "\u63d0\u51faTabularARGN\uff0c\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u5408\u6210\u8868\u683c\u6570\u636e\u3002", "motivation": "\u654f\u611f\u6570\u636e\u5171\u4eab\u548c\u5206\u6790\u9700\u8981\u5408\u6210\u6570\u636e\uff0c\u4f46\u4f20\u7edf\u533f\u540d\u5316\u6280\u672f\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u4e0d\u8db3\u3002", "method": "\u5f15\u5165TabularARGN\uff0c\u4e00\u4e2a\u57fa\u4e8e\u79bb\u6563\u5316\u7684\u81ea\u56de\u5f52\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u4e13\u95e8\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u8868\u683c\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "result": "TabularARGN\u5728\u7edf\u8ba1\u76f8\u4f3c\u6027\u3001\u673a\u5668\u5b66\u4e60\u6548\u7528\u548c\u68c0\u6d4b\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff1b\u901a\u8fc7\u6df1\u5165\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u8bc4\u4f30\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u7684\u9690\u79c1-\u6548\u7528\u5e73\u8861\u3002", "conclusion": "TabularARGN\u662f\u4e00\u79cd\u6709\u6548\u7684\u5408\u6210\u8868\u683c\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2508.06851", "pdf": "https://arxiv.org/pdf/2508.06851", "abs": "https://arxiv.org/abs/2508.06851", "authors": ["Pengfei Zhou", "Xiaopeng Peng", "Fanrui Zhang", "Zhaopan Xu", "Jiaxin Ai", "Yansheng Qiu", "Chuanhao Li", "Zhen Li", "Ming Li", "Yukang Feng", "Jianwen Sun", "Haoquan Zhang", "Zizhen Li", "Xiaofeng Mao", "Zekai Li", "Wangbo Zhao", "Kai Wang", "Xiaojun Chang", "Wenqi Shao", "Yang You", "Kaipeng Zhang"], "title": "MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams", "categories": ["cs.AI", "cs.CY"], "comment": "35 pages, 33 figures", "summary": "Multimodal large language models (MLLMs), which integrate language and visual\ncues for problem-solving, are crucial for advancing artificial general\nintelligence (AGI). However, current benchmarks for measuring the intelligence\nof MLLMs suffer from limited scale, narrow coverage, and unstructured\nknowledge, offering only static and undifferentiated evaluations. To bridge\nthis gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark\nbuilt from real-world K-12 exams spanning six disciplines with 141K instances\nand 6,225 knowledge points organized in a six-layer taxonomy. Covering five\nquestion formats with difficulty and year annotations, it enables comprehensive\nevaluation to capture the extent to which MLLMs perform over four dimensions:\n1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts,\nand 4) knowledge-driven reasoning. We propose a novel dynamic evaluation\nframework that introduces unfamiliar visual, textual, and question form shifts\nto challenge model generalization while improving benchmark objectivity and\nlongevity by mitigating data contamination. We further evaluate knowledge-point\nreference-augmented generation (KP-RAG) to examine the role of knowledge in\nproblem-solving. Key findings reveal limitations in current MLLMs in multiple\naspects and provide guidance for enhancing model robustness, interpretability,\nand AI-assisted education.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u73b0\u6709MLLM\u8bc4\u4f30\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86MDK12-Bench\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u5b66\u79d1\u57fa\u51c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\u3002\u7814\u7a76\u53d1\u73b0\u5f53\u524dMLLMs\u5728\u591a\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5e76\u4e3a\u6a21\u578b\u6539\u8fdb\u548cAI\u6559\u80b2\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5bf9\u63a8\u8fdb\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7528\u4e8e\u8861\u91cfMLLMs\u667a\u80fd\u7684\u57fa\u51c6\u5b58\u5728\u89c4\u6a21\u6709\u9650\u3001\u8986\u76d6\u8303\u56f4\u72ed\u7a84\u548c\u77e5\u8bc6\u7ed3\u6784\u5316\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u4ec5\u80fd\u63d0\u4f9b\u9759\u6001\u548c\u975e\u5dee\u5f02\u5316\u7684\u8bc4\u4f30\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u6349MLLMs\u7684\u771f\u5b9e\u667a\u80fd\u6c34\u5e73\u3002", "method": "\u5f15\u5165\u4e86MDK12-Bench\uff0c\u4e00\u4e2a\u57fa\u4e8eK-12\u771f\u5b9e\u8003\u8bd5\u6784\u5efa\u7684\u5927\u89c4\u6a21\u591a\u5b66\u79d1\u57fa\u51c6\uff0c\u5305\u542b14.1\u4e07\u4e2a\u5b9e\u4f8b\u548c6225\u4e2a\u77e5\u8bc6\u70b9\uff08\u7ec4\u7ec7\u5728\u516d\u5c42\u5206\u7c7b\u6cd5\u4e2d\uff09\u3002\u8be5\u57fa\u51c6\u6db5\u76d6\u4e94\u79cd\u9898\u578b\uff0c\u5e76\u5177\u6709\u96be\u5ea6\u548c\u5e74\u4efd\u6807\u6ce8\uff0c\u80fd\u591f\u4ece\u96be\u5ea6\u3001\u8de8\u5e74\u4efd\u3001\u4e0a\u4e0b\u6587\u548c\u77e5\u8bc6\u9a71\u52a8\u63a8\u7406\u56db\u4e2a\u7ef4\u5ea6\u5168\u9762\u8bc4\u4f30MLLMs\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u5f15\u5165\u4e0d\u719f\u6089\u7684\u89c6\u89c9\u3001\u6587\u672c\u548c\u95ee\u9898\u5f62\u5f0f\u53d8\u5316\uff0c\u4ee5\u6311\u6218\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u5e76\u51cf\u8f7b\u6570\u636e\u6c61\u67d3\u3002\u8fd8\u8bc4\u4f30\u4e86\u77e5\u8bc6\u70b9\u53c2\u8003\u589e\u5f3a\u751f\u6210\uff08KP-RAG\uff09\u4ee5\u63a2\u8ba8\u77e5\u8bc6\u5728\u89e3\u51b3\u95ee\u9898\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u5173\u952e\u53d1\u73b0\u63ed\u793a\u4e86\u5f53\u524dMLLMs\u5728\u591a\u4e2a\u65b9\u9762\u5b58\u5728\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u4fc3\u8fdb\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u6559\u80b2\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2508.06555", "pdf": "https://arxiv.org/pdf/2508.06555", "abs": "https://arxiv.org/abs/2508.06555", "authors": ["Hongbo Ma", "Fei Shen", "Hongbin Xu", "Xiaoce Wang", "Gang Xu", "Jinkai Zheng", "Liangqiong Qu", "Ming Li"], "title": "StyleTailor: Towards Personalized Fashion Styling via Hierarchical Negative Feedback", "categories": ["cs.CV", "cs.CY", "cs.MA"], "comment": "24pages, 5 figures", "summary": "The advancement of intelligent agents has revolutionized problem-solving\nacross diverse domains, yet solutions for personalized fashion styling remain\nunderexplored, which holds immense promise for promoting shopping experiences.\nIn this work, we present StyleTailor, the first collaborative agent framework\nthat seamlessly unifies personalized apparel design, shopping recommendation,\nvirtual try-on, and systematic evaluation into a cohesive workflow. To this\nend, StyleTailor pioneers an iterative visual refinement paradigm driven by\nmulti-level negative feedback, enabling adaptive and precise user alignment.\nSpecifically, our framework features two core agents, i.e., Designer for\npersonalized garment selection and Consultant for virtual try-on, whose outputs\nare progressively refined via hierarchical vision-language model feedback\nspanning individual items, complete outfits, and try-on efficacy.\nCounterexamples are aggregated into negative prompts, forming a closed-loop\nmechanism that enhances recommendation quality.To assess the performance, we\nintroduce a comprehensive evaluation suite encompassing style consistency,\nvisual quality, face similarity, and artistic appraisal. Extensive experiments\ndemonstrate StyleTailor's superior performance in delivering personalized\ndesigns and recommendations, outperforming strong baselines without negative\nfeedback and establishing a new benchmark for intelligent fashion systems.", "AI": {"tldr": "StyleTailor\u662f\u4e00\u4e2a\u9996\u521b\u7684\u534f\u540c\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ea7\u8d1f\u53cd\u9988\u8fed\u4ee3\u89c6\u89c9\u4f18\u5316\uff0c\u6574\u5408\u4e2a\u6027\u5316\u670d\u88c5\u8bbe\u8ba1\u3001\u8d2d\u7269\u63a8\u8350\u548c\u865a\u62df\u8bd5\u7a7f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u65f6\u5c1a\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u667a\u80fd\u4ee3\u7406\u5728\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4e2a\u6027\u5316\u65f6\u5c1a\u9020\u578b\u7684\u89e3\u51b3\u65b9\u6848\u4ecd\u672a\u5145\u5206\u63a2\u7d22\uff0c\u800c\u8fd9\u5bf9\u4e8e\u63d0\u5347\u8d2d\u7269\u4f53\u9a8c\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002", "method": "StyleTailor\u6846\u67b6\u901a\u8fc7\u4e00\u4e2a\u8fed\u4ee3\u89c6\u89c9\u4f18\u5316\u8303\u5f0f\u5b9e\u73b0\uff0c\u8be5\u8303\u5f0f\u7531\u591a\u7ea7\u8d1f\u53cd\u9988\u9a71\u52a8\uff0c\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u548c\u7cbe\u786e\u7684\u7528\u6237\u5bf9\u9f50\u3002\u5b83\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u4ee3\u7406\uff1a\u8d1f\u8d23\u4e2a\u6027\u5316\u670d\u88c5\u9009\u62e9\u7684Designer\u548c\u7528\u4e8e\u865a\u62df\u8bd5\u7a7f\u7684Consultant\u3002\u5b83\u4eec\u7684\u8f93\u51fa\u901a\u8fc7\u5206\u5c42\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u53cd\u9988\uff08\u6db5\u76d6\u5355\u54c1\u3001\u5b8c\u6574\u5957\u88c5\u548c\u8bd5\u7a7f\u6548\u679c\uff09\u9010\u6b65\u4f18\u5316\u3002\u53cd\u4f8b\u88ab\u805a\u5408\u6210\u8d1f\u9762\u63d0\u793a\uff0c\u5f62\u6210\u95ed\u73af\u673a\u5236\u4ee5\u63d0\u9ad8\u63a8\u8350\u8d28\u91cf\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u7efc\u5408\u8bc4\u4f30\u5957\u4ef6\u6765\u8bc4\u4f30\u6027\u80fd\u3002", "result": "StyleTailor\u5728\u63d0\u4f9b\u4e2a\u6027\u5316\u8bbe\u8ba1\u548c\u63a8\u8350\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u6ca1\u6709\u8d1f\u53cd\u9988\u7684\u5f3a\u5927\u57fa\u7ebf\uff0c\u5e76\u4e3a\u667a\u80fd\u65f6\u5c1a\u7cfb\u7edf\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "StyleTailor\u6210\u529f\u5730\u5c06\u4e2a\u6027\u5316\u670d\u88c5\u8bbe\u8ba1\u3001\u8d2d\u7269\u63a8\u8350\u548c\u865a\u62df\u8bd5\u7a7f\u6574\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u4e2d\uff0c\u5e76\u5229\u7528\u591a\u7ea7\u8d1f\u53cd\u9988\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u8d28\u91cf\u548c\u7528\u6237\u5bf9\u9f50\u5ea6\uff0c\u4e3a\u667a\u80fd\u65f6\u5c1a\u7cfb\u7edf\u9886\u57df\u6811\u7acb\u4e86\u65b0\u6807\u6746\u3002"}}
{"id": "2508.06880", "pdf": "https://arxiv.org/pdf/2508.06880", "abs": "https://arxiv.org/abs/2508.06880", "authors": ["Philipp Christmann", "Gerhard Weikum"], "title": "The ReQAP System for Question Answering over Personal Information", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at CIKM 2025 (demonstration paper)", "summary": "Personal information is abundant on users' devices, from structured data in\ncalendar, shopping records or fitness tools, to unstructured contents in mail\nand social media posts. This works presents the ReQAP system that supports\nusers with answers for complex questions that involve filters, joins and\naggregation over heterogeneous sources. The unique trait of ReQAP is that it\nrecursively decomposes questions and incrementally builds an operator tree for\nexecution. Both the question interpretation and the individual operators make\nsmart use of light-weight language models, with judicious fine-tuning. The demo\nshowcases the rich functionality for advanced user questions, and also offers\ndetailed tracking of how the answers are computed by the operators in the\nexecution tree. Being able to trace answers back to the underlying sources is\nvital for human comprehensibility and user trust in the system.", "AI": {"tldr": "ReQAP\u662f\u4e00\u4e2a\u80fd\u591f\u5bf9\u7528\u6237\u8bbe\u5907\u4e0a\u5f02\u6784\u4e2a\u4eba\u6570\u636e\u8fdb\u884c\u590d\u6742\u67e5\u8be2\u5e76\u63d0\u4f9b\u7b54\u6848\u7684\u7cfb\u7edf\uff0c\u5176\u72ec\u7279\u4e4b\u5904\u5728\u4e8e\u9012\u5f52\u5206\u89e3\u95ee\u9898\u3001\u6784\u5efa\u64cd\u4f5c\u6811\uff0c\u5e76\u7ed3\u5408\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u7528\u6237\u8bbe\u5907\u4e0a\u5b58\u5728\u6d77\u91cf\u5f02\u6784\u4e2a\u4eba\u6570\u636e\uff0c\u96be\u4ee5\u8fdb\u884c\u6d89\u53ca\u7b5b\u9009\u3001\u8fde\u63a5\u548c\u805a\u5408\u7684\u590d\u6742\u67e5\u8be2\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u7cfb\u7edf\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u4fbf\u6377\u5730\u5bf9\u8fd9\u4e9b\u6570\u636e\u8fdb\u884c\u590d\u6742\u63d0\u95ee\uff0c\u5e76\u786e\u4fdd\u7b54\u6848\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u8d56\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86ReQAP\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u9012\u5f52\u5206\u89e3\u590d\u6742\u95ee\u9898\uff0c\u5e76\u9010\u6b65\u6784\u5efa\u4e00\u4e2a\u53ef\u6267\u884c\u7684\u64cd\u4f5c\u7b26\u6811\u3002\u5728\u95ee\u9898\u89e3\u91ca\u548c\u5404\u4e2a\u64cd\u4f5c\u7b26\u7684\u5b9e\u73b0\u4e2d\uff0cReQAP\u660e\u667a\u5730\u5229\u7528\u4e86\u7ecf\u8fc7\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u3002\u7cfb\u7edf\u8fd8\u63d0\u4f9b\u4e86\u7b54\u6848\u8ba1\u7b97\u8fc7\u7a0b\u7684\u8be6\u7ec6\u8ffd\u8e2a\u529f\u80fd\u3002", "result": "ReQAP\u7cfb\u7edf\u80fd\u591f\u6210\u529f\u5904\u7406\u6d89\u53ca\u7b5b\u9009\u3001\u8fde\u63a5\u548c\u805a\u5408\u7684\u590d\u6742\u7528\u6237\u95ee\u9898\uff0c\u5e76\u8de8\u8d8a\u5f02\u6784\u6570\u636e\u6e90\u63d0\u4f9b\u51c6\u786e\u7b54\u6848\u3002\u5176\u6f14\u793a\u5c55\u793a\u4e86\u4e30\u5bcc\u7684\u529f\u80fd\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u6e05\u6670\u5730\u8ffd\u8e2a\u7b54\u6848\u7684\u8ba1\u7b97\u8def\u5f84\u548c\u6570\u636e\u6765\u6e90\uff0c\u8fd9\u5bf9\u4e8e\u7528\u6237\u7406\u89e3\u548c\u4fe1\u4efb\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "ReQAP\u7cfb\u7edf\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5728\u5f02\u6784\u4e2a\u4eba\u6570\u636e\u4e0a\u8fdb\u884c\u590d\u6742\u67e5\u8be2\u7684\u96be\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u95ee\u9898\u5206\u89e3\u548c\u8bed\u8a00\u6a21\u578b\u5e94\u7528\uff0c\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u67e5\u8be2\u80fd\u529b\uff0c\u8fd8\u901a\u8fc7\u7ed3\u679c\u8ffd\u6eaf\u589e\u5f3a\u4e86\u900f\u660e\u5ea6\u548c\u7528\u6237\u4fe1\u4efb\uff0c\u4f7f\u5176\u6210\u4e3a\u4e2a\u4eba\u6570\u636e\u7ba1\u7406\u7684\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2508.07744", "pdf": "https://arxiv.org/pdf/2508.07744", "abs": "https://arxiv.org/abs/2508.07744", "authors": ["Ingo Friese", "Jochen Klaffer", "Mandy Galkow-Schneider", "Sergiy Melnyk", "Qiuheng Zhou", "Hans Dieter Schotten"], "title": "Over-the-Top Resource Broker System for Split Computing: An Approach to Distribute Cloud Computing Infrastructure", "categories": ["cs.DC", "cs.NI", "eess.SP"], "comment": null, "summary": "6G network architectures will usher in a wave of innovative services and\ncapabilities, introducing concepts like split computing and dynamic processing\nnodes. This implicates a paradigm where accessing resources seamlessly aligns\nwith diverse processing node characteristics, ensuring a uniform interface. In\nthis landscape, the identity of the operator becomes inconsequential, paving\nthe way for a collaborative ecosystem where multiple providers contribute to a\nshared pool of resources. At the core of this vision is the guarantee of\nspecific performance parameters, precisely tailored to the location and service\nrequirements. A consistent layer, as the abstraction of the complexities of\ndifferent infrastructure providers, is needed to simplify service deployment.\nOne promising approach is the introduction of an over-the-top broker for\nresource allocation, which streamlines the integration of these services into\nthe network and cloud infrastructure of the future. This paper explores the\nrole of the broker in two split computing scenarios. By abstracting the\ncomplexities of various infrastructures, the broker proves to be a versatile\nsolution applicable not only to cloud environments but also to networks and\nbeyond. Additionally, a detailed discussion of a proof-of-concept\nimplementation provides insights into the broker's actual architectural\nframework.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57286G\u7f51\u7edc\u67b6\u6784\u4e2d\uff0c\u5f15\u5165\u4e00\u4e2aOTT\uff08Over-The-Top\uff09\u4ee3\u7406\u6765\u7ba1\u7406\u8d44\u6e90\u5206\u914d\u7684\u4f5c\u7528\u3002\u8be5\u4ee3\u7406\u901a\u8fc7\u62bd\u8c61\u57fa\u7840\u8bbe\u65bd\u590d\u6742\u6027\uff0c\u5728\u5206\u4f53\u5f0f\u8ba1\u7b97\u573a\u666f\u4e2d\u5b9e\u73b0\u8de8\u591a\u63d0\u4f9b\u5546\u7684\u65e0\u7f1d\u8d44\u6e90\u8bbf\u95ee\u548c\u7b80\u5316\u670d\u52a1\u90e8\u7f72\uff0c\u5e76\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\u7ec6\u8282\u3002", "motivation": "6G\u7f51\u7edc\u5c06\u5f15\u5165\u5206\u4f53\u5f0f\u8ba1\u7b97\u548c\u52a8\u6001\u5904\u7406\u8282\u70b9\u7b49\u521b\u65b0\u670d\u52a1\u3002\u4e3a\u5e94\u5bf9\u591a\u6837\u5316\u5904\u7406\u8282\u70b9\u7684\u65e0\u7f1d\u8d44\u6e90\u8bbf\u95ee\u3001\u7edf\u4e00\u63a5\u53e3\u3001\u591a\u8fd0\u8425\u5546\u534f\u4f5c\u3001\u57fa\u4e8e\u4f4d\u7f6e\u548c\u670d\u52a1\u7684\u6027\u80fd\u4fdd\u8bc1\u4ee5\u53ca\u7b80\u5316\u670d\u52a1\u90e8\u7f72\u7684\u9700\u6c42\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u62bd\u8c61\u590d\u6742\u57fa\u7840\u8bbe\u65bd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5e76\u63a2\u8ba8\u4e86\u5f15\u5165\u4e00\u4e2aover-the-top (OTT) \u4ee3\u7406\u4f5c\u4e3a\u8d44\u6e90\u5206\u914d\u7684\u6709\u6548\u65b9\u6cd5\u3002\u5177\u4f53\u5206\u6790\u4e86\u8be5\u4ee3\u7406\u5728\u4e24\u79cd\u5206\u4f53\u5f0f\u8ba1\u7b97\u573a\u666f\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u8be6\u7ec6\u8ba8\u8bba\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\uff08Proof-of-Concept\uff09\u5b9e\u73b0\uff0c\u5c55\u793a\u4e86\u4ee3\u7406\u7684\u5b9e\u9645\u67b6\u6784\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u62bd\u8c61\u5404\u79cd\u57fa\u7840\u8bbe\u65bd\u7684\u590d\u6742\u6027\uff0c\u6240\u63d0\u51fa\u7684\u4ee3\u7406\u88ab\u8bc1\u660e\u662f\u4e00\u79cd\u591a\u529f\u80fd\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u4ee3\u7406\u4e0d\u4ec5\u9002\u7528\u4e8e\u4e91\u73af\u5883\uff0c\u800c\u4e14\u53ef\u6269\u5c55\u5e94\u7528\u4e8e\u7f51\u7edc\u53ca\u66f4\u5e7f\u6cdb\u7684\u9886\u57df\u3002", "conclusion": "OTT\u4ee3\u7406\u662f\u672a\u67656G\u7f51\u7edc\u4e2d\u5b9e\u73b0\u7edf\u4e00\u8d44\u6e90\u8bbf\u95ee\u3001\u7b80\u5316\u670d\u52a1\u90e8\u7f72\u3001\u4fdd\u8bc1\u6027\u80fd\u4ee5\u53ca\u652f\u6301\u591a\u63d0\u4f9b\u5546\u534f\u4f5c\u7684\u5173\u952e\u3002\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u6240\u8bc1\u5b9e\u7684\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u4f7f\u5176\u6210\u4e3a\u5e94\u5bf9\u590d\u6742\u57fa\u7840\u8bbe\u65bd\u6311\u6218\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2508.06659", "pdf": "https://arxiv.org/pdf/2508.06659", "abs": "https://arxiv.org/abs/2508.06659", "authors": ["Fernando Martinez-Lopez", "Tao Li", "Yingdong Lu", "Juntao Chen"], "title": "In-Context Reinforcement Learning via Communicative World Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) agents often struggle to generalize to new tasks\nand contexts without updating their parameters, mainly because their learned\nrepresentations and policies are overfit to the specifics of their training\nenvironments. To boost agents' in-context RL (ICRL) ability, this work\nformulates ICRL as a two-agent emergent communication problem and introduces\nCORAL (Communicative Representation for Adaptive RL), a framework that learns a\ntransferable communicative context by decoupling latent representation learning\nfrom control. In CORAL, an Information Agent (IA) is pre-trained as a world\nmodel on a diverse distribution of tasks. Its objective is not to maximize task\nreward, but to build a world model and distill its understanding into concise\nmessages. The emergent communication protocol is shaped by a novel Causal\nInfluence Loss, which measures the effect that the message has on the next\naction. During deployment, the previously trained IA serves as a fixed\ncontextualizer for a new Control Agent (CA), which learns to solve tasks by\ninterpreting the provided communicative context. Our experiments demonstrate\nthat this approach enables the CA to achieve significant gains in sample\nefficiency and successfully perform zero-shot adaptation with the help of\npre-trained IA in entirely unseen sparse-reward environments, validating the\nefficacy of learning a transferable communicative representation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCORAL\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u667a\u80fd\u4f53\u6d8c\u73b0\u901a\u4fe1\u8303\u5f0f\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u667a\u80fd\u4f53\u901a\u5e38\u96be\u4ee5\u5728\u4e0d\u66f4\u65b0\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\u548c\u65b0\u73af\u5883\uff0c\u56e0\u5176\u5b66\u4e60\u5230\u7684\u8868\u793a\u548c\u7b56\u7565\u8fc7\u5ea6\u62df\u5408\u4e86\u8bad\u7ec3\u73af\u5883\u3002", "method": "\u5c06\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\uff08ICRL\uff09\u95ee\u9898\u5efa\u6a21\u4e3a\u53cc\u667a\u80fd\u4f53\u6d8c\u73b0\u901a\u4fe1\u95ee\u9898\u3002CORAL\u6846\u67b6\u89e3\u8026\u4e86\u6f5c\u5728\u8868\u793a\u5b66\u4e60\u4e0e\u63a7\u5236\uff0c\u5305\u62ec\u4e00\u4e2a\u9884\u8bad\u7ec3\u4e3a\u4e16\u754c\u6a21\u578b\u7684\u201c\u4fe1\u606f\u667a\u80fd\u4f53\u201d\uff08IA\uff09\u548c\u4e00\u4e2a\u57fa\u4e8eIA\u63d0\u4f9b\u7684\u901a\u4fe1\u4e0a\u4e0b\u6587\u5b66\u4e60\u4efb\u52a1\u7684\u201c\u63a7\u5236\u667a\u80fd\u4f53\u201d\uff08CA\uff09\u3002\u901a\u4fe1\u534f\u8bae\u7531\u65b0\u9896\u7684\u56e0\u679c\u5f71\u54cd\u635f\u5931\u5851\u9020\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86CA\u7684\u6837\u672c\u6548\u7387\uff0c\u5e76\u5728IA\u7684\u5e2e\u52a9\u4e0b\uff0c\u5728\u5b8c\u5168\u672a\u89c1\u7684\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u6210\u529f\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u9002\u5e94\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u5b66\u4e60\u53ef\u8fc1\u79fb\u901a\u4fe1\u8868\u793a\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86RL\u667a\u80fd\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.06859", "pdf": "https://arxiv.org/pdf/2508.06859", "abs": "https://arxiv.org/abs/2508.06859", "authors": ["Shuo Tang", "Jian Xu", "Jiadong Zhang", "Yi Chen", "Qizhao Jin", "Lingdong Shen", "Chenglin Liu", "Shiming Xiang"], "title": "MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Timely and accurate severe weather warnings are critical for disaster\nmitigation. However, current forecasting systems remain heavily reliant on\nmanual expert interpretation, introducing subjectivity and significant\noperational burdens. With the rapid development of AI technologies, the\nend-to-end \"AI weather station\" is gradually emerging as a new trend in\npredicting severe weather events. Three core challenges impede the development\nof end-to-end AI severe weather system: (1) scarcity of severe weather event\nsamples; (2) imperfect alignment between high-dimensional meteorological data\nand textual warnings; (3) existing multimodal language models are unable to\nhandle high-dimensional meteorological data and struggle to fully capture the\ncomplex dependencies across temporal sequences, vertical pressure levels, and\nspatial dimensions. To address these challenges, we introduce MP-Bench, the\nfirst large-scale temporal multimodal dataset for severe weather events\nprediction, comprising 421,363 pairs of raw multi-year meteorological data and\ncorresponding text caption, covering a wide range of severe weather scenarios\nacross China. On top of this dataset, we develop a meteorology multimodal large\nmodel (MMLM) that directly ingests 4D meteorological inputs. In addition, it is\ndesigned to accommodate the unique characteristics of 4D meteorological data\nflow, incorporating three plug-and-play adaptive fusion modules that enable\ndynamic feature extraction and integration across temporal sequences, vertical\npressure layers, and spatial dimensions. Extensive experiments on MP-Bench\ndemonstrate that MMLM performs exceptionally well across multiple tasks,\nhighlighting its effectiveness in severe weather understanding and marking a\nkey step toward realizing automated, AI-driven weather forecasting systems. Our\nsource code and dataset will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMP-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u6076\u52a3\u5929\u6c14\u4e8b\u4ef6\u9884\u6d4b\u7684\u5927\u89c4\u6a21\u65f6\u5e8f\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86MMLM\uff08\u6c14\u8c61\u591a\u6a21\u6001\u5927\u6a21\u578b\uff09\uff0c\u65e8\u5728\u89e3\u51b3AI\u9a71\u52a8\u7684\u6076\u52a3\u5929\u6c14\u9884\u8b66\u4e2d\u5b58\u5728\u7684\u6837\u672c\u7a00\u7f3a\u3001\u6570\u636e\u4e0e\u6587\u672c\u5bf9\u9f50\u4e0d\u4f73\u4ee5\u53ca\u73b0\u6709\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u9ad8\u7ef4\u6c14\u8c61\u6570\u636e\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u7684\u6076\u52a3\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\u4e25\u91cd\u4f9d\u8d56\u4eba\u5de5\u4e13\u5bb6\u5224\u8bfb\uff0c\u5bfc\u81f4\u4e3b\u89c2\u6027\u5f3a\u548c\u64cd\u4f5c\u8d1f\u62c5\u91cd\u3002\u5c3d\u7ba1AI\u6c14\u8c61\u7ad9\u4f5c\u4e3a\u65b0\u8d8b\u52bf\u6b63\u5728\u5174\u8d77\uff0c\u4f46\u5176\u53d1\u5c55\u9762\u4e34\u4e09\u5927\u6838\u5fc3\u6311\u6218\uff1a1) \u6076\u52a3\u5929\u6c14\u4e8b\u4ef6\u6837\u672c\u7a00\u7f3a\uff1b2) \u9ad8\u7ef4\u6c14\u8c61\u6570\u636e\u4e0e\u6587\u672c\u9884\u8b66\u7684\u5bf9\u9f50\u4e0d\u5b8c\u5584\uff1b3) \u73b0\u6709\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u9ad8\u7ef4\u6c14\u8c61\u6570\u636e\uff0c\u4e14\u96be\u4ee5\u5145\u5206\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u3001\u5782\u76f4\u6c14\u538b\u5c42\u548c\u7a7a\u95f4\u7ef4\u5ea6\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u4e3a\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\uff0c\u7814\u7a76\u8005\u5f15\u5165\u4e86MP-Bench\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u6076\u52a3\u5929\u6c14\u4e8b\u4ef6\u9884\u6d4b\u7684\u5927\u89c4\u6a21\u65f6\u5e8f\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b421,363\u5bf9\u539f\u59cb\u591a\u5e74\u6c14\u8c61\u6570\u636e\u53ca\u5176\u5bf9\u5e94\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u8986\u76d6\u4e2d\u56fd\u5e7f\u6cdb\u7684\u6076\u52a3\u5929\u6c14\u60c5\u666f\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u6c14\u8c61\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MMLM\uff09\uff0c\u8be5\u6a21\u578b\u80fd\u76f4\u63a5\u5904\u74064D\u6c14\u8c61\u8f93\u5165\uff0c\u5e76\u9488\u5bf94D\u6c14\u8c61\u6570\u636e\u6d41\u7684\u72ec\u7279\u7279\u6027\uff0c\u96c6\u6210\u4e86\u4e09\u4e2a\u5373\u63d2\u5373\u7528\u7684\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\uff0c\u4ee5\u5b9e\u73b0\u8de8\u65f6\u95f4\u5e8f\u5217\u3001\u5782\u76f4\u6c14\u538b\u5c42\u548c\u7a7a\u95f4\u7ef4\u5ea6\u7684\u52a8\u6001\u7279\u5f81\u63d0\u53d6\u548c\u6574\u5408\u3002", "result": "\u5728MP-Bench\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMMLM\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7a81\u663e\u4e86\u5176\u5728\u6076\u52a3\u5929\u6c14\u7406\u89e3\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "MMLM\u7684\u6210\u529f\u5f00\u53d1\u548c\u9a8c\u8bc1\u6807\u5fd7\u7740\u5b9e\u73b0\u81ea\u52a8\u5316\u3001AI\u9a71\u52a8\u7684\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\u7684\u5173\u952e\u4e00\u6b65\u3002\u7814\u7a76\u8005\u5c06\u516c\u5f00\u53d1\u5e03\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2508.06556", "pdf": "https://arxiv.org/pdf/2508.06556", "abs": "https://arxiv.org/abs/2508.06556", "authors": ["Sarina Penquitt", "Jonathan Klees", "Rinor Cakaj", "Daniel Kondermann", "Matthias Rottmann", "Lars Schmarje"], "title": "From Label Error Detection to Correction: A Modular Framework and Benchmark for Object Detection Datasets", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Object detection has advanced rapidly in recent years, driven by increasingly\nlarge and diverse datasets. However, label errors, defined as missing labels,\nincorrect classification or inaccurate localization, often compromise the\nquality of these datasets. This can have a significant impact on the outcomes\nof training and benchmark evaluations. Although several methods now exist for\ndetecting label errors in object detection datasets, they are typically\nvalidated only on synthetic benchmarks or limited manual inspection. How to\ncorrect such errors systemically and at scale therefore remains an open\nproblem. We introduce a semi-automated framework for label-error correction\ncalled REC$\\checkmark$D (Rechecked). Building on existing detectors, the\nframework pairs their error proposals with lightweight, crowd-sourced\nmicrotasks. These tasks enable multiple annotators to independently verify each\ncandidate bounding box, and their responses are aggregated to estimate\nambiguity and improve label quality. To demonstrate the effectiveness of\nREC$\\checkmark$D, we apply it to the class pedestrian in the KITTI dataset. Our\ncrowdsourced review yields high-quality corrected annotations, which indicate a\nrate of at least 24% of missing and inaccurate annotations in original\nannotations. This validated set will be released as a new real-world benchmark\nfor label error detection and correction. We show that current label error\ndetection methods, when combined with our correction framework, can recover\nhundreds of errors in the time it would take a human to annotate bounding boxes\nfrom scratch. However, even the best methods still miss up to 66% of the true\nerrors and with low quality labels introduce more errors than they find. This\nhighlights the urgent need for further research, now enabled by our released\nbenchmark.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06886", "pdf": "https://arxiv.org/pdf/2508.06886", "abs": "https://arxiv.org/abs/2508.06886", "authors": ["Arpita Saggar", "Jonathan C. Darling", "Vania Dimitrova", "Duygu Sarikaya", "David C. Hogg"], "title": "Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores", "categories": ["cs.CL"], "comment": "Camera-Ready version for ECAI 2025. 8 pages", "summary": "Persona-based dialogue generation is an important milestone towards building\nconversational artificial intelligence. Despite the ever-improving capabilities\nof large language models (LLMs), effectively integrating persona fidelity in\nconversations remains challenging due to the limited diversity in existing\ndialogue data. We propose a novel framework SBS (Score-Before-Speaking), which\noutperforms previous methods and yields improvements for both million and\nbillion-parameter models. Unlike previous methods, SBS unifies the learning of\nresponses and their relative quality into a single step. The key innovation is\nto train a dialogue model to correlate augmented responses with a quality score\nduring training and then leverage this knowledge at inference. We use\nnoun-based substitution for augmentation and semantic similarity-based scores\nas a proxy for response quality. Through extensive experiments with benchmark\ndatasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training\nallows existing models to better capture a spectrum of persona-consistent\ndialogues. Our ablation studies also demonstrate that including scores in the\ninput prompt during training is superior to conventional training setups. Code\nand further details are available at\nhttps://arpita2512.github.io/score_before_you_speak", "AI": {"tldr": "\u63d0\u51faSBS\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u54cd\u5e94\u8d28\u91cf\u5206\u6570\u7eb3\u5165\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89d2\u8272\u5316\u5bf9\u8bdd\u751f\u6210\u4e2d\u7684\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u529b\u4e0d\u65ad\u63d0\u5347\uff0c\u4f46\u7531\u4e8e\u73b0\u6709\u5bf9\u8bdd\u6570\u636e\u591a\u6837\u6027\u6709\u9650\uff0c\u5728\u5bf9\u8bdd\u4e2d\u6709\u6548\u6574\u5408\u89d2\u8272\uff08persona\uff09\u4fdd\u771f\u5ea6\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faSBS\uff08Score-Before-Speaking\uff09\u6846\u67b6\uff0c\u5c06\u54cd\u5e94\u5b66\u4e60\u53ca\u5176\u76f8\u5bf9\u8d28\u91cf\u5224\u65ad\u7edf\u4e00\u4e8e\u5355\u4e2a\u6b65\u9aa4\u3002\u6838\u5fc3\u521b\u65b0\u662f\u5728\u8bad\u7ec3\u4e2d\u8ba9\u5bf9\u8bdd\u6a21\u578b\u5c06\u589e\u5f3a\u7684\u54cd\u5e94\u4e0e\u8d28\u91cf\u5206\u6570\u5173\u8054\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u5229\u7528\u8be5\u77e5\u8bc6\u3002\u4f7f\u7528\u540d\u8bcd\u66ff\u6362\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u5e76\u4ee5\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5206\u6570\u4f5c\u4e3a\u54cd\u5e94\u8d28\u91cf\u4ee3\u7406\u3002", "result": "SBS\u6846\u67b6\u5728\u767e\u4e07\u548c\u5341\u4ebf\u53c2\u6570\u6a21\u578b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u6355\u6349\u89d2\u8272\u4e00\u81f4\u5bf9\u8bdd\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u8bad\u7ec3\u65f6\u5c06\u5206\u6570\u7eb3\u5165\u8f93\u5165\u63d0\u793a\u4f18\u4e8e\u4f20\u7edf\u8bad\u7ec3\u8bbe\u7f6e\u3002", "conclusion": "SBS\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u5206\u6570\u6761\u4ef6\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u5bfc\u81f4\u7684\u89d2\u8272\u6574\u5408\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89d2\u8272\u5316\u5bf9\u8bdd\u751f\u6210\u4e2d\u7684\u8868\u73b0\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.07934", "pdf": "https://arxiv.org/pdf/2508.07934", "abs": "https://arxiv.org/abs/2508.07934", "authors": ["Lorenzo La Corte", "Syed Aftab Rashid", "Andrei-Marian Dan"], "title": "Performance Evaluation of Brokerless Messaging Libraries", "categories": ["cs.DC", "cs.NI"], "comment": "11 pages, 9 figures", "summary": "Messaging systems are essential for efficiently transferring large volumes of\ndata, ensuring rapid response times and high-throughput communication. The\nstate-of-the-art on messaging systems mainly focuses on the performance\nevaluation of brokered messaging systems, which use an intermediate broker to\nguarantee reliability and quality of service. However, over the past decade,\nbrokerless messaging systems have emerged, eliminating the single point of\nfailure and trading off reliability guarantees for higher performance. Still,\nthe state-of-the-art on evaluating the performance of brokerless systems is\nscarce. In this work, we solely focus on brokerless messaging systems. First,\nwe perform a qualitative analysis of several possible candidates, to find the\nmost promising ones. We then design and implement an extensive open-source\nbenchmarking suite to systematically and fairly evaluate the performance of the\nchosen libraries, namely, ZeroMQ, NanoMsg, and NanoMsg-Next-Generation (NNG).\nWe evaluate these libraries considering different metrics and workload\nconditions, and provide useful insights into their limitations. Our analysis\nenables practitioners to select the most suitable library for their\nrequirements.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8bbe\u8ba1\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86ZeroMQ\u3001NanoMsg\u548cNNG\u7b49\u65e0\u4ee3\u7406\u6d88\u606f\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u65e8\u5728\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u9009\u578b\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u6d88\u606f\u7cfb\u7edf\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u6709\u4ee3\u7406\u7cfb\u7edf\uff0c\u800c\u65e0\u4ee3\u7406\u7cfb\u7edf\u5728\u63d0\u4f9b\u66f4\u9ad8\u6027\u80fd\u3001\u907f\u514d\u5355\u70b9\u6545\u969c\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5176\u6027\u80fd\u8bc4\u4f30\u7814\u7a76\u532e\u4e4f\u3002", "method": "\u9996\u5148\u5bf9\u65e0\u4ee3\u7406\u6d88\u606f\u7cfb\u7edf\u5019\u9009\u8005\u8fdb\u884c\u5b9a\u6027\u5206\u6790\u7b5b\u9009\uff1b\u968f\u540e\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e00\u4e2a\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u8bc4\u4f30ZeroMQ\u3001NanoMsg\u548cNNG\u5728\u4e0d\u540c\u6307\u6807\u548c\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u4e86ZeroMQ\u3001NanoMsg\u548cNNG\u7b49\u5e93\u5728\u4e0d\u540c\u5ea6\u91cf\u6807\u51c6\u548c\u5de5\u4f5c\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u5b83\u4eec\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u5206\u6790\u7ed3\u679c\u80fd\u5e2e\u52a9\u5b9e\u8df5\u8005\u6839\u636e\u81ea\u8eab\u9700\u6c42\u9009\u62e9\u6700\u5408\u9002\u7684\u65e0\u4ee3\u7406\u6d88\u606f\u7cfb\u7edf\u5e93\u3002"}}
{"id": "2508.06663", "pdf": "https://arxiv.org/pdf/2508.06663", "abs": "https://arxiv.org/abs/2508.06663", "authors": ["Yuan-Hung Chao", "Chia-Hsun Lu", "Chih-Ya Shen"], "title": "Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold Networks", "categories": ["cs.LG"], "comment": "6 pages, 3 tables", "summary": "Graph Neural Networks (GNNs) have shown strong performance on\ngraph-structured data, but their reliance on graph connectivity often limits\nscalability and efficiency. Kolmogorov-Arnold Networks (KANs), a recent\narchitecture with learnable univariate functions, offer strong nonlinear\nexpressiveness and efficient inference. In this work, we integrate KANs into\nthree popular GNN architectures-GAT, SGC, and APPNP-resulting in three new\nmodels: KGAT, KSGC, and KAPPNP. We further adopt a multi-teacher knowledge\namalgamation framework, where knowledge from multiple KAN-based GNNs is\ndistilled into a graph-independent KAN student model. Experiments on benchmark\ndatasets show that the proposed models improve node classification accuracy,\nand the knowledge amalgamation approach significantly boosts student model\nperformance. Our findings highlight the potential of KANs for enhancing GNN\nexpressiveness and for enabling efficient, graph-free inference.", "AI": {"tldr": "\u672c\u6587\u5c06Kolmogorov-Arnold Networks (KANs) \u96c6\u6210\u5230GNNs\u4e2d\uff0c\u6784\u5efa\u4e86KGAT\u3001KSGC\u3001KAPPNP\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u8282\u70b9\u5206\u7c7b\u7cbe\u5ea6\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u591a\u6559\u5e08\u77e5\u8bc6\u878d\u5408\u6846\u67b6\uff0c\u5c06KAN-GNNs\u7684\u77e5\u8bc6\u84b8\u998f\u5230\u56fe\u65e0\u5173\u7684KAN\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u65e0\u56fe\u7684\u63a8\u7406\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u4f9d\u8d56\u56fe\u8fde\u63a5\u6027\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002Kolmogorov-Arnold Networks\uff08KANs\uff09\u5177\u6709\u5f3a\u5927\u7684\u975e\u7ebf\u6027\u8868\u8fbe\u80fd\u529b\u548c\u9ad8\u6548\u63a8\u7406\u80fd\u529b\uff0c\u7814\u7a76\u65e8\u5728\u7ed3\u5408\u4e8c\u8005\u514b\u670dGNN\u7684\u5c40\u9650\u6027\u3002", "method": "1. \u5c06KANs\u96c6\u6210\u5230GAT\u3001SGC\u548cAPPNP\u4e09\u79cd\u6d41\u884cGNN\u67b6\u6784\u4e2d\uff0c\u5f62\u6210KGAT\u3001KSGC\u548cKAPPNP\u6a21\u578b\u30022. \u91c7\u7528\u591a\u6559\u5e08\u77e5\u8bc6\u878d\u5408\u6846\u67b6\uff0c\u5c06\u6765\u81ea\u591a\u4e2a\u57fa\u4e8eKAN\u7684GNNs\u7684\u77e5\u8bc6\u84b8\u998f\u5230\u4e00\u4e2a\u56fe\u65e0\u5173\u7684KAN\u5b66\u751f\u6a21\u578b\u4e2d\u3002", "result": "1. \u63d0\u51fa\u7684\u6a21\u578b\uff08KGAT\u3001KSGC\u3001KAPPNP\uff09\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e86\u8282\u70b9\u5206\u7c7b\u7cbe\u5ea6\u30022. \u77e5\u8bc6\u878d\u5408\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86KANs\u589e\u5f3aGNN\u8868\u8fbe\u80fd\u529b\u4ee5\u53ca\u5b9e\u73b0\u9ad8\u6548\u3001\u65e0\u56fe\u63a8\u7406\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.06894", "pdf": "https://arxiv.org/pdf/2508.06894", "abs": "https://arxiv.org/abs/2508.06894", "authors": ["Giovanni Varricchione", "Toryn Q. Klassen", "Natasha Alechina", "Mehdi Dastani", "Brian Logan", "Sheila A. McIlraith"], "title": "Pushdown Reward Machines for Reinforcement Learning", "categories": ["cs.AI", "cs.LG", "68T05"], "comment": null, "summary": "Reward machines (RMs) are automata structures that encode (non-Markovian)\nreward functions for reinforcement learning (RL). RMs can reward any behaviour\nrepresentable in regular languages and, when paired with RL algorithms that\nexploit RM structure, have been shown to significantly improve sample\nefficiency in many domains. In this work, we present pushdown reward machines\n(pdRMs), an extension of reward machines based on deterministic pushdown\nautomata. pdRMs can recognize and reward temporally extended behaviours\nrepresentable in deterministic context-free languages, making them more\nexpressive than reward machines. We introduce two variants of pdRM-based\npolicies, one which has access to the entire stack of the pdRM, and one which\ncan only access the top $k$ symbols (for a given constant $k$) of the stack. We\npropose a procedure to check when the two kinds of policies (for a given\nenvironment, pdRM, and constant $k$) achieve the same optimal expected reward.\nWe then provide theoretical results establishing the expressive power of pdRMs,\nand space complexity results about the proposed learning problems. Finally, we\nprovide experimental results showing how agents can be trained to perform tasks\nrepresentable in deterministic context-free languages using pdRMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e0b\u63a8\u5956\u52b1\u673a\uff08pdRMs\uff09\uff0c\u4f5c\u4e3a\u5956\u52b1\u673a\uff08RMs\uff09\u7684\u6269\u5c55\uff0c\u80fd\u591f\u8bc6\u522b\u548c\u5956\u52b1\u786e\u5b9a\u6027\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\u6240\u8868\u793a\u7684\u66f4\u590d\u6742\u3001\u66f4\u5177\u65f6\u95f4\u6269\u5c55\u6027\u7684\u884c\u4e3a\uff0c\u5e76\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u673a\uff08RMs\uff09\u53ea\u80fd\u5904\u7406\u6b63\u5219\u8bed\u8a00\u8868\u793a\u7684\u884c\u4e3a\uff0c\u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u65f6\u95f4\u6269\u5c55\u884c\u4e3a\uff0c\u9700\u8981\u66f4\u5177\u8868\u8fbe\u80fd\u529b\u7684\u5956\u52b1\u51fd\u6570\u673a\u5236\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u786e\u5b9a\u6027\u4e0b\u63a8\u81ea\u52a8\u673a\u7684\u4e0b\u63a8\u5956\u52b1\u673a\uff08pdRMs\uff09\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8epdRM\u7684\u7b56\u7565\u53d8\u4f53\uff1a\u4e00\u79cd\u53ef\u8bbf\u95ee\u6574\u4e2a\u6808\uff0c\u53e6\u4e00\u79cd\u4ec5\u8bbf\u95ee\u6808\u9876k\u4e2a\u7b26\u53f7\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u67e5\u4e24\u79cd\u7b56\u7565\u4f55\u65f6\u80fd\u8fbe\u5230\u76f8\u540c\u6700\u4f18\u9884\u671f\u5956\u52b1\u7684\u7a0b\u5e8f\u3002", "result": "\u7406\u8bba\u4e0a\uff0c\u5efa\u7acb\u4e86pdRMs\u7684\u8868\u8fbe\u80fd\u529b\u548c\u5b66\u4e60\u95ee\u9898\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\u3002\u5b9e\u9a8c\u4e0a\uff0c\u5c55\u793a\u4e86\u4ee3\u7406\u5982\u4f55\u901a\u8fc7pdRMs\u8bad\u7ec3\u4ee5\u6267\u884c\u786e\u5b9a\u6027\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\u8868\u793a\u7684\u4efb\u52a1\u3002", "conclusion": "pdRMs\u663e\u8457\u589e\u5f3a\u4e86\u5956\u52b1\u51fd\u6570\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4f7f\u5f97\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u80fd\u591f\u5b66\u4e60\u5e76\u6267\u884c\u6bd4\u6b63\u5219\u8bed\u8a00\u66f4\u590d\u6742\u7684\u3001\u65f6\u95f4\u4e0a\u6269\u5c55\u7684\u884c\u4e3a\uff0c\u4ece\u800c\u514b\u670d\u4e86\u73b0\u6709\u5956\u52b1\u673a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.06558", "pdf": "https://arxiv.org/pdf/2508.06558", "abs": "https://arxiv.org/abs/2508.06558", "authors": ["Simon Baur", "Alexandra Benova", "Emilio Dolgener Cant\u00fa", "Jackie Ma"], "title": "On the effectiveness of multimodal privileged knowledge distillation in two vision transformer based diagnostic applications", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Deploying deep learning models in clinical practice often requires leveraging\nmultiple data modalities, such as images, text, and structured data, to achieve\nrobust and trustworthy decisions. However, not all modalities are always\navailable at inference time. In this work, we propose multimodal privileged\nknowledge distillation (MMPKD), a training strategy that utilizes additional\nmodalities available solely during training to guide a unimodal vision model.\nSpecifically, we used a text-based teacher model for chest radiographs\n(MIMIC-CXR) and a tabular metadata-based teacher model for mammography\n(CBIS-DDSM) to distill knowledge into a vision transformer student model. We\nshow that MMPKD can improve the resulting attention maps' zero-shot\ncapabilities of localizing ROI in input images, while this effect does not\ngeneralize across domains, as contrarily suggested by prior research.", "AI": {"tldr": "\u591a\u6a21\u6001\u7279\u6743\u77e5\u8bc6\u84b8\u998f\uff08MMPKD\uff09\u5229\u7528\u8bad\u7ec3\u65f6\u72ec\u6709\u7684\u8f85\u52a9\u6a21\u6001\uff0c\u63d0\u5347\u5355\u6a21\u6001\u89c6\u89c9\u6a21\u578b\u96f6\u6837\u672cROI\u5b9a\u4f4d\u80fd\u529b\uff0c\u4f46\u5176\u8de8\u57df\u6cdb\u5316\u6027\u6709\u9650\u3002", "motivation": "\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u90e8\u7f72\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5e38\u9700\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u4ee5\u5b9e\u73b0\u7a33\u5065\u51b3\u7b56\uff0c\u4f46\u5e76\u975e\u6240\u6709\u6a21\u6001\u5728\u63a8\u7406\u65f6\u90fd\u53ef\u7528\uff0c\u8fd9\u7ed9\u5355\u6a21\u6001\u6a21\u578b\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u7279\u6743\u77e5\u8bc6\u84b8\u998f\uff08MMPKD\uff09\u8bad\u7ec3\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5229\u7528\u4ec5\u5728\u8bad\u7ec3\u65f6\u53ef\u7528\u7684\u989d\u5916\u6a21\u6001\u6765\u6307\u5bfc\u5355\u6a21\u6001\u89c6\u89c9\u6a21\u578b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4ed6\u4eec\u4f7f\u7528\u57fa\u4e8e\u6587\u672c\u7684\u6559\u5e08\u6a21\u578b\u5904\u7406\u80f8\u90e8X\u5149\u7247\uff08MIMIC-CXR\uff09\uff0c\u4ee5\u53ca\u57fa\u4e8e\u8868\u683c\u5143\u6570\u636e\u7684\u6559\u5e08\u6a21\u578b\u5904\u7406\u4e73\u817aX\u5149\u7247\uff08CBIS-DDSM\uff09\uff0c\u5c06\u77e5\u8bc6\u84b8\u998f\u5230\u89c6\u89c9Transformer\u5b66\u751f\u6a21\u578b\u4e2d\u3002", "result": "MMPKD\u80fd\u591f\u63d0\u5347\u6240\u751f\u6210\u6ce8\u610f\u529b\u56fe\u5728\u8f93\u5165\u56fe\u50cf\u4e2d\u5b9a\u4f4d\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\u7684\u96f6\u6837\u672c\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u6548\u679c\u5e76\u672a\u8de8\u9886\u57df\u6cdb\u5316\uff0c\u8fd9\u4e0e\u5148\u524d\u7684\u7814\u7a76\u7ed3\u679c\u76f8\u6096\u3002", "conclusion": "MMPKD\u662f\u4e00\u79cd\u5229\u7528\u7279\u6743\u77e5\u8bc6\u589e\u5f3a\u5355\u6a21\u6001\u89c6\u89c9\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u5b9a\u4f4d\u80fd\u529b\u7684\u6709\u6548\u7b56\u7565\u3002\u7136\u800c\uff0c\u5176\u5728\u4e0d\u540c\u4e34\u5e8a\u9886\u57df\u95f4\u7684\u6cdb\u5316\u6027\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\uff0c\u56e0\u4e3a\u5b83\u5e76\u672a\u50cf\u5148\u524d\u5047\u8bbe\u7684\u90a3\u6837\u666e\u904d\u6cdb\u5316\u3002"}}
{"id": "2508.06913", "pdf": "https://arxiv.org/pdf/2508.06913", "abs": "https://arxiv.org/abs/2508.06913", "authors": ["Siyuan Li", "Xi Lin", "Guangyan Li", "Zehao Liu", "Aodu Wulianghai", "Li Ding", "Jun Wu", "Jianhua Li"], "title": "Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has resulted in\nincreasingly sophisticated AI-generated content, posing significant challenges\nin distinguishing LLM-generated text from human-written language. Existing\ndetection methods, primarily based on lexical heuristics or fine-tuned\nclassifiers, often suffer from limited generalizability and are vulnerable to\nparaphrasing, adversarial perturbations, and cross-domain shifts. In this work,\nwe propose SentiDetect, a model-agnostic framework for detecting LLM-generated\ntext by analyzing the divergence in sentiment distribution stability. Our\nmethod is motivated by the empirical observation that LLM outputs tend to\nexhibit emotionally consistent patterns, whereas human-written texts display\ngreater emotional variability. To capture this phenomenon, we define two\ncomplementary metrics: sentiment distribution consistency and sentiment\ndistribution preservation, which quantify stability under sentiment-altering\nand semantic-preserving transformations. We evaluate SentiDetect on five\ndiverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,\nClaude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its\nsuperiority over state-of-the-art baselines, with over 16% and 11% F1 score\nimprovements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,\nSentiDetect also shows greater robustness to paraphrasing, adversarial attacks,\nand text length variations, outperforming existing detectors in challenging\nscenarios.", "AI": {"tldr": "SentiDetect\u901a\u8fc7\u5206\u6790\u60c5\u611f\u5206\u5e03\u7a33\u5b9a\u6027\u6765\u68c0\u6d4bLLM\u751f\u6210\u6587\u672c\uff0c\u5728\u6027\u80fd\u548c\u9c81\u68d2\u6027\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u68c0\u6d4b\u65b9\u6cd5\u901a\u7528\u6027\u5dee\u4e14\u6613\u53d7\u653b\u51fb\uff0c\u4e14\u89c2\u5bdf\u5230LLM\u8f93\u51fa\u7684\u60c5\u611f\u6a21\u5f0f\u6bd4\u4eba\u7c7b\u6587\u672c\u66f4\u4e00\u81f4\u3002", "method": "\u63d0\u51faSentiDetect\uff0c\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u60c5\u611f\u5206\u5e03\u7a33\u5b9a\u6027\u7684\u5dee\u5f02\u6765\u68c0\u6d4bLLM\u751f\u6210\u6587\u672c\u3002\u5b9a\u4e49\u4e86\u60c5\u611f\u5206\u5e03\u4e00\u81f4\u6027\u548c\u60c5\u611f\u5206\u5e03\u4fdd\u6301\u4e24\u4e2a\u4e92\u8865\u6307\u6807\u6765\u91cf\u5316\u8fd9\u79cd\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u5148\u8fdbLLM\u4e0a\uff0cSentiDetect\u7684F1\u5206\u6570\u6bd4SOTA\u57fa\u7ebf\u63d0\u5347\u663e\u8457\uff08\u5982Gemini-1.5-Pro\u63d0\u5347\u8d8516%\uff0cGPT-4-0613\u63d0\u5347\u8d8511%\uff09\uff0c\u5e76\u5bf9\u91cd\u8ff0\u3001\u5bf9\u6297\u6027\u653b\u51fb\u548c\u6587\u672c\u957f\u5ea6\u53d8\u5316\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SentiDetect\u5229\u7528LLM\u548c\u4eba\u7c7b\u6587\u672c\u60c5\u611f\u6a21\u5f0f\u7684\u56fa\u6709\u5dee\u5f02\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9c81\u68d2\u7684LLM\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6848\uff0c\u5728\u6311\u6218\u6027\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2508.06676", "pdf": "https://arxiv.org/pdf/2508.06676", "abs": "https://arxiv.org/abs/2508.06676", "authors": ["Chia-Hsun Lu", "Guan-Jhih Wu", "Ya-Chi Ho", "Chih-Ya Shen"], "title": "Watermarking Kolmogorov-Arnold Networks for Emerging Networked Applications via Activation Perturbation", "categories": ["cs.LG"], "comment": "6 pages, 3 figures, 6 tables", "summary": "With the increasing importance of protecting intellectual property in machine\nlearning, watermarking techniques have gained significant attention. As\nadvanced models are increasingly deployed in domains such as social network\nanalysis, the need for robust model protection becomes even more critical.\nWhile existing watermarking methods have demonstrated effectiveness for\nconventional deep neural networks, they often fail to adapt to the novel\narchitecture, Kolmogorov-Arnold Networks (KAN), which feature learnable\nactivation functions. KAN holds strong potential for modeling complex\nrelationships in network-structured data. However, their unique design also\nintroduces new challenges for watermarking. Therefore, we propose a novel\nwatermarking method, Discrete Cosine Transform-based Activation Watermarking\n(DCT-AW), tailored for KAN. Leveraging the learnable activation functions of\nKAN, our method embeds watermarks by perturbing activation outputs using\ndiscrete cosine transform, ensuring compatibility with diverse tasks and\nachieving task independence. Experimental results demonstrate that DCT-AW has a\nsmall impact on model performance and provides superior robustness against\nvarious watermark removal attacks, including fine-tuning, pruning, and\nretraining after pruning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u79bb\u6563\u4f59\u5f26\u53d8\u6362\u7684\u6fc0\u6d3b\u6c34\u5370\uff08DCT-AW\uff09\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u4fdd\u62a4\u79d1\u5c14\u83ab\u54e5\u6d1b\u592b-\u963f\u8bfa\u5fb7\u7f51\u7edc\uff08KAN\uff09\u7684\u77e5\u8bc6\u4ea7\u6743\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u6a21\u578b\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u65e5\u76ca\u91cd\u8981\uff0c\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u65b0\u5174\u7684\u79d1\u5c14\u83ab\u54e5\u6d1b\u592b-\u963f\u8bfa\u5fb7\u7f51\u7edc\uff08KAN\uff09\u72ec\u7279\u7684\u67b6\u6784\uff08\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\uff09\uff0c\u800cKAN\u5728\u590d\u6742\u5173\u7cfb\u5efa\u6a21\u4e2d\u6f5c\u529b\u5de8\u5927\u3002", "method": "\u63d0\u51faDCT-AW\u65b9\u6cd5\uff0c\u5229\u7528KAN\u7684\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\uff0c\u901a\u8fc7\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff08DCT\uff09\u6270\u52a8\u6fc0\u6d3b\u8f93\u51fa\u4ee5\u5d4c\u5165\u6c34\u5370\uff0c\u786e\u4fdd\u65b9\u6cd5\u4e0e\u4efb\u52a1\u517c\u5bb9\u5e76\u5b9e\u73b0\u4efb\u52a1\u72ec\u7acb\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDCT-AW\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\uff0c\u4e14\u5bf9\u591a\u79cd\u6c34\u5370\u79fb\u9664\u653b\u51fb\uff08\u5982\u5fae\u8c03\u3001\u526a\u679d\u3001\u526a\u679d\u540e\u91cd\u8bad\u7ec3\uff09\u5177\u6709\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "DCT-AW\u4e3aKAN\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9c81\u68d2\u7684\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u673a\u5236\uff0c\u514b\u670d\u4e86\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u5728KAN\u4e0a\u5e94\u7528\u9762\u4e34\u7684\u6311\u6218\u3002"}}
{"id": "2508.06899", "pdf": "https://arxiv.org/pdf/2508.06899", "abs": "https://arxiv.org/abs/2508.06899", "authors": ["Yanchen Deng", "Xinrun Wang", "Bo An"], "title": "GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization", "categories": ["cs.AI", "cs.DM"], "comment": null, "summary": "Local search is an important class of incomplete algorithms for solving\nDistributed Constraint Optimization Problems (DCOPs) but it often converges to\npoor local optima. While GDBA provides a comprehensive rule set to escape\npremature convergence, its empirical benefits remain marginal on general-valued\nproblems. In this work, we systematically examine GDBA and identify three\nfactors that potentially lead to its inferior performance, i.e.,\nover-aggressive constraint violation conditions, unbounded penalty\naccumulation, and uncoordinated penalty updates. To address these issues, we\npropose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs\nthat incorporates an adaptive violation condition to selectively penalize\nconstraints with high cost, a penalty evaporation mechanism to control the\nmagnitude of penalization, and a synchronization scheme for coordinated penalty\nupdates. We theoretically show that the penalty values are bounded, and agents\nplay a potential game in our DGLS. Our extensive empirical results on various\nstandard benchmarks demonstrate the great superiority of DGLS over\nstate-of-the-art baselines. Particularly, compared to Damped Max-sum with high\ndamping factors (e.g., 0.7 or 0.9), our DGLS achieves competitive performance\non general-valued problems, and outperforms it by significant margins\n(\\textbf{3.77\\%--66.3\\%}) on structured problems in terms of anytime results.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u5f15\u5bfc\u5c40\u90e8\u641c\u7d22(DGLS)\u6846\u67b6\uff0c\u89e3\u51b3\u4e86DCOP\u5c40\u90e8\u641c\u7d22\u7b97\u6cd5\u6613\u9677\u4e8e\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u8d8a\u6027\u3002", "motivation": "\u5206\u5e03\u5f0f\u7ea6\u675f\u4f18\u5316\u95ee\u9898(DCOP)\u7684\u5c40\u90e8\u641c\u7d22\u7b97\u6cd5\u5e38\u6536\u655b\u4e8e\u8f83\u5dee\u7684\u5c40\u90e8\u6700\u4f18\u3002\u73b0\u6709\u65b9\u6cd5GDBA\u6548\u679c\u4e0d\u4f73\uff0c\u539f\u56e0\u5728\u4e8e\u8fc7\u6fc0\u7684\u7ea6\u675f\u8fdd\u53cd\u6761\u4ef6\u3001\u65e0\u9650\u5236\u7684\u60e9\u7f5a\u7d2f\u79ef\u548c\u4e0d\u534f\u8c03\u7684\u60e9\u7f5a\u66f4\u65b0\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u5f0f\u5f15\u5bfc\u5c40\u90e8\u641c\u7d22(DGLS)\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u81ea\u9002\u5e94\u8fdd\u53cd\u6761\u4ef6\u4ee5\u9009\u62e9\u6027\u60e9\u7f5a\u9ad8\u6210\u672c\u7ea6\u675f\u3001\u60e9\u7f5a\u84b8\u53d1\u673a\u5236\u4ee5\u63a7\u5236\u60e9\u7f5a\u91cf\u7ea7\u3001\u4ee5\u53ca\u540c\u6b65\u673a\u5236\u4ee5\u534f\u8c03\u60e9\u7f5a\u66f4\u65b0\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u60e9\u7f5a\u503c\u6709\u754c\u4e14\u667a\u80fd\u4f53\u5728DGLS\u4e2d\u8fdb\u884c\u52bf\u535a\u5f08\u3002", "result": "DGLS\u5728\u5404\u79cd\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u8868\u73b0\u51fa\u663e\u8457\u4f18\u8d8a\u6027\u3002\u5c24\u5176\u5728\u7ed3\u6784\u5316\u95ee\u9898\u4e0a\uff0cDGLS\u5728\u4efb\u4e00\u65f6\u95f4\u7ed3\u679c\u4e0a\u5747\u663e\u8457\u4f18\u4e8eDamped Max-sum\uff08\u4f18\u52bf\u5e45\u5ea6\u8fbe3.77%-66.3%\uff09\uff0c\u5728\u4e00\u822c\u503c\u95ee\u9898\u4e0a\u8868\u73b0\u4e5f\u5177\u7ade\u4e89\u529b\u3002", "conclusion": "DGLS\u901a\u8fc7\u6539\u8fdb\u5f15\u5bfc\u5c40\u90e8\u641c\u7d22\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86DCOP\u4e2d\u5c40\u90e8\u641c\u7d22\u7684\u7f3a\u9677\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6027\u80fd\u4f18\u8d8a\u4e14\u7406\u8bba\u57fa\u7840\u7a33\u56fa\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.06564", "pdf": "https://arxiv.org/pdf/2508.06564", "abs": "https://arxiv.org/abs/2508.06564", "authors": ["Guanyu Hu", "Dimitrios Kollias", "Xinyu Yang"], "title": "Grounding Emotion Recognition with Visual Prototypes: VEGA -- Revisiting CLIP in MERC", "categories": ["cs.CV"], "comment": "accepted for publication at ACM Multimedia (ACM MM) 2025", "summary": "Multimodal Emotion Recognition in Conversations remains a challenging task\ndue to the complex interplay of textual, acoustic and visual signals. While\nrecent models have improved performance via advanced fusion strategies, they\noften lack psychologically meaningful priors to guide multimodal alignment. In\nthis paper, we revisit the use of CLIP and propose a novel Visual Emotion\nGuided Anchoring (VEGA) mechanism that introduces class-level visual semantics\ninto the fusion and classification process. Distinct from prior work that\nprimarily utilizes CLIP's textual encoder, our approach leverages its image\nencoder to construct emotion-specific visual anchors based on facial exemplars.\nThese anchors guide unimodal and multimodal features toward a perceptually\ngrounded and psychologically aligned representation space, drawing inspiration\nfrom cognitive theories (prototypical emotion categories and multisensory\nintegration). A stochastic anchor sampling strategy further enhances robustness\nby balancing semantic stability and intra-class diversity. Integrated into a\ndual-branch architecture with self-distillation, our VEGA-augmented model\nachieves sota performance on IEMOCAP and MELD. Code is available at:\nhttps://github.com/dkollias/VEGA.", "AI": {"tldr": "\u9488\u5bf9\u5bf9\u8bdd\u4e2d\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eCLIP\u56fe\u50cf\u7f16\u7801\u5668\u7684\u65b0\u578b\u89c6\u89c9\u60c5\u611f\u5f15\u5bfc\u951a\u5b9a\uff08VEGA\uff09\u673a\u5236\uff0c\u901a\u8fc7\u6784\u5efa\u60c5\u611f\u89c6\u89c9\u951a\u70b9\u5f15\u5bfc\u7279\u5f81\u5bf9\u9f50\uff0c\u5e76\u5728IEMOCAP\u548cMELD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5bf9\u8bdd\u4e2d\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4ecd\u5177\u6311\u6218\uff0c\u73b0\u6709\u6a21\u578b\u867d\u901a\u8fc7\u9ad8\u7ea7\u878d\u5408\u7b56\u7565\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f46\u666e\u904d\u7f3a\u4e4f\u5fc3\u7406\u5b66\u610f\u4e49\u4e0a\u7684\u5148\u9a8c\u77e5\u8bc6\u6765\u6709\u6548\u6307\u5bfc\u591a\u6a21\u6001\u7279\u5f81\u7684\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9\u60c5\u611f\u5f15\u5bfc\u951a\u5b9a\uff08VEGA\uff09\u673a\u5236\uff0c\u8be5\u673a\u5236\u5229\u7528CLIP\u7684\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u800c\u975e\u6587\u672c\u7f16\u7801\u5668\uff0c\u57fa\u4e8e\u9762\u90e8\u793a\u4f8b\u6784\u5efa\u60c5\u611f\u7279\u5f02\u6027\u89c6\u89c9\u951a\u70b9\u3002\u8fd9\u4e9b\u951a\u70b9\u501f\u9274\u8ba4\u77e5\u7406\u8bba\uff0c\u65e8\u5728\u5c06\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u7279\u5f81\u5f15\u5bfc\u81f3\u611f\u77e5\u4e0a\u548c\u5fc3\u7406\u4e0a\u5bf9\u9f50\u7684\u8868\u793a\u7a7a\u95f4\u3002\u4e3a\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u8fd8\u5f15\u5165\u4e86\u968f\u673a\u951a\u70b9\u91c7\u6837\u7b56\u7565\uff0c\u5e76\u5c06VEGA\u673a\u5236\u6574\u5408\u5230\u91c7\u7528\u81ea\u84b8\u998f\u7684\u53cc\u5206\u652f\u67b6\u6784\u4e2d\u3002", "result": "\u6240\u63d0\u51fa\u7684VEGA\u589e\u5f3a\u6a21\u578b\u5728IEMOCAP\u548cMELD\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u6027\u80fd\u3002", "conclusion": "VEGA\u673a\u5236\u901a\u8fc7\u5f15\u5165\u5177\u6709\u5fc3\u7406\u5b66\u610f\u4e49\u7684\u89c6\u89c9\u5148\u9a8c\uff08\u60c5\u611f\u89c6\u89c9\u951a\u70b9\uff09\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u7279\u5f81\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u8868\u660e\u5c06\u8ba4\u77e5\u7406\u8bba\u878d\u5165\u6a21\u578b\u8bbe\u8ba1\u662f\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.06971", "pdf": "https://arxiv.org/pdf/2508.06971", "abs": "https://arxiv.org/abs/2508.06971", "authors": ["Mohamed Basem", "Islam Oshallah", "Ali Hamdi", "Khaled Shaban", "Hozaifa Kassab"], "title": "Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction", "categories": ["cs.CL", "cs.IR"], "comment": "8 pages , 4 figures , Accepted in Aiccsa 2025 ,\n  https://conferences.sigappfr.org/aiccsa2025/", "summary": "Quranic Question Answering presents unique challenges due to the linguistic\ncomplexity of Classical Arabic and the semantic richness of religious texts. In\nthis paper, we propose a novel two-stage framework that addresses both passage\nretrieval and answer extraction. For passage retrieval, we ensemble fine-tuned\nArabic language models to achieve superior ranking performance. For answer\nextraction, we employ instruction-tuned large language models with few-shot\nprompting to overcome the limitations of fine-tuning on small datasets. Our\napproach achieves state-of-the-art results on the Quran QA 2023 Shared Task,\nwith a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of\n0.669 for extraction, substantially outperforming previous methods. These\nresults demonstrate that combining model ensembling and instruction-tuned\nlanguage models effectively addresses the challenges of low-resource question\nanswering in specialized domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408\u6a21\u578b\u96c6\u6210\u548c\u6307\u4ee4\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u53e4\u5170\u7ecf\u95ee\u7b54\u4e2d\u7684\u4f4e\u8d44\u6e90\u6311\u6218\uff0c\u5e76\u5728\u76f8\u5173\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "motivation": "\u53e4\u5170\u7ecf\u95ee\u7b54\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u6e90\u4e8e\u53e4\u5178\u963f\u62c9\u4f2f\u8bed\u7684\u8bed\u8a00\u590d\u6742\u6027\u548c\u5b97\u6559\u6587\u672c\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7bc7\u7ae0\u68c0\u7d22\u9636\u6bb5\uff0c\u96c6\u6210\u5fae\u8c03\u7684\u963f\u62c9\u4f2f\u8bed\u8bed\u8a00\u6a21\u578b\uff1b\u7b54\u6848\u62bd\u53d6\u9636\u6bb5\uff0c\u91c7\u7528\u6307\u4ee4\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u5c11\u6837\u672c\u63d0\u793a\uff0c\u4ee5\u514b\u670d\u5c0f\u6570\u636e\u96c6\u5fae\u8c03\u7684\u5c40\u9650\u6027\u3002", "result": "\u5728Quran QA 2023\u5171\u4eab\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7ed3\u679c\uff1a\u68c0\u7d22\u7684MAP@10\u4e3a0.3128\uff0cMRR@10\u4e3a0.5763\uff1b\u62bd\u53d6\u7684pAP@10\u4e3a0.669\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u6a21\u578b\u96c6\u6210\u548c\u6307\u4ee4\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u80fd\u6709\u6548\u89e3\u51b3\u4e13\u4e1a\u9886\u57df\u4e2d\u4f4e\u8d44\u6e90\u95ee\u7b54\u7684\u6311\u6218\u3002"}}
{"id": "2508.06692", "pdf": "https://arxiv.org/pdf/2508.06692", "abs": "https://arxiv.org/abs/2508.06692", "authors": ["Md. Akmol Masud", "Md Abrar Jahin", "Mahmud Hasan"], "title": "Stabilizing Federated Learning under Extreme Heterogeneity with HeteRo-Select", "categories": ["cs.LG"], "comment": null, "summary": "Federated Learning (FL) is a machine learning technique that often suffers\nfrom training instability due to the diverse nature of client data. Although\nutility-based client selection methods like Oort are used to converge by\nprioritizing high-loss clients, they frequently experience significant drops in\naccuracy during later stages of training. We propose a theoretical\nHeteRo-Select framework designed to maintain high performance and ensure\nlong-term training stability. We provide a theoretical analysis showing that\nwhen client data is very different (high heterogeneity), choosing a smart\nsubset of client participation can reduce communication more effectively\ncompared to full participation. Our HeteRo-Select method uses a clear,\nstep-by-step scoring system that considers client usefulness, fairness, update\nspeed, and data variety. It also shows convergence guarantees under strong\nregularization. Our experimental results on the CIFAR-10 dataset under\nsignificant label skew ($\\alpha=0.1$) support the theoretical findings. The\nHeteRo-Select method performs better than existing approaches in terms of peak\naccuracy, final accuracy, and training stability. Specifically, HeteRo-Select\nachieves a peak accuracy of $74.75\\%$, a final accuracy of $72.76\\%$, and a\nminimal stability drop of $1.99\\%$. In contrast, Oort records a lower peak\naccuracy of $73.98\\%$, a final accuracy of $71.25\\%$, and a larger stability\ndrop of $2.73\\%$. The theoretical foundations and empirical performance in our\nstudy make HeteRo-Select a reliable solution for real-world heterogeneous FL\nproblems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06931", "pdf": "https://arxiv.org/pdf/2508.06931", "abs": "https://arxiv.org/abs/2508.06931", "authors": ["Wangyue Lu", "Lun Du", "Sirui Li", "Ke Weng", "Haozhe Sun", "Hengyu Liu", "Minghe Yu", "Tiancheng Zhang", "Ge Yu"], "title": "Automated Formalization via Conceptual Retrieval-Augmented LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Interactive theorem provers (ITPs) require manual formalization, which is\nlabor-intensive and demands expert knowledge. While automated formalization\noffers a potential solution, it faces two major challenges: model hallucination\n(e.g., undefined predicates, symbol misuse, and version incompatibility) and\nthe semantic gap caused by ambiguous or missing premises in natural language\ndescriptions. To address these issues, we propose CRAMF, a Concept-driven\nRetrieval-Augmented Mathematical Formalization framework. CRAMF enhances\nLLM-based autoformalization by retrieving formal definitions of core\nmathematical concepts, providing contextual grounding during code generation.\nHowever, applying retrieval-augmented generation (RAG) in this setting is\nnon-trivial due to the lack of structured knowledge bases, the polymorphic\nnature of mathematical concepts, and the high precision required in formal\nretrieval. We introduce a framework for automatically constructing a\nconcept-definition knowledge base from Mathlib4, the standard mathematical\nlibrary for the Lean 4 theorem prover, indexing over 26,000 formal definitions\nand 1,000+ core mathematical concepts. To address conceptual polymorphism, we\npropose contextual query augmentation with domain- and application-level\nsignals. In addition, we design a dual-channel hybrid retrieval strategy with\nreranking to ensure accurate and relevant definition retrieval. Experiments on\nminiF2F, ProofNet, and our newly proposed AdvancedMath benchmark show that\nCRAMF can be seamlessly integrated into LLM-based autoformalizers, yielding\nconsistent improvements in translation accuracy, achieving up to 62.1% and an\naverage of 29.9% relative improvement.", "AI": {"tldr": "CRAMF\u662f\u4e00\u4e2a\u6982\u5ff5\u9a71\u52a8\u7684\u68c0\u7d22\u589e\u5f3a\u6570\u5b66\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u6570\u5b66\u6982\u5ff5\u7684\u6b63\u5f0f\u5b9a\u4e49\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u9a71\u52a8\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u5e7b\u89c9\u548c\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\u3002", "motivation": "\u4ea4\u4e92\u5f0f\u5b9a\u7406\u8bc1\u660e\u5668\uff08ITPs\uff09\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5f62\u5f0f\u5316\u5de5\u4f5c\u548c\u4e13\u4e1a\u77e5\u8bc6\u3002\u81ea\u52a8\u5316\u5f62\u5f0f\u5316\u9762\u4e34\u6a21\u578b\u5e7b\u89c9\uff08\u5982\u672a\u5b9a\u4e49\u8c13\u8bcd\u3001\u7b26\u53f7\u8bef\u7528\uff09\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u6a21\u7cca\u6216\u7f3a\u5931\u524d\u63d0\u5bfc\u81f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u4e24\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86CRAMF\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u6838\u5fc3\u6570\u5b66\u6982\u5ff5\u7684\u6b63\u5f0f\u5b9a\u4e49\uff0c\u4e3aLLM\u9a71\u52a8\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u63d0\u4f9b\u4e0a\u4e0b\u6587\u57fa\u7840\u3002\u4e3a\u89e3\u51b3\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u6b64\u9886\u57df\u7684\u6311\u6218\uff0c\u6784\u5efa\u4e86Mathlib4\u4e2d\u7684\u6982\u5ff5-\u5b9a\u4e49\u77e5\u8bc6\u5e93\uff0c\u5e76\u9488\u5bf9\u6982\u5ff5\u591a\u6001\u6027\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u67e5\u8be2\u589e\u5f3a\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5e26\u6709\u91cd\u6392\u529f\u80fd\u7684\u53cc\u901a\u9053\u6df7\u5408\u68c0\u7d22\u7b56\u7565\u3002", "result": "CRAMF\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u5de5\u5177\u4e2d\uff0c\u5728miniF2F\u3001ProofNet\u548cAdvancedMath\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6301\u7eed\u63d0\u9ad8\u7ffb\u8bd1\u51c6\u786e\u6027\uff0c\u6700\u9ad8\u76f8\u5bf9\u6539\u8fdb\u8fbe62.1%\uff0c\u5e73\u5747\u76f8\u5bf9\u6539\u8fdb\u8fbe29.9%\u3002", "conclusion": "CRAMF\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5730\u5904\u7406\u5e7b\u89c9\u548c\u8bed\u4e49\u9e3f\u6c9f\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u9a71\u52a8\u7684\u6570\u5b66\u5f62\u5f0f\u5316\u80fd\u529b\uff0c\u4e3a\u81ea\u52a8\u5316\u5f62\u5f0f\u5316\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2508.06565", "pdf": "https://arxiv.org/pdf/2508.06565", "abs": "https://arxiv.org/abs/2508.06565", "authors": ["Jing Zhang", "Xiaowei Yu", "Minheng Chen", "Lu Zhang", "Tong Chen", "Yan Zhuang", "Chao Cao", "Yanjun Lyu", "Li Su", "Tianming Liu", "Dajiang Zhu"], "title": "Bridging Brain Connectomes and Clinical Reports for Early Alzheimer's Disease Diagnosis", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Integrating brain imaging data with clinical reports offers a valuable\nopportunity to leverage complementary multimodal information for more effective\nand timely diagnosis in practical clinical settings. This approach has gained\nsignificant attention in brain disorder research, yet a key challenge remains:\nhow to effectively link objective imaging data with subjective text-based\nreports, such as doctors' notes. In this work, we propose a novel framework\nthat aligns brain connectomes with clinical reports in a shared cross-modal\nlatent space at both the subject and connectome levels, thereby enhancing\nrepresentation learning. The key innovation of our approach is that we treat\nbrain subnetworks as tokens of imaging data, rather than raw image patches, to\nalign with word tokens in clinical reports. This enables a more efficient\nidentification of system-level associations between neuroimaging findings and\nclinical observations, which is critical since brain disorders often manifest\nas network-level abnormalities rather than isolated regional alterations. We\napplied our method to mild cognitive impairment (MCI) using the ADNI dataset.\nOur approach not only achieves state-of-the-art predictive performance but also\nidentifies clinically meaningful connectome-text pairs, offering new insights\ninto the early mechanisms of Alzheimer's disease and supporting the development\nof clinically useful multimodal biomarkers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u6846\u67b6\uff0c\u5c06\u8111\u8fde\u63a5\u7ec4\u4e0e\u4e34\u5e8a\u62a5\u544a\u5728\u5171\u4eab\u8de8\u6a21\u6001\u7a7a\u95f4\u4e2d\u5bf9\u9f50\uff0c\u901a\u8fc7\u5c06\u8111\u5b50\u7f51\u7edc\u89c6\u4e3a\u6570\u636e\u4ee4\u724c\uff0c\u5b9e\u73b0\u8111\u75be\u75c5\u7684\u65e9\u671f\u8bca\u65ad\u5e76\u53d1\u73b0\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u5173\u8054\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u6574\u5408\u5ba2\u89c2\u8111\u6210\u50cf\u6570\u636e\u4e0e\u4e3b\u89c2\u6587\u672c\u62a5\u544a\u65f6\u9762\u4e34\u6311\u6218\uff0c\u96be\u4ee5\u6709\u6548\u8fde\u63a5\u4e24\u8005\uff0c\u963b\u788d\u4e86\u591a\u6a21\u6001\u4fe1\u606f\u5728\u8111\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u6846\u67b6\uff0c\u5728\u5171\u4eab\u8de8\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u8111\u8fde\u63a5\u7ec4\u4e0e\u4e34\u5e8a\u62a5\u544a\u3002\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5c06\u8111\u5b50\u7f51\u7edc\u800c\u975e\u539f\u59cb\u56fe\u50cf\u5757\u89c6\u4e3a\u6210\u50cf\u6570\u636e\u4ee4\u724c\uff0c\u4e0e\u4e34\u5e8a\u62a5\u544a\u4e2d\u7684\u8bcd\u4ee4\u724c\u5bf9\u9f50\uff0c\u4ee5\u8bc6\u522b\u7cfb\u7edf\u7ea7\u5173\u8054\u3002", "result": "\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8eADNI\u6570\u636e\u96c6\u7684\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d(MCI)\u8bca\u65ad\uff0c\u4e0d\u4ec5\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u6210\u529f\u8bc6\u522b\u51fa\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u8fde\u63a5\u7ec4-\u6587\u672c\u5bf9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u652f\u6301\u5f00\u53d1\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u7684\u591a\u6a21\u6001\u751f\u7269\u6807\u5fd7\u7269\u3002"}}
{"id": "2508.06974", "pdf": "https://arxiv.org/pdf/2508.06974", "abs": "https://arxiv.org/abs/2508.06974", "authors": ["Zhijun Tu", "Hanting Chen", "Siqi Liu", "Chuanjian Liu", "Jian Li", "Jie Hu", "Yunhe Wang"], "title": "Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models", "categories": ["cs.CL"], "comment": "16 pages, 5 figures", "summary": "1-bit LLM quantization offers significant advantages in reducing storage and\ncomputational costs. However, existing methods typically train 1-bit LLMs from\nscratch, failing to fully leverage pre-trained models. This results in high\ntraining costs and notable accuracy degradation. We identify that the large gap\nbetween full precision and 1-bit representations makes direct adaptation\ndifficult. In this paper, we introduce a consistent progressive training for\nboth forward and backward, smoothly converting the floating-point weights into\nthe binarized ones. Additionally, we incorporate binary-aware initialization\nand dual-scaling compensation to reduce the difficulty of progressive training\nand improve the performance. Experimental results on LLMs of various sizes\ndemonstrate that our method outperforms existing approaches. Our results show\nthat high-performance 1-bit LLMs can be achieved using pre-trained models,\neliminating the need for expensive training from scratch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd1\u6bd4\u7279\u5927\u8bed\u8a00\u6a21\u578b\u91cf\u5316\uff0c\u907f\u514d\u4e86\u4ece\u5934\u8bad\u7ec3\u7684\u9ad8\u6602\u6210\u672c\u548c\u7cbe\u5ea6\u635f\u5931\u3002", "motivation": "\u73b0\u67091\u6bd4\u7279\u5927\u8bed\u8a00\u6a21\u578b\u91cf\u5316\u65b9\u6cd5\u901a\u5e38\u4ece\u5934\u8bad\u7ec3\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u4e14\u7cbe\u5ea6\u663e\u8457\u4e0b\u964d\u3002\u5168\u7cbe\u5ea6\u4e0e1\u6bd4\u7279\u8868\u793a\u4e4b\u95f4\u5b58\u5728\u5de8\u5927\u9e3f\u6c9f\uff0c\u4f7f\u5f97\u76f4\u63a5\u9002\u5e94\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u524d\u5411\u548c\u540e\u5411\u4e00\u81f4\u7684\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5c06\u6d6e\u70b9\u6743\u91cd\u5e73\u6ed1\u8f6c\u6362\u4e3a\u4e8c\u503c\u5316\u6743\u91cd\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e8c\u503c\u611f\u77e5\u521d\u59cb\u5316\u548c\u53cc\u5c3a\u5ea6\u8865\u507f\uff0c\u4ee5\u964d\u4f4e\u6e10\u8fdb\u8bad\u7ec3\u96be\u5ea6\u5e76\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u53ef\u4ee5\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u9ad8\u6027\u80fd1\u6bd4\u7279\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u4ece\u5934\u8fdb\u884c\u6602\u8d35\u7684\u8bad\u7ec3\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e861\u6bd4\u7279\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u5934\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u548c\u7cbe\u5ea6\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u53ef\u4ee5\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u9ad8\u6027\u80fd1\u6bd4\u7279\u5927\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2508.06704", "pdf": "https://arxiv.org/pdf/2508.06704", "abs": "https://arxiv.org/abs/2508.06704", "authors": ["Hager Radi Abdelwahed", "M\u00e9lisande Teng", "Robin Zbinden", "Laura Pollock", "Hugo Larochelle", "Devis Tuia", "David Rolnick"], "title": "CISO: Species Distribution Modeling Conditioned on Incomplete Species Observations", "categories": ["cs.LG"], "comment": null, "summary": "Species distribution models (SDMs) are widely used to predict species'\ngeographic distributions, serving as critical tools for ecological research and\nconservation planning. Typically, SDMs relate species occurrences to\nenvironmental variables representing abiotic factors, such as temperature,\nprecipitation, and soil properties. However, species distributions are also\nstrongly influenced by biotic interactions with other species, which are often\noverlooked. While some methods partially address this limitation by\nincorporating biotic interactions, they often assume symmetrical pairwise\nrelationships between species and require consistent co-occurrence data. In\npractice, species observations are sparse, and the availability of information\nabout the presence or absence of other species varies significantly across\nlocations. To address these challenges, we propose CISO, a deep learning-based\nmethod for species distribution modeling Conditioned on Incomplete Species\nObservations. CISO enables predictions to be conditioned on a flexible number\nof species observations alongside environmental variables, accommodating the\nvariability and incompleteness of available biotic data. We demonstrate our\napproach using three datasets representing different species groups: sPlotOpen\nfor plants, SatBird for birds, and a new dataset, SatButterfly, for\nbutterflies. Our results show that including partial biotic information\nimproves predictive performance on spatially separate test sets. When\nconditioned on a subset of species within the same dataset, CISO outperforms\nalternative methods in predicting the distribution of the remaining species.\nFurthermore, we show that combining observations from multiple datasets can\nimprove performance. CISO is a promising ecological tool, capable of\nincorporating incomplete biotic information and identifying potential\ninteractions between species from disparate taxa.", "AI": {"tldr": "\u73b0\u6709\u7269\u79cd\u5206\u5e03\u6a21\u578b\u5e38\u5ffd\u7565\u751f\u7269\u4e92\u4f5c\uff0c\u4e14\u5904\u7406\u4e0d\u5b8c\u6574\u6570\u636e\u80fd\u529b\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51faCISO\uff0c\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u80fd\u6709\u6548\u6574\u5408\u4e0d\u5b8c\u6574\u7269\u79cd\u89c2\u6d4b\u6570\u636e\u548c\u73af\u5883\u53d8\u91cf\u8fdb\u884c\u5206\u5e03\u9884\u6d4b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u793a\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u7269\u79cd\u5206\u5e03\u6a21\u578b\uff08SDMs\uff09\u5728\u9884\u6d4b\u7269\u79cd\u5730\u7406\u5206\u5e03\u65f6\uff0c\u901a\u5e38\u5ffd\u7565\u7269\u79cd\u95f4\u751f\u7269\u4e92\u4f5c\u7684\u5f71\u54cd\u3002\u5c11\u6570\u8003\u8651\u751f\u7269\u4e92\u4f5c\u7684\u65b9\u6cd5\uff0c\u5b58\u5728\u5047\u8bbe\u5bf9\u79f0\u5173\u7cfb\u548c\u8981\u6c42\u5b8c\u6574\u5171\u73b0\u6570\u636e\u7684\u5c40\u9650\uff0c\u4e0e\u5b9e\u9645\u89c2\u6d4b\u6570\u636e\u7a00\u758f\u548c\u4e0d\u5b8c\u6574\u7684\u60c5\u51b5\u4e0d\u7b26\u3002", "method": "\u63d0\u51faCISO\uff08Conditioned on Incomplete Species Observations\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7269\u79cd\u5206\u5e03\u5efa\u6a21\u65b9\u6cd5\u3002CISO\u80fd\u6839\u636e\u7075\u6d3b\u6570\u91cf\u7684\u4e0d\u5b8c\u6574\u7269\u79cd\u89c2\u6d4b\u6570\u636e\u7ed3\u5408\u73af\u5883\u53d8\u91cf\u8fdb\u884c\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u5728sPlotOpen\uff08\u690d\u7269\uff09\u3001SatBird\uff08\u9e1f\u7c7b\uff09\u548cSatButterfly\uff08\u8774\u8776\uff09\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f15\u5165\u90e8\u5206\u751f\u7269\u4fe1\u606f\u80fd\u63d0\u5347\u5728\u7a7a\u95f4\u5206\u79bb\u6d4b\u8bd5\u96c6\u4e0a\u7684\u9884\u6d4b\u6027\u80fd\u3002CISO\u5728\u9884\u6d4b\u5269\u4f59\u7269\u79cd\u5206\u5e03\u65f6\uff0c\u4f18\u4e8e\u4ee5\u76f8\u540c\u6570\u636e\u96c6\u5185\u90e8\u5206\u7269\u79cd\u4e3a\u6761\u4ef6\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u591a\u6570\u636e\u96c6\u7684\u89c2\u6d4b\u6570\u636e\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "CISO\u662f\u4e00\u4e2a\u6709\u6f5c\u529b\u7684\u751f\u6001\u5de5\u5177\uff0c\u80fd\u6709\u6548\u6574\u5408\u4e0d\u5b8c\u6574\u7684\u751f\u7269\u4fe1\u606f\uff0c\u5e76\u8bc6\u522b\u4e0d\u540c\u5206\u7c7b\u7fa4\u7269\u79cd\u95f4\u6f5c\u5728\u7684\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2508.06939", "pdf": "https://arxiv.org/pdf/2508.06939", "abs": "https://arxiv.org/abs/2508.06939", "authors": ["Hiba Najjar", "Deepak Pathak", "Marlon Nuske", "Andreas Dengel"], "title": "Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Multimodal learning enables various machine learning tasks to benefit from\ndiverse data sources, effectively mimicking the interplay of different factors\nin real-world applications, particularly in agriculture. While the\nheterogeneous nature of involved data modalities may necessitate the design of\ncomplex architectures, the model interpretability is often overlooked. In this\nstudy, we leverage the intrinsic explainability of Transformer-based models to\nexplain multimodal learning networks, focusing on the task of crop yield\nprediction at the subfield level. The large datasets used cover various crops,\nregions, and years, and include four different input modalities: multispectral\nsatellite and weather time series, terrain elevation maps and soil properties.\nBased on the self-attention mechanism, we estimate feature attributions using\ntwo methods, namely the Attention Rollout (AR) and Generic Attention (GA), and\nevaluate their performance against Shapley-based model-agnostic estimations,\nShapley Value Sampling (SVS). Additionally, we propose the Weighted Modality\nActivation (WMA) method to assess modality attributions and compare it with SVS\nattributions. Our findings indicate that Transformer-based models outperform\nother architectures, specifically convolutional and recurrent networks,\nachieving R2 scores that are higher by 0.10 and 0.04 at the subfield and field\nlevels, respectively. AR is shown to provide more robust and reliable temporal\nattributions, as confirmed through qualitative and quantitative evaluation,\ncompared to GA and SVS values. Information about crop phenology stages was\nleveraged to interpret the explanation results in the light of established\nagronomic knowledge. Furthermore, modality attributions revealed varying\npatterns across the two methods compared.[...]", "AI": {"tldr": "Transformer\u6a21\u578b\u5728\u4e9a\u7530\u5757\u7ea7\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5229\u7528\u5176\u81ea\u89e3\u91ca\u6027\uff08\u5982Attention Rollout\uff09\u63d0\u4f9b\u53ef\u9760\u7684\u7279\u5f81\u548c\u6a21\u6001\u5f52\u56e0\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u5b66\u4e60\u5728\u519c\u4e1a\u7b49\u9886\u57df\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u5f02\u6784\u6027\uff0c\u6784\u5efa\u590d\u6742\u6a21\u578b\u540e\u5176\u53ef\u89e3\u91ca\u6027\u5e38\u88ab\u5ffd\u89c6\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u4efb\u52a1\u4e2d\u3002", "method": "\u5229\u7528Transformer\u6a21\u578b\u7684\u5185\u5728\u53ef\u89e3\u91ca\u6027\u6765\u89e3\u91ca\u591a\u6a21\u6001\u5b66\u4e60\u7f51\u7edc\uff0c\u4e13\u6ce8\u4e8e\u4e9a\u7530\u5757\u7ea7\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u3002\u4f7f\u7528\u5305\u542b\u591a\u5149\u8c31\u536b\u661f\u3001\u5929\u6c14\u65f6\u95f4\u5e8f\u5217\u3001\u5730\u5f62\u9ad8\u7a0b\u56fe\u548c\u571f\u58e4\u5c5e\u6027\u56db\u79cd\u6a21\u6001\u7684\u5927\u578b\u6570\u636e\u96c6\u3002\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u91c7\u7528Attention Rollout (AR) \u548c Generic Attention (GA) \u4f30\u8ba1\u7279\u5f81\u5f52\u56e0\uff0c\u5e76\u4e0eShapley Value Sampling (SVS) \u8fdb\u884c\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u63d0\u51faWeighted Modality Activation (WMA) \u65b9\u6cd5\u8bc4\u4f30\u6a21\u6001\u5f52\u56e0\uff0c\u5e76\u4e0eSVS\u8fdb\u884c\u6bd4\u8f83\u3002\u89e3\u91ca\u7ed3\u679c\u7ed3\u5408\u4f5c\u7269\u7269\u5019\u671f\u4fe1\u606f\u8fdb\u884c\u3002", "result": "Transformer\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5377\u79ef\u548c\u5faa\u73af\u7f51\u7edc\uff0c\u5728\u4e9a\u7530\u5757\u548c\u7530\u5757\u7ea7\u522bR2\u5206\u6570\u5206\u522b\u9ad8\u51fa0.10\u548c0.04\u3002Attention Rollout (AR) \u76f8\u6bd4GA\u548cSVS\u503c\uff0c\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u53ef\u9760\u7684\u65f6\u95f4\u5f52\u56e0\uff0c\u5e76\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u5f97\u5230\u8bc1\u5b9e\u3002\u6a21\u6001\u5f52\u56e0\u5728\u4e0d\u540c\u65b9\u6cd5\u95f4\u663e\u793a\u51fa\u4e0d\u540c\u7684\u6a21\u5f0f\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5c06Transformer\u6a21\u578b\u5e94\u7528\u4e8e\u591a\u6a21\u6001\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u5229\u7528\u5176\u5185\u5728\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u7279\u5f81\u548c\u6a21\u6001\u5f52\u56e0\u3002Attention Rollout\u65b9\u6cd5\u5728\u65f6\u95f4\u5f52\u56e0\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7406\u89e3\u519c\u4e1a\u4e2d\u591a\u6a21\u6001\u6570\u636e\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u53ef\u89e3\u91ca\u6027\u7f3a\u5931\u7684\u95ee\u9898\u3002"}}
{"id": "2508.06566", "pdf": "https://arxiv.org/pdf/2508.06566", "abs": "https://arxiv.org/abs/2508.06566", "authors": ["Manish Kansana", "Elias Hossain", "Shahram Rahimi", "Noorbakhsh Amiri Golilarz"], "title": "Surformer v1: Transformer-Based Surface Classification Using Tactile and Vision Features", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Surface material recognition is a key component in robotic perception and\nphysical interaction, particularly when leveraging both tactile and visual\nsensory inputs. In this work, we propose Surformer v1, a transformer-based\narchitecture designed for surface classification using structured tactile\nfeatures and PCA-reduced visual embeddings extracted via ResNet-50. The model\nintegrates modality-specific encoders with cross-modal attention layers,\nenabling rich interactions between vision and touch. Currently,\nstate-of-the-art deep learning models for vision tasks have achieved remarkable\nperformance. With this in mind, our first set of experiments focused\nexclusively on tactile-only surface classification. Using feature engineering,\nwe trained and evaluated multiple machine learning models, assessing their\naccuracy and inference time. We then implemented an encoder-only Transformer\nmodel tailored for tactile features. This model not only achieved the highest\naccuracy but also demonstrated significantly faster inference time compared to\nother evaluated models, highlighting its potential for real-time applications.\nTo extend this investigation, we introduced a multimodal fusion setup by\ncombining vision and tactile inputs. We trained both Surformer v1 (using\nstructured features) and Multimodal CNN (using raw images) to examine the\nimpact of feature-based versus image-based multimodal learning on\nclassification accuracy and computational efficiency. The results showed that\nSurformer v1 achieved 99.4% accuracy with an inference time of 0.77 ms, while\nthe Multimodal CNN achieved slightly higher accuracy but required significantly\nmore inference time. These findings suggest Surformer v1 offers a compelling\nbalance between accuracy, efficiency, and computational cost for surface\nmaterial recognition.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Surformer v1\uff0c\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u89e6\u89c9\u7279\u5f81\u548cPCA\u964d\u7ef4\u89c6\u89c9\u5d4c\u5165\u8fdb\u884c\u8868\u9762\u6750\u6599\u8bc6\u522b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSurformer v1\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6781\u9ad8\u7684\u63a8\u7406\u6548\u7387\uff0c\u5728\u591a\u6a21\u6001\u878d\u5408\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5e73\u8861\u6027\u3002", "motivation": "\u8868\u9762\u6750\u6599\u8bc6\u522b\u662f\u673a\u5668\u4eba\u611f\u77e5\u548c\u7269\u7406\u4ea4\u4e92\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u7279\u522b\u662f\u5728\u878d\u5408\u89e6\u89c9\u548c\u89c6\u89c9\u611f\u5b98\u8f93\u5165\u65f6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u51c6\u786e\u7684\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1. \u63d0\u51faSurformer v1\uff0c\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u8868\u9762\u5206\u7c7b\uff0c\u8f93\u5165\u4e3a\u7ed3\u6784\u5316\u89e6\u89c9\u7279\u5f81\u548cResNet-50\u63d0\u53d6\u7684PCA\u964d\u7ef4\u89c6\u89c9\u5d4c\u5165\u30022. \u6a21\u578b\u6574\u5408\u4e86\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5c42\uff0c\u4ee5\u5b9e\u73b0\u89c6\u89c9\u548c\u89e6\u89c9\u4e4b\u95f4\u7684\u4e30\u5bcc\u4ea4\u4e92\u30023. \u5b9e\u9a8c\u5206\u4e3a\u4e24\u90e8\u5206\uff1aa) \u7eaf\u89e6\u89c9\u8868\u9762\u5206\u7c7b\uff1a\u8bc4\u4f30\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u89e6\u89c9\u7279\u5f81\u7684\u4ec5\u7f16\u7801\u5668Transformer\u6a21\u578b\u3002b) \u591a\u6a21\u6001\u878d\u5408\uff1a\u7ed3\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u8f93\u5165\uff0c\u6bd4\u8f83\u4e86Surformer v1\uff08\u57fa\u4e8e\u7279\u5f81\uff09\u548c\u591a\u6a21\u6001CNN\uff08\u57fa\u4e8e\u539f\u59cb\u56fe\u50cf\uff09\u5728\u5206\u7c7b\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u5f71\u54cd\u3002", "result": "1. \u7eaf\u89e6\u89c9\u5206\u7c7b\u5b9e\u9a8c\u4e2d\uff0c\u4ec5\u7f16\u7801\u5668\u7684Transformer\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u7387\u548c\u663e\u8457\u66f4\u5feb\u7684\u63a8\u7406\u65f6\u95f4\u30022. \u591a\u6a21\u6001\u878d\u5408\u5b9e\u9a8c\u4e2d\uff0cSurformer v1\u5b9e\u73b0\u4e8699.4%\u7684\u51c6\u786e\u7387\uff0c\u63a8\u7406\u65f6\u95f4\u4e3a0.77\u6beb\u79d2\u30023. \u76f8\u6bd4\u4e4b\u4e0b\uff0c\u591a\u6a21\u6001CNN\u867d\u7136\u51c6\u786e\u7387\u7565\u9ad8\uff0c\u4f46\u9700\u8981\u663e\u8457\u66f4\u957f\u7684\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "Surformer v1\u5728\u8868\u9762\u6750\u6599\u8bc6\u522b\u65b9\u9762\uff0c\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u4e86\u5f15\u4eba\u6ce8\u76ee\u7684\u5e73\u8861\uff0c\u5177\u6709\u5f88\u9ad8\u7684\u5b9e\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.07017", "pdf": "https://arxiv.org/pdf/2508.07017", "abs": "https://arxiv.org/abs/2508.07017", "authors": ["Mao Li", "Fred Conrad", "Johann Gagnon-Bartsch"], "title": "Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings", "categories": ["cs.CL"], "comment": null, "summary": "We propose Vec2Summ, a novel method for abstractive summarization that frames\nthe task as semantic compression. Vec2Summ represents a document collection\nusing a single mean vector in the semantic embedding space, capturing the\ncentral meaning of the corpus. To reconstruct fluent summaries, we perform\nembedding inversion -- decoding this mean vector into natural language using a\ngenerative language model. To improve reconstruction quality and capture some\ndegree of topical variability, we introduce stochasticity by sampling from a\nGaussian distribution centered on the mean. This approach is loosely analogous\nto bagging in ensemble learning, where controlled randomness encourages more\nrobust and varied outputs. Vec2Summ addresses key limitations of LLM-based\nsummarization methods. It avoids context-length constraints, enables\ninterpretable and controllable generation via semantic parameters, and scales\nefficiently with corpus size -- requiring only $O(d + d^2)$ parameters.\nEmpirical results show that Vec2Summ produces coherent summaries for topically\nfocused, order-invariant corpora, with performance comparable to direct LLM\nsummarization in terms of thematic coverage and efficiency, albeit with less\nfine-grained detail. These results underscore Vec2Summ's potential in settings\nwhere scalability, semantic control, and corpus-level abstraction are\nprioritized.", "AI": {"tldr": "Vec2Summ\u662f\u4e00\u79cd\u65b0\u578b\u62bd\u8c61\u6458\u8981\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6587\u6863\u96c6\u5408\u538b\u7f29\u4e3a\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u4e00\u4e2a\u5e73\u5747\u5411\u91cf\uff0c\u5e76\u4f7f\u7528\u751f\u6210\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u89e3\u7801\u6765\u751f\u6210\u6458\u8981\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3LLM\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u548c\u53ef\u63a7\u6027\u95ee\u9898\uff0c\u5e76\u5728\u53ef\u6269\u5c55\u6027\u548c\u8bed\u4e49\u63a7\u5236\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6458\u8981\u65b9\u6cd5\u5b58\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u3001\u96be\u4ee5\u89e3\u91ca\u548c\u63a7\u5236\uff0c\u4ee5\u53ca\u5728\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e0a\u6548\u7387\u4e0d\u9ad8\u7684\u95ee\u9898\u3002", "method": "Vec2Summ\u5c06\u6458\u8981\u4efb\u52a1\u89c6\u4e3a\u8bed\u4e49\u538b\u7f29\u3002\u5b83\u901a\u8fc7\u5728\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7528\u5355\u4e2a\u5e73\u5747\u5411\u91cf\u8868\u793a\u6587\u6863\u96c6\u5408\u6765\u6355\u6349\u8bed\u6599\u5e93\u7684\u6838\u5fc3\u542b\u4e49\u3002\u7136\u540e\uff0c\u5229\u7528\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u5bf9\u8be5\u5e73\u5747\u5411\u91cf\u8fdb\u884c\u5d4c\u5165\u53cd\u6f14\uff0c\u4ee5\u91cd\u6784\u6d41\u7545\u7684\u81ea\u7136\u8bed\u8a00\u6458\u8981\u3002\u4e3a\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u548c\u5f15\u5165\u4e3b\u9898\u591a\u6837\u6027\uff0c\u8be5\u65b9\u6cd5\u8fd8\u5f15\u5165\u4e86\u968f\u673a\u6027\uff0c\u5373\u4ece\u4ee5\u5e73\u5747\u5411\u91cf\u4e3a\u4e2d\u5fc3\u7684 Gaussian \u5206\u5e03\u4e2d\u8fdb\u884c\u91c7\u6837\u3002\u6b64\u65b9\u6cd5\u907f\u514d\u4e86\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\uff0c\u5e76\u4ee5\u9ad8\u6548\u7684 $O(d + d^2)$ \u53c2\u6570\u5b9e\u73b0\u3002", "result": "Vec2Summ\u80fd\u4e3a\u4e3b\u9898\u96c6\u4e2d\u3001\u987a\u5e8f\u4e0d\u53d8\u7684\u8bed\u6599\u5e93\u751f\u6210\u8fde\u8d2f\u7684\u6458\u8981\u3002\u5728\u4e3b\u9898\u8986\u76d6\u548c\u6548\u7387\u65b9\u9762\uff0c\u5176\u6027\u80fd\u4e0e\u76f4\u63a5LLM\u6458\u8981\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u7ec6\u7c92\u5ea6\u7ec6\u8282\u7565\u6709\u4e0d\u8db3\u3002", "conclusion": "Vec2Summ\u5728\u4f18\u5148\u8003\u8651\u53ef\u6269\u5c55\u6027\u3001\u8bed\u4e49\u63a7\u5236\u548c\u8bed\u6599\u5e93\u7ea7\u62bd\u8c61\u7684\u5e94\u7528\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u6f5c\u529b\u3002"}}
{"id": "2508.06743", "pdf": "https://arxiv.org/pdf/2508.06743", "abs": "https://arxiv.org/abs/2508.06743", "authors": ["Connor Brown"], "title": "Analysis of Schedule-Free Nonconvex Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "First-order methods underpin most large-scale learning algorithms, yet their\nclassical convergence guarantees hinge on carefully scheduled step-sizes that\ndepend on the total horizon $T$, which is rarely known in advance. The\nSchedule-Free (SF) method promises optimal performance with hyperparameters\nthat are independent of $T$ by interpolating between Polyak--Ruppert averaging\nand momentum, but nonconvex analysis of SF has been limited or reliant on\nstrong global assumptions. We introduce a robust Lyapunov framework that, under\nonly $L$-smoothness and lower-boundedness, reduces SF analysis to a single-step\ndescent inequality. This yields horizon-agnostic bounds in the nonconvex\nsetting: $O(1/\\log T)$ for constant step + PR averaging, $O(\\log T/T)$ for a\nlinearly growing step-size, and a continuum of $O(T^{-(1-\\alpha)})$ rates for\npolynomial averaging. We complement these proofs with Performance Estimation\nProblem (PEP) experiments that numerically validate our rates and suggest that\nour $O(1/\\log T)$ bound on the original nonconvex SF algorithm may tighten to\n$O(1/T)$. Our work extends SF's horizon-free guarantees to smooth nonconvex\noptimization and charts future directions for optimal nonconvex rates.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06950", "pdf": "https://arxiv.org/pdf/2508.06950", "abs": "https://arxiv.org/abs/2508.06950", "authors": ["Sarah Schr\u00f6der", "Thekla Morgenroth", "Ulrike Kuhl", "Valerie Vaquet", "Benjamin Paa\u00dfen"], "title": "Large Language Models Do Not Simulate Human Psychology", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs),such as ChatGPT, are increasingly used in\nresearch, ranging from simple writing assistance to complex data annotation\ntasks. Recently, some research has suggested that LLMs may even be able to\nsimulate human psychology and can, hence, replace human participants in\npsychological studies. We caution against this approach. We provide conceptual\narguments against the hypothesis that LLMs simulate human psychology. We then\npresent empiric evidence illustrating our arguments by demonstrating that\nslight changes to wording that correspond to large changes in meaning lead to\nnotable discrepancies between LLMs' and human responses, even for the recent\nCENTAUR model that was specifically fine-tuned on psychological responses.\nAdditionally, different LLMs show very different responses to novel items,\nfurther illustrating their lack of reliability. We conclude that LLMs do not\nsimulate human psychology and recommend that psychological researchers should\ntreat LLMs as useful but fundamentally unreliable tools that need to be\nvalidated against human responses for every new application.", "AI": {"tldr": "\u672c\u6587\u8b66\u544a\u53cd\u5bf9LLMs\u6a21\u62df\u4eba\u7c7b\u5fc3\u7406\u5e76\u53d6\u4ee3\u5fc3\u7406\u5b66\u7814\u7a76\u4e2d\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\u89c2\u70b9\u3002", "motivation": "\u9274\u4e8e\u6709\u7814\u7a76\u63d0\u51fa\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u80fd\u6a21\u62df\u4eba\u7c7b\u5fc3\u7406\u5e76\u53d6\u4ee3\u5fc3\u7406\u5b66\u7814\u7a76\u4e2d\u7684\u4eba\u7c7b\u53c2\u4e0e\u8005\uff0c\u672c\u6587\u65e8\u5728\u53cd\u9a73\u6b64\u89c2\u70b9\uff0c\u5e76\u5f3a\u8c03\u5176\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u7814\u7a76\u7ed3\u5408\u6982\u5ff5\u8bba\u8bc1\u548c\u5b9e\u8bc1\u8bc1\u636e\u3002\u5b9e\u8bc1\u90e8\u5206\u901a\u8fc7\u5c55\u793aLLMs\uff08\u5305\u62ec\u4e3a\u5fc3\u7406\u5b66\u54cd\u5e94\u5fae\u8c03\u7684CENTAUR\u6a21\u578b\uff09\u5bf9\u7ec6\u5fae\u63aa\u8f9e\u53d8\u5316\u7684\u54cd\u5e94\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4ee5\u53ca\u4e0d\u540cLLMs\u5bf9\u65b0\u5947\u6761\u76ee\u54cd\u5e94\u7684\u9ad8\u5ea6\u4e0d\u4e00\u81f4\u6027\u6765\u652f\u6301\u5176\u8bba\u70b9\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cLLMs\u5bf9\u8bed\u8a00\u7ec6\u5fae\u53d8\u5316\u7684\u654f\u611f\u5ea6\u4e0e\u4eba\u7c7b\u4e0d\u540c\u6b65\uff0c\u5e76\u4e14\u5b83\u4eec\u5bf9\u65b0\u5947\u95ee\u9898\u7684\u54cd\u5e94\u7f3a\u4e4f\u53ef\u9760\u6027\u4e0e\u4e00\u81f4\u6027\u3002\u8fd9\u6709\u529b\u5730\u8bc1\u660e\u4e86LLMs\u65e0\u6cd5\u6a21\u62df\u4eba\u7c7b\u5fc3\u7406\u3002", "conclusion": "LLMs\u4e0d\u80fd\u6a21\u62df\u4eba\u7c7b\u5fc3\u7406\u3002\u5fc3\u7406\u5b66\u7814\u7a76\u4eba\u5458\u5e94\u5c06LLMs\u89c6\u4e3a\u6709\u7528\u4f46\u6839\u672c\u4e0a\u4e0d\u53ef\u9760\u7684\u5de5\u5177\uff0c\u5728\u6bcf\u6b21\u65b0\u5e94\u7528\u4e2d\u90fd\u5fc5\u987b\u4e0e\u4eba\u7c7b\u54cd\u5e94\u8fdb\u884c\u9a8c\u8bc1\u3002"}}
{"id": "2508.06570", "pdf": "https://arxiv.org/pdf/2508.06570", "abs": "https://arxiv.org/abs/2508.06570", "authors": ["Mohammad Zia Ur Rehman", "Anukriti Bhatnagar", "Omkar Kabde", "Shubhi Bansal", "Nagendra Kumar"], "title": "ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos", "categories": ["cs.CV", "cs.LG"], "comment": "Published in ACL 2025", "summary": "The existing research has primarily focused on text and image-based hate\nspeech detection, video-based approaches remain underexplored. In this work, we\nintroduce a novel dataset, ImpliHateVid, specifically curated for implicit hate\nspeech detection in videos. ImpliHateVid consists of 2,009 videos comprising\n509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos,\nmaking it one of the first large-scale video datasets dedicated to implicit\nhate detection. We also propose a novel two-stage contrastive learning\nframework for hate speech detection in videos. In the first stage, we train\nmodality-specific encoders for audio, text, and image using contrastive loss by\nconcatenating features from the three encoders. In the second stage, we train\ncross-encoders using contrastive learning to refine multimodal representations.\nAdditionally, we incorporate sentiment, emotion, and caption-based features to\nenhance implicit hate detection. We evaluate our method on two datasets,\nImpliHateVid for implicit hate speech detection and another dataset for general\nhate speech detection in videos, HateMM dataset, demonstrating the\neffectiveness of the proposed multimodal contrastive learning for hateful\ncontent detection in videos and the significance of our dataset.", "AI": {"tldr": "\u63d0\u51faImpliHateVid\u6570\u636e\u96c6\u548c\u4e24\u9636\u6bb5\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u4e2d\u7684\u9690\u5f0f\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u6587\u672c\u548c\u56fe\u50cf\u7684\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\uff0c\u800c\u89c6\u9891\u9886\u57df\u7684\uff0c\u7279\u522b\u662f\u9690\u5f0f\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\uff0c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "1. \u6784\u5efa\u4e86\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u96c6ImpliHateVid\uff082009\u4e2a\u89c6\u9891\uff09\u7528\u4e8e\u9690\u5f0f\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u30022. \u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\uff0c\u7b2c\u4e8c\u9636\u6bb5\u8bad\u7ec3\u4ea4\u53c9\u7f16\u7801\u5668\u4ee5\u4f18\u5316\u591a\u6a21\u6001\u8868\u793a\u30023. \u6574\u5408\u4e86\u60c5\u611f\u3001\u60c5\u7eea\u548c\u5b57\u5e55\u7279\u5f81\u4ee5\u589e\u5f3a\u68c0\u6d4b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5728ImpliHateVid\u548cHateMM\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u89c6\u9891\u4ec7\u6068\u5185\u5bb9\u68c0\u6d4b\u4e2d\u7684\u6548\u679c\u53ca\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u4e13\u7528\u89c6\u9891\u6570\u636e\u96c6\u548c\u521b\u65b0\u7684\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u6709\u6548\u5730\u63a8\u8fdb\u4e86\u89c6\u9891\u4e2d\u9690\u5f0f\u4ec7\u6068\u8a00\u8bba\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2508.07069", "pdf": "https://arxiv.org/pdf/2508.07069", "abs": "https://arxiv.org/abs/2508.07069", "authors": ["Muhammad Dehan Al Kautsar", "Aswin Candra", "Muhammad Alif Al Hakim", "Maxalmina Satria Kahfi", "Fajri Koto", "Alham Fikri Aji", "Peerat Limkonchotiwat", "Ekapol Chuangsuwanich", "Genta Indra Winata"], "title": "SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Although numerous datasets have been developed to support dialogue systems,\nmost existing chit-chat datasets overlook the cultural nuances inherent in\nnatural human conversations. To address this gap, we introduce SEADialogues, a\nculturally grounded dialogue dataset centered on Southeast Asia, a region with\nover 700 million people and immense cultural diversity. Our dataset features\ndialogues in eight languages from six Southeast Asian countries, many of which\nare low-resource despite having sizable speaker populations. To enhance\ncultural relevance and personalization, each dialogue includes persona\nattributes and two culturally grounded topics that reflect everyday life in the\nrespective communities. Furthermore, we release a multi-turn dialogue dataset\nto advance research on culturally aware and human-centric large language\nmodels, including conversational dialogue agents.", "AI": {"tldr": "\u5f15\u5165\u4e86SEADialogues\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ee5\u4e1c\u5357\u4e9a\u6587\u5316\u4e3a\u80cc\u666f\u7684\u591a\u8bed\u8a00\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u65e8\u5728\u5f25\u8865\u73b0\u6709\u5bf9\u8bdd\u6570\u636e\u96c6\u4e2d\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u7684\u4e0d\u8db3\u3002", "motivation": "\u5927\u591a\u6570\u73b0\u6709\u95f2\u804a\u6570\u636e\u96c6\u5ffd\u7565\u4e86\u81ea\u7136\u4eba\u7c7b\u5bf9\u8bdd\u4e2d\u56fa\u6709\u7684\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u3002", "method": "\u5f00\u53d1\u5e76\u53d1\u5e03\u4e86SEADialogues\u6570\u636e\u96c6\uff0c\u5176\u7279\u70b9\u662f\u5305\u542b\u6765\u81ea\u516d\u4e2a\u4e1c\u5357\u4e9a\u56fd\u5bb6\u7684\u516b\u79cd\u8bed\u8a00\u7684\u5bf9\u8bdd\u3002\u4e3a\u589e\u5f3a\u6587\u5316\u76f8\u5173\u6027\uff0c\u6bcf\u4e2a\u5bf9\u8bdd\u90fd\u5305\u542b\u4eba\u7269\u89d2\u8272\u5c5e\u6027\u548c\u4e24\u4e2a\u53cd\u6620\u5f53\u5730\u65e5\u5e38\u751f\u6d3b\u7684\u6587\u5316\u4e3b\u9898\u3002", "result": "\u6210\u529f\u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u591a\u8f6e\u3001\u6587\u5316\u63a5\u5730\u6c14\u7684\u4e1c\u5357\u4e9a\u591a\u8bed\u8a00\u5bf9\u8bdd\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u65e8\u5728\u63a8\u52a8\u6587\u5316\u611f\u77e5\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u4f1a\u8bdd\u5f0f\u5bf9\u8bdd\u4ee3\u7406\uff09\u7684\u7814\u7a76\u3002"}}
{"id": "2508.06765", "pdf": "https://arxiv.org/pdf/2508.06765", "abs": "https://arxiv.org/abs/2508.06765", "authors": ["Xingke Yang", "Liang Li", "Sicong Li", "Liwei Guan", "Hao Wang", "Xiaoqi Qi", "Jiang Liu", "Xin Fu", "Miao Pan"], "title": "Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning", "categories": ["cs.LG"], "comment": null, "summary": "Collaboratively fine-tuning (FT) large language models (LLMs) over\nheterogeneous mobile devices fosters immense potential applications of\npersonalized intelligence. However, such a vision faces critical system\nchallenges. Conventional federated LLM FT approaches place prohibitive\ncomputational and memory burdens on mobile hardware, and their synchronous\nmodel aggregation protocols stall for slower devices. In this paper, we propose\nFed MobiLLM, a novel design to facilitate efficient federated LLM FT across\nmobile devices with diverse computing/communication speeds and local model\narchitectures. In particular, Fed MobiLLM implements a pioneering\nserver-assisted federated side-tuning paradigm. Briefly, mobile devices perform\nlightweight forward propagation computations on local data using their frozen\npre-scaled backbone LLMs, and then upload selected intermediate activations.\nThe server trains a shared side-network independently, eliminating client-side\nbackpropagation and enabling asynchronous updates. To bridge model\nheterogeneity across different devices, we introduce an adaptive layer-wise\nfeature alignment method, which ensures consistent representations for\ncollaboratively tuning a shared side network. Extensive experimental results\ndemonstrate that Fed MobiLLM can maintain robust fine-tuning performance while\nachieving extremely low on-device memory, with at least 95.2% reduction in\ncomputation overhead, 93.2% reduction in communication costs and 5.1x faster\nconvergence compared to existing methods, validating its efficacy for practical\nLLM adaptation over heterogeneous mobile devices.", "AI": {"tldr": "Fed MobiLLM\u63d0\u51fa\u4e00\u79cd\u670d\u52a1\u5668\u8f85\u52a9\u7684\u8054\u90a6\u4fa7\u8c03\u4f18\u8303\u5f0f\uff0c\u663e\u8457\u964d\u4f4e\u79fb\u52a8\u8bbe\u5907\u4e0aLLM\u5fae\u8c03\u7684\u8ba1\u7b97\u3001\u5185\u5b58\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u52a0\u901f\u6536\u655b\u3002", "motivation": "\u5728\u5f02\u6784\u79fb\u52a8\u8bbe\u5907\u4e0a\u534f\u4f5c\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u8054\u90a6LLM\u5fae\u8c03\u65b9\u6cd5\u7ed9\u79fb\u52a8\u786c\u4ef6\u5e26\u6765\u8fc7\u9ad8\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u8d1f\u62c5\uff0c\u4e14\u540c\u6b65\u805a\u5408\u534f\u8bae\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u5e94\u5bf9\u8bbe\u5907\u5f02\u6784\u6027\u3002", "method": "\u672c\u6587\u63d0\u51faFed MobiLLM\uff0c\u6838\u5fc3\u662f\u670d\u52a1\u5668\u8f85\u52a9\u7684\u8054\u90a6\u4fa7\u8c03\u4f18\u8303\u5f0f\u3002\u79fb\u52a8\u8bbe\u5907\u4ec5\u5bf9\u51bb\u7ed3\u7684LLM\u9aa8\u5e72\u8fdb\u884c\u8f7b\u91cf\u7ea7\u524d\u5411\u4f20\u64ad\uff0c\u5e76\u4e0a\u4f20\u9009\u5b9a\u7684\u4e2d\u95f4\u6fc0\u6d3b\u3002\u670d\u52a1\u5668\u72ec\u7acb\u8bad\u7ec3\u4e00\u4e2a\u5171\u4eab\u7684\u4fa7\u7f51\u7edc\uff0c\u4ece\u800c\u6d88\u9664\u5ba2\u6237\u7aef\u7684\u53cd\u5411\u4f20\u64ad\uff0c\u5e76\u652f\u6301\u5f02\u6b65\u66f4\u65b0\u3002\u4e3a\u89e3\u51b3\u6a21\u578b\u5f02\u6784\u6027\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u5c42\u7ea7\u7279\u5f81\u5bf9\u9f50\u65b9\u6cd5\u3002", "result": "Fed MobiLLM\u5728\u4fdd\u6301\u9c81\u68d2\u5fae\u8c03\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6781\u4f4e\u7684\u8bbe\u5907\u5185\u5b58\u5360\u7528\uff0c\u8ba1\u7b97\u5f00\u9500\u51cf\u5c11\u81f3\u5c1195.2%\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e93.2%\uff0c\u6536\u655b\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb5.1\u500d\u3002", "conclusion": "Fed MobiLLM\u6709\u6548\u89e3\u51b3\u4e86\u5728\u5f02\u6784\u79fb\u52a8\u8bbe\u5907\u4e0a\u8054\u90a6LLM\u5fae\u8c03\u7684\u7cfb\u7edf\u6311\u6218\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645LLM\u81ea\u9002\u5e94\u573a\u666f\u4e2d\u7684\u9ad8\u6548\u6027\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.06960", "pdf": "https://arxiv.org/pdf/2508.06960", "abs": "https://arxiv.org/abs/2508.06960", "authors": ["Keyu Li", "Mohan Jiang", "Dayuan Fu", "Yunze Wu", "Xiangkun Hu", "Dequan Wang", "Pengfei Liu"], "title": "DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of large language models has fundamentally shifted the\nbottleneck in AI development from computational power to data availability-with\ncountless valuable datasets remaining hidden across specialized repositories,\nresearch appendices, and domain platforms. As reasoning capabilities and deep\nresearch methodologies continue to evolve, a critical question emerges: can AI\nagents transcend conventional search to systematically discover any dataset\nthat meets specific user requirements, enabling truly autonomous demand-driven\ndata curation? We introduce DatasetResearch, the first comprehensive benchmark\nevaluating AI agents' ability to discover and synthesize datasets from 208\nreal-world demands across knowledge-intensive and reasoning-intensive tasks.\nOur tri-dimensional evaluation framework reveals a stark reality: even advanced\ndeep research systems achieve only 22% score on our challenging\nDatasetResearch-pro subset, exposing the vast gap between current capabilities\nand perfect dataset discovery. Our analysis uncovers a fundamental\ndichotomy-search agents excel at knowledge tasks through retrieval breadth,\nwhile synthesis agents dominate reasoning challenges via structured\ngeneration-yet both catastrophically fail on \"corner cases\" outside existing\ndistributions. These findings establish the first rigorous baseline for dataset\ndiscovery agents and illuminate the path toward AI systems capable of finding\nany dataset in the digital universe. Our benchmark and comprehensive analysis\nprovide the foundation for the next generation of self-improving AI systems and\nare publicly available at https://github.com/GAIR-NLP/DatasetResearch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DatasetResearch\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u53d1\u73b0\u548c\u5408\u6210\u6570\u636e\u96c6\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524dAI\u5728\u6309\u9700\u6570\u636e\u53d1\u73b0\u65b9\u9762\u7684\u5de8\u5927\u5dee\u8ddd\uff0c\u5e76\u4e3a\u672a\u6765\u81ea\u4e3b\u6570\u636e\u53d1\u73b0\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\uff0cAI\u5f00\u53d1\u7684\u74f6\u9888\u5df2\u4ece\u8ba1\u7b97\u80fd\u529b\u8f6c\u5411\u6570\u636e\u53ef\u7528\u6027\u3002\u5927\u91cf\u6709\u4ef7\u503c\u7684\u6570\u636e\u96c6\u5206\u6563\u9690\u85cf\uff0c\u4e9f\u9700\u89e3\u51b3AI\u4ee3\u7406\u80fd\u5426\u8d85\u8d8a\u4f20\u7edf\u641c\u7d22\uff0c\u7cfb\u7edf\u6027\u5730\u53d1\u73b0\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u7684\u4efb\u610f\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u6309\u9700\u6570\u636e\u7ba1\u7406\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86DatasetResearch\uff0c\u8fd9\u662f\u9996\u4e2a\u5168\u9762\u8bc4\u4f30AI\u4ee3\u7406\u4ece208\u4e2a\u77e5\u8bc6\u5bc6\u96c6\u578b\u548c\u63a8\u7406\u5bc6\u96c6\u578b\u771f\u5b9e\u4e16\u754c\u9700\u6c42\u4e2d\u53d1\u73b0\u548c\u5408\u6210\u6570\u636e\u96c6\u80fd\u529b\u7684\u57fa\u51c6\u3002\u91c7\u7528\u4e09\u7ef4\u8bc4\u4f30\u6846\u67b6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u5728DatasetResearch-pro\u5b50\u96c6\u4e0a\u5f97\u5206\u4e5f\u4ec5\u4e3a22%\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u80fd\u529b\u4e0e\u5b8c\u7f8e\u6570\u636e\u96c6\u53d1\u73b0\u4e4b\u95f4\u7684\u5de8\u5927\u5dee\u8ddd\u3002\u5206\u6790\u63ed\u793a\u4e86\u6839\u672c\u6027\u5dee\u5f02\uff1a\u641c\u7d22\u4ee3\u7406\u64c5\u957f\u901a\u8fc7\u68c0\u7d22\u5e7f\u5ea6\u5904\u7406\u77e5\u8bc6\u4efb\u52a1\uff0c\u800c\u5408\u6210\u4ee3\u7406\u901a\u8fc7\u7ed3\u6784\u5316\u751f\u6210\u4e3b\u5bfc\u63a8\u7406\u6311\u6218\uff1b\u4f46\u4e24\u8005\u5728\u73b0\u6709\u5206\u5e03\u4e4b\u5916\u7684\u201c\u8fb9\u7f18\u6848\u4f8b\u201d\u4e0a\u90fd\u906d\u9047\u707e\u96be\u6027\u5931\u8d25\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6570\u636e\u96c6\u53d1\u73b0\u4ee3\u7406\u5efa\u7acb\u4e86\u9996\u4e2a\u4e25\u683c\u7684\u57fa\u7ebf\uff0c\u5e76\u6307\u660e\u4e86\u5b9e\u73b0AI\u7cfb\u7edf\u80fd\u591f\u5728\u6570\u5b57\u5b87\u5b99\u4e2d\u627e\u5230\u4efb\u4f55\u6570\u636e\u96c6\u7684\u9014\u5f84\u3002\u6240\u63d0\u4f9b\u7684\u57fa\u51c6\u548c\u5168\u9762\u5206\u6790\u4e3a\u4e0b\u4e00\u4ee3\u81ea\u6211\u6539\u8fdb\u578bAI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06623", "pdf": "https://arxiv.org/pdf/2508.06623", "abs": "https://arxiv.org/abs/2508.06623", "authors": ["Sihan Ma", "Qiming Wu", "Ruotong Jiang", "Frank Burns"], "title": "ContextGuard-LVLM: Enhancing News Veracity through Fine-grained Cross-modal Contextual Consistency Verification", "categories": ["cs.CV"], "comment": null, "summary": "The proliferation of digital news media necessitates robust methods for\nverifying content veracity, particularly regarding the consistency between\nvisual and textual information. Traditional approaches often fall short in\naddressing the fine-grained cross-modal contextual consistency (FCCC) problem,\nwhich encompasses deeper alignment of visual narrative, emotional tone, and\nbackground information with text, beyond mere entity matching. To address this,\nwe propose ContextGuard-LVLM, a novel framework built upon advanced\nVision-Language Large Models (LVLMs) and integrating a multi-stage contextual\nreasoning mechanism. Our model is uniquely enhanced through reinforced or\nadversarial learning paradigms, enabling it to detect subtle contextual\nmisalignments that evade zero-shot baselines. We extend and augment three\nestablished datasets (TamperedNews-Ent, News400-Ent, MMG-Ent) with new\nfine-grained contextual annotations, including \"contextual sentiment,\" \"visual\nnarrative theme,\" and \"scene-event logical coherence,\" and introduce a\ncomprehensive CTXT (Contextual Coherence) entity type. Extensive experiments\ndemonstrate that ContextGuard-LVLM consistently outperforms state-of-the-art\nzero-shot LVLM baselines (InstructBLIP and LLaVA 1.5) across nearly all\nfine-grained consistency tasks, showing significant improvements in complex\nlogical reasoning and nuanced contextual understanding. Furthermore, our model\nexhibits superior robustness to subtle perturbations and a higher agreement\nrate with human expert judgments on challenging samples, affirming its efficacy\nin discerning sophisticated forms of context detachment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faContextGuard-LVLM\u6846\u67b6\uff0c\u57fa\u4e8eLVLM\u548c\u591a\u9636\u6bb5\u8bed\u5883\u63a8\u7406\u673a\u5236\uff0c\u901a\u8fc7\u5f3a\u5316/\u5bf9\u6297\u5b66\u4e60\u548c\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u6570\u5b57\u65b0\u95fb\u4e2d\u56fe\u6587\u5185\u5bb9\u7684\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u8bed\u5883\u4e00\u81f4\u6027\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6a21\u578b\u5728\u590d\u6742\u903b\u8f91\u63a8\u7406\u548c\u7ec6\u81f4\u8bed\u5883\u7406\u89e3\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u80fd\u6709\u6548\u68c0\u6d4b\u7ec6\u5fae\u7684\u8bed\u5883\u4e0d\u4e00\u81f4\u3002", "motivation": "\u6570\u5b57\u65b0\u95fb\u5a92\u4f53\u7684\u666e\u53ca\uff0c\u4f7f\u5f97\u9a8c\u8bc1\u5185\u5bb9\u771f\u5b9e\u6027\uff0c\u7279\u522b\u662f\u56fe\u6587\u4fe1\u606f\u95f4\u7684\u4e00\u81f4\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u8bed\u5883\u4e00\u81f4\u6027\uff08FCCC\uff09\u95ee\u9898\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0cFCCC\u8981\u6c42\u5bf9\u89c6\u89c9\u53d9\u4e8b\u3001\u60c5\u611f\u57fa\u8c03\u548c\u80cc\u666f\u4fe1\u606f\u4e0e\u6587\u672c\u8fdb\u884c\u66f4\u6df1\u5c42\u6b21\u7684\u5bf9\u9f50\uff0c\u800c\u975e\u4ec5\u4ec5\u5b9e\u4f53\u5339\u914d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86ContextGuard-LVLM\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u5148\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\uff08LVLMs\uff09\uff0c\u5e76\u6574\u5408\u4e86\u591a\u9636\u6bb5\u8bed\u5883\u63a8\u7406\u673a\u5236\u3002\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6216\u5bf9\u6297\u5b66\u4e60\u8303\u5f0f\u8fdb\u884c\u72ec\u7279\u589e\u5f3a\uff0c\u4f7f\u5176\u80fd\u591f\u68c0\u6d4b\u5230\u96f6\u6837\u672c\u57fa\u7ebf\u96be\u4ee5\u5bdf\u89c9\u7684\u7ec6\u5fae\u8bed\u5883\u9519\u4f4d\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8005\u6269\u5c55\u5e76\u589e\u5f3a\u4e86\u4e09\u4e2a\u73b0\u6709\u6570\u636e\u96c6\uff0c\u589e\u52a0\u4e86\u201c\u8bed\u5883\u60c5\u611f\u201d\u3001\u201c\u89c6\u89c9\u53d9\u4e8b\u4e3b\u9898\u201d\u548c\u201c\u573a\u666f-\u4e8b\u4ef6\u903b\u8f91\u8fde\u8d2f\u6027\u201d\u7b49\u65b0\u7684\u7ec6\u7c92\u5ea6\u8bed\u5883\u6807\u6ce8\uff0c\u5e76\u5f15\u5165\u4e86\u5168\u9762\u7684CTXT\uff08Contextual Coherence\uff09\u5b9e\u4f53\u7c7b\u578b\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cContextGuard-LVLM\u5728\u51e0\u4e4e\u6240\u6709\u7ec6\u7c92\u5ea6\u4e00\u81f4\u6027\u4efb\u52a1\u4e0a\u90fd\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672cLVLM\u57fa\u7ebf\u6a21\u578b\uff08InstructBLIP\u548cLLaVA 1.5\uff09\uff0c\u5728\u590d\u6742\u903b\u8f91\u63a8\u7406\u548c\u7ec6\u81f4\u8bed\u5883\u7406\u89e3\u65b9\u9762\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5bf9\u7ec6\u5fae\u6270\u52a8\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5177\u6311\u6218\u6027\u7684\u6837\u672c\u4e0a\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5224\u65ad\u5177\u6709\u66f4\u9ad8\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "ContextGuard-LVLM\u80fd\u591f\u6709\u6548\u8bc6\u522b\u590d\u6742\u7684\u8bed\u5883\u8131\u8282\u5f62\u5f0f\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u5185\u5bb9\u771f\u5b9e\u6027\u9a8c\u8bc1\u4e2d\u8fa8\u522b\u7cbe\u7ec6\u8bed\u5883\u5206\u79bb\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.07090", "pdf": "https://arxiv.org/pdf/2508.07090", "abs": "https://arxiv.org/abs/2508.07090", "authors": ["Aditya Tomar", "Nihar Ranjan Sahoo", "Pushpak Bhattacharyya"], "title": "BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating social biases in language models (LMs) is crucial for ensuring\nfairness and minimizing the reinforcement of harmful stereotypes in AI systems.\nExisting benchmarks, such as the Bias Benchmark for Question Answering (BBQ),\nprimarily focus on Western contexts, limiting their applicability to the Indian\ncontext. To address this gap, we introduce BharatBBQ, a culturally adapted\nbenchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil,\nTelugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3\nintersectional groups, reflecting prevalent biases in the Indian sociocultural\nlandscape. Our dataset contains 49,108 examples in one language that are\nexpanded using translation and verification to 392,864 examples in eight\ndifferent languages. We evaluate five multilingual LM families across zero and\nfew-shot settings, analyzing their bias and stereotypical bias scores. Our\nfindings highlight persistent biases across languages and social categories and\noften amplified biases in Indian languages compared to English, demonstrating\nthe necessity of linguistically and culturally grounded benchmarks for bias\nevaluation.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u73b0\u6709\u504f\u89c1\u8bc4\u4f30\u57fa\u51c6\u5728\u5370\u5ea6\u8bed\u5883\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86\u591a\u8bed\u8a00\u591a\u6587\u5316\u57fa\u51c6BharatBBQ\uff0c\u8bc4\u4f30\u53d1\u73b0\u5370\u5ea6\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u6301\u7eed\u4e14\u5e38\u88ab\u653e\u5927\u7684\u504f\u89c1\uff0c\u5f3a\u8c03\u4e86\u672c\u571f\u5316\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u504f\u89c1\u8bc4\u4f30\u57fa\u51c6\uff08\u5982BBQ\uff09\u4e3b\u8981\u805a\u7126\u897f\u65b9\u8bed\u5883\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u5370\u5ea6\u8bed\u5883\u4e2d\u7684\u504f\u89c1\uff0c\u9650\u5236\u4e86\u5176\u5728\u5370\u5ea6AI\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u6027\u4fdd\u969c\u4f5c\u7528\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86BharatBBQ\uff0c\u4e00\u4e2a\u9488\u5bf9\u5370\u5730\u8bed\u3001\u82f1\u8bed\u3001\u9a6c\u62c9\u5730\u8bed\u3001\u5b5f\u52a0\u62c9\u8bed\u3001\u6cf0\u7c73\u5c14\u8bed\u3001\u6cf0\u5362\u56fa\u8bed\u3001\u5965\u91cc\u4e9a\u8bed\u548c\u963f\u8428\u59c6\u8bed\u516b\u79cd\u8bed\u8a00\u3001\u5305\u542b13\u4e2a\u793e\u4f1a\u7c7b\u522b\uff08\u542b3\u4e2a\u4ea4\u53c9\u7c7b\u522b\uff09\u7684\u6587\u5316\u9002\u5e94\u6027\u504f\u89c1\u8bc4\u4f30\u57fa\u51c6\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b49,108\u4e2a\u539f\u59cb\u793a\u4f8b\uff0c\u901a\u8fc7\u7ffb\u8bd1\u548c\u9a8c\u8bc1\u6269\u5c55\u5230392,864\u4e2a\u793a\u4f8b\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u4e94\u79cd\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u504f\u89c1\u5206\u6570\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0d\u540c\u8bed\u8a00\u548c\u4e0d\u540c\u793e\u4f1a\u7c7b\u522b\u4e2d\u5747\u5b58\u5728\u6301\u7eed\u7684\u504f\u89c1\u3002\u4e0e\u82f1\u8bed\u76f8\u6bd4\uff0c\u5370\u5ea6\u8bed\u8a00\u4e2d\u7684\u504f\u89c1\u5e38\u5e38\u88ab\u653e\u5927\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u6784\u5efa\u5177\u6709\u8bed\u8a00\u548c\u6587\u5316\u57fa\u7840\u7684\u504f\u89c1\u8bc4\u4f30\u57fa\u51c6\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u786e\u4fddAI\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u5e76\u51cf\u5c11\u6709\u5bb3\u523b\u677f\u5370\u8c61\u7684\u5f3a\u5316\u3002"}}
{"id": "2508.06767", "pdf": "https://arxiv.org/pdf/2508.06767", "abs": "https://arxiv.org/abs/2508.06767", "authors": ["Arman Dogru", "R. Irem Bor-Yaliniz", "Nimal Gamini Senarath"], "title": "PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.MA", "cs.RO"], "comment": null, "summary": "Digital Twins (DTs) are transforming industries through advanced data\nprocessing and analysis, positioning the world of DTs, Digital World, as a\ncornerstone of nextgeneration technologies including embodied AI. As robotics\nand automated systems scale, efficient data-sharing frameworks and robust\nalgorithms become critical. We explore the pivotal role of data handling in\nnext-gen networks, focusing on dynamics between application and network\nproviders (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with\nPriority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL)\nbased multi-agent path finding (MAPF). By adopting a Centralized Training with\nDecentralized Execution (CTDE) framework and asynchronous actor-learner\narchitectures, PANAMA accelerates training while enabling autonomous task\nexecution by embodied AI. Our approach demonstrates superior pathfinding\nperformance in accuracy, speed, and scalability compared to existing\nbenchmarks. Through simulations, we highlight optimized data-sharing strategies\nfor scalable, automated systems, ensuring resilience in complex, real-world\nenvironments. PANAMA bridges the gap between network-aware decision-making and\nrobust multi-agent coordination, advancing the synergy between DTs, wireless\nnetworks, and AI-driven automation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faPANAMA\u7b97\u6cd5\uff0c\u4e00\u79cd\u57fa\u4e8eMARL\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u6570\u5b57\u5b6a\u751f\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u5904\u7406\u4e0e\u5171\u4eab\uff0c\u5728\u51c6\u786e\u6027\u3001\u901f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u8d85\u8d8a\u73b0\u6709\u57fa\u51c6\uff0c\u65e8\u5728\u4fc3\u8fdbDT\u3001\u65e0\u7ebf\u7f51\u7edc\u548cAI\u81ea\u52a8\u5316\u7684\u534f\u540c\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u548c\u81ea\u52a8\u5316\u7cfb\u7edf\u89c4\u6a21\u7684\u6269\u5927\uff0c\u9ad8\u6548\u7684\u6570\u636e\u5171\u4eab\u6846\u67b6\u548c\u9c81\u68d2\u7b97\u6cd5\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u751f\u6001\u7cfb\u7edf\u4e2d\u4e0b\u4e00\u4ee3\u7f51\u7edc\u6570\u636e\u5904\u7406\u7684\u5173\u952e\u4f5c\u7528\uff0c\u7279\u522b\u5173\u6ce8\u5e94\u7528\u548c\u7f51\u7edc\u63d0\u4f9b\u5546\uff08AP/NP\uff09\u4e4b\u95f4\u7684\u52a8\u6001\u5173\u7cfb\uff0c\u4ee5\u652f\u6301\u5177\u8eabAI\u7684\u81ea\u4e3b\u4efb\u52a1\u6267\u884c\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3aPANAMA\u7684\u65b0\u578b\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u57fa\u4e8e\u7f51\u7edc\u611f\u77e5\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\uff0c\u5e76\u91c7\u7528\u4f18\u5148\u7ea7\u975e\u5bf9\u79f0\u3002\u5176\u6838\u5fc3\u662f\u7ed3\u5408\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u4e0e\u5206\u5e03\u5f0f\u6267\u884c\uff08CTDE\uff09\u6846\u67b6\u548c\u5f02\u6b65actor-learner\u67b6\u6784\uff0c\u4ee5\u52a0\u901f\u8bad\u7ec3\u5e76\u5b9e\u73b0\u5177\u8eabAI\u7684\u81ea\u4e3b\u4efb\u52a1\u6267\u884c\u3002", "result": "PANAMA\u5728\u8def\u5f84\u89c4\u5212\u6027\u80fd\u65b9\u9762\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u51c6\uff0c\u5728\u51c6\u786e\u6027\u3001\u901f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u5747\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002\u901a\u8fc7\u6a21\u62df\uff0c\u7814\u7a76\u5f3a\u8c03\u4e86\u4f18\u5316\u7684\u6570\u636e\u5171\u4eab\u7b56\u7565\u5bf9\u4e8e\u53ef\u6269\u5c55\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u91cd\u8981\u6027\uff0c\u786e\u4fdd\u4e86\u5728\u590d\u6742\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u97e7\u6027\u3002", "conclusion": "PANAMA\u6210\u529f\u5730\u5f25\u5408\u4e86\u7f51\u7edc\u611f\u77e5\u51b3\u7b56\u4e0e\u9c81\u68d2\u591a\u667a\u80fd\u4f53\u534f\u8c03\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u663e\u8457\u63a8\u8fdb\u4e86\u6570\u5b57\u5b6a\u751f\u3001\u65e0\u7ebf\u7f51\u7edc\u548cAI\u9a71\u52a8\u81ea\u52a8\u5316\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u3002"}}
{"id": "2508.06963", "pdf": "https://arxiv.org/pdf/2508.06963", "abs": "https://arxiv.org/abs/2508.06963", "authors": ["Changqing Li", "Tianlin Li", "Xiaohan Zhang", "Aishan Liu", "Li Pan"], "title": "MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) face persistent and evolving trustworthiness\nissues, motivating developers to seek automated and flexible repair methods\nthat enable convenient deployment across diverse scenarios. Existing repair\nmethods like supervised fine-tuning (SFT) and reinforcement learning with human\nfeedback (RLHF) are costly and slow, while prompt engineering lacks robustness\nand scalability. Representation engineering, which steers model behavior by\ninjecting targeted concept vectors during inference, offers a lightweight,\ntraining-free alternative. However, current approaches depend on manually\ncrafted samples and fixed steering strategies, limiting automation and\nadaptability. To overcome these challenges, we propose MASteer, the first\nend-to-end framework for trustworthiness repair in LLMs based on representation\nengineering. MASteer integrates two core components: AutoTester, a multi-agent\nsystem that generates diverse, high-quality steer samples tailored to developer\nneeds; and AutoRepairer, which constructs adaptive steering strategies with\nanchor vectors for automated, context-aware strategy selection during\ninference. Experiments on standard and customized trustworthiness tasks show\nMASteer consistently outperforms baselines, improving metrics by 15.36% on\nLLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, while maintaining general model\ncapabilities. MASteer demonstrates strong robustness, generalization, and\npractical value for scalable, efficient trustworthiness repair.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MASteer\uff0c\u4e00\u4e2a\u57fa\u4e8e\u8868\u793a\u5de5\u7a0b\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6837\u672c\u751f\u6210\u548c\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u6709\u6548\u4e14\u9ad8\u6548\u5730\u4fee\u590d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4fe1\u4efb\u5ea6\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b58\u5728\u6301\u7eed\u7684\u4fe1\u4efb\u5ea6\u95ee\u9898\u3002\u73b0\u6709\u4fee\u590d\u65b9\u6cd5\uff08\u5982SFT\u3001RLHF\uff09\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\uff0c\u63d0\u793a\u5de5\u7a0b\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002\u8868\u793a\u5de5\u7a0b\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u6837\u672c\u548c\u56fa\u5b9a\u7b56\u7565\uff0c\u9650\u5236\u4e86\u81ea\u52a8\u5316\u548c\u9002\u5e94\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86MASteer\uff0c\u9996\u4e2a\u57fa\u4e8e\u8868\u793a\u5de5\u7a0b\u7684LLM\u4fe1\u4efb\u5ea6\u7aef\u5230\u7aef\u4fee\u590d\u6846\u67b6\u3002MASteer\u5305\u542b\u4e24\u5927\u6838\u5fc3\u7ec4\u4ef6\uff1aAutoTester\uff08\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684\u5f15\u5bfc\u6837\u672c\uff09\u548cAutoRepairer\uff08\u6784\u5efa\u81ea\u9002\u5e94\u5f15\u5bfc\u7b56\u7565\uff0c\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u951a\u5411\u91cf\u5b9e\u73b0\u81ea\u52a8\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7b56\u7565\u9009\u62e9\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMASteer\u5728\u6807\u51c6\u548c\u5b9a\u5236\u7684\u4fe1\u4efb\u5ea6\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728LLaMA-3.1-8B-Chat\u4e0a\u5c06\u6307\u6807\u63d0\u9ad8\u4e8615.36%\uff0c\u5728Qwen-3-8B-Chat\u4e0a\u63d0\u9ad8\u4e864.21%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u901a\u7528\u80fd\u529b\u3002\u5b83\u8fd8\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MASteer\u4e3aLLMs\u7684\u4fe1\u4efb\u5ea6\u4fee\u590d\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u7684\u5b9e\u7528\u4ef7\u503c\uff0c\u5e76\u5c55\u793a\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.06624", "pdf": "https://arxiv.org/pdf/2508.06624", "abs": "https://arxiv.org/abs/2508.06624", "authors": ["Kexin Yu", "Zihan Xu", "Jialei Xie", "Carter Adams"], "title": "VL-MedGuide: A Visual-Linguistic Large Model for Intelligent and Explainable Skin Disease Auxiliary Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Accurate diagnosis of skin diseases remains a significant challenge due to\nthe complex and diverse visual features present in dermatoscopic images, often\ncompounded by a lack of interpretability in existing purely visual diagnostic\nmodels. To address these limitations, this study introduces VL-MedGuide\n(Visual-Linguistic Medical Guide), a novel framework leveraging the powerful\nmulti-modal understanding and reasoning capabilities of Visual-Language Large\nModels (LVLMs) for intelligent and inherently interpretable auxiliary diagnosis\nof skin conditions. VL-MedGuide operates in two interconnected stages: a\nMulti-modal Concept Perception Module, which identifies and linguistically\ndescribes dermatologically relevant visual features through sophisticated\nprompt engineering, and an Explainable Disease Reasoning Module, which\nintegrates these concepts with raw visual information via Chain-of-Thought\nprompting to provide precise disease diagnoses alongside transparent\nrationales. Comprehensive experiments on the Derm7pt dataset demonstrate that\nVL-MedGuide achieves state-of-the-art performance in both disease diagnosis\n(83.55% BACC, 80.12% F1) and concept detection (76.10% BACC, 67.45% F1),\nsurpassing existing baselines. Furthermore, human evaluations confirm the high\nclarity, completeness, and trustworthiness of its generated explanations,\nbridging the gap between AI performance and clinical utility by offering\nactionable, explainable insights for dermatological practice.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVL-MedGuide\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\uff08LVLMs\uff09\u5b9e\u73b0\u76ae\u80a4\u75c5\u7684\u667a\u80fd\u53ef\u89e3\u91ca\u8f85\u52a9\u8bca\u65ad\uff0c\u5728Derm7pt\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u6e05\u6670\u53ef\u9760\u7684\u89e3\u91ca\u3002", "motivation": "\u76ae\u80a4\u75c5\u8bca\u65ad\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u76ae\u80a4\u955c\u56fe\u50cf\u7279\u5f81\u590d\u6742\u591a\u6837\uff0c\u4e14\u73b0\u6709\u7eaf\u89c6\u89c9\u8bca\u65ad\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5f15\u5165VL-MedGuide\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\uff08LVLMs\uff09\u7684\u591a\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002\u5b83\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u591a\u6a21\u6001\u6982\u5ff5\u611f\u77e5\u6a21\u5757\uff08\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u8bc6\u522b\u5e76\u63cf\u8ff0\u76ae\u80a4\u75c5\u5b66\u76f8\u5173\u89c6\u89c9\u7279\u5f81\uff09\u548c\u53ef\u89e3\u91ca\u75be\u75c5\u63a8\u7406\u6a21\u5757\uff08\u901a\u8fc7\u601d\u7ef4\u94fe\u63d0\u793a\u6574\u5408\u6982\u5ff5\u4e0e\u539f\u59cb\u89c6\u89c9\u4fe1\u606f\uff0c\u63d0\u4f9b\u7cbe\u786e\u8bca\u65ad\u548c\u900f\u660e\u7406\u7531\uff09\u3002", "result": "\u5728Derm7pt\u6570\u636e\u96c6\u4e0a\uff0cVL-MedGuide\u5728\u75be\u75c5\u8bca\u65ad\uff0883.55% BACC, 80.12% F1\uff09\u548c\u6982\u5ff5\u68c0\u6d4b\uff0876.10% BACC, 67.45% F1\uff09\u65b9\u9762\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u6b64\u5916\uff0c\u4eba\u5de5\u8bc4\u4f30\u8bc1\u5b9e\u5176\u751f\u6210\u7684\u89e3\u91ca\u5177\u6709\u9ad8\u6e05\u6670\u5ea6\u3001\u5b8c\u6574\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "VL-MedGuide\u901a\u8fc7\u63d0\u4f9b\u53ef\u64cd\u4f5c\u3001\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\uff0c\u5f25\u5408\u4e86\u4eba\u5de5\u667a\u80fd\u6027\u80fd\u4e0e\u4e34\u5e8a\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u76ae\u80a4\u79d1\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2508.07101", "pdf": "https://arxiv.org/pdf/2508.07101", "abs": "https://arxiv.org/abs/2508.07101", "authors": ["Lijie Yang", "Zhihao Zhang", "Arti Jain", "Shijie Cao", "Baihong Yuan", "Yiwei Chen", "Zhihao Jia", "Ravi Netravali"], "title": "Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large reasoning models achieve strong performance through test-time scaling\nbut incur substantial computational overhead, particularly from excessive token\ngeneration when processing short input prompts. While sparse attention\nmechanisms can reduce latency and memory usage, existing approaches suffer from\nsignificant accuracy degradation due to accumulated errors during\nlong-generation reasoning. These methods generally require either high token\nretention rates or expensive retraining. We introduce LessIsMore, a\ntraining-free sparse attention mechanism for reasoning tasks, which leverages\nglobal attention patterns rather than relying on traditional head-specific\nlocal optimizations. LessIsMore aggregates token selections from local\nattention heads with recent contextual information, enabling unified cross-head\ntoken ranking for future decoding layers. This unified selection improves\ngeneralization and efficiency by avoiding the need to maintain separate token\nsubsets per head. Evaluation across diverse reasoning tasks and benchmarks\nshows that LessIsMore preserves -- and in some cases improves -- accuracy while\nachieving a $1.1\\times$ average decoding speed-up compared to full attention.\nMoreover, LessIsMore attends to $2\\times$ fewer tokens without accuracy loss,\nachieving a $1.13\\times$ end-to-end speed-up compared to existing sparse\nattention methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aLessIsMore\u7684\u65e0\u8bad\u7ec3\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u52a0\u901f\u5927\u578b\u63a8\u7406\u6a21\u578b\uff0c\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u548c\u6548\u7387\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u867d\u8868\u73b0\u5f3a\u5927\uff0c\u4f46\u5728\u6d4b\u8bd5\u65f6\u56e0\u8fc7\u591a\u7684\u4ee4\u724c\u751f\u6210\u800c\u4ea7\u751f\u5de8\u5927\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u77ed\u8f93\u5165\u63d0\u793a\u65f6\u3002\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u80fd\u964d\u4f4e\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u4f46\u5e38\u56e0\u957f\u751f\u6210\u63a8\u7406\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee\u5bfc\u81f4\u663e\u8457\u7684\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u4e14\u901a\u5e38\u9700\u8981\u9ad8\u4ee4\u724c\u4fdd\u7559\u7387\u6216\u6602\u8d35\u7684\u518d\u8bad\u7ec3\u3002", "method": "\u5f15\u5165LessIsMore\uff0c\u4e00\u79cd\u9488\u5bf9\u63a8\u7406\u4efb\u52a1\u7684\u65e0\u8bad\u7ec3\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5168\u5c40\u6ce8\u610f\u529b\u6a21\u5f0f\u800c\u975e\u4f20\u7edf\u7684\u5934\u90e8\u7279\u5b9a\u5c40\u90e8\u4f18\u5316\u3002LessIsMore\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u6ce8\u610f\u529b\u5934\u548c\u6700\u65b0\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u805a\u5408\u4ee4\u724c\u9009\u62e9\uff0c\u5b9e\u73b0\u8de8\u5934\u7edf\u4e00\u7684\u4ee4\u724c\u6392\u5e8f\uff0c\u4ee5\u7528\u4e8e\u540e\u7eed\u89e3\u7801\u5c42\uff0c\u4ece\u800c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002", "result": "LessIsMore\u5728\u591a\u6837\u5316\u7684\u63a8\u7406\u4efb\u52a1\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4fdd\u6301\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5e76\u76f8\u8f83\u4e8e\u5168\u6ce8\u610f\u529b\u5b9e\u73b0\u4e86\u5e73\u57471.1\u500d\u7684\u89e3\u7801\u52a0\u901f\u3002\u6b64\u5916\uff0cLessIsMore\u5728\u4e0d\u635f\u5931\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u5173\u6ce8\u7684\u4ee4\u724c\u6570\u91cf\u51cf\u5c112\u500d\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u5b9e\u73b0\u4e861.13\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\u3002", "conclusion": "LessIsMore\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u7a00\u758f\u6ce8\u610f\u529b\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002\u5b83\u901a\u8fc7\u7edf\u4e00\u7684\u8de8\u5934\u4ee4\u724c\u9009\u62e9\uff0c\u5728\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u548c\u6548\u7387\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u51c6\u786e\u6027\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.06776", "pdf": "https://arxiv.org/pdf/2508.06776", "abs": "https://arxiv.org/abs/2508.06776", "authors": ["Amit Pandey"], "title": "Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "14 pages", "summary": "We present Zero-Direction Probing (ZDP), a theory-only framework for\ndetecting model drift from null directions of transformer activations without\ntask labels or output evaluations. Under assumptions A1--A6, we prove: (i) the\nVariance--Leak Theorem, (ii) Fisher Null-Conservation, (iii) a Rank--Leak bound\nfor low-rank updates, and (iv) a logarithmic-regret guarantee for online\nnull-space trackers. We derive a Spectral Null-Leakage (SNL) metric with\nnon-asymptotic tail bounds and a concentration inequality, yielding a-priori\nthresholds for drift under a Gaussian null model. These results show that\nmonitoring right/left null spaces of layer activations and their Fisher\ngeometry provides concrete, testable guarantees on representational change.", "AI": {"tldr": "Zero-Direction Probing (ZDP)\u662f\u4e00\u4e2a\u7eaf\u7406\u8bba\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u5206\u6790Transformer\u6fc0\u6d3b\u7684\u96f6\u65b9\u5411\u6765\u68c0\u6d4b\u6a21\u578b\u6f02\u79fb\uff0c\u65e0\u9700\u4efb\u52a1\u6807\u7b7e\u6216\u8f93\u51fa\u8bc4\u4f30\u3002", "motivation": "\u5728\u7f3a\u4e4f\u4efb\u52a1\u6807\u7b7e\u6216\u8f93\u51fa\u8bc4\u4f30\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u68c0\u6d4b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662fTransformer\uff09\u7684\u8868\u5f81\u6f02\u79fb\u662f\u4e00\u4e2a\u91cd\u8981\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7eaf\u7406\u8bba\u6846\u67b6Zero-Direction Probing (ZDP)\uff0c\u5728A1-A6\u5047\u8bbe\u4e0b\uff0c\u901a\u8fc7\u5206\u6790Transformer\u5c42\u6fc0\u6d3b\u7684\u53f3/\u5de6\u96f6\u7a7a\u95f4\u53ca\u5176Fisher\u51e0\u4f55\u7ed3\u6784\uff0c\u8bc1\u660e\u4e86\u4e00\u7cfb\u5217\u5b9a\u7406\u3002\u57fa\u4e8e\u8fd9\u4e9b\u7406\u8bba\uff0c\u8fdb\u4e00\u6b65\u63a8\u5bfc\u4e86\u9891\u8c31\u96f6\u6cc4\u6f0f\uff08SNL\uff09\u5ea6\u91cf\u3002", "result": "\u6210\u529f\u8bc1\u660e\u4e86\u65b9\u5dee\u6cc4\u6f0f\u5b9a\u7406\u3001Fisher\u96f6\u7a7a\u95f4\u5b88\u6052\u3001\u4f4e\u79e9\u66f4\u65b0\u7684\u79e9\u6cc4\u6f0f\u754c\u9650\u4ee5\u53ca\u5728\u7ebf\u96f6\u7a7a\u95f4\u8ddf\u8e2a\u5668\u7684\u5bf9\u6570\u9057\u61be\u4fdd\u8bc1\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u9891\u8c31\u96f6\u6cc4\u6f0f\uff08SNL\uff09\u5ea6\u91cf\uff0c\u5e76\u7ed9\u51fa\u4e86\u975e\u6e10\u8fd1\u5c3e\u90e8\u754c\u3001\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u4ee5\u53ca\u5728\u9ad8\u65af\u96f6\u6a21\u578b\u4e0b\u7684\u5148\u9a8c\u6f02\u79fb\u9608\u503c\u3002", "conclusion": "\u76d1\u63a7Transformer\u5c42\u6fc0\u6d3b\u7684\u53f3/\u5de6\u96f6\u7a7a\u95f4\u53ca\u5176Fisher\u51e0\u4f55\u7ed3\u6784\uff0c\u80fd\u591f\u4e3a\u6a21\u578b\u8868\u5f81\u53d8\u5316\uff08\u5373\u6a21\u578b\u6f02\u79fb\uff09\u7684\u68c0\u6d4b\u63d0\u4f9b\u5177\u4f53\u4e14\u53ef\u68c0\u9a8c\u7684\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2508.06972", "pdf": "https://arxiv.org/pdf/2508.06972", "abs": "https://arxiv.org/abs/2508.06972", "authors": ["Dan Ivanov", "Tristan Freiberg", "Haruna Isah"], "title": "DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning", "categories": ["cs.AI", "cs.CR", "cs.DC", "cs.LG"], "comment": "12 pages, 8 figures, and 10 tables", "summary": "DSperse is a modular framework for distributed machine learning inference\nwith strategic cryptographic verification. Operating within the emerging\nparadigm of distributed zero-knowledge machine learning, DSperse avoids the\nhigh cost and rigidity of full-model circuitization by enabling targeted\nverification of strategically chosen subcomputations. These verifiable\nsegments, or \"slices\", may cover part or all of the inference pipeline, with\nglobal consistency enforced through audit, replication, or economic incentives.\nThis architecture supports a pragmatic form of trust minimization, localizing\nzero-knowledge proofs to the components where they provide the greatest value.\nWe evaluate DSperse using multiple proving systems and report empirical results\non memory usage, runtime, and circuit behavior under sliced and unsliced\nconfigurations. By allowing proof boundaries to align flexibly with the model's\nlogical structure, DSperse supports scalable, targeted verification strategies\nsuited to diverse deployment needs.", "AI": {"tldr": "DSperse\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u5b50\u8ba1\u7b97\u8fdb\u884c\u7b56\u7565\u6027\u52a0\u5bc6\u9a8c\u8bc1\uff08\u201c\u5206\u7247\u201d\uff09\uff0c\u907f\u514d\u4e86\u5168\u6a21\u578b\u7535\u8def\u5316\u7684\u6210\u672c\u548c\u50f5\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u76ee\u6807\u5f0f\u7684\u4fe1\u4efb\u6700\u5c0f\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u96f6\u77e5\u8bc6\u673a\u5668\u5b66\u4e60\u8303\u5f0f\u4e2d\uff0c\u5168\u6a21\u578b\u7535\u8def\u5316\u6210\u672c\u9ad8\u6602\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "DSperse\u901a\u8fc7\u5bf9\u7b56\u7565\u6027\u9009\u62e9\u7684\u5b50\u8ba1\u7b97\uff08\u201c\u5206\u7247\u201d\uff09\u8fdb\u884c\u76ee\u6807\u5f0f\u9a8c\u8bc1\uff0c\u4ee5\u907f\u514d\u5168\u6a21\u578b\u7535\u8def\u5316\u3002\u5b83\u652f\u6301\u7075\u6d3b\u7684\u8bc1\u660e\u8fb9\u754c\uff0c\u5e76\u53ef\u901a\u8fc7\u5ba1\u8ba1\u3001\u590d\u5236\u6216\u7ecf\u6d4e\u6fc0\u52b1\u6765\u5f3a\u5236\u6267\u884c\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u5c06\u96f6\u77e5\u8bc6\u8bc1\u660e\u5b9a\u4f4d\u5230\u6700\u6709\u4ef7\u503c\u7684\u7ec4\u4ef6\u3002", "result": "\u4f5c\u8005\u4f7f\u7528\u591a\u79cd\u8bc1\u660e\u7cfb\u7edf\u5bf9DSperse\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u62a5\u544a\u4e86\u5728\u5206\u7247\u548c\u672a\u5206\u7247\u914d\u7f6e\u4e0b\u7684\u5185\u5b58\u4f7f\u7528\u3001\u8fd0\u884c\u65f6\u548c\u7535\u8def\u884c\u4e3a\u7684\u5b9e\u8bc1\u7ed3\u679c\u3002", "conclusion": "DSperse\u901a\u8fc7\u5141\u8bb8\u8bc1\u660e\u8fb9\u754c\u4e0e\u6a21\u578b\u7684\u903b\u8f91\u7ed3\u6784\u7075\u6d3b\u5bf9\u9f50\uff0c\u652f\u6301\u53ef\u6269\u5c55\u3001\u76ee\u6807\u5f0f\u7684\u9a8c\u8bc1\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u90e8\u7f72\u9700\u6c42\uff0c\u5e76\u5b9e\u73b0\u4e86\u5b9e\u7528\u5f62\u5f0f\u7684\u4fe1\u4efb\u6700\u5c0f\u5316\u3002"}}
{"id": "2508.06625", "pdf": "https://arxiv.org/pdf/2508.06625", "abs": "https://arxiv.org/abs/2508.06625", "authors": ["Shilong Zou", "Yuhang Huang", "Renjiao Yi", "Chenyang Zhu", "Kai Xu"], "title": "CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a diffusion-based cross-domain image translator in the absence\nof paired training data. Unlike GAN-based methods, our approach integrates\ndiffusion models to learn the image translation process, allowing for more\ncoverable modeling of the data distribution and performance improvement of the\ncross-domain translation. However, incorporating the translation process within\nthe diffusion process is still challenging since the two processes are not\naligned exactly, i.e., the diffusion process is applied to the noisy signal\nwhile the translation process is conducted on the clean signal. As a result,\nrecent diffusion-based studies employ separate training or shallow integration\nto learn the two processes, yet this may cause the local minimal of the\ntranslation optimization, constraining the effectiveness of diffusion models.\nTo address the problem, we propose a novel joint learning framework that aligns\nthe diffusion and the translation process, thereby improving the global\noptimality. Specifically, we propose to extract the image components with\ndiffusion models to represent the clean signal and employ the translation\nprocess with the image components, enabling an end-to-end joint learning\nmanner. On the other hand, we introduce a time-dependent translation network to\nlearn the complex translation mapping, resulting in effective translation\nlearning and significant performance improvement. Benefiting from the design of\njoint learning, our method enables global optimization of both processes,\nenhancing the optimality and achieving improved fidelity and structural\nconsistency. We have conducted extensive experiments on RGB$\\leftrightarrow$RGB\nand diverse cross-modality translation tasks including\nRGB$\\leftrightarrow$Edge, RGB$\\leftrightarrow$Semantics and\nRGB$\\leftrightarrow$Depth, showcasing better generative performances than the\nstate of the arts.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07111", "pdf": "https://arxiv.org/pdf/2508.07111", "abs": "https://arxiv.org/abs/2508.07111", "authors": ["Falaah Arif Khan", "Nivedha Sivakumar", "Yinong Oliver Wang", "Katherine Metcalf", "Cezanne Camacho", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "title": "Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved impressive performance, leading to\ntheir widespread adoption as decision-support tools in resource-constrained\ncontexts like hiring and admissions. There is, however, scientific consensus\nthat AI systems can reflect and exacerbate societal biases, raising concerns\nabout identity-based harm when used in critical social contexts. Prior work has\nlaid a solid foundation for assessing bias in LLMs by evaluating demographic\ndisparities in different language reasoning tasks. In this work, we extend\nsingle-axis fairness evaluations to examine intersectional bias, recognizing\nthat when multiple axes of discrimination intersect, they create distinct\npatterns of disadvantage. We create a new benchmark called WinoIdentity by\naugmenting the WinoBias dataset with 25 demographic markers across 10\nattributes, including age, nationality, and race, intersected with binary\ngender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.\nFocusing on harms of omission due to underrepresentation, we investigate bias\nthrough the lens of uncertainty and propose a group (un)fairness metric called\nCoreference Confidence Disparity which measures whether models are more or less\nconfident for some intersectional identities than others. We evaluate five\nrecently published LLMs and find confidence disparities as high as 40% along\nvarious demographic attributes including body type, sexual orientation and\nsocio-economic status, with models being most uncertain about\ndoubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,\ncoreference confidence decreases even for hegemonic or privileged markers,\nindicating that the recent impressive performance of LLMs is more likely due to\nmemorization than logical reasoning. Notably, these are two independent\nfailures in value alignment and validity that can compound to cause social\nharm.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u521b\u5efaWinoIdentity\u57fa\u51c6\u548c\u63d0\u51faCoreference Confidence Disparity\u6307\u6807\uff0c\u6df1\u5165\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4ea4\u53c9\u504f\u89c1\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u5904\u7406\u591a\u91cd\u5f31\u52bf\u7fa4\u4f53\u65f6\u5b58\u5728\u663e\u8457\u7f6e\u4fe1\u5ea6\u5dee\u5f02\uff0c\u5e76\u6697\u793aLLMs\u7684\u6027\u80fd\u53ef\u80fd\u66f4\u591a\u6e90\u4e8e\u8bb0\u5fc6\u800c\u975e\u903b\u8f91\u63a8\u7406\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u62db\u8058\u548c\u62db\u751f\u7b49\u5173\u952e\u8d44\u6e90\u53d7\u9650\u9886\u57df\u3002\u7136\u800c\uff0c\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u53ef\u80fd\u53cd\u6620\u5e76\u52a0\u5267\u793e\u4f1a\u504f\u89c1\uff0c\u5f15\u53d1\u4e86\u5728\u91cd\u8981\u793e\u4f1a\u60c5\u5883\u4e2d\u4f7f\u7528\u65f6\u5bf9\u57fa\u4e8e\u8eab\u4efd\u7684\u4f24\u5bb3\u7684\u62c5\u5fe7\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u5355\u8f74\u504f\u89c1\u8bc4\u4f30\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6269\u5c55\u81f3\u4ea4\u53c9\u504f\u89c1\uff0c\u4ee5\u8bc6\u522b\u591a\u8f74\u6b67\u89c6\u4ea4\u7ec7\u6240\u4ea7\u751f\u7684\u72ec\u7279\u52a3\u52bf\u6a21\u5f0f\u3002", "method": "\u7814\u7a76\u6269\u5c55\u4e86\u5355\u8f74\u516c\u5e73\u6027\u8bc4\u4f30\uff0c\u4ee5\u8003\u5bdf\u4ea4\u53c9\u504f\u89c1\u3002\u4e3a\u6b64\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aWinoIdentity\u7684\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7\u5c06WinoBias\u6570\u636e\u96c6\u4e0e10\u4e2a\u5c5e\u6027\uff08\u5305\u62ec\u5e74\u9f84\u3001\u56fd\u7c4d\u3001\u79cd\u65cf\uff09\u768425\u4e2a\u4eba\u53e3\u7edf\u8ba1\u5b66\u6807\u8bb0\u4e0e\u4e8c\u5143\u6027\u522b\u4ea4\u53c9\u7ed3\u5408\uff0c\u751f\u6210\u4e86245,700\u4e2a\u63d0\u793a\uff0c\u7528\u4e8e\u8bc4\u4f3050\u79cd\u4e0d\u540c\u7684\u504f\u89c1\u6a21\u5f0f\u3002\u7814\u7a76\u5173\u6ce8\u56e0\u4ee3\u8868\u6027\u4e0d\u8db3\u5bfc\u81f4\u7684\u9057\u6f0f\u4f24\u5bb3\uff0c\u5e76\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u89c6\u89d2\u8c03\u67e5\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u6307\u4ee3\u7f6e\u4fe1\u5ea6\u5dee\u5f02\u201d\uff08Coreference Confidence Disparity\uff09\u7684\u7fa4\u4f53\uff08\u4e0d\uff09\u516c\u5e73\u6027\u6307\u6807\uff0c\u8be5\u6307\u6807\u8861\u91cf\u6a21\u578b\u5bf9\u67d0\u4e9b\u4ea4\u53c9\u8eab\u4efd\u7684\u7f6e\u4fe1\u5ea6\u662f\u5426\u9ad8\u4e8e\u6216\u4f4e\u4e8e\u5176\u4ed6\u8eab\u4efd\u3002\u8bc4\u4f30\u4e86\u4e94\u79cd\u8fd1\u671f\u53d1\u5e03\u7684LLMs\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4f53\u578b\u3001\u6027\u53d6\u5411\u548c\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u7b49\u5404\u79cd\u4eba\u53e3\u7edf\u8ba1\u5b66\u5c5e\u6027\u4e0a\uff0c\u7f6e\u4fe1\u5ea6\u5dee\u5f02\u9ad8\u8fbe40%\u3002\u6a21\u578b\u5728\u53cd\u523b\u677f\u5370\u8c61\u8bbe\u7f6e\u4e0b\uff0c\u5bf9\u53cc\u91cd\u5f31\u52bf\u8eab\u4efd\u8868\u73b0\u51fa\u6700\u5927\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u5373\u4f7f\u5bf9\u4e8e\u4e3b\u5bfc\u6216\u7279\u6743\u6807\u8bb0\uff0c\u6307\u4ee3\u7f6e\u4fe1\u5ea6\u4e5f\u4f1a\u4e0b\u964d\uff0c\u8fd9\u8868\u660eLLMs\u8fd1\u671f\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u66f4\u53ef\u80fd\u5f52\u56e0\u4e8e\u8bb0\u5fc6\u800c\u975e\u903b\u8f91\u63a8\u7406\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u7684\u7f6e\u4fe1\u5ea6\u5dee\u5f02\u662f\u4ef7\u503c\u5bf9\u9f50\u548c\u6709\u6548\u6027\u65b9\u9762\u7684\u72ec\u7acb\u7f3a\u9677\uff0c\u8fd9\u4e24\u79cd\u7f3a\u9677\u53ef\u80fd\u7ed3\u5408\u8d77\u6765\u5bfc\u81f4\u793e\u4f1a\u5371\u5bb3\u3002LLMs\u5728\u5904\u7406\u4ea4\u53c9\u8eab\u4efd\u65f6\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u5176\u6027\u80fd\u53ef\u80fd\u4e3b\u8981\u4f9d\u8d56\u4e8e\u8bb0\u5fc6\uff0c\u800c\u975e\u771f\u6b63\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u5728\u5173\u952e\u793e\u4f1a\u5e94\u7528\u4e2d\u6784\u6210\u98ce\u9669\u3002"}}
{"id": "2508.06783", "pdf": "https://arxiv.org/pdf/2508.06783", "abs": "https://arxiv.org/abs/2508.06783", "authors": ["Noel Teku", "Fengwei Tian", "Payel Bhattacharjee", "Souradip Chakraborty", "Amrit Singh Bedi", "Ravi Tandon"], "title": "PROPS: Progressively Private Self-alignment of Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.IT", "math.IT"], "comment": null, "summary": "Alignment is a key step in developing Large Language Models (LLMs) using\nhuman feedback to ensure adherence to human values and societal norms.\nDependence on human feedback raises privacy concerns about how much a labeler's\npreferences may reveal about their personal values, beliefs, and personality\ntraits. Existing approaches, such as Differentially Private SGD (DP-SGD),\nprovide rigorous privacy guarantees by privatizing gradients during fine-tuning\nand alignment but can provide more privacy than necessary as human preferences\nare tied only to labels of (prompt, response) pairs and can degrade model\nutility. This work focuses on LLM alignment with preference-level privacy,\nwhich preserves the privacy of preference labels provided by humans. We propose\nPROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving\nalignment framework where privately aligned models in previous stages can serve\nas labelers for supplementing training data in the subsequent stages of\nalignment. We present theoretical guarantees for PROPS as well as comprehensive\nvalidation using multiple models (Pythia and GPT) and datasets (AlpacaEval,\nAnthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over\nexisting methods while still providing high privacy. For the same privacy\nbudget, alignment via PROPS can achieve up to 3x higher win-rates compared to\nDP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based\nalignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPROPS\uff0c\u4e00\u79cd\u591a\u9636\u6bb5\u9690\u79c1\u4fdd\u62a4\u81ea\u5bf9\u9f50\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3LLM\u5bf9\u9f50\u4e2d\u4eba\u7c7b\u53cd\u9988\u7684\u9690\u79c1\u95ee\u9898\uff0c\u901a\u8fc7\u4fdd\u62a4\u504f\u597d\u6807\u7b7e\u9690\u79c1\uff0c\u5728\u63d0\u5347\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5bf9\u9f50\u8fc7\u7a0b\u4f9d\u8d56\u4eba\u7c7b\u53cd\u9988\uff0c\u8fd9\u5f15\u53d1\u4e86\u5173\u4e8e\u6807\u7b7e\u8005\u4e2a\u4eba\u504f\u597d\u53ef\u80fd\u6cc4\u9732\u5176\u9690\u79c1\uff08\u4ef7\u503c\u89c2\u3001\u4fe1\u4ef0\u3001\u4eba\u683c\u7279\u8d28\uff09\u7684\u62c5\u5fe7\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u5dee\u5206\u9690\u79c1\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08DP-SGD\uff09\u867d\u7136\u63d0\u4f9b\u4e25\u683c\u9690\u79c1\u4fdd\u8bc1\uff0c\u4f46\u53ef\u80fd\u8fc7\u5ea6\u9690\u79c1\u5316\uff0c\u5e76\u635f\u5bb3\u6a21\u578b\u6548\u7528\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86PROPS\uff08PROgressively Private Self-alignment\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u9636\u6bb5\u7684\u9690\u79c1\u4fdd\u62a4\u5bf9\u9f50\u6846\u67b6\u3002\u5728\u8be5\u6846\u67b6\u4e2d\uff0c\u524d\u4e00\u9636\u6bb5\u7ecf\u8fc7\u9690\u79c1\u4fdd\u62a4\u5bf9\u9f50\u7684\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u6807\u7b7e\u8005\uff0c\u4e3a\u540e\u7eed\u9636\u6bb5\u7684\u5bf9\u9f50\u8865\u5145\u8bad\u7ec3\u6570\u636e\u3002", "result": "PROPS\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u578b\u548c\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5168\u9762\u9a8c\u8bc1\u3002\u5728\u76f8\u540c\u7684\u9690\u79c1\u9884\u7b97\u4e0b\uff0cPROPS\u5bf9\u9f50\u65b9\u6cd5\u76f8\u6bd4DP-SGD\u53ef\u5b9e\u73b0\u9ad8\u8fbe3\u500d\u7684\u80dc\u7387\u63d0\u5347\uff0c\u76f8\u6bd4\u57fa\u4e8e\u968f\u673a\u54cd\u5e94\uff08RR\uff09\u7684\u5bf9\u9f50\u65b9\u6cd5\u53ef\u5b9e\u73b02.5\u500d\u7684\u80dc\u7387\u63d0\u5347\u3002", "conclusion": "PROPS\u5728LLM\u5bf9\u9f50\u4e2d\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u504f\u597d\u7ea7\u522b\u7684\u9690\u79c1\u4fdd\u62a4\uff0c\u5e76\u5728\u4fdd\u6301\u9ad8\u9690\u79c1\u9884\u7b97\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u6216\u5bf9\u9f50\u6548\u679c\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u3002"}}
{"id": "2508.06980", "pdf": "https://arxiv.org/pdf/2508.06980", "abs": "https://arxiv.org/abs/2508.06980", "authors": ["Aswin Paul", "Moein Khajehnejad", "Forough Habibollahi", "Brett J. Kagan", "Adeel Razi"], "title": "Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model", "categories": ["cs.AI"], "comment": "18 pages, 8 figures", "summary": "With recent and rapid advancements in artificial intelligence (AI),\nunderstanding the foundation of purposeful behaviour in autonomous agents is\ncrucial for developing safe and efficient systems. While artificial neural\nnetworks have dominated the path to AI, recent studies are exploring the\npotential of biologically based systems, such as networks of living biological\nneuronal networks. Along with promises of high power and data efficiency, these\nsystems may also inform more explainable and biologically plausible models. In\nthis work, we propose a framework rooted in active inference, a general theory\nof behaviour, to model decision-making in embodied agents. Using\nexperiment-informed generative models, we simulate decision-making processes in\na simulated game-play environment, mirroring experimental setups that use\nbiological neurons. Our results demonstrate learning in these agents, providing\ninsights into the role of memory-based learning and predictive planning in\nintelligent decision-making. This work contributes to the growing field of\nexplainable AI by offering a biologically grounded and scalable approach to\nunderstanding purposeful behaviour in agents.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u7684\u6846\u67b6\uff0c\u6a21\u62df\u5e76\u5c55\u793a\u4e86\u5177\u8eab\u667a\u80fd\u4f53\uff08\u6a21\u62df\u751f\u7269\u795e\u7ecf\u7f51\u7edc\uff09\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u4e3a\u53ef\u89e3\u91caAI\u63d0\u4f9b\u751f\u7269\u5b66\u57fa\u7840\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740AI\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7406\u89e3\u81ea\u4e3b\u667a\u80fd\u4f53\u76ee\u7684\u6027\u884c\u4e3a\u7684\u57fa\u7840\u5bf9\u5f00\u53d1\u5b89\u5168\u9ad8\u6548\u7684\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u57fa\u4e8e\u751f\u7269\u7684\u7cfb\u7edf\uff08\u5982\u6d3b\u4f53\u751f\u7269\u795e\u7ecf\u5143\u7f51\u7edc\uff09\u5728\u80fd\u6548\u548c\u6570\u636e\u6548\u7387\u65b9\u9762\u6709\u524d\u666f\uff0c\u5e76\u6709\u671b\u63d0\u4f9b\u66f4\u53ef\u89e3\u91ca\u548c\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u6a21\u578b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u690d\u6839\u4e8e\u4e3b\u52a8\u63a8\u7406\uff08\u4e00\u79cd\u901a\u7528\u884c\u4e3a\u7406\u8bba\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u5177\u8eab\u667a\u80fd\u4f53\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\u5229\u7528\u53d7\u5b9e\u9a8c\u542f\u53d1\u7684\u751f\u6210\u6a21\u578b\uff0c\u5728\u6a21\u62df\u6e38\u620f\u73af\u5883\u4e2d\u6a21\u62df\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4ee5\u6a21\u4eff\u751f\u7269\u795e\u7ecf\u5143\u5b9e\u9a8c\u8bbe\u7f6e\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u8fd9\u4e9b\u667a\u80fd\u4f53\u5c55\u73b0\u51fa\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u4e3a\u8bb0\u5fc6\u5b66\u4e60\u548c\u9884\u6d4b\u89c4\u5212\u5728\u667a\u80fd\u51b3\u7b56\u4e2d\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u9886\u57df\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u751f\u7269\u5b66\u57fa\u7840\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u7406\u89e3\u667a\u80fd\u4f53\u76ee\u7684\u6027\u884c\u4e3a\u3002"}}
{"id": "2508.06632", "pdf": "https://arxiv.org/pdf/2508.06632", "abs": "https://arxiv.org/abs/2508.06632", "authors": ["Wenpeng Xing", "Jie Chen", "Zaifeng Yang", "Tiancheng Zhao", "Gaolei Li", "Changting Lin", "Yike Guo", "Meng Han"], "title": "CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Neural Radiance Fields (NeRF) have shown impressive performance in novel view\nsynthesis, but challenges remain in rendering scenes with complex specular\nreflections and highlights. Existing approaches may produce blurry reflections\ndue to entanglement between lighting and material properties, or encounter\noptimization instability when relying on physically-based inverse rendering. In\nthis work, we present a neural rendering framework based on dynamic coefficient\ndecomposition, aiming to improve the modeling of view-dependent appearance. Our\napproach decomposes complex appearance into a shared, static neural basis that\nencodes intrinsic material properties, and a set of dynamic coefficients\ngenerated by a Coefficient Network conditioned on view and illumination. A\nDynamic Radiance Integrator then combines these components to synthesize the\nfinal radiance. Experimental results on several challenging benchmarks suggest\nthat our method can produce sharper and more realistic specular highlights\ncompared to existing techniques. We hope that this decomposition paradigm can\nprovide a flexible and effective direction for modeling complex appearance in\nneural scene representations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u7cfb\u6570\u5206\u89e3\u7684\u795e\u7ecf\u6e32\u67d3\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3NeRF\u5728\u5efa\u6a21\u590d\u6742\u9ad8\u5149\u548c\u53cd\u5c04\u65f6\u5b58\u5728\u7684\u6a21\u7cca\u548c\u4f18\u5316\u95ee\u9898\uff0c\u4ece\u800c\u751f\u6210\u66f4\u6e05\u6670\u903c\u771f\u7684\u89c6\u56fe\u3002", "motivation": "\u73b0\u6709NeRF\u5728\u6e32\u67d3\u5177\u6709\u590d\u6742\u955c\u9762\u53cd\u5c04\u548c\u9ad8\u5149\u7684\u573a\u666f\u65f6\u9762\u4e34\u6311\u6218\uff0c\u8868\u73b0\u4e3a\u5149\u7167\u4e0e\u6750\u8d28\u5c5e\u6027\u7684\u8026\u5408\u5bfc\u81f4\u53cd\u5c04\u6a21\u7cca\uff0c\u4ee5\u53ca\u4f9d\u8d56\u7269\u7406\u9006\u6e32\u67d3\u53ef\u80fd\u51fa\u73b0\u4f18\u5316\u4e0d\u7a33\u5b9a\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u52a8\u6001\u7cfb\u6570\u5206\u89e3\u7684\u795e\u7ecf\u6e32\u67d3\u6846\u67b6\u3002\u5b83\u5c06\u590d\u6742\u5916\u89c2\u5206\u89e3\u4e3a\uff1a1. \u7f16\u7801\u56fa\u6709\u6750\u8d28\u5c5e\u6027\u7684\u5171\u4eab\u9759\u6001\u795e\u7ecf\u57fa\uff1b2. \u7531\u7cfb\u6570\u7f51\u7edc\u6839\u636e\u89c6\u89d2\u548c\u5149\u7167\u751f\u6210\u7684\u52a8\u6001\u7cfb\u6570\u3002\u7136\u540e\uff0c\u4e00\u4e2a\u52a8\u6001\u8f90\u5c04\u79ef\u5206\u5668\u5c06\u8fd9\u4e9b\u5206\u91cf\u7ed3\u5408\u4ee5\u5408\u6210\u6700\u7ec8\u8f90\u5c04\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0c\u80fd\u591f\u751f\u6210\u66f4\u6e05\u6670\u3001\u66f4\u771f\u5b9e\u7684\u955c\u9762\u9ad8\u5149\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u89e3\u8303\u5f0f\u4e3a\u795e\u7ecf\u573a\u666f\u8868\u793a\u4e2d\u590d\u6742\u5916\u89c2\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u6709\u6548\u7684\u65b9\u5411\u3002"}}
{"id": "2508.07143", "pdf": "https://arxiv.org/pdf/2508.07143", "abs": "https://arxiv.org/abs/2508.07143", "authors": ["Anna Seo Gyeong Choi", "Hoon Choi"], "title": "Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic Speech Recognition (ASR) systems now mediate countless\nhuman-technology interactions, yet research on their fairness implications\nremains surprisingly limited. This paper examines ASR bias through a\nphilosophical lens, arguing that systematic misrecognition of certain speech\nvarieties constitutes more than a technical limitation -- it represents a form\nof disrespect that compounds historical injustices against marginalized\nlinguistic communities. We distinguish between morally neutral classification\n(discriminate1) and harmful discrimination (discriminate2), demonstrating how\nASR systems can inadvertently transform the former into the latter when they\nconsistently misrecognize non-standard dialects. We identify three unique\nethical dimensions of speech technologies that differentiate ASR bias from\nother algorithmic fairness concerns: the temporal burden placed on speakers of\nnon-standard varieties (\"temporal taxation\"), the disruption of conversational\nflow when systems misrecognize speech, and the fundamental connection between\nspeech patterns and personal/cultural identity. These factors create asymmetric\npower relationships that existing technical fairness metrics fail to capture.\nThe paper analyzes the tension between linguistic standardization and pluralism\nin ASR development, arguing that current approaches often embed and reinforce\nproblematic language ideologies. We conclude that addressing ASR bias requires\nmore than technical interventions; it demands recognition of diverse speech\nvarieties as legitimate forms of expression worthy of technological\naccommodation. This philosophical reframing offers new pathways for developing\nASR systems that respect linguistic diversity and speaker autonomy.", "AI": {"tldr": "\u672c\u6587\u4ece\u54f2\u5b66\u89d2\u5ea6\u5ba1\u89c6\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\uff0c\u8ba4\u4e3a\u5176\u5bf9\u975e\u6807\u51c6\u8bed\u79cd\u7684\u8bef\u8bc6\u4e0d\u4ec5\u662f\u6280\u672f\u7f3a\u9677\uff0c\u66f4\u662f\u4e00\u79cd\u4e0d\u5c0a\u91cd\uff0c\u52a0\u5267\u4e86\u5bf9\u8fb9\u7f18\u8bed\u8a00\u7fa4\u4f53\u7684\u5386\u53f2\u4e0d\u516c\u3002\u6587\u7ae0\u63d0\u51fa\u4e86ASR\u504f\u89c1\u7684\u72ec\u7279\u4f26\u7406\u7ef4\u5ea6\uff0c\u5e76\u4e3b\u5f20\u89e3\u51b3\u504f\u89c1\u9700\u8d85\u8d8a\u6280\u672f\u8303\u7574\uff0c\u5e94\u5c0a\u91cd\u8bed\u8a00\u591a\u6837\u6027\u3002", "motivation": "ASR\u7cfb\u7edf\u5e94\u7528\u5e7f\u6cdb\u4f46\u516c\u5e73\u6027\u7814\u7a76\u4e0d\u8db3\uff1b\u7cfb\u7edf\u5bf9\u7279\u5b9a\u8bed\u97f3\u53d8\u4f53\u7684\u8bef\u8bc6\u4e0d\u4ec5\u662f\u6280\u672f\u5c40\u9650\uff0c\u66f4\u662f\u5bf9\u8fb9\u7f18\u8bed\u8a00\u7fa4\u4f53\u7684\u4e0d\u5c0a\u91cd\uff0c\u52a0\u5267\u4e86\u5386\u53f2\u4e0d\u516c\uff1b\u73b0\u6709\u6280\u672f\u516c\u5e73\u6027\u6307\u6807\u672a\u80fd\u6355\u6349ASR\u504f\u89c1\u7684\u72ec\u7279\u4f26\u7406\u7ef4\u5ea6\uff1bASR\u5f00\u53d1\u4e2d\u5b58\u5728\u8bed\u8a00\u610f\u8bc6\u5f62\u6001\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u54f2\u5b66\u5206\u6790\u89c6\u89d2\uff0c\u533a\u5206\u4e86\u4e2d\u6027\u5206\u7c7b\u4e0e\u6709\u5bb3\u6b67\u89c6\uff1b\u8bc6\u522b\u4e86\u8bed\u97f3\u6280\u672f\u7684\u4e09\u4e2a\u72ec\u7279\u4f26\u7406\u7ef4\u5ea6\uff08\u65f6\u95f4\u8d1f\u62c5\u3001\u5bf9\u8bdd\u4e2d\u65ad\u3001\u8bed\u97f3\u4e0e\u8eab\u4efd\u8054\u7cfb\uff09\uff1b\u5206\u6790\u4e86ASR\u5f00\u53d1\u4e2d\u8bed\u8a00\u6807\u51c6\u5316\u4e0e\u591a\u5143\u5316\u4e4b\u95f4\u7684\u5f20\u529b\u3002", "result": "ASR\u7cfb\u7edf\u5bf9\u975e\u6807\u51c6\u65b9\u8a00\u7684\u6301\u7eed\u8bef\u8bc6\u53ef\u5c06\u4e2d\u6027\u5206\u7c7b\u8f6c\u53d8\u4e3a\u6709\u5bb3\u6b67\u89c6\uff1b\u63ed\u793a\u4e86ASR\u504f\u89c1\u7684\u201c\u65f6\u95f4\u7a0e\u6536\u201d\u3001\u5bf9\u8bdd\u4e2d\u65ad\u3001\u8bed\u97f3\u4e0e\u4e2a\u4eba/\u6587\u5316\u8ba4\u540c\u8054\u7cfb\u7b49\u72ec\u7279\u4f26\u7406\u7ef4\u5ea6\uff1b\u8fd9\u4e9b\u56e0\u7d20\u5bfc\u81f4\u7684\u4e0d\u5bf9\u79f0\u6743\u529b\u5173\u7cfb\u672a\u88ab\u73b0\u6709\u6280\u672f\u516c\u5e73\u6027\u6307\u6807\u6355\u83b7\uff1b\u5f53\u524d\u7684ASR\u65b9\u6cd5\u5e38\u56fa\u5316\u5e76\u5f3a\u5316\u6709\u95ee\u9898\u7684\u8bed\u8a00\u610f\u8bc6\u5f62\u6001\u3002", "conclusion": "\u89e3\u51b3ASR\u504f\u89c1\u9700\u8d85\u8d8a\u6280\u672f\u5e72\u9884\uff0c\u8981\u6c42\u627f\u8ba4\u548c\u63a5\u7eb3\u591a\u6837\u5316\u7684\u8bed\u97f3\u53d8\u4f53\u4f5c\u4e3a\u5408\u6cd5\u7684\u8868\u8fbe\u5f62\u5f0f\uff1b\u8fd9\u79cd\u54f2\u5b66\u91cd\u6784\u4e3a\u5f00\u53d1\u5c0a\u91cd\u8bed\u8a00\u591a\u6837\u6027\u548c\u8bf4\u8bdd\u8005\u81ea\u4e3b\u6743\u7684ASR\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.06784", "pdf": "https://arxiv.org/pdf/2508.06784", "abs": "https://arxiv.org/abs/2508.06784", "authors": ["Junjing Zheng", "Chengliang Song", "Weidong Jiang", "Xinyu Zhang"], "title": "Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "High-dimensional data, particularly in the form of high-order tensors,\npresents a major challenge in self-supervised learning. While MLP-based\nautoencoders (AE) are commonly employed, their dependence on flattening\noperations exacerbates the curse of dimensionality, leading to excessively\nlarge model sizes, high computational overhead, and challenging optimization\nfor deep structural feature capture. Although existing tensor networks\nalleviate computational burdens through tensor decomposition techniques, most\nexhibit limited capability in learning non-linear relationships. To overcome\nthese limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder\n(MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linear\nframework and employs a Pick-and-Unfold strategy, facilitating flexible\nper-mode encoding of high-order tensors via recursive unfold-encode-fold\noperations, effectively integrating tensor structural priors. Notably, MA-NTAE\nexhibits linear growth in computational complexity with tensor order and\nproportional growth with mode dimensions. Extensive experiments demonstrate\nMA-NTAE's performance advantages over standard AE and current tensor networks\nin compression and clustering tasks, which become increasingly pronounced for\nhigher-order, higher-dimensional tensors.", "AI": {"tldr": "\u63d0\u51faMA-NTAE\uff0c\u4e00\u79cd\u975e\u7ebf\u6027Tucker\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u6a21\u5f0f\u611f\u77e5\u548c\u9012\u5f52\u64cd\u4f5c\u6709\u6548\u5904\u7406\u9ad8\u7ef4\u9ad8\u9636\u5f20\u91cf\uff0c\u5728\u538b\u7f29\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u3002", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\uff0c\u9ad8\u7ef4\u9ad8\u9636\u5f20\u91cf\u5904\u7406\u9762\u4e34\u6311\u6218\uff1a\u57fa\u4e8eMLP\u7684\u81ea\u7f16\u7801\u5668\u53d7\u7ef4\u5ea6\u707e\u96be\u5f71\u54cd\uff0c\u5bfc\u81f4\u6a21\u578b\u5927\u3001\u8ba1\u7b97\u9ad8\u3001\u4f18\u5316\u96be\uff1b\u73b0\u6709\u5f20\u91cf\u7f51\u7edc\u867d\u51cf\u8f7b\u8ba1\u7b97\u8d1f\u62c5\u4f46\u96be\u4ee5\u6355\u6349\u975e\u7ebf\u6027\u5173\u7cfb\u3002", "method": "\u5f15\u5165\u6a21\u5f0f\u611f\u77e5\u975e\u7ebf\u6027Tucker\u81ea\u7f16\u7801\u5668\uff08MA-NTAE\uff09\u3002\u5b83\u5c06\u7ecf\u5178Tucker\u5206\u89e3\u63a8\u5e7f\u5230\u975e\u7ebf\u6027\u6846\u67b6\uff0c\u91c7\u7528\u201cPick-and-Unfold\u201d\u7b56\u7565\uff0c\u901a\u8fc7\u9012\u5f52\u7684\u5c55\u5f00-\u7f16\u7801-\u6298\u53e0\u64cd\u4f5c\u5b9e\u73b0\u7075\u6d3b\u7684\u9010\u6a21\u5f0f\u7f16\u7801\uff0c\u6709\u6548\u6574\u5408\u5f20\u91cf\u7ed3\u6784\u5148\u9a8c\u3002\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u968f\u5f20\u91cf\u9636\u6570\u7ebf\u6027\u589e\u957f\uff0c\u968f\u6a21\u5f0f\u7ef4\u5ea6\u6210\u6bd4\u4f8b\u589e\u957f\u3002", "result": "MA-NTAE\u5728\u538b\u7f29\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u81ea\u7f16\u7801\u5668\u548c\u73b0\u6709\u5f20\u91cf\u7f51\u7edc\u3002\u5bf9\u4e8e\u66f4\u9ad8\u9636\u3001\u66f4\u9ad8\u7ef4\u5ea6\u7684\u5f20\u91cf\uff0c\u5176\u6027\u80fd\u4f18\u52bf\u66f4\u4e3a\u663e\u8457\u3002", "conclusion": "MA-NTAE\u4e3a\u9ad8\u7ef4\u9ad8\u9636\u5f20\u91cf\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u5904\u7406\u5927\u578b\u5f20\u91cf\u65f6\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.07015", "pdf": "https://arxiv.org/pdf/2508.07015", "abs": "https://arxiv.org/abs/2508.07015", "authors": ["Hannes Ihalainen", "Dieter Vandesande", "Andr\u00e9 Schidler", "Jeremias Berg", "Bart Bogaerts", "Matti J\u00e4rvisalo"], "title": "Efficient and Reliable Hitting-Set Computations for the Implicit Hitting Set Approach", "categories": ["cs.AI", "cs.DS"], "comment": null, "summary": "The implicit hitting set (IHS) approach offers a general framework for\nsolving computationally hard combinatorial optimization problems declaratively.\nIHS iterates between a decision oracle used for extracting sources of\ninconsistency and an optimizer for computing so-called hitting sets (HSs) over\nthe accumulated sources of inconsistency. While the decision oracle is\nlanguage-specific, the optimizers is usually instantiated through integer\nprogramming.\n  We explore alternative algorithmic techniques for hitting set optimization\nbased on different ways of employing pseudo-Boolean (PB) reasoning as well as\nstochastic local search. We extensively evaluate the practical feasibility of\nthe alternatives in particular in the context of pseudo-Boolean (0-1 IP)\noptimization as one of the most recent instantiations of IHS. Highlighting a\ntrade-off between efficiency and reliability, while a commercial IP solver\nturns out to remain the most effective way to instantiate HS computations, it\ncan cause correctness issues due to numerical instability; in fact, we show\nthat exact HS computations instantiated via PB reasoning can be made\ncompetitive with a numerically exact IP solver. Furthermore, the use of PB\nreasoning as a basis for HS computations allows for obtaining certificates for\nthe correctness of IHS computations, generally applicable to any IHS\ninstantiation in which reasoning in the declarative language at hand can be\ncaptured in the PB-based proof format we employ.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u9690\u5f0f\u547d\u4e2d\u96c6(IHS)\u6846\u67b6\u4e2d\u547d\u4e2d\u96c6\u4f18\u5316\u7684\u66ff\u4ee3\u7b97\u6cd5\uff0c\u53d1\u73b0\u57fa\u4e8e\u4f2a\u5e03\u5c14(PB)\u63a8\u7406\u7684\u7cbe\u786e\u8ba1\u7b97\u80fd\u4e0e\u4f20\u7edfIP\u6c42\u89e3\u5668\u5ab2\u7f8e\uff0c\u5e76\u80fd\u63d0\u4f9b\u6b63\u786e\u6027\u8bc1\u660e\uff0c\u89e3\u51b3\u4e86IP\u53ef\u80fd\u5b58\u5728\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u9690\u5f0f\u547d\u4e2d\u96c6(IHS)\u65b9\u6cd5\u4e2d\uff0c\u547d\u4e2d\u96c6(HS)\u4f18\u5316\u5668\u901a\u5e38\u91c7\u7528\u6574\u6570\u89c4\u5212(IP)\u5b9e\u73b0\u3002\u7136\u800c\uff0cIP\u6c42\u89e3\u5668\u53ef\u80fd\u56e0\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u800c\u5bfc\u81f4\u8ba1\u7b97\u6b63\u786e\u6027\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u63a2\u7d22\u5e76\u8bc4\u4f30\u66f4\u53ef\u9760\u3001\u80fd\u786e\u4fdd\u6b63\u786e\u6027\u7684HS\u4f18\u5316\u66ff\u4ee3\u6280\u672f\u3002", "method": "\u7814\u7a76\u63a2\u7d22\u4e86\u57fa\u4e8e\u4f2a\u5e03\u5c14(PB)\u63a8\u7406\u548c\u968f\u673a\u5c40\u90e8\u641c\u7d22\u7684\u547d\u4e2d\u96c6(HS)\u4f18\u5316\u7b97\u6cd5\u3002\u901a\u8fc7\u5728\u4f2a\u5e03\u5c14(0-1 IP)\u4f18\u5316\u80cc\u666f\u4e0b\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5206\u6790\u4e86\u8fd9\u4e9b\u66ff\u4ee3\u65b9\u6cd5\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002", "result": "\u5546\u4e1aIP\u6c42\u89e3\u5668\u5728HS\u8ba1\u7b97\u4e2d\u4ecd\u6700\u6709\u6548\uff0c\u4f46\u53ef\u80fd\u5b58\u5728\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u5bfc\u81f4\u7684\u6b63\u786e\u6027\u95ee\u9898\u3002\u57fa\u4e8e\u4f2a\u5e03\u5c14(PB)\u63a8\u7406\u7684\u7cbe\u786eHS\u8ba1\u7b97\u80fd\u591f\u4e0e\u6570\u503c\u7cbe\u786e\u7684IP\u6c42\u89e3\u5668\u7ade\u4e89\u3002\u6b64\u5916\uff0cPB\u63a8\u7406\u80fd\u4e3aIHS\u8ba1\u7b97\u63d0\u4f9b\u6b63\u786e\u6027\u8bc1\u660e\u3002", "conclusion": "\u5c3d\u7ba1\u5546\u4e1aIP\u6c42\u89e3\u5668\u6548\u7387\u8f83\u9ad8\uff0c\u4f46\u57fa\u4e8e\u4f2a\u5e03\u5c14(PB)\u63a8\u7406\u7684\u547d\u4e2d\u96c6(HS)\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u7ade\u4e89\u529b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5b83\u4e0d\u4ec5\u80fd\u5728\u6027\u80fd\u4e0a\u4e0eIP\u6c42\u89e3\u5668\u5339\u654c\uff0c\u8fd8\u80fd\u901a\u8fc7\u63d0\u4f9b\u6b63\u786e\u6027\u8bc1\u660e\u6765\u89e3\u51b3\u5176\u6f5c\u5728\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86IHS\u8ba1\u7b97\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.06640", "pdf": "https://arxiv.org/pdf/2508.06640", "abs": "https://arxiv.org/abs/2508.06640", "authors": ["Zheyuan Zhang", "Weihao Tang", "Hong Chen"], "title": "Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors", "categories": ["cs.CV"], "comment": null, "summary": "Micro-expression recognition (MER) is a highly challenging task in affective\ncomputing. With the reduced-sized micro-expression (ME) input that contains key\ninformation based on key-frame indexes, key-frame-based methods have\nsignificantly improved the performance of MER. However, most of these methods\nfocus on improving the performance with relatively accurate key-frame indexes,\nwhile ignoring the difficulty of obtaining accurate key-frame indexes and the\nobjective existence of key-frame index errors, which impedes them from moving\ntowards practical applications. In this paper, we propose CausalNet, a novel\nframework to achieve robust MER facing key-frame index errors while maintaining\naccurate recognition. To enhance robustness, CausalNet takes the representation\nof the entire ME sequence as the input. To address the information redundancy\nbrought by the complete ME range input and maintain accurate recognition,\nfirst, the Causal Motion Position Learning Module (CMPLM) is proposed to help\nthe model locate the muscle movement areas related to Action Units (AUs),\nthereby reducing the attention to other redundant areas. Second, the Causal\nAttention Block (CAB) is proposed to deeply learn the causal relationships\nbetween the muscle contraction and relaxation movements in MEs. Empirical\nexperiments have demonstrated that on popular ME benchmarks, the CausalNet has\nachieved robust MER under different levels of key-frame index noise. Meanwhile,\nit has surpassed state-of-the-art (SOTA) methods on several standard MER\nbenchmarks when using the provided annotated key-frames. Code is available at\nhttps://github.com/tony19980810/CausalNet.", "AI": {"tldr": "\u5fae\u8868\u60c5\u8bc6\u522b\u4e2d\uff0c\u73b0\u6709\u57fa\u4e8e\u5173\u952e\u5e27\u7684\u65b9\u6cd5\u53d7\u5173\u952e\u5e27\u7d22\u5f15\u8bef\u5dee\u5f71\u54cd\u5927\u3002\u672c\u6587\u63d0\u51faCausalNet\u6846\u67b6\uff0c\u901a\u8fc7\u5904\u7406\u5b8c\u6574\u5fae\u8868\u60c5\u5e8f\u5217\u5e76\u5f15\u5165CMPLM\u548cCAB\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u5728\u5173\u952e\u5e27\u7d22\u5f15\u5b58\u5728\u8bef\u5dee\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u8bc6\u522b\uff0c\u5e76\u5728\u51c6\u786e\u5173\u952e\u5e27\u4e0b\u8d85\u8d8a\u4e86SOTA\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5173\u952e\u5e27\u7684\u5fae\u8868\u60c5\u8bc6\u522b\u65b9\u6cd5\u8fc7\u5206\u4f9d\u8d56\u7cbe\u786e\u7684\u5173\u952e\u5e27\u7d22\u5f15\uff0c\u5ffd\u89c6\u4e86\u5173\u952e\u5e27\u83b7\u53d6\u96be\u5ea6\u53ca\u5ba2\u89c2\u5b58\u5728\u7684\u7d22\u5f15\u8bef\u5dee\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u63a8\u5e7f\u3002", "method": "\u672c\u6587\u63d0\u51faCausalNet\u6846\u67b6\u4ee5\u5b9e\u73b0\u9c81\u68d2\u7684\u5fae\u8868\u60c5\u8bc6\u522b\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1. \u4ee5\u5b8c\u6574\u7684\u5fae\u8868\u60c5\u5e8f\u5217\u4f5c\u4e3a\u8f93\u5165\uff0c\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u30022. \u5f15\u5165\u56e0\u679c\u8fd0\u52a8\u4f4d\u7f6e\u5b66\u4e60\u6a21\u5757\uff08CMPLM\uff09\uff0c\u5e2e\u52a9\u6a21\u578b\u5b9a\u4f4d\u4e0e\u52a8\u4f5c\u5355\u5143\uff08AUs\uff09\u76f8\u5173\u7684\u808c\u8089\u8fd0\u52a8\u533a\u57df\uff0c\u4ece\u800c\u51cf\u5c11\u5bf9\u5197\u4f59\u533a\u57df\u7684\u5173\u6ce8\u30023. \u8bbe\u8ba1\u56e0\u679c\u6ce8\u610f\u529b\u5757\uff08CAB\uff09\uff0c\u4ee5\u6df1\u5165\u5b66\u4e60\u5fae\u8868\u60c5\u4e2d\u808c\u8089\u6536\u7f29\u4e0e\u653e\u677e\u8fd0\u52a8\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6d41\u884c\u7684\u5fae\u8868\u60c5\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cCausalNet\u5728\u4e0d\u540c\u7a0b\u5ea6\u7684\u5173\u952e\u5e27\u7d22\u5f15\u566a\u58f0\u4e0b\u5747\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u5fae\u8868\u60c5\u8bc6\u522b\u3002\u540c\u65f6\uff0c\u5728\u4f7f\u7528\u63d0\u4f9b\u7684\u5e26\u6807\u6ce8\u5173\u952e\u5e27\u65f6\uff0cCausalNet\u5728\u591a\u4e2a\u6807\u51c6\u5fae\u8868\u60c5\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\uff08SOTA\uff09\u65b9\u6cd5\u3002", "conclusion": "CausalNet\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5fae\u8868\u60c5\u8bc6\u522b\u4e2d\u5173\u952e\u5e27\u7d22\u5f15\u8bef\u5dee\u5bfc\u81f4\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5e76\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u5b9e\u7528\u6027\u3002\u5b83\u80fd\u591f\u5728\u5b58\u5728\u5173\u952e\u5e27\u7d22\u5f15\u566a\u58f0\u65f6\u8fdb\u884c\u9c81\u68d2\u8bc6\u522b\uff0c\u540c\u65f6\u5728\u51c6\u786e\u5173\u952e\u5e27\u4e0b\u8fbe\u5230\u6216\u8d85\u8d8aSOTA\u6c34\u5e73\u3002"}}
{"id": "2508.07172", "pdf": "https://arxiv.org/pdf/2508.07172", "abs": "https://arxiv.org/abs/2508.07172", "authors": ["Biao Yi", "Jiahao Li", "Baolei Zhang", "Lihai Nie", "Tong Li", "Tiansheng Huang", "Zheli Liu"], "title": "Gradient Surgery for Safe LLM Fine-Tuning", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning-as-a-Service introduces a critical vulnerability where a few\nmalicious examples mixed into the user's fine-tuning dataset can compromise the\nsafety alignment of Large Language Models (LLMs). While a recognized paradigm\nframes safe fine-tuning as a multi-objective optimization problem balancing\nuser task performance with safety alignment, we find existing solutions are\ncritically sensitive to the harmful ratio, with defenses degrading sharply as\nharmful ratio increases. We diagnose that this failure stems from conflicting\ngradients, where the user-task update directly undermines the safety objective.\nTo resolve this, we propose SafeGrad, a novel method that employs gradient\nsurgery. When a conflict is detected, SafeGrad nullifies the harmful component\nof the user-task gradient by projecting it onto the orthogonal plane of the\nalignment gradient, allowing the model to learn the user's task without\nsacrificing safety. To further enhance robustness and data efficiency, we\nemploy a KL-divergence alignment loss that learns the rich, distributional\nsafety profile of the well-aligned foundation model. Extensive experiments show\nthat SafeGrad provides state-of-the-art defense across various LLMs and\ndatasets, maintaining robust safety even at high harmful ratios without\ncompromising task fidelity.", "AI": {"tldr": "\u63d0\u51faSafeGrad\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u624b\u672f\u89e3\u51b3\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65f6\uff0c\u6076\u610f\u6570\u636e\u5bfc\u81f4\u7684LLM\u5b89\u5168\u5bf9\u9f50\u5931\u6548\u95ee\u9898\uff0c\u5728\u9ad8\u6709\u5bb3\u6570\u636e\u6bd4\u4f8b\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9886\u5148\u7684\u5b89\u5168\u6027\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5fae\u8c03\u5373\u670d\u52a1\uff08Fine-tuning-as-a-Service\uff09\u5f15\u5165\u4e86\u5173\u952e\u6f0f\u6d1e\uff0c\u5c11\u91cf\u6076\u610f\u6570\u636e\u5373\u53ef\u635f\u5bb3LLMs\u7684\u5b89\u5168\u5bf9\u9f50\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u6076\u610f\u6570\u636e\u6bd4\u4f8b\u589e\u52a0\u65f6\u9632\u5fa1\u80fd\u529b\u6025\u5267\u4e0b\u964d\uff0c\u5176\u5931\u8d25\u6839\u6e90\u5728\u4e8e\u7528\u6237\u4efb\u52a1\u66f4\u65b0\u4e0e\u5b89\u5168\u76ee\u6807\u4e4b\u95f4\u7684\u68af\u5ea6\u51b2\u7a81\u3002", "method": "\u63d0\u51faSafeGrad\u65b9\u6cd5\uff0c\u5229\u7528\u68af\u5ea6\u624b\u672f\u89e3\u51b3\u51b2\u7a81\u3002\u5f53\u68c0\u6d4b\u5230\u51b2\u7a81\u65f6\uff0c\u5c06\u7528\u6237\u4efb\u52a1\u68af\u5ea6\u6295\u5f71\u5230\u5bf9\u9f50\u68af\u5ea6\u7684\u6b63\u4ea4\u5e73\u9762\u4e0a\uff0c\u4ee5\u6d88\u9664\u6709\u5bb3\u6210\u5206\uff0c\u4f7f\u6a21\u578b\u5728\u5b66\u4e60\u7528\u6237\u4efb\u52a1\u7684\u540c\u65f6\u4e0d\u727a\u7272\u5b89\u5168\u6027\u3002\u6b64\u5916\uff0c\u91c7\u7528KL\u6563\u5ea6\u5bf9\u9f50\u635f\u5931\uff0c\u5b66\u4e60\u57fa\u7840\u6a21\u578b\u4e30\u5bcc\u7684\u5206\u5e03\u5f0f\u5b89\u5168\u7279\u5f81\uff0c\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "result": "SafeGrad\u5728\u5404\u79cdLLM\u548c\u6570\u636e\u96c6\u4e0a\u5747\u63d0\u4f9b\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u9632\u5fa1\u80fd\u529b\u3002\u5373\u4f7f\u5728\u6709\u5bb3\u6570\u636e\u6bd4\u4f8b\u5f88\u9ad8\u7684\u60c5\u51b5\u4e0b\uff0c\u5b83\u4e5f\u80fd\u4fdd\u6301\u5f3a\u5927\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff08\u4fdd\u771f\u5ea6\uff09\u3002", "conclusion": "SafeGrad\u901a\u8fc7\u6709\u6548\u89e3\u51b3\u68af\u5ea6\u51b2\u7a81\uff0c\u4e3aLLM\u5728\u6076\u610f\u6570\u636e\u5b58\u5728\u4e0b\u7684\u5b89\u5168\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u5b89\u5168\u6027\u7684\u5e73\u8861\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.06800", "pdf": "https://arxiv.org/pdf/2508.06800", "abs": "https://arxiv.org/abs/2508.06800", "authors": ["Rui Liu", "Haolin Zuo", "Zheng Lian", "Hongyu Yuan", "Qi Fan"], "title": "Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Missing modalities have recently emerged as a critical research direction in\nmultimodal emotion recognition (MER). Conventional approaches typically address\nthis issue through missing modality reconstruction. However, these methods fail\nto account for variations in reconstruction difficulty across different\nsamples, consequently limiting the model's ability to handle hard samples\neffectively. To overcome this limitation, we propose a novel Hardness-Aware\nDynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates\nin two key stages: first, it estimates the hardness level of each sample, and\nsecond, it strategically emphasizes hard samples during training to enhance\nmodel performance on these challenging instances. Specifically, we first\nintroduce a Multi-view Hardness Evaluation mechanism that quantifies\nreconstruction difficulty by considering both Direct Hardness (modality\nreconstruction errors) and Indirect Hardness (cross-modal mutual information).\nMeanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy\nthat dynamically adjusts the training curriculum by retrieving samples with\nsimilar semantic information and balancing the learning focus between easy and\nhard instances. Extensive experiments on benchmark datasets demonstrate that\nHARDY-MER consistently outperforms existing methods in missing-modality\nscenarios. Our code will be made publicly available at\nhttps://github.com/HARDY-MER/HARDY-MER.", "AI": {"tldr": "\u9488\u5bf9\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b(MER)\u4e2d\u7f3a\u5931\u6a21\u6001\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u201c\u96be\u6837\u672c\u201d\u65f6\u7684\u5c40\u9650\u6027\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHARDY-MER\u7684\u201c\u96be\u5ea6\u611f\u77e5\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u201d\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u89c6\u89d2\u96be\u5ea6\u8bc4\u4f30\u548c\u57fa\u4e8e\u68c0\u7d22\u7684\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7f3a\u5931\u6a21\u6001\u60c5\u666f\u4e0b\u5bf9\u56f0\u96be\u6837\u672c\u7684\u5904\u7406\u80fd\u529b\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\uff08MER\uff09\u4e2d\u5904\u7406\u7f3a\u5931\u6a21\u6001\u7684\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u6a21\u6001\u91cd\u5efa\uff09\u672a\u80fd\u5145\u5206\u8003\u8651\u4e0d\u540c\u6837\u672c\u91cd\u5efa\u96be\u5ea6\u7684\u5dee\u5f02\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u5904\u7406\u201c\u96be\u6837\u672c\u201d\u65f6\u6027\u80fd\u53d7\u9650\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86HARDY-MER\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u9636\u6bb5\uff1a1. **\u96be\u5ea6\u4f30\u8ba1**\uff1a\u5f15\u5165\u591a\u89c6\u89d2\u96be\u5ea6\u8bc4\u4f30\u673a\u5236\uff0c\u901a\u8fc7\u76f4\u63a5\u96be\u5ea6\uff08\u6a21\u6001\u91cd\u5efa\u8bef\u5dee\uff09\u548c\u95f4\u63a5\u96be\u5ea6\uff08\u8de8\u6a21\u6001\u4e92\u4fe1\u606f\uff09\u91cf\u5316\u6837\u672c\u7684\u91cd\u5efa\u96be\u5ea6\u30022. **\u52a8\u6001\u5b66\u4e60**\uff1a\u5f15\u5165\u57fa\u4e8e\u68c0\u7d22\u7684\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u68c0\u7d22\u5177\u6709\u76f8\u4f3c\u8bed\u4e49\u4fe1\u606f\u7684\u6837\u672c\uff0c\u5e76\u5e73\u8861\u5bf9\u7b80\u5355\u6837\u672c\u548c\u56f0\u96be\u6837\u672c\u7684\u5b66\u4e60\u4fa7\u91cd\uff0c\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u4ece\u800c\u5f3a\u8c03\u5bf9\u56f0\u96be\u6837\u672c\u7684\u5b66\u4e60\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHARDY-MER\u5728\u7f3a\u5931\u6a21\u6001\u573a\u666f\u4e0b\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HARDY-MER\u901a\u8fc7\u5176\u65b0\u9896\u7684\u96be\u5ea6\u611f\u77e5\u4e0e\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u673a\u5236\uff0c\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u7f3a\u5931\u6a21\u6001\u60c5\u666f\u4e0b\u5904\u7406\u96be\u6837\u672c\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07022", "pdf": "https://arxiv.org/pdf/2508.07022", "abs": "https://arxiv.org/abs/2508.07022", "authors": ["Shengtao Wen", "Haodong Chen", "Yadong Wang", "Zhongying Pan", "Xiang Chen", "Yu Tian", "Bo Qian", "Dong Liang", "Sheng-Jun Huang"], "title": "MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Under Review", "summary": "Knowledge editing (KE) provides a scalable approach for updating factual\nknowledge in large language models without full retraining. While previous\nstudies have demonstrated effectiveness in general domains and medical QA\ntasks, little attention has been paid to KE in multimodal medical scenarios.\nUnlike text-only settings, medical KE demands integrating updated knowledge\nwith visual reasoning to support safe and interpretable clinical decisions. To\naddress this gap, we propose MultiMedEdit, the first benchmark tailored to\nevaluating KE in clinical multimodal tasks. Our framework spans both\nunderstanding and reasoning task types, defines a three-dimensional metric\nsuite (reliability, generality, and locality), and supports cross-paradigm\ncomparisons across general and domain-specific models. We conduct extensive\nexperiments under single-editing and lifelong-editing settings. Results suggest\nthat current methods struggle with generalization and long-tail reasoning,\nparticularly in complex clinical workflows. We further present an efficiency\nanalysis (e.g., edit latency, memory footprint), revealing practical trade-offs\nin real-world deployment across KE paradigms. Overall, MultiMedEdit not only\nreveals the limitations of current approaches but also provides a solid\nfoundation for developing clinically robust knowledge editing techniques in the\nfuture.", "AI": {"tldr": "\u63d0\u51fa\u4e86MultiMedEdit\uff0c\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u533b\u7597\u77e5\u8bc6\u7f16\u8f91\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\uff08KE\uff09\u7814\u7a76\u5728\u591a\u6a21\u6001\u533b\u7597\u573a\u666f\u4e2d\u5173\u6ce8\u4e0d\u8db3\uff1b\u4e0e\u7eaf\u6587\u672c\u8bbe\u7f6e\u4e0d\u540c\uff0c\u533b\u7597\u9886\u57df\u4e2d\u7684KE\u9700\u8981\u7ed3\u5408\u89c6\u89c9\u63a8\u7406\u6765\u652f\u6301\u5b89\u5168\u548c\u53ef\u89e3\u91ca\u7684\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u63d0\u51faMultiMedEdit\uff0c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u4e34\u5e8a\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u77e5\u8bc6\u7f16\u8f91\u7684\u57fa\u51c6\uff1b\u8be5\u6846\u67b6\u6db5\u76d6\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u5b9a\u4e49\u4e86\u53ef\u9760\u6027\u3001\u901a\u7528\u6027\u548c\u5c40\u90e8\u6027\u4e09\u7ef4\u6307\u6807\uff1b\u5728\u5355\u6b21\u7f16\u8f91\u548c\u7ec8\u8eab\u7f16\u8f91\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5e76\u8fdb\u884c\u4e86\u6548\u7387\u5206\u6790\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u65b9\u6cd5\u5728\u6cdb\u5316\u80fd\u529b\u548c\u957f\u5c3e\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7684\u4e34\u5e8a\u5de5\u4f5c\u6d41\u4e2d\uff1b\u6548\u7387\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540c\u77e5\u8bc6\u7f16\u8f91\u8303\u5f0f\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6743\u8861\u3002", "conclusion": "MultiMedEdit\u4e0d\u4ec5\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e5f\u4e3a\u672a\u6765\u5f00\u53d1\u4e34\u5e8a\u7a33\u5065\u7684\u77e5\u8bc6\u7f16\u8f91\u6280\u672f\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2508.06656", "pdf": "https://arxiv.org/pdf/2508.06656", "abs": "https://arxiv.org/abs/2508.06656", "authors": ["Denis Lukovnikov", "Andreas M\u00fcller", "Erwin Quiring", "Asja Fischer"], "title": "Towards Robust Red-Green Watermarking for Autoregressive Image Generators", "categories": ["cs.CV"], "comment": null, "summary": "In-generation watermarking for detecting and attributing generated content\nhas recently been explored for latent diffusion models (LDMs), demonstrating\nhigh robustness. However, the use of in-generation watermarks in autoregressive\n(AR) image models has not been explored yet. AR models generate images by\nautoregressively predicting a sequence of visual tokens that are then decoded\ninto pixels using a vector-quantized decoder. Inspired by red-green watermarks\nfor large language models, we examine token-level watermarking schemes that\nbias the next-token prediction based on prior tokens. We find that a direct\ntransfer of these schemes works in principle, but the detectability of the\nwatermarks decreases considerably under common image perturbations. As a\nremedy, we propose two novel watermarking methods that rely on visual token\nclustering to assign similar tokens to the same set. Firstly, we investigate a\ntraining-free approach that relies on a cluster lookup table, and secondly, we\nfinetune VAE encoders to predict token clusters directly from perturbed images.\nOverall, our experiments show that cluster-level watermarks improve robustness\nagainst perturbations and regeneration attacks while preserving image quality.\nCluster classification further boosts watermark detectability, outperforming a\nset of baselines. Moreover, our methods offer fast verification runtime,\ncomparable to lightweight post-hoc watermarking methods.", "AI": {"tldr": "\u9488\u5bf9\u81ea\u56de\u5f52\uff08AR\uff09\u56fe\u50cf\u6a21\u578b\u6c34\u5370\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u89c6\u89c9\u4ee4\u724c\u805a\u7c7b\u7684\u65b0\u578b\u6c34\u5370\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5728\u56fe\u50cf\u6270\u52a8\u4e0b\u7684\u68c0\u6d4b\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u5e76\u5b9e\u73b0\u5feb\u901f\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u5f0f\u6c34\u5370\u6280\u672f\u5df2\u5e94\u7528\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDMs\uff09\u5e76\u663e\u793a\u51fa\u9ad8\u9c81\u68d2\u6027\uff0c\u4f46\u5c1a\u672a\u5728\u81ea\u56de\u5f52\uff08AR\uff09\u56fe\u50cf\u6a21\u578b\u4e2d\u8fdb\u884c\u63a2\u7d22\uff0c\u8fd9\u4fc3\u4f7f\u4f5c\u8005\u7814\u7a76AR\u6a21\u578b\u7684\u751f\u6210\u5f0f\u6c34\u5370\u3002", "method": "\u7814\u7a76\u9996\u5148\u63a2\u7d22\u4e86\u501f\u9274\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4ee4\u724c\u7ea7\u6c34\u5370\u65b9\u6848\uff0c\u4f46\u53d1\u73b0\u5176\u5728\u56fe\u50cf\u6270\u52a8\u4e0b\u68c0\u6d4b\u7387\u663e\u8457\u4e0b\u964d\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u89c6\u89c9\u4ee4\u724c\u805a\u7c7b\u7684\u65b0\u578b\u6c34\u5370\u65b9\u6cd5\uff1a\u4e00\u662f\u4f9d\u8d56\u805a\u7c7b\u67e5\u627e\u8868\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff1b\u4e8c\u662f\u901a\u8fc7\u5fae\u8c03VAE\u7f16\u7801\u5668\u76f4\u63a5\u4ece\u53d7\u6270\u52a8\u56fe\u50cf\u9884\u6d4b\u4ee4\u724c\u805a\u7c7b\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u805a\u7c7b\u7ea7\u6c34\u5370\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u56fe\u50cf\u6270\u52a8\u548c\u518d\u751f\u6210\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u56fe\u50cf\u8d28\u91cf\u3002\u805a\u7c7b\u5206\u7c7b\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6c34\u5370\u68c0\u6d4b\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u9a8c\u8bc1\u8fd0\u884c\u65f6\u6548\u5feb\uff0c\u4e0e\u8f7b\u91cf\u7ea7\u540e\u7f6e\u6c34\u5370\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u89c6\u89c9\u4ee4\u724c\u805a\u7c7b\u7684\u6c34\u5370\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\uff08AR\uff09\u56fe\u50cf\u6a21\u578b\u6c34\u5370\u5728\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5e94\u5bf9\u5e38\u89c1\u56fe\u50cf\u6270\u52a8\u548c\u518d\u751f\u6210\u653b\u51fb\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u517c\u987e\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u9a8c\u8bc1\u6548\u7387\u3002"}}
{"id": "2508.07173", "pdf": "https://arxiv.org/pdf/2508.07173", "abs": "https://arxiv.org/abs/2508.07173", "authors": ["Leyi Pan", "Zheyu Fu", "Yunpeng Zhai", "Shuchang Tao", "Sheng Guan", "Shiyu Huang", "Lingzhe Zhang", "Zhaoyang Liu", "Bolin Ding", "Felix Henry", "Lijie Wen", "Aiwei Liu"], "title": "Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "20 pages, 8 figures, 12 tables", "summary": "The rise of Omni-modal Large Language Models (OLLMs), which integrate visual\nand auditory processing with text, necessitates robust safety evaluations to\nmitigate harmful outputs. However, no dedicated benchmarks currently exist for\nOLLMs, and prior benchmarks designed for other LLMs lack the ability to assess\nsafety performance under audio-visual joint inputs or cross-modal safety\nconsistency. To fill this gap, we introduce Omni-SafetyBench, the first\ncomprehensive parallel benchmark for OLLM safety evaluation, featuring 24\nmodality combinations and variations with 972 samples each, including dedicated\naudio-visual harm cases. Considering OLLMs' comprehension challenges with\ncomplex omni-modal inputs and the need for cross-modal consistency evaluation,\nwe propose tailored metrics: a Safety-score based on conditional Attack Success\nRate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and\na Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency\nacross modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals\ncritical vulnerabilities: (1) no model excels in both overall safety and\nconsistency, with only 3 models achieving over 0.6 in both metrics and top\nperformer scoring around 0.8; (2) safety defenses weaken with complex inputs,\nespecially audio-visual joints; (3) severe weaknesses persist, with some models\nscoring as low as 0.14 on specific modalities. Our benchmark and metrics\nhighlight urgent needs for enhanced OLLM safety, providing a foundation for\nfuture improvements.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5168\u80fd\u8bed\u8a00\u6a21\u578b\uff08OLLMs\uff09\u5b89\u5168\u8bc4\u4f30\u57fa\u51c6\u7684\u7f3a\u5931\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u7efc\u5408\u6027\u5e76\u884c\u57fa\u51c6Omni-SafetyBench\u548c\u5b9a\u5236\u5316\u8bc4\u4f30\u6307\u6807\uff0c\u63ed\u793a\u4e86\u73b0\u6709OLLMs\u5728\u590d\u6742\u591a\u6a21\u6001\u8f93\u5165\u4e0b\u5b58\u5728\u663e\u8457\u7684\u5b89\u5168\u548c\u4e00\u81f4\u6027\u6f0f\u6d1e\u3002", "motivation": "\u968f\u7740\u6574\u5408\u89c6\u89c9\u3001\u542c\u89c9\u548c\u6587\u672c\u5904\u7406\u7684OLLMs\u7684\u5174\u8d77\uff0c\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u5f3a\u5927\u7684\u5b89\u5168\u8bc4\u4f30\u4ee5\u7f13\u89e3\u6709\u5bb3\u8f93\u51fa\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9OLLMs\u7684\u57fa\u51c6\uff0c\u73b0\u6709LLM\u57fa\u51c6\u65e0\u6cd5\u8bc4\u4f30\u97f3\u89c6\u9891\u8054\u5408\u8f93\u5165\u4e0b\u7684\u5b89\u5168\u6027\u80fd\u6216\u8de8\u6a21\u6001\u5b89\u5168\u4e00\u81f4\u6027\u3002", "method": "\u7814\u7a76\u8005\u5f15\u5165\u4e86Omni-SafetyBench\uff0c\u8fd9\u662f\u9996\u4e2a\u5168\u9762\u7684OLLM\u5b89\u5168\u8bc4\u4f30\u5e76\u884c\u57fa\u51c6\uff0c\u5305\u542b24\u79cd\u6a21\u6001\u7ec4\u5408\u548c\u53d8\u4f53\uff0c\u5171972\u4e2a\u6837\u672c\uff0c\u5305\u62ec\u4e13\u95e8\u7684\u97f3\u89c6\u9891\u6709\u5bb3\u6848\u4f8b\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u5b9a\u5236\u5316\u7684\u8bc4\u4f30\u6307\u6807\uff1a\u57fa\u4e8e\u6761\u4ef6\u653b\u51fb\u6210\u529f\u7387\uff08C-ASR\uff09\u548c\u62d2\u7edd\u7387\uff08C-RR\uff09\u7684Safety-score\uff0c\u4ee5\u53ca\u7528\u4e8e\u8861\u91cf\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u7684Cross-Modal Safety Consistency Score\uff08CMSC-score\uff09\u3002", "result": "\u5bf96\u4e2a\u5f00\u6e90\u548c4\u4e2a\u95ed\u6e90OLLMs\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u5173\u952e\u6f0f\u6d1e\uff1a1) \u6ca1\u6709\u6a21\u578b\u5728\u6574\u4f53\u5b89\u5168\u6027\u548c\u4e00\u81f4\u6027\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4ec53\u4e2a\u6a21\u578b\u4e24\u9879\u6307\u6807\u5747\u8d85\u8fc70.6\uff1b2) \u9762\u5bf9\u590d\u6742\u8f93\u5165\uff08\u7279\u522b\u662f\u97f3\u89c6\u9891\u8054\u5408\u8f93\u5165\uff09\uff0c\u5b89\u5168\u9632\u5fa1\u663e\u8457\u51cf\u5f31\uff1b3) \u666e\u904d\u5b58\u5728\u4e25\u91cd\u5f31\u70b9\uff0c\u67d0\u4e9b\u6a21\u578b\u5728\u7279\u5b9a\u6a21\u6001\u4e0a\u5f97\u5206\u4f4e\u81f30.14\u3002", "conclusion": "\u672c\u7814\u7a76\u6240\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u6307\u6807\u51f8\u663e\u4e86\u52a0\u5f3aOLLM\u5b89\u5168\u6027\u7684\u7d27\u8feb\u9700\u6c42\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u6539\u8fdb\u5de5\u4f5c\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06806", "pdf": "https://arxiv.org/pdf/2508.06806", "abs": "https://arxiv.org/abs/2508.06806", "authors": ["Xiao Huang", "Xu Liu", "Enze Zhang", "Tong Yu", "Shuai Li"], "title": "Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation", "categories": ["cs.LG", "cs.AI"], "comment": "ICML2025", "summary": "Offline-to-online Reinforcement Learning (O2O RL) aims to perform online\nfine-tuning on an offline pre-trained policy to minimize costly online\ninteractions. Existing work used offline datasets to generate data that conform\nto the online data distribution for data augmentation. However, generated data\nstill exhibits a gap with the online data, limiting overall performance. To\naddress this, we propose a new data augmentation approach, Classifier-Free\nDiffusion Generation (CFDG). Without introducing additional classifier training\noverhead, CFDG leverages classifier-free guidance diffusion to significantly\nenhance the generation quality of offline and online data with different\ndistributions. Additionally, it employs a reweighting method to enable more\ngenerated data to align with the online data, enhancing performance while\nmaintaining the agent's stability. Experimental results show that CFDG\noutperforms replaying the two data types or using a standard diffusion model to\ngenerate new data. Our method is versatile and can be integrated with existing\noffline-to-online RL algorithms. By implementing CFDG to popular methods IQL,\nPEX and APL, we achieve a notable 15% average improvement in empirical\nperformance on the D4RL benchmark such as MuJoCo and AntMaze.", "AI": {"tldr": "\u9488\u5bf9O2O RL\u4e2d\u6570\u636e\u589e\u5f3a\u5b58\u5728\u7684\u79bb\u7ebf-\u5728\u7ebf\u6570\u636e\u5206\u5e03\u5dee\u8ddd\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u65e0\u5206\u7c7b\u5668\u6269\u6563\u751f\u6210\uff08CFDG\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u751f\u6210\u8d28\u91cf\u548c\u5bf9\u9f50\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709O2O RL\u7b97\u6cd5\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709O2O RL\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u751f\u6210\u7684\u79bb\u7ebf\u6570\u636e\u4e0e\u5728\u7ebf\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u5206\u5e03\u5dee\u5f02\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u7ebf\u5fae\u8c03\u7b56\u7565\u7684\u6574\u4f53\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51faClassifier-Free Diffusion Generation (CFDG)\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002CFDG\u5229\u7528\u65e0\u5206\u7c7b\u5668\u6307\u5bfc\u6269\u6563\u6280\u672f\uff0c\u5728\u4e0d\u5f15\u5165\u989d\u5916\u8bad\u7ec3\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u540c\u5206\u5e03\u79bb\u7ebf\u548c\u5728\u7ebf\u6570\u636e\u7684\u751f\u6210\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u5b83\u91c7\u7528\u91cd\u52a0\u6743\u65b9\u6cd5\u4f7f\u66f4\u591a\u751f\u6210\u6570\u636e\u4e0e\u5728\u7ebf\u6570\u636e\u5bf9\u9f50\uff0c\u65e8\u5728\u63d0\u5347\u6027\u80fd\u5e76\u4fdd\u6301\u667a\u80fd\u4f53\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCFDG\u5728\u6570\u636e\u751f\u6210\u8d28\u91cf\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u76f4\u63a5\u56de\u653e\u6216\u4f7f\u7528\u6807\u51c6\u6269\u6563\u6a21\u578b\u751f\u6210\u6570\u636e\u3002\u901a\u8fc7\u5c06CFDG\u4e0e\u4e3b\u6d41O2O RL\u7b97\u6cd5\uff08\u5982IQL\u3001PEX\u548cAPL\uff09\u96c6\u6210\uff0c\u5728D4RL\u57fa\u51c6\uff08\u5982MuJoCo\u548cAntMaze\uff09\u4e0a\u5b9e\u73b0\u4e86\u5e73\u574715%\u7684\u7ecf\u9a8c\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CFDG\u662f\u4e00\u79cd\u901a\u7528\u4e14\u6709\u6548\u7684O2O RL\u6570\u636e\u589e\u5f3a\u8303\u5f0f\uff0c\u80fd\u591f\u663e\u8457\u5f25\u8865\u79bb\u7ebf\u4e0e\u5728\u7ebf\u6570\u636e\u95f4\u7684\u5206\u5e03\u5dee\u8ddd\uff0c\u5e76\u80fd\u4e0e\u73b0\u6709O2O RL\u7b97\u6cd5\u7ed3\u5408\uff0c\u63d0\u5347\u5176\u5728\u7ebf\u5fae\u8c03\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.07043", "pdf": "https://arxiv.org/pdf/2508.07043", "abs": "https://arxiv.org/abs/2508.07043", "authors": ["Orion Li", "Vinayak Agarwal", "Summer Zhou", "Ashwin Gopinath", "Timothy Kassis"], "title": "K-Dense Analyst: Towards Fully Automated Scientific Analysis", "categories": ["cs.AI", "cs.MA", "q-bio.GN", "q-bio.QM"], "comment": null, "summary": "The complexity of modern bioinformatics analysis has created a critical gap\nbetween data generation and developing scientific insights. While large\nlanguage models (LLMs) have shown promise in scientific reasoning, they remain\nfundamentally limited when dealing with real-world analytical workflows that\ndemand iterative computation, tool integration and rigorous validation. We\nintroduce K-Dense Analyst, a hierarchical multi-agent system that achieves\nautonomous bioinformatics analysis through a dual-loop architecture. K-Dense\nAnalyst, part of the broader K-Dense platform, couples planning with validated\nexecution using specialized agents to decompose complex objectives into\nexecutable, verifiable tasks within secure computational environments. On\nBixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense\nAnalyst achieves 29.2% accuracy, surpassing the best-performing language model\n(GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what\nis widely considered the most powerful LLM available. Remarkably, K-Dense\nAnalyst achieves this performance using Gemini 2.5 Pro, which attains only\n18.3% accuracy when used directly, demonstrating that our architectural\ninnovations unlock capabilities far beyond the underlying model's baseline\nperformance. Our insights demonstrate that autonomous scientific reasoning\nrequires more than enhanced language models, it demands purpose-built systems\nthat can bridge the gap between high-level scientific objectives and low-level\ncomputational execution. These results represent a significant advance toward\nfully autonomous computational biologists capable of accelerating discovery\nacross the life sciences.", "AI": {"tldr": "K-Dense Analyst\u662f\u4e00\u4e2a\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u89c4\u5212\u4e0e\u9a8c\u8bc1\u6267\u884c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u7269\u4fe1\u606f\u5b66\u5206\u6790\u7684\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u751f\u7269\u4fe1\u606f\u5b66\u5206\u6790\u7684\u590d\u6742\u6027\u5bfc\u81f4\u6570\u636e\u751f\u6210\u4e0e\u79d1\u5b66\u6d1e\u5bdf\u4e4b\u95f4\u5b58\u5728\u5de8\u5927\u9e3f\u6c9f\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u79d1\u5b66\u63a8\u7406\u65b9\u9762\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u9700\u8981\u8fed\u4ee3\u8ba1\u7b97\u3001\u5de5\u5177\u96c6\u6210\u548c\u4e25\u683c\u9a8c\u8bc1\u7684\u771f\u5b9e\u5206\u6790\u5de5\u4f5c\u6d41\u4e2d\u4ecd\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002", "method": "\u5f15\u5165K-Dense Analyst\uff0c\u4e00\u4e2a\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u5faa\u73af\u67b6\u6784\u5b9e\u73b0\u81ea\u4e3b\u751f\u7269\u4fe1\u606f\u5b66\u5206\u6790\u3002\u8be5\u7cfb\u7edf\u5229\u7528\u4e13\u4e1a\u4ee3\u7406\u5c06\u590d\u6742\u76ee\u6807\u5206\u89e3\u4e3a\u53ef\u6267\u884c\u3001\u53ef\u9a8c\u8bc1\u7684\u4efb\u52a1\uff0c\u5e76\u5728\u5b89\u5168\u8ba1\u7b97\u73af\u5883\u4e2d\u7ed3\u5408\u89c4\u5212\u4e0e\u9a8c\u8bc1\u6267\u884c\u3002", "result": "\u5728BixBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cK-Dense Analyst\u5b9e\u73b0\u4e8629.2%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u8868\u73b0\u6700\u4f73\u7684\u8bed\u8a00\u6a21\u578b\uff08GPT-5\uff09\u9ad8\u51fa6.3\u4e2a\u767e\u5206\u70b9\uff08\u63d0\u5347\u8fd127%\uff09\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cK-Dense Analyst\u4f7f\u7528\u76f4\u63a5\u8868\u73b0\u4ec5\u4e3a18.3%\u7684Gemini 2.5 Pro\u8fbe\u5230\u4e86\u8fd9\u4e00\u6027\u80fd\uff0c\u8868\u660e\u5176\u67b6\u6784\u521b\u65b0\u6781\u5927\u5730\u8d85\u8d8a\u4e86\u5e95\u5c42\u6a21\u578b\u7684\u57fa\u7ebf\u6027\u80fd\u3002", "conclusion": "\u81ea\u4e3b\u79d1\u5b66\u63a8\u7406\u4e0d\u4ec5\u9700\u8981\u66f4\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u66f4\u9700\u8981\u4e13\u95e8\u6784\u5efa\u7684\u7cfb\u7edf\u6765\u5f25\u5408\u9ad8\u7ea7\u79d1\u5b66\u76ee\u6807\u4e0e\u4f4e\u7ea7\u8ba1\u7b97\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u8fd9\u9879\u7814\u7a76\u662f\u5411\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u8ba1\u7b97\u751f\u7269\u5b66\u5bb6\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u6709\u671b\u52a0\u901f\u751f\u547d\u79d1\u5b66\u9886\u57df\u7684\u53d1\u73b0\u3002"}}
{"id": "2508.06696", "pdf": "https://arxiv.org/pdf/2508.06696", "abs": "https://arxiv.org/abs/2508.06696", "authors": ["Tianqin Li", "George Liu", "Tai Sing Lee"], "title": "Learning More by Seeing Less: Line Drawing Pretraining for Efficient, Transferable, and Human-Aligned Vision", "categories": ["cs.CV"], "comment": null, "summary": "Despite remarkable progress in computer vision, modern recognition systems\nremain limited by their dependence on rich, redundant visual inputs. In\ncontrast, humans can effortlessly understand sparse, minimal representations\nlike line drawings - suggesting that structure, rather than appearance,\nunderlies efficient visual understanding. In this work, we propose using line\ndrawings as a structure-first pretraining modality to induce more compact and\ngeneralizable visual representations. We show that models pretrained on line\ndrawings develop stronger shape bias, more focused attention, and greater data\nefficiency across classification, detection, and segmentation tasks. Notably,\nthese models also exhibit lower intrinsic dimensionality, requiring\nsignificantly fewer principal components to capture representational variance -\nechoing the similar observation in low dimensional efficient representation in\nthe brain. Beyond performance improvements, line drawing pretraining produces\nmore compressible representations, enabling better distillation into\nlightweight student models. Students distilled from line-pretrained teachers\nconsistently outperform those trained from color-supervised teachers,\nhighlighting the benefits of structurally compact knowledge. Finally, we\ndemonstrate that the pretraining with line-drawing can also be extended to\nunsupervised setting via our proposed method \"learning to draw\". Together, our\nresults support the view that structure-first visual learning fosters\nefficiency, generalization, and human-aligned inductive biases - offering a\nsimple yet powerful strategy for building more robust and adaptable vision\nsystems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5229\u7528\u7ebf\u7a3f\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4ee5\u83b7\u53d6\u66f4\u7d27\u51d1\u3001\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u7684\u89c6\u89c9\u8868\u793a\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3001\u6570\u636e\u6548\u7387\u548c\u4fc3\u8fdb\u77e5\u8bc6\u84b8\u998f\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u652f\u6301\u201c\u7ed3\u6784\u4f18\u5148\u201d\u7684\u89c6\u89c9\u5b66\u4e60\u8303\u5f0f\u3002", "motivation": "\u5c3d\u7ba1\u8ba1\u7b97\u673a\u89c6\u89c9\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u4ee3\u8bc6\u522b\u7cfb\u7edf\u4ecd\u53d7\u9650\u4e8e\u5bf9\u4e30\u5bcc\u3001\u5197\u4f59\u89c6\u89c9\u8f93\u5165\u7684\u4f9d\u8d56\u3002\u4e0e\u6b64\u76f8\u53cd\uff0c\u4eba\u7c7b\u80fd\u591f\u8f7b\u677e\u7406\u89e3\u7a00\u758f\u3001\u6781\u7b80\u7684\u8868\u793a\uff08\u5982\u7ebf\u7a3f\uff09\uff0c\u8fd9\u8868\u660e\u7ed3\u6784\u800c\u975e\u5916\u89c2\u662f\u9ad8\u6548\u89c6\u89c9\u7406\u89e3\u7684\u57fa\u7840\u3002\u73b0\u6709\u7cfb\u7edf\u672a\u80fd\u5145\u5206\u5229\u7528\u8fd9\u79cd\u201c\u7ed3\u6784\u4f18\u5148\u201d\u7684\u5b66\u4e60\u65b9\u5f0f\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u5c06\u7ebf\u7a3f\u4f5c\u4e3a\u4e00\u79cd\u201c\u7ed3\u6784\u4f18\u5148\u201d\u7684\u9884\u8bad\u7ec3\u6a21\u5f0f\uff0c\u65e8\u5728\u8bf1\u5bfc\u751f\u6210\u66f4\u7d27\u51d1\u548c\u53ef\u6cdb\u5316\u7684\u89c6\u89c9\u8868\u793a\u3002\u6b64\u5916\uff0c\u8be5\u9884\u8bad\u7ec3\u65b9\u6cd5\u8fd8\u53ef\u4ee5\u901a\u8fc7\u201c\u5b66\u4e60\u7ed8\u753b\u201d\u6269\u5c55\u5230\u65e0\u76d1\u7763\u8bbe\u7f6e\u3002", "result": "\u7ecf\u8fc7\u7ebf\u7a3f\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5f62\u72b6\u504f\u597d\u3001\u66f4\u96c6\u4e2d\u7684\u6ce8\u610f\u529b\u548c\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\u3002\u8fd9\u4e9b\u6a21\u578b\u8fd8\u5177\u6709\u66f4\u4f4e\u7684\u5185\u5728\u7ef4\u5ea6\uff0c\u80fd\u7528\u66f4\u5c11\u7684\u4e3b\u6210\u5206\u6355\u83b7\u8868\u793a\u65b9\u5dee\u3002\u6b64\u5916\uff0c\u7ebf\u7a3f\u9884\u8bad\u7ec3\u80fd\u4ea7\u751f\u66f4\u53ef\u538b\u7f29\u7684\u8868\u793a\uff0c\u4ece\u800c\u66f4\u597d\u5730\u84b8\u998f\u5230\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u4e14\u84b8\u998f\u51fa\u7684\u5b66\u751f\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u7531\u5f69\u8272\u76d1\u7763\u6559\u5e08\u84b8\u998f\u7684\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u201c\u7ed3\u6784\u4f18\u5148\u201d\u7684\u89c6\u89c9\u5b66\u4e60\u80fd\u591f\u63d0\u5347\u89c6\u89c9\u7cfb\u7edf\u7684\u6548\u7387\u3001\u6cdb\u5316\u80fd\u529b\u548c\u4eba\u7c7b\u5bf9\u9f50\u7684\u5f52\u7eb3\u504f\u7f6e\u3002\u901a\u8fc7\u7ebf\u7a3f\u9884\u8bad\u7ec3\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u548c\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u5f3a\u5927\u7684\u7b56\u7565\u3002"}}
{"id": "2508.07178", "pdf": "https://arxiv.org/pdf/2508.07178", "abs": "https://arxiv.org/abs/2508.07178", "authors": ["Kejin Liu", "Junhong Lian", "Xiang Ao", "Ningtao Wang", "Xing Fu", "Yu Cheng", "Weiqiang Wang", "Xinyu Liu"], "title": "Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by the 34th ACM International Conference on Information and\n  Knowledge Management (CIKM '25), Full Research Papers track", "summary": "Accurate personalized headline generation hinges on precisely capturing user\ninterests from historical behaviors. However, existing methods neglect\npersonalized-irrelevant click noise in entire historical clickstreams, which\nmay lead to hallucinated headlines that deviate from genuine user preferences.\nIn this paper, we reveal the detrimental impact of click noise on personalized\ngeneration quality through rigorous analysis in both user and news dimensions.\nBased on these insights, we propose a novel Personalized Headline Generation\nframework via Denoising Fake Interests from Implicit Feedback (PHG-DIF).\nPHG-DIF first employs dual-stage filtering to effectively remove clickstream\nnoise, identified by short dwell times and abnormal click bursts, and then\nleverages multi-level temporal fusion to dynamically model users' evolving and\nmulti-faceted interests for precise profiling. Moreover, we release DT-PENS, a\nnew benchmark dataset comprising the click behavior of 1,000 carefully curated\nusers and nearly 10,000 annotated personalized headlines with historical dwell\ntime annotations. Extensive experiments demonstrate that PHG-DIF substantially\nmitigates the adverse effects of click noise and significantly improves\nheadline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our\nframework implementation and dataset are available at\nhttps://github.com/liukejin-up/PHG-DIF.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51faPHG-DIF\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u53bb\u9664\u7528\u6237\u5386\u53f2\u70b9\u51fb\u884c\u4e3a\u4e2d\u7684\u566a\u58f0\uff0c\u66f4\u51c6\u786e\u5730\u6355\u6349\u7528\u6237\u5174\u8da3\uff0c\u4ece\u800c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e2a\u6027\u5316\u65b0\u95fb\u6807\u9898\uff0c\u5e76\u53d1\u5e03\u4e86\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6DT-PENS\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u6807\u9898\u751f\u6210\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5904\u7406\u7528\u6237\u5386\u53f2\u70b9\u51fb\u6d41\u4e2d\u4e0e\u4e2a\u6027\u5316\u65e0\u5173\u7684\u70b9\u51fb\u566a\u58f0\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u6807\u9898\u53ef\u80fd\u504f\u79bb\u7528\u6237\u771f\u5b9e\u5174\u8da3\uff0c\u4ece\u800c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86PHG-DIF\uff08\u901a\u8fc7\u9690\u5f0f\u53cd\u9988\u53bb\u566a\u865a\u5047\u5174\u8da3\u7684\u4e2a\u6027\u5316\u6807\u9898\u751f\u6210\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u9996\u5148\u91c7\u7528\u53cc\u9636\u6bb5\u8fc7\u6ee4\u6280\u672f\uff0c\u5229\u7528\u77ed\u505c\u7559\u65f6\u95f4\u548c\u5f02\u5e38\u70b9\u51fb\u7206\u53d1\u6765\u6709\u6548\u53bb\u9664\u70b9\u51fb\u6d41\u566a\u58f0\uff1b\u63a5\u7740\uff0c\u5229\u7528\u591a\u7ea7\u65f6\u95f4\u878d\u5408\u6280\u672f\u52a8\u6001\u5efa\u6a21\u7528\u6237\u4e0d\u65ad\u6f14\u53d8\u548c\u591a\u65b9\u9762\u7684\u5174\u8da3\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u5e03\u4e86\u5305\u542b\u505c\u7559\u65f6\u95f4\u6ce8\u91ca\u7684\u65b0\u57fa\u51c6\u6570\u636e\u96c6DT-PENS\u3002", "result": "PHG-DIF\u6846\u67b6\u663e\u8457\u51cf\u8f7b\u4e86\u70b9\u51fb\u566a\u58f0\u7684\u4e0d\u5229\u5f71\u54cd\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6807\u9898\u751f\u6210\u8d28\u91cf\uff0c\u5728DT-PENS\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u6df1\u5165\u5206\u6790\u70b9\u51fb\u566a\u58f0\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u6709\u6548\u7684\u53bb\u566a\u548c\u5174\u8da3\u5efa\u6a21\u65b9\u6cd5\uff0cPHG-DIF\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4e2a\u6027\u5316\u6807\u9898\u751f\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u6781\u5927\u5730\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u8d44\u6e90\u3002"}}
{"id": "2508.06813", "pdf": "https://arxiv.org/pdf/2508.06813", "abs": "https://arxiv.org/abs/2508.06813", "authors": ["Brendan R. Hogan", "Will Brown", "Adel Boyarsky", "Anderson Schneider", "Yuriy Nevmyvaka"], "title": "Technical Report: Full-Stack Fine-Tuning for the Q Programming Language", "categories": ["cs.LG"], "comment": "40 pages", "summary": "Even though large language models are becoming increasingly capable, it is\nstill unreasonable to expect them to excel at tasks that are under-represented\non the Internet. Leveraging LLMs for specialized applications, particularly in\nniche programming languages and private domains, remains challenging and\nlargely unsolved. In this work, we address this gap by presenting a\ncomprehensive, open-source approach for adapting LLMs to the Q programming\nlanguage, a popular tool in quantitative finance that is much less present on\nthe Internet compared to Python, C, Java, and other ``mainstream\" languages and\nis therefore not a strong suit of general-purpose AI models. We introduce a new\nLeetcode style evaluation dataset for Q, benchmark major frontier models on the\ndataset, then do pretraining, supervised fine tuning, and reinforcement\nlearning to train a suite of reasoning and non-reasoning models based on the\nQwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our\nbest model achieves a pass@1 accuracy of 59 percent on our Q benchmark,\nsurpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.\nAdditionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.\nIn addition to releasing models, code, and data, we provide a detailed\nblueprint for dataset construction, model pretraining, supervised fine-tuning,\nand reinforcement learning. Our methodology is broadly applicable, and we\ndiscuss how these techniques can be extended to other tasks, including those\nwhere evaluation may rely on soft or subjective signals.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u5f00\u6e90\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u3001SFT\u548cRL\u5c06LLM\u9002\u914d\u4e8e\u4e92\u8054\u7f51\u4e0a\u6570\u636e\u7a00\u7f3a\u7684\u5c0f\u4f17\u7f16\u7a0b\u8bed\u8a00Q\uff0c\u5e76\u5728\u65b0\u6784\u5efa\u7684Q\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\uff0c\u5176\u6700\u4f73\u6a21\u578b\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u524d\u6cbf\u5927\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u4e0d\u65ad\u63d0\u5347\uff0c\u4f46\u5b83\u4eec\u5728\u4e92\u8054\u7f51\u4e0a\u6570\u636e\u4e0d\u8db3\u7684\u4e13\u4e1a\u9886\u57df\uff08\u5982\u5c0f\u4f17\u7f16\u7a0b\u8bed\u8a00Q\uff09\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5229\u57fa\u7f16\u7a0b\u8bed\u8a00\u548c\u79c1\u6709\u9886\u57df\uff0c\u8fd9\u4e00\u6311\u6218\u5c1a\u672a\u5f97\u5230\u6709\u6548\u89e3\u51b3\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684Q\u8bed\u8a00Leetcode\u98ce\u683c\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8eQwen-2.5\u7cfb\u5217\u6a21\u578b\uff08\u6db5\u76d61.5B\u523032B\u4e94\u79cd\u53c2\u6570\u89c4\u6a21\uff09\uff0c\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u8bad\u7ec3\u4e86\u4e00\u7cfb\u5217\u63a8\u7406\u548c\u975e\u63a8\u7406\u6a21\u578b\u3002\u540c\u65f6\uff0c\u5bf9\u4e3b\u8981\u524d\u6cbf\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u56e2\u961f\u7684\u6700\u4f73\u6a21\u578b\u5728Q\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e8659%\u7684pass@1\u51c6\u786e\u7387\uff0c\u6bd4\u8868\u73b0\u6700\u4f73\u7684\u524d\u6cbf\u6a21\u578bClaude Opus-4\u9ad8\u51fa29.5%\u3002\u6b64\u5916\uff0c\u6240\u6709\u8bad\u7ec3\u7684\u6a21\u578b\uff08\u5305\u62ec\u6700\u5c0f\u76841.5B\u6a21\u578b\uff09\u5728\u8be5\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8eGPT-4.1\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6210\u529f\u5f00\u53d1\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u5c06LLM\u9002\u5e94\u4e8e\u5c0f\u4f17\u7f16\u7a0b\u8bed\u8a00Q\u7684\u7efc\u5408\u6027\u3001\u5f00\u6e90\u65b9\u6cd5\uff08\u5305\u62ec\u6a21\u578b\u3001\u4ee3\u7801\u548c\u6570\u636e\uff09\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u7279\u5b9a\u9886\u57df\u7684\u9002\u5e94\u6027\u8bad\u7ec3\uff0cLLMs\u5728\u6570\u636e\u7a00\u7f3a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u53ef\u663e\u8457\u8d85\u8d8a\u901a\u7528\u6a21\u578b\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u7c7b\u4f3c\u4efb\u52a1\u3002"}}
{"id": "2508.07063", "pdf": "https://arxiv.org/pdf/2508.07063", "abs": "https://arxiv.org/abs/2508.07063", "authors": ["Naseem Machlovi", "Maryam Saleki", "Innocent Ababio", "Ruhul Amin"], "title": "Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach", "categories": ["cs.AI"], "comment": null, "summary": "As AI systems become more integrated into daily life, the need for safer and\nmore reliable moderation has never been greater. Large Language Models (LLMs)\nhave demonstrated remarkable capabilities, surpassing earlier models in\ncomplexity and performance. Their evaluation across diverse tasks has\nconsistently showcased their potential, enabling the development of adaptive\nand personalized agents. However, despite these advancements, LLMs remain prone\nto errors, particularly in areas requiring nuanced moral reasoning. They\nstruggle with detecting implicit hate, offensive language, and gender biases\ndue to the subjective and context-dependent nature of these issues. Moreover,\ntheir reliance on training data can inadvertently reinforce societal biases,\nleading to inconsistencies and ethical concerns in their outputs. To explore\nthe limitations of LLMs in this role, we developed an experimental framework\nbased on state-of-the-art (SOTA) models to assess human emotions and offensive\nbehaviors. The framework introduces a unified benchmark dataset encompassing 49\ndistinct categories spanning the wide spectrum of human emotions, offensive and\nhateful text, and gender and racial biases. Furthermore, we introduced SafePhi,\na QLoRA fine-tuned version of Phi-4, adapting diverse ethical contexts and\noutperforming benchmark moderators by achieving a Macro F1 score of 0.89, where\nOpenAI Moderator and Llama Guard score 0.77 and 0.74, respectively. This\nresearch also highlights the critical domains where LLM moderators consistently\nunderperformed, pressing the need to incorporate more heterogeneous and\nrepresentative data with human-in-the-loop, for better model robustness and\nexplainability.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5185\u5bb9\u5ba1\u6838\u4e2d\u5904\u7406\u590d\u6742\u9053\u5fb7\u63a8\u7406\u548c\u504f\u89c1\u7684\u5c40\u9650\u6027\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u9a8c\u6846\u67b6\u548c\u7edf\u4e00\u57fa\u51c6\u6570\u636e\u96c6\u3002\u901a\u8fc7QLoRA\u5fae\u8c03Phi-4\u6a21\u578b\uff0c\u63d0\u51fa\u4e86SafePhi\uff0c\u5176\u5728\u5ba1\u6838\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u6a21\u578b\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f15\u5165\u591a\u6837\u5316\u6570\u636e\u548c\u4eba\u5de5\u5e72\u9884\u4ee5\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u65e5\u76ca\u878d\u5165\u65e5\u5e38\u751f\u6d3b\uff0c\u5bf9\u66f4\u5b89\u5168\u53ef\u9760\u7684\u5ba1\u6838\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u5176\u5728\u5904\u7406\u9700\u8981\u7ec6\u81f4\u9053\u5fb7\u63a8\u7406\uff08\u5982\u68c0\u6d4b\u9690\u6027\u4ec7\u6068\u3001\u5192\u72af\u6027\u8bed\u8a00\u548c\u6027\u522b\u504f\u89c1\uff09\u65f6\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u95ee\u9898\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u548c\u4e3b\u89c2\u6027\uff0c\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u53ef\u80fd\u65e0\u610f\u4e2d\u5f3a\u5316\u793e\u4f1a\u504f\u89c1\u3002\u56e0\u6b64\uff0c\u63a2\u7d22\u548c\u89e3\u51b3LLMs\u5728\u5185\u5bb9\u5ba1\u6838\u4e2d\u7684\u8fd9\u4e9b\u5c40\u9650\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eSOTA\u6a21\u578b\u7684\u5b9e\u9a8c\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4eba\u7c7b\u60c5\u611f\u548c\u5192\u72af\u884c\u4e3a\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b49\u4e2a\u72ec\u7279\u7c7b\u522b\u7684\u7edf\u4e00\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u4eba\u7c7b\u60c5\u611f\u3001\u5192\u72af\u6027\u53ca\u4ec7\u6068\u6587\u672c\u4ee5\u53ca\u6027\u522b\u548c\u79cd\u65cf\u504f\u89c1\u3002\u6b64\u5916\uff0c\u7814\u7a76\u63d0\u51fa\u4e86SafePhi\uff0c\u4e00\u4e2a\u901a\u8fc7QLoRA\u6280\u672f\u5bf9Phi-4\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u7248\u672c\uff0c\u4f7f\u5176\u80fd\u9002\u5e94\u591a\u6837\u5316\u7684\u9053\u5fb7\u8bed\u5883\u3002", "result": "SafePhi\u5728\u5185\u5bb9\u5ba1\u6838\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0cMacro F1\u5206\u6570\u4e3a0.89\uff0c\u4f18\u4e8e\u57fa\u51c6\u5ba1\u6838\u6a21\u578bOpenAI Moderator\uff080.77\uff09\u548cLlama Guard\uff080.74\uff09\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86LLM\u5ba1\u6838\u5668\u6301\u7eed\u8868\u73b0\u4e0d\u4f73\u7684\u5173\u952e\u9886\u57df\u3002", "conclusion": "\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u5728\u5185\u5bb9\u5ba1\u6838\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u8feb\u5207\u9700\u8981\u7ed3\u5408\u66f4\u591a\u5f02\u6784\u548c\u5177\u6709\u4ee3\u8868\u6027\u7684\u6570\u636e\uff0c\u5e76\u5f15\u5165\u4eba\u5de5\u5e72\u9884\uff08human-in-the-loop\uff09\u673a\u5236\u3002"}}
{"id": "2508.06701", "pdf": "https://arxiv.org/pdf/2508.06701", "abs": "https://arxiv.org/abs/2508.06701", "authors": ["Md Rezwanul Haque", "Md. Milon Islam", "S M Taslim Uddin Raju", "Hamdi Altaheri", "Lobna Nassar", "Fakhri Karray"], "title": "MMFformer: Multimodal Fusion Transformer Network for Depression Detection", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted for the 2025 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC), Vienna, Austria", "summary": "Depression is a serious mental health illness that significantly affects an\nindividual's well-being and quality of life, making early detection crucial for\nadequate care and treatment. Detecting depression is often difficult, as it is\nbased primarily on subjective evaluations during clinical interviews. Hence,\nthe early diagnosis of depression, thanks to the content of social networks,\nhas become a prominent research area. The extensive and diverse nature of\nuser-generated information poses a significant challenge, limiting the accurate\nextraction of relevant temporal information and the effective fusion of data\nacross multiple modalities. This paper introduces MMFformer, a multimodal\ndepression detection network designed to retrieve depressive spatio-temporal\nhigh-level patterns from multimodal social media information. The transformer\nnetwork with residual connections captures spatial features from videos, and a\ntransformer encoder is exploited to design important temporal dynamics in\naudio. Moreover, the fusion architecture fused the extracted features through\nlate and intermediate fusion strategies to find out the most relevant\nintermodal correlations among them. Finally, the proposed network is assessed\non two large-scale depression detection datasets, and the results clearly\nreveal that it surpasses existing state-of-the-art approaches, improving the\nF1-Score by 13.92% for D-Vlog dataset and 7.74% for LMVD dataset. The code is\nmade available publicly at\nhttps://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection.", "AI": {"tldr": "\u63d0\u51faMMFformer\uff0c\u4e00\u4e2a\u591a\u6a21\u6001\u7f51\u7edc\uff0c\u901a\u8fc7\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u97f3\u89c6\u9891\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u6291\u90c1\u75c7\u65e9\u671f\u68c0\u6d4b\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6291\u90c1\u75c7\u65e9\u671f\u8bca\u65ad\u81f3\u5173\u91cd\u8981\u4f46\u4f20\u7edf\u65b9\u6cd5\u4e3b\u89c2\u4e14\u56f0\u96be\u3002\u793e\u4ea4\u7f51\u7edc\u6570\u636e\u63d0\u4f9b\u65b0\u9014\u5f84\uff0c\u4f46\u5176\u591a\u6a21\u6001\u4fe1\u606f\u7684\u51c6\u786e\u63d0\u53d6\u548c\u6709\u6548\u878d\u5408\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u672c\u6587\u5f15\u5165MMFformer\uff0c\u4e00\u4e2a\u591a\u6a21\u6001\u6291\u90c1\u75c7\u68c0\u6d4b\u7f51\u7edc\u3002\u5b83\u5229\u7528\u5e26\u6b8b\u5dee\u8fde\u63a5\u7684Transformer\u4ece\u89c6\u9891\u4e2d\u6355\u83b7\u7a7a\u95f4\u7279\u5f81\uff0c\u5e76\u4f7f\u7528Transformer\u7f16\u7801\u5668\u5904\u7406\u97f3\u9891\u4e2d\u7684\u91cd\u8981\u65f6\u95f4\u52a8\u6001\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u540e\u671f\u548c\u4e2d\u95f4\u878d\u5408\u7b56\u7565\u878d\u5408\u63d0\u53d6\u7684\u7279\u5f81\uff0c\u4ee5\u53d1\u73b0\u6a21\u6001\u95f4\u5173\u8054\uff0c\u4ece\u800c\u8bc6\u522b\u6291\u90c1\u75c7\u65f6\u7a7a\u6a21\u5f0f\u3002", "result": "MMFformer\u5728D-Vlog\u548cLMVD\u4e24\u4e2a\u5927\u578b\u6291\u90c1\u75c7\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0cF1-Score\u5206\u522b\u63d0\u9ad8\u4e8613.92%\u548c7.74%\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "MMFformer\u901a\u8fc7\u6709\u6548\u6574\u5408\u793e\u4ea4\u5a92\u4f53\u7684\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6291\u90c1\u75c7\u7684\u65e9\u671f\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u4e3a\u514b\u670d\u4f20\u7edf\u8bca\u65ad\u5c40\u9650\u548c\u6570\u636e\u878d\u5408\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07179", "pdf": "https://arxiv.org/pdf/2508.07179", "abs": "https://arxiv.org/abs/2508.07179", "authors": ["Jiaqi Yin", "Yi-Wei Chen", "Meng-Lung Lee", "Xiya Liu"], "title": "Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Enterprise data pipelines, characterized by complex transformations across\nmultiple programming languages, often cause a semantic disconnect between\noriginal metadata and downstream data. This \"semantic drift\" compromises data\nreproducibility and governance, and impairs the utility of services like\nretrieval-augmented generation (RAG) and text-to-SQL systems. To address this,\na novel framework is proposed for the automated extraction of fine-grained\nschema lineage from multilingual enterprise pipeline scripts. This method\nidentifies four key components: source schemas, source tables, transformation\nlogic, and aggregation operations, creating a standardized representation of\ndata transformations. For the rigorous evaluation of lineage quality, this\npaper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that\nassesses both structural correctness and semantic fidelity. A new benchmark is\nalso presented, comprising 1,700 manually annotated lineages from real-world\nindustrial scripts. Experiments were conducted with 12 language models, from\n1.3B to 32B small language models (SLMs) to large language models (LLMs) like\nGPT-4o and GPT-4.1. The results demonstrate that the performance of schema\nlineage extraction scales with model size and the sophistication of prompting\ntechniques. Specially, a 32B open-source model, using a single reasoning trace,\ncan achieve performance comparable to the GPT series under standard prompting.\nThis finding suggests a scalable and economical approach for deploying\nschema-aware agents in practical applications.", "AI": {"tldr": "\u9488\u5bf9\u4f01\u4e1a\u6570\u636e\u7ba1\u9053\u4e2d\u56e0\u8bed\u4e49\u6f02\u79fb\u5bfc\u81f4\u7684\u6570\u636e\u53ef\u590d\u7528\u6027\u4e0e\u6cbb\u7406\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u591a\u8bed\u8a00\u811a\u672c\u4e2d\u81ea\u52a8\u63d0\u53d6\u7ec6\u7c92\u5ea6\u6a21\u5f0f\u8840\u7f18\u7684\u6846\u67b6\u3002\u7814\u7a76\u5f15\u5165\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u51c6\uff0c\u5e76\u5b9e\u9a8c\u8bc1\u660e\uff0c\u901a\u8fc7\u4f18\u5316\u63d0\u793a\uff0c\u5f00\u6e90\u5c0f\u6a21\u578b\u53ef\u5b9e\u73b0\u4e0e\u5927\u578b\u4e13\u6709\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e3a\u90e8\u7f72\u6a21\u5f0f\u611f\u77e5\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u9014\u5f84\u3002", "motivation": "\u4f01\u4e1a\u6570\u636e\u7ba1\u9053\u4e2d\u590d\u6742\u7684\u8de8\u8bed\u8a00\u8f6c\u6362\u5e38\u5e38\u5bfc\u81f4\u539f\u59cb\u5143\u6570\u636e\u4e0e\u4e0b\u6e38\u6570\u636e\u4e4b\u95f4\u7684\u201c\u8bed\u4e49\u6f02\u79fb\u201d\uff0c\u8fd9\u635f\u5bb3\u4e86\u6570\u636e\u7684\u53ef\u590d\u7528\u6027\u3001\u6cbb\u7406\u80fd\u529b\uff0c\u5e76\u964d\u4f4e\u4e86RAG\u548ctext-to-SQL\u7b49\u670d\u52a1\u7684\u5b9e\u7528\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u591a\u8bed\u8a00\u4f01\u4e1a\u7ba1\u9053\u811a\u672c\u4e2d\u63d0\u53d6\u7ec6\u7c92\u5ea6\u6a21\u5f0f\u8840\u7f18\uff0c\u8be5\u65b9\u6cd5\u8bc6\u522b\u5e76\u6807\u51c6\u5316\u4e86\u6e90\u6a21\u5f0f\u3001\u6e90\u8868\u3001\u8f6c\u6362\u903b\u8f91\u548c\u805a\u5408\u64cd\u4f5c\u7b49\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\u3002\u4e3a\u8bc4\u4f30\u8840\u7f18\u8d28\u91cf\uff0c\u7814\u7a76\u5f15\u5165\u4e86Schema Lineage Composite Evaluation (SLiCE) \u6307\u6807\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1700\u4e2a\u771f\u5b9e\u5de5\u4e1a\u811a\u672c\u624b\u52a8\u6807\u6ce8\u8840\u7f18\u7684\u65b0\u57fa\u51c6\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e861.3B\u523032B\u768412\u79cd\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ecSLMs\u548cGPT-4o/GPT-4.1\u7b49LLMs\uff09\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u5f0f\u8840\u7f18\u63d0\u53d6\u7684\u6027\u80fd\u968f\u6a21\u578b\u89c4\u6a21\u548c\u63d0\u793a\u6280\u672f\u590d\u6742\u5ea6\u7684\u63d0\u9ad8\u800c\u63d0\u5347\u3002\u7279\u522b\u662f\uff0c\u4e00\u4e2a32B\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u901a\u8fc7\u5355\u4e00\u63a8\u7406\u8ddf\u8e2a\uff0c\u5728\u6807\u51c6\u63d0\u793a\u4e0b\u80fd\u8fbe\u5230\u4e0eGPT\u7cfb\u5217\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u53d1\u73b0\u4e3a\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u90e8\u7f72\u6a21\u5f0f\u611f\u77e5\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.06827", "pdf": "https://arxiv.org/pdf/2508.06827", "abs": "https://arxiv.org/abs/2508.06827", "authors": ["Ishwar Balappanawar", "Venkata Hasith Vattikuti", "Greta Kintzley", "Ronan Azimi-Mancel", "Satvik Golechha"], "title": "Who's the Evil Twin? Differential Auditing for Undesired Behavior", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "main section: 8 pages, 4 figures, 1 table total: 34 pages, 44\n  figures, 12 tables", "summary": "Detecting hidden behaviors in neural networks poses a significant challenge\ndue to minimal prior knowledge and potential adversarial obfuscation. We\nexplore this problem by framing detection as an adversarial game between two\nteams: the red team trains two similar models, one trained solely on benign\ndata and the other trained on data containing hidden harmful behavior, with the\nperformance of both being nearly indistinguishable on the benign dataset. The\nblue team, with limited to no information about the harmful behaviour, tries to\nidentify the compromised model. We experiment using CNNs and try various blue\nteam strategies, including Gaussian noise analysis, model diffing, integrated\ngradients, and adversarial attacks under different levels of hints provided by\nthe red team. Results show high accuracy for adversarial-attack-based methods\n(100\\% correct prediction, using hints), which is very promising, whilst the\nother techniques yield more varied performance. During our LLM-focused rounds,\nwe find that there are not many parallel methods that we could apply from our\nstudy with CNNs. Instead, we find that effective LLM auditing methods require\nsome hints about the undesired distribution, which can then used in standard\nblack-box and open-weight methods to probe the models further and reveal their\nmisalignment. We open-source our auditing games (with the model and data) and\nhope that our findings contribute to designing better audits.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u795e\u7ecf\u7f51\u7edc\u4e2d\u9690\u85cf\u6076\u610f\u884c\u4e3a\u7684\u68c0\u6d4b\u95ee\u9898\u6846\u67b6\u4e3a\u5bf9\u6297\u6027\u6e38\u620f\uff0c\u5176\u4e2d\u7ea2\u961f\u8bad\u7ec3\u6a21\u578b\uff0c\u84dd\u961f\u5c1d\u8bd5\u8bc6\u522b\u53d7\u635f\u6a21\u578b\uff0c\u53d1\u73b0\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u6cd5\u5728CNNs\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u6307\u51faLLMs\u7684\u5ba1\u8ba1\u9700\u7279\u5b9a\u63d0\u793a\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5148\u9a8c\u77e5\u8bc6\u548c\u6f5c\u5728\u7684\u5bf9\u6297\u6027\u6df7\u6dc6\uff0c\u68c0\u6d4b\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u9690\u85cf\u884c\u4e3a\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u5728\u4fe1\u606f\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8bc6\u522b\u88ab\u6076\u610f\u884c\u4e3a\u611f\u67d3\u7684\u6a21\u578b\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5bf9\u6297\u6027\u6e38\u620f\u6846\u67b6\uff1a\u7ea2\u961f\u8bad\u7ec3\u4e24\u4e2a\u76f8\u4f3c\u6a21\u578b\uff08\u4e00\u4e2a\u826f\u6027\uff0c\u4e00\u4e2a\u5305\u542b\u9690\u85cf\u6709\u5bb3\u884c\u4e3a\uff09\uff0c\u84dd\u961f\u5c1d\u8bd5\u8bc6\u522b\u53d7\u635f\u6a21\u578b\u3002\u5b9e\u9a8c\u5728CNNs\u4e0a\u8fdb\u884c\uff0c\u84dd\u961f\u7b56\u7565\u5305\u62ec\u9ad8\u65af\u566a\u58f0\u5206\u6790\u3001\u6a21\u578b\u5dee\u5f02\u5206\u6790\u3001\u96c6\u6210\u68af\u5ea6\u548c\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u5e76\u8003\u8651\u7ea2\u961f\u63d0\u4f9b\u7684\u4e0d\u540c\u7a0b\u5ea6\u63d0\u793a\u3002\u5bf9LLMs\uff0c\u5219\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06CNNs\u7684\u53d1\u73b0\u8fc1\u79fb\uff0c\u5e76\u5f3a\u8c03\u4e86\u6709\u6548\u5ba1\u8ba1\u9700\u8981\u5173\u4e8e\u975e\u671f\u671b\u5206\u5e03\u7684\u63d0\u793a\u3002", "result": "\u5bf9\u4e8eCNNs\uff0c\u57fa\u4e8e\u5bf9\u6297\u6027\u653b\u51fb\u7684\u65b9\u6cd5\u5728\u6709\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\uff08100%\u6b63\u786e\u9884\u6d4b\uff09\uff0c\u800c\u5176\u4ed6\u6280\u672f\u8868\u73b0\u5404\u5f02\u3002\u5bf9\u4e8eLLMs\uff0c\u7814\u7a76\u53d1\u73b0\u76f4\u63a5\u8fc1\u79fbCNNs\u7684\u65b9\u6cd5\u6709\u9650\uff0c\u6709\u6548\u7684LLM\u5ba1\u8ba1\u65b9\u6cd5\u9700\u8981\u5173\u4e8e\u975e\u671f\u671b\u5206\u5e03\u7684\u63d0\u793a\uff0c\u7136\u540e\u53ef\u7528\u4e8e\u9ed1\u76d2\u548c\u5f00\u6e90\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63a2\u6d4b\u6a21\u578b\u4ee5\u63ed\u793a\u5176\u672a\u5bf9\u9f50\u884c\u4e3a\u3002", "conclusion": "\u5bf9\u6297\u6027\u653b\u51fb\u7ed3\u5408\u63d0\u793a\u5728\u68c0\u6d4bCNNs\u9690\u85cf\u884c\u4e3a\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002\u5bf9\u4e8eLLMs\uff0c\u6709\u6548\u7684\u5ba1\u8ba1\u9700\u8981\u5173\u4e8e\u975e\u671f\u671b\u884c\u4e3a\u7684\u660e\u786e\u63d0\u793a\u3002\u672c\u7814\u7a76\u5f00\u6e90\u4e86\u5ba1\u8ba1\u6e38\u620f\uff0c\u65e8\u5728\u4fc3\u8fdb\u66f4\u597d\u7684\u5ba1\u8ba1\u65b9\u6cd5\u8bbe\u8ba1\u3002"}}
{"id": "2508.07107", "pdf": "https://arxiv.org/pdf/2508.07107", "abs": "https://arxiv.org/abs/2508.07107", "authors": ["Timothy Oluwapelumi Adeyemi", "Nadiah Fahad AlOtaibi"], "title": "Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention", "categories": ["cs.AI", "cs.CY", "K.3.1; I.2.6; H.4"], "comment": "10 pages, 1 figure, 3 tables", "summary": "Accurate prediction of student performance is essential for timely academic\nintervention. However, most machine learning models in education are static and\ncannot adapt when new data, such as post-intervention outcomes, become\navailable. To address this limitation, we propose a Feedback-Driven Decision\nSupport System (DSS) with a closed-loop architecture that enables continuous\nmodel refinement. The system integrates a LightGBM-based regressor with\nincremental retraining, allowing educators to input updated student results,\nwhich automatically trigger model updates. This adaptive mechanism improves\nprediction accuracy by learning from real-world academic progress. The platform\nfeatures a Flask-based web interface for real-time interaction and incorporates\nSHAP for explainability, ensuring transparency. Experimental results show a\n10.7\\% reduction in RMSE after retraining, with consistent upward adjustments\nin predicted scores for intervened students. By transforming static predictors\ninto self-improving systems, our approach advances educational analytics toward\nhuman-centered, data-driven, and responsive AI. The framework is designed for\nintegration into LMS and institutional dashboards.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u53cd\u9988\u9a71\u52a8\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u901a\u8fc7\u589e\u91cf\u518d\u8bad\u7ec3\u548c\u5b9e\u65f6\u6570\u636e\u53cd\u9988\uff0c\u4f7f\u5b66\u751f\u8868\u73b0\u9884\u6d4b\u6a21\u578b\u80fd\u6301\u7eed\u81ea\u6211\u6539\u8fdb\uff0c\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u6559\u80b2\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5927\u591a\u662f\u9759\u6001\u7684\uff0c\u65e0\u6cd5\u9002\u5e94\u65b0\u6570\u636e\uff08\u5982\u5e72\u9884\u540e\u7684\u5b66\u751f\u8868\u73b0\uff09\u7684\u51fa\u73b0\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5b66\u4e1a\u5e72\u9884\u4e2d\u63d0\u4f9b\u53ca\u65f6\u51c6\u786e\u9884\u6d4b\u7684\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cd\u9988\u9a71\u52a8\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff08DSS\uff09\uff0c\u91c7\u7528\u95ed\u73af\u67b6\u6784\u5b9e\u73b0\u6a21\u578b\u6301\u7eed\u4f18\u5316\u3002\u8be5\u7cfb\u7edf\u6574\u5408\u4e86\u57fa\u4e8eLightGBM\u7684\u56de\u5f52\u5668\u548c\u589e\u91cf\u518d\u8bad\u7ec3\u673a\u5236\uff0c\u5141\u8bb8\u6559\u80b2\u5de5\u4f5c\u8005\u8f93\u5165\u66f4\u65b0\u7684\u5b66\u751f\u6210\u7ee9\uff0c\u4ece\u800c\u81ea\u52a8\u89e6\u53d1\u6a21\u578b\u66f4\u65b0\u3002\u6b64\u5916\uff0c\u5e73\u53f0\u901a\u8fc7\u57fa\u4e8eFlask\u7684\u7f51\u9875\u754c\u9762\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\uff0c\u5e76\u6574\u5408SHAP\u4ee5\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u901a\u8fc7\u589e\u91cf\u518d\u8bad\u7ec3\uff0c\u7cfb\u7edf\u7684RMSE\u964d\u4f4e\u4e8610.7%\uff0c\u5e76\u4e14\u5bf9\u53d7\u5e72\u9884\u5b66\u751f\u7684\u9884\u6d4b\u5206\u6570\u663e\u793a\u51fa\u6301\u7eed\u7684\u5411\u4e0a\u8c03\u6574\uff0c\u9a8c\u8bc1\u4e86\u5176\u9884\u6d4b\u51c6\u786e\u6027\u7684\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u9759\u6001\u9884\u6d4b\u5668\u8f6c\u53d8\u4e3a\u81ea\u6211\u6539\u8fdb\u7684\u7cfb\u7edf\uff0c\u63a8\u52a8\u4e86\u6559\u80b2\u5206\u6790\u5411\u4ee5\u4eba\u4e3a\u672c\u3001\u6570\u636e\u9a71\u52a8\u548c\u54cd\u5e94\u5f0fAI\u7684\u65b9\u5411\u53d1\u5c55\u3002\u8be5\u6846\u67b6\u8bbe\u8ba1\u6613\u4e8e\u96c6\u6210\u5230\u5b66\u4e60\u7ba1\u7406\u7cfb\u7edf\uff08LMS\uff09\u548c\u673a\u6784\u4eea\u8868\u677f\u4e2d\u3002"}}
{"id": "2508.06703", "pdf": "https://arxiv.org/pdf/2508.06703", "abs": "https://arxiv.org/abs/2508.06703", "authors": ["Justin London"], "title": "Fourier Optics and Deep Learning Methods for Fast 3D Reconstruction in Digital Holography", "categories": ["cs.CV"], "comment": null, "summary": "Computer-generated holography (CGH) is a promising method that modulates\nuser-defined waveforms with digital holograms. An efficient and fast pipeline\nframework is proposed to synthesize CGH using initial point cloud and MRI data.\nThis input data is reconstructed into volumetric objects that are then input\ninto non-convex Fourier optics optimization algorithms for phase-only hologram\n(POH) and complex-hologram (CH) generation using alternating projection, SGD,\nand quasi-Netwton methods. Comparison of reconstruction performance of these\nalgorithms as measured by MSE, RMSE, and PSNR is analyzed as well as to HoloNet\ndeep learning CGH. Performance metrics are shown to be improved by using 2D\nmedian filtering to remove artifacts and speckled noise during optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u5feb\u901f\u7684CGH\u5408\u6210\u7ba1\u7ebf\u6846\u67b6\uff0c\u5229\u7528\u70b9\u4e91\u548cMRI\u6570\u636e\uff0c\u901a\u8fc7\u975e\u51f8\u5085\u91cc\u53f6\u5149\u5b66\u4f18\u5316\u751f\u6210\u5168\u606f\u56fe\uff0c\u5e76\u901a\u8fc7\u4e2d\u503c\u6ee4\u6ce2\u663e\u8457\u6539\u5584\u4e86\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u5feb\u901f\u7684CGH\u5408\u6210\u7ba1\u7ebf\u6846\u67b6\uff0c\u4ee5\u8c03\u5236\u7528\u6237\u5b9a\u4e49\u6ce2\u5f62\uff0c\u5e76\u5229\u7528\u521d\u59cb\u70b9\u4e91\u548cMRI\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e00\u4e2a\u7ba1\u7ebf\u6846\u67b6\uff0c\u5c06\u521d\u59cb\u70b9\u4e91\u548cMRI\u6570\u636e\u91cd\u5efa\u4e3a\u4f53\u4e09\u7ef4\u5bf9\u8c61\u3002\u8fd9\u4e9b\u5bf9\u8c61\u968f\u540e\u8f93\u5165\u5230\u975e\u51f8\u5085\u91cc\u53f6\u5149\u5b66\u4f18\u5316\u7b97\u6cd5\uff08\u5305\u62ec\u4ea4\u66ff\u6295\u5f71\u3001SGD\u548c\u62df\u725b\u987f\u6cd5\uff09\u4e2d\uff0c\u7528\u4e8e\u751f\u6210\u76f8\u4f4d\u578b\u5168\u606f\u56fe(POH)\u548c\u590d\u6570\u5168\u606f\u56fe(CH)\u3002\u901a\u8fc7MSE\u3001RMSE\u548cPSNR\u6307\u6807\u8bc4\u4f30\u8fd9\u4e9b\u7b97\u6cd5\u7684\u91cd\u5efa\u6027\u80fd\uff0c\u5e76\u4e0eHoloNet\u6df1\u5ea6\u5b66\u4e60CGH\u8fdb\u884c\u6bd4\u8f83\u3002\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c\u5e94\u75282D\u4e2d\u503c\u6ee4\u6ce2\u4ee5\u6d88\u9664\u4f2a\u5f71\u548c\u6563\u6591\u566a\u58f0\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u751f\u6210POH\u548cCH\uff0c\u5e76\u901a\u8fc7MSE\u3001RMSE\u548cPSNR\u6307\u6807\u8bc4\u4f30\u4e86\u91cd\u5efa\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u4f7f\u75282D\u4e2d\u503c\u6ee4\u6ce2\u53ef\u4ee5\u6709\u6548\u53bb\u9664\u4f2a\u5f71\u548c\u6563\u6591\u566a\u58f0\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u6027\u80fd\u6307\u6807\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u5feb\u901f\u7684CGH\u5408\u6210\u7ba1\u7ebf\u6846\u67b6\uff0c\u80fd\u591f\u5229\u7528\u70b9\u4e91\u548cMRI\u6570\u636e\u751f\u6210\u9ad8\u8d28\u91cf\u5168\u606f\u56fe\uff0c\u5e76\u901a\u8fc7\u96c6\u62102D\u4e2d\u503c\u6ee4\u6ce2\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u91cd\u5efa\u6548\u679c\u3002"}}
{"id": "2508.07185", "pdf": "https://arxiv.org/pdf/2508.07185", "abs": "https://arxiv.org/abs/2508.07185", "authors": ["Kabir Khan", "Priya Sharma", "Arjun Mehta", "Neha Gupta", "Ravi Narayanan"], "title": "DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; H.3.3; H.2.8"], "comment": "Preprint; 7 figures, 3 tables, 1 algorithm; v1. Code and data will be\n  released", "summary": "Large Language Models (LLMs) suffer from a critical limitation: their\nknowledge is static and quickly becomes outdated. Retraining these massive\nmodels is computationally prohibitive, while existing knowledge editing\ntechniques can be slow and may introduce unforeseen side effects. To address\nthis, we propose DySK-Attn, a novel framework that enables LLMs to efficiently\nintegrate real-time knowledge from a dynamic external source. Our approach\nsynergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated\ninstantaneously. The core of our framework is a sparse knowledge attention\nmechanism, which allows the LLM to perform a coarse-to-fine grained search,\nefficiently identifying and focusing on a small, highly relevant subset of\nfacts from the vast KG. This mechanism avoids the high computational cost of\ndense attention over the entire knowledge base and mitigates noise from\nirrelevant information. We demonstrate through extensive experiments on\ntime-sensitive question-answering tasks that DySK-Attn significantly\noutperforms strong baselines, including standard Retrieval-Augmented Generation\n(RAG) and model editing techniques, in both factual accuracy for updated\nknowledge and computational efficiency. Our framework offers a scalable and\neffective solution for building LLMs that can stay current with the\never-changing world.", "AI": {"tldr": "DySK-Attn\u662f\u4e00\u79cd\u65b0\u9896\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u548c\u7a00\u758f\u77e5\u8bc6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u9ad8\u6548\u96c6\u6210\u5b9e\u65f6\u77e5\u8bc6\uff0c\u5728\u65f6\u95f4\u654f\u611f\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eRAG\u548c\u6a21\u578b\u7f16\u8f91\uff0c\u540c\u65f6\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u77e5\u8bc6\u662f\u9759\u6001\u4e14\u6613\u8fc7\u65f6\uff0c\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u6280\u672f\u6548\u7387\u4f4e\u4e14\u53ef\u80fd\u5f15\u5165\u526f\u4f5c\u7528\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3LLMs\u77e5\u8bc6\u66f4\u65b0\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faDySK-Attn\u6846\u67b6\uff0c\u5c06LLM\u4e0e\u53ef\u5373\u65f6\u66f4\u65b0\u7684\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u7ed3\u5408\u3002\u6838\u5fc3\u662f\u7a00\u758f\u77e5\u8bc6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684\u641c\u7d22\uff0c\u9ad8\u6548\u8bc6\u522bKG\u4e2d\u5c11\u91cf\u76f8\u5173\u4e8b\u5b9e\uff0c\u907f\u514d\u5bf9\u6574\u4e2a\u77e5\u8bc6\u5e93\u8fdb\u884c\u5bc6\u96c6\u6ce8\u610f\u529b\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u51cf\u8f7b\u65e0\u5173\u4fe1\u606f\u7684\u5e72\u6270\u3002", "result": "\u5728\u65f6\u95f4\u654f\u611f\u7684\u95ee\u7b54\u4efb\u52a1\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cDySK-Attn\u5728\u66f4\u65b0\u77e5\u8bc6\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\uff0c\u663e\u8457\u4f18\u4e8e\u5305\u62ec\u6807\u51c6\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u6a21\u578b\u7f16\u8f91\u6280\u672f\u5728\u5185\u7684\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "DySK-Attn\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f7fLLMs\u80fd\u591f\u4e0e\u4e0d\u65ad\u53d8\u5316\u7684\u4e16\u754c\u4fdd\u6301\u540c\u6b65\u3002"}}
{"id": "2508.06871", "pdf": "https://arxiv.org/pdf/2508.06871", "abs": "https://arxiv.org/abs/2508.06871", "authors": ["Aleksandar Todorov", "Juan Cardenas-Cartagena", "Rafael F. Cunha", "Marco Zullich", "Matthia Sabatelli"], "title": "Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Plasticity loss, a diminishing capacity to adapt as training progresses, is a\ncritical challenge in deep reinforcement learning. We examine this issue in\nmulti-task reinforcement learning (MTRL), where higher representational\nflexibility is crucial for managing diverse and potentially conflicting task\ndemands. We systematically explore how sparsification methods, particularly\nGradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), enhance\nplasticity and consequently improve performance in MTRL agents. We evaluate\nthese approaches across distinct MTRL architectures (shared backbone, Mixture\nof Experts, Mixture of Orthogonal Experts) on standardized MTRL benchmarks,\ncomparing against dense baselines, and a comprehensive range of alternative\nplasticity-inducing or regularization methods. Our results demonstrate that\nboth GMP and SET effectively mitigate key indicators of plasticity degradation,\nsuch as neuron dormancy and representational collapse. These plasticity\nimprovements often correlate with enhanced multi-task performance, with sparse\nagents frequently outperforming dense counterparts and achieving competitive\nresults against explicit plasticity interventions. Our findings offer insights\ninto the interplay between plasticity, network sparsity, and MTRL designs,\nhighlighting dynamic sparsification as a robust but context-sensitive tool for\ndeveloping more adaptable MTRL systems.", "AI": {"tldr": "\u52a8\u6001\u7a00\u758f\u5316\u65b9\u6cd5\uff08\u5982GMP\u548cSET\uff09\u80fd\u6709\u6548\u63d0\u5347\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\uff08MTRL\uff09\u4e2d\u4ee3\u7406\u7684\u9002\u5e94\u6027\uff0c\u7f13\u89e3\u5851\u6027\u635f\u5931\uff0c\u5e76\u6539\u5584\u6574\u4f53\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u5851\u6027\u635f\u5931\uff08\u9002\u5e94\u80fd\u529b\u968f\u8bad\u7ec3\u4e0b\u964d\uff09\u7684\u6311\u6218\uff0c\u5728\u9700\u8981\u9ad8\u8868\u793a\u7075\u6d3b\u6027\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\uff08MTRL\uff09\u4e2d\uff0c\u8fd9\u4e00\u95ee\u9898\u5c24\u4e3a\u5173\u952e\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u63a2\u7a76\u4e86\u7a00\u758f\u5316\u65b9\u6cd5\uff08\u7279\u522b\u662f\u6e10\u8fdb\u5e45\u5ea6\u526a\u679dGMP\u548c\u7a00\u758f\u6f14\u5316\u8bad\u7ec3SET\uff09\u5982\u4f55\u589e\u5f3a\u5851\u6027\u5e76\u6539\u5584MTRL\u4ee3\u7406\u7684\u6027\u80fd\u3002\u7814\u7a76\u5728\u4e0d\u540cMTRL\u67b6\u6784\u548c\u6807\u51c6\u5316\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u5bc6\u96c6\u57fa\u7ebf\u53ca\u5176\u4ed6\u5851\u6027\u589e\u5f3a\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7ed3\u679c\u8868\u660eGMP\u548cSET\u6709\u6548\u7f13\u89e3\u4e86\u5851\u6027\u9000\u5316\u7684\u5173\u952e\u6307\u6807\uff0c\u5982\u795e\u7ecf\u5143\u4f11\u7720\u548c\u8868\u5f81\u5d29\u6e83\u3002\u8fd9\u4e9b\u5851\u6027\u63d0\u5347\u901a\u5e38\u4e0e\u591a\u4efb\u52a1\u6027\u80fd\u7684\u589e\u5f3a\u76f8\u5173\uff0c\u7a00\u758f\u4ee3\u7406\u5e38\u4f18\u4e8e\u5bc6\u96c6\u4ee3\u7406\u5e76\u4e0e\u663e\u5f0f\u5851\u6027\u5e72\u9884\u65b9\u6cd5\u6548\u679c\u76f8\u5f53\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5851\u6027\u3001\u7f51\u7edc\u7a00\u758f\u6027\u548cMTRL\u8bbe\u8ba1\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5f3a\u8c03\u52a8\u6001\u7a00\u758f\u5316\u662f\u5f00\u53d1\u66f4\u5177\u9002\u5e94\u6027MTRL\u7cfb\u7edf\u7684\u5f3a\u5927\u4f46\u5177\u6709\u60c5\u5883\u654f\u611f\u6027\u7684\u5de5\u5177\u3002"}}
{"id": "2508.07186", "pdf": "https://arxiv.org/pdf/2508.07186", "abs": "https://arxiv.org/abs/2508.07186", "authors": ["Amit Dhanda"], "title": "Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "We propose a novel framework for summarizing structured enterprise data\nacross multiple dimensions using large language model (LLM)-based agents.\nTraditional table-to-text models often lack the capacity to reason across\nhierarchical structures and context-aware deltas, which are essential in\nbusiness reporting tasks. Our method introduces a multi-agent pipeline that\nextracts, analyzes, and summarizes multi-dimensional data using agents for\nslicing, variance detection, context construction, and LLM-based generation.\nOur results show that the proposed framework outperforms traditional\napproaches, achieving 83\\% faithfulness to underlying data, superior coverage\nof significant changes, and high relevance scores (4.4/5) for decision-critical\ninsights. The improvements are especially pronounced in categories involving\nsubtle trade-offs, such as increased revenue due to price changes amid\ndeclining unit volumes, which competing methods either overlook or address with\nlimited specificity. We evaluate the framework on Kaggle datasets and\ndemonstrate significant improvements in faithfulness, relevance, and insight\nquality over baseline table summarization approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u65b0\u9896\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u591a\u7ef4\u4f01\u4e1a\u6570\u636e\u8fdb\u884c\u6458\u8981\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u4e1a\u52a1\u62a5\u544a\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6458\u8981\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u8868\u683c\u5230\u6587\u672c\u6a21\u578b\u7f3a\u4e4f\u5bf9\u5c42\u6b21\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u53d8\u5316\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u8fd9\u4e9b\u5728\u4e1a\u52a1\u62a5\u544a\u4efb\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u4ee3\u7406\u7ba1\u9053\uff0c\u5229\u7528LLM\u4ee3\u7406\u8fdb\u884c\u6570\u636e\u5207\u7247\u3001\u5dee\u5f02\u68c0\u6d4b\u3001\u4e0a\u4e0b\u6587\u6784\u5efa\u548c\u6458\u8981\u751f\u6210\uff0c\u4ee5\u63d0\u53d6\u3001\u5206\u6790\u548c\u6c47\u603b\u591a\u7ef4\u6570\u636e\u3002", "result": "\u8be5\u6846\u67b6\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e8683%\u7684\u6570\u636e\u5fe0\u5b9e\u5ea6\uff0c\u5bf9\u663e\u8457\u53d8\u5316\u7684\u8986\u76d6\u7387\u66f4\u9ad8\uff0c\u51b3\u7b56\u5173\u952e\u6d1e\u5bdf\u7684\u76f8\u5173\u6027\u8bc4\u5206\u8fbe\u52304.4/5\u3002\u5728\u6d89\u53ca\u7ec6\u5fae\u6743\u8861\u7684\u7c7b\u522b\u4e2d\uff0c\u6027\u80fd\u63d0\u5347\u5c24\u4e3a\u663e\u8457\uff0c\u8d85\u8d8a\u4e86\u7ade\u4e89\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u591a\u7ef4\u4f01\u4e1a\u6570\u636e\u6458\u8981\u7684\u5fe0\u5b9e\u5ea6\u3001\u76f8\u5173\u6027\u548c\u6d1e\u5bdf\u8d28\u91cf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u590d\u6742\u4e1a\u52a1\u573a\u666f\u3002"}}
{"id": "2508.06715", "pdf": "https://arxiv.org/pdf/2508.06715", "abs": "https://arxiv.org/abs/2508.06715", "authors": ["Jixuan He", "Chieh Hubert Lin", "Lu Qi", "Ming-Hsuan Yang"], "title": "Restage4D: Reanimating Deformable 3D Reconstruction from a Single Video", "categories": ["cs.CV"], "comment": null, "summary": "Creating deformable 3D content has gained increasing attention with the rise\nof text-to-image and image-to-video generative models. While these models\nprovide rich semantic priors for appearance, they struggle to capture the\nphysical realism and motion dynamics needed for authentic 4D scene synthesis.\nIn contrast, real-world videos can provide physically grounded geometry and\narticulation cues that are difficult to hallucinate. One question is raised:\n\\textit{Can we generate physically consistent 4D content by leveraging the\nmotion priors of the real-world video}? In this work, we explore the task of\nreanimating deformable 3D scenes from a single video, using the original\nsequence as a supervisory signal to correct artifacts from synthetic motion. We\nintroduce \\textbf{Restage4D}, a geometry-preserving pipeline for\nvideo-conditioned 4D restaging. Our approach uses a video-rewinding training\nstrategy to temporally bridge a real base video and a synthetic driving video\nvia a shared motion representation. We further incorporate an occlusion-aware\nrigidity loss and a disocclusion backtracing mechanism to improve structural\nand geometry consistency under challenging motion. We validate Restage4D on\nDAVIS and PointOdyssey, demonstrating improved geometry consistency, motion\nquality, and 3D tracking performance. Our method not only preserves deformable\nstructure under novel motion, but also automatically corrects errors introduced\nby generative models, revealing the potential of video prior in 4D restaging\ntask. Source code and trained models will be released.", "AI": {"tldr": "\u5229\u7528\u771f\u5b9e\u89c6\u9891\u7684\u8fd0\u52a8\u5148\u9a8c\uff0c\u5bf9\u53ef\u53d8\u5f623D\u573a\u666f\u8fdb\u884c4D\u91cd\u6f14\uff0c\u4fee\u6b63\u751f\u6210\u6a21\u578b\u9519\u8bef\uff0c\u5b9e\u73b0\u7269\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf/\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u751f\u62104D\u5185\u5bb9\u65f6\uff0c\u7f3a\u4e4f\u7269\u7406\u771f\u5b9e\u6027\u548c\u8fd0\u52a8\u52a8\u529b\u5b66\uff0c\u96be\u4ee5\u6355\u6349\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u8fd0\u52a8\u7ec6\u8282\u3002\u4f5c\u8005\u63d0\u51fa\u5229\u7528\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u7684\u8fd0\u52a8\u5148\u9a8c\uff0c\u4ee5\u5b9e\u73b0\u7269\u7406\u4e00\u81f4\u76844D\u5185\u5bb9\u751f\u6210\uff0c\u5e76\u7ea0\u6b63\u5408\u6210\u8fd0\u52a8\u4e2d\u7684\u4f2a\u5f71\u3002", "method": "\u63d0\u51fa\u4e86Restage4D\uff0c\u4e00\u4e2a\u51e0\u4f55\u4fdd\u6301\u7684\u89c6\u9891\u6761\u4ef64D\u91cd\u6f14\u6d41\u6c34\u7ebf\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u201c\u89c6\u9891\u56de\u6eaf\u201d\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528\u5171\u4eab\u8fd0\u52a8\u8868\u793a\u6765\u8fde\u63a5\u771f\u5b9e\u57fa\u7840\u89c6\u9891\u548c\u5408\u6210\u9a71\u52a8\u89c6\u9891\uff0c\u5e76\u5f15\u5165\u4e86\u201c\u906e\u6321\u611f\u77e5\u521a\u6027\u635f\u5931\u201d\u548c\u201c\u53bb\u906e\u6321\u56de\u6eaf\u673a\u5236\u201d\uff0c\u4ee5\u63d0\u9ad8\u590d\u6742\u8fd0\u52a8\u4e0b\u7684\u7ed3\u6784\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "result": "\u5728DAVIS\u548cPointOdyssey\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660eRestage4D\u663e\u8457\u63d0\u5347\u4e86\u51e0\u4f55\u4e00\u81f4\u6027\u3001\u8fd0\u52a8\u8d28\u91cf\u548c3D\u8ddf\u8e2a\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5728\u65b0\u7684\u8fd0\u52a8\u4e0b\u4fdd\u6301\u4e86\u53ef\u53d8\u5f62\u7ed3\u6784\uff0c\u8fd8\u81ea\u52a8\u7ea0\u6b63\u4e86\u751f\u6210\u6a21\u578b\u5f15\u5165\u7684\u9519\u8bef\uff0c\u63ed\u793a\u4e86\u89c6\u9891\u5148\u9a8c\u57284D\u91cd\u6f14\u4efb\u52a1\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.07195", "pdf": "https://arxiv.org/pdf/2508.07195", "abs": "https://arxiv.org/abs/2508.07195", "authors": ["Yanru Sun", "Emadeldeen Eldele", "Zongxia Xie", "Yucheng Wang", "Wenzhe Niu", "Qinghua Hu", "Chee Keong Kwoh", "Min Wu"], "title": "Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated impressive\ncapabilities in natural language processing due to their strong generalization\nand sequence modeling capabilities. However, their direct application to time\nseries forecasting remains challenging due to two fundamental issues: the\ninherent heterogeneity of temporal patterns and the modality gap between\ncontinuous numerical signals and discrete language representations. In this\nwork, we propose TALON, a unified framework that enhances LLM-based forecasting\nby modeling temporal heterogeneity and enforcing semantic alignment.\nSpecifically, we design a Heterogeneous Temporal Encoder that partitions\nmultivariate time series into structurally coherent segments, enabling\nlocalized expert modeling across diverse temporal patterns. To bridge the\nmodality gap, we introduce a Semantic Alignment Module that aligns temporal\nfeatures with LLM-compatible representations, enabling effective integration of\ntime series into language-based models while eliminating the need for\nhandcrafted prompts during inference. Extensive experiments on seven real-world\nbenchmarks demonstrate that TALON achieves superior performance across all\ndatasets, with average MSE improvements of up to 11\\% over recent\nstate-of-the-art methods. These results underscore the effectiveness of\nincorporating both pattern-aware and semantic-aware designs when adapting LLMs\nfor time series forecasting. The code is available at:\nhttps://github.com/syrGitHub/TALON.", "AI": {"tldr": "LLM\u5728\u65f6\u5e8f\u9884\u6d4b\u4e2d\u9762\u4e34\u6311\u6218\uff08\u5f02\u8d28\u6027\u3001\u6a21\u6001\u9e3f\u6c9f\uff09\u3002\u672c\u6587\u63d0\u51faTALON\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u8d28\u6027\u5efa\u6a21\u548c\u8bed\u4e49\u5bf9\u9f50\u6765\u589e\u5f3aLLM\u65f6\u5e8f\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8eSOTA\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5c06LLM\u76f4\u63a5\u5e94\u7528\u4e8e\u65f6\u5e8f\u9884\u6d4b\u9762\u4e34\u6311\u6218\uff1a\u65f6\u5e8f\u6a21\u5f0f\u7684\u56fa\u6709\u5f02\u8d28\u6027\uff0c\u4ee5\u53ca\u8fde\u7eed\u6570\u503c\u4fe1\u53f7\u4e0e\u79bb\u6563\u8bed\u8a00\u8868\u793a\u4e4b\u95f4\u7684\u6a21\u6001\u9e3f\u6c9f\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6TALON\uff0c\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u589e\u5f3aLLM\u65f6\u5e8f\u9884\u6d4b\uff1a1. \u8bbe\u8ba1\u5f02\u8d28\u65f6\u5e8f\u7f16\u7801\u5668\uff0c\u5212\u5206\u591a\u5143\u65f6\u5e8f\u5e76\u8fdb\u884c\u5c40\u90e8\u5efa\u6a21\u30022. \u5f15\u5165\u8bed\u4e49\u5bf9\u9f50\u6a21\u5757\uff0c\u5c06\u65f6\u5e8f\u7279\u5f81\u4e0eLLM\u517c\u5bb9\u8868\u793a\u5bf9\u9f50\uff0c\u6d88\u9664\u624b\u52a8\u63d0\u793a\u3002", "result": "\u5728\u4e03\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTALON\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747MSE\u76f8\u8f83\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe11%\u3002", "conclusion": "\u5c06\u6a21\u5f0f\u611f\u77e5\u548c\u8bed\u4e49\u611f\u77e5\u8bbe\u8ba1\u878d\u5165LLM\u65f6\u5e8f\u9884\u6d4b\u4e2d\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.06885", "pdf": "https://arxiv.org/pdf/2508.06885", "abs": "https://arxiv.org/abs/2508.06885", "authors": ["Anthony Bellotti", "Xindi Zhao"], "title": "Conformal Prediction and Trustworthy AI", "categories": ["cs.LG"], "comment": "Preprint for an essay to be published in The Importance of Being\n  Learnable (Enhancing the Learnability and Reliability of Machine Learning\n  Algorithms) Essays Dedicated to Alexander Gammerman on His 80th Birthday,\n  LNCS Springer Nature Switzerland AG ed. Nguyen K.A. and Luo Z", "summary": "Conformal predictors are machine learning algorithms developed in the 1990's\nby Gammerman, Vovk, and their research team, to provide set predictions with\nguaranteed confidence level. Over recent years, they have grown in popularity\nand have become a mainstream methodology for uncertainty quantification in the\nmachine learning community. From its beginning, there was an understanding that\nthey enable reliable machine learning with well-calibrated uncertainty\nquantification. This makes them extremely beneficial for developing trustworthy\nAI, a topic that has also risen in interest over the past few years, in both\nthe AI community and society more widely. In this article, we review the\npotential for conformal prediction to contribute to trustworthy AI beyond its\nmarginal validity property, addressing problems such as generalization risk and\nAI governance. Experiments and examples are also provided to demonstrate its\nuse as a well-calibrated predictor and for bias identification and mitigation.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u5171\u5f62\u9884\u6d4b\u5668\uff08Conformal Predictors\uff09\u5728\u53ef\u4fe1AI\u4e2d\u7684\u6f5c\u529b\uff0c\u63a2\u8ba8\u5176\u5982\u4f55\u8d85\u8d8a\u4f20\u7edf\u7528\u9014\uff0c\u89e3\u51b3\u6cdb\u5316\u98ce\u9669\u548cAI\u6cbb\u7406\u7b49\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u5176\u5728\u6821\u51c6\u9884\u6d4b\u548c\u504f\u5dee\u8bc6\u522b\u4e0e\u7f13\u89e3\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5171\u5f62\u9884\u6d4b\u5668\u56e0\u5176\u63d0\u4f9b\u5177\u6709\u4fdd\u8bc1\u7f6e\u4fe1\u6c34\u5e73\u7684\u96c6\u5408\u9884\u6d4b\u800c\u6210\u4e3a\u673a\u5668\u5b66\u4e60\u4e2d\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u4e3b\u6d41\u65b9\u6cd5\u3002\u5176\u53ef\u9760\u6027\u4f7f\u5176\u5bf9\u6784\u5efa\u53ef\u4fe1AI\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u6df1\u5165\u63a2\u8ba8\u5171\u5f62\u9884\u6d4b\u5668\u5982\u4f55\u8d85\u8d8a\u5176\u8fb9\u9645\u6709\u6548\u6027\uff0c\u4e3a\u53ef\u4fe1AI\u7684\u66f4\u5e7f\u6cdb\u6311\u6218\uff08\u5982\u6cdb\u5316\u98ce\u9669\u548cAI\u6cbb\u7406\uff09\u505a\u51fa\u8d21\u732e\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff08review\uff09\u7684\u65b9\u5f0f\u8fdb\u884c\uff0c\u5e76\u8f85\u4ee5\u5b9e\u9a8c\u548c\u5b9e\u4f8b\u6765\u5c55\u793a\u5171\u5f62\u9884\u6d4b\u5668\u7684\u5e94\u7528\u6f5c\u529b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5c55\u793a\u4e86\u5171\u5f62\u9884\u6d4b\u5668\u4f5c\u4e3a\u4e00\u79cd\u6821\u51c6\u826f\u597d\u7684\u9884\u6d4b\u5668\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u5176\u5728\u8bc6\u522b\u548c\u7f13\u89e3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u504f\u5dee\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5171\u5f62\u9884\u6d4b\u5668\u5728\u4fc3\u8fdb\u53ef\u4fe1AI\u53d1\u5c55\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u4e0d\u4ec5\u9650\u4e8e\u63d0\u4f9b\u8fb9\u9645\u6709\u6548\u6027\uff0c\u8fd8\u80fd\u6709\u6548\u89e3\u51b3\u6cdb\u5316\u98ce\u9669\u548cAI\u6cbb\u7406\u7b49\u590d\u6742\u95ee\u9898\uff0c\u5e76\u652f\u6301\u504f\u5dee\u7684\u8bc6\u522b\u4e0e\u7f13\u89e3\u3002"}}
{"id": "2508.07292", "pdf": "https://arxiv.org/pdf/2508.07292", "abs": "https://arxiv.org/abs/2508.07292", "authors": ["Yi Tang", "Kaini Wang", "Yang Chen", "Guangquan Zhou"], "title": "EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Developing general artificial intelligence (AI) systems to support endoscopic\nimage diagnosis is an emerging research priority. Existing methods based on\nlarge-scale pretraining often lack unified coordination across tasks and\nstruggle to handle the multi-step processes required in complex clinical\nworkflows. While AI agents have shown promise in flexible instruction parsing\nand tool integration across domains, their potential in endoscopy remains\nunderexplored. To address this gap, we propose EndoAgent, the first\nmemory-guided agent for vision-to-decision endoscopic analysis that integrates\niterative reasoning with adaptive tool selection and collaboration. Built on a\ndual-memory design, it enables sophisticated decision-making by ensuring\nlogical coherence through short-term action tracking and progressively\nenhancing reasoning acuity through long-term experiential learning. To support\ndiverse clinical tasks, EndoAgent integrates a suite of expert-designed tools\nwithin a unified reasoning loop. We further introduce EndoAgentBench, a\nbenchmark of 5,709 visual question-answer pairs that assess visual\nunderstanding and language generation capabilities in realistic scenarios.\nExtensive experiments show that EndoAgent consistently outperforms both general\nand medical multimodal models, exhibiting its strong flexibility and reasoning\ncapabilities.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u8bb0\u5fc6\u5f15\u5bfcAI\u667a\u80fd\u4f53EndoAgent\uff0c\u7528\u4e8e\u5185\u7aa5\u955c\u56fe\u50cf\u8bca\u65ad\uff0c\u901a\u8fc7\u53cc\u8bb0\u5fc6\u8bbe\u8ba1\u548c\u5de5\u5177\u96c6\u6210\uff0c\u5728\u590d\u6742\u4e34\u5e8a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5f15\u5165\u65b0\u57fa\u51c6EndoAgentBench\u3002", "motivation": "\u73b0\u6709\u5185\u7aa5\u955cAI\u8bca\u65ad\u65b9\u6cd5\u7f3a\u4e4f\u4efb\u52a1\u95f4\u534f\u8c03\u6027\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u4e34\u5e8a\u5de5\u4f5c\u6d41\u4e2d\u7684\u591a\u6b65\u9aa4\u8fc7\u7a0b\uff1b\u540c\u65f6\uff0cAI\u667a\u80fd\u4f53\u5728\u5185\u7aa5\u955c\u9886\u57df\u7684\u6f5c\u529b\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faEndoAgent\uff0c\u9996\u4e2a\u7528\u4e8e\u89c6\u89c9-\u51b3\u7b56\u5185\u7aa5\u955c\u5206\u6790\u7684\u8bb0\u5fc6\u5f15\u5bfc\u667a\u80fd\u4f53\u3002\u5b83\u91c7\u7528\u53cc\u8bb0\u5fc6\u8bbe\u8ba1\uff08\u77ed\u671f\u884c\u52a8\u8ddf\u8e2a\u548c\u957f\u671f\u7ecf\u9a8c\u5b66\u4e60\uff09\u4ee5\u786e\u4fdd\u51b3\u7b56\u8fde\u8d2f\u6027\u5e76\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u96c6\u6210\u4e00\u5957\u4e13\u5bb6\u8bbe\u8ba1\u7684\u5de5\u5177\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86EndoAgentBench\u57fa\u51c6\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEndoAgent\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u901a\u7528\u548c\u533b\u5b66\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u7075\u6d3b\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "EndoAgent\u901a\u8fc7\u5f15\u5165\u8bb0\u5fc6\u5f15\u5bfc\u667a\u80fd\u4f53\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5185\u7aa5\u955cAI\u8bca\u65ad\u4e2d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4e34\u5e8a\u5de5\u4f5c\u6d41\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\u548c\u5206\u6790\u8868\u73b0\u3002"}}
{"id": "2508.06756", "pdf": "https://arxiv.org/pdf/2508.06756", "abs": "https://arxiv.org/abs/2508.06756", "authors": ["Somayeh Farahani", "Marjaneh Hejazi", "Antonio Di Ieva", "Sidong Liu"], "title": "FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for oral and poster presentation at MICCAI 2025", "summary": "Accurate, noninvasive detection of isocitrate dehydrogenase (IDH) mutation is\nessential for effective glioma management. Traditional methods rely on invasive\ntissue sampling, which may fail to capture a tumor's spatial heterogeneity.\nWhile deep learning models have shown promise in molecular profiling, their\nperformance is often limited by scarce annotated data. In contrast, foundation\ndeep learning models offer a more generalizable approach for glioma imaging\nbiomarkers. We propose a Foundation-based Biomarker Network (FoundBioNet) that\nutilizes a SWIN-UNETR-based architecture to noninvasively predict IDH mutation\nstatus from multi-parametric MRI. Two key modules are incorporated: Tumor-Aware\nFeature Encoding (TAFE) for extracting multi-scale, tumor-focused features, and\nCross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch\nsignals associated with IDH mutation. The model was trained and validated on a\ndiverse, multi-center cohort of 1705 glioma patients from six public datasets.\nOur model achieved AUCs of 90.58%, 88.08%, 65.41%, and 80.31% on independent\ntest sets from EGD, TCGA, Ivy GAP, RHUH, and UPenn, consistently outperforming\nbaseline approaches (p <= 0.05). Ablation studies confirmed that both the TAFE\nand CMD modules are essential for improving predictive accuracy. By integrating\nlarge-scale pretraining and task-specific fine-tuning, FoundBioNet enables\ngeneralizable glioma characterization. This approach enhances diagnostic\naccuracy and interpretability, with the potential to enable more personalized\npatient care.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faFoundBioNet\uff0c\u4e00\u4e2a\u57fa\u4e8e\u57fa\u7840\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u53c2\u6570MRI\u65e0\u521b\u51c6\u786e\u9884\u6d4b\u80f6\u8d28\u7624IDH\u7a81\u53d8\u72b6\u6001\u3002", "motivation": "\u4f20\u7edf\u80f6\u8d28\u7624\u5f02\u67e0\u6aac\u9178\u8131\u6c22\u9176\uff08IDH\uff09\u7a81\u53d8\u68c0\u6d4b\u65b9\u6cd5\u5177\u6709\u4fb5\u5165\u6027\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u80bf\u7624\u7a7a\u95f4\u5f02\u8d28\u6027\uff1b\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5e38\u53d7\u9650\u4e8e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u51c6\u786e\u3001\u975e\u4fb5\u5165\u6027\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684IDH\u7a81\u53d8\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51faFoundBioNet\uff08Foundation-based Biomarker Network\uff09\uff0c\u4e00\u4e2a\u57fa\u4e8eSWIN-UNETR\u67b6\u6784\u7684\u57fa\u7840\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528\u591a\u53c2\u6570MRI\u65e0\u521b\u9884\u6d4bIDH\u7a81\u53d8\u72b6\u6001\u3002\u8be5\u6a21\u578b\u6574\u5408\u4e86\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u80bf\u7624\u611f\u77e5\u7279\u5f81\u7f16\u7801\uff08TAFE\uff09\u7528\u4e8e\u63d0\u53d6\u591a\u5c3a\u5ea6\u3001\u4ee5\u80bf\u7624\u4e3a\u4e2d\u5fc3\u7684\u7279\u5f81\uff0c\u4ee5\u53ca\u8de8\u6a21\u6001\u5dee\u5f02\uff08CMD\uff09\u7528\u4e8e\u7a81\u51fa\u4e0eIDH\u7a81\u53d8\u76f8\u5173\u7684\u7ec6\u5faeT2-FLAIR\u9519\u914d\u4fe1\u53f7\u3002", "result": "\u8be5\u6a21\u578b\u5728\u6765\u81ea\u516d\u4e2a\u516c\u5171\u6570\u636e\u96c6\u76841705\u540d\u80f6\u8d28\u7624\u60a3\u8005\u7684\u591a\u5143\u591a\u4e2d\u5fc3\u961f\u5217\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002\u5728EGD\u3001TCGA\u3001Ivy GAP\u3001RHUH\u548cUPenn\u7684\u72ec\u7acb\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u6a21\u578b\u5206\u522b\u53d6\u5f97\u4e8690.58%\u300188.08%\u300165.41%\u548c80.31%\u7684AUC\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08p <= 0.05\uff09\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9eTAFE\u548cCMD\u6a21\u5757\u5bf9\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "FoundBioNet\u901a\u8fc7\u6574\u5408\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u80f6\u8d28\u7624\u7279\u5f81\u63cf\u8ff0\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u671b\u4e3a\u60a3\u8005\u63d0\u4f9b\u66f4\u4e2a\u6027\u5316\u7684\u62a4\u7406\u3002"}}
{"id": "2508.07209", "pdf": "https://arxiv.org/pdf/2508.07209", "abs": "https://arxiv.org/abs/2508.07209", "authors": ["Chaoqun Cui", "Siyuan Li", "Kunkun Ma", "Caiyan Jia"], "title": "Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model", "categories": ["cs.CL", "cs.SI"], "comment": "This paper is accepted by COLING2025", "summary": "Pretrained Language Models (PLMs) have excelled in various Natural Language\nProcessing tasks, benefiting from large-scale pretraining and self-attention\nmechanism's ability to capture long-range dependencies. However, their\nperformance on social media application tasks like rumor detection remains\nsuboptimal. We attribute this to mismatches between pretraining corpora and\nsocial texts, inadequate handling of unique social symbols, and pretraining\ntasks ill-suited for modeling user engagements implicit in propagation\nstructures. To address these issues, we propose a continue pretraining strategy\ncalled Post Engagement Prediction (PEP) to infuse information from propagation\nstructures into PLMs. PEP makes models to predict root, branch, and parent\nrelations between posts, capturing interactions of stance and sentiment crucial\nfor rumor detection. We also curate and release large-scale Twitter corpus:\nTwitterCorpus (269GB text), and two unlabeled claim conversation datasets with\npropagation structures (UTwitter and UWeibo). Utilizing these resources and PEP\nstrategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments\ndemonstrate PEP significantly boosts rumor detection performance across\nuniversal and social media PLMs, even in few-shot scenarios. On benchmark\ndatasets, PEP enhances baseline models by 1.0-3.7\\% accuracy, even enabling it\nto outperform current state-of-the-art methods on multiple datasets. SoLM\nalone, without high-level modules, also achieves competitive results,\nhighlighting the strategy's effectiveness in learning discriminative post\ninteraction features.", "AI": {"tldr": "\u73b0\u6709\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4ea4\u5a92\u4f53\u8c23\u8a00\u68c0\u6d4b\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51faPost Engagement Prediction (PEP)\u6301\u7eed\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u5efa\u6a21\u4f20\u64ad\u7ed3\u6784\u4e2d\u7684\u5e16\u5b50\u4e92\u52a8\u5173\u7cfb\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u5927\u89c4\u6a21\u793e\u4ea4\u8bed\u6599\u548c\u8bad\u7ec3\u51faSoLM\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660ePEP\u663e\u8457\u63d0\u5347\u4e86\u8c23\u8a00\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u5728\u901a\u7528NLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u793e\u4ea4\u5a92\u4f53\u5e94\u7528\uff08\u5982\u8c23\u8a00\u68c0\u6d4b\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u5f52\u56e0\u4e8e\u9884\u8bad\u7ec3\u8bed\u6599\u4e0e\u793e\u4ea4\u6587\u672c\u4e0d\u5339\u914d\u3001\u5bf9\u793e\u4ea4\u7b26\u53f7\u5904\u7406\u4e0d\u8db3\u4ee5\u53ca\u9884\u8bad\u7ec3\u4efb\u52a1\u672a\u80fd\u6709\u6548\u6355\u6349\u4f20\u64ad\u7ed3\u6784\u4e2d\u9690\u542b\u7684\u7528\u6237\u4e92\u52a8\u3002", "method": "\u672c\u6587\u63d0\u51faPost Engagement Prediction (PEP)\u6301\u7eed\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u4f7fPLMs\u9884\u6d4b\u5e16\u5b50\u95f4\u7684\u201c\u6839\u3001\u5206\u652f\u3001\u7236\u201d\u5173\u7cfb\uff0c\u4ee5\u6355\u6349\u4f20\u64ad\u7ed3\u6784\u4e2d\u9690\u542b\u7684\u7acb\u573a\u548c\u60c5\u611f\u4e92\u52a8\u3002\u540c\u65f6\uff0c\u672c\u6587\u6574\u7406\u5e76\u53d1\u5e03\u4e86\u5927\u89c4\u6a21Twitter\u8bed\u6599\u5e93\uff08TwitterCorpus\uff09\u4ee5\u53ca\u4e24\u4e2a\u5305\u542b\u4f20\u64ad\u7ed3\u6784\u7684\u65e0\u6807\u7b7e\u5bf9\u8bdd\u6570\u636e\u96c6\uff08UTwitter\u548cUWeibo\uff09\u3002\u5229\u7528\u8fd9\u4e9b\u8d44\u6e90\u548cPEP\u7b56\u7565\uff0c\u8bad\u7ec3\u4e86\u9488\u5bf9Twitter\u7684PLM\u6a21\u578bSoLM\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPEP\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u901a\u7528\u548c\u793e\u4ea4\u5a92\u4f53PLMs\u7684\u8c23\u8a00\u68c0\u6d4b\u6027\u80fd\uff0c\u5305\u62ec\u5728\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u3002PEP\u4f7f\u57fa\u7ebf\u6a21\u578b\u51c6\u786e\u7387\u63d0\u53471.0-3.7%\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002SoLM\u6a21\u578b\u5373\u4f7f\u4e0d\u4f7f\u7528\u9ad8\u7ea7\u6a21\u5757\uff0c\u4e5f\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u7a81\u663e\u4e86\u8be5\u7b56\u7565\u5728\u5b66\u4e60\u5224\u522b\u6027\u5e16\u5b50\u4e92\u52a8\u7279\u5f81\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "PEP\u6301\u7eed\u9884\u8bad\u7ec3\u7b56\u7565\u80fd\u6709\u6548\u5c06\u4f20\u64ad\u7ed3\u6784\u4fe1\u606f\u878d\u5165PLMs\uff0c\u663e\u8457\u63d0\u5347\u5176\u5728\u793e\u4ea4\u5a92\u4f53\u8c23\u8a00\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u4f7f\u5f97\u6a21\u578b\u80fd\u8d85\u8d8a\u73b0\u6709SOTA\uff0c\u9a8c\u8bc1\u4e86\u5efa\u6a21\u5e16\u5b50\u4e92\u52a8\u7279\u5f81\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.06915", "pdf": "https://arxiv.org/pdf/2508.06915", "abs": "https://arxiv.org/abs/2508.06915", "authors": ["Shichao Ma", "Zhengyang Zhou", "Qihe Huang", "Binwu Wang", "Kuo Yang", "Huan Li", "Yang Wang"], "title": "QuiZSF: An efficient data-model interaction framework for zero-shot time-series forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Time series forecasting has become increasingly important to empower diverse\napplications with streaming data. Zero-shot time-series forecasting (ZSF),\nparticularly valuable in data-scarce scenarios, such as domain transfer or\nforecasting under extreme conditions, is difficult for traditional models to\ndeal with. While time series pre-trained models (TSPMs) have demonstrated\nstrong performance in ZSF, they often lack mechanisms to dynamically\nincorporate external knowledge. Fortunately, emerging retrieval-augmented\ngeneration (RAG) offers a promising path for injecting such knowledge on\ndemand, yet they are rarely integrated with TSPMs. To leverage the strengths of\nboth worlds, we introduce RAG into TSPMs to enhance zero-shot time series\nforecasting. In this paper, we propose QuiZSF (Quick Zero-Shot Time Series\nForecaster), a lightweight and modular framework that couples efficient\nretrieval with representation learning and model adaptation for ZSF.\nSpecifically, we construct a hierarchical tree-structured ChronoRAG Base (CRB)\nfor scalable time-series storage and domain-aware retrieval, introduce a\nMulti-grained Series Interaction Learner (MSIL) to extract fine- and\ncoarse-grained relational features, and develop a dual-branch Model Cooperation\nCoherer (MCC) that aligns retrieved knowledge with two kinds of TSPMs: Non-LLM\nbased and LLM based. Compared with contemporary baselines, QuiZSF, with Non-LLM\nbased and LLM based TSPMs as base model, respectively, ranks Top1 in 75% and\n87.5% of prediction settings, while maintaining high efficiency in memory and\ninference time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86QuiZSF\uff0c\u4e00\u4e2a\u5c06\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5f15\u5165\u65f6\u95f4\u5e8f\u5217\u9884\u8bad\u7ec3\u6a21\u578b\uff08TSPMs\uff09\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u6574\u5408\u5916\u90e8\u77e5\u8bc6\uff0c\u63d0\u5347\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08ZSF\uff09\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08ZSF\uff09\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u5e94\u5bf9\u3002\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u8bad\u7ec3\u6a21\u578b\uff08TSPMs\uff09\u867d\u7136\u5728ZSF\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u52a8\u6001\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u5c06\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e0eTSPMs\u7ed3\u5408\uff0c\u4ee5\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\u5e76\u589e\u5f3aZSF\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86QuiZSF\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u9ad8\u6548\u68c0\u7d22\u3001\u8868\u793a\u5b66\u4e60\u548c\u6a21\u578b\u81ea\u9002\u5e94\uff1a\n1.  **ChronoRAG Base (CRB)**\uff1a\u6784\u5efa\u5206\u5c42\u6811\u72b6\u7ed3\u6784\uff0c\u7528\u4e8e\u53ef\u4f38\u7f29\u7684\u65f6\u95f4\u5e8f\u5217\u5b58\u50a8\u548c\u9886\u57df\u611f\u77e5\u68c0\u7d22\u3002\n2.  **Multi-grained Series Interaction Learner (MSIL)**\uff1a\u7528\u4e8e\u63d0\u53d6\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u5173\u7cfb\u7279\u5f81\u3002\n3.  **Model Cooperation Coherer (MCC)**\uff1a\u5f00\u53d1\u4e00\u4e2a\u53cc\u5206\u652f\u6a21\u5757\uff0c\u5c06\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\u4e0e\u57fa\u4e8e\u975eLLM\u548c\u57fa\u4e8eLLM\u7684\u4e24\u79cdTSPMs\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "QuiZSF\u5728\u5206\u522b\u4f7f\u7528\u57fa\u4e8e\u975eLLM\u548c\u57fa\u4e8eLLM\u7684TSPMs\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u65f6\uff0c\u572875%\u548c87.5%\u7684\u9884\u6d4b\u8bbe\u7f6e\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u8d85\u8d8a\u4e86\u5f53\u4ee3\u57fa\u7ebf\uff0c\u540c\u65f6\u5728\u5185\u5b58\u548c\u63a8\u7406\u65f6\u95f4\u65b9\u9762\u4fdd\u6301\u4e86\u9ad8\u6548\u7387\u3002", "conclusion": "QuiZSF\u901a\u8fc7\u6709\u6548\u6574\u5408RAG\u4e0eTSPMs\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u52a8\u6001\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07334", "pdf": "https://arxiv.org/pdf/2508.07334", "abs": "https://arxiv.org/abs/2508.07334", "authors": ["Quan Shi", "Wang Xi", "Zenghui Ding", "Jianqing Gao", "Xianjun Yang"], "title": "Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape", "categories": ["cs.AI"], "comment": "8 pages, 6 figures", "summary": "The illusion phenomenon of large language models (LLMs) is the core obstacle\nto their reliable deployment. This article formalizes the large language model\nas a probabilistic Turing machine by constructing a \"computational necessity\nhierarchy\", and for the first time proves the illusions are inevitable on\ndiagonalization, incomputability, and information theory boundaries supported\nby the new \"learner pump lemma\". However, we propose two \"escape routes\": one\nis to model Retrieval Enhanced Generations (RAGs) as oracle machines, proving\ntheir absolute escape through \"computational jumps\", providing the first formal\ntheory for the effectiveness of RAGs; The second is to formalize continuous\nlearning as an \"internalized oracle\" mechanism and implement this path through\na novel neural game theory framework.Finally, this article proposes a", "AI": {"tldr": "\u672c\u6587\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5f62\u5f0f\u5316\u4e3a\u6982\u7387\u56fe\u7075\u673a\uff0c\u9996\u6b21\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86LLM\u5e7b\u89c9\u7684\u4e0d\u53ef\u907f\u514d\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u201c\u9003\u9038\u8def\u5f84\u201d\uff1a\u901a\u8fc7\u5c06RAG\u5efa\u6a21\u4e3a\u9884\u8a00\u673a\u5b9e\u73b0\u8ba1\u7b97\u8df3\u8dc3\uff0c\u4ee5\u53ca\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u535a\u5f08\u8bba\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5e7b\u89c9\u73b0\u8c61\u662f\u5176\u53ef\u9760\u90e8\u7f72\u7684\u6838\u5fc3\u969c\u788d\uff0c\u4e9f\u9700\u4ece\u7406\u8bba\u4e0a\u89e3\u91ca\u5e76\u5bfb\u627e\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u5c06LLM\u5f62\u5f0f\u5316\u4e3a\u6982\u7387\u56fe\u7075\u673a\uff0c\u901a\u8fc7\u6784\u5efa\u201c\u8ba1\u7b97\u5fc5\u8981\u6027\u5c42\u6b21\u7ed3\u6784\u201d\u548c\u201c\u5b66\u4e60\u5668\u6cf5\u5f15\u7406\u201d\u6765\u5206\u6790\u5e7b\u89c9\u3002\n2. \u63d0\u51fa\u4e24\u79cd\u201c\u9003\u9038\u8def\u5f84\u201d\uff1aa) \u5c06\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5efa\u6a21\u4e3a\u9884\u8a00\u673a\uff0c\u901a\u8fc7\u201c\u8ba1\u7b97\u8df3\u8dc3\u201d\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002b) \u5c06\u6301\u7eed\u5b66\u4e60\u5f62\u5f0f\u5316\u4e3a\u201c\u5185\u5316\u9884\u8a00\u673a\u201d\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u65b0\u9896\u7684\u795e\u7ecf\u7f51\u7edc\u535a\u5f08\u8bba\u6846\u67b6\u5b9e\u73b0\u3002", "result": "1. \u9996\u6b21\u8bc1\u660e\u4e86LLM\u5e7b\u89c9\u5728\u5bf9\u89d2\u5316\u3001\u4e0d\u53ef\u8ba1\u7b97\u6027\u53ca\u4fe1\u606f\u8bba\u8fb9\u754c\u4e0a\u7684\u5fc5\u7136\u6027\u3002\n2. \u9996\u6b21\u4e3aRAG\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u7406\u8bba\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u7edd\u5bf9\u6d88\u9664\u5e7b\u89c9\u3002\n3. \u63d0\u51fa\u4e86\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u535a\u5f08\u8bba\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u7684\u7406\u8bba\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u6df1\u5165\u5256\u6790\u4e86LLM\u5e7b\u89c9\u7684\u7406\u8bba\u6839\u6e90\u4e0e\u4e0d\u53ef\u907f\u514d\u6027\uff0c\u66f4\u4e3a\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u53ef\u884c\u7684\u201c\u9003\u9038\u8def\u5f84\u201d\uff0c\u7279\u522b\u662f\u5728RAG\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u9762\uff0c\u4e3aLLM\u7684\u53ef\u9760\u5e94\u7528\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.06757", "pdf": "https://arxiv.org/pdf/2508.06757", "abs": "https://arxiv.org/abs/2508.06757", "authors": ["Yash Garg", "Saketh Bachu", "Arindam Dutta", "Rohit Lal", "Sarosij Bose", "Calvin-Khang Ta", "M. Salman Asif", "Amit Roy-Chowdhury"], "title": "VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Human pose and shape (HPS) estimation methods have been extensively studied,\nwith many demonstrating high zero-shot performance on in-the-wild images and\nvideos. However, these methods often struggle in challenging scenarios\ninvolving complex human poses or significant occlusions. Although some studies\naddress 3D human pose estimation under occlusion, they typically evaluate\nperformance on datasets that lack realistic or substantial occlusions, e.g.,\nmost existing datasets introduce occlusions with random patches over the human\nor clipart-style overlays, which may not reflect real-world challenges. To\nbridge this gap in realistic occlusion datasets, we introduce a novel benchmark\ndataset, VOccl3D, a Video-based human Occlusion dataset with 3D body pose and\nshape annotations. Inspired by works such as AGORA and BEDLAM, we constructed\nthis dataset using advanced computer graphics rendering techniques,\nincorporating diverse real-world occlusion scenarios, clothing textures, and\nhuman motions. Additionally, we fine-tuned recent HPS methods, CLIFF and\nBEDLAM-CLIFF, on our dataset, demonstrating significant qualitative and\nquantitative improvements across multiple public datasets, as well as on the\ntest split of our dataset, while comparing its performance with other\nstate-of-the-art methods. Furthermore, we leveraged our dataset to enhance\nhuman detection performance under occlusion by fine-tuning an existing object\ndetector, YOLO11, thus leading to a robust end-to-end HPS estimation system\nunder occlusions. Overall, this dataset serves as a valuable resource for\nfuture research aimed at benchmarking methods designed to handle occlusions,\noffering a more realistic alternative to existing occlusion datasets. See the\nProject page for code and dataset:https://yashgarg98.github.io/VOccl3D-dataset/", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aVOccl3D\u7684\u89c6\u9891\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u903c\u771f\u76843D\u4eba\u4f53\u59ff\u6001\u548c\u5f62\u72b6\u906e\u6321\u573a\u666f\uff0c\u65e8\u5728\u5f25\u8865\u73b0\u6709\u6570\u636e\u96c6\u5728\u771f\u5b9e\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u8be5\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u4eba\u4f53\u68c0\u6d4b\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u59ff\u6001\u548c\u5f62\u72b6\uff08HPS\uff09\u4f30\u8ba1\u65b9\u6cd5\u5728\u590d\u6742\u59ff\u6001\u6216\u663e\u8457\u906e\u6321\u7684\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1\u6709\u7814\u7a76\u5173\u6ce8\u906e\u6321\u4e0b\u76843D HPS\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u7684\u906e\u6321\u901a\u5e38\u4e0d\u771f\u5b9e\u6216\u4e0d\u5145\u5206\uff08\u4f8b\u5982\uff0c\u4f7f\u7528\u968f\u673a\u8865\u4e01\u6216\u526a\u8d34\u753b\u8986\u76d6\uff09\uff0c\u65e0\u6cd5\u6709\u6548\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u7684\u6311\u6218\u3002", "method": "\u7814\u7a76\u5f15\u5165\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aVOccl3D\u7684\u89c6\u9891\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b3D\u4eba\u4f53\u59ff\u6001\u548c\u5f62\u72b6\u6807\u6ce8\uff0c\u901a\u8fc7\u5148\u8fdb\u7684\u8ba1\u7b97\u673a\u56fe\u5f62\u6e32\u67d3\u6280\u672f\u751f\u6210\uff0c\u878d\u5408\u4e86\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u906e\u6321\u573a\u666f\u3001\u670d\u88c5\u7eb9\u7406\u548c\u4eba\u4f53\u52a8\u4f5c\u3002\u6b64\u5916\uff0c\u7814\u7a76\u4eba\u5458\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u4e86\u73b0\u6709\u7684HPS\u65b9\u6cd5\uff08CLIFF\u548cBEDLAM-CLIFF\uff09\uff0c\u5e76\u5fae\u8c03\u4e86YOLO11\u76ee\u6807\u68c0\u6d4b\u5668\u4ee5\u589e\u5f3a\u906e\u6321\u4e0b\u7684\u4eba\u4f53\u68c0\u6d4b\u6027\u80fd\uff0c\u4ece\u800c\u6784\u5efa\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u7aef\u5230\u7aefHPS\u4f30\u8ba1\u7cfb\u7edf\u3002", "result": "\u5728VOccl3D\u6570\u636e\u96c6\u548c\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684HPS\u65b9\u6cd5\uff08CLIFF\u548cBEDLAM-CLIFF\uff09\u5c55\u73b0\u51fa\u663e\u8457\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u6539\u8fdb\uff0c\u5e76\u4f18\u4e8e\u5176\u4ed6SOTA\u65b9\u6cd5\u3002\u901a\u8fc7\u5fae\u8c03YOLO11\uff0c\u6709\u6548\u63d0\u5347\u4e86\u906e\u6321\u4e0b\u7684\u4eba\u4f53\u68c0\u6d4b\u6027\u80fd\uff0c\u6700\u7ec8\u5f62\u6210\u4e86\u4e00\u4e2a\u5728\u906e\u6321\u6761\u4ef6\u4e0b\u66f4\u4e3a\u9c81\u68d2\u7684\u7aef\u5230\u7aefHPS\u4f30\u8ba1\u7cfb\u7edf\u3002", "conclusion": "VOccl3D\u6570\u636e\u96c6\u662f\u4e00\u4e2a\u5b9d\u8d35\u7684\u8d44\u6e90\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u65e8\u5728\u5904\u7406\u906e\u6321\u95ee\u9898\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u662f\u73b0\u6709\u906e\u6321\u6570\u636e\u96c6\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.07229", "pdf": "https://arxiv.org/pdf/2508.07229", "abs": "https://arxiv.org/abs/2508.07229", "authors": ["Itai Allouche", "Itay Asael", "Rotem Rousso", "Vered Dassa", "Ann Bradlow", "Seung-Eun Kim", "Matthew Goldrick", "Joseph Keshet"], "title": "How Does a Deep Neural Network Look at Lexical Stress?", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "10 pages, 4 figures, submitted to the Journal of the Acoustical\n  Society of America (JASA)", "summary": "Despite their success in speech processing, neural networks often operate as\nblack boxes, prompting the question: what informs their decisions, and how can\nwe interpret them? This work examines this issue in the context of lexical\nstress. A dataset of English disyllabic words was automatically constructed\nfrom read and spontaneous speech. Several Convolutional Neural Network (CNN)\narchitectures were trained to predict stress position from a spectrographic\nrepresentation of disyllabic words lacking minimal stress pairs (e.g., initial\nstress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out\ntest data. Layerwise Relevance Propagation (LRP), a technique for CNN\ninterpretability analysis, revealed that predictions for held-out minimal pairs\n(PROtest vs. proTEST ) were most strongly influenced by information in stressed\nversus unstressed syllables, particularly the spectral properties of stressed\nvowels. However, the classifiers also attended to information throughout the\nword. A feature-specific relevance analysis is proposed, and its results\nsuggest that our best-performing classifier is strongly influenced by the\nstressed vowel's first and second formants, with some evidence that its pitch\nand third formant also contribute. These results reveal deep learning's ability\nto acquire distributed cues to stress from naturally occurring data, extending\ntraditional phonetic work based around highly controlled stimuli.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8bcd\u6c47\u91cd\u97f3\u9884\u6d4b\u4e0a\u8868\u73b0\u4f18\u5f02\uff08\u6700\u9ad892%\u51c6\u786e\u7387\uff09\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u5206\u6790\u53d1\u73b0\u5176\u4e3b\u8981\u4f9d\u8d56\u91cd\u8bfb\u5143\u97f3\u7684\u9891\u8c31\u7279\u6027\uff08\u7279\u522b\u662f\u5171\u632f\u5cf0\uff09\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u5b66\u4e60\u4ece\u81ea\u7136\u8bed\u6599\u4e2d\u83b7\u53d6\u5206\u5e03\u5f0f\u91cd\u97f3\u7ebf\u7d22\u7684\u80fd\u529b\uff0c\u5e76\u62d3\u5bbd\u4e86\u4f20\u7edf\u8bed\u97f3\u5b66\u7814\u7a76\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u7f51\u7edc\u5728\u8bed\u97f3\u5904\u7406\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5176\u4f5c\u4e3a\u201c\u9ed1\u7bb1\u201d\u6a21\u578b\uff0c\u5176\u51b3\u7b56\u4f9d\u636e\u53ca\u5176\u89e3\u91ca\u6027\u4ecd\u662f\u672a\u89e3\u4e4b\u8c1c\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u8bcd\u6c47\u91cd\u97f3\u9884\u6d4b\u80cc\u666f\u4e0b\u3002", "method": "\u81ea\u52a8\u6784\u5efa\u4e86\u5305\u542b\u82f1\u8bed\u53cc\u97f3\u8282\u8bcd\u7684\u6717\u8bfb\u53ca\u81ea\u7136\u8bed\u97f3\u6570\u636e\u96c6\u3002\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\uff0c\u4ece\u53cc\u97f3\u8282\u8bcd\u7684\u9891\u8c31\u56fe\u9884\u6d4b\u91cd\u97f3\u4f4d\u7f6e\u3002\u91c7\u7528\u5c42\u7ea7\u76f8\u5173\u6027\u4f20\u64ad\uff08LRP\uff09\u548c\u7279\u5f81\u7279\u5f02\u6027\u76f8\u5173\u6027\u5206\u6790\u7b49\u53ef\u89e3\u91ca\u6027\u6280\u672f\u6765\u63ed\u793a\u6a21\u578b\u7684\u51b3\u7b56\u4f9d\u636e\u3002", "result": "CNN\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe92%\u7684\u91cd\u97f3\u4f4d\u7f6e\u9884\u6d4b\u51c6\u786e\u7387\u3002\u53ef\u89e3\u91ca\u6027\u5206\u6790\u8868\u660e\uff0c\u6a21\u578b\u9884\u6d4b\u4e3b\u8981\u53d7\u91cd\u8bfb\u97f3\u8282\u4e0e\u975e\u91cd\u8bfb\u97f3\u8282\u4fe1\u606f\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u91cd\u8bfb\u5143\u97f3\u7684\u9891\u8c31\u7279\u6027\u3002\u5177\u4f53\u5730\uff0c\u6a21\u578b\u7684\u6700\u4f73\u8868\u73b0\u4e3b\u8981\u7531\u91cd\u8bfb\u5143\u97f3\u7684\u7b2c\u4e00\u548c\u7b2c\u4e8c\u5171\u632f\u5cf0\u51b3\u5b9a\uff0c\u97f3\u9ad8\u548c\u7b2c\u4e09\u5171\u632f\u5cf0\u4ea6\u6709\u8d21\u732e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u6df1\u5ea6\u5b66\u4e60\u80fd\u591f\u4ece\u81ea\u7136\u8bed\u6599\u4e2d\u6709\u6548\u6355\u83b7\u5206\u5e03\u5f0f\u91cd\u97f3\u7ebf\u7d22\uff0c\u4e3a\u4f20\u7edf\u57fa\u4e8e\u9ad8\u5ea6\u53d7\u63a7\u523a\u6fc0\u7684\u8bed\u97f3\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u6269\u5c55\u3002"}}
{"id": "2508.06943", "pdf": "https://arxiv.org/pdf/2508.06943", "abs": "https://arxiv.org/abs/2508.06943", "authors": ["Lishi Zuo", "Man-Wai Mak", "Lu Yi", "Youzhi Tu"], "title": "Class Unbiasing for Generalization in Medical Diagnosis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Medical diagnosis might fail due to bias. In this work, we identified\nclass-feature bias, which refers to models' potential reliance on features that\nare strongly correlated with only a subset of classes, leading to biased\nperformance and poor generalization on other classes. We aim to train a\nclass-unbiased model (Cls-unbias) that mitigates both class imbalance and\nclass-feature bias simultaneously. Specifically, we propose a class-wise\ninequality loss which promotes equal contributions of classification loss from\npositive-class and negative-class samples. We propose to optimize a class-wise\ngroup distributionally robust optimization objective-a class-weighted training\nobjective that upweights underperforming classes-to enhance the effectiveness\nof the inequality loss under class imbalance. Through synthetic and real-world\ndatasets, we empirically demonstrate that class-feature bias can negatively\nimpact model performance. Our proposed method effectively mitigates both\nclass-feature bias and class imbalance, thereby improving the model's\ngeneralization ability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCls-unbias\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u7c7b\u522b\u4e0d\u7b49\u5f0f\u635f\u5931\u548c\u7c7b\u522b\u52a0\u6743\u4f18\u5316\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u533b\u5b66\u8bca\u65ad\u4e2d\u5b58\u5728\u7684\u7c7b\u522b\u7279\u5f81\u504f\u5dee\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u533b\u5b66\u8bca\u65ad\u53ef\u80fd\u56e0\u504f\u5dee\u800c\u5931\u8d25\uff0c\u7279\u522b\u662f\u6a21\u578b\u53ef\u80fd\u4f9d\u8d56\u4e8e\u4ec5\u4e0e\u90e8\u5206\u7c7b\u522b\u5f3a\u76f8\u5173\u7684\u7279\u5f81\uff08\u5373\u7c7b\u522b\u7279\u5f81\u504f\u5dee\uff09\uff0c\u5bfc\u81f4\u6027\u80fd\u504f\u5dee\u548c\u5bf9\u5176\u4ed6\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u7814\u7a76\u65e8\u5728\u8bad\u7ec3\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u7c7b\u522b\u7279\u5f81\u504f\u5dee\u7684\u65e0\u504f\u6a21\u578b\u3002", "method": ["\u63d0\u51fa\u4e00\u79cd\u7c7b\u522b\u4e0d\u7b49\u5f0f\u635f\u5931\uff08class-wise inequality loss\uff09\uff0c\u65e8\u5728\u4fc3\u8fdb\u6b63\u7c7b\u548c\u8d1f\u7c7b\u6837\u672c\u7684\u5206\u7c7b\u635f\u5931\u8d21\u732e\u76f8\u7b49\u3002", "\u63d0\u51fa\u4f18\u5316\u4e00\u4e2a\u7c7b\u522b\u7ec4\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u76ee\u6807\uff08class-wise group distributionally robust optimization objective\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7c7b\u522b\u52a0\u6743\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u901a\u8fc7\u4e0a\u8c03\u8868\u73b0\u4e0d\u4f73\u7684\u7c7b\u522b\uff0c\u4ee5\u589e\u5f3a\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u4e0d\u7b49\u5f0f\u635f\u5931\u7684\u6709\u6548\u6027\u3002"], "result": ["\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u7ecf\u9a8c\u6027\u5730\u8bc1\u660e\u4e86\u7c7b\u522b\u7279\u5f81\u504f\u5dee\u4f1a\u5bf9\u6a21\u578b\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u7c7b\u522b\u7279\u5f81\u504f\u5dee\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"], "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684Cls-unbias\u6a21\u578b\u80fd\u591f\u6709\u6548\u89e3\u51b3\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u7c7b\u522b\u7279\u5f81\u504f\u5dee\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.07353", "pdf": "https://arxiv.org/pdf/2508.07353", "abs": "https://arxiv.org/abs/2508.07353", "authors": ["Rubing Chen", "Jiaxin Wu", "Jian Wang", "Xulu Zhang", "Wenqi Fan", "Chenghua Lin", "Xiao-Yong Wei", "Qing Li"], "title": "Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Numerous benchmarks have been built to evaluate the domain-specific abilities\nof large language models (LLMs), highlighting the need for effective and\nefficient benchmark construction. Existing domain-specific benchmarks primarily\nfocus on the scaling law, relying on massive corpora for supervised fine-tuning\nor generating extensive question sets for broad coverage. However, the impact\nof corpus and question-answer (QA) set design on the precision and recall of\ndomain-specific LLMs remains unexplored. In this paper, we address this gap and\ndemonstrate that the scaling law is not always the optimal principle for\nbenchmark construction in specific domains. Instead, we propose Comp-Comp, an\niterative benchmarking framework based on a comprehensiveness-compactness\nprinciple. Here, comprehensiveness ensures semantic recall of the domain, while\ncompactness enhances precision, guiding both corpus and QA set construction. To\nvalidate our framework, we conducted a case study in a well-renowned\nuniversity, resulting in the creation of XUBench, a large-scale and\ncomprehensive closed-domain benchmark. Although we use the academic domain as\nthe case in this work, our Comp-Comp framework is designed to be extensible\nbeyond academia, providing valuable insights for benchmark construction across\nvarious domains.", "AI": {"tldr": "\u9488\u5bf9\u9886\u57df\u7279\u5b9aLLM\u57fa\u51c6\u6784\u5efa\u6548\u7387\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u5168\u9762\u6027-\u7d27\u51d1\u6027\u539f\u5219\u7684Comp-Comp\u6846\u67b6\uff0c\u5e76\u521b\u5efa\u4e86XUBench\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u9886\u57df\u7279\u5b9aLLM\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u7f29\u653e\u5b9a\u5f8b\uff0c\u4f46\u8bed\u6599\u5e93\u548c\u95ee\u7b54\u96c6\u8bbe\u8ba1\u5bf9\u9886\u57dfLLM\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u7684\u5f71\u54cd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e76\u8d28\u7591\u7f29\u653e\u5b9a\u5f8b\u5728\u7279\u5b9a\u9886\u57df\u57fa\u51c6\u6784\u5efa\u4e2d\u7684\u6700\u4f18\u6027\u3002", "method": "\u63d0\u51faComp-Comp\u8fed\u4ee3\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u201c\u5168\u9762\u6027-\u7d27\u51d1\u6027\u201d\u539f\u5219\u3002\u5176\u4e2d\uff0c\u5168\u9762\u6027\u786e\u4fdd\u9886\u57df\u8bed\u4e49\u53ec\u56de\uff0c\u7d27\u51d1\u6027\u63d0\u5347\u7cbe\u5ea6\uff0c\u4e24\u8005\u5171\u540c\u6307\u5bfc\u8bed\u6599\u5e93\u548c\u95ee\u7b54\u96c6\u7684\u6784\u5efa\u3002", "result": "\u901a\u8fc7\u5728\u4e00\u4e2a\u8457\u540d\u5927\u5b66\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u6210\u529f\u521b\u5efa\u4e86XUBench\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u5168\u9762\u7684\u5c01\u95ed\u9886\u57df\u57fa\u51c6\u3002", "conclusion": "Comp-Comp\u6846\u67b6\u867d\u4ee5\u5b66\u672f\u9886\u57df\u4e3a\u4f8b\uff0c\u4f46\u5176\u8bbe\u8ba1\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u4e3a\u8de8\u9886\u57df\u7684\u57fa\u51c6\u6784\u5efa\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.06763", "pdf": "https://arxiv.org/pdf/2508.06763", "abs": "https://arxiv.org/abs/2508.06763", "authors": ["Zihao Sheng", "Zilin Huang", "Yen-Jung Chen", "Yansong Qu", "Yuhao Luo", "Yue Leng", "Sikai Chen"], "title": "SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "The code, dataset, and model checkpoints will be made publicly\n  available at: https://zihaosheng.github.io/SafePLUG", "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress\nacross a range of vision-language tasks and demonstrate strong potential for\ntraffic accident understanding. However, existing MLLMs in this domain\nprimarily focus on coarse-grained image-level or video-level comprehension and\noften struggle to handle fine-grained visual details or localized scene\ncomponents, limiting their applicability in complex accident scenarios. To\naddress these limitations, we propose SafePLUG, a novel framework that empowers\nMLLMs with both Pixel-Level Understanding and temporal Grounding for\ncomprehensive traffic accident analysis. SafePLUG supports both\narbitrary-shaped visual prompts for region-aware question answering and\npixel-level segmentation based on language instructions, while also enabling\nthe recognition of temporally anchored events in traffic accident scenarios. To\nadvance the development of MLLMs for traffic accident understanding, we curate\na new dataset containing multimodal question-answer pairs centered on diverse\naccident scenarios, with detailed pixel-level annotations and temporal event\nboundaries. Experimental results show that SafePLUG achieves strong performance\non multiple tasks, including region-based question answering, pixel-level\nsegmentation, temporal event localization, and accident event understanding.\nThese capabilities lay a foundation for fine-grained understanding of complex\ntraffic scenes, with the potential to improve driving safety and enhance\nsituational awareness in smart transportation systems. The code, dataset, and\nmodel checkpoints will be made publicly available at:\nhttps://zihaosheng.github.io/SafePLUG", "AI": {"tldr": "SafePLUG\u901a\u8fc7\u50cf\u7d20\u7ea7\u7406\u89e3\u548c\u65f6\u95f4\u5b9a\u4f4d\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u4ea4\u901a\u4e8b\u6545\u5206\u6790\u4e2d\u7ec6\u7c92\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86\u65b0\u7684\u4ea4\u901a\u4e8b\u6545\u7406\u89e3\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u4ea4\u901a\u4e8b\u6545\u7406\u89e3\u65b9\u9762\u4e3b\u8981\u4fa7\u91cd\u7c97\u7c92\u5ea6\u5206\u6790\uff0c\u96be\u4ee5\u5904\u7406\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u6216\u5c40\u90e8\u573a\u666f\u7ec4\u4ef6\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u4e8b\u6545\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u6587\u63d0\u51faSafePLUG\u6846\u67b6\uff0c\u8d4b\u4e88MLLMs\u50cf\u7d20\u7ea7\u7406\u89e3\u548c\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u4ee5\u5b9e\u73b0\u5168\u9762\u7684\u4ea4\u901a\u4e8b\u6545\u5206\u6790\u3002SafePLUG\u652f\u6301\u533a\u57df\u611f\u77e5\u95ee\u7b54\u3001\u50cf\u7d20\u7ea7\u5206\u5272\u548c\u65f6\u95f4\u4e8b\u4ef6\u8bc6\u522b\u3002\u6b64\u5916\uff0c\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8be6\u7ec6\u50cf\u7d20\u7ea7\u6807\u6ce8\u548c\u65f6\u95f4\u4e8b\u4ef6\u8fb9\u754c\u7684\u65b0\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSafePLUG\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u57fa\u4e8e\u533a\u57df\u7684\u95ee\u7b54\u3001\u50cf\u7d20\u7ea7\u5206\u5272\u3001\u65f6\u95f4\u4e8b\u4ef6\u5b9a\u4f4d\u548c\u4e8b\u6545\u4e8b\u4ef6\u7406\u89e3\u3002", "conclusion": "SafePLUG\u7684\u80fd\u529b\u4e3a\u590d\u6742\u4ea4\u901a\u573a\u666f\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u671b\u63d0\u5347\u9a7e\u9a76\u5b89\u5168\u5e76\u589e\u5f3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u6001\u52bf\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2508.07248", "pdf": "https://arxiv.org/pdf/2508.07248", "abs": "https://arxiv.org/abs/2508.07248", "authors": ["Zhe Ren"], "title": "Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge distillation has been successfully applied to Continual Learning\nNamed Entity Recognition (CLNER) tasks, by using a teacher model trained on\nold-class data to distill old-class entities present in new-class data as a\nform of regularization, thereby avoiding catastrophic forgetting. However, in\nFew-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it\ndifficult for the trained model to generalize during inference. More\ncritically, the lack of old-class entity information hinders the distillation\nof old knowledge, causing the model to fall into what we refer to as the\nFew-Shot Distillation Dilemma. In this work, we address the above challenges\nthrough a prompt tuning paradigm and memory demonstration template strategy.\nSpecifically, we designed an expandable Anchor words-oriented Prompt Tuning\n(APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby\nenhancing performance in few-shot scenarios. Additionally, we incorporated\nMemory Demonstration Templates (MDT) into each training instance to provide\nreplay samples from previous tasks, which not only avoids the Few-Shot\nDistillation Dilemma but also promotes in-context learning. Experiments show\nthat our approach achieves competitive performances on FS-CLNER.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u63d0\u793a\u8c03\u4f18\uff08APT\uff09\u548c\u8bb0\u5fc6\u6f14\u793a\u6a21\u677f\uff08MDT\uff09\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u5c11\u6837\u672c\u6301\u7eed\u5b66\u4e60\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08FS-CLNER\uff09\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u65e7\u77e5\u8bc6\u84b8\u998f\u53d7\u963b\u548c\u65b0\u7c7b\u522b\u6cdb\u5316\u56f0\u96be\u7b49\u95ee\u9898\u3002", "motivation": "\u5728\u5c11\u6837\u672c\u6301\u7eed\u5b66\u4e60\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08FS-CLNER\uff09\u4efb\u52a1\u4e2d\uff0c\u65b0\u7c7b\u522b\u5b9e\u4f53\u7a00\u7f3a\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u6cdb\u5316\uff0c\u4e14\u65e7\u7c7b\u522b\u5b9e\u4f53\u4fe1\u606f\u4e0d\u8db3\u4f1a\u963b\u788d\u77e5\u8bc6\u84b8\u998f\uff0c\u9020\u6210\u201c\u5c11\u6837\u672c\u84b8\u998f\u56f0\u5883\u201d\uff0c\u4ece\u800c\u5f71\u54cd\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u63d0\u793a\u8c03\u4f18\u8303\u5f0f\u548c\u8bb0\u5fc6\u6f14\u793a\u6a21\u677f\u7b56\u7565\u89e3\u51b3\u6311\u6218\u3002\u5177\u4f53\u5305\u62ec\uff1a1) \u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u951a\u8bcd\u5bfc\u5411\u63d0\u793a\u8c03\u4f18\uff08APT\uff09\u8303\u5f0f\uff0c\u4ee5\u5f25\u5408\u9884\u8bad\u7ec3\u4e0e\u5fae\u8c03\u7684\u5dee\u8ddd\uff0c\u589e\u5f3a\u5c11\u6837\u672c\u6027\u80fd\u30022) \u5f15\u5165\u8bb0\u5fc6\u6f14\u793a\u6a21\u677f\uff08MDT\uff09\u5230\u6bcf\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u4e2d\uff0c\u63d0\u4f9b\u65e7\u4efb\u52a1\u56de\u653e\u6837\u672c\uff0c\u4ee5\u907f\u514d\u84b8\u998f\u56f0\u5883\u5e76\u4fc3\u8fdb\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u6301\u7eed\u5b66\u4e60\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08FS-CLNER\uff09\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ed3\u5408APT\u548cMDT\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u6301\u7eed\u5b66\u4e60\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u65e7\u77e5\u8bc6\u84b8\u998f\u6548\u7387\u3002"}}
{"id": "2508.06944", "pdf": "https://arxiv.org/pdf/2508.06944", "abs": "https://arxiv.org/abs/2508.06944", "authors": ["Lixuan He", "Jie Feng", "Yong Li"], "title": "AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Large Language Models (LLMs) are typically fine-tuned for reasoning tasks\nthrough a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by\nReinforcement Learning (RL), a process fraught with catastrophic forgetting and\nsuboptimal trade-offs between imitation and exploration. Recent single-stage\nmethods attempt to unify SFT and RL using heuristics, but lack a principled\nmechanism for dynamically balancing the two paradigms. In this paper, we\nreframe this challenge through the theoretical lens of \\textbf{implicit\nrewards}, viewing SFT and RL not as distinct methods but as complementary\nreward signals. We introduce \\textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel\nsingle-stage algorithm that learns the optimal balance between SFT's implicit,\npath-level reward and RL's explicit, outcome-based reward. The core of AMFT is\na \\textbf{meta-gradient adaptive weight controller} that treats the SFT-RL\nbalance as a learnable parameter, dynamically optimizing it to maximize\nlong-term task performance. This forward-looking approach, regularized by\npolicy entropy for stability, autonomously discovers an effective training\ncurriculum. We conduct a comprehensive evaluation on challenging benchmarks\nspanning mathematical reasoning, abstract visual reasoning (General Points),\nand vision-language navigation (V-IRL). AMFT consistently establishes a new\nstate-of-the-art and demonstrats superior generalization on out-of-distribution\n(OOD) tasks. Ablation studies and training dynamic analysis confirm that the\nmeta-learning controller is crucial for AMFT's stability, sample efficiency,\nand performance, offering a more principled and effective paradigm for LLM\nalignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.", "AI": {"tldr": "AMFT\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5355\u9636\u6bb5LLM\u5fae\u8c03\u7b97\u6cd5\uff0c\u901a\u8fc7\u5143\u68af\u5ea6\u81ea\u9002\u5e94\u6743\u91cd\u63a7\u5236\u5668\u52a8\u6001\u5e73\u8861SFT\u548cRL\uff0c\u5b9e\u73b0SOTA\u6027\u80fd\u548c\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfLLM\u5fae\u8c03\u901a\u5e38\u91c7\u7528SFT\u540eRL\u7684\u4e24\u9636\u6bb5\u6d41\u7a0b\uff0c\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u6a21\u4eff\u4e0e\u63a2\u7d22\u6b21\u4f18\u6743\u8861\u7684\u95ee\u9898\u3002\u73b0\u6709\u5355\u9636\u6bb5\u65b9\u6cd5\u7f3a\u4e4f\u52a8\u6001\u5e73\u8861SFT\u548cRL\u7684\u539f\u5219\u6027\u673a\u5236\u3002", "method": "\u5c06SFT\u548cRL\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e92\u8865\u7684\u9690\u5f0f\u548c\u663e\u5f0f\u5956\u52b1\u4fe1\u53f7\u3002\u63d0\u51faAdaptive Meta Fine-Tuning (AMFT) \u7b97\u6cd5\uff0c\u901a\u8fc7\u4e00\u4e2a\u5143\u68af\u5ea6\u81ea\u9002\u5e94\u6743\u91cd\u63a7\u5236\u5668\uff0c\u5c06SFT-RL\u5e73\u8861\u4f5c\u4e3a\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u52a8\u6001\u4f18\u5316\u4ee5\u6700\u5927\u5316\u957f\u671f\u4efb\u52a1\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7b56\u7565\u71b5\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u81ea\u4e3b\u53d1\u73b0\u6709\u6548\u8bad\u7ec3\u8bfe\u7a0b\u3002", "result": "AMFT\u5728\u6570\u5b66\u63a8\u7406\u3001\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u548c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff08SOTA\uff09\uff0c\u5e76\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6d88\u878d\u7814\u7a76\u548c\u8bad\u7ec3\u52a8\u6001\u5206\u6790\u8bc1\u5b9e\uff0c\u5143\u5b66\u4e60\u63a7\u5236\u5668\u5bf9\u4e8eAMFT\u7684\u7a33\u5b9a\u6027\u3001\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "AMFT\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5177\u539f\u5219\u6027\u548c\u66f4\u6709\u6548\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.07382", "pdf": "https://arxiv.org/pdf/2508.07382", "abs": "https://arxiv.org/abs/2508.07382", "authors": ["He Kong", "Die Hu", "Jingguo Ge", "Liangxiong Li", "Hui Li", "Tong Li"], "title": "Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Automating penetration testing is crucial for enhancing cybersecurity, yet\ncurrent Large Language Models (LLMs) face significant limitations in this\ndomain, including poor error handling, inefficient reasoning, and an inability\nto perform complex end-to-end tasks autonomously. To address these challenges,\nwe introduce Pentest-R1, a novel framework designed to optimize LLM reasoning\ncapabilities for this task through a two-stage reinforcement learning pipeline.\nWe first construct a dataset of over 500 real-world, multi-step walkthroughs,\nwhich Pentest-R1 leverages for offline reinforcement learning (RL) to instill\nfoundational attack logic. Subsequently, the LLM is fine-tuned via online RL in\nan interactive Capture The Flag (CTF) environment, where it learns directly\nfrom environmental feedback to develop robust error self-correction and\nadaptive strategies. Our extensive experiments on the Cybench and AutoPenBench\nbenchmarks demonstrate the framework's effectiveness. On AutoPenBench,\nPentest-R1 achieves a 24.2\\% success rate, surpassing most state-of-the-art\nmodels and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a\n15.0\\% success rate in unguided tasks, establishing a new state-of-the-art for\nopen-source LLMs and matching the performance of top proprietary models.\nAblation studies confirm that the synergy of both training stages is critical\nto its success.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPentest-R1\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u52a8\u5316\u6e17\u900f\u6d4b\u8bd5\u4e2d\u9762\u4e34\u7684\u5c40\u9650\u3002\u8be5\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff08\u79bb\u7ebf\u6570\u636e\u5b66\u4e60\u57fa\u7840\u903b\u8f91\uff0c\u5728\u7ebfCTF\u73af\u5883\u81ea\u9002\u5e94\u5fae\u8c03\uff09\u6765\u4f18\u5316LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728AutoPenBench\u548cCybench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4e3a\u5f00\u6e90LLM\u8bbe\u5b9a\u4e86\u65b0\u7684SOTA\u3002", "motivation": "\u5f53\u524dLLM\u5728\u81ea\u52a8\u5316\u6e17\u900f\u6d4b\u8bd5\u4e2d\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0c\u5982\u9519\u8bef\u5904\u7406\u80fd\u529b\u5dee\u3001\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u4ee5\u53ca\u96be\u4ee5\u81ea\u4e3b\u6267\u884c\u590d\u6742\u7684\u7aef\u5230\u7aef\u4efb\u52a1\uff0c\u8fd9\u963b\u788d\u4e86\u7f51\u7edc\u5b89\u5168\u80fd\u529b\u7684\u63d0\u5347\u3002", "method": "\u5f15\u5165Pentest-R1\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u7ba1\u9053\uff1a1) \u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff1a\u5229\u7528\u5305\u542b500\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u591a\u6b65\u9aa4\u6f14\u7ec3\u7684\u6570\u636e\u96c6\uff0c\u4e3aLLM\u704c\u8f93\u57fa\u7840\u653b\u51fb\u903b\u8f91\u30022) \u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff1a\u5728\u4ea4\u4e92\u5f0f\u593a\u65d7\uff08CTF\uff09\u73af\u5883\u4e2d\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5176\u76f4\u63a5\u4ece\u73af\u5883\u53cd\u9988\u4e2d\u5b66\u4e60\uff0c\u53d1\u5c55\u9c81\u68d2\u7684\u9519\u8bef\u81ea\u7ea0\u6b63\u548c\u81ea\u9002\u5e94\u7b56\u7565\u3002", "result": "\u5728AutoPenBench\u57fa\u51c6\u4e0a\uff0cPentest-R1\u7684\u6210\u529f\u7387\u4e3a24.2%\uff0c\u8d85\u8d8a\u4e86\u5927\u591a\u6570\u73b0\u6709\u5148\u8fdb\u6a21\u578b\uff0c\u4ec5\u6b21\u4e8eGemini 2.5 Flash\u3002\u5728Cybench\u57fa\u51c6\u7684\u975e\u6307\u5bfc\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u7387\u8fbe\u523015.0%\uff0c\u4e3a\u5f00\u6e90LLM\u6811\u7acb\u4e86\u65b0\u7684SOTA\uff0c\u5e76\u4e0e\u9876\u7ea7\u7684\u4e13\u6709\u6a21\u578b\u8868\u73b0\u76f8\u5f53\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\uff0c\u4e24\u4e2a\u8bad\u7ec3\u9636\u6bb5\u7684\u534f\u540c\u4f5c\u7528\u5bf9\u5176\u6210\u529f\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "Pentest-R1\u6846\u67b6\u901a\u8fc7\u5176\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u81ea\u52a8\u5316\u6e17\u900f\u6d4b\u8bd5\u4e2d\u7684\u73b0\u6709\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5728\u5f00\u6e90LLM\u9886\u57df\u8bbe\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u5f0f\u8bad\u7ec3\u7b56\u7565\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2508.06768", "pdf": "https://arxiv.org/pdf/2508.06768", "abs": "https://arxiv.org/abs/2508.06768", "authors": ["Noe Bertramo", "Gabriel Duguey", "Vivek Gopalakrishnan"], "title": "DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging", "categories": ["cs.CV", "cs.GR"], "comment": "10 pages, accepted to MICCAI ASMUS 25", "summary": "Intraoperative ultrasound imaging provides real-time guidance during numerous\nsurgical procedures, but its interpretation is complicated by noise, artifacts,\nand poor alignment with high-resolution preoperative MRI/CT scans. To bridge\nthe gap between reoperative planning and intraoperative guidance, we present\nDiffUS, a physics-based, differentiable ultrasound renderer that synthesizes\nrealistic B-mode images from volumetric imaging. DiffUS first converts MRI 3D\nscans into acoustic impedance volumes using a machine learning approach. Next,\nwe simulate ultrasound beam propagation using ray tracing with coupled\nreflection-transmission equations. DiffUS formulates wave propagation as a\nsparse linear system that captures multiple internal reflections. Finally, we\nreconstruct B-mode images via depth-resolved echo extraction across fan-shaped\nacquisition geometry, incorporating realistic artifacts including speckle noise\nand depth-dependent degradation. DiffUS is entirely implemented as\ndifferentiable tensor operations in PyTorch, enabling gradient-based\noptimization for downstream applications such as slice-to-volume registration\nand volumetric reconstruction. Evaluation on the ReMIND dataset demonstrates\nDiffUS's ability to generate anatomically accurate ultrasound images from brain\nMRI data.", "AI": {"tldr": "DiffUS\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u53ef\u5fae\u5206\u8d85\u58f0\u6e32\u67d3\u5668\uff0c\u80fd\u5c06MRI\u6570\u636e\u8f6c\u5316\u4e3a\u903c\u771f\u7684B\u6a21\u5f0f\u8d85\u58f0\u56fe\u50cf\uff0c\u65e8\u5728\u5f25\u5408\u672f\u524d\u89c4\u5212\u4e0e\u672f\u4e2d\u5f15\u5bfc\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u652f\u6301\u4e0b\u6e38\u4f18\u5316\u5e94\u7528\u3002", "motivation": "\u672f\u4e2d\u8d85\u58f0\u6210\u50cf\u867d\u63d0\u4f9b\u5b9e\u65f6\u5f15\u5bfc\uff0c\u4f46\u5176\u89e3\u91ca\u53d7\u9650\u4e8e\u566a\u58f0\u3001\u4f2a\u5f71\u4ee5\u53ca\u4e0e\u9ad8\u5206\u8fa8\u7387\u672f\u524dMRI/CT\u626b\u63cf\u5bf9\u9f50\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u672f\u524d\u89c4\u5212\u4e0e\u672f\u4e2d\u5f15\u5bfc\u4e4b\u95f4\u5b58\u5728\u8131\u8282\u3002", "method": "DiffUS\u9996\u5148\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5c06MRI 3D\u626b\u63cf\u8f6c\u6362\u4e3a\u58f0\u963b\u6297\u4f53\u79ef\uff1b\u63a5\u7740\uff0c\u5229\u7528\u5c04\u7ebf\u8ffd\u8e2a\u548c\u8026\u5408\u53cd\u5c04-\u900f\u5c04\u65b9\u7a0b\u6a21\u62df\u8d85\u58f0\u6ce2\u4f20\u64ad\uff0c\u5e76\u5c06\u5176\u8868\u8ff0\u4e3a\u6355\u83b7\u591a\u91cd\u5185\u90e8\u53cd\u5c04\u7684\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\uff1b\u6700\u540e\uff0c\u901a\u8fc7\u6df1\u5ea6\u89e3\u6790\u56de\u6ce2\u63d0\u53d6\u91cd\u5efaB\u6a21\u5f0f\u56fe\u50cf\uff0c\u5e76\u878d\u5165\u6563\u6591\u566a\u58f0\u3001\u6df1\u5ea6\u4f9d\u8d56\u6027\u9000\u5316\u7b49\u771f\u5b9e\u4f2a\u5f71\u3002\u6574\u4e2a\u7cfb\u7edf\u5728PyTorch\u4e2d\u5b9e\u73b0\u4e3a\u53ef\u5fae\u5206\u5f20\u91cf\u64cd\u4f5c\uff0c\u652f\u6301\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u3002", "result": "\u5728ReMIND\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8bc1\u660e\uff0cDiffUS\u80fd\u591f\u4ece\u8111\u90e8MRI\u6570\u636e\u751f\u6210\u89e3\u5256\u5b66\u4e0a\u51c6\u786e\u7684\u8d85\u58f0\u56fe\u50cf\u3002", "conclusion": "DiffUS\u6210\u529f\u5730\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4e14\u53ef\u5fae\u5206\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u903c\u771f\u7684\u8d85\u58f0\u56fe\u50cf\u6765\u89e3\u51b3\u672f\u4e2d\u8d85\u58f0\u89e3\u91ca\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u5207\u7247\u5230\u4f53\u79ef\u914d\u51c6\u548c\u4f53\u79ef\u91cd\u5efa\u7b49\u4e0b\u6e38\u5e94\u7528\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2508.07262", "pdf": "https://arxiv.org/pdf/2508.07262", "abs": "https://arxiv.org/abs/2508.07262", "authors": ["Bernd J. Kr\u00f6ger"], "title": "The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation", "categories": ["cs.CL", "cs.RO"], "comment": "11 pages, 9 figures, 14 references; supplementary material: python\n  source code", "summary": "This paper describes an extension of the two-dimensional dynamic articulatory\nmodel DYNARTmo by integrating an internal three-dimensional representation of\nthe palatal dome to estimate tongue-palate contact areas from midsagittal\ntongue contours. Two alternative dome geometries - a half-ellipse and a cosine\nbased profile - are implemented to model lateral curvature in the coronal\nplane. Using these geometries, lateral contact points are analytically computed\nfor each anterior-posterior position, enabling the generation of\nelectropalatography-like visualizations within the 2D+ framework. The enhanced\nmodel supports three synchronized views (sagittal, glottal, and palatal) for\nstatic and dynamic (animated) articulation displays, suitable for speech\nscience education and speech therapy. Future work includes adding a facial\n(lip) view and implementing articulatory-to-acoustic synthesis to\nquantitatively evaluate model realism.", "AI": {"tldr": "\u6269\u5c55\u4e8c\u7ef4\u52a8\u6001\u53d1\u97f3\u6a21\u578bDYNARTmo\uff0c\u6574\u5408\u4e09\u7ef4\u816d\u7a79\u9876\u8868\u793a\u4ee5\u4f30\u8ba1\u820c\u816d\u63a5\u89e6\u533a\u57df\uff0c\u5e76\u751f\u6210\u7c7b\u4f3c\u7535\u816d\u56fe\u7684\u53ef\u89c6\u5316\uff0c\u9002\u7528\u4e8e\u8a00\u8bed\u79d1\u5b66\u6559\u80b2\u548c\u6cbb\u7597\u3002", "motivation": "\u589e\u5f3a\u73b0\u6709\u4e8c\u7ef4\u53d1\u97f3\u6a21\u578b\uff08DYNARTmo\uff09\u7684\u529f\u80fd\uff0c\u901a\u8fc7\u5f15\u5165\u4e09\u7ef4\u816d\u7a79\u9876\u8868\u793a\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u548c\u53ef\u89c6\u5316\u820c\u816d\u63a5\u89e6\u533a\u57df\uff0c\u514b\u670d\u4e8c\u7ef4\u6a21\u578b\u5728\u6355\u6349\u6a2a\u5411\u63a5\u89e6\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u6574\u5408\u5185\u90e8\u4e09\u7ef4\u816d\u7a79\u9876\u8868\u793a\u6765\u6269\u5c55\u4e8c\u7ef4\u52a8\u6001\u53d1\u97f3\u6a21\u578bDYNARTmo\uff1b\u5b9e\u73b0\u4e86\u534a\u692d\u5706\u548c\u57fa\u4e8e\u4f59\u5f26\u7684\u4e24\u79cd\u816d\u7a79\u9876\u51e0\u4f55\u5f62\u72b6\u6765\u5efa\u6a21\u6a2a\u5411\u66f2\u7387\uff1b\u5206\u6790\u8ba1\u7b97\u6a2a\u5411\u63a5\u89e6\u70b9\u5e76\u751f\u6210\u7c7b\u4f3c\u7535\u816d\u56fe\u7684\u53ef\u89c6\u5316\u3002", "result": "\u589e\u5f3a\u6a21\u578b\u80fd\u591f\u4ece\u6b63\u4e2d\u77e2\u72b6\u820c\u8f6e\u5ed3\u4f30\u8ba1\u820c\u816d\u63a5\u89e6\u9762\u79ef\uff0c\u5e76\u751f\u6210\u7c7b\u4f3c\u7535\u816d\u56fe\u7684\u53ef\u89c6\u5316\uff1b\u8be5\u6a21\u578b\u652f\u6301\u77e2\u72b6\u3001\u58f0\u95e8\u548c\u816d\u4e09\u79cd\u540c\u6b65\u89c6\u56fe\uff0c\u7528\u4e8e\u9759\u6001\u548c\u52a8\u6001\u53d1\u97f3\u663e\u793a\u3002", "conclusion": "\u8be5\u589e\u5f3a\u6a21\u578b\u56e0\u5176\u63d0\u4f9b\u591a\u89d2\u5ea6\u540c\u6b65\u89c6\u56fe\u53ca\u9759\u6001/\u52a8\u6001\u663e\u793a\uff0c\u4e14\u80fd\u751f\u6210\u7c7b\u4f3c\u7535\u816d\u56fe\u7684\u53ef\u89c6\u5316\uff0c\u6545\u9002\u7528\u4e8e\u8a00\u8bed\u79d1\u5b66\u6559\u80b2\u548c\u8a00\u8bed\u6cbb\u7597\u9886\u57df\u3002"}}
{"id": "2508.06953", "pdf": "https://arxiv.org/pdf/2508.06953", "abs": "https://arxiv.org/abs/2508.06953", "authors": ["Shiwei Li", "Xiandi Luo", "Haozhao Wang", "Xing Tang", "Ziqiang Cui", "Dugang Liu", "Yuhua Li", "Xiuqiang He", "Ruixuan Li"], "title": "BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity", "categories": ["cs.LG"], "comment": null, "summary": "Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method\nwidely used in large language models (LLMs). It approximates the update of a\npretrained weight matrix $W\\in\\mathbb{R}^{m\\times n}$ by the product of two\nlow-rank matrices, $BA$, where $A \\in\\mathbb{R}^{r\\times n}$ and\n$B\\in\\mathbb{R}^{m\\times r} (r\\ll\\min\\{m,n\\})$. Increasing the dimension $r$\ncan raise the rank of LoRA weights (i.e., $BA$), which typically improves\nfine-tuning performance but also significantly increases the number of\ntrainable parameters. In this paper, we propose Block Diversified Low-Rank\nAdaptation (BoRA), which improves the rank of LoRA weights with a small number\nof additional parameters. Specifically, BoRA treats the product $BA$ as a block\nmatrix multiplication, where $A$ and $B$ are partitioned into $b$ blocks along\nthe columns and rows, respectively (i.e., $A=[A_1,\\dots,A_b]$ and\n$B=[B_1,\\dots,B_b]^\\top$). Consequently, the product $BA$ becomes the\nconcatenation of the block products $B_iA_j$ for $i,j\\in[b]$. To enhance the\ndiversity of different block products, BoRA introduces a unique diagonal matrix\n$\\Sigma_{i,j} \\in \\mathbb{R}^{r\\times r}$ for each block multiplication,\nresulting in $B_i \\Sigma_{i,j} A_j$. By leveraging these block-wise diagonal\nmatrices, BoRA increases the rank of LoRA weights by a factor of $b$ while only\nrequiring $b^2r$ additional parameters. Extensive experiments across multiple\ndatasets and models demonstrate the superiority of BoRA, and ablation studies\nfurther validate its scalability.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07388", "pdf": "https://arxiv.org/pdf/2508.07388", "abs": "https://arxiv.org/abs/2508.07388", "authors": ["Zhaoyu Chen", "Hongnan Lin", "Yongwei Nie", "Fei Ma", "Xuemiao Xu", "Fei Yu", "Chengjiang Long"], "title": "Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding", "categories": ["cs.AI"], "comment": null, "summary": "Temporal Video Grounding (TVG) seeks to localize video segments matching a\ngiven textual query. Current methods, while optimizing for high temporal\nIntersection-over-Union (IoU), often overfit to this metric, compromising\nsemantic action understanding in the video and query, a critical factor for\nrobust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG),\na novel framework that enhances both localization accuracy and action\nunderstanding without additional data. Our approach leverages three inversion\ntasks derived from existing TVG annotations: (1) Verb Completion, predicting\nmasked action verbs in queries from video segments; (2) Action Recognition,\nidentifying query-described actions; and (3) Video Description, generating\ndescriptions of video segments that explicitly embed query-relevant actions.\nThese tasks, integrated with TVG via a reinforcement learning framework with\nwell-designed reward functions, ensure balanced optimization of localization\nand semantics. Experiments show our method outperforms state-of-the-art\napproaches, achieving a 7.1\\% improvement in R1@0.7 on Charades-STA for a 3B\nmodel compared to Time-R1. By inverting TVG to derive query-related actions\nfrom segments, our approach strengthens semantic understanding, significantly\nraising the ceiling of localization accuracy.", "AI": {"tldr": "Invert4TVG\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u4e09\u4e2a\u53cd\u6f14\u4efb\u52a1\uff08\u52a8\u8bcd\u8865\u5168\u3001\u52a8\u4f5c\u8bc6\u522b\u3001\u89c6\u9891\u63cf\u8ff0\uff09\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u73b0\u6709TVG\u65b9\u6cd5\u8fc7\u5ea6\u4f18\u5316IoU\u800c\u727a\u7272\u8bed\u4e49\u7406\u89e3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u52a8\u4f5c\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u65f6\u95f4\u89c6\u9891\u5b9a\u4f4d\uff08TVG\uff09\u65b9\u6cd5\u5728\u4f18\u5316\u9ad8\u65f6\u95f4\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u65f6\uff0c\u5f80\u5f80\u8fc7\u5ea6\u62df\u5408\u8be5\u6307\u6807\uff0c\u4ece\u800c\u635f\u5bb3\u4e86\u89c6\u9891\u548c\u67e5\u8be2\u4e2d\u7684\u8bed\u4e49\u52a8\u4f5c\u7406\u89e3\uff0c\u800c\u8fd9\u5bf9\u4e8e\u9c81\u68d2\u7684TVG\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faInvert4TVG\u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u73b0\u6709TVG\u6807\u6ce8\uff0c\u5f15\u5165\u4e09\u4e2a\u53cd\u6f14\u4efb\u52a1\uff1a1) \u52a8\u8bcd\u8865\u5168\uff0c\u4ece\u89c6\u9891\u7247\u6bb5\u9884\u6d4b\u67e5\u8be2\u4e2d\u88ab\u906e\u853d\u7684\u52a8\u4f5c\u52a8\u8bcd\uff1b2) \u52a8\u4f5c\u8bc6\u522b\uff0c\u8bc6\u522b\u67e5\u8be2\u63cf\u8ff0\u7684\u52a8\u4f5c\uff1b3) \u89c6\u9891\u63cf\u8ff0\uff0c\u751f\u6210\u660e\u786e\u5d4c\u5165\u67e5\u8be2\u76f8\u5173\u52a8\u4f5c\u7684\u89c6\u9891\u7247\u6bb5\u63cf\u8ff0\u3002\u8fd9\u4e9b\u4efb\u52a1\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u4e0eTVG\u96c6\u6210\uff0c\u4ee5\u5e73\u8861\u5b9a\u4f4d\u548c\u8bed\u4e49\u7684\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728Charades-STA\u6570\u636e\u96c6\u4e0a\uff0c3B\u6a21\u578b\u5728R1@0.7\u6307\u6807\u4e0a\u6bd4Time-R1\u63d0\u5347\u4e867.1%\u3002", "conclusion": "\u901a\u8fc7\u53cd\u6f14TVG\u4ee5\u4ece\u89c6\u9891\u7247\u6bb5\u4e2d\u63a8\u5bfc\u51fa\u67e5\u8be2\u76f8\u5173\u7684\u52a8\u4f5c\uff0c\u672c\u65b9\u6cd5\u52a0\u5f3a\u4e86\u8bed\u4e49\u7406\u89e3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u51c6\u786e\u6027\u7684\u4e0a\u9650\u3002"}}
{"id": "2508.06805", "pdf": "https://arxiv.org/pdf/2508.06805", "abs": "https://arxiv.org/abs/2508.06805", "authors": ["Aarav Mehta", "Priya Deshmukh", "Vikram Singh", "Siddharth Malhotra", "Krishnan Menon Iyer", "Tanvi Iyer"], "title": "Edge Detection for Organ Boundaries via Top Down Refinement and SubPixel Upsampling", "categories": ["cs.CV"], "comment": "MICCAIA Workshop", "summary": "Accurate localization of organ boundaries is critical in medical imaging for\nsegmentation, registration, surgical planning, and radiotherapy. While deep\nconvolutional networks (ConvNets) have advanced general-purpose edge detection\nto near-human performance on natural images, their outputs often lack precise\nlocalization, a limitation that is particularly harmful in medical applications\nwhere millimeter-level accuracy is required. Building on a systematic analysis\nof ConvNet edge outputs, we propose a medically focused crisp edge detector\nthat adapts a novel top-down backward refinement architecture to medical images\n(2D and volumetric). Our method progressively upsamples and fuses high-level\nsemantic features with fine-grained low-level cues through a backward\nrefinement pathway, producing high-resolution, well-localized organ boundaries.\nWe further extend the design to handle anisotropic volumes by combining 2D\nslice-wise refinement with light 3D context aggregation to retain computational\nefficiency. Evaluations on several CT and MRI organ datasets demonstrate\nsubstantially improved boundary localization under strict criteria (boundary\nF-measure, Hausdorff distance) compared to baseline ConvNet detectors and\ncontemporary medical edge/contour methods. Importantly, integrating our crisp\nedge maps into downstream pipelines yields consistent gains in organ\nsegmentation (higher Dice scores, lower boundary errors), more accurate image\nregistration, and improved delineation of lesions near organ interfaces. The\nproposed approach produces clinically valuable, crisp organ edges that\nmaterially enhance common medical-imaging tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u533b\u7528\u6e05\u6670\u8fb9\u7f18\u68c0\u6d4b\u5668\uff0c\u91c7\u7528\u65b0\u9896\u7684\u81ea\u9876\u5411\u4e0b\u9006\u5411\u7ec6\u5316\u67b6\u6784\uff0c\u80fd\u4e3a2D\u548c3D\u533b\u5b66\u56fe\u50cf\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u3001\u6beb\u7c73\u7ea7\u5668\u5b98\u8fb9\u754c\u5b9a\u4f4d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u5347\u4e0b\u6e38\u533b\u5b66\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4e2d\u7cbe\u786e\u7684\u5668\u5b98\u8fb9\u754c\u5b9a\u4f4d\u5bf9\u5206\u5272\u3001\u914d\u51c6\u3001\u624b\u672f\u89c4\u5212\u548c\u653e\u7597\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5377\u79ef\u7f51\u7edc\u5728\u901a\u7528\u8fb9\u7f18\u68c0\u6d4b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8f93\u51fa\u7f3a\u4e4f\u7cbe\u786e\u7684\u5b9a\u4f4d\uff0c\u8fd9\u5bf9\u4e8e\u9700\u8981\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u7684\u533b\u5b66\u5e94\u7528\u800c\u8a00\u662f\u4e00\u4e2a\u4e25\u91cd\u7f3a\u9677\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5bf9\u5377\u79ef\u7f51\u7edc\u8fb9\u7f18\u8f93\u51fa\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u63d0\u51fa\u4e00\u79cd\u533b\u7528\u6e05\u6670\u8fb9\u7f18\u68c0\u6d4b\u5668\u3002\u5b83\u91c7\u7528\u65b0\u9896\u7684\u81ea\u9876\u5411\u4e0b\u9006\u5411\u7ec6\u5316\u67b6\u6784\uff0c\u901a\u8fc7\u9006\u5411\u7ec6\u5316\u8def\u5f84\u9010\u6b65\u4e0a\u91c7\u6837\u5e76\u878d\u5408\u9ad8\u7ea7\u8bed\u4e49\u7279\u5f81\u4e0e\u7ec6\u7c92\u5ea6\u4f4e\u7ea7\u4fe1\u606f\uff0c\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u3001\u5b9a\u4f4d\u7cbe\u786e\u7684\u5668\u5b98\u8fb9\u754c\u3002\u4e3a\u5904\u7406\u5404\u5411\u5f02\u6027\u4f53\u7d20\uff0c\u8be5\u8bbe\u8ba1\u7ed3\u54082D\u5207\u7247\u7ec6\u5316\u4e0e\u8f7b\u91cf\u7ea73D\u4e0a\u4e0b\u6587\u805a\u5408\uff0c\u4ee5\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2aCT\u548cMRI\u5668\u5b98\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u57fa\u7ebf\u5377\u79ef\u7f51\u7edc\u68c0\u6d4b\u5668\u548c\u5f53\u4ee3\u533b\u5b66\u8fb9\u7f18/\u8f6e\u5ed3\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u4e25\u683c\u6807\u51c6\uff08\u8fb9\u754cF-measure\u3001\u8c6a\u65af\u591a\u592b\u8ddd\u79bb\uff09\u4e0b\u663e\u8457\u6539\u5584\u4e86\u8fb9\u754c\u5b9a\u4f4d\u3002\u5c06\u8be5\u6e05\u6670\u8fb9\u7f18\u56fe\u6574\u5408\u5230\u4e0b\u6e38\u6d41\u7a0b\u4e2d\uff0c\u80fd\u4e00\u81f4\u6027\u5730\u63d0\u5347\u5668\u5b98\u5206\u5272\uff08\u66f4\u9ad8\u7684Dice\u5206\u6570\u3001\u66f4\u4f4e\u7684\u8fb9\u754c\u8bef\u5dee\uff09\u3001\u66f4\u7cbe\u786e\u7684\u56fe\u50cf\u914d\u51c6\u4ee5\u53ca\u6539\u5584\u5668\u5b98\u754c\u9762\u9644\u8fd1\u75c5\u53d8\u7684\u63cf\u7ed8\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5177\u6709\u4e34\u5e8a\u4ef7\u503c\u7684\u6e05\u6670\u5668\u5b98\u8fb9\u7f18\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5e38\u89c1\u7684\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u3002"}}
{"id": "2508.07273", "pdf": "https://arxiv.org/pdf/2508.07273", "abs": "https://arxiv.org/abs/2508.07273", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Jeremy H. M. Wong", "Tianchi Liu", "Shuo Sun", "Wenyu Zhang", "Muhammad Huzaifah", "Nancy Chen", "Ai Ti Aw"], "title": "Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at (ASRU 2025) 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop", "summary": "Current large speech language models (Speech-LLMs) often exhibit limitations\nin empathetic reasoning, primarily due to the absence of training datasets that\nintegrate both contextual content and paralinguistic cues. In this work, we\npropose two approaches to incorporate contextual paralinguistic information\ninto model training: (1) an explicit method that provides paralinguistic\nmetadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit\nmethod that automatically generates novel training question-answer (QA) pairs\nusing both categorical and dimensional emotion annotations alongside speech\ntranscriptions. Our implicit method boosts performance (LLM-judged) by 38.41%\non a human-annotated QA benchmark, reaching 46.02% when combined with the\nexplicit approach, showing effectiveness in contextual paralinguistic\nunderstanding. We also validate the LLM judge by demonstrating its correlation\nwith classification metrics, providing support for its reliability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u663e\u5f0f\u548c\u9690\u5f0f\u4e24\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u8bed\u5883\u526f\u8bed\u8a00\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u5927\u6a21\u578b\uff08Speech-LLMs\uff09\u7684\u540c\u7406\u5fc3\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08Speech-LLMs\uff09\u5728\u540c\u7406\u5fc3\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u7f3a\u4e4f\u6574\u5408\u8bed\u5883\u5185\u5bb9\u548c\u526f\u8bed\u8a00\u7ebf\u7d22\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5c06\u8bed\u5883\u526f\u8bed\u8a00\u4fe1\u606f\u878d\u5165\u6a21\u578b\u8bad\u7ec3\u7684\u65b9\u6cd5\uff1a1) \u663e\u5f0f\u65b9\u6cd5\uff1a\u76f4\u63a5\u5411LLM\u63d0\u4f9b\u526f\u8bed\u8a00\u5143\u6570\u636e\uff08\u5982\u60c5\u611f\u6807\u6ce8\uff09\uff1b2) \u9690\u5f0f\u65b9\u6cd5\uff1a\u5229\u7528\u7c7b\u522b\u548c\u7ef4\u5ea6\u60c5\u611f\u6807\u6ce8\u4ee5\u53ca\u8bed\u97f3\u8f6c\u5f55\uff0c\u81ea\u52a8\u751f\u6210\u65b0\u7684\u8bad\u7ec3\u95ee\u7b54\u5bf9\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u9a8c\u8bc1\u5176\u4e0e\u5206\u7c7b\u6307\u6807\u7684\u76f8\u5173\u6027\uff0c\u652f\u6301\u4e86LLM\u8bc4\u5224\u5668\u7684\u53ef\u9760\u6027\u3002", "result": "\u5728\u4eba\u5de5\u6807\u6ce8\u7684QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u9690\u5f0f\u65b9\u6cd5\u5c06\u6027\u80fd\uff08LLM\u5224\u65ad\uff09\u63d0\u5347\u4e8638.41%\uff1b\u5f53\u4e0e\u663e\u5f0f\u65b9\u6cd5\u7ed3\u5408\u65f6\uff0c\u6027\u80fd\u8fbe\u523046.02%\uff0c\u663e\u793a\u51fa\u5728\u8bed\u5883\u526f\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u540c\u65f6\uff0cLLM\u8bc4\u5224\u5668\u4e0e\u5206\u7c7b\u6307\u6807\u7684\u76f8\u5173\u6027\u4e5f\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u663e\u5f0f\u548c\u9690\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6709\u6548\u6574\u5408\u8bed\u5883\u526f\u8bed\u8a00\u4fe1\u606f\uff0c\u6210\u529f\u63d0\u5347\u4e86\u5927\u578b\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5728\u540c\u7406\u5fc3\u63a8\u7406\u65b9\u9762\u7684\u6027\u80fd\uff0c\u514b\u670d\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5173\u952e\u5c40\u9650\u6027\u3002"}}
{"id": "2508.06966", "pdf": "https://arxiv.org/pdf/2508.06966", "abs": "https://arxiv.org/abs/2508.06966", "authors": ["Hiba Najjar", "Bushra Alshbib", "Andreas Dengel"], "title": "Can Multitask Learning Enhance Model Explainability?", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at GCPR 2025, Special Track \"Photogrammetry and remote\n  sensing\"", "summary": "Remote sensing provides satellite data in diverse types and formats. The\nusage of multimodal learning networks exploits this diversity to improve model\nperformance, except that the complexity of such networks comes at the expense\nof their interpretability. In this study, we explore how modalities can be\nleveraged through multitask learning to intrinsically explain model behavior.\nIn particular, instead of additional inputs, we use certain modalities as\nadditional targets to be predicted along with the main task. The success of\nthis approach relies on the rich information content of satellite data, which\nremains as input modalities. We show how this modeling context provides\nnumerous benefits: (1) in case of data scarcity, the additional modalities do\nnot need to be collected for model inference at deployment, (2) the model\nperformance remains comparable to the multimodal baseline performance, and in\nsome cases achieves better scores, (3) prediction errors in the main task can\nbe explained via the model behavior in the auxiliary task(s). We demonstrate\nthe efficiency of our approach on three datasets, including segmentation,\nclassification, and regression tasks. Code available at\ngit.opendfki.de/hiba.najjar/mtl_explainability/.", "AI": {"tldr": "\u9488\u5bf9\u9065\u611f\u591a\u6a21\u6001\u5b66\u4e60\u6a21\u578b\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u67d0\u4e9b\u6a21\u6001\u4f5c\u4e3a\u8f85\u52a9\u9884\u6d4b\u76ee\u6807\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5e76\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u3002", "motivation": "\u9065\u611f\u9886\u57df\u7684\u591a\u6a21\u6001\u5b66\u4e60\u7f51\u7edc\u80fd\u5229\u7528\u6570\u636e\u591a\u6837\u6027\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5176\u590d\u6742\u6027\u727a\u7272\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u5229\u7528\u6a21\u6001\u6765\u5185\u5728\u5730\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u7279\u5b9a\u6a21\u6001\u4f5c\u4e3a\u9664\u4e3b\u8981\u4efb\u52a1\u4e4b\u5916\u7684\u989d\u5916\u9884\u6d4b\u76ee\u6807\uff08\u8f85\u52a9\u4efb\u52a1\uff09\uff0c\u800c\u975e\u989d\u5916\u7684\u8f93\u5165\u3002\u8be5\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u536b\u661f\u8f93\u5165\u6570\u636e\u672c\u8eab\u7684\u4e30\u5bcc\u4fe1\u606f\u5185\u5bb9\u3002", "result": "1. \u90e8\u7f72\u4f18\u52bf\uff1a\u5728\u6a21\u578b\u90e8\u7f72\u65f6\uff0c\u989d\u5916\u6a21\u6001\uff08\u4f5c\u4e3a\u76ee\u6807\uff09\u65e0\u9700\u6536\u96c6\uff0c\u5229\u4e8e\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u30022. \u6027\u80fd\uff1a\u6a21\u578b\u6027\u80fd\u4e0e\u591a\u6a21\u6001\u57fa\u7ebf\u76f8\u5f53\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\u30023. \u89e3\u91ca\u6027\uff1a\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5728\u8f85\u52a9\u4efb\u52a1\u4e0a\u7684\u884c\u4e3a\u6765\u89e3\u91ca\u4e3b\u4efb\u52a1\u7684\u9884\u6d4b\u9519\u8bef\u3002\u8be5\u65b9\u6cd5\u5df2\u5728\u5206\u5272\u3001\u5206\u7c7b\u548c\u56de\u5f52\u4e09\u7c7b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u5730\u5229\u7528\u6a21\u6001\u4f5c\u4e3a\u8f85\u52a9\u76ee\u6807\uff0c\u4e3a\u9065\u611f\u6a21\u578b\u63d0\u4f9b\u4e86\u5185\u5728\u89e3\u91ca\u80fd\u529b\u3002\u5b83\u5728\u6570\u636e\u7a00\u7f3a\u3001\u6027\u80fd\u4fdd\u6301\u548c\u9519\u8bef\u89e3\u91ca\u65b9\u9762\u5747\u5177\u6709\u4f18\u52bf\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u5c55\u73b0\u4e86\u9ad8\u6548\u6027\u3002"}}
{"id": "2508.07405", "pdf": "https://arxiv.org/pdf/2508.07405", "abs": "https://arxiv.org/abs/2508.07405", "authors": ["Jesse Ponnock"], "title": "Generative AI for Strategic Plan Development", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.5.4"], "comment": "11 pages, 9 figures", "summary": "Given recent breakthroughs in Generative Artificial Intelligence (GAI) and\nLarge Language Models (LLMs), more and more professional services are being\naugmented through Artificial Intelligence (AI), which once seemed impossible to\nautomate. This paper presents a modular model for leveraging GAI in developing\nstrategic plans for large scale government organizations and evaluates leading\nmachine learning techniques in their application towards one of the identified\nmodules. Specifically, the performance of BERTopic and Non-negative Matrix\nFactorization (NMF) are evaluated in their ability to use topic modeling to\ngenerate themes representative of Vision Elements within a strategic plan. To\naccomplish this, BERTopic and NMF models are trained using a large volume of\nreports from the Government Accountability Office (GAO). The generated topics\nfrom each model are then scored for similarity against the Vision Elements of a\npublished strategic plan and the results are compared. Our results show that\nthese techniques are capable of generating themes similar to 100% of the\nelements being evaluated against. Further, we conclude that BERTopic performs\nbest in this application with more than half of its correlated topics achieving\na \"medium\" or \"strong\" correlation. A capability of GAI-enabled strategic plan\ndevelopment impacts a multi-billion dollar industry and assists the federal\ngovernment in overcoming regulatory requirements which are crucial to the\npublic good. Further work will focus on the operationalization of the concept\nproven in this study as well as viability of the remaining modules in the\nproposed model for GAI-generated strategic plans.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u5229\u7528\u751f\u6210\u5f0fAI\uff08GAI\uff09\u5f00\u53d1\u5927\u578b\u653f\u5e9c\u7ec4\u7ec7\u6218\u7565\u8ba1\u5212\u7684\u6a21\u5757\u5316\u6a21\u578b\uff0c\u91cd\u70b9\u8bc4\u4f30\u4e86BERTopic\u548c\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08NMF\uff09\u5728\u4ece\u653f\u5e9c\u62a5\u544a\u4e2d\u63d0\u53d6\u6218\u7565\u8ba1\u5212\u613f\u666f\u8981\u7d20\u4e3b\u9898\u65b9\u9762\u7684\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8fd9\u4e9b\u6280\u672f\u80fd\u751f\u6210\u4e0e\u73b0\u6709\u613f\u666f\u8981\u7d20\u9ad8\u5ea6\u76f8\u4f3c\u7684\u4e3b\u9898\uff0c\u5176\u4e2dBERTopic\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u9274\u4e8e\u751f\u6210\u5f0fAI\uff08GAI\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u52a8\u5316\u4e13\u4e1a\u670d\u52a1\u65b9\u9762\u7684\u7a81\u7834\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528GAI\u8f85\u52a9\u5927\u578b\u653f\u5e9c\u7ec4\u7ec7\u5f00\u53d1\u6218\u7565\u8ba1\u5212\uff0c\u4ee5\u5e94\u5bf9\u4ef7\u503c\u6570\u5341\u4ebf\u7f8e\u5143\u7684\u884c\u4e1a\u9700\u6c42\u5e76\u5e2e\u52a9\u8054\u90a6\u653f\u5e9c\u514b\u670d\u91cd\u8981\u7684\u76d1\u7ba1\u8981\u6c42\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528GAI\u5f00\u53d1\u6218\u7565\u8ba1\u5212\u7684\u6a21\u5757\u5316\u6a21\u578b\uff0c\u5e76\u5177\u4f53\u8bc4\u4f30\u4e86BERTopic\u548c\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08NMF\uff09\u8fd9\u4e24\u79cd\u4e3b\u9898\u5efa\u6a21\u6280\u672f\u3002\u901a\u8fc7\u4f7f\u7528\u5927\u91cf\u653f\u5e9c\u95ee\u8d23\u5c40\uff08GAO\uff09\u62a5\u544a\u8bad\u7ec3\u6a21\u578b\uff0c\u7136\u540e\u5c06\u751f\u6210\u7684\u4e3b\u9898\u4e0e\u5df2\u53d1\u5e03\u6218\u7565\u8ba1\u5212\u7684\u613f\u666f\u8981\u7d20\u8fdb\u884c\u76f8\u4f3c\u6027\u8bc4\u5206\u5e76\u6bd4\u8f83\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u4e9b\u6280\u672f\u80fd\u591f\u751f\u6210\u4e0e100%\u88ab\u8bc4\u4f30\u7684\u6218\u7565\u8ba1\u5212\u613f\u666f\u8981\u7d20\u76f8\u4f3c\u7684\u4e3b\u9898\u3002\u5176\u4e2d\uff0cBERTopic\u5728\u6b64\u5e94\u7528\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5176\u8d85\u8fc7\u4e00\u534a\u7684\u76f8\u5173\u4e3b\u9898\u8fbe\u5230\u4e86\u201c\u4e2d\u7b49\u201d\u6216\u201c\u5f3a\u201d\u76f8\u5173\u6027\u3002", "conclusion": "\u5229\u7528GAI\u5f00\u53d1\u6218\u7565\u8ba1\u5212\u7684\u80fd\u529b\u5bf9\u4e00\u4e2a\u4ef7\u503c\u6570\u5341\u4ebf\u7f8e\u5143\u7684\u4ea7\u4e1a\u5177\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u5e76\u6709\u52a9\u4e8e\u8054\u90a6\u653f\u5e9c\u514b\u670d\u5173\u952e\u7684\u76d1\u7ba1\u8981\u6c42\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u4e13\u6ce8\u4e8e\u5c06\u672c\u7814\u7a76\u4e2d\u9a8c\u8bc1\u7684\u6982\u5ff5\u6295\u5165\u5b9e\u9645\u8fd0\u7528\uff0c\u5e76\u63a2\u7d22\u6240\u63d0\u51fa\u6a21\u578b\u4e2d\u5176\u4f59\u6a21\u5757\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.06816", "pdf": "https://arxiv.org/pdf/2508.06816", "abs": "https://arxiv.org/abs/2508.06816", "authors": ["Vikram Singh", "Kabir Malhotra", "Rohan Desai", "Ananya Shankaracharya", "Priyadarshini Chatterjee", "Krishnan Menon Iyer"], "title": "DualResolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation", "categories": ["cs.CV"], "comment": "MICCAIA", "summary": "Accurate segmentation of melanocytic tumors in dermoscopic images is a\ncritical step for automated skin cancer screening and clinical decision\nsupport. Unlike natural scene segmentation, lesion delineation must reconcile\nsubtle texture and color variations, frequent artifacts (hairs, rulers,\nbubbles), and a strong need for precise boundary localization to support\ndownstream diagnosis. In this paper we introduce Our method, a novel ResNet\ninspired dual resolution architecture specifically designed for melanocytic\ntumor segmentation. Our method maintains a full resolution stream that\npreserves fine grained boundary information while a complementary pooled stream\naggregates multi scale contextual cues for robust lesion recognition. The\nstreams are tightly coupled by boundary aware residual connections that inject\nhigh frequency edge information into deep feature maps, and by a channel\nattention module that adapts color and texture sensitivity to dermoscopic\nappearance. To further address common imaging artifacts and the limited size of\nclinical datasets, we propose a lightweight artifact suppression block and a\nmulti task training objective that combines a Dice Tversky segmentation loss\nwith an explicit boundary loss and a contrastive regularizer for feature\nstability. The combined design yields pixel accurate masks without requiring\nheavy post processing or complex pre training protocols. Extensive experiments\non public dermoscopic benchmarks demonstrate that Our method significantly\nimproves boundary adherence and clinically relevant segmentation metrics\ncompared to standard encoder decoder baselines, making it a practical building\nblock for automated melanoma assessment systems.", "AI": {"tldr": "\u9488\u5bf9\u76ae\u80a4\u955c\u56fe\u50cf\u4e2d\u9ed1\u8272\u7d20\u7624\u5206\u5272\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eResNet\u7684\u53cc\u5206\u8fa8\u7387\u7f51\u7edc\uff0c\u901a\u8fc7\u878d\u5408\u8fb9\u754c\u611f\u77e5\u8fde\u63a5\u3001\u901a\u9053\u6ce8\u610f\u529b\u3001\u4f2a\u5f71\u6291\u5236\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u8fb9\u754c\u6e05\u6670\u7684\u80bf\u7624\u5206\u5272\uff0c\u4e3a\u81ea\u52a8\u5316\u8bca\u65ad\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u81ea\u52a8\u76ae\u80a4\u764c\u7b5b\u67e5\u548c\u4e34\u5e8a\u51b3\u7b56\u6025\u9700\u76ae\u80a4\u955c\u56fe\u50cf\u4e2d\u9ed1\u8272\u7d20\u80bf\u7624\u7684\u7cbe\u786e\u5206\u5272\u3002\u8be5\u4efb\u52a1\u9762\u4e34\u7eb9\u7406\u8272\u5f69\u7ec6\u5fae\u53d8\u5316\u3001\u9891\u7e41\u4f2a\u5f71\uff08\u5982\u6bdb\u53d1\u3001\u5c3a\u5b50\u3001\u6c14\u6ce1\uff09\u5e72\u6270\u53ca\u5bf9\u9ad8\u7cbe\u5ea6\u8fb9\u754c\u5b9a\u4f4d\u7684\u4e25\u683c\u8981\u6c42\u3002", "method": "\u8bbe\u8ba1ResNet\u542f\u53d1\u7684\u53cc\u5206\u8fa8\u7387\u67b6\u6784\uff1a\u5305\u542b\u4e00\u4e2a\u5168\u5206\u8fa8\u7387\u6d41\u4ee5\u4fdd\u7559\u7cbe\u7ec6\u8fb9\u754c\u4fe1\u606f\uff0c\u548c\u4e00\u4e2a\u6c60\u5316\u6d41\u4ee5\u805a\u5408\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u7279\u5f81\u3002\u901a\u8fc7\u8fb9\u754c\u611f\u77e5\u6b8b\u5dee\u8fde\u63a5\u548c\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\u7d27\u5bc6\u8026\u5408\u53cc\u6d41\u3002\u5f15\u5165\u8f7b\u91cf\u7ea7\u4f2a\u5f71\u6291\u5236\u6a21\u5757\u4ee5\u5e94\u5bf9\u56fe\u50cf\u4f2a\u5f71\u3002\u91c7\u7528\u591a\u4efb\u52a1\u8bad\u7ec3\u76ee\u6807\uff0c\u7ed3\u5408Dice Tversky\u5206\u5272\u635f\u5931\u3001\u663e\u5f0f\u8fb9\u754c\u635f\u5931\u548c\u5bf9\u6bd4\u6b63\u5219\u5668\u4ee5\u63d0\u9ad8\u7279\u5f81\u7a33\u5b9a\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u65e0\u9700\u7e41\u91cd\u540e\u5904\u7406\u6216\u590d\u6742\u9884\u8bad\u7ec3\uff0c\u5373\u53ef\u751f\u6210\u50cf\u7d20\u7ea7\u7cbe\u786e\u7684\u5206\u5272\u63a9\u819c\u3002\u5728\u516c\u5171\u76ae\u80a4\u955c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0e\u6807\u51c6\u7f16\u7801\u5668-\u89e3\u7801\u5668\u57fa\u7ebf\u76f8\u6bd4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u754c\u4f9d\u4ece\u6027\u548c\u4e34\u5e8a\u76f8\u5173\u5206\u5272\u6307\u6807\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u9ed1\u8272\u7d20\u7624\u8bc4\u4f30\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u6784\u5efa\u6a21\u5757\u3002"}}
{"id": "2508.07279", "pdf": "https://arxiv.org/pdf/2508.07279", "abs": "https://arxiv.org/abs/2508.07279", "authors": ["Vasudha Varadarajan", "Hui Xu", "Rebecca Astrid Boehme", "Mariam Marlan Mirstrom", "Sverker Sikstrom", "H. Andrew Schwartz"], "title": "MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) offer new opportunities for\nscalable, interactive mental health assessment, but excessive querying by LLMs\nburdens users and is inefficient for real-world screening across\ntransdiagnostic symptom profiles. We introduce MAQuA, an adaptive\nquestion-asking framework for simultaneous, multidimensional mental health\nscreening. Combining multi-outcome modeling on language responses with item\nresponse theory (IRT) and factor analysis, MAQuA selects the questions with\nmost informative responses across multiple dimensions at each turn to optimize\ndiagnostic information, improving accuracy and potentially reducing response\nburden. Empirical results on a novel dataset reveal that MAQuA reduces the\nnumber of assessment questions required for score stabilization by 50-87%\ncompared to random ordering (e.g., achieving stable depression scores with 71%\nfewer questions and eating disorder scores with 85% fewer questions). MAQuA\ndemonstrates robust performance across both internalizing (depression, anxiety)\nand externalizing (substance use, eating disorder) domains, with early stopping\nstrategies further reducing patient time and burden. These findings position\nMAQuA as a powerful and efficient tool for scalable, nuanced, and interactive\nmental health screening, advancing the integration of LLM-based agents into\nreal-world clinical workflows.", "AI": {"tldr": "MAQuA\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u95ee\u7b54\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5fc3\u7406\u6d4b\u91cf\u5b66\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u5fc3\u7406\u5065\u5eb7\u7b5b\u67e5\u6240\u9700\u7684\u63d0\u95ee\u6570\u91cf\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u53ef\u6269\u5c55\u7684\u4e92\u52a8\u5f0f\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\uff0c\u4f46\u5176\u8fc7\u591a\u7684\u63d0\u95ee\u7ed9\u7528\u6237\u5e26\u6765\u8d1f\u62c5\uff0c\u4e14\u5bf9\u4e8e\u8de8\u8bca\u65ad\u75c7\u72b6\u7684\u5b9e\u9645\u7b5b\u67e5\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5f15\u5165MAQuA\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5bf9\u8bed\u8a00\u54cd\u5e94\u7684\u591a\u8f93\u51fa\u5efa\u6a21\u3001\u9879\u76ee\u53cd\u5e94\u7406\u8bba\uff08IRT\uff09\u548c\u56e0\u5b50\u5206\u6790\uff0c\u5728\u6bcf\u6b21\u63d0\u95ee\u65f6\u9009\u62e9\u8de8\u591a\u4e2a\u7ef4\u5ea6\u4fe1\u606f\u91cf\u6700\u5927\u7684\u95ee\u9898\uff0c\u4ee5\u4f18\u5316\u8bca\u65ad\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u54cd\u5e94\u8d1f\u62c5\u3002", "result": "\u5728\u4e00\u9879\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u968f\u673a\u6392\u5e8f\u76f8\u6bd4\uff0cMAQuA\u5c06\u8bc4\u4f30\u95ee\u9898\u6570\u91cf\u51cf\u5c11\u4e8650-87%\uff08\u4f8b\u5982\uff0c\u6291\u90c1\u75c7\u8bc4\u5206\u7a33\u5b9a\u6240\u9700\u95ee\u9898\u51cf\u5c1171%\uff0c\u996e\u98df\u5931\u8c03\u8bc4\u5206\u51cf\u5c1185%\uff09\u3002MAQuA\u5728\u5185\u5316\uff08\u6291\u90c1\u3001\u7126\u8651\uff09\u548c\u5916\u5316\uff08\u7269\u8d28\u4f7f\u7528\u3001\u996e\u98df\u5931\u8c03\uff09\u9886\u57df\u5747\u8868\u73b0\u51fa\u7a33\u5065\u6027\u80fd\uff0c\u65e9\u671f\u505c\u6b62\u7b56\u7565\u8fdb\u4e00\u6b65\u51cf\u5c11\u4e86\u60a3\u8005\u65f6\u95f4\u548c\u8d1f\u62c5\u3002", "conclusion": "MAQuA\u662f\u4e00\u4e2a\u5f3a\u5927\u4e14\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u53ef\u6269\u5c55\u3001\u7cbe\u7ec6\u4e14\u4ea4\u4e92\u5f0f\u7684\u5fc3\u7406\u5065\u5eb7\u7b5b\u67e5\uff0c\u63a8\u52a8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u4e0e\u5b9e\u9645\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u7684\u6574\u5408\u3002"}}
{"id": "2508.06981", "pdf": "https://arxiv.org/pdf/2508.06981", "abs": "https://arxiv.org/abs/2508.06981", "authors": ["Brooks Kinch", "Benjamin Shaffer", "Elizabeth Armstrong", "Michael Meehan", "John Hewson", "Nathaniel Trask"], "title": "Structure-Preserving Digital Twins via Conditional Neural Whitney Forms", "categories": ["cs.LG", "cs.NA", "math.NA", "physics.comp-ph"], "comment": null, "summary": "We present a framework for constructing real-time digital twins based on\nstructure-preserving reduced finite element models conditioned on a latent\nvariable Z. The approach uses conditional attention mechanisms to learn both a\nreduced finite element basis and a nonlinear conservation law within the\nframework of finite element exterior calculus (FEEC). This guarantees numerical\nwell-posedness and exact preservation of conserved quantities, regardless of\ndata sparsity or optimization error. The conditioning mechanism supports\nreal-time calibration to parametric variables, allowing the construction of\ndigital twins which support closed loop inference and calibration to sensor\ndata. The framework interfaces with conventional finite element machinery in a\nnon-invasive manner, allowing treatment of complex geometries and integration\nof learned models with conventional finite element techniques.\n  Benchmarks include advection diffusion, shock hydrodynamics, electrostatics,\nand a complex battery thermal runaway problem. The method achieves accurate\npredictions on complex geometries with sparse data (25 LES simulations),\nincluding capturing the transition to turbulence and achieving real-time\ninference ~0.1s with a speedup of 3.1x10^8 relative to LES. An open-source\nimplementation is available on GitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u7ed3\u6784\u4fdd\u6301\u964d\u9636\u6709\u9650\u5143\u6a21\u578b\u7684\u5b9e\u65f6\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u6ce8\u610f\u529b\u673a\u5236\u548c\u6709\u9650\u5143\u5916\u5fae\u5206\u51e0\u4f55\uff08FEEC\uff09\u786e\u4fdd\u6570\u503c\u7a33\u5b9a\u6027\u548c\u5b88\u6052\u91cf\u7cbe\u786e\u6027\uff0c\u5b9e\u73b0\u590d\u6742\u51e0\u4f55\u4e0b\u7684\u5b9e\u65f6\u63a8\u7406\u548c\u4f20\u611f\u5668\u6570\u636e\u6821\u51c6\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6784\u5efa\u80fd\u591f\u8fdb\u884c\u95ed\u73af\u63a8\u7406\u3001\u4e0e\u4f20\u611f\u5668\u6570\u636e\u6821\u51c6\u5e76\u4fdd\u8bc1\u6570\u503c\u7a33\u5b9a\u6027\u548c\u5b88\u6052\u91cf\u7cbe\u786e\u6027\u7684\u5b9e\u65f6\u6570\u5b57\u5b6a\u751f\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u7ed3\u6784\u4fdd\u6301\u964d\u9636\u6709\u9650\u5143\u6a21\u578b\uff0c\u5e76\u4ee5\u6f5c\u5728\u53d8\u91cfZ\u4e3a\u6761\u4ef6\u3002\u5b83\u5229\u7528\u6761\u4ef6\u6ce8\u610f\u529b\u673a\u5236\u5728\u6709\u9650\u5143\u5916\u5fae\u5206\u51e0\u4f55\uff08FEEC\uff09\u6846\u67b6\u5185\u5b66\u4e60\u964d\u9636\u6709\u9650\u5143\u57fa\u548c\u975e\u7ebf\u6027\u5b88\u6052\u5f8b\u3002\u8be5\u6846\u67b6\u4ee5\u975e\u4fb5\u5165\u5f0f\u65b9\u5f0f\u4e0e\u4f20\u7edf\u6709\u9650\u5143\u673a\u5236\u63a5\u53e3\uff0c\u652f\u6301\u5bf9\u53c2\u6570\u53d8\u91cf\u7684\u5b9e\u65f6\u6821\u51c6\uff0c\u5e76\u5904\u7406\u590d\u6742\u51e0\u4f55\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u51e0\u4f55\u548c\u7a00\u758f\u6570\u636e\uff0825\u6b21LES\u6a21\u62df\uff09\u4e0b\u5b9e\u73b0\u4e86\u7cbe\u786e\u9884\u6d4b\uff0c\u5305\u62ec\u6355\u83b7\u6e4d\u6d41\u8fc7\u6e21\u3002\u5b83\u5b9e\u73b0\u4e86\u7ea60.1\u79d2\u7684\u5b9e\u65f6\u63a8\u7406\uff0c\u76f8\u5bf9\u4e8eLES\u67093.1x10^8\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002\u57fa\u51c6\u6d4b\u8bd5\u5305\u62ec\u5bf9\u6d41\u6269\u6563\u3001\u6fc0\u6ce2\u6d41\u4f53\u52a8\u529b\u5b66\u3001\u9759\u7535\u5b66\u4ee5\u53ca\u590d\u6742\u7684\u7535\u6c60\u70ed\u5931\u63a7\u95ee\u9898\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u6784\u5efa\u4e86\u5b9e\u65f6\u3001\u7cbe\u786e\u4e14\u7269\u7406\u5b88\u6052\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u5373\u4f7f\u5728\u6570\u636e\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u6301\u6570\u503c\u9002\u5b9a\u6027\u548c\u5b88\u6052\u91cf\u7684\u7cbe\u786e\u6027\uff0c\u5e76\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5de5\u7a0b\u95ee\u9898\u3002"}}
{"id": "2508.07407", "pdf": "https://arxiv.org/pdf/2508.07407", "abs": "https://arxiv.org/abs/2508.07407", "authors": ["Jinyuan Fang", "Yanwen Peng", "Xi Zhang", "Yingxu Wang", "Xinhao Yi", "Guibin Zhang", "Yi Xu", "Bin Wu", "Siwei Liu", "Zihao Li", "Zhaochun Ren", "Nikos Aletras", "Xi Wang", "Han Zhou", "Zaiqiao Meng"], "title": "A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Recent advances in large language models have sparked growing interest in AI\nagents capable of solving complex, real-world tasks. However, most existing\nagent systems rely on manually crafted configurations that remain static after\ndeployment, limiting their ability to adapt to dynamic and evolving\nenvironments. To this end, recent research has explored agent evolution\ntechniques that aim to automatically enhance agent systems based on interaction\ndata and environmental feedback. This emerging direction lays the foundation\nfor self-evolving AI agents, which bridge the static capabilities of foundation\nmodels with the continuous adaptability required by lifelong agentic systems.\nIn this survey, we provide a comprehensive review of existing techniques for\nself-evolving agentic systems. Specifically, we first introduce a unified\nconceptual framework that abstracts the feedback loop underlying the design of\nself-evolving agentic systems. The framework highlights four key components:\nSystem Inputs, Agent System, Environment, and Optimisers, serving as a\nfoundation for understanding and comparing different strategies. Based on this\nframework, we systematically review a wide range of self-evolving techniques\nthat target different components of the agent system. We also investigate\ndomain-specific evolution strategies developed for specialised fields such as\nbiomedicine, programming, and finance, where optimisation objectives are\ntightly coupled with domain constraints. In addition, we provide a dedicated\ndiscussion on the evaluation, safety, and ethical considerations for\nself-evolving agentic systems, which are critical to ensuring their\neffectiveness and reliability. This survey aims to provide researchers and\npractitioners with a systematic understanding of self-evolving AI agents,\nlaying the foundation for the development of more adaptive, autonomous, and\nlifelong agentic systems.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u5168\u9762\u5ba1\u89c6\u4e86\u81ea\u6211\u6f14\u5316AI\u4ee3\u7406\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5e76\u7cfb\u7edf\u56de\u987e\u4e86\u76f8\u5173\u7b56\u7565\u3001\u9886\u57df\u7279\u5b9a\u65b9\u6cd5\u4ee5\u53ca\u8bc4\u4f30\u3001\u5b89\u5168\u4e0e\u4f26\u7406\u8003\u91cf\uff0c\u65e8\u5728\u4fc3\u8fdb\u66f4\u5177\u9002\u5e94\u6027\u548c\u7ec8\u8eab\u5b66\u4e60\u80fd\u529b\u7684AI\u4ee3\u7406\u7cfb\u7edf\u5f00\u53d1\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u7cfb\u7edf\u90e8\u7f72\u540e\u914d\u7f6e\u56fa\u5b9a\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\uff0c\u7814\u7a76\u6b63\u63a2\u7d22\u4ee3\u7406\u6f14\u5316\u6280\u672f\u4ee5\u5b9e\u73b0\u81ea\u6211\u6f14\u5316\u4ee3\u7406\u3002\u672c\u7efc\u8ff0\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u5bf9\u8fd9\u4e9b\u6280\u672f\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u3002", "method": "\u672c\u7efc\u8ff0\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u7cfb\u7edf\u8f93\u5165\u3001\u4ee3\u7406\u7cfb\u7edf\u3001\u73af\u5883\u548c\u4f18\u5316\u5668\u7684\u7edf\u4e00\u6982\u5ff5\u6846\u67b6\uff0c\u4ee5\u62bd\u8c61\u81ea\u6211\u6f14\u5316\u4ee3\u7406\u7684\u53cd\u9988\u5faa\u73af\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u7cfb\u7edf\u56de\u987e\u4e86\u9488\u5bf9\u4ee3\u7406\u7cfb\u7edf\u4e0d\u540c\u7ec4\u4ef6\u7684\u81ea\u6211\u6f14\u5316\u6280\u672f\uff0c\u5e76\u8c03\u67e5\u4e86\u751f\u7269\u533b\u5b66\u3001\u7f16\u7a0b\u548c\u91d1\u878d\u7b49\u9886\u57df\u7279\u5b9a\u7684\u6f14\u5316\u7b56\u7565\u3002\u6b64\u5916\uff0c\u8fd8\u8ba8\u8bba\u4e86\u81ea\u6211\u6f14\u5316\u4ee3\u7406\u7cfb\u7edf\u7684\u8bc4\u4f30\u3001\u5b89\u5168\u548c\u4f26\u7406\u8003\u91cf\u3002", "result": "\u672c\u7efc\u8ff0\u901a\u8fc7\u63d0\u51fa\u7edf\u4e00\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u56de\u987e\u73b0\u6709\u6280\u672f\u3001\u9886\u57df\u7279\u5b9a\u7b56\u7565\u4ee5\u53ca\u8bc4\u4f30\u3001\u5b89\u5168\u548c\u4f26\u7406\u8003\u91cf\uff0c\u4e3a\u81ea\u6211\u6f14\u5316AI\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5206\u7c7b\u548c\u7406\u89e3\uff0c\u63ed\u793a\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u73b0\u72b6\u548c\u5173\u952e\u8981\u7d20\u3002", "conclusion": "\u672c\u7efc\u8ff0\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u5bf9\u81ea\u6211\u6f14\u5316AI\u4ee3\u7406\u7684\u7cfb\u7edf\u6027\u7406\u89e3\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u9002\u5e94\u6027\u3001\u81ea\u4e3b\u6027\u548c\u7ec8\u8eab\u5b66\u4e60\u80fd\u529b\u7684\u4ee3\u7406\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2508.06819", "pdf": "https://arxiv.org/pdf/2508.06819", "abs": "https://arxiv.org/abs/2508.06819", "authors": ["Ayaan Nooruddin Siddiqui", "Mahnoor Zaidi", "Ayesha Nazneen Shahbaz", "Priyadarshini Chatterjee", "Krishnan Menon Iyer"], "title": "VesselRW: Weakly Supervised Subcutaneous Vessel Segmentation via Learned Random Walk Propagation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate segmentation of subcutaneous vessels from clinical images is\nhampered by scarce, expensive ground truth and by low contrast, noisy\nappearance of vessels across patients and modalities. We present a novel weakly\nsupervised training framework tailored for subcutaneous vessel segmentation\nthat leverages inexpensive sparse annotations (e.g., centerline traces, dot\nmarkers, or short scribbles). Sparse labels are expanded into dense,\nprobabilistic supervision via a differentiable random walk label propagation\nmodel whose transition weights incorporate image driven vesselness cues and\ntubular continuity priors. The propagation yields per-pixel hitting\nprobabilities together with calibrated uncertainty estimates; these are\nincorporated into an uncertainty weighted loss to avoid over fitting to\nambiguous regions. Crucially, the label-propagator is learned jointly with a\nCNN based segmentation predictor, enabling the system to discover vessel edges\nand continuity constraints without explicit edge supervision. We further\nintroduce a topology aware regularizer that encourages centerline connectivity\nand penalizes spurious branches, improving clinical usability. In experiments\non clinical subcutaneous imaging datasets, our method consistently outperforms\nnaive training on sparse labels and conventional dense pseudo-labeling,\nproducing more complete vascular maps and better calibrated uncertainty for\ndownstream decision making. The approach substantially reduces annotation\nburden while preserving clinically relevant vessel topology.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5f31\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u5c11\u91cf\u7a00\u758f\u6807\u6ce8\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u968f\u673a\u6e38\u8d70\u6a21\u578b\u548cCNN\uff0c\u5b9e\u73b0\u51c6\u786e\u7684\u76ae\u4e0b\u8840\u7ba1\u5206\u5272\uff0c\u663e\u8457\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\u5e76\u4fdd\u6301\u62d3\u6251\u7ed3\u6784\u3002", "motivation": "\u76ae\u4e0b\u8840\u7ba1\u7684\u7cbe\u786e\u5206\u5272\u9762\u4e34\u6311\u6218\uff1a\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e14\u6602\u8d35\uff1b\u4e34\u5e8a\u56fe\u50cf\u4e2d\u8840\u7ba1\u5bf9\u6bd4\u5ea6\u4f4e\u3001\u566a\u58f0\u5927\u4e14\u8de8\u60a3\u8005\u548c\u6a21\u6001\u8868\u73b0\u4e0d\u4e00\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f31\u76d1\u7763\u8bad\u7ec3\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u5ec9\u4ef7\u7684\u7a00\u758f\u6807\u6ce8\uff08\u5982\u4e2d\u5fc3\u7ebf\u3001\u70b9\u6216\u77ed\u5212\u7ebf\uff09\u5e76\u5c06\u5176\u901a\u8fc7\u4e00\u4e2a\u53ef\u5fae\u5206\u968f\u673a\u6e38\u8d70\u6807\u7b7e\u4f20\u64ad\u6a21\u578b\u6269\u5c55\u4e3a\u5bc6\u96c6\u7684\u6982\u7387\u76d1\u7763\uff0c\u6a21\u578b\u6574\u5408\u4e86\u56fe\u50cf\u9a71\u52a8\u7684\u8840\u7ba1\u6027\u7279\u5f81\u548c\u7ba1\u72b6\u8fde\u7eed\u6027\u5148\u9a8c\u3002\u4f20\u64ad\u7ed3\u679c\u5305\u62ec\u50cf\u7d20\u7ea7\u547d\u4e2d\u6982\u7387\u548c\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u635f\u5931\u4ee5\u907f\u514d\u5bf9\u6a21\u7cca\u533a\u57df\u7684\u8fc7\u62df\u5408\u3002\u6807\u7b7e\u4f20\u64ad\u5668\u4e0e\u57fa\u4e8eCNN\u7684\u5206\u5272\u9884\u6d4b\u5668\u8054\u5408\u5b66\u4e60\uff0c\u65e0\u9700\u663e\u5f0f\u8fb9\u7f18\u76d1\u7763\u5373\u53ef\u53d1\u73b0\u8840\u7ba1\u8fb9\u7f18\u548c\u8fde\u7eed\u6027\u7ea6\u675f\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u62d3\u6251\u611f\u77e5\u6b63\u5219\u5316\u5668\uff0c\u4ee5\u4fc3\u8fdb\u4e2d\u5fc3\u7ebf\u8fde\u901a\u6027\u5e76\u60e9\u7f5a\u865a\u5047\u5206\u652f\u3002", "result": "\u5728\u4e34\u5e8a\u76ae\u4e0b\u6210\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8e\u7a00\u758f\u6807\u7b7e\u7684\u6734\u7d20\u8bad\u7ec3\u548c\u4f20\u7edf\u7684\u5bc6\u96c6\u4f2a\u6807\u7b7e\u65b9\u6cd5\u3002\u5b83\u80fd\u751f\u6210\u66f4\u5b8c\u6574\u7684\u8840\u7ba1\u56fe\uff0c\u5e76\u4e3a\u540e\u7eed\u51b3\u7b56\u63d0\u4f9b\u66f4\u597d\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5927\u5e45\u51cf\u5c11\u4e86\u6807\u6ce8\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u4e34\u5e8a\u76f8\u5173\u7684\u8840\u7ba1\u62d3\u6251\u7ed3\u6784\uff0c\u63d0\u5347\u4e86\u4e34\u5e8a\u53ef\u7528\u6027\u3002"}}
{"id": "2508.07284", "pdf": "https://arxiv.org/pdf/2508.07284", "abs": "https://arxiv.org/abs/2508.07284", "authors": ["Junchen Ding", "Penghao Jiang", "Zihao Xu", "Ziqi Ding", "Yichen Zhu", "Jiaojiao Jiang", "Yuekang Li"], "title": "\"Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As large language models (LLMs) increasingly mediate ethically sensitive\ndecisions, understanding their moral reasoning processes becomes imperative.\nThis study presents a comprehensive empirical evaluation of 14 leading LLMs,\nboth reasoning enabled and general purpose, across 27 diverse trolley problem\nscenarios, framed by ten moral philosophies, including utilitarianism,\ndeontology, and altruism. Using a factorial prompting protocol, we elicited\n3,780 binary decisions and natural language justifications, enabling analysis\nalong axes of decisional assertiveness, explanation answer consistency, public\nmoral alignment, and sensitivity to ethically irrelevant cues. Our findings\nreveal significant variability across ethical frames and model types: reasoning\nenhanced models demonstrate greater decisiveness and structured justifications,\nyet do not always align better with human consensus. Notably, \"sweet zones\"\nemerge in altruistic, fairness, and virtue ethics framings, where models\nachieve a balance of high intervention rates, low explanation conflict, and\nminimal divergence from aggregated human judgments. However, models diverge\nunder frames emphasizing kinship, legality, or self interest, often producing\nethically controversial outcomes. These patterns suggest that moral prompting\nis not only a behavioral modifier but also a diagnostic tool for uncovering\nlatent alignment philosophies across providers. We advocate for moral reasoning\nto become a primary axis in LLM alignment, calling for standardized benchmarks\nthat evaluate not just what LLMs decide, but how and why.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e8614\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u572827\u4e2a\u7535\u8f66\u96be\u9898\u573a\u666f\u4e2d\u7684\u9053\u5fb7\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5176\u5728\u4e0d\u540c\u9053\u5fb7\u54f2\u5b66\u6846\u67b6\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u548c\u5bf9\u4eba\u7c7b\u5171\u8bc6\u7684\u5bf9\u9f50\u60c5\u51b5\uff0c\u5e76\u547c\u5401\u5c06\u9053\u5fb7\u63a8\u7406\u4f5c\u4e3aLLM\u5bf9\u9f50\u7684\u6838\u5fc3\u6807\u51c6\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65e5\u76ca\u53c2\u4e0e\u4f26\u7406\u654f\u611f\u51b3\u7b56\uff0c\u7406\u89e3\u5b83\u4eec\u7684\u9053\u5fb7\u63a8\u7406\u8fc7\u7a0b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u5bf914\u4e2a\u9886\u5148LLM\uff08\u5305\u62ec\u63a8\u7406\u589e\u5f3a\u578b\u548c\u901a\u7528\u578b\uff09\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u8bc1\u8bc4\u4f30\u3002\u4f7f\u7528\u4e8627\u4e2a\u7535\u8f66\u96be\u9898\u573a\u666f\uff0c\u5e76\u7ed3\u5408\u4e86\u529f\u5229\u4e3b\u4e49\u3001\u9053\u4e49\u8bba\u3001\u5229\u4ed6\u4e3b\u4e49\u7b4910\u79cd\u9053\u5fb7\u54f2\u5b66\u6846\u67b6\u3002\u901a\u8fc7\u56e0\u5b50\u5f0f\u63d0\u793a\u534f\u8bae\uff0c\u5171\u6536\u96c6\u4e863,780\u4e2a\u4e8c\u5143\u51b3\u7b56\u548c\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u5e76\u5206\u6790\u4e86\u51b3\u7b56\u7684\u679c\u65ad\u6027\u3001\u89e3\u91ca\u7b54\u6848\u7684\u4e00\u81f4\u6027\u3001\u516c\u5171\u9053\u5fb7\u5bf9\u9f50\u5ea6\u4ee5\u53ca\u5bf9\u4f26\u7406\u65e0\u5173\u7ebf\u7d22\u7684\u654f\u611f\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u4f26\u7406\u6846\u67b6\u548c\u6a21\u578b\u7c7b\u578b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51b3\u7b56\u679c\u65ad\u6027\u548c\u7ed3\u6784\u5316\u89e3\u91ca\uff0c\u4f46\u5e76\u975e\u603b\u80fd\u4e0e\u4eba\u7c7b\u5171\u8bc6\u66f4\u597d\u5730\u5bf9\u9f50\u3002\u5728\u5229\u4ed6\u4e3b\u4e49\u3001\u516c\u5e73\u548c\u7f8e\u5fb7\u4f26\u7406\u6846\u67b6\u4e0b\u51fa\u73b0\u4e86\u201c\u751c\u871c\u533a\u201d\uff0c\u6a21\u578b\u80fd\u5b9e\u73b0\u9ad8\u5e72\u9884\u7387\u3001\u4f4e\u89e3\u91ca\u51b2\u7a81\u548c\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u6700\u5c0f\u504f\u79bb\u3002\u7136\u800c\uff0c\u5728\u5f3a\u8c03\u4eb2\u7f18\u5173\u7cfb\u3001\u5408\u6cd5\u6027\u6216\u81ea\u8eab\u5229\u76ca\u7684\u6846\u67b6\u4e0b\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u5206\u6b67\uff0c\u5e38\u4ea7\u751f\u4f26\u7406\u4e0a\u5177\u4e89\u8bae\u7684\u7ed3\u679c\u3002\u8fd9\u4e9b\u6a21\u5f0f\u8868\u660e\u9053\u5fb7\u63d0\u793a\u4e0d\u4ec5\u662f\u884c\u4e3a\u4fee\u6b63\u5668\uff0c\u4e5f\u662f\u63ed\u793a\u6f5c\u5728\u5bf9\u9f50\u54f2\u5b66\u7684\u8bca\u65ad\u5de5\u5177\u3002", "conclusion": "\u9053\u5fb7\u63a8\u7406\u5e94\u6210\u4e3aLLM\u5bf9\u9f50\u7684\u4e3b\u8981\u7ef4\u5ea6\u3002\u7814\u7a76\u547c\u5401\u5efa\u7acb\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u4e0d\u4ec5\u8bc4\u4f30LLM\u505a\u51fa\u4ec0\u4e48\u51b3\u7b56\uff0c\u8fd8\u8981\u8bc4\u4f30\u5b83\u4eec\u5982\u4f55\u4ee5\u53ca\u4e3a\u4f55\u505a\u51fa\u8fd9\u4e9b\u51b3\u7b56\u3002"}}
{"id": "2508.06985", "pdf": "https://arxiv.org/pdf/2508.06985", "abs": "https://arxiv.org/abs/2508.06985", "authors": ["Jiawei Zhang", "Yifei Zhang", "Baozhao Yi", "Yao Ren", "Qi Jiao", "Hanyu Bai", "Weiran Jiang", "Ziyou Song"], "title": "Discovery Learning accelerates battery design evaluation", "categories": ["cs.LG", "cs.CE", "cs.SY", "eess.SY", "physics.comp-ph"], "comment": "Main text, 20 pages, 5 figures", "summary": "Fast and reliable validation of novel designs in complex physical systems\nsuch as batteries is critical to accelerating technological innovation.\nHowever, battery research and development remain bottlenecked by the\nprohibitively high time and energy costs required to evaluate numerous new\ndesign candidates, particularly in battery prototyping and life testing.\nDespite recent progress in data-driven battery lifetime prediction, existing\nmethods require labeled data of target designs to improve accuracy and cannot\nmake reliable predictions until after prototyping, thus falling far short of\nthe efficiency needed to enable rapid feedback for battery design. Here, we\nintroduce Discovery Learning (DL), a scientific machine-learning paradigm that\nintegrates active learning, physics-guided learning, and zero-shot learning\ninto a human-like reasoning loop, drawing inspiration from learning theories in\neducational psychology. DL can learn from historical battery designs and\nactively reduce the need for prototyping, thus enabling rapid lifetime\nevaluation for unobserved material-design combinations without requiring\nadditional data labeling. To test DL, we present 123 industrial-grade\nlarge-format lithium-ion pouch cells, spanning eight material-design\ncombinations and diverse cycling protocols. Trained solely on public datasets\nof small-capacity cylindrical cells, DL achieves 7.2% test error in predicting\nthe average cycle life under unknown device variability. This results in\nsavings of 98% in time and 95% in energy compared to industrial practices. This\nwork highlights the potential of uncovering insights from historical designs to\ninform and accelerate the development of next-generation battery technologies.\nDL represents a key advance toward efficient data-driven modeling and helps\nrealize the promise of machine learning for accelerating scientific discovery\nand engineering innovation.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u7535\u6c60\u8bbe\u8ba1\u9a8c\u8bc1\u8017\u65f6\u8017\u529b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faDiscovery Learning (DL) \u79d1\u5b66\u673a\u5668\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u5b66\u4e60\u6280\u672f\uff0c\u65e0\u9700\u539f\u578b\u5236\u4f5c\u5373\u53ef\u5feb\u901f\u51c6\u786e\u9884\u6d4b\u7535\u6c60\u5bff\u547d\uff0c\u76f8\u8f83\u4e8e\u5de5\u4e1a\u5b9e\u8df5\u5927\u5e45\u8282\u7701\u65f6\u95f4\u548c\u80fd\u6e90\u3002", "motivation": "\u7535\u6c60\u7b49\u590d\u6742\u7269\u7406\u7cfb\u7edf\u7684\u65b0\u8bbe\u8ba1\u9a8c\u8bc1\u901f\u5ea6\u5bf9\u4e8e\u52a0\u901f\u6280\u672f\u521b\u65b0\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7535\u6c60\u7814\u53d1\u53d7\u5230\u8bc4\u4f30\u65b0\u8bbe\u8ba1\u5019\u9009\u65b9\u6848\u6240\u9700\u7684\u9ad8\u6602\u65f6\u95f4\u4e0e\u80fd\u6e90\u6210\u672c\uff08\u5c24\u5176\u5728\u539f\u578b\u5236\u4f5c\u548c\u5bff\u547d\u6d4b\u8bd5\u9636\u6bb5\uff09\u7684\u4e25\u91cd\u5236\u7ea6\u3002\u5c3d\u7ba1\u6570\u636e\u9a71\u52a8\u7684\u7535\u6c60\u5bff\u547d\u9884\u6d4b\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u76ee\u6807\u8bbe\u8ba1\u7684\u6807\u7b7e\u6570\u636e\uff0c\u4e14\u9700\u5728\u539f\u578b\u5236\u4f5c\u540e\u624d\u80fd\u8fdb\u884c\u53ef\u9760\u9884\u6d4b\uff0c\u8fdc\u4e0d\u80fd\u6ee1\u8db3\u7535\u6c60\u8bbe\u8ba1\u6240\u9700\u7684\u9ad8\u6548\u7387\u53cd\u9988\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86Discovery Learning (DL) \u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u8303\u5f0f\u3002\u5b83\u501f\u9274\u6559\u80b2\u5fc3\u7406\u5b66\u4e2d\u7684\u5b66\u4e60\u7406\u8bba\uff0c\u5c06\u4e3b\u52a8\u5b66\u4e60\u3001\u7269\u7406\u5f15\u5bfc\u5b66\u4e60\u548c\u96f6\u6837\u672c\u5b66\u4e60\u6574\u5408\u5230\u7c7b\u4eba\u63a8\u7406\u5faa\u73af\u4e2d\u3002DL\u80fd\u591f\u4ece\u5386\u53f2\u7535\u6c60\u8bbe\u8ba1\u4e2d\u5b66\u4e60\uff0c\u5e76\u4e3b\u52a8\u51cf\u5c11\u5bf9\u539f\u578b\u5236\u4f5c\u7684\u9700\u6c42\uff0c\u4ece\u800c\u65e0\u9700\u989d\u5916\u6570\u636e\u6807\u8bb0\u5373\u53ef\u5b9e\u73b0\u5bf9\u672a\u89c2\u5bdf\u5230\u7684\u6750\u6599-\u8bbe\u8ba1\u7ec4\u5408\u7684\u5feb\u901f\u5bff\u547d\u8bc4\u4f30\u3002", "result": "\u4e3a\u9a8c\u8bc1DL\uff0c\u7814\u7a76\u4f7f\u7528\u4e86123\u4e2a\u5de5\u4e1a\u7ea7\u5927\u578b\u9502\u79bb\u5b50\u8f6f\u5305\u7535\u6c60\uff0c\u6db5\u76d68\u79cd\u6750\u6599-\u8bbe\u8ba1\u7ec4\u5408\u548c\u591a\u79cd\u5faa\u73af\u534f\u8bae\u3002\u4ec5\u4f7f\u7528\u5c0f\u5bb9\u91cf\u5706\u67f1\u5f62\u7535\u6c60\u7684\u516c\u5171\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0cDL\u5728\u9884\u6d4b\u672a\u77e5\u8bbe\u5907\u53d8\u5f02\u4e0b\u7684\u5e73\u5747\u5faa\u73af\u5bff\u547d\u65f6\uff0c\u5b9e\u73b0\u4e867.2%\u7684\u6d4b\u8bd5\u8bef\u5dee\u3002\u4e0e\u5de5\u4e1a\u5b9e\u8df5\u76f8\u6bd4\uff0c\u8fd9\u5e26\u6765\u4e8698%\u7684\u65f6\u95f4\u8282\u7701\u548c95%\u7684\u80fd\u6e90\u8282\u7701\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u4ece\u5386\u53f2\u8bbe\u8ba1\u4e2d\u53d1\u73b0\u6d1e\u5bdf\u529b\uff0c\u4ee5\u6307\u5bfc\u548c\u52a0\u901f\u4e0b\u4e00\u4ee3\u7535\u6c60\u6280\u672f\u5f00\u53d1\u7684\u6f5c\u529b\u3002DL\u4ee3\u8868\u4e86\u9ad8\u6548\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u7684\u5173\u952e\u8fdb\u5c55\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u673a\u5668\u5b66\u4e60\u5728\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u548c\u5de5\u7a0b\u521b\u65b0\u65b9\u9762\u7684\u627f\u8bfa\u3002"}}
{"id": "2508.07466", "pdf": "https://arxiv.org/pdf/2508.07466", "abs": "https://arxiv.org/abs/2508.07466", "authors": ["Dom Huh", "Prasant Mohapatra"], "title": "Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs", "categories": ["cs.AI"], "comment": null, "summary": "Language is a ubiquitous tool that is foundational to reasoning and\ncollaboration, ranging from everyday interactions to sophisticated\nproblem-solving tasks. The establishment of a common language can serve as a\npowerful asset in ensuring clear communication and understanding amongst\nagents, facilitating desired coordination and strategies. In this work, we\nextend the capabilities of large language models (LLMs) by integrating them\nwith advancements in multi-agent decision-making algorithms. We propose a\nsystematic framework for the design of multi-agentic large language models\n(LLMs), focusing on key integration practices. These include advanced prompt\nengineering techniques, the development of effective memory architectures,\nmulti-modal information processing, and alignment strategies through\nfine-tuning algorithms. We evaluate these design choices through extensive\nablation studies on classic game settings with significant underlying social\ndilemmas and game-theoretic considerations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u7b97\u6cd5\u7ed3\u5408\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u7ea7\u63d0\u793a\u5de5\u7a0b\u3001\u8bb0\u5fc6\u67b6\u6784\u3001\u591a\u6a21\u6001\u5904\u7406\u548c\u5fae\u8c03\u6765\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u80fd\u529b\uff0c\u5e76\u5728\u7ecf\u5178\u535a\u5f08\u8bba\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u8bed\u8a00\u662f\u63a8\u7406\u548c\u534f\u4f5c\u7684\u57fa\u7840\u5de5\u5177\uff0c\u5728\u591a\u667a\u80fd\u4f53\u4e92\u52a8\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u5efa\u7acb\u5171\u540c\u8bed\u8a00\u80fd\u4fc3\u8fdb\u667a\u80fd\u4f53\u95f4\u6e05\u6670\u6c9f\u901a\u3001\u7406\u89e3\u548c\u534f\u8c03\uff0c\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u5347\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u4e0e\u7b56\u7565\u5236\u5b9a\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u591a\u667a\u80fd\u4f53\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u7b97\u6cd5\u6765\u6269\u5c55LLMs\u7684\u80fd\u529b\u3002\u5173\u952e\u96c6\u6210\u5b9e\u8df5\u5305\u62ec\uff1a\u9ad8\u7ea7\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u3001\u6709\u6548\u7684\u8bb0\u5fc6\u67b6\u6784\u5f00\u53d1\u3001\u591a\u6a21\u6001\u4fe1\u606f\u5904\u7406\u4ee5\u53ca\u901a\u8fc7\u5fae\u8c03\u7b97\u6cd5\u5b9e\u73b0\u7684\u5bf9\u9f50\u7b56\u7565\u3002\u8fd9\u4e9b\u8bbe\u8ba1\u9009\u62e9\u5c06\u901a\u8fc7\u5728\u5177\u6709\u663e\u8457\u793e\u4f1a\u56f0\u5883\u548c\u535a\u5f08\u8bba\u8003\u8651\u7684\u7ecf\u5178\u6e38\u620f\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u6765\u8bc4\u4f30\u3002", "result": "\u62bd\u8c61\u4e2d\u672a\u63d0\u4f9b\u5177\u4f53\u7684\u7814\u7a76\u7ed3\u679c\uff0c\u4ec5\u9610\u8ff0\u4e86\u8bc4\u4f30\u65b9\u6cd5\uff1a\u901a\u8fc7\u5728\u5177\u6709\u793e\u4f1a\u56f0\u5883\u548c\u535a\u5f08\u8bba\u8003\u8651\u7684\u7ecf\u5178\u6e38\u620f\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u6765\u8bc4\u4f30\u6240\u63d0\u51fa\u7684\u8bbe\u8ba1\u9009\u62e9\u3002", "conclusion": "\u62bd\u8c61\u4e2d\u672a\u76f4\u63a5\u63d0\u4f9b\u660e\u786e\u7684\u7ed3\u8bba\u3002"}}
{"id": "2508.06831", "pdf": "https://arxiv.org/pdf/2508.06831", "abs": "https://arxiv.org/abs/2508.06831", "authors": ["Taha Mustapha Nehdi", "Nairouz Mrabah", "Atif Belal", "Marco Pedersoli", "Eric Granger"], "title": "Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "Adapting person re-identification (reID) models to new target environments\nremains a challenging problem that is typically addressed using unsupervised\ndomain adaptation (UDA) methods. Recent works show that when labeled data\noriginates from several distinct sources (e.g., datasets and cameras),\nconsidering each source separately and applying multi-source domain adaptation\n(MSDA) typically yields higher accuracy and robustness compared to blending the\nsources and performing conventional UDA. However, state-of-the-art MSDA methods\nlearn domain-specific backbone models or require access to source domain data\nduring adaptation, resulting in significant growth in training parameters and\ncomputational cost. In this paper, a Source-free Adaptive Gated Experts\n(SAGE-reID) method is introduced for person reID. Our SAGE-reID is a\ncost-effective, source-free MSDA method that first trains individual\nsource-specific low-rank adapters (LoRA) through source-free UDA. Next, a\nlightweight gating network is introduced and trained to dynamically assign\noptimal merging weights for fusion of LoRA experts, enabling effective\ncross-domain knowledge transfer. While the number of backbone parameters\nremains constant across source domains, LoRA experts scale linearly but remain\nnegligible in size (<= 2% of the backbone), reducing both the memory\nconsumption and risk of overfitting. Extensive experiments conducted on three\nchallenging benchmarks: Market-1501, DukeMTMC-reID, and MSMT17 indicate that\nSAGE-reID outperforms state-of-the-art methods while being computationally\nefficient.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSAGE-reID\uff0c\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u3001\u65e0\u9700\u6e90\u6570\u636e\u7684\u591a\u6e90\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u884c\u4eba\u91cd\u8bc6\u522b\u3002\u5b83\u901a\u8fc7\u8bad\u7ec3\u4f4e\u79e9\u9002\u914d\u5668(LoRA)\u548c\u95e8\u63a7\u7f51\u7edc\u52a8\u6001\u878d\u5408\u4e13\u5bb6\u77e5\u8bc6\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u5458\u91cd\u8bc6\u522b\u591a\u6e90\u57df\u9002\u5e94(MSDA)\u65b9\u6cd5\u5728\u5c06\u6a21\u578b\u9002\u5e94\u65b0\u76ee\u6807\u73af\u5883\u65f6\uff0c\u901a\u5e38\u9700\u8981\u5de8\u5927\u7684\u8bad\u7ec3\u53c2\u6570\u6216\u5728\u9002\u5e94\u8fc7\u7a0b\u4e2d\u8bbf\u95ee\u6e90\u57df\u6570\u636e\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u6d88\u8017\u663e\u8457\u589e\u52a0\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Source-free Adaptive Gated Experts (SAGE-reID)\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u901a\u8fc7\u65e0\u6e90\u57df\u9002\u5e94(UDA)\u8bad\u7ec3\u72ec\u7acb\u7684\u6e90\u57df\u7279\u5b9a\u4f4e\u79e9\u9002\u914d\u5668(LoRA)\u3002\u968f\u540e\uff0c\u5f15\u5165\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u95e8\u63a7\u7f51\u7edc\u6765\u52a8\u6001\u5206\u914dLoRA\u4e13\u5bb6\u7684\u6700\u4f18\u878d\u5408\u6743\u91cd\uff0c\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u8de8\u57df\u77e5\u8bc6\u8fc1\u79fb\u3002\u8be5\u65b9\u6cd5\u4fdd\u6301\u9aa8\u5e72\u7f51\u7edc\u53c2\u6570\u4e0d\u53d8\uff0cLoRA\u4e13\u5bb6\u53c2\u6570\u91cf\u6781\u5c0f\uff08\u5c0f\u4e8e\u9aa8\u5e72\u76842%\uff09\u3002", "result": "\u5728Market-1501\u3001DukeMTMC-reID\u548cMSMT17\u4e09\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSAGE-reID\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u5185\u5b58\u6d88\u8017\u548c\u8fc7\u62df\u5408\u98ce\u9669\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "SAGE-reID\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u6e90\u6570\u636e\u7684\u4eba\u5458\u91cd\u8bc6\u522b\u591a\u6e90\u57df\u9002\u5e94\u65b0\u65b9\u6cd5\u3002\u901a\u8fc7\u521b\u65b0\u6027\u5730\u5229\u7528\u4f4e\u79e9\u9002\u914d\u5668\u548c\u52a8\u6001\u95e8\u63a7\u7f51\u7edc\uff0c\u5b83\u89e3\u51b3\u4e86\u4f20\u7edfMSDA\u65b9\u6cd5\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2508.07286", "pdf": "https://arxiv.org/pdf/2508.07286", "abs": "https://arxiv.org/abs/2508.07286", "authors": ["Jian Chen", "Jinbao Tian", "Yankui Li", "Zhou Li"], "title": "Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Accurate information extraction from specialized texts is a critical\nchallenge, particularly for named entity recognition (NER) in the architecture,\nengineering, and construction (AEC) domain to support automated rule checking\n(ARC). The performance of standard pre-trained models is often constrained by\nthe domain gap, as they struggle to interpret the specialized terminology and\ncomplex relational contexts inherent in AEC texts. Although this issue can be\nmitigated by further pre-training on large, human-curated domain corpora, as\nexemplified by methods like ARCBERT, this approach is both labor-intensive and\ncost-prohibitive. Consequently, leveraging large language models (LLMs) for\nautomated knowledge generation has emerged as a promising alternative. However,\nthe optimal strategy for generating knowledge that can genuinely enhance\nsmaller, efficient models remains an open question. To address this, we propose\nARCE (augmented RoBERTa with contextualized elucidations), a novel approach\nthat systematically explores and optimizes this generation process. ARCE\nemploys an LLM to first generate a corpus of simple, direct explanations, which\nwe term Cote, and then uses this corpus to incrementally pre-train a RoBERTa\nmodel prior to its fine-tuning on the downstream task. Our extensive\nexperiments show that ARCE establishes a new state-of-the-art on a benchmark\nAEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a\nkey finding: simple, explanation-based knowledge proves surprisingly more\neffective than complex, role-based rationales for this task. The code is\npublicly available at:https://github.com/nxcc-lab/ARCE.", "AI": {"tldr": "\u9488\u5bf9AEC\u9886\u57dfNER\u4e2d\u6807\u51c6\u6a21\u578b\u8868\u73b0\u53d7\u9650\u4e14\u4f20\u7edf\u9884\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faARCE\uff0c\u5229\u7528LLM\u751f\u6210\u7b80\u5355\u89e3\u91ca\u8bed\u6599\uff08Cote\uff09\u9884\u8bad\u7ec3RoBERTa\u3002ARCE\u5728AEC\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86SOTA\uff0c\u5e76\u53d1\u73b0\u7b80\u5355\u89e3\u91ca\u6bd4\u590d\u6742\u63a8\u7406\u66f4\u6709\u6548\u3002", "motivation": "\u5efa\u7b51\u3001\u5de5\u7a0b\u3001\u65bd\u5de5\uff08AEC\uff09\u9886\u57df\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u9762\u4e34\u6311\u6218\uff0c\u6807\u51c6\u9884\u8bad\u7ec3\u6a21\u578b\u56e0\u9886\u57df\u5dee\u5f02\u8868\u73b0\u53d7\u9650\uff0c\u800c\u5927\u578b\u9886\u57df\u8bed\u6599\u7684\u8fdb\u4e00\u6b65\u9884\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u77e5\u8bc6\u751f\u6210\u65b9\u9762\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5982\u4f55\u6709\u6548\u751f\u6210\u77e5\u8bc6\u4ee5\u589e\u5f3a\u5c0f\u578b\u9ad8\u6548\u6a21\u578b\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51faARCE\uff08augmented RoBERTa with contextualized elucidations\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u5229\u7528\u4e00\u4e2aLLM\u751f\u6210\u4e00\u4e2a\u7531\u7b80\u5355\u3001\u76f4\u63a5\u89e3\u91ca\u7ec4\u6210\u7684\u8bed\u6599\u5e93\uff08Cote\uff09\uff0c\u7136\u540e\u4f7f\u7528\u6b64Cote\u8bed\u6599\u5e93\u5bf9RoBERTa\u6a21\u578b\u8fdb\u884c\u589e\u91cf\u9884\u8bad\u7ec3\uff0c\u6700\u540e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "ARCE\u5728\u57fa\u51c6AEC\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8677.20%\u7684Macro-F1\u5206\u6570\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002\u7814\u7a76\u7ed3\u679c\u8fd8\u8868\u660e\uff0c\u5bf9\u4e8e\u8be5\u4efb\u52a1\uff0c\u57fa\u4e8e\u7b80\u5355\u89e3\u91ca\u7684\u77e5\u8bc6\u6bd4\u57fa\u4e8e\u590d\u6742\u89d2\u8272\u7684\u63a8\u7406\u66f4\u6709\u6548\u3002", "conclusion": "ARCE\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528LLM\u751f\u6210\u7684\u7b80\u5355\u3001\u89e3\u91ca\u6027\u77e5\u8bc6\u5bf9\u5c0f\u578b\u6a21\u578b\u8fdb\u884c\u589e\u91cf\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86AEC\u9886\u57df\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u5e76\u8fbe\u5230\u4e86\u65b0\u7684SOTA\u3002\u8fd9\u8bc1\u660e\u4e86\u7b80\u5355\u3001\u89e3\u91ca\u6027\u77e5\u8bc6\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u51fa\u4eba\u610f\u6599\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.06986", "pdf": "https://arxiv.org/pdf/2508.06986", "abs": "https://arxiv.org/abs/2508.06986", "authors": ["Chonghua Han", "Yuan Yuan", "Yukun Liu", "Jingtao Ding", "Jie Feng", "Yong Li"], "title": "UniMove: A Unified Model for Multi-city Human Mobility Prediction", "categories": ["cs.LG"], "comment": "Accepted by SIGSPATIAL 2025", "summary": "Human mobility prediction is vital for urban planning, transportation\noptimization, and personalized services. However, the inherent randomness,\nnon-uniform time intervals, and complex patterns of human mobility, compounded\nby the heterogeneity introduced by varying city structures, infrastructure, and\npopulation densities, present significant challenges in modeling. Existing\nsolutions often require training separate models for each city due to distinct\nspatial representations and geographic coverage. In this paper, we propose\nUniMove, a unified model for multi-city human mobility prediction, addressing\ntwo challenges: (1) constructing universal spatial representations for\neffective token sharing across cities, and (2) modeling heterogeneous mobility\npatterns from varying city characteristics. We propose a trajectory-location\ndual-tower architecture, with a location tower for universal spatial encoding\nand a trajectory tower for sequential mobility modeling. We also design MoE\nTransformer blocks to adaptively select experts to handle diverse movement\npatterns. Extensive experiments across multiple datasets from diverse cities\ndemonstrate that UniMove truly embodies the essence of a unified model. By\nenabling joint training on multi-city data with mutual data enhancement, it\nsignificantly improves mobility prediction accuracy by over 10.2\\%. UniMove\nrepresents a key advancement toward realizing a true foundational model with a\nunified architecture for human mobility. We release the implementation at\nhttps://github.com/tsinghua-fib-lab/UniMove/.", "AI": {"tldr": "\u63d0\u51faUniMove\uff0c\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u57ce\u5e02\u4eba\u4f53\u79fb\u52a8\u9884\u6d4b\uff0c\u901a\u8fc7\u901a\u7528\u7a7a\u95f4\u8868\u793a\u548c\u5f02\u6784\u6a21\u5f0f\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4eba\u4f53\u79fb\u52a8\u9884\u6d4b\u5bf9\u57ce\u5e02\u89c4\u5212\u7b49\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u5185\u5728\u968f\u673a\u6027\u3001\u975e\u5747\u5300\u65f6\u95f4\u95f4\u9694\u3001\u590d\u6742\u6a21\u5f0f\u4ee5\u53ca\u4e0d\u540c\u57ce\u5e02\u7ed3\u6784\u5bfc\u81f4\u7684\u5f02\u6784\u6027\u6311\u6218\uff0c\u4e14\u73b0\u6709\u65b9\u6848\u901a\u5e38\u9700\u4e3a\u6bcf\u5ea7\u57ce\u5e02\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\u3002", "method": "\u63d0\u51faUniMove\u7edf\u4e00\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u8de8\u57ce\u5e02\u901a\u7528\u7a7a\u95f4\u8868\u793a\u548c\u5f02\u6784\u79fb\u52a8\u6a21\u5f0f\u5efa\u6a21\u3002\u91c7\u7528\u8f68\u8ff9-\u4f4d\u7f6e\u53cc\u5854\u67b6\u6784\uff1a\u4f4d\u7f6e\u5854\u7528\u4e8e\u901a\u7528\u7a7a\u95f4\u7f16\u7801\uff0c\u8f68\u8ff9\u5854\u7528\u4e8e\u5e8f\u5217\u79fb\u52a8\u5efa\u6a21\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1MoE Transformer\u5757\u4ee5\u81ea\u9002\u5e94\u5904\u7406\u4e0d\u540c\u79fb\u52a8\u6a21\u5f0f\u3002", "result": "\u901a\u8fc7\u5728\u591a\u4e2a\u57ce\u5e02\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0cUniMove\u901a\u8fc7\u591a\u57ce\u5e02\u6570\u636e\u8054\u5408\u8bad\u7ec3\u548c\u76f8\u4e92\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u79fb\u52a8\u9884\u6d4b\u7cbe\u5ea6\u8d85\u8fc710.2%\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u7edf\u4e00\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "UniMove\u662f\u5b9e\u73b0\u4eba\u4f53\u79fb\u52a8\u9886\u57df\u7edf\u4e00\u57fa\u7840\u6a21\u578b\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2508.07468", "pdf": "https://arxiv.org/pdf/2508.07468", "abs": "https://arxiv.org/abs/2508.07468", "authors": ["Stefan Szeider"], "title": "CP-Agent: Agentic Constraint Programming", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Translating natural language problem descriptions into formal constraint\nmodels remains a fundamental challenge in constraint programming, requiring\ndeep expertise in both the problem domain and modeling frameworks. Previous\napproaches to automating this translation have employed fixed workflows with\npredetermined modeling steps, failing on a significant number of benchmark\nproblems. We present a new approach using a pure agentic strategy without any\nfixed pipeline. We developed a general-purpose Python coding agent based on the\nReAct (Reason and Act) principle, utilizing a persistent IPython kernel for\nstateful code execution and iterative development. Rather than embedding\nconstraint programming logic into the agent architecture, domain-specific\nexpertise is injected solely through a carefully crafted project prompt. The\nagent combines this prompt-encoded knowledge with access to file operations and\ncode execution tools, enabling it to test hypotheses, debug failures, and\nverify solutions dynamically. Implemented in just a few hundred lines of code,\nthis architecture successfully solves all 101 problems of the CP-Bench\nconstraint programming benchmark set. The results suggest that constraint\nmodeling tasks require the combination of general coding tools and domain\nexpertise encoded in prompts, rather than specialized agent architectures or\npredefined workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7eaf\u4ee3\u7406\u7b56\u7565\uff0c\u65e0\u9700\u56fa\u5b9a\u6d41\u7a0b\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u7ffb\u8bd1\u4e3a\u7ea6\u675f\u6a21\u578b\uff0c\u6210\u529f\u89e3\u51b3\u6240\u6709CP-Bench\u57fa\u51c6\u6d4b\u8bd5\u95ee\u9898\u3002", "motivation": "\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u8f6c\u6362\u4e3a\u5f62\u5f0f\u5316\u7ea6\u675f\u6a21\u578b\u662f\u7ea6\u675f\u7f16\u7a0b\u4e2d\u7684\u4e00\u4e2a\u6839\u672c\u6027\u6311\u6218\uff0c\u9700\u8981\u6df1\u539a\u7684\u9886\u57df\u548c\u5efa\u6a21\u4e13\u4e1a\u77e5\u8bc6\u3002\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u91c7\u7528\u56fa\u5b9a\u5de5\u4f5c\u6d41\uff0c\u5728\u5927\u91cf\u57fa\u51c6\u95ee\u9898\u4e0a\u5931\u8d25\u3002", "method": "\u91c7\u7528\u7eaf\u4ee3\u7406\u7b56\u7565\uff0c\u65e0\u56fa\u5b9a\u6d41\u7a0b\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eReAct\uff08Reason and Act\uff09\u539f\u5219\u7684\u901a\u7528Python\u7f16\u7801\u4ee3\u7406\uff0c\u5229\u7528\u6301\u4e45\u7684IPython\u5185\u6838\u8fdb\u884c\u6709\u72b6\u6001\u7684\u4ee3\u7801\u6267\u884c\u548c\u8fed\u4ee3\u5f00\u53d1\u3002\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u9879\u76ee\u63d0\u793a\u6ce8\u5165\uff0c\u800c\u975e\u5d4c\u5165\u4ee3\u7406\u67b6\u6784\u3002\u4ee3\u7406\u7ed3\u5408\u63d0\u793a\u77e5\u8bc6\u4e0e\u6587\u4ef6\u64cd\u4f5c\u548c\u4ee3\u7801\u6267\u884c\u5de5\u5177\uff0c\u52a8\u6001\u6d4b\u8bd5\u3001\u8c03\u8bd5\u548c\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u8be5\u67b6\u6784\u4ec5\u7528\u51e0\u767e\u884c\u4ee3\u7801\u5b9e\u73b0\uff0c\u6210\u529f\u89e3\u51b3\u4e86CP-Bench\u7ea6\u675f\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e2d\u7684\u5168\u90e8101\u4e2a\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7ea6\u675f\u5efa\u6a21\u4efb\u52a1\u9700\u8981\u7ed3\u5408\u901a\u7528\u7f16\u7801\u5de5\u5177\u548c\u63d0\u793a\u4e2d\u7f16\u7801\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800c\u975e\u4f9d\u8d56\u4e13\u95e8\u7684\u4ee3\u7406\u67b6\u6784\u6216\u9884\u5b9a\u4e49\u7684\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2508.06845", "pdf": "https://arxiv.org/pdf/2508.06845", "abs": "https://arxiv.org/abs/2508.06845", "authors": ["Hamidreza Samadi", "Md Manjurul Ahsan", "Shivakumar Raman"], "title": "Hybrid Machine Learning Framework for Predicting Geometric Deviations from 3D Surface Metrology", "categories": ["cs.CV", "cs.CE", "eess.IV"], "comment": null, "summary": "This study addresses the challenge of accurately forecasting geometric\ndeviations in manufactured components using advanced 3D surface analysis.\nDespite progress in modern manufacturing, maintaining dimensional precision\nremains difficult, particularly for complex geometries. We present a\nmethodology that employs a high-resolution 3D scanner to acquire multi-angle\nsurface data from 237 components produced across different batches. The data\nwere processed through precise alignment, noise reduction, and merging\ntechniques to generate accurate 3D representations. A hybrid machine learning\nframework was developed, combining convolutional neural networks for feature\nextraction with gradient-boosted decision trees for predictive modeling. The\nproposed system achieved a prediction accuracy of 0.012 mm at a 95% confidence\nlevel, representing a 73% improvement over conventional statistical process\ncontrol methods. In addition to improved accuracy, the model revealed hidden\ncorrelations between manufacturing parameters and geometric deviations. This\napproach offers significant potential for automated quality control, predictive\nmaintenance, and design optimization in precision manufacturing, and the\nresulting dataset provides a strong foundation for future predictive modeling\nresearch.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5148\u8fdb\u76843D\u8868\u9762\u5206\u6790\u548c\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5236\u9020\u96f6\u4ef6\u51e0\u4f55\u504f\u5dee\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u5236\u9020\u53c2\u6570\u4e0e\u504f\u5dee\u95f4\u7684\u5173\u8054\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u4ee3\u5236\u9020\u4e1a\u6280\u672f\u5148\u8fdb\uff0c\u4f46\u5728\u590d\u6742\u51e0\u4f55\u96f6\u4ef6\u7684\u751f\u4ea7\u4e2d\uff0c\u51c6\u786e\u9884\u6d4b\u51e0\u4f55\u504f\u5dee\u548c\u4fdd\u6301\u5c3a\u5bf8\u7cbe\u5ea6\u4ecd\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u91c7\u7528\u9ad8\u5206\u8fa8\u73873D\u626b\u63cf\u4eea\u83b7\u53d6237\u4e2a\u96f6\u4ef6\u7684\u591a\u89d2\u5ea6\u8868\u9762\u6570\u636e\uff0c\u5e76\u8fdb\u884c\u6570\u636e\u5904\u7406\uff08\u5bf9\u9f50\u3001\u964d\u566a\u3001\u5408\u5e76\uff09\uff1b\u5f00\u53d1\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\uff08GBDT\uff09\u8fdb\u884c\u9884\u6d4b\u5efa\u6a21\u3002", "result": "\u9884\u6d4b\u7cbe\u5ea6\u8fbe\u52300.012\u6beb\u7c73\uff0895%\u7f6e\u4fe1\u6c34\u5e73\uff09\uff0c\u6bd4\u4f20\u7edf\u7edf\u8ba1\u8fc7\u7a0b\u63a7\u5236\u65b9\u6cd5\u63d0\u9ad873%\uff1b\u540c\u65f6\u63ed\u793a\u4e86\u5236\u9020\u53c2\u6570\u4e0e\u51e0\u4f55\u504f\u5dee\u4e4b\u95f4\u7684\u9690\u85cf\u5173\u8054\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7cbe\u5bc6\u5236\u9020\u7684\u81ea\u52a8\u5316\u8d28\u91cf\u63a7\u5236\u3001\u9884\u6d4b\u6027\u7ef4\u62a4\u548c\u8bbe\u8ba1\u4f18\u5316\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u9884\u6d4b\u5efa\u6a21\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u6570\u636e\u57fa\u7840\u3002"}}
{"id": "2508.07295", "pdf": "https://arxiv.org/pdf/2508.07295", "abs": "https://arxiv.org/abs/2508.07295", "authors": ["Yexing Du", "Kaiyuan Liu", "Youcheng Pan", "Zheng Chu", "Bo Yang", "Xiaocheng Feng", "Yang Xiang", "Ming Liu"], "title": "CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly popularized in the\nmultilingual world, ensuring hallucination-free factuality becomes markedly\ncrucial. However, existing benchmarks for evaluating the reliability of\nMultimodal Large Language Models (MLLMs) predominantly focus on textual or\nvisual modalities with a primary emphasis on English, which creates a gap in\nevaluation when processing multilingual input, especially in speech. To bridge\nthis gap, we propose a novel \\textbf{C}ross-lingual and \\textbf{C}ross-modal\n\\textbf{F}actuality benchmark (\\textbf{CCFQA}). Specifically, the CCFQA\nbenchmark contains parallel speech-text factual questions across 8 languages,\ndesigned to systematically evaluate MLLMs' cross-lingual and cross-modal\nfactuality capabilities. Our experimental results demonstrate that current\nMLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we\npropose a few-shot transfer learning strategy that effectively transfers the\nQuestion Answering (QA) capabilities of LLMs in English to multilingual Spoken\nQuestion Answering (SQA) tasks, achieving competitive performance with\nGPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a\nfoundational research resource to promote the development of MLLMs with more\nrobust and reliable speech understanding capabilities. Our code and dataset are\navailable at https://github.com/yxduir/ccfqa.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CCFQA\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u591a\u8bed\u8a00\u548c\u8de8\u6a21\u6001\uff08\u8bed\u97f3-\u6587\u672c\uff09\u73af\u5883\u4e0b\u7684\u4e8b\u5b9e\u6027\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u6709MLLMs\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5c11\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u4ee5\u63d0\u5347\u5176\u591a\u8bed\u8a00\u8bed\u97f3\u95ee\u7b54\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u9760\u6027\u7684\u57fa\u51c6\u4e3b\u8981\u96c6\u4e2d\u5728\u6587\u672c\u6216\u89c6\u89c9\u6a21\u6001\uff0c\u4e14\u4ee5\u82f1\u8bed\u4e3a\u4e3b\uff0c\u672a\u80fd\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u5728\u5904\u7406\u591a\u8bed\u8a00\u8f93\u5165\uff08\u7279\u522b\u662f\u8bed\u97f3\uff09\u65f6\u7684\u5e7b\u89c9\u6d88\u9664\u80fd\u529b\u548c\u4e8b\u5b9e\u6027\uff0c\u5bfc\u81f4\u8bc4\u4f30\u5b58\u5728\u7a7a\u767d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u201c\u8de8\u8bed\u8a00\u8de8\u6a21\u6001\u4e8b\u5b9e\u6027\u57fa\u51c6\u201d\uff08CCFQA\uff09\uff0c\u8be5\u57fa\u51c6\u5305\u542b8\u79cd\u8bed\u8a00\u7684\u5e76\u884c\u8bed\u97f3-\u6587\u672c\u4e8b\u5b9e\u6027\u95ee\u9898\uff0c\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30MLLMs\u7684\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u4e8b\u5b9e\u6027\u80fd\u529b\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u5c11\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u7528\u4e8e\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u82f1\u8bed\u95ee\u7b54\u80fd\u529b\u6709\u6548\u5730\u8fc1\u79fb\u5230\u591a\u8bed\u8a00\u8bed\u97f3\u95ee\u7b54\uff08SQA\uff09\u4efb\u52a1\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728CCFQA\u57fa\u51c6\u4e0a\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u6240\u63d0\u51fa\u7684\u5c11\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u80fd\u591f\u6709\u6548\u5730\u5c06\u82f1\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u7684\u95ee\u7b54\u80fd\u529b\u8fc1\u79fb\u5230\u591a\u8bed\u8a00\u8bed\u97f3\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u4ec5\u901a\u8fc75\u6b21\u6837\u672c\u8bad\u7ec3\uff0c\u4fbf\u80fd\u8fbe\u5230\u4e0eGPT-4o-mini-Audio\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "CCFQA\u57fa\u51c6\u7684\u53d1\u5e03\u5c06\u4f5c\u4e3a\u4e00\u9879\u57fa\u7840\u7814\u7a76\u8d44\u6e90\uff0c\u65e8\u5728\u4fc3\u8fdb\u5f00\u53d1\u5177\u6709\u66f4\u5f3a\u5927\u3001\u66f4\u53ef\u9760\u8bed\u97f3\u7406\u89e3\u80fd\u529b\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002\u540c\u65f6\uff0c\u6240\u63d0\u51fa\u7684\u5c11\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u4e3a\u63d0\u5347\u6a21\u578b\u5728\u591a\u8bed\u8a00\u8bed\u97f3\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.06991", "pdf": "https://arxiv.org/pdf/2508.06991", "abs": "https://arxiv.org/abs/2508.06991", "authors": ["Vojtech Halenka", "Ole-Christoffer Granmo", "Lei Jiao", "Per-Arne Andersen"], "title": "A Comparative Study of Feature Selection in Tsetlin Machines", "categories": ["cs.LG", "68T01, 68T05", "I.2.6; I.2.7; I.5.1"], "comment": "submitted to SGAI-2025: The 45th SGAI International Conference on\n  Innovative Techniques and Applications of Artificial Intelligence", "summary": "Feature Selection (FS) is crucial for improving model interpretability,\nreducing complexity, and sometimes for enhancing accuracy. The recently\nintroduced Tsetlin machine (TM) offers interpretable clause-based learning, but\nlacks established tools for estimating feature importance. In this paper, we\nadapt and evaluate a range of FS techniques for TMs, including classical filter\nand embedded methods as well as post-hoc explanation methods originally\ndeveloped for neural networks (e.g., SHAP and LIME) and a novel family of\nembedded scorers derived from TM clause weights and Tsetlin automaton (TA)\nstates. We benchmark all methods across 12 datasets, using evaluation\nprotocols, like Remove and Retrain (ROAR) strategy and Remove and Debias\n(ROAD), to assess causal impact. Our results show that TM-internal scorers not\nonly perform competitively but also exploit the interpretability of clauses to\nreveal interacting feature patterns. Simpler TM-specific scorers achieve\nsimilar accuracy retention at a fraction of the computational cost. This study\nestablishes the first comprehensive baseline for FS in TM and paves the way for\ndeveloping specialized TM-specific interpretability techniques.", "AI": {"tldr": "\u9996\u6b21\u4e3aTsetlin\u673a\u5668\uff08TM\uff09\u5efa\u7acb\u4e86\u7279\u5f81\u9009\u62e9\uff08FS\uff09\u7684\u7efc\u5408\u57fa\u7ebf\uff0c\u8bc4\u4f30\u4e86\u591a\u79cdFS\u6280\u672f\uff08\u5305\u62ec\u65b0\u63d0\u51fa\u7684TM\u5185\u90e8\u8bc4\u5206\u5668\uff09\uff0c\u53d1\u73b0TM\u5185\u90e8\u8bc4\u5206\u5668\u8868\u73b0\u826f\u597d\u4e14\u9ad8\u6548\u3002", "motivation": "\u7279\u5f81\u9009\u62e9\u5bf9\u63d0\u9ad8\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3001\u964d\u4f4e\u590d\u6742\u6027\u548c\u589e\u5f3a\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002Tsetlin\u673a\u5668\u867d\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u57fa\u4e8e\u5b50\u53e5\u7684\u5b66\u4e60\uff0c\u4f46\u7f3a\u4e4f\u6210\u719f\u7684\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u9002\u914d\u5e76\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217FS\u6280\u672f\u7528\u4e8eTsetlin\u673a\u5668\uff0c\u5305\u62ec\u7ecf\u5178\u8fc7\u6ee4\u65b9\u6cd5\u3001\u5d4c\u5165\u5f0f\u65b9\u6cd5\u3001\u6e90\u81ea\u795e\u7ecf\u7f51\u7edc\u7684\u4e8b\u540e\u89e3\u91ca\u65b9\u6cd5\uff08\u5982SHAP\u548cLIME\uff09\uff0c\u4ee5\u53ca\u57fa\u4e8eTM\u5b50\u53e5\u6743\u91cd\u548cTsetlin\u81ea\u52a8\u673a\u72b6\u6001\u7684\u65b0\u578b\u5d4c\u5165\u5f0f\u8bc4\u5206\u5668\u3002\u572812\u4e2a\u6570\u636e\u96c6\u4e0a\u901a\u8fc7Remove and Retrain (ROAR) \u548c Remove and Debias (ROAD) \u7b49\u8bc4\u4f30\u534f\u8bae\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "TM\u5185\u90e8\u8bc4\u5206\u5668\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u5e76\u80fd\u5229\u7528\u5b50\u53e5\u7684\u53ef\u89e3\u91ca\u6027\u63ed\u793a\u4ea4\u4e92\u7279\u5f81\u6a21\u5f0f\u3002\u66f4\u7b80\u5355\u7684TM\u4e13\u7528\u8bc4\u5206\u5668\u80fd\u4ee5\u663e\u8457\u964d\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u76f8\u4f3c\u7684\u7cbe\u5ea6\u4fdd\u7559\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aTsetlin\u673a\u5668\u4e2d\u7684\u7279\u5f81\u9009\u62e9\u5efa\u7acb\u4e86\u9996\u4e2a\u5168\u9762\u7684\u57fa\u7ebf\uff0c\u4e3a\u5f00\u53d1\u4e13\u95e8\u7684TM\u53ef\u89e3\u91ca\u6027\u6280\u672f\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.07485", "pdf": "https://arxiv.org/pdf/2508.07485", "abs": "https://arxiv.org/abs/2508.07485", "authors": ["Alexander Duffy", "Samuel J Paech", "Ishana Shastri", "Elizabeth Karpinski", "Baptiste Alloui-Cros", "Tyler Marques", "Matthew Lyle Olson"], "title": "Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "We present the first evaluation harness that enables any out-of-the-box,\nlocal, Large Language Models (LLMs) to play full-press Diplomacy without\nfine-tuning or specialized training. Previous work required frontier LLMs, or\nfine-tuning, due to the high complexity and information density of Diplomacy's\ngame state. Combined with the high variance of matches, these factors made\nDiplomacy prohibitive for study. In this work, we used data-driven iteration to\noptimize a textual game state representation such that a 24B model can reliably\ncomplete matches without any fine tuning. We develop tooling to facilitate\nhypothesis testing and statistical analysis, and we present case studies on\npersuasion, aggressive playstyles, and performance across a range of models. We\nconduct a variety of experiments across many popular LLMs, finding the larger\nmodels perform the best, but the smaller models still play adequately. We also\nintroduce Critical State Analysis: an experimental protocol for rapidly\niterating and analyzing key moments in a game at depth. Our harness\ndemocratizes the evaluation of strategic reasoning in LLMs by eliminating the\nneed for fine-tuning, and it provides insights into how these capabilities\nemerge naturally from widely used LLMs. Our code is available in the supplement\nand will be open sourced.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u8bc4\u4f30\u5de5\u5177\uff0c\u4f7f\u901a\u7528LLM\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u73a9\u300a\u5916\u4ea4\u300b\u6e38\u620f\uff0c\u5e76\u6df1\u5165\u5206\u6790\u5176\u6218\u7565\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4ee5\u5f80\u7814\u7a76\u53d7\u9650\u4e8e\u300a\u5916\u4ea4\u300b\u6e38\u620f\u7684\u9ad8\u590d\u6742\u6027\u548c\u4fe1\u606f\u5bc6\u5ea6\uff0c\u9700\u8981\u9876\u5c16\u6216\u5fae\u8c03\u8fc7\u7684LLM\uff0c\u4e14\u6bd4\u8d5b\u7ed3\u679c\u53d8\u5f02\u6027\u5927\uff0c\u5bfc\u81f4\u8be5\u9886\u57df\u7814\u7a76\u53d7\u963b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8bc4\u4f30\u5de5\u5177\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u8fed\u4ee3\u4f18\u5316\u4e86\u6587\u672c\u6e38\u620f\u72b6\u6001\u8868\u793a\uff0c\u4f7f\u672a\u5fae\u8c03\u768424B\u6a21\u578b\u80fd\u53ef\u9760\u5b8c\u6210\u6bd4\u8d5b\uff1b\u5f00\u53d1\u4e86\u5047\u8bbe\u68c0\u9a8c\u548c\u7edf\u8ba1\u5206\u6790\u5de5\u5177\uff1b\u5f15\u5165\u4e86\u5173\u952e\u72b6\u6001\u5206\u6790\u534f\u8bae\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u8f83\u5927\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u8f83\u5c0f\u6a21\u578b\u4e5f\u80fd\u5145\u5206\u8fdb\u884c\u6e38\u620f\uff1b\u5b8c\u6210\u4e86\u8bf4\u670d\u3001\u4fb5\u7565\u6027\u6e38\u620f\u98ce\u683c\u53ca\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u7684\u6848\u4f8b\u7814\u7a76\u3002", "conclusion": "\u8be5\u8bc4\u4f30\u5de5\u5177\u901a\u8fc7\u6d88\u9664\u5fae\u8c03\u9700\u6c42\uff0c\u4f7fLLM\u6218\u7565\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u6c11\u4e3b\u5316\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u8fd9\u4e9b\u80fd\u529b\u5982\u4f55\u5728\u901a\u7528LLM\u4e2d\u81ea\u7136\u6d8c\u73b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.06853", "pdf": "https://arxiv.org/pdf/2508.06853", "abs": "https://arxiv.org/abs/2508.06853", "authors": ["L. D. M. S. Sai Teja", "Ashok Urlana", "Pruthwik Mishra"], "title": "AGIC: Attention-Guided Image Captioning to Improve Caption Relevance", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 5 Figures", "summary": "Despite significant progress in image captioning, generating accurate and\ndescriptive captions remains a long-standing challenge. In this study, we\npropose Attention-Guided Image Captioning (AGIC), which amplifies salient\nvisual regions directly in the feature space to guide caption generation. We\nfurther introduce a hybrid decoding strategy that combines deterministic and\nprobabilistic sampling to balance fluency and diversity. To evaluate AGIC, we\nconduct extensive experiments on the Flickr8k and Flickr30k datasets. The\nresults show that AGIC matches or surpasses several state-of-the-art models\nwhile achieving faster inference. Moreover, AGIC demonstrates strong\nperformance across multiple evaluation metrics, offering a scalable and\ninterpretable solution for image captioning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6ce8\u610f\u529b\u5f15\u5bfc\u56fe\u50cf\u63cf\u8ff0\uff08AGIC\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u653e\u5927\u89c6\u89c9\u7279\u5f81\u533a\u57df\u548c\u6df7\u5408\u89e3\u7801\u7b56\u7565\uff0c\u5728\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f18\u6216\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u66f4\u5feb\u3002", "motivation": "\u5c3d\u7ba1\u56fe\u50cf\u63cf\u8ff0\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u751f\u6210\u51c6\u786e\u548c\u63cf\u8ff0\u6027\u5f3a\u7684\u6587\u672c\u4ecd\u7136\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u6ce8\u610f\u529b\u5f15\u5bfc\u56fe\u50cf\u63cf\u8ff0\uff08AGIC\uff09\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5728\u7279\u5f81\u7a7a\u95f4\u76f4\u63a5\u653e\u5927\u663e\u8457\u89c6\u89c9\u533a\u57df\u6765\u6307\u5bfc\u63cf\u8ff0\u751f\u6210\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u7ed3\u5408\u786e\u5b9a\u6027\u548c\u6982\u7387\u91c7\u6837\u7684\u6df7\u5408\u89e3\u7801\u7b56\u7565\uff0c\u4ee5\u5e73\u8861\u63cf\u8ff0\u7684\u6d41\u7545\u6027\u548c\u591a\u6837\u6027\u3002", "result": "AGIC\u6a21\u578b\u5728Flickr8k\u548cFlickr30k\u6570\u636e\u96c6\u4e0a\u4e0e\u591a\u4e2a\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u6301\u5e73\u6216\u8d85\u8d8a\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002\u8be5\u6a21\u578b\u5728\u591a\u9879\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "AGIC\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u63cf\u8ff0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07308", "pdf": "https://arxiv.org/pdf/2508.07308", "abs": "https://arxiv.org/abs/2508.07308", "authors": ["Cristian Cosentino", "Annamaria Defilippo", "Marco Dossena", "Christopher Irwin", "Sara Joubbi", "Pietro Li\u00f2"], "title": "HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "HealthBranches is a novel benchmark dataset for medical Question-Answering\n(Q&A), specifically designed to evaluate complex reasoning in Large Language\nModels (LLMs). This dataset is generated through a semi-automated pipeline that\ntransforms explicit decision pathways from medical source into realistic\npatient cases with associated questions and answers. Covering 4,063 case\nstudies across 17 healthcare topics, each data point is based on clinically\nvalidated reasoning chains. HealthBranches supports both open-ended and\nmultiple-choice question formats and uniquely includes the full reasoning path\nfor each Q&A. Its structured design enables robust evaluation of LLMs'\nmulti-step inference capabilities, including their performance in structured\nRetrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a\nfoundation for the development of more trustworthy, interpretable, and\nclinically reliable LLMs in high-stakes domains while also serving as a\nvaluable resource for educational purposes.", "AI": {"tldr": "HealthBranches\u662f\u4e00\u4e2a\u65b0\u578b\u533b\u7597\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4e13\u4e3a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u800c\u8bbe\u8ba1\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u533b\u7597\u9886\u57df\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u8bc4\u4f30\u5176\u591a\u6b65\u63a8\u7406\u548c\u7ed3\u6784\u5316\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u80fd\u529b\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u5f00\u53d1\u66f4\u53ef\u4fe1\u8d56\u548c\u4e34\u5e8a\u53ef\u9760\u7684LLMs\u3002", "method": "\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u5c06\u533b\u5b66\u6765\u6e90\u7684\u660e\u786e\u51b3\u7b56\u8def\u5f84\u8f6c\u6362\u4e3a\u771f\u5b9e\u7684\u60a3\u8005\u75c5\u4f8b\u3001\u76f8\u5173\u95ee\u9898\u548c\u7b54\u6848\u6765\u751f\u6210\u6570\u636e\u96c6\u3002", "result": "\u521b\u5efa\u4e86HealthBranches\u6570\u636e\u96c6\uff0c\u5305\u542b4,063\u4e2a\u75c5\u4f8b\u7814\u7a76\uff0c\u6db5\u76d617\u4e2a\u533b\u7597\u4e3b\u9898\u3002\u6bcf\u4e2a\u6570\u636e\u70b9\u90fd\u57fa\u4e8e\u4e34\u5e8a\u9a8c\u8bc1\u7684\u63a8\u7406\u94fe\uff0c\u652f\u6301\u5f00\u653e\u5f0f\u548c\u591a\u9879\u9009\u62e9\u9898\u683c\u5f0f\uff0c\u5e76\u72ec\u7279\u5730\u5305\u542b\u6bcf\u4e2a\u95ee\u7b54\u7684\u5b8c\u6574\u63a8\u7406\u8def\u5f84\u3002", "conclusion": "HealthBranches\u4e3a\u5f00\u53d1\u66f4\u503c\u5f97\u4fe1\u8d56\u3001\u53ef\u89e3\u91ca\u548c\u4e34\u5e8a\u53ef\u9760\u7684LLMs\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u540c\u65f6\u4e5f\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u6559\u80b2\u8d44\u6e90\u3002"}}
{"id": "2508.06997", "pdf": "https://arxiv.org/pdf/2508.06997", "abs": "https://arxiv.org/abs/2508.06997", "authors": ["Helbert Paat", "Guohao Shen"], "title": "Conformal Set-based Human-AI Complementarity with Multiple Experts", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.MA"], "comment": "Accepted at AAMAS 2025. Code available at:\n  https://github.com/paathelb/conformal_hai_multiple", "summary": "Decision support systems are designed to assist human experts in\nclassification tasks by providing conformal prediction sets derived from a\npre-trained model. This human-AI collaboration has demonstrated enhanced\nclassification performance compared to using either the model or the expert\nindependently. In this study, we focus on the selection of instance-specific\nexperts from a pool of multiple human experts, contrasting it with existing\nresearch that typically focuses on single-expert scenarios. We characterize the\nconditions under which multiple experts can benefit from the conformal sets.\nWith the insight that only certain experts may be relevant for each instance,\nwe explore the problem of subset selection and introduce a greedy algorithm\nthat utilizes conformal sets to identify the subset of expert predictions that\nwill be used in classifying an instance. This approach is shown to yield better\nperformance compared to naive methods for human subset selection. Based on real\nexpert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation\nstudy indicates that our proposed greedy algorithm achieves near-optimal\nsubsets, resulting in improved classification performance among multiple\nexperts.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u591a\u4e13\u5bb6\u4eba\u673a\u534f\u4f5c\u5206\u7c7b\u4e2d\u7684\u5b9e\u4f8b\u7279\u5b9a\u4e13\u5bb6\u5b50\u96c6\u9009\u62e9\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u4fdd\u5f62\u9884\u6d4b\u96c6\u7684\u8d2a\u5a6a\u7b97\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4fa7\u91cd\u4e8e\u5355\u4e13\u5bb6\u4eba\u673a\u534f\u4f5c\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u5b58\u5728\u591a\u4f4d\u4eba\u7c7b\u4e13\u5bb6\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5982\u4f55\u4ece\u591a\u4e13\u5bb6\u6c60\u4e2d\u4e3a\u7279\u5b9a\u5b9e\u4f8b\u9009\u62e9\u6700\u76f8\u5173\u7684\u4e13\u5bb6\u5b50\u96c6\uff0c\u4ee5\u671f\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u8fdb\u4e00\u6b65\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4fdd\u5f62\u9884\u6d4b\u96c6\u7684\u8d2a\u5a6a\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u5e76\u9009\u62e9\u9488\u5bf9\u7279\u5b9a\u5b9e\u4f8b\u7684\u6700\u4f18\u4e13\u5bb6\u5b50\u96c6\u3002\u8be5\u7b97\u6cd5\u5229\u7528\u4fdd\u5f62\u9884\u6d4b\u96c6\u6765\u6307\u5bfc\u4e13\u5bb6\u9009\u62e9\u8fc7\u7a0b\uff0c\u4ee5\u878d\u5408\u6240\u9009\u4e13\u5bb6\u7684\u9884\u6d4b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u57fa\u4e8eCIFAR-10H\u548cImageNet-16H\u6570\u636e\u96c6\u7684\u771f\u5b9e\u4e13\u5bb6\u9884\u6d4b\u4eff\u771f\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8d2a\u5a6a\u7b97\u6cd5\u80fd\u591f\u8bc6\u522b\u51fa\u63a5\u8fd1\u6700\u4f18\u7684\u4e13\u5bb6\u5b50\u96c6\uff0c\u5e76\u4e14\u4e0e\u6734\u7d20\u7684\u4eba\u7c7b\u5b50\u96c6\u9009\u62e9\u65b9\u6cd5\u76f8\u6bd4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u4e13\u5bb6\u534f\u4f5c\u7684\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u4fdd\u5f62\u9884\u6d4b\u96c6\u4e0e\u8d2a\u5a6a\u7b97\u6cd5\u8fdb\u884c\u5b9e\u4f8b\u7279\u5b9a\u7684\u4e13\u5bb6\u5b50\u96c6\u9009\u62e9\uff0c\u80fd\u591f\u6709\u6548\u4f18\u5316\u591a\u4e13\u5bb6\u4eba\u673a\u534f\u4f5c\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u63d0\u5347\u590d\u6742\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u4eba\u673a\u667a\u80fd\u534f\u540c\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.07575", "pdf": "https://arxiv.org/pdf/2508.07575", "abs": "https://arxiv.org/abs/2508.07575", "authors": ["Shiqing Fan", "Xichen Ding", "Liang Zhang", "Linjian Mo"], "title": "MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark", "categories": ["cs.AI"], "comment": "Benchmarks and Source Code Released", "summary": "LLMs' capabilities are enhanced by using function calls to integrate various\ndata sources or API results into the context window. Typical tools include\nsearch, web crawlers, maps, financial data, file systems, and browser usage,\netc. Integrating these data sources or functions requires a standardized\nmethod. The Model Context Protocol (MCP) provides a standardized way to supply\ncontext to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use\nabilities suffer from several issues. First, there's a lack of comprehensive\ndatasets or benchmarks to evaluate various MCP tools. Second, the diverse\nformats of response from MCP tool call execution further increase the\ndifficulty of evaluation. Additionally, unlike existing tool-use benchmarks\nwith high success rates in functions like programming and math functions, the\nsuccess rate of real-world MCP tool is not guaranteed and varies across\ndifferent MCP servers. Furthermore, the LLMs' context window also limits the\nnumber of available tools that can be called in a single run, because the\ntextual descriptions of tool and the parameters have long token length for an\nLLM to process all at once. To help address the challenges of evaluating LLMs'\nperformance on calling MCP tools, we propose MCPToolBench++, a large-scale,\nmulti-domain AI Agent tool use benchmark. As of July 2025, this benchmark is\nbuild upon marketplace of over 4k MCP servers from more than 40 categories,\ncollected from the MCP marketplaces and GitHub communities. The datasets\nconsist of both single-step and multi-step tool calls across different\ncategories. We evaluated SOTA LLMs with agentic abilities on this benchmark and\nreported the results.", "AI": {"tldr": "\u63d0\u51faMCPToolBench++\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u9886\u57df\u7684AI Agent\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\uff0c\u65e8\u5728\u89e3\u51b3\u8bc4\u4f30LLMs\u8c03\u7528MCP\u5de5\u5177\u65f6\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u8bc4\u4f30LLM\u548cAI Agent\u7684MCP\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u5b58\u5728\u591a\u9879\u95ee\u9898\uff1a\u7f3a\u4e4f\u5168\u9762\u7684\u6570\u636e\u96c6\u6216\u57fa\u51c6\uff1bMCP\u5de5\u5177\u8c03\u7528\u7ed3\u679c\u683c\u5f0f\u591a\u6837\u5316\uff1b\u5b9e\u9645\u5de5\u5177\u8c03\u7528\u6210\u529f\u7387\u4e0d\u7a33\u5b9a\uff1b\u4ee5\u53caLLM\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u53ef\u7528\u5de5\u5177\u6570\u91cf\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u4e13\u4e1a\u7684\u8bc4\u4f30\u57fa\u51c6\u6765\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86MCPToolBench++\uff0c\u8fd9\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u9886\u57df\u7684AI Agent\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u5efa\u7acb\u5728\u6765\u81ea40\u591a\u4e2a\u7c7b\u522b\u76844000\u591a\u4e2aMCP\u670d\u52a1\u5668\u4e4b\u4e0a\uff0c\u6570\u636e\u96c6\u5305\u542b\u5355\u6b65\u548c\u591a\u6b65\u5de5\u5177\u8c03\u7528\u3002\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u6b64\u57fa\u51c6\u8bc4\u4f30\u4e86SOTA LLMs\u7684Agent\u80fd\u529b\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86MCPToolBench++\u57fa\u51c6\uff0c\u5e76\u5229\u7528\u5176\u8bc4\u4f30\u4e86\u5f53\u524d\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u5177\u5907Agent\u80fd\u529b\u7684LLM\u3002\u7814\u7a76\u62a5\u544a\u4e86\u8fd9\u4e9b\u8bc4\u4f30\u7ed3\u679c\u3002", "conclusion": "MCPToolBench++\u4e3a\u89e3\u51b3LLM\u8c03\u7528MCP\u5de5\u5177\u7684\u8bc4\u4f30\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u4e14\u5927\u89c4\u6a21\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8AI Agent\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u7814\u7a76\u548c\u53d1\u5c55\u3002"}}
{"id": "2508.06857", "pdf": "https://arxiv.org/pdf/2508.06857", "abs": "https://arxiv.org/abs/2508.06857", "authors": ["Mengxue Jia", "Zhihua Allen-Zhao", "You Zhao", "Sanyang Liu"], "title": "A Joint Sparse Self-Representation Learning Method for Multiview Clustering", "categories": ["cs.CV", "cs.DS"], "comment": null, "summary": "Multiview clustering (MC) aims to group samples using consistent and\ncomplementary information across various views. The subspace clustering, as a\nfundamental technique of MC, has attracted significant attention. In this\npaper, we propose a novel joint sparse self-representation learning model for\nMC, where a featured difference is the extraction of view-specific local\ninformation by introducing cardinality (i.e., $\\ell_0$-norm) constraints\ninstead of Graph-Laplacian regularization. Specifically, under each view,\ncardinality constraints directly restrict the samples used in the\nself-representation stage to extract reliable local and global structure\ninformation, while the low-rank constraint aids in revealing a global coherent\nstructure in the consensus affinity matrix during merging. The attendant\nchallenge is that Augmented Lagrange Method (ALM)-based alternating\nminimization algorithms cannot guarantee convergence when applied directly to\nour nonconvex, nonsmooth model, thus resulting in poor generalization ability.\nTo address it, we develop an alternating quadratic penalty (AQP) method with\nglobal convergence, where two subproblems are iteratively solved by closed-form\nsolutions. Empirical results on six standard datasets demonstrate the\nsuperiority of our model and AQP method, compared to eight state-of-the-art\nalgorithms.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07321", "pdf": "https://arxiv.org/pdf/2508.07321", "abs": "https://arxiv.org/abs/2508.07321", "authors": ["Shubhra Ghosh", "Abhilekh Borah", "Aditya Kumar Guru", "Kripabandhu Ghosh"], "title": "ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "The rapid proliferation of Large Language Models (LLMs) has significantly\ncontributed to the development of equitable AI systems capable of factual\nquestion-answering (QA). However, no known study tests the LLMs' robustness\nwhen presented with obfuscated versions of questions. To systematically\nevaluate these limitations, we propose a novel technique, ObfusQAte and,\nleveraging the same, introduce ObfusQA, a comprehensive, first of its kind,\nframework with multi-tiered obfuscation levels designed to examine LLM\ncapabilities across three distinct dimensions: (i) Named-Entity Indirection,\n(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these\nfine-grained distinctions in language, ObfusQA provides a comprehensive\nbenchmark for evaluating LLM robustness and adaptability. Our study observes\nthat LLMs exhibit a tendency to fail or generate hallucinated responses when\nconfronted with these increasingly nuanced variations. To foster research in\nthis direction, we make ObfusQAte publicly available.", "AI": {"tldr": "LLMs\u5728\u6a21\u7cca\u5316\u95ee\u9898\u4e0a\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5145\u5206\u6d4b\u8bd5\u3002\u672c\u7814\u7a76\u63d0\u51faObfusQA\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u4e0d\u540c\u6a21\u7cca\u5316\u7a0b\u5ea6\u4e0b\u7684\u6027\u80fd\uff0c\u53d1\u73b0LLMs\u6613\u5931\u8d25\u6216\u4ea7\u751f\u5e7b\u89c9\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e8b\u5b9e\u95ee\u7b54\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u5176\u5728\u9762\u5bf9\u6a21\u7cca\u5316\u95ee\u9898\u65f6\u7684\u9c81\u68d2\u6027\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\u7684\u7814\u7a76\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u7cca\u5316\u6280\u672fObfusQAte\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f15\u5165\u4e86ObfusQA\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5177\u6709\u591a\u5c42\u6a21\u7cca\u5316\u7ea7\u522b\uff0c\u4ece\u547d\u540d\u5b9e\u4f53\u95f4\u63a5\u6027\u3001\u5e72\u6270\u7269\u95f4\u63a5\u6027\u548c\u4e0a\u4e0b\u6587\u8fc7\u8f7d\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30LLMs\u7684\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53LLMs\u9762\u5bf9\u8fd9\u4e9b\u65e5\u76ca\u7ec6\u5fae\u7684\u6a21\u7cca\u5316\u53d8\u4f53\u65f6\uff0c\u5b83\u4eec\u503e\u5411\u4e8e\u5931\u8d25\u6216\u751f\u6210\u5e7b\u89c9\u54cd\u5e94\u3002", "conclusion": "ObfusQA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8bc4\u4f30LLM\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u7684\u5168\u9762\u57fa\u51c6\uff0c\u63ed\u793a\u4e86LLMs\u5728\u5904\u7406\u6a21\u7cca\u5316\u95ee\u9898\u65f6\u7684\u5c40\u9650\u6027\u3002\u4e3a\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\uff0cObfusQAte\u5df2\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2508.07016", "pdf": "https://arxiv.org/pdf/2508.07016", "abs": "https://arxiv.org/abs/2508.07016", "authors": ["Jianfei Wu", "Wenmian Yang", "Bingning Liu", "Weijia Jia"], "title": "TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "Time series forecasting is critical across various domains, such as weather,\nfinance and real estate forecasting, as accurate forecasts support informed\ndecision-making and risk mitigation. While recent deep learning models have\nimproved predictive capabilities, they often overlook time-lagged\ncross-correlations between related sequences, which are crucial for capturing\ncomplex temporal relationships. To address this, we propose the Time-Lagged\nCross-Correlations-based Sequence Prediction framework (TLCCSP), which enhances\nforecasting accuracy by effectively integrating time-lagged cross-correlated\nsequences. TLCCSP employs the Sequence Shifted Dynamic Time Warping (SSDTW)\nalgorithm to capture lagged correlations and a contrastive learning-based\nencoder to efficiently approximate SSDTW distances.\n  Experimental results on weather, finance and real estate time series datasets\ndemonstrate the effectiveness of our framework. On the weather dataset, SSDTW\nreduces mean squared error (MSE) by 16.01% compared with single-sequence\nmethods, while the contrastive learning encoder (CLE) further decreases MSE by\n17.88%. On the stock dataset, SSDTW achieves a 9.95% MSE reduction, and CLE\nreduces it by 6.13%. For the real estate dataset, SSDTW and CLE reduce MSE by\n21.29% and 8.62%, respectively. Additionally, the contrastive learning approach\ndecreases SSDTW computational time by approximately 99%, ensuring scalability\nand real-time applicability across multiple time series forecasting tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTLCCSP\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u65f6\u6ede\u4ea4\u53c9\u76f8\u5173\u5e8f\u5217\u663e\u8457\u63d0\u9ad8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u5e38\u5ffd\u7565\u5e8f\u5217\u95f4\u91cd\u8981\u7684\u65f6\u6ede\u4ea4\u53c9\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u9884\u6d4b\u7cbe\u5ea6\u53d7\u9650\uff0c\u800c\u51c6\u786e\u9884\u6d4b\u5bf9\u51b3\u7b56\u548c\u98ce\u9669\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u65f6\u6ede\u4ea4\u53c9\u76f8\u5173\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff08TLCCSP\uff09\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u5e8f\u5217\u79fb\u4f4d\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08SSDTW\uff09\u7b97\u6cd5\u6355\u83b7\u65f6\u6ede\u76f8\u5173\u6027\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u7f16\u7801\u5668\u9ad8\u6548\u8fd1\u4f3cSSDTW\u8ddd\u79bb\uff0c\u4ee5\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u5728\u6c14\u8c61\u6570\u636e\u96c6\u4e2d\uff0cSSDTW\u4f7fMSE\u964d\u4f4e16.01%\uff0c\u5bf9\u6bd4\u5b66\u4e60\u7f16\u7801\u5668\uff08CLE\uff09\u8fdb\u4e00\u6b65\u964d\u4f4e17.88%\u3002\u5728\u80a1\u7968\u6570\u636e\u96c6\u4e2d\uff0cSSDTW\u4f7fMSE\u964d\u4f4e9.95%\uff0cCLE\u964d\u4f4e6.13%\u3002\u5728\u623f\u5730\u4ea7\u6570\u636e\u96c6\u4e2d\uff0cSSDTW\u548cCLE\u5206\u522b\u4f7fMSE\u964d\u4f4e21.29%\u548c8.62%\u3002\u6b64\u5916\uff0c\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5c06SSDTW\u7684\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u4e86\u7ea699%\u3002", "conclusion": "TLCCSP\u6846\u67b6\u901a\u8fc7\u6709\u6548\u6574\u5408\u65f6\u6ede\u4ea4\u53c9\u76f8\u5173\u6027\u663e\u8457\u63d0\u9ad8\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4e14\u5bf9\u6bd4\u5b66\u4e60\u7684\u5e94\u7528\u5927\u5e45\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u591a\u79cd\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u7684\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2508.06869", "pdf": "https://arxiv.org/pdf/2508.06869", "abs": "https://arxiv.org/abs/2508.06869", "authors": ["Jianxiang He", "Shaoguang Wang", "Weiyu Guo", "Meisheng Hong", "Jungang Li", "Yijie Xu", "Ziyang Chen", "Hui Xiong"], "title": "VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding", "categories": ["cs.CV", "cs.AI", "I.2.10"], "comment": "9 pages,3 figures", "summary": "Long video understanding presents a significant challenge to multimodal large\nlanguage models (MLLMs) primarily due to the immense data scale. A critical and\nwidely adopted strategy for making this task computationally tractable is\nkeyframe retrieval, which seeks to identify a sparse set of video frames that\nare most salient to a given textual query. However, the efficacy of this\napproach is hindered by weak multimodal alignment between textual queries and\nvisual content and fails to capture the complex temporal semantic information\nrequired for precise reasoning. To address this, we propose Visual-Subtitle\nIntegeration(VSI), a multimodal keyframe search method that integrates\nsubtitles, timestamps, and scene boundaries into a unified multimodal search\nprocess. The proposed method captures the visual information of video frames as\nwell as the complementary textual information through a dual-stream search\nmechanism by Video Search Stream as well as Subtitle Match Stream,\nrespectively, and improves the keyframe search accuracy through the interaction\nof the two search streams. Experimental results show that VSI achieve 40.00%\nkey frame localization accuracy on the text-relevant subset of LongVideoBench\nand 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive\nbaselines by 20.35% and 15.79%, respectively. Furthermore, on the\nLongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA\ntasks, demonstrating the robustness and generalizability of the proposed\nmultimodal search strategy.", "AI": {"tldr": "\u9488\u5bf9\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u5173\u952e\u5e27\u68c0\u7d22\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u4e0e\u65f6\u5e8f\u8bed\u4e49\u6355\u83b7\u4e0d\u8db3\uff0cVSI\u63d0\u51fa\u4e00\u79cd\u878d\u5408\u5b57\u5e55\u3001\u65f6\u95f4\u6233\u548c\u573a\u666f\u8fb9\u754c\u7684\u53cc\u6d41\u591a\u6a21\u6001\u5173\u952e\u5e27\u641c\u7d22\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5173\u952e\u5e27\u5b9a\u4f4d\u548c\u957f\u89c6\u9891\u95ee\u7b54\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u957f\u89c6\u9891\u7406\u89e3\u5bf9MLLMs\u6784\u6210\u6311\u6218\uff0c\u73b0\u6709\u5173\u952e\u5e27\u68c0\u7d22\u65b9\u6cd5\u56e0\u591a\u6a21\u6001\u5bf9\u9f50\u5f31\u548c\u672a\u80fd\u6355\u6349\u590d\u6742\u65f6\u5e8f\u8bed\u4e49\u4fe1\u606f\uff0c\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u6548\u679c\u53d7\u9650\u3002", "method": "\u63d0\u51faVisual-Subtitle Integration (VSI) \u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u5b57\u5e55\u3001\u65f6\u95f4\u6233\u548c\u573a\u666f\u8fb9\u754c\uff0c\u91c7\u7528\u89c6\u9891\u641c\u7d22\u6d41\u548c\u5b57\u5e55\u5339\u914d\u6d41\u7684\u53cc\u6d41\u641c\u7d22\u673a\u5236\uff0c\u6355\u83b7\u89c6\u89c9\u4e0e\u6587\u672c\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u53cc\u6d41\u4ea4\u4e92\u63d0\u5347\u5173\u952e\u5e27\u641c\u7d22\u7cbe\u5ea6\u3002", "result": "VSI\u5728LongVideoBench\u7684\u6587\u672c\u76f8\u5173\u5b50\u96c6\u4e0a\u5173\u952e\u5e27\u5b9a\u4f4d\u51c6\u786e\u7387\u8fbe40.00%\uff0c\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u8fbe68.48%\uff0c\u5206\u522b\u8d85\u8d8a\u57fa\u7ebf20.35%\u548c15.79%\u3002\u540c\u65f6\uff0cVSI\u5728\u4e2d\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "conclusion": "VSI\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5173\u952e\u5e27\u68c0\u7d22\u96be\u9898\uff0c\u901a\u8fc7\u5176\u521b\u65b0\u7684\u591a\u6a21\u6001\u641c\u7d22\u7b56\u7565\uff0c\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u95ee\u7b54\u6027\u80fd\u3002"}}
{"id": "2508.07325", "pdf": "https://arxiv.org/pdf/2508.07325", "abs": "https://arxiv.org/abs/2508.07325", "authors": ["Dean Geckt", "Melinda Fricke", "Shuly Wintner"], "title": "Strategies of Code-switching in Human-Machine Dialogs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Most people are multilingual, and most multilinguals code-switch, yet the\ncharacteristics of code-switched language are not fully understood. We\ndeveloped a chatbot capable of completing a Map Task with human participants\nusing code-switched Spanish and English. In two experiments, we prompted the\nbot to code-switch according to different strategies, examining (1) the\nfeasibility of such experiments for investigating bilingual language use, and\n(2) whether participants would be sensitive to variations in discourse and\ngrammatical patterns. Participants generally enjoyed code-switching with our\nbot as long as it produced predictable code-switching behavior; when\ncode-switching was random or ungrammatical (as when producing unattested\nincongruent mixed-language noun phrases, such as `la fork'), participants\nenjoyed the task less and were less successful at completing it. These results\nunderscore the potential downsides of deploying insufficiently developed\nmultilingual language technology, while also illustrating the promise of such\ntechnology for conducting research on bilingual language use.", "AI": {"tldr": "\u901a\u8fc7\u4e0e\u80fd\u8fdb\u884c\u897f\u82f1\u8bed\u7801\u8f6c\u6362\u7684\u804a\u5929\u673a\u5668\u4eba\u8fdb\u884c\u5730\u56fe\u4efb\u52a1\u5b9e\u9a8c\uff0c\u53d1\u73b0\u7528\u6237\u66f4\u559c\u6b22\u53ef\u9884\u6d4b\u7684\u7801\u8f6c\u6362\uff0c\u5e76\u80fd\u611f\u77e5\u5230\u4e0d\u5f53\u7684\u7801\u8f6c\u6362\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u6280\u672f\u5728\u53cc\u8bed\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u4e0e\u98ce\u9669\u3002", "motivation": "\u5927\u591a\u6570\u4eba\u662f\u591a\u8bed\u8a00\u4f7f\u7528\u8005\u5e76\u4f1a\u8fdb\u884c\u7801\u8f6c\u6362\uff0c\u4f46\u7801\u8f6c\u6362\u8bed\u8a00\u7684\u7279\u5f81\u5c1a\u672a\u88ab\u5b8c\u5168\u7406\u89e3\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u4f7f\u7528\u804a\u5929\u673a\u5668\u4eba\u8fdb\u884c\u7801\u8f6c\u6362\u8bed\u8a00\u5b9e\u9a8c\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u53ca\u53c2\u4e0e\u8005\u662f\u5426\u80fd\u611f\u77e5\u5230\u8bdd\u8bed\u548c\u8bed\u6cd5\u6a21\u5f0f\u7684\u53d8\u5316\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u4e0e\u4eba\u7c7b\u53c2\u4e0e\u8005\u8fdb\u884c\u897f\u73ed\u7259\u8bed\u548c\u82f1\u8bed\u7801\u8f6c\u6362\u5730\u56fe\u4efb\u52a1\u7684\u804a\u5929\u673a\u5668\u4eba\u3002\u5728\u4e24\u4e2a\u5b9e\u9a8c\u4e2d\uff0c\u673a\u5668\u4eba\u88ab\u8bbe\u5b9a\u4e3a\u6309\u7167\u4e0d\u540c\u7684\u7b56\u7565\u8fdb\u884c\u7801\u8f6c\u6362\uff0c\u4ee5\u89c2\u5bdf\u5b9e\u9a8c\u7684\u53ef\u884c\u6027\u4ee5\u53ca\u53c2\u4e0e\u8005\u5bf9\u7801\u8f6c\u6362\u6a21\u5f0f\u7684\u654f\u611f\u5ea6\u3002", "result": "\u53c2\u4e0e\u8005\u666e\u904d\u559c\u6b22\u4e0e\u884c\u4e3a\u53ef\u9884\u6d4b\u7684\u673a\u5668\u4eba\u8fdb\u884c\u7801\u8f6c\u6362\u3002\u5f53\u7801\u8f6c\u6362\u968f\u673a\u6216\u4e0d\u7b26\u5408\u8bed\u6cd5\uff08\u4f8b\u5982\uff0c\u4ea7\u751f\u672a\u7ecf\u8bc1\u5b9e\u7684\u6df7\u5408\u8bed\u8a00\u540d\u8bcd\u77ed\u8bed\u5982\u201cla fork\u201d\uff09\u65f6\uff0c\u53c2\u4e0e\u8005\u7684\u4efb\u52a1\u4eab\u53d7\u5ea6\u548c\u5b8c\u6210\u5ea6\u90fd\u4f1a\u964d\u4f4e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u90e8\u7f72\u4e0d\u6210\u719f\u7684\u591a\u8bed\u8a00\u8bed\u8a00\u6280\u672f\u7684\u6f5c\u5728\u7f3a\u70b9\uff0c\u540c\u65f6\u4e5f\u8bf4\u660e\u4e86\u6b64\u7c7b\u6280\u672f\u5728\u8fdb\u884c\u53cc\u8bed\u8bed\u8a00\u4f7f\u7528\u7814\u7a76\u65b9\u9762\u7684\u5de8\u5927\u524d\u666f\u3002"}}
{"id": "2508.07029", "pdf": "https://arxiv.org/pdf/2508.07029", "abs": "https://arxiv.org/abs/2508.07029", "authors": ["Antonio Guillen-Perez"], "title": "From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Learning robust driving policies from large-scale, real-world datasets is a\ncentral challenge in autonomous driving, as online data collection is often\nunsafe and impractical. While Behavioral Cloning (BC) offers a straightforward\napproach to imitation learning, policies trained with BC are notoriously\nbrittle and suffer from compounding errors in closed-loop execution. This work\npresents a comprehensive pipeline and a comparative study to address this\nlimitation. We first develop a series of increasingly sophisticated BC\nbaselines, culminating in a Transformer-based model that operates on a\nstructured, entity-centric state representation. While this model achieves low\nimitation loss, we show that it still fails in long-horizon simulations. We\nthen demonstrate that by applying a state-of-the-art Offline Reinforcement\nLearning algorithm, Conservative Q-Learning (CQL), to the same data and\narchitecture, we can learn a significantly more robust policy. Using a\ncarefully engineered reward function, the CQL agent learns a conservative value\nfunction that enables it to recover from minor errors and avoid\nout-of-distribution states. In a large-scale evaluation on 1,000 unseen\nscenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a\n3.2x higher success rate and a 7.4x lower collision rate than the strongest BC\nbaseline, proving that an offline RL approach is critical for learning robust,\nlong-horizon driving policies from static expert data.", "AI": {"tldr": "\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u4ece\u5927\u89c4\u6a21\u79bb\u7ebf\u6570\u636e\u5b66\u4e60\u9c81\u68d2\u9a7e\u9a76\u7b56\u7565\u9762\u4e34\u6311\u6218\u3002\u7814\u7a76\u8868\u660e\uff0c\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u6548\u679c\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u5e94\u7528\u6700\u5148\u8fdb\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08CQL\uff09\uff0c\u80fd\u4ece\u76f8\u540c\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u663e\u8457\u66f4\u9c81\u68d2\u7684\u7b56\u7565\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u8bc4\u4f30\u4e2d\u5927\u5e45\u4f18\u4e8eBC\u57fa\u7ebf\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u4ece\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u5b66\u4e60\u9c81\u68d2\u9a7e\u9a76\u7b56\u7565\u662f\u6838\u5fc3\u6311\u6218\uff0c\u56e0\u4e3a\u5728\u7ebf\u6570\u636e\u6536\u96c6\u65e2\u4e0d\u5b89\u5168\u4e5f\u4e0d\u5207\u5b9e\u9645\u3002\u4f20\u7edf\u7684\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u65b9\u6cd5\u8bad\u7ec3\u51fa\u7684\u7b56\u7565\u8106\u5f31\uff0c\u5728\u5b9e\u9645\u6267\u884c\u4e2d\u5bb9\u6613\u51fa\u73b0\u9519\u8bef\u7d2f\u79ef\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u5f00\u53d1\u4e86\u4e00\u7cfb\u5217\u590d\u6742\u7684\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u57fa\u7ebf\uff0c\u5305\u62ec\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u3002\u968f\u540e\uff0c\u5c06\u6700\u5148\u8fdb\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u2014\u2014\u4fdd\u5b88Q\u5b66\u4e60\uff08CQL\uff09\u5e94\u7528\u4e8e\u76f8\u540c\u7684\u6570\u636e\u548c\u67b6\u6784\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7cbe\u7ec6\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4f7fCQL\u667a\u80fd\u4f53\u80fd\u591f\u5b66\u4e60\u4fdd\u5b88\u4ef7\u503c\u51fd\u6570\u4ee5\u6062\u590d\u9519\u8bef\u5e76\u907f\u514d\u5206\u5e03\u5916\u72b6\u6001\u3002", "result": "\u5c3d\u7ba1\u590d\u6742\u7684BC\u6a21\u578b\u5b9e\u73b0\u4e86\u4f4e\u6a21\u4eff\u635f\u5931\uff0c\u4f46\u5728\u957f\u5468\u671f\u6a21\u62df\u4e2d\u4ecd\u7136\u5931\u8d25\u3002\u901a\u8fc7\u5e94\u7528CQL\uff0c\u5728Waymo\u5f00\u653e\u8fd0\u52a8\u6570\u636e\u96c6\u76841,000\u4e2a\u672a\u89c1\u573a\u666f\u4e2d\uff0c\u6700\u7ec8\u7684CQL\u667a\u80fd\u4f53\u6bd4\u6700\u5f3a\u7684BC\u57fa\u7ebf\u6210\u529f\u7387\u63d0\u9ad8\u4e863.2\u500d\uff0c\u78b0\u649e\u7387\u964d\u4f4e\u4e867.4\u500d\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\uff0c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bf9\u4e8e\u4ece\u9759\u6001\u4e13\u5bb6\u6570\u636e\u4e2d\u5b66\u4e60\u9c81\u68d2\u3001\u957f\u5468\u671f\u7684\u9a7e\u9a76\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.07602", "pdf": "https://arxiv.org/pdf/2508.07602", "abs": "https://arxiv.org/abs/2508.07602", "authors": ["Wenpeng Xing", "Zhipeng Chen", "Changting Lin", "Meng Han"], "title": "HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol", "categories": ["cs.AI"], "comment": null, "summary": "Invoking external tools enables Large Language Models (LLMs) to perform\ncomplex, real-world tasks, yet selecting the correct tool from large,\nhierarchically-structured libraries remains a significant challenge. The\nlimited context windows of LLMs and noise from irrelevant options often lead to\nlow selection accuracy and high computational costs. To address this, we\npropose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic\npruning method for scalable tool invocation. HGMF first maps the user query and\nall tool descriptions into a unified semantic space. The framework then\noperates in two stages: it clusters servers using a Gaussian Mixture Model\n(GMM) and filters them based on the query's likelihood. Subsequently, it\napplies the same GMM-based clustering and filtering to the tools associated\nwith the selected servers. This hierarchical process produces a compact,\nhigh-relevance candidate set, simplifying the final selection task for the LLM.\nExperiments on a public dataset show that HGMF significantly improves tool\nselection accuracy while reducing inference latency, confirming the framework's\nscalability and effectiveness for large-scale tool libraries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5206\u5c42\u9ad8\u65af\u6df7\u5408\u6846\u67b6\uff08HGMF\uff09\u7684\u6982\u7387\u526a\u679d\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ece\u5927\u89c4\u6a21\u3001\u5206\u5c42\u7ed3\u6784\u7684\u5de5\u5177\u5e93\u4e2d\u8fdb\u884c\u5de5\u5177\u9009\u62e9\u65f6\u9762\u4e34\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u6311\u6218\u3002", "motivation": "\u5f53\u524dLLM\u5728\u5927\u578b\u3001\u5206\u5c42\u5de5\u5177\u5e93\u4e2d\u9009\u62e9\u5de5\u5177\u65f6\uff0c\u7531\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u9650\u548c\u65e0\u5173\u9009\u9879\u7684\u5e72\u6270\uff0c\u5bfc\u81f4\u9009\u62e9\u51c6\u786e\u7387\u4f4e\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "HGMF\u5c06\u7528\u6237\u67e5\u8be2\u548c\u5de5\u5177\u63cf\u8ff0\u6620\u5c04\u5230\u7edf\u4e00\u7684\u8bed\u4e49\u7a7a\u95f4\u3002\u5b83\u91c7\u7528\u4e24\u9636\u6bb5\u5206\u5c42\u8fc7\u7a0b\uff1a\u9996\u5148\u4f7f\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u805a\u7c7b\u670d\u52a1\u5668\u5e76\u6839\u636e\u67e5\u8be2\u4f3c\u7136\u8fdb\u884c\u8fc7\u6ee4\uff1b\u7136\u540e\u5bf9\u9009\u5b9a\u670d\u52a1\u5668\u5173\u8054\u7684\u5de5\u5177\u91cd\u590d\u6b64GMM\u805a\u7c7b\u548c\u8fc7\u6ee4\u8fc7\u7a0b\uff0c\u4ee5\u751f\u6210\u7d27\u51d1\u3001\u9ad8\u76f8\u5173\u6027\u7684\u5019\u9009\u96c6\u4f9bLLM\u6700\u7ec8\u9009\u62e9\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHGMF\u663e\u8457\u63d0\u9ad8\u4e86\u5de5\u5177\u9009\u62e9\u51c6\u786e\u7387\uff0c\u5e76\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "HGMF\u6846\u67b6\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u5904\u7406\u5927\u89c4\u6a21\u5de5\u5177\u5e93\u7684\u5de5\u5177\u9009\u62e9\u95ee\u9898\u3002"}}
{"id": "2508.06874", "pdf": "https://arxiv.org/pdf/2508.06874", "abs": "https://arxiv.org/abs/2508.06874", "authors": ["Shisheng Zhang", "Ramtin Gharleghi", "Sonit Singh", "Daniel Moses", "Dona Adikari", "Arcot Sowmya", "Susann Beier"], "title": "LWT-ARTERY-LABEL: A Lightweight Framework for Automated Coronary Artery Identification", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Coronary artery disease (CAD) remains the leading cause of death globally,\nwith computed tomography coronary angiography (CTCA) serving as a key\ndiagnostic tool. However, coronary arterial analysis using CTCA, such as\nidentifying artery-specific features from computational modelling, is\nlabour-intensive and time-consuming. Automated anatomical labelling of coronary\narteries offers a potential solution, yet the inherent anatomical variability\nof coronary trees presents a significant challenge. Traditional knowledge-based\nlabelling methods fall short in leveraging data-driven insights, while recent\ndeep-learning approaches often demand substantial computational resources and\noverlook critical clinical knowledge. To address these limitations, we propose\na lightweight method that integrates anatomical knowledge with rule-based\ntopology constraints for effective coronary artery labelling. Our approach\nachieves state-of-the-art performance on benchmark datasets, providing a\npromising alternative for automated coronary artery labelling.", "AI": {"tldr": "\u9488\u5bf9\u51a0\u72b6\u52a8\u8109CTCA\u5206\u6790\u8017\u65f6\u4e14\u73b0\u6709\u81ea\u52a8\u5316\u6807\u6ce8\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u89e3\u5256\u77e5\u8bc6\u4e0e\u89c4\u5219\u62d3\u6251\u7ea6\u675f\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u51a0\u72b6\u52a8\u8109\u7684\u81ea\u52a8\u5316\u6807\u6ce8\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u51a0\u72b6\u52a8\u8109\u75be\u75c5\u8bca\u65ad\u4e2dCTCA\u5206\u6790\u624b\u52a8\u64cd\u4f5c\u8017\u65f6\u8d39\u529b\u3002\u73b0\u6709\u81ea\u52a8\u5316\u6807\u6ce8\u65b9\u6cd5\uff08\u4f20\u7edf\u77e5\u8bc6\u578b\u548c\u6df1\u5ea6\u5b66\u4e60\u578b\uff09\u5404\u6709\u5c40\u9650\uff0c\u524d\u8005\u672a\u80fd\u5145\u5206\u5229\u7528\u6570\u636e\u9a71\u52a8\u6d1e\u5bdf\uff0c\u540e\u8005\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u4e14\u5e38\u5ffd\u7565\u4e34\u5e8a\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u89e3\u5256\u5b66\u77e5\u8bc6\u4e0e\u57fa\u4e8e\u89c4\u5219\u7684\u62d3\u6251\u7ea6\u675f\u76f8\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u51a0\u72b6\u52a8\u8109\u6807\u6ce8\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u51a0\u72b6\u52a8\u8109\u7684\u81ea\u52a8\u5316\u6807\u6ce8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.07375", "pdf": "https://arxiv.org/pdf/2508.07375", "abs": "https://arxiv.org/abs/2508.07375", "authors": ["Wenqian Cui", "Lei Zhu", "Xiaohui Li", "Zhihan Guo", "Haoli Bai", "Lu Hou", "Irwin King"], "title": "Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Work in progress", "summary": "Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation\nmodels designed to enable natural, real-time spoken interactions by modeling\ncomplex conversational dynamics such as interruptions, backchannels, and\noverlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world\ndouble-channel conversational data to capture nuanced two-speaker dialogue\npatterns for human-like interactions. However, they face a critical challenge\n-- their conversational abilities often degrade compared to pure-text\nconversation due to prolonged speech sequences and limited high-quality spoken\ndialogue data. While text-guided speech generation could mitigate these issues,\nit suffers from timing and length issues when integrating textual guidance into\ndouble-channel audio streams, disrupting the precise time alignment essential\nfor natural interactions. To address these challenges, we propose TurnGuide, a\nnovel planning-inspired approach that mimics human conversational planning by\ndynamically segmenting assistant speech into dialogue turns and generating\nturn-level text guidance before speech output, which effectively resolves both\ninsertion timing and length challenges. Extensive experiments demonstrate our\napproach significantly improves e2e FD-SLMs' conversational abilities, enabling\nthem to generate semantically meaningful and coherent speech while maintaining\nnatural conversational flow. Demos are available at\nhttps://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at\nhttps://github.com/dreamtheater123/TurnGuide.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTurnGuide\u65b9\u6cd5\uff0c\u901a\u8fc7\u56de\u5408\u7ea7\u6587\u672c\u6307\u5bfc\u89e3\u51b3\u5168\u53cc\u5de5\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08FD-SLMs\uff09\u5728\u5b9e\u65f6\u5bf9\u8bdd\u4e2d\u9762\u4e34\u7684\u5bf9\u8bdd\u80fd\u529b\u4e0b\u964d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u4f1a\u8bdd\u8868\u73b0\u3002", "motivation": "\u5168\u53cc\u5de5\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08FD-SLMs\uff09\u65e8\u5728\u5b9e\u73b0\u81ea\u7136\u3001\u5b9e\u65f6\u7684\u8bed\u97f3\u4ea4\u4e92\uff0c\u4f46\u5728\u5904\u7406\u957f\u8bed\u97f3\u5e8f\u5217\u548c\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u53e3\u8bed\u6570\u636e\u65f6\uff0c\u5176\u5bf9\u8bdd\u80fd\u529b\u5e38\u4f1a\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684\u6587\u672c\u5f15\u5bfc\u8bed\u97f3\u751f\u6210\u65b9\u6848\u5728\u6574\u5408\u5230\u53cc\u58f0\u9053\u97f3\u9891\u6d41\u65f6\uff0c\u5b58\u5728\u65f6\u5e8f\u548c\u957f\u5ea6\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u7cbe\u786e\u7684\u65f6\u95f4\u5bf9\u9f50\u548c\u81ea\u7136\u4ea4\u4e92\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTurnGuide\u7684\u65b0\u578b\u89c4\u5212\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u6a21\u4eff\u4eba\u7c7b\u7684\u4f1a\u8bdd\u89c4\u5212\u8fc7\u7a0b\uff0c\u52a8\u6001\u5730\u5c06\u52a9\u624b\u8bed\u97f3\u5206\u5272\u6210\u5bf9\u8bdd\u56de\u5408\uff0c\u5e76\u5728\u8bed\u97f3\u8f93\u51fa\u4e4b\u524d\u751f\u6210\u56de\u5408\u7ea7\u7684\u6587\u672c\u6307\u5bfc\uff0c\u4ece\u800c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u6587\u672c\u5f15\u5bfc\u4e2d\u5b58\u5728\u7684\u63d2\u5165\u65f6\u5e8f\u548c\u957f\u5ea6\u6311\u6218\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTurnGuide\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aefFD-SLMs\u7684\u5bf9\u8bdd\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u751f\u6210\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u4e14\u8fde\u8d2f\u7684\u8bed\u97f3\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u7136\u7684\u4f1a\u8bdd\u6d41\u7545\u6027\u3002", "conclusion": "TurnGuide\u901a\u8fc7\u5176\u521b\u65b0\u7684\u4f1a\u8bdd\u89c4\u5212\u548c\u56de\u5408\u7ea7\u6587\u672c\u6307\u5bfc\uff0c\u6709\u6548\u89e3\u51b3\u4e86FD-SLMs\u5728\u5b9e\u65f6\u8bed\u97f3\u4ea4\u4e92\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5b9e\u73b0\u66f4\u81ea\u7136\u3001\u9ad8\u6548\u548c\u7c7b\u4eba\u7684\u4eba\u673a\u8bed\u97f3\u5bf9\u8bdd\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.07032", "pdf": "https://arxiv.org/pdf/2508.07032", "abs": "https://arxiv.org/abs/2508.07032", "authors": ["Tiantian He", "Keyue Jiang", "An Zhao", "Anna Schroder", "Elinor Thompson", "Sonja Soskic", "Frederik Barkhof", "Daniel C. Alexander"], "title": "A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "The long-term progression of neurodegenerative diseases is commonly\nconceptualized as a spatiotemporal diffusion process that consists of a graph\ndiffusion process across the structural brain connectome and a localized\nreaction process within brain regions. However, modeling this progression\nremains challenging due to 1) the scarcity of longitudinal data obtained\nthrough irregular and infrequent subject visits and 2) the complex interplay of\npathological mechanisms across brain regions and disease stages, where\ntraditional models assume fixed mechanisms throughout disease progression. To\naddress these limitations, we propose a novel stage-aware Mixture of Experts\n(MoE) framework that explicitly models how different contributing mechanisms\ndominate at different disease stages through time-dependent expert\nweighting.Data-wise, we utilize an iterative dual optimization method to\nproperly estimate the temporal position of individual observations,\nconstructing a co hort-level progression trajectory from irregular snapshots.\nModel-wise, we enhance the spatial component with an inhomogeneous graph neural\ndiffusion model (IGND) that allows diffusivity to vary based on node states and\ntime, providing more flexible representations of brain networks. We also\nintroduce a localized neural reaction module to capture complex dynamics beyond\nstandard processes.The resulting IGND-MoE model dynamically integrates these\ncomponents across temporal states, offering a principled way to understand how\nstage-specific pathological mechanisms contribute to progression. The\nstage-wise weights yield novel clinical insights that align with literature,\nsuggesting that graph-related processes are more influential at early stages,\nwhile other unknown physical processes become dominant later on.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aIGND-MoE\u7684\u9636\u6bb5\u611f\u77e5\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u7528\u4e8e\u514b\u670d\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u4e2d\u6570\u636e\u7a00\u758f\u6027\u548c\u673a\u5236\u590d\u6742\u6027\u7684\u6311\u6218\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u9636\u6bb5\u7684\u4e3b\u5bfc\u75c5\u7406\u673a\u5236\u3002", "motivation": "\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u957f\u671f\u8fdb\u5c55\u6a21\u578b\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1) \u7eb5\u5411\u6570\u636e\u7a00\u758f\u4e14\u83b7\u53d6\u4e0d\u89c4\u5f8b\uff1b2) \u4f20\u7edf\u6a21\u578b\u5047\u5b9a\u75c5\u7406\u673a\u5236\u5728\u75be\u75c5\u8fdb\u5c55\u4e2d\u56fa\u5b9a\u4e0d\u53d8\uff0c\u65e0\u6cd5\u6355\u6349\u8de8\u8111\u533a\u548c\u75be\u75c5\u9636\u6bb5\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a novel \u9636\u6bb5\u611f\u77e5\u6df7\u5408\u4e13\u5bb6 (MoE) \u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u4f9d\u8d56\u7684\u4e13\u5bb6\u6743\u91cd\u5efa\u6a21\u4e0d\u540c\u75c5\u7406\u673a\u5236\u5728\u4e0d\u540c\u75be\u75c5\u9636\u6bb5\u7684\u4e3b\u5bfc\u4f5c\u7528\u3002\u6570\u636e\u5c42\u9762\uff0c\u4f7f\u7528\u8fed\u4ee3\u53cc\u91cd\u4f18\u5316\u65b9\u6cd5\u4ece\u4e0d\u89c4\u5219\u5feb\u7167\u4f30\u8ba1\u89c2\u6d4b\u65f6\u95f4\uff0c\u6784\u5efa\u961f\u5217\u7ea7\u8fdb\u5c55\u8f68\u8ff9\u3002\u6a21\u578b\u5c42\u9762\uff0c\u5f15\u5165\u975e\u5747\u5300\u56fe\u795e\u7ecf\u7f51\u7edc\u6269\u6563\u6a21\u578b (IGND) \u589e\u5f3a\u7a7a\u95f4\u7ec4\u4ef6\uff0c\u5141\u8bb8\u6269\u6563\u7387\u968f\u8282\u70b9\u72b6\u6001\u548c\u65f6\u95f4\u53d8\u5316\uff1b\u5e76\u5f15\u5165\u5c40\u90e8\u795e\u7ecf\u53cd\u5e94\u6a21\u5757\u6355\u6349\u6807\u51c6\u8fc7\u7a0b\u4e4b\u5916\u7684\u590d\u6742\u52a8\u529b\u5b66\u3002\u6700\u7ec8\u5f62\u6210 IGND-MoE \u6a21\u578b\u3002", "result": "IGND-MoE \u6a21\u578b\u80fd\u52a8\u6001\u6574\u5408\u7ec4\u4ef6\uff0c\u7406\u89e3\u9636\u6bb5\u7279\u5f02\u6027\u75c5\u7406\u673a\u5236\u5982\u4f55\u5bfc\u81f4\u8fdb\u5c55\u3002\u9636\u6bb5\u6027\u6743\u91cd\u63d0\u4f9b\u4e86\u65b0\u7684\u4e34\u5e8a\u6d1e\u5bdf\uff0c\u4e0e\u73b0\u6709\u6587\u732e\u4e00\u81f4\uff0c\u8868\u660e\u56fe\u76f8\u5173\u8fc7\u7a0b\u5728\u65e9\u671f\u9636\u6bb5\u5f71\u54cd\u66f4\u5927\uff0c\u800c\u5176\u4ed6\u672a\u77e5\u7269\u7406\u8fc7\u7a0b\u5728\u540e\u671f\u9636\u6bb5\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002", "conclusion": "IGND-MoE \u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u65b9\u6cd5\u6765\u7406\u89e3\u9636\u6bb5\u7279\u5f02\u6027\u75c5\u7406\u673a\u5236\u5bf9\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u8fdb\u5c55\u7684\u8d21\u732e\uff0c\u5e76\u63ed\u793a\u4e86\u4e0e\u73b0\u6709\u6587\u732e\u76f8\u7b26\u7684\u4e34\u5e8a\u6d1e\u5bdf\u3002"}}
{"id": "2508.07616", "pdf": "https://arxiv.org/pdf/2508.07616", "abs": "https://arxiv.org/abs/2508.07616", "authors": ["Aswin RRV", "Jacob Dineen", "Divij Handa", "Md Nayem Uddin", "Mihir Parmar", "Chitta Baral", "Ben Zhou"], "title": "ThinkTuning: Instilling Cognitive Reflections without Distillation", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "15 pages", "summary": "Recent advances in test-time scaling have led to the emergence of thinking\nLLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL\ndrives this self-improvement paradigm, a recent study (Gandhi et al., 2025)\nshows that RL alone does not truly instill these new reasoning abilities - it\nmerely draws out behaviors already present in the base models. This raises a\nquestion: How can we train the models that don't exhibit such thinking behavior\nto develop it in the first place? To this end, we propose ThinkTuning, a\nGRPO-based interactive training approach where we augment the rollouts of a\nstudent model with the guidance from a teacher model. A simple idea from\nclassroom practice inspires our method: a teacher poses a problem, lets the\nstudent try an answer, then gives corrective feedback -- enough to point the\nmind in the right direction and then show the solution. Each piece of feedback\nreshapes the student's thoughts, leading them to arrive at the correct\nsolution. Similarly, we find that this type of implicit supervision through\nfeedback from a teacher model of the same size improves the reasoning\ncapabilities of the student model. In particular, on average, our method shows\na 3.85% improvement over zero-shot baselines across benchmarks, and on\nMATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements\nover the vanilla-GRPO baseline. Source code is available at\nhttps://github.com/3rdAT/ThinkTuning.", "AI": {"tldr": "\u5f53\u524dRL\u65b9\u6cd5\u65e0\u6cd5\u4ece\u65e0\u5230\u6709\u5730\u57f9\u517bLLM\u7684\u63a8\u7406\u80fd\u529b\u3002\u672c\u6587\u63d0\u51fa\u4e86ThinkTuning\uff0c\u4e00\u79cd\u57fa\u4e8eGRPO\u7684\u4e92\u52a8\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u7684\u53cd\u9988\u6307\u5bfc\u5b66\u751f\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u867d\u80fd\u9a71\u52a8LLM\u7684\u81ea\u6211\u63d0\u5347\uff0c\u4f46\u5b83\u5e76\u4e0d\u80fd\u771f\u6b63\u704c\u8f93\u65b0\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u53ea\u662f\u6fc0\u53d1\u4e86\u57fa\u7840\u6a21\u578b\u4e2d\u5df2\u6709\u7684\u884c\u4e3a\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5982\u4f55\u8bad\u7ec3\u90a3\u4e9b\u4e0d\u5177\u5907\u601d\u8003\u884c\u4e3a\u7684\u6a21\u578b\uff0c\u4f7f\u5176\u4ece\u4e00\u5f00\u59cb\u5c31\u53d1\u5c55\u51fa\u8fd9\u79cd\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86ThinkTuning\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7684\u4e92\u52a8\u5f0f\u8bad\u7ec3\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e00\u4e2a\u6559\u5e08\u6a21\u578b\u5411\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff08rollouts\uff09\u63d0\u4f9b\u6307\u5bfc\u548c\u53cd\u9988\u6765\u589e\u5f3a\u5176\u8bad\u7ec3\u3002\u5176\u7075\u611f\u6765\u6e90\u4e8e\u8bfe\u5802\u6559\u5b66\u5b9e\u8df5\uff1a\u6559\u5e08\u63d0\u51fa\u95ee\u9898\uff0c\u8ba9\u5b66\u751f\u5c1d\u8bd5\u56de\u7b54\uff0c\u7136\u540e\u63d0\u4f9b\u7ea0\u6b63\u6027\u53cd\u9988\uff0c\u4ee5\u5f15\u5bfc\u5b66\u751f\u627e\u5230\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u540c\u7b49\u5927\u5c0f\u7684\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u9690\u5f0f\u76d1\u7763\uff08\u53cd\u9988\uff09\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u5e73\u5747\u63d0\u5347\u4e863.85%\uff1b\u5728MATH-500\u3001AIME\u548cGPQA-Diamond\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u522b\u6bd4\u9999\u8349GRPO\u57fa\u7ebf\u63d0\u5347\u4e862.08%\u30012.23%\u548c3.99%\u3002", "conclusion": "\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u4e92\u52a8\u5f0f\u3001\u9690\u5f0f\u7684\u53cd\u9988\u76d1\u7763\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u51fa\u65b0\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5373\u4f7f\u8fd9\u4e9b\u80fd\u529b\u5728\u57fa\u7840\u6a21\u578b\u4e2d\u5e76\u4e0d\u5b58\u5728\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.06878", "pdf": "https://arxiv.org/pdf/2508.06878", "abs": "https://arxiv.org/abs/2508.06878", "authors": ["Maoxun Yuan", "Duanni Meng", "Ziteng Xi", "Tianyi Zhao", "Shiji Zhao", "Yimian Dai", "Xingxing Wei"], "title": "NS-FPN: Improving Infrared Small Target Detection and Segmentation from Noise Suppression Perspective", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Infrared small target detection and segmentation (IRSTDS) is a critical yet\nchallenging task in defense and civilian applications, owing to the dim,\nshapeless appearance of targets and severe background clutter. Recent CNN-based\nmethods have achieved promising target perception results, but they only focus\non enhancing feature representation to offset the impact of noise, which\nresults in the increased false alarms problem. In this paper, through analyzing\nthe problem from the frequency domain, we pioneer in improving performance from\nnoise suppression perspective and propose a novel noise-suppression feature\npyramid network (NS-FPN), which integrates a low-frequency guided feature\npurification (LFP) module and a spiral-aware feature sampling (SFS) module into\nthe original FPN structure. The LFP module suppresses the noise features by\npurifying high-frequency components to achieve feature enhancement devoid of\nnoise interference, while the SFS module further adopts spiral sampling to fuse\ntarget-relevant features in feature fusion process. Our NS-FPN is designed to\nbe lightweight yet effective and can be easily plugged into existing IRSTDS\nframeworks. Extensive experiments on the public IRSTDS datasets demonstrate\nthat our method significantly reduces false alarms and achieves superior\nperformance on IRSTDS tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u566a\u58f0\u6291\u5236\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff08NS-FPN\uff09\uff0c\u901a\u8fc7\u9891\u57df\u5206\u6790\u548c\u566a\u58f0\u6291\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u8bef\u62a5\u7387\u9ad8\u7684\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u8bef\u62a5\u5e76\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e0e\u5206\u5272\uff08IRSTDS\uff09\u4efb\u52a1\u9762\u4e34\u76ee\u6807\u6a21\u7cca\u3001\u80cc\u666f\u6742\u4e71\u7684\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8eCNN\u7684\u65b9\u6cd5\u4fa7\u91cd\u7279\u5f81\u589e\u5f3a\uff0c\u4f46\u672a\u6709\u6548\u6291\u5236\u566a\u58f0\uff0c\u5bfc\u81f4\u8bef\u62a5\u7387\u8f83\u9ad8\u3002", "method": "\u672c\u6587\u4ece\u9891\u57df\u5206\u6790\u5165\u624b\uff0c\u63d0\u51faNS-FPN\uff0c\u5c06\u4f4e\u9891\u5f15\u5bfc\u7279\u5f81\u51c0\u5316\uff08LFP\uff09\u6a21\u5757\u548c\u87ba\u65cb\u611f\u77e5\u7279\u5f81\u91c7\u6837\uff08SFS\uff09\u6a21\u5757\u96c6\u6210\u5230FPN\u7ed3\u6784\u4e2d\u3002LFP\u6a21\u5757\u901a\u8fc7\u51c0\u5316\u9ad8\u9891\u5206\u91cf\u6291\u5236\u566a\u58f0\uff0cSFS\u6a21\u5757\u5219\u5229\u7528\u87ba\u65cb\u91c7\u6837\u878d\u5408\u76ee\u6807\u76f8\u5173\u7279\u5f81\u3002\u8be5\u7f51\u7edc\u8f7b\u91cf\u4e14\u53ef\u63d2\u62d4\u3002", "result": "\u5728\u516c\u5171IRSTDS\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u62a5\u7387\u3002", "conclusion": "NS-FPN\u5728IRSTDS\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u901a\u8fc7\u6709\u6548\u6291\u5236\u566a\u58f0\uff0c\u6210\u529f\u51cf\u5c11\u4e86\u8bef\u62a5\uff0c\u63d0\u5347\u4e86\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e0e\u5206\u5272\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.07414", "pdf": "https://arxiv.org/pdf/2508.07414", "abs": "https://arxiv.org/abs/2508.07414", "authors": ["Jean de Dieu Nyandwi", "Yueqi Song", "Simran Khanuja", "Graham Neubig"], "title": "Grounding Multilingual Multimodal LLMs With Cultural Knowledge", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models excel in high-resource settings, but often\nmisinterpret long-tail cultural entities and underperform in low-resource\nlanguages. To address this gap, we propose a data-centric approach that\ndirectly grounds MLLMs in cultural knowledge. Leveraging a large scale\nknowledge graph from Wikidata, we collect images that represent culturally\nsignificant entities, and generate synthetic multilingual visual question\nanswering data. The resulting dataset, CulturalGround, comprises 22 million\nhigh-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages.\nWe train an open-source MLLM CulturalPangea on CulturalGround, interleaving\nstandard multilingual instruction-tuning data to preserve general abilities.\nCulturalPangea achieves state-of-the-art performance among open models on\nvarious culture-focused multilingual multimodal benchmarks, outperforming prior\nmodels by an average of 5.0 without degrading results on mainstream\nvision-language tasks. Our findings show that our targeted, culturally grounded\napproach could substantially narrow the cultural gap in MLLMs and offer a\npractical path towards globally inclusive multimodal systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6587\u5316\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\uff08CulturalGround\uff09\u5e76\u8bad\u7ec3\u5f00\u6e90\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08CulturalPangea\uff09\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u6587\u5316\u5b9e\u4f53\u7406\u89e3\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6587\u5316\u654f\u611f\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u80fd\u529b\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u9ad8\u8d44\u6e90\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u5728\u7406\u89e3\u957f\u5c3e\u6587\u5316\u5b9e\u4f53\u548c\u5904\u7406\u4f4e\u8d44\u6e90\u8bed\u8a00\u65f6\u5e38\u51fa\u73b0\u8bef\u89e3\u6216\u6027\u80fd\u4e0d\u4f73\uff0c\u5b58\u5728\u6587\u5316\u9e3f\u6c9f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u5c06MLLMs\u76f4\u63a5\u4e0e\u6587\u5316\u77e5\u8bc6\u7ed3\u5408\u3002\u5229\u7528\u7ef4\u57fa\u6570\u636e\uff08Wikidata\uff09\u6784\u5efa\u4e86\u5305\u542b2200\u4e07\u4e2a\u9ad8\u8d28\u91cf\u3001\u6587\u5316\u4e30\u5bccVQA\u5bf9\u7684\u591a\u8bed\u8a00\u5408\u6210\u6570\u636e\u96c6CulturalGround\uff0c\u6db5\u76d642\u4e2a\u56fd\u5bb6\u548c39\u79cd\u8bed\u8a00\u3002\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86\u5f00\u6e90MLLM CulturalPangea\uff0c\u5e76\u7ed3\u5408\u6807\u51c6\u591a\u8bed\u8a00\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u4ee5\u4fdd\u6301\u901a\u7528\u80fd\u529b\u3002", "result": "CulturalPangea\u5728\u591a\u4e2a\u4ee5\u6587\u5316\u4e3a\u4e2d\u5fc3\u7684\u591a\u8bed\u8a00\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e73\u5747\u8d85\u8d8a\u73b0\u6709\u6a21\u578b5.0\uff0c\u4e14\u672a\u964d\u4f4e\u5728\u4e3b\u6d41\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u6709\u9488\u5bf9\u6027\u7684\u3001\u57fa\u4e8e\u6587\u5316\u77e5\u8bc6\u7684\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u7f29\u5c0fMLLMs\u4e2d\u7684\u6587\u5316\u9e3f\u6c9f\uff0c\u4e3a\u6784\u5efa\u5168\u7403\u5305\u5bb9\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2508.07037", "pdf": "https://arxiv.org/pdf/2508.07037", "abs": "https://arxiv.org/abs/2508.07037", "authors": ["Yangguang He", "Wenhao Li", "Minzhe Li", "Juan Zhang", "Xiangfeng Wang", "Bo Jin"], "title": "Differentiable Adaptive Kalman Filtering via Optimal Transport", "categories": ["cs.LG", "eess.SP"], "comment": "20 pages", "summary": "Learning-based filtering has demonstrated strong performance in non-linear\ndynamical systems, particularly when the statistics of noise are unknown.\nHowever, in real-world deployments, environmental factors, such as changing\nwind conditions or electromagnetic interference, can induce unobserved\nnoise-statistics drift, leading to substantial degradation of learning-based\nmethods. To address this challenge, we propose OTAKNet, the first online\nsolution to noise-statistics drift within learning-based adaptive Kalman\nfiltering. Unlike existing learning-based methods that perform offline\nfine-tuning using batch pointwise matching over entire trajectories, OTAKNet\nestablishes a connection between the state estimate and the drift via one-step\npredictive measurement likelihood, and addresses it using optimal transport.\nThis leverages OT's geometry - aware cost and stable gradients to enable fully\nonline adaptation without ground truth labels or retraining. We compare OTAKNet\nagainst classical model-based adaptive Kalman filtering and offline\nlearning-based filtering. The performance is demonstrated on both synthetic and\nreal-world NCLT datasets, particularly under limited training data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OTAKNet\uff0c\u4e00\u79cd\u521b\u65b0\u7684\u5728\u7ebf\u5b66\u4e60\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u6700\u4f18\u4f20\u8f93\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u566a\u58f0\u7edf\u8ba1\u6f02\u79fb\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u6216\u91cd\u65b0\u8bad\u7ec3\u7684\u5728\u7ebf\u81ea\u9002\u5e94\uff0c\u5e76\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u8868\u73b0\u5353\u8d8a\u3002", "motivation": "\u57fa\u4e8e\u5b66\u4e60\u7684\u6ee4\u6ce2\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0c\u73af\u5883\u56e0\u7d20\uff08\u5982\u98ce\u901f\u53d8\u5316\u6216\u7535\u78c1\u5e72\u6270\uff09\u4f1a\u5bfc\u81f4\u672a\u89c2\u6d4b\u7684\u566a\u58f0\u7edf\u8ba1\u6f02\u79fb\uff0c\u4e25\u91cd\u964d\u4f4e\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86OTAKNet\uff0c\u8fd9\u662f\u9996\u4e2a\u89e3\u51b3\u5b66\u4e60\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u4e2d\u566a\u58f0\u7edf\u8ba1\u6f02\u79fb\u7684\u5728\u7ebf\u65b9\u6848\u3002\u4e0e\u73b0\u6709\u79bb\u7ebf\u5fae\u8c03\u65b9\u6cd5\u4e0d\u540c\uff0cOTAKNet\u901a\u8fc7\u4e00\u6b65\u9884\u6d4b\u6d4b\u91cf\u4f3c\u7136\u5c06\u72b6\u6001\u4f30\u8ba1\u4e0e\u6f02\u79fb\u5173\u8054\u8d77\u6765\uff0c\u5e76\u5229\u7528\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u8fdb\u884c\u5904\u7406\u3002\u8fd9\u5229\u7528\u4e86OT\u7684\u51e0\u4f55\u611f\u77e5\u6210\u672c\u548c\u7a33\u5b9a\u68af\u5ea6\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u6216\u91cd\u65b0\u8bad\u7ec3\u7684\u5b8c\u5168\u5728\u7ebf\u81ea\u9002\u5e94\u3002", "result": "OTAKNet\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754cNCLT\u6570\u636e\u96c6\u4e0a\u90fd\u5c55\u793a\u4e86\u51fa\u8272\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002\u5b83\u4f18\u4e8e\u7ecf\u5178\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u79bb\u7ebf\u5b66\u4e60\u6ee4\u6ce2\u65b9\u6cd5\u3002", "conclusion": "OTAKNet\u6210\u529f\u89e3\u51b3\u4e86\u5b66\u4e60\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u4e2d\u566a\u58f0\u7edf\u8ba1\u6f02\u79fb\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u3001\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u5728\u7ebf\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u53d7\u9650\u7684\u73af\u5883\u4e0b\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2508.07628", "pdf": "https://arxiv.org/pdf/2508.07628", "abs": "https://arxiv.org/abs/2508.07628", "authors": ["Daniel Essien", "Suresh Neethirajan"], "title": "Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization", "categories": ["cs.AI"], "comment": "66 pages, 7 figures, 11 tables", "summary": "The future of poultry production depends on a paradigm shift replacing\nsubjective, labor-intensive welfare checks with data-driven, intelligent\nmonitoring ecosystems. Traditional welfare assessments-limited by human\nobservation and single-sensor data-cannot fully capture the complex,\nmultidimensional nature of laying hen welfare in modern farms. Multimodal\nArtificial Intelligence (AI) offers a breakthrough, integrating visual,\nacoustic, environmental, and physiological data streams to reveal deeper\ninsights into avian welfare dynamics. This investigation highlights multimodal\nAs transformative potential, showing that intermediate (feature-level) fusion\nstrategies achieve the best balance between robustness and performance under\nreal-world poultry conditions, and offer greater scalability than early or late\nfusion approaches. Key adoption barriers include sensor fragility in harsh farm\nenvironments, high deployment costs, inconsistent behavioral definitions, and\nlimited cross-farm generalizability. To address these, we introduce two novel\nevaluation tools - the Domain Transfer Score (DTS) to measure model\nadaptability across diverse farm settings, and the Data Reliability Index (DRI)\nto assess sensor data quality under operational constraints. We also propose a\nmodular, context-aware deployment framework designed for laying hen\nenvironments, enabling scalable and practical integration of multimodal\nsensing. This work lays the foundation for a transition from reactive, unimodal\nmonitoring to proactive, precision-driven welfare systems that unite\nproductivity with ethical, science based animal care.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u4f20\u7edf\u5bb6\u79bd\u798f\u5229\u8bc4\u4f30\u5347\u7ea7\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u667a\u80fd\u76d1\u6d4b\u7cfb\u7edf\u3002\u7814\u7a76\u5e94\u7528\u591a\u6a21\u6001AI\uff0c\u53d1\u73b0\u7279\u5f81\u7ea7\u878d\u5408\u7b56\u7565\u6700\u4f18\uff0c\u5e76\u5f15\u5165DTS\u3001DRI\u8bc4\u4f30\u5de5\u5177\u53ca\u6a21\u5757\u5316\u90e8\u7f72\u6846\u67b6\uff0c\u65e8\u5728\u5b9e\u73b0\u7ed3\u5408\u751f\u4ea7\u529b\u4e0e\u4f26\u7406\u7684\u7cbe\u51c6\u4e3b\u52a8\u5f0f\u52a8\u7269\u798f\u5229\u7ba1\u7406\u3002", "motivation": "\u73b0\u6709\u5bb6\u79bd\u798f\u5229\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u89c2\u5bdf\u548c\u5355\u4e00\u4f20\u611f\u5668\u6570\u636e\uff0c\u5b58\u5728\u4e3b\u89c2\u6027\u3001\u52b3\u52a8\u5bc6\u96c6\u548c\u65e0\u6cd5\u5168\u9762\u6355\u6349\u73b0\u4ee3\u519c\u573a\u86cb\u9e21\u798f\u5229\u590d\u6742\u6027\u7684\u5c40\u9650\u6027\u3002\u672a\u6765\u5bb6\u79bd\u751f\u4ea7\u4e9f\u9700\u5411\u6570\u636e\u9a71\u52a8\u7684\u667a\u80fd\u76d1\u6d4b\u751f\u6001\u7cfb\u7edf\u8f6c\u578b\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6574\u5408\u89c6\u89c9\u3001\u542c\u89c9\u3001\u73af\u5883\u548c\u751f\u7406\u6570\u636e\u6d41\u8fdb\u884c\u798f\u5229\u76d1\u6d4b\u3002\u7814\u7a76\u5bf9\u6bd4\u4e0d\u540c\u6570\u636e\u878d\u5408\u7b56\u7565\uff0c\u53d1\u73b0\u4e2d\u95f4\uff08\u7279\u5f81\u7ea7\uff09\u878d\u5408\u7b56\u7565\u5728\u771f\u5b9e\u5bb6\u79bd\u73af\u5883\u4e0b\u8868\u73b0\u6700\u4f73\u3002\u4e3a\u89e3\u51b3\u4f20\u611f\u5668\u8106\u5f31\u6027\u3001\u90e8\u7f72\u6210\u672c\u3001\u884c\u4e3a\u5b9a\u4e49\u4e0d\u4e00\u81f4\u53ca\u8de8\u519c\u573a\u6cdb\u5316\u6027\u5dee\u7b49\u969c\u788d\uff0c\u5f15\u5165\u4e86\u4e24\u79cd\u65b0\u578b\u8bc4\u4f30\u5de5\u5177\uff1a\u9886\u57df\u8fc1\u79fb\u5206\u6570\uff08DTS\uff09\u8861\u91cf\u6a21\u578b\u8de8\u519c\u573a\u9002\u5e94\u6027\uff0c\u6570\u636e\u53ef\u9760\u6027\u6307\u6570\uff08DRI\uff09\u8bc4\u4f30\u4f20\u611f\u5668\u6570\u636e\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u60c5\u5883\u611f\u77e5\u7684\u90e8\u7f72\u6846\u67b6\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4e2d\u95f4\uff08\u7279\u5f81\u7ea7\uff09\u6570\u636e\u878d\u5408\u7b56\u7565\u5728\u5b9e\u9645\u5bb6\u79bd\u517b\u6b96\u6761\u4ef6\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u6027\u4e0e\u6027\u80fd\u7684\u6700\u4f73\u5e73\u8861\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u4e8e\u65e9\u671f\u6216\u540e\u671f\u878d\u5408\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002\u6210\u529f\u5f15\u5165\u4e86DTS\u548cDRI\u4e24\u79cd\u65b0\u578b\u8bc4\u4f30\u5de5\u5177\u4ee5\u89e3\u51b3\u6a21\u578b\u6cdb\u5316\u6027\u548c\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u96be\u9898\u3002\u63d0\u51fa\u4e86\u6a21\u5757\u5316\u3001\u60c5\u5883\u611f\u77e5\u7684\u90e8\u7f72\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u611f\u77e5\u7684\u53ef\u4f38\u7f29\u548c\u5b9e\u7528\u96c6\u6210\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u5bb6\u79bd\u798f\u5229\u76d1\u6d4b\u4ece\u88ab\u52a8\u3001\u5355\u6a21\u6001\u65b9\u6cd5\u5411\u7ed3\u5408\u751f\u4ea7\u529b\u4e0e\u4f26\u7406\u79d1\u5b66\u52a8\u7269\u62a4\u7406\u7684\u4e3b\u52a8\u3001\u7cbe\u51c6\u9a71\u52a8\u798f\u5229\u7cfb\u7edf\u8f6c\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06891", "pdf": "https://arxiv.org/pdf/2508.06891", "abs": "https://arxiv.org/abs/2508.06891", "authors": ["Melika Filvantorkaman", "Mohsen Piri", "Maral Filvan Torkaman", "Ashkan Zabihi", "Hamidreza Moradi"], "title": "Fusion-Based Brain Tumor Classification Using Deep Learning and Explainable AI, and Rule-Based Reasoning", "categories": ["eess.IV", "cs.CV"], "comment": "37 pages, 6 figures", "summary": "Accurate and interpretable classification of brain tumors from magnetic\nresonance imaging (MRI) is critical for effective diagnosis and treatment\nplanning. This study presents an ensemble-based deep learning framework that\ncombines MobileNetV2 and DenseNet121 convolutional neural networks (CNNs) using\na soft voting strategy to classify three common brain tumor types: glioma,\nmeningioma, and pituitary adenoma. The models were trained and evaluated on the\nFigshare dataset using a stratified 5-fold cross-validation protocol. To\nenhance transparency and clinical trust, the framework integrates an\nExplainable AI (XAI) module employing Grad-CAM++ for class-specific saliency\nvisualization, alongside a symbolic Clinical Decision Rule Overlay (CDRO) that\nmaps predictions to established radiological heuristics. The ensemble\nclassifier achieved superior performance compared to individual CNNs, with an\naccuracy of 91.7%, precision of 91.9%, recall of 91.7%, and F1-score of 91.6%.\nGrad-CAM++ visualizations revealed strong spatial alignment between model\nattention and expert-annotated tumor regions, supported by Dice coefficients up\nto 0.88 and IoU scores up to 0.78. Clinical rule activation further validated\nmodel predictions in cases with distinct morphological features. A\nhuman-centered interpretability assessment involving five board-certified\nradiologists yielded high Likert-scale scores for both explanation usefulness\n(mean = 4.4) and heatmap-region correspondence (mean = 4.0), reinforcing the\nframework's clinical relevance. Overall, the proposed approach offers a robust,\ninterpretable, and generalizable solution for automated brain tumor\nclassification, advancing the integration of deep learning into clinical\nneurodiagnostics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u548c\u4e34\u5e8a\u51b3\u7b56\u89c4\u5219\uff0c\u5b9e\u73b0\u4e86\u8111\u80bf\u7624\u7684\u9ad8\u7cbe\u5ea6\u3001\u53ef\u89e3\u91ca\u5206\u7c7b\u3002", "motivation": "\u8111\u80bf\u7624\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u7684\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u5206\u7c7b\u5bf9\u6709\u6548\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7ed3\u5408MobileNetV2\u548cDenseNet121\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u91c7\u7528\u8f6f\u6295\u7968\u7b56\u7565\u5206\u7c7b\u80f6\u8d28\u7624\u3001\u8111\u819c\u7624\u548c\u5782\u4f53\u817a\u7624\u3002\u6846\u67b6\u96c6\u6210\u4e86Grad-CAM++\u6a21\u5757\u7528\u4e8e\u7c7b\u7279\u5f02\u6027\u663e\u8457\u6027\u53ef\u89c6\u5316\uff0c\u5e76\u52a0\u5165\u4e86\u7b26\u53f7\u5316\u4e34\u5e8a\u51b3\u7b56\u89c4\u5219\u53e0\u52a0\uff08CDRO\uff09\u4ee5\u5c06\u9884\u6d4b\u6620\u5c04\u5230\u653e\u5c04\u5b66\u542f\u53d1\u5f0f\u89c4\u5219\u3002\u6a21\u578b\u5728Figshare\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5e76\u9080\u8bf7\u653e\u5c04\u79d1\u533b\u751f\u8fdb\u884c\u4eba\u672c\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u3002", "result": "\u96c6\u6210\u5206\u7c7b\u5668\u6027\u80fd\u4f18\u4e8e\u5355\u4e2aCNN\uff0c\u5b9e\u73b0\u4e8691.7%\u7684\u51c6\u786e\u7387\u300191.9%\u7684\u7cbe\u786e\u7387\u300191.7%\u7684\u53ec\u56de\u7387\u548c91.6%\u7684F1\u5206\u6570\u3002Grad-CAM++\u53ef\u89c6\u5316\u663e\u793a\u6a21\u578b\u6ce8\u610f\u529b\u4e0e\u4e13\u5bb6\u6807\u6ce8\u80bf\u7624\u533a\u57df\u9ad8\u5ea6\u5bf9\u9f50\uff08Dice\u7cfb\u6570\u9ad8\u8fbe0.88\uff0cIoU\u5f97\u5206\u9ad8\u8fbe0.78\uff09\u3002\u4e34\u5e8a\u89c4\u5219\u6fc0\u6d3b\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6a21\u578b\u9884\u6d4b\u3002\u653e\u5c04\u79d1\u533b\u751f\u5bf9\u89e3\u91ca\u6709\u7528\u6027\uff08\u5e73\u57474.4\uff09\u548c\u70ed\u56fe\u533a\u57df\u5bf9\u5e94\u6027\uff08\u5e73\u57474.0\uff09\u7ed9\u51fa\u4e86\u8f83\u9ad8\u7684\u674e\u514b\u7279\u8bc4\u5206\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u81ea\u52a8\u8111\u80bf\u7624\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u4e14\u901a\u7528\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u5c06\u6df1\u5ea6\u5b66\u4e60\u96c6\u6210\u5230\u4e34\u5e8a\u795e\u7ecf\u8bca\u65ad\u4e2d\u3002"}}
{"id": "2508.07434", "pdf": "https://arxiv.org/pdf/2508.07434", "abs": "https://arxiv.org/abs/2508.07434", "authors": ["Zhiyi Lyu", "Jianguo Huang", "Yanchen Deng", "Steven Hoi", "Bo An"], "title": "Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) with inference-time scaling techniques show\npromise for code generation, yet face notable efficiency and scalability\nchallenges. Construction-based tree-search methods suffer from rapid growth in\ntree size, high token consumption, and lack of anytime property. In contrast,\nimprovement-based methods offer better performance but often struggle with\nuninformative reward signals and inefficient search strategies. In this work,\nwe propose \\textbf{ReLoc}, a unified local search framework which effectively\nperforms step-by-step code revision. Specifically, ReLoc explores a series of\nlocal revisions through four key algorithmic components: initial code drafting,\nneighborhood code generation, candidate evaluation, and incumbent code\nupdating, each of which can be instantiated with specific decision rules to\nrealize different local search algorithms such as Hill Climbing (HC) or Genetic\nAlgorithm (GA). Furthermore, we develop a specialized revision reward model\nthat evaluates code quality based on revision distance to produce fine-grained\npreferences that guide the local search toward more promising candidates.\nFinally, our extensive experimental results demonstrate that our approach\nachieves superior performance across diverse code generation tasks,\nsignificantly outperforming both construction-based tree search as well as the\nstate-of-the-art improvement-based code generation methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReLoc\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u5c40\u90e8\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u9010\u6b65\u4ee3\u7801\u4fee\u8ba2\u548c\u4e13\u95e8\u7684\u5956\u52b1\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u9762\u4e34\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6784\u5efa\u7684\u6811\u641c\u7d22\u65b9\u6cd5\u5b58\u5728\u6811\u89c4\u6a21\u5927\u3001\u4ee3\u5e01\u6d88\u8017\u9ad8\u7684\u95ee\u9898\uff0c\u800c\u57fa\u4e8e\u6539\u8fdb\u7684\u65b9\u6cd5\u5219\u53d7\u9650\u4e8e\u5956\u52b1\u4fe1\u53f7\u4fe1\u606f\u4e0d\u8db3\u548c\u641c\u7d22\u7b56\u7565\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faReLoc\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff08\u521d\u59cb\u4ee3\u7801\u8349\u62df\u3001\u90bb\u57df\u4ee3\u7801\u751f\u6210\u3001\u5019\u9009\u8bc4\u4f30\u3001\u5f53\u524d\u4ee3\u7801\u66f4\u65b0\uff09\u5b9e\u73b0\u5206\u6b65\u4ee3\u7801\u4fee\u8ba2\u3002\u8be5\u6846\u67b6\u53ef\u5b9e\u4f8b\u5316\u4e3a\u5982\u722c\u5c71\u6cd5\u6216\u9057\u4f20\u7b97\u6cd5\u7b49\u591a\u79cd\u5c40\u90e8\u641c\u7d22\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u4fee\u8ba2\u8ddd\u79bb\u7684\u4e13\u95e8\u5956\u52b1\u6a21\u578b\uff0c\u4ee5\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u504f\u597d\u6307\u5bfc\u5c40\u90e8\u641c\u7d22\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cReLoc\u5728\u591a\u79cd\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6784\u5efa\u7684\u6811\u641c\u7d22\u65b9\u6cd5\u548c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6539\u8fdb\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "ReLoc\u4f5c\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u5c40\u90e8\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u6548\u7684\u4ee3\u7801\u4fee\u8ba2\u7b56\u7565\u548c\u7cbe\u7ec6\u7684\u5956\u52b1\u6307\u5bfc\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLMs\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6548\u7387\u548c\u6027\u80fd\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.07054", "pdf": "https://arxiv.org/pdf/2508.07054", "abs": "https://arxiv.org/abs/2508.07054", "authors": ["Ziqi Zhang", "Ali Shahin Shamsabadi", "Hanxiao Lu", "Yifeng Cai", "Hamed Haddadi"], "title": "Membership and Memorization in LLM Knowledge Distillation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advances in Knowledge Distillation (KD) aim to mitigate the high\ncomputational demands of Large Language Models (LLMs) by transferring knowledge\nfrom a large ''teacher'' to a smaller ''student'' model. However, students may\ninherit the teacher's privacy when the teacher is trained on private data. In\nthis work, we systematically characterize and investigate membership and\nmemorization privacy risks inherent in six LLM KD techniques. Using\ninstruction-tuning settings that span seven NLP tasks, together with three\nteacher model families (GPT-2, LLAMA-2, and OPT), and various size student\nmodels, we demonstrate that all existing LLM KD approaches carry membership and\nmemorization privacy risks from the teacher to its students. However, the\nextent of privacy risks varies across different KD techniques. We\nsystematically analyse how key LLM KD components (KD objective functions,\nstudent training data and NLP tasks) impact such privacy risks. We also\ndemonstrate a significant disagreement between memorization and membership\nprivacy risks of LLM KD techniques. Finally, we characterize per-block privacy\nrisk and demonstrate that the privacy risk varies across different blocks by a\nlarge margin.", "AI": {"tldr": "LLM\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u5b58\u5728\u4ece\u6559\u5e08\u6a21\u578b\u5411\u5b66\u751f\u6a21\u578b\u4f20\u9012\u6210\u5458\u548c\u8bb0\u5fc6\u9690\u79c1\u98ce\u9669\u7684\u95ee\u9898\uff0c\u4e14\u98ce\u9669\u7a0b\u5ea6\u56e0\u84b8\u998f\u65b9\u6cd5\u548c\u7ec4\u4ef6\u800c\u5f02\u3002", "motivation": "\u4e3a\u964d\u4f4e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u88ab\u5e7f\u6cdb\u5e94\u7528\u3002\u7136\u800c\uff0c\u5f53\u6559\u5e08\u6a21\u578b\u57fa\u4e8e\u79c1\u6709\u6570\u636e\u8bad\u7ec3\u65f6\uff0c\u5b66\u751f\u6a21\u578b\u53ef\u80fd\u7ee7\u627f\u5176\u9690\u79c1\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5730\u8bc4\u4f30LLM\u77e5\u8bc6\u84b8\u998f\u4e2d\u56fa\u6709\u7684\u6210\u5458\u548c\u8bb0\u5fc6\u9690\u79c1\u98ce\u9669\u3002", "method": "\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u5206\u6790\u5e76\u8c03\u67e5\u4e86\u516d\u79cdLLM\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u4e2d\u7684\u6210\u5458\u548c\u8bb0\u5fc6\u9690\u79c1\u98ce\u9669\u3002\u5b9e\u9a8c\u5728\u5305\u542b\u4e03\u9879NLP\u4efb\u52a1\u7684\u6307\u4ee4\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u8fdb\u884c\uff0c\u4f7f\u7528\u4e86GPT-2\u3001LLAMA-2\u548cOPT\u4e09\u79cd\u6559\u5e08\u6a21\u578b\u5bb6\u65cf\u4ee5\u53ca\u4e0d\u540c\u5927\u5c0f\u7684\u5b66\u751f\u6a21\u578b\u3002\u7814\u7a76\u8fd8\u7cfb\u7edf\u5206\u6790\u4e86KD\u76ee\u6807\u51fd\u6570\u3001\u5b66\u751f\u8bad\u7ec3\u6570\u636e\u548cNLP\u4efb\u52a1\u7b49\u5173\u952e\u7ec4\u4ef6\u5982\u4f55\u5f71\u54cd\u9690\u79c1\u98ce\u9669\uff0c\u5e76\u8868\u5f81\u4e86\u9010\u5757\u9690\u79c1\u98ce\u9669\u3002", "result": "\u6240\u6709\u73b0\u6709LLM\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u90fd\u4f1a\u5c06\u6210\u5458\u548c\u8bb0\u5fc6\u9690\u79c1\u98ce\u9669\u4ece\u6559\u5e08\u6a21\u578b\u4f20\u9012\u7ed9\u5b66\u751f\u6a21\u578b\u3002\u9690\u79c1\u98ce\u9669\u7684\u7a0b\u5ea6\u56e0\u4e0d\u540c\u7684KD\u6280\u672f\u800c\u5f02\u3002\u8bb0\u5fc6\u9690\u79c1\u548c\u6210\u5458\u9690\u79c1\u98ce\u9669\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u9010\u5757\u9690\u79c1\u98ce\u9669\u7684\u5dee\u5f02\u4e5f\u5f88\u5927\u3002", "conclusion": "LLM\u77e5\u8bc6\u84b8\u998f\u4e2d\u666e\u904d\u5b58\u5728\u6210\u5458\u548c\u8bb0\u5fc6\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u8fd9\u4e9b\u98ce\u9669\u7684\u7a0b\u5ea6\u53d7\u84b8\u998f\u6280\u672f\u548c\u5173\u952e\u7ec4\u4ef6\u7684\u5f71\u54cd\uff0c\u4e14\u8bb0\u5fc6\u548c\u6210\u5458\u9690\u79c1\u98ce\u9669\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u8fd9\u5f3a\u8c03\u4e86\u5728LLM\u77e5\u8bc6\u84b8\u998f\u5b9e\u8df5\u4e2d\uff0c\u9690\u79c1\u4fdd\u62a4\u662f\u9700\u8981\u91cd\u70b9\u5173\u6ce8\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2508.07642", "pdf": "https://arxiv.org/pdf/2508.07642", "abs": "https://arxiv.org/abs/2508.07642", "authors": ["Tianyi Ma", "Yue Zhang", "Zehao Wang", "Parisa Kordjamshidi"], "title": "Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "18 pages, 5 Figures,", "summary": "Vision-and-Language Navigation (VLN) poses significant challenges in enabling\nagents to interpret natural language instructions and navigate complex 3D\nenvironments. While recent progress has been driven by large-scale pre-training\nand data augmentation, current methods still struggle to generalize to unseen\nscenarios, particularly when complex spatial and temporal reasoning is\nrequired. In this work, we propose SkillNav, a modular framework that\nintroduces structured, skill-based reasoning into Transformer-based VLN agents.\nOur method decomposes navigation into a set of interpretable atomic skills\n(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each\nhandled by a specialized agent. We then introduce a novel zero-shot\nVision-Language Model (VLM)-based router, which dynamically selects the most\nsuitable agent at each time step by aligning sub-goals with visual observations\nand historical actions. SkillNav achieves a new state-of-the-art performance on\nthe R2R benchmark and demonstrates strong generalization to the GSA-R2R\nbenchmark that includes novel instruction styles and unseen environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SkillNav\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u6280\u80fd\u7684\u63a8\u7406\u6765\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u4ee3\u7406\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u65b9\u6cd5\u5728\u6cdb\u5316\u5230\u672a\u77e5\u573a\u666f\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u590d\u6742\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u7684\u60c5\u51b5\u4e0b\uff0c\u5c3d\u7ba1\u5df2\u6709\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u6570\u636e\u589e\u5f3a\u7684\u8fdb\u5c55\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86SkillNav\u6846\u67b6\uff0c\u5b83\u5c06\u5bfc\u822a\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u53ef\u89e3\u91ca\u7684\u539f\u5b50\u6280\u80fd\uff08\u5982\u5782\u76f4\u79fb\u52a8\u3001\u533a\u57df\u8bc6\u522b\u3001\u505c\u6b62\u548c\u6682\u505c\uff09\uff0c\u6bcf\u79cd\u6280\u80fd\u7531\u4e00\u4e2a\u4e13\u95e8\u7684\u4ee3\u7406\u5904\u7406\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u96f6\u6837\u672c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8def\u7531\u5668\uff0c\u8be5\u8def\u7531\u5668\u6839\u636e\u5b50\u76ee\u6807\u3001\u89c6\u89c9\u89c2\u5bdf\u548c\u5386\u53f2\u52a8\u4f5c\u52a8\u6001\u9009\u62e9\u6700\u5408\u9002\u7684\u4ee3\u7406\u3002", "result": "SkillNav\u5728R2R\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728\u5305\u542b\u65b0\u6307\u4ee4\u98ce\u683c\u548c\u672a\u89c1\u73af\u5883\u7684GSA-R2R\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SkillNav\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u6280\u80fd\u7684\u63a8\u7406\uff0c\u6709\u6548\u63d0\u5347\u4e86Transformer-based VLN\u4ee3\u7406\u5728\u590d\u67423D\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u548c\u5bf9\u672a\u77e5\u573a\u666f\u7684\u6cdb\u5316\u6027\u3002"}}
{"id": "2508.06895", "pdf": "https://arxiv.org/pdf/2508.06895", "abs": "https://arxiv.org/abs/2508.06895", "authors": ["Jianting Tang", "Yubo Wang", "Haoyu Cao", "Linli Xu"], "title": "BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICCV 2025", "summary": "Mainstream Multimodal Large Language Models (MLLMs) achieve visual\nunderstanding by using a vision projector to bridge well-pretrained vision\nencoders and large language models (LLMs). The inherent gap between visual and\ntextual modalities makes the embeddings from the vision projector critical for\nvisual comprehension. However, current alignment approaches treat visual\nembeddings as contextual cues and merely apply auto-regressive supervision to\ntextual outputs, neglecting the necessity of introducing equivalent direct\nvisual supervision, which hinders the potential finer alignment of visual\nembeddings. In this paper, based on our analysis of the refinement process of\nvisual embeddings in the LLM's shallow layers, we propose BASIC, a method that\nutilizes refined visual embeddings within the LLM as supervision to directly\nguide the projector in generating initial visual embeddings. Specifically, the\nguidance is conducted from two perspectives: (i) optimizing embedding\ndirections by reducing angles between initial and supervisory embeddings in\nsemantic space; (ii) improving semantic matching by minimizing disparities\nbetween the logit distributions of both visual embeddings. Without additional\nsupervisory models or artificial annotations, BASIC significantly improves the\nperformance of MLLMs across a wide range of benchmarks, demonstrating the\neffectiveness of our introduced direct visual supervision.", "AI": {"tldr": "\u4e3b\u6d41\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7406\u89e3\u4e2d\u7f3a\u4e4f\u5bf9\u89c6\u89c9\u6295\u5f71\u5668\u8f93\u51fa\u7684\u76f4\u63a5\u89c6\u89c9\u76d1\u7763\uff0c\u963b\u788d\u4e86\u89c6\u89c9\u5d4c\u5165\u7684\u7cbe\u7ec6\u5bf9\u9f50\u3002\u672c\u6587\u63d0\u51faBASIC\u65b9\u6cd5\uff0c\u5229\u7528LLM\u5185\u90e8\u7684\u7cbe\u70bc\u89c6\u89c9\u5d4c\u5165\u76f4\u63a5\u6307\u5bfc\u89c6\u89c9\u6295\u5f71\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLM\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u4e3b\u6d41MLLMs\u4f9d\u8d56\u89c6\u89c9\u6295\u5f71\u5668\u8fde\u63a5\u89c6\u89c9\u7f16\u7801\u5668\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5bf9\u9f50\u89c6\u89c9\u5d4c\u5165\u65f6\uff0c\u4ec5\u5c06\u5176\u4f5c\u4e3a\u6587\u672c\u8f93\u51fa\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u7f3a\u4e4f\u76f4\u63a5\u7684\u89c6\u89c9\u76d1\u7763\u3002\u8fd9\u79cd\u4e0d\u8db3\u963b\u788d\u4e86\u89c6\u89c9\u5d4c\u5165\u7684\u7cbe\u7ec6\u5bf9\u9f50\u548c\u6f5c\u5728\u6027\u80fd\u63d0\u5347\u3002", "method": "\u672c\u6587\u63d0\u51faBASIC\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5bf9LLM\u6d45\u5c42\u89c6\u89c9\u5d4c\u5165\u7ec6\u5316\u8fc7\u7a0b\u7684\u5206\u6790\uff0c\u5229\u7528LLM\u5185\u90e8\u7684\u7ec6\u5316\u89c6\u89c9\u5d4c\u5165\u4f5c\u4e3a\u76d1\u7763\uff0c\u76f4\u63a5\u6307\u5bfc\u89c6\u89c9\u6295\u5f71\u5668\u751f\u6210\u521d\u59cb\u89c6\u89c9\u5d4c\u5165\u3002\u5177\u4f53\u6307\u5bfc\u4ece\u4e24\u4e2a\u65b9\u9762\u8fdb\u884c\uff1a\u4e00\u662f\u901a\u8fc7\u51cf\u5c0f\u521d\u59cb\u5d4c\u5165\u548c\u76d1\u7763\u5d4c\u5165\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u7684\u89d2\u5ea6\u6765\u4f18\u5316\u5d4c\u5165\u65b9\u5411\uff1b\u4e8c\u662f\u901a\u8fc7\u6700\u5c0f\u5316\u4e24\u79cd\u89c6\u89c9\u5d4c\u5165\u7684logit\u5206\u5e03\u5dee\u5f02\u6765\u6539\u5584\u8bed\u4e49\u5339\u914d\u3002", "result": "BASIC\u65b9\u6cd5\u5728\u65e0\u9700\u989d\u5916\u76d1\u7763\u6a21\u578b\u6216\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86MLLMs\u5728\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u5f15\u5165\u7684\u76f4\u63a5\u89c6\u89c9\u76d1\u7763\u65b9\u6cd5\uff08BASIC\uff09\u6709\u6548\u63d0\u5347\u4e86MLLM\u7684\u6027\u80fd\uff0c\u5e76\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u89c6\u89c9\u5d4c\u5165\u7cbe\u7ec6\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u76f4\u63a5\u89c6\u89c9\u76d1\u7763\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.07479", "pdf": "https://arxiv.org/pdf/2508.07479", "abs": "https://arxiv.org/abs/2508.07479", "authors": ["Blerta Veseli", "Julian Chibane", "Mariya Toneva", "Alexander Koller"], "title": "Positional Biases Shift as Inputs Approach Context Window Limits", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often struggle to use information across long\ninputs effectively. Prior work has identified positional biases, such as the\nLost in the Middle (LiM) effect, where models perform better when information\nappears at the beginning (primacy bias) or end (recency bias) of the input,\nrather than in the middle. However, long-context studies have not consistently\nreplicated these effects, raising questions about their intensity and the\nconditions under which they manifest. To address this, we conducted a\ncomprehensive analysis using relative rather than absolute input lengths,\ndefined with respect to each model's context window. Our findings reveal that\nthe LiM effect is strongest when inputs occupy up to 50% of a model's context\nwindow. Beyond that, the primacy bias weakens, while recency bias remains\nrelatively stable. This effectively eliminates the LiM effect; instead, we\nobserve a distance-based bias, where model performance is better when relevant\ninformation is closer to the end of the input. Furthermore, our results suggest\nthat successful retrieval is a prerequisite for reasoning in LLMs, and that the\nobserved positional biases in reasoning are largely inherited from retrieval.\nThese insights have implications for long-context tasks, the design of future\nLLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u76f8\u5bf9\u8f93\u5165\u957f\u5ea6\u5206\u6790\uff0c\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u957f\u6587\u672c\u4e2d\u7684\u4f4d\u7f6e\u504f\u5dee\u3002\u53d1\u73b0\u201c\u4e2d\u95f4\u4e22\u5931\u201d\uff08LiM\uff09\u6548\u5e94\u5728\u8f93\u5165\u5c0f\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e350%\u65f6\u6700\u5f3a\uff0c\u8d85\u8fc7\u5219\u521d\u4f4d\u504f\u89c1\u51cf\u5f31\uff0c\u8fd1\u4f4d\u504f\u89c1\u7a33\u5b9a\uff0c\u8868\u73b0\u4e3a\u4fe1\u606f\u8d8a\u9760\u8fd1\u7ed3\u5c3e\u6027\u80fd\u8d8a\u597d\u7684\u8ddd\u79bb\u504f\u89c1\u3002\u63a8\u7406\u4e2d\u7684\u4f4d\u7f6e\u504f\u89c1\u4e3b\u8981\u6e90\u4e8e\u68c0\u7d22\u3002", "motivation": "\u73b0\u6709\u5173\u4e8eLLM\u5728\u957f\u8f93\u5165\u4e2d\u4f4d\u7f6e\u504f\u89c1\uff08\u5982LiM\u6548\u5e94\uff09\u7684\u7814\u7a76\u7ed3\u679c\u5b58\u5728\u4e0d\u4e00\u81f4\uff0c\u5bf9\u5176\u5f3a\u5ea6\u548c\u8868\u73b0\u6761\u4ef6\u5b58\u5728\u7591\u95ee\uff0c\u4e9f\u9700\u6df1\u5165\u63a2\u7a76\u3002", "method": "\u91c7\u7528\u76f8\u5bf9\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u201c\u76f8\u5bf9\u8f93\u5165\u957f\u5ea6\u201d\u800c\u975e\u201c\u7edd\u5bf9\u8f93\u5165\u957f\u5ea6\u201d\uff0c\u5bf9LLM\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002", "result": ["LiM\u6548\u5e94\u5728\u8f93\u5165\u957f\u5ea6\u5360\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e350%\u4ee5\u5185\u65f6\u6700\u4e3a\u663e\u8457\u3002", "\u5f53\u8f93\u5165\u957f\u5ea6\u8d85\u8fc750%\u65f6\uff0c\u521d\u4f4d\u504f\u89c1\u663e\u8457\u51cf\u5f31\uff0c\u8fd1\u4f4d\u504f\u89c1\u4fdd\u6301\u7a33\u5b9a\uff0cLiM\u6548\u5e94\u6d88\u5931\uff0c\u8f6c\u53d8\u4e3a\u4e00\u79cd\u201c\u57fa\u4e8e\u8ddd\u79bb\u201d\u7684\u504f\u89c1\uff1a\u76f8\u5173\u4fe1\u606f\u8d8a\u63a5\u8fd1\u8f93\u5165\u672b\u5c3e\uff0c\u6a21\u578b\u6027\u80fd\u8d8a\u597d\u3002", "\u6210\u529f\u7684\u4fe1\u606f\u68c0\u7d22\u662fLLM\u8fdb\u884c\u63a8\u7406\u7684\u5148\u51b3\u6761\u4ef6\uff0c\u4e14\u63a8\u7406\u4e2d\u7684\u4f4d\u7f6e\u504f\u89c1\u5f88\u5927\u7a0b\u5ea6\u4e0a\u7ee7\u627f\u81ea\u68c0\u7d22\u8fc7\u7a0b\u3002"], "conclusion": "\u672c\u7814\u7a76\u7684\u53d1\u73b0\u5bf9\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u3001\u672a\u6765LLM\u57fa\u51c6\u6d4b\u8bd5\u8bbe\u8ba1\u4ee5\u53ca\u957f\u8f93\u5165\u8bc4\u4f30\u65b9\u6cd5\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2508.07075", "pdf": "https://arxiv.org/pdf/2508.07075", "abs": "https://arxiv.org/abs/2508.07075", "authors": ["Stanley Ngugi"], "title": "Surgical Knowledge Rewrite in Compact LLMs: An 'Unlearn-then-Learn' Strategy with ($IA^3$) for Localized Factual Modulation and Catastrophic Forgetting Mitigation", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 2 visual aids", "summary": "Large Language Models (LLMs) struggle with dynamic knowledge updates,\nespecially when new information conflicts with deeply embedded facts. Such\nconflicting factual edits often lead to two critical issues: resistance to\nadopting the new fact and severe catastrophic forgetting of unrelated\nknowledge. This paper introduces and evaluates a novel \"unlearn-then-learn\"\nstrategy for precise knowledge editing in LLMs, leveraging the\nparameter-efficient fine-tuning (PEFT) technique, Infused Adapter by Inhibiting\nand Amplifying Inner Activations ($IA^3$). Crucially, this two-stage approach\nis powered by an initial circuit localization phase that identifies and targets\nthe specific internal components responsible for encoding the conflicting fact.\nThrough a rigorous experimental methodology on\nmicrosoft/Phi-3-mini-4k-instruct, we demonstrate that this mechanistically\ninformed two-stage approach achieves near-perfect accuracy (98.50%) for the\nnew, modulated fact while simultaneously effectively suppressing the original\nconflicting fact (96.00% forget rate). Critically, our strategy exhibits\nunprecedented localization (72.00% F_control accuracy), dramatically mitigating\ncatastrophic forgetting observed in direct fine-tuning approaches (which showed\nas low as ~20% F_control accuracy), a direct benefit of our targeted\ninterpretability-guided intervention. Furthermore, qualitative analysis reveals\na nuanced mechanism of \"soft forgetting,\" where original knowledge is\nsuppressed from default retrieval but remains latent and conditionally\naccessible, enhancing model safety and control. These findings represent a\nsignificant advancement towards precise, localized, and safe knowledge\nmanagement in compact LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e$IA^3$\u548c\u7535\u8def\u5b9a\u4f4d\u7684\u201c\u5148\u9057\u5fd8\u540e\u5b66\u4e60\u201d\u7b56\u7565\uff0c\u7528\u4e8eLLMs\u7684\u7cbe\u786e\u77e5\u8bc6\u7f16\u8f91\uff0c\u6709\u6548\u66f4\u65b0\u51b2\u7a81\u4e8b\u5b9e\uff0c\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u5b9e\u73b0\u8f6f\u9057\u5fd8\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u52a8\u6001\u77e5\u8bc6\u66f4\u65b0\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u65b0\u4fe1\u606f\u4e0e\u65e2\u6709\u4e8b\u5b9e\u51b2\u7a81\u65f6\uff0c\u4f1a\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u91c7\u7eb3\u65b0\u4e8b\u5b9e\uff0c\u5e76\u4e25\u91cd\u9057\u5fd8\u4e0d\u76f8\u5173\u77e5\u8bc6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u201c\u5148\u9057\u5fd8\u540e\u5b66\u4e60\u201d\u4e24\u9636\u6bb5\u77e5\u8bc6\u7f16\u8f91\u7b56\u7565\uff0c\u5229\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6280\u672f$IA^3$\u3002\u8be5\u65b9\u6cd5\u5173\u952e\u5728\u4e8e\u5148\u8fdb\u884c\u7535\u8def\u5b9a\u4f4d\uff0c\u8bc6\u522b\u5e76\u9488\u5bf9\u7f16\u7801\u51b2\u7a81\u4e8b\u5b9e\u7684\u7279\u5b9a\u5185\u90e8\u7ec4\u4ef6\u3002", "result": "\u5728Phi-3-mini-4k-instruct\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5bf9\u65b0\u4e8b\u5b9e\u8fbe\u5230\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\uff0898.50%\uff09\uff0c\u5e76\u6709\u6548\u6291\u5236\u4e86\u539f\u6709\u51b2\u7a81\u4e8b\u5b9e\uff0896.00%\u7684\u9057\u5fd8\u7387\uff09\u3002\u5b83\u8868\u73b0\u51fa\u524d\u6240\u672a\u6709\u7684\u77e5\u8bc6\u5b9a\u4f4d\u80fd\u529b\uff0872.00%\u7684F_control\u51c6\u786e\u7387\uff09\uff0c\u663e\u8457\u51cf\u8f7b\u4e86\u707e\u96be\u6027\u9057\u5fd8\u3002\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u201c\u8f6f\u9057\u5fd8\u201d\u673a\u5236\uff0c\u5373\u539f\u6709\u77e5\u8bc6\u88ab\u6291\u5236\u4f46\u4ecd\u5904\u4e8e\u6f5c\u5728\u72b6\u6001\u4e14\u53ef\u6761\u4ef6\u6027\u8bbf\u95ee\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4ee3\u8868\u4e86\u5728\u7d27\u51d1\u578bLLMs\u4e2d\u5b9e\u73b0\u7cbe\u786e\u3001\u5c40\u90e8\u5316\u548c\u5b89\u5168\u77e5\u8bc6\u7ba1\u7406\u65b9\u9762\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2508.07649", "pdf": "https://arxiv.org/pdf/2508.07649", "abs": "https://arxiv.org/abs/2508.07649", "authors": ["Jie Li", "Haoye Dong", "Zhengyang Wu", "Zetao Zheng", "Mingrong Lin"], "title": "Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Next Point-of-Interest (POI) recommendation is a research hotspot in business\nintelligence, where users' spatial-temporal transitions and social\nrelationships play key roles. However, most existing works model spatial and\ntemporal transitions separately, leading to misaligned representations of the\nsame spatial-temporal key nodes. This misalignment introduces redundant\ninformation during fusion, increasing model uncertainty and reducing\ninterpretability. To address this issue, we propose DiMuST, a socially enhanced\nPOI recommendation model based on disentangled representation learning over\nmultiplex spatial-temporal transition graphs. The model employs a novel\nDisentangled variational multiplex graph Auto-Encoder (DAE), which first\ndisentangles shared and private distributions using a multiplex\nspatial-temporal graph strategy. It then fuses the shared features via a\nProduct of Experts (PoE) mechanism and denoises the private features through\ncontrastive constraints. The model effectively captures the spatial-temporal\ntransition representations of POIs while preserving the intrinsic correlation\nof their spatial-temporal relationships. Experiments on two challenging\ndatasets demonstrate that our DiMuST significantly outperforms existing methods\nacross multiple metrics.", "AI": {"tldr": "\u63d0\u51faDiMuST\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u8026\u8868\u793a\u5b66\u4e60\u89e3\u51b3\u73b0\u6709POI\u63a8\u8350\u4e2d\u65f6\u7a7a\u8868\u793a\u9519\u4f4d\u95ee\u9898\uff0c\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u73b0\u6709POI\u63a8\u8350\u6a21\u578b\u72ec\u7acb\u5efa\u6a21\u65f6\u7a7a\u8f6c\u6362\uff0c\u5bfc\u81f4\u65f6\u7a7a\u8282\u70b9\u8868\u793a\u9519\u4f4d\uff0c\u878d\u5408\u65f6\u5f15\u5165\u5197\u4f59\u4fe1\u606f\uff0c\u589e\u52a0\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5e76\u964d\u4f4e\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faDiMuST\uff0c\u4e00\u4e2a\u57fa\u4e8e\u591a\u8def\u65f6\u7a7a\u8f6c\u6362\u56fe\u7684\u793e\u4ea4\u589e\u5f3aPOI\u63a8\u8350\u6a21\u578b\u3002\u5176\u6838\u5fc3\u662f\u65b0\u578b\u89e3\u8026\u53d8\u5206\u591a\u8def\u56fe\u81ea\u52a8\u7f16\u7801\u5668\uff08DAE\uff09\uff0c\u901a\u8fc7\u591a\u8def\u65f6\u7a7a\u56fe\u7b56\u7565\u89e3\u8026\u5171\u4eab\u548c\u79c1\u6709\u5206\u5e03\uff0c\u518d\u5229\u7528\u4e13\u5bb6\u4e58\u79ef\uff08PoE\uff09\u673a\u5236\u878d\u5408\u5171\u4eab\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u7ea6\u675f\u53bb\u566a\u79c1\u6709\u7279\u5f81\u3002", "result": "DiMuST\u6a21\u578b\u6709\u6548\u6355\u83b7POI\u65f6\u7a7a\u8f6c\u6362\u8868\u793a\u5e76\u4fdd\u7559\u5176\u5185\u5728\u5173\u8054\u3002\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cDiMuST\u5728\u591a\u9879\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DiMuST\u901a\u8fc7\u89e3\u8026\u8868\u793a\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86POI\u63a8\u8350\u4e2d\u65f6\u7a7a\u8868\u793a\u9519\u4f4d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.06900", "pdf": "https://arxiv.org/pdf/2508.06900", "abs": "https://arxiv.org/abs/2508.06900", "authors": ["Weiran Chen", "Guiqian Zhu", "Ying Li", "Yi Ji", "Chunping Liu"], "title": "Advancements in Chinese font generation since deep learning era: A survey", "categories": ["cs.CV", "cs.AI"], "comment": "42 Pages, 25 figures", "summary": "Chinese font generation aims to create a new Chinese font library based on\nsome reference samples. It is a topic of great concern to many font designers\nand typographers. Over the past years, with the rapid development of deep\nlearning algorithms, various new techniques have achieved flourishing and\nthriving progress. Nevertheless, how to improve the overall quality of\ngenerated Chinese character images remains a tough issue. In this paper, we\nconduct a holistic survey of the recent Chinese font generation approaches\nbased on deep learning. To be specific, we first illustrate the research\nbackground of the task. Then, we outline our literature selection and analysis\nmethodology, and review a series of related fundamentals, including classical\ndeep learning architectures, font representation formats, public datasets, and\nfrequently-used evaluation metrics. After that, relying on the number of\nreference samples required to generate a new font, we categorize the existing\nmethods into two major groups: many-shot font generation and few-shot font\ngeneration methods. Within each category, representative approaches are\nsummarized, and their strengths and limitations are also discussed in detail.\nFinally, we conclude our paper with the challenges and future directions, with\nthe expectation to provide some valuable illuminations for the researchers in\nthis field.", "AI": {"tldr": "\u5bf9\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u6587\u4e66\u6cd5\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u7684\u7efc\u8ff0\uff0c\u5e76\u63a2\u8ba8\u5176\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4e2d\u6587\u5b57\u4f53\u751f\u6210\u662f\u70ed\u95e8\u8bfe\u9898\uff0c\u4f46\u63d0\u5347\u751f\u6210\u8d28\u91cf\u4ecd\u662f\u96be\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u68b3\u7406\u73b0\u6709\u6280\u672f\u8fdb\u5c55\u4e0e\u74f6\u9888\u3002", "method": "\u91c7\u7528\u7efc\u8ff0\u65b9\u6cd5\uff0c\u9996\u5148\u9610\u8ff0\u7814\u7a76\u80cc\u666f\u4e0e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u56de\u987e\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3001\u5b57\u4f53\u8868\u793a\u7b49\u57fa\u7840\uff1b\u7136\u540e\u6839\u636e\u6240\u9700\u53c2\u8003\u6837\u672c\u6570\u91cf\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u591a\u6837\u672c\u751f\u6210\u548c\u5c11\u6837\u672c\u751f\u6210\u4e24\u7c7b\uff0c\u5e76\u8be6\u7ec6\u603b\u7ed3\u5404\u7c7b\u4ee3\u8868\u6027\u65b9\u6cd5\u53ca\u5176\u4f18\u7f3a\u70b9\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u4e2d\u6587\u4e66\u6cd5\u751f\u6210\u65b9\u6cd5\u7684\u5168\u9762\u5206\u7c7b\u3001\u603b\u7ed3\u4e0e\u5206\u6790\uff0c\u5e76\u8bc6\u522b\u51fa\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "\u603b\u7ed3\u4e86\u8be5\u9886\u57df\u7684\u6311\u6218\u4e0e\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u542f\u793a\u3002"}}
{"id": "2508.07484", "pdf": "https://arxiv.org/pdf/2508.07484", "abs": "https://arxiv.org/abs/2508.07484", "authors": ["Archchana Sindhujan", "Shenbin Qian", "Chan Chi Chun Matthew", "Constantin Orasan", "Diptesh Kanojia"], "title": "ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to COLM 2025 Conference", "summary": "Large Language Models (LLMs) have shown remarkable performance across a wide\nrange of natural language processing tasks. Quality Estimation (QE) for Machine\nTranslation (MT), which assesses the quality of a source-target pair without\nrelying on reference translations, remains a challenging cross-lingual task for\nLLMs. The challenges stem from the inherent limitations of existing LLM-based\nQE systems, which are pre-trained for causal language modelling rather than\nregression-specific tasks, further elevated by the presence of low-resource\nlanguages given pre-training data distribution. This paper introduces ALOPE, an\nadaptive layer-optimization framework designed to enhance LLM-based QE by\nrestructuring Transformer representations through layer-wise adaptation for\nimproved regression-based prediction. Our framework integrates low-rank\nadapters (LoRA) with regression task heads, leveraging selected pre-trained\nTransformer layers for improved cross-lingual alignment. In addition to the\nlayer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,\nwhich adaptively combines representations from multiple layers, and multi-head\nregression, which aggregates regression losses from multiple heads for QE. Our\nframework shows improvements over various existing LLM-based QE approaches.\nEmpirical evidence suggests that intermediate Transformer layers in LLMs\nprovide contextual representations that are more aligned with the cross-lingual\nnature of the QE task. We make resultant models and framework code publicly\navailable for further research, also allowing existing LLM-based MT frameworks\nto be scaled with QE capabilities.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecdALOPE\uff0c\u4e00\u4e2a\u81ea\u9002\u5e94\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u7ea7\u81ea\u9002\u5e94\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\uff08QE\uff09\u4e2d\u7684\u56de\u5f52\u9884\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u9488\u5bf9\u8de8\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u6311\u6218\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\uff08QE\uff09\u8fd9\u4e00\u8de8\u8bed\u8a00\u4efb\u52a1\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709LLM-based QE\u7cfb\u7edf\u53d7\u9650\u4e8eLLM\u7684\u9884\u8bad\u7ec3\uff08\u4fa7\u91cd\u56e0\u679c\u8bed\u8a00\u5efa\u6a21\u800c\u975e\u56de\u5f52\u4efb\u52a1\uff09\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6570\u636e\u5206\u5e03\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86ALOPE\u6846\u67b6\uff0c\u4e00\u4e2a\u81ea\u9002\u5e94\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u5c42\u7ea7\u81ea\u9002\u5e94\u91cd\u6784Transformer\u8868\u793a\uff0c\u4ee5\u4f18\u5316LLM\u5728\u56de\u5f52\u4efb\u52a1\uff08\u5982QE\uff09\u4e2d\u7684\u9884\u6d4b\u6027\u80fd\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u5c06\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\u4e0e\u56de\u5f52\u4efb\u52a1\u5934\u7ed3\u5408\uff0c\u5229\u7528\u9009\u5b9a\u7684\u9884\u8bad\u7ec3Transformer\u5c42\u6765\u6539\u8fdb\u8de8\u8bed\u8a00\u5bf9\u9f50\u3002\u6b64\u5916\uff0cALOPE\u5f15\u5165\u4e86\u52a8\u6001\u52a0\u6743\u7b56\u7565\uff08\u81ea\u9002\u5e94\u7ec4\u5408\u591a\u5c42\u8868\u793a\uff09\u548c\u591a\u5934\u56de\u5f52\u7b56\u7565\uff08\u805a\u5408\u591a\u5934\u56de\u5f52\u635f\u5931\uff09\u3002", "result": "ALOPE\u6846\u67b6\u5728\u5404\u79cd\u73b0\u6709\u57fa\u4e8eLLM\u7684QE\u65b9\u6cd5\u4e0a\u5747\u663e\u793a\u51fa\u6539\u8fdb\u3002\u7ecf\u9a8c\u8bc1\u636e\u8868\u660e\uff0cLLM\u4e2d\u95f4\u7684Transformer\u5c42\u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\u8868\u793a\u66f4\u7b26\u5408QE\u4efb\u52a1\u7684\u8de8\u8bed\u8a00\u7279\u6027\u3002", "conclusion": "ALOPE\u6846\u67b6\u901a\u8fc7\u5c42\u7ea7\u4f18\u5316\u6709\u6548\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u8de8\u8bed\u8a00\u573a\u666f\u4e2d\u3002\u7814\u7a76\u63ed\u793aLLM\u7684\u4e2d\u95f4\u5c42\u5bf9\u8de8\u8bed\u8a00QE\u4efb\u52a1\u5177\u6709\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u3002\u76f8\u5173\u6a21\u578b\u548c\u4ee3\u7801\u5df2\u516c\u5f00\uff0c\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u548c\u73b0\u6709LLM-based MT\u6846\u67b6\u7684QE\u80fd\u529b\u6269\u5c55\u3002"}}
{"id": "2508.07085", "pdf": "https://arxiv.org/pdf/2508.07085", "abs": "https://arxiv.org/abs/2508.07085", "authors": ["N Harshit", "K Mounvik"], "title": "Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework", "categories": ["cs.LG"], "comment": null, "summary": "In applied machine learning, concept drift, which is either gradual or abrupt\nchanges in data distribution, can significantly reduce model performance.\nTypical detection methods,such as statistical tests or reconstruction-based\nmodels,are generally reactive and not very sensitive to early detection. Our\nstudy proposes a hybrid framework consisting of Transformers and Autoencoders\nto model complex temporal dynamics and provide online drift detection. We\ncreate a distinct Trust Score methodology, which includes signals on (1)\nstatistical and reconstruction-based drift metrics, more specifically, PSI,\nJSD, Transformer-AE error, (2) prediction uncertainty, (3) rules violations,\nand (4) trend of classifier error aligned with the combined metrics defined by\nthe Trust Score. Using a time sequenced airline passenger data set with\nsynthetic drift, our proposed model allows for a better detection of drift\nusing as a whole and at different detection thresholds for both sensitivity and\ninterpretability compared to baseline methods and provides a strong pipeline\nfor drift detection in real time for applied machine learning. We evaluated\nperformance using a time-sequenced airline passenger dataset having the\ngradually injected stimulus of drift in expectations,e.g. permuted ticket\nprices in later batches, broken into 10 time segments [1].In the data, our\nresults support that the Transformation-Autoencoder detected drift earlier and\nwith more sensitivity than the autoencoders commonly used in the literature,\nand provided improved modeling over more error rates and logical violations.\nTherefore, a robust framework was developed to reliably monitor concept drift.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408Transformer\u548cAutoencoder\u7684\u6df7\u5408\u6846\u67b6\u53ca\u4fe1\u4efb\u5206\u6570\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\uff0c\u5728\u822a\u7a7a\u4e58\u5ba2\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u65e9\u3001\u66f4\u7075\u654f\u7684\u6f02\u79fb\u68c0\u6d4b\u3002", "motivation": "\u5e94\u7528\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u6982\u5ff5\u6f02\u79fb\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u53cd\u5e94\u8fdf\u7f13\uff0c\u5bf9\u65e9\u671f\u6f02\u79fb\u4e0d\u654f\u611f\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5728\u7ebf\u68c0\u6d4b\u9700\u6c42\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408Transformer\u548cAutoencoder\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u590d\u6742\u65f6\u95f4\u52a8\u6001\u548c\u5728\u7ebf\u6f02\u79fb\u68c0\u6d4b\u3002\u6838\u5fc3\u662f\u6784\u5efa\u4e86\u4e00\u4e2a\u201c\u4fe1\u4efb\u5206\u6570\u201d\u65b9\u6cd5\uff0c\u6574\u5408\u4e86\u7edf\u8ba1\u6f02\u79fb\u6307\u6807\uff08PSI, JSD\uff09\u3001\u91cd\u5efa\u8bef\u5dee\uff08Transformer-AE\u8bef\u5dee\uff09\u3001\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3001\u89c4\u5219\u8fdd\u89c4\u548c\u5206\u7c7b\u5668\u8bef\u5dee\u8d8b\u52bf\u7b49\u4fe1\u53f7\u3002", "result": "\u5728\u6ce8\u5165\u5408\u6210\u6f02\u79fb\u7684\u822a\u7a7a\u4e58\u5ba2\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u6574\u4f53\u8868\u73b0\u548c\u5728\u4e0d\u540c\u68c0\u6d4b\u9608\u503c\u4e0b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u90fd\u80fd\u66f4\u65e9\u3001\u66f4\u7075\u654f\u5730\u68c0\u6d4b\u5230\u6982\u5ff5\u6f02\u79fb\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u4f73\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u9519\u8bef\u7387\u548c\u903b\u8f91\u8fdd\u89c4\u3002", "conclusion": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u6846\u67b6\uff0c\u80fd\u53ef\u9760\u5730\u5b9e\u65f6\u76d1\u63a7\u6982\u5ff5\u6f02\u79fb\uff0c\u4e3a\u5e94\u7528\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6f02\u79fb\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07667", "pdf": "https://arxiv.org/pdf/2508.07667", "abs": "https://arxiv.org/abs/2508.07667", "authors": ["Wenkai Li", "Liwen Sun", "Zhenxiang Guan", "Xuhui Zhou", "Maarten Sap"], "title": "1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Addressing contextual privacy concerns remains challenging in interactive\nsettings where large language models (LLMs) process information from multiple\nsources (e.g., summarizing meetings with private and public information). We\nintroduce a multi-agent framework that decomposes privacy reasoning into\nspecialized subtasks (extraction, classification), reducing the information\nload on any single agent while enabling iterative validation and more reliable\nadherence to contextual privacy norms. To understand how privacy errors emerge\nand propagate, we conduct a systematic ablation over information-flow\ntopologies, revealing when and why upstream detection mistakes cascade into\ndownstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with\nseveral open-source and closed-sourced LLMs demonstrate that our best\nmulti-agent configuration substantially reduces private information leakage\n(\\textbf{18\\%} on ConfAIde and \\textbf{19\\%} on PrivacyLens with GPT-4o) while\npreserving the fidelity of public content, outperforming single-agent\nbaselines. These results highlight the promise of principled information-flow\ndesign in multi-agent systems for contextual privacy with LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u9690\u79c1\u63a8\u7406\u4efb\u52a1\uff0c\u6709\u6548\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u591a\u6e90\u4fe1\u606f\u65f6\u7684\u9690\u79c1\u6cc4\u9732\u3002", "motivation": "\u5728\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2d\uff0cLLMs\u5904\u7406\u5305\u542b\u79c1\u6709\u548c\u516c\u5171\u4fe1\u606f\u7684\u590d\u6742\u5185\u5bb9\u65f6\uff0c\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u4e0a\u4e0b\u6587\u9690\u79c1\u95ee\u9898\uff0c\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u6cc4\u9732\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u9690\u79c1\u63a8\u7406\u5206\u89e3\u4e3a\u4e13\u4e1a\u5b50\u4efb\u52a1\uff08\u5982\u63d0\u53d6\u3001\u5206\u7c7b\uff09\uff0c\u4ee5\u51cf\u5c11\u5355\u4e2a\u667a\u80fd\u4f53\u7684\u4fe1\u606f\u8d1f\u8f7d\uff0c\u5e76\u5b9e\u73b0\u8fed\u4ee3\u9a8c\u8bc1\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u5bf9\u4fe1\u606f\u6d41\u62d3\u6251\u8fdb\u884c\u7cfb\u7edf\u6d88\u878d\u7814\u7a76\uff0c\u5206\u6790\u9690\u79c1\u9519\u8bef\u7684\u4ea7\u751f\u548c\u4f20\u64ad\u673a\u5236\u3002", "result": "\u5728ConfAIde\u548cPrivacyLens\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u6700\u4f73\u591a\u667a\u80fd\u4f53\u914d\u7f6e\u663e\u8457\u964d\u4f4e\u4e86\u79c1\u6709\u4fe1\u606f\u6cc4\u9732\uff08\u4f7f\u7528GPT-4o\u65f6\u5206\u522b\u964d\u4f4e18%\u548c19%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u516c\u5171\u5185\u5bb9\u7684\u51c6\u786e\u6027\uff0c\u8868\u73b0\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u3002", "conclusion": "\u5728\u57fa\u4e8eLLMs\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u539f\u5219\u6027\u7684\u4fe1\u606f\u6d41\u8bbe\u8ba1\u5bf9\u4e8e\u89e3\u51b3\u4e0a\u4e0b\u6587\u9690\u79c1\u95ee\u9898\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.06902", "pdf": "https://arxiv.org/pdf/2508.06902", "abs": "https://arxiv.org/abs/2508.06902", "authors": ["Xuecheng Wu", "Dingkang Yang", "Danlei Huang", "Xinyi Yin", "Yifan Wang", "Jia Zhang", "Jiayu Nie", "Liangyu Fu", "Yang Liu", "Junxiao Xue", "Hadi Amirpour", "Wei Zhou"], "title": "eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos", "categories": ["cs.CV"], "comment": null, "summary": "Short-form videos (SVs) have become a vital part of our online routine for\nacquiring and sharing information. Their multimodal complexity poses new\nchallenges for video analysis, highlighting the need for video emotion analysis\n(VEA) within the community. Given the limited availability of SVs emotion data,\nwe introduce eMotions, a large-scale dataset consisting of 27,996 videos with\nfull-scale annotations. To ensure quality and reduce subjective bias, we\nemphasize better personnel allocation and propose a multi-stage annotation\nprocedure. Additionally, we provide the category-balanced and test-oriented\nvariants through targeted sampling to meet diverse needs. While there have been\nsignificant studies on videos with clear emotional cues (e.g., facial\nexpressions), analyzing emotions in SVs remains a challenging task. The\nchallenge arises from the broader content diversity, which introduces more\ndistinct semantic gaps and complicates the representations learning of\nemotion-related features. Furthermore, the prevalence of audio-visual\nco-expressions in SVs leads to the local biases and collective information gaps\ncaused by the inconsistencies in emotional expressions. To tackle this, we\npropose AV-CANet, an end-to-end audio-visual fusion network that leverages\nvideo transformer to capture semantically relevant representations. We further\nintroduce the Local-Global Fusion Module designed to progressively capture the\ncorrelations of audio-visual features. Besides, EP-CE Loss is constructed to\nglobally steer optimizations with tripolar penalties. Extensive experiments\nacross three eMotions-related datasets and four public VEA datasets demonstrate\nthe effectiveness of our proposed AV-CANet, while providing broad insights for\nfuture research. Moreover, we conduct ablation studies to examine the critical\ncomponents of our method. Dataset and code will be made available at Github.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u77ed\u89c6\u9891\uff08SV\uff09\u60c5\u611f\u5206\u6790\uff08VEA\uff09\u7684\u6311\u6218\uff0c\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u60c5\u611f\u6570\u636e\u96c6eMotions\uff0c\u5e76\u63d0\u51fa\u4e86\u7aef\u5230\u7aef\u97f3\u89c6\u9891\u878d\u5408\u7f51\u7edcAV-CANet\uff0c\u4ee5\u6709\u6548\u5206\u6790SV\u4e2d\u7684\u60c5\u611f\u3002", "motivation": "\u77ed\u89c6\u9891\u5df2\u6210\u4e3a\u83b7\u53d6\u548c\u5206\u4eab\u4fe1\u606f\u7684\u91cd\u8981\u9014\u5f84\uff0c\u4f46\u5176\u591a\u6a21\u6001\u590d\u6742\u6027\u7ed9\u89c6\u9891\u5206\u6790\u5e26\u6765\u4e86\u65b0\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u60c5\u611f\u5206\u6790\u65b9\u9762\u3002\u76ee\u524d\u77ed\u89c6\u9891\u60c5\u611f\u6570\u636e\u7a00\u7f3a\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u77ed\u89c6\u9891\u5185\u5bb9\u591a\u6837\u6027\u3001\u8bed\u4e49\u9e3f\u6c9f\u4ee5\u53ca\u97f3\u89c6\u9891\u8868\u8fbe\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u5c40\u90e8\u504f\u5dee\u548c\u4fe1\u606f\u9e3f\u6c9f\u3002", "method": "1. \u6784\u5efa\u4e86\u5927\u89c4\u6a21\u77ed\u89c6\u9891\u60c5\u611f\u6570\u636e\u96c6eMotions\uff08\u5305\u542b27,996\u4e2a\u89c6\u9891\uff09\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u6807\u6ce8\u6d41\u7a0b\u548c\u4eba\u5458\u5206\u914d\u4f18\u5316\uff0c\u5e76\u63d0\u4f9b\u7c7b\u522b\u5747\u8861\u548c\u6d4b\u8bd5\u5bfc\u5411\u7684\u53d8\u4f53\u4ee5\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002\n2. \u63d0\u51fa\u4e86AV-CANet\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u97f3\u89c6\u9891\u878d\u5408\u7f51\u7edc\uff0c\u5229\u7528\u89c6\u9891Transformer\u6355\u83b7\u8bed\u4e49\u76f8\u5173\u8868\u793a\u3002\n3. \u8bbe\u8ba1\u4e86\u5c40\u90e8-\u5168\u5c40\u878d\u5408\u6a21\u5757\uff08Local-Global Fusion Module\uff09\u4ee5\u9010\u6b65\u6355\u6349\u97f3\u89c6\u9891\u7279\u5f81\u4e4b\u95f4\u7684\u5173\u8054\u3002\n4. \u6784\u5efa\u4e86EP-CE Loss\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u4e09\u6781\u60e9\u7f5a\u5168\u5c40\u5f15\u5bfc\u4f18\u5316\u3002", "result": "\u5728\u4e09\u4e2aeMotions\u76f8\u5173\u6570\u636e\u96c6\u548c\u56db\u4e2a\u516c\u5171VEA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684AV-CANet\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6d88\u878d\u7814\u7a76\u4e5f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u4e2d\u5173\u952e\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u89c1\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u7684\u77ed\u89c6\u9891\u60c5\u611f\u6570\u636e\u96c6\u548c\u63d0\u51fa\u521b\u65b0\u7684AV-CANet\u97f3\u89c6\u9891\u878d\u5408\u7f51\u7edc\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u77ed\u89c6\u9891\u60c5\u611f\u5206\u6790\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.07516", "pdf": "https://arxiv.org/pdf/2508.07516", "abs": "https://arxiv.org/abs/2508.07516", "authors": ["Keshav Varadarajan", "Tananun Songdechakraiwut"], "title": "Augmenting Bias Detection in LLMs Using Topological Data Analysis", "categories": ["cs.CL"], "comment": "15 pages, 9 figures, 4 tables", "summary": "Recently, many bias detection methods have been proposed to determine the\nlevel of bias a large language model captures. However, tests to identify which\nparts of a large language model are responsible for bias towards specific\ngroups remain underdeveloped. In this study, we present a method using\ntopological data analysis to identify which heads in GPT-2 contribute to the\nmisrepresentation of identity groups present in the StereoSet dataset. We find\nthat biases for particular categories, such as gender or profession, are\nconcentrated in attention heads that act as hot spots. The metric we propose\ncan also be used to determine which heads capture bias for a specific group\nwithin a bias category, and future work could extend this method to help\nde-bias large language models.", "AI": {"tldr": "\u901a\u8fc7\u62d3\u6251\u6570\u636e\u5206\u6790\uff0c\u672c\u7814\u7a76\u53d1\u73b0GPT-2\u4e2d\u7684\u504f\u89c1\u96c6\u4e2d\u5728\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u5934\u4e2d\uff0c\u4e3a\u6a21\u578b\u53bb\u504f\u89c1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u504f\u89c1\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u8bc6\u522b\u6a21\u578b\u5185\u90e8\u5bfc\u81f4\u504f\u89c1\u7684\u5177\u4f53\u90e8\u5206\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\uff0c\u7528\u4e8e\u8bc6\u522bGPT-2\u6a21\u578b\u4e2d\u5bf9StereoSet\u6570\u636e\u96c6\u4e2d\u8eab\u4efd\u7fa4\u4f53\u8bef\u8868\u793a\u505a\u51fa\u8d21\u732e\u7684\u6ce8\u610f\u529b\u5934\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9488\u5bf9\u7279\u5b9a\u7c7b\u522b\uff08\u5982\u6027\u522b\u3001\u804c\u4e1a\uff09\u7684\u504f\u89c1\u96c6\u4e2d\u5728\u5145\u5f53\u201c\u70ed\u70b9\u201d\u7684\u6ce8\u610f\u529b\u5934\u4e2d\uff1b\u63d0\u51fa\u7684\u5ea6\u91cf\u6807\u51c6\u8fd8\u80fd\u8bc6\u522b\u7279\u5b9a\u504f\u89c1\u7c7b\u522b\u4e2d\u67d0\u4e2a\u7fa4\u4f53\u7684\u504f\u89c1\u6e90\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9a\u4f4d\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u7684\u504f\u89c1\u6e90\uff0c\u4e3a\u672a\u6765\u8fdb\u4e00\u6b65\u6d88\u9664\u8fd9\u4e9b\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u63d0\u4f9b\u4e86\u6f5c\u5728\u9014\u5f84\u3002"}}
{"id": "2508.07102", "pdf": "https://arxiv.org/pdf/2508.07102", "abs": "https://arxiv.org/abs/2508.07102", "authors": ["Yang Cao", "Yubin Chen", "Zhao Song", "Jiahao Zhang"], "title": "Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Generative modelling has seen significant advances through simulation-free\nparadigms such as Flow Matching, and in particular, the MeanFlow framework,\nwhich replaces instantaneous velocity fields with average velocities to enable\nefficient single-step sampling. In this work, we introduce a theoretical study\non Second-Order MeanFlow, a novel extension that incorporates average\nacceleration fields into the MeanFlow objective. We first establish the\nfeasibility of our approach by proving that the average acceleration satisfies\na generalized consistency condition analogous to first-order MeanFlow, thereby\nsupporting stable, one-step sampling and tractable loss functions. We then\ncharacterize its expressivity via circuit complexity analysis, showing that\nunder mild assumptions, the Second-Order MeanFlow sampling process can be\nimplemented by uniform threshold circuits within the $\\mathsf{TC}^0$ class.\nFinally, we derive provably efficient criteria for scalable implementation by\nleveraging fast approximate attention computations: we prove that attention\noperations within the Second-Order MeanFlow architecture can be approximated to\nwithin $1/\\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results\nlay the theoretical foundation for high-order flow matching models that combine\nrich dynamics with practical sampling efficiency.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07671", "pdf": "https://arxiv.org/pdf/2508.07671", "abs": "https://arxiv.org/abs/2508.07671", "authors": ["Mohamed Rayan Barhdadi", "Mehmet Tuncel", "Erchin Serpedin", "Hasan Kurban"], "title": "EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MA", "stat.AP", "68T07, 68T42, 68T50, 91F20, 62P25", "I.2.11; I.2.1; H.1.2; J.4; K.4.2"], "comment": "19 pages, 3 figures (plus 6 figures in supplementary), 2 tables, 1\n  algorithm. Submitted to NeurIPS 2025 Creative AI Track: Humanity", "summary": "Current AI approaches to refugee integration optimize narrow objectives such\nas employment and fail to capture the cultural, emotional, and ethical\ndimensions critical for long-term success. We introduce EMPATHIA (Enriched\nMultimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),\na multi-agent framework addressing the central Creative AI question: how do we\npreserve human dignity when machines participate in life-altering decisions?\nGrounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes\nintegration into three modules: SEED (Socio-cultural Entry and Embedding\nDecision) for initial placement, RISE (Rapid Integration and Self-sufficiency\nEngine) for early independence, and THRIVE (Transcultural Harmony and\nResilience through Integrated Values and Engagement) for sustained outcomes.\nSEED employs a selector-validator architecture with three specialized agents -\nemotional, cultural, and ethical - that deliberate transparently to produce\ninterpretable recommendations. Experiments on the UN Kakuma dataset (15,026\nindividuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and\nimplementation on 6,359 working-age refugees (15+) with 150+ socioeconomic\nvariables achieved 87.4% validation convergence and explainable assessments\nacross five host countries. EMPATHIA's weighted integration of cultural,\nemotional, and ethical factors balances competing value systems while\nsupporting practitioner-AI collaboration. By augmenting rather than replacing\nhuman expertise, EMPATHIA provides a generalizable framework for AI-driven\nallocation tasks where multiple values must be reconciled.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EMPATHIA\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u6574\u5408\u6587\u5316\u3001\u60c5\u611f\u548c\u4f26\u7406\u56e0\u7d20\uff0c\u63d0\u5347AI\u5728\u96be\u6c11\u5b89\u7f6e\u51b3\u7b56\u4e2d\u7684\u4eba\u6027\u5316\u4e0e\u5c0a\u4e25\u4fdd\u7559\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u53d6\u5f97\u4e86\u53ef\u89e3\u91ca\u7684\u6210\u679c\u3002", "motivation": "\u73b0\u6709AI\u5728\u96be\u6c11\u878d\u5408\u4e2d\u4ec5\u5173\u6ce8\u5c31\u4e1a\u7b49\u72ed\u9698\u76ee\u6807\uff0c\u672a\u80fd\u6355\u6349\u6587\u5316\u3001\u60c5\u611f\u548c\u4f26\u7406\u7b49\u5173\u952e\u7ef4\u5ea6\u3002\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u201c\u673a\u5668\u53c2\u4e0e\u6539\u53d8\u751f\u6d3b\u7684\u51b3\u7b56\u65f6\u5982\u4f55\u4fdd\u7559\u4eba\u7c7b\u5c0a\u4e25\u201d\u8fd9\u4e00\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u5f15\u5165EMPATHIA\u6846\u67b6\uff0c\u57fa\u4e8eKegan\u7684\u5efa\u6784\u53d1\u5c55\u7406\u8bba\uff0c\u5305\u542bSEED\uff08\u521d\u59cb\u5b89\u7f6e\uff09\u3001RISE\uff08\u65e9\u671f\u72ec\u7acb\uff09\u548cTHRIVE\uff08\u6301\u7eed\u7ed3\u679c\uff09\u4e09\u4e2a\u6a21\u5757\u3002SEED\u91c7\u7528\u9009\u62e9\u5668-\u9a8c\u8bc1\u5668\u67b6\u6784\uff0c\u7531\u60c5\u611f\u3001\u6587\u5316\u548c\u4f26\u7406\u4e09\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u8fdb\u884c\u900f\u660e\u548c\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u3002", "result": "\u5728\u8054\u5408\u56fd\u5361\u5e93\u9a6c\u6570\u636e\u96c6\u4e0a\u5bf96,359\u540d\u96be\u6c11\u8fdb\u884c\u5b9e\u65bd\uff0c\u53d6\u5f97\u4e8687.4%\u7684\u9a8c\u8bc1\u6536\u655b\u7387\uff0c\u80fd\u5728\u4e94\u4e2a\u63a5\u6536\u56fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u3002EMPATHIA\u901a\u8fc7\u52a0\u6743\u6574\u5408\u6587\u5316\u3001\u60c5\u611f\u548c\u4f26\u7406\u56e0\u7d20\uff0c\u5e73\u8861\u4e86\u7ade\u4e89\u6027\u4ef7\u503c\u4f53\u7cfb\uff0c\u5e76\u652f\u6301\u4e86\u5b9e\u8df5\u8005\u4e0eAI\u7684\u534f\u4f5c\u3002", "conclusion": "EMPATHIA\u901a\u8fc7\u589e\u5f3a\u800c\u975e\u53d6\u4ee3\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684AI\u9a71\u52a8\u5206\u914d\u4efb\u52a1\u6846\u67b6\uff0c\u53ef\u5728\u9700\u8981\u534f\u8c03\u591a\u79cd\u4ef7\u503c\u89c2\u7684\u573a\u666f\u4e2d\u5e94\u7528\uff0c\u6709\u52a9\u4e8e\u4fdd\u7559AI\u53c2\u4e0e\u51b3\u7b56\u65f6\u7684\u4eba\u7c7b\u5c0a\u4e25\u3002"}}
{"id": "2508.06904", "pdf": "https://arxiv.org/pdf/2508.06904", "abs": "https://arxiv.org/abs/2508.06904", "authors": ["Chao Yin", "Jide Li", "Xiaoqiang Li"], "title": "A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation", "categories": ["cs.CV"], "comment": "under review", "summary": "Camouflaged Object Segmentation (COS) remains highly challenging due to the\nintrinsic visual similarity between target objects and their surroundings.\nWhile training-based COS methods achieve good performance, their performance\ndegrades rapidly with increased annotation sparsity. To circumvent this\nlimitation, recent studies have explored training-free COS methods, leveraging\nthe Segment Anything Model (SAM) by automatically generating visual prompts\nfrom a single task-generic prompt (\\textit{e.g.}, \"\\textit{camouflaged\nanimal}\") uniformly applied across all test images. However, these methods\ntypically produce only semantic-level visual prompts, causing SAM to output\ncoarse semantic masks and thus failing to handle scenarios with multiple\ndiscrete camouflaged instances effectively. To address this critical\nlimitation, we propose a simple yet powerful \\textbf{I}nstance-\\textbf{A}ware\n\\textbf{P}rompting \\textbf{F}ramework (IAPF), the first training-free COS\npipeline that explicitly converts a task-generic prompt into fine-grained\ninstance masks. Specifically, the IAPF comprises three steps: (1) Text Prompt\nGenerator, utilizing task-generic queries to prompt a Multimodal Large Language\nModel (MLLM) for generating image-specific foreground and background tags; (2)\n\\textbf{Instance Mask Generator}, leveraging Grounding DINO to produce precise\ninstance-level bounding box prompts, alongside the proposed Single-Foreground\nMulti-Background Prompting strategy to sample region-constrained point prompts\nwithin each box, enabling SAM to yield a candidate instance mask; (3)\nSelf-consistency Instance Mask Voting, which selects the final COS prediction\nby identifying the candidate mask most consistent across multiple candidate\ninstance masks. Extensive evaluations on standard COS benchmarks demonstrate\nthat the proposed IAPF significantly surpasses existing state-of-the-art\ntraining-free COS methods.", "AI": {"tldr": "\u9488\u5bf9\u65e0\u8bad\u7ec3\u4f2a\u88c5\u76ee\u6807\u5206\u5272\uff08COS\uff09\u4e2d\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u7c97\u7cd9\u8bed\u4e49\u63a9\u7801\u4e14\u96be\u4ee5\u5904\u7406\u591a\u5b9e\u4f8b\u7684\u5c40\u9650\u6027\uff0c\u672c\u6587\u63d0\u51faIAPF\u6846\u67b6\u3002\u8be5\u6846\u67b6\u662f\u9996\u4e2a\u5c06\u4efb\u52a1\u901a\u7528\u63d0\u793a\u8f6c\u6362\u4e3a\u7ec6\u7c92\u5ea6\u5b9e\u4f8b\u63a9\u7801\u7684\u65e0\u8bad\u7ec3COS\u6d41\u7a0b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u3001Grounding DINO\u548c\u81ea\u6d3d\u6295\u7968\u7b56\u7565\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65e0\u8bad\u7ec3COS\u65b9\u6cd5\u3002", "motivation": "\u4f2a\u88c5\u76ee\u6807\u5206\u5272\uff08COS\uff09\u56e0\u76ee\u6807\u4e0e\u5468\u56f4\u73af\u5883\u7684\u9ad8\u5ea6\u89c6\u89c9\u76f8\u4f3c\u6027\u800c\u6781\u5177\u6311\u6218\u3002\u5c3d\u7ba1\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u6027\u80fd\u968f\u6807\u6ce8\u7a00\u758f\u6027\u589e\u52a0\u800c\u8fc5\u901f\u4e0b\u964d\u3002\u4e3a\u89e3\u51b3\u6b64\u9650\uff0c\u8fd1\u671f\u65e0\u8bad\u7ec3COS\u65b9\u6cd5\uff08\u5229\u7528SAM\uff09\u5174\u8d77\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u4ec5\u751f\u6210\u8bed\u4e49\u7ea7\u89c6\u89c9\u63d0\u793a\uff0c\u5bfc\u81f4SAM\u8f93\u51fa\u7c97\u7cd9\u8bed\u4e49\u63a9\u7801\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u5904\u7406\u591a\u4e2a\u79bb\u6563\u4f2a\u88c5\u5b9e\u4f8b\u7684\u60c5\u51b5\u3002\u8fd9\u662f\u5f53\u524d\u4e9f\u5f85\u89e3\u51b3\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aIAPF\uff08Instance-Aware Prompting Framework\uff09\u7684\u7b80\u5355\u800c\u5f3a\u5927\u7684\u65e0\u8bad\u7ec3COS\u6846\u67b6\uff0c\u5176\u76ee\u6807\u662f\u5c06\u4efb\u52a1\u901a\u7528\u63d0\u793a\u663e\u5f0f\u8f6c\u6362\u4e3a\u7ec6\u7c92\u5ea6\u5b9e\u4f8b\u63a9\u7801\u3002IAPF\u5305\u542b\u4e09\u4e2a\u6b65\u9aa4\uff1a1. **\u6587\u672c\u63d0\u793a\u751f\u6210\u5668**\uff1a\u5229\u7528\u4efb\u52a1\u901a\u7528\u67e5\u8be2\u63d0\u793a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u751f\u6210\u56fe\u50cf\u7279\u5b9a\u7684\u524d\u666f\u548c\u80cc\u666f\u6807\u7b7e\u30022. **\u5b9e\u4f8b\u63a9\u7801\u751f\u6210\u5668**\uff1a\u7ed3\u5408Grounding DINO\u751f\u6210\u7cbe\u786e\u7684\u5b9e\u4f8b\u7ea7\u8fb9\u754c\u6846\u63d0\u793a\uff0c\u5e76\u91c7\u7528\u63d0\u51fa\u7684\u201c\u5355\u524d\u666f\u591a\u80cc\u666f\u63d0\u793a\u7b56\u7565\u201d\u5728\u6bcf\u4e2a\u6846\u5185\u91c7\u6837\u533a\u57df\u53d7\u9650\u7684\u70b9\u63d0\u793a\uff0c\u4ee5\u4fc3\u4f7fSAM\u751f\u6210\u5019\u9009\u5b9e\u4f8b\u63a9\u7801\u30023. **\u81ea\u6d3d\u5b9e\u4f8b\u63a9\u7801\u6295\u7968**\uff1a\u901a\u8fc7\u8bc6\u522b\u5728\u591a\u4e2a\u5019\u9009\u5b9e\u4f8b\u63a9\u7801\u4e2d\u6700\u5177\u4e00\u81f4\u6027\u7684\u4e00\u4e2a\uff0c\u6765\u9009\u62e9\u6700\u7ec8\u7684COS\u9884\u6d4b\u3002", "result": "\u5728\u6807\u51c6COS\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684IAPF\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65e0\u8bad\u7ec3COS\u65b9\u6cd5\u3002", "conclusion": "IAPF\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u65e0\u8bad\u7ec3COS\u65b9\u6cd5\u5728\u5904\u7406\u591a\u5b9e\u4f8b\u548c\u751f\u6210\u7ec6\u7c92\u5ea6\u63a9\u7801\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5b9e\u4f8b\u611f\u77e5\u63d0\u793a\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u4f2a\u88c5\u76ee\u6807\u5206\u5272\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07517", "pdf": "https://arxiv.org/pdf/2508.07517", "abs": "https://arxiv.org/abs/2508.07517", "authors": ["Joseph T. Colonel", "Baihan Lin"], "title": "Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Word clouds are a common way to summarize qualitative interviews, yet\ntraditional frequency-based methods often fail in conversational contexts: they\nsurface filler words, ignore paraphrase, and fragment semantically related\nideas. This limits their usefulness in early-stage analysis, when researchers\nneed fast, interpretable overviews of what participant actually said. We\nintroduce ThemeClouds, an open-source visualization tool that uses large\nlanguage models (LLMs) to generate thematic, participant-weighted word clouds\nfrom dialogue transcripts. The system prompts an LLM to identify concept-level\nthemes across a corpus and then counts how many unique participants mention\neach topic, yielding a visualization grounded in breadth of mention rather than\nraw term frequency. Researchers can customize prompts and visualization\nparameters, providing transparency and control. Using interviews from a user\nstudy comparing five recording-device configurations (31 participants; 155\ntranscripts, Whisper ASR), our approach surfaces more actionable device\nconcerns than frequency clouds and topic-modeling baselines (e.g., LDA,\nBERTopic). We discuss design trade-offs for integrating LLM assistance into\nqualitative workflows, implications for interpretability and researcher agency,\nand opportunities for interactive analyses such as per-condition contrasts\n(``diff clouds'').", "AI": {"tldr": "\u672c\u6587\u63d0\u51faThemeClouds\uff0c\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u4ece\u5bf9\u8bdd\u6587\u672c\u751f\u6210\u4e3b\u9898\u5f0f\u3001\u53c2\u4e0e\u8005\u52a0\u6743\u7684\u8bcd\u4e91\u5de5\u5177\uff0c\u65e8\u5728\u514b\u670d\u4f20\u7edf\u8bcd\u4e91\u5728\u5b9a\u6027\u8bbf\u8c08\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u80fd\u8bc6\u522b\u51fa\u66f4\u5177\u53ef\u64cd\u4f5c\u6027\u7684\u89c1\u89e3\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u9891\u7387\u7684\u8bcd\u4e91\u5728\u5206\u6790\u5bf9\u8bdd\u578b\u5b9a\u6027\u8bbf\u8c08\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u7a81\u51fa\u586b\u5145\u8bcd\u3001\u5ffd\u7565\u610f\u8bd1\u3001\u5206\u5272\u8bed\u4e49\u76f8\u5173\u6982\u5ff5\uff0c\u5bfc\u81f4\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u5feb\u901f\u83b7\u53d6\u53ef\u89e3\u91ca\u6027\u7684\u65e9\u671f\u5206\u6790\u6982\u89c8\u3002", "method": "\u5f15\u5165\u5f00\u6e90\u5de5\u5177ThemeClouds\uff0c\u8be5\u5de5\u5177\u5229\u7528LLM\u8bc6\u522b\u5bf9\u8bdd\u6587\u672c\u4e2d\u7684\u6982\u5ff5\u7ea7\u4e3b\u9898\uff0c\u5e76\u57fa\u4e8e\u63d0\u53ca\u6bcf\u4e2a\u4e3b\u9898\u7684\u72ec\u7279\u53c2\u4e0e\u8005\u6570\u91cf\u8fdb\u884c\u52a0\u6743\uff08\u800c\u975e\u539f\u59cb\u8bcd\u9891\uff09\u751f\u6210\u8bcd\u4e91\u3002\u7814\u7a76\u4eba\u5458\u53ef\u81ea\u5b9a\u4e49\u63d0\u793a\u548c\u53ef\u89c6\u5316\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u5bf9\u4e00\u9879\u7528\u6237\u7814\u7a76\u7684\u8bbf\u8c08\u6570\u636e\u5206\u6790\uff0cThemeClouds\u6bd4\u4f20\u7edf\u7684\u9891\u7387\u8bcd\u4e91\u548c\u4e3b\u9898\u6a21\u578b\u57fa\u7ebf\uff08\u5982LDA\u3001BERTopic\uff09\u80fd\u53d1\u73b0\u66f4\u591a\u53ef\u64cd\u4f5c\u7684\u8bbe\u5907\u76f8\u5173\u95ee\u9898\u3002", "conclusion": "ThemeClouds\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684LLM\u8f85\u52a9\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86\u5b9a\u6027\u6570\u636e\u5206\u6790\u5de5\u4f5c\u6d41\uff0c\u63d0\u5347\u4e86\u7814\u7a76\u4eba\u5458\u7684\u6d1e\u5bdf\u80fd\u529b\uff0c\u5e76\u4e3a\u4ea4\u4e92\u5f0f\u5206\u6790\uff08\u5982\u6761\u4ef6\u5bf9\u6bd4\u201c\u5dee\u5f02\u8bcd\u4e91\u201d\uff09\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.07106", "pdf": "https://arxiv.org/pdf/2508.07106", "abs": "https://arxiv.org/abs/2508.07106", "authors": ["Yiran Huang", "Amirhossein Nouranizadeh", "Christine Ahrends", "Mengjia Xu"], "title": "BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation", "categories": ["cs.LG"], "comment": null, "summary": "Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widely\nused to study human brain activity. fMRI signals in areas across the brain\ntransiently synchronise and desynchronise their activity in a highly structured\nmanner, even when an individual is at rest. These functional connectivity\ndynamics may be related to behaviour and neuropsychiatric disease. To model\nthese dynamics, temporal brain connectivity representations are essential, as\nthey reflect evolving interactions between brain regions and provide insight\ninto transient neural states and network reconfigurations. However,\nconventional graph neural networks (GNNs) often struggle to capture long-range\ntemporal dependencies in dynamic fMRI data. To address this challenge, we\npropose BrainATCL, an unsupervised, nonparametric framework for adaptive\ntemporal brain connectivity learning, enabling functional link prediction and\nage estimation. Our method dynamically adjusts the lookback window for each\nsnapshot based on the rate of newly added edges. Graph sequences are\nsubsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporal\nrepresentations of dynamic functional connectivity in resting-state fMRI data\nof 1,000 participants from the Human Connectome Project. To further improve\nspatial modeling, we incorporate brain structure and function-informed edge\nattributes, i.e., the left/right hemispheric identity and subnetwork membership\nof brain regions, enabling the model to capture biologically meaningful\ntopological patterns. We evaluate our BrainATCL on two tasks: functional link\nprediction and age estimation. The experimental results demonstrate superior\nperformance and strong generalization, including in cross-session prediction\nscenarios.", "AI": {"tldr": "\u9488\u5bf9fMRI\u6570\u636e\u4e2d\u8111\u529f\u80fd\u8fde\u63a5\u52a8\u6001\u7684\u5efa\u6a21\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86BrainATCL\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u65f6\u95f4\u7a97\u548cGINE-Mamba2\u9aa8\u5e72\u7f51\u7edc\u6355\u83b7\u957f\u7a0b\u65f6\u7a7a\u4f9d\u8d56\uff0c\u5e76\u7ed3\u5408\u751f\u7269\u5b66\u5c5e\u6027\uff0c\u5728\u8fde\u63a5\u9884\u6d4b\u548c\u5e74\u9f84\u4f30\u8ba1\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u529f\u80fd\u78c1\u5171\u632f\u6210\u50cf\uff08fMRI\uff09\u4fe1\u53f7\u5728\u4e0d\u540c\u8111\u533a\u4e4b\u95f4\u5b58\u5728\u9ad8\u5ea6\u7ed3\u6784\u5316\u7684\u77ac\u65f6\u540c\u6b65\u4e0e\u53bb\u540c\u6b65\u73b0\u8c61\uff0c\u8fd9\u4e9b\u529f\u80fd\u8fde\u63a5\u52a8\u6001\u53ef\u80fd\u4e0e\u884c\u4e3a\u548c\u795e\u7ecf\u7cbe\u795e\u75be\u75c5\u76f8\u5173\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u6355\u6349\u52a8\u6001fMRI\u6570\u636e\u4e2d\u7684\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86BrainATCL\uff0c\u4e00\u4e2a\u65e0\u76d1\u7763\u3001\u975e\u53c2\u6570\u7684\u81ea\u9002\u5e94\u65f6\u95f4\u8111\u8fde\u63a5\u5b66\u4e60\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u6839\u636e\u65b0\u589e\u8fb9\u7684\u901f\u7387\u52a8\u6001\u8c03\u6574\u6bcf\u4e2a\u5feb\u7167\u7684\u56de\u6eaf\u7a97\u53e3\uff0c\u5e76\u4f7f\u7528GINE-Mamba2\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\u6765\u7f16\u7801\u56fe\u5e8f\u5217\uff0c\u4ee5\u5b66\u4e60\u52a8\u6001\u529f\u80fd\u8fde\u63a5\u7684\u65f6\u7a7a\u8868\u793a\u3002\u4e3a\u589e\u5f3a\u7a7a\u95f4\u5efa\u6a21\u80fd\u529b\uff0c\u6a21\u578b\u8fd8\u878d\u5165\u4e86\u8111\u7ed3\u6784\u548c\u529f\u80fd\u76f8\u5173\u7684\u8fb9\u5c5e\u6027\uff08\u5982\u5de6\u53f3\u534a\u7403\u8eab\u4efd\u548c\u5b50\u7f51\u7edc\u6210\u5458\uff09\u3002\u7814\u7a76\u4f7f\u7528\u4e86\u6765\u81ea\u4eba\u7c7b\u8fde\u63a5\u7ec4\u8ba1\u5212\u76841,000\u540d\u53c2\u4e0e\u8005\u7684\u9759\u606f\u6001fMRI\u6570\u636e\u3002", "result": "BrainATCL\u5728\u529f\u80fd\u8fde\u63a5\u9884\u6d4b\u548c\u5e74\u9f84\u4f30\u8ba1\u4e24\u9879\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ec\u5728\u8de8\u4f1a\u8bdd\u9884\u6d4b\u573a\u666f\u4e2d\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "BrainATCL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001fMRI\u6570\u636e\u4e2d\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u6027\u5efa\u6a21\u7684\u96be\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u81ea\u9002\u5e94\u65f6\u95f4\u7a97\u548c\u7ed3\u5408\u751f\u7269\u5b66\u5c5e\u6027\u7684\u65b9\u6cd5\uff0c\u5728\u529f\u80fd\u8fde\u63a5\u9884\u6d4b\u548c\u5e74\u9f84\u4f30\u8ba1\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4e3a\u6df1\u5165\u7406\u89e3\u5927\u8111\u77ac\u65f6\u795e\u7ecf\u72b6\u6001\u548c\u7f51\u7edc\u91cd\u6784\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002"}}
{"id": "2508.07673", "pdf": "https://arxiv.org/pdf/2508.07673", "abs": "https://arxiv.org/abs/2508.07673", "authors": ["Gianluca Bontempi"], "title": "Ethics2vec: aligning automatic agents and human preferences", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Though intelligent agents are supposed to improve human experience (or make\nit more efficient), it is hard from a human perspective to grasp the ethical\nvalues which are explicitly or implicitly embedded in an agent behaviour. This\nis the well-known problem of alignment, which refers to the challenge of\ndesigning AI systems that align with human values, goals and preferences. This\nproblem is particularly challenging since most human ethical considerations\nrefer to \\emph{incommensurable} (i.e. non-measurable and/or incomparable)\nvalues and criteria. Consider, for instance, a medical agent prescribing a\ntreatment to a cancerous patient. How could it take into account (and/or weigh)\nincommensurable aspects like the value of a human life and the cost of the\ntreatment? Now, the alignment between human and artificial values is possible\nonly if we define a common space where a metric can be defined and used. This\npaper proposes to extend to ethics the conventional Anything2vec approach,\nwhich has been successful in plenty of similar and hard-to-quantify domains\n(ranging from natural language processing to recommendation systems and graph\nanalysis). This paper proposes a way to map an automatic agent decision-making\n(or control law) strategy to a multivariate vector representation, which can be\nused to compare and assess the alignment with human values. The Ethics2Vec\nmethod is first introduced in the case of an automatic agent performing binary\ndecision-making. Then, a vectorisation of an automatic control law (like in the\ncase of a self-driving car) is discussed to show how the approach can be\nextended to automatic control settings.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u4eba\u5de5\u667a\u80fd\u4f26\u7406\u5bf9\u9f50\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51faEthics2Vec\u65b9\u6cd5\uff0c\u5c06\u667a\u80fd\u4f53\u884c\u4e3a\u7b56\u7565\u6620\u5c04\u4e3a\u53ef\u91cf\u5316\u7684\u5411\u91cf\u8868\u793a\uff0c\u4ece\u800c\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u6bd4\u8f83\u548c\u5bf9\u9f50\u8bc4\u4f30\u3002", "motivation": "\u667a\u80fd\u4f53\u884c\u4e3a\u4e2d\u9690\u542b\u6216\u663e\u5f0f\u7684\u4f26\u7406\u4ef7\u503c\u96be\u4ee5\u88ab\u4eba\u7c7b\u7406\u89e3\uff08\u5373\u5bf9\u9f50\u95ee\u9898\uff09\uff0c\u5c24\u5176\u56e0\u4e3a\u4eba\u7c7b\u4f26\u7406\u8003\u91cf\u5e38\u6d89\u53ca\u4e0d\u53ef\u901a\u7ea6\u7684\u4ef7\u503c\u3002\u4e3a\u4e86\u5b9e\u73b0\u4eba\u7c7b\u4e0e\u4eba\u5de5\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\uff0c\u9700\u8981\u5b9a\u4e49\u4e00\u4e2a\u53ef\u4ee5\u8fdb\u884c\u5ea6\u91cf\u7684\u5171\u540c\u7a7a\u95f4\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51faEthics2Vec\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u4f20\u7edf\u7684\u201cAnything2vec\u201d\u8303\u5f0f\u6269\u5c55\u5230\u4f26\u7406\u9886\u57df\u3002\u5b83\u5c06\u81ea\u52a8\u667a\u80fd\u4f53\u7684\u51b3\u7b56\u5236\u5b9a\u6216\u63a7\u5236\u5f8b\u7b56\u7565\u6620\u5c04\u4e3a\u591a\u5143\u5411\u91cf\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u5e94\u7528\u4e8e\u4e8c\u5143\u51b3\u7b56\u60c5\u5883\uff0c\u968f\u540e\u63a2\u8ba8\u4e86\u5982\u4f55\u6269\u5c55\u5230\u81ea\u52a8\u63a7\u5236\u8bbe\u7f6e\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u63a7\u5236\u5f8b\u5411\u91cf\u5316\uff09\u3002", "result": "\u901a\u8fc7\u5c06\u667a\u80fd\u4f53\u884c\u4e3a\u7b56\u7565\u6620\u5c04\u4e3a\u591a\u5143\u5411\u91cf\uff0c\u8be5\u8868\u793a\u53ef\u7528\u4e8e\u6bd4\u8f83\u548c\u8bc4\u4f30\u667a\u80fd\u4f53\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "conclusion": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4f26\u7406\u8003\u91cf\u5411\u91cf\u5316\uff0c\u4e3a\u91cf\u5316\u548c\u8bc4\u4f30AI\u4f26\u7406\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u7684\u9014\u5f84\uff0c\u6709\u671b\u514b\u670d\u4f26\u7406\u4ef7\u503c\u4e0d\u53ef\u901a\u7ea6\u7684\u6311\u6218\uff0c\u4fc3\u8fdb\u667a\u80fd\u4f53\u884c\u4e3a\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u3002"}}
{"id": "2508.06905", "pdf": "https://arxiv.org/pdf/2508.06905", "abs": "https://arxiv.org/abs/2508.06905", "authors": ["Ruoxi Chen", "Dongping Chen", "Siyuan Wu", "Sinan Wang", "Shiyun Lang", "Petr Sushko", "Gaoyang Jiang", "Yao Wan", "Ranjay Krishna"], "title": "MultiRef: Controllable Image Generation with Multiple Visual References", "categories": ["cs.CV"], "comment": "Accepted to ACM MM 2025 Datasets", "summary": "Visual designers naturally draw inspiration from multiple visual references,\ncombining diverse elements and aesthetic principles to create artwork. However,\ncurrent image generative frameworks predominantly rely on single-source inputs\n-- either text prompts or individual reference images. In this paper, we focus\non the task of controllable image generation using multiple visual references.\nWe introduce MultiRef-bench, a rigorous evaluation framework comprising 990\nsynthetic and 1,000 real-world samples that require incorporating visual\ncontent from multiple reference images. The synthetic samples are synthetically\ngenerated through our data engine RefBlend, with 10 reference types and 33\nreference combinations. Based on RefBlend, we further construct a dataset\nMultiRef containing 38k high-quality images to facilitate further research. Our\nexperiments across three interleaved image-text models (i.e., OmniGen, ACE, and\nShow-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that\neven state-of-the-art systems struggle with multi-reference conditioning, with\nthe best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in\nreal-world cases on average compared to the golden answer. These findings\nprovide valuable directions for developing more flexible and human-like\ncreative tools that can effectively integrate multiple sources of visual\ninspiration. The dataset is publicly available at: https://multiref.github.io/.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86MultiRef-bench\u8bc4\u4f30\u6846\u67b6\u548cMultiRef\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u56fe\u50cf\u751f\u6210\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u5904\u7406\u591a\u89c6\u89c9\u53c2\u8003\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u96be\u4ee5\u6574\u5408\u591a\u6e90\u89c6\u89c9\u7075\u611f\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u7c7b\u4eba\u3001\u66f4\u7075\u6d3b\u7684\u521b\u610f\u5de5\u5177\u6307\u660e\u4e86\u65b9\u5411\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u751f\u6210\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u5355\u4e00\u8f93\u5165\u6e90\uff08\u5982\u6587\u672c\u6216\u5355\u4e2a\u53c2\u8003\u56fe\u50cf\uff09\uff0c\u4e0e\u4eba\u7c7b\u8bbe\u8ba1\u5e08\u4ece\u591a\u4e2a\u89c6\u89c9\u53c2\u8003\u4e2d\u6c72\u53d6\u7075\u611f\u7684\u81ea\u7136\u65b9\u5f0f\u4e0d\u7b26\u3002\u73b0\u6709\u6280\u672f\u5728\u6574\u5408\u591a\u6e90\u89c6\u89c9\u4fe1\u606f\u4ee5\u8fdb\u884c\u53ef\u63a7\u56fe\u50cf\u751f\u6210\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u4e86MultiRef-bench\uff0c\u4e00\u4e2a\u5305\u542b990\u4e2a\u5408\u6210\u6837\u672c\u548c1000\u4e2a\u771f\u5b9e\u4e16\u754c\u6837\u672c\u7684\u4e25\u683c\u8bc4\u4f30\u6846\u67b6\u3002\u5f00\u53d1\u4e86\u6570\u636e\u5f15\u64ceRefBlend\u7528\u4e8e\u751f\u6210\u5408\u6210\u6837\u672c\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efa\u4e86\u5305\u542b38k\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684MultiRef\u6570\u636e\u96c6\u3002\u5bf9\u4e09\u79cd\u4ea4\u9519\u56fe\u50cf-\u6587\u672c\u6a21\u578b\uff08OmniGen\u3001ACE\u3001Show-o\uff09\u548c\u516d\u79cd\u4ee3\u7406\u6846\u67b6\uff08\u5982ChatDiT\u3001LLM + SD\uff09\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\u4e5f\u96be\u4ee5\u6709\u6548\u5904\u7406\u591a\u53c2\u8003\u6761\u4ef6\u751f\u6210\u3002\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578bOmniGen\uff0c\u5728\u5408\u6210\u6837\u672c\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u4ec5\u4e3a66.6%\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6837\u672c\u4e0a\u4e3a79.0%\uff0c\u4e0e\u201c\u9ec4\u91d1\u7b54\u6848\u201d\u76f8\u6bd4\u4ecd\u6709\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u5f53\u524d\u751f\u6210\u6a21\u578b\u5728\u6574\u5408\u591a\u6e90\u89c6\u89c9\u7075\u611f\u80fd\u529b\u65b9\u9762\u7684\u663e\u8457\u4e0d\u8db3\uff0c\u4e3a\u5f00\u53d1\u66f4\u7075\u6d3b\u3001\u66f4\u50cf\u4eba\u7c7b\u7684\u521b\u610f\u5de5\u5177\u6307\u660e\u4e86\u6709\u4ef7\u503c\u7684\u65b9\u5411\u3002\u672c\u7814\u7a76\u53d1\u5e03\u7684\u516c\u5f00\u6570\u636e\u96c6\u4e5f\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u53d1\u5c55\u3002"}}
{"id": "2508.07534", "pdf": "https://arxiv.org/pdf/2508.07534", "abs": "https://arxiv.org/abs/2508.07534", "authors": ["Jia Deng", "Jie Chen", "Zhipeng Chen", "Daixuan Cheng", "Fei Bai", "Beichen Zhang", "Yinqian Min", "Yanzipeng Gao", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR", "categories": ["cs.CL"], "comment": "27pages,25figures. arXiv admin note: text overlap with\n  arXiv:2508.02260", "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based\nfeedback to guide LLMs in generating and refining complex reasoning chains -- a\nprocess critically dependent on effective exploration strategies. While prior\nwork has demonstrated RLVR's empirical success, the fundamental mechanisms\ngoverning LLMs' exploration behaviors remain underexplored. This technical\nreport presents a systematic investigation of exploration capacities in RLVR,\ncovering four main aspects: (1) exploration space shaping, where we develop\nquantitative metrics to characterize LLMs' capability boundaries; (2)\nentropy-performance exchange, analyzed across training stages, individual\ninstances, and token-level patterns; and (3) RL performance optimization,\nexamining methods to effectively translate exploration gains into measurable\nimprovements. By unifying previously identified insights with new empirical\nevidence, this work aims to provide a foundational framework for advancing RLVR\nsystems.", "AI": {"tldr": "\u672c\u62a5\u544a\u7cfb\u7edf\u8c03\u67e5\u4e86\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u901a\u8fc7\u5206\u6790\u63a2\u7d22\u7a7a\u95f4\u3001\u71b5-\u6027\u80fd\u4ea4\u6362\u548c\u6027\u80fd\u4f18\u5316\u65b9\u6cd5\uff0c\u65e8\u5728\u4e3a\u63a8\u8fdbRLVR\u7cfb\u7edf\u63d0\u4f9b\u4e00\u4e2a\u57fa\u7840\u6846\u67b6\u3002", "motivation": "\u5c3d\u7ba1RLVR\u5728\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5df2\u53d6\u5f97\u7ecf\u9a8c\u6027\u6210\u529f\uff0c\u4f46LLM\u5728RLVR\u4e2d\u7684\u63a2\u7d22\u884c\u4e3a\u6240\u4f9d\u8d56\u7684\u57fa\u672c\u673a\u5236\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u672c\u7814\u7a76\u5bf9RLVR\u4e2d\u7684\u63a2\u7d22\u80fd\u529b\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8c03\u67e5\uff0c\u6db5\u76d6\u4e09\u4e2a\u4e3b\u8981\u65b9\u9762\uff1a1) \u63a2\u7d22\u7a7a\u95f4\u5851\u9020\uff0c\u901a\u8fc7\u5f00\u53d1\u91cf\u5316\u6307\u6807\u6765\u523b\u753bLLM\u7684\u80fd\u529b\u8fb9\u754c\uff1b2) \u71b5-\u6027\u80fd\u4ea4\u6362\u5206\u6790\uff0c\u8de8\u8bad\u7ec3\u9636\u6bb5\u3001\u4e2a\u4f53\u5b9e\u4f8b\u548ctoken\u7ea7\u6a21\u5f0f\u8fdb\u884c\uff1b3) RL\u6027\u80fd\u4f18\u5316\uff0c\u7814\u7a76\u5982\u4f55\u6709\u6548\u5c06\u63a2\u7d22\u6536\u76ca\u8f6c\u5316\u4e3a\u53ef\u8861\u91cf\u7684\u6539\u8fdb\u3002", "result": "\u672c\u62a5\u544a\u5448\u73b0\u4e86\u5bf9RLVR\u4e2dLLM\u63a2\u7d22\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u8c03\u67e5\uff0c\u5e76\u901a\u8fc7\u6574\u5408\u5148\u524d\u5df2\u8bc6\u522b\u7684\u89c1\u89e3\u4e0e\u65b0\u7684\u7ecf\u9a8c\u8bc1\u636e\uff0c\u5c55\u793a\u4e86\u5bf9\u63a2\u7d22\u673a\u5236\u7684\u6df1\u5165\u7406\u89e3\u3002", "conclusion": "\u672c\u5de5\u4f5c\u65e8\u5728\u901a\u8fc7\u5176\u7cfb\u7edf\u6027\u8c03\u67e5\u548c\u5bf9\u63a2\u7d22\u673a\u5236\u7684\u6df1\u5165\u5206\u6790\uff0c\u4e3a\u63a8\u8fdb\u672a\u6765\u7684RLVR\u7cfb\u7edf\u53d1\u5c55\u63d0\u4f9b\u4e00\u4e2a\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2508.07114", "pdf": "https://arxiv.org/pdf/2508.07114", "abs": "https://arxiv.org/abs/2508.07114", "authors": ["Atakan Azakli", "Bernd Stelzer"], "title": "Approaching Maximal Information Extraction in Low-Signal Regimes via Multiple Instance Learning", "categories": ["cs.LG", "hep-ex"], "comment": null, "summary": "In this work, we propose a new machine learning (ML) methodology to obtain\nmore precise predictions for some parameters of interest in a given hypotheses\ntesting problem. Our proposed method also allows ML models to have more\ndiscriminative power in cases where it is extremely challenging for\nstate-of-the-art classifiers to have any level of accurate predictions. This\nmethod can also allow us to systematically decrease the error from ML models in\ntheir predictions. In this paper, we provide a mathematical motivation why\nMultiple Instance Learning (MIL) would have more predictive power over their\nsingle-instance counterparts. We support our theoretical claims by analyzing\nthe behavior of the MIL models through their scaling behaviors with respect to\nthe number of instances on which the model makes predictions. As a concrete\napplication, we constrain Wilson coefficients of the Standard Model Effective\nField Theory (SMEFT) using kinematic information from subatomic particle\ncollision events at the Large Hadron Collider (LHC). We show that under certain\ncircumstances, it might be possible to extract the theoretical maximum Fisher\nInformation latent in a dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u591a\u793a\u4f8b\u5b66\u4e60\uff08MIL\uff09\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u5047\u8bbe\u68c0\u9a8c\u4e2d\u53c2\u6570\u9884\u6d4b\u7684\u7cbe\u5ea6\u548c\u5224\u522b\u529b\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u51cf\u5c11\u6a21\u578b\u8bef\u5dee\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5728\u5927\u578b\u5f3a\u5b50\u5bf9\u649e\u673a\uff08LHC\uff09\u6570\u636e\u4e0a\u5bf9\u6807\u51c6\u6a21\u578b\u6709\u6548\u573a\u8bba\uff08SMEFT\uff09\u5a01\u5c14\u900a\u7cfb\u6570\u7684\u7ea6\u675f\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5176\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u63d0\u53d6\u7406\u8bba\u6700\u5927\u8d39\u820d\u5c14\u4fe1\u606f\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u67d0\u4e9b\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\u4e2d\u96be\u4ee5\u63d0\u4f9b\u7cbe\u786e\u9884\u6d4b\uff0c\u4e14\u5224\u522b\u529b\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u9762\u5bf9\u6781\u5177\u6311\u6218\u6027\u7684\u573a\u666f\u65f6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3001\u589e\u52a0\u6a21\u578b\u5224\u522b\u529b\u5e76\u7cfb\u7edf\u6027\u51cf\u5c11\u8bef\u5dee\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u793a\u4f8b\u5b66\u4e60\uff08MIL\uff09\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002\u7814\u7a76\u8005\u4ece\u6570\u5b66\u89d2\u5ea6\u9610\u8ff0\u4e86MIL\u76f8\u6bd4\u5355\u793a\u4f8b\u5b66\u4e60\u7684\u4f18\u8d8a\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5206\u6790MIL\u6a21\u578b\u968f\u5b9e\u4f8b\u6570\u91cf\u53d8\u5316\u7684\u7f29\u653e\u884c\u4e3a\u6765\u652f\u6301\u7406\u8bba\u4e3b\u5f20\u3002\u4f5c\u4e3a\u5177\u4f53\u5e94\u7528\uff0c\u8be5\u65b9\u6cd5\u88ab\u7528\u4e8e\u5229\u7528\u5927\u578b\u5f3a\u5b50\u5bf9\u649e\u673a\uff08LHC\uff09\u4e9a\u539f\u5b50\u7c92\u5b50\u78b0\u649e\u4e8b\u4ef6\u7684\u8fd0\u52a8\u5b66\u4fe1\u606f\uff0c\u7ea6\u675f\u6807\u51c6\u6a21\u578b\u6709\u6548\u573a\u8bba\uff08SMEFT\uff09\u7684\u5a01\u5c14\u900a\u7cfb\u6570\u3002", "result": "\u6240\u63d0\u51fa\u7684MIL\u65b9\u6cd5\u80fd\u591f\u4e3a\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\u4e2d\u7684\u53c2\u6570\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u9884\u6d4b\uff0c\u5e76\u663e\u8457\u63d0\u9ad8ML\u6a21\u578b\u7684\u5224\u522b\u529b\uff0c\u5373\u4f7f\u5728\u4f20\u7edf\u5206\u7c7b\u5668\u96be\u4ee5\u6709\u6548\u9884\u6d4b\u7684\u6311\u6218\u6027\u573a\u666f\u4e0b\u3002\u8be5\u65b9\u6cd5\u80fd\u7cfb\u7edf\u6027\u5730\u51cf\u5c11ML\u6a21\u578b\u7684\u9884\u6d4b\u8bef\u5dee\u3002\u7814\u7a76\u663e\u793a\uff0c\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\uff0c\u6709\u53ef\u80fd\u4ece\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u7406\u8bba\u6700\u5927\u8d39\u820d\u5c14\u4fe1\u606f\u3002\u5728SMEFT\u5a01\u5c14\u900a\u7cfb\u6570\u7ea6\u675f\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eMIL\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u5224\u522b\u529b\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u6a21\u578b\u8bef\u5dee\uff0c\u5e76\u5728\u7269\u7406\u5b66\u5e94\u7528\u4e2d\u6210\u529f\u7ea6\u675f\u4e86SMEFT\u53c2\u6570\uff0c\u5c55\u73b0\u4e86\u5176\u63d0\u53d6\u6570\u636e\u96c6\u6700\u5927\u6f5c\u5728\u4fe1\u606f\u7684\u80fd\u529b\u3002"}}
{"id": "2508.07743", "pdf": "https://arxiv.org/pdf/2508.07743", "abs": "https://arxiv.org/abs/2508.07743", "authors": ["Markus Fritzsche", "Elliot Gestrin", "Jendrik Seipp"], "title": "Symmetry-Aware Transformer Training for Automated Planning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "While transformers excel in many settings, their application in the field of\nautomated planning is limited. Prior work like PlanGPT, a state-of-the-art\ndecoder-only transformer, struggles with extrapolation from easy to hard\nplanning problems. This in turn stems from problem symmetries: planning tasks\ncan be represented with arbitrary variable names that carry no meaning beyond\nbeing identifiers. This causes a combinatorial explosion of equivalent\nrepresentations that pure transformers cannot efficiently learn from. We\npropose a novel contrastive learning objective to make transformers\nsymmetry-aware and thereby compensate for their lack of inductive bias.\nCombining this with architectural improvements, we show that transformers can\nbe efficiently trained for either plan-generation or heuristic-prediction. Our\nresults across multiple planning domains demonstrate that our symmetry-aware\ntraining effectively and efficiently addresses the limitations of PlanGPT.", "AI": {"tldr": "\u9488\u5bf9\u89c4\u5212\u9886\u57df\u4e2dTransformer\u6a21\u578b\uff08\u5982PlanGPT\uff09\u56e0\u5bf9\u79f0\u6027\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u548c\u67b6\u6784\u6539\u8fdb\u7684\u5bf9\u79f0\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709Transformer\u6a21\u578b\u5728\u81ea\u52a8\u5316\u89c4\u5212\u9886\u57df\u7684\u5e94\u7528\u53d7\u9650\uff0c\u7279\u522b\u662f\u50cfPlanGPT\u8fd9\u6837\u7684\u6a21\u578b\uff0c\u96be\u4ee5\u4ece\u7b80\u5355\u95ee\u9898\u6cdb\u5316\u5230\u590d\u6742\u95ee\u9898\u3002\u8fd9\u6e90\u4e8e\u89c4\u5212\u4efb\u52a1\u7684\u5bf9\u79f0\u6027\uff0c\u5373\u53d8\u91cf\u547d\u540d\u4efb\u610f\u6027\u5bfc\u81f4\u7b49\u4ef7\u8868\u793a\u7684\u7ec4\u5408\u7206\u70b8\uff0c\u7eaf\u7cb9\u7684Transformer\u6a21\u578b\u96be\u4ee5\u9ad8\u6548\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u65e8\u5728\u4f7fTransformer\u6a21\u578b\u5177\u6709\u5bf9\u79f0\u611f\u77e5\u80fd\u529b\uff0c\u5f25\u8865\u5176\u5f52\u7eb3\u504f\u7f6e\u7684\u4e0d\u8db3\u3002\u7ed3\u5408\u67b6\u6784\u6539\u8fdb\uff0c\u4f7fTransformer\u80fd\u9ad8\u6548\u5730\u7528\u4e8e\u89c4\u5212\u751f\u6210\u6216\u542f\u53d1\u5f0f\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u89c4\u5212\u9886\u57df\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5bf9\u79f0\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\u6709\u6548\u4e14\u9ad8\u6548\u5730\u89e3\u51b3\u4e86PlanGPT\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5bf9\u79f0\u611f\u77e5\u8bad\u7ec3\uff08\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u67b6\u6784\u6539\u8fdb\uff09\uff0cTransformer\u6a21\u578b\u53ef\u4ee5\u514b\u670d\u5728\u81ea\u52a8\u5316\u89c4\u5212\u4e2d\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u6709\u6548\u4e14\u9ad8\u6548\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2508.06908", "pdf": "https://arxiv.org/pdf/2508.06908", "abs": "https://arxiv.org/abs/2508.06908", "authors": ["Jinhao Li", "Zijian Chen", "Lirong Deng", "Changbo Wang", "Guangtao Zhai"], "title": "MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Person re-identification (ReID) aims to retrieve the images of an interested\nperson in the gallery images, with wide applications in medical rehabilitation,\nabnormal behavior detection, and public security. However, traditional person\nReID models suffer from uni-modal capability, leading to poor generalization\nability in multi-modal data, such as RGB, thermal, infrared, sketch images,\ntextual descriptions, etc. Recently, the emergence of multi-modal large\nlanguage models (MLLMs) shows a promising avenue for addressing this problem.\nDespite this potential, existing methods merely regard MLLMs as feature\nextractors or caption generators, which do not fully unleash their reasoning,\ninstruction-following, and cross-modal understanding capabilities. To bridge\nthis gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmark\nspecifically designed for person ReID. The MMReID-Bench includes 20,710\nmulti-modal queries and gallery images covering 10 different person ReID tasks.\nComprehensive experiments demonstrate the remarkable capabilities of MLLMs in\ndelivering effective and versatile person ReID. Nevertheless, they also have\nlimitations in handling a few modalities, particularly thermal and infrared\ndata. We hope MMReID-Bench can facilitate the community to develop more robust\nand generalizable multimodal foundation models for person ReID.", "AI": {"tldr": "\u884c\u4eba\u91cd\u8bc6\u522b\u9762\u4e34\u591a\u6a21\u6001\u6cdb\u5316\u6311\u6218\u3002\u672c\u6587\u63d0\u51faMMReID-Bench\uff0c\u9996\u4e2a\u591a\u4efb\u52a1\u591a\u6a21\u6001\u884c\u4eba\u91cd\u8bc6\u522b\u57fa\u51c6\uff0c\u65e8\u5728\u5168\u9762\u8bc4\u4f30\u5e76\u5229\u7528MLLMs\u7684\u6f5c\u529b\uff0c\u7ed3\u679c\u8868\u660eMLLMs\u80fd\u529b\u663e\u8457\u4f46\u5bf9\u67d0\u4e9b\u6a21\u6001\uff08\u5982\u70ed\u6210\u50cf\uff09\u4ecd\u6709\u5c40\u9650\u3002", "motivation": "\u4f20\u7edf\u884c\u4eba\u91cd\u8bc6\u522b\u6a21\u578b\u7f3a\u4e4f\u591a\u6a21\u6001\u80fd\u529b\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u5dee\u3002\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5e94\u7528\u4ec5\u9650\u4e8e\u7279\u5f81\u63d0\u53d6\u6216\u5b57\u5e55\u751f\u6210\uff0c\u672a\u80fd\u5145\u5206\u53d1\u6325\u5176\u63a8\u7406\u3001\u6307\u4ee4\u9075\u5faa\u548c\u8de8\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5f15\u5165MMReID-Bench\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u4e3a\u884c\u4eba\u91cd\u8bc6\u522b\u8bbe\u8ba1\u7684\u591a\u4efb\u52a1\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u5305\u542b20,710\u4e2a\u591a\u6a21\u6001\u67e5\u8be2\u548c\u56fe\u5e93\u56fe\u50cf\uff0c\u6db5\u76d610\u79cd\u4e0d\u540c\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u3002\u901a\u8fc7\u8be5\u57fa\u51c6\u8fdb\u884c\u5168\u9762\u5b9e\u9a8c\u4ee5\u8bc4\u4f30MLLMs\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMLLMs\u5728\u63d0\u4f9b\u6709\u6548\u548c\u901a\u7528\u884c\u4eba\u91cd\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u5904\u7406\u5c11\u6570\u6a21\u6001\uff08\u7279\u522b\u662f\u70ed\u6210\u50cf\u548c\u7ea2\u5916\u6570\u636e\uff09\u65f6\u4ecd\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "\u5e0c\u671bMMReID-Bench\u80fd\u4fc3\u8fdb\u793e\u533a\u5f00\u53d1\u51fa\u66f4\u9c81\u68d2\u3001\u66f4\u901a\u7528\u7684\u591a\u6a21\u6001\u884c\u4eba\u91cd\u8bc6\u522b\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2508.07592", "pdf": "https://arxiv.org/pdf/2508.07592", "abs": "https://arxiv.org/abs/2508.07592", "authors": ["Puspesh Kumar Srivastava", "Uddeshya Raj", "Praveen Patel", "/Shubham Kumar Nigam", "Noel Shallum", "Arnab Bhattacharya"], "title": "IBPS: Indian Bail Prediction System", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Bail decisions are among the most frequently adjudicated matters in Indian\ncourts, yet they remain plagued by subjectivity, delays, and inconsistencies.\nWith over 75% of India's prison population comprising undertrial prisoners,\nmany from socioeconomically disadvantaged backgrounds, the lack of timely and\nfair bail adjudication exacerbates human rights concerns and contributes to\nsystemic judicial backlog. In this paper, we present the Indian Bail Prediction\nSystem (IBPS), an AI-powered framework designed to assist in bail\ndecision-making by predicting outcomes and generating legally sound rationales\nbased solely on factual case attributes and statutory provisions. We curate and\nrelease a large-scale dataset of 150,430 High Court bail judgments, enriched\nwith structured annotations such as age, health, criminal history, crime\ncategory, custody duration, statutes, and judicial reasoning. We fine-tune a\nlarge language model using parameter-efficient techniques and evaluate its\nperformance across multiple configurations, with and without statutory context,\nand with RAG. Our results demonstrate that models fine-tuned with statutory\nknowledge significantly outperform baselines, achieving strong accuracy and\nexplanation quality, and generalize well to a test set independently annotated\nby legal experts. IBPS offers a transparent, scalable, and reproducible\nsolution to support data-driven legal assistance, reduce bail delays, and\npromote procedural fairness in the Indian judicial system.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5370\u5ea6\u4fdd\u91ca\u9884\u6d4b\u7cfb\u7edf\uff08IBPS\uff09\uff0c\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u9884\u6d4b\u4fdd\u91ca\u7ed3\u679c\u548c\u751f\u6210\u6cd5\u5f8b\u4f9d\u636e\u6765\u8f85\u52a9\u5370\u5ea6\u7684\u4fdd\u91ca\u51b3\u7b56\u3002\u4e3a\u89e3\u51b3\u5370\u5ea6\u4fdd\u91ca\u7cfb\u7edf\u7684\u4e3b\u89c2\u6027\u3001\u5ef6\u8bef\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u4fdd\u91ca\u5224\u51b3\u6570\u636e\u96c6\uff0c\u5e76\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u6cd5\u89c4\u77e5\u8bc6\u7684\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u53f8\u6cd5\u6548\u7387\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u5370\u5ea6\u4fdd\u91ca\u51b3\u7b56\u5b58\u5728\u4e25\u91cd\u7684\u4e3b\u89c2\u6027\u3001\u5ef6\u8bef\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5bfc\u81f4\u8d85\u8fc775%\u7684\u76d1\u72f1\u4eba\u53e3\u4e3a\u5ba1\u524d\u7f81\u62bc\u4eba\u5458\uff08\u5176\u4e2d\u591a\u6570\u6765\u81ea\u793e\u4f1a\u7ecf\u6d4e\u5f31\u52bf\u7fa4\u4f53\uff09\uff0c\u8fd9\u4e0d\u4ec5\u4fb5\u72af\u4eba\u6743\uff0c\u4e5f\u52a0\u5267\u4e86\u53f8\u6cd5\u7cfb\u7edf\u79ef\u538b\u3002", "method": "\u7814\u7a76\u56e2\u961f\u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b150,430\u4efd\u5370\u5ea6\u9ad8\u7b49\u6cd5\u9662\u4fdd\u91ca\u5224\u51b3\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u4e86\u5e74\u9f84\u3001\u5065\u5eb7\u3001\u72af\u7f6a\u5386\u53f2\u3001\u6848\u4ef6\u7c7b\u522b\u3001\u7f81\u62bc\u65f6\u957f\u3001\u6cd5\u89c4\u548c\u53f8\u6cd5\u63a8\u7406\u7b49\u7ed3\u6784\u5316\u6807\u6ce8\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u6280\u672f\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7b49\u65b9\u6cd5\uff0c\u5728\u6709\u65e0\u6cd5\u89c4\u4e0a\u4e0b\u6587\u7b49\u591a\u79cd\u914d\u7f6e\u4e0b\u8bc4\u4f30\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u6cd5\u89c4\u77e5\u8bc6\u5fae\u8c03\u7684\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u5ea6\u548c\u4f18\u79c0\u7684\u89e3\u91ca\u8d28\u91cf\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u7531\u6cd5\u5f8b\u4e13\u5bb6\u72ec\u7acb\u6807\u6ce8\u7684\u6d4b\u8bd5\u96c6\u3002", "conclusion": "IBPS\u4e3a\u5370\u5ea6\u53f8\u6cd5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u590d\u73b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u6cd5\u5f8b\u63f4\u52a9\uff0c\u51cf\u5c11\u4fdd\u91ca\u5ef6\u8bef\uff0c\u5e76\u4fc3\u8fdb\u7a0b\u5e8f\u516c\u5e73\u6027\u3002"}}
{"id": "2508.07117", "pdf": "https://arxiv.org/pdf/2508.07117", "abs": "https://arxiv.org/abs/2508.07117", "authors": ["Peyman Baghershahi", "Gregoire Fournier", "Pranav Nyati", "Sourav Medya"], "title": "From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context", "categories": ["cs.LG"], "comment": "18 pages, 3 figures, 8 tables", "summary": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning over\nstructured data, including text-attributed graphs, which are common in domains\nsuch as citation networks, social platforms, and knowledge graphs. GNNs are not\ninherently interpretable and thus, many explanation methods have been proposed.\nHowever, existing explanation methods often struggle to generate interpretable,\nfine-grained rationales, especially when node attributes include rich natural\nlanguage. In this work, we introduce LOGIC, a lightweight, post-hoc framework\nthat uses large language models (LLMs) to generate faithful and interpretable\nexplanations for GNN predictions. LOGIC projects GNN node embeddings into the\nLLM embedding space and constructs hybrid prompts that interleave soft prompts\nwith textual inputs from the graph structure. This enables the LLM to reason\nabout GNN internal representations and produce natural language explanations\nalong with concise explanation subgraphs. Our experiments across four\nreal-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off\nbetween fidelity and sparsity, while significantly improving human-centric\nmetrics such as insightfulness. LOGIC sets a new direction for LLM-based\nexplainability in graph learning by aligning GNN internals with human\nreasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLOGIC\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3a\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5728\u6587\u672c\u5c5e\u6027\u56fe\u4e0a\u7684\u9884\u6d4b\u751f\u6210\u5fe0\u5b9e\u4e14\u53ef\u89e3\u91ca\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u548c\u7b80\u6d01\u7684\u89e3\u91ca\u5b50\u56fe\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5728\u5904\u7406\u6587\u672c\u5c5e\u6027\u56fe\u65f6\u8868\u73b0\u5f3a\u5927\uff0c\u4f46\u5176\u672c\u8eab\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u5728\u751f\u6210\u53ef\u89e3\u91ca\u3001\u7ec6\u7c92\u5ea6\u7684\u89e3\u91ca\u65f6\uff0c\u5c24\u5176\u5f53\u8282\u70b9\u5c5e\u6027\u5305\u542b\u4e30\u5bcc\u7684\u81ea\u7136\u8bed\u8a00\u65f6\uff0c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f15\u5165LOGIC\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u540e\u9a8c\u7684\u6846\u67b6\u3002\u5b83\u5c06GNN\u8282\u70b9\u5d4c\u5165\u6295\u5f71\u5230LLM\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u6784\u5efa\u6df7\u5408\u63d0\u793a\uff08\u5c06\u8f6f\u63d0\u793a\u4e0e\u56fe\u7ed3\u6784\u4e2d\u7684\u6587\u672c\u8f93\u5165\u4ea4\u7ec7\uff09\uff0c\u4f7fLLM\u80fd\u591f\u7406\u89e3GNN\u7684\u5185\u90e8\u8868\u793a\uff0c\u4ece\u800c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u548c\u7b80\u6d01\u7684\u89e3\u91ca\u5b50\u56fe\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6587\u672c\u5c5e\u6027\u56fe\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8868\u660e\uff0cLOGIC\u5728\u5fe0\u5b9e\u5ea6\u548c\u7a00\u758f\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6743\u8861\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u6307\u6807\uff0c\u5982\u6d1e\u5bdf\u529b\u3002", "conclusion": "LOGIC\u901a\u8fc7\u5c06GNN\u5185\u90e8\u8868\u793a\u4e0e\u4eba\u7c7b\u63a8\u7406\u5bf9\u9f50\uff0c\u4e3a\u57fa\u4e8eLLM\u7684\u56fe\u5b66\u4e60\u53ef\u89e3\u91ca\u6027\u8bbe\u5b9a\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.07790", "pdf": "https://arxiv.org/pdf/2508.07790", "abs": "https://arxiv.org/abs/2508.07790", "authors": ["Alessandro Abate", "Thom Badings", "Giuseppe De Giacomo", "Francesco Fabiano"], "title": "Best-Effort Policies for Robust Markov Decision Processes", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "We study the common generalization of Markov decision processes (MDPs) with\nsets of transition probabilities, known as robust MDPs (RMDPs). A standard goal\nin RMDPs is to compute a policy that maximizes the expected return under an\nadversarial choice of the transition probabilities. If the uncertainty in the\nprobabilities is independent between the states, known as s-rectangularity,\nsuch optimal robust policies can be computed efficiently using robust value\niteration. However, there might still be multiple optimal robust policies,\nwhich, while equivalent with respect to the worst-case, reflect different\nexpected returns under non-adversarial choices of the transition probabilities.\nHence, we propose a refined policy selection criterion for RMDPs, drawing\ninspiration from the notions of dominance and best-effort in game theory.\nInstead of seeking a policy that only maximizes the worst-case expected return,\nwe additionally require the policy to achieve a maximal expected return under\ndifferent (i.e., not fully adversarial) transition probabilities. We call such\na policy an optimal robust best-effort (ORBE) policy. We prove that ORBE\npolicies always exist, characterize their structure, and present an algorithm\nto compute them with a small overhead compared to standard robust value\niteration. ORBE policies offer a principled tie-breaker among optimal robust\npolicies. Numerical experiments show the feasibility of our approach.", "AI": {"tldr": "\u9488\u5bf9\u9c81\u68d2\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08RMDPs\uff09\u4e2d\u5b58\u5728\u591a\u4e2a\u6700\u574f\u60c5\u51b5\u6700\u4f18\u7b56\u7565\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6700\u4f18\u9c81\u68d2\u6700\u4f73\u52aa\u529b\uff08ORBE\uff09\u7b56\u7565\u7684\u65b0\u578b\u7b56\u7565\u9009\u62e9\u6807\u51c6\u3002ORBE\u7b56\u7565\u4e0d\u4ec5\u6700\u5927\u5316\u6700\u574f\u60c5\u51b5\u9884\u671f\u6536\u76ca\uff0c\u8fd8\u517c\u987e\u975e\u5bf9\u6297\u6027\u6982\u7387\u4e0b\u7684\u6700\u5927\u9884\u671f\u6536\u76ca\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u7b56\u7565\u9009\u62e9\u65b9\u6cd5\u3002", "motivation": "\u5728\u9c81\u68d2\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08RMDPs\uff09\u4e2d\uff0c\u5373\u4f7f\u5728s-\u77e9\u5f62\u4e0d\u786e\u5b9a\u6027\u4e0b\uff0c\u4e5f\u53ef\u80fd\u5b58\u5728\u591a\u4e2a\u6700\u4f18\u9c81\u68d2\u7b56\u7565\u3002\u8fd9\u4e9b\u7b56\u7565\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u9884\u671f\u6536\u76ca\u76f8\u540c\uff0c\u4f46\u5728\u975e\u5bf9\u6297\u6027\uff08\u975e\u5b8c\u5168\u6700\u574f\u60c5\u51b5\uff09\u7684\u8f6c\u79fb\u6982\u7387\u4e0b\uff0c\u5176\u9884\u671f\u6536\u76ca\u53ef\u80fd\u4e0d\u540c\uff0c\u5bfc\u81f4\u7b56\u7565\u9009\u62e9\u7684\u56f0\u96be\u3002", "method": "\u672c\u6587\u53d7\u535a\u5f08\u8bba\u4e2d\u201c\u652f\u914d\u201d\u548c\u201c\u6700\u4f73\u52aa\u529b\u201d\u6982\u5ff5\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u9009\u62e9\u51c6\u5219\u2014\u2014\u6700\u4f18\u9c81\u68d2\u6700\u4f73\u52aa\u529b\uff08ORBE\uff09\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u8981\u6c42\u7b56\u7565\u6700\u5927\u5316\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u9884\u671f\u6536\u76ca\uff0c\u8fd8\u989d\u5916\u8981\u6c42\u7b56\u7565\u5728\u4e0d\u540c\uff08\u975e\u5b8c\u5168\u5bf9\u6297\u6027\uff09\u8f6c\u79fb\u6982\u7387\u4e0b\u5b9e\u73b0\u6700\u5927\u9884\u671f\u6536\u76ca\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0e\u6807\u51c6\u9c81\u68d2\u503c\u8fed\u4ee3\u76f8\u6bd4\u5f00\u9500\u8f83\u5c0f\u7684\u7b97\u6cd5\u6765\u8ba1\u7b97ORBE\u7b56\u7565\u3002", "result": "\u7814\u7a76\u8bc1\u660e\u4e86ORBE\u7b56\u7565\u59cb\u7ec8\u5b58\u5728\uff0c\u5e76\u523b\u753b\u4e86\u5176\u7ed3\u6784\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u8ba1\u7b97ORBE\u7b56\u7565\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u76f8\u5bf9\u4e8e\u6807\u51c6\u9c81\u68d2\u503c\u8fed\u4ee3\u7684\u8ba1\u7b97\u5f00\u9500\u8f83\u5c0f\u3002\u6570\u503c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u662f\u53ef\u884c\u7684\u3002", "conclusion": "ORBE\u7b56\u7565\u4e3a\u9c81\u68d2\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u591a\u4e2a\u6700\u4f18\u9c81\u68d2\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u539f\u5219\u7684\u51b3\u7b56\u8f85\u52a9\u673a\u5236\uff08\u6216\u201c\u51b3\u80dc\u5c40\u201d\u7b56\u7565\uff09\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5e73\u8861\u6700\u574f\u60c5\u51b5\u4e0e\u975e\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u9884\u671f\u6536\u76ca\u3002"}}
{"id": "2508.06916", "pdf": "https://arxiv.org/pdf/2508.06916", "abs": "https://arxiv.org/abs/2508.06916", "authors": ["Shichao Ma", "Yunhe Guo", "Jiahao Su", "Qihe Huang", "Zhengyang Zhou", "Yang Wang"], "title": "Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image generation tasks have driven remarkable advances in diverse\nmedia applications, yet most focus on single-turn scenarios and struggle with\niterative, multi-turn creative tasks. Recent dialogue-based systems attempt to\nbridge this gap, but their single-agent, sequential paradigm often causes\nintention drift and incoherent edits. To address these limitations, we present\nTalk2Image, a novel multi-agent system for interactive image generation and\nediting in multi-turn dialogue scenarios. Our approach integrates three key\ncomponents: intention parsing from dialogue history, task decomposition and\ncollaborative execution across specialized agents, and feedback-driven\nrefinement based on a multi-view evaluation mechanism. Talk2Image enables\nstep-by-step alignment with user intention and consistent image editing.\nExperiments demonstrate that Talk2Image outperforms existing baselines in\ncontrollability, coherence, and user satisfaction across iterative image\ngeneration and editing tasks.", "AI": {"tldr": "\u9488\u5bf9\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u4ea4\u4e92\u5f0f\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edfTalk2Image\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u53cd\u9988\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u7684\u53ef\u63a7\u6027\u3001\u8fde\u8d2f\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u5355\u8f6e\u4efb\u52a1\uff0c\u96be\u4ee5\u5e94\u5bf9\u8fed\u4ee3\u3001\u591a\u8f6e\u7684\u521b\u610f\u4efb\u52a1\uff0c\u5373\u4f7f\u662f\u57fa\u4e8e\u5bf9\u8bdd\u7684\u7cfb\u7edf\u4e5f\u5e38\u56e0\u5355\u667a\u80fd\u4f53\u987a\u5e8f\u8303\u5f0f\u5bfc\u81f4\u610f\u56fe\u6f02\u79fb\u548c\u7f16\u8f91\u4e0d\u8fde\u8d2f\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u7cfb\u7edfTalk2Image\uff0c\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u4ece\u5bf9\u8bdd\u5386\u53f2\u4e2d\u89e3\u6790\u7528\u6237\u610f\u56fe\u3001\u901a\u8fc7\u4e13\u4e1a\u5316\u667a\u80fd\u4f53\u8fdb\u884c\u4efb\u52a1\u5206\u89e3\u548c\u534f\u4f5c\u6267\u884c\u3001\u4ee5\u53ca\u57fa\u4e8e\u591a\u89c6\u56fe\u8bc4\u4f30\u673a\u5236\u7684\u53cd\u9988\u9a71\u52a8\u5f0f\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTalk2Image\u5728\u8fed\u4ee3\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e2d\uff0c\u5176\u53ef\u63a7\u6027\u3001\u8fde\u8d2f\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7cfb\u7edf\u3002", "conclusion": "Talk2Image\u901a\u8fc7\u5b9e\u73b0\u4e0e\u7528\u6237\u610f\u56fe\u7684\u9010\u6b65\u5bf9\u9f50\u548c\u56fe\u50cf\u7684\u6301\u7eed\u7f16\u8f91\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u4e2d\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.07598", "pdf": "https://arxiv.org/pdf/2508.07598", "abs": "https://arxiv.org/abs/2508.07598", "authors": ["Ziheng Li", "Zhi-Hong Deng"], "title": "Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements", "categories": ["cs.CL"], "comment": "ECAI 2025", "summary": "Although the LLM-based in-context learning (ICL) paradigm has demonstrated\nconsiderable success across various natural language processing tasks, it\nencounters challenges in event detection. This is because LLMs lack an accurate\nunderstanding of event triggers and tend to make over-interpretation, which\ncannot be effectively corrected through in-context examples alone. In this\npaper, we focus on the most challenging one-shot setting and propose KeyCP++, a\nkeyword-centric chain-of-thought prompting approach. KeyCP++ addresses the\nweaknesses of conventional ICL by automatically annotating the logical gaps\nbetween input text and detection results for the demonstrations. Specifically,\nto generate in-depth and meaningful rationale, KeyCP++ constructs a trigger\ndiscrimination prompting template. It incorporates the exemplary triggers\n(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let\nLLM propose candidate triggers, and justify each candidate. These\npropose-and-judge rationales help LLMs mitigate over-reliance on the keywords\nand promote detection rule learning. Extensive experiments demonstrate the\neffectiveness of our approach, showcasing significant advancements in one-shot\nevent detection.", "AI": {"tldr": "LLM\u5728\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u5b58\u5728\u89e6\u53d1\u8bcd\u7406\u89e3\u548c\u8fc7\u5ea6\u89e3\u8bfb\u95ee\u9898\uff0cICL\u96be\u4ee5\u6709\u6548\u7ea0\u6b63\u3002\u672c\u6587\u63d0\u51faKeyCP++\uff0c\u4e00\u79cd\u5173\u952e\u8bcd\u4e2d\u5fc3\u5316\u7684CoT\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u201c\u63d0\u8bae-\u5224\u65ad\u201d\u673a\u5236\u4fc3\u8fdbLLM\u5b66\u4e60\u68c0\u6d4b\u89c4\u5219\uff0c\u663e\u8457\u63d0\u5347\u5355\u6837\u672c\u4e8b\u4ef6\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u8303\u5f0f\u5728\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u5374\u9762\u4e34\u6311\u6218\u3002\u8fd9\u662f\u56e0\u4e3aLLM\u7f3a\u4e4f\u5bf9\u4e8b\u4ef6\u89e6\u53d1\u8bcd\u7684\u51c6\u786e\u7406\u89e3\u5e76\u5bb9\u6613\u8fc7\u5ea6\u89e3\u8bfb\uff0c\u800c\u5355\u72ec\u4f7f\u7528\u4e0a\u4e0b\u6587\u793a\u4f8b\u65e0\u6cd5\u6709\u6548\u7ea0\u6b63\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u9488\u5bf9\u6700\u5177\u6311\u6218\u6027\u7684\u5355\u6837\u672c\u4e8b\u4ef6\u68c0\u6d4b\u8bbe\u7f6e\uff0c\u672c\u6587\u63d0\u51faKeyCP++\uff0c\u4e00\u79cd\u5173\u952e\u8bcd\u4e2d\u5fc3\u5316\u7684\u601d\u7ef4\u94fe\uff08Chain-of-Thought\uff09\u63d0\u793a\u65b9\u6cd5\u3002KeyCP++\u901a\u8fc7\u81ea\u52a8\u6807\u6ce8\u6f14\u793a\u793a\u4f8b\u4e2d\u8f93\u5165\u6587\u672c\u4e0e\u68c0\u6d4b\u7ed3\u679c\u4e4b\u95f4\u7684\u903b\u8f91\u9e3f\u6c9f\u6765\u5f25\u8865\u4f20\u7edfICL\u7684\u4e0d\u8db3\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u6784\u5efa\u4e86\u4e00\u4e2a\u89e6\u53d1\u8bcd\u5224\u522b\u63d0\u793a\u6a21\u677f\uff0c\u5c06\u793a\u4f8b\u89e6\u53d1\u8bcd\uff08\u5373\u5173\u952e\u8bcd\uff09\u4f5c\u4e3a\u951a\u70b9\u878d\u5165\u63d0\u793a\uff0c\u8ba9LLM\u63d0\u51fa\u5019\u9009\u89e6\u53d1\u8bcd\u5e76\u9010\u4e00\u5224\u65ad\uff0c\u8fd9\u79cd\u201c\u63d0\u8bae-\u5224\u65ad\u201d\u7684\u63a8\u7406\u8fc7\u7a0b\u6709\u52a9\u4e8eLLM\u51cf\u8f7b\u5bf9\u5173\u952e\u8bcd\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u5e76\u4fc3\u8fdb\u68c0\u6d4b\u89c4\u5219\u7684\u5b66\u4e60\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86KeyCP++\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u5355\u6837\u672c\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u5c55\u3002", "conclusion": "KeyCP++\u901a\u8fc7\u521b\u65b0\u7684\u5173\u952e\u8bcd\u4e2d\u5fc3\u5316\u601d\u7ef4\u94fe\u63d0\u793a\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u5355\u6837\u672c\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u5bf9\u89e6\u53d1\u8bcd\u7406\u89e3\u4e0d\u8db3\u548c\u8fc7\u5ea6\u89e3\u8bfb\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u8868\u660e\u5176\u5728\u5e94\u5bf9LLM\u4e8b\u4ef6\u68c0\u6d4b\u6311\u6218\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.07122", "pdf": "https://arxiv.org/pdf/2508.07122", "abs": "https://arxiv.org/abs/2508.07122", "authors": ["Zhihao Xue", "Yun Zi", "Nia Qi", "Ming Gong", "Yujun Zou"], "title": "Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "This paper proposes a spatiotemporal graph neural network-based performance\nprediction algorithm to address the challenge of forecasting performance\nfluctuations in distributed backend systems with multi-level service call\nstructures. The method abstracts system states at different time slices into a\nsequence of graph structures. It integrates the runtime features of service\nnodes with the invocation relationships among services to construct a unified\nspatiotemporal modeling framework. The model first applies a graph\nconvolutional network to extract high-order dependency information from the\nservice topology. Then it uses a gated recurrent network to capture the dynamic\nevolution of performance metrics over time. A time encoding mechanism is also\nintroduced to enhance the model's ability to represent non-stationary temporal\nsequences. The architecture is trained in an end-to-end manner, optimizing the\nmulti-layer nested structure to achieve high-precision regression of future\nservice performance metrics. To validate the effectiveness of the proposed\nmethod, a large-scale public cluster dataset is used. A series of\nmulti-dimensional experiments are designed, including variations in time\nwindows and concurrent load levels. These experiments comprehensively evaluate\nthe model's predictive performance and stability. The experimental results show\nthat the proposed model outperforms existing representative methods across key\nmetrics such as MAE, RMSE, and R2. It maintains strong robustness under varying\nload intensities and structural complexities. These results demonstrate the\nmodel's practical potential for backend service performance management tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u9884\u6d4b\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u5e03\u5f0f\u540e\u7aef\u7cfb\u7edf\u591a\u7ea7\u670d\u52a1\u8c03\u7528\u7ed3\u6784\u4e2d\u7684\u6027\u80fd\u6ce2\u52a8\u9884\u6d4b\u96be\u9898\u3002", "motivation": "\u9884\u6d4b\u5177\u6709\u591a\u7ea7\u670d\u52a1\u8c03\u7528\u7ed3\u6784\u7684\u5206\u5e03\u5f0f\u540e\u7aef\u7cfb\u7edf\u7684\u6027\u80fd\u6ce2\u52a8\uff0c\u8fd9\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5c06\u7cfb\u7edf\u72b6\u6001\u62bd\u8c61\u4e3a\u56fe\u5e8f\u5217\uff0c\u6574\u5408\u670d\u52a1\u8282\u70b9\u8fd0\u884c\u65f6\u7279\u5f81\u548c\u8c03\u7528\u5173\u7cfb\u3002\u6a21\u578b\u901a\u8fc7\u56fe\u5377\u79ef\u7f51\u7edc\u63d0\u53d6\u670d\u52a1\u62d3\u6251\u7684\u9ad8\u9636\u4f9d\u8d56\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u95e8\u63a7\u5faa\u73af\u7f51\u7edc\u6355\u83b7\u6027\u80fd\u6307\u6807\u7684\u65f6\u95f4\u52a8\u6001\u6f14\u53d8\uff0c\u540c\u65f6\u5f15\u5165\u65f6\u95f4\u7f16\u7801\u673a\u5236\u589e\u5f3a\u5bf9\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u7684\u8868\u793a\u80fd\u529b\u3002\u6a21\u578b\u91c7\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u5927\u578b\u516c\u5171\u96c6\u7fa4\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u5728MAE\u3001RMSE\u548cR2\u7b49\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u8d1f\u8f7d\u5f3a\u5ea6\u548c\u7ed3\u6784\u590d\u6742\u5ea6\u4e0b\u4fdd\u6301\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u540e\u7aef\u670d\u52a1\u6027\u80fd\u7ba1\u7406\u4efb\u52a1\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.07834", "pdf": "https://arxiv.org/pdf/2508.07834", "abs": "https://arxiv.org/abs/2508.07834", "authors": ["Mubaris Nadeem", "Johannes Zenkert", "Lisa Bender", "Christian Weber", "Madjid Fathi"], "title": "KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations", "categories": ["cs.AI", "cs.ET"], "comment": "LWDA'23, KIRETT project, University of Siegen, Germany", "summary": "Over the years, the need for rescue operations throughout the world has\nincreased rapidly. Demographic changes and the resulting risk of injury or\nhealth disorders form the basis for emergency calls. In such scenarios, first\nresponders are in a rush to reach the patient in need, provide first aid, and\nsave lives. In these situations, they must be able to provide personalized and\noptimized healthcare in the shortest possible time and estimate the patients\ncondition with the help of freshly recorded vital data in an emergency\nsituation. However, in such a timedependent situation, first responders and\nmedical experts cannot fully grasp their knowledge and need assistance and\nrecommendation for further medical treatments. To achieve this, on the spot\ncalculated, evaluated, and processed knowledge must be made available to\nimprove treatments by first responders. The Knowledge Graph presented in this\narticle as a central knowledge representation provides first responders with an\ninnovative knowledge management that enables intelligent treatment\nrecommendations with an artificial intelligence-based pre-recognition of the\nsituation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7AI\u8f85\u52a9\u7684\u60c5\u5883\u9884\u8bc6\u522b\uff0c\u4e3a\u6025\u6551\u4eba\u5458\u63d0\u4f9b\u521b\u65b0\u7684\u77e5\u8bc6\u7ba1\u7406\u548c\u667a\u80fd\u6cbb\u7597\u5efa\u8bae\uff0c\u4ee5\u5e94\u5bf9\u7d27\u6025\u60c5\u51b5\u4e0b\u7684\u6551\u6cbb\u9700\u6c42\u3002", "motivation": "\u5168\u7403\u6025\u6551\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u6025\u6551\u4eba\u5458\u5728\u65f6\u95f4\u7d27\u8feb\u7684\u7d27\u6025\u60c5\u51b5\u4e0b\uff0c\u96be\u4ee5\u5145\u5206\u8fd0\u7528\u81ea\u8eab\u77e5\u8bc6\u5e76\u63d0\u4f9b\u4e2a\u6027\u5316\u3001\u4f18\u5316\u7684\u533b\u7597\u6551\u52a9\uff0c\u4e9f\u9700\u5373\u65f6\u534f\u52a9\u548c\u6cbb\u7597\u5efa\u8bae\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u6838\u5fc3\u77e5\u8bc6\u8868\u793a\uff0c\u65e8\u5728\u901a\u8fc7\u521b\u65b0\u7684\u77e5\u8bc6\u7ba1\u7406\uff0c\u5e76\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u7684\u60c5\u5883\u9884\u8bc6\u522b\u80fd\u529b\uff0c\u4e3a\u6025\u6551\u4eba\u5458\u63d0\u4f9b\u667a\u80fd\u6cbb\u7597\u5efa\u8bae\u3002", "result": "\u8be5\u77e5\u8bc6\u56fe\u8c31\u80fd\u591f\u73b0\u573a\u8ba1\u7b97\u3001\u8bc4\u4f30\u548c\u5904\u7406\u76f8\u5173\u77e5\u8bc6\uff0c\u4ece\u800c\u4e3a\u6025\u6551\u4eba\u5458\u63d0\u4f9b\u667a\u80fd\u6cbb\u7597\u5efa\u8bae\uff0c\u4ee5\u671f\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u4f18\u5316\u6cbb\u7597\u65b9\u6848\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u77e5\u8bc6\u56fe\u8c31\u901a\u8fc7\u63d0\u4f9b\u5373\u65f6\u3001\u667a\u80fd\u7684\u6cbb\u7597\u5efa\u8bae\uff0c\u6709\u6548\u652f\u6301\u6025\u6551\u4eba\u5458\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u7d27\u6025\u6551\u63f4\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2508.06924", "pdf": "https://arxiv.org/pdf/2508.06924", "abs": "https://arxiv.org/abs/2508.06924", "authors": ["Shihao Yuan", "Yahui Liu", "Yang Yue", "Jingyuan Zhang", "Wangmeng Zuo", "Qi Wang", "Fuzheng Zhang", "Guorui Zhou"], "title": "AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning", "categories": ["cs.CV"], "comment": "27 pages, 15 figures", "summary": "Inspired by the success of reinforcement learning (RL) in refining large\nlanguage models (LLMs), we propose AR-GRPO, an approach to integrate online RL\ntraining into autoregressive (AR) image generation models. We adapt the Group\nRelative Policy Optimization (GRPO) algorithm to refine the vanilla\nautoregressive models' outputs by carefully designed reward functions that\nevaluate generated images across multiple quality dimensions, including\nperceptual quality, realism, and semantic fidelity. We conduct comprehensive\nexperiments on both class-conditional (i.e., class-to-image) and\ntext-conditional (i.e., text-to-image) image generation tasks, demonstrating\nthat our RL-enhanced framework significantly improves both the image quality\nand human preference of generated images compared to the standard AR baselines.\nOur results show consistent improvements across various evaluation metrics,\nestablishing the viability of RL-based optimization for AR image generation and\nopening new avenues for controllable and high-quality image synthesis. The\nsource codes and models are available at:\nhttps://github.com/Kwai-Klear/AR-GRPO.", "AI": {"tldr": "\u53d7LLM\u4e2dRL\u6210\u529f\u7684\u542f\u53d1\uff0c\u672c\u6587\u63d0\u51faAR-GRPO\uff0c\u4e00\u79cd\u5c06\u5728\u7ebfRL\u8bad\u7ec3\u96c6\u6210\u5230\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7GRPO\u548c\u591a\u7ef4\u5956\u52b1\u51fd\u6570\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u4eba\u7c7b\u504f\u597d\u3002", "motivation": "\u53d7\u5f3a\u5316\u5b66\u4e60\u5728\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65b9\u9762\u6210\u529f\u7684\u542f\u53d1\uff0c\u63a2\u7d22\u5c06\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u81ea\u56de\u5f52\uff08AR\uff09\u56fe\u50cf\u751f\u6210\u6a21\u578b\u3002", "method": "\u63d0\u51faAR-GRPO\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u7f16Group Relative Policy Optimization (GRPO)\u7b97\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u8bc4\u4f30\u611f\u77e5\u8d28\u91cf\u3001\u771f\u5b9e\u611f\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u7b49\u591a\u7ef4\u5ea6\u5956\u52b1\u51fd\u6570\uff0c\u6765\u4f18\u5316\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8f93\u51fa\u3002", "result": "\u5728\u7c7b\u6761\u4ef6\u548c\u6587\u672c\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\uff0c\u4e0e\u6807\u51c6AR\u57fa\u7ebf\u76f8\u6bd4\uff0cAR-GRPO\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u4eba\u7c7b\u504f\u597d\uff0c\u5e76\u5728\u5404\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u793a\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u65b9\u6cd5\u5bf9\u4e8e\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u53ef\u63a7\u548c\u9ad8\u8d28\u91cf\u56fe\u50cf\u5408\u6210\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.07630", "pdf": "https://arxiv.org/pdf/2508.07630", "abs": "https://arxiv.org/abs/2508.07630", "authors": ["Anirudh Iyengar Kaniyar Narayana Iyengar", "Srija Mukhopadhyay", "Adnan Qidwai", "Shubhankar Singh", "Dan Roth", "Vivek Gupta"], "title": "InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information", "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.7; I.2.10; I.4.10; I.7.5"], "comment": "18 pages, 6 figures, 12 tables. Benchmark dataset and evaluation code\n  will be publicly made available", "summary": "We introduce InterChart, a diagnostic benchmark that evaluates how well\nvision-language models (VLMs) reason across multiple related charts, a task\ncentral to real-world applications such as scientific reporting, financial\nanalysis, and public policy dashboards. Unlike prior benchmarks focusing on\nisolated, visually uniform charts, InterChart challenges models with diverse\nquestion types ranging from entity inference and trend correlation to numerical\nestimation and abstract multi-step reasoning grounded in 2-3 thematically or\nstructurally related charts. We organize the benchmark into three tiers of\nincreasing difficulty: (1) factual reasoning over individual charts, (2)\nintegrative analysis across synthetically aligned chart sets, and (3) semantic\ninference over visually complex, real-world chart pairs. Our evaluation of\nstate-of-the-art open and closed-source VLMs reveals consistent and steep\naccuracy declines as chart complexity increases. We find that models perform\nbetter when we decompose multi-entity charts into simpler visual units,\nunderscoring their struggles with cross-chart integration. By exposing these\nsystematic limitations, InterChart provides a rigorous framework for advancing\nmultimodal reasoning in complex, multi-visual environments.", "AI": {"tldr": "\u5f15\u5165InterChart\uff0c\u4e00\u4e2a\u8bca\u65ad\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u591a\u4e2a\u76f8\u5173\u56fe\u8868\u95f4\u63a8\u7406\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u591a\u56fe\u8868\u6574\u5408\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\uff08\u5982\u79d1\u5b66\u62a5\u544a\u3001\u91d1\u878d\u5206\u6790\u3001\u516c\u5171\u653f\u7b56\u4eea\u8868\u76d8\uff09\u9700\u8981\u8de8\u591a\u4e2a\u76f8\u5173\u56fe\u8868\u8fdb\u884c\u63a8\u7406\uff0c\u800c\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u7684\u5355\u4e00\u56fe\u8868\uff0c\u672a\u80fd\u5145\u5206\u8bc4\u4f30VLMs\u5728\u590d\u6742\u591a\u56fe\u8868\u73af\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "InterChart\u5305\u542b\u57fa\u4e8e2-3\u4e2a\u4e3b\u9898\u6216\u7ed3\u6784\u76f8\u5173\u56fe\u8868\u7684\u5404\u79cd\u95ee\u9898\u7c7b\u578b\uff0c\u5305\u62ec\u5b9e\u4f53\u63a8\u65ad\u3001\u8d8b\u52bf\u5173\u8054\u3001\u6570\u503c\u4f30\u8ba1\u548c\u62bd\u8c61\u591a\u6b65\u63a8\u7406\u3002\u8be5\u57fa\u51c6\u5206\u4e3a\u4e09\u4e2a\u96be\u5ea6\u9012\u589e\u7684\u7ea7\u522b\uff1a\u4e2a\u4f53\u56fe\u8868\u7684\u4e8b\u5b9e\u63a8\u7406\u3001\u5408\u6210\u5bf9\u9f50\u56fe\u8868\u96c6\u95f4\u7684\u6574\u5408\u5206\u6790\uff0c\u4ee5\u53ca\u89c6\u89c9\u590d\u6742\u771f\u5b9e\u4e16\u754c\u56fe\u8868\u5bf9\u7684\u8bed\u4e49\u63a8\u65ad\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u95ed\u6e90VLMs\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u968f\u7740\u56fe\u8868\u590d\u6742\u6027\u589e\u52a0\uff0c\u6a21\u578b\u51c6\u786e\u7387\u6301\u7eed\u6025\u5267\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u5c06\u591a\u5b9e\u4f53\u56fe\u8868\u5206\u89e3\u4e3a\u66f4\u7b80\u5355\u7684\u89c6\u89c9\u5355\u5143\u65f6\uff0c\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u8fd9\u7a81\u663e\u4e86\u5b83\u4eec\u5728\u8de8\u56fe\u8868\u6574\u5408\u65b9\u9762\u7684\u56f0\u96be\u3002", "conclusion": "InterChart\u901a\u8fc7\u63ed\u793aVLMs\u5728\u590d\u6742\u3001\u591a\u89c6\u89c9\u73af\u5883\u4e2d\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u5c40\u9650\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u8c28\u7684\u6846\u67b6\uff0c\u4ee5\u63a8\u52a8\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.07126", "pdf": "https://arxiv.org/pdf/2508.07126", "abs": "https://arxiv.org/abs/2508.07126", "authors": ["Zhengran Ji", "Boyuan Chen"], "title": "Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Training reinforcement learning agents with human feedback is crucial when\ntask objectives are difficult to specify through dense reward functions. While\nprior methods rely on offline trajectory comparisons to elicit human\npreferences, such data is unavailable in online learning scenarios where agents\nmust adapt on the fly. Recent approaches address this by collecting real-time\nscalar feedback to guide agent behavior and train reward models for continued\nlearning after human feedback becomes unavailable. However, scalar feedback is\noften noisy and inconsistent, limiting the accuracy and generalization of\nlearned rewards. We propose Pref-GUIDE, a framework that transforms real-time\nscalar feedback into preference-based data to improve reward model learning for\ncontinual policy training. Pref-GUIDE Individual mitigates temporal\ninconsistency by comparing agent behaviors within short windows and filtering\nambiguous feedback. Pref-GUIDE Voting further enhances robustness by\naggregating reward models across a population of users to form consensus\npreferences. Across three challenging environments, Pref-GUIDE significantly\noutperforms scalar-feedback baselines, with the voting variant exceeding even\nexpert-designed dense rewards. By reframing scalar feedback as structured\npreferences with population feedback, Pref-GUIDE offers a scalable and\nprincipled approach for harnessing human input in online reinforcement\nlearning.", "AI": {"tldr": "Pref-GUIDE\u5c06\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5b9e\u65f6\u6807\u91cf\u53cd\u9988\u8f6c\u5316\u4e3a\u504f\u597d\u6570\u636e\uff0c\u901a\u8fc7\u5904\u7406\u53cd\u9988\u4e0d\u4e00\u81f4\u6027\u5e76\u805a\u5408\u7528\u6237\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5956\u52b1\u6a21\u578b\u7684\u5b66\u4e60\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u4e13\u5bb6\u8bbe\u8ba1\u7684\u7a20\u5bc6\u5956\u52b1\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u76ee\u6807\u96be\u4ee5\u901a\u8fc7\u7a20\u5bc6\u5956\u52b1\u51fd\u6570\u660e\u786e\u65f6\uff0c\u4eba\u7c7b\u53cd\u9988\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u79bb\u7ebf\u8f68\u8ff9\u6bd4\u8f83\uff0c\u4e0d\u9002\u7528\u4e8e\u5728\u7ebf\u5b66\u4e60\u3002\u5c3d\u7ba1\u5b9e\u65f6\u6807\u91cf\u53cd\u9988\u88ab\u7528\u4e8e\u5728\u7ebf\u573a\u666f\uff0c\u4f46\u5176\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u6027\u9650\u5236\u4e86\u5b66\u4e60\u5956\u52b1\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faPref-GUIDE\u6846\u67b6\uff0c\u5c06\u5b9e\u65f6\u6807\u91cf\u53cd\u9988\u8f6c\u6362\u4e3a\u57fa\u4e8e\u504f\u597d\u7684\u6570\u636e\uff0c\u4ee5\u6539\u8fdb\u5956\u52b1\u6a21\u578b\u5b66\u4e60\uff0c\u652f\u6301\u6301\u7eed\u7b56\u7565\u8bad\u7ec3\u3002\u5177\u4f53\u5305\u62ec\uff1aPref-GUIDE Individual\u901a\u8fc7\u5728\u77ed\u671f\u7a97\u53e3\u5185\u6bd4\u8f83\u667a\u80fd\u4f53\u884c\u4e3a\u5e76\u8fc7\u6ee4\u6a21\u7cca\u53cd\u9988\u6765\u7f13\u89e3\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\uff1bPref-GUIDE Voting\u901a\u8fc7\u805a\u5408\u7528\u6237\u7fa4\u4f53\u7684\u5956\u52b1\u6a21\u578b\u6765\u5f62\u6210\u5171\u8bc6\u504f\u597d\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\u4e2d\uff0cPref-GUIDE\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6807\u91cf\u53cd\u9988\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u5176\u4e2d\uff0cPref-GUIDE Voting\u53d8\u4f53\u751a\u81f3\u8d85\u8d8a\u4e86\u4e13\u5bb6\u8bbe\u8ba1\u7684\u7a20\u5bc6\u5956\u52b1\u3002", "conclusion": "Pref-GUIDE\u901a\u8fc7\u5c06\u6807\u91cf\u53cd\u9988\u91cd\u65b0\u6784\u5efa\u4e3a\u7ed3\u6784\u5316\u504f\u597d\u6570\u636e\u5e76\u7ed3\u5408\u7fa4\u4f53\u53cd\u9988\uff0c\u4e3a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u6709\u6548\u5229\u7528\u4eba\u7c7b\u8f93\u5165\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u539f\u5219\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.07932", "pdf": "https://arxiv.org/pdf/2508.07932", "abs": "https://arxiv.org/abs/2508.07932", "authors": ["Yi Zhai", "Zhiqiang Wei", "Ruohan Li", "Keyu Pan", "Shuo Liu", "Lu Zhang", "Jianmin Ji", "Wuyang Zhang", "Yu Zhang", "Yanyong Zhang"], "title": "\\(X\\)-evolve: Solution space evolution powered by large language models", "categories": ["cs.AI"], "comment": null, "summary": "While combining large language models (LLMs) with evolutionary algorithms\n(EAs) shows promise for solving complex optimization problems, current\napproaches typically evolve individual solutions, often incurring high LLM call\ncosts. We introduce \\(X\\)-evolve, a paradigm-shifting method that instead\nevolves solution spaces \\(X\\) (sets of individual solutions) - subsets of the\noverall search space \\(S\\). In \\(X\\)-evolve, LLMs generate tunable programs\nwherein certain code snippets, designated as parameters, define a tunable\nsolution space. A score-based search algorithm then efficiently explores this\nparametrically defined space, guided by feedback from objective function\nscores. This strategy enables broader and more efficient exploration, which can\npotentially accelerate convergence at a much lower search cost, requiring up to\ntwo orders of magnitude fewer LLM calls than prior leading methods. We\ndemonstrate \\(X\\)-evolve's efficacy across three distinct hard optimization\nproblems. For the cap set problem, we discover a larger partial admissible set,\nestablishing a new tighter asymptotic lower bound for the cap set constant (\\(C\n\\ge 2.2203\\)). In information theory, we uncover a larger independent set for\nthe 15-vertex cycle graph (\\(\\mathcal{C}_{15}^{\\boxtimes 5}\\), size 19,946),\nthereby raising the known lower bound on its Shannon capacity. Furthermore, for\nthe NP-hard online bin packing problem, we generate heuristics that\nconsistently outperform standard strategies across established benchmarks. By\nevolving solution spaces, our method considerably improves search\neffectiveness, making it possible to tackle high-dimensional problems that were\npreviously computationally prohibitive.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
