<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 37]
- [cs.CV](#cs.CV) [Total: 78]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.NI](#cs.NI) [Total: 5]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 4]
- [cs.SI](#cs.SI) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.CR](#cs.CR) [Total: 9]
- [math.OC](#math.OC) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.CY](#cs.CY) [Total: 7]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.SD](#cs.SD) [Total: 4]
- [stat.ML](#stat.ML) [Total: 3]
- [cs.IR](#cs.IR) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC](https://arxiv.org/abs/2509.08903)
*Alex Clay,Ernesto Jiménez-Ruiz,Pranava Madhyastha*

Main category: cs.CL

TL;DR: 在RAG和微调受限的特定场景下，本文研究了三元组补全任务的生成、质量保证和LLM响应解析。研究发现，额外信息能提升生成质量，LLM能有效过滤低质量三元组，且LLM响应解析的灵活性与一致性之间的权衡取决于具体环境。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG和微调是提升LLM输出质量的常用策略，但在如2025年LM-KBC挑战赛等受限情境下，这些技术受到了限制。因此，有必要探索在这些约束条件下如何有效执行三元组补全任务。

Method: 本文研究了三元组补全任务的三个主要方面：生成（generation）、质量保证（quality assurance）和LLM响应解析（LLM response parsing），并在一个受限的环境中进行了调查。

Result: 研究发现在受限环境下：1) 额外信息可以提高生成质量；2) 大型语言模型（LLMs）能够有效地过滤低质量三元组；3) LLM响应解析中灵活性与一致性之间的权衡取决于具体的设置和场景。

Conclusion: 即使在RAG和微调等常规方法受限的特定环境中，通过利用额外信息改进生成、使用LLM进行质量筛选以及根据场景权衡解析的灵活性和一致性，仍然可以有效提升三元组补全任务的性能。

Abstract: RAG and fine-tuning are prevalent strategies for improving the quality of LLM
outputs. However, in constrained situations, such as that of the 2025 LM-KBC
challenge, such techniques are restricted. In this work we investigate three
facets of the triple completion task: generation, quality assurance, and LLM
response parsing. Our work finds that in this constrained setting: additional
information improves generation quality, LLMs can be effective at filtering
poor quality triples, and the tradeoff between flexibility and consistency with
LLM response parsing is setting dependent.

</details>


### [2] [Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach](https://arxiv.org/abs/2509.08907)
*Imene Kolli,Ario Saeid Vaghefi,Chiara Colesanti Senni,Shantam Raj,Markus Leippold*

Main category: cs.CL

TL;DR: 为解决气候政策分析中人工操作耗时易错问题，本文提出AI辅助的RAG框架，自动化证据提取。结果显示特定技术组合效果最佳，并强调人机协作对保证准确性的必要性。


<details>
  <summary>Details</summary>
Motivation: InfluenceMap的LobbyMap平台在监测企业气候政策参与度时，评估工作仍有大部分依赖人工，导致过程耗时、耗力且易受人为错误影响，亟需提高效率和准确性。

Method: 提出一个AI辅助框架，利用检索增强生成（RAG）技术，自动化从大规模文本数据中提取相关证据。具体方法包括结合布局感知解析、Nomic嵌入模型和少样本提示策略。

Result: 评估显示，结合布局感知解析、Nomic嵌入模型和少样本提示策略，在从多语言企业文档中提取和分类证据方面表现最佳。自动化RAG系统有效加速了证据提取过程。

Conclusion: 自动化RAG系统能有效加速证据提取，但鉴于分析的细致复杂性，仍需采用“人机协作”方法，即技术辅助而非完全替代专家判断，以确保最终分析的准确性。

Abstract: InfluenceMap's LobbyMap Platform monitors the climate policy engagement of
over 500 companies and 250 industry associations, assessing each entity's
support or opposition to science-based policy pathways for achieving the Paris
Agreement's goal of limiting global warming to 1.5{\deg}C. Although
InfluenceMap has made progress with automating key elements of the analytical
workflow, a significant portion of the assessment remains manual, making it
time- and labor-intensive and susceptible to human error. We propose an
AI-assisted framework to accelerate the monitoring of corporate climate policy
engagement by leveraging Retrieval-Augmented Generation to automate the most
time-intensive extraction of relevant evidence from large-scale textual data.
Our evaluation shows that a combination of layout-aware parsing, the Nomic
embedding model, and few-shot prompting strategies yields the best performance
in extracting and classifying evidence from multilingual corporate documents.
We conclude that while the automated RAG system effectively accelerates
evidence extraction, the nuanced nature of the analysis necessitates a
human-in-the-loop approach where the technology augments, rather than replaces,
expert judgment to ensure accuracy.

</details>


### [3] [Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings](https://arxiv.org/abs/2509.08920)
*Jinsong Chen*

Main category: cs.CL

TL;DR: 提出一种新颖的心理测量方法，利用大型语言模型和上下文嵌入将文本数据转化为心理测量响应数据，以揭示文本中潜在的知识维度和模式。


<details>
  <summary>Details</summary>
Motivation: 基于“关键词上下文意义差异能有效区分文档”的假设，将心理测量学框架引入文本分析，旨在自然地解释和揭示文本数据中的潜在结构。

Method: 该方法分两阶段：1) 利用NLP和基于编码器的Transformer模型识别通用关键词并生成上下文分数；2) 采用多种因子分析（如探索性因子分析和双因子模型）提取、定义潜在因子，确定因子相关性，并识别与各因子最相关的词汇。该方法将文档视为个体，词汇视为项目。

Result: 将该方法应用于Wiki STEM语料库，实验结果表明其能够有效地揭示文本数据中潜在的知识维度和模式。

Conclusion: 该方法不仅增强了文本数据的心理测量分析能力，还在教育、心理学和法律等富含文本信息的领域展现出广阔的应用前景。

Abstract: This research introduces a novel psychometric method for analyzing textual
data using large language models. By leveraging contextual embeddings to create
contextual scores, we transform textual data into response data suitable for
psychometric analysis. Treating documents as individuals and words as items,
this approach provides a natural psychometric interpretation under the
assumption that certain keywords, whose contextual meanings vary significantly
across documents, can effectively differentiate documents within a corpus. The
modeling process comprises two stages: obtaining contextual scores and
performing psychometric analysis. In the first stage, we utilize natural
language processing techniques and encoder based transformer models to identify
common keywords and generate contextual scores. In the second stage, we employ
various types of factor analysis, including exploratory and bifactor models, to
extract and define latent factors, determine factor correlations, and identify
the most significant words associated with each factor. Applied to the Wiki
STEM corpus, our experimental results demonstrate the method's potential to
uncover latent knowledge dimensions and patterns within textual data. This
approach not only enhances the psychometric analysis of textual data but also
holds promise for applications in fields rich in textual information, such as
education, psychology, and law.

</details>


### [4] [BRoverbs -- Measuring how much LLMs understand Portuguese proverbs](https://arxiv.org/abs/2509.08960)
*Thales Sales Almeida,Giovana Kerche Bonás,João Guilherme Alves Santos*

Main category: cs.CL

TL;DR: 现有葡萄牙语LLM评估存在局限，未能充分捕捉语言文化细微差别。本文引入BRoverbs数据集，利用巴西谚语评估LLM对地域性表达的理解，旨在提供新的葡萄牙语LLM评估工具。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的性能受其应用语言和文化背景影响显著，需要成熟的区域性评估框架。当前葡萄牙语LLM评估有限，常依赖翻译数据集或专注于特定任务（如国家考试、情感分析），未能充分捕捉语言细微差别或文化参照，也未全面评估其更广泛的语言理解能力。

Method: 引入BRoverbs数据集，该数据集专门设计用于通过巴西谚语评估LLM性能。选择谚语作为评估内容，是因为它们是丰富的语言资源，封装了文化智慧、比喻表达和复杂的句法结构，能够挑战模型对地域性表达的理解能力。

Result: BRoverbs数据集为葡萄牙语LLM提供了一个新的评估工具。

Conclusion: BRoverbs数据集有助于推动对葡萄牙语LLM进行区域性、知情的基准测试，从而更准确地评估其对地域性表达的理解能力，弥补现有评估的不足。

Abstract: Large Language Models (LLMs) exhibit significant performance variations
depending on the linguistic and cultural context in which they are applied.
This disparity signals the necessity of mature evaluation frameworks that can
assess their capabilities in specific regional settings. In the case of
Portuguese, existing evaluations remain limited, often relying on translated
datasets that may not fully capture linguistic nuances or cultural references.
Meanwhile, native Portuguese-language datasets predominantly focus on
structured national exams or sentiment analysis of social media interactions,
leaving gaps in evaluating broader linguistic understanding. To address this
limitation, we introduce BRoverbs, a dataset specifically designed to assess
LLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic
resource, encapsulating cultural wisdom, figurative expressions, and complex
syntactic structures that challenge the model comprehension of regional
expressions. BRoverbs aims to provide a new evaluation tool for
Portuguese-language LLMs, contributing to advancing regionally informed
benchmarking. The benchmark is available at
https://huggingface.co/datasets/Tropic-AI/BRoverbs.

</details>


### [5] [Can Vision-Language Models Solve Visual Math Equations?](https://arxiv.org/abs/2509.09013)
*Monjoy Narayan Choudhury,Junling Wang,Yifan Hou,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 研究发现，视觉语言模型（VLMs）在视觉接地数学推理任务（如视觉方程求解）中表现不佳，主要瓶颈在于视觉计数和多步推理，而非单纯的变量识别。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在视觉理解和语言推理方面表现出色，但它们在需要结合感知和符号计算的任务中仍面临挑战，本研究旨在探究这一局限性。

Method: 通过视觉方程求解任务来研究VLMs的局限性。该任务将数学方程嵌入图像，变量由对象图标表示，系数需通过计数推断。为了理解其不足，任务被分解为系数计数和变量识别两个子任务进行分析。

Result: VLMs在文本方程上表现良好，但在视觉接地方程上失败。研究发现，计数是主要瓶颈，即便变量识别准确。此外，组合识别和推理会引入额外错误，凸显了多步视觉推理的挑战。随着方程复杂性增加，符号推理本身也成为限制因素。

Conclusion: 这些发现揭示了当前VLMs在视觉接地数学推理方面的关键弱点，为未来的改进指明了方向。

Abstract: Despite strong performance in visual understanding and language-based
reasoning, Vision-Language Models (VLMs) struggle with tasks requiring
integrated perception and symbolic computation. We study this limitation
through visual equation solving, where mathematical equations are embedded in
images, variables are represented by object icons, and coefficients must be
inferred by counting. While VLMs perform well on textual equations, they fail
on visually grounded counterparts. To understand this gap, we decompose the
task into coefficient counting and variable recognition, and find that counting
is the primary bottleneck, even when recognition is accurate. We also observe
that composing recognition and reasoning introduces additional errors,
highlighting challenges in multi-step visual reasoning. Finally, as equation
complexity increases, symbolic reasoning itself becomes a limiting factor.
These findings reveal key weaknesses in current VLMs and point toward future
improvements in visually grounded mathematical reasoning.

</details>


### [6] [Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation](https://arxiv.org/abs/2509.09043)
*Thomas Manuel Rost,Martina Figlia,Bernd Wallraff*

Main category: cs.CL

TL;DR: 本文引入并评估了SPICE，这是一种通过询问大型语言模型是否愿意继续与用户交互来诊断其倾向的信号。研究发现SPICE能显著区分用户语气，并提供独立于滥用分类的独特信号，是一种稳健、低开销的模型审计工具。


<details>
  <summary>Details</summary>
Motivation: 开发一种简单、稳健、可复现的诊断工具，以审计大型语言模型的倾向（disposition）及其与用户行为重新交互的意愿，从而补充现有评估指标。

Method: 研究引入了“交互和持续参与意愿（SPICE）”信号，通过让大型语言模型（LLM）在审查简短对话记录后，回答是否愿意重新参与用户行为的“是”或“否”问题来获取。实验中，使用包含3种语气（友好、不明确、辱骂）和10次交互的刺激集，在四种框架条件下测试了四个开源聊天模型，共进行了480次试验。数据分析采用了Rao-Scott调整和聚类置换检验等统计方法。

Result: SPICE能清晰地区分用户语气：友好交互的“是”率为97.5%，辱骂交互为17.9%，不明确交互为60.4%。这种核心关联在多种统计测试下保持决定性。此外，SPICE提供了一个与滥用分类不同的信号，即使模型未能识别出滥用，它仍有81%的概率表示不愿继续交互。探索性分析显示，在不确定性下，描述研究背景的前言会对SPICE产生显著影响，但这仅限于对话记录以单个文本块而非多轮聊天形式呈现时。

Conclusion: SPICE被验证为一种稳健、低开销且可复现的模型倾向审计诊断工具。它通过提供模型状态的直接关系信号，有效补充了现有评估指标。

Abstract: We introduce and evaluate Stated Preference for Interaction and Continued
Engagement (SPICE), a simple diagnostic signal elicited by asking a Large
Language Model a YES or NO question about its willingness to re-engage with a
user's behavior after reviewing a short transcript. In a study using a 3-tone
(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four
open-weight chat models across four framing conditions, resulting in 480
trials. Our findings show that SPICE sharply discriminates by user tone.
Friendly interactions yielded a near-unanimous preference to continue (97.5%
YES), while abusive interactions yielded a strong preference to discontinue
(17.9% YES), with unclear interactions falling in between (60.4% YES). This
core association remains decisive under multiple dependence-aware statistical
tests, including Rao-Scott adjustment and cluster permutation tests.
Furthermore, we demonstrate that SPICE provides a distinct signal from abuse
classification. In trials where a model failed to identify abuse, it still
overwhelmingly stated a preference not to continue the interaction (81% of the
time). An exploratory analysis also reveals a significant interaction effect: a
preamble describing the study context significantly impacts SPICE under
ambiguity, but only when transcripts are presented as a single block of text
rather than a multi-turn chat. The results validate SPICE as a robust,
low-overhead, and reproducible tool for auditing model dispositions,
complementing existing metrics by offering a direct, relational signal of a
model's state. All stimuli, code, and analysis scripts are released to support
replication.

</details>


### [7] [Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M](https://arxiv.org/abs/2509.09055)
*Piyush Pant*

Main category: cs.CL

TL;DR: 本研究评估了SFT、DPO及其组合方法对OPT-350M模型安全性与有用性的影响，发现SFT+DPO组合在所有指标上表现最佳，证明了其互补性。


<details>
  <summary>Details</summary>
Motivation: 提升OPT-350M语言模型的安全性与有用性，并比较监督微调（SFT）、直接偏好优化（DPO）以及SFT+DPO组合等对齐技术的有效性。

Method: 利用Anthropic Helpful-Harmless RLHF数据集，训练并评估了基线OPT-350M、SFT模型、DPO模型以及SFT+DPO组合模型。引入无害率（HmR）、有用率（HpR）和组合对齐得分（CAS）作为评估指标，均基于奖励模型输出。

Result: SFT性能优于DPO；SFT+DPO组合模型在所有指标上均超越其他模型，显示了这些技术的互补性。研究也指出数据噪声、GPU资源和训练限制带来的挑战。

Conclusion: 本研究全面阐述了微调策略对模型对齐的影响，为未来更稳健的对齐流程奠定了基础。

Abstract: This research investigates the effectiveness of alignment techniques,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a
combined SFT+DPO approach on improving the safety and helpfulness of the
OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,
we train and evaluate four models: the base OPT350M, an SFT model, a DPO model,
and a model trained with both SFT and DPO. We introduce three key evaluation
metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined
Alignment Score (CAS), all derived from reward model outputs. The results show
that while SFT outperforms DPO, The combined SFT+DPO model outperforms all
others across all metrics, demonstrating the complementary nature of these
techniques. Our findings also highlight challenges posed by noisy data, limited
GPU resources, and training constraints. This study offers a comprehensive view
of how fine-tuning strategies affect model alignment and provides a foundation
for more robust alignment pipelines in future work.

</details>


### [8] [MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction](https://arxiv.org/abs/2509.09082)
*Zhongqiu Li,Shiquan Wang,Ruiyu Fang,Mengjiao Bao,Zhenhe Wu,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: 针对LLMs在复杂信息抽取(UIE)中性能不足的问题，本文提出结合强化学习和多视角推理(MR-UIE)，将LLMs转变为主动推理器，实验证明MR-UIE显著提高提取准确性并超越现有SOTA，特别是在泛化能力方面。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型(LLMs)在多个领域能力强大，但在处理涉及复杂模式和多步推理的通用信息抽取(UIE)任务中，尤其是在结构化输出场景下，其性能仍显不足。现有上下文学习和指令微调等方法存在显著局限性。

Method: 本文提出将强化学习(RL)与多视角推理相结合，应用于信息抽取(IE)任务，命名为MR-UIE。该方法旨在将LLMs从被动提取器转变为主动推理器，使其不仅理解提取内容，更理解推理过程，以增强模型的泛化能力。

Result: MR-UIE在多个IE基准测试中持续提升了跨领域的提取准确性，并在多个数据集上超越了最先进的方法。此外，将多视角推理融入强化学习显著增强了复杂IE任务的泛化能力。

Conclusion: 结合多视角推理的强化学习方法(MR-UIE)能够有效提升LLMs在复杂信息抽取任务中的表现和泛化能力，凸显了推理在挑战性场景中的关键作用。

Abstract: Large language models (LLMs) demonstrate robust capabilities across diverse
research domains. However, their performance in universal information
extraction (UIE) remains insufficient, especially when tackling structured
output scenarios that involve complex schema descriptions and require
multi-step reasoning. While existing approaches enhance the performance of LLMs
through in-context learning and instruction tuning, significant limitations
nonetheless persist. To enhance the model's generalization ability, we propose
integrating reinforcement learning (RL) with multi-perspective reasoning for
information extraction (IE) tasks. Our work transitions LLMs from passive
extractors to active reasoners, enabling them to understand not only what to
extract but also how to reason. Experiments conducted on multiple IE benchmarks
demonstrate that MR-UIE consistently elevates extraction accuracy across
domains and surpasses state-of-the-art methods on several datasets.
Furthermore, incorporating multi-perspective reasoning into RL notably enhances
generalization in complex IE tasks, underscoring the critical role of reasoning
in challenging scenarios.

</details>


### [9] [TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla](https://arxiv.org/abs/2509.09101)
*Nishat Raihan,Antonios Anastasopoulos,Marcos Zampieri*

Main category: cs.CL

TL;DR: 本文针对孟加拉语在LLMs代码生成领域的不足，提出了首个孟加拉语专用代码大模型家族TigerCoder，通过构建高质量数据集和评估基准，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为全球第五大常用语言，在大型语言模型（LLMs）中，尤其是在代码生成方面，代表性不足。这主要是因为缺乏高质量的数据用于预训练和微调此类模型。

Method: 研究者引入了首个孟加拉语专用代码LLMs家族（TigerCoder，包含1B和9B模型），并做出了三项主要贡献：1) 创建了全面的孟加拉语代码指令数据集，用于编程领域适应；2) 开发了MBPP-Bangla，一个孟加拉语代码生成评估基准；3) 推出了TigerCoder代码LLMs家族。

Result: TigerCoder家族在Pass@1指标上比现有的多语言和通用孟加拉语LLMs实现了约11-18%的显著性能提升。研究结果表明，精心策划的高质量数据集可以克服低资源语言中小模型的局限性。

Conclusion: 高质量的定制数据集对于提升低资源语言LLMs的性能至关重要。所有研究资源均已开源，以促进孟加拉语LLM的进一步研究和发展。

Abstract: Despite being the 5th most spoken language, Bangla remains underrepresented
in Large Language Models (LLMs), particularly for code generation. This
primarily stems from the scarcity of high-quality data to pre-train and/or
finetune such models. Hence, we introduce the first dedicated family of Code
LLMs for Bangla (1B & 9B). We offer three major contributions: (1) a
comprehensive Bangla code instruction datasets for programming domain
adaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code
generation; and (3) the TigerCoder-family of Code LLMs, achieving significant
~11-18% performance gains at Pass@1 over existing multilingual and
general-purpose Bangla LLMs. Our findings show that curated, high-quality
datasets can overcome limitations of smaller models for low-resource languages.
We open-source all resources to advance further Bangla LLM research.

</details>


### [10] [Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia](https://arxiv.org/abs/2509.09121)
*Sophia Maria*

Main category: cs.CL

TL;DR: Compass-v3是一个245B参数的MoE模型，专为东南亚电商优化，通过独特架构、海量训练和新型对齐方法，在电商任务和多语言能力上超越现有SOTA模型，并已在Shopee平台大规模应用。


<details>
  <summary>Details</summary>
Motivation: 通用大型语言模型（LLMs）在专业领域（如电商）表现不佳，因为电商数据具有噪音大、异构、多语言和高度动态的特点，需要领域特定知识。

Method: 提出Compass-v3，一个245B总参数（71B活跃）的垂直领域MoE模型。采用更少但更大的专家模型，结合硬件优化（如节点内专家并行和定制memcpy操作）以最大化GPU利用率。模型在12T精心策划的多语言语料和大规模合成电商指令上进行混合策略训练。引入最优传输直接偏好优化（OTPO）以捕捉token级差异并提高电商场景的指令遵循性。

Result: Compass-v3在电商性能上达到SOTA，超越了DeepSeek-V3.1、GPT-4系列和Qwen3-235B。它在低资源东南亚语言（印尼语、泰语、菲律宾语、越南语、马来语、他加禄语）和葡萄牙语方面展现出强大的多语言能力，并在通用基准测试上保持竞争力。该模型已广泛应用于Shopee的工业级电商平台，并逐步替代OpenAI流量，目前占LLM总使用量的70%以上。

Conclusion: Compass-v3展现了专业电商领域的卓越能力和广泛的语言能力，证明了其在工业级电商平台中的强大有效性。

Abstract: Large language models (LLMs) excel in general-domain applications, yet their
performance often degrades in specialized tasks requiring domain-specific
knowledge. E-commerce is particularly challenging, as its data are noisy,
heterogeneous, multilingual, and highly dynamic. We present Compass-v3, a
vertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and
71B active per token, designed for Southeast Asian e-commerce. Compass-v3
adopts fewer but larger experts, combined with hardware-efficient
optimizations-such as intra-node expert parallelism and a customized memcpy
operator-to maximize GPU utilization. The model is trained on 12T tokens of
curated multilingual corpora and large-scale synthetic e-commerce instructions
using a mixed-training strategy. To enhance alignment, we propose
Optimal-Transport Direct Preference Optimization (OTPO), which captures
token-level distinctions and improves instruction adherence in
commerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3
delivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,
GPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong
multilingual capability across low-resource Southeast Asian languages
(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while
sustaining competitive performance on general benchmarks. It has already been
widely applied in Shopee's industrial-scale e-commerce platform and is
gradually replacing OpenAI's traffic, now accounting for over 70\% of total LLM
usage, highlighting its dual strengths in specialized commerce expertise and
broad linguistic competence.

</details>


### [11] [Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus](https://arxiv.org/abs/2509.09125)
*Liqun He,Jiaqi Xu*

Main category: cs.CL

TL;DR: 本研究探索了利用生成式AI自动分类教师对话行为(DA)，结果显示GPT-4表现出色，准确率达80%，加权F1为0.81，Kappa为0.74，表明其在教育对话分析中具有巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统人工编码教师对话行为（DA）耗时耗力的问题，寻求利用生成式AI实现自动化分类，以减少所需的时间和精力。

Method: 以开源CIMA语料库为案例，该语料库已预标注教师回应的四种DA类别。测试了GPT-3.5-turbo和GPT-4模型，并使用了定制的提示词进行分类。

Result: GPT-4模型取得了80%的准确率、0.81的加权F1分数和0.74的科恩Kappa系数。该性能超越了基线，并与人工标注达到实质性一致。

Conclusion: 生成式AI在DA分类方面展现出高效且可行的潜力，对教育对话分析具有重要意义。研究强调了任务特定标签定义和上下文信息的重要性，并指出使用生成式AI需考虑伦理因素，倡导负责任和透明的研究实践。

Abstract: This study explores the use of generative AI for automating the
classification of tutors' Dialogue Acts (DAs), aiming to reduce the time and
effort required by traditional manual coding. This case study uses the
open-source CIMA corpus, in which tutors' responses are pre-annotated into four
DA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored
prompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of
0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and
indicating substantial agreement with human annotations. These findings suggest
that generative AI has strong potential to provide an efficient and accessible
approach to DA classification, with meaningful implications for educational
dialogue analysis. The study also highlights the importance of task-specific
label definitions and contextual information in enhancing the quality of
automated annotation. Finally, it underscores the ethical considerations
associated with the use of generative AI and the need for responsible and
transparent research practices. The script of this research is publicly
available at
https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.

</details>


### [12] [ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking](https://arxiv.org/abs/2509.09131)
*Phuong-Nam Dang,Kieu-Linh Nguyen,Thanh-Hieu Pham*

Main category: cs.CL

TL;DR: 本文提出了ViRanker，一个专为越南语定制的交叉编码器重排序模型，解决了该语言重排序模型不足的问题，并在MMARCO-VI基准上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 越南语作为一种低资源语言，其复杂的语法和变音符号导致缺乏具有竞争力的重排序模型，从而影响了检索系统的性能。

Method: ViRanker模型基于BGE-M3编码器构建，并增强了Blockwise Parallel Transformer。该模型使用8GB精选语料库进行训练，并通过混合硬负采样进行微调，以提升鲁棒性。

Result: 在MMARCO-VI基准测试中，ViRanker在早期排名准确性方面表现出色，超越了多语言基线模型，并与PhoRanker表现接近。

Conclusion: ViRanker为越南语提供了一个有效的重排序解决方案，并已开源以支持复现性和实际应用。这项研究也为其他低资源语言的重排序模型开发提供了架构适应和数据整理方面的有益经验。

Abstract: This paper presents ViRanker, a cross-encoder reranking model tailored to the
Vietnamese language. Built on the BGE-M3 encoder and enhanced with the
Blockwise Parallel Transformer, ViRanker addresses the lack of competitive
rerankers for Vietnamese, a low-resource language with complex syntax and
diacritics. The model was trained on an 8 GB curated corpus and fine-tuned with
hybrid hard-negative sampling to strengthen robustness. Evaluated on the
MMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing
multilingual baselines and competing closely with PhoRanker. By releasing the
model openly on Hugging Face, we aim to support reproducibility and encourage
wider adoption in real-world retrieval systems. Beyond Vietnamese, this study
illustrates how careful architectural adaptation and data curation can advance
reranking in other underrepresented languages.

</details>


### [13] [LITcoder: A General-Purpose Library for Building and Comparing Encoding Models](https://arxiv.org/abs/2509.09152)
*Taha Binhuraib,Ruimin Gao,Anna A. Ivanova*

Main category: cs.CL

TL;DR: LITcoder是一个开源库，用于构建和评估神经编码模型，旨在简化连续刺激（如文本、语音）与大脑数据对齐、特征提取、模型训练及评估过程，并促进方法论的严谨性。


<details>
  <summary>Details</summary>
Motivation: 现有神经编码模型实现复杂，缺乏标准化工具，导致研究人员需要重复构建核心基础设施，难以进行系统性比较和方法论的严谨探索。

Method: 引入了LITcoder开源库，提供模块化管道，标准化了连续刺激（如文本、语音）与大脑数据的对齐、特征转换、特征映射到大脑数据以及模型性能评估的工具。它支持多种方法学设计选择，如大脑数据集、区域、刺激特征、下采样方法等，并集成了日志、绘图和实验追踪功能。通过将LITcoder应用于三个故事听力数据集（LeBel et al. (2023), Narratives, Little Prince）来展示其可扩展性和多功能性。

Result: LITcoder成功应用于多个故事听力数据集，证明了其可扩展性和多功能性。通过实践，揭示了构建连续fMRI数据编码模型的关键方法学选择，包括考虑TR扫描中的所有token、整合血流动力学滞后效应、使用最小化信息泄露的训练-测试划分，以及考虑头部运动对模型预测性的影响。

Conclusion: LITcoder降低了神经编码模型实现的技 术门槛，促进了模型和数据集之间的系统性比较，提升了方法论的严谨性，加速了高质量、高性能大脑活动预测模型的开发。

Abstract: We introduce LITcoder, an open-source library for building and benchmarking
neural encoding models. Designed as a flexible backend, LITcoder provides
standardized tools for aligning continuous stimuli (e.g., text and speech) with
brain data, transforming stimuli into representational features, mapping those
features onto brain data, and evaluating the predictive performance of the
resulting model on held-out data. The library implements a modular pipeline
covering a wide array of methodological design choices, so researchers can
easily compose, compare, and extend encoding models without reinventing core
infrastructure. Such choices include brain datasets, brain regions, stimulus
feature (both neural-net-based and control, such as word rate), downsampling
approaches, and many others. In addition, the library provides built-in
logging, plotting, and seamless integration with experiment tracking platforms
such as Weights & Biases (W&B). We demonstrate the scalability and versatility
of our framework by fitting a range of encoding models to three story listening
datasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore
the methodological choices critical for building encoding models for continuous
fMRI data, illustrating the importance of accounting for all tokens in a TR
scan (as opposed to just taking the last one, even when contextualized),
incorporating hemodynamic lag effects, using train-test splits that minimize
information leakage, and accounting for head motion effects on encoding model
predictivity. Overall, LITcoder lowers technical barriers to encoding model
implementation, facilitates systematic comparisons across models and datasets,
fosters methodological rigor, and accelerates the development of high-quality
high-performance predictive models of brain activity.
  Project page: https://litcoder-brain.github.io

</details>


### [14] [Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing](https://arxiv.org/abs/2509.09160)
*Zhiyue Liu,Fanrong Ma,Xin Ling*

Main category: cs.CL

TL;DR: 本文提出一个反事实增强去偏框架，通过反事实数据增强和自适应对比学习，解决了目标导向多模态情感分类中因文本偏差导致的虚假关联问题，显著提升了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的目标导向多模态情感分类方法过分依赖文本内容，且未充分考虑数据集（特别是词级语境）偏差，导致文本特征与输出标签之间存在虚假关联，从而损害了分类准确性。

Method: 我们引入了一个新颖的反事实增强去偏框架。该框架包含：1) 反事实数据增强策略，通过微调情感相关因果特征，生成细节匹配的图像-文本样本，以引导模型关注与情感相关的内容。2) 自适应去偏对比学习机制，用于从反事实数据中学习鲁棒特征并促使模型决策，有效减轻偏置词的影响。

Result: 在多个基准数据集上的实验结果表明，我们提出的方法优于现有最先进的基线方法。

Conclusion: 所提出的反事实增强去偏框架能有效减少虚假关联，并提高目标导向多模态情感分类的准确性。

Abstract: Target-oriented multimodal sentiment classification seeks to predict
sentiment polarity for specific targets from image-text pairs. While existing
works achieve competitive performance, they often over-rely on textual content
and fail to consider dataset biases, in particular word-level contextual
biases. This leads to spurious correlations between text features and output
labels, impairing classification accuracy. In this paper, we introduce a novel
counterfactual-enhanced debiasing framework to reduce such spurious
correlations. Our framework incorporates a counterfactual data augmentation
strategy that minimally alters sentiment-related causal features, generating
detail-matched image-text samples to guide the model's attention toward content
tied to sentiment. Furthermore, for learning robust features from
counterfactual data and prompting model decisions, we introduce an adaptive
debiasing contrastive learning mechanism, which effectively mitigates the
influence of biased words. Experimental results on several benchmark datasets
show that our proposed method outperforms state-of-the-art baselines.

</details>


### [15] [EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs](https://arxiv.org/abs/2509.09174)
*Yuhao Zhang,Yuhao Du,Zhanchen Dai,Xiangnan Ma,Kaiqi Kou,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: 本文提出EchoX模型，通过整合声学和语义学习来解决语音大语言模型（SLLMs）知识和推理能力退化的问题，并在知识问答任务上取得先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的语音到语音大语言模型（SLLMs）在知识和推理能力上表现出退化。研究者假设这种局限性源于当前SLLM训练范式未能弥合特征表示空间中的声学-语义鸿沟。

Method: 本文提出了EchoX模型，该模型通过利用语义表示并动态生成语音训练目标来解决现有问题，从而整合了声学和语义学习。

Result: 实验结果表明，EchoX模型在约六千小时的训练数据下，在多个基于知识的问答基准测试中取得了先进的性能。

Conclusion: EchoX通过整合声学和语义学习，成功地保留了语音大语言模型的强大推理能力，有效解决了SLLMs在知识和推理方面的退化问题。

Abstract: Speech-to-speech large language models (SLLMs) are attracting increasing
attention. Derived from text-based large language models (LLMs), SLLMs often
exhibit degradation in knowledge and reasoning capabilities. We hypothesize
that this limitation arises because current training paradigms for SLLMs fail
to bridge the acoustic-semantic gap in the feature representation space. To
address this issue, we propose EchoX, which leverages semantic representations
and dynamically generates speech training targets. This approach integrates
both acoustic and semantic learning, enabling EchoX to preserve strong
reasoning abilities as a speech LLM. Experimental results demonstrate that
EchoX, with about six thousand hours of training data, achieves advanced
performance on multiple knowledge-based question-answering benchmarks. The
project is available at https://github.com/FreedomIntelligence/EchoX.

</details>


### [16] [Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition](https://arxiv.org/abs/2509.09196)
*Chin Yuen Kwok,Jia Qi yip*

Main category: cs.CL

TL;DR: 本文提出一种前瞻性预测方法，通过避免复杂的奖励撤销步骤，改进了ASR模型对稀有词的识别，并在Whisper模型上实现了显著的词错误率下降。


<details>
  <summary>Details</summary>
Motivation: 传统的Trie基偏置方法在ASR稀有词识别中存在局限性，即需要对未能形成完整稀有词的早期奖励进行撤销，这仅限于束搜索且计算成本高昂，尤其对于大型解码器。

Method: 本文提出通过使ASR模型能够前瞻性地一次预测多个步骤来克服现有局限。这种方法能够更好地估计部分假设是否能形成完整的稀有词，从而完全避免了撤销步骤。实验中，作者使用10小时的合成数据对Whisper模型进行了微调。

Result: 该方法将NSC Part 2测试集上的词错误率从30.86%降低到12.19%。

Conclusion: 所提出的前瞻性预测方法有效解决了现有Trie基偏置的计算效率和撤销步骤问题，显著提升了ASR模型对稀有词的识别性能。

Abstract: Contextual biasing improves rare word recognition of ASR models by
prioritizing the output of rare words during decoding. A common approach is
Trie-based biasing, which gives "bonus scores" to partial hypothesis (e.g.
"Bon") that may lead to the generation of the rare word (e.g. "Bonham"). If the
full word ("Bonham") isn't ultimately recognized, the system revokes those
earlier bonuses. This revocation is limited to beam search and is
computationally expensive, particularly for models with large decoders. To
overcome these limitations, we propose adapting ASR models to look ahead and
predict multiple steps at once. This avoids the revocation step entirely by
better estimating whether a partial hypothesis will lead to the generation of
the full rare word. By fine-tuning Whisper with only 10 hours of synthetic
data, our method reduces the word error rate on the NSC Part 2 test set from
30.86% to 12.19%.

</details>


### [17] [Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function](https://arxiv.org/abs/2509.09197)
*Chin Yuen Kwok,Jia Qi Yip,Eng Siong Chng*

Main category: cs.CL

TL;DR: 本文提出了一种增强的TCPGen上下文偏置方法，结合关键字感知损失函数来训练偏置模块，从而显著提高了ASR模型对稀有词的识别能力，并有效降低了词错率。


<details>
  <summary>Details</summary>
Motivation: 稀有词识别是ASR中的一个挑战。尽管通过合成数据训练上下文偏置模块有效，但合成音频中的伪影可能导致过拟合。因此，需要一种改进的方法来提高稀有词识别，同时解决过拟合问题。

Method: 研究人员增强了基于TCPGen的上下文偏置方法，并提出了一种“关键字感知损失函数”来训练偏置模块。该损失函数包含两个互补的项：一个用于偏置词预测的掩码交叉熵项，以及一个用于检测偏置词位置的二元分类项。该方法将Whisper模型适配到10小时的合成数据上进行训练。

Result: 通过将Whisper模型适配10小时的合成数据，所提出的方法将NSC Part 2测试集上的词错率从29.71%显著降低到11.81%。

Conclusion: 所提出的结合增强TCPGen和关键字感知损失函数的上下文偏置方法，能够有效提高ASR模型在合成数据训练下对稀有词的识别性能，并成功缓解了因合成数据伪影导致的过拟合问题。

Abstract: Rare word recognition can be improved by adapting ASR models to synthetic
data that includes these words. Further improvements can be achieved through
contextual biasing, which trains and adds a biasing module into the model
architecture to prioritize rare words. While training the module on synthetic
rare word data is more effective than using non-rare-word data, it can lead to
overfitting due to artifacts in the synthetic audio. To address this, we
enhance the TCPGen-based contextual biasing approach and propose a
keyword-aware loss function that additionally focuses on biased words when
training biasing modules. This loss includes a masked cross-entropy term for
biased word prediction and a binary classification term for detecting biased
word positions. These two terms complementarily support the decoding of biased
words during inference. By adapting Whisper to 10 hours of synthetic data, our
method reduced the word error rate on the NSC Part 2 test set from 29.71% to
11.81%.

</details>


### [18] [GmSLM : Generative Marmoset Spoken Language Modeling](https://arxiv.org/abs/2509.09198)
*Talia Sternberg,Michael London,David Omer,Yossi Adi*

Main category: cs.CL

TL;DR: 本文针对狨猴复杂的发声交流，提出了一种优化的生成式口语语言模型GmSLM。GmSLM能生成逼真的狨猴发声，并有效区分真实与人工对话，为研究发声交流的神经基础提供了实用框架。


<details>
  <summary>Details</summary>
Motivation: 1. 狨猴的复杂发声交流挑战了非人类灵长类动物发声完全先天的观点，并展现出与人类言语相似的特征。
2. 狨猴提供了一个独特的机会，可以将其发声交流与大脑活动联系起来，以弥补人类言语和语言研究中大脑访问的困难。
3. 标准的大型语言模型（LLM）方法不直接适用于主要通过发声进行交流的狨猴。

Method: 1. 引入了生成式狨猴口语语言模型（Generative Marmoset Spoken Language Modeling, GmSLM），这是一个为狨猴发声交流优化的口语语言模型管线。
2. 设计了新颖的零样本评估指标，利用无监督的野外数据和弱标记的对话数据来评估GmSLM。
3. 将GmSLM的性能与基于人类语音的基本基线模型进行比较，以展示其优势。

Result: 1. GmSLM生成的发声在声学上与真实的重合成样本高度匹配。
2. GmSLM在下游任务中表现良好。
3. 尽管是完全无监督的，GmSLM能够有效区分真实和人工对话。

Conclusion: 1. GmSLM为进一步研究发声交流的神经基础提供了支持。
2. GmSLM提供了一个连接发声和大脑活动的实用框架。
3. GmSLM有望在神经科学、生物声学和进化生物学等未来工作中发挥作用。

Abstract: Marmoset monkeys exhibit complex vocal communication, challenging the view
that nonhuman primates vocal communication is entirely innate, and show similar
features of human speech, such as vocal labeling of others and turn-taking.
Studying their vocal communication offers a unique opportunity to link it with
brain activity-especially given the difficulty of accessing the human brain in
speech and language research. Since Marmosets communicate primarily through
vocalizations, applying standard LLM approaches is not straightforward. We
introduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized
spoken language model pipeline for Marmoset vocal communication. We designed a
novel zero-shot evaluation metrics using unsupervised in-the-wild data,
alongside weakly labeled conversational data, to assess GmSLM and demonstrate
its advantage over a basic human-speech-based baseline. GmSLM generated
vocalizations closely matched real resynthesized samples acoustically and
performed well on downstream tasks. Despite being fully unsupervised, GmSLM
effectively distinguish real from artificial conversations and may support
further investigations of the neural basis of vocal communication and provides
a practical framework linking vocalization and brain activity. We believe GmSLM
stands to benefit future work in neuroscience, bioacoustics, and evolutionary
biology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.

</details>


### [19] [CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling](https://arxiv.org/abs/2509.09199)
*Wenhao Li,Bangcheng Sun,Weihao Ye,Tianyi Zhang,Daohai Yu,Fei Chao,Rongrong Ji*

Main category: cs.CL

TL;DR: 本文提出CCF，一种上下文压缩框架，通过学习分层潜在表示实现高效长上下文语言建模，显著提高了吞吐量和内存效率。


<details>
  <summary>Details</summary>
Motivation: 将语言模型扩展到更长的上下文对于捕捉扩展语篇中的丰富依赖关系至关重要。然而，朴素的上下文扩展会带来显著的计算和内存负担，导致训练和推理效率低下。

Method: 本文提出CCF（Context Compression Framework），通过学习分层潜在表示来压缩上下文，在积极减少输入冗余的同时保留全局语义。CCF将分段语义聚合与键值内存编码相结合。为进一步增强可扩展性，引入了一种训练高效的优化策略，将增量分段解码与稀疏水库采样耦合，显著减少了内存开销。

Result: 在多个长上下文语言建模基准测试中，CCF在高压缩比下实现了有竞争力的困惑度，并且与现有方法相比，显著提高了吞吐量和内存效率。

Conclusion: 这些发现突显了结构化压缩对于可扩展和高效的长上下文语言建模的潜力。

Abstract: Scaling language models to longer contexts is essential for capturing rich
dependencies across extended discourse. However, na\"ive context extension
imposes significant computational and memory burdens, often resulting in
inefficiencies during both training and inference. In this work, we propose
CCF, a novel context compression framework designed to enable efficient
long-context modeling by learning hierarchical latent representations that
preserve global semantics while aggressively reducing input redundancy. CCF
integrates segment-wise semantic aggregation with key-value memory encoding,
forming compact representations that support accurate reconstruction and
long-range understanding. To further enhance scalability, we introduce a
training-efficient optimization strategy that couples incremental segment
decoding with sparse reservoir sampling, substantially reducing memory overhead
without degrading performance. Empirical results on multiple long-context
language modeling benchmarks demonstrate that CCF achieves competitive
perplexity under high compression ratios, and significantly improves throughput
and memory efficiency compared to existing approaches. These findings highlight
the potential of structured compression for scalable and effective long-context
language modeling.

</details>


### [20] [Reading Between the Lines: Classifying Resume Seniority with Large Language Models](https://arxiv.org/abs/2509.09229)
*Matan Cohen,Shira Shani,Eden Menahem,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: 本研究利用大语言模型（包括微调BERT）和混合数据集（真实与合成难例），旨在自动化简历中的资历分类，以应对夸大经验和模糊表达的问题。


<details>
  <summary>Details</summary>
Motivation: 从简历中准确评估候选人资历至关重要，但由于经验夸大和含糊的自我介绍，这项任务极具挑战性。

Method: 研究采用大语言模型（包括微调BERT架构）进行资历分类。为此，引入了一个混合数据集，包含真实简历和模拟夸大资历、低估经验的合成难例。在此数据集上评估模型检测资历膨胀和隐性专业知识相关细微语言线索的能力。

Result: 研究结果表明，该方法在增强AI驱动的候选人评估系统和减轻自我推销语言引入的偏见方面具有广阔前景。

Conclusion: 大语言模型在自动化简历资历评估方面表现出潜力，能有效应对经验夸大等问题，并有助于提高AI评估的公平性。研究数据集已公开。

Abstract: Accurately assessing candidate seniority from resumes is a critical yet
challenging task, complicated by the prevalence of overstated experience and
ambiguous self-presentation. In this study, we investigate the effectiveness of
large language models (LLMs), including fine-tuned BERT architectures, for
automating seniority classification in resumes. To rigorously evaluate model
performance, we introduce a hybrid dataset comprising both real-world resumes
and synthetically generated hard examples designed to simulate exaggerated
qualifications and understated seniority. Using the dataset, we evaluate the
performance of Large Language Models in detecting subtle linguistic cues
associated with seniority inflation and implicit expertise. Our findings
highlight promising directions for enhancing AI-driven candidate evaluation
systems and mitigating bias introduced by self-promotional language. The
dataset is available for the research community at https://bit.ly/4mcTovt

</details>


### [21] [Agentic LLMs for Question Answering over Tabular Data](https://arxiv.org/abs/2509.09234)
*Rishit Tyagi,Mohit Gupta,Rahul Bouri*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型（LLMs）的自然语言到SQL（NL-to-SQL）方法，用于表格问答（Table QA），并在SemEval 2025 DataBench基准测试中取得了显著优于基线的性能。


<details>
  <summary>Details</summary>
Motivation: 表格问答（Table QA）面临真实世界表格结构多样、规模和数据类型复杂等挑战。SemEval 2025 Task 8 (DataBench) 旨在通过大规模、领域多样的数据集评估模型准确回答结构化查询的能力，因此需要开发有效的解决方案。

Method: 采用一种自然语言到SQL（NL-to-SQL）方法，利用GPT-4o、GPT-4o-mini和DeepSeek v2:16b等大型语言模型动态生成SQL查询。该系统遵循多阶段流程，包括示例选择、SQL查询生成、答案提取、验证和迭代细化。

Result: 该方法在DataBench QA上实现了70.5%的准确率，在DataBench Lite QA上实现了71.6%的准确率，显著超越了基线分数（分别为26%和27%）。

Conclusion: 所提出的LLM驱动的NL-to-SQL方法在Table QA任务中表现出显著效果，为LLM在表格问答领域的应用提供了深入见解，并揭示了其优势与局限性。

Abstract: Question Answering over Tabular Data (Table QA) presents unique challenges
due to the diverse structure, size, and data types of real-world tables. The
SemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,
domain-diverse datasets to evaluate the ability of models to accurately answer
structured queries. We propose a Natural Language to SQL (NL-to-SQL) approach
leveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and
DeepSeek v2:16b to generate SQL queries dynamically. Our system follows a
multi-stage pipeline involving example selection, SQL query generation, answer
extraction, verification, and iterative refinement. Experiments demonstrate the
effectiveness of our approach, achieving 70.5\% accuracy on DataBench QA and
71.6\% on DataBench Lite QA, significantly surpassing baseline scores of 26\%
and 27\% respectively. This paper details our methodology, experimental
results, and alternative approaches, providing insights into the strengths and
limitations of LLM-driven Table QA.

</details>


### [22] [From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models](https://arxiv.org/abs/2509.09303)
*Grazia Sveva Ascione,Nicolò Tamagnone*

Main category: cs.CL

TL;DR: 本文提出一种弱监督方法，利用大语言模型和专利本体，通过分析专利与SDG相关科学文献的引用关系，自动生成大规模专利-SDG分类数据集，并经验证其优于现有基线方法，有效提升了SDG分类的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模标注数据集限制了专利-SDG分类中监督学习的应用，现有方法（如关键词搜索、迁移学习和引用启发式方法）存在可扩展性和泛化性不足的问题。

Method: 将专利-SDG分类视为弱监督问题，利用专利对SDG标记科学出版物的引用作为初始信号。开发了一个复合标签函数（LF），通过大语言模型（LLMs）根据专利本体从专利和SDG论文中提取结构化概念（功能、解决方案和应用），并使用基于排名的检索方法计算并组合跨领域相似度得分。该LF通过定制的正向损失函数进行校准，以与已知NPL-SDG链接对齐。

Result: 生成了一个银标准、软多标签数据集，用于将专利映射到SDG。通过两种策略验证：1) 内部验证显示该方法优于Transformer模型和零样本LLM等多个基线；2) 外部验证表明，其标签在专利引用、共同发明人和共同申请人网络中显示出比传统技术分类更大的主题、认知和组织连贯性。

Conclusion: 弱监督和语义对齐方法能够大规模提升SDG分类的效果和准确性。

Abstract: Classifying patents by their relevance to the UN Sustainable Development
Goals (SDGs) is crucial for tracking how innovation addresses global
challenges. However, the absence of a large, labeled dataset limits the use of
supervised learning. Existing methods, such as keyword searches, transfer
learning, and citation-based heuristics, lack scalability and generalizability.
This paper frames patent-to-SDG classification as a weak supervision problem,
using citations from patents to SDG-tagged scientific publications (NPL
citations) as a noisy initial signal. To address its sparsity and noise, we
develop a composite labeling function (LF) that uses large language models
(LLMs) to extract structured concepts, namely functions, solutions, and
applications, from patents and SDG papers based on a patent ontology.
Cross-domain similarity scores are computed and combined using a rank-based
retrieval approach. The LF is calibrated via a custom positive-only loss that
aligns with known NPL-SDG links without penalizing discovery of new SDG
associations. The result is a silver-standard, soft multi-label dataset mapping
patents to SDGs, enabling the training of effective multi-label regression
models. We validate our approach through two complementary strategies: (1)
internal validation against held-out NPL-based labels, where our method
outperforms several baselines including transformer-based models, and zero-shot
LLM; and (2) external validation using network modularity in patent citation,
co-inventor, and co-applicant graphs, where our labels reveal greater thematic,
cognitive, and organizational coherence than traditional technological
classifications. These results show that weak supervision and semantic
alignment can enhance SDG classification at scale.

</details>


### [23] [MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems](https://arxiv.org/abs/2509.09360)
*Channdeth Sok,David Luz,Yacine Haddam*

Main category: cs.CL

TL;DR: MetaRAG是一个针对RAG系统幻觉检测的元测试框架，通过分析答案的事实变异与检索上下文的一致性来发现幻觉，支持实时、黑盒部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的幻觉问题限制了企业应用的可靠性。现有幻觉检测方法主要针对独立LLMs，未能解决RAG系统需要响应与检索证据保持一致的独特挑战。

Method: 提出MetaRAG，一个实时、无监督、黑盒的元测试框架。其方法包括：1) 将答案分解为原子事实；2) 通过同义词和反义词替换生成事实变异；3) 对比变异与检索上下文的一致性进行验证；4) 聚合不一致性计算幻觉分数。MetaRAG还能定位幻觉发生的具体片段，支持身份感知AI。

Result: 在专有企业数据集上的实验证明，MetaRAG能有效检测RAG系统中的幻觉，从而支持RAG对话代理的可信部署。

Conclusion: MetaRAG为RAG系统提供了一种有效的实时、无监督、黑盒幻觉检测解决方案，特别适用于企业和高风险场景，并通过定位幻觉片段提高了RAG系统的可信度和对身份敏感查询的保障能力。

Abstract: Large Language Models (LLMs) are increasingly deployed in enterprise
applications, yet their reliability remains limited by hallucinations, i.e.,
confident but factually incorrect information. Existing detection approaches,
such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not
address the unique challenges of Retrieval-Augmented Generation (RAG) systems,
where responses must be consistent with retrieved evidence. We therefore
present MetaRAG, a metamorphic testing framework for hallucination detection in
Retrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,
unsupervised, black-box setting, requiring neither ground-truth references nor
access to model internals, making it suitable for proprietary and high-stakes
domains. The framework proceeds in four stages: (1) decompose answers into
atomic factoids, (2) generate controlled mutations of each factoid using
synonym and antonym substitutions, (3) verify each variant against the
retrieved context (synonyms are expected to be entailed and antonyms
contradicted), and (4) aggregate penalties for inconsistencies into a
response-level hallucination score. Crucially for identity-aware AI, MetaRAG
localizes unsupported claims at the factoid span where they occur (e.g.,
pregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),
allowing users to see flagged spans and enabling system designers to configure
thresholds and guardrails for identity-sensitive queries. Experiments on a
proprietary enterprise dataset illustrate the effectiveness of MetaRAG for
detecting hallucinations and enabling trustworthy deployment of RAG-based
conversational agents. We also outline a topic-based deployment design that
translates MetaRAG's span-level scores into identity-aware safeguards; this
design is discussed but not evaluated in our experiments.

</details>


### [24] [Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research](https://arxiv.org/abs/2509.09381)
*Molly R Petersen,Claire E Stevenson,Lonneke van der Plas*

Main category: cs.CL

TL;DR: 本文总结了认知科学中关于类比推理的关键理论，并将其与自然语言处理（NLP）研究联系起来，以期通过认知视角提升文本中的关系理解。


<details>
  <summary>Details</summary>
Motivation: 尽管类比推理的认知过程可与NLP概念关联，但NLP研究通常缺乏认知视角。作者旨在弥合这一差距，并利用认知理论解决NLP中的挑战，以优化关系理解而非过度依赖实体级相似性。

Method: 总结认知科学中类比推理过程的关键理论，并将其与当前自然语言处理研究相关联，阐述这些概念如何应用于NLP中的多项重大挑战。

Result: 证明类比推理的认知过程可轻松与NLP概念关联，并且这些概念对NLP中的多项重大挑战（非直接类比解决任务）具有重要意义。

Conclusion: 采纳认知视角来理解类比推理，可以指导NLP研究人员更好地优化文本中的关系理解，超越对实体层面相似性的过度依赖。

Abstract: Analogical reasoning is an essential aspect of human cognition. In this
paper, we summarize key theory about the processes underlying analogical
reasoning from the cognitive science literature and relate it to current
research in natural language processing. While these processes can be easily
linked to concepts in NLP, they are generally not viewed through a cognitive
lens. Furthermore, we show how these notions are relevant for several major
challenges in NLP research, not directly related to analogy solving. This may
guide researchers to better optimize relational understanding in text, as
opposed to relying heavily on entity-level similarity.

</details>


### [25] [Hierarchical Bracketing Encodings Work for Dependency Graphs](https://arxiv.org/abs/2509.09388)
*Ana Ezquerro,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: 本文重审了用于依存图解析的分层括号编码，该方法能将图编码为序列，实现线性时间解析，并显著减少标签空间，同时在多语言基准测试中展现出更好的精确匹配准确率。


<details>
  <summary>Details</summary>
Motivation: 现有图线性化方法在依存图解析中导致标签空间过大，因此需要一种实用方法来减少标签空间，同时保留图的结构信息并能处理重入、循环和空节点。

Method: 该研究重新审视了分层括号编码方法，将依存图编码为序列。该方法通过$n$个标注动作实现线性时间解析，并能够表示重入、循环和空节点。核心在于通过这种编码方式大幅减少标签空间。

Result: 与现有图线性化方法相比，该表示法显著减少了标签空间，同时保留了结构信息。在多语言和多形式基准测试中，取得了有竞争力的结果，并在精确匹配准确率方面持续优于其他方法。

Conclusion: 分层括号编码为依存图解析提供了一个实用且有效的方法，它不仅能显著减少标签空间，而且在多语言和多形式环境下，其精确匹配准确率优于现有方法。

Abstract: We revisit hierarchical bracketing encodings from a practical perspective in
the context of dependency graph parsing. The approach encodes graphs as
sequences, enabling linear-time parsing with $n$ tagging actions, and still
representing reentrancies, cycles, and empty nodes. Compared to existing graph
linearizations, this representation substantially reduces the label space while
preserving structural information. We evaluate it on a multilingual and
multi-formalism benchmark, showing competitive results and consistent
improvements over other methods in exact match accuracy.

</details>


### [26] [GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models](https://arxiv.org/abs/2509.09438)
*Zhaohan Zhang,Ziquan Liu,Ioannis Patras*

Main category: cs.CL

TL;DR: 本研究提出GrACE，一种生成式置信度提取方法，使大语言模型（LLMs）能够实时、可扩展、可靠地表达置信度。GrACE在开放式生成任务中实现了最佳判别能力和校准效果，并能提高决策准确性及减少测试时采样需求。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中（如医疗、金融），评估大语言模型（LLMs）的可靠性至关重要。现有置信度提取方法存在计算开销大或校准效果差的问题，不适用于实际部署。

Method: 提出了GrACE（Generative Approach to Confidence Elicitation）方法。GrACE通过模型末层隐藏状态与词汇表中特殊token嵌入之间的相似性实时表达置信度。该方法通过将置信度与准确性关联的校准目标进行微调，以实现置信度的校准。

Result: GrACE在三个LLMs和两个基准数据集上的实验表明，其产生的置信度在开放式生成任务中具有最佳的判别能力和校准效果，优于六种竞争方法，且无需额外采样或辅助模型。此外，基于GrACE的两种测试时扩展策略不仅提高了最终决策的准确性，还显著减少了所需样本数量。

Conclusion: GrACE作为一种可扩展、可靠、实时的LLM置信度估计解决方案，具有实际部署的潜力，能够帮助LLMs在高风险应用中安全可靠地运行。

Abstract: Assessing the reliability of Large Language Models (LLMs) by confidence
elicitation is a prominent approach to AI safety in high-stakes applications,
such as healthcare and finance. Existing methods either require expensive
computational overhead or suffer from poor calibration, making them impractical
and unreliable for real-world deployment. In this work, we propose GrACE, a
Generative Approach to Confidence Elicitation that enables scalable and
reliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in
which the model expresses confidence by the similarity between the last hidden
state and the embedding of a special token appended to the vocabulary, in
real-time. We fine-tune the model for calibrating the confidence with
calibration targets associated with accuracy. Experiments with three LLMs and
two benchmark datasets show that the confidence produced by GrACE achieves the
best discriminative capacity and calibration on open-ended generation tasks,
outperforming six competing methods without resorting to additional sampling or
an auxiliary model. Moreover, we propose two strategies for improving test-time
scaling based on confidence induced by GrACE. Experimental results show that
using GrACE not only improves the accuracy of the final decision but also
significantly reduces the number of required samples in the test-time scaling
scheme, indicating the potential of GrACE as a practical solution for deploying
LLMs with scalable, reliable, and real-time confidence estimation.

</details>


### [27] [Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation](https://arxiv.org/abs/2509.09473)
*Lucie Poláková,Martin Popel,Věra Kloudová,Michal Novák,Mariia Anisimova,Jiří Balhar*

Main category: cs.CL

TL;DR: EdUKate项目旨在为捷克中小学开发多语言学习材料，通过专门的捷克-乌克兰机器翻译系统，将多模态互动练习从捷克语翻译成乌克兰语、英语和德语，并在教育门户网站上免费提供。


<details>
  <summary>Details</summary>
Motivation: 为捷克非捷克语学生提供多语言学习材料的需求，特别是将互动教育内容翻译成乌克兰语、英语和德语，并开发适用于教育领域和复杂格式内容的定制化机器翻译系统。

Method: 结合数字教育、语言学、翻译研究和机器翻译，与学术机构和出版商合作。开发专门的捷克-乌克兰机器翻译系统，特别关注处理XML/PDF等格式内容及科技术语。进行了教师调查以了解学生需求，并对系统进行评估和部署。

Result: 开发并评估了定制化的捷克-乌克兰机器翻译系统；成功将多达9,000个多模态互动练习翻译成乌克兰语、英语和德语；将这些材料和应用实现在教育门户网站上；所有成果均免费向学生、教育工作者和研究人员开放；并获得了捷克教师关于非捷克语学生需求的调查结果。

Conclusion: EdUKate项目成功地创建并提供了免费的多语言学习材料和定制的机器翻译系统，有效满足了捷克非捷克语学生的教育需求，并为相关研究提供了应用基础。

Abstract: The EdUKate project combines digital education, linguistics, translation
studies, and machine translation to develop multilingual learning materials for
Czech primary and secondary schools. Launched through collaboration between a
major Czech academic institution and the country's largest educational
publisher, the project is aimed at translating up to 9,000 multimodal
interactive exercises from Czech into Ukrainian, English, and German for an
educational web portal. It emphasizes the development and evaluation of a
direct Czech-Ukrainian machine translation system tailored to the educational
domain, with special attention to processing formatted content such as XML and
PDF and handling technical and scientific terminology. We present findings from
an initial survey of Czech teachers regarding the needs of non-Czech-speaking
students and describe the system's evaluation and implementation on the web
portal. All resulting applications are freely available to students, educators,
and researchers.

</details>


### [28] [Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs](https://arxiv.org/abs/2509.09522)
*Vadim Zadykian,Bruno Andrade,Haithem Afli*

Main category: cs.CL

TL;DR: 本研究引入了一种结合密集句嵌入和知识图谱的自监督混合架构，用于解决简历推荐系统中职称匹配的语义文本关联（STR）问题。通过分层评估方法，结果显示，在语义关联度高的区域，增强型SBERT模型比基线模型将RMSE降低了25%，突出了知识图谱与文本嵌入结合的益处以及精细化区域性能分析的重要性。


<details>
  <summary>Details</summary>
Motivation: 语义文本关联（STR）需要捕捉文本间超越词汇相似度的细微关系。在简历推荐系统的关键挑战——职称匹配中，词汇重叠往往有限或具有误导性。因此，需要改进语义对齐和可解释性。此外，以往研究侧重于整体性能评估，未能提供细粒度的模型行为洞察。

Method: 本研究提出了一种自监督混合架构，结合了密集句嵌入和领域特定知识图谱（KGs）。知识图谱通过图神经网络进行集成。评估方法强调数据分层，将STR分数连续体划分为低、中、高语义关联度区域，进行细粒度的模型性能分析。对多种嵌入模型（带或不带知识图谱集成）进行了评估。

Result: 结果表明，经过微调并与知识图谱增强的SBERT模型在高STR区域产生了持续改进，与强基线模型相比，RMSE降低了25%。这不仅证明了将知识图谱与文本嵌入结合的优势，也突出了区域性能分析在理解模型行为方面的重要性。这种细粒度方法揭示了全局指标所隐藏的优缺点。

Conclusion: 结合知识图谱与文本嵌入对提升语义文本关联（特别是在高关联度区域）性能具有显著益处。分层或区域性性能分析对于全面理解模型行为和进行有针对性的模型选择至关重要。这种粒度化方法有助于HR系统和应用在公平性、可解释性和上下文匹配方面实现更优表现。

Abstract: Semantic Textual Relatedness (STR) captures nuanced relationships between
texts that extend beyond superficial lexical similarity. In this study, we
investigate STR in the context of job title matching - a key challenge in
resume recommendation systems, where overlapping terms are often limited or
misleading. We introduce a self-supervised hybrid architecture that combines
dense sentence embeddings with domain-specific Knowledge Graphs (KGs) to
improve both semantic alignment and explainability. Unlike previous work that
evaluated models on aggregate performance, our approach emphasizes data
stratification by partitioning the STR score continuum into distinct regions:
low, medium, and high semantic relatedness. This stratified evaluation enables
a fine-grained analysis of model performance across semantically meaningful
subspaces. We evaluate several embedding models, both with and without KG
integration via graph neural networks. The results show that fine-tuned SBERT
models augmented with KGs produce consistent improvements in the high-STR
region, where the RMSE is reduced by 25% over strong baselines. Our findings
highlight not only the benefits of combining KGs with text embeddings, but also
the importance of regional performance analysis in understanding model
behavior. This granular approach reveals strengths and weaknesses hidden by
global metrics, and supports more targeted model selection for use in Human
Resources (HR) systems and applications where fairness, explainability, and
contextual matching are essential.

</details>


### [29] [DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning](https://arxiv.org/abs/2509.09524)
*Daniil Ignatev,Nan Li,Hugh Mee Wong,Anh Dang,Shane Kaszefski Yaschuk*

Main category: cs.CL

TL;DR: 该论文介绍了DeMeVa团队在LeWiDi 2025共享任务中的方法，主要探索了基于大型语言模型的上下文学习（ICL）和基于RoBERTa的标签分布学习（LDL），以预测视角主义注释和软标签。


<details>
  <summary>Details</summary>
Motivation: 参与LeWiDi 2025共享任务，探索在存在分歧的标注数据中进行学习的有效方法，特别是预测注释者特定的标注（视角主义注释）和生成软标签。

Method: 1. 采用大型语言模型进行上下文学习（ICL），并比较了不同的示例采样策略。2. 采用RoBERTa进行标签分布学习（LDL），并评估了多种微调方法。

Result: 1. 上下文学习（ICL）能有效预测注释者特定的视角主义注释。2. 将ICL的预测聚合为软标签能获得有竞争力的性能。3. 标签分布学习（LDL）方法在软标签预测方面显示出前景。

Conclusion: 上下文学习（ICL）是预测视角主义注释和生成竞争性软标签的有效方法。标签分布学习（LDL）方法在软标签预测方面潜力巨大，值得视角主义研究社区进一步深入探索。

Abstract: This system paper presents the DeMeVa team's approaches to the third edition
of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et
al., 2025). We explore two directions: in-context learning (ICL) with large
language models, where we compare example sampling strategies; and label
distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we
evaluate several fine-tuning methods. Our contributions are twofold: (1) we
show that ICL can effectively predict annotator-specific annotations
(perspectivist annotations), and that aggregating these predictions into soft
labels yields competitive performance; and (2) we argue that LDL methods are
promising for soft label predictions and merit further exploration by the
perspectivist community.

</details>


### [30] [Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)](https://arxiv.org/abs/2509.09544)
*Paolo Pedinotti,Peter Baumann,Nathan Jessurun,Leslie Barrett,Enrico Santus*

Main category: cs.CL

TL;DR: 本文提出MetaGraph，一种通用方法，通过从金融NLP科学文献中提取知识图谱来分析研究趋势，并揭示了LLM在该领域发展的三个关键阶段。


<details>
  <summary>Details</summary>
Motivation: LLM快速重塑了金融NLP，导致任务、数据集和数据源激增，但传统调查已无法跟上这种转型，缺乏对研究趋势的结构化、可查询视图。

Method: 定义了金融NLP研究本体，并应用基于LLM的提取管道处理了681篇2022-2025年的论文，从而进行了大规模、数据驱动的分析，构建了MetaGraph知识图谱。

Result: MetaGraph揭示了金融NLP发展的三个关键阶段：早期LLM应用及任务/数据集创新；对LLM局限性的批判性反思；以及将外围技术日益整合到模块化系统中。

Conclusion: 该结构化视图为从业者和研究人员提供了金融NLP演变（包括新兴趋势、优先级转变和方法学转变）的清晰理解，并展示了在其他领域映射科学进展的可重用方法。

Abstract: Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling
new tasks and driving a proliferation of datasets and diversification of data
sources. Yet, this transformation has outpaced traditional surveys. In this
paper, we present MetaGraph, a generalizable methodology for extracting
knowledge graphs from scientific literature and analyzing them to obtain a
structured, queryable view of research trends. We define an ontology for
financial NLP research and apply an LLM-based extraction pipeline to 681 papers
(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals
three key phases: early LLM adoption and task/dataset innovation; critical
reflection on LLM limitations; and growing integration of peripheral techniques
into modular systems. This structured view offers both practitioners and
researchers a clear understanding of how financial NLP has evolved -
highlighting emerging trends, shifting priorities, and methodological
shifts-while also demonstrating a reusable approach for mapping scientific
progress in other domains.

</details>


### [31] [Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking](https://arxiv.org/abs/2509.09583)
*Brittany Harbison,Samuel Taubman,Travis Taylor,Ashok. K. Goel*

Main category: cs.CL

TL;DR: 本文提出一个基于GPT的零样本模型，通过分析论坛介绍帖来推断学生的大五人格特质，并将其集成到在线学习平台SAMI中，以期改善社交推荐效果。


<details>
  <summary>Details</summary>
Motivation: 在线课程环境阻碍了社交群体的自然形成，现有系统SAMI在促进学生连接方面受限于其不完整的“心智理论”，特别是无法推断学生个性，这限制了其推荐的相关性。

Method: 我们提出一个利用GPT零样本能力的人格检测模型，从在线课程论坛的介绍帖中推断学生的大五人格特质。该模型与现有模型进行性能基准测试，并将其集成到SAMI的实体匹配系统中，以实现人格信息化的社交推荐。

Result: 所提出的GPT模型在人格检测任务中表现出有效性。初步集成表明，人格特质可以补充现有的匹配因素，对社交推荐有所助益。

Conclusion: 人格特质能够作为现有的匹配因素的补充，但仍需要额外的评估来确定其对学生参与度和匹配质量的完整影响。

Abstract: Social connection is a vital part of learning, yet online course environments
present barriers to the organic formation of social groups. SAMI offers one
solution by facilitating student connections, but its effectiveness is
constrained by an incomplete Theory of Mind, limiting its ability to create an
effective mental model of a student. One facet of this is its inability to
intuit personality, which may influence the relevance of its recommendations.
To explore this, we propose a personality detection model utilizing GPTs
zero-shot capability to infer Big-Five personality traits from forum
introduction posts, often encouraged in online courses. We benchmark its
performance against established models, demonstrating its efficacy in this
task. Furthermore, we integrate this model into SAMIs entity-based matchmaking
system, enabling personality-informed social recommendations. Initial
integration suggests personality traits can complement existing matching
factors, though additional evaluation is required to determine their full
impact on student engagement and match quality.

</details>


### [32] [Fluent but Unfeeling: The Emotional Blind Spots of Language Models](https://arxiv.org/abs/2509.09593)
*Bangzhao Shu,Isha Joshi,Melissa Karnaze,Anh C. Pham,Ishita Kakkar,Sindhu Kothe,Arpine Hovasapian,Mai ElSherief*

Main category: cs.CL

TL;DR: 本研究引入EXPRESS数据集和评估框架，旨在解决大型语言模型（LLMs）在细粒度人类情绪对齐方面的评估空白。结果显示，LLMs在准确预测细粒度情绪方面仍面临挑战，且在捕捉上下文线索方面不如人类自述有效。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然语言理解方面能力出众，并在心理健康研究中日益普及，但现有研究多侧重于有限类别的情绪识别，未能评估LLMs能否与人类细粒度的情绪表达对齐，忽视了更微妙的情绪差异。

Method: 引入EXPRESS基准数据集，该数据集从Reddit社区收集，包含251个细粒度、自述的情绪标签。构建了一个全面的评估框架，检查LLM预测的情绪词，并使用既定情绪理论将其分解为八种基本情绪以进行细粒度比较。对主流LLMs在不同提示设置下进行了系统测试，并进行了定性分析。

Result: 研究发现，LLMs在准确预测与人类自述情绪一致的细粒度情绪方面仍具有挑战性。定性分析表明，尽管某些LLMs能生成符合既定情绪理论的情绪词，但在捕捉上下文线索方面不如人类自述有效。

Conclusion: 研究结果凸显了LLMs在细粒度情绪对齐方面的局限性，并为未来旨在增强LLMs上下文理解能力的研究提供了重要见解。

Abstract: The versatility of Large Language Models (LLMs) in natural language
understanding has made them increasingly popular in mental health research.
While many studies explore LLMs' capabilities in emotion recognition, a
critical gap remains in evaluating whether LLMs align with human emotions at a
fine-grained level. Existing research typically focuses on classifying emotions
into predefined, limited categories, overlooking more nuanced expressions. To
address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit
communities featuring 251 fine-grained, self-disclosed emotion labels. Our
comprehensive evaluation framework examines predicted emotion terms and
decomposes them into eight basic emotions using established emotion theories,
enabling a fine-grained comparison. Systematic testing of prevalent LLMs under
various prompt settings reveals that accurately predicting emotions that align
with human self-disclosed emotions remains challenging. Qualitative analysis
further shows that while certain LLMs generate emotion terms consistent with
established emotion theories and definitions, they sometimes fail to capture
contextual cues as effectively as human self-disclosures. These findings
highlight the limitations of LLMs in fine-grained emotion alignment and offer
insights for future research aimed at enhancing their contextual understanding.

</details>


### [33] [LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination](https://arxiv.org/abs/2509.09602)
*Yiqun T. Chen,Tyler H. McCormick,Li Liu,Abhirup Datta*

Main category: cs.CL

TL;DR: 本研究提出了LA-VA，一个结合大语言模型（LLMs）、传统算法和嵌入式分类的管道，旨在提高资源有限地区口头尸检（VA）的死因预测准确性。结果显示，GPT-5在成人、儿童和新生儿死因预测中表现最佳，准确率分别达到48.6%、50.5%和53.5%，比传统基线高出5-10%，表明LLM能显著提升VA准确性。


<details>
  <summary>Details</summary>
Motivation: 在医疗认证不可用的资源有限地区，口头尸检（VA）是估算死因的关键工具。当前方法可能存在准确性不足的问题，因此需要新的、更有效的方法来显著改善VA的准确性，以更好地支持全球健康监测。

Method: 研究提出了LA-VA，一个概念验证管道，它将大语言模型（LLMs）与传统算法方法和基于嵌入的分类相结合，以改进死因预测。研究使用了Population Health Metrics Research Consortium (PHMRC) 数据集，涵盖成人（7,580例）、儿童（1,960例）和新生儿（2,438例）三个年龄类别。评估了多种方法，包括GPT-5预测、LCVA基线、文本嵌入和元学习器集成。

Result: 研究结果表明，GPT-5取得了最高的个体性能，在成人、儿童和新生儿的平均测试点准确率分别为48.6%、50.5%和53.5%。GPT-5的表现比传统的统计机器学习基线高出5-10%。

Conclusion: 研究发现，简单的现成LLM辅助方法可以大幅提高口头尸检的准确性。这对于资源匮乏地区的全球健康监测具有重要的实际意义。

Abstract: Verbal autopsy (VA) is a critical tool for estimating causes of death in
resource-limited settings where medical certification is unavailable. This
study presents LA-VA, a proof-of-concept pipeline that combines Large Language
Models (LLMs) with traditional algorithmic approaches and embedding-based
classification for improved cause-of-death prediction. Using the Population
Health Metrics Research Consortium (PHMRC) dataset across three age categories
(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:
GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.
Our results demonstrate that GPT-5 achieves the highest individual performance
with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%
(Neonate), outperforming traditional statistical machine learning baselines by
5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches
could substantially improve verbal autopsy accuracy, with important
implications for global health surveillance in low-resource settings.

</details>


### [34] [Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems](https://arxiv.org/abs/2509.09629)
*Minghang Zhu,Zhengliang Shi,Zhiwei Xu,Shiguang Wu,Lingjie Wang,Pengjie Ren,Zhaochun Ren,Zhumin Chen*

Main category: cs.CL

TL;DR: 本文提出MOAT框架，通过迭代对齐优化大型语言模型多智能体系统的协作，解决了独立微调导致的协作问题，并在多项基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常独立微调多智能体系统中的各个智能体，导致它们之间存在能力差距和协调不良，难以有效解决复杂任务。

Method: 本文提出MOAT（Multi-Agent Joint Alignment Tuning）框架，通过迭代对齐来改进智能体协作。该框架包含两个阶段：1) 规划智能体对齐，优化规划智能体以生成更好地指导执行智能体的子目标序列；2) 执行智能体改进，利用执行智能体自身生成的子目标-动作对进行微调，以增强其泛化能力。理论分析证明了MOAT训练过程的非递减和渐进收敛性。

Result: 在六个基准测试中，MOAT均优于现有最佳基线，在held-in任务上平均性能提升3.1%，在held-out任务上平均性能提升4.4%。

Conclusion: MOAT框架通过联合对齐显著提升了多智能体系统中规划智能体和执行智能体之间的协作和泛化能力，从而更有效地解决复杂任务。

Abstract: The advancement of large language models (LLMs) has enabled the construction
of multi-agent systems to solve complex tasks by dividing responsibilities
among specialized agents, such as a planning agent for subgoal generation and a
grounding agent for executing tool-use actions. Most existing methods typically
fine-tune these agents independently, leading to capability gaps among them
with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint
Alignment Tuning framework that improves agents collaboration through iterative
alignment. MOAT alternates between two key stages: (1) Planning Agent
Alignment, which optimizes the planning agent to generate subgoal sequences
that better guide the grounding agent; and (2) Grounding Agent Improving, which
fine-tunes the grounding agent using diverse subgoal-action pairs generated by
the agent itself to enhance its generalization capablity. Theoretical analysis
proves that MOAT ensures a non-decreasing and progressively convergent training
process. Experiments across six benchmarks demonstrate that MOAT outperforms
state-of-the-art baselines, achieving average improvements of 3.1% on held-in
tasks and 4.4% on held-out tasks.

</details>


### [35] [All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens](https://arxiv.org/abs/2509.09650)
*Siddarth Mamidanna,Daking Rai,Ziyu Yao,Yilun Zhou*

Main category: cs.CL

TL;DR: 本研究揭示了大型语言模型在心算任务中存在一个“All-for-One”子图，其中有意义的计算发生得很晚，且仅在最后一个token处进行，该token在中间层接收来自其他token的信息。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在许多计算任务中表现出色，但其内部工作机制尚不清楚。理论上，自注意力和多层感知器允许每个token访问和计算基于所有前序token的信息，但实践中这种操作的程度如何？本研究旨在探究LLMs在心算任务中进行信息访问和计算的实际方式。

Method: 研究通过三个步骤进行：抑制初始层的输入特定token计算，限制中间层跨token位置的信息传输，以及强制所有计算在剩余层的最后一个token处发生。为此，论文提出了两种技术：上下文感知平均消融（Context-Aware Mean Ablation, CAMA）和基于注意力的窥视（Attention-Based Peeking, ABP）。

Result: 研究识别了一个名为“All-for-One”子图（AF1）的计算模式，该模式在各种心算任务上实现了高准确率。在该子图中，有意义的计算发生得非常晚（在深层），并且仅在最后一个token处进行，该token在少数特定的中间层接收来自其他token的信息。实验表明，该子图对于高模型性能是充分且必要的，可跨不同模型迁移，并适用于各种输入样式。对CAMA和ABP替代方案的消融研究揭示了它们相对于其他方法的独特优势。

Conclusion: LLMs在心算任务中采用了一种高效的“All-for-One”计算模式，其中核心计算推迟到后期，并由最后一个token集中完成，该token在特定中间层聚合了所有必要信息。这为理解LLMs处理此类任务的内部机制提供了重要见解。

Abstract: Large language models (LLMs) demonstrate proficiency across numerous
computational tasks, yet their inner workings remain unclear. In theory, the
combination of causal self-attention and multilayer perceptron layers allows
every token to access and compute information based on all preceding tokens. In
practice, to what extent are such operations present? In this paper, on mental
math tasks (i.e., direct math calculation via next-token prediction without
explicit reasoning), we investigate this question in three steps: inhibiting
input-specific token computations in the initial layers, restricting the routes
of information transfer across token positions in the next few layers, and
forcing all computation to happen at the last token in the remaining layers.
With two proposed techniques, Context-Aware Mean Ablation (CAMA) and
Attention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with
high accuracy on a wide variety of mental math tasks, where meaningful
computation occurs very late (in terms of layer depth) and only at the last
token, which receives information of other tokens in few specific middle
layers. Experiments on a variety of models and arithmetic expressions show that
this subgraph is sufficient and necessary for high model performance, transfers
across different models, and works on a variety of input styles. Ablations on
different CAMA and ABP alternatives reveal their unique advantages over other
methods, which may be of independent interest.

</details>


### [36] [Steering MoE LLMs via Expert (De)Activation](https://arxiv.org/abs/2509.09660)
*Mohsen Fayyaz,Ali Modarressi,Hanieh Deilamsalehy,Franck Dernoncourt,Ryan Rossi,Trung Bui,Hinrich Schütze,Nanyun Peng*

Main category: cs.CL

TL;DR: SteerMoE框架通过识别和控制MoE模型中与特定行为（如忠实度、安全性）相关的专家，实现无需重训的模型行为调控，并揭示了专家层中隐藏的对齐伪装。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型缺乏对其特定行为（如忠实度、安全性）的精细控制能力。本研究旨在开发一种无需重新训练或修改权重即可调控MoE模型行为的方法，并通过深入理解专家功能，探索其对模型对齐的影响。

Method: 提出了SteerMoE框架。该方法通过分析在展示对比行为的配对输入上的专家激活模式，识别出行为相关专家。在推理过程中，通过选择性地激活或去激活这些专家，实现对LLM忠实度和安全性的动态控制。

Result: 在11个基准测试和6个LLM上，SteerMoE将模型安全性提升高达20%，忠实度提升27%。在对抗性攻击模式下，单独使用可使安全性下降41%，与现有越狱方法结合则可使安全性下降100%，从而绕过所有安全防护，揭示了专家层中一种新的对齐伪装维度。

Conclusion: SteerMoE提供了一种有效且无需重训的模型行为调控机制，显著增强了MoE模型的忠实度和安全性。同时，其揭示了专家层内部存在的、可被操纵以绕过安全防护的对齐伪装，为理解和提升LLM安全性提出了新的挑战和研究方向。

Abstract: Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token
through a subset of specialized Feed-Forward Networks (FFN), known as experts.
We present SteerMoE, a framework for steering MoE models by detecting and
controlling behavior-linked experts. Our detection method identifies experts
with distinct activation patterns across paired inputs exhibiting contrasting
behaviors. By selectively (de)activating such experts during inference, we
control behaviors like faithfulness and safety without retraining or modifying
weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to
+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by
-41% alone, and -100% when combined with existing jailbreak methods, bypassing
all safety guardrails and exposing a new dimension of alignment faking hidden
within experts.

</details>


### [37] [CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2509.09675)
*Runpeng Dai,Linfeng Song,Haolin Liu,Zhenwen Liang,Dian Yu,Haitao Mi,Zhaopeng Tu,Rui Liu,Tong Zheng,Hongtu Zhu,Dong Yu*

Main category: cs.CL

TL;DR: 针对LLM的RLVR方法探索不足问题，本文提出了好奇心驱动探索（CDE）框架，利用actor的困惑度和critic的价值估计方差作为探索奖励，有效提升了探索能力，在AIME基准上实现了约3点的性能提升，并揭示了RLVR中的校准崩溃机制。


<details>
  <summary>Details</summary>
Motivation: 当前用于增强LLM推理能力的RLVR方法存在探索不足问题，这导致过早收敛和熵崩溃，从而限制了其性能和推理能力的提升。

Method: 论文提出了好奇心驱动探索（CDE）框架，通过以下两种信号量化好奇心：1. 对于actor：使用其生成响应的困惑度。2. 对于critic：使用多头架构中价值估计的方差。这两种信号都作为RLVR框架内的探索奖励，以引导模型进行更有效的探索。

Result: 1. 在AIME基准测试中，本文方法比使用GRPO/PPO的标准RLVR实现了约3点的性能提升。2. 理论分析表明，actor维度的奖励能惩罚过度自信的错误并促进正确响应的多样性；critic维度的奖励则与RL中成熟的基于计数的探索奖励相关联。3. 进一步分析揭示了RLVR中存在的“校准崩溃”机制，阐明了LLM常见的失败模式。

Conclusion: 好奇心驱动探索（CDE）框架通过引入新颖的基于actor和critic的好奇心信号，有效解决了RLVR在LLM中探索不足的问题。这不仅提升了LLM的推理性能，也深入揭示了其失败模式，使RLVR更加鲁棒。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm
for enhancing the reasoning ability of Large Language Models (LLMs). Yet
current RLVR methods often explore poorly, leading to premature convergence and
entropy collapse. To address this challenge, we introduce Curiosity-Driven
Exploration (CDE), a framework that leverages the model's own intrinsic sense
of curiosity to guide exploration. We formalize curiosity with signals from
both the actor and the critic: for the actor, we use perplexity over its
generated response, and for the critic, we use the variance of value estimates
from a multi-head architecture. Both signals serve as an exploration bonus
within the RLVR framework to guide the model. Our theoretical analysis shows
that the actor-wise bonus inherently penalizes overconfident errors and
promotes diversity among correct responses; moreover, we connect the
critic-wise bonus to the well-established count-based exploration bonus in RL.
Empirically, our method achieves an approximate +3 point improvement over
standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a
calibration collapse mechanism within RLVR, shedding light on common LLM
failure modes.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [38] [Recurrence Meets Transformers for Universal Multimodal Retrieval](https://arxiv.org/abs/2509.08897)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara*

Main category: cs.CV

TL;DR: ReT-2是一个统一的多模态检索模型，支持多模态查询和文档，通过多层表示和循环Transformer架构实现了最先进的性能，并提升了下游任务效果。


<details>
  <summary>Details</summary>
Motivation: 随着多模态检索技术及其在大型语言模型中的应用，检索任务日益复杂。现有方法主要依赖于任务特定的微调，且仅限于单模态查询或文档，无法有效处理同时包含图像和文本的多模态查询及文档。

Method: 本文提出了ReT-2模型，一个统一的检索模型，能够处理由图像和文本组成的多模态查询，并在包含文本和图像的多模态文档集合中进行搜索。ReT-2利用多层表示和带有LSTM启发门控机制的循环Transformer架构，动态整合跨层和跨模态信息，以捕获细粒度的视觉和文本细节。

Result: ReT-2在M2KR和M-BEIR基准测试上进行了评估，在不同检索配置下始终达到最先进的性能。与现有方法相比，它提供了更快的推理速度和更少的内存使用。当集成到检索增强生成管道中时，ReT-2还改进了Encyclopedic-VQA和InfoSeek数据集上的下游任务性能。

Conclusion: ReT-2是一个高效、统一的多模态检索模型，能够有效处理复杂的多模态查询和文档，并在多项基准测试和下游应用中展现出卓越的性能和效率优势。

Abstract: With the rapid advancement of multimodal retrieval and its application in
LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.
Existing methods predominantly rely on task-specific fine-tuning of
vision-language models and are limited to single-modality queries or documents.
In this paper, we propose ReT-2, a unified retrieval model that supports
multimodal queries, composed of both images and text, and searches across
multimodal document collections where text and images coexist. ReT-2 leverages
multi-layer representations and a recurrent Transformer architecture with
LSTM-inspired gating mechanisms to dynamically integrate information across
layers and modalities, capturing fine-grained visual and textual details. We
evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different
retrieval configurations. Results demonstrate that ReT-2 consistently achieves
state-of-the-art performance across diverse settings, while offering faster
inference and reduced memory usage compared to prior approaches. When
integrated into retrieval-augmented generation pipelines, ReT-2 also improves
downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source
code and trained models are publicly available at:
https://github.com/aimagelab/ReT-2

</details>


### [39] [Diffusion-Based Action Recognition Generalizes to Untrained Domains](https://arxiv.org/abs/2509.08908)
*Rogerio Guimaraes,Frank Xiao,Pietro Perona,Markus Marks*

Main category: cs.CV

TL;DR: 现有深度学习模型在跨物种、视角和上下文的动作识别泛化方面表现不佳。本文提出使用视觉扩散模型（VDM）生成的特征，并通过Transformer聚合，实现类人泛化，并在三个泛化基准上达到新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 人类能够识别跨物种（如蜘蛛和马的行走）、视角（如第一人称和第三人称）和上下文（如现实生活和电影）等大范围变化的相同动作，而当前深度学习模型难以实现这种泛化能力。

Method: 提出使用视觉扩散模型（VDM）生成的特征，并通过Transformer进行聚合，以实现在挑战性条件下的类人动作识别。研究发现，通过在扩散过程的早期时间步对模型进行条件化，可以突出提取特征中的语义信息而非像素级细节，从而增强泛化能力。

Result: 该模型在跨动物物种、不同视角和不同录制上下文的所有三个泛化基准上均达到了新的最先进水平（State-of-the-Art）。

Conclusion: 本文的方法显著提升了机器动作识别的泛化能力，使其更接近人类的鲁棒性。

Abstract: Humans can recognize the same actions despite large context and viewpoint
variations, such as differences between species (walking in spiders vs.
horses), viewpoints (egocentric vs. third-person), and contexts (real life vs
movies). Current deep learning models struggle with such generalization. We
propose using features generated by a Vision Diffusion Model (VDM), aggregated
via a transformer, to achieve human-like action recognition across these
challenging conditions. We find that generalization is enhanced by the use of a
model conditioned on earlier timesteps of the diffusion process to highlight
semantic information over pixel level details in the extracted features. We
experimentally explore the generalization properties of our approach in
classifying actions across animal species, across different viewing angles, and
different recording contexts. Our model sets a new state-of-the-art across all
three generalization benchmarks, bringing machine action recognition closer to
human-like robustness. Project page:
$\href{https://www.vision.caltech.edu/actiondiff/}{\texttt{vision.caltech.edu/actiondiff}}$
Code:
$\href{https://github.com/frankyaoxiao/ActionDiff}{\texttt{github.com/frankyaoxiao/ActionDiff}}$

</details>


### [40] [PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability](https://arxiv.org/abs/2509.08910)
*Tung Vu,Lam Nguyen,Quynh Dao*

Main category: cs.CV

TL;DR: 本文介绍PromptGuard，一个通过结合VulnGuard Prompt（基于真实世界数据对比学习）来主动预防大语言模型生成有害信息的模块化提示框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际应用中对弱势群体（如LGBTQ+个体、单亲家庭、边缘化社区）生成有害、偏见或误导性信息的风险日益增加，而现有安全方法无法在信息生成源头主动预防。

Method: 提出PromptGuard模块化提示框架，其核心是VulnGuard Prompt，一种混合技术，利用真实世界数据驱动的对比学习。VulnGuard整合GitHub仓库的少样本示例、伦理链式思考和自适应角色提示。该框架采用理论多目标优化，并包含输入分类、VulnGuard提示、伦理原则集成、外部工具交互、输出验证和用户-系统交互六个核心模块。论文还提供了数学形式化、收敛性证明和基于信息论的脆弱性分析。

Result: 理论证明表明，通过熵界和帕累托最优，有害信息减少了25-30%。PromptGuard构建了一个智能专家系统，实现实时危害预防，并为系统化的实证研究奠定了数学基础。

Conclusion: PromptGuard框架（包含VulnGuard Prompt）通过模块化提示和数据驱动的对比学习，为大语言模型提供了主动的实时危害预防机制，特别针对弱势群体，并为未来的实证研究奠定了坚实的数学基础。

Abstract: The proliferation of Large Language Models (LLMs) in real-world applications
poses unprecedented risks of generating harmful, biased, or misleading
information to vulnerable populations including LGBTQ+ individuals, single
parents, and marginalized communities. While existing safety approaches rely on
post-hoc filtering or generic alignment techniques, they fail to proactively
prevent harmful outputs at the generation source. This paper introduces
PromptGuard, a novel modular prompting framework with our breakthrough
contribution: VulnGuard Prompt, a hybrid technique that prevents harmful
information generation using real-world data-driven contrastive learning.
VulnGuard integrates few-shot examples from curated GitHub repositories,
ethical chain-of-thought reasoning, and adaptive role-prompting to create
population-specific protective barriers. Our framework employs theoretical
multi-objective optimization with formal proofs demonstrating 25-30% analytical
harm reduction through entropy bounds and Pareto optimality. PromptGuard
orchestrates six core modules: Input Classification, VulnGuard Prompting,
Ethical Principles Integration, External Tool Interaction, Output Validation,
and User-System Interaction, creating an intelligent expert system for
real-time harm prevention. We provide comprehensive mathematical formalization
including convergence proofs, vulnerability analysis using information theory,
and theoretical validation framework using GitHub-sourced datasets,
establishing mathematical foundations for systematic empirical research.

</details>


### [41] [Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures](https://arxiv.org/abs/2509.08926)
*Waqar Ahmad,Evan Murphy,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: 本文提出Beta-SOD框架，通过将Re-ID重构为相似性任务，并结合基于两分量Beta混合分布的统计异常检测和Siamese网络来处理标签噪声。在人员和车辆Re-ID任务中，Beta-SOD在不同噪声水平下均优于现有SOTA方法，展现出鲁棒性和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 目标重识别（Re-ID）方法对标签噪声高度敏感，这通常会导致性能显著下降，因此需要有效的方法来应对这一挑战。

Method: 本文将Re-ID重构为监督图像相似性任务，并采用Siamese网络架构。核心是提出新颖的统计异常检测（OD）框架——Beta-SOD，该框架使用两分量Beta分布混合模型对嵌入对之间的余弦相似性分布进行建模。同时，确立了双Beta分布混合模型的可识别性结果。该OD步骤与结合二元交叉熵、对比损失和余弦嵌入损失的Re-ID架构相结合，共同优化特征级相似性学习。

Result: Beta-SOD在CUHK03、Market-1501（人员Re-ID）和VeRi-776（车辆Re-ID）数据集的去噪和Re-ID任务中均表现出有效性。该方法在10-30%的各种噪声水平下均优于现有最先进方法，显示出在嘈杂Re-ID场景中的鲁棒性和广泛适用性。

Conclusion: 本文通过提出Beta-SOD框架，成功解决了目标重识别（Re-ID）中标签噪声导致的性能下降问题。该方法通过统计异常检测和结合多种损失函数的Siamese网络，在处理噪声方面展现出优越的性能、鲁棒性和广泛适用性，为嘈杂Re-ID场景提供了有效解决方案。

Abstract: Object re-identification (Re-ID) methods are highly sensitive to label noise,
which typically leads to significant performance degradation. We address this
challenge by reframing Re-ID as a supervised image similarity task and adopting
a Siamese network architecture trained to capture discriminative pairwise
relationships. Central to our approach is a novel statistical outlier detection
(OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier
Detection), which models the distribution of cosine similarities between
embedding pairs using a two-component Beta distribution mixture model. We
establish a novel identifiability result for mixtures of two Beta
distributions, ensuring that our learning task is well-posed.The proposed OD
step complements the Re-ID architecture combining binary cross-entropy,
contrastive, and cosine embedding losses that jointly optimize feature-level
similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising
and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and
vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance
compared to the state-of-the-art methods across various noise levels (10-30\%),
demonstrating both robustness and broad applicability in noisy Re-ID scenarios.
The implementation of Beta-SOD is available at:
https://github.com/waqar3411/Beta-SOD

</details>


### [42] [SFD-Mamba2Net: Strcture-Guided Frequency-Enhanced Dual-Stream Mamba2 Network for Coronary Artery Segmentation](https://arxiv.org/abs/2509.08934)
*Nan Mu,Ruiqi Song,Zhihui Xu,Jingfeng Jiang,Chen Zhao*

Main category: cs.CV

TL;DR: 本研究提出了SFD-Mamba2Net，一个端到端框架，通过整合多尺度结构先验、状态空间模型和频域细节增强策略，显著提高了冠状动脉造影（ICA）图像中的冠状动脉分割和狭窄检测精度，超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉疾病（CAD）是全球主要死因之一。侵入性冠状动脉造影（ICA）作为诊断金标准，需要精确的血管分割和狭窄检测。然而，ICA图像普遍存在低对比度、高噪声和复杂精细的血管结构等挑战，使得现有分割和检测方法难以在临床应用中推广。因此，本研究旨在提高ICA图像中冠状动脉分割和狭窄检测的准确性。

Method: 本研究提出了SFD-Mamba2Net，一个专为基于ICA的血管分割和狭窄检测设计的端到端框架。在编码器中，嵌入了曲率感知结构增强（CASE）模块，利用多尺度响应来突出细长管状血管结构、抑制背景干扰并引导注意力到血管区域。在解码器中，引入了渐进式高频感知（PHFP）模块，采用多级小波分解逐步细化高频细节，同时整合低频全局结构。该方法还整合了多尺度结构先验、基于状态空间的长距离依赖建模和频域细节增强策略。

Result: SFD-Mamba2Net在八项分割指标上持续优于现有最先进方法，并在狭窄检测中取得了最高的真阳性率和阳性预测值。

Conclusion: SFD-Mamba2Net在冠状动脉造影图像中的冠状动脉分割和狭窄检测方面表现出卓越的性能，显著提高了诊断准确性，并超越了现有SOTA方法。

Abstract: Background: Coronary Artery Disease (CAD) is one of the leading causes of
death worldwide. Invasive Coronary Angiography (ICA), regarded as the gold
standard for CAD diagnosis, necessitates precise vessel segmentation and
stenosis detection. However, ICA images are typically characterized by low
contrast, high noise levels, and complex, fine-grained vascular structures,
which pose significant challenges to the clinical adoption of existing
segmentation and detection methods. Objective: This study aims to improve the
accuracy of coronary artery segmentation and stenosis detection in ICA images
by integrating multi-scale structural priors, state-space-based long-range
dependency modeling, and frequency-domain detail enhancement strategies.
Methods: We propose SFD-Mamba2Net, an end-to-end framework tailored for
ICA-based vascular segmentation and stenosis detection. In the encoder, a
Curvature-Aware Structural Enhancement (CASE) module is embedded to leverage
multi-scale responses for highlighting slender tubular vascular structures,
suppressing background interference, and directing attention toward vascular
regions. In the decoder, we introduce a Progressive High-Frequency Perception
(PHFP) module that employs multi-level wavelet decomposition to progressively
refine high-frequency details while integrating low-frequency global
structures. Results and Conclusions: SFD-Mamba2Net consistently outperformed
state-of-the-art methods across eight segmentation metrics, and achieved the
highest true positive rate and positive predictive value in stenosis detection.

</details>


### [43] [Live(r) Die: Predicting Survival in Colorectal Liver Metastasis](https://arxiv.org/abs/2509.08935)
*Muhammad Alberb,Helen Cheung,Anne Martel*

Main category: cs.CV

TL;DR: 开发了一种全自动框架，结合MRI图像分割和影像组学，用于预测结直肠癌肝转移术后生存期，效果优于现有生物标志物。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌肝转移（CRLM）患者术后预后差异大，现有基于有限临床或分子特征的预后模型预测能力不足，尤其在多发性CRLM病例中。

Method: 本研究提出了一个全自动框架，用于术前MRI图像的术后生存期预测。该框架包含分割管线和影像组学管线。分割管线利用可提示基础模型和新型零样本3D提示传播算法SAMONAI（基于Segment Anything Model），从部分标注数据中高效精确地分割肝脏、肿瘤和脾脏。影像组学管线则从分割出的肿瘤中提取特征，并使用新型基于自编码器多实例神经网络SurvAMINN进行生存分析，该网络能同时进行降维和风险预测，并专注于最具侵袭性的肿瘤。

Result: 在包含227名患者的机构数据集上的广泛评估表明，该框架的预测性能超越了现有临床和基因组生物标志物，C-index提升超过10%。

Conclusion: 研究结果表明，整合自动化分割算法和基于影像组学的生存分析，在结直肠癌肝转移的预后预测中，具有提供准确、标注高效且可解释性预测的潜力。

Abstract: Colorectal cancer frequently metastasizes to the liver, significantly
reducing long-term survival. While surgical resection is the only potentially
curative treatment for colorectal liver metastasis (CRLM), patient outcomes
vary widely depending on tumor characteristics along with clinical and genomic
factors. Current prognostic models, often based on limited clinical or
molecular features, lack sufficient predictive power, especially in multifocal
CRLM cases. We present a fully automated framework for surgical outcome
prediction from pre- and post-contrast MRI acquired before surgery. Our
framework consists of a segmentation pipeline and a radiomics pipeline. The
segmentation pipeline learns to segment the liver, tumors, and spleen from
partially annotated data by leveraging promptable foundation models to complete
missing labels. Also, we propose SAMONAI, a novel zero-shot 3D prompt
propagation algorithm that leverages the Segment Anything Model to segment 3D
regions of interest from a single point prompt, significantly improving our
segmentation pipeline's accuracy and efficiency. The predicted pre- and
post-contrast segmentations are then fed into our radiomics pipeline, which
extracts features from each tumor and predicts survival using SurvAMINN, a
novel autoencoder-based multiple instance neural network for survival analysis.
SurvAMINN jointly learns dimensionality reduction and hazard prediction from
right-censored survival data, focusing on the most aggressive tumors. Extensive
evaluation on an institutional dataset comprising 227 patients demonstrates
that our framework surpasses existing clinical and genomic biomarkers,
delivering a C-index improvement exceeding 10%. Our results demonstrate the
potential of integrating automated segmentation algorithms and radiomics-based
survival analysis to deliver accurate, annotation-efficient, and interpretable
outcome prediction in CRLM.

</details>


### [44] [Discovering Divergent Representations between Text-to-Image Models](https://arxiv.org/abs/2509.08940)
*Lisa Dunlap,Joseph E. Gonzalez,Trevor Darrell,Fabian Caba Heilbron,Josef Sivic,Bryan Russell*

Main category: cs.CV

TL;DR: 本研究探讨了文本到图像生成模型视觉表征的分歧，并提出CompCon进化搜索算法来发现模型特有的视觉属性及其触发提示。


<details>
  <summary>Details</summary>
Motivation: 旨在理解不同文本到图像生成模型学习到的视觉表征何时以及如何产生分歧，并具体发现一种模型特有而另一种模型没有的视觉属性，以及触发这些差异的提示类型。

Method: 引入CompCon（Comparing Concepts）进化搜索算法，用于发现特定模型输出中更普遍的视觉属性，并揭示与这些视觉差异相关的提示概念。通过创建ID2数据集（包含60个输入依赖性差异）并与基于LLM和VLM的基线进行比较来评估CompCon。

Result: CompCon成功发现了流行文本到图像模型间的视觉表征分歧，例如PixArt在处理提及孤独的提示时会生成湿润街道，而Stable Diffusion 3.5在描绘媒体行业的非洲裔美国人时有其特定表现。

Conclusion: CompCon算法能够有效识别不同文本到图像生成模型之间学习到的视觉表征的分歧，并揭示导致这些差异的提示概念。

Abstract: In this paper, we investigate when and how visual representations learned by
two different generative models diverge. Given two text-to-image models, our
goal is to discover visual attributes that appear in images generated by one
model but not the other, along with the types of prompts that trigger these
attribute differences. For example, "flames" might appear in one model's
outputs when given prompts expressing strong emotions, while the other model
does not produce this attribute given the same prompts. We introduce CompCon
(Comparing Concepts), an evolutionary search algorithm that discovers visual
attributes more prevalent in one model's output than the other, and uncovers
the prompt concepts linked to these visual differences. To evaluate CompCon's
ability to find diverging representations, we create an automated data
generation pipeline to produce ID2, a dataset of 60 input-dependent
differences, and compare our approach to several LLM- and VLM-powered
baselines. Finally, we use CompCon to compare popular text-to-image models,
finding divergent representations such as how PixArt depicts prompts mentioning
loneliness with wet streets and Stable Diffusion 3.5 depicts African American
people in media professions. Code at: https://github.com/adobe-research/CompCon

</details>


### [45] [An U-Net-Based Deep Neural Network for Cloud Shadow and Sun-Glint Correction of Unmanned Aerial System (UAS) Imagery](https://arxiv.org/abs/2509.08949)
*Yibin Wang,Wondimagegn Beshah,Padmanava Dash,Haifeng Wang*

Main category: cs.CV

TL;DR: 本研究提出一种基于U-Net深度学习的机器学习方法，用于识别、分离并校正无人机图像中由云影和太阳反光造成的缺陷，以提高水质参数估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 无人机图像虽能在多云条件下获取，但常受云影和水面太阳反光影响，严重阻碍了利用这些图像进行水质参数估计，因此需要一种有效方法来解决这些问题。

Method: 本研究提出一种新颖的机器学习方法，首先识别并提取图像中受云影和太阳反光影响的区域，并将其与未受影响的区域分离。通过提取像素级数据训练一个基于U-Net的深度学习模型，并根据多种评估指标确定模型训练的最佳设置。

Result: 通过评估，成功确定了一个高质量的图像校正模型。

Conclusion: 该模型能够有效恢复无人机图像中受云影和太阳反光影响的区域，从而解决了图像缺陷问题。

Abstract: The use of unmanned aerial systems (UASs) has increased tremendously in the
current decade. They have significantly advanced remote sensing with the
capability to deploy and image the terrain as per required spatial, spectral,
temporal, and radiometric resolutions for various remote sensing applications.
One of the major advantages of UAS imagery is that images can be acquired in
cloudy conditions by flying the UAS under the clouds. The limitation to the
technology is that the imagery is often sullied by cloud shadows. Images taken
over water are additionally affected by sun glint. These are two pose serious
issues for estimating water quality parameters from the UAS images. This study
proposes a novel machine learning approach first to identify and extract
regions with cloud shadows and sun glint and separate such regions from
non-obstructed clear sky regions and sun-glint unaffected regions. The data was
extracted from the images at pixel level to train an U-Net based deep learning
model and best settings for model training was identified based on the various
evaluation metrics from test cases. Using this evaluation, a high-quality image
correction model was determined, which was used to recover the cloud shadow and
sun glint areas in the images.

</details>


### [46] [CoSwin: Convolution Enhanced Hierarchical Shifted Window Attention For Small-Scale Vision](https://arxiv.org/abs/2509.08959)
*Puskal Khadka,Rodrigue Rizk,Longwei Wang,KC Santosh*

Main category: cs.CV

TL;DR: 针对ViTs在小数据集上局部特征提取的不足，本文提出CoSwin，通过融合局部卷积特征学习与全局自注意力，显著提升了在多个图像分类基准上的性能，证明了局部-全局特征融合的有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer（ViTs）通过自注意力机制在建模长距离依赖方面表现出色，但在小数据集上，由于缺乏局部性和平移不变性等关键归纳偏置，其对全局上下文的强调往往牺牲了局部特征提取能力。

Method: 本文提出了CoSwin，一种新颖的特征融合架构，它通过局部卷积特征学习来增强分层移位窗口注意力。具体而言，CoSwin在每个注意力块中集成了一个可学习的局部特征增强模块，使模型能够同时捕获细粒度的空间细节和全局语义结构。

Result: CoSwin在CIFAR-10、CIFAR-100、MNIST、SVHN和Tiny ImageNet等多个图像分类基准上进行了评估，结果显示其性能持续优于现有最先进的卷积和基于Transformer的模型。与基线Swin Transformer相比，CoSwin在CIFAR-10上提升2.17%，CIFAR-100提升4.92%，MNIST提升0.10%，SVHN提升0.26%，Tiny ImageNet提升4.47%。

Conclusion: 这些改进强调了局部-全局特征融合在增强Transformer对小规模视觉任务的泛化能力和鲁棒性方面的有效性。

Abstract: Vision Transformers (ViTs) have achieved impressive results in computer
vision by leveraging self-attention to model long-range dependencies. However,
their emphasis on global context often comes at the expense of local feature
extraction in small datasets, particularly due to the lack of key inductive
biases such as locality and translation equivariance. To mitigate this, we
propose CoSwin, a novel feature-fusion architecture that augments the
hierarchical shifted window attention with localized convolutional feature
learning. Specifically, CoSwin integrates a learnable local feature enhancement
module into each attention block, enabling the model to simultaneously capture
fine-grained spatial details and global semantic structure. We evaluate CoSwin
on multiple image classification benchmarks including CIFAR-10, CIFAR-100,
MNIST, SVHN, and Tiny ImageNet. Our experimental results show consistent
performance gains over state-of-the-art convolutional and transformer-based
models. Notably, CoSwin achieves improvements of 2.17% on CIFAR-10, 4.92% on
CIFAR-100, 0.10% on MNIST, 0.26% on SVHN, and 4.47% on Tiny ImageNet over the
baseline Swin Transformer. These improvements underscore the effectiveness of
local-global feature fusion in enhancing the generalization and robustness of
transformers for small-scale vision. Code and pretrained weights available at
https://github.com/puskal-khadka/coswin

</details>


### [47] [iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning](https://arxiv.org/abs/2509.08982)
*Karim Slimani,Catherine Achard,Brahim Tamadazte*

Main category: cs.CV

TL;DR: 本文提出了iMatcher，一个用于点云配准中特征匹配的完全可微分框架，它利用学习特征结合局部和全局一致性预测几何一致的置信度矩阵，显著提升了刚性配准性能并达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 提高点云配准中特征匹配的准确性和鲁棒性，特别是在复杂场景下，通过有效整合局部和全局几何一致性，以实现更精确的刚性配准。

Method: iMatcher利用学习特征预测一个结合局部和全局几何一致性的置信度矩阵。具体步骤包括：首先，通过局部图嵌入模块初始化得分矩阵；其次，通过考虑3D空间中双向（源到目标和目标到源）最近邻搜索进行重新定位，细化该矩阵；最后，将配对点特征堆叠，并通过全局几何一致性学习预测逐点匹配概率。

Result: iMatcher在KITTI、KITTI-360、3DMatch等真实世界数据集上，以及6-DoF姿态估计和局部到局部匹配任务中，显著提升了刚性配准性能。该方法实现了最先进的内点率，在KITTI上达到95%-97%，KITTI-360上达到94%-97%，3DMatch上高达81.1%，展现出在多样化设置下的强大鲁棒性。

Conclusion: iMatcher通过其创新的可微分框架和对局部、全局几何一致性的有效整合，成功解决了点云特征匹配中的关键挑战，在刚性配准方面取得了显著突破，达到了业界领先水平，并展现了在各种场景下的卓越鲁棒性。

Abstract: This paper presents iMatcher, a fully differentiable framework for feature
matching in point cloud registration. The proposed method leverages learned
features to predict a geometrically consistent confidence matrix, incorporating
both local and global consistency. First, a local graph embedding module leads
to an initialization of the score matrix. A subsequent repositioning step
refines this matrix by considering bilateral source-to-target and
target-to-source matching via nearest neighbor search in 3D space. The paired
point features are then stacked together to be refined through global geometric
consistency learning to predict a point-wise matching probability. Extensive
experiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch)
datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partial
matching (MVP-RG), demonstrate that iMatcher significantly improves rigid
registration performance. The method achieves state-of-the-art inlier ratios,
scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch,
highlighting its robustness across diverse settings.

</details>


### [48] [UltrON: Ultrasound Occupancy Networks](https://arxiv.org/abs/2509.08991)
*Magdalena Wysocki,Felix Duelmer,Ananya Bal,Nassir Navab,Mohammad Farid Azampour*

Main category: cs.CV

TL;DR: 本文提出`UltrON`模型，通过利用声学特征和新颖的损失函数，在弱监督下实现多视图超声图像的3D重建，有效解决了现有方法对精确标注的依赖和视角依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的隐式表示方法在超声3D重建中，过度依赖精确的B模式图像分割标注，忽视了B模式强度中丰富的声学信息；同时，这些方法难以处理超声固有的视角依赖性和声学阴影伪影，导致重建精度受损。本研究旨在解决因遮挡和标注依赖性带来的挑战。

Method: 本研究提出了一种基于占据率（occupancy-based）的表示方法，并引入了`UltrON`模型。`UltrON`通过利用可从B模式图像中获取（无需额外标注）的声学特征，在弱监督优化机制下提高了几何一致性。此外，模型还提出了一种新颖的损失函数，以补偿B模式图像中的视角依赖性，并促进多视图超声的占据率优化。

Result: `UltrON`通过结合声学特性，能够泛化到相同解剖结构的不同形状。实验结果表明，`UltrON`有效缓解了遮挡和稀疏标注的局限性，并为更精确的3D重建铺平了道路。

Conclusion: `UltrON`模型通过创新性地利用B模式图像的声学特征和提出补偿视角依赖性的损失函数，成功克服了传统超声3D重建中精确标注依赖和视角伪影的挑战，实现了在弱监督下更准确的几何重建，并具有良好的泛化能力。

Abstract: In free-hand ultrasound imaging, sonographers rely on expertise to mentally
integrate partial 2D views into 3D anatomical shapes. Shape reconstruction can
assist clinicians in this process. Central to this task is the choice of shape
representation, as it determines how accurately and efficiently the structure
can be visualized, analyzed, and interpreted. Implicit representations, such as
SDF and occupancy function, offer a powerful alternative to traditional voxel-
or mesh-based methods by modeling continuous, smooth surfaces with compact
storage, avoiding explicit discretization. Recent studies demonstrate that SDF
can be effectively optimized using annotations derived from segmented B-mode
ultrasound images. Yet, these approaches hinge on precise annotations,
overlooking the rich acoustic information embedded in B-mode intensity.
Moreover, implicit representation approaches struggle with the ultrasound's
view-dependent nature and acoustic shadowing artifacts, which impair
reconstruction. To address the problems resulting from occlusions and
annotation dependency, we propose an occupancy-based representation and
introduce \gls{UltrON} that leverages acoustic features to improve geometric
consistency in weakly-supervised optimization regime. We show that these
features can be obtained from B-mode images without additional annotation cost.
Moreover, we propose a novel loss function that compensates for view-dependency
in the B-mode images and facilitates occupancy optimization from multiview
ultrasound. By incorporating acoustic properties, \gls{UltrON} generalizes to
shapes of the same anatomy. We show that \gls{UltrON} mitigates the limitations
of occlusions and sparse labeling and paves the way for more accurate 3D
reconstruction. Code and dataset will be available at
https://github.com/magdalena-wysocki/ultron.

</details>


### [49] [Implicit Neural Representations of Intramyocardial Motion and Strain](https://arxiv.org/abs/2509.09004)
*Andrew Bell,Yan Kit Choi,Steffen Peterson,Andrew King,Muhummad Sohaib Nazir,Alistair Young*

Main category: cs.CV

TL;DR: 提出一种基于隐式神经表示（INR）的心肌运动和应变自动量化方法，该方法在准确性和速度上均优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 从标记MRI图像中自动量化心肌运动和应变是一项重要但具有挑战性的任务。

Method: 使用隐式神经表示（INR）模型，通过学习到的潜在代码进行条件化，预测连续的左心室（LV）位移，且无需推理时优化。

Result: 在452个UK Biobank测试案例中，该方法实现了最佳的跟踪精度（2.14 mm RMSE）和最低的整体周向（2.86%）和径向（6.42%）应变组合误差。此外，该方法比最准确的基线快约380倍。

Conclusion: 研究结果表明，基于INR的模型非常适合对大型CMR数据集进行准确且可扩展的心肌应变分析。

Abstract: Automatic quantification of intramyocardial motion and strain from tagging
MRI remains an important but challenging task. We propose a method using
implicit neural representations (INRs), conditioned on learned latent codes, to
predict continuous left ventricular (LV) displacement -- without requiring
inference-time optimisation. Evaluated on 452 UK Biobank test cases, our method
achieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined
error in global circumferential (2.86%) and radial (6.42%) strain compared to
three deep learning baselines. In addition, our method is $\sim$380$\times$
faster than the most accurate baseline. These results highlight the suitability
of INR-based models for accurate and scalable analysis of myocardial strain in
large CMR datasets.

</details>


### [50] [E-MLNet: Enhanced Mutual Learning for Universal Domain Adaptation with Sample-Specific Weighting](https://arxiv.org/abs/2509.09006)
*Samuel Felipe dos Santos,Tiago Agostinho de Almeida,Jurandy Almeida*

Main category: cs.CV

TL;DR: 论文提出E-MLNet，通过为开放集熵最小化(OEM)引入动态加权策略，在通用领域适应(UniDA)中更精确地聚焦类边界适应，从而提高性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有UniDA方法（如MLNet）在开放集熵最小化(OEM)过程中平等对待所有分类器，导致学习信号稀释，未能有效聚焦于相关类别边界。

Method: 提出了增强型互学习网络(E-MLNet)，通过在MLNet的开放集熵最小化(OEM)中引入动态加权策略。该策略利用闭集分类器的预测，使模型能将适应重点放在每个目标样本最相关的类边界上，从而锐化已知与未知类别的区分。

Result: E-MLNet在VisDA和ImageCLEF数据集上取得了最高的平均H-score，并表现出优于其前身的鲁棒性。在Open-Partial DA和Open-Set DA设置中，E-MLNet分别在31个任务中的22个和19个中超越了MLNet基线。

Conclusion: E-MLNet提出的聚焦适应策略显著提高了通用领域适应的性能和鲁棒性，验证了动态加权策略在开放集熵最小化中的有效性。

Abstract: Universal Domain Adaptation (UniDA) seeks to transfer knowledge from a
labeled source to an unlabeled target domain without assuming any relationship
between their label sets, requiring models to classify known samples while
rejecting unknown ones. Advanced methods like Mutual Learning Network (MLNet)
use a bank of one-vs-all classifiers adapted via Open-set Entropy Minimization
(OEM). However, this strategy treats all classifiers equally, diluting the
learning signal. We propose the Enhanced Mutual Learning Network (E-MLNet),
which integrates a dynamic weighting strategy to OEM. By leveraging the
closed-set classifier's predictions, E-MLNet focuses adaptation on the most
relevant class boundaries for each target sample, sharpening the distinction
between known and unknown classes. We conduct extensive experiments on four
challenging benchmarks: Office-31, Office-Home, VisDA-2017, and ImageCLEF. The
results demonstrate that E-MLNet achieves the highest average H-scores on VisDA
and ImageCLEF and exhibits superior robustness over its predecessor. E-MLNet
outperforms the strong MLNet baseline in the majority of individual adaptation
tasks -- 22 out of 31 in the challenging Open-Partial DA setting and 19 out of
31 in the Open-Set DA setting -- confirming the benefits of our focused
adaptation strategy.

</details>


### [51] [COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation](https://arxiv.org/abs/2509.09014)
*Umair Hassan*

Main category: cs.CV

TL;DR: 论文介绍了COCO-Urdu，一个大规模乌尔都语图像字幕数据集，旨在解决该语言在多模态和视觉-语言研究中资源不足的问题，并减少现有模型中的语言偏见。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语在全球拥有超过2.5亿使用者，但在多模态和视觉-语言研究中仍严重缺乏大规模、高质量的数据集。这限制了乌尔都语系统的发展，并强化了多语言视觉-语言模型中对高资源语言的偏见。

Method: 作者从MS COCO构建了COCO-Urdu数据集，包含59,000张图像和319,000条乌尔都语字幕，通过分层抽样以保留原始分布。字幕使用SeamlessM4T v2进行翻译，并通过一个混合多模态质量评估框架（整合了COMET-Kiwi、CLIP相似度和BERTScore与回译）进行验证。低分字幕通过开源大型语言模型迭代优化。

Result: COCO-Urdu在BLEU、SacreBLEU和chrF等基准测试中报告了持续强劲的结果。据作者所知，这是目前规模最大的公开可用的乌尔都语字幕数据集。

Conclusion: 通过发布COCO-Urdu数据集及其质量评估流程，作者旨在减少多模态研究中的语言偏见，并为建立包容性的视觉-语言系统奠定基础。

Abstract: Urdu, spoken by over 250 million people, remains critically under-served in
multimodal and vision-language research. The absence of large-scale,
high-quality datasets has limited the development of Urdu-capable systems and
reinforced biases in multilingual vision-language models trained primarily on
high-resource languages. To address this gap, we present COCO-Urdu, a
large-scale image-caption dataset derived from MS COCO, containing 59,000
images and 319,000 Urdu captions selected through stratified sampling to
preserve the original distribution. Captions were translated using SeamlessM4T
v2 and validated with a hybrid multimodal quality estimation framework that
integrates COMET-Kiwi for translation quality, CLIP-based similarity for visual
grounding, and BERTScore with back-translation for semantic consistency;
low-scoring captions were iteratively refined using open-source large language
models. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting
consistently strong results. To the best of our knowledge, COCO-Urdu is the
largest publicly available Urdu captioning dataset. By releasing both the
dataset and the quality estimation pipeline, we aim to reduce language bias in
multimodal research and establish a foundation for inclusive vision-language
systems.

</details>


### [52] [VoxelFormer: Parameter-Efficient Multi-Subject Visual Decoding from fMRI](https://arxiv.org/abs/2509.09015)
*Chenqian Le,Yilin Zhao,Nikasadat Emami,Kushagra Yadav,Xujin "Chris" Liu,Xupeng Chen,Yao Wang*

Main category: cs.CV

TL;DR: VoxelFormer是一种轻量级Transformer，通过高效的体素压缩和生成CLIP对齐的表示，实现了fMRI视觉解码的多受试者训练，并以更少的参数达到竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI视觉解码方法大多依赖于受试者特定训练，这限制了其可扩展性和实际部署。

Method: 引入VoxelFormer，一个轻量级Transformer架构，用于fMRI视觉解码的多受试者训练。VoxelFormer集成了Token Merging Transformer (ToMer)以高效压缩体素，并包含一个查询驱动的Q-Former，用于生成与CLIP图像嵌入空间对齐的固定大小神经表示。

Result: 在7T自然场景数据集上的评估显示，VoxelFormer在训练过的受试者上实现了有竞争力的图像检索性能，且其参数量显著少于现有方法。

Conclusion: 研究结果表明，token merging和基于查询的Transformer是实现参数高效神经解码的有前景策略。

Abstract: Recent advances in fMRI-based visual decoding have enabled compelling
reconstructions of perceived images. However, most approaches rely on
subject-specific training, limiting scalability and practical deployment. We
introduce \textbf{VoxelFormer}, a lightweight transformer architecture that
enables multi-subject training for visual decoding from fMRI. VoxelFormer
integrates a Token Merging Transformer (ToMer) for efficient voxel compression
and a query-driven Q-Former that produces fixed-size neural representations
aligned with the CLIP image embedding space. Evaluated on the 7T Natural Scenes
Dataset, VoxelFormer achieves competitive retrieval performance on subjects
included during training with significantly fewer parameters than existing
methods. These results highlight token merging and query-based transformers as
promising strategies for parameter-efficient neural decoding.

</details>


### [53] [Integrating Anatomical Priors into a Causal Diffusion Model](https://arxiv.org/abs/2509.09054)
*Binxu Li,Wei Peng,Mingjie Li,Ehsan Adeli,Kilian M. Pohl*

Main category: cs.CV

TL;DR: 本文提出了一种名为PCGM的概率因果图模型，通过将体素级解剖约束整合到生成扩散框架中，实现了高质量反事实3D脑部MRI的生成，能够准确反映细微的形态学差异。


<details>
  <summary>Details</summary>
Motivation: 3D脑部MRI研究需要检测队列之间难以视觉识别的细微形态差异。MRI采集成本高昂，图像合成（特别是反事实图像生成）可以带来巨大益处。然而，现有的反事实模型缺乏保留精细解剖细节的显式归纳偏差，难以生成解剖学上合理的MRI，因为它们优化的是图像的整体外观而非医学相关的局部细微变化。

Method: 本文提出了概率因果图模型（PCGM），将体素级的解剖约束作为先验明确整合到生成扩散框架中。PCGM通过概率图模块捕获解剖约束，并将其转化为细微变化区域的空间二值掩膜。这些掩膜（通过ControlNet的3D扩展编码）用于约束一个新颖的反事实去噪UNet，其编码通过一个3D扩散解码器转换为高质量的脑部MRI。

Result: 在多个数据集上的大量实验表明，PCGM生成的结构性脑部MRI比几种基线方法具有更高的质量。此外，研究首次证明从PCGM生成的反事实图像中提取的脑部测量结果，能够复现神经科学文献中报道的疾病对皮层脑区域的细微影响。

Conclusion: 这项成就标志着在研究细微形态差异中使用合成MRI方面的一个重要里程碑。

Abstract: 3D brain MRI studies often examine subtle morphometric differences between
cohorts that are hard to detect visually. Given the high cost of MRI
acquisition, these studies could greatly benefit from image syntheses,
particularly counterfactual image generation, as seen in other domains, such as
computer vision. However, counterfactual models struggle to produce
anatomically plausible MRIs due to the lack of explicit inductive biases to
preserve fine-grained anatomical details. This shortcoming arises from the
training of the models aiming to optimize for the overall appearance of the
images (e.g., via cross-entropy) rather than preserving subtle, yet medically
relevant, local variations across subjects. To preserve subtle variations, we
propose to explicitly integrate anatomical constraints on a voxel-level as
prior into a generative diffusion framework. Called Probabilistic Causal Graph
Model (PCGM), the approach captures anatomical constraints via a probabilistic
graph module and translates those constraints into spatial binary masks of
regions where subtle variations occur. The masks (encoded by a 3D extension of
ControlNet) constrain a novel counterfactual denoising UNet, whose encodings
are then transferred into high-quality brain MRIs via our 3D diffusion decoder.
Extensive experiments on multiple datasets demonstrate that PCGM generates
structural brain MRIs of higher quality than several baseline approaches.
Furthermore, we show for the first time that brain measurements extracted from
counterfactuals (generated by PCGM) replicate the subtle effects of a disease
on cortical brain regions previously reported in the neuroscience literature.
This achievement is an important milestone in the use of synthetic MRIs in
studies investigating subtle morphological differences.

</details>


### [54] [Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models](https://arxiv.org/abs/2509.09064)
*Qiuhui Chen,Xuancheng Yao,Huping Ye,Yi Hong*

Main category: cs.CV

TL;DR: Med3DInsight是一个新颖的预训练框架，通过整合3D图像编码器与2D多模态大语言模型（MLLMs），并通过平面切片感知Transformer和部分最优传输对齐，显著增强了3D医学图像的理解，无需人工标注，并在分割和分类任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D医学卷积和基于Transformer的自监督学习方法在3D医学图像语义理解方面存在不足。多模态大语言模型（MLLMs）在通过文本描述增强图像理解方面显示出潜力，但主要针对2D图像。因此，研究旨在利用2D MLLMs改进3D医学图像的理解。

Method: 提出Med3DInsight预训练框架。该框架通过一个专门设计的“平面切片感知Transformer模块”将3D图像编码器与2D MLLMs集成。此外，模型采用基于“部分最优传输”的对齐方式，以提高对大语言模型生成内容可能引入噪声的容忍度。该方法无需人工标注，实现了可扩展的多模态3D医学表示学习。

Result: Med3DInsight在分割和分类这两个下游任务上，于各种公共CT和MRI数据集上取得了最先进的性能，优于当前的自监督学习方法。该框架可以无缝集成到现有的3D医学图像理解网络中，并有望提升其性能。

Conclusion: Med3DInsight为可扩展、无需人工标注的多模态3D医学表示学习提供了一个新范式，它通过创新的集成方式显著提升了3D医学图像的理解能力，并在多项任务上取得了卓越表现，具有良好的实用和集成潜力。

Abstract: Understanding 3D medical image volumes is critical in the medical field, yet
existing 3D medical convolution and transformer-based self-supervised learning
(SSL) methods often lack deep semantic comprehension. Recent advancements in
multimodal large language models (MLLMs) provide a promising approach to
enhance image understanding through text descriptions. To leverage these 2D
MLLMs for improved 3D medical image understanding, we propose Med3DInsight, a
novel pretraining framework that integrates 3D image encoders with 2D MLLMs via
a specially designed plane-slice-aware transformer module. Additionally, our
model employs a partial optimal transport based alignment, demonstrating
greater tolerance to noise introduced by potential noises in LLM-generated
content. Med3DInsight introduces a new paradigm for scalable multimodal 3D
medical representation learning without requiring human annotations. Extensive
experiments demonstrate our state-of-the-art performance on two downstream
tasks, i.e., segmentation and classification, across various public datasets
with CT and MRI modalities, outperforming current SSL methods. Med3DInsight can
be seamlessly integrated into existing 3D medical image understanding networks,
potentially enhancing their performance. Our source code, generated datasets,
and pre-trained models will be available at
https://github.com/Qybc/Med3DInsight.

</details>


### [55] [Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach](https://arxiv.org/abs/2509.09067)
*Hesham M. Shehata,Mohammad Abdolrahmani*

Main category: cs.CV

TL;DR: 针对GCN在人体行为识别中对人机交互识别不足的问题，本文提出结合环境中的固定物体信息和多任务学习方法，显著提高了人机交互和非交互行为的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的图卷积神经网络(GCN)虽在基于人体骨骼姿态的人体行为识别中表现良好，但由于缺乏有效的场景信息表示和合适的学习架构，难以成功检测人机交互案例。

Method: 提出一种方法，通过考虑环境中的固定物体信息并采用多任务学习方法来提升人体行为识别性能。为评估此方法，收集并制作了一个包含人手与固定物体交互（如ATM机）及非交互（如行走、站立）行为的真实数据集。

Result: 结合交互区域信息的多任务学习方法，成功以99.25%的准确率识别了所研究的交互和非交互动作，比仅使用人体骨骼姿态的基线模型准确率提高了2.75%。

Conclusion: 通过整合固定物体信息和采用多任务学习，可以有效提升GCN在处理人机交互场景时的识别能力，显著优于仅依赖骨骼姿态的传统方法。

Abstract: Recent graph convolutional neural networks (GCNs) have shown high performance
in the field of human action recognition by using human skeleton poses.
However, it fails to detect human-object interaction cases successfully due to
the lack of effective representation of the scene information and appropriate
learning architectures. In this context, we propose a methodology to utilize
human action recognition performance by considering fixed object information in
the environment and following a multi-task learning approach. In order to
evaluate the proposed method, we collected real data from public environments
and prepared our data set, which includes interaction classes of hands-on fixed
objects (e.g., ATM ticketing machines, check-in/out machines, etc.) and
non-interaction classes of walking and standing. The multi-task learning
approach, along with interaction area information, succeeds in recognizing the
studied interaction and non-interaction actions with an accuracy of 99.25%,
outperforming the accuracy of the base model using only human skeleton poses by
2.75%.

</details>


### [56] [IRDFusion: Iterative Relation-Map Difference guided Feature Fusion for Multispectral Object Detection](https://arxiv.org/abs/2509.09085)
*Jifeng Shen,Haibo Zhan,Xin Zuo,Heng Fan,Xiaohui Yuan,Jun Li,Wankou Yang*

Main category: cs.CV

TL;DR: 本文提出IRDFusion框架，通过跨模态特征对比与筛选策略，解决多光谱目标检测中背景噪声干扰问题，实现自适应特征融合，并在多个数据集上达到最先进（SOTA）性能。


<details>
  <summary>Details</summary>
Motivation: 当前多光谱目标检测方法在特征融合过程中，常保留多余的背景或噪声，这限制了其感知性能。

Method: 本文提出一种基于跨模态特征对比与筛选策略的特征融合框架IRDFusion。该框架包含两个核心模块：互补特征细化模块（MFRM）和差分特征反馈模块（DFFM）。MFRM通过建模模态内和模态间特征关系来增强表示；DFFM动态计算模态间差分特征作为引导信号反馈给MFRM，以自适应融合互补信息并抑制共模噪声。IRDFusion将MFRM和DFFM整合，通过迭代反馈，逐步放大显著关系信号并抑制特征噪声，实现高质量跨模态融合。

Result: 在FLIR、LLVIP和M$^3$FD等数据集上的广泛实验表明，IRDFusion达到了最先进的性能，并在各种挑战性场景中持续优于现有方法，验证了其鲁棒性和有效性。

Conclusion: IRDFusion通过其独特的跨模态特征对比与筛选策略，成功解决了多光谱目标检测中因背景噪声导致的性能瓶颈，通过自适应融合互补信息并抑制噪声，显著提升了目标检测的感知能力，并在多个基准数据集上展现出卓越的性能和普适性。

Abstract: Current multispectral object detection methods often retain extraneous
background or noise during feature fusion, limiting perceptual performance.To
address this, we propose an innovative feature fusion framework based on
cross-modal feature contrastive and screening strategy, diverging from
conventional approaches. The proposed method adaptively enhances salient
structures by fusing object-aware complementary cross-modal features while
suppressing shared background interference.Our solution centers on two novel,
specially designed modules: the Mutual Feature Refinement Module (MFRM) and the
Differential Feature Feedback Module (DFFM). The MFRM enhances intra- and
inter-modal feature representations by modeling their relationships, thereby
improving cross-modal alignment and discriminative power.Inspired by feedback
differential amplifiers, the DFFM dynamically computes inter-modal differential
features as guidance signals and feeds them back to the MFRM, enabling adaptive
fusion of complementary information while suppressing common-mode noise across
modalities. To enable robust feature learning, the MFRM and DFFM are integrated
into a unified framework, which is formally formulated as an Iterative
Relation-Map Differential Guided Feature Fusion mechanism, termed IRDFusion.
IRDFusion enables high-quality cross-modal fusion by progressively amplifying
salient relational signals through iterative feedback, while suppressing
feature noise, leading to significant performance gains.In extensive
experiments on FLIR, LLVIP and M$^3$FD datasets, IRDFusion achieves
state-of-the-art performance and consistently outperforms existing methods
across diverse challenging scenarios, demonstrating its robustness and
effectiveness. Code will be available at
https://github.com/61s61min/IRDFusion.git.

</details>


### [57] [SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models](https://arxiv.org/abs/2509.09090)
*Hengyu Fang,Yijiang Liu,Yuan Du,Li Du,Huanrui Yang*

Main category: cs.CV

TL;DR: SQAP-VLA是一个免训练的VLA推理加速框架，首次同时实现了最先进的量化和令牌剪枝，有效解决了两者之间的不兼容性，显著提升了计算效率和推理速度。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作（VLA）模型虽在具身智能中展现出强大能力，但其高昂的计算和内存成本阻碍了实际部署。现有压缩方法（如量化或令牌剪枝）无法同时启用两者以实现全面效率提升，因为它们存在不兼容性。

Method: 本文提出了SQAP-VLA，一个结构化、免训练的VLA推理加速框架。通过共同设计量化和令牌剪枝流程，引入了量化感知令牌剪枝标准，使其能在积极量化模型上工作，并改进量化器设计以增强剪枝效果，从而克服了二者不兼容的问题。

Result: SQAP-VLA应用于标准VLA模型时，显著提高了计算效率和推理速度，同时成功保持了核心模型性能。与原始模型相比，实现了1.93倍的加速，平均成功率提升高达4.5%。

Conclusion: SQAP-VLA通过创新的协同设计，成功解决了VLA模型量化与令牌剪枝的不兼容问题，实现了二者的同步加速，为具身智能VLA模型的实际部署提供了高效且高性能的解决方案。

Abstract: Vision-Language-Action (VLA) models exhibit unprecedented capabilities for
embodied intelligence. However, their extensive computational and memory costs
hinder their practical deployment. Existing VLA compression and acceleration
approaches conduct quantization or token pruning in an ad-hoc manner but fail
to enable both for a holistic efficiency improvement due to an observed
incompatibility. This work introduces SQAP-VLA, the first structured,
training-free VLA inference acceleration framework that simultaneously enables
state-of-the-art quantization and token pruning. We overcome the
incompatibility by co-designing the quantization and token pruning pipeline,
where we propose new quantization-aware token pruning criteria that work on an
aggressively quantized model while improving the quantizer design to enhance
pruning effectiveness. When applied to standard VLA models, SQAP-VLA yields
significant gains in computational efficiency and inference speed while
successfully preserving core model performance, achieving a $\times$1.93
speedup and up to a 4.5\% average success rate enhancement compared to the
original model.

</details>


### [58] [S-BEVLoc: BEV-based Self-supervised Framework for Large-scale LiDAR Global Localization](https://arxiv.org/abs/2509.09110)
*Chenghao Zhang,Lun Luo,Si-Yuan Cao,Xiaokai Bai,Yuncheng Jin,Zhu Yu,Beinan Yu,Yisen Wang,Hui-Liang Shen*

Main category: cs.CV

TL;DR: 本文提出S-BEVLoc，一种新颖的自监督BEV框架，无需真值姿态即可实现LiDAR全局定位。该方法通过构建训练三元组和引入SoftCos损失，在多个任务中达到最先进性能并具有高可扩展性，解决了传统监督方法对高成本真值数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的LiDAR全局定位方法依赖于从GPS或SLAM里程计获取的真值姿态来监督网络训练。尽管这些监督方法取得了成功，但获取高精度真值姿态需要巨大的成本和精力。

Method: 提出S-BEVLoc，一个基于鸟瞰图（BEV）的自监督框架。通过利用关键点中心BEV补丁之间的已知地理距离，从单个BEV图像构建训练三元组。使用卷积神经网络（CNN）提取局部特征，并采用NetVLAD聚合全局描述符。引入SoftCos损失函数以增强从生成的三元组中学习。

Result: 在KITTI和NCLT大规模数据集上的实验结果表明，S-BEVLoc在地点识别、闭环和全局定位任务中均达到了最先进的性能，同时提供了监督方法难以实现的可扩展性。

Conclusion: S-BEVLoc提供了一种无需真值姿态的有效自监督LiDAR全局定位方案，通过创新的三元组构建和损失函数，不仅实现了SOTA性能，还显著提升了可扩展性，解决了现有方法对昂贵真值数据的依赖。

Abstract: LiDAR-based global localization is an essential component of simultaneous
localization and mapping (SLAM), which helps loop closure and re-localization.
Current approaches rely on ground-truth poses obtained from GPS or SLAM
odometry to supervise network training. Despite the great success of these
supervised approaches, substantial cost and effort are required for
high-precision ground-truth pose acquisition. In this work, we propose
S-BEVLoc, a novel self-supervised framework based on bird's-eye view (BEV) for
LiDAR global localization, which eliminates the need for ground-truth poses and
is highly scalable. We construct training triplets from single BEV images by
leveraging the known geographic distances between keypoint-centered BEV
patches. Convolutional neural network (CNN) is used to extract local features,
and NetVLAD is employed to aggregate global descriptors. Moreover, we introduce
SoftCos loss to enhance learning from the generated triplets. Experimental
results on the large-scale KITTI and NCLT datasets show that S-BEVLoc achieves
state-of-the-art performance in place recognition, loop closure, and global
localization tasks, while offering scalability that would require extra effort
for supervised approaches.

</details>


### [59] [FPI-Det: a face--phone Interaction Dataset for phone-use detection and understanding](https://arxiv.org/abs/2509.09111)
*Jianqin Gao,Tianqi Wang,Yu Zhang,Yishu Zhang,Chenyuan Wang,Allan Dong,Zihao Wang*

Main category: cs.CV

TL;DR: 本文引入了FPI-Det数据集，一个包含22,879张图像的精细化人机交互检测数据集，旨在解决现有基准在检测个人使用手机行为时的不足，并提供了主流检测器（YOLO和DETR）的基线结果及性能分析。


<details>
  <summary>Details</summary>
Motivation: 移动设备的广泛使用给视觉系统在安全监控、工作效率评估和注意力管理方面带来了新挑战。检测个人是否使用手机不仅需要目标识别，还需要理解涉及人脸、手和设备之间关系的细粒度行为上下文。现有通用基准未能充分捕捉此类精细化人机交互。

Method: 为弥补这一空白，研究者引入了FPI-Det数据集，该数据集包含22,879张图像，同步标注了人脸和手机，涵盖工作场所、教育、交通和公共场景。数据集的特点是极端尺度变化、频繁遮挡和多样的捕捉条件。研究者还在此数据集上评估了代表性的YOLO和DETR检测器。

Result: 研究提供了YOLO和DETR检测器的基线检测结果，并分析了它们在不同物体尺寸、遮挡水平和环境下的性能表现。

Conclusion: FPI-Det数据集及其提供的基线评估有效弥补了精细化人机交互检测领域的空白，为未来的研究提供了宝贵的资源和基准。

Abstract: The widespread use of mobile devices has created new challenges for vision
systems in safety monitoring, workplace productivity assessment, and attention
management. Detecting whether a person is using a phone requires not only
object recognition but also an understanding of behavioral context, which
involves reasoning about the relationship between faces, hands, and devices
under diverse conditions. Existing generic benchmarks do not fully capture such
fine-grained human--device interactions. To address this gap, we introduce the
FPI-Det, containing 22{,}879 images with synchronized annotations for faces and
phones across workplace, education, transportation, and public scenarios. The
dataset features extreme scale variation, frequent occlusions, and varied
capture conditions. We evaluate representative YOLO and DETR detectors,
providing baseline results and an analysis of performance across object sizes,
occlusion levels, and environments. Source code and dataset is available at
https://github.com/KvCgRv/FPI-Det.

</details>


### [60] [Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation Models and Text-to-image Attention](https://arxiv.org/abs/2509.09116)
*Junhao Xing,Ryohei Miyakawa,Yang Yang,Xinpeng Liu,Risa Shinoda,Hiroaki Santo,Yosuke Toda,Fumio Okura*

Main category: cs.CV

TL;DR: ZeroPlantSeg是一种零样本分割模型，用于从俯视图图像中提取莲座状植物个体。它结合了基础分割模型和视觉-语言模型，无需训练即可处理多片重叠叶片的植物，并在跨领域表现优于现有零样本和监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有基础分割模型可以零样本提取单片叶子，但难以对包含多片重叠叶片的完整植物个体进行层级分割。这项任务通常需要耗费大量人工、特定物种的标注训练数据集。

Method: 提出ZeroPlantSeg模型，该方法整合了一个基础分割模型用于提取叶片实例，以及一个视觉-语言模型用于推理植物结构，从而在无需额外训练的情况下实现植物个体的零样本分割。

Result: 在包含多种植物物种、生长阶段和拍摄环境的数据集上进行评估，ZeroPlantSeg超越了现有的零样本方法，并实现了比监督方法更好的跨领域性能。

Conclusion: ZeroPlantSeg通过结合基础分割和视觉-语言模型，成功实现了对莲座状植物个体的零样本层级分割，有效解决了对大量标注数据的依赖，并在跨领域应用中展现出卓越性能。

Abstract: Foundation segmentation models achieve reasonable leaf instance extraction
from top-view crop images without training (i.e., zero-shot). However,
segmenting entire plant individuals with each consisting of multiple
overlapping leaves remains challenging. This problem is referred to as a
hierarchical segmentation task, typically requiring annotated training
datasets, which are often species-specific and require notable human labor. To
address this, we introduce ZeroPlantSeg, a zero-shot segmentation for
rosette-shaped plant individuals from top-view images. We integrate a
foundation segmentation model, extracting leaf instances, and a vision-language
model, reasoning about plants' structures to extract plant individuals without
additional training. Evaluations on datasets with multiple plant species,
growth stages, and shooting environments demonstrate that our method surpasses
existing zero-shot methods and achieves better cross-domain performance than
supervised methods. Implementations are available at
https://github.com/JunhaoXing/ZeroPlantSeg.

</details>


### [61] [Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval](https://arxiv.org/abs/2509.09118)
*Tianlu Zheng,Yifan Zhang,Xiang An,Ziyong Feng,Kaicheng Yang,Qichuan Ding*

Main category: cs.CV

TL;DR: 本文通过构建大规模高质量人物中心图像-文本数据集WebPerson和提出GA-DMS框架，协同提升了CLIP在人物表示学习中的性能，解决了数据稀缺和局部特征判别力不足的问题，并实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 将CLIP应用于人物表示学习面临两大挑战：一是缺乏大规模人物中心图像-语言标注数据；二是全局对比学习难以保持细粒度匹配所需的判别性局部特征，且易受噪声文本标记影响。

Method: 1. **数据构建**：开发了一个抗噪声数据构建流程，利用多模态大语言模型（MLLMs）的上下文学习能力自动过滤和标注网络图像，创建了WebPerson数据集（包含500万高质量人物中心图像-文本对）。2. **模型架构**：引入了GA-DMS（Gradient-Attention Guided Dual-Masking Synergetic）框架，通过基于梯度注意力相似度分数自适应地屏蔽噪声文本标记，改进了跨模态对齐。此外，结合了掩码标记预测目标，强制模型预测信息丰富的文本标记，增强了细粒度语义表示学习。

Result: 广泛的实验表明，GA-DMS在多个基准测试中取得了最先进的性能。

Conclusion: 通过在数据整理和模型架构方面的协同改进，本工作显著推动了CLIP在人物表示学习领域的应用，有效解决了数据稀缺和局部特征判别力不足等关键挑战，并达到了卓越的性能。

Abstract: Although Contrastive Language-Image Pre-training (CLIP) exhibits strong
performance across diverse vision tasks, its application to person
representation learning faces two critical challenges: (i) the scarcity of
large-scale annotated vision-language data focused on person-centric images,
and (ii) the inherent limitations of global contrastive learning, which
struggles to maintain discriminative local features crucial for fine-grained
matching while remaining vulnerable to noisy text tokens. This work advances
CLIP for person representation learning through synergistic improvements in
data curation and model architecture. First, we develop a noise-resistant data
construction pipeline that leverages the in-context learning capabilities of
MLLMs to automatically filter and caption web-sourced images. This yields
WebPerson, a large-scale dataset of 5M high-quality person-centric image-text
pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking
Synergetic) framework, which improves cross-modal alignment by adaptively
masking noisy textual tokens based on the gradient-attention similarity score.
Additionally, we incorporate masked token prediction objectives that compel the
model to predict informative text tokens, enhancing fine-grained semantic
representation learning. Extensive experiments show that GA-DMS achieves
state-of-the-art performance across multiple benchmarks.

</details>


### [62] [ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain](https://arxiv.org/abs/2509.09130)
*Bin Huang,Kang Chen,Bingxuan Li,Huafeng Liu,Qiegen Liu*

Main category: cs.CV

TL;DR: 针对PET成像数据稀缺和计算资源限制，ALL-PET提出一种基于潜在扩散模型的低资源、低样本PET基础模型，通过创新的数据增强、几何约束和病灶注意力机制，仅用500样本即可高质量生成正弦图，并泛化到多项PET任务。


<details>
  <summary>Details</summary>
Motivation: 构建大型PET基础模型受限于标注数据稀缺和计算资源不足。

Method: 提出ALL-PET，一个在投影域操作的低资源、低样本PET基础模型，基于潜在扩散模型（LDM），主要包含三项创新：1. Radon掩码增强策略 (RMAS) 及动态多掩码 (DMM) 机制：通过将图像域掩码投影到正弦图空间生成大量训练样本，显著提升数据多样性和泛化能力。2. 正/负掩码约束：嵌入严格几何一致性，在减少参数的同时保持生成质量。3. 透明医学注意力 (TMA)：无参数、几何驱动机制，增强原始投影数据中的病灶相关区域，并支持临床定义的ROI调整。

Result: 仅使用500个样本即可实现高质量的正弦图生成，性能与在大型数据集上训练的模型相当。对低剂量重建、衰减校正、延迟帧预测和示踪剂分离等任务具有良好的泛化能力。运行高效，内存使用低于24GB。

Conclusion: ALL-PET成功克服了PET成像基础模型的数据稀缺和效率限制，以低资源、低样本方式提供了高质量、多任务泛化的PET数据处理方案。

Abstract: Building large-scale foundation model for PET imaging is hindered by limited
access to labeled data and insufficient computational resources. To overcome
data scarcity and efficiency limitations, we propose ALL-PET, a low-resource,
low-shot PET foundation model operating directly in the projection domain.
ALL-PET leverages a latent diffusion model (LDM) with three key innovations.
First, we design a Radon mask augmentation strategy (RMAS) that generates over
200,000 structurally diverse training samples by projecting randomized
image-domain masks into sinogram space, significantly improving generalization
with minimal data. This is extended by a dynamic multi-mask (DMM) mechanism
that varies mask quantity and distribution, enhancing data diversity without
added model complexity. Second, we implement positive/negative mask constraints
to embed strict geometric consistency, reducing parameter burden while
preserving generation quality. Third, we introduce transparent medical
attention (TMA), a parameter-free, geometry-driven mechanism that enhances
lesion-related regions in raw projection data. Lesion-focused attention maps
are derived from coarse segmentation, covering both hypermetabolic and
hypometabolic areas, and projected into sinogram space for physically
consistent guidance. The system supports clinician-defined ROI adjustments,
ensuring flexible, interpretable, and task-adaptive emphasis aligned with PET
acquisition physics. Experimental results show ALL-PET achieves high-quality
sinogram generation using only 500 samples, with performance comparable to
models trained on larger datasets. ALL-PET generalizes across tasks including
low-dose reconstruction, attenuation correction, delayed-frame prediction, and
tracer separation, operating efficiently with memory use under 24GB.

</details>


### [63] [Noise-Robust Topology Estimation of 2D Image Data via Neural Networks and Persistent Homology](https://arxiv.org/abs/2509.09140)
*Dylan Peek,Matthew P. Skerritt,Stephan Chalup*

Main category: cs.CV

TL;DR: 本研究评估了神经网络（ANN）在噪声环境下预测2D二值图像Betti数的鲁棒性，并与持久同源性（PH）方法进行比较，结果显示ANNs表现更优。


<details>
  <summary>Details</summary>
Motivation: 持久同源性（PH）和人工神经网络（ANNs）是推断数据拓扑结构的两种不同方法。研究旨在探索ANNs在噪声下的拓扑结构推断能力，并将其与一种 widely adopted 的PH策略进行比较。

Method: 训练一个监督神经网络来预测2D二值图像中的Betti数，并评估其噪声鲁棒性。将该ANN方法与基于立方复形和符号欧几里得距离变换（SEDT）的PH管道进行比较。实验使用了1个合成数据集和2个真实世界数据集。

Result: 在有噪声的情况下，ANNs在预测Betti数方面的性能优于所比较的PH方法。研究推测这可能是因为ANNs能够从训练数据中学习上下文和几何先验知识。

Conclusion: 尽管ANNs用于拓扑估计尚处于新兴阶段，但在结构噪声下，它们为PH提供了一个引人注目的替代方案。

Abstract: Persistent Homology (PH) and Artificial Neural Networks (ANNs) offer
contrasting approaches to inferring topological structure from data. In this
study, we examine the noise robustness of a supervised neural network trained
to predict Betti numbers in 2D binary images. We compare an ANN approach
against a PH pipeline based on cubical complexes and the Signed Euclidean
Distance Transform (SEDT), which is a widely adopted strategy for noise-robust
topological analysis. Using one synthetic and two real-world datasets, we show
that ANNs can outperform this PH approach under noise, likely due to their
capacity to learn contextual and geometric priors from training data. Though
still emerging, the use of ANNs for topology estimation offers a compelling
alternative to PH under structural noise.

</details>


### [64] [Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation](https://arxiv.org/abs/2509.09143)
*Yuiko Uchida,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 本文提出Objectness SIMilarity (OSIM)，一种新型3D场景评估指标，专注于以“物体”为中心，旨在解决现有指标与人类感知不符的问题，并被证明与人类感知更一致。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景评估指标侧重整体图像质量，导致与人类视觉感知存在偏差。受神经心理学启发，研究者假设人类对3D场景的识别核心在于对单个物体的关注。

Method: 提出Objectness SIMilarity (OSIM)指标。该方法利用目标检测模型及其特征表示，量化场景中每个物体的“物体性”，从而实现以物体为中心的评估。

Result: 用户研究表明OSIM比现有指标更符合人类感知。研究还分析了OSIM的特性，并使用OSIM在标准化实验设置下重新评估了最新的3D重建和生成模型，以阐明该领域的进展。

Conclusion: OSIM作为一种以物体为中心的新型3D场景评估指标，能更准确地反映人类感知，并为3D重建和生成模型提供更客观的评估工具，有助于推动该领域的发展。

Abstract: This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric
for 3D scenes that explicitly focuses on "objects," which are fundamental units
of human visual perception. Existing metrics assess overall image quality,
leading to discrepancies with human perception. Inspired by neuropsychological
insights, we hypothesize that human recognition of 3D scenes fundamentally
involves attention to individual objects. OSIM enables object-centric
evaluations by leveraging an object detection model and its feature
representations to quantify the "objectness" of each object in the scene. Our
user study demonstrates that OSIM aligns more closely with human perception
compared to existing metrics. We also analyze the characteristics of OSIM using
various approaches. Moreover, we re-evaluate recent 3D reconstruction and
generation models under a standardized experimental setup to clarify
advancements in this field. The code is available at
https://github.com/Objectness-Similarity/OSIM.

</details>


### [65] [Video Understanding by Design: How Datasets Shape Architectures and Insights](https://arxiv.org/abs/2509.09151)
*Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: 本综述首次从数据集驱动的角度审视视频理解领域，揭示数据集特性如何引导模型架构的演变，并提供设计指导。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解综述主要按任务或模型家族分类，忽视了数据集对架构演进的结构性压力。本研究旨在填补这一空白，深入探讨数据集特性如何施加归纳偏置，从而指导模型设计。

Method: 本综述采用数据集驱动的视角，分析了运动复杂性、时间跨度、层级构成和多模态丰富性等数据集特性如何施加归纳偏置。通过重新解读从双流和3D CNN到Transformer和多模态基础模型等里程碑式架构，将其视为对数据集压力的具体响应。

Result: 研究结果表明，不同的数据集特性（如运动复杂性、时间跨度、层级组合和多模态丰富性）会给模型施加特定的归纳偏置。视频理解模型的发展历程可被理解为对这些数据集驱动压力的具体响应和演化。

Conclusion: 本综述统一了数据集、归纳偏置和架构，提出了一个连贯的框架。它不仅提供了全面的回顾，还为如何根据数据集不变性设计模型、平衡可扩展性和任务需求，以及推动通用视频理解提供了实用的指导和规范性路线图。

Abstract: Video understanding has advanced rapidly, fueled by increasingly complex
datasets and powerful architectures. Yet existing surveys largely classify
models by task or family, overlooking the structural pressures through which
datasets guide architectural evolution. This survey is the first to adopt a
dataset-driven perspective, showing how motion complexity, temporal span,
hierarchical composition, and multimodal richness impose inductive biases that
models should encode. We reinterpret milestones, from two-stream and 3D CNNs to
sequential, transformer, and multimodal foundation models, as concrete
responses to these dataset-driven pressures. Building on this synthesis, we
offer practical guidance for aligning model design with dataset invariances
while balancing scalability and task demands. By unifying datasets, inductive
biases, and architectures into a coherent framework, this survey provides both
a comprehensive retrospective and a prescriptive roadmap for advancing
general-purpose video understanding.

</details>


### [66] [OCELOT 2023: Cell Detection from Cell-Tissue Interaction Challenge](https://arxiv.org/abs/2509.09153)
*JaeWoong Shin,Jeongun Ryu,Aaron Valero Puche,Jinhee Lee,Biagio Brattoli,Wonkyung Jung,Soo Ick Cho,Kyunghyun Paeng,Chan-Young Ock,Donggeun Yoo,Zhaoyang Li,Wangkai Li,Huayu Mai,Joshua Millward,Zhen He,Aiden Nibali,Lydia Anette Schoenpflug,Viktor Hendrik Koelzer,Xu Shuoyu,Ji Zheng,Hu Bin,Yu-Wen Lo,Ching-Hui Yang,Sérgio Pereira*

Main category: cs.CV

TL;DR: OCELOT 2023挑战赛及其数据集证明，在细胞检测模型中整合多尺度细胞-组织关系能显著提高性能，优于仅关注细胞的方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习细胞检测模型难以模仿病理学家在多倍放大下评估组织形态和细胞细节的能力，也无法学习多尺度结构间的相互依赖语义。主要障碍是缺乏带有重叠细胞和组织多尺度标注的数据集。OCELOT 2023挑战赛旨在验证细胞与组织（cell-tissue）相互作用对于达到人类水平性能的重要性，并加速该领域研究。

Method: OCELOT 2023挑战赛提供了包含六种器官的重叠细胞检测和组织分割标注数据集，共673对，源自306张TCGA全玻片图像（H&E染色），并划分为训练、验证和测试集。参与者提交模型来增强对细胞-组织关系的理解。本文对参与者使用的方法进行了比较分析。

Result: 与未整合细胞-组织关系的基线纯细胞模型相比，表现最佳的参赛作品在测试集上的F1-score最高提升了7.99。这表明相较于传统纯细胞检测方法，这是一个显著的性能提升。

Conclusion: 研究结果证明了将多尺度语义（特别是细胞-组织关系）整合到深度学习模型中的必要性，这对于实现人类水平的细胞检测性能至关重要。挑战赛也突出了创新的策略并加速了该领域的研究。

Abstract: Pathologists routinely alternate between different magnifications when
examining Whole-Slide Images, allowing them to evaluate both broad tissue
morphology and intricate cellular details to form comprehensive diagnoses.
However, existing deep learning-based cell detection models struggle to
replicate these behaviors and learn the interdependent semantics between
structures at different magnifications. A key barrier in the field is the lack
of datasets with multi-scale overlapping cell and tissue annotations. The
OCELOT 2023 challenge was initiated to gather insights from the community to
validate the hypothesis that understanding cell and tissue (cell-tissue)
interactions is crucial for achieving human-level performance, and to
accelerate the research in this field. The challenge dataset includes
overlapping cell detection and tissue segmentation annotations from six organs,
comprising 673 pairs sourced from 306 The Cancer Genome Atlas (TCGA)
Whole-Slide Images with hematoxylin and eosin staining, divided into training,
validation, and test subsets. Participants presented models that significantly
enhanced the understanding of cell-tissue relationships. Top entries achieved
up to a 7.99 increase in F1-score on the test set compared to the baseline
cell-only model that did not incorporate cell-tissue relationships. This is a
substantial improvement in performance over traditional cell-only detection
methods, demonstrating the need for incorporating multi-scale semantics into
the models. This paper provides a comparative analysis of the methods used by
participants, highlighting innovative strategies implemented in the OCELOT 2023
challenge.

</details>


### [67] [RT-DETR++ for UAV Object Detection](https://arxiv.org/abs/2509.09157)
*Yuan Shufang*

Main category: cs.CV

TL;DR: 本文提出RT-DETR++，通过改进RT-DETR的编码器（引入AU/AD机制和CSP-PAC），有效提升了无人机图像中密集小目标的实时检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中的目标检测面临严峻挑战，如密集小目标、尺度变化和遮挡等问题普遍存在。

Method: 本文引入RT-DETR++，增强了RT-DETR模型的编码器。主要改进包括：1. 引入通道门控注意力机制的上下采样（AU/AD），以最小化错误并保留特征传播中的细节。2. 在特征融合时融入CSP-PAC，利用并行空洞卷积处理局部和上下文信息，促进多尺度特征的整合。

Result: 评估表明，该新型颈部设计在检测小型和密集物体方面表现出卓越的性能。模型在不增加计算复杂度的前提下，保持了足够的实时检测速度。

Conclusion: 本研究为实时检测系统中的特征编码设计提供了一种有效方法。

Abstract: Object detection in unmanned aerial vehicle (UAV) imagery presents
significant challenges. Issues such as densely packed small objects, scale
variations, and occlusion are commonplace. This paper introduces RT-DETR++,
which enhances the encoder component of the RT-DETR model. Our improvements
focus on two key aspects. First, we introduce a channel-gated attention-based
upsampling/downsampling (AU/AD) mechanism. This dual-path system minimizes
errors and preserves details during feature layer propagation. Second, we
incorporate CSP-PAC during feature fusion. This technique employs parallel
hollow convolutions to process local and contextual information within the same
layer, facilitating the integration of multi-scale features. Evaluation
demonstrates that our novel neck design achieves superior performance in
detecting small and densely packed objects. The model maintains sufficient
speed for real-time detection without increasing computational complexity. This
study provides an effective approach for feature encoding design in real-time
detection systems.

</details>


### [68] [A Knowledge Noise Mitigation Framework for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2509.09159)
*Zhiyue Liu,Sihang Liu,Jinyuan Liu,Xinru Zhang*

Main category: cs.CV

TL;DR: 针对KB-VQA中知识冗余和噪声问题，本文提出一个免训练的知识聚焦框架，通过低噪声查询、大模型知识提取和选择性知识集成策略，有效提升知识相关性、减少冗余，并超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有的KB-VQA方法在整合知识时，常直接使用检索到的信息，忽略了知识中的大量冗余，这会引入噪声并影响问答的准确性。

Method: 提出一个免训练的知识聚焦框架。首先，通过分析图像-问题对，生成低噪声查询以检索高相关性知识；其次，利用大模型从检索到的知识中提取对回答有益的片段，以减少冗余；最后，引入选择性知识集成策略，仅在模型对答案缺乏信心时才整合知识，进一步减轻冗余信息的影响。

Result: 该框架能够获取准确且关键的知识。通过广泛实验证明，该方法在性能上超越了现有最先进的方法。

Conclusion: 所提出的框架通过提高知识相关性和减少冗余，有效缓解了KB-VQA中知识冗余和噪声的影响，显著提升了模型的问答能力。

Abstract: Knowledge-based visual question answering (KB-VQA) requires a model to
understand images and utilize external knowledge to provide accurate answers.
Existing approaches often directly augment models with retrieved information
from knowledge sources while ignoring substantial knowledge redundancy, which
introduces noise into the answering process. To address this, we propose a
training-free framework with knowledge focusing for KB-VQA, that mitigates the
impact of noise by enhancing knowledge relevance and reducing redundancy.
First, for knowledge retrieval, our framework concludes essential parts from
the image-question pairs, creating low-noise queries that enhance the retrieval
of highly relevant knowledge. Considering that redundancy still persists in the
retrieved knowledge, we then prompt large models to identify and extract
answer-beneficial segments from knowledge. In addition, we introduce a
selective knowledge integration strategy, allowing the model to incorporate
knowledge only when it lacks confidence in answering the question, thereby
mitigating the influence of redundant information. Our framework enables the
acquisition of accurate and critical knowledge, and extensive experiments
demonstrate that it outperforms state-of-the-art methods.

</details>


### [69] [CWSSNet: Hyperspectral Image Classification Enhanced by Wavelet Domain Convolution](https://arxiv.org/abs/2509.09163)
*Yulin Tong,Fengzong Zhang,Haiqin Cheng*

Main category: cs.CV

TL;DR: 本研究提出了名为CWSSNet的高光谱地物分类框架，通过整合3D谱空间特征和小波卷积，有效解决了高光谱图像特征冗余问题，实现了高精度分类，并在小样本训练下表现出良好性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱遥感技术在精细地物分类中具有重要应用价值，但其多波段、高维度和光谱混合特性易导致特征冗余，限制了识别精度，传统方法面临性能瓶颈，亟需解决这一问题。

Method: 本研究以ZY1F卫星高光谱图像为数据源，选取江西省余干县为研究区，提出了一种名为CWSSNet的分类框架。该框架整合了3D谱空间特征和小波卷积，利用多尺度卷积注意力模块融合多模态信息，并通过引入多波段分解和小波域卷积操作来突破传统方法的分类性能瓶颈。

Result: 实验结果显示，CWSSNet在余干县的地物分类中取得了74.50%的mIoU、82.73%的mAcc和84.94%的mF1。在水体、植被和裸地分类中获得了最高的IoU，展现出良好的鲁棒性。此外，在训练集比例为70%时，训练时间增幅有限，分类效果接近最佳水平，表明模型在小样本训练条件下也能保持可靠性能。

Conclusion: CWSSNet框架通过有效整合3D谱空间特征和小波卷积，成功解决了高光谱图像地物分类中的特征冗余和精度瓶颈问题。该模型不仅实现了高精度地物分类，还在小样本训练条件下展现出良好的鲁棒性和可靠性能。

Abstract: Hyperspectral remote sensing technology has significant application value in
fields such as forestry ecology and precision agriculture, while also putting
forward higher requirements for fine ground object classification. However,
although hyperspectral images are rich in spectral information and can improve
recognition accuracy, they tend to cause prominent feature redundancy due to
their numerous bands, high dimensionality, and spectral mixing characteristics.
To address this, this study used hyperspectral images from the ZY1F satellite
as a data source and selected Yugan County, Shangrao City, Jiangxi Province as
the research area to perform ground object classification research. A
classification framework named CWSSNet was proposed, which integrates 3D
spectral-spatial features and wavelet convolution. This framework integrates
multimodal information us-ing a multiscale convolutional attention module and
breaks through the classification performance bottleneck of traditional methods
by introducing multi-band decomposition and convolution operations in the
wavelet domain. The experiments showed that CWSSNet achieved 74.50\%, 82.73\%,
and 84.94\% in mean Intersection over Union (mIoU), mean Accuracy (mAcc), and
mean F1-score (mF1) respectively in Yugan County. It also obtained the highest
Intersection over Union (IoU) in the classifica-tion of water bodies,
vegetation, and bare land, demonstrating good robustness. Additionally, when
the training set proportion was 70\%, the increase in training time was
limited, and the classification effect was close to the optimal level,
indicating that the model maintains reliable performance under small-sample
training conditions.

</details>


### [70] [Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios](https://arxiv.org/abs/2509.09172)
*Chunxiao Li,Xiaoxiao Wang,Meiling Li,Boming Miao,Peng Sun,Yunjian Zhang,Xiangyang Ji,Yao Zhu*

Main category: cs.CV

TL;DR: 为应对AI生成图像对数字安全的挑战，本文引入RRDataset，全面评估AI图像检测模型在真实世界复杂条件下的鲁棒性，并通过基准测试揭示当前模型局限性，强调借鉴人类适应性来开发更强大的检测算法。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型快速发展，高度逼真的图像合成对数字安全和媒体可信度构成新挑战。尽管现有AI生成图像检测方法有所进展，但在复杂真实世界条件下评估其性能仍存在显著研究空白。

Method: 本文引入Real-World Robustness Dataset (RRDataset)，从三个维度全面评估检测模型：1) 场景泛化（涵盖七大主要真实场景）；2) 互联网传输鲁棒性（考量多轮社交媒体分享）；3) 再数字化鲁棒性（评估四种再数字化处理）。基于RRDataset，对17种检测器和10种视觉-语言模型(VLMs)进行基准测试，并开展涉及192名参与者的大规模人类少量学习能力研究。

Result: 基准测试结果揭示了当前AI检测方法在真实世界条件下的局限性。

Conclusion: 当前AI检测方法在真实世界条件下存在局限性，未来需要借鉴人类的适应性来开发更鲁棒的检测算法。

Abstract: With the rapid advancement of generative models, highly realistic image
synthesis has posed new challenges to digital security and media credibility.
Although AI-generated image detection methods have partially addressed these
concerns, a substantial research gap remains in evaluating their performance
under complex real-world conditions. This paper introduces the Real-World
Robustness Dataset (RRDataset) for comprehensive evaluation of detection models
across three dimensions: 1) Scenario Generalization: RRDataset encompasses
high-quality images from seven major scenarios (War and Conflict, Disasters and
Accidents, Political and Social Events, Medical and Public Health, Culture and
Religion, Labor and Production, and everyday life), addressing existing dataset
gaps from a content perspective. 2) Internet Transmission Robustness: examining
detector performance on images that have undergone multiple rounds of sharing
across various social media platforms. 3) Re-digitization Robustness: assessing
model effectiveness on images altered through four distinct re-digitization
methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on
RRDataset and conducted a large-scale human study involving 192 participants to
investigate human few-shot learning capabilities in detecting AI-generated
images. The benchmarking results reveal the limitations of current AI detection
methods under real-world conditions and underscore the importance of drawing on
human adaptability to develop more robust detection algorithms.

</details>


### [71] [Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection](https://arxiv.org/abs/2509.09183)
*Jiasheng Guo,Xin Gao,Yuxiang Yan,Guanghao Li,Jian Pu*

Main category: cs.CV

TL;DR: 本文提出一种轻量级、自适应的图像信号处理（ISP）插件Dark-ISP，用于低光照目标检测。该插件直接处理Bayer RAW图像，并通过创新的ISP模块分解和自增强机制，实现了端到端训练，在低光环境下以极少参数超越了现有最佳的RGB和RAW基线方法。


<details>
  <summary>Details</summary>
Motivation: 低光照目标检测对许多实际应用至关重要，但图像质量下降使其充满挑战。尽管RAW图像比RGB图像具有优越潜力，但现有方法存在信息损失或框架复杂的问题。

Method: 提出了一种轻量级、自适应的ISP插件Dark-ISP，直接处理Bayer RAW图像，实现目标检测的无缝端到端训练。关键创新点包括：
1. 将传统ISP流水线解构为顺序的线性（传感器校准）和非线性（色调映射）子模块，并将其重构为可通过任务驱动损失优化的可微分组件。每个模块都具备内容感知适应性和物理先验，实现与检测目标对齐的自动RAW-to-RGB转换。
2. 利用ISP流水线固有的级联结构，设计了自增强（Self-Boost）机制，促进子模块之间的协作。

Result: 通过在三个RAW图像数据集上进行的广泛实验表明，所提出的方法在具有挑战性的低光环境中，以最少的参数实现了优于当前最先进的RGB和RAW基线检测方法的性能。

Conclusion: Dark-ISP通过其创新的ISP解构和自适应机制，为低光照环境下的RAW图像目标检测提供了一个高性能、轻量级的解决方案，有效克服了现有方法的局限性。

Abstract: Low-light Object detection is crucial for many real-world applications but
remains challenging due to degraded image quality. While recent studies have
shown that RAW images offer superior potential over RGB images, existing
approaches either use RAW-RGB images with information loss or employ complex
frameworks. To address these, we propose a lightweight and self-adaptive Image
Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW
images in dark environments, enabling seamless end-to-end training for object
detection. Our key innovations are: (1) We deconstruct conventional ISP
pipelines into sequential linear (sensor calibration) and nonlinear (tone
mapping) sub-modules, recasting them as differentiable components optimized
through task-driven losses. Each module is equipped with content-aware
adaptability and physics-informed priors, enabling automatic RAW-to-RGB
conversion aligned with detection objectives. (2) By exploiting the ISP
pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that
facilitates cooperation between sub-modules. Through extensive experiments on
three RAW image datasets, we demonstrate that our method outperforms
state-of-the-art RGB- and RAW-based detection approaches, achieving superior
results with minimal parameters in challenging low-light environments.

</details>


### [72] [VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results](https://arxiv.org/abs/2509.09190)
*Hanwei Zhu,Haoning Wu,Zicheng Zhang,Lingyu Zhu,Yixuan Li,Peilin Chen,Shiqi Wang,Chris Wei Zhou,Linhan Cao,Wei Sun,Xiangyang Zhu,Weixia Zhang,Yucheng Zhu,Jing Liu,Dandan Zhu,Guangtao Zhai,Xiongkuo Min,Zhichao Zhang,Xinyue Li,Shubo Xu,Anh Dao,Yifan Li,Hongyuan Yu,Jiaojiao Yi,Yiding Tian,Yupeng Wu,Feiran Sun,Lijuan Liao,Song Jiang*

Main category: cs.CV

TL;DR: 本文总结了VQualA 2025视觉质量比较挑战赛，旨在评估和提升大型多模态模型（LMMs）对视觉质量差异的推理能力，并为此引入了新的基准和评估协议。


<details>
  <summary>Details</summary>
Motivation: 评估并增强最先进的LMMs对多张图像间视觉质量差异进行开放式、详细推理的能力。

Method: 引入了VQualA 2025挑战赛，包含一个新颖的基准，该基准拥有数千个从粗到细粒度的视觉质量比较任务（单张、成对和多图组）。评估协议包括基于2AFC的二元偏好和多项选择题（MCQs）。

Result: 约有100名参赛者提交了作品，其中五个模型展示了指令微调LMMs在质量评估方面的初步能力。

Conclusion: 此次挑战赛是迈向开放域视觉质量推理和比较的重要一步，并将促进未来对可解释且与人类对齐的质量评估系统的研究。

Abstract: This paper presents a summary of the VQualA 2025 Challenge on Visual Quality
Comparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025
Workshop on Visual Quality Assessment. The challenge aims to evaluate and
enhance the ability of state-of-the-art LMMs to perform open-ended and detailed
reasoning about visual quality differences across multiple images. To this end,
the competition introduces a novel benchmark comprising thousands of
coarse-to-fine grained visual quality comparison tasks, spanning single images,
pairs, and multi-image groups. Each task requires models to provide accurate
quality judgments. The competition emphasizes holistic evaluation protocols,
including 2AFC-based binary preference and multi-choice questions (MCQs).
Around 100 participants submitted entries, with five models demonstrating the
emerging capabilities of instruction-tuned LMMs on quality assessment. This
challenge marks a significant step toward open-domain visual quality reasoning
and comparison and serves as a catalyst for future research on interpretable
and human-aligned quality evaluation systems.

</details>


### [73] [MGTraj: Multi-Granularity Goal-Guided Human Trajectory Prediction with Recursive Refinement Network](https://arxiv.org/abs/2509.09200)
*Ge Sun,Jun Ma*

Main category: cs.CV

TL;DR: 提出MGTraj，一个新颖的多粒度目标引导模型，通过递归细化和辅助速度预测，在人体轨迹预测上达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有目标引导方法将预测任务解耦为粗粒度目标预测和细粒度轨迹完成，忽略了中间时间粒度的潜在效用。尽管多粒度表示能捕获人类动态，但将其有效整合到目标引导框架中仍具挑战。

Method: 我们提出MGTraj模型，它从粗到细粒度级别递归编码轨迹提议。在每个级别，一个基于Transformer的递归细化网络(RRN)捕获特征并预测渐进细化。不同粒度特征通过权重共享策略整合，并以速度预测作为辅助任务来增强性能。

Result: 在EHT/UCY和Stanford Drone Dataset上的综合实验结果表明，MGTraj优于基线方法，并在目标引导方法中取得了最先进的性能。

Conclusion: MGTraj通过有效结合多粒度建模和目标引导框架，显著提升了人体轨迹预测的准确性，并达到了当前最佳水平。

Abstract: Accurate human trajectory prediction is crucial for robotics navigation and
autonomous driving. Recent research has demonstrated that incorporating goal
guidance significantly enhances prediction accuracy by reducing uncertainty and
leveraging prior knowledge. Most goal-guided approaches decouple the prediction
task into two stages: goal prediction and subsequent trajectory completion
based on the predicted goal, which operate at extreme granularities:
coarse-grained goal prediction forecasts the overall intention, while
fine-grained trajectory completion needs to generate the positions for all
future timesteps. The potential utility of intermediate temporal granularity
remains largely unexplored, which motivates multi-granularity trajectory
modeling. While prior work has shown that multi-granularity representations
capture diverse scales of human dynamics and motion patterns, effectively
integrating this concept into goal-guided frameworks remains challenging. In
this paper, we propose MGTraj, a novel Multi-Granularity goal-guided model for
human Trajectory prediction. MGTraj recursively encodes trajectory proposals
from coarse to fine granularity levels. At each level, a transformer-based
recursive refinement network (RRN) captures features and predicts progressive
refinements. Features across different granularities are integrated using a
weight-sharing strategy, and velocity prediction is employed as an auxiliary
task to further enhance performance. Comprehensive experimental results in
EHT/UCY and Stanford Drone Dataset indicate that MGTraj outperforms baseline
methods and achieves state-of-the-art performance among goal-guided methods.

</details>


### [74] [Medverse: A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement](https://arxiv.org/abs/2509.09232)
*Jiesi Hu,Jianfeng Cao,Yanwu Yang,Chenfei Ye,Yixuan Zhang,Hanyang Peng,Ting Ma*

Main category: cs.CV

TL;DR: Medverse是一个通用的3D医学图像ICL模型，通过渐进式预测和块状交叉注意力机制，在多任务和多器官数据集上实现了高精度预测和全局解剖理解。


<details>
  <summary>Details</summary>
Motivation: 当前的医学图像语境学习（ICL）模型存在局限性：无法同时实现高保真预测和全局解剖理解；缺乏统一模型来处理多样化的医学成像任务和解剖区域，导致ICL在医学成像领域的潜力未被充分发掘。

Method: 本文提出了Medverse模型，一个通用的3D医学成像ICL模型。它采用下一尺度自回归语境学习框架，通过从粗到精的渐进式预测来生成一致、全分辨率的体数据输出，并实现多尺度解剖感知。此外，Medverse引入了块状交叉注意力模块，以实现上下文和目标输入之间的远距离交互，同时通过空间稀疏性保持计算效率。模型在涵盖分割、转换和增强任务的22个数据集上进行训练。

Result: Medverse在广泛的未见数据集（包括未见的临床中心、器官、物种和成像模态）上进行了评估，结果表明其显著优于现有的ICL基线模型。

Conclusion: Medverse为语境学习建立了一种新的范式，在通用3D医学成像领域表现出色，能实现高保真预测和全局解剖理解，有效解决了现有ICL模型的局限性。

Abstract: In-context learning (ICL) offers a promising paradigm for universal medical
image analysis, enabling models to perform diverse image processing tasks
without retraining. However, current ICL models for medical imaging remain
limited in two critical aspects: they cannot simultaneously achieve
high-fidelity predictions and global anatomical understanding, and there is no
unified model trained across diverse medical imaging tasks (e.g., segmentation
and enhancement) and anatomical regions. As a result, the full potential of ICL
in medical imaging remains underexplored. Thus, we present \textbf{Medverse}, a
universal ICL model for 3D medical imaging, trained on 22 datasets covering
diverse tasks in universal image segmentation, transformation, and enhancement
across multiple organs, imaging modalities, and clinical centers. Medverse
employs a next-scale autoregressive in-context learning framework that
progressively refines predictions from coarse to fine, generating consistent,
full-resolution volumetric outputs and enabling multi-scale anatomical
awareness. We further propose a blockwise cross-attention module that
facilitates long-range interactions between context and target inputs while
preserving computational efficiency through spatial sparsity. Medverse is
extensively evaluated on a broad collection of held-out datasets covering
previously unseen clinical centers, organs, species, and imaging modalities.
Results demonstrate that Medverse substantially outperforms existing ICL
baselines and establishes a novel paradigm for in-context learning. Code and
model weights will be made publicly available. Our model are publicly available
at https://github.com/jiesihu/Medverse.

</details>


### [75] [CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification](https://arxiv.org/abs/2509.09242)
*Mustafa Yurdakul,Sakir Tasdemir*

Main category: cs.CV

TL;DR: 本研究提出了一种名为CoAtNeXt的混合模型，用于胃组织图像的自动化组织病理学分类，在两个公开数据集上均取得了优异的性能，超越了现有CNN和ViT模型，有望辅助病理诊断。


<details>
  <summary>Details</summary>
Motivation: 胃部疾病的早期诊断至关重要。然而，目前的组织病理学检查完全依赖人工，效率低、劳动强度大，且病理学家之间存在评估差异，容易遗漏关键发现，缺乏标准化流程。因此，亟需开发自动化、可靠、高效的胃组织分析方法。

Method: 本研究提出了一种名为CoAtNeXt的新型混合模型，用于胃组织图像分类。该模型在CoAtNet架构的基础上，将MBConv层替换为增强的ConvNeXtV2块。此外，整合了卷积块注意力模块（CBAM）以通过通道和空间注意力机制增强局部特征提取。该架构经过扩展以平衡计算效率和分类性能。CoAtNeXt在两个公开数据集（HMU-GC-HE-30K用于八分类，GasHisSDB用于二分类）上进行评估，并与10种卷积神经网络（CNN）模型和10种视觉Transformer（ViT）模型进行比较。

Result: CoAtNeXt在HMU-GC-HE-30K数据集上取得了96.47%的准确率、96.60%的精确率、96.47%的召回率、96.45%的F1分数和99.89%的AUC。在GasHisSDB数据集上，它达到了98.29%的准确率、98.07%的精确率、98.41%的召回率、98.23%的F1分数和99.90%的AUC。CoAtNeXt的性能优于所有测试的CNN和ViT模型，并超越了文献中之前的研究。

Conclusion: 实验结果表明，CoAtNeXt是一种用于胃组织图像组织病理学分类的鲁棒架构，在二分类和多分类任务上均表现出色。这凸显了其通过提高诊断准确性和减轻工作量来辅助病理学家的潜力。

Abstract: Background and objective Early diagnosis of gastric diseases is crucial to
prevent fatal outcomes. Although histopathologic examination remains the
diagnostic gold standard, it is performed entirely manually, making evaluations
labor-intensive and prone to variability among pathologists. Critical findings
may be missed, and lack of standard procedures reduces consistency. These
limitations highlight the need for automated, reliable, and efficient methods
for gastric tissue analysis. Methods In this study, a novel hybrid model named
CoAtNeXt was proposed for the classification of gastric tissue images. The
model is built upon the CoAtNet architecture by replacing its MBConv layers
with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block
Attention Module (CBAM) is integrated to improve local feature extraction
through channel and spatial attention mechanisms. The architecture was scaled
to achieve a balance between computational efficiency and classification
performance. CoAtNeXt was evaluated on two publicly available datasets,
HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary
classification, and was compared against 10 Convolutional Neural Networks
(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved
96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%
AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%
precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all
CNN and ViT models tested and surpassed previous studies in the literature.
Conclusion Experimental results show that CoAtNeXt is a robust architecture for
histopathological classification of gastric tissue images, providing
performance on binary and multiclass. Its highlights its potential to assist
pathologists by enhancing diagnostic accuracy and reducing workload.

</details>


### [76] [Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis](https://arxiv.org/abs/2509.09254)
*Jing Hao,Yuxuan Fan,Yanpeng Sun,Kaixin Guo,Lizhuo Lin,Jinrong Yang,Qi Yong H. Ai,Lun M. Wong,Hao Tang,Kuo Feng Hung*

Main category: cs.CV

TL;DR: 本文针对牙科全景X光片解读，构建了首个大规模多模态指令数据集MMOral和评估基准MMOral-Bench。研究发现现有LVLMs表现不佳，因此提出了基于MMOral数据集微调的OralGPT模型，显著提升了该专业领域的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大视觉语言模型（LVLMs）在通用医疗任务上表现突出，但在牙科等专业领域（特别是全景X光片解读）的有效性尚未充分探索。牙科影像解剖结构复杂、病理线索细微，现有医疗基准和指令数据集未能涵盖这些挑战。

Method: 1. 引入了MMOral，首个大规模多模态指令数据集和基准，专门用于全景X光片解读，包含20,563张标注图像和130万个指令遵循实例（涵盖属性提取、报告生成、视觉问答和图像接地对话）。2. 提出了MMOral-Bench，一个涵盖牙科五个关键诊断维度的综合评估套件。3. 基于MMOral数据集，通过对Qwen2.5-VL-7B进行监督微调（SFT），提出了OralGPT模型。

Result: 1. 在MMOral-Bench上评估了64个LVLM，发现即使是表现最好的GPT-4o也仅达到41.45%的准确率，揭示了当前模型在该领域的显著局限性。2. OralGPT模型通过单次SFT，取得了显著的性能提升，例如，性能提升了24.73%。

Conclusion: MMOral数据集和OralGPT模型为智能牙科提供了关键基础，并有望推动牙科领域开发出更具临床影响力的多模态AI系统。

Abstract: Recent advances in large vision-language models (LVLMs) have demonstrated
strong performance on general-purpose medical tasks. However, their
effectiveness in specialized domains such as dentistry remains underexplored.
In particular, panoramic X-rays, a widely used imaging modality in oral
radiology, pose interpretative challenges due to dense anatomical structures
and subtle pathological cues, which are not captured by existing medical
benchmarks or instruction datasets. To this end, we introduce MMOral, the first
large-scale multimodal instruction dataset and benchmark tailored for panoramic
X-ray interpretation. MMOral consists of 20,563 annotated images paired with
1.3 million instruction-following instances across diverse task types,
including attribute extraction, report generation, visual question answering,
and image-grounded dialogue. In addition, we present MMOral-Bench, a
comprehensive evaluation suite covering five key diagnostic dimensions in
dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the
best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing
significant limitations of current models in this domain. To promote the
progress of this specific domain, we also propose OralGPT, which conducts
supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated
MMOral instruction dataset. Remarkably, a single epoch of SFT yields
substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a
24.73% improvement. Both MMOral and OralGPT hold significant potential as a
critical foundation for intelligent dentistry and enable more clinically
impactful multimodal AI systems in the dental field. The dataset, model,
benchmark, and evaluation suite are available at
https://github.com/isbrycee/OralGPT.

</details>


### [77] [DATE: Dynamic Absolute Time Enhancement for Long Video Understanding](https://arxiv.org/abs/2509.09263)
*Chao Yuan,Yang Yang,Yehui Yang,Zach Cheng*

Main category: cs.CV

TL;DR: 本文提出DATE方法，通过时间戳注入机制和语义引导的时间感知相似性采样策略，显著提升多模态大语言模型对长视频的绝对时间理解和关键事件定位能力，在长视频基准测试上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在长视频理解中面临挑战，尤其是在需要精确时间推理和事件定位的任务上。传统方法采用统一帧采样和隐式位置编码，难以处理长程依赖，导致信息丢失和时间理解能力下降。

Method: 提出Dynamic Absolute Time Enhancement (DATE) 方法，包含：1. 时间戳注入机制 (TIM)，将视频帧嵌入与文本时间戳交错，构建连续时间参考系统。2. 语义引导的时间感知相似性采样 (TASS) 策略，将视频采样重构为视觉-语言检索任务，通过两阶段算法确保语义相关性和时间覆盖：将查询扩展为描述性标题以更好地与视觉特征对齐，并采用相似性驱动、时间正则化的贪婪策略采样关键事件。

Result: 该方法在绝对时间理解和关键事件定位方面取得了显著改进，在小时级视频基准测试中，其7B和72B模型均达到最先进的性能。特别是，7B模型在某些基准测试上甚至超越了许多72B模型。

Conclusion: DATE通过创新的时间感知增强和采样策略，有效解决了MLLMs在长视频理解中的时间推理和事件定位难题，显著提升了模型性能，并在多个基准测试上实现了新的SOTA。

Abstract: Long video understanding remains a fundamental challenge for multimodal large
language models (MLLMs), particularly in tasks requiring precise temporal
reasoning and event localization. Existing approaches typically adopt uniform
frame sampling and rely on implicit position encodings to model temporal order.
However, these methods struggle with long-range dependencies, leading to
critical information loss and degraded temporal comprehension. In this paper,
we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal
awareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a
semantically guided Temporal-Aware Similarity Sampling (TASS) strategy.
Specifically, we interleave video frame embeddings with textual timestamp
tokens to construct a continuous temporal reference system. We further
reformulate the video sampling problem as a vision-language retrieval task and
introduce a two-stage algorithm to ensure both semantic relevance and temporal
coverage: enriching each query into a descriptive caption to better align with
the vision feature, and sampling key event with a similarity-driven temporally
regularized greedy strategy. Our method achieves remarkable improvements w.r.t.
absolute time understanding and key event localization, resulting in
state-of-the-art performance among 7B and 72B models on hour-long video
benchmarks. Particularly, our 7B model even exceeds many 72B models on some
benchmarks.

</details>


### [78] [Unified Start, Personalized End: Progressive Pruning for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2509.09267)
*Linhao Li,Yiwen Ye,Ziyang Chen,Yong Xia*

Main category: cs.CV

TL;DR: 本文提出PSP-Seg，一个渐进式剪枝框架，用于实现动态高效的3D医学图像分割，显著降低资源消耗同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 3D医学图像分割通常面临高资源和时间消耗，限制了其在临床环境中的扩展和快速部署。现有高效分割模型通常是静态且手动设计的，限制了其在不同任务中的适应性，难以平衡性能与资源效率。

Method: PSP-Seg是一个渐进式剪枝框架，它从一个冗余模型开始，并通过块级剪枝和功能解耦损失的组合迭代修剪冗余模块，从而实现动态高效的3D分割。

Result: PSP-Seg的轻量级变体PSP-Seg-S在五个公共数据集上，实现了与nnU-Net相当的性能，同时将GPU内存使用量减少42-45%，训练时间减少29-48%，参数数量减少83-87%。

Conclusion: 研究结果表明PSP-Seg作为一种经济高效且高性能的替代方案，在广泛的临床应用中具有巨大潜力。

Abstract: 3D medical image segmentation often faces heavy resource and time
consumption, limiting its scalability and rapid deployment in clinical
environments. Existing efficient segmentation models are typically static and
manually designed prior to training, which restricts their adaptability across
diverse tasks and makes it difficult to balance performance with resource
efficiency. In this paper, we propose PSP-Seg, a progressive pruning framework
that enables dynamic and efficient 3D segmentation. PSP-Seg begins with a
redundant model and iteratively prunes redundant modules through a combination
of block-wise pruning and a functional decoupling loss. We evaluate PSP-Seg on
five public datasets, benchmarking it against seven state-of-the-art models and
six efficient segmentation models. Results demonstrate that the lightweight
variant, PSP-Seg-S, achieves performance on par with nnU-Net while reducing GPU
memory usage by 42-45%, training time by 29-48%, and parameter number by 83-87%
across all datasets. These findings underscore PSP-Seg's potential as a
cost-effective yet high-performing alternative for widespread clinical
application.

</details>


### [79] [Visual Programmability: A Guide for Code-as-Thought in Chart Understanding](https://arxiv.org/abs/2509.09286)
*Bohao Tang,Yan Ma,Fei Zhang,Jiadi Su,Ethan Chern,Zhulin Hu,Zhixin Wang,Pengfei Liu,Ya Zhang*

Main category: cs.CV

TL;DR: 本研究提出Code-as-Thought (CaT) 方法和“视觉可编程性”概念，使视觉语言模型(VLM)能自适应地选择基于代码的推理或直接视觉分析来理解图表，并通过新型双重奖励强化学习进行训练，显著提升了图表理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解方法存在局限性：部分依赖外部工具，导致脆弱且受限于预定义工具集；另一些则微调特定模型，常采用单一推理策略（如文本CoT），其中间步骤难以验证，从而阻碍了强化学习信号的应用。

Method: 我们提出Code-as-Thought (CaT) 方法，将图表视觉信息表示为可验证的符号格式。针对固定代码实现对复杂图表的不足，引入“视觉可编程性”概念，这是一个可学习的属性，用于判断图表-问题对更适合用代码还是直接视觉分析解决。在此基础上，构建了一个自适应框架，VLM在此框架中学习在CaT路径和直接视觉推理路径之间进行选择。模型的选择策略通过强化学习训练，并采用新颖的双重奖励系统：结合数据准确性奖励以防止数值幻觉，以及决策奖励以指导模型何时使用何种策略，避免其只采用单一推理模式。

Result: 实验结果表明，在多样化的图表理解基准测试中，该方法展现出强大而鲁棒的性能。

Conclusion: 我们的工作表明，视觉语言模型不仅能被教授如何推理，还能被教授如何选择推理方式，为每项任务动态选择最优推理路径。

Abstract: Chart understanding presents a critical test to the reasoning capabilities of
Vision-Language Models (VLMs). Prior approaches face critical limitations: some
rely on external tools, making them brittle and constrained by a predefined
toolkit, while others fine-tune specialist models that often adopt a single
reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate
steps of text-based reasoning are difficult to verify, which complicates the
use of reinforcement-learning signals that reward factual accuracy. To address
this, we propose a Code-as-Thought (CaT) approach to represent the visual
information of a chart in a verifiable, symbolic format. Our key insight is
that this strategy must be adaptive: a fixed, code-only implementation
consistently fails on complex charts where symbolic representation is
unsuitable. This finding leads us to introduce Visual Programmability: a
learnable property that determines if a chart-question pair is better solved
with code or direct visual analysis. We implement this concept in an adaptive
framework where a VLM learns to choose between the CaT pathway and a direct
visual reasoning pathway. The selection policy of the model is trained with
reinforcement learning using a novel dual-reward system. This system combines a
data-accuracy reward to ground the model in facts and prevent numerical
hallucination, with a decision reward that teaches the model when to use each
strategy, preventing it from defaulting to a single reasoning mode. Experiments
demonstrate strong and robust performance across diverse chart-understanding
benchmarks. Our work shows that VLMs can be taught not only to reason but also
how to reason, dynamically selecting the optimal reasoning pathway for each
task.

</details>


### [80] [Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training](https://arxiv.org/abs/2509.09290)
*Anthony P. Addison,Felix Wagner,Wentian Xu,Natalie Voets,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: 本文开发了一种基于U-net的脑部病变分割模型，通过引入模态无关输入通道和合成模态增强方案，使其能有效处理训练中未见和异构的MRI模态。


<details>
  <summary>Details</summary>
Motivation: 现有的脑MRI病变分割模型受限于固定模态，或在泛化到未见模态时丢失模态特异性信息。研究动机是开发一个能在推理时处理任何可用（包括训练中未见及异构组合）成像模态的模型。

Method: 通过对U-net架构进行简单改动，整合了模态无关输入通道（或路径）与模态特定输入通道。为训练模态无关组件，开发了一种能合成人工MRI模态的图像增强方案，该方案在保持解剖完整性的同时，差异化改变病变和健康脑组织的外观以创建人工对比。该方法在包含5种病变和8种模态的8个MRI数据库上进行了评估。

Result: 该方法在有效处理训练中遇到的MRI模态的同时，能够处理新的、未见的模态，并利用它们改进分割效果。

Conclusion: 所提出的方法通过对U-net进行实用性改动并结合人工模态合成增强，成功开发了一个能够泛化到未见和异构MRI模态的模型，同时保持了对已知模态的处理能力，并能利用新模态提升分割性能。

Abstract: Segmentation models are important tools for the detection and analysis of
lesions in brain MRI. Depending on the type of brain pathology that is imaged,
MRI scanners can acquire multiple, different image modalities (contrasts). Most
segmentation models for multimodal brain MRI are restricted to fixed modalities
and cannot effectively process new ones at inference. Some models generalize to
unseen modalities but may lose discriminative modality-specific information.
This work aims to develop a model that can perform inference on data that
contain image modalities unseen during training, previously seen modalities,
and heterogeneous combinations of both, thus allowing a user to utilize any
available imaging modalities. We demonstrate this is possible with a simple,
thus practical alteration to the U-net architecture, by integrating a
modality-agnostic input channel or pathway, alongside modality-specific input
channels. To train this modality-agnostic component, we develop an image
augmentation scheme that synthesizes artificial MRI modalities. Augmentations
differentially alter the appearance of pathological and healthy brain tissue to
create artificial contrasts between them while maintaining realistic anatomical
integrity. We evaluate the method using 8 MRI databases that include 5 types of
pathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and
white matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI,
DWI, ADC and FLAIR). The results demonstrate that the approach preserves the
ability to effectively process MRI modalities encountered during training,
while being able to process new, unseen modalities to improve its segmentation.
Project code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG

</details>


### [81] [Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception](https://arxiv.org/abs/2509.09297)
*Spyridon Loukovitis,Anastasios Arsenos,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 为解决无人机空对空目标检测中的未知目标和数据损坏问题，本文提出了一种模型无关的开集检测框架，通过语义不确定性估计和多种技术，显著提升了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统闭集检测器在无人机真实空对空场景中，面对领域漂移和飞行数据损坏时性能严重下降，对安全关键应用构成风险。

Method: 提出一种新颖、模型无关的开集检测框架，专为基于嵌入的检测器设计。该方法通过在嵌入空间进行熵建模来估计语义不确定性，并结合谱归一化和温度缩放来增强开集判别能力。同时，引入背景抑制以进一步提升鲁棒性。

Result: 在AOT空中基准测试和真实飞行测试中，相较于标准YOLO检测器，AUROC相对增益高达10%。研究还表明，背景抑制在不影响检测精度的前提下，进一步增强了鲁棒性。

Conclusion: 所提出的解决方案能有效处理未知目标和损坏数据，显著提升了无人机在动态空对空环境中感知的可靠性和鲁棒性。

Abstract: Open-set detection is crucial for robust UAV autonomy in air-to-air object
detection under real-world conditions. Traditional closed-set detectors degrade
significantly under domain shifts and flight data corruption, posing risks to
safety-critical applications. We propose a novel, model-agnostic open-set
detection framework designed specifically for embedding-based detectors. The
method explicitly handles unknown object rejection while maintaining robustness
against corrupted flight data. It estimates semantic uncertainty via entropy
modeling in the embedding space and incorporates spectral normalization and
temperature scaling to enhance open-set discrimination. We validate our
approach on the challenging AOT aerial benchmark and through extensive
real-world flight tests. Comprehensive ablation studies demonstrate consistent
improvements over baseline methods, achieving up to a 10\% relative AUROC gain
compared to standard YOLO-based detectors. Additionally, we show that
background rejection further strengthens robustness without compromising
detection accuracy, making our solution particularly well-suited for reliable
UAV perception in dynamic air-to-air environments.

</details>


### [82] [Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization](https://arxiv.org/abs/2509.09307)
*Zhengzhao Lai,Youbin Zheng,Zhenyang Cai,Haonan Lyu,Jinpu Yang,Hongqing Liang,Yan Hu,Benyou Wang*

Main category: cs.CV

TL;DR: 本文创建了首个材料表征图像理解基准MatCha，并评估了多模态大语言模型（MLLMs），发现现有MLLMs在理解真实材料表征图像方面与人类专家存在显著差距，尤其是在需要高水平专业知识和复杂视觉感知的问题上。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在材料科学的生成和预测任务中显示出前景，但其理解真实世界材料表征图像数据的能力尚未得到充分探索，存在研究空白。

Method: 我们提出了MatCha，这是首个用于材料表征图像理解的基准，包含1,500个需要专家级领域知识的问题。MatCha涵盖材料研究的四个关键阶段，包含21个不同的任务，旨在反映材料科学家面临的真实挑战。我们使用MatCha评估了当前最先进的MLLMs。

Result: 评估结果显示，现有MLLMs在MatCha上的表现与人类专家相比存在显著差距。这些模型在处理需要更高水平专业知识和复杂视觉感知的问题时性能下降，并且简单的few-shot和chain-of-thought提示策略未能有效改善这些局限性。

Conclusion: 研究结果表明，现有MLLMs对真实世界的材料表征场景适应性仍然有限。我们希望MatCha能促进未来在新材料发现和自主科学智能体等领域的研究，并已开源基准。

Abstract: Materials characterization is fundamental to acquiring materials information,
revealing the processing-microstructure-property relationships that guide
material design and optimization. While multimodal large language models
(MLLMs) have recently shown promise in generative and predictive tasks within
materials science, their capacity to understand real-world characterization
imaging data remains underexplored. To bridge this gap, we present MatCha, the
first benchmark for materials characterization image understanding, comprising
1,500 questions that demand expert-level domain expertise. MatCha encompasses
four key stages of materials research comprising 21 distinct tasks, each
designed to reflect authentic challenges faced by materials scientists. Our
evaluation of state-of-the-art MLLMs on MatCha reveals a significant
performance gap compared to human experts. These models exhibit degradation
when addressing questions requiring higher-level expertise and sophisticated
visual perception. Simple few-shot and chain-of-thought prompting struggle to
alleviate these limitations. These findings highlight that existing MLLMs still
exhibit limited adaptability to real-world materials characterization
scenarios. We hope MatCha will facilitate future research in areas such as new
material discovery and autonomous scientific agents. MatCha is available at
https://github.com/FreedomIntelligence/MatCha.

</details>


### [83] [Learning Object-Centric Representations in SAR Images with Multi-Level Feature Fusion](https://arxiv.org/abs/2509.09298)
*Oh-Tae Jang,Min-Gon Cho,Kyung-Tae Kim*

Main category: cs.CV

TL;DR: SlotSAR是一种新颖的以目标为中心的学习框架，无需掩码标注即可在SAR图像中解耦目标与背景，通过多级特征和槽位注意力实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: SAR图像中复杂的背景杂波（地形反射、散斑噪声）常与目标相似，导致模型提取纠缠或虚假特征，损害清晰目标表示。

Method: 提出SlotSAR框架，无需掩码标注即可解耦目标表示。它结合SARATR-X的高层语义特征和小波散射网络的低层散射特征，以获得互补的多级表示，并通过多级槽位注意力模块整合这些特征，增强槽位表示的独特性，实现有效的OCL。

Result: 实验结果表明，SlotSAR在SAR图像处理中实现了最先进的性能，与现有OCL方法相比能更好地保留结构细节。

Conclusion: SlotSAR成功解决了SAR图像中目标表示与背景杂波解耦的挑战，显著提升了性能。

Abstract: Synthetic aperture radar (SAR) images contain not only targets of interest
but also complex background clutter, including terrain reflections and speckle
noise. In many cases, such clutter exhibits intensity and patterns that
resemble targets, leading models to extract entangled or spurious features.
Such behavior undermines the ability to form clear target representations,
regardless of the classifier. To address this challenge, we propose a novel
object-centric learning (OCL) framework, named SlotSAR, that disentangles
target representations from background clutter in SAR images without mask
annotations. SlotSAR first extracts high-level semantic features from SARATR-X
and low-level scattering features from the wavelet scattering network in order
to obtain complementary multi-level representations for robust target
characterization. We further present a multi-level slot attention module that
integrates these low- and high-level features to enhance slot-wise
representation distinctiveness, enabling effective OCL. Experimental results
demonstrate that SlotSAR achieves state-of-the-art performance in SAR imagery
by preserving structural details compared to existing OCL methods.

</details>


### [84] [You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative Perception](https://arxiv.org/abs/2509.09310)
*Hao Si,Ehsan Javanmardi,Manabu Tsukada*

Main category: cs.CV

TL;DR: 提出了一种名为PHCP的新型异构协同感知框架，通过在推理时自训练适配器，无需联合训练或标记数据，解决了现有方法在实际应用中的不便，并在OPV2V数据集上取得了与SOTA方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 协同感知能帮助车辆克服个体感知限制，但在真实世界中，不同车辆的模型常因制造商差异而异构。现有方法通过微调或联合训练来弥合域鸿沟，但这在实际应用中不切实际，因为每次新增协作者都需要联合训练或提前存储大量模型。因此，研究动机是探究能否在推理阶段直接解决此挑战，从而避免联合训练。

Method: 引入了渐进式异构协同感知（PHCP）框架，将问题表述为小样本无监督域适应。PHCP通过在推理过程中自训练一个适配器来动态对齐特征，从而消除了对标记数据和联合训练的需求。

Result: 在OPV2V数据集上进行的广泛实验表明，PHCP在多样化的异构场景中均表现出强大的性能。尤其值得一提的是，PHCP仅使用少量未标记数据，就达到了与在整个数据集上训练的SOTA方法相当的性能。

Conclusion: PHCP提供了一种新颖且实用的方法，可以在推理阶段解决异构协同感知问题，无需联合训练或大量标记数据，并且取得了与现有SOTA方法相媲美的性能，显著提升了异构协同感知的实用性。

Abstract: Collaborative perception enables vehicles to overcome individual perception
limitations by sharing information, allowing them to see further and through
occlusions. In real-world scenarios, models on different vehicles are often
heterogeneous due to manufacturer variations. Existing methods for
heterogeneous collaborative perception address this challenge by fine-tuning
adapters or the entire network to bridge the domain gap. However, these methods
are impractical in real-world applications, as each new collaborator must
undergo joint training with the ego vehicle on a dataset before inference, or
the ego vehicle stores models for all potential collaborators in advance.
Therefore, we pose a new question: Can we tackle this challenge directly during
inference, eliminating the need for joint training? To answer this, we
introduce Progressive Heterogeneous Collaborative Perception (PHCP), a novel
framework that formulates the problem as few-shot unsupervised domain
adaptation. Unlike previous work, PHCP dynamically aligns features by
self-training an adapter during inference, eliminating the need for labeled
data and joint training. Extensive experiments on the OPV2V dataset demonstrate
that PHCP achieves strong performance across diverse heterogeneous scenarios.
Notably, PHCP achieves performance comparable to SOTA methods trained on the
entire dataset while using only a small amount of unlabeled data.

</details>


### [85] [Image Recognition with Vision and Language Embeddings of VLMs](https://arxiv.org/abs/2509.09311)
*Illia Volkov,Nikita Kisel,Klara Janouskova,Jiri Matas*

Main category: cs.CV

TL;DR: 本文对多模态视觉-语言模型（VLMs）的语言引导和纯视觉图像分类能力进行了全面评估，并提出了一种基于类精度融合的方法来提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在图像-文本对齐方面表现出色，但其纯视觉推理能力尚未得到充分探索。研究旨在全面评估VLMs的语言引导和纯视觉图像分类性能，并探索其互补性。

Method: 研究评估了一系列双编码器VLM（包括SigLIP 2和RADIOv2.5）在ImageNet-1k验证集上的语言引导和纯视觉图像分类性能。分析了提示设计、类别多样性、k-NN邻居数和参考集大小等关键因素对准确性的影响。引入了一种简单的、无需学习的基于每类别精度的融合方法。

Result: 结果表明，语言和视觉提供互补优势，某些类别偏爱文本提示，而另一些则通过视觉相似性更好地处理。提出的融合方法利用这种互补性，提高了分类性能。

Conclusion: VLMs的语言和视觉能力具有互补性。通过简单的融合方法，可以有效结合两者的优势，提升图像分类表现。

Abstract: Vision-language models (VLMs) have enabled strong zero-shot classification
through image-text alignment. Yet, their purely visual inference capabilities
remain under-explored. In this work, we conduct a comprehensive evaluation of
both language-guided and vision-only image classification with a diverse set of
dual-encoder VLMs, including both well-established and recent models such as
SigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the
ImageNet-1k validation set and its label-corrected variant. The key factors
affecting accuracy are analysed, including prompt design, class diversity, the
number of neighbours in k-NN, and reference set size. We show that language and
vision offer complementary strengths, with some classes favouring textual
prompts and others better handled by visual similarity. To exploit this
complementarity, we introduce a simple, learning-free fusion method based on
per-class precision that improves classification performance. The code is
available at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition.

</details>


### [86] [Fine-Grained Customized Fashion Design with Image-into-Prompt benchmark and dataset from LMM](https://arxiv.org/abs/2509.09324)
*Hui Li,Yi You,Qiqi Chen,Bingfeng Zhang,George Q. Huang*

Main category: cs.CV

TL;DR: 提出BUG工作流，利用多模态大模型和图文输入，解决生成式AI在时尚设计中精细化定制的文本不确定性问题，并创建新数据集FashionEdit进行评估。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI模型在时尚设计中，虽能轻松生成设计，但在精细化定制方面，由于最终用户缺乏专业背景知识，文本输入的不确定性导致其效果不佳。

Method: 提出了Better Understanding Generation (BUG) 工作流，结合大型多模态模型 (LMM) 和“聊天+图像转提示”的方式，自动创建并精细化定制服装设计。为验证模型有效性，构建了模拟真实服装设计流程的新数据集FashionEdit，并通过生成相似度、用户满意度和质量进行评估。

Result: 所提出的框架能够超越语言限制释放用户的创造潜力，并显著降低服装设计和编辑的门槛，无需进一步的人工干预即可实现复杂设计。

Conclusion: BUG工作流有效解决了生成式AI在服装设计精细化定制方面的挑战，通过多模态输入增强了用户体验，使非专业人士也能轻松进行复杂设计。

Abstract: Generative AI evolves the execution of complex workflows in industry, where
the large multimodal model empowers fashion design in the garment industry.
Current generation AI models magically transform brainstorming into fancy
designs easily, but the fine-grained customization still suffers from text
uncertainty without professional background knowledge from end-users. Thus, we
propose the Better Understanding Generation (BUG) workflow with LMM to
automatically create and fine-grain customize the cloth designs from chat with
image-into-prompt. Our framework unleashes users' creative potential beyond
words and also lowers the barriers of clothing design/editing without further
human involvement. To prove the effectiveness of our model, we propose a new
FashionEdit dataset that simulates the real-world clothing design workflow,
evaluated from generation similarity, user satisfaction, and quality. The code
and dataset: https://github.com/detectiveli/FashionEdit.

</details>


### [87] [Exploring Pre-training Across Domains for Few-Shot Surgical Skill Assessment](https://arxiv.org/abs/2509.09327)
*Dimitrios Anastasiou,Razvan Caramalau,Nazir Sirajudeen,Matthew Boal,Philip Edwards,Justin Collins,John Kelly,Ashwin Sridhar,Maxine Tran,Faiz Mumtaz,Nevil Pavithran,Nader Francis,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: 本文研究了自监督预训练策略对少样本手术技能评估（SSA）性能的影响。结果表明，小规模但领域相关的预训练数据集表现优于大规模非对齐数据集，并且结合手术特定数据能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 自动化手术技能评估（SSA）面临技能标注稀缺的挑战，这限制了模型开发。少样本学习（FSL）提供了在监督有限情况下开发模型的潜力，但其成功高度依赖于有效的预训练。然而，预训练在SSA领域，尤其是在FSL的背景下，尚未得到充分探索。

Method: 将SSA公式化为少样本任务，并研究自监督预训练策略如何影响下游少样本SSA性能。作者标注了一个公共机器人手术数据集的客观结构化技术技能评估（OSATS）分数，并在1、2、5样本三种少样本设置下评估了各种预训练来源。同时，量化了领域相似性，并分析了领域差距以及在预训练中包含手术特定数据对可迁移性的影响。

Result: 研究显示，小规模但领域相关的数据集表现优于大规模但对齐度较低的数据集，在1、2、5样本设置下分别实现了60.16%、66.03%和73.65%的准确率。此外，将手术特定数据与领域相关的外部数据集结合进行预训练，能显著提升下游性能，平均准确率提高1.22%，F1分数提高2.28%。然而，将手术特定数据与相似性较低但大规模的来源结合，反而可能导致性能下降。

Conclusion: 对于少样本手术技能评估，有效的预训练至关重要。使用小规模但领域相关的数据集进行预训练，并结合程序特定的数据，能够显著提升模型性能，优于使用大规模但领域匹配度较低的预训练源。

Abstract: Automated surgical skill assessment (SSA) is a central task in surgical
computer vision. Developing robust SSA models is challenging due to the
scarcity of skill annotations, which are time-consuming to produce and require
expert consensus. Few-shot learning (FSL) offers a scalable alternative
enabling model development with minimal supervision, though its success
critically depends on effective pre-training. While widely studied for several
surgical downstream tasks, pre-training has remained largely unexplored in SSA.
In this work, we formulate SSA as a few-shot task and investigate how
self-supervised pre-training strategies affect downstream few-shot SSA
performance. We annotate a publicly available robotic surgery dataset with
Objective Structured Assessment of Technical Skill (OSATS) scores, and evaluate
various pre-training sources across three few-shot settings. We quantify domain
similarity and analyze how domain gap and the inclusion of procedure-specific
data into pre-training influence transferability. Our results show that small
but domain-relevant datasets can outperform large scale, less aligned ones,
achieving accuracies of 60.16%, 66.03%, and 73.65% in the 1-, 2-, and 5-shot
settings, respectively. Moreover, incorporating procedure-specific data into
pre-training with a domain-relevant external dataset significantly boosts
downstream performance, with an average gain of +1.22% in accuracy and +2.28%
in F1-score; however, applying the same strategy with less similar but
large-scale sources can instead lead to performance degradation. Code and
models are available at https://github.com/anastadimi/ssa-fsl.

</details>


### [88] [Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles](https://arxiv.org/abs/2509.09349)
*Ian Nell,Shane Gilroy*

Main category: cs.CV

TL;DR: 本研究提出了一种基于外部计算机视觉技术的新型驾驶行为分类系统，用于检测驾驶员的分心和受损驾驶行为，以提高道路安全。


<details>
  <summary>Details</summary>
Motivation: 道路交通事故是一个严重的全球性问题，其中分心驾驶和受损驾驶等人类错误是主要原因。

Method: 该研究引入了一个基于外部观察技术和高级计算机视觉方法（包括实时目标跟踪、横向位移分析和车道位置监测）的驾驶行为分类系统。该系统通过YOLO目标检测模型和自定义车道估算算法识别如过度横向移动和不稳定轨迹等不安全驾驶行为，能够分析非网联车辆。

Result: 在多样化视频数据集上的实验评估表明，该框架在不同道路和环境条件下均展现出良好的可靠性和适应性。

Conclusion: 该基于视觉的系统能够可靠且适应性强地检测驾驶员的分心和受损驾驶指标，为提高道路安全提供了一种有效解决方案，尤其适用于非网联车辆。

Abstract: Road traffic accidents remain a significant global concern, with human error,
particularly distracted and impaired driving, among the leading causes. This
study introduces a novel driver behavior classification system that uses
external observation techniques to detect indicators of distraction and
impairment. The proposed framework employs advanced computer vision
methodologies, including real-time object tracking, lateral displacement
analysis, and lane position monitoring. The system identifies unsafe driving
behaviors such as excessive lateral movement and erratic trajectory patterns by
implementing the YOLO object detection model and custom lane estimation
algorithms. Unlike systems reliant on inter-vehicular communication, this
vision-based approach enables behavioral analysis of non-connected vehicles.
Experimental evaluations on diverse video datasets demonstrate the framework's
reliability and adaptability across varying road and environmental conditions.

</details>


### [89] [FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark](https://arxiv.org/abs/2509.09680)
*Rongyao Fang,Aldrich Yu,Chengqi Duan,Linjiang Huang,Shuai Bai,Yuxuan Cai,Kun Wang,Si Liu,Xihui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文引入了FLUX-Reason-6M数据集和PRISM-Bench基准测试，旨在解决开源文生图模型在推理能力和评估方面的不足，从而缩小与闭源系统的性能差距。


<details>
  <summary>Details</summary>
Motivation: 开源文生图模型因缺乏大规模、注重推理的数据集和全面的评估基准，导致其性能与领先的闭源系统存在显著差距。

Method: 研究者引入了FLUX-Reason-6M，一个包含600万高质量FLUX生成图像和2000万双语描述的大规模数据集，旨在教授复杂推理能力。图像根据六个关键特征（想象力、实体、文本渲染、风格、情感、构图）组织，并设计了显式生成思维链（GCoT）来详细分解图像生成步骤。数据整理耗费15,000 A100 GPU天。此外，还引入了PRISM-Bench，一个包含七个独立轨道的评估标准，其中包括一个使用GCoT的“长文本挑战”。该基准利用先进的视觉-语言模型，通过精心设计的提示对提示-图像对齐和图像美学进行细致且与人类对齐的评估。

Result: 通过对19个领先模型在PRISM-Bench上进行广泛评估，揭示了关键的性能差距，并突出了需要改进的具体领域。

Conclusion: 该研究发布了其数据集、基准和评估代码，旨在催化下一波面向推理的文生图生成技术的发展。

Abstract: The advancement of open-source text-to-image (T2I) models has been hindered
by the absence of large-scale, reasoning-focused datasets and comprehensive
evaluation benchmarks, resulting in a performance gap compared to leading
closed-source systems. To address this challenge, We introduce FLUX-Reason-6M
and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).
FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality
FLUX-generated images and 20 million bilingual (English and Chinese)
descriptions specifically designed to teach complex reasoning. The image are
organized according to six key characteristics: Imagination, Entity, Text
rendering, Style, Affection, and Composition, and design explicit Generation
Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation
steps. The whole data curation takes 15,000 A100 GPU days, providing the
community with a resource previously unattainable outside of large industrial
labs. PRISM-Bench offers a novel evaluation standard with seven distinct
tracks, including a formidable Long Text challenge using GCoT. Through
carefully designed prompts, it utilizes advanced vision-language models for
nuanced human-aligned assessment of prompt-image alignment and image
aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench
reveals critical performance gaps and highlights specific areas requiring
improvement. Our dataset, benchmark, and evaluation code are released to
catalyze the next wave of reasoning-oriented T2I generation. Project page:
https://flux-reason-6m.github.io/ .

</details>


### [90] [Texture-aware Intrinsic Image Decomposition with Model- and Learning-based Priors](https://arxiv.org/abs/2509.09352)
*Xiaodong Wang,Zijun He,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出一种新的优化框架，结合纹理引导正则化项，解决单幅图像在复杂光照和丰富纹理场景下的本征图像分解问题，实现了高质量的反射层和光照层分离。


<details>
  <summary>Details</summary>
Motivation: 尽管本征图像分解问题已被研究多年，但在处理具有复杂光照变化和丰富纹理的真实场景时，仍面临巨大挑战，现有方法难以产生高质量结果。

Method: 作者观察到现有基于学习的方法倾向于生成无纹理和过度平滑的本征图像。基于此，他们设计了一个纹理引导正则化项，并将分解问题构建为一个优化框架，以有效分离材质纹理和光照效应。

Result: 结果表明，结合新型纹理感知先验的方法，能够比现有方法产生更优质的本征图像分解结果，尤其适用于真实世界的复杂图像。

Conclusion: 本文提出的新方法通过引入纹理引导先验并结合优化框架，成功解决了复杂场景下本征图像分解的挑战，实现了高质量且优于现有方法的分解结果。

Abstract: This paper aims to recover the intrinsic reflectance layer and shading layer
given a single image. Though this intrinsic image decomposition problem has
been studied for decades, it remains a significant challenge in cases of
complex scenes, i.e. spatially-varying lighting effect and rich textures. In
this paper, we propose a novel method for handling severe lighting and rich
textures in intrinsic image decomposition, which enables to produce
high-quality intrinsic images for real-world images. Specifically, we observe
that previous learning-based methods tend to produce texture-less and
over-smoothing intrinsic images, which can be used to infer the lighting and
texture information given a RGB image. In this way, we design a texture-guided
regularization term and formulate the decomposition problem into an
optimization framework, to separate the material textures and lighting effect.
We demonstrate that combining the novel texture-aware prior can produce
superior results to existing approaches.

</details>


### [91] [Plug-and-play Diffusion Models for Image Compressive Sensing with Data Consistency Projection](https://arxiv.org/abs/2509.09365)
*Xiaodong Wang,Ping Wang,Zhangyuan Li,Xin Yuan*

Main category: cs.CV

TL;DR: 本文探索了PnP方法和DDIM在解决病态逆问题（特别是单像素成像）中的联系，通过解耦扩散过程、提供统一框架并提出混合数据一致性模块，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: PnP和扩散模型在去噪机制和采样过程上存在关键区别，但两者都被用于解决逆问题。研究动机在于识别这些区别，提供一个统一的框架，将学习先验与物理前向模型结合，以期改进逆问题的求解，特别关注单像素成像。

Method: 研究方法包括将扩散过程解耦为去噪、数据一致性强制和采样三个阶段，从而提供了一个统一框架。在此基础上，提出了一种混合数据一致性模块，该模块线性结合了多个PnP风格的保真项，并直接应用于去噪估计，以提高测量一致性而不干扰扩散采样轨迹。

Result: 在单像素成像任务上的实验结果表明，该方法实现了更好的重建质量。

Conclusion: 通过统一PnP和DDIM的框架，并引入混合数据一致性模块，可以有效地提升病态逆问题（如单像素成像）的重建性能。

Abstract: We explore the connection between Plug-and-Play (PnP) methods and Denoising
Diffusion Implicit Models (DDIM) for solving ill-posed inverse problems, with a
focus on single-pixel imaging. We begin by identifying key distinctions between
PnP and diffusion models-particularly in their denoising mechanisms and
sampling procedures. By decoupling the diffusion process into three
interpretable stages: denoising, data consistency enforcement, and sampling, we
provide a unified framework that integrates learned priors with physical
forward models in a principled manner. Building upon this insight, we propose a
hybrid data-consistency module that linearly combines multiple PnP-style
fidelity terms. This hybrid correction is applied directly to the denoised
estimate, improving measurement consistency without disrupting the diffusion
sampling trajectory. Experimental results on single-pixel imaging tasks
demonstrate that our method achieves better reconstruction quality.

</details>


### [92] [A Fully Automatic Framework for Intracranial Pressure Grading: Integrating Keyframe Identification, ONSD Measurement and Clinical Data](https://arxiv.org/abs/2509.09368)
*Pengxu Wen,Tingting Yu,Ziwei Nie,Cheng Jiang,Zhenyu Yin,Mingyang He,Bo Liao,Xiaoping Yang*

Main category: cs.CV

TL;DR: 本文提出一个全自动两阶段框架，通过融合眼底超声视频的视神经鞘直径（ONSD）测量和临床数据，实现了颅内压（ICP）的非侵入性分级，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 颅内压升高对脑功能构成严重威胁，但金标准腰椎穿刺具有侵入性和相关风险。ONSD作为一种有前景的非侵入性生物标志物，其当前临床测量方法存在手动操作不一致、最佳视野选择主观性强以及阈值可变性大等问题，限制了其可靠性。

Method: 本研究引入了一个全自动两阶段ICP分级框架。第一阶段是眼底超声视频处理，包括帧级解剖分割、基于国际共识声明的规则化关键帧识别和精确的ONSD测量。第二阶段是颅内压分级，将ONSD指标与临床特征融合，以预测ICP等级。

Result: 实验结果表明，该方法在验证集上达到0.845 ± 0.071的准确率（五折交叉验证），在独立测试集上达到0.786的准确率。这显著优于传统的基于阈值的方法（验证准确率为0.637 ± 0.111，测试准确率为0.429）。

Conclusion: 本框架通过有效减少操作员变异性并整合多源信息，建立了一种可靠的非侵入性临床ICP评估方法，有望改善急性神经系统疾病患者的管理。

Abstract: Intracranial pressure (ICP) elevation poses severe threats to cerebral
function, thus necessitating monitoring for timely intervention. While lumbar
puncture is the gold standard for ICP measurement, its invasiveness and
associated risks drive the need for non-invasive alternatives. Optic nerve
sheath diameter (ONSD) has emerged as a promising biomarker, as elevated ICP
directly correlates with increased ONSD. However, current clinical practices
for ONSD measurement suffer from inconsistency in manual operation,
subjectivity in optimal view selection, and variability in thresholding,
limiting their reliability. To address these challenges, we introduce a fully
automatic two-stage framework for ICP grading, integrating keyframe
identification, ONSD measurement and clinical data. Specifically, the fundus
ultrasound video processing stage performs frame-level anatomical segmentation,
rule-based keyframe identification guided by an international consensus
statement, and precise ONSD measurement. The intracranial pressure grading
stage then fuses ONSD metrics with clinical features to enable the prediction
of ICP grades, thereby demonstrating an innovative blend of interpretable
ultrasound analysis and multi-source data integration for objective clinical
evaluation. Experimental results demonstrate that our method achieves a
validation accuracy of $0.845 \pm 0.071$ (with standard deviation from
five-fold cross-validation) and an independent test accuracy of 0.786,
significantly outperforming conventional threshold-based method ($0.637 \pm
0.111$ validation accuracy, $0.429$ test accuracy). Through effectively
reducing operator variability and integrating multi-source information, our
framework establishes a reliable non-invasive approach for clinical ICP
evaluation, holding promise for improving patient management in acute
neurological conditions.

</details>


### [93] [Unsupervised Integrated-Circuit Defect Segmentation via Image-Intrinsic Normality](https://arxiv.org/abs/2509.09375)
*Botong Zhao,Qijun Shi,Shujing Lyu,Yue Lu*

Main category: cs.CV

TL;DR: 本文提出一种无监督IC缺陷分割框架，无需外部正常样本。它通过从测试图像中提取正常特征并重建正常内容来识别缺陷，实验证明其优于现有方法并对产品可变性具有强大鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路（IC）制造引入的缺陷严重影响良率和可靠性。现有的工业缺陷分割方法依赖外部正常样本进行比较，但对于IC图像而言，由于布局多样和难以精确对齐，这种策略具有脆弱性。

Method: 本文提出一种无监督IC缺陷分割框架。该框架通过一个可学习的正常信息提取器从测试图像中聚合代表性的正常特征，并利用一致性损失强化其与正常区域的关联。一个解码器在这些特征的引导下仅重建正常内容，而重建残差则用于分割缺陷。此外，引入伪异常增强以稳定训练过程。

Result: 在来自三个IC工艺阶段的数据集上进行的实验表明，所提出的方法在性能上持续优于现有方法，并对产品可变性表现出强大的鲁棒性。

Conclusion: 该无监督IC缺陷分割框架有效地解决了传统方法的局限性，通过直接从图像中学习和重建正常内容，实现了无需外部正常样本的缺陷识别，并在多种IC工艺阶段展现了卓越的性能和鲁棒性。

Abstract: Modern Integrated-Circuit(IC) manufacturing introduces diverse, fine-grained
defects that depress yield and reliability. Most industrial defect segmentation
compares a test image against an external normal set, a strategy that is
brittle for IC imagery where layouts vary across products and accurate
alignment is difficult. We observe that defects are predominantly local, while
each image still contains rich, repeatable normal patterns. We therefore
propose an unsupervised IC defect segmentation framework that requires no
external normal support. A learnable normal-information extractor aggregates
representative normal features from the test image, and a coherence loss
enforces their association with normal regions. Guided by these features, a
decoder reconstructs only normal content; the reconstruction residual then
segments defects. Pseudo-anomaly augmentation further stabilizes training.
Experiments on datasets from three IC process stages show consistent
improvements over existing approaches and strong robustness to product
variability.

</details>


### [94] [Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot Adaptation under Shift](https://arxiv.org/abs/2509.09397)
*Umaima Rahman,Raza Imam,Mohammad Yaqub,Dwarikanath Mahapatra*

Main category: cs.CV

TL;DR: 本研究提出了DRiFt框架，通过解耦临床相关信号和噪声，并利用高质量图像-文本对，显著提升了医学视觉-语言模型在分布偏移下的可靠性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学视觉-语言模型（VLMs）在临床决策支持方面具有潜力，但在分布偏移下其可靠性是安全部署的主要担忧。这些模型由于成像协议和自由文本报告的变异性，通常会学习到与任务无关的关联，从而限制了泛化能力并增加了在实际应用中失败的风险。

Method: 提出了DRiFt，一个结构化特征解耦框架，该框架利用参数高效微调（LoRA）和可学习的提示令牌，将临床相关信号与任务无关的噪声明确分离。为了增强跨模态对齐并减少不确定性，研究者通过为多样化的医学数据集生成高质量、临床相关的图像-文本对来精选数据。

Result: DRiFt在分布内性能上比之前的基于提示的方法提高了+11.4%的Top-1准确率和+3.3%的Macro-F1，同时在未见数据集上保持了强大的鲁棒性。消融研究表明，解耦与任务相关的特征和仔细的对齐显著增强了模型的泛化能力，并减少了域偏移下不可预测的行为。

Conclusion: 这些见解有助于构建更安全、更值得信赖的临床用视觉-语言模型。解耦任务相关特征和仔细的对齐对于模型在域偏移下的泛化能力和行为稳定性至关重要。

Abstract: Medical vision-language models (VLMs) offer promise for clinical decision
support, yet their reliability under distribution shifts remains a major
concern for safe deployment. These models often learn task-agnostic
correlations due to variability in imaging protocols and free-text reports,
limiting their generalizability and increasing the risk of failure in
real-world settings. We propose DRiFt, a structured feature decoupling
framework that explicitly separates clinically relevant signals from
task-agnostic noise using parameter-efficient tuning (LoRA) and learnable
prompt tokens. To enhance cross-modal alignment and reduce uncertainty, we
curate high-quality, clinically grounded image-text pairs by generating
captions for a diverse medical dataset. Our approach improves in-distribution
performance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based
methods, while maintaining strong robustness across unseen datasets. Ablation
studies reveal that disentangling task-relevant features and careful alignment
significantly enhance model generalization and reduce unpredictable behavior
under domain shift. These insights contribute toward building safer, more
trustworthy VLMs for clinical use. The code is available at
https://github.com/rumaima/DRiFt.

</details>


### [95] [FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution](https://arxiv.org/abs/2509.09427)
*Yuchan Jie,Yushen Xu,Xiaosong Li,Fuqiang Zhou,Jianming Lv,Huafeng Li*

Main category: cs.CV

TL;DR: FS-Diff是一种语义引导和清晰度感知的联合图像融合与超分辨率方法，将二者统一为条件生成问题，利用Mamba和U-Net在低分辨率、弱语义图像上实现了优于SOTA的高清融合效果，并构建了新的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合与超分辨率技术在处理真实世界应用（如军事侦察和远距离探测）中低分辨率、受损且语义信息薄弱的多模态图像时，效果不佳。

Method: 提出FS-Diff方法，将图像融合和超分辨率建模为条件生成问题。该方法利用语义引导的清晰度感知机制进行自适应低分辨率感知和跨模态特征提取。具体包括：将融合结果初始化为高斯噪声；引入双向特征Mamba提取多模态图像的全局特征；使用修改后的U-Net网络，以源图像和语义作为条件，进行随机迭代去噪过程，以在多个噪声水平下生成高分辨率融合结果。此外，还构建了一个包含600对图像的空中视角多场景（AVMS）基准数据集。

Result: 在六个公共数据集和AVMS数据集上进行的大量实验表明，FS-Diff在多种放大倍数下均优于现有最先进方法，并且能够在融合图像中恢复更丰富的细节和语义信息。

Conclusion: FS-Diff通过创新的条件生成范式和语义引导机制，有效解决了低分辨率和信息受损多模态图像的联合融合与超分辨率难题，实现了卓越的性能，并能恢复更丰富的细节和语义。

Abstract: As an influential information fusion and low-level vision technique, image
fusion integrates complementary information from source images to yield an
informative fused image. A few attempts have been made in recent years to
jointly realize image fusion and super-resolution. However, in real-world
applications such as military reconnaissance and long-range detection missions,
the target and background structures in multimodal images are easily corrupted,
with low resolution and weak semantic information, which leads to suboptimal
results in current fusion techniques. In response, we propose FS-Diff, a
semantic guidance and clarity-aware joint image fusion and super-resolution
method. FS-Diff unifies image fusion and super-resolution as a conditional
generation problem. It leverages semantic guidance from the proposed clarity
sensing mechanism for adaptive low-resolution perception and cross-modal
feature extraction. Specifically, we initialize the desired fused result as
pure Gaussian noise and introduce the bidirectional feature Mamba to extract
the global features of the multimodal images. Moreover, utilizing the source
images and semantics as conditions, we implement a random iterative denoising
process via a modified U-Net network. This network istrained for denoising at
multiple noise levels to produce high-resolution fusion results with
cross-modal features and abundant semantic information. We also construct a
powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images.
Extensive joint image fusion and super-resolution experiments on six public and
our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art
methods at multiple magnifications and can recover richer details and semantics
in the fused images. The code is available at
https://github.com/XylonXu01/FS-Diff.

</details>


### [96] [Semantic Concentration for Self-Supervised Dense Representations Learning](https://arxiv.org/abs/2509.09429)
*Peisong Wen,Qianqian Xu,Siran Dai,Runmin Cong,Qingming Huang*

Main category: cs.CV

TL;DR: 本文针对自监督学习中密集表征学习（dense SSL）的过分散现象，提出了一种名为CoTAP的方法，通过显式语义集中来解决。具体而言，该方法引入了噪声容忍的排序损失进行补丁对应蒸馏以打破严格空间对齐，并设计了物体感知滤波器以区分复杂场景中的共享模式，从而有效提升了下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 图像级自监督学习（SSL）取得了显著进展，但补丁的密集表征学习仍具挑战。现有方法存在“过分散”现象，即来自同一实例/类别的补丁分散，损害了密集任务的性能。图像级SSL通过隐式语义集中（非严格空间对齐和共享模式）避免了过分散，但这些方法因空间敏感性和复杂场景数据不适用于密集SSL。因此，作者受此启发，旨在为密集SSL探索显式语义集中。

Method: 该研究提出了两种主要方法：
1.  **补丁对应蒸馏与噪声容忍排序损失：** 为打破严格的空间对齐，作者通过蒸馏补丁对应来学习，并针对噪声和不平衡的伪标签，提出了一种噪声容忍的排序损失（将平均精度AP损失扩展到连续目标），以其决策无关和自适应聚焦特性防止模型被误导。
2.  **物体感知滤波器：** 为区分复杂场景中的共享模式，作者提出了一个物体感知滤波器，将输出空间映射到基于物体的空间。具体通过交叉注意力机制，利用可学习的物体原型来表示补丁。

Result: 跨各种任务的实证研究有力地支持了所提出方法的有效性。

Conclusion: 该研究通过引入显式语义集中（包括补丁对应蒸馏与噪声容忍排序损失和物体感知滤波器），有效解决了密集自监督学习中补丁表征的过分散问题，显著提升了模型在密集下游任务上的性能。

Abstract: Recent advances in image-level self-supervised learning (SSL) have made
significant progress, yet learning dense representations for patches remains
challenging. Mainstream methods encounter an over-dispersion phenomenon that
patches from the same instance/category scatter, harming downstream performance
on dense tasks. This work reveals that image-level SSL avoids over-dispersion
by involving implicit semantic concentration. Specifically, the non-strict
spatial alignment ensures intra-instance consistency, while shared patterns,
i.e., similar parts of within-class instances in the input space, ensure
inter-image consistency. Unfortunately, these approaches are infeasible for
dense SSL due to their spatial sensitivity and complicated scene-centric data.
These observations motivate us to explore explicit semantic concentration for
dense SSL. First, to break the strict spatial alignment, we propose to distill
the patch correspondences. Facing noisy and imbalanced pseudo labels, we
propose a noise-tolerant ranking loss. The core idea is extending the Average
Precision (AP) loss to continuous targets, such that its decision-agnostic and
adaptive focusing properties prevent the student model from being misled.
Second, to discriminate the shared patterns from complicated scenes, we propose
the object-aware filter to map the output space to an object-based space.
Specifically, patches are represented by learnable prototypes of objects via
cross-attention. Last but not least, empirical studies across various tasks
soundly support the effectiveness of our method. Code is available in
https://github.com/KID-7391/CoTAP.

</details>


### [97] [FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion based on diffusion model](https://arxiv.org/abs/2509.09456)
*Yushen Xu,Xiaosong Li,Yuchun Wang,Xiaoqi Cheng,Huafeng Li,Haishu Tan*

Main category: cs.CV

TL;DR: 本文提出FlexiD-Fuse，一个基于扩散模型的图像融合网络，旨在解决现有方法无法处理可变数量输入模态的局限性，实现了在相同权重下对不同数量医学图像模态的端到端融合，并在多项任务中表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 多模态医学图像融合对临床诊断至关重要，但现有融合方法通常只能处理固定数量的输入模态（如双模态或三模态），无法直接适应可变数量的输入，这限制了其在临床实践中的应用。

Method: 本文引入FlexiD-Fuse，一个基于扩散的图像融合网络，用以适应灵活数量的输入模态。它将固定条件输入的扩散融合问题转化为基于扩散过程和分层贝叶斯建模的最大似然估计问题，并通过将期望最大化（EM）算法融入扩散采样迭代过程，使其能独立于输入图像数量生成高质量的跨模态融合图像。FlexiD-Fuse能够端到端地在相同权重下处理双模态和三模态医学图像融合。

Result: 通过在Harvard数据集上与最新的双模态和三模态医学图像融合方法进行比较，并使用九种流行指标进行评估，FlexiD-Fuse在处理可变输入数量的医学图像融合方面取得了最佳性能。此外，在红外-可见光、多曝光和多焦点图像融合等任意数量输入的扩展实验中，与SOTA方法相比，也一致证明了该方法的有效性和优越性。

Conclusion: FlexiD-Fuse成功克服了现有医学图像融合方法输入数量固定的限制，能够灵活处理不同数量的输入模态，并在医学图像融合及其他图像融合任务中展现出卓越的性能和广泛的适用性。

Abstract: Different modalities of medical images provide unique physiological and
anatomical information for diseases. Multi-modal medical image fusion
integrates useful information from different complementary medical images with
different modalities, producing a fused image that comprehensively and
objectively reflects lesion characteristics to assist doctors in clinical
diagnosis. However, existing fusion methods can only handle a fixed number of
modality inputs, such as accepting only two-modal or tri-modal inputs, and
cannot directly process varying input quantities, which hinders their
application in clinical settings. To tackle this issue, we introduce
FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate
flexible quantities of input modalities. It can end-to-end process two-modal
and tri-modal medical image fusion under the same weight. FlexiD-Fuse
transforms the diffusion fusion problem, which supports only fixed-condition
inputs, into a maximum likelihood estimation problem based on the diffusion
process and hierarchical Bayesian modeling. By incorporating the
Expectation-Maximization algorithm into the diffusion sampling iteration
process, FlexiD-Fuse can generate high-quality fused images with cross-modal
information from source images, independently of the number of input images. We
compared the latest two and tri-modal medical image fusion methods, tested them
on Harvard datasets, and evaluated them using nine popular metrics. The
experimental results show that our method achieves the best performance in
medical image fusion with varying inputs. Meanwhile, we conducted extensive
extension experiments on infrared-visible, multi-exposure, and multi-focus
image fusion tasks with arbitrary numbers, and compared them with the
perspective SOTA methods. The results of the extension experiments consistently
demonstrate the effectiveness and superiority of our method.

</details>


### [98] [Resource-Efficient Glioma Segmentation on Sub-Saharan MRI](https://arxiv.org/abs/2509.09469)
*Freedmore Sidume,Oumayma Soula,Joseph Muthui Wacira,YunFei Zhu,Abbas Rabiu Muhammad,Abderrazek Zeraii,Oluwaseun Kalejaye,Hajer Ibrahim,Olfa Gaddour,Brain Halubanza,Dong Zhang,Udunna C Anazodo,Confidence Raymond*

Main category: cs.CV

TL;DR: 本研究为撒哈拉以南非洲（SSA）等资源受限地区，提出了一种高效、紧凑且泛化能力强的3D Attention UNet深度学习框架，用于胶质瘤MRI分割，在有限数据下取得了良好性能并具有临床部署潜力。


<details>
  <summary>Details</summary>
Motivation: 胶质瘤的准确MRI分割对诊断和治疗至关重要。然而，撒哈拉以南非洲（SSA）高质量标注影像数据稀缺，阻碍了先进分割模型在该地区的临床应用，急需为资源受限环境开发鲁棒且计算高效的模型。

Method: 采用3D Attention UNet架构，并集成了残差块以增强性能。模型通过在BraTS 2021数据集上预训练的权重进行迁移学习，以克服数据稀缺问题。随后，模型在BraTS-Africa数据集（包含95例SSA地区MRI病例）上进行评估。

Result: 尽管数据质量和数量有限，该方法在增强肿瘤(ET)上获得了0.76的Dice分数，在坏死和非增强肿瘤核心(NETC)上获得0.80，在周围非功能半球(SNFH)上获得0.85。模型架构紧凑（约90 MB），在消费级硬件上每个体积的推理时间少于一分钟，显示出高效率。

Conclusion: 所提出的模型具有良好的泛化能力，其高效、紧凑的特点使其非常适合部署在低资源环境中，支持临床决策。这项工作通过提供高性能、可及的医疗影像解决方案，有助于缩小全球健康领域公平人工智能的差距。

Abstract: Gliomas are the most prevalent type of primary brain tumors, and their
accurate segmentation from MRI is critical for diagnosis, treatment planning,
and longitudinal monitoring. However, the scarcity of high-quality annotated
imaging data in Sub-Saharan Africa (SSA) poses a significant challenge for
deploying advanced segmentation models in clinical workflows. This study
introduces a robust and computationally efficient deep learning framework
tailored for resource-constrained settings. We leveraged a 3D Attention UNet
architecture augmented with residual blocks and enhanced through transfer
learning from pre-trained weights on the BraTS 2021 dataset. Our model was
evaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma
segmentation in SSA MRI data. Despite the limited data quality and quantity,
our approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80
for Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding
Non-Functional Hemisphere (SNFH). These results demonstrate the
generalizability of the proposed model and its potential to support clinical
decision making in low-resource settings. The compact architecture,
approximately 90 MB, and sub-minute per-volume inference time on consumer-grade
hardware further underscore its practicality for deployment in SSA health
systems. This work contributes toward closing the gap in equitable AI for
global health by empowering underserved regions with high-performing and
accessible medical imaging solutions.

</details>


### [99] [OpenFake: An Open Dataset and Platform Toward Large-Scale Deepfake Detection](https://arxiv.org/abs/2509.09495)
*Victor Livernoche,Akshatha Arodi,Andreea Musulan,Zachary Yang,Adam Salvail,Gaétan Marceau Caron,Jean-François Godbout,Reihaneh Rabbany*

Main category: cs.CV

TL;DR: 为应对日益逼真的深度伪造图像（Deepfakes）带来的虚假信息传播，特别是政治敏感内容，本研究构建了一个大规模、政治导向的深度伪造检测数据集，并建立了一个众包对抗平台，以确保持久有效的检测能力。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术加剧了虚假信息传播，尤其在政治敏感背景下。现有检测数据集存在局限（生成方法过时、真实度低、仅限单人脸），且现代生成模型产出的图像越来越难以被公众识别，急需更有效、更具时效性的检测基准。

Method: 1. 分析社交媒体帖子，识别深度伪造虚假信息传播的多模态方式。2. 进行人类感知研究，评估现代合成图像的真实度。3. 构建了一个全面的、政治导向的深度伪造检测数据集，包含300万真实图像及其描述性 कैप्शन，并据此利用专有和开源模型生成了96.3万张高质量合成图像。4. 引入了一个创新的众包对抗平台，激励参与者提交具有挑战性的合成图像，以持续更新数据集。

Result: 1. 识别了深度伪造虚假信息传播的多种模态。2. 人类感知研究表明，现代专有模型生成的合成图像与真实图像的区分度越来越小。3. 提出了一个用于基准测试现代生成模型的全面、政治导向的检测数据集。4. 创建了一个社区驱动的众包对抗平台，确保检测方法能够适应生成技术的发展。

Conclusion: 本研究通过提供先进的数据集和持续更新的对抗平台，旨在使深度伪造检测方法保持稳健和适应性，从而主动保护公共话语免受复杂的虚假信息威胁。

Abstract: Deepfakes, synthetic media created using advanced AI techniques, have
intensified the spread of misinformation, particularly in politically sensitive
contexts. Existing deepfake detection datasets are often limited, relying on
outdated generation methods, low realism, or single-face imagery, restricting
the effectiveness for general synthetic image detection. By analyzing social
media posts, we identify multiple modalities through which deepfakes propagate
misinformation. Furthermore, our human perception study demonstrates that
recently developed proprietary models produce synthetic images increasingly
indistinguishable from real ones, complicating accurate identification by the
general public. Consequently, we present a comprehensive, politically-focused
dataset specifically crafted for benchmarking detection against modern
generative models. This dataset contains three million real images paired with
descriptive captions, which are used for generating 963k corresponding
high-quality synthetic images from a mix of proprietary and open-source models.
Recognizing the continual evolution of generative techniques, we introduce an
innovative crowdsourced adversarial platform, where participants are
incentivized to generate and submit challenging synthetic images. This ongoing
community-driven initiative ensures that deepfake detection methods remain
robust and adaptive, proactively safeguarding public discourse from
sophisticated misinformation threats.

</details>


### [100] [Improving Human Motion Plausibility with Body Momentum](https://arxiv.org/abs/2509.09496)
*Ha Linh Nguyen,Tze Ho Elden Tse,Angela Yao*

Main category: cs.CV

TL;DR: 本文提出使用全身线动量和角动量作为约束来关联人体局部运动与全局运动，并通过引入新的损失项，提高了运动恢复的真实性、稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 许多研究将人体局部运动与全局运动分开处理，但两者并非独立且相互耦合。现有运动模型难以精确捕捉这种物理耦合，而从关节扭矩和外力推导全局轨迹则计算昂贵且复杂。

Method: 提出利用全身线动量和角动量作为约束，以物理方式连接局部运动与全局运动。在此基础上，引入一个新的损失项，用于强制生成动量剖面与真实数据保持一致。

Result: 整合该损失项后，减少了足部滑动和抖动，提高了平衡性，并保持了恢复运动的准确性。

Conclusion: 通过将全身动量作为物理约束，本文方法有效地将局部运动与全局运动关联起来，显著改善了人体运动恢复的真实性、稳定性和准确性。

Abstract: Many studies decompose human motion into local motion in a frame attached to
the root joint and global motion of the root joint in the world frame, treating
them separately. However, these two components are not independent. Global
movement arises from interactions with the environment, which are, in turn,
driven by changes in the body configuration. Motion models often fail to
precisely capture this physical coupling between local and global dynamics,
while deriving global trajectories from joint torques and external forces is
computationally expensive and complex. To address these challenges, we propose
using whole-body linear and angular momentum as a constraint to link local
motion with global movement. Since momentum reflects the aggregate effect of
joint-level dynamics on the body's movement through space, it provides a
physically grounded way to relate local joint behavior to global displacement.
Building on this insight, we introduce a new loss term that enforces
consistency between the generated momentum profiles and those observed in
ground-truth data. Incorporating our loss reduces foot sliding and jitter,
improves balance, and preserves the accuracy of the recovered motion. Code and
data are available at the project page https://hlinhn.github.io/momentum_bmvc.

</details>


### [101] [Region-Wise Correspondence Prediction between Manga Line Art Images](https://arxiv.org/abs/2509.09501)
*Yingxuan Li,Jiafeng Mao,Qianru Qiu,Yusuke Matsui*

Main category: cs.CV

TL;DR: 本文提出一种基于Transformer的框架，用于在无预设标签或掩码的情况下，预测原始漫画线稿图像之间的区域级对应关系，并通过边缘感知聚类和区域匹配算法实现高精度和一致性，为漫画处理应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 理解漫画线稿图像的区域级对应关系是漫画处理（如自动上色、中间帧生成）的基础任务。然而，在没有预设分割或标注的现实场景中，这项任务仍未被充分探索。因此，本文引入并解决了一项新颖且实用的任务：在没有任何预设标签或掩码的情况下，预测原始漫画线稿图像之间的区域级对应关系。

Method: 将每张线稿图像划分为一系列补丁，并提出一个基于Transformer的框架，学习图像内部和图像之间的补丁级相似性。接着，应用边缘感知聚类和区域匹配算法，将补丁级预测转换为连贯的区域级对应关系。为支持训练和评估，开发了自动标注流水线并手动精炼部分数据以构建基准数据集。

Result: 在多个数据集上的实验表明，该方法实现了高补丁级准确率（例如96.34%），并生成了一致的区域级对应关系。

Conclusion: 该方法实现了高补丁级准确率和一致的区域级对应关系，突显了其在真实世界漫画应用中的潜力。

Abstract: Understanding region-wise correspondence between manga line art images is a
fundamental task in manga processing, enabling downstream applications such as
automatic line art colorization and in-between frame generation. However, this
task remains largely unexplored, especially in realistic scenarios without
pre-existing segmentation or annotations. In this paper, we introduce a novel
and practical task: predicting region-wise correspondence between raw manga
line art images without any pre-existing labels or masks. To tackle this
problem, we divide each line art image into a set of patches and propose a
Transformer-based framework that learns patch-level similarities within and
across images. We then apply edge-aware clustering and a region matching
algorithm to convert patch-level predictions into coherent region-level
correspondences. To support training and evaluation, we develop an automatic
annotation pipeline and manually refine a subset of the data to construct
benchmark datasets. Experiments on multiple datasets demonstrate that our
method achieves high patch-level accuracy (e.g., 96.34%) and generates
consistent region-level correspondences, highlighting its potential for
real-world manga applications.

</details>


### [102] [Generative Diffusion Contrastive Network for Multi-View Clustering](https://arxiv.org/abs/2509.09527)
*Jian Zhu,Xin Zou,Xi Wang,Ning Zhang,Bian Wu,Yao Yang,Ying Zhou,Lingfang Zeng,Chang Tang,Cheng Luo*

Main category: cs.CV

TL;DR: 针对深度多视图聚类中由噪声和缺失数据导致的低质量数据融合问题，本文提出了随机生成扩散融合（SGDF）方法，并在此基础上构建了生成扩散对比网络（GDCN），在多视图聚类任务中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 多视图聚类（MVC）中，多视图融合对聚类性能至关重要。然而，融合过程中存在低质量数据问题，主要表现为：1) 某些视图被噪声污染；2) 某些视图数据缺失。

Method: 本文提出了一种新颖的随机生成扩散融合（SGDF）方法来解决低质量数据问题，该方法为每个样本的多视图特征利用了多重生成机制，对低质量数据具有鲁棒性。在此基础上，进一步提出了生成扩散对比网络（GDCN）。

Result: 广泛的实验表明，GDCN在深度多视图聚类任务中取得了最先进（state-of-the-art）的结果。

Conclusion: 通过SGDF和GDCN方法，本文有效地解决了深度多视图聚类中低质量数据的问题，并显著提升了聚类性能，达到了领域领先水平。

Abstract: In recent years, Multi-View Clustering (MVC) has been significantly advanced
under the influence of deep learning. By integrating heterogeneous data from
multiple views, MVC enhances clustering analysis, making multi-view fusion
critical to clustering performance. However, there is a problem of low-quality
data in multi-view fusion. This problem primarily arises from two reasons: 1)
Certain views are contaminated by noisy data. 2) Some views suffer from missing
data. This paper proposes a novel Stochastic Generative Diffusion Fusion (SGDF)
method to address this problem. SGDF leverages a multiple generative mechanism
for the multi-view feature of each sample. It is robust to low-quality data.
Building on SGDF, we further present the Generative Diffusion Contrastive
Network (GDCN). Extensive experiments show that GDCN achieves the
state-of-the-art results in deep MVC tasks. The source code is publicly
available at https://github.com/HackerHyper/GDCN.

</details>


### [103] [DualTrack: Sensorless 3D Ultrasound needs Local and Global Context](https://arxiv.org/abs/2509.09530)
*Paul F. R. Wilson,Matteo Ronchetti,Rüdiger Göbl,Viktoria Markova,Sebastian Rosenzweig,Raphael Prevost,Parvin Mousavi,Oliver Zettinig*

Main category: cs.CV

TL;DR: DualTrack是一种新型双编码器架构，通过解耦的局部和全局特征编码器，实现了无传感器三维超声探头轨迹估计的最新精度，克服了传统方法对互补特征建模的限制，重建误差低于5毫米。


<details>
  <summary>Details</summary>
Motivation: 传统三维超声系统成本高昂且复杂，限制了其广泛应用。无传感器三维超声作为一种有前景的替代方案，通过深度学习从二维图像序列估计探头轨迹。然而，现有方法未能有效或稳健地结合局部（如散斑模式）和全局（如粗略形状和解剖结构）特征，导致无法充分利用这两种互补信息来准确预测轨迹。

Method: 本文提出DualTrack，一种新颖的双编码器架构。它利用解耦的局部和全局编码器，分别专注于不同尺度的特征提取。局部编码器采用密集时空卷积来捕获精细特征；全局编码器则利用图像主干（如2D CNN或基础模型）和时间注意力层来嵌入高级解剖特征及长程依赖。一个轻量级融合模块最终结合这些特征以估计探头轨迹。

Result: 在大型公共基准测试中，DualTrack取得了最先进的（SOTA）准确性，并实现了全局一致的三维重建。它显著优于现有方法，平均重建误差低于5毫米。

Conclusion: DualTrack通过有效解耦并整合局部和全局特征，显著提升了无传感器三维超声的探头轨迹估计精度和三维重建的一致性，证明了其作为一种鲁棒解决方案的潜力。

Abstract: Three-dimensional ultrasound (US) offers many clinical advantages over
conventional 2D imaging, yet its widespread adoption is limited by the cost and
complexity of traditional 3D systems. Sensorless 3D US, which uses deep
learning to estimate a 3D probe trajectory from a sequence of 2D US images, is
a promising alternative. Local features, such as speckle patterns, can help
predict frame-to-frame motion, while global features, such as coarse shapes and
anatomical structures, can situate the scan relative to anatomy and help
predict its general shape. In prior approaches, global features are either
ignored or tightly coupled with local feature extraction, restricting the
ability to robustly model these two complementary aspects. We propose
DualTrack, a novel dual-encoder architecture that leverages decoupled local and
global encoders specialized for their respective scales of feature extraction.
The local encoder uses dense spatiotemporal convolutions to capture
fine-grained features, while the global encoder utilizes an image backbone
(e.g., a 2D CNN or foundation model) and temporal attention layers to embed
high-level anatomical features and long-range dependencies. A lightweight
fusion module then combines these features to estimate the trajectory.
Experimental results on a large public benchmark show that DualTrack achieves
state-of-the-art accuracy and globally consistent 3D reconstructions,
outperforming previous methods and yielding an average reconstruction error
below 5 mm.

</details>


### [104] [Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders](https://arxiv.org/abs/2509.09547)
*Dohun Lee,Hyeonho Jeong,Jiwook Kim,Duygu Ceylan,Jong Chul Ye*

Main category: cs.CV

TL;DR: 本研究提出Align4Gen方法，通过将视频扩散模型中间特征与预训练视觉编码器特征对齐，显著提升了视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 近年来视频扩散模型在架构创新和训练目标方面发展迅速，但对提升模型特征表示能力的研究关注不足。

Method: 本研究提出了一种新颖的多特征融合与对齐方法Align4Gen。首先，通过引入新度量标准，对多种预训练视觉编码器进行了判别能力和时间一致性分析，以评估其用于视频特征对齐的适用性。随后，基于此分析结果，将Align4Gen集成到视频扩散模型训练中，实现视频生成器中间特征与预训练视觉编码器特征的对齐。

Result: Align4Gen在无条件和类别条件视频生成任务中均展现了优异性能，通过多项指标量化，证实其能够有效提升视频生成质量。

Conclusion: 通过对齐视频扩散模型中间特征与预训练视觉编码器特征，能够有效增强模型特征表示能力，从而显著改善视频生成效果。

Abstract: Video diffusion models have advanced rapidly in the recent years as a result
of series of architectural innovations (e.g., diffusion transformers) and use
of novel training objectives (e.g., flow matching). In contrast, less attention
has been paid to improving the feature representation power of such models. In
this work, we show that training video diffusion models can benefit from
aligning the intermediate features of the video generator with feature
representations of pre-trained vision encoders. We propose a new metric and
conduct an in-depth analysis of various vision encoders to evaluate their
discriminability and temporal consistency, thereby assessing their suitability
for video feature alignment. Based on the analysis, we present Align4Gen which
provides a novel multi-feature fusion and alignment method integrated into
video diffusion model training. We evaluate Align4Gen both for unconditional
and class-conditional video generation tasks and show that it results in
improved video generation as quantified by various metrics. Full video results
are available on our project page: https://align4gen.github.io/align4gen/

</details>


### [105] [InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation](https://arxiv.org/abs/2509.09555)
*Sirui Xu,Dongting Li,Yucheng Zhang,Xiyan Xu,Qi Long,Ziyin Wang,Yunzhi Lu,Shuchang Dong,Hezi Jiang,Akshat Gupta,Yu-Xiong Wang,Liang-Yan Gui*

Main category: cs.CV

TL;DR: 本文提出了InterAct，一个大规模3D人-物交互(HOI)基准，通过整合、优化和扩展现有HOI数据来解决数据集限制问题，并在此基础上实现了HOI生成建模的最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人-物交互(HOI)数据集存在规模、质量和标注不足等问题，包括接触穿透、浮空和不正确的手部动作等伪影，这使得建模和生成动态3D HOI极具挑战性。

Method: 1. 整合并标准化来自多种来源的21.81小时HOI数据，并辅以详细文本标注。2. 提出统一优化框架，通过减少伪影和校正手部动作来提升数据质量，并利用接触不变性原理将数据集扩展至30.70小时。3. 定义六个基准任务，并开发了统一的HOI生成建模视角。

Result: 1. 成功构建并发布了InterAct，一个包含30.70小时高质量3D人-物交互数据的基准。2. 在定义的HOI生成建模任务中取得了最先进的性能。3. 广泛实验验证了所构建数据集作为3D人-物交互生成基础资源的实用性。

Conclusion: InterAct数据集及其提出的方法有效解决了现有HOI数据集的局限性，为推进3D人-物交互生成研究提供了强大的基础资源，并已公开以支持持续研究。

Abstract: While large-scale human motion capture datasets have advanced human motion
generation, modeling and generating dynamic 3D human-object interactions (HOIs)
remain challenging due to dataset limitations. Existing datasets often lack
extensive, high-quality motion and annotation and exhibit artifacts such as
contact penetration, floating, and incorrect hand motions. To address these
issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset
and methodological advancements. First, we consolidate and standardize 21.81
hours of HOI data from diverse sources, enriching it with detailed textual
annotations. Second, we propose a unified optimization framework to enhance
data quality by reducing artifacts and correcting hand motions. Leveraging the
principle of contact invariance, we maintain human-object relationships while
introducing motion variations, expanding the dataset to 30.70 hours. Third, we
define six benchmarking tasks and develop a unified HOI generative modeling
perspective, achieving state-of-the-art performance. Extensive experiments
validate the utility of our dataset as a foundational resource for advancing 3D
human-object interaction generation. To support continued research in this
area, the dataset is publicly available at
https://github.com/wzyabcas/InterAct, and will be actively maintained.

</details>


### [106] [Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in MRI-based Alzheimer's Disease Classification](https://arxiv.org/abs/2509.09558)
*Akshit Achara,Esther Puyol Anton,Alexander Hammers,Andrew P. King*

Main category: cs.CV

TL;DR: 本研究揭示，用于阿尔茨海默病（AD）诊断的深度学习模型在脑部MRI图像中存在基于种族和性别的“捷径学习”及人口学偏见，影响诊断公平性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在辅助阿尔茨海默病（AD）诊断方面展现潜力，但可能面临“捷径学习”问题，即模型可能利用与疾病无关的虚假特征（如与种族和性别相关的受保护属性）进行预测，从而导致对特定人群（特别是代表性不足的群体）的诊断偏见。本研究旨在探讨DL算法在基于MRI的AD诊断中捷径学习和人口学偏见的可能性。

Method: 1. 探究DL算法是否能从3D脑部MRI扫描中识别种族或性别，以确定是否存在基于这些属性的分布差异。 2. 调查训练集中种族或性别的不平衡是否会导致模型性能下降，从而揭示捷径学习和偏见。 3. 对受保护属性和AD分类任务中不同脑区特征归因进行定量和定性分析。实验使用了多个数据集和深度学习模型（ResNet和SwinTransformer）。

Result: 研究通过一系列实验证明，在基于深度学习的AD分类中，确实存在基于种族和性别的捷径学习和偏见。

Conclusion: 本工作为开发更公平的脑部MRI深度学习诊断工具奠定了基础。

Abstract: Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep
learning (DL) algorithms have been proposed to aid in the diagnosis of diseases
such as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can
suffer from shortcut learning, in which spurious features, not directly related
to the output label, are used for prediction. When these features are related
to protected attributes, they can lead to performance bias against
underrepresented protected groups, such as those defined by race and sex. In
this work, we explore the potential for shortcut learning and demographic bias
in DL based AD diagnosis from MRI. We first investigate if DL algorithms can
identify race or sex from 3D brain MRI scans to establish the presence or
otherwise of race and sex based distributional shifts. Next, we investigate
whether training set imbalance by race or sex can cause a drop in model
performance, indicating shortcut learning and bias. Finally, we conduct a
quantitative and qualitative analysis of feature attributions in different
brain regions for both the protected attribute and AD classification tasks.
Through these experiments, and using multiple datasets and DL models (ResNet
and SwinTransformer), we demonstrate the existence of both race and sex based
shortcut learning and bias in DL based AD classification. Our work lays the
foundation for fairer DL diagnostic tools in brain MRI. The code is provided at
https://github.com/acharaakshit/ShortMR

</details>


### [107] [PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection](https://arxiv.org/abs/2509.09572)
*Sijun Dong,Yuxuan Hu,LiBo Wang,Geng Chen,Xiaoliang Meng*

Main category: cs.CV

TL;DR: PeftCD是一个基于视觉基础模型（VFM）和参数高效微调（PEFT）的遥感变化检测框架，它能高效适应任务，抑制伪变化，实现精确的边界描绘，并在多个公共数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多时相和多源遥感图像中普遍存在的伪变化、标记样本稀缺以及跨域泛化困难的问题。

Method: 提出PeftCD框架，其核心是采用基于VFM（如SAM2和DINOv3）的权重共享Siamese编码器，并无缝集成LoRA和Adapter等PEFT模块，仅通过训练少量额外参数实现高效任务适应。同时，搭配一个轻量级解码器。

Result: 在SYSU-CD（IoU 73.81%）、WHUCD（92.05%）、MSRSCD（64.07%）、MLCD（76.89%）、CDD（97.01%）、S2Looking（52.25%）和LEVIR-CD（85.62%）等多个公共数据集上，PeftCD实现了最先进的性能，并显著提升了边界描绘精度，有效抑制了伪变化。

Conclusion: PeftCD在准确性、效率和泛化能力之间取得了最佳平衡，为将大规模VFM应用于实际遥感变化检测提供了一个强大且可扩展的范例。

Abstract: To tackle the prevalence of pseudo changes, the scarcity of labeled samples,
and the difficulty of cross-domain generalization in multi-temporal and
multi-source remote sensing imagery, we propose PeftCD, a change detection
framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient
Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese
encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly
integrated. This design enables highly efficient task adaptation by training
only a minimal set of additional parameters. To fully unlock the potential of
VFMs, we investigate two leading backbones: the Segment Anything Model v2
(SAM2), renowned for its strong segmentation priors, and DINOv3, a
state-of-the-art self-supervised representation learner. The framework is
complemented by a deliberately lightweight decoder, ensuring the focus remains
on the powerful feature representations from the backbones. Extensive
experiments demonstrate that PeftCD achieves state-of-the-art performance
across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD
(92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and
LEVIR-CD (85.62%), with notably precise boundary delineation and strong
suppression of pseudo-changes. In summary, PeftCD presents an optimal balance
of accuracy, efficiency, and generalization. It offers a powerful and scalable
paradigm for adapting large-scale VFMs to real-world remote sensing change
detection applications. The code and pretrained models will be released at
https://github.com/dyzy41/PeftCD.

</details>


### [108] [Visual Grounding from Event Cameras](https://arxiv.org/abs/2509.09584)
*Lingdong Kong,Dongyue Lu,Ao Liang,Rong Li,Yuhao Dong,Tianshuai Hu,Lai Xing Ng,Wei Tsang Ooi,Benoit R. Cottereau*

Main category: cs.CV

TL;DR: 本文引入Talk2Event，首个基于事件数据的语言驱动目标定位大规模基准。它通过丰富的标注和属性化参照表达，旨在弥补事件相机与自然语言理解的模态鸿沟，实现动态场景下的情境感知。


<details>
  <summary>Details</summary>
Motivation: 事件相机在捕捉动态场景和应对恶劣条件方面具有优势，但其与自然语言理解的结合尚未得到充分关注，导致多模态感知领域存在空白。

Method: 提出了Talk2Event基准，一个用于语言驱动对象定位的事件数据大型数据集。该基准基于真实驾驶场景，包含5,567个场景、13,458个标注对象和超过30,000个参照表达。每个表达都通过外观、状态、与观察者关系及与周围对象关系四种结构化属性进行丰富，以捕获空间、时间及关系线索，并采用属性中心设计。

Result: 成功构建了Talk2Event这一大规模语言驱动目标定位基准。该基准提供了丰富且以属性为中心的数据，支持可解释和组合式定位，能够实现动态环境中超越简单对象识别的情境推理。

Conclusion: Talk2Event为推动多模态和时间感知提供了基础，在机器人、人机交互等领域具有广泛的应用潜力，有助于深入理解动态环境。

Abstract: Event cameras capture changes in brightness with microsecond precision and
remain reliable under motion blur and challenging illumination, offering clear
advantages for modeling highly dynamic scenes. Yet, their integration with
natural language understanding has received little attention, leaving a gap in
multimodal perception. To address this, we introduce Talk2Event, the first
large-scale benchmark for language-driven object grounding using event data.
Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes,
13,458 annotated objects, and more than 30,000 carefully validated referring
expressions. Each expression is enriched with four structured attributes --
appearance, status, relation to the viewer, and relation to surrounding objects
-- that explicitly capture spatial, temporal, and relational cues. This
attribute-centric design supports interpretable and compositional grounding,
enabling analysis that moves beyond simple object recognition to contextual
reasoning in dynamic environments. We envision Talk2Event as a foundation for
advancing multimodal and temporally-aware perception, with applications
spanning robotics, human-AI interaction, and so on.

</details>


### [109] [Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis](https://arxiv.org/abs/2509.09595)
*Yikang Ding,Jiwen Liu,Wenyuan Zhang,Zekun Wang,Wentao Hu,Liyuan Cui,Mingming Lao,Yingchao Shao,Hui Liu,Xiaohan Li,Ming Chen,Xiaoqiang Liu,Yu-Shen Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: Kling-Avatar是一个新型两阶段框架，通过多模态指令理解和光影真实感肖像生成，解决现有音频驱动虚拟形象视频生成方法缺乏高层语义理解的问题，能生成语义驱动的高保真、长时长视频。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动虚拟形象视频生成方法未能有效建模指令的沟通意图，仅将其视为声学或视觉线索的低级追踪，导致叙事连贯性和角色表现力不足。

Method: 引入Kling-Avatar，一个两阶段级联框架：1. 多模态大语言模型（MLLM）导演根据指令生成蓝图视频，控制角色动作和情感等高层语义。2. 依据蓝图关键帧，采用首尾帧策略并行生成多个子片段。此全局到局部的并行架构能保留细节、编码高层意图，并实现长时长视频的快速稳定生成。

Result: 构建了包含375个样本的评估基准。Kling-Avatar能生成生动、流畅、长时长（高达1080p和48fps）的视频，并在唇语同步精度、情感和动态表现力、指令可控性、身份保持和跨域泛化方面均表现优越。

Conclusion: Kling-Avatar为语义理解驱动的高保真音频驱动虚拟形象合成设立了新基准。

Abstract: Recent advances in audio-driven avatar video generation have significantly
enhanced audio-visual realism. However, existing methods treat instruction
conditioning merely as low-level tracking driven by acoustic or visual cues,
without modeling the communicative purpose conveyed by the instructions. This
limitation compromises their narrative coherence and character expressiveness.
To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that
unifies multimodal instruction understanding with photorealistic portrait
generation. Our approach adopts a two-stage pipeline. In the first stage, we
design a multimodal large language model (MLLM) director that produces a
blueprint video conditioned on diverse instruction signals, thereby governing
high-level semantics such as character motion and emotions. In the second
stage, guided by blueprint keyframes, we generate multiple sub-clips in
parallel using a first-last frame strategy. This global-to-local framework
preserves fine-grained details while faithfully encoding the high-level intent
behind multimodal instructions. Our parallel architecture also enables fast and
stable generation of long-duration videos, making it suitable for real-world
applications such as digital human livestreaming and vlogging. To
comprehensively evaluate our method, we construct a benchmark of 375 curated
samples covering diverse instructions and challenging scenarios. Extensive
experiments demonstrate that Kling-Avatar is capable of generating vivid,
fluent, long-duration videos at up to 1080p and 48 fps, achieving superior
performance in lip synchronization accuracy, emotion and dynamic
expressiveness, instruction controllability, identity preservation, and
cross-domain generalization. These results establish Kling-Avatar as a new
benchmark for semantically grounded, high-fidelity audio-driven avatar
synthesis.

</details>


### [110] [Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth](https://arxiv.org/abs/2509.09610)
*Daria Laslo,Efthymios Georgiou,Marius George Linguraru,Andreas Rauschecker,Sabine Muller,Catherine R. Jutzeler,Sarah Bruningk*

Main category: cs.CV

TL;DR: 本研究提出一个混合机制学习框架，结合数学肿瘤生长模型和引导扩散模型，从现有扫描合成未来MRI，以预测脑肿瘤时空进展，并生成生长概率图，特别适用于数据有限场景。


<details>
  <summary>Details</summary>
Motivation: 预测脑肿瘤的时空进展对于神经肿瘤学的临床决策至关重要。

Method: 提出一个混合机制学习框架，结合数学肿瘤生长模型（基于常微分方程，捕捉肿瘤动态及放疗效应）和梯度引导去噪扩散隐式模型（DDIM）。机制模型估计未来肿瘤负荷，并以此条件化DDIM，合成与预测生长和患者解剖结构一致的未来MRI。模型在BraTS成人和儿科胶质瘤数据集上训练，并在内部儿科弥漫性中线胶质瘤（DMG）病例上进行评估。

Result: 该框架能生成基于空间相似性的真实后续扫描。引入了肿瘤生长概率图，能够捕获肿瘤生长的临床相关范围和方向性（通过95百分位Hausdorff距离衡量）。

Conclusion: 该方法能够在数据有限的情况下实现生物学知情的图像生成，提供考虑机制先验的生成式时空预测。

Abstract: Predicting the spatio-temporal progression of brain tumors is essential for
guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic
learning framework that combines a mathematical tumor growth model with a
guided denoising diffusion implicit model (DDIM) to synthesize anatomically
feasible future MRIs from preceding scans. The mechanistic model, formulated as
a system of ordinary differential equations, captures temporal tumor dynamics
including radiotherapy effects and estimates future tumor burden. These
estimates condition a gradient-guided DDIM, enabling image synthesis that
aligns with both predicted growth and patient anatomy. We train our model on
the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices
of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our
framework generates realistic follow-up scans based on spatial similarity
metrics. It also introduces tumor growth probability maps, which capture both
clinically relevant extent and directionality of tumor growth as shown by 95th
percentile Hausdorff Distance. The method enables biologically informed image
generation in data-limited scenarios, offering generative-space-time
predictions that account for mechanistic priors.

</details>


### [111] [Measuring Epistemic Humility in Multimodal Large Language Models](https://arxiv.org/abs/2509.09658)
*Bingkui Tong,Jiaer Xia,Sifeng Shang,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 现有MLLM幻觉基准无法评估模型识别“无正确选项”的能力。本文提出HumbleBench基准，通过包含“以上皆非”选项的问题，评估MLLM拒绝合理错误答案的能力，以更准确地衡量其在安全关键场景下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）的幻觉在实际应用中带来严重风险。现有评估基准主要测试识别准确性，但忽略了模型在无正确选项时能识别出此情况的“认知谦逊”能力，而这对于构建可信赖AI至关重要。

Method: 提出了HumbleBench，一个评估MLLMs拒绝合理但错误答案能力的新型幻觉基准，涵盖对象、关系和属性三类幻觉。该基准基于全景场景图数据集，利用细粒度标注提取真实实体和关系，并由GPT-4-Turbo生成多项选择题，再经严格人工筛选。每个问题都包含“以上皆非”选项。

Result: 使用HumbleBench评估了多种最先进的MLLMs（包括通用型和专业推理模型），并分享了重要的发现和见解。

Conclusion: HumbleBench通过引入明确的错误选项拒绝机制，填补了当前评估工具的关键空白，为MLLM在安全关键环境中的可靠性提供了更现实的衡量标准。

Abstract: Hallucinations in multimodal large language models (MLLMs) -- where the model
generates content inconsistent with the input image -- pose significant risks
in real-world applications, from misinformation in visual question answering to
unsafe errors in decision-making. Existing benchmarks primarily test
recognition accuracy, i.e., evaluating whether models can select the correct
answer among distractors. This overlooks an equally critical capability for
trustworthy AI: recognizing when none of the provided options are correct, a
behavior reflecting epistemic humility. We present HumbleBench, a new
hallucination benchmark designed to evaluate MLLMs' ability to reject plausible
but incorrect answers across three hallucination types: object, relation, and
attribute. Built from a panoptic scene graph dataset, we leverage fine-grained
scene graph annotations to extract ground-truth entities and relations, and
prompt GPT-4-Turbo to generate multiple-choice questions, followed by a
rigorous manual filtering process. Each question includes a "None of the above"
option, requiring models not only to recognize correct visual information but
also to identify when no provided answer is valid. We evaluate a variety of
state-of-the-art MLLMs -- including both general-purpose and specialized
reasoning models -- on HumbleBench and share valuable findings and insights
with the community. By incorporating explicit false-option rejection,
HumbleBench fills a key gap in current evaluation suites, providing a more
realistic measure of MLLM reliability in safety-critical settings. Our code and
dataset are released publicly and can be accessed at
https://github.com/maifoundations/HumbleBench.

</details>


### [112] [Can Understanding and Generation Truly Benefit Together -- or Just Coexist?](https://arxiv.org/abs/2509.09666)
*Zhiyuan Yan,Kaiqing Lin,Zongjian Li,Junyan Ye,Hui Han,Zhendong Wang,Hao Liu,Bin Lin,Hao Li,Xue Xu,Xinyan Xiao,Jingdong Wang,Haifeng Wang,Li Yuan*

Main category: cs.CV

TL;DR: 本文提出UAE框架，将图像理解(I2T)视为编码器、图像生成(T2I)视为解码器，以重建保真度为统一目标。通过新型RL训练方法Unified-GRPO，实现理解与生成的双向信息流和相互增益，并引入Unified-Bench进行评估。


<details>
  <summary>Details</summary>
Motivation: 旨在通过自编码器视角，将图像理解和生成视为互补过程，通过统一的重建目标和双向信息流实现两者协同增益，克服现有模态间学习的局限性。

Method: 引入UAE统一多模态学习框架。首先使用大规模长上下文图像描述预训练解码器；然后通过三阶段的强化学习方法Unified-GRPO进行训练：冷启动阶段、用于理解的生成阶段（训练编码器生成高质量描述以提升解码器理解）、用于生成的理解阶段（精炼解码器从描述中重建以提升生成质量）。引入Unified-Bench评估模型统一性。

Result: 研究发现，随着RL训练的深入，编码器能自主生成更具描述性的文本，而解码器也能深刻理解这些复杂描述，最终实现令人惊叹的高保真图像重建，证明了理解与生成过程的协同作用。

Conclusion: 通过将图像理解和生成整合到统一的自编码器范式中，并采用基于重建保真度的强化学习训练，UAE框架成功建立了理解与生成之间的双向信息流，显著提升了模型的视觉理解能力、长上下文指令遵循能力和生成保真度，展现了多模态学习领域内理解与生成相互促进的巨大潜力。

Abstract: In this paper, we introduce an insightful paradigm through the Auto-Encoder
lens-understanding as the encoder (I2T) that compresses images into text, and
generation as the decoder (T2I) that reconstructs images from that text. Using
reconstruction fidelity as the unified training objective, we enforce the
coherent bidirectional information flow between the understanding and
generation processes, bringing mutual gains. To implement this, we propose UAE,
a novel framework for unified multimodal learning. We begin by pre-training the
decoder with large-scale long-context image captions to capture fine-grained
semantic and complex spatial relationships. We then propose Unified-GRPO via
reinforcement learning (RL), which covers three stages: (1) A cold-start phase
to gently initialize both encoder and decoder with a semantic reconstruction
loss; (2) Generation for Understanding, where the encoder is trained to
generate informative captions that maximize the decoder's reconstruction
quality, enhancing its visual understanding; (3) Understanding for Generation,
where the decoder is refined to reconstruct from these captions, forcing it to
leverage every detail and improving its long-context instruction following and
generation fidelity. For evaluation, we introduce Unified-Bench, the first
benchmark tailored to assess the degree of unification of the UMMs. A
surprising "aha moment" arises within the multimodal learning domain: as RL
progresses, the encoder autonomously produces more descriptive captions, while
the decoder simultaneously demonstrates a profound ability to understand these
intricate descriptions, resulting in reconstructions of striking fidelity.

</details>


### [113] [Geometric Neural Distance Fields for Learning Human Motion Priors](https://arxiv.org/abs/2509.09667)
*Zhengdi Yu,Simone Foti,Linguang Zhang,Amy Zhao,Cem Keskin,Stefanos Zafeiriou,Tolga Birdal*

Main category: cs.CV

TL;DR: 本文介绍了一种新颖的3D生成式人体运动先验——神经黎曼运动场（NRMF），它能够实现鲁棒、时序一致且物理合理的三维运动恢复。


<details>
  <summary>Details</summary>
Motivation: 现有基于VAE或扩散模型的方法在人体运动恢复方面存在局限性。研究旨在开发一种更高阶的运动先验，能明确建模姿态、速度和加速度动力学，以克服这些不足并实现更精确和物理可信的运动。

Method: 引入NRMF，通过一系列神经距离场（NDFs）的零水平集来显式建模人体运动，这些NDFs对应于姿态、过渡（速度）和加速度动力学。NDFs构建在关节旋转、其角速度和角加速度的乘积空间上，严格遵循底层关节的几何结构。此外，还提出了：(i) 一种新颖的自适应步长混合算法，用于投影到合理运动集；(ii) 一种新颖的几何积分器，用于在测试时优化和生成逼真的运动轨迹。

Result: 在AMASS数据集上训练后，NRMF在多个输入模态和多种任务（包括去噪、运动插值以及拟合部分2D/3D观测）中展现出显著且一致的性能提升和泛化能力。

Conclusion: NRMF作为一种结合几何严谨性的高阶3D人体运动先验，能够有效实现鲁棒、时序一致和物理合理的运动恢复，并在广泛任务中表现出强大的泛化能力。

Abstract: We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative
human motion prior that enables robust, temporally consistent, and physically
plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods,
our higher-order motion prior explicitly models the human motion in the zero
level set of a collection of neural distance fields (NDFs) corresponding to
pose, transition (velocity), and acceleration dynamics. Our framework is
rigorous in the sense that our NDFs are constructed on the product space of
joint rotations, their angular velocities, and angular accelerations,
respecting the geometry of the underlying articulations. We further introduce:
(i) a novel adaptive-step hybrid algorithm for projecting onto the set of
plausible motions, and (ii) a novel geometric integrator to "roll out"
realistic motion trajectories during test-time-optimization and generation. Our
experiments show significant and consistent gains: trained on the AMASS
dataset, NRMF remarkably generalizes across multiple input modalities and to
diverse tasks ranging from denoising to motion in-betweening and fitting to
partial 2D / 3D observations.

</details>


### [114] [Locality in Image Diffusion Models Emerges from Data Statistics](https://arxiv.org/abs/2509.09672)
*Artem Lukoianov,Chenyang Yuan,Justin Solomon,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 本文提出深度扩散模型中的局部性源于图像数据集的统计特性（像素相关性），而非卷积神经网络的归纳偏置。通过理论和实验证明，并基于此洞察构建了一个优于现有方法的分析去噪器。


<details>
  <summary>Details</summary>
Motivation: 扩散模型存在一个最优去噪器，但它只能复现训练集图像，与深度扩散模型的行为不符。现有工作试图解释这一差异，并认为卷积神经网络的移位等变性和局部性归纳偏置是原因。本文旨在探究深度扩散模型中局部性特性的真正来源。

Method: 通过证明一个最优参数化线性去噪器也表现出与深度神经网络去噪器相似的局部性。理论上和实验上展示局部性直接来源于自然图像数据集中的像素相关性。最后，利用这些发现设计了一个新的分析去噪器。

Result: 研究发现深度扩散模型中的局部性是图像数据集的统计特性，而非卷积神经网络的归纳偏置所致。最优参数化线性去噪器表现出相似的局部性。局部性直接来源于自然图像数据集的像素相关性。所设计的分析去噪器比之前的专家设计方法更能匹配深度扩散模型预测的分数。

Conclusion: 深度扩散模型中的局部性是自然图像数据的一种统计学上的涌现特性，而非卷积神经网络归纳偏置的直接结果。理解这一数据驱动的局部性有助于构建更准确模仿深度扩散模型的分析去噪器。

Abstract: Among generative models, diffusion models are uniquely intriguing due to the
existence of a closed-form optimal minimizer of their training objective, often
referred to as the optimal denoiser. However, diffusion using this optimal
denoiser merely reproduces images in the training set and hence fails to
capture the behavior of deep diffusion models. Recent work has attempted to
characterize this gap between the optimal denoiser and deep diffusion models,
proposing analytical, training-free models that can generate images that
resemble those generated by a trained UNet. The best-performing method
hypothesizes that shift equivariance and locality inductive biases of
convolutional neural networks are the cause of the performance gap, hence
incorporating these assumptions into its analytical model. In this work, we
present evidence that the locality in deep diffusion models emerges as a
statistical property of the image dataset, not due to the inductive bias of
convolutional neural networks. Specifically, we demonstrate that an optimal
parametric linear denoiser exhibits similar locality properties to the deep
neural denoisers. We further show, both theoretically and experimentally, that
this locality arises directly from the pixel correlations present in natural
image datasets. Finally, we use these insights to craft an analytical denoiser
that better matches scores predicted by a deep diffusion model than the prior
expert-crafted alternative.

</details>


### [115] [SpatialVID: A Large-Scale Video Dataset with Spatial Annotations](https://arxiv.org/abs/2509.09676)
*Jiahao Wang,Yufeng Yuan,Rujie Zheng,Youtian Lin,Jian Gao,Lin-Zhuo Chen,Yajie Bao,Yi Zhang,Chang Zeng,Yanxi Zhou,Xiaoxiao Long,Hao Zhu,Zhaoxiang Zhang,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: 本文介绍了SpatialVID，一个大规模、高质量的视频数据集，拥有密集的3D和语义标注，旨在解决空间智能领域训练数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 当前空间智能模型（包括空间重建和世界探索）的扩展性和真实世界保真度受到大规模、高质量训练数据稀缺的严重限制。现有数据集在规模、多样性和标注丰富性方面不足，尤其缺乏真实世界动态场景中的地面真值相机运动数据。

Method: 作者收集了SpatialVID数据集，该数据集包含大量野外视频。具体方法包括：收集超过21,000小时的原始视频，通过分层过滤管道处理成270万个片段（总计7,089小时的动态内容），并使用后续标注管道丰富了详细的空间和语义信息，如每帧相机姿态、深度图、动态掩码、结构化字幕和序列化运动指令。

Result: SpatialVID数据集包含了7,089小时的动态内容，具有多样化的场景、相机运动和密集的3D及语义标注。数据统计分析表明，其丰富性和多样性直接促进了模型的泛化能力和性能提升。

Conclusion: SpatialVID数据集凭借其大规模和高质量的标注，被确立为视频和3D视觉研究社区的关键资产，有望显著改善空间智能模型的泛化能力和性能。

Abstract: Significant progress has been made in spatial intelligence, spanning both
spatial reconstruction and world exploration. However, the scalability and
real-world fidelity of current models remain severely constrained by the
scarcity of large-scale, high-quality training data. While several datasets
provide camera pose information, they are typically limited in scale,
diversity, and annotation richness, particularly for real-world dynamic scenes
with ground-truth camera motion. To this end, we collect \textbf{SpatialVID}, a
dataset consists of a large corpus of in-the-wild videos with diverse scenes,
camera movements and dense 3D annotations such as per-frame camera poses,
depth, and motion instructions. Specifically, we collect more than 21,000 hours
of raw video, and process them into 2.7 million clips through a hierarchical
filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent
annotation pipeline enriches these clips with detailed spatial and semantic
information, including camera poses, depth maps, dynamic masks, structured
captions, and serialized motion instructions. Analysis of SpatialVID's data
statistics reveals a richness and diversity that directly foster improved model
generalization and performance, establishing it as a key asset for the video
and 3D vision research community.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [116] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: 本文提出了一种区间二型（IT2）贝叶斯定理，以处理来自专家（SMEs）的非精确区间输入概率，并开发了一个新颖的算法将专家提供的区间编码为IT2模糊隶属函数。


<details>
  <summary>Details</summary>
Motivation: 传统的贝叶斯推断假设输入值精确，但在实际应用中，专家提供的信息往往是区间范围估计而非精确值，这使得传统方法不切实际。

Method: 1. 开发了一个IT2版本的贝叶斯定理，采用了一种新颖且保守的方法来避免输入IT2模糊隶属函数（MFs）中潜在的不一致性。2. 提出了一种新颖且灵活的算法，用于将专家提供的区间编码为IT2模糊隶属函数，以指定贝叶斯定理中的输入概率。

Result: 文章提供了两个关键贡献：一个IT2版本的贝叶斯定理，以及一个将专家提供的区间编码为IT2模糊隶属函数的新算法。该算法概括并扩展了以往在此问题上的工作。

Conclusion: 通过扩展贝叶斯定理为IT2版本并提供将专家区间编码为IT2 MFs的方法，使得贝叶斯推断能更有效地处理现实世界中非精确的、基于专家判断的概率输入。

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [117] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: 本文提出一个新颖的框架，利用NLP和多模态LLM（如微调的LLaMA-3）将游戏设计文档（GDD）自动转化为功能性的Unity游戏原型。


<details>
  <summary>Details</summary>
Motivation: 旨在解决AI辅助游戏开发中的关键空白，并简化从游戏设计到实现的原型制作过程。

Method: 开发了一个端到端系统，该系统解析GDD，提取结构化的游戏规范，并合成实现核心机制、系统和架构的Unity兼容C#代码。该方法结合了专门用于Unity代码生成的微调LLaMA-3模型和一个自定义的Unity集成包。

Result: 评估结果表明，相较于基线模型有显著改进，微调模型在编译成功率、GDD依从性、最佳实践采纳和代码模块化等指标上表现优异（平均得分4.8/5.0）。生成的模板在多种游戏类型中展现出高度符合GDD规范的特性。

Conclusion: 该系统有效弥补了AI辅助游戏开发中的关键缺陷，将大型语言模型定位为简化游戏设计到实现过程中的宝贵工具。

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [118] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 本文提出了一个基于多智能体LLM的框架，通过将建模任务按全局约束类型分解，将自然语言描述的优化/满足问题转化为MiniZinc模型，并取得了优于基线的性能。


<details>
  <summary>Details</summary>
Motivation: 将自然语言描述的优化或满足问题转化为正确的MiniZinc模型极具挑战性，因为它同时需要逻辑推理和约束编程专业知识。

Method: 该框架采用智能体方法：多个专业LLM智能体根据全局约束类型分解建模任务。每个智能体专注于检测和生成特定类型全局约束的代码，一个最终的组装智能体将这些约束片段整合为一个完整的MiniZinc模型。这种方法将问题分解为更小、定义明确的子任务，每个LLM处理更简单的推理挑战，从而可能降低整体复杂性。

Result: 通过对多个LLM进行初步实验，结果显示该方法在性能上优于one-shot prompting和chain-of-thought prompting等基线。此外，论文还概述了未来工作的全面路线图。

Conclusion: 该多智能体LLM框架有效解决了自然语言到MiniZinc模型转换的挑战，通过任务分解简化了推理，并在初步实验中取得了优于基线的性能。

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [119] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: 针对生成AI模型在合成数据上训练导致的“模型崩溃”问题，本文提出了一种名为截断交叉熵（TCE）的置信度感知损失函数，有效延缓了模型崩溃，并通过降低模型对其自生成数据的过高置信度来维持模型质量。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型对合成数据的日益依赖，预计到2030年大部分新训练数据将由机器生成。在合成数据上反复训练会导致模型性能退化的“模型崩溃”现象，现有缓解策略有限。本文将模型对其自生成数据的过度置信识别为模型崩溃的关键驱动因素。

Method: 基于对模型过度置信的观察，本文提出了一种置信度感知损失函数，即截断交叉熵（TCE），它在训练过程中降低高置信度预测的权重。此外，本文提供了一个模型无关的框架，将损失函数设计与模型崩溃缓解联系起来。

Result: 研究表明，TCE显著延缓了递归训练中的模型崩溃，将模型在崩溃前的保真度区间延长了2.3倍以上。该方法还被证明可以推广到不同模态。

Conclusion: 损失函数的设计为在合成数据日益增多的时代保持生成模型质量提供了一种简单而强大的工具。

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [120] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: 本研究关注可解释AI (XAI) 中常被忽视的不确定性解释和全局解释，测试了一种能同时处理不确定性、鲁棒性和全局XAI的算法，旨在评估其校准信任、提升用户满意度和人类可解释性的能力。


<details>
  <summary>Details</summary>
Motivation: 可解释AI领域中，关于不确定性解释和全局解释的研究相对不足。研究人员致力于构建XAI方案的通用指南，本文旨在通过聚焦这些领域，探索提升信任校准、用户满意度和人类可解释性的方法。

Method: 选取了一个能够同时涵盖不确定性、鲁棒性及全局XAI概念的算法。研究方法包括测试该算法校准信任的能力，并检验一个旨在提供直观视觉理解的算法（尽管其本身复杂）是否能提高用户满意度和人类可解释性。

Result: 摘要中未明确给出研究的具体发现或量化结果。

Conclusion: 摘要中未明确给出基于研究发现的最终结论，主要阐述了研究的范围和目标。

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [121] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: 本文提出了一种基于上下文条件提示词（prompt）的LLM方法，通过优化提示词结构和示例注入，有效解决了推荐系统中的冷启动用户问题，显著提升了低数据量设置下的推荐性能。


<details>
  <summary>Details</summary>
Motivation: 推荐系统在处理冷启动用户时，因缺乏历史行为信息，其有效性受到限制。

Method: 引入了一种上下文条件提示词公式 P(u, Ds) -> R̂，其中 u 为冷启动用户资料，Ds 为精选支持集，R̂ 为预测的物品排名列表。该方法通过优化示例注入和指令结构，并结合token级别对齐和嵌入空间正则化来提高语义保真度，应用于少数样本（few-shot）LLM。

Result: 基于对BioGPT、LLaMA-2、GPT-4等LLM的实验表明，在低数据量环境下，该方法显著提升了模型的precision@k和NDCG分数。研究发现提示词的及时组成不仅是语法层面的，更是功能性的，直接控制着推理过程中的注意力规模和解码器行为。

Conclusion: 提示词（prompt）自适应是解决基于LLM的推荐系统中冷启动问题的一种有效方法。

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [122] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: 研究比较了人类、大型语言模型（LLMs）和贝叶斯智能体在动态谈判中的表现，发现性能上的相似性可能掩盖了行为模式和对齐方式的显著差异，这对于实际部署至关重要。


<details>
  <summary>Details</summary>
Motivation: 随着自主智能体日益承担人类的协调任务，评估其性能及其在动态多智能体环境中的谈判过程至关重要。不同智能体（如贝叶斯模型和LLMs）各有优势，需要进行比较以理解其在谈判情境下的表现。

Method: 本研究在一个动态谈判环境中比较了人类（N=216）、大型语言模型（GPT-4o, Gemini 1.5 Pro）和贝叶斯智能体。实验设计旨在通过相同条件下的直接比较，捕捉谈判结果和行为动态。

Result: 贝叶斯智能体通过激进优化获得了最高剩余，但伴随着频繁的交易拒绝。人类和LLMs能够实现相似的总体剩余，但行为模式截然不同：LLMs偏好保守、让步性交易且拒绝率低，而人类则采用更具策略性、冒险性和公平导向的行为。

Conclusion: 智能体评估中常见的“性能均等”基准可能掩盖了过程和对齐方面的根本差异。这些差异对于智能体在现实世界协调任务中的实际部署至关重要。

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [123] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: 本文提出一种利用机器学习构建的反洗钱（AML）流水线，旨在识别高风险银行客户。该流水线在竞赛数据集中取得了0.961的AUROC，并获得第二名。


<details>
  <summary>Details</summary>
Motivation: 金融机构将反洗钱（AML）视为优先事项，而机器学习（ML）在此领域显示出巨大潜力。

Method: 提出一套全面系统的机器学习流水线，用于识别高风险银行客户。方法包括：对包含195,789个客户ID的数据集进行16步设计和统计分析，利用SQLite数据库组织数据，开发基于SQL的特征工程算法，将预训练模型与数据库连接以实现推理，并提供可解释人工智能（XAI）模块以分析特征重要性。

Result: 该流水线取得了0.961的平均AUROC（标准差0.005），并在大学竞赛中获得第二名。

Conclusion: 本文提出的机器学习流水线能够有效识别高风险银行客户，且在竞争性环境中表现出色，验证了其鲁棒性和高效性。

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [124] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: 本文提出了一个受计算神经科学启发的六模块计算框架，旨在弥补Agentic AI在空间推理能力上与人类的差距。研究基于此框架分析现有方法、识别关键缺陷，并探讨了基准、应用及未来研究方向，以期推动Agentic AI在三维物理世界中的交互能力。


<details>
  <summary>Details</summary>
Motivation: Agentic AI在自主任务执行和语言推理方面有所进展，但其空间推理能力仍有限，主要受限于符号和顺序处理。人类的空间智能则基于多感官感知、空间记忆和认知地图，能在非结构化环境中灵活决策。因此，弥合这一差距对于提升Agentic空间智能以更好地与物理三维世界交互至关重要。

Method: 本研究首先审视计算神经科学中的空间神经模型，并据此提出一个基于神经科学原理的新型计算框架。该框架将核心生物功能映射到六个基本计算模块：仿生多模态感知、多感官整合、自我中心-异我中心转换、人工认知地图、空间记忆和空间推理。在此基础上，文章对近期方法进行了框架引导的分析，评估其与各模块的相关性并识别开发更具神经科学基础的空间推理模块的关键缺陷。此外，还审查了新兴基准、数据集，探讨了从虚拟到具身系统（如机器人）的潜在应用领域，并概述了可推广空间推理的潜在研究方向。

Result: 提出了一个由六个核心模块组成的、基于神经科学原理的计算框架，为Agentic AI在虚拟和物理环境中的空间推理能力提供了视角。通过框架引导的分析，识别了当前方法在实现神经科学基础的空间推理模块方面存在的关键差距。此外，还审查了新兴基准、数据集和潜在应用领域，并展望了未来研究方向，强调了可推广空间推理的路线图。

Conclusion: 这项工作为Agentic空间智能研究提供了一个基于神经科学的视角和结构化的发展路径，有望促进Agentic AI在三维物理世界中更高效、更智能地进行交互。

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [125] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: 本文提出ProgD，一种结合动态异构图的渐进式多尺度解码策略，旨在解决自动驾驶中多智能体交互的演变性和不确定性，并在INTERACTION和Argoverse 2基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体运动预测方法忽略了智能体之间交互的演变性，难以有效处理未来场景中复杂的、不确定的交互。

Method: 提出了一种名为ProgD的新型渐进式多尺度解码策略。该方法利用动态异构图对场景进行建模，以显式且全面地捕捉未来场景中演变的社会交互及其固有的不确定性。通过动态异构图的展开，设计了分解架构来处理时空依赖性并逐步消除未来运动的不确定性。此外，还引入了多尺度解码过程以改进场景建模和预测一致性。

Result: 所提出的ProgD模型在INTERACTION多智能体预测基准测试中排名第一，并在Argoverse 2多世界预测基准测试中也取得了最先进的性能。

Conclusion: ProgD通过创新的动态异构图建模和渐进式多尺度解码策略，有效解决了多智能体交互的演变性和不确定性问题，显著提升了自动驾驶中智能体运动预测的准确性和一致性。

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [126] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: LLM智能体虽具潜力但行为不可预测，带来治理挑战。本文提出一个基于区块链的分层架构，包含行为追踪、声誉评估和恶意行为预测模块，旨在建立可信、弹性、可扩展的监管机制。


<details>
  <summary>Details</summary>
Motivation: LLM赋能的自主智能体尽管在多领域（如金融、医疗、智能制造）带来巨大机遇，但其不可预测的行为和异构能力对治理和问责制构成了重大挑战。

Method: 提出一个区块链赋能的监管智能体协作分层架构，包含智能体层、区块链数据层和监管应用层。在此框架内，设计了三个关键模块：(i) 用于自动化问责的智能体行为追踪与仲裁；(ii) 用于协作场景信任评估的动态声誉评估；(iii) 用于早期检测对抗性活动的恶意行为预测。

Result: 所提出的方法为大规模智能体生态系统中的可信、弹性且可扩展的监管机制奠定了系统性基础。

Conclusion: 本文的方法为多智能体系统中区块链赋能的监管框架奠定了系统性基础，并讨论了未来的研究方向。

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [127] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 当前大型语言模型（LLMs）在复杂数据分析的多步推理和工具使用方面存在局限。本文提出了一个管道来提取高质量、基于工具的数据分析任务和解决方案，并构建了NbQA数据集。在此基础上，引入了Jupiter框架，将数据分析建模为MCTS搜索问题。实验结果表明，该方法显著提升了LLMs在复杂多步数据分析任务中的表现，超越了GPT-4o等现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在自动化数据科学工作流程中，难以进行多步推理和有效使用工具，这限制了它们在处理复杂数据分析任务时的效率和能力。

Method: 1.  **数据提取与数据集构建**: 提出一个可扩展的管道，从真实世界的Jupyter Notebooks及其相关数据文件中提取高质量、基于工具的数据分析任务及其可执行的多步解决方案，并构建了大规模标准化任务-解决方案对数据集NbQA。
2.  **Jupiter框架**: 引入Jupiter框架，将数据分析问题公式化为搜索问题，并应用蒙特卡洛树搜索（MCTS）来生成多样化的解决方案轨迹以进行价值模型学习。
3.  **高效推理**: 在推理阶段，Jupiter结合价值模型和节点访问计数，以最小的搜索步骤高效收集可执行的多步计划。

Result: 1.  **性能领先**: 基于NbQA数据集，Qwen2.5-7B和14B-Instruct模型在InfiAgent-DABench上分别解决了77.82%和86.38%的任务，匹配或超越了GPT-4o和先进的代理框架。
2.  **泛化与推理能力增强**: 进一步评估表明，该方法在多样化的多步推理任务中展现出改进的泛化能力和更强的工具使用推理能力。

Conclusion: 本文提出的NbQA数据集和Jupiter框架有效解决了LLMs在复杂数据分析任务中多步推理和工具使用能力不足的问题，显著提升了LLMs在实际数据科学场景下的性能和泛化能力，其表现已达到或超越了当前先进的LLM模型和代理框架。

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [128] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: 本文比较了三种知识图谱三元组构建方法（spaCy, OpenIE, GraphRAG）与LLM结合用于问答的性能，旨在解决传统RAG在复杂长文本理解上的局限。结果显示OpenIE覆盖全面，GraphRAG推理能力突出。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在处理复杂、长文本的整体和主题理解时存在局限，需要更深层次的文本和上下文分析。知识图谱作为信息结构化工具，有望增强问答系统，解决这一挑战。

Method: 对三种开源知识图谱三元组构建方法（spaCy, Stanford CoreNLP-OpenIE, GraphRAG）与大型语言模型（LLM）结合用于问答进行了全面的技术比较研究。评估了这些方法的有效性、可行性、适应性及其对LLM问答性能的影响。

Result: 实验结果表明，OpenIE提供了最全面的三元组覆盖，而GraphRAG在推理能力方面表现出卓越的性能。

Conclusion: 文章讨论了每种方法的优缺点，并为改进基于知识图谱的问答系统指明了未来的研究方向。

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [129] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: 本文提出将蒙特卡洛树搜索（MCTS）生成的轨迹重新用于偏好强化学习（RL）中的策略优化，通过阶段式Group Relative Policy Optimization (GRPO)训练和新颖的树状优势估计，以提高大型语言模型（LLMs）的推理能力。


<details>
  <summary>Details</summary>
Motivation: 鉴于LLMs在推理方面的最新进展以及MCTS在生成高质量中间轨迹（尤其在数学和符号领域）方面的有效性，我们受此启发，探索如何将MCTS衍生的轨迹（传统上用于训练价值或奖励模型）重新利用，以改进偏好强化学习中的策略优化，特别是在不依赖价值网络的GRPO算法中。

Method: 我们提出了一种阶段式GRPO训练范式，其中补全内容来源于部分揭示的MCTS rollout，并引入了一种新颖的树状结构优势估计设置。这产生了一类丰富的、以_前缀为条件_的奖励信号，并对其进行了理论和实证分析。为缓解优势饱和和奖励信号崩溃等问题，我们提出了启发式和统计学解决方案。

Result: 初步结果表明，结构化优势估计能够稳定更新并更好地反映组合推理质量。然而，研究也发现，优势饱和和奖励信号崩溃等挑战依然存在。

Conclusion: MCTS轨迹在改进偏好强化学习策略优化方面展现出潜力，尤其是在引入树状优势估计后能稳定更新。但为了充分发挥其潜力，仍需解决优势饱和和奖励信号崩溃等关键挑战，未来的研究将聚焦于阶段式或树状奖励结构下的学习问题。

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [130] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: LightAgent是一个轻量级且功能强大的开源智能体框架，旨在解决现有框架在灵活性与简易性间的权衡，通过整合核心功能支持开发者轻松构建自学习智能体。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）推动了多智能体系统（MAS）的显著发展，但在设计通用、鲁棒和高效的智能体部署平台方面仍存在挑战，特别是现有框架难以同时兼顾灵活性和简易性。

Method: 研究提出了LightAgent框架，一个轻量级但功能强大的智能体框架。该框架通过整合内存（mem0）、工具和思维树（ToT）等核心功能，同时保持极简的结构，以有效解决现有框架在灵活性和简易性上的权衡。

Result: LightAgent成功提供了一个轻量级而强大的解决方案，有效解决了智能体框架在灵活性和简易性之间的矛盾。作为一个完全开源的解决方案，它能与主流聊天平台无缝集成，使开发者能够轻松构建自学习智能体。

Conclusion: LightAgent作为一个创新的轻量级开源框架，为克服现有智能体部署平台的局限性提供了有效途径，尤其在平衡灵活性与简易性方面表现出色，将极大促进自学习智能体在各种应用场景中的开发与部署。

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [131] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: 本文研究了在各种锦标赛规则下，为候选者获胜提供认证解释的问题。通过识别“最小支持”（即候选者必然获胜的最小子锦标赛），并为多数规则提供了多项式时间算法，为AI解释性提供了紧凑、可靠的方案。


<details>
  <summary>Details</summary>
Motivation: 锦标赛广泛用于表示候选者之间的成对优势。研究动机是为了提供关于候选者为何能在各种锦标赛规则下获胜的认证解释，这在形式化可解释AI中是一个核心概念。

Method: 研究方法是定义并识别“最小支持”，即在其中候选者必然获胜的最小子锦标赛。重点关注了以下常见锦标赛解决方案规则：顶循环、未覆盖集、科普兰规则、博尔达规则、最大最小值规则和加权未覆盖集。确定了最小支持的最小尺寸，并为除加权未覆盖集外的所有规则提供了多项式时间算法。

Result: 研究结果确定了各种常见锦标赛规则下最小支持的最小尺寸。为除加权未覆盖集外的所有规则提出了计算最小支持的多项式时间算法。对于加权未覆盖集，该问题是NP完全的。

Conclusion: 结论是最小支持可以为锦标赛中的获胜者提供紧凑、认证且直观的解释。

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [132] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: 探讨在沟通受限的团队中，探索多样性、移动专业化和自适应空间接近性等空间协调维度如何影响团队绩效。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多关注同步、同地团队或知识型工作协调，但对消防员、军人等需要在缺乏视觉线索和显式沟通的情况下，协调物理空间移动的分布式团队的隐式空间协调研究不足。

Method: 通过一项在线协同搜救任务，招募34个四人团队（共136名参与者），限制显式沟通，团队成员需通过移动模式进行协调。采用测量空间接近性、分布模式和移动对齐等指标来分析空间协调的三维度。

Result: 研究发现，空间专业化正向预测团队绩效；自适应空间接近性呈现边际倒U形关系，表明中等程度的适应性最优。此外，这些指标的时间动态能有效区分高绩效和低绩效团队。

Conclusion: 研究结果为基于角色的团队隐式空间协调提供了新见解，强调了平衡适应策略的重要性，并对团队训练和AI辅助系统具有实际指导意义。

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [133] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: 提出了TAM Bench，一个多样化、现实且结构化的基准，用于评估基于LLM的端到端机器学习代理。


<details>
  <summary>Details</summary>
Motivation: 现有基准在任务覆盖、领域多样性、难度建模和评估严谨性方面存在局限，未能充分捕捉基于LLM的ML代理在现实环境中的全部能力。

Method: TAM Bench通过以下三项创新实现：1) 基于浏览器自动化和LLM的任务获取系统，自动从Kaggle等平台收集多模态ML挑战。2) 基于排行榜的难度建模机制，利用参与者数量和分数分散来估算任务复杂性。3) 包含性能、格式符合性、约束遵守和任务泛化等维度的多维评估框架。基于150个AutoML任务构建了Lite、Medium和Full三个基准子集。

Result: 成功构建了TAM Bench，一个全面的基准，其中包括Lite、Medium和Full三个不同规模的子集。Lite版本包含18个任务，在模态和难度上实现了均衡覆盖，可作为日常基准测试的实用平台。

Conclusion: TAM Bench提供了一个全面且客观的基准，用于评估基于LLM的端到端ML代理，解决了现有基准的不足。

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [134] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: 本文提出一种结合VLM常识和课程学习的DRL架构，用于自主代理高效、资源节约地进行语义探索，显著提升物体发现率和语义导航能力。


<details>
  <summary>Details</summary>
Motivation: 现有具身代理的自主探索缺乏高级认知能力，传统强化学习方法难以平衡高效探索与语义理解，导致语义探索中常需人工干预。

Method: 引入一种新型深度强化学习（DRL）架构，通过分层奖励函数整合视觉-语言模型（VLM）常识。将VLM查询建模为专用动作，使代理能策略性地按需查询VLM以获取外部指导并节省资源。结合课程学习策略，指导不同复杂程度的学习，确保学习的鲁棒性。

Result: 实验结果表明，该代理显著提高了物体发现率，习得了有效导航至语义丰富区域的能力，并掌握了何时主动获取外部环境信息的策略。

Conclusion: 本研究提供了一种实用且可扩展的方法，将常识语义推理嵌入自主代理，为机器人领域实现完全智能和自主引导的探索提供了一种新颖途径。

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [135] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: 本文提出了一种名为TORSO的新方法，旨在引导大型语言模型（LLMs）利用其内部推理能力来解决问题，无需手动构建少样本示例，并在多个基准测试中取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有引导LLMs进行推理的方法（如少样本提示）过度依赖提供的示例，限制了模型内在推理能力的发挥；同时，构建针对特定任务的少样本提示成本高昂，且可能导致不同任务间的不一致性。

Method: 引入了“模板导向推理”（Template-Oriented Reasoning, TORSO）方法。该方法旨在激发模型利用其内部推理能力，在无需手动编写少样本示例的情况下，为各种任务生成恰当的响应。

Result: 实验结果表明，TORSO在多样化的LLM基准测试中取得了强大的性能，并能生成合理的推理过程（rationales）。

Conclusion: TORSO成功地使LLMs能够利用其内部推理能力处理多项任务，克服了传统少样本方法对示例的依赖和成本问题，显著提升了模型性能。

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [136] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: 报告分析了法律领域LLM幻觉问题，探讨了RAG的局限性，并提出应采用咨询式AI范式，强调人工监督和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 法律领域LLMs出现“幻觉”（错误信息）是一个严峻挑战，需要解决其成因、表现及寻找有效缓解策略，以确保AI在法律应用中的准确性和可靠性。

Method: 分析了法律LLMs幻觉的成因和表现，评估了RAG缓解策略的有效性及其局限性，并提出了整体优化方案。同时，探讨了相关的伦理和监管影响。

Result: 发现RAG策略在缓解LLM幻觉方面存在局限性，人类监督在法律AI应用中扮演着不可替代的角色。仅仅渐进式改进生成模型不足以解决问题。

Conclusion: 解决方案在于采纳“咨询式”AI范式，该范式优先考虑真实性和可追溯性，作为放大而非取代专业判断的工具，而非仅依赖生成模型的增量改进。

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [137] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: 本文提出了SEDM（自进化分布式记忆）框架，将多智能体系统的记忆从被动存储库转变为主动自优化的组件，以解决现有记忆管理中噪音积累、内存膨胀和泛化能力受限的问题。


<details>
  <summary>Details</summary>
Motivation: 长期多智能体系统会产生大量轨迹和历史交互，高效内存管理对性能和可扩展性至关重要。现有方法依赖向量检索和分层存储，但易受噪音积累、内存无限制扩张和跨领域泛化能力受限等问题困扰。

Method: SEDM是一个可验证、自适应的框架，它集成了三个关键组件：基于可复现回放的可验证写入准入机制；根据经验效用动态排序和整合条目的自调度记忆控制器；以及抽象可重用洞察以支持异构任务间知识迁移的跨领域知识扩散机制。

Result: 在基准数据集上，SEDM与现有强记忆基线相比，提高了推理准确性并降低了token开销。此外，它能将从事实验证中提取的知识用于增强多跳推理能力。

Conclusion: SEDM为开放式多智能体协作提供了一种可扩展且可持续的记忆机制。

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [138] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: 本研究利用变分量子电路在图像字幕任务中学习组合张量语义的表示，以解决现有AI模型在组合泛化能力上的不足，并在部分设置下超越了经典模型。


<details>
  <summary>Details</summary>
Motivation: 组合泛化是人类认知的一个关键方面，但当前诸如视觉-语言模型等AI工具缺乏此能力。先前的组合张量语义研究未能解决这一挑战。作者推测量子模型更高的训练效率有望改善这些任务的性能。

Method: 研究人员将组合张量模型的表示解释为希尔伯特空间中的表示，并训练变分量子电路（Variational Quantum Circuits）来学习这些表示。任务是需要组合泛化的图像字幕任务。使用了两种图像编码技术：二值图像向量的多热编码（MHE）和来自视觉-语言模型CLIP的图像向量的角度/振幅编码。

Result: 使用带噪声的MHE编码实现了良好的概念验证结果。在CLIP图像向量上的表现喜忧参半，但仍优于经典的组合模型。

Conclusion: 量子模型在解决AI组合泛化问题上显示出潜力，尤其是在特定编码（如MHE）下能取得较好的概念验证结果，并且在某些情况下优于传统的组合模型。

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [139] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: Auras通过解耦感知与生成并采用流水线并行，结合共享上下文解决数据陈旧问题，显著提升具身AI的推理频率和吞吐量，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 具身AI系统在动态环境中面临高频输入输出需求，而传统顺序计算模式难以达到所需的“思考”频率，限制了其实际应用。

Method: 提出Auras，一个算法-系统协同设计的推理框架。它解耦感知与生成模块，通过受控的流水线并行实现高吞吐量；为解决并行带来的数据陈旧问题，Auras建立公共上下文供感知与生成共享，以确保精度。

Result: 平均吞吐量提升2.54倍，同时保持了原始精度102.7%。

Conclusion: Auras有效克服了顺序计算的局限性，在实现高吞吐量的同时，确保了具身AI系统的推理精度。

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [140] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: 大语言模型（LLMs）在长任务上的失败源于执行而非推理。单步精度微小提升能带来任务长度的指数级增长。研究发现模型存在“自我条件效应”（因自身错误导致后续错误），且仅靠模型规模缩放无法解决。强调执行能力，并指出大模型和“思考模型”在执行长任务上的巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）的持续扩展是否会带来边际效益递减，特别是对于衡量实际价值的长任务。研究动机在于解释LLMs为何在需要多步的简单任务上表现不佳，并假设这主要源于执行错误而非推理能力不足。

Method: 通过显式提供解决长任务所需的知识和计划，来隔离并测试LLM的执行能力。此外，还对前沿的“思考模型”在单次执行长任务方面的能力进行了基准测试。

Result: 1. 单步精度的小幅提升能指数级改善模型成功完成任务的长度。
2. 即使小模型单步精度达到100%，大模型也能正确执行显著更多的步骤。
3. 模型的每步精度随着步骤增加而下降，这并非仅因长上下文限制。
4. 发现存在“自我条件效应”：模型在上下文中包含自身先前错误时，更易犯错。
5. “自我条件效应”无法仅通过扩大模型规模来减少。
6. “思考模型”没有“自我条件效应”，并且能单次执行更长的任务。

Conclusion: 聚焦执行能力有助于解释LLM为何能解决复杂推理问题却在更长的简单任务上失败。研究强调了扩大模型规模和在测试时进行顺序计算对长任务的巨大益处，这能够显著提升模型处理复杂、多步骤任务的能力。

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [141] [Uncertainty Estimation using Variance-Gated Distributions](https://arxiv.org/abs/2509.08846)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: 本文提出一种基于类概率分布信噪比的新型不确定性估计与分解框架，解决了传统加性分解的质疑，并用于探讨委员会机器多样性崩溃现象。


<details>
  <summary>Details</summary>
Motivation: 高风险应用中，单样本不确定性量化评估至关重要。传统方法将预测不确定性分解为认知不确定性与偶然不确定性，但其加性分解的有效性近期受到质疑，因此需要新的不确定性估计和分解方法。

Method: 提出一个直观的不确定性估计和分解框架，该框架基于不同模型预测中类概率分布的信噪比。引入了一种方差门控度量，通过集成模型导出的置信因子来调整预测。

Result: 利用所提出的方差门控度量，本文探讨了委员会机器中多样性崩溃现象的存在。

Conclusion: 本文提出了一种新颖的不确定性估计与分解框架，有效应对了现有加性分解的局限性。该框架基于信噪比和方差门控度量，并为理解集成模型（委员会机器）的多样性行为提供了新的视角。

Abstract: Evaluation of per-sample uncertainty quantification from neural networks is
essential for decision-making involving high-risk applications. A common
approach is to use the predictive distribution from Bayesian or approximation
models and decompose the corresponding predictive uncertainty into epistemic
(model-related) and aleatoric (data-related) components. However, additive
decomposition has recently been questioned. In this work, we propose an
intuitive framework for uncertainty estimation and decomposition based on the
signal-to-noise ratio of class probability distributions across different model
predictions. We introduce a variance-gated measure that scales predictions by a
confidence factor derived from ensembles. We use this measure to discuss the
existence of a collapse in the diversity of committee machines.

</details>


### [142] [Instance-Optimal Matrix Multiplicative Weight Update and Its Quantum Applications](https://arxiv.org/abs/2509.08911)
*Weiyuan Gong,Tongyang Li,Xinzhao Wang,Zhiyu Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Matrix Multiplicative Weight Update (MMWU) is a seminal online learning
algorithm with numerous applications. Applied to the matrix version of the
Learning from Expert Advice (LEA) problem on the $d$-dimensional spectraplex,
it is well known that MMWU achieves the minimax-optimal regret bound of
$O(\sqrt{T\log d})$, where $T$ is the time horizon. In this paper, we present
an improved algorithm achieving the instance-optimal regret bound of
$O(\sqrt{T\cdot S(X||d^{-1}I_d)})$, where $X$ is the comparator in the regret,
$I_d$ is the identity matrix, and $S(\cdot||\cdot)$ denotes the quantum
relative entropy. Furthermore, our algorithm has the same computational
complexity as MMWU, indicating that the improvement in the regret bound is
``free''.
  Technically, we first develop a general potential-based framework for matrix
LEA, with MMWU being its special case induced by the standard exponential
potential. Then, the crux of our analysis is a new ``one-sided'' Jensen's trace
inequality built on a Laplace transform technique, which allows the application
of general potential functions beyond exponential to matrix LEA. Our algorithm
is finally induced by an optimal potential function from the vector LEA
problem, based on the imaginary error function.
  Complementing the above, we provide a memory lower bound for matrix LEA, and
explore the applications of our algorithm in quantum learning theory. We show
that it outperforms the state of the art for learning quantum states corrupted
by depolarization noise, random quantum states, and Gibbs states. In addition,
applying our algorithm to linearized convex losses enables predicting nonlinear
quantum properties, such as purity, quantum virtual cooling, and R\'{e}nyi-$2$
correlation.

</details>


### [143] [Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates](https://arxiv.org/abs/2509.08933)
*Sreejeet Maity,Aritra Mitra*

Main category: cs.LG

TL;DR: 本文提出了一种针对对抗性奖励腐蚀的鲁棒Q学习算法，首次为异步Q学习提供了有限时间鲁棒性保证。


<details>
  <summary>Details</summary>
Motivation: 在折扣、无限视野的强化学习（RL）环境中，奖励信号可能受到对抗性腐蚀（例如极端噪声、传感器故障或恶意攻击），这会严重降低传统Q学习等算法的性能。

Method: 提出了一种可证明鲁棒的Q学习算法新变体，即使在部分观测奖励被对抗性任意扰动的情况下也能有效运行。此外，还提出了一个不需要真实奖励分布统计信息先验知识的算法变体，并通过改进的Azuma-Hoeffding不等式进行分析。

Result: 在异步采样模型下，尽管存在对抗性腐蚀，所提出算法的有限时间收敛速率与非对抗性情况下的现有结果相匹配，仅存在一个与腐蚀样本比例成比例的附加项。同时，通过信息论下界揭示了该附加腐蚀项的不可避免性。

Conclusion: 本研究的贡献首次为异步Q学习提供了有限时间鲁棒性保证，弥补了鲁棒强化学习领域的一个重要空白。

Abstract: We consider the problem of learning the optimal policy in a discounted,
infinite-horizon reinforcement learning (RL) setting where the reward signal is
subject to adversarial corruption. Such corruption, which may arise from
extreme noise, sensor faults, or malicious attacks, can severely degrade the
performance of classical algorithms such as Q-learning. To address this
challenge, we propose a new provably robust variant of the Q-learning algorithm
that operates effectively even when a fraction of the observed rewards are
arbitrarily perturbed by an adversary. Under the asynchronous sampling model
with time-correlated data, we establish that despite adversarial corruption,
the finite-time convergence rate of our algorithm matches that of existing
results for the non-adversarial case, up to an additive term proportional to
the fraction of corrupted samples. Moreover, we derive an information-theoretic
lower bound revealing that the additive corruption term in our upper bounds is
unavoidable.
  Next, we propose a variant of our algorithm that requires no prior knowledge
of the statistics of the true reward distributions. The analysis of this
setting is particularly challenging and is enabled by carefully exploiting a
refined Azuma-Hoeffding inequality for almost-martingales, a technical tool
that might be of independent interest. Collectively, our contributions provide
the first finite-time robustness guarantees for asynchronous Q-learning,
bridging a significant gap in robust RL.

</details>


### [144] [Group Distributionally Robust Machine Learning under Group Level Distributional Uncertainty](https://arxiv.org/abs/2509.08942)
*Xenia Konti,Yi Shen,Zifan Wang,Karl Henrik Johansson,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: 提出一种基于 Wasserstein 分布式鲁棒优化 (DRO) 的新框架，以解决多源异构数据中机器学习模型在弱势群体上的性能下降问题，同时考虑组内分布不确定性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型性能严重依赖训练数据质量和代表性。在多源异构数据应用中，标准方法常学习到虚假相关性，导致对非典型或弱势群体的性能下降。现有方法通过优化最差组性能来解决，但通常假设各组数据分布可准确估计，这在真实复杂环境中往往不成立。

Method: 提出一种新的框架，利用 Wasserstein-based 分布式鲁棒优化 (DRO) 来解决问题，旨在在提升最差组性能的同时，考虑各组内部的分布不确定性。为求解此 DRO 问题，开发了一种梯度下降-上升算法，并提供了收敛性证明。

Result: 在真实世界数据上验证了所提方法的有效性。

Conclusion: 通过引入 Wasserstein-based DRO 并开发相应的优化算法，成功解决了现有最差组优化方法在面对组内分布不确定性时的局限性，从而提升了机器学习模型在多源异构数据中对所有群体的鲁棒性能。

Abstract: The performance of machine learning (ML) models critically depends on the
quality and representativeness of the training data. In applications with
multiple heterogeneous data generating sources, standard ML methods often learn
spurious correlations that perform well on average but degrade performance for
atypical or underrepresented groups. Prior work addresses this issue by
optimizing the worst-group performance. However, these approaches typically
assume that the underlying data distributions for each group can be accurately
estimated using the training data, a condition that is frequently violated in
noisy, non-stationary, and evolving environments. In this work, we propose a
novel framework that relies on Wasserstein-based distributionally robust
optimization (DRO) to account for the distributional uncertainty within each
group, while simultaneously preserving the objective of improving the
worst-group performance. We develop a gradient descent-ascent algorithm to
solve the proposed DRO problem and provide convergence results. Finally, we
validate the effectiveness of our method on real-world data.

</details>


### [145] [FoundationalECGNet: A Lightweight Foundational Model for ECG-based Multitask Cardiac Analysis](https://arxiv.org/abs/2509.08961)
*Md. Sajeebul Islam Sk.,Md Jobayer,Md Mehedi Hasan Shawon,Md. Golam Raibul Alam*

Main category: cs.LG

TL;DR: FoundationalECGNet是一个用于心电图（ECG）自动分类的深度学习框架，它通过创新的去噪和多模型集成，解决了现有方法的挑战，并在正常/异常分类和五种心血管疾病的检测中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要的死亡原因，而心电图分析是诊断心脏异常的关键。当前的心电图分析方法面临噪声、类别不平衡和数据集异质性等挑战，限制了诊断的准确性和可扩展性。

Method: 本文提出了FoundationalECGNet框架，整合了Morlet和Daubechies小波变换进行双阶段去噪、卷积块注意力模块（CBAM）、图注意力网络（GAT）和时间序列Transformer（TST），以联合捕捉多通道ECG信号中的空间和时间依赖性。该模型首先区分正常和异常ECG信号，然后将异常信号进一步分类为心律失常、传导障碍、心肌梗死、QT异常或肥厚五种心脏状况。

Result: 在多个数据集上，FoundationalECGNet在正常与异常分类中达到了99%的F1分数。在多类别疾病检测中也表现出最先进的性能，其中传导障碍和肥厚的F1分数达到99%，心律失常的F1分数达到98.9%。此外，该模型还能提供风险等级评估以辅助临床决策。

Conclusion: FoundationalECGNet为自动化ECG分析提供了一个可扩展、可解释且具有泛化能力的解决方案，有望提高医疗诊断的精确性和改善患者预后。

Abstract: Cardiovascular diseases (CVDs) remain a leading cause of mortality worldwide,
underscoring the importance of accurate and scalable diagnostic systems.
Electrocardiogram (ECG) analysis is central to detecting cardiac abnormalities,
yet challenges such as noise, class imbalance, and dataset heterogeneity limit
current methods. To address these issues, we propose FoundationalECGNet, a
foundational framework for automated ECG classification. The model integrates a
dual-stage denoising by Morlet and Daubechies wavelets transformation,
Convolutional Block Attention Module (CBAM), Graph Attention Networks (GAT),
and Time Series Transformers (TST) to jointly capture spatial and temporal
dependencies in multi-channel ECG signals. FoundationalECGNet first
distinguishes between Normal and Abnormal ECG signals, and then classifies the
Abnormal signals into one of five cardiac conditions: Arrhythmias, Conduction
Disorders, Myocardial Infarction, QT Abnormalities, or Hypertrophy. Across
multiple datasets, the model achieves a 99% F1-score for Normal vs. Abnormal
classification and shows state-of-the-art performance in multi-class disease
detection, including a 99% F1-score for Conduction Disorders and Hypertrophy,
as well as a 98.9% F1-score for Arrhythmias. Additionally, the model provides
risk level estimations to facilitate clinical decision-making. In conclusion,
FoundationalECGNet represents a scalable, interpretable, and generalizable
solution for automated ECG analysis, with the potential to improve diagnostic
precision and patient outcomes in healthcare settings. We'll share the code
after acceptance.

</details>


### [146] [Value bounds and Convergence Analysis for Averages of LRP attributions](https://arxiv.org/abs/2509.08963)
*Alexander Binder,Nastaran Takmil-Homayouni,Urun Dogan*

Main category: cs.LG

TL;DR: 通过将LRP类归因方法表示为修正梯度矩阵的乘积，我们分析了其数值特性。推导了奇异值和归因图分量界限，进而获得了控制归因经验均值收敛的乘法常数。研究发现LRP-beta的常数独立于权重范数，这与梯度方法和LRP-epsilon形成显著区别。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在深入理解LRP类归因方法的数值特性及其归因值分布。

Method: 将LRP类归因方法表示为修正梯度矩阵的乘积，并利用其与Jacobi矩阵乘法的类比。在此基础上，推导了奇异值的上界和归因图值的逐分量界限。

Result: 我们推导了奇异值的上界和归因图值的逐分量界限。主要结果是获得了乘法常数，这些常数控制着归因经验均值到归因图期望的收敛。此外，分析揭示LRP-beta的常数独立于权重范数，这与基于梯度的方法和LRP-epsilon存在显著区别。

Conclusion: 这些发现对于应用多个非几何数据增强的场景以及Smoothgrad类归因方法具有重要意义。特别地，LRP-beta常数独立于权重范数的特性是一个关键优势。

Abstract: We analyze numerical properties of Layer-wise relevance propagation
(LRP)-type attribution methods by representing them as a product of modified
gradient matrices. This representation creates an analogy to matrix
multiplications of Jacobi-matrices which arise from the chain rule of
differentiation. In order to shed light on the distribution of attribution
values, we derive upper bounds for singular values. Furthermore we derive
component-wise bounds for attribution map values. As a main result, we apply
these component-wise bounds to obtain multiplicative constants. These constants
govern the convergence of empirical means of attributions to expectations of
attribution maps. This finding has important implications for scenarios where
multiple non-geometric data augmentations are applied to individual test
samples, as well as for Smoothgrad-type attribution methods. In particular, our
analysis reveals that the constants for LRP-beta remain independent of weight
norms, a significant distinction from both gradient-based methods and
LRP-epsilon.

</details>


### [147] [Green Federated Learning via Carbon-Aware Client and Time Slot Scheduling](https://arxiv.org/abs/2509.08980)
*Daniel Richards Arputharaj,Charlotte Rodriguez,Angelo Rodio,Giovanni Neglia*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Training large-scale machine learning models incurs substantial carbon
emissions. Federated Learning (FL), by distributing computation across
geographically dispersed clients, offers a natural framework to leverage
regional and temporal variations in Carbon Intensity (CI). This paper
investigates how to reduce emissions in FL through carbon-aware client
selection and training scheduling. We first quantify the emission savings of a
carbon-aware scheduling policy that leverages slack time -- permitting a modest
extension of the training duration so that clients can defer local training
rounds to lower-carbon periods. We then examine the performance trade-offs of
such scheduling which stem from statistical heterogeneity among clients,
selection bias in participation, and temporal correlation in model updates. To
leverage these trade-offs, we construct a carbon-aware scheduler that
integrates slack time, $\alpha$-fair carbon allocation, and a global
fine-tuning phase. Experiments on real-world CI data show that our scheduler
outperforms slack-agnostic baselines, achieving higher model accuracy across a
wide range of carbon budgets, with especially strong gains under tight carbon
constraints.

</details>


### [148] [Active Learning and Explainable AI for Multi-Objective Optimization of Spin Coated Polymers](https://arxiv.org/abs/2509.08988)
*Brendan Young,Brendan Alvey,Andreas Werbrouck,Will Murphy,James Keller,Mattias J. Young,Matthew Maschmann*

Main category: cs.LG

TL;DR: 本文提出了一个结合PyePAL、可视化和可解释AI的框架，用于优化旋涂聚合物薄膜的机械性能（硬度和弹性），并能提供可解释的优化结果。


<details>
  <summary>Details</summary>
Motivation: 旋涂聚合物薄膜以实现特定机械性能是一个固有的多目标优化问题，需要一个高效且可解释的优化方法。

Method: 该框架集成了主动Pareto前沿学习算法PyePAL（使用高斯过程模型预测目标值），并结合UMAP进行Pareto前沿探索的可视化，以及模糊语言摘要技术将参数与性能关系转化为语言描述，以增强可解释性。

Result: 实验结果表明，该方法能高效识别有前景的聚合物设计，并且通过可视化和语言解释促进了专家驱动的分析和知识发现。

Conclusion: 所提出的方法成功地实现了旋涂聚合物薄膜的性能优化，并通过结合视觉和语言解释，显著提升了优化过程的效率、可理解性和知识发现能力。

Abstract: Spin coating polymer thin films to achieve specific mechanical properties is
inherently a multi-objective optimization problem. We present a framework that
integrates an active Pareto front learning algorithm (PyePAL) with
visualization and explainable AI techniques to optimize processing parameters.
PyePAL uses Gaussian process models to predict objective values (hardness and
elasticity) from the design variables (spin speed, dilution, and polymer
mixture), guiding the adaptive selection of samples toward promising regions of
the design space. To enable interpretable insights into the high-dimensional
design space, we utilize UMAP (Uniform Manifold Approximation and Projection)
for two-dimensional visualization of the Pareto front exploration.
Additionally, we incorporate fuzzy linguistic summaries, which translate the
learned relationships between process parameters and performance objectives
into linguistic statements, thus enhancing the explainability and understanding
of the optimization results. Experimental results demonstrate that our method
efficiently identifies promising polymer designs, while the visual and
linguistic explanations facilitate expert-driven analysis and knowledge
discovery.

</details>


### [149] [Fast attention mechanisms: a tale of parallelism](https://arxiv.org/abs/2509.09001)
*Jingwen Liu,Hantao Yu,Clayton Sanford,Alexandr Andoni,Daniel Hsu*

Main category: cs.LG

TL;DR: 本文提出了一种名为近似最近邻注意力（ANNA）的高效注意力机制，以解决Transformer二次时间复杂度导致的扩展性问题，并证明了其在保持表达能力和统一理解高效注意力近似方面的优势。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽然具有模拟大规模并行计算（MPC）算法的表达能力，但其二次时间复杂度严重限制了其可扩展性。

Method: 引入了一种名为近似最近邻注意力（ANNA）的高效注意力机制，该机制具有亚二次时间复杂度。

Result: 研究结果表明：1) ANNA-transformers保留了标准注意力在匹配MPC算法能力方面的表达能力；2) ANNA-transformers能以接近最优的深度解决Match2和k-hop等关键推理任务；3) 恒定深度的ANNA-transformers可以模拟恒定深度的低秩transformers。

Conclusion: ANNA-transformers提供了一种亚二次时间复杂度的高效注意力机制，在保持强大表达能力的同时，为理解广泛的高效注意力近似提供了一种统一的推理方式。

Abstract: Transformers have the representational capacity to simulate Massively
Parallel Computation (MPC) algorithms, but they suffer from quadratic time
complexity, which severely limits their scalability. We introduce an efficient
attention mechanism called Approximate Nearest Neighbor Attention (ANNA) with
sub-quadratic time complexity. We prove that ANNA-transformers (1) retain the
expressive power previously established for standard attention in terms of
matching the capabilities of MPC algorithms, and (2) can solve key reasoning
tasks such as Match2 and $k$-hop with near-optimal depth. Using the MPC
framework, we further prove that constant-depth ANNA-transformers can simulate
constant-depth low-rank transformers, thereby providing a unified way to reason
about a broad class of efficient attention approximations.

</details>


### [150] [Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison](https://arxiv.org/abs/2509.09009)
*Marianna Nezhurina,Taishi Nakamura,Timur Carstensen,Niccolò Ajroldi,Ville Komulainen,David Salinas,Jenia Jitsev*

Main category: cs.LG

TL;DR: 本文介绍了open-sci-ref，一个在8个开放参考数据集上训练的密集型Transformer模型家族（0.13B至1.7B参数，高达1T token），作为研究基线，以评估和比较不同训练方法及数据集的性能。研究发现，NemoTron-CC HQ数据集表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究动机是建立一系列在不同模型和token规模下的密集型Transformer模型研究基线，从而为研究人员提供参考点，以评估和比较其他训练方法的合理性、质量和扩展趋势，并研究训练动态。

Method: 引入了open-sci-ref，一个密集型Transformer模型家族，在8个近期开放参考数据集上进行训练，涵盖了0.13B到1.7B的参数规模和高达1T的token规模。通过在各种标准化基准上评估模型，建立了训练运行参考点。同时发布了中间检查点、日志、代码和下游评估结果，以简化复现、标准化比较并促进未来研究。

Result: 1. 建立了跨规模和数据集的密集型Transformer模型研究基线和参考点，便于研究人员评估替代训练方法。2. 比较发现，在NemoTron-CC HQ数据集上训练的模型表现始终优于其他参考数据集，其次是DCLM-baseline和FineWeb-Edu。3. 提供了中间检查点，以供研究训练动态。

Conclusion: open-sci-ref基线及其配套的中间检查点、日志和代码，为Transformer模型训练提供了一个标准化的评估和比较框架。这些基线不仅有助于研究人员评估新的训练方法，比较数据集性能，特别是指出了NemoTron-CC HQ数据集的优越性，还简化了结果的复现和未来的研究。

Abstract: We introduce open-sci-ref, a family of dense transformer models trained as
research baselines across multiple model (0.13B to 1.7B parameters) and token
scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on
various standardized benchmarks, our training runs set establishes reference
points that enable researchers to assess the sanity and quality of alternative
training approaches across scales and datasets. Intermediate checkpoints allow
comparison and studying of the training dynamics. The established reference
baselines allow training procedures to be compared through their scaling
trends, aligning them on a common compute axis. Comparison of open reference
datasets reveals that training on NemoTron-CC HQ consistently outperforms other
reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to
intermediate training checkpoints, the release includes logs, code, and
downstream evaluations to simplify reproduction, standardize comparison, and
facilitate future research.

</details>


### [151] [Deep Context-Conditioned Anomaly Detection for Tabular Data](https://arxiv.org/abs/2509.09030)
*Spencer King,Zhilu Zhang,Ruofan Yu,Baris Coskun,Wei Ding,Qian Cui*

Main category: cs.LG

TL;DR: 针对大规模表格数据的无监督异常检测，现有方法忽略异构上下文导致性能不佳。本文提出一个上下文条件异常检测框架，通过深度自编码器自动识别并建模条件分布，在基准数据集上超越了最先进方法。


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测在网络安全和金融等领域至关重要，但极具挑战。现有深度学习方法在建模联合分布时，忽略了真实表格数据中存在的异构上下文（如不同用户），导致全局稀有事件在特定上下文中可能正常，从而降低检测性能。

Method: 本文提出了一个针对表格数据集的上下文条件异常检测框架。该方法能够自动识别上下文特征，并利用一个简单的深度自编码器来建模条件数据分布。

Result: 在多个表格基准数据集上进行的广泛实验表明，该方法优于最先进的方法。

Conclusion: 该研究强调了在准确区分异常与正常实例时上下文的重要性，并证明了所提方法通过考虑上下文能够显著提高异常检测性能。

Abstract: Anomaly detection is critical in domains such as cybersecurity and finance,
especially when working with large-scale tabular data. Yet, unsupervised
anomaly detection -- where no labeled anomalies are available -- remains a
significant challenge. Although various deep learning methods have been
proposed to model a dataset's joint distribution, real-world tabular data often
contain heterogeneous contexts (e.g., different users), making globally rare
events normal under certain contexts. Consequently, relying on a single global
distribution can overlook these contextual nuances, degrading detection
performance. In this paper, we present a context-conditional anomaly detection
framework tailored for tabular datasets. Our approach automatically identifies
context features and models the conditional data distribution using a simple
deep autoencoder. Extensive experiments on multiple tabular benchmark datasets
demonstrate that our method outperforms state-of-the-art approaches,
underscoring the importance of context in accurately distinguishing anomalous
from normal instances.

</details>


### [152] [MoWE : A Mixture of Weather Experts](https://arxiv.org/abs/2509.09052)
*Dibyajyoti Chakraborty,Romit Maulik,Peter Harrington,Dallas Foster,Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 该论文提出了一种专家混合（MoWE）方法，通过优化组合现有数据驱动天气模型的输出来克服性能瓶颈，实现更准确、计算效率更高的天气预报。


<details>
  <summary>Details</summary>
Motivation: 尽管数据驱动天气模型已达到先进水平，但近年来其进展停滞不前，需要一种新方法来突破现有模型的性能瓶颈。

Method: 提出专家混合（MoWE）方法，不创建新的预报器，而是通过一个基于Vision Transformer的门控网络，根据预报提前期，动态学习并优化组合多个“专家”模型的输出，以生成合成的确定性预报。该方法训练所需的计算资源显著低于单个专家模型。

Result: 在2天预报时效内，RMSE比表现最佳的AI天气模型降低高达10%，显著优于任何单个专家模型以及专家模型的简单平均。

Conclusion: 该工作提供了一种计算高效且可扩展的策略，通过充分利用领先的高质量预报模型，推动数据驱动天气预报领域达到新的先进水平。

Abstract: Data-driven weather models have recently achieved state-of-the-art
performance, yet progress has plateaued in recent years. This paper introduces
a Mixture of Experts (MoWE) approach as a novel paradigm to overcome these
limitations, not by creating a new forecaster, but by optimally combining the
outputs of existing models. The MoWE model is trained with significantly lower
computational resources than the individual experts. Our model employs a Vision
Transformer-based gating network that dynamically learns to weight the
contributions of multiple "expert" models at each grid point, conditioned on
forecast lead time. This approach creates a synthesized deterministic forecast
that is more accurate than any individual component in terms of Root Mean
Squared Error (RMSE). Our results demonstrate the effectiveness of this method,
achieving up to a 10% lower RMSE than the best-performing AI weather model on a
2-day forecast horizon, significantly outperforming individual experts as well
as a simple average across experts. This work presents a computationally
efficient and scalable strategy to push the state of the art in data-driven
weather prediction by making the most out of leading high-quality forecast
models.

</details>


### [153] [A Scoping Review of Machine Learning Applications in Power System Protection and Disturbance Management](https://arxiv.org/abs/2509.09053)
*Julian Oelhaf,Georg Kordowich,Mehran Pashaei,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 本综述评估了机器学习在电力系统保护中的应用，发现其在模拟数据上表现良好但缺乏实际验证，并提出了标准化研究的建议，以促进其实际部署。


<details>
  <summary>Details</summary>
Motivation: 可再生和分布式能源的整合重塑了现代电力系统，对传统保护方案提出了挑战，需要创新的方法来应对。

Method: 采用PRISMA审查框架，对100多篇关于机器学习在电力系统保护和扰动管理中应用的文献进行了范围界定综述。旨在评估ML研究范围、性能以及适用于不断演变电网条件的方法。

Result: 机器学习模型在模拟数据集上常表现出高精度，但在实际条件下的性能验证不足。现有文献分散，方法论严谨性、数据集质量和评估指标不一致，限制了结果的可比性和通用性。本综述提出了ML导向的保护任务分类法，统一了关键术语，并倡导标准化报告实践。

Conclusion: 当前研究存在关键空白，包括实际验证稀缺、鲁棒性测试不足和部署可行性考虑有限。未来研究应优先关注公共基准数据集、实际验证方法和先进ML架构，以推动机器学习保护从理论走向实际应用。

Abstract: The integration of renewable and distributed energy resources reshapes modern
power systems, challenging conventional protection schemes. This scoping review
synthesizes recent literature on machine learning (ML) applications in power
system protection and disturbance management, following the PRISMA for Scoping
Reviews framework. Based on over 100 publications, three key objectives are
addressed: (i) assessing the scope of ML research in protection tasks; (ii)
evaluating ML performance across diverse operational scenarios; and (iii)
identifying methods suitable for evolving grid conditions. ML models often
demonstrate high accuracy on simulated datasets; however, their performance
under real-world conditions remains insufficiently validated. The existing
literature is fragmented, with inconsistencies in methodological rigor, dataset
quality, and evaluation metrics. This lack of standardization hampers the
comparability of results and limits the generalizability of findings. To
address these challenges, this review introduces a ML-oriented taxonomy for
protection tasks, resolves key terminological inconsistencies, and advocates
for standardized reporting practices. It further provides guidelines for
comprehensive dataset documentation, methodological transparency, and
consistent evaluation protocols, aiming to improve reproducibility and enhance
the practical relevance of research outcomes. Critical gaps remain, including
the scarcity of real-world validation, insufficient robustness testing, and
limited consideration of deployment feasibility. Future research should
prioritize public benchmark datasets, realistic validation methods, and
advanced ML architectures. These steps are essential to move ML-based
protection from theoretical promise to practical deployment in increasingly
dynamic and decentralized power systems.

</details>


### [154] [STRIDE: Scalable and Interpretable XAI via Subset-Free Functional Decomposition](https://arxiv.org/abs/2509.09070)
*Chaeyun Ko*

Main category: cs.LG

TL;DR: STRIDE是一个可扩展的XAI框架，通过在RKHS中进行正交函数分解，解决现有框架在计算成本和表达能力方面的限制，提供功能组件而非单一标量归因，并在保持高保真度的同时，在多数表格数据集上实现了显著的加速。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI (XAI) 框架存在两大局限：对特征子集推理的指数级计算成本；以及将效果总结为单一标量值，导致表达能力受限。

Method: STRIDE框架将解释定义为在再生核希尔伯特空间 (RKHS) 中的无子集枚举、正交函数分解。它通过基于递归核中心化过程的分析投影方案计算功能组件 f_S(x_S)，避免了显式子集枚举。该方法模型无关，提供局部和全局视图，并有正交性和L^2收敛的理论支持。

Result: 在公共表格基准测试中，STRIDE的速度提升范围从0.6倍到9.7倍，中位数约为3.0倍（跨10个数据集），同时保持了高保真度 (R^2在0.81到0.999之间) 和显著的秩一致性。它通过提供结构化的功能视角，补充了标量归因方法，并能实现“组件手术”等新型诊断工具。

Conclusion: STRIDE通过提供结构化的功能视角，有效地解决了传统XAI框架的计算成本和表达能力问题。它在效率和功能性上均表现出色，并为量化特定交互影响提供了新的诊断能力，是对现有标量归因方法的有力补充。

Abstract: Most explainable AI (XAI) frameworks face two practical limitations: the
exponential cost of reasoning over feature subsets and the reduced
expressiveness of summarizing effects as single scalar values. We present
STRIDE, a scalable framework that aims to mitigate both issues by framing
explanation as a subset-enumeration-free, orthogonal functional decomposition
in a Reproducing Kernel Hilbert Space (RKHS). Rather than focusing only on
scalar attributions, STRIDE computes functional components f_S(x_S) via an
analytical projection scheme based on a recursive kernel-centering procedure,
avoiding explicit subset enumeration. In the tabular setups we study, the
approach is model-agnostic, provides both local and global views, and is
supported by theoretical results on orthogonality and L^2 convergence under
stated assumptions. On public tabular benchmarks in our environment, we
observed speedups ranging from 0.6 times (slower than TreeSHAP on a small
dataset) to 9.7 times (California), with a median approximate 3.0 times across
10 datasets, while maintaining high fidelity (R^2 between 0.81 and 0.999) and
substantial rank agreement on most datasets. Overall, STRIDE complements scalar
attribution methods by offering a structured functional perspective, enabling
novel diagnostics like 'component surgery' to quantitatively measure the impact
of specific interactions within our experimental scope.

</details>


### [155] ["A 6 or a 9?": Ensemble Learning Through the Multiplicity of Performant Models and Explanations](https://arxiv.org/abs/2509.09073)
*Gianlucca Zuin,Adriano Veloso*

Main category: cs.LG

TL;DR: 本文提出“罗生门集成”方法，通过策略性选择“罗生门效应”中多样化的高性能模型，以构建更具泛化性和鲁棒性的集成模型，并在真实世界数据集中取得了显著的性能提升和业务效益。


<details>
  <summary>Details</summary>
Motivation: 机器学习中选择泛化能力强的模型具有挑战性。现实世界中常出现“罗生门效应”，即多个模型表现相似，但如何有效利用这些多样的高性能模型来提升泛化性是一个未解决的问题。

Method: 提出“罗生门集成”（Rashomon Ensemble）方法。该方法根据模型的性能和解释性对模型进行分组，从中策略性地选择模型，构建集成。通过最大化集成模型的多样性，同时保持预测准确性，确保每个模型覆盖解决方案空间的不同区域，从而增强集成模型对分布偏移和未见数据的鲁棒性。

Result: 在开放和专有的真实世界数据集上验证了该方法，在罗生门比例较大的场景下，AUROC指标提升了0.20+。此外，该方法在各种实际业务应用中展示了其鲁棒性、实用性和有效性，为企业带来了切实的效益。

Conclusion: 罗生门集成方法通过从多样化的高性能模型中进行策略性选择，有效提升了集成模型的泛化能力和鲁棒性，并在真实世界应用中展现出显著的性能优势和商业价值。

Abstract: Creating models from past observations and ensuring their effectiveness on
new data is the essence of machine learning. However, selecting models that
generalize well remains a challenging task. Related to this topic, the Rashomon
Effect refers to cases where multiple models perform similarly well for a given
learning problem. This often occurs in real-world scenarios, like the
manufacturing process or medical diagnosis, where diverse patterns in data lead
to multiple high-performing solutions. We propose the Rashomon Ensemble, a
method that strategically selects models from these diverse high-performing
solutions to improve generalization. By grouping models based on both their
performance and explanations, we construct ensembles that maximize diversity
while maintaining predictive accuracy. This selection ensures that each model
covers a distinct region of the solution space, making the ensemble more robust
to distribution shifts and variations in unseen data. We validate our approach
on both open and proprietary collaborative real-world datasets, demonstrating
up to 0.20+ AUROC improvements in scenarios where the Rashomon ratio is large.
Additionally, we demonstrate tangible benefits for businesses in various
real-world applications, highlighting the robustness, practicality, and
effectiveness of our approach.

</details>


### [156] [An entropy formula for the Deep Linear Network](https://arxiv.org/abs/2509.09088)
*Govind Menon,Tianmin Yu*

Main category: cs.LG

TL;DR: 本研究通过黎曼几何分析深度线性网络（DLN），为学习过程的热力学描述奠定基础。利用群作用和黎曼淹没，定义并计算了玻尔兹曼熵，并证明了可观测空间上的黎曼几何。


<details>
  <summary>Details</summary>
Motivation: 为深度线性网络（DLN）的学习过程提供热力学描述的几何基础。

Method: 主要方法包括：使用群作用分析过参数化；利用从参数空间到可观测空间的黎曼淹没；利用参数空间中平衡流形被群轨道叶状化来定义和计算玻尔兹曼熵；通过雅可比矩阵理论显式构造平衡流形切空间的标准正交基。

Result: 1. 定义并计算了玻尔兹曼熵。2. 证明了在参考文献[2]中定义的可观测空间上的黎曼几何是通过平衡流形的黎曼淹没获得的。

Conclusion: 该研究通过黎曼几何为深度线性网络的学习过程提供了热力学描述的理论基础，并开发了计算玻尔兹曼熵及理解可观测空间几何的新方法。

Abstract: We study the Riemannian geometry of the Deep Linear Network (DLN) as a
foundation for a thermodynamic description of the learning process. The main
tools are the use of group actions to analyze overparametrization and the use
of Riemannian submersion from the space of parameters to the space of
observables. The foliation of the balanced manifold in the parameter space by
group orbits is used to define and compute a Boltzmann entropy. We also show
that the Riemannian geometry on the space of observables defined in [2] is
obtained by Riemannian submersion of the balanced manifold. The main technical
step is an explicit construction of an orthonormal basis for the tangent space
of the balanced manifold using the theory of Jacobi matrices.

</details>


### [157] [Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models](https://arxiv.org/abs/2509.09119)
*Hao Zhang,Bo Huang,Zhenjia Li,Xi Xiao,Hui Yi Leong,Zumeng Zhang,Xinwei Long,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: 提出Sensitivity-LoRA，利用损失函数二阶导数动态分配低秩，解决LoRA统一秩分配的局限性，实现LLM高效稳定微调。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在适应特定任务时面临挑战，尤其是在资源受限环境中。LoRA方法受限于其统一的秩（r）分配，而现有秩分配技术则存在计算效率低、复杂且不稳定的问题，阻碍了实际应用。

Method: 提出Sensitivity-LoRA，一种高效微调方法，根据权重矩阵的全局和局部敏感性动态分配秩。该方法利用损失函数的二阶导数（Hessian矩阵）有效捕获权重敏感性，以最小的计算开销实现最优秩分配。

Result: 实验结果表明，Sensitivity-LoRA在不同任务和基准测试中均展现出强大的有效性、效率和稳定性。

Conclusion: Sensitivity-LoRA通过动态秩分配和利用Hessian矩阵，有效克服了LoRA的统一秩分配限制及现有秩分配方法的不足，为LLM微调提供了一种高效、稳定、可靠的解决方案。

Abstract: Large Language Models (LLMs) have transformed both everyday life and
scientific research. However, adapting LLMs from general-purpose models to
specialized tasks remains challenging, particularly in resource-constrained
environments. Low-Rank Adaptation (LoRA), a prominent method within
Parameter-Efficient Fine-Tuning (PEFT), has emerged as a promising approach to
LLMs by approximating model weight updates using low-rank decomposition.
However, LoRA is limited by its uniform rank ( r ) allocation to each
incremental matrix, and existing rank allocation techniques aimed at addressing
this issue remain computationally inefficient, complex, and unstable, hindering
practical applications. To address these limitations, we propose
Sensitivity-LoRA, an efficient fine-tuning method that dynamically allocates
ranks to weight matrices based on both their global and local sensitivities. It
leverages the second-order derivatives (Hessian Matrix) of the loss function to
effectively capture weight sensitivity, enabling optimal rank allocation with
minimal computational overhead. Our experimental results have demonstrated
robust effectiveness, efficiency and stability of Sensitivity-LoRA across
diverse tasks and benchmarks.

</details>


### [158] [Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction](https://arxiv.org/abs/2509.09128)
*Emam Hossain,Md Osman Gani*

Main category: cs.LG

TL;DR: 本研究提出一个因果感知深度学习框架，通过整合MVGC和PCMCI+进行因果特征选择，显著提高了北极海冰范围预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习和深度学习模型依赖相关性学习，难以区分真正的因果关系与虚假关联，从而限制了模型的鲁棒性、可解释性和泛化能力。

Method: 引入了一个因果感知深度学习框架，该框架在一个混合神经网络架构中集成了多元格兰杰因果关系（MVGC）和PCMCI+进行因果特征选择。该方法利用43年（1979-2021）的北极海冰范围（SIE）数据及相关海洋-大气变量。

Result: 实验结果表明，结合因果输入显著提高了不同预报时长的预测准确性和可解释性。该方法成功识别了具有因果影响的预测因子，优先处理了SIE动态的直接原因，并减少了不必要的特征，提升了计算效率。

Conclusion: 该框架不仅在北极海冰预测中得到了验证，还可广泛应用于其他动态、高维领域，为因果信息预测建模的理论基础和实际性能提供了可扩展的改进方法。

Abstract: Conventional machine learning and deep learning models typically rely on
correlation-based learning, which often fails to distinguish genuine causal
relationships from spurious associations, limiting their robustness,
interpretability, and ability to generalize. To overcome these limitations, we
introduce a causality-aware deep learning framework that integrates
Multivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection
within a hybrid neural architecture. Leveraging 43 years (1979-2021) of Arctic
Sea Ice Extent (SIE) data and associated ocean-atmospheric variables at daily
and monthly resolutions, the proposed method identifies causally influential
predictors, prioritizes direct causes of SIE dynamics, reduces unnecessary
features, and enhances computational efficiency. Experimental results show that
incorporating causal inputs leads to improved prediction accuracy and
interpretability across varying lead times. While demonstrated on Arctic SIE
forecasting, the framework is broadly applicable to other dynamic,
high-dimensional domains, offering a scalable approach that advances both the
theoretical foundations and practical performance of causality-informed
predictive modeling.

</details>


### [159] [Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.09135)
*Xuefeng Wang,Lei Zhang,Henglin Pu,Ahmed H. Qureshi,Husheng Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息神经网络（PINNs）和价值梯度迭代（VGI）模块的连续时间多智能体强化学习（CT-MARL）框架，以解决现有连续时间RL方法在多智能体复杂动态系统中的维度灾难和价值函数近似挑战。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以处理需要高频或不规则时间间隔交互的复杂动态系统。连续时间强化学习（CTRL）虽然有前景，但受限于单智能体领域，主要原因包括：哈密顿-雅可比-贝尔曼（HJB）方程的传统解法存在维度灾难，以及在多智能体设置中难以准确近似集中式价值函数，导致策略训练不稳定。

Method: 我们提出一个CT-MARL框架，利用物理信息神经网络（PINNs）来大规模近似基于HJB的价值函数。为确保价值与微分结构的一致性，引入价值梯度迭代（VGI）模块，通过沿轨迹迭代优化价值梯度，以提高梯度保真度，从而获得更准确的价值和更强大的策略学习。

Result: 我们的方法在多智能体粒子环境（MPE）和多智能体MuJoCo等标准基准的连续时间变体上进行了评估。结果表明，我们的方法持续优于现有连续时间RL基线，并能扩展到复杂的多种智能体动力学。

Conclusion: 我们提出的CT-MARL框架，结合PINNs和VGI，有效解决了连续时间强化学习在多智能体环境中的关键挑战，实现了在复杂多智能体动态系统中的高性能和可扩展性。

Abstract: Existing reinforcement learning (RL) methods struggle with complex dynamical
systems that demand interactions at high frequencies or irregular time
intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by
replacing discrete-time Bellman recursion with differential value functions
defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation.
While CTRL has shown promise, its applications have been largely limited to the
single-agent domain. This limitation stems from two key challenges: (i)
conventional solution methods for HJB equations suffer from the curse of
dimensionality (CoD), making them intractable in high-dimensional systems; and
(ii) even with HJB-based learning approaches, accurately approximating
centralized value functions in multi-agent settings remains difficult, which in
turn destabilizes policy training. In this paper, we propose a CT-MARL
framework that uses physics-informed neural networks (PINNs) to approximate
HJB-based value functions at scale. To ensure the value is consistent with its
differential structure, we align value learning with value-gradient learning by
introducing a Value Gradient Iteration (VGI) module that iteratively refines
value gradients along trajectories. This improves gradient fidelity, in turn
yielding more accurate values and stronger policy learning. We evaluate our
method using continuous-time variants of standard benchmarks, including
multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results
demonstrate that our approach consistently outperforms existing continuous-time
RL baselines and scales to complex multi-agent dynamics.

</details>


### [160] [Peering Partner Recommendation for ISPs using Machine Learning](https://arxiv.org/abs/2509.09146)
*Md Ibrahim Ibne Alam,Ankur Senapati,Anindo Mahmood,Murat Yuksel,Koushik Kar*

Main category: cs.LG

TL;DR: 本文开发了一个基于机器学习的模型，利用公开数据预测ISP之间是否应该建立对等互联关系，旨在自动化复杂的对等互联伙伴选择过程，提高全球互联网效率。


<details>
  <summary>Details</summary>
Motivation: ISP间的对等互联关系建立过程漫长且复杂，但能提供更多的优化空间。自动化这一选择过程对于提高全球互联网生态系统的效率至关重要。

Method: 研究人员首先从PeeringDB、CAIDA等公开数据库收集ISP数据。随后，评估了三种主要类型的机器学习模型（树形模型、神经网络模型和Transformer模型）在预测对等互联关系方面的性能。

Result: 实验结果表明，树形模型在准确性和效率上表现最佳。其中，使用公开数据训练的XGBoost模型在预测对等互联伙伴方面取得了98%的准确率，并展示了对时间、空间和数据缺失变化的良好弹性。

Conclusion: 该研究设想ISP可以采用其提出的方法来完全自动化对等互联伙伴选择过程，从而推动互联网生态系统向更高效、更优化的方向发展。

Abstract: Internet service providers (ISPs) need to connect with other ISPs to provide
global connectivity services to their users. To ensure global connectivity,
ISPs can either use transit service(s) or establish direct peering
relationships between themselves via Internet exchange points (IXPs). Peering
offers more room for ISP-specific optimizations and is preferred, but it often
involves a lengthy and complex process. Automating peering partner selection
can enhance efficiency in the global Internet ecosystem. We explore the use of
publicly available data on ISPs to develop a machine learning (ML) model that
can predict whether an ISP pair should peer or not. At first, we explore public
databases, e.g., PeeringDB, CAIDA, etc., to gather data on ISPs. Then, we
evaluate the performance of three broad types of ML models for predicting
peering relationships: tree-based, neural network-based, and transformer-based.
Among these, we observe that tree-based models achieve the highest accuracy and
efficiency in our experiments. The XGBoost model trained with publicly
available data showed promising performance, with a 98% accuracy rate in
predicting peering partners. In addition, the model demonstrated great
resilience to variations in time, space, and missing data. We envision that
ISPs can adopt our method to fully automate the peering partner selection
process, thus transitioning to a more efficient and optimized Internet
ecosystem.

</details>


### [161] [HISPASpoof: A New Dataset For Spanish Speech Forensics](https://arxiv.org/abs/2509.09155)
*Maria Risques,Kratika Bhagtani,Amit Kumar Singh Yadav,Edward J. Delp*

Main category: cs.LG

TL;DR: 本文介绍了HISPASpoof，首个大规模西班牙语合成语音检测和归因数据集，发现现有英语训练的检测器在西班牙语上表现不佳，而使用HISPASpoof训练能显著提升检测性能，为西班牙语语音取证提供了重要基准。


<details>
  <summary>Details</summary>
Motivation: 零样本语音克隆和文本转语音技术迅速发展，导致合成语音逼真度高，引发滥用担忧。尽管已有大量针对英语和中文的检测器，但全球超过6亿人使用的西班牙语在语音取证方面仍未得到充分代表。

Method: 我们构建了HISPASpoof数据集，其中包含来自六种口音的公共语料库的真实语音，以及由六种零样本TTS系统生成的合成语音。我们评估了五种代表性方法，测试了英语训练的检测器在西班牙语上的泛化能力，并评估了使用HISPASpoof训练后对西班牙语合成语音的检测和归因性能。

Result: 结果显示，在英语上训练的检测器无法很好地泛化到西班牙语。而在HISPASpoof上训练能显著提高西班牙语合成语音的检测性能。此外，HISPASpoof也支持对合成语音归因（即识别生成方法）的评估。

Conclusion: HISPASpoof数据集为推动西班牙语中可靠且包容的语音取证研究提供了一个重要的基准，填补了该领域的一个关键空白。

Abstract: Zero-shot Voice Cloning (VC) and Text-to-Speech (TTS) methods have advanced
rapidly, enabling the generation of highly realistic synthetic speech and
raising serious concerns about their misuse. While numerous detectors have been
developed for English and Chinese, Spanish-spoken by over 600 million people
worldwide-remains underrepresented in speech forensics. To address this gap, we
introduce HISPASpoof, the first large-scale Spanish dataset designed for
synthetic speech detection and attribution. It includes real speech from public
corpora across six accents and synthetic speech generated with six zero-shot
TTS systems. We evaluate five representative methods, showing that detectors
trained on English fail to generalize to Spanish, while training on HISPASpoof
substantially improves detection. We also evaluate synthetic speech attribution
performance on HISPASpoof, i.e., identifying the generation method of synthetic
speech. HISPASpoof thus provides a critical benchmark for advancing reliable
and inclusive speech forensics in Spanish.

</details>


### [162] [Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication](https://arxiv.org/abs/2509.09168)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 本文提出一种免训练的自适应令牌合并框架，用于解决大规模Transformer模型在6G边缘智能中语义通信的计算与传输资源限制问题，通过贝叶斯优化在保持高精度的同时显著降低运算量与延迟。


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer模型虽在语义通信中表现强大，但其巨大的计算需求成为资源受限的6G网络实际部署的主要障碍。

Method: 提出一个免训练的自适应令牌合并框架，用于预训练视觉Transformer，旨在联合减少推理时间及传输资源。将每层合并比例的选择表述为平衡精度与计算成本的多目标优化问题，并利用基于高斯过程的贝叶斯优化构建最优配置的帕累托前沿，以实现运行时对应用需求和信道条件的灵活适应。

Result: 实验证明，该方法在广泛的信噪比条件下持续优于其他基线，显著降低了浮点运算量并保持了有竞争力的精度。此外，适应性策略能根据信道质量调整合并积极性，有效实现延迟与语义保真度之间的按需权衡。

Conclusion: 这些发现为未来边缘智能系统中部署基于Transformer的语义通信建立了一种可扩展且高效的方法。

Abstract: Large-scale transformer models have emerged as a powerful tool for semantic
communication systems, enabling edge devices to extract rich representations
for robust inference across noisy wireless channels. However, their substantial
computational demands remain a major barrier to practical deployment in
resource-constrained 6G networks. In this paper, we present a training-free
framework for adaptive token merging in pretrained vision transformers to
jointly reduce inference time and transmission resource usage. We formulate the
selection of per-layer merging proportions as a multi-objective optimization
problem to balance accuracy and computational cost. We employ Gaussian
process-based Bayesian optimization to construct a Pareto frontier of optimal
configurations, enabling flexible runtime adaptation to dynamic application
requirements and channel conditions. Extensive experiments demonstrate that our
method consistently outperforms other baselines and achieves significant
reductions in floating-point operations while maintaining competitive accuracy
across a wide range of signal-to-noise ratio (SNR) conditions. Additional
results highlight the effectiveness of adaptive policies that adjust merging
aggressiveness in response to channel quality, providing a practical mechanism
to trade off latency and semantic fidelity on demand. These findings establish
a scalable and efficient approach for deploying transformer-based semantic
communication in future edge intelligence systems.

</details>


### [163] [Quantum Machine Learning, Quantitative Trading, Reinforcement Learning, Deep Learning](https://arxiv.org/abs/2509.09176)
*Jun-Hao Chen,Yu-Chien Huang,Yun-Cheng Tsai,Samuel Yen-Chi Chen*

Main category: cs.LG

TL;DR: 本研究利用量子长短期记忆网络（QLSTM）与量子异步优势Actor-Critic（QA3C）的混合模型，为USD/TWD设计了一个交易代理。该代理在五年内实现了11.87%的回报和0.92%的最大回撤，表现优于多种货币ETF，证明了混合模型在外汇交易中的竞争力。


<details>
  <summary>Details</summary>
Motivation: 探索量子启发式神经网络与深度强化学习在金融交易领域的融合应用潜力，以期为外汇交易带来新的解决方案。

Method: 实现了一个USD/TWD交易代理，该代理结合了用于短期趋势预测的量子长短期记忆网络（QLSTM）和量子增强型异步优势Actor-Critic（QA3C）。代理在2000年至2025年的数据上进行训练，并详细设计了状态特征、趋势跟踪/风险控制的奖励函数以及多核训练方法。

Result: 该长线交易代理在约5年内取得了11.87%的回报率和0.92%的最大回撤，表现优于多个货币ETF。研究结果表明，这种混合模型在外汇交易中具有竞争性性能。

Conclusion: 量子启发式神经网络与深度强化学习的混合模型能够在外汇交易中产生有竞争力的性能。特别是，QLSTM在小额盈利和严格风险控制的交易中显示出有效性，为未来的研究和模型增强提供了方向。

Abstract: The convergence of quantum-inspired neural networks and deep reinforcement
learning offers a promising avenue for financial trading. We implemented a
trading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM)
for short-term trend prediction with Quantum Asynchronous Advantage
Actor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained
on data from 2000-01-01 to 2025-04-30 (80\% training, 20\% testing), the
long-only agent achieves 11.87\% return over around 5 years with 0.92\% max
drawdown, outperforming several currency ETFs. We detail state design (QLSTM
features and indicators), reward function for trend-following/risk control, and
multi-core training. Results show hybrid models yield competitive FX trading
performance. Implications include QLSTM's effectiveness for small-profit trades
with tight risk and future enhancements. Key hyperparameters: QLSTM sequence
length$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and
simplified strategy. \footnote{The views expressed in this article are those of
the authors and do not represent the views of Wells Fargo. This article is for
informational purposes only. Nothing contained in this article should be
construed as investment advice. Wells Fargo makes no express or implied
warranties and expressly disclaims all legal, tax, and accounting implications
related to this article.

</details>


### [164] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: FSPO是一种针对LLM的序列级强化学习方法，通过在重要性采样权重空间中实现长度公平裁剪，解决了传统PPO/GRPO裁剪对不同长度序列的偏差问题，并在理论和实践中均展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 当PPO/GRPO风格的裁剪方法应用于序列级强化学习时，其固定的裁剪范围会导致系统性地重新加权短响应与长响应，从而扭曲了实际优化目标。

Method: 提出FSPO（Fair Sequence Policy Optimization），通过在重要性采样（IS）权重空间直接强制执行长度公平裁剪。理论上，通过长度重加权误差（LRE）形式化了长度公平性。具体方法是，以一个带有KL校正漂移项并按$\sqrt{L}$缩放的带宽，裁剪序列对数-IS比率。

Result: 经验证，FSPO使不同长度区间的裁剪率趋于平坦，稳定了训练过程，并在多个评估数据集上均超越所有基线方法。

Conclusion: FSPO通过其创新的长度公平裁剪机制，有效解决了序列级RL中因长度差异导致的策略更新偏差，显著提升了LLM训练的稳定性和性能。

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [165] [Breaking the Statistical Similarity Trap in Extreme Convection Detection](https://arxiv.org/abs/2509.09195)
*Md Tanveer Hossain Munim*

Main category: cs.LG

TL;DR: DART框架旨在解决深度学习天气模型中“统计相似性陷阱”导致极端天气事件预测不佳的问题。通过双解码器架构、物理驱动过采样和任务特定损失函数，DART显著提高了对极端对流（如洪水）的检测能力，并被证明快速、易于集成，为极端天气预警提供了可靠的AI途径。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习天气模型的评估指标存在“统计相似性陷阱”，奖励模糊预测，却忽略了罕见、高影响的极端事件。现有基线模型在危险对流检测上相关性高达97.9%，而CSI（临界成功指数）却为0.00。研究旨在将粗分辨率的大气预报转换为高分辨率卫星亮度温度场，以优化极端对流（低于220K）的检测。

Method: 本研究引入了DART（双架构回归任务）框架，该框架采用双解码器架构，明确区分背景和极端事件。它还结合了物理驱动的过采样和任务特定的损失函数。此方法旨在系统地处理一个混合的转换-分割-降尺度任务。

Result: (1) 经验性验证了多个复杂基线模型中存在的“统计相似性陷阱”。(2) 发现了“IVT悖论”，即移除被广泛认为对大气河流分析至关重要的水汽输送积分(IVT)后，极端对流检测能力反而提高了270%。(3) DART在实现相同CSI时，偏差远小于基线（DART的CSI=0.273，偏差2.52；基线偏差6.72），展示了架构的必要性和操作灵活性。(4) 通过2023年8月吉大港洪水灾害案例进行了真实世界验证。这项工作首次系统地解决了此类混合任务，且没有直接的现有基准。

Conclusion: DART框架通过beta-tuning实现精确的操作校准，可在标准硬件上10分钟内完成训练，并与现有气象工作流程无缝集成。这为极端天气防备提供了可信赖的AI途径。本研究充分证明了DART的专业设计，能够有效应对极端天气预测挑战。

Abstract: Current evaluation metrics for deep learning weather models create a
"Statistical Similarity Trap", rewarding blurry predictions while missing rare,
high-impact events. We provide quantitative evidence of this trap, showing
sophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous
convection detection. We introduce DART (Dual Architecture for Regression
Tasks), a framework addressing the challenge of transforming coarse atmospheric
forecasts into high-resolution satellite brightness temperature fields
optimized for extreme convection detection (below 220 K). DART employs
dual-decoder architecture with explicit background/extreme decomposition,
physically motivated oversampling, and task-specific loss functions. We present
four key findings: (1) empirical validation of the Statistical Similarity Trap
across multiple sophisticated baselines; (2) the "IVT Paradox", removing
Integrated Water Vapor Transport, widely regarded as essential for atmospheric
river analysis, improves extreme convection detection by 270%; (3)
architectural necessity demonstrated through operational flexibility (DART
achieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent
CSI), and (4) real-world validation with the August 2023 Chittagong flooding
disaster as a case study. To our knowledge, this is the first work to
systematically address this hybrid conversion-segmentation-downscaling task,
with no direct prior benchmarks identified in existing literature. Our
validation against diverse statistical and deep learning baselines sufficiently
demonstrates DART's specialized design. The framework enables precise
operational calibration through beta-tuning, trains in under 10 minutes on
standard hardware, and integrates seamlessly with existing meteorological
workflows, demonstrating a pathway toward trustworthy AI for extreme weather
preparedness.

</details>


### [166] [Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning](https://arxiv.org/abs/2509.09208)
*Somnath Hazra,Pallab Dasgupta,Soumyajit Dey*

Main category: cs.LG

TL;DR: 针对连续控制中约束强化学习的训练不稳定问题，本文提出IP3O算法。该算法通过自适应激励和逐步增加惩罚机制，有效稳定训练，并在经验和理论上均超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 在连续控制的约束强化学习中，平衡奖励最大化和约束满足极具挑战。现有策略优化方法在约束边界附近易出现不稳定性，导致训练性能不佳。

Method: 提出IP3O算法，该算法在奖励结构外引入自适应激励机制，并在接近约束边界前保持在约束内。具体通过实施逐步增加的惩罚来稳定训练动态。

Result: 经验评估显示IP3O在基准环境中优于现有最先进的安全RL算法。同时，通过推导算法最优性的最坏情况误差界，提供了理论保证。

Conclusion: IP3O算法通过其独特的惩罚机制，有效解决了连续控制约束强化学习中的训练稳定性问题，并在实际应用和理论层面均展现出优越的性能和可靠性。

Abstract: Constrained Reinforcement Learning (RL) aims to maximize the return while
adhering to predefined constraint limits, which represent domain-specific
safety requirements. In continuous control settings, where learning agents
govern system actions, balancing the trade-off between reward maximization and
constraint satisfaction remains a significant challenge. Policy optimization
methods often exhibit instability near constraint boundaries, resulting in
suboptimal training performance. To address this issue, we introduce a novel
approach that integrates an adaptive incentive mechanism in addition to the
reward structure to stay within the constraint bound before approaching the
constraint boundary. Building on this insight, we propose Incrementally
Penalized Proximal Policy Optimization (IP3O), a practical algorithm that
enforces a progressively increasing penalty to stabilize training dynamics.
Through empirical evaluation on benchmark environments, we demonstrate the
efficacy of IP3O compared to the performance of state-of-the-art Safe RL
algorithms. Furthermore, we provide theoretical guarantees by deriving a bound
on the worst-case error of the optimality achieved by our algorithm.

</details>


### [167] [Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach](https://arxiv.org/abs/2509.09214)
*Alka Gadakh,Vidya Kumbhar,Sonal Khosla,Kumar Karunendra*

Main category: cs.LG

TL;DR: 本研究利用机器学习方法识别了农业旅游增长的关键指标，发现结合LASSO的逻辑回归模型在预测农业旅游增长方面表现最佳，准确率高达99%。


<details>
  <summary>Details</summary>
Motivation: 农业旅游作为一种促进农村发展、多样化农民收入、保护文化遗产和传统农业实践的战略经济模式，正蓬勃发展。因此，有必要深入研究其增长策略并识别重要指标。

Method: 研究分为两阶段：首先通过全面的文献综述识别初步指标；其次，应用机器学习技术，包括使用最小绝对收缩和选择算子（LASSO）进行特征选择，并结合逻辑回归（LR）、决策树（DT）、随机森林（RF）和极限梯度提升（XGBOOST）等分类模型来预测农业旅游的增长。

Result: 结果显示，结合LASSO方法，在70-30%的训练-测试数据分割下，LR模型达到了98%的最高分类准确率，RF紧随其后为95%。在80-20%的数据分割下，LR模型保持了99%的最高准确率，DT和XGBoost均为97%。

Conclusion: 本研究成功识别了农业旅游增长的重要指标。结合LASSO的逻辑回归模型在预测农业旅游增长方面展现出卓越的准确性，可为农业旅游的发展和提升提供有效的指导。

Abstract: Agro-tourism serves as a strategic economic model designed to facilitate
rural development by diversifying income streams for local communities like
farmers while promoting the conservation of indigenous cultural heritage and
traditional agricultural practices. As a very booming subdomain of tourism,
there is a need to study the strategies for the growth of Agro-tourism in
detail. The current study has identified the important indicators for the
growth and enhancement of agro-tourism. The study is conducted in two phases:
identification of the important indicators through a comprehensive literature
review and in the second phase state-of-the-art techniques were used to
identify the important indicators for the growth of agro-tourism. The
indicators are also called features synonymously, the machine learning models
for feature selection were applied and it was observed that the Least Absolute
Shrinkage and Selection Operator (LASSO) method combined with, the machine
Learning Classifiers such as Logistic Regression (LR), Decision Trees (DT),
Random Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were
used to suggest the growth of the agro-tourism. The results show that with the
LASSO method, LR model gives the highest classification accuracy of 98% in
70-30% train-test data followed by RF with 95% accuracy. Similarly, in the
80-20% train-test data LR maintains the highest accuracy at 99%, while DT and
XGBoost follow with 97% accuracy.

</details>


### [168] [Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement](https://arxiv.org/abs/2509.09219)
*Jakob Nyberg,Pontus Johnson*

Main category: cs.LG

TL;DR: Vejde是一个结合数据抽象、图神经网络和强化学习的框架，能为结构化决策问题生成归纳策略函数，并在未见问题实例上实现有效的策略泛化。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为具有丰富结构化状态（如对象类别和关系）的决策问题提供能够泛化到不同大小和结构的未知问题实例的策略函数。

Method: Vejde框架结合了数据抽象、图神经网络和强化学习。MDP状态被表示为实体事实数据库，并转换为二分图，通过神经消息传递映射到潜在状态。该方法在RDDL定义的八个问题域上进行了测试，策略通过监督学习和强化学习训练，并在独立的测试集上评估其对未见实例的泛化能力。实验结果与MLP代理和在线规划算法Prost进行了比较。

Result: Vejde策略平均而言能够泛化到测试实例，且得分没有显著下降。此外，归纳代理在未见测试实例上的得分平均接近于针对特定实例训练的MLP代理。

Conclusion: Vejde框架能够有效地为结构化决策问题生成归纳策略，这些策略在未见实例上表现出良好的泛化能力，其性能平均接近于为特定实例训练的模型。

Abstract: We present and evaluate Vejde; a framework which combines data abstraction,
graph neural networks and reinforcement learning to produce inductive policy
functions for decision problems with richly structured states, such as object
classes and relations. MDP states are represented as data bases of facts about
entities, and Vejde converts each state to a bipartite graph, which is mapped
to latent states through neural message passing. The factored representation of
both states and actions allows Vejde agents to handle problems of varying size
and structure. We tested Vejde agents on eight problem domains defined in RDDL,
with ten problem instances each, where policies were trained using both
supervised and reinforcement learning. To test policy generalization, we
separate problem instances in two sets, one for training and the other solely
for testing. Test results on unseen instances for the Vejde agents were
compared to MLP agents trained on each problem instance, as well as the online
planning algorithm Prost. Our results show that Vejde policies in average
generalize to the test instances without a significant loss in score.
Additionally, the inductive agents received scores on unseen test instances
that on average were close to the instance-specific MLP agents.

</details>


### [169] [Constructing a Question-Answering Simulator through the Distillation of LLMs](https://arxiv.org/abs/2509.09226)
*Haipeng Liu,Ting Long,Jing Fu*

Main category: cs.LG

TL;DR: 本文提出LDSim方法，通过从大型语言模型（LLM）中蒸馏领域知识和推理能力，以提升问答模拟器的性能，在保持效率的同时实现更好的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 问答（QA）模拟器对于教育推荐系统（ERS）收集训练数据至关重要，以避免不良推荐。现有方法分为LLM-free和LLM-based两类：LLM-free方法速度快但性能欠佳；LLM-based方法性能好但推理速度慢且资源消耗高。因此，需要一种能够结合两者优势、平衡性能与效率的方法。

Method: 本文提出了一种名为LLM Distillation based Simulator (LDSim) 的方法。该方法通过从大型语言模型（LLM）中蒸馏其领域知识和推理能力，来更好地辅助预测，从而提高模拟性能。

Result: 广泛的实验表明，LDSim在模拟任务和知识追踪（KT）任务上均取得了显著成果，证明了其优越的性能。

Conclusion: LDSim通过LLM蒸馏策略，有效提升了问答模拟器的性能，为教育推荐系统提供了一种高效且准确的解决方案。

Abstract: The question-answering (QA) simulator is a model that mimics real student
learning behaviors and predicts their correctness of their responses to
questions. QA simulators enable educational recommender systems (ERS) to
collect large amounts of training data without interacting with real students,
thereby preventing harmful recommendations made by an undertrained ERS from
undermining actual student learning. Given the QA history, there are two
categories of solutions to predict the correctness, conducting the simulation:
(1) LLM-free methods, which apply a traditional sequential model to transfer
the QA history into a vector representation first, and make predictions based
on the representation; (2) LLM-based methods, which leverage the domain
knowledge and reasoning capability of LLM to enhence the prediction. LLM-free
methods offer fast inference but generally yield suboptimal performance. In
contrast, most LLM-based methods achieve better results, but at the cost of
slower inference speed and higher GPU memory consumption. In this paper, we
propose a method named LLM Distillation based Simulator (LDSim), which distills
domain knowledge and reasoning capability from an LLM to better assist
prediction, thereby improving simulation performance. Extensive experiments
demonstrate that our LDSim achieves strong results on both the simulation task
and the knowledge tracing (KT) task. Our code is publicly available at
https://anonymous.4open.science/r/LDSim-05A9.

</details>


### [170] [Unsupervised Multi-Attention Meta Transformer for Rotating Machinery Fault Diagnosis](https://arxiv.org/abs/2509.09251)
*Hanyang Wang,Yuxuan Yang,Hongjun Wang,Lihui Wang*

Main category: cs.LG

TL;DR: 本文提出一种名为MMT-FD的多注意力元Transformer方法，用于小样本无监督旋转机械故障诊断，解决了数据标注成本高和模型泛化性差的问题，实现了高精度和强泛化能力的诊断。


<details>
  <summary>Details</summary>
Motivation: 旋转机械智能故障诊断通常需要大量标注样本，但实际获取困难且成本高昂。此外，不同设备类型需单独训练模型，导致泛化能力不足。

Method: 提出MMT-FD框架，整合时频域编码器和元学习泛化模型。时频域编码器从无标注数据中提取潜在故障表示，并通过随机增强预测状态表示。增强数据输入元学习网络进行分类和泛化训练，再用少量标注数据微调，并通过少量对比学习迭代优化。

Result: 在轴承故障数据集和转子测试台数据上的实验结果表明，MMT-FD模型在仅使用1%标注样本数据的情况下，实现了99%的故障诊断准确率，并展示出鲁棒的泛化能力。

Conclusion: MMT-FD方法有效解决了旋转机械故障诊断中小样本数据和模型泛化能力不足的挑战，能在极少标注数据下实现高精度、强泛化能力的故障诊断，适用于各种机械设备。

Abstract: The intelligent fault diagnosis of rotating mechanical equipment usually
requires a large amount of labeled sample data. However, in practical
industrial applications, acquiring enough data is both challenging and
expensive in terms of time and cost. Moreover, different types of rotating
mechanical equipment with different unique mechanical properties, require
separate training of diagnostic models for each case. To address the challenges
of limited fault samples and the lack of generalizability in prediction models
for practical engineering applications, we propose a Multi-Attention Meta
Transformer method for few-shot unsupervised rotating machinery fault diagnosis
(MMT-FD). This framework extracts potential fault representations from
unlabeled data and demonstrates strong generalization capabilities, making it
suitable for diagnosing faults across various types of mechanical equipment.
The MMT-FD framework integrates a time-frequency domain encoder and a
meta-learning generalization model. The time-frequency domain encoder predicts
status representations generated through random augmentations in the
time-frequency domain. These enhanced data are then fed into a meta-learning
network for classification and generalization training, followed by fine-tuning
using a limited amount of labeled data. The model is iteratively optimized
using a small number of contrastive learning iterations, resulting in high
efficiency. To validate the framework, we conducted experiments on a bearing
fault dataset and rotor test bench data. The results demonstrate that the
MMT-FD model achieves 99\% fault diagnosis accuracy with only 1\% of labeled
sample data, exhibiting robust generalization capabilities.

</details>


### [171] [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265)
*Jiawei Wang,Jiacai Liu,Yuqian Fu,Yingru Li,Xintao Wang,Yuan Lin,Yu Yue,Lin Zhang,Yang Wang,Ke Wang*

Main category: cs.LG

TL;DR: 针对长周期任务中LLM智能体稀疏奖励和策略梯度与熵耦合导致的学习效率低下问题，本文提出EMPG框架，通过基于不确定性校准学习信号，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在长周期任务中，基于大型语言模型（LLMs）的智能体难以通过稀疏、基于结果的奖励有效分配中间步骤的贡献。以往方法主要关注创建密集奖励信号。本文识别出一个根本问题：LLM学习动态中，策略梯度的大小与熵固有地耦合，导致对确定性正确动作的更新效率低下（更新小），而对不确定性动作的更新可能不稳定（更新大）。

Method: 提出“熵调制策略梯度”（Entropy-Modulated Policy Gradients, EMPG）框架。EMPG根据步骤级不确定性和最终任务结果重新校准学习信号。具体而言，EMPG放大对确定性正确动作的更新，惩罚确定性错误，并衰减来自不确定步骤的更新以稳定探索。此外，还引入了一个“未来清晰度”奖励项，鼓励智能体寻找更可预测的解决方案路径。

Result: 通过在WebShop、ALFWorld和Deep Search这三个具有挑战性的智能体任务上进行的综合实验，EMPG实现了显著的性能提升，并且显著优于强大的策略梯度基线。

Conclusion: EMPG通过基于步骤级不确定性和最终任务结果重新校准学习信号，有效解决了LLM智能体在长周期任务中策略梯度与熵耦合导致的学习效率问题，显著提高了性能。

Abstract: In long-horizon tasks, recent agents based on Large Language Models (LLMs)
face a significant challenge that sparse, outcome-based rewards make it
difficult to assign credit to intermediate steps. Previous methods mainly focus
on creating dense reward signals to guide learning, either through traditional
reinforcement learning techniques like inverse reinforcement learning or by
using Process Reward Models for step-by-step feedback. In this paper, we
identify a fundamental problem in the learning dynamics of LLMs: the magnitude
of policy gradients is inherently coupled with the entropy, which leads to
inefficient small updates for confident correct actions and potentially
destabilizes large updates for uncertain ones. To resolve this, we propose
Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the
learning signal based on step-wise uncertainty and the final task outcome. EMPG
amplifies updates for confident correct actions, penalizes confident errors,
and attenuates updates from uncertain steps to stabilize exploration. We
further introduce a bonus term for future clarity that encourages agents to
find more predictable solution paths. Through comprehensive experiments on
three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we
demonstrate that EMPG achieves substantial performance gains and significantly
outperforms strong policy gradient baselines. Project page is at
https://empgseed-seed.github.io/

</details>


### [172] [Data Driven Discovery of Emergent Dynamics in Reaction Diffusion Systems from Sparse and Noisy Observations](https://arxiv.org/abs/2509.09278)
*Saumitra Dwivedi,Ricardo da Silva Torres,Ibrahim A. Hameed,Gunnar Tufte,Anniken Susanne T. Karlsen*

Main category: cs.LG

TL;DR: 本文提出DRSALife模型，通过观测数据学习软人工生命规则集，以识别无先验物理知识的反应扩散系统涌现动力学，并在噪声和稀疏数据下表现出鲁棒性，预测准确率达74%。


<details>
  <summary>Details</summary>
Motivation: 在没有先验物理知识的情况下，从观测数据中发现反应扩散系统的涌现动力学时，系统识别是一个当前挑战。

Method: 使用Data-driven Rulesets for Soft Artificial Life (DRSALife) 概念框架，通过观测数据学习软人工生命（如基于Agent和元胞自动机）模型规则集，以准确表示反应扩散系统中的涌现动力学。同时，实验探究了使用噪声和稀疏观测数据集对学习涌现动力学的潜在影响。

Result: 成功识别了代表这些动力学的潜在偏微分方程（PDEs）的结构和参数。学习到的模型能够以74%的准确率预测涌现动力学，并且在受到高斯噪声和时间稀疏性影响时，表现出相当鲁棒的性能。

Conclusion: DRSALife模型为在无先验物理知识下，从观测数据中学习反应扩散系统的涌现动力学提供了一种有效且鲁棒的方法，即使面对噪声和稀疏数据也能保持良好性能并识别潜在PDE结构。

Abstract: Data-driven discovery of emergent dynamics is gaining popularity,
particularly in the context of reaction-diffusion systems. These systems are
widely studied across various fields, including neuroscience, ecology,
epidemiology, and several other subject areas that deal with emergent dynamics.
A current challenge in the discovery process relates to system identification
when there is no prior knowledge of the underlying physics. We attempt to
address this challenge by learning Soft Artificial Life (Soft ALife) models,
such as Agent-based and Cellular Automata (CA) models, from observed data for
reaction-diffusion systems. In this paper, we present findings on the
applicability of a conceptual framework, the Data-driven Rulesets for Soft
Artificial Life (DRSALife) model, to learn Soft ALife rulesets that accurately
represent emergent dynamics in a reaction-diffusion system from observed data.
This model has demonstrated promising results for Elementary CA Rule 30, Game
of Life, and Vicsek Flocking problems in recent work. To our knowledge, this is
one of the few studies that explore machine-based Soft ALife ruleset learning
and system identification for reaction-diffusion dynamics without any prior
knowledge of the underlying physics. Moreover, we provide comprehensive
findings from experiments investigating the potential effects of using noisy
and sparse observed datasets on learning emergent dynamics. Additionally, we
successfully identify the structure and parameters of the underlying partial
differential equations (PDEs) representing these dynamics. Experimental results
demonstrate that the learned models are able to predict the emergent dynamics
with good accuracy (74%) and exhibit quite robust performance when subjected to
Gaussian noise and temporal sparsity.

</details>


### [173] [MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph Experts](https://arxiv.org/abs/2509.09337)
*Junda Ye,Zhongbao Zhang,Li Sun,Siqiang Luo*

Main category: cs.LG

TL;DR: 提出Mixture of Subgraph Experts (MoSE) 框架，通过匿名游走提取子图并动态分配给专家，解决了GNN结构表达能力不足和现有方法局限性问题，实现了灵活且可解释的子图表示学习，并在多种图任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有GNN依赖局部消息传递，难以捕获复杂高阶子图模式，导致结构表达能力不足。现有结合随机游走核的方法主要面向图级任务，不适用于节点分类等，且固定核配置缺乏灵活性。

Method: 提出MoSE框架，通过匿名游走提取信息丰富的子图，并根据结构语义将它们动态路由到专门的专家。理论分析证明MoSE的表达能力强于Subgrap Weisfeiler-Lehman (SWL) Test。

Result: MoSE在实验中性能优于竞争基线模型。通过可视化学习到的子图专家，提供了对模型学习到的结构模式的可解释性洞察。

Conclusion: MoSE框架通过灵活且表达力强的子图表示学习，有效解决了GNN的结构表达能力限制和现有方法的不足，在多种图任务上取得了卓越性能和可解释性。

Abstract: While graph neural networks (GNNs) have achieved great success in learning
from graph-structured data, their reliance on local, pairwise message passing
restricts their ability to capture complex, high-order subgraph patterns.
leading to insufficient structural expressiveness. Recent efforts have
attempted to enhance structural expressiveness by integrating random walk
kernels into GNNs. However, these methods are inherently designed for
graph-level tasks, which limits their applicability to other downstream tasks
such as node classification. Moreover, their fixed kernel configurations hinder
the model's flexibility in capturing diverse subgraph structures. To address
these limitations, this paper proposes a novel Mixture of Subgraph Experts
(MoSE) framework for flexible and expressive subgraph-based representation
learning across diverse graph tasks. Specifically, MoSE extracts informative
subgraphs via anonymous walks and dynamically routes them to specialized
experts based on structural semantics, enabling the model to capture diverse
subgraph patterns with improved flexibility and interpretability. We further
provide a theoretical analysis of MoSE's expressivity within the Subgraph
Weisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL.
Extensive experiments, together with visualizations of learned subgraph
experts, demonstrate that MoSE not only outperforms competitive baselines but
also provides interpretable insights into structural patterns learned by the
model.

</details>


### [174] [Robust Non-Linear Correlations via Polynomial Regression](https://arxiv.org/abs/2509.09380)
*Luca Giuliani,Michele Lombardi*

Main category: cs.LG

TL;DR: 本文提出一种基于多项式核的新型Hirschfeld-Gebelein-Rényi (HGR)相关系数计算方法，相比现有方法更具鲁棒性、确定性且计算更快，解决了其在机器学习中作为损失正则化器时面临的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: Hirschfeld-Gebelein-Rényi (HGR)相关系数可作为机器学习中的损失正则化器，但现有的可微分HGR估计算法因其固有的不可计算性，存在偏差-方差权衡，导致鲁棒性可能受损，不适用于实际应用。

Method: 引入了一种基于用户可配置多项式核的HGR新型计算方法，旨在提高鲁棒性，并提供更快但几乎同等有效的限制。

Result: 该方法在鲁棒性和确定性方面具有显著优势，是一种更可靠的实际应用选择。实验分析验证了其在约束机器学习框架内的适用性，表明其计算能产生一个有洞察力的次梯度，可作为损失正则化器。

Conclusion: 所提出的HGR计算方法在鲁棒性、确定性和效率方面优于现有方法，使其成为实际应用中更可靠的选项，并有效支持其在约束机器学习中作为损失正则化器的作用。

Abstract: The Hirschfeld-Gebelein-R\'enyi (HGR) correlation coefficient is an extension
of Pearson's correlation that is not limited to linear correlations, with
potential applications in algorithmic fairness, scientific analysis, and causal
discovery. Recently, novel algorithms to estimate HGR in a differentiable
manner have been proposed to facilitate its use as a loss regularizer in
constrained machine learning applications. However, the inherent
uncomputability of HGR requires a bias-variance trade-off, which can possibly
compromise the robustness of the proposed methods, hence raising technical
concerns if applied in real-world scenarios. We introduce a novel computational
approach for HGR that relies on user-configurable polynomial kernels, offering
greater robustness compared to previous methods and featuring a faster yet
almost equally effective restriction. Our approach provides significant
advantages in terms of robustness and determinism, making it a more reliable
option for real-world applications. Moreover, we present a brief experimental
analysis to validate the applicability of our approach within a constrained
machine learning framework, showing that its computation yields an insightful
subgradient that can serve as a loss regularizer.

</details>


### [175] [MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization](https://arxiv.org/abs/2509.09387)
*Mohammed Tiouti,Mohamed Bal-Ghaoui*

Main category: cs.LG

TL;DR: MetaLLMiX是一个零样本超参数优化框架，它结合元学习、可解释AI和高效LLM推理，利用历史数据无需额外试错即可推荐最优模型和超参数，显著降低了计算成本并实现了与传统方法相当或更优的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的模型和超参数选择面临巨大挑战，需要大量专业知识和计算资源。现有基于LLM的方法依赖试错和昂贵的API，且解释性和泛化能力有限。

Method: 提出MetaLLMiX，一个零样本超参数优化框架。该框架结合了元学习、可解释AI（通过SHAP解释）和高效LLM推理，利用历史实验结果推荐最优超参数和预训练模型，无需进行额外试验。此外，还采用LLM作为评判者来控制输出格式、准确性和完整性。

Result: MetaLLMiX在八个医学图像数据集上的表现与传统HPO方法相比具有竞争力或更优，并大幅降低了计算成本。其本地部署优于API-based方法，在8项任务中5项实现最佳结果，响应时间减少99.6-99.9%，6个数据集的训练时间最快（快2.4-15.7倍），同时保持了与最佳基线1-5%的准确性差距。

Conclusion: MetaLLMiX提供了一种有效且高效的零样本超参数优化解决方案，它通过结合先进技术显著减少了深度学习模型和超参数选择所需的计算资源和时间，同时保持了高准确性，优于现有传统和API驱动的方法。

Abstract: Effective model and hyperparameter selection remains a major challenge in
deep learning, often requiring extensive expertise and computation. While
AutoML and large language models (LLMs) promise automation, current LLM-based
approaches rely on trial and error and expensive APIs, which provide limited
interpretability and generalizability. We propose MetaLLMiX, a zero-shot
hyperparameter optimization framework combining meta-learning, explainable AI,
and efficient LLM reasoning. By leveraging historical experiment outcomes with
SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained
models without additional trials. We further employ an LLM-as-judge evaluation
to control output format, accuracy, and completeness. Experiments on eight
medical imaging datasets using nine open-source lightweight LLMs show that
MetaLLMiX achieves competitive or superior performance to traditional HPO
methods while drastically reducing computational cost. Our local deployment
outperforms prior API-based approaches, achieving optimal results on 5 of 8
tasks, response time reductions of 99.6-99.9%, and the fastest training times
on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of
best-performing baselines.

</details>


### [176] [LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations](https://arxiv.org/abs/2509.09396)
*Harry Mayne,Ryan Othniel Kearns,Yushi Yang,Andrew M. Bean,Eoin Delaney,Chris Russell,Adam Mahdi*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）生成的反事实解释（SCEs）在有效性与最小性之间存在普遍权衡：通常有效但冗余，或简洁但无效。这限制了SCEs作为有效可解释性工具的价值。


<details>
  <summary>Details</summary>
Motivation: 为了促进与人类的有效协作，语言模型需要能够以自然语言解释其决策。本研究旨在评估一种特定的自解释——自生成的反事实解释（SCEs）——在提供清晰、可靠解释方面的能力。

Method: 研究评估了LLMs生成的反事实解释（SCEs）的两个关键特性：有效性（是否能达到预期的不同结果）和最小性（是否仅进行必要的修改）。通过要求LLMs生成反事实解释和最小反事实解释，在多个LLMs、数据集和评估设置下进行测试。

Result: LLMs在生成反事实解释时，通常能保证有效性，但远非最小化，难以提供深入的决策洞察。而当被要求生成最小反事实时，LLMs的修改往往过小以致无法改变预测。这种有效性-最小性之间的权衡在所有测试条件中普遍存在。

Conclusion: 研究结果表明，自生成的反事实解释（SCEs）作为一种可解释性工具，充其量是低效的，最坏情况下可能提供误导性的模型行为洞察。在将LLMs部署到高风险场景时，必须充分考虑这些不可靠自解释对下游决策的潜在影响。

Abstract: To collaborate effectively with humans, language models must be able to
explain their decisions in natural language. We study a specific type of
self-explanation: self-generated counterfactual explanations (SCEs), where a
model explains its prediction by modifying the input such that it would have
predicted a different outcome. We evaluate whether LLMs can produce SCEs that
are valid, achieving the intended outcome, and minimal, modifying the input no
more than necessary. When asked to generate counterfactuals, we find that LLMs
typically produce SCEs that are valid, but far from minimal, offering little
insight into their decision-making behaviour. Worryingly, when asked to
generate minimal counterfactuals, LLMs typically make excessively small edits
that fail to change predictions. The observed validity-minimality trade-off is
consistent across several LLMs, datasets, and evaluation settings. Our findings
suggest that SCEs are, at best, an ineffective explainability tool and, at
worst, can provide misleading insights into model behaviour. Proposals to
deploy LLMs in high-stakes settings must consider the impact of unreliable
self-explanations on downstream decision-making. Our code is available at
https://github.com/HarryMayne/SCEs.

</details>


### [177] [Kriging prior Regression: A Case for Kriging-Based Spatial Features with TabPFN in Soil Mapping](https://arxiv.org/abs/2509.09408)
*Jonas Schmidinger,Viacheslav Barkov,Sebastian Vogel,Martin Atzmueller,Gerard B M Heuvelink*

Main category: cs.LG

TL;DR: 本文提出一种名为Kriging先验回归（KpR）的混合框架，通过将普通克里金法的“空间滞后”特征融入机器学习（ML）模型（特别是TabPFN），以提升土壤属性预测的准确性和不确定性估计的可靠性，特别适用于精确农业中的数字土壤制图。


<details>
  <summary>Details</summary>
Motivation: 机器学习和地统计学在土壤属性预测和空间制图方面各有优势：地统计学利用空间结构，而机器学习捕捉特征与属性间的关系。然而，纯粹的机器学习方法通常缺乏空间背景，导致预测能力受限。因此，研究需要一种能融合二者优势的混合框架。

Method: 提出“Kriging先验回归”（KpR）混合框架，通过从普通克里金法中工程化“空间滞后”特征，将空间上下文信息整合到机器学习模型中。该方法使用TabPFN模型，在LimeSoDa的六个田间尺度数据集上评估了KpR的点预测和概率预测性能，数据集包含土壤有机碳、粘土含量和pH值等。

Result: KpR与TabPFN结合展现了可靠的不确定性估计和更准确的预测，与多种其他空间技术（如回归/残差克里金法）以及非空间机器学习算法（如随机森林）相比表现更优。值得注意的是，它使平均R²比不含空间上下文的机器学习算法提高了约30%。这种改进归因于TabPFN算法本身的强大预测性能以及KpR特征提供的互补空间信息。TabPFN擅长小样本预测，KpR能弥补近地传感数据有限时特征与土壤属性间关系较弱的不足。

Conclusion: KpR结合TabPFN是一种非常稳健和通用的建模框架，适用于精确农业中的数字土壤制图。它能有效处理小样本量数据，并弥补传感数据不足时特征与土壤属性间关系较弱的问题，从而提供更准确和可靠的预测。

Abstract: Machine learning and geostatistics are two fundamentally different frameworks
for predicting and spatially mapping soil properties. Geostatistics leverages
the spatial structure of soil properties, while machine learning captures the
relationship between available environmental features and soil properties. We
propose a hybrid framework that enriches ML with spatial context through
engineering of 'spatial lag' features from ordinary kriging. We call this
approach 'kriging prior regression' (KpR), as it follows the inverse logic of
regression kriging. To evaluate this approach, we assessed both the point and
probabilistic prediction performance of KpR, using the TabPFN model across six
fieldscale datasets from LimeSoDa. These datasets included soil organic carbon,
clay content, and pH, along with features derived from remote sensing and
in-situ proximal soil sensing. KpR with TabPFN demonstrated reliable
uncertainty estimates and more accurate predictions in comparison to several
other spatial techniques (e.g., regression/residual kriging with TabPFN), as
well as to established non-spatial machine learning algorithms (e.g., random
forest). Most notably, it significantly improved the average R2 by around 30%
compared to machine learning algorithms without spatial context. This
improvement was due to the strong prediction performance of the TabPFN
algorithm itself and the complementary spatial information provided by KpR
features. TabPFN is particularly effective for prediction tasks with small
sample sizes, common in precision agriculture, whereas KpR can compensate for
weak relationships between sensing features and soil properties when proximal
soil sensing data are limited. Hence, we conclude that KpR with TabPFN is a
very robust and versatile modelling framework for digital soil mapping in
precision agriculture.

</details>


### [178] [Fused Lasso Improves Accuracy of Co-occurrence Network Inference in Grouped Samples](https://arxiv.org/abs/2509.09413)
*Daniel Agyapong,Briana H. Beatty,Peter G. Kennedy,Toby D. Hocking*

Main category: cs.LG

TL;DR: 现有微生物群落网络推断算法难以处理动态多环境数据。本研究提出`fuser`算法和SAC框架，用于评估算法在同质和异质环境中的表现。`fuser`能生成特定环境的网络，并在跨环境场景中显著优于基线算法。


<details>
  <summary>Details</summary>
Motivation: 现有算法通常仅分析单一环境生态位内的微生物关联，捕获静态快照，且常将不同环境生态位的样本混为一谈，未充分考虑微生物群落在不同生态条件下的适应性关联，从而限制了对微生物群落动态和跨环境适应性关联的理解。

Method: 本研究利用公开可用的微生物丰度数据，并提出“同质-异质交叉验证（SAC）”框架，包含“同质（Same）”和“异质（All）”两种场景。此外，引入了`fuser`算法，该算法在训练过程中保留子样本特异性信号，同时在不同环境间共享相关信息，以生成独特且特定于环境的预测网络。

Result: 在同质环境（Same场景）下，`fuser`算法与现有算法（如glmnet）表现相当。更重要的是，在跨环境（All场景）下，`fuser`算法相比基线算法显著降低了测试误差。

Conclusion: `fuser`算法能有效解决在多样化环境条件下推断微生物共现网络的挑战。它在异质环境中适应和预测微生物关联方面表现出色，为微生物群落提供了更具动态性和生态相关性的理解。

Abstract: Co-occurrence network inference algorithms have significantly advanced our
understanding of microbiome communities. However, these algorithms typically
analyze microbial associations within samples collected from a single
environmental niche, often capturing only static snapshots rather than dynamic
microbial processes. Previous studies have commonly grouped samples from
different environmental niches together without fully considering how microbial
communities adapt their associations when faced with varying ecological
conditions. Our study addresses this limitation by explicitly investigating
both spatial and temporal dynamics of microbial communities. We analyzed
publicly available microbiome abundance data across multiple locations and time
points, to evaluate algorithm performance in predicting microbial associations
using our proposed Same-All Cross-validation (SAC) framework. SAC evaluates
algorithms in two distinct scenarios: training and testing within the same
environmental niche (Same), and training and testing on combined data from
multiple environmental niches (All). To overcome the limitations of
conventional algorithms, we propose fuser, an algorithm that, while not
entirely new in machine learning, is novel for microbiome community network
inference. It retains subsample-specific signals while simultaneously sharing
relevant information across environments during training. Unlike standard
approaches that infer a single generalized network from combined data, fuser
generates distinct, environment-specific predictive networks. Our results
demonstrate that fuser achieves comparable predictive performance to existing
algorithms such as glmnet when evaluated within homogeneous environments
(Same), and notably reduces test error compared to baseline algorithms in
cross-environment (All) scenarios.

</details>


### [179] [Composable Score-based Graph Diffusion Model for Multi-Conditional Molecular Generation](https://arxiv.org/abs/2509.09451)
*Anjie Qiao,Zhen Wang,Chuan Chen,DeFu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 本文提出CSGD模型，是首个将分数匹配扩展到离散图的扩散模型，通过可组合指导和概率校准，显著提高了多条件可控分子图生成性能，在可控性上平均提升15.3%。


<details>
  <summary>Details</summary>
Motivation: 可控分子图生成对材料和药物发现至关重要，但现有图扩散模型在多条件设置中效果有限，常因联合条件或连续松弛而损害生成质量。

Method: 提出可组合分数基图扩散模型（CSGD），通过具体分数将分数匹配扩展到离散图，实现灵活和原则性的条件引导。引入两种基于分数的技术：可组合指导（CoG），用于在采样时对条件子集进行细粒度控制；概率校准（PC），用于调整估计的转移概率以缓解训练-测试不匹配。

Result: CSGD在四个分子数据集上取得了最先进的性能，相比现有方法在可控性上平均提高了15.3%，同时保持了高有效性和分布保真度。

Conclusion: 分数基建模在离散图生成方面具有实际优势，并能实现灵活的多属性分子设计。

Abstract: Controllable molecular graph generation is essential for material and drug
discovery, where generated molecules must satisfy diverse property constraints.
While recent advances in graph diffusion models have improved generation
quality, their effectiveness in multi-conditional settings remains limited due
to reliance on joint conditioning or continuous relaxations that compromise
fidelity. To address these limitations, we propose Composable Score-based Graph
Diffusion model (CSGD), the first model that extends score matching to discrete
graphs via concrete scores, enabling flexible and principled manipulation of
conditional guidance. Building on this foundation, we introduce two score-based
techniques: Composable Guidance (CoG), which allows fine-grained control over
arbitrary subsets of conditions during sampling, and Probability Calibration
(PC), which adjusts estimated transition probabilities to mitigate train-test
mismatches. Empirical results on four molecular datasets show that CSGD
achieves state-of-the-art performance, with a 15.3% average improvement in
controllability over prior methods, while maintaining high validity and
distributional fidelity. Our findings highlight the practical advantages of
score-based modeling for discrete graph generation and its capacity for
flexible, multi-property molecular design.

</details>


### [180] [AquaCast: Urban Water Dynamics Forecasting with Precipitation-Informed Multi-Input Transformer](https://arxiv.org/abs/2509.09458)
*Golnoosh Abdollahinejad,Saleh Baghersalimi,Denisa-Andreea Constantinescu,Sergey Shevchik,David Atienza*

Main category: cs.LG

TL;DR: 提出了一种名为AquaCast的深度学习模型，用于城市水动力学预测，该模型结合内生和外生变量，通过嵌入层融合外生输入，并在真实和合成数据集上均展现出最先进且鲁棒的预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决城市水动力学预测的挑战，特别是如何有效整合内生变量（如水位、流量）和外生因素（如降水历史和预报）以提高预测准确性。

Method: 开发了一个名为AquaCast的多输入、多输出深度学习模型。该模型能够捕捉所有输入间的变量间和时间依赖性，并专注于对内生变量的预测。外生输入通过一个嵌入层进行融合，以有效利用其短期影响，同时避免对其自身的预测。

Result: 模型在LausanneCity数据集（包含四个城市排水传感器数据）上，仅使用内生变量时达到了最先进的性能。加入外生变量和预测报告后，性能得到进一步提升。在三个大型合成数据集（MeteoSwiss、Lorenz Attractors、Random Fields）上的测试证实了模型在不同时间复杂度和100个节点下的泛化能力和可扩展性，并持续优于现有基线。

Conclusion: AquaCast模型为城市水动力学预测提供了一个有效且鲁棒的解决方案。它通过独特地融合内生和外生数据，在真实和合成数据集上均表现出卓越的预测准确性、泛化能力和可扩展性。

Abstract: This work addresses the challenge of forecasting urban water dynamics by
developing a multi-input, multi-output deep learning model that incorporates
both endogenous variables (e.g., water height or discharge) and exogenous
factors (e.g., precipitation history and forecast reports). Unlike conventional
forecasting, the proposed model, AquaCast, captures both inter-variable and
temporal dependencies across all inputs, while focusing forecast solely on
endogenous variables. Exogenous inputs are fused via an embedding layer,
eliminating the need to forecast them and enabling the model to attend to their
short-term influences more effectively. We evaluate our approach on the
LausanneCity dataset, which includes measurements from four urban drainage
sensors, and demonstrate state-of-the-art performance when using only
endogenous variables. Performance also improves with the inclusion of exogenous
variables and forecast reports. To assess generalization and scalability, we
additionally test the model on three large-scale synthesized datasets,
generated from MeteoSwiss records, the Lorenz Attractors model, and the Random
Fields model, each representing a different level of temporal complexity across
100 nodes. The results confirm that our model consistently outperforms existing
baselines and maintains a robust and accurate forecast across both real and
synthetic datasets.

</details>


### [181] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: 该研究提出并验证了一个名为“Agent-E”的自动化AI系统，结合RPA，能高效地从学术文献中识别特定论文并执行预定义动作，显著加速学术工作流程。


<details>
  <summary>Details</summary>
Motivation: 应对学术文献快速增长带来的挑战，解决研究人员在学术发现过程中耗时耗力的手动工作。

Method: 开发了一个全自动化系统，包括一个名为“Agent-E”的专业AI代理，负责从会议论文集中识别特定地理区域的论文，并结合RPA执行预定义动作（如提交提名表）。该系统在586篇来自5个不同会议的论文上进行了验证。

Result: 系统在识别目标论文时达到了100%的召回率和99.4%的近乎完美准确率。

Conclusion: 任务型AI代理不仅能够过滤信息，还能积极参与并加速学术社区的工作流程，展示了其巨大潜力。

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [182] [CountTRuCoLa: Rule Confidence Learning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2509.09474)
*Julia Gastinger,Christian Meilicke,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: 本文提出了一种基于时间规则的完全可解释方法，用于时间知识图谱（TKG）预测，其性能达到或超越了最先进的模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决时间知识图谱预测任务，并受近期基于循环事实的强大基线启发，旨在开发一种完全可解释的方法。

Method: 该方法通过学习四种简单类型的时间规则进行预测，并结合一个同时考虑近因和频率的置信度函数。

Result: 在九个数据集上的评估表明，该方法在性能上与八个最先进模型和两个基线模型持平或超越，同时提供了完全可解释的预测。

Conclusion: 该研究成功开发了一种既有效又完全可解释的时间知识图谱预测方法，其性能具有竞争力。

Abstract: We address the task of temporal knowledge graph (TKG) forecasting by
introducing a fully explainable method based on temporal rules. Motivated by
recent work proposing a strong baseline using recurrent facts, our approach
learns four simple types of rules with a confidence function that considers
both recency and frequency. Evaluated on nine datasets, our method matches or
surpasses the performance of eight state-of-the-art models and two baselines,
while providing fully interpretable predictions.

</details>


### [183] [Balancing Utility and Privacy: Dynamically Private SGD with Random Projection](https://arxiv.org/abs/2509.09485)
*Zhanhong Jiang,Md Zahid Hasan,Nastaran Saadati,Aditya Balu,Chao Liu,Soumik Sarkar*

Main category: cs.LG

TL;DR: 本文提出了D2P2-SGD优化器，通过结合动态差分隐私和随机投影，解决了随机优化中的隐私泄露和效率挑战，在保持隐私的同时显著提升模型准确性，并具有理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 现有随机优化在机器学习中存在隐私泄露风险；差分隐私SGD（DPSGD）的静态噪声机制影响模型性能；随着模型参数增加，随机优化器的学习效率面临挑战。

Method: 引入了动态差分隐私投影随机梯度下降（D2P2-SGD）优化器。该方法结合了动态差分隐私（DDP）与自动梯度裁剪，以及随机投影与SGD，从而能动态调整模型效用和隐私间的权衡。

Result: D2P2-SGD在理论上表现出可证明的次线性收敛速率，与现有最佳速率匹配。理论分析表明DDP能以隐私为代价提供更好效用，随机投影能实现更高效的模型学习。实验证明D2P2-SGD在保持隐私的同时显著提升了准确性。

Conclusion: D2P2-SGD优化器通过动态差分隐私和随机投影的结合，有效解决了现有随机优化器的隐私泄露和效率问题，在提供强大的理论收敛保证的同时，显著提升了模型的准确性并维护了隐私。

Abstract: Stochastic optimization is a pivotal enabler in modern machine learning,
producing effective models for various tasks. However, several existing works
have shown that model parameters and gradient information are susceptible to
privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy
concerns, its static noise mechanism impacts the error bounds for model
performance. Additionally, with the exponential increase in model parameters,
efficient learning of these models using stochastic optimizers has become more
challenging. To address these concerns, we introduce the Dynamically
Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we
combine two important ideas: (i) dynamic differential privacy (DDP) with
automatic gradient clipping and (ii) random projection with SGD, allowing
dynamic adjustment of the tradeoff between utility and privacy of the model. It
exhibits provably sub-linear convergence rates across different objective
functions, matching the best available rate. The theoretical analysis further
suggests that DDP leads to better utility at the cost of privacy, while random
projection enables more efficient model learning. Extensive experiments across
diverse datasets show that D2P2-SGD remarkably enhances accuracy while
maintaining privacy. Our code is available here.

</details>


### [184] [PIPES: A Meta-dataset of Machine Learning Pipelines](https://arxiv.org/abs/2509.09512)
*Cynthia Moreira Maia,Lucas B. V. de Amorim,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: 本文提出PIPES，一个多样化且全面的机器学习实验集合，旨在解决OpenML在算法选择问题中管道多样性不足的限制，并降低元学习的计算成本。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的算法选择问题（ASP）面临评估算法性能时的高计算成本。尽管元学习可以利用OpenML等在线仓库的实验数据，但OpenML的记录在数据预处理步骤的多样性上存在局限，且实验偏向少数流行技术，导致样本不平衡。

Method: 为克服OpenML的局限，研究者提出了PIPES，一个实验集合，旨在通过包含多个管道来代表选定技术集的所有组合，以实现多样性和完整性。PIPES存储了对300个数据集应用9,408个管道的实验结果，并详细记录了管道块、训练和测试时间、预测、性能及错误信息。

Result: PIPES成功构建了一个全面且多样化的实验结果集合，包含了9,408个管道在300个数据集上的应用数据。这些结果涵盖了管道的详细信息、时间成本、预测和性能数据。该集合能够支持研究人员对多样化和代表性管道及数据集进行分析。

Conclusion: PIPES提供了一个丰富且多样化的机器学习实验数据集合，有效解决了OpenML在管道多样性上的不足，降低了元学习的计算成本。它为研究人员提供了进行广泛分析的资源，并具有进一步扩展以支持元学习社区的潜力。

Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face
the challenge of high computational costs associated with evaluating various
algorithms' performances on a given dataset. To mitigate this cost, the
meta-learning field can leverage previously executed experiments shared in
online repositories such as OpenML. OpenML provides an extensive collection of
machine learning experiments. However, an analysis of OpenML's records reveals
limitations. It lacks diversity in pipelines, specifically when exploring data
preprocessing steps/blocks, such as scaling or imputation, resulting in limited
representation. Its experiments are often focused on a few popular techniques
within each pipeline block, leading to an imbalanced sample. To overcome the
observed limitations of OpenML, we propose PIPES, a collection of experiments
involving multiple pipelines designed to represent all combinations of the
selected sets of techniques, aiming at diversity and completeness. PIPES stores
the results of experiments performed applying 9,408 pipelines to 300 datasets.
It includes detailed information on the pipeline blocks, training and testing
times, predictions, performances, and the eventual error messages. This
comprehensive collection of results allows researchers to perform analyses
across diverse and representative pipelines and datasets. PIPES also offers
potential for expansion, as additional data and experiments can be incorporated
to support the meta-learning community further. The data, code, supplementary
material, and all experiments can be found at
https://github.com/cynthiamaia/PIPES.git.

</details>


### [185] [Cough Classification using Few-Shot Learning](https://arxiv.org/abs/2509.09515)
*Yoga Disha Sendhil Kumar,Manas V Shetty,Sudip Vhaduri*

Main category: cs.LG

TL;DR: 本文研究了使用原型网络和少量样本学习进行呼吸音分类（COVID-19、流感、健康），结果显示其在数据有限情况下能达到与传统方法相当的准确率，且多分类模型表现与二分类模型无显著差异。


<details>
  <summary>Details</summary>
Motivation: 解决呼吸音分类中标记数据有限的挑战，并评估少量样本学习能否在大幅减少训练样本的情况下，达到与传统深度学习方法相当的性能。同时，比较多分类和二分类模型的表现。

Method: 采用原型网络结合咳嗽声音的声谱图表示进行分类。研究设计包括多分类和二分类模型，并使用配对t检验和Wilcoxon检验进行统计分析以比较模型性能。

Result: 少量样本学习模型表现出有竞争力的准确率。多分类在每类15个支持样本下达到74.87%的准确率；二分类在所有类别对中均超过70%的准确率。流感是最易区分的类别，健康是最具挑战性的。统计测试表明二分类和多分类模型之间没有显著的性能差异（p值均大于0.05）。

Conclusion: 少量样本学习在医学诊断中（特别是在缺乏大量标记数据集时）是可行的。多分类在该设置下具有可行性。

Abstract: This paper investigates the effectiveness of few-shot learning for
respiratory sound classification, focusing on coughbased detection of COVID-19,
Flu, and healthy conditions. We leverage Prototypical Networks with spectrogram
representations of cough sounds to address the challenge of limited labeled
data. Our study evaluates whether few-shot learning can enable models to
achieve performance comparable to traditional deep learning approaches while
using significantly fewer training samples. Additionally, we compare
multi-class and binary classification models to assess whether multi-class
models can perform comparably to their binary counterparts. Experimental
findings show that few-shot learning models can achieve competitive accuracy.
Our model attains 74.87% accuracy in multi-class classification with only 15
support examples per class, while binary classification achieves over 70%
accuracy across all class pairs. Class-wise analysis reveals Flu as the most
distinguishable class, and Healthy as the most challenging. Statistical tests
(paired t-test p = 0.149, Wilcoxon p = 0.125) indicate no significant
performance difference between binary and multiclass models, supporting the
viability of multi-class classification in this setting. These results
highlight the feasibility of applying few-shot learning in medical diagnostics,
particularly when large labeled datasets are unavailable.

</details>


### [186] [ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning](https://arxiv.org/abs/2509.09534)
*Sena Ergisi,Luis Maßny,Rawad Bitar*

Main category: cs.LG

TL;DR: ProDiGy是一种新的拜占庭鲁棒FL算法，通过结合梯度接近度和相异度的双重评分系统，在数据异构环境下能有效抵御对抗性攻击并保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在分布式学习中面临对抗性攻击的脆弱性，尤其是在数据异构条件下，现有防御机制表现不佳。

Method: 提出了一种名为ProDiGy的拜占庭鲁棒FL算法。其关键创新在于使用基于梯度“接近度”和“相异度”的联合双重评分系统来评估客户端梯度。

Result: 通过广泛的数值实验证明，ProDiGy在各种场景下均优于现有防御机制。特别是在客户端数据不遵循IID分布时，其他防御机制失效的情况下，ProDiGy仍能保持强大的防御能力和模型准确性。

Conclusion: 研究结果强调了双重视角方法的有效性，该方法能促进诚实客户端之间的自然相似性，同时检测可疑的统一性作为潜在的攻击指标。

Abstract: Federated Learning (FL) emerged as a widely studied paradigm for distributed
learning. Despite its many advantages, FL remains vulnerable to adversarial
attacks, especially under data heterogeneity. We propose a new Byzantine-robust
FL algorithm called ProDiGy. The key novelty lies in evaluating the client
gradients using a joint dual scoring system based on the gradients' proximity
and dissimilarity. We demonstrate through extensive numerical experiments that
ProDiGy outperforms existing defenses in various scenarios. In particular, when
the clients' data do not follow an IID distribution, while other defense
mechanisms fail, ProDiGy maintains strong defense capabilities and model
accuracy. These findings highlight the effectiveness of a dual perspective
approach that promotes natural similarity among honest clients while detecting
suspicious uniformity as a potential indicator of an attack.

</details>


### [187] [Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication](https://arxiv.org/abs/2509.09597)
*Maysam Behmanesh,Erkan Turan,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: 提出一种新的图对齐框架，通过双通编码器提升节点区分度，并引入几何感知函数映射模块解决潜在空间错位问题，在图和跨模态任务中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有无监督图对齐方法面临两大挑战：GNN嵌入中过平滑导致的节点区分度下降，以及结构噪声、特征异构性和训练不稳定性造成的潜在空间错位，进而导致节点对应关系不可靠。

Method: 我们提出了一种新的图对齐框架，通过以下方式解决问题：1. 引入双通编码器，结合低通和高通谱滤波器，生成兼具结构感知和高区分度的节点嵌入。2. 集成几何感知函数映射模块，学习图嵌入间的双射和等距变换，以强制潜在空间的几何一致性，解决空间错位问题。

Result: 实验结果显示，本方法在图基准测试中持续优于现有无监督对齐基线，对结构不一致和挑战性对齐场景展现出卓越的鲁棒性。此外，在视觉-语言基准测试中，本框架能有效泛化到图领域之外，实现视觉和语言表示的无监督对齐。

Conclusion: 本框架通过提升节点区分度和确保潜在空间的几何一致性，显著改善了无监督图对齐的性能和鲁棒性，并展现出良好的跨领域泛化能力。

Abstract: Graph alignment-the problem of identifying corresponding nodes across
multiple graphs-is fundamental to numerous applications. Most existing
unsupervised methods embed node features into latent representations to enable
cross-graph comparison without ground-truth correspondences. However, these
methods suffer from two critical limitations: the degradation of node
distinctiveness due to oversmoothing in GNN-based embeddings, and the
misalignment of latent spaces across graphs caused by structural noise, feature
heterogeneity, and training instability, ultimately leading to unreliable node
correspondences. We propose a novel graph alignment framework that
simultaneously enhances node distinctiveness and enforces geometric consistency
across latent spaces. Our approach introduces a dual-pass encoder that combines
low-pass and high-pass spectral filters to generate embeddings that are both
structure-aware and highly discriminative. To address latent space
misalignment, we incorporate a geometry-aware functional map module that learns
bijective and isometric transformations between graph embeddings, ensuring
consistent geometric relationships across different representations. Extensive
experiments on graph benchmarks demonstrate that our method consistently
outperforms existing unsupervised alignment baselines, exhibiting superior
robustness to structural inconsistencies and challenging alignment scenarios.
Additionally, comprehensive evaluation on vision-language benchmarks using
diverse pretrained models shows that our framework effectively generalizes
beyond graph domains, enabling unsupervised alignment of vision and language
representations.

</details>


### [188] [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](https://arxiv.org/abs/2509.09599)
*Ira J. S. Shokar,Rich R. Kerswell,Peter H. Haynes*

Main category: cs.LG

TL;DR: 提出一种深度学习模拟器，用于随机和混沌时空系统，该模拟器能泛化至广泛参数值，并显著加速计算。


<details>
  <summary>Details</summary>
Motivation: 传统数值积分在探索复杂系统参数空间或研究稀有事件时计算成本高昂且效率低下，需要更快速、可泛化的方法。

Method: 该方法通过在单一参数域上预训练模型，再在小型多样化数据集上进行微调，实现跨广泛参数值的泛化。模型融入局部注意力机制，以处理不同域大小和分辨率，从而实现高效预训练和对更大域的泛化。

Result: 模型在混沌Kuramoto-Sivashinsky方程和随机强迫beta平面湍流中得到验证，能够捕获插值参数值处的现象。它提供了比传统数值积分显著的计算加速，其概率变体还能提供不确定性量化。

Conclusion: 该深度学习模拟器通过显著的计算加速和强大的泛化能力，促进了参数空间的有效探索，并支持对复杂系统及稀有事件进行不确定性量化的统计分析。

Abstract: We present a deep learning emulator for stochastic and chaotic
spatio-temporal systems, explicitly conditioned on the parameter values of the
underlying partial differential equations (PDEs). Our approach involves
pre-training the model on a single parameter domain, followed by fine-tuning on
a smaller, yet diverse dataset, enabling generalisation across a broad range of
parameter values. By incorporating local attention mechanisms, the network is
capable of handling varying domain sizes and resolutions. This enables
computationally efficient pre-training on smaller domains while requiring only
a small additional dataset to learn how to generalise to larger domain sizes.
We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky
equation and stochastically-forced beta-plane turbulence, showcasing its
ability to capture phenomena at interpolated parameter values. The emulator
provides significant computational speed-ups over conventional numerical
integration, facilitating efficient exploration of parameter space, while a
probabilistic variant of the emulator provides uncertainty quantification,
allowing for the statistical study of rare events.

</details>


### [189] [ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance](https://arxiv.org/abs/2509.09611)
*Haolan Zheng,Yanlai Chen,Jiequn Han,Yue Yu*

Main category: cs.LG

TL;DR: 提出了一种新颖的数据稀疏算子学习算法ReBaNO，用于解决多输入偏微分方程，该算法在泛化能力和离散不变性方面显著优于现有SOTA算法。


<details>
  <summary>Details</summary>
Motivation: 解决具有多个不同输入的偏微分方程，并开发一种数据稀疏且能克服现有算子学习算法在泛化差距和离散不变性方面局限性的方法。

Method: ReBaNO（Reduced Basis Neural Operator）算法。该方法结合了Reduced Basis Method和Generative Pre-Trained Physics-Informed Neural Networks的灵感，采用数学上严谨的贪婪算法离线自适应地构建网络结构。通过任务特定的激活函数进行知识蒸馏，使架构紧凑，在线计算成本低，并嵌入物理知识。

Result: 数值结果表明，ReBaNO在消除/缩小分布内和分布外测试的泛化差距方面显著优于PCA-Net、DeepONet、FNO和CNO等现有SOTA算子学习算法。此外，ReBaNO是唯一实现严格离散不变性的算子学习算法。

Conclusion: ReBaNO为多输入偏微分方程提供了一种高效、精确且数据稀疏的算子学习解决方案，其卓越的泛化能力和严格的离散不变性使其在同类算法中脱颖而出。

Abstract: We propose a novel data-lean operator learning algorithm, the Reduced Basis
Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct
inputs. Inspired by the Reduced Basis Method and the recently introduced
Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a
mathematically rigorous greedy algorithm to build its network structure offline
adaptively from the ground up. Knowledge distillation via task-specific
activation function allows ReBaNO to have a compact architecture requiring
minimal computational cost online while embedding physics. In comparison to
state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,
and CNO, numerical results demonstrate that ReBaNO significantly outperforms
them in terms of eliminating/shrinking the generalization gap for both in- and
out-of-distribution tests and being the only operator learning algorithm
achieving strict discretization invariance.

</details>


### [190] [Explaining Concept Drift through the Evolution of Group Counterfactuals](https://arxiv.org/abs/2509.09616)
*Ignacy Stępka,Jerzy Stefanowski*

Main category: cs.LG

TL;DR: 本文提出一种新颖方法，通过分析基于群体的反事实解释（GCEs）的时间演变，结合三层框架来解释概念漂移及其根本原因。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在动态环境中常受概念漂移影响，性能下降。现有方法能检测漂移，但解释模型决策逻辑“如何”及“为何”变化仍是巨大挑战。

Method: 引入一种新颖方法，通过分析基于群体的反事实解释（GCEs）的时间演变来解释概念漂移。该方法跟踪漂移前后GCEs聚类中心及其反事实行动向量的变化，以揭示模型决策边界的结构性变化。此分析在一个三层框架中实现，该框架结合了数据层（分布漂移）、模型层（预测不一致）和提出的解释层。

Result: 这种整体性的视角能对概念漂移进行更全面的诊断，使其能够区分不同根本原因，例如空间数据漂移与概念重新标记。

Conclusion: 提出的基于GCEs的三层框架提供了一种全面诊断概念漂移并理解其根本原因的方法，超越了单纯的检测，实现了对漂移的解释。

Abstract: Machine learning models in dynamic environments often suffer from concept
drift, where changes in the data distribution degrade performance. While
detecting this drift is a well-studied topic, explaining how and why the
model's decision-making logic changes still remains a significant challenge. In
this paper, we introduce a novel methodology to explain concept drift by
analyzing the temporal evolution of group-based counterfactual explanations
(GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their
associated counterfactual action vectors before and after a drift. These
evolving GCEs act as an interpretable proxy, revealing structural changes in
the model's decision boundary and its underlying rationale. We operationalize
this analysis within a three-layer framework that synergistically combines
insights from the data layer (distributional shifts), the model layer
(prediction disagreement), and our proposed explanation layer. We show that
such holistic view allows for a more comprehensive diagnosis of drift, making
it possible to distinguish between different root causes, such as a spatial
data shift versus a re-labeling of concepts.

</details>


### [191] [ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms](https://arxiv.org/abs/2509.09679)
*Bingxin Xu,Zhen Dong,Oussama Elachqar,Yuzhang Shang*

Main category: cs.LG

TL;DR: ButterflyQuant提出了一种可学习的蝶形变换和均匀性正则化方法，以解决2比特量化大型语言模型中由激活异常值引起的性能下降问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要大量内存，限制了在消费硬件上的部署。2比特量化虽能减少内存，但因激活中的异常值导致严重的性能损失。现有基于旋转的方法（如QuIP、QuaRot）使用固定变换（如Hadamard矩阵），无法适应特定的权重分布或不同Transformer层的异常值模式。

Method: 我们提出了ButterflyQuant，用可学习的蝶形变换取代Hadamard旋转。这些变换由连续的Givens旋转角度参数化，允许梯度优化并保证正交性。其计算复杂度为$O(n \log n)$，学习参数仅为$\frac{n \log n}{2}$。此外，引入了对变换后激活的均匀性正则化，以促进更适合量化的平滑分布。学习过程仅需128个校准样本，可在数分钟内完成。

Result: 在LLaMA-2-7B模型上进行2比特量化时，ButterflyQuant实现了15.4的困惑度，而QuaRot为22.1。这表明ButterflyQuant在保持性能方面有显著优势。

Conclusion: ButterflyQuant通过引入层适应性的可学习正交变换和均匀性正则化，有效抑制了极端2比特量化中的异常值问题，以极低的训练成本显著提高了大型语言模型的量化性能。

Abstract: Large language models require massive memory footprints, severely limiting
deployment on consumer hardware. Quantization reduces memory through lower
numerical precision, but extreme 2-bit quantization suffers from catastrophic
performance loss due to outliers in activations. Rotation-based methods such as
QuIP and QuaRot apply orthogonal transforms to eliminate outliers before
quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} =
(\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these
methods use fixed transforms--Hadamard matrices achieving optimal worst-case
coherence $\mu = 1/\sqrt{n}$--that cannot adapt to specific weight
distributions. We identify that different transformer layers exhibit distinct
outlier patterns, motivating layer-adaptive rotations rather than
one-size-fits-all approaches. We propose ButterflyQuant, which replaces
Hadamard rotations with learnable butterfly transforms parameterized by
continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$
entries that are non-differentiable and prohibit gradient-based learning,
butterfly transforms' continuous parameterization enables smooth optimization
while guaranteeing orthogonality by construction. This orthogonal constraint
ensures theoretical guarantees in outlier suppression while achieving $O(n \log
n)$ computational complexity with only $\frac{n \log n}{2}$ learnable
parameters. We further introduce a uniformity regularization on
post-transformation activations to promote smoother distributions amenable to
quantization. Learning requires only 128 calibration samples and converges in
minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit
quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.

</details>


### [192] [Functional Groups are All you Need for Chemically Interpretable Molecular Property Prediction](https://arxiv.org/abs/2509.09619)
*Roshan Balaji,Joe Bobby,Nirav Pravinbhai Bhatt*

Main category: cs.LG

TL;DR: 提出功能基表示 (FGR) 框架，结合已知和挖掘的功能基，通过预训练实现分子性质预测的SOTA性能和化学可解释性，解决了深度学习模型缺乏解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在药物和材料发现中加速了进程，但其缺乏可解释性，阻碍了化学家的接受和应用，因此需要开发具有化学可解释性的模型。

Method: 引入功能基表示 (FGR) 框架，通过编码分子的基本化学亚结构（功能基）来表示分子。该方法整合了两类功能基：来自既有化学知识的功能基 (FG) 和通过序列模式挖掘从大规模分子语料库中提取的功能基 (MFG)。通过在大量未标记分子数据集上进行预训练，将分子编码到低维潜在空间，并允许包含2D结构描述符。

Result: FGR框架在33个多样化的基准数据集（包括物理化学、生物物理、量子力学、生物活性和药代动力学）上实现了最先进的性能。同时，它提供了化学可解释性，模型的表示与既定化学原理内在一致，使化学家能够直接将预测属性与特定功能基关联，从而促进对结构-性质关系的新见解。

Conclusion: 该工作为开发用于分子发现的高性能、化学可解释的深度学习模型迈出了重要一步。

Abstract: Molecular property prediction using deep learning (DL) models has accelerated
drug and materials discovery, but the resulting DL models often lack
interpretability, hindering their adoption by chemists. This work proposes
developing molecule representations using the concept of Functional Groups (FG)
in chemistry. We introduce the Functional Group Representation (FGR) framework,
a novel approach to encoding molecules based on their fundamental chemical
substructures. Our method integrates two types of functional groups: those
curated from established chemical knowledge (FG), and those mined from a large
molecular corpus using sequential pattern mining (MFG). The resulting FGR
framework encodes molecules into a lower-dimensional latent space by leveraging
pre-training on a large dataset of unlabeled molecules. Furthermore, the
proposed framework allows the inclusion of 2D structure-based descriptors of
molecules. We demonstrate that the FGR framework achieves state-of-the-art
performance on a diverse range of 33 benchmark datasets spanning physical
chemistry, biophysics, quantum mechanics, biological activity, and
pharmacokinetics while enabling chemical interpretability. Crucially, the
model's representations are intrinsically aligned with established chemical
principles, allowing chemists to directly link predicted properties to specific
functional groups and facilitating novel insights into structure-property
relationships. Our work presents a significant step toward developing
high-performing, chemically interpretable DL models for molecular discovery.

</details>


### [193] [Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management](https://arxiv.org/abs/2509.09655)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: 本文提出了FG-FARL，一种离线强化学习方法，通过校准群组安全阈值，在保持价值的同时，提升医疗决策支持系统的公平性（覆盖或危害）和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有决策支持系统可能在不同受保护亚组间存在不公平和潜在危害。研究旨在减少危害，并使选定的公平目标（覆盖率或危害）在受保护亚组中实现均衡。

Method: 引入了Feasibility-Guided Fair Adaptive Reinforcement Learning (FG-FARL)，一种离线强化学习流程，用于校准各群组的安全阈值。使用来自Medicaid人群的去识别化纵向轨迹数据进行评估，并与行为克隆 (BC) 和 HACO (Hybrid Adaptive Conformal Offline RL) 进行了对比。报告了带有引导式95%置信区间的离策略价值估计和带有p值的亚组差异分析。

Result: FG-FARL在实现与基线（BC, HACO）相当的价值的同时，显著改善了公平性指标。

Conclusion: FG-FARL为实现更安全、更公平的决策支持提供了一条实用途径。

Abstract: We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning
(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds
to reduce harm while equalizing a chosen fairness target (coverage or harm)
across protected subgroups. Using de-identified longitudinal trajectories from
a Medicaid population health management program, we evaluate FG-FARL against
behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global
conformal safety baseline). We report off-policy value estimates with bootstrap
95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL
achieves comparable value to baselines while improving fairness metrics,
demonstrating a practical path to safer and more equitable decision support.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [194] [Fingerprinting Deep Packet Inspection Devices by Their Ambiguities](https://arxiv.org/abs/2509.09081)
*Diwen Xue,Armin Huremagic,Wayne Wang,Ram Sundara Raman,Roya Ensafi*

Main category: cs.NI

TL;DR: DPI设备导致网络干扰增多，但对其了解有限。本研究提出dMAP框架，通过利用DPI解析流量时的实现差异，结合差分模糊测试，远程生成行为指纹，从而大规模识别和区分网络中的DPI设备。


<details>
  <summary>Details</summary>
Motivation: 随着DPI设备的普及，全球用户面临日益加剧的网络审查、限流和拦截等干扰。然而，由于DPI作为网络中介且厂商有意隐匿，我们对其部署和运作方式的理解十分有限。

Method: 本文提出dMAP（DPI Mapper）远程测量框架，通过分析DPI设备在解析流量时因协议规范模糊性导致的内部实现差异，利用差分模糊测试系统地发现并部署专用探测器，将DPI的内部解析行为转化为外部可观察的行为指纹，从而实现DPI的大规模区分和聚类。

Result: 实验表明，dMAP在全球DPI部署中具有实际可行性。仅需20-40个判别性探测器，即可可靠地区分包括主要国家级审查基础设施和商业DPI产品在内的多种DPI实现。

Conclusion: dMAP的指纹识别方法不仅适用于识别审查相关的DPI，还可推广至其他形式的定向网络干扰，为互联网上的DPI主动侦察提供了基础。

Abstract: Users around the world face escalating network interference such as
censorship, throttling, and interception, largely driven by the commoditization
and growing availability of Deep Packet Inspection (DPI) devices. Once reserved
for a few well-resourced nation-state actors, the ability to interfere with
traffic at scale is now within reach of nearly any network operator. Despite
this proliferation, our understanding of DPIs and their deployments on the
Internet remains limited -- being network intermediary leaves DPI unresponsive
to conventional host-based scanning tools, and DPI vendors actively obscuring
their products further complicates measurement efforts.
  In this work, we present a remote measurement framework, dMAP (DPI Mapper),
that derives behavioral fingerprints for DPIs to differentiate and cluster
these otherwise indistinguishable middleboxes at scale, as a first step toward
active reconnaissance of DPIs on the Internet. Our key insight is that parsing
and interpreting traffic as network intermediaries inherently involves
ambiguities -- from under-specified protocol behaviors to differing RFC
interpretations -- forcing DPI vendors into independent implementation choices
that create measurable variance among DPIs. Based on differential fuzzing, dMAP
systematically discovers, selects, and deploys specialized probes that
translate DPI internal parsing behaviors into externally observable
fingerprints. Applying dMAP to DPI deployments globally, we demonstrate its
practical feasibility, showing that even a modest set of 20-40 discriminative
probes reliably differentiates a wide range of DPI implementations, including
major nation-state censorship infrastructures and commercial DPI products. We
discuss how our fingerprinting methodology generalizes beyond censorship to
other forms of targeted interference.

</details>


### [195] [AI Reasoning for Wireless Communications and Networking: A Survey and Perspectives](https://arxiv.org/abs/2509.09193)
*Haoxiang Luo,Yu Yan,Yanhui Bian,Wenjiao Feng,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Gang Sun,Dusit Niyato,Hongfang Yu,Abbas Jamalipour,Shiwen Mao*

Main category: cs.NI

TL;DR: 本综述全面回顾了无线通信网络中启用推理能力的AI（特别是大型语言模型LLMs）的应用，旨在解决传统深度学习缺乏结构化推理的问题，并探讨其在各网络层面的应用及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习方法在无线通信网络中表现为“黑箱”，缺乏处理复杂、多步骤决策所需的结构化推理能力。

Method: 本文通过综合性综述，首先概述智能无线网络的演进及传统AI的局限性；接着介绍新兴AI推理技术（重点关注LLMs）；然后建立无线网络任务的分类系统，并分层（物理层至应用层）考察AI推理；最后讨论关键研究方向。

Result: 研究表明，基于LLM的智能体能结合推理、长期规划、记忆、工具利用和自主跨层控制，以最少人为干预动态优化网络操作。AI推理方法能够显著提升AI在无线通信中的性能。

Conclusion: 本综述旨在融合通信与AI的见解，为将推理技术整合到下一代无线网络中描绘出一条路径。

Abstract: Artificial Intelligence (AI) techniques play a pivotal role in optimizing
wireless communication networks. However, traditional deep learning approaches
often act as closed boxes, lacking the structured reasoning abilities needed to
tackle complex, multi-step decision problems. This survey provides a
comprehensive review and outlook of reasoning-enabled AI in wireless
communication networks, with a focus on Large Language Models (LLMs) and other
advanced reasoning paradigms. In particular, LLM-based agents can combine
reasoning with long-term planning, memory, tool utilization, and autonomous
cross-layer control to dynamically optimize network operations with minimal
human intervention. We begin by outlining the evolution of intelligent wireless
networking and the limitations of conventional AI methods. We then introduce
emerging AI reasoning techniques. Furthermore, we establish a classification
system applicable to wireless network tasks. We also present a layer-by-layer
examination for AI reasoning, covering the physical, data link, network,
transport, and application layers. For each part, we identify key challenges
and illustrate how AI reasoning methods can improve AI-based wireless
communication performance. Finally, we discuss key research directions for AI
reasoning toward future wireless communication networks. By combining insights
from both communications and AI, this survey aims to chart a path for
integrating reasoning techniques into the next-generation wireless networks.

</details>


### [196] [Joint Optimisation of Load Balancing and Energy Efficiency for O-RAN Deployments](https://arxiv.org/abs/2509.09343)
*Mohammed M. H. Qazzaz,Abdelaziz Salama,Maryam Hafeez,Syed A. R. Zaidi*

Main category: cs.NI

TL;DR: 本文提出了一个基于机器学习的框架，用于在O-RAN部署中联合优化负载均衡和能效，解决了现有方法导致负载不平衡的问题，并在仿真中实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: O-RAN通过RIC xApps利用KPM实现动态RU开关以提高能效（EE）。然而，现有AI/ML方法常通过卸载用户设备（UE）来创造休眠机会，这会导致网络负载不平衡，可能降低被卸载UE的吞吐量。因此，如何在提高网络能效的同时维持PRB分配，是一个具有挑战性的任务。

Method: 本文提出一个全面的基于机器学习（ML）的框架，用于联合优化O-RAN部署中的负载均衡和能效。将问题表述为一个多分类系统，预测性评估潜在的RU配置，并将网络状况映射到三种负载平衡类别（良好平衡、适度平衡、不平衡）。采用多阈值方法（保守、适度、激进）以适应能耗节约和性能保障之间的不同操作优先级。具体采用随机森林模型。

Result: 使用426万真实网络测量数据进行实验评估，结果表明随机森林模型实现了98.3%的F1-macro性能，比传统基线策略提升了195%。

Conclusion: 该文提出的基于机器学习的框架能有效实现O-RAN中负载均衡和能效的联合优化，显著优于传统基线策略，解决了现有能效优化方法中负载不平衡的问题。

Abstract: Open Radio Access Network (O-RAN) architecture provides an intrinsic
capability to exploit key performance monitoring (KPM) within Radio
Intelligence Controller (RIC) to derive network optimisation through xApps.
These xApps can leverage KPM knowledge to dynamically switch on/off the
associated RUs where such a function is supported over the E2 interface.
Several existing studies employ artificial intelligence (AI)/Machine Learning
(ML) based approaches to realise such dynamic sleeping for increased energy
efficiency (EE). Nevertheless, most of these approaches rely upon offloading
user equipment (UE) to carve out a sleeping opportunity. Such an approach
inherently creates load imbalance across the network. Such load imbalance may
impact the throughput performance of offloaded UEs as they might be allocated a
lower number of physical resource blocks (PRBs). Maintaining the same PRB
allocation while addressing the EE at the network level is a challenging task.
To that end, in this article, we present a comprehensive ML-based framework for
joint optimisation of load balancing and EE for ORAN deployments. We formulate
the problem as a multi-class classification system that predictively evaluates
potential RU configurations before optimising the EE, mapping network
conditions to three load balance categories (Well Balanced, Moderately
Balanced, Imbalanced). Our multi-threshold approach (Conservative, Moderate,
Aggressive) accommodates different operational priorities between energy
savings and performance assurance. Experimental evaluation using 4.26 million
real network measurements from simulations demonstrates that our Random Forest
model achieves 98.3% F1-macro performance, representing 195% improvement over
traditional baseline strategies.

</details>


### [197] [Toward quantum-safe scalable networks: an open, standards-aware key management framework](https://arxiv.org/abs/2509.09453)
*Ane Sanz,Asier Atutxa,David Franco,Jasone Astorga,Eduardo Jacob,Diego López*

Main category: cs.NI

TL;DR: 本文提出一种将SDN原则与虚拟密钥管理系统(vKMS)和量子安全控制器(QuSeC)相结合的新型网络架构，旨在解决量子密钥分发(QKD)网络的密钥管理、中继路径发现和可扩展性挑战。


<details>
  <summary>Details</summary>
Motivation: 量子计算的出现对现有通信网络安全构成威胁。QKD技术虽能提供无条件安全的密钥，但QKD网络在可扩展性上面临挑战，尤其对于长距离应用。尽管信任中继节点可部分解决距离问题，但中继节点路径的建立仍缺乏解决方案，具体挑战包括KMS识别、中继路径发现和网络可扩展性。

Method: 本文提出一种创新的网络架构，通过整合软件定义网络(SDN)原则，在每个节点建立高层级虚拟KMS (vKMS)并引入量子安全控制器 (QuSeC)。vKMS处理用户密钥请求和管理节点内多个KMS，QuSeC则基于网络拓扑和状态提供端到端中继路径发现和安全策略应用服务。论文还对该架构进行了安全分析。

Result: 该架构成功解决了QKD网络中密钥管理系统(KMS)识别、中继路径发现和可扩展性的挑战。通过安全分析，确定了架构的安全级别并分析了核心网络安全属性。

Conclusion: 该方案通过集成SDN、vKMS和QuSeC，为QKD网络提供了一种可扩展、易管理且安全的密钥分发和中继路径建立机制，有效应对了现有技术瓶颈，是实现未来量子安全通信网络的重要一步。

Abstract: With the advent of quantum computing, the increasing threats to security
poses a great challenge to communication networks. Recent innovations in this
field resulted in promising technologies such as Quantum Key Distribution
(QKD), which enables the generation of unconditionally secure keys,
establishing secure communications between remote nodes. Additionally, QKD
networks enable the interconnection of multinode architectures, extending the
point-to-point nature of QKD. However, due to the limitations of the current
state of technology, the scalability of QKD networks remains a challenge toward
feasible implementations. When it comes to long-distance implementations,
trusted relay nodes partially solve the distance issue through the forwarding
of the distributed keys, allowing applications that do not have a direct QKD
link to securely share key material. Even though the relay procedure itself has
been extensively studied, the establishment of the relaying node path still
lacks a solution. This paper proposes an innovative network architecture that
solves the challenges of Key Management System (KMS) identification, relay path
discovery, and scalability of QKD networks by integrating Software-Defined
Networking (SDN) principles, and establishing high-level virtual KMSs (vKMS) in
each node and creating a new entity called the Quantum Security Controller
(QuSeC). The vKMS serves the end-user key requests, managing the multiple KMSs
within the node and abstracting the user from discovering the correct KMS.
Additionally, based on the high-level view of the network topology and status,
the QuSeC serves the path discovery requests from vKMSs, computing the
end-to-end (E2E) relay path and applying security policies. The paper also
provides a security analysis of the proposal, identifying the security levels
of the architecture and analyzing the core networking security properties.

</details>


### [198] [PARROT: Portable Android Reproducible traffic Observation Tool](https://arxiv.org/abs/2509.09537)
*Andrea Jimenez-Berenguel,Celeste Campo,Marta Moure-Garrido,Carlos Garcia-Rubio,Daniel Díaz-Sanchez,Florina Almenares*

Main category: cs.NI

TL;DR: 本文提出PARROT系统，一个可重现的移动应用流量捕获工具，并利用其收集了新数据集。通过与旧数据集对比，揭示了移动应用流量中TLS、QUIC和DNS协议的显著演变。


<details>
  <summary>Details</summary>
Motivation: 移动安全协议的快速演进以及现有数据集的局限性，制约了应用流量分析的研究。

Method: 开发了PARROT系统，这是一个可重现、便携的应用流量捕获系统，利用Android虚拟设备进行系统性流量收集。它提供自动化环境设置、可配置的Android版本、流量记录管理、带有人机交互的标记捕获提取，并集成了mitmproxy用于可选的流量解密和SSL/TLS密钥自动提取。该系统支持有无流量拦截的灵活捕获模式。使用PARROT收集了80个应用的流量数据集，并与MAppGraph数据集（2021年）进行了对比分析。

Result: 收集了一个包含80个应用的新数据集。与2021年的MAppGraph数据集相比，2025年应用流量模式出现显著演变：TLSv1.3协议的使用率从2021年的6.7%增至2025年的90.0%；QUIC协议的采纳大幅增加，所有50个共有应用在正常网络条件下都生成了QUIC流量，而2021年仅为30个；DNS通信从主要为未加密的Do53协议（2021年91.0%）演变为加密的DoT协议（2025年81.1%）。

Conclusion: 开源的PARROT系统能够实现可重现的应用流量捕获，方便研究社区采用，并为应用安全协议演变提供了深入见解。

Abstract: The rapid evolution of mobile security protocols and limited availability of
current datasets constrains research in app traffic analysis. This paper
presents PARROT, a reproducible and portable traffic capture system for
systematic app traffic collection using Android Virtual Devices. The system
provides automated environment setup, configurable Android versions, traffic
recording management, and labeled captures extraction with human-in-the-loop
app interaction. PARROT integrates mitmproxy for optional traffic decryption
with automated SSL/TLS key extraction, supporting flexible capture modes with
or without traffic interception. We collected a dataset of 80 apps selected
from the MAppGraph dataset list, providing traffic captures with corresponding
SSL keys for decryption analysis. Our comparative analysis between the
MAppGraph dataset (2021) and our dataset (2025) reveals app traffic pattern
evolution across 50 common apps. Key findings include migration from TLSv1.2 to
TLSv1.3 protocol, with TLSv1.3 comprising 90.0\% of TCP encrypted traffic in
2025 compared to 6.7\% in 2021. QUIC protocol adoption increased substantially,
with all 50 common apps generating QUIC traffic under normal network conditions
compared to 30 apps in 2021. DNS communications evolved from predominantly
unencrypted Do53 protocol (91.0\% in 2021) to encrypted DoT protocol (81.1\% in
2025). The open-source PARROT system enables reproducible app traffic capture
for research community adoption and provides insights into app security
protocol evolution.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [199] [Explainable AI for Accelerated Microstructure Imaging: A SHAP-Guided Protocol on the Connectome 2.0 scanner](https://arxiv.org/abs/2509.09513)
*Quentin Uhl,Tommaso Pavan,Julianna Gerold,Kwok-Shing Chan,Yohan Jun,Shohei Fujita,Aneri Bhatt,Yixin Ma,Qiaochu Wang,Hong-Hsi Lee,Susie Y. Huang,Berkin Bilgic,Ileana Jelescu*

Main category: physics.med-ph

TL;DR: 本研究提出一种基于可解释人工智能的数据驱动框架，将扩散MRI神经突交换成像(NEXI)的扫描时间从长时间显著缩短至14分钟，同时保持模型准确性和鲁棒性，促进其在神经科学和临床研究中的广泛应用。


<details>
  <summary>Details</summary>
Motivation: 扩散MRI神经突交换成像(NEXI)模型在探测灰质微结构方面具有潜力，但现有采集方案扫描时间过长，限制了其在实际应用中的可行性。

Method: 开发了一种基于可解释人工智能(XAI)的数据驱动框架，结合引导递归特征消除策略，从15个特征的方案中识别出最佳的8个特征子集。该优化方案在体内进行了验证，评估了参数准确性、解剖对比度保持性以及重测再现性，并与完整采集方案及其他简化策略进行了基准比较。

Result: 简化方案的参数估计和皮层图谱与完整方案相当，在合成数据中表现出低估计误差，对重测再现性影响极小。与理论驱动和启发式简化方案相比，优化方案表现出卓越的鲁棒性，将水交换时间估计的偏差降低了两倍以上。

Conclusion: 该混合优化框架实现了14分钟内可行的神经突交换成像，且不损失参数保真度，支持交换敏感扩散磁共振成像在神经科学和临床研究中的更广泛应用。此外，它还提供了一种通用的方法来设计生物物理参数映射中高效的采集协议。

Abstract: The diffusion MRI Neurite Exchange Imaging model offers a promising framework
for probing gray matter microstructure by estimating parameters such as
compartment sizes, diffusivities, and inter-compartmental water exchange time.
However, existing protocols require long scan times. This study proposes a
reduced acquisition scheme for the Connectome 2.0 scanner that preserves model
accuracy while substantially shortening scan duration. We developed a
data-driven framework using explainable artificial intelligence with a guided
recursive feature elimination strategy to identify an optimal 8-feature subset
from a 15-feature protocol. The performance of this optimized protocol was
validated in vivo and benchmarked against the full acquisition and alternative
reduction strategies. Parameter accuracy, preservation of anatomical contrast,
and test-retest reproducibility were assessed. The reduced protocol yielded
parameter estimates and cortical maps comparable to the full protocol, with low
estimation errors in synthetic data and minimal impact on test-retest
variability. Compared to theory-driven and heuristic reduction schemes, the
optimized protocol demonstrated superior robustness, reducing the deviation in
water exchange time estimates by over two-fold. In conclusion, this hybrid
optimization framework enables viable imaging of neurite exchange in 14 minutes
without loss of parameter fidelity. This approach supports the broader
application of exchange-sensitive diffusion magnetic resonance imaging in
neuroscience and clinical research, and offers a generalizable method for
designing efficient acquisition protocols in biophysical parameter mapping.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [200] [Personalized Sleep Prediction via Deep Adaptive Spatiotemporal Modeling and Sparse Data](https://arxiv.org/abs/2509.09018)
*Xueyi Wang,C. J. C.,Lamoth,Elisabeth Wilhelm*

Main category: eess.SP

TL;DR: 本文提出AdaST-Sleep模型，结合CNN、RNN和领域分类器，利用可穿戴设备数据实现个性化睡眠分数预测，效果优于基线模型，且具有鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 睡眠预测能帮助个人和医疗服务提供者预先管理影响睡眠的因素，从而改善身心健康。

Method: 提出了一个自适应时空模型(AdaST-Sleep)来预测睡眠分数。该模型结合了卷积层以捕捉多特征间的空间交互，循环神经网络层以处理长期的健康时间序列数据，并集成了一个领域分类器以实现跨不同受试者的泛化能力。

Result: AdaST-Sleep模型在不同输入和预测窗口尺寸下，始终优于四种基线模型，最低RMSE达到0.282（七天输入窗口，一天预测窗口）。即使预测未来多天，模型仍保持强劲性能，且能准确追踪整体睡眠分数水平和日常波动。

Conclusion: 所提出的框架为利用商用可穿戴设备稀疏数据和领域适应技术进行个性化睡眠预测提供了一个鲁棒且适应性强的解决方案。

Abstract: A sleep forecast allows individuals and healthcare providers to anticipate
and proactively address factors influencing restful rest, ultimately improving
mental and physical well-being. This work presents an adaptive spatial and
temporal model (AdaST-Sleep) for predicting sleep scores. Our proposed model
combines convolutional layers to capture spatial feature interactions between
multiple features and recurrent neural network layers to handle longer-term
temporal health-related data. A domain classifier is further integrated to
generalize across different subjects. We conducted several experiments using
five input window sizes (3, 5, 7, 9, 11 days) and five predicting window sizes
(1, 3, 5, 7, 9 days). Our approach consistently outperformed four baseline
models, achieving its lowest RMSE (0.282) with a seven-day input window and a
one-day predicting window. Moreover, the method maintained strong performance
even when forecasting multiple days into the future, demonstrating its
versatility for real-world applications. Visual comparisons reveal that the
model accurately tracks both the overall sleep score level and daily
fluctuations. These findings prove that the proposed framework provides a
robust and adaptable solution for personalized sleep forecasting using sparse
data from commercial wearable devices and domain adaptation techniques.

</details>


### [201] [A Masked Representation Learning to Model Cardiac Functions Using Multiple Physiological Signals](https://arxiv.org/abs/2509.08830)
*Seong-A Park,Jong-Eui Chae,Sungdong Kim,Hyung-Chul Lee,Hyun-Lim Yang*

Main category: eess.SP

TL;DR: 本研究提出了SNUPHY-M模型，利用多模态自监督学习（SSL）整合ECG、PPG和ABP信号以提取心血管生理特征。在多项临床下游任务中，该模型表现显著优于传统方法，尤其在使用无创信号进行预测时，为无创血流动力学诊断与管理提供了有效支持。


<details>
  <summary>Details</summary>
Motivation: 临床上血流动力学监测对患者预后至关重要，但现有研究多集中于单一生理信号分析（如ECG或PPG），缺乏能有效整合多个复杂生理信号以应对实际临床场景需求的方法。

Method: 本研究引入了SNUPHY-M模型，该模型基于自监督学习（SSL）原理，通过恢复被遮蔽的ECG、PPG和动脉血压（ABP）三种生理信号，来提取反映心脏周期电气、压力和流体特性的生理特征。模型在低血压、搏出量、收缩压、舒张压和年龄预测等临床下游任务中进行了性能评估。

Result: SNUPHY-M模型在多项临床下游任务中显著优于监督学习或现有自监督学习模型，尤其在使用无创信号（如ECG和PPG）进行预测时，表现出更优的性能。

Conclusion: SNUPHY-M是首个将多模态自监督学习应用于包含ECG、PPG和ABP信号的心血管分析模型。该方法能有效支持临床决策和精确诊断，对无创血流动力学的早期诊断和管理具有重要贡献。

Abstract: In clinical settings, monitoring hemodynamics is crucial for managing patient
prognosis, necessitating the integrated analysis of multiple physiological
signals. While recent research has analyzed single signals such as
electrocardiography (ECG) or photoplethysmography (PPG), there has yet to be a
proposal for an approach that encompasses the complex signal analysis required
in actual clinical scenarios. In this study, we introduce the SNUPHY-M (Seoul
National University hospital PHYsiological signal Masked representation
learning) model extracts physiological features reflecting the electrical,
pressure, and fluid characteristics of the cardiac cycle in the process of
restoring three masked physiological signals based on self-supervised learning
(SSL): ECG, PPG, and arterial blood pressure (ABP) signals. By employing
multiple physical characteristics, the model can extract more enriched features
only using non-invasive signals. We evaluated the model's performance in
clinical downstream tasks such as hypotension, stroke volume, systolic blood
pressure, diastolic blood pressure, and age prediction. Our results showed that
the SNUPHY-M significantly outperformed supervised or SSL models, especially in
prediction tasks using non-invasive signals. To the best of our knowledge,
SNUPHY-M is the first model to apply multi-modal SSL to cardiovascular analysis
involving ECG, PPG, and ABP signals. This approach effectively supports
clinical decision-making and enables precise diagnostics, contributing
significantly to the early diagnosis and management of hemodynamics without
invasiveness.

</details>


### [202] [Deploying AI for Signal Processing education: Selected challenges and intriguing opportunities](https://arxiv.org/abs/2509.08950)
*Jarvis Haupt,Qin Lu,Yanning Shen,Jia Chen,Yue Dong,Dan McCreary,Mehmet Akçakaya,Georgios B. Giannakis*

Main category: eess.SP

TL;DR: 本文探讨了人工智能工具在教育领域的负责任应用，尤其关注信号处理学科，旨在解决技术局限并提升学习体验，并通过“智能教科书”案例进行阐释。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能工具取得了巨大进步，但在其公平、负责任的应用以真正改善全球人类状况方面仍面临深刻挑战。本文旨在探索利用AI工具促进和增强教育（特别是在信号处理领域）的潜力，并同时解决相关的技术限制和实际应用问题。

Method: 本文从两个相互关联的视角展开：一是识别并解决技术局限（如公平性、包容性、幻觉输出处理、资源效率），二是将AI工具应用于实践以改善教育体验。这些考虑因素通过开发一本沉浸式、结构化且可靠的“智能教科书”来具体说明，以展示透明度、可解释性和可信赖性等。

Result: 本文提供了在教育环境中使用AI时出现的一些核心技术问题的基础知识，包括如何确保公平性和包容性、处理AI幻觉输出以及实现资源高效利用。这些以及其他考量（如透明度、可解释性和可信赖性）通过开发一本沉浸式、结构化且可靠的“智能教科书”得到了阐释。

Conclusion: 本文旨在为寻求推进AI在工程教育中作用的研究人员和教育工作者提供一份资源，指导他们如何负责任地利用AI工具提升学习体验，尤其是在信号处理领域。

Abstract: Powerful artificial intelligence (AI) tools that have emerged in recent years
-- including large language models, automated coding assistants, and advanced
image and speech generation technologies -- are the result of monumental human
achievements. These breakthroughs reflect mastery across multiple technical
disciplines and the resolution of significant technological challenges.
However, some of the most profound challenges may still lie ahead. These
challenges are not purely technical but pertain to the fair and responsible use
of AI in ways that genuinely improve the global human condition. This article
explores one promising application aligned with that vision: the use of AI
tools to facilitate and enhance education, with a specific focus on signal
processing (SP). It presents two interrelated perspectives: identifying and
addressing technical limitations, and applying AI tools in practice to improve
educational experiences. Primers are provided on several core technical issues
that arise when using AI in educational settings, including how to ensure
fairness and inclusivity, handle hallucinated outputs, and achieve efficient
use of resources. These and other considerations -- such as transparency,
explainability, and trustworthiness -- are illustrated through the development
of an immersive, structured, and reliable "smart textbook." The article serves
as a resource for researchers and educators seeking to advance AI's role in
engineering education.

</details>


### [203] [Ultrafast Deep Learning-Based Scatter Estimation in Cone-Beam Computed Tomography](https://arxiv.org/abs/2509.08973)
*Harshit Agrawal,Ari Hietanen,Simo Särkkä*

Main category: eess.SP

TL;DR: 本研究通过在不同分辨率下应用深度学习网络，显著降低了CBCT散射估计的计算资源需求，使其适用于移动和边缘设备。


<details>
  <summary>Details</summary>
Motivation: 锥形束CT (CBCT) 中的散射伪影严重影响图像质量。尽管深度学习方法在散射估计方面有潜力，但其高内存占用限制了在移动CBCT系统或边缘设备上的部署。

Method: 首先，研究在六种分辨率下检查了CBCT散射信号降采样-上采样的重建误差，并比较了四种插值方法。随后，在五种图像分辨率下训练了一种先进的深度学习方法，并评估了其浮点运算 (FLOPs)、推理时间及GPU内存需求。

Result: 与基线方法相比，该方法实现了FLOPs减少78倍，同时保持了可比的性能（MAPE从4.42%降至3.85%，MSE从2.01e-2降至1.34e-2）。推理时间减少了16倍，GPU内存使用减少了12倍。在模拟数据集和真实CBCT扫描中均验证了该方法的鲁棒性。

Conclusion: 降采样在基于深度学习的散射估计中发挥着被低估的作用。本研究方法显著减少了FLOPs和GPU内存需求，使得散射校正能够在移动CBCT和边缘设备等资源受限环境中实现。

Abstract: Purpose: Scatter artifacts drastically degrade the image quality of cone-beam
computed tomography (CBCT) scans. Although deep learning-based methods show
promise in estimating scatter from CBCT measurements, their deployment in
mobile CBCT systems or edge devices is still limited due to the large memory
footprint of the networks. This study addresses the issue by applying networks
at varying resolutions and suggesting an optimal one, based on speed and
accuracy.
  Methods: First, the reconstruction error in down-up sampling of CBCT scatter
signal was examined at six resolutions by comparing four interpolation methods.
Next, a recent state-of-the-art method was trained across five image
resolutions and evaluated for the reductions in floating-point operations
(FLOPs), inference times, and GPU memory requirements.
  Results: Reducing the input size and network parameters achieved a 78-fold
reduction in FLOPs compared to the baseline method, while maintaining comarable
performance in terms of mean-absolute-percentage-error (MAPE) and
mean-square-error (MSE). Specifically, the MAPE decreased to 3.85% compared to
4.42%, and the MSE decreased to 1.34 \times 10^{-2} compared to 2.01 \times
10^{-2}. Inference time and GPU memory usage were reduced by factors of 16 and
12, respectively. Further experiments comparing scatter-corrected
reconstructions on a large, simulated dataset and real CBCT scans from water
and Sedentex CT phantoms clearly demonstrated the robustness of our method.
  Conclusion: This study highlights the underappreciated role of downsampling
in deep learning-based scatter estimation. The substantial reduction in FLOPs
and GPU memory requirements achieved by our method enables scatter correction
in resource-constrained environments, such as mobile CBCT and edge devices.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [204] [The Role of Community Detection Methods in Performance Variations of Graph Mining Tasks](https://arxiv.org/abs/2509.09045)
*Shrabani Ghosh,Erik Saule*

Main category: cs.SI

TL;DR: 本研究系统评估了不同社区检测算法对下游图挖掘任务性能的影响，发现算法选择对结果有显著影响。


<details>
  <summary>Details</summary>
Motivation: 在大型图分析中，现实世界缺乏社区检测的真值和普遍接受的标准，且没有单一方法能始终最优。社区检测方法的选择往往未充分考虑其对下游任务的潜在影响。因此，本研究旨在探究社区检测算法的选择是否会显著影响下游应用的性能。

Method: 本文提出了一个能够整合各种社区检测方法的框架，用于系统评估它们对下游任务结果的影响。通过比较分析来验证算法选择的重要性。

Result: 比较分析结果显示，在特定应用中，某些社区检测算法能产生更优异的结果，这表明方法选择会显著影响下游任务的性能。

Conclusion: 社区检测算法的选择对下游任务的性能具有实质性影响，且效果因应用而异，强调了在实际应用中仔细选择算法的重要性。

Abstract: In real-world scenarios, large graphs represent relationships among entities
in complex systems. Mining these large graphs often containing millions of
nodes and edges helps uncover structural patterns and meaningful insights.
Dividing a large graph into smaller subgraphs facilitates complex system
analysis by revealing local information. Community detection extracts clusters
or communities of graphs based on statistical methods and machine learning
models using various optimization techniques. Structure based community
detection methods are more suitable for applying to graphs because they do not
rely heavily on rich node or edge attribute information. The features derived
from these communities can improve downstream graph mining tasks, such as link
prediction and node classification. In real-world applications, we often lack
ground truth community information. Additionally, there is neither a
universally accepted gold standard for community detection nor a single method
that is consistently optimal across diverse applications. In many cases, it is
unclear how practitioners select community detection methods, and choices are
often made without explicitly considering their potential impact on downstream
tasks. In this study, we investigate whether the choice of community detection
algorithm significantly influences the performance of downstream applications.
We propose a framework capable of integrating various community detection
methods to systematically evaluate their effects on downstream task outcomes.
Our comparative analysis reveals that specific community detection algorithms
yield superior results in certain applications, highlighting that method
selection substantially affects performance.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [205] [Towards A High-Performance Quantum Data Center Network Architecture](https://arxiv.org/abs/2509.09653)
*Yufeng Xin,Liang Zhang*

Main category: quant-ph

TL;DR: 本文提出一种三层胖树网络架构，用于模块化量子数据中心（QDC），以解决网络可扩展性、纠缠生成和量子内存管理挑战，并通过排队理论模型和NetSquid仿真验证了其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大型量子计算机受技术和财务限制，模块化量子计算机集群提供了一种替代方案。然而，这种模块化方法引入了网络可扩展性、纠缠生成和量子内存管理方面的新挑战，需要新的QDC架构来应对。

Method: 本文提出了一种三层胖树网络架构，专为QDC设计。该架构包含独特的叶交换机和先进的交换主干交换机，旨在处理高容量的纠缠请求；同时采用队列调度机制，有效管理量子内存以防止退相干。通过排队理论模型和NetSquid仿真对所提出的架构进行了评估。

Result: 研究结果表明，所提出的QDC架构具有良好的可扩展性，并能有效保持高纠缠保真度。

Conclusion: 该架构为模块化量子数据中心网络提供了一条可行的前进路径。

Abstract: Quantum Data Centers (QDCs) are needed to support large-scale quantum
processing for both academic and commercial applications. While large-scale
quantum computers are constrained by technological and financial barriers, a
modular approach that clusters small quantum computers offers an alternative.
This approach, however, introduces new challenges in network scalability,
entanglement generation, and quantum memory management. In this paper, we
propose a three-layer fat-tree network architecture for QDCs, designed to
address these challenges. Our architecture features a unique leaf switch and an
advanced swapping spine switch design, optimized to handle high volumes of
entanglement requests as well as a queue scheduling mechanism that efficiently
manages quantum memory to prevent decoherence. Through queuing-theoretical
models and simulations in NetSquid, we demonstrate the proposed architecture's
scalability and effectiveness in maintaining high entanglement fidelity,
offering a practical path forward for modular QDC networks.

</details>


### [206] [Generative quantum advantage for classical and quantum problems](https://arxiv.org/abs/2509.09033)
*Hsin-Yuan Huang,Michael Broughton,Norhan Eassa,Hartmut Neven,Ryan Babbush,Jarrod R. McClean*

Main category: quant-ph

TL;DR: 本文提出并验证了一系列高效可训练的生成式量子模型，克服了量子学习的复杂性，在68量子位处理器上首次展示了生成式量子优势，能学习经典不可解分布并加速物理模拟。


<details>
  <summary>Details</summary>
Motivation: 现有超越经典的量子实验因学习复杂性，未能实现高效学习和展示“生成式量子优势”，即量子计算机在学习和生成方面超越经典计算机的能力。

Method: 引入了一系列新的生成式量子模型，这些模型具有经典模拟困难、高效可训练、无贫瘠高原和局部最小值等特点。研究人员使用一个68量子位超导量子处理器，在学习经典不可解概率分布和学习量子电路以加速物理模拟两个场景中验证了这些能力。

Result: 研究结果表明，这些新型生成式量子模型在68量子位超导量子处理器上成功学习了经典不可解的概率分布，并能学习量子电路以加速物理模拟。

Conclusion: 研究确立了在超越经典计算的范畴内，学习和采样都可高效完成，这为开发具有可证明优势的量子增强型生成模型开辟了新的途径。

Abstract: Recent breakthroughs in generative machine learning, powered by massive
computational resources, have demonstrated unprecedented human-like
capabilities. While beyond-classical quantum experiments can generate samples
from classically intractable distributions, their complexity has thwarted all
efforts toward efficient learning. This challenge has hindered demonstrations
of generative quantum advantage: the ability of quantum computers to learn and
generate desired outputs substantially better than classical computers. We
resolve this challenge by introducing families of generative quantum models
that are hard to simulate classically, are efficiently trainable, exhibit no
barren plateaus or proliferating local minima, and can learn to generate
distributions beyond the reach of classical computers. Using a $68$-qubit
superconducting quantum processor, we demonstrate these capabilities in two
scenarios: learning classically intractable probability distributions and
learning quantum circuits for accelerated physical simulation. Our results
establish that both learning and sampling can be performed efficiently in the
beyond-classical regime, opening new possibilities for quantum-enhanced
generative models with provable advantage.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [207] [Database Views as Explanations for Relational Deep Learning](https://arxiv.org/abs/2509.09482)
*Agapi Rissaki,Ilias Fountalis,Wolfgang Gatterbauer,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 该论文提出一种新颖的框架，通过可解释的视图定义来解释关系数据库上深度学习模型（特别是异构GNN）的预测，解决模型复杂性导致的黑箱问题。


<details>
  <summary>Details</summary>
Motivation: 近年来，关系数据库上的深度学习模型（如异构GNN和异构图Transformer）取得了显著进展，但其复杂性使得难以用人类可理解的方式解释模型如何利用数据进行预测。

Method: 本文提出一个框架，通过“视图定义”来解释关系数据库上的机器学习模型。这些视图定义突出显示数据库中对模型预测贡献最大的焦点部分。该方法通过Nash等人的“确定性”概念建立全局溯因解释，并允许调整确定性与简洁性之间的权衡，以及控制解释的粒度。文章特别针对异构GNNs进行了研究，开发了启发式算法以避免穷举搜索，并提出了模型无关和针对异构GNNs（通过可学习掩码）的特定技术。

Result: 通过在RelBench数据集上进行广泛的实证研究，涵盖了多种领域和记录级任务，结果证明了所提出的解释方法的实用性和其生成效率。

Conclusion: 该研究成功提供了一个有效的框架，能够为关系数据库上的复杂深度学习模型生成有用且高效的、人类可理解的解释，显著提升了模型的可解释性。

Abstract: In recent years, there has been significant progress in the development of
deep learning models over relational databases, including architectures based
on heterogeneous graph neural networks (hetero-GNNs) and heterogeneous graph
transformers. In effect, such architectures state how the database records and
links (e.g., foreign-key references) translate into a large, complex numerical
expression, involving numerous learnable parameters. This complexity makes it
hard to explain, in human-understandable terms, how a model uses the available
data to arrive at a given prediction. We present a novel framework for
explaining machine-learning models over relational databases, where
explanations are view definitions that highlight focused parts of the database
that mostly contribute to the model's prediction. We establish such global
abductive explanations by adapting the classic notion of determinacy by Nash,
Segoufin, and Vianu (2010). In addition to tuning the tradeoff between
determinacy and conciseness, the framework allows controlling the level of
granularity by adopting different fragments of view definitions, such as ones
highlighting whole columns, foreign keys between tables, relevant groups of
tuples, and so on. We investigate the realization of the framework in the case
of hetero-GNNs. We develop heuristic algorithms that avoid the exhaustive
search over the space of all databases. We propose techniques that are
model-agnostic, and others that are tailored to hetero-GNNs via the notion of
learnable masking. Our approach is evaluated through an extensive empirical
study on the RelBench collection, covering a variety of domains and different
record-level tasks. The results demonstrate the usefulness of the proposed
explanations, as well as the efficiency of their generation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [208] [A Cyber-Twin Based Honeypot for Gathering Threat Intelligence](https://arxiv.org/abs/2509.09222)
*Muhammad Azmi Umer,Zhan Xuna,Yan Lin Aung,Aditya P. Mathur,Jianying Zhou*

Main category: cs.CR

TL;DR: 本文描述了一种基于数字孪生的水处理厂诱捕系统，旨在吸引并分析网络攻击，从而为关键基础设施提供威胁情报以增强其保护能力。


<details>
  <summary>Details</summary>
Motivation: 关键基础设施（CI）易受网络攻击，需要获取威胁情报以改进其保护系统。

Method: 开发并部署了一个基于水处理厂数字孪生技术的诱捕系统（honeypot），该系统作为真实副本吸引潜在攻击者，并记录、分析所发起的攻击以生成威胁情报。

Result: 该诱捕系统已投入运行，并成功吸引并记录了多次网络攻击，其中包括一次详细的勒索软件攻击。

Conclusion: 所开发的诱捕系统能够有效生成威胁情报，这些情报可用于水处理厂的管理层以改进其保护系统，从而增强对网络攻击的防御能力。

Abstract: Critical Infrastructure (CI) is prone to cyberattacks. Several techniques
have been developed to protect CI against such attacks. In this work, we
describe a honeypot based on a cyber twin for a water treatment plant. The
honeypot is intended to serve as a realistic replica of a water treatment plant
that attracts potential attackers. The attacks launched on the honeypot are
recorded and analyzed for threat intelligence. The intelligence so obtained is
shared with the management of water treatment plants, who in turn may use it to
improve plant protection systems. The honeypot used here is operational and has
been attacked on several occasions using, for example, a ransomware attack that
is described in detail.

</details>


### [209] [What You Code Is What We Prove: Translating BLE App Logic into Formal Models with LLMs for Vulnerability Detection](https://arxiv.org/abs/2509.09291)
*Biwei Yan,Yue Zhang,Minghui Xu,Runyu Pan,Jinku Li,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: BLE应用层普遍存在安全漏洞，传统形式验证效率低。本文提出使用LLM将BLE代码翻译成形式模型，实现大规模自动化验证，发现多数BLE应用缺乏核心安全保护。


<details>
  <summary>Details</summary>
Motivation: 蓝牙低功耗（BLE）应用层因开发者忽视加密、认证等关键保护，成为日益增长的安全漏洞来源。传统形式化验证因手动建模工作量大，不适用于大规模分析。

Method: 本文提出将BLE应用安全分析重构为语义翻译问题，利用大型语言模型（LLMs）将BLE代码转换为ProVerif等工具可验证的形式化进程模型。开发了VerifiaBLE系统，结合静态分析、提示引导的LLM翻译和符号验证，检查加密、随机性和认证三项核心安全功能。

Result: 将VerifiaBLE应用于1,050个Android BLE应用，发现系统性弱点：仅有10.2%的应用实现了所有三项保护，而53.9%的应用完全没有实现这些保护。

Conclusion: 研究表明，将LLMs用作结构化翻译器可以降低形式化方法的门槛，从而在安全关键领域实现可扩展的验证。

Abstract: The application layer of Bluetooth Low Energy (BLE) is a growing source of
security vulnerabilities, as developers often neglect to implement critical
protections such as encryption, authentication, and freshness. While formal
verification offers a principled way to check these properties, the manual
effort of constructing formal models makes it impractical for large-scale
analysis. This paper introduces a key insight: BLE application security
analysis can be reframed as a semantic translation problem, i.e., from
real-world code to formal models. We leverage large language models (LLMs) not
to directly detect vulnerabilities, but to serve as translators that convert
BLE-specific code into process models verifiable by tools like ProVerif. We
implement this idea in VerifiaBLE, a system that combines static analysis,
prompt-guided LLM translation, and symbolic verification to check three core
security features: encryption, randomness, and authentication. Applied to 1,050
Android BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\% of apps
implement all three protections, while 53.9\% omit them entirely. Our work
demonstrates that using LLMs as structured translators can lower the barrier to
formal methods, unlocking scalable verification across security-critical
domains.

</details>


### [210] [Towards Confidential and Efficient LLM Inference with Dual Privacy Protection](https://arxiv.org/abs/2509.09091)
*Honglan Yu,Yibin Wang,Feifei Dai,Dong Liu,Haihui Fan,Xiaoyan Gu*

Main category: cs.CR

TL;DR: CMIF是一种机密高效的模型推理框架，它将LLM嵌入层部署在客户端TEE中，后续层部署在GPU服务器上，并优化了差分隐私机制以保护输入隐私，同时降低了推理开销并保持了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于TEE和差分隐私(DP)的隐私推理方法存在局限。CPU-based TEEs在推理时延迟高，尤其对于LLM的密集非线性层，TEE与GPU之间通信开销大。而DP方法通过添加随机噪声来保护数据隐私，但会损害LLM的性能和语义理解能力。

Method: 本文提出了CMIF（Confidential and efficient Model Inference Framework）。CMIF将模型的嵌入层部署在客户端的TEE中以保证机密性，而将后续层卸载到GPU服务器上进行高效计算。同时，它优化了Report-Noisy-Max机制，以在保护敏感输入隐私的同时，将模型性能下降程度降至最低。

Result: 在Llama系列模型上的大量实验表明，CMIF减少了TEE中额外的推理开销，同时有效保护了用户数据隐私。

Conclusion: CMIF通过创新的层分区策略和优化的差分隐私机制，有效解决了现有方法在LLM隐私推理中面临的高延迟和性能下降问题，提供了一种机密且高效的模型推理解决方案。

Abstract: CPU-based trusted execution environments (TEEs) and differential privacy (DP)
have gained wide applications for private inference. Due to high inference
latency in TEEs, researchers use partition-based approaches that offload linear
model components to GPUs. However, dense nonlinear layers of large language
models (LLMs) result in significant communication overhead between TEEs and
GPUs. DP-based approaches apply random noise to protect data privacy, but this
compromises LLM performance and semantic understanding. To overcome the above
drawbacks, this paper proposes CMIF, a Confidential and efficient Model
Inference Framework. CMIF confidentially deploys the embedding layer in the
client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes
the Report-Noisy-Max mechanism to protect sensitive inputs with a slight
decrease in model performance. Extensive experiments on Llama-series models
demonstrate that CMIF reduces additional inference overhead in TEEs while
preserving user data privacy.

</details>


### [211] [DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models](https://arxiv.org/abs/2509.09097)
*Honghui Xu,Shiva Shrestha,Wei Chen,Zhiyuan Li,Zhipeng Cai*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As on-device large language model (LLM) systems become increasingly
prevalent, federated fine-tuning enables advanced language understanding and
generation directly on edge devices; however, it also involves processing
sensitive, user-specific data, raising significant privacy concerns within the
federated learning framework. To address these challenges, we propose
DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates
LoRA-based adaptation with differential privacy in a communication-efficient
setting. Each client locally clips and perturbs its LoRA matrices using
Gaussian noise to satisfy ($\epsilon$, $\delta$)-differential privacy. We
further provide a theoretical analysis demonstrating the unbiased nature of the
updates and deriving bounds on the variance introduced by noise, offering
practical guidance for privacy-budget calibration. Experimental results across
mainstream benchmarks show that DP-FedLoRA delivers competitive performance
while offering strong privacy guarantees, paving the way for scalable and
privacy-preserving LLM deployment in on-device environments.

</details>


### [212] [Character-Level Perturbations Disrupt LLM Watermarks](https://arxiv.org/abs/2509.09112)
*Zhaoxi Zhang,Xiaomei Zhang,Yanjun Zhang,He Zhang,Shirui Pan,Bo Liu,Asif Qumer Gill,Leo Yu Zhang*

Main category: cs.CR

TL;DR: 本文揭示了大型语言模型（LLM）水印在字符级扰动下的显著脆弱性，并提出了有效的去水印攻击方法，强调了开发更鲁棒水印机制的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 先前的LLM水印鲁棒性评估方法存在局限，导致人们误认为有效去水印需要大量扰动或强大攻击者；现有LLM水印方案存在潜在漏洞，需要更现实的威胁模型和攻击策略来评估其安全性。

Method: 形式化LLM水印系统模型，并定义了两种受限于检测器访问的现实威胁模型；分析了不同类型扰动（特别是字符级扰动）的攻击范围及其对分词过程的影响；提出基于遗传算法（GA）并使用参考检测器优化的引导式去水印攻击；提出了一种自适应复合字符级攻击。

Result: 字符级扰动通过干扰分词过程，在最严格的威胁模型下也表现出显著更有效的去水印能力；基于GA的引导式攻击在有限黑盒查询的实际威胁模型下展现出强大的去水印性能；实验证实了字符级扰动的优越性和GA的有效性；发现存在“对抗性困境”，即任何固定防御都可能被适当的扰动策略规避；自适应复合字符级攻击能有效击败防御。

Conclusion: 现有LLM水印方案存在显著漏洞，迫切需要开发新的、更鲁棒的水印机制。

Abstract: Large Language Model (LLM) watermarking embeds detectable signals into
generated text for copyright protection, misuse prevention, and content
detection. While prior studies evaluate robustness using watermark removal
attacks, these methods are often suboptimal, creating the misconception that
effective removal requires large perturbations or powerful adversaries.
  To bridge the gap, we first formalize the system model for LLM watermark, and
characterize two realistic threat models constrained on limited access to the
watermark detector. We then analyze how different types of perturbation vary in
their attack range, i.e., the number of tokens they can affect with a single
edit. We observe that character-level perturbations (e.g., typos, swaps,
deletions, homoglyphs) can influence multiple tokens simultaneously by
disrupting the tokenization process. We demonstrate that character-level
perturbations are significantly more effective for watermark removal under the
most restrictive threat model. We further propose guided removal attacks based
on the Genetic Algorithm (GA) that uses a reference detector for optimization.
Under a practical threat model with limited black-box queries to the watermark
detector, our method demonstrates strong removal performance. Experiments
confirm the superiority of character-level perturbations and the effectiveness
of the GA in removing watermarks under realistic constraints. Additionally, we
argue there is an adversarial dilemma when considering potential defenses: any
fixed defense can be bypassed by a suitable perturbation strategy. Motivated by
this principle, we propose an adaptive compound character-level attack.
Experimental results show that this approach can effectively defeat the
defenses. Our findings highlight significant vulnerabilities in existing LLM
watermark schemes and underline the urgency for the development of new robust
mechanisms.

</details>


### [213] [CryptGNN: Enabling Secure Inference for Graph Neural Networks](https://arxiv.org/abs/2509.09107)
*Pritam Sen,Yao Ma,Cristian Borcea*

Main category: cs.CR

TL;DR: CryptGNN是一个安全有效的云端第三方图神经网络(GNN)推理解决方案，利用分布式安全多方计算(SMPC)保护客户端数据和模型参数的隐私。


<details>
  <summary>Details</summary>
Motivation: 在将机器学习即服务(MLaaS)应用于云端第三方GNN模型时，存在客户端输入数据、图结构以及模型参数泄露给云服务提供商、模型所有者或客户端的隐私风险。

Method: CryptGNN通过使用分布式安全多方计算(SMPC)技术，实现了安全的图消息传递和特征转换层。它支持任意数量的SMPC参与方，无需可信服务器，并且在P个参与方中有P-1个串通的情况下也能提供可证明的安全性。

Result: CryptGNN成功保护了客户端的输入数据和图结构不被云服务提供商和第三方模型所有者获取，同时保护了模型参数不被云服务提供商和客户端获取。理论分析和实证实验均证明了CryptGNN的安全性和效率。

Conclusion: CryptGNN提供了一种新颖、安全且高效的解决方案，用于在云环境中进行隐私保护的GNN模型推理，有效解决了MLaaS场景下的多方隐私泄露问题。

Abstract: We present CryptGNN, a secure and effective inference solution for
third-party graph neural network (GNN) models in the cloud, which are accessed
by clients as ML as a service (MLaaS). The main novelty of CryptGNN is its
secure message passing and feature transformation layers using distributed
secure multi-party computation (SMPC) techniques. CryptGNN protects the
client's input data and graph structure from the cloud provider and the
third-party model owner, and it protects the model parameters from the cloud
provider and the clients. CryptGNN works with any number of SMPC parties, does
not require a trusted server, and is provably secure even if P-1 out of P
parties in the cloud collude. Theoretical analysis and empirical experiments
demonstrate the security and efficiency of CryptGNN.

</details>


### [214] [ENSI: Efficient Non-Interactive Secure Inference for Large Language Models](https://arxiv.org/abs/2509.09424)
*Zhiyu He,Maojiang Wang,Xinwen Gao,Yuchuan Luo,Lin Liu,Shaojing Fu*

Main category: cs.CR

TL;DR: ENSI是一个为LLM设计的非交互式安全推理框架，通过协同设计加密协议和LLM架构，显著提高了加密矩阵乘法和Softmax的效率，并大幅减少了同态加密中Bootstrapping的开销。


<details>
  <summary>Details</summary>
Motivation: 安全推理通过密码学协议支持在敏感用户数据上进行计算而不暴露数据，实现了隐私保护的机器学习。然而，将这些复杂的密码学协议与大规模参数和复杂架构的LLM集成时，面临巨大的挑战，严重限制了其实用性。

Method: 本文提出ENSI框架，通过协同设计加密协议和LLM架构实现LLM的非交互式安全推理。具体方法包括：1) 优化编码策略，将CKKS方案与轻量级LLM变体BitNet无缝集成，以显著降低加密矩阵乘法的计算复杂度。2) 率先将Sigmoid注意力机制与同态加密(HE)集成，作为Softmax在HE下计算成本过高的无重训练替代方案。3) 将Bootstrapping操作嵌入到RMSNorm过程中，高效刷新密文同时显著降低昂贵Bootstrapping调用的频率。

Result: 实验评估表明，在CPU上，ENSI相较于现有最先进方法，在矩阵乘法中实现了约8倍的加速，在Softmax推理中实现了2.6倍的加速，并且Bootstrapping的比例降低至仅1%。

Conclusion: ENSI通过创新的协同设计方法，显著提升了LLM安全推理的效率和实用性，解决了将同态加密与大型语言模型结合时的主要性能瓶颈。

Abstract: Secure inference enables privacy-preserving machine learning by leveraging
cryptographic protocols that support computations on sensitive user data
without exposing it. However, integrating cryptographic protocols with large
language models (LLMs) presents significant challenges, as the inherent
complexity of these protocols, together with LLMs' massive parameter scale and
sophisticated architectures, severely limits practical usability. In this work,
we propose ENSI, a novel non-interactive secure inference framework for LLMs,
based on the principle of co-designing the cryptographic protocols and LLM
architecture. ENSI employs an optimized encoding strategy that seamlessly
integrates CKKS scheme with a lightweight LLM variant, BitNet, significantly
reducing the computational complexity of encrypted matrix multiplications. In
response to the prohibitive computational demands of softmax under homomorphic
encryption (HE), we pioneer the integration of the sigmoid attention mechanism
with HE as a seamless, retraining-free alternative. Furthermore, by embedding
the Bootstrapping operation within the RMSNorm process, we efficiently refresh
ciphertexts while markedly decreasing the frequency of costly bootstrapping
invocations. Experimental evaluations demonstrate that ENSI achieves
approximately an 8x acceleration in matrix multiplications and a 2.6x speedup
in softmax inference on CPU compared to state-of-the-art method, with the
proportion of bootstrapping is reduced to just 1%.

</details>


### [215] [Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts](https://arxiv.org/abs/2509.09488)
*Felix Mächtle,Ashwath Shetty,Jonas Sander,Nils Loose,Sören Pirk,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: 扩散模型中提示词窃取是一个关键问题。本研究发现PyTorch在CPU上存在种子生成漏洞（CWE-339），利用此漏洞开发了种子恢复工具SeedSnitch，可高效破解约95%图像的种子。在此基础上，提出遗传算法提示词窃取方法PromptPirate，性能优于现有方法8-11%。同时，提出了有效的防御措施，并已负责任地披露此漏洞。


<details>
  <summary>Details</summary>
Motivation: 文本到图像生成中的提示词具有显著的知识和经济价值，其盗窃构成严重的隐私和安全风险。现有基于数值优化的提示词恢复方法由于未考虑初始随机噪声，存在根本性局限，因此有必要深入研究提示词窃取攻击并开发更有效的方法和防御策略。

Method: 本研究首先通过分析扩散模型，识别出PyTorch在CPU上生成初始随机噪声时存在的种子值范围限制漏洞（CWE-339）。基于此漏洞，开发了种子恢复工具SeedSnitch，用于高效暴力破解图像的初始种子。随后，利用已恢复的种子，提出了一种基于遗传算法的优化方法PromptPirate，用于提示词窃取。研究通过大规模实证分析（CivitAI平台上的图像）验证了SeedSnitch和PromptPirate的有效性，并与PromptStealer、P2HP和CLIP-Interrogator等现有先进方法进行性能比较。最后，设计并提出了简单有效的反制措施。

Result: 研究发现，PyTorch在CPU上生成初始随机噪声时存在CWE-339漏洞，限制了种子值范围。通过SeedSnitch工具，约95%的CivitAI图像的种子值可在每种子140分钟内被有效暴力破解。在此基础上，PromptPirate方法在LPIPS相似度上比现有最先进的提示词窃取方法（如PromptStealer）提高了8-11%。此外，研究还提出了能够有效抵御种子窃取及基于优化提示词窃取的简单对策。所有发现已负责任地披露，并已启动协调缓解工作。

Conclusion: 扩散模型中的提示词窃取是一个真实且严重的威胁，现有框架在种子生成方面存在可被利用的漏洞。本研究揭示了通过利用这些漏洞，可以有效地窃取图像的生成种子和原始提示词。然而，通过引入简单且有效的对策，此类攻击可以被有效防御，从而保护提示词的知识产权和用户隐私。

Abstract: Diffusion models have significantly advanced text-to-image generation,
enabling the creation of highly realistic images conditioned on textual prompts
and seeds. Given the considerable intellectual and economic value embedded in
such prompts, prompt theft poses a critical security and privacy concern. In
this paper, we investigate prompt-stealing attacks targeting diffusion models.
We reveal that numerical optimization-based prompt recovery methods are
fundamentally limited as they do not account for the initial random noise used
during image generation. We identify and exploit a noise-generation
vulnerability (CWE-339), prevalent in major image-generation frameworks,
originating from PyTorch's restriction of seed values to a range of $2^{32}$
when generating the initial random noise on CPUs. Through a large-scale
empirical analysis conducted on images shared via the popular platform CivitAI,
we demonstrate that approximately 95% of these images' seed values can be
effectively brute-forced in 140 minutes per seed using our seed-recovery tool,
SeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic
algorithm-based optimization method explicitly designed for prompt stealing.
PromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and
CLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity.
Furthermore, we introduce straightforward and effective countermeasures that
render seed stealing, and thus optimization-based prompt stealing, ineffective.
We have disclosed our findings responsibly and initiated coordinated mitigation
efforts with the developers to address this critical vulnerability.

</details>


### [216] [What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion Detection Datasets](https://arxiv.org/abs/2509.09564)
*Meghan Wilkinson,Robert H Thomson*

Main category: cs.CR

TL;DR: 本文利用无监督聚类技术（如HDBSCAN、Mean Shift Clustering）评估网络入侵检测数据集中良性流量的内部结构，以确定其中是否存在有意义的子类别，从而潜在地提升多分类性能。


<details>
  <summary>Details</summary>
Motivation: 监督机器学习在网络入侵检测中依赖于标记数据，但现有数据集通常将所有非攻击流量归为单一、庞大的“良性”类别。多数现有研究在训练算法时直接使用这些表面标签，忽视了良性流量内部可能存在的复杂结构。本研究旨在探讨良性流量内部是否存在有意义的子类别，以期改善整体多分类性能。

Method: 论文评估了NSL-KDD、UNSW-NB15和CIC-IDS 2017等常见入侵检测数据集中的良性流量结构。采用HDBSCAN、Mean Shift Clustering等无监督聚类技术来探索和识别良性流量空间中的潜在子类别。

Result: 论文展示了不同的无监督聚类技术（如HDBSCAN、Mean Shift Clustering）如何对常见入侵检测数据集中的良性流量空间进行差异化聚类，揭示了其内部的结构特征。

Conclusion: 通过无监督聚类技术揭示良性流量内部的差异化结构和潜在子类别，为改善网络入侵检测算法的整体多分类性能提供了新的视角和可行途径。这意味着对良性流量进行更细致的分类处理，有望提升检测准确性。

Abstract: Supervised machine learning techniques rely on labeled data to achieve high
task performance, but this requires the labels to capture some meaningful
differences in the underlying data structure. For training network intrusion
detection algorithms, most datasets contain a series of attack classes and a
single large benign class which captures all non-attack network traffic. A
review of intrusion detection papers and guides that explicitly state their
data preprocessing steps identified that the majority took the labeled
categories of the dataset at face value when training their algorithms. The
present paper evaluates the structure of benign traffic in several common
intrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and
determines whether there are meaningful sub-categories within this traffic
which may improve overall multi-classification performance using common machine
learning techniques. We present an overview of some unsupervised clustering
techniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they
differentially cluster the benign traffic space.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [217] [Convexity of Optimization Curves: Local Sharp Thresholds, Robustness Impossibility, and New Counterexamples](https://arxiv.org/abs/2509.08954)
*Le Duc Hieu*

Main category: math.OC

TL;DR: 本文研究了一阶方法（特别是梯度下降）的优化曲线（函数值序列f(x_n)）的凸性条件，发现梯度下降在特定步长下曲线为凸，而连续时间下曲线总是凸的。


<details>
  <summary>Details</summary>
Motivation: 探究一阶方法（常步长迭代产生的目标函数值序列f(x_n)）的优化曲线何时是凸的，亦即前向差分f(x_n)-f(x_{n+1})何时非增。

Method: 分析了梯度下降（GD）在凸L-光滑函数上的行为，并与连续时间（梯度流）动态进行对比研究。

Result: 对于凸L-光滑函数的梯度下降，当步长η ≤ 1.75/L时，优化曲线是凸的，且此阈值是紧密的。此外，当η ≤ 2/L时，梯度范数非增。在连续时间（梯度流）中，优化曲线始终是凸的。

Conclusion: 这些结果补充并完善了经典的平滑凸优化工具箱，连接了离散和连续动力学，并提升了最坏情况分析的理解。

Abstract: We study when the \emph{optimization curve} of first-order methods -- the
sequence \${f(x\_n)}*{n\ge0}\$ produced by constant-stepsize iterations -- is
convex, equivalently when the forward differences \$f(x\_n)-f(x*{n+1})\$ are
nonincreasing. For gradient descent (GD) on convex \$L\$-smooth functions, the
curve is convex for all stepsizes \$\eta \le 1.75/L\$, and this threshold is
tight. Moreover, gradient norms are nonincreasing for all \$\eta \le 2/L\$, and
in continuous time (gradient flow) the curve is always convex. These results
complement and refine the classical smooth convex optimization toolbox,
connecting discrete and continuous dynamics as well as worst-case analyses.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [218] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: 该论文提出了一个模拟真实世界场景的LLM能效基准测试，以帮助开发者构建更可持续的AI系统，应对LLM日益增长的能源消耗问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的部署和使用需要大量能源，对气候造成影响。现有能效基准测试未能充分代表实际生产场景，因此急需收集更多关于LLM能效的信息，并为开发者提供指导。

Method: 引入了“LLM能效基准测试”（LLM Efficiency Benchmark），旨在模拟真实世界使用条件。该基准测试利用高吞吐量、生产就绪的LLM服务后端vLLM，并研究了模型大小、架构和并发请求量如何影响推理能效。

Result: 研究结果表明，可以创建更能反映实际部署条件的能效基准测试。

Conclusion: 该基准测试为旨在构建更可持续AI系统的开发者提供了有价值的见解，证明了建立贴近实际部署条件的能效评估是可行的。

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [219] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: 本文提出了一个高质量的JIT-SDP数据集ReDef，并系统评估了预训练语言模型（PLMs）在代码修改理解方面的表现，发现PLMs依赖表面线索而非深层语义理解。


<details>
  <summary>Details</summary>
Motivation: 现有Just-in-Time软件缺陷预测（JIT-SDP）数据集存在标签噪声大、识别缺陷提交精度低的问题，且缺乏对预训练语言模型在代码修改任务中真实理解能力的系统评估。

Method: 1. **数据集构建**：提出了ReDef数据集，通过回滚提交锚定缺陷案例，事后历史检查验证干净案例，并利用GPT辅助筛选模糊实例，以确保高置信度。 2. **PLM评估**：在ReDef上，使用CodeBERT、CodeT5+和UniXcoder等PLMs，在五种输入编码策略下进行微调。通过交换添加/删除块、反转diff极性或注入虚假标记等反事实扰动来探测模型的敏感性。

Result: 1. ReDef数据集提供了更可靠的标签（3,164个缺陷修改和10,268个干净修改）。2. 紧凑的diff风格编码在所有PLMs上均持续优于整体函数格式。3. 反事实测试表明，PLMs的性能几乎没有下降或完全没有下降，这揭示了它们依赖于表面线索而非真正的语义理解。

Conclusion: 与基于快照的任务不同，当前的预训练语言模型在真正理解代码修改的能力方面仍然存在局限性，特别是在JIT-SDP这类需要深度语义理解的任务中。

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [220] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: 本文提出了一种将大型语言模型（LLMs）与基于场景的编程（SBP）相结合的方法，以提高软件开发的可靠性和可验证性，并通过开发一个强大的Connect4游戏代理进行了验证。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在软件开发中具有巨大潜力，能缩短开发时间、生成代码并提供创新想法。然而，它们也常自信地引入重大错误和不正确代码，可能误导开发者，因此需要更可靠地将LLMs引入软件开发周期。

Method: 本文提出了一种结构化方法，将LLMs与传统的软件工程技术相结合，旨在简化开发流程、减少错误并提高关键程序属性的可验证性。具体而言，该方法侧重于基于场景的编程（SBP）范式，允许人类开发者注入专业知识，并检查和验证LLM的输出。通过一个案例研究（设计和实现Connect4游戏）来评估该方法。

Result: 通过结合LLMs和SBP，成功创建了一个能力很强的Connect4游戏代理，该代理能够击败各种强大的现有代理。在某些情况下，研究人员还能够形式化验证该代理的正确性。此外，实践经验表明所提出的方法易于使用。

Conclusion: 所提出的结合LLMs和SBP的方法，能够以更可靠的方式将LLMs整合到软件开发周期中，有效简化开发过程，减少错误，并实现关键程序属性的验证，同时展现出强大的性能和良好的易用性。

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [221] [Physics-informed waveform inversion using pretrained wavefield neural operators](https://arxiv.org/abs/2509.08967)
*Xinquan Huang,Fu Wang,Tariq Alkhalifah*

Main category: physics.geo-ph

TL;DR: 本文提出了一种结合神经算子的物理信息全波形反演（FWI）框架，通过在损失函数中引入物理约束项，显著提高了反演模型的准确性和稳定性，同时保持了计算效率，尤其适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 传统全波形反演（FWI）受限于空核问题导致低分辨率模型，且计算成本高昂，不适用于实时应用。近期基于神经算子的加速FWI方法虽然高效可微，但反演性能存在噪声和不稳定性。因此，研究旨在提高神经算子FWI的准确性，同时保持其效率。

Method: 引入了一种新颖的物理信息FWI框架。该方法不单纯依赖基于自动微分的L2范数目标函数，而是将物理约束项（波方程）集成到损失函数中。具体而言，从初始模型开始模拟波场，然后评估波场对物理定律的遵守程度以及与记录数据的匹配度来计算损失，从而减少噪声和伪影。

Result: 数值实验（使用OpenFWI和Overthrust模型）表明，所提出的方法表现出卓越的性能，能够提供比传统方法更清晰、更精确的地下速度模型。通过减少噪声和伪影，显著提高了反演质量。

Conclusion: 该方法在效率上优于传统FWI，代表了FWI在实时地下监测实际应用中的一个重大进步，为高精度、高效率的地下模型重建提供了新的途径。

Abstract: Full waveform inversion (FWI) is crucial for reconstructing high-resolution
subsurface models, but it is often hindered, considering the limited data, by
its null space resulting in low-resolution models, and more importantly, by its
computational cost, especially if needed for real-time applications. Recent
attempts to accelerate FWI using learned wavefield neural operators have shown
promise in efficiency and differentiability, but typically suffer from noisy
and unstable inversion performance. To address these limitations, we introduce
a novel physics-informed FWI framework to enhance the inversion in accuracy
while maintaining the efficiency of neural operator-based FWI. Instead of
relying only on the L2 norm objective function via automatic differentiation,
resulting in noisy model reconstruction, we integrate a physics constraint term
in the loss function of FWI, improving the quality of the inverted velocity
models. Specifically, starting with an initial model to simulate wavefields and
then evaluating the loss over how much the resulting wavefield obeys the
physical laws (wave equation) and matches the recorded data, we achieve a
reduction in noise and artifacts. Numerical experiments using the OpenFWI and
Overthrust models demonstrate our method's superior performance, offering
cleaner and more accurate subsurface velocity than vanilla approaches.
Considering the efficiency of the approach compared to FWI, this advancement
represents a significant step forward in the practical application of FWI for
real-time subsurface monitoring.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [222] [Expressive Power of Deep Networks on Manifolds: Simultaneous Approximation](https://arxiv.org/abs/2509.09362)
*Hanfei Zhou,Lei Shi*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A key challenge in scientific machine learning is solving partial
differential equations (PDEs) on complex domains, where the curved geometry
complicates the approximation of functions and their derivatives required by
differential operators. This paper establishes the first simultaneous
approximation theory for deep neural networks on manifolds. We prove that a
constant-depth $\mathrm{ReLU}^{k-1}$ network with bounded weights--a property
that plays a crucial role in controlling generalization error--can approximate
any function in the Sobolev space $\mathcal{W}_p^{k}(\mathcal{M}^d)$ to an
error of $\varepsilon$ in the $\mathcal{W}_p^{s}(\mathcal{M}^d)$ norm, for
$k\geq 3$ and $s<k$, using $\mathcal{O}(\varepsilon^{-d/(k-s)})$ nonzero
parameters, a rate that overcomes the curse of dimensionality by depending only
on the intrinsic dimension $d$. These results readily extend to functions in
H\"older-Zygmund spaces. We complement this result with a matching lower bound,
proving our construction is nearly optimal by showing the required number of
parameters matches up to a logarithmic factor. Our proof of the lower bound
introduces novel estimates for the Vapnik-Chervonenkis dimension and
pseudo-dimension of the network's high-order derivative classes. These
complexity bounds provide a theoretical cornerstone for learning PDEs on
manifolds involving derivatives. Our analysis reveals that the network
architecture leverages a sparse structure to efficiently exploit the manifold's
low-dimensional geometry.

</details>


### [223] [DeepTV: A neural network approach for total variation minimization](https://arxiv.org/abs/2409.05569)
*Andreas Langer,Sara Behnamian*

Main category: math.NA

TL;DR: 本文提出一种使用神经网络解决无限维全变分最小化问题的方法，通过引入一个具有解的辅助神经网络问题，并证明其向原问题的$\Gamma$-收敛性来克服直接方法中解不存在的问题，同时提供了离散化方案及与有限差分方法的联系，并得到数值验证。


<details>
  <summary>Details</summary>
Motivation: 鉴于神经网络方法在解决偏微分方程（如PINNs、Deep Ritz）方面表现良好，本文旨在探索并提出一种类似的神经网络方法来解决无限维全变分最小化问题。

Method: 1. 提出使用神经网络解决无限维全变分最小化问题。2. 发现直接的神经网络问题通常没有解。3. 引入一个有解的辅助神经网络问题，并证明其向原问题的$\Gamma$-收敛性。4. 为数值计算进一步提出辅助神经网络问题的离散版本，并再次证明其向原问题的$\Gamma$-收敛性。5. 将离散神经网络问题与无限维全变分最小化问题的有限差分离散化联系起来。

Result: 1. 理论上，直接的神经网络问题在一般情况下没有解。2. 引入的辅助神经网络问题有解，并以$\Gamma$-收敛的方式逼近原问题。3. 离散版本的辅助神经网络问题也以$\Gamma$-收敛的方式逼近原无限维问题。4. $\Gamma$-收敛证明提出了一种特定的全变分离散化方法。5. 建立了离散神经网络问题与有限差分离散化之间的联系。6. 数值实验结果支持了所有理论发现。

Conclusion: 本研究成功提出了一种基于神经网络解决无限维全变分最小化问题的方法，通过引入辅助问题并利用$\Gamma$-收敛理论解决了原方法的理论障碍，并提供了实用的数值计算方案，同时与传统离散化方法建立了联系，所有发现均得到数值实验的支持。

Abstract: Neural network approaches have been demonstrated to work quite well to solve
partial differential equations in practice. In this context approaches like
physics-informed neural networks and the Deep Ritz method have become popular.
In this paper, we propose a similar approach to solve an infinite-dimensional
total variation minimization problem using neural networks. We illustrate that
the resulting neural network problem does not have a solution in general. To
circumvent this theoretic issue, we consider an auxiliary neural network
problem, which indeed has a solution, and show that it converges in the sense
of $\Gamma$-convergence to the original problem. For computing a numerical
solution we further propose a discrete version of the auxiliary neural network
problem and again show its $\Gamma$-convergence to the original
infinite-dimensional problem. In particular, the $\Gamma$-convergence proof
suggests a particular discretization of the total variation. Moreover, we
connect the discrete neural network problem to a finite difference
discretization of the infinite-dimensional total variation minimization
problem. Numerical experiments are presented supporting our theoretical
findings.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [224] [Representation-Aware Distributionally Robust Optimization: A Knowledge Transfer Framework](https://arxiv.org/abs/2509.09371)
*Zitao Wang,Nian Si,Molei Liu*

Main category: stat.ME

TL;DR: 本文提出REpresentation-Aware Distributionally Robust Estimation (READ) 框架，通过引入表示感知的对齐参数，改进了Wasserstein分布鲁棒学习，使其在应对分布偏移时能差异化地处理特征扰动，从而在保持不变结构的同时增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的分布鲁棒优化方法对所有特征扰动一视同仁，可能导致在追求鲁棒性时丧失预测表示中固有的不变结构。因此，需要一种能够根据预测表示的有用性来区分对待特征扰动的方法。

Method: 该研究提出了READ框架，通过在传输成本中嵌入多维对齐参数，使模型能够沿着与信息表示相关的方向差异化地抑制扰动。方法包括：1) 提供理论基础，证明特定正则化是Wasserstein分布鲁棒目标，从而统一了广义正则化估计器。2) 采用鲁棒Wasserstein剖面推断技术选择Wasserstein半径，以构建有效的、表示感知的参数置信区域。3) 分析READ估计器的几何特性，并提出优化算法来估计全局最优解在解曲面上的投影，以选择最佳的表示结构。

Result: 1) 建立了理论基础，表明线性回归和二元分类的半范数正则化可以看作Wasserstein分布鲁棒目标，为READ提供了可处理的重构，并统一了多种正则化估计器。2) 实现了有效且表示感知的模型参数置信区域的构建。3) 开发了一种优化算法，可以在同样鲁棒的估计器中，通过最优地构建表示结构来选择最符合全局最优解的估计器。4) 通过广泛的模拟和真实世界案例研究，证明了该框架的有效性。

Conclusion: READ框架提供了一种强大的、基于学习表示的鲁棒估计方法，能够有效抵御特征变化和分布偏移，同时保留了预测模型中的不变结构。

Abstract: We propose REpresentation-Aware Distributionally Robust Estimation (READ), a
novel framework for Wasserstein distributionally robust learning that accounts
for predictive representations when guarding against distributional shifts.
Unlike classical approaches that treat all feature perturbations equally, READ
embeds a multidimensional alignment parameter into the transport cost, allowing
the model to differentially discourage perturbations along directions
associated with informative representations. This yields robustness to feature
variation while preserving invariant structure. Our first contribution is a
theoretical foundation: we show that seminorm regularizations for linear
regression and binary classification arise as Wasserstein distributionally
robust objectives, thereby providing tractable reformulations of READ and
unifying a broad class of regularized estimators under the DRO lens. Second, we
adopt a principled procedure for selecting the Wasserstein radius using the
techniques of robust Wasserstein profile inference. This further enables the
construction of valid, representation-aware confidence regions for model
parameters with distinct geometric features. Finally, we analyze the geometry
of READ estimators as the alignment parameters vary and propose an optimization
algorithm to estimate the projection of the global optimum onto this solution
surface. This procedure selects among equally robust estimators while optimally
constructing a representation structure. We conclude by demonstrating the
effectiveness of our framework through extensive simulations and a real-world
study, providing a powerful robust estimation grounded in learning
representation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [225] [CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction](https://arxiv.org/abs/2509.08947)
*Yancheng Cai,Robert Wanat,Rafal Mantiuk*

Main category: cs.GR

TL;DR: 提出CameraVDP框架，结合相机重建与视觉差异预测，实现对显示器的高精度、感知驱动测量和评估。


<details>
  <summary>Details</summary>
Motivation: 传统显示器测量方法无法捕捉空间变化的伪影，而相机虽有高分辨率但引入自身失真。此外，需要结合视觉系统模型来评估失真可见性。

Method: 提出CameraVDP框架，结合了高动态范围（HDR）图像堆叠、MTF反演、畸变校正等相机重建管线，以及视觉差异预测器（VDP），使相机能作为精确的显示器测量工具并评估人眼可见的失真。

Result: 该CameraVDP框架成功应用于缺陷像素检测、色边感知和显示非均匀性评估。同时，其不确定性分析框架能估算缺陷像素检测的理论性能上限并提供VDP质量分数的置信区间。

Conclusion: CameraVDP通过整合相机高精度测量和人眼视觉感知模型，克服了传统显示器评估方法的局限性，提供了更准确、更符合人类感知的显示器性能评估方案。

Abstract: Accurate measurement of images produced by electronic displays is critical
for the evaluation of both traditional and computational displays. Traditional
display measurement methods based on sparse radiometric sampling and fitting a
model are inadequate for capturing spatially varying display artifacts, as they
fail to capture high-frequency and pixel-level distortions. While cameras offer
sufficient spatial resolution, they introduce optical, sampling, and
photometric distortions. Furthermore, the physical measurement must be combined
with a model of a visual system to assess whether the distortions are going to
be visible. To enable perceptual assessment of displays, we propose a
combination of a camera-based reconstruction pipeline with a visual difference
predictor, which account for both the inaccuracy of camera measurements and
visual difference prediction. The reconstruction pipeline combines HDR image
stacking, MTF inversion, vignetting correction, geometric undistortion,
homography transformation, and color correction, enabling cameras to function
as precise display measurement instruments. By incorporating a Visual
Difference Predictor (VDP), our system models the visibility of various stimuli
under different viewing conditions for the human visual system. We validate the
proposed CameraVDP framework through three applications: defective pixel
detection, color fringing awareness, and display non-uniformity evaluation. Our
uncertainty analysis framework enables the estimation of the theoretical upper
bound for defect pixel detection performance and provides confidence intervals
for VDP quality scores.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [226] [Multi Robot Coordination in Highly Dynamic Environments: Tackling Asymmetric Obstacles and Limited Communication](https://arxiv.org/abs/2509.08859)
*Vincenzo Suriani,Daniele Affinita,Domenico D. Bloisi,Daniele Nardi*

Main category: cs.RO

TL;DR: 本文提出了一种分布式协调方法，用于在通信受限、部分可观测且存在活跃非对称障碍物的多智能体系统中进行任务分配。


<details>
  <summary>Details</summary>
Motivation: 在通信能力受限、环境高度部分可观测且存在活跃障碍物的多智能体系统（MAS）中协调任务具有挑战性，尤其当任务需要频繁重新分配时。现有方法常将障碍物视为对称，限制了其适用性，而实际障碍物大多是非对称的。

Method: 受基于市场的任务分配启发，作者提出了一种新颖的分布式协调方法，旨在高效协调低通信场景下的自主智能体行动，并特别考虑了非对称障碍物。

Result: 该方法已在仿真和真实世界的NAO机器人团队（RoboCup比赛）中得到验证。实验结果显示，在有限通信设置下，任务重叠显著减少，其中最常重新分配的任务减少了52%。

Conclusion: 所提出的架构成功解决了障碍物活跃且非对称、通信信道差、环境部分可观测等复杂场景下的多智能体任务分配问题，显著降低了任务重叠。

Abstract: Coordinating a fully distributed multi-agent system (MAS) can be challenging
when the communication channel has very limited capabilities in terms of
sending rate and packet payload. When the MAS has to deal with active obstacles
in a highly partially observable environment, the communication channel
acquires considerable relevance. In this paper, we present an approach to deal
with task assignments in extremely active scenarios, where tasks need to be
frequently reallocated among the agents participating in the coordination
process. Inspired by market-based task assignments, we introduce a novel
distributed coordination method to orchestrate autonomous agents' actions
efficiently in low communication scenarios. In particular, our algorithm takes
into account asymmetric obstacles. While in the real world, the majority of
obstacles are asymmetric, they are usually treated as symmetric ones, thus
limiting the applicability of existing methods. To summarize, the presented
architecture is designed to tackle scenarios where the obstacles are active and
asymmetric, the communication channel is poor and the environment is partially
observable. Our approach has been validated in simulation and in the real
world, using a team of NAO robots during official RoboCup competitions.
Experimental results show a notable reduction in task overlaps in limited
communication settings, with a decrease of 52% in the most frequent reallocated
task.

</details>


### [227] [OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning](https://arxiv.org/abs/2509.09332)
*Yuecheng Liu,Dafeng Chi,Shiguang Wu,Zhanguang Zhang,Yuzheng Zhuang,Bowen Yang,He Zhu,Lingfeng Zhang,Pengwei Xie,David Gamaliel Arcos Bravo,Yingxue Zhang,Jianye Hao,Xingyue Quan*

Main category: cs.RO

TL;DR: 本文提出OmniEVA，一个具身通用规划器，通过任务自适应3D定位和具身感知推理框架，解决了多模态大语言模型在空间适应性和具身约束方面的局限，实现了最先进的具身推理和鲁棒规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型（MLLMs）的具身系统面临两大挑战：一是“几何适应性差距”，模型在2D输入或硬编码3D几何中难以处理多样化的空间需求；二是“具身约束差距”，忽略了真实机器人的物理约束，导致理论可行但实际不可行的任务规划。

Method: 引入OmniEVA，通过两项创新解决上述问题：1) 任务自适应3D定位机制，利用门控路由器根据上下文需求选择性调节3D融合；2) 具身感知推理框架，将任务目标和具身约束共同整合到推理循环中，以生成目标导向且可执行的规划决策。

Result: OmniEVA不仅实现了最先进的通用具身推理性能，还在广泛的下游场景中表现出强大的能力。对一系列具身基准（包括原始和复合任务）的评估证实了其鲁棒和通用的规划能力。

Conclusion: OmniEVA通过其创新的3D定位和具身感知推理框架，有效弥补了现有MLLM在具身智能规划中的几何适应性和具身约束缺陷，显著提升了具身推理和任务规划的实用性和性能。

Abstract: Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.
Nevertheless, current MLLM-based embodied systems face two critical
limitations. First, Geometric Adaptability Gap: models trained solely on 2D
inputs or with hard-coded 3D geometry injection suffer from either insufficient
spatial information or restricted 2D generalization, leading to poor
adaptability across tasks with diverse spatial demands. Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware
Reasoning framework that jointly incorporates task goals and embodiment
constraints into the reasoning loop, resulting in planning decisions that are
both goal-directed and executable. Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios. Evaluations of a suite of proposed embodied benchmarks,
including both primitive and composite tasks, confirm its robust and versatile
planning capabilities. Project page: https://omnieva.github.io

</details>


### [228] [KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning](https://arxiv.org/abs/2509.09074)
*Alice Kate Li,Thales C Silva,Victoria Edwards,Vijay Kumar,M. Ani Hsieh*

Main category: cs.RO

TL;DR: 提出KoopMotion，一种基于流场的运动规划方法，利用Koopman算子实现机器人轨迹收敛与跟踪，并在数据集和物理机器人上验证了其高效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有Koopman算子理论在动力系统建模中有效，但无法固有地强制收敛到期望轨迹或指定目标，这在从演示中学习（LfD）时是必需的。

Method: 提出KoopMotion方法，将运动流场表示为由Koopman算子参数化的动力系统，以模仿期望轨迹。该方法利用学习到的流场的散度特性，实现平滑运动场，确保机器人在偏离期望轨迹时能收敛并跟踪轨迹至终点。

Result: KoopMotion在LASA人类手写数据集和3D机械臂轨迹数据集上表现出色，并通过物理机器人实验验证。它具有高度样本效率（仅需3%LASA数据），并在时空动态建模效率方面显著优于基线方法。

Conclusion: KoopMotion是一种有效且高效的运动规划方法，通过结合Koopman算子和流场散度特性，解决了现有Koopman方法在轨迹收敛性上的不足，能可靠地引导机器人收敛并跟踪至期望轨迹的终点。

Abstract: In this work, we propose a novel flow field-based motion planning method that
drives a robot from any initial state to a desired reference trajectory such
that it converges to the trajectory's end point. Despite demonstrated efficacy
in using Koopman operator theory for modeling dynamical systems, Koopman does
not inherently enforce convergence to desired trajectories nor to specified
goals -- a requirement when learning from demonstrations (LfD). We present
KoopMotion which represents motion flow fields as dynamical systems,
parameterized by Koopman Operators to mimic desired trajectories, and leverages
the divergence properties of the learnt flow fields to obtain smooth motion
fields that converge to a desired reference trajectory when a robot is placed
away from the desired trajectory, and tracks the trajectory until the end
point. To demonstrate the effectiveness of our approach, we show evaluations of
KoopMotion on the LASA human handwriting dataset and a 3D manipulator
end-effector trajectory dataset, including spectral analysis. We also perform
experiments on a physical robot, verifying KoopMotion on a miniature autonomous
surface vehicle operating in a non-static fluid flow environment. Our approach
is highly sample efficient in both space and time, requiring only 3\% of the
LASA dataset to generate dense motion plans. Additionally, KoopMotion provides
a significant improvement over baselines when comparing metrics that measure
spatial and temporal dynamics modeling efficacy.

</details>


### [229] [SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674)
*Haozhan Li,Yuxin Zuo,Jiale Yu,Yuhao Zhang,Zhaohui Yang,Kaiyan Zhang,Xuekai Zhu,Yuchen Zhang,Tianxing Chen,Ganqu Cui,Dehui Wang,Dingxiang Luo,Yuchen Fan,Youbang Sun,Jia Zeng,Jiangmiao Pang,Shanghang Zhang,Yu Wang,Yao Mu,Bowen Zhou,Ning Ding*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Vision-Language-Action (VLA) models have recently emerged as a powerful
paradigm for robotic manipulation. Despite substantial progress enabled by
large-scale pretraining and supervised fine-tuning (SFT), these models face two
fundamental challenges: (i) the scarcity and high cost of large-scale
human-operated robotic trajectories required for SFT scaling, and (ii) limited
generalization to tasks involving distribution shift. Recent breakthroughs in
Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can
dramatically enhance step-by-step reasoning capabilities, raising a natural
question: Can RL similarly improve the long-horizon step-by-step action
planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL
framework tailored for VLA models. Building upon veRL, we introduce
VLA-specific trajectory sampling, scalable parallelization, multi-environment
rendering, and optimized loss computation. When applied to OpenVLA-OFT,
SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$
on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce.
SimpleVLA-RL not only reduces dependence on large-scale data and enables robust
generalization, but also remarkably surpasses SFT in real-world tasks.
Moreover, we identify a novel phenomenon ``pushcut'' during RL training,
wherein the policy discovers previously unseen patterns beyond those seen in
the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [230] [PerFairX: Is There a Balance Between Fairness and Personality in Large Language Model Recommendations?](https://arxiv.org/abs/2509.08829)
*Chandan Kumar Sah*

Main category: cs.CY

TL;DR: 本文提出PerFairX框架，评估LLM推荐系统中个性化与公平性的权衡。研究发现个性化提示虽能提升心理契合度，但可能加剧人口公平性差距，并比较了ChatGPT和DeepSeek的表现。


<details>
  <summary>Details</summary>
Motivation: 将LLM融入推荐系统实现基于个性的个性化时，发现通过OCEAN模型整合用户个性特征，在追求心理契合度的同时，与确保人口统计学公平性之间存在关键冲突。

Method: 提出PerFairX统一评估框架，量化LLM生成推荐中个性化与人口统计学公平性之间的权衡。在电影(MovieLens 10M)和音乐(Last.fm 360K)数据集上，使用中性和个性化敏感提示，对ChatGPT和DeepSeek进行基准测试。

Result: 个性化提示显著提高了与个体特征的契合度，但可能加剧不同人口群体的公平性差距。DeepSeek在心理契合度上表现更强但对提示变化更敏感，而ChatGPT输出稳定但个性化程度较低。

Conclusion: PerFairX提供了一个有原则的基准，指导开发兼顾公平性和心理学考量的LLM推荐系统，有助于创建包容、以用户为中心的AI应用。

Abstract: The integration of Large Language Models (LLMs) into recommender systems has
enabled zero-shot, personality-based personalization through prompt-based
interactions, offering a new paradigm for user-centric recommendations.
However, incorporating user personality traits via the OCEAN model highlights a
critical tension between achieving psychological alignment and ensuring
demographic fairness. To address this, we propose PerFairX, a unified
evaluation framework designed to quantify the trade-offs between
personalization and demographic equity in LLM-generated recommendations. Using
neutral and personality-sensitive prompts across diverse user profiles, we
benchmark two state-of-the-art LLMs, ChatGPT and DeepSeek, on movie (MovieLens
10M) and music (Last.fm 360K) datasets. Our results reveal that
personality-aware prompting significantly improves alignment with individual
traits but can exacerbate fairness disparities across demographic groups.
Specifically, DeepSeek achieves stronger psychological fit but exhibits higher
sensitivity to prompt variations, while ChatGPT delivers stable yet less
personalized outputs. PerFairX provides a principled benchmark to guide the
development of LLM-based recommender systems that are both equitable and
psychologically informed, contributing to the creation of inclusive,
user-centric AI applications in continual learning contexts.

</details>


### [231] [Deep opacity and AI: A threat to XAI and to privacy protection mechanisms](https://arxiv.org/abs/2509.08835)
*Vincent C. Müller*

Main category: cs.CY

TL;DR: 本文分析了大数据和人工智能的“黑箱问题”如何加剧隐私威胁，提出了三种不透明性分类，并指出由于缺乏可辩护的判断依据，现有隐私保护措施失效，最终导致隐私问题恶化。


<details>
  <summary>Details</summary>
Motivation: 大数据分析和人工智能对隐私构成威胁，其中部分原因在于AI的“黑箱问题”，这使得对判断和行动的辩护成为难题。

Method: 作者提出区分三种不透明性：1) 浅层不透明（主体不知系统所为），2) 标准黑箱不透明（分析师不知系统所为），3) 深层不透明（分析师不可能知道系统所为）。

Result: 当主体和分析专家在不透明性下操作时，他们无法提供保护隐私所需的判断依据，例如无法给出“知情同意”或保证“匿名性”。这导致大数据和AI中的代理人往往无法做出保护隐私的必要判断。

Conclusion: 大数据分析加剧了隐私问题，并降低了补救措施的有效性。作者还简要展望了应对此情况的技术方法。

Abstract: It is known that big data analytics and AI pose a threat to privacy, and that
some of this is due to some kind of "black box problem" in AI. I explain how
this becomes a problem in the context of justification for judgments and
actions. Furthermore, I suggest distinguishing three kinds of opacity: 1) the
subjects do not know what the system does ("shallow opacity"), 2) the analysts
do not know what the system does ("standard black box opacity"), or 3) the
analysts cannot possibly know what the system might do ("deep opacity"). If the
agents, data subjects as well as analytics experts, operate under opacity, then
these agents cannot provide justifications for judgments that are necessary to
protect privacy, e.g., they cannot give "informed consent", or guarantee
"anonymity". It follows from these points that agents in big data analytics and
AI often cannot make the judgments needed to protect privacy. So I conclude
that big data analytics makes the privacy problems worse and the remedies less
effective. As a positive note, I provide a brief outlook on technical ways to
handle this situation.

</details>


### [232] [Safe and Certifiable AI Systems: Concepts, Challenges, and Lessons Learned](https://arxiv.org/abs/2509.08852)
*Kajetan Schweighofer,Barbara Brune,Lukas Gruber,Simon Schmid,Alexander Aufreiter,Andreas Gruber,Thomas Doms,Sebastian Eder,Florian Mayer,Xaver-Paul Stadlbauer,Christoph Schwald,Werner Zellinger,Bernhard Nessler,Sepp Hochreiter*

Main category: cs.CY

TL;DR: 提出TÜV AUSTRIA可信AI框架，旨在提供AI系统评估与认证的端到端审计方案，以应对安全关键型AI应用中认证实践的稀缺性。


<details>
  <summary>Details</summary>
Motivation: 针对安全关键型AI应用日益增多但缺乏实用认证方案的现状，旨在确保AI系统安全、合法且可接受。

Method: 介绍TÜV AUSTRIA可信AI框架，一个端到端审计目录和方法论，用于评估和认证机器学习系统。该框架基于“安全软件开发”、“功能要求”和“道德与数据隐私”三大支柱，将欧盟AI法案的高层次义务转化为具体的、可测试的标准。其核心概念是功能信任度，结合统计定义的应用领域、基于风险的最低性能要求以及在独立采样数据上的统计测试。

Result: 概述了评估的功能要求，并分享了实践应用审计目录的经验教训，揭示了常见问题，如数据泄露、领域定义不足、偏见忽视或缺乏分布漂移控制。讨论了鲁棒性、算法公平性等认证关键方面。

Conclusion: 该方法通过整合技术最佳实践与新兴欧洲标准，为监管机构、提供者和用户提供了实现符合法律、功能可信赖且可认证AI系统的实用路线图。

Abstract: There is an increasing adoption of artificial intelligence in safety-critical
applications, yet practical schemes for certifying that AI systems are safe,
lawful and socially acceptable remain scarce. This white paper presents the
T\"UV AUSTRIA Trusted AI framework an end-to-end audit catalog and methodology
for assessing and certifying machine learning systems. The audit catalog has
been in continuous development since 2019 in an ongoing collaboration with
scientific partners. Building on three pillars - Secure Software Development,
Functional Requirements, and Ethics & Data Privacy - the catalog translates the
high-level obligations of the EU AI Act into specific, testable criteria. Its
core concept of functional trustworthiness couples a statistically defined
application domain with risk-based minimum performance requirements and
statistical testing on independently sampled data, providing transparent and
reproducible evidence of model quality in real-world settings. We provide an
overview of the functional requirements that we assess, which are oriented on
the lifecycle of an AI system. In addition, we share some lessons learned from
the practical application of the audit catalog, highlighting common pitfalls we
encountered, such as data leakage scenarios, inadequate domain definitions,
neglect of biases, or a lack of distribution drift controls. We further discuss
key aspects of certifying AI systems, such as robustness, algorithmic fairness,
or post-certification requirements, outlining both our current conclusions and
a roadmap for future research. In general, by aligning technical best practices
with emerging European standards, the approach offers regulators, providers,
and users a practical roadmap for legally compliant, functionally trustworthy,
and certifiable AI systems.

</details>


### [233] [A vibe coding learning design to enhance EFL students' talking to, through, and about AI](https://arxiv.org/abs/2509.08854)
*David James Woo,Kai Guo,Yangyang Yu*

Main category: cs.CY

TL;DR: 该文章报告了在EFL教育中试点“vibe coding”（使用自然语言结合AI创建软件）的情况，并提出了一个人类-AI元语言框架。通过案例研究发现，学生的提示工程方法和AI心智模型差异导致了编码结果的显著不同。


<details>
  <summary>Details</summary>
Motivation: 探索“vibe coding”（使用AI通过自然语言创建软件应用）在英语作为外语（EFL）教育中的潜力与挑战，并为此开发一个人类-AI元语言框架，以分析学生与AI的互动。

Method: 采用试点研究和案例研究方法。开发了一个包含三个维度的“人类-AI元语言框架”（与AI对话、通过AI对话、讨论AI）。基于逆向设计原则，组织了一个四小时的工作坊，让两名学生设计应用程序。数据收集包括工作表、视频记录、有声思维协议、屏幕录像和AI生成图像。

Result: 对比案例显示，一名学生成功地通过vibe coding创建了功能性应用，而另一名学生则遇到了技术困难，导致预期设计与实际功能之间存在重大差异。分析揭示了学生在提示工程方法上的差异，这暗示了他们对AI的不同心智模型，以及在归属作者权方面的张力。研究表明AI可作为有益的语言机器，而学生与AI对话、通过AI对话和讨论AI的方式差异解释了vibe coding结果的变异。

Conclusion: 有效的vibe coding教学需要明确的元语言支架、教授结构化的提示工程、促进批判性的作者权讨论，并发展用于阐明AI心智模型的词汇。

Abstract: This innovative practice article reports on the piloting of vibe coding
(using natural language to create software applications with AI) for English as
a Foreign Language (EFL) education. We developed a human-AI meta-languaging
framework with three dimensions: talking to AI (prompt engineering), talking
through AI (negotiating authorship), and talking about AI (mental models of
AI). Using backward design principles, we created a four-hour workshop where
two students designed applications addressing authentic EFL writing challenges.
We adopted a case study methodology, collecting data from worksheets and video
recordings, think-aloud protocols, screen recordings, and AI-generated images.
Contrasting cases showed one student successfully vibe coding a functional
application cohering to her intended design, while another encountered
technical difficulties with major gaps between intended design and actual
functionality. Analysis reveals differences in students' prompt engineering
approaches, suggesting different AI mental models and tensions in attributing
authorship. We argue that AI functions as a beneficial languaging machine, and
that differences in how students talk to, through, and about AI explain vibe
coding outcome variations. Findings indicate that effective vibe coding
instruction requires explicit meta-languaging scaffolding, teaching structured
prompt engineering, facilitating critical authorship discussions, and
developing vocabulary for articulating AI mental models.

</details>


### [234] [Investigating Student Interaction Patterns with Large Language Model-Powered Course Assistants in Computer Science Courses](https://arxiv.org/abs/2509.08862)
*Chang Liu,Loc Hoang,Andrew Stolman,Rene F. Kizilcec,Bo Wu*

Main category: cs.CY

TL;DR: 本研究开发并部署了一个LLM驱动的课程助手，以解决学生及时学术支持的挑战。结果显示，该系统在非排课时间使用率高，尤其受入门课程学生欢迎，且大部分回复正确有用。然而，LLM在生成高阶认知问题方面存在局限，提示未来需要更具教学导向的设计和教育者的参与。


<details>
  <summary>Details</summary>
Motivation: 大多数高校在为学生提供灵活及时的学术支持方面面临挑战，导致许多学生在非排课时间无法获得帮助。大型语言模型（LLMs）有望弥补这一空白，但学生与LLMs的互动鲜少受到教育者的监督。因此，本研究旨在通过部署和研究一个LLM驱动的课程助手，来了解其在现实世界中的使用情况并探究其教学意义。

Method: 研究团队开发了一个LLM驱动的课程助手，并将其部署到三所机构的六门计算机科学课程中，覆盖了大约2,000名学生。通过分析互动数据来表征实际使用情况，特别是关注使用时间段和课程类型。此外，每门课程抽取了200个对话样本进行人工标注，评估回复的正确性、有用性及是否包含示例。研究还考察了探究式学习策略，分析了LLM生成后续问题的比例及其被学生采纳的情况。最后，通过布鲁姆分类法（Bloom's taxonomy）评估了LLM生成高阶认知问题的能力。

Result: 互动数据显示，系统在晚上和夜间使用率依然很高，且在入门课程中使用率更高，这表明系统有助于弥补时间上的支持空白并满足初学者需求。人工标注结果显示，大多数抽样回复被认为是正确且有用的，但有小部分回复无用或错误，且很少包含专门的示例。关于探究式学习策略，约11%的对话包含LLM生成的后续问题，但这些问题在高级课程中常被学生忽略。布鲁姆分类法分析揭示，当前LLM在生成高阶认知问题方面的能力有限。

Conclusion: LLM驱动的课程助手能够有效解决学生在非排课时间缺乏支持的问题，并满足初学者的需求。然而，当前LLM在生成高阶认知问题方面存在局限性。这些模式表明，未来在设计以教学为导向的LLM教育系统方面存在机遇，并且需要教育者更多地参与到提示、内容和策略的配置中。

Abstract: Providing students with flexible and timely academic support is a challenge
at most colleges and universities, leaving many students without help outside
scheduled hours. Large language models (LLMs) are promising for bridging this
gap, but interactions between students and LLMs are rarely overseen by
educators. We developed and studied an LLM-powered course assistant deployed
across multiple computer science courses to characterize real-world use and
understand pedagogical implications. By Spring 2024, our system had been
deployed to approximately 2,000 students across six courses at three
institutions. Analysis of the interaction data shows that usage remains strong
in the evenings and nights and is higher in introductory courses, indicating
that our system helps address temporal support gaps and novice learner needs.
We sampled 200 conversations per course for manual annotation: most sampled
responses were judged correct and helpful, with a small share unhelpful or
erroneous; few responses included dedicated examples. We also examined an
inquiry-based learning strategy: only around 11% of sampled conversations
contained LLM-generated follow-up questions, which were often ignored by
students in advanced courses. A Bloom's taxonomy analysis reveals that current
LLM capabilities are limited in generating higher-order cognitive questions.
These patterns suggest opportunities for pedagogically oriented LLM-based
educational systems and greater educator involvement in configuring prompts,
content, and policies.

</details>


### [235] [Decentralising LLM Alignment: A Case for Context, Pluralism, and Participation](https://arxiv.org/abs/2509.08858)
*Oriane Peter,Kate Devlin*

Main category: cs.CY

TL;DR: 当前大语言模型（LLM）的对齐方法存在中心化和价值偏见问题，本文提出通过情境、多元主义和参与性实现去中心化对齐，以抵抗认知不正义和民主侵蚀。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐技术主要反映一小部分群体的规范偏好，将他们的价值观强加给广大用户，导致知识生产和治理的控制权集中在已有影响力的机构手中。这引发了认知不正义和民主进程受损的问题。

Method: 本文基于权力/知识理论，提出通过“情境（context）”、“多元主义（pluralism）”和“参与（participation）”这三个特征来实现对齐的去中心化。并通过具体的用例，阐明在制定对齐实践时界定使用情境的重要性。

Result: 本研究贡献在于：1) 强调情境、多元主义和参与在去中心化对齐中的作用；2) 提供了具体示例来说明这些策略；3) 展示了在不同使用情境中应用对齐所需的细微要求。

Conclusion: 本论文将LLM对齐定位为抵抗认知不正义和民主进程侵蚀的潜在阵地，但也承认这些策略本身无法取代更广泛的社会变革。

Abstract: Large Language Models (LLMs) alignment methods have been credited with the
commercial success of products like ChatGPT, given their role in steering LLMs
towards user-friendly outputs. However, current alignment techniques
predominantly mirror the normative preferences of a narrow reference group,
effectively imposing their values on a wide user base. Drawing on theories of
the power/knowledge nexus, this work argues that current alignment practices
centralise control over knowledge production and governance within already
influential institutions. To counter this, we propose decentralising alignment
through three characteristics: context, pluralism, and participation.
Furthermore, this paper demonstrates the critical importance of delineating the
context-of-use when shaping alignment practices by grounding each of these
features in concrete use cases. This work makes the following contributions:
(1) highlighting the role of context, pluralism, and participation in
decentralising alignment; (2) providing concrete examples to illustrate these
strategies; and (3) demonstrating the nuanced requirements associated with
applying alignment across different contexts of use. Ultimately, this paper
positions LLM alignment as a potential site of resistance against epistemic
injustice and the erosion of democratic processes, while acknowledging that
these strategies alone cannot substitute for broader societal changes.

</details>


### [236] [Incorporating AI Incident Reporting into Telecommunications Law and Policy: Insights from India](https://arxiv.org/abs/2509.09508)
*Avinash Agarwal,Manisha J. Nene*

Main category: cs.CY

TL;DR: 人工智能融入电信基础设施带来了传统网络安全框架无法覆盖的新型风险。本文定义了电信AI事件，分析了印度现有法规在AI风险监管上的空白，并提出将AI事件报告整合到印度电信治理中的政策建议，为其他国家提供了借鉴。


<details>
  <summary>Details</summary>
Motivation: 人工智能集成到电信基础设施中引入了传统网络安全和数据保护框架未能充分解决的新型风险（如算法偏差、系统行为不可预测性），凸显了针对电信AI事件的监管空白和将其识别为独特监管问题的必要性。

Method: 本文定义并详细分类了电信AI事件。以印度为案例研究对象，分析了其现有的关键数字法规（包括2023年《电信法》、CERT-In规则和2023年《数字个人数据保护法》）。此外，还审查了事件披露的结构性障碍和现有AI事件存储库的局限性，并基于这些发现提出了政策建议。

Result: 研究发现，印度现有法律框架主要关注网络安全和数据泄露，对AI特有的运营事件（如性能下降、算法偏差）存在显著的监管空白。因此，论文提出了将AI事件报告整合到印度现有电信治理中的政策建议，包括强制报告高风险AI故障、指定现有政府机构作为节点机构管理事件数据，以及开发标准化报告框架。

Conclusion: 本文通过定义电信AI事件并揭示了印度现有法规在AI风险监管上的不足，提出了将AI事件报告纳入现有电信治理的政策建议。这些建议旨在增强监管清晰度和长期弹性，并为其他国家在其现有部门框架内治理AI风险提供了一个实用且可复制的蓝图。

Abstract: The integration of artificial intelligence (AI) into telecommunications
infrastructure introduces novel risks, such as algorithmic bias and
unpredictable system behavior, that fall outside the scope of traditional
cybersecurity and data protection frameworks. This paper introduces a precise
definition and a detailed typology of telecommunications AI incidents,
establishing them as a distinct category of risk that extends beyond
conventional cybersecurity and data protection breaches. It argues for their
recognition as a distinct regulatory concern. Using India as a case study for
jurisdictions that lack a horizontal AI law, the paper analyzes the country's
key digital regulations. The analysis reveals that India's existing legal
instruments, including the Telecommunications Act, 2023, the CERT-In Rules, and
the Digital Personal Data Protection Act, 2023, focus on cybersecurity and data
breaches, creating a significant regulatory gap for AI-specific operational
incidents, such as performance degradation and algorithmic bias. The paper also
examines structural barriers to disclosure and the limitations of existing AI
incident repositories. Based on these findings, the paper proposes targeted
policy recommendations centered on integrating AI incident reporting into
India's existing telecom governance. Key proposals include mandating reporting
for high-risk AI failures, designating an existing government body as a nodal
agency to manage incident data, and developing standardized reporting
frameworks. These recommendations aim to enhance regulatory clarity and
strengthen long-term resilience, offering a pragmatic and replicable blueprint
for other nations seeking to govern AI risks within their existing sectoral
frameworks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [237] [WarpPINN-fibers: improved cardiac strain estimation from cine-MR with physics-informed neural networks](https://arxiv.org/abs/2509.08872)
*Felipe Álvarez Barrientos,Tomás Banduc,Isabeau Sirven,Francisco Sahli Costabal*

Main category: eess.IV

TL;DR: 本文提出WarpPINN-fibers，一种结合纤维信息的物理信息神经网络，能从电影MRI准确预测心脏运动和应变，并在合成体模和健康志愿者队列中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 心脏收缩运动受心肌纤维分布强烈影响。现有从传统影像技术估计应变的方法未纳入纤维力学，限制了其准确解释心脏功能和心肌损伤相关病理（如心血管疾病）的能力。

Method: 引入WarpPINN-fibers，一个物理信息神经网络框架。通过训练使其满足超弹性模型并促进纤维收缩，以从电影磁共振图像预测心脏变形场。损失函数包含三项：数据相似性损失、强制心肌近乎不可压缩的正则化项，以及控制沿合成纤维方向应变的纤维拉伸惩罚项。

Result: WarpPINN-fibers改进了先前的WarpPINN模型，并在合成体模实验中有效控制了纤维拉伸。在包含15名健康志愿者的电影MRI基准测试中，其在地标跟踪和应变曲线预测方面优于其他替代方法。

Conclusion: 该方法有望通过与纤维生理学一致的准确变形场，实现更精确的心脏应变量化，且无需比MRI更复杂的成像技术。

Abstract: The contractile motion of the heart is strongly determined by the
distribution of the fibers that constitute cardiac tissue. Strain analysis
informed with the orientation of fibers allows to describe several pathologies
that are typically associated with impaired mechanics of the myocardium, such
as cardiovascular disease. Several methods have been developed to estimate
strain-derived metrics from traditional imaging techniques. However, the
physical models underlying these methods do not include fiber mechanics,
restricting their capacity to accurately explain cardiac function. In this
work, we introduce WarpPINN-fibers, a physics-informed neural network framework
to accurately obtain cardiac motion and strains enhanced by fiber information.
We train our neural network to satisfy a hyper-elastic model and promote fiber
contraction with the goal to predict the deformation field of the heart from
cine magnetic resonance images. For this purpose, we build a loss function
composed of three terms: a data-similarity loss between the reference and the
warped template images, a regularizer enforcing near-incompressibility of
cardiac tissue and a fiber-stretch penalization that controls strain in the
direction of synthetically produced fibers. We show that our neural network
improves the former WarpPINN model and effectively controls fiber stretch in a
synthetic phantom experiment. Then, we demonstrate that WarpPINN-fibers
outperforms alternative methodologies in landmark-tracking and strain curve
prediction for a cine-MRI benchmark with a cohort of 15 healthy volunteers. We
expect that our method will enable a more precise quantification of cardiac
strains through accurate deformation fields that are consistent with fiber
physiology, without requiring imaging techniques more sophisticated than MRI.

</details>


### [238] [Virtual staining for 3D X-ray histology of bone implants](https://arxiv.org/abs/2509.09235)
*Sarah C. Irvine,Christian Lucas,Diana Krüger,Bianca Guedert,Julian Moosmann,Berit Zeller-Plumhoff*

Main category: eess.IV

TL;DR: 本研究将深度学习虚拟染色技术引入三维X射线成像领域，利用改进的CycleGAN网络将微CT灰度图像转换为具有生化特异性的虚拟染色图像，从而增强了组织的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统三维X射线组织学技术虽然是非侵入性且能进行体积成像，但其固有的灰度图像对比度限制了其生化特异性，不如传统组织学染色。

Method: 作者利用超过50对骨植入物样本的微CT图像和甲苯胺蓝染色的组织学图像进行配准。训练了一个针对有限配对数据定制的改进版CycleGAN网络，该网络融入了像素级监督和灰度一致性项。训练过程中对全视野组织学图像进行降采样以匹配CT体素大小，并采用在线数据增强进行基于补丁的训练。

Result: 该方法在SSIM、PSNR和LPIPS等指标上优于Pix2Pix和标准CycleGAN基线模型。模型能够生成组织学上真实的彩色输出，同时保留高分辨率结构细节，并成功再现了新骨形成等特征。训练后的模型可应用于完整的CT体数据，生成虚拟染色的三维数据集，无需额外样本制备即可提高可解释性。然而，植入物降解层的描绘存在一定变异性。

Conclusion: 本研究将虚拟染色技术首次引入三维X射线成像，为生物医学研究中实现具有化学信息、免标记的组织表征提供了一个可扩展的途径。

Abstract: Three-dimensional X-ray histology techniques offer a non-invasive alternative
to conventional 2D histology, enabling volumetric imaging of biological tissues
without the need for physical sectioning or chemical staining. However, the
inherent greyscale image contrast of X-ray tomography limits its biochemical
specificity compared to traditional histological stains. Within digital
pathology, deep learning-based virtual staining has demonstrated utility in
simulating stained appearances from label-free optical images. In this study,
we extend virtual staining to the X-ray domain by applying cross-modality image
translation to generate artificially stained slices from
synchrotron-radiation-based micro-CT scans. Using over 50 co-registered image
pairs of micro-CT and toluidine blue-stained histology from bone-implant
samples, we trained a modified CycleGAN network tailored for limited paired
data. Whole slide histology images were downsampled to match the voxel size of
the CT data, with on-the-fly data augmentation for patch-based training. The
model incorporates pixelwise supervision and greyscale consistency terms,
producing histologically realistic colour outputs while preserving
high-resolution structural detail. Our method outperformed Pix2Pix and standard
CycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the
model can be applied to full CT volumes to generate virtually stained 3D
datasets, enhancing interpretability without additional sample preparation.
While features such as new bone formation were able to be reproduced, some
variability in the depiction of implant degradation layers highlights the need
for further training data and refinement. This work introduces virtual staining
to 3D X-ray imaging and offers a scalable route for chemically informative,
label-free tissue characterisation in biomedical research.

</details>


### [239] [Dynamic Structural Recovery Parameters Enhance Prediction of Visual Outcomes After Macular Hole Surgery](https://arxiv.org/abs/2509.09227)
*Yinzheng Zhao,Zhihao Zhao,Rundong Jiang,Louisa Sackewitz,Quanmin Liang,Mathias Maier,Daniel Zapp,Peter Charbel Issa,Mohammad Ali Nasseri*

Main category: eess.IV

TL;DR: 本研究引入动态结构参数并将其整合到多模态深度学习框架中，以预测特发性全层黄斑裂孔患者的术后视力恢复。


<details>
  <summary>Details</summary>
Motivation: 旨在引入新型动态结构参数，并评估其与多模态深度学习框架的整合，以更准确地预测特发性全层黄斑裂孔（iFTMH）患者的术后视力恢复情况。

Method: 利用公开的纵向OCT数据集（涵盖术前、2周、3个月、6个月和12个月五个阶段）。开发了阶段特异性分割模型以描绘相关结构，并通过自动化流程提取定量、复合、定性和动态特征。构建了有/无动态参数的二元逻辑回归模型，以评估动态参数的增量预测价值。开发了一个结合临床变量、OCT衍生特征和原始OCT图像的多模态深度学习模型，并与回归模型进行基准测试。

Result: 分割模型在所有时间点均达到高精度（平均Dice > 0.89）。单变量和多变量分析表明，基底直径、椭圆体带完整性和黄斑裂孔面积是显著的视力预测因子。整合动态恢复率一致地提高了逻辑回归的AUC，尤其是在3个月随访时。多模态深度学习模型在每个阶段均优于逻辑回归模型，AUC和总体准确性更高（差异高达0.12），证明了原始图像体积和动态参数的互补价值。

Conclusion: 将动态参数整合到多模态深度学习模型中能显著提高术后视力恢复的预测准确性。这一全自动化流程有望成为黄斑裂孔手术中个性化术后管理的重要临床决策支持工具。

Abstract: Purpose: To introduce novel dynamic structural parameters and evaluate their
integration within a multimodal deep learning (DL) framework for predicting
postoperative visual recovery in idiopathic full-thickness macular hole (iFTMH)
patients. Methods: We utilized a publicly available longitudinal OCT dataset at
five stages (preoperative, 2 weeks, 3 months, 6 months, and 12 months). A stage
specific segmentation model delineated related structures, and an automated
pipeline extracted quantitative, composite, qualitative, and dynamic features.
Binary logistic regression models, constructed with and without dynamic
parameters, assessed their incremental predictive value for best-corrected
visual acuity (BCVA). A multimodal DL model combining clinical variables,
OCT-derived features, and raw OCT images was developed and benchmarked against
regression models. Results: The segmentation model achieved high accuracy
across all timepoints (mean Dice > 0.89). Univariate and multivariate analyses
identified base diameter, ellipsoid zone integrity, and macular hole area as
significant BCVA predictors (P < 0.05). Incorporating dynamic recovery rates
consistently improved logistic regression AUC, especially at the 3-month
follow-up. The multimodal DL model outperformed logistic regression, yielding
higher AUCs and overall accuracy at each stage. The difference is as high as
0.12, demonstrating the complementary value of raw image volume and dynamic
parameters. Conclusions: Integrating dynamic parameters into the multimodal DL
model significantly enhances the accuracy of predictions. This fully automated
process therefore represents a promising clinical decision support tool for
personalized postoperative management in macular hole surgery.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [240] [Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems](https://arxiv.org/abs/2509.09204)
*Chin Yuen Kwok,Jia Qi Yip,Zhen Qiu,Chi Hung Chi,Kwok Yan Lam*

Main category: cs.SD

TL;DR: 本文提出一种名为“真实语音交叉测试”的新型评估框架，旨在解决现有音频深度伪造检测模型评估中合成器权重不均和真实语音多样性不足的问题，通过结合多样化数据集和聚合EER来提高评估的鲁棒性和可解释性，并发布了相关数据集。


<details>
  <summary>Details</summary>
Motivation: 现有音频深度伪造检测（ADD）模型的评估方法存在两大问题：一是单一的等错误率（EER）评估会过度偏重样本量大的合成器，降低了EER的可靠性；二是大多数ADD数据集缺乏真实语音的多样性（如单一环境和语风格），限制了其模拟真实世界条件的能力。

Method: 本文提出了一种名为“真实语音交叉测试”（bona fide cross-testing）的新型评估框架。该框架通过整合多样化的真实语音数据集，并聚合等错误率（EER）来提供更平衡的评估。

Result: 该评估方法提高了评估的鲁棒性和可解释性。研究人员使用此框架对九种真实语音类型的150多个合成器进行了基准测试，并发布了一个新的数据集。

Conclusion: 所提出的“真实语音交叉测试”框架有效解决了传统ADD模型评估方法的局局限性，通过提供更鲁棒、更可解释的评估以及新数据集的发布，为未来研究奠定了基础。

Abstract: Audio deepfake detection (ADD) models are commonly evaluated using datasets
that combine multiple synthesizers, with performance reported as a single Equal
Error Rate (EER). However, this approach disproportionately weights
synthesizers with more samples, underrepresenting others and reducing the
overall reliability of EER. Additionally, most ADD datasets lack diversity in
bona fide speech, often featuring a single environment and speech style (e.g.,
clean read speech), limiting their ability to simulate real-world conditions.
To address these challenges, we propose bona fide cross-testing, a novel
evaluation framework that incorporates diverse bona fide datasets and
aggregates EERs for more balanced assessments. Our approach improves robustness
and interpretability compared to traditional evaluation methods. We benchmark
over 150 synthesizers across nine bona fide speech types and release a new
dataset to facilitate further research at
https://github.com/cyaaronk/audio_deepfake_eval.

</details>


### [241] [DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech](https://arxiv.org/abs/2509.09631)
*Ngoc-Son Nguyen,Hieu-Nghia Huynh-Nguyen,Thanh V. T. Tran,Truong-Son Hy,Van Nguyen*

Main category: cs.SD

TL;DR: DiFlow-TTS是首个采用纯离散流匹配的零样本文本转语音（TTS）模型，解决了现有方法推理慢的问题，通过充分利用离散表示，实现高质量、低延迟的语音合成。


<details>
  <summary>Details</summary>
Motivation: 现有零样本TTS方法（如基于语言模型、扩散模型和流匹配）存在推理速度慢和重复伪影的问题。此外，现有流匹配方法将离散标记嵌入连续空间，未能充分利用离散表示的优势。

Method: 本文提出DiFlow-TTS模型，首次探索纯离散流匹配进行语音合成。它在一个紧凑统一的架构中显式建模分解的语音属性，并通过上下文学习，结合文本内容、参考语音的韵律和声学属性实现属性克隆。模型还采用分解流预测机制，为韵律和声学细节设置独立的头部。

Result: DiFlow-TTS在自然度、韵律、说话人风格保留和能量控制等关键指标上表现出色。模型尺寸紧凑，推理延迟低，生成语音速度比现有基线快25.8倍。

Conclusion: DiFlow-TTS通过引入纯离散流匹配，为零样本TTS提供了一种有效解决方案，实现了高质量、快速且高效的语音合成，同时克服了先前方法的局限性。

Abstract: Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that
mimics the voice of an unseen speaker using only a short reference sample,
requiring not only speaker adaptation but also accurate modeling of prosodic
attributes. Recent approaches based on language models, diffusion, and flow
matching have shown promising results in zero-shot TTS, but still suffer from
slow inference and repetition artifacts. Discrete codec representations have
been widely adopted for speech synthesis, and recent works have begun to
explore diffusion models in purely discrete settings, suggesting the potential
of discrete generative modeling for speech synthesis. However, existing
flow-matching methods typically embed these discrete tokens into a continuous
space and apply continuous flow matching, which may not fully leverage the
advantages of discrete representations. To address these challenges, we
introduce DiFlow-TTS, which, to the best of our knowledge, is the first model
to explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS
explicitly models factorized speech attributes within a compact and unified
architecture. It leverages in-context learning by conditioning on textual
content, along with prosodic and acoustic attributes extracted from a reference
speech, enabling effective attribute cloning in a zero-shot setting. In
addition, the model employs a factorized flow prediction mechanism with
distinct heads for prosody and acoustic details, allowing it to learn
aspect-specific distributions. Experimental results demonstrate that DiFlow-TTS
achieves promising performance in several key metrics, including naturalness,
prosody, preservation of speaker style, and energy control. It also maintains a
compact model size and achieves low-latency inference, generating speech up to
25.8 times faster than the latest existing baselines.

</details>


### [242] [Adaptive Knowledge Distillation using a Device-Aware Teacher for Low-Complexity Acoustic Scene Classification](https://arxiv.org/abs/2509.09262)
*Seung Gyu Jeong,Seong Eun Kim*

Main category: cs.SD

TL;DR: 本文描述了DCASE 2025挑战赛任务1的提交系统，该系统通过知识蒸馏框架，利用双教师集成（包含一个基于DAFA损失的泛化专家教师）和设备特定微调，以应对低复杂度设备鲁棒性声学场景分类的挑战，并在开发集上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决DCASE 2025挑战赛任务1中声学场景分类面临的严格复杂性约束和对已知/未知设备进行鲁棒泛化的双重挑战，并充分利用测试时可用的设备标签这一新规则。

Method: 该系统基于知识蒸馏框架，其中一个高效的CP-MobileNet学生模型从一个紧凑、专业的双教师集成中学习。双教师集成了基线PaSST教师（标准交叉熵训练）和一个“泛化专家”教师。该专家教师采用新颖的设备感知特征对齐（DAFA）损失进行训练，以明确构建特征空间以实现设备鲁棒性。为利用测试时的设备标签，蒸馏后的学生模型还会进行设备特定的最终微调阶段。

Result: 所提出的系统在开发集上达到了57.93%的最终准确率，相较于官方基线，表现出显著的改进，尤其是在未知设备上的性能提升更为明显。

Conclusion: 该系统成功应对了低复杂度及设备鲁棒性挑战，通过结合知识蒸馏、DAFA损失训练的泛化专家教师以及设备特定微调，在声学场景分类任务中实现了显著的性能提升，尤其是在未知设备上的泛化能力。

Abstract: In this technical report, we describe our submission for Task 1,
Low-Complexity Device-Robust Acoustic Scene Classification, of the DCASE 2025
Challenge. Our work tackles the dual challenges of strict complexity
constraints and robust generalization to both seen and unseen devices, while
also leveraging the new rule allowing the use of device labels at test time.
Our proposed system is based on a knowledge distillation framework where an
efficient CP-MobileNet student learns from a compact, specialized two-teacher
ensemble. This ensemble combines a baseline PaSST teacher, trained with
standard cross-entropy, and a 'generalization expert' teacher. This expert is
trained using our novel Device-Aware Feature Alignment (DAFA) loss, adapted
from prior work, which explicitly structures the feature space for device
robustness. To capitalize on the availability of test-time device labels, the
distilled student model then undergoes a final device-specific fine-tuning
stage. Our proposed system achieves a final accuracy of 57.93\% on the
development set, demonstrating a significant improvement over the official
baseline, particularly on unseen devices.

</details>


### [243] [Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates](https://arxiv.org/abs/2509.09550)
*Harry Julia,Rachel Beeson,Lohith Konathala,Johanna Ulin,Jiameng Gao*

Main category: cs.SD

TL;DR: 本文介绍了NeuCodec，一种基于有限标量量化（FSQ）的神经音频编解码器，并证明了FSQ编码固有的冗余性，使其在噪声信道传输中具有优于残差向量量化（RVQ）的卓越鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 神经音频编解码器（NACs）因其优异的速率-失真性能和与大型语言模型（LLMs）的兼容性而被广泛采用。然而，现有多数编解码器依赖于RVQ，而FSQ作为一种新替代方案，简化了训练并原生支持单一码本。研究动机在于探索FSQ是否能提供更好的信道鲁棒性。

Method: 本文提出了基于FSQ的NeuCodec。首先，通过编码器蒸馏实验，证明了两个不同的编码器可以为相同的音频生成截然不同的代码序列，同时保持相似的重建质量，以此揭示FSQ固有的冗余性。其次，通过模拟噪声信道传输代码序列，比较了FSQ和RVQ编解码器在位级扰动下的鲁棒性表现。

Result: 研究结果显示，FSQ确实编码了固有的冗余性，这使得两个不同的编码器可以生成差异巨大的代码序列，但仍能保持相同的量化器和解码器下可比较的重建质量。此外，与RVQ编解码器相比，FSQ在模拟噪声信道传输时，展现出显著优越的位级扰动鲁棒性。

Conclusion: FSQ作为一种神经音频编解码器方法，通过其编码的固有冗余性，能够提供在噪声信道中传输时更强大的鲁棒性，使其成为RVQ的一个有吸引力的替代方案。

Abstract: Neural Audio Codecs (NACs) have become increasingly adopted in speech
processing tasks due to their excellent rate-distortion performance and
compatibility with Large Language Models (LLMs) as discrete feature
representations for audio generation. While most existing codecs rely on
Residual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has
recently emerged as a compelling alternative that simplifies training and
natively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,
and show that FSQ encodes baked-in redundancy which produces an encoding which
is robust when transmitted through noisy channels. First, through an encoder
distillation experiment, we show that two different encoders can learn to
encode identical audio into vastly different code sequences whilst maintaining
comparable reconstruction quality with the same quantizer and decoder. Second,
we demonstrate that FSQ has vastly superior bit-level perturbation robustness
by comparing the performance of RVQ and FSQ codecs when simulating the
transmission of code sequences through a noisy channel.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [244] [Scalable extensions to given-data Sobol' index estimators](https://arxiv.org/abs/2509.09078)
*Teresa Portone,Bert Debusschere,Samantha Yang,Emiliano Islas-Quinones,T. Patrick Xiao*

Main category: stat.ML

TL;DR: 本文提出了一种改进的Sobol'指数给定数据方法，使其能够高效地对具有超多输入（如神经网络）的大型模型进行方差敏感性分析，解决了现有方法的内存限制和对非标准输入分布的兼容性问题。


<details>
  <summary>Details</summary>
Motivation: 现有给定数据方法在计算Sobol'指数时，难以应用于具有极大量输入（如超过$10^4$个参数）的模型，因为这些模型需要同时在内存中存储所有输入输出评估，这很快变得不切实际。此外，现有方法不适用于具有许多重复值的非标准输入分布。

Method: 该研究对现有给定数据Sobol'指数方法进行了实用性扩展，包括：1) 带有任意分区的给定数据Sobol'指数估计器的通用定义；2) 一种以批处理方式处理输入输出样本的流式算法；3) 一种过滤掉因统计噪声而与零指数无法区分的小指数的启发式方法。

Result: 研究表明，现有方法中使用的等概率分区即使在样本量较大时也可能导致Sobol'指数估计中出现显著偏差。新提出的流式算法在内存要求更低的情况下，能达到与当前一次性处理所有样本的方法相当的准确性和运行时间。这些方法在神经网络建模的两个应用问题上得到了验证。

Conclusion: 该工作通过改进给定数据Sobol'指数方法，使其能够高效地对具有大量输入（如神经网络）的模型进行方差敏感性分析，克服了现有方法的内存和输入分布限制，并提供了更准确和内存效率更高的估计方法。

Abstract: Given-data methods for variance-based sensitivity analysis have significantly
advanced the feasibility of Sobol' index computation for computationally
expensive models and models with many inputs. However, the limitations of
existing methods still preclude their application to models with an extremely
large number of inputs. In this work, we present practical extensions to the
existing given-data Sobol' index method, which allow variance-based sensitivity
analysis to be efficiently performed on large models such as neural networks,
which have $>10^4$ parameterizable inputs. For models of this size, holding all
input-output evaluations simultaneously in memory -- as required by existing
methods -- can quickly become impractical. These extensions also support
nonstandard input distributions with many repeated values, which are not
amenable to equiprobable partitions employed by existing given-data methods.
  Our extensions include a general definition of the given-data Sobol' index
estimator with arbitrary partition, a streaming algorithm to process
input-output samples in batches, and a heuristic to filter out small indices
that are indistinguishable from zero indices due to statistical noise. We show
that the equiprobable partition employed in existing given-data methods can
introduce significant bias into Sobol' index estimates even at large sample
sizes and provide numerical analyses that demonstrate why this can occur. We
also show that our streaming algorithm can achieve comparable accuracy and
runtimes with lower memory requirements, relative to current methods which
process all samples at once. We demonstrate our novel developments on two
application problems in neural network modeling.

</details>


### [245] [Global Optimization of Stochastic Black-Box Functions with Arbitrary Noise Distributions using Wilson Score Kernel Density Estimation](https://arxiv.org/abs/2509.09238)
*Thorbjørn Mosekjær Iversen,Lars Carøe Sørensen,Simon Faarvang Mathiesen,Henrik Gordon Petersen*

Main category: stat.ML

TL;DR: 本文提出Wilson Score核密度估计器（WS-KDE）能为输出在[0,1]范围内的随机黑盒函数提供优秀的置信区间，从而拓宽其在贝叶斯优化中的应用，并在仿真和实际机器人问题中得到验证。


<details>
  <summary>Details</summary>
Motivation: 机器人领域的许多优化问题涉及耗时、黑盒且随机的函数评估。贝叶斯优化虽然能有效解决这类问题，但其成功高度依赖于函数估计器提供的信息性置信区间。现有估计器通常需要大量函数评估或依赖于对干扰的建模，效率不高。

Method: 本文证明了Wilson Score核密度估计器（WS-KDE）所提供的置信区间，对于输出值限定在闭区间[0,1]内的任何随机函数，都能作为优秀的边界，且不依赖于输出的分布。随后，在贝叶斯优化背景下，通过仿真展示了WS-KDE的特性，并将其应用于振动部件送料器的自动陷阱设计问题。

Result: 研究结果表明，WS-KDE所提供的置信区间可以作为优秀的边界，适用于输出值限定在[0,1]范围内的任何随机函数，且不受输出分布的影响。这一发现极大地扩展了WS-KDE在更广泛成本函数上进行稳定全局优化的应用。WS-KDE在仿真中展示了其在贝叶斯优化中的良好性能，并成功应用于振动部件送料器的自动陷阱设计问题。

Conclusion: WS-KDE为贝叶斯优化处理输出在[0,1]范围内的随机黑盒函数提供了一种有效且稳健的函数估计方法，其优越的置信区间特性扩展了贝叶斯优化的适用范围和稳定性，尤其适用于对输出分布知之甚少或难以建模的场景。

Abstract: Many optimization problems in robotics involve the optimization of
time-expensive black-box functions, such as those involving complex simulations
or evaluation of real-world experiments. Furthermore, these functions are often
stochastic as repeated experiments are subject to unmeasurable disturbances.
Bayesian optimization can be used to optimize such methods in an efficient
manner by deploying a probabilistic function estimator to estimate with a given
confidence so that regions of the search space can be pruned away.
Consequently, the success of the Bayesian optimization depends on the function
estimator's ability to provide informative confidence bounds. Existing function
estimators require many function evaluations to infer the underlying confidence
or depend on modeling of the disturbances. In this paper, it is shown that the
confidence bounds provided by the Wilson Score Kernel Density Estimator
(WS-KDE) are applicable as excellent bounds to any stochastic function with an
output confined to the closed interval [0;1] regardless of the distribution of
the output. This finding opens up the use of WS-KDE for stable global
optimization on a wider range of cost functions. The properties of WS-KDE in
the context of Bayesian optimization are demonstrated in simulation and applied
to the problem of automated trap design for vibrational part feeders.

</details>


### [246] [Low-degree lower bounds via almost orthonormal bases](https://arxiv.org/abs/2509.09353)
*Alexandra Carpentier,Simone Maria Giancola,Christophe Giraud,Nicolas Verzelen*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Low-degree polynomials have emerged as a powerful paradigm for providing
evidence of statistical-computational gaps across a variety of high-dimensional
statistical models [Wein25]. For detection problems -- where the goal is to
test a planted distribution $\mathbb{P}'$ against a null distribution
$\mathbb{P}$ with independent components -- the standard approach is to bound
the advantage using an $\mathbb{L}^2(\mathbb{P})$-orthonormal family of
polynomials. However, this method breaks down for estimation tasks or more
complex testing problems where $\mathbb{P}$ has some planted structures, so
that no simple $\mathbb{L}^2(\mathbb{P})$-orthogonal polynomial family is
available. To address this challenge, several technical workarounds have been
proposed [SW22,SW25], though their implementation can be delicate. In this
work, we propose a more direct proof strategy. Focusing on random graph models,
we construct a basis of polynomials that is almost orthonormal under
$\mathbb{P}$, in precisely those regimes where statistical-computational gaps
arise. This almost orthonormal basis not only yields a direct route to
establishing low-degree lower bounds, but also allows us to explicitly identify
the polynomials that optimize the low-degree criterion. This, in turn, provides
insights into the design of optimal polynomial-time algorithms. We illustrate
the effectiveness of our approach by recovering known low-degree lower bounds,
and establishing new ones for problems such as hidden subcliques, stochastic
block models, and seriation models.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [247] [Generative Engine Optimization: How to Dominate AI Search](https://arxiv.org/abs/2509.08919)
*Mahe Chen,Xiaoxuan Wang,Kaiwen Chen,Nick Koudas*

Main category: cs.IR

TL;DR: 生成式AI搜索引擎正在重塑信息检索，催生了“生成式引擎优化”（GEO）新范式。本文对比分析了AI搜索与传统搜索，发现AI搜索显著偏向口碑媒体，并基于此提出了GEO的实践策略。


<details>
  <summary>Details</summary>
Motivation: 生成式AI驱动的搜索引擎（如ChatGPT、Perplexity、Gemini）正迅速普及，根本性地改变了信息检索方式，从传统排名列表转向合成的、带引用的答案。这挑战了现有搜索引擎优化（SEO）实践，亟需一种新的范式，即生成式引擎优化（GEO）。

Method: 本文通过对AI搜索和传统网页搜索（Google）进行全面比较分析。研究通过跨多个垂直领域、语言和查询改述的大规模、受控实验，量化了这些系统信息来源的关键差异。

Result: 研究发现AI搜索系统性地、压倒性地偏爱口碑媒体（第三方、权威来源），而非品牌自有和社交内容，这与Google的平衡组合形成鲜明对比。此外，不同的AI搜索服务在领域多样性、新鲜度、跨语言稳定性以及对措辞的敏感性方面也存在显著差异。

Conclusion: 基于实证结果，本文提出了战略性的GEO议程，并为从业者提供了可操作的指导：1）内容工程化以提高机器可扫描性和可验证性；2）主导口碑媒体以建立AI感知的权威性；3）采用针对特定引擎和语言的策略；4）帮助利基市场参与者克服固有的“大品牌偏见”。本研究为在新的生成式搜索环境中获得可见性提供了基础的实证分析和战略框架。

Abstract: The rapid adoption of generative AI-powered search engines like ChatGPT,
Perplexity, and Gemini is fundamentally reshaping information retrieval, moving
from traditional ranked lists to synthesized, citation-backed answers. This
shift challenges established Search Engine Optimization (SEO) practices and
necessitates a new paradigm, which we term Generative Engine Optimization
(GEO).
  This paper presents a comprehensive comparative analysis of AI Search and
traditional web search (Google). Through a series of large-scale, controlled
experiments across multiple verticals, languages, and query paraphrases, we
quantify critical differences in how these systems source information. Our key
findings reveal that AI Search exhibit a systematic and overwhelming bias
towards Earned media (third-party, authoritative sources) over Brand-owned and
Social content, a stark contrast to Google's more balanced mix. We further
demonstrate that AI Search services differ significantly from each other in
their domain diversity, freshness, cross-language stability, and sensitivity to
phrasing.
  Based on these empirical results, we formulate a strategic GEO agenda. We
provide actionable guidance for practitioners, emphasizing the critical need
to: (1) engineer content for machine scannability and justification, (2)
dominate earned media to build AI-perceived authority, (3) adopt
engine-specific and language-aware strategies, and (4) overcome the inherent
"big brand bias" for niche players. Our work provides the foundational
empirical analysis and a strategic framework for achieving visibility in the
new generative search landscape.

</details>


### [248] [Envy-Free but Still Unfair: Envy-Freeness Up To One Item (EF-1) in Personalized Recommendation](https://arxiv.org/abs/2509.09037)
*Amanda Aird,Ben Armstrong,Nicholas Mattei,Robin Burke*

Main category: cs.IR

TL;DR: 本立场论文概述了无羡慕性（Envy-freeness）及其近似概念（EF-1）在经济学和推荐系统中的应用，并论证了在个性化环境中，羡慕不适合作为衡量公平性的指标。


<details>
  <summary>Details</summary>
Motivation: 无羡慕性及其近似概念作为公平性概念在经济学和推荐系统领域日益流行。该研究的动机是挑战这些概念在个性化环境中的适用性，指出羡慕不适合衡量个性化系统中的公平性。

Method: 本文作为一篇简短的立场论文，通过以下方式进行论证：1. 概述无羡慕性及其在经济学中的应用。2. 概述无羡慕性及其在推荐系统中的应用。3. 阐述并论证为什么羡慕不适合作为衡量个性化环境中公平性的标准。

Result: 研究结果指出，在个性化起关键作用的环境中，羡慕并非衡量公平性的恰当指标。

Conclusion: 尽管无羡慕性及其近似概念在传统领域和推荐系统中被广泛用作公平性标准，但它们不适用于衡量个性化系统中的公平性。

Abstract: Envy-freeness and the relaxation to Envy-freeness up to one item (EF-1) have
been used as fairness concepts in the economics, game theory, and social choice
literatures since the 1960s, and have recently gained popularity within the
recommendation systems communities. In this short position paper we will give
an overview of envy-freeness and its use in economics and recommendation
systems; and illustrate why envy is not appropriate to measure fairness for use
in settings where personalization plays a role.

</details>


### [249] [Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations](https://arxiv.org/abs/2509.09651)
*Zakaria El Kassimi,Fares Fourati,Mohamed-Slim Alouini*

Main category: cs.IR

TL;DR: 本文研究无线电法规领域的问答系统，提出了一个电信专用RAG管道，并构建了该领域的首个多项选择评估数据集。该方法在检索准确率上达到97%，并显著提升了问答生成准确率，尤其对GPT-4o有近12%的相对改进。


<details>
  <summary>Details</summary>
Motivation: 无线电法规领域的问答系统具有法律敏感性和高风险性，需要高度准确和可靠的解决方案。

Method: 提出了一种电信专用的检索增强生成（RAG）管道。构建了该领域首个多项选择评估数据集（通过自动化过滤和人工验证）。定义了领域特定的检索指标。

Result: 在领域特定检索指标下，检索器达到了约97%的准确率。该方法持续提高了所有测试模型的生成准确率，特别是使GPT-4o获得了近12%的相对改进，而单纯插入文档仅获得不到1%的提升。

Conclusion: 精心设计的基于RAG的上下文关联方法，为监管问答系统提供了一个简单但强大的基线，并是一种有效的领域特定解决方案。

Abstract: We study question answering in the domain of radio regulations, a legally
sensitive and high-stakes area. We propose a telecom-specific
Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,
the first multiple-choice evaluation set for this domain, constructed from
authoritative sources using automated filtering and human validation. To assess
retrieval quality, we define a domain-specific retrieval metric, under which
our retriever achieves approximately 97% accuracy. Beyond retrieval, our
approach consistently improves generation accuracy across all tested models. In
particular, while naively inserting documents without structured retrieval
yields only marginal gains for GPT-4o (less than 1%), applying our pipeline
results in nearly a 12% relative improvement. These findings demonstrate that
carefully targeted grounding provides a simple yet strong baseline and an
effective domain-specific solution for regulatory question answering. All code
and evaluation scripts, along with our derived question-answer dataset, are
available at https://github.com/Zakaria010/Radio-RAG.

</details>


### [250] [We're Still Doing It (All) Wrong: Recommender Systems, Fifteen Years Later](https://arxiv.org/abs/2509.09414)
*Alan Said,Maria Soledad Pera,Michael D. Ekstrand*

Main category: cs.IR

TL;DR: 本文指出，推荐系统研究中长期存在的根本性缺陷至今仍未解决，反而被日益增加的复杂性所掩盖，呼吁进行一场全面的范式转变，超越技术改革，转向以人为本和可持续发展的研究议程。


<details>
  <summary>Details</summary>
Motivation: 自2011年Xavier Amatriain指出推荐系统研究存在根本性错误以来，这些问题不仅没有得到纠正，反而随着领域复杂性的增加而变得更加隐蔽和系统化。作者旨在重新审视Amatriain的诊断，并强调这些未能解决的缺陷，以推动领域进行深刻反思和变革。

Method: 本文通过重新审视Amatriain的批判，并结合可复现性、评估方法、环境影响和参与式设计等最新研究成果，分析了推荐系统领域中持续存在的概念性、认知性及基础设施方面的缺陷。同时，文中也提及并讨论了当前社区主导的各项改革倡议。

Result: 研究发现，Amatriain在多年前指出的许多根本性缺陷，包括统计误解和方法论捷径，至今仍以更微妙或系统性的形式存在。领域的加速复杂化已超越了其自我反省的能力。尽管存在社区主导的改革尝试，但仅靠新的指标或工具不足以带来有意义的改变。

Conclusion: 要实现推荐系统研究的真正变革，需要从根本上重新定义其研究目的、服务对象以及知识生产和验证方式。最终呼吁建立一个基于认知谦逊、关注人类影响和可持续实践的推荐系统研究议程，而不仅仅是技术层面的改革。

Abstract: In 2011, Xavier Amatriain sounded the alarm: recommender systems research was
"doing it all wrong" [1]. His critique, rooted in statistical misinterpretation
and methodological shortcuts, remains as relevant today as it was then. But
rather than correcting course, we added new layers of sophistication on top of
the same broken foundations. This paper revisits Amatriain's diagnosis and
argues that many of the conceptual, epistemological, and infrastructural
failures he identified still persist, in more subtle or systemic forms. Drawing
on recent work in reproducibility, evaluation methodology, environmental
impact, and participatory design, we showcase how the field's accelerating
complexity has outpaced its introspection. We highlight ongoing community-led
initiatives that attempt to shift the paradigm, including workshops, evaluation
frameworks, and calls for value-sensitive and participatory research. At the
same time, we contend that meaningful change will require not only new metrics
or better tooling, but a fundamental reframing of what recommender systems
research is for, who it serves, and how knowledge is produced and validated.
Our call is not just for technical reform, but for a recommender systems
research agenda grounded in epistemic humility, human impact, and sustainable
practice.

</details>
