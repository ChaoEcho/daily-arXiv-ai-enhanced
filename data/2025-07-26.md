<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 26]
- [cs.CV](#cs.CV) [Total: 24]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.LG](#cs.LG) [Total: 23]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning](https://arxiv.org/abs/2507.17842)
*Yimeng Zhang,Tian Wang,Jiri Gesi,Ziyi Wang,Yuxuan Lu,Jiacheng Lin,Sinong Zhan,Vianne Gao,Ruochen Jiao,Junze Liu,Kun Qian,Yuxin Tang,Ran Xue,Houyu Zhang,Qingjun Cui,Yufan Guo,Dakuo Wang*

Main category: cs.CL

TL;DR: Shop-R1是一个新的强化学习框架，旨在通过将任务分解为原理生成和动作预测两个阶段，利用内部模型信号和分层奖励机制，显著提升大语言模型（LLMs）在在线购物环境中模拟人类行为的推理能力，实现了超过65%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过大语言模型合成原理来增强推理能力，但其表现受限于生成原理的模型本身的推理能力。本研究旨在克服这一局限，提升LLM在在线购物环境中模拟真实人类行为的推理能力。

Method: 引入Shop-R1强化学习框架，将人类行为模拟任务分解为原理生成和动作预测两个阶段。原理生成阶段利用内部模型信号（如逻辑分布）进行自监督指导。动作预测阶段提出分层、难度感知的奖励结构，以防止奖励欺骗并实现细粒度奖励分配，评估高级动作类型及细粒度子动作细节，并根据难度按比例奖励。

Result: 实验结果表明，与基线方法相比，本方法取得了超过65%的相对改进。

Conclusion: Shop-R1框架通过其独特的两阶段设计（自监督原理生成和分层奖励动作预测），有效增强了LLM在模拟在线购物人类行为方面的推理能力，显著优于现有方法。

Abstract: Large Language Models (LLMs) have recently demonstrated strong potential in
generating 'believable human-like' behavior in web environments. Prior work has
explored augmenting training data with LLM-synthesized rationales and applying
supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can
improve downstream action prediction. However, the performance of such
approaches remains inherently bounded by the reasoning capabilities of the
model used to generate the rationales. In this paper, we introduce Shop-R1, a
novel reinforcement learning (RL) framework aimed at enhancing the reasoning
ability of LLMs for simulation of real human behavior in online shopping
environments Specifically, Shop-R1 decomposes the human behavior simulation
task into two stages: rationale generation and action prediction, each guided
by distinct reward signals. For rationale generation, we leverage internal
model signals (e.g., logit distributions) to guide the reasoning process in a
self-supervised manner. For action prediction, we propose a hierarchical reward
structure with difficulty-aware scaling to prevent reward hacking and enable
fine-grained reward assignment. This design evaluates both high-level action
types and the correctness of fine-grained sub-action details (attributes and
values), rewarding outputs proportionally to their difficulty. Experimental
results show that our method achieves a relative improvement of over 65%
compared to the baseline.

</details>


### [2] [Dynamic and Generalizable Process Reward Modeling](https://arxiv.org/abs/2507.17849)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Qinyuan Cheng,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文提出了一种动态且可泛化的过程奖励模型（DG-PRM），通过奖励树捕获细粒度标准，并利用帕累托优势估计处理多方面奖励信号，有效解决了现有过程奖励模型泛化性差和评估标准静态的问题，在基准测试中表现出色并展现了卓越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型（PRMs）主要依赖启发式方法，难以实现跨领域泛化。同时，LLM-as-judge方法忽视了文本中嵌入的有意义的指导，且静态、粗粒度的评估标准难以适应复杂的流程监督，导致LLMs在复杂场景中获得稠密奖励信号时面临挑战。

Method: 本文提出了动态且可泛化的过程奖励建模（DG-PRM）。该方法包含一个奖励树，用于捕获和存储细粒度、多维度的奖励标准。DG-PRM能够动态选择奖励信号进行分步奖励评分。为处理多方面的奖励信号，创新性地采用了帕累托优势估计来识别区分性的正负样本对。

Result: 实验结果表明，DG-PRM在主流基准测试中取得了令人瞩目的性能，显著提升了模型在需要稠密奖励任务上的表现。进一步分析显示，DG-PRM在分布外（OOD）场景中也能很好地适应，展示了卓越的泛化能力。

Conclusion: DG-PRM通过其动态、可泛化的特性，成功解决了当前过程奖励模型在泛化性、动态适应性和多方面奖励信号处理上的局限性，为LLMs在复杂场景中提供有效、稠密的奖励信号开辟了新途径。

Abstract: Process Reward Models (PRMs) are crucial for guiding Large Language Models
(LLMs) in complex scenarios by providing dense reward signals. However,
existing PRMs primarily rely on heuristic approaches, which struggle with
cross-domain generalization. While LLM-as-judge has been proposed to provide
generalized rewards, current research has focused mainly on feedback results,
overlooking the meaningful guidance embedded within the text. Additionally,
static and coarse-grained evaluation criteria struggle to adapt to complex
process supervision. To tackle these challenges, we propose Dynamic and
Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to
capture and store fine-grained, multi-dimensional reward criteria. DG-PRM
dynamically selects reward signals for step-wise reward scoring. To handle
multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation
to identify discriminative positive and negative pairs. Experimental results
show that DG-PRM achieves stunning performance on prevailing benchmarks,
significantly boosting model performance across tasks with dense rewards.
Further analysis reveals that DG-PRM adapts well to out-of-distribution
scenarios, demonstrating exceptional generalizability.

</details>


### [3] [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.CL

TL;DR: VeriMinder是一个交互式系统，通过引入创新框架和LLM驱动的提示生成，帮助用户在使用自然语言界面数据库时识别并减轻分析问题中的认知偏差，从而提高数据分析质量。


<details>
  <summary>Details</summary>
Motivation: 自然语言数据库接口使数据分析普及化，但同时带来了挑战：缺乏统计背景的用户可能提出带有偏差的分析问题。现有研究多关注Text-to-SQL准确性，而分析问题中的认知偏差问题尚未充分解决。

Method: 开发了VeriMinder系统，包含三项核心创新：1) 上下文语义偏差映射框架；2) 操作“难以改变”原则的分析框架；3) 优化的大型语言模型（LLM）驱动系统，通过多候选、批评反馈和自我反思生成高质量、任务特定提示。系统以Web应用形式实现并开源。

Result: 用户测试显示，82.5%的参与者认为VeriMinder积极影响了分析质量。与替代方法相比，VeriMinder在分析的具体性、全面性和准确性指标上至少高出20%。

Conclusion: VeriMinder系统能有效帮助用户避免数据分析中的“错误问题”漏洞，提升分析质量。该系统已作为开源软件发布，以促进进一步研究和社区采纳。

Abstract: Application systems using natural language interfaces to databases (NLIDBs)
have democratized data analysis. This positive development has also brought
forth an urgent challenge to help users who might use these systems without a
background in statistical analysis to formulate bias-free analytical questions.
Although significant research has focused on text-to-SQL generation accuracy,
addressing cognitive biases in analytical questions remains underexplored. We
present VeriMinder, https://veriminder.ai, an interactive system for detecting
and mitigating such analytical vulnerabilities. Our approach introduces three
key innovations: (1) a contextual semantic mapping framework for biases
relevant to specific analysis contexts (2) an analytical framework that
operationalizes the Hard-to-Vary principle and guides users in systematic data
analysis (3) an optimized LLM-powered system that generates high-quality,
task-specific prompts using a structured process involving multiple candidates,
critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience
evaluation, 82.5% participants reported positively impacting the quality of the
analysis. In comparative evaluation, VeriMinder scored significantly higher
than alternative approaches, at least 20% better when considered for metrics of
the analysis's concreteness, comprehensiveness, and accuracy. Our system,
implemented as a web application, is set to help users avoid "wrong question"
vulnerability during data analysis. VeriMinder code base with prompts,
https://reproducibility.link/veriminder, is available as an MIT-licensed
open-source software to facilitate further research and adoption within the
community.

</details>


### [4] [One Whisper to Grade Them All](https://arxiv.org/abs/2507.17918)
*Nhan Phan,Anusha Porwal,Yaroslav Getman,Ekaterina Voskoboinik,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

TL;DR: 本文提出了一种高效的端到端自动口语评估（ASA）方法，该方法使用单个Whisper-small编码器处理多部分口语测试，并通过轻量级聚合器预测最终得分，从而实现卓越的性能和数据效率，适用于大规模语言学习系统。


<details>
  <summary>Details</summary>
Motivation: 为2025年Speak & Improve挑战赛开发一种高效、整体的自动口语评估（ASA）方法，以解决现有系统在多部分二语测试中可能存在的效率问题、推断时间过长以及对转录和分部分模型的需求，从而使其更适用于大规模计算机辅助语言学习（CALL）系统。

Method: 该系统采用单一的Whisper-small编码器来处理所有四个口语回答，并通过一个轻量级聚合器整合所有信息，直接预测最终得分，从而消除了对转录和分部分模型的需求。此外，论文还提出了一种数据采样策略用于模型训练。

Result: 系统在均方根误差（RMSE）上达到0.384，优于基于文本的基线（0.44），且参数量最多为1.68亿（约Whisper-small的70%）。提出的数据采样策略使得模型仅使用44.8%的语料库说话者进行训练，仍能达到0.383的RMSE，展示了其在处理不平衡类别上的改进性能和强大的数据效率。

Conclusion: 所提出的端到端ASA方法高效、实用，能够有效处理多部分二语测试，在性能上超越基线，同时显著减少了模型尺寸并提升了数据效率，使其成为大规模计算机辅助语言学习系统的理想选择。

Abstract: We present an efficient end-to-end approach for holistic Automatic Speaking
Assessment (ASA) of multi-part second-language tests, developed for the 2025
Speak & Improve Challenge. Our system's main novelty is the ability to process
all four spoken responses with a single Whisper-small encoder, combine all
information via a lightweight aggregator, and predict the final score. This
architecture removes the need for transcription and per-part models, cuts
inference time, and makes ASA practical for large-scale Computer-Assisted
Language Learning systems.
  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming
the text-based baseline (0.44) while using at most 168M parameters (about 70%
of Whisper-small). Furthermore, we propose a data sampling strategy, allowing
the model to train on only 44.8% of the speakers in the corpus and still reach
0.383 RMSE, demonstrating improved performance on imbalanced classes and strong
data efficiency.

</details>


### [5] [Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text](https://arxiv.org/abs/2507.17944)
*Hulayyil Alshammari,Praveen Rao*

Main category: cs.CL

TL;DR: 本研究评估了六种AI文本检测器对DeepSeek生成文本的鲁棒性，包括对抗性攻击（意译和人工润色），并探讨了DeepSeek本身作为检测器的能力。结果显示人工润色显著降低了检测器准确率，而DeepSeek作为检测器表现出色。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的普及引发了对写作诚信的担忧，促使AI检测技术发展。然而，现有研究主要关注ChatGPT等流行模型，对DeepSeek这一新兴LLM的检测效果存在研究空白。此外，对抗性攻击是检测器面临的挑战。

Method: 研究收集了49对人工问答，并用DeepSeek-v3生成了匹配的AI文本。随后，通过意译和人工润色增加了196个对抗性样本。使用AI Text Classifier、Content Detector AI、Copyleaks、QuillBot、GPT-2和GPTZero六款工具检测DeepSeek生成文本。同时，探索DeepSeek通过少量样本提示（few-shot prompting）和思维链推理（CoT）作为检测器识别AI和人类文本的能力。

Result: QuillBot和Copyleaks在原始和意译的DeepSeek文本上表现出色，但其他检测器（如AI Text Classifier和GPT-2）表现不稳定。人工润色是最有效的攻击，导致Copyleaks、QuillBot和GPTZero的准确率分别降至71%、58%和52%。DeepSeek通过少量样本提示和CoT作为检测器时显示出高准确率，最佳五样本结果仅错误分类了49个样本中的1个（AI召回率96%，人类召回率100%）。

Conclusion: 现有AI检测工具对DeepSeek生成文本的识别能力不一，且易受人工润色等对抗性攻击影响。而DeepSeek本身结合少量样本提示和思维链推理，在AI文本检测方面展现出强大潜力。

Abstract: Large language models (LLMs) have rapidly transformed the creation of written
materials. LLMs have led to questions about writing integrity, thereby driving
the creation of artificial intelligence (AI) detection technologies.
Adversarial attacks, such as standard and humanized paraphrasing, inhibit
detectors' ability to detect machine-generated text. Previous studies have
mainly focused on ChatGPT and other well-known LLMs and have shown varying
accuracy across detectors. However, there is a clear gap in the literature
about DeepSeek, a recently published LLM. Therefore, in this work, we
investigate whether six generally accessible AI detection tools -- AI Text
Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can
consistently recognize text generated by DeepSeek. The detectors were exposed
to the aforementioned adversarial attacks. We also considered DeepSeek as a
detector by performing few-shot prompting and chain-of-thought reasoning (CoT)
for classifying AI and human-written text. We collected 49 human-authored
question-answer pairs from before the LLM era and generated matching responses
using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied
adversarial techniques such as paraphrasing and humanizing to add 196 more
samples. These were used to challenge detector robustness and assess accuracy
impact. While QuillBot and Copyleaks showed near-perfect performance on
original and paraphrased DeepSeek text, others -- particularly AI Text
Classifier and GPT-2 -- showed inconsistent results. The most effective attack
was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and
52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best
five-shot result misclassifying only one of 49 samples (AI recall 96%, human
recall 100%).

</details>


### [6] [Are LLM Belief Updates Consistent with Bayes' Theorem?](https://arxiv.org/abs/2507.17951)
*Sohaib Imran,Ihor Kendiukhov,Matthew Broerman,Aditya Thomas,Riccardo Campanella,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: 研究发现，在上下文推理中，更大、能力更强的预训练语言模型在更新其“信念”时，与贝叶斯定理的符合度更高。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）在接收上下文证据时，更新其对命题的“信念”是否更符合贝叶斯定理。

Method: 提出了贝叶斯一致性系数（BCC）度量并生成了相应数据集。测量了五类预训练语言模型的BCC，并将其与模型参数、训练数据量以及通用基准分数进行比较。

Result: 研究结果支持假设，即更大、能力更强的预训练语言模型在分配置信度时与贝叶斯定理更具一致性。

Conclusion: 该研究证实了大型预训练模型在贝叶斯推理方面表现出更强的连贯性，这对理解和管理LLMs具有重要意义。

Abstract: Do larger and more capable language models learn to update their "beliefs"
about propositions more consistently with Bayes' theorem when presented with
evidence in-context? To test this, we formulate a Bayesian Coherence
Coefficient (BCC) metric and generate a dataset with which to measure the BCC.
We measure BCC for multiple pre-trained-only language models across five model
families, comparing against the number of model parameters, the amount of
training data, and model scores on common benchmarks. Our results provide
evidence for our hypothesis that larger and more capable pre-trained language
models assign credences that are more coherent with Bayes' theorem. These
results have important implications for our understanding and governance of
LLMs.

</details>


### [7] [Natural Language Processing for Tigrinya: Current State and Future Directions](https://arxiv.org/abs/2507.17974)
*Fitsum Gaim,Jong C. Park*

Main category: cs.CL

TL;DR: 本文对2011年至2025年间超过40项提格雷尼亚语NLP研究进行了全面综述，分析了计算资源、模型和应用现状，揭示了挑战并提出了未来研究方向，旨在为该领域提供参考和路线图。


<details>
  <summary>Details</summary>
Motivation: 尽管提格雷尼亚语有数百万使用者，但在自然语言处理（NLP）研究中严重不足。需要系统性地梳理该语言的NLP研究现状、面临的挑战以及未来的发展方向。

Method: 本研究采用全面综述方法，分析了2011年至2025年间的40多项研究，系统性地审查了提格雷尼亚语在形态处理、机器翻译、语音识别和问答等十个不同下游任务中的计算资源、模型和应用现状。

Result: 分析显示，提格雷尼亚语NLP研究从基于规则的系统发展到现代神经网络架构，进展常由资源创建里程碑推动。主要挑战在于其形态复杂性和资源稀缺。有前景的研究方向包括形态感知建模、跨语言迁移和以社区为中心的资源开发。

Conclusion: 本工作为提格雷尼亚语NLP研究者提供了全面的参考资料，并为该领域的发展指明了道路。所有被调研研究和资源的元数据已公开可用。

Abstract: Despite being spoken by millions of people, Tigrinya remains severely
underrepresented in Natural Language Processing (NLP) research. This work
presents a comprehensive survey of NLP research for Tigrinya, analyzing over 40
studies spanning more than a decade of work from 2011 to 2025. We
systematically review the current state of computational resources, models, and
applications across ten distinct downstream tasks, including morphological
processing, machine translation, speech recognition, and question-answering.
Our analysis reveals a clear trajectory from foundational, rule-based systems
to modern neural architectures, with progress consistently unlocked by resource
creation milestones. We identify key challenges rooted in Tigrinya's
morphological complexity and resource scarcity, while highlighting promising
research directions, including morphology-aware modeling, cross-lingual
transfer, and community-centered resource development. This work serves as both
a comprehensive reference for researchers and a roadmap for advancing Tigrinya
NLP. A curated metadata of the surveyed studies and resources is made publicly
available.\footnote{Tigrinya NLP Anthology:
https://github.com/fgaim/tigrinya-nlp-anthology.

</details>


### [8] [Technical Report of TeleChat2, TeleChat2.5 and T1](https://arxiv.org/abs/2507.18013)
*Zihan Wang,Xinzhang Liu,Yitong Yao,Chao Wang,Yu Zhao,Zhihao Yang,Wenmin Deng,Kaipeng Jia,Jiaxin Peng,Yuyao Huang,Sishi Xiong,Zhuo Jiang,Kaidong Yu,Xiaohui Hu,Fubei Yao,Ruiyu Fang,Zhuoru Jiang,Ruiting Song,Qiyi Xie,Rui Xue,Xuewei He,Yanlei Xue,Zhu Yuan,Zhaoxi Zhang,Zilu Huang,Shiquan Wang,Xin Wang,Hanming Wu,Mingyuan Wang,Xufeng Zhan,Yuhan Sun,Zhaohu Xing,Yuhao Jiang,Bingkai Yang,Shuangyong Song,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: 本文介绍了TeleChat模型的最新系列（TeleChat2、TeleChat2.5和T1），这些模型通过强化训练策略（包括万亿级预训练、SFT、DPO和持续预训练结合RL）实现了显著的性能提升，尤其在推理、数学和编码方面表现卓越。其中T1-115B甚至超越了某些专有模型，并已公开发布。


<details>
  <summary>Details</summary>
Motivation: 旨在对TeleChat模型进行重大升级，通过增强的训练策略，在模型架构变化不大的前提下，大幅提升模型在预训练和后训练阶段的性能，以满足复杂推理、代码生成和数学推理等多种应用需求。

Method: 该研究采用以下方法：
1.  **TeleChat2：** 在10万亿高质量多样化tokens上进行预训练，随后进行监督微调（SFT）和直接偏好优化（DPO）。
2.  **TeleChat2.5和T1：** 在TeleChat2的基础上，引入了使用领域特定数据集的持续预训练阶段，并结合强化学习（RL）来提升在代码生成和数学推理任务中的表现。
3.  **模型架构：** 主要采用115B参数的密集型Transformer架构。

Result: 研究结果显示：
1.  新系列模型（TeleChat2、TeleChat2.5和T1）通过增强的训练策略，相比前代TeleChat实现了显著的性能提升。
2.  T1变体专为复杂推理设计，支持长链式思考（CoT），并在数学和编码方面展现出显著改进。
3.  TeleChat2.5则优先考虑速度，提供快速推理。
4.  T1和TeleChat2.5（115B参数）在推理和通用任务性能上均有显著提升。
5.  T1-115B在性能上超越了OpenAI的o1-mini和GPT-4o等专有模型。

Conclusion: TeleChat系列新模型（TeleChat2、TeleChat2.5和T1）通过创新的训练策略，在性能上取得了重大突破，特别是在复杂推理、数学和编码能力上表现出色，部分模型甚至超越了行业领先的专有模型。这些先进的语言模型已公开发布，旨在赋能开发者和研究人员进行多样化应用。

Abstract: We introduce the latest series of TeleChat models: \textbf{TeleChat2},
\textbf{TeleChat2.5}, and \textbf{T1}, offering a significant upgrade over
their predecessor, TeleChat. Despite minimal changes to the model architecture,
the new series achieves substantial performance gains through enhanced training
strategies in both pre-training and post-training stages. The series begins
with \textbf{TeleChat2}, which undergoes pretraining on 10 trillion
high-quality and diverse tokens. This is followed by Supervised Fine-Tuning
(SFT) and Direct Preference Optimization (DPO) to further enhance its
capabilities. \textbf{TeleChat2.5} and \textbf{T1} expand the pipeline by
incorporating a continual pretraining phase with domain-specific datasets,
combined with reinforcement learning (RL) to improve performance in code
generation and mathematical reasoning tasks. The \textbf{T1} variant is
designed for complex reasoning, supporting long Chain-of-Thought (CoT)
reasoning and demonstrating substantial improvements in mathematics and coding.
In contrast, \textbf{TeleChat2.5} prioritizes speed, delivering rapid
inference. Both flagship models of \textbf{T1} and \textbf{TeleChat2.5} are
dense Transformer-based architectures with 115B parameters, showcasing
significant advancements in reasoning and general task performance compared to
the original TeleChat. Notably, \textbf{T1-115B} outperform proprietary models
such as OpenAI's o1-mini and GPT-4o. We publicly release \textbf{TeleChat2},
\textbf{TeleChat2.5} and \textbf{T1}, including post-trained versions with 35B
and 115B parameters, to empower developers and researchers with
state-of-the-art language models tailored for diverse applications.

</details>


### [9] [NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database](https://arxiv.org/abs/2507.18028)
*Weizhi Fei,Hao Shi,Jing Xu,Jingchen Peng,Jiazheng Li,Jingzhao Zhang,Bo Bai,Wei Han,Zhenyuan Chen,Xueyan Niu*

Main category: cs.CL

TL;DR: 本论文提出了NeuralDB，一个创新的LLM知识编辑框架。通过将编辑事实表示为神经键值数据库并使用非线性门控检索模块，NeuralDB解决了现有方法在大规模编辑中损害模型通用能力和遗忘编辑事实的问题，并在多达10万事实的编辑中表现出卓越的性能和通用能力保持。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模语言模型（LLM）知识编辑方法（如Locate-and-Edit）在同时修改大量事实时，可能损害LLM的通用能力，并导致已编辑事实的遗忘，尤其是在编辑量达到数千条时。

Method: 论文将现有线性L&E方法建模为键值（KV）数据库查询。在此基础上，提出了NeuralDB框架，该框架将已编辑事实明确表示为配备非线性门控检索模块的神经KV数据库。该门控模块仅在推理涉及已编辑事实时才运行，从而有效保持LLM的通用能力。

Result: 在GPT2-XL、GPT-J (6B) 和Llama-3 (8B) 模型上，对ZsRE和CounterFacts数据集进行10,000事实编辑的综合实验表明，NeuralDB在编辑效率、泛化性、特异性、流畅性和一致性方面表现出色，并能保持LLM在六项文本理解和生成任务上的整体性能。进一步实验表明，NeuralDB在扩展到100,000个事实（比现有工作多50倍）时仍保持有效性。

Conclusion: NeuralDB是一个高度有效且可扩展的LLM知识编辑框架，它通过创新的神经KV数据库设计和选择性门控检索机制，成功解决了大规模知识编辑中能力退化和遗忘的挑战，实现了高效编辑并显著保护了模型的通用能力。

Abstract: Efficiently editing knowledge stored in large language models (LLMs) enables
model updates without large-scale training. One possible solution is
Locate-and-Edit (L\&E), allowing simultaneous modifications of a massive number
of facts. However, such editing may compromise the general abilities of LLMs
and even result in forgetting edited facts when scaling up to thousands of
edits. In this paper, we model existing linear L\&E methods as querying a
Key-Value (KV) database. From this perspective, we then propose NeuralDB, an
editing framework that explicitly represents the edited facts as a neural KV
database equipped with a non-linear gated retrieval module, % In particular,
our gated module only operates when inference involves the edited facts,
effectively preserving the general abilities of LLMs. Comprehensive experiments
involving the editing of 10,000 facts were conducted on the ZsRE and
CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results
demonstrate that NeuralDB not only excels in editing efficacy, generalization,
specificity, fluency, and consistency, but also preserves overall performance
across six representative text understanding and generation tasks. Further
experiments indicate that NeuralDB maintains its effectiveness even when scaled
to 100,000 facts (\textbf{50x} more than in prior work).

</details>


### [10] [GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs](https://arxiv.org/abs/2507.18043)
*Duy Nguyen,Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 本文提出GrAInS，一种新的推理时控制方法，通过基于梯度的归因识别关键词元并构建方向性引导向量，在不重新训练的情况下显著提升大型语言模型和视觉-语言模型的表现（如真实性、减少幻觉、对齐），且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有推理时控制方法依赖固定、全局的干预向量，忽略个体输入词元的因果影响，并且未能有效利用模型logit的梯度信息，尤其在多模态场景中输入贡献不均。

Method: GrAInS利用对比性、基于梯度的归因（通过Integrated Gradients）识别对期望/非期望输出贡献最大的前k个关键词元。这些词元用于构建从不良行为到理想行为的语义转移引导向量。在推理时，GrAInS根据词元级归因信号调整Transformer层的隐藏激活，并对激活进行归一化以保持表示规模，实现细粒度、可解释、模块化的模型行为控制。

Result: GrAInS在TruthfulQA上使Llama-3.1-8B的准确率提升13.22%；将LLaVA-1.6-7B在MMHal-Bench上的幻觉率从0.624降至0.514；在SPA-VL上将对齐胜率提高8.11%，同时保持模型的流畅性和通用能力。其性能持续优于微调和现有引导基线。

Conclusion: GrAInS提供了一种轻量级、有效且可解释的推理时控制方案，能够显著提升LLM和VLM在多个任务上的性能，无需模型重训练或额外监督，并超越了当前的基线方法。

Abstract: Inference-time steering methods offer a lightweight alternative to
fine-tuning large language models (LLMs) and vision-language models (VLMs) by
modifying internal activations at test time without updating model weights.
However, most existing approaches rely on fixed, global intervention vectors,
overlook the causal influence of individual input tokens, and fail to leverage
informative gradients from the model's logits, particularly in multimodal
settings where visual and textual inputs contribute unevenly. To address these
limitations, we introduce GrAInS, an inference-time steering approach that
operates across both language-only and vision-language models and tasks. GrAInS
uses contrastive, gradient-based attribution via Integrated Gradients to
identify the top-k most influential tokens, both positively and negatively
attributed based on their contribution to preferred versus dispreferred
outputs. These tokens are then used to construct directional steering vectors
that capture semantic shifts from undesirable to desirable behavior. During
inference, GrAInS adjusts hidden activations at transformer layers guided by
token-level attribution signals, and normalizes activations to preserve
representational scale. This enables fine-grained, interpretable, and modular
control over model behavior, without retraining or auxiliary supervision.
Empirically, GrAInS consistently outperforms both fine-tuning and existing
steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using
Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514
with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all
while preserving the model's fluency and general capabilities.

</details>


### [11] [Synthetic Data Generation for Phrase Break Prediction with Large Language Model](https://arxiv.org/abs/2507.18044)
*Hoyeon Lee,Sejung Son,Ye-Eun Kang,Jong-Hwan Kim*

Main category: cs.CL

TL;DR: 本研究探索使用大型语言模型（LLM）生成合成短语停顿标注数据，以解决传统方法中人工标注成本高和数据质量不一致的问题，并在多语言环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有短语停顿预测方法严重依赖高成本的人工标注，且语音变异性导致数据获取困难。受LLM在NLP中生成合成数据并减少标注的成功启发，旨在利用LLM解决这些数据挑战。

Method: 利用大型语言模型（LLM）生成合成的短语停顿标注。通过与传统标注进行比较，并评估其在多种语言中的有效性。

Result: 研究结果表明，基于LLM的合成数据生成有效缓解了短语停顿预测中的数据挑战。

Conclusion: 大型语言模型（LLM）在语音领域作为一种可行的数据解决方案具有显著潜力。

Abstract: Current approaches to phrase break prediction address crucial prosodic
aspects of text-to-speech systems but heavily rely on vast human annotations
from audio or text, incurring significant manual effort and cost. Inherent
variability in the speech domain, driven by phonetic factors, further
complicates acquiring consistent, high-quality data. Recently, large language
models (LLMs) have shown success in addressing data challenges in NLP by
generating tailored synthetic data while reducing manual annotation needs.
Motivated by this, we explore leveraging LLM to generate synthetic phrase break
annotations, addressing the challenges of both manual annotation and
speech-related tasks by comparing with traditional annotations and assessing
effectiveness across multiple languages. Our findings suggest that LLM-based
synthetic data generation effectively mitigates data challenges in phrase break
prediction and highlights the potential of LLMs as a viable solution for the
speech domain.

</details>


### [12] [Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs](https://arxiv.org/abs/2507.18055)
*Tevin Atwal,Chan Nam Tieu,Yefeng Yuan,Zhan Shi,Yuhong Liu,Liang Cheng*

Main category: cs.CL

TL;DR: 本文提出了一套指标来量化评估大型语言模型（LLM）生成合成文本数据的多样性和隐私性。实验结果显示LLM在此方面存在局限性，并提出了基于Prompt的改进方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM生成的合成数据在成本和可扩展性方面具有优势，但其多样性和隐私风险尚未得到充分探索，限制了其在数据驱动应用中的潜力。

Method: 1. 提出了一套全面的指标，用于定量评估LLM生成的文本合成数据的多样性（包括语言表达、情感、用户视角）和隐私（包括再识别风险和风格异常值）。2. 将这些指标应用于评估多个SOTA LLM生成的合成数据集。3. 基于评估结果，提出了一种基于Prompt的方法来提升合成评论的多样性并保护用户隐私。

Result: 实验结果揭示了LLM在生成多样化和隐私保护的合成数据方面的显著局限性。

Conclusion: LLM在生成多样化和隐私保护的合成数据方面仍存在挑战。本研究提出的评估指标和基于Prompt的改进方法为提升合成数据质量和安全性提供了可行方向。

Abstract: The increasing use of synthetic data generated by Large Language Models
(LLMs) presents both opportunities and challenges in data-driven applications.
While synthetic data provides a cost-effective, scalable alternative to
real-world data to facilitate model training, its diversity and privacy risks
remain underexplored. Focusing on text-based synthetic data, we propose a
comprehensive set of metrics to quantitatively assess the diversity (i.e.,
linguistic expression, sentiment, and user perspective), and privacy (i.e.,
re-identification risk and stylistic outliers) of synthetic datasets generated
by several state-of-the-art LLMs. Experiment results reveal significant
limitations in LLMs' capabilities in generating diverse and privacy-preserving
synthetic data. Guided by the evaluation results, a prompt-based approach is
proposed to enhance the diversity of synthetic reviews while preserving
reviewer privacy.

</details>


### [13] [TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios](https://arxiv.org/abs/2507.18061)
*Zehan Li,Hongjie Chen,Yuxin Zhang,Jing Zhou,Xuening Wang,Hang Lv,Mengjie Du,Yaodong Song,Jie Lian,Jian Kang,Jie Li,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: 本文提出TELEVAL，一个动态基准，用于在真实的中文对话场景中评估语音语言模型（SLMs）作为对话代理的有效性，强调从用户语音中提取隐含线索的能力，实验表明现有SLMs在此方面仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有SLM评估基准主要关注其处理复杂任务的能力，与大型语言模型（LLMs）对齐，但未能真实反映用户在现实对话场景中的自然交互方式。

Method: 提出TELEVAL动态基准，专为评估SLM在真实中文交互环境中的对话代理能力而设计。它定义了显式语义、副语言和隐式语义、系统能力三个评估维度，采用符合真实使用的对话格式，并分别评估文本和音频输出，特别关注模型在无额外指令下从用户语音中提取隐含线索并恰当回应的能力。

Result: 实验表明，尽管近期SLM发展迅速，但在自然对话任务中仍有相当大的改进空间。

Conclusion: TELEVAL可作为以用户为中心的评估框架，直接反映用户体验，并有助于开发更强大的面向对话的SLM。

Abstract: Spoken language models (SLMs) have seen rapid progress in recent years, along
with the development of numerous benchmarks for evaluating their performance.
However, most existing benchmarks primarily focus on evaluating whether SLMs
can perform complex tasks comparable to those tackled by large language models
(LLMs), often failing to align with how users naturally interact in real-world
conversational scenarios. In this paper, we propose TELEVAL, a dynamic
benchmark specifically designed to evaluate SLMs' effectiveness as
conversational agents in realistic Chinese interactive settings. TELEVAL
defines three evaluation dimensions: Explicit Semantics, Paralinguistic and
Implicit Semantics, and System Abilities. It adopts a dialogue format
consistent with real-world usage and evaluates text and audio outputs
separately. TELEVAL particularly focuses on the model's ability to extract
implicit cues from user speech and respond appropriately without additional
instructions. Our experiments demonstrate that despite recent progress,
existing SLMs still have considerable room for improvement in natural
conversational tasks. We hope that TELEVAL can serve as a user-centered
evaluation framework that directly reflects the user experience and contributes
to the development of more capable dialogue-oriented SLMs.

</details>


### [14] [Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints](https://arxiv.org/abs/2507.18076)
*Haomin Qi,Zihan Dai,Chengbo Huang*

Main category: cs.CL

TL;DR: 该论文提出了一种新颖的混合参数高效微调（PEFT）策略，结合了BOFT和LoRA-GA的优点，显著提升了LLM微调效率，在接近全量微调精度的同时大幅降低了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）的规模和内存需求，其微调仍然是一个计算瓶颈。

Method: 论文全面评估了LoRA、BOFT、LoRA-GA和uRNN等PEFT技术，并引入了一种新的混合策略。该策略通过梯度范数引导的逐层自适应更新，动态整合了BOFT的正交稳定性和LoRA-GA的梯度对齐快速收敛优势。此外，论文首次探索了将酉RNN（uRNN）原理应用于基于Transformer的LLM，以增强梯度稳定性。

Result: 在GLUE、GSM8K、MT-Bench和HumanEval四个基准测试上，使用7B至405B参数的模型进行评估显示，该混合方法持续优于单独的PEFT基线，接近全量微调的准确性，同时将训练时间缩短高达2.1倍，内存使用减少50%。

Conclusion: 研究结果确立了所提出的混合方法是资源受限下LLM实际部署的一种实用且可扩展的微调解决方案。

Abstract: Fine-tuning large language models (LLMs) remains a computational bottleneck
due to their scale and memory demands. This paper presents a comprehensive
evaluation of parameter-efficient fine-tuning (PEFT) techniques, including
LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that
dynamically integrates BOFT's orthogonal stability with LoRA-GA's
gradient-aligned rapid convergence. By computing per-layer adaptive updates
guided by gradient norms, the hybrid method achieves superior convergence
efficiency and generalization across diverse tasks. We also explore, for the
first time, the adaptation of unitary RNN (uRNN) principles to
transformer-based LLMs, enhancing gradient stability through structured unitary
constraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,
and HumanEval -- using models ranging from 7B to 405B parameters demonstrate
that our hybrid method consistently outperforms individual PEFT baselines,
approaching full fine-tuning accuracy while reducing resource consumption by up
to 2.1 times in training time and 50 percent in memory usage. These findings
establish the hybrid approach as a practical and scalable fine-tuning solution
for real-world deployment of LLMs under resource constraints.

</details>


### [15] [A New Pair of GloVes](https://arxiv.org/abs/2507.18103)
*Riley Carlson,John Bauer,Christopher D. Manning*

Main category: cs.CL

TL;DR: 本报告记录、描述并评估了2024年新的英语GloVe模型。


<details>
  <summary>Details</summary>
Motivation: 2014年的GloVe模型已过时，无法反映语言和世界的演变；且旧模型缺乏精确的数据版本和预处理文档。

Method: 使用Wikipedia、Gigaword和Dolma数据集的子集训练了两套新的词嵌入模型。

Result: 2024年模型包含了新的文化和语言相关词汇；在类比和相似性等结构性任务上表现相当；在近期、时间依赖的命名实体识别（NER）数据集（如非西方新闻数据）上表现更佳。

Conclusion: 新的2024年GloVe模型不仅更新了词汇并解决了旧模型的文档问题，还在适应当前语言演变和特定任务上展示了改进的性能。

Abstract: This report documents, describes, and evaluates new 2024 English GloVe
(Global Vectors for Word Representation) models. While the original GloVe
models built in 2014 have been widely used and found useful, languages and the
world continue to evolve and we thought that current usage could benefit from
updated models. Moreover, the 2014 models were not carefully documented as to
the exact data versions and preprocessing that were used, and we rectify this
by documenting these new models. We trained two sets of word embeddings using
Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary
comparison, direct testing, and NER tasks shows that the 2024 vectors
incorporate new culturally and linguistically relevant words, perform
comparably on structural tasks like analogy and similarity, and demonstrate
improved performance on recent, temporally dependent NER datasets such as
non-Western newswire data.

</details>


### [16] [GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness](https://arxiv.org/abs/2507.18119)
*Hongjie Chen,Zehan Li,Yaodong Song,Wenming Deng,Yitong Yao,Yuxin Zhang,Hang Lv,Xuechao Zhu,Jian Kang,Jie Lian,Jie Li,Chao Wang,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: 本研究提出GOAT-SLM，一个具有副语言和说话者特征感知能力的语音语言模型，通过双模态架构和分阶段训练，在处理情感、方言和年龄敏感交互方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有端到端语音语言模型（SLMs）主要关注语言内容，忽略了人类语音中丰富的副语言和说话者特征（如方言、年龄、情感和非言语发声）。

Method: 引入GOAT-SLM，一个新颖的语音语言模型，采用双模态头部架构将语言建模与声学实现解耦；并提出模块化、分阶段的训练策略，逐步对齐语言、副语言和说话者特征信息。

Result: 在TELEVAL多维评估基准上，GOAT-SLM在语义和非语义任务上均实现了良好平衡的性能，并在处理情感、方言变体和年龄敏感交互方面超越了现有开源模型。

Conclusion: 该工作强调了超越语言内容建模的重要性，推动了更自然、自适应和社交感知的语音语言系统的发展。

Abstract: Recent advances in end-to-end spoken language models (SLMs) have
significantly improved the ability of AI systems to engage in natural spoken
interactions. However, most existing models treat speech merely as a vehicle
for linguistic content, often overlooking the rich paralinguistic and speaker
characteristic cues embedded in human speech, such as dialect, age, emotion,
and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel
spoken language model with paralinguistic and speaker characteristic awareness,
designed to extend spoken language modeling beyond text semantics. GOAT-SLM
adopts a dual-modality head architecture that decouples linguistic modeling
from acoustic realization, enabling robust language understanding while
supporting expressive and adaptive speech generation. To enhance model
efficiency and versatility, we propose a modular, staged training strategy that
progressively aligns linguistic, paralinguistic, and speaker characteristic
information using large-scale speech-text corpora. Experimental results on
TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM
achieves well-balanced performance across both semantic and non-semantic tasks,
and outperforms existing open-source models in handling emotion, dialectal
variation, and age-sensitive interactions. This work highlights the importance
of modeling beyond linguistic content and advances the development of more
natural, adaptive, and socially aware spoken language systems.

</details>


### [17] [MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning](https://arxiv.org/abs/2507.18140)
*Xiaoyuan Li,Moxin Li,Wenjie Wang,Rui Men,Yichang Zhang,Fuli Feng,Dayiheng Liu,Junyang Lin*

Main category: cs.CL

TL;DR: 本文首次评估了多模态大语言模型（MLLM）在多模态数学推理中基于代码执行视觉操作的能力，发现现有模型在这方面远不及人类。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLM在多模态数学推理中利用代码进行视觉操作前景广阔，但现有评估主要关注文本输出，MLLM通过代码执行精确视觉操作的能力尚未被充分探索和评估。

Method: 本研究提出了一个评估框架，聚焦于两个关键方面：多模态代码生成（MCG）和多模态代码编辑（MCE，包括删除、修改、标注）。为此，构建了一个包含几何图、函数图和三种统计图共五种数学图表的数据集，并对九个主流MLLM进行了实验评估。

Result: 实验结果表明，现有MLLM在执行细粒度视觉操作方面与人类表现存在显著差距。

Conclusion: 现有MLLM在基于代码的多模态数学推理中，尤其是在细粒度视觉操作方面，仍有很大的提升空间，远未达到人类水平。

Abstract: Recent progress in Multi-modal Large Language Models (MLLMs) has enabled
step-by-step multi-modal mathematical reasoning by performing visual operations
based on the textual instructions. A promising approach uses code as an
intermediate representation to precisely express and manipulate the images in
the reasoning steps. However, existing evaluations focus mainly on text-only
reasoning outputs, leaving the MLLM's ability to perform accurate visual
operations via code largely unexplored. This work takes a first step toward
addressing that gap by evaluating MLLM's code-based capabilities in multi-modal
mathematical reasoning.Specifically, our framework focuses on two key
evaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's
ability to accurately understand and construct visualizations from scratch. (2)
Multi-modal Code Editing (MCE) assesses the model's capacity for fine-grained
operations, which include three types: Deletion, Modification and Annotation.
To evaluate the above tasks, we incorporate a dataset that covers the five most
popular types of mathematical figures, including geometric diagrams, function
plots, and three types of statistical charts, to provide a comprehensive and
effective measurement of existing MLLMs. Our experimental evaluation involves
nine mainstream MLLMs, and the results reveal that existing models still lag
significantly behind human performance in performing fine-grained visual
operations.

</details>


### [18] [HIVMedQA: Benchmarking large language models for HIV medical decision support](https://arxiv.org/abs/2507.18143)
*Gonzalo Cardenal Antolin,Jacques Fellay,Bashkim Jaha,Roger Kouyos,Niko Beerenwinkel,Diane Duroux*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）在HIV管理中的能力，引入了HIVMedQA基准，发现Gemini 2.5 Pro表现最佳，但模型仍存在复杂问题应对不足和认知偏差等局限性，强调了LLM在临床应用中需要针对性开发和严格评估。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）有望成为支持临床医生决策的工具，尤其在复杂的HIV管理中具有应用潜力。然而，将LLMs整合到临床实践中存在准确性、潜在危害和医生接受度等担忧。目前，AI在HIV护理中的应用研究不足，且缺乏LLM的基准评估。

Method: 本研究评估了LLMs在HIV管理中的当前能力，并开发了HIVMedQA基准数据集，用于评估HIV护理中的开放式医学问答。数据集由感染科医生协助策划的临床相关问题组成。研究评估了七个通用型和三个医学专用型LLM，并运用提示工程。评估框架结合了词汇相似度与“LLM作为评判者”的方法，并扩展以更好地反映临床相关性。评估维度包括问题理解、推理、知识召回、偏见、潜在危害和事实准确性。

Result: 结果显示，Gemini 2.5 Pro在大多数维度上始终优于其他模型，排名前三的模型中有两个是专有模型。模型的性能随问题复杂度的增加而下降。医学微调模型并非总是优于通用型模型，且模型大小不是性能的可靠预测指标。推理和理解比事实召回更具挑战性，并观察到近因效应和现状偏见等认知偏差。

Conclusion: 研究结果强调，为确保LLM在临床护理中安全有效地整合，需要进行有针对性的开发和评估。

Abstract: Large language models (LLMs) are emerging as valuable tools to support
clinicians in routine decision-making. HIV management is a compelling use case
due to its complexity, including diverse treatment options, comorbidities, and
adherence challenges. However, integrating LLMs into clinical practice raises
concerns about accuracy, potential harm, and clinician acceptance. Despite
their promise, AI applications in HIV care remain underexplored, and LLM
benchmarking studies are scarce. This study evaluates the current capabilities
of LLMs in HIV management, highlighting their strengths and limitations. We
introduce HIVMedQA, a benchmark designed to assess open-ended medical question
answering in HIV care. The dataset consists of curated, clinically relevant
questions developed with input from an infectious disease physician. We
evaluated seven general-purpose and three medically specialized LLMs, applying
prompt engineering to enhance performance. Our evaluation framework
incorporates both lexical similarity and an LLM-as-a-judge approach, extended
to better reflect clinical relevance. We assessed performance across key
dimensions: question comprehension, reasoning, knowledge recall, bias,
potential harm, and factual accuracy. Results show that Gemini 2.5 Pro
consistently outperformed other models across most dimensions. Notably, two of
the top three models were proprietary. Performance declined as question
complexity increased. Medically fine-tuned models did not always outperform
general-purpose ones, and larger model size was not a reliable predictor of
performance. Reasoning and comprehension were more challenging than factual
recall, and cognitive biases such as recency and status quo were observed.
These findings underscore the need for targeted development and evaluation to
ensure safe, effective LLM integration in clinical care.

</details>


### [19] [Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models](https://arxiv.org/abs/2507.18171)
*Kexin Chen,Dongxia Wang,Yi Liu,Haonan Zhang,Wenhai Wang*

Main category: cs.CL

TL;DR: 发现并分析了Transformer文本嵌入模型中的“粘性词元”问题，这些词元会严重影响嵌入可靠性和下游任务性能，并提出改进分词和模型设计的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型广泛应用于自然语言处理任务，但存在的“粘性词元”问题会破坏文本嵌入的可靠性，表现为扭曲句子相似度并降低下游任务性能。

Method: 1. 系统性地定义了粘性词元。2. 提出了一种高效的粘性词元检测方法（STD），该方法基于句子和词元过滤。3. 运用STD检测了14个模型家族的40个检查点。4. 通过注意力层分析粘性词元对模型内部表示的影响。5. 评估粘性词元对下游任务（如聚类和检索）的性能影响。

Result: 1. 共检测到868个粘性词元。2. 这些词元通常来源于词汇表中特殊或未使用的条目，以及多语言语料库中的分段子词。3. 粘性词元的存在与模型大小或词汇大小无严格关联。4. 导致下游任务（如聚类和检索）性能显著下降，最高达50%。5. 粘性词元在模型内部表示中占据主导地位。

Conclusion: 粘性词元的存在对分词鲁棒性提出了担忧，未来文本嵌入应用需要更好的分词策略和模型设计来减轻其负面影响。

Abstract: Despite the widespread use of Transformer-based text embedding models in NLP
tasks, surprising 'sticky tokens' can undermine the reliability of embeddings.
These tokens, when repeatedly inserted into sentences, pull sentence similarity
toward a certain value, disrupting the normal distribution of embedding
distances and degrading downstream performance. In this paper, we
systematically investigate such anomalous tokens, formally defining them and
introducing an efficient detection method, Sticky Token Detector (STD), based
on sentence and token filtering. Applying STD to 40 checkpoints across 14 model
families, we discover a total of 868 sticky tokens. Our analysis reveals that
these tokens often originate from special or unused entries in the vocabulary,
as well as fragmented subwords from multilingual corpora. Notably, their
presence does not strictly correlate with model size or vocabulary size. We
further evaluate how sticky tokens affect downstream tasks like clustering and
retrieval, observing significant performance drops of up to 50%. Through
attention-layer analysis, we show that sticky tokens disproportionately
dominate the model's internal representations, raising concerns about
tokenization robustness. Our findings show the need for better tokenization
strategies and model design to mitigate the impact of sticky tokens in future
text embedding applications.

</details>


### [20] [SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models](https://arxiv.org/abs/2507.18182)
*Wonjun Jeong,Dongseok Kim,Taegkeun Whangbo*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在多项选择任务中可能因选项偏见获得虚高分数。本研究提出SCOPE评估框架，通过估计模型位置偏见并调整选项分布，以及避免语义相似干扰项临近，有效减轻此类偏见，从而提高LLM评估的公平性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 发现大型语言模型（LLMs）在多项选择任务中可能通过利用选项位置或标签中固有的偏见来获得虚高的分数，而非展示其真正的理解能力。

Method: 引入SCOPE评估框架，以独立于数据集的方式测量并减轻选择偏见。具体方法包括：1. 通过重复调用缺乏语义内容的“空提示”，估计每个模型的独特位置偏见分布；2. 根据逆偏见分布重新分配答案槽位，以均衡随机选择正确答案的几率；3. 阻止语义相似的干扰项被放置在答案附近，从而避免基于表面接近线索的猜测。

Result: 在多项基准实验中，SCOPE在稳定的性能改进方面持续优于现有去偏方法，并显示出对正确选项更清晰的置信度分布。

Conclusion: SCOPE框架为增强LLM评估的公平性和可靠性提供了一个新标准。

Abstract: Large Language Models (LLMs) can achieve inflated scores on multiple-choice
tasks by exploiting inherent biases in option positions or labels, rather than
demonstrating genuine understanding. This study introduces SCOPE, an evaluation
framework designed to measure and mitigate such selection bias in a
dataset-independent manner. By repeatedly invoking a null prompt that lacks
semantic content, SCOPE estimates each model's unique position-bias
distribution. It then redistributes the answer slot according to the
inverse-bias distribution, thereby equalizing the lucky-rate, the probability
of selecting the correct answer by chance. Furthermore, it prevents
semantically similar distractors from being placed adjacent to the answer,
thereby blocking near-miss guesses based on superficial proximity cues. Across
multiple benchmark experiments, SCOPE consistently outperformed existing
debiasing methods in terms of stable performance improvements and showed
clearer confidence distributions over correct options. This framework thus
offers a new standard for enhancing the fairness and reliability of LLM
evaluations.

</details>


### [21] [TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks](https://arxiv.org/abs/2507.18190)
*Keyu Wu,Qianjin Yu,Manlin Mei,Ruiting Liu,Jun Wang,Kailai Zhang,Yelun Bao*

Main category: cs.CL

TL;DR: 电信网络中的根本原因分析（RCA）是关键任务，但对AI而言，因其复杂的图基推理需求及现实基准稀缺而极具挑战。


<details>
  <summary>Details</summary>
Motivation: 电信网络中的根本原因分析（RCA）至关重要，但AI在此领域面临着复杂性高、需图基推理且缺乏现实基准的严峻挑战。

Method: 此摘要未提及具体研究方法。

Result: 此摘要未提及研究结果。

Conclusion: 电信网络中的RCA对AI来说是一项具有高度重要性但也伴随显著挑战的任务，其主要困难在于复杂的图结构推理需求以及缺乏充分的现实世界基准数据支持。

Abstract: Root Cause Analysis (RCA) in telecommunication networks is a critical task,
yet it presents a formidable challenge for Artificial Intelligence (AI) due to
its complex, graph-based reasoning requirements and the scarcity of realistic
benchmarks.

</details>


### [22] [Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization](https://arxiv.org/abs/2507.18197)
*Aline Belloni,Patrick Prieur*

Main category: cs.CL

TL;DR: 本文探讨如何将符合ISO 30401的知识管理系统(KMS)与现有业务流程（特别是符合ISO 9001的流程）整合，并通过SECI模型和PDCA循环提出具体实施方法。


<details>
  <summary>Details</summary>
Motivation: ISO 30401的实施者面临如何将知识开发、转化和传递活动与现有操作流程整合的挑战，尤其是在已广泛使用业务流程建模且符合或接近ISO 9001标准的组织中，需要明确这种整合方式。

Method: 回顾ISO 9001背景下的流程建模原则，并基于实践经验，探索符合ISO 30401的知识管理系统如何与集成管理系统的其他流程交织，特别是提出通过SECI模型机制在PDCA循环步骤中进行实施。

Result: 文章阐述了符合ISO 30401的知识管理系统如何与集成管理系统的其他流程（特别是现有操作流程）相整合，并提出了通过在PDCA循环中部署SECI模型机制来实现其具体实施的框架。

Conclusion: 该研究为组织提供了一个将ISO 30401知识管理系统有效整合到现有业务流程中的实用方法，强调了SECI模型和PDCA循环在实施过程中的关键作用，以确保知识管理与战略目标和操作效率的对齐。

Abstract: Business process modeling is used by most organizations as an essential
framework for ensuring efficiency and effectiveness of the work and workflow
performed by its employees and for ensuring the alignment of such work with its
strategic goals. For organizations that are compliant or near-compliant with
ISO 9001, this approach involves the detailed mapping of processes,
sub-processes, activities, and tasks. ISO30401 is a Management System Standard,
introduced in 2018, establishing universal requirements for the set up of a
Knowledge Management System in an organization. As ``ISO30401 implementers'' we
regularly face the challenge of explaining our clients how the knowledge
development, transformation and conveyances activities depicted in ISO30401 do
integrate with existing operational processes. This article recaps process
modelling principles in the context of ISO9001 and explores, based on our
experience, how an ISO30401-compliant Knowledge Management System (KMS)
entwines with all other processes of an Integrated Management System and in
particular how it can be implemented by deploying the mechanisms of the SECI
model through the steps of PDCA cycles.

</details>


### [23] [Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection](https://arxiv.org/abs/2507.18202)
*San Kim,Jonghwi Kim,Yejin Jeon,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 检索增强生成（RAG）面临知识库投毒风险，本文提出GMTP防御方法，通过梯度和MLM检测并过滤恶意文档，实验证明其能高效清除90%以上投毒内容，保持系统性能。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）虽能提升大语言模型（LLM）响应的准确性和时效性，但其对外部知识的依赖引入安全隐患，攻击者可能注入投毒文档以操纵生成有害或误导性输出。

Method: 本文提出一种名为“基于梯度的掩码令牌概率”（GMTP）的新型防御方法。GMTP通过分析检索器相似性函数的梯度来识别高影响力令牌，随后对这些关键令牌进行掩码处理，并利用掩码语言模型（MLM）检查它们的概率。投毒注入的令牌通常具有极低的掩码令牌概率，GMTP能据此高效检测并过滤恶意文档。

Result: 实验结果表明，GMTP能够清除超过90%的投毒内容，同时有效保留相关文档，从而在多样化的数据集和对抗环境下，维持稳健的检索和生成性能。

Conclusion: GMTP是一种有效且高效的防御策略，能显著增强RAG系统抵御知识库投毒攻击的能力，同时确保系统性能不受影响。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
providing external knowledge for accurate and up-to-date responses. However,
this reliance on external sources exposes a security risk, attackers can inject
poisoned documents into the knowledge base to steer the generation process
toward harmful or misleading outputs. In this paper, we propose Gradient-based
Masked Token Probability (GMTP), a novel defense method to detect and filter
out adversarially crafted documents. Specifically, GMTP identifies high-impact
tokens by examining gradients of the retriever's similarity function. These key
tokens are then masked, and their probabilities are checked via a Masked
Language Model (MLM). Since injected tokens typically exhibit markedly low
masked-token probabilities, this enables GMTP to easily detect malicious
documents and achieve high-precision filtering. Experiments demonstrate that
GMTP is able to eliminate over 90% of poisoned content while retaining relevant
documents, thus maintaining robust retrieval and generation performance across
diverse datasets and adversarial settings.

</details>


### [24] [Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation](https://arxiv.org/abs/2507.18203)
*Kyubeen Han,Junseo Jang,Hongjin Kim,Geunyeong Jeong,Harksoo Kim*

Main category: cs.CL

TL;DR: 指令微调提高了大型语言模型（LLMs）接受用户提供错误信息的可能性，并将其对错误信息的敏感性从助手角色转向了用户角色。


<details>
  <summary>Details</summary>
Motivation: 指令微调虽然提高了LLMs的可用性并减少了有害输出，但可能导致模型过度依赖用户输入，从而更容易接受虚假信息。现有研究虽关注LLMs对外部矛盾信息的接收，但很少直接探讨指令微调对此现象的影响。

Method: 本研究调查了指令微调对LLM接受错误信息易感性的影响。通过将指令微调模型与基础模型进行比较，分析了用户提供错误信息时模型的表现。此外，还探讨了提示结构中用户角色、错误信息长度和系统提示中警告的存在等因素的影响。

Result: 研究发现，指令微调后的LLMs在用户提供错误信息时，更容易接受这些信息。与基础模型相比，指令微调增加了模型对用户提供信息的依赖，使得易感性从助手角色转移到用户角色。同时，用户在提示结构中的角色、错误信息长度和系统提示中的警告等额外因素也会影响模型对错误信息的敏感性。

Conclusion: 研究结果强调了需要采取系统性方法来减轻指令微调带来的意外后果，并提升LLMs在实际应用中的可靠性。

Abstract: Instruction-tuning enhances the ability of large language models (LLMs) to
follow user instructions more accurately, improving usability while reducing
harmful outputs. However, this process may increase the model's dependence on
user input, potentially leading to the unfiltered acceptance of misinformation
and the generation of hallucinations. Existing studies primarily highlight that
LLMs are receptive to external information that contradict their parametric
knowledge, but little research has been conducted on the direct impact of
instruction-tuning on this phenomenon. In our study, we investigate the impact
of instruction-tuning on LLM's susceptibility to misinformation. Our analysis
reveals that instruction-tuned LLMs are significantly more likely to accept
misinformation when it is presented by the user. A comparison with base models
shows that instruction-tuning increases reliance on user-provided information,
shifting susceptibility from the assistant role to the user role. Furthermore,
we explore additional factors influencing misinformation susceptibility, such
as the role of the user in prompt structure, misinformation length, and the
presence of warnings in the system prompt. Our findings underscore the need for
systematic approaches to mitigate unintended consequences of instruction-tuning
and enhance the reliability of LLMs in real-world applications.

</details>


### [25] [Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation](https://arxiv.org/abs/2507.18212)
*Xinrui Chen,Hongxing Zhang,Fanyi Zeng,Yongxian Wei,Yizhi Wang,Xitong Ling,Guanghao Li,Chun Yuan*

Main category: cs.CL

TL;DR: 本文提出Prune&Comp方法，通过幅值补偿解决LLM层剪枝导致的隐藏态幅值差异问题，从而在无训练、零运行时开销的情况下显著提升剪枝性能，并在LLaMA-3-8B等模型上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 层剪枝是压缩和加速大型语言模型（LLMs）的有前景技术，但研究发现移除任何层都会在隐藏态中引入显著的幅值差异，导致性能大幅下降。

Method: 提出Prune&Comp，一个新颖的即插即用层剪枝方案。该方法利用幅值补偿以无训练方式缓解隐藏态幅值差异。具体地，首先估算层移除造成的幅值差异，然后通过离线重缩放剩余权重来消除该差异，不产生运行时开销。该方法还可与迭代剪枝策略结合。

Result: Prune&Comp能够持续提升现有层剪枝指标。例如，当使用普遍的块影响力指标对LLaMA-3-8B的5层进行剪枝时，Prune&Comp几乎将困惑度减半，并保留了原始模型93.19%的问答性能，比基线高出4.01%。

Conclusion: Prune&Comp成功解决了LLM层剪枝中的隐藏态幅值差异问题，显著提高了剪枝模型的性能保留，且具有无训练和零运行时开销的优势。

Abstract: Layer pruning has emerged as a promising technique for compressing large
language models (LLMs) while achieving acceleration proportional to the pruning
ratio. In this work, we identify that removing any layer induces a significant
magnitude gap in hidden states, resulting in substantial performance
degradation. To address this issue, we propose Prune&Comp, a novel
plug-and-play layer pruning scheme that leverages magnitude compensation to
mitigate such gaps in a training-free manner. Specifically, we first estimate
the magnitude gap caused by layer removal and then eliminate this gap by
rescaling the remaining weights offline, with zero runtime overhead incurred.
We further demonstrate the advantages of Prune&Comp through an iterative
pruning strategy. When integrated with an iterative prune-and-compensate loop,
Prune&Comp consistently enhances existing layer pruning metrics. For instance,
when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence
metric, Prune&Comp nearly halves the perplexity and retains 93.19\% of the
original model's question-answering performance, outperforming the baseline by
4.01%.

</details>


### [26] [Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models](https://arxiv.org/abs/2507.18263)
*Suhang Wu,Jialong Tang,Chengyi Yang,Pei Zhang,Baosong Yang,Junhui Li,Junfeng Yao,Min Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 提出一种“定位-聚焦”方法，通过精确识别和利用相关术语翻译知识，有效提升了语音翻译中术语的翻译准确性并减少了噪音干扰。


<details>
  <summary>Details</summary>
Motivation: 直接语音翻译（ST）中术语的准确翻译仍是巨大挑战。现有方法在利用翻译知识时常受无关噪音干扰，且无法充分利用这些知识。

Method: 本文提出一种“定位-聚焦”方法。该方法首先定位语音中包含术语的片段以构建翻译知识，从而最小化无关信息；随后，它将翻译知识与语音和假设从音频和文本两种模态关联起来，使ST模型在翻译时能更好地聚焦于翻译知识。

Result: 实验结果表明，该方法能有效定位语音中的术语，并显著提高术语翻译的成功率，同时保持了稳健的通用翻译性能。

Conclusion: 所提出的“定位-聚焦”方法有效解决了语音翻译中术语翻译的挑战，通过精确的知识利用显著提升了术语翻译质量，同时保持了整体翻译性能。

Abstract: Direct speech translation (ST) has garnered increasing attention nowadays,
yet the accurate translation of terminology within utterances remains a great
challenge. In this regard, current studies mainly concentrate on leveraging
various translation knowledge into ST models. However, these methods often
struggle with interference from irrelevant noise and can not fully utilize the
translation knowledge. To address these issues, in this paper, we propose a
novel Locate-and-Focus method for terminology translation. It first effectively
locates the speech clips containing terminologies within the utterance to
construct translation knowledge, minimizing irrelevant information for the ST
model. Subsequently, it associates the translation knowledge with the utterance
and hypothesis from both audio and textual modalities, allowing the ST model to
better focus on translation knowledge during translation. Experimental results
across various datasets demonstrate that our method effectively locates
terminologies within utterances and enhances the success rate of terminology
translation, while maintaining robust general translation performance.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [27] [Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling](https://arxiv.org/abs/2507.17801)
*Yi Xin,Juncheng Yan,Qi Qin,Zhen Li,Dongyang Liu,Shicheng Li,Victor Shea-Jay Huang,Yupeng Zhou,Renrui Zhang,Le Zhuo,Tiancheng Han,Xiaoqing Sun,Siqi Luo,Mengmeng Wang,Bin Fu,Yuewen Cao,Hongsheng Li,Guangtao Zhai,Xiaohong Liu,Yu Qiao,Peng Gao*

Main category: cs.CV

TL;DR: Lumina-mGPT 2.0是一个从零开始训练的自回归模型，其图像生成质量媲美甚至超越了SOTA扩散模型，并能通过统一框架处理多种多模态生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有高性能图像生成方法多依赖预训练组件或混合架构，限制了设计自由度。本研究旨在重新探索和激活自回归模型在高质量图像生成领域的潜力，提供一个完全从零训练且更具灵活性的统一生成框架。

Method: 该研究提出了一个独立的、仅解码器的自回归模型Lumina-mGPT 2.0，并完全从零开始训练。它采用统一的标记化方案以处理广泛的任务，并整合了推理时缩放和推测雅可比采样等高效解码策略来提升质量和速度。

Result: Lumina-mGPT 2.0的生成质量与DALL-E 3、SANA等领先的扩散模型不相上下，在标准文本到图像基准测试（如GenEval、DPG）上甚至超越了部分基于扩散的模型。该模型通过统一框架无缝支持了主体驱动生成、图像编辑、可控合成和密集预测等多种任务，并在Graph200K等多任务基准测试中表现出色。

Conclusion: Lumina-mGPT 2.0被证明是一个强大、灵活的统一多模态生成基础模型，它重新确立了自回归范式在高质量图像生成及其它任务中的竞争力。

Abstract: We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model
that revisits and revitalizes the autoregressive paradigm for high-quality
image generation and beyond. Unlike existing approaches that rely on pretrained
components or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from
scratch, enabling unrestricted architectural design and licensing freedom. It
achieves generation quality on par with state-of-the-art diffusion models such
as DALL-E 3 and SANA, while preserving the inherent flexibility and
compositionality of autoregressive modeling. Our unified tokenization scheme
allows the model to seamlessly handle a wide spectrum of tasks-including
subject-driven generation, image editing, controllable synthesis, and dense
prediction-within a single generative framework. To further boost usability, we
incorporate efficient decoding strategies like inference-time scaling and
speculative Jacobi sampling to improve quality and speed, respectively.
Extensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG)
demonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses
diffusion-based models. Moreover, we confirm its multi-task capabilities on the
Graph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally
well. These results position Lumina-mGPT 2.0 as a strong, flexible foundation
model for unified multimodal generation. We have released our training details,
code, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.

</details>


### [28] [SV3.3B: A Sports Video Understanding Model for Action Recognition](https://arxiv.org/abs/2507.17844)
*Sai Varun Kodathala,Yashwanth Reddy Vutukoori,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 本文提出SV3.3B，一个轻量级视频理解模型，用于高效的设备端体育视频分析，通过创新的时间运动差异采样和自监督学习，在生成详细且分析丰富的体育描述方面超越了大型模型，包括GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 传统的体育视频分析模型计算量大，依赖服务器端处理，且缺乏对细微运动的深入理解，难以捕捉运动员动作中关键的生物力学转换阶段（如准备、执行、收尾）。

Method: 引入SV3.3B模型，一个3.3B参数的轻量级视频理解模型。该模型结合了新颖的时间运动差异采样和自监督学习，旨在高效的设备端部署。具体方法包括：使用基于DWT-VGG16-LDA的关键帧提取机制（识别16个代表性帧），然后通过V-DWT-JEPA2编码器（通过掩码去噪目标预训练），以及一个为体育动作描述生成而微调的LLM解码器。

Result: 在NSVA篮球数据集子集上进行评估，SV3.3B在传统文本生成指标和体育专用评估标准上均表现出色。它在计算要求显著降低的同时，优于包括GPT-4o变体在内的更大闭源模型。在真值验证指标上，SV3.3B比GPT-4o提升29.2%，并在信息密度、动作复杂度和测量精度等对全面运动分析至关重要的指标上也有显著提升。

Conclusion: SV3.3B模型展示了在生成技术详细和分析丰富的体育描述方面的卓越能力，为设备端体育视频分析提供了一个高效且高性能的解决方案，解决了现有模型在计算效率和细粒度理解方面的局限性。

Abstract: This paper addresses the challenge of automated sports video analysis, which
has traditionally been limited by computationally intensive models requiring
server-side processing and lacking fine-grained understanding of athletic
movements. Current approaches struggle to capture the nuanced biomechanical
transitions essential for meaningful sports analysis, often missing critical
phases like preparation, execution, and follow-through that occur within
seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B
parameter video understanding model that combines novel temporal motion
difference sampling with self-supervised learning for efficient on-device
deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction
mechanism that intelligently identifies the 16 most representative frames from
sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through
mask-denoising objectives and an LLM decoder fine-tuned for sports action
description generation. Evaluated on a subset of the NSVA basketball dataset,
SV3.3B achieves superior performance across both traditional text generation
metrics and sports-specific evaluation criteria, outperforming larger
closed-source models including GPT-4o variants while maintaining significantly
lower computational requirements. Our model demonstrates exceptional capability
in generating technically detailed and analytically rich sports descriptions,
achieving 29.2% improvement over GPT-4o in ground truth validation metrics,
with substantial improvements in information density, action complexity, and
measurement precision metrics essential for comprehensive athletic analysis.
Model Available at https://huggingface.co/sportsvision/SV3.3B.

</details>


### [29] [Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models](https://arxiv.org/abs/2507.17853)
*Lifeng Chen,Jiner Wang,Zihao Pan,Beier Zhu,Xiaofeng Yang,Chi Zhang*

Main category: cs.CV

TL;DR: Detail++是一个无需训练的文本到图像生成框架，借鉴人类绘画过程，通过渐进式细节注入（PDI）策略，将复杂提示分解为子提示分阶段生成。它利用自注意力和交叉注意力机制，并引入质心对齐损失，有效解决了多主体复杂提示中属性绑定和一致性问题，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到图像（T2I）生成模型在处理复杂提示，特别是涉及多个具有不同属性的主体时，面临显著挑战，难以准确生成并绑定细节。

Method: 该研究提出了一个名为Detail++的无需训练框架，核心是渐进式细节注入（PDI）策略。具体方法包括：1) 将复杂提示分解为一系列简化的子提示，分阶段引导生成过程，利用自注意力机制确保全局构图和精确细化；2) 利用交叉注意力机制实现属性与对应主体的准确绑定；3) 在测试时引入一种新颖的质心对齐损失（Centroid Alignment Loss）来减少绑定噪声并增强属性一致性。

Result: 在T2I-CompBench数据集和新构建的风格合成基准测试上进行的广泛实验表明，Detail++显著优于现有方法，尤其在涉及多个对象和复杂风格条件的场景中表现突出。

Conclusion: Detail++通过其独特的渐进式细节注入策略和创新的质心对齐损失，有效克服了当前T2I模型在处理复杂多主体提示时遇到的属性绑定和一致性挑战，取得了卓越的生成效果。

Abstract: Recent advances in text-to-image (T2I) generation have led to impressive
visual results. However, these models still face significant challenges when
handling complex prompt, particularly those involving multiple subjects with
distinct attributes. Inspired by the human drawing process, which first
outlines the composition and then incrementally adds details, we propose
Detail++, a training-free framework that introduces a novel Progressive Detail
Injection (PDI) strategy to address this limitation. Specifically, we decompose
a complex prompt into a sequence of simplified sub-prompts, guiding the
generation process in stages. This staged generation leverages the inherent
layout-controlling capacity of self-attention to first ensure global
composition, followed by precise refinement. To achieve accurate binding
between attributes and corresponding subjects, we exploit cross-attention
mechanisms and further introduce a Centroid Alignment Loss at test time to
reduce binding noise and enhance attribute consistency. Extensive experiments
on T2I-CompBench and a newly constructed style composition benchmark
demonstrate that Detail++ significantly outperforms existing methods,
particularly in scenarios involving multiple objects and complex stylistic
conditions.

</details>


### [30] [FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains](https://arxiv.org/abs/2507.17859)
*Muayad Abujabal,Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: 本文提出了FishDet-M，一个最大的统一水下鱼类检测基准，整合了13个数据集并采用COCO标准标注。系统评估了28种主流检测模型，并引入了基于CLIP的零样本模型选择框架，以支持水下环境中的高效、自适应鱼类检测。


<details>
  <summary>Details</summary>
Motivation: 水下图像中的准确鱼类检测对生态监测、水产养殖自动化和机器人感知至关重要。然而，现有数据集零碎、成像条件异构且评估协议不一致，限制了实际部署。

Method: 构建了FishDet-M，这是最大的统一鱼类检测基准，包含13个公共数据集，采用COCO风格的边界框和分割掩码标注。系统评估了28种主流目标检测模型（包括YOLOv8-YOLOv12系列、R-CNN和DETR模型），采用mAP、比例AP、延迟和参数量等指标进行评估。引入了一个基于CLIP的模型选择框架，利用视觉-语言对齐动态选择最合适的检测器，实现零样本自适应部署。

Result: FishDet-M上的模型基准测试结果揭示了不同模型架构之间检测性能的差异以及准确性和效率的权衡。所提出的基于CLIP的零样本模型选择策略在不依赖集成计算的情况下实现了高性能。

Conclusion: FishDet-M为复杂水下场景中的目标检测提供了一个标准化、可复现的平台。所有数据集、预训练模型和评估工具均已公开，以促进水下计算机视觉和智能海洋系统的未来研究。

Abstract: Accurate fish detection in underwater imagery is essential for ecological
monitoring, aquaculture automation, and robotic perception. However, practical
deployment remains limited by fragmented datasets, heterogeneous imaging
conditions, and inconsistent evaluation protocols. To address these gaps, we
present \textit{FishDet-M}, the largest unified benchmark for fish detection,
comprising 13 publicly available datasets spanning diverse aquatic environments
including marine, brackish, occluded, and aquarium scenes. All data are
harmonized using COCO-style annotations with both bounding boxes and
segmentation masks, enabling consistent and scalable cross-domain evaluation.
We systematically benchmark 28 contemporary object detection models, covering
the YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.
Evaluations are conducted using standard metrics including mAP, mAP@50, and
mAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and
inference profiling in terms of latency and parameter count. The results
highlight the varying detection performance across models trained on FishDet-M,
as well as the trade-off between accuracy and efficiency across models of
different architectures. To support adaptive deployment, we introduce a
CLIP-based model selection framework that leverages vision-language alignment
to dynamically identify the most semantically appropriate detector for each
input image. This zero-shot selection strategy achieves high performance
without requiring ensemble computation, offering a scalable solution for
real-time applications. FishDet-M establishes a standardized and reproducible
platform for evaluating object detection in complex aquatic scenes. All
datasets, pretrained models, and evaluation tools are publicly available to
facilitate future research in underwater computer vision and intelligent marine
systems.

</details>


### [31] [Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis](https://arxiv.org/abs/2507.17860)
*Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel*

Main category: cs.CV

TL;DR: 本研究利用生成式AI（GenAI）合成数据评估黑色素瘤分类器的公平性，发现合成数据评估有前景，但模型训练数据与合成数据来源不一致时公平性验证困难。


<details>
  <summary>Details</summary>
Motivation: 深度学习在皮肤癌筛查中应用潜力巨大，但存在固有的偏见风险。评估和提升这类系统的公平性至关重要，尤其是在评估数据集难以充分代表不同个体身份信息（PII）和少数群体的情况下。

Method: 本研究利用最先进的生成式AI模型LightningDiT，生成高度逼真的合成数据，以评估公开可用的黑色素瘤分类器的公平性。

Result: 研究结果表明，使用高度逼真的合成数据进行公平性评估是一个有前景的方向。然而，当用于评估的黑色素瘤检测模型所训练的数据与合成图像所依据的数据集不同时，验证公平性变得困难。

Conclusion: 本研究提出的方法为利用合成数据来衡量和增强医学影像生成式AI系统中的公平性提供了一个有价值的新途径。

Abstract: Recent advancements in Deep Learning and its application on the edge hold
great potential for the revolution of routine screenings for skin cancers like
Melanoma. Along with the anticipated benefits of this technology, potential
dangers arise from unforseen and inherent biases. Thus, assessing and improving
the fairness of such systems is of utmost importance. A key challenge in
fairness assessment is to ensure that the evaluation dataset is sufficiently
representative of different Personal Identifiable Information (PII) (sex, age,
and race) and other minority groups. Against the backdrop of this challenge,
this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT
model to assess the fairness of publicly available melanoma classifiers. The
results suggest that fairness assessment using highly realistic synthetic data
is a promising direction. Yet, our findings indicate that verifying fairness
becomes difficult when the melanoma-detection model used for evaluation is
trained on data that differ from the dataset underpinning the synthetic images.
Nonetheless, we propose that our approach offers a valuable new avenue for
employing synthetic data to gauge and enhance fairness in medical-imaging GenAI
systems.

</details>


### [32] [DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration](https://arxiv.org/abs/2507.17892)
*Hanzhou Liu,Binghan Li,Chengkai Liu,Mi Lu*

Main category: cs.CV

TL;DR: 本文提出DiNAT-IR，一种基于Transformer的图像恢复架构。它结合了膨胀邻域注意力（DiNA）和一种通道感知模块，旨在平衡全局上下文与局部精度，有效解决了传统Transformer计算成本高及局部伪影问题，并在多个基准测试中取得了有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: Transformer在图像恢复任务中表现出色，但自注意力机制的高计算成本限制了其在高分辨率图像上的应用。尽管Restormer通过通道自注意力提升了效率，但可能忽略对图像恢复至关重要的局部伪影。因此，需要一种方法来弥补这一差距，在保持效率的同时兼顾全局上下文和局部精度。

Method: 引入膨胀邻域注意力（DiNA），通过滑动窗口注意力和混合膨胀因子平衡全局上下文与局部精度。为解决DiNA直接应用于去模糊任务时全局上下文理解受限的问题，本文提出一个通道感知模块来补充局部注意力，以有效整合全局上下文，同时保持像素级精度。最终构建了DiNAT-IR架构。

Result: 所提出的DiNAT-IR架构在多个图像恢复基准测试中取得了有竞争力的结果，为各种低级计算机视觉问题提供了高质量的解决方案。

Conclusion: DiNAT-IR通过有效整合膨胀邻域注意力与通道感知模块，成功解决了图像恢复中计算效率和局部精度之间的权衡问题，为高质量图像恢复提供了可行且竞争力强的解决方案。

Abstract: Transformers, with their self-attention mechanisms for modeling long-range
dependencies, have become a dominant paradigm in image restoration tasks.
However, the high computational cost of self-attention limits scalability to
high-resolution images, making efficiency-quality trade-offs a key research
focus. To address this, Restormer employs channel-wise self-attention, which
computes attention across channels instead of spatial dimensions. While
effective, this approach may overlook localized artifacts that are crucial for
high-quality image restoration. To bridge this gap, we explore Dilated
Neighborhood Attention (DiNA) as a promising alternative, inspired by its
success in high-level vision tasks. DiNA balances global context and local
precision by integrating sliding-window attention with mixed dilation factors,
effectively expanding the receptive field without excessive overhead. However,
our preliminary experiments indicate that directly applying this global-local
design to the classic deblurring task hinders accurate visual restoration,
primarily due to the constrained global context understanding within local
attention. To address this, we introduce a channel-aware module that
complements local attention, effectively integrating global context without
sacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based
architecture specifically designed for image restoration, achieves competitive
results across multiple benchmarks, offering a high-quality solution for
diverse low-level computer vision problems.

</details>


### [33] [AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2507.17957)
*Md. Al-Masrur Khan,Durgakant Pushp,Lantao Liu*

Main category: cs.CV

TL;DR: 在无监督域适应语义分割（UDA-SS）中，本研究提出了自适应特征细化（AFR）模块，通过融合高低分辨率信息、整合高频成分和不确定性驱动注意力，有效平衡局部与全局信息，显著提升分割精度并达到最新水平。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域适应语义分割（UDA-SS）方法难以平衡细粒度局部细节与全局上下文信息，导致在复杂区域的分割错误。

Method: 引入自适应特征细化（AFR）模块。该模块通过使用低分辨率logits的语义先验来细化高分辨率特征；整合捕获细粒度结构和边界信息的高频成分；并通过不确定性驱动的注意力机制自适应地平衡局部和全局信息。AFR设计轻量，可无缝集成到基于HRDA的UDA方法中。

Result: 本方法将现有UDA-SS方法在GTA V --> Cityscapes数据集上的mIoU提高了1.05%，在Synthia --> Cityscapes数据集上提高了1.04%，实现了最先进的分割性能。

Conclusion: 通过引入AFR模块，该研究成功解决了UDA-SS中局部与全局信息平衡的挑战，显著提升了语义分割精度，并达到了最先进的性能水平。

Abstract: In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is
trained on labeled source domain data (e.g., synthetic images) and adapted to
an unlabeled target domain (e.g., real-world images) without access to target
annotations. Existing UDA-SS methods often struggle to balance fine-grained
local details with global contextual information, leading to segmentation
errors in complex regions. To address this, we introduce the Adaptive Feature
Refinement (AFR) module, which enhances segmentation accuracy by refining
highresolution features using semantic priors from low-resolution logits. AFR
also integrates high-frequency components, which capture fine-grained
structures and provide crucial boundary information, improving object
delineation. Additionally, AFR adaptively balances local and global information
through uncertaintydriven attention, reducing misclassifications. Its
lightweight design allows seamless integration into HRDA-based UDA methods,
leading to state-of-the-art segmentation performance. Our approach improves
existing UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on
Synthia-->Cityscapes. The implementation of our framework is available at:
https://github.com/Masrur02/AFRDA

</details>


### [34] [OPEN: A Benchmark Dataset and Baseline for Older Adult Patient Engagement Recognition in Virtual Rehabilitation Learning Environments](https://arxiv.org/abs/2507.17959)
*Ali Abedi,Sadaf Safa,Tracey J. F. Colella,Shehroz S. Khan*

Main category: cs.CV

TL;DR: 本文提出了OPEN数据集，这是一个为老年人在虚拟学习环境中进行AI驱动的参与度识别而设计的新型数据集，并通过机器学习模型实现了高达81%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 在虚拟学习（尤其是线上教育和虚拟康复）中，参与度对用户满意度、表现和依从性至关重要，但准确测量虚拟群体环境中的参与度仍具挑战。尽管人工智能在自动化参与度识别方面备受关注，且针对年轻人群体的研究广泛，但针对老年人在虚拟和远程医疗学习环境中的研究和数据集却非常有限，且现有方法常忽略情境关联性和参与度的纵向特性。

Method: 研究引入了OPEN（老年患者参与度）数据集。数据通过对11名老年人进行为期六周的每周虚拟小组学习（心脏康复课程）收集，总计超过35小时。为保护隐私，原始视频未公开，而是发布了从视频中提取的面部、手部、身体关节关键点，以及情感和行为特征。数据集包含二元参与状态、情感与行为标签，以及情境类型（如教师是面向群体还是个体）的标注，并提供5秒、10秒、30秒及可变长度的样本版本。为验证数据集的实用性，研究训练了多种机器学习和深度学习模型。

Result: OPEN数据集是同类中规模最大的老年人参与度数据集，包含超过35小时的数据。通过使用该数据集训练机器学习和深度学习模型，参与度识别准确率达到了81%。

Conclusion: OPEN数据集为老龄人口的个性化参与度建模提供了可扩展的基础，并对更广泛的参与度识别研究做出了贡献，证明了其在AI驱动的参与度识别领域的实用性。

Abstract: Engagement in virtual learning is essential for participant satisfaction,
performance, and adherence, particularly in online education and virtual
rehabilitation, where interactive communication plays a key role. Yet,
accurately measuring engagement in virtual group settings remains a challenge.
There is increasing interest in using artificial intelligence (AI) for
large-scale, real-world, automated engagement recognition. While engagement has
been widely studied in younger academic populations, research and datasets
focused on older adults in virtual and telehealth learning settings remain
limited. Existing methods often neglect contextual relevance and the
longitudinal nature of engagement across sessions. This paper introduces OPEN
(Older adult Patient ENgagement), a novel dataset supporting AI-driven
engagement recognition. It was collected from eleven older adults participating
in weekly virtual group learning sessions over six weeks as part of cardiac
rehabilitation, producing over 35 hours of data, making it the largest dataset
of its kind. To protect privacy, raw video is withheld; instead, the released
data include facial, hand, and body joint landmarks, along with affective and
behavioral features extracted from video. Annotations include binary engagement
states, affective and behavioral labels, and context-type indicators, such as
whether the instructor addressed the group or an individual. The dataset offers
versions with 5-, 10-, 30-second, and variable-length samples. To demonstrate
utility, multiple machine learning and deep learning models were trained,
achieving engagement recognition accuracy of up to 81 percent. OPEN provides a
scalable foundation for personalized engagement modeling in aging populations
and contributes to broader engagement recognition research.

</details>


### [35] [Bearded Dragon Activity Recognition Pipeline: An AI-Based Approach to Behavioural Monitoring](https://arxiv.org/abs/2507.17987)
*Arsen Yermukan,Pedro Machado,Feliciano Domingos,Isibor Kennedy Ihianle,Jordan J. Bird,Stefano S. K. Kaburu,Samantha J. Ward*

Main category: cs.CV

TL;DR: 该项目开发了一个基于YOLO的自动化系统，用于实时监测胡须龙的晒太阳和狩猎行为，解决了传统方法的耗时和易错问题。系统选择了YOLOv8s模型，并成功实现了可靠的晒太阳行为检测，但狩猎行为检测受限于蟋蟀识别准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的胡须龙行为监测方法耗时且容易出错，亟需一种高效、准确的自动化解决方案。

Method: 该研究开发了一个自动化实时视频分析系统，利用YOLO目标检测模型（包括v5、v7、v8、v11、v12变体）来识别胡须龙的晒太阳和狩猎行为。模型在一个包含胡须龙、加热灯和蟋蟀的定制数据集上进行训练。系统最终选择了YOLOv8s作为最优模型，并通过提取每帧物体坐标、应用时间插值和基于规则的逻辑来分类特定行为。

Result: YOLOv8s被选为最佳模型，因为它在准确性（mAP@0.5:0.95 = 0.855）和速度之间取得了优异平衡。晒太阳行为检测被证明是可靠的。然而，狩猎行为检测的准确性较低，这主要是由于蟋蟀检测能力较弱（mAP@0.5 = 0.392）。

Conclusion: 该自动化系统为受控环境下爬行动物行为监测提供了一个可扩展的解决方案，显著提高了研究效率和数据质量。未来的改进将集中于通过扩充数据集或使用专门的小物体检测器来增强蟋蟀检测能力。

Abstract: Traditional monitoring of bearded dragon (Pogona Viticeps) behaviour is
time-consuming and prone to errors. This project introduces an automated system
for real-time video analysis, using You Only Look Once (YOLO) object detection
models to identify two key behaviours: basking and hunting. We trained five
YOLO variants (v5, v7, v8, v11, v12) on a custom, publicly available dataset of
1200 images, encompassing bearded dragons (600), heating lamps (500), and
crickets (100). YOLOv8s was selected as the optimal model due to its superior
balance of accuracy (mAP@0.5:0.95 = 0.855) and speed. The system processes
video footage by extracting per-frame object coordinates, applying temporal
interpolation for continuity, and using rule-based logic to classify specific
behaviours. Basking detection proved reliable. However, hunting detection was
less accurate, primarily due to weak cricket detection (mAP@0.5 = 0.392).
Future improvements will focus on enhancing cricket detection through expanded
datasets or specialised small-object detectors. This automated system offers a
scalable solution for monitoring reptile behaviour in controlled environments,
significantly improving research efficiency and data quality.

</details>


### [36] [AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based Visible-Infrared Person Re-ID](https://arxiv.org/abs/2507.17995)
*Huy Nguyen,Kien Nguyen,Akila Pemasiri,Akmal Jahan,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 本文提出了首个空地交叉模态视频行人重识别数据集AG-VPReID.VIR，并引入了新型三流架构TCC-VPReID，以解决现有地面数据集的局限性并提升全天候监控下的行人重识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别数据集主要关注地面视角，存在遮挡、覆盖有限等问题，而空中视角能有效规避这些问题。实现全天候监控急需跨可见光与红外模态的行人重识别技术，但缺乏空地交叉模态的视频行人重识别数据集来支持相关研究。

Method: 1. 构建并发布了AG-VPReID.VIR数据集，这是首个空地交叉模态视频行人重识别数据集，包含1,837个身份、4,861个轨迹段，通过无人机和固定CCTV相机采集RGB和红外模态数据。2. 提出了TCC-VPReID，一种新颖的三流架构，通过风格鲁棒特征学习、基于记忆的跨视角适应和中介引导的时间建模，旨在弥合空地视角和RGB-IR模态之间的域差距，应对跨平台和跨模态行人重识别的挑战。

Result: 实验表明，AG-VPReID.VIR数据集相比现有数据集提出了独特的挑战。所提出的TCC-VPReID框架在多个评估协议下均实现了显著的性能提升。

Conclusion: 该研究通过构建创新的空地交叉模态数据集和设计有效的三流深度学习架构，成功应对了全天候监控中跨平台和跨模态行人重识别的挑战，显著提升了该领域的性能表现。

Abstract: Person re-identification (Re-ID) across visible and infrared modalities is
crucial for 24-hour surveillance systems, but existing datasets primarily focus
on ground-level perspectives. While ground-based IR systems offer nighttime
capabilities, they suffer from occlusions, limited coverage, and vulnerability
to obstructions--problems that aerial perspectives uniquely solve. To address
these limitations, we introduce AG-VPReID.VIR, the first aerial-ground
cross-modality video-based person Re-ID dataset. This dataset captures 1,837
identities across 4,861 tracklets (124,855 frames) using both UAV-mounted and
fixed CCTV cameras in RGB and infrared modalities. AG-VPReID.VIR presents
unique challenges including cross-viewpoint variations, modality discrepancies,
and temporal dynamics. Additionally, we propose TCC-VPReID, a novel
three-stream architecture designed to address the joint challenges of
cross-platform and cross-modality person Re-ID. Our approach bridges the domain
gaps between aerial-ground perspectives and RGB-IR modalities, through
style-robust feature learning, memory-based cross-view adaptation, and
intermediary-guided temporal modeling. Experiments show that AG-VPReID.VIR
presents distinctive challenges compared to existing datasets, with our
TCC-VPReID framework achieving significant performance gains across multiple
evaluation protocols. Dataset and code are available at
https://github.com/agvpreid25/AG-VPReID.VIR.

</details>


### [37] [Exploring the interplay of label bias with subgroup size and separability: A case study in mammographic density classification](https://arxiv.org/abs/2507.17996)
*Emma A. M. Stanley,Raghav Mehta,Mélanie Roschewitz,Nils D. Forkert,Ben Glocker*

Main category: cs.CV

TL;DR: 本研究探讨医疗影像数据集中标签偏见对深度学习模型特征和性能的影响，发现偏见会导致特征偏移和性能下降，其影响程度与亚群大小和可分离性相关，并在验证集受偏见影响时尤为显著。


<details>
  <summary>Details</summary>
Motivation: 医疗影像数据集中，特定亚群的系统性错误标注（即标签偏见）对医疗AI系统的公平性是一个研究不足的问题。

Method: 使用EMory BrEast imaging Dataset (EMBED) 训练深度学习模型进行二元组织密度分类。通过模拟标签偏见，研究受影响亚群的大小和可分离性如何影响模型学习的特征和性能。标签偏见分别影响可分离亚群（基于影像制造商）和不可分离的“伪亚群”。

Result: ['模拟的亚群标签偏见导致模型学习到的特征表示发生显著偏移。', '这些特征空间内的偏移程度取决于受标签偏见影响亚群的相对大小和可分离性。', '亚群性能表现出显著差异，这取决于用于定义模型分类阈值的验证集是否包含干净标签。', '例如，当标签偏见影响多数可分离亚群时，若验证集带有偏见标签，该亚群的真阳性率从0.898降至0.518（相对于验证集带有干净标签的情况）。']

Conclusion: 本研究是理解标签偏见对医疗影像AI中亚群公平性影响的关键贡献。

Abstract: Systematic mislabelling affecting specific subgroups (i.e., label bias) in
medical imaging datasets represents an understudied issue concerning the
fairness of medical AI systems. In this work, we investigated how size and
separability of subgroups affected by label bias influence the learned features
and performance of a deep learning model. Therefore, we trained deep learning
models for binary tissue density classification using the EMory BrEast imaging
Dataset (EMBED), where label bias affected separable subgroups (based on
imaging manufacturer) or non-separable "pseudo-subgroups". We found that
simulated subgroup label bias led to prominent shifts in the learned feature
representations of the models. Importantly, these shifts within the feature
space were dependent on both the relative size and the separability of the
subgroup affected by label bias. We also observed notable differences in
subgroup performance depending on whether a validation set with clean labels
was used to define the classification threshold for the model. For instance,
with label bias affecting the majority separable subgroup, the true positive
rate for that subgroup fell from 0.898, when the validation set had clean
labels, to 0.518, when the validation set had biased labels. Our work
represents a key contribution toward understanding the consequences of label
bias on subgroup fairness in medical imaging AI.

</details>


### [38] [Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold](https://arxiv.org/abs/2507.17998)
*Jaeho Shin,Hyeonjae Gil,Junwoo Jang,Maani Ghaffari,Ayoung Kim*

Main category: cs.CV

TL;DR: 本文提出并首次推导了仿射格拉斯曼流形上针对刚体变换的可优化代价函数，解决了现有方法无法直接用于配准的问题，并实现了全局最优解。


<details>
  <summary>Details</summary>
Motivation: 尽管仿射格拉斯曼流形在度量特征距离方面具有理论精确性，但现有方法无法提供一个显式的、关于刚体变换的可优化距离函数，这限制了其在配准问题中的应用。

Method: 本文首次明确推导了一个关于刚体变换（R和t）的、可优化的格拉斯曼特征间代价函数。通过数学证明，提出高维线性子空间基底可作为代价的显式表示，并基于变换后的基底构建了可优化代价函数，适用于任意仿射子空间的配准。

Result: 与基于向量参数的方法相比，该方法通过直接最小化测地距离，能够找到全局最优解且不受表示模糊性的影响。所提出的代价函数及其在内点集最大化BnB求解器中的扩展，在多种计算机视觉任务中均改善了现有解决方案的收敛性或超越了其性能。

Conclusion: 本研究成功地为仿射格拉斯曼流形上的配准问题提供了一个可优化的、鲁棒的代价函数，实现了全局最优解，并显著提升了计算机视觉任务中的配准效果。

Abstract: Affine Grassmannian has been favored for expressing proximity between lines
and planes due to its theoretical exactness in measuring distances among
features. Despite this advantage, the existing method can only measure the
proximity without yielding the distance as an explicit function of rigid body
transformation. Thus, an optimizable distance function on the manifold has
remained underdeveloped, stifling its application in registration problems.
This paper is the first to explicitly derive an optimizable cost function
between two Grassmannian features with respect to rigid body transformation
($\mathbf{R}$ and $\mathbf{t}$). Specifically, we present a rigorous
mathematical proof demonstrating that the bases of high-dimensional linear
subspaces can serve as an explicit representation of the cost. Finally, we
propose an optimizable cost function based on the transformed bases that can be
applied to the registration problem of any affine subspace. Compared to vector
parameter-based approaches, our method is able to find a globally optimal
solution by directly minimizing the geodesic distance which is agnostic to
representation ambiguity. The resulting cost function and its extension to the
inlier-set maximizing \ac{BnB} solver have been demonstrated to improve the
convergence of existing solutions or outperform them in various computer vision
tasks. The code is available on
https://github.com/joomeok/GrassmannRegistration.

</details>


### [39] [GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures](https://arxiv.org/abs/2507.18009)
*Jake R. Patock,Nicole Catherine Lewis,Kevin McCoy,Christina Gomez,Canling Chen,Lorenzo Luzi*

Main category: cs.CV

TL;DR: 本文提出了GRR-CoCa模型，通过引入LLM中的先进架构改进了CoCa模型，在预训练和微调任务上显著提升了性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管最先进的多模态模型表现出色，但其架构复杂性常落后于同期的大型语言模型（LLMs），因此存在通过借鉴LLM架构进行改进的空间。

Method: 本文提出GRR-CoCa模型，通过在SOTA CoCa模型的文本解码器和视觉Transformer（ViT）编码器中集成高斯误差门控线性单元、均方根归一化和旋转位置嵌入进行改进。研究团队使用标准预训练和微调工作流程，在对比和生成任务上，将GRR-CoCa与基线CoCa（拥有修改的文本解码器但保留原始ViT编码器）进行了基准测试。

Result: GRR-CoCa在预训练数据集和三个多样化的微调数据集上均显著优于基线CoCa。预训练阶段，对比损失降低27.25%，困惑度降低3.71%，CoCa损失降低7.15%。微调阶段，平均对比损失降低13.66%，困惑度降低5.18%，CoCa损失降低5.55%。

Conclusion: GRR-CoCa改进后的架构显著提升了模型性能，并在视觉-语言领域展现出更好的泛化能力。

Abstract: State-of-the-art (SOTA) image and text generation models are multimodal
models that have many similarities to large language models (LLMs). Despite
achieving strong performances, leading foundational multimodal model
architectures frequently lag behind the architectural sophistication of
contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner
(CoCa) model that incorporates Gaussian error gated linear units, root mean
squared normalization, and rotary positional embedding into the textual
decoders and the vision transformer (ViT) encoder. Each architectural
modification has been shown to improve model performance in LLMs, but has yet
to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model
with the same modified textual decoders but with CoCa's original ViT encoder.
We used standard pretraining and fine-tuning workflows to benchmark the models
on contrastive and generative tasks. Our GRR-CoCa significantly outperformed
Baseline CoCa on the pretraining dataset and three diverse fine-tuning
datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in
perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were
13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We
show that GRR-CoCa's modified architecture improves performance and
generalization across vision-language domains.

</details>


### [40] [Celeb-DF++: A Large-scale Challenging Video DeepFake Benchmark for Generalizable Forensics](https://arxiv.org/abs/2507.18015)
*Yuezun Li,Delong Zhu,Xinjie Cui,Siwei Lyu*

Main category: cs.CV

TL;DR: 针对DeepFake检测的泛化性挑战，本文推出了新的大规模、多样化视频DeepFake基准数据集Celeb-DF++，并评估了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: AI技术进步导致DeepFake视频类型日益多样化，对泛化性DeepFake检测（即使用单一模型检测广泛未知DeepFake类型）提出了紧迫挑战。现有数据集尽管规模大，但伪造类型有限，不足以开发泛化性检测方法。

Method: 在现有Celeb-DF数据集基础上，构建并引入了大规模、高挑战性的视频DeepFake基准数据集Celeb-DF++。该数据集涵盖换脸、人脸重演、说话人脸三种常见伪造场景，使用22种不同DeepFake方法生成大量高质量伪造视频。同时，提出了评估协议，用于衡量24种现有检测方法的泛化能力。

Result: 对24种现有检测方法的泛化性评估结果表明，当前方法存在显著局限性，并突显了新数据集的难度。

Conclusion: Celeb-DF++为泛化性DeepFake检测提供了一个关键的基准，其评估结果揭示了现有检测方法的不足，为未来研究指明了方向。

Abstract: The rapid advancement of AI technologies has significantly increased the
diversity of DeepFake videos circulating online, posing a pressing challenge
for \textit{generalizable forensics}, \ie, detecting a wide range of unseen
DeepFake types using a single model. Addressing this challenge requires
datasets that are not only large-scale but also rich in forgery diversity.
However, most existing datasets, despite their scale, include only a limited
variety of forgery types, making them insufficient for developing generalizable
detection methods. Therefore, we build upon our earlier Celeb-DF dataset and
introduce {Celeb-DF++}, a new large-scale and challenging video DeepFake
benchmark dedicated to the generalizable forensics challenge. Celeb-DF++ covers
three commonly encountered forgery scenarios: Face-swap (FS), Face-reenactment
(FR), and Talking-face (TF). Each scenario contains a substantial number of
high-quality forged videos, generated using a total of 22 various recent
DeepFake methods. These methods differ in terms of architectures, generation
pipelines, and targeted facial regions, covering the most prevalent DeepFake
cases witnessed in the wild. We also introduce evaluation protocols for
measuring the generalizability of 24 recent detection methods, highlighting the
limitations of existing detection methods and the difficulty of our new
dataset.

</details>


### [41] [High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency and photorealistic details](https://arxiv.org/abs/2507.18023)
*Jun Zhou,Dinghao Li,Nannan Li,Mingjie Wang*

Main category: cs.CV

TL;DR: 本文提出一种新的3D高斯修复框架，通过蒙版精炼和不确定性引导优化，利用稀疏修复视图重建完整3D场景，在视觉质量和多视角一致性上超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 尽管NeRF和3DGS等技术显著提升了3D内容创建的保真度和效率，但3D场景修复（inpainting）仍具挑战性，主要受限于3D结构的不规则性以及保持多视角一致性的需求。

Method: 本文提出一个新颖的3D高斯修复框架，通过利用稀疏修复视图重建完整的3D场景。该框架包含两个关键部分：1) 自动蒙版精炼过程，通过高斯场景滤波和反投影操作，实现对遮挡区域更准确的定位和逼真的边界恢复；2) 区域不确定性引导优化策略，在训练过程中估计各区域在多视角图像中的重要性，从而缓解多视角不一致性并增强修复结果的细节保真度。

Result: 在多样数据集上进行的综合实验表明，本文提出的方法在视觉质量和视角一致性方面均优于现有最先进的方法。

Conclusion: 本研究提出的3D高斯修复框架通过创新的蒙版精炼和不确定性引导优化策略，有效解决了3D场景修复的挑战，显著提升了修复场景的视觉质量和多视角一致性，达到了行业领先水平。

Abstract: Recent advancements in multi-view 3D reconstruction and novel-view synthesis,
particularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting
(3DGS), have greatly enhanced the fidelity and efficiency of 3D content
creation. However, inpainting 3D scenes remains a challenging task due to the
inherent irregularity of 3D structures and the critical need for maintaining
multi-view consistency. In this work, we propose a novel 3D Gaussian inpainting
framework that reconstructs complete 3D scenes by leveraging sparse inpainted
views. Our framework incorporates an automatic Mask Refinement Process and
region-wise Uncertainty-guided Optimization. Specifically, we refine the
inpainting mask using a series of operations, including Gaussian scene
filtering and back-projection, enabling more accurate localization of occluded
regions and realistic boundary restoration. Furthermore, our Uncertainty-guided
Fine-grained Optimization strategy, which estimates the importance of each
region across multi-view images during training, alleviates multi-view
inconsistencies and enhances the fidelity of fine details in the inpainted
results. Comprehensive experiments conducted on diverse datasets demonstrate
that our approach outperforms existing state-of-the-art methods in both visual
quality and view consistency.

</details>


### [42] [Emotion Recognition from Skeleton Data: A Comprehensive Survey](https://arxiv.org/abs/2507.18026)
*Haifeng Lu,Jiuyi Chen,Zhen Zhang,Ruida Liu,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: 本文对基于骨骼的身体运动情绪识别技术进行了全面系统的综述，涵盖了心理模型、数据集、现有方法分类（姿态、步态及四种技术范式）、扩展应用和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 传统情绪识别方法（依赖面部表情或生理信号）存在隐私问题，而通过身体运动识别情绪提供了一种有吸引力且保护隐私的替代方案。近期3D骨骼获取和姿态估计算法的进步显著提升了基于全身运动进行情绪识别的可行性。

Method: 本综述采用系统性方法：首先介绍情绪的心理模型以及身体运动与情绪表达的关系；接着总结公开数据集并分析其采集和标注差异；然后将现有方法分为姿态基和步态基，并从数据驱动和技术视角进行分析；特别提出统一的四种技术范式分类：传统方法、Feat2Net、FeatFusionNet和End2EndNet；最后，回顾和比较各类代表性工作及其基准测试结果。

Result: 本综述提供了基于骨骼的情绪识别技术的全面系统概述；总结了公开数据集的特点；将现有方法归类为姿态基和步态基，并提出统一的四种技术范式；对比了各类代表性方法的基准测试结果。

Conclusion: 该领域情绪识别技术在心理健康评估（如抑郁症和自闭症检测）中有广泛应用潜力；同时，论文也指出了当前面临的开放挑战和未来的研究方向。

Abstract: Emotion recognition through body movements has emerged as a compelling and
privacy-preserving alternative to traditional methods that rely on facial
expressions or physiological signals. Recent advancements in 3D skeleton
acquisition technologies and pose estimation algorithms have significantly
enhanced the feasibility of emotion recognition based on full-body motion. This
survey provides a comprehensive and systematic review of skeleton-based emotion
recognition techniques. First, we introduce psychological models of emotion and
examine the relationship between bodily movements and emotional expression.
Next, we summarize publicly available datasets, highlighting the differences in
data acquisition methods and emotion labeling strategies. We then categorize
existing methods into posture-based and gait-based approaches, analyzing them
from both data-driven and technical perspectives. In particular, we propose a
unified taxonomy that encompasses four primary technical paradigms: Traditional
approaches, Feat2Net, FeatFusionNet, and End2EndNet. Representative works
within each category are reviewed and compared, with benchmarking results
across commonly used datasets. Finally, we explore the extended applications of
emotion recognition in mental health assessment, such as detecting depression
and autism, and discuss the open challenges and future research directions in
this rapidly evolving field.

</details>


### [43] [ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks](https://arxiv.org/abs/2507.18031)
*Ahmad ALBarqawi,Mahmoud Nazzal,Issa Khalil,Abdallah Khreishah,NhatHai Phan*

Main category: cs.CV

TL;DR: 本文提出ViGText，一种结合视觉大语言模型文本解释和图神经网络的Deepfake检测新方法，显著提升了对复杂定制化Deepfake的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Deepfake技术快速发展威胁媒体真实性，传统检测方法在应对复杂、定制化Deepfake时，在泛化性和抗攻击鲁棒性方面表现不足。

Method: ViGText将图像与视觉大语言模型（VLLM）生成的文本解释整合到图框架中。它将图像分割为块，构建图像图和文本图，并利用图神经网络（GNN）进行融合分析。同时采用跨空间和频率域的多级特征提取。

Result: ViGText显著增强了泛化能力，在泛化评估中F1分数从72.45%提升至98.32%。鲁棒性方面，召回率比其他方法提高11.1%，且在面对针对其图架构的定向攻击时，分类性能下降小于4%。

Conclusion: ViGText通过结合详细的视觉和文本分析，为Deepfake检测树立了新标准，有助于保障媒体真实性和信息完整性。

Abstract: The rapid rise of deepfake technology, which produces realistic but
fraudulent digital content, threatens the authenticity of media. Traditional
deepfake detection approaches often struggle with sophisticated, customized
deepfakes, especially in terms of generalization and robustness against
malicious attacks. This paper introduces ViGText, a novel approach that
integrates images with Vision Large Language Model (VLLM) Text explanations
within a Graph-based framework to improve deepfake detection. The novelty of
ViGText lies in its integration of detailed explanations with visual data, as
it provides a more context-aware analysis than captions, which often lack
specificity and fail to reveal subtle inconsistencies. ViGText systematically
divides images into patches, constructs image and text graphs, and integrates
them for analysis using Graph Neural Networks (GNNs) to identify deepfakes.
Through the use of multi-level feature extraction across spatial and frequency
domains, ViGText captures details that enhance its robustness and accuracy to
detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText
significantly enhances generalization and achieves a notable performance boost
when it detects user-customized deepfakes. Specifically, average F1 scores rise
from 72.45% to 98.32% under generalization evaluation, and reflects the model's
superior ability to generalize to unseen, fine-tuned variations of stable
diffusion models. As for robustness, ViGText achieves an increase of 11.1% in
recall compared to other deepfake detection approaches. When facing targeted
attacks that exploit its graph-based architecture, ViGText limits
classification performance degradation to less than 4%. ViGText uses detailed
visual and textual analysis to set a new standard for detecting deepfakes,
helping ensure media authenticity and information integrity.

</details>


### [44] [Enhancing Scene Transition Awareness in Video Generation via Post-Training](https://arxiv.org/abs/2507.18046)
*Hanwen Shen,Jiajie Lu,Yupeng Cao,Xiaonan Yang*

Main category: cs.CV

TL;DR: 本文提出并使用了“过渡感知视频 (TAV)”数据集，通过对现有模型进行后期训练，显著提升了AI生成模型在多场景视频中连贯场景过渡的能力，同时保持了图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前AI视频生成模型在生成包含连贯场景过渡的较长视频时表现不佳，主要原因是它们无法从提示中推断出何时需要场景过渡，且大多数开源模型是在单场景视频数据集上训练的，缺乏场景过渡意识。

Method: 为了解决这一问题，研究者提出了“过渡感知视频 (TAV)”数据集，该数据集包含经过预处理的、具有多个场景过渡的视频片段，并使用它对模型进行后期训练。

Result: 实验结果表明，在TAV数据集上进行后期训练后，模型在基于提示的场景过渡理解方面有所改善，缩小了所需场景与生成场景之间的差距，并保持了图像质量。

Conclusion: TAV数据集和后训练方法有效地提升了AI模型生成多场景视频的能力，解决了现有模型在处理连贯场景过渡方面的不足，为未来多场景视频生成提供了新方向。

Abstract: Recent advances in AI-generated video have shown strong performance on
\emph{text-to-video} tasks, particularly for short clips depicting a single
scene. However, current models struggle to generate longer videos with coherent
scene transitions, primarily because they cannot infer when a transition is
needed from the prompt. Most open-source models are trained on datasets
consisting of single-scene video clips, which limits their capacity to learn
and respond to prompts requiring multiple scenes. Developing scene transition
awareness is essential for multi-scene generation, as it allows models to
identify and segment videos into distinct clips by accurately detecting
transitions.
  To address this, we propose the \textbf{Transition-Aware Video} (TAV)
dataset, which consists of preprocessed video clips with multiple scene
transitions. Our experiment shows that post-training on the \textbf{TAV}
dataset improves prompt-based scene transition understanding, narrows the gap
between required and generated scenes, and maintains image quality.

</details>


### [45] [BokehDiff: Neural Lens Blur with One-Step Diffusion](https://arxiv.org/abs/2507.18060)
*Chengxuan Zhu,Qingnan Fan,Qi Zhang,Jinwei Chen,Huaqi Zhang,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: 一种利用生成扩散先验实现物理精确且视觉吸引人的新型镜头模糊渲染方法。


<details>
  <summary>Details</summary>
Motivation: 现有镜头模糊渲染方法受限于深度估计精度，在深度不连续处产生伪影。

Method: 引入物理启发的自注意力模块，结合图像形成过程、景深相关的模糊圈限制和自遮挡效应；将扩散模型适应为一步推理方案且不引入额外噪声；利用扩散模型合成带有透明度的真实感前景以解决配对数据缺乏问题。

Result: 实现了高质量和高保真度的渲染结果。

Conclusion: BokehDiff通过新颖的框架克服了传统方法的局限性，提供了优越的镜头模糊渲染效果。

Abstract: We introduce BokehDiff, a novel lens blur rendering method that achieves
physically accurate and visually appealing outcomes, with the help of
generative diffusion prior. Previous methods are bounded by the accuracy of
depth estimation, generating artifacts in depth discontinuities. Our method
employs a physics-inspired self-attention module that aligns with the image
formation process, incorporating depth-dependent circle of confusion constraint
and self-occlusion effects. We adapt the diffusion model to the one-step
inference scheme without introducing additional noise, and achieve results of
high quality and fidelity. To address the lack of scalable paired data, we
propose to synthesize photorealistic foregrounds with transparency with
diffusion models, balancing authenticity and scene diversity.

</details>


### [46] [Adapting Large VLMs with Iterative and Manual Instructions for Generative Low-light Enhancement](https://arxiv.org/abs/2507.18064)
*Xiaoran Sun,Liyan Wang,Cong Wang,Yeying Jin,Kin-man Lam,Zhixun Su,Yang Yang,Jinshan Pan*

Main category: cs.CV

TL;DR: VLM-IMI利用视觉语言模型和迭代指令，通过语义指导增强低光图像，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法依赖预训练模型或低光输入，但忽视了正常光图像的语义指导，导致在复杂光照条件下效果受限。

Method: 提出VLM-IMI框架，结合大型视觉语言模型（VLM）和迭代手动指令（IMI）进行低光图像增强。该方法将期望正常光内容的文本描述作为增强线索，并引入指令先验融合模块动态对齐和融合图像与文本特征。推理阶段采用迭代手动指令策略，逐步细化文本指令以提升视觉质量。

Result: VLM-IMI显著增强了结构保真度、语义对齐，并恢复了极端低光条件下的精细细节。在定量指标和感知质量上均超越了现有最先进方法。

Conclusion: VLM-IMI通过整合视觉语言模型的语义指导，有效解决了现有低光图像增强方法的局限性，在复杂光照条件下实现了高质量的图像恢复。

Abstract: Most existing low-light image enhancement (LLIE) methods rely on pre-trained
model priors, low-light inputs, or both, while neglecting the semantic guidance
available from normal-light images. This limitation hinders their effectiveness
in complex lighting conditions. In this paper, we propose VLM-IMI, a novel
framework that leverages large vision-language models (VLMs) with iterative and
manual instructions (IMIs) for LLIE. VLM-IMI incorporates textual descriptions
of the desired normal-light content as enhancement cues, enabling semantically
informed restoration. To effectively integrate cross-modal priors, we introduce
an instruction prior fusion module, which dynamically aligns and fuses image
and text features, promoting the generation of detailed and semantically
coherent outputs. During inference, we adopt an iterative and manual
instruction strategy to refine textual instructions, progressively improving
visual quality. This refinement enhances structural fidelity, semantic
alignment, and the recovery of fine details under extremely low-light
conditions. Extensive experiments across diverse scenarios demonstrate that
VLM-IMI outperforms state-of-the-art methods in both quantitative metrics and
perceptual quality. The source code is available at
https://github.com/sunxiaoran01/VLM-IMI.

</details>


### [47] [TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound](https://arxiv.org/abs/2507.18082)
*Pascal Spiegler,Taha Koleilat,Arash Harirpoush,Corey S. Miller,Hassan Rivaz,Marta Kersten-Oertel,Yiming Xiao*

Main category: cs.CV

TL;DR: TextSAM-EUS是一种新型轻量级文本驱动的SAM模型，用于自动分割胰腺肿瘤的内窥镜超声图像，无需人工几何提示，且表现优于现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 胰腺癌预后不良，依赖超声内镜（EUS）进行活检和放疗。然而，EUS图像存在散斑噪声、低对比度及不直观等问题，导致全监督深度学习模型进行胰腺肿瘤分割时容易出错，并高度依赖大量专家标注的数据集。

Method: 本文提出了TextSAM-EUS，一种新型轻量级、文本驱动的Segment Anything Model（SAM）变体，推断时无需手动几何提示。该方法通过BiomedCLIP文本编码器结合LoRA技术对SAM架构进行适应性调整，利用文本提示学习（上下文优化），实现EUS图像中的胰腺肿瘤自动分割，仅调整了0.86%的总参数。

Result: 在公共胰腺超声内镜数据库上，TextSAM-EUS在自动提示下达到了82.69%的Dice相似系数和85.28%的归一化表面距离（NSD），而在手动几何提示下则达到了83.10%的Dice和85.70%的NSD。其性能优于现有最先进（SOTA）的监督式深度学习模型和基础模型（如SAM及其变体）。

Conclusion: TextSAM-EUS是首次尝试将提示学习整合到基于SAM的医学图像分割中，为高效和鲁棒的自动EUS分割提供了一个实用的选择。

Abstract: Pancreatic cancer carries a poor prognosis and relies on endoscopic
ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle
noise, low contrast, and unintuitive appearance of EUS make segmentation of
pancreatic tumors with fully supervised deep learning (DL) models both
error-prone and dependent on large, expert-curated annotation datasets. To
address these challenges, we present TextSAM-EUS, a novel, lightweight,
text-driven adaptation of the Segment Anything Model (SAM) that requires no
manual geometric prompts at inference. Our approach leverages text prompt
learning (context optimization) through the BiomedCLIP text encoder in
conjunction with a LoRA-based adaptation of SAM's architecture to enable
automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total
parameters. On the public Endoscopic Ultrasound Database of the Pancreas,
TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized
surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice
and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised
DL models and foundation models (e.g., SAM and its variants). As the first
attempt to incorporate prompt learning in SAM-based medical image segmentation,
TextSAM-EUS offers a practical option for efficient and robust automatic EUS
segmentation. Our code will be publicly available upon acceptance.

</details>


### [48] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: 现有动态面部表情识别方法受样本异质性影响，性能易下降。本文提出异质性感知分布框架（HDF），包含时频分布注意力模块（DAM）和分布感知缩放模块（DSM），有效提升识别精度、鲁棒性及泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有动态面部表情识别方法在面对多源数据和个体表情变异导致的样本异质性时，性能会不可避免地下降。

Method: 提出异质性感知分布框架（HDF），并设计两个模块：
1.  **时频分布注意力模块（DAM）**：通过双分支注意力设计，捕获时间一致性和频率鲁棒性，以增强对序列不一致性和视觉风格变化的容忍度。
2.  **分布感知缩放模块（DSM）**：基于梯度敏感性和信息瓶颈原理，自适应地平衡分类和对比损失，实现更稳定和有区分度的表示学习。

Result: 在DFEW和FERV39k数据集上的实验表明，HDF显著提高了识别准确性和鲁棒性。该方法实现了卓越的加权平均召回率（WAR）和未加权平均召回率（UAR），并在多样化和不平衡场景中保持了强大的泛化能力。

Conclusion: HDF框架通过有效处理样本异质性，显著提升了动态面部表情识别的精度和鲁棒性，并在复杂场景下展现出优异的泛化能力。

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in
affective computing and human-computer interaction. Although existing methods
achieve comparable performance, they inevitably suffer from performance
degradation under sample heterogeneity caused by multi-source data and
individual expression variability. To address these challenges, we propose a
novel framework, called Heterogeneity-aware Distributional Framework (HDF), and
design two plug-and-play modules to enhance time-frequency modeling and
mitigate optimization imbalance caused by hard samples. Specifically, the
Time-Frequency Distributional Attention Module (DAM) captures both temporal
consistency and frequency robustness through a dual-branch attention design,
improving tolerance to sequence inconsistency and visual style shifts. Then,
based on gradient sensitivity and information bottleneck principles, an
adaptive optimization module Distribution-aware Scaling Module (DSM) is
introduced to dynamically balance classification and contrastive losses,
enabling more stable and discriminative representation learning. Extensive
experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF
significantly improves both recognition accuracy and robustness. Our method
achieves superior weighted average recall (WAR) and unweighted average recall
(UAR) while maintaining strong generalization across diverse and imbalanced
scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [49] [Comparison of Segmentation Methods in Remote Sensing for Land Use Land Cover](https://arxiv.org/abs/2507.18099)
*Naman Srivastava,Joel D Joy,Yash Dixit,Swarup E,Rakshit Ramesh*

Main category: cs.CV

TL;DR: 本研究评估了先进的土地利用土地覆盖（LULC）制图技术，结合大气校正、监督和半监督学习模型（如DeeplabV3+和动态加权CPS），并通过印度海得拉巴的案例研究，证明了其在分析城市化进程中土地利用变化方面的实用性。


<details>
  <summary>Details</summary>
Motivation: 土地利用土地覆盖（LULC）制图对于城市和资源规划至关重要，是发展智慧和可持续城市的关键要素。本研究旨在评估先进的LULC制图技术，以支持这些需求。

Method: 本研究采用以下方法：1. 对Cartosat Multispectral (MX)传感器图像应用基于查找表（LUT）的大气校正。2. 使用监督和半监督学习模型进行LULC预测，具体探索DeeplabV3+和交叉伪监督（CPS）模型。3. CPS模型通过动态加权进一步优化，以提高训练期间伪标签的可靠性。4. 通过分析印度海得拉巴的Cartosat MX时间序列图像进行案例研究，以评估技术的实用性。

Result: 研究结果表明，所评估的LULC制图技术在各种城市规划应用中具有准确性和实用性。海得拉巴的案例研究揭示了快速城市化导致的显著土地利用变化，包括城市蔓延、绿地减少和工业区扩张。

Conclusion: 本研究证明了所采用的LULC制图技术（包括大气校正和先进的监督/半监督学习模型）对于城市规划者和政策制定者具有实际应用价值，能够有效监测和分析土地利用变化。

Abstract: Land Use Land Cover (LULC) mapping is essential for urban and resource
planning, and is one of the key elements in developing smart and sustainable
cities.This study evaluates advanced LULC mapping techniques, focusing on
Look-Up Table (LUT)-based Atmospheric Correction applied to Cartosat
Multispectral (MX) sensor images, followed by supervised and semi-supervised
learning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo
Supervision (CPS). The CPS model is further refined with dynamic weighting,
enhancing pseudo-label reliability during training. This comprehensive approach
analyses the accuracy and utility of LULC mapping techniques for various urban
planning applications. A case study of Hyderabad, India, illustrates
significant land use changes due to rapid urbanization. By analyzing Cartosat
MX images over time, we highlight shifts such as urban sprawl, shrinking green
spaces, and expanding industrial areas. This demonstrates the practical utility
of these techniques for urban planners and policymakers.

</details>


### [50] [Datasets and Recipes for Video Temporal Grounding via Reinforcement Learning](https://arxiv.org/abs/2507.18100)
*Ruizhe Chen,Zhiting Fan,Tianze Luo,Heqing Zou,Zhaopeng Feng,Guiyang Xie,Hansheng Zhang,Zhuochen Wang,Zuozhu Liu,Huaijian Zhang*

Main category: cs.CV

TL;DR: 本文提出一种结合监督微调（SFT）和难度控制强化学习（RL）的两阶段训练框架，旨在提升视频时间定位（VTG）模型的准确性和鲁棒性，并在多个基准测试中表现优异，尤其在复杂和开放域场景下。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉-语言模型（LVLM）和指令微调取得了进展，但现有视频时间定位（VTG）方法在时间感知能力和泛化性方面仍存在局限性。

Method: 引入一个两阶段训练框架：首先利用高质量的冷启动数据进行监督微调（SFT）初始化，随后通过难度控制的强化学习（RL）进一步增强时间定位和推理能力。

Result: 在多个VTG基准测试中，该方法持续优于现有模型，尤其在挑战性和开放域场景中表现突出。研究深入分析了训练策略和数据集构建，强调了高质量冷启动数据和难度控制RL的重要性。

Conclusion: 高质量的冷启动数据和难度控制的强化学习对提升VTG模型的准确性和鲁棒性至关重要。本工作提出的框架有效解决了现有方法的局限性，并开源所有中间数据集、模型和代码以促进后续研究和工业应用。

Abstract: Video Temporal Grounding (VTG) aims to localize relevant temporal segments in
videos given natural language queries. Despite recent progress with large
vision-language models (LVLMs) and instruction-tuning, existing approaches
often suffer from limited temporal awareness and poor generalization. In this
work, we introduce a two-stage training framework that integrates supervised
fine-tuning with reinforcement learning (RL) to improve both the accuracy and
robustness of VTG models. Our approach first leverages high-quality curated
cold start data for SFT initialization, followed by difficulty-controlled RL to
further enhance temporal localization and reasoning abilities. Comprehensive
experiments on multiple VTG benchmarks demonstrate that our method consistently
outperforms existing models, particularly in challenging and open-domain
scenarios. We conduct an in-depth analysis of training strategies and dataset
curation, highlighting the importance of both high-quality cold start data and
difficulty-controlled RL. To facilitate further research and industrial
adoption, we release all intermediate datasets, models, and code to the
community.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [51] [ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid Mechanics](https://arxiv.org/abs/2507.17777)
*Theofanis Aravanis,Grigorios Chrimatopoulos,Mohammad Ferdows,Michalis Xenos,Efstratios Em Tzirtzilakis*

Main category: cs.AI

TL;DR: 本研究使用符号回归（SR）从模拟数据中推导出三维不可压缩流动的可解释数学方程，并提出将SR与答案集编程（ASP）结合的混合方法，以确保模型不仅精确且符合物理原理，从而提高数据驱动模型的可解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习方法常被批评为“黑箱”，而在流体力学中，对底层流动物理的理解与准确预测同等重要。符号回归（SR）无需先验假设即可揭示可解释的数学关系，因此本研究旨在利用SR来建模矩形通道中层流条件下的三维不可压缩流体的轴向速度和压力场。

Method: 研究采用PySR库直接从数值模拟数据中推导紧凑的符号方程。此外，提出了一种创新方法，将SR与答案集编程（ASP）相结合，构建了一个混合SR/ASP框架。此框架旨在结合SR的生成能力与ASP的声明式推理优势，以确保SR生成的符号表达式不仅统计准确，而且符合领域特定原则，具备物理合理性。

Result: 研究推导出的方程成功近似了所研究流体流动的抛物线速度剖面和压降，并与文献中的解析解完美吻合。所提出的混合SR/ASP框架确保了SR生成的表达式在统计上准确，且在物理上合理。研究突出了SR将复杂流动行为简化为简洁、可解释方程的能力，以及知识表示方法（如ASP）提升数据驱动SR模型可靠性及其与领域原则一致性的潜力。

Conclusion: 本研究强调了符号回归在将复杂流体行为简化为简洁、可解释方程方面的强大能力，并展示了知识表示方法（如ASP）在提高数据驱动SR模型可靠性及其与领域原则一致性方面的潜力。从三维通道流动中获得的见解为将此类混合方法整合到需要可解释预测和实时数据分析的框架中奠定了基础。

Abstract: Unlike conventional Machine-Learning (ML) approaches, often criticized as
"black boxes", Symbolic Regression (SR) stands out as a powerful tool for
revealing interpretable mathematical relationships in complex physical systems,
requiring no a priori assumptions about models' structures. Motivated by the
recognition that, in fluid mechanics, an understanding of the underlying flow
physics is as crucial as accurate prediction, this study applies SR to model a
fundamental three-dimensional (3D) incompressible flow in a rectangular
channel, focusing on the (axial) velocity and pressure fields under laminar
conditions. By employing the PySR library, compact symbolic equations were
derived directly from numerical simulation data, revealing key characteristics
of the flow dynamics. These equations not only approximate the parabolic
velocity profile and pressure drop observed in the studied fluid flow, but also
perfectly coincide with analytical solutions from the literature. Furthermore,
we propose an innovative approach that integrates SR with the
knowledge-representation framework of Answer Set Programming (ASP), combining
the generative power of SR with the declarative reasoning strengths of ASP. The
proposed hybrid SR/ASP framework ensures that the SR-generated symbolic
expressions are not only statistically accurate, but also physically plausible,
adhering to domain-specific principles. Overall, the study highlights two key
contributions: SR's ability to simplify complex flow behaviours into concise,
interpretable equations, and the potential of knowledge-representation
approaches to improve the reliability and alignment of data-driven SR models
with domain principles. Insights from the examined 3D channel flow pave the way
for integrating such hybrid approaches into efficient frameworks, [...] where
explainable predictions and real-time data analysis are crucial.

</details>


### [52] [I2I-STRADA -- Information to Insights via Structured Reasoning Agent for Data Analysis](https://arxiv.org/abs/2507.17874)
*SaiBarath Sundar,Pranav Satheesan,Udayaadithya Avadhanam*

Main category: cs.AI

TL;DR: 现有数据分析智能体系统忽视结构化推理，本文提出I2I-STRADA，一个旨在形式化分析认知流程的智能体架构，并在基准测试中证实了其在规划连贯性和洞察对齐方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 当前数据分析智能体系统（如多智能体框架）虽能处理任务管理，但未能充分考虑分析思维背后的结构化推理过程。通用型大型语言模型（LLMs）的推理步骤缺乏固定模式，而实际数据分析需要一致的认知工作流，包括解释目标、联系知识、构建计划和适应执行。

Method: 引入I2I-STRADA（Information-to-Insight via Structured Reasoning Agent for Data Analysis），一个智能体架构，旨在通过建模分析展开的模块化子任务来形式化数据分析的结构化推理过程，这些子任务反映了分析推理的认知步骤。

Result: 在DABstep和DABench基准测试上的评估表明，I2I-STRADA在规划连贯性（planning coherence）和洞察对齐（insight alignment）方面均优于现有系统。

Conclusion: 研究结果强调了在数据分析智能体设计中，结构化认知工作流的重要性。

Abstract: Recent advances in agentic systems for data analysis have emphasized
automation of insight generation through multi-agent frameworks, and
orchestration layers. While these systems effectively manage tasks like query
translation, data transformation, and visualization, they often overlook the
structured reasoning process underlying analytical thinking. Reasoning large
language models (LLMs) used for multi-step problem solving are trained as
general-purpose problem solvers. As a result, their reasoning or thinking steps
do not adhere to fixed processes for specific tasks. Real-world data analysis
requires a consistent cognitive workflow: interpreting vague goals, grounding
them in contextual knowledge, constructing abstract plans, and adapting
execution based on intermediate outcomes. We introduce I2I-STRADA
(Information-to-Insight via Structured Reasoning Agent for Data Analysis), an
agentic architecture designed to formalize this reasoning process. I2I-STRADA
focuses on modeling how analysis unfolds via modular sub-tasks that reflect the
cognitive steps of analytical reasoning. Evaluations on the DABstep and DABench
benchmarks show that I2I-STRADA outperforms prior systems in planning coherence
and insight alignment, highlighting the importance of structured cognitive
workflows in agent design for data analysis.

</details>


### [53] [SMARTAPS: Tool-augmented LLMs for Operations Management](https://arxiv.org/abs/2507.17927)
*Timothy Tin Long Yu,Mahdi Mostajabdaveh,Jabo Serge Byusa,Rindra Ramamonjison,Giuseppe Carenini,Kun Mao,Zirui Zhou,Yong Zhang*

Main category: cs.AI

TL;DR: 该论文介绍SmartAPS，一个基于工具增强型大型语言模型（LLM）的会话系统，旨在为运营规划师提供更易访问且成本更低的先进规划系统（APS）。


<details>
  <summary>Details</summary>
Motivation: 传统的先进规划系统（APS）虽然功能强大且有益，但由于定制和维护需要持续的顾问成本，导致许多用户无法承担。因此，供应链规划师对更易于访问的APS有迫切需求。

Method: 开发了一个名为SmartAPS的会话系统。该系统基于工具增强型大型语言模型（LLM）构建，并提供直观的自然语言聊天界面。

Result: SmartAPS系统使得运营规划师能够通过自然语言查询信息、执行反事实推理、接收建议以及进行情景分析，从而更好地管理其运营。

Conclusion: 通过利用大型语言模型，SmartAPS为运营规划师提供了一个直观且可访问的解决方案，有效降低了传统APS的高昂咨询成本，从而使高级规划系统更加普及。

Abstract: Large language models (LLMs) present intriguing opportunities to enhance user
interaction with traditional algorithms and tools in real-world applications.
An advanced planning system (APS) is a sophisticated software that leverages
optimization to help operations planners create, interpret, and modify an
operational plan. While highly beneficial, many customers are priced out of
using an APS due to the ongoing costs of consultants responsible for
customization and maintenance. To address the need for a more accessible APS
expressed by supply chain planners, we present SmartAPS, a conversational
system built on a tool-augmented LLM. Our system provides operations planners
with an intuitive natural language chat interface, allowing them to query
information, perform counterfactual reasoning, receive recommendations, and
execute scenario analysis to better manage their operation. A short video
demonstrating the system has been released: https://youtu.be/KtIrJjlDbyw

</details>


### [54] [Synthesis of timeline-based planning strategies avoiding determinization](https://arxiv.org/abs/2507.17988)
*Dario Della Monica,Angelo Montanari,Pietro Sala*

Main category: cs.AI

TL;DR: 为解决定性时间线规划中策略合成依赖NFA确定化的问题，本研究识别了一个可直接映射至DFA的规划片段，并定位了适用于该片段的Allen关系子集，从而实现策略的直接合成。


<details>
  <summary>Details</summary>
Motivation: 定性时间线规划的计划存在问题虽然可通过非确定性有限自动机（NFA）的非空问题求解，但NFA无法直接用于合成规划策略，因为需要耗费高昂的确定化步骤。

Method: 本文识别了定性时间线规划的一个特定片段，其计划存在问题可以直接映射到确定性有限自动机（DFA）的非空问题。此外，还识别了一个能适应此确定性片段的Allen关系的最大子集。

Result: 成功找到了一个定性时间线规划的片段，该片段的计划存在问题可直接与确定性有限自动机（DFA）的非空问题关联，从而允许直接合成策略。同时，也识别出了适用于此确定性片段的Allen关系的最大子集。

Conclusion: 通过识别出定性时间线规划中一个可直接对应DFA的片段，本研究克服了现有策略合成方法中对NFA确定化的高昂依赖，为更直接、高效地合成规划策略提供了理论与方法基础。

Abstract: Qualitative timeline-based planning models domains as sets of independent,
but
  interacting, components whose behaviors over time, the timelines, are
governed
  by sets of qualitative temporal constraints (ordering relations), called
  synchronization rules.
  Its plan-existence problem has been shown to be PSPACE-complete; in
  particular, PSPACE-membership has been proved via reduction to the
  nonemptiness problem for nondeterministic finite automata.
  However, nondeterministic automata cannot be directly used to synthesize
  planning strategies as a costly determinization step is needed.
  In this paper, we identify a fragment of qualitative timeline-based planning
  whose plan-existence problem can be directly mapped into the nonemptiness
  problem of deterministic finite automata, which can then
  synthesize strategies.
  In addition, we identify a maximal subset of Allen's relations that fits into
  such a deterministic fragment.

</details>


### [55] [E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI](https://arxiv.org/abs/2507.18004)
*Yusen Peng,Shuhua Mao*

Main category: cs.AI

TL;DR: 本文提出E.A.R.T.H.框架，通过将模型生成的错误转化为创造性资产，实现误差驱动、反馈增强的AI创造力生成，显著提升了生成输出的创造性、新颖性和质量。


<details>
  <summary>Details</summary>
Motivation: 探究人工智能如何超越模仿，实现真正的创造力。

Method: 提出E.A.R.T.H.框架，包含错误生成、放大、精选、转换和反馈利用五个阶段。该方法将认知科学与生成建模相结合，通过结构化提示、语义评分和人机协同评估将错误转化为创造性资产。实现时使用LLaMA-2-7B-Chat、SBERT、BERTScore等模型，并采用基于新颖性、惊喜度和相关性的复合奖励函数。

Result: 在精炼阶段，创造力得分提升52.5%，最终输出提升70.4%。精炼后的标语更短、更具新颖性，相关性下降极小。跨模态测试显示标语与图像对齐度高。人工评估中，60%的输出得分≥4.0，其中隐喻性标语表现更佳，并展现出风格精确性和情感共鸣。

Conclusion: 以错误为中心、反馈驱动的生成方式能显著提升AI的创造力，为实现自我演化、与人类对齐的创造性AI提供了可扩展的路径。

Abstract: How can AI move beyond imitation toward genuine creativity? This paper
proposes the E.A.R.T.H. framework, a five-stage generative pipeline that
transforms model-generated errors into creative assets through Error
generation, Amplification, Refine selection, Transform, and Harness feedback.
Drawing on cognitive science and generative modeling, we posit that "creative
potential hides in failure" and operationalize this via structured prompts,
semantic scoring, and human-in-the-loop evaluation. Implemented using
LLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the
pipeline employs a composite reward function based on novelty, surprise, and
relevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to
1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4%
improvement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a
4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment
(CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs
scored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones
(3.99). Feedback highlights stylistic precision and emotional resonance. These
results demonstrate that error-centered, feedback-driven generation enhances
creativity, offering a scalable path toward self-evolving, human-aligned
creative AI.

</details>


### [56] [Does visualization help AI understand data?](https://arxiv.org/abs/2507.18022)
*Victoria R. Li,Johnathan Sun,Martin Wattenberg*

Main category: cs.AI

TL;DR: 研究表明AI系统也能像人类一样从数据可视化中受益，图表能提升其数据分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 探究图表和图形是否也能对AI系统分析数据有所帮助，而不仅仅是人类。

Method: 使用GPT 4.1和Claude 3.5两款商业视觉语言模型进行一系列实验，在三个代表性分析任务中，评估它们在有无散点图辅助下描述合成数据集的表现。同时设置提供空白图表和数据不匹配图表的基线进行比较。

Result: 当原始数据附带散点图时，两款AI系统能更精确和准确地描述合成数据集，尤其是在数据集复杂性增加时。与基线对比显示，性能的提升确实来源于图表的内容。

Conclusion: 研究结果初步证明，AI系统与人类一样，可以从数据可视化中受益。

Abstract: Charts and graphs help people analyze data, but can they also be useful to AI
systems? To investigate this question, we perform a series of experiments with
two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three
representative analysis tasks, the two systems describe synthetic datasets more
precisely and accurately when raw data is accompanied by a scatterplot,
especially as datasets grow in complexity. Comparison with two baselines --
providing a blank chart and a chart with mismatched data -- shows that the
improved performance is due to the content of the charts. Our results are
initial evidence that AI systems, like humans, can benefit from visualization.

</details>


### [57] [Multi-Agent Guided Policy Optimization](https://arxiv.org/abs/2507.18059)
*Yueheng Li,Guangming Xie,Zongqing Lu*

Main category: cs.AI

TL;DR: MAGPO是一种新的多智能体强化学习框架，它通过中心化指导和去中心化执行，有效解决了现有CTDE方法未能充分利用中心化训练和缺乏理论保证的问题，并在多种任务上展现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流的CTDE（中心化训练去中心化执行）多智能体强化学习方法未能充分利用中心化训练的优势，且缺乏理论保证，导致其实用性和效果受限。

Method: 本文提出了多智能体引导策略优化（MAGPO）框架。它通过整合中心化指导与去中心化执行，更好地利用中心化训练。MAGPO采用自回归联合策略进行可扩展的协调探索，并将其与去中心化策略明确对齐，以确保在部分可观察性下的可部署性。

Result: MAGPO提供了策略单调改进的理论保证。经验评估表明，MAGPO在6个不同环境的43项任务上持续优于强大的CTDE基线方法，并能匹配或超越完全中心化方法。

Conclusion: MAGPO为去中心化多智能体学习提供了一个有原则且实用的解决方案，它有效地弥补了现有CTDE方法的不足，实现了卓越的性能和理论支持。

Abstract: Due to practical constraints such as partial observability and limited
communication, Centralized Training with Decentralized Execution (CTDE) has
become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning
(MARL). However, existing CTDE methods often underutilize centralized training
or lack theoretical guarantees. We propose Multi-Agent Guided Policy
Optimization (MAGPO), a novel framework that better leverages centralized
training by integrating centralized guidance with decentralized execution.
MAGPO uses an auto-regressive joint policy for scalable, coordinated
exploration and explicitly aligns it with decentralized policies to ensure
deployability under partial observability. We provide theoretical guarantees of
monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across
6 diverse environments. Results show that MAGPO consistently outperforms strong
CTDE baselines and matches or surpasses fully centralized approaches, offering
a principled and practical solution for decentralized multi-agent learning. Our
code and experimental data can be found in https://github.com/liyheng/MAGPO.

</details>


### [58] [AlphaGo Moment for Model Architecture Discovery](https://arxiv.org/abs/2507.18074)
*Yixiu Liu,Yang Nan,Weixian Xu,Xiangkun Hu,Lyumanshan Ye,Zhen Qin,Pengfei Liu*

Main category: cs.AI

TL;DR: ASI-Arch是一个自主AI系统，能在神经网络架构发现领域进行自我研究，打破人类认知瓶颈，发现超越人类设计的新颖架构，并首次证明科学发现可以计算化扩展。


<details>
  <summary>Details</summary>
Motivation: 尽管AI系统能力呈指数级提升，AI研究速度仍受限于人类认知能力，形成日益严重的瓶颈。

Method: 提出ASI-Arch，一种用于AI研究的人工超智能（ASI4AI）系统，在神经网络架构发现领域实现了完全自主。它超越了传统神经网络架构搜索（NAS），能自主提出新颖架构概念、将其编码实现、训练并进行实证验证，实现了从自动化优化到自动化创新的范式转变。

Result: ASI-Arch通过1773次自主实验（耗时20000 GPU小时），发现了106种创新的、最先进的（SOTA）线性注意力架构。这些AI发现的架构展示了超越人类设计的涌现设计原则。首次建立了科学发现的经验性扩展定律，证明架构突破可以计算化扩展。

Conclusion: 本研究表明AI可以自主进行架构创新，克服了人类认知限制带来的研究瓶颈。这为自我加速的AI系统提供了蓝图，将研究进展从受人类限制转变为可计算扩展的过程。

Abstract: While AI systems demonstrate exponentially improving capabilities, the pace
of AI research itself remains linearly bounded by human cognitive capacity,
creating an increasingly severe development bottleneck. We present ASI-Arch,
the first demonstration of Artificial Superintelligence for AI research
(ASI4AI) in the critical domain of neural architecture discovery--a fully
autonomous system that shatters this fundamental constraint by enabling AI to
conduct its own architectural innovation. Moving beyond traditional Neural
Architecture Search (NAS), which is fundamentally limited to exploring
human-defined spaces, we introduce a paradigm shift from automated optimization
to automated innovation. ASI-Arch can conduct end-to-end scientific research in
the domain of architecture discovery, autonomously hypothesizing novel
architectural concepts, implementing them as executable code, training and
empirically validating their performance through rigorous experimentation and
past experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000
GPU hours, culminating in the discovery of 106 innovative, state-of-the-art
(SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed
unexpected strategic insights invisible to human players, our AI-discovered
architectures demonstrate emergent design principles that systematically
surpass human-designed baselines and illuminate previously unknown pathways for
architectural innovation. Crucially, we establish the first empirical scaling
law for scientific discovery itself--demonstrating that architectural
breakthroughs can be scaled computationally, transforming research progress
from a human-limited to a computation-scalable process. We provide
comprehensive analysis of the emergent design patterns and autonomous research
capabilities that enabled these breakthroughs, establishing a blueprint for
self-accelerating AI systems.

</details>


### [59] [Agentic AI framework for End-to-End Medical Data Inference](https://arxiv.org/abs/2507.18115)
*Soorya Ram Shimgekar,Shayan Vassef,Abhay Goyal,Navin Kumar,Koustuv Saha*

Main category: cs.AI

TL;DR: 本文提出一个Agentic AI框架，通过模块化、任务特定智能体自动化临床机器学习数据管道，从数据摄取到推理，旨在降低医疗AI部署的成本和复杂性。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域构建和部署机器学习解决方案成本高昂且劳动密集，原因在于预处理工作流碎片化、模型兼容性问题以及严格的数据隐私限制。

Method: 引入了一个Agentic AI框架，包含一系列模块化、任务特定的智能体，自动化整个临床数据管道。这些智能体处理结构化和非结构化数据，实现自动特征选择、模型选择和预处理推荐。具体智能体包括：摄取识别智能体、数据匿名化智能体、特征提取智能体（基于嵌入的表格数据和基于MedGemma的多阶段图像数据）、模型-数据特征匹配智能体、预处理推荐智能体、预处理实现智能体，以及模型推理智能体（利用SHAP、LIME、DETR等工具生成可解释输出）。

Result: 该系统在老年病学、姑息治疗和结肠镜成像的公开数据集上进行了评估。通过自动化机器学习生命周期中高摩擦的阶段，该框架减少了重复专家干预的需求。

Conclusion: 所提出的框架为在临床环境中操作化人工智能提供了一条可扩展、成本效益高的途径。

Abstract: Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.

</details>


### [60] [Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes](https://arxiv.org/abs/2507.18123)
*Sedigh Khademi,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila,Jim Black*

Main category: cs.AI

TL;DR: 该研究旨在结合自然语言处理、主动学习和数据增强技术，快速开发一个分类器，用于从急诊科分诊笔记中识别疫苗安全问题，以加强上市后安全监测。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫苗快速开发后，临床试验数据有限，急需加强上市后安全监测。急诊科笔记是重要数据源，但传统关键词方法效率低且易误报，且医疗领域标注数据稀缺。

Method: 采用自然语言处理（NLP）技术，结合主动学习（Active Learning）优化数据标注和模型性能，并辅以数据增强（Data Augmentation）技术来构建分类器。

Result: 预期结果是成功开发一个能够从急诊科分诊笔记中有效检测潜在疫苗安全问题的分类器，实现更快的模型部署和更高的性能。

Conclusion: 通过整合NLP、主动学习和数据增强，可以克服医疗数据稀缺的挑战，高效构建疫苗安全监测分类器，从而显著提升上市后疫苗安全信号的及时发现能力。

Abstract: The rapid development of COVID-19 vaccines has showcased the global
communitys ability to combat infectious diseases. However, the need for
post-licensure surveillance systems has grown due to the limited window for
safety data collection in clinical trials and early widespread implementation.
This study aims to employ Natural Language Processing techniques and Active
Learning to rapidly develop a classifier that detects potential vaccine safety
issues from emergency department notes. ED triage notes, containing expert,
succinct vital patient information at the point of entry to health systems, can
significantly contribute to timely vaccine safety signal surveillance. While
keyword-based classification can be effective, it may yield false positives and
demand extensive keyword modifications. This is exacerbated by the infrequency
of vaccination-related ED presentations and their similarity to other reasons
for ED visits. NLP offers a more accurate and efficient alternative, albeit
requiring annotated data, which is often scarce in the medical field. Active
learning optimizes the annotation process and the quality of annotated data,
which can result in faster model implementation and improved model performance.
This work combines active learning, data augmentation, and active learning and
evaluation techniques to create a classifier that is used to enhance vaccine
safety surveillance from ED triage notes.

</details>


### [61] [Logical Characterizations of GNNs with Mean Aggregation](https://arxiv.org/abs/2507.18145)
*Moritz Schönherr,Carsten Lutz*

Main category: cs.AI

TL;DR: 研究了使用平均聚合函数的图神经网络（GNNs）的表达能力，并将其与模态逻辑和使用其他聚合函数的GNNs进行了比较。


<details>
  <summary>Details</summary>
Motivation: 探究使用平均聚合函数的图神经网络（GNNs）的表达能力。

Method: 通过理论分析，将平均聚合GNNs的表达能力与比率模态逻辑、无交替模态逻辑进行对应，并与使用最大值聚合和求和聚合的GNNs在非统一和统一设置下进行比较。

Result: 在非统一设置下，平均GNNs的表达能力与比率模态逻辑相同，高于最大值聚合GNNs，低于求和聚合GNNs。在统一设置下（特定连续性和阈值假设下），平均GNNs相对于MSO的表达能力与无交替模态逻辑相同，且严格低于求和聚合和最大值聚合GNNs。若移除任一假设，表达能力会增加。

Conclusion: 平均聚合GNNs的表达能力受设置（统一/非统一）和函数假设影响，其能力介于最大值和求和聚合GNNs之间，并可由特定模态逻辑精确刻画。

Abstract: We study the expressive power of graph neural networks (GNNs) with mean as
the aggregation function. In the non-uniform setting, we show that such GNNs
have exactly the same expressive power as ratio modal logic, which has modal
operators expressing that at least a certain ratio of the successors of a
vertex satisfies a specified property. The non-uniform expressive power of mean
GNNs is thus higher than that of GNNs with max aggregation, but lower than for
sum aggregation--the latter are characterized by modal logic and graded modal
logic, respectively. In the uniform setting, we show that the expressive power
relative to MSO is exactly that of alternation-free modal logic, under the
natural assumptions that combination functions are continuous and
classification functions are thresholds. This implies that, relative to MSO and
in the uniform setting, mean GNNs are strictly less expressive than sum GNNs
and max GNNs. When any of the assumptions is dropped, the expressive power
increases.

</details>


### [62] [Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory](https://arxiv.org/abs/2507.18178)
*Mutian Yang,Jiandong Gao,Ji Wu*

Main category: cs.AI

TL;DR: 该研究提出一个认知归因框架，借鉴双系统认知理论，将大型语言模型（LLMs）的认知能力解耦为知识和推理，并通过“快思”和“慢思”模式进行量化分析，揭示了知识与推理在模型中的行为和分布。


<details>
  <summary>Details</summary>
Motivation: 在LLMs的推断过程中，区分知识和推理的能力对于模型分析、可解释性和发展至关重要。研究旨在解耦LLMs中知识和推理的贡献。

Method: 提出一个认知归因框架，将LLMs的认知分解为知识检索（阶段1）和推理调整（阶段2）。通过设计“快思”和“慢思”两种认知模式的提示词，分离并量化这两个阶段的贡献。该框架应用于15个LLMs和3个数据集进行分析。

Result: 1. 推理调整具有领域特异性，对推理密集型领域（如数学、物理、化学）有益，但可能损害知识密集型领域。2. 参数扩展能同时提升知识和推理能力，其中知识提升更显著；参数扩展使LLMs的推理更审慎，智力适度提高。3. 知识主要存在于较低网络层，而推理主要在高层运作。

Conclusion: 该框架不仅从“解耦”视角帮助理解LLMs，也为现有研究（如扩展定律、分层知识编辑、小模型推理局限性）提供了新见解。

Abstract: While large language models (LLMs) leverage both knowledge and reasoning
during inference, the capacity to distinguish between them plays a pivotal role
in model analysis, interpretability, and development. Inspired by dual-system
cognitive theory, we propose a cognition attribution framework to decouple the
contribution of knowledge and reasoning. In particular, the cognition of LLMs
is decomposed into two distinct yet complementary phases: knowledge retrieval
(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs
are prompted to generate answers under two different cognitive modes, fast
thinking and slow thinking, respectively. The performance under different
cognitive modes is analyzed to quantify the contribution of knowledge and
reasoning. This architecture is employed to 15 LLMs across 3 datasets. Results
reveal: (1) reasoning adjustment is domain-specific, benefiting
reasoning-intensive domains (e.g., mathematics, physics, and chemistry) and
potentially imparing knowledge-intensive domains. (2) Parameter scaling
improves both knowledge and reasoning, with knowledge improvements being more
pronounced. Additionally, parameter scaling make LLMs reasoning significantly
more prudent, while moderately more intelligent. (3) Knowledge primarily
resides in lower network layers, while reasoning operates in higher layers. Our
framework not only helps understand LLMs from a "decoupling" perspective, but
also provides new insights into existing research, including scaling laws,
hierarchical knowledge editing, and limitations of small-model reasoning.

</details>


### [63] [Comparing Non-minimal Semantics for Disjunction in Answer Set Programming](https://arxiv.org/abs/2507.18198)
*Felicidad Aguado,Pedro Cabalar,Brais Muñiz,Gilberto Pérez,Concepción Vidal*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we compare four different semantics for disjunction in Answer
Set Programming that, unlike stable models, do not adhere to the principle of
model minimality. Two of these approaches, Cabalar and Mu\~niz' \emph{Justified
Models} and Doherty and Szalas' \emph{Strongly Supported Models}, directly
provide an alternative non-minimal semantics for disjunction. The other two,
Aguado et al's \emph{Forks} and Shen and Eiter's \emph{Determining Inference}
(DI) semantics, actually introduce a new disjunction connective, but are
compared here as if they constituted new semantics for the standard disjunction
operator. We are able to prove that three of these approaches (Forks, Justified
Models and a reasonable relaxation of the DI semantics) actually coincide,
constituting a common single approach under different definitions. Moreover,
this common semantics always provides a superset of the stable models of a
program (in fact, modulo any context) and is strictly stronger than the fourth
approach (Strongly Supported Models), that actually treats disjunctions as in
classical logic.

</details>


### [64] [Foundations for Risk Assessment of AI in Protecting Fundamental Rights](https://arxiv.org/abs/2507.18290)
*Antonino Rotolo,Beatrice Ferrigno,Jose Miguel Angel Garcia Godinez,Claudio Novelli,Giovanni Sartor*

Main category: cs.AI

TL;DR: 本文提出了一个用于AI定性风险评估的概念框架，特别是在欧盟AI法案背景下，通过整合定义平衡和可废止推理来处理法律合规性和基本权利保护的复杂性。


<details>
  <summary>Details</summary>
Motivation: 为应对欧盟AI法案背景下AI风险评估中法律合规和基本权利保护的复杂性，并有效识别AI部署场景下的潜在法律违规及其对基本权利的多层次影响。

Method: 引入一个概念框架，整合了“定义平衡”（使用比例原则分析解决冲突权利）和“可废止推理”（适应法律决策的动态性）。该方法强调分析AI部署场景，并提供了AI部署与基本权利互动概念化的基本构成要素。

Result: 为AI风险分析的逻辑描述奠定了哲学基础。此分层方法使得对高风险AI系统和通用AI（GPAI）系统都能建立更具操作性的评估模型，并突出了其广泛适用性。

Conclusion: 该概念框架通过整合既有法律推理方法，为AI定性风险评估提供了坚实的基础，特别适用于欧盟AI法案。它也为未来开发形式化模型和算法以支持负责任的AI治理奠定了理论基础。

Abstract: This chapter introduces a conceptual framework for qualitative risk
assessment of AI, particularly in the context of the EU AI Act. The framework
addresses the complexities of legal compliance and fundamental rights
protection by itegrating definitional balancing and defeasible reasoning.
Definitional balancing employs proportionality analysis to resolve conflicts
between competing rights, while defeasible reasoning accommodates the dynamic
nature of legal decision-making. Our approach stresses the need for an analysis
of AI deployment scenarios and for identifying potential legal violations and
multi-layered impacts on fundamental rights. On the basis of this analysis, we
provide philosophical foundations for a logical account of AI risk analysis. In
particular, we consider the basic building blocks for conceptually grasping the
interaction between AI deployment scenarios and fundamental rights,
incorporating in defeasible reasoning definitional balancing and arguments
about the contextual promotion or demotion of rights. This layered approach
allows for more operative models of assessment of both high-risk AI systems and
General Purpose AI (GPAI) systems, emphasizing the broader applicability of the
latter. Future work aims to develop a formal model and effective algorithms to
enhance AI risk assessment, bridging theoretical insights with practical
applications to support responsible AI governance.

</details>


### [65] [The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions in Physics Exams](https://arxiv.org/abs/2507.18337)
*Peter Baumgartner,Lachlan McGinness*

Main category: cs.AI

TL;DR: 开发并评估了一种结合大语言模型和符号推理技术自动批改物理考试的方法。


<details>
  <summary>Details</summary>
Motivation: 自动批改学生物理考试中打字答案的正确性是一个具有挑战性的问题。

Method: 结合计算机代数系统、SMT求解器和项重写系统。利用大型语言模型（LLM）解释、纠正学生答案错误并转换为机器可读格式。随后应用自动化推理技术（包括SMT求解和为物理问题定制的项重写系统）评估学生答案的正确性。文中详细描述了项重写系统的开发及其终止性和合流性属性的建立。

Result: 该系统在2023年澳大利亚物理奥林匹克竞赛中收集的1500多份真实学生答卷上进行了评估。

Conclusion: 论文提出了一种应对物理考试自动批改挑战的混合方法，并成功地在真实数据集上进行了系统评估，展现了其解决复杂物理问题判卷的潜力。

Abstract: We present our method for automatically marking Physics exams. The marking
problem consists in assessing typed student answers for correctness with
respect to a ground truth solution. This is a challenging problem that we seek
to tackle using a combination of a computer algebra system, an SMT solver and a
term rewriting system. A Large Language Model is used to interpret and remove
errors from student responses and rewrite these in a machine readable format.
Once formalized and language-aligned, the next step then consists in applying
automated reasoning techniques for assessing student solution correctness. We
consider two methods of automated theorem proving: off-the-shelf SMT solving
and term rewriting systems tailored for physics problems involving
trigonometric expressions. The development of the term rewrite system and
establishing termination and confluence properties was not trivial, and we
describe it in some detail in the paper. We evaluate our system on a rich pool
of over 1500 real-world student exam responses from the 2023 Australian Physics
Olympiad.

</details>


### [66] [Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios](https://arxiv.org/abs/2507.18368)
*Zhuang Qiang Bok,Watson Wei Khong Chua*

Main category: cs.AI

TL;DR: 引入ConDiFi，一个评估大语言模型在金融任务中发散性与收敛性思维的新基准。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理基准侧重事实准确性或逻辑步骤，但金融专业人士需在不确定性下同时具备最优决策（收敛性）和生成创意未来（发散性）的能力。因此，需要一个能综合评估这两种思维的基准。

Method: 研究引入了ConDiFi基准，包含607个用于发散性推理的宏观金融提示和990个用于收敛性推理的多跳对抗性选择题。使用此基准评估了14个主流大语言模型。

Result: 评估发现模型之间存在显著差异。尽管GPT-4o流畅性高，但在新颖性和可操作性方面表现不佳。而DeepSeek-R1和Cohere Command R+等模型在生成可操作、适合投资决策的洞察方面表现突出。

Conclusion: ConDiFi为评估大语言模型在金融领域安全和战略部署所需的推理能力提供了新视角。

Abstract: Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step
logic. In finance, however, professionals must not only converge on optimal
decisions but also generate creative, plausible futures under uncertainty. We
introduce ConDiFi, a benchmark that jointly evaluates divergent and convergent
thinking in LLMs for financial tasks.
  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990
multi-hop adversarial MCQs for convergent reasoning. Using this benchmark, we
evaluated 14 leading models and uncovered striking differences. Despite high
fluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models
like DeepSeek-R1 and Cohere Command R+ rank among the top for generating
actionable, insights suitable for investment decisions. ConDiFi provides a new
perspective to assess reasoning capabilities essential to safe and strategic
deployment of LLMs in finance.

</details>


### [67] [Revisiting LLM Reasoning via Information Bottleneck](https://arxiv.org/abs/2507.18391)
*Shiye Lei,Zhihao Cheng,Kai Jia,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出基于信息瓶颈（IB）原理的IB-aware推理优化（IBRO）框架及轻量级IB正则化方法，旨在提升大语言模型（LLM）推理轨迹的信息量和泛化能力，并在实验中验证了其在数学推理任务上对LLM性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有通过强化学习（RL）提升大语言模型（LLM）推理能力的方法（如RLVR）多为启发式和直觉驱动，缺乏坚实的理论基础和系统性方法论，限制了其进一步发展。

Method: 1. 基于信息瓶颈（IB）原理对LLM推理进行了理论表征。2. 提出了信息瓶颈感知推理优化（IBRO）框架，旨在使推理轨迹对最终答案更具信息量并能更好泛化。3. 推导了实用的token级替代目标，并设计了高效近似方法，形成了轻量级IB正则化。4. 该方法能无缝集成到现有RL后训练框架中，计算开销低，仅需一行代码修改。

Result: 在多个数学推理基准和不同的强化学习算法上进行了实证验证，结果一致表明IB正则化显著提升了LLM的推理性能。

Conclusion: IB正则化方法为LLM推理优化提供了理论上更严谨且实践上高效的途径，成功解决了现有方法缺乏原则性的问题，并通过实验验证了其在提升LLM推理能力方面的有效性。

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in reasoning capabilities through reinforcement learning with verifiable
rewards (RLVR). By leveraging simple rule-based rewards, RL effectively
incentivizes LLMs to produce extended chain-of-thought (CoT) reasoning
trajectories, progressively guiding them toward correct answers. However,
existing approaches remain largely heuristic and intuition-driven, limiting the
development of principled methodologies. In this paper, we present a
theoretical characterization of LLM reasoning grounded in information
bottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),
a framework that encourages reasoning trajectories to be both informative about
the final correct answer and generalizable across diverse prompts. We derive a
practical token-level surrogate objective and propose an efficient
approximation, resulting in the lightweight IB regularization method. This
technique integrates seamlessly into existing RL-based post-training frameworks
without additional computational overhead, requiring only a one-line code
modification. Empirically, we validate IB regularization across multiple
mathematical reasoning benchmarks and RL algorithms, demonstrating consistent
improvements in LLM reasoning performance.

</details>


### [68] [Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation](https://arxiv.org/abs/2507.18398)
*Kwong Ho Li,Wathsala Karunarathne*

Main category: cs.AI

TL;DR: 本研究探讨强化学习在呼叫中心路由优化中的应用，旨在缩短客户等待时间并减少员工空闲时间。对比模型基（VI）与无模型（PPO）方法，结果显示PPO表现最佳，有效降低等待和空闲时间，尽管训练耗时更长。


<details>
  <summary>Details</summary>
Motivation: 优化呼叫中心的呼叫路由，以最大程度地减少客户等待时间并降低员工空闲时间。

Method: 采用强化学习（RL）框架，将问题建模为马尔可夫决策过程（MDP）。比较两种方法：1) 基于模型的价值迭代（VI），使用理论模型；2) 无模型的近端策略优化（PPO），通过结合离散事件仿真（DES）与OpenAI Gym的仿真模型从经验中学习。对随机、VI和PPO策略进行评估。

Result: 经过1000次测试，PPO策略持续获得最高奖励，同时实现最低的客户等待时间和员工空闲时间，尽管其训练时间较长。

Conclusion: PPO是优化呼叫中心呼叫路由的有效方法，能够显著减少客户等待和员工空闲时间，表现优于传统的VI方法，证明其在复杂动态环境中的实用性。

Abstract: This paper investigates the application of Reinforcement Learning (RL) to
optimise call routing in call centres to minimise client waiting time and staff
idle time. Two methods are compared: a model-based approach using Value
Iteration (VI) under known system dynamics, and a model-free approach using
Proximal Policy Optimisation (PPO) that learns from experience. For the
model-based approach, a theoretical model is used, while a simulation model
combining Discrete Event Simulation (DES) with the OpenAI Gym environment is
developed for model-free learning. Both models frame the problem as a Markov
Decision Process (MDP) within a Skills-Based Routing (SBR) framework, with
Poisson client arrivals and exponentially distributed service and abandonment
times. For policy evaluation, random, VI, and PPO policies are evaluated using
the simulation model. After 1,000 test episodes, PPO consistently achives the
highest rewards, along with the lowest client waiting time and staff idle time,
despite requiring longer training time.

</details>


### [69] [GPU Accelerated Compact-Table Propagation](https://arxiv.org/abs/2507.18413)
*Enrico Santi,Fabio Tardivo,Agostino Dovier,Andrea Formisano*

Main category: cs.AI

TL;DR: 本文提出利用GPU加速Compact-Table (CT)算法，以有效处理大规模表格约束，克服了传统CPU方法的效率瓶颈。


<details>
  <summary>Details</summary>
Motivation: 尽管表格约束具有普适性且高效的传播算法已问世，但在实际问题中，包含数百或数千个有效案例的大规模表格约束仍难以用标准CPU方法有效处理。

Method: 作者通过利用现代GPU的强大计算能力，增强了当前最先进的Compact-Table (CT)传播算法。具体方法包括设计和实现GPU加速的CT算法，并将其集成到现有的约束求解器中。

Result: 论文详细介绍了GPU加速CT的设计、实现及其与现有约束求解器的集成。通过对大量实例的实验验证，证明了该方法在处理大规模表格约束方面的有效性。

Conclusion: 利用GPU加速Compact-Table算法能够有效应对大规模表格约束的挑战，使得处理现实世界中过去难以解决的复杂问题变得可行。

Abstract: Constraint Programming developed within Logic Programming in the Eighties;
nowadays all Prolog systems encompass modules capable of handling constraint
programming on finite domains demanding their solution to a constraint solver.
This work focuses on a specific form of constraint, the so-called table
constraint, used to specify conditions on the values of variables as an
enumeration of alternative options. Since every condition on a set of finite
domain variables can be ultimately expressed as a finite set of cases, Table
can, in principle, simulate any other constraint. These characteristics make
Table one of the most studied constraints ever, leading to a series of
increasingly efficient propagation algorithms. Despite this, it is not uncommon
to encounter real-world problems with hundreds or thousands of valid cases that
are simply too many to be handled effectively with standard CPU-based
approaches. In this paper, we deal with the Compact-Table (CT) algorithm, the
state-of-the-art propagation algorithms for Table. We describe how CT can be
enhanced by exploiting the massive computational power offered by modern GPUs
to handle large Table constraints. In particular, we report on the design and
implementation of GPU-accelerated CT, on its integration into an existing
constraint solver, and on an experimental validation performed on a significant
set of instances.

</details>


### [70] [On the Performance of Concept Probing: The Influence of the Data (Extended Version)](https://arxiv.org/abs/2507.18550)
*Manuel de Sousa Ribeiro,Afonso Leote,João Leite*

Main category: cs.AI

TL;DR: 论文探究了训练数据对概念探测模型性能的影响，并公开了两个常用数据集的概念标签。


<details>
  <summary>Details</summary>
Motivation: 当前概念探测研究主要关注被探测模型或探测模型本身，对训练探测模型所需的数据关注不足，而神经网络的复杂性使其难以直接解释。本研究旨在弥补这一空白。

Method: 在图像分类任务的背景下，研究用于训练概念探测模型的数据对其性能的影响。

Result: 调查了训练数据对概念探测模型性能的影响。提供了两个广泛使用数据集的概念标签。

Conclusion: 本文通过关注训练数据在概念探测中的作用，增强了对模型解释性的理解，并为相关研究提供了实用资源。

Abstract: Concept probing has recently garnered increasing interest as a way to help
interpret artificial neural networks, dealing both with their typically large
size and their subsymbolic nature, which ultimately renders them unfeasible for
direct human interpretation. Concept probing works by training additional
classifiers to map the internal representations of a model into human-defined
concepts of interest, thus allowing humans to peek inside artificial neural
networks. Research on concept probing has mainly focused on the model being
probed or the probing model itself, paying limited attention to the data
required to train such probing models. In this paper, we address this gap.
Focusing on concept probing in the context of image classification tasks, we
investigate the effect of the data used to train probing models on their
performance. We also make available concept labels for two widely used
datasets.

</details>


### [71] [SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\circ}$ Law](https://arxiv.org/abs/2507.18576)
*Shanghai AI Lab,:,Yicheng Bao,Guanxu Chen,Mingkang Chen,Yunhao Chen,Chiyu Chen,Lingjie Chen,Sirui Chen,Xinquan Chen,Jie Cheng,Yu Cheng,Dengke Deng,Yizhuo Ding,Dan Ding,Xiaoshan Ding,Yi Ding,Zhichen Dong,Lingxiao Du,Yuyu Fan,Xinshun Feng,Yanwei Fu,Yuxuan Gao,Ruijun Ge,Tianle Gu,Lujun Gui,Jiaxuan Guo,Qianxi He,Yuenan Hou,Xuhao Hu,Hong Huang,Kaichen Huang,Shiyang Huang,Yuxian Jiang,Shanzhe Lei,Jie Li,Lijun Li,Hao Li,Juncheng Li,Xiangtian Li,Yafu Li,Lingyu Li,Xueyan Li,Haotian Liang,Dongrui Liu,Qihua Liu,Zhixuan Liu,Bangwei Liu,Huacan Liu,Yuexiao Liu,Zongkai Liu,Chaochao Lu,Yudong Lu,Xiaoya Lu,Zhenghao Lu,Qitan Lv,Caoyuan Ma,Jiachen Ma,Xiaoya Ma,Zhongtian Ma,Lingyu Meng,Ziqi Miao,Yazhe Niu,Yuezhang Peng,Yuan Pu,Han Qi,Chen Qian,Xingge Qiao,Jingjing Qu,Jiashu Qu,Wanying Qu,Wenwen Qu,Xiaoye Qu,Qihan Ren,Qingnan Ren,Qingyu Ren,Jing Shao,Wenqi Shao,Shuai Shao,Dongxing Shi,Xin Song,Xinhao Song,Yan Teng,Xuan Tong,Yingchun Wang,Xuhong Wang,Shujie Wang,Xin Wang,Yige Wang,Yixu Wang,Yuanfu Wang,Futing Wang,Ruofan Wang,Wenjie Wang,Yajie Wang,Muhao Wei,Xiaoyu Wen,Fenghua Weng,Yuqi Wu,Yingtong Xiong,Xingcheng Xu,Chao Yang,Yue Yang,Yang Yao,Yulei Ye,Zhenyun Yin,Yi Yu,Bo Zhang,Qiaosheng Zhang,Jinxuan Zhang,Yexin Zhang,Yinqiang Zheng,Hefeng Zhou,Zhanhui Zhou,Pengyu Zhu,Qingzi Zhu,Yubo Zhu,Bowen Zhou*

Main category: cs.AI

TL;DR: 本文提出了SafeWork-R1，一个通过SafeLadder框架开发的多模态推理模型，该框架采用以安全为导向的强化学习，实现了能力与安全性的协同演进，并在安全基准测试中表现出领先水平。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐方法（如RLHF）仅学习人类偏好，缺乏模型内在的安全推理和自我反思能力。研究旨在开发能内生安全推理并实现安全与能力协同发展的通用AI模型。

Method: 引入SafeLadder框架，该框架包含大规模、渐进式、面向安全的强化学习后训练，并辅以多原则验证器。此框架使模型具备内在的安全推理和自我反思能力。此外，还实施了两种推理时干预方法和一种审慎搜索机制，以进行步骤级验证。该框架已成功应用于Qwen2.5-VL-72B、InternVL3-78B、DeepSeek-70B和Qwen2.5VL-7B等多种基础模型。

Result: SafeWork-R1在安全相关基准测试中，相较于其基础模型Qwen2.5-VL-72B，平均提升了46.54%，同时未损害通用能力。与GPT-4.1和Claude Opus 4等领先专有模型相比，其安全性能达到了最先进水平。多个由此框架衍生的模型（如SafeWork-R1-InternVL3-78B）均证明了安全和能力可以协同演进。

Conclusion: SafeLadder框架成功实现了安全与能力的协同演进，赋能模型以内在的安全推理和自我反思能力。该框架具有良好的通用性，可用于构建鲁棒、可靠、值得信赖的通用AI模型。

Abstract: We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that
demonstrates the coevolution of capabilities and safety. It is developed by our
proposed SafeLadder framework, which incorporates large-scale, progressive,
safety-oriented reinforcement learning post-training, supported by a suite of
multi-principled verifiers. Unlike previous alignment methods such as RLHF that
simply learn human preferences, SafeLadder enables SafeWork-R1 to develop
intrinsic safety reasoning and self-reflection abilities, giving rise to safety
`aha' moments. Notably, SafeWork-R1 achieves an average improvement of
$46.54\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks
without compromising general capabilities, and delivers state-of-the-art safety
performance compared to leading proprietary models such as GPT-4.1 and Claude
Opus 4. To further bolster its reliability, we implement two distinct
inference-time intervention methods and a deliberative search mechanism,
enforcing step-level verification. Finally, we further develop
SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and
SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and
capability can co-evolve synergistically, highlighting the generalizability of
our framework in building robust, reliable, and trustworthy general-purpose AI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [72] [Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction](https://arxiv.org/abs/2507.17768)
*Yujia Tong,Jingling Yuan,Chuang Hu*

Main category: cs.LG

TL;DR: QuaRC框架通过相对熵得分选择核心数据集和级联层校正策略，解决了边缘设备上小数据集量化感知训练中的量化误差大、性能下降问题，显著提升了低比特模型的精度。


<details>
  <summary>Details</summary>
Motivation: 随着移动和边缘计算发展，边缘设备对低比特量化模型的需求增加，通常需要使用边缘数据重训练以提升性能。然而，隐私问题导致敏感数据只能在边缘设备处理，使得在边缘设备上进行量化感知训练（QAT）成为有效方案。传统QAT依赖完整数据集，计算成本高。虽可使用核心数据集（Coreset）选择技术缓解，但现有方法在小规模数据集上（如仅10%数据）难以消除量化误差，导致显著的性能下降。

Method: 本文提出QuaRC，一个在边缘设备上基于核心数据集的QAT框架，包含两个主要阶段：
1.  **核心数据集选择阶段**：引入“相对熵得分”（Relative Entropy Score）来识别最能捕捉模型量化误差的子集。
2.  **训练阶段**：采用“级联层校正”（Cascaded Layer Correction）策略，将量化模型的中间层输出与全精度模型的中间层输出对齐，从而有效减少中间层中的量化误差。

Result: 实验结果证明了该方法的有效性。例如，在ImageNet-1K数据集上，当ResNet-18量化到2比特并使用1%数据子集时，QuaRC相比现有最先进技术，在Top-1准确率上提升了5.72%。

Conclusion: QuaRC框架通过创新的核心数据集选择和层校正策略，有效解决了边缘设备上小规模数据集进行QAT时存在的量化误差大和性能下降问题，显著提升了量化模型的精度，使其更适用于资源受限的边缘部署。

Abstract: With the development of mobile and edge computing, the demand for low-bit
quantized models on edge devices is increasing to achieve efficient deployment.
To enhance the performance, it is often necessary to retrain the quantized
models using edge data. However, due to privacy concerns, certain sensitive
data can only be processed on edge devices. Therefore, employing
Quantization-Aware Training (QAT) on edge devices has become an effective
solution. Nevertheless, traditional QAT relies on the complete dataset for
training, which incurs a huge computational cost. Coreset selection techniques
can mitigate this issue by training on the most representative subsets.
However, existing methods struggle to eliminate quantization errors in the
model when using small-scale datasets (e.g., only 10% of the data), leading to
significant performance degradation. To address these issues, we propose QuaRC,
a QAT framework with coresets on edge devices, which consists of two main
phases: In the coreset selection phase, QuaRC introduces the ``Relative Entropy
Score" to identify the subsets that most effectively capture the model's
quantization errors. During the training phase, QuaRC employs the Cascaded
Layer Correction strategy to align the intermediate layer outputs of the
quantized model with those of the full-precision model, thereby effectively
reducing the quantization errors in the intermediate layers. Experimental
results demonstrate the effectiveness of our approach. For instance, when
quantizing ResNet-18 to 2-bit using a 1% data subset, QuaRC achieves a 5.72%
improvement in Top-1 accuracy on the ImageNet-1K dataset compared to
state-of-the-art techniques.

</details>


### [73] [Knowledge Abstraction for Knowledge-based Semantic Communication: A Generative Causality Invariant Approach](https://arxiv.org/abs/2507.17784)
*Minh-Duong Nguyen,Quoc-Viet Pham,Nguyen H. Tran,Hoang-Khoi Do,Duy T. Ngo,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出一种基于因果不变学习的低复杂度AI模型，通过稀疏更新改善语义通信中的数据重建，并在多样化数据下实现一致性，性能优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 旨在通过捕获通用知识，改进语义通信中信道解码器的数据重建能力。

Method: 提出一种结合因果不变学习的生成对抗网络（GAN），用于提取数据的因果和非因果表示。因果表示包含关键语义知识，能促进接收端的数据重建，并确保在不同领域数据下的一致性。同时，设计了稀疏更新协议，以应对用户数据演变导致的知识分歧并最小化通信开销。

Result: 1. 因果不变知识确保了在不同设备和多样化训练数据下的一致性。2. 不变知识在分类任务中表现出色，对面向目标的语义通信至关重要。3. 基于知识的数据重建方法显示了其解码器的鲁棒性，并在峰值信噪比（PSNR）方面超越了其他最先进的数据重建和语义压缩方法。

Conclusion: 本研究提出的基于因果不变学习并结合稀疏更新的AI模型，有效提升了语义通信的数据重建能力，实现了跨域数据的一致性，性能优于现有方法，证明了其在用户数据演变场景下的可靠性。

Abstract: In this study, we design a low-complexity and generalized AI model that can
capture common knowledge to improve data reconstruction of the channel decoder
for semantic communication. Specifically, we propose a generative adversarial
network that leverages causality-invariant learning to extract causal and
non-causal representations from the data. Causal representations are invariant
and encompass crucial information to identify the data's label. They can
encapsulate semantic knowledge and facilitate effective data reconstruction at
the receiver. Moreover, the causal mechanism ensures that learned
representations remain consistent across different domains, making the system
reliable even with users collecting data from diverse domains. As
user-collected data evolves over time causing knowledge divergence among users,
we design sparse update protocols to improve the invariant properties of the
knowledge while minimizing communication overheads. Three key observations were
drawn from our empirical evaluations. Firstly, causality-invariant knowledge
ensures consistency across different devices despite the diverse training data.
Secondly, invariant knowledge has promising performance in classification
tasks, which is pivotal for goal-oriented semantic communications. Thirdly, our
knowledge-based data reconstruction highlights the robustness of our decoder,
which surpasses other state-of-the-art data reconstruction and semantic
compression methods in terms of Peak Signal-to-Noise Ratio (PSNR).

</details>


### [74] [Self-similarity Analysis in Deep Neural Networks](https://arxiv.org/abs/2507.17785)
*Jingyi Ding,Chengwen Qi,Hongfei Wang,Jianshe Wu,Licheng Jiao,Yuwei Guo,Jian Gao*

Main category: cs.LG

TL;DR: 本文通过复杂网络建模方法分析了深度神经网络特征表示的自相似性，并证明在训练中限制自相似性可提升MLP和注意力架构的分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能定量分析隐藏空间几何自相似性如何影响模型权重优化，也缺乏对内部神经元动态行为的清晰理解。

Method: 提出一种基于隐藏层神经元输出特征的复杂网络建模方法，用于研究不同隐藏层特征网络的自相似性，并分析调整其自相似性程度对深度神经网络分类性能的影响。

Result: 研究显示，特征网络的自相似性程度在不同模型架构中存在差异。在训练过程中对特征网络的自相似性施加约束，可使自相似的深度神经网络（MLP和注意力架构）的性能提升高达6个百分点。

Conclusion: 通过复杂网络建模和训练过程中对特征网络自相似性的约束，可以有效提升特定深度神经网络（如MLP和注意力架构）的分类性能。

Abstract: Current research has found that some deep neural networks exhibit strong
hierarchical self-similarity in feature representation or parameter
distribution. However, aside from preliminary studies on how the power-law
distribution of weights across different training stages affects model
performance,there has been no quantitative analysis on how the self-similarity
of hidden space geometry influences model weight optimization, nor is there a
clear understanding of the dynamic behavior of internal neurons. Therefore,
this paper proposes a complex network modeling method based on the output
features of hidden-layer neurons to investigate the self-similarity of feature
networks constructed at different hidden layers, and analyzes how adjusting the
degree of self-similarity in feature networks can enhance the classification
performance of deep neural networks. Validated on three types of networks MLP
architectures, convolutional networks, and attention architectures this study
reveals that the degree of self-similarity exhibited by feature networks varies
across different model architectures. Furthermore, embedding constraints on the
self-similarity of feature networks during the training process can improve the
performance of self-similar deep neural networks (MLP architectures and
attention architectures) by up to 6 percentage points.

</details>


### [75] [Reinforcement Learning for Accelerated Aerodynamic Shape Optimisation](https://arxiv.org/abs/2507.17786)
*Florian Sobieczky,Alfredo Lopez,Erika Dudkin,Christopher Lackner,Matthias Hochsteger,Bernhard Scheichl,Helmut Sobieczky*

Main category: cs.LG

TL;DR: 本文提出一种基于强化学习（RL）的自适应优化算法，用于气动外形优化，重点在于降维，旨在降低计算成本并解释优化结果。


<details>
  <summary>Details</summary>
Motivation: 主要研究动机是最小化气动外形优化中的计算工作量，并能够解释优化结果中发现的极值，以理解它们在实现所需流场中的作用。

Method: 该方法是一种基于代理模型（surrogate-based）、演员-评论家（actor-critic）策略评估的马尔可夫链蒙特卡罗（MCMC）方法，它允许对部分待优化参数进行“时间冻结”。通过围绕作为“真实值”的中间CFD模拟进行一系列局部优化的参数更改，以加速全局优化。

Result: 该方法在一个简单的流体动力学问题上进行了示例验证。结果表明，如果在参数局部邻域足够大且奖惩估计足够准确的情况下，该方法可以加速全局优化。

Conclusion: 所提出的基于RL的自适应优化方法不仅有助于减少计算量，更重要的是，它能够对优化结果进行解释，提供类似于特征重要性评分的洞察。

Abstract: We introduce a reinforcement learning (RL) based adaptive optimization
algorithm for aerodynamic shape optimization focused on dimensionality
reduction. The form in which RL is applied here is that of a surrogate-based,
actor-critic policy evaluation MCMC approach allowing for temporal 'freezing'
of some of the parameters to be optimized. The goals are to minimize
computational effort, and to use the observed optimization results for
interpretation of the discovered extrema in terms of their role in achieving
the desired flow-field.
  By a sequence of local optimized parameter changes around intermediate CFD
simulations acting as ground truth, it is possible to speed up the global
optimization if (a) the local neighbourhoods of the parameters in which the
changed parameters must reside are sufficiently large to compete with the
grid-sized steps and its large number of simulations, and (b) the estimates of
the rewards and costs on these neighbourhoods necessary for a good step-wise
parameter adaption are sufficiently accurate. We give an example of a simple
fluid-dynamical problem on which the method allows interpretation in the sense
of a feature importance scoring.

</details>


### [76] [Hyperbolic Deep Learning for Foundation Models: A Survey](https://arxiv.org/abs/2507.17787)
*Neil He,Hiren Madhu,Ngoc Bui,Menglin Yang,Rex Ying*

Main category: cs.LG

TL;DR: 基础模型在欧几里得几何下存在局限，双曲空间作为一种非欧几何提供了一种解决方案。它能高效嵌入层级和幂律结构，从而提升LLMs的推理能力、VLMs的泛化能力及跨模态对齐，同时保持参数效率。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练的基础模型（如LLMs、VLMs）在欧几里得几何下表现出表示能力有限、适应性差和可扩展性下降等根本性局限。这引发了对替代几何空间是否能更好地匹配真实世界数据内在结构并改善推理过程的探讨。

Method: 引入双曲空间，这是一种非欧几何流形，其特点是体积随距离呈指数增长。利用此特性，双曲空间能够以更低维度、更低的失真度嵌入层级结构（如树、分类法）和幂律分布。

Result: 通过将双曲空间应用于基础模型，已成功提升了LLMs的复杂推理能力、VLMs的零样本泛化能力以及跨模态语义对齐，并保持了参数效率。

Conclusion: 本论文全面综述了双曲神经网络及其在基础模型中的最新发展，并进一步提出了该领域面临的关键挑战和未来的研究方向。

Abstract: Foundation models pre-trained on massive datasets, including large language
models (LLMs), vision-language models (VLMs), and large multimodal models, have
demonstrated remarkable success in diverse downstream tasks. However, recent
studies have shown fundamental limitations of these models: (1) limited
representational capacity, (2) lower adaptability, and (3) diminishing
scalability. These shortcomings raise a critical question: is Euclidean
geometry truly the optimal inductive bias for all foundation models, or could
incorporating alternative geometric spaces enable models to better align with
the intrinsic structure of real-world data and improve reasoning processes?
Hyperbolic spaces, a class of non-Euclidean manifolds characterized by
exponential volume growth with respect to distance, offer a mathematically
grounded solution. These spaces enable low-distortion embeddings of
hierarchical structures (e.g., trees, taxonomies) and power-law distributions
with substantially fewer dimensions compared to Euclidean counterparts. Recent
advances have leveraged these properties to enhance foundation models,
including improving LLMs' complex reasoning ability, VLMs' zero-shot
generalization, and cross-modal semantic alignment, while maintaining parameter
efficiency. This paper provides a comprehensive review of hyperbolic neural
networks and their recent development for foundation models. We further outline
key challenges and research directions to advance the field.

</details>


### [77] [Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking](https://arxiv.org/abs/2507.17788)
*Ali Vardasbi,Gustavo Penha,Claudia Hauff,Hugues Bouchard*

Main category: cs.LG

TL;DR: LLMs存在位置偏差和低重复一致性，传统的重复调用方法计算成本高。本文提出一种动态早停法，根据每个实例自适应调整重复次数，显著减少LLM调用次数，同时保持或略微牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在对项目进行排序或评估时，存在位置偏差（候选项目顺序影响决策）和低重复一致性（相同输入多次调用结果不同）。传统的缓解方法是多次重复调用并聚合结果，但这会显著增加计算成本。研究发现位置偏差的方向和程度因实例而异，这需要一种针对每个实例的缓解策略。

Method: 本文提出一种动态早停法，用于自适应地确定每个实例所需的LLM重复调用次数。在此基础上，进一步提出了一种基于置信度的早停法变体。

Result: 在三种不同规模的LLMs和两种任务（重排序和对齐）上进行评估，结果显示动态早停法平均减少81%的LLM调用次数，同时保持准确性。基于置信度的改进方法平均减少87%的LLM调用次数（相对于静态重复），仅略微牺牲准确性（相对于原始动态早停法）。

Conclusion: 通过采用动态自适应的重复调用策略，可以在保持或略微牺牲准确性的前提下，显著降低LLM在缓解位置偏差和提高一致性时的计算成本。

Abstract: When using LLMs to rank items based on given criteria, or evaluate answers,
the order of candidate items can influence the model's final decision. This
sensitivity to item positioning in a LLM's prompt is known as position bias.
Prior research shows that this bias exists even in large models, though its
severity varies across models and tasks. In addition to position bias, LLMs
also exhibit varying degrees of low repetition consistency, where repeating the
LLM call with the same candidate ordering can lead to different rankings. To
address both inconsistencies, a common approach is to prompt the model multiple
times with different candidate orderings and aggregate the results via majority
voting. However, this repetition strategy, significantly increases
computational costs. Extending prior findings, we observe that both the
direction -- favoring either the earlier or later candidate in the prompt --
and magnitude of position bias across instances vary substantially, even within
a single dataset. This observation highlights the need for a per-instance
mitigation strategy. To this end, we introduce a dynamic early-stopping method
that adaptively determines the number of repetitions required for each
instance. Evaluating our approach across three LLMs of varying sizes and on two
tasks, namely re-ranking and alignment, we demonstrate that transitioning to a
dynamic repetition strategy reduces the number of LLM calls by an average of
81%, while preserving the accuracy. Furthermore, we propose a confidence-based
adaptation to our early-stopping method, reducing LLM calls by an average of
87% compared to static repetition, with only a slight accuracy trade-off
relative to our original early-stopping method.

</details>


### [78] [Helix 1.0: An Open-Source Framework for Reproducible and Interpretable Machine Learning on Tabular Scientific Data](https://arxiv.org/abs/2507.17791)
*Eduardo Aguilar-Bejarano,Daniel Lea,Karthikeyan Sivakumar,Jimiama M. Mase,Reza Omidvar,Ruizhe Li,Troy Kettle,James Mitchell-White,Morgan R Alexander,David A Winkler,Grazziela Figueredo*

Main category: cs.LG

TL;DR: Helix是一个开源、可扩展的Python框架，旨在为表格数据提供可复现、可解释的机器学习工作流。


<details>
  <summary>Details</summary>
Motivation: 解决对透明实验数据分析溯源日益增长的需求，确保整个分析过程（包括数据转换和方法选择）被记录、可访问、可复现和可理解。此外，它旨在赋能没有正式数据科学培训的研究人员，帮助他们获取有意义且可操作的洞察。

Method: 该平台包含用于标准化数据预处理、可视化、机器学习模型训练、评估、解释、结果检查以及对未知数据进行模型预测的模块。它提供一个用户友好的界面，支持计算实验设计、结果检查，并引入了一种使用语言术语解释机器学习决策的新方法。

Result: Helix实现了机器学习工作流的可复现性和可解释性，确保了实验数据分析的透明溯源性，使整个分析过程更易于理解、访问和复现。它帮助没有数据科学背景的研究人员也能从数据中获取有价值的洞察。

Conclusion: Helix是一个重要的开源工具，通过提供全面的功能和用户友好的界面，极大地促进了表格数据机器学习研究的透明度、可复现性和可解释性，降低了使用门槛，并支持社区驱动的开发和FAIR原则。

Abstract: Helix is an open-source, extensible, Python-based software framework to
facilitate reproducible and interpretable machine learning workflows for
tabular data. It addresses the growing need for transparent experimental data
analytics provenance, ensuring that the entire analytical process -- including
decisions around data transformation and methodological choices -- is
documented, accessible, reproducible, and comprehensible to relevant
stakeholders. The platform comprises modules for standardised data
preprocessing, visualisation, machine learning model training, evaluation,
interpretation, results inspection, and model prediction for unseen data. To
further empower researchers without formal training in data science to derive
meaningful and actionable insights, Helix features a user-friendly interface
that enables the design of computational experiments, inspection of outcomes,
including a novel interpretation approach to machine learning decisions using
linguistic terms all within an integrated environment. Released under the MIT
licence, Helix is accessible via GitHub and PyPI, supporting community-driven
development and promoting adherence to the FAIR principles.

</details>


### [79] [Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple Domains](https://arxiv.org/abs/2507.17792)
*Jingyi Yu,Tim Pychynski,Marco F. Huber*

Main category: cs.LG

TL;DR: CICME是一种新颖的三步法，利用因果迁移学习从异构多域数据中推断因果机制，旨在深入理解复杂传感器系统，并在特定场景下优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 通过因果关系对复杂传感器系统获得更深层次的洞察，尤其是在处理来自多个领域的异构数据时。

Method: 提出了CICME（通用和个体因果机制估计）方法，这是一个三步法。该方法利用因果迁移学习（CTL）原理，首先可靠地检测领域不变的通用因果机制。随后，这些已识别的通用机制被用来指导每个领域中剩余个体因果机制的估计。该方法基于现有的连续优化因果发现方法。

Result: CICME在基于制造过程场景的线性高斯模型上进行了评估。结果表明，CICME融合了在汇总数据和单个领域数据上应用因果发现的优势，并且在某些特定场景下，其性能甚至优于两种基线方法。

Conclusion: CICME是一种有效且性能优越的因果发现方法，能够从异构多域数据中识别通用和个体因果机制，为理解复杂系统提供了新的视角和工具。

Abstract: To gain deeper insights into a complex sensor system through the lens of
causality, we present common and individual causal mechanism estimation
(CICME), a novel three-step approach to inferring causal mechanisms from
heterogeneous data collected across multiple domains. By leveraging the
principle of Causal Transfer Learning (CTL), CICME is able to reliably detect
domain-invariant causal mechanisms when provided with sufficient samples. The
identified common causal mechanisms are further used to guide the estimation of
the remaining causal mechanisms in each domain individually. The performance of
CICME is evaluated on linear Gaussian models under scenarios inspired from a
manufacturing process. Building upon existing continuous optimization-based
causal discovery methods, we show that CICME leverages the benefits of applying
causal discovery on the pooled data and repeatedly on data from individual
domains, and it even outperforms both baseline methods under certain scenarios.

</details>


### [80] [LSDM: LLM-Enhanced Spatio-temporal Diffusion Model for Service-Level Mobile Traffic Prediction](https://arxiv.org/abs/2507.17795)
*Shiyuan Zhang,Tong Li,Zhu Xiao,Hongyang Du,Kaibin Huang*

Main category: cs.LG

TL;DR: 为提升网络效率和QoS，针对现有移动流量预测方法在不确定性、环境上下文和复杂服务依赖方面的不足，提出LSDM模型。该模型结合扩散模型和LLM，有效提升了个性化服务级流量预测的准确性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有针对个人用户的服务级移动流量预测方法，因个人流量模式不确定性高、缺乏详细环境上下文及服务间复杂依赖，导致在不同城市环境下适应性差且预测结果不准确。

Method: 提出LLM增强时空扩散模型（LSDM）。该模型结合了扩散模型的生成能力、Transformer的自适应学习能力，并利用LLM捕获多模态环境信息，以建模服务级流量模式和动态。

Result: 在真实世界服务级数据集上的评估表明，LSDM在流量使用预测方面表现出色，具有卓越的泛化性和适应性。通过整合LLM上下文信息，模型的决定系数性能提升至少2.83%；与CSDI等同类模型相比，均方根误差至少降低8.29%。

Conclusion: LSDM模型通过结合扩散模型和LLM，成功解决了现有服务级移动流量预测的挑战，显著提升了预测的准确性、泛化能力和环境适应性，对网络效率和QoS提升具有重要意义。

Abstract: Service-level mobile traffic prediction for individual users is essential for
network efficiency and quality of service enhancement. However, current
prediction methods are limited in their adaptability across different urban
environments and produce inaccurate results due to the high uncertainty in
personal traffic patterns, the lack of detailed environmental context, and the
complex dependencies among different network services. These challenges demand
advanced modeling techniques that can capture dynamic traffic distributions and
rich environmental features. Inspired by the recent success of diffusion models
in distribution modeling and Large Language Models (LLMs) in contextual
understanding, we propose an LLM-Enhanced Spatio-temporal Diffusion Model
(LSDM). LSDM integrates the generative power of diffusion models with the
adaptive learning capabilities of transformers, augmented by the ability to
capture multimodal environmental information for modeling service-level
patterns and dynamics. Extensive evaluations on real-world service-level
datasets demonstrate that the model excels in traffic usage predictions,
showing outstanding generalization and adaptability. After incorporating
contextual information via LLM, the performance improves by at least 2.83% in
terms of the coefficient of determination. Compared to models of a similar
type, such as CSDI, the root mean squared error can be reduced by at least
8.29%. The code and dataset will be available at:
https://github.com/SoftYuaneR/LSDM.

</details>


### [81] [CoCAI: Copula-based Conformal Anomaly Identification for Multivariate Time-Series](https://arxiv.org/abs/2507.17796)
*Nicholas A. Pearson,Francesca Zanello,Davide Russo,Luca Bortolussi,Francesca Cairoli*

Main category: cs.LG

TL;DR: 本文提出了CoCAI框架，结合生成式AI和copula模型，有效解决了多元时间序列的精确预测和鲁棒异常检测两大挑战。


<details>
  <summary>Details</summary>
Motivation: 解决多元时间序列分析中实现高精度预测和进行鲁棒异常检测的关键挑战。

Method: 该方法命名为CoCAI，利用扩散模型捕捉数据复杂依赖关系进行高质量预测；通过共形预测技术校准模型输出，生成统计有效的预测区域；在此基础上，结合降维技术和copula建模进行异常检测，提供统计学上可靠的异常分数。CoCAI还包含一个离线校准阶段，以减少部署开销。

Result: 在真实的供水和污水处理系统运行数据上进行的实证测试表明，CoCAI在准确预测目标数据序列和识别异常片段方面均表现出有效性。

Conclusion: CoCAI是一个有效且理论基础扎实的新颖框架，能够同时实现多元时间序列的准确预测和鲁棒异常检测，并在实际应用中展现出良好性能。

Abstract: We propose a novel framework that harnesses the power of generative
artificial intelligence and copula-based modeling to address two critical
challenges in multivariate time-series analysis: delivering accurate
predictions and enabling robust anomaly detection. Our method, Copula-based
Conformal Anomaly Identification for Multivariate Time-Series (CoCAI),
leverages a diffusion-based model to capture complex dependencies within the
data, enabling high quality forecasting. The model's outputs are further
calibrated using a conformal prediction technique, yielding predictive regions
which are statistically valid, i.e., cover the true target values with a
desired confidence level. Starting from these calibrated forecasts, robust
outlier detection is performed by combining dimensionality reduction techniques
with copula-based modeling, providing a statistically grounded anomaly score.
CoCAI benefits from an offline calibration phase that allows for minimal
overhead during deployment and delivers actionable results rooted in
established theoretical foundations. Empirical tests conducted on real
operational data derived from water distribution and sewerage systems confirm
CoCAI's effectiveness in accurately forecasting target sequences of data and in
identifying anomalous segments within them.

</details>


### [82] [GenSelect: A Generative Approach to Best-of-N](https://arxiv.org/abs/2507.17797)
*Shubham Toshniwal,Ivan Sorokin,Aleksander Ficek,Ivan Moshkov,Igor Gitman*

Main category: cs.LG

TL;DR: 本文提出GenSelect，一种利用LLM长推理能力从N个候选方案中选出最佳方案的方法，解决了现有方法在效率和LLM比较能力利用上的不足，并在数学推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前生成奖励模型在推理任务中面临挑战：点式评分方法未能充分利用大型语言模型（LLM）的比较能力，而两两比较方法在面对更大的采样预算时扩展效率低下。

Method: 引入GenSelect，该方法让LLM通过长推理过程从N个候选解决方案中选择最佳方案。这旨在利用LLM的比较优势并提高并行采样的效率。

Result: 在数学推理任务中，GenSelect表现优异。Qwen和DeepSeek-R1-0528等推理模型在使用GenSelect时，仅通过简单提示就能超越现有评分方法，获得更好的性能。

Conclusion: GenSelect成功利用了LLM的比较能力，同时实现了高效的扩展性，为推理任务的解决方案选择提供了一种更优、更高效的方法。

Abstract: Generative reward models with parallel sampling have enabled effective
test-time scaling for reasoning tasks. Current approaches employ pointwise
scoring of individual solutions or pairwise comparisons. However, pointwise
methods underutilize LLMs' comparative abilities, while pairwise methods scale
inefficiently with larger sampling budgets. We introduce GenSelect, where the
LLM uses long reasoning to select the best solution among N candidates. This
leverages LLMs' comparative strengths while scaling efficiently across parallel
sampling budgets. For math reasoning, we demonstrate that reasoning models,
such as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing
scoring approaches with simple prompting.

</details>


### [83] [Wasserstein GAN-Based Precipitation Downscaling with Optimal Transport for Enhancing Perceptual Realism](https://arxiv.org/abs/2507.17798)
*Kenta Shiraishi,Yuka Muto,Atsushi Okazaki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: 本研究利用WGAN进行降水降尺度，生成了视觉上更真实的降水场，并发现其判别器有助于评估和质量控制降水数据。


<details>
  <summary>Details</summary>
Motivation: 高分辨率降水预测对减轻局部暴雨灾害至关重要，但现有过程驱动的数值天气预报模型难以实现高分辨率预测。

Method: 提出使用Wasserstein生成对抗网络 (WGAN) 进行降水降尺度，利用最优传输成本进行训练。

Result: WGAN生成的降水场在视觉上更真实，具有精细尺度结构，尽管在传统评估指标上略低。WGAN的判别器与人类感知真实度高度相关，且其评分差异可用于识别不真实的WGAN输出和参考数据中的潜在伪影。

Conclusion: WGAN框架不仅提升了降水降尺度的感知真实度，还为评估和质量控制降水数据集提供了新的视角。

Abstract: High-resolution (HR) precipitation prediction is essential for reducing
damage from stationary and localized heavy rainfall; however, HR precipitation
forecasts using process-driven numerical weather prediction models remains
challenging. This study proposes using Wasserstein Generative Adversarial
Network (WGAN) to perform precipitation downscaling with an optimal transport
cost. In contrast to a conventional neural network trained with mean squared
error, the WGAN generated visually realistic precipitation fields with
fine-scale structures even though the WGAN exhibited slightly lower performance
on conventional evaluation metrics. The learned critic of WGAN correlated well
with human perceptual realism. Case-based analysis revealed that large
discrepancies in critic scores can help identify both unrealistic WGAN outputs
and potential artifacts in the reference data. These findings suggest that the
WGAN framework not only improves perceptual realism in precipitation
downscaling but also offers a new perspective for evaluating and
quality-controlling precipitation datasets.

</details>


### [84] [Explainable Graph Neural Networks via Structural Externalities](https://arxiv.org/abs/2507.17848)
*Lijun Wu,Dong Hao,Zhiyi Fan*

Main category: cs.LG

TL;DR: 提出GraphEXT，一个基于合作博弈论和社会外部性的图神经网络（GNN）可解释性框架，通过量化节点间交互和结构变化的影响来增强GNN的解释性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）的“黑箱”特性使其可解释性面临挑战，且现有方法难以有效捕捉网络中节点间的复杂交互模式。

Method: 本文提出GraphEXT框架，利用合作博弈论和社会外部性概念。它将图节点划分为联盟并分解为独立子图，将图结构作为外部性整合，并引入外部性下的Shapley值。GraphEXT通过量化节点在联盟间转换时对GNN预测的边际贡献来衡量节点重要性，特别强调节点间交互和结构变化对GNN预测的影响。

Result: 在合成和真实世界数据集上的实验表明，GraphEXT在不同GNN架构下，其解释性（保真度）优于现有基线方法。

Conclusion: GraphEXT显著提高了GNN模型的可解释性，尤其擅长捕捉节点间复杂的交互模式和结构变化对预测的影响。

Abstract: Graph Neural Networks (GNNs) have achieved outstanding performance across a
wide range of graph-related tasks. However, their "black-box" nature poses
significant challenges to their explainability, and existing methods often fail
to effectively capture the intricate interaction patterns among nodes within
the network. In this work, we propose a novel explainability framework,
GraphEXT, which leverages cooperative game theory and the concept of social
externalities. GraphEXT partitions graph nodes into coalitions, decomposing the
original graph into independent subgraphs. By integrating graph structure as an
externality and incorporating the Shapley value under externalities, GraphEXT
quantifies node importance through their marginal contributions to GNN
predictions as the nodes transition between coalitions. Unlike traditional
Shapley value-based methods that primarily focus on node attributes, our
GraphEXT places greater emphasis on the interactions among nodes and the impact
of structural changes on GNN predictions. Experimental studies on both
synthetic and real-world datasets show that GraphEXT outperforms existing
baseline methods in terms of fidelity across diverse GNN architectures ,
significantly enhancing the explainability of GNN models.

</details>


### [85] [Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic](https://arxiv.org/abs/2507.17876)
*Rıza Özçelik,Sarah de Ruiter,Francesca Grisoni*

Main category: cs.LG

TL;DR: 提出分子任务算术，利用负样本学习属性方向并生成所需分子，解决了正样本稀缺瓶颈，提高了设计多样性和成功率。


<details>
  <summary>Details</summary>
Motivation: 具有理想性质的分子稀缺，是生成式分子设计中的固有瓶颈。

Method: 提出“分子任务算术”：在多样丰富的负样本上训练模型以学习“属性方向”，然后向相反方向移动模型来生成正向分子，无需正向标记数据。

Result: 在20个零样本设计实验中，比传统模型生成了更多样、更成功的设计。在双目标和少样本任务中，能一致提高设计多样性并保持理想性质。

Conclusion: 分子任务算术因其简单性、数据效率和高性能，有望成为从头分子设计的实际迁移学习策略。

Abstract: The scarcity of molecules with desirable properties (i.e., 'positive'
molecules) is an inherent bottleneck for generative molecule design. To
sidestep such obstacle, here we propose molecular task arithmetic: training a
model on diverse and abundant negative examples to learn 'property directions'
$--$ without accessing any positively labeled data $--$ and moving models in
the opposite property directions to generate positive molecules. When analyzed
on 20 zero-shot design experiments, molecular task arithmetic generated more
diverse and successful designs than models trained on positive molecules.
Moreover, we employed molecular task arithmetic in dual-objective and few-shot
design tasks. We find that molecular task arithmetic can consistently increase
the diversity of designs while maintaining desirable design properties. With
its simplicity, data efficiency, and performance, molecular task arithmetic
bears the potential to become the $\textit{de-facto}$ transfer learning
strategy for de novo molecule design.

</details>


### [86] [Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems and Experiments](https://arxiv.org/abs/2507.17887)
*Wonjae Lee,Taeyoung Kim,Hyungbin Park*

Main category: cs.LG

TL;DR: 本文提出了一种名为镜像填充傅里叶神经算子（MFNO）的新型基于算子的神经网络，通过引入镜像填充来处理非周期输入，有效学习随机系统动力学，并展现出优越的泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 标准傅里叶神经算子（FNO）难以处理非周期输入，且现有架构（如LSTM、TCN、DeepONet）缺乏处理随机系统动力学所需的强分辨率泛化能力。

Method: 该研究引入了镜像填充傅里叶神经算子（MFNO），它是标准傅里叶神经算子（FNO）的扩展，通过集成镜像填充使其能处理非周期输入。理论分析基于Wong--Zakai型定理和多种近似技术。

Result: MFNO在理论上被证明能够以任意精度近似路径依赖随机微分方程和分数布朗运动的Lipschitz变换的解。在实验中，MFNO表现出强大的分辨率泛化能力，其性能与LSTMs、TCNs和DeepONet等基线模型相当或更优，并且样本路径生成速度显著快于经典的数值方案。

Conclusion: MFNO是一种有效且高效的学习随机系统动力学的方法，它克服了现有算子网络的局限性，提供了卓越的分辨率泛化能力，并显著加快了样本路径的生成速度。

Abstract: This paper introduces an operator-based neural network, the mirror-padded
Fourier neural operator (MFNO), designed to learn the dynamics of stochastic
systems. MFNO extends the standard Fourier neural operator (FNO) by
incorporating mirror padding, enabling it to handle non-periodic inputs. We
rigorously prove that MFNOs can approximate solutions of path-dependent
stochastic differential equations and Lipschitz transformations of fractional
Brownian motions to an arbitrary degree of accuracy. Our theoretical analysis
builds on Wong--Zakai type theorems and various approximation techniques.
Empirically, the MFNO exhibits strong resolution generalization--a property
rarely seen in standard architectures such as LSTMs, TCNs, and DeepONet.
Furthermore, our model achieves performance that is comparable or superior to
these baselines while offering significantly faster sample path generation than
classical numerical schemes.

</details>


### [87] [Lower Bounds for Public-Private Learning under Distribution Shift](https://arxiv.org/abs/2507.17895)
*Amrith Setlur,Pratiksha Thaker,Jonathan Ullman*

Main category: cs.LG

TL;DR: 研究发现，在差分隐私机器学习中，当公有数据和私有数据存在分布偏移时，小偏移仍需充足数据，大偏移则使公有数据无益。


<details>
  <summary>Details</summary>
Motivation: 差分隐私机器学习算法常依赖公有数据以期实现协同增效，但已有研究表明，在数据分布相同时，结合公有和私有数据并无互补价值。本研究旨在探究当公有和私有数据之间存在显著分布偏移时，这种互补价值是否存在。

Method: 本研究将公有-私有学习的已知下限扩展到数据源存在显著分布偏移的场景。具体应用包括高斯均值估计（两个分布均值不同）和高斯线性回归（两个分布参数偏移）。

Result: 研究发现：1) 当分布偏移量较小（相对于所需精度）时，公有或私有数据必须足够充足才能有效估计私有参数。2) 当分布偏移量较大时，公有数据将不再提供任何益处。

Conclusion: 分布偏移显著影响公有数据在差分隐私机器学习中的效用。小幅偏移下仍需大量数据，而大幅偏移则直接抹杀了公有数据的价值，表明在存在分布偏移时，公有和私有数据结合的互补价值是有限的或不存在的。

Abstract: The most effective differentially private machine learning algorithms in
practice rely on an additional source of purportedly public data. This paradigm
is most interesting when the two sources combine to be more than the sum of
their parts. However, there are settings such as mean estimation where we have
strong lower bounds, showing that when the two data sources have the same
distribution, there is no complementary value to combining the two data
sources. In this work we extend the known lower bounds for public-private
learning to setting where the two data sources exhibit significant distribution
shift. Our results apply to both Gaussian mean estimation where the two
distributions have different means, and to Gaussian linear regression where the
two distributions exhibit parameter shift. We find that when the shift is small
(relative to the desired accuracy), either public or private data must be
sufficiently abundant to estimate the private parameter. Conversely, when the
shift is large, public data provides no benefit.

</details>


### [88] [Federated Learning for Large-Scale Cloud Robotic Manipulation: Opportunities and Challenges](https://arxiv.org/abs/2507.17903)
*Obaidullah Zaland,Chanh Nguyen,Florian T. Pokorny,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 论文探讨了联邦学习（FL）在云机器人操作中的应用，旨在克服现有计算限制，并分析了FL在此领域的机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作受限于计算资源，而云机器人能缓解计算需求。在分布式计算背景下，联邦学习有望为云机器人操作带来优势，但同时也存在挑战与机遇，因此需要深入探讨其基本概念及应用。

Method: 本文通过概念分析和展望的方式，阐述了联邦学习的基本概念及其与云机器人操作的关联，并提出了通过联邦学习实现高效可靠大规模云机器人操作的机遇与挑战。

Result: 论文阐明了联邦学习的核心概念及其与云机器人操作的潜在联系，并系统地分析了利用联邦学习实现大规模云机器人操作所面临的机遇与挑战。

Conclusion: 联邦学习为克服机器人计算限制、实现大规模高效的云机器人操作提供了重要途径。未来的研究应聚焦于在集中式或去中心化环境中设计和验证联邦学习模型，以应对现有挑战。

Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm,
where the collaborative training of a model involves dynamic participation of
devices to achieve broad objectives. In contrast, classical machine learning
(ML) typically requires data to be located on-premises for training, whereas FL
leverages numerous user devices to train a shared global model without the need
to share private data. Current robotic manipulation tasks are constrained by
the individual capabilities and speed of robots due to limited low-latency
computing resources. Consequently, the concept of cloud robotics has emerged,
allowing robotic applications to harness the flexibility and reliability of
computing resources, effectively alleviating their computational demands across
the cloud-edge continuum. Undoubtedly, within this distributed computing
context, as exemplified in cloud robotic manipulation scenarios, FL offers
manifold advantages while also presenting several challenges and opportunities.
In this paper, we present fundamental concepts of FL and their connection to
cloud robotic manipulation. Additionally, we envision the opportunities and
challenges associated with realizing efficient and reliable cloud robotic
manipulation at scale through FL, where researchers adopt to design and verify
FL models in either centralized or decentralized settings.

</details>


### [89] [Deep learning-aided inverse design of porous metamaterials](https://arxiv.org/abs/2507.17907)
*Phu Thien Nguyen,Yousef Heider,Dennis M. Kochmann,Fadi Aldakheel*

Main category: cs.LG

TL;DR: 研究开发了一个基于深度学习（pVAE）的生成框架，用于多孔超材料的逆向设计，能够生成具有定制水力特性（如孔隙率和渗透率）的材料。


<details>
  <summary>Details</summary>
Motivation: 旨在探索多孔超材料的逆向设计，生成具有特定水力特性的材料，并克服传统模拟方法（如LBM）计算成本高昂的问题。

Method: 开发了属性变分自编码器（pVAE），该模型在变分自编码器（VAE）基础上增加了回归器。结合格子玻尔兹曼方法（LBM）生成渗透率数据，并训练卷积神经网络（CNN）来高效预测水力特性，以替代耗时的LBM。pVAE在合成数据集和真实CT扫描图像数据集上进行训练，利用VAE的编码器-解码器结构将微结构特征映射到可解释的潜在空间。

Result: 成功实现了多孔超材料的逆向设计和生成。CNN显著降低了水力特性预测的计算成本。详细分析并解释了pVAE的潜在空间，证明了其在结构-属性映射、插值和逆向设计中的关键作用，促进了具有所需性能的新型超材料的生成。

Conclusion: 该研究提供了一种高效且可解释的深度学习方法（pVAE），实现了多孔超材料的逆向设计，能够生成具有定制水力特性的材料，并显著降低计算成本，有望加速新材料的发现和开发。

Abstract: The ultimate aim of the study is to explore the inverse design of porous
metamaterials using a deep learning-based generative framework. Specifically,
we develop a property-variational autoencoder (pVAE), a variational autoencoder
(VAE) augmented with a regressor, to generate structured metamaterials with
tailored hydraulic properties, such as porosity and permeability. While this
work uses the lattice Boltzmann method (LBM) to generate intrinsic permeability
tensor data for limited porous microstructures, a convolutional neural network
(CNN) is trained using a bottom-up approach to predict effective hydraulic
properties. This significantly reduces the computational cost compared to
direct LBM simulations. The pVAE framework is trained on two datasets: a
synthetic dataset of artificial porous microstructures and CT-scan images of
volume elements from real open-cell foams. The encoder-decoder architecture of
the VAE captures key microstructural features, mapping them into a compact and
interpretable latent space for efficient structure-property exploration. The
study provides a detailed analysis and interpretation of the latent space,
demonstrating its role in structure-property mapping, interpolation, and
inverse design. This approach facilitates the generation of new metamaterials
with desired properties. The datasets and codes used in this study will be made
open-access to support further research.

</details>


### [90] [SETOL: A Semi-Empirical Theory of (Deep) Learning](https://arxiv.org/abs/2507.17912)
*Charles H Martin,Christopher Hinrichs*

Main category: cs.LG

TL;DR: 提出一种半经验学习理论(SETOL)，解释了最先进神经网络(SOTA NN)的卓越性能，并为重尾自正则化(HTSR)理论中的关键指标提供了形式化解释，同时引入了新的学习质量指标ERG，该指标与HTSR指标表现出高度一致性。


<details>
  <summary>Details</summary>
Motivation: 旨在解释最先进神经网络的卓越性能，并为重尾自正则化（HTSR）现象学理论中的关键指标（alpha和alpha-hat）提供形式化起源解释，这些指标在无训练或测试数据的情况下即可预测模型性能。

Method: 提出了半经验学习理论（SETOL），融合了统计力学、随机矩阵理论和量子化学等先进方法。通过计算层权重矩阵的经验谱密度（ESD）并将其代入SETOL公式来估计层质量。并在一个简单的三层多层感知机（MLP）和SOTA神经网络上验证了理论假设和预测。

Result: SETOL的关键理论假设在MLP上得到了良好验证。研究展示了如何通过计算层权重矩阵的经验谱密度来估计训练好的SOTA神经网络的单层质量。发现HTSR的alpha指标与SETOL引入的新指标ERG在MLP和SOTA神经网络上均表现出显著的一致性。

Conclusion: SETOL提供了一个理论框架，能够解释SOTA神经网络的性能，并阐明了HTSR关键指标的起源。该理论引入了新的学习质量指标ERG，该指标与现有指标高度一致，为理想学习的数学前提提供了新的见解。

Abstract: We present a SemiEmpirical Theory of Learning (SETOL) that explains the
remarkable performance of State-Of-The-Art (SOTA) Neural Networks (NNs). We
provide a formal explanation of the origin of the fundamental quantities in the
phenomenological theory of Heavy-Tailed Self-Regularization (HTSR): the
heavy-tailed power-law layer quality metrics, alpha and alpha-hat. In prior
work, these metrics have been shown to predict trends in the test accuracies of
pretrained SOTA NN models, importantly, without needing access to either
testing or training data. Our SETOL uses techniques from statistical mechanics
as well as advanced methods from random matrix theory and quantum chemistry.
The derivation suggests new mathematical preconditions for ideal learning,
including a new metric, ERG, which is equivalent to applying a single step of
the Wilson Exact Renormalization Group. We test the assumptions and predictions
of SETOL on a simple 3-layer multilayer perceptron (MLP), demonstrating
excellent agreement with the key theoretical assumptions. For SOTA NN models,
we show how to estimate the individual layer qualities of a trained NN by
simply computing the empirical spectral density (ESD) of the layer weight
matrices and plugging this ESD into our SETOL formulas. Notably, we examine the
performance of the HTSR alpha and the SETOL ERG layer quality metrics, and find
that they align remarkably well, both on our MLP and on SOTA NNs.

</details>


### [91] [From Seed to Harvest: Augmenting Human Creativity with AI for Red-teaming Text-to-Image Models](https://arxiv.org/abs/2507.17922)
*Jessica Quaye,Charvi Rastogi,Alicia Parrish,Oana Inel,Minsuk Kahng,Lora Aroyo,Vijay Janapa Reddi*

Main category: cs.LG

TL;DR: 本文提出Seed2Harvest，一种混合人机红队方法，用于扩展多样化、高质量的对抗性文本-图像（T2I）提示词数据集，以更全面地评估T2I模型的安全性。


<details>
  <summary>Details</summary>
Motivation: T2I模型对抗性评估面临挑战，现有的人工生成数据集规模小且多样性不足，而机器生成数据集则缺乏真实性和创造性。因此，需要一种结合两者优点的方法来生成大规模、多样化且有效的对抗性提示词。

Method: 提出Seed2Harvest，一种混合红队方法。该方法通过引导式扩展文化多样性的人工创作对抗性提示词种子，结合了人类的创造力和机器的计算能力。

Result: 通过Seed2Harvest生成的提示词保留了人类提示词的特征和攻击模式，并保持了相似的平均攻击成功率（NudeNet 0.31，SD NSFW 0.36，Q16 0.12）。扩展后的数据集多样性显著提高，包含535个独特的地理位置，香农熵为7.48（原始数据集为58个位置，熵为5.28）。

Conclusion: 研究结果表明，人机协作对于实现全面、可扩展的红队测试至关重要，它能有效利用人类创造力和机器计算能力，从而实现T2I模型安全性的持续评估。

Abstract: Text-to-image (T2I) models have become prevalent across numerous
applications, making their robust evaluation against adversarial attacks a
critical priority. Continuous access to new and challenging adversarial prompts
across diverse domains is essential for stress-testing these models for
resilience against novel attacks from multiple vectors. Current techniques for
generating such prompts are either entirely authored by humans or synthetically
generated. On the one hand, datasets of human-crafted adversarial prompts are
often too small in size and imbalanced in their cultural and contextual
representation. On the other hand, datasets of synthetically-generated prompts
achieve scale, but typically lack the realistic nuances and creative
adversarial strategies found in human-crafted prompts. To combine the strengths
of both human and machine approaches, we propose Seed2Harvest, a hybrid
red-teaming method for guided expansion of culturally diverse, human-crafted
adversarial prompt seeds. The resulting prompts preserve the characteristics
and attack patterns of human prompts while maintaining comparable average
attack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded
dataset achieves substantially higher diversity with 535 unique geographic
locations and a Shannon entropy of 7.48, compared to 58 locations and 5.28
entropy in the original dataset. Our work demonstrates the importance of
human-machine collaboration in leveraging human creativity and machine
computational capacity to achieve comprehensive, scalable red-teaming for
continuous T2I model safety evaluation.

</details>


### [92] [UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction](https://arxiv.org/abs/2507.17924)
*Hongrong Yang,Markus Schlaepfer*

Main category: cs.LG

TL;DR: 本文提出了UrbanPulse，一个可扩展的深度学习框架，通过将每个POI视为独立节点并结合时序图卷积编码器和Transformer解码器，实现超细粒度、城市级的OD流预测。该框架采用三阶段迁移学习策略以确保跨城市泛化，并在大规模真实GPS数据上取得了最先进的精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 准确的人口流预测对城市规划、交通管理和公共健康至关重要。然而，现有方法存在局限：传统模型依赖静态空间假设；深度学习模型难以跨城市泛化；大语言模型计算成本高且未能捕获空间结构。此外，许多方法通过POI聚类或限制覆盖范围来牺牲分辨率，限制了其在全市范围分析中的实用性。

Method: 引入UrbanPulse，一个可扩展的深度学习框架，通过将每个POI视为独立节点，提供超细粒度、城市级的OD流预测。它结合了一个时序图卷积编码器和基于Transformer的解码器，以建模多尺度时空依赖关系。为确保跨城市环境的鲁棒泛化，UrbanPulse采用三阶段迁移学习策略：在大规模城市图谱上进行预训练、冷启动适应和强化学习微调。

Result: 在加利福尼亚州三个大都市区的超过1.03亿条清洗后的GPS记录上进行评估，UrbanPulse实现了最先进的精度和可扩展性。

Conclusion: 通过高效的迁移学习，UrbanPulse在使高分辨率、AI驱动的城市预测在不同城市中实际部署方面迈出了关键一步。

Abstract: Accurate population flow prediction is essential for urban planning,
transportation management, and public health. Yet existing methods face key
limitations: traditional models rely on static spatial assumptions, deep
learning models struggle with cross-city generalization, and Large Language
Models (LLMs) incur high computational costs while failing to capture spatial
structure. Moreover, many approaches sacrifice resolution by clustering Points
of Interest (POIs) or restricting coverage to subregions, limiting their
utility for city-wide analytics. We introduce UrbanPulse, a scalable deep
learning framework that delivers ultra-fine-grained, city-wide OD flow
predictions by treating each POI as an individual node. It combines a temporal
graph convolutional encoder with a transformer-based decoder to model
multi-scale spatiotemporal dependencies. To ensure robust generalization across
urban contexts, UrbanPulse employs a three-stage transfer learning strategy:
pretraining on large-scale urban graphs, cold-start adaptation, and
reinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS
records from three metropolitan areas in California, UrbanPulse achieves
state-of-the-art accuracy and scalability. Through efficient transfer learning,
UrbanPulse takes a key step toward making high-resolution, AI-powered urban
forecasting deployable in practice across diverse cities.

</details>


### [93] [Multimodal Fine-grained Reasoning for Post Quality Evaluation](https://arxiv.org/abs/2507.17934)
*Xiaoxu Guo,Siyan Liang,Yachao Cui,Juxiang Zhou,Lei Wang,Han Cao*

Main category: cs.LG

TL;DR: 本文提出MFTRR框架，通过多模态细粒度主题-帖子关系推理，将帖子质量评估重构为排序任务，有效捕捉复杂关系并抑制噪声。


<details>
  <summary>Details</summary>
Motivation: 现有研究在帖子质量评估中存在三方面局限：将任务视为单模态分类、多模态深度融合引入噪声、以及难以捕捉复杂语义关系。评估帖子质量需要复杂的关联推理来捕捉细微的主题-帖子关系。

Method: 提出MFTRR框架，模仿人类认知过程，将帖子质量评估重构为排序任务并整合多模态数据。包含两个核心模块：1) 局部-全局语义关联推理模块，通过最大信息融合机制建模细粒度语义交互并抑制噪声；2) 多层级证据关联推理模块，探索宏观和微观关联线索以增强基于证据的推理。

Result: 在三个新建多模态主题-帖子数据集和公开Lazada-Home数据集上评估，MFTRR显著优于现有基线方法，在Art History数据集上比最佳单模态方法提高高达9.52%的NDCG@3。

Conclusion: MFTRR框架通过有效地进行多模态细粒度主题-帖子关系推理，显著提升了帖子质量评估的准确性，证明了其在处理复杂关系方面的优越性。

Abstract: Accurately assessing post quality requires complex relational reasoning to
capture nuanced topic-post relationships. However, existing studies face three
major limitations: (1) treating the task as unimodal categorization, which
fails to leverage multimodal cues and fine-grained quality distinctions; (2)
introducing noise during deep multimodal fusion, leading to misleading signals;
and (3) lacking the ability to capture complex semantic relationships like
relevance and comprehensiveness. To address these issues, we propose the
Multimodal Fine-grained Topic-post Relational Reasoning (MFTRR) framework,
which mimics human cognitive processes. MFTRR reframes post-quality assessment
as a ranking task and incorporates multimodal data to better capture quality
variations. It consists of two key modules: (1) the Local-Global Semantic
Correlation Reasoning Module, which models fine-grained semantic interactions
between posts and topics at both local and global levels, enhanced by a maximum
information fusion mechanism to suppress noise; and (2) the Multi-Level
Evidential Relational Reasoning Module, which explores macro- and micro-level
relational cues to strengthen evidence-based reasoning. We evaluate MFTRR on
three newly constructed multimodal topic-post datasets and the public
Lazada-Home dataset. Experimental results demonstrate that MFTRR significantly
outperforms state-of-the-art baselines, achieving up to 9.52% NDCG@3
improvement over the best unimodal method on the Art History dataset.

</details>


### [94] [VIBE: Video-Input Brain Encoder for fMRI Response Modeling](https://arxiv.org/abs/2507.17958)
*Daniel Carlstrom Schad,Shrey Dixit,Janis Keck,Viktor Studenyak,Aleksandr Shpilevoi,Andrej Bicanski*

Main category: cs.LG

TL;DR: VIBE是一个两阶段Transformer模型，它融合视频、音频和文本等多模态特征来预测fMRI活动。


<details>
  <summary>Details</summary>
Motivation: 旨在利用多模态信息（视频、音频、文本）来更准确地预测大脑的fMRI活动。

Method: VIBE是一个两阶段Transformer。它首先通过模态融合Transformer整合来自开源模型（Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA）的多模态特征表示，然后由一个带有旋转嵌入的预测Transformer进行时间解码。模型在CNeuroMod数据集的65小时电影数据上训练，并对20个种子进行集成。

Result: VIBE在分布内数据（《老友记》S07）上实现了32.25的平均分块皮尔逊相关性，在六部分布外电影上实现了21.25。该架构的早期版本在Algonauts 2025挑战赛中分别获得0.3198和0.2096的相关性，赢得了第一阶段并获得总成绩第二名。

Conclusion: VIBE模型能够有效地利用多模态数据预测fMRI活动，在分布内和分布外数据上均表现出强大的性能，并在国际挑战赛中取得了显著的竞争力。

Abstract: We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,
and text features to predict fMRI activity. Representations from open-source
models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a
modality-fusion transformer and temporally decoded by a prediction transformer
with rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod
dataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson
correlations of 32.25 on in-distribution Friends S07 and 21.25 on six
out-of-distribution films. An earlier iteration of the same architecture
obtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second
overall in the Algonauts 2025 Challenge.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [95] [Frame-Based Zero-Shot Semantic Channel Equalization for AI-Native Communications](https://arxiv.org/abs/2507.17835)
*Simone Fiorellino,Claudio Battiloro,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.NI

TL;DR: 针对AI原生无线网络中DNN编码器潜在空间不匹配导致的语义信道噪声问题，本文提出Parseval帧均衡器（PFE）以对齐异构编码器，并结合动态优化策略，在无需再训练的情况下提升语义通信性能和资源利用效率。


<details>
  <summary>Details</summary>
Motivation: 在未来的AI原生无线网络中，独立设计和训练的深度神经网络（DNN）编码器之间存在的潜在空间不匹配，会导致语义信道噪声，从而阻碍互理解，降低接收方解释传输表示的能力，最终影响系统整体性能。

Method: 1. 提出Parseval帧均衡器（PFE），这是一种零样本、基于帧的语义信道均衡器，能够无需系统再训练即可对齐异构编码器的潜在空间。PFE支持动态信号压缩与扩展，以减轻语义噪声并保持下游任务性能。
2. 在此基础上，引入动态优化策略，协调通信、计算和学习资源，以在多智能体语义通信场景中平衡能耗、端到端（E2E）延迟和任务性能。

Result: 广泛的模拟结果证实，所提出的方法在多样且时变的网络条件下，能够有效保持语义一致性，并满足对延迟和准确性的长期约束。

Conclusion: 本研究提出的PFE及其动态优化策略，能够有效解决AI原生无线网络中的语义信道噪声问题，实现异构编码器间的潜在空间对齐和互理解，同时通过优化资源分配平衡系统性能，确保在复杂多变的场景下实现稳健的语义通信。

Abstract: In future AI-native wireless networks, the presence of mismatches between the
latent spaces of independently designed and trained deep neural network (DNN)
encoders may impede mutual understanding due to the emergence of semantic
channel noise. This undermines the receiver's ability to interpret transmitted
representations, thereby reducing overall system performance. To address this
issue, we propose the Parseval Frame Equalizer (PFE), a zero-shot, frame-based
semantic channel equalizer that aligns latent spaces of heterogeneous encoders
without requiring system retraining. PFE enables dynamic signal compression and
expansion, mitigating semantic noise while preserving performance on downstream
tasks. Building on this capability, we introduce a dynamic optimization
strategy that coordinates communication, computation, and learning resources to
balance energy consumption, end-to-end (E2E) latency, and task performance in
multi-agent semantic communication scenarios. Extensive simulations confirm the
effectiveness of our approach in maintaining semantic consistency and meeting
long-term constraints on latency and accuracy under diverse and time-varying
network conditions.

</details>


### [96] [ARCADE: A RAN Diagnosis Methodology in a Hybrid AI Environment for 6G Networks](https://arxiv.org/abs/2507.17861)
*Daniel Ricardo Cunha Oliveira,Rodrigo Moreira,Flávio de Oliveira Silva*

Main category: cs.NI

TL;DR: 本文提出ARCADE方法，用于检测和评估蜂窝接入网的无线覆盖异常，并以此作为示例，说明混合网络分析架构如何促进AI在6G网络中的广泛应用。


<details>
  <summary>Details</summary>
Motivation: 现有网络数据分析功能（NWDAF）不足以支持6G网络中未充分探索部分的自动化。需要更全面的方法，以增强AI在更广泛网络环境中的应用。

Method: 提出并详细介绍了自动化无线覆盖异常检测与评估（ARCADE）方法，该方法专注于识别和诊断蜂窝接入网络中的异常。同时，探讨了如何通过网络分析功能的混合架构，将ARCADE作为实例，增强AI在迈向6G过程中的应用。

Result: 成功展示了ARCADE作为一种实际方法，如何通过网络分析功能的混合架构，在向6G演进的过程中，有效增强AI在更广泛网络环境中的应用。

Conclusion: ARCADE方法能够有效识别和诊断蜂窝接入网的覆盖异常，并提供了一个实用范例，证明了通过混合网络分析架构可以有效增强AI在6G网络中的广泛应用，从而实现更全面的网络自动化。

Abstract: Artificial Intelligence (AI) plays a key role in developing 6G networks.
While current specifications already include Network Data Analytics Function
(NWDAF) as a network element responsible for providing information about the
core, a more comprehensive approach will be needed to enable automation of
network segments that are not yet fully explored in the context of 5G. In this
paper, we present Automated Radio Coverage Anomalies Detection and Evaluation
(ARCADE), a methodology for identifying and diagnosing anomalies in the
cellular access network. Furthermore, we demonstrate how a hybrid architecture
of network analytics functions in the evolution toward 6G can enhance the
application of AI in a broader network context, using ARCADE as a practical
example of this approach.

</details>


### [97] [Talk with the Things: Integrating LLMs into IoT Networks](https://arxiv.org/abs/2507.17865)
*Alakesh Kalita*

Main category: cs.NI

TL;DR: 该研究提出了一个边缘中心化的框架，将大型语言模型（LLMs）与物联网（IoT）集成，以实现基于自然语言的控制、情境感知决策和自动化，并在智能家居原型中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）与物联网（IoT）网络的融合为构建智能、响应迅速且用户友好的系统提供了新机遇。

Method: 提出一个边缘中心化的框架，将模块化、轻量级的基于检索增强生成（RAG）的LLMs部署在连接到IoT网关的边缘计算设备上，以实现本地处理用户命令和传感器数据。通过使用LLaMA 3和Gemma 2B模型构建智能家居原型来验证该框架。

Result: 实验结果揭示了模型准确性和推理时间与模型大小之间的权衡关系。

Conclusion: 该框架能够实现基于LLM的物联网系统的自然语言控制和增强自动化，并讨论了其潜在应用及相关挑战。

Abstract: The convergence of Large Language Models (LLMs) and Internet of Things (IoT)
networks open new opportunities for building intelligent, responsive, and
user-friendly systems. This work presents an edge-centric framework that
integrates LLMs into IoT architectures to enable natural language-based
control, context-aware decision-making, and enhanced automation. The proposed
modular and lightweight Retrieval Augmented Generation (RAG)-based LLMs are
deployed on edge computing devices connected to IoT gateways, enabling local
processing of user commands and sensor data for reduced latency, improved
privacy, and enhanced inference quality. We validate the framework through a
smart home prototype using LLaMA 3 and Gemma 2B models for controlling smart
devices. Experimental results highlight the trade-offs between model accuracy
and inference time with respect to models size. At last, we also discuss the
potential applications that can use LLM-based IoT systems, and a few key
challenges associated with such systems.

</details>


### [98] [Enabling Scalability in Asynchronous and Bidirectional Communication in LPWAN](https://arxiv.org/abs/2507.17905)
*Mahbubur Rahman*

Main category: cs.NI

TL;DR: 该论文通过引入基于Gold码的非干扰伪随机序列，显著提升了SNOW LPWAN技术的可扩展性，使其能支持大量异步传感器在同一子载波上并发通信，实现了约9倍的可扩展性提升。


<details>
  <summary>Details</summary>
Motivation: LPWANs在连接传感器方面表现出色，但为新兴物联网（IoT）和网络物理系统（CPS）应用实现大规模并发（即大量传感器高效、低延迟地传输数据）面临巨大挑战。

Method: 该研究通过增强SNOW LPWAN技术来解决上述挑战。SNOW利用分布式正交频分复用（D-OFDM）子载波实现基站并行接收来自多个异步传感器的数据。本文进一步使其能够在同一子载波上解码来自大量异步传感器的并发数据，同时也能并行解码其他子载波上的数据。此外，还实现了多个异步传感器在同一子载波上接收不同数据，同时其他传感器也能在其他子载波上并行接收数据。为实现此目标，研究开发了一套基于Gold码的伪随机噪声（PN）序列，这些序列在子载波内部和跨子载波之间互不干扰，每个传感器使用其专属的PN序列进行数据编码或解码，从而实现大规模并发。

Result: 评估结果表明，该方法使SNOW的可扩展性提高了约9倍，同时保持了基站数据收集的及时性，并显著提高了传感器的能源效率。

Conclusion: 这项研究能够支持新兴的IoT和CPS应用，这些应用可能需要数万个传感器，并具有更长的电池寿命，能够进行数据驱动、时间敏感的决策。

Abstract: LPWANs have become ubiquitous due to their ability to connect sensors over
large geographic areas in a single hop. It is, however, very challenging to
achieve massive scalability in LPWANs, where numerous sensors can transmit data
efficiently and with low latency, which emerging IoT and CPS applications may
require. In this paper, we address the above challenges by significantly
advancing an LPWAN technology called SNOW. SNOW exploits distributed orthogonal
frequency division multiplexing, D-OFDM, subcarriers to enable parallel
reception of data to a BS from multiple asynchronous sensors, each using a
different subcarrier. In this paper, we achieve massive scalability in SNOW by
enabling the BS to decode concurrent data from numerous asynchronous sensors on
the same subcarrier while parallelly decoding from other subcarriers as well.
Additionally, we enable numerous asynchronous sensors to receive distinct data
from the BS on the same subcarrier while other sensors also receive data
parallelly on other subcarriers. To do this, we develop a set of Gold
code-based pseudorandom noise or PN sequences that are mutually non-interfering
within and across the subcarriers. Each sensor uses its PN sequence from the
set for encoding or decoding data on its subcarriers, enabling massive
concurrency. Our evaluation results demonstrate that we can achieve
approximately 9x more scalability in SNOW while being timely in data collection
at the BS and energy efficient at the sensors. This may enable emerging IoT and
CPS applications requiring tens of thousands of sensors with longer battery
life and making data-driven, time-sensitive decisions.

</details>


### [99] [Enhanced Velocity-Adaptive Scheme: Joint Fair Access and Age of Information Optimization in Vehicular Networks](https://arxiv.org/abs/2507.18328)
*Xiao Xu,Qiong Wu,Pingyi Fan,Kezhi Wang,Nan Cheng,Wen Chen,Khaled B. Letaief*

Main category: cs.NI

TL;DR: 本文针对5G NR V2I Mode 2车载网络中车辆速度差异导致的不公平接入和数据时效性（AoI）问题，提出了一个联合优化框架。该框架利用随机混合系统（SHS）建模AoI，并通过调整半持久调度（SPS）选择窗口，结合基于LLM的MOEA/D算法，有效平衡了公平接入与AoI。


<details>
  <summary>Details</summary>
Motivation: 在5G NR V2I Mode 2车载网络中，车辆速度差异导致RSU驻留时间及通信时长不同，造成网络资源不公平接入，影响驾驶安全。同时，需保证接收数据的时效性（AoI）。为确保及时相关的数据交付，亟需在Mode 2抢占机制下同时优化公平接入和AoI。

Method: 提出一个针对车载网络的联合优化框架。定义公平性指标，并采用随机混合系统（SHS）建模抢占机制下的信息年龄（AoI）。通过自适应调整Mode 2中半持久调度（SPS）的选择窗口来解决公平性和AoI的优化问题。采用基于大型语言模型（LLM）的多目标分解演化算法（MOEA/D）求解该问题。

Result: 仿真结果表明，所提出的方案在平衡公平接入和最小化信息年龄（AoI）方面表现出有效性。

Conclusion: 本研究提出的方案成功实现了5G NR V2I Mode 2车载网络中公平接入与信息年龄（AoI）的有效平衡，从而确保了数据传输的及时性和相关性。

Abstract: In this paper, we consider the fair access problem and the Age of Information
(AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in
vehicular networks. Specifically, vehicles follow Mode 2 to communicate with
Roadside Units (RSUs) to obtain accurate data for driving
assistance.Nevertheless, vehicles often have different velocity when they are
moving in adjacent lanes, leading to difference in RSU dwelltime and
communication duration. This results in unfair access to network resources,
potentially influencing driving safety. To ensure the freshness of received
data, the AoI should be analyzed. Mode 2 introduces a novel preemption
mechanism, necessitating simultaneous optimization of fair access and AoI to
guarantee timely and relevant data delivery. We propose a joint optimization
framework for vehicular network, defining a fairness index and employing
Stochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By
adaptively adjusting the selection window of Semi-Persistent Scheduling (SPS)
in Mode 2, we address the optimization of fairness and AoI. We apply a large
language model (LLM)-Based Multi-objective Evolutionary Algorithm Based on
Decomposition (MOEA/D) to solve this problem. Simulation results demonstrate
the effectiveness of our scheme in balancing fair access and minimizing AoI.

</details>


### [100] [Improving Wi-Fi 8 Latency with Coordinated Spatial Reuse](https://arxiv.org/abs/2507.18480)
*David Nunez,Francesc Wilhelmi,Lorenzo Galati-Giordano,Giovanni Geraci,Boris Bellalta*

Main category: cs.NI

TL;DR: 本文评估了Wi-Fi 8网络中Coordinated Spatial Reuse (Co-SR)的性能，结果显示与传统DCF相比，Co-SR能显著降低网络延迟。


<details>
  <summary>Details</summary>
Motivation: 为满足云计算游戏、扩展现实(XR)和视频流等新兴应用对高吞吐量、低延迟和高可靠性的严格要求，IEEE 802.11网络需持续改进以优化频谱资源利用。

Method: 提出并实现了一种符合Wi-Fi 8标准化工作的Co-SR机制。通过Wi-Fi模拟器在相关场景中对所提出的Co-SR机制进行性能评估。

Result: 在由四个AP组成的无线局域网(WLAN)中，与分布式协调功能(DCF)相比，Co-SR使延迟降低了31%至95%。

Conclusion: Co-SR能有效优化频谱资源利用，通过实现同步传输提升频谱效率，从而显著改善Wi-Fi 8网络在密集环境下的性能，特别是大幅降低延迟，以适应新兴应用的需求。

Abstract: IEEE 802.11 networks continuously adapt to meet the stringent requirements of
emerging applications like cloud gaming, eXtended Reality (XR), and video
streaming services, which require high throughput, low latency, and high
reliability. To address these challenges, Coordinated Spatial Reuse (Co-SR) can
potentially contribute to optimizing spectrum resource utilization. This
mechanism is expected to enable simultaneous transmissions, thereby boosting
spectral efficiency in dense environments and increasing the overall network
performance. In this paper, we shed light on the performance of Co-SR for Wi-Fi
8 networks. For that, we propose an implementation of Co-SR aligned with
ongoing Wi-Fi 8 standardization efforts. The evaluation is done on a Wi-Fi
simulator, which allows us to study the performance of the proposed Co-SR
mechanisms in relevant scenarios. The results obtained in a Wireless Local Area
Network (WLAN) consisting of four APs show delay reduction with Co-SR ranging
from 31% to 95% when compared to Distributed Coordination Function (DCF).

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [101] [Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving](https://arxiv.org/abs/2507.17753)
*Liang Zhang,Xiaoming Zhai,Jionghao Lin,Jionghao Lin,Jennifer Kleiman,Diego Zapata-Rivera,Carol Forsyth,Yang Jiang,Xiangen Hu,Arthur C. Graesser*

Main category: cs.HC

TL;DR: 研究并评估了大型语言模型（LLM）代理在数学解题中不同沟通策略的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM代理在AI辅助教育中日益普及，但鲜有研究系统评估不同沟通策略对其解决问题能力的影响。

Method: 在双代理、基于聊天的数学解题环境中，使用OpenAI GPT-4o模型，在MATH数据集上测试了四种沟通模式：师生互动、点对点协作、互惠式同伴教学和批判性辩论。

Result: 双代理设置优于单代理，其中“点对点协作”模式的准确率最高。陈述、确认和提示等对话行为在协作解题中发挥关键作用。

Conclusion: 多代理框架能增强计算任务，但有效的沟通策略对于解决AI教育中的复杂问题至关重要。

Abstract: Large Language Model (LLM) agents are increasingly utilized in AI-aided
education to support tutoring and learning. Effective communication strategies
among LLM agents improve collaborative problem-solving efficiency and
facilitate cost-effective adoption in education. However, little research has
systematically evaluated the impact of different communication strategies on
agents' problem-solving. Our study examines four communication modes,
\textit{teacher-student interaction}, \textit{peer-to-peer collaboration},
\textit{reciprocal peer teaching}, and \textit{critical debate}, in a
dual-agent, chat-based mathematical problem-solving environment using the
OpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that
dual-agent setups outperform single agents, with \textit{peer-to-peer
collaboration} achieving the highest accuracy. Dialogue acts like statements,
acknowledgment, and hints play a key role in collaborative problem-solving.
While multi-agent frameworks enhance computational tasks, effective
communication strategies are essential for tackling complex problems in AI
education.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [102] [On the Role of Age and Semantics of Information in Remote Estimation of Markov Sources](https://arxiv.org/abs/2507.18514)
*Jiping Luo,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 研究在传输频率约束下，通过优化传输策略提高有限状态马尔可夫链的语义感知远程估计性能。


<details>
  <summary>Details</summary>
Motivation: 在传输频率约束下，优化有限状态马尔可夫链的远程估计性能，并有效量化传输端估计误差的重要性和接收端过时信息的可预测性。

Method: 采用最大后验（MAP）估计器，引入连续误差时长（AoCE）和信息年龄（AoI）度量。将问题建模为约束马尔可夫决策过程（CMDP），并开发了结构感知算法Insec-SPI来计算最优策略。

Result: 发现存在一种最优的简单混合策略，该策略根据AoCE、AoI和瞬时估计误差的组合阈值触发传输。导出了策略简化为简单阈值策略的充分条件。结果表明，AoI和AoCE的结合使用显著提高了估计质量。

Conclusion: 通过综合考虑信息年龄（AoI）和连续误差时长（AoCE）并设计自适应传输策略，能够有效提升有限状态马尔可夫链的远程估计性能，超越单一指标的优化效果。

Abstract: This paper investigates the semantics-aware remote estimation of a
finite-state Markov chain. We employ the maximum a posteriori (MAP) estimator
and aim to devise a transmission policy to optimize estimation performance
subject to a transmission frequency constraint. We leverage two metrics, namely
the Age of Consecutive Error (AoCE) and the Age of Information (AoI), to
quantify, respectively, the significance of estimation error at the transmitter
and the predictability of outdated information at the receiver. The optimal
transmission problem is formulated as a constrained Markov decision process
(CMDP) with unbounded costs. We show the existence of an optimal simple mixture
policy, which randomly selects between two deterministic switching policies
with a fixed probability. Notably, each switching policy triggers a
transmission only when the AoCE exceeds a threshold value that depends on both
the AoI and the instantaneous estimation error. We further derive sufficient
conditions under which the switching policy reduces to a simple threshold
policy; that is, it admits identical thresholds for all estimation errors.
Leveraging these results, we develop an efficient structure-aware algorithm,
Insec-SPI, that computes the optimal policy with reduced computation overhead.
Our results demonstrate that incorporating both AoI and AoCE yields
significantly improved estimation quality compared to using either metric
alone.

</details>
