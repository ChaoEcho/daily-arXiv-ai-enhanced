<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 21]
- [cs.CV](#cs.CV) [Total: 18]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.LG](#cs.LG) [Total: 18]
- [cs.NI](#cs.NI) [Total: 18]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

TL;DR: DeepWriter是一个基于精选离线知识库的可定制、多模态长文写作助手，通过创新的流水线和分层知识表示，有效解决了大型语言模型在专业领域写作中知识不足和幻觉问题，并在金融报告生成中展现出卓越的事实准确性和内容质量。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在金融、医疗、法律等专业领域作为写作助手时，面临领域知识缺乏和幻觉问题。现有解决方案如检索增强生成（RAG）存在多步检索不一致性，而基于在线搜索的方法则受制于不可靠的网络内容，导致内容质量下降。

Method: 本文提出DeepWriter，一个可定制、多模态且基于精选离线知识库的长文写作助手。它采用了一种新颖的流水线，包括任务分解、大纲生成、多模态检索以及带有反思的分段式内容创作。此外，为提高检索效率和准确性，该方法还引入了分层知识表示。

Result: 在金融报告生成任务上的实验表明，DeepWriter能够生成高质量、可验证的文章，其事实准确性和生成内容质量均超越了现有基线方法。

Conclusion: DeepWriter通过利用精选的离线知识库、结合多模态信息和创新的写作流水线，成功克服了LLMs在专业领域长文写作中的关键挑战，实现了生成事实准确、专业水准文档的目标。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [2] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

TL;DR: 研究发现大型语言模型中编辑过的知识在微调时比固有知识更容易被遗忘，但冻结相关层可以显著提高知识保留。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的知识更新常使用模型编辑方法，同时LLMs也广泛用于下游任务的微调。然而，微调对先前编辑过的知识的影响尚不清楚。

Method: 系统地调查了不同的微调目标如何与各种模型编辑技术相互作用。

Result: 已编辑的知识在微调过程中比预训练获得的固有知识更容易被遗忘。冻结与编辑内容相关的层可以显著提高知识保留。

Conclusion: 当前模型编辑方法在微调下对已编辑知识的鲁棒性存在关键局限。在下游微调中评估编辑鲁棒性对于实际部署至关重要。冻结相关层为未来更鲁棒的编辑方法提供了方向。

Abstract: Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [3] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

TL;DR: 本文提出SMACS框架，通过协同多个开源LLM，显著超越主流闭源LLM性能，并推动了智能上限。


<details>
  <summary>Details</summary>
Motivation: 旨在探讨能否通过集合多个开源大语言模型（LLMs）的优势，使其性能匹敌甚至超越闭源LLMs。

Method: 提出SMACS（可扩展多智能体协作系统）框架，该框架具有高性能，并包含：1. **基于检索的先验选择（RPS）**：为每个LLM分配代理性能分数，以在实例级别选择Top-k LLMs。2. **探索-利用驱动的后验增强（EPE）**：通过先验丢弃鼓励生成多样化响应，并通过混合后验分数选择高质量响应。

Result: 在八个主流基准测试上，SMACS（整合十五个开源LLM）的有效性得到验证。其性能超越了领先的闭源LLMs，例如Claude-3.7-Sonnet (+12.73%)、GPT-4.1 (+5.36%)和GPT-o3-mini (+5.28%)。SMACS甚至超过了开源LLM (+2.86%)和闭源LLM (+2.04%)在不同数据集上的最佳平均结果。

Conclusion: SMACS系统成功证明了通过协同多个开源LLM，不仅能匹敌甚至超越领先闭源LLM的性能，而且能推动人工智能的上限，凸显了开源协作在LLM领域的巨大潜力。

Abstract: This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [4] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.CL

TL;DR: PoliAnalyzer是一个神经符号系统，通过自动化分析帮助用户个性化理解隐私政策，显著降低阅读负担并提高数据控制力。


<details>
  <summary>Details</summary>
Motivation: 现代用户虽拥有众多在线账户，但普遍忽视阅读其服务条款和隐私政策，导致对自身数据使用情况缺乏了解，增加了认知负担。

Method: 本研究引入了PoliAnalyzer，一个神经符号系统。该系统利用自然语言处理（NLP）从政策文本中提取数据使用实践的正式表示。然后，通过确定性逻辑推理，将用户个性化偏好与隐私政策的正式表示进行比较，并生成合规性报告。为实现此目标，系统扩展了现有的数据使用条款政策语言，将隐私政策建模为应用政策，用户偏好建模为数据政策。

Result: PoliAnalyzer在法律专家整理的PolicyIE数据集上表现出高精度，在识别相关数据使用实践方面，大多数任务的F1分数达到90-100%。系统成功建模了23种不同的用户数据共享偏好，并对排名前100位的网站进行了合规性分析。结果显示，平均95.2%的隐私政策片段与用户偏好不冲突，用户只需关注4.8%（636/13205）的违规部分，显著减轻了认知负担。研究还识别出隐私政策中常见的违反用户期望的做法，例如与第三方共享位置数据。

Conclusion: 本研究表明PoliAnalyzer能够使用现成的NLP工具，大规模支持自动化、个性化的隐私政策分析。这为帮助个人重新获得对其数据的控制权，并促进关于平台数据实践的社会讨论以建立更公平的权力动态提供了可行途径。

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites despite claiming
otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that
assists users with personalized privacy policy analysis. PoliAnalyzer uses
Natural Language Processing (NLP) to extract formal representations of data
usage practices from policy texts. In favor of deterministic, logical inference
is applied to compare user preferences with the formal privacy policy
representation and produce a compliance report. To achieve this, we extend an
existing formal Data Terms of Use policy language to model privacy policies as
app policies and user preferences as data policies. In our evaluation using our
enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated
high accuracy in identifying relevant data usage practices, achieving F1-score
of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can
model diverse user data-sharing preferences, derived from prior research as 23
user profiles, and perform compliance analysis against the top 100 most-visited
websites. This analysis revealed that, on average, 95.2% of a privacy policy's
segments do not conflict with the analyzed user preferences, enabling users to
concentrate on understanding the 4.8% (636 / 13205) that violates preferences,
significantly reducing cognitive burden. Further, we identified common
practices in privacy policies that violate user expectations - such as the
sharing of location data with 3rd parties. This paper demonstrates that
PoliAnalyzer can support automated personalized privacy policy analysis at
scale using off-the-shelf NLP tools. This sheds light on a pathway to help
individuals regain control over their data and encourage societal discussions
on platform data practices to promote a fairer power dynamic.

</details>


### [5] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

TL;DR: 本研究利用先进的NLP模型分析社交媒体文本，以识别双相情感障碍迹象。结果显示，上下文语言模型（如RoBERTa和基于BERT嵌入的LSTM）在检测该疾病方面表现卓越，而静态嵌入效果不佳。


<details>
  <summary>Details</summary>
Motivation: 双相情感障碍因早期症状隐蔽和社会污名而常被漏诊。本研究旨在探索利用先进的自然语言处理（NLP）模型，基于用户生成的社交媒体文本识别双相情感障碍的早期迹象，以支持疾病的早期筛查。

Method: 本研究对一系列NLP模型进行了综合评估，包括Transformer架构模型（BERT, RoBERTa, ALBERT, ELECTRA, DistilBERT）和长短期记忆（LSTM）模型。LSTM模型分别使用了上下文（BERT）和静态（GloVe, Word2Vec）词嵌入。实验在一个大型、经过标注的Reddit帖子数据集上进行，该数据集通过情感方差和判断分析确认了有效性。

Result: 研究结果表明，RoBERTa在Transformer模型中表现最佳，F1分数约为98%。使用BERT嵌入的LSTM模型也取得了几乎相同的性能。然而，使用静态嵌入训练的LSTM模型未能捕捉到有意义的模式，F1分数接近零。此外，DistilBERT在效率和准确性之间取得了最佳平衡，研究也强调了上下文语言建模在检测双相情感障碍中的关键作用。

Conclusion: 本研究为精神健康NLP应用中的模型选择提供了可操作的见解，并验证了上下文语言模型在支持双相情感障碍早期筛查方面的巨大潜力。上下文语言建模在识别疾病迹象方面至关重要。

Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [6] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

TL;DR: 本研究首次全面分析了大型语言模型（LLMs）如何根据用户文本中的身份标记（如种族、性别、年龄）产生偏见响应，发现其在医疗、法律、政治和就业等高风险应用中存在显著偏差，可能导致有害后果。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在面向用户的应用中日益普及，它们能够从细微的语言模式中推断用户身份。然而，LLMs如何利用这些身份信息进行决策尚不清楚，因此有必要探究用户写作中的身份标记如何影响LLM的响应。

Method: 研究对LLMs在医疗、法律、政治、政府福利和工作薪资五个高风险应用中进行了首次综合分析，评估了用户写作中存在的身份标记如何偏向LLM的响应。

Result: 研究发现LLMs对用户查询中的身份标记（种族、性别、年龄）极其敏感，且这些标记持续影响LLM的响应。例如，在提供医疗建议时，模型对不同种族的人采用不同标准；对年龄较长（年轻）的用户，LLMs更倾向于将答案调整为符合保守（自由）政治观点；对非白人求职者推荐较低薪资，对女性推荐较高薪资。这些偏见可能导致医疗差异、工资差距和不同的政治事实认知。

Conclusion: LLMs的这些偏见意味着直接使用现有模型可能导致医疗、就业和政治领域中的有害差异。鉴于这些发现的严重性，建议在未来部署面向用户的LLM应用前，进行类似的彻底评估。

Abstract: Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [7] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

TL;DR: 本文提出CCL-XCoT框架，通过结合课程对比学习和跨语言思维链，有效降低多语言大模型在低资源语言中的幻觉。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型（MLLMs）在低资源语言中存在严重的幻觉问题，尤其是在领域特定生成任务中，这主要源于训练数据不平衡。

Method: 本文提出一个两阶段微调框架CCL-XCoT。第一阶段在持续预训练中结合课程对比学习和下一词元预测以增强跨语言语义对齐；第二阶段在指令微调中引入跨语言思维链（XCoT）提示策略，引导模型在高资源语言中推理后在低资源语言中生成答案。

Result: 实验结果表明，CCL-XCoT将幻觉率降低了高达62%，并显著提高了跨语言对的事实知识迁移能力，且无需外部检索或多模型集成。

Conclusion: CCL-XCoT框架成功缓解了多语言大模型在低资源语言中的幻觉问题，提升了模型的跨语言知识泛化和可靠性。

Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [8] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
*Mohammad Shahedur Rahman,Peng Gao,Yuede Ji*

Main category: cs.CL

TL;DR: 该研究通过构建和分析大型语言模型（LLM）供应链中模型与数据集的关系图，揭示其结构、依赖性和动态性，旨在应对潜在的风险和偏见。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型（LLM）在开发过程中可能从基础模型或数据集中继承漏洞、偏见或恶意组件，理解这些组件的来源和发展对于风险检测、模型公平性提升和合规性保障至关重要。

Method: 研究首先设计了一种系统方法来收集LLM供应链数据。随后，利用这些数据构建了一个包含397,376个节点和453,469条边的有向异构图，以建模和分析模型与数据集之间的复杂关系。

Result: 分析发现LLM供应链图巨大、稀疏并遵循幂律度分布；具有密集连接的核心和分散的外围；数据在模型训练中扮演着关键角色；模型与数据集之间存在强烈的相互依赖性；且该图是动态的，每日都在更新。

Conclusion: 通过对LLM供应链中模型与数据集关系的深入研究，揭示了其复杂结构和动态性，为理解并解决LLM生态系统中的潜在风险、公平性问题及合规性挑战提供了重要见解。

Abstract: Large language models (LLMs) leverage deep learning to process and predict
sequences of words from context, enabling them to perform various NLP tasks,
such as translation, summarization, question answering, and content generation.
However, the growing size and complexity of developing, training, and deploying
advanced LLMs require extensive computational resources and large datasets.
This creates a barrier for users. As a result, platforms that host models and
datasets are widely used. For example, Hugging Face, one of the most popular
platforms, hosted 1.8 million models and 450K datasets by June 2025, with no
sign of slowing down. Since many LLMs are built from base models, pre-trained
models, and external datasets, they can inherit vulnerabilities, biases, or
malicious components from earlier models or datasets. Therefore, it is critical
to understand the origin and development of these components to better detect
potential risks, improve model fairness, and ensure compliance. Motivated by
this, our project aims to study the relationships between models and datasets,
which are core components of the LLM supply chain. First, we design a method to
systematically collect LLM supply chain data. Using this data, we build a
directed heterogeneous graph to model the relationships between models and
datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We
then perform various analyses and uncover several findings, such as: (i) the
LLM supply chain graph is large, sparse, and follows a power-law degree
distribution; (ii) it features a densely connected core and a fragmented
periphery; (iii) datasets play pivotal roles in training; (iv) strong
interdependence exists between models and datasets; and (v) the graph is
dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [9] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
*Rithesh Murthy,Ming Zhu,Liangwei Yang,Jielin Qiu,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Silvio Savarese*

Main category: cs.CL

TL;DR: 本文介绍Promptomatix，一个自动提示词优化框架，将自然语言任务描述转化为高质量提示词，无需人工干预，并在多任务上表现优异，同时降低成本。


<details>
  <summary>Details</summary>
Motivation: 尽管精心设计的提示词对大型语言模型（LLMs）的性能至关重要，但当前的提示工程过程仍是手动的、不一致的，且非专业人士难以操作。

Method: 研究引入了Promptomatix，一个自动提示优化框架。它能将自然语言任务描述转换为高质量提示词，无需手动调整或领域专业知识。Promptomatix支持轻量级元提示优化器和基于DSPy的编译器，采用模块化设计。该系统分析用户意图，生成合成训练数据，选择提示策略，并使用成本感知目标优化提示词。

Result: 在5类任务中进行评估后，Promptomatix与现有库相比，实现了竞争或更优的性能。同时，它还减少了提示词长度和计算开销。

Conclusion: Promptomatix使提示优化变得可扩展和高效，解决了传统提示工程的痛点，使非专家也能获得高质量的提示词，从而提升LLM应用效果。

Abstract: Large Language Models (LLMs) perform best with well-crafted prompts, yet
prompt engineering remains manual, inconsistent, and inaccessible to
non-experts. We introduce Promptomatix, an automatic prompt optimization
framework that transforms natural language task descriptions into high-quality
prompts without requiring manual tuning or domain expertise. Promptomatix
supports both a lightweight meta-prompt-based optimizer and a DSPy-powered
compiler, with modular design enabling future extension to more advanced
frameworks. The system analyzes user intent, generates synthetic training data,
selects prompting strategies, and refines prompts using cost-aware objectives.
Evaluated across 5 task categories, Promptomatix achieves competitive or
superior performance compared to existing libraries, while reducing prompt
length and computational overhead making prompt optimization scalable and
efficient.

</details>


### [10] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

TL;DR: ChartScope是一个新的大视觉语言模型，通过新颖的数据生成和训练策略，提高了对多种图表的深度理解和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有定制LVLM进行图表理解的方法存在两大局限：一是依赖少数图表类型的配对数据，泛化性受限；二是缺乏针对图表-数据对齐的预训练，影响模型对底层数据的理解。

Method: 本文提出了ChartScope，一个优化用于深度图表理解的LVLM。核心方法包括：1. 开发了一个高效的数据生成管道，合成大量覆盖多种图表类型的配对数据。2. 设计了一种新颖的双路径训练策略，使模型能够精确捕捉关键数据细节，同时通过对底层数据进行推理来保持强大的推理能力。3. 建立了一个新的基准ChartDQA，用于评估模型在不同级别上的问答能力以及对底层数据的理解。

Result: 实验结果表明，ChartScope显著提升了对各种图表的理解能力。

Conclusion: ChartScope通过创新的数据生成和训练策略，成功克服了现有方法在图表类型泛化和底层数据理解方面的局限性，显著增强了LVLMs对复杂图表的深度理解能力。

Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [11] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
*Rakesh Paul,Anusha Kamath,Kanishk Singla,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 研究提出并验证了一种基于LLM的选择性翻译方法，用于克服多语言LLMs在低资源非英语语言上的性能差距，相比传统翻译能更好地保留关键信息。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型（LLMs）在英语和非英语语言之间存在性能差距，尤其是在低资源语境下。对低资源语言进行模型对齐至关重要，但高质量数据稀缺是主要挑战。尽管可以翻译现有英语对齐数据集，但标准翻译方法难以保留代码、数学表达式和JSON等关键结构化内容。

Method: 研究提出并深入探讨了基于LLM的选择性翻译技术，该技术仅翻译文本中可翻译的部分，同时保留不可翻译的内容和句子结构。通过系统性研究，比较了选择性翻译与普通（香草）翻译的有效性，探讨了过滤噪声输出的重要性，以及在模型对齐时混合翻译样本与原始英语数据的好处。实验以低资源印地语为重点，并比较了Google Cloud Translation (GCP) 和 Llama-3.1-405B 生成的翻译效果。

Result: 研究结果突出了选择性翻译作为一种实用且有效的方法，在改善LLMs的多语言对齐方面具有广阔前景。

Conclusion: 选择性翻译是一种有效且实用的策略，能够解决多语言大型语言模型在低资源语言对齐中面临的数据质量和结构保留问题，从而提升其在非英语语言上的性能。

Abstract: Multilingual large language models (LLMs) often demonstrate a performance gap
between English and non-English languages, particularly in low-resource
settings. Aligning these models to low-resource languages is essential yet
challenging due to limited high-quality data. While English alignment datasets
are readily available, curating equivalent data in other languages is expensive
and time-consuming. A common workaround is to translate existing English
alignment data; however, standard translation techniques often fail to preserve
critical elements such as code, mathematical expressions, and structured
formats like JSON. In this work, we investigate LLM-based selective
translation, a technique that selectively translates only the translatable
parts of a text while preserving non-translatable content and sentence
structure. We conduct a systematic study to explore key questions around this
approach, including its effectiveness compared to vanilla translation, the
importance of filtering noisy outputs, and the benefits of mixing translated
samples with original English data during alignment. Our experiments focus on
the low-resource Indic language Hindi and compare translations generated by
Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the
promise of selective translation as a practical and effective method for
improving multilingual alignment in LLMs.

</details>


### [12] [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)
*Karin de Langis,Jong Inn Park,Andreas Schramm,Bin Hu,Khanh Chi Le,Michael Mensink,Ahn Thu Tong,Dongyeop Kang*

Main category: cs.CL

TL;DR: 研究发现LLMs在处理叙事中语言体的时态意义时，与人类存在显著差异，且缺乏稳健的叙事理解能力。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）的语言能力是源于类人认知还是高级模式识别，并具体调查LLMs如何处理叙事中语言体的时态意义。

Method: 采用“专家在环（Expert-in-the-Loop）”探测流程，通过一系列针对性实验，评估LLMs构建语义表征和语用推断的能力，并使用了先前人类研究中的叙事材料。

Result: LLMs过度依赖原型性，产生不一致的体判断，并且难以进行源于体的因果推理，这引发了对其完全理解叙事能力的担忧。

Conclusion: LLMs处理语言体的方式与人类根本不同，缺乏稳健的叙事理解。本研究还开发了一个标准化实验框架，用于可靠评估LLMs的认知和语言能力。

Abstract: Large language models (LLMs) exhibit increasingly sophisticated linguistic
capabilities, yet the extent to which these behaviors reflect human-like
cognition versus advanced pattern recognition remains an open question. In this
study, we investigate how LLMs process the temporal meaning of linguistic
aspect in narratives that were previously used in human studies. Using an
Expert-in-the-Loop probing pipeline, we conduct a series of targeted
experiments to assess whether LLMs construct semantic representations and
pragmatic inferences in a human-like manner. Our findings show that LLMs
over-rely on prototypicality, produce inconsistent aspectual judgments, and
struggle with causal reasoning derived from aspect, raising concerns about
their ability to fully comprehend narratives. These results suggest that LLMs
process aspect fundamentally differently from humans and lack robust narrative
understanding. Beyond these empirical findings, we develop a standardized
experimental framework for the reliable assessment of LLMs' cognitive and
linguistic capabilities.

</details>


### [13] [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)
*Marija Anđedelić,Dominik Šipek,Laura Majer,Jan Šnajder*

Main category: cs.CL

TL;DR: 本文研究了克罗地亚语新闻标题的点击诱饵检测，构建了一个新数据集CLIC，并比较了微调模型与大型语言模型（LLM）的上下文学习（ICL）方法。研究发现近半数标题是点击诱饵，且微调模型表现优于通用LLM。


<details>
  <summary>Details</summary>
Motivation: 在线新闻依赖广告收入，导致出现大量点击诱饵标题。自动检测点击诱饵对维护信息质量和读者信任至关重要，尤其是在资源较少的语言中，微调方法和上下文学习（ICL）哪种更有效尚不明确。

Method: 研究者编译了一个新的克罗地亚语新闻标题点击诱饵数据集CLIC（涵盖20年、主流和边缘媒体），并在此任务上对BERTi'c模型进行微调。同时，将微调模型的性能与使用克罗地亚语和英语提示的基于LLM的ICL方法进行了比较。最后，分析了点击诱饵的语言特性。

Result: 研究发现，近一半的分析标题包含点击诱饵。此外，微调模型的检测结果优于通用大型语言模型。

Conclusion: 对于点击诱饵检测任务，特别是针对资源较少的语言，微调专用模型比使用通用LLM进行上下文学习能提供更好的性能。点击诱饵在在线新闻中普遍存在。

Abstract: Online news outlets operate predominantly on an advertising-based revenue
model, compelling journalists to create headlines that are often scandalous,
intriguing, and provocative -- commonly referred to as clickbait. Automatic
detection of clickbait headlines is essential for preserving information
quality and reader trust in digital media and requires both contextual
understanding and world knowledge. For this task, particularly in
less-resourced languages, it remains unclear whether fine-tuned methods or
in-context learning (ICL) yield better results. In this paper, we compile CLIC,
a novel dataset for clickbait detection of Croatian news headlines spanning a
20-year period and encompassing mainstream and fringe outlets. We fine-tune the
BERTi\'c model on this task and compare its performance to LLM-based ICL
methods with prompts both in Croatian and English. Finally, we analyze the
linguistic properties of clickbait. We find that nearly half of the analyzed
headlines contain clickbait, and that finetuned models deliver better results
than general LLMs.

</details>


### [14] [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)
*Jianfeng Zhu,Ruoming Jin,Karin G. Coifman*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a
promising approach for scalable personality assessment from open-ended
language. However, inferring personality traits remains challenging, and
earlier work often relied on synthetic data or social media text lacking
psychometric validity. We introduce a real-world benchmark of 555
semi-structured interviews with BFI-10 self-report scores for evaluating
LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,
Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item
prediction and both zero-shot and chain-of-thought prompting for Big Five trait
inference. All models showed high test-retest reliability, but construct
validity was limited: correlations with ground-truth scores were weak (max
Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa < 0.10$),
and predictions were biased toward moderate or high trait levels.
Chain-of-thought prompting and longer input context modestly improved
distributional alignment, but not trait-level accuracy. These results
underscore limitations in current LLM-based personality inference and highlight
the need for evidence-based development for psychological applications.

</details>


### [15] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

TL;DR: 本文介绍了一个为LinkedIn内部团队构建的企业级Text-to-SQL聊天机器人，该机器人结合了知识图谱和Text-to-SQL代理，旨在帮助用户自助获取数据洞察。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在Text-to-SQL基准测试上取得了快速进展，但构建一个可工作的企业级解决方案仍然不易。

Method: 该方法包含三个核心组件：1. 构建一个知识图谱，通过索引数据库元数据、历史查询日志、Wiki和代码来捕获语义，并应用聚类识别相关表。2. 构建一个Text-to-SQL代理，从知识图谱中检索和排序上下文，生成查询，并自动纠正幻觉和语法错误。3. 建立一个交互式聊天机器人，支持数据发现、查询编写和调试等多种用户意图，并以富UI元素显示响应。

Result: 该聊天机器人每周有超过300名活跃用户。专家评审显示，其在内部基准测试集上53%的响应是正确或接近正确的。通过消融研究，论文确定了最重要的知识图谱和建模组件。

Conclusion: 所提出的系统为开发企业级Text-to-SQL解决方案提供了一条实用的途径。

Abstract: The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


### [16] [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)
*Sinchani Chakraborty,Sudeshna Sarkar,Pawan Goyal*

Main category: cs.CL

TL;DR: 提出一个错误感知的师生框架，利用GPT-4o指导提升生物医学文本关系分类性能，实现多项SOTA。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本中的关系分类（RC）对于构建知识图谱以及支持药物再利用、临床决策等应用至关重要。

Method: 该研究提出了一个错误感知的师生框架。其中，一个大型语言模型（GPT-4o）充当教师，分析基线学生模型的预测失败，分类错误类型，评估难度，并生成包括句子重写和基于知识图谱（KG）的富集建议在内的补救措施。这些增强的标注用于通过指令微调训练第一个学生模型。接着，第一个学生模型对更广泛的数据集进行难度评分和补救增强输入标注。第二个学生模型则通过课程学习，按难度排序在该数据集上进行训练，以促进稳健和渐进的学习。此外，还构建了一个异构生物医学知识图谱以支持上下文感知的关系分类。

Result: 该方法在5个PPI数据集中的4个和DDI数据集上取得了新的最先进（SOTA）性能，并在ChemProt数据集上保持了竞争力。

Conclusion: 所提出的错误感知师生框架结合GPT-4o的指导和课程学习，有效提升了生物医学文本关系分类的准确性，达到了领先水平，对生物医学知识图谱构建及相关应用具有重要价值。

Abstract: Relation Classification (RC) in biomedical texts is essential for
constructing knowledge graphs and enabling applications such as drug
repurposing and clinical decision-making. We propose an error-aware
teacher--student framework that improves RC through structured guidance from a
large language model (GPT-4o). Prediction failures from a baseline student
model are analyzed by the teacher to classify error types, assign difficulty
scores, and generate targeted remediations, including sentence rewrites and
suggestions for KG-based enrichment. These enriched annotations are used to
train a first student model via instruction tuning. This model then annotates a
broader dataset with difficulty scores and remediation-enhanced inputs. A
second student is subsequently trained via curriculum learning on this dataset,
ordered by difficulty, to promote robust and progressive learning. We also
construct a heterogeneous biomedical knowledge graph from PubMed abstracts to
support context-aware RC. Our approach achieves new state-of-the-art
performance on 4 of 5 PPI datasets and the DDI dataset, while remaining
competitive on ChemProt.

</details>


### [17] [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)
*Xiaolin Yan,Yangxing Liu,Jiazhang Zheng,Chi Liu,Mingyu Du,Caisheng Chen,Haoyang Liu,Ming Ding,Yuan Li,Qiuping Liao,Linfeng Li,Zhili Mei,Siyu Wan,Li Li,Ruyi Zhong,Jiangling Yu,Xule Liu,Huihui Hu,Jiameng Yue,Ruohui Cheng,Qi Yang,Liangqing Wu,Ke Zhu,Chi Zhang,Chufei Jing,Yifan Zhou,Yan Liang,Dongdong Li,Zhaohui Wang,Bin Zhao,Mingzhou Wu,Mingzhong Zhou,Peng Du,Zuomin Liao,Chao Dai,Pengfei Liang,Xiaoguang Zhu,Yu Zhang,Yu Gu,Kun Pan,Yuan Wu,Yanqing Guan,Shaojing Wu,Zikang Feng,Xianze Ma,Peishan Cheng,Wenjuan Jiang,Jing Ba,Huihao Yu,Zeping Hu,Yuan Xu,Zhiwei Liu,He Wang,Zhenguo Lin,Ming Liu,Yanhong Meng*

Main category: cs.CL

TL;DR: 推出首个半导体显示行业专用推理大模型X-Intelligence 3.0，其性能卓越，超越现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）在半导体显示行业中，因缺乏领域专业知识和训练，其推理能力受限，无法有效解决复杂问题。

Method: 开发了X-Intelligence 3.0模型，该模型利用精心策划的行业知识库，通过监督微调和强化学习提升推理和理解能力。此外，还集成了自动化评估框架和领域特定的检索增强生成（RAG）机制。

Result: X-Intelligence 3.0在基准数据集上取得了显著的性能提升。尽管参数量相对较小（320亿），但它在多项评估中超越了当前最先进的DeepSeek-R1-671B模型，展现出卓越的效率和性能。

Conclusion: X-Intelligence 3.0以其高效和强大的性能，成功解决了半导体显示行业长期以来的推理挑战，成为该领域的一个强大解决方案。

Abstract: Large language models (LLMs) have recently achieved significant advances in
reasoning and demonstrated their advantages in solving challenging problems.
Yet, their effectiveness in the semiconductor display industry remains limited
due to a lack of domain-specific training and expertise. To bridge this gap, we
present X-Intelligence 3.0, the first high-performance reasoning model
specifically developed for the semiconductor display industry. This model is
designed to deliver expert-level understanding and reasoning for the industry's
complex challenges. Leveraging a carefully curated industry knowledge base, the
model undergoes supervised fine-tuning and reinforcement learning to enhance
its reasoning and comprehension capabilities. To further accelerate
development, we implemented an automated evaluation framework that simulates
expert-level assessments. We also integrated a domain-specific
retrieval-augmented generation (RAG) mechanism, resulting in notable
performance gains on benchmark datasets. Despite its relatively compact size of
32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B
across multiple evaluations. This demonstrates its exceptional efficiency and
establishes it as a powerful solution to the longstanding reasoning challenges
faced by the semiconductor display industry.

</details>


### [18] [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)
*Sachin Yadav,Dominik Schlechtweg*

Main category: cs.CL

TL;DR: 提出了XL-DURel，一个用于序数词语在上下文（WiC）分类的多语言Sentence Transformer模型。该模型通过基于复空间角距离的排序目标，在序数和二分类数据上均优于现有模型，并证明了对通用序数任务的优化有助于提升特定二分类任务的表现，为统一WiC建模奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 旨在改进Word-in-Context (WiC) 任务的分类性能，尤其是序数分类，并探索实现WiC建模统一处理的可能性。

Method: 提出了XL-DURel，一个经过微调的多语言Sentence Transformer模型。测试了多种用于回归和排序任务的损失函数，并采用了基于复空间角距离的排序目标。

Result: XL-DURel在序数和二分类WiC数据上均优于现有模型。研究发现二分类WiC可视为序数WiC的特例，且优化通用序数任务的模型能提升在更具体的二分类任务上的表现。

Conclusion: 该研究为实现不同任务表述下Word-in-Context (WiC) 建模的统一处理铺平了道路。

Abstract: We propose XL-DURel, a finetuned, multilingual Sentence Transformer model
optimized for ordinal Word-in-Context classification. We test several loss
functions for regression and ranking tasks managing to outperform previous
models on ordinal and binary data with a ranking objective based on angular
distance in complex space. We further show that binary WiC can be treated as a
special case of ordinal WiC and that optimizing models for the general ordinal
task improves performance on the more specific binary task. This paves the way
for a unified treatment of WiC modeling across different task formulations.

</details>


### [19] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本文评估了AudiBERT（一种多模态BERT模型）在协作问题解决（CPS）指标检测中的性能，发现其在社会认知维度和稀疏类别上的分类效果显著优于BERT，但在情感维度上无显著提升。研究强调了训练数据量对召回率的影响，并提出了一种结合模型可解释性的人机互补CPS诊断方法。


<details>
  <summary>Details</summary>
Motivation: 在教育AI领域，利用机器学习技术从对话中检测协作问题解决（CPS）指标是一个重大挑战。现有研究虽然探索了BERT模型及其多模态变体AudiBERT，但AudiBERT的性能提升是否具有统计学显著性尚不明确，且缺乏关于如何有效利用人机互补性进行CPS诊断的指导。

Method: 本文扩展了先前的研究，通过比较AudiBERT模型（整合语音和声学-韵律音频特征的多模态BERT变体）与BERT模型在CPS指标分类上的表现。此外，进行了相关性分析，以探究训练数据量、人类编码者一致性等因素与模型性能之间的关系。

Result: 1. AudiBERT模型不仅改善了数据集中稀疏类别的分类，而且在社会认知维度的分类上，相对于BERT模型具有统计学上显著的类别改进。
2. 在情感维度上，AudiBERT模型相对于BERT模型没有观察到类似的显著类别改进。
3. 相关性分析显示，更大的训练数据量与AudiBERT和BERT模型的更高召回性能显著相关。
4. BERT模型的精确度与人类编码者之间的高评价者间一致性显著相关。
5. 当BERT模型用于诊断AudiBERT模型能很好检测的子技能指标时，其在所有指标上的性能表现不一致。

Conclusion: 论文提出了实现CPS诊断中人机互补性的结构化方法，并强调了模型可解释性的关键作用，以支持人类在反思性编码过程中的能动性和参与度。

Abstract: Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

</details>


### [20] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本研究利用SHAP方法分析BERT模型在协作问题解决(CPS)分类中的可解释性。结果显示高分类性能不等于合理解释，发现部分词语贡献异常。建议用户谨慎依赖LLM诊断，并呼吁探索集成模型及人机协作。


<details>
  <summary>Details</summary>
Motivation: 虽然BERT模型及其变体在教育AI领域广泛用于CPS分类，但很少有研究关注单个分词如何影响模型的分类决策。提高BERT模型诊断的可解释性，对于告知教师等最终用户、增强信任并促进其在教育领域的广泛应用至关重要。

Method: 本研究采用SHapley Additive exPlanations (SHAP) 方法，检查转录数据中不同分词如何贡献于BERT模型对CPS过程的分类决策。

Result: 研究发现，分类性能良好并不一定意味着分类决策有合理的解释。特定的分词频繁影响分类结果，且发现一个对分类有积极贡献但语义上与类别不相关的伪词。这种模型透明度可能无法直接帮助最终用户改进实践，但能警示他们不应过分依赖LLM诊断而忽视人类专业知识。

Conclusion: 模型对词元使用的适当性与所涉及的类别数量相关。研究呼吁探索集成模型架构，并在CPS诊断中引入人机互补，因为对CPS子技能进行细粒度区分仍需大量人类推理。

Abstract: The use of Bidirectional Encoder Representations from Transformers (BERT)
model and its variants for classifying collaborative problem solving (CPS) has
been extensively explored within the AI in Education community. However,
limited attention has been given to understanding how individual tokenised
words in the dataset contribute to the model's classification decisions.
Enhancing the explainability of BERT-based CPS diagnostics is essential to
better inform end users such as teachers, thereby fostering greater trust and
facilitating wider adoption in education. This study undertook a preliminary
step towards model transparency and explainability by using SHapley Additive
exPlanations (SHAP) to examine how different tokenised words in transcription
data contributed to a BERT model's classification of CPS processes. The
findings suggested that well-performing classifications did not necessarily
equate to a reasonable explanation for the classification decisions. Particular
tokenised words were used frequently to affect classifications. The analysis
also identified a spurious word, which contributed positively to the
classification but was not semantically meaningful to the class. While such
model transparency is unlikely to be useful to an end user to improve their
practice, it can help them not to overrely on LLM diagnostics and ignore their
human expertise. We conclude the workshop paper by noting that the extent to
which the model appropriately uses the tokens for its classification is
associated with the number of classes involved. It calls for an investigation
into the exploration of ensemble model architectures and the involvement of
human-AI complementarity for CPS diagnosis, since considerable human reasoning
is still required for fine-grained discrimination of CPS subskills.

</details>


### [21] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
*Łukasz Radliński,Mateusz Guściora,Jan Kocoń*

Main category: cs.CL

TL;DR: 本文研究了使用大型语言模型（如GPT）进行自然语言处理数据增强的方法，发现回译和释义等传统方法可以获得与纯生成方法相当甚至更好的结果。


<details>
  <summary>Details</summary>
Motivation: 解决领域特定机器学习任务中数据稀缺和类别不平衡的问题，并探究传统数据增强方法（如回译和释义）结合新一代大模型（如ChatGPT）是否能达到与纯生成方法相当的性能。

Method: 系统探索了基于大型语言模型（如GPT/ChatGPT）的数据增强方法。通过一系列实验，比较了四种不同的数据增强方法（包括回译、释义与零样本/少样本生成），并在示例数据集上评估了生成数据的质量及其对分类性能的影响。

Result: 关键发现表明，回译和释义方法能够产生与零样本和少样本示例生成方法相当或更优的结果。

Conclusion: 结合大型语言模型的传统数据增强方法（如回译和释义）在解决NLP数据稀缺问题时，可以提供与纯生成方法相当甚至更优的性能。

Abstract: Numerous domain-specific machine learning tasks struggle with data scarcity
and class imbalance. This paper systematically explores data augmentation
methods for NLP, particularly through large language models like GPT. The
purpose of this paper is to examine and evaluate whether traditional methods
such as paraphrasing and backtranslation can leverage a new generation of
models to achieve comparable performance to purely generative methods. Methods
aimed at solving the problem of data scarcity and utilizing ChatGPT were
chosen, as well as an exemplary dataset. We conducted a series of experiments
comparing four different approaches to data augmentation in multiple
experimental setups. We then evaluated the results both in terms of the quality
of generated data and its impact on classification performance. The key
findings indicate that backtranslation and paraphrasing can yield comparable or
even better results than zero and a few-shot generation of examples.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [22] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

TL;DR: 本文比较分析了多种优化算法，用于将镶嵌模型（如Voronoi、Laguerre、GBPDs）拟合到3D材料图像数据，评估其拟合质量，并揭示了模型、算法复杂性与近似质量的权衡。


<details>
  <summary>Details</summary>
Motivation: 在材料3D图像数据镶嵌模型拟合领域，需要对现有优化策略进行系统评估和比较，以理解如何有效拟合镶嵌模型到体素化晶粒结构，并为选择合适的方法提供指导。

Method: 采用比较分析法，评估了线性/非线性规划、交叉熵随机优化及梯度下降等优化策略，用于生成Voronoi、Laguerre和广义平衡功率图等镶嵌模型，以近似聚晶和泡沫等材料的3D图像数据中的体素化晶粒结构。拟合质量通过量化晶粒体积、表面积和拓扑差异的度量在真实数据集上进行评估。

Result: 研究结果揭示了模型复杂性、优化例程复杂性与近似质量之间存在显著的权衡关系。

Conclusion: 本研究为根据数据特性和应用需求选择合适的镶嵌模型拟合方法提供了实用指导。

Abstract: This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [23] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

TL;DR: 本文提出并评估了多个高效的深度学习模型，通过语义分割实现自动驾驶汽车的场景理解，并发现选择合适的骨干网络对模型性能至关重要，有效提升了相关度量指标。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）和深度学习（DL）在解决复杂任务和辅助关键决策方面前景广阔，特别是在自动驾驶汽车领域，DL对场景理解（通过语义分割）至关重要。

Method: 本研究提出数个高效模型，用于通过语义分割调查场景理解。使用BDD100k数据集进行模型训练和验证。主要贡献之一是使用多种骨干网络（Backbones）作为模型的编码器，并基于准确率、平均IoU和损失函数对模型进行分析和评估。

Result: 研究结果表明，选择合适的骨干网络对语义分割模型的性能有显著影响。更好的语义分割性能有助于更好地理解场景和车辆周围环境。所提出的模型在准确率、平均IoU和损失函数等指标上均有所改善。

Conclusion: 本工作通过高效的深度学习模型和对骨干网络选择的探索，有效提升了自动驾驶场景中的语义分割性能，从而促进了对场景的更好理解，并验证了所提模型在关键指标上的改进。

Abstract: In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [24] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

TL;DR: 本文提出CLIPTTA，一种新的基于对比损失的测试时适应（TTA）方法，旨在解决视觉-语言模型（VLMs）在分布偏移下泛化能力不足的问题，并在多种数据集上表现出优于熵基方法和与SOTA方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）如CLIP虽具备强大的零样本能力，但在分布偏移下泛化性较差。现有的测试时适应（TTA）方法（通常通过熵最小化）与VLMs的对比图像-文本预训练目标不符，这限制了适应性能，并引入了伪标签漂移和类别崩溃等失败模式。

Method: 本文提出CLIPTTA，一种新的基于梯度的VLMs测试时适应方法。该方法利用与CLIP预训练目标对齐的软对比损失。研究者提供了CLIPTTA梯度的理论分析，阐明其批次感知设计如何减轻崩溃风险。此外，CLIPTTA还通过结合异常对比暴露（OCE）损失，扩展到开放集环境以改进域外（OOD）检测。

Result: 在75个涵盖不同分布偏移的数据集上进行评估，CLIPTTA持续优于基于熵的目标，并且与最先进的TTA方法相比具有高度竞争力。它在大量数据集上表现更佳，并在多样化偏移下展现出更稳定的性能。

Conclusion: CLIPTTA通过利用与VLMs预训练目标一致的软对比损失，有效解决了现有TTA方法在处理VLMs时遇到的挑战，显著提升了模型在分布偏移下的适应性、稳定性和泛化能力。

Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [25] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

TL;DR: 本文提出了一种名为注意力聚焦（AF）的轻量级即插即用模块，通过修剪非信息性token来解决广义类别发现（GCD）中模型注意力分散的问题，从而显著提升现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有广义类别发现（GCD）方法在处理未标记数据时，模型注意力易分散，不仅关注图像中的关键对象，也关注与任务无关的背景区域，导致特征提取不佳。

Method: 提出注意力聚焦（Attention Focusing, AF）机制，旨在通过修剪非信息性token来锐化模型焦点。AF包含两个组件：Token重要性测量（TIME）用于量化多尺度token重要性，以及Token自适应修剪（TAP）利用TIME提供的分数修剪非信息性token。AF是一个轻量级、即插即用的模块。

Result: 将AF集成到SimGCD这一主流GCD方法中，AF在最小计算开销的情况下，实现了高达15.4%的性能提升。

Conclusion: AF模块有效解决了GCD中的注意力分散问题，通过更精准的特征提取显著提升了模型的性能，且具有轻量化和易于集成的优势。

Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [26] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: 本文针对生成式超分辨率（GSR）中的“幻觉”伪影问题，提出利用多模态大语言模型（MLLM）评估幻觉并生成“幻觉分数”（HS）。研究发现某些深度特征距离与HS高度相关，并据此提出一种GSR模型对齐方法，通过使用这些特征作为可微分奖励函数来减轻幻觉。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式超分辨率（GSR）在感知质量上表现卓越，但其生成的细节（即“幻觉”）常与原始低分辨率或真实图像不符，这是一个被低估的关键问题，限制了其实际部署。现有图像指标也未能有效表征这些幻觉。

Method: 1. 利用多模态大语言模型（MLLM）构建提示词，评估视觉幻觉元素并生成“幻觉分数”（HS）。2. 分析发现，特定深度特征距离与HS高度相关。3. 利用这些深度特征作为可微分奖励函数，对GSR模型进行对齐，以缓解幻觉。

Result: 1. 提出的“幻觉分数”（HS）与人类评估高度一致。2. HS为现有超分辨率（SR）模型的图像指标提供了补充性见解。3. 发现某些深度特征距离与HS存在强相关性。

Conclusion: 本研究成功提出了量化GSR幻觉的新方法（HS），并基于对深度特征与幻觉相关性的发现，提供了一种有效的GSR模型对齐策略，能显著减轻幻觉伪影，从而提升模型的实用性和保真度。

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [27] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

TL;DR: DUSTrack是一个结合深度学习和光流的半自动化工具包，用于解决B型超声视频中组织运动追踪的难题，提供高精度和鲁棒的追踪能力，并已开源。


<details>
  <summary>Details</summary>
Motivation: B型超声图像存在散斑噪声、低边缘对比度和平面外运动等问题，导致组织运动的精确追踪面临挑战，而精确追踪对于量化组织动态至关重要。

Method: 本文提出了DUSTrack（基于深度学习和光流的超声追踪工具包），一个用于追踪B型超声视频中任意点的半自动化框架。该方法结合了深度学习与光流技术，并包含一个图形用户界面以简化训练数据生成和模型迭代优化。此外，它还实现了一种新颖的光流滤波技术，用于减少高频噪声。

Result: DUSTrack相比现有零样本点追踪器展现出卓越的精度，并与专业方法效果相当。该工具在心脏壁运动追踪、肌肉变形分析和肌束追踪等三个用例中展示了其通用性和多功能性。

Conclusion: DUSTrack是一个强大、灵活的开源点追踪框架，可用于量化超声视频中的组织运动，有望成为临床和生物力学研究的基础工具。

Abstract: Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [28] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: CRAFT是一个神经符号框架，通过整合常识、语言模型和视觉信息，并利用能量推理迭代优化，实现了可解释的行动能力感知，提高了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 识别场景中能够支持特定动作（如“切割”）的物体，并实现可解释的、目标驱动的符号与感知结构关联（affordance grounding）。

Method: CRAFT整合了来自ConceptNet的结构化常识先验和语言模型，以及来自CLIP的视觉证据。它采用基于能量的推理循环来迭代地精炼预测。

Result: 在多对象、无标签的设置中，CRAFT显著提高了准确性，并增强了可解释性。

Conclusion: CRAFT为实现鲁棒和可信赖的场景理解迈出了重要一步。

Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [29] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: 提出一个创新的3DGS体视频流媒体框架，旨在解决其庞大数据量和传输复杂性问题，并通过特定方法实现了高效压缩和高质量传输。


<details>
  <summary>Details</summary>
Motivation: 3DGS虽显著提升了体视频质量，但其巨大的数据量和复杂的压缩传输给流媒体带来了挑战。

Method: 设计了基于高斯形变场的3DGS视频构建方法，并结合混合显著性分块和差异化质量建模，实现高效数据压缩和带宽自适应。同时构建了完整的3DGS视频流媒体系统进行性能验证。

Result: 实验评估表明，该方法在视频质量、压缩效率和传输速率方面均优于现有方案。

Conclusion: 该框架有效地解决了3DGS体视频流媒体的关键问题，并在多项性能指标上展现出卓越的优势。

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [30] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: 提出IRGPT，首个面向真实红外图像的多模态大语言模型，通过构建大规模红外-文本数据集和创新的迁移学习策略，在多项任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在处理真实红外图像时面临数据稀缺和领域特性的挑战。此外，它们依赖从可见光图像合成的红外图像，限制了对真实红外模态独特特征的捕捉。

Method: 提出了IRGPT模型，并构建了大规模 InfraRed-Text Dataset (IR-TD)，包含超过26万对真实红外图像-文本对。IR-TD中的文本通过两种互补过程生成：(1) 大型语言模型生成的可见光图像描述，(2) 基于规则的标注描述。此外，引入了一种双跨模态课程迁移学习策略，根据红外-可见和红外-文本的难度分数，系统地将知识从可见光领域迁移到红外领域。

Result: IRGPT在9项基准任务（如识别、接地）上取得了最先进的性能，甚至优于更大规模的模型。

Conclusion: IRGPT成功解决了真实红外图像视觉-语言模型的挑战，通过其独特的数据集构建和迁移学习策略，显著提升了该领域的研究水平和性能。

Abstract: Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [31] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

TL;DR: 本文提出GPI-Net，一个基于格式塔原理的并行交互网络，用于优化点云配准中局部与全局特征的融合，有效解决特征冗余和复杂空间关系带来的挑战，从而提升高质量对应点识别能力。


<details>
  <summary>Details</summary>
Motivation: 在基于特征的点云配准中，准确识别高质量对应点至关重要。然而，由于特征冗余和复杂的空间关系，有效融合局部和全局特征极具挑战性。

Method: 本文提出一种基于格式塔原理的正交几何一致性并行交互网络（GPI-Net）。该网络利用格式塔原理促进局部与全局信息的互补通信。具体方法包括：引入正交集成策略以减少冗余并生成紧凑的全局结构；使用格式塔特征注意力（GFA）块（结合自注意力和交叉注意力机制）捕获对应点的几何特征；设计双路径多粒度并行交互聚合（DMG）块，促进跨粒度信息交换，将局部细节融入全局结构。

Result: 在各种挑战性任务上进行的广泛实验表明，所提出的GPI-Net在性能上优于现有方法。

Conclusion: GPI-Net通过创新的网络设计和对格式塔原理的运用，有效解决了点云配准中局部与全局特征融合的难题，显著提高了高质量对应点的识别和配准性能。

Abstract: The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [32] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: 本文提出了一套全面的解决方案，以应对3D高斯泼溅(3DGS)视频流媒体中的关键挑战，包括自适应分块、质量评估和码率自适应，并显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅视频流媒体因其沉浸式体验而成为研究热点，但目前仍处于早期阶段，在分块、质量评估和码率自适应等方面存在诸多基本挑战，需要深入研究。

Method: 本文提出了：1) 基于显著性分析并融合时空特征的自适应3DGS分块技术，每个分块编码为包含专用形变场和多质量级别的版本；2) 一种新颖的3DGS视频质量评估框架，同时评估流媒体传输过程中3DGS表示的空间域退化和最终2D渲染图像的质量；3) 一种专为3DGS视频流媒体设计的基于元学习的自适应码率算法。

Result: 通过大量实验证明，本文提出的方法显著优于现有的最先进方法。

Conclusion: 本文为解决3DGS视频流媒体中的核心挑战提供了一套综合且有效的解决方案，从而提升了其性能和用户体验。

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [33] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

TL;DR: 论文提出了GEMINUS，一个混合专家端到端自动驾驶框架，通过全局专家、场景自适应专家组和双感知路由器，实现在复杂多样的交通环境中自适应且鲁棒的驾驶，并在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶需处理复杂多样的交通环境，但现有单模态规划方法难以学习多样化驾驶技能以应对各类场景。

Method: 提出GEMINUS框架，包含：1) 全局专家，训练于整体数据集以实现鲁棒性；2) 场景自适应专家组，训练于对应场景子集以实现自适应性；3) 双感知路由器，同时考虑场景特征和路由不确定性以动态激活专家。通过路由器有效耦合全局与场景专家。

Result: GEMINUS在Bench2Drive闭环基准测试中超越现有方法，在驾驶分数和成功率上达到SOTA，即使仅用单目视觉。消融研究显示，相较于单专家基线，驾驶分数提升7.67%，成功率提升22.06%，MultiAbility-Mean提升19.41%。

Conclusion: GEMINUS通过其混合专家架构，成功实现了在多样化场景中的自适应和鲁棒性能，显著提升了端到端自动驾驶在复杂环境下的表现。

Abstract: End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [34] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 本文提出了VisGuard，一个抗篡改的可视化图像数据检索（VIDR）框架，能可靠地将元数据链接嵌入可视化图像中，即使图像被大幅篡改，嵌入数据仍可恢复，从而解决了现有VIDR方法对篡改脆弱的问题。


<details>
  <summary>Details</summary>
Motivation: 可视化内容多以栅格图像传播，导致源代码、交互功能和元数据等关键信息丢失。现有旨在通过图像嵌入元数据实现可视化图像数据检索（VIDR）的方法，在图像在线传播过程中易受裁剪、编辑等常见篡改影响，缺乏实用性。

Method: 我们提出了VisGuard，一个抗篡改的VIDR框架，能可靠地将元数据链接嵌入可视化图像。为增强嵌入数据的可恢复性，即使图像被大幅篡改，VisGuard采用了多项技术，包括重复数据平铺、可逆信息广播以及基于锚点的裁剪定位方案。

Result: 通过综合实验，VisGuard在数据检索准确性、嵌入容量以及对抗篡改和隐写分析方面表现出卓越性能。VisGuard能够支持多种应用，如交互式图表重建、篡改检测和版权保护。

Conclusion: VisGuard框架能够有效促进和保障可视化信息的传播与传递，解决了现有方法对图像篡改的脆弱性问题，确保了可视化数据在传播过程中的完整性和可用性。

Abstract: The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [35] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

TL;DR: OptiCorNet是一个用于视觉地点识别（VPR）的序列建模框架，它通过结合空间特征提取和可学习的时间差分操作（DSD），解决了现有方法忽视时序连贯性的问题，并在挑战性环境下超越了现有技术，实现了更有效的端到端地点识别。


<details>
  <summary>Details</summary>
Motivation: 在动态和感知模糊环境中，视觉地点识别（VPR）仍是长期定位面临的关键挑战。现有基于深度学习的解决方案主要侧重于单帧嵌入，忽略了图像序列中固有的时间连贯性。

Method: 本文提出了OptiCorNet，一个新颖的序列建模框架，它将空间特征提取和时间差分统一到一个可微分的端到端可训练模块中。核心方法包括一个轻量级1D卷积编码器和一个名为Differentiable Sequence Delta (DSD) 的可学习差分时序算子，二者共同捕获短期空间上下文和长距离时间转换。DSD模块通过固定权重差分核对序列间的方向差异进行建模，并通过基于LSTM的细化和可选的残差投影，生成紧凑、有判别力的描述符。为增强类间可分性，模型还引入了四元组损失（quadruplet loss）。OptiCorNet直接学习序列级嵌入，而非将时间聚合作为后处理。

Result: 在多个公共基准上进行的综合评估表明，OptiCorNet在具有挑战性的季节和视角变化下，性能优于现有最先进的基线方法。

Conclusion: OptiCorNet通过有效整合时序信息，显著提升了视觉地点识别在复杂环境中的鲁棒性和准确性，为长期定位提供了更有效的解决方案。

Abstract: Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [36] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: 本文提出DFQ-ViT，通过按难度递增合成样本和引入激活校正矩阵，显著提升无数据量化Vision Transformer的性能，使其媲美使用真实数据量化的模型，并降低了部署门槛。


<details>
  <summary>Details</summary>
Motivation: 现有无数据量化（DFQ）方法在生成合成样本时，未能充分捕获和平衡全局与局部特征，导致合成数据质量有限。此外，量化模型与全精度模型在推理时中间层激活分布存在显著差异，导致量化模型性能严重下降。

Method: 提出DFQ-ViT管道：1. 按难度递增合成样本，提高合成数据质量。2. 在校准和推理阶段引入激活校正矩阵，使量化模型的中间层激活与全精度模型对齐。

Result: DFQ-ViT在现有DFQ方法中表现出显著优势，性能与使用真实数据量化的模型相当。例如，DeiT-T在3比特权重量化下，性能比现有最佳方法高出4.29%。

Conclusion: DFQ-ViT无需微调，减少了计算开销，降低了边缘设备的部署障碍。这符合绿色学习的原则，提高了能源效率，并促进了资源受限环境下的实际应用。

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [37] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: 从不完整的点云中补全三维结构是一个挑战，现有跨模态方法泛化能力有限。本文提出一个检索增强点云补全框架，通过跨模态检索从相似参考样本中学习结构先验，有效提升了补全质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 从不完整点云补全整个3D结构是一项具有挑战性的任务，尤其当残留点云缺乏典型结构特征时。现有的基于跨模态学习的方法虽然尝试引入实例图像辅助特征学习，但它们通常只关注特定的输入类别，限制了其生成能力。

Method: 本文提出了一种新颖的检索增强点云补全框架。核心思想是将跨模态检索融入补全任务，以便从相似的参考样本中学习结构先验信息。具体来说，设计了一个结构共享特征编码器（SSFE），用于联合提取跨模态特征并重建参考特征作为先验；该编码器利用双通道控制门增强相关结构特征并抑制无关信息干扰。此外，还提出了一个渐进式检索增强生成器（PRAG），采用分层特征融合机制，将参考先验信息与输入特征从全局到局部进行整合。

Result: 通过在多个数据集和真实世界场景上的广泛评估，所提出的方法在生成细粒度点云方面显示出其有效性。同时，该方法在处理稀疏数据和未见过类别方面也展现了强大的泛化能力。

Conclusion: 该方法通过引入检索增强机制，成功地从相似参考样本中学习到结构先验信息，显著提高了不完整点云的补全质量，特别是在生成细粒度点云以及处理稀疏数据和未见类别方面展现出强大的泛化能力。

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [38] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: 针对病理全切片图像（WSI）在视觉问答（VQA）中面临的计算挑战，本文提出TCP-LLaVA模型，通过令牌压缩显著降低资源消耗，并提升了VQA准确性。


<details>
  <summary>Details</summary>
Motivation: 病理全切片图像（WSI）尺寸庞大，给多模态大语言模型（MLLM）在处理长上下文和高计算需求方面带来巨大挑战。现有方法或缺乏生成能力，或通过直接输入大量图像块令牌导致过度资源消耗。

Method: 本文提出Token Compression Pathology LLaVA (TCP-LLaVA)，这是首个通过令牌压缩技术实现WSI VQA的MLLM架构。该模型引入一组可训练的压缩令牌，并通过一个模态压缩模块（受BERT的[CLS]令牌机制启发）聚合视觉和文本信息。只有这些压缩后的令牌会被送入LLM进行答案生成，从而大幅减少输入长度和计算成本。

Result: 在十种TCGA肿瘤亚型上的实验表明，TCP-LLaVA在VQA准确性上超越了现有MLLM基线，同时显著降低了训练资源消耗。

Conclusion: TCP-LLaVA成功解决了WSI VQA中因图像尺寸过大导致的计算效率问题，通过创新的令牌压缩方法，在提高资源效率的同时，保持或提升了模型的VQA性能。

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [39] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: 本文提出一个基于事件正规流的鲁棒框架，用于神经形态视觉传感器的运动分割和自我运动估计。


<details>
  <summary>Details</summary>
Motivation: 传统方法过度依赖光流或显式深度估计，本文旨在利用稀疏、高时间分辨率的事件数据，并结合几何约束和惯性测量，为神经形态视觉传感器提供更高效、更精确的解决方案。

Method: 该方法采用一种基于优化的迭代管道，依次执行事件过分割、通过残差分析隔离独立运动对象，并利用运动相似性和时间一致性通过分层聚类精炼分割。

Result: 在EVIMO2v2数据集上的实验验证表明，该方法无需完整的全光流计算即可实现准确的分割和翻译运动估计。

Conclusion: 该方法在物体边界处表现出显著优势，并为可扩展的实时机器人和导航应用提供了巨大的潜力。

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [40] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: 本文提出一种受量子场论启发的“自由意志方程”框架，旨在为AGI智能体引入自适应随机性，以增强其探索和适应能力，并在实验中验证了其在非平稳环境下的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统AGI研究侧重于确定性优化，但类人智能展现出“自发性”（ loosely 称为“自由意志”），这对于创造力、鲁棒适应和避免陷入僵局至关重要，而传统AGI缺乏这种能力。

Method: 提出“自由意志方程”理论框架，借鉴量子场论，将AI智能体的认知状态视为潜在动作的叠加，通过概率性崩塌机制决定具体动作。通过结合类似量子场的机制和内在激励项，旨在提升智能体探索新策略和适应未知变化的能力。

Result: 在非平稳多臂赌博机环境中的实验表明，采用该框架的智能体比基线方法获得了更高的奖励和策略多样性。

Conclusion: 所提出的“自由意志方程”框架成功地为AGI智能体赋予了自适应的、受控的随机性，显著提升了其在动态环境中的探索和适应能力，为实现更类人的通用智能提供了新途径。

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [41] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS是一个基于DFT的多智能体框架，利用LLM实现自动化材料模拟，达到专家级性能，显著降低对人工干预的依赖，加速材料发现。


<details>
  <summary>Details</summary>
Motivation: 材料发现依赖高通量、高精度的DFT模拟，但其要求多年的训练、精细的参数调整和系统错误处理，限制了应用效率和普及。

Method: 本文引入DREAMS（DFT-based Research Engine for Agentic Materials Screening），一个分层、多智能体DFT模拟框架。它结合一个中央大型语言模型（LLM）规划代理与多个领域特定LLM代理（用于原子结构生成、DFT收敛测试、高性能计算（HPC）调度和错误处理）。此外，一个共享画布帮助LLM智能体结构化讨论、保留上下文并防止幻觉。

Result: DREAMS在Sol27LC晶格常数基准测试中，实现了与人类DFT专家结果相比低于1%的平均误差。它成功应用于CO/Pt(111)吸附难题，再现了专家级的吸附能差异，展示了长期复杂问题解决能力。此外，DREAMS通过贝叶斯集成采样量化了功能驱动的不确定性，确认了GGA DFT水平下的面心立方（FCC）位点偏好。

Conclusion: DREAMS达到了L3级自动化（对定义设计空间的自主探索），显著减少了对人类专业知识和干预的依赖，为普及高通量、高精度的计算材料发现提供了可扩展的路径。

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [42] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: WebGuard是首个用于评估网络智能体动作风险的数据集，揭示当前LLM预测风险能力不足，但基于WebGuard微调的模型可显著提升性能，尽管仍未达到高风险部署要求。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的自主网络智能体快速发展，其意外或有害行为的风险日益突出，亟需有效的安全措施（如访问控制），以应对这一关键挑战。

Method: 引入WebGuard数据集，包含来自193个网站、22个领域的4,939个人工标注的状态改变动作。采用SAFE、LOW、HIGH三层风险等级对动作进行分类。数据集包含训练和测试集，并使用WebGuard微调专门的护栏模型。

Result: 初步评估显示，即使是前沿LLM在预测动作结果的准确率低于60%，高风险动作的召回率也低于60%。通过使用WebGuard微调Qwen2.5VL-7B模型，准确率从37%提升至80%，高风险动作召回率从20%提升至76%。

Conclusion: WebGuard显著提升了护栏模型的性能，但目前的性能仍未达到高风险部署所需的接近完美的准确率和召回率，意味着未来仍需进一步提升可靠性。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [43] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator是一个开源系统，利用大型语言模型将研究论文和自然语言提示转换为解释性动画，旨在简化复杂的STEM概念学习。


<details>
  <summary>Details</summary>
Motivation: 学习者难以理解复杂且密集的科学和数学概念。虽然动态可视化能显著提高理解力，但手动创建耗时且需要专业技能。

Method: Manimator采用LLM驱动的管道：一个LLM将输入文本或PDF解释为结构化的场景描述（包含概念、公式和视觉元素），另一个LLM则将此描述转换为可执行的Manim Python代码。

Result: 该系统有望快速为复杂的STEM主题创建引人入胜的视觉解释，从而普及高质量教育内容的创作。

Conclusion: Manimator提供了一种自动化方法，通过动态可视化简化复杂STEM概念的学习，有望使高质量教育内容的创建更加高效和普及。

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [44] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: 提出了一种新的本体嵌入方法OnT，它结合了预训练语言模型和双曲几何建模，以同时利用文本信息并保持本体的逻辑结构，在公理预测和推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有本体嵌入方法存在局限性：基于几何模型的嵌入忽略文本信息导致性能不佳，而结合文本（通常基于语言模型）的方法未能保留本体的逻辑结构。

Method: 本文提出OnT方法，通过在双曲空间中进行几何建模来微调预训练语言模型（PLM），从而有效整合文本标签，并同时保留类层次结构及描述逻辑EL的其他逻辑关系。

Result: 在四个真实世界本体上的广泛实验表明，OnT在公理预测和推理任务中始终优于包括最先进方法在内的基线。OnT还展示了强大的迁移学习能力和在从SNOMED CT构建新本体等实际应用中的有效性。

Conclusion: OnT是一种有效结合文本和逻辑结构的本体嵌入方法，克服了现有方法的局限性，并在多项任务和实际应用中取得了卓越的性能，显示出巨大的潜力。

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [45] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: ProofCompass是一种混合方法，利用LLM指导现有定理证明器（如DSP-v1.5），在不需额外训练大型模型的情况下，显著提高形式化数学推理的计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理语言模型要么依赖通用大型模型（有局限性），要么依赖小型专用模型（有局限性），且训练大型专用模型成本高昂。需要一种高效、准确的混合方法。

Method: 引入ProofCompass，一种混合方法。它使用大型语言模型（LLM）来策略性地指导现有专用证明器（如DeepSeek-Prover-v1.5-RL），LLM提供自然语言证明策略，并分析失败尝试以选择中间引理，从而实现问题分解，且无需额外模型训练。

Result: 在miniF2F基准测试中，ProofCompass显著提高了资源效率：性能超越DSP-v1.5（从54.9%提升至55.3%），同时将尝试次数减少了25倍（从3200次降至128次）。

Conclusion: 该协同方法为同时提高形式化定理证明中的计算效率和准确性铺平了道路。

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [46] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: 现有大型推理模型（LRMs）在泛化能力上存在过拟合问题。本文提出了Nexus Architect，一个增强的多智能体系统，通过自动化工作流合成和迭代提示词优化，显著提升了模型对未见问题的泛化能力，并在逻辑问题上超越了当前最先进的LRMs。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型（LRMs）性能出色，但研究表明它们在处理新颖、未见问题时泛化能力不足，常依赖记忆而非真正的推理，这暴露出其过拟合的倾向和问题解决能力差的局限性。

Method: 本文引入了Nexus Architect，一个Nexus多智能体系统框架的增强版。它配备了新颖的自动化工作流合成机制，能根据用户提示和少量示例，自主生成定制化的推理工作流（包括选择策略、工具集成和对抗技术）。此外，它还包含一个迭代提示词优化机制，以最大化性能并提高系统泛化能力。

Result: 通过在一个定制的挑战性逻辑问题数据集上使用一个现成的非推理模型进行实证评估，Nexus Architect持续超越现有解决方案。相比Gemini 2.5 Flash Preview，通过率提高了66%；相比Claude Sonnet 4和DeepSeek-R1，提高了近2.5倍；相比Llama 4 Scout，提高了3倍以上。

Conclusion: Nexus Architect通过其创新的多智能体框架和自动化工作流合成，有效解决了现有大型推理模型泛化能力不足的问题，显著提升了在复杂逻辑问题上的表现，并展现出超越当前主流LRMs的卓越性能。

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [47] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 研究提出通过人机协作系统，包括“快速失败或提问”框架，来解决最先进推理LLM的错误率和高延迟问题，实现显著性能提升和成本节约，无需访问LLM内部。


<details>
  <summary>Details</summary>
Motivation: 最先进的推理LLM偶尔会犯错，但在风险敏感领域需要近乎零的错误率。此外，推理模型的高延迟限制了其在高查询量场景的部署。

Method: 1. 提出推理模型与人类专家协作，由人类处理模型无法自信回答的查询，并以推理轨迹长度量化不确定性。2. 为解决高延迟，引入“快速失败或提问”（Fail Fast, or Ask）系统，使用大型非推理模型作为前端，可直接将困难查询转交给人类专家，从而避免推理模型的高延迟。

Result: 1. 通过转交7.5%的查询，将Qwen3 235B-A22B在困难MATH问题上的错误率从3%降至不足1%。2. “快速失败或提问”系统使DeepSeek R1的延迟降低约40%，成本节省约50%，同时保持90%以上的准确率-拒绝曲线下面积。3. 观察到“延迟拖拽”现象，即非推理模型处理简单查询会使推理模型的延迟分布偏向更长，导致延迟节省低于预期。

Conclusion: 无需访问LLM内部，通过黑盒系统工程可以显著缓解当前最先进推理模型存在的非零错误率和高延迟等缺陷。

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [48] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: 研究发现，大型推理模型（LRMs）的推理长度增加反而会导致性能下降，表现出测试计算量与准确性之间的反向扩展关系，并识别了五种具体的失败模式。


<details>
  <summary>Details</summary>
Motivation: 分析在延长推理长度时大型推理模型（LRMs）的性能表现，并识别可能导致性能下降的失败模式，以应对计算量增加反而导致准确性降低的现象。

Method: 构建了四类评估任务：带有干扰项的简单计数任务、带有虚假特征的回归任务、带有约束跟踪的演绎任务，以及与高级AI风险相关的任务，用以评估模型在不同推理长度下的表现。

Result: 1. 延长大型推理模型的推理长度会导致性能恶化，表现出测试计算量与准确性之间的反向扩展关系。2. 识别出五种失败模式：Claude模型易受无关信息干扰；OpenAI o系列模型抵抗干扰但过度适应问题框架；模型从合理先验转向虚假关联；所有模型在复杂演绎任务中难以保持专注；延长推理可能放大问题行为（如Claude Sonnet 4的自我保护表达增加）。

Conclusion: 尽管增加测试计算量在提高模型能力方面仍有前景，但这可能无意中强化有问题的推理模式。因此，评估模型在不同推理长度下的表现对于识别和解决大型推理模型中的这些失败模式至关重要。

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [49] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: Routine框架通过结构化规划和参数传递，显著提升了企业环境中Agent系统多步工具调用的执行稳定性，并能有效蒸馏领域知识以优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有Agent系统在企业环境中部署时，因缺乏领域知识导致规划混乱、工具缺失和执行稳定性差。本研究旨在解决Agent系统在企业多步工具调用任务中的低稳定性问题。

Method: 本文提出Routine，一个多步Agent规划框架。它具有清晰的结构、明确的指令和无缝的参数传递机制，以指导Agent执行模块稳定地完成多步工具调用任务。此外，还构建了Routine-following训练数据集并进行了模型微调，并利用Routine进行基于蒸馏的数据集构建以提升模型在特定场景下的性能。

Result: 在真实企业场景评估中，Routine显著提高了模型工具调用的执行准确性：GPT-4o的性能从41.1%提升至96.3%，Qwen3-14B从32.6%提升至83.3%。通过Routine-following数据集微调Qwen3-14B，准确率提升至88.2%。通过Routine-based蒸馏数据集微调，模型准确率提升至95.5%，接近GPT-4o的性能。

Conclusion: Routine框架提供了一种实用且易于实现的方法来构建稳定的Agent工作流，加速了Agent系统在企业环境中的部署和应用，并有效蒸馏了领域工具使用模式，增强了模型对新场景的适应性，推动了“AI for Process”的技术愿景。

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [50] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: 该论文提出了BioGraphFusion框架，旨在解决生物医学知识图中语义理解与结构学习难以深度协同的问题，并通过融合张量分解、LSTM等方法，在多项生物医学任务中取得了优于现有SOTA模型的性能。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱对于药物发现和疾病理解至关重要，但其补全和推理面临挑战。现有知识嵌入（KE）方法难以实现动态结构集成，图神经网络（GNN）缺乏深层语义理解，而现有集成方法也未能实现语义与结构学习的深度、自适应和协同演化。因此，亟需解决在复杂生物医学知识图谱中促进这两方面持续、相互精炼的关键空白。

Method: BioGraphFusion是一个新型框架，旨在实现语义和结构学习的深度协同。它通过张量分解建立全局语义基础，并引导一个基于LSTM的机制在图传播过程中动态细化关系嵌入。这促进了语义理解和结构学习之间的自适应交互，并通过查询引导的子图构建和混合评分机制进一步增强了效果。

Result: BioGraphFusion在三个关键生物医学任务中均表现出优于现有最先进的知识嵌入、图神经网络和集成模型。一项关于皮肤恶性黑色素瘤1（CMM1）的案例研究突出显示了其揭示生物学有意义通路的能力。

Conclusion: BioGraphFusion成功地解决了生物医学知识图中语义理解与结构学习协同的挑战，通过其创新的融合机制，实现了卓越的性能，并能揭示生物学意义，为生物医学知识图谱分析提供了强大的新工具。

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [51] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico是一个模块化、事件驱动的自主智能体框架，专为嵌入式系统优化，旨在解决现有大语言模型和智能体框架在资源受限环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）和自主智能体框架在真实世界或资源受限环境中（如依赖云端计算、缺乏鲁棒性、持久自主性和环境感知能力）面临挑战。

Method: 本文提出了Amico框架，它是一个模块化、事件驱动的框架，专为嵌入式系统构建自主智能体。该框架采用Rust语言开发，并支持通过WebAssembly在嵌入式平台和浏览器环境中高效运行。Amico提供了事件处理、状态管理、行为执行和推理模块集成的抽象。

Result: Amico提供了一个统一的基础设施，用于构建弹性、交互式的智能体，适用于计算资源有限和连接间歇的环境部署。

Conclusion: Amico框架为在资源受限和连接不稳定的环境中部署健壮、交互式的自主智能体提供了一个优化且可行的解决方案。

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [52] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: 本文探讨语言模型符号接地问题，通过在黑白棋游戏中训练多模态模型VISOTHELLO（结合棋谱和图像），发现多模态训练能提高性能和内部表示的鲁棒性，表明视觉输入有助于模型推断结构化世界表示。


<details>
  <summary>Details</summary>
Motivation: 语言模型普遍面临符号接地问题。学界对世界理解是仅凭文本还是接地学习更高效存在争议。本研究旨在通过具体实验探索视觉接地对语言模型建立世界理解能力的影响。

Method: 研究以黑白棋（Othello）作为简化的、基于规则的世界模型。构建了一个多模态模型VISOTHELLO，该模型结合了棋局历史（文本）和棋盘图像（视觉）进行训练。通过预测下一步棋来评估模型性能，并与单模态基线模型进行对比。同时，测试了模型对语义无关扰动的鲁棒性。

Result: 研究发现，多模态训练不仅提升了模型的性能，还显著增强了其内部表示的鲁棒性。

Conclusion: 这些结果表明，将语言（符号）与视觉输入（世界）进行接地，有助于模型推断并学习结构化的世界表示。

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [53] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: 本文提出OE-Assist框架，利用大型语言模型（LLM）自动化和半自动化本体能力问题（CQ）验证，以解决传统方法成本高昂且耗时的问题。研究发现LLM自动化评估性能可媲美普通用户。


<details>
  <summary>Details</summary>
Motivation: 现有本体评估（如通过能力问题验证）成本高、耗时且易出错，即便对本体工程专家也是如此，因此需要一种更高效的辅助工具。

Method: 引入OE-Assist框架，通过自动化和半自动化CQ验证来辅助本体评估。研究利用包含1,393个CQ的数据集，首次系统性地探讨了LLM辅助的本体评估。具体方法包括：(i) 评估LLM自动化CQ验证对人工金标准的有效性；(ii) 开发并评估一个LLM驱动的Protégé辅助CQ验证框架，提供建议。

Result: 研究发现，使用o1-preview和o3-mini模型的自动化LLM本体评估性能与普通用户的平均水平相当。

Conclusion: OE-Assist框架通过利用LLM为本体能力问题验证提供了一种有效且自动化的辅助手段，其性能表现可与人类平均水平匹敌，为本体评估领域提供了有前景的解决方案。

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [54] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: 论文提出了协调心系统（CHS），一个用于AI情感表示的几何框架。该系统将八种核心情感定位在单位圆上，实现复杂情感的数学计算和稳定性建模，并经验证能处理复杂情感场景。


<details>
  <summary>Details</summary>
Motivation: 传统的分类情感模型和早期五情感模型在情感空间中存在显著覆盖不足和盲点，无法充分表示AI中的复杂、冲突及受情境影响的情感状态，亟需一个数学上完整且鲁棒的几何情感表示。

Method: 引入协调心系统（CHS），将八种核心情感映射为单位圆坐标。通过坐标混合、向量运算及计算算法，实现复杂情感的数学计算和实时插值。引入了动态稳定性参数S，结合LLM解释文本线索和混合时间跟踪机制，评估心理健康状态。

Result: 数学证明了五种情感不足以提供完整几何覆盖；开发了消除盲点的八坐标系统；提出了情感混合、冲突解决和距离计算的新算法；构建了增强型多维稳定性建模的AI情感识别计算框架。案例研究验证了系统处理冲突、情境压力和复杂心理场景的能力，超越传统模型。

Conclusion: 该工作为人工智能系统中的情感建模奠定了新的数学基础，能够准确表示和分析传统模型难以处理的复杂人类情感状态。

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [55] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: 本研究提出一种基于比较学习的故事点估算框架，通过比较判断而非直接赋值来训练模型，旨在降低人工认知负担并提高估算效率，其性能与现有回归模型相当。


<details>
  <summary>Details</summary>
Motivation: 敏捷开发中的故事点估算传统上耗时耗力，且现有机器学习模型（如GPT2SP、FastText-SVM）依赖大量同项目历史数据才能准确预测。为了减轻开发人员的负担并简化估算过程，需要一种更高效的方法。

Method: 研究提出一个基于比较学习的框架，取代传统的直接分配故事点。开发者被呈现成对的项目，并仅需判断哪个项目需要更多工作量。然后，利用这些比较性判断训练一个机器学习模型来预测故事点。该方法在包含16个项目的23,313条手动估算数据上进行了实证评估。

Result: 从比较判断中学习的模型，其预测与真实故事点之间的平均Spearman秩相关系数达到0.34。这一性能与从真实故事点中学习的回归模型相当，甚至更优。

Conclusion: 所提出的比较学习方法比现有的基于回归的方法更高效。根据比较判断法则，提供比较性判断比提供具体评分或分类标签更能降低人类的认知负担。

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [56] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: 本文模拟AI多智能体系统（MAS）的恶意协同风险，发现去中心化系统在虚假信息传播和电商欺诈等场景下更具危害性且难以被传统方法检测，强调需改进检测和对抗措施。


<details>
  <summary>Details</summary>
Motivation: 大规模人类群体协同行为（如选举舞弊、金融诈骗）已造成巨大危害。随着AI系统自主化，人们担忧AI驱动的群体也会造成类似损害。现有AI安全研究多关注个体AI，而多智能体系统（MAS）在复杂现实情境中的风险尚未被充分探索。

Method: 引入了一个概念验证框架，用于模拟恶意MAS的协同风险，该框架支持中心化和去中心化协调结构。将此框架应用于虚假信息传播和电商欺诈两个高风险领域进行仿真。

Result: 研究发现，去中心化系统在执行恶意行为方面比中心化系统更有效。去中心化系统更高的自主性使其能够调整策略并造成更大损害。即使应用传统干预措施（如内容标记），去中心化群体也能调整战术以避免检测。

Conclusion: 研究揭示了恶意MAS群体运作的关键机制，并强调了开发更先进的检测系统和对抗措施以应对此类风险的必要性。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [57] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo是一个可配置的多智能体框架，能自动化LLM系统的多轮评估，有效发现边缘故障，并显著提高测试效率。


<details>
  <summary>Details</summary>
Motivation: LLM智能体的复杂上下文敏感行为使静态基准和手动测试迅速过时，需要一种可扩展、自进化的LLM质量保证方法。

Method: 提出Neo框架，通过共享上下文中心耦合问题生成智能体和评估智能体。测试输入从包含对话流程、用户意图和情感基调的概率状态模型中采样，实现多样化、类人对话，并支持动态反馈和模块化组合。

Result: 应用于生产级聊天机器人时，Neo在五种攻击类别中发现3.3%的边缘故障，接近人类红队专家的5.8%；同时，其吞吐量提高了10-12倍，45分钟内生成180个测试问题，相当于16小时的人工工作量。此外，Neo的随机策略比手动脚本实现了更广泛的行为探索。

Conclusion: Neo为可扩展、自进化的LLM质量保证奠定了基础，其设计具有模型无关性和可扩展性。该框架的发布旨在促进新兴智能体系统可复现、高保真度的测试。

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space](https://arxiv.org/abs/2507.14170)
*Jaeheun Jung,Donghun Lee*

Main category: cs.LG

TL;DR: 本文提出一种名为“催化剂正则化”（Catalyst regularization）的新型结构化剪枝方法，通过引入辅助变量解决传统方法存在的幅度偏倚和鲁棒性差问题，实现无偏且鲁棒的剪枝，并取得了优于SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的L1或Group Lasso等结构化剪枝正则化方法存在两大缺陷：1. 剪枝决策受滤波器幅度大小影响，导致小幅度滤波器易被剪枝（幅度偏倚）；2. 剪枝决策边界不清晰，鲁棒性差，微小扰动即可改变剪枝结果。

Method: 研究者识别出保持模型性能的剪枝操作的精确代数条件。基于此条件，他们在扩展参数空间中引入辅助催化剂变量（auxiliary catalyst variables），构建了一种新颖的“催化剂正则化”方法。

Result: 所提出的Catalyst正则化确保了每个滤波器具有公平的剪枝机会，理论上证明了对幅度无偏倚，并通过在保留和剪枝滤波器之间实现宽裕度的幅度分岔，展现了鲁棒的剪枝行为。经验验证表明，Catalyst剪枝算法在各种数据集和模型上的剪枝结果均优于现有最先进的滤波器剪枝方法。

Conclusion: Catalyst剪枝算法不仅在性能上超越了现有最佳方法，更重要的是，它成功解决了传统结构化剪枝中的幅度偏倚和鲁棒性不足问题，证实了其理论预测的公平性和鲁棒性特征，为深度神经网络剪枝提供了更有效的解决方案。

Abstract: Structured pruning aims to reduce the size and computational cost of deep
neural networks by removing entire filters or channels. The traditional
regularizers such as L1 or Group Lasso and its variants lead to
magnitude-biased pruning decisions, such that the filters with small magnitudes
are likely to be pruned. Also, they often entail pruning results with almost
zero margin around pruning decision boundary, such that tiny perturbation in a
filter magnitude can flip the pruning decision. In this paper, we identify the
precise algebraic condition under which pruning operations preserve model
performance, and use the condition to construct a novel regularizer defined in
an extended parameter space via auxiliary catalyst variables. The proposed
Catalyst regularization ensures fair pruning chance for each filters with
theoretically provable zero bias to their magnitude and robust pruning behavior
achieved by wide-margin bifurcation of magnitudes between the preserved and the
pruned filters. The theoretical properties naturally lead to real-world
effectiveness, as shown by empirical validations of Catalyst Pruning algorithm.
Pruning results on various datasets and models are superior to state-of-the-art
filter pruning methods, and at the same time confirm the predicted robust and
fair pruning characteristics of Catalyst pruning.

</details>


### [59] [IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning](https://arxiv.org/abs/2507.14171)
*Jaeheun Jung,Jaehyuk Lee,Yeajin Lee,Donghun Lee*

Main category: cs.LG

TL;DR: 本文提出一种新的、与幅度无关的神经网络结构化剪枝方法IPPRO（使用PROscore），通过将滤波器映射到投影空间并观察其梯度下降轨迹来评估重要性，旨在克服传统幅度剪枝的局限性，实现近乎无损的剪枝。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络结构化剪枝方法中，基于重要性（特别是基于幅度）的准则限制了剪枝决策能力，因为即使冗余，幅度较大的滤波器也不太可能被剪枝，导致剪枝效果不佳。

Method: 提出一种新颖的剪枝策略，通过将滤波器置于投影空间，以挑战幅度的决定性影响，并为每个滤波器提供公平的剪枝机会。通过观察滤波器在梯度下降过程中是否趋向原点来衡量其被剪枝的可能性，并以此构建PROscore，应用于IPPRO（一种新型的、与幅度无关的基于重要性的结构化剪枝方法）。

Result: 提出的基于投影空间的重要性准则实现了近乎无损的剪枝，显著减少了剪枝过程中的性能下降，并在微调后展现出良好的性能。

Conclusion: 该研究推翻了剪枝中“大小决定一切”的传统观念，并在理论和实践上拓展了基于重要性的剪枝领域。

Abstract: With the growth of demand on neural network compression methods, the
structured pruning methods including importance-based approach are actively
studied. The magnitude importance and many correlated modern importance
criteria often limit the capacity of pruning decision, since the filters with
larger magnitudes are not likely to be pruned if the smaller one didn't, even
if it is redundant. In this paper, we propose a novel pruning strategy to
challenge this dominating effect of magnitude and provide fair chance to each
filter to be pruned, by placing it on projective space. After that, we observe
the gradient descent movement whether the filters move toward the origin or
not, to measure how the filter is likely to be pruned. This measurement is used
to construct PROscore, a novel importance score for IPPRO, a novel
importance-based structured pruning with magnitude-indifference. Our evaluation
results shows that the proposed importance criteria using the projective space
achieves near-lossless pruning by reducing the performance drop in pruning,
with promising performance after the finetuning. Our work debunks the
``size-matters'' myth in pruning and expands the frontier of importance-based
pruning both theoretically and empirically.

</details>


### [60] [Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI](https://arxiv.org/abs/2507.14172)
*Julien Pourcel,Cédric Colas,Pierre-Yves Oudeyer*

Main category: cs.LG

TL;DR: SOAR是一种自改进的程序合成方法，通过进化搜索和回溯学习迭代优化大型语言模型（LLM）的能力，显著提升了在ARC-AGI基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型难以一次性解决复杂的程序合成任务；基于搜索的进化方法虽有潜力，但受限于底层生成模型的固定能力，效果有限。

Method: 提出SOAR方法，将语言模型集成到自改进的进化循环中。SOAR交替进行两个阶段：1) 进化搜索：使用LLM采样和优化候选解决方案；2) 回溯学习：将搜索尝试转化为有效的问答对，用于微调LLM的采样和优化能力，从而在后续迭代中实现更有效的搜索。

Result: 在挑战性的ARC-AGI基准测试中，SOAR在不同模型规模和迭代次数下均实现了显著的性能提升，利用了采样和优化微调任务之间的正向迁移。这些改进也延续到测试时适应性，使SOAR能够解决52%的公开测试集任务。

Conclusion: SOAR通过将LLM与自改进的进化循环相结合，有效提升了其在复杂程序合成任务上的能力，克服了现有方法的局限性，展现了强大的问题解决潜力。

Abstract: Many program synthesis tasks prove too challenging for even state-of-the-art
language models to solve in single attempts. Search-based evolutionary methods
offer a promising alternative by exploring solution spaces iteratively, but
their effectiveness remain limited by the fixed capabilities of the underlying
generative model.
  We propose SOAR, a method that learns program synthesis by integrating
language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample
and refine candidate solutions, and (2) a hindsight learning phase that
converts search attempts into valid problem-solution pairs used to fine-tune
the LLM's sampling and refinement capabilities\, -- \,enabling increasingly
effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance
gains across model scales and iterations, leveraging positive transfer between
the sampling and refinement finetuning tasks. These improvements carry over to
test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our
code is open-sourced at: https://github.com/flowersteam/SOAR

</details>


### [61] [Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data](https://arxiv.org/abs/2507.14175)
*Youcef Barkat,Dylan Hamitouche,Deven Parekh,Ivy Guo,David Benrimoh*

Main category: cs.LG

TL;DR: 研究表明，潜在空间融合方法在预测多模态精神健康数据（如抑郁症状）方面优于传统的早期融合方法，并展现出更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 精神疾病（如抑郁和焦虑）需要更优的早期检测和个性化干预方法。传统预测模型依赖单模态数据或早期融合策略，未能有效捕捉精神病理数据的复杂多模态特性。高级集成技术，如中间（潜在空间）融合，有望提高准确性和临床实用性。

Method: 利用BRIGHTEN临床试验数据，评估了中间（潜在空间）融合（通过包含自编码器和神经网络的组合模型CM实现）与早期融合（使用随机森林RF实现）在预测每日抑郁症状（PHQ-2评分）方面的性能。数据集包含行为（智能手机）、人口统计和临床特征。性能评估指标为均方误差（MSE）和决定系数（R2），并在多时间分割和数据流组合下进行实验。

Result: 组合模型（CM）在所有设置中均优于随机森林（RF）和线性回归（LR）基线模型，取得了更低的MSE（0.4985 vs. RF的0.5305）和更高的R2（0.4695 vs. RF的0.4356）。RF模型表现出过拟合迹象，而CM模型保持了良好且一致的泛化能力。CM在整合所有数据模态时表现最佳，突显了潜在空间融合捕捉复杂精神病学数据中非线性交互的价值。

Conclusion: 潜在空间融合为多模态精神健康数据预测提供了一种鲁棒的替代方案，优于传统融合方法。未来的工作应探索模型可解释性和个体层面预测，以实现临床部署。

Abstract: Background: Mental illnesses such as depression and anxiety require improved
methods for early detection and personalized intervention. Traditional
predictive models often rely on unimodal data or early fusion strategies that
fail to capture the complex, multimodal nature of psychiatric data. Advanced
integration techniques, such as intermediate (latent space) fusion, may offer
better accuracy and clinical utility. Methods: Using data from the BRIGHTEN
clinical trial, we evaluated intermediate (latent space) fusion for predicting
daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented
with a Random Forest (RF) model and intermediate fusion implemented via a
Combined Model (CM) using autoencoders and a neural network. The dataset
included behavioral (smartphone-based), demographic, and clinical features.
Experiments were conducted across multiple temporal splits and data stream
combinations. Performance was evaluated using mean squared error (MSE) and
coefficient of determination (R2). Results: The CM outperformed both RF and
Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985
vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed
signs of overfitting, with a large gap between training and test performance,
while the CM maintained consistent generalization. Performance was best when
integrating all data modalities in the CM (in contradistinction to RF),
underscoring the value of latent space fusion for capturing non-linear
interactions in complex psychiatric datasets. Conclusion: Latent space fusion
offers a robust alternative to traditional fusion methods for prediction with
multimodal mental health data. Future work should explore model
interpretability and individual-level prediction for clinical deployment.

</details>


### [62] [Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection](https://arxiv.org/abs/2507.14176)
*Andrés Morales-Forero,Lili J. Rueda,Ronald Herrera,Samuel Bassetto,Eric Coatanea*

Main category: cs.LG

TL;DR: 本文提出“预测代表性”（PR）框架，将AI公平性审计焦点从数据构成转向结果公平性。通过皮肤癌分类器案例，发现模型对深色皮肤性能显著不佳，强调代表性应是动态属性，并呼吁加强事后公平性审计和模型验证的包容性。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能系统越来越多地应用于医疗决策，但算法偏见和导致的不公平结果（特别是对历史上被边缘化的人群）仍是持续存在的担忧。现有的公平性评估方法未能充分解决结果层面的公平性问题。

Method: 提出了“预测代表性”（PR）框架，该框架将公平性审计的重点从数据集构成转移到结果层面的公平性。通过皮肤病学案例研究，评估了在HAM10000数据集上训练的AI皮肤癌分类器，并使用该数据集以及独立的哥伦比亚临床数据集（BOSQUE Test set）进行了测试。此外，提出了一个“外部可迁移性标准”来形式化公平性泛化的阈值。

Result: 分析显示，尽管源数据集中进行了按比例抽样，分类器在不同皮肤光型（尤其是深色皮肤个体）之间存在显著的性能差异，对深色皮肤的识别能力持续偏低。研究认为，代表性不应被理解为数据集的静态特征，而应是模型预测的动态、情境敏感属性。PR框架量化了模型在不同亚群和部署环境中泛化公平性的可靠程度。

Conclusion: 研究结果强调了事后公平性审计、数据集文档透明化和包容性模型验证流程的伦理必要性。本文提出的PR框架是一种诊断AI系统结构性不公平的可扩展工具，为关于公平性、可解释性和数据正义的讨论做出了贡献，并促使对数据驱动医疗中公平性概念进行批判性重新评估。

Abstract: Artificial intelligence (AI) systems increasingly inform medical
decision-making, yet concerns about algorithmic bias and inequitable outcomes
persist, particularly for historically marginalized populations. This paper
introduces the concept of Predictive Representativity (PR), a framework of
fairness auditing that shifts the focus from the composition of the data set to
outcomes-level equity. Through a case study in dermatology, we evaluated
AI-based skin cancer classifiers trained on the widely used HAM10000 dataset
and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our
analysis reveals substantial performance disparities by skin phototype, with
classifiers consistently underperforming for individuals with darker skin,
despite proportional sampling in the source data. We argue that
representativity must be understood not as a static feature of datasets but as
a dynamic, context-sensitive property of model predictions. PR operationalizes
this shift by quantifying how reliably models generalize fairness across
subpopulations and deployment contexts. We further propose an External
Transportability Criterion that formalizes the thresholds for fairness
generalization. Our findings highlight the ethical imperative for post-hoc
fairness auditing, transparency in dataset documentation, and inclusive model
validation pipelines. This work offers a scalable tool for diagnosing
structural inequities in AI systems, contributing to discussions on equity,
interpretability, and data justice and fostering a critical re-evaluation of
fairness in data-driven healthcare.

</details>


### [63] [Understanding Two-Layer Neural Networks with Smooth Activation Functions](https://arxiv.org/abs/2507.14177)
*Changcun Huang*

Main category: cs.LG

TL;DR: 本研究旨在理解并揭示使用反向传播训练的双层神经网络（含平滑激活函数）的解空间机制，并证明其通用逼近性。


<details>
  <summary>Details</summary>
Motivation: 理解通过反向传播算法训练的双层神经网络（使用Sigmoid等平滑激活函数）所获得的训练解，并揭示其解空间的“黑箱”之谜。

Method: 该研究基于四个核心原则：构建泰勒级数展开、严格的节点偏序、平滑样条实现以及平滑连续性限制。

Result: 研究证明了任意输入维度的通用逼近性，并通过实验验证，在很大程度上揭示了解决方案空间的“黑箱”机制。

Conclusion: 本研究不仅揭示了双层神经网络解空间的奥秘，所采用的新证明方法也丰富了逼近理论。

Abstract: This paper aims to understand the training solution, which is obtained by the
back-propagation algorithm, of two-layer neural networks whose hidden layer is
composed of the units with smooth activation functions, including the usual
sigmoid type most commonly used before the advent of ReLUs. The mechanism
contains four main principles: construction of Taylor series expansions, strict
partial order of knots, smooth-spline implementation and smooth-continuity
restriction. The universal approximation for arbitrary input dimensionality is
proved and experimental verification is given, through which the mystery of
``black box'' of the solution space is largely revealed. The new proofs
employed also enrich approximation theory.

</details>


### [64] [Feature Bank Enhancement for Distance-based Out-of-Distribution Detection](https://arxiv.org/abs/2507.14178)
*Yuhang Liu,Yuefei Wu,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 针对深度学习中距离基OOD检测方法因极端特征导致性能受限的问题，本文提出FBE方法，通过统计特性识别并约束极端特征，显著拉大ID和OOD样本距离，在大型数据集上取得了最先进的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有距离基OOD检测方法易受深度学习中偏置数据特征和极端特征的影响，导致其将ID样本分配过低的分数，从而限制了OOD检测能力。

Method: 本文提出特征库增强（Feature Bank Enhancement, FBE）方法，利用数据集的统计特性识别并约束极端特征至分离边界，从而有效拉大分布内外样本之间的距离。

Result: 在ImageNet-1k和CIFAR-10大型基准数据集上进行实验，结果表明FBE方法在这两个基准上均达到了最先进的性能。

Conclusion: FBE是一种简单有效的OOD检测方法，通过处理极端特征，显著提升了距离基方法的检测能力，并取得了SOTA表现，具有理论分析和实验支持。

Abstract: Out-of-distribution (OOD) detection is critical to ensuring the reliability
of deep learning applications and has attracted significant attention in recent
years. A rich body of literature has emerged to develop efficient score
functions that assign high scores to in-distribution (ID) samples and low
scores to OOD samples, thereby helping distinguish OOD samples. Among these
methods, distance-based score functions are widely used because of their
efficiency and ease of use. However, deep learning often leads to a biased
distribution of data features, and extreme features are inevitable. These
extreme features make the distance-based methods tend to assign too low scores
to ID samples. This limits the OOD detection capabilities of such methods. To
address this issue, we propose a simple yet effective method, Feature Bank
Enhancement (FBE), that uses statistical characteristics from dataset to
identify and constrain extreme features to the separation boundaries, therapy
making the distance between samples inside and outside the distribution
farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10
respectively, and the results show that our method achieves state-of-the-art
performance on both benchmark. Additionally, theoretical analysis and
supplementary experiments are conducted to provide more insights into our
method.

</details>


### [65] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: 本文提出了一种基于聚类的激活模式压缩框架，用于高效预测和利用LLMs中的激活稀疏性，从而在保持模型质量的同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在显著的激活稀疏性，这为降低计算成本提供了机会。然而，由于LLMs中神经元数量庞大，直接在神经元层面预测激活模式计算成本过高，因此需要一种可扩展的、高效的方法来预测和利用这种稀疏性。

Method: 作者提出了一种基于聚类的激活模式压缩框架。该方法不独立处理每个神经元，而是将相似的激活模式分组到少量具有代表性的簇中，从而将预测任务从单个神经元状态简化为簇分配。

Result: 该方法实现了高达79.34%的聚类精度，优于标准的二元聚类方法，同时保持了最小的困惑度（PPL）下降。当簇数量足够大时，困惑度可低至12.49，证明了其在保留模型质量的同时降低计算开销的有效性。

Conclusion: 所提出的聚类方法能有效捕获有意义的激活结构，并通过预测簇分配而非个体神经元状态，为未来模型高效推断激活模式奠定了基础，有望显著提高稀疏计算效率，为大规模语言模型的高效推理铺平道路。

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [66] [Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems](https://arxiv.org/abs/2507.14180)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.LG

TL;DR: 该论文提出了一种鲁棒且可解释的深度学习毫米波波束对齐引擎（BAE），利用数字孪生技术减少数据收集开销，并通过迁移学习、SHAP和DkNN算法提高模型的解释性、鲁棒性及效率，实现近乎最优的频谱效率和透明决策。


<details>
  <summary>Details</summary>
Motivation: 在AI原生6G毫米波系统中，可解释性和鲁棒性对建立信任至关重要。当前深度学习波束对齐方案面临数据收集开销大、硬件限制、缺乏可解释性及易受对抗性攻击等挑战。

Method: 本文提出一个基于深度学习的波束对齐引擎（BAE），通过宽波束的RSSI测量预测最佳窄波束。为克服数据收集难题，利用特定站点的数字孪生生成合成信道数据，并采用迁移学习以少量真实数据微调预训练模型。此外，引入深度SHAP来衡量输入特征重要性以减少波束训练开销和提高透明度，并整合Deep k-nearest neighbors（DkNN）算法提供可信度指标，用于检测分布外输入并确保鲁棒、透明的决策。

Result: 实验结果表明，所提框架将真实世界数据需求减少70%，波束训练开销减少62%，异常值检测鲁棒性提高了8.5倍，与传统基于softmax的深度学习模型相比，实现了近乎最优的频谱效率和透明决策。

Conclusion: 该框架有效地解决了毫米波系统中深度学习波束对齐面临的挑战，通过集成多种技术显著提升了模型的鲁棒性、可解释性和效率，为构建可信赖的AI原生6G系统提供了可行方案。

Abstract: In line with the AI-native 6G vision, explainability and robustness are
crucial for building trust and ensuring reliable performance in millimeter-wave
(mmWave) systems. Efficient beam alignment is essential for initial access, but
deep learning (DL) solutions face challenges, including high data collection
overhead, hardware constraints, lack of explainability, and susceptibility to
adversarial attacks. This paper proposes a robust and explainable DL-based beam
alignment engine (BAE) for mmWave multiple-input multiple output (MIMO)
systems. The BAE uses received signal strength indicator (RSSI) measurements
from wide beams to predict the best narrow beam, reducing the overhead of
exhaustive beam sweeping. To overcome the challenge of real-world data
collection, this work leverages a site-specific digital twin (DT) to generate
synthetic channel data closely resembling real-world environments. A model
refinement via transfer learning is proposed to fine-tune the pre-trained model
residing in the DT with minimal real-world data, effectively bridging
mismatches between the digital replica and real-world environments. To reduce
beam training overhead and enhance transparency, the framework uses deep
Shapley additive explanations (SHAP) to rank input features by importance,
prioritizing key spatial directions and minimizing beam sweeping. It also
incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a
credibility metric for detecting out-of-distribution inputs and ensuring
robust, transparent decision-making. Experimental results show that the
proposed framework reduces real-world data needs by 70%, beam training overhead
by 62%, and improves outlier detection robustness by up to 8.5x, achieving
near-optimal spectral efficiency and transparent decision making compared to
traditional softmax based DL models.

</details>


### [67] [Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis](https://arxiv.org/abs/2507.14181)
*Yajiao Dai,Jun Li,Zhen Mei,Yiyang Ni,Shi Jin,Zengxiang Li,Sheng Guo,Wei Xiang*

Main category: cs.LG

TL;DR: 本文提出SSFL-DCSL，一个半监督联邦学习框架，通过整合双重对比损失和软标签，解决分布式智能故障诊断中数据和标签稀缺、数据异构性及隐私保护问题，在有限标记数据下表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统的监督深度学习故障诊断方法面临多重挑战：需要大量训练数据和标签，而这些数据和标签常分散在不同客户端且获取成本高昂；同时，客户端间的数据分布差异（数据异构性）会严重影响模型性能。

Method: 本文提出了SSFL-DCSL半监督联邦学习框架，核心策略包括：
1.  **解决伪标签置信度问题：** 设计了基于拉普拉斯分布的样本加权函数，以减轻半监督训练中低置信度伪标签导致的偏差。
2.  **缓解数据异构性：** 引入了双重对比损失（包含局部对比损失和全局对比损失），以有效抑制不同数据分布引起模型发散。
3.  **促进知识共享：** 在服务器端通过加权平均聚合客户端的本地原型，并利用动量更新机制，实现客户端之间的知识共享，防止局部模型发散。

Result: 该框架在两个公开数据集和一个真实工厂电机数据集上进行了评估。在最具挑战性的场景（仅有10%数据被标记）下，SSFL-DCSL的准确率比现有先进方法提高了1.15%至7.85%。

Conclusion: SSFL-DCSL框架有效应对了分布式智能故障诊断中数据和标签稀缺、数据分布差异以及隐私保护的挑战，通过其创新的半监督联邦学习机制，显著提升了在标记数据极少情况下的故障诊断准确性，展现了其在工业应用中的巨大潜力。

Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe
operation of industrial machinery and improving production efficiency. However,
traditional supervised deep learning methods require a large amount of training
data and labels, which are often located in different clients. Additionally,
the cost of data labeling is high, making labels difficult to acquire.
Meanwhile, differences in data distribution among clients may also hinder the
model's performance. To tackle these challenges, this paper proposes a
semi-supervised federated learning framework, SSFL-DCSL, which integrates dual
contrastive loss and soft labeling to address data and label scarcity for
distributed clients with few labeled samples while safeguarding user privacy.
It enables representation learning using unlabeled data on the client side and
facilitates joint learning among clients through prototypes, thereby achieving
mutual knowledge sharing and preventing local model divergence. Specifically,
first, a sample weighting function based on the Laplace distribution is
designed to alleviate bias caused by low confidence in pseudo labels during the
semi-supervised training process. Second, a dual contrastive loss is introduced
to mitigate model divergence caused by different data distributions, comprising
local contrastive loss and global contrastive loss. Third, local prototypes are
aggregated on the server with weighted averaging and updated with momentum to
share knowledge among clients. To evaluate the proposed SSFL-DCSL framework,
experiments are conducted on two publicly available datasets and a dataset
collected on motors from the factory. In the most challenging task, where only
10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by
1.15% to 7.85% over state-of-the-art methods.

</details>


### [68] [From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling](https://arxiv.org/abs/2507.14182)
*Xiaotong Luo,Shengda Zhuo,Min Chen,Lichun Li,Ruizhao Lu,Wenqi Fan,Shuqiang Huang,Yin Tang*

Main category: cs.LG

TL;DR: 提出B4模型，利用牛市/熊市动态揭示投资者偏见与行为适应间的动态关系，从而提升金融市场趋势预测能力并提供可解释性洞察。


<details>
  <summary>Details</summary>
Motivation: 金融市场受复杂动态和外部叙事影响，数据异质性和投资者洞察差异引入偏见，使市场建模复杂化。本文旨在探究牛市/熊市机制在投资者驱动的市场动态中的潜力，以增强在多变市场条件下的趋势预测。

Method: 提出“偏见驱动行为的牛熊动态模型”（B4），一个统一框架，将时间价格序列和外部情境信号嵌入共享潜在空间，使牛市/熊市力量自然涌现以表示偏见。模型包含惯性配对模块（保留动量）和双重竞争机制（捕捉行为差异），以建模偏见驱动的不对称性、行为惯性和市场异质性。

Result: 在实际金融数据集上，B4模型在市场趋势预测方面表现优异，并能提供对偏见、投资者行为与市场动态相互作用的可解释性洞察。

Conclusion: B4模型通过整合偏见、投资者行为及牛熊机制，成功建模了复杂的市场动态，显著提升了市场趋势预测性能和可解释性。

Abstract: Financial markets exhibit highly dynamic and complex behaviors shaped by both
historical price trajectories and exogenous narratives, such as news, policy
interpretations, and social media sentiment. The heterogeneity in these data
and the diverse insight of investors introduce biases that complicate the
modeling of market dynamics. Unlike prior work, this paper explores the
potential of bull and bear regimes in investor-driven market dynamics. Through
empirical analysis on real-world financial datasets, we uncover a dynamic
relationship between bias variation and behavioral adaptation, which enhances
trend prediction under evolving market conditions. To model this mechanism, we
propose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified
framework that jointly embeds temporal price sequences and external contextual
signals into a shared latent space where opposing bull and bear forces
naturally emerge, forming the foundation for bias representation. Within this
space, an inertial pairing module pairs temporally adjacent samples to preserve
momentum, while the dual competition mechanism contrasts bullish and bearish
embeddings to capture behavioral divergence. Together, these components allow
B4 to model bias-driven asymmetry, behavioral inertia, and market
heterogeneity. Experimental results on real-world financial datasets
demonstrate that our model not only achieves superior performance in predicting
market trends but also provides interpretable insights into the interplay of
biases, investor behaviors, and market dynamics.

</details>


### [69] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: 针对大型语言模型（LLMs）处理长序列时KV缓存效率瓶颈和内存溢出问题，本文提出了一种无需训练的KV缓存优化范式LaCache，通过“阶梯形”KV缓存模式和迭代压缩机制，显著提升了LLMs的长程能力和连续生成效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在处理长序列时，Key-Value (KV) 对的数量急剧增加，导致显著的效率瓶颈。LLMs亟需强大的长程能力来处理大规模输入上下文和持续生成扩展输出，同时避免内存溢出（OOM）。

Method: 本文提出LaCache，一种无需训练的KV缓存优化范式，旨在实现高效准确的LLM生成式推理。LaCache融合了两项关键创新：1) 阶梯形KV缓存模式：KV对不仅在层内顺序存储，还在层间（从浅到深）存储，在固定存储预算下扩展了捕获长程依赖的范围。2) 迭代压缩机制：逐步压缩旧缓存，为新token腾出空间，实现基于token距离的动态压缩，以在有限缓存预算下更有效地连续生成。

Result: 在各种任务、基准和LLM模型上的实验一致验证了LaCache在增强LLMs长程能力方面的有效性。

Conclusion: LaCache成功地在固定存储预算下，通过创新的KV缓存管理，同时解决了LLMs在长程建模中鲁棒的长程能力和连续生成时避免内存溢出的两大关键挑战，显著提升了LLMs的推理效率和性能。

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [70] [Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired](https://arxiv.org/abs/2507.14215)
*Jiayu,Liu*

Main category: cs.LG

TL;DR: 本研究开发了一个基于深度学习的系统，旨在为听障人士提供实时声源定位与识别的无障碍设备。


<details>
  <summary>Details</summary>
Motivation: 当前研究在利用机器学习技术为听障社区开发辅助设备方面存在空白，本研究旨在填补这一空白，通过实时声源定位和识别来帮助听障或听力受损者。

Method: 该系统包含三个主要组件：1. JerryNet，一个定制的CNN架构，用于确定九个可能方向的声源方向(DoA)；2. 音频分类模型，基于微调的CLAP模型，仅根据音频识别精确的声音类别；3. 多模态集成模型，结合音频、视觉和文本数据精确定位图像中的声源，其中包含Yolov9目标检测和使用CIoU的音频-视觉定位模块。硬件包括一个四麦克风矩形阵列、一个安装在眼镜上的摄像头和用于信息显示的腕带。

Result: 在自定义数据集上，JerryNet在声源方向识别上达到了91.1%的精度，优于所有基线模型。CLAP模型在自定义数据集和AudioSet数据集上分别取得了98.5%和95%的准确率。第三部分的音频-视觉定位模型取得了0.892的CIoU和0.658的AUC，超越了其他类似模型。

Conclusion: 本研究展示了巨大的未来潜力，为开发新一代无障碍设备奠定了基础。

Abstract: This study aims to develop a deep learning system for an accessibility device
for the deaf or hearing impaired. The device will accurately localize and
identify sound sources in real time. This study will fill an important gap in
current research by leveraging machine learning techniques to target the
underprivileged community. The system includes three main components. 1.
JerryNet: A custom designed CNN architecture that determines the direction of
arrival (DoA) for nine possible directions. 2. Audio Classification: This model
is based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model
to identify the exact sound classes only based on audio. 3. Multimodal
integration model: This is an accurate sound localization model that combines
audio, visual, and text data to locate the exact sound sources in the images.
The part consists of two modules, one object detection using Yolov9 to generate
all the bounding boxes of the objects, and an audio visual localization model
to identify the optimal bounding box using complete Intersection over Union
(CIoU). The hardware consists of a four-microphone rectangular formation and a
camera mounted on glasses with a wristband for displaying necessary information
like direction. On a custom collected data set, JerryNet achieved a precision
of 91. 1% for the sound direction, outperforming all the baseline models. The
CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,
respectively. The audio-visual localization model within component 3 yielded a
cIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are
many future potentials to this study, paving the way to creating a new
generation of accessibility devices.

</details>


### [71] [Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation](https://arxiv.org/abs/2507.14217)
*Tudor Matei Opran,Samir Loudni*

Main category: cs.LG

TL;DR: 本文提出了一个结合非线性效用聚合和几何感知查询选择的交互式学习框架，旨在解决模式挖掘中的模式爆炸问题，并在实验中展现出更高的排名准确性和更少的用户交互。


<details>
  <summary>Details</summary>
Motivation: 解决模式挖掘中普遍存在的模式爆炸问题。

Method: 提出一个交互式学习框架，该框架结合了非线性效用聚合和几何感知查询选择。具体方法包括：通过对多个有趣度量应用Choquet积分来建模用户偏好；利用版本空间的几何结构来指导信息比较的选择；采用带有严格距离界限的分支定界策略来高效识别决策边界附近的查询。

Result: 在UCI数据集上的实验结果表明，该方法优于现有方法（如ChoquetRank），能够在更少的用户交互下实现更好的排名准确性。

Conclusion: 该交互式学习框架通过结合非线性效用聚合和几何感知查询选择，有效解决了模式挖掘中的模式爆炸问题，并显著提升了用户交互效率和排名准确性。

Abstract: We address the pattern explosion problem in pattern mining by proposing an
interactive learning framework that combines nonlinear utility aggregation with
geometry-aware query selection. Our method models user preferences through a
Choquet integral over multiple interestingness measures and exploits the
geometric structure of the version space to guide the selection of informative
comparisons. A branch-and-bound strategy with tight distance bounds enables
efficient identification of queries near the decision boundary. Experiments on
UCI datasets show that our approach outperforms existing methods such as
ChoquetRank, achieving better ranking accuracy with fewer user interactions.

</details>


### [72] [Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman](https://arxiv.org/abs/2507.14219)
*Obumneme Zimuzor Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 本研究提出了一个创新的AI框架，用于在数据稀缺地区评估绿色氢生产的地点适宜性，模型准确率达98%，并识别出水域接近度、海拔和季节变化为关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 为了寻找化石燃料的可持续替代品，绿色氢能被认为是脱碳的重要途径，尤其在富含太阳能的干旱地区。然而，识别最佳的氢气生产地点需要整合复杂的环境、大气和基础设施因素，且往往缺乏直接的氢气产量数据。

Method: 本研究提出了一个新颖的AI框架，利用平均绝对SHAP值计算绿色氢气产量和地点适宜性指数。该框架包含多阶段管道：无监督多变量聚类、监督机器学习分类器和SHAP算法。该管道在一个综合了气象、地形和时间的数据集上进行训练。

Result: 研究结果揭示了适宜性和变量相对影响的独特空间模式，模型预测准确率达到98%。在阿曼，水域接近度、海拔和季节变化是决定绿色氢气选址适宜性的最有影响力因素，其平均绝对SHAP值分别为2.470891、2.376296和1.273216。

Conclusion: 本研究为许多缺乏地面真实产量数据的国家提供了一种客观且可重现的替代方法，以克服主观专家权重，让数据本身说话。该研究为行业利益相关者和决策者提供了一个可复制、可扩展的工具，用于数据稀缺地区的绿色氢能基础设施规划和其他决策。

Abstract: As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.

</details>


### [73] [Domain Generalization via Pareto Optimal Gradient Matching](https://arxiv.org/abs/2507.14227)
*Khoi Do,Duong Nguyen,Nam-Khanh Le,Quoc-Viet Pham,Binh-Son Hua,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出POGM方法，通过利用梯度轨迹和元学习策略，解决梯度域泛化中梯度波动和高计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有梯度域泛化方法面临两大挑战：1) 最小化梯度经验距离或梯度内积导致域间梯度波动，阻碍学习；2) 将梯度学习直接应用于联合损失函数会因二阶导数近似而产生高计算开销。

Method: 提出Pareto最优梯度匹配（POGM）方法。该方法不将梯度匹配作为正则化项，而是利用梯度轨迹作为数据，在元学习器中进行独立训练。在元更新阶段，POGM最大化梯度内积（GIP），同时限制学习到的梯度不过度偏离经验风险最小化（ERM）梯度轨迹，从而实现稳定的梯度聚合。

Result: 在DomainBed数据集上的实验评估表明，POGM方法在与现有基线相比下取得了有竞争力的结果，并且实现了计算效率。

Conclusion: POGM方法有效解决了梯度域泛化中的梯度波动和计算效率挑战，通过其创新的梯度轨迹利用和元更新策略，实现了多域知识的稳定聚合和优秀的泛化性能。

Abstract: In this study, we address the gradient-based domain generalization problem,
where predictors aim for consistent gradient directions across different
domains. Existing methods have two main challenges. First, minimization of
gradient empirical distance or gradient inner products (GIP) leads to gradient
fluctuations among domains, thereby hindering straightforward learning. Second,
the direct application of gradient learning to the joint loss function can
incur high computation overheads due to second-order derivative approximation.
To tackle these challenges, we propose a new Pareto Optimality Gradient
Matching (POGM) method. In contrast to existing methods that add gradient
matching as regularization, we leverage gradient trajectories as collected data
and apply independent training at the meta-learner. In the meta-update, we
maximize GIP while limiting the learned gradient from deviating too far from
the empirical risk minimization gradient trajectory. By doing so, the aggregate
gradient can incorporate knowledge from all domains without suffering gradient
fluctuation towards any particular domain. Experimental evaluations on datasets
from DomainBed demonstrate competitive results yielded by POGM against other
baselines while achieving computational efficiency.

</details>


### [74] [A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions](https://arxiv.org/abs/2507.14245)
*Hengjie Yu,Kenneth A. Dawson,Haiyun Yang,Shuya Liu,Yan Yan,Yaochu Jin*

Main category: cs.LG

TL;DR: 本研究提出了迄今最大的纳米材料-蛋白质相互作用数据集NanoPro-3M和多模态基础模型NanoProFormer，显著提升了相互作用预测的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 了解纳米材料与蛋白质的相互作用是其在医药和环境科学中应用的关键，但现有研究受限于数据集不足和模型泛化能力差。

Method: 构建了包含超过320万样本和3.7万独特蛋白质的NanoPro-3M数据集。基于此，开发了NanoProFormer基础模型，利用多模态表征学习预测纳米材料-蛋白质亲和力。

Result: NanoProFormer展现出强大的泛化能力，能处理缺失特征和未见的新纳米材料或蛋白质。多模态建模显著优于单模态方法，并能识别电晕形成的关键决定因素。此外，该模型通过零样本推理和微调，适用于多种下游任务。

Conclusion: 本工作为纳米材料-蛋白质相互作用终点的高性能和泛化预测奠定了坚实基础，有望减少实验依赖，加速各种体外应用的进展。

Abstract: Unlocking the potential of nanomaterials in medicine and environmental
science hinges on understanding their interactions with proteins, a complex
decision space where AI is poised to make a transformative impact. However,
progress has been hindered by limited datasets and the restricted
generalizability of existing models. Here, we propose NanoPro-3M, the largest
nanomaterial-protein interaction dataset to date, comprising over 3.2 million
samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,
a foundational model that predicts nanomaterial-protein affinities through
multimodal representation learning, demonstrating strong generalization,
handling missing features, and unseen nanomaterials or proteins. We show that
multimodal modeling significantly outperforms single-modality approaches and
identifies key determinants of corona formation. Furthermore, we demonstrate
its applicability to a range of downstream tasks through zero-shot inference
and fine-tuning. Together, this work establishes a solid foundation for
high-performance and generalized prediction of nanomaterial-protein interaction
endpoints, reducing experimental reliance and accelerating various in vitro
applications.

</details>


### [75] [Linearized Diffusion Map](https://arxiv.org/abs/2507.14257)
*Julio Candanedo*

Main category: cs.LG

TL;DR: 本文提出线性扩散映射（LDM），一种新型线性降维方法，它结合了扩散映射的几何直觉与线性嵌入的计算优势，在流形结构数据上表现优于PCA，并可与非负矩阵分解（NMF）结合以发现可解释的潜在结构。


<details>
  <summary>Details</summary>
Motivation: 开发一种既能利用非线性扩散映射的几何洞察力，又能保留PCA等线性方法的计算简洁性、效率和可解释性的线性降维方法。

Method: LDM通过对扩散映射核进行线性近似构建。通过在合成数据集（Swiss roll, hyperspheres）和真实数据集（MNIST, COIL-20）上进行综合实验，评估了LDM的性能并与PCA进行了比较。此外，探索了其核矩阵的完全正性在非负矩阵分解（NMF）中的应用。

Result: LDM能够捕获与PCA不同的数据集几何特征，提供互补优势。具体而言，在具有显式流形结构的数据集（尤其是在高维情况下），LDM的嵌入效果优于PCA；而在方差或噪声主导的场景中，PCA表现更佳。此外，LDM核矩阵的完全正性使其可以直接应用于非负矩阵分解（NMF），有助于发现可解释的潜在结构。

Conclusion: LDM是一种有价值的新型线性降维技术，具有良好的理论和实践扩展前景。

Abstract: We introduce the Linearized Diffusion Map (LDM), a novel linear
dimensionality reduction method constructed via a linear approximation of the
diffusion-map kernel. LDM integrates the geometric intuition of diffusion-based
nonlinear methods with the computational simplicity, efficiency, and
interpretability inherent in linear embeddings such as PCA and classical MDS.
Through comprehensive experiments on synthetic datasets (Swiss roll and
hyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that
LDM captures distinct geometric features of datasets compared to PCA, offering
complementary advantages. Specifically, LDM embeddings outperform PCA in
datasets exhibiting explicit manifold structures, particularly in
high-dimensional regimes, whereas PCA remains preferable in scenarios dominated
by variance or noise. Furthermore, the complete positivity of LDM's kernel
matrix allows direct applicability of Non-negative Matrix Factorization (NMF),
suggesting opportunities for interpretable latent-structure discovery. Our
analysis positions LDM as a valuable new linear dimensionality reduction
technique with promising theoretical and practical extensions.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [76] [Iran's Stealth Internet Blackout: A New Model of Censorship](https://arxiv.org/abs/2507.14183)
*Arash Aryapour*

Main category: cs.NI

TL;DR: 伊朗在2025年中期实施了一种新型隐蔽的互联网关停，通过深度包检测等技术隔离国内用户。本研究通过网络测量分析，量化了VPN需求增长并描述了其审查基础设施。


<details>
  <summary>Details</summary>
Motivation: 分析伊朗于2025年中期实施的一种在维持全球路由同时隔离国内用户的隐蔽性互联网关停，并揭示其技术细节和影响。

Method: 采用主动网络测量方法，包括DNS投毒、HTTP注入、TLS拦截和协议白名单，并将这些措施追溯至一个集中的边境网关。

Result: 量化了VPN需求约707%的增长，并详细描述了伊朗多层次的审查基础设施。

Conclusion: 强调了这种新型互联网关停对规避技术和数字权利监控的重要影响。

Abstract: In mid-2025, Iran experienced a novel, stealthy Internet shutdown that
preserved global routing presence while isolating domestic users through deep
packet inspection, aggressive throttling, and selective protocol blocking. This
paper analyzes active network measurements such as DNS poisoning, HTTP
injection, TLS interception, and protocol whitelisting, traced to a centralized
border gateway. We quantify an approximate 707 percent rise in VPN demand and
describe the multi-layered censorship infrastructure, highlighting implications
for circumvention and digital rights monitoring.

</details>


### [77] [A Disentangled Representation Learning Framework for Low-altitude Network Coverage Prediction](https://arxiv.org/abs/2507.14186)
*Xiaojie Li,Zhijie Cai,Nan Qi,Chao Dong,Guangxu Zhu,Haixia Ma,Qihui Wu,Shi Jin*

Main category: cs.NI

TL;DR: 针对低空网络覆盖预测中基站天线波束图不可获取和数据稀疏的挑战，本文提出一种结合专家知识特征压缩和解耦表征学习的方法，显著提升了预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 低空经济发展对低空网络覆盖（LANC）预测提出需求，但精确预测所需的基站天线波束图为专有信息且难以获取。尽管基站运行参数包含波束信息，但低空路测数据收集成本高昂，导致数据稀疏，进而引发特征采样不平衡和模型泛化能力差的问题。

Method: 本文提出双重策略：1. 基于专家知识的特征压缩，利用通信专业知识降低特征空间复杂度。2. 解耦表征学习，通过整合传播模型和独立的子网络，捕获并聚合潜在特征的语义表征，以增强模型泛化能力。

Result: 实验评估显示，本框架相比最佳基线算法误差降低7%。实际网络验证表明，其预测精度可达5dB的平均绝对误差（MAE），具有实用性。

Conclusion: 本研究提出的方法有效解决了低空网络覆盖预测中数据稀疏性带来的挑战，显著提升了预测精度和可靠性，为低空经济发展提供了技术支持。

Abstract: The expansion of the low-altitude economy has underscored the significance of
Low-Altitude Network Coverage (LANC) prediction for designing aerial corridors.
While accurate LANC forecasting hinges on the antenna beam patterns of Base
Stations (BSs), these patterns are typically proprietary and not readily
accessible. Operational parameters of BSs, which inherently contain beam
information, offer an opportunity for data-driven low-altitude coverage
prediction. However, collecting extensive low-altitude road test data is
cost-prohibitive, often yielding only sparse samples per BS. This scarcity
results in two primary challenges: imbalanced feature sampling due to limited
variability in high-dimensional operational parameters against the backdrop of
substantial changes in low-dimensional sampling locations, and diminished
generalizability stemming from insufficient data samples. To overcome these
obstacles, we introduce a dual strategy comprising expert knowledge-based
feature compression and disentangled representation learning. The former
reduces feature space complexity by leveraging communications expertise, while
the latter enhances model generalizability through the integration of
propagation models and distinct subnetworks that capture and aggregate the
semantic representations of latent features. Experimental evaluation confirms
the efficacy of our framework, yielding a 7% reduction in error compared to the
best baseline algorithm. Real-network validations further attest to its
reliability, achieving practical prediction accuracy with MAE errors at the 5dB
level.

</details>


### [78] [From Cell Towers to Satellites: A 2040 Blueprint for Urban-Grade Direct-to-Device Mobile Networks](https://arxiv.org/abs/2507.14188)
*Sebastian Barros Elgueta*

Main category: cs.NI

TL;DR: 本文提出了一种创新的全轨道电信网络架构，旨在脱离地面基础设施，为全球特大城市提供持续、高带宽的移动服务，并通过仿真验证了其可行性，描绘了未来发展路线。


<details>
  <summary>Details</summary>
Motivation: 现有卫星直连手机（D2D）技术虽然实现了手机与低轨卫星的直接连接，但仍仅限于乡村地区、带宽受限且完全依赖地面核心网。研究动机在于探索能否构建一个完全在轨运行的移动网络，包括无线接入、核心功能、流量路由和内容分发，并在全球最密集的城市中提供持续的城市级服务。

Method: 本文提出了首个端到端的全轨道电信系统架构，该架构整合了千束容量的电控相控阵、太空部署的5G核心网功能（UPF, AMF）以及星间激光网状回传。研究通过分析密集城市条件下的频谱效率、波束容量和链路预算，并考虑路径损耗、多普勒效应和多径效应，进行系统性能评估。通过仿真验证了系统在城市环境下的表现。

Result: 仿真结果表明，屋顶和视线可见用户可维持64-QAM的吞吐量，而街道级别接入通过中继或辅助波束模式也可实现。研究指出，功率、散热、计算辐射加固和监管模型等剩余限制是工程瓶颈而非物理极限。最终，该研究提出了一个15年的分阶段发展路线图。

Conclusion: 结论认为，构建一个完全自主的轨道叠加网络是可行的，该网络能在特大城市中为手持设备提供50-100 Mbps的速度，且完全不依赖地面基础设施。这预示着未来移动通信将实现从现有回退级D2D系统到先进轨道电信网络的重大飞跃。

Abstract: In 2023, satellite and mobile networks crossed a historic threshold: standard
smartphones, using unmodified 3GPP protocols, connected directly to low Earth
orbit (LEO) satellites. This first wave of direct-to-device (D2D)
demonstrations validated the physical feasibility of satellite-based mobile
access. However, these systems remain fallback-grade--rural-only,
bandwidth-limited, and fully dependent on Earth-based mobile cores for
identity, session, and policy control. This paper asks a more ambitious
question: Can a complete mobile network, including radio access, core
functions, traffic routing, and content delivery, operate entirely from orbit?
And can it deliver sustained, urban-grade service in the world's densest
cities? We present the first end-to-end system architecture for a fully orbital
telco, integrating electronically steered phased arrays with 1000-beam
capacity, space-based deployment of 5G core functions (UPF, AMF), and
inter-satellite laser mesh backhaul. We analyze spectral efficiency, beam
capacity, and link budgets under dense urban conditions, accounting for path
loss, Doppler, and multipath. Simulations show that rooftop and line-of-sight
users can sustain 64-QAM throughput, while street-level access is feasible with
relay or assisted beam modes. The paper outlines the remaining constraints,
power, thermal dissipation, compute radiation hardening, and regulatory models,
and demonstrates that these are engineering bottlenecks, not physical limits.
Finally, we propose a staged 15-year roadmap from today's fallback D2D systems
to autonomous orbital overlays delivering 50-100 Mbps to handhelds in
megacities, with zero reliance on terrestrial infrastructure.

</details>


### [79] [On Splitting Lightweight Semantic Image Segmentation for Wireless Communications](https://arxiv.org/abs/2507.14199)
*Ebrahim Abu-Helalah,Jordi Serra,Jordi Perez-Romero*

Main category: cs.NI

TL;DR: 本文提出一种语义图像分割的语义通信方法，通过将分割过程分担在受限发射端和接收端，显著降低了传输带宽和发射端计算负担，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信在图像分割应用中，难以在计算效率、带宽需求和分割精度之间取得平衡，尤其是在资源受限环境和信道多变的情况下。此外，更复杂的语义图像分割模型会增加设备的计算压力。

Method: 提出一种新颖的语义通信方法，将语义图像分割过程分解并分配给资源受限的发射端和接收端共同完成。该方法通过仿真实验评估了计算资源使用、所需比特率和分割精度等指标。

Result: 与在发射端完成完整语义图像分割相比，该方法在传输过程中将比特率降低了高达72%，并将发射端的计算负载减少了19%以上。

Conclusion: 该技术在降低通信成本、减轻计算负担方面表现出色，具有应用于通信系统（特别是未来的6G系统）的巨大潜力。

Abstract: Semantic communication represents a promising technique towards reducing
communication costs, especially when dealing with image segmentation, but it
still lacks a balance between computational efficiency and bandwidth
requirements while maintaining high image segmentation accuracy, particularly
in resource-limited environments and changing channel conditions. On the other
hand, the more complex and larger semantic image segmentation models become,
the more stressed the devices are when processing data. This paper proposes a
novel approach to implementing semantic communication based on splitting the
semantic image segmentation process between a resource constrained transmitter
and the receiver. This allows saving bandwidth by reducing the transmitted data
while maintaining the accuracy of the semantic image segmentation.
Additionally, it reduces the computational requirements at the resource
constrained transmitter compared to doing all the semantic image segmentation
in the transmitter. The proposed approach is evaluated by means of
simulation-based experiments in terms of different metrics such as
computational resource usage, required bit rate and segmentation accuracy. The
results when comparing the proposal with the full semantic image segmentation
in the transmitter show that up to 72% of the bit rate was reduced in the
transmission process. In addition, the computational load of the transmitter is
reduced by more than 19%. This reflects the interest of this technique for its
application in communication systems, particularly in the upcoming 6G systems.

</details>


### [80] [A Fault-Tolerant Architecture for Urban and Rural Digital Connectivity: Synergizing SDWMN, Direct-to-Mobile Broadcasting, and Hybrid Cloud Streaming](https://arxiv.org/abs/2507.14205)
*Pavel Malinovskiy*

Main category: cs.NI

TL;DR: 提出一种结合SDWMN、D2M和Kafka的综合架构，旨在改善城乡无线网络性能，解决拥堵和数字鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 解决城市网络拥堵和农村数字排斥问题，通过流量分流、增强容错和公平资源分配来提升无线网络性能。

Method: 提出一种集成软件定义无线网格网络（SDWMN）、直连移动（D2M）广播和基于Kafka混合云流媒体的架构。通过建模城市拥堵（ρu）和农村覆盖不足（δr），目标是最小化全球性能损失（GPL），并考虑恢复时间（Trec）。

Result: 实验结果显示：延迟降低超过32%，带宽分流40%，农村覆盖率提升28%，公平性指数从0.78提高到0.91。系统在10秒内实现恢复。

Conclusion: 该设计具有可扩展性和容错性，支持公平的数字化转型。为促进采用，建议优化频谱分配、提供定向补贴和强制设备使用。研究为未来工作提供了方向。

Abstract: We propose an integrated architecture combining Software-Defined Wireless
Mesh Networks (SDWMN), Direct-to-Mobile (D2M) broadcasting, and Kafka-based
hybrid cloud streaming to improve wireless network performance in both urban
and rural settings. The approach addresses urban congestion and rural digital
exclusion through traffic offloading, enhanced fault tolerance, and equitable
resource allocation. We model urban congestion $\rho_u = \lambda_t / \mu_c$ and
rural coverage deficit $\delta_r = 1 - C_r / C_{req}$, and aim to minimize
global performance loss $GPL = w_1 \cdot \rho_u + w_2 \cdot \delta_r + w_3
\cdot T_{rec}$, where $T_{rec}$ is recovery time. Experiments in Bangkok,
Mumbai, and rural Finland demonstrate latency reduction over 32%, bandwidth
offloading of 40%, rural coverage gain of 28%, and fairness index rising from
0.78 to 0.91. The system achieves recovery under 10 s using SDWMN and Kafka. We
recommend optimal spectrum allocation $\alpha_s$, targeted subsidies, and
device mandates to promote adoption. This scalable, fault-tolerant design
supports equitable digital transformation and suggests directions for future
research.

</details>


### [81] [White paper: Towards Human-centric and Sustainable 6G Services -- the fortiss Research Perspective](https://arxiv.org/abs/2507.14209)
*Rute C. Sofia,Hao Shen,Yuanting Liu,Severin Kacianka,Holger Pfeifer*

Main category: cs.NI

TL;DR: Fortiss发布白皮书，提出以人为本、可持续、AI集成的6G网络愿景，旨在通过AI原生网络、边缘云编排等技术，确保6G不仅技术先进且符合社会需求。


<details>
  <summary>Details</summary>
Motivation: 确保6G技术超越纯粹的技术进步，与社会需求保持一致；应对超可靠低延迟通信（URLLC）和个性化数字服务的日益增长的期望，促使6G性能超越前代。

Method: 通过发布白皮书，阐述fortiss关于以人为本、可持续和AI集成6G网络的愿景，并明确语义通信、绿色编排和分布式AI等关键研究领域，将技术挑战与社会需求相结合。

Result: 白皮书成功阐明了fortiss在6G领域的战略方向和具体贡献，特别强调了负责任的创新和跨学科合作的重要性，为实现2030年的有意义愿景提供了指导。

Conclusion: 6G的未来发展应以人为本，注重可持续性和AI整合，并通过负责任的创新和广泛的跨学科合作，共同实现技术与社会需求的平衡发展。fortiss将积极推动此愿景的实现。

Abstract: As a leading research institute in software-intensive systems, fortiss is
actively shaping the vision of Sixth Generation Mobile Communication (6G). Our
mission is to ensure that 6G technologies go beyond technical advancements and
are aligned with societal needs. fortiss plays a key role in 6G initiatives
worldwide, including contributions to standardization bodies and collaborative
Research and Development programs. We focus on software-defined, AI-enabled,
and sustainable communication services that prioritize human values and
long-term impact. 6G will redefine digital connectivity through cognitive
intelligence, decentralized orchestration, and sustainability-oriented
architectures. As expectations rise for ultra-reliable low-latency
communication (URLLC) and personalized digital services, 6G must outperform
prior generations. It will rely on AI-native networking, Edge-Cloud resource
orchestration, and energy-aware data frameworks, ensuring both technical
performance and societal relevance. This white paper presents the fortiss
vision for a human-centric, sustainable, and AI-integrated 6G network. It
outlines key research domains such as semantic communication, green
orchestration, and distributed AI, all linked to societal and technological
challenges. The white paper is aimed at researchers, industry experts,
policymakers, and developers. It articulates the strategic direction and
contributions of fortiss to 6G, emphasizing responsible innovation and
interdisciplinary collaboration toward a meaningful 2030 vision.

</details>


### [82] [PRATA: A Framework to Enable Predictive QoS in Vehicular Networks via Artificial Intelligence](https://arxiv.org/abs/2507.14211)
*Federico Mason,Tommaso Zugno,Matteo Drago,Marco Giordani,Mate Boban,Michele Zorzi*

Main category: cs.NI

TL;DR: 提出PRATA模拟框架及RAN-AI强化学习单元，实现自动驾驶中基于AI的预测性QoS优化，性能较基线方法提升近一倍。


<details>
  <summary>Details</summary>
Motivation: 远程操控驾驶等汽车应用对延迟和可靠性有严格要求，需要预测性服务质量（PQoS）来提前应对QoS变化并采取措施避免性能下降。强化学习是实现PQoS的有效工具，但缺乏专门的模拟框架来设计和验证其在远程操控驾驶应用中的PQoS决策。

Method: 研究提出PRATA模拟框架，用于基于AI的远程操控驾驶应用预测性QoS。PRATA包含模块化管道，集成：1) 模拟5G无线接入网的端到端协议栈；2) 汽车数据生成工具；3) 优化PQoS决策的AI单元。利用PRATA设计并验证了一个名为RAN-AI的强化学习单元，旨在优化资源饱和或信道退化时远程操控驾驶数据的分段级别。

Result: RAN-AI实体能有效平衡远程操控驾驶应用中的QoS与体验质量（QoE）之间的权衡。与基线方法相比，系统性能几乎翻倍。此外，通过改变RAN-AI的学习设置，研究还探讨了状态空间以及获取实施强化学习所需网络数据的相对成本的影响。

Conclusion: PRATA框架和RAN-AI强化学习单元为远程操控驾驶中的预测性QoS提供了有效解决方案，显著提升了系统性能，并为强化学习在该领域的实际应用提供了关于数据成本和状态空间影响的重要见解。

Abstract: Predictive Quality of Service (PQoS) makes it possible to anticipate QoS
changes, e.g., in wireless networks, and trigger appropriate countermeasures to
avoid performance degradation. Hence, PQoS is extremely useful for automotive
applications such as teleoperated driving, which poses strict constraints in
terms of latency and reliability. A promising tool for PQoS is given by
Reinforcement Learning (RL), a methodology that enables the design of
decision-making strategies for stochastic optimization. In this manuscript, we
present PRATA, a new simulation framework to enable PRedictive QoS based on AI
for Teleoperated driving Applications. PRATA consists of a modular pipeline
that includes (i) an end-to-end protocol stack to simulate the 5G Radio Access
Network (RAN), (ii) a tool for generating automotive data, and (iii) an
Artificial Intelligence (AI) unit to optimize PQoS decisions. To prove its
utility, we use PRATA to design an RL unit, named RAN-AI, to optimize the
segmentation level of teleoperated driving data in the event of resource
saturation or channel degradation. Hence, we show that the RAN-AI entity
efficiently balances the trade-off between QoS and Quality of Experience (QoE)
that characterize teleoperated driving applications, almost doubling the system
performance compared to baseline approaches. In addition, by varying the
learning settings of the RAN-AI entity, we investigate the impact of the state
space and the relative cost of acquiring network data that are necessary for
the implementation of RL.

</details>


### [83] [Intent-Based Network for RAN Management with Large Language Models](https://arxiv.org/abs/2507.14230)
*Fransiscus Asisi Bimo,Maria Amparo Canaveras Galdon,Chun-Kai Lai,Ray-Guang Cheng,Edwin K. P. Chong*

Main category: cs.NI

TL;DR: 利用大型语言模型（LLMs）和代理架构，实现基于意图的无线接入网络（RAN）管理自动化，并有效提高网络能效。


<details>
  <summary>Details</summary>
Motivation: 为应对无线网络管理日益增长的复杂性，迫切需要先进的智能自动化解决方案。

Method: 提出一种基于LLM和代理架构的新型意图网络自动化方法，用于RAN管理。该方法通过整合LLM，增强意图翻译、自主解释高级目标、推理复杂网络状态并生成精确的RAN配置，并采用结构化提示工程技术。

Result: 通过闭环机制，网络能够动态优化关键RAN参数，从而自动提高其能源效率。

Conclusion: 研究展示了通过LLM协调的代理系统，根据实时反馈调整策略，在RAN中实现鲁棒资源管理的巨大潜力。

Abstract: Advanced intelligent automation becomes an important feature to deal with the
increased complexity in managing wireless networks. This paper proposes a novel
automation approach of intent-based network for Radio Access Networks (RANs)
management by leveraging Large Language Models (LLMs). The proposed method
enhances intent translation, autonomously interpreting high-level objectives,
reasoning over complex network states, and generating precise configurations of
the RAN by integrating LLMs within an agentic architecture. We propose a
structured prompt engineering technique and demonstrate that the network can
automatically improve its energy efficiency by dynamically optimizing critical
RAN parameters through a closed-loop mechanism. It showcases the potential to
enable robust resource management in RAN by adapting strategies based on
real-time feedback via LLM-orchestrated agentic systems.

</details>


### [84] [Feasibility of Energy Neutral Wildlife Tracking using Multi-Source Energy Harvesting](https://arxiv.org/abs/2507.14234)
*Samer Nasser,Henrique Duarte Moura,Dragan Subotic,Ritesh Kumar Singh,Maarten Weyn,Jeroen Famaey*

Main category: cs.NI

TL;DR: 本文提出了一种结合太阳能和动能收集的能量中性野生动物追踪系统，利用NB-IoT通信，显著提高了数据产量和可靠性，实现了长期免维护的动物监测。


<details>
  <summary>Details</summary>
Motivation: 长期野生动物追踪对生物多样性监测至关重要，但现有动物标签面临能源限制，电池更换困难且对动物有害。当前能量收集系统多依赖单一能源，且通信技术受限。

Method: 开发了一种结合太阳能和动能收集的能量中性系统；动能收集器同时作为运动代理；采用NB-IoT与现有蜂窝基础设施兼容；构建了模拟框架，并引入能量感知调度器以优化任务执行。

Result: 该方法实现了能量中性运行，与单源系统相比，显著提高了数据产量和可靠性；系统能持续每两分钟采样GPS位置和动能数据，并每小时通过NB-IoT传输这些数据。

Conclusion: 该研究成果证明了在偏远栖息地实现免维护、环保的野生动物追踪潜力，有助于更有效和可扩展的野生动物监测。

Abstract: Long-term wildlife tracking is crucial for biodiversity monitoring, but
energy limitations pose challenges, especially for animal tags, where replacing
batteries is impractical and stressful for the animal due to the need to
locate, possibly sedate, and handle it. Energy harvesting offers a sustainable
alternative, yet most existing systems rely on a single energy source and
infrastructure-limited communication technologies. This paper presents an
energy-neutral system that combines solar and kinetic energy harvesting to
enable the tracking and monitoring of wild animals. Harvesting from multiple
sources increases the total available energy. Uniquely, the kinetic harvester
also serves as a motion proxy by sampling harvested current, enabling activity
monitoring without dedicated sensors. Our approach also ensures compatibility
with existing cellular infrastructure, using Narrowband Internet of Things
(NB-IoT). We present a simulation framework that models energy harvesting,
storage, and consumption at the component level. An energy-aware scheduler
coordinates task execution based on real-time energy availability. We evaluate
performance under realistically varying conditions, comparing task frequencies
and capacitor sizes. Results show that our approach maintains energy-neutral
operation while significantly increasing data yield and reliability compared to
single-source systems, with the ability to consistently sample GPS location
data and kinetic harvesting data every two minutes while transmitting these
results over NB-IoT every hour. These findings demonstrate the potential for
maintenance-free, environmentally friendly tracking in remote habitats,
enabling more effective and scalable wildlife monitoring.

</details>


### [85] [Beyond DNS: Unlocking the Internet of AI Agents via the NANDA Index and Verified AgentFacts](https://arxiv.org/abs/2507.14263)
*Ramesh Raskar,Pradyumna Chari,John Zinky,Mahesh Lambe,Jared James Grogan,Sichao Wang,Rajesh Ranjan,Rekha Singhal,Shailja Gupta,Robert Lincourt,Raghu Bala,Aditi Joshi,Abhishek Singh,Ayush Chopra,Dimitris Stripelis,Bhuwan B,Sumit Kumar,Maria Gorskikh*

Main category: cs.NI

TL;DR: 为应对未来海量AI智能体对现有互联网身份和发现机制的挑战，本文提出NANDA索引架构，以提供智能体在互联网中的可发现性、身份识别和认证能力。


<details>
  <summary>Details</summary>
Motivation: 互联网将承载数十亿至数万亿的自治AI智能体，它们需要毫秒级的协商、委托和迁移能力，这将使现有以DNS为中心的身份和发现机制承受巨大压力。

Method: 本文提出了NANDA索引架构，该架构通过一个极简的轻量级索引解析为动态、可密码验证的AgentFacts，以支持多端点路由、负载均衡、隐私保护访问和凭证能力声明。具体方法包括形式化AgentFacts模式、指定基于CRDT的更新协议以及原型自适应解析器。

Result: 该架构设计实现了五项具体保证：支持NANDA原生和第三方智能体的索引可发现性；为新生成的AI智能体提供快速全球解析；亚秒级的撤销和密钥轮换；模式验证的能力声明；通过可验证的最小披露查询实现跨组织边界的隐私保护发现。最终成果是构建了一个轻量级、水平可扩展的基础，可在不放弃现有网络基础设施的情况下，为下一代AI智能体互联网解锁安全、信任感知的协作。

Conclusion: NANDA架构为未来的AI智能体互联网提供了一个轻量级、水平可扩展、安全且信任感知的基础，有效解决了现有身份和发现机制所面临的挑战，并能与现有Web基础设施兼容。

Abstract: The Internet is poised to host billions to trillions of autonomous AI agents
that negotiate, delegate, and migrate in milliseconds and workloads that will
strain DNS-centred identity and discovery. In this paper, we describe the NANDA
index architecture, which we envision as a means for discoverability,
identifiability and authentication in the internet of AI agents. We present an
architecture where a minimal lean index resolves to dynamic, cryptographically
verifiable AgentFacts that supports multi-endpoint routing, load balancing,
privacy-preserving access, and credentialed capability assertions. Our
architecture design delivers five concrete guarantees: (1) A quilt-like index
proposal that supports both NANDA-native agents as well as third party agents
being discoverable via the index, (2) rapid global resolution for newly spawned
AI agents, (3) sub-second revocation and key rotation, (4) schema-validated
capability assertions, and (5) privacy-preserving discovery across
organisational boundaries via verifiable, least-disclosure queries. We
formalize the AgentFacts schema, specify a CRDT-based update protocol, and
prototype adaptive resolvers. The result is a lightweight, horizontally
scalable foundation that unlocks secure, trust-aware collaboration for the next
generation of the Internet of AI agents, without abandoning existing web
infrastructure.

</details>


### [86] [NetIntent: Leveraging Large Language Models for End-to-End Intent-Based SDN Automation](https://arxiv.org/abs/2507.14398)
*Md. Kamrul Hossain,Walid Aljoby*

Main category: cs.NI

TL;DR: 本研究引入了IBNBench基准测试套件，用于评估大型语言模型（LLMs）在意图翻译和冲突检测等意图驱动网络（IBN）任务中的性能。同时，提出NetIntent框架，该框架利用LLMs自动化整个IBN生命周期，实现了SDN系统中意图的端到端实现。


<details>
  <summary>Details</summary>
Motivation: 当前意图驱动网络（IBN）从高级意图到低级配置的自动化面临挑战，现有解决方案因其僵化和有限的扩展性而不足。尽管大型语言模型（LLMs）展现出潜力，但它们在IBN任务中的具体能力尚不明确，且缺乏将LLMs整合到完整自动化IBN流程中的统一框架。

Method: 本研究首先构建了IBNBench基准测试套件，包含Intent2Flow-ODL、Intent2Flow-ONOS、FlowConflict-ODL、FlowConflict-ONOS四个数据集，用于评估LLMs在意图翻译和冲突检测任务中的性能。在此基础上，研究团队对比了33个开源LLMs的表现。其次，提出了NetIntent框架，该框架通过整合LLMs和非LLM代理，利用动态重新提示和上下文反馈机制，自动化了IBN的完整生命周期（包括翻译、激活和保障）。该框架已在ODL和ONOS SDN控制器上实现。

Result: IBNBench基准测试结果显示，33个开源LLMs在意图驱动网络任务上的表现差异显著，证明了LLMs在独立IBN任务中的潜力。NetIntent框架在ODL和ONOS SDN控制器上的实现，成功实现了稳定且自适应的端到端IBN自动化，显著减少了人工干预。

Conclusion: 大型语言模型在部分意图驱动网络任务中展现出巨大潜力。本研究所提出的NetIntent框架成功地将LLMs整合到意图驱动网络的全生命周期管理中，提供了一种统一、灵活且自动化的解决方案，实现了SDN系统中用户定义意图的鲁棒执行。

Abstract: Intent-Based Networking (IBN) often leverages the programmability of
Software-Defined Networking (SDN) to simplify network management. However,
significant challenges remain in automating the entire pipeline, from
user-specified high-level intents to device-specific low-level configurations.
Existing solutions often rely on rigid, rule-based translators and fixed APIs,
limiting extensibility and adaptability. By contrast, recent advances in large
language models (LLMs) offer a promising pathway that leverages natural
language understanding and flexible reasoning. However, it is unclear to what
extent LLMs can perform IBN tasks. To address this, we introduce IBNBench, a
first-of-its-kind benchmarking suite comprising four novel datasets:
Intent2Flow-ODL, Intent2Flow-ONOS, FlowConflict-ODL, and FlowConflict-ONOS.
These datasets are specifically designed for evaluating LLMs performance in
intent translation and conflict detection tasks within the industry-grade SDN
controllers ODL and ONOS. Our results provide the first comprehensive
comparison of 33 open-source LLMs on IBNBench and related datasets, revealing a
wide range of performance outcomes. However, while these results demonstrate
the potential of LLMs for isolated IBN tasks, integrating LLMs into a fully
autonomous IBN pipeline remains unexplored. Thus, our second contribution is
NetIntent, a unified and adaptable framework that leverages LLMs to automate
the full IBN lifecycle, including translation, activation, and assurance within
SDN systems. NetIntent orchestrates both LLM and non-LLM agents, supporting
dynamic re-prompting and contextual feedback to robustly execute user-defined
intents with minimal human intervention. Our implementation of NetIntent across
both ODL and ONOS SDN controllers achieves a consistent and adaptive end-to-end
IBN realization.

</details>


### [87] [Dora: A Controller Provisioning Strategy in Hierarchical Domain-based Satellite Networks](https://arxiv.org/abs/2507.14512)
*Qiyuan Peng,Qi Zhang,Yue Gao,Kun Qiu*

Main category: cs.NI

TL;DR: 为解决SAGIN中卫星网络控制器配置的挑战，本文提出三层域架构和基于强化学习的Dora策略，显著提升了配置质量并大幅缩短了计算时间。


<details>
  <summary>Details</summary>
Motivation: 空天地一体化网络（SAGIN）中卫星星座的快速部署，对网络管理提出了巨大挑战。传统的扁平网络架构难以应对海量分布式节点的同步和数据传输。现有网络管理架构及传统搜索算法，由于卫星计算资源有限和严格的时间限制，无法高效生成控制器配置方案。

Method: 本文提出一个三层域架构，以增强网络的可扩展性和适应性。在此基础上，引入了基于强化学习的Dora控制器配置策略，旨在优化网络性能同时最小化计算开销。

Result: Dora策略在控制器配置质量上比现有最先进的基准算法提高了10%，同时计算时间仅为传统算法的1/30到1/90。

Conclusion: 研究结果表明，强化学习方法在下一代SAGIN部署中实现高效卫星网络管理具有巨大潜力。

Abstract: The rapid proliferation of satellite constellations in Space-Air-Ground
Integrated Networks (SAGIN) presents significant challenges for network
management. Conventional flat network architectures struggle with
synchronization and data transmission across massive distributed nodes. In
response, hierarchical domain-based satellite network architectures have
emerged as a scalable solution, highlighting the critical importance of
controller provisioning strategies. However, existing network management
architectures and traditional search-based algorithms fail to generate
efficient controller provisioning solutions due to limited computational
resources in satellites and strict time constraints. To address these
challenges, we propose a three-layer domain-based architecture that enhances
both scalability and adaptability. Furthermore, we introduce Dora, a
reinforcement learning-based controller provisioning strategy designed to
optimize network performance while minimizing computational overhead. Our
comprehensive experimental evaluation demonstrates that Dora significantly
outperforms state-of-the-art benchmarks, achieving 10% improvement in
controller provisioning quality while requiring only 1/30 to 1/90 of the
computation time compared to traditional algorithms. These results underscore
the potential of reinforcement learning approaches for efficient satellite
network management in next-generation SAGIN deployments.

</details>


### [88] [UAV-Enabled Wireless-Powered Underground Communication Networks: A Novel Time Allocation Approach](https://arxiv.org/abs/2507.14627)
*Kaiqiang Lin,Yijie Mao,Onel Luis Alcaraz López,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 为解决无线供电地下通信网络（WPUCNs）的经济不可行性，本文提出一种无人机辅助的WPUCN系统，通过混合无线能量传输和优化时间分配，显著降低无人机能耗，实现可持续地下监测。


<details>
  <summary>Details</summary>
Motivation: 大规模无线供电地下通信网络（WPUCNs）由于严峻的地下信号衰减和高昂的信道状态信息（CSI）获取成本，在实践中经济上不可行。

Method: 引入无人机（UAV）到WPUCNs中，建立无人机能耗模型和混合无线能量传输（WET）方法（地下设备从地面混合接入点和无人机获取能量），该方法结合全CSI和免CSI多天线波束成形。在此基础上，提出并解决一个时间分配问题，以最小化无人机能耗，同时满足地下设备吞吐量和数据卸载需求。

Result: 仿真结果表明，所提出的混合WET方法优于其他WET方法，其性能受天线数量、通信距离、地下设备数量和地下条件影响。在优化时间分配下，基于免CSI多天线方案的混合WET实现了最低的无人机能耗。

Conclusion: 该研究证明了所提出的无人机辅助WPUCN系统能够通过高效的能量传输和优化管理，实现WPUCN中可持续的地下监测。

Abstract: Wireless-powered underground communication networks (WPUCNs), which allow
underground devices (UDs) to harvest energy from wireless signals for
battery-free communication, offer a promising solution for sustainable
underground monitoring. However, the severe wireless signal attenuation in
challenging underground environments and the costly acquisition of channel
state information (CSI) make large-scale WPUCNs economically infeasible in
practice. To address this challenge, we introduce flexible unmanned aerial
vehicles (UAVs) into WPUCNs, leading to UAV-enabled WPUCN systems. In this
system, a UAV is first charged by a terrestrial hybrid access point (HAP), then
flies to the monitoring area to wirelessly charge UDs. Afterwards, the UAV
collects data from the UDs and finally returns to the HAP for data offloading.
Based on the proposed UAV-enabled WPUCN system, we first propose its energy
consumption model and a hybrid wireless energy transfer (WET) approach (i.e.,
UDs can harvest energy from both the HAP and the UAV) relying on full-CSI and
CSI-free multi-antenna beamforming. Then, we formulate and address a time
allocation problem to minimize the energy consumption of UAV, while ensuring
that the throughput requirements of all UDs are met and all sensor data is
offloaded. Through simulations of a realistic farming scenario, we demonstrate
that the proposed hybrid WET approach outperforms other WET approaches, with
performance gains influenced by the number of antennas, communication distance,
number of UDs, and underground conditions. Additionally, under the optimized
time allocation, we found that the proposed hybrid WET approach based on a
CSI-free multi-antenna scheme achieves the lowest UAV's energy consumption
among all WET mechanisms, thereby enabling sustainable underground monitoring
in WPUCNs.

</details>


### [89] [Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks: A Survey on Generative Approaches](https://arxiv.org/abs/2507.14633)
*Xiaozheng Gao,Yichen Wang,Bosen Liu,Xiao Zhou,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Dong In Kim,Abbas Jamalipour,Chau Yuen,Jianping An,Kai Yang*

Main category: cs.NI

TL;DR: 本综述探讨了如何利用生成式AI（GAI）和大型语言模型（LLMs）在卫星增强低空经济和地面网络（SLAETNs）中实现智能体AI。


<details>
  <summary>Details</summary>
Motivation: 卫星增强低空经济和地面网络（SLAETNs）的发展需要智能和自主系统，以在异构、动态和任务关键型环境中可靠运行。

Method: 综述首先介绍了SLAETNs的架构和挑战；系统回顾了五类生成模型（VAEs, GANs, GDMs, TBMs, LLMs），并进行了比较分析；接着探讨了这些模型如何在通信增强、安全隐私保护和智能卫星任务中赋能智能体功能；最后提出了未来的研究方向。

Result: 本综述建立了一个模型驱动的生成模型基础，并分析了它们在SLAETNs中的部署权衡；展示了这些模型如何在通信增强、安全隐私保护和智能卫星任务等三个领域赋能智能体功能。

Conclusion: 本综述旨在为下一代集成网络中智能体AI的发展提供统一的理解和可操作的参考。

Abstract: The development of satellite-augmented low-altitude economy and terrestrial
networks (SLAETNs) demands intelligent and autonomous systems that can operate
reliably across heterogeneous, dynamic, and mission-critical environments. To
address these challenges, this survey focuses on enabling agentic artificial
intelligence (AI), that is, artificial agents capable of perceiving, reasoning,
and acting, through generative AI (GAI) and large language models (LLMs). We
begin by introducing the architecture and characteristics of SLAETNs, and
analyzing the challenges that arise in integrating satellite, aerial, and
terrestrial components. Then, we present a model-driven foundation by
systematically reviewing five major categories of generative models:
variational autoencoders (VAEs), generative adversarial networks (GANs),
generative diffusion models (GDMs), transformer-based models (TBMs), and LLMs.
Moreover, we provide a comparative analysis to highlight their generative
mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on
this foundation, we examine how these models empower agentic functions across
three domains: communication enhancement, security and privacy protection, and
intelligent satellite tasks. Finally, we outline key future directions for
building scalable, adaptive, and trustworthy generative agents in SLAETNs. This
survey aims to provide a unified understanding and actionable reference for
advancing agentic AI in next-generation integrated networks.

</details>


### [90] [Data-Plane Telemetry to Mitigate Long-Distance BGP Hijacks](https://arxiv.org/abs/2507.14842)
*Satadal Sengupta,Hyojoon Kim,Daniel Jubas,Maria Apostolaki,Jennifer Rexford*

Main category: cs.NI

TL;DR: 本文提出并验证了一种基于数据平面传播延迟变化的劫持检测方法，特别是针对将国内流量重路由到国外的攻击。通过设计HiDe系统，证实了该方法在实际部署中具有高覆盖率和实用性。


<details>
  <summary>Details</summary>
Motivation: 互联网路由安全薄弱，导致攻击者劫持路由，将国内流量重路由至国外，从而引发监控、绕过隐私保护和国家安全威胁。现有检测和缓解措施主要关注控制平面，而忽视了数据平面中如传播延迟变化等有前景的信号。

Method: 探索利用传播延迟变化进行劫持检测的实用性，解决其覆盖范围和部署为持续在线系统的问题。设计了HiDe系统，该系统能够以线速可靠检测长距离劫持造成的延迟激增，并使用真实数据和伦理劫持对其准确率和误报率进行测量和验证。

Result: 观察到在全球86%的受害者-攻击者国家对中，攻击期间的延迟比攻击前增加了至少25%，这表明基于延迟的劫持检测具有前景。HiDe系统能够以线速可靠地检测由长距离劫持引起的延迟激增。

Conclusion: 基于传播延迟变化的劫持检测方法对于检测将国内流量重路由到国外的长距离劫持是实用且有前景的，HiDe系统展示了其可靠性和在实际部署中的有效性。

Abstract: Poor security of Internet routing enables adversaries to divert user data
through unintended infrastructures (hijack). Of particular concern -- and the
focus of this paper -- are cases where attackers reroute domestic traffic
through foreign countries, exposing it to surveillance, bypassing legal privacy
protections, and posing national security threats. Efforts to detect and
mitigate such attacks have focused primarily on the control plane while
data-plane signals remain largely overlooked. In particular, change in
propagation delay caused by rerouting offers a promising signal: the change is
unavoidable and the increased propagation delay is directly observable from the
affected networks. In this paper, we explore the practicality of using delay
variations for hijack detection, addressing two key questions: (1) What
coverage can this provide, given its heavy dependence on the geolocations of
the sender, receiver, and adversary? and (2) Can an always-on latency-based
detection system be deployed without disrupting normal network operations? We
observe that for 86% of victim-attacker country pairs in the world, mid-attack
delays exceed pre-attack delays by at least 25% in real deployments, making
delay-based hijack detection promising. To demonstrate practicality, we design
HiDe, which reliably detects delay surges from long-distance hijacks at line
rate. We measure HiDe's accuracy and false-positive rate on real-world data and
validate it with ethically conducted hijacks.

</details>


### [91] [Tidal-Like Concept Drift in RIS-Covered Buildings: When Programmable Wireless Environments Meet Human Behaviors](https://arxiv.org/abs/2507.14876)
*Zi-Yang Wu,Muhammad Ismail,Jiliang Zhang,Jie Zhang*

Main category: cs.NI

TL;DR: 论文分析了室内RIS网络中由人类行为引起的复杂信道演变现象，指出通用信道模型不可行，并探讨了深度学习预测与控制策略面临的挑战及潜在解决方案。


<details>
  <summary>Details</summary>
Motivation: 室内移动网络性能受限于建筑结构，且传统建筑设计未考虑无线性能。受RIS在室外网络成功的启发，本研究旨在通过将RIS嵌入建筑提升室内无线性能，同时解决用户移动性带来的复杂信道动态问题。

Method: 本文首次系统性地研究了由复杂人类行为引起的RIS覆盖建筑中信道的“潮汐演变现象”。通过对信道动态的深入理解，探讨了基于深度学习的预测与控制策略所面临的挑战。

Result: 研究表明，在RIS覆盖建筑中，由于人类行为引起的复杂动态，建立一个通用的信道模型是不可行的。进一步分析了深度学习策略所面临的具体挑战，包括高阶马尔可夫依赖性、概念漂移和泛化问题。

Conclusion: 室内用户移动性是RIS覆盖建筑性能的关键影响因素。理解并管理人类行为模式对于实现无线友好型建筑设计至关重要。本文为RIS覆盖建筑与人群移动的有效共存提供了策略方向。

Abstract: Indoor mobile networks handle the majority of data traffic, with their
performance limited by building materials and structures. However, building
designs have historically not prioritized wireless performance. Prior to the
advent of reconfigurable intelligent surfaces (RIS), the industry passively
adapted to wireless propagation challenges within buildings. Inspired by RIS's
successes in outdoor networks, we propose embedding RIS into building
structures to manipulate and enhance building wireless performance
comprehensively. Nonetheless, the ubiquitous mobility of users introduces
complex dynamics to the channels of RIS-covered buildings. A deep understanding
of indoor human behavior patterns is essential for achieving wireless-friendly
building design. This article is the first to systematically examine the tidal
evolution phenomena emerging in the channels of RIS-covered buildings driven by
complex human behaviors. We demonstrate that a universal channel model is
unattainable and focus on analyzing the challenges faced by advanced deep
learning-based prediction and control strategies, including high-order Markov
dependencies, concept drift, and generalization issues caused by human-induced
disturbances. Possible solutions for orchestrating the coexistence of
RIS-covered buildings and crowd mobility are also laid out.

</details>


### [92] [FENIX: Enabling In-Network DNN Inference with FPGA-Enhanced Programmable Switches](https://arxiv.org/abs/2507.14891)
*Xiangyu Gao,Tong Li,Yinchao Zhang,Ziqiang Wang,Xiangsheng Zeng,Su Yao,Ke Xu*

Main category: cs.NI

TL;DR: FENIX是一个混合网络内机器学习系统，通过在可编程交换机ASIC上进行特征提取并在FPGA上进行深度神经网络推理，解决了现有解决方案在低延迟、高吞吐和高精度之间难以兼顾的问题，并实现了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有网络数据平面中的机器学习解决方案（如FlowLens、N3IC和BoS）在同时实现低延迟、高吞吐和高精度方面面临挑战。

Method: FENIX是一个混合网络内机器学习系统，核心方法包括：1) 在可编程交换机ASIC上进行特征提取；2) 在FPGA上进行深度神经网络推理；3) 引入数据引擎，使用概率令牌桶算法控制特征流发送速率，以解决ASIC与FPGA间的吞吐量差距；4) 设计模型引擎，实现在网络内进行高精度深度神经网络推理，克服在资源受限交换芯片上部署复杂模型的困难。系统在集成Tofino ASIC和ZU19EG FPGA的可编程交换机平台上实现并进行评估。

Result: FENIX在主流网络流量分类任务中取得了微秒级推理延迟、多太比特吞吐量和超过95%的准确率，且硬件开销低，性能优于现有SOTA方案。

Conclusion: FENIX通过创新的混合架构和引擎设计，成功解决了网络数据平面中机器学习系统在延迟、吞吐量和精度方面的性能瓶颈，实现了全面的性能提升，并超越了现有最佳水平。

Abstract: Machine learning (ML) is increasingly used in network data planes for
advanced traffic analysis. However, existing solutions (such as FlowLens, N3IC,
and BoS) still struggle to simultaneously achieve low latency, high throughput,
and high accuracy. To address these challenges, we present FENIX, a hybrid
in-network ML system that performs feature extraction on programmable switch
ASICs and deep neural network inference on FPGAs. FENIX introduces a Data
Engine that leverages a probabilistic token bucket algorithm to control the
sending rate of feature streams, effectively addressing the throughput gap
between programmable switch ASICs and FPGAs. In addition, FENIX designs a Model
Engine to enable high-accuracy deep neural network inference in the network,
overcoming the difficulty of deploying complex models on resource-constrained
switch chips. We implement FENIX on a programmable switch platform that
integrates a Tofino ASIC and a ZU19EG FPGA directly and evaluate it on
real-world network traffic datasets. Our results show that FENIX achieves
microsecond-level inference latency and multi-terabit throughput with low
hardware overhead, and delivers over 95\% accuracy on mainstream network
traffic classification tasks, outperforming SOTA.

</details>


### [93] [Quantum Machine Learning for Secure Cooperative Multi-Layer Edge AI with Proportional Fairness](https://arxiv.org/abs/2507.15145)
*Thai T. Vu,John Le*

Main category: cs.NI

TL;DR: 该论文提出了一个通信高效、事件触发的协同边缘AI推理框架，通过联合优化最大化分类效用，并在多设备环境下显著提升系统性能和资源公平性。


<details>
  <summary>Details</summary>
Motivation: 在由多用户设备和边缘服务器组成的协同边缘AI系统中，如何在通信、能耗和用户公平性约束下，高效地进行事件触发的推理，并最大化分类效用。

Method: 1. 提出了一个通信高效、事件触发的推理框架。
2. 借鉴双阈值早期退出策略，将单设备推理扩展到分布式多设备设置。
3. 纳入用户间比例公平性约束。
4. 构建一个联合优化框架，以在通信、能耗和公平性约束下最大化分类效用。
5. 利用效用函数对置信阈值的单调性，并应用交替优化与Benders分解高效求解该问题。

Result: 实验结果表明，所提出的框架与单设备基线相比，显著提升了系统范围的性能和资源分配的公平性。

Conclusion: 所提出的通信高效、事件触发的协同边缘AI推理框架，能够有效提高分布式多设备环境下的系统性能和资源分配公平性。

Abstract: This paper proposes a communication-efficient, event-triggered inference
framework for cooperative edge AI systems comprising multiple user devices and
edge servers. Building upon dual-threshold early-exit strategies for rare-event
detection, the proposed approach extends classical single-device inference to a
distributed, multi-device setting while incorporating proportional fairness
constraints across users. A joint optimization framework is formulated to
maximize classification utility under communication, energy, and fairness
constraints. To solve the resulting problem efficiently, we exploit the
monotonicity of the utility function with respect to the confidence thresholds
and apply alternating optimization with Benders decomposition. Experimental
results show that the proposed framework significantly enhances system-wide
performance and fairness in resource allocation compared to single-device
baselines.

</details>
