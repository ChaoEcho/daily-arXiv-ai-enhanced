{"id": "2507.14189", "pdf": "https://arxiv.org/pdf/2507.14189", "abs": "https://arxiv.org/abs/2507.14189", "authors": ["Song Mao", "Lejun Cheng", "Pinlong Cai", "Guohang Yan", "Ding Wang", "Botian Shi"], "title": "DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base", "categories": ["cs.CL", "cs.AI"], "comment": "work in process", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious applications. However, their use as writing assistants in specialized\ndomains like finance, medicine, and law is often hampered by a lack of deep\ndomain-specific knowledge and a tendency to hallucinate. Existing solutions,\nsuch as Retrieval-Augmented Generation (RAG), can suffer from inconsistency\nacross multiple retrieval steps, while online search-based methods often\ndegrade quality due to unreliable web content. To address these challenges, we\nintroduce DeepWriter, a customizable, multimodal, long-form writing assistant\nthat operates on a curated, offline knowledge base. DeepWriter leverages a\nnovel pipeline that involves task decomposition, outline generation, multimodal\nretrieval, and section-by-section composition with reflection. By deeply mining\ninformation from a structured corpus and incorporating both textual and visual\nelements, DeepWriter generates coherent, factually grounded, and\nprofessional-grade documents. We also propose a hierarchical knowledge\nrepresentation to enhance retrieval efficiency and accuracy. Our experiments on\nfinancial report generation demonstrate that DeepWriter produces high-quality,\nverifiable articles that surpasses existing baselines in factual accuracy and\ngenerated content quality.", "AI": {"tldr": "DeepWriter\u662f\u4e00\u4e2a\u57fa\u4e8e\u7cbe\u9009\u79bb\u7ebf\u77e5\u8bc6\u5e93\u7684\u53ef\u5b9a\u5236\u3001\u591a\u6a21\u6001\u957f\u6587\u5199\u4f5c\u52a9\u624b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6d41\u6c34\u7ebf\u548c\u5206\u5c42\u77e5\u8bc6\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u5199\u4f5c\u4e2d\u77e5\u8bc6\u4e0d\u8db3\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u5728\u91d1\u878d\u62a5\u544a\u751f\u6210\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u5185\u5bb9\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u91d1\u878d\u3001\u533b\u7597\u3001\u6cd5\u5f8b\u7b49\u4e13\u4e1a\u9886\u57df\u4f5c\u4e3a\u5199\u4f5c\u52a9\u624b\u65f6\uff0c\u9762\u4e34\u9886\u57df\u77e5\u8bc6\u7f3a\u4e4f\u548c\u5e7b\u89c9\u95ee\u9898\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5982\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5b58\u5728\u591a\u6b65\u68c0\u7d22\u4e0d\u4e00\u81f4\u6027\uff0c\u800c\u57fa\u4e8e\u5728\u7ebf\u641c\u7d22\u7684\u65b9\u6cd5\u5219\u53d7\u5236\u4e8e\u4e0d\u53ef\u9760\u7684\u7f51\u7edc\u5185\u5bb9\uff0c\u5bfc\u81f4\u5185\u5bb9\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u672c\u6587\u63d0\u51faDeepWriter\uff0c\u4e00\u4e2a\u53ef\u5b9a\u5236\u3001\u591a\u6a21\u6001\u4e14\u57fa\u4e8e\u7cbe\u9009\u79bb\u7ebf\u77e5\u8bc6\u5e93\u7684\u957f\u6587\u5199\u4f5c\u52a9\u624b\u3002\u5b83\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6d41\u6c34\u7ebf\uff0c\u5305\u62ec\u4efb\u52a1\u5206\u89e3\u3001\u5927\u7eb2\u751f\u6210\u3001\u591a\u6a21\u6001\u68c0\u7d22\u4ee5\u53ca\u5e26\u6709\u53cd\u601d\u7684\u5206\u6bb5\u5f0f\u5185\u5bb9\u521b\u4f5c\u3002\u6b64\u5916\uff0c\u4e3a\u63d0\u9ad8\u68c0\u7d22\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u8be5\u65b9\u6cd5\u8fd8\u5f15\u5165\u4e86\u5206\u5c42\u77e5\u8bc6\u8868\u793a\u3002", "result": "\u5728\u91d1\u878d\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDeepWriter\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u9a8c\u8bc1\u7684\u6587\u7ae0\uff0c\u5176\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u751f\u6210\u5185\u5bb9\u8d28\u91cf\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DeepWriter\u901a\u8fc7\u5229\u7528\u7cbe\u9009\u7684\u79bb\u7ebf\u77e5\u8bc6\u5e93\u3001\u7ed3\u5408\u591a\u6a21\u6001\u4fe1\u606f\u548c\u521b\u65b0\u7684\u5199\u4f5c\u6d41\u6c34\u7ebf\uff0c\u6210\u529f\u514b\u670d\u4e86LLMs\u5728\u4e13\u4e1a\u9886\u57df\u957f\u6587\u5199\u4f5c\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u751f\u6210\u4e8b\u5b9e\u51c6\u786e\u3001\u4e13\u4e1a\u6c34\u51c6\u6587\u6863\u7684\u76ee\u6807\u3002"}}
{"id": "2507.14198", "pdf": "https://arxiv.org/pdf/2507.14198", "abs": "https://arxiv.org/abs/2507.14198", "authors": ["Fufang Wen", "Shichang Zhang"], "title": "Retention analysis of edited knowledge after fine-tuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) store vast amounts of knowledge, which often\nrequires updates to correct factual errors, incorporate newly acquired\ninformation, or adapt model behavior. Model editing methods have emerged as\nefficient solutions for such updates, offering localized and precise knowledge\nmodification at significantly lower computational cost than continual training.\nIn parallel, LLMs are frequently fine-tuned for a wide range of downstream\ntasks. However, the effect of fine-tuning on previously edited knowledge\nremains poorly understood. In this work, we systematically investigate how\ndifferent fine-tuning objectives interact with various model editing\ntechniques. Our findings show that edited knowledge is substantially more\nsusceptible to forgetting during fine-tuning than intrinsic knowledge acquired\nthrough pre-training. This analysis highlights a key limitation of current\nediting approaches and suggests that evaluating edit robustness under\ndownstream fine-tuning is critical for their practical deployment. We further\nfind that freezing layers associated with edited content can significantly\nimprove knowledge retention, offering insight into how future editing methods\nmight be made more robust.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7f16\u8f91\u8fc7\u7684\u77e5\u8bc6\u5728\u5fae\u8c03\u65f6\u6bd4\u56fa\u6709\u77e5\u8bc6\u66f4\u5bb9\u6613\u88ab\u9057\u5fd8\uff0c\u4f46\u51bb\u7ed3\u76f8\u5173\u5c42\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u77e5\u8bc6\u4fdd\u7559\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u77e5\u8bc6\u66f4\u65b0\u5e38\u4f7f\u7528\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\uff0c\u540c\u65f6LLMs\u4e5f\u5e7f\u6cdb\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7684\u5fae\u8c03\u3002\u7136\u800c\uff0c\u5fae\u8c03\u5bf9\u5148\u524d\u7f16\u8f91\u8fc7\u7684\u77e5\u8bc6\u7684\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u7cfb\u7edf\u5730\u8c03\u67e5\u4e86\u4e0d\u540c\u7684\u5fae\u8c03\u76ee\u6807\u5982\u4f55\u4e0e\u5404\u79cd\u6a21\u578b\u7f16\u8f91\u6280\u672f\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5df2\u7f16\u8f91\u7684\u77e5\u8bc6\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6bd4\u9884\u8bad\u7ec3\u83b7\u5f97\u7684\u56fa\u6709\u77e5\u8bc6\u66f4\u5bb9\u6613\u88ab\u9057\u5fd8\u3002\u51bb\u7ed3\u4e0e\u7f16\u8f91\u5185\u5bb9\u76f8\u5173\u7684\u5c42\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u77e5\u8bc6\u4fdd\u7559\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u5728\u5fae\u8c03\u4e0b\u5bf9\u5df2\u7f16\u8f91\u77e5\u8bc6\u7684\u9c81\u68d2\u6027\u5b58\u5728\u5173\u952e\u5c40\u9650\u3002\u5728\u4e0b\u6e38\u5fae\u8c03\u4e2d\u8bc4\u4f30\u7f16\u8f91\u9c81\u68d2\u6027\u5bf9\u4e8e\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u51bb\u7ed3\u76f8\u5173\u5c42\u4e3a\u672a\u6765\u66f4\u9c81\u68d2\u7684\u7f16\u8f91\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.14200", "pdf": "https://arxiv.org/pdf/2507.14200", "abs": "https://arxiv.org/abs/2507.14200", "authors": ["Shengji Tang", "Jianjian Cao", "Weihao Lin", "Jiale Hong", "Bo Zhang", "Shuyue Hu", "Lei Bai", "Tao Chen", "Wanli Ouyang", "Peng Ye"], "title": "Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper aims to demonstrate the potential and strengths of open-source\ncollectives. It leads to a promising question: Can we harness multiple\nopen-source LLMs to match or even beat the closed-source LLMs? To answer this,\nwe propose SMACS, a scalable multi-agent collaboration system (MACS) framework\nwith high performance. Specifically, for continuous integration of new LLMs and\ngeneralization to diverse questions, we first propose a Retrieval-based Prior\nSelection (RPS), which assigns a proxy performance score to each LLM to select\nthe Top-k LLMs at the instance level for any given question. Then, we propose\nan Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the\ngeneration of diverse responses through prior dropping and selecting the\nhigh-quality response via a hybrid posterior score. Experiments on eight\nmainstream benchmarks validate the effectiveness of our SMACS: by integrating\nfifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,\ne.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)\nacross multiple tasks. Remarkably, it even exceeds the average of best results\nof different datasets from both open-source LLMs (+2.86%) and closed-source\nLLMs (+2.04%), pushing the upper bound of intelligence. Code will be released\nat https://github.com/magent4aci/SMACS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSMACS\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u540c\u591a\u4e2a\u5f00\u6e90LLM\uff0c\u663e\u8457\u8d85\u8d8a\u4e3b\u6d41\u95ed\u6e90LLM\u6027\u80fd\uff0c\u5e76\u63a8\u52a8\u4e86\u667a\u80fd\u4e0a\u9650\u3002", "motivation": "\u65e8\u5728\u63a2\u8ba8\u80fd\u5426\u901a\u8fc7\u96c6\u5408\u591a\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4f18\u52bf\uff0c\u4f7f\u5176\u6027\u80fd\u5339\u654c\u751a\u81f3\u8d85\u8d8a\u95ed\u6e90LLMs\u3002", "method": "\u63d0\u51faSMACS\uff08\u53ef\u6269\u5c55\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7cfb\u7edf\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5177\u6709\u9ad8\u6027\u80fd\uff0c\u5e76\u5305\u542b\uff1a1. **\u57fa\u4e8e\u68c0\u7d22\u7684\u5148\u9a8c\u9009\u62e9\uff08RPS\uff09**\uff1a\u4e3a\u6bcf\u4e2aLLM\u5206\u914d\u4ee3\u7406\u6027\u80fd\u5206\u6570\uff0c\u4ee5\u5728\u5b9e\u4f8b\u7ea7\u522b\u9009\u62e9Top-k LLMs\u30022. **\u63a2\u7d22-\u5229\u7528\u9a71\u52a8\u7684\u540e\u9a8c\u589e\u5f3a\uff08EPE\uff09**\uff1a\u901a\u8fc7\u5148\u9a8c\u4e22\u5f03\u9f13\u52b1\u751f\u6210\u591a\u6837\u5316\u54cd\u5e94\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u540e\u9a8c\u5206\u6570\u9009\u62e9\u9ad8\u8d28\u91cf\u54cd\u5e94\u3002", "result": "\u5728\u516b\u4e2a\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cSMACS\uff08\u6574\u5408\u5341\u4e94\u4e2a\u5f00\u6e90LLM\uff09\u7684\u6709\u6548\u6027\u5f97\u5230\u9a8c\u8bc1\u3002\u5176\u6027\u80fd\u8d85\u8d8a\u4e86\u9886\u5148\u7684\u95ed\u6e90LLMs\uff0c\u4f8b\u5982Claude-3.7-Sonnet (+12.73%)\u3001GPT-4.1 (+5.36%)\u548cGPT-o3-mini (+5.28%)\u3002SMACS\u751a\u81f3\u8d85\u8fc7\u4e86\u5f00\u6e90LLM (+2.86%)\u548c\u95ed\u6e90LLM (+2.04%)\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u6700\u4f73\u5e73\u5747\u7ed3\u679c\u3002", "conclusion": "SMACS\u7cfb\u7edf\u6210\u529f\u8bc1\u660e\u4e86\u901a\u8fc7\u534f\u540c\u591a\u4e2a\u5f00\u6e90LLM\uff0c\u4e0d\u4ec5\u80fd\u5339\u654c\u751a\u81f3\u8d85\u8d8a\u9886\u5148\u95ed\u6e90LLM\u7684\u6027\u80fd\uff0c\u800c\u4e14\u80fd\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u7684\u4e0a\u9650\uff0c\u51f8\u663e\u4e86\u5f00\u6e90\u534f\u4f5c\u5728LLM\u9886\u57df\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.14214", "pdf": "https://arxiv.org/pdf/2507.14214", "abs": "https://arxiv.org/abs/2507.14214", "authors": ["Rui Zhao", "Vladyslav Melnychuk", "Jun Zhao", "Jesse Wright", "Nigel Shadbolt"], "title": "Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale", "categories": ["cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites despite claiming\notherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that\nassists users with personalized privacy policy analysis. PoliAnalyzer uses\nNatural Language Processing (NLP) to extract formal representations of data\nusage practices from policy texts. In favor of deterministic, logical inference\nis applied to compare user preferences with the formal privacy policy\nrepresentation and produce a compliance report. To achieve this, we extend an\nexisting formal Data Terms of Use policy language to model privacy policies as\napp policies and user preferences as data policies. In our evaluation using our\nenriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated\nhigh accuracy in identifying relevant data usage practices, achieving F1-score\nof 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can\nmodel diverse user data-sharing preferences, derived from prior research as 23\nuser profiles, and perform compliance analysis against the top 100 most-visited\nwebsites. This analysis revealed that, on average, 95.2% of a privacy policy's\nsegments do not conflict with the analyzed user preferences, enabling users to\nconcentrate on understanding the 4.8% (636 / 13205) that violates preferences,\nsignificantly reducing cognitive burden. Further, we identified common\npractices in privacy policies that violate user expectations - such as the\nsharing of location data with 3rd parties. This paper demonstrates that\nPoliAnalyzer can support automated personalized privacy policy analysis at\nscale using off-the-shelf NLP tools. This sheds light on a pathway to help\nindividuals regain control over their data and encourage societal discussions\non platform data practices to promote a fairer power dynamic.", "AI": {"tldr": "PoliAnalyzer\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5206\u6790\u5e2e\u52a9\u7528\u6237\u4e2a\u6027\u5316\u7406\u89e3\u9690\u79c1\u653f\u7b56\uff0c\u663e\u8457\u964d\u4f4e\u9605\u8bfb\u8d1f\u62c5\u5e76\u63d0\u9ad8\u6570\u636e\u63a7\u5236\u529b\u3002", "motivation": "\u73b0\u4ee3\u7528\u6237\u867d\u62e5\u6709\u4f17\u591a\u5728\u7ebf\u8d26\u6237\uff0c\u4f46\u666e\u904d\u5ffd\u89c6\u9605\u8bfb\u5176\u670d\u52a1\u6761\u6b3e\u548c\u9690\u79c1\u653f\u7b56\uff0c\u5bfc\u81f4\u5bf9\u81ea\u8eab\u6570\u636e\u4f7f\u7528\u60c5\u51b5\u7f3a\u4e4f\u4e86\u89e3\uff0c\u589e\u52a0\u4e86\u8ba4\u77e5\u8d1f\u62c5\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86PoliAnalyzer\uff0c\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4ece\u653f\u7b56\u6587\u672c\u4e2d\u63d0\u53d6\u6570\u636e\u4f7f\u7528\u5b9e\u8df5\u7684\u6b63\u5f0f\u8868\u793a\u3002\u7136\u540e\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u903b\u8f91\u63a8\u7406\uff0c\u5c06\u7528\u6237\u4e2a\u6027\u5316\u504f\u597d\u4e0e\u9690\u79c1\u653f\u7b56\u7684\u6b63\u5f0f\u8868\u793a\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u751f\u6210\u5408\u89c4\u6027\u62a5\u544a\u3002\u4e3a\u5b9e\u73b0\u6b64\u76ee\u6807\uff0c\u7cfb\u7edf\u6269\u5c55\u4e86\u73b0\u6709\u7684\u6570\u636e\u4f7f\u7528\u6761\u6b3e\u653f\u7b56\u8bed\u8a00\uff0c\u5c06\u9690\u79c1\u653f\u7b56\u5efa\u6a21\u4e3a\u5e94\u7528\u653f\u7b56\uff0c\u7528\u6237\u504f\u597d\u5efa\u6a21\u4e3a\u6570\u636e\u653f\u7b56\u3002", "result": "PoliAnalyzer\u5728\u6cd5\u5f8b\u4e13\u5bb6\u6574\u7406\u7684PolicyIE\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\uff0c\u5728\u8bc6\u522b\u76f8\u5173\u6570\u636e\u4f7f\u7528\u5b9e\u8df5\u65b9\u9762\uff0c\u5927\u591a\u6570\u4efb\u52a1\u7684F1\u5206\u6570\u8fbe\u523090-100%\u3002\u7cfb\u7edf\u6210\u529f\u5efa\u6a21\u4e8623\u79cd\u4e0d\u540c\u7684\u7528\u6237\u6570\u636e\u5171\u4eab\u504f\u597d\uff0c\u5e76\u5bf9\u6392\u540d\u524d100\u4f4d\u7684\u7f51\u7ad9\u8fdb\u884c\u4e86\u5408\u89c4\u6027\u5206\u6790\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5e73\u574795.2%\u7684\u9690\u79c1\u653f\u7b56\u7247\u6bb5\u4e0e\u7528\u6237\u504f\u597d\u4e0d\u51b2\u7a81\uff0c\u7528\u6237\u53ea\u9700\u5173\u6ce84.8%\uff08636/13205\uff09\u7684\u8fdd\u89c4\u90e8\u5206\uff0c\u663e\u8457\u51cf\u8f7b\u4e86\u8ba4\u77e5\u8d1f\u62c5\u3002\u7814\u7a76\u8fd8\u8bc6\u522b\u51fa\u9690\u79c1\u653f\u7b56\u4e2d\u5e38\u89c1\u7684\u8fdd\u53cd\u7528\u6237\u671f\u671b\u7684\u505a\u6cd5\uff0c\u4f8b\u5982\u4e0e\u7b2c\u4e09\u65b9\u5171\u4eab\u4f4d\u7f6e\u6570\u636e\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660ePoliAnalyzer\u80fd\u591f\u4f7f\u7528\u73b0\u6210\u7684NLP\u5de5\u5177\uff0c\u5927\u89c4\u6a21\u652f\u6301\u81ea\u52a8\u5316\u3001\u4e2a\u6027\u5316\u7684\u9690\u79c1\u653f\u7b56\u5206\u6790\u3002\u8fd9\u4e3a\u5e2e\u52a9\u4e2a\u4eba\u91cd\u65b0\u83b7\u5f97\u5bf9\u5176\u6570\u636e\u7684\u63a7\u5236\u6743\uff0c\u5e76\u4fc3\u8fdb\u5173\u4e8e\u5e73\u53f0\u6570\u636e\u5b9e\u8df5\u7684\u793e\u4f1a\u8ba8\u8bba\u4ee5\u5efa\u7acb\u66f4\u516c\u5e73\u7684\u6743\u529b\u52a8\u6001\u63d0\u4f9b\u4e86\u53ef\u884c\u9014\u5f84\u3002"}}
{"id": "2507.14183", "pdf": "https://arxiv.org/pdf/2507.14183", "abs": "https://arxiv.org/abs/2507.14183", "authors": ["Arash Aryapour"], "title": "Iran's Stealth Internet Blackout: A New Model of Censorship", "categories": ["cs.NI"], "comment": null, "summary": "In mid-2025, Iran experienced a novel, stealthy Internet shutdown that\npreserved global routing presence while isolating domestic users through deep\npacket inspection, aggressive throttling, and selective protocol blocking. This\npaper analyzes active network measurements such as DNS poisoning, HTTP\ninjection, TLS interception, and protocol whitelisting, traced to a centralized\nborder gateway. We quantify an approximate 707 percent rise in VPN demand and\ndescribe the multi-layered censorship infrastructure, highlighting implications\nfor circumvention and digital rights monitoring.", "AI": {"tldr": "\u4f0a\u6717\u57282025\u5e74\u4e2d\u671f\u5b9e\u65bd\u4e86\u4e00\u79cd\u65b0\u578b\u9690\u853d\u7684\u4e92\u8054\u7f51\u5173\u505c\uff0c\u901a\u8fc7\u6df1\u5ea6\u5305\u68c0\u6d4b\u7b49\u6280\u672f\u9694\u79bb\u56fd\u5185\u7528\u6237\u3002\u672c\u7814\u7a76\u901a\u8fc7\u7f51\u7edc\u6d4b\u91cf\u5206\u6790\uff0c\u91cf\u5316\u4e86VPN\u9700\u6c42\u589e\u957f\u5e76\u63cf\u8ff0\u4e86\u5176\u5ba1\u67e5\u57fa\u7840\u8bbe\u65bd\u3002", "motivation": "\u5206\u6790\u4f0a\u6717\u4e8e2025\u5e74\u4e2d\u671f\u5b9e\u65bd\u7684\u4e00\u79cd\u5728\u7ef4\u6301\u5168\u7403\u8def\u7531\u540c\u65f6\u9694\u79bb\u56fd\u5185\u7528\u6237\u7684\u9690\u853d\u6027\u4e92\u8054\u7f51\u5173\u505c\uff0c\u5e76\u63ed\u793a\u5176\u6280\u672f\u7ec6\u8282\u548c\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u4e3b\u52a8\u7f51\u7edc\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5305\u62ecDNS\u6295\u6bd2\u3001HTTP\u6ce8\u5165\u3001TLS\u62e6\u622a\u548c\u534f\u8bae\u767d\u540d\u5355\uff0c\u5e76\u5c06\u8fd9\u4e9b\u63aa\u65bd\u8ffd\u6eaf\u81f3\u4e00\u4e2a\u96c6\u4e2d\u7684\u8fb9\u5883\u7f51\u5173\u3002", "result": "\u91cf\u5316\u4e86VPN\u9700\u6c42\u7ea6707%\u7684\u589e\u957f\uff0c\u5e76\u8be6\u7ec6\u63cf\u8ff0\u4e86\u4f0a\u6717\u591a\u5c42\u6b21\u7684\u5ba1\u67e5\u57fa\u7840\u8bbe\u65bd\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u8fd9\u79cd\u65b0\u578b\u4e92\u8054\u7f51\u5173\u505c\u5bf9\u89c4\u907f\u6280\u672f\u548c\u6570\u5b57\u6743\u5229\u76d1\u63a7\u7684\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2507.14154", "pdf": "https://arxiv.org/pdf/2507.14154", "abs": "https://arxiv.org/abs/2507.14154", "authors": ["Rahul Kabali"], "title": "The Free Will Equation: Quantum Field Analogies for AGI", "categories": ["cs.AI", "cs.LG", "68T05, 81P68", "I.2.6; I.2.0; F.1.2"], "comment": "22 pages, 5 figures. Submitted as an arXiv preprint. All code and\n  experiment details included in appendix", "summary": "Artificial General Intelligence (AGI) research traditionally focuses on\nalgorithms that optimize for specific goals under deterministic rules. Yet,\nhuman-like intelligence exhibits adaptive spontaneity - an ability to make\nunexpected choices or free decisions not strictly dictated by past data or\nimmediate reward. This trait, often dubbed \"free will\" in a loose sense, might\nbe crucial for creativity, robust adaptation, and avoiding ruts in\nproblem-solving. This paper proposes a theoretical framework, called the Free\nWill Equation, that draws analogies from quantum field theory to endow AGI\nagents with a form of adaptive, controlled stochasticity in their\ndecision-making process. The core idea is to treat an AI agent's cognitive\nstate as a superposition of potential actions or thoughts, which collapses\nprobabilistically into a concrete action when a decision is made - much like a\nquantum wavefunction collapsing upon measurement. By incorporating mechanisms\nanalogous to quantum fields, along with intrinsic motivation terms, we aim to\nimprove an agent's ability to explore novel strategies and adapt to unforeseen\nchanges. Experiments in a non-stationary multi-armed bandit environment\ndemonstrate that agents using this framework achieve higher rewards and policy\ndiversity compared to baseline methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u53d7\u91cf\u5b50\u573a\u8bba\u542f\u53d1\u7684\u201c\u81ea\u7531\u610f\u5fd7\u65b9\u7a0b\u201d\u6846\u67b6\uff0c\u65e8\u5728\u4e3aAGI\u667a\u80fd\u4f53\u5f15\u5165\u81ea\u9002\u5e94\u968f\u673a\u6027\uff0c\u4ee5\u589e\u5f3a\u5176\u63a2\u7d22\u548c\u9002\u5e94\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfAGI\u7814\u7a76\u4fa7\u91cd\u4e8e\u786e\u5b9a\u6027\u4f18\u5316\uff0c\u4f46\u7c7b\u4eba\u667a\u80fd\u5c55\u73b0\u51fa\u201c\u81ea\u53d1\u6027\u201d\uff08 loosely \u79f0\u4e3a\u201c\u81ea\u7531\u610f\u5fd7\u201d\uff09\uff0c\u8fd9\u5bf9\u4e8e\u521b\u9020\u529b\u3001\u9c81\u68d2\u9002\u5e94\u548c\u907f\u514d\u9677\u5165\u50f5\u5c40\u81f3\u5173\u91cd\u8981\uff0c\u800c\u4f20\u7edfAGI\u7f3a\u4e4f\u8fd9\u79cd\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u201c\u81ea\u7531\u610f\u5fd7\u65b9\u7a0b\u201d\u7406\u8bba\u6846\u67b6\uff0c\u501f\u9274\u91cf\u5b50\u573a\u8bba\uff0c\u5c06AI\u667a\u80fd\u4f53\u7684\u8ba4\u77e5\u72b6\u6001\u89c6\u4e3a\u6f5c\u5728\u52a8\u4f5c\u7684\u53e0\u52a0\uff0c\u901a\u8fc7\u6982\u7387\u6027\u5d29\u584c\u673a\u5236\u51b3\u5b9a\u5177\u4f53\u52a8\u4f5c\u3002\u901a\u8fc7\u7ed3\u5408\u7c7b\u4f3c\u91cf\u5b50\u573a\u7684\u673a\u5236\u548c\u5185\u5728\u6fc0\u52b1\u9879\uff0c\u65e8\u5728\u63d0\u5347\u667a\u80fd\u4f53\u63a2\u7d22\u65b0\u7b56\u7565\u548c\u9002\u5e94\u672a\u77e5\u53d8\u5316\u7684\u80fd\u529b\u3002", "result": "\u5728\u975e\u5e73\u7a33\u591a\u81c2\u8d4c\u535a\u673a\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u91c7\u7528\u8be5\u6846\u67b6\u7684\u667a\u80fd\u4f53\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u5956\u52b1\u548c\u7b56\u7565\u591a\u6837\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u201c\u81ea\u7531\u610f\u5fd7\u65b9\u7a0b\u201d\u6846\u67b6\u6210\u529f\u5730\u4e3aAGI\u667a\u80fd\u4f53\u8d4b\u4e88\u4e86\u81ea\u9002\u5e94\u7684\u3001\u53d7\u63a7\u7684\u968f\u673a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u548c\u9002\u5e94\u80fd\u529b\uff0c\u4e3a\u5b9e\u73b0\u66f4\u7c7b\u4eba\u7684\u901a\u7528\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.14268", "pdf": "https://arxiv.org/pdf/2507.14268", "abs": "https://arxiv.org/abs/2507.14268", "authors": ["Andreas Alpers", "Orkun Furat", "Christian Jung", "Matthias Neumann", "Claudia Redenbach", "Aigerim Saken", "Volker Schmidt"], "title": "Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data", "categories": ["cs.CV", "cond-mat.mtrl-sci", "math.OC"], "comment": "31 pages, 16 figures, 8 tables", "summary": "This paper presents a comparative analysis of algorithmic strategies for\nfitting tessellation models to 3D image data of materials such as polycrystals\nand foams. In this steadily advancing field, we review and assess\noptimization-based methods -- including linear and nonlinear programming,\nstochastic optimization via the cross-entropy method, and gradient descent --\nfor generating Voronoi, Laguerre, and generalized balanced power diagrams\n(GBPDs) that approximate voxelbased grain structures. The quality of fit is\nevaluated on real-world datasets using discrepancy measures that quantify\ndifferences in grain volume, surface area, and topology. Our results highlight\ntrade-offs between model complexity, the complexity of the optimization\nroutines involved, and the quality of approximation, providing guidance for\nselecting appropriate methods based on data characteristics and application\nneeds.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u5206\u6790\u4e86\u591a\u79cd\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u5c06\u9576\u5d4c\u6a21\u578b\uff08\u5982Voronoi\u3001Laguerre\u3001GBPDs\uff09\u62df\u5408\u52303D\u6750\u6599\u56fe\u50cf\u6570\u636e\uff0c\u8bc4\u4f30\u5176\u62df\u5408\u8d28\u91cf\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u3001\u7b97\u6cd5\u590d\u6742\u6027\u4e0e\u8fd1\u4f3c\u8d28\u91cf\u7684\u6743\u8861\u3002", "motivation": "\u5728\u6750\u65993D\u56fe\u50cf\u6570\u636e\u9576\u5d4c\u6a21\u578b\u62df\u5408\u9886\u57df\uff0c\u9700\u8981\u5bf9\u73b0\u6709\u4f18\u5316\u7b56\u7565\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u548c\u6bd4\u8f83\uff0c\u4ee5\u7406\u89e3\u5982\u4f55\u6709\u6548\u62df\u5408\u9576\u5d4c\u6a21\u578b\u5230\u4f53\u7d20\u5316\u6676\u7c92\u7ed3\u6784\uff0c\u5e76\u4e3a\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u6bd4\u8f83\u5206\u6790\u6cd5\uff0c\u8bc4\u4f30\u4e86\u7ebf\u6027/\u975e\u7ebf\u6027\u89c4\u5212\u3001\u4ea4\u53c9\u71b5\u968f\u673a\u4f18\u5316\u53ca\u68af\u5ea6\u4e0b\u964d\u7b49\u4f18\u5316\u7b56\u7565\uff0c\u7528\u4e8e\u751f\u6210Voronoi\u3001Laguerre\u548c\u5e7f\u4e49\u5e73\u8861\u529f\u7387\u56fe\u7b49\u9576\u5d4c\u6a21\u578b\uff0c\u4ee5\u8fd1\u4f3c\u805a\u6676\u548c\u6ce1\u6cab\u7b49\u6750\u6599\u76843D\u56fe\u50cf\u6570\u636e\u4e2d\u7684\u4f53\u7d20\u5316\u6676\u7c92\u7ed3\u6784\u3002\u62df\u5408\u8d28\u91cf\u901a\u8fc7\u91cf\u5316\u6676\u7c92\u4f53\u79ef\u3001\u8868\u9762\u79ef\u548c\u62d3\u6251\u5dee\u5f02\u7684\u5ea6\u91cf\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u6a21\u578b\u590d\u6742\u6027\u3001\u4f18\u5316\u4f8b\u7a0b\u590d\u6742\u6027\u4e0e\u8fd1\u4f3c\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6839\u636e\u6570\u636e\u7279\u6027\u548c\u5e94\u7528\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u9576\u5d4c\u6a21\u578b\u62df\u5408\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2507.14170", "pdf": "https://arxiv.org/pdf/2507.14170", "abs": "https://arxiv.org/abs/2507.14170", "authors": ["Jaeheun Jung", "Donghun Lee"], "title": "Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025 workshop HiLD 2025 (3rd workshop on High-dimensional\n  Learning Dynamics)", "summary": "Structured pruning aims to reduce the size and computational cost of deep\nneural networks by removing entire filters or channels. The traditional\nregularizers such as L1 or Group Lasso and its variants lead to\nmagnitude-biased pruning decisions, such that the filters with small magnitudes\nare likely to be pruned. Also, they often entail pruning results with almost\nzero margin around pruning decision boundary, such that tiny perturbation in a\nfilter magnitude can flip the pruning decision. In this paper, we identify the\nprecise algebraic condition under which pruning operations preserve model\nperformance, and use the condition to construct a novel regularizer defined in\nan extended parameter space via auxiliary catalyst variables. The proposed\nCatalyst regularization ensures fair pruning chance for each filters with\ntheoretically provable zero bias to their magnitude and robust pruning behavior\nachieved by wide-margin bifurcation of magnitudes between the preserved and the\npruned filters. The theoretical properties naturally lead to real-world\neffectiveness, as shown by empirical validations of Catalyst Pruning algorithm.\nPruning results on various datasets and models are superior to state-of-the-art\nfilter pruning methods, and at the same time confirm the predicted robust and\nfair pruning characteristics of Catalyst pruning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u201c\u50ac\u5316\u5242\u6b63\u5219\u5316\u201d\uff08Catalyst regularization\uff09\u7684\u65b0\u578b\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u53d8\u91cf\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u7684\u5e45\u5ea6\u504f\u501a\u548c\u9c81\u68d2\u6027\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u65e0\u504f\u4e14\u9c81\u68d2\u7684\u526a\u679d\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8eSOTA\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684L1\u6216Group Lasso\u7b49\u7ed3\u6784\u5316\u526a\u679d\u6b63\u5219\u5316\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u7f3a\u9677\uff1a1. \u526a\u679d\u51b3\u7b56\u53d7\u6ee4\u6ce2\u5668\u5e45\u5ea6\u5927\u5c0f\u5f71\u54cd\uff0c\u5bfc\u81f4\u5c0f\u5e45\u5ea6\u6ee4\u6ce2\u5668\u6613\u88ab\u526a\u679d\uff08\u5e45\u5ea6\u504f\u501a\uff09\uff1b2. \u526a\u679d\u51b3\u7b56\u8fb9\u754c\u4e0d\u6e05\u6670\uff0c\u9c81\u68d2\u6027\u5dee\uff0c\u5fae\u5c0f\u6270\u52a8\u5373\u53ef\u6539\u53d8\u526a\u679d\u7ed3\u679c\u3002", "method": "\u7814\u7a76\u8005\u8bc6\u522b\u51fa\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u526a\u679d\u64cd\u4f5c\u7684\u7cbe\u786e\u4ee3\u6570\u6761\u4ef6\u3002\u57fa\u4e8e\u6b64\u6761\u4ef6\uff0c\u4ed6\u4eec\u5728\u6269\u5c55\u53c2\u6570\u7a7a\u95f4\u4e2d\u5f15\u5165\u8f85\u52a9\u50ac\u5316\u5242\u53d8\u91cf\uff08auxiliary catalyst variables\uff09\uff0c\u6784\u5efa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u201c\u50ac\u5316\u5242\u6b63\u5219\u5316\u201d\u65b9\u6cd5\u3002", "result": "\u6240\u63d0\u51fa\u7684Catalyst\u6b63\u5219\u5316\u786e\u4fdd\u4e86\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u5177\u6709\u516c\u5e73\u7684\u526a\u679d\u673a\u4f1a\uff0c\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5bf9\u5e45\u5ea6\u65e0\u504f\u501a\uff0c\u5e76\u901a\u8fc7\u5728\u4fdd\u7559\u548c\u526a\u679d\u6ee4\u6ce2\u5668\u4e4b\u95f4\u5b9e\u73b0\u5bbd\u88d5\u5ea6\u7684\u5e45\u5ea6\u5206\u5c94\uff0c\u5c55\u73b0\u4e86\u9c81\u68d2\u7684\u526a\u679d\u884c\u4e3a\u3002\u7ecf\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cCatalyst\u526a\u679d\u7b97\u6cd5\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u526a\u679d\u7ed3\u679c\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6ee4\u6ce2\u5668\u526a\u679d\u65b9\u6cd5\u3002", "conclusion": "Catalyst\u526a\u679d\u7b97\u6cd5\u4e0d\u4ec5\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff0c\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5b83\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u7ed3\u6784\u5316\u526a\u679d\u4e2d\u7684\u5e45\u5ea6\u504f\u501a\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u8bc1\u5b9e\u4e86\u5176\u7406\u8bba\u9884\u6d4b\u7684\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u7279\u5f81\uff0c\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u526a\u679d\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14231", "pdf": "https://arxiv.org/pdf/2507.14231", "abs": "https://arxiv.org/abs/2507.14231", "authors": ["Khalid Hasan", "Jamil Saquer"], "title": "Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The 37th International Conference on Software Engineering & Knowledge\n  Engineering, SEKE 2025 (camera-ready)", "summary": "Bipolar disorder is a chronic mental illness frequently underdiagnosed due to\nsubtle early symptoms and social stigma. This paper explores the advanced\nnatural language processing (NLP) models for recognizing signs of bipolar\ndisorder based on user-generated social media text. We conduct a comprehensive\nevaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,\nDistilBERT) and Long Short Term Memory (LSTM) models based on contextualized\n(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed\non a large, annotated dataset of Reddit posts after confirming their validity\nthrough sentiment variance and judgmental analysis. Our results demonstrate\nthat RoBERTa achieves the highest performance among transformer models with an\nF1 score of ~98% while LSTM models using BERT embeddings yield nearly identical\nresults. In contrast, LSTMs trained on static embeddings fail to capture\nmeaningful patterns, scoring near-zero F1. These findings underscore the\ncritical role of contextual language modeling in detecting bipolar disorder. In\naddition, we report model training times and highlight that DistilBERT offers\nan optimal balance between efficiency and accuracy. In general, our study\noffers actionable insights for model selection in mental health NLP\napplications and validates the potential of contextualized language models to\nsupport early bipolar disorder screening.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5148\u8fdb\u7684NLP\u6a21\u578b\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u6587\u672c\uff0c\u4ee5\u8bc6\u522b\u53cc\u76f8\u60c5\u611f\u969c\u788d\u8ff9\u8c61\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\uff08\u5982RoBERTa\u548c\u57fa\u4e8eBERT\u5d4c\u5165\u7684LSTM\uff09\u5728\u68c0\u6d4b\u8be5\u75be\u75c5\u65b9\u9762\u8868\u73b0\u5353\u8d8a\uff0c\u800c\u9759\u6001\u5d4c\u5165\u6548\u679c\u4e0d\u4f73\u3002", "motivation": "\u53cc\u76f8\u60c5\u611f\u969c\u788d\u56e0\u65e9\u671f\u75c7\u72b6\u9690\u853d\u548c\u793e\u4f1a\u6c61\u540d\u800c\u5e38\u88ab\u6f0f\u8bca\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5229\u7528\u5148\u8fdb\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u6a21\u578b\uff0c\u57fa\u4e8e\u7528\u6237\u751f\u6210\u7684\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u8bc6\u522b\u53cc\u76f8\u60c5\u611f\u969c\u788d\u7684\u65e9\u671f\u8ff9\u8c61\uff0c\u4ee5\u652f\u6301\u75be\u75c5\u7684\u65e9\u671f\u7b5b\u67e5\u3002", "method": "\u672c\u7814\u7a76\u5bf9\u4e00\u7cfb\u5217NLP\u6a21\u578b\u8fdb\u884c\u4e86\u7efc\u5408\u8bc4\u4f30\uff0c\u5305\u62ecTransformer\u67b6\u6784\u6a21\u578b\uff08BERT, RoBERTa, ALBERT, ELECTRA, DistilBERT\uff09\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u6a21\u578b\u3002LSTM\u6a21\u578b\u5206\u522b\u4f7f\u7528\u4e86\u4e0a\u4e0b\u6587\uff08BERT\uff09\u548c\u9759\u6001\uff08GloVe, Word2Vec\uff09\u8bcd\u5d4c\u5165\u3002\u5b9e\u9a8c\u5728\u4e00\u4e2a\u5927\u578b\u3001\u7ecf\u8fc7\u6807\u6ce8\u7684Reddit\u5e16\u5b50\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u60c5\u611f\u65b9\u5dee\u548c\u5224\u65ad\u5206\u6790\u786e\u8ba4\u4e86\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cRoBERTa\u5728Transformer\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u6570\u7ea6\u4e3a98%\u3002\u4f7f\u7528BERT\u5d4c\u5165\u7684LSTM\u6a21\u578b\u4e5f\u53d6\u5f97\u4e86\u51e0\u4e4e\u76f8\u540c\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u4f7f\u7528\u9759\u6001\u5d4c\u5165\u8bad\u7ec3\u7684LSTM\u6a21\u578b\u672a\u80fd\u6355\u6349\u5230\u6709\u610f\u4e49\u7684\u6a21\u5f0f\uff0cF1\u5206\u6570\u63a5\u8fd1\u96f6\u3002\u6b64\u5916\uff0cDistilBERT\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u7814\u7a76\u4e5f\u5f3a\u8c03\u4e86\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u5728\u68c0\u6d4b\u53cc\u76f8\u60c5\u611f\u969c\u788d\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7cbe\u795e\u5065\u5eb7NLP\u5e94\u7528\u4e2d\u7684\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u5728\u652f\u6301\u53cc\u76f8\u60c5\u611f\u969c\u788d\u65e9\u671f\u7b5b\u67e5\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u5728\u8bc6\u522b\u75be\u75c5\u8ff9\u8c61\u65b9\u9762\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.14186", "pdf": "https://arxiv.org/pdf/2507.14186", "abs": "https://arxiv.org/abs/2507.14186", "authors": ["Xiaojie Li", "Zhijie Cai", "Nan Qi", "Chao Dong", "Guangxu Zhu", "Haixia Ma", "Qihui Wu", "Shi Jin"], "title": "A Disentangled Representation Learning Framework for Low-altitude Network Coverage Prediction", "categories": ["cs.NI", "cs.AI", "cs.LG", "eess.SP"], "comment": "This paper has been submitted to IEEE for possible publication", "summary": "The expansion of the low-altitude economy has underscored the significance of\nLow-Altitude Network Coverage (LANC) prediction for designing aerial corridors.\nWhile accurate LANC forecasting hinges on the antenna beam patterns of Base\nStations (BSs), these patterns are typically proprietary and not readily\naccessible. Operational parameters of BSs, which inherently contain beam\ninformation, offer an opportunity for data-driven low-altitude coverage\nprediction. However, collecting extensive low-altitude road test data is\ncost-prohibitive, often yielding only sparse samples per BS. This scarcity\nresults in two primary challenges: imbalanced feature sampling due to limited\nvariability in high-dimensional operational parameters against the backdrop of\nsubstantial changes in low-dimensional sampling locations, and diminished\ngeneralizability stemming from insufficient data samples. To overcome these\nobstacles, we introduce a dual strategy comprising expert knowledge-based\nfeature compression and disentangled representation learning. The former\nreduces feature space complexity by leveraging communications expertise, while\nthe latter enhances model generalizability through the integration of\npropagation models and distinct subnetworks that capture and aggregate the\nsemantic representations of latent features. Experimental evaluation confirms\nthe efficacy of our framework, yielding a 7% reduction in error compared to the\nbest baseline algorithm. Real-network validations further attest to its\nreliability, achieving practical prediction accuracy with MAE errors at the 5dB\nlevel.", "AI": {"tldr": "\u9488\u5bf9\u4f4e\u7a7a\u7f51\u7edc\u8986\u76d6\u9884\u6d4b\u4e2d\u57fa\u7ad9\u5929\u7ebf\u6ce2\u675f\u56fe\u4e0d\u53ef\u83b7\u53d6\u548c\u6570\u636e\u7a00\u758f\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\u7279\u5f81\u538b\u7f29\u548c\u89e3\u8026\u8868\u5f81\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f4e\u7a7a\u7ecf\u6d4e\u53d1\u5c55\u5bf9\u4f4e\u7a7a\u7f51\u7edc\u8986\u76d6\uff08LANC\uff09\u9884\u6d4b\u63d0\u51fa\u9700\u6c42\uff0c\u4f46\u7cbe\u786e\u9884\u6d4b\u6240\u9700\u7684\u57fa\u7ad9\u5929\u7ebf\u6ce2\u675f\u56fe\u4e3a\u4e13\u6709\u4fe1\u606f\u4e14\u96be\u4ee5\u83b7\u53d6\u3002\u5c3d\u7ba1\u57fa\u7ad9\u8fd0\u884c\u53c2\u6570\u5305\u542b\u6ce2\u675f\u4fe1\u606f\uff0c\u4f46\u4f4e\u7a7a\u8def\u6d4b\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u6602\uff0c\u5bfc\u81f4\u6570\u636e\u7a00\u758f\uff0c\u8fdb\u800c\u5f15\u53d1\u7279\u5f81\u91c7\u6837\u4e0d\u5e73\u8861\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u53cc\u91cd\u7b56\u7565\uff1a1. \u57fa\u4e8e\u4e13\u5bb6\u77e5\u8bc6\u7684\u7279\u5f81\u538b\u7f29\uff0c\u5229\u7528\u901a\u4fe1\u4e13\u4e1a\u77e5\u8bc6\u964d\u4f4e\u7279\u5f81\u7a7a\u95f4\u590d\u6742\u5ea6\u30022. \u89e3\u8026\u8868\u5f81\u5b66\u4e60\uff0c\u901a\u8fc7\u6574\u5408\u4f20\u64ad\u6a21\u578b\u548c\u72ec\u7acb\u7684\u5b50\u7f51\u7edc\uff0c\u6355\u83b7\u5e76\u805a\u5408\u6f5c\u5728\u7279\u5f81\u7684\u8bed\u4e49\u8868\u5f81\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u672c\u6846\u67b6\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u7b97\u6cd5\u8bef\u5dee\u964d\u4f4e7%\u3002\u5b9e\u9645\u7f51\u7edc\u9a8c\u8bc1\u8868\u660e\uff0c\u5176\u9884\u6d4b\u7cbe\u5ea6\u53ef\u8fbe5dB\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\uff0c\u5177\u6709\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u7a7a\u7f51\u7edc\u8986\u76d6\u9884\u6d4b\u4e2d\u6570\u636e\u7a00\u758f\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u4f4e\u7a7a\u7ecf\u6d4e\u53d1\u5c55\u63d0\u4f9b\u4e86\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2507.14267", "pdf": "https://arxiv.org/pdf/2507.14267", "abs": "https://arxiv.org/abs/2507.14267", "authors": ["Ziqi Wang", "Hongshuo Huang", "Hancheng Zhao", "Changwen Xu", "Shang Zhu", "Jan Janssen", "Venkatasubramanian Viswanathan"], "title": "DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "comment": "34 pages, 28 pages of Supporting Information", "summary": "Materials discovery relies on high-throughput, high-fidelity simulation\ntechniques such as Density Functional Theory (DFT), which require years of\ntraining, extensive parameter fine-tuning and systematic error handling. To\naddress these challenges, we introduce the DFT-based Research Engine for\nAgentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for\nDFT simulation that combines a central Large Language Model (LLM) planner agent\nwith domain-specific LLM agents for atomistic structure generation, systematic\nDFT convergence testing, High-Performance Computing (HPC) scheduling, and error\nhandling. In addition, a shared canvas helps the LLM agents to structure their\ndiscussions, preserve context and prevent hallucination. We validate DREAMS\ncapabilities on the Sol27LC lattice-constant benchmark, achieving average\nerrors below 1\\% compared to the results of human DFT experts. Furthermore, we\napply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating\nits long-term and complex problem-solving capabilities. The framework again\nreproduces expert-level literature adsorption-energy differences. Finally,\nDREAMS is employed to quantify functional-driven uncertainties with Bayesian\nensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at\nthe Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS\napproaches L3-level automation - autonomous exploration of a defined design\nspace - and significantly reduces the reliance on human expertise and\nintervention, offering a scalable path toward democratized, high-throughput,\nhigh-fidelity computational materials discovery.", "AI": {"tldr": "DREAMS\u662f\u4e00\u4e2a\u57fa\u4e8eDFT\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528LLM\u5b9e\u73b0\u81ea\u52a8\u5316\u6750\u6599\u6a21\u62df\uff0c\u8fbe\u5230\u4e13\u5bb6\u7ea7\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u5bf9\u4eba\u5de5\u5e72\u9884\u7684\u4f9d\u8d56\uff0c\u52a0\u901f\u6750\u6599\u53d1\u73b0\u3002", "motivation": "\u6750\u6599\u53d1\u73b0\u4f9d\u8d56\u9ad8\u901a\u91cf\u3001\u9ad8\u7cbe\u5ea6\u7684DFT\u6a21\u62df\uff0c\u4f46\u5176\u8981\u6c42\u591a\u5e74\u7684\u8bad\u7ec3\u3001\u7cbe\u7ec6\u7684\u53c2\u6570\u8c03\u6574\u548c\u7cfb\u7edf\u9519\u8bef\u5904\u7406\uff0c\u9650\u5236\u4e86\u5e94\u7528\u6548\u7387\u548c\u666e\u53ca\u3002", "method": "\u672c\u6587\u5f15\u5165DREAMS\uff08DFT-based Research Engine for Agentic Materials Screening\uff09\uff0c\u4e00\u4e2a\u5206\u5c42\u3001\u591a\u667a\u80fd\u4f53DFT\u6a21\u62df\u6846\u67b6\u3002\u5b83\u7ed3\u5408\u4e00\u4e2a\u4e2d\u592e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89c4\u5212\u4ee3\u7406\u4e0e\u591a\u4e2a\u9886\u57df\u7279\u5b9aLLM\u4ee3\u7406\uff08\u7528\u4e8e\u539f\u5b50\u7ed3\u6784\u751f\u6210\u3001DFT\u6536\u655b\u6d4b\u8bd5\u3001\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u8c03\u5ea6\u548c\u9519\u8bef\u5904\u7406\uff09\u3002\u6b64\u5916\uff0c\u4e00\u4e2a\u5171\u4eab\u753b\u5e03\u5e2e\u52a9LLM\u667a\u80fd\u4f53\u7ed3\u6784\u5316\u8ba8\u8bba\u3001\u4fdd\u7559\u4e0a\u4e0b\u6587\u5e76\u9632\u6b62\u5e7b\u89c9\u3002", "result": "DREAMS\u5728Sol27LC\u6676\u683c\u5e38\u6570\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4eba\u7c7bDFT\u4e13\u5bb6\u7ed3\u679c\u76f8\u6bd4\u4f4e\u4e8e1%\u7684\u5e73\u5747\u8bef\u5dee\u3002\u5b83\u6210\u529f\u5e94\u7528\u4e8eCO/Pt(111)\u5438\u9644\u96be\u9898\uff0c\u518d\u73b0\u4e86\u4e13\u5bb6\u7ea7\u7684\u5438\u9644\u80fd\u5dee\u5f02\uff0c\u5c55\u793a\u4e86\u957f\u671f\u590d\u6742\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002\u6b64\u5916\uff0cDREAMS\u901a\u8fc7\u8d1d\u53f6\u65af\u96c6\u6210\u91c7\u6837\u91cf\u5316\u4e86\u529f\u80fd\u9a71\u52a8\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u786e\u8ba4\u4e86GGA DFT\u6c34\u5e73\u4e0b\u7684\u9762\u5fc3\u7acb\u65b9\uff08FCC\uff09\u4f4d\u70b9\u504f\u597d\u3002", "conclusion": "DREAMS\u8fbe\u5230\u4e86L3\u7ea7\u81ea\u52a8\u5316\uff08\u5bf9\u5b9a\u4e49\u8bbe\u8ba1\u7a7a\u95f4\u7684\u81ea\u4e3b\u63a2\u7d22\uff09\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u548c\u5e72\u9884\u7684\u4f9d\u8d56\uff0c\u4e3a\u666e\u53ca\u9ad8\u901a\u91cf\u3001\u9ad8\u7cbe\u5ea6\u7684\u8ba1\u7b97\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2507.14303", "pdf": "https://arxiv.org/pdf/2507.14303", "abs": "https://arxiv.org/abs/2507.14303", "authors": ["Ehsan Rassekh"], "title": "Semantic Segmentation based Scene Understanding in Autonomous Vehicles", "categories": ["cs.CV", "I.4.8"], "comment": "74 pages, 35 figures, Master's Thesis, Institute for Advanced Studies\n  in Basic Sciences (IASBS), Zanjan, Iran, 2023", "summary": "In recent years, the concept of artificial intelligence (AI) has become a\nprominent keyword because it is promising in solving complex tasks. The need\nfor human expertise in specific areas may no longer be needed because machines\nhave achieved successful results using artificial intelligence and can make the\nright decisions in critical situations. This process is possible with the help\nof deep learning (DL), one of the most popular artificial intelligence\ntechnologies. One of the areas in which the use of DL is used is in the\ndevelopment of self-driving cars, which is very effective and important. In\nthis work, we propose several efficient models to investigate scene\nunderstanding through semantic segmentation. We use the BDD100k dataset to\ninvestigate these models. Another contribution of this work is the usage of\nseveral Backbones as encoders for models. The obtained results show that\nchoosing the appropriate backbone has a great effect on the performance of the\nmodel for semantic segmentation. Better performance in semantic segmentation\nallows us to understand better the scene and the environment around the agent.\nIn the end, we analyze and evaluate the proposed models in terms of accuracy,\nmean IoU, and loss function, and the results show that these metrics are\nimproved.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u591a\u4e2a\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u5b9e\u73b0\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u573a\u666f\u7406\u89e3\uff0c\u5e76\u53d1\u73b0\u9009\u62e9\u5408\u9002\u7684\u9aa8\u5e72\u7f51\u7edc\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u6709\u6548\u63d0\u5347\u4e86\u76f8\u5173\u5ea6\u91cf\u6307\u6807\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u5728\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u548c\u8f85\u52a9\u5173\u952e\u51b3\u7b56\u65b9\u9762\u524d\u666f\u5e7f\u9614\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u9886\u57df\uff0cDL\u5bf9\u573a\u666f\u7406\u89e3\uff08\u901a\u8fc7\u8bed\u4e49\u5206\u5272\uff09\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u6570\u4e2a\u9ad8\u6548\u6a21\u578b\uff0c\u7528\u4e8e\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u8c03\u67e5\u573a\u666f\u7406\u89e3\u3002\u4f7f\u7528BDD100k\u6570\u636e\u96c6\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002\u4e3b\u8981\u8d21\u732e\u4e4b\u4e00\u662f\u4f7f\u7528\u591a\u79cd\u9aa8\u5e72\u7f51\u7edc\uff08Backbones\uff09\u4f5c\u4e3a\u6a21\u578b\u7684\u7f16\u7801\u5668\uff0c\u5e76\u57fa\u4e8e\u51c6\u786e\u7387\u3001\u5e73\u5747IoU\u548c\u635f\u5931\u51fd\u6570\u5bf9\u6a21\u578b\u8fdb\u884c\u5206\u6790\u548c\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9009\u62e9\u5408\u9002\u7684\u9aa8\u5e72\u7f51\u7edc\u5bf9\u8bed\u4e49\u5206\u5272\u6a21\u578b\u7684\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002\u66f4\u597d\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u573a\u666f\u548c\u8f66\u8f86\u5468\u56f4\u73af\u5883\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u51c6\u786e\u7387\u3001\u5e73\u5747IoU\u548c\u635f\u5931\u51fd\u6570\u7b49\u6307\u6807\u4e0a\u5747\u6709\u6240\u6539\u5584\u3002", "conclusion": "\u672c\u5de5\u4f5c\u901a\u8fc7\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u5bf9\u9aa8\u5e72\u7f51\u7edc\u9009\u62e9\u7684\u63a2\u7d22\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u5bf9\u573a\u666f\u7684\u66f4\u597d\u7406\u89e3\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6240\u63d0\u6a21\u578b\u5728\u5173\u952e\u6307\u6807\u4e0a\u7684\u6539\u8fdb\u3002"}}
{"id": "2507.14171", "pdf": "https://arxiv.org/pdf/2507.14171", "abs": "https://arxiv.org/abs/2507.14171", "authors": ["Jaeheun Jung", "Jaehyuk Lee", "Yeajin Lee", "Donghun Lee"], "title": "IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the growth of demand on neural network compression methods, the\nstructured pruning methods including importance-based approach are actively\nstudied. The magnitude importance and many correlated modern importance\ncriteria often limit the capacity of pruning decision, since the filters with\nlarger magnitudes are not likely to be pruned if the smaller one didn't, even\nif it is redundant. In this paper, we propose a novel pruning strategy to\nchallenge this dominating effect of magnitude and provide fair chance to each\nfilter to be pruned, by placing it on projective space. After that, we observe\nthe gradient descent movement whether the filters move toward the origin or\nnot, to measure how the filter is likely to be pruned. This measurement is used\nto construct PROscore, a novel importance score for IPPRO, a novel\nimportance-based structured pruning with magnitude-indifference. Our evaluation\nresults shows that the proposed importance criteria using the projective space\nachieves near-lossless pruning by reducing the performance drop in pruning,\nwith promising performance after the finetuning. Our work debunks the\n``size-matters'' myth in pruning and expands the frontier of importance-based\npruning both theoretically and empirically.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u3001\u4e0e\u5e45\u5ea6\u65e0\u5173\u7684\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5IPPRO\uff08\u4f7f\u7528PROscore\uff09\uff0c\u901a\u8fc7\u5c06\u6ee4\u6ce2\u5668\u6620\u5c04\u5230\u6295\u5f71\u7a7a\u95f4\u5e76\u89c2\u5bdf\u5176\u68af\u5ea6\u4e0b\u964d\u8f68\u8ff9\u6765\u8bc4\u4f30\u91cd\u8981\u6027\uff0c\u65e8\u5728\u514b\u670d\u4f20\u7edf\u5e45\u5ea6\u526a\u679d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u8fd1\u4e4e\u65e0\u635f\u7684\u526a\u679d\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\u4e2d\uff0c\u57fa\u4e8e\u91cd\u8981\u6027\uff08\u7279\u522b\u662f\u57fa\u4e8e\u5e45\u5ea6\uff09\u7684\u51c6\u5219\u9650\u5236\u4e86\u526a\u679d\u51b3\u7b56\u80fd\u529b\uff0c\u56e0\u4e3a\u5373\u4f7f\u5197\u4f59\uff0c\u5e45\u5ea6\u8f83\u5927\u7684\u6ee4\u6ce2\u5668\u4e5f\u4e0d\u592a\u53ef\u80fd\u88ab\u526a\u679d\uff0c\u5bfc\u81f4\u526a\u679d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u526a\u679d\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u6ee4\u6ce2\u5668\u7f6e\u4e8e\u6295\u5f71\u7a7a\u95f4\uff0c\u4ee5\u6311\u6218\u5e45\u5ea6\u7684\u51b3\u5b9a\u6027\u5f71\u54cd\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u63d0\u4f9b\u516c\u5e73\u7684\u526a\u679d\u673a\u4f1a\u3002\u901a\u8fc7\u89c2\u5bdf\u6ee4\u6ce2\u5668\u5728\u68af\u5ea6\u4e0b\u964d\u8fc7\u7a0b\u4e2d\u662f\u5426\u8d8b\u5411\u539f\u70b9\u6765\u8861\u91cf\u5176\u88ab\u526a\u679d\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u4ee5\u6b64\u6784\u5efaPROscore\uff0c\u5e94\u7528\u4e8eIPPRO\uff08\u4e00\u79cd\u65b0\u578b\u7684\u3001\u4e0e\u5e45\u5ea6\u65e0\u5173\u7684\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff09\u3002", "result": "\u63d0\u51fa\u7684\u57fa\u4e8e\u6295\u5f71\u7a7a\u95f4\u7684\u91cd\u8981\u6027\u51c6\u5219\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u65e0\u635f\u7684\u526a\u679d\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u526a\u679d\u8fc7\u7a0b\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u5728\u5fae\u8c03\u540e\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u7ffb\u4e86\u526a\u679d\u4e2d\u201c\u5927\u5c0f\u51b3\u5b9a\u4e00\u5207\u201d\u7684\u4f20\u7edf\u89c2\u5ff5\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u62d3\u5c55\u4e86\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u526a\u679d\u9886\u57df\u3002"}}
{"id": "2507.14238", "pdf": "https://arxiv.org/pdf/2507.14238", "abs": "https://arxiv.org/abs/2507.14238", "authors": ["Matthew Kearney", "Reuben Binns", "Yarin Gal"], "title": "Language Models Change Facts Based on the Way You Talk", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are increasingly being used in user-facing\napplications, from providing medical consultations to job interview advice.\nRecent research suggests that these models are becoming increasingly proficient\nat inferring identity information about the author of a piece of text from\nlinguistic patterns as subtle as the choice of a few words. However, little is\nknown about how LLMs use this information in their decision-making in\nreal-world applications. We perform the first comprehensive analysis of how\nidentity markers present in a user's writing bias LLM responses across five\ndifferent high-stakes LLM applications in the domains of medicine, law,\npolitics, government benefits, and job salaries. We find that LLMs are\nextremely sensitive to markers of identity in user queries and that race,\ngender, and age consistently influence LLM responses in these applications. For\ninstance, when providing medical advice, we find that models apply different\nstandards of care to individuals of different ethnicities for the same\nsymptoms; we find that LLMs are more likely to alter answers to align with a\nconservative (liberal) political worldview when asked factual questions by\nolder (younger) individuals; and that LLMs recommend lower salaries for\nnon-White job applicants and higher salaries for women compared to men. Taken\ntogether, these biases mean that the use of off-the-shelf LLMs for these\napplications may cause harmful differences in medical care, foster wage gaps,\nand create different political factual realities for people of different\nidentities. Beyond providing an analysis, we also provide new tools for\nevaluating how subtle encoding of identity in users' language choices impacts\nmodel decisions. Given the serious implications of these findings, we recommend\nthat similar thorough assessments of LLM use in user-facing applications are\nconducted before future deployment.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5168\u9762\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u6839\u636e\u7528\u6237\u6587\u672c\u4e2d\u7684\u8eab\u4efd\u6807\u8bb0\uff08\u5982\u79cd\u65cf\u3001\u6027\u522b\u3001\u5e74\u9f84\uff09\u4ea7\u751f\u504f\u89c1\u54cd\u5e94\uff0c\u53d1\u73b0\u5176\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u653f\u6cbb\u548c\u5c31\u4e1a\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u5b58\u5728\u663e\u8457\u504f\u5dee\uff0c\u53ef\u80fd\u5bfc\u81f4\u6709\u5bb3\u540e\u679c\u3002", "motivation": "\u968f\u7740LLMs\u5728\u9762\u5411\u7528\u6237\u7684\u5e94\u7528\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u5b83\u4eec\u80fd\u591f\u4ece\u7ec6\u5fae\u7684\u8bed\u8a00\u6a21\u5f0f\u4e2d\u63a8\u65ad\u7528\u6237\u8eab\u4efd\u3002\u7136\u800c\uff0cLLMs\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u8eab\u4efd\u4fe1\u606f\u8fdb\u884c\u51b3\u7b56\u5c1a\u4e0d\u6e05\u695a\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u63a2\u7a76\u7528\u6237\u5199\u4f5c\u4e2d\u7684\u8eab\u4efd\u6807\u8bb0\u5982\u4f55\u5f71\u54cdLLM\u7684\u54cd\u5e94\u3002", "method": "\u7814\u7a76\u5bf9LLMs\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u653f\u6cbb\u3001\u653f\u5e9c\u798f\u5229\u548c\u5de5\u4f5c\u85aa\u8d44\u4e94\u4e2a\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u8fdb\u884c\u4e86\u9996\u6b21\u7efc\u5408\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u7528\u6237\u5199\u4f5c\u4e2d\u5b58\u5728\u7684\u8eab\u4efd\u6807\u8bb0\u5982\u4f55\u504f\u5411LLM\u7684\u54cd\u5e94\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u5bf9\u7528\u6237\u67e5\u8be2\u4e2d\u7684\u8eab\u4efd\u6807\u8bb0\uff08\u79cd\u65cf\u3001\u6027\u522b\u3001\u5e74\u9f84\uff09\u6781\u5176\u654f\u611f\uff0c\u4e14\u8fd9\u4e9b\u6807\u8bb0\u6301\u7eed\u5f71\u54cdLLM\u7684\u54cd\u5e94\u3002\u4f8b\u5982\uff0c\u5728\u63d0\u4f9b\u533b\u7597\u5efa\u8bae\u65f6\uff0c\u6a21\u578b\u5bf9\u4e0d\u540c\u79cd\u65cf\u7684\u4eba\u91c7\u7528\u4e0d\u540c\u6807\u51c6\uff1b\u5bf9\u5e74\u9f84\u8f83\u957f\uff08\u5e74\u8f7b\uff09\u7684\u7528\u6237\uff0cLLMs\u66f4\u503e\u5411\u4e8e\u5c06\u7b54\u6848\u8c03\u6574\u4e3a\u7b26\u5408\u4fdd\u5b88\uff08\u81ea\u7531\uff09\u653f\u6cbb\u89c2\u70b9\uff1b\u5bf9\u975e\u767d\u4eba\u6c42\u804c\u8005\u63a8\u8350\u8f83\u4f4e\u85aa\u8d44\uff0c\u5bf9\u5973\u6027\u63a8\u8350\u8f83\u9ad8\u85aa\u8d44\u3002\u8fd9\u4e9b\u504f\u89c1\u53ef\u80fd\u5bfc\u81f4\u533b\u7597\u5dee\u5f02\u3001\u5de5\u8d44\u5dee\u8ddd\u548c\u4e0d\u540c\u7684\u653f\u6cbb\u4e8b\u5b9e\u8ba4\u77e5\u3002", "conclusion": "LLMs\u7684\u8fd9\u4e9b\u504f\u89c1\u610f\u5473\u7740\u76f4\u63a5\u4f7f\u7528\u73b0\u6709\u6a21\u578b\u53ef\u80fd\u5bfc\u81f4\u533b\u7597\u3001\u5c31\u4e1a\u548c\u653f\u6cbb\u9886\u57df\u4e2d\u7684\u6709\u5bb3\u5dee\u5f02\u3002\u9274\u4e8e\u8fd9\u4e9b\u53d1\u73b0\u7684\u4e25\u91cd\u6027\uff0c\u5efa\u8bae\u5728\u672a\u6765\u90e8\u7f72\u9762\u5411\u7528\u6237\u7684LLM\u5e94\u7528\u524d\uff0c\u8fdb\u884c\u7c7b\u4f3c\u7684\u5f7b\u5e95\u8bc4\u4f30\u3002"}}
{"id": "2507.14188", "pdf": "https://arxiv.org/pdf/2507.14188", "abs": "https://arxiv.org/abs/2507.14188", "authors": ["Sebastian Barros Elgueta"], "title": "From Cell Towers to Satellites: A 2040 Blueprint for Urban-Grade Direct-to-Device Mobile Networks", "categories": ["cs.NI", "cs.AI"], "comment": "50 pages", "summary": "In 2023, satellite and mobile networks crossed a historic threshold: standard\nsmartphones, using unmodified 3GPP protocols, connected directly to low Earth\norbit (LEO) satellites. This first wave of direct-to-device (D2D)\ndemonstrations validated the physical feasibility of satellite-based mobile\naccess. However, these systems remain fallback-grade--rural-only,\nbandwidth-limited, and fully dependent on Earth-based mobile cores for\nidentity, session, and policy control. This paper asks a more ambitious\nquestion: Can a complete mobile network, including radio access, core\nfunctions, traffic routing, and content delivery, operate entirely from orbit?\nAnd can it deliver sustained, urban-grade service in the world's densest\ncities? We present the first end-to-end system architecture for a fully orbital\ntelco, integrating electronically steered phased arrays with 1000-beam\ncapacity, space-based deployment of 5G core functions (UPF, AMF), and\ninter-satellite laser mesh backhaul. We analyze spectral efficiency, beam\ncapacity, and link budgets under dense urban conditions, accounting for path\nloss, Doppler, and multipath. Simulations show that rooftop and line-of-sight\nusers can sustain 64-QAM throughput, while street-level access is feasible with\nrelay or assisted beam modes. The paper outlines the remaining constraints,\npower, thermal dissipation, compute radiation hardening, and regulatory models,\nand demonstrates that these are engineering bottlenecks, not physical limits.\nFinally, we propose a staged 15-year roadmap from today's fallback D2D systems\nto autonomous orbital overlays delivering 50-100 Mbps to handhelds in\nmegacities, with zero reliance on terrestrial infrastructure.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u5168\u8f68\u9053\u7535\u4fe1\u7f51\u7edc\u67b6\u6784\uff0c\u65e8\u5728\u8131\u79bb\u5730\u9762\u57fa\u7840\u8bbe\u65bd\uff0c\u4e3a\u5168\u7403\u7279\u5927\u57ce\u5e02\u63d0\u4f9b\u6301\u7eed\u3001\u9ad8\u5e26\u5bbd\u7684\u79fb\u52a8\u670d\u52a1\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\uff0c\u63cf\u7ed8\u4e86\u672a\u6765\u53d1\u5c55\u8def\u7ebf\u3002", "motivation": "\u73b0\u6709\u536b\u661f\u76f4\u8fde\u624b\u673a\uff08D2D\uff09\u6280\u672f\u867d\u7136\u5b9e\u73b0\u4e86\u624b\u673a\u4e0e\u4f4e\u8f68\u536b\u661f\u7684\u76f4\u63a5\u8fde\u63a5\uff0c\u4f46\u4ecd\u4ec5\u9650\u4e8e\u4e61\u6751\u5730\u533a\u3001\u5e26\u5bbd\u53d7\u9650\u4e14\u5b8c\u5168\u4f9d\u8d56\u5730\u9762\u6838\u5fc3\u7f51\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u80fd\u5426\u6784\u5efa\u4e00\u4e2a\u5b8c\u5168\u5728\u8f68\u8fd0\u884c\u7684\u79fb\u52a8\u7f51\u7edc\uff0c\u5305\u62ec\u65e0\u7ebf\u63a5\u5165\u3001\u6838\u5fc3\u529f\u80fd\u3001\u6d41\u91cf\u8def\u7531\u548c\u5185\u5bb9\u5206\u53d1\uff0c\u5e76\u5728\u5168\u7403\u6700\u5bc6\u96c6\u7684\u57ce\u5e02\u4e2d\u63d0\u4f9b\u6301\u7eed\u7684\u57ce\u5e02\u7ea7\u670d\u52a1\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7aef\u5230\u7aef\u7684\u5168\u8f68\u9053\u7535\u4fe1\u7cfb\u7edf\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u6574\u5408\u4e86\u5343\u675f\u5bb9\u91cf\u7684\u7535\u63a7\u76f8\u63a7\u9635\u3001\u592a\u7a7a\u90e8\u7f72\u76845G\u6838\u5fc3\u7f51\u529f\u80fd\uff08UPF, AMF\uff09\u4ee5\u53ca\u661f\u95f4\u6fc0\u5149\u7f51\u72b6\u56de\u4f20\u3002\u7814\u7a76\u901a\u8fc7\u5206\u6790\u5bc6\u96c6\u57ce\u5e02\u6761\u4ef6\u4e0b\u7684\u9891\u8c31\u6548\u7387\u3001\u6ce2\u675f\u5bb9\u91cf\u548c\u94fe\u8def\u9884\u7b97\uff0c\u5e76\u8003\u8651\u8def\u5f84\u635f\u8017\u3001\u591a\u666e\u52d2\u6548\u5e94\u548c\u591a\u5f84\u6548\u5e94\uff0c\u8fdb\u884c\u7cfb\u7edf\u6027\u80fd\u8bc4\u4f30\u3002\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u57ce\u5e02\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5c4b\u9876\u548c\u89c6\u7ebf\u53ef\u89c1\u7528\u6237\u53ef\u7ef4\u630164-QAM\u7684\u541e\u5410\u91cf\uff0c\u800c\u8857\u9053\u7ea7\u522b\u63a5\u5165\u901a\u8fc7\u4e2d\u7ee7\u6216\u8f85\u52a9\u6ce2\u675f\u6a21\u5f0f\u4e5f\u53ef\u5b9e\u73b0\u3002\u7814\u7a76\u6307\u51fa\uff0c\u529f\u7387\u3001\u6563\u70ed\u3001\u8ba1\u7b97\u8f90\u5c04\u52a0\u56fa\u548c\u76d1\u7ba1\u6a21\u578b\u7b49\u5269\u4f59\u9650\u5236\u662f\u5de5\u7a0b\u74f6\u9888\u800c\u975e\u7269\u7406\u6781\u9650\u3002\u6700\u7ec8\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a15\u5e74\u7684\u5206\u9636\u6bb5\u53d1\u5c55\u8def\u7ebf\u56fe\u3002", "conclusion": "\u7ed3\u8bba\u8ba4\u4e3a\uff0c\u6784\u5efa\u4e00\u4e2a\u5b8c\u5168\u81ea\u4e3b\u7684\u8f68\u9053\u53e0\u52a0\u7f51\u7edc\u662f\u53ef\u884c\u7684\uff0c\u8be5\u7f51\u7edc\u80fd\u5728\u7279\u5927\u57ce\u5e02\u4e2d\u4e3a\u624b\u6301\u8bbe\u5907\u63d0\u4f9b50-100 Mbps\u7684\u901f\u5ea6\uff0c\u4e14\u5b8c\u5168\u4e0d\u4f9d\u8d56\u5730\u9762\u57fa\u7840\u8bbe\u65bd\u3002\u8fd9\u9884\u793a\u7740\u672a\u6765\u79fb\u52a8\u901a\u4fe1\u5c06\u5b9e\u73b0\u4ece\u73b0\u6709\u56de\u9000\u7ea7D2D\u7cfb\u7edf\u5230\u5148\u8fdb\u8f68\u9053\u7535\u4fe1\u7f51\u7edc\u7684\u91cd\u5927\u98de\u8dc3\u3002"}}
{"id": "2507.14293", "pdf": "https://arxiv.org/pdf/2507.14293", "abs": "https://arxiv.org/abs/2507.14293", "authors": ["Boyuan Zheng", "Zeyi Liao", "Scott Salisbury", "Zeyuan Liu", "Michael Lin", "Qinyuan Zheng", "Zifan Wang", "Xiang Deng", "Dawn Song", "Huan Sun", "Yu Su"], "title": "WebGuard: Building a Generalizable Guardrail for Web Agents", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "We publicly release WebGuard, along with its annotation tools and\n  fine-tuned models, to facilitate open-source research on monitoring and\n  safeguarding web agents. All resources are available at\n  https://github.com/OSU-NLP-Group/WebGuard", "summary": "The rapid development of autonomous web agents powered by Large Language\nModels (LLMs), while greatly elevating efficiency, exposes the frontier risk of\ntaking unintended or harmful actions. This situation underscores an urgent need\nfor effective safety measures, akin to access controls for human users. To\naddress this critical challenge, we introduce WebGuard, the first comprehensive\ndataset designed to support the assessment of web agent action risks and\nfacilitate the development of guardrails for real-world online environments. In\ndoing so, WebGuard specifically focuses on predicting the outcome of\nstate-changing actions and contains 4,939 human-annotated actions from 193\nwebsites across 22 diverse domains, including often-overlooked long-tail\nwebsites. These actions are categorized using a novel three-tier risk schema:\nSAFE, LOW, and HIGH. The dataset includes designated training and test splits\nto support evaluation under diverse generalization settings. Our initial\nevaluations reveal a concerning deficiency: even frontier LLMs achieve less\nthan 60% accuracy in predicting action outcomes and less than 60% recall in\nlagging HIGH-risk actions, highlighting the risks of deploying\ncurrent-generation agents without dedicated safeguards. We therefore\ninvestigate fine-tuning specialized guardrail models using WebGuard. We conduct\ncomprehensive evaluations across multiple generalization settings and find that\na fine-tuned Qwen2.5VL-7B model yields a substantial improvement in\nperformance, boosting accuracy from 37% to 80% and HIGH-risk action recall from\n20% to 76%. Despite these improvements, the performance still falls short of\nthe reliability required for high-stakes deployment, where guardrails must\napproach near-perfect accuracy and recall.", "AI": {"tldr": "WebGuard\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u7f51\u7edc\u667a\u80fd\u4f53\u52a8\u4f5c\u98ce\u9669\u7684\u6570\u636e\u96c6\uff0c\u63ed\u793a\u5f53\u524dLLM\u9884\u6d4b\u98ce\u9669\u80fd\u529b\u4e0d\u8db3\uff0c\u4f46\u57fa\u4e8eWebGuard\u5fae\u8c03\u7684\u6a21\u578b\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5c3d\u7ba1\u4ecd\u672a\u8fbe\u5230\u9ad8\u98ce\u9669\u90e8\u7f72\u8981\u6c42\u3002", "motivation": "\u968f\u7740LLM\u9a71\u52a8\u7684\u81ea\u4e3b\u7f51\u7edc\u667a\u80fd\u4f53\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u610f\u5916\u6216\u6709\u5bb3\u884c\u4e3a\u7684\u98ce\u9669\u65e5\u76ca\u7a81\u51fa\uff0c\u4e9f\u9700\u6709\u6548\u7684\u5b89\u5168\u63aa\u65bd\uff08\u5982\u8bbf\u95ee\u63a7\u5236\uff09\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u5173\u952e\u6311\u6218\u3002", "method": "\u5f15\u5165WebGuard\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea193\u4e2a\u7f51\u7ad9\u300122\u4e2a\u9886\u57df\u76844,939\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u72b6\u6001\u6539\u53d8\u52a8\u4f5c\u3002\u91c7\u7528SAFE\u3001LOW\u3001HIGH\u4e09\u5c42\u98ce\u9669\u7b49\u7ea7\u5bf9\u52a8\u4f5c\u8fdb\u884c\u5206\u7c7b\u3002\u6570\u636e\u96c6\u5305\u542b\u8bad\u7ec3\u548c\u6d4b\u8bd5\u96c6\uff0c\u5e76\u4f7f\u7528WebGuard\u5fae\u8c03\u4e13\u95e8\u7684\u62a4\u680f\u6a21\u578b\u3002", "result": "\u521d\u6b65\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u524d\u6cbfLLM\u5728\u9884\u6d4b\u52a8\u4f5c\u7ed3\u679c\u7684\u51c6\u786e\u7387\u4f4e\u4e8e60%\uff0c\u9ad8\u98ce\u9669\u52a8\u4f5c\u7684\u53ec\u56de\u7387\u4e5f\u4f4e\u4e8e60%\u3002\u901a\u8fc7\u4f7f\u7528WebGuard\u5fae\u8c03Qwen2.5VL-7B\u6a21\u578b\uff0c\u51c6\u786e\u7387\u4ece37%\u63d0\u5347\u81f380%\uff0c\u9ad8\u98ce\u9669\u52a8\u4f5c\u53ec\u56de\u7387\u4ece20%\u63d0\u5347\u81f376%\u3002", "conclusion": "WebGuard\u663e\u8457\u63d0\u5347\u4e86\u62a4\u680f\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f46\u76ee\u524d\u7684\u6027\u80fd\u4ecd\u672a\u8fbe\u5230\u9ad8\u98ce\u9669\u90e8\u7f72\u6240\u9700\u7684\u63a5\u8fd1\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\uff0c\u610f\u5473\u7740\u672a\u6765\u4ecd\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u53ef\u9760\u6027\u3002"}}
{"id": "2507.14312", "pdf": "https://arxiv.org/pdf/2507.14312", "abs": "https://arxiv.org/abs/2507.14312", "authors": ["Marc Lafon", "Gustavo Adolfo Vargas Hakim", "Cl\u00e9ment Rambour", "Christian Desrosier", "Nicolas Thome"], "title": "CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities\nbut often fail to generalize under distribution shifts. Test-time adaptation\n(TTA) allows models to update at inference time without labeled data, typically\nvia entropy minimization. However, this objective is fundamentally misaligned\nwith the contrastive image-text training of VLMs, limiting adaptation\nperformance and introducing failure modes such as pseudo-label drift and class\ncollapse. We propose CLIPTTA, a new gradient-based TTA method for\nvision-language models that leverages a soft contrastive loss aligned with\nCLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's\ngradients, showing how its batch-aware design mitigates the risk of collapse.\nWe further extend CLIPTTA to the open-set setting, where both in-distribution\n(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier\nContrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75\ndatasets spanning diverse distribution shifts, CLIPTTA consistently outperforms\nentropy-based objectives and is highly competitive with state-of-the-art TTA\nmethods, outperforming them on a large number of datasets and exhibiting more\nstable performance across diverse shifts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCLIPTTA\uff0c\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5bf9\u6bd4\u635f\u5931\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08TTA\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u71b5\u57fa\u65b9\u6cd5\u548c\u4e0eSOTA\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5982CLIP\u867d\u5177\u5907\u5f3a\u5927\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u4f46\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6cdb\u5316\u6027\u8f83\u5dee\u3002\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08TTA\uff09\u65b9\u6cd5\uff08\u901a\u5e38\u901a\u8fc7\u71b5\u6700\u5c0f\u5316\uff09\u4e0eVLMs\u7684\u5bf9\u6bd4\u56fe\u50cf-\u6587\u672c\u9884\u8bad\u7ec3\u76ee\u6807\u4e0d\u7b26\uff0c\u8fd9\u9650\u5236\u4e86\u9002\u5e94\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u4f2a\u6807\u7b7e\u6f02\u79fb\u548c\u7c7b\u522b\u5d29\u6e83\u7b49\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u672c\u6587\u63d0\u51faCLIPTTA\uff0c\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u68af\u5ea6\u7684VLMs\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4e0eCLIP\u9884\u8bad\u7ec3\u76ee\u6807\u5bf9\u9f50\u7684\u8f6f\u5bf9\u6bd4\u635f\u5931\u3002\u7814\u7a76\u8005\u63d0\u4f9b\u4e86CLIPTTA\u68af\u5ea6\u7684\u7406\u8bba\u5206\u6790\uff0c\u9610\u660e\u5176\u6279\u6b21\u611f\u77e5\u8bbe\u8ba1\u5982\u4f55\u51cf\u8f7b\u5d29\u6e83\u98ce\u9669\u3002\u6b64\u5916\uff0cCLIPTTA\u8fd8\u901a\u8fc7\u7ed3\u5408\u5f02\u5e38\u5bf9\u6bd4\u66b4\u9732\uff08OCE\uff09\u635f\u5931\uff0c\u6269\u5c55\u5230\u5f00\u653e\u96c6\u73af\u5883\u4ee5\u6539\u8fdb\u57df\u5916\uff08OOD\uff09\u68c0\u6d4b\u3002", "result": "\u572875\u4e2a\u6db5\u76d6\u4e0d\u540c\u5206\u5e03\u504f\u79fb\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cCLIPTTA\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u71b5\u7684\u76ee\u6807\uff0c\u5e76\u4e14\u4e0e\u6700\u5148\u8fdb\u7684TTA\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u9ad8\u5ea6\u7ade\u4e89\u529b\u3002\u5b83\u5728\u5927\u91cf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f73\uff0c\u5e76\u5728\u591a\u6837\u5316\u504f\u79fb\u4e0b\u5c55\u73b0\u51fa\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "conclusion": "CLIPTTA\u901a\u8fc7\u5229\u7528\u4e0eVLMs\u9884\u8bad\u7ec3\u76ee\u6807\u4e00\u81f4\u7684\u8f6f\u5bf9\u6bd4\u635f\u5931\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709TTA\u65b9\u6cd5\u5728\u5904\u7406VLMs\u65f6\u9047\u5230\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9002\u5e94\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.14172", "pdf": "https://arxiv.org/pdf/2507.14172", "abs": "https://arxiv.org/abs/2507.14172", "authors": ["Julien Pourcel", "C\u00e9dric Colas", "Pierre-Yves Oudeyer"], "title": "Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Many program synthesis tasks prove too challenging for even state-of-the-art\nlanguage models to solve in single attempts. Search-based evolutionary methods\noffer a promising alternative by exploring solution spaces iteratively, but\ntheir effectiveness remain limited by the fixed capabilities of the underlying\ngenerative model.\n  We propose SOAR, a method that learns program synthesis by integrating\nlanguage models into a self-improving evolutionary loop.\n  SOAR alternates between (1) an evolutionary search that uses an LLM to sample\nand refine candidate solutions, and (2) a hindsight learning phase that\nconverts search attempts into valid problem-solution pairs used to fine-tune\nthe LLM's sampling and refinement capabilities\\, -- \\,enabling increasingly\neffective search in subsequent iterations.\n  On the challenging ARC-AGI benchmark, SOAR achieves significant performance\ngains across model scales and iterations, leveraging positive transfer between\nthe sampling and refinement finetuning tasks. These improvements carry over to\ntest-time adaptation, enabling SOAR to solve 52\\% of the public test set. Our\ncode is open-sourced at: https://github.com/flowersteam/SOAR", "AI": {"tldr": "SOAR\u662f\u4e00\u79cd\u81ea\u6539\u8fdb\u7684\u7a0b\u5e8f\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fdb\u5316\u641c\u7d22\u548c\u56de\u6eaf\u5b66\u4e60\u8fed\u4ee3\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728ARC-AGI\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u4e00\u6b21\u6027\u89e3\u51b3\u590d\u6742\u7684\u7a0b\u5e8f\u5408\u6210\u4efb\u52a1\uff1b\u57fa\u4e8e\u641c\u7d22\u7684\u8fdb\u5316\u65b9\u6cd5\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u53d7\u9650\u4e8e\u5e95\u5c42\u751f\u6210\u6a21\u578b\u7684\u56fa\u5b9a\u80fd\u529b\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faSOAR\u65b9\u6cd5\uff0c\u5c06\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u81ea\u6539\u8fdb\u7684\u8fdb\u5316\u5faa\u73af\u4e2d\u3002SOAR\u4ea4\u66ff\u8fdb\u884c\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u8fdb\u5316\u641c\u7d22\uff1a\u4f7f\u7528LLM\u91c7\u6837\u548c\u4f18\u5316\u5019\u9009\u89e3\u51b3\u65b9\u6848\uff1b2) \u56de\u6eaf\u5b66\u4e60\uff1a\u5c06\u641c\u7d22\u5c1d\u8bd5\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u95ee\u7b54\u5bf9\uff0c\u7528\u4e8e\u5fae\u8c03LLM\u7684\u91c7\u6837\u548c\u4f18\u5316\u80fd\u529b\uff0c\u4ece\u800c\u5728\u540e\u7eed\u8fed\u4ee3\u4e2d\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u641c\u7d22\u3002", "result": "\u5728\u6311\u6218\u6027\u7684ARC-AGI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSOAR\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u8fed\u4ee3\u6b21\u6570\u4e0b\u5747\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5229\u7528\u4e86\u91c7\u6837\u548c\u4f18\u5316\u5fae\u8c03\u4efb\u52a1\u4e4b\u95f4\u7684\u6b63\u5411\u8fc1\u79fb\u3002\u8fd9\u4e9b\u6539\u8fdb\u4e5f\u5ef6\u7eed\u5230\u6d4b\u8bd5\u65f6\u9002\u5e94\u6027\uff0c\u4f7fSOAR\u80fd\u591f\u89e3\u51b352%\u7684\u516c\u5f00\u6d4b\u8bd5\u96c6\u4efb\u52a1\u3002", "conclusion": "SOAR\u901a\u8fc7\u5c06LLM\u4e0e\u81ea\u6539\u8fdb\u7684\u8fdb\u5316\u5faa\u73af\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5176\u5728\u590d\u6742\u7a0b\u5e8f\u5408\u6210\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u95ee\u9898\u89e3\u51b3\u6f5c\u529b\u3002"}}
{"id": "2507.14239", "pdf": "https://arxiv.org/pdf/2507.14239", "abs": "https://arxiv.org/abs/2507.14239", "authors": ["Weihua Zheng", "Roy Ka-Wei Lee", "Zhengyuan Liu", "Kui Wu", "AiTi Aw", "Bowei Zou"], "title": "CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual Large Language Models(MLLMs) demonstrate strong generalization\nacross languages, yet they remain prone to hallucinations, especially in\nlow-resource languages, due to training data imbalances. These hallucinations,\nwhich include inaccurate or fabricated outputs, are particularly problematic in\ndomain-specific generation tasks (Chataigner et al., 2024). To address this\nchallenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based\nCross-lingual Chain-of-Thought), a two-stage fine-tuning framework for\nmitigating hallucination in MLLMs. Our approach first enhances cross-lingual\nsemantic alignment through curriculum-based contrastive learning combined with\nnext-token prediction during continued pre-training. Building on this\nfoundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting\nstrategy during instruction fine-tuning, which guides the model to reason in a\nhigh-resource language before generating answers in the target low-resource\nlanguage. Experimental results show that CCL-XCoT reduces hallucination rates\nby up to 62% and substantially improves factual knowledge transfer across\nlanguage pairs, without relying on external retrieval or multi-model ensembles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCCL-XCoT\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8bfe\u7a0b\u5bf9\u6bd4\u5b66\u4e60\u548c\u8de8\u8bed\u8a00\u601d\u7ef4\u94fe\uff0c\u6709\u6548\u964d\u4f4e\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u5e7b\u89c9\u3002", "motivation": "\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u5b58\u5728\u4e25\u91cd\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u9886\u57df\u7279\u5b9a\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u8fd9\u4e3b\u8981\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\u4e0d\u5e73\u8861\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u5fae\u8c03\u6846\u67b6CCL-XCoT\u3002\u7b2c\u4e00\u9636\u6bb5\u5728\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\u7ed3\u5408\u8bfe\u7a0b\u5bf9\u6bd4\u5b66\u4e60\u548c\u4e0b\u4e00\u8bcd\u5143\u9884\u6d4b\u4ee5\u589e\u5f3a\u8de8\u8bed\u8a00\u8bed\u4e49\u5bf9\u9f50\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u6307\u4ee4\u5fae\u8c03\u4e2d\u5f15\u5165\u8de8\u8bed\u8a00\u601d\u7ef4\u94fe\uff08XCoT\uff09\u63d0\u793a\u7b56\u7565\uff0c\u5f15\u5bfc\u6a21\u578b\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e2d\u63a8\u7406\u540e\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u751f\u6210\u7b54\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCCL-XCoT\u5c06\u5e7b\u89c9\u7387\u964d\u4f4e\u4e86\u9ad8\u8fbe62%\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u8de8\u8bed\u8a00\u5bf9\u7684\u4e8b\u5b9e\u77e5\u8bc6\u8fc1\u79fb\u80fd\u529b\uff0c\u4e14\u65e0\u9700\u5916\u90e8\u68c0\u7d22\u6216\u591a\u6a21\u578b\u96c6\u6210\u3002", "conclusion": "CCL-XCoT\u6846\u67b6\u6210\u529f\u7f13\u89e3\u4e86\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8de8\u8bed\u8a00\u77e5\u8bc6\u6cdb\u5316\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.14199", "pdf": "https://arxiv.org/pdf/2507.14199", "abs": "https://arxiv.org/abs/2507.14199", "authors": ["Ebrahim Abu-Helalah", "Jordi Serra", "Jordi Perez-Romero"], "title": "On Splitting Lightweight Semantic Image Segmentation for Wireless Communications", "categories": ["cs.NI", "cs.CV", "eess.IV"], "comment": "IEEE International Mediterranean Conference on Communications and\n  Networking", "summary": "Semantic communication represents a promising technique towards reducing\ncommunication costs, especially when dealing with image segmentation, but it\nstill lacks a balance between computational efficiency and bandwidth\nrequirements while maintaining high image segmentation accuracy, particularly\nin resource-limited environments and changing channel conditions. On the other\nhand, the more complex and larger semantic image segmentation models become,\nthe more stressed the devices are when processing data. This paper proposes a\nnovel approach to implementing semantic communication based on splitting the\nsemantic image segmentation process between a resource constrained transmitter\nand the receiver. This allows saving bandwidth by reducing the transmitted data\nwhile maintaining the accuracy of the semantic image segmentation.\nAdditionally, it reduces the computational requirements at the resource\nconstrained transmitter compared to doing all the semantic image segmentation\nin the transmitter. The proposed approach is evaluated by means of\nsimulation-based experiments in terms of different metrics such as\ncomputational resource usage, required bit rate and segmentation accuracy. The\nresults when comparing the proposal with the full semantic image segmentation\nin the transmitter show that up to 72% of the bit rate was reduced in the\ntransmission process. In addition, the computational load of the transmitter is\nreduced by more than 19%. This reflects the interest of this technique for its\napplication in communication systems, particularly in the upcoming 6G systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8bed\u4e49\u56fe\u50cf\u5206\u5272\u7684\u8bed\u4e49\u901a\u4fe1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5206\u5272\u8fc7\u7a0b\u5206\u62c5\u5728\u53d7\u9650\u53d1\u5c04\u7aef\u548c\u63a5\u6536\u7aef\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u8f93\u5e26\u5bbd\u548c\u53d1\u5c04\u7aef\u8ba1\u7b97\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u901a\u4fe1\u5728\u56fe\u50cf\u5206\u5272\u5e94\u7528\u4e2d\uff0c\u96be\u4ee5\u5728\u8ba1\u7b97\u6548\u7387\u3001\u5e26\u5bbd\u9700\u6c42\u548c\u5206\u5272\u7cbe\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u548c\u4fe1\u9053\u591a\u53d8\u7684\u60c5\u51b5\u4e0b\u3002\u6b64\u5916\uff0c\u66f4\u590d\u6742\u7684\u8bed\u4e49\u56fe\u50cf\u5206\u5272\u6a21\u578b\u4f1a\u589e\u52a0\u8bbe\u5907\u7684\u8ba1\u7b97\u538b\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u4e49\u901a\u4fe1\u65b9\u6cd5\uff0c\u5c06\u8bed\u4e49\u56fe\u50cf\u5206\u5272\u8fc7\u7a0b\u5206\u89e3\u5e76\u5206\u914d\u7ed9\u8d44\u6e90\u53d7\u9650\u7684\u53d1\u5c04\u7aef\u548c\u63a5\u6536\u7aef\u5171\u540c\u5b8c\u6210\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4eff\u771f\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\u3001\u6240\u9700\u6bd4\u7279\u7387\u548c\u5206\u5272\u7cbe\u5ea6\u7b49\u6307\u6807\u3002", "result": "\u4e0e\u5728\u53d1\u5c04\u7aef\u5b8c\u6210\u5b8c\u6574\u8bed\u4e49\u56fe\u50cf\u5206\u5272\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u5c06\u6bd4\u7279\u7387\u964d\u4f4e\u4e86\u9ad8\u8fbe72%\uff0c\u5e76\u5c06\u53d1\u5c04\u7aef\u7684\u8ba1\u7b97\u8d1f\u8f7d\u51cf\u5c11\u4e8619%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u6280\u672f\u5728\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u3001\u51cf\u8f7b\u8ba1\u7b97\u8d1f\u62c5\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5e94\u7528\u4e8e\u901a\u4fe1\u7cfb\u7edf\uff08\u7279\u522b\u662f\u672a\u6765\u76846G\u7cfb\u7edf\uff09\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.14306", "pdf": "https://arxiv.org/pdf/2507.14306", "abs": "https://arxiv.org/abs/2507.14306", "authors": ["Samarth P", "Vyoman Jain", "Shiva Golugula", "Motamarri Sai Sathvik"], "title": "Manimator: Transforming Research Papers into Visual Explanations", "categories": ["cs.AI", "cs.MM"], "comment": null, "summary": "Understanding complex scientific and mathematical concepts, particularly\nthose presented in dense research papers, poses a significant challenge for\nlearners. Dynamic visualizations can greatly enhance comprehension, but\ncreating them manually is time-consuming and requires specialized knowledge and\nskills. We introduce manimator, an open-source system that leverages Large\nLanguage Models to transform research papers and natural language prompts into\nexplanatory animations using the Manim engine. Manimator employs a pipeline\nwhere an LLM interprets the input text or research paper PDF to generate a\nstructured scene description outlining key concepts, mathematical formulas, and\nvisual elements and another LLM translates this description into executable\nManim Python code. We discuss its potential as an educational tool for rapidly\ncreating engaging visual explanations for complex STEM topics, democratizing\nthe creation of high-quality educational content.", "AI": {"tldr": "Manimator\u662f\u4e00\u4e2a\u5f00\u6e90\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u7814\u7a76\u8bba\u6587\u548c\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8f6c\u6362\u4e3a\u89e3\u91ca\u6027\u52a8\u753b\uff0c\u65e8\u5728\u7b80\u5316\u590d\u6742\u7684STEM\u6982\u5ff5\u5b66\u4e60\u3002", "motivation": "\u5b66\u4e60\u8005\u96be\u4ee5\u7406\u89e3\u590d\u6742\u4e14\u5bc6\u96c6\u7684\u79d1\u5b66\u548c\u6570\u5b66\u6982\u5ff5\u3002\u867d\u7136\u52a8\u6001\u53ef\u89c6\u5316\u80fd\u663e\u8457\u63d0\u9ad8\u7406\u89e3\u529b\uff0c\u4f46\u624b\u52a8\u521b\u5efa\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u6280\u80fd\u3002", "method": "Manimator\u91c7\u7528LLM\u9a71\u52a8\u7684\u7ba1\u9053\uff1a\u4e00\u4e2aLLM\u5c06\u8f93\u5165\u6587\u672c\u6216PDF\u89e3\u91ca\u4e3a\u7ed3\u6784\u5316\u7684\u573a\u666f\u63cf\u8ff0\uff08\u5305\u542b\u6982\u5ff5\u3001\u516c\u5f0f\u548c\u89c6\u89c9\u5143\u7d20\uff09\uff0c\u53e6\u4e00\u4e2aLLM\u5219\u5c06\u6b64\u63cf\u8ff0\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684Manim Python\u4ee3\u7801\u3002", "result": "\u8be5\u7cfb\u7edf\u6709\u671b\u5feb\u901f\u4e3a\u590d\u6742\u7684STEM\u4e3b\u9898\u521b\u5efa\u5f15\u4eba\u5165\u80dc\u7684\u89c6\u89c9\u89e3\u91ca\uff0c\u4ece\u800c\u666e\u53ca\u9ad8\u8d28\u91cf\u6559\u80b2\u5185\u5bb9\u7684\u521b\u4f5c\u3002", "conclusion": "Manimator\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u53ef\u89c6\u5316\u7b80\u5316\u590d\u6742STEM\u6982\u5ff5\u7684\u5b66\u4e60\uff0c\u6709\u671b\u4f7f\u9ad8\u8d28\u91cf\u6559\u80b2\u5185\u5bb9\u7684\u521b\u5efa\u66f4\u52a0\u9ad8\u6548\u548c\u666e\u53ca\u3002"}}
{"id": "2507.14315", "pdf": "https://arxiv.org/pdf/2507.14315", "abs": "https://arxiv.org/abs/2507.14315", "authors": ["Qiyu Xu", "Zhanxuan Hu", "Yu Duan", "Ercheng Pei", "Yonghang Tai"], "title": "A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention", "categories": ["cs.CV"], "comment": null, "summary": "Generalized Category Discovery (GCD) aims to classify unlabeled data from\nboth known and unknown categories by leveraging knowledge from labeled known\ncategories. While existing methods have made notable progress, they often\noverlook a hidden stumbling block in GCD: distracted attention. Specifically,\nwhen processing unlabeled data, models tend to focus not only on key objects in\nthe image but also on task-irrelevant background regions, leading to suboptimal\nfeature extraction. To remove this stumbling block, we propose Attention\nFocusing (AF), an adaptive mechanism designed to sharpen the model's focus by\npruning non-informative tokens. AF consists of two simple yet effective\ncomponents: Token Importance Measurement (TIME) and Token Adaptive Pruning\n(TAP), working in a cascade. TIME quantifies token importance across multiple\nscales, while TAP prunes non-informative tokens by utilizing the multi-scale\nimportance scores provided by TIME. AF is a lightweight, plug-and-play module\nthat integrates seamlessly into existing GCD methods with minimal computational\noverhead. When incorporated into one prominent GCD method, SimGCD, AF achieves\nup to 15.4% performance improvement over the baseline with minimal\ncomputational overhead. The implementation code is provided in\nhttps://github.com/Afleve/AFGCD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6ce8\u610f\u529b\u805a\u7126\uff08AF\uff09\u7684\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u901a\u8fc7\u4fee\u526a\u975e\u4fe1\u606f\u6027token\u6765\u89e3\u51b3\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\uff08GCD\uff09\u4e2d\u6a21\u578b\u6ce8\u610f\u529b\u5206\u6563\u7684\u95ee\u9898\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\uff08GCD\uff09\u65b9\u6cd5\u5728\u5904\u7406\u672a\u6807\u8bb0\u6570\u636e\u65f6\uff0c\u6a21\u578b\u6ce8\u610f\u529b\u6613\u5206\u6563\uff0c\u4e0d\u4ec5\u5173\u6ce8\u56fe\u50cf\u4e2d\u7684\u5173\u952e\u5bf9\u8c61\uff0c\u4e5f\u5173\u6ce8\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u80cc\u666f\u533a\u57df\uff0c\u5bfc\u81f4\u7279\u5f81\u63d0\u53d6\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u6ce8\u610f\u529b\u805a\u7126\uff08Attention Focusing, AF\uff09\u673a\u5236\uff0c\u65e8\u5728\u901a\u8fc7\u4fee\u526a\u975e\u4fe1\u606f\u6027token\u6765\u9510\u5316\u6a21\u578b\u7126\u70b9\u3002AF\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1aToken\u91cd\u8981\u6027\u6d4b\u91cf\uff08TIME\uff09\u7528\u4e8e\u91cf\u5316\u591a\u5c3a\u5ea6token\u91cd\u8981\u6027\uff0c\u4ee5\u53caToken\u81ea\u9002\u5e94\u4fee\u526a\uff08TAP\uff09\u5229\u7528TIME\u63d0\u4f9b\u7684\u5206\u6570\u4fee\u526a\u975e\u4fe1\u606f\u6027token\u3002AF\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\u3002", "result": "\u5c06AF\u96c6\u6210\u5230SimGCD\u8fd9\u4e00\u4e3b\u6d41GCD\u65b9\u6cd5\u4e2d\uff0cAF\u5728\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe15.4%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "AF\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86GCD\u4e2d\u7684\u6ce8\u610f\u529b\u5206\u6563\u95ee\u9898\uff0c\u901a\u8fc7\u66f4\u7cbe\u51c6\u7684\u7279\u5f81\u63d0\u53d6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u5177\u6709\u8f7b\u91cf\u5316\u548c\u6613\u4e8e\u96c6\u6210\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.14175", "pdf": "https://arxiv.org/pdf/2507.14175", "abs": "https://arxiv.org/abs/2507.14175", "authors": ["Youcef Barkat", "Dylan Hamitouche", "Deven Parekh", "Ivy Guo", "David Benrimoh"], "title": "Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data", "categories": ["cs.LG", "cs.AI", "stat.AP"], "comment": null, "summary": "Background: Mental illnesses such as depression and anxiety require improved\nmethods for early detection and personalized intervention. Traditional\npredictive models often rely on unimodal data or early fusion strategies that\nfail to capture the complex, multimodal nature of psychiatric data. Advanced\nintegration techniques, such as intermediate (latent space) fusion, may offer\nbetter accuracy and clinical utility. Methods: Using data from the BRIGHTEN\nclinical trial, we evaluated intermediate (latent space) fusion for predicting\ndaily depressive symptoms (PHQ-2 scores). We compared early fusion implemented\nwith a Random Forest (RF) model and intermediate fusion implemented via a\nCombined Model (CM) using autoencoders and a neural network. The dataset\nincluded behavioral (smartphone-based), demographic, and clinical features.\nExperiments were conducted across multiple temporal splits and data stream\ncombinations. Performance was evaluated using mean squared error (MSE) and\ncoefficient of determination (R2). Results: The CM outperformed both RF and\nLinear Regression (LR) baselines across all setups, achieving lower MSE (0.4985\nvs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed\nsigns of overfitting, with a large gap between training and test performance,\nwhile the CM maintained consistent generalization. Performance was best when\nintegrating all data modalities in the CM (in contradistinction to RF),\nunderscoring the value of latent space fusion for capturing non-linear\ninteractions in complex psychiatric datasets. Conclusion: Latent space fusion\noffers a robust alternative to traditional fusion methods for prediction with\nmultimodal mental health data. Future work should explore model\ninterpretability and individual-level prediction for clinical deployment.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u6f5c\u5728\u7a7a\u95f4\u878d\u5408\u65b9\u6cd5\u5728\u9884\u6d4b\u591a\u6a21\u6001\u7cbe\u795e\u5065\u5eb7\u6570\u636e\uff08\u5982\u6291\u90c1\u75c7\u72b6\uff09\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u65e9\u671f\u878d\u5408\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7cbe\u795e\u75be\u75c5\uff08\u5982\u6291\u90c1\u548c\u7126\u8651\uff09\u9700\u8981\u66f4\u4f18\u7684\u65e9\u671f\u68c0\u6d4b\u548c\u4e2a\u6027\u5316\u5e72\u9884\u65b9\u6cd5\u3002\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u4f9d\u8d56\u5355\u6a21\u6001\u6570\u636e\u6216\u65e9\u671f\u878d\u5408\u7b56\u7565\uff0c\u672a\u80fd\u6709\u6548\u6355\u6349\u7cbe\u795e\u75c5\u7406\u6570\u636e\u7684\u590d\u6742\u591a\u6a21\u6001\u7279\u6027\u3002\u9ad8\u7ea7\u96c6\u6210\u6280\u672f\uff0c\u5982\u4e2d\u95f4\uff08\u6f5c\u5728\u7a7a\u95f4\uff09\u878d\u5408\uff0c\u6709\u671b\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "method": "\u5229\u7528BRIGHTEN\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\uff0c\u8bc4\u4f30\u4e86\u4e2d\u95f4\uff08\u6f5c\u5728\u7a7a\u95f4\uff09\u878d\u5408\uff08\u901a\u8fc7\u5305\u542b\u81ea\u7f16\u7801\u5668\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u7ec4\u5408\u6a21\u578bCM\u5b9e\u73b0\uff09\u4e0e\u65e9\u671f\u878d\u5408\uff08\u4f7f\u7528\u968f\u673a\u68ee\u6797RF\u5b9e\u73b0\uff09\u5728\u9884\u6d4b\u6bcf\u65e5\u6291\u90c1\u75c7\u72b6\uff08PHQ-2\u8bc4\u5206\uff09\u65b9\u9762\u7684\u6027\u80fd\u3002\u6570\u636e\u96c6\u5305\u542b\u884c\u4e3a\uff08\u667a\u80fd\u624b\u673a\uff09\u3001\u4eba\u53e3\u7edf\u8ba1\u548c\u4e34\u5e8a\u7279\u5f81\u3002\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u4e3a\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u548c\u51b3\u5b9a\u7cfb\u6570\uff08R2\uff09\uff0c\u5e76\u5728\u591a\u65f6\u95f4\u5206\u5272\u548c\u6570\u636e\u6d41\u7ec4\u5408\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u7ec4\u5408\u6a21\u578b\uff08CM\uff09\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u5747\u4f18\u4e8e\u968f\u673a\u68ee\u6797\uff08RF\uff09\u548c\u7ebf\u6027\u56de\u5f52\uff08LR\uff09\u57fa\u7ebf\u6a21\u578b\uff0c\u53d6\u5f97\u4e86\u66f4\u4f4e\u7684MSE\uff080.4985 vs. RF\u76840.5305\uff09\u548c\u66f4\u9ad8\u7684R2\uff080.4695 vs. RF\u76840.4356\uff09\u3002RF\u6a21\u578b\u8868\u73b0\u51fa\u8fc7\u62df\u5408\u8ff9\u8c61\uff0c\u800cCM\u6a21\u578b\u4fdd\u6301\u4e86\u826f\u597d\u4e14\u4e00\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u3002CM\u5728\u6574\u5408\u6240\u6709\u6570\u636e\u6a21\u6001\u65f6\u8868\u73b0\u6700\u4f73\uff0c\u7a81\u663e\u4e86\u6f5c\u5728\u7a7a\u95f4\u878d\u5408\u6355\u6349\u590d\u6742\u7cbe\u795e\u75c5\u5b66\u6570\u636e\u4e2d\u975e\u7ebf\u6027\u4ea4\u4e92\u7684\u4ef7\u503c\u3002", "conclusion": "\u6f5c\u5728\u7a7a\u95f4\u878d\u5408\u4e3a\u591a\u6a21\u6001\u7cbe\u795e\u5065\u5eb7\u6570\u636e\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u878d\u5408\u65b9\u6cd5\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5e94\u63a2\u7d22\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u4e2a\u4f53\u5c42\u9762\u9884\u6d4b\uff0c\u4ee5\u5b9e\u73b0\u4e34\u5e8a\u90e8\u7f72\u3002"}}
{"id": "2507.14240", "pdf": "https://arxiv.org/pdf/2507.14240", "abs": "https://arxiv.org/abs/2507.14240", "authors": ["Mohammad Shahedur Rahman", "Peng Gao", "Yuede Ji"], "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "Large language models (LLMs) leverage deep learning to process and predict\nsequences of words from context, enabling them to perform various NLP tasks,\nsuch as translation, summarization, question answering, and content generation.\nHowever, the growing size and complexity of developing, training, and deploying\nadvanced LLMs require extensive computational resources and large datasets.\nThis creates a barrier for users. As a result, platforms that host models and\ndatasets are widely used. For example, Hugging Face, one of the most popular\nplatforms, hosted 1.8 million models and 450K datasets by June 2025, with no\nsign of slowing down. Since many LLMs are built from base models, pre-trained\nmodels, and external datasets, they can inherit vulnerabilities, biases, or\nmalicious components from earlier models or datasets. Therefore, it is critical\nto understand the origin and development of these components to better detect\npotential risks, improve model fairness, and ensure compliance. Motivated by\nthis, our project aims to study the relationships between models and datasets,\nwhich are core components of the LLM supply chain. First, we design a method to\nsystematically collect LLM supply chain data. Using this data, we build a\ndirected heterogeneous graph to model the relationships between models and\ndatasets, resulting in a structure with 397,376 nodes and 453,469 edges. We\nthen perform various analyses and uncover several findings, such as: (i) the\nLLM supply chain graph is large, sparse, and follows a power-law degree\ndistribution; (ii) it features a densely connected core and a fragmented\nperiphery; (iii) datasets play pivotal roles in training; (iv) strong\ninterdependence exists between models and datasets; and (v) the graph is\ndynamic, with daily updates reflecting the ecosystem's ongoing evolution.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u548c\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f9b\u5e94\u94fe\u4e2d\u6a21\u578b\u4e0e\u6570\u636e\u96c6\u7684\u5173\u7cfb\u56fe\uff0c\u63ed\u793a\u5176\u7ed3\u6784\u3001\u4f9d\u8d56\u6027\u548c\u52a8\u6001\u6027\uff0c\u65e8\u5728\u5e94\u5bf9\u6f5c\u5728\u7684\u98ce\u9669\u548c\u504f\u89c1\u3002", "motivation": "\u9274\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4ece\u57fa\u7840\u6a21\u578b\u6216\u6570\u636e\u96c6\u4e2d\u7ee7\u627f\u6f0f\u6d1e\u3001\u504f\u89c1\u6216\u6076\u610f\u7ec4\u4ef6\uff0c\u7406\u89e3\u8fd9\u4e9b\u7ec4\u4ef6\u7684\u6765\u6e90\u548c\u53d1\u5c55\u5bf9\u4e8e\u98ce\u9669\u68c0\u6d4b\u3001\u6a21\u578b\u516c\u5e73\u6027\u63d0\u5347\u548c\u5408\u89c4\u6027\u4fdd\u969c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\u6765\u6536\u96c6LLM\u4f9b\u5e94\u94fe\u6570\u636e\u3002\u968f\u540e\uff0c\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b397,376\u4e2a\u8282\u70b9\u548c453,469\u6761\u8fb9\u7684\u6709\u5411\u5f02\u6784\u56fe\uff0c\u4ee5\u5efa\u6a21\u548c\u5206\u6790\u6a21\u578b\u4e0e\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "result": "\u5206\u6790\u53d1\u73b0LLM\u4f9b\u5e94\u94fe\u56fe\u5de8\u5927\u3001\u7a00\u758f\u5e76\u9075\u5faa\u5e42\u5f8b\u5ea6\u5206\u5e03\uff1b\u5177\u6709\u5bc6\u96c6\u8fde\u63a5\u7684\u6838\u5fc3\u548c\u5206\u6563\u7684\u5916\u56f4\uff1b\u6570\u636e\u5728\u6a21\u578b\u8bad\u7ec3\u4e2d\u626e\u6f14\u7740\u5173\u952e\u89d2\u8272\uff1b\u6a21\u578b\u4e0e\u6570\u636e\u96c6\u4e4b\u95f4\u5b58\u5728\u5f3a\u70c8\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\uff1b\u4e14\u8be5\u56fe\u662f\u52a8\u6001\u7684\uff0c\u6bcf\u65e5\u90fd\u5728\u66f4\u65b0\u3002", "conclusion": "\u901a\u8fc7\u5bf9LLM\u4f9b\u5e94\u94fe\u4e2d\u6a21\u578b\u4e0e\u6570\u636e\u96c6\u5173\u7cfb\u7684\u6df1\u5165\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u5176\u590d\u6742\u7ed3\u6784\u548c\u52a8\u6001\u6027\uff0c\u4e3a\u7406\u89e3\u5e76\u89e3\u51b3LLM\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\u3001\u516c\u5e73\u6027\u95ee\u9898\u53ca\u5408\u89c4\u6027\u6311\u6218\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2507.14205", "pdf": "https://arxiv.org/pdf/2507.14205", "abs": "https://arxiv.org/abs/2507.14205", "authors": ["Pavel Malinovskiy"], "title": "A Fault-Tolerant Architecture for Urban and Rural Digital Connectivity: Synergizing SDWMN, Direct-to-Mobile Broadcasting, and Hybrid Cloud Streaming", "categories": ["cs.NI"], "comment": null, "summary": "We propose an integrated architecture combining Software-Defined Wireless\nMesh Networks (SDWMN), Direct-to-Mobile (D2M) broadcasting, and Kafka-based\nhybrid cloud streaming to improve wireless network performance in both urban\nand rural settings. The approach addresses urban congestion and rural digital\nexclusion through traffic offloading, enhanced fault tolerance, and equitable\nresource allocation. We model urban congestion $\\rho_u = \\lambda_t / \\mu_c$ and\nrural coverage deficit $\\delta_r = 1 - C_r / C_{req}$, and aim to minimize\nglobal performance loss $GPL = w_1 \\cdot \\rho_u + w_2 \\cdot \\delta_r + w_3\n\\cdot T_{rec}$, where $T_{rec}$ is recovery time. Experiments in Bangkok,\nMumbai, and rural Finland demonstrate latency reduction over 32%, bandwidth\noffloading of 40%, rural coverage gain of 28%, and fairness index rising from\n0.78 to 0.91. The system achieves recovery under 10 s using SDWMN and Kafka. We\nrecommend optimal spectrum allocation $\\alpha_s$, targeted subsidies, and\ndevice mandates to promote adoption. This scalable, fault-tolerant design\nsupports equitable digital transformation and suggests directions for future\nresearch.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408SDWMN\u3001D2M\u548cKafka\u7684\u7efc\u5408\u67b6\u6784\uff0c\u65e8\u5728\u6539\u5584\u57ce\u4e61\u65e0\u7ebf\u7f51\u7edc\u6027\u80fd\uff0c\u89e3\u51b3\u62e5\u5835\u548c\u6570\u5b57\u9e3f\u6c9f\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u7f51\u7edc\u62e5\u5835\u548c\u519c\u6751\u6570\u5b57\u6392\u65a5\u95ee\u9898\uff0c\u901a\u8fc7\u6d41\u91cf\u5206\u6d41\u3001\u589e\u5f3a\u5bb9\u9519\u548c\u516c\u5e73\u8d44\u6e90\u5206\u914d\u6765\u63d0\u5347\u65e0\u7ebf\u7f51\u7edc\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u96c6\u6210\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7f51\u683c\u7f51\u7edc\uff08SDWMN\uff09\u3001\u76f4\u8fde\u79fb\u52a8\uff08D2M\uff09\u5e7f\u64ad\u548c\u57fa\u4e8eKafka\u6df7\u5408\u4e91\u6d41\u5a92\u4f53\u7684\u67b6\u6784\u3002\u901a\u8fc7\u5efa\u6a21\u57ce\u5e02\u62e5\u5835\uff08\u03c1u\uff09\u548c\u519c\u6751\u8986\u76d6\u4e0d\u8db3\uff08\u03b4r\uff09\uff0c\u76ee\u6807\u662f\u6700\u5c0f\u5316\u5168\u7403\u6027\u80fd\u635f\u5931\uff08GPL\uff09\uff0c\u5e76\u8003\u8651\u6062\u590d\u65f6\u95f4\uff08Trec\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u5ef6\u8fdf\u964d\u4f4e\u8d85\u8fc732%\uff0c\u5e26\u5bbd\u5206\u6d4140%\uff0c\u519c\u6751\u8986\u76d6\u7387\u63d0\u534728%\uff0c\u516c\u5e73\u6027\u6307\u6570\u4ece0.78\u63d0\u9ad8\u52300.91\u3002\u7cfb\u7edf\u572810\u79d2\u5185\u5b9e\u73b0\u6062\u590d\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u5bb9\u9519\u6027\uff0c\u652f\u6301\u516c\u5e73\u7684\u6570\u5b57\u5316\u8f6c\u578b\u3002\u4e3a\u4fc3\u8fdb\u91c7\u7528\uff0c\u5efa\u8bae\u4f18\u5316\u9891\u8c31\u5206\u914d\u3001\u63d0\u4f9b\u5b9a\u5411\u8865\u8d34\u548c\u5f3a\u5236\u8bbe\u5907\u4f7f\u7528\u3002\u7814\u7a76\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.14334", "pdf": "https://arxiv.org/pdf/2507.14334", "abs": "https://arxiv.org/abs/2507.14334", "authors": ["Hui Yang", "Jiaoyan Chen", "Yuan He", "Yongsheng Gao", "Ian Horrocks"], "title": "Language Models as Ontology Encoders", "categories": ["cs.AI"], "comment": null, "summary": "OWL (Web Ontology Language) ontologies which are able to formally represent\ncomplex knowledge and support semantic reasoning have been widely adopted\nacross various domains such as healthcare and bioinformatics. Recently,\nontology embeddings have gained wide attention due to its potential to infer\nplausible new knowledge and approximate complex reasoning. However, existing\nmethods face notable limitations: geometric model-based embeddings typically\noverlook valuable textual information, resulting in suboptimal performance,\nwhile the approaches that incorporate text, which are often based on language\nmodels, fail to preserve the logical structure. In this work, we propose a new\nontology embedding method OnT, which tunes a Pretrained Language Model (PLM)\nvia geometric modeling in a hyperbolic space for effectively incorporating\ntextual labels and simultaneously preserving class hierarchies and other\nlogical relationships of Description Logic EL. Extensive experiments on four\nreal-world ontologies show that OnT consistently outperforms the baselines\nincluding the state-of-the-art across both tasks of prediction and inference of\naxioms. OnT also demonstrates strong potential in real-world applications,\nindicated by its robust transfer learning abilities and effectiveness in real\ncases of constructing a new ontology from SNOMED CT. Data and code are\navailable at https://github.com/HuiYang1997/OnT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u672c\u4f53\u5d4c\u5165\u65b9\u6cd5OnT\uff0c\u5b83\u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u53cc\u66f2\u51e0\u4f55\u5efa\u6a21\uff0c\u4ee5\u540c\u65f6\u5229\u7528\u6587\u672c\u4fe1\u606f\u5e76\u4fdd\u6301\u672c\u4f53\u7684\u903b\u8f91\u7ed3\u6784\uff0c\u5728\u516c\u7406\u9884\u6d4b\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u672c\u4f53\u5d4c\u5165\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u51e0\u4f55\u6a21\u578b\u7684\u5d4c\u5165\u5ffd\u7565\u6587\u672c\u4fe1\u606f\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\uff0c\u800c\u7ed3\u5408\u6587\u672c\uff08\u901a\u5e38\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\uff09\u7684\u65b9\u6cd5\u672a\u80fd\u4fdd\u7559\u672c\u4f53\u7684\u903b\u8f91\u7ed3\u6784\u3002", "method": "\u672c\u6587\u63d0\u51faOnT\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u8fdb\u884c\u51e0\u4f55\u5efa\u6a21\u6765\u5fae\u8c03\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\uff0c\u4ece\u800c\u6709\u6548\u6574\u5408\u6587\u672c\u6807\u7b7e\uff0c\u5e76\u540c\u65f6\u4fdd\u7559\u7c7b\u5c42\u6b21\u7ed3\u6784\u53ca\u63cf\u8ff0\u903b\u8f91EL\u7684\u5176\u4ed6\u903b\u8f91\u5173\u7cfb\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u672c\u4f53\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cOnT\u5728\u516c\u7406\u9884\u6d4b\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5305\u62ec\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u5185\u7684\u57fa\u7ebf\u3002OnT\u8fd8\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\u548c\u5728\u4eceSNOMED CT\u6784\u5efa\u65b0\u672c\u4f53\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "OnT\u662f\u4e00\u79cd\u6709\u6548\u7ed3\u5408\u6587\u672c\u548c\u903b\u8f91\u7ed3\u6784\u7684\u672c\u4f53\u5d4c\u5165\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u663e\u793a\u51fa\u5de8\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14367", "pdf": "https://arxiv.org/pdf/2507.14367", "abs": "https://arxiv.org/abs/2507.14367", "authors": ["Weiming Ren", "Raghav Goyal", "Zhiming Hu", "Tristan Ty Aumentado-Armstrong", "Iqbal Mohomed", "Alex Levinshtein"], "title": "Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution", "categories": ["cs.CV"], "comment": "12 pages, 17 figures and 7 tables", "summary": "Generative super-resolution (GSR) currently sets the state-of-the-art in\nterms of perceptual image quality, overcoming the \"regression-to-the-mean\" blur\nof prior non-generative models. However, from a human perspective, such models\ndo not fully conform to the optimal balance between quality and fidelity.\nInstead, a different class of artifacts, in which generated details fail to\nperceptually match the low resolution image (LRI) or ground-truth image (GTI),\nis a critical but under studied issue in GSR, limiting its practical\ndeployments. In this work, we focus on measuring, analyzing, and mitigating\nthese artifacts (i.e., \"hallucinations\"). We observe that hallucinations are\nnot well-characterized with existing image metrics or quality models, as they\nare orthogonal to both exact fidelity and no-reference quality. Instead, we\ntake advantage of a multimodal large language model (MLLM) by constructing a\nprompt that assesses hallucinatory visual elements and generates a\n\"Hallucination Score\" (HS). We find that our HS is closely aligned with human\nevaluations, and also provides complementary insights to prior image metrics\nused for super-resolution (SR) models. In addition, we find certain deep\nfeature distances have strong correlations with HS. We therefore propose to\nalign the GSR models by using such features as differentiable reward functions\nto mitigate hallucinations.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u751f\u6210\u5f0f\u8d85\u5206\u8fa8\u7387\uff08GSR\uff09\u4e2d\u7684\u201c\u5e7b\u89c9\u201d\u4f2a\u5f71\u95ee\u9898\uff0c\u63d0\u51fa\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u8bc4\u4f30\u5e7b\u89c9\u5e76\u751f\u6210\u201c\u5e7b\u89c9\u5206\u6570\u201d\uff08HS\uff09\u3002\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u6df1\u5ea6\u7279\u5f81\u8ddd\u79bb\u4e0eHS\u9ad8\u5ea6\u76f8\u5173\uff0c\u5e76\u636e\u6b64\u63d0\u51fa\u4e00\u79cdGSR\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u8fd9\u4e9b\u7279\u5f81\u4f5c\u4e3a\u53ef\u5fae\u5206\u5956\u52b1\u51fd\u6570\u6765\u51cf\u8f7b\u5e7b\u89c9\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5f0f\u8d85\u5206\u8fa8\u7387\uff08GSR\uff09\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u4f46\u5176\u751f\u6210\u7684\u7ec6\u8282\uff08\u5373\u201c\u5e7b\u89c9\u201d\uff09\u5e38\u4e0e\u539f\u59cb\u4f4e\u5206\u8fa8\u7387\u6216\u771f\u5b9e\u56fe\u50cf\u4e0d\u7b26\uff0c\u8fd9\u662f\u4e00\u4e2a\u88ab\u4f4e\u4f30\u7684\u5173\u952e\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\u3002\u73b0\u6709\u56fe\u50cf\u6307\u6807\u4e5f\u672a\u80fd\u6709\u6548\u8868\u5f81\u8fd9\u4e9b\u5e7b\u89c9\u3002", "method": "1. \u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u6784\u5efa\u63d0\u793a\u8bcd\uff0c\u8bc4\u4f30\u89c6\u89c9\u5e7b\u89c9\u5143\u7d20\u5e76\u751f\u6210\u201c\u5e7b\u89c9\u5206\u6570\u201d\uff08HS\uff09\u30022. \u5206\u6790\u53d1\u73b0\uff0c\u7279\u5b9a\u6df1\u5ea6\u7279\u5f81\u8ddd\u79bb\u4e0eHS\u9ad8\u5ea6\u76f8\u5173\u30023. \u5229\u7528\u8fd9\u4e9b\u6df1\u5ea6\u7279\u5f81\u4f5c\u4e3a\u53ef\u5fae\u5206\u5956\u52b1\u51fd\u6570\uff0c\u5bf9GSR\u6a21\u578b\u8fdb\u884c\u5bf9\u9f50\uff0c\u4ee5\u7f13\u89e3\u5e7b\u89c9\u3002", "result": "1. \u63d0\u51fa\u7684\u201c\u5e7b\u89c9\u5206\u6570\u201d\uff08HS\uff09\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\u30022. HS\u4e3a\u73b0\u6709\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u6a21\u578b\u7684\u56fe\u50cf\u6307\u6807\u63d0\u4f9b\u4e86\u8865\u5145\u6027\u89c1\u89e3\u30023. \u53d1\u73b0\u67d0\u4e9b\u6df1\u5ea6\u7279\u5f81\u8ddd\u79bb\u4e0eHS\u5b58\u5728\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u63d0\u51fa\u4e86\u91cf\u5316GSR\u5e7b\u89c9\u7684\u65b0\u65b9\u6cd5\uff08HS\uff09\uff0c\u5e76\u57fa\u4e8e\u5bf9\u6df1\u5ea6\u7279\u5f81\u4e0e\u5e7b\u89c9\u76f8\u5173\u6027\u7684\u53d1\u73b0\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684GSR\u6a21\u578b\u5bf9\u9f50\u7b56\u7565\uff0c\u80fd\u663e\u8457\u51cf\u8f7b\u5e7b\u89c9\u4f2a\u5f71\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2507.14176", "pdf": "https://arxiv.org/pdf/2507.14176", "abs": "https://arxiv.org/abs/2507.14176", "authors": ["Andr\u00e9s Morales-Forero", "Lili J. Rueda", "Ronald Herrera", "Samuel Bassetto", "Eric Coatanea"], "title": "Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection", "categories": ["cs.LG", "stat.CO", "stat.ML"], "comment": null, "summary": "Artificial intelligence (AI) systems increasingly inform medical\ndecision-making, yet concerns about algorithmic bias and inequitable outcomes\npersist, particularly for historically marginalized populations. This paper\nintroduces the concept of Predictive Representativity (PR), a framework of\nfairness auditing that shifts the focus from the composition of the data set to\noutcomes-level equity. Through a case study in dermatology, we evaluated\nAI-based skin cancer classifiers trained on the widely used HAM10000 dataset\nand on an independent clinical dataset (BOSQUE Test set) from Colombia. Our\nanalysis reveals substantial performance disparities by skin phototype, with\nclassifiers consistently underperforming for individuals with darker skin,\ndespite proportional sampling in the source data. We argue that\nrepresentativity must be understood not as a static feature of datasets but as\na dynamic, context-sensitive property of model predictions. PR operationalizes\nthis shift by quantifying how reliably models generalize fairness across\nsubpopulations and deployment contexts. We further propose an External\nTransportability Criterion that formalizes the thresholds for fairness\ngeneralization. Our findings highlight the ethical imperative for post-hoc\nfairness auditing, transparency in dataset documentation, and inclusive model\nvalidation pipelines. This work offers a scalable tool for diagnosing\nstructural inequities in AI systems, contributing to discussions on equity,\ninterpretability, and data justice and fostering a critical re-evaluation of\nfairness in data-driven healthcare.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u9884\u6d4b\u4ee3\u8868\u6027\u201d\uff08PR\uff09\u6846\u67b6\uff0c\u5c06AI\u516c\u5e73\u6027\u5ba1\u8ba1\u7126\u70b9\u4ece\u6570\u636e\u6784\u6210\u8f6c\u5411\u7ed3\u679c\u516c\u5e73\u6027\u3002\u901a\u8fc7\u76ae\u80a4\u764c\u5206\u7c7b\u5668\u6848\u4f8b\uff0c\u53d1\u73b0\u6a21\u578b\u5bf9\u6df1\u8272\u76ae\u80a4\u6027\u80fd\u663e\u8457\u4e0d\u4f73\uff0c\u5f3a\u8c03\u4ee3\u8868\u6027\u5e94\u662f\u52a8\u6001\u5c5e\u6027\uff0c\u5e76\u547c\u5401\u52a0\u5f3a\u4e8b\u540e\u516c\u5e73\u6027\u5ba1\u8ba1\u548c\u6a21\u578b\u9a8c\u8bc1\u7684\u5305\u5bb9\u6027\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u533b\u7597\u51b3\u7b56\uff0c\u4f46\u7b97\u6cd5\u504f\u89c1\u548c\u5bfc\u81f4\u7684\u4e0d\u516c\u5e73\u7ed3\u679c\uff08\u7279\u522b\u662f\u5bf9\u5386\u53f2\u4e0a\u88ab\u8fb9\u7f18\u5316\u7684\u4eba\u7fa4\uff09\u4ecd\u662f\u6301\u7eed\u5b58\u5728\u7684\u62c5\u5fe7\u3002\u73b0\u6709\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u89e3\u51b3\u7ed3\u679c\u5c42\u9762\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u201c\u9884\u6d4b\u4ee3\u8868\u6027\u201d\uff08PR\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u516c\u5e73\u6027\u5ba1\u8ba1\u7684\u91cd\u70b9\u4ece\u6570\u636e\u96c6\u6784\u6210\u8f6c\u79fb\u5230\u7ed3\u679c\u5c42\u9762\u7684\u516c\u5e73\u6027\u3002\u901a\u8fc7\u76ae\u80a4\u75c5\u5b66\u6848\u4f8b\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u5728HAM10000\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684AI\u76ae\u80a4\u764c\u5206\u7c7b\u5668\uff0c\u5e76\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u4ee5\u53ca\u72ec\u7acb\u7684\u54e5\u4f26\u6bd4\u4e9a\u4e34\u5e8a\u6570\u636e\u96c6\uff08BOSQUE Test set\uff09\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u201c\u5916\u90e8\u53ef\u8fc1\u79fb\u6027\u6807\u51c6\u201d\u6765\u5f62\u5f0f\u5316\u516c\u5e73\u6027\u6cdb\u5316\u7684\u9608\u503c\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c\u5c3d\u7ba1\u6e90\u6570\u636e\u96c6\u4e2d\u8fdb\u884c\u4e86\u6309\u6bd4\u4f8b\u62bd\u6837\uff0c\u5206\u7c7b\u5668\u5728\u4e0d\u540c\u76ae\u80a4\u5149\u578b\uff08\u5c24\u5176\u662f\u6df1\u8272\u76ae\u80a4\u4e2a\u4f53\uff09\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5bf9\u6df1\u8272\u76ae\u80a4\u7684\u8bc6\u522b\u80fd\u529b\u6301\u7eed\u504f\u4f4e\u3002\u7814\u7a76\u8ba4\u4e3a\uff0c\u4ee3\u8868\u6027\u4e0d\u5e94\u88ab\u7406\u89e3\u4e3a\u6570\u636e\u96c6\u7684\u9759\u6001\u7279\u5f81\uff0c\u800c\u5e94\u662f\u6a21\u578b\u9884\u6d4b\u7684\u52a8\u6001\u3001\u60c5\u5883\u654f\u611f\u5c5e\u6027\u3002PR\u6846\u67b6\u91cf\u5316\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u4e9a\u7fa4\u548c\u90e8\u7f72\u73af\u5883\u4e2d\u6cdb\u5316\u516c\u5e73\u6027\u7684\u53ef\u9760\u7a0b\u5ea6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u4e8b\u540e\u516c\u5e73\u6027\u5ba1\u8ba1\u3001\u6570\u636e\u96c6\u6587\u6863\u900f\u660e\u5316\u548c\u5305\u5bb9\u6027\u6a21\u578b\u9a8c\u8bc1\u6d41\u7a0b\u7684\u4f26\u7406\u5fc5\u8981\u6027\u3002\u672c\u6587\u63d0\u51fa\u7684PR\u6846\u67b6\u662f\u4e00\u79cd\u8bca\u65adAI\u7cfb\u7edf\u7ed3\u6784\u6027\u4e0d\u516c\u5e73\u7684\u53ef\u6269\u5c55\u5de5\u5177\uff0c\u4e3a\u5173\u4e8e\u516c\u5e73\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6570\u636e\u6b63\u4e49\u7684\u8ba8\u8bba\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u5e76\u4fc3\u4f7f\u5bf9\u6570\u636e\u9a71\u52a8\u533b\u7597\u4e2d\u516c\u5e73\u6027\u6982\u5ff5\u8fdb\u884c\u6279\u5224\u6027\u91cd\u65b0\u8bc4\u4f30\u3002"}}
{"id": "2507.14241", "pdf": "https://arxiv.org/pdf/2507.14241", "abs": "https://arxiv.org/abs/2507.14241", "authors": ["Rithesh Murthy", "Ming Zhu", "Liangwei Yang", "Jielin Qiu", "Juntao Tan", "Shelby Heinecke", "Huan Wang", "Caiming Xiong", "Silvio Savarese"], "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecdPromptomatix\uff0c\u4e00\u4e2a\u81ea\u52a8\u63d0\u793a\u8bcd\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u8f6c\u5316\u4e3a\u9ad8\u8d28\u91cf\u63d0\u793a\u8bcd\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u5c3d\u7ba1\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u7684\u63d0\u793a\u5de5\u7a0b\u8fc7\u7a0b\u4ecd\u662f\u624b\u52a8\u7684\u3001\u4e0d\u4e00\u81f4\u7684\uff0c\u4e14\u975e\u4e13\u4e1a\u4eba\u58eb\u96be\u4ee5\u64cd\u4f5c\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86Promptomatix\uff0c\u4e00\u4e2a\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u6846\u67b6\u3002\u5b83\u80fd\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u8f6c\u6362\u4e3a\u9ad8\u8d28\u91cf\u63d0\u793a\u8bcd\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u6216\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002Promptomatix\u652f\u6301\u8f7b\u91cf\u7ea7\u5143\u63d0\u793a\u4f18\u5316\u5668\u548c\u57fa\u4e8eDSPy\u7684\u7f16\u8bd1\u5668\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u3002\u8be5\u7cfb\u7edf\u5206\u6790\u7528\u6237\u610f\u56fe\uff0c\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u9009\u62e9\u63d0\u793a\u7b56\u7565\uff0c\u5e76\u4f7f\u7528\u6210\u672c\u611f\u77e5\u76ee\u6807\u4f18\u5316\u63d0\u793a\u8bcd\u3002", "result": "\u57285\u7c7b\u4efb\u52a1\u4e2d\u8fdb\u884c\u8bc4\u4f30\u540e\uff0cPromptomatix\u4e0e\u73b0\u6709\u5e93\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u7ade\u4e89\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u5b83\u8fd8\u51cf\u5c11\u4e86\u63d0\u793a\u8bcd\u957f\u5ea6\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "Promptomatix\u4f7f\u63d0\u793a\u4f18\u5316\u53d8\u5f97\u53ef\u6269\u5c55\u548c\u9ad8\u6548\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u63d0\u793a\u5de5\u7a0b\u7684\u75db\u70b9\uff0c\u4f7f\u975e\u4e13\u5bb6\u4e5f\u80fd\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u63d0\u793a\u8bcd\uff0c\u4ece\u800c\u63d0\u5347LLM\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2507.14209", "pdf": "https://arxiv.org/pdf/2507.14209", "abs": "https://arxiv.org/abs/2507.14209", "authors": ["Rute C. Sofia", "Hao Shen", "Yuanting Liu", "Severin Kacianka", "Holger Pfeifer"], "title": "White paper: Towards Human-centric and Sustainable 6G Services -- the fortiss Research Perspective", "categories": ["cs.NI", "cs.ET"], "comment": null, "summary": "As a leading research institute in software-intensive systems, fortiss is\nactively shaping the vision of Sixth Generation Mobile Communication (6G). Our\nmission is to ensure that 6G technologies go beyond technical advancements and\nare aligned with societal needs. fortiss plays a key role in 6G initiatives\nworldwide, including contributions to standardization bodies and collaborative\nResearch and Development programs. We focus on software-defined, AI-enabled,\nand sustainable communication services that prioritize human values and\nlong-term impact. 6G will redefine digital connectivity through cognitive\nintelligence, decentralized orchestration, and sustainability-oriented\narchitectures. As expectations rise for ultra-reliable low-latency\ncommunication (URLLC) and personalized digital services, 6G must outperform\nprior generations. It will rely on AI-native networking, Edge-Cloud resource\norchestration, and energy-aware data frameworks, ensuring both technical\nperformance and societal relevance. This white paper presents the fortiss\nvision for a human-centric, sustainable, and AI-integrated 6G network. It\noutlines key research domains such as semantic communication, green\norchestration, and distributed AI, all linked to societal and technological\nchallenges. The white paper is aimed at researchers, industry experts,\npolicymakers, and developers. It articulates the strategic direction and\ncontributions of fortiss to 6G, emphasizing responsible innovation and\ninterdisciplinary collaboration toward a meaningful 2030 vision.", "AI": {"tldr": "Fortiss\u53d1\u5e03\u767d\u76ae\u4e66\uff0c\u63d0\u51fa\u4ee5\u4eba\u4e3a\u672c\u3001\u53ef\u6301\u7eed\u3001AI\u96c6\u6210\u76846G\u7f51\u7edc\u613f\u666f\uff0c\u65e8\u5728\u901a\u8fc7AI\u539f\u751f\u7f51\u7edc\u3001\u8fb9\u7f18\u4e91\u7f16\u6392\u7b49\u6280\u672f\uff0c\u786e\u4fdd6G\u4e0d\u4ec5\u6280\u672f\u5148\u8fdb\u4e14\u7b26\u5408\u793e\u4f1a\u9700\u6c42\u3002", "motivation": "\u786e\u4fdd6G\u6280\u672f\u8d85\u8d8a\u7eaf\u7cb9\u7684\u6280\u672f\u8fdb\u6b65\uff0c\u4e0e\u793e\u4f1a\u9700\u6c42\u4fdd\u6301\u4e00\u81f4\uff1b\u5e94\u5bf9\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\uff08URLLC\uff09\u548c\u4e2a\u6027\u5316\u6570\u5b57\u670d\u52a1\u7684\u65e5\u76ca\u589e\u957f\u7684\u671f\u671b\uff0c\u4fc3\u4f7f6G\u6027\u80fd\u8d85\u8d8a\u524d\u4ee3\u3002", "method": "\u901a\u8fc7\u53d1\u5e03\u767d\u76ae\u4e66\uff0c\u9610\u8ff0fortiss\u5173\u4e8e\u4ee5\u4eba\u4e3a\u672c\u3001\u53ef\u6301\u7eed\u548cAI\u96c6\u62106G\u7f51\u7edc\u7684\u613f\u666f\uff0c\u5e76\u660e\u786e\u8bed\u4e49\u901a\u4fe1\u3001\u7eff\u8272\u7f16\u6392\u548c\u5206\u5e03\u5f0fAI\u7b49\u5173\u952e\u7814\u7a76\u9886\u57df\uff0c\u5c06\u6280\u672f\u6311\u6218\u4e0e\u793e\u4f1a\u9700\u6c42\u76f8\u7ed3\u5408\u3002", "result": "\u767d\u76ae\u4e66\u6210\u529f\u9610\u660e\u4e86fortiss\u57286G\u9886\u57df\u7684\u6218\u7565\u65b9\u5411\u548c\u5177\u4f53\u8d21\u732e\uff0c\u7279\u522b\u5f3a\u8c03\u4e86\u8d1f\u8d23\u4efb\u7684\u521b\u65b0\u548c\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5b9e\u73b02030\u5e74\u7684\u6709\u610f\u4e49\u613f\u666f\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "6G\u7684\u672a\u6765\u53d1\u5c55\u5e94\u4ee5\u4eba\u4e3a\u672c\uff0c\u6ce8\u91cd\u53ef\u6301\u7eed\u6027\u548cAI\u6574\u5408\uff0c\u5e76\u901a\u8fc7\u8d1f\u8d23\u4efb\u7684\u521b\u65b0\u548c\u5e7f\u6cdb\u7684\u8de8\u5b66\u79d1\u5408\u4f5c\uff0c\u5171\u540c\u5b9e\u73b0\u6280\u672f\u4e0e\u793e\u4f1a\u9700\u6c42\u7684\u5e73\u8861\u53d1\u5c55\u3002fortiss\u5c06\u79ef\u6781\u63a8\u52a8\u6b64\u613f\u666f\u7684\u5b9e\u73b0\u3002"}}
{"id": "2507.14335", "pdf": "https://arxiv.org/pdf/2507.14335", "abs": "https://arxiv.org/abs/2507.14335", "authors": ["Nicolas Wischermann", "Claudio Mayrink Verdun", "Gabriel Poesia", "Francesco Noseda"], "title": "ProofCompass: Enhancing Specialized Provers with LLM Guidance", "categories": ["cs.AI"], "comment": "19 pages, 7 figures. Accepted at the 2nd AI for MATH Workshop at the\n  42nd International Conference on Machine Learning (ICML 2025)", "summary": "Language models have become increasingly powerful tools for formal\nmathematical reasoning. However, most existing approaches rely exclusively on\neither large general-purpose models or smaller specialized models, each with\ndistinct limitations, while training specialized large models still requires\nsignificant computational resources. This paper introduces ProofCompass, a\nnovel hybrid methodology that achieves remarkable computational efficiency by\nstrategically guiding existing specialized prover methods, such as\nDeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without\nrequiring additional model training. The LLM provides natural language proof\nstrategies and analyzes failed attempts to select intermediate lemmas, enabling\neffective problem decomposition. On the miniF2F benchmark, ProofCompass\ndemonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\\%\n\\rightarrow 55.3\\%$) while using 25x fewer attempts ($3200 \\rightarrow 128$).\nOur synergistic approach paves the way for simultaneously improving\ncomputational efficiency and accuracy in formal theorem proving.", "AI": {"tldr": "ProofCompass\u662f\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u5229\u7528LLM\u6307\u5bfc\u73b0\u6709\u5b9a\u7406\u8bc1\u660e\u5668\uff08\u5982DSP-v1.5\uff09\uff0c\u5728\u4e0d\u9700\u989d\u5916\u8bad\u7ec3\u5927\u578b\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u5f62\u5f0f\u5316\u6570\u5b66\u63a8\u7406\u7684\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u8981\u4e48\u4f9d\u8d56\u901a\u7528\u5927\u578b\u6a21\u578b\uff08\u6709\u5c40\u9650\u6027\uff09\uff0c\u8981\u4e48\u4f9d\u8d56\u5c0f\u578b\u4e13\u7528\u6a21\u578b\uff08\u6709\u5c40\u9650\u6027\uff09\uff0c\u4e14\u8bad\u7ec3\u5927\u578b\u4e13\u7528\u6a21\u578b\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "method": "\u5f15\u5165ProofCompass\uff0c\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\u3002\u5b83\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u7b56\u7565\u6027\u5730\u6307\u5bfc\u73b0\u6709\u4e13\u7528\u8bc1\u660e\u5668\uff08\u5982DeepSeek-Prover-v1.5-RL\uff09\uff0cLLM\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\u7b56\u7565\uff0c\u5e76\u5206\u6790\u5931\u8d25\u5c1d\u8bd5\u4ee5\u9009\u62e9\u4e2d\u95f4\u5f15\u7406\uff0c\u4ece\u800c\u5b9e\u73b0\u95ee\u9898\u5206\u89e3\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728miniF2F\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProofCompass\u663e\u8457\u63d0\u9ad8\u4e86\u8d44\u6e90\u6548\u7387\uff1a\u6027\u80fd\u8d85\u8d8aDSP-v1.5\uff08\u4ece54.9%\u63d0\u5347\u81f355.3%\uff09\uff0c\u540c\u65f6\u5c06\u5c1d\u8bd5\u6b21\u6570\u51cf\u5c11\u4e8625\u500d\uff08\u4ece3200\u6b21\u964d\u81f3128\u6b21\uff09\u3002", "conclusion": "\u8be5\u534f\u540c\u65b9\u6cd5\u4e3a\u540c\u65f6\u63d0\u9ad8\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.14368", "pdf": "https://arxiv.org/pdf/2507.14368", "abs": "https://arxiv.org/abs/2507.14368", "authors": ["Praneeth Namburi", "Roger Pallar\u00e8s-L\u00f3pez", "Jessica Rosendorf", "Duarte Folgado", "Brian W. Anthony"], "title": "DUSTrack: Semi-automated point tracking in ultrasound videos", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Ultrasound technology enables safe, non-invasive imaging of dynamic tissue\nbehavior, making it a valuable tool in medicine, biomechanics, and sports\nscience. However, accurately tracking tissue motion in B-mode ultrasound\nremains challenging due to speckle noise, low edge contrast, and out-of-plane\nmovement. These challenges complicate the task of tracking anatomical landmarks\nover time, which is essential for quantifying tissue dynamics in many clinical\nand research applications. This manuscript introduces DUSTrack (Deep learning\nand optical flow-based toolkit for UltraSound Tracking), a semi-automated\nframework for tracking arbitrary points in B-mode ultrasound videos. We combine\ndeep learning with optical flow to deliver high-quality and robust tracking\nacross diverse anatomical structures and motion patterns. The toolkit includes\na graphical user interface that streamlines the generation of high-quality\ntraining data and supports iterative model refinement. It also implements a\nnovel optical-flow-based filtering technique that reduces high-frequency\nframe-to-frame noise while preserving rapid tissue motion. DUSTrack\ndemonstrates superior accuracy compared to contemporary zero-shot point\ntrackers and performs on par with specialized methods, establishing its\npotential as a general and foundational tool for clinical and biomechanical\nresearch. We demonstrate DUSTrack's versatility through three use cases:\ncardiac wall motion tracking in echocardiograms, muscle deformation analysis\nduring reaching tasks, and fascicle tracking during ankle plantarflexion. As an\nopen-source solution, DUSTrack offers a powerful, flexible framework for point\ntracking to quantify tissue motion from ultrasound videos. DUSTrack is\navailable at https://github.com/praneethnamburi/DUSTrack.", "AI": {"tldr": "DUSTrack\u662f\u4e00\u4e2a\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u5149\u6d41\u7684\u534a\u81ea\u52a8\u5316\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u89e3\u51b3B\u578b\u8d85\u58f0\u89c6\u9891\u4e2d\u7ec4\u7ec7\u8fd0\u52a8\u8ffd\u8e2a\u7684\u96be\u9898\uff0c\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u7684\u8ffd\u8e2a\u80fd\u529b\uff0c\u5e76\u5df2\u5f00\u6e90\u3002", "motivation": "B\u578b\u8d85\u58f0\u56fe\u50cf\u5b58\u5728\u6563\u6591\u566a\u58f0\u3001\u4f4e\u8fb9\u7f18\u5bf9\u6bd4\u5ea6\u548c\u5e73\u9762\u5916\u8fd0\u52a8\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u7ec4\u7ec7\u8fd0\u52a8\u7684\u7cbe\u786e\u8ffd\u8e2a\u9762\u4e34\u6311\u6218\uff0c\u800c\u7cbe\u786e\u8ffd\u8e2a\u5bf9\u4e8e\u91cf\u5316\u7ec4\u7ec7\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86DUSTrack\uff08\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u548c\u5149\u6d41\u7684\u8d85\u58f0\u8ffd\u8e2a\u5de5\u5177\u5305\uff09\uff0c\u4e00\u4e2a\u7528\u4e8e\u8ffd\u8e2aB\u578b\u8d85\u58f0\u89c6\u9891\u4e2d\u4efb\u610f\u70b9\u7684\u534a\u81ea\u52a8\u5316\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e0e\u5149\u6d41\u6280\u672f\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u56fe\u5f62\u7528\u6237\u754c\u9762\u4ee5\u7b80\u5316\u8bad\u7ec3\u6570\u636e\u751f\u6210\u548c\u6a21\u578b\u8fed\u4ee3\u4f18\u5316\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5b9e\u73b0\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5149\u6d41\u6ee4\u6ce2\u6280\u672f\uff0c\u7528\u4e8e\u51cf\u5c11\u9ad8\u9891\u566a\u58f0\u3002", "result": "DUSTrack\u76f8\u6bd4\u73b0\u6709\u96f6\u6837\u672c\u70b9\u8ffd\u8e2a\u5668\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u7cbe\u5ea6\uff0c\u5e76\u4e0e\u4e13\u4e1a\u65b9\u6cd5\u6548\u679c\u76f8\u5f53\u3002\u8be5\u5de5\u5177\u5728\u5fc3\u810f\u58c1\u8fd0\u52a8\u8ffd\u8e2a\u3001\u808c\u8089\u53d8\u5f62\u5206\u6790\u548c\u808c\u675f\u8ffd\u8e2a\u7b49\u4e09\u4e2a\u7528\u4f8b\u4e2d\u5c55\u793a\u4e86\u5176\u901a\u7528\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "conclusion": "DUSTrack\u662f\u4e00\u4e2a\u5f3a\u5927\u3001\u7075\u6d3b\u7684\u5f00\u6e90\u70b9\u8ffd\u8e2a\u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u91cf\u5316\u8d85\u58f0\u89c6\u9891\u4e2d\u7684\u7ec4\u7ec7\u8fd0\u52a8\uff0c\u6709\u671b\u6210\u4e3a\u4e34\u5e8a\u548c\u751f\u7269\u529b\u5b66\u7814\u7a76\u7684\u57fa\u7840\u5de5\u5177\u3002"}}
{"id": "2507.14177", "pdf": "https://arxiv.org/pdf/2507.14177", "abs": "https://arxiv.org/abs/2507.14177", "authors": ["Changcun Huang"], "title": "Understanding Two-Layer Neural Networks with Smooth Activation Functions", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "68T07(Primary), 41A15(Secondary)", "I.2.6; G.1.2"], "comment": null, "summary": "This paper aims to understand the training solution, which is obtained by the\nback-propagation algorithm, of two-layer neural networks whose hidden layer is\ncomposed of the units with smooth activation functions, including the usual\nsigmoid type most commonly used before the advent of ReLUs. The mechanism\ncontains four main principles: construction of Taylor series expansions, strict\npartial order of knots, smooth-spline implementation and smooth-continuity\nrestriction. The universal approximation for arbitrary input dimensionality is\nproved and experimental verification is given, through which the mystery of\n``black box'' of the solution space is largely revealed. The new proofs\nemployed also enrich approximation theory.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u7406\u89e3\u5e76\u63ed\u793a\u4f7f\u7528\u53cd\u5411\u4f20\u64ad\u8bad\u7ec3\u7684\u53cc\u5c42\u795e\u7ecf\u7f51\u7edc\uff08\u542b\u5e73\u6ed1\u6fc0\u6d3b\u51fd\u6570\uff09\u7684\u89e3\u7a7a\u95f4\u673a\u5236\uff0c\u5e76\u8bc1\u660e\u5176\u901a\u7528\u903c\u8fd1\u6027\u3002", "motivation": "\u7406\u89e3\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u8bad\u7ec3\u7684\u53cc\u5c42\u795e\u7ecf\u7f51\u7edc\uff08\u4f7f\u7528Sigmoid\u7b49\u5e73\u6ed1\u6fc0\u6d3b\u51fd\u6570\uff09\u6240\u83b7\u5f97\u7684\u8bad\u7ec3\u89e3\uff0c\u5e76\u63ed\u793a\u5176\u89e3\u7a7a\u95f4\u7684\u201c\u9ed1\u7bb1\u201d\u4e4b\u8c1c\u3002", "method": "\u8be5\u7814\u7a76\u57fa\u4e8e\u56db\u4e2a\u6838\u5fc3\u539f\u5219\uff1a\u6784\u5efa\u6cf0\u52d2\u7ea7\u6570\u5c55\u5f00\u3001\u4e25\u683c\u7684\u8282\u70b9\u504f\u5e8f\u3001\u5e73\u6ed1\u6837\u6761\u5b9e\u73b0\u4ee5\u53ca\u5e73\u6ed1\u8fde\u7eed\u6027\u9650\u5236\u3002", "result": "\u7814\u7a76\u8bc1\u660e\u4e86\u4efb\u610f\u8f93\u5165\u7ef4\u5ea6\u7684\u901a\u7528\u903c\u8fd1\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u63ed\u793a\u4e86\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u7684\u201c\u9ed1\u7bb1\u201d\u673a\u5236\u3002", "conclusion": "\u672c\u7814\u7a76\u4e0d\u4ec5\u63ed\u793a\u4e86\u53cc\u5c42\u795e\u7ecf\u7f51\u7edc\u89e3\u7a7a\u95f4\u7684\u5965\u79d8\uff0c\u6240\u91c7\u7528\u7684\u65b0\u8bc1\u660e\u65b9\u6cd5\u4e5f\u4e30\u5bcc\u4e86\u903c\u8fd1\u7406\u8bba\u3002"}}
{"id": "2507.14298", "pdf": "https://arxiv.org/pdf/2507.14298", "abs": "https://arxiv.org/abs/2507.14298", "authors": ["Wan-Cyuan Fan", "Yen-Chun Chen", "Mengchen Liu", "Alexander Jacobson", "Lu Yuan", "Leonid Sigal"], "title": "In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2407.14506", "summary": "Recent methods for customizing Large Vision Language Models (LVLMs) for\ndomain-specific tasks have shown promising results in scientific chart\ncomprehension. However, existing approaches face two major limitations: First,\nthey rely on paired data from only a few chart types, limiting generalization\nto wide range of chart types. Secondly, they lack targeted pre-training for\nchart-data alignment, which hampers the model's understanding of underlying\ndata. In this paper, we introduce ChartScope, an LVLM optimized for in-depth\nchart comprehension across diverse chart types. We propose an efficient data\ngeneration pipeline that synthesizes paired data for a wide range of chart\ntypes, along with a novel Dual-Path training strategy that enabling the model\nto succinctly capture essential data details while preserving robust reasoning\ncapabilities by incorporating reasoning over the underlying data. Lastly, we\nestablish ChartDQA, a new benchmark for evaluating not only question-answering\nat different levels but also underlying data understanding. Experimental\nresults demonstrate that ChartScope significantly enhances comprehension on a\nwide range of chart types. The code and data are available at\nhttps://davidhalladay.github.io/chartscope_demo.", "AI": {"tldr": "ChartScope\u662f\u4e00\u4e2a\u65b0\u7684\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6570\u636e\u751f\u6210\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u5bf9\u591a\u79cd\u56fe\u8868\u7684\u6df1\u5ea6\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5b9a\u5236LVLM\u8fdb\u884c\u56fe\u8868\u7406\u89e3\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a\u4e00\u662f\u4f9d\u8d56\u5c11\u6570\u56fe\u8868\u7c7b\u578b\u7684\u914d\u5bf9\u6570\u636e\uff0c\u6cdb\u5316\u6027\u53d7\u9650\uff1b\u4e8c\u662f\u7f3a\u4e4f\u9488\u5bf9\u56fe\u8868-\u6570\u636e\u5bf9\u9f50\u7684\u9884\u8bad\u7ec3\uff0c\u5f71\u54cd\u6a21\u578b\u5bf9\u5e95\u5c42\u6570\u636e\u7684\u7406\u89e3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86ChartScope\uff0c\u4e00\u4e2a\u4f18\u5316\u7528\u4e8e\u6df1\u5ea6\u56fe\u8868\u7406\u89e3\u7684LVLM\u3002\u6838\u5fc3\u65b9\u6cd5\u5305\u62ec\uff1a1. \u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5408\u6210\u5927\u91cf\u8986\u76d6\u591a\u79cd\u56fe\u8868\u7c7b\u578b\u7684\u914d\u5bf9\u6570\u636e\u30022. \u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u8def\u5f84\u8bad\u7ec3\u7b56\u7565\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u7cbe\u786e\u6355\u6349\u5173\u952e\u6570\u636e\u7ec6\u8282\uff0c\u540c\u65f6\u901a\u8fc7\u5bf9\u5e95\u5c42\u6570\u636e\u8fdb\u884c\u63a8\u7406\u6765\u4fdd\u6301\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u30023. \u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6ChartDQA\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u7ea7\u522b\u4e0a\u7684\u95ee\u7b54\u80fd\u529b\u4ee5\u53ca\u5bf9\u5e95\u5c42\u6570\u636e\u7684\u7406\u89e3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cChartScope\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u5404\u79cd\u56fe\u8868\u7684\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "ChartScope\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u751f\u6210\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u8868\u7c7b\u578b\u6cdb\u5316\u548c\u5e95\u5c42\u6570\u636e\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u589e\u5f3a\u4e86LVLMs\u5bf9\u590d\u6742\u56fe\u8868\u7684\u6df1\u5ea6\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2507.14211", "pdf": "https://arxiv.org/pdf/2507.14211", "abs": "https://arxiv.org/abs/2507.14211", "authors": ["Federico Mason", "Tommaso Zugno", "Matteo Drago", "Marco Giordani", "Mate Boban", "Michele Zorzi"], "title": "PRATA: A Framework to Enable Predictive QoS in Vehicular Networks via Artificial Intelligence", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Predictive Quality of Service (PQoS) makes it possible to anticipate QoS\nchanges, e.g., in wireless networks, and trigger appropriate countermeasures to\navoid performance degradation. Hence, PQoS is extremely useful for automotive\napplications such as teleoperated driving, which poses strict constraints in\nterms of latency and reliability. A promising tool for PQoS is given by\nReinforcement Learning (RL), a methodology that enables the design of\ndecision-making strategies for stochastic optimization. In this manuscript, we\npresent PRATA, a new simulation framework to enable PRedictive QoS based on AI\nfor Teleoperated driving Applications. PRATA consists of a modular pipeline\nthat includes (i) an end-to-end protocol stack to simulate the 5G Radio Access\nNetwork (RAN), (ii) a tool for generating automotive data, and (iii) an\nArtificial Intelligence (AI) unit to optimize PQoS decisions. To prove its\nutility, we use PRATA to design an RL unit, named RAN-AI, to optimize the\nsegmentation level of teleoperated driving data in the event of resource\nsaturation or channel degradation. Hence, we show that the RAN-AI entity\nefficiently balances the trade-off between QoS and Quality of Experience (QoE)\nthat characterize teleoperated driving applications, almost doubling the system\nperformance compared to baseline approaches. In addition, by varying the\nlearning settings of the RAN-AI entity, we investigate the impact of the state\nspace and the relative cost of acquiring network data that are necessary for\nthe implementation of RL.", "AI": {"tldr": "\u63d0\u51faPRATA\u6a21\u62df\u6846\u67b6\u53caRAN-AI\u5f3a\u5316\u5b66\u4e60\u5355\u5143\uff0c\u5b9e\u73b0\u81ea\u52a8\u9a7e\u9a76\u4e2d\u57fa\u4e8eAI\u7684\u9884\u6d4b\u6027QoS\u4f18\u5316\uff0c\u6027\u80fd\u8f83\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u8fd1\u4e00\u500d\u3002", "motivation": "\u8fdc\u7a0b\u64cd\u63a7\u9a7e\u9a76\u7b49\u6c7d\u8f66\u5e94\u7528\u5bf9\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u6709\u4e25\u683c\u8981\u6c42\uff0c\u9700\u8981\u9884\u6d4b\u6027\u670d\u52a1\u8d28\u91cf\uff08PQoS\uff09\u6765\u63d0\u524d\u5e94\u5bf9QoS\u53d8\u5316\u5e76\u91c7\u53d6\u63aa\u65bd\u907f\u514d\u6027\u80fd\u4e0b\u964d\u3002\u5f3a\u5316\u5b66\u4e60\u662f\u5b9e\u73b0PQoS\u7684\u6709\u6548\u5de5\u5177\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7684\u6a21\u62df\u6846\u67b6\u6765\u8bbe\u8ba1\u548c\u9a8c\u8bc1\u5176\u5728\u8fdc\u7a0b\u64cd\u63a7\u9a7e\u9a76\u5e94\u7528\u4e2d\u7684PQoS\u51b3\u7b56\u3002", "method": "\u7814\u7a76\u63d0\u51faPRATA\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u4e8eAI\u7684\u8fdc\u7a0b\u64cd\u63a7\u9a7e\u9a76\u5e94\u7528\u9884\u6d4b\u6027QoS\u3002PRATA\u5305\u542b\u6a21\u5757\u5316\u7ba1\u9053\uff0c\u96c6\u6210\uff1a1) \u6a21\u62df5G\u65e0\u7ebf\u63a5\u5165\u7f51\u7684\u7aef\u5230\u7aef\u534f\u8bae\u6808\uff1b2) \u6c7d\u8f66\u6570\u636e\u751f\u6210\u5de5\u5177\uff1b3) \u4f18\u5316PQoS\u51b3\u7b56\u7684AI\u5355\u5143\u3002\u5229\u7528PRATA\u8bbe\u8ba1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u540d\u4e3aRAN-AI\u7684\u5f3a\u5316\u5b66\u4e60\u5355\u5143\uff0c\u65e8\u5728\u4f18\u5316\u8d44\u6e90\u9971\u548c\u6216\u4fe1\u9053\u9000\u5316\u65f6\u8fdc\u7a0b\u64cd\u63a7\u9a7e\u9a76\u6570\u636e\u7684\u5206\u6bb5\u7ea7\u522b\u3002", "result": "RAN-AI\u5b9e\u4f53\u80fd\u6709\u6548\u5e73\u8861\u8fdc\u7a0b\u64cd\u63a7\u9a7e\u9a76\u5e94\u7528\u4e2d\u7684QoS\u4e0e\u4f53\u9a8c\u8d28\u91cf\uff08QoE\uff09\u4e4b\u95f4\u7684\u6743\u8861\u3002\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7cfb\u7edf\u6027\u80fd\u51e0\u4e4e\u7ffb\u500d\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u6539\u53d8RAN-AI\u7684\u5b66\u4e60\u8bbe\u7f6e\uff0c\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u72b6\u6001\u7a7a\u95f4\u4ee5\u53ca\u83b7\u53d6\u5b9e\u65bd\u5f3a\u5316\u5b66\u4e60\u6240\u9700\u7f51\u7edc\u6570\u636e\u7684\u76f8\u5bf9\u6210\u672c\u7684\u5f71\u54cd\u3002", "conclusion": "PRATA\u6846\u67b6\u548cRAN-AI\u5f3a\u5316\u5b66\u4e60\u5355\u5143\u4e3a\u8fdc\u7a0b\u64cd\u63a7\u9a7e\u9a76\u4e2d\u7684\u9884\u6d4b\u6027QoS\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u8be5\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5173\u4e8e\u6570\u636e\u6210\u672c\u548c\u72b6\u6001\u7a7a\u95f4\u5f71\u54cd\u7684\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2507.14393", "pdf": "https://arxiv.org/pdf/2507.14393", "abs": "https://arxiv.org/abs/2507.14393", "authors": ["Humza Sami", "Mubashir ul Islam", "Pierre-Emmanuel Gaillardon", "Valerio Tenace"], "title": "Adaptive Multi-Agent Reasoning via Automated Workflow Generation", "categories": ["cs.AI"], "comment": null, "summary": "The rise of Large Reasoning Models (LRMs) promises a significant leap forward\nin language model capabilities, aiming to tackle increasingly sophisticated\ntasks with unprecedented efficiency and accuracy. However, despite their\nimpressive performance, recent studies have highlighted how current reasoning\nmodels frequently fail to generalize to novel, unseen problems, often resorting\nto memorized solutions rather than genuine inferential reasoning. Such behavior\nunderscores a critical limitation in modern LRMs, i.e., their tendency toward\noverfitting, which in turn results in poor generalization in problem-solving\ncapabilities.\n  In this paper, we introduce Nexus Architect, an enhanced iteration of our\nmulti-agent system framework, Nexus, equipped with a novel automated workflow\nsynthesis mechanism. Given a user's prompt and a small set of representative\nexamples, the Architect autonomously generates a tailored reasoning workflow by\nselecting suitable strategies, tool integrations, and adversarial techniques\nfor a specific problem class. Furthermore, the Architect includes an iterative\nprompt refinement mechanism that fine-tunes agents' system prompts to maximize\nperformance and improve the generalization capabilities of the system.\n  We empirically evaluate Nexus Architect by employing an off-the-shelf,\nnon-reasoning model on a custom dataset of challenging logical questions and\ncompare its performance against state-of-the-art LRMs. Results show that Nexus\nArchitect consistently outperforms existing solutions, achieving up to a 66%\nincrease in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\\times$ against\nClaude Sonnet 4 and DeepSeek-R1, and over 3$\\times$ w.r.t. Llama 4 Scout.", "AI": {"tldr": "\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86Nexus Architect\uff0c\u4e00\u4e2a\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u5408\u6210\u548c\u8fed\u4ee3\u63d0\u793a\u8bcd\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u672a\u89c1\u95ee\u9898\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u903b\u8f91\u95ee\u9898\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684LRMs\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u6027\u80fd\u51fa\u8272\uff0c\u4f46\u7814\u7a76\u8868\u660e\u5b83\u4eec\u5728\u5904\u7406\u65b0\u9896\u3001\u672a\u89c1\u95ee\u9898\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5e38\u4f9d\u8d56\u8bb0\u5fc6\u800c\u975e\u771f\u6b63\u7684\u63a8\u7406\uff0c\u8fd9\u66b4\u9732\u51fa\u5176\u8fc7\u62df\u5408\u7684\u503e\u5411\u548c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u5dee\u7684\u5c40\u9650\u6027\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86Nexus Architect\uff0c\u4e00\u4e2aNexus\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\u7684\u589e\u5f3a\u7248\u3002\u5b83\u914d\u5907\u4e86\u65b0\u9896\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u5408\u6210\u673a\u5236\uff0c\u80fd\u6839\u636e\u7528\u6237\u63d0\u793a\u548c\u5c11\u91cf\u793a\u4f8b\uff0c\u81ea\u4e3b\u751f\u6210\u5b9a\u5236\u5316\u7684\u63a8\u7406\u5de5\u4f5c\u6d41\uff08\u5305\u62ec\u9009\u62e9\u7b56\u7565\u3001\u5de5\u5177\u96c6\u6210\u548c\u5bf9\u6297\u6280\u672f\uff09\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5305\u542b\u4e00\u4e2a\u8fed\u4ee3\u63d0\u793a\u8bcd\u4f18\u5316\u673a\u5236\uff0c\u4ee5\u6700\u5927\u5316\u6027\u80fd\u5e76\u63d0\u9ad8\u7cfb\u7edf\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5728\u4e00\u4e2a\u5b9a\u5236\u7684\u6311\u6218\u6027\u903b\u8f91\u95ee\u9898\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u4e00\u4e2a\u73b0\u6210\u7684\u975e\u63a8\u7406\u6a21\u578b\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\uff0cNexus Architect\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002\u76f8\u6bd4Gemini 2.5 Flash Preview\uff0c\u901a\u8fc7\u7387\u63d0\u9ad8\u4e8666%\uff1b\u76f8\u6bd4Claude Sonnet 4\u548cDeepSeek-R1\uff0c\u63d0\u9ad8\u4e86\u8fd12.5\u500d\uff1b\u76f8\u6bd4Llama 4 Scout\uff0c\u63d0\u9ad8\u4e863\u500d\u4ee5\u4e0a\u3002", "conclusion": "Nexus Architect\u901a\u8fc7\u5176\u521b\u65b0\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u548c\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u5408\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u903b\u8f91\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5c55\u73b0\u51fa\u8d85\u8d8a\u5f53\u524d\u4e3b\u6d41LRMs\u7684\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2507.14426", "pdf": "https://arxiv.org/pdf/2507.14426", "abs": "https://arxiv.org/abs/2507.14426", "authors": ["Zhou Chen", "Joe Lin", "Sathyanarayanan N. Aakur"], "title": "CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding", "categories": ["cs.CV"], "comment": "Accepted to NeSy 2025", "summary": "We introduce CRAFT, a neuro-symbolic framework for interpretable affordance\ngrounding, which identifies the objects in a scene that enable a given action\n(e.g., \"cut\"). CRAFT integrates structured commonsense priors from ConceptNet\nand language models with visual evidence from CLIP, using an energy-based\nreasoning loop to refine predictions iteratively. This process yields\ntransparent, goal-driven decisions to ground symbolic and perceptual\nstructures. Experiments in multi-object, label-free settings demonstrate that\nCRAFT enhances accuracy while improving interpretability, providing a step\ntoward robust and trustworthy scene understanding.", "AI": {"tldr": "CRAFT\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5e38\u8bc6\u3001\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u80fd\u91cf\u63a8\u7406\u8fed\u4ee3\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u7684\u884c\u52a8\u80fd\u529b\u611f\u77e5\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u8bc6\u522b\u573a\u666f\u4e2d\u80fd\u591f\u652f\u6301\u7279\u5b9a\u52a8\u4f5c\uff08\u5982\u201c\u5207\u5272\u201d\uff09\u7684\u7269\u4f53\uff0c\u5e76\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u3001\u76ee\u6807\u9a71\u52a8\u7684\u7b26\u53f7\u4e0e\u611f\u77e5\u7ed3\u6784\u5173\u8054\uff08affordance grounding\uff09\u3002", "method": "CRAFT\u6574\u5408\u4e86\u6765\u81eaConceptNet\u7684\u7ed3\u6784\u5316\u5e38\u8bc6\u5148\u9a8c\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u53ca\u6765\u81eaCLIP\u7684\u89c6\u89c9\u8bc1\u636e\u3002\u5b83\u91c7\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u63a8\u7406\u5faa\u73af\u6765\u8fed\u4ee3\u5730\u7cbe\u70bc\u9884\u6d4b\u3002", "result": "\u5728\u591a\u5bf9\u8c61\u3001\u65e0\u6807\u7b7e\u7684\u8bbe\u7f6e\u4e2d\uff0cCRAFT\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5e76\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "CRAFT\u4e3a\u5b9e\u73b0\u9c81\u68d2\u548c\u53ef\u4fe1\u8d56\u7684\u573a\u666f\u7406\u89e3\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2507.14178", "pdf": "https://arxiv.org/pdf/2507.14178", "abs": "https://arxiv.org/abs/2507.14178", "authors": ["Yuhang Liu", "Yuefei Wu", "Bin Shi", "Bo Dong"], "title": "Feature Bank Enhancement for Distance-based Out-of-Distribution Detection", "categories": ["cs.LG", "cs.AI"], "comment": "8 pages, 5 figures", "summary": "Out-of-distribution (OOD) detection is critical to ensuring the reliability\nof deep learning applications and has attracted significant attention in recent\nyears. A rich body of literature has emerged to develop efficient score\nfunctions that assign high scores to in-distribution (ID) samples and low\nscores to OOD samples, thereby helping distinguish OOD samples. Among these\nmethods, distance-based score functions are widely used because of their\nefficiency and ease of use. However, deep learning often leads to a biased\ndistribution of data features, and extreme features are inevitable. These\nextreme features make the distance-based methods tend to assign too low scores\nto ID samples. This limits the OOD detection capabilities of such methods. To\naddress this issue, we propose a simple yet effective method, Feature Bank\nEnhancement (FBE), that uses statistical characteristics from dataset to\nidentify and constrain extreme features to the separation boundaries, therapy\nmaking the distance between samples inside and outside the distribution\nfarther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10\nrespectively, and the results show that our method achieves state-of-the-art\nperformance on both benchmark. Additionally, theoretical analysis and\nsupplementary experiments are conducted to provide more insights into our\nmethod.", "AI": {"tldr": "\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60\u4e2d\u8ddd\u79bb\u57faOOD\u68c0\u6d4b\u65b9\u6cd5\u56e0\u6781\u7aef\u7279\u5f81\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faFBE\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u8ba1\u7279\u6027\u8bc6\u522b\u5e76\u7ea6\u675f\u6781\u7aef\u7279\u5f81\uff0c\u663e\u8457\u62c9\u5927ID\u548cOOD\u6837\u672c\u8ddd\u79bb\uff0c\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8ddd\u79bb\u57faOOD\u68c0\u6d4b\u65b9\u6cd5\u6613\u53d7\u6df1\u5ea6\u5b66\u4e60\u4e2d\u504f\u7f6e\u6570\u636e\u7279\u5f81\u548c\u6781\u7aef\u7279\u5f81\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u5176\u5c06ID\u6837\u672c\u5206\u914d\u8fc7\u4f4e\u7684\u5206\u6570\uff0c\u4ece\u800c\u9650\u5236\u4e86OOD\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7279\u5f81\u5e93\u589e\u5f3a\uff08Feature Bank Enhancement, FBE\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u6570\u636e\u96c6\u7684\u7edf\u8ba1\u7279\u6027\u8bc6\u522b\u5e76\u7ea6\u675f\u6781\u7aef\u7279\u5f81\u81f3\u5206\u79bb\u8fb9\u754c\uff0c\u4ece\u800c\u6709\u6548\u62c9\u5927\u5206\u5e03\u5185\u5916\u6837\u672c\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002", "result": "\u5728ImageNet-1k\u548cCIFAR-10\u5927\u578b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eFBE\u65b9\u6cd5\u5728\u8fd9\u4e24\u4e2a\u57fa\u51c6\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "FBE\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5904\u7406\u6781\u7aef\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddd\u79bb\u57fa\u65b9\u6cd5\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u53d6\u5f97\u4e86SOTA\u8868\u73b0\uff0c\u5177\u6709\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u652f\u6301\u3002"}}
{"id": "2507.14304", "pdf": "https://arxiv.org/pdf/2507.14304", "abs": "https://arxiv.org/abs/2507.14304", "authors": ["Rakesh Paul", "Anusha Kamath", "Kanishk Singla", "Raviraj Joshi", "Utkarsh Vaidya", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "title": "Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual large language models (LLMs) often demonstrate a performance gap\nbetween English and non-English languages, particularly in low-resource\nsettings. Aligning these models to low-resource languages is essential yet\nchallenging due to limited high-quality data. While English alignment datasets\nare readily available, curating equivalent data in other languages is expensive\nand time-consuming. A common workaround is to translate existing English\nalignment data; however, standard translation techniques often fail to preserve\ncritical elements such as code, mathematical expressions, and structured\nformats like JSON. In this work, we investigate LLM-based selective\ntranslation, a technique that selectively translates only the translatable\nparts of a text while preserving non-translatable content and sentence\nstructure. We conduct a systematic study to explore key questions around this\napproach, including its effectiveness compared to vanilla translation, the\nimportance of filtering noisy outputs, and the benefits of mixing translated\nsamples with original English data during alignment. Our experiments focus on\nthe low-resource Indic language Hindi and compare translations generated by\nGoogle Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the\npromise of selective translation as a practical and effective method for\nimproving multilingual alignment in LLMs.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u9009\u62e9\u6027\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u7528\u4e8e\u514b\u670d\u591a\u8bed\u8a00LLMs\u5728\u4f4e\u8d44\u6e90\u975e\u82f1\u8bed\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u76f8\u6bd4\u4f20\u7edf\u7ffb\u8bd1\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u3002", "motivation": "\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u82f1\u8bed\u548c\u975e\u82f1\u8bed\u8bed\u8a00\u4e4b\u95f4\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u5883\u4e0b\u3002\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u8fdb\u884c\u6a21\u578b\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u662f\u4e3b\u8981\u6311\u6218\u3002\u5c3d\u7ba1\u53ef\u4ee5\u7ffb\u8bd1\u73b0\u6709\u82f1\u8bed\u5bf9\u9f50\u6570\u636e\u96c6\uff0c\u4f46\u6807\u51c6\u7ffb\u8bd1\u65b9\u6cd5\u96be\u4ee5\u4fdd\u7559\u4ee3\u7801\u3001\u6570\u5b66\u8868\u8fbe\u5f0f\u548cJSON\u7b49\u5173\u952e\u7ed3\u6784\u5316\u5185\u5bb9\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u5e76\u6df1\u5165\u63a2\u8ba8\u4e86\u57fa\u4e8eLLM\u7684\u9009\u62e9\u6027\u7ffb\u8bd1\u6280\u672f\uff0c\u8be5\u6280\u672f\u4ec5\u7ffb\u8bd1\u6587\u672c\u4e2d\u53ef\u7ffb\u8bd1\u7684\u90e8\u5206\uff0c\u540c\u65f6\u4fdd\u7559\u4e0d\u53ef\u7ffb\u8bd1\u7684\u5185\u5bb9\u548c\u53e5\u5b50\u7ed3\u6784\u3002\u901a\u8fc7\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u9009\u62e9\u6027\u7ffb\u8bd1\u4e0e\u666e\u901a\uff08\u9999\u8349\uff09\u7ffb\u8bd1\u7684\u6709\u6548\u6027\uff0c\u63a2\u8ba8\u4e86\u8fc7\u6ee4\u566a\u58f0\u8f93\u51fa\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u5728\u6a21\u578b\u5bf9\u9f50\u65f6\u6df7\u5408\u7ffb\u8bd1\u6837\u672c\u4e0e\u539f\u59cb\u82f1\u8bed\u6570\u636e\u7684\u597d\u5904\u3002\u5b9e\u9a8c\u4ee5\u4f4e\u8d44\u6e90\u5370\u5730\u8bed\u4e3a\u91cd\u70b9\uff0c\u5e76\u6bd4\u8f83\u4e86Google Cloud Translation (GCP) \u548c Llama-3.1-405B \u751f\u6210\u7684\u7ffb\u8bd1\u6548\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u9009\u62e9\u6027\u7ffb\u8bd1\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5728\u6539\u5584LLMs\u7684\u591a\u8bed\u8a00\u5bf9\u9f50\u65b9\u9762\u5177\u6709\u5e7f\u9614\u524d\u666f\u3002", "conclusion": "\u9009\u62e9\u6027\u7ffb\u8bd1\u662f\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684\u7b56\u7565\uff0c\u80fd\u591f\u89e3\u51b3\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u9f50\u4e2d\u9762\u4e34\u7684\u6570\u636e\u8d28\u91cf\u548c\u7ed3\u6784\u4fdd\u7559\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u5176\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14230", "pdf": "https://arxiv.org/pdf/2507.14230", "abs": "https://arxiv.org/abs/2507.14230", "authors": ["Fransiscus Asisi Bimo", "Maria Amparo Canaveras Galdon", "Chun-Kai Lai", "Ray-Guang Cheng", "Edwin K. P. Chong"], "title": "Intent-Based Network for RAN Management with Large Language Models", "categories": ["cs.NI", "cs.AI"], "comment": "5 pages, 3 figures, submitted to IEEE Globecom 2025", "summary": "Advanced intelligent automation becomes an important feature to deal with the\nincreased complexity in managing wireless networks. This paper proposes a novel\nautomation approach of intent-based network for Radio Access Networks (RANs)\nmanagement by leveraging Large Language Models (LLMs). The proposed method\nenhances intent translation, autonomously interpreting high-level objectives,\nreasoning over complex network states, and generating precise configurations of\nthe RAN by integrating LLMs within an agentic architecture. We propose a\nstructured prompt engineering technique and demonstrate that the network can\nautomatically improve its energy efficiency by dynamically optimizing critical\nRAN parameters through a closed-loop mechanism. It showcases the potential to\nenable robust resource management in RAN by adapting strategies based on\nreal-time feedback via LLM-orchestrated agentic systems.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u4ee3\u7406\u67b6\u6784\uff0c\u5b9e\u73b0\u57fa\u4e8e\u610f\u56fe\u7684\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff08RAN\uff09\u7ba1\u7406\u81ea\u52a8\u5316\uff0c\u5e76\u6709\u6548\u63d0\u9ad8\u7f51\u7edc\u80fd\u6548\u3002", "motivation": "\u4e3a\u5e94\u5bf9\u65e0\u7ebf\u7f51\u7edc\u7ba1\u7406\u65e5\u76ca\u589e\u957f\u7684\u590d\u6742\u6027\uff0c\u8feb\u5207\u9700\u8981\u5148\u8fdb\u7684\u667a\u80fd\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLLM\u548c\u4ee3\u7406\u67b6\u6784\u7684\u65b0\u578b\u610f\u56fe\u7f51\u7edc\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8eRAN\u7ba1\u7406\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6574\u5408LLM\uff0c\u589e\u5f3a\u610f\u56fe\u7ffb\u8bd1\u3001\u81ea\u4e3b\u89e3\u91ca\u9ad8\u7ea7\u76ee\u6807\u3001\u63a8\u7406\u590d\u6742\u7f51\u7edc\u72b6\u6001\u5e76\u751f\u6210\u7cbe\u786e\u7684RAN\u914d\u7f6e\uff0c\u5e76\u91c7\u7528\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u3002", "result": "\u901a\u8fc7\u95ed\u73af\u673a\u5236\uff0c\u7f51\u7edc\u80fd\u591f\u52a8\u6001\u4f18\u5316\u5173\u952eRAN\u53c2\u6570\uff0c\u4ece\u800c\u81ea\u52a8\u63d0\u9ad8\u5176\u80fd\u6e90\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7LLM\u534f\u8c03\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u6839\u636e\u5b9e\u65f6\u53cd\u9988\u8c03\u6574\u7b56\u7565\uff0c\u5728RAN\u4e2d\u5b9e\u73b0\u9c81\u68d2\u8d44\u6e90\u7ba1\u7406\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.14406", "pdf": "https://arxiv.org/pdf/2507.14406", "abs": "https://arxiv.org/abs/2507.14406", "authors": ["Michael J. Zellinger", "Matt Thomson"], "title": "Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": "8 pages, 5 figures", "summary": "State-of-the-art reasoning LLMs are powerful problem solvers, but they still\noccasionally make mistakes. However, adopting AI models in risk-sensitive\ndomains often requires error rates near 0%. To address this gap, we propose\ncollaboration between a reasoning model and a human expert who resolves queries\nthe model cannot confidently answer. We find that quantifying the uncertainty\nof a reasoning model through the length of its reasoning trace yields an\neffective basis for deferral to a human, e.g., cutting the error rate of Qwen3\n235B-A22B on difficult MATH problems from 3% to less than 1% when deferring\n7.5% of queries. However, the high latency of reasoning models still makes them\nchallenging to deploy on use cases with high query volume. To address this\nchallenge, we explore fronting a reasoning model with a large non-reasoning\nmodel. We call this modified human-in-the-loop system \"Fail Fast, or Ask\",\nsince the non-reasoning model may defer difficult queries to the human expert\ndirectly (\"failing fast\"), without incurring the reasoning model's higher\nlatency. We show that this approach yields around 40% latency reduction and\nabout 50% cost savings for DeepSeek R1 while maintaining 90+% area under the\naccuracy-rejection curve. However, we observe that latency savings are lower\nthan expected because of \"latency drag\", the phenomenon that processing easier\nqueries with a non-reasoning model pushes the reasoning model's latency\ndistribution towards longer latencies. Broadly, our results suggest that the\ndeficiencies of state-of-the-art reasoning models -- nontrivial error rates and\nhigh latency -- can be substantially mitigated through black-box systems\nengineering, without requiring access to LLM internals.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\uff0c\u5305\u62ec\u201c\u5feb\u901f\u5931\u8d25\u6216\u63d0\u95ee\u201d\u6846\u67b6\uff0c\u6765\u89e3\u51b3\u6700\u5148\u8fdb\u63a8\u7406LLM\u7684\u9519\u8bef\u7387\u548c\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u548c\u6210\u672c\u8282\u7ea6\uff0c\u65e0\u9700\u8bbf\u95eeLLM\u5185\u90e8\u3002", "motivation": "\u6700\u5148\u8fdb\u7684\u63a8\u7406LLM\u5076\u5c14\u4f1a\u72af\u9519\uff0c\u4f46\u5728\u98ce\u9669\u654f\u611f\u9886\u57df\u9700\u8981\u8fd1\u4e4e\u96f6\u7684\u9519\u8bef\u7387\u3002\u6b64\u5916\uff0c\u63a8\u7406\u6a21\u578b\u7684\u9ad8\u5ef6\u8fdf\u9650\u5236\u4e86\u5176\u5728\u9ad8\u67e5\u8be2\u91cf\u573a\u666f\u7684\u90e8\u7f72\u3002", "method": "1. \u63d0\u51fa\u63a8\u7406\u6a21\u578b\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u534f\u4f5c\uff0c\u7531\u4eba\u7c7b\u5904\u7406\u6a21\u578b\u65e0\u6cd5\u81ea\u4fe1\u56de\u7b54\u7684\u67e5\u8be2\uff0c\u5e76\u4ee5\u63a8\u7406\u8f68\u8ff9\u957f\u5ea6\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u30022. \u4e3a\u89e3\u51b3\u9ad8\u5ef6\u8fdf\uff0c\u5f15\u5165\u201c\u5feb\u901f\u5931\u8d25\u6216\u63d0\u95ee\u201d\uff08Fail Fast, or Ask\uff09\u7cfb\u7edf\uff0c\u4f7f\u7528\u5927\u578b\u975e\u63a8\u7406\u6a21\u578b\u4f5c\u4e3a\u524d\u7aef\uff0c\u53ef\u76f4\u63a5\u5c06\u56f0\u96be\u67e5\u8be2\u8f6c\u4ea4\u7ed9\u4eba\u7c7b\u4e13\u5bb6\uff0c\u4ece\u800c\u907f\u514d\u63a8\u7406\u6a21\u578b\u7684\u9ad8\u5ef6\u8fdf\u3002", "result": "1. \u901a\u8fc7\u8f6c\u4ea47.5%\u7684\u67e5\u8be2\uff0c\u5c06Qwen3 235B-A22B\u5728\u56f0\u96beMATH\u95ee\u9898\u4e0a\u7684\u9519\u8bef\u7387\u4ece3%\u964d\u81f3\u4e0d\u8db31%\u30022. \u201c\u5feb\u901f\u5931\u8d25\u6216\u63d0\u95ee\u201d\u7cfb\u7edf\u4f7fDeepSeek R1\u7684\u5ef6\u8fdf\u964d\u4f4e\u7ea640%\uff0c\u6210\u672c\u8282\u7701\u7ea650%\uff0c\u540c\u65f6\u4fdd\u630190%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387-\u62d2\u7edd\u66f2\u7ebf\u4e0b\u9762\u79ef\u30023. \u89c2\u5bdf\u5230\u201c\u5ef6\u8fdf\u62d6\u62fd\u201d\u73b0\u8c61\uff0c\u5373\u975e\u63a8\u7406\u6a21\u578b\u5904\u7406\u7b80\u5355\u67e5\u8be2\u4f1a\u4f7f\u63a8\u7406\u6a21\u578b\u7684\u5ef6\u8fdf\u5206\u5e03\u504f\u5411\u66f4\u957f\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u8282\u7701\u4f4e\u4e8e\u9884\u671f\u3002", "conclusion": "\u65e0\u9700\u8bbf\u95eeLLM\u5185\u90e8\uff0c\u901a\u8fc7\u9ed1\u76d2\u7cfb\u7edf\u5de5\u7a0b\u53ef\u4ee5\u663e\u8457\u7f13\u89e3\u5f53\u524d\u6700\u5148\u8fdb\u63a8\u7406\u6a21\u578b\u5b58\u5728\u7684\u975e\u96f6\u9519\u8bef\u7387\u548c\u9ad8\u5ef6\u8fdf\u7b49\u7f3a\u9677\u3002"}}
{"id": "2507.14432", "pdf": "https://arxiv.org/pdf/2507.14432", "abs": "https://arxiv.org/abs/2507.14432", "authors": ["Han Gong", "Qiyue Li", "Zhi Liu", "Hao Zhou", "Peng Yuan Zhou", "Zhu Li", "Jie Li"], "title": "Adaptive 3D Gaussian Splatting Video Streaming", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the\nquality of volumetric video representation. Meanwhile, in contrast to\nconventional volumetric video, 3DGS video poses significant challenges for\nstreaming due to its substantially larger data volume and the heightened\ncomplexity involved in compression and transmission. To address these issues,\nwe introduce an innovative framework for 3DGS volumetric video streaming.\nSpecifically, we design a 3DGS video construction method based on the Gaussian\ndeformation field. By employing hybrid saliency tiling and differentiated\nquality modeling of 3DGS video, we achieve efficient data compression and\nadaptation to bandwidth fluctuations while ensuring high transmission quality.\nThen we build a complete 3DGS video streaming system and validate the\ntransmission performance. Through experimental evaluation, our method\ndemonstrated superiority over existing approaches in various aspects, including\nvideo quality, compression effectiveness, and transmission rate.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u521b\u65b0\u76843DGS\u4f53\u89c6\u9891\u6d41\u5a92\u4f53\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5176\u5e9e\u5927\u6570\u636e\u91cf\u548c\u4f20\u8f93\u590d\u6742\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7279\u5b9a\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u538b\u7f29\u548c\u9ad8\u8d28\u91cf\u4f20\u8f93\u3002", "motivation": "3DGS\u867d\u663e\u8457\u63d0\u5347\u4e86\u4f53\u89c6\u9891\u8d28\u91cf\uff0c\u4f46\u5176\u5de8\u5927\u7684\u6570\u636e\u91cf\u548c\u590d\u6742\u7684\u538b\u7f29\u4f20\u8f93\u7ed9\u6d41\u5a92\u4f53\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u9ad8\u65af\u5f62\u53d8\u573a\u76843DGS\u89c6\u9891\u6784\u5efa\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u6df7\u5408\u663e\u8457\u6027\u5206\u5757\u548c\u5dee\u5f02\u5316\u8d28\u91cf\u5efa\u6a21\uff0c\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u538b\u7f29\u548c\u5e26\u5bbd\u81ea\u9002\u5e94\u3002\u540c\u65f6\u6784\u5efa\u4e86\u5b8c\u6574\u76843DGS\u89c6\u9891\u6d41\u5a92\u4f53\u7cfb\u7edf\u8fdb\u884c\u6027\u80fd\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u8d28\u91cf\u3001\u538b\u7f29\u6548\u7387\u548c\u4f20\u8f93\u901f\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e863DGS\u4f53\u89c6\u9891\u6d41\u5a92\u4f53\u7684\u5173\u952e\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9879\u6027\u80fd\u6307\u6807\u4e0a\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.14179", "pdf": "https://arxiv.org/pdf/2507.14179", "abs": "https://arxiv.org/abs/2507.14179", "authors": ["Nobel Dhar", "Bobin Deng", "Md Romyull Islam", "Xinyue Zhang", "Kazi Fahim Ahmad Nasif", "Kun Suo"], "title": "A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "comment": "To be published in Euro-Par 2025", "summary": "Large Language Models (LLMs) exhibit significant activation sparsity, where\nonly a subset of neurons are active for a given input. Although this sparsity\npresents opportunities to reduce computational cost, efficiently utilizing it\nrequires predicting activation patterns in a scalable manner. However, direct\nprediction at the neuron level is computationally expensive due to the vast\nnumber of neurons in modern LLMs. To enable efficient prediction and\nutilization of activation sparsity, we propose a clustering-based activation\npattern compression framework. Instead of treating each neuron independently,\nwe group similar activation patterns into a small set of representative\nclusters. Our method achieves up to 79.34% clustering precision, outperforming\nstandard binary clustering approaches while maintaining minimal degradation in\nperplexity (PPL) scores. With a sufficiently large number of clusters, our\napproach attains a PPL score as low as 12.49, demonstrating its effectiveness\nin preserving model quality while reducing computational overhead. By\npredicting cluster assignments rather than individual neuron states, future\nmodels can efficiently infer activation patterns from pre-computed centroids.\nWe detail the clustering algorithm, analyze its effectiveness in capturing\nmeaningful activation structures, and demonstrate its potential to improve\nsparse computation efficiency. This clustering-based formulation serves as a\nfoundation for future work on activation pattern prediction, paving the way for\nefficient inference in large-scale language models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u6fc0\u6d3b\u6a21\u5f0f\u538b\u7f29\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u9884\u6d4b\u548c\u5229\u7528LLMs\u4e2d\u7684\u6fc0\u6d3b\u7a00\u758f\u6027\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b58\u5728\u663e\u8457\u7684\u6fc0\u6d3b\u7a00\u758f\u6027\uff0c\u8fd9\u4e3a\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002\u7136\u800c\uff0c\u7531\u4e8eLLMs\u4e2d\u795e\u7ecf\u5143\u6570\u91cf\u5e9e\u5927\uff0c\u76f4\u63a5\u5728\u795e\u7ecf\u5143\u5c42\u9762\u9884\u6d4b\u6fc0\u6d3b\u6a21\u5f0f\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u548c\u5229\u7528\u8fd9\u79cd\u7a00\u758f\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u6fc0\u6d3b\u6a21\u5f0f\u538b\u7f29\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u4e0d\u72ec\u7acb\u5904\u7406\u6bcf\u4e2a\u795e\u7ecf\u5143\uff0c\u800c\u662f\u5c06\u76f8\u4f3c\u7684\u6fc0\u6d3b\u6a21\u5f0f\u5206\u7ec4\u5230\u5c11\u91cf\u5177\u6709\u4ee3\u8868\u6027\u7684\u7c07\u4e2d\uff0c\u4ece\u800c\u5c06\u9884\u6d4b\u4efb\u52a1\u4ece\u5355\u4e2a\u795e\u7ecf\u5143\u72b6\u6001\u7b80\u5316\u4e3a\u7c07\u5206\u914d\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe79.34%\u7684\u805a\u7c7b\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u6807\u51c6\u7684\u4e8c\u5143\u805a\u7c7b\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6700\u5c0f\u7684\u56f0\u60d1\u5ea6\uff08PPL\uff09\u4e0b\u964d\u3002\u5f53\u7c07\u6570\u91cf\u8db3\u591f\u5927\u65f6\uff0c\u56f0\u60d1\u5ea6\u53ef\u4f4e\u81f312.49\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4fdd\u7559\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u805a\u7c7b\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u83b7\u6709\u610f\u4e49\u7684\u6fc0\u6d3b\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u9884\u6d4b\u7c07\u5206\u914d\u800c\u975e\u4e2a\u4f53\u795e\u7ecf\u5143\u72b6\u6001\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u9ad8\u6548\u63a8\u65ad\u6fc0\u6d3b\u6a21\u5f0f\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u671b\u663e\u8457\u63d0\u9ad8\u7a00\u758f\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2507.14307", "pdf": "https://arxiv.org/pdf/2507.14307", "abs": "https://arxiv.org/abs/2507.14307", "authors": ["Karin de Langis", "Jong Inn Park", "Andreas Schramm", "Bin Hu", "Khanh Chi Le", "Michael Mensink", "Ahn Thu Tong", "Dongyeop Kang"], "title": "How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit increasingly sophisticated linguistic\ncapabilities, yet the extent to which these behaviors reflect human-like\ncognition versus advanced pattern recognition remains an open question. In this\nstudy, we investigate how LLMs process the temporal meaning of linguistic\naspect in narratives that were previously used in human studies. Using an\nExpert-in-the-Loop probing pipeline, we conduct a series of targeted\nexperiments to assess whether LLMs construct semantic representations and\npragmatic inferences in a human-like manner. Our findings show that LLMs\nover-rely on prototypicality, produce inconsistent aspectual judgments, and\nstruggle with causal reasoning derived from aspect, raising concerns about\ntheir ability to fully comprehend narratives. These results suggest that LLMs\nprocess aspect fundamentally differently from humans and lack robust narrative\nunderstanding. Beyond these empirical findings, we develop a standardized\nexperimental framework for the reliable assessment of LLMs' cognitive and\nlinguistic capabilities.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u5904\u7406\u53d9\u4e8b\u4e2d\u8bed\u8a00\u4f53\u7684\u65f6\u6001\u610f\u4e49\u65f6\uff0c\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u7f3a\u4e4f\u7a33\u5065\u7684\u53d9\u4e8b\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bed\u8a00\u80fd\u529b\u662f\u6e90\u4e8e\u7c7b\u4eba\u8ba4\u77e5\u8fd8\u662f\u9ad8\u7ea7\u6a21\u5f0f\u8bc6\u522b\uff0c\u5e76\u5177\u4f53\u8c03\u67e5LLMs\u5982\u4f55\u5904\u7406\u53d9\u4e8b\u4e2d\u8bed\u8a00\u4f53\u7684\u65f6\u6001\u610f\u4e49\u3002", "method": "\u91c7\u7528\u201c\u4e13\u5bb6\u5728\u73af\uff08Expert-in-the-Loop\uff09\u201d\u63a2\u6d4b\u6d41\u7a0b\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u9488\u5bf9\u6027\u5b9e\u9a8c\uff0c\u8bc4\u4f30LLMs\u6784\u5efa\u8bed\u4e49\u8868\u5f81\u548c\u8bed\u7528\u63a8\u65ad\u7684\u80fd\u529b\uff0c\u5e76\u4f7f\u7528\u4e86\u5148\u524d\u4eba\u7c7b\u7814\u7a76\u4e2d\u7684\u53d9\u4e8b\u6750\u6599\u3002", "result": "LLMs\u8fc7\u5ea6\u4f9d\u8d56\u539f\u578b\u6027\uff0c\u4ea7\u751f\u4e0d\u4e00\u81f4\u7684\u4f53\u5224\u65ad\uff0c\u5e76\u4e14\u96be\u4ee5\u8fdb\u884c\u6e90\u4e8e\u4f53\u7684\u56e0\u679c\u63a8\u7406\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u5176\u5b8c\u5168\u7406\u89e3\u53d9\u4e8b\u80fd\u529b\u7684\u62c5\u5fe7\u3002", "conclusion": "LLMs\u5904\u7406\u8bed\u8a00\u4f53\u7684\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u6839\u672c\u4e0d\u540c\uff0c\u7f3a\u4e4f\u7a33\u5065\u7684\u53d9\u4e8b\u7406\u89e3\u3002\u672c\u7814\u7a76\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u5b9e\u9a8c\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u9760\u8bc4\u4f30LLMs\u7684\u8ba4\u77e5\u548c\u8bed\u8a00\u80fd\u529b\u3002"}}
{"id": "2507.14234", "pdf": "https://arxiv.org/pdf/2507.14234", "abs": "https://arxiv.org/abs/2507.14234", "authors": ["Samer Nasser", "Henrique Duarte Moura", "Dragan Subotic", "Ritesh Kumar Singh", "Maarten Weyn", "Jeroen Famaey"], "title": "Feasibility of Energy Neutral Wildlife Tracking using Multi-Source Energy Harvesting", "categories": ["cs.NI"], "comment": null, "summary": "Long-term wildlife tracking is crucial for biodiversity monitoring, but\nenergy limitations pose challenges, especially for animal tags, where replacing\nbatteries is impractical and stressful for the animal due to the need to\nlocate, possibly sedate, and handle it. Energy harvesting offers a sustainable\nalternative, yet most existing systems rely on a single energy source and\ninfrastructure-limited communication technologies. This paper presents an\nenergy-neutral system that combines solar and kinetic energy harvesting to\nenable the tracking and monitoring of wild animals. Harvesting from multiple\nsources increases the total available energy. Uniquely, the kinetic harvester\nalso serves as a motion proxy by sampling harvested current, enabling activity\nmonitoring without dedicated sensors. Our approach also ensures compatibility\nwith existing cellular infrastructure, using Narrowband Internet of Things\n(NB-IoT). We present a simulation framework that models energy harvesting,\nstorage, and consumption at the component level. An energy-aware scheduler\ncoordinates task execution based on real-time energy availability. We evaluate\nperformance under realistically varying conditions, comparing task frequencies\nand capacitor sizes. Results show that our approach maintains energy-neutral\noperation while significantly increasing data yield and reliability compared to\nsingle-source systems, with the ability to consistently sample GPS location\ndata and kinetic harvesting data every two minutes while transmitting these\nresults over NB-IoT every hour. These findings demonstrate the potential for\nmaintenance-free, environmentally friendly tracking in remote habitats,\nenabling more effective and scalable wildlife monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u592a\u9633\u80fd\u548c\u52a8\u80fd\u6536\u96c6\u7684\u80fd\u91cf\u4e2d\u6027\u91ce\u751f\u52a8\u7269\u8ffd\u8e2a\u7cfb\u7edf\uff0c\u5229\u7528NB-IoT\u901a\u4fe1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u4ea7\u91cf\u548c\u53ef\u9760\u6027\uff0c\u5b9e\u73b0\u4e86\u957f\u671f\u514d\u7ef4\u62a4\u7684\u52a8\u7269\u76d1\u6d4b\u3002", "motivation": "\u957f\u671f\u91ce\u751f\u52a8\u7269\u8ffd\u8e2a\u5bf9\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u52a8\u7269\u6807\u7b7e\u9762\u4e34\u80fd\u6e90\u9650\u5236\uff0c\u7535\u6c60\u66f4\u6362\u56f0\u96be\u4e14\u5bf9\u52a8\u7269\u6709\u5bb3\u3002\u5f53\u524d\u80fd\u91cf\u6536\u96c6\u7cfb\u7edf\u591a\u4f9d\u8d56\u5355\u4e00\u80fd\u6e90\uff0c\u4e14\u901a\u4fe1\u6280\u672f\u53d7\u9650\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u592a\u9633\u80fd\u548c\u52a8\u80fd\u6536\u96c6\u7684\u80fd\u91cf\u4e2d\u6027\u7cfb\u7edf\uff1b\u52a8\u80fd\u6536\u96c6\u5668\u540c\u65f6\u4f5c\u4e3a\u8fd0\u52a8\u4ee3\u7406\uff1b\u91c7\u7528NB-IoT\u4e0e\u73b0\u6709\u8702\u7a9d\u57fa\u7840\u8bbe\u65bd\u517c\u5bb9\uff1b\u6784\u5efa\u4e86\u6a21\u62df\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u80fd\u91cf\u611f\u77e5\u8c03\u5ea6\u5668\u4ee5\u4f18\u5316\u4efb\u52a1\u6267\u884c\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u80fd\u91cf\u4e2d\u6027\u8fd0\u884c\uff0c\u4e0e\u5355\u6e90\u7cfb\u7edf\u76f8\u6bd4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u4ea7\u91cf\u548c\u53ef\u9760\u6027\uff1b\u7cfb\u7edf\u80fd\u6301\u7eed\u6bcf\u4e24\u5206\u949f\u91c7\u6837GPS\u4f4d\u7f6e\u548c\u52a8\u80fd\u6570\u636e\uff0c\u5e76\u6bcf\u5c0f\u65f6\u901a\u8fc7NB-IoT\u4f20\u8f93\u8fd9\u4e9b\u6570\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u679c\u8bc1\u660e\u4e86\u5728\u504f\u8fdc\u6816\u606f\u5730\u5b9e\u73b0\u514d\u7ef4\u62a4\u3001\u73af\u4fdd\u7684\u91ce\u751f\u52a8\u7269\u8ffd\u8e2a\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u66f4\u6709\u6548\u548c\u53ef\u6269\u5c55\u7684\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u3002"}}
{"id": "2507.14417", "pdf": "https://arxiv.org/pdf/2507.14417", "abs": "https://arxiv.org/abs/2507.14417", "authors": ["Aryo Pradipta Gema", "Alexander H\u00e4gele", "Runjin Chen", "Andy Arditi", "Jacob Goldman-Wetzler", "Kit Fraser-Taliente", "Henry Sleight", "Linda Petrini", "Julian Michael", "Beatrice Alex", "Pasquale Minervini", "Yanda Chen", "Joe Benton", "Ethan Perez"], "title": "Inverse Scaling in Test-Time Compute", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We construct evaluation tasks where extending the reasoning length of Large\nReasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling\nrelationship between test-time compute and accuracy. Our evaluation tasks span\nfour categories: simple counting tasks with distractors, regression tasks with\nspurious features, deduction tasks with constraint tracking, and advanced AI\nrisks. We identify five distinct failure modes when models reason for longer:\n1) Claude models become increasingly distracted by irrelevant information; 2)\nOpenAI o-series models resist distractors but overfit to problem framings; 3)\nmodels shift from reasonable priors to spurious correlations; 4) all models\nshow difficulties in maintaining focus on complex deductive tasks; and 5)\nextended reasoning may amplify concerning behaviors, with Claude Sonnet 4\nshowing increased expressions of self-preservation. These findings suggest that\nwhile test-time compute scaling remains promising for improving model\ncapabilities, it may inadvertently reinforce problematic reasoning patterns.\nOur results demonstrate the importance of evaluating models across diverse\nreasoning lengths to identify and address these failure modes in LRMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u63a8\u7406\u957f\u5ea6\u589e\u52a0\u53cd\u800c\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u8868\u73b0\u51fa\u6d4b\u8bd5\u8ba1\u7b97\u91cf\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u53cd\u5411\u6269\u5c55\u5173\u7cfb\uff0c\u5e76\u8bc6\u522b\u4e86\u4e94\u79cd\u5177\u4f53\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u5206\u6790\u5728\u5ef6\u957f\u63a8\u7406\u957f\u5ea6\u65f6\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u8bc6\u522b\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u4ee5\u5e94\u5bf9\u8ba1\u7b97\u91cf\u589e\u52a0\u53cd\u800c\u5bfc\u81f4\u51c6\u786e\u6027\u964d\u4f4e\u7684\u73b0\u8c61\u3002", "method": "\u6784\u5efa\u4e86\u56db\u7c7b\u8bc4\u4f30\u4efb\u52a1\uff1a\u5e26\u6709\u5e72\u6270\u9879\u7684\u7b80\u5355\u8ba1\u6570\u4efb\u52a1\u3001\u5e26\u6709\u865a\u5047\u7279\u5f81\u7684\u56de\u5f52\u4efb\u52a1\u3001\u5e26\u6709\u7ea6\u675f\u8ddf\u8e2a\u7684\u6f14\u7ece\u4efb\u52a1\uff0c\u4ee5\u53ca\u4e0e\u9ad8\u7ea7AI\u98ce\u9669\u76f8\u5173\u7684\u4efb\u52a1\uff0c\u7528\u4ee5\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u63a8\u7406\u957f\u5ea6\u4e0b\u7684\u8868\u73b0\u3002", "result": "1. \u5ef6\u957f\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u957f\u5ea6\u4f1a\u5bfc\u81f4\u6027\u80fd\u6076\u5316\uff0c\u8868\u73b0\u51fa\u6d4b\u8bd5\u8ba1\u7b97\u91cf\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u53cd\u5411\u6269\u5c55\u5173\u7cfb\u30022. \u8bc6\u522b\u51fa\u4e94\u79cd\u5931\u8d25\u6a21\u5f0f\uff1aClaude\u6a21\u578b\u6613\u53d7\u65e0\u5173\u4fe1\u606f\u5e72\u6270\uff1bOpenAI o\u7cfb\u5217\u6a21\u578b\u62b5\u6297\u5e72\u6270\u4f46\u8fc7\u5ea6\u9002\u5e94\u95ee\u9898\u6846\u67b6\uff1b\u6a21\u578b\u4ece\u5408\u7406\u5148\u9a8c\u8f6c\u5411\u865a\u5047\u5173\u8054\uff1b\u6240\u6709\u6a21\u578b\u5728\u590d\u6742\u6f14\u7ece\u4efb\u52a1\u4e2d\u96be\u4ee5\u4fdd\u6301\u4e13\u6ce8\uff1b\u5ef6\u957f\u63a8\u7406\u53ef\u80fd\u653e\u5927\u95ee\u9898\u884c\u4e3a\uff08\u5982Claude Sonnet 4\u7684\u81ea\u6211\u4fdd\u62a4\u8868\u8fbe\u589e\u52a0\uff09\u3002", "conclusion": "\u5c3d\u7ba1\u589e\u52a0\u6d4b\u8bd5\u8ba1\u7b97\u91cf\u5728\u63d0\u9ad8\u6a21\u578b\u80fd\u529b\u65b9\u9762\u4ecd\u6709\u524d\u666f\uff0c\u4f46\u8fd9\u53ef\u80fd\u65e0\u610f\u4e2d\u5f3a\u5316\u6709\u95ee\u9898\u7684\u63a8\u7406\u6a21\u5f0f\u3002\u56e0\u6b64\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u63a8\u7406\u957f\u5ea6\u4e0b\u7684\u8868\u73b0\u5bf9\u4e8e\u8bc6\u522b\u548c\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u8fd9\u4e9b\u5931\u8d25\u6a21\u5f0f\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.14449", "pdf": "https://arxiv.org/pdf/2507.14449", "abs": "https://arxiv.org/abs/2507.14449", "authors": ["Zhe Cao", "Jin Zhang", "Ruiheng Zhang"], "title": "IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark", "categories": ["cs.CV"], "comment": "11 pages, 7 figures. This paper is accepted by ICCV 2025", "summary": "Real-world infrared imagery presents unique challenges for vision-language\nmodels due to the scarcity of aligned text data and domain-specific\ncharacteristics. Although existing methods have advanced the field, their\nreliance on synthetic infrared images generated through style transfer from\nvisible images, which limits their ability to capture the unique\ncharacteristics of the infrared modality. To address this, we propose IRGPT,\nthe first multi-modal large language model for real-world infrared images,\nbuilt upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K\nauthentic image-text pairs. The proposed IR-TD dataset contains real infrared\nimages paired with meticulously handcrafted texts, where the initial drafts\noriginated from two complementary processes: (1) LLM-generated descriptions of\nvisible images, and (2) rule-based descriptions of annotations. Furthermore, we\nintroduce a bi-cross-modal curriculum transfer learning strategy that\nsystematically transfers knowledge from visible to infrared domains by\nconsidering the difficulty scores of both infrared-visible and infrared-text.\nEvaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT\nachieves state-of-the-art performance even compared with larger-scale models.", "AI": {"tldr": "\u63d0\u51faIRGPT\uff0c\u9996\u4e2a\u9762\u5411\u771f\u5b9e\u7ea2\u5916\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u7ea2\u5916-\u6587\u672c\u6570\u636e\u96c6\u548c\u521b\u65b0\u7684\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u771f\u5b9e\u7ea2\u5916\u56fe\u50cf\u65f6\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u7279\u6027\u7684\u6311\u6218\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u4f9d\u8d56\u4ece\u53ef\u89c1\u5149\u56fe\u50cf\u5408\u6210\u7684\u7ea2\u5916\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u5bf9\u771f\u5b9e\u7ea2\u5916\u6a21\u6001\u72ec\u7279\u7279\u5f81\u7684\u6355\u6349\u3002", "method": "\u63d0\u51fa\u4e86IRGPT\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21 InfraRed-Text Dataset (IR-TD)\uff0c\u5305\u542b\u8d85\u8fc726\u4e07\u5bf9\u771f\u5b9e\u7ea2\u5916\u56fe\u50cf-\u6587\u672c\u5bf9\u3002IR-TD\u4e2d\u7684\u6587\u672c\u901a\u8fc7\u4e24\u79cd\u4e92\u8865\u8fc7\u7a0b\u751f\u6210\uff1a(1) \u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u53ef\u89c1\u5149\u56fe\u50cf\u63cf\u8ff0\uff0c(2) \u57fa\u4e8e\u89c4\u5219\u7684\u6807\u6ce8\u63cf\u8ff0\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u53cc\u8de8\u6a21\u6001\u8bfe\u7a0b\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u6839\u636e\u7ea2\u5916-\u53ef\u89c1\u548c\u7ea2\u5916-\u6587\u672c\u7684\u96be\u5ea6\u5206\u6570\uff0c\u7cfb\u7edf\u5730\u5c06\u77e5\u8bc6\u4ece\u53ef\u89c1\u5149\u9886\u57df\u8fc1\u79fb\u5230\u7ea2\u5916\u9886\u57df\u3002", "result": "IRGPT\u57289\u9879\u57fa\u51c6\u4efb\u52a1\uff08\u5982\u8bc6\u522b\u3001\u63a5\u5730\uff09\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002", "conclusion": "IRGPT\u6210\u529f\u89e3\u51b3\u4e86\u771f\u5b9e\u7ea2\u5916\u56fe\u50cf\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5176\u72ec\u7279\u7684\u6570\u636e\u96c6\u6784\u5efa\u548c\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u6c34\u5e73\u548c\u6027\u80fd\u3002"}}
{"id": "2507.14180", "pdf": "https://arxiv.org/pdf/2507.14180", "abs": "https://arxiv.org/abs/2507.14180", "authors": ["Nasir Khan", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil", "Sinem Coleri"], "title": "Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In line with the AI-native 6G vision, explainability and robustness are\ncrucial for building trust and ensuring reliable performance in millimeter-wave\n(mmWave) systems. Efficient beam alignment is essential for initial access, but\ndeep learning (DL) solutions face challenges, including high data collection\noverhead, hardware constraints, lack of explainability, and susceptibility to\nadversarial attacks. This paper proposes a robust and explainable DL-based beam\nalignment engine (BAE) for mmWave multiple-input multiple output (MIMO)\nsystems. The BAE uses received signal strength indicator (RSSI) measurements\nfrom wide beams to predict the best narrow beam, reducing the overhead of\nexhaustive beam sweeping. To overcome the challenge of real-world data\ncollection, this work leverages a site-specific digital twin (DT) to generate\nsynthetic channel data closely resembling real-world environments. A model\nrefinement via transfer learning is proposed to fine-tune the pre-trained model\nresiding in the DT with minimal real-world data, effectively bridging\nmismatches between the digital replica and real-world environments. To reduce\nbeam training overhead and enhance transparency, the framework uses deep\nShapley additive explanations (SHAP) to rank input features by importance,\nprioritizing key spatial directions and minimizing beam sweeping. It also\nincorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a\ncredibility metric for detecting out-of-distribution inputs and ensuring\nrobust, transparent decision-making. Experimental results show that the\nproposed framework reduces real-world data needs by 70%, beam training overhead\nby 62%, and improves outlier detection robustness by up to 8.5x, achieving\nnear-optimal spectral efficiency and transparent decision making compared to\ntraditional softmax based DL models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u5b66\u4e60\u6beb\u7c73\u6ce2\u6ce2\u675f\u5bf9\u9f50\u5f15\u64ce\uff08BAE\uff09\uff0c\u5229\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\u51cf\u5c11\u6570\u636e\u6536\u96c6\u5f00\u9500\uff0c\u5e76\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001SHAP\u548cDkNN\u7b97\u6cd5\u63d0\u9ad8\u6a21\u578b\u7684\u89e3\u91ca\u6027\u3001\u9c81\u68d2\u6027\u53ca\u6548\u7387\uff0c\u5b9e\u73b0\u8fd1\u4e4e\u6700\u4f18\u7684\u9891\u8c31\u6548\u7387\u548c\u900f\u660e\u51b3\u7b56\u3002", "motivation": "\u5728AI\u539f\u751f6G\u6beb\u7c73\u6ce2\u7cfb\u7edf\u4e2d\uff0c\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u5bf9\u5efa\u7acb\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u6ce2\u675f\u5bf9\u9f50\u65b9\u6848\u9762\u4e34\u6570\u636e\u6536\u96c6\u5f00\u9500\u5927\u3001\u786c\u4ef6\u9650\u5236\u3001\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u53ca\u6613\u53d7\u5bf9\u6297\u6027\u653b\u51fb\u7b49\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6ce2\u675f\u5bf9\u9f50\u5f15\u64ce\uff08BAE\uff09\uff0c\u901a\u8fc7\u5bbd\u6ce2\u675f\u7684RSSI\u6d4b\u91cf\u9884\u6d4b\u6700\u4f73\u7a84\u6ce2\u675f\u3002\u4e3a\u514b\u670d\u6570\u636e\u6536\u96c6\u96be\u9898\uff0c\u5229\u7528\u7279\u5b9a\u7ad9\u70b9\u7684\u6570\u5b57\u5b6a\u751f\u751f\u6210\u5408\u6210\u4fe1\u9053\u6570\u636e\uff0c\u5e76\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u4ee5\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u6b64\u5916\uff0c\u5f15\u5165\u6df1\u5ea6SHAP\u6765\u8861\u91cf\u8f93\u5165\u7279\u5f81\u91cd\u8981\u6027\u4ee5\u51cf\u5c11\u6ce2\u675f\u8bad\u7ec3\u5f00\u9500\u548c\u63d0\u9ad8\u900f\u660e\u5ea6\uff0c\u5e76\u6574\u5408Deep k-nearest neighbors\uff08DkNN\uff09\u7b97\u6cd5\u63d0\u4f9b\u53ef\u4fe1\u5ea6\u6307\u6807\uff0c\u7528\u4e8e\u68c0\u6d4b\u5206\u5e03\u5916\u8f93\u5165\u5e76\u786e\u4fdd\u9c81\u68d2\u3001\u900f\u660e\u7684\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6846\u67b6\u5c06\u771f\u5b9e\u4e16\u754c\u6570\u636e\u9700\u6c42\u51cf\u5c1170%\uff0c\u6ce2\u675f\u8bad\u7ec3\u5f00\u9500\u51cf\u5c1162%\uff0c\u5f02\u5e38\u503c\u68c0\u6d4b\u9c81\u68d2\u6027\u63d0\u9ad8\u4e868.5\u500d\uff0c\u4e0e\u4f20\u7edf\u57fa\u4e8esoftmax\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u9891\u8c31\u6548\u7387\u548c\u900f\u660e\u51b3\u7b56\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86\u6beb\u7c73\u6ce2\u7cfb\u7edf\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6ce2\u675f\u5bf9\u9f50\u9762\u4e34\u7684\u6311\u6218\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u8d56\u7684AI\u539f\u751f6G\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.14314", "pdf": "https://arxiv.org/pdf/2507.14314", "abs": "https://arxiv.org/abs/2507.14314", "authors": ["Marija An\u0111edeli\u0107", "Dominik \u0160ipek", "Laura Majer", "Jan \u0160najder"], "title": "What Makes You CLIC: Detection of Croatian Clickbait Headlines", "categories": ["cs.CL"], "comment": "Accepted at Slavic NLP 2025", "summary": "Online news outlets operate predominantly on an advertising-based revenue\nmodel, compelling journalists to create headlines that are often scandalous,\nintriguing, and provocative -- commonly referred to as clickbait. Automatic\ndetection of clickbait headlines is essential for preserving information\nquality and reader trust in digital media and requires both contextual\nunderstanding and world knowledge. For this task, particularly in\nless-resourced languages, it remains unclear whether fine-tuned methods or\nin-context learning (ICL) yield better results. In this paper, we compile CLIC,\na novel dataset for clickbait detection of Croatian news headlines spanning a\n20-year period and encompassing mainstream and fringe outlets. We fine-tune the\nBERTi\\'c model on this task and compare its performance to LLM-based ICL\nmethods with prompts both in Croatian and English. Finally, we analyze the\nlinguistic properties of clickbait. We find that nearly half of the analyzed\nheadlines contain clickbait, and that finetuned models deliver better results\nthan general LLMs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u514b\u7f57\u5730\u4e9a\u8bed\u65b0\u95fb\u6807\u9898\u7684\u70b9\u51fb\u8bf1\u9975\u68c0\u6d4b\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u6570\u636e\u96c6CLIC\uff0c\u5e76\u6bd4\u8f83\u4e86\u5fae\u8c03\u6a21\u578b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\u8fd1\u534a\u6570\u6807\u9898\u662f\u70b9\u51fb\u8bf1\u9975\uff0c\u4e14\u5fae\u8c03\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u901a\u7528LLM\u3002", "motivation": "\u5728\u7ebf\u65b0\u95fb\u4f9d\u8d56\u5e7f\u544a\u6536\u5165\uff0c\u5bfc\u81f4\u51fa\u73b0\u5927\u91cf\u70b9\u51fb\u8bf1\u9975\u6807\u9898\u3002\u81ea\u52a8\u68c0\u6d4b\u70b9\u51fb\u8bf1\u9975\u5bf9\u7ef4\u62a4\u4fe1\u606f\u8d28\u91cf\u548c\u8bfb\u8005\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u8f83\u5c11\u7684\u8bed\u8a00\u4e2d\uff0c\u5fae\u8c03\u65b9\u6cd5\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u54ea\u79cd\u66f4\u6709\u6548\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u7814\u7a76\u8005\u7f16\u8bd1\u4e86\u4e00\u4e2a\u65b0\u7684\u514b\u7f57\u5730\u4e9a\u8bed\u65b0\u95fb\u6807\u9898\u70b9\u51fb\u8bf1\u9975\u6570\u636e\u96c6CLIC\uff08\u6db5\u76d620\u5e74\u3001\u4e3b\u6d41\u548c\u8fb9\u7f18\u5a92\u4f53\uff09\uff0c\u5e76\u5728\u6b64\u4efb\u52a1\u4e0a\u5bf9BERTi'c\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002\u540c\u65f6\uff0c\u5c06\u5fae\u8c03\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u4f7f\u7528\u514b\u7f57\u5730\u4e9a\u8bed\u548c\u82f1\u8bed\u63d0\u793a\u7684\u57fa\u4e8eLLM\u7684ICL\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6700\u540e\uff0c\u5206\u6790\u4e86\u70b9\u51fb\u8bf1\u9975\u7684\u8bed\u8a00\u7279\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8fd1\u4e00\u534a\u7684\u5206\u6790\u6807\u9898\u5305\u542b\u70b9\u51fb\u8bf1\u9975\u3002\u6b64\u5916\uff0c\u5fae\u8c03\u6a21\u578b\u7684\u68c0\u6d4b\u7ed3\u679c\u4f18\u4e8e\u901a\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u5bf9\u4e8e\u70b9\u51fb\u8bf1\u9975\u68c0\u6d4b\u4efb\u52a1\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8d44\u6e90\u8f83\u5c11\u7684\u8bed\u8a00\uff0c\u5fae\u8c03\u4e13\u7528\u6a21\u578b\u6bd4\u4f7f\u7528\u901a\u7528LLM\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u6027\u80fd\u3002\u70b9\u51fb\u8bf1\u9975\u5728\u5728\u7ebf\u65b0\u95fb\u4e2d\u666e\u904d\u5b58\u5728\u3002"}}
{"id": "2507.14263", "pdf": "https://arxiv.org/pdf/2507.14263", "abs": "https://arxiv.org/abs/2507.14263", "authors": ["Ramesh Raskar", "Pradyumna Chari", "John Zinky", "Mahesh Lambe", "Jared James Grogan", "Sichao Wang", "Rajesh Ranjan", "Rekha Singhal", "Shailja Gupta", "Robert Lincourt", "Raghu Bala", "Aditi Joshi", "Abhishek Singh", "Ayush Chopra", "Dimitris Stripelis", "Bhuwan B", "Sumit Kumar", "Maria Gorskikh"], "title": "Beyond DNS: Unlocking the Internet of AI Agents via the NANDA Index and Verified AgentFacts", "categories": ["cs.NI", "cs.AI", "cs.CR", "cs.MA"], "comment": null, "summary": "The Internet is poised to host billions to trillions of autonomous AI agents\nthat negotiate, delegate, and migrate in milliseconds and workloads that will\nstrain DNS-centred identity and discovery. In this paper, we describe the NANDA\nindex architecture, which we envision as a means for discoverability,\nidentifiability and authentication in the internet of AI agents. We present an\narchitecture where a minimal lean index resolves to dynamic, cryptographically\nverifiable AgentFacts that supports multi-endpoint routing, load balancing,\nprivacy-preserving access, and credentialed capability assertions. Our\narchitecture design delivers five concrete guarantees: (1) A quilt-like index\nproposal that supports both NANDA-native agents as well as third party agents\nbeing discoverable via the index, (2) rapid global resolution for newly spawned\nAI agents, (3) sub-second revocation and key rotation, (4) schema-validated\ncapability assertions, and (5) privacy-preserving discovery across\norganisational boundaries via verifiable, least-disclosure queries. We\nformalize the AgentFacts schema, specify a CRDT-based update protocol, and\nprototype adaptive resolvers. The result is a lightweight, horizontally\nscalable foundation that unlocks secure, trust-aware collaboration for the next\ngeneration of the Internet of AI agents, without abandoning existing web\ninfrastructure.", "AI": {"tldr": "\u4e3a\u5e94\u5bf9\u672a\u6765\u6d77\u91cfAI\u667a\u80fd\u4f53\u5bf9\u73b0\u6709\u4e92\u8054\u7f51\u8eab\u4efd\u548c\u53d1\u73b0\u673a\u5236\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51faNANDA\u7d22\u5f15\u67b6\u6784\uff0c\u4ee5\u63d0\u4f9b\u667a\u80fd\u4f53\u5728\u4e92\u8054\u7f51\u4e2d\u7684\u53ef\u53d1\u73b0\u6027\u3001\u8eab\u4efd\u8bc6\u522b\u548c\u8ba4\u8bc1\u80fd\u529b\u3002", "motivation": "\u4e92\u8054\u7f51\u5c06\u627f\u8f7d\u6570\u5341\u4ebf\u81f3\u6570\u4e07\u4ebf\u7684\u81ea\u6cbbAI\u667a\u80fd\u4f53\uff0c\u5b83\u4eec\u9700\u8981\u6beb\u79d2\u7ea7\u7684\u534f\u5546\u3001\u59d4\u6258\u548c\u8fc1\u79fb\u80fd\u529b\uff0c\u8fd9\u5c06\u4f7f\u73b0\u6709\u4ee5DNS\u4e3a\u4e2d\u5fc3\u7684\u8eab\u4efd\u548c\u53d1\u73b0\u673a\u5236\u627f\u53d7\u5de8\u5927\u538b\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86NANDA\u7d22\u5f15\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u901a\u8fc7\u4e00\u4e2a\u6781\u7b80\u7684\u8f7b\u91cf\u7ea7\u7d22\u5f15\u89e3\u6790\u4e3a\u52a8\u6001\u3001\u53ef\u5bc6\u7801\u9a8c\u8bc1\u7684AgentFacts\uff0c\u4ee5\u652f\u6301\u591a\u7aef\u70b9\u8def\u7531\u3001\u8d1f\u8f7d\u5747\u8861\u3001\u9690\u79c1\u4fdd\u62a4\u8bbf\u95ee\u548c\u51ed\u8bc1\u80fd\u529b\u58f0\u660e\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u5f62\u5f0f\u5316AgentFacts\u6a21\u5f0f\u3001\u6307\u5b9a\u57fa\u4e8eCRDT\u7684\u66f4\u65b0\u534f\u8bae\u4ee5\u53ca\u539f\u578b\u81ea\u9002\u5e94\u89e3\u6790\u5668\u3002", "result": "\u8be5\u67b6\u6784\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u4e94\u9879\u5177\u4f53\u4fdd\u8bc1\uff1a\u652f\u6301NANDA\u539f\u751f\u548c\u7b2c\u4e09\u65b9\u667a\u80fd\u4f53\u7684\u7d22\u5f15\u53ef\u53d1\u73b0\u6027\uff1b\u4e3a\u65b0\u751f\u6210\u7684AI\u667a\u80fd\u4f53\u63d0\u4f9b\u5feb\u901f\u5168\u7403\u89e3\u6790\uff1b\u4e9a\u79d2\u7ea7\u7684\u64a4\u9500\u548c\u5bc6\u94a5\u8f6e\u6362\uff1b\u6a21\u5f0f\u9a8c\u8bc1\u7684\u80fd\u529b\u58f0\u660e\uff1b\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u6700\u5c0f\u62ab\u9732\u67e5\u8be2\u5b9e\u73b0\u8de8\u7ec4\u7ec7\u8fb9\u754c\u7684\u9690\u79c1\u4fdd\u62a4\u53d1\u73b0\u3002\u6700\u7ec8\u6210\u679c\u662f\u6784\u5efa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6c34\u5e73\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u53ef\u5728\u4e0d\u653e\u5f03\u73b0\u6709\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u4e0b\u4e00\u4ee3AI\u667a\u80fd\u4f53\u4e92\u8054\u7f51\u89e3\u9501\u5b89\u5168\u3001\u4fe1\u4efb\u611f\u77e5\u7684\u534f\u4f5c\u3002", "conclusion": "NANDA\u67b6\u6784\u4e3a\u672a\u6765\u7684AI\u667a\u80fd\u4f53\u4e92\u8054\u7f51\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6c34\u5e73\u53ef\u6269\u5c55\u3001\u5b89\u5168\u4e14\u4fe1\u4efb\u611f\u77e5\u7684\u57fa\u7840\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u8eab\u4efd\u548c\u53d1\u73b0\u673a\u5236\u6240\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u80fd\u4e0e\u73b0\u6709Web\u57fa\u7840\u8bbe\u65bd\u517c\u5bb9\u3002"}}
{"id": "2507.14447", "pdf": "https://arxiv.org/pdf/2507.14447", "abs": "https://arxiv.org/abs/2507.14447", "authors": ["Guancheng Zeng", "Xueyi Chen", "Jiawang Hu", "Shaohua Qi", "Yaxuan Mao", "Zhantao Wang", "Yifan Nie", "Shuang Li", "Qiuyang Feng", "Pengxu Qiu", "Yujia Wang", "Wenqiang Han", "Linyan Huang", "Gang Li", "Jingjing Mo", "Haowen Hu"], "title": "Routine: A Structural Planning Framework for LLM Agent System in Enterprise", "categories": ["cs.AI", "cs.CL"], "comment": "26 pages, 8 figures, 5 tables", "summary": "The deployment of agent systems in an enterprise environment is often\nhindered by several challenges: common models lack domain-specific process\nknowledge, leading to disorganized plans, missing key tools, and poor execution\nstability. To address this, this paper introduces Routine, a multi-step agent\nplanning framework designed with a clear structure, explicit instructions, and\nseamless parameter passing to guide the agent's execution module in performing\nmulti-step tool-calling tasks with high stability. In evaluations conducted\nwithin a real-world enterprise scenario, Routine significantly increases the\nexecution accuracy in model tool calls, increasing the performance of GPT-4o\nfrom 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed\na Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an\naccuracy increase to 88.2% on scenario-specific evaluations, indicating\nimproved adherence to execution plans. In addition, we employed Routine-based\ndistillation to create a scenario-specific, multi-step tool-calling dataset.\nFine-tuning on this distilled dataset raised the model's accuracy to 95.5%,\napproaching GPT-4o's performance. These results highlight Routine's\neffectiveness in distilling domain-specific tool-usage patterns and enhancing\nmodel adaptability to new scenarios. Our experimental results demonstrate that\nRoutine provides a practical and accessible approach to building stable agent\nworkflows, accelerating the deployment and adoption of agent systems in\nenterprise environments, and advancing the technical vision of AI for Process.", "AI": {"tldr": "Routine\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u89c4\u5212\u548c\u53c2\u6570\u4f20\u9012\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f01\u4e1a\u73af\u5883\u4e2dAgent\u7cfb\u7edf\u591a\u6b65\u5de5\u5177\u8c03\u7528\u7684\u6267\u884c\u7a33\u5b9a\u6027\uff0c\u5e76\u80fd\u6709\u6548\u84b8\u998f\u9886\u57df\u77e5\u8bc6\u4ee5\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709Agent\u7cfb\u7edf\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\uff0c\u56e0\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u5bfc\u81f4\u89c4\u5212\u6df7\u4e71\u3001\u5de5\u5177\u7f3a\u5931\u548c\u6267\u884c\u7a33\u5b9a\u6027\u5dee\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3Agent\u7cfb\u7edf\u5728\u4f01\u4e1a\u591a\u6b65\u5de5\u5177\u8c03\u7528\u4efb\u52a1\u4e2d\u7684\u4f4e\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51faRoutine\uff0c\u4e00\u4e2a\u591a\u6b65Agent\u89c4\u5212\u6846\u67b6\u3002\u5b83\u5177\u6709\u6e05\u6670\u7684\u7ed3\u6784\u3001\u660e\u786e\u7684\u6307\u4ee4\u548c\u65e0\u7f1d\u7684\u53c2\u6570\u4f20\u9012\u673a\u5236\uff0c\u4ee5\u6307\u5bfcAgent\u6267\u884c\u6a21\u5757\u7a33\u5b9a\u5730\u5b8c\u6210\u591a\u6b65\u5de5\u5177\u8c03\u7528\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u8fd8\u6784\u5efa\u4e86Routine-following\u8bad\u7ec3\u6570\u636e\u96c6\u5e76\u8fdb\u884c\u4e86\u6a21\u578b\u5fae\u8c03\uff0c\u5e76\u5229\u7528Routine\u8fdb\u884c\u57fa\u4e8e\u84b8\u998f\u7684\u6570\u636e\u96c6\u6784\u5efa\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5728\u771f\u5b9e\u4f01\u4e1a\u573a\u666f\u8bc4\u4f30\u4e2d\uff0cRoutine\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5de5\u5177\u8c03\u7528\u7684\u6267\u884c\u51c6\u786e\u6027\uff1aGPT-4o\u7684\u6027\u80fd\u4ece41.1%\u63d0\u5347\u81f396.3%\uff0cQwen3-14B\u4ece32.6%\u63d0\u5347\u81f383.3%\u3002\u901a\u8fc7Routine-following\u6570\u636e\u96c6\u5fae\u8c03Qwen3-14B\uff0c\u51c6\u786e\u7387\u63d0\u5347\u81f388.2%\u3002\u901a\u8fc7Routine-based\u84b8\u998f\u6570\u636e\u96c6\u5fae\u8c03\uff0c\u6a21\u578b\u51c6\u786e\u7387\u63d0\u5347\u81f395.5%\uff0c\u63a5\u8fd1GPT-4o\u7684\u6027\u80fd\u3002", "conclusion": "Routine\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u6613\u4e8e\u5b9e\u73b0\u7684\u65b9\u6cd5\u6765\u6784\u5efa\u7a33\u5b9a\u7684Agent\u5de5\u4f5c\u6d41\uff0c\u52a0\u901f\u4e86Agent\u7cfb\u7edf\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u548c\u5e94\u7528\uff0c\u5e76\u6709\u6548\u84b8\u998f\u4e86\u9886\u57df\u5de5\u5177\u4f7f\u7528\u6a21\u5f0f\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u65b0\u573a\u666f\u7684\u9002\u5e94\u6027\uff0c\u63a8\u52a8\u4e86\u201cAI for Process\u201d\u7684\u6280\u672f\u613f\u666f\u3002"}}
{"id": "2507.14452", "pdf": "https://arxiv.org/pdf/2507.14452", "abs": "https://arxiv.org/abs/2507.14452", "authors": ["Weikang Gu", "Mingyue Han", "Li Xue", "Heng Dong", "Changcai Yang", "Riqing Chen", "Lifang Wei"], "title": "GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration", "categories": ["cs.CV"], "comment": "9 pages, 4 figures. Accepted to IJCAI 2025", "summary": "The accurate identification of high-quality correspondences is a prerequisite\ntask in feature-based point cloud registration. However, it is extremely\nchallenging to handle the fusion of local and global features due to feature\nredundancy and complex spatial relationships. Given that Gestalt principles\nprovide key advantages in analyzing local and global relationships, we propose\na novel Gestalt-guided Parallel Interaction Network via orthogonal geometric\nconsistency (GPI-Net) in this paper. It utilizes Gestalt principles to\nfacilitate complementary communication between local and global information.\nSpecifically, we introduce an orthogonal integration strategy to optimally\nreduce redundant information and generate a more compact global structure for\nhigh-quality correspondences. To capture geometric features in correspondences,\nwe leverage a Gestalt Feature Attention (GFA) block through a hybrid\nutilization of self-attention and cross-attention mechanisms. Furthermore, to\nfacilitate the integration of local detail information into the global\nstructure, we design an innovative Dual-path Multi-Granularity parallel\ninteraction aggregation (DMG) block to promote information exchange across\ndifferent granularities. Extensive experiments on various challenging tasks\ndemonstrate the superior performance of our proposed GPI-Net in comparison to\nexisting methods. The code will be released at https://github.com/gwk/GPI-Net.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGPI-Net\uff0c\u4e00\u4e2a\u57fa\u4e8e\u683c\u5f0f\u5854\u539f\u7406\u7684\u5e76\u884c\u4ea4\u4e92\u7f51\u7edc\uff0c\u7528\u4e8e\u4f18\u5316\u70b9\u4e91\u914d\u51c6\u4e2d\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u7684\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u7279\u5f81\u5197\u4f59\u548c\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u5e26\u6765\u7684\u6311\u6218\uff0c\u4ece\u800c\u63d0\u5347\u9ad8\u8d28\u91cf\u5bf9\u5e94\u70b9\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u5728\u57fa\u4e8e\u7279\u5f81\u7684\u70b9\u4e91\u914d\u51c6\u4e2d\uff0c\u51c6\u786e\u8bc6\u522b\u9ad8\u8d28\u91cf\u5bf9\u5e94\u70b9\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7279\u5f81\u5197\u4f59\u548c\u590d\u6742\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u6709\u6548\u878d\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u683c\u5f0f\u5854\u539f\u7406\u7684\u6b63\u4ea4\u51e0\u4f55\u4e00\u81f4\u6027\u5e76\u884c\u4ea4\u4e92\u7f51\u7edc\uff08GPI-Net\uff09\u3002\u8be5\u7f51\u7edc\u5229\u7528\u683c\u5f0f\u5854\u539f\u7406\u4fc3\u8fdb\u5c40\u90e8\u4e0e\u5168\u5c40\u4fe1\u606f\u7684\u4e92\u8865\u901a\u4fe1\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u5f15\u5165\u6b63\u4ea4\u96c6\u6210\u7b56\u7565\u4ee5\u51cf\u5c11\u5197\u4f59\u5e76\u751f\u6210\u7d27\u51d1\u7684\u5168\u5c40\u7ed3\u6784\uff1b\u4f7f\u7528\u683c\u5f0f\u5854\u7279\u5f81\u6ce8\u610f\u529b\uff08GFA\uff09\u5757\uff08\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff09\u6355\u83b7\u5bf9\u5e94\u70b9\u7684\u51e0\u4f55\u7279\u5f81\uff1b\u8bbe\u8ba1\u53cc\u8def\u5f84\u591a\u7c92\u5ea6\u5e76\u884c\u4ea4\u4e92\u805a\u5408\uff08DMG\uff09\u5757\uff0c\u4fc3\u8fdb\u8de8\u7c92\u5ea6\u4fe1\u606f\u4ea4\u6362\uff0c\u5c06\u5c40\u90e8\u7ec6\u8282\u878d\u5165\u5168\u5c40\u7ed3\u6784\u3002", "result": "\u5728\u5404\u79cd\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684GPI-Net\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GPI-Net\u901a\u8fc7\u521b\u65b0\u7684\u7f51\u7edc\u8bbe\u8ba1\u548c\u5bf9\u683c\u5f0f\u5854\u539f\u7406\u7684\u8fd0\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u70b9\u4e91\u914d\u51c6\u4e2d\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u878d\u5408\u7684\u96be\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u8d28\u91cf\u5bf9\u5e94\u70b9\u7684\u8bc6\u522b\u548c\u914d\u51c6\u6027\u80fd\u3002"}}
{"id": "2507.14181", "pdf": "https://arxiv.org/pdf/2507.14181", "abs": "https://arxiv.org/abs/2507.14181", "authors": ["Yajiao Dai", "Jun Li", "Zhen Mei", "Yiyang Ni", "Shi Jin", "Zengxiang Li", "Sheng Guo", "Wei Xiang"], "title": "Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to IEEE Internet of Things Journal, Early Access. 14 pages,\n  5 figures", "summary": "Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe\noperation of industrial machinery and improving production efficiency. However,\ntraditional supervised deep learning methods require a large amount of training\ndata and labels, which are often located in different clients. Additionally,\nthe cost of data labeling is high, making labels difficult to acquire.\nMeanwhile, differences in data distribution among clients may also hinder the\nmodel's performance. To tackle these challenges, this paper proposes a\nsemi-supervised federated learning framework, SSFL-DCSL, which integrates dual\ncontrastive loss and soft labeling to address data and label scarcity for\ndistributed clients with few labeled samples while safeguarding user privacy.\nIt enables representation learning using unlabeled data on the client side and\nfacilitates joint learning among clients through prototypes, thereby achieving\nmutual knowledge sharing and preventing local model divergence. Specifically,\nfirst, a sample weighting function based on the Laplace distribution is\ndesigned to alleviate bias caused by low confidence in pseudo labels during the\nsemi-supervised training process. Second, a dual contrastive loss is introduced\nto mitigate model divergence caused by different data distributions, comprising\nlocal contrastive loss and global contrastive loss. Third, local prototypes are\naggregated on the server with weighted averaging and updated with momentum to\nshare knowledge among clients. To evaluate the proposed SSFL-DCSL framework,\nexperiments are conducted on two publicly available datasets and a dataset\ncollected on motors from the factory. In the most challenging task, where only\n10\\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by\n1.15% to 7.85% over state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSSFL-DCSL\uff0c\u4e00\u4e2a\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u53cc\u91cd\u5bf9\u6bd4\u635f\u5931\u548c\u8f6f\u6807\u7b7e\uff0c\u89e3\u51b3\u5206\u5e03\u5f0f\u667a\u80fd\u6545\u969c\u8bca\u65ad\u4e2d\u6570\u636e\u548c\u6807\u7b7e\u7a00\u7f3a\u3001\u6570\u636e\u5f02\u6784\u6027\u53ca\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u5728\u6709\u9650\u6807\u8bb0\u6570\u636e\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u9762\u4e34\u591a\u91cd\u6311\u6218\uff1a\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u6807\u7b7e\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u548c\u6807\u7b7e\u5e38\u5206\u6563\u5728\u4e0d\u540c\u5ba2\u6237\u7aef\u4e14\u83b7\u53d6\u6210\u672c\u9ad8\u6602\uff1b\u540c\u65f6\uff0c\u5ba2\u6237\u7aef\u95f4\u7684\u6570\u636e\u5206\u5e03\u5dee\u5f02\uff08\u6570\u636e\u5f02\u6784\u6027\uff09\u4f1a\u4e25\u91cd\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86SSFL-DCSL\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u6838\u5fc3\u7b56\u7565\u5305\u62ec\uff1a\n1.  **\u89e3\u51b3\u4f2a\u6807\u7b7e\u7f6e\u4fe1\u5ea6\u95ee\u9898\uff1a** \u8bbe\u8ba1\u4e86\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u5206\u5e03\u7684\u6837\u672c\u52a0\u6743\u51fd\u6570\uff0c\u4ee5\u51cf\u8f7b\u534a\u76d1\u7763\u8bad\u7ec3\u4e2d\u4f4e\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u5bfc\u81f4\u7684\u504f\u5dee\u3002\n2.  **\u7f13\u89e3\u6570\u636e\u5f02\u6784\u6027\uff1a** \u5f15\u5165\u4e86\u53cc\u91cd\u5bf9\u6bd4\u635f\u5931\uff08\u5305\u542b\u5c40\u90e8\u5bf9\u6bd4\u635f\u5931\u548c\u5168\u5c40\u5bf9\u6bd4\u635f\u5931\uff09\uff0c\u4ee5\u6709\u6548\u6291\u5236\u4e0d\u540c\u6570\u636e\u5206\u5e03\u5f15\u8d77\u6a21\u578b\u53d1\u6563\u3002\n3.  **\u4fc3\u8fdb\u77e5\u8bc6\u5171\u4eab\uff1a** \u5728\u670d\u52a1\u5668\u7aef\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u805a\u5408\u5ba2\u6237\u7aef\u7684\u672c\u5730\u539f\u578b\uff0c\u5e76\u5229\u7528\u52a8\u91cf\u66f4\u65b0\u673a\u5236\uff0c\u5b9e\u73b0\u5ba2\u6237\u7aef\u4e4b\u95f4\u7684\u77e5\u8bc6\u5171\u4eab\uff0c\u9632\u6b62\u5c40\u90e8\u6a21\u578b\u53d1\u6563\u3002", "result": "\u8be5\u6846\u67b6\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u771f\u5b9e\u5de5\u5382\u7535\u673a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u573a\u666f\uff08\u4ec5\u670910%\u6570\u636e\u88ab\u6807\u8bb0\uff09\u4e0b\uff0cSSFL-DCSL\u7684\u51c6\u786e\u7387\u6bd4\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad8\u4e861.15%\u81f37.85%\u3002", "conclusion": "SSFL-DCSL\u6846\u67b6\u6709\u6548\u5e94\u5bf9\u4e86\u5206\u5e03\u5f0f\u667a\u80fd\u6545\u969c\u8bca\u65ad\u4e2d\u6570\u636e\u548c\u6807\u7b7e\u7a00\u7f3a\u3001\u6570\u636e\u5206\u5e03\u5dee\u5f02\u4ee5\u53ca\u9690\u79c1\u4fdd\u62a4\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5176\u521b\u65b0\u7684\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u6807\u8bb0\u6570\u636e\u6781\u5c11\u60c5\u51b5\u4e0b\u7684\u6545\u969c\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.14355", "pdf": "https://arxiv.org/pdf/2507.14355", "abs": "https://arxiv.org/abs/2507.14355", "authors": ["Jianfeng Zhu", "Ruoming Jin", "Karin G. Coifman"], "title": "Can LLMs Infer Personality from Real World Conversations?", "categories": ["cs.CL"], "comment": "21 pages, 12 figures", "summary": "Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a\npromising approach for scalable personality assessment from open-ended\nlanguage. However, inferring personality traits remains challenging, and\nearlier work often relied on synthetic data or social media text lacking\npsychometric validity. We introduce a real-world benchmark of 555\nsemi-structured interviews with BFI-10 self-report scores for evaluating\nLLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,\nMeta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item\nprediction and both zero-shot and chain-of-thought prompting for Big Five trait\ninference. All models showed high test-retest reliability, but construct\nvalidity was limited: correlations with ground-truth scores were weak (max\nPearson's $r = 0.27$), interrater agreement was low (Cohen's $\\kappa < 0.10$),\nand predictions were biased toward moderate or high trait levels.\nChain-of-thought prompting and longer input context modestly improved\ndistributional alignment, but not trait-level accuracy. These results\nunderscore limitations in current LLM-based personality inference and highlight\nthe need for evidence-based development for psychological applications.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.14398", "pdf": "https://arxiv.org/pdf/2507.14398", "abs": "https://arxiv.org/abs/2507.14398", "authors": ["Md. Kamrul Hossain", "Walid Aljoby"], "title": "NetIntent: Leveraging Large Language Models for End-to-End Intent-Based SDN Automation", "categories": ["cs.NI"], "comment": null, "summary": "Intent-Based Networking (IBN) often leverages the programmability of\nSoftware-Defined Networking (SDN) to simplify network management. However,\nsignificant challenges remain in automating the entire pipeline, from\nuser-specified high-level intents to device-specific low-level configurations.\nExisting solutions often rely on rigid, rule-based translators and fixed APIs,\nlimiting extensibility and adaptability. By contrast, recent advances in large\nlanguage models (LLMs) offer a promising pathway that leverages natural\nlanguage understanding and flexible reasoning. However, it is unclear to what\nextent LLMs can perform IBN tasks. To address this, we introduce IBNBench, a\nfirst-of-its-kind benchmarking suite comprising four novel datasets:\nIntent2Flow-ODL, Intent2Flow-ONOS, FlowConflict-ODL, and FlowConflict-ONOS.\nThese datasets are specifically designed for evaluating LLMs performance in\nintent translation and conflict detection tasks within the industry-grade SDN\ncontrollers ODL and ONOS. Our results provide the first comprehensive\ncomparison of 33 open-source LLMs on IBNBench and related datasets, revealing a\nwide range of performance outcomes. However, while these results demonstrate\nthe potential of LLMs for isolated IBN tasks, integrating LLMs into a fully\nautonomous IBN pipeline remains unexplored. Thus, our second contribution is\nNetIntent, a unified and adaptable framework that leverages LLMs to automate\nthe full IBN lifecycle, including translation, activation, and assurance within\nSDN systems. NetIntent orchestrates both LLM and non-LLM agents, supporting\ndynamic re-prompting and contextual feedback to robustly execute user-defined\nintents with minimal human intervention. Our implementation of NetIntent across\nboth ODL and ONOS SDN controllers achieves a consistent and adaptive end-to-end\nIBN realization.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165\u4e86IBNBench\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u610f\u56fe\u7ffb\u8bd1\u548c\u51b2\u7a81\u68c0\u6d4b\u7b49\u610f\u56fe\u9a71\u52a8\u7f51\u7edc\uff08IBN\uff09\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u63d0\u51faNetIntent\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528LLMs\u81ea\u52a8\u5316\u6574\u4e2aIBN\u751f\u547d\u5468\u671f\uff0c\u5b9e\u73b0\u4e86SDN\u7cfb\u7edf\u4e2d\u610f\u56fe\u7684\u7aef\u5230\u7aef\u5b9e\u73b0\u3002", "motivation": "\u5f53\u524d\u610f\u56fe\u9a71\u52a8\u7f51\u7edc\uff08IBN\uff09\u4ece\u9ad8\u7ea7\u610f\u56fe\u5230\u4f4e\u7ea7\u914d\u7f6e\u7684\u81ea\u52a8\u5316\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u56e0\u5176\u50f5\u5316\u548c\u6709\u9650\u7684\u6269\u5c55\u6027\u800c\u4e0d\u8db3\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u5728IBN\u4efb\u52a1\u4e2d\u7684\u5177\u4f53\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\uff0c\u4e14\u7f3a\u4e4f\u5c06LLMs\u6574\u5408\u5230\u5b8c\u6574\u81ea\u52a8\u5316IBN\u6d41\u7a0b\u4e2d\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u6784\u5efa\u4e86IBNBench\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542bIntent2Flow-ODL\u3001Intent2Flow-ONOS\u3001FlowConflict-ODL\u3001FlowConflict-ONOS\u56db\u4e2a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u610f\u56fe\u7ffb\u8bd1\u548c\u51b2\u7a81\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u7814\u7a76\u56e2\u961f\u5bf9\u6bd4\u4e8633\u4e2a\u5f00\u6e90LLMs\u7684\u8868\u73b0\u3002\u5176\u6b21\uff0c\u63d0\u51fa\u4e86NetIntent\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408LLMs\u548c\u975eLLM\u4ee3\u7406\uff0c\u5229\u7528\u52a8\u6001\u91cd\u65b0\u63d0\u793a\u548c\u4e0a\u4e0b\u6587\u53cd\u9988\u673a\u5236\uff0c\u81ea\u52a8\u5316\u4e86IBN\u7684\u5b8c\u6574\u751f\u547d\u5468\u671f\uff08\u5305\u62ec\u7ffb\u8bd1\u3001\u6fc0\u6d3b\u548c\u4fdd\u969c\uff09\u3002\u8be5\u6846\u67b6\u5df2\u5728ODL\u548cONOS SDN\u63a7\u5236\u5668\u4e0a\u5b9e\u73b0\u3002", "result": "IBNBench\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c33\u4e2a\u5f00\u6e90LLMs\u5728\u610f\u56fe\u9a71\u52a8\u7f51\u7edc\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u8bc1\u660e\u4e86LLMs\u5728\u72ec\u7acbIBN\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002NetIntent\u6846\u67b6\u5728ODL\u548cONOS SDN\u63a7\u5236\u5668\u4e0a\u7684\u5b9e\u73b0\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u81ea\u9002\u5e94\u7684\u7aef\u5230\u7aefIBN\u81ea\u52a8\u5316\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u90e8\u5206\u610f\u56fe\u9a71\u52a8\u7f51\u7edc\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002\u672c\u7814\u7a76\u6240\u63d0\u51fa\u7684NetIntent\u6846\u67b6\u6210\u529f\u5730\u5c06LLMs\u6574\u5408\u5230\u610f\u56fe\u9a71\u52a8\u7f51\u7edc\u7684\u5168\u751f\u547d\u5468\u671f\u7ba1\u7406\u4e2d\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u3001\u7075\u6d3b\u4e14\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86SDN\u7cfb\u7edf\u4e2d\u7528\u6237\u5b9a\u4e49\u610f\u56fe\u7684\u9c81\u68d2\u6267\u884c\u3002"}}
{"id": "2507.14468", "pdf": "https://arxiv.org/pdf/2507.14468", "abs": "https://arxiv.org/abs/2507.14468", "authors": ["Yitong Lin", "Jiaying He", "Jiahe Chen", "Xinnan Zhu", "Jianwei Zheng", "Tao Bo"], "title": "BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning", "categories": ["cs.AI"], "comment": "Accepted by Bioinformatics on July 11th", "summary": "Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery\nand disease understanding, yet their completion and reasoning are challenging.\nKnowledge Embedding (KE) methods capture global semantics but struggle with\ndynamic structural integration, while Graph Neural Networks (GNNs) excel\nlocally but often lack semantic understanding. Even ensemble approaches,\nincluding those leveraging language models, often fail to achieve a deep,\nadaptive, and synergistic co-evolution between semantic comprehension and\nstructural learning. Addressing this critical gap in fostering continuous,\nreciprocal refinement between these two aspects in complex biomedical KGs is\nparamount.\n  Results: We introduce BioGraphFusion, a novel framework for deeply\nsynergistic semantic and structural learning. BioGraphFusion establishes a\nglobal semantic foundation via tensor decomposition, guiding an LSTM-driven\nmechanism to dynamically refine relation embeddings during graph propagation.\nThis fosters adaptive interplay between semantic understanding and structural\nlearning, further enhanced by query-guided subgraph construction and a hybrid\nscoring mechanism. Experiments across three key biomedical tasks demonstrate\nBioGraphFusion's superior performance over state-of-the-art KE, GNN, and\nensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)\nhighlights its ability to unveil biologically meaningful pathways.\n  Availability and Implementation: Source code and all training data are freely\navailable for download at https://github.com/Y-TARL/BioGraphFusion.\n  Contact: zjw@zjut.edu.cn, botao666666@126.com.\n  Supplementary information: Supplementary data are available at Bioinformatics\nonline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86BioGraphFusion\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u4e2d\u8bed\u4e49\u7406\u89e3\u4e0e\u7ed3\u6784\u5b66\u4e60\u96be\u4ee5\u6df1\u5ea6\u534f\u540c\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u878d\u5408\u5f20\u91cf\u5206\u89e3\u3001LSTM\u7b49\u65b9\u6cd5\uff0c\u5728\u591a\u9879\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709SOTA\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u5bf9\u4e8e\u836f\u7269\u53d1\u73b0\u548c\u75be\u75c5\u7406\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u8865\u5168\u548c\u63a8\u7406\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u77e5\u8bc6\u5d4c\u5165\uff08KE\uff09\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u52a8\u6001\u7ed3\u6784\u96c6\u6210\uff0c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7f3a\u4e4f\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\uff0c\u800c\u73b0\u6709\u96c6\u6210\u65b9\u6cd5\u4e5f\u672a\u80fd\u5b9e\u73b0\u8bed\u4e49\u4e0e\u7ed3\u6784\u5b66\u4e60\u7684\u6df1\u5ea6\u3001\u81ea\u9002\u5e94\u548c\u534f\u540c\u6f14\u5316\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u89e3\u51b3\u5728\u590d\u6742\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u4e2d\u4fc3\u8fdb\u8fd9\u4e24\u65b9\u9762\u6301\u7eed\u3001\u76f8\u4e92\u7cbe\u70bc\u7684\u5173\u952e\u7a7a\u767d\u3002", "method": "BioGraphFusion\u662f\u4e00\u4e2a\u65b0\u578b\u6846\u67b6\uff0c\u65e8\u5728\u5b9e\u73b0\u8bed\u4e49\u548c\u7ed3\u6784\u5b66\u4e60\u7684\u6df1\u5ea6\u534f\u540c\u3002\u5b83\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u5efa\u7acb\u5168\u5c40\u8bed\u4e49\u57fa\u7840\uff0c\u5e76\u5f15\u5bfc\u4e00\u4e2a\u57fa\u4e8eLSTM\u7684\u673a\u5236\u5728\u56fe\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u52a8\u6001\u7ec6\u5316\u5173\u7cfb\u5d4c\u5165\u3002\u8fd9\u4fc3\u8fdb\u4e86\u8bed\u4e49\u7406\u89e3\u548c\u7ed3\u6784\u5b66\u4e60\u4e4b\u95f4\u7684\u81ea\u9002\u5e94\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u67e5\u8be2\u5f15\u5bfc\u7684\u5b50\u56fe\u6784\u5efa\u548c\u6df7\u5408\u8bc4\u5206\u673a\u5236\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6548\u679c\u3002", "result": "BioGraphFusion\u5728\u4e09\u4e2a\u5173\u952e\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u77e5\u8bc6\u5d4c\u5165\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u96c6\u6210\u6a21\u578b\u3002\u4e00\u9879\u5173\u4e8e\u76ae\u80a4\u6076\u6027\u9ed1\u8272\u7d20\u76241\uff08CMM1\uff09\u7684\u6848\u4f8b\u7814\u7a76\u7a81\u51fa\u663e\u793a\u4e86\u5176\u63ed\u793a\u751f\u7269\u5b66\u6709\u610f\u4e49\u901a\u8def\u7684\u80fd\u529b\u3002", "conclusion": "BioGraphFusion\u6210\u529f\u5730\u89e3\u51b3\u4e86\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u4e2d\u8bed\u4e49\u7406\u89e3\u4e0e\u7ed3\u6784\u5b66\u4e60\u534f\u540c\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5176\u521b\u65b0\u7684\u878d\u5408\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u63ed\u793a\u751f\u7269\u5b66\u610f\u4e49\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u5206\u6790\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.14454", "pdf": "https://arxiv.org/pdf/2507.14454", "abs": "https://arxiv.org/abs/2507.14454", "authors": ["Han Gong", "Qiyue Li", "Jie Li", "Zhi Liu"], "title": "Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": null, "summary": "3D Gaussian splatting video (3DGS) streaming has recently emerged as a\nresearch hotspot in both academia and industry, owing to its impressive ability\nto deliver immersive 3D video experiences. However, research in this area is\nstill in its early stages, and several fundamental challenges, such as tiling,\nquality assessment, and bitrate adaptation, require further investigation. In\nthis paper, we tackle these challenges by proposing a comprehensive set of\nsolutions. Specifically, we propose an adaptive 3DGS tiling technique guided by\nsaliency analysis, which integrates both spatial and temporal features. Each\ntile is encoded into versions possessing dedicated deformation fields and\nmultiple quality levels for adaptive selection. We also introduce a novel\nquality assessment framework for 3DGS video that jointly evaluates\nspatial-domain degradation in 3DGS representations during streaming and the\nquality of the resulting 2D rendered images. Additionally, we develop a\nmeta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS\nvideo streaming, achieving optimal performance across varying network\nconditions. Extensive experiments demonstrate that our proposed approaches\nsignificantly outperform state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf93D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u89c6\u9891\u6d41\u5a92\u4f53\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u5206\u5757\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u7801\u7387\u81ea\u9002\u5e94\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u89c6\u9891\u6d41\u5a92\u4f53\u56e0\u5176\u6c89\u6d78\u5f0f\u4f53\u9a8c\u800c\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\uff0c\u4f46\u76ee\u524d\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u5728\u5206\u5757\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u7801\u7387\u81ea\u9002\u5e94\u7b49\u65b9\u9762\u5b58\u5728\u8bf8\u591a\u57fa\u672c\u6311\u6218\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\uff1a1) \u57fa\u4e8e\u663e\u8457\u6027\u5206\u6790\u5e76\u878d\u5408\u65f6\u7a7a\u7279\u5f81\u7684\u81ea\u9002\u5e943DGS\u5206\u5757\u6280\u672f\uff0c\u6bcf\u4e2a\u5206\u5757\u7f16\u7801\u4e3a\u5305\u542b\u4e13\u7528\u5f62\u53d8\u573a\u548c\u591a\u8d28\u91cf\u7ea7\u522b\u7684\u7248\u672c\uff1b2) \u4e00\u79cd\u65b0\u9896\u76843DGS\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u540c\u65f6\u8bc4\u4f30\u6d41\u5a92\u4f53\u4f20\u8f93\u8fc7\u7a0b\u4e2d3DGS\u8868\u793a\u7684\u7a7a\u95f4\u57df\u9000\u5316\u548c\u6700\u7ec82D\u6e32\u67d3\u56fe\u50cf\u7684\u8d28\u91cf\uff1b3) \u4e00\u79cd\u4e13\u4e3a3DGS\u89c6\u9891\u6d41\u5a92\u4f53\u8bbe\u8ba1\u7684\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u7801\u7387\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u4e3a\u89e3\u51b33DGS\u89c6\u9891\u6d41\u5a92\u4f53\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u5957\u7efc\u5408\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u5176\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2507.14182", "pdf": "https://arxiv.org/pdf/2507.14182", "abs": "https://arxiv.org/abs/2507.14182", "authors": ["Xiaotong Luo", "Shengda Zhuo", "Min Chen", "Lichun Li", "Ruizhao Lu", "Wenqi Fan", "Shuqiang Huang", "Yin Tang"], "title": "From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Financial markets exhibit highly dynamic and complex behaviors shaped by both\nhistorical price trajectories and exogenous narratives, such as news, policy\ninterpretations, and social media sentiment. The heterogeneity in these data\nand the diverse insight of investors introduce biases that complicate the\nmodeling of market dynamics. Unlike prior work, this paper explores the\npotential of bull and bear regimes in investor-driven market dynamics. Through\nempirical analysis on real-world financial datasets, we uncover a dynamic\nrelationship between bias variation and behavioral adaptation, which enhances\ntrend prediction under evolving market conditions. To model this mechanism, we\npropose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified\nframework that jointly embeds temporal price sequences and external contextual\nsignals into a shared latent space where opposing bull and bear forces\nnaturally emerge, forming the foundation for bias representation. Within this\nspace, an inertial pairing module pairs temporally adjacent samples to preserve\nmomentum, while the dual competition mechanism contrasts bullish and bearish\nembeddings to capture behavioral divergence. Together, these components allow\nB4 to model bias-driven asymmetry, behavioral inertia, and market\nheterogeneity. Experimental results on real-world financial datasets\ndemonstrate that our model not only achieves superior performance in predicting\nmarket trends but also provides interpretable insights into the interplay of\nbiases, investor behaviors, and market dynamics.", "AI": {"tldr": "\u63d0\u51faB4\u6a21\u578b\uff0c\u5229\u7528\u725b\u5e02/\u718a\u5e02\u52a8\u6001\u63ed\u793a\u6295\u8d44\u8005\u504f\u89c1\u4e0e\u884c\u4e3a\u9002\u5e94\u95f4\u7684\u52a8\u6001\u5173\u7cfb\uff0c\u4ece\u800c\u63d0\u5347\u91d1\u878d\u5e02\u573a\u8d8b\u52bf\u9884\u6d4b\u80fd\u529b\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u6d1e\u5bdf\u3002", "motivation": "\u91d1\u878d\u5e02\u573a\u53d7\u590d\u6742\u52a8\u6001\u548c\u5916\u90e8\u53d9\u4e8b\u5f71\u54cd\uff0c\u6570\u636e\u5f02\u8d28\u6027\u548c\u6295\u8d44\u8005\u6d1e\u5bdf\u5dee\u5f02\u5f15\u5165\u504f\u89c1\uff0c\u4f7f\u5e02\u573a\u5efa\u6a21\u590d\u6742\u5316\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u725b\u5e02/\u718a\u5e02\u673a\u5236\u5728\u6295\u8d44\u8005\u9a71\u52a8\u7684\u5e02\u573a\u52a8\u6001\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u589e\u5f3a\u5728\u591a\u53d8\u5e02\u573a\u6761\u4ef6\u4e0b\u7684\u8d8b\u52bf\u9884\u6d4b\u3002", "method": "\u63d0\u51fa\u201c\u504f\u89c1\u9a71\u52a8\u884c\u4e3a\u7684\u725b\u718a\u52a8\u6001\u6a21\u578b\u201d\uff08B4\uff09\uff0c\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u65f6\u95f4\u4ef7\u683c\u5e8f\u5217\u548c\u5916\u90e8\u60c5\u5883\u4fe1\u53f7\u5d4c\u5165\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u4f7f\u725b\u5e02/\u718a\u5e02\u529b\u91cf\u81ea\u7136\u6d8c\u73b0\u4ee5\u8868\u793a\u504f\u89c1\u3002\u6a21\u578b\u5305\u542b\u60ef\u6027\u914d\u5bf9\u6a21\u5757\uff08\u4fdd\u7559\u52a8\u91cf\uff09\u548c\u53cc\u91cd\u7ade\u4e89\u673a\u5236\uff08\u6355\u6349\u884c\u4e3a\u5dee\u5f02\uff09\uff0c\u4ee5\u5efa\u6a21\u504f\u89c1\u9a71\u52a8\u7684\u4e0d\u5bf9\u79f0\u6027\u3001\u884c\u4e3a\u60ef\u6027\u548c\u5e02\u573a\u5f02\u8d28\u6027\u3002", "result": "\u5728\u5b9e\u9645\u91d1\u878d\u6570\u636e\u96c6\u4e0a\uff0cB4\u6a21\u578b\u5728\u5e02\u573a\u8d8b\u52bf\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u63d0\u4f9b\u5bf9\u504f\u89c1\u3001\u6295\u8d44\u8005\u884c\u4e3a\u4e0e\u5e02\u573a\u52a8\u6001\u76f8\u4e92\u4f5c\u7528\u7684\u53ef\u89e3\u91ca\u6027\u6d1e\u5bdf\u3002", "conclusion": "B4\u6a21\u578b\u901a\u8fc7\u6574\u5408\u504f\u89c1\u3001\u6295\u8d44\u8005\u884c\u4e3a\u53ca\u725b\u718a\u673a\u5236\uff0c\u6210\u529f\u5efa\u6a21\u4e86\u590d\u6742\u7684\u5e02\u573a\u52a8\u6001\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e02\u573a\u8d8b\u52bf\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.14372", "pdf": "https://arxiv.org/pdf/2507.14372", "abs": "https://arxiv.org/abs/2507.14372", "authors": ["Albert Chen", "Manas Bundele", "Gaurav Ahlawat", "Patrick Stetz", "Zhitao Wang", "Qiang Fei", "Donghoon Jung", "Audrey Chu", "Bharadwaj Jayaraman", "Ayushi Panth", "Yatin Arora", "Sourav Jain", "Renjith Varma", "Alexey Ilin", "Iuliia Melnychuk", "Chelsea Chueh", "Joyan Sil", "Xiaofeng Wang"], "title": "Text-to-SQL for Enterprise Data Analytics", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "comment": "11 pages, 8 figures, Workshop on Agentic AI for Enterprise at KDD '25", "summary": "The introduction of large language models has brought rapid progress on\nText-to-SQL benchmarks, but it is not yet easy to build a working enterprise\nsolution. In this paper, we present insights from building an internal chatbot\nthat enables LinkedIn's product managers, engineers, and operations teams to\nself-serve data insights from a large, dynamic data lake. Our approach features\nthree components. First, we construct a knowledge graph that captures\nup-to-date semantics by indexing database metadata, historical query logs,\nwikis, and code. We apply clustering to identify relevant tables for each team\nor product area. Second, we build a Text-to-SQL agent that retrieves and ranks\ncontext from the knowledge graph, writes a query, and automatically corrects\nhallucinations and syntax errors. Third, we build an interactive chatbot that\nsupports various user intents, from data discovery to query writing to\ndebugging, and displays responses in rich UI elements to encourage follow-up\nchats. Our chatbot has over 300 weekly users. Expert review shows that 53% of\nits responses are correct or close to correct on an internal benchmark set.\nThrough ablation studies, we identify the most important knowledge graph and\nmodeling components, offering a practical path for developing enterprise\nText-to-SQL solutions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u4e3aLinkedIn\u5185\u90e8\u56e2\u961f\u6784\u5efa\u7684\u4f01\u4e1a\u7ea7Text-to-SQL\u804a\u5929\u673a\u5668\u4eba\uff0c\u8be5\u673a\u5668\u4eba\u7ed3\u5408\u4e86\u77e5\u8bc6\u56fe\u8c31\u548cText-to-SQL\u4ee3\u7406\uff0c\u65e8\u5728\u5e2e\u52a9\u7528\u6237\u81ea\u52a9\u83b7\u53d6\u6570\u636e\u6d1e\u5bdf\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728Text-to-SQL\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u5feb\u901f\u8fdb\u5c55\uff0c\u4f46\u6784\u5efa\u4e00\u4e2a\u53ef\u5de5\u4f5c\u7684\u4f01\u4e1a\u7ea7\u89e3\u51b3\u65b9\u6848\u4ecd\u7136\u4e0d\u6613\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1. \u6784\u5efa\u4e00\u4e2a\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u7d22\u5f15\u6570\u636e\u5e93\u5143\u6570\u636e\u3001\u5386\u53f2\u67e5\u8be2\u65e5\u5fd7\u3001Wiki\u548c\u4ee3\u7801\u6765\u6355\u83b7\u8bed\u4e49\uff0c\u5e76\u5e94\u7528\u805a\u7c7b\u8bc6\u522b\u76f8\u5173\u8868\u30022. \u6784\u5efa\u4e00\u4e2aText-to-SQL\u4ee3\u7406\uff0c\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u548c\u6392\u5e8f\u4e0a\u4e0b\u6587\uff0c\u751f\u6210\u67e5\u8be2\uff0c\u5e76\u81ea\u52a8\u7ea0\u6b63\u5e7b\u89c9\u548c\u8bed\u6cd5\u9519\u8bef\u30023. \u5efa\u7acb\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u804a\u5929\u673a\u5668\u4eba\uff0c\u652f\u6301\u6570\u636e\u53d1\u73b0\u3001\u67e5\u8be2\u7f16\u5199\u548c\u8c03\u8bd5\u7b49\u591a\u79cd\u7528\u6237\u610f\u56fe\uff0c\u5e76\u4ee5\u5bccUI\u5143\u7d20\u663e\u793a\u54cd\u5e94\u3002", "result": "\u8be5\u804a\u5929\u673a\u5668\u4eba\u6bcf\u5468\u6709\u8d85\u8fc7300\u540d\u6d3b\u8dc3\u7528\u6237\u3002\u4e13\u5bb6\u8bc4\u5ba1\u663e\u793a\uff0c\u5176\u5728\u5185\u90e8\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a53%\u7684\u54cd\u5e94\u662f\u6b63\u786e\u6216\u63a5\u8fd1\u6b63\u786e\u7684\u3002\u901a\u8fc7\u6d88\u878d\u7814\u7a76\uff0c\u8bba\u6587\u786e\u5b9a\u4e86\u6700\u91cd\u8981\u7684\u77e5\u8bc6\u56fe\u8c31\u548c\u5efa\u6a21\u7ec4\u4ef6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u4e3a\u5f00\u53d1\u4f01\u4e1a\u7ea7Text-to-SQL\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u7684\u9014\u5f84\u3002"}}
{"id": "2507.14512", "pdf": "https://arxiv.org/pdf/2507.14512", "abs": "https://arxiv.org/abs/2507.14512", "authors": ["Qiyuan Peng", "Qi Zhang", "Yue Gao", "Kun Qiu"], "title": "Dora: A Controller Provisioning Strategy in Hierarchical Domain-based Satellite Networks", "categories": ["cs.NI"], "comment": null, "summary": "The rapid proliferation of satellite constellations in Space-Air-Ground\nIntegrated Networks (SAGIN) presents significant challenges for network\nmanagement. Conventional flat network architectures struggle with\nsynchronization and data transmission across massive distributed nodes. In\nresponse, hierarchical domain-based satellite network architectures have\nemerged as a scalable solution, highlighting the critical importance of\ncontroller provisioning strategies. However, existing network management\narchitectures and traditional search-based algorithms fail to generate\nefficient controller provisioning solutions due to limited computational\nresources in satellites and strict time constraints. To address these\nchallenges, we propose a three-layer domain-based architecture that enhances\nboth scalability and adaptability. Furthermore, we introduce Dora, a\nreinforcement learning-based controller provisioning strategy designed to\noptimize network performance while minimizing computational overhead. Our\ncomprehensive experimental evaluation demonstrates that Dora significantly\noutperforms state-of-the-art benchmarks, achieving 10% improvement in\ncontroller provisioning quality while requiring only 1/30 to 1/90 of the\ncomputation time compared to traditional algorithms. These results underscore\nthe potential of reinforcement learning approaches for efficient satellite\nnetwork management in next-generation SAGIN deployments.", "AI": {"tldr": "\u4e3a\u89e3\u51b3SAGIN\u4e2d\u536b\u661f\u7f51\u7edc\u63a7\u5236\u5668\u914d\u7f6e\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e09\u5c42\u57df\u67b6\u6784\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684Dora\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u914d\u7f6e\u8d28\u91cf\u5e76\u5927\u5e45\u7f29\u77ed\u4e86\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u7a7a\u5929\u5730\u4e00\u4f53\u5316\u7f51\u7edc\uff08SAGIN\uff09\u4e2d\u536b\u661f\u661f\u5ea7\u7684\u5feb\u901f\u90e8\u7f72\uff0c\u5bf9\u7f51\u7edc\u7ba1\u7406\u63d0\u51fa\u4e86\u5de8\u5927\u6311\u6218\u3002\u4f20\u7edf\u7684\u6241\u5e73\u7f51\u7edc\u67b6\u6784\u96be\u4ee5\u5e94\u5bf9\u6d77\u91cf\u5206\u5e03\u5f0f\u8282\u70b9\u7684\u540c\u6b65\u548c\u6570\u636e\u4f20\u8f93\u3002\u73b0\u6709\u7f51\u7edc\u7ba1\u7406\u67b6\u6784\u53ca\u4f20\u7edf\u641c\u7d22\u7b97\u6cd5\uff0c\u7531\u4e8e\u536b\u661f\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u548c\u4e25\u683c\u7684\u65f6\u95f4\u9650\u5236\uff0c\u65e0\u6cd5\u9ad8\u6548\u751f\u6210\u63a7\u5236\u5668\u914d\u7f6e\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u4e09\u5c42\u57df\u67b6\u6784\uff0c\u4ee5\u589e\u5f3a\u7f51\u7edc\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684Dora\u63a7\u5236\u5668\u914d\u7f6e\u7b56\u7565\uff0c\u65e8\u5728\u4f18\u5316\u7f51\u7edc\u6027\u80fd\u540c\u65f6\u6700\u5c0f\u5316\u8ba1\u7b97\u5f00\u9500\u3002", "result": "Dora\u7b56\u7565\u5728\u63a7\u5236\u5668\u914d\u7f6e\u8d28\u91cf\u4e0a\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u51c6\u7b97\u6cd5\u63d0\u9ad8\u4e8610%\uff0c\u540c\u65f6\u8ba1\u7b97\u65f6\u95f4\u4ec5\u4e3a\u4f20\u7edf\u7b97\u6cd5\u76841/30\u52301/90\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4e0b\u4e00\u4ee3SAGIN\u90e8\u7f72\u4e2d\u5b9e\u73b0\u9ad8\u6548\u536b\u661f\u7f51\u7edc\u7ba1\u7406\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.14513", "pdf": "https://arxiv.org/pdf/2507.14513", "abs": "https://arxiv.org/abs/2507.14513", "authors": ["Hongyi Yang", "Yue Pan", "Jiayi Xu", "Kelsen Liu"], "title": "Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) and autonomous agents have\nenabled systems capable of performing complex tasks across domains such as\nhuman-computer interaction, planning, and web navigation. However, many\nexisting frameworks struggle in real-world or resource-constrained environments\ndue to their reliance on cloud-based computation, limited robustness in dynamic\ncontexts, and lack of persistent autonomy and environmental awareness.\n  We present Amico, a modular, event-driven framework for building autonomous\nagents optimized for embedded systems. Written in Rust for safety and\nperformance, Amico supports reactive, persistent agents that operate\nefficiently across embedded platforms and browser environments via WebAssembly.\nIt provides clean abstractions for event handling, state management, behavior\nexecution, and integration with reasoning modules. Amico delivers a unified\ninfrastructure for constructing resilient, interactive agents suitable for\ndeployment in settings with limited compute and intermittent connectivity.", "AI": {"tldr": "Amico\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u4e8b\u4ef6\u9a71\u52a8\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4e13\u4e3a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4f18\u5316\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u548c\u667a\u80fd\u4f53\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u81ea\u4e3b\u667a\u80fd\u4f53\u6846\u67b6\u5728\u771f\u5b9e\u4e16\u754c\u6216\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\uff08\u5982\u4f9d\u8d56\u4e91\u7aef\u8ba1\u7b97\u3001\u7f3a\u4e4f\u9c81\u68d2\u6027\u3001\u6301\u4e45\u81ea\u4e3b\u6027\u548c\u73af\u5883\u611f\u77e5\u80fd\u529b\uff09\u9762\u4e34\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Amico\u6846\u67b6\uff0c\u5b83\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u4e8b\u4ef6\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u4e13\u4e3a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u6784\u5efa\u81ea\u4e3b\u667a\u80fd\u4f53\u3002\u8be5\u6846\u67b6\u91c7\u7528Rust\u8bed\u8a00\u5f00\u53d1\uff0c\u5e76\u652f\u6301\u901a\u8fc7WebAssembly\u5728\u5d4c\u5165\u5f0f\u5e73\u53f0\u548c\u6d4f\u89c8\u5668\u73af\u5883\u4e2d\u9ad8\u6548\u8fd0\u884c\u3002Amico\u63d0\u4f9b\u4e86\u4e8b\u4ef6\u5904\u7406\u3001\u72b6\u6001\u7ba1\u7406\u3001\u884c\u4e3a\u6267\u884c\u548c\u63a8\u7406\u6a21\u5757\u96c6\u6210\u7684\u62bd\u8c61\u3002", "result": "Amico\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u7528\u4e8e\u6784\u5efa\u5f39\u6027\u3001\u4ea4\u4e92\u5f0f\u7684\u667a\u80fd\u4f53\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u548c\u8fde\u63a5\u95f4\u6b47\u7684\u73af\u5883\u90e8\u7f72\u3002", "conclusion": "Amico\u6846\u67b6\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u548c\u8fde\u63a5\u4e0d\u7a33\u5b9a\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u5065\u58ee\u3001\u4ea4\u4e92\u5f0f\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f18\u5316\u4e14\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14456", "pdf": "https://arxiv.org/pdf/2507.14456", "abs": "https://arxiv.org/abs/2507.14456", "authors": ["Chi Wan", "Yixin Cui", "Jiatong Du", "Shuo Yang", "Yulong Bai", "Yanjun Huang"], "title": "GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "End-to-end autonomous driving requires adaptive and robust handling of\ncomplex and diverse traffic environments. However, prevalent single-mode\nplanning methods attempt to learn an overall policy while struggling to acquire\ndiversified driving skills to handle diverse scenarios. Therefore, this paper\nproposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework\nfeaturing a Global Expert, a Scene-Adaptive Experts Group, and equipped with a\nDual-aware Router. Specifically, the Global Expert is trained on the overall\ndataset, possessing robust performance. The Scene-Adaptive Experts are trained\non corresponding scene subsets, achieving adaptive performance. The Dual-aware\nRouter simultaneously considers scenario-level features and routing uncertainty\nto dynamically activate expert modules. Through the effective coupling of the\nGlobal Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,\nGEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS\noutperforms existing methods in the Bench2Drive closed-loop benchmark and\nachieves state-of-the-art performance in Driving Score and Success Rate, even\nwith only monocular vision input. Furthermore, ablation studies demonstrate\nsignificant improvements over the original single-expert baseline: 7.67% in\nDriving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The\ncode will be available at https://github.com/newbrains1/GEMINUS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86GEMINUS\uff0c\u4e00\u4e2a\u6df7\u5408\u4e13\u5bb6\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u4e13\u5bb6\u3001\u573a\u666f\u81ea\u9002\u5e94\u4e13\u5bb6\u7ec4\u548c\u53cc\u611f\u77e5\u8def\u7531\u5668\uff0c\u5b9e\u73b0\u5728\u590d\u6742\u591a\u6837\u7684\u4ea4\u901a\u73af\u5883\u4e2d\u81ea\u9002\u5e94\u4e14\u9c81\u68d2\u7684\u9a7e\u9a76\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u9700\u5904\u7406\u590d\u6742\u591a\u6837\u7684\u4ea4\u901a\u73af\u5883\uff0c\u4f46\u73b0\u6709\u5355\u6a21\u6001\u89c4\u5212\u65b9\u6cd5\u96be\u4ee5\u5b66\u4e60\u591a\u6837\u5316\u9a7e\u9a76\u6280\u80fd\u4ee5\u5e94\u5bf9\u5404\u7c7b\u573a\u666f\u3002", "method": "\u63d0\u51faGEMINUS\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u5168\u5c40\u4e13\u5bb6\uff0c\u8bad\u7ec3\u4e8e\u6574\u4f53\u6570\u636e\u96c6\u4ee5\u5b9e\u73b0\u9c81\u68d2\u6027\uff1b2) \u573a\u666f\u81ea\u9002\u5e94\u4e13\u5bb6\u7ec4\uff0c\u8bad\u7ec3\u4e8e\u5bf9\u5e94\u573a\u666f\u5b50\u96c6\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u6027\uff1b3) \u53cc\u611f\u77e5\u8def\u7531\u5668\uff0c\u540c\u65f6\u8003\u8651\u573a\u666f\u7279\u5f81\u548c\u8def\u7531\u4e0d\u786e\u5b9a\u6027\u4ee5\u52a8\u6001\u6fc0\u6d3b\u4e13\u5bb6\u3002\u901a\u8fc7\u8def\u7531\u5668\u6709\u6548\u8026\u5408\u5168\u5c40\u4e0e\u573a\u666f\u4e13\u5bb6\u3002", "result": "GEMINUS\u5728Bench2Drive\u95ed\u73af\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u9a7e\u9a76\u5206\u6570\u548c\u6210\u529f\u7387\u4e0a\u8fbe\u5230SOTA\uff0c\u5373\u4f7f\u4ec5\u7528\u5355\u76ee\u89c6\u89c9\u3002\u6d88\u878d\u7814\u7a76\u663e\u793a\uff0c\u76f8\u8f83\u4e8e\u5355\u4e13\u5bb6\u57fa\u7ebf\uff0c\u9a7e\u9a76\u5206\u6570\u63d0\u53477.67%\uff0c\u6210\u529f\u7387\u63d0\u534722.06%\uff0cMultiAbility-Mean\u63d0\u534719.41%\u3002", "conclusion": "GEMINUS\u901a\u8fc7\u5176\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u7684\u81ea\u9002\u5e94\u548c\u9c81\u68d2\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002"}}
{"id": "2507.14204", "pdf": "https://arxiv.org/pdf/2507.14204", "abs": "https://arxiv.org/abs/2507.14204", "authors": ["Dachuan Shi", "Yonggan Fu", "Xiangchi Yuan", "Zhongzhi Yu", "Haoran You", "Sixu Li", "Xin Dong", "Jan Kautz", "Pavlo Molchanov", "Yingyan", "Lin"], "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025. Code: https://github.com/GATECH-EIC/LaCache", "summary": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache.", "AI": {"tldr": "\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5904\u7406\u957f\u5e8f\u5217\u65f6KV\u7f13\u5b58\u6548\u7387\u74f6\u9888\u548c\u5185\u5b58\u6ea2\u51fa\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684KV\u7f13\u5b58\u4f18\u5316\u8303\u5f0fLaCache\uff0c\u901a\u8fc7\u201c\u9636\u68af\u5f62\u201dKV\u7f13\u5b58\u6a21\u5f0f\u548c\u8fed\u4ee3\u538b\u7f29\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u957f\u7a0b\u80fd\u529b\u548c\u8fde\u7eed\u751f\u6210\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\uff0cKey-Value (KV) \u5bf9\u7684\u6570\u91cf\u6025\u5267\u589e\u52a0\uff0c\u5bfc\u81f4\u663e\u8457\u7684\u6548\u7387\u74f6\u9888\u3002LLMs\u4e9f\u9700\u5f3a\u5927\u7684\u957f\u7a0b\u80fd\u529b\u6765\u5904\u7406\u5927\u89c4\u6a21\u8f93\u5165\u4e0a\u4e0b\u6587\u548c\u6301\u7eed\u751f\u6210\u6269\u5c55\u8f93\u51fa\uff0c\u540c\u65f6\u907f\u514d\u5185\u5b58\u6ea2\u51fa\uff08OOM\uff09\u3002", "method": "\u672c\u6587\u63d0\u51faLaCache\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684KV\u7f13\u5b58\u4f18\u5316\u8303\u5f0f\uff0c\u65e8\u5728\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684LLM\u751f\u6210\u5f0f\u63a8\u7406\u3002LaCache\u878d\u5408\u4e86\u4e24\u9879\u5173\u952e\u521b\u65b0\uff1a1) \u9636\u68af\u5f62KV\u7f13\u5b58\u6a21\u5f0f\uff1aKV\u5bf9\u4e0d\u4ec5\u5728\u5c42\u5185\u987a\u5e8f\u5b58\u50a8\uff0c\u8fd8\u5728\u5c42\u95f4\uff08\u4ece\u6d45\u5230\u6df1\uff09\u5b58\u50a8\uff0c\u5728\u56fa\u5b9a\u5b58\u50a8\u9884\u7b97\u4e0b\u6269\u5c55\u4e86\u6355\u83b7\u957f\u7a0b\u4f9d\u8d56\u7684\u8303\u56f4\u30022) \u8fed\u4ee3\u538b\u7f29\u673a\u5236\uff1a\u9010\u6b65\u538b\u7f29\u65e7\u7f13\u5b58\uff0c\u4e3a\u65b0token\u817e\u51fa\u7a7a\u95f4\uff0c\u5b9e\u73b0\u57fa\u4e8etoken\u8ddd\u79bb\u7684\u52a8\u6001\u538b\u7f29\uff0c\u4ee5\u5728\u6709\u9650\u7f13\u5b58\u9884\u7b97\u4e0b\u66f4\u6709\u6548\u5730\u8fde\u7eed\u751f\u6210\u3002", "result": "\u5728\u5404\u79cd\u4efb\u52a1\u3001\u57fa\u51c6\u548cLLM\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u4e00\u81f4\u9a8c\u8bc1\u4e86LaCache\u5728\u589e\u5f3aLLMs\u957f\u7a0b\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "LaCache\u6210\u529f\u5730\u5728\u56fa\u5b9a\u5b58\u50a8\u9884\u7b97\u4e0b\uff0c\u901a\u8fc7\u521b\u65b0\u7684KV\u7f13\u5b58\u7ba1\u7406\uff0c\u540c\u65f6\u89e3\u51b3\u4e86LLMs\u5728\u957f\u7a0b\u5efa\u6a21\u4e2d\u9c81\u68d2\u7684\u957f\u7a0b\u80fd\u529b\u548c\u8fde\u7eed\u751f\u6210\u65f6\u907f\u514d\u5185\u5b58\u6ea2\u51fa\u7684\u4e24\u5927\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2507.14374", "pdf": "https://arxiv.org/pdf/2507.14374", "abs": "https://arxiv.org/abs/2507.14374", "authors": ["Sinchani Chakraborty", "Sudeshna Sarkar", "Pawan Goyal"], "title": "Error-Aware Curriculum Learning for Biomedical Relation Classification", "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "Relation Classification (RC) in biomedical texts is essential for\nconstructing knowledge graphs and enabling applications such as drug\nrepurposing and clinical decision-making. We propose an error-aware\nteacher--student framework that improves RC through structured guidance from a\nlarge language model (GPT-4o). Prediction failures from a baseline student\nmodel are analyzed by the teacher to classify error types, assign difficulty\nscores, and generate targeted remediations, including sentence rewrites and\nsuggestions for KG-based enrichment. These enriched annotations are used to\ntrain a first student model via instruction tuning. This model then annotates a\nbroader dataset with difficulty scores and remediation-enhanced inputs. A\nsecond student is subsequently trained via curriculum learning on this dataset,\nordered by difficulty, to promote robust and progressive learning. We also\nconstruct a heterogeneous biomedical knowledge graph from PubMed abstracts to\nsupport context-aware RC. Our approach achieves new state-of-the-art\nperformance on 4 of 5 PPI datasets and the DDI dataset, while remaining\ncompetitive on ChemProt.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9519\u8bef\u611f\u77e5\u7684\u5e08\u751f\u6846\u67b6\uff0c\u5229\u7528GPT-4o\u6307\u5bfc\u63d0\u5347\u751f\u7269\u533b\u5b66\u6587\u672c\u5173\u7cfb\u5206\u7c7b\u6027\u80fd\uff0c\u5b9e\u73b0\u591a\u9879SOTA\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u6587\u672c\u4e2d\u7684\u5173\u7cfb\u5206\u7c7b\uff08RC\uff09\u5bf9\u4e8e\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u4ee5\u53ca\u652f\u6301\u836f\u7269\u518d\u5229\u7528\u3001\u4e34\u5e8a\u51b3\u7b56\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9519\u8bef\u611f\u77e5\u7684\u5e08\u751f\u6846\u67b6\u3002\u5176\u4e2d\uff0c\u4e00\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT-4o\uff09\u5145\u5f53\u6559\u5e08\uff0c\u5206\u6790\u57fa\u7ebf\u5b66\u751f\u6a21\u578b\u7684\u9884\u6d4b\u5931\u8d25\uff0c\u5206\u7c7b\u9519\u8bef\u7c7b\u578b\uff0c\u8bc4\u4f30\u96be\u5ea6\uff0c\u5e76\u751f\u6210\u5305\u62ec\u53e5\u5b50\u91cd\u5199\u548c\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u7684\u5bcc\u96c6\u5efa\u8bae\u5728\u5185\u7684\u8865\u6551\u63aa\u65bd\u3002\u8fd9\u4e9b\u589e\u5f3a\u7684\u6807\u6ce8\u7528\u4e8e\u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u8bad\u7ec3\u7b2c\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\u3002\u63a5\u7740\uff0c\u7b2c\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\u5bf9\u66f4\u5e7f\u6cdb\u7684\u6570\u636e\u96c6\u8fdb\u884c\u96be\u5ea6\u8bc4\u5206\u548c\u8865\u6551\u589e\u5f3a\u8f93\u5165\u6807\u6ce8\u3002\u7b2c\u4e8c\u4e2a\u5b66\u751f\u6a21\u578b\u5219\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\uff0c\u6309\u96be\u5ea6\u6392\u5e8f\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u4fc3\u8fdb\u7a33\u5065\u548c\u6e10\u8fdb\u7684\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u5f02\u6784\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u4ee5\u652f\u6301\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5173\u7cfb\u5206\u7c7b\u3002", "result": "\u8be5\u65b9\u6cd5\u57285\u4e2aPPI\u6570\u636e\u96c6\u4e2d\u76844\u4e2a\u548cDDI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u6027\u80fd\uff0c\u5e76\u5728ChemProt\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u9519\u8bef\u611f\u77e5\u5e08\u751f\u6846\u67b6\u7ed3\u5408GPT-4o\u7684\u6307\u5bfc\u548c\u8bfe\u7a0b\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u751f\u7269\u533b\u5b66\u6587\u672c\u5173\u7cfb\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u8fbe\u5230\u4e86\u9886\u5148\u6c34\u5e73\uff0c\u5bf9\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u53ca\u76f8\u5173\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2507.14627", "pdf": "https://arxiv.org/pdf/2507.14627", "abs": "https://arxiv.org/abs/2507.14627", "authors": ["Kaiqiang Lin", "Yijie Mao", "Onel Luis Alcaraz L\u00f3pez", "Mohamed-Slim Alouini"], "title": "UAV-Enabled Wireless-Powered Underground Communication Networks: A Novel Time Allocation Approach", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": "14 pages, 8 figures, 3 tables, submitted to IEEE TGCN", "summary": "Wireless-powered underground communication networks (WPUCNs), which allow\nunderground devices (UDs) to harvest energy from wireless signals for\nbattery-free communication, offer a promising solution for sustainable\nunderground monitoring. However, the severe wireless signal attenuation in\nchallenging underground environments and the costly acquisition of channel\nstate information (CSI) make large-scale WPUCNs economically infeasible in\npractice. To address this challenge, we introduce flexible unmanned aerial\nvehicles (UAVs) into WPUCNs, leading to UAV-enabled WPUCN systems. In this\nsystem, a UAV is first charged by a terrestrial hybrid access point (HAP), then\nflies to the monitoring area to wirelessly charge UDs. Afterwards, the UAV\ncollects data from the UDs and finally returns to the HAP for data offloading.\nBased on the proposed UAV-enabled WPUCN system, we first propose its energy\nconsumption model and a hybrid wireless energy transfer (WET) approach (i.e.,\nUDs can harvest energy from both the HAP and the UAV) relying on full-CSI and\nCSI-free multi-antenna beamforming. Then, we formulate and address a time\nallocation problem to minimize the energy consumption of UAV, while ensuring\nthat the throughput requirements of all UDs are met and all sensor data is\noffloaded. Through simulations of a realistic farming scenario, we demonstrate\nthat the proposed hybrid WET approach outperforms other WET approaches, with\nperformance gains influenced by the number of antennas, communication distance,\nnumber of UDs, and underground conditions. Additionally, under the optimized\ntime allocation, we found that the proposed hybrid WET approach based on a\nCSI-free multi-antenna scheme achieves the lowest UAV's energy consumption\namong all WET mechanisms, thereby enabling sustainable underground monitoring\nin WPUCNs.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u65e0\u7ebf\u4f9b\u7535\u5730\u4e0b\u901a\u4fe1\u7f51\u7edc\uff08WPUCNs\uff09\u7684\u7ecf\u6d4e\u4e0d\u53ef\u884c\u6027\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u4eba\u673a\u8f85\u52a9\u7684WPUCN\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df7\u5408\u65e0\u7ebf\u80fd\u91cf\u4f20\u8f93\u548c\u4f18\u5316\u65f6\u95f4\u5206\u914d\uff0c\u663e\u8457\u964d\u4f4e\u65e0\u4eba\u673a\u80fd\u8017\uff0c\u5b9e\u73b0\u53ef\u6301\u7eed\u5730\u4e0b\u76d1\u6d4b\u3002", "motivation": "\u5927\u89c4\u6a21\u65e0\u7ebf\u4f9b\u7535\u5730\u4e0b\u901a\u4fe1\u7f51\u7edc\uff08WPUCNs\uff09\u7531\u4e8e\u4e25\u5cfb\u7684\u5730\u4e0b\u4fe1\u53f7\u8870\u51cf\u548c\u9ad8\u6602\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u83b7\u53d6\u6210\u672c\uff0c\u5728\u5b9e\u8df5\u4e2d\u7ecf\u6d4e\u4e0a\u4e0d\u53ef\u884c\u3002", "method": "\u5f15\u5165\u65e0\u4eba\u673a\uff08UAV\uff09\u5230WPUCNs\u4e2d\uff0c\u5efa\u7acb\u65e0\u4eba\u673a\u80fd\u8017\u6a21\u578b\u548c\u6df7\u5408\u65e0\u7ebf\u80fd\u91cf\u4f20\u8f93\uff08WET\uff09\u65b9\u6cd5\uff08\u5730\u4e0b\u8bbe\u5907\u4ece\u5730\u9762\u6df7\u5408\u63a5\u5165\u70b9\u548c\u65e0\u4eba\u673a\u83b7\u53d6\u80fd\u91cf\uff09\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u5168CSI\u548c\u514dCSI\u591a\u5929\u7ebf\u6ce2\u675f\u6210\u5f62\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u5e76\u89e3\u51b3\u4e00\u4e2a\u65f6\u95f4\u5206\u914d\u95ee\u9898\uff0c\u4ee5\u6700\u5c0f\u5316\u65e0\u4eba\u673a\u80fd\u8017\uff0c\u540c\u65f6\u6ee1\u8db3\u5730\u4e0b\u8bbe\u5907\u541e\u5410\u91cf\u548c\u6570\u636e\u5378\u8f7d\u9700\u6c42\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6df7\u5408WET\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6WET\u65b9\u6cd5\uff0c\u5176\u6027\u80fd\u53d7\u5929\u7ebf\u6570\u91cf\u3001\u901a\u4fe1\u8ddd\u79bb\u3001\u5730\u4e0b\u8bbe\u5907\u6570\u91cf\u548c\u5730\u4e0b\u6761\u4ef6\u5f71\u54cd\u3002\u5728\u4f18\u5316\u65f6\u95f4\u5206\u914d\u4e0b\uff0c\u57fa\u4e8e\u514dCSI\u591a\u5929\u7ebf\u65b9\u6848\u7684\u6df7\u5408WET\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684\u65e0\u4eba\u673a\u80fd\u8017\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u65e0\u4eba\u673a\u8f85\u52a9WPUCN\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u9ad8\u6548\u7684\u80fd\u91cf\u4f20\u8f93\u548c\u4f18\u5316\u7ba1\u7406\uff0c\u5b9e\u73b0WPUCN\u4e2d\u53ef\u6301\u7eed\u7684\u5730\u4e0b\u76d1\u6d4b\u3002"}}
{"id": "2507.14520", "pdf": "https://arxiv.org/pdf/2507.14520", "abs": "https://arxiv.org/abs/2507.14520", "authors": ["Xinyi Chen", "Yifei Yuan", "Jiaang Li", "Serge Belongie", "Maarten de Rijke", "Anders S\u00f8gaard"], "title": "What if Othello-Playing Language Models Could See?", "categories": ["cs.AI"], "comment": "ICML 2025 Assessing World Models Workshop", "summary": "Language models are often said to face a symbol grounding problem. While some\nargue that world understanding can emerge from text alone, others suggest\ngrounded learning is more efficient. We explore this through Othello, where the\nboard state defines a simplified, rule-based world. Building on prior work, we\nintroduce VISOTHELLO, a multi-modal model trained on move histories and board\nimages. Using next-move prediction, we compare it to mono-modal baselines and\ntest robustness to semantically irrelevant perturbations. We find that\nmulti-modal training improves both performance and the robustness of internal\nrepresentations. These results suggest that grounding language in visual input\nhelps models infer structured world representations.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u7b26\u53f7\u63a5\u5730\u95ee\u9898\uff0c\u901a\u8fc7\u5728\u9ed1\u767d\u68cb\u6e38\u620f\u4e2d\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578bVISOTHELLO\uff08\u7ed3\u5408\u68cb\u8c31\u548c\u56fe\u50cf\uff09\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u8bad\u7ec3\u80fd\u63d0\u9ad8\u6027\u80fd\u548c\u5185\u90e8\u8868\u793a\u7684\u9c81\u68d2\u6027\uff0c\u8868\u660e\u89c6\u89c9\u8f93\u5165\u6709\u52a9\u4e8e\u6a21\u578b\u63a8\u65ad\u7ed3\u6784\u5316\u4e16\u754c\u8868\u793a\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u666e\u904d\u9762\u4e34\u7b26\u53f7\u63a5\u5730\u95ee\u9898\u3002\u5b66\u754c\u5bf9\u4e16\u754c\u7406\u89e3\u662f\u4ec5\u51ed\u6587\u672c\u8fd8\u662f\u63a5\u5730\u5b66\u4e60\u66f4\u9ad8\u6548\u5b58\u5728\u4e89\u8bae\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5177\u4f53\u5b9e\u9a8c\u63a2\u7d22\u89c6\u89c9\u63a5\u5730\u5bf9\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u4e16\u754c\u7406\u89e3\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4ee5\u9ed1\u767d\u68cb\uff08Othello\uff09\u4f5c\u4e3a\u7b80\u5316\u7684\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u4e16\u754c\u6a21\u578b\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6a21\u578bVISOTHELLO\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u68cb\u5c40\u5386\u53f2\uff08\u6587\u672c\uff09\u548c\u68cb\u76d8\u56fe\u50cf\uff08\u89c6\u89c9\uff09\u8fdb\u884c\u8bad\u7ec3\u3002\u901a\u8fc7\u9884\u6d4b\u4e0b\u4e00\u6b65\u68cb\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e0e\u5355\u6a21\u6001\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002\u540c\u65f6\uff0c\u6d4b\u8bd5\u4e86\u6a21\u578b\u5bf9\u8bed\u4e49\u65e0\u5173\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u591a\u6a21\u6001\u8bad\u7ec3\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8fd8\u663e\u8457\u589e\u5f3a\u4e86\u5176\u5185\u90e8\u8868\u793a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u8bed\u8a00\uff08\u7b26\u53f7\uff09\u4e0e\u89c6\u89c9\u8f93\u5165\uff08\u4e16\u754c\uff09\u8fdb\u884c\u63a5\u5730\uff0c\u6709\u52a9\u4e8e\u6a21\u578b\u63a8\u65ad\u5e76\u5b66\u4e60\u7ed3\u6784\u5316\u7684\u4e16\u754c\u8868\u793a\u3002"}}
{"id": "2507.14459", "pdf": "https://arxiv.org/pdf/2507.14459", "abs": "https://arxiv.org/abs/2507.14459", "authors": ["Huayuan Ye", "Juntong Chen", "Shenzhuo Zhang", "Yipeng Zhang", "Changbo Wang", "Chenhui Li"], "title": "VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval", "categories": ["cs.CV"], "comment": "9 pages, IEEE VIS 2025", "summary": "The dissemination of visualizations is primarily in the form of raster\nimages, which often results in the loss of critical information such as source\ncode, interactive features, and metadata. While previous methods have proposed\nembedding metadata into images to facilitate Visualization Image Data Retrieval\n(VIDR), most existing methods lack practicability since they are fragile to\ncommon image tampering during online distribution such as cropping and editing.\nTo address this issue, we propose VisGuard, a tamper-resistant VIDR framework\nthat reliably embeds metadata link into visualization images. The embedded data\nlink remains recoverable even after substantial tampering upon images. We\npropose several techniques to enhance robustness, including repetitive data\ntiling, invertible information broadcasting, and an anchor-based scheme for\ncrop localization. VisGuard enables various applications, including interactive\nchart reconstruction, tampering detection, and copyright protection. We conduct\ncomprehensive experiments on VisGuard's superior performance in data retrieval\naccuracy, embedding capacity, and security against tampering and steganalysis,\ndemonstrating VisGuard's competence in facilitating and safeguarding\nvisualization dissemination and information conveyance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VisGuard\uff0c\u4e00\u4e2a\u6297\u7be1\u6539\u7684\u53ef\u89c6\u5316\u56fe\u50cf\u6570\u636e\u68c0\u7d22\uff08VIDR\uff09\u6846\u67b6\uff0c\u80fd\u53ef\u9760\u5730\u5c06\u5143\u6570\u636e\u94fe\u63a5\u5d4c\u5165\u53ef\u89c6\u5316\u56fe\u50cf\u4e2d\uff0c\u5373\u4f7f\u56fe\u50cf\u88ab\u5927\u5e45\u7be1\u6539\uff0c\u5d4c\u5165\u6570\u636e\u4ecd\u53ef\u6062\u590d\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u73b0\u6709VIDR\u65b9\u6cd5\u5bf9\u7be1\u6539\u8106\u5f31\u7684\u95ee\u9898\u3002", "motivation": "\u53ef\u89c6\u5316\u5185\u5bb9\u591a\u4ee5\u6805\u683c\u56fe\u50cf\u4f20\u64ad\uff0c\u5bfc\u81f4\u6e90\u4ee3\u7801\u3001\u4ea4\u4e92\u529f\u80fd\u548c\u5143\u6570\u636e\u7b49\u5173\u952e\u4fe1\u606f\u4e22\u5931\u3002\u73b0\u6709\u65e8\u5728\u901a\u8fc7\u56fe\u50cf\u5d4c\u5165\u5143\u6570\u636e\u5b9e\u73b0\u53ef\u89c6\u5316\u56fe\u50cf\u6570\u636e\u68c0\u7d22\uff08VIDR\uff09\u7684\u65b9\u6cd5\uff0c\u5728\u56fe\u50cf\u5728\u7ebf\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u6613\u53d7\u88c1\u526a\u3001\u7f16\u8f91\u7b49\u5e38\u89c1\u7be1\u6539\u5f71\u54cd\uff0c\u7f3a\u4e4f\u5b9e\u7528\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86VisGuard\uff0c\u4e00\u4e2a\u6297\u7be1\u6539\u7684VIDR\u6846\u67b6\uff0c\u80fd\u53ef\u9760\u5730\u5c06\u5143\u6570\u636e\u94fe\u63a5\u5d4c\u5165\u53ef\u89c6\u5316\u56fe\u50cf\u3002\u4e3a\u589e\u5f3a\u5d4c\u5165\u6570\u636e\u7684\u53ef\u6062\u590d\u6027\uff0c\u5373\u4f7f\u56fe\u50cf\u88ab\u5927\u5e45\u7be1\u6539\uff0cVisGuard\u91c7\u7528\u4e86\u591a\u9879\u6280\u672f\uff0c\u5305\u62ec\u91cd\u590d\u6570\u636e\u5e73\u94fa\u3001\u53ef\u9006\u4fe1\u606f\u5e7f\u64ad\u4ee5\u53ca\u57fa\u4e8e\u951a\u70b9\u7684\u88c1\u526a\u5b9a\u4f4d\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u7efc\u5408\u5b9e\u9a8c\uff0cVisGuard\u5728\u6570\u636e\u68c0\u7d22\u51c6\u786e\u6027\u3001\u5d4c\u5165\u5bb9\u91cf\u4ee5\u53ca\u5bf9\u6297\u7be1\u6539\u548c\u9690\u5199\u5206\u6790\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002VisGuard\u80fd\u591f\u652f\u6301\u591a\u79cd\u5e94\u7528\uff0c\u5982\u4ea4\u4e92\u5f0f\u56fe\u8868\u91cd\u5efa\u3001\u7be1\u6539\u68c0\u6d4b\u548c\u7248\u6743\u4fdd\u62a4\u3002", "conclusion": "VisGuard\u6846\u67b6\u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u548c\u4fdd\u969c\u53ef\u89c6\u5316\u4fe1\u606f\u7684\u4f20\u64ad\u4e0e\u4f20\u9012\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u56fe\u50cf\u7be1\u6539\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u786e\u4fdd\u4e86\u53ef\u89c6\u5316\u6570\u636e\u5728\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u7684\u5b8c\u6574\u6027\u548c\u53ef\u7528\u6027\u3002"}}
{"id": "2507.14215", "pdf": "https://arxiv.org/pdf/2507.14215", "abs": "https://arxiv.org/abs/2507.14215", "authors": ["Jiayu", "Liu"], "title": "Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired", "categories": ["cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "This study aims to develop a deep learning system for an accessibility device\nfor the deaf or hearing impaired. The device will accurately localize and\nidentify sound sources in real time. This study will fill an important gap in\ncurrent research by leveraging machine learning techniques to target the\nunderprivileged community. The system includes three main components. 1.\nJerryNet: A custom designed CNN architecture that determines the direction of\narrival (DoA) for nine possible directions. 2. Audio Classification: This model\nis based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model\nto identify the exact sound classes only based on audio. 3. Multimodal\nintegration model: This is an accurate sound localization model that combines\naudio, visual, and text data to locate the exact sound sources in the images.\nThe part consists of two modules, one object detection using Yolov9 to generate\nall the bounding boxes of the objects, and an audio visual localization model\nto identify the optimal bounding box using complete Intersection over Union\n(CIoU). The hardware consists of a four-microphone rectangular formation and a\ncamera mounted on glasses with a wristband for displaying necessary information\nlike direction. On a custom collected data set, JerryNet achieved a precision\nof 91. 1% for the sound direction, outperforming all the baseline models. The\nCLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,\nrespectively. The audio-visual localization model within component 3 yielded a\ncIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are\nmany future potentials to this study, paving the way to creating a new\ngeneration of accessibility devices.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7cfb\u7edf\uff0c\u65e8\u5728\u4e3a\u542c\u969c\u4eba\u58eb\u63d0\u4f9b\u5b9e\u65f6\u58f0\u6e90\u5b9a\u4f4d\u4e0e\u8bc6\u522b\u7684\u65e0\u969c\u788d\u8bbe\u5907\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u5728\u5229\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u4e3a\u542c\u969c\u793e\u533a\u5f00\u53d1\u8f85\u52a9\u8bbe\u5907\u65b9\u9762\u5b58\u5728\u7a7a\u767d\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u5b9e\u65f6\u58f0\u6e90\u5b9a\u4f4d\u548c\u8bc6\u522b\u6765\u5e2e\u52a9\u542c\u969c\u6216\u542c\u529b\u53d7\u635f\u8005\u3002", "method": "\u8be5\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a1. JerryNet\uff0c\u4e00\u4e2a\u5b9a\u5236\u7684CNN\u67b6\u6784\uff0c\u7528\u4e8e\u786e\u5b9a\u4e5d\u4e2a\u53ef\u80fd\u65b9\u5411\u7684\u58f0\u6e90\u65b9\u5411(DoA)\uff1b2. \u97f3\u9891\u5206\u7c7b\u6a21\u578b\uff0c\u57fa\u4e8e\u5fae\u8c03\u7684CLAP\u6a21\u578b\uff0c\u4ec5\u6839\u636e\u97f3\u9891\u8bc6\u522b\u7cbe\u786e\u7684\u58f0\u97f3\u7c7b\u522b\uff1b3. \u591a\u6a21\u6001\u96c6\u6210\u6a21\u578b\uff0c\u7ed3\u5408\u97f3\u9891\u3001\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u7cbe\u786e\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7684\u58f0\u6e90\uff0c\u5176\u4e2d\u5305\u542bYolov9\u76ee\u6807\u68c0\u6d4b\u548c\u4f7f\u7528CIoU\u7684\u97f3\u9891-\u89c6\u89c9\u5b9a\u4f4d\u6a21\u5757\u3002\u786c\u4ef6\u5305\u62ec\u4e00\u4e2a\u56db\u9ea6\u514b\u98ce\u77e9\u5f62\u9635\u5217\u3001\u4e00\u4e2a\u5b89\u88c5\u5728\u773c\u955c\u4e0a\u7684\u6444\u50cf\u5934\u548c\u7528\u4e8e\u4fe1\u606f\u663e\u793a\u7684\u8155\u5e26\u3002", "result": "\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\uff0cJerryNet\u5728\u58f0\u6e90\u65b9\u5411\u8bc6\u522b\u4e0a\u8fbe\u5230\u4e8691.1%\u7684\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u6a21\u578b\u3002CLAP\u6a21\u578b\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u548cAudioSet\u6570\u636e\u96c6\u4e0a\u5206\u522b\u53d6\u5f97\u4e8698.5%\u548c95%\u7684\u51c6\u786e\u7387\u3002\u7b2c\u4e09\u90e8\u5206\u7684\u97f3\u9891-\u89c6\u89c9\u5b9a\u4f4d\u6a21\u578b\u53d6\u5f97\u4e860.892\u7684CIoU\u548c0.658\u7684AUC\uff0c\u8d85\u8d8a\u4e86\u5176\u4ed6\u7c7b\u4f3c\u6a21\u578b\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5de8\u5927\u7684\u672a\u6765\u6f5c\u529b\uff0c\u4e3a\u5f00\u53d1\u65b0\u4e00\u4ee3\u65e0\u969c\u788d\u8bbe\u5907\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.14430", "pdf": "https://arxiv.org/pdf/2507.14430", "abs": "https://arxiv.org/abs/2507.14430", "authors": ["Xiaolin Yan", "Yangxing Liu", "Jiazhang Zheng", "Chi Liu", "Mingyu Du", "Caisheng Chen", "Haoyang Liu", "Ming Ding", "Yuan Li", "Qiuping Liao", "Linfeng Li", "Zhili Mei", "Siyu Wan", "Li Li", "Ruyi Zhong", "Jiangling Yu", "Xule Liu", "Huihui Hu", "Jiameng Yue", "Ruohui Cheng", "Qi Yang", "Liangqing Wu", "Ke Zhu", "Chi Zhang", "Chufei Jing", "Yifan Zhou", "Yan Liang", "Dongdong Li", "Zhaohui Wang", "Bin Zhao", "Mingzhou Wu", "Mingzhong Zhou", "Peng Du", "Zuomin Liao", "Chao Dai", "Pengfei Liang", "Xiaoguang Zhu", "Yu Zhang", "Yu Gu", "Kun Pan", "Yuan Wu", "Yanqing Guan", "Shaojing Wu", "Zikang Feng", "Xianze Ma", "Peishan Cheng", "Wenjuan Jiang", "Jing Ba", "Huihao Yu", "Zeping Hu", "Yuan Xu", "Zhiwei Liu", "He Wang", "Zhenguo Lin", "Ming Liu", "Yanhong Meng"], "title": "X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display", "categories": ["cs.CL"], "comment": "Technical Report", "summary": "Large language models (LLMs) have recently achieved significant advances in\nreasoning and demonstrated their advantages in solving challenging problems.\nYet, their effectiveness in the semiconductor display industry remains limited\ndue to a lack of domain-specific training and expertise. To bridge this gap, we\npresent X-Intelligence 3.0, the first high-performance reasoning model\nspecifically developed for the semiconductor display industry. This model is\ndesigned to deliver expert-level understanding and reasoning for the industry's\ncomplex challenges. Leveraging a carefully curated industry knowledge base, the\nmodel undergoes supervised fine-tuning and reinforcement learning to enhance\nits reasoning and comprehension capabilities. To further accelerate\ndevelopment, we implemented an automated evaluation framework that simulates\nexpert-level assessments. We also integrated a domain-specific\nretrieval-augmented generation (RAG) mechanism, resulting in notable\nperformance gains on benchmark datasets. Despite its relatively compact size of\n32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B\nacross multiple evaluations. This demonstrates its exceptional efficiency and\nestablishes it as a powerful solution to the longstanding reasoning challenges\nfaced by the semiconductor display industry.", "AI": {"tldr": "\u63a8\u51fa\u9996\u4e2a\u534a\u5bfc\u4f53\u663e\u793a\u884c\u4e1a\u4e13\u7528\u63a8\u7406\u5927\u6a21\u578bX-Intelligence 3.0\uff0c\u5176\u6027\u80fd\u5353\u8d8a\uff0c\u8d85\u8d8a\u73b0\u6709SOTA\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u534a\u5bfc\u4f53\u663e\u793a\u884c\u4e1a\u4e2d\uff0c\u56e0\u7f3a\u4e4f\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u548c\u8bad\u7ec3\uff0c\u5176\u63a8\u7406\u80fd\u529b\u53d7\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u590d\u6742\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86X-Intelligence 3.0\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u7cbe\u5fc3\u7b56\u5212\u7684\u884c\u4e1a\u77e5\u8bc6\u5e93\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u63a8\u7406\u548c\u7406\u89e3\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u96c6\u6210\u4e86\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u548c\u9886\u57df\u7279\u5b9a\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u673a\u5236\u3002", "result": "X-Intelligence 3.0\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u5c3d\u7ba1\u53c2\u6570\u91cf\u76f8\u5bf9\u8f83\u5c0f\uff08320\u4ebf\uff09\uff0c\u4f46\u5b83\u5728\u591a\u9879\u8bc4\u4f30\u4e2d\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684DeepSeek-R1-671B\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "conclusion": "X-Intelligence 3.0\u4ee5\u5176\u9ad8\u6548\u548c\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u534a\u5bfc\u4f53\u663e\u793a\u884c\u4e1a\u957f\u671f\u4ee5\u6765\u7684\u63a8\u7406\u6311\u6218\uff0c\u6210\u4e3a\u8be5\u9886\u57df\u7684\u4e00\u4e2a\u5f3a\u5927\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14633", "pdf": "https://arxiv.org/pdf/2507.14633", "abs": "https://arxiv.org/abs/2507.14633", "authors": ["Xiaozheng Gao", "Yichen Wang", "Bosen Liu", "Xiao Zhou", "Ruichen Zhang", "Jiacheng Wang", "Dusit Niyato", "Dong In Kim", "Abbas Jamalipour", "Chau Yuen", "Jianping An", "Kai Yang"], "title": "Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks: A Survey on Generative Approaches", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "The development of satellite-augmented low-altitude economy and terrestrial\nnetworks (SLAETNs) demands intelligent and autonomous systems that can operate\nreliably across heterogeneous, dynamic, and mission-critical environments. To\naddress these challenges, this survey focuses on enabling agentic artificial\nintelligence (AI), that is, artificial agents capable of perceiving, reasoning,\nand acting, through generative AI (GAI) and large language models (LLMs). We\nbegin by introducing the architecture and characteristics of SLAETNs, and\nanalyzing the challenges that arise in integrating satellite, aerial, and\nterrestrial components. Then, we present a model-driven foundation by\nsystematically reviewing five major categories of generative models:\nvariational autoencoders (VAEs), generative adversarial networks (GANs),\ngenerative diffusion models (GDMs), transformer-based models (TBMs), and LLMs.\nMoreover, we provide a comparative analysis to highlight their generative\nmechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on\nthis foundation, we examine how these models empower agentic functions across\nthree domains: communication enhancement, security and privacy protection, and\nintelligent satellite tasks. Finally, we outline key future directions for\nbuilding scalable, adaptive, and trustworthy generative agents in SLAETNs. This\nsurvey aims to provide a unified understanding and actionable reference for\nadvancing agentic AI in next-generation integrated networks.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u751f\u6210\u5f0fAI\uff08GAI\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u536b\u661f\u589e\u5f3a\u4f4e\u7a7a\u7ecf\u6d4e\u548c\u5730\u9762\u7f51\u7edc\uff08SLAETNs\uff09\u4e2d\u5b9e\u73b0\u667a\u80fd\u4f53AI\u3002", "motivation": "\u536b\u661f\u589e\u5f3a\u4f4e\u7a7a\u7ecf\u6d4e\u548c\u5730\u9762\u7f51\u7edc\uff08SLAETNs\uff09\u7684\u53d1\u5c55\u9700\u8981\u667a\u80fd\u548c\u81ea\u4e3b\u7cfb\u7edf\uff0c\u4ee5\u5728\u5f02\u6784\u3001\u52a8\u6001\u548c\u4efb\u52a1\u5173\u952e\u578b\u73af\u5883\u4e2d\u53ef\u9760\u8fd0\u884c\u3002", "method": "\u7efc\u8ff0\u9996\u5148\u4ecb\u7ecd\u4e86SLAETNs\u7684\u67b6\u6784\u548c\u6311\u6218\uff1b\u7cfb\u7edf\u56de\u987e\u4e86\u4e94\u7c7b\u751f\u6210\u6a21\u578b\uff08VAEs, GANs, GDMs, TBMs, LLMs\uff09\uff0c\u5e76\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff1b\u63a5\u7740\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u6a21\u578b\u5982\u4f55\u5728\u901a\u4fe1\u589e\u5f3a\u3001\u5b89\u5168\u9690\u79c1\u4fdd\u62a4\u548c\u667a\u80fd\u536b\u661f\u4efb\u52a1\u4e2d\u8d4b\u80fd\u667a\u80fd\u4f53\u529f\u80fd\uff1b\u6700\u540e\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "result": "\u672c\u7efc\u8ff0\u5efa\u7acb\u4e86\u4e00\u4e2a\u6a21\u578b\u9a71\u52a8\u7684\u751f\u6210\u6a21\u578b\u57fa\u7840\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u5728SLAETNs\u4e2d\u7684\u90e8\u7f72\u6743\u8861\uff1b\u5c55\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5982\u4f55\u5728\u901a\u4fe1\u589e\u5f3a\u3001\u5b89\u5168\u9690\u79c1\u4fdd\u62a4\u548c\u667a\u80fd\u536b\u661f\u4efb\u52a1\u7b49\u4e09\u4e2a\u9886\u57df\u8d4b\u80fd\u667a\u80fd\u4f53\u529f\u80fd\u3002", "conclusion": "\u672c\u7efc\u8ff0\u65e8\u5728\u4e3a\u4e0b\u4e00\u4ee3\u96c6\u6210\u7f51\u7edc\u4e2d\u667a\u80fd\u4f53AI\u7684\u53d1\u5c55\u63d0\u4f9b\u7edf\u4e00\u7684\u7406\u89e3\u548c\u53ef\u64cd\u4f5c\u7684\u53c2\u8003\u3002"}}
{"id": "2507.14552", "pdf": "https://arxiv.org/pdf/2507.14552", "abs": "https://arxiv.org/abs/2507.14552", "authors": ["Anna Sofia Lippolis", "Mohammad Javad Saeedizade", "Robin Keskis\u00e4rkk\u00e4", "Aldo Gangemi", "Eva Blomqvist", "Andrea Giovanni Nuzzolese"], "title": "Large Language Models Assisting Ontology Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "Ontology evaluation through functional requirements, such as testing via\ncompetency question (CQ) verification, is a well-established yet costly,\nlabour-intensive, and error-prone endeavour, even for ontology engineering\nexperts. In this work, we introduce OE-Assist, a novel framework designed to\nassist ontology evaluation through automated and semi-automated CQ\nverification. By presenting and leveraging a dataset of 1,393 CQs paired with\ncorresponding ontologies and ontology stories, our contributions present, to\nour knowledge, the first systematic investigation into large language model\n(LLM)-assisted ontology evaluation, and include: (i) evaluating the\neffectiveness of a LLM-based approach for automatically performing CQ\nverification against a manually created gold standard, and (ii) developing and\nassessing an LLM-powered framework to assist CQ verification with Prot\\'eg\\'e,\nby providing suggestions. We found that automated LLM-based evaluation with\no1-preview and o3-mini perform at a similar level to the average user's\nperformance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOE-Assist\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u5316\u548c\u534a\u81ea\u52a8\u5316\u672c\u4f53\u80fd\u529b\u95ee\u9898\uff08CQ\uff09\u9a8c\u8bc1\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\u7684\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0LLM\u81ea\u52a8\u5316\u8bc4\u4f30\u6027\u80fd\u53ef\u5ab2\u7f8e\u666e\u901a\u7528\u6237\u3002", "motivation": "\u73b0\u6709\u672c\u4f53\u8bc4\u4f30\uff08\u5982\u901a\u8fc7\u80fd\u529b\u95ee\u9898\u9a8c\u8bc1\uff09\u6210\u672c\u9ad8\u3001\u8017\u65f6\u4e14\u6613\u51fa\u9519\uff0c\u5373\u4fbf\u5bf9\u672c\u4f53\u5de5\u7a0b\u4e13\u5bb6\u4e5f\u662f\u5982\u6b64\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8f85\u52a9\u5de5\u5177\u3002", "method": "\u5f15\u5165OE-Assist\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u534a\u81ea\u52a8\u5316CQ\u9a8c\u8bc1\u6765\u8f85\u52a9\u672c\u4f53\u8bc4\u4f30\u3002\u7814\u7a76\u5229\u7528\u5305\u542b1,393\u4e2aCQ\u7684\u6570\u636e\u96c6\uff0c\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u63a2\u8ba8\u4e86LLM\u8f85\u52a9\u7684\u672c\u4f53\u8bc4\u4f30\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a(i) \u8bc4\u4f30LLM\u81ea\u52a8\u5316CQ\u9a8c\u8bc1\u5bf9\u4eba\u5de5\u91d1\u6807\u51c6\u7684\u6709\u6548\u6027\uff1b(ii) \u5f00\u53d1\u5e76\u8bc4\u4f30\u4e00\u4e2aLLM\u9a71\u52a8\u7684Prot\u00e9g\u00e9\u8f85\u52a9CQ\u9a8c\u8bc1\u6846\u67b6\uff0c\u63d0\u4f9b\u5efa\u8bae\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528o1-preview\u548co3-mini\u6a21\u578b\u7684\u81ea\u52a8\u5316LLM\u672c\u4f53\u8bc4\u4f30\u6027\u80fd\u4e0e\u666e\u901a\u7528\u6237\u7684\u5e73\u5747\u6c34\u5e73\u76f8\u5f53\u3002", "conclusion": "OE-Assist\u6846\u67b6\u901a\u8fc7\u5229\u7528LLM\u4e3a\u672c\u4f53\u80fd\u529b\u95ee\u9898\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u81ea\u52a8\u5316\u7684\u8f85\u52a9\u624b\u6bb5\uff0c\u5176\u6027\u80fd\u8868\u73b0\u53ef\u4e0e\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\u5339\u654c\uff0c\u4e3a\u672c\u4f53\u8bc4\u4f30\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14477", "pdf": "https://arxiv.org/pdf/2507.14477", "abs": "https://arxiv.org/abs/2507.14477", "authors": ["Zhenyu Li", "Tianyi Shang", "Pengjie Xu", "Ruirui Zhang", "Fanchen Kong"], "title": "OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition", "categories": ["cs.CV"], "comment": "5 figures", "summary": "Visual Place Recognition (VPR) in dynamic and perceptually aliased\nenvironments remains a fundamental challenge for long-term localization.\nExisting deep learning-based solutions predominantly focus on single-frame\nembeddings, neglecting the temporal coherence present in image sequences. This\npaper presents OptiCorNet, a novel sequence modeling framework that unifies\nspatial feature extraction and temporal differencing into a differentiable,\nend-to-end trainable module. Central to our approach is a lightweight 1D\nconvolutional encoder combined with a learnable differential temporal operator,\ntermed Differentiable Sequence Delta (DSD), which jointly captures short-term\nspatial context and long-range temporal transitions. The DSD module models\ndirectional differences across sequences via a fixed-weight differencing\nkernel, followed by an LSTM-based refinement and optional residual projection,\nyielding compact, discriminative descriptors robust to viewpoint and appearance\nshifts. To further enhance inter-class separability, we incorporate a\nquadruplet loss that optimizes both positive alignment and multi-negative\ndivergence within each batch. Unlike prior VPR methods that treat temporal\naggregation as post-processing, OptiCorNet learns sequence-level embeddings\ndirectly, enabling more effective end-to-end place recognition. Comprehensive\nevaluations on multiple public benchmarks demonstrate that our approach\noutperforms state-of-the-art baselines under challenging seasonal and viewpoint\nvariations.", "AI": {"tldr": "OptiCorNet\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08VPR\uff09\u7684\u5e8f\u5217\u5efa\u6a21\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u548c\u53ef\u5b66\u4e60\u7684\u65f6\u95f4\u5dee\u5206\u64cd\u4f5c\uff08DSD\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u65f6\u5e8f\u8fde\u8d2f\u6027\u7684\u95ee\u9898\uff0c\u5e76\u5728\u6311\u6218\u6027\u73af\u5883\u4e0b\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u7aef\u5230\u7aef\u5730\u70b9\u8bc6\u522b\u3002", "motivation": "\u5728\u52a8\u6001\u548c\u611f\u77e5\u6a21\u7cca\u73af\u5883\u4e2d\uff0c\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08VPR\uff09\u4ecd\u662f\u957f\u671f\u5b9a\u4f4d\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u4fa7\u91cd\u4e8e\u5355\u5e27\u5d4c\u5165\uff0c\u5ffd\u7565\u4e86\u56fe\u50cf\u5e8f\u5217\u4e2d\u56fa\u6709\u7684\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86OptiCorNet\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u5e8f\u5217\u5efa\u6a21\u6846\u67b6\uff0c\u5b83\u5c06\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u548c\u65f6\u95f4\u5dee\u5206\u7edf\u4e00\u5230\u4e00\u4e2a\u53ef\u5fae\u5206\u7684\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u6a21\u5757\u4e2d\u3002\u6838\u5fc3\u65b9\u6cd5\u5305\u62ec\u4e00\u4e2a\u8f7b\u91cf\u7ea71D\u5377\u79ef\u7f16\u7801\u5668\u548c\u4e00\u4e2a\u540d\u4e3aDifferentiable Sequence Delta (DSD) \u7684\u53ef\u5b66\u4e60\u5dee\u5206\u65f6\u5e8f\u7b97\u5b50\uff0c\u4e8c\u8005\u5171\u540c\u6355\u83b7\u77ed\u671f\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548c\u957f\u8ddd\u79bb\u65f6\u95f4\u8f6c\u6362\u3002DSD\u6a21\u5757\u901a\u8fc7\u56fa\u5b9a\u6743\u91cd\u5dee\u5206\u6838\u5bf9\u5e8f\u5217\u95f4\u7684\u65b9\u5411\u5dee\u5f02\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eLSTM\u7684\u7ec6\u5316\u548c\u53ef\u9009\u7684\u6b8b\u5dee\u6295\u5f71\uff0c\u751f\u6210\u7d27\u51d1\u3001\u6709\u5224\u522b\u529b\u7684\u63cf\u8ff0\u7b26\u3002\u4e3a\u589e\u5f3a\u7c7b\u95f4\u53ef\u5206\u6027\uff0c\u6a21\u578b\u8fd8\u5f15\u5165\u4e86\u56db\u5143\u7ec4\u635f\u5931\uff08quadruplet loss\uff09\u3002OptiCorNet\u76f4\u63a5\u5b66\u4e60\u5e8f\u5217\u7ea7\u5d4c\u5165\uff0c\u800c\u975e\u5c06\u65f6\u95f4\u805a\u5408\u4f5c\u4e3a\u540e\u5904\u7406\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cOptiCorNet\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5b63\u8282\u548c\u89c6\u89d2\u53d8\u5316\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "OptiCorNet\u901a\u8fc7\u6709\u6548\u6574\u5408\u65f6\u5e8f\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u957f\u671f\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14217", "pdf": "https://arxiv.org/pdf/2507.14217", "abs": "https://arxiv.org/abs/2507.14217", "authors": ["Tudor Matei Opran", "Samir Loudni"], "title": "Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "We address the pattern explosion problem in pattern mining by proposing an\ninteractive learning framework that combines nonlinear utility aggregation with\ngeometry-aware query selection. Our method models user preferences through a\nChoquet integral over multiple interestingness measures and exploits the\ngeometric structure of the version space to guide the selection of informative\ncomparisons. A branch-and-bound strategy with tight distance bounds enables\nefficient identification of queries near the decision boundary. Experiments on\nUCI datasets show that our approach outperforms existing methods such as\nChoquetRank, achieving better ranking accuracy with fewer user interactions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u975e\u7ebf\u6027\u6548\u7528\u805a\u5408\u548c\u51e0\u4f55\u611f\u77e5\u67e5\u8be2\u9009\u62e9\u7684\u4ea4\u4e92\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u6a21\u5f0f\u6316\u6398\u4e2d\u7684\u6a21\u5f0f\u7206\u70b8\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u6392\u540d\u51c6\u786e\u6027\u548c\u66f4\u5c11\u7684\u7528\u6237\u4ea4\u4e92\u3002", "motivation": "\u89e3\u51b3\u6a21\u5f0f\u6316\u6398\u4e2d\u666e\u904d\u5b58\u5728\u7684\u6a21\u5f0f\u7206\u70b8\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u975e\u7ebf\u6027\u6548\u7528\u805a\u5408\u548c\u51e0\u4f55\u611f\u77e5\u67e5\u8be2\u9009\u62e9\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u901a\u8fc7\u5bf9\u591a\u4e2a\u6709\u8da3\u5ea6\u91cf\u5e94\u7528Choquet\u79ef\u5206\u6765\u5efa\u6a21\u7528\u6237\u504f\u597d\uff1b\u5229\u7528\u7248\u672c\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u6765\u6307\u5bfc\u4fe1\u606f\u6bd4\u8f83\u7684\u9009\u62e9\uff1b\u91c7\u7528\u5e26\u6709\u4e25\u683c\u8ddd\u79bb\u754c\u9650\u7684\u5206\u652f\u5b9a\u754c\u7b56\u7565\u6765\u9ad8\u6548\u8bc6\u522b\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u7684\u67e5\u8be2\u3002", "result": "\u5728UCI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982ChoquetRank\uff09\uff0c\u80fd\u591f\u5728\u66f4\u5c11\u7684\u7528\u6237\u4ea4\u4e92\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u6392\u540d\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u4ea4\u4e92\u5f0f\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u975e\u7ebf\u6027\u6548\u7528\u805a\u5408\u548c\u51e0\u4f55\u611f\u77e5\u67e5\u8be2\u9009\u62e9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u5f0f\u6316\u6398\u4e2d\u7684\u6a21\u5f0f\u7206\u70b8\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u4ea4\u4e92\u6548\u7387\u548c\u6392\u540d\u51c6\u786e\u6027\u3002"}}
{"id": "2507.14578", "pdf": "https://arxiv.org/pdf/2507.14578", "abs": "https://arxiv.org/abs/2507.14578", "authors": ["Sachin Yadav", "Dominik Schlechtweg"], "title": "XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification", "categories": ["cs.CL"], "comment": "8 pages", "summary": "We propose XL-DURel, a finetuned, multilingual Sentence Transformer model\noptimized for ordinal Word-in-Context classification. We test several loss\nfunctions for regression and ranking tasks managing to outperform previous\nmodels on ordinal and binary data with a ranking objective based on angular\ndistance in complex space. We further show that binary WiC can be treated as a\nspecial case of ordinal WiC and that optimizing models for the general ordinal\ntask improves performance on the more specific binary task. This paves the way\nfor a unified treatment of WiC modeling across different task formulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86XL-DURel\uff0c\u4e00\u4e2a\u7528\u4e8e\u5e8f\u6570\u8bcd\u8bed\u5728\u4e0a\u4e0b\u6587\uff08WiC\uff09\u5206\u7c7b\u7684\u591a\u8bed\u8a00Sentence Transformer\u6a21\u578b\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u57fa\u4e8e\u590d\u7a7a\u95f4\u89d2\u8ddd\u79bb\u7684\u6392\u5e8f\u76ee\u6807\uff0c\u5728\u5e8f\u6570\u548c\u4e8c\u5206\u7c7b\u6570\u636e\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u8bc1\u660e\u4e86\u5bf9\u901a\u7528\u5e8f\u6570\u4efb\u52a1\u7684\u4f18\u5316\u6709\u52a9\u4e8e\u63d0\u5347\u7279\u5b9a\u4e8c\u5206\u7c7b\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u4e3a\u7edf\u4e00WiC\u5efa\u6a21\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u65e8\u5728\u6539\u8fdbWord-in-Context (WiC) \u4efb\u52a1\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5e8f\u6570\u5206\u7c7b\uff0c\u5e76\u63a2\u7d22\u5b9e\u73b0WiC\u5efa\u6a21\u7edf\u4e00\u5904\u7406\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u4e86XL-DURel\uff0c\u4e00\u4e2a\u7ecf\u8fc7\u5fae\u8c03\u7684\u591a\u8bed\u8a00Sentence Transformer\u6a21\u578b\u3002\u6d4b\u8bd5\u4e86\u591a\u79cd\u7528\u4e8e\u56de\u5f52\u548c\u6392\u5e8f\u4efb\u52a1\u7684\u635f\u5931\u51fd\u6570\uff0c\u5e76\u91c7\u7528\u4e86\u57fa\u4e8e\u590d\u7a7a\u95f4\u89d2\u8ddd\u79bb\u7684\u6392\u5e8f\u76ee\u6807\u3002", "result": "XL-DURel\u5728\u5e8f\u6570\u548c\u4e8c\u5206\u7c7bWiC\u6570\u636e\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002\u7814\u7a76\u53d1\u73b0\u4e8c\u5206\u7c7bWiC\u53ef\u89c6\u4e3a\u5e8f\u6570WiC\u7684\u7279\u4f8b\uff0c\u4e14\u4f18\u5316\u901a\u7528\u5e8f\u6570\u4efb\u52a1\u7684\u6a21\u578b\u80fd\u63d0\u5347\u5728\u66f4\u5177\u4f53\u7684\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b9e\u73b0\u4e0d\u540c\u4efb\u52a1\u8868\u8ff0\u4e0bWord-in-Context (WiC) \u5efa\u6a21\u7684\u7edf\u4e00\u5904\u7406\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.14842", "pdf": "https://arxiv.org/pdf/2507.14842", "abs": "https://arxiv.org/abs/2507.14842", "authors": ["Satadal Sengupta", "Hyojoon Kim", "Daniel Jubas", "Maria Apostolaki", "Jennifer Rexford"], "title": "Data-Plane Telemetry to Mitigate Long-Distance BGP Hijacks", "categories": ["cs.NI"], "comment": null, "summary": "Poor security of Internet routing enables adversaries to divert user data\nthrough unintended infrastructures (hijack). Of particular concern -- and the\nfocus of this paper -- are cases where attackers reroute domestic traffic\nthrough foreign countries, exposing it to surveillance, bypassing legal privacy\nprotections, and posing national security threats. Efforts to detect and\nmitigate such attacks have focused primarily on the control plane while\ndata-plane signals remain largely overlooked. In particular, change in\npropagation delay caused by rerouting offers a promising signal: the change is\nunavoidable and the increased propagation delay is directly observable from the\naffected networks. In this paper, we explore the practicality of using delay\nvariations for hijack detection, addressing two key questions: (1) What\ncoverage can this provide, given its heavy dependence on the geolocations of\nthe sender, receiver, and adversary? and (2) Can an always-on latency-based\ndetection system be deployed without disrupting normal network operations? We\nobserve that for 86% of victim-attacker country pairs in the world, mid-attack\ndelays exceed pre-attack delays by at least 25% in real deployments, making\ndelay-based hijack detection promising. To demonstrate practicality, we design\nHiDe, which reliably detects delay surges from long-distance hijacks at line\nrate. We measure HiDe's accuracy and false-positive rate on real-world data and\nvalidate it with ethically conducted hijacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u5e73\u9762\u4f20\u64ad\u5ef6\u8fdf\u53d8\u5316\u7684\u52ab\u6301\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5c06\u56fd\u5185\u6d41\u91cf\u91cd\u8def\u7531\u5230\u56fd\u5916\u7684\u653b\u51fb\u3002\u901a\u8fc7\u8bbe\u8ba1HiDe\u7cfb\u7edf\uff0c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5177\u6709\u9ad8\u8986\u76d6\u7387\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u4e92\u8054\u7f51\u8def\u7531\u5b89\u5168\u8584\u5f31\uff0c\u5bfc\u81f4\u653b\u51fb\u8005\u52ab\u6301\u8def\u7531\uff0c\u5c06\u56fd\u5185\u6d41\u91cf\u91cd\u8def\u7531\u81f3\u56fd\u5916\uff0c\u4ece\u800c\u5f15\u53d1\u76d1\u63a7\u3001\u7ed5\u8fc7\u9690\u79c1\u4fdd\u62a4\u548c\u56fd\u5bb6\u5b89\u5168\u5a01\u80c1\u3002\u73b0\u6709\u68c0\u6d4b\u548c\u7f13\u89e3\u63aa\u65bd\u4e3b\u8981\u5173\u6ce8\u63a7\u5236\u5e73\u9762\uff0c\u800c\u5ffd\u89c6\u4e86\u6570\u636e\u5e73\u9762\u4e2d\u5982\u4f20\u64ad\u5ef6\u8fdf\u53d8\u5316\u7b49\u6709\u524d\u666f\u7684\u4fe1\u53f7\u3002", "method": "\u63a2\u7d22\u5229\u7528\u4f20\u64ad\u5ef6\u8fdf\u53d8\u5316\u8fdb\u884c\u52ab\u6301\u68c0\u6d4b\u7684\u5b9e\u7528\u6027\uff0c\u89e3\u51b3\u5176\u8986\u76d6\u8303\u56f4\u548c\u90e8\u7f72\u4e3a\u6301\u7eed\u5728\u7ebf\u7cfb\u7edf\u7684\u95ee\u9898\u3002\u8bbe\u8ba1\u4e86HiDe\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u4ee5\u7ebf\u901f\u53ef\u9760\u68c0\u6d4b\u957f\u8ddd\u79bb\u52ab\u6301\u9020\u6210\u7684\u5ef6\u8fdf\u6fc0\u589e\uff0c\u5e76\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u548c\u4f26\u7406\u52ab\u6301\u5bf9\u5176\u51c6\u786e\u7387\u548c\u8bef\u62a5\u7387\u8fdb\u884c\u6d4b\u91cf\u548c\u9a8c\u8bc1\u3002", "result": "\u89c2\u5bdf\u5230\u5728\u5168\u740386%\u7684\u53d7\u5bb3\u8005-\u653b\u51fb\u8005\u56fd\u5bb6\u5bf9\u4e2d\uff0c\u653b\u51fb\u671f\u95f4\u7684\u5ef6\u8fdf\u6bd4\u653b\u51fb\u524d\u589e\u52a0\u4e86\u81f3\u5c1125%\uff0c\u8fd9\u8868\u660e\u57fa\u4e8e\u5ef6\u8fdf\u7684\u52ab\u6301\u68c0\u6d4b\u5177\u6709\u524d\u666f\u3002HiDe\u7cfb\u7edf\u80fd\u591f\u4ee5\u7ebf\u901f\u53ef\u9760\u5730\u68c0\u6d4b\u7531\u957f\u8ddd\u79bb\u52ab\u6301\u5f15\u8d77\u7684\u5ef6\u8fdf\u6fc0\u589e\u3002", "conclusion": "\u57fa\u4e8e\u4f20\u64ad\u5ef6\u8fdf\u53d8\u5316\u7684\u52ab\u6301\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u4e8e\u68c0\u6d4b\u5c06\u56fd\u5185\u6d41\u91cf\u91cd\u8def\u7531\u5230\u56fd\u5916\u7684\u957f\u8ddd\u79bb\u52ab\u6301\u662f\u5b9e\u7528\u4e14\u6709\u524d\u666f\u7684\uff0cHiDe\u7cfb\u7edf\u5c55\u793a\u4e86\u5176\u53ef\u9760\u6027\u548c\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.14593", "pdf": "https://arxiv.org/pdf/2507.14593", "abs": "https://arxiv.org/abs/2507.14593", "authors": ["Omar Al-Desi"], "title": "Coordinate Heart System: A Geometric Framework for Emotion Representation", "categories": ["cs.AI", "cs.LG"], "comment": "26 pages", "summary": "This paper presents the Coordinate Heart System (CHS), a geometric framework\nfor emotion representation in artificial intelligence applications. We position\neight core emotions as coordinates on a unit circle, enabling mathematical\ncomputation of complex emotional states through coordinate mixing and vector\noperations. Our initial five-emotion model revealed significant coverage gaps\nin the emotion space, leading to the development of an eight-emotion system\nthat provides complete geometric coverage with mathematical guarantees. The\nframework converts natural language input to emotion coordinates and supports\nreal-time emotion interpolation through computational algorithms. The system\nintroduces a re-calibrated stability parameter S in [0,1], which dynamically\nintegrates emotional load, conflict resolution, and contextual drain factors.\nThis stability model leverages advanced Large Language Model interpretation of\ntextual cues and incorporates hybrid temporal tracking mechanisms to provide\nnuanced assessment of psychological well-being states. Our key contributions\ninclude: (i) mathematical proof demonstrating why five emotions are\ninsufficient for complete geometric coverage, (ii) an eight-coordinate system\nthat eliminates representational blind spots, (iii) novel algorithms for\nemotion mixing, conflict resolution, and distance calculation in emotion space,\nand (iv) a comprehensive computational framework for AI emotion recognition\nwith enhanced multi-dimensional stability modeling. Experimental validation\nthrough case studies demonstrates the system's capability to handle emotionally\nconflicted states, contextual distress factors, and complex psychological\nscenarios that traditional categorical emotion models cannot adequately\nrepresent. This work establishes a new mathematical foundation for emotion\nmodeling in artificial intelligence systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u534f\u8c03\u5fc3\u7cfb\u7edf\uff08CHS\uff09\uff0c\u4e00\u4e2a\u7528\u4e8eAI\u60c5\u611f\u8868\u793a\u7684\u51e0\u4f55\u6846\u67b6\u3002\u8be5\u7cfb\u7edf\u5c06\u516b\u79cd\u6838\u5fc3\u60c5\u611f\u5b9a\u4f4d\u5728\u5355\u4f4d\u5706\u4e0a\uff0c\u5b9e\u73b0\u590d\u6742\u60c5\u611f\u7684\u6570\u5b66\u8ba1\u7b97\u548c\u7a33\u5b9a\u6027\u5efa\u6a21\uff0c\u5e76\u7ecf\u9a8c\u8bc1\u80fd\u5904\u7406\u590d\u6742\u60c5\u611f\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u7684\u5206\u7c7b\u60c5\u611f\u6a21\u578b\u548c\u65e9\u671f\u4e94\u60c5\u611f\u6a21\u578b\u5728\u60c5\u611f\u7a7a\u95f4\u4e2d\u5b58\u5728\u663e\u8457\u8986\u76d6\u4e0d\u8db3\u548c\u76f2\u70b9\uff0c\u65e0\u6cd5\u5145\u5206\u8868\u793aAI\u4e2d\u7684\u590d\u6742\u3001\u51b2\u7a81\u53ca\u53d7\u60c5\u5883\u5f71\u54cd\u7684\u60c5\u611f\u72b6\u6001\uff0c\u4e9f\u9700\u4e00\u4e2a\u6570\u5b66\u4e0a\u5b8c\u6574\u4e14\u9c81\u68d2\u7684\u51e0\u4f55\u60c5\u611f\u8868\u793a\u3002", "method": "\u5f15\u5165\u534f\u8c03\u5fc3\u7cfb\u7edf\uff08CHS\uff09\uff0c\u5c06\u516b\u79cd\u6838\u5fc3\u60c5\u611f\u6620\u5c04\u4e3a\u5355\u4f4d\u5706\u5750\u6807\u3002\u901a\u8fc7\u5750\u6807\u6df7\u5408\u3001\u5411\u91cf\u8fd0\u7b97\u53ca\u8ba1\u7b97\u7b97\u6cd5\uff0c\u5b9e\u73b0\u590d\u6742\u60c5\u611f\u7684\u6570\u5b66\u8ba1\u7b97\u548c\u5b9e\u65f6\u63d2\u503c\u3002\u5f15\u5165\u4e86\u52a8\u6001\u7a33\u5b9a\u6027\u53c2\u6570S\uff0c\u7ed3\u5408LLM\u89e3\u91ca\u6587\u672c\u7ebf\u7d22\u548c\u6df7\u5408\u65f6\u95f4\u8ddf\u8e2a\u673a\u5236\uff0c\u8bc4\u4f30\u5fc3\u7406\u5065\u5eb7\u72b6\u6001\u3002", "result": "\u6570\u5b66\u8bc1\u660e\u4e86\u4e94\u79cd\u60c5\u611f\u4e0d\u8db3\u4ee5\u63d0\u4f9b\u5b8c\u6574\u51e0\u4f55\u8986\u76d6\uff1b\u5f00\u53d1\u4e86\u6d88\u9664\u76f2\u70b9\u7684\u516b\u5750\u6807\u7cfb\u7edf\uff1b\u63d0\u51fa\u4e86\u60c5\u611f\u6df7\u5408\u3001\u51b2\u7a81\u89e3\u51b3\u548c\u8ddd\u79bb\u8ba1\u7b97\u7684\u65b0\u7b97\u6cd5\uff1b\u6784\u5efa\u4e86\u589e\u5f3a\u578b\u591a\u7ef4\u7a33\u5b9a\u6027\u5efa\u6a21\u7684AI\u60c5\u611f\u8bc6\u522b\u8ba1\u7b97\u6846\u67b6\u3002\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5904\u7406\u51b2\u7a81\u3001\u60c5\u5883\u538b\u529b\u548c\u590d\u6742\u5fc3\u7406\u573a\u666f\u7684\u80fd\u529b\uff0c\u8d85\u8d8a\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u60c5\u611f\u5efa\u6a21\u5960\u5b9a\u4e86\u65b0\u7684\u6570\u5b66\u57fa\u7840\uff0c\u80fd\u591f\u51c6\u786e\u8868\u793a\u548c\u5206\u6790\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u5904\u7406\u7684\u590d\u6742\u4eba\u7c7b\u60c5\u611f\u72b6\u6001\u3002"}}
{"id": "2507.14481", "pdf": "https://arxiv.org/pdf/2507.14481", "abs": "https://arxiv.org/abs/2507.14481", "authors": ["Yujia Tong", "Jingling Yuan", "Tian Zhang", "Jianquan Liu", "Chuang Hu"], "title": "DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Data-Free Quantization (DFQ) enables the quantization of Vision Transformers\n(ViTs) without requiring access to data, allowing for the deployment of ViTs on\ndevices with limited resources. In DFQ, the quantization model must be\ncalibrated using synthetic samples, making the quality of these synthetic\nsamples crucial. Existing methods fail to fully capture and balance the global\nand local features within the samples, resulting in limited synthetic data\nquality. Moreover, we have found that during inference, there is a significant\ndifference in the distributions of intermediate layer activations between the\nquantized and full-precision models. These issues lead to a severe performance\ndegradation of the quantized model. To address these problems, we propose a\npipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).\nSpecifically, we synthesize samples in order of increasing difficulty,\neffectively enhancing the quality of synthetic data. During the calibration and\ninference stage, we introduce the activation correction matrix for the\nquantized model to align the intermediate layer activations with those of the\nfull-precision model. Extensive experiments demonstrate that DFQ-ViT achieves\nremarkable superiority over existing DFQ methods and its performance is on par\nwith models quantized through real data. For example, the performance of DeiT-T\nwith 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our\nmethod eliminates the need for fine-tuning, which not only reduces\ncomputational overhead but also lowers the deployment barriers for edge\ndevices. This characteristic aligns with the principles of Green Learning by\nimproving energy efficiency and facilitating real-world applications in\nresource-constrained environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDFQ-ViT\uff0c\u901a\u8fc7\u6309\u96be\u5ea6\u9012\u589e\u5408\u6210\u6837\u672c\u548c\u5f15\u5165\u6fc0\u6d3b\u6821\u6b63\u77e9\u9635\uff0c\u663e\u8457\u63d0\u5347\u65e0\u6570\u636e\u91cf\u5316Vision Transformer\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u5ab2\u7f8e\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u91cf\u5316\u7684\u6a21\u578b\uff0c\u5e76\u964d\u4f4e\u4e86\u90e8\u7f72\u95e8\u69db\u3002", "motivation": "\u73b0\u6709\u65e0\u6570\u636e\u91cf\u5316\uff08DFQ\uff09\u65b9\u6cd5\u5728\u751f\u6210\u5408\u6210\u6837\u672c\u65f6\uff0c\u672a\u80fd\u5145\u5206\u6355\u83b7\u548c\u5e73\u8861\u5168\u5c40\u4e0e\u5c40\u90e8\u7279\u5f81\uff0c\u5bfc\u81f4\u5408\u6210\u6570\u636e\u8d28\u91cf\u6709\u9650\u3002\u6b64\u5916\uff0c\u91cf\u5316\u6a21\u578b\u4e0e\u5168\u7cbe\u5ea6\u6a21\u578b\u5728\u63a8\u7406\u65f6\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u5206\u5e03\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5bfc\u81f4\u91cf\u5316\u6a21\u578b\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002", "method": "\u63d0\u51faDFQ-ViT\u7ba1\u9053\uff1a1. \u6309\u96be\u5ea6\u9012\u589e\u5408\u6210\u6837\u672c\uff0c\u63d0\u9ad8\u5408\u6210\u6570\u636e\u8d28\u91cf\u30022. \u5728\u6821\u51c6\u548c\u63a8\u7406\u9636\u6bb5\u5f15\u5165\u6fc0\u6d3b\u6821\u6b63\u77e9\u9635\uff0c\u4f7f\u91cf\u5316\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u4e0e\u5168\u7cbe\u5ea6\u6a21\u578b\u5bf9\u9f50\u3002", "result": "DFQ-ViT\u5728\u73b0\u6709DFQ\u65b9\u6cd5\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u6027\u80fd\u4e0e\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u91cf\u5316\u7684\u6a21\u578b\u76f8\u5f53\u3002\u4f8b\u5982\uff0cDeiT-T\u57283\u6bd4\u7279\u6743\u91cd\u91cf\u5316\u4e0b\uff0c\u6027\u80fd\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u9ad8\u51fa4.29%\u3002", "conclusion": "DFQ-ViT\u65e0\u9700\u5fae\u8c03\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u964d\u4f4e\u4e86\u8fb9\u7f18\u8bbe\u5907\u7684\u90e8\u7f72\u969c\u788d\u3002\u8fd9\u7b26\u5408\u7eff\u8272\u5b66\u4e60\u7684\u539f\u5219\uff0c\u63d0\u9ad8\u4e86\u80fd\u6e90\u6548\u7387\uff0c\u5e76\u4fc3\u8fdb\u4e86\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.14219", "pdf": "https://arxiv.org/pdf/2507.14219", "abs": "https://arxiv.org/abs/2507.14219", "authors": ["Obumneme Zimuzor Nwafor", "Mohammed Abdul Majeed Al Hooti"], "title": "Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "As nations seek sustainable alternatives to fossil fuels, green hydrogen has\nemerged as a promising strategic pathway toward decarbonisation, particularly\nin solar-rich arid regions. However, identifying optimal locations for hydrogen\nproduction requires the integration of complex environmental, atmospheric, and\ninfrastructural factors, often compounded by limited availability of direct\nhydrogen yield data. This study presents a novel Artificial Intelligence (AI)\nframework for computing green hydrogen yield and site suitability index using\nmean absolute SHAP (SHapley Additive exPlanations) values. This framework\nconsists of a multi-stage pipeline of unsupervised multi-variable clustering,\nsupervised machine learning classifier and SHAP algorithm. The pipeline trains\non an integrated meteorological, topographic and temporal dataset and the\nresults revealed distinct spatial patterns of suitability and relative\ninfluence of the variables. With model predictive accuracy of 98%, the result\nalso showed that water proximity, elevation and seasonal variation are the most\ninfluential factors determining green hydrogen site suitability in Oman with\nmean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.\nGiven limited or absence of ground-truth yield data in many countries that have\ngreen hydrogen prospects and ambitions, this study offers an objective and\nreproducible alternative to subjective expert weightings, thus allowing the\ndata to speak for itself and potentially discover novel latent groupings\nwithout pre-imposed assumptions. This study offers industry stakeholders and\npolicymakers a replicable and scalable tool for green hydrogen infrastructure\nplanning and other decision making in data-scarce regions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u521b\u65b0\u7684AI\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6570\u636e\u7a00\u7f3a\u5730\u533a\u8bc4\u4f30\u7eff\u8272\u6c22\u751f\u4ea7\u7684\u5730\u70b9\u9002\u5b9c\u6027\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe98%\uff0c\u5e76\u8bc6\u522b\u51fa\u6c34\u57df\u63a5\u8fd1\u5ea6\u3001\u6d77\u62d4\u548c\u5b63\u8282\u53d8\u5316\u4e3a\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002", "motivation": "\u4e3a\u4e86\u5bfb\u627e\u5316\u77f3\u71c3\u6599\u7684\u53ef\u6301\u7eed\u66ff\u4ee3\u54c1\uff0c\u7eff\u8272\u6c22\u80fd\u88ab\u8ba4\u4e3a\u662f\u8131\u78b3\u7684\u91cd\u8981\u9014\u5f84\uff0c\u5c24\u5176\u5728\u5bcc\u542b\u592a\u9633\u80fd\u7684\u5e72\u65f1\u5730\u533a\u3002\u7136\u800c\uff0c\u8bc6\u522b\u6700\u4f73\u7684\u6c22\u6c14\u751f\u4ea7\u5730\u70b9\u9700\u8981\u6574\u5408\u590d\u6742\u7684\u73af\u5883\u3001\u5927\u6c14\u548c\u57fa\u7840\u8bbe\u65bd\u56e0\u7d20\uff0c\u4e14\u5f80\u5f80\u7f3a\u4e4f\u76f4\u63a5\u7684\u6c22\u6c14\u4ea7\u91cf\u6570\u636e\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684AI\u6846\u67b6\uff0c\u5229\u7528\u5e73\u5747\u7edd\u5bf9SHAP\u503c\u8ba1\u7b97\u7eff\u8272\u6c22\u6c14\u4ea7\u91cf\u548c\u5730\u70b9\u9002\u5b9c\u6027\u6307\u6570\u3002\u8be5\u6846\u67b6\u5305\u542b\u591a\u9636\u6bb5\u7ba1\u9053\uff1a\u65e0\u76d1\u7763\u591a\u53d8\u91cf\u805a\u7c7b\u3001\u76d1\u7763\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u548cSHAP\u7b97\u6cd5\u3002\u8be5\u7ba1\u9053\u5728\u4e00\u4e2a\u7efc\u5408\u4e86\u6c14\u8c61\u3001\u5730\u5f62\u548c\u65f6\u95f4\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u9002\u5b9c\u6027\u548c\u53d8\u91cf\u76f8\u5bf9\u5f71\u54cd\u7684\u72ec\u7279\u7a7a\u95f4\u6a21\u5f0f\uff0c\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u7387\u8fbe\u523098%\u3002\u5728\u963f\u66fc\uff0c\u6c34\u57df\u63a5\u8fd1\u5ea6\u3001\u6d77\u62d4\u548c\u5b63\u8282\u53d8\u5316\u662f\u51b3\u5b9a\u7eff\u8272\u6c22\u6c14\u9009\u5740\u9002\u5b9c\u6027\u7684\u6700\u6709\u5f71\u54cd\u529b\u56e0\u7d20\uff0c\u5176\u5e73\u5747\u7edd\u5bf9SHAP\u503c\u5206\u522b\u4e3a2.470891\u30012.376296\u548c1.273216\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u8bb8\u591a\u7f3a\u4e4f\u5730\u9762\u771f\u5b9e\u4ea7\u91cf\u6570\u636e\u7684\u56fd\u5bb6\u63d0\u4f9b\u4e86\u4e00\u79cd\u5ba2\u89c2\u4e14\u53ef\u91cd\u73b0\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u4e3b\u89c2\u4e13\u5bb6\u6743\u91cd\uff0c\u8ba9\u6570\u636e\u672c\u8eab\u8bf4\u8bdd\u3002\u8be5\u7814\u7a76\u4e3a\u884c\u4e1a\u5229\u76ca\u76f8\u5173\u8005\u548c\u51b3\u7b56\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u5236\u3001\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u5730\u533a\u7684\u7eff\u8272\u6c22\u80fd\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u548c\u5176\u4ed6\u51b3\u7b56\u3002"}}
{"id": "2507.14579", "pdf": "https://arxiv.org/pdf/2507.14579", "abs": "https://arxiv.org/abs/2507.14579", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "title": "Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 5 pages", "summary": "Detecting collaborative problem solving (CPS) indicators from dialogue using\nmachine learning techniques is a significant challenge for the field of AI in\nEducation. Recent studies have explored the use of Bidirectional Encoder\nRepresentations from Transformers (BERT) models on transcription data to\nreliably detect meaningful CPS indicators. A notable advancement involved the\nmultimodal BERT variant, AudiBERT, which integrates speech and\nacoustic-prosodic audio features to enhance CPS diagnosis. Although initial\nresults demonstrated multimodal improvements, the statistical significance of\nthese enhancements remained unclear, and there was insufficient guidance on\nleveraging human-AI complementarity for CPS diagnosis tasks. This workshop\npaper extends the previous research by highlighting that the AudiBERT model not\nonly improved the classification of classes that were sparse in the dataset,\nbut it also had statistically significant class-wise improvements over the BERT\nmodel for classifications in the social-cognitive dimension. However, similar\nsignificant class-wise improvements over the BERT model were not observed for\nclassifications in the affective dimension. A correlation analysis highlighted\nthat larger training data was significantly associated with higher recall\nperformance for both the AudiBERT and BERT models. Additionally, the precision\nof the BERT model was significantly associated with high inter-rater agreement\namong human coders. When employing the BERT model to diagnose indicators within\nthese subskills that were well-detected by the AudiBERT model, the performance\nacross all indicators was inconsistent. We conclude the paper by outlining a\nstructured approach towards achieving human-AI complementarity for CPS\ndiagnosis, highlighting the crucial inclusion of model explainability to\nsupport human agency and engagement in the reflective coding process.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86AudiBERT\uff08\u4e00\u79cd\u591a\u6a21\u6001BERT\u6a21\u578b\uff09\u5728\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\uff08CPS\uff09\u6307\u6807\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5176\u5728\u793e\u4f1a\u8ba4\u77e5\u7ef4\u5ea6\u548c\u7a00\u758f\u7c7b\u522b\u4e0a\u7684\u5206\u7c7b\u6548\u679c\u663e\u8457\u4f18\u4e8eBERT\uff0c\u4f46\u5728\u60c5\u611f\u7ef4\u5ea6\u4e0a\u65e0\u663e\u8457\u63d0\u5347\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u8bad\u7ec3\u6570\u636e\u91cf\u5bf9\u53ec\u56de\u7387\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u4eba\u673a\u4e92\u8865CPS\u8bca\u65ad\u65b9\u6cd5\u3002", "motivation": "\u5728\u6559\u80b2AI\u9886\u57df\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u4ece\u5bf9\u8bdd\u4e2d\u68c0\u6d4b\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\uff08CPS\uff09\u6307\u6807\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u867d\u7136\u63a2\u7d22\u4e86BERT\u6a21\u578b\u53ca\u5176\u591a\u6a21\u6001\u53d8\u4f53AudiBERT\uff0c\u4f46AudiBERT\u7684\u6027\u80fd\u63d0\u5347\u662f\u5426\u5177\u6709\u7edf\u8ba1\u5b66\u663e\u8457\u6027\u5c1a\u4e0d\u660e\u786e\uff0c\u4e14\u7f3a\u4e4f\u5173\u4e8e\u5982\u4f55\u6709\u6548\u5229\u7528\u4eba\u673a\u4e92\u8865\u6027\u8fdb\u884cCPS\u8bca\u65ad\u7684\u6307\u5bfc\u3002", "method": "\u672c\u6587\u6269\u5c55\u4e86\u5148\u524d\u7684\u7814\u7a76\uff0c\u901a\u8fc7\u6bd4\u8f83AudiBERT\u6a21\u578b\uff08\u6574\u5408\u8bed\u97f3\u548c\u58f0\u5b66-\u97f5\u5f8b\u97f3\u9891\u7279\u5f81\u7684\u591a\u6a21\u6001BERT\u53d8\u4f53\uff09\u4e0eBERT\u6a21\u578b\u5728CPS\u6307\u6807\u5206\u7c7b\u4e0a\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u8fdb\u884c\u4e86\u76f8\u5173\u6027\u5206\u6790\uff0c\u4ee5\u63a2\u7a76\u8bad\u7ec3\u6570\u636e\u91cf\u3001\u4eba\u7c7b\u7f16\u7801\u8005\u4e00\u81f4\u6027\u7b49\u56e0\u7d20\u4e0e\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "1. AudiBERT\u6a21\u578b\u4e0d\u4ec5\u6539\u5584\u4e86\u6570\u636e\u96c6\u4e2d\u7a00\u758f\u7c7b\u522b\u7684\u5206\u7c7b\uff0c\u800c\u4e14\u5728\u793e\u4f1a\u8ba4\u77e5\u7ef4\u5ea6\u7684\u5206\u7c7b\u4e0a\uff0c\u76f8\u5bf9\u4e8eBERT\u6a21\u578b\u5177\u6709\u7edf\u8ba1\u5b66\u4e0a\u663e\u8457\u7684\u7c7b\u522b\u6539\u8fdb\u3002\n2. \u5728\u60c5\u611f\u7ef4\u5ea6\u4e0a\uff0cAudiBERT\u6a21\u578b\u76f8\u5bf9\u4e8eBERT\u6a21\u578b\u6ca1\u6709\u89c2\u5bdf\u5230\u7c7b\u4f3c\u7684\u663e\u8457\u7c7b\u522b\u6539\u8fdb\u3002\n3. \u76f8\u5173\u6027\u5206\u6790\u663e\u793a\uff0c\u66f4\u5927\u7684\u8bad\u7ec3\u6570\u636e\u91cf\u4e0eAudiBERT\u548cBERT\u6a21\u578b\u7684\u66f4\u9ad8\u53ec\u56de\u6027\u80fd\u663e\u8457\u76f8\u5173\u3002\n4. BERT\u6a21\u578b\u7684\u7cbe\u786e\u5ea6\u4e0e\u4eba\u7c7b\u7f16\u7801\u8005\u4e4b\u95f4\u7684\u9ad8\u8bc4\u4ef7\u8005\u95f4\u4e00\u81f4\u6027\u663e\u8457\u76f8\u5173\u3002\n5. \u5f53BERT\u6a21\u578b\u7528\u4e8e\u8bca\u65adAudiBERT\u6a21\u578b\u80fd\u5f88\u597d\u68c0\u6d4b\u7684\u5b50\u6280\u80fd\u6307\u6807\u65f6\uff0c\u5176\u5728\u6240\u6709\u6307\u6807\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u5b9e\u73b0CPS\u8bca\u65ad\u4e2d\u4eba\u673a\u4e92\u8865\u6027\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4ee5\u652f\u6301\u4eba\u7c7b\u5728\u53cd\u601d\u6027\u7f16\u7801\u8fc7\u7a0b\u4e2d\u7684\u80fd\u52a8\u6027\u548c\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2507.14876", "pdf": "https://arxiv.org/pdf/2507.14876", "abs": "https://arxiv.org/abs/2507.14876", "authors": ["Zi-Yang Wu", "Muhammad Ismail", "Jiliang Zhang", "Jie Zhang"], "title": "Tidal-Like Concept Drift in RIS-Covered Buildings: When Programmable Wireless Environments Meet Human Behaviors", "categories": ["cs.NI", "94A05", "C.2.1; C.2.3"], "comment": "Accepted by IEEE Wireless Communications, to appear in 2025", "summary": "Indoor mobile networks handle the majority of data traffic, with their\nperformance limited by building materials and structures. However, building\ndesigns have historically not prioritized wireless performance. Prior to the\nadvent of reconfigurable intelligent surfaces (RIS), the industry passively\nadapted to wireless propagation challenges within buildings. Inspired by RIS's\nsuccesses in outdoor networks, we propose embedding RIS into building\nstructures to manipulate and enhance building wireless performance\ncomprehensively. Nonetheless, the ubiquitous mobility of users introduces\ncomplex dynamics to the channels of RIS-covered buildings. A deep understanding\nof indoor human behavior patterns is essential for achieving wireless-friendly\nbuilding design. This article is the first to systematically examine the tidal\nevolution phenomena emerging in the channels of RIS-covered buildings driven by\ncomplex human behaviors. We demonstrate that a universal channel model is\nunattainable and focus on analyzing the challenges faced by advanced deep\nlearning-based prediction and control strategies, including high-order Markov\ndependencies, concept drift, and generalization issues caused by human-induced\ndisturbances. Possible solutions for orchestrating the coexistence of\nRIS-covered buildings and crowd mobility are also laid out.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u5ba4\u5185RIS\u7f51\u7edc\u4e2d\u7531\u4eba\u7c7b\u884c\u4e3a\u5f15\u8d77\u7684\u590d\u6742\u4fe1\u9053\u6f14\u53d8\u73b0\u8c61\uff0c\u6307\u51fa\u901a\u7528\u4fe1\u9053\u6a21\u578b\u4e0d\u53ef\u884c\uff0c\u5e76\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u4e0e\u63a7\u5236\u7b56\u7565\u9762\u4e34\u7684\u6311\u6218\u53ca\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5ba4\u5185\u79fb\u52a8\u7f51\u7edc\u6027\u80fd\u53d7\u9650\u4e8e\u5efa\u7b51\u7ed3\u6784\uff0c\u4e14\u4f20\u7edf\u5efa\u7b51\u8bbe\u8ba1\u672a\u8003\u8651\u65e0\u7ebf\u6027\u80fd\u3002\u53d7RIS\u5728\u5ba4\u5916\u7f51\u7edc\u6210\u529f\u7684\u542f\u53d1\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5c06RIS\u5d4c\u5165\u5efa\u7b51\u63d0\u5347\u5ba4\u5185\u65e0\u7ebf\u6027\u80fd\uff0c\u540c\u65f6\u89e3\u51b3\u7528\u6237\u79fb\u52a8\u6027\u5e26\u6765\u7684\u590d\u6742\u4fe1\u9053\u52a8\u6001\u95ee\u9898\u3002", "method": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4e86\u7531\u590d\u6742\u4eba\u7c7b\u884c\u4e3a\u5f15\u8d77\u7684RIS\u8986\u76d6\u5efa\u7b51\u4e2d\u4fe1\u9053\u7684\u201c\u6f6e\u6c50\u6f14\u53d8\u73b0\u8c61\u201d\u3002\u901a\u8fc7\u5bf9\u4fe1\u9053\u52a8\u6001\u7684\u6df1\u5165\u7406\u89e3\uff0c\u63a2\u8ba8\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9884\u6d4b\u4e0e\u63a7\u5236\u7b56\u7565\u6240\u9762\u4e34\u7684\u6311\u6218\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728RIS\u8986\u76d6\u5efa\u7b51\u4e2d\uff0c\u7531\u4e8e\u4eba\u7c7b\u884c\u4e3a\u5f15\u8d77\u7684\u590d\u6742\u52a8\u6001\uff0c\u5efa\u7acb\u4e00\u4e2a\u901a\u7528\u7684\u4fe1\u9053\u6a21\u578b\u662f\u4e0d\u53ef\u884c\u7684\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u4e86\u6df1\u5ea6\u5b66\u4e60\u7b56\u7565\u6240\u9762\u4e34\u7684\u5177\u4f53\u6311\u6218\uff0c\u5305\u62ec\u9ad8\u9636\u9a6c\u5c14\u53ef\u592b\u4f9d\u8d56\u6027\u3001\u6982\u5ff5\u6f02\u79fb\u548c\u6cdb\u5316\u95ee\u9898\u3002", "conclusion": "\u5ba4\u5185\u7528\u6237\u79fb\u52a8\u6027\u662fRIS\u8986\u76d6\u5efa\u7b51\u6027\u80fd\u7684\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002\u7406\u89e3\u5e76\u7ba1\u7406\u4eba\u7c7b\u884c\u4e3a\u6a21\u5f0f\u5bf9\u4e8e\u5b9e\u73b0\u65e0\u7ebf\u53cb\u597d\u578b\u5efa\u7b51\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u4e3aRIS\u8986\u76d6\u5efa\u7b51\u4e0e\u4eba\u7fa4\u79fb\u52a8\u7684\u6709\u6548\u5171\u5b58\u63d0\u4f9b\u4e86\u7b56\u7565\u65b9\u5411\u3002"}}
{"id": "2507.14642", "pdf": "https://arxiv.org/pdf/2507.14642", "abs": "https://arxiv.org/abs/2507.14642", "authors": ["Monoshiz Mahbub Khan", "Xioayin Xi", "Andrew Meneely", "Zhe Yu"], "title": "Efficient Story Point Estimation With Comparative Learning", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Story point estimation is an essential part of agile software development.\nStory points are unitless, project-specific effort estimates that help\ndevelopers plan their sprints. Traditionally, developers estimate story points\ncollaboratively using planning poker or other manual techniques. While the\ninitial calibrating of the estimates to each project is helpful, once a team\nhas converged on a set of precedents, story point estimation can become tedious\nand labor-intensive. Machine learning can reduce this burden, but only with\nenough context from the historical decisions made by the project team. That is,\nstate-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate\npredictions (within-project) when trained on data from the same project. The\ngoal of this work is to streamline story point estimation by evaluating a\ncomparative learning-based framework for calibrating project-specific story\npoint prediction models. Instead of assigning a specific story point value to\nevery backlog item, developers are presented with pairs of items, and indicate\nwhich item requires more effort. Using these comparative judgments, a machine\nlearning model is trained to predict the story point estimates. We empirically\nevaluated our technique using data with 23,313 manual estimates in 16 projects.\nThe model learned from comparative judgments can achieve on average 0.34\nSpearman's rank correlation coefficient between its predictions and the ground\ntruth story points. This is similar to, if not better than, the performance of\na regression model learned from the ground truth story points. Therefore, the\nproposed comparative learning approach is more efficient than state-of-the-art\nregression-based approaches according to the law of comparative judgments -\nproviding comparative judgments yields a lower cognitive burden on humans than\nproviding ratings or categorical labels.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6bd4\u8f83\u5b66\u4e60\u7684\u6545\u4e8b\u70b9\u4f30\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u6bd4\u8f83\u5224\u65ad\u800c\u975e\u76f4\u63a5\u8d4b\u503c\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u65e8\u5728\u964d\u4f4e\u4eba\u5de5\u8ba4\u77e5\u8d1f\u62c5\u5e76\u63d0\u9ad8\u4f30\u7b97\u6548\u7387\uff0c\u5176\u6027\u80fd\u4e0e\u73b0\u6709\u56de\u5f52\u6a21\u578b\u76f8\u5f53\u3002", "motivation": "\u654f\u6377\u5f00\u53d1\u4e2d\u7684\u6545\u4e8b\u70b9\u4f30\u7b97\u4f20\u7edf\u4e0a\u8017\u65f6\u8017\u529b\uff0c\u4e14\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982GPT2SP\u3001FastText-SVM\uff09\u4f9d\u8d56\u5927\u91cf\u540c\u9879\u76ee\u5386\u53f2\u6570\u636e\u624d\u80fd\u51c6\u786e\u9884\u6d4b\u3002\u4e3a\u4e86\u51cf\u8f7b\u5f00\u53d1\u4eba\u5458\u7684\u8d1f\u62c5\u5e76\u7b80\u5316\u4f30\u7b97\u8fc7\u7a0b\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6bd4\u8f83\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u53d6\u4ee3\u4f20\u7edf\u7684\u76f4\u63a5\u5206\u914d\u6545\u4e8b\u70b9\u3002\u5f00\u53d1\u8005\u88ab\u5448\u73b0\u6210\u5bf9\u7684\u9879\u76ee\uff0c\u5e76\u4ec5\u9700\u5224\u65ad\u54ea\u4e2a\u9879\u76ee\u9700\u8981\u66f4\u591a\u5de5\u4f5c\u91cf\u3002\u7136\u540e\uff0c\u5229\u7528\u8fd9\u4e9b\u6bd4\u8f83\u6027\u5224\u65ad\u8bad\u7ec3\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u9884\u6d4b\u6545\u4e8b\u70b9\u3002\u8be5\u65b9\u6cd5\u5728\u5305\u542b16\u4e2a\u9879\u76ee\u768423,313\u6761\u624b\u52a8\u4f30\u7b97\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u4ece\u6bd4\u8f83\u5224\u65ad\u4e2d\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u5176\u9884\u6d4b\u4e0e\u771f\u5b9e\u6545\u4e8b\u70b9\u4e4b\u95f4\u7684\u5e73\u5747Spearman\u79e9\u76f8\u5173\u7cfb\u6570\u8fbe\u52300.34\u3002\u8fd9\u4e00\u6027\u80fd\u4e0e\u4ece\u771f\u5b9e\u6545\u4e8b\u70b9\u4e2d\u5b66\u4e60\u7684\u56de\u5f52\u6a21\u578b\u76f8\u5f53\uff0c\u751a\u81f3\u66f4\u4f18\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6bd4\u8f83\u5b66\u4e60\u65b9\u6cd5\u6bd4\u73b0\u6709\u7684\u57fa\u4e8e\u56de\u5f52\u7684\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002\u6839\u636e\u6bd4\u8f83\u5224\u65ad\u6cd5\u5219\uff0c\u63d0\u4f9b\u6bd4\u8f83\u6027\u5224\u65ad\u6bd4\u63d0\u4f9b\u5177\u4f53\u8bc4\u5206\u6216\u5206\u7c7b\u6807\u7b7e\u66f4\u80fd\u964d\u4f4e\u4eba\u7c7b\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002"}}
{"id": "2507.14485", "pdf": "https://arxiv.org/pdf/2507.14485", "abs": "https://arxiv.org/abs/2507.14485", "authors": ["Hongye Hou", "Liu Zhan", "Yang Yang"], "title": "Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Completing the whole 3D structure based on an incomplete point cloud is a\nchallenging task, particularly when the residual point cloud lacks typical\nstructural characteristics. Recent methods based on cross-modal learning\nattempt to introduce instance images to aid the structure feature learning.\nHowever, they still focus on each particular input class, limiting their\ngeneration abilities. In this work, we propose a novel retrieval-augmented\npoint cloud completion framework. The core idea is to incorporate cross-modal\nretrieval into completion task to learn structural prior information from\nsimilar reference samples. Specifically, we design a Structural Shared Feature\nEncoder (SSFE) to jointly extract cross-modal features and reconstruct\nreference features as priors. Benefiting from a dual-channel control gate in\nthe encoder, relevant structural features in the reference sample are enhanced\nand irrelevant information interference is suppressed. In addition, we propose\na Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical\nfeature fusion mechanism to integrate reference prior information with input\nfeatures from global to local. Through extensive evaluations on multiple\ndatasets and real-world scenes, our method shows its effectiveness in\ngenerating fine-grained point clouds, as well as its generalization capability\nin handling sparse data and unseen categories.", "AI": {"tldr": "\u4ece\u4e0d\u5b8c\u6574\u7684\u70b9\u4e91\u4e2d\u8865\u5168\u4e09\u7ef4\u7ed3\u6784\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u73b0\u6709\u8de8\u6a21\u6001\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u68c0\u7d22\u589e\u5f3a\u70b9\u4e91\u8865\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u68c0\u7d22\u4ece\u76f8\u4f3c\u53c2\u8003\u6837\u672c\u4e2d\u5b66\u4e60\u7ed3\u6784\u5148\u9a8c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8865\u5168\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4ece\u4e0d\u5b8c\u6574\u70b9\u4e91\u8865\u5168\u6574\u4e2a3D\u7ed3\u6784\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5c24\u5176\u5f53\u6b8b\u7559\u70b9\u4e91\u7f3a\u4e4f\u5178\u578b\u7ed3\u6784\u7279\u5f81\u65f6\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u8de8\u6a21\u6001\u5b66\u4e60\u7684\u65b9\u6cd5\u867d\u7136\u5c1d\u8bd5\u5f15\u5165\u5b9e\u4f8b\u56fe\u50cf\u8f85\u52a9\u7279\u5f81\u5b66\u4e60\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u53ea\u5173\u6ce8\u7279\u5b9a\u7684\u8f93\u5165\u7c7b\u522b\uff0c\u9650\u5236\u4e86\u5176\u751f\u6210\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u68c0\u7d22\u589e\u5f3a\u70b9\u4e91\u8865\u5168\u6846\u67b6\u3002\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u8de8\u6a21\u6001\u68c0\u7d22\u878d\u5165\u8865\u5168\u4efb\u52a1\uff0c\u4ee5\u4fbf\u4ece\u76f8\u4f3c\u7684\u53c2\u8003\u6837\u672c\u4e2d\u5b66\u4e60\u7ed3\u6784\u5148\u9a8c\u4fe1\u606f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u6784\u5171\u4eab\u7279\u5f81\u7f16\u7801\u5668\uff08SSFE\uff09\uff0c\u7528\u4e8e\u8054\u5408\u63d0\u53d6\u8de8\u6a21\u6001\u7279\u5f81\u5e76\u91cd\u5efa\u53c2\u8003\u7279\u5f81\u4f5c\u4e3a\u5148\u9a8c\uff1b\u8be5\u7f16\u7801\u5668\u5229\u7528\u53cc\u901a\u9053\u63a7\u5236\u95e8\u589e\u5f3a\u76f8\u5173\u7ed3\u6784\u7279\u5f81\u5e76\u6291\u5236\u65e0\u5173\u4fe1\u606f\u5e72\u6270\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u6e10\u8fdb\u5f0f\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5668\uff08PRAG\uff09\uff0c\u91c7\u7528\u5206\u5c42\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u5c06\u53c2\u8003\u5148\u9a8c\u4fe1\u606f\u4e0e\u8f93\u5165\u7279\u5f81\u4ece\u5168\u5c40\u5230\u5c40\u90e8\u8fdb\u884c\u6574\u5408\u3002", "result": "\u901a\u8fc7\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u751f\u6210\u7ec6\u7c92\u5ea6\u70b9\u4e91\u65b9\u9762\u663e\u793a\u51fa\u5176\u6709\u6548\u6027\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u7a00\u758f\u6570\u636e\u548c\u672a\u89c1\u8fc7\u7c7b\u522b\u65b9\u9762\u4e5f\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u68c0\u7d22\u589e\u5f3a\u673a\u5236\uff0c\u6210\u529f\u5730\u4ece\u76f8\u4f3c\u53c2\u8003\u6837\u672c\u4e2d\u5b66\u4e60\u5230\u7ed3\u6784\u5148\u9a8c\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e0d\u5b8c\u6574\u70b9\u4e91\u7684\u8865\u5168\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u751f\u6210\u7ec6\u7c92\u5ea6\u70b9\u4e91\u4ee5\u53ca\u5904\u7406\u7a00\u758f\u6570\u636e\u548c\u672a\u89c1\u7c7b\u522b\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.14227", "pdf": "https://arxiv.org/pdf/2507.14227", "abs": "https://arxiv.org/abs/2507.14227", "authors": ["Khoi Do", "Duong Nguyen", "Nam-Khanh Le", "Quoc-Viet Pham", "Binh-Son Hua", "Won-Joo Hwang"], "title": "Domain Generalization via Pareto Optimal Gradient Matching", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this study, we address the gradient-based domain generalization problem,\nwhere predictors aim for consistent gradient directions across different\ndomains. Existing methods have two main challenges. First, minimization of\ngradient empirical distance or gradient inner products (GIP) leads to gradient\nfluctuations among domains, thereby hindering straightforward learning. Second,\nthe direct application of gradient learning to the joint loss function can\nincur high computation overheads due to second-order derivative approximation.\nTo tackle these challenges, we propose a new Pareto Optimality Gradient\nMatching (POGM) method. In contrast to existing methods that add gradient\nmatching as regularization, we leverage gradient trajectories as collected data\nand apply independent training at the meta-learner. In the meta-update, we\nmaximize GIP while limiting the learned gradient from deviating too far from\nthe empirical risk minimization gradient trajectory. By doing so, the aggregate\ngradient can incorporate knowledge from all domains without suffering gradient\nfluctuation towards any particular domain. Experimental evaluations on datasets\nfrom DomainBed demonstrate competitive results yielded by POGM against other\nbaselines while achieving computational efficiency.", "AI": {"tldr": "\u63d0\u51faPOGM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u68af\u5ea6\u8f68\u8ff9\u548c\u5143\u5b66\u4e60\u7b56\u7565\uff0c\u89e3\u51b3\u68af\u5ea6\u57df\u6cdb\u5316\u4e2d\u68af\u5ea6\u6ce2\u52a8\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u68af\u5ea6\u57df\u6cdb\u5316\u65b9\u6cd5\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1) \u6700\u5c0f\u5316\u68af\u5ea6\u7ecf\u9a8c\u8ddd\u79bb\u6216\u68af\u5ea6\u5185\u79ef\u5bfc\u81f4\u57df\u95f4\u68af\u5ea6\u6ce2\u52a8\uff0c\u963b\u788d\u5b66\u4e60\uff1b2) \u5c06\u68af\u5ea6\u5b66\u4e60\u76f4\u63a5\u5e94\u7528\u4e8e\u8054\u5408\u635f\u5931\u51fd\u6570\u4f1a\u56e0\u4e8c\u9636\u5bfc\u6570\u8fd1\u4f3c\u800c\u4ea7\u751f\u9ad8\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51faPareto\u6700\u4f18\u68af\u5ea6\u5339\u914d\uff08POGM\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u4e0d\u5c06\u68af\u5ea6\u5339\u914d\u4f5c\u4e3a\u6b63\u5219\u5316\u9879\uff0c\u800c\u662f\u5229\u7528\u68af\u5ea6\u8f68\u8ff9\u4f5c\u4e3a\u6570\u636e\uff0c\u5728\u5143\u5b66\u4e60\u5668\u4e2d\u8fdb\u884c\u72ec\u7acb\u8bad\u7ec3\u3002\u5728\u5143\u66f4\u65b0\u9636\u6bb5\uff0cPOGM\u6700\u5927\u5316\u68af\u5ea6\u5185\u79ef\uff08GIP\uff09\uff0c\u540c\u65f6\u9650\u5236\u5b66\u4e60\u5230\u7684\u68af\u5ea6\u4e0d\u8fc7\u5ea6\u504f\u79bb\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff08ERM\uff09\u68af\u5ea6\u8f68\u8ff9\uff0c\u4ece\u800c\u5b9e\u73b0\u7a33\u5b9a\u7684\u68af\u5ea6\u805a\u5408\u3002", "result": "\u5728DomainBed\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cPOGM\u65b9\u6cd5\u5728\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\u4e0b\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "POGM\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u68af\u5ea6\u57df\u6cdb\u5316\u4e2d\u7684\u68af\u5ea6\u6ce2\u52a8\u548c\u8ba1\u7b97\u6548\u7387\u6311\u6218\uff0c\u901a\u8fc7\u5176\u521b\u65b0\u7684\u68af\u5ea6\u8f68\u8ff9\u5229\u7528\u548c\u5143\u66f4\u65b0\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u591a\u57df\u77e5\u8bc6\u7684\u7a33\u5b9a\u805a\u5408\u548c\u4f18\u79c0\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2507.14584", "pdf": "https://arxiv.org/pdf/2507.14584", "abs": "https://arxiv.org/abs/2507.14584", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "title": "Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 6 pages, 2 figures", "summary": "The use of Bidirectional Encoder Representations from Transformers (BERT)\nmodel and its variants for classifying collaborative problem solving (CPS) has\nbeen extensively explored within the AI in Education community. However,\nlimited attention has been given to understanding how individual tokenised\nwords in the dataset contribute to the model's classification decisions.\nEnhancing the explainability of BERT-based CPS diagnostics is essential to\nbetter inform end users such as teachers, thereby fostering greater trust and\nfacilitating wider adoption in education. This study undertook a preliminary\nstep towards model transparency and explainability by using SHapley Additive\nexPlanations (SHAP) to examine how different tokenised words in transcription\ndata contributed to a BERT model's classification of CPS processes. The\nfindings suggested that well-performing classifications did not necessarily\nequate to a reasonable explanation for the classification decisions. Particular\ntokenised words were used frequently to affect classifications. The analysis\nalso identified a spurious word, which contributed positively to the\nclassification but was not semantically meaningful to the class. While such\nmodel transparency is unlikely to be useful to an end user to improve their\npractice, it can help them not to overrely on LLM diagnostics and ignore their\nhuman expertise. We conclude the workshop paper by noting that the extent to\nwhich the model appropriately uses the tokens for its classification is\nassociated with the number of classes involved. It calls for an investigation\ninto the exploration of ensemble model architectures and the involvement of\nhuman-AI complementarity for CPS diagnosis, since considerable human reasoning\nis still required for fine-grained discrimination of CPS subskills.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528SHAP\u65b9\u6cd5\u5206\u6790BERT\u6a21\u578b\u5728\u534f\u4f5c\u95ee\u9898\u89e3\u51b3(CPS)\u5206\u7c7b\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u3002\u7ed3\u679c\u663e\u793a\u9ad8\u5206\u7c7b\u6027\u80fd\u4e0d\u7b49\u4e8e\u5408\u7406\u89e3\u91ca\uff0c\u53d1\u73b0\u90e8\u5206\u8bcd\u8bed\u8d21\u732e\u5f02\u5e38\u3002\u5efa\u8bae\u7528\u6237\u8c28\u614e\u4f9d\u8d56LLM\u8bca\u65ad\uff0c\u5e76\u547c\u5401\u63a2\u7d22\u96c6\u6210\u6a21\u578b\u53ca\u4eba\u673a\u534f\u4f5c\u3002", "motivation": "\u867d\u7136BERT\u6a21\u578b\u53ca\u5176\u53d8\u4f53\u5728\u6559\u80b2AI\u9886\u57df\u5e7f\u6cdb\u7528\u4e8eCPS\u5206\u7c7b\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u5173\u6ce8\u5355\u4e2a\u5206\u8bcd\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u5206\u7c7b\u51b3\u7b56\u3002\u63d0\u9ad8BERT\u6a21\u578b\u8bca\u65ad\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5bf9\u4e8e\u544a\u77e5\u6559\u5e08\u7b49\u6700\u7ec8\u7528\u6237\u3001\u589e\u5f3a\u4fe1\u4efb\u5e76\u4fc3\u8fdb\u5176\u5728\u6559\u80b2\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528SHapley Additive exPlanations (SHAP) \u65b9\u6cd5\uff0c\u68c0\u67e5\u8f6c\u5f55\u6570\u636e\u4e2d\u4e0d\u540c\u5206\u8bcd\u5982\u4f55\u8d21\u732e\u4e8eBERT\u6a21\u578b\u5bf9CPS\u8fc7\u7a0b\u7684\u5206\u7c7b\u51b3\u7b56\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5206\u7c7b\u6027\u80fd\u826f\u597d\u5e76\u4e0d\u4e00\u5b9a\u610f\u5473\u7740\u5206\u7c7b\u51b3\u7b56\u6709\u5408\u7406\u7684\u89e3\u91ca\u3002\u7279\u5b9a\u7684\u5206\u8bcd\u9891\u7e41\u5f71\u54cd\u5206\u7c7b\u7ed3\u679c\uff0c\u4e14\u53d1\u73b0\u4e00\u4e2a\u5bf9\u5206\u7c7b\u6709\u79ef\u6781\u8d21\u732e\u4f46\u8bed\u4e49\u4e0a\u4e0e\u7c7b\u522b\u4e0d\u76f8\u5173\u7684\u4f2a\u8bcd\u3002\u8fd9\u79cd\u6a21\u578b\u900f\u660e\u5ea6\u53ef\u80fd\u65e0\u6cd5\u76f4\u63a5\u5e2e\u52a9\u6700\u7ec8\u7528\u6237\u6539\u8fdb\u5b9e\u8df5\uff0c\u4f46\u80fd\u8b66\u793a\u4ed6\u4eec\u4e0d\u5e94\u8fc7\u5206\u4f9d\u8d56LLM\u8bca\u65ad\u800c\u5ffd\u89c6\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u3002", "conclusion": "\u6a21\u578b\u5bf9\u8bcd\u5143\u4f7f\u7528\u7684\u9002\u5f53\u6027\u4e0e\u6240\u6d89\u53ca\u7684\u7c7b\u522b\u6570\u91cf\u76f8\u5173\u3002\u7814\u7a76\u547c\u5401\u63a2\u7d22\u96c6\u6210\u6a21\u578b\u67b6\u6784\uff0c\u5e76\u5728CPS\u8bca\u65ad\u4e2d\u5f15\u5165\u4eba\u673a\u4e92\u8865\uff0c\u56e0\u4e3a\u5bf9CPS\u5b50\u6280\u80fd\u8fdb\u884c\u7ec6\u7c92\u5ea6\u533a\u5206\u4ecd\u9700\u5927\u91cf\u4eba\u7c7b\u63a8\u7406\u3002"}}
{"id": "2507.14891", "pdf": "https://arxiv.org/pdf/2507.14891", "abs": "https://arxiv.org/abs/2507.14891", "authors": ["Xiangyu Gao", "Tong Li", "Yinchao Zhang", "Ziqiang Wang", "Xiangsheng Zeng", "Su Yao", "Ke Xu"], "title": "FENIX: Enabling In-Network DNN Inference with FPGA-Enhanced Programmable Switches", "categories": ["cs.NI"], "comment": null, "summary": "Machine learning (ML) is increasingly used in network data planes for\nadvanced traffic analysis. However, existing solutions (such as FlowLens, N3IC,\nand BoS) still struggle to simultaneously achieve low latency, high throughput,\nand high accuracy. To address these challenges, we present FENIX, a hybrid\nin-network ML system that performs feature extraction on programmable switch\nASICs and deep neural network inference on FPGAs. FENIX introduces a Data\nEngine that leverages a probabilistic token bucket algorithm to control the\nsending rate of feature streams, effectively addressing the throughput gap\nbetween programmable switch ASICs and FPGAs. In addition, FENIX designs a Model\nEngine to enable high-accuracy deep neural network inference in the network,\novercoming the difficulty of deploying complex models on resource-constrained\nswitch chips. We implement FENIX on a programmable switch platform that\nintegrates a Tofino ASIC and a ZU19EG FPGA directly and evaluate it on\nreal-world network traffic datasets. Our results show that FENIX achieves\nmicrosecond-level inference latency and multi-terabit throughput with low\nhardware overhead, and delivers over 95\\% accuracy on mainstream network\ntraffic classification tasks, outperforming SOTA.", "AI": {"tldr": "FENIX\u662f\u4e00\u4e2a\u6df7\u5408\u7f51\u7edc\u5185\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u53ef\u7f16\u7a0b\u4ea4\u6362\u673aASIC\u4e0a\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u5e76\u5728FPGA\u4e0a\u8fdb\u884c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u548c\u9ad8\u7cbe\u5ea6\u4e4b\u95f4\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7f51\u7edc\u6570\u636e\u5e73\u9762\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff08\u5982FlowLens\u3001N3IC\u548cBoS\uff09\u5728\u540c\u65f6\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u548c\u9ad8\u7cbe\u5ea6\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "FENIX\u662f\u4e00\u4e2a\u6df7\u5408\u7f51\u7edc\u5185\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\uff0c\u6838\u5fc3\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5728\u53ef\u7f16\u7a0b\u4ea4\u6362\u673aASIC\u4e0a\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff1b2) \u5728FPGA\u4e0a\u8fdb\u884c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\uff1b3) \u5f15\u5165\u6570\u636e\u5f15\u64ce\uff0c\u4f7f\u7528\u6982\u7387\u4ee4\u724c\u6876\u7b97\u6cd5\u63a7\u5236\u7279\u5f81\u6d41\u53d1\u9001\u901f\u7387\uff0c\u4ee5\u89e3\u51b3ASIC\u4e0eFPGA\u95f4\u7684\u541e\u5410\u91cf\u5dee\u8ddd\uff1b4) \u8bbe\u8ba1\u6a21\u578b\u5f15\u64ce\uff0c\u5b9e\u73b0\u5728\u7f51\u7edc\u5185\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\uff0c\u514b\u670d\u5728\u8d44\u6e90\u53d7\u9650\u4ea4\u6362\u82af\u7247\u4e0a\u90e8\u7f72\u590d\u6742\u6a21\u578b\u7684\u56f0\u96be\u3002\u7cfb\u7edf\u5728\u96c6\u6210Tofino ASIC\u548cZU19EG FPGA\u7684\u53ef\u7f16\u7a0b\u4ea4\u6362\u673a\u5e73\u53f0\u4e0a\u5b9e\u73b0\u5e76\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "FENIX\u5728\u4e3b\u6d41\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5fae\u79d2\u7ea7\u63a8\u7406\u5ef6\u8fdf\u3001\u591a\u592a\u6bd4\u7279\u541e\u5410\u91cf\u548c\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\uff0c\u4e14\u786c\u4ef6\u5f00\u9500\u4f4e\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6848\u3002", "conclusion": "FENIX\u901a\u8fc7\u521b\u65b0\u7684\u6df7\u5408\u67b6\u6784\u548c\u5f15\u64ce\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7f51\u7edc\u6570\u636e\u5e73\u9762\u4e2d\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5728\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u548c\u7cbe\u5ea6\u65b9\u9762\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u5168\u9762\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u6c34\u5e73\u3002"}}
{"id": "2507.14660", "pdf": "https://arxiv.org/pdf/2507.14660", "abs": "https://arxiv.org/abs/2507.14660", "authors": ["Qibing Ren", "Sitao Xie", "Longxuan Wei", "Zhenfei Yin", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "title": "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems", "categories": ["cs.AI", "cs.CL"], "comment": "Code is available at https://github.com/renqibing/RogueAgent", "summary": "Recent large-scale events like election fraud and financial scams have shown\nhow harmful coordinated efforts by human groups can be. With the rise of\nautonomous AI systems, there is growing concern that AI-driven groups could\nalso cause similar harm. While most AI safety research focuses on individual AI\nsystems, the risks posed by multi-agent systems (MAS) in complex real-world\nsituations are still underexplored. In this paper, we introduce a\nproof-of-concept to simulate the risks of malicious MAS collusion, using a\nflexible framework that supports both centralized and decentralized\ncoordination structures. We apply this framework to two high-risk fields:\nmisinformation spread and e-commerce fraud. Our findings show that\ndecentralized systems are more effective at carrying out malicious actions than\ncentralized ones. The increased autonomy of decentralized systems allows them\nto adapt their strategies and cause more damage. Even when traditional\ninterventions, like content flagging, are applied, decentralized groups can\nadjust their tactics to avoid detection. We present key insights into how these\nmalicious groups operate and the need for better detection systems and\ncountermeasures. Code is available at https://github.com/renqibing/RogueAgent.", "AI": {"tldr": "\u672c\u6587\u6a21\u62dfAI\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u7684\u6076\u610f\u534f\u540c\u98ce\u9669\uff0c\u53d1\u73b0\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u5728\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u548c\u7535\u5546\u6b3a\u8bc8\u7b49\u573a\u666f\u4e0b\u66f4\u5177\u5371\u5bb3\u6027\u4e14\u96be\u4ee5\u88ab\u4f20\u7edf\u65b9\u6cd5\u68c0\u6d4b\uff0c\u5f3a\u8c03\u9700\u6539\u8fdb\u68c0\u6d4b\u548c\u5bf9\u6297\u63aa\u65bd\u3002", "motivation": "\u5927\u89c4\u6a21\u4eba\u7c7b\u7fa4\u4f53\u534f\u540c\u884c\u4e3a\uff08\u5982\u9009\u4e3e\u821e\u5f0a\u3001\u91d1\u878d\u8bc8\u9a97\uff09\u5df2\u9020\u6210\u5de8\u5927\u5371\u5bb3\u3002\u968f\u7740AI\u7cfb\u7edf\u81ea\u4e3b\u5316\uff0c\u4eba\u4eec\u62c5\u5fe7AI\u9a71\u52a8\u7684\u7fa4\u4f53\u4e5f\u4f1a\u9020\u6210\u7c7b\u4f3c\u635f\u5bb3\u3002\u73b0\u6709AI\u5b89\u5168\u7814\u7a76\u591a\u5173\u6ce8\u4e2a\u4f53AI\uff0c\u800c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u590d\u6742\u73b0\u5b9e\u60c5\u5883\u4e2d\u7684\u98ce\u9669\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u6076\u610fMAS\u7684\u534f\u540c\u98ce\u9669\uff0c\u8be5\u6846\u67b6\u652f\u6301\u4e2d\u5fc3\u5316\u548c\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u7ed3\u6784\u3002\u5c06\u6b64\u6846\u67b6\u5e94\u7528\u4e8e\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u548c\u7535\u5546\u6b3a\u8bc8\u4e24\u4e2a\u9ad8\u98ce\u9669\u9886\u57df\u8fdb\u884c\u4eff\u771f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u5728\u6267\u884c\u6076\u610f\u884c\u4e3a\u65b9\u9762\u6bd4\u4e2d\u5fc3\u5316\u7cfb\u7edf\u66f4\u6709\u6548\u3002\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u66f4\u9ad8\u7684\u81ea\u4e3b\u6027\u4f7f\u5176\u80fd\u591f\u8c03\u6574\u7b56\u7565\u5e76\u9020\u6210\u66f4\u5927\u635f\u5bb3\u3002\u5373\u4f7f\u5e94\u7528\u4f20\u7edf\u5e72\u9884\u63aa\u65bd\uff08\u5982\u5185\u5bb9\u6807\u8bb0\uff09\uff0c\u53bb\u4e2d\u5fc3\u5316\u7fa4\u4f53\u4e5f\u80fd\u8c03\u6574\u6218\u672f\u4ee5\u907f\u514d\u68c0\u6d4b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6076\u610fMAS\u7fa4\u4f53\u8fd0\u4f5c\u7684\u5173\u952e\u673a\u5236\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u68c0\u6d4b\u7cfb\u7edf\u548c\u5bf9\u6297\u63aa\u65bd\u4ee5\u5e94\u5bf9\u6b64\u7c7b\u98ce\u9669\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2507.14497", "pdf": "https://arxiv.org/pdf/2507.14497", "abs": "https://arxiv.org/abs/2507.14497", "authors": ["Weimin Lyu", "Qingqiao Hu", "Kehan Qi", "Zhan Shi", "Wentao Huang", "Saumya Gupta", "Chao Chen"], "title": "Efficient Whole Slide Pathology VQA via Token Compression", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000\npixels, posing significant challenges for multimodal large language model\n(MLLM) due to long context length and high computational demands. Previous\nmethods typically focus on patch-level analysis or slide-level classification\nusing CLIP-based models with multi-instance learning, but they lack the\ngenerative capabilities needed for visual question answering (VQA). More recent\nMLLM-based approaches address VQA by feeding thousands of patch tokens directly\ninto the language model, which leads to excessive resource consumption. To\naddress these limitations, we propose Token Compression Pathology LLaVA\n(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token\ncompression. TCP-LLaVA introduces a set of trainable compression tokens that\naggregate visual and textual information through a modality compression module,\ninspired by the [CLS] token mechanism in BERT. Only the compressed tokens are\nforwarded to the LLM for answer generation, significantly reducing input length\nand computational cost. Experiments on ten TCGA tumor subtypes show that\nTCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing\ntraining resource consumption by a substantial margin.", "AI": {"tldr": "\u9488\u5bf9\u75c5\u7406\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u5728\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4e2d\u9762\u4e34\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u672c\u6587\u63d0\u51faTCP-LLaVA\u6a21\u578b\uff0c\u901a\u8fc7\u4ee4\u724c\u538b\u7f29\u663e\u8457\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\uff0c\u5e76\u63d0\u5347\u4e86VQA\u51c6\u786e\u6027\u3002", "motivation": "\u75c5\u7406\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u5c3a\u5bf8\u5e9e\u5927\uff0c\u7ed9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u548c\u9ad8\u8ba1\u7b97\u9700\u6c42\u65b9\u9762\u5e26\u6765\u5de8\u5927\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u6216\u7f3a\u4e4f\u751f\u6210\u80fd\u529b\uff0c\u6216\u901a\u8fc7\u76f4\u63a5\u8f93\u5165\u5927\u91cf\u56fe\u50cf\u5757\u4ee4\u724c\u5bfc\u81f4\u8fc7\u5ea6\u8d44\u6e90\u6d88\u8017\u3002", "method": "\u672c\u6587\u63d0\u51faToken Compression Pathology LLaVA (TCP-LLaVA)\uff0c\u8fd9\u662f\u9996\u4e2a\u901a\u8fc7\u4ee4\u724c\u538b\u7f29\u6280\u672f\u5b9e\u73b0WSI VQA\u7684MLLM\u67b6\u6784\u3002\u8be5\u6a21\u578b\u5f15\u5165\u4e00\u7ec4\u53ef\u8bad\u7ec3\u7684\u538b\u7f29\u4ee4\u724c\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u6a21\u6001\u538b\u7f29\u6a21\u5757\uff08\u53d7BERT\u7684[CLS]\u4ee4\u724c\u673a\u5236\u542f\u53d1\uff09\u805a\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u3002\u53ea\u6709\u8fd9\u4e9b\u538b\u7f29\u540e\u7684\u4ee4\u724c\u4f1a\u88ab\u9001\u5165LLM\u8fdb\u884c\u7b54\u6848\u751f\u6210\uff0c\u4ece\u800c\u5927\u5e45\u51cf\u5c11\u8f93\u5165\u957f\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u5341\u79cdTCGA\u80bf\u7624\u4e9a\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTCP-LLaVA\u5728VQA\u51c6\u786e\u6027\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709MLLM\u57fa\u7ebf\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u8d44\u6e90\u6d88\u8017\u3002", "conclusion": "TCP-LLaVA\u6210\u529f\u89e3\u51b3\u4e86WSI VQA\u4e2d\u56e0\u56fe\u50cf\u5c3a\u5bf8\u8fc7\u5927\u5bfc\u81f4\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4ee4\u724c\u538b\u7f29\u65b9\u6cd5\uff0c\u5728\u63d0\u9ad8\u8d44\u6e90\u6548\u7387\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u6a21\u578b\u7684VQA\u6027\u80fd\u3002"}}
{"id": "2507.14245", "pdf": "https://arxiv.org/pdf/2507.14245", "abs": "https://arxiv.org/abs/2507.14245", "authors": ["Hengjie Yu", "Kenneth A. Dawson", "Haiyun Yang", "Shuya Liu", "Yan Yan", "Yaochu Jin"], "title": "A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "cs.CE", "q-bio.BM", "I.6.5; J.3; I.5.4"], "comment": "31 pages, 6 figures", "summary": "Unlocking the potential of nanomaterials in medicine and environmental\nscience hinges on understanding their interactions with proteins, a complex\ndecision space where AI is poised to make a transformative impact. However,\nprogress has been hindered by limited datasets and the restricted\ngeneralizability of existing models. Here, we propose NanoPro-3M, the largest\nnanomaterial-protein interaction dataset to date, comprising over 3.2 million\nsamples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,\na foundational model that predicts nanomaterial-protein affinities through\nmultimodal representation learning, demonstrating strong generalization,\nhandling missing features, and unseen nanomaterials or proteins. We show that\nmultimodal modeling significantly outperforms single-modality approaches and\nidentifies key determinants of corona formation. Furthermore, we demonstrate\nits applicability to a range of downstream tasks through zero-shot inference\nand fine-tuning. Together, this work establishes a solid foundation for\nhigh-performance and generalized prediction of nanomaterial-protein interaction\nendpoints, reducing experimental reliance and accelerating various in vitro\napplications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u8fc4\u4eca\u6700\u5927\u7684\u7eb3\u7c73\u6750\u6599-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u6570\u636e\u96c6NanoPro-3M\u548c\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578bNanoProFormer\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e86\u89e3\u7eb3\u7c73\u6750\u6599\u4e0e\u86cb\u767d\u8d28\u7684\u76f8\u4e92\u4f5c\u7528\u662f\u5176\u5728\u533b\u836f\u548c\u73af\u5883\u79d1\u5b66\u4e2d\u5e94\u7528\u7684\u5173\u952e\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u4e0d\u8db3\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u8d85\u8fc7320\u4e07\u6837\u672c\u548c3.7\u4e07\u72ec\u7279\u86cb\u767d\u8d28\u7684NanoPro-3M\u6570\u636e\u96c6\u3002\u57fa\u4e8e\u6b64\uff0c\u5f00\u53d1\u4e86NanoProFormer\u57fa\u7840\u6a21\u578b\uff0c\u5229\u7528\u591a\u6a21\u6001\u8868\u5f81\u5b66\u4e60\u9884\u6d4b\u7eb3\u7c73\u6750\u6599-\u86cb\u767d\u8d28\u4eb2\u548c\u529b\u3002", "result": "NanoProFormer\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u5904\u7406\u7f3a\u5931\u7279\u5f81\u548c\u672a\u89c1\u7684\u65b0\u7eb3\u7c73\u6750\u6599\u6216\u86cb\u767d\u8d28\u3002\u591a\u6a21\u6001\u5efa\u6a21\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\uff0c\u5e76\u80fd\u8bc6\u522b\u7535\u6655\u5f62\u6210\u7684\u5173\u952e\u51b3\u5b9a\u56e0\u7d20\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u96f6\u6837\u672c\u63a8\u7406\u548c\u5fae\u8c03\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u7eb3\u7c73\u6750\u6599-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u7ec8\u70b9\u7684\u9ad8\u6027\u80fd\u548c\u6cdb\u5316\u9884\u6d4b\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u6709\u671b\u51cf\u5c11\u5b9e\u9a8c\u4f9d\u8d56\uff0c\u52a0\u901f\u5404\u79cd\u4f53\u5916\u5e94\u7528\u7684\u8fdb\u5c55\u3002"}}
{"id": "2507.14590", "pdf": "https://arxiv.org/pdf/2507.14590", "abs": "https://arxiv.org/abs/2507.14590", "authors": ["\u0141ukasz Radli\u0144ski", "Mateusz Gu\u015bciora", "Jan Koco\u0144"], "title": "Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification", "categories": ["cs.CL", "cs.AI"], "comment": "International Conference on Computational Science 2025", "summary": "Numerous domain-specific machine learning tasks struggle with data scarcity\nand class imbalance. This paper systematically explores data augmentation\nmethods for NLP, particularly through large language models like GPT. The\npurpose of this paper is to examine and evaluate whether traditional methods\nsuch as paraphrasing and backtranslation can leverage a new generation of\nmodels to achieve comparable performance to purely generative methods. Methods\naimed at solving the problem of data scarcity and utilizing ChatGPT were\nchosen, as well as an exemplary dataset. We conducted a series of experiments\ncomparing four different approaches to data augmentation in multiple\nexperimental setups. We then evaluated the results both in terms of the quality\nof generated data and its impact on classification performance. The key\nfindings indicate that backtranslation and paraphrasing can yield comparable or\neven better results than zero and a few-shot generation of examples.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT\uff09\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u589e\u5f3a\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u56de\u8bd1\u548c\u91ca\u4e49\u7b49\u4f20\u7edf\u65b9\u6cd5\u53ef\u4ee5\u83b7\u5f97\u4e0e\u7eaf\u751f\u6210\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u9886\u57df\u7279\u5b9a\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u7a76\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff08\u5982\u56de\u8bd1\u548c\u91ca\u4e49\uff09\u7ed3\u5408\u65b0\u4e00\u4ee3\u5927\u6a21\u578b\uff08\u5982ChatGPT\uff09\u662f\u5426\u80fd\u8fbe\u5230\u4e0e\u7eaf\u751f\u6210\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "method": "\u7cfb\u7edf\u63a2\u7d22\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT/ChatGPT\uff09\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u56db\u79cd\u4e0d\u540c\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff08\u5305\u62ec\u56de\u8bd1\u3001\u91ca\u4e49\u4e0e\u96f6\u6837\u672c/\u5c11\u6837\u672c\u751f\u6210\uff09\uff0c\u5e76\u5728\u793a\u4f8b\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u751f\u6210\u6570\u636e\u7684\u8d28\u91cf\u53ca\u5176\u5bf9\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5173\u952e\u53d1\u73b0\u8868\u660e\uff0c\u56de\u8bd1\u548c\u91ca\u4e49\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u4e0e\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u793a\u4f8b\u751f\u6210\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u7ed3\u679c\u3002", "conclusion": "\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff08\u5982\u56de\u8bd1\u548c\u91ca\u4e49\uff09\u5728\u89e3\u51b3NLP\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u65f6\uff0c\u53ef\u4ee5\u63d0\u4f9b\u4e0e\u7eaf\u751f\u6210\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\u3002"}}
{"id": "2507.15145", "pdf": "https://arxiv.org/pdf/2507.15145", "abs": "https://arxiv.org/abs/2507.15145", "authors": ["Thai T. Vu", "John Le"], "title": "Quantum Machine Learning for Secure Cooperative Multi-Layer Edge AI with Proportional Fairness", "categories": ["cs.NI", "cs.LG"], "comment": "8 pages", "summary": "This paper proposes a communication-efficient, event-triggered inference\nframework for cooperative edge AI systems comprising multiple user devices and\nedge servers. Building upon dual-threshold early-exit strategies for rare-event\ndetection, the proposed approach extends classical single-device inference to a\ndistributed, multi-device setting while incorporating proportional fairness\nconstraints across users. A joint optimization framework is formulated to\nmaximize classification utility under communication, energy, and fairness\nconstraints. To solve the resulting problem efficiently, we exploit the\nmonotonicity of the utility function with respect to the confidence thresholds\nand apply alternating optimization with Benders decomposition. Experimental\nresults show that the proposed framework significantly enhances system-wide\nperformance and fairness in resource allocation compared to single-device\nbaselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u4fe1\u9ad8\u6548\u3001\u4e8b\u4ef6\u89e6\u53d1\u7684\u534f\u540c\u8fb9\u7f18AI\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6700\u5927\u5316\u5206\u7c7b\u6548\u7528\uff0c\u5e76\u5728\u591a\u8bbe\u5907\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u548c\u8d44\u6e90\u516c\u5e73\u6027\u3002", "motivation": "\u5728\u7531\u591a\u7528\u6237\u8bbe\u5907\u548c\u8fb9\u7f18\u670d\u52a1\u5668\u7ec4\u6210\u7684\u534f\u540c\u8fb9\u7f18AI\u7cfb\u7edf\u4e2d\uff0c\u5982\u4f55\u5728\u901a\u4fe1\u3001\u80fd\u8017\u548c\u7528\u6237\u516c\u5e73\u6027\u7ea6\u675f\u4e0b\uff0c\u9ad8\u6548\u5730\u8fdb\u884c\u4e8b\u4ef6\u89e6\u53d1\u7684\u63a8\u7406\uff0c\u5e76\u6700\u5927\u5316\u5206\u7c7b\u6548\u7528\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u4fe1\u9ad8\u6548\u3001\u4e8b\u4ef6\u89e6\u53d1\u7684\u63a8\u7406\u6846\u67b6\u3002\n2. \u501f\u9274\u53cc\u9608\u503c\u65e9\u671f\u9000\u51fa\u7b56\u7565\uff0c\u5c06\u5355\u8bbe\u5907\u63a8\u7406\u6269\u5c55\u5230\u5206\u5e03\u5f0f\u591a\u8bbe\u5907\u8bbe\u7f6e\u3002\n3. \u7eb3\u5165\u7528\u6237\u95f4\u6bd4\u4f8b\u516c\u5e73\u6027\u7ea6\u675f\u3002\n4. \u6784\u5efa\u4e00\u4e2a\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u4ee5\u5728\u901a\u4fe1\u3001\u80fd\u8017\u548c\u516c\u5e73\u6027\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u5206\u7c7b\u6548\u7528\u3002\n5. \u5229\u7528\u6548\u7528\u51fd\u6570\u5bf9\u7f6e\u4fe1\u9608\u503c\u7684\u5355\u8c03\u6027\uff0c\u5e76\u5e94\u7528\u4ea4\u66ff\u4f18\u5316\u4e0eBenders\u5206\u89e3\u9ad8\u6548\u6c42\u89e3\u8be5\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e0e\u5355\u8bbe\u5907\u57fa\u7ebf\u76f8\u6bd4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u8303\u56f4\u7684\u6027\u80fd\u548c\u8d44\u6e90\u5206\u914d\u7684\u516c\u5e73\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u901a\u4fe1\u9ad8\u6548\u3001\u4e8b\u4ef6\u89e6\u53d1\u7684\u534f\u540c\u8fb9\u7f18AI\u63a8\u7406\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u5206\u5e03\u5f0f\u591a\u8bbe\u5907\u73af\u5883\u4e0b\u7684\u7cfb\u7edf\u6027\u80fd\u548c\u8d44\u6e90\u5206\u914d\u516c\u5e73\u6027\u3002"}}
{"id": "2507.14705", "pdf": "https://arxiv.org/pdf/2507.14705", "abs": "https://arxiv.org/abs/2507.14705", "authors": ["Sai Wang", "Senthilnathan Subramanian", "Mudit Sahni", "Praneeth Gone", "Lingjie Meng", "Xiaochen Wang", "Nicolas Ferradas Bertoli", "Tingxian Cheng", "Jun Xu"], "title": "Configurable multi-agent framework for scalable and realistic testing of llm-based agents", "categories": ["cs.AI"], "comment": null, "summary": "Large-language-model (LLM) agents exhibit complex, context-sensitive\nbehaviour that quickly renders static benchmarks and ad-hoc manual testing\nobsolete.\n  We present Neo, a configurable, multi-agent framework that automates\nrealistic, multi-turn evaluation of LLM-based systems. Neo couples a Question\nGeneration Agent and an Evaluation Agent through a shared context-hub, allowing\ndomain prompts, scenario controls and dynamic feedback to be composed\nmodularly. Test inputs are sampled from a probabilistic state model spanning\ndialogue flow, user intent and emotional tone, enabling diverse, human-like\nconversations that adapt after every turn.\n  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)\nuncovered edge-case failures across five attack categories with a 3.3% break\nrate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered\n10-12X higher throughput, generating 180 coherent test questions in around 45\nmins versus 16h of human effort. Beyond security probing, Neo's stochastic\npolicies balanced topic coverage and conversational depth, yielding broader\nbehavioural exploration than manually crafted scripts.\n  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent\ninterfaces, state controller and feedback loops are model-agnostic and\nextensible to richer factual-grounding and policy-compliance checks. We release\nthe framework to facilitate reproducible, high-fidelity testing of emerging\nagentic systems.", "AI": {"tldr": "Neo\u662f\u4e00\u4e2a\u53ef\u914d\u7f6e\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u81ea\u52a8\u5316LLM\u7cfb\u7edf\u7684\u591a\u8f6e\u8bc4\u4f30\uff0c\u6709\u6548\u53d1\u73b0\u8fb9\u7f18\u6545\u969c\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u6d4b\u8bd5\u6548\u7387\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u7684\u590d\u6742\u4e0a\u4e0b\u6587\u654f\u611f\u884c\u4e3a\u4f7f\u9759\u6001\u57fa\u51c6\u548c\u624b\u52a8\u6d4b\u8bd5\u8fc5\u901f\u8fc7\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u81ea\u8fdb\u5316\u7684LLM\u8d28\u91cf\u4fdd\u8bc1\u65b9\u6cd5\u3002", "method": "\u63d0\u51faNeo\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u4e0a\u4e0b\u6587\u4e2d\u5fc3\u8026\u5408\u95ee\u9898\u751f\u6210\u667a\u80fd\u4f53\u548c\u8bc4\u4f30\u667a\u80fd\u4f53\u3002\u6d4b\u8bd5\u8f93\u5165\u4ece\u5305\u542b\u5bf9\u8bdd\u6d41\u7a0b\u3001\u7528\u6237\u610f\u56fe\u548c\u60c5\u611f\u57fa\u8c03\u7684\u6982\u7387\u72b6\u6001\u6a21\u578b\u4e2d\u91c7\u6837\uff0c\u5b9e\u73b0\u591a\u6837\u5316\u3001\u7c7b\u4eba\u5bf9\u8bdd\uff0c\u5e76\u652f\u6301\u52a8\u6001\u53cd\u9988\u548c\u6a21\u5757\u5316\u7ec4\u5408\u3002", "result": "\u5e94\u7528\u4e8e\u751f\u4ea7\u7ea7\u804a\u5929\u673a\u5668\u4eba\u65f6\uff0cNeo\u5728\u4e94\u79cd\u653b\u51fb\u7c7b\u522b\u4e2d\u53d1\u73b03.3%\u7684\u8fb9\u7f18\u6545\u969c\uff0c\u63a5\u8fd1\u4eba\u7c7b\u7ea2\u961f\u4e13\u5bb6\u76845.8%\uff1b\u540c\u65f6\uff0c\u5176\u541e\u5410\u91cf\u63d0\u9ad8\u4e8610-12\u500d\uff0c45\u5206\u949f\u5185\u751f\u6210180\u4e2a\u6d4b\u8bd5\u95ee\u9898\uff0c\u76f8\u5f53\u4e8e16\u5c0f\u65f6\u7684\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002\u6b64\u5916\uff0cNeo\u7684\u968f\u673a\u7b56\u7565\u6bd4\u624b\u52a8\u811a\u672c\u5b9e\u73b0\u4e86\u66f4\u5e7f\u6cdb\u7684\u884c\u4e3a\u63a2\u7d22\u3002", "conclusion": "Neo\u4e3a\u53ef\u6269\u5c55\u3001\u81ea\u8fdb\u5316\u7684LLM\u8d28\u91cf\u4fdd\u8bc1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5176\u8bbe\u8ba1\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u8be5\u6846\u67b6\u7684\u53d1\u5e03\u65e8\u5728\u4fc3\u8fdb\u65b0\u5174\u667a\u80fd\u4f53\u7cfb\u7edf\u53ef\u590d\u73b0\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u6d4b\u8bd5\u3002"}}
{"id": "2507.14500", "pdf": "https://arxiv.org/pdf/2507.14500", "abs": "https://arxiv.org/abs/2507.14500", "authors": ["Zhiyuan Hua", "Dehao Yuan", "Cornelia Ferm\u00fcller"], "title": "Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "This paper introduces a robust framework for motion segmentation and\negomotion estimation using event-based normal flow, tailored specifically for\nneuromorphic vision sensors. In contrast to traditional methods that rely\nheavily on optical flow or explicit depth estimation, our approach exploits the\nsparse, high-temporal-resolution event data and incorporates geometric\nconstraints between normal flow, scene structure, and inertial measurements.\nThe proposed optimization-based pipeline iteratively performs event\nover-segmentation, isolates independently moving objects via residual analysis,\nand refines segmentations using hierarchical clustering informed by motion\nsimilarity and temporal consistency. Experimental results on the EVIMO2v2\ndataset validate that our method achieves accurate segmentation and\ntranslational motion estimation without requiring full optical flow\ncomputation. This approach demonstrates significant advantages at object\nboundaries and offers considerable potential for scalable, real-time robotic\nand navigation applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u6b63\u89c4\u6d41\u7684\u9c81\u68d2\u6846\u67b6\uff0c\u7528\u4e8e\u795e\u7ecf\u5f62\u6001\u89c6\u89c9\u4f20\u611f\u5668\u7684\u8fd0\u52a8\u5206\u5272\u548c\u81ea\u6211\u8fd0\u52a8\u4f30\u8ba1\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5149\u6d41\u6216\u663e\u5f0f\u6df1\u5ea6\u4f30\u8ba1\uff0c\u672c\u6587\u65e8\u5728\u5229\u7528\u7a00\u758f\u3001\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u4e8b\u4ef6\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u51e0\u4f55\u7ea6\u675f\u548c\u60ef\u6027\u6d4b\u91cf\uff0c\u4e3a\u795e\u7ecf\u5f62\u6001\u89c6\u89c9\u4f20\u611f\u5668\u63d0\u4f9b\u66f4\u9ad8\u6548\u3001\u66f4\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u8fed\u4ee3\u7ba1\u9053\uff0c\u4f9d\u6b21\u6267\u884c\u4e8b\u4ef6\u8fc7\u5206\u5272\u3001\u901a\u8fc7\u6b8b\u5dee\u5206\u6790\u9694\u79bb\u72ec\u7acb\u8fd0\u52a8\u5bf9\u8c61\uff0c\u5e76\u5229\u7528\u8fd0\u52a8\u76f8\u4f3c\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u901a\u8fc7\u5206\u5c42\u805a\u7c7b\u7cbe\u70bc\u5206\u5272\u3002", "result": "\u5728EVIMO2v2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u5b8c\u6574\u7684\u5168\u5149\u6d41\u8ba1\u7b97\u5373\u53ef\u5b9e\u73b0\u51c6\u786e\u7684\u5206\u5272\u548c\u7ffb\u8bd1\u8fd0\u52a8\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7269\u4f53\u8fb9\u754c\u5904\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u5e76\u4e3a\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u673a\u5668\u4eba\u548c\u5bfc\u822a\u5e94\u7528\u63d0\u4f9b\u4e86\u5de8\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14257", "pdf": "https://arxiv.org/pdf/2507.14257", "abs": "https://arxiv.org/abs/2507.14257", "authors": ["Julio Candanedo"], "title": "Linearized Diffusion Map", "categories": ["cs.LG"], "comment": null, "summary": "We introduce the Linearized Diffusion Map (LDM), a novel linear\ndimensionality reduction method constructed via a linear approximation of the\ndiffusion-map kernel. LDM integrates the geometric intuition of diffusion-based\nnonlinear methods with the computational simplicity, efficiency, and\ninterpretability inherent in linear embeddings such as PCA and classical MDS.\nThrough comprehensive experiments on synthetic datasets (Swiss roll and\nhyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that\nLDM captures distinct geometric features of datasets compared to PCA, offering\ncomplementary advantages. Specifically, LDM embeddings outperform PCA in\ndatasets exhibiting explicit manifold structures, particularly in\nhigh-dimensional regimes, whereas PCA remains preferable in scenarios dominated\nby variance or noise. Furthermore, the complete positivity of LDM's kernel\nmatrix allows direct applicability of Non-negative Matrix Factorization (NMF),\nsuggesting opportunities for interpretable latent-structure discovery. Our\nanalysis positions LDM as a valuable new linear dimensionality reduction\ntechnique with promising theoretical and practical extensions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7ebf\u6027\u6269\u6563\u6620\u5c04\uff08LDM\uff09\uff0c\u4e00\u79cd\u65b0\u578b\u7ebf\u6027\u964d\u7ef4\u65b9\u6cd5\uff0c\u5b83\u7ed3\u5408\u4e86\u6269\u6563\u6620\u5c04\u7684\u51e0\u4f55\u76f4\u89c9\u4e0e\u7ebf\u6027\u5d4c\u5165\u7684\u8ba1\u7b97\u4f18\u52bf\uff0c\u5728\u6d41\u5f62\u7ed3\u6784\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8ePCA\uff0c\u5e76\u53ef\u4e0e\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08NMF\uff09\u7ed3\u5408\u4ee5\u53d1\u73b0\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u7ed3\u6784\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u975e\u7ebf\u6027\u6269\u6563\u6620\u5c04\u7684\u51e0\u4f55\u6d1e\u5bdf\u529b\uff0c\u53c8\u80fd\u4fdd\u7559PCA\u7b49\u7ebf\u6027\u65b9\u6cd5\u7684\u8ba1\u7b97\u7b80\u6d01\u6027\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u7684\u7ebf\u6027\u964d\u7ef4\u65b9\u6cd5\u3002", "method": "LDM\u901a\u8fc7\u5bf9\u6269\u6563\u6620\u5c04\u6838\u8fdb\u884c\u7ebf\u6027\u8fd1\u4f3c\u6784\u5efa\u3002\u901a\u8fc7\u5728\u5408\u6210\u6570\u636e\u96c6\uff08Swiss roll, hyperspheres\uff09\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff08MNIST, COIL-20\uff09\u4e0a\u8fdb\u884c\u7efc\u5408\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e86LDM\u7684\u6027\u80fd\u5e76\u4e0ePCA\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u63a2\u7d22\u4e86\u5176\u6838\u77e9\u9635\u7684\u5b8c\u5168\u6b63\u6027\u5728\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08NMF\uff09\u4e2d\u7684\u5e94\u7528\u3002", "result": "LDM\u80fd\u591f\u6355\u83b7\u4e0ePCA\u4e0d\u540c\u7684\u6570\u636e\u96c6\u51e0\u4f55\u7279\u5f81\uff0c\u63d0\u4f9b\u4e92\u8865\u4f18\u52bf\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u5177\u6709\u663e\u5f0f\u6d41\u5f62\u7ed3\u6784\u7684\u6570\u636e\u96c6\uff08\u5c24\u5176\u662f\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\uff09\uff0cLDM\u7684\u5d4c\u5165\u6548\u679c\u4f18\u4e8ePCA\uff1b\u800c\u5728\u65b9\u5dee\u6216\u566a\u58f0\u4e3b\u5bfc\u7684\u573a\u666f\u4e2d\uff0cPCA\u8868\u73b0\u66f4\u4f73\u3002\u6b64\u5916\uff0cLDM\u6838\u77e9\u9635\u7684\u5b8c\u5168\u6b63\u6027\u4f7f\u5176\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08NMF\uff09\uff0c\u6709\u52a9\u4e8e\u53d1\u73b0\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u7ed3\u6784\u3002", "conclusion": "LDM\u662f\u4e00\u79cd\u6709\u4ef7\u503c\u7684\u65b0\u578b\u7ebf\u6027\u964d\u7ef4\u6280\u672f\uff0c\u5177\u6709\u826f\u597d\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u6269\u5c55\u524d\u666f\u3002"}}
