{"id": "2505.00725", "pdf": "https://arxiv.org/pdf/2505.00725", "abs": "https://arxiv.org/abs/2505.00725", "authors": ["Bithiah Yuan"], "title": "FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.1; H.3.3"], "comment": "Submitted in partial fulfillment of the requirements for the Master\n  of Science degree in Computer Science at the University of Freiburg, July 31,\n  2020", "summary": "Motivated by the emerging demand in the financial industry for the automatic\nanalysis of unstructured and structured data at scale, Question Answering (QA)\nsystems can provide lucrative and competitive advantages to companies by\nfacilitating the decision making of financial advisers. Consequently, we\npropose a novel financial QA system using the transformer-based pre-trained\nBERT language model to address the limitations of data scarcity and language\nspecificity in the financial domain. Our system focuses on financial\nnon-factoid answer selection, which retrieves a set of passage-level texts and\nselects the most relevant as the answer. To increase efficiency, we formulate\nthe answer selection task as a re-ranking problem, in which our system consists\nof an Answer Retriever using BM25, a simple information retrieval approach, to\nfirst return a list of candidate answers, and an Answer Re-ranker built with\nvariants of pre-trained BERT language models to re-rank and select the most\nrelevant answers. We investigate various learning, further pre-training, and\nfine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a\nmodel built from applying the Transfer and Adapt further fine-tuning and\npointwise learning approach, is the most effective, improving the\nstate-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on\nNDCG, and 21% on Precision@1.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBERT\u7684\u91d1\u878d\u95ee\u7b54\u7cfb\u7edf\uff08FinBERT-QA\uff09\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u68c0\u7d22\u548c\u91cd\u6392\u5e8f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91d1\u878d\u9886\u57df\u975e\u4e8b\u5b9e\u6027\u95ee\u7b54\u7684\u6027\u80fd\u3002", "motivation": "\u91d1\u878d\u884c\u4e1a\u5bf9\u81ea\u52a8\u5206\u6790\u5927\u89c4\u6a21\u975e\u7ed3\u6784\u5316\u548c\u7ed3\u6784\u5316\u6570\u636e\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u95ee\u7b54\u7cfb\u7edf\u80fd\u8f85\u52a9\u91d1\u878d\u987e\u95ee\u51b3\u7b56\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u5728\u91d1\u878d\u9886\u57df\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u8bed\u8a00 \u0441\u043f\u0435\u0446\u0438\u0444\u0438\u0447\u043d\u043e\u0441\u0442\u044c \u7684\u9650\u5236\u3002", "method": "\u5c06\u7b54\u6848\u9009\u62e9\u89c6\u4e3a\u91cd\u6392\u5e8f\u95ee\u9898\u3002\u7cfb\u7edf\u5305\u542b\u4e24\u90e8\u5206\uff1a1) \u4f7f\u7528BM25\u7684\u7b54\u6848\u68c0\u7d22\u5668\u521d\u6b65\u7b5b\u9009\u5019\u9009\u7b54\u6848\u5217\u8868\uff1b2) \u4f7f\u7528\u9884\u8bad\u7ec3BERT\u6a21\u578b\u53d8\u4f53\u7684\u7b54\u6848\u91cd\u6392\u5e8f\u5668\u5bf9\u5019\u9009\u7b54\u6848\u8fdb\u884c\u91cd\u6392\u5e8f\uff0c\u9009\u51fa\u6700\u76f8\u5173\u7684\u7b54\u6848\u3002\u7814\u7a76\u4e86\u591a\u79cdBERT\u7684\u5b66\u4e60\u3001\u8fdb\u4e00\u6b65\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u91c7\u7528\u201c\u8fc1\u79fb\u548c\u9002\u5e94\u201d\uff08Transfer and Adapt\uff09\u8fdb\u4e00\u6b65\u5fae\u8c03\u548c\u9010\u70b9\u5b66\u4e60\u65b9\u6cd5\u6784\u5efa\u7684FinBERT-QA\u6a21\u578b\u6548\u679c\u6700\u597d\uff0c\u5728FiQA\u6570\u636e\u96c6\u7684\u4efb\u52a12\u4e0a\uff0c\u5176MRR\u63d0\u5347\u4e8616%\uff0cNDCG\u63d0\u5347\u4e8617%\uff0cPrecision@1\u63d0\u5347\u4e8621%\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u4f73\u6c34\u5e73\u3002", "conclusion": "\u63d0\u51fa\u7684FinBERT-QA\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408BM25\u68c0\u7d22\u548c\u4f18\u5316\u7684BERT\u91cd\u6392\u5e8f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u91d1\u878d\u95ee\u7b54\u7684\u6311\u6218\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.00753", "pdf": "https://arxiv.org/pdf/2505.00753", "abs": "https://arxiv.org/abs/2505.00753", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Yankai Chen", "Chunyu Miao", "Hoang Nguyen", "Yue Zhou", "Weizhi Zhang", "Liancheng Fang", "Langzhou He", "Yangning Li", "Yuwei Cao", "Dongyuan Li", "Renhe Jiang", "Philip S. Yu"], "title": "A Survey on Large Language Model based Human-Agent Systems", "categories": ["cs.CL", "cs.LG"], "comment": "Paper lists and resources are available at\n  \\url{https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers}", "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building fully autonomous agents. However, fully autonomous LLM-based agents\nstill face significant challenges, including limited reliability due to\nhallucinations, difficulty in handling complex tasks, and substantial safety\nand ethical risks, all of which limit their feasibility and trustworthiness in\nreal-world applications. To overcome these limitations, LLM-based human-agent\nsystems (LLM-HAS) incorporate human-provided information, feedback, or control\ninto the agent system to enhance system performance, reliability and safety.\nThis paper provides the first comprehensive and structured survey of LLM-HAS.\nIt clarifies fundamental concepts, systematically presents core components\nshaping these systems, including environment & profiling, human feedback,\ninteraction types, orchestration and communication, explores emerging\napplications, and discusses unique challenges and opportunities. By\nconsolidating current knowledge and offering a structured overview, we aim to\nfoster further research and innovation in this rapidly evolving\ninterdisciplinary field. Paper lists and resources are available at\nhttps://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u9996\u6b21\u5168\u9762\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf (LLM-HAS)\uff0c\u65e8\u5728\u901a\u8fc7\u6574\u5408\u4eba\u7c7b\u53cd\u9988\u548c\u63a7\u5236\u6765\u63d0\u5347LLM\u667a\u80fd\u4f53\u7684\u6027\u80fd\u3001\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5b8c\u5168\u81ea\u4e3b\u7684LLM\u667a\u80fd\u4f53\u9762\u4e34\u5e7b\u89c9\u3001\u5904\u7406\u590d\u6742\u4efb\u52a1\u80fd\u529b\u6709\u9650\u4ee5\u53ca\u5b89\u5168\u4f26\u7406\u98ce\u9669\u7b49\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u6587\u5bf9LLM-HAS\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7684\u6587\u732e\u7efc\u8ff0\uff0c\u9610\u8ff0\u4e86\u57fa\u672c\u6982\u5ff5\uff0c\u68b3\u7406\u4e86\u7cfb\u7edf\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\uff08\u5982\u73af\u5883\u3001\u4eba\u7c7b\u53cd\u9988\u3001\u4ea4\u4e92\u7c7b\u578b\u3001\u7f16\u6392\u901a\u4fe1\uff09\uff0c\u63a2\u8ba8\u4e86\u65b0\u5174\u5e94\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u6311\u6218\u4e0e\u673a\u9047\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8eLLM-HAS\u7684\u7ed3\u6784\u5316\u6982\u8ff0\uff0c\u6574\u5408\u4e86\u5f53\u524d\u77e5\u8bc6\uff0c\u660e\u786e\u4e86\u6784\u5efa\u8fd9\u4e9b\u7cfb\u7edf\u7684\u5173\u952e\u8981\u7d20\uff0c\u5e76\u6307\u51fa\u4e86\u4eba\u673a\u534f\u4f5c\u7684\u4f18\u52bf\u3002", "conclusion": "LLM-HAS\u662f\u5f25\u8865\u7eafLLM\u667a\u80fd\u4f53\u4e0d\u8db3\u7684\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002\u672c\u7efc\u8ff0\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u6982\u89c8\uff0c\u4fc3\u8fdb\u8be5\u4ea4\u53c9\u5b66\u79d1\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u521b\u65b0\u3002"}}
{"id": "2505.00776", "pdf": "https://arxiv.org/pdf/2505.00776", "abs": "https://arxiv.org/abs/2505.00776", "authors": ["Alessandro Raganato", "Rafael Pe\u00f1aloza", "Marco Viviani", "Gabriella Pasi"], "title": "Reasoning Capabilities and Invariability of Large Language Models", "categories": ["cs.CL"], "comment": "Accepted for publication in the Proceedings of the 23rd IEEE/WIC\n  International Conference on Web Intelligence and Intelligent Agent Technology\n  (WI-IAT 2024)", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nmanipulating natural language across multiple applications, but their ability\nto handle simple reasoning tasks is often questioned. In this work, we aim to\nprovide a comprehensive analysis of LLMs' reasoning competence, specifically\nfocusing on their prompt dependency. In particular, we introduce a new\nbenchmark dataset with a series of simple reasoning questions demanding shallow\nlogical reasoning. Aligned with cognitive psychology standards, the questions\nare confined to a basic domain revolving around geometric figures, ensuring\nthat responses are independent of any pre-existing intuition about the world\nand rely solely on deduction. An empirical analysis involving zero-shot and\nfew-shot prompting across 24 LLMs of different sizes reveals that, while LLMs\nwith over 70 billion parameters perform better in the zero-shot setting, there\nis still a large room for improvement. An additional test with chain-of-thought\nprompting over 22 LLMs shows that this additional prompt can aid or damage the\nperformance of models, depending on whether the rationale is required before or\nafter the answer.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4e00\u4e2a\u65b0\u7684\u51e0\u4f55\u56fe\u5f62\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7b80\u5355\u63a8\u7406\u80fd\u529b\u53ca\u5176\u5bf9\u63d0\u793a\uff08prompt\uff09\u7684\u4f9d\u8d56\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5904\u7406\u7b80\u5355\u63a8\u7406\u4efb\u52a1\u7684\u80fd\u529b\u5e38\u53d7\u8d28\u7591\u3002\u672c\u7814\u7a76\u65e8\u5728\u5168\u9762\u5206\u6790LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5176\u5bf9\u63d0\u793a\u7684\u4f9d\u8d56\u7a0b\u5ea6\u3002", "method": "1. \u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u9700\u8981\u6d45\u5c42\u903b\u8f91\u63a8\u7406\u7684\u7b80\u5355\u51e0\u4f55\u56fe\u5f62\u95ee\u9898\uff0c\u6392\u9664\u4e16\u754c\u77e5\u8bc6\u5e72\u6270\u3002 2. \u5bf924\u4e2a\u4e0d\u540c\u89c4\u6a21\u7684LLMs\u8fdb\u884c\u4e86\u96f6\u6837\u672c\uff08zero-shot\uff09\u548c\u5c11\u6837\u672c\uff08few-shot\uff09\u63d0\u793a\u6d4b\u8bd5\u3002 3. \u5bf922\u4e2aLLMs\u8fdb\u884c\u4e86\u601d\u7ef4\u94fe\uff08chain-of-thought\uff09\u63d0\u793a\u6d4b\u8bd5\uff0c\u5e76\u6bd4\u8f83\u4e86\u63a8\u7406\u8fc7\u7a0b\u5728\u7b54\u6848\u4e4b\u524d\u6216\u4e4b\u540e\u5448\u73b0\u7684\u6548\u679c\u3002", "result": "\u53c2\u6570\u8d85\u8fc7700\u4ebf\u7684\u5927\u578b\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\u3002\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u6548\u679c\uff08\u63d0\u5347\u6216\u635f\u5bb3\u6027\u80fd\uff09\u53d6\u51b3\u4e8e\u63a8\u7406\u8fc7\u7a0b\u662f\u5728\u7b54\u6848\u4e4b\u524d\u8fd8\u662f\u4e4b\u540e\u88ab\u8981\u6c42\u63d0\u4f9b\u3002", "conclusion": "LLMs\u7684\u7b80\u5355\u63a8\u7406\u80fd\u529b\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d7\u6a21\u578b\u89c4\u6a21\u548c\u63d0\u793a\u65b9\u5f0f\u7684\u5f71\u54cd\uff0c\u5373\u4f7f\u662f\u5927\u578b\u6a21\u578b\u4e5f\u5b58\u5728\u4e0d\u8db3\u3002\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u5e94\u7528\u9700\u8981\u8c28\u614e\u8bbe\u8ba1\u624d\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2505.00814", "pdf": "https://arxiv.org/pdf/2505.00814", "abs": "https://arxiv.org/abs/2505.00814", "authors": ["Mario S\u00e4nger", "Ulf Leser"], "title": "Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction", "categories": ["cs.CL"], "comment": null, "summary": "Automatic relationship extraction (RE) from biomedical literature is critical\nfor managing the vast amount of scientific knowledge produced each year. In\nrecent years, utilizing pre-trained language models (PLMs) has become the\nprevalent approach in RE. Several studies report improved performance when\nincorporating additional context information while fine-tuning PLMs for RE.\nHowever, variations in the PLMs applied, the databases used for augmentation,\nhyper-parameter optimization, and evaluation methods complicate direct\ncomparisons between studies and raise questions about the generalizability of\nthese findings. Our study addresses this research gap by evaluating PLMs\nenhanced with contextual information on five datasets spanning four relation\nscenarios within a consistent evaluation framework. We evaluate three baseline\nPLMs and first conduct extensive hyperparameter optimization. After selecting\nthe top-performing model, we enhance it with additional data, including textual\nentity descriptions, relational information from knowledge graphs, and\nmolecular structure encodings. Our findings illustrate the importance of i) the\nchoice of the underlying language model and ii) a comprehensive hyperparameter\noptimization for achieving strong extraction performance. Although inclusion of\ncontext information yield only minor overall improvements, an ablation study\nreveals substantial benefits for smaller PLMs when such external data was\nincluded during fine-tuning.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5728\u7edf\u4e00\u6846\u67b6\u4e0b\uff0c\u5411\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u6dfb\u52a0\u4e0a\u4e0b\u6587\u4fe1\u606f\uff08\u5982\u5b9e\u4f53\u63cf\u8ff0\u3001\u77e5\u8bc6\u56fe\u8c31\u4fe1\u606f\u3001\u5206\u5b50\u7ed3\u6784\uff09\u5bf9\u751f\u7269\u533b\u5b66\u5173\u7cfb\u62bd\u53d6\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u4e0a\u4e0b\u6587\u4fe1\u606f\u80fd\u63d0\u5347PLMs\u5728\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4f46\u56e0\u6a21\u578b\u3001\u6570\u636e\u3001\u4f18\u5316\u548c\u8bc4\u4f30\u65b9\u6cd5\u7684\u5dee\u5f02\uff0c\u96be\u4ee5\u76f4\u63a5\u6bd4\u8f83\u548c\u63a8\u5e7f\u7814\u7a76\u7ed3\u679c\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u9996\u5148\u8bc4\u4f30\u4e86\u4e09\u79cd\u57fa\u7ebfPLMs\uff0c\u5e76\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8d85\u53c2\u6570\u4f18\u5316\u3002\u7136\u540e\uff0c\u9009\u53d6\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u6587\u672c\u5b9e\u4f53\u63cf\u8ff0\u3001\u77e5\u8bc6\u56fe\u8c31\u5173\u7cfb\u4fe1\u606f\u548c\u5206\u5b50\u7ed3\u6784\u7f16\u7801\u7b49\u989d\u5916\u6570\u636e\u5bf9\u5176\u8fdb\u884c\u589e\u5f3a\u3002\u6574\u4e2a\u8bc4\u4f30\u8fc7\u7a0b\u5728\u5305\u542b\u4e94\u79cd\u6570\u636e\u96c6\uff08\u6db5\u76d6\u56db\u79cd\u5173\u7cfb\u573a\u666f\uff09\u7684\u7edf\u4e00\u6846\u67b6\u4e0b\u8fdb\u884c\uff0c\u5e76\u5305\u542b\u6d88\u878d\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9009\u62e9\u5408\u9002\u7684\u5e95\u5c42\u8bed\u8a00\u6a21\u578b\u548c\u8fdb\u884c\u5168\u9762\u7684\u8d85\u53c2\u6570\u4f18\u5316\u5bf9\u4e8e\u8fbe\u5230\u9ad8\u62bd\u53d6\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u52a0\u5165\u4e0a\u4e0b\u6587\u4fe1\u606f\u5e26\u6765\u7684\u6574\u4f53\u6027\u80fd\u63d0\u5347\u8f83\u5c0f\uff0c\u4f46\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u8f83\u5c0f\u7684PLMs\uff0c\u5728\u5fae\u8c03\u65f6\u52a0\u5165\u8fd9\u4e9b\u5916\u90e8\u6570\u636e\u80fd\u5e26\u6765\u663e\u8457\u7684\u597d\u5904\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u7684\u9009\u62e9\u548c\u8d85\u53c2\u6570\u4f18\u5316\u662f\u751f\u7269\u533b\u5b66\u5173\u7cfb\u62bd\u53d6\u7684\u5173\u952e\u3002\u867d\u7136\u6dfb\u52a0\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u5927\u578b\u6a21\u578b\u7684\u6574\u4f53\u589e\u76ca\u6709\u9650\uff0c\u4f46\u53ef\u4ee5\u663e\u8457\u6539\u5584\u8f83\u5c0fPLMs\u7684\u6027\u80fd\u3002"}}
{"id": "2505.00734", "pdf": "https://arxiv.org/pdf/2505.00734", "abs": "https://arxiv.org/abs/2505.00734", "authors": ["Neil Joshi", "Joshua Carney", "Nathanael Kuo", "Homer Li", "Cheng Peng", "Myron Brown"], "title": "Unconstrained Large-scale 3D Reconstruction and Rendering across Altitudes", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Production of photorealistic, navigable 3D site models requires a large\nvolume of carefully collected images that are often unavailable to first\nresponders for disaster relief or law enforcement. Real-world challenges\ninclude limited numbers of images, heterogeneous unposed cameras, inconsistent\nlighting, and extreme viewpoint differences for images collected from varying\naltitudes. To promote research aimed at addressing these challenges, we have\ndeveloped the first public benchmark dataset for 3D reconstruction and novel\nview synthesis based on multiple calibrated ground-level, security-level, and\nairborne cameras. We present datasets that pose real-world challenges,\nindependently evaluate calibration of unposed cameras and quality of novel\nrendered views, demonstrate baseline performance using recent state-of-practice\nmethods, and identify challenges for further research.", "AI": {"tldr": "\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b\u5730\u9762\u3001\u5b89\u9632\u548c\u7a7a\u4e2d\u591a\u89c6\u89d2\u56fe\u50cf\u7684\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63a8\u52a8\u89e3\u51b3\u7d27\u6025\u54cd\u5e94\u573a\u666f\u4e0b\u56e0\u56fe\u50cf\u6570\u636e\u53d7\u9650\u800c\u96be\u4ee5\u8fdb\u884c\u4e09\u7ef4\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u707e\u96be\u6551\u63f4\u6216\u6267\u6cd5\u7b49\u7d27\u6025\u54cd\u5e94\u573a\u666f\u4e2d\uff0c\u7528\u4e8e\u521b\u5efa\u771f\u5b9e\u611f\u4e09\u7ef4\u6a21\u578b\u7684\u56fe\u50cf\u6570\u636e\u5f80\u5f80\u6570\u91cf\u6709\u9650\u3001\u6765\u6e90\u591a\u6837\uff08\u672a\u6807\u5b9a\u76f8\u673a\uff09\u3001\u5149\u7167\u4e0d\u4e00\u81f4\u4e14\u89c6\u89d2\u5dee\u5f02\u5de8\u5927\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u91cd\u5efa\u3002", "method": "\u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u7ed3\u5408\u4e86\u591a\u79cd\u5df2\u6821\u51c6\u7684\u5730\u9762\u3001\u5b89\u9632\u9ad8\u5ea6\u548c\u7a7a\u4e2d\u76f8\u673a\u89c6\u89d2\u7684\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u7528\u4e8e\u4e09\u7ef4\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u7684\u7814\u7a76\u3002", "result": "\u63d0\u4f9b\u4e86\u80fd\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u6311\u6218\u7684\u6570\u636e\u96c6\uff1b\u72ec\u7acb\u8bc4\u4f30\u4e86\u672a\u6807\u5b9a\u76f8\u673a\u7684\u6807\u5b9a\u6548\u679c\u548c\u65b0\u89c6\u89d2\u6e32\u67d3\u7684\u8d28\u91cf\uff1b\u4f7f\u7528\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u5c55\u793a\u4e86\u57fa\u7ebf\u6027\u80fd\uff1b\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6570\u636e\u96c6\u7684\u53d1\u5e03\u65e8\u5728\u4fc3\u8fdb\u5b66\u672f\u754c\u7814\u53d1\u80fd\u591f\u5e94\u5bf9\u56fe\u50cf\u7a00\u758f\u3001\u76f8\u673a\u5f02\u6784\u3001\u5149\u7167\u548c\u89c6\u89d2\u53d8\u5316\u7b49\u6311\u6218\u7684\u4e09\u7ef4\u91cd\u5efa\u4e0e\u65b0\u89c6\u89d2\u5408\u6210\u6280\u672f\u3002"}}
{"id": "2505.00733", "pdf": "https://arxiv.org/pdf/2505.00733", "abs": "https://arxiv.org/abs/2505.00733", "authors": ["Gustavo Rezende Silva", "Juliane P\u00e4\u00dfler", "S. Lizeth Tapia Tarifa", "Einar Broch Johnsen", "Carlos Hern\u00e1ndez Corbato"], "title": "ROSA: A Knowledge-based Solution for Robot Self-Adaptation", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Autonomous robots must operate in diverse environments and handle multiple\ntasks despite uncertainties. This creates challenges in designing software\narchitectures and task decision-making algorithms, as different contexts may\nrequire distinct task logic and architectural configurations. To address this,\nrobotic systems can be designed as self-adaptive systems capable of adapting\ntheir task execution and software architecture at runtime based on their\ncontext.This paper introduces ROSA, a novel knowledge-based framework for RObot\nSelf-Adaptation, which enables task-and-architecture co-adaptation (TACA) in\nrobotic systems. ROSA achieves this by providing a knowledge model that\ncaptures all application-specific knowledge required for adaptation and by\nreasoning over this knowledge at runtime to determine when and how adaptation\nshould occur. In addition to a conceptual framework, this work provides an\nopen-source ROS 2-based reference implementation of ROSA and evaluates its\nfeasibility and performance in an underwater robotics application. Experimental\nresults highlight ROSA's advantages in reusability and development effort for\ndesigning self-adaptive robotic systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aROSA\u7684\u57fa\u4e8e\u77e5\u8bc6\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u81ea\u6211\u9002\u5e94\uff0c\u5b9e\u73b0\u4efb\u52a1\u548c\u67b6\u6784\u7684\u534f\u540c\u9002\u5e94\uff08TACA\uff09\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u9700\u8981\u5728\u591a\u6837\u5316\u73af\u5883\u548c\u4e0d\u786e\u5b9a\u6027\u4e0b\u5904\u7406\u591a\u4efb\u52a1\uff0c\u8fd9\u7ed9\u8f6f\u4ef6\u67b6\u6784\u548c\u4efb\u52a1\u51b3\u7b56\u7b97\u6cd5\u8bbe\u8ba1\u5e26\u6765\u4e86\u6311\u6218\uff0c\u56e0\u4e3a\u4e0d\u540c\u60c5\u5883\u9700\u8981\u4e0d\u540c\u7684\u4efb\u52a1\u903b\u8f91\u548c\u67b6\u6784\u914d\u7f6e\u3002", "method": "ROSA\u5229\u7528\u4e00\u4e2a\u77e5\u8bc6\u6a21\u578b\u6355\u83b7\u9002\u5e94\u6240\u9700\u7684\u7279\u5b9a\u5e94\u7528\u77e5\u8bc6\uff0c\u5e76\u5728\u8fd0\u884c\u65f6\u57fa\u4e8e\u8be5\u77e5\u8bc6\u8fdb\u884c\u63a8\u7406\uff0c\u4ee5\u51b3\u5b9a\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u8fdb\u884c\u4efb\u52a1\u6267\u884c\u548c\u8f6f\u4ef6\u67b6\u6784\u7684\u9002\u5e94\u3002\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6982\u5ff5\u6846\u67b6\u548c\u57fa\u4e8eROS 2\u7684\u5f00\u6e90\u53c2\u8003\u5b9e\u73b0\u3002", "result": "\u5728\u6c34\u4e0b\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8bc1\u660e\u4e86ROSA\u7684\u53ef\u884c\u6027\u548c\u6027\u80fd\uff0c\u5e76\u7a81\u663e\u4e86\u5176\u5728\u8bbe\u8ba1\u81ea\u9002\u5e94\u673a\u5668\u4eba\u7cfb\u7edf\u65b9\u9762\u7684\u53ef\u91cd\u7528\u6027\u548c\u5f00\u53d1\u6548\u7387\u4f18\u52bf\u3002", "conclusion": "ROSA\u901a\u8fc7\u57fa\u4e8e\u77e5\u8bc6\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u4efb\u52a1\u4e0e\u67b6\u6784\u534f\u540c\u9002\u5e94\uff08TACA\uff09\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u3001\u53ef\u91cd\u7528\u6027\u5e76\u964d\u4f4e\u4e86\u5f00\u53d1\u96be\u5ea6\u3002"}}
{"id": "2505.00787", "pdf": "https://arxiv.org/pdf/2505.00787", "abs": "https://arxiv.org/abs/2505.00787", "authors": ["Lucas N. Alegre", "Ana L. C. Bazzan", "Andr\u00e9 Barreto", "Bruno C. da Silva"], "title": "Constructing an Optimal Behavior Basis for the Option Keyboard", "categories": ["cs.LG", "cs.AI", "I.2"], "comment": null, "summary": "Multi-task reinforcement learning aims to quickly identify solutions for new\ntasks with minimal or no additional interaction with the environment.\nGeneralized Policy Improvement (GPI) addresses this by combining a set of base\npolicies to produce a new one that is at least as good -- though not\nnecessarily optimal -- as any individual base policy. Optimality can be\nensured, particularly in the linear-reward case, via techniques that compute a\nConvex Coverage Set (CCS). However, these are computationally expensive and do\nnot scale to complex domains. The Option Keyboard (OK) improves upon GPI by\nproducing policies that are at least as good -- and often better. It achieves\nthis through a learned meta-policy that dynamically combines base policies.\nHowever, its performance critically depends on the choice of base policies.\nThis raises a key question: is there an optimal set of base policies -- an\noptimal behavior basis -- that enables zero-shot identification of optimal\nsolutions for any linear tasks? We solve this open problem by introducing a\nnovel method that efficiently constructs such an optimal behavior basis. We\nshow that it significantly reduces the number of base policies needed to ensure\noptimality in new tasks. We also prove that it is strictly more expressive than\na CCS, enabling particular classes of non-linear tasks to be solved optimally.\nWe empirically evaluate our technique in challenging domains and show that it\noutperforms state-of-the-art approaches, increasingly so as task complexity\nincreases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u6700\u4f18\u884c\u4e3a\u57fa\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\uff0c\u53ef\u4ee5\u96f6\u6837\u672c\u8bc6\u522b\u7ebf\u6027\u4efb\u52a1\u7684\u6700\u4f18\u89e3\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982GPI\u3001CCS\u3001OK\uff09\u5728\u96f6\u6837\u672c\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\u65f6\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4f9d\u8d56\u57fa\u7840\u7b56\u7565\u9009\u62e9\u6216\u65e0\u6cd5\u4fdd\u8bc1\u6700\u4f18\u6027\u7684\u95ee\u9898\u3002\u7279\u522b\u662f\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u6700\u4f18\u7684\u57fa\u7840\u7b56\u7565\u96c6\uff08\u884c\u4e3a\u57fa\uff09\u6765\u9ad8\u6548\u89e3\u51b3\u4efb\u610f\u7ebf\u6027\u4efb\u52a1\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5730\u6784\u5efa\u4e00\u4e2a\u201c\u6700\u4f18\u884c\u4e3a\u57fa\u201d\u3002\u8fd9\u4e2a\u884c\u4e3a\u57fa\u7531\u4e00\u7ec4\u57fa\u7840\u7b56\u7565\u7ec4\u6210\uff0c\u65e8\u5728\u80fd\u591f\u96f6\u6837\u672c\u5730\u7ec4\u5408\u51fa\u4efb\u4f55\u7ebf\u6027\u4efb\u52a1\u7684\u6700\u4f18\u7b56\u7565\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u786e\u4fdd\u65b0\u4efb\u52a1\u6700\u4f18\u6027\u6240\u9700\u7684\u57fa\u7840\u7b56\u7565\u6570\u91cf\u3002\u7406\u8bba\u8bc1\u660e\uff0c\u5b83\u6bd4\u51f8\u8986\u76d6\u96c6\uff08CCS\uff09\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\uff0c\u80fd\u591f\u6700\u4f18\u5730\u89e3\u51b3\u7279\u5b9a\u7c7b\u522b\u7684\u975e\u7ebf\u6027\u4efb\u52a1\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6280\u672f\u5728\u6311\u6218\u6027\u9886\u57df\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u4e14\u4efb\u52a1\u8d8a\u590d\u6742\uff0c\u4f18\u52bf\u8d8a\u660e\u663e\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u89e3\u51b3\u4e86\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6784\u5efa\u6700\u4f18\u884c\u4e3a\u57fa\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u80fd\u591f\u66f4\u9ad8\u6548\u3001\u66f4\u5177\u8868\u8fbe\u529b\u5730\u5b9e\u73b0\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u96f6\u6837\u672c\u6700\u4f18\u6cdb\u5316\u3002"}}
{"id": "2505.00931", "pdf": "https://arxiv.org/pdf/2505.00931", "abs": "https://arxiv.org/abs/2505.00931", "authors": ["Timur Jaganov", "John Blake", "Juli\u00e1n Villegas", "Nicholas Carr"], "title": "Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 8 Figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "This study investigates the potential for Large Language Models (LLMs) to\nscale-up Dynamic Assessment (DA). To facilitate such an investigation, we first\ndeveloped DynaWrite-a modular, microservices-based grammatical tutoring\napplication which supports multiple LLMs to generate dynamic feedback to\nlearners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural\nchat to have the most potential to scale-up DA in the language learning\nclassroom. Further testing of these two candidates found both models performed\nsimilarly in their ability to accurately identify grammatical errors in user\nsentences. However, GPT-4o consistently outperformed neural chat in the quality\nof its DA by generating clear, consistent, and progressively explicit hints.\nReal-time responsiveness and system stability were also confirmed through\ndetailed performance testing, with GPT-4o exhibiting sufficient speed and\nstability. This study shows that LLMs can be used to scale-up dynamic\nassessment and thus enable dynamic assessment to be delivered to larger groups\nthan possible in traditional teacher-learner settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86DynaWrite\u5e94\u7528\uff0c\u8bc4\u4f30\u4e86LLM\u5728\u82f1\u8bed\u8bed\u6cd5\u52a8\u6001\u8bc4\u4f30\uff08DA\uff09\u4e2d\u7684\u6269\u5c55\u6f5c\u529b\uff0c\u53d1\u73b0GPT-4o\u5728\u63d0\u4f9b\u9ad8\u8d28\u91cf\u52a8\u6001\u53cd\u9988\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4f20\u7edf\u7684\u52a8\u6001\u8bc4\u4f30\u96be\u4ee5\u89c4\u6a21\u5316\u5e94\u7528\u4e8e\u5927\u578b\u5b66\u4e60\u7fa4\u4f53\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u5426\u80fd\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u6a21\u5757\u5316\u7684\u8bed\u6cd5\u8f85\u5bfc\u5e94\u7528DynaWrite\uff1b\u521d\u6b65\u6d4b\u8bd5\u4e8621\u4e2aLLM\uff1b\u7b5b\u9009\u51faGPT-4o\u548cneural chat\u8fdb\u884c\u6df1\u5165\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u5b83\u4eec\u8bc6\u522b\u8bed\u6cd5\u9519\u8bef\u548c\u751f\u6210DA\u63d0\u793a\u7684\u8d28\u91cf\uff1b\u8fdb\u884c\u4e86\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u6d4b\u8bd5\u3002", "result": "GPT-4o\u548cneural chat\u5728\u8bc6\u522b\u8bed\u6cd5\u9519\u8bef\u65b9\u9762\u8868\u73b0\u76f8\u4f3c\uff0c\u4f46GPT-4o\u5728\u751f\u6210\u6e05\u6670\u3001\u4e00\u81f4\u4e14\u6e10\u8fdb\u660e\u786e\u7684DA\u63d0\u793a\u65b9\u9762\u663e\u8457\u66f4\u4f18\uff0c\u5e76\u8868\u73b0\u51fa\u8db3\u591f\u7684\u901f\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660eLLM\uff08\u7279\u522b\u662fGPT-4o\uff09\u53ef\u7528\u4e8e\u6709\u6548\u6269\u5c55\u52a8\u6001\u8bc4\u4f30\uff0c\u4f7f\u5176\u80fd\u670d\u52a1\u4e8e\u6bd4\u4f20\u7edf\u5e08\u751f\u4e92\u52a8\u66f4\u5927\u7684\u7fa4\u4f53\u3002"}}
{"id": "2505.00739", "pdf": "https://arxiv.org/pdf/2505.00739", "abs": "https://arxiv.org/abs/2505.00739", "authors": ["Qiushi Yang", "Yuan Yao", "Miaomiao Cui", "Liefeng Bo"], "title": "MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The recent Segment Anything Model 2 (SAM2) has demonstrated exceptional\ncapabilities in interactive object segmentation for both images and videos.\nHowever, as a foundational model on interactive segmentation, SAM2 performs\nsegmentation directly based on mask memory from the past six frames, leading to\ntwo significant challenges. Firstly, during inference in videos, objects may\ndisappear since SAM2 relies solely on memory without accounting for object\nmotion information, which limits its long-range object tracking capabilities.\nSecondly, its memory is constructed from fixed past frames, making it\nsusceptible to challenges associated with object disappearance or occlusion,\ndue to potentially inaccurate segmentation results in memory. To address these\nproblems, we present MoSAM, incorporating two key strategies to integrate\nobject motion cues into the model and establish more reliable feature memory.\nFirstly, we propose Motion-Guided Prompting (MGP), which represents the object\nmotion in both sparse and dense manners, then injects them into SAM2 through a\nset of motion-guided prompts. MGP enables the model to adjust its focus towards\nthe direction of motion, thereby enhancing the object tracking capabilities.\nFurthermore, acknowledging that past segmentation results may be inaccurate, we\ndevise a Spatial-Temporal Memory Selection (ST-MS) mechanism that dynamically\nidentifies frames likely to contain accurate segmentation in both pixel- and\nframe-level. By eliminating potentially inaccurate mask predictions from\nmemory, we can leverage more reliable memory features to exploit similar\nregions for improving segmentation results. Extensive experiments on various\nbenchmarks of video object segmentation and video instance segmentation\ndemonstrate that our MoSAM achieves state-of-the-art results compared to other\ncompetitors.", "AI": {"tldr": "MoSAM \u901a\u8fc7\u5f15\u5165\u8fd0\u52a8\u5f15\u5bfc\u63d0\u793a\u548c\u65f6\u7a7a\u8bb0\u5fc6\u9009\u62e9\u673a\u5236\uff0c\u6539\u8fdb\u4e86 SAM2 \u5728\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4e2d\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u5176\u5bf9\u7269\u4f53\u6d88\u5931/\u906e\u6321\u654f\u611f\u548c\u7f3a\u4e4f\u8fd0\u52a8\u611f\u77e5\u7684\u95ee\u9898\u3002", "motivation": "\u6700\u65b0\u7684 SAM2 \u6a21\u578b\u5728\u4ea4\u4e92\u5f0f\u5206\u5272\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u76f4\u63a5\u4f9d\u8d56\u8fc7\u53bb\u56fa\u5b9a\u5e27\u7684\u63a9\u7801\u8bb0\u5fc6\u8fdb\u884c\u89c6\u9891\u5206\u5272\u5b58\u5728\u4e24\u5927\u6311\u6218\uff1a1) \u7f3a\u4e4f\u7269\u4f53\u8fd0\u52a8\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u957f\u8ddd\u79bb\u8ddf\u8e2a\u80fd\u529b\uff0c\u5bfc\u81f4\u7269\u4f53\u6d88\u5931\u65f6\u96be\u4ee5\u5904\u7406\uff1b2) \u56fa\u5b9a\u5e27\u8bb0\u5fc6\u6613\u53d7\u7269\u4f53\u6d88\u5931\u6216\u906e\u6321\u5f71\u54cd\uff0c\u56e0\u8bb0\u5fc6\u4e2d\u53ef\u80fd\u5305\u542b\u4e0d\u51c6\u786e\u7684\u5206\u5272\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86 MoSAM \u6a21\u578b\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7b56\u7565\uff1a1) \u8fd0\u52a8\u5f15\u5bfc\u63d0\u793a (MGP)\uff1a\u4ee5\u7a00\u758f\u548c\u5bc6\u96c6\u65b9\u5f0f\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0c\u5e76\u901a\u8fc7\u4e00\u7ec4\u8fd0\u52a8\u5f15\u5bfc\u63d0\u793a\u5c06\u5176\u6ce8\u5165 SAM2\uff0c\u4f7f\u6a21\u578b\u5173\u6ce8\u8fd0\u52a8\u65b9\u5411\uff0c\u589e\u5f3a\u8ddf\u8e2a\u80fd\u529b\uff1b2) \u65f6\u7a7a\u8bb0\u5fc6\u9009\u62e9 (ST-MS)\uff1a\u5728\u50cf\u7d20\u7ea7\u548c\u5e27\u7ea7\u52a8\u6001\u8bc6\u522b\u53ef\u80fd\u5305\u542b\u51c6\u786e\u5206\u5272\u7684\u5e27\uff0c\u5254\u9664\u6f5c\u5728\u4e0d\u51c6\u786e\u7684\u63a9\u7801\u9884\u6d4b\uff0c\u5229\u7528\u66f4\u53ef\u9760\u7684\u8bb0\u5fc6\u7279\u5f81\u6539\u8fdb\u5206\u5272\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u5bf9\u8c61\u5206\u5272 (VOS) \u548c\u89c6\u9891\u5b9e\u4f8b\u5206\u5272 (VIS) \u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMoSAM \u76f8\u8f83\u4e8e\u5176\u4ed6\u7ade\u4e89\u65b9\u6cd5\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f73 (SOTA) \u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u8fd0\u52a8\u7ebf\u7d22\u5e76\u5efa\u7acb\u66f4\u53ef\u9760\u7684\u7279\u5f81\u8bb0\u5fc6\uff0cMoSAM \u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u4e8e SAM2 \u7684\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u957f\u8ddd\u79bb\u8ddf\u8e2a\u3001\u7269\u4f53\u6d88\u5931\u548c\u906e\u6321\u7b49\u6311\u6218\u65b9\u9762\u3002"}}
{"id": "2505.00795", "pdf": "https://arxiv.org/pdf/2505.00795", "abs": "https://arxiv.org/abs/2505.00795", "authors": ["Dibyangshu Mukherjee", "Shivaram Kalyanakrishnan"], "title": "Howard's Policy Iteration is Subexponential for Deterministic Markov Decision Problems with Rewards of Fixed Bit-size and Arbitrary Discount Factor", "categories": ["cs.AI"], "comment": null, "summary": "Howard's Policy Iteration (HPI) is a classic algorithm for solving Markov\nDecision Problems (MDPs). HPI uses a \"greedy\" switching rule to update from any\nnon-optimal policy to a dominating one, iterating until an optimal policy is\nfound. Despite its introduction over 60 years ago, the best-known upper bounds\non HPI's running time remain exponential in the number of states -- indeed even\non the restricted class of MDPs with only deterministic transitions (DMDPs).\nMeanwhile, the tightest lower bound for HPI for MDPs with a constant number of\nactions per state is only linear. In this paper, we report a significant\nimprovement: a subexponential upper bound for HPI on DMDPs, which is\nparameterised by the bit-size of the rewards, while independent of the discount\nfactor. The same upper bound also applies to DMDPs with only two possible\nrewards (which may be of arbitrary size).", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u970d\u534e\u5fb7\u7b56\u7565\u8fed\u4ee3\uff08HPI\uff09\u7b97\u6cd5\u5728\u786e\u5b9a\u6027\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08DMDPs\uff09\u4e0a\u7684\u8fd0\u884c\u65f6\u95f4\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6b21\u6307\u6570\u7ea7\u4e0a\u754c\u3002", "motivation": "\u5c3d\u7ba1HPI\u7b97\u6cd5\u5df2\u5b58\u572860\u591a\u5e74\uff0c\u4f46\u5176\u5df2\u77e5\u7684\u8fd0\u884c\u65f6\u95f4\u4e0a\u754c\u4ecd\u7136\u662f\u6307\u6570\u7ea7\u7684\uff08\u5373\u4f7f\u5728DMDPs\u4e0a\uff09\uff0c\u4e0e\u5df2\u77e5\u7684\u7ebf\u6027\u4e0b\u754c\u4e4b\u95f4\u5b58\u5728\u5de8\u5927\u5dee\u8ddd\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u5206\u6790\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76HPI\u5728DMDPs\u4e0a\u7684\u8fd0\u884c\u65f6\u95f4\uff0c\u63a8\u5bfc\u51fa\u4f9d\u8d56\u4e8e\u5956\u52b1\u503c\u6bd4\u7279\u5927\u5c0f\u3001\u4e0d\u4f9d\u8d56\u4e8e\u6298\u6263\u56e0\u5b50\u7684\u65b0\u4e0a\u754c\u3002", "result": "\u4e3aHPI\u5728DMDPs\u4e0a\u627e\u5230\u4e86\u4e00\u4e2a\u6b21\u6307\u6570\u7ea7\u7684\u8fd0\u884c\u65f6\u95f4\u4e0a\u754c\u3002\u6b64\u4e0a\u754c\u4e0e\u5956\u52b1\u503c\u7684\u6bd4\u7279\u5927\u5c0f\u76f8\u5173\uff0c\u4e0e\u6298\u6263\u56e0\u5b50\u65e0\u5173\uff0c\u5e76\u4e14\u540c\u6837\u9002\u7528\u4e8e\u53ea\u6709\u4e24\u79cd\u5956\u52b1\u503c\u7684DMDPs\u3002", "conclusion": "\u8be5\u7814\u7a76\u663e\u8457\u6539\u8fdb\u4e86\u5bf9HPI\u6548\u7387\u7684\u7406\u8bba\u7406\u89e3\uff0c\u4e3aDMDPs\u63d0\u4f9b\u4e86\u9996\u4e2a\u6b21\u6307\u6570\u7ea7\u4e0a\u754c\uff0c\u7f29\u5c0f\u4e86\u5176\u8fd0\u884c\u65f6\u95f4\u4e0a\u754c\u4e0e\u4e0b\u754c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2505.00792", "pdf": "https://arxiv.org/pdf/2505.00792", "abs": "https://arxiv.org/abs/2505.00792", "authors": ["Tam Nguyen", "Ngoc N. Tran", "Khai Nguyen", "Richard G. Baraniuk"], "title": "Improving Routing in Sparse Mixture of Experts with Graph of Tokens", "categories": ["cs.LG"], "comment": "20 pages, 5 figures, 10 tables", "summary": "Sparse Mixture of Experts (SMoE) has emerged as a key to achieving\nunprecedented scalability in deep learning. By activating only a small subset\nof parameters per sample, SMoE achieves an exponential increase in parameter\ncounts while maintaining a constant computational overhead. However, SMoE\nmodels are susceptible to routing fluctuations--changes in the routing of a\ngiven input to its target expert--at the late stage of model training, leading\nto model non-robustness. In this work, we unveil the limitation of SMoE through\nthe perspective of the probabilistic graphical model (PGM). Through this PGM\nframework, we highlight the independence in the expert-selection of tokens,\nwhich exposes the model to routing fluctuation and non-robustness. Alleviating\nthis independence, we propose the novel Similarity-Aware (S)MoE, which\nconsiders interactions between tokens during expert selection. We then derive a\nnew PGM underlying an (S)MoE-Attention block, going beyond just a single (S)MoE\nlayer. Leveraging the token similarities captured by the attention matrix, we\npropose the innovative Attention-Aware (S)MoE, which employs the attention\nmatrix to guide the routing of tokens to appropriate experts in (S)MoE. We\ntheoretically prove that Similarity/Attention-Aware routing help reduce the\nentropy of expert selection, resulting in more stable token routing mechanisms.\nWe empirically validate our models on various tasks and domains, showing\nsignificant improvements in reducing routing fluctuations, enhancing accuracy,\nand increasing model robustness over the baseline MoE-Transformer with token\nrouting via softmax gating.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u5229\u7528 token \u95f4\u76f8\u4f3c\u6027\u6216\u6ce8\u610f\u529b\u4fe1\u606f\u6307\u5bfc\u4e13\u5bb6\u8def\u7531\u7684\u65b0\u578b (S)MoE \u65b9\u6cd5\uff08Similarity/Attention-Aware (S)MoE\uff09\uff0c\u4ee5\u89e3\u51b3\u6807\u51c6 SMoE \u6a21\u578b\u7684\u8def\u7531\u6ce2\u52a8\u548c\u4e0d\u9c81\u68d2\u95ee\u9898\u3002", "motivation": "\u6807\u51c6\u7684\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\uff08SMoE\uff09\u6a21\u578b\u5728\u8bad\u7ec3\u540e\u671f\u5b58\u5728\u8def\u7531\u6ce2\u52a8\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u548c\u4e0d\u9c81\u68d2\u3002\u7814\u7a76\u8ba4\u4e3a\u8fd9\u662f\u56e0\u4e3a\u5176\u4e13\u5bb6\u9009\u62e9\u673a\u5236\u5ffd\u7565\u4e86 token \u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff08\u72ec\u7acb\u6027\u5047\u8bbe\uff09\u3002", "method": "1. \u4f7f\u7528\u6982\u7387\u56fe\u6a21\u578b\uff08PGM\uff09\u6846\u67b6\u5206\u6790\u5e76\u63ed\u793a\u4e86 SMoE \u7684\u5c40\u9650\u6027\u3002 2. \u63d0\u51fa\u4e86 Similarity-Aware (S)MoE\uff0c\u5728\u4e13\u5bb6\u9009\u62e9\u4e2d\u8003\u8651 token \u95f4\u7684\u4ea4\u4e92\u3002 3. \u63d0\u51fa\u4e86 Attention-Aware (S)MoE\uff0c\u5229\u7528\u6ce8\u610f\u529b\u77e9\u9635\u6307\u5bfc token \u8def\u7531\u3002 4. \u7406\u8bba\u8bc1\u660e\u65b0\u65b9\u6cd5\u6709\u52a9\u4e8e\u964d\u4f4e\u4e13\u5bb6\u9009\u62e9\u7684\u71b5\uff0c\u7a33\u5b9a\u8def\u7531\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u9886\u57df\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf MoE-Transformer \u76f8\u6bd4\uff0c\u63d0\u51fa\u7684 Similarity-Aware \u548c Attention-Aware (S)MoE \u6a21\u578b\u663e\u8457\u51cf\u5c11\u4e86\u8def\u7531\u6ce2\u52a8\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u548c\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u5728 SMoE \u7684\u4e13\u5bb6\u8def\u7531\u673a\u5236\u4e2d\u8003\u8651 token \u95f4\u7684\u76f8\u4f3c\u6027\u6216\u6ce8\u610f\u529b\u4fe1\u606f\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u3001\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.00848", "pdf": "https://arxiv.org/pdf/2505.00848", "abs": "https://arxiv.org/abs/2505.00848", "authors": ["Negar Erfaniantaghvayi", "Zhongyuan Zhao", "Kevin Chan", "Ananthram Swami", "Santiago Segarra"], "title": "SeLR: Sparsity-enhanced Lagrangian Relaxation for Computation Offloading at the Edge", "categories": ["cs.NI", "cs.SY", "eess.SP", "eess.SY", "C.2.1"], "comment": "10 pages, 6 figures, submitted to ACM Mobihoc'25", "summary": "This paper introduces a novel computational approach for offloading sensor\ndata processing tasks to servers in edge networks for better accuracy and\nmakespan. A task is assigned with one of several offloading options, each\ncomprises a server, a route for uploading data to the server, and a service\nprofile that specifies the performance and resource consumption at the server\nand in the network. This offline offloading and routing problem is formulated\nas mixed integer programming (MIP), which is non-convex and HP-hard due to the\ndiscrete decision variables associated to the offloading options. The novelty\nof our approach is to transform this non-convex problem into iterative convex\noptimization by relaxing integer decision variables into continuous space,\ncombining primal-dual optimization for penalizing constraint violations and\nreweighted $L_1$-minimization for promoting solution sparsity, which achieves\nbetter convergence through a smoother path in a continuous search space.\nCompared to existing greedy heuristics, our approach can achieve a better\nPareto frontier in accuracy and latency, scales better to larger problem\ninstances, and can achieve a 7.72--9.17$\\times$ reduction in computational\noverhead of scheduling compared to the optimal solver in hierarchically\norganized edge networks with 300 nodes and 50--100 tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fed\u4ee3\u51f8\u4f18\u5316\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u4f20\u611f\u5668\u6570\u636e\u5904\u7406\u4efb\u52a1\u5378\u8f7d\u5230\u8fb9\u7f18\u7f51\u7edc\u670d\u52a1\u5668\uff0c\u4ee5\u4f18\u5316\u51c6\u786e\u6027\u548c\u5904\u7406\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u4f20\u611f\u5668\u6570\u636e\u5904\u7406\u4efb\u52a1\u5378\u8f7d\u4e2d\u7684\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u6743\u8861\u95ee\u9898\u3002\u4f20\u7edf\u7684\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u6a21\u578b\u662f\u975e\u51f8\u4e14NP\u96be\u7684\uff0c\u96be\u4ee5\u9ad8\u6548\u6c42\u89e3\u3002", "method": "\u5c06\u79bb\u7ebf\u5378\u8f7d\u548c\u8def\u7531\u95ee\u9898\u5efa\u6a21\u4e3aMIP\u3002\u901a\u8fc7\u6574\u6570\u53d8\u91cf\u677e\u5f1b\u3001\u539f\u59cb\u5bf9\u5076\u4f18\u5316\u548c\u91cd\u52a0\u6743L1\u6700\u5c0f\u5316\uff0c\u5c06\u975e\u51f8\u95ee\u9898\u8f6c\u5316\u4e3a\u8fed\u4ee3\u51f8\u4f18\u5316\u95ee\u9898\u6c42\u89e3\u3002", "result": "\u4e0e\u8d2a\u5fc3\u542f\u53d1\u5f0f\u7b97\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u51c6\u786e\u6027\u4e0e\u5ef6\u8fdf\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff1b\u6269\u5c55\u6027\u66f4\u597d\uff1b\u76f8\u6bd4\u6700\u4f18\u6c42\u89e3\u5668\uff0c\u5728\u7279\u5b9a\u89c4\u6a21\u7f51\u7edc\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u8c03\u5ea6\u8ba1\u7b97\u5f00\u9500\uff087.72-9.17\u500d\uff09\u3002", "conclusion": "\u8be5\u8fed\u4ee3\u51f8\u4f18\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u6c42\u89e3\u4f20\u611f\u5668\u6570\u636e\u5378\u8f7d\u95ee\u9898\uff0c\u5728\u89e3\u7684\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.00949", "pdf": "https://arxiv.org/pdf/2505.00949", "abs": "https://arxiv.org/abs/2505.00949", "authors": ["Akhiad Bercovich", "Itay Levy", "Izik Golan", "Mohammad Dabbah", "Ran El-Yaniv", "Omri Puny", "Ido Galil", "Zach Moshe", "Tomer Ronen", "Najeeb Nabwani", "Ido Shahaf", "Oren Tropp", "Ehud Karpas", "Ran Zilberstein", "Jiaqi Zeng", "Soumye Singhal", "Alexander Bukharin", "Yian Zhang", "Tugrul Konuk", "Gerald Shen", "Ameya Sunil Mahabaleshwarkar", "Bilal Kartal", "Yoshi Suhara", "Olivier Delalleau", "Zijia Chen", "Zhilin Wang", "David Mosallanezhad", "Adi Renduchintala", "Haifeng Qian", "Dima Rekesh", "Fei Jia", "Somshubra Majumdar", "Vahid Noroozi", "Wasi Uddin Ahmad", "Sean Narenthiran", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Igor Gitman", "Ivan Moshkov", "Wei Du", "Shubham Toshniwal", "George Armstrong", "Branislav Kisacanin", "Matvei Novikov", "Daria Gitman", "Evelina Bakhturina", "Jane Polak Scowcroft", "John Kamalu", "Dan Su", "Kezhi Kong", "Markus Kliegl", "Rabeeh Karimi", "Ying Lin", "Sanjeev Satheesh", "Jupinder Parmar", "Pritam Gundecha", "Brandon Norick", "Joseph Jennings", "Shrimai Prabhumoye", "Syeda Nahida Akter", "Mostofa Patwary", "Abhinav Khattar", "Deepak Narayanan", "Roger Waleffe", "Jimmy Zhang", "Bor-Yiing Su", "Guyue Huang", "Terry Kong", "Parth Chadha", "Sahil Jain", "Christine Harvey", "Elad Segal", "Jining Huang", "Sergey Kashirsky", "Robert McQueen", "Izzy Putterman", "George Lam", "Arun Venkatesan", "Sherry Wu", "Vinh Nguyen", "Manoj Kilaru", "Andrew Wang", "Anna Warno", "Abhilash Somasamudramath", "Sandip Bhaskar", "Maka Dong", "Nave Assaf", "Shahar Mor", "Omer Ullman Argov", "Scot Junkin", "Oleksandr Romanenko", "Pedro Larroy", "Monika Katariya", "Marco Rovinelli", "Viji Balas", "Nicholas Edelman", "Anahita Bhiwandiwalla", "Muthu Subramaniam", "Smita Ithape", "Karthik Ramamoorthy", "Yuting Wu", "Suguna Varshini Velury", "Omri Almog", "Joyjit Daw", "Denys Fridman", "Erick Galinkin", "Michael Evans", "Katherine Luna", "Leon Derczynski", "Nikki Pope", "Eileen Long", "Seth Schneider", "Guillermo Siman", "Tomasz Grzegorzek", "Pablo Ribalta", "Monika Katariya", "Joey Conway", "Trisha Saar", "Ann Guan", "Krzysztof Pawelec", "Shyamala Prayaga", "Oleksii Kuchaiev", "Boris Ginsburg", "Oluwatobi Olabiyi", "Kari Briski", "Jonathan Cohen", "Bryan Catanzaro", "Jonah Alben", "Yonatan Geifman", "Eric Chung"], "title": "Llama-Nemotron: Efficient Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.", "AI": {"tldr": "\u63a8\u51fa Llama-Nemotron \u7cfb\u5217\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u3001\u5177\u6709\u4f18\u5f02\u63a8\u7406\u80fd\u529b\u548c\u9ad8\u6548\u7387\u7684\u5f02\u6784\u63a8\u7406\u6a21\u578b\u5bb6\u65cf\uff088B\u300149B\u3001253B\uff09\uff0c\u5e76\u63d0\u4f9b\u5f00\u653e\u7684\u4f01\u4e1a\u4f7f\u7528\u8bb8\u53ef\u3002", "motivation": "\u521b\u5efa\u4e0e\u9876\u5c16\u63a8\u7406\u6a21\u578b\uff08\u5982 DeepSeek-R1\uff09\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u5177\u6709\u66f4\u9ad8\u63a8\u7406\u541e\u5410\u91cf\u548c\u5185\u5b58\u6548\u7387\uff0c\u5e76\u91c7\u7528\u5f00\u653e\u8bb8\u53ef\u7684\u63a8\u7406\u6a21\u578b\uff0c\u4ee5\u6ee1\u8db3\u4f01\u4e1a\u9700\u6c42\u548c\u4fc3\u8fdb\u5f00\u653e\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e Llama 3 \u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u4ee5\u52a0\u901f\u63a8\u7406\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u3001\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u4ee5\u53ca\u4e13\u6ce8\u4e8e\u63a8\u7406\u7684\u540e\u8bad\u7ec3\u9636\u6bb5\uff08\u5305\u62ec\u76d1\u7763\u5fae\u8c03\u548c\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\uff09\u3002\u5f15\u5165\u4e86\u52a8\u6001\u63a8\u7406\u5207\u6362\u529f\u80fd\u3002", "result": "\u5f00\u53d1\u4e86 Llama-Nemotron \u7cfb\u5217\u6a21\u578b\uff08Nano\u3001Super\u3001Ultra\uff09\uff0c\u6027\u80fd\u4e0e\u5148\u8fdb\u6a21\u578b\u76f8\u5f53\uff0c\u63a8\u7406\u6548\u7387\u66f4\u4f18\u3002\u6a21\u578b\u3001\u540e\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u4ee3\u7801\u5e93\uff08NeMo, NeMo-Aligner, Megatron-LM\uff09\u5747\u5df2\u5f00\u6e90\uff0c\u5e76\u9996\u6b21\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u652f\u6301\u52a8\u6001\u63a8\u7406\u5207\u6362\u3002", "conclusion": "Llama-Nemotron \u7cfb\u5217\u63d0\u4f9b\u4e86\u4e00\u5957\u5f3a\u5927\u3001\u9ad8\u6548\u4e14\u5f00\u653e\u8bb8\u53ef\u7684\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u53d1\u5e03\u6a21\u578b\u3001\u6570\u636e\u548c\u4ee3\u7801\u4fc3\u8fdb\u4e86\u5f00\u653e\u7814\u7a76\u548c\u6a21\u578b\u5f00\u53d1\uff0c\u5176\u52a8\u6001\u63a8\u7406\u5207\u6362\u529f\u80fd\u662f\u4e00\u5927\u7279\u8272\u3002"}}
{"id": "2505.00740", "pdf": "https://arxiv.org/pdf/2505.00740", "abs": "https://arxiv.org/abs/2505.00740", "authors": ["Zhengbin Zhang", "Yan Wu", "Hongkun Zhang"], "title": "Fast2comm:Collaborative perception combined with prior knowledge", "categories": ["cs.CV", "cs.MA"], "comment": "8pages,8figures", "summary": "Collaborative perception has the potential to significantly enhance\nperceptual accuracy through the sharing of complementary information among\nagents. However, real-world collaborative perception faces persistent\nchallenges, particularly in balancing perception performance and bandwidth\nlimitations, as well as coping with localization errors. To address these\nchallenges, we propose Fast2comm, a prior knowledge-based collaborative\nperception framework. Specifically, (1)we propose a prior-supervised confidence\nfeature generation method, that effectively distinguishes foreground from\nbackground by producing highly discriminative confidence features; (2)we\npropose GT Bounding Box-based spatial prior feature selection strategy to\nensure that only the most informative prior-knowledge features are selected and\nshared, thereby minimizing background noise and optimizing bandwidth efficiency\nwhile enhancing adaptability to localization inaccuracies; (3)we decouple the\nfeature fusion strategies between model training and testing phases, enabling\ndynamic bandwidth adaptation. To comprehensively validate our framework, we\nconduct extensive experiments on both real-world and simulated datasets. The\nresults demonstrate the superior performance of our model and highlight the\nnecessity of the proposed methods. Our code is available at\nhttps://github.com/Zhangzhengbin-TJ/Fast2comm.", "AI": {"tldr": "\u63d0\u51faFast2comm\u6846\u67b6\uff0c\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u4f18\u5316\u534f\u540c\u611f\u77e5\uff0c\u4ee5\u5e94\u5bf9\u5e26\u5bbd\u9650\u5236\u548c\u5b9a\u4f4d\u8bef\u5dee\u6311\u6218\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u534f\u540c\u611f\u77e5\u5728\u5e73\u8861\u611f\u77e5\u6027\u80fd\u4e0e\u5e26\u5bbd\u9650\u5236\u4ee5\u53ca\u5904\u7406\u5b9a\u4f4d\u8bef\u5dee\u65b9\u9762\u5b58\u5728\u6301\u7eed\u6311\u6218\u3002", "method": "\u63d0\u51faFast2comm\u6846\u67b6\uff1a(1) \u91c7\u7528\u5148\u9a8c\u76d1\u7763\u7684\u7f6e\u4fe1\u5ea6\u7279\u5f81\u751f\u6210\u65b9\u6cd5\u533a\u5206\u524d\u666f\u80cc\u666f\uff1b(2) \u91c7\u7528\u57fa\u4e8eGT Bbox\u7684\u7a7a\u95f4\u5148\u9a8c\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff0c\u7b5b\u9009\u4fe1\u606f\u7279\u5f81\uff0c\u51cf\u5c11\u80cc\u666f\u566a\u58f0\uff0c\u4f18\u5316\u5e26\u5bbd\uff0c\u9002\u5e94\u5b9a\u4f4d\u8bef\u5dee\uff1b(3) \u89e3\u8026\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u7684\u7279\u5f81\u878d\u5408\u7b56\u7565\u4ee5\u9002\u5e94\u52a8\u6001\u5e26\u5bbd\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u7a81\u663e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "Fast2comm\u6846\u67b6\u901a\u8fc7\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u534f\u540c\u611f\u77e5\u5728\u5e26\u5bbd\u53d7\u9650\u548c\u5b9a\u4f4d\u4e0d\u7cbe\u786e\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.00802", "pdf": "https://arxiv.org/pdf/2505.00802", "abs": "https://arxiv.org/abs/2505.00802", "authors": ["Vasiliki Papanikou", "Danae Pla Karidi", "Evaggelia Pitoura", "Emmanouil Panagiotou", "Eirini Ntoutsi"], "title": "Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods for Fairness Exploration", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "As Artificial Intelligence (AI) is increasingly used in areas that\nsignificantly impact human lives, concerns about fairness and transparency have\ngrown, especially regarding their impact on protected groups. Recently, the\nintersection of explainability and fairness has emerged as an important area to\npromote responsible AI systems. This paper explores how explainability methods\ncan be leveraged to detect and interpret unfairness. We propose a pipeline that\nintegrates local post-hoc explanation methods to derive fairness-related\ninsights. During the pipeline design, we identify and address critical\nquestions arising from the use of explanations as bias detectors such as the\nrelationship between distributive and procedural fairness, the effect of\nremoving the protected attribute, the consistency and quality of results across\ndifferent explanation methods, the impact of various aggregation strategies of\nlocal explanations on group fairness evaluations, and the overall\ntrustworthiness of explanations as bias detectors. Our results show the\npotential of explanation methods used for fairness while highlighting the need\nto carefully consider the aforementioned critical aspects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528AI\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u89e3\u8bfb\u4e0d\u516c\u5e73\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u6b64\u65b9\u6cd5\u7684\u5173\u952e\u95ee\u9898\u548c\u6f5c\u529b\u3002", "motivation": "\u968f\u7740AI\u5728\u5173\u952e\u9886\u57df\u65e5\u76ca\u666e\u53ca\uff0c\u5176\u5bf9\u53d7\u4fdd\u62a4\u7fa4\u4f53\u53ef\u80fd\u4ea7\u751f\u7684\u504f\u89c1\u5f15\u53d1\u4e86\u5bf9\u516c\u5e73\u6027\u548c\u900f\u660e\u5ea6\u7684\u62c5\u5fe7\uff0c\u9700\u8981\u6709\u6548\u65b9\u6cd5\u6765\u8bc6\u522b\u548c\u7406\u89e3\u8fd9\u4e9b\u504f\u89c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u5c40\u90e8\u4e8b\u540e\u89e3\u91ca\uff08local post-hoc explanation\uff09\u65b9\u6cd5\u7684\u6d41\u7a0b\u6765\u83b7\u53d6\u516c\u5e73\u6027\u76f8\u5173\u7684\u89c1\u89e3\u3002\u7814\u7a76\u4e86\u4f7f\u7528\u89e3\u91ca\u4f5c\u4e3a\u504f\u89c1\u68c0\u6d4b\u5668\u65f6\u51fa\u73b0\u7684\u5173\u952e\u95ee\u9898\uff0c\u5982\u4e0d\u540c\u516c\u5e73\u6027\u6982\u5ff5\u7684\u5173\u7cfb\u3001\u79fb\u9664\u53d7\u4fdd\u62a4\u5c5e\u6027\u7684\u5f71\u54cd\u3001\u4e0d\u540c\u89e3\u91ca\u65b9\u6cd5\u7684\u4e00\u81f4\u6027\u3001\u805a\u5408\u7b56\u7565\u7684\u5f71\u54cd\u4ee5\u53ca\u89e3\u91ca\u672c\u8eab\u7684\u53ef\u4fe1\u5ea6\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u89e3\u91ca\u65b9\u6cd5\u5728\u7528\u4e8e\u516c\u5e73\u6027\u5206\u6790\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4e5f\u5f3a\u8c03\u4e86\u4ed4\u7ec6\u8003\u8651\u65b9\u6cd5\u9009\u62e9\u3001\u805a\u5408\u7b56\u7565\u548c\u89e3\u91ca\u53ef\u4fe1\u5ea6\u7b49\u5173\u952e\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u5229\u7528\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u68c0\u6d4b\u548c\u89e3\u8bfbAI\u4e0d\u516c\u5e73\u6027\u662f\u53ef\u884c\u7684\uff0c\u4f46\u9700\u8981\u8c28\u614e\u5904\u7406\u76f8\u5173\u7684\u65b9\u6cd5\u8bba\u6311\u6218\uff0c\u4ee5\u786e\u4fdd\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2505.00793", "pdf": "https://arxiv.org/pdf/2505.00793", "abs": "https://arxiv.org/abs/2505.00793", "authors": ["Iurii Kemaev", "Dan A Calian", "Luisa M Zintgraf", "Gregory Farquhar", "Hado van Hasselt"], "title": "Scalable Meta-Learning via Mixed-Mode Differentiation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Gradient-based bilevel optimisation is a powerful technique with applications\nin hyperparameter optimisation, task adaptation, algorithm discovery,\nmeta-learning more broadly, and beyond. It often requires differentiating\nthrough the gradient-based optimisation process itself, leading to\n\"gradient-of-a-gradient\" calculations with computationally expensive\nsecond-order and mixed derivatives. While modern automatic differentiation\nlibraries provide a convenient way to write programs for calculating these\nderivatives, they oftentimes cannot fully exploit the specific structure of\nthese problems out-of-the-box, leading to suboptimal performance. In this\npaper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or\nMixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to\nconstruct more efficient and scalable computational graphs yielding over 10x\nmemory and up to 25% wall-clock time improvements over standard implementations\nin modern meta-learning setups.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMixFlow-MG\u7684\u6df7\u5408\u6a21\u5f0f\u5fae\u5206\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u53cc\u5c42\u4f18\u5316\u4e2d\u7684\u5143\u68af\u5ea6\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5185\u5b58\u548c\u65f6\u95f4\u6548\u7387\u3002", "motivation": "\u6807\u51c6\u7684\u81ea\u52a8\u5fae\u5206\u5e93\u5728\u5904\u7406\u53cc\u5c42\u4f18\u5316\u4e2d\u7684\u201c\u68af\u5ea6\u4e4b\u68af\u5ea6\u201d\u8ba1\u7b97\u65f6\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u95ee\u9898\u7ed3\u6784\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6027\u80fd\uff08\u5185\u5b58\u548c\u65f6\u95f4\uff09\u4e0d\u4f73\u3002", "method": "\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86MixFlow-MG\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f7f\u7528\u6df7\u5408\u6a21\u5f0f\u5fae\u5206\uff08mixed-mode differentiation\uff09\u6765\u6784\u5efa\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8ba1\u7b97\u56fe\u3002", "result": "\u4e0e\u6807\u51c6\u5b9e\u73b0\u76f8\u6bd4\uff0cMixFlow-MG\u5728\u73b0\u4ee3\u5143\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0c\u5185\u5b58\u5360\u7528\u51cf\u5c11\u4e8610\u500d\u4ee5\u4e0a\uff0c\u8fd0\u884c\u65f6\u95f4\u7f29\u77ed\u4e86\u9ad8\u8fbe25%\u3002", "conclusion": "MixFlow-MG\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u8ba1\u7b97\u56fe\uff0c\u663e\u8457\u6539\u5584\u4e86\u57fa\u4e8e\u68af\u5ea6\u7684\u53cc\u5c42\u4f18\u5316\u4e2d\u5143\u68af\u5ea6\u8ba1\u7b97\u7684\u6027\u80fd\u3002"}}
{"id": "2505.01185", "pdf": "https://arxiv.org/pdf/2505.01185", "abs": "https://arxiv.org/abs/2505.01185", "authors": ["Nahshon Mokua Obiri", "Kristof Van Laerhoven"], "title": "EnviKal-Loc: Sub-10m Indoor LoRaWAN Localization using an Environmental-Aware Path Loss and Adaptive RSSI Smoothing", "categories": ["cs.NI", "cs.AI", "cs.LG", "eess.SP"], "comment": null, "summary": "LoRaWAN technology's extensive coverage positions it as a strong contender\nfor large-scale IoT deployments. However, achieving sub-10 m accuracy in indoor\nlocalization remains challenging due to complex environmental conditions,\nmultipath fading, and transient obstructions. This paper proposes a lightweight\nbut robust approach combining adaptive filtering with an extended log-distance,\nmulti-wall path loss and shadowing (PLS) model. Our methodology augments\nconventional models with critical LoRaWAN parameters (received signal strength\nindicator (RSSI), frequency, and signal-to-noise ratio (SNR)) and dynamic\nenvironmental indicators (temperature, humidity, carbon dioxide, particulate\nmatter, and barometric pressure). An adaptive Kalman filter reduces RSSI\nfluctuations, isolating persistent trends from momentary noise. Using a\nsix-month dataset of 1,328,334 field measurements, we evaluate three models:\nthe baseline COST 231 multi-wall model (MWM), the baseline model augmented with\nenvironmental parameters (MWM-EP), and a forward-only adaptive Kalman-filtered\nRSSI version of the latter (MWM-EP-KF). Results confirm that the MWM-EP-KF\nachieves a mean absolute error (MAE) of 5.81 m, outperforming both the MWM-EP\n(10.56 m) and the baseline MWM framework (17.98 m). Environmental augmentation\nreduces systematic errors by 41.22%, while Kalman filtering significantly\nenhances robustness under high RSSI volatility by 42.63%, on average across all\ndevices. These findings present an interpretable, efficient solution for\nprecise indoor LoRaWAN localization in dynamically changing environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u81ea\u9002\u5e94\u6ee4\u6ce2\u548c\u73af\u5883\u53c2\u6570\u7684\u589e\u5f3a\u578bLoRaWAN\u5ba4\u5185\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "LoRaWAN\u6280\u672f\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7269\u8054\u7f51\u90e8\u7f72\uff0c\u4f46\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u5b9e\u73b0\u4e9a10\u7c73\u7cbe\u5ea6\u7684\u5b9a\u4f4d\u4ecd\u5177\u6311\u6218\u6027\uff0c\u4e3b\u8981\u53d7\u591a\u5f84\u6548\u5e94\u3001\u77ac\u6001\u969c\u788d\u7269\u7b49\u56e0\u7d20\u5f71\u54cd\u3002", "method": "\u7ed3\u5408\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u4e0e\u6269\u5c55\u7684\u5bf9\u6570\u8ddd\u79bb\u591a\u5899\u8def\u5f84\u635f\u8017\u548c\u9634\u5f71\uff08PLS\uff09\u6a21\u578b\u3002\u8be5\u6a21\u578b\u4e0d\u4ec5\u4f7f\u7528\u4f20\u7edfLoRaWAN\u53c2\u6570\uff08RSSI\u3001\u9891\u7387\u3001SNR\uff09\uff0c\u8fd8\u878d\u5165\u4e86\u52a8\u6001\u73af\u5883\u6307\u6807\uff08\u6e29\u5ea6\u3001\u6e7f\u5ea6\u3001\u4e8c\u6c27\u5316\u78b3\u3001\u9897\u7c92\u7269\u3001\u6c14\u538b\uff09\u3002\u901a\u8fc7\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u51cf\u5c11RSSI\u6ce2\u52a8\u3002\u57fa\u4e8e6\u4e2a\u6708\u7684130\u591a\u4e07\u6b21\u73b0\u573a\u6d4b\u91cf\u6570\u636e\uff0c\u5bf9\u6bd4\u4e86\u4e09\u79cd\u6a21\u578b\uff1a\u57fa\u51c6COST 231\u591a\u5899\u6a21\u578b\uff08MWM\uff09\u3001\u589e\u52a0\u73af\u5883\u53c2\u6570\u7684\u6a21\u578b\uff08MWM-EP\uff09\u4ee5\u53ca\u589e\u52a0\u73af\u5883\u53c2\u6570\u5e76\u4f7f\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5904\u7406RSSI\u7684\u6a21\u578b\uff08MWM-EP-KF\uff09\u3002", "result": "MWM-EP-KF\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4e3a5.81\u7c73\uff0c\u4f18\u4e8eMWM-EP\uff0810.56\u7c73\uff09\u548c\u57fa\u51c6MWM\uff0817.98\u7c73\uff09\u3002\u73af\u5883\u53c2\u6570\u7684\u52a0\u5165\u4f7f\u7cfb\u7edf\u8bef\u5dee\u51cf\u5c11\u4e8641.22%\uff0c\u5361\u5c14\u66fc\u6ee4\u6ce2\u5728\u9ad8RSSI\u6ce2\u52a8\u4e0b\u5c06\u9c81\u68d2\u6027\u5e73\u5747\u63d0\u9ad8\u4e8642.63%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u878d\u5408\u73af\u5883\u56e0\u7d20\u548c\u81ea\u9002\u5e94\u6ee4\u6ce2\uff0c\u53ef\u5728\u52a8\u6001\u53d8\u5316\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u5b9e\u73b0\u7cbe\u786e\u7684LoRaWAN\u5b9a\u4f4d\u3002"}}
{"id": "2505.00977", "pdf": "https://arxiv.org/pdf/2505.00977", "abs": "https://arxiv.org/abs/2505.00977", "authors": ["Yingquan Chen", "Qianmu Li", "Xiaocong Wu", "Huifeng Li", "Qing Chang"], "title": "A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Generating high-quality steganographic text is a fundamental challenge in the\nfield of generative linguistic steganography. This challenge arises primarily\nfrom two aspects: firstly, the capabilities of existing models in text\ngeneration are limited; secondly, embedding algorithms fail to effectively\nmitigate the negative impacts of sensitive information's properties, such as\nsemantic content or randomness. Specifically, to ensure that the recipient can\naccurately extract hidden information, embedding algorithms often have to\nconsider selecting candidate words with relatively low probabilities. This\nphenomenon leads to a decrease in the number of high-probability candidate\nwords and an increase in low-probability candidate words, thereby compromising\nthe semantic coherence and logical fluency of the steganographic text and\ndiminishing the overall quality of the generated steganographic material. To\naddress this issue, this paper proposes a novel embedding algorithm,\ncharacter-based diffusion embedding algorithm (CDEA). Unlike existing embedding\nalgorithms that strive to eliminate the impact of sensitive information's\nproperties on the generation process, CDEA leverages sensitive information's\nproperties. It enhances the selection frequency of high-probability candidate\nwords in the candidate pool based on general statistical properties at the\ncharacter level and grouping methods based on power-law distributions, while\nreducing the selection frequency of low-probability candidate words in the\ncandidate pool. Furthermore, to ensure the effective transformation of\nsensitive information in long sequences, we also introduce the XLNet model.\nExperimental results demonstrate that the combination of CDEA and XLNet\nsignificantly improves the quality of generated steganographic text,\nparticularly in terms of perceptual-imperceptibility.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5b57\u7b26\u6269\u6563\u7684\u5d4c\u5165\u7b97\u6cd5\uff08CDEA\uff09\uff0c\u7ed3\u5408XLNet\u6a21\u578b\uff0c\u5229\u7528\u654f\u611f\u4fe1\u606f\u7279\u6027\u63d0\u5347\u751f\u6210\u5f0f\u8bed\u8a00\u9690\u5199\u7684\u6587\u672c\u8d28\u91cf\u548c\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u8bed\u8a00\u9690\u5199\u6a21\u578b\u80fd\u529b\u6709\u9650\uff0c\u4e14\u5d4c\u5165\u7b97\u6cd5\u96be\u4ee5\u6d88\u9664\u654f\u611f\u4fe1\u606f\u7279\u6027\uff08\u5982\u8bed\u4e49\u3001\u968f\u673a\u6027\uff09\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5bfc\u81f4\u4e3a\u4fdd\u8bc1\u4fe1\u606f\u63d0\u53d6\u800c\u9009\u62e9\u4f4e\u6982\u7387\u8bcd\uff0c\u635f\u5bb3\u4e86\u9690\u5199\u6587\u672c\u7684\u8bed\u4e49\u8fde\u8d2f\u6027\u548c\u6d41\u7545\u6027\uff0c\u964d\u4f4e\u4e86\u6587\u672c\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u5b57\u7b26\u7ea7\u6269\u6563\u5d4c\u5165\u7b97\u6cd5\uff08CDEA\uff09\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u654f\u611f\u4fe1\u606f\u7684\u5b57\u7b26\u7ea7\u7edf\u8ba1\u7279\u6027\u548c\u57fa\u4e8e\u5e42\u5f8b\u5206\u5e03\u7684\u5206\u7ec4\u65b9\u6cd5\uff0c\u63d0\u9ad8\u9ad8\u6982\u7387\u5019\u9009\u8bcd\u7684\u9009\u62e9\u9891\u7387\uff0c\u964d\u4f4e\u4f4e\u6982\u7387\u5019\u9009\u8bcd\u7684\u9009\u62e9\u9891\u7387\u3002\u540c\u65f6\u5f15\u5165XLNet\u6a21\u578b\u5904\u7406\u957f\u5e8f\u5217\u4e2d\u7684\u654f\u611f\u4fe1\u606f\u8f6c\u6362\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCDEA\u4e0eXLNet\u7684\u7ed3\u5408\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u9690\u5199\u6587\u672c\u7684\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u611f\u77e5\u4e0d\u53ef\u5bdf\u89c9\u6027\u65b9\u9762\u6709\u660e\u663e\u6539\u5584\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684CDEA\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u4e0eXLNet\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u901a\u8fc7\u5229\u7528\u654f\u611f\u4fe1\u606f\u81ea\u8eab\u7684\u7279\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86\u751f\u6210\u5f0f\u9690\u5199\u6587\u672c\u7684\u8d28\u91cf\u548c\u9690\u853d\u6027\u3002"}}
{"id": "2505.00741", "pdf": "https://arxiv.org/pdf/2505.00741", "abs": "https://arxiv.org/abs/2505.00741", "authors": ["Srinivas Kanakala", "Sneha Ningappa"], "title": "Detection and Classification of Diseases in Multi-Crop Leaves using LSTM and CNN Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Plant diseases pose a serious challenge to agriculture by reducing crop yield\nand affecting food quality. Early detection and classification of these\ndiseases are essential for minimising losses and improving crop management\npractices. This study applies Convolutional Neural Networks (CNN) and Long\nShort-Term Memory (LSTM) models to classify plant leaf diseases using a dataset\ncontaining 70,295 training images and 17,572 validation images across 38\ndisease classes. The CNN model was trained using the Adam optimiser with a\nlearning rate of 0.0001 and categorical cross-entropy as the loss function.\nAfter 10 training epochs, the model achieved a training accuracy of 99.1% and a\nvalidation accuracy of 96.4%. The LSTM model reached a validation accuracy of\n93.43%. Performance was evaluated using precision, recall, F1-score, and\nconfusion matrix, confirming the reliability of the CNN-based approach. The\nresults suggest that deep learning models, particularly CNN, enable an\neffective solution for accurate and scalable plant disease classification,\nsupporting practical applications in agricultural monitoring.", "AI": {"tldr": "\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc(LSTM)\u5bf9\u690d\u7269\u53f6\u7247\u75c5\u5bb3\u8fdb\u884c\u5206\u7c7b\uff0cCNN\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u690d\u7269\u75c5\u5bb3\u4e25\u91cd\u5f71\u54cd\u4f5c\u7269\u4ea7\u91cf\u548c\u98df\u54c1\u8d28\u91cf\uff0c\u56e0\u6b64\u65e9\u671f\u51c6\u786e\u7684\u68c0\u6d4b\u548c\u5206\u7c7b\u5bf9\u4e8e\u51cf\u5c11\u635f\u5931\u548c\u6539\u5584\u4f5c\u7269\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u5305\u542b70,295\u5f20\u8bad\u7ec3\u56fe\u50cf\u548c17,572\u5f20\u9a8c\u8bc1\u56fe\u50cf\uff08\u6db5\u76d638\u4e2a\u75c5\u5bb3\u7c7b\u522b\uff09\u7684\u6570\u636e\u96c6\uff0c\u5e94\u7528\u4e86CNN\u548cLSTM\u6a21\u578b\u3002CNN\u6a21\u578b\u4f7f\u7528Adam\u4f18\u5316\u5668\uff08\u5b66\u4e60\u73870.0001\uff09\u548c\u5206\u7c7b\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\uff0c\u5171\u8bad\u7ec310\u4e2a\u5468\u671f\u3002", "result": "CNN\u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u4e0a\u8fbe\u523099.1%\u7684\u51c6\u786e\u7387\uff0c\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523096.4%\u7684\u51c6\u786e\u7387\u3002LSTM\u6a21\u578b\u7684\u9a8c\u8bc1\u51c6\u786e\u7387\u4e3a93.43%\u3002\u901a\u8fc7\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548c\u6df7\u6dc6\u77e9\u9635\u8bc4\u4f30\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u786e\u8ba4\u4e86CNN\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7279\u522b\u662fCNN\uff0c\u4e3a\u690d\u7269\u75c5\u5bb3\u7684\u51c6\u786e\u548c\u53ef\u6269\u5c55\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u652f\u6301\u519c\u4e1a\u76d1\u6d4b\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2505.00827", "pdf": "https://arxiv.org/pdf/2505.00827", "abs": "https://arxiv.org/abs/2505.00827", "authors": ["Jing Wang", "Xing Niu", "Juyong Kim", "Jie Shen", "Tong Zhang", "Jeremy C. Weiss"], "title": "MIMIC-\\RNum{4}-Ext-22MCTS: A 22 Millions-Event Temporal Clinical Time-Series Dataset with Relative Timestamp for Risk Prediction", "categories": ["cs.AI"], "comment": null, "summary": "Clinical risk prediction based on machine learning algorithms plays a vital\nrole in modern healthcare. A crucial component in developing a reliable\nprediction model is collecting high-quality time series clinical events. In\nthis work, we release such a dataset that consists of 22,588,586 Clinical Time\nSeries events, which we term MIMIC-\\RNum{4}-Ext-22MCTS. Our source data are\ndischarge summaries selected from the well-known yet unstructured MIMIC-IV-Note\n\\cite{Johnson2023-pg}. We then extract clinical events as short text span from\nthe discharge summaries, along with the timestamps of these events as temporal\ninformation. The general-purpose MIMIC-IV-Note pose specific challenges for our\nwork: it turns out that the discharge summaries are too lengthy for typical\nnatural language models to process, and the clinical events of interest often\nare not accompanied with explicit timestamps. Therefore, we propose a new\nframework that works as follows: 1) we break each discharge summary into\nmanageably small text chunks; 2) we apply contextual BM25 and contextual\nsemantic search to retrieve chunks that have a high potential of containing\nclinical events; and 3) we carefully design prompts to teach the recently\nreleased Llama-3.1-8B \\cite{touvron2023llama} model to identify or infer\ntemporal information of the chunks. We show that the obtained dataset is so\ninformative and transparent that standard models fine-tuned on our dataset are\nachieving significant improvements in healthcare applications. In particular,\nthe BERT model fine-tuned based on our dataset achieves 10\\% improvement in\naccuracy on medical question answering task, and 3\\% improvement in clinical\ntrial matching task compared with the classic BERT. The GPT-2 model, fine-tuned\non our dataset, produces more clinically reliable results for clinical\nquestions.", "AI": {"tldr": "\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b 22,588,586 \u4e2a\u4e34\u5e8a\u65f6\u95f4\u5e8f\u5217\u4e8b\u4ef6\u7684\u65b0\u6570\u636e\u96c6 (MIMIC-IV-Ext-22MCTS)\uff0c\u8be5\u6570\u636e\u96c6\u4ece\u975e\u7ed3\u6784\u5316\u7684 MIMIC-IV-Note \u51fa\u9662\u5c0f\u7ed3\u4e2d\u63d0\u53d6\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u63d0\u5347\u533b\u7597\u5e94\u7528\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4e34\u5e8a\u6570\u636e\u96c6\uff08\u5982 MIMIC-IV-Note\uff09\u662f\u975e\u7ed3\u6784\u5316\u7684\uff0c\u6587\u672c\u8fc7\u957f\u96be\u4ee5\u5904\u7406\uff0c\u4e14\u8bb8\u591a\u4e34\u5e8a\u4e8b\u4ef6\u7f3a\u4e4f\u660e\u786e\u7684\u65f6\u95f4\u6233\uff0c\u8fd9\u963b\u788d\u4e86\u5f00\u53d1\u53ef\u9760\u7684\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u65f6\u95f4\u5e8f\u5217\u4e34\u5e8a\u4e8b\u4ef6\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff1a1) \u5c06\u5197\u957f\u7684\u51fa\u9662\u5c0f\u7ed3\u5206\u5272\u6210\u53ef\u7ba1\u7406\u7684\u5c0f\u6587\u672c\u5757\uff1b2) \u5e94\u7528\u4e0a\u4e0b\u6587 BM25 \u548c\u4e0a\u4e0b\u6587\u8bed\u4e49\u641c\u7d22\u6765\u68c0\u7d22\u53ef\u80fd\u5305\u542b\u4e34\u5e8a\u4e8b\u4ef6\u7684\u6587\u672c\u5757\uff1b3) \u7cbe\u5fc3\u8bbe\u8ba1\u63d0\u793a (prompts) \u6765\u6307\u5bfc Llama-3.1-8B \u6a21\u578b\u8bc6\u522b\u6216\u63a8\u65ad\u6587\u672c\u5757\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u53d6\u5e26\u6709\u65f6\u95f4\u6233\u7684\u4e34\u5e8a\u4e8b\u4ef6\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u5305\u542b\u8d85\u8fc7 2200 \u4e07\u4e2a\u4e8b\u4ef6\u7684 MIMIC-IV-Ext-22MCTS \u6570\u636e\u96c6\u3002\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u5fae\u8c03\u7684\u6807\u51c6\u6a21\u578b\u5728\u533b\u7597\u5e94\u7528\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff1aBERT \u6a21\u578b\u5728\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u63d0\u9ad8 10%\uff0c\u5728\u4e34\u5e8a\u8bd5\u9a8c\u5339\u914d\u4efb\u52a1\u4e0a\u63d0\u9ad8 3%\uff1bGPT-2 \u6a21\u578b\u5728\u4e34\u5e8a\u95ee\u9898\u4e0a\u751f\u6210\u66f4\u53ef\u9760\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u4fe1\u606f\u4e30\u5bcc\u4e14\u900f\u660e\u7684\u5927\u578b\u4e34\u5e8a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u8be5\u6570\u636e\u96c6\u663e\u8457\u63d0\u9ad8\u4e86\u6807\u51c6\u6a21\u578b\u5728\u4e0b\u6e38\u533b\u7597\u4fdd\u5065\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2505.00808", "pdf": "https://arxiv.org/pdf/2505.00808", "abs": "https://arxiv.org/abs/2505.00808", "authors": ["Kola Ayonrinde", "Louis Jaburi"], "title": "A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "15 pages (plus appendices), 2 figures", "summary": "Mechanistic Interpretability aims to understand neural networks through\ncausal explanations. We argue for the Explanatory View Hypothesis: that\nMechanistic Interpretability research is a principled approach to understanding\nmodels because neural networks contain implicit explanations which can be\nextracted and understood. We hence show that Explanatory Faithfulness, an\nassessment of how well an explanation fits a model, is well-defined. We propose\na definition of Mechanistic Interpretability (MI) as the practice of producing\nModel-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural\nnetworks, allowing us to distinguish MI from other interpretability paradigms\nand detail MI's inherent limits. We formulate the Principle of Explanatory\nOptimism, a conjecture which we argue is a necessary precondition for the\nsuccess of Mechanistic Interpretability.", "AI": {"tldr": "\u672c\u6587\u5b9a\u4e49\u4e86\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff08MI\uff09\uff0c\u8ba4\u4e3a\u795e\u7ecf\u7f51\u7edc\u5305\u542b\u53ef\u63d0\u53d6\u7684\u5185\u5728\u89e3\u91ca\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u89e3\u91ca\u548cMI\u6210\u529f\u7684\u539f\u5219\u3002", "motivation": "\u4e3a\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff08MI\uff09\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u539f\u5219\u6027\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u5176\u4f5c\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u7684\u5408\u7406\u6027\u3002", "method": "\u63d0\u51fa\u201c\u89e3\u91ca\u6027\u89c6\u56fe\u5047\u8bf4\u201d\uff1b\u5b9a\u4e49\u201c\u89e3\u91ca\u6027\u5fe0\u5b9e\u5ea6\u201d\uff1b\u5c06MI\u5b9a\u4e49\u4e3a\u4ea7\u751f\u6a21\u578b\u7ea7\u3001\u5b9e\u4f53\u6027\u3001\u56e0\u679c\u673a\u5236\u6027\u3001\u53ef\u8bc1\u4f2a\u7684\u89e3\u91ca\uff1b\u9610\u8ff0\u201c\u89e3\u91ca\u6027\u4e50\u89c2\u539f\u5219\u201d\u3002", "result": "\u8bba\u8bc1\u4e86MI\u662f\u4e00\u79cd\u539f\u5219\u6027\u7684\u65b9\u6cd5\uff1b\u8868\u660e\u201c\u89e3\u91ca\u6027\u5fe0\u5b9e\u5ea6\u201d\u662f\u660e\u786e\u5b9a\u4e49\u7684\uff1b\u63d0\u4f9b\u4e86MI\u7684\u6b63\u5f0f\u5b9a\u4e49\uff0c\u533a\u5206\u4e86MI\u4e0e\u5176\u4ed6\u8303\u5f0f\u5e76\u660e\u786e\u4e86\u5176\u5c40\u9650\u6027\uff1b\u63d0\u51fa\u4e86MI\u6210\u529f\u7684\u5fc5\u8981\u524d\u63d0\u201c\u89e3\u91ca\u6027\u4e50\u89c2\u539f\u5219\u201d\u3002", "conclusion": "\u673a\u5236\u53ef\u89e3\u91ca\u6027\u662f\u4e00\u79cd\u72ec\u7279\u7684\u3001\u6709\u539f\u5219\u7684\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u5176\u901a\u8fc7\u63d0\u53d6\u6a21\u578b\u5185\u5728\u7684\u56e0\u679c\u89e3\u91ca\u5b9e\u73b0\uff0c\u4f46\u5176\u6210\u529f\u53ef\u80fd\u4f9d\u8d56\u4e8e\u89e3\u91ca\u6027\u4e50\u89c2\u539f\u5219\u3002"}}
{"id": "2505.01222", "pdf": "https://arxiv.org/pdf/2505.01222", "abs": "https://arxiv.org/abs/2505.01222", "authors": ["Yunlu Xiao", "Ljiljana Simi\u0107"], "title": "Performance of Cell-Free Massive MIMO in Realistic Urban Propagation Environments", "categories": ["cs.NI", "eess.SP"], "comment": "This paper is accepted to be published in IEEE WCNC25", "summary": "While UE-centric cell-free massive MIMO (CF-mMIMO) provides high and uniform\nthroughput performance under the assumption of a uniform propagation\nenvironment modeled by the log-distance path loss channel model, the\nperformance under a realistic urban propagation environment is not yet fully\naddressed. In this paper we conduct the first comparative performance study of\nCF-mMIMO under both the widely assumed log-distance channel model and the\nrealistic urban propagation environment obtained via raytracing using real 3D\ncity layouts and practical AP locations. Our results show that with the\nraytracing channel model, CF-mMIMO cannot achieve as high and uniform\nthroughput performance as observed with the log-distance channel model, putting\ninto question the attractiveness in practice of CF-mMIMO for real urban\ndeployments.", "AI": {"tldr": "\u7814\u7a76\u5bf9\u6bd4\u4e86\u5728\u5e38\u7528\u5bf9\u6570\u8ddd\u79bb\u4fe1\u9053\u6a21\u578b\u548c\u57fa\u4e8e\u771f\u5b9e\u57ce\u5e02\u5e03\u5c40\u7684\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u578b\u4e0b\uff0c\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u540e\u8005\u6027\u80fd\u663e\u8457\u964d\u4f4e\uff0c\u5bf9\u5176\u5728\u57ce\u5e02\u90e8\u7f72\u7684\u5b9e\u9645\u5438\u5f15\u529b\u63d0\u51fa\u8d28\u7591\u3002", "motivation": "\u867d\u7136\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\uff08CF-mMIMO\uff09\u5728\u5747\u5300\u4f20\u64ad\u73af\u5883\uff08\u5bf9\u6570\u8ddd\u79bb\u6a21\u578b\uff09\u4e0b\u8868\u73b0\u51fa\u9ad8\u4e14\u5747\u5300\u7684\u541e\u5410\u91cf\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u7684\u57ce\u5e02\u4f20\u64ad\u73af\u5883\u4e0b\u7684\u6027\u80fd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u9996\u6b21\u5bf9CF-mMIMO\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u5bf9\u6570\u8ddd\u79bb\u4fe1\u9053\u6a21\u578b\u548c\u901a\u8fc7\u5c04\u7ebf\u8ffd\u8e2a\uff08\u4f7f\u7528\u771f\u5b9e3D\u57ce\u5e02\u5e03\u5c40\u548c\u5b9e\u9645AP\u4f4d\u7f6e\uff09\u83b7\u5f97\u7684\u73b0\u5b9e\u57ce\u5e02\u4f20\u64ad\u73af\u5883\u4e0b\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u6bd4\u8f83\u7814\u7a76\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u5c04\u7ebf\u8ffd\u8e2a\u4fe1\u9053\u6a21\u578b\u65f6\uff0cCF-mMIMO\u65e0\u6cd5\u8fbe\u5230\u50cf\u5bf9\u6570\u8ddd\u79bb\u4fe1\u9053\u6a21\u578b\u6240\u89c2\u5bdf\u5230\u7684\u90a3\u6837\u9ad8\u4e14\u5747\u5300\u7684\u541e\u5410\u91cf\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u7ed3\u679c\u5bf9CF-mMIMO\u5728\u5b9e\u9645\u57ce\u5e02\u90e8\u7f72\u4e2d\u7684\u5438\u5f15\u529b\u63d0\u51fa\u4e86\u8d28\u7591\uff0c\u56e0\u4e3a\u5176\u6027\u80fd\u53ef\u80fd\u88ab\u7b80\u5316\u7684\u4fe1\u9053\u6a21\u578b\u9ad8\u4f30\u4e86\u3002"}}
{"id": "2505.00979", "pdf": "https://arxiv.org/pdf/2505.00979", "abs": "https://arxiv.org/abs/2505.00979", "authors": ["Xuhui Jiang", "Shengjie Ma", "Chengjin Xu", "Cehao Yang", "Liyu Zhang", "Jian Guo"], "title": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success but remain\ndata-inefficient, especially when learning from small, specialized corpora with\nlimited and proprietary data. Existing synthetic data generation methods for\ncontinue pre-training focus on intra-document content and overlook\ncross-document knowledge associations, limiting content diversity and depth. We\npropose Synthetic-on-Graph (SoG), a synthetic data generation framework that\nincorporates cross-document knowledge associations for efficient corpus\nexpansion. SoG constructs a context graph by extracting entities and concepts\nfrom the original corpus, representing cross-document associations, and\nemploying a graph walk strategy for knowledge-associated sampling. This\nenhances synthetic data diversity and coherence, enabling models to learn\ncomplex knowledge structures and handle rare knowledge. To further improve\nsynthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive\nClarifying (CC) synthetic, enhancing reasoning processes and discriminative\npower. Experiments show that SoG outperforms the state-of-the-art (SOTA) method\nin a multi-hop document Q&A dataset while performing comparably to the SOTA\nmethod on the reading comprehension task datasets, which also underscores the\nbetter generalization capability of SoG. Our work advances synthetic data\ngeneration and provides practical solutions for efficient knowledge acquisition\nin LLMs, especially in domains with limited data availability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aSoG\u7684\u56fe\u4e0a\u5408\u6210\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u6355\u83b7\u8de8\u6587\u6863\u5173\u8054\uff0c\u63d0\u5347LLM\u5728\u5c0f\u8bed\u6599\u5e93\u4e0a\u7684\u5b66\u4e60\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5c0f\u578b\u3001\u4e13\u4e1a\u3001\u6570\u636e\u6709\u9650\u7684\u8bed\u6599\u5e93\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u73b0\u6709\u5408\u6210\u6570\u636e\u65b9\u6cd5\u7f3a\u4e4f\u8de8\u6587\u6863\u77e5\u8bc6\u5173\u8054\uff0c\u9650\u5236\u4e86\u5185\u5bb9\u591a\u6837\u6027\u548c\u6df1\u5ea6\u3002", "method": "\u6784\u5efa\u4e0a\u4e0b\u6587\u56fe\u8c31\uff08\u63d0\u53d6\u5b9e\u4f53\u6982\u5ff5\u8868\u793a\u8de8\u6587\u6863\u5173\u8054\uff09\uff0c\u91c7\u7528\u56fe\u6e38\u8d70\u7b56\u7565\u8fdb\u884c\u77e5\u8bc6\u5173\u8054\u91c7\u6837\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u601d\u7ef4\u94fe\uff08CoT\uff09\u548c\u5bf9\u6bd4\u6f84\u6e05\uff08CC\uff09\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\u3002", "result": "SoG\u5728\u591a\u8df3\u6587\u6863\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\uff08SOTA\uff09\u65b9\u6cd5\uff0c\u5728\u9605\u8bfb\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0eSOTA\u76f8\u5f53\uff0c\u5e76\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SoG\u6846\u67b6\u63a8\u52a8\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4e3a\u6570\u636e\u6709\u9650\u9886\u57df\u4e2dLLM\u7684\u9ad8\u6548\u77e5\u8bc6\u83b7\u53d6\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00742", "pdf": "https://arxiv.org/pdf/2505.00742", "abs": "https://arxiv.org/abs/2505.00742", "authors": ["Jiaxu Qian", "Chendong Wang", "Yifan Yang", "Chaoyun Zhang", "Huiqiang Jiang", "Xufang Luo", "Yu Kang", "Qingwei Lin", "Anlan Zhang", "Shiqi Jiang", "Ting Cao", "Tianjun Mao", "Suman Banerjee", "Guyue Liu", "Saravan Rajmohan", "Dongmei Zhang", "Yuqing Yang", "Qi Zhang", "Lili Qiu"], "title": "Zoomer: Adaptive Image Focus Optimization for Black-box MLLM", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have\nbroadened the scope of vision-language tasks, excelling in applications like\nimage captioning and interactive question-answering. However, these models\nstruggle with accurately processing visual data, particularly in tasks\nrequiring precise object recognition and fine visual details. Stringent token\nlimits often result in the omission of critical information, hampering\nperformance. To address these limitations, we introduce \\SysName, a novel\nvisual prompting mechanism designed to enhance MLLM performance while\npreserving essential visual details within token limits. \\SysName features\nthree key innovations: a prompt-aware strategy that dynamically highlights\nrelevant image regions, a spatial-preserving orchestration schema that\nmaintains object integrity, and a budget-aware prompting method that balances\nglobal context with crucial visual details. Comprehensive evaluations across\nmultiple datasets demonstrate that \\SysName consistently outperforms baseline\nmethods, achieving up to a $26.9\\%$ improvement in accuracy while significantly\nreducing token consumption.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a \\SysName \u7684\u65b0\u89c6\u89c9\u63d0\u793a\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u9ad8\u4eae\u533a\u57df\u3001\u4fdd\u6301\u7a7a\u95f4\u4fe1\u606f\u548c\u5e73\u8861\u9884\u7b97\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLM\uff09\u5904\u7406\u89c6\u89c9\u7ec6\u8282\u7684\u80fd\u529b\uff0c\u540c\u65f6\u51cf\u5c11 token \u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u7cbe\u786e\u7269\u4f53\u8bc6\u522b\u548c\u7cbe\u7ec6\u89c6\u89c9\u7ec6\u8282\u7684\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4e25\u683c\u7684 token \u9650\u5236\u5e38\u5bfc\u81f4\u5173\u952e\u89c6\u89c9\u4fe1\u606f\u4e22\u5931\u3002", "method": "\u5f15\u5165 \\SysName \u89c6\u89c9\u63d0\u793a\u673a\u5236\uff0c\u5305\u542b\u4e09\u5927\u521b\u65b0\uff1a1) \u611f\u77e5\u63d0\u793a\u7684\u7b56\u7565\u52a8\u6001\u9ad8\u4eae\u76f8\u5173\u56fe\u50cf\u533a\u57df\uff1b2) \u7a7a\u95f4\u4fdd\u6301\u7684\u7f16\u6392\u6a21\u5f0f\u7ef4\u62a4\u5bf9\u8c61\u5b8c\u6574\u6027\uff1b3) \u611f\u77e5\u9884\u7b97\u7684\u63d0\u793a\u65b9\u6cd5\u5e73\u8861\u5168\u5c40\u4e0a\u4e0b\u6587\u4e0e\u5173\u952e\u89c6\u89c9\u7ec6\u8282\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\\SysName \u6bd4\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\uff0c\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u5347\u4e86 26.9%\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86 token \u6d88\u8017\u3002", "conclusion": "\\SysName \u80fd\u591f\u6709\u6548\u589e\u5f3a MLLM \u5728 token \u9650\u5236\u5185\u5904\u7406\u7cbe\u786e\u89c6\u89c9\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.00875", "pdf": "https://arxiv.org/pdf/2505.00875", "abs": "https://arxiv.org/abs/2505.00875", "authors": ["Ramesh Manuvinakurike", "Emanuel Moss", "Elizabeth Anne Watkins", "Saurav Sahay", "Giuseppe Raffa", "Lama Nachman"], "title": "Thoughts without Thinking: Reconsidering the Explanatory Value of Chain-of-Thought Reasoning in LLMs through Agentic Pipelines", "categories": ["cs.AI"], "comment": null, "summary": "Agentic pipelines present novel challenges and opportunities for\nhuman-centered explainability. The HCXAI community is still grappling with how\nbest to make the inner workings of LLMs transparent in actionable ways. Agentic\npipelines consist of multiple LLMs working in cooperation with minimal human\ncontrol. In this research paper, we present early findings from an agentic\npipeline implementation of a perceptive task guidance system. Through\nquantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT)\nreasoning, a common vehicle for explainability in LLMs, operates within agentic\npipelines. We demonstrate that CoT reasoning alone does not lead to better\noutputs, nor does it offer explainability, as it tends to produce explanations\nwithout explainability, in that they do not improve the ability of end users to\nbetter understand systems or achieve their goals.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u591a\u667a\u80fd\u4f53\u6d41\u7a0b\u4e2d\uff0c\u601d\u7ef4\u94fe\uff08CoT\uff09\u672c\u8eab\u65e2\u4e0d\u80fd\u63d0\u9ad8\u8f93\u51fa\u8d28\u91cf\uff0c\u4e5f\u4e0d\u80fd\u4e3a\u7528\u6237\u63d0\u4f9b\u6709\u6548\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u6d41\u7a0b\uff08\u591a\u4e2aLLM\u534f\u540c\u5de5\u4f5c\uff0c\u4eba\u5de5\u5e72\u9884\u5c11\uff09\u7ed9\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\u53ef\u89e3\u91ca\u6027\uff08HCXAI\uff09\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u548c\u673a\u9047\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u6709\u6548\u89e3\u91ca\u5176\u5185\u90e8\u8fd0\u4f5c\u3002", "method": "\u5b9e\u73b0\u4e86\u4e00\u4e2a\u611f\u77e5\u4efb\u52a1\u6307\u5bfc\u7cfb\u7edf\u7684\u591a\u667a\u80fd\u4f53\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u5176\u4e2d\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u4f5c\u4e3a\u89e3\u91ca\u673a\u5236\u7684\u6548\u679c\u3002", "result": "\u5355\u72ec\u4f7f\u7528\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u5e76\u672a\u6539\u5584\u591a\u667a\u80fd\u4f53\u6d41\u7a0b\u7684\u8f93\u51fa\u7ed3\u679c\uff0c\u4e5f\u672a\u80fd\u63d0\u4f9b\u6709\u6548\u7684\u53ef\u89e3\u91ca\u6027\u3002\u5b83\u4ea7\u751f\u7684\u89e3\u91ca\u66f4\u50cf\u662f\u201c\u6ca1\u6709\u89e3\u91ca\u529b\u7684\u89e3\u91ca\u201d\uff0c\u65e0\u6cd5\u5e2e\u52a9\u6700\u7ec8\u7528\u6237\u66f4\u597d\u5730\u7406\u89e3\u7cfb\u7edf\u6216\u5b9e\u73b0\u76ee\u6807\u3002", "conclusion": "\u5728\u591a\u667a\u80fd\u4f53\u6d41\u7a0b\u4e2d\uff0c\u4ec5\u9760\u601d\u7ef4\u94fe\uff08CoT\uff09\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u63a2\u7d22\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.00810", "pdf": "https://arxiv.org/pdf/2505.00810", "abs": "https://arxiv.org/abs/2505.00810", "authors": ["Jordi de la Torre"], "title": "Scalable Unit Harmonization in Medical Informatics Using Bi-directional Transformers and Bayesian-Optimized BM25 and Sentence Embedding Retrieval", "categories": ["cs.LG"], "comment": null, "summary": "Objective: To develop and evaluate a scalable methodology for harmonizing\ninconsistent units in large-scale clinical datasets, addressing a key barrier\nto data interoperability.\n  Materials and Methods: We designed a novel unit harmonization system\ncombining BM25, sentence embeddings, Bayesian optimization, and a bidirectional\ntransformer based binary classifier for retrieving and matching laboratory test\nentries. The system was evaluated using the Optum Clinformatics Datamart\ndataset (7.5 billion entries). We implemented a multi-stage pipeline:\nfiltering, identification, harmonization proposal generation, automated\nre-ranking, and manual validation. Performance was assessed using Mean\nReciprocal Rank (MRR) and other standard information retrieval metrics.\n  Results: Our hybrid retrieval approach combining BM25 and sentence embeddings\n(MRR: 0.8833) significantly outperformed both lexical-only (MRR: 0.7985) and\nembedding-only (MRR: 0.5277) approaches. The transformer-based reranker further\nimproved performance (absolute MRR improvement: 0.10), bringing the final\nsystem MRR to 0.9833. The system achieved 83.39\\% precision at rank 1 and\n94.66\\% recall at rank 5.\n  Discussion: The hybrid architecture effectively leverages the complementary\nstrengths of lexical and semantic approaches. The reranker addresses cases\nwhere initial retrieval components make errors due to complex semantic\nrelationships in medical terminology.\n  Conclusion: Our framework provides an efficient, scalable solution for unit\nharmonization in clinical datasets, reducing manual effort while improving\naccuracy. Once harmonized, data can be reused seamlessly in different analyses,\nensuring consistency across healthcare systems and enabling more reliable\nmulti-institutional studies and meta-analyses.", "AI": {"tldr": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7edf\u4e00\u5927\u89c4\u6a21\u4e34\u5e8a\u6570\u636e\u96c6\u4e2d\u7684\u4e0d\u4e00\u81f4\u5355\u4f4d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u4e92\u64cd\u4f5c\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u4e34\u5e8a\u6570\u636e\u96c6\u4e2d\u6d4b\u91cf\u5355\u4f4d\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u662f\u5b9e\u73b0\u6570\u636e\u4e92\u64cd\u4f5c\u6027\u7684\u5173\u952e\u969c\u788d\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u5408 BM25\u3001\u53e5\u5b50\u5d4c\u5165\u3001\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u57fa\u4e8e\u53cc\u5411 Transformer \u7684\u4e8c\u5143\u5206\u7c7b\u5668\u7684\u65b0\u578b\u5355\u4f4d\u7edf\u4e00\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u91c7\u7528\u591a\u9636\u6bb5\u6d41\u7a0b\uff1a\u8fc7\u6ee4\u3001\u8bc6\u522b\u3001\u7edf\u4e00\u65b9\u6848\u751f\u6210\u3001\u81ea\u52a8\u91cd\u6392\u5e8f\u548c\u624b\u52a8\u9a8c\u8bc1\u3002", "result": "\u6df7\u5408\u68c0\u7d22\u65b9\u6cd5\uff08BM25+\u53e5\u5b50\u5d4c\u5165\uff09\u7684\u5e73\u5747\u5012\u6570\u6392\u540d\uff08MRR\uff09\u4e3a 0.8833\uff0c\u663e\u8457\u4f18\u4e8e\u7eaf\u8bcd\u6c47\uff080.7985\uff09\u548c\u7eaf\u5d4c\u5165\uff080.5277\uff09\u65b9\u6cd5\u3002Transformer \u91cd\u6392\u5e8f\u5668\u5c06\u6700\u7ec8 MRR \u63d0\u5347\u81f3 0.9833\u3002\u7cfb\u7edf\u5728\u6392\u540d 1 \u65f6\u7684\u7cbe\u786e\u5ea6\u4e3a 83.39%\uff0c\u5728\u6392\u540d\u524d 5 \u65f6\u7684\u53ec\u56de\u7387\u4e3a 94.66%\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u4e34\u5e8a\u6570\u636e\u5355\u4f4d\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u4f7f\u7edf\u4e00\u540e\u7684\u6570\u636e\u80fd\u591f\u65e0\u7f1d\u91cd\u7528\u4e8e\u591a\u673a\u6784\u7814\u7a76\u548c\u5143\u5206\u6790\u3002"}}
