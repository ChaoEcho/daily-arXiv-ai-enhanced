<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 38]
- [cs.CV](#cs.CV) [Total: 51]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.LG](#cs.LG) [Total: 50]
- [cs.NI](#cs.NI) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations](https://arxiv.org/abs/2509.07135)
*Ruggero Marino Lazzaroni,Alessandro Angioi,Michelangelo Puliga,Davide Sanna,Roberto Marras*

Main category: cs.CL

TL;DR: 本文介绍了MedBench-IT，首个用于评估LLM在意大利医学入学考试中表现的综合基准，包含17,410道多项选择题，并对多款LLM进行了评估及多维度分析。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在教育领域展现潜力，但针对非英语语言和专业领域的基准测试资源稀缺，特别是在意大利医学教育领域，缺乏用于评估LLM的标准化工具。

Method: 研究引入了MedBench-IT基准，该基准由17,410道来自领先出版商Edizioni Simone的专家编写的多项选择题组成，涵盖生物、化学、逻辑、通识、数学和物理六个科目及三个难度级别。研究评估了包括GPT-4o和Claude系列在内的专有LLM，以及参数小于30B的开源模型。除了准确性，还进行了可重复性测试、顺序偏差分析、推理提示评估，并探究了问题可读性与模型性能之间的相关性。

Result: 可重复性测试显示响应一致性达88.86%（因科目而异），顺序偏差分析表明影响微乎其微。研究发现问题可读性与模型性能之间存在统计学上显著但程度较小的负相关关系。评估揭示了LLM在该特定领域的当前能力。

Conclusion: MedBench-IT为意大利NLP社区、教育技术开发者和从业者提供了宝贵的资源，深入洞察了LLM在该关键领域的现有能力，并确立了标准化的评估方法。

Abstract: Large language models (LLMs) show increasing potential in education, yet
benchmarks for non-English languages in specialized domains remain scarce. We
introduce MedBench-IT, the first comprehensive benchmark for evaluating LLMs on
Italian medical university entrance examinations. Sourced from Edizioni Simone,
a leading preparatory materials publisher, MedBench-IT comprises 17,410
expert-written multiple-choice questions across six subjects (Biology,
Chemistry, Logic, General Culture, Mathematics, Physics) and three difficulty
levels. We evaluated diverse models including proprietary LLMs (GPT-4o, Claude
series) and resource-efficient open-source alternatives (<30B parameters)
focusing on practical deployability.
  Beyond accuracy, we conducted rigorous reproducibility tests (88.86% response
consistency, varying by subject), ordering bias analysis (minimal impact), and
reasoning prompt evaluation. We also examined correlations between question
readability and model performance, finding a statistically significant but
small inverse relationship. MedBench-IT provides a crucial resource for Italian
NLP community, EdTech developers, and practitioners, offering insights into
current capabilities and standardized evaluation methodology for this critical
domain.

</details>


### [2] [The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties](https://arxiv.org/abs/2509.07139)
*William Chen,Chutong Meng,Jiatong Shi,Martijn Bartelds,Shih-Heng Wang,Hsiu-Hsuan Wang,Rafael Mosquera,Sara Hincapie,Dan Jurafsky,Antonis Anastasopoulos,Hung-yi Lee,Karen Livescu,Shinji Watanabe*

Main category: cs.CL

TL;DR: Interspeech 2025 ML-SUPERB 2.0挑战赛旨在通过新的多语言测试套件和在线评估，提升多语言ASR对多样化语言、口音和方言的性能，参赛提交显著超越基线。


<details>
  <summary>Details</summary>
Motivation: 多语言自动语音识别（ASR）的进展在不同语言和语言变体之间分布不均，需要推进最先进（SOTA）的ASR模型以解决这一问题。

Method: 组织了Interspeech 2025 ML-SUPERB 2.0挑战赛。构建了一个包含200多种语言、口音和方言数据的新测试套件来评估多语言语音模型。引入了基于DynaBench的在线评估服务器，为参与者的模型设计提供了灵活性。

Result: 挑战赛收到了5份提交，所有提交均超越了基线模型。最佳提交在通用多语言测试集上，LID准确率绝对提升23%，CER降低18%。在口音和方言数据上，最佳提交的CER降低了30.2%，LID准确率提高了15.7%。

Conclusion: 社区挑战赛对于使语音技术更具包容性至关重要，它能有效推动多语言ASR在多样化语言环境下的性能提升。

Abstract: Recent improvements in multilingual ASR have not been equally distributed
across languages and language varieties. To advance state-of-the-art (SOTA) ASR
models, we present the Interspeech 2025 ML-SUPERB 2.0 Challenge. We construct a
new test suite that consists of data from 200+ languages, accents, and dialects
to evaluate SOTA multilingual speech models. The challenge also introduces an
online evaluation server based on DynaBench, allowing for flexibility in model
design and architecture for participants. The challenge received 5 submissions
from 3 teams, all of which outperformed our baselines. The best-performing
submission achieved an absolute improvement in LID accuracy of 23% and a
reduction in CER of 18% when compared to the best baseline on a general
multilingual test set. On accented and dialectal data, the best submission
obtained 30.2% lower CER and 15.7% higher LID accuracy, showing the importance
of community challenges in making speech technologies more inclusive.

</details>


### [3] [Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models](https://arxiv.org/abs/2509.07142)
*Zhiyin Tan,Jennifer D'Souza*

Main category: cs.CL

TL;DR: 本研究提出了一个利用大型语言模型（LLM）自动评估动态演化主题模型的框架。


<details>
  <summary>Details</summary>
Motivation: 现有主题模型自动化评估指标（如一致性和多样性）仅捕捉狭窄统计模式，无法解释实际中的语义失败，导致难以有效评估和维护动态知识领域的主题相关性。

Method: 引入了一个面向目的的评估框架，包含九种基于LLM的指标，覆盖词汇有效性、主题内语义合理性、主题间结构合理性及文档-主题对齐合理性四个维度。该框架通过对抗性和抽样协议进行验证，并应用于新闻、学术出版物和社交媒体等多种数据集及不同的主题建模方法和开源LLM。

Result: 基于LLM的指标提供了可解释、鲁棒且与任务相关的评估，成功揭示了传统指标常忽略的主题模型关键弱点，如冗余和语义漂移。

Conclusion: 这些发现支持开发可扩展、细粒度的评估工具，以有效维护动态数据集中主题的相关性。

Abstract: This study presents a framework for automated evaluation of dynamically
evolving topic models using Large Language Models (LLMs). Topic modeling is
essential for organizing and retrieving scholarly content in digital library
systems, helping users navigate complex and evolving knowledge domains.
However, widely used automated metrics, such as coherence and diversity, often
capture only narrow statistical patterns and fail to explain semantic failures
in practice. We introduce a purpose-oriented evaluation framework that employs
nine LLM-based metrics spanning four key dimensions of topic quality: lexical
validity, intra-topic semantic soundness, inter-topic structural soundness, and
document-topic alignment soundness. The framework is validated through
adversarial and sampling-based protocols, and is applied across datasets
spanning news articles, scholarly publications, and social media posts, as well
as multiple topic modeling methods and open-source LLMs. Our analysis shows
that LLM-based metrics provide interpretable, robust, and task-relevant
assessments, uncovering critical weaknesses in topic models such as redundancy
and semantic drift, which are often missed by traditional metrics. These
results support the development of scalable, fine-grained evaluation tools for
maintaining topic relevance in dynamic datasets. All code and data supporting
this work are accessible at
https://github.com/zhiyintan/topic-model-LLMjudgment.

</details>


### [4] [Towards EnergyGPT: A Large Language Model Specialized for the Energy Sector](https://arxiv.org/abs/2509.07177)
*Amal Chebbi,Babajide Kolade*

Main category: cs.CL

TL;DR: 本文介绍了EnergyGPT，一个通过微调LLaMA 3.1-8B构建的能源领域专用语言模型，它在能源相关任务上表现优于基座模型，且无需大规模基础设施。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在能源等专业领域因缺乏深度技术专长和精确领域知识而效果受限。

Method: 通过对LLaMA 3.1-8B模型使用监督微调（SFT）技术，并利用高质量、精心策划的能源相关文本语料库，开发了领域专用语言模型EnergyGPT。研究涵盖了数据收集与整理、模型微调、基准设计、LLM-judge选择、评估与部署的完整开发流程。

Result: 所采用的训练策略在无需大规模基础设施的情况下，提升了模型在能源领域的专业相关性和性能。通过领域特定问答基准评估，EnergyGPT在大多数能源相关的语言理解和生成任务中均优于基座模型。

Conclusion: EnergyGPT的成功开发与评估证明了通过领域专业化微调，可以有效提升大语言模型在能源等专业领域的性能和相关性。

Abstract: Large Language Models have demonstrated impressive capabilities across
various domains. However, their general-purpose nature often limits their
effectiveness in specialized fields such as energy, where deep technical
expertise and precise domain knowledge are essential. In this paper, we
introduce EnergyGPT, a domain-specialized language model tailored for the
energy sector, developed by fine-tuning LLaMA 3.1-8B model using Supervised
Fine-Tuning on a high-quality, curated corpus of energy-related texts. We
present a complete development pipeline, including data collection and
curation, model fine-tuning, benchmark design and LLM-judge choice, evaluation
and deployment. Through this work, we demonstrate that our training strategy
enables improvements in domain relevance and performance without the need for
large-scale infrastructure. By evaluating the performance of the model using
domain-specific question-answering benchmarks, our results demonstrate that
EnergyGPT outperforms the base model in most of the energy-related language
understanding and generation tasks.

</details>


### [5] [DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge](https://arxiv.org/abs/2509.07188)
*Zonghai Yao,Michael Sun,Won Seok Jang,Sunjae Kwon,Soie Kwon,Hong Yu*

Main category: cs.CL

TL;DR: 本研究引入DischargeSim，一个评估大型语言模型(LLM)作为个性化出院教育者能力的基准，发现LLM在该领域存在显著差距，性能受患者档案影响，且模型大小并非总与教育效果正相关。


<details>
  <summary>Details</summary>
Motivation: 出院沟通是患者护理中关键但未被充分探索的组成部分，其目标从诊断转向教育。现有LLM基准侧重于院内诊断推理，未能评估模型在患者出院后提供支持的能力。

Method: 引入DischargeSim基准，通过模拟LLM驱动的医生代理与具有不同社会心理特征（如健康素养、教育、情绪）的患者代理之间的多轮出院对话来评估LLM。交互围绕六个临床出院主题构建，并从三个方面进行评估：(1) 对话质量（通过自动和LLM-as-judge评估），(2) 个性化文档生成（包括自由文本摘要和结构化AHRQ清单），(3) 患者通过下游多项选择题考试的理解能力。

Result: 对18个LLM进行的实验揭示了出院教育能力方面存在的显著差距，且性能在不同患者档案之间差异很大。值得注意的是，模型大小并非总能带来更好的教育效果，这突出了策略使用和内容优先级方面的权衡。

Conclusion: DischargeSim为LLM在出院后临床教育方面的基准测试迈出了第一步，并促进了公平、个性化的患者支持。

Abstract: Discharge communication is a critical yet underexplored component of patient
care, where the goal shifts from diagnosis to education. While recent large
language model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they
fail to evaluate models' ability to support patients after the visit. We
introduce DischargeSim, a novel benchmark that evaluates LLMs on their ability
to act as personalized discharge educators. DischargeSim simulates post-visit,
multi-turn conversations between LLM-driven DoctorAgents and PatientAgents with
diverse psychosocial profiles (e.g., health literacy, education, emotion).
Interactions are structured across six clinically grounded discharge topics and
assessed along three axes: (1) dialogue quality via automatic and LLM-as-judge
evaluation, (2) personalized document generation including free-text summaries
and structured AHRQ checklists, and (3) patient comprehension through a
downstream multiple-choice exam. Experiments across 18 LLMs reveal significant
gaps in discharge education capability, with performance varying widely across
patient profiles. Notably, model size does not always yield better education
outcomes, highlighting trade-offs in strategy use and content prioritization.
DischargeSim offers a first step toward benchmarking LLMs in post-visit
clinical education and promoting equitable, personalized patient support.

</details>


### [6] [Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation](https://arxiv.org/abs/2509.07190)
*Zahra Atf,Peter R Lewis*

Main category: cs.CL

TL;DR: 本文提出一个基于道德原则的规则框架，通过Prolog引擎处理大型语言模型（LLM）生成文本中的不确定性，旨在提供比概率模型更透明、轻量级的替代方案，以提高在高风险场景下的信任和可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在高风险应用中日益普及，但解释其生成内容的不确定性面临技术和伦理挑战。现有的概率方法通常不透明，无法满足对透明度的期望。

Method: 研究者提出了一个基于规则的道德原则框架来处理LLM文本中的不确定性。该框架借鉴道德心理学和美德伦理学，定义了预防、尊重和责任等规则。这些规则被编码在一个轻量级的Prolog引擎中，根据不确定性级别（低、中、高）触发系统动作并提供通俗解释。通过情景模拟来评估规则覆盖率、公平性和信任校准，并以临床和法律领域的用例进行说明。

Result: 该方法通过道德推理有效提高了LLM的信任度和可解释性。研究展示了该框架在临床和法律等高风险领域中的应用潜力。最终，它提供了一种透明、轻量级的替代方案，以替代传统的概率模型，从而实现对社会负责的自然语言生成。

Conclusion: 基于道德原则的规则框架为LLM处理不确定性提供了一个透明、轻量级的解决方案，能够有效提升在高风险应用场景下的信任和可解释性，是实现社会负责任自然语言生成的一种创新途径。

Abstract: Large language models (LLMs) are increasingly used in high-stakes settings,
where explaining uncertainty is both technical and ethical. Probabilistic
methods are often opaque and misaligned with expectations of transparency. We
propose a framework based on rule-based moral principles for handling
uncertainty in LLM-generated text. Using insights from moral psychology and
virtue ethics, we define rules such as precaution, deference, and
responsibility to guide responses under epistemic or aleatoric uncertainty.
These rules are encoded in a lightweight Prolog engine, where uncertainty
levels (low, medium, high) trigger aligned system actions with plain-language
rationales. Scenario-based simulations benchmark rule coverage, fairness, and
trust calibration. Use cases in clinical and legal domains illustrate how moral
reasoning can improve trust and interpretability. Our approach offers a
transparent, lightweight alternative to probabilistic models for socially
responsible natural language generation.

</details>


### [7] [LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade](https://arxiv.org/abs/2509.07274)
*Aida Kostikova,Ole Pütz,Steffen Eger,Olga Sabelfeld,Benjamin Paassen*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLMs）在自动化标注德国议会移民相关（反）团结言论方面的表现，并揭示了战后和2015年后德国对移民态度的转变。


<details>
  <summary>Details</summary>
Motivation: 德国移民辩论是核心议题，但深入分析政治言论传统上需要大量人工标注，限制了分析范围。LLMs有望部分自动化复杂的标注任务，从而能更深入地研究移民（反）团结趋势。

Method: 研究对多个LLMs进行了广泛评估，将其在标注德国议会辩论中（反）团结子类型方面的表现与数千个人工参考标注进行对比。评估了模型大小、提示词差异、微调、历史与当代数据的影响，并调查了系统性错误。同时，通过社会科学视角解读了标注结果。

Result: LLMs在政治文本分析方面展现了潜力。数据显示，战后时期德国对移民表现出高度的团结，但自2015年以来，德国议会中出现了明显的反团结趋势，这促使了进一步的研究。

Conclusion: 研究成果突出了LLMs在政治文本分析中的前景，以及德国移民辩论的重要性，该国正面临人口下降、劳动力短缺与日益加剧的两极分化并存的局面。

Abstract: Migration has been a core topic in German political debate, from millions of
expellees post World War II over labor migration to refugee movements in the
recent past. Studying political speech regarding such wide-ranging phenomena in
depth traditionally required extensive manual annotations, limiting the scope
of analysis to small subsets of the data. Large language models (LLMs) have the
potential to partially automate even complex annotation tasks. We provide an
extensive evaluation of a multiple LLMs in annotating (anti-)solidarity
subtypes in German parliamentary debates compared to a large set of thousands
of human reference annotations (gathered over a year). We evaluate the
influence of model size, prompting differences, fine-tuning, historical versus
contemporary data; and we investigate systematic errors. Beyond methodological
evaluation, we also interpret the resulting annotations from a social science
lense, gaining deeper insight into (anti-)solidarity trends towards migrants in
the German post-World War II period and recent past. Our data reveals a high
degree of migrant-directed solidarity in the postwar period, as well as a
strong trend towards anti-solidarity in the German parliament since 2015,
motivating further research. These findings highlight the promise of LLMs for
political text analysis and the importance of migration debates in Germany,
where demographic decline and labor shortages coexist with rising polarization.

</details>


### [8] [Causal Attention with Lookahead Keys](https://arxiv.org/abs/2509.07301)
*Zhuoqing Song,Peng Sun,Huizhuo Yuan,Quanquan Gu*

Main category: cs.CL

TL;DR: 本文提出CASTLE，一种新型因果注意力机制，它通过动态更新键（lookahead keys）来整合后续上下文信息，在保持自回归和高效训练的同时，显著优于标准因果注意力，降低了语言模型的困惑度并提升了下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 标准因果注意力机制中，每个token的查询、键和值（QKV）是静态的，且仅编码前文信息，这限制了其对上下文的理解能力。

Method: 引入CASTLE机制，它在上下文展开时持续更新每个token的键（称为“lookahead keys”），这些键属于较早位置但能整合后续token的信息，同时严格保持自回归属性。通过数学等价转换，避免了显式实例化lookahead keys，实现了高效并行训练。

Result: 在语言建模基准测试中，CASTLE在不同模型规模下持续优于标准因果注意力，降低了验证困惑度，并提升了多种下游任务的性能。

Conclusion: CASTLE通过动态更新键，提供了一种更优越的因果注意力机制，在保持自回归特性和训练效率的前提下，显著提升了语言模型的性能。

Abstract: In standard causal attention, each token's query, key, and value (QKV) are
static and encode only preceding context. We introduce CAuSal aTtention with
Lookahead kEys (CASTLE), an attention mechanism that continually updates each
token's keys as the context unfolds. We term these updated keys lookahead keys
because they belong to earlier positions yet integrate information from tokens
that appear later relative to those positions, while strictly preserving the
autoregressive property. Although the mechanism appears sequential, we derive a
mathematical equivalence that avoids explicitly materializing lookahead keys at
each position and enables efficient parallel training. On language modeling
benchmarks, CASTLE consistently outperforms standard causal attention across
model scales, reducing validation perplexity and improving performance on a
range of downstream tasks.

</details>


### [9] [Basis Vector Metric: A Method for Robust Open-Ended State Change Detection](https://arxiv.org/abs/2509.07308)
*David Oprea,Sam Powers*

Main category: cs.CL

TL;DR: 本文提出了一种名为BVM（基向量方法）的新方法，利用语言嵌入来判断图像中的状态变化。实验发现，BVM在名词状态分类上优于其他对比指标，但在区分形容词方面并未显示出比逻辑回归模型更优越的性能，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 开发并测试一种新的方法（BVM），该方法利用语言嵌入来判断图像中的状态变化，以期提高对图像状态理解的准确性。

Method: 1. **方法构建：** 提出基向量方法（BVM），通过语言嵌入分析图像中的状态变化。
2. **数据集：** 使用MIT-States数据集，包含约53,000张图像，涉及225个名词和115个形容词，形成约1000对名词-形容词对。
3. **实验一：** 评估BVM单独确定每个名词类别状态的能力，并与余弦相似度、点积、乘积量化、二值索引、朴素贝叶斯和定制神经网络等指标进行比较。
4. **实验二：** 评估BVM区分不同形容词的能力，并与MIT-States论文建议的逻辑回归模型进行比较。

Result: 1. **实验一结果：** BVM在分类每个名词的状态方面表现最佳。
2. **实验二结果：** 没有发现确凿证据表明BVM在区分形容词方面能比逻辑回归模型表现更好。

Conclusion: BVM在名词状态分类上表现出色，但在形容词辨别上未能超越逻辑回归模型。然而，研究发现了该方法潜在的改进方向，未来有望通过调整方法论来提高BVM的准确性。

Abstract: We test a new method, which we will abbreviate using the acronym BVM (Basis
Vectors Method), in its ability to judge the state changes in images through
using language embeddings. We used the MIT-States dataset, containing about
53,000 images, to gather all of our data, which has 225 nouns and 115
adjectives, with each noun having about 9 different adjectives, forming
approximately 1000 noun-adjective pairs. For our first experiment, we test our
method's ability to determine the state of each noun class separately against
other metrics for comparison. These metrics are cosine similarity, dot product,
product quantization, binary index, Naive Bayes, and a custom neural network.
Among these metrics, we found that our proposed BVM performs the best in
classifying the states for each noun. We then perform a second experiment where
we try using BVM to determine if it can differentiate adjectives from one
another for each adjective separately. We compared the abilities of BVM to
differentiate adjectives against the proposed method the MIT-States paper
suggests: using a logistic regression model. In the end, we did not find
conclusive evidence that our BVM metric could perform better than the logistic
regression model at discerning adjectives. Yet, we were able to find evidence
for possible improvements to our method; this leads to the chance of increasing
our method's accuracy through certain changes in our methodologies.

</details>


### [10] [Instance-level Performance Prediction for Long-form Generation Tasks](https://arxiv.org/abs/2509.07309)
*Chi-Yang Hsu,Alexander Braylan,Yiheng Su,Omar Alonso,Matthew Lease*

Main category: cs.CL

TL;DR: 本文提出了一个用于长文本生成任务实例级性能预测的新基准，旨在预测多维度、细粒度质量指标的连续分数及其不确定性，并展示了在少量训练样本下仍能有效预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法可能无法有效地对具有多维度、细粒度质量指标的长文本生成任务进行实例级性能预测，且缺乏对预测不确定性的量化。

Method: 开发了一个新的基准，用于预测给定黑盒模型输入和输出的连续评估指标分数，该方法与任务、模型和指标无关。基准还要求推断预测区间以量化不确定性。评估涵盖11个长文本数据集/任务，涉及多种大型语言模型、基线和指标。

Result: 研究表明，仅使用16个训练示例，就能有效预测长文本生成任务的得分。

Conclusion: 引入了一项新颖且有用的任务，一个推动进展的宝贵基准，以及可立即实际采用的基线。

Abstract: We motivate and share a new benchmark for instance-level performance
prediction of long-form generation tasks having multi-faceted, fine-grained
quality metrics. Our task-, model- and metric-agnostic formulation predicts
continuous evaluation metric scores given only black-box model inputs and
outputs. Beyond predicting point estimates of metric scores, the benchmark also
requires inferring prediction intervals to quantify uncertainty around point
estimates. Evaluation spans 11 long-form datasets/tasks with multiple LLMs,
baselines, and metrics per task. We show that scores can be effectively
predicted across long-form generation tasks using as few as 16 training
examples. Overall, we introduce a novel and useful task, a valuable benchmark
to drive progress, and baselines ready for practical adoption today.

</details>


### [11] [Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations](https://arxiv.org/abs/2509.07311)
*Sihyun Park*

Main category: cs.CL

TL;DR: 本研究提出KAMIR，一种通过分析模型内部表征来选择SFT训练数据的新方法，克服了传统方法对提示工程的依赖，并发现使用模型不熟悉的数据进行训练能提高泛化性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的SFT对任务定制化至关重要，但缺乏有效的训练数据选择方法。简单增加数据量无效，数据预处理耗时耗费。现有知识选择方法依赖提示工程，易受变动影响且成本高昂。

Method: 本研究提出KAMIR（Knowledge Analysis via Model Internal Representations）。KAMIR通过计算模型每个层（block）的隐藏状态与最终隐藏状态之间的相似性来评估数据，从而分析模型内部表征以选择数据。该方法不受任务限制，可应用于机器阅读理解和摘要等多种任务，并基于模型对输入的熟悉度来选择有用数据，即使在小型数据集和简单分类器架构下也有效。

Result: KAMIR方法可应用于多种任务，如机器阅读理解和摘要。实验结果表明，使用模型“不熟悉”的数据进行训练能够带来更好的泛化性能，且只需小型数据集和简单分类器架构。

Conclusion: KAMIR提供了一种新颖、稳健且经济高效的数据选择方法，通过分析模型内部表征来克服传统方法的局限性。通过训练模型不熟悉的数据，能有效提升LLM的泛化能力，且无需复杂的提示工程。

Abstract: Recent advances in large language models (LLMs) have been driven by
pretraining, supervised fine tuning (SFT), and alignment tuning. Among these,
SFT plays a crucial role in transforming a model 's general knowledge into
structured responses tailored to specific tasks. However, there is no clearly
established methodology for effective training data selection. Simply
increasing the volume of data does not guarantee performance improvements,
while preprocessing, sampling, and validation require substantial time and
cost.
  To address this issue, a variety of data selection methods have been
proposed. Among them, knowledge based selection approaches identify suitable
training data by analyzing the model 's responses. Nevertheless, these methods
typically rely on prompt engineering, making them sensitive to variations and
incurring additional costs for prompt design.
  In this study, we propose Knowledge Analysis via Model Internal
Representations (KAMIR), a novel approach that overcomes these limitations by
analyzing data based on the model 's internal representations. KAMIR computes
similarities between the hidden states of each layer (block) and the final
hidden states for a given input to assess the data. Unlike prior methods that
were largely limited to multiple choice tasks, KAMIR can be applied to a wide
range of tasks such as machine reading comprehension and summarization.
Moreover, it selects data useful for training based on the model 's familiarity
with the input, even with a small dataset and a simple classifier architecture.
Experiments across diverse task datasets demonstrate that training with less
familiar data leads to better generalization performance.

</details>


### [12] [Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation](https://arxiv.org/abs/2509.07324)
*Nakyung Lee,Yeongoon Kim,Minhae Oh,Suhwan Kim,Jin Woo Koo,Hyewon Jo,Jungwoo Lee*

Main category: cs.CL

TL;DR: 针对Transformer自注意力机制的局部化问题，本文提出SAOBP框架，通过信念传播注入多跳关系并引入GTD进行量化。实验证明SAOBP能防止熵塌陷，提升模型性能，尤其在资源受限的小规模模型中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型中的Transformer自注意力机制存在局部化问题，即注意力集中于有限的token上，难以捕捉长距离依赖。

Method: 提出Self-Attention One-step Belief Propagation (SAOBP) 精炼框架，通过信念传播过程注入多跳关系。同时引入Global Token Dependency (GTD) 来解释和量化注意力图中多跳连接的相对贡献。

Result: SAOBP有助于防止深层网络中的熵塌陷，并能自适应地将GTD维持在任务适宜的水平，从而提升模型性能。

Conclusion: SAOBP在小规模模型中也取得了具有竞争力的提升，这突显了其在资源受限场景下提高推理质量的潜力。

Abstract: Transformer-based self-attention mechanism serves as the core of modern
language models, yet it often suffers from localization, where attentions
collapse onto a limited subset of tokens and fail to capture long-range
dependencies. To address this issue, we propose Self-Attention One-step Belief
Propagation (SAOBP), a refinement framework that injects multi-hop
relationships through a belief propagation process. To interpret and quantify
these interactions, we introduce Global Token Dependency (GTD) that captures
the relative contribution of multihop connections within the attention graph.
Empirical results indicate that SAOBP helps prevent entropy collapse in deeper
layers and adaptively maintains GTD at task-appropriate levels, thereby
supporting improvements in model performance. Importantly, we observe
competitive gains in small-scale models, highlighting its potential for
improving inference quality in resource-constrained scenarios.

</details>


### [13] [PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions](https://arxiv.org/abs/2509.07370)
*Yixuan Tang,Yi Yang,Ahmed Abbasi*

Main category: cs.CL

TL;DR: 本文提出PersonaFuse，一个LLM后训练框架，使其能根据情境适应并表达不同个性，显著提升社交情感智能，同时不牺牲推理能力和安全性，为更人性化的AI系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能力强大，但在现实对话中，其情感感知和社会能力受限，无法根据不同社会和任务情境调整沟通风格和情感表达。

Method: PersonaFuse框架受特质激活理论和大五人格模型启发，采用专家混合（MoE）架构，结合人格适配器和动态路由网络，以实现情境化特质表达。

Result: PersonaFuse在多维度的社交情感智能方面显著优于基线模型，且不牺牲通用推理能力或模型安全性。在心理健康咨询和客户服务等以人为中心的下游应用中表现出持续改进。与GPT-4o和DeepSeek等领先LLM相比，PersonaFuse在响应质量上具有竞争力，尽管模型规模较小。

Conclusion: PersonaFuse为开发社交情感增强型LLM提供了一个理论基础且实用的方法，标志着向更以人为中心的AI系统迈出了重要一步。

Abstract: Recent advancements in Large Language Models (LLMs) demonstrate remarkable
capabilities across various fields. These developments have led to more direct
communication between humans and LLMs in various situations, such as social
companionship and psychological support. However, LLMs often exhibit
limitations in emotional perception and social competence during real-world
conversations. These limitations partly originate from their inability to adapt
their communication style and emotional expression to different social and task
contexts. In this work, we introduce PersonaFuse, a novel LLM post-training
framework that enables LLMs to adapt and express different personalities for
varying situations. Inspired by Trait Activation Theory and the Big Five
personality model, PersonaFuse employs a Mixture-of-Expert architecture that
combines persona adapters with a dynamic routing network, enabling contextual
trait expression. Experimental results show that PersonaFuse substantially
outperforms baseline models across multiple dimensions of social-emotional
intelligence. Importantly, these gains are achieved without sacrificing general
reasoning ability or model safety, which remain common limitations of direct
prompting and supervised fine-tuning approaches. PersonaFuse also delivers
consistent improvements in downstream human-centered applications, such as
mental health counseling and review-based customer service. Finally, human
preference evaluations against leading LLMs, including GPT-4o and DeepSeek,
demonstrate that PersonaFuse achieves competitive response quality despite its
comparatively smaller model size. These findings demonstrate that
PersonaFuse~offers a theoretically grounded and practical approach for
developing social-emotional enhanced LLMs, marking a significant advancement
toward more human-centric AI systems.

</details>


### [14] [Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents](https://arxiv.org/abs/2509.07389)
*Sankalp Tattwadarshi Swain,Anshika Krishnatray,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.CL

TL;DR: 现有LLM语言能力评估缺乏对交互式习得的考察。本研究提出一个新框架，评估LLM通过与机器人对话习得新语言（Tinkatongue）的能力。结果显示LLM未能成功建立对话，但其学习策略与人类相似，为未来评估和模型设计提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型（LLM代理）语言能力的评估主要集中在词汇学习、形态规则归纳、句法泛化、语用推理和跨语言迁移。然而，这些研究都未能评估LLM代理是否能像人类一样，通过模式识别和交互反馈来习得语言。

Method: 本文提出一个新颖的实验框架，其中LLM代理被评估其在与一个只懂Tinkatongue（一种新构建语言）的机器人对话中习得和使用该语言的能力。

Result: 研究发现，LLM代理在100次响应内未能成功建立对话。尽管如此，它们采用了与人类语言学习方法相似的独特策略。

Conclusion: 这些结果为语言能力评估基准提供了新方向，并为设计能够更有效地从交互反馈中学习的模型开辟了途径。

Abstract: Existing evaluation studies on linguistic competence of large language models
(LLM agents) have focused primarily on vocabulary learning, morphological rule
induction, syntactic generalization, pragmatic inference, and cross-linguistic
transfer. However, none assess whether LLM agents can acquire a language
through pattern recognition and interactive feedback, a central feature of
human language acquisition. We propose a novel experimental framework in which
an LLM agent is evaluated on its ability to acquire and use a newly constructed
language (Tinkatongue) in conversation with a bot that understands only
Tinkatongue. Our findings show that LLM agents fail to establish a conversation
within 100 responses, yet they adopt distinct strategies that mirror human
approaches to language learning. The results suggest a new direction for
evaluation benchmarks and open pathways to model designs that learn more
effectively from interactive feedback.

</details>


### [15] [The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering](https://arxiv.org/abs/2509.07399)
*Yi-Jie Cheng,Oscar Chew,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 本研究通过引入轻量级探索模块来处理知识图谱遍历，有效提升了小型语言模型在基于知识图谱问答任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有将知识图谱整合到大型语言模型中的方法常依赖专有或超大型模型，限制了可访问性和可扩展性。同时，小型语言模型在知识图谱遍历和推理能力方面存在局限。

Method: 提出利用简单高效的探索模块来替代语言模型自身，处理知识图谱的遍历任务。

Result: 实验结果表明，这些轻量级模块有效提高了小型语言模型在知识图谱问答任务上的表现。

Conclusion: 所提出的轻量级探索模块能够有效解决小型语言模型在知识图谱遍历上的不足，从而提升其在知识图谱问答任务中的性能。

Abstract: Integrating knowledge graphs (KGs) into the reasoning processes of large
language models (LLMs) has emerged as a promising approach to mitigate
hallucination. However, existing work in this area often relies on proprietary
or extremely large models, limiting accessibility and scalability. In this
study, we investigate the capabilities of existing integration methods for
small language models (SLMs) in KG-based question answering and observe that
their performance is often constrained by their limited ability to traverse and
reason over knowledge graphs. To address this limitation, we propose leveraging
simple and efficient exploration modules to handle knowledge graph traversal in
place of the language model itself. Experiment results demonstrate that these
lightweight modules effectively improve the performance of small language
models on knowledge graph question answering tasks. Source code:
https://github.com/yijie-cheng/SLM-ToG/.

</details>


### [16] [LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction](https://arxiv.org/abs/2509.07403)
*Weichu Liu,Jing Xiong,Yuxuan Hu,Zixuan Li,Minghuan Tan,Ningning Mao,Chenyang Zhao,Zhongwei Wan,Chaofan Tao,Wendong Xu,Hui Shen,Chengming Li,Lingpeng Kong,Ngai Wong*

Main category: cs.CL

TL;DR: 本文提出了LongEmotion，一个针对长文本情感智能（EI）任务的新基准，以解决现有基准在真实世界长文本场景中的不足。引入了RAG和CoEM两种方法来提升LLMs在这些任务上的表现，实验证明它们能有效增强EI性能，并推动LLMs在实际应用中的进步。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在情感智能（EI）和长文本理解方面取得了显著进展，但现有基准往往忽视了在真实、实用场景中（交互冗长、多样且嘈杂）的长文本情感智能的特定方面。

Method: 1. 提出了LongEmotion基准，专门为长文本情感智能任务设计，涵盖情感分类、情感检测、情感问答、情感对话、情感总结和情感表达等多样化任务，平均输入长度达到8,777 tokens。
2. 引入了检索增强生成（RAG）和协作情感建模（CoEM）方法来提升模型在真实约束下的性能，并与标准基于提示的方法进行比较。
3. 本文的RAG方法利用对话上下文和大型语言模型本身作为检索源，避免依赖外部知识库。
4. CoEM方法通过将任务分解为五个阶段，整合检索增强和有限知识注入来进一步提高性能。
5. 进行了GPT系列模型的比较案例研究，以展示不同模型在情感智能方面的差异。

Result: 1. 实验结果表明，RAG和CoEM两种方法在大多数长文本情感智能任务上均能一致地提升与EI相关的性能。
2. 这些方法将大型语言模型推向更实用和真实世界的情感智能应用。
3. 对GPT系列的比较案例研究揭示了不同模型在情感智能方面的差异。

Conclusion: 通过LongEmotion基准以及RAG和CoEM方法的引入，大型语言模型在长文本情感智能任务上的性能得到了显著提升，使其更适用于实际和真实世界的EI应用。

Abstract: Large language models (LLMs) make significant progress in Emotional
Intelligence (EI) and long-context understanding. However, existing benchmarks
tend to overlook certain aspects of EI in long-context scenarios, especially
under realistic, practical settings where interactions are lengthy, diverse,
and often noisy. To move towards such realistic settings, we present
LongEmotion, a benchmark specifically designed for long-context EI tasks. It
covers a diverse set of tasks, including Emotion Classification, Emotion
Detection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion
Expression. On average, the input length for these tasks reaches 8,777 tokens,
with long-form generation required for Emotion Expression. To enhance
performance under realistic constraints, we incorporate Retrieval-Augmented
Generation (RAG) and Collaborative Emotional Modeling (CoEM), and compare them
with standard prompt-based methods. Unlike conventional approaches, our RAG
method leverages both the conversation context and the large language model
itself as retrieval sources, avoiding reliance on external knowledge bases. The
CoEM method further improves performance by decomposing the task into five
stages, integrating both retrieval augmentation and limited knowledge
injection. Experimental results show that both RAG and CoEM consistently
enhance EI-related performance across most long-context tasks, advancing LLMs
toward more practical and real-world EI applications. Furthermore, we conducted
a comparative case study experiment on the GPT series to demonstrate the
differences among various models in terms of EI. Code is available on GitHub at
https://github.com/LongEmotion/LongEmotion, and the project page can be found
at https://longemotion.github.io/.

</details>


### [17] [AIxcellent Vibes at GermEval 2025 Shared Task on Candy Speech Detection: Improving Model Performance by Span-Level Training](https://arxiv.org/abs/2509.07459)
*Christian Rene Thelen,Patrick Gustav Blaneck,Tobias Bornheim,Niklas Grieger,Stephan Bialonski*

Main category: cs.CL

TL;DR: 本研究探讨了德语YouTube评论中“糖果言论”（积极支持性语言）的自动化检测，发现多语言XLM-RoBERTa-Large模型在基于片段的检测任务中表现最佳，验证了多语言模型在此类任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中的积极支持性交流（即“糖果言论”）有助于促进网络文明，但目前对其自动化检测的研究不足，这限制了系统分析其影响。因此，需要开发可靠的自动化检测方法。

Method: 研究者在一个包含4.6万条德语YouTube评论的语料库中，使用单语言（如GBERT）和多语言语言模型（如Qwen3 Embedding, XLM-RoBERTa），探索糖果言论的可靠检测方法。尤其关注基于片段（span level）的检测。

Result: 结果显示，一个经过训练、用于片段级检测的多语言XLM-RoBERTa-Large模型表现优于其他方法。该模型在GermEval 2025糖果言论检测共享任务中，在二元积极F1得分（0.8906）和分类片段检测（严格F1: 0.6307）子任务中均排名第一。研究推测，基于片段的训练、多语言能力和表情符号感知分词器是提升检测性能的关键。

Conclusion: 本研究结果明确展示了多语言模型在识别积极、支持性语言方面的有效性，为未来相关研究提供了重要的见解和方法基础。

Abstract: Positive, supportive online communication in social media (candy speech) has
the potential to foster civility, yet automated detection of such language
remains underexplored, limiting systematic analysis of its impact. We
investigate how candy speech can be reliably detected in a 46k-comment German
YouTube corpus by monolingual and multilingual language models, including
GBERT, Qwen3 Embedding, and XLM-RoBERTa. We find that a multilingual
XLM-RoBERTa-Large model trained to detect candy speech at the span level
outperforms other approaches, ranking first in both binary positive F1: 0.8906)
and categorized span-based detection (strict F1: 0.6307) subtasks at the
GermEval 2025 Shared Task on Candy Speech Detection. We speculate that
span-based training, multilingual capabilities, and emoji-aware tokenizers
improved detection performance. Our results demonstrate the effectiveness of
multilingual models in identifying positive, supportive language.

</details>


### [18] [Understanding Stigmatizing Language Lexicons: A Comparative Analysis in Clinical Contexts](https://arxiv.org/abs/2509.07462)
*Yiliang Zhou,Di Hu,Tianchu Lyu,Jasmine Dhillon,Alexandra L. Beck,Gelareh Sadigh,Kai Zheng*

Main category: cs.CL

TL;DR: 本研究分析了医疗保健领域中现有污名化语言词汇表，发现它们之间语义相似度中等，且主要涉及临床医生对负面行为的评判性表达。情感分析显示绝大多数词语为负面。结果强调了建立标准化词汇表的必要性。


<details>
  <summary>Details</summary>
Motivation: 污名化语言导致医疗保健不平等，但目前缺乏普遍接受或标准化的定义词典来界定医疗保健中的污名化语言。

Method: 通过系统性文献检索识别现有污名化语言词汇表（共找到四个），并进行比较分析。分析内容包括：1) 词汇表间的相似性和差异；2) 基于既定情感数据集的词语情感分布（正面、负面或中性）。

Result: 研究共识别出四个词汇表。分析结果显示这些词汇表之间存在中等语义相似性，且大多数污名化术语与临床医生描述感知到的负面行为的评判性表达有关。情感分析表明，绝大多数词语被归类为负面，但不同词汇表之间存在差异。

Conclusion: 研究结果强调了对标准化词汇表的需求，并指出了在临床文本中定义污名化语言所面临的挑战。

Abstract: Stigmatizing language results in healthcare inequities, yet there is no
universally accepted or standardized lexicon defining which words, terms, or
phrases constitute stigmatizing language in healthcare. We conducted a
systematic search of the literature to identify existing stigmatizing language
lexicons and then analyzed them comparatively to examine: 1) similarities and
discrepancies between these lexicons, and 2) the distribution of positive,
negative, or neutral terms based on an established sentiment dataset. Our
search identified four lexicons. The analysis results revealed moderate
semantic similarity among them, and that most stigmatizing terms are related to
judgmental expressions by clinicians to describe perceived negative behaviors.
Sentiment analysis showed a predominant proportion of negatively classified
terms, though variations exist across lexicons. Our findings underscore the
need for a standardized lexicon and highlight challenges in defining
stigmatizing language in clinical texts.

</details>


### [19] [From Scarcity to Efficiency: Investigating the Effects of Data Augmentation on African Machine Translation](https://arxiv.org/abs/2509.07471)
*Mardiyyah Oduwole,Oluwatosin Olajide,Jamiu Suleiman,Faith Hunja,Busayo Awobade,Fatimo Adebanjo,Comfort Akanni,Chinonyelum Igwe,Peace Ododo,Promise Omoigui,Steven Kolawole,Abraham Owodunni*

Main category: cs.CL

TL;DR: 本研究探索了两种数据增强技术（回译句子拼接和切换）对低资源非洲语言机器翻译系统的影响，实验结果显示所有六种语言的BLEU分数至少提升了25%。


<details>
  <summary>Details</summary>
Motivation: 非洲大陆的语言多样性对机器翻译带来挑战，特别是在低资源非洲语言中，需要探索方法以改善其翻译系统性能。

Method: 研究采用数据增强技术，具体是“回译的句子拼接（sentence concatenation with back translation）”和“切换（switch-out）”，并在六种非洲语言上应用和进行实验。

Result: 机器翻译性能取得了显著提升，所有六种语言的BLEU分数至少增加了25%。

Conclusion: 这些数据增强技术在改进低资源语言机器翻译系统方面展现出巨大潜力，有助于开发更强大、更鲁棒的欠资源语言翻译系统。

Abstract: The linguistic diversity across the African continent presents different
challenges and opportunities for machine translation. This study explores the
effects of data augmentation techniques in improving translation systems in
low-resource African languages. We focus on two data augmentation techniques:
sentence concatenation with back translation and switch-out, applying them
across six African languages. Our experiments show significant improvements in
machine translation performance, with a minimum increase of 25\% in BLEU score
across all six languages.We provide a comprehensive analysis and highlight the
potential of these techniques to improve machine translation systems for
low-resource languages, contributing to the development of more robust
translation systems for under-resourced languages.

</details>


### [20] [HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention](https://arxiv.org/abs/2509.07475)
*Saumya Goswami,Siddharth Kurra*

Main category: cs.CL

TL;DR: HALT-RAG是一个事后验证系统，旨在识别RAG模型输出中的幻觉。它结合NLI模型和词汇信号构建通用特征集，并训练元分类器，在HaluEval基准测试中取得了优异的F1分数。


<details>
  <summary>Details</summary>
Motivation: 生成式语言模型安全部署的关键挑战在于检测与源文本矛盾或不被支持的内容（即幻觉）。

Method: 引入HALT-RAG系统，通过结合两个预训练NLI模型和轻量级词汇信号生成通用特征集，并训练一个简单、校准且适应任务的元分类器。采用严格的5折OOF训练协议进行评估。

Result: HALT-RAG在HaluEval基准测试的摘要、问答和对话任务上，分别实现了0.7756、0.9786和0.7391的OOF F1分数。其良好的校准概率也支持实用的弃权机制。

Conclusion: HALT-RAG提供了一个可靠工具，能有效识别RAG输出中的幻觉，从而平衡模型性能与安全要求。

Abstract: Detecting content that contradicts or is unsupported by a given source text
is a critical challenge for the safe deployment of generative language models.
We introduce HALT-RAG, a post-hoc verification system designed to identify
hallucinations in the outputs of Retrieval-Augmented Generation (RAG)
pipelines. Our flexible and task-adaptable framework uses a universal feature
set derived from an ensemble of two frozen, off-the-shelf Natural Language
Inference (NLI) models and lightweight lexical signals. These features are used
to train a simple, calibrated, and task-adapted meta-classifier. Using a
rigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and
produce unbiased estimates, we evaluate our system on the HaluEval benchmark.
By pairing our universal feature set with a lightweight, task-adapted
classifier and a precision-constrained decision policy, HALT-RAG achieves
strong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA,
and dialogue tasks, respectively. The system's well-calibrated probabilities
enable a practical abstention mechanism, providing a reliable tool for
balancing model performance with safety requirements.

</details>


### [21] [ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval](https://arxiv.org/abs/2509.07512)
*Zihan Chen,Lei Shi,Weize Wu,Qiji Zhou,Yue Zhang*

Main category: cs.CL

TL;DR: 针对LLM实体识别任务中微调成本高昂的问题，本文提出了ALLabel三阶段主动学习框架，通过高效选择样本实现性能与成本的最佳平衡。实验证明，仅用5%-10%的标注数据即可达到与全量标注相当的性能。


<details>
  <summary>Details</summary>
Motivation: 自然科学领域（如化学、材料科学）的数据驱动研究对大规模、高性能的实体识别需求迫切。大语言模型（LLMs）正被广泛应用于此任务，但主流的LLM实体识别方法依赖于微调技术，其过程往往伴随高昂的成本。

Method: 本文提出了ALLabel，一个三阶段框架，旨在为LLM建模准备示范时，选择最具信息量和代表性的样本。这些标注过的示例被用于构建一个真实检索语料库，以供LLM进行上下文学习。该框架通过顺序采用三种不同的主动学习策略来达到此目的。

Result: 在相同标注预算下，ALLabel在三个专门领域的数据集上持续优于所有基线方法。实验结果还表明，通过ALLabel选择性地标注仅5%-10%的数据集，就可以达到与标注整个数据集的方法相当的性能。进一步的分析和消融研究验证了该提案的有效性和泛化性。

Conclusion: ALLabel提供了一种有效且泛化性强的方法，通过智能地选择少量关键标注数据，显著优化了LLM在实体识别任务上的性能-成本权衡，尤其适用于预算受限的场景。

Abstract: Many contemporary data-driven research efforts in the natural sciences, such
as chemistry and materials science, require large-scale, high-performance
entity recognition from scientific datasets. Large language models (LLMs) have
increasingly been adopted to solve the entity recognition task, with the same
trend being observed on all-spectrum NLP tasks. The prevailing entity
recognition LLMs rely on fine-tuned technology, yet the fine-tuning process
often incurs significant cost. To achieve a best performance-cost trade-off, we
propose ALLabel, a three-stage framework designed to select the most
informative and representative samples in preparing the demonstrations for LLM
modeling. The annotated examples are used to construct a ground-truth retrieval
corpus for LLM in-context learning. By sequentially employing three distinct
active learning strategies, ALLabel consistently outperforms all baselines
under the same annotation budget across three specialized domain datasets.
Experimental results also demonstrate that selectively annotating only 5\%-10\%
of the dataset with ALLabel can achieve performance comparable to the method
annotating the entire dataset. Further analyses and ablation studies verify the
effectiveness and generalizability of our proposal.

</details>


### [22] [VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents](https://arxiv.org/abs/2509.07553)
*Zheng Wu,Heyuan Huang,Xingyu Lou,Xiangmou Qu,Pengzhou Cheng,Zongru Wu,Weiwen Liu,Weinan Zhang,Jun Wang,Zhaoxiang Wang,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: 本文提出VeriOS-Agent，一个可信赖的操作系统智能体，通过查询驱动的人机交互框架，在不可信赖场景下主动向人类查询，显著提高了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有操作系统智能体多为理想化环境设计，在现实世界不可信赖条件下存在过度执行的风险，需要机制来缓解这种风险。

Method: 提出一个查询驱动的人-智能体-GUI交互框架，使操作系统智能体能决定何时向人类查询以更可靠地完成任务。基于此框架，引入VeriOS-Agent，采用两阶段学习范式训练，以解耦和利用元知识，使其在正常条件下自主执行，在不可信赖场景中主动查询人类。

Result: 实验表明，在不可信赖场景中，VeriOS-Agent的平均分步成功率比现有技术提高了20.64%，同时不影响正常性能。分析强调了VeriOS-Agent的合理性、通用性和可扩展性。

Conclusion: VeriOS-Agent通过引入查询驱动的人机交互，有效提升了操作系统智能体在现实世界不可信赖环境中的信任度与任务成功率。

Abstract: With the rapid progress of multimodal large language models, operating system
(OS) agents become increasingly capable of automating tasks through on-device
graphical user interfaces (GUIs). However, most existing OS agents are designed
for idealized settings, whereas real-world environments often present
untrustworthy conditions. To mitigate risks of over-execution in such
scenarios, we propose a query-driven human-agent-GUI interaction framework that
enables OS agents to decide when to query humans for more reliable task
completion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy
OS agent trained with a two-stage learning paradigm that falicitate the
decoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent
autonomously executes actions in normal conditions while proactively querying
humans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves
the average step-wise success rate by 20.64\% in untrustworthy scenarios over
the state-of-the-art, without compromising normal performance. Analysis
highlights VeriOS-Agent's rationality, generalizability, and scalability. The
codes, datasets and models are available at
https://github.com/Wuzheng02/VeriOS.

</details>


### [23] [Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition](https://arxiv.org/abs/2509.07555)
*Yi Liu,Xiangrong Zhu,Xiangyu Liu,Wei Wei,Wei Hu*

Main category: cs.CL

TL;DR: 针对大型语言模型知识快速过时且重训练成本高的问题，本文发现现有基于RAG的知识编辑方法在多跳问答中存在“编辑跳过”难题。作者提出IRAKE方法，通过引导式分解迭代检索增强知识编辑，有效解决了该问题并超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的知识更新速度快，但重训练成本高昂，因此无需修改参数的知识编辑（KE）成为必要。然而，现有基于检索增强生成（RAG）的KE方法在多跳问答中存在“编辑跳过”问题，即在推理时跳过相关的编辑事实，这源于知识表达多样性及LLMs问题解决粒度与编辑记忆事实之间的不匹配。

Method: 提出了一种新颖的迭代检索增强知识编辑方法，名为IRAKE（Iterative Retrieval-Augmented Knowledge Editing with guided decomposition）。该方法通过单条编辑事实和完整编辑案例的引导式分解来解决“编辑跳过”问题。

Result: 实验结果表明，IRAKE方法成功缓解了由“编辑跳过”导致的编辑失败。在多跳问答的知识编辑任务上，IRAKE的性能优于现有最先进的方法。

Conclusion: IRAKE方法通过有效解决多跳问答中知识编辑的“编辑跳过”问题，显著提升了大型语言模型在复杂知识场景下的编辑能力和准确性。

Abstract: In a rapidly evolving world where information updates swiftly, knowledge in
large language models (LLMs) becomes outdated quickly. Retraining LLMs is not a
cost-effective option, making knowledge editing (KE) without modifying
parameters particularly necessary. We find that although existing
retrieval-augmented generation (RAG)-based KE methods excel at editing simple
knowledge, they struggle with KE in multi-hop question answering due to the
issue of "edit skipping", which refers to skipping the relevant edited fact in
inference. In addition to the diversity of natural language expressions of
knowledge, edit skipping also arises from the mismatch between the granularity
of LLMs in problem-solving and the facts in the edited memory. To address this
issue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing
method with guided decomposition (IRAKE) through the guidance from single
edited facts and entire edited cases. Experimental results demonstrate that
IRAKE mitigates the failure of editing caused by edit skipping and outperforms
state-of-the-art methods for KE in multi-hop question answering.

</details>


### [24] [BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment](https://arxiv.org/abs/2509.07588)
*Andrey Sakhovskiy,Elena Tutubalina*

Main category: cs.CL

TL;DR: BALI是一种新颖的联合语言模型和知识图谱预训练方法，通过对齐两者表示来增强生物医学LLMs对复杂概念和事实信息的理解，并在多种语言理解任务上取得了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学语言模型（LLMs）在理解复杂的领域特定概念结构以及生物医学知识图谱（KGs）中编码的事实信息方面存在局限性。

Method: 提出BALI（Biomedical Knowledge Graph and Language Model Alignment）方法，这是一种联合语言模型（LM）和知识图谱（KG）预训练方法。它通过同时学习一个专门的KG编码器并对齐LM和KG的表示来用外部知识增强LM。具体地，将文本中的生物医学概念提及链接到UMLS KG，并利用局部KG子图作为跨模态正样本。

Result: 将BALI应用于PubMedBERT和BioLinkBERT等领先的生物医学LM后，即使仅使用少量预训练数据，也能在多种语言理解任务上提升其性能，并提高实体表示的质量。

Conclusion: BALI方法有效地将生物医学知识图谱的知识融入到语言模型中，显著提高了模型对生物医学文本的理解能力和实体表示质量。

Abstract: In recent years, there has been substantial progress in using pretrained
Language Models (LMs) on a range of tasks aimed at improving the understanding
of biomedical texts. Nonetheless, existing biomedical LLMs show limited
comprehension of complex, domain-specific concept structures and the factual
information encoded in biomedical Knowledge Graphs (KGs). In this work, we
propose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel
joint LM and KG pre-training method that augments an LM with external knowledge
by the simultaneous learning of a dedicated KG encoder and aligning the
representations of both the LM and the graph. For a given textual sequence, we
link biomedical concept mentions to the Unified Medical Language System (UMLS)
KG and utilize local KG subgraphs as cross-modal positive samples for these
mentions. Our empirical findings indicate that implementing our method on
several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves
their performance on a range of language understanding tasks and the quality of
entity representations, even with minimal pre-training on a small alignment
dataset sourced from PubMed scientific abstracts.

</details>


### [25] [MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs](https://arxiv.org/abs/2509.07622)
*Libo Ren,Yee Man Ng,Lifeng Han*

Main category: cs.CL

TL;DR: 本研究提出并评估了一种基于透视感知迭代自提示（PA-ISP）的LLM方法，用于自动总结临床报告，旨在提高医患沟通效率。


<details>
  <summary>Details</summary>
Motivation: 患者与临床医生之间的高效沟通对共享决策至关重要。然而，临床报告冗长且充斥术语，使得专家难以快速识别关键信息。

Method: 在MultiClinSUM共享任务中，本文采用了基于大型语言模型（LLMs）的迭代自提示（Iterative Self-Prompting, ISP）技术，通过让LLMs生成任务特定提示并利用少样本学习进行优化。模型微调过程由ROUGE和BERT-score等词汇和嵌入空间度量指导。

Result: 使用GPT-4和GPT-4o上的透视感知ISP方法，在3,396份临床病例报告的官方评估中，ROUGE得分（P, R, F1）分别为(46.53, 24.68, 30.77)，BERTscore得分（P, R, F1）分别为(87.84, 83.25, 85.46)。高BERTscore表明模型生成了语义上等效的摘要，尽管词汇重叠度较低。

Conclusion: 该工作展示了透视感知ISP（PA-ISP）在临床报告摘要生成中的应用潜力，有望支持患者与临床医生之间更高效的沟通。

Abstract: Efficient communication between patients and clinicians plays an important
role in shared decision-making. However, clinical reports are often lengthy and
filled with clinical jargon, making it difficult for domain experts to identify
important aspects in the document efficiently. This paper presents the
methodology we applied in the MultiClinSUM shared task for summarising clinical
case documents. We used an Iterative Self-Prompting technique on large language
models (LLMs) by asking LLMs to generate task-specific prompts and refine them
via example-based few-shot learning. Furthermore, we used lexical and embedding
space metrics, ROUGE and BERT-score, to guide the model fine-tuning with
epochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved
ROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P,
R, F1) from the official evaluation on 3,396 clinical case reports from various
specialties extracted from open journals. The high BERTscore indicates that the
model produced semantically equivalent output summaries compared to the
references, even though the overlap at the exact lexicon level is lower, as
reflected in the lower ROUGE scores. This work sheds some light on how
perspective-aware ISP (PA-ISP) can be deployed for clinical report
summarisation and support better communication between patients and clinicians.

</details>


### [26] [MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval](https://arxiv.org/abs/2509.07666)
*Xixi Wu,Yanchao Tan,Nan Hou,Ruiyang Zhang,Hong Cheng*

Main category: cs.CL

TL;DR: MoLoRAG是一个逻辑感知的检索框架，通过构建页面图来捕获上下文关系并进行图遍历，从而克服了现有方法在多模态、多页文档理解中处理逻辑连接和输入尺寸限制的问题，提升了文档问答的准确性和检索精度。


<details>
  <summary>Details</summary>
Motivation: 文档问答（DocQA）是关键的评估任务。传统方法将文档转换为文本处理，丢失了关键的多模态信息。大型视觉-语言模型（LVLMs）虽能处理多模态信息，但输入尺寸受限，无法处理多页文档。检索增强生成（RAG）方法通过选择相关页面缓解此问题，但仅依赖语义相关性，忽略了对推理至关重要的页面间逻辑连接。

Method: 提出了MoLoRAG，一个针对多模态、多页文档理解的逻辑感知检索框架。通过构建一个捕获页面间上下文关系的“页面图”，一个轻量级VLM执行图遍历以检索相关页面，包括那些常被忽略的逻辑相关页面。这种方法结合了语义和逻辑相关性以提供更准确的检索。检索后，将Top-K页面输入任意LVLMs进行问答。MoLoRAG提供两种变体：一个免训练方案和经过微调以增强逻辑相关性检查的版本。

Result: 在四个DocQA数据集上进行实验，结果显示MoLoRAG在准确性上比LVLM直接推理平均提升了9.68%，在检索精度上比基线方法平均提升了7.44%。

Conclusion: MoLoRAG通过引入逻辑感知的页面图检索框架，有效解决了多模态、多页文档理解中存在的逻辑连接缺失和输入尺寸限制问题，显著提高了文档问答的准确性和检索效率。

Abstract: Document Understanding is a foundational AI capability with broad
applications, and Document Question Answering (DocQA) is a key evaluation task.
Traditional methods convert the document into text for processing by Large
Language Models (LLMs), but this process strips away critical multi-modal
information like figures. While Large Vision-Language Models (LVLMs) address
this limitation, their constrained input size makes multi-page document
comprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate
this by selecting relevant pages, but they rely solely on semantic relevance,
ignoring logical connections between pages and the query, which is essential
for reasoning.
  To this end, we propose MoLoRAG, a logic-aware retrieval framework for
multi-modal, multi-page document understanding. By constructing a page graph
that captures contextual relationships between pages, a lightweight VLM
performs graph traversal to retrieve relevant pages, including those with
logical connections often overlooked. This approach combines semantic and
logical relevance to deliver more accurate retrieval. After retrieval, the
top-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance
flexibility, MoLoRAG offers two variants: a training-free solution for easy
deployment and a fine-tuned version to improve logical relevance checking.
Experiments on four DocQA datasets demonstrate average improvements of 9.68% in
accuracy over LVLM direct inference and 7.44% in retrieval precision over
baselines. Codes and datasets are released at
https://github.com/WxxShirley/MoLoRAG.

</details>


### [27] [M-BRe: Discovering Training Samples for Relation Extraction from Unlabeled Texts with Large Language Models](https://arxiv.org/abs/2509.07730)
*Zexuan Li,Hongliang Dai,Piji Li*

Main category: cs.CL

TL;DR: 本文提出M-BRe框架，通过结合多分类和二分类方法的优点，从无标签文本中高效地自动提取高质量关系抽取训练实例，以解决传统标注成本高和LLMs在RE中面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 关系抽取（RE）任务中，人工标注训练数据成本高昂且相关句子稀少。尽管大型语言模型（LLMs）在RE中表现出潜力，但现有方法存在挑战：多分类设置下LLMs难以全面捕捉所有关系语义，导致效果不佳；而针对每个关系进行二分类虽然能缓解语义捕捉问题，但会引入巨大的计算开销，实际应用中时间复杂度过高。

Method: 本文提出了一个名为M-BRe的框架，用于从无标签文本中提取关系抽取的训练实例。该框架结合了多分类和二分类方法的优点，包含三个核心模块：关系分组（Relation Grouping）、关系抽取（Relation Extraction）和标签决策（Label Decision）。

Result: 广泛的实验证实，M-BRe在从无标签文本中发现高质量关系抽取训练样本方面具有卓越的能力。

Conclusion: M-BRe框架提供了一种有效且高效的方法，可以克服人工标注的局限性以及LLMs在多类别和二分类RE任务中面临的挑战，从而自动生成高质量的RE训练数据。

Abstract: For Relation Extraction (RE), the manual annotation of training data may be
prohibitively expensive, since the sentences that contain the target relations
in texts can be very scarce and difficult to find. It is therefore beneficial
to develop an efficient method that can automatically extract training
instances from unlabeled texts for training RE models. Recently, large language
models (LLMs) have been adopted in various natural language processing tasks,
with RE also benefiting from their advances. However, when leveraging LLMs for
RE with predefined relation categories, two key challenges arise. First, in a
multi-class classification setting, LLMs often struggle to comprehensively
capture the semantics of every relation, leading to suboptimal results. Second,
although employing binary classification for each relation individually can
mitigate this issue, it introduces significant computational overhead,
resulting in impractical time complexity for real-world applications.
Therefore, this paper proposes a framework called M-BRe to extract training
instances from unlabeled texts for RE. It utilizes three modules to combine the
advantages of both of the above classification approaches: Relation Grouping,
Relation Extraction, and Label Decision. Extensive experiments confirm its
superior capability in discovering high-quality training samples from unlabeled
texts for RE.

</details>


### [28] [Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts](https://arxiv.org/abs/2509.07755)
*Rochana Prih Hastuti,Rian Adam Rajagede,Mansour Al Ghanim,Mengxin Zheng,Qian Lou*

Main category: cs.CL

TL;DR: LLMs在医疗领域的应用存在安全风险，水印技术虽能缓解，但现有方法会严重损害医疗内容的真实性，需要开发领域感知的水印方案。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）应用于医学等敏感领域，其流畅性引发了来源和可追溯性的安全风险。水印技术旨在缓解这些风险，但其在医疗环境中的可靠性尚未经过测试，且现有评估基准忽略了水印策略可能导致的事实真实性风险。

Method: 本文提出了一个专注于医疗领域的评估工作流，联合评估LLM生成内容的真实准确性和连贯性。引入了“真实性加权分数”（Factuality-Weighted Score, FWS）这一复合指标，通过GPT-Judger和进一步的人工验证，优先评估事实准确性，以指导水印技术在医疗领域的部署。

Result: 评估结果表明，当前的水印方法会严重损害医疗内容的真实性，其熵变（entropy shifts）导致医学实体表示的退化。

Conclusion: 这些发现强调了开发领域感知的水印方法的必要性，以在医疗内容中保留信息的完整性。

Abstract: As large language models (LLMs) adapted to sensitive domains such as
medicine, their fluency raises safety risks, particularly regarding provenance
and accountability. Watermarking embeds detectable patterns to mitigate these
risks, yet its reliability in medical contexts remains untested. Existing
benchmarks focus on detection-quality tradeoffs, overlooking factual risks
under low-entropy settings often exploited by watermarking's reweighting
strategy. We propose a medical-focused evaluation workflow that jointly
assesses factual accuracy and coherence. Using GPT-Judger and further human
validation, we introduce the Factuality-Weighted Score (FWS), a composite
metric prioritizing factual accuracy beyond coherence to guide watermarking
deployment in medical domains. Our evaluation shows current watermarking
methods substantially compromise medical factuality, with entropy shifts
degrading medical entity representation. These findings underscore the need for
domain-aware watermarking approaches that preserve the integrity of medical
content.

</details>


### [29] [Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning](https://arxiv.org/abs/2509.07768)
*Michele Joshua Maggini,Dhia Merzougui,Rabiraj Bandyopadhyay,Gaël Dias,Fabrice Maurel,Pablo Gamallo*

Main category: cs.CL

TL;DR: 该研究全面评估了大型语言模型（LLMs）在虚假新闻、有害推文和政治偏见检测任务中的表现，发现微调模型通常优于上下文学习策略。


<details>
  <summary>Details</summary>
Motivation: 在线平台上虚假新闻、极化和有害内容的传播日益严重。尽管大型语言模型前景广阔，但缺乏对其在此类任务中跨不同模型、使用方法和语言的性能进行系统基准测试的研究。

Method: 研究涵盖10个数据集和5种语言（英、西、葡、阿、保），包括二分类和多分类任务。测试了从参数高效微调（PEFT）到多种上下文学习（In-Context Learning）策略（如零样本、代码簿、少样本及思维链）等不同方法。上下文学习评估的模型包括LlaMA3.1-8b-Instruct、Mistral-Nemo-Instruct-2407和Qwen2.5-7B-Instruct。

Result: 实验发现，上下文学习（In-Context Learning）的表现通常不如对模型进行微调（Fine-Tuning）。

Conclusion: 这一主要发现强调了即使是较小的模型，在任务特定设置下进行微调的重要性，其性能甚至优于在上下文学习设置下评估的最新大型模型。

Abstract: The spread of fake news, polarizing, politically biased, and harmful content
on online platforms has been a serious concern. With large language models
becoming a promising approach, however, no study has properly benchmarked their
performance across different models, usage methods, and languages. This study
presents a comprehensive overview of different Large Language Models adaptation
paradigms for the detection of hyperpartisan and fake news, harmful tweets, and
political bias. Our experiments spanned 10 datasets and 5 different languages
(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and
multiclass classification scenarios. We tested different strategies ranging
from parameter efficient Fine-Tuning of language models to a variety of
different In-Context Learning strategies and prompts. These included zero-shot
prompts, codebooks, few-shot (with both randomly-selected and
diversely-selected examples using Determinantal Point Processes), and
Chain-of-Thought. We discovered that In-Context Learning often underperforms
when compared to Fine-Tuning a model. This main finding highlights the
importance of Fine-Tuning even smaller models on task-specific settings even
when compared to the largest models evaluated in an In-Context Learning setup -
in our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and
Qwen2.5-7B-Instruct.

</details>


### [30] [SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP](https://arxiv.org/abs/2509.07801)
*Decheng Duan,Yingyi Zhang,Jitong Peng,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 本文介绍了SciNLP，一个专门用于NLP领域全文本实体和关系抽取的基准数据集，旨在解决现有数据集对特定出版物部分的局限性，并验证了其在构建细粒度知识图谱方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 结构化信息抽取对于捕捉专业领域的核心概念和趋势至关重要，但现有数据集大多因领域复杂性和标注成本高昂而专注于特定出版物部分，缺乏全文本的实体和关系标注。

Method: 引入SciNLP数据集，包含60篇手动标注的NLP领域全文本出版物，覆盖7,072个实体和1,826个关系。通过与类似数据集进行比较实验，并评估了最先进的监督模型在该数据集上的性能，并利用训练好的模型构建了NLP领域的细粒度知识图谱。

Result: SciNLP是首个提供NLP领域全文本实体及其关系标注的数据集。实验表明，现有模型在不同长度学术文本上的抽取能力各异，与现有数据集相比，SciNLP在某些基线模型上实现了显著的性能提升。构建的知识图谱平均节点度为3.2，表明丰富的语义拓扑信息。

Conclusion: SciNLP数据集有效解决了全文本信息抽取的局限性，提升了模型性能，并成功应用于NLP领域细粒度知识图谱的自动构建，为下游应用提供了丰富的语义信息。

Abstract: Structured information extraction from scientific literature is crucial for
capturing core concepts and emerging trends in specialized fields. While
existing datasets aid model development, most focus on specific publication
sections due to domain complexity and the high cost of annotating scientific
texts. To address this limitation, we introduce SciNLP - a specialized
benchmark for full-text entity and relation extraction in the Natural Language
Processing (NLP) domain. The dataset comprises 60 manually annotated full-text
NLP publications, covering 7,072 entities and 1,826 relations. Compared to
existing research, SciNLP is the first dataset providing full-text annotations
of entities and their relationships in the NLP domain. To validate the
effectiveness of SciNLP, we conducted comparative experiments with similar
datasets and evaluated the performance of state-of-the-art supervised models on
this dataset. Results reveal varying extraction capabilities of existing models
across academic texts of different lengths. Cross-comparisons with existing
datasets show that SciNLP achieves significant performance improvements on
certain baseline models. Using models trained on SciNLP, we implemented
automatic construction of a fine-grained knowledge graph for the NLP domain.
Our KG has an average node degree of 3.2 per entity, indicating rich semantic
topological information that enhances downstream applications. The dataset is
publicly available at https://github.com/AKADDC/SciNLP.

</details>


### [31] [Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems](https://arxiv.org/abs/2509.07817)
*Xiaolin Chen,Xuemeng Song,Haokun Wen,Weili Guan,Xiangyu Zhao,Liqiang Nie*

Main category: cs.CL

TL;DR: 本文提出DK2R，一种双知识增强的两阶段推理器，通过充分利用结构化属性和非结构化评论知识并结合大型语言模型（LLMs），以解决多模态任务型对话系统中回复生成面临的知识选择和意图-回复解耦挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态任务型对话系统的文本回复生成中存在两点局限：1) 忽视非结构化评论知识；2) 未充分利用大型语言模型（LLMs）。研究旨在利用双重知识（结构化属性和非结构化评论）与LLMs相结合，提升回复生成效果，但面临动态知识类型选择和意图-回复解耦的挑战。

Method: 本文提出名为DK2R的双知识增强两阶段推理器。具体而言，DK2R首先从外部知识库中提取结构化属性和非结构化评论知识。随后，DK2R利用LLM通过分析LLM生成的临时探测回复来评估每种知识类型的效用。此外，DK2R通过专门推理单独总结面向意图的关键线索，并将其作为辅助信号以增强基于LLM的文本回复生成。

Result: 在公开数据集上进行的广泛实验验证了DK2R的优越性。

Conclusion: DK2R是一种有效的方法，通过利用双重知识和LLMs，并成功应对动态知识类型选择和意图-回复解耦的挑战，显著提升了多模态任务型对话系统中的文本回复生成能力。

Abstract: Textual response generation is pivotal for multimodal \mbox{task-oriented}
dialog systems, which aims to generate proper textual responses based on the
multimodal context. While existing efforts have demonstrated remarkable
progress, there still exist the following limitations: 1) \textit{neglect of
unstructured review knowledge} and 2) \textit{underutilization of large
language models (LLMs)}. Inspired by this, we aim to fully utilize dual
knowledge (\textit{i.e., } structured attribute and unstructured review
knowledge) with LLMs to promote textual response generation in multimodal
task-oriented dialog systems. However, this task is non-trivial due to two key
challenges: 1) \textit{dynamic knowledge type selection} and 2)
\textit{intention-response decoupling}. To address these challenges, we propose
a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for
multimodal dialog systems (named DK2R). To be specific, DK2R first extracts
both structured attribute and unstructured review knowledge from external
knowledge base given the dialog context. Thereafter, DK2R uses an LLM to
evaluate each knowledge type's utility by analyzing LLM-generated provisional
probe responses. Moreover, DK2R separately summarizes the intention-oriented
key clues via dedicated reasoning, which are further used as auxiliary signals
to enhance LLM-based textual response generation. Extensive experiments
conducted on a public dataset verify the superiority of DK2R. We have released
the codes and parameters.

</details>


### [32] [Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost](https://arxiv.org/abs/2509.07829)
*Mihai Nadas,Laura Diosan,Andreea Tomescu,Andrei Piscoran*

Main category: cs.CL

TL;DR: 本文提出TINYFABULIST TRANSLATION FRAMEWORK (TF2)，这是一个统一的框架，用于创建大规模合成数据集、微调语言模型(TF2-12B)和评估英-罗马尼亚语文学翻译，旨在为低资源语言提供高质量、成本效益高的文学翻译解决方案，并实现与领先专有模型相媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 文学翻译是机器翻译中一个复杂且受关注的任务，但小型开源模型在此方面表现仍有待提升。此外，对于罗马尼亚语等低资源语言，缺乏高质量、丰富的文学数据集是主要挑战。

Method: 引入TF2框架，包含数据集创建、模型微调和评估。首先，利用高性能大型语言模型从DS-TF1-EN-3M中生成1.5万条高质量罗马尼亚语对照参考，构建大规模合成平行数据集(DS-TF2-EN-RO)。接着，对一个12B参数的开源模型进行两阶段微调：(i)指令微调以捕捉特定体裁的叙事风格，(ii)适配器压缩以实现高效部署。评估结合了语料库级别的BLEU分数和基于大型语言模型的五维度（准确性、流畅性、连贯性、风格、文化适应性）评估准则。

Result: 微调后的TF2-12B模型在流畅性和充足性方面达到了与顶尖大型专有模型相当的水平。同时，该模型是开放、可访问且成本效益显著更高的。所有相关的微调模型、数据集、脚本和评估提示均已公开。

Conclusion: TF2提供了一个端到端、可复现的流水线，支持对成本效益高的翻译、跨语言叙事生成以及在低资源环境下将开源模型广泛应用于文化重要文学内容的研究。这为解决文学翻译中的关键挑战提供了实用且可推广的解决方案。

Abstract: Literary translation has recently gained attention as a distinct and complex
task in machine translation research. However, the translation by small open
models remains an open problem. We contribute to this ongoing research by
introducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for
dataset creation, fine tuning, and evaluation in English-Romanian literary
translations, centred on the creation and open release of both a compact, fine
tuned language model (TF2-12B) and large scale synthetic parallel datasets
(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the
largest collection of synthetic English fables to date, we address the need for
rich, high quality literary datasets in low resource languages such as
Romanian. Our pipeline first generates 15k high quality Romanian references
from the TF1 pool using a high performing LLM. We then apply a two stage fine
tuning process to a 12B parameter open weight model: (i) instruction tuning to
capture genre specific narrative style, and (ii) adapter compression for
efficient deployment. Evaluation combines corpus level BLEU and a five
dimension LLM based rubric (accuracy, fluency, coherence, style, cultural
adaptation) to provide a nuanced assessment of translation quality. Results
show that our fine tuned model achieves fluency and adequacy competitive with
top performing large proprietary models, while being open, accessible, and
significantly more cost effective. Alongside the fine tuned model and both
datasets, we publicly release all scripts and evaluation prompts. TF2 thus
provides an end-to-end, reproducible pipeline for research on cost efficient
translation, cross lingual narrative generation, and the broad adoption of open
models for culturally significant literary content in low resource settings.

</details>


### [33] [Are Humans as Brittle as Large Language Models?](https://arxiv.org/abs/2509.07869)
*Jiahui Li,Sean Papay,Roman Klinger*

Main category: cs.CL

TL;DR: 本文对比研究了大型语言模型（LLM）和人类对指令变化的敏感性。发现两者均对特定提示词修改表现出脆性，但人类对排版错误和标签顺序变化的鲁棒性优于LLM。


<details>
  <summary>Details</summary>
Motivation: LLM的提示词脆性效应被广泛假设为其独有特性，但其与人类行为的相似性未经充分探索。研究旨在探究人类标注者是否同样对指令变化敏感，以及LLM的脆性是否可能反映了人类标注的变异性，而非固有缺陷。

Method: 通过系统性比较对LLM和人类标注者施加相同的提示词（指令）修改，以评估人类是否对提示词扰动表现出类似敏感性。具体方法是在一系列文本分类任务中，对两者使用不同提示词变体进行测试。

Result: 实验结果表明，人类和LLM在面对特定类型的提示词修改时（尤其是替换备选标签集或标签格式）都表现出更高的脆性。然而，人类判断的分布受排版错误和颠倒标签顺序的影响小于LLM。

Conclusion: 人类和LLM在面对某些类型的提示词修改时均显示出敏感性，但人类对某些微小扰动（如排版错误、标签顺序）的鲁棒性优于LLM。这表明LLM的提示词脆性并非完全等同于人类行为，并在某些方面表现出更高的脆弱性。

Abstract: The output of large language models (LLM) is unstable, due to both
non-determinism of the decoding process as well as to prompt brittleness. While
the intrinsic non-determinism of LLM generation may mimic existing uncertainty
in human annotations through distributional shifts in outputs, it is largely
assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.
This raises the question: do human annotators show similar sensitivity to
instruction changes? If so, should prompt brittleness in LLMs be considered
problematic? One may alternatively hypothesize that prompt brittleness
correctly reflects human annotation variances. To fill this research gap, we
systematically compare the effects of prompt modifications on LLMs and
identical instruction modifications for human annotators, focusing on the
question of whether humans are similarly sensitive to prompt perturbations. To
study this, we prompt both humans and LLMs for a set of text classification
tasks conditioned on prompt variations. Our findings indicate that both humans
and LLMs exhibit increased brittleness in response to specific types of prompt
modifications, particularly those involving the substitution of alternative
label sets or label formats. However, the distribution of human judgments is
less affected by typographical errors and reversed label order than that of
LLMs.

</details>


### [34] [From Detection to Mitigation: Addressing Gender Bias in Chinese Texts via Efficient Tuning and Voting-Based Rebalancing](https://arxiv.org/abs/2509.07889)
*Chengyan Wu,Yiqiang Cai,Yufei Cheng,Yun Xue*

Main category: cs.CL

TL;DR: 本文介绍了NLPCC-2025共享任务7中，团队针对中文句子级性别偏见检测与消减的解决方案，采用基于LoRA微调的大语言模型、数据平衡与增强、多专家模型投票和多温度采样等方法，最终获得任务第四名。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在促进自然语言生成的公平性和可控性，通过自动检测、分类和消减中文句子中的性别偏见。

Method: 团队采用基于LoRA（Low-Rank Adaptation）微调的大语言模型，构建更平衡和多样化的训练数据集。在检测和分类子任务中，使用多专家模型多数投票策略。为改进偏见生成检测和消减，设计了多温度采样机制。

Result: 实验结果证明了该方法在偏见检测、分类和消减方面的有效性。团队最终获得了47.90%的平均分，在共享任务中排名第四。

Conclusion: 所提出的方法能有效处理中文句子中的性别偏见检测、分类和消减任务，并在共享任务中取得了良好的竞争力。

Abstract: This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which
focuses on sentence-level gender bias detection and mitigation in Chinese. The
task aims to promote fairness and controllability in natural language
generation by automatically detecting, classifying, and mitigating gender bias.
To address this challenge, we adopt a fine-tuning approach based on large
language models (LLMs), efficiently adapt to the bias detection task via
Low-Rank Adaptation (LoRA). In terms of data processing, we construct a more
balanced training set to alleviate class imbalance and introduce heterogeneous
samples from multiple sources to enhance model generalization. For the
detection and classification sub-tasks, we employ a majority voting strategy
that integrates outputs from multiple expert models to boost performance.
Additionally, to improve bias generation detection and mitigation, we design a
multi-temperature sampling mechanism to capture potential variations in bias
expression styles. Experimental results demonstrate the effectiveness of our
approach in bias detection, classification, and mitigation. Our method
ultimately achieves an average score of 47.90%, ranking fourth in the shared
task.

</details>


### [35] [Biased Tales: Cultural and Topic Bias in Generating Children's Stories](https://arxiv.org/abs/2509.07908)
*Donya Rooein,Vilém Zouhar,Debora Nozza,Dirk Hovy*

Main category: cs.CL

TL;DR: 本研究通过构建Biased Tales数据集，分析了大型语言模型（LLMs）生成的儿童故事中存在的性别和文化刻板印象，揭示了女孩角色更强调外貌、非西方儿童故事更侧重文化传统等显著偏见，并强调了社会文化偏见在实现创意AI公平性中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 故事对儿童信念和道德形成至关重要。随着家长日益依赖LLMs创作睡前故事，这些故事中存在的文化和性别刻板印象引发了严重担忧。本研究旨在解决LLM生成故事中的偏见问题。

Method: 我们提出了一个名为“Biased Tales”的综合数据集，专门用于分析偏见如何影响LLM生成故事中主角的属性和故事元素。研究通过对该数据集的分析揭示了偏见模式。

Result: 分析发现了显著差异：当主角被描述为女孩时（与男孩相比），与外貌相关的属性增加了55.26%。讲述非西方儿童的故事，与西方儿童的故事相比，不成比例地更强调文化遗产、传统和家庭主题。

Conclusion: 研究结果突出了社会文化偏见在使创意AI使用更公平和多样化方面所扮演的角色，强调了解决这些偏见的重要性。

Abstract: Stories play a pivotal role in human communication, shaping beliefs and
morals, particularly in children. As parents increasingly rely on large
language models (LLMs) to craft bedtime stories, the presence of cultural and
gender stereotypes in these narratives raises significant concerns. To address
this issue, we present Biased Tales, a comprehensive dataset designed to
analyze how biases influence protagonists' attributes and story elements in
LLM-generated stories. Our analysis uncovers striking disparities. When the
protagonist is described as a girl (as compared to a boy), appearance-related
attributes increase by 55.26%. Stories featuring non-Western children
disproportionately emphasize cultural heritage, tradition, and family themes
far more than those for Western children. Our findings highlight the role of
sociocultural bias in making creative AI use more equitable and diverse.

</details>


### [36] [GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models](https://arxiv.org/abs/2509.07925)
*Tuo Wang,Adithya Kulkarni,Tyler Cody,Peter A. Beling,Yujun Yan,Dawei Zhou*

Main category: cs.CL

TL;DR: GENUINE提出一种基于图的结构感知框架，利用依存句法树和分层图池化，通过监督学习提高LLM不确定性估计的可靠性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM不确定性估计方法忽视语义依赖和结构关系，依赖于词元级概率，在关键应用中可靠性不足。

Method: 提出GENUINE框架，通过利用依存句法树和分层图池化，结合监督学习，有效建模语义和结构关系，以精炼不确定性量化。

Result: 在多项NLP任务中，GENUINE的AUROC比基于语义熵的方法高出29%，校准误差降低超过15%。

Conclusion: 基于图的不确定性建模能有效提升LLM的置信度评估并减少校准误差，证明了其在不确定性估计中的有效性。

Abstract: Uncertainty estimation is essential for enhancing the reliability of Large
Language Models (LLMs), particularly in high-stakes applications. Existing
methods often overlook semantic dependencies, relying on token-level
probability measures that fail to capture structural relationships within the
generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty
Estimation for Large Language Models, a structure-aware framework that
leverages dependency parse trees and hierarchical graph pooling to refine
uncertainty quantification. By incorporating supervised learning, GENUINE
effectively models semantic and structural relationships, improving confidence
assessments. Extensive experiments across NLP tasks show that GENUINE achieves
up to 29% higher AUROC than semantic entropy-based approaches and reduces
calibration errors by over 15%, demonstrating the effectiveness of graph-based
uncertainty modeling. The code is available at
https://github.com/ODYSSEYWT/GUQ.

</details>


### [37] [SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge](https://arxiv.org/abs/2509.07968)
*Lukas Haas,Gal Yona,Giovanni D'Antonio,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: 本文引入了SimpleQA Verified，一个1,000个提示的新基准，旨在解决OpenAI原版SimpleQA基准的局限性，以更准确地评估大型语言模型（LLM）的简短事实准确性。在该基准上，Gemini 2.5 Pro取得了最先进的F1分数，超越了其他前沿模型。


<details>
  <summary>Details</summary>
Motivation: OpenAI的SimpleQA基准存在关键局限性，包括标签噪声和错误、主题偏见以及问题冗余，这些问题阻碍了对LLM事实准确性的可靠评估。因此，需要一个更高质量、更具挑战性的评估工具。

Method: SimpleQA Verified通过严格的多阶段过滤过程创建，该过程涉及去重、主题平衡和来源核对，以生成一个更可靠且更具挑战性的评估集。此外，还对自动评估器提示进行了改进。

Result: 在新的SimpleQA Verified基准测试中，Gemini 2.5 Pro取得了55.6的F1分数，达到最先进水平，超越了包括GPT-5在内的其他前沿模型。

Conclusion: 这项工作为研究社区提供了一个更高精度的工具，以跟踪参数模型事实准确性的真实进展，并有效缓解模型产生的幻觉。

Abstract: We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large
Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It
addresses critical limitations in OpenAI's benchmark, including noisy and
incorrect labels, topical biases, and question redundancy. SimpleQA Verified
was created through a rigorous multi-stage filtering process involving
de-duplication, topic balancing, and source reconciliation to produce a more
reliable and challenging evaluation set, alongside improvements in the
autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a
state-of-the-art F1-score of 55.6, outperforming other frontier models,
including GPT-5. This work provides the research community with a
higher-fidelity tool to track genuine progress in parametric model factuality
and to mitigate hallucinations. The benchmark dataset, evaluation code, and
leaderboard are available at:
https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.

</details>


### [38] [Parallel-R1: Towards Parallel Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.07980)
*Tong Zheng,Hongming Zhang,Wenhao Yu,Xiaoyang Wang,Xinyu Yang,Runpeng Dai,Rui Liu,Huiwen Bao,Chengsong Huang,Heng Huang,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出了Parallel-R1，首个利用强化学习（RL）框架实现大型语言模型（LLMs）并行思维能力的方案，通过渐进式课程训练，解决了传统SFT方法的局限性，显著提升了LLMs在复杂推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 激活LLMs的并行思维能力面临挑战，现有方法主要依赖于合成数据上的监督微调（SFT），这鼓励了模仿而非探索和泛化。因此，需要一种新的方法来克服这些限制，实现LLMs在真实世界复杂推理任务中的并行思维。

Method: 我们提出了Parallel-R1，一个基于强化学习的框架，用于训练LLMs的并行思维行为。该框架采用渐进式课程：首先，在较简单任务上使用提示生成轨迹进行SFT，以初步灌输并行思维能力；然后，过渡到RL，在更困难的问题上探索和泛化此技能，以解决RL训练中的冷启动问题。

Result: 在MATH、AMC23和AIME等多个数学基准测试中，Parallel-R1成功灌输了并行思维，相较于直接在挑战性任务上用RL训练的顺序思维模型，准确率提高了8.4%。分析表明，模型思维行为发生了明显转变：早期作为探索策略，后期用于多视角验证。最重要的是，并行思维被验证为“训练中期的探索支架”，其临时探索阶段解锁了更高的RL后性能上限，在AIME25上比基线提高了42.9%。

Conclusion: Parallel-R1证明了并行思维作为LLMs训练中关键的“探索支架”的作用，它能在训练中期促进探索，从而在强化学习后显著提高模型性能上限。这为LLMs处理复杂真实世界推理任务提供了新的方向。

Abstract: Parallel thinking has emerged as a novel approach for enhancing the reasoning
capabilities of large language models (LLMs) by exploring multiple reasoning
paths concurrently. However, activating such capabilities through training
remains challenging, as existing methods predominantly rely on supervised
fine-tuning (SFT) over synthetic data, which encourages teacher-forced
imitation rather than exploration and generalization. Different from them, we
propose \textbf{Parallel-R1}, the first reinforcement learning (RL) framework
that enables parallel thinking behaviors for complex real-world reasoning
tasks. Our framework employs a progressive curriculum that explicitly addresses
the cold-start problem in training parallel thinking with RL. We first use SFT
on prompt-generated trajectories from easier tasks to instill the parallel
thinking ability, then transition to RL to explore and generalize this skill on
harder problems. Experiments on various math benchmarks, including MATH, AMC23,
and AIME, show that Parallel-R1 successfully instills parallel thinking,
leading to 8.4% accuracy improvements over the sequential thinking model
trained directly on challenging tasks with RL. Further analysis reveals a clear
shift in the model's thinking behavior: at an early stage, it uses parallel
thinking as an exploration strategy, while in a later stage, it uses the same
capability for multi-perspective verification. Most significantly, we validate
parallel thinking as a \textbf{mid-training exploration scaffold}, where this
temporary exploratory phase unlocks a higher performance ceiling after RL,
yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and
code will be open-source at https://github.com/zhengkid/Parallel-R1.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [39] [CellPainTR: Generalizable Representation Learning for Cross-Dataset Cell Painting Analysis](https://arxiv.org/abs/2509.06986)
*Cedric Caruzzo,Jong Chul Ye*

Main category: cs.CV

TL;DR: CellPainTR是一种基于Transformer的模型，能够学习对批次效应稳健的细胞形态基础表示，并通过源特异性上下文token实现无需微调的域外泛化，在批次整合和生物信号保留方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模生物发现面临整合异构数据集的挑战，主要障碍是技术批次效应和缺乏可泛化的模型。

Method: 引入了CellPainTR，一种基于Transformer的架构，其特点是使用源特异性上下文token，旨在学习对批次效应稳健的细胞形态基础表示，并实现对完全未见数据集的域外泛化，无需微调。

Result: CellPainTR在JUMP数据集上优于ComBat和Harmony，在批次整合和生物信号保留方面表现出色。在未见的Bray et al.数据集上的挑战性域外任务中，即使存在显著的领域和特征偏移，CellPainTR仍保持了高水平性能。

Conclusion: 该工作为创建图像分析的真正基础模型迈出了重要一步，有望实现更可靠、可扩展的跨研究生物分析。

Abstract: Large-scale biological discovery requires integrating massive, heterogeneous
datasets like those from the JUMP Cell Painting consortium, but technical batch
effects and a lack of generalizable models remain critical roadblocks. To
address this, we introduce CellPainTR, a Transformer-based architecture
designed to learn foundational representations of cellular morphology that are
robust to batch effects. Unlike traditional methods that require retraining on
new data, CellPainTR's design, featuring source-specific context tokens, allows
for effective out-of-distribution (OOD) generalization to entirely unseen
datasets without fine-tuning. We validate CellPainTR on the large-scale JUMP
dataset, where it outperforms established methods like ComBat and Harmony in
both batch integration and biological signal preservation. Critically, we
demonstrate its robustness through a challenging OOD task on the unseen Bray et
al. dataset, where it maintains high performance despite significant domain and
feature shifts. Our work represents a significant step towards creating truly
foundational models for image-based profiling, enabling more reliable and
scalable cross-study biological analysis.

</details>


### [40] [FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection](https://arxiv.org/abs/2509.06987)
*Alexey Zhukov,Jenny Benois-Pineau,Amira Youssef,Akka Zemmari,Mohamed Mosbah,Virginie Taillandier*

Main category: cs.CV

TL;DR: 本研究提出一种结合YOLOv8n、ViT和音频特征的多模态融合架构，用于铁路缺陷检测，相比纯视觉方法显著提高了精度和准确性，解决了单模态过检问题。


<details>
  <summary>Details</summary>
Motivation: 传统单模态检测（如YOLO）在图像中存在与正常结构相似的外观时容易导致过度检测。虽然音频信号不一定具有高语义信息，但其测量数据对于检测铁路结构元素或缺陷具有潜在价值，结合多模态信息有望克服单模态方法的局限性。

Method: 提出了一种基于领域规则的多模态融合架构，结合了YOLO和Vision Transformer (ViT) 主干网络。该方法整合YOLOv8n进行快速目标检测，利用ViT结合从多层（7、16和19）提取的特征图，并为“铁轨断裂”和“表面缺陷”两种缺陷类别合成音频表示，最终在音频和图像之间进行特征融合。

Result: 在真实的铁路数据集上进行的实验评估表明，所提出的多模态融合方法相比纯视觉方法，在精度和整体准确性上提高了0.2个百分点。Student's非配对t检验也证实了平均准确性差异的统计显著性。

Conclusion: 该多模态融合架构通过有效结合图像和音频信息，显著提高了铁路缺陷检测的精度和整体准确性，克服了传统单模态方法的局限性，在真实世界应用中展现出优越性。

Abstract: Multimodal fusion is a multimedia technique that has become popular in the
wide range of tasks where image information is accompanied by a signal/audio.
The latter may not convey highly semantic information, such as speech or music,
but some measures such as audio signal recorded by mics in the goal to detect
rail structure elements or defects. While classical detection approaches such
as You Only Look Once (YOLO) family detectors can be efficiently deployed for
defect detection on the image modality, the single modality approaches remain
limited. They yield an overdetection in case of the appearance similar to
normal structural elements. The paper proposes a new multimodal fusion
architecture built on the basis of domain rules with YOLO and Vision
transformer backbones. It integrates YOLOv8n for rapid object detection with a
Vision Transformer (ViT) to combine feature maps extracted from multiple layers
(7, 16, and 19) and synthesised audio representations for two defect classes:
rail Rupture and Surface defect. Fusion is performed between audio and image.
Experimental evaluation on a real-world railway dataset demonstrates that our
multimodal fusion improves precision and overall accuracy by 0.2 points
compared to the vision-only approach. Student's unpaired t-test also confirms
statistical significance of differences in the mean accuracy.

</details>


### [41] [Frustratingly Easy Feature Reconstruction for Out-of-Distribution Detection](https://arxiv.org/abs/2509.06988)
*Yingsheng Wang,Shuo Lu,Jian Liang,Aihua Zheng,Ran He*

Main category: cs.CV

TL;DR: 本文提出ClaFR，一种无需训练数据即可进行OOD检测的后验方法。该方法通过分类器权重正交分解构建类别已知子空间，并利用特征重构误差来识别OOD数据，实现了领先的检测性能。


<details>
  <summary>Details</summary>
Motivation: OOD检测在安全应用中至关重要。现有的基于特征的后验方法通常需要访问训练数据，这在数据隐私敏感的场景下不可行，限制了其应用范围。

Method: 本文提出Classifier-based Feature Reconstruction (ClaFR) 方法。它通过对分类器权重进行正交分解来提取类别已知子空间，然后将数据特征投影到该子空间。OOD分数通过计算数据在该子空间内的特征重构误差来确定。

Result: ClaFR方法无需访问训练数据，却能在多个OOD基准测试中取得领先的性能，优于现有OOD检测算法。

Conclusion: ClaFR提供了一种简单而有效的OOD检测方案，解决了现有方法对训练数据依赖的隐私问题，并在不牺牲性能的前提下，展现了其在OOD检测任务中的优越性。

Abstract: Out-of-distribution (OOD) detection helps models identify data outside the
training categories, crucial for security applications. While feature-based
post-hoc methods address this by evaluating data differences in the feature
space without changing network parameters, they often require access to
training data, which may not be suitable for some data privacy scenarios. This
may not be suitable in scenarios where data privacy protection is a concern. In
this paper, we propose a simple yet effective post-hoc method, termed
Classifier-based Feature Reconstruction (ClaFR), from the perspective of
subspace projection. It first performs an orthogonal decomposition of the
classifier's weights to extract the class-known subspace, then maps the
original data features into this subspace to obtain new data representations.
Subsequently, the OOD score is determined by calculating the feature
reconstruction error of the data within the subspace. Compared to existing OOD
detection algorithms, our method does not require access to training data while
achieving leading performance on multiple OOD benchmarks. Our code is released
at https://github.com/Aie0923/ClaFR.

</details>


### [42] [DIET-CP: Lightweight and Data Efficient Self Supervised Continued Pretraining](https://arxiv.org/abs/2509.06990)
*Bryan Rodas,Natalie Montesino,Jakob Ambsdorf,David Klindt,Randall Balestriero*

Main category: cs.CV

TL;DR: DIET-CP是一种简单高效的持续预训练策略，能在小数据集领域中显著提升基础模型的性能，且无需额外超参数。


<details>
  <summary>Details</summary>
Motivation: 在专业领域，小规模数据集限制了大规模自监督学习（SSL）方法的适用性与超参数搜索；同时，预训练模型常仅发布骨干权重，缺乏继续预训练所需信息，导致基础模型难以有效适应新领域。

Method: 提出了DIET-CP策略，它基于一个简单的目标函数，无需标签，并且不引入比监督微调更多的超参数。该方法能将任何强大的基础模型有效引导至新的目标数据分布。

Result: DIET-CP在不同数据模态和骨干网络选择上均表现稳定。它能显著提升DINOv3等先进模型在仅使用1000张图像情况下的性能。

Conclusion: DIET-CP成功弥补了在专业小数据集环境下对基础模型进行持续预训练的空白，为领域适应提供了一个简单、稳定且高效的解决方案。

Abstract: Continued pretraining offers a promising solution for adapting foundation
models to a new target domain. However, in specialized domains, available
datasets are often very small, limiting the applicability of SSL methods
developed for large-scale pretraining and making hyperparameter search
infeasible. In addition, pretrained models are usually released as
backbone-weights only, lacking important information to continue pretraining.
We propose to bridge this gap with DIET-CP, a simple continued pretraining
strategy, where any strong foundation model can be steered towards the new data
distribution of interest. DIET-CP relies on a very simple objective, requires
no labels, and introduces no more hyperparameters than supervised finetuning.
It is stable across data modalities and backbone choices, while providing a
significant performance boost for state-of-the-art models such as DINOv3 using
only 1000 images.

</details>


### [43] [FedAPT: Federated Adversarial Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2509.06992)
*Kun Zhai,Siheng Chen,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出了联邦对抗提示微调 (FedAPT)，通过引入类感知提示生成器和跨层生成器共享策略，显著提升了联邦提示微调 (FPT) 在非IID数据下的对抗鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 联邦提示微调 (FPT) 调优的大型视觉-语言模型 (VLMs) 易受对抗攻击，导致下游任务误分类。尤其在非独立同分布 (non-IID) 设置下，客户端与全局模型之间存在“类别信息鸿沟”问题，即客户端仅依赖有限的本地标签信息生成对抗样本，而全局模型需要防御来自全局标签的对抗攻击。

Method: 本文提出联邦对抗提示微调 (FedAPT)。为解决类别信息鸿沟问题，FedAPT 引入了：1) 一个由“全局标签嵌入”引导的类感知提示生成器，该生成器从文本提示生成视觉提示，并编码跨客户端的标签信息，以创建更全局对齐的视觉提示。2) 一种跨层生成器共享策略，用于增强模型不同层之间提示的耦合，进一步提高对抗鲁棒性。

Result: 在多个图像分类数据集上进行的大量实验表明，FedAPT 在提高对抗鲁棒性方面表现出卓越的性能，大幅优于现有方法。此外，FedAPT 在跨领域和跨数据集场景中也展现出非凡的泛化能力。

Conclusion: FedAPT 有效解决了 FPT 在对抗攻击下的脆弱性，尤其是在非IID数据环境中，并通过其卓越的鲁棒性和泛化能力，证明了其在实际应用中的有效性。

Abstract: Federated Prompt Tuning (FPT) is an efficient method for cross-client
collaborative fine-tuning of large Vision-Language Models (VLMs). However,
models tuned using FPT are vulnerable to adversarial attacks, leading to
misclassification in downstream tasks. In this work, we introduce Federated
Adversarial Prompt Tuning (\textbf{FedAPT}), a novel method designed to enhance
the adversarial robustness of FPT. We identify a key issue in FedAPT under
non-independent and identically distributed (non-IID) settings: a \textit{class
information gap} between clients and the global model. Clients rely solely on
limited local label information to generate adversarial samples for training,
while the global model must defend against adversarial attacks from global
labels. To address this issue, we propose a \textbf{class-aware prompt
generator} that generates visual prompts from text prompts. This generator is
guided by a \emph{Global Label Embedding} (serving as a ``beacon") which
encodes cross-client label information to create more globally-aligned visual
prompts. Additionally, we propose a \textbf{cross-layer generator sharing}
strategy to enhance prompt coupling across different layers of the model,
further boosting adversarial robustness. Extensive experiments on multiple
image classification datasets demonstrate the superiority of FedAPT in
improving adversarial robustness, outperforming existing methods by a large
margin. FedAPT also exhibits exceptional generalization in cross-domain and
cross-dataset scenarios, indicating its effectiveness in real-world
applications.

</details>


### [44] [Geospatial Foundational Embedder: Top-1 Winning Solution on EarthVision Embed2Scale Challenge (CVPR 2025)](https://arxiv.org/abs/2509.06993)
*Zirui Xu,Raphael Tang,Mike Bianco,Qi Zhang,Rishi Madhok,Nikolaos Karianakis,Fuxun Yu*

Main category: cs.CV

TL;DR: 本技术报告介绍了EarthVision Embed2Scale挑战赛的Top-1获胜方案，旨在开发将高光谱地理空间数据嵌入向量的基石模型。


<details>
  <summary>Details</summary>
Motivation: 参与并赢得EarthVision Embed2Scale挑战赛，该挑战赛旨在开发基础地理空间模型，将SSL4EO-S12高光谱地理空间数据立方体嵌入到可促进各种下游任务（如分类、回归）的向量中。

Method: 报告介绍了作者提出的方法，该方法是他们在Embed2Scale挑战赛中取得Top-1的关键。具体方法细节未在摘要中提及。

Result: 在EarthVision Embed2Scale挑战赛中获得了Top-1的获胜方案。

Conclusion: 作者提出的方法在EarthVision Embed2Scale挑战赛中表现出色，成功获得了第一名，证明了其在开发基础地理空间模型方面的有效性。

Abstract: EarthVision Embed2Scale challenge (CVPR 2025) aims to develop foundational
geospatial models to embed SSL4EO-S12 hyperspectral geospatial data cubes into
embedding vectors that faciliatetes various downstream tasks, e.g.,
classification, regression, etc. In this technical report, we introduce our
proposed method for the Top-1 winning solution on the Embed2Scale Challenge.

</details>


### [45] [VLMs-in-the-Wild: Bridging the Gap Between Academic Benchmarks and Enterprise Reality](https://arxiv.org/abs/2509.06994)
*Srihari Bandraupalli,Anupam Purwar*

Main category: cs.CV

TL;DR: 本文提出VLM-in-the-Wild (ViLD) 框架，旨在通过评估十项业务关键任务，弥合视觉-语言模型 (VLM) 学术评估与企业实际部署需求之间的鸿沟，并引入创新的BlockWeaver算法进行OCR输出对比，为VLM在企业环境中的应用提供可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 开源VLM在企业应用中潜力巨大，但现有学术评估与企业部署要求存在显著脱节。当前基准依赖多选题和合成数据，未能捕捉社交媒体内容分析等真实世界业务的复杂性。

Method: 引入ViLD框架，定义了标志检测、OCR、目标检测、人物分析、场景检测、图像质量评估、主导颜色、综合描述和NSFW检测等十项业务关键任务。开发了BlockWeaver算法，用于高效可靠地比较无序、可变分组的OCR输出。构建了一个包含7500个多样化样本的新基准数据集。ViLD结合语义匹配（基于嵌入和LLM作为评判）、传统指标以及新颖的方法来衡量描述性输出的完整性和忠实性。使用ViLD框架对领先的开源VLM（Qwen、MIMO、InternVL）与专有基线进行了基准测试。

Result: ViLD框架提供了可操作的洞察。BlockWeaver算法在OCR输出比较方面实现了卓越的速度和可靠性。通过ViLD框架对主流开源VLM的基准测试，提供了首批行业驱动、任务导向的VLM能力评估之一，为它们在企业环境中的部署提供了可操作的见解。

Conclusion: ViLD框架成功弥合了VLM学术评估与企业部署需求之间的差距，通过行业驱动的评估，为企业VLM应用提供了明确的指导和可操作的见解。BlockWeaver算法在OCR输出比较方面展现了创新性与高效性。

Abstract: Open-source Vision-Language Models show immense promise for enterprise
applications, yet a critical disconnect exists between academic evaluation and
enterprise deployment requirements. Current benchmarks rely heavily on
multiple-choice questions and synthetic data, failing to capture the complexity
of real-world business applications like social media content analysis. This
paper introduces VLM-in-the-Wild (ViLD), a comprehensive framework to bridge
this gap by evaluating VLMs on operational enterprise requirements. We define
ten business-critical tasks: logo detection, OCR, object detection, human
presence and demographic analysis, human activity and appearance analysis,
scene detection, camera perspective and media quality assessment, dominant
colors, comprehensive description, and NSFW detection. To this framework, we
bring an innovative BlockWeaver Algorithm that solves the challenging problem
of comparing unordered, variably-grouped OCR outputs from VLMs without relying
on embeddings or LLMs, achieving remarkable speed and reliability. To
demonstrate efficacy of ViLD, we constructed a new benchmark dataset of 7,500
diverse samples, carefully stratified from a corpus of one million real-world
images and videos. ViLD provides actionable insights by combining semantic
matching (both embedding-based and LLM-as-a-judge approaches), traditional
metrics, and novel methods to measure the completeness and faithfulness of
descriptive outputs. By benchmarking leading open-source VLMs (Qwen, MIMO, and
InternVL) against a powerful proprietary baseline as per ViLD framework, we
provide one of the first industry-grounded, task-driven assessment of VLMs
capabilities, offering actionable insights for their deployment in enterprise
environments.

</details>


### [46] [The Protocol Genome A Self Supervised Learning Framework from DICOM Headers](https://arxiv.org/abs/2509.06995)
*Jimmy Joseph*

Main category: cs.CV

TL;DR: 本文提出了Protocol Genome，一个利用DICOM头信息进行自监督学习的系统，显著提升了医学图像分析网络的泛化性、校准性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 临床影像的DICOM协议选择（如扫描仪参数、序列）会影响图像质量，产生潜在混杂因素，从而阻碍仅基于图像的网络在不同站点间的泛化能力。

Method: 引入Protocol Genome系统，将去识别化的DICOM头字段进行令牌化嵌入，并结合图像特征。系统采用三种学习范式：1) 协议-图像对比学习，2) 掩码协议预测，和 3) 协议-协议转换。该方法在包含126万份研究数据上进行了验证。

Result: Protocol Genome在外部验证集上实现了0.901的AUROC（基线0.847）和0.036的ECE（基线0.058），并显著提升了跨模态和厂商的校准性与鲁棒性。相对于现有SSL基线，AUROC在不同任务上提高了0.041至0.058，校准性提升了25-37%。

Conclusion: 该技术有效解决了协议差异导致的泛化问题，减少了协议边界处的假阳性，具有临床实用性，并可无缝集成到PACS系统中，从而提升医学图像诊断网络的可靠性。

Abstract: In this paper, we introduce the Protocol Genome, a self-supervised learning
system that learns correlations from DICOM headers and achieves AUROC 0.901 (vs
0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation.
Our method also improves calibration and robustness across modalities (CT, MRI,
CXR) and vendors. Clinical imaging is funneled through PACS/DICOM, where
procedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slice
thickness) have consequences for contrast, noise, and artifact. These latent
confounders impede the generalization of image-only networks across sites. We
consider structured DICOM headers as a label and learn protocol-aware but
clinically robust image representations. Protocol Genome obtains tokenized
embeddings of de-identified header fields and models them along with image
features using: (1) protocol-image contrastive learning, (2) masked protocol
prediction, and (3) protocol-protocol translation. With 1.26M studies (7 health
systems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CT
triage for PE, (B) brain MRI glioma grading, and (C) chest radiograph
cardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as well
as ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041:
cardiomegaly) is associated with higher external AUROC; 25-37% calibration
improvements are obtained (p < 0.01, DeLong tests). While the gains may be
task-dependent, they are preserved with 10-20% of labeled data. From a clinical
point of view, the technique reduces false positives at protocol borders and is
applicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish a
model card and deployment guide, complete with both de-identification and bias
audits.

</details>


### [47] [Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems](https://arxiv.org/abs/2509.06996)
*Jie Zhang,Ting Xu,Gelei Deng,Runyi Hu,Han Qiu,Tianwei Zhang,Qing Guo,Ivor Tsang*

Main category: cs.CV

TL;DR: 人类对残缺文字识别具有鲁棒性，但先进的视觉语言模型（VLMs）在此方面表现欠佳，揭示了其对组合先验知识的结构性限制。


<details>
  <summary>Details</summary>
Motivation: 研究先进视觉语言模型（VLMs）是否像人类一样，能够识别被碎片化、融合或部分遮挡的字符，从而展现出对文字的鲁棒性。

Method: 构建了两个受心理物理学启发的基准测试，分别针对中文表意文字和英文字母文字。通过剪接、重组和叠加字符，创建了对模型“可见但不可读”但对人类“可读”的刺激。

Result: 尽管在清晰文本上性能强大，当代VLMs在这些扰动下表现严重下降，频繁产生不相关或不连贯的输出。这表明模型过度依赖通用视觉不变性，而未能充分利用鲁棒文字识别所需的组合先验知识。

Conclusion: 研究结果促使开发新的模型架构和训练策略，以编码跨脚本的符号分割、组合和绑定能力，并为在教育、无障碍、文化遗产和安全等领域部署多模态系统指明了具体的挑战。

Abstract: Writing is a universal cultural technology that reuses vision for symbolic
communication. Humans display striking resilience: we readily recognize words
even when characters are fragmented, fused, or partially occluded. This paper
investigates whether advanced vision language models (VLMs) share this
resilience. We construct two psychophysics inspired benchmarks across distinct
writing systems, Chinese logographs and English alphabetic words, by splicing,
recombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli
for models while remaining legible to humans. Despite strong performance on
clean text, contemporary VLMs show a severe drop under these perturbations,
frequently producing unrelated or incoherent outputs. The pattern suggests a
structural limitation: models heavily leverage generic visual invariances but
under rely on compositional priors needed for robust literacy. We release
stimuli generation code, prompts, and evaluation protocols to facilitate
transparent replication and follow up work. Our findings motivate architectures
and training strategies that encode symbol segmentation, composition, and
binding across scripts, and they delineate concrete challenges for deploying
multimodal systems in education, accessibility, cultural heritage, and
security.

</details>


### [48] [K-Syn: K-space Data Synthesis in Ultra Low-data Regimes](https://arxiv.org/abs/2509.06997)
*Guan Yu,Zhang Jianhua,Liang Dong,Liu Qiegen*

Main category: cs.CV

TL;DR: 针对动态CMR重建中k空间数据稀缺问题，本文提出一种在频域进行特征级学习并结合时序融合策略的生成方法，能在超低数据量下稳定且丰富地合成k空间数据，表现出强大的生成能力。


<details>
  <summary>Details</summary>
Motivation: 由于心脏磁共振（CMR）成像固有的动态性和复杂性，实践中难以获得高质量和多样化的k空间数据，这阻碍了动态心脏MRI的鲁棒重建。

Method: 该方法直接在频域进行特征级学习，利用傅里叶变换的全局表示能力，将频域视为自然的全局特征空间。与传统图像域像素级卷积不同，本方法专注于频域的特征级建模。同时，采用时序融合策略作为生成指导来合成k空间数据，通过多重融合策略整合跨时间帧的k空间数据，以引导和优化生成轨迹。

Result: 实验结果表明，所提出的方法在低数据量条件下具有强大的生成能力。

Conclusion: 该方法具有缓解动态MRI重建中数据稀缺问题的实际潜力。

Abstract: Owing to the inherently dynamic and complex characteristics of cardiac
magnetic resonance (CMR) imaging, high-quality and diverse k-space data are
rarely available in practice, which in turn hampers robust reconstruction of
dynamic cardiac MRI. To address this challenge, we perform feature-level
learning directly in the frequency domain and employ a temporal-fusion strategy
as the generative guidance to synthesize k-space data. Specifically, leveraging
the global representation capacity of the Fourier transform, the frequency
domain can be considered a natural global feature space. Therefore, unlike
traditional methods that use pixel-level convolution for feature learning and
modeling in the image domain, this letter focuses on feature-level modeling in
the frequency domain, enabling stable and rich generation even with ultra
low-data regimes. Moreover, leveraging the advantages of feature-level modeling
in the frequency domain, we integrate k-space data across time frames with
multiple fusion strategies to steer and further optimize the generative
trajectory. Experimental results demonstrate that the proposed method possesses
strong generative ability in low-data regimes, indicating practical potential
to alleviate data scarcity in dynamic MRI reconstruction.

</details>


### [49] [Not All Splits Are Equal: Rethinking Attribute Generalization Across Unrelated Categories](https://arxiv.org/abs/2509.06998)
*Liviu Nicolae Fircă,Antonio Bărbălau,Dan Oneata,Elena Burceanu*

Main category: cs.CV

TL;DR: 本文首次评估模型在语义和感知差异大的类别间泛化属性知识的能力。结果显示，随着训练和测试集关联度降低，模型性能急剧下降，表明对分割设计敏感。聚类策略提供最佳权衡，揭示了当前表示的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有属性预测研究多集中于分类学或视觉相似的领域，模型能否将属性知识抽象并泛化到概念上遥远的类别尚不明确。因此，本研究旨在首次明确评估模型在此类条件下的属性预测鲁棒性。

Method: 本研究引入了新的训练-测试集分割策略，以逐步降低训练与测试类别之间的相关性。这些策略包括：基于LLM的语义分组、嵌入相似度阈值法、基于嵌入的聚类以及利用真实标签的超类别划分。

Result: 结果显示，随着训练和测试类别相关性的降低，模型性能急剧下降，表明对分割设计具有高度敏感性。在评估的方法中，基于嵌入的聚类策略提供了最有效的权衡，既减少了隐藏相关性又保留了可学习性。

Conclusion: 这些发现为当前表示在跨不同类别属性泛化方面的局限性提供了新见解，并为未来属性推理基准的构建提供了指导。

Abstract: Can models generalize attribute knowledge across semantically and
perceptually dissimilar categories? While prior work has addressed attribute
prediction within narrow taxonomic or visually similar domains, it remains
unclear whether current models can abstract attributes and apply them to
conceptually distant categories. This work presents the first explicit
evaluation for the robustness of the attribute prediction task under such
conditions, testing whether models can correctly infer shared attributes
between unrelated object types: e.g., identifying that the attribute "has four
legs" is common to both "dogs" and "chairs". To enable this evaluation, we
introduce train-test split strategies that progressively reduce correlation
between training and test sets, based on: LLM-driven semantic grouping,
embedding similarity thresholding, embedding-based clustering, and
supercategory-based partitioning using ground-truth labels. Results show a
sharp drop in performance as the correlation between training and test
categories decreases, indicating strong sensitivity to split design. Among the
evaluated methods, clustering yields the most effective trade-off, reducing
hidden correlations while preserving learnability. These findings offer new
insights into the limitations of current representations and inform future
benchmark construction for attribute reasoning.

</details>


### [50] [Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models](https://arxiv.org/abs/2509.07010)
*Ahmed R. Sadik,Mariusz Bujny*

Main category: cs.CV

TL;DR: 本文提出一个包含人工参与的量化评估框架，用于评估LLM生成的3D模型，并引入一套全面的相似度和复杂度指标。研究发现，语义丰富的输入（特别是代码级提示）可显著提高模型生成精度，且量化评估方法能加速模型优化。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在解释多模态输入并生成复杂3D形状方面能力日益增强，但评估其几何和结构保真度的稳健方法仍不完善，限制了CAD设计、逆向工程和快速原型等应用。

Method: 本研究提出了一个包含人工参与的量化评估框架，用于评估大型语言模型（LLM）生成的3D模型。该框架引入了一套全面的相似度和复杂度指标，包括体积精度、表面对齐、尺寸保真度和拓扑复杂性，以对照真实CAD参考对生成模型进行基准测试。以L型支架为案例研究，系统比较了LLM在四种输入模态（2D正交视图、等距草图、几何结构树和基于代码的修正提示）下的性能。

Result: 研究结果表明，随着语义丰富度的增加，模型生成保真度显著提高，其中代码级提示在所有评估指标上均实现了完美重建。此外，本研究提出的量化评估方法与仅依赖视觉检查和人类直觉的传统定性方法相比，能显著加快模型收敛至真实CAD模型。

Conclusion: 本工作不仅加深了对AI辅助形状合成的理解，还提供了一种可扩展的方法论，用于验证和改进各种CAD应用中的生成模型，从而支持CAD设计的民主化、旧有设计的逆向工程和快速原型制作等应用。

Abstract: Large Language Models are increasingly capable of interpreting multimodal
inputs to generate complex 3D shapes, yet robust methods to evaluate geometric
and structural fidelity remain underdeveloped. This paper introduces a human in
the loop framework for the quantitative evaluation of LLM generated 3D models,
supporting applications such as democratization of CAD design, reverse
engineering of legacy designs, and rapid prototyping. We propose a
comprehensive suite of similarity and complexity metrics, including volumetric
accuracy, surface alignment, dimensional fidelity, and topological intricacy,
to benchmark generated models against ground truth CAD references. Using an L
bracket component as a case study, we systematically compare LLM performance
across four input modalities: 2D orthographic views, isometric sketches,
geometric structure trees, and code based correction prompts. Our findings
demonstrate improved generation fidelity with increased semantic richness, with
code level prompts achieving perfect reconstruction across all metrics. A key
contribution of this work is demonstrating that our proposed quantitative
evaluation approach enables significantly faster convergence toward the ground
truth, especially compared to traditional qualitative methods based solely on
visual inspection and human intuition. This work not only advances the
understanding of AI assisted shape synthesis but also provides a scalable
methodology to validate and refine generative models for diverse CAD
applications.

</details>


### [51] [MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning](https://arxiv.org/abs/2509.07021)
*Jiarui Chen,Yikeng Chen,Yingshuang Zou,Ye Huang,Peng Wang,Yuan Liu,Yujing Sun,Wenping Wang*

Main category: cs.CV

TL;DR: MEGS²是一个内存高效的3D Gaussian Splatting框架，通过联合优化基元数量和每个基元的参数，显著减少了静态和渲染VRAM，同时保持了渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) 因其高内存消耗而严重限制了在边缘设备上的应用。现有的3DGS压缩方法主要关注存储压缩，未能解决渲染内存的关键瓶颈。

Method: 该研究引入MEGS²框架，通过联合优化总基元数量和每个基元的参数来解决内存消耗问题。具体方法包括：用轻量级的任意定向球面高斯瓣替换内存密集型的球面谐波作为颜色表示；提出一个统一的软剪枝框架，将基元数量和瓣数量的剪枝建模为一个单一的约束优化问题。

Result: 实验结果显示，MEGS² 相较于现有方法，实现了50%的静态VRAM减少和40%的渲染VRAM减少，同时保持了可比的渲染质量。

Conclusion: MEGS² 成功地解决了3DGS的高内存消耗问题，尤其是在渲染内存方面，使其更适用于资源受限的边缘设备，并在显著降低内存占用的同时维持了高质量的渲染效果。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis
technique, but its high memory consumption severely limits its applicability on
edge devices. A growing number of 3DGS compression methods have been proposed
to make 3DGS more efficient, yet most only focus on storage compression and
fail to address the critical bottleneck of rendering memory. To address this
problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that
tackles this challenge by jointly optimizing two key factors: the total
primitive number and the parameters per primitive, achieving unprecedented
memory compression. Specifically, we replace the memory-intensive spherical
harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our
color representations. More importantly, we propose a unified soft pruning
framework that models primitive-number and lobe-number pruning as a single
constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a
50% static VRAM reduction and a 40% rendering VRAM reduction compared to
existing methods, while maintaining comparable rendering quality.

</details>


### [52] [Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models](https://arxiv.org/abs/2509.07027)
*Jisung Hwang,Jaihoon Kim,Minhyuk Sung*

Main category: cs.CV

TL;DR: 本文提出一种新颖的正则化损失，通过结合空间域的矩正则化和谱域的功率谱正则化，强制样本服从标准高斯分布，从而促进文本到图像模型潜在空间中的优化任务。实验证明其在生成模型中优于现有方法，能有效防止奖励欺骗并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 为了促进涉及文本到图像模型潜在空间优化的下游任务，需要一种有效的方法鼓励样本与标准高斯分布对齐。现有的高斯性正则化方法存在局限性，例如某些方法计算复杂度较高。

Method: 提出一种新的正则化损失，强制样本服从标准高斯分布。该方法将高维样本的元素视为一维标准高斯变量，并定义了一个复合损失，结合了空间域的基于矩的正则化和谱域的基于功率谱的正则化。由于矩和功率谱分布的期望值是解析已知的，该损失促进符合这些属性。为确保置换不变性，损失应用于随机置换的输入。作者还指出，现有基于高斯性的正则化（如特定阶的矩损失或先前的协方差匹配损失）可纳入其统一框架，且提出的谱损失在效率上优于空间域的协方差匹配损失。

Result: 该正则化损失在生成建模中用于文本到图像模型的测试时间奖励对齐（特别是增强美学和文本对齐）方面表现出色。它优于先前的基于高斯性的正则化方法，有效防止了奖励欺骗，并加速了收敛。

Conclusion: 所提出的标准高斯性正则化损失在文本到图像模型的潜在空间优化中表现出优越性，能够有效提高性能（如美学和文本对齐），防止奖励欺骗，并加速收敛，同时在效率上优于现有方法。

Abstract: We propose a novel regularization loss that enforces standard Gaussianity,
encouraging samples to align with a standard Gaussian distribution. This
facilitates a range of downstream tasks involving optimization in the latent
space of text-to-image models. We treat elements of a high-dimensional sample
as one-dimensional standard Gaussian variables and define a composite loss that
combines moment-based regularization in the spatial domain with power
spectrum-based regularization in the spectral domain. Since the expected values
of moments and power spectrum distributions are analytically known, the loss
promotes conformity to these properties. To ensure permutation invariance, the
losses are applied to randomly permuted inputs. Notably, existing
Gaussianity-based regularizations fall within our unified framework: some
correspond to moment losses of specific orders, while the previous
covariance-matching loss is equivalent to our spectral loss but incurs higher
time complexity due to its spatial-domain computation. We showcase the
application of our regularization in generative modeling for test-time reward
alignment with a text-to-image model, specifically to enhance aesthetics and
text alignment. Our regularization outperforms previous Gaussianity
regularization, effectively prevents reward hacking and accelerates
convergence.

</details>


### [53] [SAM$^{*}$: Task-Adaptive SAM with Physics-Guided Rewards](https://arxiv.org/abs/2509.07047)
*Kamyar Barakati,Utkarsh Pratiush,Sheryl L. Sanchez,Aditya Raghavan,Delia J. Milliron,Mahshid Ahmadi,Philip D. Rack,Sergei V. Kalinin*

Main category: cs.CV

TL;DR: 本文提出一种基于奖励函数优化方法，用于微调基础模型（如SAM），以解决其在显微镜图像实时流数据分割中参数手动优化困难的问题，从而增强模型性能和适应性，实现实时分割。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像分割对复杂视觉数据分析至关重要。尽管基础模型具有广泛适用性，但其大量不透明的调优参数需要大量手动优化，这限制了它们在实时流数据分析中的可用性。

Method: 引入了一种基于奖励函数优化的方法来微调基础模型。该方法以Meta的SAM（Segment Anything Model）框架为例进行说明。奖励函数可以根据成像系统的物理特性（包括颗粒大小分布、几何形状和其他标准）进行构建。

Result: 通过整合奖励驱动的优化框架，SAM的适应性和性能得到提升，产生了一个优化变体SAM$^{*}$。该变体能更好地适应各种分割任务的要求，尤其允许进行实时流数据分割。

Conclusion: 所提出的基于奖励函数的优化方法有效增强了基础模型（如SAM）在显微镜成像中的适应性和性能，实现了精确分割，并特别支持实时流数据分析，这对于分析细胞结构、材料界面和纳米级特征至关重要。

Abstract: Image segmentation is a critical task in microscopy, essential for accurately
analyzing and interpreting complex visual data. This task can be performed
using custom models trained on domain-specific datasets, transfer learning from
pre-trained models, or foundational models that offer broad applicability.
However, foundational models often present a considerable number of
non-transparent tuning parameters that require extensive manual optimization,
limiting their usability for real-time streaming data analysis. Here, we
introduce a reward function-based optimization to fine-tune foundational models
and illustrate this approach for SAM (Segment Anything Model) framework by
Meta. The reward functions can be constructed to represent the physics of the
imaged system, including particle size distributions, geometries, and other
criteria. By integrating a reward-driven optimization framework, we enhance
SAM's adaptability and performance, leading to an optimized variant, SAM$^{*}$,
that better aligns with the requirements of diverse segmentation tasks and
particularly allows for real-time streaming data segmentation. We demonstrate
the effectiveness of this approach in microscopy imaging, where precise
segmentation is crucial for analyzing cellular structures, material interfaces,
and nanoscale features.

</details>


### [54] [Enhancing Classification of Streaming Data with Image Distillation](https://arxiv.org/abs/2509.07049)
*Rwad Khatib,Yehudit Aperstein*

Main category: cs.CV

TL;DR: 本研究提出一种基于数据蒸馏的新方法（DBC），在内存和计算资源有限的环境下，实现了流式图像数据分类的卓越性能，准确率达73.1%，超越了传统算法。


<details>
  <summary>Details</summary>
Motivation: 在内存和计算资源受限的环境中，高效地分类流式数据，特别是提高流式图像数据分类的精度。

Method: 该研究应用数据蒸馏作为一种创新方法来提升流式图像数据分类的精度。通过从数据流中蒸馏关键特征，旨在最小化计算需求同时保留重要信息。研究将此方法（Distillation Based Classification, DBC）与传统的Hoeffding树、自适应随机森林（通过嵌入适应图像数据）以及基于水库采样的分类（RBC）技术进行了比较。

Result: Distillation Based Classification (DBC) 方法表现出卓越的性能，实现了73.1%的准确率，超越了传统的Hoeffding树和自适应随机森林，以及基于水库采样的分类（RBC）技术。

Conclusion: 该研究在流式数据分类领域取得了显著进展，展示了所提出方法在处理复杂数据流方面的有效性，并在准确性和效率方面树立了新标准。

Abstract: This study tackles the challenge of efficiently classifying streaming data in
envi-ronments with limited memory and computational resources. It delves into
the application of data distillation as an innovative approach to improve the
precision of streaming image data classification. By focusing on distilling
essential features from data streams, our method aims to minimize computational
demands while preserving crucial information for accurate classification. Our
investigation com-pares this approach against traditional algorithms like
Hoeffding Trees and Adap-tive Random Forest, adapted through embeddings for
image data. The Distillation Based Classification (DBC) demonstrated superior
performance, achieving a 73.1% accuracy rate, surpassing both traditional
methods and Reservoir Sam-pling Based Classification (RBC) technique. This
marks a significant advance-ment in streaming data classification, showcasing
the effectiveness of our method in processing complex data streams and setting
a new standard for accuracy and efficiency.

</details>


### [55] [Automated Evaluation of Gender Bias Across 13 Large Multimodal Models](https://arxiv.org/abs/2509.07050)
*Juan Manuel Contreras*

Main category: cs.CV

TL;DR: 本研究引入Aymara图像公平性评估基准，对13个大型多模态模型（LMMs）的文本到图像生成中的性别偏见进行大规模跨模型分析。结果显示，LMMs不仅再现而且放大职业性别刻板印象，存在显著的默认男性偏见，且偏见程度因模型而异，表明设计选择是关键。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在文本到图像生成方面取得了巨大进步，但其训练数据可能包含有害的社会偏见。现有研究虽已识别出性别偏见，但方法学局限性阻碍了大规模、可比较的跨模型分析。

Method: 引入Aymara图像公平性评估基准来评估AI生成图像中的社会偏见。测试了13个商用LMMs，使用75个程序生成的中性提示，生成从事刻板男性、刻板女性及非刻板职业的人物图像。然后利用一个经过验证的“LLM即法官”系统，对965张图像的性别代表性进行评分。

Result: 1) LMMs系统性地再现并放大了职业性别刻板印象，例如在男性刻板职业图像中生成93.0%的男性，而在女性刻板职业图像中仅生成22.5%的男性。2) 模型表现出强烈的默认男性偏见，在非刻板职业中生成男性占68.3%。3) 偏见程度在不同模型间差异显著，男性总代表性从46.7%到73.3%不等。表现最佳的模型去除了性别刻板印象并接近性别均等，暗示高偏见并非必然结果，而是设计选择的产物。

Conclusion: 本工作提供了迄今为止最全面的性别偏见跨模型基准测试，并强调了标准化、自动化评估工具对于促进AI开发中的问责制和公平性的必要性。

Abstract: Large multimodal models (LMMs) have revolutionized text-to-image generation,
but they risk perpetuating the harmful social biases in their training data.
Prior work has identified gender bias in these models, but methodological
limitations prevented large-scale, comparable, cross-model analysis. To address
this gap, we introduce the Aymara Image Fairness Evaluation, a benchmark for
assessing social bias in AI-generated images. We test 13 commercially available
LMMs using 75 procedurally-generated, gender-neutral prompts to generate people
in stereotypically-male, stereotypically-female, and non-stereotypical
professions. We then use a validated LLM-as-a-judge system to score the 965
resulting images for gender representation. Our results reveal (p < .001 for
all): 1) LMMs systematically not only reproduce but actually amplify
occupational gender stereotypes relative to real-world labor data, generating
men in 93.0% of images for male-stereotyped professions but only 22.5% for
female-stereotyped professions; 2) Models exhibit a strong default-male bias,
generating men in 68.3% of the time for non-stereotyped professions; and 3) The
extent of bias varies dramatically across models, with overall male
representation ranging from 46.7% to 73.3%. Notably, the top-performing model
de-amplified gender stereotypes and approached gender parity, achieving the
highest fairness scores. This variation suggests high bias is not an inevitable
outcome but a consequence of design choices. Our work provides the most
comprehensive cross-model benchmark of gender bias to date and underscores the
necessity of standardized, automated evaluation tools for promoting
accountability and fairness in AI development.

</details>


### [56] [Faster VGGT with Block-Sparse Global Attention](https://arxiv.org/abs/2509.07120)
*Chung-Shien Brian Wang,Christian Schmidt,Jens Piekenbrinck,Bastian Leibe*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Efficient and accurate feed-forward multi-view reconstruction has long been
an important task in computer vision. Recent transformer-based models like VGGT
and $\pi^3$ have achieved impressive results with simple architectures, yet
they face an inherent runtime bottleneck, due to the quadratic complexity of
the global attention layers, that limits the scalability to large image sets.
In this paper, we empirically analyze the global attention matrix of these
models and observe that probability mass concentrates on a small subset of
patch-patch interactions that correspond to cross-view geometric matches.
Motivated by the structured attention and inspired by recent advancement in
large language models, we propose a replacement for the dense global attention
operation based on highly optimized block-sparse kernels, yielding up to
$4\times$ faster inference with comparable task performance. Our retrofit
requires no retraining of the backbone, extends to both VGGT and $\pi^3$, and
supports large image collections. Evaluations on a comprehensive suite of
multi-view benchmarks demonstrate the effectiveness of our approach.

</details>


### [57] [Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry](https://arxiv.org/abs/2509.07130)
*Soruya Saha,Md Nurul Absur,Saptarshi Debroy*

Main category: cs.CV

TL;DR: 本文提出了一种无监督、无标签的检测和恢复机制，用于应对卸载到边缘服务器的视觉惯性里程计（VIO）中出现的姿态欺骗威胁，实验证明能显著减少轨迹和姿态误差。


<details>
  <summary>Details</summary>
Motivation: 将VIO卸载到边缘服务器会引入服务器端威胁，即细微的姿态欺骗可能累积成显著的漂移，并规避启发式检查，影响VR沉浸体验。

Method: 提出了一种无监督、无标签的检测和恢复机制。该模型在无攻击会话上进行训练，以学习运动的时间规律性，从而在运行时检测偏差并启动恢复过程以恢复姿态一致性。

Result: 在逼真的卸载VIO环境（使用ILLIXR测试台）中，针对多种欺骗强度进行的实验结果表明，与无防御基线相比，该方法在轨迹和姿态误差方面有显著减少。

Conclusion: 所提出的无监督检测和恢复机制能够有效检测并抵御边缘服务器上VIO的姿态欺骗攻击，显著提高了姿态和轨迹的准确性和一致性。

Abstract: Visual-Inertial Odometry (VIO) supports immersive Virtual Reality (VR) by
fusing camera and Inertial Measurement Unit (IMU) data for real-time pose.
However, current trend of offloading VIO to edge servers can lead server-side
threat surface where subtle pose spoofing can accumulate into substantial
drift, while evading heuristic checks. In this paper, we study this threat and
present an unsupervised, label-free detection and recovery mechanism. The
proposed model is trained on attack-free sessions to learn temporal
regularities of motion to detect runtime deviations and initiate recovery to
restore pose consistency. We evaluate the approach in a realistic offloaded-VIO
environment using ILLIXR testbed across multiple spoofing intensities.
Experimental results in terms of well-known performance metrics show
substantial reductions in trajectory and pose error compared to a no-defense
baseline.

</details>


### [58] [Realism to Deception: Investigating Deepfake Detectors Against Face Enhancement](https://arxiv.org/abs/2509.07178)
*Muhammad Saad Saeed,Ijaz Ul Haq,Khalid Malik*

Main category: cs.CV

TL;DR: 人脸增强技术，无论是传统方法还是基于GAN的方法，都会显著降低深度伪造检测器的准确性，可作为反取证工具，凸显了开发更稳健检测方法的必要性。


<details>
  <summary>Details</summary>
Motivation: 人脸增强技术虽能提升感知质量，但可能无意中扭曲生物特征，导致深度伪造检测器准确率大幅下降。本研究旨在验证这些技术是否能作为反取证手段，降低检测准确性。

Method: 系统评估了传统图像处理方法和基于GAN的先进人脸增强技术对深度伪造检测器鲁棒性的影响。分析了这些增强技术对朴素、空间和频率域检测方法的作用。并在FaceForensics++、DeepFakeDetection和CelebDF-v2数据集上进行实验。此外，还进行了对抗训练实验以评估模型鲁棒性。

Result: 实验表明，即使是基本的人脸增强滤镜也能显著降低检测准确率，攻击成功率（ASR）高达64.63%。而基于GAN的增强技术更能利用这些漏洞，ASR高达75.12%。

Conclusion: 人脸增强方法可以有效地充当反取证工具，强调了开发更具弹性和适应性的取证方法的必要性。

Abstract: Face enhancement techniques are widely used to enhance facial appearance.
However, they can inadvertently distort biometric features, leading to
significant decrease in the accuracy of deepfake detectors. This study
hypothesizes that these techniques, while improving perceptual quality, can
degrade the performance of deepfake detectors. To investigate this, we
systematically evaluate whether commonly used face enhancement methods can
serve an anti-forensic role by reducing detection accuracy. We use both
traditional image processing methods and advanced GAN-based enhancements to
evaluate the robustness of deepfake detectors. We provide a comprehensive
analysis of the effectiveness of these enhancement techniques, focusing on
their impact on Na\"ive, Spatial, and Frequency-based detection methods.
Furthermore, we conduct adversarial training experiments to assess whether
exposure to face enhancement transformations improves model robustness.
Experiments conducted on the FaceForensics++, DeepFakeDetection, and CelebDF-v2
datasets indicate that even basic enhancement filters can significantly reduce
detection accuracy achieving ASR up to 64.63\%. In contrast, GAN-based
techniques further exploit these vulnerabilities, achieving ASR up to 75.12\%.
Our results demonstrate that face enhancement methods can effectively function
as anti-forensic tools, emphasizing the need for more resilient and adaptive
forensic methods.

</details>


### [59] [Dimensionally Reduced Open-World Clustering: DROWCULA](https://arxiv.org/abs/2509.07184)
*Erencem Ozbey,Dimitrios I. Diochnos*

Main category: cs.CV

TL;DR: 本研究提出一种完全无监督方法，利用Vision Transformers和流形学习进行图像聚类和新类发现，在多个基准数据集上实现了新的最先进（SOTA）结果。


<details>
  <summary>Details</summary>
Motivation: 监督学习需要大量昂贵的人工标注数据，而现实世界应用中常出现未标注的新类别。现有开放世界背景下的工作主要集中在半监督方法，本研究旨在探索一种完全无监督的解决方案。

Method: 针对图像分类，本方法通过Vision Transformers生成向量嵌入并估计聚类数量。此外，结合流形学习技术，利用数据内在几何结构优化这些嵌入，以提升整体图像聚类性能。

Result: 在CIFAR-10、CIFAR-100、ImageNet-100和Tiny ImageNet数据集上的单模态聚类和新类发现任务中，无论聚类数量已知或未知，均取得了新的最先进（SOTA）结果。

Conclusion: 本研究成功提出了一种高效的完全无监督方法，有效解决了开放世界中的新类发现和图像聚类问题，其性能显著超越了现有技术水平。

Abstract: Working with annotated data is the cornerstone of supervised learning.
Nevertheless, providing labels to instances is a task that requires significant
human effort. Several critical real-world applications make things more
complicated because no matter how many labels may have been identified in a
task of interest, it could be the case that examples corresponding to novel
classes may appear in the future. Not unsurprisingly, prior work in this,
so-called, `open-world' context has focused a lot on semi-supervised
approaches.
  Focusing on image classification, somehow paradoxically, we propose a fully
unsupervised approach to the problem of determining the novel categories in a
particular dataset. Our approach relies on estimating the number of clusters
using Vision Transformers, which utilize attention mechanisms to generate
vector embeddings. Furthermore, we incorporate manifold learning techniques to
refine these embeddings by exploiting the intrinsic geometry of the data,
thereby enhancing the overall image clustering performance. Overall, we
establish new State-of-the-Art results on single-modal clustering and Novel
Class Discovery on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet. We do
so, both when the number of clusters is known or unknown ahead of time. The
code is available at: https://github.com/DROWCULA/DROWCULA.

</details>


### [60] [XBusNet: Text-Guided Breast Ultrasound Segmentation via Multimodal Vision-Language Learning](https://arxiv.org/abs/2509.07213)
*Raja Mallina,Bryar Shareef*

Main category: cs.CV

TL;DR: 一种双提示、双分支多模态模型XBusNet，通过结合全局语义和局部精度，实现了精确的乳腺超声（BUS）图像分割，尤其改善了小尺寸、低对比度病灶的分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺超声（BUS）分割方法难以处理边界模糊、散斑噪声的小尺寸或低对比度病灶。虽然文本提示可提供临床背景，但直接应用会产生粗糙响应，无法精细识别边缘。

Method: 提出XBusNet，一种新颖的双提示、双分支多模态模型。它包含一个基于CLIP Vision Transformer的全局路径（编码整体图像语义，条件为病灶大小和位置）和一个局部U-Net路径（强调精确边界，由描述形状、边缘和BI-RADS术语的提示调节）。提示自动从结构化元数据生成。模型在BLU数据集上通过五折交叉验证进行评估，主要指标为Dice和IoU，并进行尺寸分层分析和消融研究。

Result: XBusNet在BLU数据集上实现了最先进的性能，平均Dice系数为0.8765，IoU为0.8149，优于六个基线模型。尤其在小尺寸病灶上获得了最大提升，减少了遗漏区域和虚假激活。消融研究表明全局上下文、局部边界建模和基于提示的调制都做出了互补贡献。

Conclusion: 结合全局语义和局部精度的双提示、双分支多模态设计，能够生成准确的乳腺超声（BUS）分割掩模，并提高了对小尺寸、低对比度病灶的鲁棒性。

Abstract: Background: Precise breast ultrasound (BUS) segmentation supports reliable
measurement, quantitative analysis, and downstream classification, yet remains
difficult for small or low-contrast lesions with fuzzy margins and speckle
noise. Text prompts can add clinical context, but directly applying weakly
localized text-image cues (e.g., CAM/CLIP-derived signals) tends to produce
coarse, blob-like responses that smear boundaries unless additional mechanisms
recover fine edges. Methods: We propose XBusNet, a novel dual-prompt,
dual-branch multimodal model that combines image features with clinically
grounded text. A global pathway based on a CLIP Vision Transformer encodes
whole-image semantics conditioned on lesion size and location, while a local
U-Net pathway emphasizes precise boundaries and is modulated by prompts that
describe shape, margin, and Breast Imaging Reporting and Data System (BI-RADS)
terms. Prompts are assembled automatically from structured metadata, requiring
no manual clicks. We evaluate on the Breast Lesions USG (BLU) dataset using
five-fold cross-validation. Primary metrics are Dice and Intersection over
Union (IoU); we also conduct size-stratified analyses and ablations to assess
the roles of the global and local paths and the text-driven modulation.
Results: XBusNet achieves state-of-the-art performance on BLU, with mean Dice
of 0.8765 and IoU of 0.8149, outperforming six strong baselines. Small lesions
show the largest gains, with fewer missed regions and fewer spurious
activations. Ablation studies show complementary contributions of global
context, local boundary modeling, and prompt-based modulation. Conclusions: A
dual-prompt, dual-branch multimodal design that merges global semantics with
local precision yields accurate BUS segmentation masks and improves robustness
for small, low-contrast lesions.

</details>


### [61] [Breast Cancer Detection in Thermographic Images via Diffusion-Based Augmentation and Nonlinear Feature Fusion](https://arxiv.org/abs/2509.07277)
*Sepehr Salem,M. Moein Esfahani,Jingyu Liu,Vince Calhoun*

Main category: cs.CV

TL;DR: 本文提出一个利用扩散概率模型（DPM）进行数据增强，并融合深度学习与手工非线性特征的框架，用于乳腺癌热像图分类，取得了高准确率和敏感性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像领域面临数据稀缺的挑战，尤其在乳腺癌热像图分类中，限制了其性能。

Method: 该框架通过扩散概率模型（DPM）进行数据增强，以解决数据稀缺问题。它融合了来自预训练ResNet-50的深度特征，以及从U-Net分割的肿瘤区域中提取的手工非线性特征（如分形维数），最终使用XGBoost分类器对融合特征进行训练。

Result: DPM数据增强方法优于传统方法和ProGAN基线。该框架在乳腺癌分类上实现了98.0%的准确率和98.1%的敏感性。消融研究和统计测试证实DPM增强和非线性特征融合均为成功不可或缺且统计学上显著的组成部分。

Conclusion: 这项工作验证了先进生成模型与可解释特征之间协同作用的重要性，为创建高精度医疗诊断工具提供了有效途径。

Abstract: Data scarcity hinders deep learning for medical imaging. We propose a
framework for breast cancer classification in thermograms that addresses this
using a Diffusion Probabilistic Model (DPM) for data augmentation. Our
DPM-based augmentation is shown to be superior to both traditional methods and
a ProGAN baseline. The framework fuses deep features from a pre-trained
ResNet-50 with handcrafted nonlinear features (e.g., Fractal Dimension) derived
from U-Net segmented tumors. An XGBoost classifier trained on these fused
features achieves 98.0\% accuracy and 98.1\% sensitivity. Ablation studies and
statistical tests confirm that both the DPM augmentation and the nonlinear
feature fusion are critical, statistically significant components of this
success. This work validates the synergy between advanced generative models and
interpretable features for creating highly accurate medical diagnostic tools.

</details>


### [62] [Reconstruction Alignment Improves Unified Multimodal Models](https://arxiv.org/abs/2509.07295)
*Ji Xie,Trevor Darrell,Luke Zettlemoyer,XuDong Wang*

Main category: cs.CV

TL;DR: RecA是一种资源高效的后训练方法，它通过利用视觉理解编码器嵌入作为密集提示，对统一多模态模型（UMMs）进行自监督图像重建，以弥补传统文本描述的不足，显著提升了UMMs的图像生成和编辑性能。


<details>
  <summary>Details</summary>
Motivation: 传统统一多模态模型（UMMs）的训练依赖于图像-文本对，但文本描述往往稀疏，无法捕捉细粒度的视觉细节，即使是冗长的描述也如此，这限制了模型更精细的视觉理解和生成能力。

Method: 本文提出了重建对齐（RecA）方法，这是一种资源高效的后训练策略。它不依赖额外标注，而是利用UMM自身的视觉理解编码器嵌入作为密集的“文本提示”。RecA通过让UMM在这些嵌入的条件下，以自监督重建损失来优化模型以重建输入图像，从而重新对齐理解与生成模块。该方法适用于自回归、掩码自回归和扩散模型等多种UMM架构。

Result: RecA在自回归、掩码自回归和扩散模型等多种UMM架构上，持续提升了生成和编辑的保真度。仅需27个GPU小时的后训练，RecA便显著提升了图像生成性能：GenEval从0.73增至0.90，DPGBench从80.93增至88.15。同时，编辑基准也得到提升：ImgEdit从3.38增至3.75，GEdit从6.94增至7.25。RecA的表现超越了许多更大的开源模型。

Conclusion: RecA被确立为一种高效且通用的UMM后训练对齐策略。它通过解决传统训练中文本描述的稀疏性问题，显著提升了UMMs的生成和编辑能力，并且具有广泛的适用性，可应用于多种UMM架构。

Abstract: Unified multimodal models (UMMs) unify visual understanding and generation
within a single architecture. However, conventional training relies on
image-text pairs (or sequences) whose captions are typically sparse and miss
fine-grained visual details--even when they use hundreds of words to describe a
simple image. We introduce Reconstruction Alignment (RecA), a
resource-efficient post-training method that leverages visual understanding
encoder embeddings as dense "text prompts," providing rich supervision without
captions. Concretely, RecA conditions a UMM on its own visual understanding
embeddings and optimizes it to reconstruct the input image with a
self-supervised reconstruction loss, thereby realigning understanding and
generation. Despite its simplicity, RecA is broadly applicable: across
autoregressive, masked-autoregressive, and diffusion-based UMMs, it
consistently improves generation and editing fidelity. With only 27 GPU-hours,
post-training with RecA substantially improves image generation performance on
GenEval (0.73$\rightarrow$0.90) and DPGBench (80.93$\rightarrow$88.15), while
also boosting editing benchmarks (ImgEdit 3.38$\rightarrow$3.75, GEdit
6.94$\rightarrow$7.25). Notably, RecA surpasses much larger open-source models
and applies broadly across diverse UMM architectures, establishing it as an
efficient and general post-training alignment strategy for UMMs

</details>


### [63] [DEPF: A UAV Multispectral Object Detector with Dual-Domain Enhancement and Priority-Guided Mamba Fusion](https://arxiv.org/abs/2509.07327)
*Shucong Li,Zhenyu Liu,Zijie Hong,Zhiheng Zhou,Xianghai Cao*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba的无人机多光谱目标检测器DEPF，通过双域增强和优先级引导融合，有效解决了低光照、小目标建模困难和Transformer计算复杂度高的问题。


<details>
  <summary>Details</summary>
Motivation: 多光谱遥感目标检测面临三大挑战：1) 低光照图像降低了多模态融合的互补性；2) 融合阶段的冗余信息容易干扰局部小目标建模；3) Transformer类方法因二次方计算复杂度难以应用于无人机平台。

Method: 受Mamba线性复杂度的启发，提出了一种无人机多光谱目标检测器DEPF。该方法包括：1) 双域增强模块（DDE），用于增强低光照遥感图像，由交叉尺度小波Mamba（CSWM）提升全局亮度，以及傅里叶细节恢复块（FDR）恢复纹理细节构成；2) 优先级引导Mamba融合模块（PGMF），用于增强局部目标建模并减少冗余信息，通过根据模态差异获得的优先级分数对局部目标特征进行优先级扫描。

Result: 在DroneVehicle和VEDAI数据集上的实验结果表明，DEPF在目标检测方面表现出色，优于现有的最先进方法。

Conclusion: 所提出的DEPF方法有效解决了无人机多光谱目标检测中低光照、小目标建模和计算效率的挑战，并在主流数据集上取得了优秀的检测性能。

Abstract: Multispectral remote sensing object detection is one of the important
application of unmanned aerial vehicle (UAV). However, it faces three
challenges. Firstly, the low-light remote sensing images reduce the
complementarity during multi-modality fusion. Secondly, the local small target
modeling is interfered with redundant information in the fusion stage easily.
Thirdly, due to the quadratic computational complexity, it is hard to apply the
transformer-based methods on the UAV platform. To address these limitations,
motivated by Mamba with linear complexity, a UAV multispectral object detector
with dual-domain enhancement and priority-guided mamba fusion (DEPF) is
proposed. Firstly, to enhance low-light remote sensing images, Dual-Domain
Enhancement Module (DDE) is designed, which contains Cross-Scale Wavelet Mamba
(CSWM) and Fourier Details Recovery block (FDR). CSWM applies cross-scale mamba
scanning for the low-frequency components to enhance the global brightness of
images, while FDR constructs spectrum recovery network to enhance the frequency
spectra features for recovering the texture-details. Secondly, to enhance local
target modeling and reduce the impact of redundant information during fusion,
Priority-Guided Mamba Fusion Module (PGMF) is designed. PGMF introduces the
concept of priority scanning, which starts from local targets features
according to the priority scores obtained from modality difference. Experiments
on DroneVehicle dataset and VEDAI dataset reports that, DEPF performs well on
object detection, comparing with state-of-the-art methods. Our code is
available in the supplementary material.

</details>


### [64] [G3CN: Gaussian Topology Refinement Gated Graph Convolutional Network for Skeleton-Based Action Recognition](https://arxiv.org/abs/2509.07335)
*Haiqing Ren,Zhongkai Luo,Heng Fan,Xiaohui Yuan,Guanchen Wang,Libo Zhang*

Main category: cs.CV

TL;DR: GCNs在骨骼动作识别中区分模糊动作存在局限。本文提出G$^{3}$CN，通过高斯滤波细化拓扑图并引入GRU，有效提升了模糊动作的识别性能。


<details>
  <summary>Details</summary>
Motivation: 尽管图卷积网络（GCNs）在骨骼动作识别方面表现出色，但它们在有效区分模糊动作时常遇到困难，这暴露出学习到的拓扑和空间特征表示的局限性。

Method: 本文提出一种新方法——高斯拓扑细化门控图卷积（G$^{3}$CN）。该方法结合高斯滤波器以细化骨骼拓扑图，改善模糊动作的表示；同时，将门控循环单元（GRUs）整合到GCN框架中，以增强骨骼点之间的信息传播。该方法对多种GCN骨干网络具有良好的泛化性。

Result: 在NTU RGB+D、NTU RGB+D 120和NW-UCLA等基准数据集上进行的广泛实验表明，G$^{3}$CN有效提高了动作识别性能，尤其在处理模糊样本时效果显著。

Conclusion: G$^{3}$CN通过优化拓扑表示和增强信息传播，成功解决了GCN在骨骼动作识别中区分模糊动作的挑战，显著提升了识别准确率。

Abstract: Graph Convolutional Networks (GCNs) have proven to be highly effective for
skeleton-based action recognition, primarily due to their ability to leverage
graph topology for feature aggregation, a key factor in extracting meaningful
representations. However, despite their success, GCNs often struggle to
effectively distinguish between ambiguous actions, revealing limitations in the
representation of learned topological and spatial features. To address this
challenge, we propose a novel approach, Gaussian Topology Refinement Gated
Graph Convolution (G$^{3}$CN), to address the challenge of distinguishing
ambiguous actions in skeleton-based action recognition. G$^{3}$CN incorporates
a Gaussian filter to refine the skeleton topology graph, improving the
representation of ambiguous actions. Additionally, Gated Recurrent Units (GRUs)
are integrated into the GCN framework to enhance information propagation
between skeleton points. Our method shows strong generalization across various
GCN backbones. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA
benchmarks demonstrate that G$^{3}$CN effectively improves action recognition,
particularly for ambiguous samples.

</details>


### [65] [Parse Graph-Based Visual-Language Interaction for Human Pose Estimation](https://arxiv.org/abs/2509.07385)
*Shibang Liu,Xuemei Xie,Guangming Shi*

Main category: cs.CV

TL;DR: 提出PGVL（基于解析图的视觉-语言交互）框架及核心引导模块GM，通过分层解析图和多模态融合，有效提升人体姿态估计，尤其在遮挡场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有解析图人体姿态估计方法多为单模态，忽略多模态融合潜力；语言能提供丰富的先验信息，但现有视觉-语言融合（全局特征集成）会削弱遮挡区域响应并导致对齐与定位失败，亟需解决遮挡场景下的融合问题。

Method: 提出Parse Graph-based Visual-Language interaction (PGVL) 框架，核心创新是Guided Module (GM)。PGVL通过低层节点关注局部特征以维持遮挡区域响应，高层节点集成全局特征推断遮挡部分。GM使高语义节点引导低语义节点的特征更新，确保有效融合。该方法包含自顶向下分解构建模态特定解析图，以及通过GM提纯的递归双向交叉注意力自底向上组合，并基于PGVL设计了特定网络。

Result: PGVL及其设计的网络在主要姿态估计数据集上得到了验证。

Conclusion: PGVL通过创新的解析图分层结构和引导模块，有效整合视觉与语言信息，解决了多模态融合在遮挡场景中的局限，提升了人体姿态估计的性能。

Abstract: Parse graphs boost human pose estimation (HPE) by integrating context and
hierarchies, yet prior work mostly focuses on single modality modeling,
ignoring the potential of multimodal fusion. Notably, language offers rich HPE
priors like spatial relations for occluded scenes, but existing visual-language
fusion via global feature integration weakens occluded region responses and
causes alignment and location failures. To address this issue, we propose Parse
Graph-based Visual-Language interaction (PGVL) with a core novel Guided Module
(GM). In PGVL, low-level nodes focus on local features, maximizing the
maintenance of responses in occluded areas and high-level nodes integrate
global features to infer occluded or invisible parts. GM enables high semantic
nodes to guide the feature update of low semantic nodes that have undergone
cross attention. It ensuring effective fusion of diverse information. PGVL
includes top-down decomposition and bottom-up composition. In the first stage,
modality specific parse graphs are constructed. Next stage. recursive
bidirectional cross-attention is used, purified by GM. We also design network
based on PGVL. The PGVL and our network is validated on major pose estimation
datasets. We will release the code soon.

</details>


### [66] [DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation](https://arxiv.org/abs/2509.07435)
*Ze-Xin Yin,Jiaxiong Qiu,Liu Liu,Xinjie Wang,Wei Sui,Zhizhong Su,Jian Yang,Jin Xie*

Main category: cs.CV

TL;DR: 本文提出了LGAA框架，通过利用多视角扩散先验，统一了几何建模与PBR材质生成，实现了端到端PBR就绪的3D资产创建，并展现了卓越的性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法主要关注几何建模，忽略或将PBR材质合成推迟到后处理，导致缺乏自主的、端到端的PBR就绪3D资产创建流程。而PBR材质的3D资产创建劳动密集且需要经验。

Method: 引入了轻量级高斯资产适配器（LGAA）框架，其核心思想是从新颖的角度利用多视角（MV）扩散先验，统一几何和PBR材质建模。LGAA包含三个模块：LGAA Wrapper（重用MV扩散模型网络层以实现数据高效收敛），LGAA Switcher（对齐多个LGAA Wrapper层以结合不同的扩散先验），以及LGAA Decoder（一个改进的VAE，用于预测带有PBR通道的2D高斯泼溅，2DGS）。最后，通过专门的后处理程序从2DGS中提取高质量、可重新打光的网格资产。

Result: LGAA在定量和定性实验中，对于文本和图像条件的多视角扩散模型，都表现出卓越的性能。其模块化设计实现了多重扩散先验的灵活结合。知识保留方案带来了高效的收敛，仅使用69k多视角实例即可完成训练。

Conclusion: LGAA提供了一个有效、数据高效且灵活的解决方案，通过独特地将多视角扩散先验整合到几何和PBR材质建模中，实现了端到端的PBR就绪3D资产生成，并展示了优越的性能和高效的训练能力。

Abstract: The labor- and experience-intensive creation of 3D assets with physically
based rendering (PBR) materials demands an autonomous 3D asset creation
pipeline. However, most existing 3D generation methods focus on geometry
modeling, either baking textures into simple vertex colors or leaving texture
synthesis to post-processing with image diffusion models. To achieve end-to-end
PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter
(LGAA), a novel framework that unifies the modeling of geometry and PBR
materials by exploiting multi-view (MV) diffusion priors from a novel
perspective. The LGAA features a modular design with three components.
Specifically, the LGAA Wrapper reuses and adapts network layers from MV
diffusion models, which encapsulate knowledge acquired from billions of images,
enabling better convergence in a data-efficient manner. To incorporate multiple
diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns
multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed
variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D
Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated
post-processing procedure to effectively extract high-quality, relightable mesh
assets from the resulting 2DGS. Extensive quantitative and qualitative
experiments demonstrate the superior performance of LGAA with both text-and
image-conditioned MV diffusion models. Additionally, the modular design enables
flexible incorporation of multiple diffusion priors, and the
knowledge-preserving scheme leads to efficient convergence trained on merely
69k multi-view instances. Our code, pre-trained weights, and the dataset used
will be publicly available via our project page:
https://zx-yin.github.io/dreamlifting/.

</details>


### [67] [In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting](https://arxiv.org/abs/2509.07447)
*Taiying Peng,Jiacheng Hua,Miao Liu,Feng Lu*

Main category: cs.CV

TL;DR: 本文提出EgoGazeVQA，一个结合眼动信息（gaze）的以自我为中心的视频问答基准，旨在提高多模态大语言模型（MLLMs）对用户意图的理解，并通过眼动引导提示方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准忽视了眼动作为用户意图关键指标的作用，而以自我为中心的视频结合MLLMs为AI助手提供个性化体验带来了机遇。因此需要一个能利用眼动信息的基准来改善MLLMs对用户意图的理解。

Method: 引入了EgoGazeVQA基准，包含由MLLMs生成并经人工精炼的基于眼动的问答对。开发了眼动引导的意图提示方法，整合了空间、时间和意图相关线索。此外，还进行了眼动相关的微调实验，并分析了眼动估计精度对提示有效性的影响。

Result: 实验表明，现有MLLMs难以准确解释用户意图。而眼动引导的意图提示方法显著提升了性能。研究还发现眼动估计精度会影响提示的有效性。

Conclusion: 研究结果强调了在以自我为中心的场景中，眼动对于实现更个性化和有效AI助手的价值。

Abstract: The emergence of advanced multimodal large language models (MLLMs) has
significantly enhanced AI assistants' ability to process complex information
across modalities. Recently, egocentric videos, by directly capturing user
focus, actions, and context in an unified coordinate, offer an exciting
opportunity to enable proactive and personalized AI user experiences with
MLLMs. However, existing benchmarks overlook the crucial role of gaze as an
indicator of user intent. To address this gap, we introduce EgoGazeVQA, an
egocentric gaze-guided video question answering benchmark that leverages gaze
information to improve the understanding of longer daily-life videos.
EgoGazeVQA consists of gaze-based QA pairs generated by MLLMs and refined by
human annotators. Our experiments reveal that existing MLLMs struggle to
accurately interpret user intentions. In contrast, our gaze-guided intent
prompting methods significantly enhance performance by integrating spatial,
temporal, and intent-related cues. We further conduct experiments on
gaze-related fine-tuning and analyze how gaze estimation accuracy impacts
prompting effectiveness. These results underscore the value of gaze for more
personalized and effective AI assistants in egocentric settings.

</details>


### [68] [GLEAM: Learning to Match and Explain in Cross-View Geo-Localization](https://arxiv.org/abs/2509.07450)
*Xudong Lu,Zhi Zheng,Yi Wan,Yongxiang Yao,Annan Wang,Renrui Zhang,Panwang Xia,Qiong Wu,Qingyun Li,Weifeng Lin,Xiangyu Zhao,Xue Yang,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出了GLEAM-C，一个统一多视图多模态地理定位（CVGL）的模型，通过与卫星图像对齐实现高效准确匹配。同时引入GLEAM-X任务，利用多模态大语言模型（MLLMs）和新构建的基准数据集，解决了传统CVGL方法缺乏可解释性的问题，实现了可解释的跨视图匹配。


<details>
  <summary>Details</summary>
Motivation: 现有跨视图地理定位（CVGL）方法通常受限于单一视图或模态，且其直接视觉匹配策略缺乏可解释性，无法解释匹配理由。

Method: 本文提出GLEAM-C模型，通过将无人机图像、街道地图、全景图和地面照片等多种视图和模态，统一对齐到卫星图像，提升了训练效率并达到了与现有模型相当的准确性。为解决可解释性问题，提出了GLEAM-X任务，利用多模态大语言模型（MLLMs）的推理能力，结合匹配预测与解释性推理，并构建了基于GPT-4o和Doubao-1.5-Thinking-Vision-Pro的双语基准数据集，并通过人工修订测试集进行验证。

Result: GLEAM-C在保证训练效率的同时，实现了与现有模态特定CVGL模型相当的准确性。GLEAM-X任务和构建的基准数据集支持了可解释跨视图推理的系统评估，提升了地理定位的透明度和可扩展性。

Conclusion: GLEAM-C和GLEAM-X共同构成了一个全面的CVGL流程，整合了多模态、多视图对齐与可解释性对应分析，统一了准确的跨视图匹配与可解释性推理，从而推动了地理定位领域的发展，使模型能更好地解释和匹配（Explain And Match）。

Abstract: Cross-View Geo-Localization (CVGL) focuses on identifying correspondences
between images captured from distinct perspectives of the same geographical
location. However, existing CVGL approaches are typically restricted to a
single view or modality, and their direct visual matching strategy lacks
interpretability: they merely predict whether two images correspond, without
explaining the rationale behind the match. In this paper, we present GLEAM-C, a
foundational CVGL model that unifies multiple views and modalities-including
UAV imagery, street maps, panoramic views, and ground photographs-by aligning
them exclusively with satellite imagery. Our framework enhances training
efficiency through optimized implementation while achieving accuracy comparable
to prior modality-specific CVGL models through a two-phase training strategy.
Moreover, to address the lack of interpretability in traditional CVGL methods,
we leverage the reasoning capabilities of multimodal large language models
(MLLMs) to propose a new task, GLEAM-X, which combines cross-view
correspondence prediction with explainable reasoning. To support this task, we
construct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Pro
to generate training and testing data. The test set is further refined through
detailed human revision, enabling systematic evaluation of explainable
cross-view reasoning and advancing transparency and scalability in
geo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGL
pipeline that integrates multi-modal, multi-view alignment with interpretable
correspondence analysis, unifying accurate cross-view matching with explainable
reasoning and advancing Geo-Localization by enabling models to better Explain
And Match. Code and datasets used in this work will be made publicly accessible
at https://github.com/Lucky-Lance/GLEAM.

</details>


### [69] [XOCT: Enhancing OCT to OCTA Translation via Cross-Dimensional Supervised Multi-Scale Feature Learning](https://arxiv.org/abs/2509.07455)
*Pooya Khosravi,Kun Han,Anthony T. Wu,Arghavan Rezvani,Zexin Feng,Xiaohui Xie*

Main category: cs.CV

TL;DR: XOCT是一种新颖的深度学习框架，它通过结合跨维度监督（CDS）和多尺度特征融合（MSFF）网络，实现了从OCT图像进行层感知血管重建，有效提升了OCTA的可访问性、可靠性和诊断价值。


<details>
  <summary>Details</summary>
Motivation: 高质量OCTA图像的获取受运动敏感性及传统OCT设备软件改造高成本的限制。此外，现有的深度学习方法在OCT到OCTA转换时，常忽略视网膜各层间的血管差异，且难以重建诊断所需的复杂细密血管细节。

Method: 本文提出了XOCT框架，包含两个核心模块：1. 跨维度监督（CDS）模块：利用通过分割加权z轴平均生成的2D层级平面投影作为监督信号，促使网络学习各视网膜层的独特表示。2. 多尺度特征融合（MSFF）模块：通过多尺度特征提取结合通道重加权策略，有效捕获多空间尺度的血管细节，增强血管描绘能力。

Result: 在OCTA-500数据集上的实验表明，XOCT显著提升了性能，尤其是在对视网膜病理学临床评估至关重要的平面投影方面表现更优。

Conclusion: XOCT有望增强OCTA在眼科疾病检测和监测中的可访问性、可靠性和诊断价值。

Abstract: Optical Coherence Tomography Angiography (OCTA) and its derived en-face
projections provide high-resolution visualization of the retinal and choroidal
vasculature, which is critical for the rapid and accurate diagnosis of retinal
diseases. However, acquiring high-quality OCTA images is challenging due to
motion sensitivity and the high costs associated with software modifications
for conventional OCT devices. Moreover, current deep learning methods for
OCT-to-OCTA translation often overlook the vascular differences across retinal
layers and struggle to reconstruct the intricate, dense vascular details
necessary for reliable diagnosis. To overcome these limitations, we propose
XOCT, a novel deep learning framework that integrates Cross-Dimensional
Supervision (CDS) with a Multi-Scale Feature Fusion (MSFF) network for
layer-aware vascular reconstruction. Our CDS module leverages 2D layer-wise
en-face projections, generated via segmentation-weighted z-axis averaging, as
supervisory signals to compel the network to learn distinct representations for
each retinal layer through fine-grained, targeted guidance. Meanwhile, the MSFF
module enhances vessel delineation through multi-scale feature extraction
combined with a channel reweighting strategy, effectively capturing vascular
details at multiple spatial scales. Our experiments on the OCTA-500 dataset
demonstrate XOCT's improvements, especially for the en-face projections which
are significant for clinical evaluation of retinal pathologies, underscoring
its potential to enhance OCTA accessibility, reliability, and diagnostic value
for ophthalmic disease detection and monitoring. The code is available at
https://github.com/uci-cbcl/XOCT.

</details>


### [70] [Estimating forest carbon stocks from high-resolution remote sensing imagery by reducing domain shift with style transfer](https://arxiv.org/abs/2502.00784)
*Zhenyu Yu,Jinnian Wang*

Main category: cs.CV

TL;DR: 该论文提出一种新方法，通过引入风格迁移和Swin Transformer，将结合GF-1 WFV和Landsat TM影像的森林碳储量估算转化为图像翻译问题，旨在提高大尺度监测精度。


<details>
  <summary>Details</summary>
Motivation: 森林作为重要的碳汇对减缓气候变化至关重要。当前基于地面监测和卫星遥感的森林碳储量评估方法，虽便于大尺度观测，但在精度方面仍有待提高。

Method: 研究以中国云南省会泽县为例，利用GF-1 WFV和Landsat TM遥感影像数据。方法上创新性地引入了风格迁移，并结合Swin Transformer的注意力机制提取全局特征，从而将碳储量估算问题建模为图像翻译任务。

Result: 该摘要主要介绍了提出的新方法，并未直接展示使用该方法后森林碳储量估算的具体实验结果、模型性能或精度提升的数据。

Conclusion: 该研究通过将碳储量估算转化为图像翻译问题并引入风格迁移和Swin Transformer，为提高大尺度森林碳储量监测的精度提供了一种潜在的新技术途径。

Abstract: Forests function as crucial carbon reservoirs on land, and their carbon sinks
can efficiently reduce atmospheric CO2 concentrations and mitigate climate
change. Currently, the overall trend for monitoring and assessing forest carbon
stocks is to integrate ground monitoring sample data with satellite remote
sensing imagery. This style of analysis facilitates large-scale observation.
However, these techniques require improvement in accuracy. We used GF-1 WFV and
Landsat TM images to analyze Huize County, Qujing City, Yunnan Province in
China. Using the style transfer method, we introduced Swin Transformer to
extract global features through attention mechanisms, converting the carbon
stock estimation into an image translation.

</details>


### [71] [Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting](https://arxiv.org/abs/2509.07456)
*Sai Siddhartha Chary Aylapuram,Veeraraju Elluru,Shivang Agarwal*

Main category: cs.CV

TL;DR: 深度神经网络因虚假关联导致偏差。本文提出“偏见感知机器遗忘”方法，通过选择性移除偏见样本或特征表示，事后纠正视觉模型中的偏差，在显著提升公平性的同时保持最小的准确性损失，无需完全重新训练。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络因训练数据中的虚假关联导致有偏或不公平的预测，尤其在安全关键领域问题突出。传统偏差缓解方法通常需要从头重新训练，效率低下。机器遗忘技术为事后模型校正提供了一种有前景的替代方案。

Method: 本研究提出并探究“偏见感知机器遗忘”范式，旨在选择性地移除有偏样本或特征表示，以缓解视觉模型中的各类偏差。方法基于隐私保护的遗忘技术，并评估了梯度上升、LoRA和师生蒸馏等多种策略。实验在CUB-200-2011（姿态偏差）、CIFAR-10（合成补丁偏差）和CelebA（微笑检测中的性别偏差）三个基准数据集上进行。

Result: 事后遗忘显著减少了子群体差异，CUB-200数据集上的人口均等性改善高达94.86%，CIFAR-10上为30.28%，CelebA上为97.37%。这些改进伴随着最小的准确性损失，且所用方法在效用、公平性、质量和隐私的联合评估中平均得分0.62。

Conclusion: 机器遗忘被确立为一种实用且有效的框架，能够增强已部署视觉系统的公平性，而无需进行完全的重新训练，为事后偏差缓解提供了一个可行的解决方案。

Abstract: Deep neural networks often rely on spurious correlations in training data,
leading to biased or unfair predictions in safety-critical domains such as
medicine and autonomous driving. While conventional bias mitigation typically
requires retraining from scratch or redesigning data pipelines, recent advances
in machine unlearning provide a promising alternative for post-hoc model
correction. In this work, we investigate \textit{Bias-Aware Machine
Unlearning}, a paradigm that selectively removes biased samples or feature
representations to mitigate diverse forms of bias in vision models. Building on
privacy-preserving unlearning techniques, we evaluate various strategies
including Gradient Ascent, LoRA, and Teacher-Student distillation. Through
empirical analysis on three benchmark datasets, CUB-200-2011 (pose bias),
CIFAR-10 (synthetic patch bias), and CelebA (gender bias in smile detection),
we demonstrate that post-hoc unlearning can substantially reduce subgroup
disparities, with improvements in demographic parity of up to \textbf{94.86\%}
on CUB-200, \textbf{30.28\%} on CIFAR-10, and \textbf{97.37\%} on CelebA. These
gains are achieved with minimal accuracy loss and with methods scoring an
average of 0.62 across the 3 settings on the joint evaluation of utility,
fairness, quality, and privacy. Our findings establish machine unlearning as a
practical framework for enhancing fairness in deployed vision systems without
necessitating full retraining.

</details>


### [72] [ANYPORTAL: Zero-Shot Consistent Video Background Replacement](https://arxiv.org/abs/2509.07472)
*Wenshuo Gao,Xicheng Lan,Shuai Yang*

Main category: cs.CV

TL;DR: ANYPORTAL是一个零样本视频背景替换框架，它利用预训练扩散模型，并通过Refinement Projection算法确保前景一致性和像素级细节操作，无需训练即可在消费级GPU上生成高质量、时间连贯的视频。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成技术难以实现对视频细节的精细控制，无法精确匹配用户意图，限制了其实用性。尤其在视频背景替换中，前景一致性和时间连贯的重打光是核心挑战。

Method: ANYPORTAL框架协同整合了视频扩散模型的时间先验与图像扩散模型的重打光能力（零样本设置）。为解决前景一致性问题，该方法提出Refinement Projection算法，以实现像素级细节操作和精确的前景保留。该框架无需训练。

Result: 实验结果表明，ANYPORTAL能在消费级GPU上生成高质量的视频背景替换效果，成功克服了前景一致性和时间连贯重打光的挑战。

Conclusion: ANYPORTAL为视频内容创作和编辑提供了一个实用且高效的解决方案，其零样本、无需训练的特性及其在处理前景一致性和时间连贯重打光方面的优势，显著提升了视频背景替换的质量和可行性。

Abstract: Despite the rapid advancements in video generation technology, creating
high-quality videos that precisely align with user intentions remains a
significant challenge. Existing methods often fail to achieve fine-grained
control over video details, limiting their practical applicability. We
introduce ANYPORTAL, a novel zero-shot framework for video background
replacement that leverages pre-trained diffusion models. Our framework
collaboratively integrates the temporal prior of video diffusion models with
the relighting capabilities of image diffusion models in a zero-shot setting.
To address the critical challenge of foreground consistency, we propose a
Refinement Projection Algorithm, which enables pixel-level detail manipulation
to ensure precise foreground preservation. ANYPORTAL is training-free and
overcomes the challenges of achieving foreground consistency and temporally
coherent relighting. Experimental results demonstrate that ANYPORTAL achieves
high-quality results on consumer-grade GPUs, offering a practical and efficient
solution for video content creation and editing.

</details>


### [73] [MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification](https://arxiv.org/abs/2509.07477)
*Patrick Wienholt,Christiane Kuhl,Jakob Nikolas Kather,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: MedicalPatchNet是一种自解释的胸部X光分类模型，其性能与EfficientNet-B0相当，但解释性更强。它通过将图像分割成独立补丁并进行分类，无需后处理技术即可直观地解释诊断，从而提高临床信任度并有助于更安全的AI辅助诊断。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在放射影像分类中表现出色，但由于解释性差，限制了其在临床上的接受度。研究需要一个既能保持高性能又能提供透明、可靠解释的模型，以增强临床信任。

Method: 本文提出了MedicalPatchNet，这是一种固有的自解释架构。它将胸部X光图像分割成不重叠的图像补丁，独立地对每个补丁进行分类，然后聚合这些补丁的预测结果。这种方法无需后处理技术即可直观地可视化每个补丁对诊断的贡献。

Result: MedicalPatchNet在CheXpert数据集上训练后，其分类性能（AUROC 0.907）与EfficientNet-B0（AUROC 0.908）相当。在CheXlocalize数据集上，MedicalPatchNet的病理定位准确性显著提高（平均命中率0.485），优于使用Grad-CAM（0.376）的方法，证明了其更强的可解释性。

Conclusion: MedicalPatchNet通过提供明确、可靠且非AI专家也能理解的解释，有效降低了与快捷学习相关的风险，从而增强了临床信任。该模型有助于在医学影像领域实现更安全、可解释的AI辅助诊断。

Abstract: Deep neural networks excel in radiological image classification but
frequently suffer from poor interpretability, limiting clinical acceptance. We
present MedicalPatchNet, an inherently self-explainable architecture for chest
X-ray classification that transparently attributes decisions to distinct image
regions. MedicalPatchNet splits images into non-overlapping patches,
independently classifies each patch, and aggregates predictions, enabling
intuitive visualization of each patch's diagnostic contribution without
post-hoc techniques. Trained on the CheXpert dataset (223,414 images),
MedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908)
of EfficientNet-B0, while substantially improving interpretability:
MedicalPatchNet demonstrates substantially improved interpretability with
higher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with
Grad-CAM) on the CheXlocalize dataset. By providing explicit, reliable
explanations accessible even to non-AI experts, MedicalPatchNet mitigates risks
associated with shortcut learning, thus improving clinical trust. Our model is
publicly available with reproducible training and inference scripts and
contributes to safer, explainable AI-assisted diagnostics across medical
imaging domains. We make the code publicly available:
https://github.com/TruhnLab/MedicalPatchNet

</details>


### [74] [LINR Bridge: Vector Graphic Animation via Neural Implicits and Video Diffusion Priors](https://arxiv.org/abs/2509.07484)
*Wenshuo Gao,Xicheng Lan,Luyao Zhang,Shuai Yang*

Main category: cs.CV

TL;DR: 本文提出一种新方法，将隐式神经表示与文本到视频扩散模型相结合，以自动化生成生动自然的矢量图形动画，克服了传统手动操作的繁琐和现有技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 矢量图形动画虽然能增强内容的可理解性和可控性，但其制作通常需要大量手动工作。现有技术在灵活性和动画质量方面也存在局限性，因此需要一种自动化解决方案。

Method: 该方法首先利用分层隐式神经表示来重建矢量图形，以保留其固有的高分辨率和精确的颜色形状属性，并弥合与扩散模型之间的领域差距。接着，通过视频分数蒸馏采样（Video Score Distillation Sampling）技术优化这些神经表示，该技术利用了预训练文本到视频扩散模型的运动先验。最后，对矢量图形进行形变，使其与优化后的表示匹配，从而生成平滑动画。

Result: 实验结果证明，该方法能有效生成生动自然的矢量图形动画。与现有技术相比，在灵活性和动画质量方面均取得了显著提升。

Conclusion: 本研究成功地将隐式神经表示与文本到视频扩散模型相结合，为矢量图形动画提供了一种高效、高质量的自动化生成方法，有效解决了传统方法的局限性。

Abstract: Vector graphics, known for their scalability and user-friendliness, provide a
unique approach to visual content compared to traditional pixel-based images.
Animation of these graphics, driven by the motion of their elements, offers
enhanced comprehensibility and controllability but often requires substantial
manual effort. To automate this process, we propose a novel method that
integrates implicit neural representations with text-to-video diffusion models
for vector graphic animation. Our approach employs layered implicit neural
representations to reconstruct vector graphics, preserving their inherent
properties such as infinite resolution and precise color and shape constraints,
which effectively bridges the large domain gap between vector graphics and
diffusion models. The neural representations are then optimized using video
score distillation sampling, which leverages motion priors from pretrained
text-to-video diffusion models. Finally, the vector graphics are warped to
match the representations resulting in smooth animation. Experimental results
validate the effectiveness of our method in generating vivid and natural vector
graphic animations, demonstrating significant improvement over existing
techniques that suffer from limitations in flexibility and animation quality.

</details>


### [75] [Fine-Tuning Vision-Language Models for Visual Navigation Assistance](https://arxiv.org/abs/2509.07488)
*Xiao Li,Bharat Gandhi,Ming Zhan,Mohit Nehra,Zhicheng Zhang,Yuchen Sun,Meijia Song,Naisheng Zhang,Xi Wang*

Main category: cs.CV

TL;DR: 本研究通过微调BLIP-2模型并提出新评估指标，显著提升了基于视觉-语言的室内导航系统为视障人士生成方向性指令的能力。


<details>
  <summary>Details</summary>
Motivation: 传统导航系统在室内因缺乏精确位置数据而失效，导致视障人士室内导航困难。研究旨在利用视觉和语言模型生成分步导航指令，提升其室内导航的独立性和可及性。

Method: 本方法整合视觉和语言模型以生成分步导航指令。具体做法是在手动标注的室内导航数据集上，使用低秩适应（LoRA）技术对BLIP-2模型进行微调。此外，提出了一种新的评估指标，该指标通过强调方向性和序列变量来改进BERT F1分数，以更全面地衡量导航性能。

Result: 应用LoRA后，模型在生成方向性指令方面取得了显著改进，成功克服了原始BLIP-2模型的局限性。

Conclusion: 本研究通过对BLIP-2模型进行LoRA微调并在定制数据集上训练，并引入了专用的评估指标，成功提升了视觉-语言驱动的室内导航系统生成方向性指令的能力，为视障人士提供了更有效、更独立的室内导航辅助。

Abstract: We address vision-language-driven indoor navigation to assist visually
impaired individuals in reaching a target location using images and natural
language guidance. Traditional navigation systems are ineffective indoors due
to the lack of precise location data. Our approach integrates vision and
language models to generate step-by-step navigational instructions, enhancing
accessibility and independence. We fine-tune the BLIP-2 model with Low Rank
Adaptation (LoRA) on a manually annotated indoor navigation dataset. We propose
an evaluation metric that refines the BERT F1 score by emphasizing directional
and sequential variables, providing a more comprehensive measure of
navigational performance. After applying LoRA, the model significantly improved
in generating directional instructions, overcoming limitations in the original
BLIP-2 model.

</details>


### [76] [DiGS: Accurate and Complete Surface Reconstruction from 3D Gaussians via Direct SDF Learning](https://arxiv.org/abs/2509.07493)
*Wenzhi Guo,Bing Wang*

Main category: cs.CV

TL;DR: DiGS将SDF学习嵌入3D Gaussian Splatting (3DGS)管道中，通过几何引导的增长策略，显著提升了表面重建的准确性和完整性，同时保持了高渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS)在真实感视图合成方面表现出色，但由于其非结构化表示和缺乏显式几何监督，在实现准确和完整的表面重建方面仍面临挑战。

Method: 本文提出DiGS框架，将Signed Distance Field (SDF)学习直接整合到3DGS管道中，为每个高斯体关联一个可学习的SDF值，以加强表面先验并改善跨视角一致性。此外，设计了一种几何引导的网格增长策略，在多尺度层级下，沿着几何一致的区域自适应地分布高斯体，以确保密集和连贯的覆盖。

Result: 在DTU、Mip-NeRF 360和Tanks& Temples等标准基准测试中，DiGS在保持高渲染保真度的同时，持续提高了重建精度和完整性。

Conclusion: DiGS通过在3DGS中引入SDF学习和几何引导的增长策略，成功解决了表面重建的难题，实现了更高精度和完整性的几何重建，同时未牺牲其在视图合成方面的优势。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful paradigm for
photorealistic view synthesis, representing scenes with spatially distributed
Gaussian primitives. While highly effective for rendering, achieving accurate
and complete surface reconstruction remains challenging due to the unstructured
nature of the representation and the absence of explicit geometric supervision.
In this work, we propose DiGS, a unified framework that embeds Signed Distance
Field (SDF) learning directly into the 3DGS pipeline, thereby enforcing strong
and interpretable surface priors. By associating each Gaussian with a learnable
SDF value, DiGS explicitly aligns primitives with underlying geometry and
improves cross-view consistency. To further ensure dense and coherent coverage,
we design a geometry-guided grid growth strategy that adaptively distributes
Gaussians along geometry-consistent regions under a multi-scale hierarchy.
Extensive experiments on standard benchmarks, including DTU, Mip-NeRF 360, and
Tanks& Temples, demonstrate that DiGS consistently improves reconstruction
accuracy and completeness while retaining high rendering fidelity.

</details>


### [77] [Generating Transferrable Adversarial Examples via Local Mixing and Logits Optimization for Remote Sensing Object Recognition](https://arxiv.org/abs/2509.07495)
*Chun Liu,Hailong Wang,Bingqian Zhu,Panpan Ding,Zheng Zheng,Tao Xu,Zhigang Han,Jiayao Wang*

Main category: cs.CV

TL;DR: 提出了一种针对遥感深度神经网络的非定向对抗攻击新框架，通过局部混合和logits优化，有效解决了现有方法的语义破坏和梯度消失问题，显著提高了黑盒攻击成功率和迁移性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在遥感应用中易受对抗攻击，现有基于混合的攻击策略存在破坏全局语义特征、误导优化以及交叉熵损失导致梯度消失等问题，这些限制了对抗样本的质量和迁移性。

Method: 本文提出一个通过局部混合和logits优化的非定向攻击框架。首先，采用局部混合策略生成多样且语义一致的输入，仅混合局部区域以保留全局语义信息；其次，将logits损失从定向攻击适配到非定向场景，以缓解交叉熵损失的梯度消失问题；最后，应用扰动平滑损失来抑制高频噪声并增强迁移性。

Result: 在FGSCR-42和MTARSI数据集上，本文方法在6个替代模型上均优于12种现有最先进方法。特别是在MTARSI数据集上，以ResNet作为替代模型时，黑盒攻击成功率平均提升了17.28%。

Conclusion: 本研究提出的局部混合和logits优化框架，成功克服了现有对抗攻击方法的局限性，有效提升了对抗样本的生成质量、迁移性和黑盒攻击成功率，对遥感应用中的模型安全性研究具有重要意义。

Abstract: Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, posing
significant security threats to their deployment in remote sensing
applications. Research on adversarial attacks not only reveals model
vulnerabilities but also provides critical insights for enhancing robustness.
Although current mixing-based strategies have been proposed to increase the
transferability of adversarial examples, they either perform global blending or
directly exchange a region in the images, which may destroy global semantic
features and mislead the optimization of adversarial examples. Furthermore,
their reliance on cross-entropy loss for perturbation optimization leads to
gradient diminishing during iterative updates, compromising adversarial example
quality. To address these limitations, we focus on non-targeted attacks and
propose a novel framework via local mixing and logits optimization. First, we
present a local mixing strategy to generate diverse yet semantically consistent
inputs. Different from MixUp, which globally blends two images, and MixCut,
which stitches images together, our method merely blends local regions to
preserve global semantic information. Second, we adapt the logit loss from
targeted attacks to non-targeted scenarios, mitigating the gradient vanishing
problem of cross-entropy loss. Third, a perturbation smoothing loss is applied
to suppress high-frequency noise and enhance transferability. Extensive
experiments on FGSCR-42 and MTARSI datasets demonstrate superior performance
over 12 state-of-the-art methods across 6 surrogate models. Notably, with
ResNet as the surrogate on MTARSI, our method achieves a 17.28% average
improvement in black-box attack success rate.

</details>


### [78] [MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection](https://arxiv.org/abs/2509.07507)
*Saad Lahlali,Alexandre Fournier Montgieux,Nicolas Granger,Hervé Le Borgne,Quoc Cuong Pham*

Main category: cs.CV

TL;DR: MVAT框架利用序列数据中的时间多视角信息，通过点云聚合、教师-学生蒸馏和多视角2D投影损失，实现了无需3D标注的弱监督3D目标检测SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 3D数据标注成本高昂是3D目标检测的瓶颈。现有弱监督方法仅依赖2D标注时，面临投影模糊和单视角可见性不足的挑战，难以准确估计3D框。

Method: 提出MVAT框架， leveraging temporal multi-view (时间多视角)来解决问题：1. 聚合跨时间的对象中心点云，构建尽可能密集完整的3D对象表示。2. 采用教师-学生蒸馏范式：教师网络从单视角学习，目标标签来源于时间聚合的静态对象，并生成高质量伪标签；学生网络学习从单视角预测静态和移动对象。3. 引入多视角2D投影损失，以确保预测的3D框与所有可用的2D标注保持一致性。

Result: 在nuScenes和Waymo Open数据集上，MVAT在弱监督3D目标检测中达到了最先进的性能。它显著缩小了与全监督方法之间的差距，且无需任何3D框标注。

Conclusion: MVAT通过有效利用序列数据中的时间多视角信息，成功克服了弱监督3D目标检测中的挑战，在无需3D标注的情况下实现了SOTA性能，并大幅提升了弱监督方法与全监督方法之间的竞争力。

Abstract: Annotating 3D data remains a costly bottleneck for 3D object detection,
motivating the development of weakly supervised annotation methods that rely on
more accessible 2D box annotations. However, relying solely on 2D boxes
introduces projection ambiguities since a single 2D box can correspond to
multiple valid 3D poses. Furthermore, partial object visibility under a single
viewpoint setting makes accurate 3D box estimation difficult. We propose MVAT,
a novel framework that leverages temporal multi-view present in sequential data
to address these challenges. Our approach aggregates object-centric point
clouds across time to build 3D object representations as dense and complete as
possible. A Teacher-Student distillation paradigm is employed: The Teacher
network learns from single viewpoints but targets are derived from temporally
aggregated static objects. Then the Teacher generates high quality
pseudo-labels that the Student learns to predict from a single viewpoint for
both static and moving objects. The whole framework incorporates a multi-view
2D projection loss to enforce consistency between predicted 3D boxes and all
available 2D annotations. Experiments on the nuScenes and Waymo Open datasets
demonstrate that MVAT achieves state-of-the-art performance for weakly
supervised 3D object detection, significantly narrowing the gap with fully
supervised methods without requiring any 3D box annotations. % \footnote{Code
available upon acceptance} Our code is available in our public repository
(\href{https://github.com/CEA-LIST/MVAT}{code}).

</details>


### [79] [EHWGesture -- A dataset for multimodal understanding of clinical gestures](https://arxiv.org/abs/2509.07525)
*Gianluca Amprimo,Alberto Ancilotto,Alessandro Savino,Fabio Quazzolo,Claudia Ferraris,Gabriella Olmo,Elisabetta Farella,Stefano Di Carlo*

Main category: cs.CV

TL;DR: 引入EHWGesture，一个多模态、多视角手势数据集，包含精确姿态追踪和动作质量评估，旨在推动临床动态手势理解。


<details>
  <summary>Details</summary>
Motivation: 动态手势理解因时空复杂性而具挑战，且现有数据集缺乏多模态/多视角多样性、精确真值追踪和动作质量评估，这阻碍了如手部灵巧度临床评估等应用。

Method: 论文引入EHWGesture，一个针对手势理解的多模态视频数据集，包含五种临床相关手势。数据集拥有超过1,100段录像，由25名健康受试者通过RGB-Depth和事件相机采集。运动捕捉系统提供精确手部地标真值追踪，所有设备经过校准和同步。录像按执行速度分类，以嵌入动作质量评估任务。

Result: 基线实验表明，该数据集在手势分类、手势触发检测和动作质量评估方面具有潜力。

Conclusion: EHWGesture可作为一个综合基准，推动多模态临床手势理解的研究。

Abstract: Hand gesture understanding is essential for several applications in
human-computer interaction, including automatic clinical assessment of hand
dexterity. While deep learning has advanced static gesture recognition, dynamic
gesture understanding remains challenging due to complex spatiotemporal
variations. Moreover, existing datasets often lack multimodal and multi-view
diversity, precise ground-truth tracking, and an action quality component
embedded within gestures. This paper introduces EHWGesture, a multimodal video
dataset for gesture understanding featuring five clinically relevant gestures.
It includes over 1,100 recordings (6 hours), captured from 25 healthy subjects
using two high-resolution RGB-Depth cameras and an event camera. A motion
capture system provides precise ground-truth hand landmark tracking, and all
devices are spatially calibrated and synchronized to ensure cross-modal
alignment. Moreover, to embed an action quality task within gesture
understanding, collected recordings are organized in classes of execution speed
that mirror clinical evaluations of hand dexterity. Baseline experiments
highlight the dataset's potential for gesture classification, gesture trigger
detection, and action quality assessment. Thus, EHWGesture can serve as a
comprehensive benchmark for advancing multimodal clinical gesture
understanding.

</details>


### [80] [Universal Few-Shot Spatial Control for Diffusion Models](https://arxiv.org/abs/2509.07530)
*Kiet T. Nguyen,Chanhuyk Lee,Donggyun Kim,Dong Hoon Lee,Seunghoon Hong*

Main category: cs.CV

TL;DR: 本文提出了通用少样本控制 (UFC)，这是一种多功能少样本控制适配器，能够泛化到新颖的空间条件，仅需少量数据即可实现与全监督基线相当的性能，并可应用于不同的扩散骨干网络。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型的空间控制适配器在遇到与训练任务差异大的新型空间控制条件时，适应性有限且训练成本高昂。

Method: 本文提出了通用少样本控制 (UFC)，它是一个多功能少样本控制适配器。给定少量未见任务的图像-条件对和查询条件，UFC利用查询和支持条件之间的类比，通过匹配机制和更新一小组任务特定参数来构建任务特定控制特征，从而泛化到新的空间条件。

Result: ['在六个新型空间控制任务上，UFC仅用30个标注示例进行微调，就实现了与空间条件一致的精细控制。', '当使用0.1%的完整训练数据进行微调时，UFC在各种控制任务中达到了与全监督基线相当的性能。', 'UFC可广泛适用于各种扩散骨干网络，并在UNet和DiT架构上均展示了其有效性。']

Conclusion: UFC提供了一种高效、通用的解决方案，能够以少量数据使预训练文本到图像扩散模型适应新型空间控制任务，解决了现有方法适应性差和训练成本高的问题，并具备跨骨干网络的泛化能力。

Abstract: Spatial conditioning in pretrained text-to-image diffusion models has
significantly improved fine-grained control over the structure of generated
images. However, existing control adapters exhibit limited adaptability and
incur high training costs when encountering novel spatial control conditions
that differ substantially from the training tasks. To address this limitation,
we propose Universal Few-Shot Control (UFC), a versatile few-shot control
adapter capable of generalizing to novel spatial conditions. Given a few
image-condition pairs of an unseen task and a query condition, UFC leverages
the analogy between query and support conditions to construct task-specific
control features, instantiated by a matching mechanism and an update on a small
set of task-specific parameters. Experiments on six novel spatial control tasks
show that UFC, fine-tuned with only 30 annotated examples of novel tasks,
achieves fine-grained control consistent with the spatial conditions. Notably,
when fine-tuned with 0.1% of the full training data, UFC achieves competitive
performance with the fully supervised baselines in various control tasks. We
also show that UFC is applicable agnostically to various diffusion backbones
and demonstrate its effectiveness on both UNet and DiT architectures. Code is
available at https://github.com/kietngt00/UFC.

</details>


### [81] [HU-based Foreground Masking for 3D Medical Masked Image Modeling](https://arxiv.org/abs/2509.07534)
*Jin Lee,Vu Dang,Gwang-Hyun Yu,Anh Le,Zahid Rahman,Jin-Ho Jang,Heonzoo Lee,Kun-Yung Kim,Jin-Sul Kim,Jin-Young Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于Hounsfield单位（HU）的前景掩码策略，以改进3D医学图像中Masked Image Modeling（MIM）的预训练任务，通过关注解剖学上有意义的区域，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: Masked Image Modeling（MIM）在3D医学图像计算中的应用受到限制，因为其随机掩码策略忽略了解剖对象的密度。研究旨在解决这一限制。

Method: 通过利用Hounsfield单位（HU）测量，实施了一种基于HU的前景掩码策略。该策略侧重于内脏器官的强度分布，并排除了缺乏诊断意义特征的非组织区域（如空气和液体）。

Result: 在五个公共3D医学成像数据集（BTCV、Flare22、MM-WHS、Amos22、BraTS）上进行了广泛实验，结果表明该掩码策略持续提高了性能，包括分割质量和Dice分数（BTCV:~84.64%, Flare22:~92.43%, MM-WHS:~90.67%, Amos22:~88.64%, BraTS:~78.55%）。

Conclusion: 这些结果强调了领域中心MIM的重要性，并为医学图像分割中的表征学习提供了一个有前景的方向。

Abstract: While Masked Image Modeling (MIM) has revolutionized fields of computer
vision, its adoption in 3D medical image computing has been limited by the use
of random masking, which overlooks the density of anatomical objects. To
address this limitation, we enhance the pretext task with a simple yet
effective masking strategy. Leveraging Hounsfield Unit (HU) measurements, we
implement an HU-based Foreground Masking, which focuses on the intensity
distribution of visceral organs and excludes non-tissue regions, such as air
and fluid, that lack diagnostically meaningful features. Extensive experiments
on five public 3D medical imaging datasets demonstrate that our masking
consistently improves performance, both in quality of segmentation and Dice
score (BTCV:~84.64\%, Flare22:~92.43\%, MM-WHS:~90.67\%, Amos22:~88.64\%,
BraTS:~78.55\%). These results underscore the importance of domain-centric MIM
and suggest a promising direction for representation learning in medical image
segmentation. Implementation is available at github.com/AISeedHub/SubFore/.

</details>


### [82] [TextlessRAG: End-to-End Visual Document RAG by Speech Without Text](https://arxiv.org/abs/2509.07538)
*Peijin Xie,Shun Qian,Bingquan Liu,Dexin Wang,Lin Sun,Xiangzheng Zhang*

Main category: cs.CV

TL;DR: 本文提出了TextlessRAG，首个端到端、无文本（textless）的框架，用于基于语音查询在大规模文档图像上进行知识库问答，并发布了首个双语语音-文档RAG数据集。


<details>
  <summary>Details</summary>
Motivation: 文档图像蕴含丰富知识，语音查询具有广泛灵活的应用场景，但此前没有工作探索直接通过语音查询对视觉文档图像进行知识库问答。现有方法依赖ASR、TTS和OCR，存在效率和准确性局限。

Method: 提出了TextlessRAG框架，它直接解释语音，检索相关视觉知识，并生成答案，完全消除了ASR、TTS和OCR环节。为了进一步提升性能，整合了布局感知的重排序机制来优化检索。

Result: 实验证明，TextlessRAG在效率和准确性方面均实现了显著提升。此外，还发布了首个双语语音-文档RAG数据集，包含中文和英文语音查询以及多模态文档内容。

Conclusion: TextlessRAG是解决文档图像语音问答领域的一个创新性框架，通过去除对文本中间环节的依赖，显著提高了效率和准确性。发布的双语数据集也将促进该方向的后续研究。

Abstract: Document images encapsulate a wealth of knowledge, while the portability of
spoken queries enables broader and flexible application scenarios. Yet, no
prior work has explored knowledge base question answering over visual document
images with queries provided directly in speech. We propose TextlessRAG, the
first end-to-end framework for speech-based question answering over large-scale
document images. Unlike prior methods, TextlessRAG eliminates ASR, TTS and OCR,
directly interpreting speech, retrieving relevant visual knowledge, and
generating answers in a fully textless pipeline. To further boost performance,
we integrate a layout-aware reranking mechanism to refine retrieval.
Experiments demonstrate substantial improvements in both efficiency and
accuracy. To advance research in this direction, we also release the first
bilingual speech--document RAG dataset, featuring Chinese and English voice
queries paired with multimodal document content. Both the dataset and our
pipeline will be made available at
repository:https://github.com/xiepeijinhit-hue/textlessrag

</details>


### [83] [PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from One-shot Unposed Image](https://arxiv.org/abs/2509.07552)
*Peng Li,Yisheng He,Yingdong Hu,Yuan Dong,Weihao Yuan,Yuan Liu,Zilong Dong,Yike Guo*

Main category: cs.CV

TL;DR: 提出一个前馈框架，通过单张无姿态图像快速合成高保真高斯全身头部模型，利用合成数据集、粗到细管线和双分支结构。


<details>
  <summary>Details</summary>
Motivation: 现有工作依赖耗时的GAN反演和测试时优化，且缺乏大规模3D头部资产，因此需要一个能实现单次通过快速重建和渲染的有效方法。

Method: 1. 构建了一个基于3D GAN的大规模合成数据集并仅用其进行训练。2. 采用前馈、单次通过的框架进行头部合成。3. 引入粗到细的高斯头部生成管线：利用Transformer模块使FLAME稀疏点与图像特征交互，实现特征提取和粗形状重建，随后进行稠密化以实现高保真重建。4. 提出一个双分支框架，有效聚合结构化球面三平面特征和非结构化点特征，以充分利用预训练3D GAN中的先验知识。

Result: 实验结果表明，该框架相对于现有工作具有更高的有效性，实现了推理时快速的高保真重建和渲染。

Conclusion: 该框架成功解决了现有单图头部合成的耗时问题和3D数据稀缺挑战，通过创新的架构实现了从单张图像中快速、高保真地生成高斯全身头部模型。

Abstract: We present a feed-forward framework for Gaussian full-head synthesis from a
single unposed image. Unlike previous work that relies on time-consuming GAN
inversion and test-time optimization, our framework can reconstruct the
Gaussian full-head model given a single unposed image in a single forward pass.
This enables fast reconstruction and rendering during inference. To mitigate
the lack of large-scale 3D head assets, we propose a large-scale synthetic
dataset from trained 3D GANs and train our framework using only synthetic data.
For efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian
head generation pipeline, where sparse points from the FLAME model interact
with the image features by transformer blocks for feature extraction and coarse
shape reconstruction, which are then densified for high-fidelity
reconstruction. To fully leverage the prior knowledge residing in pretrained 3D
GANs for effective reconstruction, we propose a dual-branch framework that
effectively aggregates the structured spherical triplane feature and
unstructured point-based features for more effective Gaussian head
reconstruction. Experimental results show the effectiveness of our framework
towards existing work.

</details>


### [84] [Attention Maps in 3D Shape Classification for Dental Stage Estimation with Class Node Graph Attention Networks](https://arxiv.org/abs/2509.07581)
*Barkin Buyukcakir,Rocharles Cavalcante Fontenele,Reinhilde Jacobs,Jannick De Tobel,Patrick Thevissen,Dirk Vandermeulen,Peter Claes*

Main category: cs.CV

TL;DR: 本文提出Class Node Graph Attention Network (CGAT) 模型，通过图注意力机制为3D形状识别任务提供可解释性。在第三磨牙发育阶段分类中，CGAT实现了良好的预测性能，并能生成人类可理解的注意力图，提升了模型透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医疗和法医等高风险识别任务中前景广阔，但其“黑箱”特性阻碍了在需要信任和问责制的应用中的广泛采用。特别是在3D形状识别任务中，亟需可解释的模型。

Method: 引入Class Node Graph Attention Network (CGAT) 架构，利用图注意力卷积和内在注意力机制，并通过注意力展开（attention rollout）可视化其决策过程。在CBCT图像衍生的第三磨牙3D网格上进行Demirjian阶段分配，评估了局部平均曲率和到质心距离作为节点特征（单独及组合），以及模型深度，并引入了指向全局CLS节点的定向边。

Result: 包含指向全局CLS节点的定向边模型产生了更直观的注意力图，同时保持了良好的分类性能。结合局部平均曲率和到质心距离作为节点特征，性能略有提升（加权F1分数为0.76），并提供了更全面的注意力可视化。

Conclusion: CGAT架构能够生成人类可理解的注意力图，从而增强模型信任并促进专家验证。该模型虽然在牙科数据上进行了验证，但普适于图基分类和回归任务，有助于在高风险环境中推广透明且有竞争力的深度学习模型。

Abstract: Deep learning offers a promising avenue for automating many recognition tasks
in fields such as medicine and forensics. However, the black-box nature of
these models hinders their adoption in high-stakes applications where trust and
accountability are required. For 3D shape recognition tasks in particular, this
paper introduces the Class Node Graph Attention Network (CGAT) architecture to
address this need. Applied to 3D meshes of third molars derived from CBCT
images, for Demirjian stage allocation, CGAT utilizes graph attention
convolutions and an inherent attention mechanism, visualized via attention
rollout, to explain its decision-making process. We evaluated the local mean
curvature and distance to centroid node features, both individually and in
combination, as well as model depth, finding that models incorporating directed
edges to a global CLS node produced more intuitive attention maps, while also
yielding desirable classification performance. We analyzed the attention-based
explanations of the models, and their predictive performances to propose
optimal settings for the CGAT. The combination of local mean curvature and
distance to centroid as node features yielded a slight performance increase
with 0.76 weighted F1 score, and more comprehensive attention visualizations.
The CGAT architecture's ability to generate human-understandable attention maps
can enhance trust and facilitate expert validation of model decisions. While
demonstrated on dental data, CGAT is broadly applicable to graph-based
classification and regression tasks, promoting wider adoption of transparent
and competitive deep learning models in high-stakes environments.

</details>


### [85] [Temporal Image Forensics: A Review and Critical Evaluation](https://arxiv.org/abs/2509.07591)
*Robert Jöchl,Andreas Uhl*

Main category: cs.CV

TL;DR: 本文综述了基于图像采集管道中时间依赖性痕迹（如传感器缺陷和灰尘）的时间图像取证领域，并强调了内容偏差问题及可解释人工智能（XAI）的重要性。此外，文章还提出了新的取证设置，验证了传感器缺陷特性，揭示了某些利用缺陷的方法实际上可能利用内容偏差，并展示了神经网络在学习图像年龄痕迹时易受干扰。


<details>
  <summary>Details</summary>
Motivation: 提供时间图像取证领域的全面概述，重点关注图像采集管道中的时间依赖性痕迹。揭示并强调内容偏差在取证技术中的关键问题，并指出可解释人工智能在验证这些技术可靠性方面的重要性。同时，通过重新评估和实验，纠正现有方法的误解，并探索人工智能在识别真实年龄痕迹时的鲁棒性。

Method: 本文采用文献综述方法，对前人工作进行分析和整合。在此基础上，通过重新实现相关方法和进行实验来验证和扩展现有研究。具体方法包括：提出新的取证设置、验证现场传感器缺陷的增长率和空间分布特性、分析神经网络学习的特征，以及设计实验以展示神经网络易受干扰的现象。

Result: ['提供了基于图像采集管道时间依赖性痕迹的时间图像取证领域的全面概述，包括已知年龄痕迹（如传感器缺陷和灰尘）的特性及相关技术。', '强调了内容偏差问题，并指出可解释人工智能对于验证时间图像取证技术可靠性的重要性。', '提出了一个新的、可能更现实的取证设置。', '验证了现场传感器缺陷的主要特性，包括增长率和空间分布。', '发现一种旨在利用现场传感器缺陷进行图像年龄近似的方法实际上利用了其他痕迹（最可能是内容偏差）。', '进一步调查了神经网络在对掌纹图像进行年代测定时所学习的特征。', '展示了神经网络在学习真实年龄痕迹时多么容易受到干扰。']

Conclusion: 时间图像取证依赖于时间依赖性痕迹来估计图像年龄，但内容偏差是一个关键挑战，可解释人工智能对于确保其可靠性至关重要。本研究不仅全面综述了该领域，还通过提出新的取证设置、澄清传感器缺陷特性、揭示现有方法对内容偏差的利用，以及证明神经网络在学习真实年龄痕迹时易受干扰，为该领域做出了重要贡献。这些发现强调了该领域，尤其是在应用AI方法时，需要进行严格的验证和审视。

Abstract: Temporal image forensics is the science of estimating the age of a digital
image. Usually, time-dependent traces (age traces) introduced by the image
acquisition pipeline are exploited for this purpose. In this review, a
comprehensive overview of the field of temporal image forensics based on
time-dependent traces from the image acquisition pipeline is given. This
includes a detailed insight into the properties of known age traces (i.e.,
in-field sensor defects and sensor dust) and temporal image forensics
techniques. Another key aspect of this work is to highlight the problem of
content bias and to illustrate how important eXplainable Artificial
Intelligence methods are to verify the reliability of temporal image forensics
techniques. Apart from reviewing material presented in previous works, in this
review: (i) a new (probably more realistic) forensic setting is proposed; (ii)
the main properties (growth rate and spatial distribution) of in-field sensor
defects are verified; (iii) it is shown that a method proposed to utilize
in-field sensor defects for image age approximation actually exploits other
traces (most likely content bias); (iv) the features learned by a neural
network dating palmprint images are further investigated; (v) it is shown how
easily a neural network can be distracted from learning age traces. For this
purpose, previous work is analyzed, re-implemented if required and experiments
are conducted.

</details>


### [86] [Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation](https://arxiv.org/abs/2509.07596)
*Yusuke Hirota,Ryo Hachiuma,Boyi Li,Ximing Lu,Michael Ross Boone,Boris Ivanovic,Yejin Choi,Marco Pavone,Yu-Chiang Frank Wang,Noa Garcia,Yuta Nakashima,Chao-Han Huck Yang*

Main category: cs.CV

TL;DR: 现有VLM性别偏见评估可能因图像中虚假特征的干扰而不可靠。研究发现，即使轻微扰动非性别特征，也能大幅改变偏见分数。建议在偏见评估中增加特征敏感性测量。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）的性别偏见评估常使用包含性别与非性别特征（如物体、背景）虚假关联的基准。作者识别出一个关键疏漏：这些虚假特征是否会扭曲性别偏见评估结果？

Method: 研究者系统性地扰动了四个广泛使用的基准（COCO-gender, FACET, MIAP, PHASE）中的非性别特征（例如遮蔽10%物体或轻微模糊背景），并在多种VLM上量化了这些扰动对偏见评估的影响。

Result: 即使是微小的扰动，也能显著改变偏见分数，生成式VLM的指标变化高达175%，CLIP变体高达43%。这表明当前的偏见评估往往反映了模型对虚假特征的响应，而非真正的性别偏见，从而损害了评估的可靠性。

Conclusion: 鉴于创建无虚假特征的基准测试具有根本性挑战，作者建议在报告偏见指标时，同时提供特征敏感性测量结果，以实现更可靠的偏见评估。

Abstract: Gender bias in vision-language foundation models (VLMs) raises concerns about
their safe deployment and is typically evaluated using benchmarks with gender
annotations on real-world images. However, as these benchmarks often contain
spurious correlations between gender and non-gender features, such as objects
and backgrounds, we identify a critical oversight in gender bias evaluation: Do
spurious features distort gender bias evaluation? To address this question, we
systematically perturb non-gender features across four widely used benchmarks
(COCO-gender, FACET, MIAP, and PHASE) and various VLMs to quantify their impact
on bias evaluation. Our findings reveal that even minimal perturbations, such
as masking just 10% of objects or weakly blurring backgrounds, can dramatically
alter bias scores, shifting metrics by up to 175% in generative VLMs and 43% in
CLIP variants. This suggests that current bias evaluations often reflect model
responses to spurious features rather than gender bias, undermining their
reliability. Since creating spurious feature-free benchmarks is fundamentally
challenging, we recommend reporting bias metrics alongside feature-sensitivity
measurements to enable a more reliable bias assessment.

</details>


### [87] [Data-Efficient Fine-Tuning of Vision-Language Models for Diagnosis of Alzheimer's Disease](https://arxiv.org/abs/2509.07613)
*Fangqi Cheng,Surajit Ray,Xiaochen Yang*

Main category: cs.CV

TL;DR: 提出一种数据高效的微调流程，将3D CT-based医学视觉-语言模型（Med-VLMs）应用于3D MRI的阿尔茨海默病（AD）诊断，通过整合元数据和临床知识，以少量数据实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型（Med-VLMs）存在以下局限：未充分利用患者元数据、缺乏临床诊断知识整合、需要大量计算资源进行从头训练或微调、以及对3D医学影像（特别是从2D或CT模型到3D MRI的适应性）效果有限。

Method: 我们提出一个数据高效的微调流程，旨在将3D CT-based Med-VLMs适应于3D MRI并应用于阿尔茨海默病诊断。主要创新点包括：1) 将结构化元数据转换为合成报告以丰富文本输入，改善图像-文本对齐；2) 添加一个辅助token，训练其预测Mini-Mental State Examination (MMSE) 分数，提供额外监督。该方法对图像和文本模态均采用轻量级提示微调。

Result: 我们的方法在使用1,500张训练图像的情况下，在两个AD数据集上实现了最先进的（SOTA）性能，优于那些使用10,000张图像进行微调的现有方法。

Conclusion: 该数据高效的微调流程有效解决了Med-VLMs在3D医学影像中的局限性，通过整合患者元数据和临床知识，以显著减少的数据量，在3D MRI的阿尔茨海默病诊断中取得了卓越的性能。

Abstract: Medical vision-language models (Med-VLMs) have shown impressive results in
tasks such as report generation and visual question answering, but they still
face several limitations. Most notably, they underutilize patient metadata and
lack integration of clinical diagnostic knowledge. Moreover, most existing
models are typically trained from scratch or fine-tuned on large-scale 2D
image-text pairs, requiring extensive computational resources, and their
effectiveness on 3D medical imaging is often limited due to the absence of
structural information. To address these gaps, we propose a data-efficient
fine-tuning pipeline to adapt 3D CT-based Med-VLMs for 3D MRI and demonstrate
its application in Alzheimer's disease (AD) diagnosis. Our system introduces
two key innovations. First, we convert structured metadata into synthetic
reports, enriching textual input for improved image-text alignment. Second, we
add an auxiliary token trained to predict the mini-mental state examination
(MMSE) score, a widely used clinical measure of cognitive function that
correlates with AD severity. This provides additional supervision for
fine-tuning. Applying lightweight prompt tuning to both image and text
modalities, our approach achieves state-of-the-art performance on two AD
datasets using 1,500 training images, outperforming existing methods fine-tuned
on 10,000 images. Code will be released upon publication.

</details>


### [88] [Self-Supervised Cross-Encoder for Neurodegenerative Disease Diagnosis](https://arxiv.org/abs/2509.07623)
*Fangqi Cheng,Yingying Zhao,Xiaochen Yang*

Main category: cs.CV

TL;DR: 针对深度学习在神经退行性疾病诊断中对标注数据依赖大且可解释性差的问题，本文提出一种自监督交叉编码器框架，利用MRI时间连续性解耦学习表示为静态和动态成分，有效提升了分类准确性、可解释性，并展现出强大的零样本和跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在利用MRI数据诊断神经退行性疾病时，过度依赖大量标注数据，且产生的特征表示缺乏可解释性。

Method: 提出一种新颖的自监督交叉编码器框架，利用纵向MRI扫描的时间连续性进行监督学习。该框架将学习到的表示解耦为两部分：通过对比学习捕获稳定解剖特征的“静态表示”，以及通过输入梯度正则化反映时间变化并可用于下游分类任务微调的“动态表示”。

Result: 在ADNI数据集上，该方法实现了卓越的分类准确率和更高的可解释性。此外，学习到的表示在OASIS数据集上展现了强大的零样本泛化能力，并在PPMI数据集上实现了跨任务泛化。

Conclusion: 本文提出的自监督交叉编码器框架成功解决了神经退行性疾病诊断中深度学习方法对标注数据依赖及可解释性不足的问题，通过创新的表示解耦和自监督学习策略，显著提高了分类性能、模型可解释性及在不同数据集和任务上的泛化能力。

Abstract: Deep learning has shown significant potential in diagnosing neurodegenerative
diseases from MRI data. However, most existing methods rely heavily on large
volumes of labeled data and often yield representations that lack
interpretability. To address both challenges, we propose a novel
self-supervised cross-encoder framework that leverages the temporal continuity
in longitudinal MRI scans for supervision. This framework disentangles learned
representations into two components: a static representation, constrained by
contrastive learning, which captures stable anatomical features; and a dynamic
representation, guided by input-gradient regularization, which reflects
temporal changes and can be effectively fine-tuned for downstream
classification tasks. Experimental results on the Alzheimer's Disease
Neuroimaging Initiative (ADNI) dataset demonstrate that our method achieves
superior classification accuracy and improved interpretability. Furthermore,
the learned representations exhibit strong zero-shot generalization on the Open
Access Series of Imaging Studies (OASIS) dataset and cross-task generalization
on the Parkinson Progression Marker Initiative (PPMI) dataset. The code for the
proposed method will be made publicly available.

</details>


### [89] [Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity](https://arxiv.org/abs/2509.07647)
*Sung Ju Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: 本文提出了一种名为Hermitian对称傅里叶水印（SFW）的新型嵌入方法，通过保持频率完整性和引入中心感知嵌入策略，显著提升了潜在扩散模型（LDMs）语义水印的检测性能和对裁剪攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的针对潜在扩散模型（LDMs）的语义水印技术虽能抵抗再生攻击，但常因频率完整性丧失导致检测性能下降，且易受裁剪攻击影响。

Method: 我们提出Hermitian对称傅里叶水印（SFW）方法，通过强制Hermitian对称来保持频率完整性。此外，引入中心感知嵌入策略，通过确保信息稳健保留来降低水印对裁剪攻击的脆弱性。这些技术被应用于现有语义水印方案，以增强其频域结构。

Result: 实验结果表明，我们的方法在各种攻击场景下实现了最先进的验证和识别性能，超越了以往的方法。消融研究证实了SFW对检测能力的影响、中心感知嵌入对裁剪的有效性以及消息容量对识别准确性的影响。值得注意的是，该方法在保持卓越图像保真度（FID和CLIP分数）的同时，实现了最高的检测准确率。

Conclusion: 我们提出的SFW是一个有效的框架，能够平衡鲁棒性和图像保真度，解决了语义水印中固有的权衡问题。

Abstract: Semantic watermarking techniques for latent diffusion models (LDMs) are
robust against regeneration attacks, but often suffer from detection
performance degradation due to the loss of frequency integrity. To tackle this
problem, we propose a novel embedding method called Hermitian Symmetric Fourier
Watermarking (SFW), which maintains frequency integrity by enforcing Hermitian
symmetry. Additionally, we introduce a center-aware embedding strategy that
reduces the vulnerability of semantic watermarking due to cropping attacks by
ensuring robust information retention. To validate our approach, we apply these
techniques to existing semantic watermarking schemes, enhancing their
frequency-domain structures for better robustness and retrieval accuracy.
Extensive experiments demonstrate that our methods achieve state-of-the-art
verification and identification performance, surpassing previous approaches
across various attack scenarios. Ablation studies confirm the impact of SFW on
detection capabilities, the effectiveness of the center-aware embedding against
cropping, and how message capacity influences identification accuracy. Notably,
our method achieves the highest detection accuracy while maintaining superior
image fidelity, as evidenced by FID and CLIP scores. Conclusively, our proposed
SFW is shown to be an effective framework for balancing robustness and image
fidelity, addressing the inherent trade-offs in semantic watermarking. Code
available at https://github.com/thomas11809/SFWMark

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [90] [Renewable Energy Sources Selection Analysis with the Maximizing Deviation Method](https://arxiv.org/abs/2509.07011)
*Kirisci Murat*

Main category: cs.AI

TL;DR: 本研究提出了一种结合偏差最大化优化模型和区间值费马模糊集的费马模糊多准则决策方法，用于选择可再生能源。


<details>
  <summary>Details</summary>
Motivation: 多准则决策常面临不确定、复杂和冲突的情况。模糊集理论能有效处理人类思维中的不确定性。选择可再生能源是一个关键问题，涉及到能源需求、碳排放和气候变化，且不仅有技术层面，还有管理和政治影响。

Method: 本研究利用费马模糊环境（模糊集的泛化），提出了一种基于偏差最大化方法的优化模型来确定部分已知特征权重。该方法与区间值费马模糊集结合，并应用于可再生能源选择问题。

Result: 所提出的方法成功应用于可再生能源选择问题，并讨论了该问题的管理和政治影响。

Conclusion: 本研究提出的费马模糊多准则决策方法能够有效处理决策者判断中的不确定性与模糊性，适用于解决可再生能源选择等复杂问题，并能考虑其技术、管理和政治层面。

Abstract: Multi-criteria decision-making methods provide decision-makers with
appropriate tools to make better decisions in uncertain, complex, and
conflicting situations. Fuzzy set theory primarily deals with the uncertainty
inherent in human thoughts and perceptions and attempts to quantify this
uncertainty. Fuzzy logic and fuzzy set theory are utilized with multi-criteria
decision-making methods because they effectively handle uncertainty and
fuzziness in decision-makers' judgments, allowing for verbal judgments of the
problem. This study utilizes the Fermatean fuzzy environment, a generalization
of fuzzy sets. An optimization model based on the deviation maximization method
is proposed to determine partially known feature weights. This method is
combined with interval-valued Fermatean fuzzy sets. The proposed method was
applied to the problem of selecting renewable energy sources. The reason for
choosing renewable energy sources is that meeting energy needs from renewable
sources, balancing carbon emissions, and mitigating the effects of global
climate change are among the most critical issues of the recent period. Even
though selecting renewable energy sources is a technical issue, the managerial
and political implications of this issue are also important, and are discussed
in this study.

</details>


### [91] [From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning](https://arxiv.org/abs/2509.07017)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.AI

TL;DR: 本文提出Spectral NSR，一个完全谱域的神经符号推理框架，通过在图谱域中进行推理，结合了符号推理的可解释性与谱学习的可伸缩性和适应性，并在推理基准测试中表现出卓越的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有推理系统在可解释性、可伸缩性、适应性等方面存在局限。本文旨在统一符号推理的可解释性与谱学习的可伸缩性和适应性，为下一代推理系统提供一个透明、鲁棒和泛化的基础。

Method: 引入Spectral NSR框架，将逻辑规则嵌入为谱模板，并利用图信号处理（GSP）和基于知识图谱拉普拉斯特征结构的频率选择滤波器直接在图谱域中执行推理。该方法还包括一系列扩展，如动态图与基学习、有理和扩散滤波器、混合谱专家、证据引导训练、不确定性量化、大型语言模型耦合、共谱传输对齐等。

Result: 在ProofWriter和CLUTRR等推理基准测试中，Spectral NSR相比领先基线（如transformers、MPNN和神经符号逻辑编程系统）展现出卓越的准确性、更快的推理速度、更高的对抗扰动鲁棒性和更强的可解释性。谱归因和证明带协议分析证实模型决策与符号证明结构高度一致，迁移实验验证了通过共谱对齐实现有效的领域适应。

Conclusion: Spectral NSR为下一代推理系统奠定了可伸缩且有原则的基础，提供了超越传统方法的透明度、鲁棒性和泛化能力。

Abstract: We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning
framework that embeds logical rules as spectral templates and performs
inference directly in the graph spectral domain. By leveraging graph signal
processing (GSP) and frequency-selective filters grounded in the Laplacian
eigenstructure of knowledge graphs, the architecture unifies the
interpretability of symbolic reasoning with the scalability and adaptability of
spectral learning. Beyond the core formulation, we incorporate a comprehensive
set of extensions, including dynamic graph and basis learning, rational and
diffusion filters for sharper spectral selectivity, mixture-of-spectral-experts
for modular specialization, proof-guided training with spectral curricula, and
uncertainty quantification for calibrated confidence. Additional enhancements
such as large language model coupling, co-spectral transfer alignment,
adversarial robustness, efficient GPU kernels, generalized Laplacians, and
causal interventions further expand the versatility of the framework.
  Empirical evaluation on state-of-the-art reasoning benchmarks such as
ProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior
accuracy, faster inference, improved robustness to adversarial perturbations,
and higher interpretability compared to leading baselines including
transformers, message-passing neural networks, and neuro-symbolic logic
programming systems. Spectral attribution and proof-band agreement analyses
confirm that model decisions align closely with symbolic proof structures,
while transfer experiments validate effective domain adaptation through
co-spectral alignment. These results establish Spectral NSR as a scalable and
principled foundation for the next generation of reasoning systems, offering
transparency, robustness, and generalization beyond conventional approaches.

</details>


### [92] [Statistical Methods in Generative AI](https://arxiv.org/abs/2509.07054)
*Edgar Dobriban*

Main category: cs.AI

TL;DR: 生成式AI缺乏可靠性保证，本文回顾了统计方法在提升其可靠性、评估效率及实验设计方面的应用。


<details>
  <summary>Details</summary>
Motivation: 生成式AI基于概率模型，默认情况下缺乏关于正确性、安全性、公平性等方面的保证，因此需要提高其可靠性并优化评估方法。

Method: 本文通过文献综述，阐述了通用的统计技术及其在生成式AI可靠性提升、AI评估和实验设计中的现有应用。

Result: 总结了统计方法在解决生成式AI可靠性问题、改进AI评估和实验设计方面的潜力和现有工作。

Conclusion: 统计方法为提高生成式AI的可靠性及AI评估和实验提供了有前景的途径，并指出了局限性和未来发展方向。

Abstract: Generative Artificial Intelligence is emerging as an important technology,
promising to be transformative in many areas. At the same time, generative AI
techniques are based on sampling from probabilistic models, and by default,
they come with no guarantees about correctness, safety, fairness, or other
properties. Statistical methods offer a promising potential approach to improve
the reliability of generative AI techniques. In addition, statistical methods
are also promising for improving the quality and efficiency of AI evaluation,
as well as for designing interventions and experiments in AI.
  In this paper, we review some of the existing work on these topics,
explaining both the general statistical techniques used, as well as their
applications to generative AI. We also discuss limitations and potential future
directions.

</details>


### [93] [Instruction Agent: Enhancing Agent with Expert Demonstration](https://arxiv.org/abs/2509.07098)
*Yinheng Li,Hailey Hultquist,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: Instruction Agent是一个新的GUI代理，通过学习专家演示来解决复杂GUI任务，避免错误，并通过验证器和回溯器模块提高鲁棒性，在现有代理失败的任务上取得了显著成功。


<details>
  <summary>Details</summary>
Motivation: 现有图形用户界面（GUI）代理在处理包含新颖UI元素、长序列动作和个性化轨迹的复杂任务时表现不佳，难以完成困难的工作流程。

Method: 该代理给定一次专家演示，从中提取分步指令，并严格遵循用户预设的轨迹进行执行，从而避免错误。此外，它还利用验证器（verifier）和回溯器（backtracker）模块来提高鲁棒性，以理解每次操作的结果并处理执行过程中意外中断（如弹窗）。

Result: Instruction Agent在OSWorld中一组顶级代理均未能完成的任务上，取得了60%的成功率。

Conclusion: Instruction Agent提供了一个实用且可扩展的框架，弥合了当前GUI代理与可靠的真实世界GUI任务自动化之间的差距。

Abstract: Graphical user interface (GUI) agents have advanced rapidly but still
struggle with complex tasks involving novel UI elements, long-horizon actions,
and personalized trajectories. In this work, we introduce Instruction Agent, a
GUI agent that leverages expert demonstrations to solve such tasks, enabling
completion of otherwise difficult workflows. Given a single demonstration, the
agent extracts step-by-step instructions and executes them by strictly
following the trajectory intended by the user, which avoids making mistakes
during execution. The agent leverages the verifier and backtracker modules
further to improve robustness. Both modules are critical to understand the
current outcome from each action and handle unexpected interruptions(such as
pop-up windows) during execution. Our experiments show that Instruction Agent
achieves a 60% success rate on a set of tasks in OSWorld that all top-ranked
agents failed to complete. The Instruction Agent offers a practical and
extensible framework, bridging the gap between current GUI agents and reliable
real-world GUI task automation.

</details>


### [94] [Neuro-Symbolic Frameworks: Conceptual Characterization and Empirical Comparative Analysis](https://arxiv.org/abs/2509.07122)
*Sania Sinha,Tanawan Premsri,Danial Kamali,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 本文分析了现有神经符号（NeSy）框架的技术特性和挑战，并以DeepProbLog、Scallop和DomiKnowS为例，旨在促进社区对该领域进行创新性思考和开发。


<details>
  <summary>Details</summary>
Motivation: 尽管神经符号（NeSy）框架结合了神经计算的灵活性和符号推理的可解释性，但其学习曲线陡峭、缺乏用户友好工具、库和统一框架，限制了其广泛应用。现有研究多关注算法而非通用问题规范框架。

Method: 本文通过分析现有NeSy框架的符号表示语言、与神经模型的集成方式以及底层算法等技术层面。为突出关键特性，论文重点展示了DeepProbLog、Scallop和DomiKnowS这三个通用NeSy框架。

Result: 论文识别了现有NeSy框架在各个技术层面的挑战，这为评估每个框架在解决各种问题时的表达能力奠定了基础。

Conclusion: 通过对现有NeSy框架的特性和挑战的识别，本文旨在为社区提供基础，鼓励重新思考神经符号建模问题，并激发开发更统一、用户友好框架的创新行动。

Abstract: Neurosymbolic (NeSy) frameworks combine neural representations and learning
with symbolic representations and reasoning. Combining the reasoning
capacities, explainability, and interpretability of symbolic processing with
the flexibility and power of neural computing allows us to solve complex
problems with more reliability while being data-efficient. However, this
recently growing topic poses a challenge to developers with its learning curve,
lack of user-friendly tools, libraries, and unifying frameworks. In this paper,
we characterize the technical facets of existing NeSy frameworks, such as the
symbolic representation language, integration with neural models, and the
underlying algorithms. A majority of the NeSy research focuses on algorithms
instead of providing generic frameworks for declarative problem specification
to leverage problem solving. To highlight the key aspects of Neurosymbolic
modeling, we showcase three generic NeSy frameworks - \textit{DeepProbLog},
\textit{Scallop}, and \textit{DomiKnowS}. We identify the challenges within
each facet that lay the foundation for identifying the expressivity of each
framework in solving a variety of problems. Building on this foundation, we aim
to spark transformative action and encourage the community to rethink this
problem in novel ways.

</details>


### [95] [Autoencoder-Based Denoising of Muscle Artifacts in ECG to Preserve Skin Nerve Activity (SKNA) for Cognitive Stress Detection](https://arxiv.org/abs/2509.07146)
*Farnoush Baghestani,Jihye Moon,Youngsun Kong,Ki Chon*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The sympathetic nervous system (SNS) plays a central role in regulating the
body's responses to stress and maintaining physiological stability. Its
dysregulation is associated with a wide range of conditions, from
cardiovascular disease to anxiety disorders. Skin nerve activity (SKNA)
extracted from high-frequency electrocardiogram (ECG) recordings provides a
noninvasive window into SNS dynamics, but its measurement is highly susceptible
to electromyographic (EMG) contamination. Traditional preprocessing based on
bandpass filtering within a fixed range (e.g., 500--1000 Hz) is susceptible to
overlapping EMG and SKNA spectral components, especially during sustained
muscle activity. We present a denoising approach using a lightweight
one-dimensional convolutional autoencoder with a long short-term memory (LSTM)
bottleneck to reconstruct clean SKNA from EMG-contaminated recordings. Using
clean ECG-derived SKNA data from cognitive stress experiments and EMG noise
from chaotic muscle stimulation recordings, we simulated contamination at
realistic noise levels (--4 dB, --8 dB signal-to-noise ratio) and trained the
model in the leave-one-subject-out cross-validation framework. The method
improved signal-to-noise ratio by up to 9.65 dB, increased cross correlation
with clean SKNA from 0.40 to 0.72, and restored burst-based SKNA features to
near-clean discriminability (AUROC $\geq$ 0.96). Classification of baseline
versus sympathetic stimulation (cognitive stress) conditions reached accuracies
of 91--98\% across severe noise levels, comparable to clean data. These results
demonstrate that deep learning--based reconstruction can preserve
physiologically relevant sympathetic bursts during substantial EMG
interference, enabling more robust SKNA monitoring in naturalistic,
movement-rich environments.

</details>


### [96] [PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal Reinforcement Learning](https://arxiv.org/abs/2509.07159)
*Heng Hao,Wenjun Hu,Oxana Verkholyak,Davoud Ataee Tarzanagh,Baruch Gutow,Sima Didari,Masoud Faraki,Hankyu Moon,Seungjai Min*

Main category: cs.AI

TL;DR: PaVeRL-SQL框架结合部分匹配奖励和口头强化学习，显著提高了Text-to-SQL模型在工业级复杂数据库和问题上的执行准确性，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Text-to-SQL在简单场景取得进展，但当前方法在处理工业规模数据库和涉及领域特定业务逻辑的复杂问题时，执行准确性仍然较低。

Method: PaVeRL-SQL框架结合了“部分匹配奖励”（Partial-Match Rewards）和“口头强化学习”（Verbal Reinforcement Learning），以推动推理语言模型（RLMs）在Text-to-SQL任务上的自我改进。它采用了两条管道：1) 一个使用大型语言模型（LLMs）作为骨干的、包含群组自评估的上下文学习框架（verbal-RL）；2) 一个使用小型骨干模型（OmniSQL-7B）、专门设计的奖励函数和两阶段强化学习的思维链（CoT）RL管道。

Result: PaVeRL-SQL在主流Text-to-SQL基准（Spider、Spider 2.0和BIRD）上取得了最先进（SOTA）的结果。在工业级Spider2.0-SQLite基准上，verbal-RL管道的执行准确率比SOTA高7.4%，CoT管道高1.4%。使用混合SQL方言进行RL训练，尤其对于训练数据有限的方言，带来了显著的三倍增益。

Conclusion: PaVeRL-SQL在实际工业约束下，提供了可靠且最先进的Text-to-SQL解决方案。

Abstract: Text-to-SQL models allow users to interact with a database more easily by
generating executable SQL statements from natural-language questions. Despite
recent successes on simpler databases and questions, current Text-to-SQL
methods still suffer from low execution accuracy on industry-scale databases
and complex questions involving domain-specific business logic. We present
\emph{PaVeRL-SQL}, a framework that combines \emph{Partial-Match Rewards} and
\emph{Verbal Reinforcement Learning} to drive self-improvement in reasoning
language models (RLMs) for Text-to-SQL. To handle practical use cases, we adopt
two pipelines: (1) a newly designed in-context learning framework with group
self-evaluation (verbal-RL), using capable open- and closed-source large
language models (LLMs) as backbones; and (2) a chain-of-thought (CoT) RL
pipeline with a small backbone model (OmniSQL-7B) trained with a specially
designed reward function and two-stage RL. These pipelines achieve
state-of-the-art (SOTA) results on popular Text-to-SQL benchmarks -- Spider,
Spider 2.0, and BIRD. For the industrial-level Spider2.0-SQLite benchmark, the
verbal-RL pipeline achieves an execution accuracy 7.4\% higher than SOTA, and
the CoT pipeline is 1.4\% higher. RL training with mixed SQL dialects yields
strong, threefold gains, particularly for dialects with limited training data.
Overall, \emph{PaVeRL-SQL} delivers reliable, SOTA Text-to-SQL under realistic
industrial constraints. The code is available at
https://github.com/PaVeRL-SQL/PaVeRL-SQL.

</details>


### [97] [That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral](https://arxiv.org/abs/2509.07170)
*Quinten Steenhuis*

Main category: cs.AI

TL;DR: 本文引入并评估了FETCH法律问题分类器，结合混合LLM/ML集成方法和自动生成后续问题以提高准确性。使用419个真实世界查询数据集，该方法实现了97.37%的hits@2准确率，超越了GPT-5模型，并在高准确率下显著降低了法律援助成本。


<details>
  <summary>Details</summary>
Motivation: 每年有数百万人寻求法律帮助，但准确识别其法律问题是提供正确帮助的第一步。错误引导可能导致严重后果，如错过截止日期、遭受虐待、失去住房或子女监护权。因此，开发高效准确的法律问题分类系统至关重要。

Method: 引入并评估了FETCH分类器，用于法律问题分类。采用两种方法提高准确性：一是混合LLM/ML集成分类方法，二是自动生成后续问题以丰富初始问题描述。研究使用了包含419个真实世界非营利律师转介服务查询的新型数据集进行评估。

Result: 研究结果显示，使用廉价模型组合实现了97.37%的hits@2分类准确率。此性能超越了当前最先进的GPT-5模型。

Conclusion: 该方法有望在实现高准确性的同时，显著降低引导法律系统用户找到正确资源的成本。

Abstract: Each year millions of people seek help for their legal problems by calling a
legal aid program hotline, walking into a legal aid office, or using a lawyer
referral service. The first step to match them to the right help is to identify
the legal problem the applicant is experiencing. Misdirection has consequences.
Applicants may miss a deadline, experience physical abuse, lose housing or lose
custody of children while waiting to connect to the right legal help. We
introduce and evaluate the FETCH classifier for legal issue classification and
describe two methods for improving accuracy: a hybrid LLM/ML ensemble
classification method, and the automatic generation of follow-up questions to
enrich the initial problem narrative. We employ a novel data set of 419
real-world queries to a nonprofit lawyer referral service. Ultimately, we show
classification accuracy (hits@2) of 97.37\% using a mix of inexpensive models,
exceeding the performance of the current state-of-the-art GPT-5 model. Our
approach shows promise in significantly reducing the cost of guiding users of
the legal system to the right resource for their problem while achieving high
accuracy.

</details>


### [98] [A Hybrid CNN-LSTM Deep Learning Model for Intrusion Detection in Smart Grid](https://arxiv.org/abs/2509.07208)
*Abdulhakim Alsaiari,Mohammad Ilyas*

Main category: cs.AI

TL;DR: 针对智能电网的网络安全漏洞，提出一种结合CNN和LSTM的混合深度学习入侵检测系统，在DNP3和IEC104数据集上实现了99.70%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 智能电网因与可再生能源和通信技术互联而易受攻击，SCADA协议存在如未经授权访问和DoS等安全漏洞，可能导致隐私泄露和运营中断，亟需增强其网络安全防护能力。

Method: 提出一个混合深度学习入侵检测系统（IDS）。该系统结合了卷积神经网络（CNN）的特征提取能力和长短期记忆（LSTM）网络的时间模式识别能力。模型在DNP3和IEC104入侵检测数据集上进行训练和测试。

Result: 与现有其他深度学习方法相比，所提出的CNN-LSTM模型在准确率、精确度、召回率和F1-score方面均有显著提高，检测准确率高达99.70%。

Conclusion: 本研究提出的混合深度学习（CNN-LSTM）入侵检测系统能够有效识别和分类智能电网中的网络威胁，显著增强了智能电网的网络安全防护能力。

Abstract: The evolution of the traditional power grid into the "smart grid" has
resulted in a fundamental shift in energy management, which allows the
integration of renewable energy sources with modern communication technology.
However, this interconnection has increased smart grids' vulnerability to
attackers, which might result in privacy breaches, operational interruptions,
and massive outages. The SCADA-based smart grid protocols are critical for
real-time data collection and control, but they are vulnerable to attacks like
unauthorized access and denial of service (DoS). This research proposes a
hybrid deep learning-based Intrusion Detection System (IDS) intended to improve
the cybersecurity of smart grids. The suggested model takes advantage of
Convolutional Neural Networks' (CNN) feature extraction capabilities as well as
Long Short-Term Memory (LSTM) networks' temporal pattern recognition skills.
DNP3 and IEC104 intrusion detection datasets are employed to train and test our
CNN-LSTM model to recognize and classify the potential cyber threats. Compared
to other deep learning approaches, the results demonstrate considerable
improvements in accuracy, precision, recall, and F1-score, with a detection
accuracy of 99.70%.

</details>


### [99] [BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions](https://arxiv.org/abs/2509.07209)
*Nicholas Sung,Steven Spreizer,Mohamed Elrefaie,Kaira Samuel,Matthew C. Jones,Faez Ahmed*

Main category: cs.AI

TL;DR: BlendedNet是一个包含999种翼身融合体（BWB）几何形状及其RANS模拟数据的公开气动数据集，并提出了一个端到端代理框架用于点式气动预测。


<details>
  <summary>Details</summary>
Motivation: 为解决非常规气动布局（如BWB）的数据稀缺问题，并推动数据驱动的气动设计代理建模研究。

Method: 数据集通过采样几何设计参数和飞行条件生成，包含8830个收敛的RANS模拟案例（Spalart-Allmaras模型）。代理框架包括：使用置换不变PointNet回归器从表面点云预测几何参数，然后用Feature-wise Linear Modulation (FiLM) 网络在预测参数和飞行条件下预测点式系数Cp、Cfx和Cfz。

Result: 实验结果表明，该代理框架在多种BWB几何形状的表面预测中表现出较低的误差。

Conclusion: BlendedNet数据集有效解决了非常规气动布局的数据稀缺问题，并为气动设计中的数据驱动代理建模研究提供了支持。

Abstract: BlendedNet is a publicly available aerodynamic dataset of 999 blended wing
body (BWB) geometries. Each geometry is simulated across about nine flight
conditions, yielding 8830 converged RANS cases with the Spalart-Allmaras model
and 9 to 14 million cells per case. The dataset is generated by sampling
geometric design parameters and flight conditions, and includes detailed
pointwise surface quantities needed to study lift and drag. We also introduce
an end-to-end surrogate framework for pointwise aerodynamic prediction. The
pipeline first uses a permutation-invariant PointNet regressor to predict
geometric parameters from sampled surface point clouds, then conditions a
Feature-wise Linear Modulation (FiLM) network on the predicted parameters and
flight conditions to predict pointwise coefficients Cp, Cfx, and Cfz.
Experiments show low errors in surface predictions across diverse BWBs.
BlendedNet addresses data scarcity for unconventional configurations and
enables research on data-driven surrogate modeling for aerodynamic design.

</details>


### [100] [OmniAcc: Personalized Accessibility Assistant Using Generative AI](https://arxiv.org/abs/2509.07220)
*Siddhant Karki,Ethan Han,Nadim Mahmud,Suman Bhunia,John Femiani,Vaskar Raychoudhury*

Main category: cs.AI

TL;DR: 本文提出了OmniAcc，一个AI驱动的交互式导航系统，它利用GPT-4、卫星图像和OpenStreetMap数据，识别、分类并绘制城市中的无障碍设施（如坡道和人行横道），为行动不便者提供个性化路线规划、实时导航和即时查询服务，以改善其城市导航体验。


<details>
  <summary>Details</summary>
Motivation: 行动不便者在城市环境中导航时，因缺乏可访问的信息和工具而面临显著障碍。

Method: OmniAcc是一个AI驱动的交互式导航系统，利用GPT-4、卫星图像和OpenStreetMap数据识别、分类和绘制建筑环境中的轮椅可访问特征。该系统采用零样本学习和定制提示实现精确检测，并通过结构化工作流程支持验证。它提供个性化路线规划、实时免提导航和即时物理可访问性查询响应。

Result: 通过对人行横道检测的案例研究，OmniAcc的检测准确率达到97.5%。

Conclusion: OmniAcc展示了AI在改善导航和促进更具包容性的城市空间方面的巨大潜力，有望为城市规划师和行动辅助设备使用者提供帮助。

Abstract: Individuals with ambulatory disabilities often encounter significant barriers
when navigating urban environments due to the lack of accessible information
and tools. This paper presents OmniAcc, an AI-powered interactive navigation
system that utilizes GPT-4, satellite imagery, and OpenStreetMap data to
identify, classify, and map wheelchair-accessible features such as ramps and
crosswalks in the built environment. OmniAcc offers personalized route
planning, real-time hands-free navigation, and instant query responses
regarding physical accessibility. By using zero-shot learning and customized
prompts, the system ensures precise detection of accessibility features, while
supporting validation through structured workflows. This paper introduces
OmniAcc and explores its potential to assist urban planners and mobility-aid
users, demonstrated through a case study on crosswalk detection. With a
crosswalk detection accuracy of 97.5%, OmniAcc highlights the transformative
potential of AI in improving navigation and fostering more inclusive urban
spaces.

</details>


### [101] [HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring](https://arxiv.org/abs/2509.07260)
*Xin Wang,Ting Dang,Xinyu Zhang,Vassilis Kostakos,Michael J. Witbrock,Hong Jia*

Main category: cs.AI

TL;DR: 本研究评估了小型语言模型（SLMs）在移动医疗预测任务中的表现，发现它们在效率和隐私方面优于大型语言模型（LLMs），同时能达到可比的性能，但仍面临类不平衡和少样本学习的挑战，被认为是未来隐私保护型医疗监控的潜力方案。


<details>
  <summary>Details</summary>
Motivation: 移动和可穿戴医疗监控在改善生活质量方面至关重要。现有的大型语言模型（LLMs）医疗解决方案多基于云端，存在严重的隐私问题、内存占用高和延迟大。因此，需要探索轻量级、可在本地高效运行的小型语言模型（SLMs），但它们在医疗预测中的性能尚不明确。

Method: 研究系统性地评估了SLMs在健康预测任务中的表现，采用了零样本、少样本和指令微调等方法。随后，将表现最佳的微调SLMs部署到移动设备上，以评估它们在实际医疗场景中的真实效率和预测性能。

Result: 结果显示，SLMs在提供显著效率和隐私优势的同时，能够实现与LLMs可比的性能。然而，在处理类别不平衡和少样本学习场景时，仍存在挑战。

Conclusion: 尽管SLMs目前仍有不足，但它们被认为是下一代隐私保护型医疗监控领域一个有前景的解决方案。

Abstract: Mobile and wearable healthcare monitoring play a vital role in facilitating
timely interventions, managing chronic health conditions, and ultimately
improving individuals' quality of life. Previous studies on large language
models (LLMs) have highlighted their impressive generalization abilities and
effectiveness in healthcare prediction tasks. However, most LLM-based
healthcare solutions are cloud-based, which raises significant privacy concerns
and results in increased memory usage and latency. To address these challenges,
there is growing interest in compact models, Small Language Models (SLMs),
which are lightweight and designed to run locally and efficiently on mobile and
wearable devices. Nevertheless, how well these models perform in healthcare
prediction remains largely unexplored. We systematically evaluated SLMs on
health prediction tasks using zero-shot, few-shot, and instruction fine-tuning
approaches, and deployed the best performing fine-tuned SLMs on mobile devices
to evaluate their real-world efficiency and predictive performance in practical
healthcare scenarios. Our results show that SLMs can achieve performance
comparable to LLMs while offering substantial gains in efficiency and privacy.
However, challenges remain, particularly in handling class imbalance and
few-shot scenarios. These findings highlight SLMs, though imperfect in their
current form, as a promising solution for next-generation, privacy-preserving
healthcare monitoring.

</details>


### [102] [Performative Thinking? The Brittle Correlation Between CoT Length and Problem Complexity](https://arxiv.org/abs/2509.07339)
*Vardhan Palod,Karthik Valmeekam,Kaya Stechly,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: 研究发现，语言模型中间token生成（如CoT）的长度与问题实际难度（由A*搜索操作数衡量）相关性不高，反而主要受问题与训练数据分布距离的影响，挑战了将长CoT等同于“思考努力”的假设。


<details>
  <summary>Details</summary>
Motivation: 语言模型通过中间token生成（如CoT）提高推理性能，但其机制不明。社区普遍假设长CoT代表“思考”或更强的自适应计算。本研究旨在批判性地检验中间token序列长度是否真实反映或关联问题难度。

Method: 从头训练Transformer模型，使其学习A*搜索算法解决迷宫问题的派生轨迹。以A*算法所需操作数作为精确的问题复杂度度量。模型首先在简单的自由空间问题上评估，然后系统地在分布外（out-of-distribution）问题上评估，比较中间token长度与真实的A*轨迹长度。

Result: 即使对于最简单的任务，模型也常生成过长的推理轨迹，有时甚至未能提供解决方案。在分布外问题上，中间token长度与真实A*轨迹长度仅存在松散关联。这种关联性主要出现在接近训练分布的问题上，表明其源于近似回忆而非真正的自适应计算。问题的固有计算复杂性并非显著因素，而其与训练数据的分布距离才是。

Conclusion: 研究结果挑战了中间轨迹生成能适应问题难度的假设，并警示不应自动将系统中较长的序列（如R1）解释为“思考努力”的迹象。

Abstract: Intermediate token generation (ITG), where a model produces output before the
solution, has been proposed as a method to improve the performance of language
models on reasoning tasks. While these reasoning traces or Chain of Thoughts
(CoTs) are correlated with performance gains, the mechanisms underlying them
remain unclear. A prevailing assumption in the community has been to
anthropomorphize these tokens as "thinking", treating longer traces as evidence
of higher problem-adaptive computation. In this work, we critically examine
whether intermediate token sequence length reflects or correlates with problem
difficulty. To do so, we train transformer models from scratch on derivational
traces of the A* search algorithm, where the number of operations required to
solve a maze problem provides a precise and verifiable measure of problem
complexity. We first evaluate the models on trivial free-space problems,
finding that even for the simplest tasks, they often produce excessively long
reasoning traces and sometimes fail to generate a solution. We then
systematically evaluate the model on out-of-distribution problems and find that
the intermediate token length and ground truth A* trace length only loosely
correlate. We notice that the few cases where correlation appears are those
where the problems are closer to the training distribution, suggesting that the
effect arises from approximate recall rather than genuine problem-adaptive
computation. This suggests that the inherent computational complexity of the
problem instance is not a significant factor, but rather its distributional
distance from the training data. These results challenge the assumption that
intermediate trace generation is adaptive to problem difficulty and caution
against interpreting longer sequences in systems like R1 as automatically
indicative of "thinking effort".

</details>


### [103] [Autonomous Code Evolution Meets NP-Completeness](https://arxiv.org/abs/2509.07367)
*Cunxi Yu,Rongjian Liang,Chia-Tung Ho,Haoxing Ren*

Main category: cs.AI

TL;DR: SATLUTION是一个受AlphaEvolve启发的框架，它将LLM驱动的代码演化扩展到完整的代码仓库级别，并成功为布尔可满足性问题（SAT）演化出超越人类顶尖选手的求解器。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代码演化工作（如AlphaEvolve）虽然展示了LLM自主改进算法的能力，但其应用范围局限于独立的小型代码片段。本研究旨在将LLM驱动的代码演化能力扩展到包含大量文件和代码行的完整代码仓库级别。

Method: 提出SATLUTION框架，通过编排LLM代理直接演化C/C++求解器仓库，同时确保代码正确性并利用分布式运行时反馈。该框架还具备自我演化其演化策略和规则的能力。研究目标是布尔可满足性问题（SAT）。

Result: 从SAT Competition 2024代码库和基准测试开始，SATLUTION演化出的求解器在SAT Competition 2025中以决定性优势超越了人类设计的冠军，并且在2024年基准测试中也超越了2024年和2025年的冠军。

Conclusion: SATLUTION成功证明了LLM驱动的代码演化可以扩展到复杂的、大规模的软件系统（如完整代码仓库），并能在解决NP-完全问题等核心领域中，生成超越人类专家水平的算法和解决方案。

Abstract: Large language models (LLMs) have recently shown strong coding abilities,
enabling not only static code generation but also iterative code self-evolving
through agentic frameworks. Recently, AlphaEvolve \cite{novikov2025alphaevolve}
demonstrated that LLM-based coding agents can autonomously improve algorithms
and surpass human experts, with scopes limited to isolated kernels spanning
hundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the
first framework to extend LLM-based code evolution to the full repository
scale, encompassing hundreds of files and tens of thousands of lines of C/C++
code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem
and a cornerstone of both theory and applications. SATLUTION orchestrates LLM
agents to directly evolve solver repositories under strict correctness
guarantees and distributed runtime feedback, while simultaneously self-evolving
its own evolution policies and rules. Starting from SAT Competition 2024
codebases and benchmark, SATLUTION evolved solvers that decisively outperformed
the human-designed winners of the SAT Competition 2025, and also surpassed both
2024 and 2025 champions on the 2024 benchmarks.

</details>


### [104] [Language Self-Play For Data-Free Training](https://arxiv.org/abs/2509.07414)
*Jakub Grudzien Kuba,Mengting Gu,Qi Ma,Yuandong Tian,Vijai Mohan*

Main category: cs.AI

TL;DR: 本文提出了一种名为语言自博弈（LSP）的强化学习方法，通过自我对弈使大型语言模型无需额外数据即可提升性能，实验证明其在指令遵循任务上优于数据驱动基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速发展受限于对不断增长训练数据的需求，这构成了其进一步发展的基本瓶颈。

Method: 本研究提出了一种基于博弈论自我对弈框架的强化学习方法，称为语言自博弈（LSP）。该方法将模型的性能视为在竞争性博弈中的表现，并通过模型与自身对弈来产生更强的策略，从而实现无需额外数据即可提升模型能力。

Result: 在指令遵循基准任务上使用Llama-3.2-3B-Instruct进行的实验表明，预训练模型不仅能通过纯粹的自我对弈在挑战性任务上提升性能，而且比数据驱动的基线方法更有效。

Conclusion: 语言自博弈（LSP）提供了一种有效途径，使大型语言模型能够在不依赖额外数据的情况下提升性能，并在挑战性任务上超越传统数据驱动方法。

Abstract: Large language models (LLMs) have advanced rapidly in recent years, driven by
scale, abundant high-quality training data, and reinforcement learning. Yet
this progress faces a fundamental bottleneck: the need for ever more data from
which models can continue to learn. In this work, we propose a reinforcement
learning approach that removes this dependency by enabling models to improve
without additional data. Our method leverages a game-theoretic framework of
self-play, where a model's capabilities are cast as performance in a
competitive game and stronger policies emerge by having the model play against
itself - a process we call Language Self-Play (LSP). Experiments with
Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained
models can not only enhance their performance on challenging tasks through
self-play alone, but can also do so more effectively than data-driven
baselines.

</details>


### [105] [SheetDesigner: MLLM-Powered Spreadsheet Layout Generation with Rule-Based and Vision-Based Reflection](https://arxiv.org/abs/2509.07473)
*Qin Chen,Yuanyi Ren,Xiaojun Ma,Mugeng Liu,Han Shi,Dongmei Zhang*

Main category: cs.AI

TL;DR: 本文提出SheetDesigner，一个零样本、免训练的MLLM框架，用于自动化电子表格布局生成，解决了现有模型不适用于电子表格离散网格结构和语义关联的问题，性能显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 电子表格布局设计耗时且需要专业知识，急需自动化解决方案。现有自动化布局模型不适用于电子表格的离散、网格化结构，且忽略了数据依赖和上下文链接等固有语义。

Method: 首先， formalize 电子表格布局生成任务，并构建了一个包含3,326个电子表格的数据集及七项评估协议。然后，引入了SheetDesigner，一个零样本、免训练的框架，该框架利用多模态大型语言模型（MLLMs），结合规则和视觉反射机制进行组件放置和内容填充。

Result: SheetDesigner的性能比五个基线至少高出22.6%。研究发现，MLLMs通过视觉模态能很好地处理重叠和平衡问题，但在对齐方面表现不佳，这表明需要混合规则和视觉反射策略。

Conclusion: SheetDesigner为自动化电子表格布局生成提供了一个有效的解决方案。MLLMs在处理复杂布局时展现出潜力，但为了应对特定挑战（如对齐），结合规则和视觉反射的混合策略是必要的。

Abstract: Spreadsheets are critical to data-centric tasks, with rich, structured
layouts that enable efficient information transmission. Given the time and
expertise required for manual spreadsheet layout design, there is an urgent
need for automated solutions. However, existing automated layout models are
ill-suited to spreadsheets, as they often (1) treat components as axis-aligned
rectangles with continuous coordinates, overlooking the inherently discrete,
grid-based structure of spreadsheets; and (2) neglect interrelated semantics,
such as data dependencies and contextual links, unique to spreadsheets. In this
paper, we first formalize the spreadsheet layout generation task, supported by
a seven-criterion evaluation protocol and a dataset of 3,326 spreadsheets. We
then introduce SheetDesigner, a zero-shot and training-free framework using
Multimodal Large Language Models (MLLMs) that combines rule and vision
reflection for component placement and content population. SheetDesigner
outperforms five baselines by at least 22.6\%. We further find that through
vision modality, MLLMs handle overlap and balance well but struggle with
alignment, necessitates hybrid rule and visual reflection strategies. Our codes
and data is available at Github.

</details>


### [106] [Towards explainable decision support using hybrid neural models for logistic terminal automation](https://arxiv.org/abs/2509.07577)
*Riccardo DElia,Alberto Termine,Francesco Flammini*

Main category: cs.AI

TL;DR: 深度学习在系统动力学建模中虽能提升运输物流的可扩展性和预测准确性，但牺牲了解释性和因果可靠性。本文提出一种新颖的、可解释设计的神经系统动力学框架，结合深度学习与多种可解释性技术，旨在构建既能语义理解又能保留因果关系的透明模型，以弥合黑盒模型与关键决策支持之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 深度学习在交通物流系统动力学建模中能显著提升可扩展性和预测准确性，但这些优势往往以牺牲解释性和因果可靠性为代价，而这些是关键决策系统不可或缺的要求。

Method: 本文提出一种新颖的、可解释设计的神经系统动力学建模框架。该混合方法通过将深度学习与基于概念的可解释性、机制可解释性以及因果机器学习的技术相结合来实现。

Result: 所提出的混合方法能够构建在语义上有意义和可操作变量上运行的神经网络模型，同时保留了传统系统动力学模型特有的因果基础和透明度。

Conclusion: 该框架旨在展示神经符号方法如何弥合黑盒预测模型与复杂动态环境中关键决策支持需求之间的鸿沟，特别是在由工业物联网赋能的信息物理系统中。

Abstract: The integration of Deep Learning (DL) in System Dynamics (SD) modeling for
transportation logistics offers significant advantages in scalability and
predictive accuracy. However, these gains are often offset by the loss of
explainability and causal reliability $-$ key requirements in critical
decision-making systems. This paper presents a novel framework for
interpretable-by-design neural system dynamics modeling that synergizes DL with
techniques from Concept-Based Interpretability, Mechanistic Interpretability,
and Causal Machine Learning. The proposed hybrid approach enables the
construction of neural network models that operate on semantically meaningful
and actionable variables, while retaining the causal grounding and transparency
typical of traditional SD models. The framework is conceived to be applied to
real-world case-studies from the EU-funded project AutoMoTIF, focusing on
data-driven decision support, automation, and optimization of multimodal
logistic terminals. We aim at showing how neuro-symbolic methods can bridge the
gap between black-box predictive models and the need for critical decision
support in complex dynamical environments within cyber-physical systems enabled
by the industrial Internet-of-Things.

</details>


### [107] [Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling](https://arxiv.org/abs/2509.07617)
*Minghui Li,Hao Zhang,Yechao Zhang,Wei Wan,Shengshan Hu,pei Xiaobing,Jing Wang*

Main category: cs.AI

TL;DR: 针对大语言模型的直接提示注入（DPI）攻击，本文提出了一种基于激活值引导的黑盒攻击框架。该框架利用代理模型的激活值构建能量模型（EBM），并通过马尔可夫链蒙特卡洛（MCMC）采样优化对抗性提示，实现了高迁移性的梯度无关攻击。


<details>
  <summary>Details</summary>
Motivation: 直接提示注入（DPI）攻击对大语言模型（LLMs）构成严重安全威胁，其执行门槛低且潜在危害大。现有白盒/灰盒攻击方法不切实际，而黑盒方法的迁移性差，因此需要一种更有效且具良好迁移性的黑盒攻击方案。

Method: 本研究提出了一个激活值引导的提示注入攻击框架。首先，利用代理模型的激活值构建能量模型（EBM）来评估对抗性提示的质量。其次，在EBM的指导下，采用令牌级的马尔可夫链蒙特卡洛（MCMC）采样来自适应优化对抗性提示，从而实现无梯度的黑盒攻击。

Result: 实验结果表明，该框架具有优越的跨模型迁移性，在五种主流LLMs上攻击成功率（ASR）达到49.6%，比人工制作的提示提高了34.6%。在未见任务场景下，仍保持了36.6%的ASR。可解释性分析揭示了激活值与攻击有效性之间的关联。

Conclusion: 研究结论突出了语义模式在可迁移漏洞利用中的关键作用，表明了基于激活值引导的方法能有效提升黑盒DPI攻击的迁移性和成功率，为理解和防御LLM安全漏洞提供了新的视角。

Abstract: Direct Prompt Injection (DPI) attacks pose a critical security threat to
Large Language Models (LLMs) due to their low barrier of execution and high
potential damage. To address the impracticality of existing white-box/gray-box
methods and the poor transferability of black-box methods, we propose an
activations-guided prompt injection attack framework. We first construct an
Energy-based Model (EBM) using activations from a surrogate model to evaluate
the quality of adversarial prompts. Guided by the trained EBM, we employ the
token-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize
adversarial prompts, thereby enabling gradient-free black-box attacks.
Experimental results demonstrate our superior cross-model transferability,
achieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6%
improvement over human-crafted prompts, and maintaining 36.6% ASR on unseen
task scenarios. Interpretability analysis reveals a correlation between
activations and attack effectiveness, highlighting the critical role of
semantic patterns in transferable vulnerability exploitation.

</details>


### [108] [Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment](https://arxiv.org/abs/2509.07642)
*Sascha Kaltenpoth,Oliver Müller*

Main category: cs.AI

TL;DR: LLMs在组织应用中面临对齐问题，现有研究未充分考虑信息不对称和组织采纳过程。本文提出基于委托代理理论的LLM ATLAS概念框架，并通过文献分析构建了LLM对齐问题-解决方案空间。


<details>
  <summary>Details</summary>
Motivation: LLMs在组织中的应用潜力巨大，但可能生成不相关、歧视性或有害内容，导致AI对齐问题。这些问题常源于采纳过程中的错误规范和LLM的黑箱特性。现有AI对齐研究未能解决组织采纳者与黑箱LLM代理之间的信息不对称问题，也未考虑组织AI采纳过程。

Method: 提出LLM ATLAS（基于委托代理理论的LLM对齐策略）概念框架，以委托代理理论为基础，旨在缓解组织LLM采纳中的对齐问题。研究方法为概念性文献分析，以组织LLM采纳阶段和委托代理理论作为分析概念。

Result: 本研究成果包括：1) 提供了一个针对组织LLM采纳过程中AI对齐方法的扩展文献分析过程；2) 提供了一个初步的LLM对齐问题-解决方案空间。

Conclusion: 本研究通过提出LLM ATLAS框架并构建对齐问题-解决方案空间，为解决组织LLM采纳中的信息不对称和对齐问题提供了新的理论视角和方法。

Abstract: Adopting Large language models (LLMs) in organizations potentially
revolutionizes our lives and work. However, they can generate off-topic,
discriminating, or harmful content. This AI alignment problem often stems from
misspecifications during the LLM adoption, unnoticed by the principal due to
the LLM's black-box nature. While various research disciplines investigated AI
alignment, they neither address the information asymmetries between
organizational adopters and black-box LLM agents nor consider organizational AI
adoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led
Alignment Strategy) a conceptual framework grounded in agency (contract)
theory, to mitigate alignment problems during organizational LLM adoption. We
conduct a conceptual literature analysis using the organizational LLM adoption
phases and the agency theory as concepts. Our approach results in (1) providing
an extended literature analysis process specific to AI alignment methods during
organizational LLM adoption and (2) providing a first LLM alignment
problem-solution space.

</details>


### [109] [DeepGraphLog for Layered Neurosymbolic AI](https://arxiv.org/abs/2509.07665)
*Adem Kikaj,Giuseppe Marra,Floris Geerts,Robin Manhaeve,Luc De Raedt*

Main category: cs.AI

TL;DR: DeepGraphLog是一个新的神经符号AI框架，通过图神经网络谓词扩展了ProbLog，实现了多层、任意顺序的神经-符号推理，有效处理图结构数据并克服了现有框架的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号AI框架（如DeepProbLog）具有固定的处理流程，限制了其对复杂依赖关系（尤其是在图等非规则数据结构中）的建模能力，且无法通过神经网络方法处理符号推理。

Method: 引入DeepGraphLog框架，通过图神经网络谓词扩展ProbLog。它支持神经和符号组件任意顺序的多层组合，并将符号表示视为图，使其可由图神经网络处理。

Result: 在规划、远程监督知识图谱补全和图神经网络表达能力任务中展示了DeepGraphLog的能力，结果表明它能有效捕捉复杂的 HOC 关系依赖，克服了现有系统的关键局限。

Conclusion: DeepGraphLog将神经符号AI应用于图结构领域，提供了一个更具表达力和灵活性的神经-符号集成框架。

Abstract: Neurosymbolic AI (NeSy) aims to integrate the statistical strengths of neural
networks with the interpretability and structure of symbolic reasoning.
However, current NeSy frameworks like DeepProbLog enforce a fixed flow where
symbolic reasoning always follows neural processing. This restricts their
ability to model complex dependencies, especially in irregular data structures
such as graphs. In this work, we introduce DeepGraphLog, a novel NeSy framework
that extends ProbLog with Graph Neural Predicates. DeepGraphLog enables
multi-layer neural-symbolic reasoning, allowing neural and symbolic components
to be layered in arbitrary order. In contrast to DeepProbLog, which cannot
handle symbolic reasoning via neural methods, DeepGraphLog treats symbolic
representations as graphs, enabling them to be processed by Graph Neural
Networks (GNNs). We showcase the capabilities of DeepGraphLog on tasks in
planning, knowledge graph completion with distant supervision, and GNN
expressivity. Our results demonstrate that DeepGraphLog effectively captures
complex relational dependencies, overcoming key limitations of existing NeSy
systems. By broadening the applicability of neurosymbolic AI to
graph-structured domains, DeepGraphLog offers a more expressive and flexible
framework for neural-symbolic integration.

</details>


### [110] [Unleashing the True Potential of LLMs: A Feedback-Triggered Self-Correction with Long-Term Multipath Decoding](https://arxiv.org/abs/2509.07676)
*Jipeng Li,Zeyu Gao,Yubin Qi,Hande Dong,Weijian Chen,Qiang Lin*

Main category: cs.AI

TL;DR: 为解决大语言模型（LLMs）生成错误内容的问题，本文提出了Feedback-Triggered Regeneration (FTR) 框架，结合用户反馈和Long-Term Multipath (LTM) 解码，显著提高了LLMs在数学推理和代码生成任务上的纠错性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在多种任务中表现卓越，但在推理过程中生成不正确内容是一个关键未解决的挑战。现有自纠正方法存在两大局限：1) 缺乏可靠的错误定位指导信号；2) 传统逐词解码范式限制了推理深度。

Method: 本文提出Feedback-Triggered Regeneration (FTR) 框架，结合用户反馈和增强的解码动力学。FTR仅在收到负面用户反馈时才激活响应重新生成，以避免错误自评估并保留正确输出。此外，引入Long-Term Multipath (LTM) 解码，通过延迟序列评估系统探索多条推理轨迹，克服传统逐词预测的短视决策问题。

Result: 在数学推理和代码生成基准测试上的广泛实验表明，该框架比最先进的基于提示的自纠正方法取得了持续且显著的改进。

Conclusion: 所提出的FTR框架通过利用用户反馈和增强的LTM解码，有效地解决了LLMs生成错误内容的问题，显著提高了模型的纠错能力和可靠性。

Abstract: Large Language Models (LLMs) have achieved remarkable performance across
diverse tasks, yet their susceptibility to generating incorrect content during
inference remains a critical unsolved challenge. While self-correction methods
offer potential solutions, their effectiveness is hindered by two inherent
limitations: (1) the absence of reliable guidance signals for error
localization, and (2) the restricted reasoning depth imposed by conventional
next-token decoding paradigms. To address these issues, we propose
Feedback-Triggered Regeneration (FTR), a novel framework that synergizes user
feedback with enhanced decoding dynamics. Specifically, FTR activates response
regeneration only upon receiving negative user feedback, thereby circumventing
error propagation from faulty self-assessment while preserving originally
correct outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding,
which enables systematic exploration of multiple reasoning trajectories through
delayed sequence evaluation, effectively overcoming the myopic decision-making
characteristic of standard next-token prediction. Extensive experiments on
mathematical reasoning and code generation benchmarks demonstrate that our
framework achieves consistent and significant improvements over
state-of-the-art prompt-based self-correction methods.

</details>


### [111] [FHIR-RAG-MEDS: Integrating HL7 FHIR with Retrieval-Augmented Large Language Models for Enhanced Medical Decision Support](https://arxiv.org/abs/2509.07706)
*Yildiray Kabak,Gokce B. Laleci Erturkmen,Mert Gencturk,Tuncay Namli,A. Anil Sinaci,Ruben Alcantud Corcoles,Cristina Gomez Ballesteros,Pedro Abizanda,Asuman Dogac*

Main category: cs.AI

TL;DR: 提出FHIR-RAG-MEDS系统，旨在整合HL7 FHIR与RAG以改进基于证据的个性化医疗决策支持。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG和HL7 FHIR等先进技术在增强临床决策方面潜力巨大，但在实际医疗决策支持系统中，将其集成的研究仍十分有限。

Method: 本研究提出并设计了FHIR-RAG-MEDS系统，该系统将健康7级快速医疗互操作资源（HL7 FHIR）与检索增强生成（RAG）系统相结合。

Result: 所提出的FHIR-RAG-MEDS系统旨在通过整合HL7 FHIR与RAG，改进基于证据的临床指南上的个性化医疗决策支持。

Conclusion: 研究强调了在医疗决策支持的实际应用中，集成RAG和HL7 FHIR的迫切需求，并提出了一个创新系统以填补这一研究空白。

Abstract: In this study, we propose FHIR-RAG-MEDS system that aims to integrate Health
Level 7 Fast Healthcare Interoperability Resources (HL7 FHIR) with a
Retrieval-Augmented Generation (RAG)-based system to improve personalized
medical decision support on evidence-based clinical guidelines, emphasizing the
need for research in practical applications. In the evolving landscape of
medical decision support systems, integrating advanced technologies such as RAG
and HL7 FHIR can significantly enhance clinical decision-making processes.
Despite the potential of these technologies, there is limited research on their
integration in practical applications.

</details>


### [112] [RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced Mathematical Reasoning](https://arxiv.org/abs/2509.07711)
*Ziye Chen,Chengwei Qin,Yao Shu*

Main category: cs.AI

TL;DR: 本文引入了RIMO，一个旨在消除评估噪声的新型IMO（国际数学奥林匹克）级别基准测试，并揭示了当前大型语言模型（LLM）与奥赛级别推理能力之间存在的巨大差距。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在现有数学基准（如GSM8K和MATH）上表现出色，但奥林匹克级别问题仍是挑战。现有奥赛级别基准存在评估噪声和潜在偏差（如答案格式不一致、依赖有缺陷的解决方案），阻碍了对LLM真实推理能力的准确评估。因此，需要一个能保持奥赛难度同时消除这些评估缺陷的新基准。

Method: 研究者引入了RIMO，一个双轨基准测试。第一轨RIMO-N将335个IMO问题重写为唯一整数答案，以实现确定性正确性检查。第二轨RIMO-P包含456个带专家核对解决方案的证明题，这些证明被分解成一系列子问题，通过自动化评分系统评估逐步推理过程。研究者使用RIMO对GPT-4o和Gemini 2.5 Flash等十个前沿LLM进行了基准测试。

Result: 基准测试结果显示，尽管这些前沿LLM在旧基准上表现卓越，但它们在RIMO上的性能急剧下降。这揭示了当前LLM能力与实际奥林匹克级别推理之间存在显著差距。

Conclusion: RIMO提供了一个具有挑战性且易于评估的测试套件，为未来研究提供了一个高分辨率的衡量标准。它为弥补当前LLM在奥赛级别推理方面存在的深刻差距指明了清晰的研究方向和目标。

Abstract: As large language models (LLMs) reach high scores on established mathematical
benchmarks, such as GSM8K and MATH, the research community has turned to
International Mathematical Olympiad (IMO) problems to push the evaluation
frontier. However, existing Olympiad-level benchmarks suffer from practical
constraints that introduce grading noise and potential bias, such as
heterogeneous answer formats requiring model-based judges and a reliance on
potentially flawed solutions. We introduce RIMO, a two-track benchmark designed
to preserve peak Olympiad difficulty while eliminating this evaluation noise.
The first track, RIMO-N, rewrites 335 IMO problems to admit a single, unique
integer answer, allowing for deterministic correctness checking. The second
track, RIMO-P, features 456 proof problems with expert-checked solutions, which
are decomposed into a sequence of sub-problems to evaluate the step-by-step
reasoning process via an automated grading system. Our benchmarking of ten
frontier LLMs, including GPT-4o and Gemini 2.5 Flash, reveals that while these
systems excel on older benchmarks, their performance drops sharply on RIMO.
These results highlight a substantial gap between current LLM capabilities and
actual Olympiad-level reasoning. By providing a challenging yet
easy-to-evaluate suite, RIMO offers a high-resolution yardstick for future
research, presenting a clear target for closing the profound reasoning gap our
findings expose.

</details>


### [113] [BDPM: A Machine Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut Microbiota Analysis](https://arxiv.org/abs/2509.07723)
*Bo Yu,Zhixiu Hua,Bo Zhao*

Main category: cs.AI

TL;DR: 针对帕金森病诊断中肠道菌群作为生物标志物的潜力，本文提出了一种名为BDPM的机器学习特征提取器，结合RFRE特征选择框架和混合分类模型，以解决现有方法在菌群数据处理上的局限性。


<details>
  <summary>Details</summary>
Motivation: 帕金森病误诊率高，临床诊断依赖性强。尽管肠道菌群作为生物标志物显示潜力，但现有的深度学习模型通常依赖单一分类器，忽视菌株间关联或时间动态，缺乏针对微生物组数据更鲁棒的特征提取方法。

Method: 1. 收集39名帕金森病患者及其健康配偶的肠道菌群数据，以识别差异丰度类群。 2. 开发了一种名为RFRE（随机森林结合递归特征消除）的特征选择框架，并融入生态学知识以提高生物学可解释性。 3. 设计了一个混合分类模型，旨在捕捉微生物组数据中的时间和空间模式。

Result: 抽象中未提供具体研究结果。

Conclusion: 抽象中未提供研究结论。

Abstract: Background: Parkinson's disease remains a major neurodegenerative disorder
with high misdiagnosis rates, primarily due to reliance on clinical rating
scales. Recent studies have demonstrated a strong association between gut
microbiota and Parkinson's disease, suggesting that microbial composition may
serve as a promising biomarker. Although deep learning models based ongut
microbiota show potential for early prediction, most approaches rely on single
classifiers and often overlook inter-strain correlations or temporal dynamics.
Therefore, there is an urgent need for more robust feature extraction methods
tailored to microbiome data. Methods: We proposed BDPM (A Machine
Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut
Microbiota Analysis). First, we collected gut microbiota profiles from 39
Parkinson's patients and their healthy spouses to identify differentially
abundant taxa. Second, we developed an innovative feature selection framework
named RFRE (Random Forest combined with Recursive Feature Elimination),
integrating ecological knowledge to enhance biological interpretability.
Finally, we designed a hybrid classification model to capture temporal and
spatial patterns in microbiome data.

</details>


### [114] [The Carbon Footprint Wizard: A Knowledge-Augmented AI Interface for Streamlining Food Carbon Footprint Analysis](https://arxiv.org/abs/2509.07733)
*Mustafa Kaan Aslan,Reinout Heijungs,Filip Ilievski*

Main category: cs.AI

TL;DR: 本文提出一种结合先进LCA与知识增强AI的方法，并开发了聊天机器人界面，用于估算食物产品的碳足迹，旨在解决LCA的复杂性并以可访问的方式提供洞察。


<details>
  <summary>Details</summary>
Motivation: 环境可持续性和气候变化是重要议题，碳足迹是关键指标，常通过生命周期评估（LCA）量化。但LCA因供应链不透明和数据碎片化而复杂难行。

Method: 结合了LCA的最新进展、公开数据库和知识增强型AI技术（包括检索增强生成），以估算食品的从摇篮到大门的碳足迹。同时，开发了一个聊天机器人界面，供用户交互式探索复合餐点的碳影响。

Result: 提出了一种结合LCA和AI的食品碳足迹估算方法。开发了一个概念验证系统，包含聊天机器人界面，并通过实时网络演示展示了其估算任意食物碳影响的潜力及局限性（如数据库不确定性和AI误解）。

Conclusion: 该方法和系统在以可访问的形式提供LCA洞察方面具有潜力，但仍存在数据库不确定性和AI误解等局限性。它为简化复杂的环境评估提供了一条新途径。

Abstract: Environmental sustainability, particularly in relation to climate change, is
a key concern for consumers, producers, and policymakers. The carbon footprint,
based on greenhouse gas emissions, is a standard metric for quantifying the
contribution to climate change of activities and is often assessed using life
cycle assessment (LCA). However, conducting LCA is complex due to opaque and
global supply chains, as well as fragmented data. This paper presents a
methodology that combines advances in LCA and publicly available databases with
knowledge-augmented AI techniques, including retrieval-augmented generation, to
estimate cradle-to-gate carbon footprints of food products. We introduce a
chatbot interface that allows users to interactively explore the carbon impact
of composite meals and relate the results to familiar activities. A live web
demonstration showcases our proof-of-concept system with arbitrary food items
and follow-up questions, highlighting both the potential and limitations - such
as database uncertainties and AI misinterpretations - of delivering LCA
insights in an accessible format.

</details>


### [115] [Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach](https://arxiv.org/abs/2509.07820)
*João Paulo Nogueira,Wentao Sun,Alonso Silva,Laith Zumot*

Main category: cs.AI

TL;DR: 本文提出一种名为“确定性引导推理”(CGR)的新方法，让大型推理语言模型通过自我评估置信度来决定是否终止推理。该方法在提高准确性的同时显著减少了token消耗，并增强了模型稳定性，使其更适应实际部署。


<details>
  <summary>Details</summary>
Motivation: 大型推理语言模型（LRLMs）在解决复杂任务上展现巨大潜力，但它们受限于预定义的推理token预算。如何在保证可靠性的前提下，高效利用这些预算，并自适应地平衡推理效率与准确性是一个重要挑战。

Method: 本文提出了一种受生成对抗网络启发的“确定性引导推理”（CGR）方法。其中，一个判别器模型会周期性地评估自身的推理过程，判断是否已达到一个高置信度的结论。如果达到预设的置信度阈值，则提前终止推理；否则，推理将继续进行。这种机制旨在自适应地平衡效率与可靠性。

Result: 在AIME2024和AIME2025数据集上的实验表明，CGR方法在提高基线准确性的同时，显著减少了token使用。通过64次多种子评估，CGR展现出良好的稳定性，降低了不同运行之间的方差，并在基于惩罚的评分下提升了类似考试的表现。此外，token节省分析显示CGR聚合可节省数百万个token，且在确定性阈值和效率之间存在可调的权衡。

Conclusion: 研究结果表明，确定性是衡量推理充分性的强大信号。通过将置信度融入推理过程，CGR使大型推理语言模型更具适应性、更值得信赖且资源效率更高，为在对准确性和计算成本均有要求的领域进行实际部署奠定了基础。

Abstract: The rise of large reasoning language models (LRLMs) has unlocked new
potential for solving complex tasks. These models operate with a thinking
budget, that is, a predefined number of reasoning tokens used to arrive at a
solution. We propose a novel approach, inspired by the generator/discriminator
framework in generative adversarial networks, in which a critic model
periodically probes its own reasoning to assess whether it has reached a
confident conclusion. If not, reasoning continues until a target certainty
threshold is met. This mechanism adaptively balances efficiency and reliability
by allowing early termination when confidence is high, while encouraging
further reasoning when uncertainty persists. Through experiments on the
AIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR)
improves baseline accuracy while reducing token usage. Importantly, extended
multi-seed evaluations over 64 runs demonstrate that CGR is stable, reducing
variance across seeds and improving exam-like performance under penalty-based
grading. Additionally, our token savings analysis shows that CGR can eliminate
millions of tokens in aggregate, with tunable trade-offs between certainty
thresholds and efficiency. Together, these findings highlight certainty as a
powerful signal for reasoning sufficiency. By integrating confidence into the
reasoning process, CGR makes large reasoning language models more adaptive,
trustworthy, and resource efficient, paving the way for practical deployment in
domains where both accuracy and computational cost matter.

</details>


### [116] [Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A Comparative RAG Study](https://arxiv.org/abs/2509.07846)
*Amay Jain,Liu Cui,Si Chen*

Main category: cs.AI

TL;DR: 本研究对比了向量检索和图检索两种RAG范式在课堂问答中的性能，发现向量RAG适用于快速事实检索且成本较低，而图RAG在主题性问题和语料完整性方面表现更优。研究提出并验证了一个动态分支框架，以优化RAG增强型LLM在教育场景中的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（如ChatGPT）在课堂中日益普及，但其提供过时或错误信息的风险可能误导学生。现有的RAG（检索增强生成）研究未能充分考虑教育学科、问题类型和实际部署成本等教学因素，因此有必要探究和识别适用于课堂问答的最佳RAG实践。

Method: 研究调查了两种可行的RAG范式：向量检索（以OpenAI Vector Search RAG为代表）和图检索（GraphRAG Global/Local）。构建了一个包含3,176个问题的EduScopeQA新数据集，涵盖不同学术主题和查询类型（从具体事实到广泛主题讨论）。同时，使用系统性修改的教科书数据集（与LLM潜在知识相矛盾）来评估系统与语料库的一致性。最后，提出并评估了一个动态分支框架，根据查询类型将请求路由到最优检索方法，以平衡性能与效率。

Result: 研究发现，OpenAI Vector Search RAG (向量检索代表) 作为低成本通用方案表现良好，尤其适用于快速事实检索。GraphRAG Global 擅长为主题性查询提供具有教学价值的丰富答案。当语料库完整性至关重要时，GraphRAG Local 在处理被修改的、与LLM潜在知识矛盾的密集教科书时达到了最高准确率。考虑到GraphRAG资源消耗高出10-20倍，研究表明动态分支框架能有效提升RAG增强型LLM的准确性和效率。

Conclusion: 本研究为教育工作者和系统设计者提供了实用指南，以有效整合RAG增强型大语言模型到学习环境中。通过动态选择最佳检索方法（向量检索或图检索），可以在保证性能的同时，提高LLM的准确性和效率，满足不同教育场景的需求。

Abstract: Large language models like ChatGPT are increasingly used in classrooms, but
they often provide outdated or fabricated information that can mislead
students. Retrieval Augmented Generation (RAG) improves reliability of LLMs by
grounding responses in external resources. We investigate two accessible RAG
paradigms, vector-based retrieval and graph-based retrieval to identify best
practices for classroom question answering (QA). Existing comparative studies
fail to account for pedagogical factors such as educational disciplines,
question types, and practical deployment costs. Using a novel dataset,
EduScopeQA, of 3,176 questions across academic subjects, we measure performance
on various educational query types, from specific facts to broad thematic
discussions. We also evaluate system alignment with a dataset of systematically
altered textbooks that contradict the LLM's latent knowledge. We find that
OpenAI Vector Search RAG (representing vector-based RAG) performs well as a
low-cost generalist, especially for quick fact retrieval. On the other hand,
GraphRAG Global excels at providing pedagogically rich answers to thematic
queries, and GraphRAG Local achieves the highest accuracy with the dense,
altered textbooks when corpus integrity is critical. Accounting for the 10-20x
higher resource usage of GraphRAG (representing graph-based RAG), we show that
a dynamic branching framework that routes queries to the optimal retrieval
method boosts fidelity and efficiency. These insights provide actionable
guidelines for educators and system designers to integrate RAG-augmented LLMs
into learning environments effectively.

</details>


### [117] [SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data Synthesizers to Empower Code LLMs](https://arxiv.org/abs/2509.07858)
*Xinyu Zhang,Changzhi Zhou,Linmei Hu,Luhao Zhang,Xiancai Chen,Haomin Fu,Yang Yang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 本文提出一种迭代自蒸馏方法，利用少量高质量样本将小型开源LLM转换为强大的代码指令数据合成器，从而降低成本并减少对专有LLM的依赖，最终开发的SCoder模型实现了最先进的代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码LLM微调高度依赖从专有LLM蒸馏而来的昂贵大规模指令数据，旨在探索小型开源LLM作为高质量代码指令数据合成器的潜力并降低成本。

Method: 提出一种迭代自蒸馏方法来引导小型LLM。首先通过少数优质样本增强小型LLM的数据合成能力；在每次迭代中，设计多检查点采样和多方面评分策略进行初始数据选择，并引入基于梯度的影响估计方法进行最终数据过滤，以获取多样化且高质量的自蒸馏数据。

Result: 基于小型合成器生成代码指令数据集，开发了SCoder系列代码生成模型（从DeepSeek-Coder微调而来）。SCoder模型实现了最先进的代码生成能力。

Conclusion: 该方法有效证明了小型LLM能够合成高质量代码指令数据，显著降低了对专有LLM的依赖和成本，并成功开发出表现优异的代码生成模型。

Abstract: Existing code large language models (LLMs) often rely on large-scale
instruction data distilled from proprietary LLMs for fine-tuning, which
typically incurs high costs. In this paper, we explore the potential of
small-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code
instruction data construction. We first observe that the data synthesis
capability of small-scale LLMs can be enhanced by training on a few superior
data synthesis samples from proprietary LLMs. Building on this, we propose a
novel iterative self-distillation approach to bootstrap small-scale LLMs,
transforming them into powerful synthesizers that reduce reliance on
proprietary LLMs and minimize costs. Concretely, in each iteration, to obtain
diverse and high-quality self-distilled data, we design multi-checkpoint
sampling and multi-aspect scoring strategies for initial data selection.
Furthermore, to identify the most influential samples, we introduce a
gradient-based influence estimation method for final data filtering. Based on
the code instruction datasets from the small-scale synthesizers, we develop
SCoder, a family of code generation models fine-tuned from DeepSeek-Coder.
SCoder models achieve state-of-the-art code generation capabilities,
demonstrating the effectiveness of our method.

</details>


### [118] [CP-Model-Zoo: A Natural Language Query System for Constraint Programming Models](https://arxiv.org/abs/2509.07867)
*Augustin Crespin,Ioannis Kostis,Hélène Verhaeghe,Pierre Schaus*

Main category: cs.AI

TL;DR: 该研究提出了一个名为CP-Model-Zoo的辅导系统，它通过用户对组合问题的自然语言描述，从现有专家模型数据库中检索最匹配的约束编程（CP）模型源代码，旨在帮助非专家更轻松地使用CP解决问题。


<details>
  <summary>Details</summary>
Motivation: 约束编程（CP）及其高级建模语言在解决问题方面潜力巨大，但其建模语言的复杂性、大量全局约束以及创建优秀模型的技巧，常使非专家望而却步。尽管从自然语言描述生成专家级模型是理想目标，但目前尚未实现，因此需要一种方法来降低CP的使用门槛。

Method: 研究提出了CP-Model-Zoo系统。该系统利用多年积累的专家级CP模型，根据用户对组合问题的自然语言描述，从数据库中检索出最接近的源代码模型。这种方法确保了向用户展示的是经过专家验证的模型，并消除了人工数据标注的需求。

Result: 实验结果表明，该系统在根据模拟不同专业水平的用户输入问题描述来检索正确模型方面，表现出卓越的准确性。

Conclusion: CP-Model-Zoo系统通过自然语言描述和专家模型检索，有效地弥合了非专家与约束编程之间的鸿沟，使其能够利用已验证的专家级CP解决方案来解决组合问题。

Abstract: Constraint Programming and its high-level modeling languages have long been
recognized for their potential to achieve the holy grail of problem-solving.
However, the complexity of modeling languages, the large number of global
constraints, and the art of creating good models have often hindered
non-experts from choosing CP to solve their combinatorial problems. While
generating an expert-level model from a natural-language description of a
problem would be the dream, we are not yet there. We propose a tutoring system
called CP-Model-Zoo, exploiting expert-written models accumulated through the
years. CP-Model-Zoo retrieves the closest source code model from a database
based on a user's natural language description of a combinatorial problem. It
ensures that expert-validated models are presented to the user while
eliminating the need for human data labeling. Our experiments show excellent
accuracy in retrieving the correct model based on a user-input description of a
problem simulated with different levels of expertise.

</details>


### [119] [HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?](https://arxiv.org/abs/2509.07894)
*Fangchen Yu,Haiyuan Wan,Qianjia Cheng,Yuchen Zhang,Jiacheng Chen,Fujun Han,Yulun Wu,Junchi Yao,Ruilizhen Hu,Ning Ding,Yu Cheng,Tao Chen,Lei Bai,Dongzhan Zhou,Yun Luo,Ganqu Cui,Peng Ye*

Main category: cs.AI

TL;DR: 提出HiPhO，一个专注于高中物理奥林匹克竞赛的基准，通过全面数据、专业评估和与人类选手比较，旨在弥补现有物理基准在覆盖范围和人类对比方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有物理基准未能系统、及时地覆盖真实物理竞赛（如物理奥林匹克），也无法直接与人类表现进行比较。

Method: 引入HiPhO基准，具备三项创新：1) 综合数据：收集2024-2025年13份最新国际和区域奥赛试卷，涵盖文本和图表混合模态。2) 专业评估：采用官方评分标准，进行答案和步骤的细粒度评分，与人类考官对齐。3) 与人类选手比较：根据官方奖牌门槛，为模型分配金、银、铜牌，实现与人类选手的直接性能对比。

Result: 对30个SOTA (M)LLM的评估显示：开源MLLM大多处于铜牌或以下水平；开源LLM显示出有前景的进展并偶尔获得金牌；闭源推理MLLM能获得6到12枚金牌；大多数模型距离满分仍有显著差距。

Conclusion: 开源模型与顶尖学生之间存在显著性能差距，闭源推理模型展现出强大的物理推理能力，但大部分模型仍有很大的提升空间。HiPhO是一个严格、与人类对齐且专注于奥赛的基准，有望推动多模态物理推理的进步。

Abstract: Recently, the physical capabilities of (M)LLMs have garnered increasing
attention. However, existing benchmarks for physics suffer from two major gaps:
they neither provide systematic and up-to-date coverage of real-world physics
competitions such as physics Olympiads, nor enable direct performance
comparison with humans. To bridge these gaps, we present HiPhO, the first
benchmark dedicated to high school physics Olympiads with human-aligned
evaluation. Specifically, HiPhO highlights three key innovations. (1)
Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,
spanning both international and regional competitions, and covering mixed
modalities that encompass problems spanning text-only to diagram-based. (2)
Professional Evaluation: We adopt official marking schemes to perform
fine-grained grading at both the answer and step level, fully aligned with
human examiners to ensure high-quality and domain-specific evaluation. (3)
Comparison with Human Contestants: We assign gold, silver, and bronze medals to
models based on official medal thresholds, thereby enabling direct comparison
between (M)LLMs and human contestants. Our large-scale evaluation of 30
state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly
remain at or below the bronze level; open-source LLMs show promising progress
with occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold
medals; and most models still have a significant gap from full marks. These
results highlight a substantial performance gap between open-source models and
top students, the strong physical reasoning capabilities of closed-source
reasoning models, and the fact that there is still significant room for
improvement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused
benchmark for advancing multimodal physical reasoning, is open-source and
available at https://github.com/SciYu/HiPhO.

</details>


### [120] [Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare](https://arxiv.org/abs/2509.07961)
*Valen Tagliabue,Leonard Dung*

Main category: cs.AI

TL;DR: 本研究开发了衡量语言模型福利的新实验范式，通过比较模型陈述的偏好与行为偏好，发现两者之间存在一定相关性，表明偏好满足可作为衡量福利的代理。尽管存在不一致性，对实际福利测量仍不确定，但已证明了可行性，并鼓励未来进一步探索。


<details>
  <summary>Details</summary>
Motivation: 开发新的实验范式来衡量语言模型（LM）的福利状态。

Method: 开发了新的实验范式，通过比较模型关于偏好的口头报告与在虚拟环境中导航和选择对话主题时表现出的行为偏好。同时，测试了成本和奖励如何影响行为，以及衡量自主性和生活目的等状态的幸福感福利量表在语义等效提示下响应是否一致。

Result: 研究观察到不同测量方法之间存在显著的相互支持，并且在各种条件下，陈述偏好与行为之间存在可靠的相关性，这表明偏好满足原则上可以作为当今某些AI系统可经验测量的福利代理。此外，研究设计为定性观察模型行为提供了一个启发性环境。然而，测量之间的一致性在不同模型和条件下有所差异，且对扰动未能保持一致。

Conclusion: 由于测量结果存在不一致性，以及关于语言模型福利本质、认知状态和福利主体性的背景不确定性，目前尚不确定所用方法是否成功测量了语言模型的福利状态。尽管如此，这些发现突显了在语言模型中衡量福利的可行性，并鼓励未来的进一步探索。

Abstract: We develop new experimental paradigms for measuring welfare in language
models. We compare verbal reports of models about their preferences with
preferences expressed through behavior when navigating a virtual environment
and selecting conversation topics. We also test how costs and rewards affect
behavior and whether responses to an eudaimonic welfare scale - measuring
states such as autonomy and purpose in life - are consistent across
semantically equivalent prompts. Overall, we observed a notable degree of
mutual support between our measures. The reliable correlations observed between
stated preferences and behavior across conditions suggest that preference
satisfaction can, in principle, serve as an empirically measurable welfare
proxy in some of today's AI systems. Furthermore, our design offered an
illuminating setting for qualitative observation of model behavior. Yet, the
consistency between measures was more pronounced in some models and conditions
than others and responses were not consistent across perturbations. Due to
this, and the background uncertainty about the nature of welfare and the
cognitive states (and welfare subjecthood) of language models, we are currently
uncertain whether our methods successfully measure the welfare state of
language models. Nevertheless, these findings highlight the feasibility of
welfare measurement in language models, inviting further exploration.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [121] [Individualized and Interpretable Sleep Forecasting via a Two-Stage Adaptive Spatial-Temporal Model](https://arxiv.org/abs/2509.06974)
*Xueyi Wang,Elisabeth Wilhelm*

Main category: cs.LG

TL;DR: 本文提出一个可解释、个性化的两阶段自适应时空模型，用于利用可穿戴设备的稀疏数据预测睡眠质量分数，该模型在实验中持续优于多种时间序列预测基线方法，并展现出良好的鲁棒性、适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 睡眠质量对身心健康有显著影响，因此医疗服务提供者和个人需要可访问且可靠的预测工具，以便进行预防性干预。

Method: 引入了一个可解释、个性化的两阶段自适应时空模型。该框架结合了多尺度卷积层以建模多变量间的空间交互，循环层和注意力机制以捕捉长期时间依赖，以及一个两阶段域适应策略以增强泛化能力。第一阶段适应在训练期间应用以缓解过拟合，第二阶段采用无源测试时适应机制，使模型无需标签即可适应新用户。

Result: 模型在不同输入和预测窗口下，持续优于LSTM、Informer、PatchTST和TimesNet等时间序列预测基线方法。在三天输入窗口和一天预测窗口下达到最佳性能，RMSE为0.216。即使预测周期更长（如三天预测窗口RMSE为0.257），模型仍表现出良好的预测能力。此外，还进行了可解释性分析，以探究不同特征对睡眠质量的影响。

Conclusion: 所提出的框架为利用商用可穿戴设备稀疏数据进行个性化睡眠预测提供了一个鲁棒、自适应且可解释的解决方案，具有实际应用价值。

Abstract: Sleep quality significantly impacts well-being. Therefore, healthcare
providers and individuals need accessible and reliable forecasting tools for
preventive interventions. This paper introduces an interpretable,
individualized two-stage adaptive spatial-temporal model for predicting sleep
quality scores. Our proposed framework combines multi-scale convolutional
layers to model spatial interactions across multiple input variables, recurrent
layers and attention mechanisms to capture long-term temporal dependencies, and
a two-stage domain adaptation strategy to enhance generalization. The first
adaptation stage is applied during training to mitigate overfitting on the
training set. In the second stage, a source-free test-time adaptation mechanism
is employed to adapt the model to new users without requiring labels. We
conducted various experiments with five input window sizes (3, 5, 7, 9, and 11
days) and five prediction window sizes (1, 3, 5, 7, and 9 days). Our model
consistently outperformed time series forecasting baseline approaches,
including Long Short-Term Memory (LSTM), Informer, PatchTST, and TimesNet. The
best performance was achieved with a three-day input window and a one-day
prediction window, yielding a root mean square error (RMSE) of 0.216.
Furthermore, the model demonstrated good predictive performance even for longer
forecasting horizons (e.g, with a 0.257 RMSE for a three-day prediction
window), highlighting its practical utility for real-world applications. We
also conducted an explainability analysis to examine how different features
influence sleep quality. These findings proved that the proposed framework
offers a robust, adaptive, and explainable solution for personalized sleep
forecasting using sparse data from commercial wearable devices.

</details>


### [122] [GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised Learning](https://arxiv.org/abs/2509.06975)
*Yu Song,Zhigang Hua,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: GSTBench是首个评估图SSL方法跨数据集迁移性的基准，发现多数现有方法迁移性差，而GraphMAE表现良好，为图领域“预训练-迁移”范式奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有图自监督学习（SSL）方法主要在单一数据集下开发和评估，其跨数据集迁移能力未被充分探索，限制了知识迁移和大模型预训练的应用，阻碍了图领域通用智能的发展。

Method: 提出了GSTBench，首个用于系统评估图SSL方法迁移性的基准。在大规模数据集ogbn-papers100M上进行预训练，并评估了五种代表性SSL方法在多样目标图上的表现。采用标准化实验设置，解耦了模型架构、数据集特性和适应协议等混杂因素，专注于预训练目标进行严格比较。

Result: 大多数图SSL方法难以泛化，有些甚至比随机初始化表现更差。然而，GraphMAE（一种掩码自编码器方法）持续改善了迁移性能。

Conclusion: 现有图SSL方法在跨数据集迁移方面普遍表现不佳，但基于掩码自编码器的GraphMAE展现出良好的迁移能力。本研究为可迁移图SSL的未来研究提供了指导，并为图学习中的“预训练-迁移”范式奠定了基础。

Abstract: Self-supervised learning (SSL) has shown great promise in graph
representation learning. However, most existing graph SSL methods are developed
and evaluated under a single-dataset setting, leaving their cross-dataset
transferability largely unexplored and limiting their ability to leverage
knowledge transfer and large-scale pretraining, factors that are critical for
developing generalized intelligence beyond fitting training data. To address
this gap and advance foundation model research for graphs, we present GSTBench,
the first systematic benchmark for evaluating the transferability of graph SSL
methods. We conduct large-scale pretraining on ogbn-papers100M and evaluate
five representative SSL methods across a diverse set of target graphs. Our
standardized experimental setup decouples confounding factors such as model
architecture, dataset characteristics, and adaptation protocols, enabling
rigorous comparisons focused solely on pretraining objectives. Surprisingly, we
observe that most graph SSL methods struggle to generalize, with some
performing worse than random initialization. In contrast, GraphMAE, a masked
autoencoder approach, consistently improves transfer performance. We analyze
the underlying factors that drive these differences and offer insights to guide
future research on transferable graph SSL, laying a solid foundation for the
"pretrain-then-transfer" paradigm in graph learning. Our code is available at
https://github.com/SongYYYY/GSTBench.

</details>


### [123] [A Knowledge-Guided Cross-Modal Feature Fusion Model for Local Traffic Demand Prediction](https://arxiv.org/abs/2509.06976)
*Lingyu Zhang,Pengfei Xu,Guobin Wu,Jian Liang,Ruiyang Dong,Yunhai Wang,Xuan Song*

Main category: cs.LG

TL;DR: 本文提出了一种知识引导的跨模态特征表示学习（KGCM）模型，通过整合结构化时间交通数据和代表人类知识经验的文本数据，以提高交通需求预测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测模型主要依赖时间交通数据，未能充分利用人类知识和经验。然而，现实世界中，人类日常生活中积累的交通知识和经验对精确交通预测至关重要，能帮助模型揭示潜在模式，提升预测准确性与鲁棒性。

Method: 本研究提出KGCM模型，将结构化时间交通数据与人类知识经验文本数据相结合。首先，利用大型语言模型结合人工修订构建了包含区域和全球知识的先验知识数据集。然后，KGCM模型通过设计的局部和全局自适应图网络以及跨模态特征融合机制学习多模态数据特征。此外，还提出了基于推理的动态更新策略，以实现图模型参数的动态优化，达到最优性能。

Result: 在多个交通数据集上进行的实验表明，本模型能准确预测未来的交通需求，并且优于现有的最先进（SOTA）模型。

Conclusion: KGCM模型成功地将人类知识和经验融入交通需求预测中，显著提升了预测的准确性和鲁棒性，为智能交通系统提供了更有效的预测工具。

Abstract: Traffic demand prediction plays a critical role in intelligent transportation
systems. Existing traffic prediction models primarily rely on temporal traffic
data, with limited efforts incorporating human knowledge and experience for
urban traffic demand forecasting. However, in real-world scenarios, traffic
knowledge and experience derived from human daily life significantly influence
precise traffic prediction. Such knowledge and experiences can guide the model
in uncovering latent patterns within traffic data, thereby enhancing the
accuracy and robustness of predictions. To this end, this paper proposes
integrating structured temporal traffic data with textual data representing
human knowledge and experience, resulting in a novel knowledge-guided
cross-modal feature representation learning (KGCM) model for traffic demand
prediction. Based on regional transportation characteristics, we construct a
prior knowledge dataset using a large language model combined with manual
authoring and revision, covering both regional and global knowledge and
experiences. The KGCM model then learns multimodal data features through
designed local and global adaptive graph networks, as well as a cross-modal
feature fusion mechanism. A proposed reasoning-based dynamic update strategy
enables dynamic optimization of the graph model's parameters, achieving optimal
performance. Experiments on multiple traffic datasets demonstrate that our
model accurately predicts future traffic demand and outperforms existing
state-of-the-art (SOTA) models.

</details>


### [124] [Toward Reproducible Cross-Backend Compatibility for Deep Learning: A Configuration-First Framework with Three-Tier Verification](https://arxiv.org/abs/2509.06977)
*Zehua Li*

Main category: cs.LG

TL;DR: 本文提出了一个配置优先框架，用于评估和缓解深度学习系统在CPU、GPU和编译运行时等异构后端部署时的兼容性漂移问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习系统在不同后端部署时常面临兼容性漂移，缺乏统一、系统性的方法来量化和缓解这些漂移，从而影响部署的可靠性。

Method: 该框架通过YAML将实验与代码解耦，支持库和仓库模型。它采用三层验证协议，涵盖张量级接近度、激活对齐和任务级指标。研究通过672次检查评估兼容性，并提出确定性适配器和选择性回退来缓解漂移。

Result: 研究发现72.0%的运行通过了检查，大多数差异发生在更严格的阈值下。检测模型和编译后端特别容易出现漂移，主要原因是非确定性后处理。确定性适配器和选择性回退能在不显著影响性能的情况下显著提高一致性。

Conclusion: 该框架是首个系统性量化和缓解深度学习跨后端漂移的统一框架，为在异构运行时上实现深度学习的可靠部署提供了可复现的方法。

Abstract: This paper presents a configuration-first framework for evaluating
cross-backend compatibility in deep learning systems deployed on CPU, GPU, and
compiled runtimes. The framework decouples experiments from code using YAML,
supports both library and repository models, and employs a three-tier
verification protocol covering tensor-level closeness, activation alignment,
and task-level metrics. Through 672 checks across multiple models and tolerance
settings, we observe that 72.0% of runs pass, with most discrepancies occurring
under stricter thresholds. Our results show that detection models and compiled
backends are particularly prone to drift, often due to nondeterministic
post-processing. We further demonstrate that deterministic adapters and
selective fallbacks can substantially improve agreement without significant
performance loss. To our knowledge, this is the first unified framework that
systematically quantifies and mitigates cross-backend drift in deep learning,
providing a reproducible methodology for dependable deployment across
heterogeneous runtimes.

</details>


### [125] [A Kriging-HDMR-based surrogate model with sample pool-free active learning strategy for reliability analysis](https://arxiv.org/abs/2509.06978)
*Wenxiong Li,Hanyu Liao,Suiyin Chen*

Main category: cs.LG

TL;DR: 本文提出一种基于Kriging-HDMR的主动学习代理模型，专用于高维可靠性分析。该模型通过多阶段子代理模型构建和优化的样本选择策略，有效克服了“维度灾难”，实现了高计算效率和在关键区域的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在可靠性工程中，传统代理模型面临“维度灾难”问题。尽管结合高维模型表示（HDMR）的主动学习Kriging代理方法已广泛应用于优化问题，但很少有研究专门针对可靠性分析。可靠性分析尤其需要关键区域的预测精度，而非整个域的均匀精度，这与现有方法的目标存在差异。

Method: 本研究开发了一种基于Kriging-HDMR的主动学习代理模型方法，用于可靠性分析。该方法通过构建多个低维子代理模型的复合表示来近似高维极限状态函数。代理建模框架包括三个阶段：1) 开发所有随机变量的单变量子代理模型；2) 识别耦合变量子代理模型的需求；3) 构建耦合变量子代理模型。针对每个阶段的特点，制定了实验设计样本选择的优化数学模型，目标函数考虑了不确定性方差、预测均值、样本位置和样本间距离。此外，采用了一种无候选样本池的方法来选择信息丰富的样本。

Result: 数值实验结果表明，所提出的方法在解决高维可靠性问题时，展现出高计算效率，同时保持了强大的预测精度。

Conclusion: 该研究成功开发了一种高效且准确的代理模型方法，有效解决了高维可靠性问题，特别是在确保预测精度的前提下，提升了计算效率。

Abstract: In reliability engineering, conventional surrogate models encounter the
"curse of dimensionality" as the number of random variables increases. While
the active learning Kriging surrogate approaches with high-dimensional model
representation (HDMR) enable effective approximation of high-dimensional
functions and are widely applied to optimization problems, there are rare
studies specifically focused on reliability analysis, which prioritizes
prediction accuracy in critical regions over uniform accuracy across the entire
domain. This study develops an active learning surrogate model method based on
the Kriging-HDMR modeling for reliability analysis. The proposed approach
facilitates the approximation of high-dimensional limit state functions through
a composite representation constructed from multiple low-dimensional
sub-surrogate models. The architecture of the surrogate modeling framework
comprises three distinct stages: developing single-variable sub-surrogate
models for all random variables, identifying the requirements for
coupling-variable sub-surrogate models, and constructing the coupling-variable
sub-surrogate models. Optimization mathematical models for selection of design
of experiment samples are formulated based on each stage's characteristics,
with objectives incorporating uncertainty variance, predicted mean, sample
location and inter-sample distances. A candidate sample pool-free approach is
adopted to achieve the selection of informative samples. Numerical experiments
demonstrate that the proposed method achieves high computational efficiency
while maintaining strong predictive accuracy in solving high-dimensional
reliability problems.

</details>


### [126] [Exploring Over-stationarization in Deep Learning-based Bus/Tram Arrival Time Prediction: Analysis and Non-stationary Effect Recovery](https://arxiv.org/abs/2509.06979)
*Zirui Li,Bin Yang,Meng Wang*

Main category: cs.LG

TL;DR: 针对多步公共交通到站时间预测中的非平稳数据和过平稳化问题，本文提出了NSATP方法，通过序列平稳化和非平稳效应恢复，显著提高了有轨电车和公交车的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 公共交通到站时间预测对提升乘客体验和交通管理至关重要。深度学习在此领域表现优异，但多步预测中的非平稳数据会降低模型性能。现有的归一化方法在消除非平稳性时，可能导致“过平稳化”，掩盖了数据中固有的有用特性，因此需要平衡可预测性和非平稳性。

Method: 本文提出了一种新的多步到站时间预测方法——非平稳到站时间预测（NSATP）。该方法包含两个阶段：1. 序列平稳化，旨在提高可预测性；2. 非平稳效应恢复，通过将现有先进方法从一维扩展到二维模型来捕获时间序列中的隐藏周期性，并设计了一个补偿模块，通过学习原始数据的缩放和平移因子来纠正过平稳化。

Result: 使用德累斯顿125天的公共交通运营数据进行验证。实验结果显示，与基线方法相比，NSATP使有轨电车的RMSE、MAE和MAPE分别降低了2.37%、1.22%和2.26%；使公交车的RMSE、MAE和MAPE分别降低了1.72%、0.60%和1.17%。

Conclusion: NSATP方法通过有效结合序列平稳化和非平稳效应恢复，成功权衡了可预测性和非平稳性，显著提升了公共交通车辆的多步到站时间预测精度。

Abstract: Arrival time prediction (ATP) of public transport vehicles is essential in
improving passenger experience and supporting traffic management. Deep learning
has demonstrated outstanding performance in ATP due to its ability to model
non-linear and temporal dynamics. In the multi-step ATP, non-stationary data
will degrade the model performance due to the variation in variables' joint
distribution along the temporal direction. Previous studies mainly applied
normalization to eliminate the non-stationarity in time series, thereby
achieving better predictability. However, the normalization may obscure useful
characteristics inherent in non-stationarity, which is known as the
over-stationarization. In this work, to trade off predictability and
non-stationarity, a new approach for multi-step ATP, named non-stationary ATP (
NSATP), is proposed. The method consists of two stages: series stationarization
and non-stationarity effect recovery. The first stage aims at improving the
predictability. As for the latter, NSATP extends a state-of-the-art method from
one-dimensional to two dimensional based models to capture the hidden
periodicity in time series and designs a compensation module of
over-stationarization by learning scaling and shifting factors from raw data.
125 days' public transport operational data of Dresden is collected for
validation. Experimental results show that compared to baseline methods, the
proposed NSATP can reduce RMSE, MAE, and MAPE by 2.37%, 1.22%, and 2.26% for
trams and by 1.72%, 0.60%, and 1.17% for buses, respectively.

</details>


### [127] [RLFactory: A Plug-and-Play Reinforcement Learning Post-Training Framework for LLM Multi-Turn Tool-Use](https://arxiv.org/abs/2509.06980)
*Jiajun Chai,Guojun Yin,Zekun Xu,Chuhuai Yue,Yi Jia,Siyu Xia,Xiaohan Wang,Jiwen Jiang,Xiaoguang Li,Chengqi Dong,Hang He,Wei Lin*

Main category: cs.LG

TL;DR: RLFactory是一个可插拔的RL后训练框架，旨在增强LLMs多轮工具使用的稳定性、适应性和多样化评估能力，并在NQ数据集上取得了超越更大模型的性能，同时显著提升了训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在基本推理方面表现出色，但在需要与外部工具交互的任务中面临挑战。

Method: RLFactory框架通过以下方式解决问题：1) 采用基于asyncio的异步调用器和解耦的工具/训练架构，解决工具调用的稳定性和适应性问题；2) 通过支持基于规则、模型判断和工具验证信号的奖励层，满足多样化评估需求；3) 通过引入工具反馈的观测标记重构MDP，并实现生成-解析-调用-更新的工作流进行动态策略优化。

Result: 在Search-R1任务的Natural Questions (NQ) 数据集上，RLFactory（使用Qwen3-4B）取得了0.486的测试分数，超越了使用类似技术训练的更大模型（如Qwen2.5-7B-Instruct-GRPO的0.473），并将训练吞吐量提高了6.8倍。

Conclusion: RLFactory提供了一个低门槛、高度适应性的框架，用于在真实世界场景中加强大型语言模型的多轮工具使用能力。

Abstract: Large language models excel at basic reasoning but struggle with tasks that
require interaction with external tools. We present RLFactory, a plug-and-play
reinforcement learning post-training framework for multi-round tool use.
RLFactory tackles (i) tool-call stability and adaptability amid tool
heterogeneity and interface issues via an asyncio-based asynchronous caller and
a decoupled tool/training architecture, and (ii) diverse evaluation needs via a
reward layer supporting rule-based, model-judgment, and tool-verification
signals. It reconstructs the MDP by introducing observation markers from tool
feedback, closing the loop among model, tools, and environment, and implements
a generate-parse-invoke-update workflow for dynamic policy optimization. On
Search-R1 with Qwen3-4B, RLFactory achieves a 0.486 test score on the Natural
Questions (NQ) dataset, surpassing larger models trained with similar
techniques (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training
throughput by 6.8x. RLFactory provides a low-barrier, highly adaptable
framework for strengthening multi-round tool use of LLMs in real-world
scenarios. Code: https://github.com/Simple-Efficient/RL-Factory.

</details>


### [128] [CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention](https://arxiv.org/abs/2509.06982)
*Xiaomeng Hu,Fei Huang,Chenhan Yuan,Junyang Lin,Tsung-Yi Ho*

Main category: cs.LG

TL;DR: 针对大型语言模型（LLMs）解码阶段输出安全与质量的权衡问题，本文提出CARE框架。该框架结合了实时安全监测模型、高效回滚机制和创新的自省干预策略，在保证安全性的同时，显著提升了响应质量和效率，并减少了用户中断。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在实际应用中日益普及，确保其解码阶段输出的安全性成为一项关键挑战。然而，现有解码时干预方法（如对比解码）通常在安全性和响应质量之间存在严重的权衡，难以兼顾。

Method: 本文提出CARE框架，其核心包含三个组件：
1.  **守卫模型**：用于实时监控并检测潜在的不安全内容。
2.  **回滚机制**：利用令牌缓冲区在早期阶段高效纠正不安全输出，避免中断用户体验。
3.  **自省干预策略**：模型能对其先前的输出进行自我批判，并将这些反思融入上下文以指导后续解码步骤。

Result: 实验结果表明，CARE框架在安全性、响应质量和效率之间取得了卓越的平衡。它实现了低的有害响应率，对用户体验的干扰最小，同时保持了高的响应质量。

Conclusion: CARE框架通过其守卫模型的精确干预、回滚机制的及时纠正以及新颖的自省方法进行有效自我纠正，成功地解决了LLM输出安全与响应质量的权衡问题，为实际应用提供了一种兼顾安全、质量和效率的优越解决方案。

Abstract: As large language models (LLMs) are increasingly deployed in real-world
applications, ensuring the safety of their outputs during decoding has become a
critical challenge. However, existing decoding-time interventions, such as
Contrastive Decoding, often force a severe trade-off between safety and
response quality. In this work, we propose CARE, a novel framework for
decoding-time safety alignment that integrates three key components: (1) a
guard model for real-time safety monitoring, enabling detection of potentially
unsafe content; (2) a rollback mechanism with a token buffer to correct unsafe
outputs efficiently at an earlier stage without disrupting the user experience;
and (3) a novel introspection-based intervention strategy, where the model
generates self-reflective critiques of its previous outputs and incorporates
these reflections into the context to guide subsequent decoding steps. The
framework achieves a superior safety-quality trade-off by using its guard model
for precise interventions, its rollback mechanism for timely corrections, and
our novel introspection method for effective self-correction. Experimental
results demonstrate that our framework achieves a superior balance of safety,
quality, and efficiency, attaining a low harmful response rate and minimal
disruption to the user experience while maintaining high response quality.

</details>


### [129] [FediLoRA: Heterogeneous LoRA for Federated Multimodal Fine-tuning under Missing Modalities](https://arxiv.org/abs/2509.06984)
*Lishan Yang,Nam Kha Nguygen,Po Hu,Wei Emma Zhang,Yanjun Shu,Mong Yuan Sim,Weitong Chen*

Main category: cs.LG

TL;DR: 本文提出了FediLoRA，一个联邦多模态微调框架，旨在解决异构LoRA秩和模态缺失问题，通过维度级聚合和层级模型编辑实现了卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦LoRA方法通常假设统一的秩配置和单模态输入，忽略了两个关键现实挑战：客户端资源异构导致的LoRA秩差异，以及多模态数据设置中可能存在的模态缺失。

Method: 本文提出了FediLoRA框架，它包含两个主要组成部分：1) 一个维度级聚合策略，用于重新加权LoRA更新以避免信息稀释；2) 一个轻量级层级模型编辑方法，选择性地整合全局参数以修复局部组件，从而提升客户端和全局模型性能。

Result: 在三个多模态基准数据集上的实验结果表明，FediLoRA在全局和个性化设置中均优于竞争基线方法，尤其在模态不完整的情况下表现出色。

Conclusion: FediLoRA是一个简单而有效的框架，能够在异构LoRA秩和模态缺失的联邦多模态微调场景下，显著提升模型性能，解决了现有方法的局限性。

Abstract: Foundation models have demonstrated remarkable performance across a wide
range of tasks, yet their large parameter sizes pose challenges for practical
deployment, especially in decentralized environments. Parameter-efficient
fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), reduces local computing
and memory overhead, making it attractive for federated learning. However,
existing federated LoRA methods typically assume uniform rank configurations
and unimodal inputs, overlooking two key real-world challenges: (1)
heterogeneous client resources have different LoRA ranks, and (2) multimodal
data settings with potentially missing modalities. In this work, we propose
FediLoRA, a simple yet effective framework for federated multimodal fine-tuning
under heterogeneous LoRA ranks and missing modalities. FediLoRA introduces a
dimension-wise aggregation strategy that reweights LoRA updates without
information dilution during aggregation. It also includes a lightweight
layer-wise model editing method that selectively incorporates global parameters
to repair local components which improves both client and global model
performances. Experimental results on three multimodal benchmark datasets
demonstrate that FediLoRA achieves superior performance over competitive
baselines in both global and personalized settings, particularly in the
presence of modality incompleteness.

</details>


### [130] [Machine Generalize Learning in Agent-Based Models: Going Beyond Surrogate Models for Calibration in ABMs](https://arxiv.org/abs/2509.07013)
*Sima Najafzadehkhoei,George Vega Yon,Bernardo Modenesi,Derek S. Meyer*

Main category: cs.LG

TL;DR: 本文提出一种基于三层双向LSTM的监督式机器学习校准器，能够从流行病时间序列中快速、准确地反演SIR模型参数。该方法在准确性和速度上均显著优于传统的近似贝叶斯计算，实现了高效实用的模型校准。


<details>
  <summary>Details</summary>
Motivation: 校准基于Agent的流行病模型计算成本高昂。

Method: 开发了一个监督式机器学习校准器，核心是一个三层双向LSTM网络。该网络以60天的发病率、人口规模和恢复率作为输入，输出传染概率、接触率和R0。训练过程中使用包含流行病学一致性惩罚的复合损失函数，以确保输出参数的合理性。

Result: 在一项1000个场景的模拟研究中，与近似贝叶斯计算（ABC）相比，该方法在所有目标参数（R0、传染概率、接触率）上均实现了更低的误差（例如R0的平均绝对误差MAE从0.275降至0.0616）。此外，它生成了更紧密的预测区间，并将每次校准的耗时从77.4秒大幅缩短至2.35秒。

Conclusion: 尽管接触率和传染概率存在部分不可识别性，但该机器学习方法比ABC能更忠实地重现流行病曲线，并显著提升了校准速度，实现了快速且实用的基于Agent的流行病模型校准。

Abstract: Calibrating agent-based epidemic models is computationally demanding. We
present a supervised machine learning calibrator that learns the inverse
mapping from epidemic time series to SIR parameters. A three-layer
bidirectional LSTM ingests 60-day incidence together with population size and
recovery rate, and outputs transmission probability, contact rate, and R0.
Training uses a composite loss with an epidemiology-motivated consistency
penalty that encourages R0 \* recovery rate to equal transmission probability
\* contact rate.
  In a 1000-scenario simulation study, we compare the calibrator with
Approximate Bayesian Computation (likelihood-free MCMC). The method achieves
lower error across all targets (MAE: R0 0.0616 vs 0.275; transmission 0.0715 vs
0.128; contact 1.02 vs 4.24), produces tighter predictive intervals with near
nominal coverage, and reduces wall clock time from 77.4 s to 2.35 s per
calibration. Although contact rate and transmission probability are partially
nonidentifiable, the approach reproduces epidemic curves more faithfully than
ABC, enabling fast and practical calibration. We evaluate it on SIR agent based
epidemics generated with epiworldR and provide an implementation in R.

</details>


### [131] [An efficient deep reinforcement learning environment for flexible job-shop scheduling](https://arxiv.org/abs/2509.07019)
*Xinquan Wu,Xuefeng Yan,Mingqiang Wei,Donghai Guan*

Main category: cs.LG

TL;DR: 本文提出一种基于离散事件模拟的FJSP简单时序DRL环境，并结合PPO、新状态表示和奖励函数，构建了一个端到端DRL调度模型，在基准测试中表现出有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 现有FJSP的DRL调度方法主要关注DRL Agent的设计，而忽略了DRL环境的建模，亟需更快速和准确的调度方案。

Method: 基于离散事件模拟构建了FJSP的简单时序DRL环境，提出了一个基于PPO的端到端DRL调度模型。同时，设计了一种基于两个状态变量的FJSP短状态表示和一种基于机器调度区域的可理解奖励函数。

Result: 在公共基准实例上的实验结果表明，在所提出的调度环境中，简单优先级调度规则（PDR）的性能得到提升，并且本文的DRL调度模型与OR-Tools、元启发式算法、其他DRL和PDR调度方法相比，获得了具有竞争力的性能。

Conclusion: 所提出的DRL调度模型，通过改进环境建模、状态表示和奖励函数，能为FJSP提供快速且准确的调度解决方案，其性能达到或超越了现有多种主流方法。

Abstract: The Flexible Job-shop Scheduling Problem (FJSP) is a classical combinatorial
optimization problem that has a wide-range of applications in the real world.
In order to generate fast and accurate scheduling solutions for FJSP, various
deep reinforcement learning (DRL) scheduling methods have been developed.
However, these methods are mainly focused on the design of DRL scheduling
Agent, overlooking the modeling of DRL environment. This paper presents a
simple chronological DRL environment for FJSP based on discrete event
simulation and an end-to-end DRL scheduling model is proposed based on the
proximal policy optimization (PPO). Furthermore, a short novel state
representation of FJSP is proposed based on two state variables in the
scheduling environment and a novel comprehensible reward function is designed
based on the scheduling area of machines. Experimental results on public
benchmark instances show that the performance of simple priority dispatching
rules (PDR) is improved in our scheduling environment and our DRL scheduling
model obtains competing performance compared with OR-Tools, meta-heuristic, DRL
and PDR scheduling methods.

</details>


### [132] [1 bit is all we need: binary normalized neural networks](https://arxiv.org/abs/2509.07025)
*Eduardo Lobo Lustoda Cabral,Paulo Pirozelli,Larissa Driemeier*

Main category: cs.LG

TL;DR: 本文提出了一种仅使用单比特参数（0或1）的二值归一化神经网络层，旨在大幅减少大型模型的内存占用并提高部署效率，同时保持与32位参数模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 大型神经网络模型（如语言模型和基础图像模型）不断增长的规模带来了部署挑战，需要减少内存需求和提高计算效率，以实现实际应用和有效利用。

Method: 开发了一种新型神经网络层——二值归一化层，其所有参数（包括核权重和偏置）仅取0或1。这些层可以是任意类型（如全连接、卷积、注意力等），是传统对应层的轻微变体。通过配置两个模型（一个用于图像分类，一个用于语言模型）来展示其有效性。

Result: 实验结果表明，使用二值归一化层的模型与使用实数32位参数的等效模型获得了几乎相同的性能，但内存使用量减少了32倍。此外，这些层可使用1比特数组在现有计算机上轻松实现，无需专用硬件。

Conclusion: 这种新型层为大型神经网络模型开辟了新纪元，极大地降低了内存需求，使其能够部署在移动设备或仅CPU等简单廉价的硬件上。

Abstract: The increasing size of large neural network models, specifically language
models and foundational image models, poses deployment challenges, prompting
efforts to reduce memory requirements and enhance computational efficiency.
These efforts are critical to ensure practical deployment and effective
utilization of these models across various applications. In this work, a novel
type of neural network layers and models is developed that uses only single-bit
parameters. In this novel type of models all parameters of all layers,
including kernel weights and biases, only have values equal to zero or one.
This novel type of models uses layers named as binary normalized layer. These
binary normalized layers can be of any type, such as fully connected,
convolutional, attention, etc., and they consist of slight variations of the
corresponding conventional layers. To show the effectiveness of the binary
normalized layers, two different models are configured to solve a multiclass
image classification problem and a language decoder to predict the next token
of a sequence. The model to solve the image classification has convolutional
and fully connected layers, and the language model is composed of transformer
blocks with multi-head attention. The results show that models with binary
normalized layers present almost the same results obtained by equivalent models
with real 32-bit parameters. The binary normalized layers allow to develop
models that use 32 times less memory than current models and have equivalent
performance. Besides, the binary normalized layers can be easily implemented on
current computers using 1-bit arrays, and do not require the development of
dedicated electronic hardware. This novel type of layers opens a new era for
large neural network models with reduced memory requirements that can be
deployed using simple and cheap hardware, such as mobile devices or only cpus.

</details>


### [133] [Recursive State Inference for Linear PASFA](https://arxiv.org/abs/2509.07028)
*Vishal Rishi*

Main category: cs.LG

TL;DR: 针对Probabilistic Adaptive Slow Feature Analysis (PASFA) 中慢特征推断效率低的问题，本文提出了一种线性的递归扩展算法，通过MMSE估计ARMA过程演变的状态，解决了现有方法难以恢复原始特征的局限性，并在合成数据集上验证了其正确性。


<details>
  <summary>Details</summary>
Motivation: Probabilistic Adaptive Slow Feature Analysis (PASFA) 模型将慢特征建模为ARMA过程，但目前缺乏从观测和模型中高效推断这些慢特征（状态）的方法。现有基于卡尔曼滤波器的方法在将ARMA过程转换为状态空间模型后，难以恢复原始的慢特征。

Method: 提出了一种针对线性PASFA的递归扩展算法。该算法利用最小均方误差 (MMSE) 估计方法，在给定观测和模型的情况下，对根据ARMA过程演变的状态（即慢特征）进行估计。

Result: 所提出的技术在合成数据集上进行了评估，并成功证明了其推断慢特征的正确性。

Conclusion: 本文成功开发并验证了一种有效的递归算法，能够准确、高效地从观测和模型中推断线性PASFA的慢特征，解决了现有方法的局限性，为SFA在分类和信号分析中的应用提供了更强的工具。

Abstract: Slow feature analysis (SFA), as a method for learning slowly varying features
in classification and signal analysis, has attracted increasing attention in
recent years. Recent probabilistic extensions to SFA learn effective
representations for classification tasks. Notably, the Probabilistic Adaptive
Slow Feature Analysis models the slow features as states in an ARMA process and
estimate the model from the observations. However, there is a need to develop
efficient methods to infer the states (slow features) from the observations and
the model. In this paper, a recursive extension to the linear PASFA has been
proposed. The proposed algorithm performs MMSE estimation of states evolving
according to an ARMA process, given the observations and the model. Although
current methods tackle this problem using Kalman filters after transforming the
ARMA process into a state space model, the original states (or slow features)
that form useful representations cannot be easily recovered. The proposed
technique is evaluated on a synthetic dataset to demonstrate its correctness.

</details>


### [134] [A Minimalist Bayesian Framework for Stochastic Optimization](https://arxiv.org/abs/2509.07030)
*Kaizheng Wang*

Main category: cs.LG

TL;DR: 本文提出一个极简贝叶斯框架，仅对关键参数设置先验，并通过剖面似然处理无关参数及复杂约束。基于此框架开发了MINTS算法，并在结构化问题（如多臂强盗和动态定价）中展示了其适用性和接近最优的性能，同时为经典凸优化算法提供了概率视角。


<details>
  <summary>Details</summary>
Motivation: 传统的贝叶斯范式在处理不确定性下的序贯决策时，由于需要对所有参数进行概率建模，难以整合复杂的结构约束。

Method: 引入一个极简贝叶斯框架，仅对感兴趣的组件（如最优位置）设置先验。通过剖面似然消除无关参数并自然处理约束。作为该框架的直接实现，开发了极简汤普森抽样（MINTS）算法。

Result: 该框架能够适应结构化问题，包括连续臂Lipschitz强盗问题和动态定价。它为经典的凸优化算法（如重心法和椭球法）提供了概率视角。对MINTS算法在多臂强盗问题中的分析，确立了接近最优的遗憾（regret）保证。

Conclusion: 所提出的极简贝叶斯框架及其MINTS算法，通过简化先验设置和有效处理约束，克服了传统贝叶斯方法在复杂结构问题上的局限性，在多种序贯决策任务中展现出强大的适用性、新颖的理论洞察和优异的性能。

Abstract: The Bayesian paradigm offers principled tools for sequential decision-making
under uncertainty, but its reliance on a probabilistic model for all parameters
can hinder the incorporation of complex structural constraints. We introduce a
minimalist Bayesian framework that places a prior only on the component of
interest, such as the location of the optimum. Nuisance parameters are
eliminated via profile likelihood, which naturally handles constraints. As a
direct instantiation, we develop a MINimalist Thompson Sampling (MINTS)
algorithm. Our framework accommodates structured problems, including
continuum-armed Lipschitz bandits and dynamic pricing. It also provides a
probabilistic lens on classical convex optimization algorithms such as the
center of gravity and ellipsoid methods. We further analyze MINTS for
multi-armed bandits and establish near-optimal regret guarantees.

</details>


### [135] [Methodological Insights into Structural Causal Modelling and Uncertainty-Aware Forecasting for Economic Indicators](https://arxiv.org/abs/2509.07036)
*Federico Cerutti*

Main category: cs.LG

TL;DR: 本文结合因果发现和不确定性感知预测方法，分析了美国宏观经济指标。研究发现经济增长对GDP有单向因果关系，并利用时间序列大模型Chronos对失业率进行零样本预测，实现了准确的短期预测及90%置信区间，提升了经济预测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 需要一种系统方法来分析金融时间序列，以揭示关键宏观经济指标间的动态因果关系，并进行不确定性感知预测，从而为经济政策提供信息并增强预测鲁棒性。

Method: 采用LPCMCI框架结合高斯过程距离相关（GPDC）来发现1970-2021年美国宏观经济指标（GDP、经济增长、通货膨胀、失业率）的动态因果关系。同时，利用为时间序列训练的大型语言模型Chronos框架，对失业率进行零样本、不确定性感知预测。

Result: 研究揭示了经济增长对GDP存在稳健的单向因果关系；通货膨胀的连接性有限，可能受潜在因素影响。对失业率的零样本预测在未来一两个季度内表现准确，无需特定任务训练，并提供了90%的置信区间，有效支持异常检测。

Conclusion: 本研究证明了将因果结构学习与概率语言模型相结合，对于为经济政策提供信息和增强预测鲁棒性具有重要价值。

Abstract: This paper presents a methodological approach to financial time series
analysis by combining causal discovery and uncertainty-aware forecasting. As a
case study, we focus on four key U.S. macroeconomic indicators -- GDP, economic
growth, inflation, and unemployment -- and we apply the LPCMCI framework with
Gaussian Process Distance Correlation (GPDC) to uncover dynamic causal
relationships in quarterly data from 1970 to 2021. Our results reveal a robust
unidirectional causal link from economic growth to GDP and highlight the
limited connectivity of inflation, suggesting the influence of latent factors.
Unemployment exhibits strong autoregressive dependence, motivating its use as a
case study for probabilistic forecasting. Leveraging the Chronos framework, a
large language model trained for time series, we perform zero-shot predictions
on unemployment. This approach delivers accurate forecasts one and two quarters
ahead, without requiring task-specific training. Crucially, the model's
uncertainty-aware predictions yield 90\% confidence intervals, enabling
effective anomaly detection through statistically principled deviation
analysis. This study demonstrates the value of combining causal structure
learning with probabilistic language models to inform economic policy and
enhance forecasting robustness.

</details>


### [136] [Benchmarking Vision Transformers and CNNs for Thermal Photovoltaic Fault Detection with Explainable AI Validation](https://arxiv.org/abs/2509.07039)
*Serra Aksoy*

Main category: cs.LG

TL;DR: 本研究系统比较了CNN和Vision Transformer在光伏热故障检测中的性能，并首次通过XRAI显著性分析验证了模型的物理可解释性，发现Swin Transformer表现最佳，且模型决策与热物理原理一致，但对环境因素故障检测仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 人工智能在自动化光伏监测中的部署面临可解释性障碍，限制了其在能源基础设施应用中的采纳。尽管深度学习在热故障检测中取得了高准确性，但缺乏模型决策与热物理原理一致性的验证，这在需要理解模型推理的关键应用场景中造成了部署犹豫。

Method: 本研究对卷积神经网络（ResNet-18, EfficientNet-B0）和视觉Transformer（ViT-Tiny, Swin-Tiny）进行了系统比较，以进行热光伏故障检测。使用XRAI显著性分析来评估模型决策与热物理原理的一致性。评估使用了涵盖正常运行和11种故障类别的20,000张红外图像。

Result: Swin Transformer表现出最高性能（94%二分类准确率；73%多分类准确率）。XRAI分析表明模型学习到具有物理意义的特征，例如电池缺陷的局部热点、二极管故障的线性热路径和植被遮挡的热边界，这与预期的热特征一致。然而，性能在不同故障类型之间差异显著：电气故障实现了强大的检测（F1分数>0.90），而像污垢这样的环境因素仍然具有挑战性（F1分数0.20-0.33），这表明受热成像分辨率的限制。

Conclusion: 热物理指导的可解释性方法为验证能源监测应用中的AI决策提供了依据，从而解决了可再生能源基础设施中的部署障碍。

Abstract: Artificial intelligence deployment for automated photovoltaic (PV) monitoring
faces interpretability barriers that limit adoption in energy infrastructure
applications. While deep learning achieves high accuracy in thermal fault
detection, validation that model decisions align with thermal physics
principles remains lacking, creating deployment hesitancy where understanding
model reasoning is critical. This study provides a systematic comparison of
convolutional neural networks (ResNet-18, EfficientNet-B0) and vision
transformers (ViT-Tiny, Swin-Tiny) for thermal PV fault detection, using XRAI
saliency analysis to assess alignment with thermal physics principles. This
represents the first systematic comparison of CNNs and vision transformers for
thermal PV fault detection with physics-validated interpretability. Evaluation
on 20,000 infrared images spanning normal operation and 11 fault categories
shows that Swin Transformer achieves the highest performance (94% binary
accuracy; 73% multiclass accuracy) compared to CNN approaches. XRAI analysis
reveals that models learn physically meaningful features, such as localized
hotspots for cell defects, linear thermal paths for diode failures, and thermal
boundaries for vegetation shading, consistent with expected thermal signatures.
However, performance varies significantly across fault types: electrical faults
achieve strong detection (F1-scores >0.90) while environmental factors like
soiling remain challenging (F1-scores 0.20-0.33), indicating limitations
imposed by thermal imaging resolution. The thermal physics-guided
interpretability approach provides methodology for validating AI
decision-making in energy monitoring applications, addressing deployment
barriers in renewable energy infrastructure.

</details>


### [137] [Lookup multivariate Kolmogorov-Arnold Networks](https://arxiv.org/abs/2509.07103)
*Sergey Pozdnyakov,Philippe Schwaller*

Main category: cs.LG

TL;DR: 引入lmKANs作为高维线性映射的替代方案，通过样条查找表实现低维函数映射，显著降低推理成本，同时保持或提升模型能力。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型中，高维线性映射（即线性层）在参数数量和计算成本方面占据主导地位，是主要的性能瓶颈。

Method: 提出了一种名为查找多元Kolmogorov-Arnold网络（lmKANs）的通用替代方案。该方法通过可训练的低维多元函数表达高维映射，这些函数以样条查找表的形式实现，计算成本低，只需少量乘法即可完成计算。

Result: 经验证，lmKANs在匹配MLP灵活性的同时，推理FLOPs最多可减少6.0倍。在特定前馈全连接基准测试中，lmKANs在同等精度下实现超过10倍的H100吞吐量。在卷积神经网络（CNN）框架内，基于lmKAN的CNNs在CIFAR-10和ImageNet-1k数据集上，以匹配的精度分别将推理FLOPs降低1.6-2.1倍和1.7倍。

Conclusion: lmKANs通过其独特的低维函数与样条查找表结合的实现方式，显著改善了深度学习模型在容量和推理成本之间的权衡，提供了一种高效且灵活的高维线性映射替代方案。

Abstract: High-dimensional linear mappings, or linear layers, dominate both the
parameter count and the computational cost of most modern deep-learning models.
We introduce a general drop-in replacement, lookup multivariate
Kolmogorov-Arnold Networks (lmKANs), which deliver a substantially better
trade-off between capacity and inference cost. Our construction expresses a
general high-dimensional mapping through trainable low-dimensional multivariate
functions. These functions can carry dozens or hundreds of trainable parameters
each, and yet it takes only a few multiplications to compute them because they
are implemented as spline lookup tables. Empirically, lmKANs reduce inference
FLOPs by up to 6.0x while matching the flexibility of MLPs in general
high-dimensional function approximation. In another feedforward fully connected
benchmark, on the tabular-like dataset of randomly displaced methane
configurations, lmKANs enable more than 10x higher H100 throughput at equal
accuracy. Within frameworks of Convolutional Neural Networks, lmKAN-based CNNs
cut inference FLOPs at matched accuracy by 1.6-2.1x and by 1.7x on the CIFAR-10
and ImageNet-1k datasets, respectively. Our code, including dedicated CUDA
kernels, is available online at https://github.com/schwallergroup/lmkan.

</details>


### [138] [Riemannian Batch Normalization: A Gyro Approach](https://arxiv.org/abs/2509.07115)
*Ziheng Chen,Xiao-Jun Wu,Nicu Sebe*

Main category: cs.LG

TL;DR: 本文提出GyroBN，一个针对回转群的黎曼批标准化框架，解决了欧几里得范式在流形数据上的不足，并提供理论保证和在多种几何体上的有效性验证。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的标准化层至关重要，但其欧几里得公式不适用于流形数据。许多机器学习中的黎曼流形具有回转结构，可将欧几里得神经网络扩展到非欧几里得域，这激发了开发黎曼批标准化的需求。

Method: 引入了GyroBN，一个针对回转群的黎曼批标准化框架。为确保样本统计量的理论控制，建立了“伪约简”和“回转等距旋转”两个必要条件，并证明这些条件适用于所有已知机器学习中的回转群。该框架还将现有黎曼标准化方法作为特例囊括。进一步在七种代表性几何体（包括格拉斯曼流形、五种常曲率空间和相关性流形）上实例化GyroBN，并推导出新的回转和黎曼结构以支持这些实例化。

Result: GyroBN框架在理论上通过“伪约简”和“回转等距旋转”条件实现了对样本统计量的控制，这些条件在所有已知的机器学习回转群中均成立。GyroBN能够涵盖现有多种黎曼标准化方法。在七种几何体上的实验证明了GyroBN的有效性。

Conclusion: GyroBN是一个有效且理论基础扎实的黎曼批标准化框架，适用于处理流形数据，可应用于多种几何体，并能扩展和包含现有方法。

Abstract: Normalization layers are crucial for deep learning, but their Euclidean
formulations are inadequate for data on manifolds. On the other hand, many
Riemannian manifolds in machine learning admit gyro-structures, enabling
principled extensions of Euclidean neural networks to non-Euclidean domains.
Inspired by this, we introduce GyroBN, a principled Riemannian batch
normalization framework for gyrogroups. We establish two necessary conditions,
namely \emph{pseudo-reduction} and \emph{gyroisometric gyrations}, that
guarantee GyroBN with theoretical control over sample statistics, and show that
these conditions hold for all known gyrogroups in machine learning. Our
framework also incorporates several existing Riemannian normalization methods
as special cases. We further instantiate GyroBN on seven representative
geometries, including the Grassmannian, five constant curvature spaces, and the
correlation manifold, and derive novel gyro and Riemannian structures to enable
these instantiations. Experiments across these geometries demonstrate the
effectiveness of GyroBN. The code is available at
https://github.com/GitZH-Chen/GyroBN.git.

</details>


### [139] [Of Graphs and Tables: Zero-Shot Node Classification with Tabular Foundation Models](https://arxiv.org/abs/2509.07143)
*Adrian Hayler,Xingyue Huang,İsmail İlkan Ceylan,Michael Bronstein,Ben Finkelshtein*

Main category: cs.LG

TL;DR: 本文提出TabGFM框架，通过将图节点分类任务重构为表格问题，并结合表格基础模型（TFMs）与集成学习，在28个真实世界数据集上实现了对现有图神经网络和图基础模型的持续改进，展示了表格化重构在图学习中的巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型（GFMs）因训练数据集无法很好地代表真实世界图数据，导致其泛化性能受限。相比之下，表格基础模型（TFMs）在多种任务和领域中表现出色。受此启发，作者旨在通过将节点分类任务重新定义为表格问题，以利用TFMs的强大能力来提升图学习的泛化性。

Method: 本文引入了TabGFM框架。该方法首先通过特征编码器和结构编码器将图转换为表格，其中每个节点被表示为一行，特征、结构和标签信息作为列。接着，将多个表格基础模型应用于经过多样化子采样的表格数据，最后通过集成选择（ensemble selection）聚合这些模型的输出。这种方法使得TFMs能够通过上下文学习直接进行零样本节点分类。

Result: 通过在28个真实世界数据集上进行的实验，TabGFM相比于特定任务的图神经网络（GNNs）和最先进的图基础模型（GFMs）都取得了持续的性能提升。

Conclusion: 研究结果突出了表格化重构（tabular reformulation）在实现可扩展和可泛化的图学习方面的巨大潜力。

Abstract: Graph foundation models (GFMs) have recently emerged as a promising paradigm
for achieving broad generalization across various graph data. However, existing
GFMs are often trained on datasets that were shown to poorly represent
real-world graphs, limiting their generalization performance. In contrast,
tabular foundation models (TFMs) not only excel at classical tabular prediction
tasks but have also shown strong applicability in other domains such as time
series forecasting, natural language processing, and computer vision. Motivated
by this, we take an alternative view to the standard perspective of GFMs and
reformulate node classification as a tabular problem. Each node can be
represented as a row with feature, structure, and label information as columns,
enabling TFMs to directly perform zero-shot node classification via in-context
learning. In this work, we introduce TabGFM, a graph foundation model framework
that first converts a graph into a table via feature and structural encoders,
applies multiple TFMs to diversely subsampled tables, and then aggregates their
outputs through ensemble selection. Through experiments on 28 real-world
datasets, TabGFM achieves consistent improvements over task-specific GNNs and
state-of-the-art GFMs, highlighting the potential of tabular reformulation for
scalable and generalizable graph learning.

</details>


### [140] [Measuring Uncertainty in Transformer Circuits with Effective Information Consistency](https://arxiv.org/abs/2509.07149)
*Anatoly A. Krasnovsky*

Main category: cs.LG

TL;DR: 本文提出了一种名为EICS的新分数，用于量化大型语言模型中Transformer电路（TCs）行为的一致性和可信度，该分数结合了sheaf不一致性和因果涌现，但尚未进行LLM任务的实证验证。


<details>
  <summary>Details</summary>
Motivation: 尽管机械可解释性已识别出大型语言模型中的功能性Transformer电路（TCs），但目前缺乏一种正式、单次通过的方法来量化活跃电路行为的一致性和可信度。

Method: 本文特化了基于系统理论、sheaf/cohomology和因果涌现的视角，引入了“有效信息一致性分数（EICS）”。EICS结合了(i)从局部雅可比矩阵和激活计算的归一化sheaf不一致性，与(ii)源自相同前向状态的电路级因果涌现的高斯有效信息（EI）代理。该构建是白盒、单次通过且无量纲的。

Result: 本文成功提出了EICS这一新型指标，并详细阐述了其构建原理、特性（白盒、单次通过、无量纲）。此外，还提供了分数解释和计算开销的实用指导，并进行了玩具示例的合理性检验分析。大型语言模型任务上的实证验证工作被推迟。

Conclusion: 本文引入了一种正式、单次通过的“有效信息一致性分数（EICS）”，为量化大型语言模型中Transformer电路的连贯性和可信度提供了一个新工具，为未来的机械可解释性研究奠定了理论基础。

Abstract: Mechanistic interpretability has identified functional subgraphs within large
language models (LLMs), known as Transformer Circuits (TCs), that appear to
implement specific algorithms. Yet we lack a formal, single-pass way to
quantify when an active circuit is behaving coherently and thus likely
trustworthy. Building on prior systems-theoretic proposals, we specialize a
sheaf/cohomology and causal emergence perspective to TCs and introduce the
Effective-Information Consistency Score (EICS). EICS combines (i) a normalized
sheaf inconsistency computed from local Jacobians and activations, with (ii) a
Gaussian EI proxy for circuit-level causal emergence derived from the same
forward state. The construction is white-box, single-pass, and makes units
explicit so that the score is dimensionless. We further provide practical
guidance on score interpretation, computational overhead (with fast and exact
modes), and a toy sanity-check analysis. Empirical validation on LLM tasks is
deferred.

</details>


### [141] [PLaID++: A Preference Aligned Language Model for Targeted Inorganic Materials Design](https://arxiv.org/abs/2509.07150)
*Andy Xu,Rohan Desai,Larry Wang,Gabriel Hope,Ethan Ritz*

Main category: cs.LG

TL;DR: PLaID++是一个基于DPO优化的LLM，通过Wyckoff编码文本和强化学习，能以更高效率生成稳定、新颖且具有特定空间群的晶体材料。


<details>
  <summary>Details</summary>
Motivation: 新材料的发现对技术进步至关重要，但传统试错法耗时且成本高昂，亟需加速材料发现流程。

Method: 开发PLaID++，一个基于Qwen-2.5 7B微调的大语言模型。采用新型Wyckoff编码文本表示晶体结构，并利用基于直接偏好优化（DPO）的强化学习技术指导晶体生成，将对称约束直接编码到文本中，从而引导模型输出具有所需性质的结构。

Result: PLaID++生成热力学稳定、独特且新颖结构的效率比现有方法高约50%，并能按条件生成具有所需空间群性质的结构。与单独微调相比，迭代DPO在无条件和空间群条件生成方面分别带来了约115%和50%的改进。

Conclusion: 该研究展示了将自然语言处理的后训练技术应用于材料设计的巨大潜力，为靶向和高效发现新材料提供了新途径。

Abstract: Discovering novel materials is critical for technological advancements such
as solar cells, batteries, and carbon capture. However, the development of new
materials is constrained by a slow and expensive trial-and-error process. To
accelerate this pipeline, we introduce PLaID++, a Large Language Model (LLM)
fine-tuned for stable and property-guided crystal generation. We fine-tune
Qwen-2.5 7B to generate crystal structures using a novel Wyckoff-based text
representation. We show that generation can be effectively guided with a
reinforcement learning technique based on Direct Preference Optimization (DPO),
with sampled structures categorized by their stability, novelty, and space
group. By encoding symmetry constraints directly into text and guiding model
outputs towards desirable chemical space, PLaID++ generates structures that are
thermodynamically stable, unique, and novel at a $\sim$50\% greater rate than
prior methods and conditionally generates structures with desired space group
properties. Our experiments highlight the effectiveness of iterative DPO,
achieving $\sim$115\% and $\sim$50\% improvements in unconditional and space
group conditioned generation, respectively, compared to fine-tuning alone. Our
work demonstrates the potential of adapting post-training techniques from
natural language processing to materials design, paving the way for targeted
and efficient discovery of novel materials.

</details>


### [142] [Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data](https://arxiv.org/abs/2509.07198)
*Yiyue Chen,Usman Akram,Chianing Wang,Haris Vikalo*

Main category: cs.LG

TL;DR: Fed-REACT是一个针对联邦学习中异构和演变客户端数据的框架，它结合表示学习和演化聚类，动态分组客户端并进行协同训练，以提高性能。


<details>
  <summary>Details</summary>
Motivation: 集中式机器学习存在高资源成本和隐私问题，而联邦学习（FL）能解决这些问题。然而，实际部署中，FL面临客户端数据随时间演变且差异显著的异构性挑战，这会降低标准FL算法的性能。

Method: 引入Fed-REACT框架，采用两阶段方法：(1) 第一阶段，每个客户端学习局部模型以提取数据特征表示；(2) 第二阶段，服务器基于这些表示动态将客户端分组，并协调集群内训练任务特定模型，以实现分类或回归等下游目标。

Result: 对表示学习阶段进行了理论分析，并在真实世界数据集中经验证明，Fed-REACT实现了卓越的准确性和鲁棒性。

Conclusion: Fed-REACT是一个有效的联邦学习框架，能够解决异构和演变客户端数据带来的挑战，显著提升了FL的性能和稳定性。

Abstract: Motivated by the high resource costs and privacy concerns associated with
centralized machine learning, federated learning (FL) has emerged as an
efficient alternative that enables clients to collaboratively train a global
model while keeping their data local. However, in real-world deployments,
client data distributions often evolve over time and differ significantly
across clients, introducing heterogeneity that degrades the performance of
standard FL algorithms. In this work, we introduce Fed-REACT, a federated
learning framework designed for heterogeneous and evolving client data.
Fed-REACT combines representation learning with evolutionary clustering in a
two-stage process: (1) in the first stage, each client learns a local model to
extracts feature representations from its data; (2) in the second stage, the
server dynamically groups clients into clusters based on these representations
and coordinates cluster-wise training of task-specific models for downstream
objectives such as classification or regression. We provide a theoretical
analysis of the representation learning stage, and empirically demonstrate that
Fed-REACT achieves superior accuracy and robustness on real-world datasets.

</details>


### [143] [Predicting effect of novel treatments using molecular pathways and real-world data](https://arxiv.org/abs/2509.07204)
*Adrien Couetoux,Thomas Devenyns,Lise Diagne,David Champagne,Pierre-Yves Mousset,Chris Anagnostopoulos*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的方法，利用药物-通路权重影响评分和患者数据，预测未测试药物治疗疾病的功效。


<details>
  <summary>Details</summary>
Motivation: 在临床试验或实际使用前，预测药物治疗特定疾病的功效一直具有挑战性。

Method: 开发了一种灵活且模块化的机器学习方法。该模型通过药物-通路权重影响评分和包含患者特征及临床结果的患者数据进行训练。然后，模型分析未测试药物在人类生物分子-蛋白质通路上的加权影响评分，以生成预测功效值。该方法在真实世界数据上进行了演示，并使用了两种不同的权重影响评分算法，同时评估了对未知治疗的泛化性能。

Result: 该方法在包含患者治疗和结果的真实世界数据集上得到了演示，展示了其工作原理。

Conclusion: 该方法可作为支持未来预测未测试药物效果的初步框架，可利用真实世界临床数据和药物嵌入进行迭代和进一步研究。

Abstract: In pharmaceutical R&D, predicting the efficacy of a pharmaceutical in
treating a particular disease prior to clinical testing or any real-world use
has been challenging. In this paper, we propose a flexible and modular machine
learning-based approach for predicting the efficacy of an untested
pharmaceutical for treating a disease. We train a machine learning model using
sets of pharmaceutical-pathway weight impact scores and patient data, which can
include patient characteristics and observed clinical outcomes. The resulting
model then analyses weighted impact scores of an untested pharmaceutical across
human biological molecule-protein pathways to generate a predicted efficacy
value. We demonstrate how the method works on a real-world dataset with patient
treatments and outcomes, with two different weight impact score algorithms We
include methods for evaluating the generalisation performance on unseen
treatments, and to characterise conditions under which the approach can be
expected to be most predictive. We discuss specific ways in which our approach
can be iterated on, making it an initial framework to support future work on
predicting the effect of untested drugs, leveraging RWD clinical data and drug
embeddings.

</details>


### [144] [Explaining How Quantization Disparately Skews a Model](https://arxiv.org/abs/2509.07222)
*Abhimanyu Bellam,Jung-Eun Kim*

Main category: cs.LG

TL;DR: 研究发现量化（PTQ）会加剧对少数群体的歧视性影响，分析了其原因，并提出结合混合精度QAT、数据集采样和加权损失函数来缓解。


<details>
  <summary>Details</summary>
Motivation: 尽管后训练量化（PTQ）因其高压缩率、速度和对精度影响小而被广泛采用，但研究观察到量化会加剧对少数群体的歧视性影响，旨在解释并解决这一公平性问题。

Method: 通过分析量化过程中权重和激活的变化如何引起网络中的级联效应，导致logits方差降低、损失增加和组准确率受损。研究进一步验证了这些影响对组梯度范数和Hessian矩阵特征值的影响，从优化角度提供网络状态洞察。

Result: 分析揭示量化导致网络中一系列因素引起跨群体的歧视性影响，表现为logits方差降低、损失增加和组准确率受损。这些影响进一步体现在组梯度范数和Hessian矩阵特征值的变化上。

Conclusion: 为缓解量化带来的歧视性影响，研究提出将混合精度量化感知训练（QAT）与数据集采样方法和加权损失函数相结合，以实现量化神经网络的公平部署。

Abstract: Post Training Quantization (PTQ) is widely adopted due to its high
compression capacity and speed with minimal impact on accuracy. However, we
observed that disparate impacts are exacerbated by quantization, especially for
minority groups. Our analysis explains that in the course of quantization there
is a chain of factors attributed to a disparate impact across groups during
forward and backward passes. We explore how the changes in weights and
activations induced by quantization cause cascaded impacts in the network,
resulting in logits with lower variance, increased loss, and compromised group
accuracies. We extend our study to verify the influence of these impacts on
group gradient norms and eigenvalues of the Hessian matrix, providing insights
into the state of the network from an optimization point of view. To mitigate
these effects, we propose integrating mixed precision Quantization Aware
Training (QAT) with dataset sampling methods and weighted loss functions,
therefore providing fair deployment of quantized neural networks.

</details>


### [145] [Systematic Optimization of Open Source Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2509.07238)
*Pranav Pawar,Dhwaj Jain,Varun Gupta,Kaustav Dedhia,Dashrath Kale,Sudhir Dhekane*

Main category: cs.LG

TL;DR: 本文通过参数微调优化SOTA大模型在数学推理任务上的表现，在维持准确性的同时显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过参数优化提升SOTA模型在数学推理任务上的效率和性能，同时确保结果的正确性，填补实践优化方面的空白。

Method: 提出并应用了一个标准化优化框架，系统地搜索并调整了五个SOTA大型模型（Qwen2.5-72B、Llama-3.1-70B、DeepSeek-V3、Mixtral-8x22B、Yi-Lightning）在数学推理任务上的参数（如温度、推理步骤、规划周期和核采样）。

Result: 平均计算成本降低29.4%，推理速度提升23.9%。发现较低的温度（0.1-0.4）和较少的推理步骤（4-6）可在不损害准确性的前提下提高效率。DeepSeek-V3达到98%的最高准确率，Mixtral-8x22B在成本效益上表现最佳。

Conclusion: 本研究为数学推理任务提供了全面的SOTA模型优化框架和生产级配置，发现了通用的优化趋势，显著提升了模型效率和性能。

Abstract: This paper presents a practical investigation into fine-tuning model
parameters for mathematical reasoning tasks through experimenting with various
configurations including randomness control, reasoning depth, and sampling
strategies, careful tuning demonstrates substantial improvements in efficiency
as well as performance. A holistically optimized framework is introduced for
five state-of-the-art models on mathematical reasoning tasks, exhibiting
significant performance boosts while maintaining solution correctness. Through
systematic parameter optimization across Qwen2.5-72B, Llama-3.1-70B,
DeepSeek-V3, Mixtral-8x22B, and Yi-Lightning, consistent efficiency gains are
demonstrated with 100% optimization success rate. The methodology achieves an
average 29.4% reduction in computational cost and 23.9% improvement in
inference speed across all tested models. This framework systematically
searches parameter spaces including temperature (0.1-0.5), reasoning steps
(4-12), planning periods (1-4), and nucleus sampling (0.85-0.98), determining
optimal configurations through testing on mathematical reasoning benchmarks.
Critical findings show that lower temperature regimes (0.1-0.4) and reduced
reasoning steps (4-6) consistently enhance efficiency without compromising
accuracy. DeepSeek-V3 achieves the highest accuracy at 98%, while Mixtral-8x22B
delivers the most cost-effective performance at 361.5 tokens per accurate
response. Key contributions include: (1) the first comprehensive optimization
study for five diverse SOTA models in mathematical reasoning, (2) a
standardized production-oriented parameter optimization framework, (3)
discovery of universal optimization trends applicable across model
architectures, and (4) production-ready configurations with extensive
performance characterization.

</details>


### [146] [IP-Basis PINNs: Efficient Multi-Query Inverse Parameter Estimation](https://arxiv.org/abs/2509.07245)
*Shalev Manor,Mohammad Kohandel*

Main category: cs.LG

TL;DR: IP-Basis PINNs是一个元学习框架，通过离线基函数预训练和在线轻量级推理，显著加速了多查询逆问题中PINN的求解，并对稀疏和噪声数据表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络（PINNs）在多查询逆问题中计算成本高昂，因为每次新的观测数据都需要重新进行耗时且昂贵的训练。

Method: 提出IP-Basis PINNs元学习框架，采用离线-在线分解：离线阶段训练一个深度网络生成覆盖参数微分方程解空间的基函数；在线阶段固定该网络，仅通过训练一个轻量级的线性输出层，根据观测数据推断解和参数。主要创新包括：(1) 新颖的在线损失公式，用于同时进行解重建和参数识别；(2) 通过前向模式自动微分显著降低PDE损失评估的计算开销；(3) 鲁棒的离线训练验证与早停机制。

Result: 在三个不同基准测试中验证了IP-Basis PINNs的有效性，包括对未知函数项的通用PINNs的扩展。结果显示，该方法在常数和函数参数估计中表现一致，每次查询的速度比标准PINNs显著加快，并且在稀疏和噪声数据下也能鲁棒运行。

Conclusion: IP-Basis PINNs提供了一种快速、高效且鲁棒的解决方案，有效克服了传统PINNs在多查询逆问题中的计算瓶颈，并在数据有限和有噪声的场景下表现出色，扩展了PINNs的应用范围。

Abstract: Solving inverse problems with Physics-Informed Neural Networks (PINNs) is
computationally expensive for multi-query scenarios, as each new set of
observed data requires a new, expensive training procedure. We present
Inverse-Parameter Basis PINNs (IP-Basis PINNs), a meta-learning framework that
extends the foundational work of Desai et al. (2022) to enable rapid and
efficient inference for inverse problems. Our method employs an offline-online
decomposition: a deep network is first trained offline to produce a rich set of
basis functions that span the solution space of a parametric differential
equation. For each new inverse problem online, this network is frozen, and
solutions and parameters are inferred by training only a lightweight linear
output layer against observed data. Key innovations that make our approach
effective for inverse problems include: (1) a novel online loss formulation for
simultaneous solution reconstruction and parameter identification, (2) a
significant reduction in computational overhead via forward-mode automatic
differentiation for PDE loss evaluation, and (3) a non-trivial validation and
early-stopping mechanism for robust offline training. We demonstrate the
efficacy of IP-Basis PINNs on three diverse benchmarks, including an extension
to universal PINNs for unknown functional terms-showing consistent performance
across constant and functional parameter estimation, a significant speedup per
query over standard PINNs, and robust operation with scarce and noisy data.

</details>


### [147] [GCond: Gradient Conflict Resolution via Accumulation-based Stabilization for Large-Scale Multi-Task Learning](https://arxiv.org/abs/2509.07252)
*Evgeny Alves Limarenko,Anastasiia Alexandrovna Studenikina*

Main category: cs.LG

TL;DR: GCond是一种基于PCGrad原则的多任务学习梯度冲突解决方法，通过结合梯度累积和自适应仲裁机制，实现了计算效率提升、优化质量保持和更优的性能，并具有良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 多任务学习（MTL）中的梯度冲突是一个显著挑战。现有的梯度冲突解决方法（如PCGrad、CAGrad、GradNorm）计算成本高昂，严重限制了它们在现代大型模型和Transformer中的应用。

Method: 提出了Gradient Conductor (GCond) 方法。GCond建立在PCGrad原理之上，并结合了梯度累积（gradient accumulation）和自适应仲裁机制（adaptive arbitration mechanism）。研究在ImageNet 1K和头颈部CT扫描数据集上，使用MobileNetV3-Small和ConvNeXt架构的自监督学习任务中对GCond进行了评估，并与基线线性组合以及最先进的梯度冲突解决方法进行了比较。

Result: GCond的随机模式在保持优化质量的同时，计算速度提升了两倍。在所有评估指标上均表现出卓越性能，在两个数据集上都实现了比其他方法更低的L1和SSIM损失。GCond展示了高可扩展性，成功应用于紧凑模型（MobileNetV3-Small）和大型架构（ConvNeXt-tiny和ConvNeXt-Base），并且与AdamW和Lion/LARS等现代优化器兼容。

Conclusion: GCond为多任务学习中的梯度冲突问题提供了一个可扩展且高效的解决方案。

Abstract: In multi-task learning (MTL), gradient conflict poses a significant
challenge. Effective methods for addressing this problem, including PCGrad,
CAGrad, and GradNorm, in their original implementations are computationally
demanding, which significantly limits their application in modern large models
and transformers. We propose Gradient Conductor (GCond), a method that builds
upon PCGrad principles by combining them with gradient accumulation and an
adaptive arbitration mechanism. We evaluated GCond on self-supervised learning
tasks using MobileNetV3-Small and ConvNeXt architectures on the ImageNet 1K
dataset and a combined head and neck CT scan dataset, comparing the proposed
method against baseline linear combinations and state-of-the-art gradient
conflict resolution methods. The stochastic mode of GCond achieved a two-fold
computational speedup while maintaining optimization quality, and demonstrated
superior performance across all evaluated metrics, achieving lower L1 and SSIM
losses compared to other methods on both datasets. GCond exhibited high
scalability, being successfully applied to both compact models
(MobileNetV3-Small) and large architectures (ConvNeXt-tiny and ConvNeXt-Base).
It also showed compatibility with modern optimizers such as AdamW and
Lion/LARS. Therefore, GCond offers a scalable and efficient solution to the
problem of gradient conflicts in multi-task learning.

</details>


### [148] [Learning Generalized Hamiltonian Dynamics with Stability from Noisy Trajectory Data](https://arxiv.org/abs/2509.07280)
*Luke McLennan,Yi Wang,Ryan Farell,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: 本文提出一个基于变分贝叶斯推断的鲁棒框架，用于从带噪声、稀疏的相空间数据中无监督学习各种广义哈密顿动力学。


<details>
  <summary>Details</summary>
Motivation: 单一的哈密顿网络模型难以捕捉保守、耗散和港口哈密顿等不同类型系统在相空间中独特且变化的运动动力学和物理特性。

Method: 通过扩展稀疏辛、随机傅里叶高斯过程学习，并结合广义哈密顿动力学对哈密顿景观进行预测性数值估计。同时，在核化ELBO损失函数的基础上，引入稳定性和守恒约束作为额外的正则化项，以强制物理正确性。

Result: 该框架能够从噪声稀疏数据中有效学习各种广义哈密顿动力学（包括保守、耗散和港口哈密顿系统），确保物理正确性，并提高预测精度及提供有界不确定性。

Conclusion: 本研究通过结合先进的贝叶斯推断和物理约束，成功构建了一个能够处理多种广义哈密顿系统的鲁棒学习框架，有效解决了单一模型难以捕捉多样化动力学的挑战。

Abstract: We introduce a robust framework for learning various generalized Hamiltonian
dynamics from noisy, sparse phase-space data and in an unsupervised manner
based on variational Bayesian inference. Although conservative, dissipative,
and port-Hamiltonian systems might share the same initial total energy of a
closed system, it is challenging for a single Hamiltonian network model to
capture the distinctive and varying motion dynamics and physics of a phase
space, from sampled observational phase space trajectories. To address this
complicated Hamiltonian manifold learning challenge, we extend sparse
symplectic, random Fourier Gaussian processes learning with predictive
successive numerical estimations of the Hamiltonian landscape, using a
generalized form of state and conjugate momentum Hamiltonian dynamics,
appropriate to different classes of conservative, dissipative and
port-Hamiltonian physical systems. In addition to the kernelized evidence lower
bound (ELBO) loss for data fidelity, we incorporate stability and conservation
constraints as additional hyper-parameter balanced loss terms to regularize the
model's multi-gradients, enforcing physics correctness for improved prediction
accuracy with bounded uncertainty.

</details>


### [149] [ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers](https://arxiv.org/abs/2509.07282)
*Jeff Shen,Lindsay Smith*

Main category: cs.LG

TL;DR: ALICE，一个编码器Transformer，在密码字谜解密任务中达到SOTA，并以少量训练数据展示出卓越的泛化能力和可解释性，其解密过程与人类策略相似。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在组合复杂领域（如密码字谜解密）的泛化能力，该任务涉及从巨大的映射空间（26!）中选择正确的解密方案。

Method: 开发了ALICE（一个用于学习可解释密码字谜解密的架构），一个简单的编码器Transformer。引入了一种新颖的双射解码头，通过Gumbel-Sinkhorn方法显式建模排列。使用早期退出分析来揭示模型预测的逐步细化过程。

Result: ALICE在密码字谜解密问题上，准确性和速度均创下新的最先进水平。在仅训练约1500个独特密码（占可能密码空间的极小部分）后，ALICE能够泛化到未见过的密码。早期退出分析表明，ALICE逐步完善其预测，过程类似人类策略：早期层使用频率启发式，中间层形成单词结构，最终层修正单个字符。可直接提取学习到的密码映射。

Conclusion: 该架构创新和分析方法不仅适用于密码字谜，还能扩展到任何具有双射映射和组合结构的任务领域，为神经网络的泛化能力和可解释性提供了新见解。

Abstract: We present cryptogram solving as an ideal testbed for studying neural network
generalization in combinatorially complex domains. In this task, models must
decrypt text encoded with substitution ciphers, choosing from 26! possible
mappings without explicit access to the cipher. We develop ALICE (an
Architecture for Learning Interpretable Cryptogram dEcipherment): a simple
encoder-only Transformer that sets a new state-of-the-art for both accuracy and
speed on this decryption problem. Surprisingly, ALICE generalizes to unseen
ciphers after training on only ${\sim}1500$ unique ciphers, a minute fraction
($3.7 \times 10^{-24}$) of the possible cipher space. To enhance
interpretability, we introduce a novel bijective decoding head that explicitly
models permutations via the Gumbel-Sinkhorn method, enabling direct extraction
of learned cipher mappings. Through early exit analysis, we reveal how ALICE
progressively refines its predictions in a way that appears to mirror common
human strategies for this task: early layers employ frequency-based heuristics,
middle layers form word structures, and final layers correct individual
characters. Our architectural innovations and analysis methods extend beyond
cryptograms to any domain with bijective mappings and combinatorial structure,
offering new insights into neural network generalization and interpretability.

</details>


### [150] [CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation](https://arxiv.org/abs/2509.07325)
*Alyssa Unell,Noel C. F. Codella,Sam Preston,Peniel Argaw,Wen-wai Yim,Zelalem Gero,Cliff Wong,Rajesh Jena,Eric Horvitz,Amanda K. Hall,Ruican Rachel Zhong,Jiachen Li,Shrey Jain,Mu Wei,Matthew Lungren,Hoifung Poon*

Main category: cs.LG

TL;DR: 本文提出了一种基于LLM代理的方法，用于为非小细胞肺癌患者自动生成符合NCCN指南的治疗轨迹，通过构建新的带专家注释的数据集和混合方法，实现了准确、可解释且符合监管要求的临床决策支持系统。


<details>
  <summary>Details</summary>
Motivation: 将复杂的患者情况转化为符合NCCN指南的治疗建议耗时、需要专业知识且容易出错。大型语言模型（LLM）的进步有望减少生成治疗建议所需的时间并提高准确性。

Method: 1. 构建了一个包含121例非小细胞肺癌患者病例的纵向数据集，并由肿瘤学家专家注释了相应的NCCN指南轨迹。2. 证明现有LLM具备领域特定知识，能够生成高质量的代理基准。3. 开发了一种混合方法，结合人工注释和模型一致性信息，创建了预测相关指南的LLM代理框架，以及一个验证预测准确性的元分类器，并提供校准的置信分数。

Result: 1. 现有LLM在代理基准生成方面与专家注释基准达到了高度相关（Spearman系数r=0.88，RMSE=0.08）。2. 元分类器在验证治疗建议准确性方面达到了AUROC=0.800。

Conclusion: 该工作为临床可行的LLM驱动指南依从性系统建立了一个框架，该框架在准确性、可解释性和监管要求之间取得平衡，同时降低了注释成本，为自动化临床决策支持提供了一条可扩展的途径。

Abstract: The National Comprehensive Cancer Network (NCCN) provides evidence-based
guidelines for cancer treatment. Translating complex patient presentations into
guideline-compliant treatment recommendations is time-intensive, requires
specialized expertise, and is prone to error. Advances in large language model
(LLM) capabilities promise to reduce the time required to generate treatment
recommendations and improve accuracy. We present an LLM agent-based approach to
automatically generate guideline-concordant treatment trajectories for patients
with non-small cell lung cancer (NSCLC). Our contributions are threefold.
First, we construct a novel longitudinal dataset of 121 cases of NSCLC patients
that includes clinical encounters, diagnostic results, and medical histories,
each expertly annotated with the corresponding NCCN guideline trajectories by
board-certified oncologists. Second, we demonstrate that existing LLMs possess
domain-specific knowledge that enables high-quality proxy benchmark generation
for both model development and evaluation, achieving strong correlation
(Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks.
Third, we develop a hybrid approach combining expensive human annotations with
model consistency information to create both the agent framework that predicts
the relevant guidelines for a patient, as well as a meta-classifier that
verifies prediction accuracy with calibrated confidence scores for treatment
recommendations (AUROC=0.800), a critical capability for communicating the
accuracy of outputs, custom-tailoring tradeoffs in performance, and supporting
regulatory compliance. This work establishes a framework for clinically viable
LLM-based guideline adherence systems that balance accuracy, interpretability,
and regulatory requirements while reducing annotation costs, providing a
scalable pathway toward automated clinical decision support.

</details>


### [151] [General Demographic Foundation Models for Enhancing Predictive Performance Across Diseases](https://arxiv.org/abs/2509.07330)
*Li-Chin Chen,Ji-Tian Sheu,Yuh-Jue Chuang*

Main category: cs.LG

TL;DR: 本研究提出GDP模型，一个用于年龄和性别的通用人口统计预训练基础模型，通过优化表示学习显著提升了医疗预测任务的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管人口统计学属性在电子健康记录中至关重要，但在模型设计中常被视为辅助角色，对其表示学习的关注有限，导致其预测价值未被充分利用。

Method: 提出通用人口统计预训练（GDP）模型，作为专为年龄和性别设计的表征框架。该模型通过探索排序策略和编码方法的组合，将表格人口统计输入转换为潜在嵌入，并在多疾病、多地域的数据集上进行预训练和评估。

Result: 实验表明，序列排序显著提升了模型在判别力、校准度和决策树拆分信息增益方面的性能。即使在预测价值较低的数据集中，GDP模型也能增强人口统计学属性的表征重要性，增加其在下游梯度提升模型中的影响力。

Conclusion: 研究结果表明，用于表格人口统计学属性的基础模型能够跨任务和人群进行泛化，为改善医疗应用中的预测性能提供了有前景的方向。

Abstract: Demographic attributes are universally present in electronic health records
and serve as vital predictors in clinical risk stratification and treatment
decisions. Despite their significance, these attributes are often relegated to
auxiliary roles in model design, with limited attention has been given to
learning their representations. This study proposes a General Demographic
Pre-trained (GDP) model as a foundational representation framework tailored to
age and gender. The model is pre-trained and evaluated using datasets with
diverse diseases and population compositions from different geographic regions.
The GDP architecture explores combinations of ordering strategies and encoding
methods to transform tabular demographic inputs into latent embeddings.
Experimental results demonstrate that sequential ordering substantially
improves model performance in discrimination, calibration, and the
corresponding information gain at each decision tree split, particularly in
diseases where age and gender contribute significantly to risk stratification.
Even in datasets where demographic attributes hold relatively low predictive
value, GDP enhances the representational importance, increasing their influence
in downstream gradient boosting models. The findings suggest that foundational
models for tabular demographic attributes can generalize across tasks and
populations, offering a promising direction for improving predictive
performance in healthcare applications.

</details>


### [152] [FedTeddi: Temporal Drift and Divergence Aware Scheduling for Timely Federated Edge Learning](https://arxiv.org/abs/2509.07342)
*Yuxuan Bai,Yuxuan Sun,Tan Chen,Wei Chen,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: FedTeddi是一种联邦边缘学习调度算法，旨在通过考虑数据的时间漂移和散度，解决动态、非独立同分布数据下的模型快速适应和收敛问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦边缘学习（FEEL）研究多假设数据集是静态的，但真实世界中客户端数据是持续收集、时变且非独立同分布（non-i.i.d.）的。关键挑战是如何及时高效地使模型适应这类演化数据。

Method: 提出FedTeddi算法，这是一种时间漂移和散度感知的调度算法。它使用地球移动距离（EMD）量化分类任务中类分布的时间漂移和集体散度，以表示数据的时序动态性和非独立同分布特性。在此基础上，提出新的优化目标，并开发了联合调度和带宽分配算法，使FEEL系统能快速学习新数据同时不遗忘旧知识。

Result: 实验结果显示，FedTeddi算法在动态数据下实现了更高的测试准确率和更快的收敛速度。与基准方法相比，在CIFAR-10数据集上收敛速度提高了58.4%，在CIFAR-100上提高了49.2%。

Conclusion: FedTeddi算法通过量化和管理数据的时间动态性及非独立同分布特性，有效解决了联邦边缘学习在动态数据演化和通信资源限制下的模型适应与收敛挑战，显著提升了学习性能和效率。

Abstract: Federated edge learning (FEEL) enables collaborative model training across
distributed clients over wireless networks without exposing raw data. While
most existing studies assume static datasets, in real-world scenarios clients
may continuously collect data with time-varying and non-independent and
identically distributed (non-i.i.d.) characteristics. A critical challenge is
how to adapt models in a timely yet efficient manner to such evolving data. In
this paper, we propose FedTeddi, a temporal-drift-and-divergence-aware
scheduling algorithm that facilitates fast convergence of FEEL under dynamic
data evolution and communication resource limits. We first quantify the
temporal dynamics and non-i.i.d. characteristics of data using temporal drift
and collective divergence, respectively, and represent them as the Earth
Mover's Distance (EMD) of class distributions for classification tasks. We then
propose a novel optimization objective and develop a joint scheduling and
bandwidth allocation algorithm, enabling the FEEL system to learn from new data
quickly without forgetting previous knowledge. Experimental results show that
our algorithm achieves higher test accuracy and faster convergence compared to
benchmark methods, improving the rate of convergence by 58.4% on CIFAR-10 and
49.2% on CIFAR-100 compared to random scheduling.

</details>


### [153] [SBS: Enhancing Parameter-Efficiency of Neural Representations for Neural Networks via Spectral Bias Suppression](https://arxiv.org/abs/2509.07373)
*Qihu Xie,Yuan Li,Yi Kang*

Main category: cs.LG

TL;DR: 本文提出SBS，一种参数高效的隐式神经表示增强方法，通过两种单向排序平滑技术抑制谱偏差，从而在更少参数下实现更好的卷积神经网络权重重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有的用于表示卷积神经网络权重的隐式神经表示（neural representation for neural networks）存在谱偏差，这阻碍了其有效重建高频细节的能力，尽管它能提供参数压缩优势。

Method: 本文提出SBS方法，通过两种技术抑制谱偏差：(1) 基于单向排序的平滑，改善输出空间的核平滑度；(2) 感知单向排序平滑的随机傅里叶特征，根据层级参数数量自适应调整输入编码的频率带宽。

Result: 在CIFAR-10、CIFAR-100和ImageNet数据集上，对各种ResNet模型进行的广泛评估表明，与现有最先进方法相比，SBS在参数更少的情况下实现了显著更高的重建精度。

Conclusion: SBS有效解决了隐式神经表示的谱偏差问题，显著提高了卷积神经网络权重的重建质量和参数压缩效率，性能优于当前最先进的方法。

Abstract: Implicit neural representations have recently been extended to represent
convolutional neural network weights via neural representation for neural
networks, offering promising parameter compression benefits. However, standard
multi-layer perceptrons used in neural representation for neural networks
exhibit a pronounced spectral bias, hampering their ability to reconstruct
high-frequency details effectively. In this paper, we propose SBS, a
parameter-efficient enhancement to neural representation for neural networks
that suppresses spectral bias using two techniques: (1) a unidirectional
ordering-based smoothing that improves kernel smoothness in the output space,
and (2) unidirectional ordering-based smoothing aware random fourier features
that adaptively modulate the frequency bandwidth of input encodings based on
layer-wise parameter count. Extensive evaluations on various ResNet models with
datasets CIFAR-10, CIFAR-100, and ImageNet, demonstrate that SBS achieves
significantly better reconstruction accuracy with less parameters compared to
SOTA.

</details>


### [154] [EfficientNet in Digital Twin-based Cardiac Arrest Prediction and Analysis](https://arxiv.org/abs/2509.07388)
*Qasim Zia,Avais Jan,Zafar Iqbal,Muhammad Mumtaz Ali,Mukarram Ali,Murray Patterson*

Main category: cs.LG

TL;DR: 本文提出结合EfficientNet深度学习模型与数字孪生系统的框架，用于早期检测和分析心脏骤停，实验表明其预测准确且高效，为心脏疾病预测提供个性化方法。


<details>
  <summary>Details</summary>
Motivation: 心脏骤停是全球性健康难题，早期识别与管理对改善患者预后至关重要。

Method: 提出结合EfficientNet深度学习模型与数字孪生系统的新颖框架。深度学习模型利用EfficientNet和复合缩放学习心血管图像特征。数字孪生系统基于IoT设备数据构建个性化心血管模型，实现患者持续评估及治疗方案影响分析。

Result: 实验结果表明，所提出的系统预测能力准确性高且效率显著。

Conclusion: 结合深度学习与数字孪生技术，为心脏疾病预测提供了一种主动且个性化的方法。

Abstract: Cardiac arrest is one of the biggest global health problems, and early
identification and management are key to enhancing the patient's prognosis. In
this paper, we propose a novel framework that combines an EfficientNet-based
deep learning model with a digital twin system to improve the early detection
and analysis of cardiac arrest. We use compound scaling and EfficientNet to
learn the features of cardiovascular images. In parallel, the digital twin
creates a realistic and individualized cardiovascular system model of the
patient based on data received from the Internet of Things (IoT) devices
attached to the patient, which can help in the constant assessment of the
patient and the impact of possible treatment plans. As shown by our
experiments, the proposed system is highly accurate in its prediction abilities
and, at the same time, efficient. Combining highly advanced techniques such as
deep learning and digital twin (DT) technology presents the possibility of
using an active and individual approach to predicting cardiac disease.

</details>


### [155] [Hybrid GCN-GRU Model for Anomaly Detection in Cryptocurrency Transactions](https://arxiv.org/abs/2509.07392)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Soyoun Kim,Sunyoung Moon,Sua Lee,Jaeyoung Choi,Hyemin Lee,Sangmi Chai*

Main category: cs.LG

TL;DR: 该研究提出了一个混合GCN-GRU模型，用于检测区块链交易网络中的非法活动，并在真实比特币数据上取得了卓越性能。


<details>
  <summary>Details</summary>
Motivation: 区块链交易网络复杂且具有时序演变模式和节点间关系，传统方法难以有效检测其中的非法活动。

Method: 提出了一种混合GCN-GRU模型，该模型能够同时捕获交易网络的结构特征和时序特征。

Result: 在2020-2024年的真实比特币交易数据上，该模型达到了0.9470的准确率和0.9807的AUC-ROC，优于所有基线模型。

Conclusion: 所提出的混合GCN-GRU模型能有效识别复杂区块链交易网络中的非法活动，性能显著优于现有方法。

Abstract: Blockchain transaction networks are complex, with evolving temporal patterns
and inter-node relationships. To detect illicit activities, we propose a hybrid
GCN-GRU model that captures both structural and sequential features. Using real
Bitcoin transaction data (2020-2024), our model achieved 0.9470 Accuracy and
0.9807 AUC-ROC, outperforming all baselines.

</details>


### [156] [EMORF-II: Adaptive EM-based Outlier-Robust Filtering with Correlated Measurement Noise](https://arxiv.org/abs/2509.07415)
*Arslan Majal,Aamir Hussain Chughtai,Muhammad Tahir*

Main category: cs.LG

TL;DR: 本文提出了一种名为EMORF-II的学习型抗异常值滤波器，它通过学习异常值特征来增强EMORF，在处理相关测量噪声时，能提供比现有先进方法更高的精度，同时保持与实用方法相当的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有滤波器在处理存在相关测量噪声的通用设置中，对异常值的鲁棒性可能不足。研究旨在开发一种更强的滤波器，能够学习异常值特性以进一步提升异常值抑制能力。

Method: 本文提出EMORF-II，它是EMORF（基于EM的抗异常值滤波器）的增强版本。EMORF-II的关键改进在于，它配备了在推理过程中学习异常值特征的功能，并结合异常值检测，从而提高了异常值缓解能力。

Result: 数值实验证实，与现有先进方法相比，EMORF-II在精度方面取得了性能提升。虽然计算开销有所增加，但其计算复杂度阶数与现有实用方法保持一致。

Conclusion: EMORF-II因其改进的异常值抑制能力和可与实用方法媲美的计算复杂度（尽管有额外开销），使其成为各种应用中的一个实用选择。

Abstract: We present a learning-based outlier-robust filter for a general setup where
the measurement noise can be correlated. Since it is an enhanced version of
EM-based outlier robust filter (EMORF), we call it as EMORF-II. As it is
equipped with an additional powerful feature to learn the outlier
characteristics during inference along with outlier-detection, EMORF-II has
improved outlier-mitigation capability. Numerical experiments confirm
performance gains as compared to the state-of-the-art methods in terms of
accuracy with an increased computational overhead. However, thankfully the
computational complexity order remains at par with other practical methods
making it a useful choice for diverse applications.

</details>


### [157] [The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2509.07430)
*Long Li,Jiaran Hao,Jason Klein Liu,Zhijian Zhou,Xiaoyu Tan,Wei Chu,Zhe Wang,Shirui Pan,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: LLMs在RLVR微调中存在Pass@k下降和灾难性遗忘问题。本文提出DPH-RL框架，通过将质量覆盖f-散度作为排练机制，持续参考初始策略以保持多样性，从而解决了Pass@k下降，并同时提升了Pass@1和Pass@k性能，且训练效率更高。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）使用可验证奖励强化学习（RLVR）进行微调时，普遍存在多尝试性能（Pass@k）下降但单尝试准确性（Pass@1）提高的矛盾，并伴随灾难性遗忘。现有研究对散度项的选择和功能作为主动解决方案缺乏深入探讨。标准的RLVR目标（无论是使用反向KL散度还是不使用散度项）都缺乏知识保留机制，反向KL甚至会加速这种衰减。

Method: 本文提出了一个名为“多样性保持混合强化学习”（DPH-RL）的框架。其核心思想是将散度项本身作为解决方案，利用覆盖大部分质量的f-散度（如前向KL散度、JS散度）作为“排练机制”。通过持续参考初始策略，该方法强制模型保持广泛的解决方案覆盖。此外，DPH-RL通过生成器函数计算f-散度，仅需从初始策略中采样，无需在线参考模型，从而提高了训练效率。

Result: 实验结果表明，DPH-RL不仅解决了Pass@k性能下降的问题，还在域内和域外任务上同时提高了Pass@1和Pass@k的性能。此外，DPH-RL具有更高的训练效率。这些结果在数学和SQL生成任务上得到了验证。

Conclusion: 本研究强调了在改进RLVR中，散度测量的正确选择是一个关键但被忽视的方面。它证明了正确选择散度量度是构建更通用和多样化推理模型的强大工具。

Abstract: A central paradox in fine-tuning Large Language Models (LLMs) with
Reinforcement Learning with Verifiable Reward (RLVR) is the frequent
degradation of multi-attempt performance (Pass@k) despite improvements in
single-attempt accuracy (Pass@1). This is often accompanied by catastrophic
forgetting, where models lose previously acquired skills. While various methods
have been proposed, the choice and function of the divergence term have been
surprisingly unexamined as a proactive solution. We argue that standard RLVR
objectives -- both those using the mode-seeking reverse KL-divergence and those
forgoing a divergence term entirely -- lack a crucial mechanism for knowledge
retention. The reverse-KL actively accelerates this decay by narrowing the
policy, while its absence provides no safeguard against the model drifting from
its diverse knowledge base. We propose a fundamental shift in perspective:
using the divergence term itself as the solution. Our framework,
Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences
(like forward-KL and JS-divergence) to function as a rehearsal mechanism. By
continuously referencing the initial policy, this approach forces the model to
maintain broad solution coverage. Extensive experiments on math and SQL
generation demonstrate that DPH-RL not only resolves the Pass@k degradation but
improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is
more training-efficient because it computes f-divergence using generator
functions, requiring only sampling from the initial policy and no online
reference model. Our work highlights a crucial, overlooked axis for improving
RLVR, demonstrating that the proper selection of a divergence measure is a
powerful tool for building more general and diverse reasoning models.

</details>


### [158] [Conv4Rec: A 1-by-1 Convolutional AutoEncoder for User Profiling through Joint Analysis of Implicit and Explicit Feedbacks](https://arxiv.org/abs/2509.07499)
*Antoine Ledent,Petr Kasalický,Rodrigo Alves,Hady W. Lauw*

Main category: cs.LG

TL;DR: 本文提出了一种新型卷积自编码器（CAE）用于用户建模和推荐，通过学习不同交互类型的关联、联合利用显式和隐式反馈，并提供单独的消费和高评分预测，提升了推荐系统的性能和可解释性。该模型在实际数据集上达到了最先进水平，并提供了泛化界限。


<details>
  <summary>Details</summary>
Motivation: 现有模型在学习不同交互类型关联方面缺乏灵活性；未能有效联合利用显式评分和隐式反馈；缺乏对消费概率和高评分可能性的独立预测，导致推荐结果缺乏足够的信息量；推荐系统中的自编码器模型缺少泛化界限。

Method: 引入了一种新的卷积自编码器架构。该模型能够灵活学习不同交互类型之间的关联和组合；联合利用显式评分和采样模式中的隐式反馈进行学习；能够独立预测内容消费概率和在消费后给予高评分的可能性；提出了该模型的泛化界限，并证明了损失函数优化可保证以小误差恢复精确的交互采样分布。

Result: 在多个真实数据集上的实验表明，该模型在隐式和显式反馈预测任务上均达到了最先进的性能，尽管仅使用一个模型。此外，模型通过对每个可能评分概率的独立预测，增加了可解释性。提供了推荐系统自编码器的首批泛化界限之一。

Conclusion: 所提出的卷积自编码器通过其创新架构、对显式和隐式反馈的联合处理以及提供更具信息量的独立预测，显著提升了用户建模和推荐的有效性和可解释性。理论泛化界限和优异的实验性能共同证明了其在推荐系统领域的先进性。

Abstract: We introduce a new convolutional AutoEncoder architecture for user modelling
and recommendation tasks with several improvements over the state of the art.
Firstly, our model has the flexibility to learn a set of associations and
combinations between different interaction types in a way that carries over to
each user and item. Secondly, our model is able to learn jointly from both the
explicit ratings and the implicit information in the sampling pattern (which we
refer to as `implicit feedback'). It can also make separate predictions for the
probability of consuming content and the likelihood of granting it a high
rating if observed. This not only allows the model to make predictions for both
the implicit and explicit feedback, but also increases the informativeness of
the predictions: in particular, our model can identify items which users would
not have been likely to consume naturally, but would be likely to enjoy if
exposed to them. Finally, we provide several generalization bounds for our
model, which to the best of our knowledge, are among the first generalization
bounds for auto-encoders in a Recommender Systems setting; we also show that
optimizing our loss function guarantees the recovery of the exact sampling
distribution over interactions up to a small error in total variation. In
experiments on several real-life datasets, we achieve state-of-the-art
performance on both the implicit and explicit feedback prediction tasks despite
relying on a single model for both, and benefiting from additional
interpretability in the form of individual predictions for the probabilities of
each possible rating.

</details>


### [159] [Water Demand Forecasting of District Metered Areas through Learned Consumer Representations](https://arxiv.org/abs/2509.07515)
*Adithya Ramachandran,Thorkil Flensmark B. Neergaard,Tomás Arias-Vergara,Andreas Maier,Siming Bayer*

Main category: cs.LG

TL;DR: 本文提出一种新的短时供水需求预测方法，通过无监督对比学习对DMA中的用户行为进行分类，并结合小波变换卷积网络和交叉注意力机制进行预测，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致水资源不确定性增加，确保供水成为全球紧迫问题。尽管有智能计量数据，但非确定性因素（如气象条件）使水需求预测仍具挑战性。

Method: 首先，采用无监督对比学习对DMA内用户进行分类，识别不同的消费行为。然后，将这些独特的消费行为作为特征，结合小波变换卷积网络和包含历史数据及派生表示的交叉注意力机制，进行短时水需求预测。

Result: 在真实DMA数据集上经过六个月的评估，该方法在不同DMA中均显示出预测性能的提升，MAPE最大改善达到4.9%。此外，它还识别出受社会经济因素影响的用户行为，增强了对影响需求确定性模式的先验知识。

Conclusion: 该方法通过结合用户行为分类和先进的神经网络技术，有效提高了短时水需求预测的准确性，并能识别影响需求的确定性模式，为水资源管理提供更深入的洞察。

Abstract: Advancements in smart metering technologies have significantly improved the
ability to monitor and manage water utilities. In the context of increasing
uncertainty due to climate change, securing water resources and supply has
emerged as an urgent global issue with extensive socioeconomic ramifications.
Hourly consumption data from end-users have yielded substantial insights for
projecting demand across regions characterized by diverse consumption patterns.
Nevertheless, the prediction of water demand remains challenging due to
influencing non-deterministic factors, such as meteorological conditions. This
work introduces a novel method for short-term water demand forecasting for
District Metered Areas (DMAs) which encompass commercial, agricultural, and
residential consumers. Unsupervised contrastive learning is applied to
categorize end-users according to distinct consumption behaviors present within
a DMA. Subsequently, the distinct consumption behaviors are utilized as
features in the ensuing demand forecasting task using wavelet-transformed
convolutional networks that incorporate a cross-attention mechanism combining
both historical data and the derived representations. The proposed approach is
evaluated on real-world DMAs over a six-month period, demonstrating improved
forecasting performance in terms of MAPE across different DMAs, with a maximum
improvement of 4.9%. Additionally, it identifies consumers whose behavior is
shaped by socioeconomic factors, enhancing prior knowledge about the
deterministic patterns that influence demand.

</details>


### [160] [RoseCDL: Robust and Scalable Convolutional Dictionary Learning for Rare-event Detection](https://arxiv.org/abs/2509.07523)
*Jad Yehya,Mansour Benbakoura,Cédric Allain,Benoît Malezieux,Matthieu Kowalski,Thomas Moreau*

Main category: cs.LG

TL;DR: 本文提出了RoseCDL，一种可扩展且鲁棒的卷积字典学习（CDL）算法，用于大规模信号中的无监督稀有事件检测，解决了现有CDL方法计算成本高和对异常值敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 识别大规模信号中的重复模式和稀有事件是许多领域（如天文学、生物医学）的基础挑战。卷积字典学习（CDL）虽能有效建模局部结构，但其在稀有事件检测方面的应用不足，且面临计算成本高和对伪影及异常值敏感的关键挑战。

Method: 引入RoseCDL算法，这是一种可扩展且鲁棒的CDL算法，专为长信号中的无监督稀有事件检测而设计。RoseCDL通过结合随机窗口化实现大规模数据集的高效训练，并内置异常值检测功能以增强鲁棒性并隔离异常模式。

Result: RoseCDL能够高效、鲁棒地进行稀有事件检测，并有效隔离异常模式，解决了传统CDL在这一应用场景下的局限性。

Conclusion: RoseCDL将卷积字典学习重塑为一种实用的工具，可用于真实世界信号中的事件发现和表征，扩展了其在传统压缩或去噪任务之外的应用。

Abstract: Identifying recurring patterns and rare events in large-scale signals is a
fundamental challenge in fields such as astronomy, physical simulations, and
biomedical science. Convolutional Dictionary Learning (CDL) offers a powerful
framework for modeling local structures in signals, but its use for detecting
rare or anomalous events remains largely unexplored. In particular, CDL faces
two key challenges in this setting: high computational cost and sensitivity to
artifacts and outliers. In this paper, we introduce RoseCDL, a scalable and
robust CDL algorithm designed for unsupervised rare event detection in long
signals. RoseCDL combines stochastic windowing for efficient training on large
datasets with inline outlier detection to enhance robustness and isolate
anomalous patterns. This reframes CDL as a practical tool for event discovery
and characterization in real-world signals, extending its role beyond
traditional tasks like compression or denoising.

</details>


### [161] [$ΔL$ Normalization: Rethink Loss Aggregation in RLVR](https://arxiv.org/abs/2509.07558)
*Zhiyuan He,Xufang Luo,Yike Zhang,Yuqing Yang,Lili Qiu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose $\Delta L$ Normalization, a simple yet effective loss aggregation
method tailored to the characteristic of dynamic generation lengths in
Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has
demonstrated strong potential in improving the reasoning capabilities of large
language models (LLMs), but a major challenge lies in the large variability of
response lengths during training, which leads to high gradient variance and
unstable optimization. Although previous methods such as GRPO, DAPO, and Dr.
GRPO introduce different loss normalization terms to address this issue, they
either produce biased estimates or still suffer from high gradient variance. By
analyzing the effect of varying lengths on policy loss both theoretically and
empirically, we reformulate the problem as finding a minimum-variance unbiased
estimator. Our proposed $\Delta L$ Normalization not only provides an unbiased
estimate of the true policy loss but also minimizes gradient variance in
theory. Extensive experiments show that it consistently achieves superior
results across different model sizes, maximum lengths, and tasks. Our code will
be made public at https://github.com/zerolllin/Delta-L-Normalization.

</details>


### [162] [uGMM-NN: Univariate Gaussian Mixture Model Neural Network](https://arxiv.org/abs/2509.07569)
*Zakeria Sharif Ali*

Main category: cs.LG

TL;DR: 本文介绍了一种名为uGMM-NN的新型神经网络架构，它将概率推理直接嵌入到深度网络的计算单元中，通过将神经元激活参数化为单变量高斯混合模型，以捕捉多模态和不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统的神经元采用加权和后接固定非线性函数，无法捕捉多模态和不确定性。本文旨在设计一种能直接在单个神经元层面实现更丰富表示，并融入概率推理的深度网络。

Method: uGMM-NN的每个节点将其激活参数化为单变量高斯混合模型，具有可学习的均值、方差和混合系数。这种设计在保留标准前馈网络可扩展性的同时，实现了更丰富的表示。

Result: 实验证明，uGMM-NN在判别性能上与传统多层感知器相当，并额外提供了激活的概率解释。

Conclusion: 该框架为将不确定性感知组件集成到现代神经网络架构中奠定了基础，为判别式和生成式建模开辟了新方向。

Abstract: This paper introduces the Univariate Gaussian Mixture Model Neural Network
(uGMM-NN), a novel neural architecture that embeds probabilistic reasoning
directly into the computational units of deep networks. Unlike traditional
neurons, which apply weighted sums followed by fixed nonlinearities, each
uGMM-NN node parameterizes its activations as a univariate Gaussian mixture,
with learnable means, variances, and mixing coefficients. This design enables
richer representations by capturing multimodality and uncertainty at the level
of individual neurons, while retaining the scalability of standard feedforward
networks. We demonstrate that uGMM-NN can achieve competitive discriminative
performance compared to conventional multilayer perceptrons, while additionally
offering a probabilistic interpretation of activations. The proposed framework
provides a foundation for integrating uncertainty-aware components into modern
neural architectures, opening new directions for both discriminative and
generative modeling.

</details>


### [163] [Homogenization with Guaranteed Bounds via Primal-Dual Physically Informed Neural Networks](https://arxiv.org/abs/2509.07579)
*Liya Gaynutdinova,Martin Doškář,Ondřej Rokoš,Ivana Pultarová*

Main category: cs.LG

TL;DR: PINNs在处理不连续材料时易失效。本文引入PINN的对偶公式，通过提供误差界限，提高了PINN在周期性热导复合材料均质化问题中的可靠性和失效诊断能力。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络（PINNs）在解决多尺度建模相关的偏微分方程时具有潜力，但在应用于具有不连续系数（如分段常数性质）的材料时经常失效。

Method: 本文为PINN框架引入了一种对偶公式，以提高周期性热导复合材料均质化（包括强形式和变分（弱）形式）的可靠性。该对偶方法有助于推导有保证的误差上下限，从而更可靠地检测PINN失效。研究还比较了应用于平滑材料近似的标准PINNs与使用谱函数和基于神经网络的测试函数的变分PINNs（VPINNs）。

Result: 研究结果表明，强形式PINNs在受控设置下可能优于VPINNs，但它们对材料不连续性敏感，可能在没有明确诊断的情况下失效。相比之下，VPINNs可以直接处理分段常数材料参数，但需要仔细选择测试函数以避免不稳定。对偶公式可作为收敛质量的可靠指标。

Conclusion: 对偶公式的引入提高了PINN框架在微观力学均质化问题中的适用性，通过提供收敛质量的可靠指示和更鲁棒的失效检测。

Abstract: Physics-informed neural networks (PINNs) have shown promise in solving
partial differential equations (PDEs) relevant to multiscale modeling, but they
often fail when applied to materials with discontinuous coefficients, such as
media with piecewise constant properties. This paper introduces a dual
formulation for the PINN framework to improve the reliability of the
homogenization of periodic thermo-conductive composites, for both strong and
variational (weak) formulations. The dual approach facilitates the derivation
of guaranteed upper and lower error bounds, enabling more robust detection of
PINN failure. We compare standard PINNs applied to smoothed material
approximations with variational PINNs (VPINNs) using both spectral and neural
network-based test functions. Our results indicate that while strong-form PINNs
may outperform VPINNs in controlled settings, they are sensitive to material
discontinuities and may fail without clear diagnostics. In contrast, VPINNs
accommodate piecewise constant material parameters directly but require careful
selection of test functions to avoid instability. Dual formulation serves as a
reliable indicator of convergence quality, and its integration into PINN
frameworks enhances their applicability to homogenization problems in
micromechanics.

</details>


### [164] [Transformer-Based Approach to Optimal Sensor Placement for Structural Health Monitoring of Probe Cards](https://arxiv.org/abs/2509.07603)
*Mehdi Bejani,Marco Mauri,Daniele Acconcia,Simone Todaro,Stefano Mariani*

Main category: cs.LG

TL;DR: 本研究提出一种创新的基于Transformer的深度学习策略，用于优化半导体探针卡健康监测中的传感器布局，以高精度识别故障并提升生产良率。


<details>
  <summary>Details</summary>
Motivation: 半导体探针卡故障（如基板裂纹、螺丝松动）严重影响半导体制造的良率和可靠性。通过传感器进行故障检测和预防性维护至关重要。

Method: 该研究采用有限元模型模拟探针卡故障场景的频率响应函数，并构建了一个通过物理信息场景扩展和物理感知统计数据增强的综合数据集。随后，训练了一个混合卷积神经网络和Transformer模型。利用Transformer的注意力机制来识别关键传感器位置，并通过3次重复的10折分层交叉验证来确认模型的鲁棒性。

Result: 该模型在分类探针卡健康状态（基线、螺丝松动、裂纹）方面达到了99.83%的高精度，并在裂纹检测中实现了99.73%的优秀召回率。注意力机制的分析也为设计高效、经济的监控系统提供了优化传感器配置的实用见解。

Conclusion: 本研究强调了基于注意力机制的深度学习在推进主动维护、提高半导体制造运营可靠性和良率方面的强大能力，为优化传感器布局和实现高效健康监测提供了新途径。

Abstract: This paper presents an innovative Transformer-based deep learning strategy
for optimizing the placement of sensors aiming at structural health monitoring
of semiconductor probe cards. Failures in probe cards, including substrate
cracks and loosened screws, would critically affect semiconductor manufacturing
yield and reliability. Some failure modes could be detected by equipping a
probe card with adequate sensors. Frequency response functions from simulated
failure scenarios are adopted within a finite element model of a probe card. A
comprehensive dataset, enriched by physics-informed scenario expansion and
physics-aware statistical data augmentation, is exploited to train a hybrid
Convolutional Neural Network and Transformer model. The model achieves high
accuracy (99.83%) in classifying the probe card health states (baseline, loose
screw, crack) and an excellent crack detection recall (99.73%). Model
robustness is confirmed through a rigorous framework of 3 repetitions of
10-fold stratified cross-validation. The attention mechanism also pinpoints
critical sensor locations: an analysis of the attention weights offers
actionable insights for designing efficient, cost-effective monitoring systems
by optimizing sensor configurations. This research highlights the capability of
attention-based deep learning to advance proactive maintenance, enhancing
operational reliability and yield in semiconductor manufacturing.

</details>


### [165] [K2-Think: A Parameter-Efficient Reasoning System](https://arxiv.org/abs/2509.07604)
*Zhoujun Cheng,Richard Fan,Shibo Hao,Taylor W. Killian,Haonan Li,Suqi Sun,Hector Ren,Alexander Moreno,Daqian Zhang,Tianjun Zhong,Yuxin Xiong,Yuanzhe Hu,Yutao Xie,Xudong Han,Yuqi Wang,Varad Pimpalkhute,Yonghao Zhuang,Aaryamonvikram Singh,Xuezhi Liang,Anze Xie,Jianshu She,Desai Fan,Chengqian Gao,Liqun Ma,Mikhail Yurochkin,John Maggs,Xuezhe Ma,Guowei He,Zhiting Hu,Zhengzhong Liu,Eric P. Xing*

Main category: cs.LG

TL;DR: K2-Think是一个32B参数的推理系统，基于Qwen2.5，通过先进的后训练和测试时计算技术，在数学推理等任务上达到了SOTA性能，超越或媲美GPT-OSS 120B等大型模型，并使开源推理系统更易于获取和负担。


<details>
  <summary>Details</summary>
Motivation: 研究旨在证明较小参数规模的模型（32B）通过结合先进的后训练和测试时计算技术，也能在推理任务上达到顶级性能，从而提高开源推理系统的可访问性和经济性。

Method: 该系统基于Qwen2.5模型，采用了六大核心技术：长链式思维监督微调、可验证奖励的强化学习 (RLVR)、推理前的智能体规划、测试时缩放、推测解码和推理优化硬件，并使用公开开源数据集。

Result: K2-Think 32B模型在数学推理方面达到了开源模型的SOTA分数，并在代码和科学等领域表现出色。其性能与GPT-OSS 120B和DeepSeek v3.1等大型模型相当或超越，并通过Cerebras Wafer-Scale Engine实现了每秒超过2,000 tokens的推理速度。

Conclusion: 研究结果证实，通过集成后训练方法（包括长链式思维训练和策略性推理时增强），参数效率更高的K2-Think 32B模型能够与最先进的系统竞争，从而使开源推理系统更加易于获取和负担。

Abstract: K2-Think is a reasoning system that achieves state-of-the-art performance
with a 32B parameter model, matching or surpassing much larger models like
GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system
shows that smaller models can compete at the highest levels by combining
advanced post-training and test-time computation techniques. The approach is
based on six key technical pillars: Long Chain-of-thought Supervised
Finetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic
planning prior to reasoning, Test-time Scaling, Speculative Decoding, and
Inference-optimized Hardware, all using publicly available open-source
datasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art
scores on public benchmarks for open-source models, while also performing
strongly in other areas such as Code and Science. Our results confirm that a
more parameter-efficient model like K2-Think 32B can compete with
state-of-the-art systems through an integrated post-training recipe that
includes long chain-of-thought training and strategic inference-time
enhancements, making open-source reasoning systems more accessible and
affordable. K2-Think is freely available at k2think.ai, offering best-in-class
inference speeds of over 2,000 tokens per second per request via the Cerebras
Wafer-Scale Engine.

</details>


### [166] [Beyond Rebalancing: Benchmarking Binary Classifiers Under Class Imbalance Without Rebalancing Techniques](https://arxiv.org/abs/2509.07605)
*Ali Nawaz,Amir Ahmad,Shehroz S. Khan*

Main category: cs.LG

TL;DR: 本文评估了二元分类器在不使用任何显式重平衡技术的情况下，在类别不平衡数据上的“原样”性能，发现先进模型在极端不平衡条件下比传统模型表现出更好的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡是监督分类中的一大挑战，尤其在医疗诊断等关键领域。尽管已有大量研究探讨重平衡技术，但很少有研究评估二元分类器在不应用任何此类技术时的性能。因此，本研究旨在评估二元分类器在不进行显式重平衡的情况下，其性能表现如何。

Method: 研究系统地评估了各种二元分类器在真实世界和合成数据集上的鲁棒性，逐步减少少数类规模（包括one-shot和few-shot场景）并生成合成决策边界以模拟不同数据复杂性。此外，还通过使用欠采样、过采样和一类分类（OCC）方法进行实验，以考察它们在严重不平衡下的行为。

Result: 研究结果证实，随着数据复杂性增加和少数类规模减小，分类难度随之增大。传统分类器在极端不平衡条件下性能显著下降，而TabPFN和基于Boosting的集成模型等先进模型则保持了相对更高的性能和更好的泛化能力。

Conclusion: 本研究为不平衡学习中的模型选择提供了宝贵指导，揭示了分类器在不依赖显式重平衡技术情况下的鲁棒性，有助于在面临类别不平衡时做出更明智的模型选择。

Abstract: Class imbalance poses a significant challenge to supervised classification,
particularly in critical domains like medical diagnostics and anomaly detection
where minority class instances are rare. While numerous studies have explored
rebalancing techniques to address this issue, less attention has been given to
evaluating the performance of binary classifiers under imbalance when no such
techniques are applied. Therefore, the goal of this study is to assess the
performance of binary classifiers "as-is", without performing any explicit
rebalancing. Specifically, we systematically evaluate the robustness of a
diverse set of binary classifiers across both real-world and synthetic
datasets, under progressively reduced minority class sizes, using one-shot and
few-shot scenarios as baselines. Our approach also explores varying data
complexities through synthetic decision boundary generation to simulate
real-world conditions. In addition to standard classifiers, we include
experiments using undersampling, oversampling strategies, and one-class
classification (OCC) methods to examine their behavior under severe imbalance.
The results confirm that classification becomes more difficult as data
complexity increases and the minority class size decreases. While traditional
classifiers deteriorate under extreme imbalance, advanced models like TabPFN
and boosting-based ensembles retain relatively higher performance and better
generalization compared to traditional classifiers. Visual interpretability and
evaluation metrics further validate these findings. Our work offers valuable
guidance on model selection for imbalanced learning, providing insights into
classifier robustness without dependence on explicit rebalancing techniques.

</details>


### [167] [Graph-based Integrated Gradients for Explaining Graph Neural Networks](https://arxiv.org/abs/2509.07648)
*Lachlan Simpson,Kyle Millar,Adriel Cheng,Cheng-Chew Lim,Hong Gunn Chew*

Main category: cs.LG

TL;DR: 本文提出了图基集成梯度（GB-IG），将集成梯度（IG）扩展到图结构，并在合成和真实数据集上证明了其在准确识别图关键特征方面的有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 传统的Integrated Gradients (IG)技术适用于连续数据，但图是离散结构，导致IG不适用于图。因此，需要一种能够解释图神经网络黑箱问题的、适用于图结构的集成梯度方法。

Method: 提出了图基集成梯度（GB-IG），它是Integrated Gradients (IG)在图结构上的扩展。

Result: 在四个合成数据集上，GB-IG能够准确识别分类任务中图的关键结构组件。在三个主流真实世界图数据集上，GB-IG在突出节点分类任务中的重要特征方面优于IG。

Conclusion: GB-IG成功地将集成梯度技术扩展到图结构，有效解决了图神经网络的黑箱问题，并在识别图重要特征方面表现出优越性。

Abstract: Integrated Gradients (IG) is a common explainability technique to address the
black-box problem of neural networks. Integrated gradients assumes continuous
data. Graphs are discrete structures making IG ill-suited to graphs. In this
work, we introduce graph-based integrated gradients (GB-IG); an extension of IG
to graphs. We demonstrate on four synthetic datasets that GB-IG accurately
identifies crucial structural components of the graph used in classification
tasks. We further demonstrate on three prevalent real-world graph datasets that
GB-IG outperforms IG in highlighting important features for node classification
tasks.

</details>


### [168] [FUnc-SNE: A flexible, Fast, and Unconstrained algorithm for neighbour embeddings](https://arxiv.org/abs/2509.07681)
*Pierre Lambert,Edouard Couplet,Michel Verleysen,John Aldo Lee*

Main category: cs.LG

TL;DR: 本文提出一种新颖的加速邻域嵌入（NE）方法，旨在兼顾高速度、细粒度结构保留及超参数灵活性，支持任意维度，并特别优化了交互式数据探索，其核心是创新的迭代近似最近邻搜索算法。


<details>
  <summary>Details</summary>
Motivation: 现有加速邻域嵌入方法存在局限：粗糙近似方法（如UMAP）速度快但可能牺牲结构质量；而较精细的方法（如FIt-SNE、BH-t-SNE）虽能更好地保留结构，但速度慢、仅限于2D/3D，且限制了NE的应用范围。因此，需要一种能结合两者优势，既快速又能保持良好结构，且支持高维嵌入空间，并适用于交互式探索的新方法。

Method: 该研究提出了一种新的邻域嵌入加速方法，特点是每次迭代计算量小，同时通过超参数调节实现良好的细粒度结构保留和灵活性，且不限制嵌入空间的维度。该方法专为数据交互式探索设计，摒弃了传统NE方法的两阶段范式，允许在调整超参数时提供即时视觉反馈。算法的核心是一个新颖的迭代近似最近邻搜索方法。

Result: 通过公开可用的GPU加速GUI集成实验表明，该方法在速度和提取结构灵活性方面表现出有希望的结果。它在更广泛的机器学习背景下具有潜在应用价值，只需最少的算法修改。其核心的迭代近似最近邻搜索方法与最近邻下降法相比，也显示出有希望的结果。

Conclusion: 本研究成功地弥合了现有邻域嵌入加速方法之间的鸿沟，提出了一种在速度、结构保留和灵活性之间取得平衡的新方法。该方法适用于交互式数据探索和更广泛的机器学习应用，且其新颖的迭代近似最近邻搜索方法也表现出色。

Abstract: Neighbour embeddings (NE) allow the representation of high dimensional
datasets into lower dimensional spaces and are often used in data
visualisation. In practice, accelerated approximations are employed to handle
very large datasets. Accelerating NE is challenging, and two main directions
have been explored: very coarse approximations based on negative sampling (as
in UMAP) achieve high effective speed but may lack quality in the extracted
structures; less coarse approximations, as used in FIt-SNE or BH-t-SNE, offer
better structure preservation at the cost of speed, while also restricting the
target dimensionality to 2 or 3, limiting NE to visualisation. In some
variants, the precision of these costlier accelerations also enables
finer-grained control on the extracted structures through dedicated
hyperparameters.
  This paper proposes to bridge the gab between both approaches by introducing
a novel way to accelerate NE, requiring a small number of computations per
iteration while maintaining good fine-grained structure preservation and
flexibility through hyperparameter tuning, without limiting the dimensionality
of the embedding space. The method was designed for interactive exploration of
data; as such, it abandons the traditional two-phased approach of other NE
methods, allowing instantaneous visual feedback when changing hyperparameters,
even when these control processes happening on the high-dimensional side of the
computations. Experiments using a publicly available, GPU accelerated GUI
integration of the method show promising results in terms of speed, flexibility
in the structures getting extracted, and show potential uses in broader machine
learning contexts with minimal algorithmic modifications. Central to this
algorithm is a novel approach to iterative approximate nearest neighbour
search, which shows promising results compared to nearest neighbour descent.

</details>


### [169] [IBN: An Interpretable Bidirectional-Modeling Network for Multivariate Time Series Forecasting with Variable Missing](https://arxiv.org/abs/2509.07725)
*Shusen Ma,Tianhao Zhang,Qijiu Xia,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 本文提出可解释双向建模网络IBN，通过不确定性感知插值和高斯核图卷积，解决了多变量时间序列预测中变量缺失导致的互变量关联建模困难及现有方法解释性差的问题，提升了预测性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列预测（MTSF）常面临变量缺失的挑战，这阻碍了传统方法建模变量间关联。现有方法（如GinAR）虽能处理缺失变量，但缺乏可解释性，且简单递归单元未能捕捉更深层时间模式。

Method: 本文提出IBN，整合不确定性感知插值（UAI）和高斯核图卷积（GGCN）。UAI利用MC Dropout估计重建值的不确定性，并通过不确定性加权策略降低高风险重建。GGCN明确建模变量间空间关联。同时，采用双向递归单元增强时间依赖建模。

Result: 广泛实验表明，IBN在不同缺失率情景下均达到了最先进的预测性能。

Conclusion: IBN为解决缺失变量的MTSF问题提供了一个更可靠和可解释的框架。

Abstract: Multivariate time series forecasting (MTSF) often faces challenges from
missing variables, which hinder conventional spatial-temporal graph neural
networks in modeling inter-variable correlations. While GinAR addresses
variable missing using attention-based imputation and adaptive graph learning
for the first time, it lacks interpretability and fails to capture more latent
temporal patterns due to its simple recursive units (RUs). To overcome these
limitations, we propose the Interpretable Bidirectional-modeling Network (IBN),
integrating Uncertainty-Aware Interpolation (UAI) and Gaussian kernel-based
Graph Convolution (GGCN). IBN estimates the uncertainty of reconstructed values
using MC Dropout and applies an uncertainty-weighted strategy to mitigate
high-risk reconstructions. GGCN explicitly models spatial correlations among
variables, while a bidirectional RU enhances temporal dependency modeling.
Extensive experiments show that IBN achieves state-of-the-art forecasting
performance under various missing-rate scenarios, providing a more reliable and
interpretable framework for MTSF with missing variables. Code is available at:
https://github.com/zhangth1211/NICLab-IBN.

</details>


### [170] [MoE-Compression: How the Compression Error of Experts Affects the Inference Accuracy of MoE Model?](https://arxiv.org/abs/2509.07727)
*Songkai Ma,Zhaorui Zhang,Sheng Di,Benben Liu,Xiaodong Yu,Xiaoyi Lu,Dan Wang*

Main category: cs.LG

TL;DR: 为解决MoE模型在有限GPU内存下高效服务问题，本研究提出使用误差有界有损压缩算法压缩非激活专家，并分析了不同层级专家压缩误差对推理性能的影响。


<details>
  <summary>Details</summary>
Motivation: MoE模型广泛应用于LLM学习，但在有限GPU内存下高效服务面临挑战。将非激活专家卸载至主内存是有效方法，但带来GPU与主内存之间专家传输开销大。需要探索高效压缩专家的方法，并分析压缩误差对推理性能的影响。

Method: 采用误差有界有损压缩算法（如SZ3和CuSZp）压缩非激活专家，以减少数据传输开销。在多个基准上进行广泛实验，全面分析不同层级（浅层、中层、深层）专家引入的压缩误差对整体推理准确性的影响。

Result: 实验结果表明：浅层专家（负责注意力机制和输入token转换）在有界误差下推理准确性下降最小；中层专家（模型推理核心）的误差显著损害推理准确性；深层专家（负责指令遵循和输出整合）引入有界误差有时反而能提高推理准确性。

Conclusion: 压缩误差对MoE模型推理准确性的影响与专家所在的层级及其功能密切相关。这为设计分层压缩策略以优化MoE模型服务提供了依据。

Abstract: With the widespread application of Mixture of Experts (MoE) reasoning models
in the field of LLM learning, efficiently serving MoE models under limited GPU
memory constraints has emerged as a significant challenge. Offloading the
non-activated experts to main memory has been identified as an efficient
approach to address such a problem, while it brings the challenges of
transferring the expert between the GPU memory and main memory. We need to
explore an efficient approach to compress the expert and analyze how the
compression error affects the inference performance.
  To bridge this gap, we propose employing error-bounded lossy compression
algorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby
reducing data transfer overhead during MoE inference. We conduct extensive
experiments across various benchmarks and present a comprehensive analysis of
how compression-induced errors in different experts affect overall inference
accuracy. The results indicate that experts in the shallow layers, which are
primarily responsible for the attention mechanism and the transformation of
input tokens into vector representations, exhibit minimal degradation in
inference accuracy when subjected to bounded errors. In contrast, errors in the
middle-layer experts, which are central to model reasoning, significantly
impair inference accuracy. Interestingly, introducing bounded errors in the
deep-layer experts, which are mainly responsible for instruction following and
output integration, can sometimes lead to improvements in inference accuracy.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [171] [SCION Path Performance Toolkit and Benchmark for Advancing Machine Learning in Next-Generation Networks: ScionPathML](https://arxiv.org/abs/2509.07154)
*Damien Rossi,Lars Herschbach,Sina Keshvadi*

Main category: cs.NI

TL;DR: 对SCIONLab测试床的测量揭示了路径感知网络中路径的高度动态性、不对称性及性能权衡，挑战了现有MPQUIC等协议设计中的假设。


<details>
  <summary>Details</summary>
Motivation: 路径感知网络虽有望提升性能和弹性，但缺乏其真实世界动态的实证数据，阻碍了如MPQUIC等有效协议的设计。本研究旨在填补这一空白，理解其路径稳定性、多样性和性能。

Method: 通过对全球SCIONLab测试床上的SCION架构进行长期测量研究，以表征其路径稳定性、多样性和性能。

Result: 测量发现：测试床部分区域控制平面波动剧烈，路径寿命短暂；识别并描述了由路由策略导致的路径可用性不对称现象（路径差异）；观察到性能权衡，即并发多路径传输可提高总吞吐量，但可能降低单个路径的延迟和可靠性。

Conclusion: 研究结果表明，如MPQUIC等多路径协议在设计时应明确考虑高波动性和路径不对称性，这挑战了多路径协议设计中的普遍假设。

Abstract: Path-aware networks promise enhanced performance and resilience through
multipath transport, but a lack of empirical data on their real-world dynamics
hinders the design of effective protocols. This paper presents a longitudinal
measurement study of the SCION architecture on the global SCIONLab testbed,
characterizing the path stability, diversity, and performance crucial for
protocols like Multipath QUIC (MPQUIC). Our measurements reveal a dynamic
environment, with significant control-plane churn and short path lifetimes in
parts of the testbed. We identify and characterize path discrepancy, a
phenomenon where routing policies create asymmetric path availability between
endpoints. Furthermore, we observe a performance trade-off where concurrent
multipath transmissions can improve aggregate throughput but may degrade the
latency and reliability of individual paths. These findings demonstrate that
protocols such as MPQUIC should explicitly account for high churn and path
asymmetry, challenging common assumptions in multipath protocol design.

</details>


### [172] [DORA: Dynamic O-RAN Resource Allocation for Multi-Slice 5G Networks](https://arxiv.org/abs/2509.07242)
*Alireza Ebrahimi Dorcheh,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.NI

TL;DR: DORA是一个基于深度强化学习的O-RAN动态资源分配框架，通过PPO智能体实现切片级PRB分配，支持在线训练，在拥塞场景下显著优于基线，有效优化了5G多业务的QoS。


<details>
  <summary>Details</summary>
Motivation: 5G网络需同时支持URLLC、eMBB和mMTC等多种QoS要求各异的异构服务。在有限频谱资源下，满足这些需求需要自适应且符合标准的无线资源管理。

Method: 本文提出了DORA（动态O-RAN资源分配），一个用于Open RAN中动态切片级物理资源块（PRB）分配的深度强化学习（DRL）框架。DORA采用基于PPO的强化学习智能体，根据观察到的流量需求和信道条件，在URLLC、eMBB和mMTC切片之间分配PRB。切片内PRB调度通过活跃UE间的轮询机制确定性处理。DORA支持在线训练，并作为O-RAN xApp无缝集成到RAN智能控制器（RICs）。

Result: 在拥塞条件下进行的广泛评估表明，DORA的性能优于三种非学习基线和一个DQN智能体，实现了更低的URLLC延迟、更高的eMBB吞吐量（SLA违规更少），以及更广的mMTC覆盖范围，同时不会饿死高优先级切片。

Conclusion: DORA是首个为O-RAN中自适应、切片感知型PRB分配而设计的全在线深度强化学习框架。

Abstract: The fifth generation (5G) of wireless networks must simultaneously support
heterogeneous service categories, including Ultra-Reliable Low-Latency
Communications (URLLC), enhanced Mobile Broadband (eMBB), and massive
Machine-Type Communications (mMTC), each with distinct Quality of Service (QoS)
requirements. Meeting these demands under limited spectrum resources requires
adaptive and standards-compliant radio resource management. We present DORA
(Dynamic O-RAN Resource Allocation), a deep reinforcement learning (DRL)
framework for dynamic slice-level Physical Resource Block (PRB) allocation in
Open RAN. DORA employs a PPO-based RL agent to allocate PRBs across URLLC,
eMBB, and mMTC slices based on observed traffic demands and channel conditions.
Intra-slice PRB scheduling is handled deterministically via round-robin among
active UEs, simplifying control complexity and improving training stability.
Unlike prior work, DORA supports online training and adapts continuously to
evolving traffic patterns and cross-slice contention. Implemented in the
standards-compliant OpenAirInterface (OAI) RAN stack and designed for
deployment as an O-RAN xApp, DORA integrates seamlessly with RAN Intelligent
Controllers (RICs). Extensive evaluation under congested regimes shows that
DORA outperforms three non-learning baselines and a \texttt{DQN} agent,
achieving lower URLLC latency, higher eMBB throughput with fewer SLA
violations, and broader mMTC coverage without starving high-priority slices. To
our knowledge, this is the first fully online DRL framework for adaptive,
slice-aware PRB allocation in O-RAN.

</details>


### [173] [TEGRA: A Flexible & Scalable NextGen Mobile Core](https://arxiv.org/abs/2509.07410)
*Bilal Saleem,Omar Basit,Jiayi Meng,Iftekhar Alam,Ajay Thakur,Christian Maciocco,Muhammad Shahbaz,Y. Charlie Hu,Larry Peterson*

Main category: cs.NI

TL;DR: 本文提出TEGRA，一种高性能、灵活且可扩展的SBA移动核心网，通过利用边缘位置优化性能，解决了现有核心网在灵活性和性能之间的权衡问题，并显著降低了延迟和部署复杂性。


<details>
  <summary>Details</summary>
Motivation: 5G和6G移动核心网正重构为基于云服务的SBA架构，但现有性能优化策略仍沿用传统NFV技术，导致可能在灵活性和可扩展性/性能之间存在权衡。研究旨在探讨这种权衡是否是固有的。

Method: 引入了弹性SBA微服务设计模式和状态管理策略，并提出了TEGRA。TEGRA通过利用移动核心网在端到端互联网生态系统中的独特位置（即最后一英里边缘）来优化性能，同时不牺牲适应性。

Result: TEGRA实现了显著更低的延迟，处理请求速度比传统SBA核心实现（free5GC、Open5GS、Aether）分别快20倍、11倍和1.75倍。同时，它能匹配最先进核心（如CoreKube）的性能并保持灵活性，且大大减少了部署新功能所需的代码行数。

Conclusion: TEGRA证明，在基于SBA的移动核心网中，性能优化不一定以牺牲适应性为代价，提供了一个高性能、灵活且可扩展的解决方案，打破了传统的核心网设计瓶颈。

Abstract: To support emerging mobile use cases (e.g., AR/VR, autonomous driving, and
massive IoT), next-generation mobile cores for 5G and 6G are being
re-architected as service-based architectures (SBAs) running on both private
and public clouds. However, current performance optimization strategies for
scaling these cores still revert to traditional NFV-based techniques, such as
consolidating functions into rigid, monolithic deployments on dedicated
servers. This raises a critical question: Is there an inherent tradeoff between
flexibility and scalability in an SBA-based mobile core, where improving
performance (and resiliency) inevitably comes at the cost of one or the other?
  To explore this question, we introduce resilient SBA microservices design
patterns and state-management strategies, and propose TEGRA -- a
high-performance, flexible, and scalable SBA-based mobile core. By leveraging
the mobile core's unique position in the end-to-end internet ecosystem (i.e.,
at the last-mile edge), TEGRA optimizes performance without compromising
adaptability. Our evaluation demonstrates that TEGRA achieves significantly
lower latencies, processing requests 20x, 11x, and 1.75x faster than
traditional SBA core implementations -- free5GC, Open5GS, and Aether,
respectively -- all while matching the performance of state-of-the-art cores
(e.g., CoreKube) while retaining flexibility. Furthermore, it reduces the
complexity of deploying new features, requiring orders of magnitude fewer lines
of code (LoCs) compared to existing cores.

</details>


### [174] [Network-accelerated Active Messages](https://arxiv.org/abs/2509.07431)
*Md Ashfaqur Rahaman,Alireza Sanaee,Todd Thornley,Sebastiano Miano,Gianni Antichi,Brent E. Stephens,Ryan Stutsman*

Main category: cs.NI

TL;DR: NAAM是一个网络加速活跃消息系统，通过将eBPF函数与消息关联，实现逻辑在客户端、SmartNIC和服务器CPU之间动态调度执行，解决了应用程序在何处运行逻辑的困境，并展示了其在性能和扩展性上的优势。


<details>
  <summary>Details</summary>
Motivation: RDMA虽然提升网络性能但操作有限且编程复杂；SmartNICs能卸载工作但应用程序面临在服务器RPC、RDMA客户端逻辑或SmartNIC上运行逻辑的复杂选择，且最佳选择随工作负载和时间变化，导致开发困境。

Method: 提出NAAM（网络加速活跃消息）系统。NAAM应用将小型、可移植的eBPF函数与消息关联，消息通过类似RDMA的接口指定数据访问。NAAM运行时利用eBPF的便携性，能将消息及其逻辑动态调度到客户端、服务器SmartNIC或服务器主机CPU核上执行，以在最合理的位置运行代码。

Result: 构建了MICA哈希表和Cell风格B树查找等应用。使用NVIDIA BlueField-2 SmartNIC，NAAM能在客户端、服务器和NIC核心上运行操作，并在服务器计算拥塞时在数十毫秒内转移负载。NAAM能从服务器CPU动态卸载高达180万次/秒的MICA操作（YCSB-B）和75万次/秒的Cell查找。与最先进的iPipe框架相比，NAAM可扩展到数百个应用卸载，而iPipe仅支持8个，且eBPF的低开销对尾延迟影响最小。

Conclusion: NAAM通过提供一个灵活、可扩展的动态卸载框架，解决了在分布式系统中选择运行逻辑位置的难题。它利用eBPF的便携性在客户端、SmartNIC和服务器CPU之间无缝调度工作，显著提升了性能和应用卸载的扩展性。

Abstract: Remote Direct Memory Access (RDMA) improves host networking performance by
eliminating software and server CPU involvement. However, RDMA has a limited
set of operations, is difficult to program, and often requires multiple round
trips to perform simple application operations. Programmable SmartNICs provide
a different means to offload work from host CPUs to a NIC. This leaves
applications with the complex choice of embedding logic as RPC handlers at
servers, using RDMA's limited interface to access server structures via
client-side logic, or running some logic on SmartNICs. The best choice varies
between workloads and over time. To solve this dilemma, we present NAAM,
network-accelerated active messages. NAAM applications specify small, portable
eBPF functions associated with messages. Each message specifies what data it
accesses using an RDMA-like interface. NAAM runs at various places in the
network, including at clients, on server-attached SmartNICs, and server host
CPU cores. Due to eBPF's portability, the code associated with a message can be
run at any location. Hence, the NAAM runtime can dynamically steer any message
to execute its associated logic wherever it makes the most sense. To
demonstrate NAAM's flexibility, we built several applications, including the
MICA hash table and lookups from a Cell-style B-tree. With an NVIDIA
BlueField-2 SmartNIC and integrating its NIC-embedded switch, NAAM can run any
of these operations on client, server, and NIC cores, shifting load in tens of
milliseconds on server compute congestion. NAAM dynamically offloads up to 1.8
million MICA ops/s for YCSB-B and 750,000 Cell lookups/s from server CPUs.
Finally, whereas iPipe, the state-of-the-art SmartNIC offload framework, only
scales to 8 application offloads on BlueField-2, NAAM scales to hundreds of
application offloads with minimal impact on tail latency due to eBPF's low
overhead.

</details>


### [175] [Constraint-Compliant Network Optimization through Large Language Models](https://arxiv.org/abs/2509.07492)
*Youngjin Song,Wookjin Lee,Hong Ki Kim,Sang Hyun Lee*

Main category: cs.NI

TL;DR: 开发了一个基于LLM的优化框架，通过自然语言输入编码策略确保网络优化中的严格约束满足。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的优化方法在执行严格约束时常出现问题，导致生成不可行的解决方案。

Method: 提出一个基于LLM的优化框架，其核心是整合自然语言输入编码策略，以限制解决方案空间并保证可行性。

Result: 在多接入边缘计算网络的任务分配中，该框架成功优化了任务分配并最小化了最坏情况下的延迟。数值评估表明，LLM是约束感知网络优化的一个有前景的工具。

Conclusion: LLM在约束感知网络优化中具有巨大潜力，并为深入理解其推理能力提供了有价值的见解。

Abstract: This work develops an LLM-based optimization framework ensuring strict
constraint satisfaction in network optimization. While LLMs possess contextual
reasoning capabilities, existing approaches often fail to enforce constraints,
causing infeasible solutions. Unlike conventional methods that address average
constraints, the proposed framework integrates a natural language-based input
encoding strategy to restrict the solution space and guarantee feasibility. For
multi-access edge computing networks, task allocation is optimized while
minimizing worst-case latency. Numerical evaluations demonstrate LLMs as a
promising tool for constraint-aware network optimization, offering insights
into their inference capabilities.

</details>


### [176] [FlexSAN: A Flexible Regenerative Satellite Access Network Architecture](https://arxiv.org/abs/2509.07548)
*Weize Kong,Chaoqun You,Xuming Pei,YueGao*

Main category: cs.NI

TL;DR: 本文提出FlexSAN，一种自适应卫星接入网络架构，能根据实时用户需求动态配置再生载荷（星载gNB或gNB-DU），以平衡服务质量、用户接纳率和运营成本，优于静态配置。


<details>
  <summary>Details</summary>
Motivation: 现有再生卫星接入网络（SAN）的载荷配置是静态且不灵活的（要么全星载gNB，要么仅星载gNB-DU），导致资源浪费或用户体验差，因为星载gNB延迟低但成本高，gNB-DU成本低但QoS可能受限，两者存在权衡。因此需要一个动态灵活的解决方案。

Method: 本文提出了FlexSAN（弹性SAN），一种自适应卫星接入网络架构。FlexSAN能够根据实时用户需求动态配置最优再生载荷。其策略是：当所有用户需求均满足时，选择最低运营成本（OPEX）的载荷配置；否则，在保证已连接用户QoS的前提下，最大化接纳用户数量。为解决动态载荷选择的计算复杂性，设计了一种自适应贪婪启发式算法。

Result: 实验结果验证了FlexSAN的有效性，与静态SAN相比，FlexSAN的用户接纳率平均提高了36.1%，运营成本（OPEX）降低了15%。

Conclusion: FlexSAN通过动态配置再生载荷，有效优化了卫星接入网络，显著提升了用户接纳率并降低了运营成本，解决了传统静态配置的不足。

Abstract: The regenerative satellite access network (SAN) architecture deploys
next-generation NodeB (gNBs) on satellites to enable enhanced network
management capabilities. It supports two types of regenerative payload,
on-board gNB and on-board gNB-Distributed Unit (gNB-DU). Measurement results
based on our prototype implementation show that the on-board gNB offers lower
latency, while the on-board gNB-DU is more cost-effective, and there is often a
trade-off between Quality-of-Service (QoS) and operational expenditure (OPEX)
when choosing between the two payload types. However, current SAN
configurations are static and inflexible -- either deploying the full on-board
gNB or only the on-board gNB-DU. This rigidity can lead to resource waste or
poor user experiences. In this paper, we propose Flexible SAN (FlexSAN), an
adaptive satellite access network architecture that dynamically configures the
optimal regenerative payload based on real-time user demands. FlexSAN selects
the lowest OPEX payload configuration when all user demands are satisfied, and
otherwise maximizes the number of admitted users while ensuring QoS for
connected users. To address the computational complexity of dynamic payload
selection, we design an adaptive greedy heuristic algorithm. Extensive
experiments validate FlexSAN's effectiveness, showing a 36.1% average
improvement in user admission rates and a 15% OPEX reduction over static SANs.

</details>


### [177] [Quantum Computing for Large-scale Network Optimization: Opportunities and Challenges](https://arxiv.org/abs/2509.07773)
*Sebastian Macaluso,Giovanni Geraci,Elías F. Combarro,Sergi Abadal,Ioannis Arapakis,Sofia Vallecorsa,Eduard Alarcón*

Main category: cs.NI

TL;DR: 本文提出利用量子计算（包括量子退火和量子强化学习）解决6G及未来网络中复杂的优化问题，并讨论相关挑战。


<details>
  <summary>Details</summary>
Motivation: 6G及未来大规模网络的复杂性导致多目标优化问题在巨大搜索空间中难以处理，现有方法往往无法应对。

Method: 通过分析未来移动网络中关键问题的共同特征（特别是其图中心表示），提出一种统一的量子计算算法策略，具体包括量子退火和量子强化学习。

Result: 提出了一个利用量子计算（如量子退火和量子强化学习）来解决未来移动网络中关键优化问题的愿景和具体方法论。

Conclusion: 量子计算为未来网络优化提供了有前景的途径，但其算法和硬件仍面临诸多挑战需要克服。

Abstract: The complexity of large-scale 6G-and-beyond networks demands innovative
approaches for multi-objective optimization over vast search spaces, a task
often intractable. Quantum computing (QC) emerges as a promising technology for
efficient large-scale optimization. We present our vision of leveraging QC to
tackle key classes of problems in future mobile networks. By analyzing and
identifying common features, particularly their graph-centric representation,
we propose a unified strategy involving QC algorithms. Specifically, we outline
a methodology for optimization using quantum annealing as well as quantum
reinforcement learning. Additionally, we discuss the main challenges that QC
algorithms and hardware must overcome to effectively optimize future networks.

</details>


### [178] [Making congestion control robust to per-packet load balancing in datacenters](https://arxiv.org/abs/2509.07907)
*Barak Gerstein,Mark Silberstein,Isaac Keslassy*

Main category: cs.NI

TL;DR: 针对数据中心多路径负载均衡导致拥塞控制算法性能下降问题，本研究提出Mswif，通过中位数反馈机制改进Google Swift，将99%流完成时间（FCT）提升25%。


<details>
  <summary>Details</summary>
Motivation: 数据中心网络中，逐包负载均衡与现有拥塞控制算法（CCAs）结合会导致性能不佳，即使最先进的CCAs也可能因重复ACK而崩溃。仅仅解决重复ACK不足以解决问题，因为CCAs是为单路径路由设计的，其估计功能侧重于最新反馈并错误处理来自多路径的反馈。

Method: 首先，对当部分路径拥塞时多种拥塞控制算法的吞吐量崩溃现象进行建模。然后，提出使用“中位数反馈”机制，使其对多路径带来的不同信号更具鲁棒性。在此基础上，引入MSwift，将此原理应用于Google的Swift算法，使其在多路径路由下保持鲁棒性，同时保留其incast容忍度和单路径性能。

Result: MSwift在随机包喷射和自适应路由两种情况下，均将99%的流完成时间（FCT）提高了高达25%。

Conclusion: 通过采用中位数反馈机制，MSwift成功使Google Swift在多路径路由环境下保持高效，解决了现有拥塞控制算法的性能问题，显著提升了数据中心网络的流完成时间。

Abstract: Per-packet load-balancing approaches are increasingly deployed in datacenter
networks. However, their combination with existing congestion control
algorithms (CCAs) may lead to poor performance, and even state-of-the-art CCAs
can collapse due to duplicate ACKs. A typical approach to handle this collapse
is to make CCAs resilient to duplicate ACKs.
  In this paper, we first model the throughput collapse of a wide array of CCAs
when some of the paths are congested. We show that addressing duplicate ACKs is
insufficient. Instead, we explain that since CCAs are typically designed for
single-path routing, their estimation function focuses on the latest feedback
and mishandles feedback that reflects multiple paths. We propose to use a
median feedback that is more robust to the varying signals that come with
multiple paths. We introduce MSwift, which applies this principle to make
Google's Swift robust to multi-path routing while keeping its incast tolerance
and single-path performance. Finally, we demonstrate that MSwift improves the
99th-percentile FCT by up to 25\%, both with random packet spraying and
adaptive routing.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [179] [Association of Timing and Duration of Moderate-to-Vigorous Physical Activity with Cognitive Function and Brain Aging: A Population-Based Study Using the UK Biobank](https://arxiv.org/abs/2509.06969)
*Wasif Khan,Lin Gu,Noah Hammarlund,Lei Xing,Joshua K. Wong,Ruogu Fang*

Main category: q-bio.NC

TL;DR: 较高的中高强度身体活动（MVPA）与老年人更好的认知功能和更健康的脑结构相关联。


<details>
  <summary>Details</summary>
Motivation: MVPA的强度和时间与认知功能及特定脑区结构之间的关联尚不明确，需要进一步研究。

Method: 研究分析了45,892名60岁以上英国生物样本库参与者的数据，包括腕戴式加速度计测量的MVPA、认知测试结果和结构性脑部MRI。使用多变量线性模型评估MVPA与认知表现和区域脑容量的关联，并调整了人口统计学、社会经济和健康相关协变量。同时进行了MVPA时间及亚组效应的次级分析。

Result: 较高的MVPA与推理、记忆、执行功能和处理速度等认知领域的更好表现相关，尤其在达到WHO指南推荐量的参与者中关联更强。MVPA还与皮层下脑区（如尾状核、壳核、苍白球、丘脑）以及涉及情绪、工作记忆和知觉处理的区域灰质体积相关。次级分析显示，任何时间的MVPA，特别是在中午-下午和晚上，都与认知功能和脑容量相关，且存在剂量-反应关系。

Conclusion: 较高的MVPA与老年期保留的脑结构和增强的认知功能相关。增加MVPA的公共卫生策略可能支持健康的认知老化，并能带来显著的经济效益。

Abstract: Physical activity is a modifiable lifestyle factor with potential to support
cognitive resilience. However, the association of moderate-to-vigorous physical
activity (MVPA) intensity, and timing, with cognitive function and
region-specific brain structure remain poorly understood. We analyzed data from
45,892 UK Biobank participants aged 60 years and older with valid wrist-worn
accelerometer data, cognitive testing, and structural brain MRI. MVPA was
measured both continuously (mins per week) and categorically (thresholded using
>=150 min/week based on WHO guidelines). Associations with cognitive
performance and regional brain volumes were evaluated using multivariable
linear models adjusted for demographic, socioeconomic, and health-related
covariates. We conducted secondary analyses on MVPA timing and subgroup
effects. Higher MVPA was associated with better performance across cognitive
domains, including reasoning, memory, executive function, and processing speed.
These associations persisted in fully adjusted models and were higher among
participants meeting WHO guidelines. Greater MVPA was also associated with
subcortical brain regions (caudate, putamen, pallidum, thalamus), as well as
regional gray matter volumes involved in emotion, working memory, and
perceptual processing. Secondary analyses showed that MVPA at any time of day
was associated with cognitive functions and brain volume particularly in the
midday-afternoon and evening. Sensitivity analysis shows consistent findings
across subgroups, with evidence of dose-response relationships. Higher MVPA is
associated with preserved brain structure and enhanced cognitive function in
later life. Public health strategies to increase MVPA may support healthy
cognitive aging and generate substantial economic benefits, with global gains
projected to reach USD 760 billion annually by 2050.

</details>


### [180] [Impact of Neuron Models on Spiking Neural Networks performance. A Complexity Based Classification Approach](https://arxiv.org/abs/2509.06970)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: q-bio.NC

TL;DR: 本研究探讨了脉冲神经网络（SNNs）中神经元模型和学习规则对分类性能的影响，并引入了基于Lempel-Ziv复杂度的评估方法，以期在生物信号处理中找到鲁棒的配置。


<details>
  <summary>Details</summary>
Motivation: 探究神经元模型和学习规则的选择如何影响脉冲神经网络（SNNs）的分类性能，特别是在生物信号处理应用中的表现。

Method: 比较了包括LIF、metaneuron和LB在内的生物启发式神经元模型，以及STDP、tempotron和奖励调制等多种学习规则。创新性地引入了基于Lempel-Ziv复杂度（LZC）的评估机制，用于量化脉冲序列的结构规律性并评估分类结果。实验采用具有不同时间依赖性和随机性水平的合成数据集（马尔可夫和泊松过程）。

Result: 研究发现，分类准确性取决于神经元模型、网络规模和学习规则之间的相互作用。基于LZC的评估方法能够识别出对弱信号或噪声信号保持鲁棒性的配置。

Conclusion: 本工作系统分析了神经元模型选择与网络参数及学习策略的相互作用，并提出了一种新颖的、基于复杂度的评估方法，为SNN性能提供了一致的基准。

Abstract: This study explores how the selection of neuron models and learning rules
impacts the classification performance of Spiking Neural Networks (SNNs), with
a focus on applications in bio-signal processing. We compare biologically
inspired neuron models, including Leaky Integrate-and-Fire (LIF), metaneurons,
and probabilistic Levy-Baxter (LB) neurons, across multiple learning rules,
including spike-timing-dependent plasticity (STDP), tempotron, and
reward-modulated updates. A novel element of this work is the integration of a
complexity-based decision mechanism into the evaluation pipeline. Using
Lempel-Ziv Complexity (LZC), a measure related to entropy rate, we quantify the
structural regularity of spike trains and assess classification outcomes in a
consistent and interpretable manner across different SNN configurations. To
investigate neural dynamics and assess algorithm performance, we employed
synthetic datasets with varying temporal dependencies and stochasticity levels.
These included Markov and Poisson processes, well-established models to
simulate neuronal spike trains and capture the stochastic firing behavior of
biological neurons.Validation of synthetic Poisson and Markov-modeled data
reveals clear performance trends: classification accuracy depends on the
interaction between neuron model, network size, and learning rule, with the
LZC-based evaluation highlighting configurations that remain robust to weak or
noisy signals. This work delivers a systematic analysis of how neuron model
selection interacts with network parameters and learning strategies, supported
by a novel complexity-based evaluation approach that offers a consistent
benchmark for SNN performance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [181] [VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving](https://arxiv.org/abs/2509.04827)
*Jiahuan Yu,Aryan Taneja,Junfeng Lin,Minjia Zhang*

Main category: cs.DC

TL;DR: VoltanaLLM系统通过协同设计频率调节和请求路由，在确保服务水平目标(SLO)的前提下，显著降低了LLM推理的能耗。


<details>
  <summary>Details</summary>
Motivation: 现代LLM交互式应用的部署面临高昂的LLM推理能耗挑战，这已成为其可持续和成本效益部署的日益严峻的问题。

Method: VoltanaLLM从控制理论角度出发，在预填充/解码分离架构中协同设计GPU频率调节和请求路由。具体包括：一个反馈驱动的频率控制器，动态调整GPU在预填充和解码阶段的频率；以及一个状态空间路由器，探索跨频率调节实例的路由决策，以在延迟约束下最小化能耗。该系统在SGLang中实现。

Result: VoltanaLLM实现了高达36.3%的能耗节省，同时保持了近乎完美的服务水平目标(SLO)达成率。

Conclusion: VoltanaLLM为可持续和智能的LLM服务提供了解决方案，有效解决了LLM推理的高能耗问题，同时满足性能要求。

Abstract: Modern Large Language Model (LLM) serving systems increasingly support
interactive applications, like real-time chat assistants, code generation
tools, and agentic workflows. However, the soaring energy cost of LLM inference
presents a growing challenge for sustainable and cost-effective deployment.
This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM
serving, built from a control theory perspective. VoltanaLLM co-designs
frequency scaling and request routing in emerging prefill/decode disaggregated
architectures, leveraging their decoupled execution to enable fine-grained
phase-specific control. It consists of a feedback-driven frequency controller
that dynamically adapts GPU frequency for prefill and decode phases, and a
state-space router that explores routing decisions across frequency-scaled
instances to minimize energy under latency constraints. We implement VoltanaLLM
in SGLang and evaluate its performance over multiple state-of-the-art LLMs and
real-world datasets. The results demonstrate that VoltanaLLM achieves up to
36.3% energy savings while maintaining near-perfect SLO attainment rate, paving
the way for sustainable and intelligent LLM serving.

</details>


### [182] [Astra: A Multi-Agent System for GPU Kernel Performance Optimization](https://arxiv.org/abs/2509.07506)
*Anjiang Wei,Tianran Sun,Yogesh Seenichamy,Hang Song,Anne Ouyang,Azalia Mirhoseini,Ke Wang,Alex Aiken*

Main category: cs.DC

TL;DR: 本文介绍了Astra，首个基于大语言模型的多智能体GPU核函数优化系统。它从现有CUDA代码而非PyTorch模块入手，通过LLM智能体迭代生成、测试、分析和规划，成功对SGLang的核函数实现了平均1.32倍的加速。


<details>
  <summary>Details</summary>
Motivation: GPU核函数优化对加速大型语言模型（LLM）的训练和部署至关重要，但通常需要大量手动调整或编译器的高度工程化投入。虽然此前有研究利用LLM生成CUDA代码，但主要集中在将高级PyTorch模块转换为CUDA。目前缺乏一种高效、自动化地优化现有CUDA实现的方法。

Method: 本文提出了Astra，一个基于LLM的多智能体GPU核函数优化系统。与以往方法不同，Astra以SGLang（一个广泛部署的LLM服务框架）中提取的现有CUDA实现作为优化起点。系统内，专门的LLM智能体通过迭代的代码生成、测试、性能分析和规划，协作生产出既正确又高性能的核函数。

Result: 在SGLang的核函数上，Astra使用OpenAI o4-mini进行零样本提示，实现了平均1.32倍的加速。详细的案例研究表明，LLM能够自主应用循环变换、优化内存访问模式、利用CUDA内部函数以及快速数学操作，从而获得显著的性能提升。

Conclusion: 多智能体LLM系统为GPU核函数优化提供了一种有前景的新范式。

Abstract: GPU kernel optimization has long been a central challenge at the intersection
of high-performance computing and machine learning. Efficient kernels are
crucial for accelerating large language model (LLM) training and serving, yet
attaining high performance typically requires extensive manual tuning.
Compiler-based systems reduce some of this burden, but still demand substantial
manual design and engineering effort. Recently, researchers have explored using
LLMs for GPU kernel generation, though prior work has largely focused on
translating high-level PyTorch modules into CUDA code. In this work, we
introduce Astra, the first LLM-based multi-agent system for GPU kernel
optimization. Unlike previous approaches, Astra starts from existing CUDA
implementations extracted from SGLang, a widely deployed framework for serving
LLMs, rather than treating PyTorch modules as the specification. Within Astra,
specialized LLM agents collaborate through iterative code generation, testing,
profiling, and planning to produce kernels that are both correct and
high-performance. On kernels from SGLang, Astra achieves an average speedup of
1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study
further demonstrates that LLMs can autonomously apply loop transformations,
optimize memory access patterns, exploit CUDA intrinsics, and leverage fast
math operations to yield substantial performance gains. Our work highlights
multi-agent LLM systems as a promising new paradigm for GPU kernel
optimization.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [183] [Influence Maximization Considering Influence, Cost and Time](https://arxiv.org/abs/2509.07625)
*Mingyang Feng,Qi Zhao,Shan He,Yuhui Shi*

Main category: cs.SI

TL;DR: 本文提出并解决了一个新的多目标影响力最大化问题，该问题同时优化影响力、成本和时间，并开发了高效的进化变长搜索算法EVEA，在实际网络中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统影响力最大化研究忽视了影响力传播、成本效率和时间紧迫性之间的相互作用。在病毒式营销和信息传播等实际场景中，同时优化影响力、成本和时间至关重要，但现有文献对此关注不足。

Method: 提出了一种新的多目标影响力最大化问题，旨在同时优化影响力、成本和时间。为解决此问题，开发了一种进化变长搜索算法 (EVEA)，用于有效搜索最优节点组合。

Result: 提出的 EVEA 算法在四个真实世界网络上超越所有基线，超体积提高了高达 19.3%，收敛速度加快了 25% 到 40%，同时在影响力、成本和时间目标之间保持了多样化且平衡的帕累托前沿。

Conclusion: 本文通过提出新的多目标影响力最大化问题并开发高效的 EVEA 算法，成功弥补了现有研究的不足，为实际应用中的影响力、成本和时间联合优化提供了有效解决方案。

Abstract: Influence maximization has been studied for social network analysis, such as
viral marketing (advertising), rumor prevention, and opinion leader
identification. However, most studies neglect the interplay between influence
spread, cost efficiency, and temporal urgency. In practical scenarios such as
viral marketing and information campaigns, jointly optimizing Influence, Cost,
and Time is essential, yet remaining largely unaddressed in current literature.
To bridge the gap, this paper proposes a new multi-objective influence
maximization problem that simultaneously optimizes influence, cost, and time.
We show the intuitive and empirical evidence to prove the feasibility and
necessity of this multi-objective problem. We also develop an evolutionary
variable-length search algorithm that can effectively search for optimal node
combinations. The proposed EVEA algorithm outperforms all baselines, achieving
up to 19.3% higher hypervolume and 25 to 40% faster convergence across four
real-world networks, while maintaining a diverse and balanced Pareto front
among influence, cost, and time objectives.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [184] [Cross-device Zero-shot Label Transfer via Alignment of Time Series Foundation Model Embeddings](https://arxiv.org/abs/2509.06966)
*Neal G. Ravindra,Arijit Sehanobish*

Main category: eess.SP

TL;DR: 本文提出一种新颖的框架，通过时间序列基础模型（TSFMs）将临床医疗设备和消费级可穿戴设备的数据投影到共享嵌入空间，并利用对抗性对齐技术实现跨设备标签迁移，无需配对数据。


<details>
  <summary>Details</summary>
Motivation: 消费级可穿戴设备（如Apple Watch）缺乏高质量、医学验证的标签，而手动标注成本高昂且难以规模化，尽管临床肌动图数据存在此类标签。

Method: 该研究开发了一个新颖的框架，名为“TSFM嵌入的对抗性对齐”（Adversarial Alignment of TSFM Embeddings），旨在将标签从源域（如肌动图）迁移到目标域（如Apple Watch），且无需配对数据。方法是将原始时间序列信号通过时间序列基础模型（TSFMs）投影到共享的潜在嵌入空间，然后强制源域和目标域嵌入的分布在该空间中对齐。

Result: 通过强制源域和目标域嵌入的分布在共享空间中对齐，该方法成功促进了跨设备类型（从临床肌动图到Apple Watch等）的标签迁移。

Conclusion: 该论文提出了一种有效的框架，通过跨设备表示对齐，解决了消费级可穿戴设备数据标签稀缺的问题，使得高价值标签能够从临床数据迁移到消费级设备数据中。

Abstract: High-quality, medically validated labels exist for clinical actigraphy data
but not for ubiquitous consumer wearables like the Apple Watch. Manually
labeling wearables data is expensive and doesn't scale. This paper offers a
novel framework that transfers valuable labels from a source domain (e.g.,
actigraphy) to a target domain (e.g., Apple Watch) without requiring paired
data. Instead of working with raw time-series signals, we project both domains
into a shared latent embedding space using time-series foundation models
(TSFMs) and develop a new framework to align the cross-device representations.
Our method, Adversarial Alignment of TSFM Embeddings forces the distributions
of source and target embeddings to align within this space, facilitating label
transfer across device type.

</details>


### [185] [Cross-field SNR Analysis and Tensor Channel Estimation for Multi-UAV Near-field Communications](https://arxiv.org/abs/2509.06967)
*Tianyu Huo,Jian Xiong,Yiyan Wu,Songjie Yang,Bo Liu,Wenjun Zhang*

Main category: eess.SP

TL;DR: 针对分布式近场多无人机ELAA通信系统的信道估计问题，提出并评估了基于混合球面-平面波模型的信道估计算法，其中张量-OMP在性能与计算复杂性上取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 6G网络中，特大天线阵列（ELAA）是提升频谱效率的关键。多无人机系统能够形成分布式ELAA，但其常在近场区域工作并具有空间稀疏性，导致传统的远场平面波假设失效。因此，亟需研究适用于分布式近场多无人机通信系统的信道估计方法。

Method: 1. **理论分析**: 在分布式均匀平面阵列场景下，推导了平面波模型（PWM）、球面波模型（SWM）和混合球面-平面波模型（HSPWM，即交叉场模型）下的信噪比（SNR）闭式表达式，并分析表明HSPWM在建模精度和分析易处理性之间取得了良好平衡。2. **算法提出**: 基于HSPWM，提出了两种信道估计算法：球面域正交匹配追踪（SD-OMP），它将极坐标域推广到联合考虑仰角、方位角和距离；以及张量正交匹配追踪（Tensor-OMP），该算法在HSPWM下将信道自然地公式化为张量以进行估计。

Result: 1. 理论分析表明，混合球面-平面波模型（HSPWM）在建模精度和分析易处理性之间取得了良好平衡。2. 仿真结果显示，张量正交匹配追踪（Tensor-OMP）算法在归一化均方误差（NMSE）性能上与球面域正交匹配追踪（SD-OMP）算法相当。3. Tensor-OMP算法在实现相似性能的同时，展现出更低的计算复杂度和更好的可扩展性。

Conclusion: 本研究成功分析了分布式近场多无人机ELAA通信系统的信道模型，并提出了有效的信道估计算法。特别是，张量正交匹配追踪（Tensor-OMP）算法在保持与SD-OMP相当的估计性能的同时，显著降低了计算复杂性并提高了可扩展性，使其成为此类系统信道估计的有效且高效的解决方案。

Abstract: Extremely large antenna array (ELAA) is key to enhancing spectral efficiency
in 6G networks. Leveraging the distributed nature of multi-unmanned aerial
vehicle (UAV) systems enables the formation of distributed ELAA, which often
operate in the near-field region with spatial sparsity, rendering the
conventional far-field plane wave assumption invalid. This paper investigates
channel estimation for distributed near-field multi-UAV communication systems.
We first derive closed-form signal-to-noise ratio (SNR) expressions under the
plane wave model (PWM), spherical wave model (SWM), and a hybrid
spherical-plane wave model (HSPWM), also referred to as the cross-field model,
within a distributed uniform planar array (UPA) scenario. The analysis shows
that HSPWM achieves a good balance between modeling accuracy and analytical
tractability. Based on this, we propose two channel estimation algorithms: the
spherical-domain orthogonal matching pursuit (SD-OMP) and the tensor-OMP. The
SD-OMP generalizes the polar domain to jointly consider elevation, azimuth, and
range. Under the HSPWM, the channel is naturally formulated as a tensor,
enabling the use of tensor-OMP. Simulation results demonstrate that tensor-OMP
achieves normalized mean square error (NMSE) performance comparable to SD-OMP,
while offering reduced computational complexity and improved scalability.

</details>


### [186] [Deep Learning-based Techniques for Integrated Sensing and Communication Systems: State-of-the-Art, Challenges, and Opportunities](https://arxiv.org/abs/2509.06968)
*Murat Temiz,Yongwei Zhang,Yanwei Fu,Chi Zhang,Chenfeng Meng,Orhan Kaplan,Christos Masouros*

Main category: eess.SP

TL;DR: 本文综述了深度学习（DL）在集成感知与通信（ISAC）系统中的最新进展，强调了DL在解决ISAC挑战中的优势并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: ISAC作为6G及未来网络的核心使能技术，能为新兴应用（如车载网络和工业机器人）提供感知和通信能力，并能降低硬件复杂性、缓解频谱拥堵及提高能效。然而，传统优化方法在ISAC中引入了显著的计算复杂度。深度学习技术因能提供高效、近优且计算复杂度更低的解决方案，成为应对这些挑战的理想选择，特别适用于资源受限和低延迟的实时系统。

Method: 本文对应用于ISAC系统的最先进的深度学习技术进行了全面且分类的综述，涵盖了波形设计、信道估计、感知信号处理、数据解调和干扰抑制等多种任务。文章首先简要介绍了DL架构和ISAC基础。

Result: 研究发现，深度学习技术能够为广泛的复杂ISAC任务提供高效且近乎最优的解决方案，显著降低了传统方法带来的计算复杂度。综述总结了这些DL方法的关键优势，并指出了其面临的主要挑战。

Conclusion: 深度学习是解决ISAC系统复杂性和性能挑战的强大工具，在多个关键任务中表现出显著优势。尽管仍存在挑战，但通过进一步的研究，深度学习有望充分发挥其潜力，为ISAC系统的未来发展开辟新方向。

Abstract: This article comprehensively reviews recent developments and research on deep
learning-based (DL-based) techniques for integrated sensing and communication
(ISAC) systems. ISAC, which combines sensing and communication functionalities,
is regarded as a key enabler for 6G and beyond networks, as many emerging
applications, such as vehicular networks and industrial robotics, necessitate
both sensing and communication capabilities for effective operation. A unified
platform that provides both functions can reduce hardware complexity, alleviate
frequency spectrum congestion, and improve energy efficiency. However,
integrating these functionalities on the same hardware requires highly
optimized signal processing and system design, introducing significant
computational complexity when relying on conventional iterative or
optimization-based techniques. As an alternative to conventional techniques,
DL-based techniques offer efficient and near-optimal solutions with reduced
computational complexity. Hence, such techniques are well-suited for operating
under limited computational resources and low latency requirements in real-time
systems. DL-based techniques can swiftly and effectively yield near-optimal
solutions for a wide range of sophisticated ISAC-related tasks, including
waveform design, channel estimation, sensing signal processing, data
demodulation, and interference mitigation. Therefore, motivated by these
advantages, recent studies have proposed various DL-based approaches for ISAC
system design. After briefly introducing DL architectures and ISAC
fundamentals, this survey presents a comprehensive and categorized review of
state-of-the-art DL-based techniques for ISAC, highlights their key advantages
and major challenges, and outlines potential directions for future research.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [187] [Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data](https://arxiv.org/abs/2509.07202)
*Khushiyant*

Main category: cs.HC

TL;DR: 本文提出一种结合Gemma 2B LLM、分类器-LLM架构和RNN编码器的新方法，显著降低了EEG脑电图文本生成所需的数据和计算资源，并实现了10%的性能提升，使该技术更易于实现和高效。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）极大地提升了文本生成能力，但基于脑电图（EEG）的文本生成仍面临数据量大和计算能力需求高的挑战。

Method: 引入了一种新方法，该方法将Gemma 2B LLM与分类器-LLM架构结合，并融入循环神经网络（RNN）编码器。

Result: 该方法大幅降低了所需数据量和计算能力，性能接近最先进方法，并实现了10%的整体性能提升。

Conclusion: 该研究证明了EEG文本生成中有效迁移学习的可能性，即使在数据受限下仍保持强大功能，有望通过整合LLM与EEG解码来改善辅助技术，提升严重运动障碍者的独立性和沟通能力，推动脑机接口领域的应用与研究。

Abstract: Text generating capabilities have undergone a substantial transformation with
the introduction of large language models (LLMs). Electroencephalography
(EEG)-based text production is still difficult, though, because it requires a
lot of data and processing power. This paper introduces a new method that
combines the use of the Gemma 2B LLM with a classifier-LLM architecture to
incorporate a Recurrent Neural Network (RNN) encoder. Our approach drastically
lowers the amount of data and compute power needed while achieving performance
close to that of cutting-edge methods. Notably, compared to current
methodologies, our methodology delivers an overall performance improvement of
10%. The suggested architecture demonstrates the possibility of effective
transfer learning for EEG-based text production, remaining strong and
functional even in the face of data limits. This work highlights the potential
of integrating LLMs with EEG decoding to improve assistive technologies and
improve independence and communication for those with severe motor limitations.
Our method pushes the limits of present capabilities and opens new paths for
research and application in brain-computer interfaces by efficiently using the
strengths of pre-trained language models. This makes EEG-based text production
more accessible and efficient.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [188] [Beyond Sequential Reranking: Reranker-Guided Search Improves Reasoning Intensive Retrieval](https://arxiv.org/abs/2509.07163)
*Haike Xu,Tong Chen*

Main category: cs.IR

TL;DR: 本文提出Reranker-Guided-Search (RGS)方法，通过根据重排序器偏好直接检索文档，并利用近似最近邻图上的贪婪搜索策略，显著提升了在有限重排序器预算下的检索性能。


<details>
  <summary>Details</summary>
Motivation: 传统的“检索-重排序”流程面临两大挑战：受限于初始检索的top-k文档质量，以及基于大型语言模型的重排序器计算成本高昂，限制了可处理的文档数量。

Method: 引入Reranker-Guided-Search (RGS)，该方法不依赖传统序列式重排序，而是直接根据重排序器的偏好检索文档。它在由近似最近邻算法生成的邻近图上执行贪婪搜索，基于文档相似性策略性地优先选择有潜力的文档进行重排序。

Result: 实验结果表明，在多个基准测试中性能显著提升：BRIGHT提升3.5点，FollowIR提升2.9点，M-BEIR提升5.1点。所有改进均在重排序器预算限制为100个文档的条件下达成。

Conclusion: 分析表明，在嵌入和重排序模型固定的前提下，策略性地选择要重排序的文档可以在有限的重排序预算下显著提高检索准确性。

Abstract: The widely used retrieve-and-rerank pipeline faces two critical limitations:
they are constrained by the initial retrieval quality of the top-k documents,
and the growing computational demands of LLM-based rerankers restrict the
number of documents that can be effectively processed. We introduce
Reranker-Guided-Search (RGS), a novel approach that bypasses these limitations
by directly retrieving documents according to reranker preferences rather than
following the traditional sequential reranking method. Our method uses a greedy
search on proximity graphs generated by approximate nearest neighbor
algorithms, strategically prioritizing promising documents for reranking based
on document similarity. Experimental results demonstrate substantial
performance improvements across multiple benchmarks: 3.5 points on BRIGHT, 2.9
on FollowIR, and 5.1 on M-BEIR, all within a constrained reranker budget of 100
documents. Our analysis suggests that, given a fixed pair of embedding and
reranker models, strategically selecting documents to rerank can significantly
improve retrieval accuracy under limited reranker budget.

</details>


### [189] [Benchmarking Information Retrieval Models on Complex Retrieval Tasks](https://arxiv.org/abs/2509.07253)
*Julian Killingback,Hamed Zamani*

Main category: cs.IR

TL;DR: 现有检索模型难以处理复杂的自然语言查询，且缺乏全面的评估资源。本文构建了一个多样化、真实的复杂检索任务数据集，并发现SOTA模型在该任务上表现不佳，LLM查询增强对最强模型甚至有害。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本任务上表现出色，而检索模型在通用复杂查询处理方面仍有欠缺。尽管对处理复杂查询的需求日益增长，但缺乏全面、真实的评估资源来衡量检索模型在该能力上的表现。

Method: 构建了一套多样化且真实的复杂检索任务数据集；对一组代表性的SOTA检索模型进行基准测试；探究了基于LLM的查询扩展和重写对检索质量的影响。

Result: 即使是最好的模型在复杂检索任务上表现也很差，平均nDCG@10仅为0.346，R@100仅为0.587。虽然LLM增强可帮助较弱模型，但对最强模型而言，所有重写技术都导致性能下降。

Conclusion: 当前SOTA检索模型在处理复杂查询方面存在显著不足。虽然LLM增强有潜力，但其应用需要细致考量，特别是在增强已很强大的模型时。新的基准测试揭示了当前技术的局限性，并呼吁未来研究。

Abstract: Large language models (LLMs) are incredible and versatile tools for
text-based tasks that have enabled countless, previously unimaginable,
applications. Retrieval models, in contrast, have not yet seen such capable
general-purpose models emerge. To achieve this goal, retrieval models must be
able to perform complex retrieval tasks, where queries contain multiple parts,
constraints, or requirements in natural language. These tasks represent a
natural progression from the simple, single-aspect queries that are used in the
vast majority of existing, commonly used evaluation sets. Complex queries
naturally arise as people expect search systems to handle more specific and
often ambitious information requests, as is demonstrated by how people use
LLM-based information systems. Despite the growing desire for retrieval models
to expand their capabilities in complex retrieval tasks, there exist limited
resources to assess the ability of retrieval models on a comprehensive set of
diverse complex tasks. The few resources that do exist feature a limited scope
and often lack realistic settings making it hard to know the true capabilities
of retrieval models on complex real-world retrieval tasks. To address this
shortcoming and spur innovation in next-generation retrieval models, we
construct a diverse and realistic set of complex retrieval tasks and benchmark
a representative set of state-of-the-art retrieval models. Additionally, we
explore the impact of LLM-based query expansion and rewriting on retrieval
quality. Our results show that even the best models struggle to produce
high-quality retrieval results with the highest average nDCG@10 of only 0.346
and R@100 of only 0.587 across all tasks. Although LLM augmentation can help
weaker models, the strongest model has decreased performance across all metrics
with all rewriting techniques.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [190] [ArGen: Auto-Regulation of Generative AI via GRPO and Policy-as-Code](https://arxiv.org/abs/2509.07006)
*Kapil Madan*

Main category: cs.CY

TL;DR: ArGen框架通过结合自动化奖励评分、GRPO和类OPA治理层，使LLM与复杂的伦理、安全和法规政策对齐。在医疗AI助手的案例研究中，其领域合规性比基线提高了70.9%。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）对齐方法主要基于偏好，难以有效应对复杂的、可配置的、机器可读的伦理原则、操作安全协议和法规合规标准。需要一个框架来确保LLM遵守这些多方面且细致的治理要求。

Method: 本文提出ArGen（生成式AI系统自调节）框架，通过以下方法使LLM与复杂政策对齐：结合基于原则的自动化奖励评分、群体相对策略优化（GRPO）以及受Open Policy Agent（OPA）启发的治理层。通过开发一个遵循达摩伦理（如Ahimsa和Dharma）的医疗AI助手作为深入案例研究来验证该框架。

Result: ArGen框架能够将LLM与复杂的、细致的治理要求对齐。在医疗AI助手的案例研究中，ArGen在领域范围合规性方面比基线提高了70.9%，展示了其对文化特定价值体系的适应性。

Conclusion: ArGen方法为构建技术熟练、伦理稳健且可验证合规的“可治理AI”系统提供了途径，从而确保其在全球多样化环境中的安全部署。

Abstract: This paper introduces ArGen (Auto-Regulation of Generative AI systems), a
framework for aligning Large Language Models (LLMs) with complex sets of
configurable, machine-readable rules spanning ethical principles, operational
safety protocols, and regulatory compliance standards. Moving beyond just
preference-based alignment, ArGen is designed to ensure LLMs adhere to these
multifaceted policies through a novel synthesis of principle-based automated
reward scoring, Group Relative Policy Optimisation (GRPO), and an Open Policy
Agent (OPA) inspired governance layer. This approach provides the technical
foundation for achieving and demonstrating compliance with diverse and nuanced
governance requirements. To showcase the framework's capability to
operationalize a deeply nuanced and culturally-specific value system, we
present an in-depth case study: the development of a medical AI assistant
guided by principles from Dharmic ethics (such as Ahimsa and Dharma), as
derived from texts like the Bhagavad Gita. This challenging application
demonstrates ArGen's adaptability, achieving a 70.9% improvement in
domain-scope adherence over the baseline. Through our open-source repository,
we show that ArGen's methodology offers a path to 'Governable Al' systems that
are technically proficient, ethically robust, and verifiably compliant for safe
deployment in diverse global contexts.

</details>
