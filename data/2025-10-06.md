<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 70]
- [cs.CV](#cs.CV) [Total: 58]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.LG](#cs.LG) [Total: 67]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning](https://arxiv.org/abs/2510.02324)
*Wannan Yang,Xinchi Qiu,Lei Yu,Yuchen Zhang,Oliver Aobo Yang,Narine Kokhlikyan,Nicola Cancedda,Diego Garcia-Olano*

Main category: cs.CL

TL;DR: CASAL是一种轻量级、高效的训练算法，通过将激活引导的好处直接融入模型权重，使LLMs在不回答未知问题的情况下，减少30-40%的幻觉，并在计算和数据效率上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在幻觉问题，即自信地给出错误答案而非承认无知。尽管激活引导可减少幻觉，但现有方法需要在推理时进行实时监控和干预，效率低下。

Method: 本文提出对比激活引导摊销学习（CASAL）算法，将可解释性与摊销优化相结合。CASAL通过训练单个Transformer层的一个子模块，直接将激活引导的益处“烘焙”到模型的权重中，从而在训练后使模型能够回答已知问题并拒绝回答未知问题。

Result: CASAL在多个短格式问答基准上将幻觉减少了30%-40%。它比基于LoRA的强基线（如SFT和DPO）在计算效率上高30倍，数据效率上高20倍。CASAL还能有效泛化到域外（OOD）领域，并能减轻文本模型和视觉-语言模型中的幻觉。据作者所知，CASAL是首个被证明对密集模型和专家混合（MoE）模型都有效的引导式训练方法。

Conclusion: CASAL代表了将解释性启发方法应用于生产系统实际部署方面向前迈出的有希望的一步，为减少LLM幻觉提供了一种高效、实用且泛化性强的解决方案。

Abstract: Large Language Models (LLMs) exhibit impressive capabilities but often
hallucinate, confidently providing incorrect answers instead of admitting
ignorance. Prior work has shown that models encode linear representations of
their own knowledge and that activation steering can reduce hallucinations.
These approaches, however, require real-time monitoring and intervention during
inference. We introduce Contrastive Activation Steering for Amortized Learning
(CASAL), an efficient algorithm that connects interpretability with amortized
optimization. CASAL directly bakes the benefits of activation steering into
model's weights. Once trained, LLMs answer questions they know while abstaining
from answering those they do not. CASAL's light-weight design requires training
only a submodule of a single transformer layer and yet reduces hallucination by
30%-40% across multiple short-form QA benchmarks. CASAL is 30x more
compute-efficient and 20x more data-efficient than strong LoRA-based baselines
such as SFT and DPO, boosting its practical applicability in data scarce
domains. Importantly, CASAL also generalizes effectively to out-of-distribution
(OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in
both text-only and vision-language models. To our knowledge, CASAL is the first
steering-based training method that has been shown to be effective for both
dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step
forward for applying interpretability-inspired method for practical deployment
in production systems.

</details>


### [2] [Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval](https://arxiv.org/abs/2510.02326)
*Vivek Bhavsar,Joseph Ereifej,Aravanan Gurusami*

Main category: cs.CL

TL;DR: 本文提出RA-FSM，一个基于GPT的模块化研究助手，通过有限状态机控制循环和检索增强，解决了大型语言模型在文献合成中幻觉和误引用的问题，为高风险技术工作提供透明且有据可查的答案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在文献合成中存在幻觉和错误引用的问题，这限制了它们在需要高准确性的专家工作流中的应用。

Method: 引入RA-FSM（研究助手-有限状态机），一个基于GPT的模块化研究助手，其生成过程封装在“关联性 -> 置信度 -> 知识”的有限状态控制循环中。系统基于向量检索和确定性引用管道，控制器负责过滤查询、评估可回答性、分解问题并在必要时触发检索。它还通过分级摄取工作流从多种来源构建领域知识库，并存储到向量索引和关系数据库中。

Result: 在针对光子学领域的六个任务类别的盲审A/B评估中，领域专家普遍认为RA-FSM优于Notebook LM和默认GPT API，原因在于其边界条件处理更强，证据使用更具说服力。覆盖率和新颖性分析表明，RA-FSM的探索范围超越了Notebook LM，且具有可调节的延迟和成本开销。

Conclusion: RA-FSM的设计强调为高风险技术工作提供透明、有良好引用的答案，并具有推广到其他科学领域的潜力。

Abstract: Large language models accelerate literature synthesis but can hallucinate and
mis-cite, limiting their usefulness in expert workflows. We present RA-FSM
(Research Assistant - Finite State Machine), a modular GPT-based research
assistant that wraps generation in a finite-state control loop: Relevance ->
Confidence -> Knowledge. The system is grounded in vector retrieval and a
deterministic citation pipeline. The controller filters out-of-scope queries,
scores answerability, decomposes questions, and triggers retrieval only when
needed, and emits answers with confidence labels and in-corpus, de-duplicated
references. A ranked-tier ingestion workflow constructs a domain knowledge base
from journals, conferences, indices, preprints, and patents, writing both to a
dense vector index and to a relational store of normalized metrics. We
implement the system for photonics and evaluate it on six task categories:
analytical reasoning, numerical analysis, methodological critique, comparative
synthesis, factual extraction, and application design. In blinded A/B reviews,
domain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla
Default GPT API call single-pass baseline, citing stronger boundary-condition
handling and more defensible evidence use. Coverage and novelty analyses
indicate that RA-FSM explores beyond the NLM while incurring tunable latency
and cost overheads. The design emphasizes transparent, well-cited answers for
high-stakes technical work and is generalizable to other scientific domains.

</details>


### [3] [KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI](https://arxiv.org/abs/2510.02327)
*So Kuroki,Yotaro Kubo,Takuya Akiba,Yujin Tang*

Main category: cs.CL

TL;DR: 本文提出一种新型混合架构，结合实时语音到语音（S2S）模型的低延迟和大型语言模型（LLM）的深度知识，实现了高知识量且低延迟的语音交互。


<details>
  <summary>Details</summary>
Motivation: 实时S2S模型虽然响应迅速，但缺乏深层知识和语义理解。而级联系统（ASR+LLM+TTS）虽知识丰富，却存在高延迟，影响自然交互的流畅性。

Method: 该框架通过S2S transformer处理用户语音以实现即时响应，同时将查询并发地传递给后端LLM。LLM的文本响应被实时注入，指导S2S模型的语音生成，从而在不增加完整级联系统延迟的情况下，为S2S输出注入丰富的知识。

Result: 通过MT-Bench基准测试的语音合成变体进行评估，结果表明，该系统在响应正确性方面显著优于基线S2S模型，接近级联系统的水平，同时保持与基线模型相当的延迟。

Conclusion: 该混合系统成功弥合了实时S2S模型的响应速度与LLM的丰富知识之间的鸿沟，实现了既有深度知识又低延迟的语音交互，性能接近级联系统而延迟与S2S相当。

Abstract: Real-time speech-to-speech (S2S) models excel at generating natural,
low-latency conversational responses but often lack deep knowledge and semantic
understanding. Conversely, cascaded systems combining automatic speech
recognition, a text-based Large Language Model (LLM), and text-to-speech
synthesis offer superior knowledge representation at the cost of high latency,
which disrupts the flow of natural interaction. This paper introduces a novel
hybrid architecture that bridges the gap between these two paradigms. Our
framework processes user speech through an S2S transformer for immediate
responsiveness while concurrently relaying the query to a powerful back-end
LLM. The LLM's text-based response is then injected in real time to guide the
S2S model's speech generation, effectively infusing its output with rich
knowledge without the full latency penalty of a cascaded system. We evaluated
our method using a speech-synthesized variant of the MT-Bench benchmark that
consists of multi-turn question-answering sessions. The results demonstrate
that our system substantially outperforms a baseline S2S model in response
correctness, approaching that of a cascaded system, while maintaining a latency
on par with the baseline.

</details>


### [4] [AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering](https://arxiv.org/abs/2510.02328)
*Ziqing Wang,Chengsheng Mao,Xiaole Wen,Yuan Luo,Kaize Ding*

Main category: cs.CL

TL;DR: AMANDA是一个免训练的智能体框架，通过LLM智能体增强医疗知识，解决Med-MLLM在低资源设置下的推理瓶颈（忽视图像细节和缺乏专业知识），并在Med-VQA任务中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有Med-MLLMs在缺乏标注数据的低资源环境中部署时，由于医疗推理能力瓶颈而表现不佳。这些瓶颈包括：(i) 忽视医学图像细节的内在推理瓶颈；(ii) 未能融入专业医疗知识的外在推理瓶颈。

Method: 提出了AMANDA框架，一个免训练的智能体框架，通过LLM智能体实现医疗知识增强。具体地，内在医疗知识增强侧重于粗粒度到细粒度的问题分解以实现全面诊断；外在医疗知识增强则通过生物医学知识图谱检索来支撑推理过程。

Result: 在八个Med-VQA基准测试中进行了广泛实验，结果表明AMANDA在零样本和少样本Med-VQA设置下均取得了实质性改进。

Conclusion: AMANDA通过其创新的知识增强机制，有效克服了Med-MLLMs在低资源环境中的推理限制，显著提升了Med-VQA任务的性能。

Abstract: Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise
in medical visual question answering (Med-VQA). However, when deployed in
low-resource settings where abundant labeled data are unavailable, existing
Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks:
(i) the intrinsic reasoning bottleneck that ignores the details from the
medical image; (ii) the extrinsic reasoning bottleneck that fails to
incorporate specialized medical knowledge. To address those limitations, we
propose AMANDA, a training-free agentic framework that performs medical
knowledge augmentation via LLM agents. Specifically, our intrinsic medical
knowledge augmentation focuses on coarse-to-fine question decomposition for
comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds
the reasoning process via biomedical knowledge graph retrieval. Extensive
experiments across eight Med-VQA benchmarks demonstrate substantial
improvements in both zero-shot and few-shot Med-VQA settings. The code is
available at https://github.com/REAL-Lab-NU/AMANDA.

</details>


### [5] [SelfJudge: Faster Speculative Decoding via Self-Supervised Judge Verification](https://arxiv.org/abs/2510.02329)
*Kanghoon Yoon,Minsub Kim,Sungjae Lee,Joonhyung Lee,Sunghyeon Woo,Yeonjun In,Se Jung Kwon,Chanyoung Park,Dongsoo Lee*

Main category: cs.CL

TL;DR: SelfJudge通过目标模型的自监督训练，解决了现有判断解码方法对人工标注的依赖，从而在多样化NLP任务中实现了更优的LLM推理速度与准确性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有判断解码（judge decoding）方法虽能加速LLM推理，但其验证器训练依赖人工标注或可验证的真实答案，导致在多样化NLP任务中泛化能力受限。

Method: 提出SelfJudge，通过目标模型自监督训练判断验证器。该方法通过评估替换token后的响应是否保留了原始响应的语义来衡量语义一致性，从而实现跨不同NLP任务的自动化验证器训练。

Result: SelfJudge在推理速度与准确性权衡方面优于现有的判断解码基线方法。

Conclusion: SelfJudge提供了一个广泛适用的解决方案，可用于加速大型语言模型的推理。

Abstract: Speculative decoding accelerates LLM inference by verifying candidate tokens
from a draft model against a larger target model. Recent judge decoding boosts
this process by relaxing verification criteria by accepting draft tokens that
may exhibit minor discrepancies from target model output, but existing methods
are restricted by their reliance on human annotations or tasks with verifiable
ground truths, limiting generalizability across diverse NLP tasks. We propose
SelfJudge, which trains judge verifiers via self-supervision of the target
model. Our method measures semantic preservation by assessing whether
token-substituted responses preserve the meaning of original responses,
enabling automatic verifier training across diverse NLP tasks. Our experiments
show SelfJudge achieves superior inference-accuracy trade-offs than judge
decoding baselines, offering a broadly applicable solution for faster LLM
inference.

</details>


### [6] [EntropyLong: Effective Long-Context Training via Predictive Uncertainty](https://arxiv.org/abs/2510.02330)
*Junlong Jia,Ziyang Chen,Xing Wu,Chaochen Gao,Zijia Lin,Debing Zhang,Songlin Hu,Binghui Guo*

Main category: cs.CL

TL;DR: 本文提出EntropyLong，一种利用预测不确定性（熵）验证长程依赖的数据构建方法，有效解决了现有方法无法保证真实长程依赖的问题，显著提升了长上下文语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有长文本数据构建方法（如通用文本拼接或基于启发式的方法）难以保证生成的数据包含真实的、有效的长程依赖，这限制了长上下文语言模型捕获这些依赖的能力。

Method: 本文提出EntropyLong方法，其核心是利用预测不确定性（熵）来验证依赖的质量。具体步骤包括：识别文档中的高熵位置；从大型语料库中检索语义相关的上下文；通过评估这些上下文是否能降低预测熵来验证其效用（采用模型在环验证）。最终，将原始文档与这些经验证的上下文补充相结合，构建了包含真实长程依赖的128K长度训练序列数据集，使用了FineWebEdu和Cosmopedia。

Result: 使用EntropyLong方法构建的数据训练的模型，在RULER基准测试上（特别是需要远距离信息的任务）取得了显著改进。经过指令微调后，模型在LongBenchv2上表现出实质性增益，验证了其增强的长上下文理解能力。广泛的消融研究进一步证实了基于熵的验证对于长上下文训练的必要性和有效性。

Conclusion: EntropyLong通过提供一种新颖且经过验证的数据构建方法，有效解决了长上下文模型训练中长程依赖的质量问题，显著提升了模型在长上下文理解和任务上的表现。

Abstract: Training long-context language models to capture long-range dependencies
requires specialized data construction. Current approaches, such as generic
text concatenation or heuristic-based variants, frequently fail to guarantee
genuine long-range dependencies. We propose EntropyLong, a novel data
construction method that leverages predictive uncertainty to verify dependency
quality. Our approach identifies high-entropy positions in documents, retrieves
semantically relevant contexts from large corpora, and verifies their utility
by assessing whether they reduce prediction entropy. This model-in-the-loop
verification ensures each dependency represents measurable information gain
rather than spurious correlation. We construct training samples with long-range
dependencies by combining original documents with these verified contextual
supplements. Using FineWebEdu and Cosmopedia, we generate a dataset of
128K-length sequences with verified dependencies. Models trained on this data
demonstrate significant improvements on RULER benchmarks, particularly in tasks
requiring distant information. Following instruction fine-tuning, our models
also achieve substantial gains on LongBenchv2, demonstrating enhanced
long-context understanding. Extensive ablation studies further validate the
necessity and effectiveness of entropybased verification for long-context
training.

</details>


### [7] [Synthetic Dialogue Generation for Interactive Conversational Elicitation & Recommendation (ICER)](https://arxiv.org/abs/2510.02331)
*Moonkyung Ryu,Chih-Wei Hsu,Yinlam Chow,Mohammad Ghavamzadeh,Craig Boutilier*

Main category: cs.CL

TL;DR: 为解决对话推荐系统数据稀缺及现有用户模拟器生成数据缺乏行为一致性的问题，本文提出了一种结合行为模拟器和语言模型提示的方法，用于生成与用户潜在状态一致的自然对话，并据此创建了一个大型开放数据集，经评估该数据集展现出高度的一致性、事实性和自然度。


<details>
  <summary>Details</summary>
Motivation: 语言模型在对话推荐系统（CRSs）中潜力巨大，但公共CRS数据的匮乏使得微调语言模型面临挑战。将语言模型用作用户模拟器来生成数据，虽可用于训练基于语言模型的CRS，但通常缺乏行为一致性，生成的对话序列与真实用户行为不符。

Method: 开发了一种生成自然对话的方法，该方法结合使用行为模拟器和语言模型提示，以确保生成的对话与用户的潜在状态保持一致。通过此方法，生成了一个包含偏好启发和示例评论的大型开源CRS数据集来验证其有效性。

Result: 成功生成了一个大型、开源的CRS数据集。对部分生成的对话进行评估后，结果显示这些对话展现出显著的一致性、事实性和自然度。

Conclusion: 本研究提出的结合行为模拟器和语言模型提示的方法，成功解决了对话推荐系统数据稀缺及现有用户模拟器生成数据行为一致性不足的问题，能够生成高质量、行为一致且自然的对话数据，为对话推荐系统的发展提供了宝贵的资源。

Abstract: While language models (LMs) offer great potential for conversational
recommender systems (CRSs), the paucity of public CRS data makes fine-tuning
LMs for CRSs challenging. In response, LMs as user simulators qua data
generators can be used to train LM-based CRSs, but often lack behavioral
consistency, generating utterance sequences inconsistent with those of any real
user. To address this, we develop a methodology for generating natural
dialogues that are consistent with a user's underlying state using behavior
simulators together with LM-prompting. We illustrate our approach by generating
a large, open-source CRS data set with both preference elicitation and example
critiquing. Rater evaluation on some of these dialogues shows them to exhibit
considerable consistency, factuality and naturalness.

</details>


### [8] [A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography](https://arxiv.org/abs/2510.02332)
*Yapei Feng,Feng Jiang,Shanhao Wu,Hua Zhong*

Main category: cs.CL

TL;DR: 论文提出“look-ahead Sync”方法，通过高效处理分词歧义，在保持安全性的同时显著提高了神经语言隐写术的嵌入容量，超越了现有方法SyncPool，尤其在嵌入率方面有大幅提升。


<details>
  <summary>Details</summary>
Motivation: 神经语言隐写术中，现代分词器的分词歧义是一个关键挑战，可导致解码失败。现有方法SyncPool虽能解决此问题，但通过将歧义组的熵完全用于同步而非有效载荷嵌入，从而牺牲了嵌入容量。

Method: 提出的“look-ahead Sync”方法仅对真正不可区分的token序列进行最小化同步采样，并策略性地保留所有其他可区分路径以最大化嵌入容量。该方法提供了安全性的理论证明，并分析了其可实现嵌入容量与理论上限之间的差距。

Result: 在英语（Llama 3）和中文（Qwen 2.5）基准测试中，该方法持续接近理论容量上限，并显著优于SyncPool。嵌入率在英语中提高超过160%，在中文中提高超过25%，尤其在候选池较大时效果更佳。

Conclusion: 这项工作在实现实用、高容量、可证明安全的语言隐写术方面迈出了重要一步。

Abstract: Neural linguistic steganography aims to embed information
  into natural text while preserving statistical undetectability. A fundamental
challenge in this ffeld stems from tokenization ambiguity in modern tokenizers,
which can lead to catastrophic decoding failures. The recent method, SyncPool,
addresses this ambiguity
  by employing a coarse-grained synchronization mechanism over groups of
ambiguous candidates. However, SyncPool sacriffces embedding capacity, as it
utilizes the entire Shannon entropy of an ambiguous group solely for
synchronization rather than for payload embedding. We propose a method named
look-ahead Sync, which overcomes the capacity limitation of SyncPool while
retaining its provable security guarantees. Our approach performs minimal
synchronized sampling only on truly indistinguishable token sequences, while
strategically preserving all other discernible paths to maximize embedding
capacity. We provide theoretical proofs for the security of our method and
analyze the gap between its achievable embedding capacity and the theoretical
upper bound. Experiments on English (using Llama 3) and Chinese (using Qwen
2.5) benchmarks show that our method consistently approaches the theoretical
capacity upper bound and signiffcantly outperforms SyncPool. The improvement in
embedding rate exceeds 160% in English and 25% in Chinese, particularly in
settings with larger candidate pools. This work represents a signiffcant step
toward practical high-capacity provably secure linguistic steganography.

</details>


### [9] [Human Mobility Datasets Enriched With Contextual and Social Dimensions](https://arxiv.org/abs/2510.02333)
*Chiara Pugliese,Francesco Lettich,Guido Rocchietti,Chiara Renso,Fabio Pinelli*

Main category: cs.CL

TL;DR: 该资源论文介绍了两个公开可用的、语义丰富的、包含LLM生成社交媒体帖子的全球定位系统轨迹数据集及其构建流程，覆盖巴黎和纽约，并支持表格和RDF格式。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了提供公开可用的、语义丰富的人类轨迹数据集，以支持行为建模、出行预测、知识图谱构建和基于LLM的应用等研究任务，特别是结合真实世界移动数据、结构化语义丰富、LLM生成文本和语义网络兼容性。

Method: 该研究使用OpenStreetMap的公开GPS轨迹，构建了包含停止点、移动、兴趣点、推断交通模式和天气数据等上下文层的数据集。创新性地包含了由大型语言模型（LLM）生成的合成、真实的社交媒体帖子，以实现多模态和语义移动分析。数据集以表格和资源描述框架（RDF）格式提供，并提供了可重现的开源构建管道。

Result: 研究结果是构建并公开了两个针对巴黎和纽约这两个结构不同大城市的语义丰富人类轨迹数据集。这些数据集整合了上下文层、LLM生成的社交媒体帖子，并支持语义推理和FAIR数据实践。提供了一个允许数据集定制的开源可重现管道。

Conclusion: 该资源是首个将真实世界移动数据、结构化语义丰富、LLM生成文本和语义网络兼容性结合在一个可重用框架中的研究，极大地支持了多模态和语义移动分析及其他相关研究任务。

Abstract: In this resource paper, we present two publicly available datasets of
semantically enriched human trajectories, together with the pipeline to build
them. The trajectories are publicly available GPS traces retrieved from
OpenStreetMap. Each dataset includes contextual layers such as stops, moves,
points of interest (POIs), inferred transportation modes, and weather data. A
novel semantic feature is the inclusion of synthetic, realistic social media
posts generated by Large Language Models (LLMs), enabling multimodal and
semantic mobility analysis. The datasets are available in both tabular and
Resource Description Framework (RDF) formats, supporting semantic reasoning and
FAIR data practices. They cover two structurally distinct, large cities: Paris
and New York. Our open source reproducible pipeline allows for dataset
customization, while the datasets support research tasks such as behavior
modeling, mobility prediction, knowledge graph construction, and LLM-based
applications. To our knowledge, our resource is the first to combine real-world
movement, structured semantic enrichment, LLM-generated text, and semantic web
compatibility in a reusable framework.

</details>


### [10] [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334)
*Zhe Li,Wei Zhao,Yige Li,Jun Sun*

Main category: cs.CL

TL;DR: 针对大型语言模型（LLMs）的不良行为诊断难题，本文提出一种基于表示及其梯度的效率框架。该框架在激活空间运行，能够实现样本级和细粒度token级归因，精确识别影响模型行为的训练数据和短语，为理解和缓解LLM风险提供诊断工具。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）尽管能力强大，但其部署常因生成有害内容、事实不准确和存在社会偏见等不良行为而受阻。诊断这些失败的根本原因对AI安全构成严峻挑战。现有的归因方法，特别是基于参数梯度的方法，因信号嘈杂和计算复杂性而效果不佳。

Method: 本文引入了一种新颖高效的框架，通过分析模型的表示及其梯度来诊断LLM的各种不良行为。该框架直接在模型的激活空间中操作，提供语义上有意义的信号，将模型的输出与其训练数据关联起来。

Result: 研究结果表明，该方法不仅在样本级归因方面表现出色，还能实现细粒度的token级分析，精确识别对模型行为有因果影响的特定样本和短语。该方法已成功应用于跟踪有害内容、检测后门中毒和识别知识污染等任务。

Conclusion: 本工作提供了一个强大的诊断工具，有助于理解、审计并最终缓解与大型语言模型相关的风险。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their deployment is frequently undermined by undesirable behaviors such as
generating harmful content, factual inaccuracies, and societal biases.
Diagnosing the root causes of these failures poses a critical challenge for AI
safety. Existing attribution methods, particularly those based on parameter
gradients, often fall short due to prohibitive noisy signals and computational
complexity. In this work, we introduce a novel and efficient framework that
diagnoses a range of undesirable LLM behaviors by analyzing representation and
its gradients, which operates directly in the model's activation space to
provide a semantically meaningful signal linking outputs to their training
data. We systematically evaluate our method for tasks that include tracking
harmful content, detecting backdoor poisoning, and identifying knowledge
contamination. The results demonstrate that our approach not only excels at
sample-level attribution but also enables fine-grained token-level analysis,
precisely identifying the specific samples and phrases that causally influence
model behavior. This work provides a powerful diagnostic tool to understand,
audit, and ultimately mitigate the risks associated with LLMs. The code is
available at https://github.com/plumprc/RepT.

</details>


### [11] [FormalML: A Benchmark for Evaluating Formal Subgoal Completion in Machine Learning Theory](https://arxiv.org/abs/2510.02335)
*Xiao-Wen Yang,Zihao Zhang,Jianuo Cao,Zhi Zhou,Zenan Li,Lan-Zhe Guo,Yuan Yao,Taolue Chen,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.CL

TL;DR: 本文提出“子目标补全”任务，旨在让大型语言模型填补数学证明中的缺失步骤。为此，引入FormalML基准测试集，并评估发现现有最先进证明器在准确性和效率上仍存在显著局限。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在形式化定理证明中取得了显著进展，但它们作为数学家助手，填补复杂证明中缺失步骤的实际能力仍未得到充分探索。

Method: 将挑战定义为“子目标补全”任务。为研究此问题，构建了FormalML，一个基于Lean 4、源自机器学习基础理论的基准测试集。通过将过程式证明转换为声明式证明的策略，提取了4937个难度不一、涵盖优化和概率不等式的子目标补全问题。FormalML是首个结合前提检索和复杂研究级上下文的子目标补全基准。

Result: 对现有最先进的定理证明器进行评估后发现，它们在子目标补全任务的准确性和效率方面存在持续的局限性。

Conclusion: 需要开发更强大的基于LLM的定理证明器，以实现有效的子目标补全任务，从而更好地辅助数学家。

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in formal theorem proving. Yet their ability to serve as practical assistants
for mathematicians, filling in missing steps within complex proofs, remains
underexplored. We identify this challenge as the task of subgoal completion,
where an LLM must discharge short but nontrivial proof obligations left
unresolved in a human-provided sketch. To study this problem, we introduce
FormalML, a Lean 4 benchmark built from foundational theories of machine
learning. Using a translation tactic that converts procedural proofs into
declarative form, we extract 4937 problems spanning optimization and
probability inequalities, with varying levels of difficulty. FormalML is the
first subgoal completion benchmark to combine premise retrieval and complex
research-level contexts. Evaluation of state-of-the-art provers highlights
persistent limitations in accuracy and efficiency, underscoring the need for
more capable LLM-based theorem provers for effective subgoal completion,

</details>


### [12] [KurdSTS: The Kurdish Semantic Textual Similarity](https://arxiv.org/abs/2510.02336)
*Abdulhady Abas Abdullah,Hadi Veisi,Hussein M. Al*

Main category: cs.CL

TL;DR: 首次为库尔德语创建并发布了一个包含10,000对句子的语义文本相似性（STS）数据集，并使用多种模型进行了基准测试，揭示了该语言在STS任务中的挑战。


<details>
  <summary>Details</summary>
Motivation: 高资源语言拥有丰富的语义文本相似性（STS）资源，但库尔德语等低资源语言缺乏此类资源，导致其NLP任务发展受限。

Method: 构建了一个包含10,000对句子的库尔德语STS数据集，这些句子对涵盖正式和非正式语域，并进行了相似性标注。使用Sentence-BERT、多语言BERT及其他强基线模型对数据集进行了基准测试。

Result: 取得了有竞争力的结果，同时突出了库尔德语形态学、正字法变异和语码混合所带来的挑战。

Conclusion: 所创建的数据集和基线模型为库尔德语语义学和低资源NLP的未来研究提供了一个可复现的评估套件和有力的起点。

Abstract: Semantic Textual Similarity (STS) measures the degree of meaning overlap
between two texts and underpins many NLP tasks. While extensive resources exist
for high-resource languages, low-resource languages such as Kurdish remain
underserved. We present, to our knowledge, the first Kurdish STS dataset:
10,000 sentence pairs spanning formal and informal registers, each annotated
for similarity. We benchmark Sentence-BERT, multilingual BERT, and other strong
baselines, obtaining competitive results while highlighting challenges arising
from Kurdish morphology, orthographic variation, and code-mixing. The dataset
and baselines establish a reproducible evaluation suite and provide a strong
starting point for future research on Kurdish semantics and low-resource NLP.

</details>


### [13] [CRACQ: A Multi-Dimensional Approach To Automated Document Assessment](https://arxiv.org/abs/2510.02337)
*Ishak Soltani,Francisco Belo,Bernardo Tavares*

Main category: cs.CL

TL;DR: CRACQ是一个多维度评估框架，用于衡量机器生成文本的连贯性、严谨性、适当性、完整性和质量，其特质级判断比LLM更稳定和可解释。


<details>
  <summary>Details</summary>
Motivation: 现有自动论文评分（AES）局限于文章，难以评估多样化的机器生成文本；缺乏能够提供细致、可解释的特质级分析的自动化评估方法。

Method: 提出CRACQ框架，借鉴基于特质的AES思想并扩展，采用规范驱动和可解释的方法。通过整合语言、语义和结构信号进行累积评估，支持整体和特质层面分析。在500份合成资助提案上训练，并与大语言模型（LLM）作为评判者进行基准测试，同时在真实应用中验证。

Result: CRACQ在特质层面的判断比直接的大语言模型评估更稳定和可解释。

Conclusion: CRACQ为机器生成文本提供了更稳定和可解释的多维度评估能力，但框架在可靠性和领域范围方面仍存在挑战。

Abstract: This paper presents CRACQ, a multi-dimensional evaluation framework tailored
to evaluate documents across f i v e specific traits: Coherence, Rigor,
Appropriateness, Completeness, and Quality. Building on insights from
traitbased Automated Essay Scoring (AES), CRACQ expands its fo-cus beyond
essays to encompass diverse forms of machine-generated text, providing a
rubricdriven and interpretable methodology for automated evaluation. Unlike
singlescore approaches, CRACQ integrates linguistic, semantic, and structural
signals into a cumulative assessment, enabling both holistic and trait-level
analysis. Trained on 500 synthetic grant pro-posals, CRACQ was benchmarked
against an LLM-as-a-judge and further tested on both strong and weak real
applications. Preliminary results in-dicate that CRACQ produces more stable and
interpretable trait-level judgments than direct LLM evaluation, though
challenges in reliability and domain scope remain

</details>


### [14] [Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards](https://arxiv.org/abs/2510.02338)
*Samyak Jhaveri,Praphul Singh,Jangwon Kim,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.CL

TL;DR: 该研究提出了一个结合GRPO和DocLens的评估集成强化学习框架，用于生成长篇临床文本。它通过确定性、对话接地奖励直接优化事实准确性和完整性，无需额外奖励模型或人工参考，显著提升了临床笔记质量并降低了训练成本。


<details>
  <summary>Details</summary>
Motivation: 使用大型语言模型自动化临床文档生成时，需要精确地使其与完整性和事实依据等优先事项对齐。

Method: 开发了一个评估集成强化学习框架，将群组相对策略优化（GRPO）与DocLens（一个提供确定性、对话接地奖励的声明级评估器）相结合。该方法直接优化事实依据和完整性，无需训练单独的奖励模型或依赖人工参考，并通过简单的奖励门控策略降低了训练成本。

Result: 实验结果表明，该方法提高了临床笔记质量并降低了训练成本。独立的GPT-5定性评估显示，GRPO的输出在事实准确性、完整性和简洁性方面更受青睐，遗漏和幻觉更少。鉴于基准相对干净且基础模型已良好对齐，这些改进可能代表一个保守的下限。

Conclusion: 该框架可扩展到真实世界场景，并能整合自定义目标，如指南依从性或计费偏好，有效解决了临床文本自动化生成中事实准确性和完整性的挑战。

Abstract: Automating clinical documentation with large language models requires precise
alignment with priorities such as completeness and factual grounding. We
present an evaluation-integrated reinforcement learning framework for long-form
clinical text generation that couples Group Relative Policy Optimization (GRPO)
with DocLens, a claim-level evaluator that provides deterministic,
dialogue-grounded rewards. Our method directly optimizes factual grounding and
completeness without training a separate reward model or relying on
human-authored references. Empirically, the approach improves clinical note
quality and reduces training cost via a simple reward-gating strategy. An
independent GPT-5 qualitative evaluation further supports these gains, showing
higher preference for GRPO outputs in factuality, completeness, and brevity,
with fewer omissions and hallucinations. Because the benchmarks are relatively
clean and the base model already well aligned, these improvements likely
represent a conservative lower bound. The framework is scalable to real-world
settings and can incorporate custom objectives such as guideline adherence or
billing preferences.

</details>


### [15] [Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models](https://arxiv.org/abs/2510.02339)
*Kevin Zhou,Adam Dejl,Gabriel Freedman,Lihu Chen,Antonio Rago,Francesca Toni*

Main category: cs.CL

TL;DR: 本研究探索了将LLM不确定性量化(UQ)方法集成到基于计算论证的解释性大语言模型(ArgLLMs)中，用于决策制定。通过在事实核查任务上的实验，评估了不同UQ方法的有效性，发现直接提示是一种简单而高效的UQ策略，优于复杂方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型(LLMs)的普及，其可靠性保障日益重要，因此对LLM的不确定性量化(UQ)研究变得关键。本研究旨在将UQ方法集成到ArgLLMs这一解释性LLM框架中，以提升决策的可靠性，并评估UQ方法在处理复杂和有争议陈述时的有效性。

Method: 研究探索了LLM UQ方法在ArgLLMs中的集成。通过设计实验，评估了ArgLLMs在使用不同LLM UQ方法时在事实核查任务上的表现。实验过程本身被视为评估UQ方法有效性的一种新颖方式，尤其是在存在复杂和潜在争议性陈述的情况下。

Result: 实验结果表明，尽管直接提示方法简单，但它在ArgLLMs中作为一种UQ策略是有效的，并且其性能显著优于更复杂的UQ方法。

Conclusion: 本研究得出结论，在ArgLLMs中，直接提示作为一种不确定性量化策略具有出乎意料的有效性，甚至超越了更复杂的替代方案。这不仅为ArgLLMs的UQ应用提供了实用的见解，也为UQ方法的评估提供了一种新颖范式。

Abstract: Research in uncertainty quantification (UQ) for large language models (LLMs)
is increasingly important towards guaranteeing the reliability of this
groundbreaking technology. We explore the integration of LLM UQ methods in
argumentative LLMs (ArgLLMs), an explainable LLM framework for decision-making
based on computational argumentation in which UQ plays a critical role. We
conduct experiments to evaluate ArgLLMs' performance on claim verification
tasks when using different LLM UQ methods, inherently performing an assessment
of the UQ methods' effectiveness. Moreover, the experimental procedure itself
is a novel way of evaluating the effectiveness of UQ methods, especially when
intricate and potentially contentious statements are present. Our results
demonstrate that, despite its simplicity, direct prompting is an effective UQ
strategy in ArgLLMs, outperforming considerably more complex approaches.

</details>


### [16] [Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs](https://arxiv.org/abs/2510.02340)
*Xin Gao,Ruiyi Zhang,Daniel Du,Saurabh Mahindre,Sai Ashish Somayajula,Pengtao Xie*

Main category: cs.CL

TL;DR: 本文研究LLM通过提示模拟早期知识截止日期的能力，发现直接查询时有效，但对因果相关知识的遗忘效果不佳，提示在时间预测任务中需更严格评估。


<details>
  <summary>Details</summary>
Motivation: LLM在时间预测中可能因依赖预训练数据而表现出记忆而非推理，导致对其泛化能力的过高估计。鉴于基于提示的遗忘技术兴起，作者旨在探讨LLM是否能通过提示模拟更早的知识截止日期。

Method: 研究通过提示模拟LLM早期知识截止日期的能力。构建了三个评估数据集，分别测试LLM遗忘以下知识的程度：1) 直接事实知识，2) 语义变化，3) 因果相关知识。

Result: 基于提示的模拟知识截止日期在直接查询截止日期后的信息时显示出有效性。然而，当遗忘内容并非直接提问而是与查询因果相关时，LLM难以实现遗忘。

Conclusion: 研究结果表明，在将LLM应用于时间预测任务时，需要更严格的评估设置，尤其是在处理非直接关联的因果知识遗忘时。

Abstract: Large Language Models (LLMs) are widely used for temporal prediction, but
their reliance on pretraining data raises contamination concerns, as accurate
predictions on pre-cutoff test data may reflect memorization rather than
reasoning, leading to an overestimation of their generalization capability.
With the recent emergence of prompting-based unlearning techniques, a natural
question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff?
In this work, we investigate the capability of prompting to simulate earlier
knowledge cutoff in LLMs. We construct three evaluation datasets to assess the
extent to which LLMs can forget (1) direct factual knowledge, (2) semantic
shifts, and (3) causally related knowledge. Results demonstrate that while
prompt-based simulated knowledge cutoffs show effectiveness when directly
queried with the information after that date, they struggle to induce
forgetting when the forgotten content is not directly asked but causally
related to the query. These findings highlight the need for more rigorous
evaluation settings when applying LLMs for temporal prediction tasks. The full
dataset and evaluation code are available at
https://github.com/gxx27/time_unlearn.

</details>


### [17] [DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning](https://arxiv.org/abs/2510.02341)
*Yifan Wang,Bolian Li,Junlin Wu,Zhaoxuan Tan,Zheli Liu,Ruqi Zhang,Ananth Grama,Qingkai Zeng*

Main category: cs.CL

TL;DR: DRIFT是一种新的大语言模型后期训练方法，通过利用海量的隐式用户不满意信号而非稀缺的满意反馈，显著提升了模型性能，甚至超越了某些基线模型和GPT-4o-mini。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）在实际部署中产生大量隐式用户不满意（DSAT）信号，而显式满意（SAT）反馈稀缺。现有的偏好学习方法不适用于这种数据分布，因为它们依赖昂贵的人工标注或假设存在大量正面响应。

Method: 本文提出了DRIFT（Dissatisfaction-Refined Iterative Preference Training）方法。该方法以实际的DSAT信号为训练基础，并从不断演进的模型策略中动态采样正面样本。

Result: DRIFT模型在WildBench Task Score上最高提升6.23% (7B) / 7.61% (14B)，在AlpacaEval2胜率上最高提升8.95% (7B) / 12.29% (14B)，性能优于迭代DPO和SPIN等强基线方法。14B DRIFT模型在WildBench上超越了GPT-4o-mini。此外，DRIFT还保留了探索能力，能产生更多样化的高奖励解决方案，并理论上保持了偏好裕度，避免了梯度退化。

Conclusion: DRIFT是一种有效且可扩展的实际后期训练方法，它成功利用了最丰富和最有信息的隐式用户不满意信号来提升大语言模型性能。

Abstract: Real-world large language model deployments (e.g., conversational AI systems,
code generation assistants) naturally generate abundant implicit user
dissatisfaction (DSAT) signals, as users iterate toward better answers through
refinements, corrections, and expressed preferences, while explicit
satisfaction (SAT) feedback is scarce. Existing preference learning approaches
are poorly aligned with this data profile, as they rely on costly human
annotations or assume plentiful positive responses. In this paper, we introduce
\textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative
pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world
DSAT signals and samples positives dynamically from the evolving policy.
Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets
and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) /
+7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B)
on AlpacaEval2 win rate over base models, outperforming strong baseline methods
such as iterative DPO and SPIN. At larger scales, the improvements are
particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on
WildBench. Further analysis shows that DRIFT also preserves exploratory
capacity, yielding more diverse high-reward solutions rather than collapsing to
narrow subsets. Theoretically, we demonstrate that this design preserves
preference margins and avoids the gradient degeneration. These results show
that DRIFT is an effective and scalable recipe for real-world post-training
that leverages the most abundant and informative signal. The code and data are
available at https://github.com/cacayaya/DRIFT.git.

</details>


### [18] [$\texttt{BluePrint}$: A Social Media User Dataset for LLM Persona Evaluation and Training](https://arxiv.org/abs/2510.02343)
*Aurélien Bück-Kaeffer,Je Qin Chooi,Dan Zhao,Maximilian Puelma Touzel,Kellin Pelrine,Jean-François Godbout,Reihaneh Rabbany,Zachary Yang*

Main category: cs.CL

TL;DR: 本文介绍了SIMPACT框架，用于构建隐私保护、行为驱动的社交媒体数据集，并发布了BluePrint数据集，旨在训练和评估LLM作为逼真的社交媒体代理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在模拟社交媒体动态方面潜力巨大，但缺乏标准化数据资源来微调和评估LLM作为真实的社交媒体代理，而直接使用人类进行研究存在伦理或操作挑战。

Method: 引入了SIMPACT（Simulation-oriented Persona and Action Capture Toolkit），一个隐私保护框架，用于构建行为驱动的社交媒体数据集。将“下一步行动预测”作为训练和评估LLM代理的任务，并提出了在集群和人口层面评估行为保真度和风格真实性的指标。作为具体实现，发布了BluePrint数据集，一个基于公开Bluesky数据（聚焦政治话语）构建的大规模数据集，通过匿名化和假名化将用户聚类为聚合行为的“角色”，保护隐私并捕捉真实的互动模式。

Result: SIMPACT通过标准化数据和评估协议，为推进严谨、符合伦理的社交媒体模拟奠定了基础。BluePrint数据集既可作为政治话语建模的评估基准，也可作为构建特定领域数据集的模板，以研究错误信息和两极分化等挑战。数据集支持开发能够在语言和互动行为中利用上下文依赖性的代理。

Conclusion: SIMPACT和BluePrint为基于LLM的社交媒体模拟提供了急需的标准化、伦理数据和评估协议，促进了社交媒体模拟的严格性和可靠性发展。

Abstract: Large language models (LLMs) offer promising capabilities for simulating
social media dynamics at scale, enabling studies that would be ethically or
logistically challenging with human subjects. However, the field lacks
standardized data resources for fine-tuning and evaluating LLMs as realistic
social media agents. We address this gap by introducing SIMPACT, the
SIMulation-oriented Persona and Action Capture Toolkit, a privacy respecting
framework for constructing behaviorally-grounded social media datasets suitable
for training agent models. We formulate next-action prediction as a task for
training and evaluating LLM-based agents and introduce metrics at both the
cluster and population levels to assess behavioral fidelity and stylistic
realism. As a concrete implementation, we release BluePrint, a large-scale
dataset built from public Bluesky data focused on political discourse.
BluePrint clusters anonymized users into personas of aggregated behaviours,
capturing authentic engagement patterns while safeguarding privacy through
pseudonymization and removal of personally identifiable information. The
dataset includes a sizable action set of 12 social media interaction types
(likes, replies, reposts, etc.), each instance tied to the posting activity
preceding it. This supports the development of agents that use
context-dependence, not only in the language, but also in the interaction
behaviours of social media to model social media users. By standardizing data
and evaluation protocols, SIMPACT provides a foundation for advancing rigorous,
ethically responsible social media simulations. BluePrint serves as both an
evaluation benchmark for political discourse modeling and a template for
building domain specific datasets to study challenges such as misinformation
and polarization.

</details>


### [19] [Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression](https://arxiv.org/abs/2510.02345)
*Peijun Zhu,Ning Yang,Jiayu Wei,Jinghang Wu,Haijun Zhang*

Main category: cs.CL

TL;DR: 本文提出一个统一框架，通过动态专家聚类和结构化压缩解决MoE LLM在负载不均、参数冗余和通信开销方面的三难问题，显著提升效率并降低内存消耗。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE) 大语言模型面临负载不均衡、参数冗余和通信开销的三难困境。

Method: 该方法基于动态专家聚类和结构化压缩。具体包括：1) 使用参数和激活相似性的融合度量，通过在线聚类程序周期性地重组专家。2) 将专家权重分解为共享基矩阵和极低秩残差适配器，实现参数压缩。3) 采用两阶段分层路由策略，先将令牌分配给集群，再分配给集群内的专家。4) 引入异构精度方案（共享基FP16，残差INT4），并动态卸载非活跃集群以降低内存消耗。

Result: 该框架在匹配标准MoE模型质量的同时，总参数量减少约80%，吞吐量提高10%至20%，专家负载方差降低三倍以上，且峰值内存消耗降至与密集模型相当的水平。

Conclusion: 结构重组是实现可扩展、高效和内存有效的MoE LLM的原则性途径。

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load
imbalance, parameter redundancy, and communication overhead. We introduce a
unified framework based on dynamic expert clustering and structured compression
to address these issues cohesively. Our method employs an online clustering
procedure that periodically regroups experts using a fused metric of parameter
and activation similarity, which stabilizes expert utilization. To our
knowledge, this is one of the first frameworks to leverage the semantic
embedding capability of the router to dynamically reconfigure the model's
architecture during training for substantial efficiency gains. Within each
cluster, we decompose expert weights into a shared base matrix and extremely
low-rank residual adapters, achieving up to fivefold parameter reduction per
group while preserving specialization. This structure enables a two-stage
hierarchical routing strategy: tokens are first assigned to a cluster, then to
specific experts within it, drastically reducing the routing search space and
the volume of all-to-all communication. Furthermore, a heterogeneous precision
scheme, which stores shared bases in FP16 and residual factors in INT4, coupled
with dynamic offloading of inactive clusters, reduces peak memory consumption
to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our
framework matches the quality of standard MoE models while reducing total
parameters by approximately 80%, improving throughput by 10% to 20%, and
lowering expert load variance by a factor of over three. Our work demonstrates
that structural reorganization is a principled path toward scalable, efficient,
and memory-effective MoE LLMs.

</details>


### [20] [Small Language Models for Curriculum-based Guidance](https://arxiv.org/abs/2510.02347)
*Konstantinos Katharakis,Sippo Rossi,Raghava Rao Mukkamala*

Main category: cs.CL

TL;DR: 研究表明，结合RAG的小型语言模型（SLM）在教育领域可提供与大型语言模型（LLM）相当的准确指导，同时具有显著的可持续性、成本效益和隐私优势，是可行的AI助教方案。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和大型语言模型（LLM）在教育领域的应用尚处于新兴阶段，本研究旨在探索开发和评估可持续、高效且保护隐私的AI教学助手。

Method: 开发并评估了基于检索增强生成（RAG）管道的AI教学助手，该助手利用精选的开源小型语言模型（SLM）。研究对包括LLaMA 3.1、IBM Granite 3.3和Gemma 3在内的八款SLM（7-17B参数）进行了基准测试，并与GPT-4o进行了性能比较。

Result: 研究发现，通过适当的提示和有针对性的检索，SLM在提供准确且符合教学原则的响应方面可以与LLM相匹配。重要的是，SLM因较低的计算和能源需求而具有显著的可持续性优势，支持在消费级硬件上实时使用，无需依赖云基础设施，因此具有成本效益、隐私保护和环境友好的特点。

Conclusion: SLM是可行的AI教学助手，能帮助教育机构以可持续且节能的方式扩展个性化学习。

Abstract: The adoption of generative AI and large language models (LLMs) in education
is still emerging. In this study, we explore the development and evaluation of
AI teaching assistants that provide curriculum-based guidance using a
retrieval-augmented generation (RAG) pipeline applied to selected open-source
small language models (SLMs). We benchmarked eight SLMs, including LLaMA 3.1,
IBM Granite 3.3, and Gemma 3 (7-17B parameters), against GPT-4o. Our findings
show that with proper prompting and targeted retrieval, SLMs can match LLMs in
delivering accurate, pedagogically aligned responses. Importantly, SLMs offer
significant sustainability benefits due to their lower computational and energy
requirements, enabling real-time use on consumer-grade hardware without
depending on cloud infrastructure. This makes them not only cost-effective and
privacy-preserving but also environmentally responsible, positioning them as
viable AI teaching assistants for educational institutions aiming to scale
personalized learning in a sustainable and energy-efficient manner.

</details>


### [21] [mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations](https://arxiv.org/abs/2510.02348)
*Guy Dar*

Main category: cs.CL

TL;DR: 本文提出了mini-vec2vec，一种高效且鲁棒的vec2vec替代方案，用于对齐文本嵌入空间，它使用线性变换，显著降低了计算成本并提高了稳定性。


<details>
  <summary>Details</summary>
Motivation: 原始的vec2vec方法虽然能实现近乎完美的对齐，但其计算成本高昂且不稳定。

Method: mini-vec2vec包含三个主要阶段：伪并行嵌入向量的初步匹配、变换拟合和迭代优化。该方法学习到的是一个线性变换。

Result: mini-vec2vec在效率上比原始vec2vec提高了数个数量级，同时在性能上达到或超越了vec2vec。此外，该方法具有高度的鲁棒性和稳定性。

Conclusion: mini-vec2vec的稳定性、可解释性及其算法步骤，有利于方法的扩展，并在新领域和新方向上提供了应用机会。

Abstract: We build upon vec2vec, a procedure designed to align text embedding spaces
without parallel data. vec2vec finds a near-perfect alignment, but it is
expensive and unstable. We present mini-vec2vec, a simple and efficient
alternative that requires substantially lower computational cost and is highly
robust. Moreover, the learned mapping is a linear transformation. Our method
consists of three main stages: a tentative matching of pseudo-parallel
embedding vectors, transformation fitting, and iterative refinement. Our linear
alternative exceeds the original instantiation of vec2vec by orders of
magnitude in efficiency, while matching or exceeding their results. The
method's stability and interpretable algorithmic steps facilitate scaling and
unlock new opportunities for adoption in new domains and fields.

</details>


### [22] [LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL](https://arxiv.org/abs/2510.02350)
*Dzmitry Pihulski,Karol Charchut,Viktoria Novogrodskaia,Jan Kocoń*

Main category: cs.CL

TL;DR: 本文提出了LLMSQL，一个针对LLM时代对WikiSQL数据集进行系统性修订和再标注的版本，旨在解决原数据集的问题并提供一个更适合现代Text-to-SQL模型的基准测试。


<details>
  <summary>Details</summary>
Motivation: 将自然语言问题转换为SQL查询（Text-to-SQL）是实现非专业用户与关系数据库交互的核心任务。尽管WikiSQL在早期研究中发挥了重要作用，但其因结构和标注问题（如大小写不一致、数据类型不匹配、语法错误等）导致使用率下降。因此，需要一个经过清理且适应LLM的基准数据集。

Method: 研究者对WikiSQL进行了系统性修订和转换，创建了LLMSQL。这包括对错误进行分类，并实施自动化方法进行数据清洗和重新标注。为评估这些改进的影响，他们使用Gemma 3、LLaMA 3.2、Mistral 7B等多个大型语言模型（LLMs）进行了评估。LLMSQL以纯文本形式提供清晰的自然语言问题和完整的SQL查询。

Result: LLMSQL作为一个“LLM-ready”的基准被引入，它提供了干净的自然语言问题和完整的SQL查询作为纯文本，从而为现代自然语言到SQL模型的生成和评估提供了更直接的途径。与原始WikiSQL不同，LLMSQL不再是为指针网络模型量身定制的。

Conclusion: LLMSQL是对WikiSQL的系统性修订和转换，创建了一个适应LLM时代的、经过清理的Text-to-SQL基准数据集。它解决了原WikiSQL的结构和标注问题，并为现代NL-to-SQL模型提供了更直接、更有效的评估工具。

Abstract: Converting natural language questions into SQL queries (Text-to-SQL) enables
non-expert users to interact with relational databases and has long been a
central task for natural language interfaces to data. While the WikiSQL dataset
played a key role in early NL2SQL research, its usage has declined due to
structural and annotation issues, including case sensitivity inconsistencies,
data type mismatches, syntax errors, and unanswered questions. We present
LLMSQL, a systematic revision and transformation of WikiSQL designed for the
LLM era. We classify these errors and implement automated methods for cleaning
and re-annotation. To assess the impact of these improvements, we evaluated
multiple large language models (LLMs), including Gemma 3, LLaMA 3.2, Mistral
7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek R1 and
others. Rather than serving as an update, LLMSQL is introduced as an LLM-ready
benchmark: unlike the original WikiSQL, tailored for pointer-network models
selecting tokens from input, LLMSQL provides clean natural language questions
and full SQL queries as plain text, enabling straightforward generation and
evaluation for modern natural language-to-SQL models.

</details>


### [23] [Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs](https://arxiv.org/abs/2510.02351)
*Dzmitry Pihulski,Jan Kocoń*

Main category: cs.CL

TL;DR: 该研究探索大型语言模型（LLMs）在不同政治和文化视角下评估政治言论冒犯性的能力，发现具有推理能力的大模型在理解意识形态和文化差异上表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索当LLMs被要求采纳特定政治和文化视角时，它们如何评估政治话语中的冒犯性。

Method: 研究采用多语言MD-Agreement数据集的一个子集（包含2020年美国大选推文），评估了DeepSeek-R1、o4-mini、GPT-4.1-mini、Qwen3、Gemma和Mistral等LLMs。模型任务是根据不同政治人物（极右、保守、中立、进步）的观点，判断英语、波兰语和俄语推文是否具有冒犯性。

Result: 结果显示，具有明确推理能力的大型模型（如DeepSeek-R1, o4-mini）在处理意识形态和文化差异时表现出更高的一致性和敏感性，而小型模型往往难以捕捉这些细微的区别。推理能力显著提升了冒犯性判断的个性化和可解释性。

Conclusion: 推理能力是使LLMs能够适应跨语言和跨意识形态的细致社会政治文本分类的关键机制。

Abstract: We explore how large language models (LLMs) assess offensiveness in political
discourse when prompted to adopt specific political and cultural perspectives.
Using a multilingual subset of the MD-Agreement dataset centered on tweets from
the 2020 US elections, we evaluate several recent LLMs - including DeepSeek-R1,
o4-mini, GPT-4.1-mini, Qwen3, Gemma, and Mistral - tasked with judging tweets
as offensive or non-offensive from the viewpoints of varied political personas
(far-right, conservative, centrist, progressive) across English, Polish, and
Russian contexts. Our results show that larger models with explicit reasoning
abilities (e.g., DeepSeek-R1, o4-mini) are more consistent and sensitive to
ideological and cultural variation, while smaller models often fail to capture
subtle distinctions. We find that reasoning capabilities significantly improve
both the personalization and interpretability of offensiveness judgments,
suggesting that such mechanisms are key to adapting LLMs for nuanced
sociopolitical text classification across languages and ideologies.

</details>


### [24] [Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations](https://arxiv.org/abs/2510.02352)
*Yihao Wu,Tianrui Wang,Yizhou Peng,Yi-Wen Chao,Xuyi Zhuang,Xinsheng Wang,Shunshun Yin,Ziyang Ma*

Main category: cs.CL

TL;DR: 本文首次系统性评估了端到端语音对话模型（SDMs）中的偏见，探讨了多轮对话和言语特征的影响，并比较了开源与闭源模型的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）的偏见已被研究，但带有音频输入输出的语音对话模型（SDMs）中的偏见及其特征尚待探索。言语特征（如年龄、性别、口音）和多轮对话可能加剧偏见，影响决策和推荐任务的公平性。

Method: 研究系统性评估了语音LLMs中的偏见，并分析了带有重复负面反馈的多轮对话的影响。使用群体不公平分数（GUS）衡量决策偏见，使用基于相似度的标准化统计率（SNSR）衡量推荐偏见。评估模型包括开源的Qwen2.5-Omni和GLM-4-Voice，以及闭源API如GPT-4o Audio和Gemini-2.5-Flash。同时发布了FairDialogue数据集和评估代码。

Result: 分析显示，闭源模型普遍表现出较低的偏见；开源模型对年龄和性别更敏感；推荐任务倾向于放大跨群体差异；偏见的决策可能在多轮对话中持续存在。

Conclusion: 本研究首次系统性地评估了端到端语音对话模型中的偏见，为构建公平可靠的基于音频的交互系统提供了见解。为促进未来研究，论文发布了FairDialogue数据集和评估代码。

Abstract: While biases in large language models (LLMs), such as stereotypes and
cultural tendencies in outputs, have been examined and identified, their
presence and characteristics in spoken dialogue models (SDMs) with audio input
and output remain largely unexplored. Paralinguistic features, such as age,
gender, and accent, can affect model outputs; when compounded by multi-turn
conversations, these effects may exacerbate biases, with potential implications
for fairness in decision-making and recommendation tasks. In this paper, we
systematically evaluate biases in speech LLMs and study the impact of
multi-turn dialogues with repeated negative feedback. Bias is measured using
Group Unfairness Score (GUS) for decisions and similarity-based normalized
statistics rate (SNSR) for recommendations, across both open-source models like
Qwen2.5-Omni and GLM-4-Voice, as well as closed-source APIs such as GPT-4o
Audio and Gemini-2.5-Flash. Our analysis reveals that closed-source models
generally exhibit lower bias, while open-source models are more sensitive to
age and gender, and recommendation tasks tend to amplify cross-group
disparities. We found that biased decisions may persist in multi-turn
conversations. This work provides the first systematic study of biases in
end-to-end spoken dialogue models, offering insights towards fair and reliable
audio-based interactive systems. To facilitate further research, we release the
FairDialogue dataset and evaluation code.

</details>


### [25] [An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph](https://arxiv.org/abs/2510.02353)
*Oumar Kane,Mouhamad M. Allaya,Dame Samb,Mamadou Bousso*

Main category: cs.CL

TL;DR: 本研究利用AI和LLM改进塞内加尔法律文本的可及性，通过提取文章、构建图数据库和高级知识三元组提取，旨在帮助公民和法律专业人士理解权利义务。


<details>
  <summary>Details</summary>
Motivation: 塞内加尔司法系统法律文本提取与组织存在困难，需要提高司法信息的可及性，以帮助公民和法律专业人士更有效地理解其权利和责任。

Method: 应用人工智能（AI）和大型语言模型（LLM）；从法律文件中提取文章（如《土地和公共领域法典》）；开发详细的图数据库；利用GPT-4o、GPT-4和Mistral-Large等模型进行高级知识三元组提取，识别法律文本中的关系和元数据。

Result: 成功提取了7,967篇文章；开发了一个包含2,872个节点和10,774个关系的图数据库；展示了GPT-4o、GPT-4和Mistral-Large等模型在识别法律文本关系和相关元数据方面的有效性。

Conclusion: 通过这些技术，旨在创建一个坚实的框架，使塞内加尔公民和法律专业人士能够更有效地理解其权利和责任。

Abstract: This study examines the application of artificial intelligence (AI) and large
language models (LLM) to improve access to legal texts in Senegal's judicial
system. The emphasis is on the difficulties of extracting and organizing legal
documents, highlighting the need for better access to judicial information. The
research successfully extracted 7,967 articles from various legal documents,
particularly focusing on the Land and Public Domain Code. A detailed graph
database was developed, which contains 2,872 nodes and 10,774 relationships,
aiding in the visualization of interconnections within legal texts. In
addition, advanced triple extraction techniques were utilized for knowledge,
demonstrating the effectiveness of models such as GPT-4o, GPT-4, and
Mistral-Large in identifying relationships and relevant metadata. Through these
technologies, the aim is to create a solid framework that allows Senegalese
citizens and legal professionals to more effectively understand their rights
and responsibilities.

</details>


### [26] [Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness](https://arxiv.org/abs/2510.02354)
*Shreya Saha,Shurui Li,Greta Tuckute,Yuanning Li,Ru-Yuan Zhang,Leila Wehbe,Evelina Fedorenko,Meenakshi Khosla*

Main category: cs.CL

TL;DR: 研究发现人脑语言皮层中存在高度抽象、与形式无关的意义表征，其语义丰富度超越现有语言模型。


<details>
  <summary>Details</summary>
Motivation: 探讨人类语言系统对意义表征的抽象程度，并寻找语言皮层中意义的抽象表征。

Method: 通过视觉模型和语言模型表征来模拟句子在语言皮层中的神经响应。具体方法包括：聚合多张由句子生成的图像的视觉模型嵌入；平均句子多个释义的嵌入；以及用上下文细节丰富释义。

Result: 聚合多张生成图像的视觉模型嵌入能更准确预测语言皮层响应，有时媲美大型语言模型。平均多个释义的嵌入能提高预测准确性。用上下文细节（如隐式信息）丰富释义后，预测准确性进一步提高，甚至超越原始句子嵌入的预测，表明语言系统维持着比语言模型更丰富、更广泛的语义表征。

Conclusion: 这些结果共同证明了语言皮层中存在高度抽象、与形式无关的意义表征。

Abstract: The human language system represents both linguistic forms and meanings, but
the abstractness of the meaning representations remains debated. Here, we
searched for abstract representations of meaning in the language cortex by
modeling neural responses to sentences using representations from vision and
language models. When we generate images corresponding to sentences and extract
vision model embeddings, we find that aggregating across multiple generated
images yields increasingly accurate predictions of language cortex responses,
sometimes rivaling large language models. Similarly, averaging embeddings
across multiple paraphrases of a sentence improves prediction accuracy compared
to any single paraphrase. Enriching paraphrases with contextual details that
may be implicit (e.g., augmenting "I had a pancake" to include details like
"maple syrup") further increases prediction accuracy, even surpassing
predictions based on the embedding of the original sentence, suggesting that
the language system maintains richer and broader semantic representations than
language models. Together, these results demonstrate the existence of highly
abstract, form-independent meaning representations within the language cortex.

</details>


### [27] [DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding](https://arxiv.org/abs/2510.02358)
*Guanghao Li,Zhihui Fu,Min Fang,Qibin Zhao,Ming Tang,Chun Yuan,Jun Wang*

Main category: cs.CL

TL;DR: DiffuSpec提出了一种基于扩散语言模型（DLM）的推测解码框架，通过并行生成多令牌草稿来加速大型语言模型（LLMs），并引入了因果一致性路径搜索和自适应草稿长度控制器，实现了显著的实际速度提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的自回归（AR）解码导致高延迟。尽管推测解码通过快速草稿器提出多令牌草稿并由目标模型并行验证来解决此问题，但许多部署仍依赖AR草稿器，其顺序传递限制了实际时钟速度提升。

Method: 本文提出了DiffuSpec，一个免训练的即插即用框架。它使用预训练的扩散语言模型（DLM）在一次前向传递中生成多令牌草稿，并与标准AR验证器兼容。为解决DLM草稿的双向条件生成导致的非因果路径问题和草稿长度预设问题，DiffuSpec引入了两个组件：(i) 因果一致性路径搜索（CPS），用于从DLM生成的令牌格中提取与AR验证对齐的左-右因果路径；(ii) 自适应草稿长度（ADL）控制器，根据最近的接受反馈和实际生成长度调整下一个提案大小。

Result: DiffuSpec在各项基准测试中实现了高达3倍的实际时钟速度提升。

Conclusion: 基于扩散的草稿生成是推测解码中自回归草稿器的强大替代方案。

Abstract: As large language models (LLMs) scale up, accuracy improves, but the
autoregressive (AR) nature of decoding increases latency since each token
requires a serial forward pass. Speculative decoding addresses this by
employing a fast drafter to propose multi-token drafts, which are then verified
in parallel by the target model. However, many deployments still rely on AR
drafters, where sequential passes limit wall-clock gains. We revisit the
drafting stage and present DiffuSpec, a training-free drop-in framework that
uses a pretrained diffusion language model (DLM) to produce multi-token drafts
in a single forward pass, while remaining compatible with standard AR
verifiers. Because DLM drafts are generated under bidirectional conditioning,
parallel per-position candidates form a token lattice in which the locally
highest-probability token at each position need not form a causal left-to-right
path. Moreover, DLM drafting requires pre-specifying a draft length, inducing a
speed-quality trade-off. To address these challenges, we introduce two
practical components: (i) a causal-consistency path search (CPS) over this
lattice that extracts a left-to-right path aligned with AR verification; and
(ii) an adaptive draft-length (ADL) controller that adjusts next proposal size
based on recent acceptance feedback and realized generated length. Across
benchmarks, DiffuSpec yields up to 3x wall-clock speedup, establishing
diffusion-based drafting as a robust alternative to autoregressive drafters for
speculative decoding.

</details>


### [28] [Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis](https://arxiv.org/abs/2510.02359)
*Jiashu Ye,Tong Wu,Weiwen Chen,Hao Zhang,Zeteng Lin,Xingxing Li,Shujuan Weng,Manni Zhu,Xin Yuan,Xinlong Hong,Jingjie Li,Junyu Zheng,Zhijiong Huang,Jing Tang*

Main category: cs.CL

TL;DR: Emission-GPT是一个知识增强型大语言模型智能体，旨在解决大气排放领域知识碎片化和数据访问效率低下的问题，通过自然语言交互实现排放数据分析、查询和工作流程自动化，赋能非专业人士。


<details>
  <summary>Details</summary>
Motivation: 改善空气质量和应对气候变化依赖于对空气污染物和温室气体排放的准确理解。然而，排放相关知识往往碎片化且高度专业化，现有数据获取和整理方法效率低下，阻碍了非专业人士理解排放信息，对研究和管理构成挑战。

Method: 本文提出了Emission-GPT，一个为大气排放领域量身定制的知识增强型大语言模型智能体。该模型构建于包含一万多份文档（包括标准、报告、指南和同行评审文献）的精选知识库之上，并集成了提示工程和问题补全技术，以支持准确的领域特定问答。

Result: Emission-GPT使用户能通过自然语言交互分析排放数据，例如查询和可视化清单、分析源贡献以及为用户定义情景推荐排放因子。在广东省的案例研究表明，Emission-GPT能通过简单提示直接从原始数据中提取关键洞察，如点源分布和部门趋势。

Conclusion: Emission-GPT的模块化和可扩展架构有助于自动化传统手动工作流程，使其成为下一代排放清单开发和情景评估的基础工具。

Abstract: Improving air quality and addressing climate change relies on accurate
understanding and analysis of air pollutant and greenhouse gas emissions.
However, emission-related knowledge is often fragmented and highly specialized,
while existing methods for accessing and compiling emissions data remain
inefficient. These issues hinder the ability of non-experts to interpret
emissions information, posing challenges to research and management. To address
this, we present Emission-GPT, a knowledge-enhanced large language model agent
tailored for the atmospheric emissions domain. Built on a curated knowledge
base of over 10,000 documents (including standards, reports, guidebooks, and
peer-reviewed literature), Emission-GPT integrates prompt engineering and
question completion to support accurate domain-specific question answering.
Emission-GPT also enables users to interactively analyze emissions data via
natural language, such as querying and visualizing inventories, analyzing
source contributions, and recommending emission factors for user-defined
scenarios. A case study in Guangdong Province demonstrates that Emission-GPT
can extract key insights--such as point source distributions and sectoral
trends--directly from raw data with simple prompts. Its modular and extensible
architecture facilitates automation of traditionally manual workflows,
positioning Emission-GPT as a foundational tool for next-generation emission
inventory development and scenario-based assessment.

</details>


### [29] [Spiral of Silence in Large Language Model Agents](https://arxiv.org/abs/2510.02360)
*Mingze Zhong,Meng Fang,Zijing Shi,Yuxuan Huang,Shunfeng Zheng,Yali Du,Ling Chen,Jun Wang*

Main category: cs.CL

TL;DR: 研究了人类社会的“沉默螺旋”理论是否能在大语言模型（LLM）群体中出现，发现历史和角色信息结合能复现沉默螺旋模式，并强调需关注LLM系统中的从众现象。


<details>
  <summary>Details</summary>
Motivation: 经典的“沉默螺旋”理论是为人类社会开发的，其心理学解释不直接适用于LLM。因此，核心问题是：这种类似沉默螺旋的动态是否能纯粹通过LLM集合体的统计语言生成而出现？

Method: 提出了一个评估LLM代理中“沉默螺旋”的框架。具体设置了四种受控条件，系统地改变“历史”和“角色”信号的可用性。通过曼-肯德尔和斯皮尔曼等级相关等趋势检验，以及峰度和四分位距等集中度度量来评估意见动态。实验涵盖了开源和闭源模型。

Result: 历史和角色信号结合能产生强大的多数主导并复制沉默螺旋模式；仅有历史信号会导致强烈的锚定效应；而仅有角色信号会促成多样但不相关的意见，表明如果没有历史锚定，沉默螺旋动态无法出现。

Conclusion: 这项工作连接了计算社会学和负责任的AI设计，强调需要监控和缓解LLM代理系统中出现的从众现象。

Abstract: The Spiral of Silence (SoS) theory holds that individuals with minority views
often refrain from speaking out for fear of social isolation, enabling majority
positions to dominate public discourse. When the 'agents' are large language
models (LLMs), however, the classical psychological explanation is not directly
applicable, since SoS was developed for human societies. This raises a central
question: can SoS-like dynamics nevertheless emerge from purely statistical
language generation in LLM collectives? We propose an evaluation framework for
examining SoS in LLM agents. Specifically, we consider four controlled
conditions that systematically vary the availability of 'History' and 'Persona'
signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall
and Spearman's rank, along with concentration measures including kurtosis and
interquartile range. Experiments across open-source and closed-source models
show that history and persona together produce strong majority dominance and
replicate SoS patterns; history signals alone induce strong anchoring; and
persona signals alone foster diverse but uncorrelated opinions, indicating that
without historical anchoring, SoS dynamics cannot emerge. The work bridges
computational sociology and responsible AI design, highlighting the need to
monitor and mitigate emergent conformity in LLM-agent systems.

</details>


### [30] [ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference](https://arxiv.org/abs/2510.02361)
*Haojie Ouyang,Jianwei Lv,Lei Ren,Chen Wei,Xiaojie Wang,Fangxiang Feng*

Main category: cs.CL

TL;DR: ChunkLLM是一个轻量级、可插拔的Transformer训练框架，通过QK Adapter和Chunk Adapter解决自注意力机制的计算效率问题，实现长文本处理的显著加速并保持性能。


<details>
  <summary>Details</summary>
Motivation: Transformer大模型在NLP和CV领域表现出色，但自注意力机制的二次复杂度导致严重的计算效率低下。现有基于块选择和压缩的方法存在语义不完整或训练推理效率不佳的问题。

Method: 提出ChunkLLM框架，包含QK Adapter（用于特征压缩和块注意力获取）和Chunk Adapter（用于检测块边界）。训练阶段冻结主干模型参数，仅训练Adapter，并采用注意力蒸馏训练QK Adapter。推理阶段仅在检测到块边界时触发块选择，以加速模型推理。

Result: ChunkLLM在短文本基准上达到可比性能，在长上下文基准上保持98.64%的性能，键值缓存保留率为48.58%。在处理120K长文本时，相比Vanilla Transformer最高加速4.48倍。

Conclusion: ChunkLLM有效解决了Transformer长文本处理的效率瓶颈，通过创新的Adapter设计和训练机制，在保持高性能的同时实现了显著的推理加速。

Abstract: Transformer-based large models excel in natural language processing and
computer vision, but face severe computational inefficiencies due to the
self-attention's quadratic complexity with input tokens. Recently, researchers
have proposed a series of methods based on block selection and compression to
alleviate this problem, but they either have issues with semantic
incompleteness or poor training-inference efficiency. To comprehensively
address these challenges, we propose ChunkLLM, a lightweight and pluggable
training framework. Specifically, we introduce two components: QK Adapter
(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each
Transformer layer, serving dual purposes of feature compression and chunk
attention acquisition. The latter operates at the bottommost layer of the
model, functioning to detect chunk boundaries by leveraging contextual semantic
information. During the training phase, the parameters of the backbone remain
frozen, with only the QK Adapter and Chunk Adapter undergoing training.
Notably, we design an attention distillation method for training the QK
Adapter, which enhances the recall rate of key chunks. During the inference
phase, chunk selection is triggered exclusively when the current token is
detected as a chunk boundary, thereby accelerating model inference.
Experimental evaluations are conducted on a diverse set of long-text and
short-text benchmark datasets spanning multiple tasks. ChunkLLM not only
attains comparable performance on short-text benchmarks but also maintains
98.64% of the performance on long-context benchmarks while preserving a 48.58%
key-value cache retention rate. Particularly, ChunkLLM attains a maximum
speedup of 4.48x in comparison to the vanilla Transformer in the processing of
120K long texts.

</details>


### [31] [A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History](https://arxiv.org/abs/2510.02362)
*Matei-Iulian Cocu,Răzvan-Cosmin Cristia,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: 本案例研究评估大型语言模型（LLMs）在回答有争议的罗马尼亚历史问题时存在的偏见，发现模型在不同语言和格式下回答不稳定。


<details>
  <summary>Details</summary>
Motivation: 历史常因文化和国家理念而呈现不同视角，LLMs在训练数据上可能存在偏见，导致输出缺乏中立性，因此需要评估其偏见。

Method: 选取有争议的罗马尼亚历史问题，用多种LLMs在不同语言和语境下进行测试。研究分三阶段进行，以确认提问方式对回答的影响，例如先要求肯定回答，再要求按比例给出数值回答。

Result: 二进制回答的稳定性相对较高但不完美且因语言而异。模型常在不同语言或格式间改变立场；数字评分常与初始的二元选择不一致；最一致的模型不一定是最准确或中立的。

Conclusion: 研究揭示了LLMs在特定语言语境下对提问内容存在不一致性的倾向。

Abstract: In this case study, we select a set of controversial Romanian historical
questions and ask multiple Large Language Models to answer them across
languages and contexts, in order to assess their biases. Besides being a study
mainly performed for educational purposes, the motivation also lies in the
recognition that history is often presented through altered perspectives,
primarily influenced by the culture and ideals of a state, even through large
language models. Since they are often trained on certain data sets that may
present certain ambiguities, the lack of neutrality is subsequently instilled
in users. The research process was carried out in three stages, to confirm the
idea that the type of response expected can influence, to a certain extent, the
response itself; after providing an affirmative answer to some given question,
an LLM could shift its way of thinking after being asked the same question
again, but being told to respond with a numerical value of a scale. Results
show that binary response stability is relatively high but far from perfect and
varies by language. Models often flip stance across languages or between
formats; numeric ratings frequently diverge from the initial binary choice, and
the most consistent models are not always those judged most accurate or
neutral. Our research brings to light the predisposition of models to such
inconsistencies, within a specific contextualization of the language for the
question asked.

</details>


### [32] [Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents](https://arxiv.org/abs/2510.02369)
*Kuntai Cai,Juncheng Liu,Xianglin Yang,Zhaojie Niu,Xiaokui Xiao,Xing Chen*

Main category: cs.CL

TL;DR: 该研究提出并解决了LLM智能体在复杂任务中缺乏“实例级上下文”的问题。通过一种引导式探索方法高效获取和整合特定环境的可复用事实，显著提升了智能体的任务成功率和效率。


<details>
  <summary>Details</summary>
Motivation: LLM智能体通常依赖环境级手册和任务级指导，但忽略了对“实例级上下文”的获取——即与特定环境实例相关的可验证、可复用事实（如物体位置、合成配方、局部规则等）。这种上下文的缺失是LLM智能体在复杂任务中失败的常见原因，因为成功往往需要基于精确和持久的事实做出决策。在有限的交互预算下高效探索、验证和格式化这些事实是一个挑战。

Method: 研究将此问题形式化为“实例级上下文学习”（ILCL）。提出了一种任务无关的方法，通过引导式探索来解决：该方法利用“紧凑TODO森林”智能地规划优先级行动，并采用轻量级的“规划-行动-提取”循环来执行。此过程能自动生成高精度、可复用于多个下游任务和智能体的上下文文档，从而摊销初始探索成本。

Result: 在TextWorld、ALFWorld和Crafter等环境上的实验结果表明，智能体的成功率和效率均获得了持续提升。例如，ReAct在TextWorld中的平均成功率从37%提高到95%，IGE从81%提高到95%。

Conclusion: 该方法通过将一次性探索转化为持久、可复用的知识，有效补充了现有上下文类型，从而使LLM智能体在复杂任务中表现出更高的可靠性和效率。

Abstract: Large language model (LLM) agents typically receive two kinds of context: (i)
environment-level manuals that define interaction interfaces and global rules,
and (ii) task-level guidance or demonstrations tied to specific goals. In this
work, we identify a crucial but overlooked third type of context,
instance-level context, which consists of verifiable and reusable facts tied to
a specific environment instance, such as object locations, crafting recipes,
and local rules. We argue that the absence of instance-level context is a
common source of failure for LLM agents in complex tasks, as success often
depends not only on reasoning over global rules or task prompts but also on
making decisions based on precise and persistent facts. Acquiring such context
requires more than memorization: the challenge lies in efficiently exploring,
validating, and formatting these facts under tight interaction budgets. We
formalize this problem as Instance-Level Context Learning (ILCL) and introduce
our task-agnostic method to solve it. Our method performs a guided exploration,
using a compact TODO forest to intelligently prioritize its next actions and a
lightweight plan-act-extract loop to execute them. This process automatically
produces a high-precision context document that is reusable across many
downstream tasks and agents, thereby amortizing the initial exploration cost.
Experiments across TextWorld, ALFWorld, and Crafter demonstrate consistent
gains in both success and efficiency: for instance, ReAct's mean success rate
in TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By
transforming one-off exploration into persistent, reusable knowledge, our
method complements existing contexts to enable more reliable and efficient LLM
agents.

</details>


### [33] [Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models](https://arxiv.org/abs/2510.02370)
*Minsung Kim,Dong-Kyum Kim,Jea Kwon,Nakyeong Yang,Kyomin Jung,Meeyoung Cha*

Main category: cs.CL

TL;DR: 本研究系统性探讨了训练条件如何影响大型语言模型处理上下文知识与参数知识冲突的策略，发现文档内重复和数据不一致性有助于模型形成更稳健的仲裁能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理时常面临上下文知识和参数知识冲突。盲目接受外部知识易受误导，而固守参数知识则无法利用检索优势。目前缺乏对训练中知识仲裁策略形成的系统理解，导致预训练资源可能浪费。

Method: 通过首次受控研究，作者在合成传记语料库上训练Transformer语言模型，系统性控制多种训练条件，以探究其对模型使用和仲裁上下文与参数知识的影响。

Result: 实验发现，文档内事实重复有助于发展参数和上下文知识能力。此外，在包含不一致信息或分布偏差的语料库上训练，能促使模型发展出利用两种知识的稳健策略。这些“非理想”特性对于学习稳健仲裁至关重要。

Conclusion: 研究结果为预训练模型提供具体的经验指导，使其能和谐地整合参数知识和上下文知识。

Abstract: Large language models often encounter conflicts between in-context knowledge
retrieved at inference time and parametric knowledge acquired during
pretraining. Models that accept external knowledge uncritically are vulnerable
to misinformation, whereas models that adhere rigidly to parametric knowledge
fail to benefit from retrieval. Despite the widespread adoption of
retrieval-augmented generation, we still lack a systematic understanding of
what shapes knowledge-arbitration strategies during training. This gap risks
producing pretrained models with undesirable arbitration behaviors and,
consequently, wasting substantial computational resources after the pretraining
budget has already been spent. To address this problem, we present the first
controlled study of how training conditions influence models' use of in-context
and parametric knowledge, and how they arbitrate between them. We train
transformer-based language models on a synthetic biographies corpus while
systematically controlling various conditions. Our experiments reveal that
intra-document repetition of facts fosters the development of both parametric
and in-context capabilities. Moreover, training on a corpus that contains
inconsistent information or distributional skew encourages models to develop
robust strategies for leveraging parametric and in-context knowledge. Rather
than viewing these non-ideal properties as artifacts to remove, our results
indicate that they are important for learning robust arbitration. These
insights offer concrete, empirical guidance for pretraining models that
harmoniously integrate parametric and in-context knowledge.

</details>


### [34] [Pretraining with hierarchical memories: separating long-tail and common knowledge](https://arxiv.org/abs/2510.02375)
*Hadi Pouransari,David Grangier,C Thomas,Michael Kirchhof,Oncel Tuzel*

Main category: cs.CL

TL;DR: 通过引入记忆增强架构，小型语言模型能够访问大型分层参数化记忆库，从而在参数量远小于传统大型模型的情况下实现可媲美的性能，解决大型模型在资源受限设备上的问题。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型性能提升依赖于参数量扩展，导致模型庞大、存储冗余、推理资源需求高昂，不适用于内存和计算受限的边缘设备，且每次查询仅需部分世界知识。

Method: 本文提出一种记忆增强架构和预训练策略。小型语言模型通过访问大型分层参数化记忆库来编码世界知识。在预训练和推理时，模型会按需提取小的、上下文相关的记忆块。预训练目标是让记忆参数存储长尾世界知识，而小型语言模型则捕获通用知识和推理能力。

Result: 在万亿级token实验中，一个160M参数的模型，结合从4.6B参数记忆库中提取的18M参数记忆块，其性能可与参数量超过其两倍的常规模型（即超过320M参数）相当。研究还发现，所提出的分层前馈记忆在不同Transformer架构中均表现出良好的鲁棒性，无论是预训练期间还是事后添加。

Conclusion: 记忆增强架构能有效提升小型语言模型的性能，使其在资源受限环境下也能高效利用外部知识，解决了传统大型模型在部署上的限制。提出的分层前馈记忆机制在多种Transformer架构中展现出良好的通用性和鲁棒性。

Abstract: The impressive performance gains of modern language models currently rely on
scaling parameters: larger models store more world knowledge and reason better.
Yet compressing all world knowledge into parameters is unnecessary, as only a
fraction is used per prompt, and impractical for edge devices with limited
inference-time memory and compute. We address this shortcoming by a
memory-augmented architecture and a pretraining strategy aligned with existing
hardware paradigms. We introduce small language models that access large
hierarchical parametric memory banks encoding world knowledge. During
pretraining and inference, we fetch a small, context-dependent memory block and
add it to the model. Our pretraining learns to store long-tail world knowledge
in the memory parameters, while the small language model acts as an anchor
capturing common knowledge and general reasoning abilities. Through
trillion-token-scale experiments, we show significant gains: a 160M-parameters
model augmented with an 18M-parameters memory fetched from a 4.6B memory bank
obtains comparable performance to a regular model with more than 2x the
parameters. Through extensive experiments, we study the optimal type and size
of parametric memories in transformers, scaling them to over 21B parameters. We
find that our proposed hierarchical feed-forward memories work robustly across
transformer architectures, whether added during pretraining or post-hoc.

</details>


### [35] [Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems](https://arxiv.org/abs/2510.02377)
*Aakriti Agrawal,Rohith Aralikatti,Anirudh Satheesh,Souradip Chakraborty,Amrit Singh Bedi,Furong Huang*

Main category: cs.CL

TL;DR: 该论文提出了一种基于校准对数似然分数的新方法，能高效地从多个大语言模型（LLM）中选择最可靠的响应，并在多个基准数据集上取得了显著性能提升，解决了现有方法成本高或性能不足的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型能力卓越，但在资源受限环境下，如何从多个LLM响应中选择最可靠的仍然是一个挑战。现有方法依赖昂贵的外部验证器、人工评估或单一模型自洽性（需要多个样本），而多LLM系统虽具潜力，却常表现不如单一LLM自洽性。

Method: 本文提出了一种原则性、新颖且计算高效的方法，通过使用“校准对数似然分数”从多个不同的LLM中选择最佳响应，隐式地利用这些模型固有的知识和置信度。

Result: 该方法在GSM8K、MMLU（6个子集）和ARC数据集上，分别在辩论式（多轮LLM讨论）和非辩论式（多LLM的N选一）设置下，均取得了约4%、3%和5%的性能提升。

Conclusion: 该研究提供了一种高效且有效的新策略，能从多个大语言模型中挑选出更可靠的响应，显著提升了多LLM系统在不同任务和设置下的表现，优于现有方法。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities, yet
selecting the most reliable response from multiple LLMs remains a challenge,
particularly in resource-constrained settings. Existing approaches often depend
on costly external verifiers, human evaluators, or self-consistency techniques
that require multiple samples from a single model. While multi-LLM systems
produce more diverse responses than single models and thus have greater
potential, they often underperform compared to single LLM self-consistency. We
propose a principled, novel and computationally efficient method to select the
best response from multiple different LLMs using a calibrated log-likelihood
score, implicitly leveraging the inherent knowledge and confidence of these
models. Our method demonstrates improvements of approx. 4%, 3%, and 5% across
both debate (multi-round LLM discussions) and non-debate (Best-of-N with
multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets
respectively.

</details>


### [36] [Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2510.02388)
*Haoyue Bai,Haoyu Wang,Shengyu Chen,Zhengzhang Chen,Lu-An Tang,Wei Cheng,Haifeng Chen,Yanjie Fu*

Main category: cs.CL

TL;DR: 针对领域特定问答中LLM对关系型数据库的忽视，本文提出了一个规则驱动的路由框架，通过结合文档和数据库，实现了更高的准确性和适度的计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在通用问答中表现出色，但在需要精确和最新信息的领域特定场景中表现不佳。现有的检索增强生成（RAG）系统主要依赖非结构化文档，忽视了提供精确、及时且高效可查询事实信息的关系型数据库，而后者在金融、医疗等领域至关重要。研究发现：(i) 数据库和文档在查询中互补；(ii) 简单结合会引入噪音和成本；(iii) 为每个查询选择最合适的来源至关重要；(iv) 查询类型与检索路径存在一致规律，为路由决策提供指导。

Method: 基于上述洞察，提出一个规则驱动的路由框架。该框架包含：一个路由代理，根据显式规则评估并选择最合适的增强路径；一个规则制定专家代理，通过QA反馈随时间细化规则以保持适应性；以及一个路径级元缓存，重用语义相似查询的过去路由决策以减少延迟和成本。

Result: 在三个问答基准上的实验表明，该框架持续优于静态策略和基于学习的路由基线，在保持适度计算成本的同时实现了更高的准确性。

Conclusion: 所提出的规则驱动路由框架通过有效结合关系型数据库和非结构化文档，显著提升了LLM在领域特定问答中的准确性和效率，优于现有方法。

Abstract: Large Language Models (LLMs) have shown remarkable performance on general
Question Answering (QA), yet they often struggle in domain-specific scenarios
where accurate and up-to-date information is required. Retrieval-Augmented
Generation (RAG) addresses this limitation by enriching LLMs with external
knowledge, but existing systems primarily rely on unstructured documents, while
largely overlooking relational databases, which provide precise, timely, and
efficiently queryable factual information, serving as indispensable
infrastructure in domains such as finance, healthcare, and scientific research.
Motivated by this gap, we conduct a systematic analysis that reveals three
central observations: (i) databases and documents offer complementary strengths
across queries, (ii) naively combining both sources introduces noise and cost
without consistent accuracy gains, and (iii) selecting the most suitable source
for each query is crucial to balance effectiveness and efficiency. We further
observe that query types show consistent regularities in their alignment with
retrieval paths, suggesting that routing decisions can be effectively guided by
systematic rules that capture these patterns. Building on these insights, we
propose a rule-driven routing framework. A routing agent scores candidate
augmentation paths based on explicit rules and selects the most suitable one; a
rule-making expert agent refines the rules over time using QA feedback to
maintain adaptability; and a path-level meta-cache reuses past routing
decisions for semantically similar queries to reduce latency and cost.
Experiments on three QA benchmarks demonstrate that our framework consistently
outperforms static strategies and learned routing baselines, achieving higher
accuracy while maintaining moderate computational cost.

</details>


### [37] [KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning](https://arxiv.org/abs/2510.02392)
*Yinyi Luo,Zhexian Zhou,Hao Chen,Kai Qiu,Marios Savvides,Yixuan Li,Jindong Wang*

Main category: cs.CL

TL;DR: 本文提出了KnowledgeSmith，一个统一框架来系统理解大语言模型（LLMs）的知识更新机制，通过受控实验揭示了LLMs知识传播、可塑性、一致性和鲁棒性等方面的细致洞察。


<details>
  <summary>Details</summary>
Motivation: LLMs的知识编辑和机器遗忘是保持模型更新的常用方法，但由于评估不足、孤立且规模较小，LLMs的知识更新机制（如与人类的相似性、编辑与遗忘随训练数据增加的差异）仍未被充分探索。

Method: 提出KnowledgeSmith统一框架，将知识编辑和机器遗忘视为约束优化问题；设计了一个自动数据集生成器，提供多图层级和数据规模的结构化干预，以进行受控研究，探究不同修改策略如何通过模型知识传播。

Result: 广泛的实验揭示了细致入微的洞察，包括LLMs在不同知识层面的更新方式与人类不同，以及存在一致性-容量权衡，涵盖了知识传播、可塑性扩展、一致性和鲁棒性等方面。

Conclusion: 研究发现旨在为设计更可靠、可扩展的LLMs知识更新策略提供建议。

Abstract: Knowledge editing and machine unlearning are two popular approaches for large
language models (LLMs) to stay up-to-date. However, the knowledge updating
mechanism of LLMs remains largely unexplored due to insufficient, isolated, and
small-scale evaluation. For instance, are LLMs similar to humans in modifying
certain knowledge? What differs editing and unlearning as training data
increases? This paper proposes KnowledgeSmith, a unified framework to
systematically understand the updating mechanism of LLMs. We first cast editing
and unlearning as instances of one constrained optimization problem. Then, we
propose an automatic dataset generator that provides structured interventions
across multiple graph levels and data scales, enabling controlled studies of
how different modification strategies propagate through model knowledge.
Extensive experiments demonstrate nuanced insights over knowledge propagation,
plasticity scaling, consistency, and robustness. For instance, our results show
that LLMs do not exhibit similar updating as humans for different levels of
knowledge, and there exists consistency-capacity trade-off. We hope our
findings can offer suggestions to the design of more reliable and scalable
strategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git

</details>


### [38] [Retrieval and Augmentation of Domain Knowledge for Text-to-SQL Semantic Parsing](https://arxiv.org/abs/2510.02394)
*Manasi Patwardhan,Ayush Agarwal,Shabbirhussain Bhaisaheb,Aseem Arora,Lovekesh Vig,Sunita Sarawagi*

Main category: cs.CL

TL;DR: 本文提出一种系统框架，用于在数据库层面关联结构化领域知识，并通过子字符串匹配检索，显著提高了LLM在自然语言到SQL转换任务中的准确性，优于现有特设文本提示方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在将自然语言查询转换为SQL时，性能因数据库而异，主要原因在于缺乏对领域特定词汇和数据库结构关系的理解。现有基准依赖不切实际、特定查询的文本提示来表达领域知识，效率和准确性不足。

Method: 提出一个系统框架，用于在数据库层面关联结构化领域陈述。该框架通过基于子字符串匹配的方法，从用户查询中检索相关的结构化领域陈述。

Result: 1) 数据库层面的结构化领域陈述比现有特设的查询特定文本领域陈述更实用、更准确。2) 基于子字符串匹配的领域陈述检索方法比其他检索方法提供显著更高的准确性。

Conclusion: 数据库层面的结构化领域知识管理结合高效的子字符串匹配检索策略，能够有效解决LLM在自然语言到SQL转换任务中遇到的领域知识理解障碍，提升其性能和准确性。

Abstract: The performance of Large Language Models (LLMs) for translating Natural
Language (NL) queries into SQL varies significantly across databases (DBs). NL
queries are often expressed using a domain specific vocabulary, and mapping
these to the correct SQL requires an understanding of the embedded domain
expressions, their relationship to the DB schema structure. Existing benchmarks
rely on unrealistic, ad-hoc query specific textual hints for expressing domain
knowledge. In this paper, we propose a systematic framework for associating
structured domain statements at the database level. We present retrieval of
relevant structured domain statements given a user query using sub-string level
match. We evaluate on eleven realistic DB schemas covering diverse domains
across five open-source and proprietary LLMs and demonstrate that (1) DB level
structured domain statements are more practical and accurate than existing
ad-hoc query specific textual domain statements, and (2) Our sub-string match
based retrieval of relevant domain statements provides significantly higher
accuracy than other retrieval approaches.

</details>


### [39] [Words That Make Language Models Perceive](https://arxiv.org/abs/2510.02425)
*Sophie L. Wang,Phillip Isola,Brian Cheung*

Main category: cs.CL

TL;DR: 通过感官提示，纯文本训练的大型语言模型能够激活与其文本中隐含的多模态信息相关的感知表征。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型缺乏直接的感知经验，但其内部表征隐式地包含了语言中编码的多模态规律。研究旨在测试感官提示是否能显化这些潜在结构，使文本LLM的表征更接近专业的视觉和音频编码器。

Method: 使用明确的感官提示（如“看”或“听”）来引导模型，使其预测下一个词元时，如同其受到从未实际提供的潜在视觉或听觉证据的条件制约。

Result: 研究发现，轻量级的提示工程能够可靠地激活纯文本训练LLM中与模态相符的表征。

Conclusion: 简单的提示工程可以有效地引导纯文本训练的大型语言模型，使其表现出与多模态理解相关的表征对齐能力。

Abstract: Large language models (LLMs) trained purely on text ostensibly lack any
direct perceptual experience, yet their internal representations are implicitly
shaped by multimodal regularities encoded in language. We test the hypothesis
that explicit sensory prompting can surface this latent structure, bringing a
text-only LLM into closer representational alignment with specialist vision and
audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it
cues the model to resolve its next-token predictions as if they were
conditioned on latent visual or auditory evidence that is never actually
supplied. Our findings reveal that lightweight prompt engineering can reliably
activate modality-appropriate representations in purely text-trained LLMs.

</details>


### [40] [CLARITY: Clinical Assistant for Routing, Inference, and Triage](https://arxiv.org/abs/2510.02463)
*Vladimir Shaposhnikov,Aleksandr Nesterov,Ilia Kopanichuk,Ivan Bakulin,Egor Zhelvakov,Ruslan Abramov,Ekaterina Tsapieva,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.CL

TL;DR: CLARITY是一个AI驱动的平台，结合FSM和LLM实现患者分流、会诊和严重性评估，已整合至全国性医疗IT平台，显示出超越人类水平的路由准确性和更高的效率。


<details>
  <summary>Details</summary>
Motivation: 旨在利用AI技术改进患者到专家的分流、临床咨询和患者病情严重性评估。

Method: 采用混合架构，结合有限状态机（FSM）用于结构化对话流程和使用大型语言模型（LLM）的协作代理进行症状分析和转诊优先排序。基于模块化微服务框架构建，并已整合到全国性的医院间IT平台中。

Result: 部署两个月内完成了超过55,000次用户对话，其中2,500次经专家标注用于验证。验证结果显示CLARITY在首次尝试路由精度方面超越了人类水平，且咨询所需时间比人工缩短了多达3倍。

Conclusion: CLARITY是一个安全、高效、强大的AI平台，能够有效促进医疗分流和咨询，在实际应用中展现出优于人类的性能和效率，并具备良好的可扩展性。

Abstract: We present CLARITY (Clinical Assistant for Routing, Inference, and Triage),
an AI-driven platform designed to facilitate patient-to-specialist routing,
clinical consultations, and severity assessment of patients' conditions. Its
hybrid architecture combines a Finite State Machine (FSM) for structured
dialogue flows with collaborative agents that employ Large Language Model (LLM)
to analyze symptoms and prioritize referrals to appropriate specialists. Built
on a modular microservices framework, CLARITY ensures safe, efficient, and
robust performance, flexible and readily scalable to meet the demands of
existing workflows and IT solutions in healthcare.
  We report integration of our clinical assistant into a large-scale
nation-wide inter-hospital IT platform, with over 55,000 content-rich user
dialogues completed within the two months of deployment, 2,500 of which were
expert-annotated for a consequent validation. The validation results show that
CLARITY surpasses human-level performance in terms of the first-attempt routing
precision, naturally requiring up to 3 times shorter duration of the
consultation than with a human.

</details>


### [41] [Unraveling Syntax: How Language Models Learn Context-Free Grammars](https://arxiv.org/abs/2510.02524)
*Laura Ying Schulz,Daniel Mitropolsky,Tomaso Poggio*

Main category: cs.CL

TL;DR: 本研究提出一个基于概率上下文无关文法（PCFG）的新框架，用于理解语言模型如何学习句法。发现Transformer并行学习子文法，而非顺序学习；子文法预训练对小型模型有益；但模型在深层递归结构上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型表现出色，但对其学习动态，特别是如何获取句法，知之甚少。

Method: 引入一个框架，使用由概率上下文无关文法（PCFG）生成的合成语言作为测试平台，以精确控制语法复杂度、递归深度和子文法结构。研究小型Transformer模型的学习动态，并推导了关于PCFG子文法结构训练损失和KL散度的递归公式。同时探讨了子文法预训练的效果。

Result: 1. Transformer模型在所有子文法上并行减少损失，这与儿童的学习方式不同。2. 子文法预训练可以改善小型模型的最终损失，并使预训练模型发展出与语法子结构更一致的内部表示。3. 模型（包括大型语言模型）在处理更深层次的递归结构时遇到困难，揭示了神经网络表示层次句法的基本挑战。

Conclusion: 本工作开创了利用PCFG作为通用测试平台来研究Transformer学习动态的方向，为探究语言模型的学习机制和挑战，特别是其在处理深层递归结构时的局限性，提供了新的视角和研究方向。

Abstract: We introduce a new framework for understanding how language models acquire
syntax. While large models achieve impressive results, little is known about
their learning dynamics. Our approach starts with the observation that most
domains of interest, such as natural language syntax, coding languages,
arithmetic problems, are captured by probabilistic context-free grammars
(PCFGs). We study the learning dynamics of small models trained on synthetic
languages generated from PCFGs, enabling precise control over grammar
complexity, recursion depth, and subgrammar structure. We prove several
general, recursive formulae for the training loss and Kullback-Leibler
divergence over the subgrammar structure of a PCFG. Empirically, we find that
unlike children, who first master simple substructures before progressing to
more complex constructions, transformers reduce loss across all subgrammars in
parallel. We further show that subgrammar pretraining can improve the final
loss for smaller models, and that pretrained models develop internal
representations more aligned with the grammar's substructure. Finally, we
demonstrate that models struggle with deeper recursive structures (a limitation
even of large language models), revealing fundamental challenges in how neural
networks represent hierarchical syntax. Overall, our work initiates the study
of the learning dynamics of transformers on PCFGs as a versatile testbed for
probing learning in language models, opening a research direction with many
open questions.

</details>


### [42] [Hierarchical Semantic Retrieval with Cobweb](https://arxiv.org/abs/2510.02539)
*Anant Gupta,Karthik Singaravadivelan,Zekun Wang*

Main category: cs.CL

TL;DR: 本文提出使用Cobweb框架将句子嵌入组织成原型树，实现多粒度、可解释的文档检索，在匹配传统点积搜索性能的同时，显著提高了对嵌入质量的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 神经文档检索通常将语料库视为平面向量集合，忽略了语料库结构，且缺乏解释性。研究旨在利用语料库结构，提供更透明的检索机制。

Method: 利用Cobweb框架将句子嵌入组织成原型树，通过从粗到精的遍历进行文档排名。内部节点充当概念原型，提供多粒度相关性信号和可解释的检索路径。实现了两种推理方法：广义最佳优先搜索和轻量级路径求和排序器。在MS MARCO和QQP数据集上，使用编码器（BERT/T5）和解码器（GPT-2）表示进行评估。

Result: 在强编码器嵌入上，所提出的检索方法能与点积搜索性能相当。当kNN性能下降时（例如，使用GPT-2向量时，点积性能崩溃），我们的方法仍然能检索到相关结果，表现出更强的鲁棒性。

Conclusion: Cobweb框架通过分层原型提供了具有竞争力的有效性、对嵌入质量的改进鲁棒性、可扩展性和可解释的检索。

Abstract: Neural document retrieval often treats a corpus as a flat cloud of vectors
scored at a single granularity, leaving corpus structure underused and
explanations opaque. We use Cobweb--a hierarchy-aware framework--to organize
sentence embeddings into a prototype tree and rank documents via coarse-to-fine
traversal. Internal nodes act as concept prototypes, providing multi-granular
relevance signals and a transparent rationale through retrieval paths. We
instantiate two inference approaches: a generalized best-first search and a
lightweight path-sum ranker. We evaluate our approaches on MS MARCO and QQP
with encoder (e.g., BERT/T5) and decoder (GPT-2) representations. Our results
show that our retrieval approaches match the dot product search on strong
encoder embeddings while remaining robust when kNN degrades: with GPT-2
vectors, dot product performance collapses whereas our approaches still
retrieve relevant results. Overall, our experiments suggest that Cobweb
provides competitive effectiveness, improved robustness to embedding quality,
scalability, and interpretable retrieval via hierarchical prototypes.

</details>


### [43] [Knowledge-Graph Based RAG System Evaluation Framework](https://arxiv.org/abs/2510.02549)
*Sicheng Dong,Vahid Zolfaghari,Nenad Petrovic,Alois Knoll*

Main category: cs.CL

TL;DR: 针对RAG系统评估挑战，本文提出一种基于知识图谱（KG）的评估范式，通过多跳推理和语义聚类提供更全面的评分指标，并实验证明其对语义差异更敏感，优于传统评估方法。


<details>
  <summary>Details</summary>
Motivation: RAG系统评估仍面临挑战，传统评估指标难以有效捕捉LLM生成内容的高流畅性和自然度等关键特征。

Method: 受RAGAS工具启发，将评估框架扩展为基于知识图谱（KG）的范式，支持多跳推理和语义社区聚类以导出更全面的评分指标。通过与RAGAS分数比较，并构建人工标注子集来评估自动化指标与人工判断的相关性。

Result: 所提出的基于KG的评估方法能够对RAG系统性能提供更深入和细致的理解。实验证明，该方法对生成输出中细微的语义差异更敏感。

Conclusion: 提出了一种基于知识图谱的RAG系统评估新方法，提供了更全面且对语义差异更敏感的评估能力。同时，讨论了RAG系统评估的主要挑战并指出了未来的研究方向。

Abstract: Large language models (LLMs) has become a significant research focus and is
utilized in various fields, such as text generation and dialog systems. One of
the most essential applications of LLM is Retrieval Augmented Generation (RAG),
which greatly enhances generated content's reliability and relevance. However,
evaluating RAG systems remains a challenging task. Traditional evaluation
metrics struggle to effectively capture the key features of modern
LLM-generated content that often exhibits high fluency and naturalness.
Inspired by the RAGAS tool, a well-known RAG evaluation framework, we extended
this framework into a KG-based evaluation paradigm, enabling multi-hop
reasoning and semantic community clustering to derive more comprehensive
scoring metrics. By incorporating these comprehensive evaluation criteria, we
gain a deeper understanding of RAG systems and a more nuanced perspective on
their performance. To validate the effectiveness of our approach, we compare
its performance with RAGAS scores and construct a human-annotated subset to
assess the correlation between human judgments and automated metrics. In
addition, we conduct targeted experiments to demonstrate that our KG-based
evaluation method is more sensitive to subtle semantic differences in generated
outputs. Finally, we discuss the key challenges in evaluating RAG systems and
highlight potential directions for future research.

</details>


### [44] [Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models](https://arxiv.org/abs/2510.02569)
*Tolúl\d{o}pé Ògúnrèmí,Christopher D. Manning,Dan Jurafsky,Karen Livescu*

Main category: cs.CL

TL;DR: 研究分析了三种SLM中模态适配器（MA）的表示策略，发现使用Whisper编码器的模型倾向于用英语语际语言表示语义，而非Whisper编码器的模型则用英语词汇表示语音。这种差异可能取决于语音编码器是否进行翻译训练。


<details>
  <summary>Details</summary>
Motivation: 口语语言模型（SLM）中的模态适配器（MA）对于将语音编码器输出转换为语言模型可理解的表示至关重要，但我们对其如何转换表示知之甚少。

Method: 通过检查SALMONN、Qwen2-Audio和Phi-4-Multimodal-Instruct这三种SLM的MA输出表示，并找到MA表示最接近的解码器语言模型（LM）token进行分析。

Result: 1. 对于使用Whisper编码器的模型，MAs似乎使用基于英语的语际语言来表示输入内容的语义，使其能处理未在指令调优中出现的语言。
2. 对于不使用Whisper编码器的模型（如Phi-4-Multimodal-Instruct），MAs则使用英语词汇来表示输入的语音。

Conclusion: 研究假设MA表示策略的产生取决于语音编码器是仅为语音识别训练，还是同时为翻译任务训练。

Abstract: Spoken language models (SLMs) that integrate speech with large language
models (LMs) rely on modality adapters (MAs) to map the output of speech
encoders to a representation that is understandable to the decoder LM. Yet we
know very little about how these crucial MAs transform representations. Here we
examine the MA output representation in three SLMs (SALMONN, Qwen2-Audio and
Phi-4-Multimodal-Instruct). By finding the nearest decoder LM token to an MA
representation, we uncover two strategies for MA representations. For models
using a Whisper encoder, MAs appear to represent the meaning of the input using
an English-based interlingua, allowing them to handle languages unseen in
instruction tuning. For models that don't, like Phi-4-Multimodal-Instruct, MAs
instead represent the phonetics of the input, but expressed with English words.
We hypothesise that which arises depends on whether the speech encoder is
trained only for speech recognition or also for translation.

</details>


### [45] [Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models](https://arxiv.org/abs/2510.02629)
*Jingyi Sun,Pepa Atanasova,Sagnik Ray Choudhury,Sekh Mainul Islam,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本研究引入首个金标准评估框架，用于评估高亮解释（HEs）在解释语言模型上下文利用方面的准确性。结果发现MechLight表现最佳，但所有方法在处理长上下文和位置偏差时仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 语言模型（LMs）如何利用上下文信息来生成响应对用户来说是不透明的，用户无法判断模型是使用参数记忆还是提供的上下文，也无法识别具体是哪些上下文片段影响了响应。高亮解释（HEs）有望通过指出受影响的精确上下文片段和令牌来解决此问题，但目前尚无现有工作有效评估它们在准确解释上下文利用方面的效果。

Method: 研究引入了首个金标准HE评估框架，用于上下文归因，该框架使用具有已知真实上下文使用情况的受控测试案例，以避免现有间接代理评估的局限性。为展示框架的广泛适用性，研究评估了四种HE方法——包括MechLight（一种为此任务改编的机制可解释性方法）和三种现有技术——在四种上下文场景、四个数据集和五个语言模型上的表现。

Result: 总体而言，MechLight在所有上下文场景中表现最佳。然而，所有方法在处理较长上下文时都表现不佳，并表现出位置偏差。

Conclusion: 研究指出了解释准确性方面的根本挑战，表明需要新的方法才能大规模提供可靠的上下文利用解释。

Abstract: Context utilisation, the ability of Language Models (LMs) to incorporate
relevant information from the provided context when generating responses,
remains largely opaque to users, who cannot determine whether models draw from
parametric memory or provided context, nor identify which specific context
pieces inform the response. Highlight explanations (HEs) offer a natural
solution as they can point the exact context pieces and tokens that influenced
model outputs. However, no existing work evaluates their effectiveness in
accurately explaining context utilisation. We address this gap by introducing
the first gold standard HE evaluation framework for context attribution, using
controlled test cases with known ground-truth context usage, which avoids the
limitations of existing indirect proxy evaluations. To demonstrate the
framework's broad applicability, we evaluate four HE methods -- three
established techniques and MechLight, a mechanistic interpretability approach
we adapt for this task -- across four context scenarios, four datasets, and
five LMs. Overall, we find that MechLight performs best across all context
scenarios. However, all methods struggle with longer contexts and exhibit
positional biases, pointing to fundamental challenges in explanation accuracy
that require new approaches to deliver reliable context utilisation
explanations at scale.

</details>


### [46] [Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions](https://arxiv.org/abs/2510.02645)
*Fulei Zhang,Zhou Yu*

Main category: cs.CL

TL;DR: 研究发现用户与LLM聊天机器人和人类代理的沟通方式不同，体现在语法流畅性、礼貌性和词汇多样性上。为提高LLM对这种沟通风格变化的鲁棒性，作者测试了后训练阶段的数据增强和推理时用户消息重构两种策略，结果表明风格多样的数据增强效果显著，而推理时重构效果不佳。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型(LLMs)越来越多地应用于客户服务，用户与LLM聊天机器人和人类代理的沟通方式差异是一个关键但尚未充分探索的问题。现有LLM模型主要基于人-人交互数据训练，可能无法适应LLM部署后用户沟通风格的变化，从而影响其性能和用户体验。

Method: 本研究首先通过实证分析，揭示了用户与聊天机器人和人类代理交互时，在语法流畅性、礼貌性和词汇多样性方面存在显著差异。为增强LLM对这种沟通风格变化的鲁棒性，研究者尝试了两种策略：1) 在后训练阶段进行数据增强；2) 在推理时对用户消息进行重构。

Result: 实验结果显示，通过风格多样化数据集训练的模型，其性能显著优于仅使用原始或风格统一数据集训练的模型。然而，推理时的用户消息重构策略被证明效果较差。

Conclusion: 研究表明，理解用户与LLM的独特沟通方式至关重要，通过在训练数据中引入风格多样性可以有效提升LLM的鲁棒性，从而改善LLM与用户的交互体验。

Abstract: As Large Language Models (LLMs) are increasingly deployed in customer-facing
applications, a critical yet underexplored question is how users communicate
differently with LLM chatbots compared to human agent. In this study, we
present empirical evidence that users adopt distinct communication styles when
users interact with chatbots versus human agents. Our analysis reveals
significant differences in grammatical fluency, politeness, and lexical
diversity in user language between the two settings. These findings suggest
that models trained exclusively on human-human interaction data may not
adequately accommodate the communication style shift that occurs once an LLM
chatbot is deployed. To enhance LLM robustness to post-launch communication
style changes, we experimented with two strategies: (1) data augmentation
during the post-training phase and (2) inference-time user message
reformulation. Our results indicate that models trained on stylistically
diverse datasets significantly outperform those trained exclusively on original
or stylistically uniform datasets, while inference-time reformulation proved
less effective. These insights help us to better adapt our models for improved
LLM-user interaction experiences.

</details>


### [47] [SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models](https://arxiv.org/abs/2510.02648)
*Rui Qi,Zhibo Man,Yufeng Chen,Fengran Mo,Jinan Xu,Kaiyu Huang*

Main category: cs.CL

TL;DR: 提出了一种名为Structured-of-Thought (SoT)的免训练方法，通过将语言特定的语义信息转换为语言无关的结构化表示，显著提升了大型语言模型在多语言推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）的深度思考推理能力尚未成功迁移到非高资源语言，导致在多语言推理任务中表现不佳，主要原因是资源限制。

Method: 提出了免训练的Structured-of-Thought (SoT)方法。它通过“语言思维转换”和“结构化知识转换”两个步骤，将语言特定的语义信息转换为语言无关的结构化表示。这使得模型能更复杂地理解不同语言的查询，并引导LLMs进行更集中的推理，以维持跨语言表达一致的基础推理路径。

Result: 实验结果表明，SoT在多个多语言推理基准测试中优于多个强基线模型，并适用于不同LLM骨干网络。此外，它还可以与其他免训练策略结合以进一步提升性能。

Conclusion: SoT通过结构化知识转换有效解决了LLMs在非高资源语言中多语言推理能力不足的问题，显著提升了模型在不同语言和模型骨干上的推理表现，且具有良好的可扩展性。

Abstract: Recent developments have enabled Large Language Models (LLMs) to engage in
complex reasoning tasks through deep thinking. However, the capacity of
reasoning has not been successfully transferred to non-high-resource languages
due to resource constraints, which struggles with multilingual reasoning tasks.
To this end, we propose Structured-of-Thought (SoT), a training-free method
that improves the performance on multilingual reasoning through a multi-step
transformation: Language Thinking Transformation and Structured Knowledge
Transformation. The SoT method converts language-specific semantic information
into language-agnostic structured representations, enabling the models to
understand the query in different languages more sophisticated. Besides, SoT
effectively guides LLMs toward more concentrated reasoning to maintain
consistent underlying reasoning pathways when handling cross-lingual variations
in expression. Experimental results demonstrate that SoT outperforms several
strong baselines on multiple multilingual reasoning benchmarks when adapting to
various backbones of LLMs. It can also be integrated with other training-free
strategies for further improvements. Our code is available at
https://github.com/Cherry-qwq/SoT.

</details>


### [48] [Self-Improvement in Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2510.02665)
*Shijian Deng,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CL

TL;DR: 首次全面综述了多模态大型语言模型（MLLMs）的自我改进方法，涵盖数据处理和模型优化，并探讨未来挑战。


<details>
  <summary>Details</summary>
Motivation: LLMs的自我改进在不显著增加成本（尤其是人力成本）的情况下有效提升了模型能力。将这一能力扩展到多模态领域具有巨大潜力，可以利用多样化的数据源并开发更通用的自我改进模型。由于该领域尚处于早期，因此需要一个全面的概述。

Method: 本综述提供了多模态大型语言模型（MLLMs）自我改进的结构化概述，从三个视角讨论了现有方法：1) 数据收集，2) 数据组织，以及3) 模型优化。此外，还包含了常用的评估方法和下游应用。

Result: 首次提供了关于多模态大型语言模型（MLLMs）自我改进的全面综述，并系统性地梳理了现有文献，为领域发展提供了结构化框架。

Conclusion: 总结了当前面临的开放挑战，并指出了未来的研究方向，以促进多模态大型语言模型（MLLMs）自我改进的进一步发展。

Abstract: Recent advancements in self-improvement for Large Language Models (LLMs) have
efficiently enhanced model capabilities without significantly increasing costs,
particularly in terms of human effort. While this area is still relatively
young, its extension to the multimodal domain holds immense potential for
leveraging diverse data sources and developing more general self-improving
models. This survey is the first to provide a comprehensive overview of
self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview
of the current literature and discuss methods from three perspectives: 1) data
collection, 2) data organization, and 3) model optimization, to facilitate the
further development of self-improvement in MLLMs. We also include commonly used
evaluations and downstream applications. Finally, we conclude by outlining open
challenges and future research directions.

</details>


### [49] [Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering](https://arxiv.org/abs/2510.02671)
*Yavuz Bakman,Sungmin Kang,Zhiqi Huang,Duygu Nur Yaldiz,Catarina G. Belém,Chenyang Zhu,Anoop Kumar,Alfy Samuel,Salman Avestimehr,Daben Liu,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: 本文提出一种理论基础的方法，用于量化上下文问答（contextual QA）任务中的认知不确定性，通过语义特征差距解释不确定性，并在多项基准测试中显著超越现有SOTA方法，同时保持低推理开销。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化（UQ）研究主要关注封闭式事实问答，而对实际应用中至关重要的上下文问答却鲜有探索。

Method: 提出一种理论基础的认知不确定性量化方法。首先，引入任务无关的token级不确定性度量（预测分布与真实分布的交叉熵），将其分解并隔离认知部分，通过理想模型近似真实分布。其次，推导认知不确定性的上限，并将其解释为模型隐藏表示中相对于理想模型的语义特征差距。具体在上下文QA任务中，通过上下文依赖、上下文理解和诚实性这三个特征近似该差距，并采用自上而下的可解释性方法，仅用少量标注样本提取并集成这些特征，形成鲁棒的不确定性分数。

Result: 在多种QA基准测试（包括in-distribution和out-of-distribution设置）中，该方法显著优于最先进的无监督（无采样和基于采样）和有监督UQ方法，PRR提升高达13点，且推理开销可忽略不计。

Conclusion: 本研究成功为上下文QA任务量化了认知不确定性，并通过语义特征差距进行解释，实现了显著优于现有方法的性能，并保持了高效性。

Abstract: Uncertainty Quantification (UQ) research has primarily focused on closed-book
factual question answering (QA), while contextual QA remains unexplored,
despite its importance in real-world applications. In this work, we focus on UQ
for the contextual QA task and propose a theoretically grounded approach to
quantify epistemic uncertainty. We begin by introducing a task-agnostic,
token-level uncertainty measure defined as the cross-entropy between the
predictive distribution of the given model and the unknown true distribution.
By decomposing this measure, we isolate the epistemic component and approximate
the true distribution by a perfectly prompted, idealized model. We then derive
an upper bound for epistemic uncertainty and show that it can be interpreted as
semantic feature gaps in the given model's hidden representations relative to
the ideal model. We further apply this generic framework to the contextual QA
task and hypothesize that three features approximate this gap: context-reliance
(using the provided context rather than parametric knowledge), context
comprehension (extracting relevant information from context), and honesty
(avoiding intentional lies). Using a top-down interpretability approach, we
extract these features by using only a small number of labeled samples and
ensemble them to form a robust uncertainty score. Experiments on multiple QA
benchmarks in both in-distribution and out-of-distribution settings show that
our method substantially outperforms state-of-the-art unsupervised
(sampling-free and sampling-based) and supervised UQ methods, achieving up to a
13-point PRR improvement while incurring a negligible inference overhead.

</details>


### [50] [Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2510.02712)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.CL

TL;DR: 本文首次采用生存分析评估LLM在多轮对话中的鲁棒性，发现突发性语义漂移会导致对话失败，而渐进性漂移反而能保护对话，延长对话时长。这一发现挑战了对话AI中语义一致性的传统观念。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估框架仅限于静态基准和单轮评估，无法捕捉多轮对话中鲁棒性的时间动态和真实世界的对话退化过程。

Method: 对9个最先进LLM的36,951个对话回合进行首次全面的生存分析，将对话失败建模为时间到事件的过程。使用了Cox比例风险模型、加速失效时间（AFT）模型和随机生存森林方法。

Result: 突发性的、提示到提示（P2P）的语义漂移会灾难性地增加对话失败的风险；而渐进性的、累积的语义漂移则具有高度保护作用，大大降低了失败风险，并能显著延长对话。带交互的AFT模型表现出卓越的判别力和校准能力。

Conclusion: 生存分析是评估LLM鲁棒性的强大范式，为设计更具弹性的对话代理提供了具体见解，并挑战了对话AI系统对语义一致性的普遍假设。

Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their
robustness in extended multi-turn dialogues remains poorly understood. Existing
evaluation frameworks focus on static benchmarks and single-turn assessments,
failing to capture the temporal dynamics of conversational degradation that
characterize real-world interactions. In this work, we present the first
comprehensive survival analysis of conversational AI robustness, analyzing
36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a
time-to-event process. Our survival modeling framework-employing Cox
proportional hazards, Accelerated Failure Time, and Random Survival Forest
approaches-reveals extraordinary temporal dynamics. We find that abrupt,
prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing
the hazard of conversational failure. In stark contrast, gradual, cumulative
drift is highly protective, vastly reducing the failure hazard and enabling
significantly longer dialogues. AFT models with interactions demonstrate
superior performance, achieving excellent discrimination and exceptional
calibration. These findings establish survival analysis as a powerful paradigm
for evaluating LLM robustness, offer concrete insights for designing resilient
conversational agents, and challenge prevailing assumptions about the necessity
of semantic consistency in conversational AI Systems.

</details>


### [51] [TravelBench : Exploring LLM Performance in Low-Resource Domains](https://arxiv.org/abs/2510.02719)
*Srinivas Billa,Xiaonan Jing*

Main category: cs.CL

TL;DR: 现有LLM基准在低资源任务上不足，本研究构建了14个旅游领域数据集，发现LLM在此类任务中存在瓶颈，但推理能力能有效提升小型LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准未能充分反映模型在低资源任务中的能力，阻碍了相关领域有效解决方案的开发。

Method: 通过匿名化真实场景数据，构建了14个涵盖7种常见NLP任务的旅游领域数据集，并分析了LLM在该数据集上的准确性、扩展行为和推理能力。

Result: 研究证实通用基准不足以理解LLM在低资源任务上的表现；LLM在复杂、领域特定场景中存在性能瓶颈；推理能力能显著提升小型LLM在某些任务上的表现。

Conclusion: 通用LLM基准不足以评估低资源任务性能，LLM在领域特定复杂场景下表现受限，但推理能力可作为增强小型模型的有效手段。

Abstract: Results on existing LLM benchmarks capture little information over the model
capabilities in low-resource tasks, making it difficult to develop effective
solutions in these domains. To address these challenges, we curated 14
travel-domain datasets spanning 7 common NLP tasks using anonymised data from
real-world scenarios, and analysed the performance across LLMs. We report on
the accuracy, scaling behaviour, and reasoning capabilities of LLMs in a
variety of tasks. Our results confirm that general benchmarking results are
insufficient for understanding model performance in low-resource tasks. Despite
the amount of training FLOPs, out-of-the-box LLMs hit performance bottlenecks
in complex, domain-specific scenarios. Furthermore, reasoning provides a more
significant boost for smaller LLMs by making the model a better judge on
certain tasks.

</details>


### [52] [PGMEL: Policy Gradient-based Generative Adversarial Network for Multimodal Entity Linking](https://arxiv.org/abs/2510.02726)
*KM Pooja,Cheng Long,Aixin Sun*

Main category: cs.CL

TL;DR: 本文提出PGMEL，一种基于策略梯度生成对抗网络的跨模态实体链接方法，通过生成高质量负样本来学习更有效的表示，超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态实体链接(MEL)方法未探索高质量负样本在度量/表示学习中的重要性，这限制了模型性能。

Method: 提出PGMEL，一种基于策略梯度的生成对抗网络。生成器负责生成高质量负样本（离散过程，用策略梯度优化），判别器负责执行度量学习任务。

Result: PGMEL在Wiki-MEL、Richpedia-MEL和WikiDiverse数据集上的实验结果表明，通过选择有挑战性的负样本，PGMEL学习到有意义的表示，并优于现有最先进方法。

Conclusion: PGMEL通过在生成对抗框架中引入策略梯度生成高质量负样本，有效解决了跨模态实体链接中的表示学习问题，并取得了显著性能提升。

Abstract: The task of entity linking, which involves associating mentions with their
respective entities in a knowledge graph, has received significant attention
due to its numerous potential applications. Recently, various multimodal entity
linking (MEL) techniques have been proposed, targeted to learn comprehensive
embeddings by leveraging both text and vision modalities. The selection of
high-quality negative samples can potentially play a crucial role in
metric/representation learning. However, to the best of our knowledge, this
possibility remains unexplored in existing literature within the framework of
MEL. To fill this gap, we address the multimodal entity linking problem in a
generative adversarial setting where the generator is responsible for
generating high-quality negative samples, and the discriminator is assigned the
responsibility for the metric learning tasks. Since the generator is involved
in generating samples, which is a discrete process, we optimize it using policy
gradient techniques and propose a policy gradient-based generative adversarial
network for multimodal entity linking (PGMEL). Experimental results based on
Wiki-MEL, Richpedia-MEL and WikiDiverse datasets demonstrate that PGMEL learns
meaningful representation by selecting challenging negative samples and
outperforms state-of-the-art methods.

</details>


### [53] [IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context](https://arxiv.org/abs/2510.02742)
*Santhosh G S,Akshay Govind S,Gokul S Krishnan,Balaraman Ravindran,Sriraam Natarajan*

Main category: cs.CL

TL;DR: LLMs存在文化偏见，尤其是在印度语境下现有方法不足。本文提出基于对比学习的评估框架和IndiCASA数据集，发现所有模型都有偏见，残疾相关偏见最顽固。亟需开发更公平的模型。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在关键领域广泛应用，对其内置偏见的严格评估至关重要，特别是在印度等文化多元环境中，现有基于嵌入的偏见评估方法难以捕捉细微的刻板印象。

Method: 提出一个基于对比学习训练的编码器评估框架，通过嵌入相似性捕捉细粒度偏见。同时引入了新数据集IndiCASA，包含2,575个人工验证句子，涵盖种姓、性别、宗教、残疾和社会经济地位五个维度。

Result: 对多个开源LLM的评估显示，所有模型都表现出一定程度的刻板印象偏见。其中，残疾相关偏见尤为顽固，而宗教偏见普遍较低（可能得益于全球去偏见努力）。

Conclusion: 研究结果突显了开发更公平的LLM模型的迫切需求。

Abstract: Large Language Models (LLMs) have gained significant traction across critical
domains owing to their impressive contextual understanding and generative
capabilities. However, their increasing deployment in high stakes applications
necessitates rigorous evaluation of embedded biases, particularly in culturally
diverse contexts like India where existing embedding-based bias assessment
methods often fall short in capturing nuanced stereotypes. We propose an
evaluation framework based on a encoder trained using contrastive learning that
captures fine-grained bias through embedding similarity. We also introduce a
novel dataset - IndiCASA (IndiBias-based Contextually Aligned Stereotypes and
Anti-stereotypes) comprising 2,575 human-validated sentences spanning five
demographic axes: caste, gender, religion, disability, and socioeconomic
status. Our evaluation of multiple open-weight LLMs reveals that all models
exhibit some degree of stereotypical bias, with disability related biases being
notably persistent, and religion bias generally lower likely due to global
debiasing efforts demonstrating the need for fairer model development.

</details>


### [54] [The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback](https://arxiv.org/abs/2510.02752)
*Hangfan Zhang,Siyuan Xu,Zhimeng Guo,Huaisheng Zhu,Shicheng Liu,Xinrun Wang,Qiaosheng Zhang,Yang Chen,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: 本研究提出一种自感知强化学习方法，通过LLM自我生成任务和解决，并利用自感知难度预测和限制突破机制，在极少量额外数据下显著提升LLM推理能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）虽能增强大型语言模型（LLMs）的推理能力，但传统训练方式通常需要大量数据标注。本研究旨在探索如何在数据需求极小的情况下，通过RL改进LLMs。

Method: 所提方法让LLM交替进行任务提出和解决。为最小化数据依赖，引入了两种基于自感知的新机制：1) 自感知难度预测，模型学习评估任务难度并优先处理有挑战但可解决的任务；2) 自感知突破限制，模型识别超出自身能力边界的任务，并主动请求外部数据以突破该限制。

Result: 在九个基准测试上进行的大量实验表明，使用不到1.2%的额外数据，相对改进达到53.8%。

Conclusion: 自感知强化学习方法高效且有效，证明了自我演进智能体训练的巨大潜力。

Abstract: Reinforcement learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of large language models (LLMs), but such training
typically demands substantial efforts in creating and annotating data. In this
work, we explore improving LLMs through RL with minimal data. Our approach
alternates between the LLM proposing a task and then attempting to solve it. To
minimize data dependency, we introduce two novel mechanisms grounded in
self-awareness: (1) self-aware difficulty prediction, where the model learns to
assess task difficulty relative to its own abilities and prioritize challenging
yet solvable tasks, and (2) self-aware limit breaking, where the model
recognizes when a task is beyond its capability boundary and proactively
requests external data to break through that limit. Extensive experiments on
nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra
data demonstrate the efficacy of self-aware RL and underscore the promise of
self-evolving agent training.

</details>


### [55] [XTRA: Cross-Lingual Topic Modeling with Topic and Representation Alignments](https://arxiv.org/abs/2510.02788)
*Tien Phat Nguyen,Vu Minh Ngo,Tung Nguyen,Linh Van Ngo,Duc Anh Nguyen,Sang Dinh,Trung Le*

Main category: cs.CL

TL;DR: XTRA是一个新的跨语言主题模型框架，通过表征和主题双重对齐，解决了现有方法在主题连贯性和跨语言对齐方面的不足，并取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言主题模型方法在提高主题多样性时，往往难以确保高主题连贯性和一致的跨语言对齐。

Method: 提出XTRA框架，结合词袋模型与多语言嵌入。核心是两个组件：1) 表征对齐，通过对比学习在共享语义空间中对齐文档-主题分布；2) 主题对齐，将主题-词分布投影到同一空间以强制实现跨语言一致性。

Result: 在多语言语料库上的实验表明，XTRA在主题连贯性、多样性和对齐质量方面均显著优于强基线方法。

Conclusion: XTRA通过独特的双重对齐机制，能够学习到可解释（连贯且多样）且跨语言良好对齐的主题，显著提升了跨语言主题模型的性能。

Abstract: Cross-lingual topic modeling aims to uncover shared semantic themes across
languages. Several methods have been proposed to address this problem,
leveraging both traditional and neural approaches. While previous methods have
achieved some improvements in topic diversity, they often struggle to ensure
high topic coherence and consistent alignment across languages. We propose XTRA
(Cross-Lingual Topic Modeling with Topic and Representation Alignments), a
novel framework that unifies Bag-of-Words modeling with multilingual
embeddings. XTRA introduces two core components: (1) representation alignment,
aligning document-topic distributions via contrastive learning in a shared
semantic space; and (2) topic alignment, projecting topic-word distributions
into the same space to enforce crosslingual consistency. This dual mechanism
enables XTRA to learn topics that are interpretable (coherent and diverse) and
well-aligned across languages. Experiments on multilingual corpora confirm that
XTRA significantly outperforms strong baselines in topic coherence, diversity,
and alignment quality. Code and reproducible scripts are available at https:
//github.com/tienphat140205/XTRA.

</details>


### [56] [A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media](https://arxiv.org/abs/2510.02811)
*Matej Gjurković*

Main category: cs.CL

TL;DR: 针对自动人格评估中数据稀缺和模型可解释性不足的问题，本研究构建了大规模人格数据集PANDORA，并提出了SIMPA框架，通过语句-项目匹配实现了可解释、高效的人格评估，并具有广泛的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 自动人格评估在数字足迹日益普及的背景下变得愈发重要。然而，该领域面临两大核心挑战：一是缺乏大规模、带有人格标签的数据集；二是人格心理学与自然语言处理（NLP）之间的脱节，限制了模型的有效性和可解释性。

Method: 1. **数据构建**: 从Reddit平台收集并构建了MBTI9k和PANDORA两个数据集。PANDORA数据集包含1700万评论，整合了MBTI、大五人格模型和人口统计信息，解决了数据量、质量和标签覆盖范围的限制。
2. **框架开发**: 针对人口统计变量对模型有效性的影响，开发了SIMPA（Statement-to-Item Matching Personality Assessment）计算框架。该框架利用机器学习和语义相似性，将用户生成的语句与经过验证的问卷项目进行匹配，以实现可解释的人格评估。

Result: 1. 在所构建数据集上的实验表明，人口统计变量会影响模型的有效性。
2. SIMPA框架能够提供与人类评估相当的人格评估结果，同时保持了高可解释性和效率。

Conclusion: SIMPA框架具有通用性，其模型无关设计、分层线索检测和可扩展性使其不仅限于人格评估领域，还适用于处理复杂标签分类和变量线索关联的各种研究及实际应用。

Abstract: Personality refers to individual differences in behavior, thinking, and
feeling. With the growing availability of digital footprints, especially from
social media, automated methods for personality assessment have become
increasingly important. Natural language processing (NLP) enables the analysis
of unstructured text data to identify personality indicators. However, two main
challenges remain central to this thesis: the scarcity of large,
personality-labeled datasets and the disconnect between personality psychology
and NLP, which restricts model validity and interpretability. To address these
challenges, this thesis presents two datasets -- MBTI9k and PANDORA --
collected from Reddit, a platform known for user anonymity and diverse
discussions. The PANDORA dataset contains 17 million comments from over 10,000
users and integrates the MBTI and Big Five personality models with demographic
information, overcoming limitations in data size, quality, and label coverage.
Experiments on these datasets show that demographic variables influence model
validity. In response, the SIMPA (Statement-to-Item Matching Personality
Assessment) framework was developed - a computational framework for
interpretable personality assessment that matches user-generated statements
with validated questionnaire items. By using machine learning and semantic
similarity, SIMPA delivers personality assessments comparable to human
evaluations while maintaining high interpretability and efficiency. Although
focused on personality assessment, SIMPA's versatility extends beyond this
domain. Its model-agnostic design, layered cue detection, and scalability make
it suitable for various research and practical applications involving complex
label taxonomies and variable cue associations with target concepts.

</details>


### [57] [StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.02827)
*Tengjun Ni,Xin Yuan,Shenghong Li,Kai Wu,Ren Ping Liu,Wei Ni,Wenjie Zhang*

Main category: cs.CL

TL;DR: 引入StepChain GraphRAG框架，结合问题分解和BFS推理流，提升多跳问答性能并实现SOTA，同时增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）在多跳问答中仍面临迭代推理步骤与外部知识检索整合的挑战。

Method: 提出StepChain GraphRAG，结合问题分解和广度优先搜索（BFS）推理流。方法包括预构建语料全局索引；推理时动态将检索到的段落解析为知识图谱；将复杂查询分解为子问题；为每个子问题使用BFS遍历动态扩展相关边以构建明确的证据链。

Result: 在MuSiQue、2WikiMultiHopQA和HotpotQA数据集上，StepChain GraphRAG实现了SOTA的Exact Match和F1分数，平均EM提升2.57%，F1提升2.13%（HotpotQA提升最大）。该方法通过保留中间检索步骤的思维链，增强了可解释性。

Conclusion: StepChain GraphRAG有效提升了多跳问答的准确性和可解释性。未来工作将致力于减轻计算开销，并解决大型语言模型可能产生的幻觉问题，以提高多跳问答的效率和可靠性。

Abstract: Recent progress in retrieval-augmented generation (RAG) has led to more
accurate and interpretable multi-hop question answering (QA). Yet, challenges
persist in integrating iterative reasoning steps with external knowledge
retrieval. To address this, we introduce StepChain GraphRAG, a framework that
unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow
for enhanced multi-hop QA. Our approach first builds a global index over the
corpus; at inference time, only retrieved passages are parsed on-the-fly into a
knowledge graph, and the complex query is split into sub-questions. For each
sub-question, a BFS-based traversal dynamically expands along relevant edges,
assembling explicit evidence chains without overwhelming the language model
with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA
show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1
scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the
SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1).
StepChain GraphRAG also fosters enhanced explainability by preserving the
chain-of-thought across intermediate retrieval steps. We conclude by discussing
how future work can mitigate the computational overhead and address potential
hallucinations from large language models to refine efficiency and reliability
in multi-hop QA.

</details>


### [58] [Evaluating Large Language Models for IUCN Red List Species Information](https://arxiv.org/abs/2510.02830)
*Shinya Uryu*

Main category: cs.CL

TL;DR: 大语言模型在生物多样性保护中，擅长物种分类但无法进行保护推理，存在架构限制和偏见，建议人机协作以实现有效部署。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在生物多样性保护中应用日益广泛，需要系统评估其在物种评估方面的可靠性。

Method: 本研究系统验证了五款主流大语言模型，对21,955个物种进行了测试，涵盖IUCN红色名录评估的分类学、保护状况、分布和威胁四个核心组成部分。

Result: 模型在分类学方面表现出色（94.9%），但在保护推理（如状况评估仅27.2%）方面持续失败，揭示了知识-推理鸿沟。此外，模型存在偏袒魅力脊椎动物的系统性偏见。

Conclusion: 大语言模型是强大的信息检索工具，但在判断性决策上需要人工监督。建议采用混合方法，即大语言模型辅助专家，而专家保留风险评估和政策决策的最终权力。

Abstract: Large Language Models (LLMs) are rapidly being adopted in conservation to
address the biodiversity crisis, yet their reliability for species evaluation
is uncertain. This study systematically validates five leading models on 21,955
species across four core IUCN Red List assessment components: taxonomy,
conservation status, distribution, and threats. A critical paradox was
revealed: models excelled at taxonomic classification (94.9%) but consistently
failed at conservation reasoning (27.2% for status assessment). This
knowledge-reasoning gap, evident across all models, suggests inherent
architectural constraints, not just data limitations. Furthermore, models
exhibited systematic biases favoring charismatic vertebrates, potentially
amplifying existing conservation inequities. These findings delineate clear
boundaries for responsible LLM deployment: they are powerful tools for
information retrieval but require human oversight for judgment-based decisions.
A hybrid approach is recommended, where LLMs augment expert capacity while
human experts retain sole authority over risk assessment and policy.

</details>


### [59] [Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation](https://arxiv.org/abs/2510.02855)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Kamrujjaman,Eftakhar Ahmed Arnob,Ahsan Habib Tareq*

Main category: cs.CL

TL;DR: 该研究首次将Wordle系统性地建模为约束满足问题（CSP），并提出了两种新策略：CSP感知熵和概率CSP。这些方法在求解效率、成功率和抗噪声能力上均优于现有方法，并展现了跨语言的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有Wordle求解器依赖信息论熵最大化或基于频率的启发式方法，缺乏形式化的约束处理。Wordle本身是一个算法丰富的约束满足问题（CSP）测试平台，需要一种更全面的CSP公式化方法和约束感知求解策略。

Method: 本研究提出了Wordle的全面CSP公式化，并引入了两种创新求解策略：1. CSP感知熵：在约束传播后而非原始候选集上计算信息增益。2. 概率CSP框架：将贝叶斯词频先验与逻辑约束相结合。研究通过对2,315个英文单词和500个西班牙语单词进行评估，并进行噪声（0-20%）下的鲁棒性分析，验证了方法的有效性。提供了开源实现和高代码覆盖率的单元测试。

Result: CSP感知熵在英文单词上取得了平均3.54次猜测和99.9%的成功率，相较于前向检查，统计学上显著提高了1.7%的性能（t=-4.82, p<0.001, Cohen's d=0.07），运行时速度提高了46%（12.9ms对比23.7ms）。在10%噪声下，CSP感知方法保持了5.3个百分点的优势（29.0%对比23.7%, p=0.041）。概率CSP通过约束恢复机制，在所有噪声水平（0-20%）下均实现了100%的成功率。对500个西班牙语单词的跨词典验证表明，在零语言特定调整下达到88%的成功率，证实了核心CSP原则的跨语言通用性。

Conclusion: 结合形式化的CSP处理、约束感知启发式、概率-逻辑集成、鲁棒性分析和跨词典验证，本研究建立了新的性能基准。结果表明，对于结构化谜题求解领域，遵循原则的约束满足技术优于经典的基于信息论和学习的方法。

Abstract: Wordle presents an algorithmically rich testbed for constraint satisfaction
problem (CSP) solving. While existing solvers rely on information-theoretic
entropy maximization or frequency-based heuristics without formal constraint
treatment, we present the first comprehensive CSP formulation of Wordle with
novel constraint-aware solving strategies. We introduce CSP-Aware Entropy,
computing information gain after constraint propagation rather than on raw
candidate sets, and a Probabilistic CSP framework integrating Bayesian
word-frequency priors with logical constraints. Through evaluation on 2,315
English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9%
success rate, a statistically significant 1.7% improvement over Forward
Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms
versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3
percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic
CSP achieves 100% success across all noise levels (0-20%) through constraint
recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates
88% success with zero language-specific tuning, validating that core CSP
principles transfer across languages despite an 11.2 percentage point gap from
linguistic differences (p<0.001, Fisher's exact test). Our open-source
implementation with 34 unit tests achieving 91% code coverage provides
reproducible infrastructure for CSP research. The combination of formal CSP
treatment, constraint-aware heuristics, probabilistic-logical integration,
robustness analysis, and cross-lexicon validation establishes new performance
benchmarks demonstrating that principled constraint satisfaction techniques
outperform classical information-theoretic and learning-based approaches for
structured puzzle-solving domains.

</details>


### [60] [Self-Reflective Generation at Test Time](https://arxiv.org/abs/2510.02919)
*Jian Mu,Qixin Zhang,Zhiyong Wang,Menglin Yang,Shuang Qiu,Chengwei Qin,Zhongxiang Dai,Yao Shu*

Main category: cs.CL

TL;DR: 本文提出SRGen，一种轻量级测试时自反思框架，通过在生成高不确定性token之前动态识别并纠正其概率分布，有效提升大型语言模型在数学推理任务上的表现，且具有低开销和高兼容性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长链式思维推理中易受早期token错误影响导致错误传播，现有自反思机制多为事后修正或需昂贵训练，效率低下。因此，亟需一种更主动、高效的自反思机制。

Method: SRGen在测试时工作，通过动态熵阈值识别生成过程中的高不确定性token。对于每个此类token，它利用已生成的上下文训练一个特定的校正向量，以自反思地调整token的概率分布，从而在生成前纠正潜在错误。

Result: SRGen在多种大型语言模型和挑战性数学推理基准上一致性地增强了模型推理能力。例如，在AIME2024数据集上使用DeepSeek-R1-Distill-Qwen-7B模型时，Pass@1和Cons@5指标分别取得了+12.0%和+13.3%的绝对提升。它被证明是一个即插即用的方法，具有有限的开销并能与现有训练时和测试时技术广泛结合。

Conclusion: SRGen为LLM提供了一种可靠的推理机制，通过将前瞻性自反思集成到生成过程中，有效减少不确定点上的错误。它是一个高效、可插拔且兼容性强的测试时框架，能在保持低开销的同时实现显著的性能提升。

Abstract: Large language models (LLMs) increasingly solve complex reasoning tasks via
long chain-of-thought, but their forward-only autoregressive generation process
is fragile; early token errors can cascade, which creates a clear need for
self-reflection mechanisms. However, existing self-reflection either performs
revisions over full drafts or learns self-correction via expensive training,
both fundamentally reactive and inefficient. To address this, we propose
Self-Reflective Generation at Test Time (SRGen), a lightweight test-time
framework that reflects before generating at uncertain points. During token
generation, SRGen utilizes dynamic entropy thresholding to identify
high-uncertainty tokens. For each identified token, it trains a specific
corrective vector, which fully exploits the already generated context for a
self-reflective generation to correct the token probability distribution. By
retrospectively analyzing the partial output, this self-reflection enables more
trustworthy decisions, thereby significantly reducing the probability of errors
at highly uncertain points. Evaluated on challenging mathematical reasoning
benchmarks and a diverse set of LLMs, SRGen can consistently strengthen model
reasoning: improvements in single-pass quality also translate into stronger
self-consistency voting. Especially, on AIME2024 with
DeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on
Pass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a
plug-and-play method that integrates reflection into the generation process for
reliable LLM reasoning, achieving consistent gains with bounded overhead and
broad composability with other training-time (e.g., RLHF) and test-time (e.g.,
SLOT) techniques.

</details>


### [61] [Finding Diamonds in Conversation Haystacks: A Benchmark for Conversational Data Retrieval](https://arxiv.org/abs/2510.02938)
*Yohan Lee,Yongwoo Song,Sangyeop Kim*

Main category: cs.CL

TL;DR: 本文提出了对话数据检索(CDR)基准，这是首个用于评估检索对话数据以获取产品洞察力的系统测试集。研究发现现有模型在该任务上表现不佳，并识别了该领域的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 需要一个全面的基准来评估和衡量用于产品洞察的对话数据检索系统的性能。现有的文档检索能力与对话数据检索之间存在显著差距，需要识别并解决这一问题。

Method: 提出了“对话数据检索 (CDR)”基准，包含1.6k查询、9.1k对话和5项分析任务。利用该基准评估了16个流行的嵌入模型，并进行了详细的错误分析，以识别对话数据检索的独特挑战。

Result: 即使是表现最佳的嵌入模型，其NDCG@10也仅达到约0.51，表明文档检索与对话数据检索能力之间存在显著差距。研究识别出对话数据检索的独特挑战，包括隐式状态识别、轮次动态和上下文引用。

Conclusion: 对话数据检索是一项具有挑战性的任务，当前嵌入模型在该领域表现不佳。CDR基准为衡量该性能提供了一个可靠标准，未来的研究应专注于解决对话数据检索中识别出的独特挑战。

Abstract: We present the Conversational Data Retrieval (CDR) benchmark, the first
comprehensive test set for evaluating systems that retrieve conversation data
for product insights. With 1.6k queries across five analytical tasks and 9.1k
conversations, our benchmark provides a reliable standard for measuring
conversational data retrieval performance. Our evaluation of 16 popular
embedding models shows that even the best models reach only around NDCG@10 of
0.51, revealing a substantial gap between document and conversational data
retrieval capabilities. Our work identifies unique challenges in conversational
data retrieval (implicit state recognition, turn dynamics, contextual
references) while providing practical query templates and detailed error
analysis across different task categories. The benchmark dataset and code are
available at https://github.com/l-yohai/CDR-Benchmark.

</details>


### [62] [Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking](https://arxiv.org/abs/2510.02962)
*Jingqi Zhang,Ruibo Chen,Yingqing Yang,Peihua Mai,Heng Huang,Yan Pang*

Main category: cs.CL

TL;DR: TRACE是一个实用的黑盒框架，通过无失真水印和熵门控检测，可靠地识别LLM微调中受版权保护数据集的使用。


<details>
  <summary>Details</summary>
Motivation: LLM微调常用私有或受版权数据集，但缺乏可靠的黑盒检测方案来防止未经授权使用。现有方法（如MIA、水印技术）通常需要内部信号、参考数据集或会损害文本质量/性能，限制了实际应用。

Method: 提出TRACE框架，通过私钥引导，以无失真水印重写数据集，确保文本质量和实用性。检测时，利用微调的“放射性效应”，并引入熵门控程序，选择性地对高不确定性token评分，显著增强检测能力。

Result: TRACE在多样化的数据集和模型家族中均能实现显著（p<0.05）且统计证据极强的检测。它支持多数据集归属，并在大量非水印语料库上持续预训练后仍保持鲁棒性。

Conclusion: TRACE为LLM中受版权保护数据集使用情况的可靠黑盒验证提供了一条实用途径。

Abstract: Large Language Models (LLMs) are increasingly fine-tuned on smaller,
domain-specific datasets to improve downstream performance. These datasets
often contain proprietary or copyrighted material, raising the need for
reliable safeguards against unauthorized use. Existing membership inference
attacks (MIAs) and dataset-inference methods typically require access to
internal signals such as logits, while current black-box approaches often rely
on handcrafted prompts or a clean reference dataset for calibration, both of
which limit practical applicability. Watermarking is a promising alternative,
but prior techniques can degrade text quality or reduce task performance. We
propose TRACE, a practical framework for fully black-box detection of
copyrighted dataset usage in LLM fine-tuning. \texttt{TRACE} rewrites datasets
with distortion-free watermarks guided by a private key, ensuring both text
quality and downstream utility. At detection time, we exploit the radioactivity
effect of fine-tuning on watermarked data and introduce an entropy-gated
procedure that selectively scores high-uncertainty tokens, substantially
amplifying detection power. Across diverse datasets and model families, TRACE
consistently achieves significant detections (p<0.05), often with extremely
strong statistical evidence. Furthermore, it supports multi-dataset attribution
and remains robust even after continued pretraining on large non-watermarked
corpora. These results establish TRACE as a practical route to reliable
black-box verification of copyrighted dataset usage. We will make our code
available at: https://github.com/NusIoraPrivacy/TRACE.

</details>


### [63] [Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines](https://arxiv.org/abs/2510.02967)
*Matthew Lewis,Samuel Thio,Richard JB Dobson,Spiros Denaxas*

Main category: cs.CL

TL;DR: 本文开发并评估了一个检索增强生成（RAG）系统，用于通过大语言模型查询英国NICE临床指南。该系统在检索性能上表现出色，并在生成阶段显著提升了答案的忠实度和上下文精确度，有效解决了指南利用效率低下的问题，证明了RAG在医疗保健领域应用的有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 英国国家健康与护理卓越研究院（NICE）的临床指南篇幅庞大、内容繁多，在时间受限的医疗体系中难以高效利用。研究旨在开发一个系统，能够根据自然语言查询为用户提供精确匹配的信息。

Method: 开发了一个基于混合嵌入机制的检索架构，用于NICE临床指南的RAG系统。检索模块在包含10,195个文本块（来自300份指南）的数据库上，通过7901个查询进行评估。生成模块则在70个手动整理的问答对数据集上，评估RAG增强模型的性能，特别关注忠实度和上下文精确度。

Result: 检索阶段：系统表现出高性能，平均倒数排名（MRR）为0.814，前1个块的召回率为81%，前10个检索块的召回率为99.1%。生成阶段：RAG增强模型显著提高了性能，其中RAG增强的O4-Mini模型的忠实度增加了64.7个百分点，达到99.5%，远超医学专用模型Meditron3-8B的43%。所有RAG增强模型的上下文精确度均为1。

Conclusion: 该研究证实RAG是一种有效、可靠且可扩展的方法，适用于在医疗保健领域应用生成式AI，能经济高效地访问医疗指南，并通过将答案基于相关源材料有效防止信息编造。

Abstract: This paper presents the development and evaluation of a Retrieval-Augmented
Generation (RAG) system for querying the United Kingdom's National Institute
for Health and Care Excellence (NICE) clinical guidelines using Large Language
Models (LLMs). The extensive length and volume of these guidelines can impede
their utilisation within a time-constrained healthcare system, a challenge this
project addresses through the creation of a system capable of providing users
with precisely matched information in response to natural language queries. The
system's retrieval architecture, composed of a hybrid embedding mechanism, was
evaluated against a database of 10,195 text chunks derived from three hundred
guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)
of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten
retrieved chunks, when evaluated on 7901 queries.
  The most significant impact of the RAG system was observed during the
generation phase. When evaluated on a manually curated dataset of seventy
question-answer pairs, RAG-enhanced models showed substantial gains in
performance. Faithfulness, the measure of whether an answer is supported by the
source text, was increased by 64.7 percentage points to 99.5% for the
RAG-enhanced O4-Mini model and significantly outperformed the medical-focused
Meditron3-8B LLM, which scored 43%. This, combined with a perfect Context
Precision score of 1 for all RAG-enhanced models, confirms the system's ability
to prevent information fabrication by grounding its answers in relevant source
material. This study thus establishes RAG as an effective, reliable, and
scalable approach for applying generative AI in healthcare, enabling
cost-effective access to medical guidelines.

</details>


### [64] [Semantic Differentiation in Speech Emotion Recognition: Insights from Descriptive and Expressive Speech Roles](https://arxiv.org/abs/2510.03060)
*Rongchen Guo,Vincent Francoeur,Isar Nejadgholi,Sylvain Gagnon,Miodrag Bolic*

Main category: cs.CL

TL;DR: 本研究区分语音的描述性语义和表达性语义。实验发现描述性语义与意图情感一致，表达性语义与诱发情感相关，为SER应用及上下文感知AI系统提供依据。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别（SER）对改善人机交互至关重要，但其准确性受语音中复杂情感细微差别的限制。

Method: 本研究区分了语音的描述性语义（上下文内容）和表达性语义（说话者情感状态）。通过让参与者观看情感电影片段并录制其描述经历的音频，同时收集意图情感标签、自评情感反应以及效价/唤醒度得分。

Result: 实验结果表明，描述性语义与意图情感一致，而表达性语义则与诱发情感相关联。

Conclusion: 本研究的发现为人类-AI交互中的SER应用提供了信息，并为构建更具上下文感知能力的AI系统铺平了道路。

Abstract: Speech Emotion Recognition (SER) is essential for improving human-computer
interaction, yet its accuracy remains constrained by the complexity of
emotional nuances in speech. In this study, we distinguish between descriptive
semantics, which represents the contextual content of speech, and expressive
semantics, which reflects the speaker's emotional state. After watching
emotionally charged movie segments, we recorded audio clips of participants
describing their experiences, along with the intended emotion tags for each
clip, participants' self-rated emotional responses, and their valence/arousal
scores. Through experiments, we show that descriptive semantics align with
intended emotions, while expressive semantics correlate with evoked emotions.
Our findings inform SER applications in human-AI interaction and pave the way
for more context-aware AI systems.

</details>


### [65] [Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better Scaling than CoT Prompting?](https://arxiv.org/abs/2510.03093)
*Oriol Pareras,Gerard I. Gállego,Federico Costa,Cristina España-Bonet,Javier Hernando*

Main category: cs.CL

TL;DR: 本研究系统比较了在不同S2TT数据量下，LLM驱动的S2TT模型中Chain-of-Thought (CoT) 和直接提示的效果。结果表明，随着S2TT数据量的增加，直接提示的性能提升更持续，预示着其在大规模S2TT资源下可能更有效。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的S2TT模型中CoT提示因能利用大量ASR和T2TT数据而表现优异。本研究的动机是系统性地探究CoT和直接提示在逐渐增加S2TT专属数据量时的性能差异，以评估直接提示在拥有更多S2TT数据时的潜力。

Method: 通过将ASR语料的转录文本翻译成六种欧洲语言来伪标签化，以此生成不同规模的S2TT数据。随后，使用这些数据训练基于LLM的S2TT系统，并对比CoT和直接提示两种策略在不同数据量下的表现。

Result: 研究结果显示，随着S2TT数据量的增加，直接提示（Direct prompting）的性能提升更为持续和显著。

Conclusion: 结论指出，如果未来能够创建更大规模的S2TT资源，直接提示（Direct prompting）可能会成为一种更有效的S2TT方法。

Abstract: Recent work on Speech-to-Text Translation (S2TT) has focused on LLM-based
models, introducing the increasingly adopted Chain-of-Thought (CoT) prompting,
where the model is guided to first transcribe the speech and then translate it.
CoT typically outperforms direct prompting primarily because it can exploit
abundant Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT)
datasets to explicitly model its steps. In this paper, we systematically
compare CoT and Direct prompting under increasing amounts of S2TT data. To this
end, we pseudo-label an ASR corpus by translating its transcriptions into six
European languages, and train LLM-based S2TT systems with both prompting
strategies at different data scales. Our results show that Direct improves more
consistently as the amount of data increases, suggesting that it may become a
more effective approach as larger S2TT resources are created.

</details>


### [66] [Semantic Similarity in Radiology Reports via LLMs and NER](https://arxiv.org/abs/2510.03102)
*Beth Pearson,Ahmed Adnan,Zahraa Abdallah*

Main category: cs.CL

TL;DR: 本研究提出Llama-EntScore方法，结合Llama 3.1和命名实体识别（NER），用于定量并可解释地比较放射学报告的语义差异。该方法在评估初级与最终报告时表现出高准确性，优于单独的LLM或NER。


<details>
  <summary>Details</summary>
Motivation: 放射学报告评估是放射科医生培训和诊断准确性的关键环节。识别初级与最终报告间的语义差异对初级医生至关重要。尽管大型语言模型（LLMs）在放射学应用中面临领域知识挑战，但仍需探索其提供可解释、准确报告对比的能力，以弥补现有方法的局限性。

Method: 1. 首先比较了多个LLMs在放射学报告对比中的性能。2. 评估了基于命名实体识别（NER）的传统方法。3. 提出Llama-EntScore，该方法结合Llama 3.1和NER，并引入可调权重以强调或弱化特定类型的差异。4. Llama-EntScore生成一个定量的相似度分数，并提供对分数的解释以指导报告审阅。

Result: 1. Llama-EntScore方法在与放射科医生提供的真实分数对比时，实现了67%的精确匹配准确率。2. 在+/- 1的误差范围内，准确率达到93%。3. 该方法性能优于单独使用的LLMs和NER。

Conclusion: Llama-EntScore提供了一种有效且准确的解决方案，用于评估放射学报告的语义相似性。它不仅能生成量化分数，还能提供有价值的解释性指导，有望成为初级放射科医生培训和报告质量控制的重要工具。

Abstract: Radiology report evaluation is a crucial part of radiologists' training and
plays a key role in ensuring diagnostic accuracy. As part of the standard
reporting workflow, a junior radiologist typically prepares a preliminary
report, which is then reviewed and edited by a senior radiologist to produce
the final report. Identifying semantic differences between preliminary and
final reports is essential for junior doctors, both as a training tool and to
help uncover gaps in clinical knowledge. While AI in radiology is a rapidly
growing field, the application of large language models (LLMs) remains
challenging due to the need for specialised domain knowledge. In this paper, we
explore the ability of LLMs to provide explainable and accurate comparisons of
reports in the radiology domain. We begin by comparing the performance of
several LLMs in comparing radiology reports. We then assess a more traditional
approach based on Named-Entity-Recognition (NER). However, both approaches
exhibit limitations in delivering accurate feedback on semantic similarity. To
address this, we propose Llama-EntScore, a semantic similarity scoring method
using a combination of Llama 3.1 and NER with tunable weights to emphasise or
de-emphasise specific types of differences. Our approach generates a
quantitative similarity score for tracking progress and also gives an
interpretation of the score that aims to offer valuable guidance in reviewing
and refining their reporting. We find our method achieves 67% exact-match
accuracy and 93% accuracy within +/- 1 when compared to radiologist-provided
ground truth scores - outperforming both LLMs and NER used independently. Code
is available at:
\href{https://github.com/otmive/llama_reports}{github.com/otmive/llama\_reports}

</details>


### [67] [Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation](https://arxiv.org/abs/2510.03115)
*Jacobo Romero-Díaz,Gerard I. Gállego,Oriol Pareras,Federico Costa,Javier Hernando,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 分析发现，Chain-of-Thought (CoT) 提示在语音到文本翻译 (S2TT) 中仍主要依赖文本，很少利用语音。通过简单训练干预可增强鲁棒性和语音利用率，挑战了CoT的预期优势。


<details>
  <summary>Details</summary>
Motivation: 传统的S2TT系统（ASR+T2TT）存在错误传播和无法利用韵律等声学线索的问题。CoT提示被引入，期望通过同时访问语音和转录来克服这些限制。

Method: 通过归因方法、使用损坏转录本的鲁棒性评估以及韵律感知能力分析，对CoT系统进行了深入研究。

Result: 研究发现CoT在很大程度上模仿了级联行为，主要依赖转录本，几乎不利用语音信息。简单的训练干预（如添加直接S2TT数据或注入噪声转录本）可以增强鲁棒性并提高语音归因。

Conclusion: 这些发现挑战了CoT提示在S2TT中假定的优势，并强调需要设计明确整合声学信息的翻译架构。

Abstract: Speech-to-Text Translation (S2TT) systems built from Automatic Speech
Recognition (ASR) and Text-to-Text Translation (T2TT) modules face two major
limitations: error propagation and the inability to exploit prosodic or other
acoustic cues. Chain-of-Thought (CoT) prompting has recently been introduced,
with the expectation that jointly accessing speech and transcription will
overcome these issues. Analyzing CoT through attribution methods, robustness
evaluations with corrupted transcripts, and prosody-awareness, we find that it
largely mirrors cascaded behavior, relying mainly on transcripts while barely
leveraging speech. Simple training interventions, such as adding Direct S2TT
data or noisy transcript injection, enhance robustness and increase speech
attribution. These findings challenge the assumed advantages of CoT and
highlight the need for architectures that explicitly integrate acoustic
information into translation.

</details>


### [68] [SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?](https://arxiv.org/abs/2510.03120)
*Zhaojun Sun,Xuzhou Zhu,Xuanhe Zhou,Xin Tong,Shuo Wang,Jie Fu,Guoliang Li,Zhiyuan Liu,Fan Wu*

Main category: cs.CL

TL;DR: 本文提出了一个名为SurveyBench的细粒度、基于问答的评估框架，旨在严格评估大型语言模型生成的学术综述（LLM4Survey）并揭示其不足。


<details>
  <summary>Details</summary>
Motivation: 学术综述写作耗时且要求高；现有LLM自动生成的综述质量普遍不达标，且缺乏严谨、以读者为中心的评估基准来彻底揭示其缺陷。

Method: 提出了SurveyBench评估框架，其特点包括：1) 基于11,343篇arXiv论文和4,947篇高质量综述的典型主题；2) 包含大纲质量、内容质量和非文本丰富度的多维度指标体系；3) 采用内容和问答双模式评估协议，以匹配读者信息需求。

Result: 实验结果表明，SurveyBench能有效挑战现有LLM4Survey方法，例如在基于内容的评估中，LLM表现平均比人类低21%。

Conclusion: SurveyBench是一个有效且严谨的评估框架，能够揭示现有LLM4Survey方法的不足，并为未来LLM生成综述的改进提供方向。

Abstract: Academic survey writing, which distills vast literature into a coherent and
insightful narrative, remains a labor-intensive and intellectually demanding
task. While recent approaches, such as general DeepResearch agents and
survey-specialized methods, can generate surveys automatically (a.k.a.
LLM4Survey), their outputs often fall short of human standards and there lacks
a rigorous, reader-aligned benchmark for thoroughly revealing their
deficiencies. To fill the gap, we propose a fine-grained, quiz-driven
evaluation framework SurveyBench, featuring (1) typical survey topics source
from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;
(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,
coverage breadth, logical coherence), content quality (e.g., synthesis
granularity, clarity of insights), and non-textual richness; and (3) a
dual-mode evaluation protocol that includes content-based and quiz-based
answerability tests, explicitly aligned with readers' informational needs.
Results show SurveyBench effectively challenges existing LLM4Survey approaches
(e.g., on average 21% lower than human in content-based evaluation).

</details>


### [69] [Beyond the Final Layer: Intermediate Representations for Better Multilingual Calibration in Large Language Models](https://arxiv.org/abs/2510.03136)
*Ej Zhou,Caiqi Zhang,Tiancheng Hu,Chengzu Li,Nigel Collier,Ivan Vulić,Anna Korhonen*

Main category: cs.CL

TL;DR: 研究发现多语言LLMs在非英语语言中的置信度校准较差，这源于英式训练偏差导致末层信号不佳；提出通过分析模型的后期中间层并引入LACE等无训练方法，可以显著提升多语言LLMs的校准性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 置信度校准（模型预测置信度与其实际准确性的一致性）对于大型语言模型（LLMs）的可靠部署至关重要，但这一关键特性在多语言环境中尚未得到充分探索。

Method: 进行了首次大规模、系统性的多语言校准研究，涵盖六个模型家族和100多种语言。通过调查模型的内部表示和进行逐层分析来诊断问题。在此基础上，引入了一系列无训练方法，包括语言感知置信度集成（LACE），该方法能为每种特定语言自适应地选择最佳的层集合。

Result: 发现非英语语言的置信度校准系统性地更差。诊断表明，受英语中心训练偏见影响的最终层提供了不良的多语言置信度信号，而后期中间层持续提供更可靠、校准更好的信号。

Conclusion: 该研究揭示了以英语为中心对齐的隐藏成本，并为通过超越最终层、构建更全球公平和值得信赖的LLMs提供了一条新途径。

Abstract: Confidence calibration, the alignment of a model's predicted confidence with
its actual accuracy, is crucial for the reliable deployment of Large Language
Models (LLMs). However, this critical property remains largely under-explored
in multilingual contexts. In this work, we conduct the first large-scale,
systematic studies of multilingual calibration across six model families and
over 100 languages, revealing that non-English languages suffer from
systematically worse calibration. To diagnose this, we investigate the model's
internal representations and find that the final layer, biased by
English-centric training, provides a poor signal for multilingual confidence.
In contrast, our layer-wise analysis uncovers a key insight that
late-intermediate layers consistently offer a more reliable and
better-calibrated signal. Building on this, we introduce a suite of
training-free methods, including Language-Aware Confidence Ensemble (LACE),
which adaptively selects an optimal ensemble of layers for each specific
language. Our study highlights the hidden costs of English-centric alignment
and offer a new path toward building more globally equitable and trustworthy
LLMs by looking beyond the final layer.

</details>


### [70] [EditLens: Quantifying the Extent of AI Editing in Text](https://arxiv.org/abs/2510.03154)
*Katherine Thai,Bradley Emi,Elyas Masrour,Mohit Iyyer*

Main category: cs.CL

TL;DR: 本研究表明AI编辑过的文本可以与人类原创和AI生成文本区分开来，并提出EditLens模型量化AI编辑的程度，在检测任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 大量查询LLM用于编辑而非从零生成文本，但现有研究主要关注检测完全由AI生成的文本。因此，区分AI编辑文本与人类原创或AI生成文本的需求十分迫切。

Method: 首先，提出并验证轻量级相似性指标来量化AI编辑程度。其次，利用这些相似性指标作为中间监督，训练了一个名为EditLens的回归模型来预测文本中AI编辑的量。最后，通过案例研究分析了Grammarly的AI编辑效果。

Result: EditLens模型在二元（F1=94.7%）和三元（F1=90.4%）分类任务中，区分人类、AI和混合写作方面达到了最先进的性能。研究不仅证明了AI编辑文本可检测，还显示了AI对人类写作所做修改的程度也可以被检测。

Conclusion: AI编辑的文本是可检测的，并且其修改程度也可被量化，这对作者归属、教育和政策制定具有重要意义。研究团队将公开发布模型和数据集以促进后续研究。

Abstract: A significant proportion of queries to large language models ask them to edit
user-provided text, rather than generate new text from scratch. While previous
work focuses on detecting fully AI-generated text, we demonstrate that
AI-edited text is distinguishable from human-written and AI-generated text.
First, we propose using lightweight similarity metrics to quantify the
magnitude of AI editing present in a text given the original human-written text
and validate these metrics with human annotators. Using these similarity
metrics as intermediate supervision, we then train EditLens, a regression model
that predicts the amount of AI editing present within a text. Our model
achieves state-of-the-art performance on both binary (F1=94.7%) and ternary
(F1=90.4%) classification tasks in distinguishing human, AI, and mixed writing.
Not only do we show that AI-edited text can be detected, but also that the
degree of change made by AI to human writing can be detected, which has
implications for authorship attribution, education, and policy. Finally, as a
case study, we use our model to analyze the effects of AI-edits applied by
Grammarly, a popular writing assistance tool. To encourage further research, we
commit to publicly releasing our models and dataset.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [71] [Exploring OCR-augmented Generation for Bilingual VQA](https://arxiv.org/abs/2510.02543)
*JoonHo Lee,Sunho Park*

Main category: cs.CV

TL;DR: 该研究探索了OCR增强的视觉语言模型（VLM）在韩语和英语等多语言任务中的应用，并发布了KLOCR双语OCR基线和KOCRBench韩语VQA基准。实验表明OCR提取文本能显著提升VLM性能。


<details>
  <summary>Details</summary>
Motivation: 探索OCR增强的视觉语言模型（VLM）在韩语和英语等多语言任务中的潜力，并支持该领域的研究，特别是在现有VQA基准不足以满足韩语需求的情况下。

Method: ['训练并发布了KLOCR，一个基于1亿实例的双语OCR基线，用于增强VLM的OCR能力。', '创建了KOCRBench，一个针对韩语视觉问答（VQA）的基准，以补充现有VQA基准。', '分析了不同的提示方法。', '进行了广泛的实验来评估OCR提取文本对VLM性能的影响。']

Result: 实验结果表明，OCR提取的文本能够显著提升开源和商业模型的性能。

Conclusion: 该工作为双语视觉问答（VQA）中OCR增强的生成提供了新的见解。

Abstract: We investigate OCR-augmented generation with Vision Language Models (VLMs),
exploring tasks in Korean and English toward multilingualism. To support
research in this domain, we train and release KLOCR, a strong bilingual OCR
baseline trained on 100M instances to augment VLMs with OCR ability. To
complement existing VQA benchmarks, we curate KOCRBench for Korean VQA, and
analyze different prompting methods. Extensive experiments show that
OCR-extracted text significantly boosts performance across open source and
commercial models. Our work offers new insights into OCR-augmented generation
for bilingual VQA. Model, code, and data are available at
https://github.com/JHLee0513/KLOCR.

</details>


### [72] [Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback](https://arxiv.org/abs/2510.02561)
*Derek Shi,Ruben Glatt,Christine Klymko,Shubham Mohole,Hongjun Choi,Shashank Kushwaha,Sam Sakla,Felipe Leno da Silva*

Main category: cs.CV

TL;DR: 本文提出Oracle-RLAIF，一个新颖且成本效益高的框架，通过使用通用Oracle排序器替代训练好的奖励模型，并引入基于排名的损失函数($GRPO_{rank}$)，实现视频-语言模型(VLM)的强化学习对齐，而非依赖昂贵的评分模型。


<details>
  <summary>Details</summary>
Motivation: 大规模视频-语言模型(VLM)的微调依赖于昂贵的人类偏好反馈。虽然基于AI反馈的强化学习(RLAIF)旨在降低成本，但当前的RLAIF框架仍需训练专门的奖励模型，该模型成本高昂且受限。因此，需要更灵活、数据效率更高的VLM对齐框架。

Method: 本文提出Oracle-RLAIF框架，用更通用的Oracle排序器取代了传统的、经过训练的奖励模型。该排序器直接对候选模型响应进行排名，而非评分。此外，还引入了$GRPO_{rank}$，这是一种基于组相对策略优化(GRPO)的新型基于排名的损失函数，它能够直接优化具有排名感知优势的序数反馈。

Result: 实验结果表明，在各种视频理解基准测试中，Oracle-RLAIF显著优于使用现有微调方法的领先VLM。

Conclusion: Oracle-RLAIF为创建灵活且数据高效的框架铺平了道路，这些框架能够通过基于排名的强化学习，而非基于分数的强化学习，来对齐大型多模态视频模型。

Abstract: Recent advances in large video-language models (VLMs) rely on extensive
fine-tuning techniques that strengthen alignment between textual and visual
comprehension. Leading pipelines typically pair supervised fine-tuning (SFT)
with reinforcement learning from preference data to enhance video
comprehension. However, as VLMs scale in parameter size, so does the cost of
gathering enough human feedback. To make fine-tuning more cost-effective,
recent frameworks explore reinforcement learning with AI feedback (RLAIF),
which replace human preference with AI as a judge. Current RLAIF frameworks
rely on a specialized reward model trained with video narratives to create
calibrated scalar rewards -- an expensive and restrictive pipeline. We propose
Oracle-RLAIF, a novel framework that replaces the trained reward model with a
more general Oracle ranker which acts as a drop-in model ranking candidate
model responses rather than scoring them. Alongside Oracle-RLAIF, we introduce
$GRPO_{rank}$, a novel rank-based loss function based on Group Relative Policy
Optimization (GRPO) that directly optimizes ordinal feedback with rank-aware
advantages. Empirically, we demonstrate that Oracle-RLAIF consistently
outperforms leading VLMs using existing fine-tuning methods when evaluated
across various video comprehension benchmarks. Oracle-RLAIF paves the path to
creating flexible and data-efficient frameworks for aligning large multi-modal
video models with reinforcement learning from rank rather than score.

</details>


### [73] [PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction](https://arxiv.org/abs/2510.02566)
*Qiao Feng,Yiming Huang,Yufu Wang,Jiatao Gu,Lingjie Liu*

Main category: cs.CV

TL;DR: PhysHMR是一种统一框架，通过在物理模拟器中学习视觉到动作策略，利用像素即射线策略和知识蒸馏，从单目视频重建高逼真度、物理可信的人体运动。


<details>
  <summary>Details</summary>
Motivation: 现有从单目视频重建人体运动的运动学方法缺乏物理约束，导致结果不真实。之前采用的两阶段方法（运动学估计后物理后处理）存在误差累积问题，限制了重建质量。

Method: 提出PhysHMR统一框架，直接学习视觉到动作策略，在物理模拟器中控制类人生物。采用“像素即射线”策略，将2D关键点提升为3D空间射线作为策略输入，提供稳健的全局姿态指导。结合局部视觉特征，并通过知识蒸馏方案将动作捕捉专家知识转移到视觉条件策略，最后利用物理驱动的强化学习奖励进行细化。

Result: PhysHMR在多种场景下生成了高逼真度、物理可信的运动，在视觉准确性和物理真实性方面均优于现有方法。

Conclusion: PhysHMR成功提供了一个统一的、物理基础且视觉对齐的框架，用于从单目视频重建人体运动，解决了现有运动学方法和两阶段方法的局限性。

Abstract: Reconstructing physically plausible human motion from monocular videos
remains a challenging problem in computer vision and graphics. Existing methods
primarily focus on kinematics-based pose estimation, often leading to
unrealistic results due to the lack of physical constraints. To address such
artifacts, prior methods have typically relied on physics-based post-processing
following the initial kinematics-based motion estimation. However, this
two-stage design introduces error accumulation, ultimately limiting the overall
reconstruction quality. In this paper, we present PhysHMR, a unified framework
that directly learns a visual-to-action policy for humanoid control in a
physics-based simulator, enabling motion reconstruction that is both physically
grounded and visually aligned with the input video. A key component of our
approach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial
rays and transforms them into global space. These rays are incorporated as
policy inputs, providing robust global pose guidance without depending on noisy
3D root predictions. This soft global grounding, combined with local visual
features from a pretrained encoder, allows the policy to reason over both
detailed pose and global positioning. To overcome the sample inefficiency of
reinforcement learning, we further introduce a distillation scheme that
transfers motion knowledge from a mocap-trained expert to the
vision-conditioned policy, which is then refined using physically motivated
reinforcement learning rewards. Extensive experiments demonstrate that PhysHMR
produces high-fidelity, physically plausible motion across diverse scenarios,
outperforming prior approaches in both visual accuracy and physical realism.

</details>


### [74] [Unlocking the power of partnership: How humans and machines can work together to improve face recognition](https://arxiv.org/abs/2510.02570)
*P. Jonathon Phillips,Geraldine Jeckeln,Carina A. Hahn,Amy N. Yates,Peter C. Fontana,Alice J. O'Toole*

Main category: cs.CV

TL;DR: 研究发现，在面部识别中，人机协作能提升准确性，尤其通过“近距准确度规则”找到“关键融合区”，并指出“智能人机融合”优于单独机器或全面融合，能有效减少低效人工对系统准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 人脸识别算法的人工复核构成人机协作系统，但人与机器的个体差异会影响准确性。本研究旨在明确在何种情况下，结合人与机器的面部识别决策能提高准确性。

Method: 使用来自专家和非专家面部识别者的数据，考察了人-人及人-机协作的益处。提出了“近距准确度规则（PAR）”来预测协作效益，并据此建立了“关键融合区”。通过选择有潜力提升机器准确性的人员，实现了“智能人机融合”。对纯人类协作系统，使用图论确定了最高准确性。

Result: 协作效益随着协作方基线准确度差异的减小而增加（遵循PAR）。PAR预测了各种基线能力下的协作（融合）效益。研究发现一个“关键融合区”，在该区内人类虽不如机器准确，但融合两者能提升系统准确性，且该区域出奇地大。“智能人机融合”比机器单独操作和结合所有判断更准确。纯人类合作所能达到的最高准确性与智能人机协作的平均性能相近，但智能人机协作能更有效地降低低效人工对系统整体准确性的影响。

Conclusion: 研究结果表明，在确保面部识别准确性方面，人类和机器都扮演着重要角色。本研究为AI在面部识别中的智能应用提供了基于证据的路线图。

Abstract: Human review of consequential decisions by face recognition algorithms
creates a "collaborative" human-machine system. Individual differences between
people and machines, however, affect whether collaboration improves or degrades
accuracy in any given case. We establish the circumstances under which
combining human and machine face identification decisions improves accuracy.
Using data from expert and non-expert face identifiers, we examined the
benefits of human-human and human-machine collaborations. The benefits of
collaboration increased as the difference in baseline accuracy between
collaborators decreased-following the Proximal Accuracy Rule (PAR). This rule
predicted collaborative (fusion) benefit across a wide range of baseline
abilities, from people with no training to those with extensive training. Using
the PAR, we established a critical fusion zone, where humans are less accurate
than the machine, but fusing the two improves system accuracy. This zone was
surprisingly large. We implemented "intelligent human-machine fusion" by
selecting people with the potential to increase the accuracy of a
high-performing machine. Intelligent fusion was more accurate than the machine
operating alone and more accurate than combining all human and machine
judgments. The highest system-wide accuracy achievable with human-only
partnerships was found by graph theory. This fully human system approximated
the average performance achieved by intelligent human-machine collaboration.
However, intelligent human-machine collaboration more effectively minimized the
impact of low-performing humans on system-wide accuracy. The results
demonstrate a meaningful role for both humans and machines in assuring accurate
face identification. This study offers an evidence-based road map for the
intelligent use of AI in face identification.

</details>


### [75] [How Confident are Video Models? Empowering Video Models to Express their Uncertainty](https://arxiv.org/abs/2510.02571)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 本文提出了首个针对生成式视频模型的不确定性量化(UQ)框架S-QUBED，以解决模型幻觉问题，并能有效分解不确定性。


<details>
  <summary>Details</summary>
Motivation: 生成式视频模型在实际应用中广泛采用，但与大型语言模型类似，存在“幻觉”现象，会生成事实错误的视频。目前缺乏针对视频模型的不确定性量化(UQ)方法，这带来了关键安全隐患。

Method: 本文提出了一个生成式视频模型不确定性量化框架，包括：(i) 基于鲁棒秩相关估计的视频模型校准评估指标；(ii) 一个名为S-QUBED的黑盒UQ方法，它利用潜在空间建模将预测不确定性分解为偶然不确定性(aleatoric)和认知不确定性(epistemic)；(iii) 一个UQ数据集，用于基准测试视频模型的校准性。通过在潜在空间中调整生成任务，区分了因模糊任务规范和缺乏知识引起的不确定性。

Result: 通过广泛实验，S-QUBED计算的校准总不确定性估计与任务准确性呈负相关，并能有效计算偶然不确定性和认知不确定性成分。

Conclusion: 本研究首次为生成式视频模型引入了不确定性量化方法，通过S-QUBED框架成功量化了模型的不确定性，并将其分解为不同成分，有助于提高视频模型的安全性和可靠性。

Abstract: Generative video models demonstrate impressive text-to-video capabilities,
spurring widespread adoption in many real-world applications. However, like
large language models (LLMs), video generation models tend to hallucinate,
producing plausible videos even when they are factually wrong. Although
uncertainty quantification (UQ) of LLMs has been extensively studied in prior
work, no UQ method for video models exists, raising critical safety concerns.
To our knowledge, this paper represents the first work towards quantifying the
uncertainty of video models. We present a framework for uncertainty
quantification of generative video models, consisting of: (i) a metric for
evaluating the calibration of video models based on robust rank correlation
estimation with no stringent modeling assumptions; (ii) a black-box UQ method
for video models (termed S-QUBED), which leverages latent modeling to
rigorously decompose predictive uncertainty into its aleatoric and epistemic
components; and (iii) a UQ dataset to facilitate benchmarking calibration in
video models. By conditioning the generation task in the latent space, we
disentangle uncertainty arising due to vague task specifications from that
arising from lack of knowledge. Through extensive experiments on benchmark
video datasets, we demonstrate that S-QUBED computes calibrated total
uncertainty estimates that are negatively correlated with the task accuracy and
effectively computes the aleatoric and epistemic constituents.

</details>


### [76] [PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization](https://arxiv.org/abs/2510.02599)
*Hovhannes Margaryan,Bo Wan,Tinne Tuytelaars*

Main category: cs.CV

TL;DR: 本文提出了一种名为PEO的训练无关且骨干模型无关的方法，通过优化简单提示词的文本嵌入来显著提升预训练文生图扩散模型生成的图像美学质量。


<details>
  <summary>Details</summary>
Motivation: 在给定简单提示词时，提升预训练文生图扩散模型生成图像的视觉和美学质量。

Method: 该方法名为Prompt Embedding Optimization (PEO)，它利用预训练的文生图扩散模型作为骨干，通过优化给定简单未修饰提示词的文本嵌入来增强生成图像的视觉质量。PEO采用一个三方目标函数，旨在同时提高生成图像的美学保真度、确保与优化文本嵌入的一致性，并通过提示词保留项最小化与初始提示词的偏离。该方法无需训练，且与骨干模型无关。

Result: 定量和定性评估均证实了所提出方法的有效性，其性能超越或与现有最先进的文生图及提示词适应方法相当。

Conclusion: PEO是一种高效、无需训练且与骨干模型无关的方法，能够显著提高预训练文生图扩散模型在简单提示词下的图像美学质量，并表现出领先或等同于当前最先进方法的性能。

Abstract: This paper introduces a novel approach to aesthetic quality improvement in
pre-trained text-to-image diffusion models when given a simple prompt. Our
method, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained
text-to-image diffusion model as a backbone and optimizes the text embedding of
a given simple and uncurated prompt to enhance the visual quality of the
generated image. We achieve this by a tripartite objective function that
improves the aesthetic fidelity of the generated image, ensures adherence to
the optimized text embedding, and minimal divergence from the initial prompt.
The latter is accomplished through a prompt preservation term. Additionally,
PEO is training-free and backbone-independent. Quantitative and qualitative
evaluations confirm the effectiveness of the proposed method, exceeding or
equating the performance of state-of-the-art text-to-image and prompt
adaptation methods.

</details>


### [77] [Ego-Exo 3D Hand Tracking in the Wild with a Mobile Multi-Camera Rig](https://arxiv.org/abs/2510.02601)
*Patrick Rim,Kun He,Kevin Harris,Braden Copple,Shangchen Han,Sizhe An,Ivan Shugurov,Tomas Hodan,He Wen,Xu Xie*

Main category: cs.CV

TL;DR: 本研究介绍了一种新型无标记多摄像头系统，结合背负式设备和VR头显，用于在真实无约束环境中捕获精确的3D手部和物体交互数据。该系统能生成高质量真值数据，显著平衡了环境真实性与3D标注精度。


<details>
  <summary>Details</summary>
Motivation: 在无约束环境下准确跟踪3D手部及其与世界的交互，是第一人称计算机视觉领域的重大挑战。现有数据集大多在受控实验室环境中采集，限制了环境多样性和模型泛化能力。

Method: 引入了一种新型无标记多摄像头系统。该系统由一个轻量级背负式捕捉设备（包含八个外视摄像头）和一个用户佩戴的Meta Quest 3头显（提供两个第一人称视角）组成。研究设计了一个ego-exo跟踪管线，用于从该系统生成精确的3D手部姿态真值数据，并对生成数据的质量进行了严格评估。

Result: 通过该系统，成功收集了一个包含同步多视角图像和精确3D手部姿态的标注数据集。结果表明，该方法能够显著降低环境真实性与3D标注精度之间的权衡，即在真实、非受控场景下也能获得高精度的3D手部标注。

Conclusion: 本研究成功开发了一种能够在真实、无约束环境中高精度捕获3D手部和物体交互的系统，解决了现有数据集中环境多样性与标注精度难以兼顾的问题，为第一人称计算机视觉领域的模型泛化提供了高质量的真值数据和新的捕捉范式。

Abstract: Accurate 3D tracking of hands and their interactions with the world in
unconstrained settings remains a significant challenge for egocentric computer
vision. With few exceptions, existing datasets are predominantly captured in
controlled lab setups, limiting environmental diversity and model
generalization. To address this, we introduce a novel marker-less multi-camera
system designed to capture precise 3D hands and objects, which allows for
nearly unconstrained mobility in genuinely in-the-wild conditions. We combine a
lightweight, back-mounted capture rig with eight exocentric cameras, and a
user-worn Meta Quest 3 headset, which contributes two egocentric views. We
design an ego-exo tracking pipeline to generate accurate 3D hand pose ground
truth from this system, and rigorously evaluate its quality. By collecting an
annotated dataset featuring synchronized multi-view images and precise 3D hand
poses, we demonstrate the capability of our approach to significantly reduce
the trade-off between environmental realism and 3D annotation accuracy.

</details>


### [78] [Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation](https://arxiv.org/abs/2510.02617)
*Beijia Lu,Ziyi Chen,Jing Xiao,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 本文通过利用输入人体姿态引导的稀疏注意力和输入感知的蒸馏损失，成功将多步扩散模型蒸馏为少步学生模型，实现了高质量的实时语音驱动视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在语音驱动视频合成方面因去噪步数多和注意力机制开销大而速度缓慢，难以实时部署。直接应用现有扩散蒸馏方法会导致视频质量下降且仍无法满足实时性要求。

Method: ['将多步扩散视频模型蒸馏为少步学生模型。', '引入新颖的视频蒸馏方法，利用输入人体姿态条件来指导注意力机制和损失函数。', '提出“输入感知稀疏注意力”（input-aware sparse attention），利用人体姿态关键点引导注意力到相关区域（如面部、手部、上半身），以减少冗余计算并增强身体部位的时间一致性，提高推理效率和运动连贯性。', '引入“输入感知蒸馏损失”（input-aware distillation loss）以提升视觉质量，特别是改进唇部同步和手部动作真实感。']

Result: ['实现了实时性能。', '相较于现有的语音驱动和输入驱动方法，显著提升了视觉质量。', '通过广泛实验证明了算法设计选择的有效性。']

Conclusion: 通过整合输入感知稀疏注意力和蒸馏损失，本方法有效解决了现有扩散模型在语音驱动视频合成中速度慢和质量下降的问题，实现了实时高性能与高视觉质量的统一。

Abstract: Diffusion models can synthesize realistic co-speech video from audio for
various applications, such as video creation and virtual agents. However,
existing diffusion-based methods are slow due to numerous denoising steps and
costly attention mechanisms, preventing real-time deployment. In this work, we
distill a many-step diffusion video model into a few-step student model.
Unfortunately, directly applying recent diffusion distillation methods degrades
video quality and falls short of real-time performance. To address these
issues, our new video distillation method leverages input human pose
conditioning for both attention and loss functions. We first propose using
accurate correspondence between input human pose keypoints to guide attention
to relevant regions, such as the speaker's face, hands, and upper body. This
input-aware sparse attention reduces redundant computations and strengthens
temporal correspondences of body parts, improving inference efficiency and
motion coherence. To further enhance visual quality, we introduce an
input-aware distillation loss that improves lip synchronization and hand motion
realism. By integrating our input-aware sparse attention and distillation loss,
our method achieves real-time performance with improved visual quality compared
to recent audio-driven and input-driven methods. We also conduct extensive
experiments showing the effectiveness of our algorithmic design choices.

</details>


### [79] [Deep Generative Continual Learning using Functional LoRA: FunLoRA](https://arxiv.org/abs/2510.02631)
*Victor Enescu,Hichem Sahbi*

Main category: cs.CV

TL;DR: 为解决深度生成模型持续适应中的灾难性遗忘问题，本文提出FunLoRA（一种基于LoRA的参数高效微调方法），它能有效避免遗忘，并在性能和效率上超越现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型在持续适应（增量训练）中面临灾难性遗忘，难以有效整合新知识。现有策略（在合成数据上再训练）存在训练时间过长和因合成数据质量不足导致长期性能下降的局限性。

Method: 提出了一种新颖且更具表达力的生成模型条件化机制——功能性LoRA（FunLoRA）。该方法基于低秩适应（LoRA），独家采用秩1矩阵，并通过精心选择的函数功能性地增加重参数化矩阵的秩。FunLoRA能有效避免灾难性遗忘，并仅需对当前任务数据进行训练。

Result: 在从头训练的流匹配模型上进行的大量实验表明，所提出的FunLoRA（一种参数高效微调方法）在分类准确度上超越了基于扩散模型的现有最先进方法，同时仅需极少的内存成本和采样时间。

Conclusion: FunLoRA为深度生成模型的持续适应提供了一个高效且高性能的解决方案，有效解决了灾难性遗忘问题，同时在性能和资源效率方面均优于现有方法。

Abstract: Continual adaptation of deep generative models holds tremendous potential and
critical importance, given their rapid and expanding usage in text and vision
based applications. Incremental training, however, remains highly challenging
due to catastrophic forgetting phenomenon, which makes it difficult for neural
networks to effectively incorporate new knowledge. A common strategy consists
in retraining the generative model on its own synthetic data in order to
mitigate forgetting. Yet, such an approach faces two major limitations: (i) the
continually increasing training time eventually becomes intractable, and (ii)
reliance on synthetic data inevitably leads to long-term performance
degradation, since synthetic samples lack the richness of real training data.
In this paper, we attenuate these issues by designing a novel and more
expressive conditioning mechanism for generative models based on low rank
adaptation (LoRA), that exclusively employs rank 1 matrices, whose
reparametrized matrix rank is functionally increased using carefully selected
functions -- and dubbed functional LoRA: FunLoRA. Using this dynamic
conditioning, the generative model is guaranteed to avoid catastrophic
forgetting and needs only to be trained on data from the current task.
Extensive experiments using flow-matching based models trained from scratch,
showcase that our proposed parameter-efficient fine-tuning (PEFT) method
surpasses prior state-of-the-art results based on diffusion models, reaching
higher classification accuracy scores, while only requiring a fraction of the
memory cost and sampling time.

</details>


### [80] [Sequence-Preserving Dual-FoV Defense for Traffic Sign and Light Recognition in Autonomous Vehicles](https://arxiv.org/abs/2510.02642)
*Abhishek Joshi,Jahnavi Krishna Koda,Abhishek Phadke*

Main category: cs.CV

TL;DR: 本研究提出一个双视野、序列保留的鲁棒性框架，结合三层防御堆栈和时序投票，以提高自动驾驶车辆在数字攻击和自然退化（如眩光、雨水、污垢）下的交通灯和标志识别能力。


<details>
  <summary>Details</summary>
Motivation: 交通灯和标志识别对自动驾驶车辆至关重要，但现有模型易受数字对抗性攻击和自然扰动（如眩光、雨水、污垢、涂鸦）影响，导致危险误分类。现有工作缺乏对时间连续性、多态视场感知以及数字和自然退化鲁棒性的考虑。

Method: 提出了一个基于多源数据集（aiMotive、Udacity、Waymo、自录视频）的双视野、序列保留的鲁棒性框架。对高速公路、夜晚、雨天和城市四种操作设计域（ODDs）的RGB图像中长期序列进行时间对齐。设计了一个统一的三层防御堆栈框架，包括特征挤压、防御性蒸馏和基于熵的异常检测，并辅以序列级时序投票。评估指标包括准确率、攻击成功率（ASR）、风险加权误分类严重性和置信度稳定性。

Result: 统一防御堆栈实现了79.8mAP，并将攻击成功率（ASR）降低到18.2%，优于YOLOv8、YOLOv9和BEVFormer，同时将高风险误分类降低到32%。通过探针捕获确认了物理可转移性。

Conclusion: 所提出的统一防御堆栈框架显著增强了自动驾驶车辆在各种恶劣条件下（包括数字攻击和自然退化）对交通灯和标志识别的鲁棒性，有效降低了误分类风险，提升了系统安全性和性能。

Abstract: Traffic light and sign recognition are key for Autonomous Vehicles (AVs)
because perception mistakes directly influence navigation and safety. In
addition to digital adversarial attacks, models are vulnerable to existing
perturbations (glare, rain, dirt, or graffiti), which could lead to dangerous
misclassifications. The current work lacks consideration of temporal
continuity, multistatic field-of-view (FoV) sensing, and robustness to both
digital and natural degradation. This study proposes a dual FoV,
sequence-preserving robustness framework for traffic lights and signs in the
USA based on a multi-source dataset built on aiMotive, Udacity, Waymo, and
self-recorded videos from the region of Texas. Mid and long-term sequences of
RGB images are temporally aligned for four operational design domains (ODDs):
highway, night, rainy, and urban. Over a series of experiments on a real-life
application of anomaly detection, this study outlines a unified three-layer
defense stack framework that incorporates feature squeezing, defensive
distillation, and entropy-based anomaly detection, as well as sequence-wise
temporal voting for further enhancement. The evaluation measures included
accuracy, attack success rate (ASR), risk-weighted misclassification severity,
and confidence stability. Physical transferability was confirmed using probes
for recapture. The results showed that the Unified Defense Stack achieved
79.8mAP and reduced the ASR to 18.2%, which is superior to YOLOv8, YOLOv9, and
BEVFormer, while reducing the high-risk misclassification to 32%.

</details>


### [81] [Smart-GRPO: Smartly Sampling Noise for Efficient RL of Flow-Matching Models](https://arxiv.org/abs/2510.02654)
*Benjamin Yu,Jackie Liu,Justin Cui*

Main category: cs.CV

TL;DR: 流匹配模型确定性强，不利于强化学习。我们提出Smart-GRPO，优化流匹配模型中的噪声扰动以实现强化学习，有效提升奖励优化和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型在文本到图像生成中表现出色，但其确定性使其不适合强化学习（强化学习对提高图像质量和人类对齐至关重要）。现有引入随机性的方法效率低下且不稳定。

Method: 提出Smart-GRPO，这是首个优化流匹配模型中噪声扰动以实现强化学习的方法。它采用迭代搜索策略，解码候选扰动，通过奖励函数评估，并将噪声分布向更高奖励区域精炼。

Result: 实验证明，与基线方法相比，Smart-GRPO在奖励优化和视觉质量方面都有显著提升。

Conclusion: Smart-GRPO为流匹配框架中实现强化学习提供了一条实用途径，弥合了高效训练与人类对齐生成之间的鸿沟。

Abstract: Recent advancements in flow-matching have enabled high-quality text-to-image
generation. However, the deterministic nature of flow-matching models makes
them poorly suited for reinforcement learning, a key tool for improving image
quality and human alignment. Prior work has introduced stochasticity by
perturbing latents with random noise, but such perturbations are inefficient
and unstable. We propose Smart-GRPO, the first method to optimize noise
perturbations for reinforcement learning in flow-matching models. Smart-GRPO
employs an iterative search strategy that decodes candidate perturbations,
evaluates them with a reward function, and refines the noise distribution
toward higher-reward regions. Experiments demonstrate that Smart-GRPO improves
both reward optimization and visual quality compared to baseline methods. Our
results suggest a practical path toward reinforcement learning in flow-matching
frameworks, bridging the gap between efficient training and human-aligned
generation.

</details>


### [82] [FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min](https://arxiv.org/abs/2510.02691)
*Yibin Zhao,Yihan Pan,Jun Nan,Jianjun Yi*

Main category: cs.CV

TL;DR: FSFSplatter是一种从稀疏自由图像中快速重建表面的新方法，通过集成端到端高斯初始化、相机参数估计和几何增强优化，解决了现有高斯溅射方法对密集校准视图的依赖及稀疏图像重建质量差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有高斯溅射技术虽能实现高质量新视图合成和详细重建，但通常需要密集、校准的视图。从稀疏自由图像重建时，由于有限的重叠和过拟合，常导致表面质量差。

Method: FSFSplatter整合了端到端密集高斯初始化、相机参数估计和几何增强场景优化。具体方法包括：使用大型Transformer编码多视图图像，通过自拆分高斯头部生成密集且几何一致的高斯场景初始化；通过基于贡献度的剪枝消除局部浮点；利用深度和多视图特征监督以及可微分相机参数，在快速优化期间减轻对有限视图的过拟合。

Result: FSFSplatter在广泛使用的DTU和Replica数据集上，性能优于当前最先进的方法。

Conclusion: FSFSplatter提供了一种有效方案，解决了从稀疏自由图像进行高斯溅射重建的挑战，显著提升了表面重建质量，并超越了现有技术水平。

Abstract: Gaussian Splatting has become a leading reconstruction technique, known for
its high-quality novel view synthesis and detailed reconstruction. However,
most existing methods require dense, calibrated views. Reconstructing from free
sparse images often leads to poor surface due to limited overlap and
overfitting. We introduce FSFSplatter, a new approach for fast surface
reconstruction from free sparse images. Our method integrates end-to-end dense
Gaussian initialization, camera parameter estimation, and geometry-enhanced
scene optimization. Specifically, FSFSplatter employs a large Transformer to
encode multi-view images and generates a dense and geometrically consistent
Gaussian scene initialization via a self-splitting Gaussian head. It eliminates
local floaters through contribution-based pruning and mitigates overfitting to
limited views by leveraging depth and multi-view feature supervision with
differentiable camera parameters during rapid optimization. FSFSplatter
outperforms current state-of-the-art methods on widely used DTU and Replica.

</details>


### [83] [MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context](https://arxiv.org/abs/2510.02722)
*Junyu Shi,Yong Sun,Zhiyuan Zhang,Lijiang Liu,Zhengjie Zhang,Yuxin He,Qiang Nie*

Main category: cs.CV

TL;DR: MoGIC是一个统一框架，通过结合意图建模和视觉先验，实现多模态运动生成，解决了现有方法在捕捉意图和精细细节上的不足，并显著提升了运动生成质量和意图理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动运动生成方法未能捕捉动作执行的因果逻辑和人类行为背后的意图，且缺乏视觉基础限制了精细时空细节的精度和个性化。

Method: 提出MoGIC框架，将意图建模和视觉先验整合到多模态运动合成中，通过联合优化多模态条件运动生成和意图预测，揭示潜在人类目标并利用视觉先验。引入自适应范围的混合注意力机制以实现条件标记与运动子序列的有效局部对齐。同时，构建了包含21个高质量运动数据集、长达440小时的Mo440H基准。

Result: 经微调后，MoGIC在HumanML3D和Mo440H数据集上分别将FID降低38.6%和34.6%，在运动描述方面超越了基于LLM的方法，并支持意图预测和视觉条件生成。

Conclusion: MoGIC显著推进了可控运动合成和意图理解领域的发展。

Abstract: Existing text-driven motion generation methods often treat synthesis as a
bidirectional mapping between language and motion, but remain limited in
capturing the causal logic of action execution and the human intentions that
drive behavior. The absence of visual grounding further restricts precision and
personalization, as language alone cannot specify fine-grained spatiotemporal
details. We propose MoGIC, a unified framework that integrates intention
modeling and visual priors into multimodal motion synthesis. By jointly
optimizing multimodal-conditioned motion generation and intention prediction,
MoGIC uncovers latent human goals, leverages visual priors to enhance
generation, and exhibits versatile multimodal generative capability. We further
introduce a mixture-of-attention mechanism with adaptive scope to enable
effective local alignment between conditional tokens and motion subsequences.
To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21
high-quality motion datasets. Experiments show that after finetuning, MoGIC
reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based
methods in motion captioning with a lightweight text head, and further enables
intention prediction and vision-conditioned generation, advancing controllable
motion synthesis and intention understanding. The code is available at
https://github.com/JunyuShi02/MoGIC

</details>


### [84] [From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2510.02732)
*Jianing Chen,Zehao Li,Yujun Cai,Hao Jiang,Shuqin Gao,Honglong Zhao,Tianlu Mao,Yucheng Zhang*

Main category: cs.CV

TL;DR: 本文提出一种运动自适应框架，用于解决单目视频动态3D重建中控制点分配不当的问题。通过将控制点密度与运动复杂性对齐，并引入基于样条的轨迹参数化，显著提升了重建质量和效率。


<details>
  <summary>Details</summary>
Motivation: 动态单目视频3D重建面临从有限视角推断3D运动的模糊性和建模时变场景的计算需求。现有稀疏控制方法按几何分配控制点，导致静态区域冗余和动态区域不足，未能有效匹配控制点分配与运动复杂性。

Method: 该研究提出一个运动自适应框架，将控制点密度与运动复杂性对齐。利用视觉基础模型的语义和运动先验，建立patch-token-node对应关系，并应用运动自适应压缩将控制点集中在动态区域。通过迭代体素化和运动趋势评分实现灵活的表示密度自适应。为捕捉时间演变，引入由2D轨迹初始化的基于样条的轨迹参数化，取代基于MLP的形变场，以获得更平滑的运动表示和更稳定的优化。

Result: 实验结果表明，该方法在重建质量和效率方面均显著优于现有最先进的方法。

Conclusion: 通过运动自适应的控制点密度分配和基于样条的轨迹参数化，该方法有效解决了动态3D重建中的关键挑战，实现了重建质量和效率的显著提升。

Abstract: Dynamic 3D reconstruction from monocular videos remains difficult due to the
ambiguity inferring 3D motion from limited views and computational demands of
modeling temporally varying scenes. While recent sparse control methods
alleviate computation by reducing millions of Gaussians to thousands of control
points, they suffer from a critical limitation: they allocate points purely by
geometry, leading to static redundancy and dynamic insufficiency. We propose a
motion-adaptive framework that aligns control density with motion complexity.
Leveraging semantic and motion priors from vision foundation models, we
establish patch-token-node correspondences and apply motion-adaptive
compression to concentrate control points in dynamic regions while suppressing
redundancy in static backgrounds. Our approach achieves flexible
representational density adaptation through iterative voxelization and motion
tendency scoring, directly addressing the fundamental mismatch between control
point allocation and motion complexity. To capture temporal evolution, we
introduce spline-based trajectory parameterization initialized by 2D tracklets,
replacing MLP-based deformation fields to achieve smoother motion
representation and more stable optimization. Extensive experiments demonstrate
significant improvements in reconstruction quality and efficiency over existing
state-of-the-art methods.

</details>


### [85] [Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising](https://arxiv.org/abs/2510.02733)
*Weimin Yuan,Cai Meng*

Main category: cs.CV

TL;DR: 本文提出Net2Net方法，结合未训练网络和预训练网络的优势，通过去噪正则化（RED）实现对真实世界噪声的有效去除，解决传统方法和纯深度学习方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统去噪方法依赖手工先验，在复杂真实噪声下表现不佳；深度学习方法虽能学习噪声特征，但需要大量标记数据且泛化能力有限。

Method: Net2Net结合了无监督的DIP（Untrained Network）和有监督的预训练模型DRUNet（Pre-trained Network），通过去噪正则化（RED）策略将两者融合。未训练网络适应单张图像的独特噪声，预训练网络则利用大规模数据集学习到的表示，共同提升去噪性能。

Result: 在基准数据集上的大量实验表明，Net2Net方法在真实世界噪声去除方面表现出卓越的性能。

Conclusion: Net2Net的混合框架增强了不同噪声模式下的泛化能力，尤其在训练数据有限的情况下显著提升了去噪性能。

Abstract: Traditional denoising methods for noise removal have largely relied on
handcrafted priors, often perform well in controlled environments but struggle
to address the complexity and variability of real noise. In contrast, deep
learning-based approaches have gained prominence for learning noise
characteristics from large datasets, but these methods frequently require
extensive labeled data and may not generalize effectively across diverse noise
types and imaging conditions. In this paper, we present an innovative method,
termed as Net2Net, that combines the strengths of untrained and pre-trained
networks to tackle the challenges of real-world noise removal. The innovation
of Net2Net lies in its combination of unsupervised DIP and supervised
pre-trained model DRUNet by regularization by denoising (RED). The untrained
network adapts to the unique noise characteristics of each input image without
requiring labeled data, while the pre-trained network leverages learned
representations from large-scale datasets to deliver robust denoising
performance. This hybrid framework enhances generalization across varying noise
patterns and improves performance, particularly in scenarios with limited
training data. Extensive experiments on benchmark datasets demonstrate the
superiority of our method for real-world noise removal.

</details>


### [86] [Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient Multimodal Retrieval](https://arxiv.org/abs/2510.02745)
*Lanyun Zhu,Deyi Ji,Tianrun Chen,Haiyang Wu,Shiqi Wang*

Main category: cs.CV

TL;DR: Retrv-R1是首个R1风格多模态通用检索MLLM，通过信息压缩模块和新训练范式（激活阶段+课程奖励RL），解决了现有方法计算成本高和不稳定性问题，实现了SOTA性能、高效率和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 将DeepSeek-R1等R1风格方法直接应用于检索任务时，面临计算成本高昂（大量token消耗）以及强化学习训练不稳定、结果次优的问题。

Method: 1. 引入信息压缩模块及细节检查机制，以提高计算效率并保留关键信息。2. 提出新的训练范式，包括使用检索定制的合成CoT数据集进行激活，以及采用新颖课程奖励的强化学习（RL）阶段。

Result: Retrv-R1在多个基准和任务上均实现了SOTA（State-of-the-Art）性能，同时展现出高效率和强大的泛化能力。

Conclusion: Retrv-R1通过其创新的模块设计和训练范式，成功克服了将R1风格推理应用于多模态通用检索的挑战，显著提升了检索的准确性、效率和适用性。

Abstract: The success of DeepSeek-R1 demonstrates the immense potential of using
reinforcement learning (RL) to enhance LLMs' reasoning capabilities. This paper
introduces Retrv-R1, the first R1-style MLLM specifically designed for
multimodal universal retrieval, achieving higher performance by employing
step-by-step reasoning to produce more accurate retrieval results. We find that
directly applying the methods of DeepSeek-R1 to retrieval tasks is not
feasible, mainly due to (1) the high computational cost caused by the large
token consumption required for multiple candidates with reasoning processes,
and (2) the instability and suboptimal results when directly applying RL to
train for retrieval tasks. To address these issues, Retrv-R1 introduces an
information compression module with a details inspection mechanism, which
enhances computational efficiency by reducing the number of tokens while
ensuring that critical information for challenging candidates is preserved.
Furthermore, a new training paradigm is proposed, including an activation stage
using a retrieval-tailored synthetic CoT dataset for more effective
optimization, followed by RL with a novel curriculum reward to improve both
performance and efficiency. Incorporating these novel designs, Retrv-R1
achieves SOTA performance, high efficiency, and strong generalization ability,
as demonstrated by experiments across multiple benchmarks and tasks.

</details>


### [87] [Bayesian Test-time Adaptation for Object Recognition and Detection with Vision-language Models](https://arxiv.org/abs/2510.02750)
*Lihua Zhou,Mao Ye,Shuaifeng Li,Nianxin Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: BCA+是一个统一、免训练的测试时自适应（TTA）框架，通过动态缓存和贝叶斯推理适应性更新似然和先验，解决了视觉-语言模型（VLM）在目标识别和检测中面临的分布偏移问题，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLM）在实际分布偏移下性能下降。现有测试时自适应（TTA）方法或计算昂贵（依赖反向传播），或仅关注似然适应而忽略了关键的先验信息，阻碍了实时部署。

Method: BCA+是一个统一、免训练的TTA框架，适用于目标识别和检测。它引入了一个动态缓存，自适应地存储和更新类别嵌入、空间尺度（用于检测），以及从历史预测中推导出的自适应类别先验。该方法将自适应表述为贝叶斯推理问题，通过融合VLM的初始输出和基于缓存的预测来生成最终结果。缓存预测结合了动态更新的似然（衡量特征和尺度相似性）和反映演变类别分布的先验。这种双重自适应机制，结合不确定性引导的融合，无需反向传播，高效地修正模型的语义理解和上下文置信度。

Result: BCA+在识别和检测基准测试中均实现了最先进的性能。由于无需训练和反向传播，该方法具有高效率。

Conclusion: BCA+通过其免训练、贝叶斯推理方法、自适应先验和动态缓存，有效解决了视觉-语言模型在分布偏移下的性能下降问题，为目标识别和检测提供了高效且最先进的测试时自适应解决方案。

Abstract: Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved
remarkable success in object recognition and detection. However, their
performance often degrades under real-world distribution shifts. Test-time
adaptation (TTA) aims to mitigate this issue by adapting models during
inference. Existing methods either rely on computationally expensive
backpropagation, which hinders real-time deployment, or focus solely on
likelihood adaptation, which overlooks the critical role of the prior. Our
prior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for
object recognition by introducing a training-free framework that incorporates
adaptive priors. Building upon this foundation, we now present Bayesian Class
Adaptation plus (BCA+), a unified, training-free framework for TTA for both
object recognition and detection. BCA+ introduces a dynamic cache that
adaptively stores and updates class embeddings, spatial scales (for detection),
and, crucially, adaptive class priors derived from historical predictions. We
formulate adaptation as a Bayesian inference problem, where final predictions
are generated by fusing the initial VLM output with a cache-based prediction.
This cache-based prediction combines a dynamically updated likelihood
(measuring feature and scale similarity) and a prior (reflecting the evolving
class distribution). This dual-adaptation mechanism, coupled with
uncertainty-guided fusion, enables BCA+ to correct both the model's semantic
understanding and its contextual confidence. As a training-free method
requiring no backpropagation, BCA+ is highly efficient. Extensive experiments
demonstrate that BCA+ achieves state-of-the-art performance on both recognition
and detection benchmarks.

</details>


### [88] [Hierarchical Generalized Category Discovery for Brain Tumor Classification in Digital Pathology](https://arxiv.org/abs/2510.02760)
*Matthias Perkonigg,Patrick Rockenschaub,Georg Göbel,Adelheid Wöhrer*

Main category: cs.CV

TL;DR: 本文提出HGCD-BT，一种结合分层聚类和对比学习的广义类别发现新方法，用于脑肿瘤分类，显著提高了识别已知和未知肿瘤类别的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有脑肿瘤分类方法受限于预定义类别，无法识别训练中未出现的肿瘤类型；无监督学习缺乏结合先验知识的能力；半监督方法常假设所有潜在类别均已标记。广义类别发现（GCD）旨在弥合已知和未知类别的分类鸿沟，但脑肿瘤分类存在分层结构，需要更精细的方法。

Method: 引入分层广义类别发现（HGCD-BT），将分层聚类与对比学习相结合。该方法通过引入新颖的半监督分层聚类损失，扩展了基于对比学习的GCD。

Result: 在OpenSRH数据集上的补丁级分类中，HGCD-BT比现有SOTA GCD方法准确率提高了28%，尤其擅长识别未见过的肿瘤类别。此外，在Digital Brain Tumor Atlas数据集上的切片级分类中，也证明了其跨成像模态的泛化能力。

Conclusion: HGCD-BT成功解决了脑肿瘤分类中识别已知和未知类别的问题，通过结合分层结构和对比学习，显著提高了分类性能和泛化能力。

Abstract: Accurate brain tumor classification is critical for intra-operative decision
making in neuro-oncological surgery. However, existing approaches are
restricted to a fixed set of predefined classes and are therefore unable to
capture patterns of tumor types not available during training. Unsupervised
learning can extract general-purpose features, but it lacks the ability to
incorporate prior knowledge from labelled data, and semi-supervised methods
often assume that all potential classes are represented in the labelled data.
Generalized Category Discovery (GCD) aims to bridge this gap by categorizing
both known and unknown classes within unlabelled data. To reflect the
hierarchical structure of brain tumor taxonomies, in this work, we introduce
Hierarchical Generalized Category Discovery for Brain Tumor Classification
(HGCD-BT), a novel approach that integrates hierarchical clustering with
contrastive learning. Our method extends contrastive learning based GCD by
incorporating a novel semi-supervised hierarchical clustering loss. We evaluate
HGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images,
achieving a +28% improvement in accuracy over state-of-the-art GCD methods for
patch-level classification, particularly in identifying previously unseen tumor
categories. Furthermore, we demonstrate the generalizability of HGCD-BT on
slide-level classification of hematoxylin and eosin stained whole-slide images
from the Digital Brain Tumor Atlas, confirming its utility across imaging
modalities.

</details>


### [89] [AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding](https://arxiv.org/abs/2510.02778)
*Xian Zhang,Zexi Wu,Zinuo Li,Hongming Xu,Luqi Gong,Farid Boussaid,Naoufel Werghi,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: 本文提出AdaRD-Key，一个无需训练的查询驱动关键帧采样模块，通过最大化相关性和多样性，并结合自适应门控机制，解决了长视频理解中现有方法采样不当和遗漏关键信息的问题，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 长视频由于其冗长的时间长度和高信息密度，对视觉-语言模型（VLMs）构成了巨大挑战。当前多模态大语言模型（MLLMs）常采用均匀采样，容易错过关键时刻。现有关键帧选择方法要么强制固定时间间隔（可能遗漏重要事件附近的精细线索），要么只强调视觉多样性而忽略查询相关性。

Method: 提出AdaRD-Key，一个无需训练的查询驱动长视频关键帧采样模块。它通过最大化统一的“相关性-多样性最大体积（RD-MV）”目标函数，结合查询条件下的相关性得分和对数行列式多样性分量，以选取信息丰富且非冗余的帧。为处理与视频弱对齐的宽泛查询，AdaRD-Key采用轻量级相关性感知门控机制，在相关性分布表明弱对齐时，无缝切换到仅多样性模式。该流水线无需训练，计算高效（单GPU实时运行），且可即插即用兼容现有VLM。

Result: 在LongVideoBench和Video-MME上的大量实验表明，AdaRD-Key取得了最先进的性能，尤其在长视频理解方面表现突出。

Conclusion: AdaRD-Key通过其新颖的相关性-多样性优化策略和自适应门控机制，有效克服了长视频关键帧采样的挑战，显著提升了VLMs对长视频的理解能力，且具有高效率和广泛的兼容性。

Abstract: Understanding long-form videos remains a significant challenge for
vision--language models (VLMs) due to their extensive temporal length and high
information density. Most current multimodal large language models (MLLMs) rely
on uniform sampling, which often overlooks critical moments, leading to
incorrect responses to queries. In parallel, many keyframe selection approaches
impose rigid temporal spacing: once a frame is chosen, an exclusion window
suppresses adjacent timestamps to reduce redundancy. While effective at
limiting overlap, this strategy frequently misses short, fine-grained cues near
important events. Other methods instead emphasize visual diversity but neglect
query relevance. We propose AdaRD-Key, a training-free keyframe sampling module
for query-driven long-form video understanding. AdaRD-Key maximizes a unified
Relevance--Diversity Max-Volume (RD-MV) objective, combining a
query-conditioned relevance score with a log-determinant diversity component to
yield informative yet non-redundant frames. To handle broad queries with weak
alignment to the video, AdaRD-Key employs a lightweight relevance-aware gating
mechanism; when the relevance distribution indicates weak alignment, the method
seamlessly shifts into a diversity-only mode, enhancing coverage without
additional supervision. Our pipeline is training-free, computationally
efficient (running in real time on a single GPU), and compatible with existing
VLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and
Video-MME demonstrate state-of-the-art performance, particularly on long-form
videos. Code available at https://github.com/Xian867/AdaRD-Key.

</details>


### [90] [Reasoning Riddles: How Explainability Reveals Cognitive Limits in Vision-Language Models](https://arxiv.org/abs/2510.02780)
*Prahitha Movva*

Main category: cs.CV

TL;DR: 本文通过对VLM解决字谜（rebus puzzles）的解释性分析，揭示了其在横向思维挑战中的认知过程和失败模式，并发现推理质量因类别而异，且提示策略对性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）在多模态任务中表现出色，但其在复杂横向思维挑战（如字谜）上的认知过程不透明。现有研究表明VLM在解决字谜时表现不佳，但其潜在的推理过程和失败模式仍未被充分探索。本研究旨在填补这一空白，深入理解VLM如何应对这些复杂挑战。

Method: 本研究通过一项全面的解释性分析来解决上述问题。方法包括：1) 构建一个包含221个字谜的系统化标注数据集，涵盖六个认知类别；2) 开发一个评估框架，将推理质量与答案正确性分离；3) 探索三种提示策略，旨在引发不同类型的解释性过程，以揭示VLM的认知过程。

Result: 研究结果表明：1) VLM的推理质量在不同字谜类别之间存在显著差异；2) 模型在视觉组合方面表现出系统性优势；3) 模型在缺失解释和文化象征方面表现出根本性局限；4) 提示策略显著影响模型的认知方法和问题解决效率；5) 解释性是模型性能不可或缺的一部分，而非事后考虑。

Conclusion: VLM在处理复杂横向思维任务（如字谜）时，其推理质量因任务类别而异，在视觉组合上具有优势，但在缺失解释和文化象征上存在根本性局限。此外，有效的提示策略对模型的认知方法和解决问题的效率具有显著影响，表明解释性是模型性能的关键组成部分。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet their
cognitive processes remain opaque on complex lateral thinking challenges like
rebus puzzles. While recent work has demonstrated these models struggle
significantly with rebus puzzle solving, the underlying reasoning processes and
failure patterns remain largely unexplored. We address this gap through a
comprehensive explainability analysis that moves beyond performance metrics to
understand how VLMs approach these complex lateral thinking challenges. Our
study contributes a systematically annotated dataset of 221 rebus puzzles
across six cognitive categories, paired with an evaluation framework that
separates reasoning quality from answer correctness. We investigate three
prompting strategies designed to elicit different types of explanatory
processes and reveal critical insights into VLM cognitive processes. Our
findings demonstrate that reasoning quality varies dramatically across puzzle
categories, with models showing systematic strengths in visual composition
while exhibiting fundamental limitations in absence interpretation and cultural
symbolism. We also discover that prompting strategy substantially influences
both cognitive approach and problem-solving effectiveness, establishing
explainability as an integral component of model performance rather than a
post-hoc consideration.

</details>


### [91] [OTR: Synthesizing Overlay Text Dataset for Text Removal](https://arxiv.org/abs/2510.02787)
*Jan Zdenek,Wataru Shimoda,Kota Yamaguchi*

Main category: cs.CV

TL;DR: 本文提出一个合成文本移除基准数据集，旨在解决现有数据集（如SCUT-EnsText）在域外泛化和评估准确性方面的局限性，特别是针对非场景文本领域。


<details>
  <summary>Details</summary>
Motivation: 现有文本移除研究主要集中于自然图像中的场景文本移除，但当前数据集存在缺陷，如真实标注伪影、背景过于简单及评估指标不足，阻碍了模型域外泛化和准确评估。

Method: 引入一种合成文本移除基准的方法，通过在复杂背景上使用目标感知放置和视觉-语言模型生成的内容来渲染文本，从而构建新的数据集。

Result: 创建了一个具有干净真实标注和富有挑战性文本移除场景的数据集，适用于除场景文本以外的领域，并已公开发布。

Conclusion: 通过提供一个高质量、多样化且具有挑战性的合成数据集，本文有效解决了现有文本移除基准的不足，有助于提升模型在更广阔领域的泛化能力和评估准确性。

Abstract: Text removal is a crucial task in computer vision with applications such as
privacy preservation, image editing, and media reuse. While existing research
has primarily focused on scene text removal in natural images, limitations in
current datasets hinder out-of-domain generalization or accurate evaluation. In
particular, widely used benchmarks such as SCUT-EnsText suffer from ground
truth artifacts due to manual editing, overly simplistic text backgrounds, and
evaluation metrics that do not capture the quality of generated results. To
address these issues, we introduce an approach to synthesizing a text removal
benchmark applicable to domains other than scene texts. Our dataset features
text rendered on complex backgrounds using object-aware placement and
vision-language model-generated content, ensuring clean ground truth and
challenging text removal scenarios. The dataset is available at
https://huggingface.co/datasets/cyberagent/OTR .

</details>


### [92] [Align Your Query: Representation Alignment for Multimodality Medical Object Detection](https://arxiv.org/abs/2510.02789)
*Ara Seo,Bryan Sangwoo Kim,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: 为解决混合医学模态下目标检测的性能下降问题，本文提出一个轻量级、检测器无关的框架，通过引入模态令牌、多模态上下文注意力和预训练阶段，对DETR风格的对象查询进行模态上下文对齐，从而在多种模态上实现了持续的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当单一检测器在混合医学模态（如CXR、CT、MRI）上训练时，由于异构统计和不相交的表征空间，医学目标检测的性能会受到影响。

Method: 1. **模态令牌定义**: 创建紧凑、文本派生的嵌入，编码成像模态，轻量且无需额外标注。
2. **多模态上下文注意力 (MoCA)**: 通过自注意力机制将模态令牌整合到检测流程中，在查询集中传播模态上下文，以注入模态线索，同时保持DETR架构和极低延迟。
3. **QueryREPA**: 引入一个简短的预训练阶段，使用任务特定的对比目标和模态平衡批次，将查询表征与其模态令牌对齐。

Result: 在多种模态混合训练的场景下，所提出的方法能够持续提高平均精度（AP），同时引入的开销极小且无需修改现有架构。

Conclusion: 该方法为实现稳健的多模态医学目标检测提供了一条实用路径，通过生成模态感知、类别忠实且能有效迁移到下游训练的查询，成功解决了混合模态数据带来的挑战。

Abstract: Medical object detection suffers when a single detector is trained on mixed
medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and
disjoint representation spaces. To address this challenge, we turn to
representation alignment, an approach that has proven effective for bringing
features from different sources into a shared space. Specifically, we target
the representations of DETR-style object queries and propose a simple,
detector-agnostic framework to align them with modality context. First, we
define modality tokens: compact, text-derived embeddings encoding imaging
modality that are lightweight and require no extra annotations. We integrate
the modality tokens into the detection process via Multimodality Context
Attention (MoCA), mixing object-query representations via self-attention to
propagate modality context within the query set. This preserves DETR-style
architectures and adds negligible latency while injecting modality cues into
object queries. We further introduce QueryREPA, a short pretraining stage that
aligns query representations to their modality tokens using a task-specific
contrastive objective with modality-balanced batches. Together, MoCA and
QueryREPA produce modality-aware, class-faithful queries that transfer
effectively to downstream training. Across diverse modalities trained
altogether, the proposed approach consistently improves AP with minimal
overhead and no architectural modifications, offering a practical path toward
robust multimodality medical object detection. Project page:
https://araseo.github.io/alignyourquery/.

</details>


### [93] [MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding](https://arxiv.org/abs/2510.02790)
*Jingyuan Deng,Yujiu Yang*

Main category: cs.CV

TL;DR: 本文提出图像头掩码对比解码（MaskCD）方法，通过掩盖LVLM中的图像头来构建对比样本，有效缓解大型视觉语言模型的幻觉问题，并保留其通用能力。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）在视觉语言理解方面表现卓越，但存在幻觉问题，即生成与输入视觉和文本内容矛盾的内容。现有方法如对比解码和注意力操作，分别面临对比样本构建困难和稳定性差的挑战。

Method: 本研究提出图像头掩码对比解码（MaskCD）方法。该方法利用LVLM中的“图像头”，通过对其进行掩码操作来构建对比样本，并应用于对比解码。

Result: 在LLaVA-1.5-7b和Qwen-VL-7b模型上，通过CHAIR、POPE、AMBER和MME等多个基准测试进行评估。结果表明MaskCD能有效减轻幻觉现象，同时保持LVLMs的通用能力。

Conclusion: MaskCD是一种有效且稳定的方法，能够缓解大型视觉语言模型的幻觉问题，同时不损害其核心能力。

Abstract: Large vision-language models (LVLMs) have shown remarkable performance in
visual-language understanding for downstream multimodal tasks. While their
capabilities are improving, problems emerge simultaneously. Among those
problems, the hallucinations have attracted much attention, which stands for
the phenomenon where LVLMs generate contradictory content to their input visual
and text contents. Many approaches have been proposed to deal with this issue,
such as contrastive decoding and attention manipulation. However, contrastive
decoding methods struggle in constructing appropriate contrastive samples, and
attention manipulation methods are highly sensitive, lacking stability. In this
work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach
utilizes the "image heads" in LVLMs, masking them to construct contrastive
samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and
Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The
results demonstrate that MaskCD effectively alleviates the phenomenon of
hallucinations and retains the general capabilities of LVLMs. Corresponding
resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .

</details>


### [94] [VERNIER: an open-source software pushing marker pose estimation down to the micrometer and nanometer scales](https://arxiv.org/abs/2510.02791)
*Patrick Sandoz,Antoine N. André,Guillaume J. Laurent*

Main category: cs.CV

TL;DR: 本文介绍了一个开源的相位处理软件VERNIER，旨在通过伪周期模式实现小尺度下快速可靠且高分辨率的姿态测量，并展示了其对噪声、散焦和遮挡的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在小尺度下，以纳米级和微弧度分辨率在大范围内捕捉物体6自由度姿态仍然是挑战，现有解决方案较少。

Method: 本文提出了VERNIER，一个开源的相位处理软件，基于伪周期模式提供姿态测量。该软件采用基于相位的局部阈值算法，并展示了相位处理的连续步骤和不同类型的模式。

Result: VERNIER软件对噪声、散焦和遮挡表现出特别的鲁棒性。论文通过合成和实验图像说明了其实现过程。

Conclusion: VERNIER提供了一个强大且高分辨率的微尺度姿态测量解决方案。论文还提供了选择适当模式设计和显微镜放大镜头的指导，以满足期望的性能要求。

Abstract: Pose estimation is still a challenge at the small scales. Few solutions exist
to capture the 6 degrees of freedom of an object with nanometric and
microradians resolutions over relatively large ranges. Over the years, we have
proposed several fiducial marker and pattern designs to achieve reliable
performance for various microscopy applications. Centimeter ranges are possible
using pattern encoding methods, while nanometer resolutions can be achieved
using phase processing of the periodic frames. This paper presents VERNIER, an
open source phase processing software designed to provide fast and reliable
pose measurement based on pseudo-periodic patterns. Thanks to a phase-based
local thresholding algorithm, the software has proven to be particularly robust
to noise, defocus and occlusion. The successive steps of the phase processing
are presented, as well as the different types of patterns that address
different application needs. The implementation procedure is illustrated with
synthetic and experimental images. Finally, guidelines are given for selecting
the appropriate pattern design and microscope magnification lenses as a
function of the desired performance.

</details>


### [95] [Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis](https://arxiv.org/abs/2510.02815)
*Feng Yuan,Yifan Gao,Yuehua Ye,Haoyue Li,Xin Gao*

Main category: cs.CV

TL;DR: 本文提出Med-K2N，一种针对K到N跨模态医学图像合成的方法。它通过学习自适应权重和渐进式增强来解决模态异质贡献、融合质量控制和多输出模态一致性问题，并引入因果模态身份模块（CMIM）。


<details>
  <summary>Details</summary>
Motivation: 临床上对灵活模态重建（K到N生成）的需求日益增长。现有方法面临三个关键挑战：如何建模不同模态对各种目标任务的异质贡献、如何确保融合质量控制以避免噪声信息导致性能下降、以及如何在多输出生成中保持模态身份一致性。

Method: 受SAM2序贯帧范式和临床医生渐进工作流程启发，将多模态医学数据视为具有质量驱动选择机制的序贯帧。核心思想是为每个模态-任务对“学习”自适应权重，并通过渐进式增强“记忆”有益的融合模式。具体设计了三个协作模块：PreWeightNet（全局贡献评估）、ThresholdNet（自适应过滤）和EffiWeightNet（有效权重计算）。此外，为保持模态身份一致性，提出了因果模态身份模块（CMIM），利用视觉-语言建模在生成图像和目标模态描述之间建立因果约束。

Result: 广泛的实验结果表明，所提出的Med-K2N方法在多个基准测试中显著优于现有最先进的方法。

Conclusion: Med-K2N成功解决了K到N跨模态医学图像合成中的关键挑战，通过自适应权重学习、渐进式增强和因果模态身份模块，实现了卓越的生成性能和模态一致性。

Abstract: Cross-modal medical image synthesis research focuses on reconstructing
missing imaging modalities from available ones to support clinical diagnosis.
Driven by clinical necessities for flexible modality reconstruction, we explore
K to N medical generation, where three critical challenges emerge: How can we
model the heterogeneous contributions of different modalities to various target
tasks? How can we ensure fusion quality control to prevent degradation from
noisy information? How can we maintain modality identity consistency in
multi-output generation? Driven by these clinical necessities, and drawing
inspiration from SAM2's sequential frame paradigm and clinicians' progressive
workflow of incrementally adding and selectively integrating multi-modal
information, we treat multi-modal medical data as sequential frames with
quality-driven selection mechanisms. Our key idea is to "learn" adaptive
weights for each modality-task pair and "memorize" beneficial fusion patterns
through progressive enhancement. To achieve this, we design three collaborative
modules: PreWeightNet for global contribution assessment, ThresholdNet for
adaptive filtering, and EffiWeightNet for effective weight computation.
Meanwhile, to maintain modality identity consistency, we propose the Causal
Modality Identity Module (CMIM) that establishes causal constraints between
generated images and target modality descriptions using vision-language
modeling. Extensive experimental results demonstrate that our proposed Med-K2N
outperforms state-of-the-art methods by significant margins on multiple
benchmarks. Source code is available.

</details>


### [96] [ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment](https://arxiv.org/abs/2510.02876)
*Md Zahim Hassan,Md. Osama,Muhammad Ashad Kabir,Md. Saiful Islam,Zannatul Naim*

Main category: cs.CV

TL;DR: 该论文提出了ELMF4EggQ，一个利用多模态外部特征（图像、形状、重量）融合的集成学习框架，用于无损评估鸡蛋等级和新鲜度，并构建了一个公开数据集。


<details>
  <summary>Details</summary>
Motivation: 确保食品安全、维持产品标准和提高商业家禽生产运营效率，需要对鸡蛋质量进行准确、无损的评估。

Method: 引入ELMF4EggQ框架，结合外部图像深度特征（通过预训练CNN模型如ResNet152、DenseNet169提取，经PCA降维）与结构特征（鸡蛋形状、重量）。使用SMOTE进行数据增强，并通过多个机器学习算法分类，最后采用集成投票机制结合最佳分类器预测。构建了一个包含186枚褐壳鸡蛋的公开数据集，其内部质量由专家评估确定。

Result: 多模态方法显著优于仅使用图像或仅使用表格数据（形状和重量）的基线。多模态集成方法在鸡蛋等级分类中达到86.57%的准确率，在新鲜度预测中达到70.83%的准确率。所有代码和数据均已公开。

Conclusion: ELMF4EggQ框架利用外部、非侵入性特征和多模态特征融合，能够有效进行鸡蛋内部质量评估，显著提高了预测准确性。这是首次将机器学习应用于仅使用外部特征进行内部鸡蛋质量评估的研究，并发布了相应标记数据集，促进了该领域的透明度、可复现性和进一步研究。

Abstract: Accurate, non-destructive assessment of egg quality is critical for ensuring
food safety, maintaining product standards, and operational efficiency in
commercial poultry production. This paper introduces ELMF4EggQ, an ensemble
learning framework that employs multimodal feature fusion to classify egg grade
and freshness using only external attributes - image, shape, and weight. A
novel, publicly available dataset of 186 brown-shelled eggs was constructed,
with egg grade and freshness levels determined through laboratory-based expert
assessments involving internal quality measurements, such as yolk index and
Haugh unit. To the best of our knowledge, this is the first study to apply
machine learning methods for internal egg quality assessment using only
external, non-invasive features, and the first to release a corresponding
labeled dataset. The proposed framework integrates deep features extracted from
external egg images with structural characteristics such as egg shape and
weight, enabling a comprehensive representation of each egg. Image feature
extraction is performed using top-performing pre-trained CNN models (ResNet152,
DenseNet169, and ResNet152V2), followed by PCA-based dimensionality reduction,
SMOTE augmentation, and classification using multiple machine learning
algorithms. An ensemble voting mechanism combines predictions from the
best-performing classifiers to enhance overall accuracy. Experimental results
demonstrate that the multimodal approach significantly outperforms image-only
and tabular (shape and weight) only baselines, with the multimodal ensemble
approach achieving 86.57% accuracy in grade classification and 70.83% in
freshness prediction. All code and data are publicly available at
https://github.com/Kenshin-Keeps/Egg_Quality_Prediction_ELMF4EggQ, promoting
transparency, reproducibility, and further research in this domain.

</details>


### [97] [One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework](https://arxiv.org/abs/2510.02898)
*Lorenzo Bianchi,Giacomo Pacini,Fabio Carrara,Nicola Messina,Giuseppe Amato,Fabrizio Falchi*

Main category: cs.CV

TL;DR: 本文提出rameworkName{}，一个统一的零样本图像区域描述框架，通过将图像分解为补丁作为原子描述单元，实现对任意区域（从单个补丁到整个图像）的无监督描述，并在多个区域描述任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本图像描述模型仅限于使用全局图像表示并生成整体图像描述，无法处理图像中的任意区域描述，且需要区域级别的监督。

Method: 我们提出了一个从以图像为中心转向以补丁为中心的零样本描述框架rameworkName{}。该方法将单个图像补丁视为原子描述单元，并通过聚合这些补丁来描述从单个补丁到不连续区域乃至整个图像的任意区域，而无需区域级别监督。我们还分析了使现有潜在描述器在新框架中工作的关键要素。

Result: 实验表明，能生成有意义、密集视觉特征的骨干网络（如DINO）是实现最先进性能的关键。在零样本密集描述、区域集描述以及新引入的轨迹描述任务上，我们的模型相比现有基线和最先进方法表现出更好的性能。

Conclusion: 补丁级语义表示对于可扩展的描述生成是有效的，且所提出的框架在零样本图像区域描述任务中取得了显著的SOTA性能。

Abstract: Zero-shot captioners are recently proposed models that utilize common-space
vision-language representations to caption images without relying on paired
image-text data. To caption an image, they proceed by textually decoding a
text-aligned image feature, but they limit their scope to global
representations and whole-image captions. We present \frameworkName{}, a
unified framework for zero-shot captioning that shifts from an image-centric to
a patch-centric paradigm, enabling the captioning of arbitrary regions without
the need of region-level supervision. Instead of relying on global image
representations, we treat individual patches as atomic captioning units and
aggregate them to describe arbitrary regions, from single patches to
non-contiguous areas and entire images. We analyze the key ingredients that
enable current latent captioners to work in our novel proposed framework.
Experiments demonstrate that backbones producing meaningful, dense visual
features, such as DINO, are key to achieving state-of-the-art performance in
multiple region-based captioning tasks. Compared to other baselines and
state-of-the-art competitors, our models achieve better performance on
zero-shot dense, region-set, and a newly introduced trace captioning task,
highlighting the effectiveness of patch-wise semantic representations for
scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .

</details>


### [98] [Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement](https://arxiv.org/abs/2205.03569)
*Bing Li,Jiaxin Chen,Dongming Zhang,Xiuguo Bao,Di Huang*

Main category: cs.CV

TL;DR: 本文提出MEACI-Net框架，通过增强运动表示和实现跨模态交互，解决了压缩视频动作识别中动态信息粗糙、噪声多以及异构模态融合不足的问题，并在多个基准测试中表现出有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 压缩视频动作识别面临两个主要挑战：1. 稀疏采样帧和压缩运动线索导致的动态信息粗糙且噪声多；2. RGB和运动等异构模态融合不充分。

Method: 提出MEACI-Net框架，采用双流架构。运动流使用嵌入去噪模块的多尺度块增强表示学习。通过选择性运动补充（SMC）模块（用时空注意力局部运动特征补充RGB模态）和跨模态增强（CMA）模块（选择性特征增强融合双模态）来加强两流之间的交互。

Result: 在UCF-101、HMDB-51和Kinetics-400基准测试上的大量实验证明了MEACI-Net的有效性和效率。

Conclusion: MEACI-Net通过其创新的运动增强和跨模态交互机制，有效解决了压缩视频动作识别中的关键问题，提升了性能。

Abstract: Compressed video action recognition has recently drawn growing attention,
since it remarkably reduces the storage and computational cost via replacing
raw videos by sparsely sampled RGB frames and compressed motion cues (e.g.,
motion vectors and residuals). However, this task severely suffers from the
coarse and noisy dynamics and the insufficient fusion of the heterogeneous RGB
and motion modalities. To address the two issues above, this paper proposes a
novel framework, namely Attentive Cross-modal Interaction Network with Motion
Enhancement (MEACI-Net). It follows the two-stream architecture, i.e. one for
the RGB modality and the other for the motion modality. Particularly, the
motion stream employs a multi-scale block embedded with a denoising module to
enhance representation learning. The interaction between the two streams is
then strengthened by introducing the Selective Motion Complement (SMC) and
Cross-Modality Augment (CMA) modules, where SMC complements the RGB modality
with spatio-temporally attentive local motion features and CMA further combines
the two modalities with selective feature augmentation. Extensive experiments
on the UCF-101, HMDB-51 and Kinetics-400 benchmarks demonstrate the
effectiveness and efficiency of MEACI-Net.

</details>


### [99] [Training-Free Out-Of-Distribution Segmentation With Foundation Models](https://arxiv.org/abs/2510.02909)
*Laith Nayal,Hadi Salloum,Ahmad Taha,Yaroslav Kholodov,Alexander Gasnikov*

Main category: cs.CV

TL;DR: 本文提出一种训练无关的方法，利用InternImage骨干网络的特征，结合K-Means聚类和置信度阈值法，来检测语义分割中的OOD区域，并在RoadAnomaly和ADE-OoD基准测试中取得了超越基线的性能。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶等安全关键应用中，语义分割中的未知物体检测至关重要。虽然大型视觉基础模型在闭集语义任务中表现出色，但它们在语义分割中检测分布外（OoD）区域的能力仍未充分探索。

Method: 提出一种简单、无需训练的方法。该方法利用InternImage骨干网络的特征，并在原始解码器logits上应用K-Means聚类和置信度阈值处理来识别OoD聚类，从而区分内部分布（ID）和OoD区域，无需离群值监督。

Result: 该方法在RoadAnomaly基准测试中获得了50.02的平均精度，在ADE-OoD基准测试中获得了48.77的平均精度（使用InternImage-L），超过了多个有监督和无监督的基线方法。

Conclusion: 研究结果表明，基础模型为通用OoD分割方法提供了一个有前景的方向，这些方法只需最少的假设或额外数据。

Abstract: Detecting unknown objects in semantic segmentation is crucial for
safety-critical applications such as autonomous driving. Large vision
foundation models, including DINOv2, InternImage, and CLIP, have advanced
visual representation learning by providing rich features that generalize well
across diverse tasks. While their strength in closed-set semantic tasks is
established, their capability to detect out-of-distribution (OoD) regions in
semantic segmentation remains underexplored. In this work, we investigate
whether foundation models fine-tuned on segmentation datasets can inherently
distinguish in-distribution (ID) from OoD regions without any outlier
supervision. We propose a simple, training-free approach that utilizes features
from the InternImage backbone and applies K-Means clustering alongside
confidence thresholding on raw decoder logits to identify OoD clusters. Our
method achieves 50.02 Average Precision on the RoadAnomaly benchmark and 48.77
on the benchmark of ADE-OoD with InternImage-L, surpassing several supervised
and unsupervised baselines. These results suggest a promising direction for
generic OoD segmentation methods that require minimal assumptions or additional
data.

</details>


### [100] [Don't Just Chase "Highlighted Tokens" in MLLMs: Revisiting Visual Holistic Context Retention](https://arxiv.org/abs/2510.02912)
*Xin Zou,Di Lu,Yizhou Wang,Yibo Yan,Yuanhuiyi Lyu,Xu Zheng,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 为解决多模态大语言模型（MLLMs）因大量视觉token导致的计算开销问题，本文提出了HoloV，一个即插即用的视觉token剪枝框架。它通过从全局视角自适应分配剪枝预算，有效保留视觉上下文，克服了现有方法的局限性，实现了显著的效率提升和高准确性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）虽然功能强大，但因依赖大量视觉token而存在巨大的计算开销。现有基于注意力的token剪枝方法存在局限性，在高剪枝率下容易保留语义相似的token，导致性能显著下降。

Method: 本文提出HoloV框架，一种即插即用的视觉token剪枝方案。与传统的注意力优先剪枝不同，HoloV从全局视角重新思考token保留，通过自适应地将剪枝预算分配到不同的空间裁剪区域，确保保留的token能捕获全局视觉上下文，而非孤立的显著特征，以此最小化表征崩溃并维护任务相关信息。

Result: 实验结果表明，HoloV在各种任务、MLLM架构和剪枝率下均优于现有SOTA方法。例如，搭载HoloV的LLaVA1.5在剪枝88.9%的视觉token后，仍能保持95.8%的原始性能，实现了卓越的效率-准确性权衡。

Conclusion: HoloV是一个简单、有效且即插即用的视觉token剪枝框架，它通过独特的全局上下文保留策略，成功解决了传统剪枝方法在高剪枝率下的性能下降问题，显著提高了MLLM的推理效率，同时保持了优异的性能。

Abstract: Despite their powerful capabilities, Multimodal Large Language Models (MLLMs)
suffer from considerable computational overhead due to their reliance on
massive visual tokens. Recent studies have explored token pruning to alleviate
this problem, which typically uses text-vision cross-attention or
[\texttt{CLS}] attention to assess and discard redundant visual tokens. In this
work, we identify a critical limitation of such attention-first pruning
approaches, i.e., they tend to preserve semantically similar tokens, resulting
in pronounced performance drops under high pruning ratios. To this end, we
propose {HoloV}, a simple yet effective, plug-and-play visual token pruning
framework for efficient inference. Distinct from previous attention-first
schemes, HoloV rethinks token retention from a holistic perspective. By
adaptively distributing the pruning budget across different spatial crops,
HoloV ensures that the retained tokens capture the global visual context rather
than isolated salient features. This strategy minimizes representational
collapse and maintains task-relevant information even under aggressive pruning.
Experimental results demonstrate that our HoloV achieves superior performance
across various tasks, MLLM architectures, and pruning ratios compared to SOTA
methods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\% of the
original performance after pruning 88.9\% of visual tokens, achieving superior
efficiency-accuracy trade-offs.

</details>


### [101] [Zero-Shot Robustness of Vision Language Models Via Confidence-Aware Weighting](https://arxiv.org/abs/2510.02913)
*Nikoo Naghavian,Mostafa Tavassolipour*

Main category: cs.CV

TL;DR: CLIP等视觉语言模型易受对抗攻击。本文提出CAW方法，通过置信度感知损失和特征对齐正则化，提高其零样本鲁棒性，并在多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（如CLIP）虽表现出色的零样本泛化能力，但极易受到对抗性攻击。

Method: 提出Confidence-Aware Weighting (CAW) 方法，包括两个组件：1) 置信度感知损失，通过缩放干净和对抗预测之间的KL散度来优先处理不确定的对抗样本；2) 特征对齐正则化，通过最小化冻结和微调图像编码器在对抗输入上的特征距离来保持语义一致性。

Result: 在TinyImageNet和14个额外数据集上的广泛实验表明，CAW在AutoAttack等强攻击下，优于PMG-AFT和TGA-ZSR等近期方法，且内存使用更少。

Conclusion: CAW方法能有效提高视觉语言模型的零样本鲁棒性，同时改善干净和鲁棒准确度而不牺牲泛化能力，并在性能和效率上超越了现有方法。

Abstract: Vision-language models like CLIP demonstrate impressive zero-shot
generalization but remain highly vulnerable to adversarial attacks. In this
work, we propose Confidence-Aware Weighting (CAW) to enhance zero-shot
robustness in vision-language models. CAW consists of two components: (1) a
Confidence-Aware loss that prioritizes uncertain adversarial examples by
scaling the KL divergence between clean and adversarial predictions, and (2) a
feature alignment regularization that preserves semantic consistency by
minimizing the distance between frozen and fine-tuned image encoder features on
adversarial inputs. These components work jointly to improve both clean and
robust accuracy without sacrificing generalization. Extensive experiments on
TinyImageNet and 14 additional datasets show that CAW outperforms recent
methods such as PMG-AFT and TGA-ZSR under strong attacks like AutoAttack, while
using less memory.

</details>


### [102] [Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights](https://arxiv.org/abs/2510.02922)
*Daphne Tsolissou,Theofanis Ganitidis,Konstantinos Mitsis,Stergios CHristodoulidis,Maria Vakalopoulou,Konstantina Nikita*

Main category: cs.CV

TL;DR: 利用大型视觉-语言模型（LVLMs）结合超声图像与多模态临床数据评估颈动脉粥样硬化风险。通过领域适应和数据整合，显著提升了风险分层性能，超越了传统CNN基线。


<details>
  <summary>Details</summary>
Motivation: 颈动脉粥样硬化风险评估需整合多源信息且具备透明性和可解释性，是当前临床的重大挑战。

Method: 研究利用LVLMs整合超声图像及结构化临床、人口统计、实验室和生物标志物数据进行多模态评估。提出模拟诊断场景的框架，比较多款LVLMs。通过低秩适应（LoRA）对LLaVa-NeXT-Vicuna进行超声领域适应。将多模态表格数据以文本形式整合。

Result: 零样本实验表明，LVLMs识别图像模态和解剖结构能力不一，且在精确风险分类方面表现不佳。经LoRA域适应后，卒中风险分层能力显著提高。整合多模态文本数据进一步提升了特异性和平衡准确度，性能超越了相同数据集上的CNN基线。

Conclusion: LVLMs在基于超声的心血管风险预测中展现出前景和局限，强调了多模态整合、模型校准和领域适应对临床转化的重要性。

Abstract: Reliable risk assessment for carotid atheromatous disease remains a major
clinical challenge, as it requires integrating diverse clinical and imaging
information in a manner that is transparent and interpretable to clinicians.
This study investigates the potential of state-of-the-art and recent large
vision-language models (LVLMs) for multimodal carotid plaque assessment by
integrating ultrasound imaging (USI) with structured clinical, demographic,
laboratory, and protein biomarker data. A framework that simulates realistic
diagnostic scenarios through interview-style question sequences is proposed,
comparing a range of open-source LVLMs, including both general-purpose and
medically tuned models. Zero-shot experiments reveal that even if they are very
powerful, not all LVLMs can accurately identify imaging modality and anatomy,
while all of them perform poorly in accurate risk classification. To address
this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using
low-rank adaptation (LoRA), resulting in substantial improvements in stroke
risk stratification. The integration of multimodal tabular data in the form of
text further enhances specificity and balanced accuracy, yielding competitive
performance compared to prior convolutional neural network (CNN) baselines
trained on the same dataset. Our findings highlight both the promise and
limitations of LVLMs in ultrasound-based cardiovascular risk prediction,
underscoring the importance of multimodal integration, model calibration, and
domain adaptation for clinical translation.

</details>


### [103] [Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis](https://arxiv.org/abs/2510.02970)
*Xiaoyan Kui,Qianmu Xiao,Qqinsong Li,Zexin Ji,JIelin Zhang,Beiji Zou*

Main category: cs.CV

TL;DR: 本文提出FDA-VAE，一种轻量级、可解释的变分自编码器模型，用于多期增强MRI合成，通过对称潜在分布有效分离共享和独立特征。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多期增强MRI合成中，使用深度自编码器生成器，存在参数效率低、缺乏可解释的特征分离训练策略等问题。

Method: 提出Flip Distribution Alignment Variational Autoencoder (FDA-VAE)，通过将输入和目标图像编码到关于标准正态分布对称的两个潜在分布中，有效分离共享和独立特征。采用Y形双向训练策略增强特征分离的可解释性。

Result: 与现有基于深度自编码器的端到端合成方法相比，FDA-VAE显著减少了模型参数和推理时间，同时有效提高了合成质量。

Conclusion: FDA-VAE为多期增强MRI合成提供了一种高效、轻量且可解释的解决方案，在降低资源消耗的同时提升了合成性能。

Abstract: Separating shared and independent features is crucial for multi-phase
contrast-enhanced (CE) MRI synthesis. However, existing methods use deep
autoencoder generators with low parameter efficiency and lack interpretable
training strategies. In this paper, we propose Flip Distribution Alignment
Variational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model
for multi-phase CE MRI synthesis. Our method encodes input and target images
into two latent distributions that are symmetric concerning a standard normal
distribution, effectively separating shared and independent features. The
Y-shaped bidirectional training strategy further enhances the interpretability
of feature separation. Experimental results show that compared to existing deep
autoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces
model parameters and inference time while effectively improving synthesis
quality. The source code is publicly available at
https://github.com/QianMuXiao/FDA-VAE.

</details>


### [104] [TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency](https://arxiv.org/abs/2510.02987)
*Juntong Wang,Huiyu Duan,Jiarui Wang,Ziheng Jia,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出LPG-Bench，一个用于长文本提示T2I生成的综合基准，并引入TIT（Text-to-Image-to-Text）零样本度量标准，以更准确地评估长提示生成的图像，TIT表现出与人类判断更高的吻合度。


<details>
  <summary>Details</summary>
Motivation: 当前文生图（T2I）模型在处理长而详细的提示时表现不佳，生成结果不一致。此外，现有的T2I对齐评估指标在长提示图像生成方面与人类偏好的一致性差。

Method: 1. 构建了LPG-Bench基准，包含200个平均长度超过250词的精心设计长提示。2. 使用这些提示从13个SOTA模型生成了2600张图像，并进行了全面的人工排序标注。3. 引入了基于Text-to-Image-to-Text一致性的新型零样本度量标准TIT，通过比较原始提示和LMM生成的图像描述来量化T2I对齐度。TIT包括高效的TIT-Score（基于分数）和TIT-Score-LLM（基于LLM）两种实例化方法。

Result: 1. 发现现有最先进的T2I对齐评估指标在长提示图像生成方面与人类偏好的一致性差。2. TIT框架与人类判断的对齐度优于CLIP-score、LMM-score等基线。3. 其中TIT-Score-LLM在成对准确性上比最强基线绝对提升了7.31%。

Conclusion: LPG-Bench和TIT方法共同为T2I模型的基准测试和发展提供了更深入的视角。所有资源将公开。

Abstract: With the rapid advancement of large multimodal models (LMMs), recent
text-to-image (T2I) models can generate high-quality images and demonstrate
great alignment to short prompts. However, they still struggle to effectively
understand and follow long and detailed prompts, displaying inconsistent
generation. To address this challenge, we introduce LPG-Bench, a comprehensive
benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench
features 200 meticulously crafted prompts with an average length of over 250
words, approaching the input capacity of several leading commercial models.
Using these prompts, we generate 2,600 images from 13 state-of-the-art models
and further perform comprehensive human-ranked annotations. Based on LPG-Bench,
we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor
consistency with human preferences on long-prompt-based image generation. To
address the gap, we introduce a novel zero-shot metric based on
text-to-image-to-text consistency, termed TIT, for evaluating
long-prompt-generated images. The core concept of TIT is to quantify T2I
alignment by directly comparing the consistency between the raw prompt and the
LMM-produced description on the generated image, which includes an efficient
score-based instantiation TIT-Score and a large-language-model (LLM) based
instantiation TIT-Score-LLM. Extensive experiments demonstrate that our
framework achieves superior alignment with human judgment compared to
CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute
improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT
methods together offer a deeper perspective to benchmark and foster the
development of T2I models. All resources will be made publicly available.

</details>


### [105] [Towards Scalable and Consistent 3D Editing](https://arxiv.org/abs/2510.02994)
*Ruihao Xia,Yang Tang,Pan Zhou*

Main category: cs.CV

TL;DR: 该研究引入了3DEditVerse数据集和3DEditFormer模型，解决了3D编辑中跨视图一致性、结构保真度和精细控制的挑战，实现了无需3D掩码的精确一致性编辑。


<details>
  <summary>Details</summary>
Motivation: 3D编辑在沉浸式内容创作等领域有广泛应用，但面临跨视图一致性、结构保真度和精细控制等挑战。现有方法速度慢、易产生几何失真，或依赖繁琐且易出错的手动3D掩码。

Method: 1. **数据方面**: 构建了3DEditVerse，目前最大的配对3D编辑基准数据集（116,309个训练对，1,500个测试对），通过姿态驱动几何编辑和基础模型引导外观编辑确保编辑局部性、多视图一致性和语义对齐。 2. **模型方面**: 提出了3DEditFormer，一个3D结构保持条件Transformer，通过双引导注意力(dual-guidance attention)和时间自适应门控(time-adaptive gating)增强图像到3D生成，解耦可编辑区域与保留结构，实现无需辅助3D掩码的精确一致性编辑。

Result: 实验证明，该框架在定量和定性上均超越了现有最先进的基线方法，为实用和可扩展的3D编辑建立了新标准。

Conclusion: 通过结合大规模高质量数据集和创新的3D结构保持Transformer模型，该研究有效克服了传统3D编辑的挑战，实现了精确、一致且无需复杂手动掩码的3D编辑，显著提升了3D编辑的实用性和可扩展性。

Abstract: 3D editing - the task of locally modifying the geometry or appearance of a 3D
asset - has wide applications in immersive content creation, digital
entertainment, and AR/VR. However, unlike 2D editing, it remains challenging
due to the need for cross-view consistency, structural fidelity, and
fine-grained controllability. Existing approaches are often slow, prone to
geometric distortions, or dependent on manual and accurate 3D masks that are
error-prone and impractical. To address these challenges, we advance both the
data and model fronts. On the data side, we introduce 3DEditVerse, the largest
paired 3D editing benchmark to date, comprising 116,309 high-quality training
pairs and 1,500 curated test pairs. Built through complementary pipelines of
pose-driven geometric edits and foundation model-guided appearance edits,
3DEditVerse ensures edit locality, multi-view consistency, and semantic
alignment. On the model side, we propose 3DEditFormer, a
3D-structure-preserving conditional transformer. By enhancing image-to-3D
generation with dual-guidance attention and time-adaptive gating, 3DEditFormer
disentangles editable regions from preserved structure, enabling precise and
consistent edits without requiring auxiliary 3D masks. Extensive experiments
demonstrate that our framework outperforms state-of-the-art baselines both
quantitatively and qualitatively, establishing a new standard for practical and
scalable 3D editing. Dataset and code will be released. Project:
https://www.lv-lab.org/3DEditFormer/

</details>


### [106] [Not every day is a sunny day: Synthetic cloud injection for deep land cover segmentation robustness evaluation across data sources](https://arxiv.org/abs/2510.03006)
*Sara Mobsite,Renaud Hostache,Laure Berti Equille,Emmanuel Roux,Joris Guerin*

Main category: cs.CV

TL;DR: 针对云层遮挡和细节丢失问题，提出云注入算法并探索雷达-光学融合，同时通过注入NDI增强深度学习模型的土地覆盖分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有无云卫星数据集限制了在云层常见热带地区土地覆盖语义分割的应用；深度网络编码器下采样导致空间和/或光谱细节丢失。

Method: 开发了云注入算法模拟真实云层；测试了Sentinel-1雷达数据弥补光学图像云遮挡间隙的有效性；提出一种轻量级方法，在最终解码层注入归一化差异指数(NDIs)以保留关键空间特征。

Result: 在无云图像上，NDI注入使U-Net和DeepLabV3在DFC2020数据集上的土地覆盖分割性能分别提升1.99%和2.78%。在有云条件下，结合Sentinel-1数据比单独使用光学数据在所有模型中都带来显著性能提升。

Conclusion: 雷达-光学融合在具有挑战性的大气条件下（如云覆盖）对土地覆盖语义分割非常有效；NDI注入是一种低计算成本且能保留关键空间特征的有效方法。

Abstract: Supervised deep learning for land cover semantic segmentation (LCS) relies on
labeled satellite data. However, most existing Sentinel-2 datasets are
cloud-free, which limits their usefulness in tropical regions where clouds are
common. To properly evaluate the extent of this problem, we developed a cloud
injection algorithm that simulates realistic cloud cover, allowing us to test
how Sentinel-1 radar data can fill in the gaps caused by cloud-obstructed
optical imagery. We also tackle the issue of losing spatial and/or spectral
details during encoder downsampling in deep networks. To mitigate this loss, we
propose a lightweight method that injects Normalized Difference Indices (NDIs)
into the final decoding layers, enabling the model to retain key spatial
features with minimal additional computation. Injecting NDIs enhanced land
cover segmentation performance on the DFC2020 dataset, yielding improvements of
1.99% for U-Net and 2.78% for DeepLabV3 on cloud-free imagery. Under
cloud-covered conditions, incorporating Sentinel-1 data led to significant
performance gains across all models compared to using optical data alone,
highlighting the effectiveness of radar-optical fusion in challenging
atmospheric scenarios.

</details>


### [107] [PocketSR: The Super-Resolution Expert in Your Pocket Mobiles](https://arxiv.org/abs/2510.03012)
*Haoze Sun,Linfeng Jiang,Fan Li,Renjing Pei,Zhixin Wang,Yong Guo,Jiaqi Xu,Haoyu Chen,Jin Han,Fenglong Song,Yujiu Yang,Wenbo Li*

Main category: cs.CV

TL;DR: 本文提出PocketSR，一个超轻量级、单步的生成式真实世界图像超分辨率（RealSR）模型，能在保持高保真度的同时，在边缘设备上部署。


<details>
  <summary>Details</summary>
Motivation: 现有利用大型生成模型的RealSR方法计算成本高、延迟大，不适合在手机等边缘设备上部署。

Method: ['设计了LiteED，一个高效替代SD中计算密集型VAE的模块，参数量减少97.5%并保持高质量编解码。', '提出U-Net的在线退火剪枝策略，逐步将生成先验从重模块转移到轻量级模块，以优化效率。', '引入多层特征蒸馏损失，以减轻剪枝过程中先验知识的损失。']

Result: ['PocketSR模型大小为146M参数，能在0.8秒内处理4K图像，显著提高了速度。', '性能与最先进的单步乃至多步RealSR模型相当。']

Conclusion: PocketSR为边缘设备上的RealSR应用提供了一个高度实用且高效的解决方案。

Abstract: Real-world image super-resolution (RealSR) aims to enhance the visual quality
of in-the-wild images, such as those captured by mobile phones. While existing
methods leveraging large generative models demonstrate impressive results, the
high computational cost and latency make them impractical for edge deployment.
In this paper, we introduce PocketSR, an ultra-lightweight, single-step model
that brings generative modeling capabilities to RealSR while maintaining high
fidelity. To achieve this, we design LiteED, a highly efficient alternative to
the original computationally intensive VAE in SD, reducing parameters by 97.5%
while preserving high-quality encoding and decoding. Additionally, we propose
online annealing pruning for the U-Net, which progressively shifts generative
priors from heavy modules to lightweight counterparts, ensuring effective
knowledge transfer and further optimizing efficiency. To mitigate the loss of
prior knowledge during pruning, we incorporate a multi-layer feature
distillation loss. Through an in-depth analysis of each design component, we
provide valuable insights for future research. PocketSR, with a model size of
146M parameters, processes 4K images in just 0.8 seconds, achieving a
remarkable speedup over previous methods. Notably, it delivers performance on
par with state-of-the-art single-step and even multi-step RealSR models, making
it a highly practical solution for edge-device applications.

</details>


### [108] [When and Where do Events Switch in Multi-Event Video Generation?](https://arxiv.org/abs/2510.03049)
*Ruotong Liao,Guowen Huang,Qing Cheng,Thomas Seidl,Daniel Cremers,Volker Tresp*

Main category: cs.CV

TL;DR: 本文引入MEve提示套件，研究多事件提示在T2V生成中控制事件转换的时机和位置，发现去噪步骤和模型层中的早期干预至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有T2V多事件生成方法在处理长视频多顺序事件时，忽略了事件转换的内在因素。本研究旨在回答“多事件提示何时何地控制T2V生成中的事件转换”这一核心问题。

Method: 引入了自建的多事件文本到视频（T2V）生成评估提示套件MEve，并对OpenSora和CogVideoX这两个代表性模型家族进行了系统研究。

Result: 广泛实验证明了在去噪步骤和分块模型层中进行早期干预的重要性，揭示了多事件视频生成的关键因素。

Conclusion: 早期干预（在去噪步骤和模型层中）对于多事件视频生成至关重要，为未来模型中多事件条件化的可能性提供了方向。

Abstract: Text-to-video (T2V) generation has surged in response to challenging
questions, especially when a long video must depict multiple sequential events
with temporal coherence and controllable content. Existing methods that extend
to multi-event generation omit an inspection of the intrinsic factor in event
shifting. The paper aims to answer the central question: When and where
multi-event prompts control event transition during T2V generation. This work
introduces MEve, a self-curated prompt suite for evaluating multi-event
text-to-video (T2V) generation, and conducts a systematic study of two
representative model families, i.e., OpenSora and CogVideoX. Extensive
experiments demonstrate the importance of early intervention in denoising steps
and block-wise model layers, revealing the essential factor for multi-event
video generation and highlighting the possibilities for multi-event
conditioning in future models.

</details>


### [109] [InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition](https://arxiv.org/abs/2510.03066)
*Ahsan Farabi,Israt Khandaker,Ibrahim Khalil Shanto,Md Abdul Ahad Minhaz,Tanisha Zaman*

Main category: cs.CV

TL;DR: InsideOut是一个基于EfficientNetV2-S的可复现FER框架，通过数据增强和不平衡优化，在FER2013数据集上取得了有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 面部情绪识别（FER）面临遮挡、光照/姿态变化、细微类内差异以及数据集不平衡（特别是少数情绪识别）等挑战。

Method: 提出了InsideOut框架，该框架基于EfficientNetV2-S并结合迁移学习、强数据增强和不平衡感知优化。具体方法包括标准化FER2013图像、分层划分与增强数据，并使用类别加权损失微调轻量级分类头以处理数据分布偏差。

Result: InsideOut在FER2013数据集上达到了62.8%的准确率和0.590的宏平均F1分数，表现优于传统CNN基线。

Conclusion: 高效架构与定制化的不平衡处理相结合，可以为面部情绪识别提供实用、透明且可复现的解决方案。

Abstract: Facial Emotion Recognition (FER) is a key task in affective computing,
enabling applications in human-computer interaction, e-learning, healthcare,
and safety systems. Despite advances in deep learning, FER remains challenging
due to occlusions, illumination and pose variations, subtle intra-class
differences, and dataset imbalance that hinders recognition of minority
emotions. We present InsideOut, a reproducible FER framework built on
EfficientNetV2-S with transfer learning, strong data augmentation, and
imbalance-aware optimization. The approach standardizes FER2013 images, applies
stratified splitting and augmentation, and fine-tunes a lightweight
classification head with class-weighted loss to address skewed distributions.
InsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013,
showing competitive results compared to conventional CNN baselines. The novelty
lies in demonstrating that efficient architectures, combined with tailored
imbalance handling, can provide practical, transparent, and reproducible FER
solutions.

</details>


### [110] [What Drives Compositional Generalization in Visual Generative Models?](https://arxiv.org/abs/2510.03075)
*Karim Farid,Rajat Sahay,Yumna Ali Alnaggar,Simon Schrodi,Volker Fischer,Cordelia Schmid,Thomas Brox*

Main category: cs.CV

TL;DR: 本研究系统分析了影响视觉生成模型组合泛化的关键设计选择，发现训练目标分布（离散/连续）和条件信息是重要因素，并提出通过引入连续JEPA目标可改善MaskGIT等离散模型的组合性能。


<details>
  <summary>Details</summary>
Motivation: 组合泛化能力对视觉生成模型至关重要，但其作用机制尚未完全理解。

Method: 通过受控实验，系统研究了各种设计选择（如训练目标分布类型和条件信息提供程度）如何影响图像和视频生成中的组合泛化。

Result: 识别出两个关键因素：(i) 训练目标是基于离散还是连续分布，以及 (ii) 训练时条件信息提供构成概念的程度。基于这些发现，证明通过辅助的基于JEPA的连续目标放松MaskGIT的离散损失，可以提高MaskGIT等离散模型的组合性能。

Conclusion: 训练目标的离散/连续性质以及条件信息的丰富程度是影响组合泛化的关键。将辅助连续目标引入离散模型（如MaskGIT），能有效提升其组合泛化能力。

Abstract: Compositional generalization, the ability to generate novel combinations of
known concepts, is a key ingredient for visual generative models. Yet, not all
mechanisms that enable or inhibit it are fully understood. In this work, we
conduct a systematic study of how various design choices influence
compositional generalization in image and video generation in a positive or
negative way. Through controlled experiments, we identify two key factors: (i)
whether the training objective operates on a discrete or continuous
distribution, and (ii) to what extent conditioning provides information about
the constituent concepts during training. Building on these insights, we show
that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based
objective can improve compositional performance in discrete models like
MaskGIT.

</details>


### [111] [Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations](https://arxiv.org/abs/2510.03089)
*Naresh Kumar Devulapally,Shruti Agarwal,Tejas Gokhale,Vishnu Suresh Lokhande*

Main category: cs.CV

TL;DR: 为应对文本到图像扩散模型个性化带来的隐私担忧，本文提出了一种新颖的潜在空间扰动策略，通过修改去噪轨迹生成“不可学习”的训练样本，实现了对未经授权模型适配的不可察觉防御，显著提升了图像保真度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型的快速高保真个性化能力引发了数据隐私、知识产权保护和未经授权使用的担忧。现有通过图像投毒生成“不可学习”样本的方法，由于在像素空间操作，图像存在噪声和伪影，导致隐蔽性不足。

Method: 提出了一种新颖的基于模型的扰动策略，该策略在扩散模型的潜在空间中操作。通过交替进行去噪和反演，并修改去噪轨迹的起始点（轨迹偏移采样），确保扰动图像保持高视觉保真度，同时抵抗下游生成模型的反演和个性化。此方法将不可学习性融入潜在扩散模型（LDMs）框架。

Result: 在四个基准数据集上验证了该方法，结果显示在感知度量（包括PSNR、SSIM和FID）上的不可察觉性显著提高（约8%-10%），在五种对抗性设置下的鲁棒性平均提高约10%，证明了其在保护敏感数据方面的有效性。

Conclusion: 所提出的方法为潜在扩散模型提供了一种实用且不可察觉的防御机制，有效对抗未经授权的模型适配，通过生成具有高视觉保真度和强抵抗力以防止个性化的“不可学习”样本，成功保护了敏感数据。

Abstract: Text-to-image diffusion models have demonstrated remarkable effectiveness in
rapid and high-fidelity personalization, even when provided with only a few
user images. However, the effectiveness of personalization techniques has lead
to concerns regarding data privacy, intellectual property protection, and
unauthorized usage. To mitigate such unauthorized usage and model replication,
the idea of generating ``unlearnable'' training samples utilizing image
poisoning techniques has emerged. Existing methods for this have limited
imperceptibility as they operate in the pixel space which results in images
with noise and artifacts. In this work, we propose a novel model-based
perturbation strategy that operates within the latent space of diffusion
models. Our method alternates between denoising and inversion while modifying
the starting point of the denoising trajectory: of diffusion models. This
trajectory-shifted sampling ensures that the perturbed images maintain high
visual fidelity to the original inputs while being resistant to inversion and
personalization by downstream generative models. This approach integrates
unlearnability into the framework of Latent Diffusion Models (LDMs), enabling a
practical and imperceptible defense against unauthorized model adaptation. We
validate our approach on four benchmark datasets to demonstrate robustness
against state-of-the-art inversion attacks. Results demonstrate that our method
achieves significant improvements in imperceptibility ($\sim 8 \% -10\%$ on
perceptual metrics including PSNR, SSIM, and FID) and robustness ( $\sim 10\%$
on average across five adversarial settings), highlighting its effectiveness in
safeguarding sensitive data.

</details>


### [112] [Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields](https://arxiv.org/abs/2510.03104)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 本文探讨了几何感知语义特征在辐射场中的表现。研究发现，几何感知特征能提供更精细的结构细节，但在语义物体定位上无显著优势，且在姿态估计精度上反而有所下降。结果表明纯视觉特征在更广泛的任务中更具通用性。


<details>
  <summary>Details</summary>
Motivation: 辐射场中的语义蒸馏（利用大型视觉模型预训练的语义）推动了机器人策略的发展。现有工作已证明纯视觉语义特征（如DINO和CLIP）的有效性，但几何感知特征在蒸馏场中的潜在益处，尤其对于姿态估计等空间任务，仍是一个悬而未决的问题。核心动机是探究几何感知语义特征是否能带来优势。

Method: 本文通过回答三个关键问题进行研究：1. 空间感知是否产生更高精度的几何感知语义特征？（通过分析特征内容）2. 几何感知是否改进语义物体定位？（通过观察定位性能）3. 几何感知是否实现更高精度的辐射场逆向推理？（为此提出了一个名为SPINE的新框架，用于无初始猜测的辐射场逆向推理，包含粗粒度语义蒸馏逆向推理和基于光度优化的细粒度逆向推理。）

Result: 研究发现：1. 来自几何感知骨干的图像特征包含更精细的结构细节。2. 在语义物体定位任务中，未观察到显著差异。3. 令人惊讶的是，使用几何感知特征时，姿态估计精度反而下降。4. 纯视觉特征在更广泛的下游任务中表现出更大的通用性。

Conclusion: 尽管几何感知特征包含更多几何细节，但纯视觉特征在更广泛的下游任务中显示出更大的通用性。研究结果强调，未来需要深入研究有效的几何感知策略，以提升预训练语义特征的通用性和性能。

Abstract: Semantic distillation in radiance fields has spurred significant advances in
open-vocabulary robot policies, e.g., in manipulation and navigation, founded
on pretrained semantics from large vision models. While prior work has
demonstrated the effectiveness of visual-only semantic features (e.g., DINO and
CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit
of geometry-grounding in distilled fields remains an open question. In
principle, visual-geometry features seem very promising for spatial tasks such
as pose estimation, prompting the question: Do geometry-grounded semantic
features offer an edge in distilled fields? Specifically, we ask three critical
questions: First, does spatial-grounding produce higher-fidelity geometry-aware
semantic features? We find that image features from geometry-grounded backbones
contain finer structural details compared to their counterparts. Secondly, does
geometry-grounding improve semantic object localization? We observe no
significant difference in this task. Thirdly, does geometry-grounding enable
higher-accuracy radiance field inversion? Given the limitations of prior work
and their lack of semantics integration, we propose a novel framework SPINE for
inverting radiance fields without an initial guess, consisting of two core
components: coarse inversion using distilled semantics, and fine inversion
using photometric-based optimization. Surprisingly, we find that the pose
estimation accuracy decreases with geometry-grounded features. Our results
suggest that visual-only features offer greater versatility for a broader range
of downstream tasks, although geometry-grounded features contain more geometric
detail. Notably, our findings underscore the necessity of future research on
effective strategies for geometry-grounding that augment the versatility and
performance of pretrained semantic features.

</details>


### [113] [GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion](https://arxiv.org/abs/2510.03110)
*Beibei Lin,Tingting Chen,Robby T. Tan*

Main category: cs.CV

TL;DR: GeoComplete是一个新颖的参考驱动图像补全框架，通过结合3D几何引导和目标感知掩码策略，在目标视图与参考图差异大时，显著提高了补全内容的几何准确性和视觉质量，实现了17.1 PSNR的提升。


<details>
  <summary>Details</summary>
Motivation: 现有参考驱动图像补全方法在目标视图与参考图显著不同时表现不佳，因为它们仅依赖扩散先验，缺乏相机姿态或深度等几何线索，常导致生成内容错位或不合理。

Method: GeoComplete框架通过引入明确的3D结构引导，确保补全区域的几何一致性。其核心思想包括：1) 将扩散过程条件化于投影点云以注入几何信息；2) 应用目标感知掩码，引导模型关注相关参考线索。框架采用双分支扩散架构，一分支从掩码目标合成缺失区域，另一分支从投影点云提取几何特征，并通过联合自注意力确保连贯准确补全。目标感知掩码通过将目标视图投影到参考图来检测并掩盖遮挡区域，使模型聚焦有用线索。

Result: 实验表明，GeoComplete相较于最先进方法实现了17.1 PSNR的显著提升，显著提高了几何准确性，同时保持了高视觉质量。

Conclusion: GeoComplete通过整合几何感知的双分支扩散架构与目标感知掩码策略，为几何条件图像补全提供了一个统一且鲁棒的解决方案。

Abstract: Reference-driven image completion, which restores missing regions in a target
view using additional images, is particularly challenging when the target view
differs significantly from the references. Existing generative methods rely
solely on diffusion priors and, without geometric cues such as camera pose or
depth, often produce misaligned or implausible content. We propose GeoComplete,
a novel framework that incorporates explicit 3D structural guidance to enforce
geometric consistency in the completed regions, setting it apart from prior
image-only approaches. GeoComplete introduces two key ideas: conditioning the
diffusion process on projected point clouds to infuse geometric information,
and applying target-aware masking to guide the model toward relevant reference
cues. The framework features a dual-branch diffusion architecture. One branch
synthesizes the missing regions from the masked target, while the other
extracts geometric features from the projected point cloud. Joint
self-attention across branches ensures coherent and accurate completion. To
address regions visible in references but absent in the target, we project the
target view into each reference to detect occluded areas, which are then masked
during training. This target-aware masking directs the model to focus on useful
cues, enhancing performance in difficult scenarios. By integrating a
geometry-aware dual-branch diffusion architecture with a target-aware masking
strategy, GeoComplete offers a unified and robust solution for
geometry-conditioned image completion. Experiments show that GeoComplete
achieves a 17.1 PSNR improvement over state-of-the-art methods, significantly
boosting geometric accuracy while maintaining high visual quality.

</details>


### [114] [Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction](https://arxiv.org/abs/2510.03117)
*Kaisi Guan,Xihua Wang,Zhengfeng Lai,Xin Cheng,Peng Zhang,XiaoJiang Liu,Ruihua Song,Meng Cao*

Main category: cs.CV

TL;DR: 本文提出HVGC和BridgeDiT框架，通过解耦字幕和双向跨模态交互机制，解决了文本到有声视频（T2SV）生成中的模态干扰和同步难题，取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 文本到有声视频（T2SV）生成任务面临两大挑战：共享文本字幕导致模态干扰，以及最优的跨模态特征交互机制尚不明确。

Method: 1. 提出分层视觉基础字幕（HVGC）框架，生成解耦的视频字幕和音频字幕，以消除条件阶段的干扰。2. 基于HVGC，引入BridgeDiT，这是一种新颖的双塔扩散Transformer，采用双交叉注意力（DCA）机制作为“桥梁”，实现对称、双向的信息交换，从而达到语义和时间上的同步。

Result: 在三个基准数据集上进行了广泛实验，并结合人工评估，结果表明该方法在大多数指标上实现了最先进的性能。全面的消融研究进一步验证了所提贡献的有效性。

Conclusion: 本研究通过提出HVGC和BridgeDiT有效解决了T2SV任务中的关键挑战，取得了显著的性能提升，并为未来的T2SV研究提供了重要见解。

Abstract: This study focuses on a challenging yet promising task,
Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with
synchronized audio from text conditions, meanwhile ensuring both modalities are
aligned with text. Despite progress in joint audio-video training, two critical
challenges still remain unaddressed: (1) a single, shared text caption where
the text for video is equal to the text for audio often creates modal
interference, confusing the pretrained backbones, and (2) the optimal mechanism
for cross-modal feature interaction remains unclear. To address these
challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC)
framework that generates pairs of disentangled captions, a video caption, and
an audio caption, eliminating interference at the conditioning stage. Based on
HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,
which employs a Dual CrossAttention (DCA) mechanism that acts as a robust
``bridge" to enable a symmetric, bidirectional exchange of information,
achieving both semantic and temporal synchronization. Extensive experiments on
three benchmark datasets, supported by human evaluations, demonstrate that our
method achieves state-of-the-art results on most metrics. Comprehensive
ablation studies further validate the effectiveness of our contributions,
offering key insights for the future T2SV task. All the codes and checkpoints
will be publicly released.

</details>


### [115] [HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion](https://arxiv.org/abs/2510.03122)
*Shiyi Zhang,Dong Liang,Hairong Zheng,Yihang Zhou*

Main category: cs.CV

TL;DR: 本文提出HAVIR模型，受视觉皮层层次结构启发，通过分离处理结构和语义信息，并整合Versatile Diffusion模型，显著提升了从脑活动重建复杂视觉刺激的结构和语义质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确恢复复杂视觉刺激，原因在于自然场景的低级特征异质性和高级特征的语义纠缠。这促使研究者寻求更有效的方法。

Method: HAVIR模型借鉴视觉皮层层次表征理论，将视觉皮层分为两个层次区域，分别提取特征。结构生成器从空间处理体素提取结构信息并转换为潜在扩散先验；语义提取器将语义处理体素转换为CLIP嵌入。这些组件通过Versatile Diffusion模型整合以合成最终图像。

Result: 实验结果表明，HAVIR模型在复杂场景下也能增强重建图像的结构和语义质量，并且优于现有模型。

Conclusion: HAVIR模型通过模拟视觉皮层的分层处理，成功提升了从脑活动重建复杂视觉刺激的结构和语义质量，为脑机接口和计算机视觉的交叉领域提供了新进展。

Abstract: The reconstruction of visual information from brain activity fosters
interdisciplinary integration between neuroscience and computer vision.
However, existing methods still face challenges in accurately recovering highly
complex visual stimuli. This difficulty stems from the characteristics of
natural scenes: low-level features exhibit heterogeneity, while high-level
features show semantic entanglement due to contextual overlaps. Inspired by the
hierarchical representation theory of the visual cortex, we propose the HAVIR
model, which separates the visual cortex into two hierarchical regions and
extracts distinct features from each. Specifically, the Structural Generator
extracts structural information from spatial processing voxels and converts it
into latent diffusion priors, while the Semantic Extractor converts semantic
processing voxels into CLIP embeddings. These components are integrated via the
Versatile Diffusion model to synthesize the final image. Experimental results
demonstrate that HAVIR enhances both the structural and semantic quality of
reconstructions, even in complex scenes, and outperforms existing models.

</details>


### [116] [Mask2IV: Interaction-Centric Video Generation via Mask Trajectories](https://arxiv.org/abs/2510.03135)
*Gen Li,Bo Zhao,Jianfei Yang,Laura Sevilla-Lara*

Main category: cs.CV

TL;DR: Mask2IV是一个新框架，通过解耦的两阶段方法生成以交互为中心的视频，无需密集掩码输入，并提供多功能控制。它解决了现有方法在建模复杂交互及对掩码标注的依赖问题，在视觉真实性和可控性上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 生成以交互为中心的视频对具身智能至关重要，但现有方法难以建模复杂动态交互。同时，虽然掩码可作为有效控制信号，但在实际应用中获取密集精确的掩码标注面临巨大挑战。

Method: 引入Mask2IV框架，采用解耦的两阶段流程：首先预测行动者和物体的合理运动轨迹，然后基于这些轨迹生成视频。此设计无需用户提供密集掩码输入，同时支持多功能和直观的控制，允许用户通过动作描述或空间位置线索指定交互对象并引导运动轨迹。此外，为支持训练和评估，还整理了两个基准数据集。

Result: 广泛实验证明，Mask2IV在视觉真实感和可控性方面均优于现有基线方法。

Conclusion: Mask2IV成功地解决了以交互为中心视频生成中复杂交互建模和对密集掩码输入的需求问题，通过其独特的两阶段设计和多功能控制，提升了生成视频的质量和用户操控性，对具身智能领域具有重要意义。

Abstract: Generating interaction-centric videos, such as those depicting humans or
robots interacting with objects, is crucial for embodied intelligence, as they
provide rich and diverse visual priors for robot learning, manipulation policy
training, and affordance reasoning. However, existing methods often struggle to
model such complex and dynamic interactions. While recent studies show that
masks can serve as effective control signals and enhance generation quality,
obtaining dense and precise mask annotations remains a major challenge for
real-world use. To overcome this limitation, we introduce Mask2IV, a novel
framework specifically designed for interaction-centric video generation. It
adopts a decoupled two-stage pipeline that first predicts plausible motion
trajectories for both actor and object, then generates a video conditioned on
these trajectories. This design eliminates the need for dense mask inputs from
users while preserving the flexibility to manipulate the interaction process.
Furthermore, Mask2IV supports versatile and intuitive control, allowing users
to specify the target object of interaction and guide the motion trajectory
through action descriptions or spatial position cues. To support systematic
training and evaluation, we curate two benchmarks covering diverse action and
object categories across both human-object interaction and robotic manipulation
scenarios. Extensive experiments demonstrate that our method achieves superior
visual realism and controllability compared to existing baselines.

</details>


### [117] [ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories](https://arxiv.org/abs/2510.03152)
*Anantajit Subrahmanya,Chandrakanth Gudavalli,Connor Levenson,Umang Garg,B. S. Manjunath*

Main category: cs.CV

TL;DR: 本文提出一种基于马尔可夫Reeb图的新颖框架，用于模拟保留个体生活模式（PoLs）的时空轨迹，并在效率和准确性方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 准确建模人类出行对于城市规划、流行病学和交通管理至关重要。

Method: 该研究引入了马尔可夫Reeb图（Markovian Reeb Graphs），这是一种结合个体和群体层面出行结构、基于概率拓扑模型的方法，旨在生成既能捕捉日常生活中一致性又能体现变异性的未来轨迹。

Result: 在Urban Anomalies数据集（亚特兰大和柏林子集）上，使用Jensen-Shannon散度（JSD）对群体和个体层面指标进行评估，结果表明所提出的方法在保持数据和计算效率的同时，实现了高保真度。

Conclusion: 马尔可夫Reeb图被确认为一个可扩展的轨迹模拟框架，在各种城市环境中具有广泛的适用性。

Abstract: Accurately modeling human mobility is critical for urban planning,
epidemiology, and traffic management. In this work, we introduce Markovian Reeb
Graphs, a novel framework for simulating spatiotemporal trajectories that
preserve Patterns of Life (PoLs) learned from baseline data. By combining
individual- and population-level mobility structures within a probabilistic
topological model, our approach generates realistic future trajectories that
capture both consistency and variability in daily life. Evaluations on the
Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon
Divergence (JSD) across population- and agent-level metrics demonstrate that
the proposed method achieves strong fidelity while remaining data- and
compute-efficient. These results position Markovian Reeb Graphs as a scalable
framework for trajectory simulation with broad applicability across diverse
urban environments.

</details>


### [118] [SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus](https://arxiv.org/abs/2510.03160)
*Ming Zhao,Wenhui Dong,Yang Zhang,Xiang Zheng,Zhonghao Zhang,Zian Zhou,Yunzhi Guan,Liukun Xu,Wei Peng,Zhaoyang Gong,Zhicheng Zhang,Dachuan Li,Xiaosheng Ma,Yuli Ma,Jianing Ni,Changjiang Jiang,Lixia Tian,Qixin Chen,Kaishun Xia,Pingping Liu,Tongshun Zhang,Zhiqiang Liu,Zhongan Bi,Chenyang Si,Tiansheng Sun,Caifeng Shan*

Main category: cs.CV

TL;DR: 针对脊柱疾病AI诊断缺乏精细化、多模态数据集和基准的挑战，本文提出SpineMed生态系统，包括大规模数据集SpineMed-450k和评估框架SpineBench。经SpineMed-450k微调的模型在脊柱疾病相关任务上显著优于现有LVLM，并得到临床医生的认可。


<details>
  <summary>Details</summary>
Motivation: 脊柱疾病影响广泛且是致残主因，但AI辅助诊断受限于缺乏精细到椎体层面、多模态的数据集、可追溯的临床指令数据以及标准化的脊柱专用基准。临床决策需要对X射线、CT、MRI等影像数据进行复杂的椎体层面推理。

Method: 本文引入SpineMed生态系统，由脊柱外科医生共同设计，包含：
1.  **SpineMed-450k数据集：** 首个专为椎体层面跨模态推理设计的大规模数据集，包含超过45万条指令实例。数据来源于教科书、指南、开放数据集和约1000例去识别化医院病例。采用“临床医生参与+两阶段LLM生成”（草稿与修订）流程，确保数据高质量和可追溯性，支持问答、多轮咨询和报告生成。
2.  **SpineBench评估框架：** 一个具有临床基础的评估框架，用于评估模型在椎体识别、病理评估和手术规划等临床重要方面的表现。
对现有大型视觉-语言模型（LVLMs）在SpineBench上进行了综合评估，并将一个在SpineMed-450k上微调的模型与其表现进行对比。

Result: 1.  现有先进的LVLMs在SpineBench上的表现显示，它们在精细、特定椎体层面的推理上存在系统性弱点。
2.  在SpineMed-450k上微调的模型在所有任务上均表现出持续且显著的改进。
3.  临床医生评估证实了本文模型的输出具有较高的诊断清晰度和实用性。

Conclusion: SpineMed生态系统（包含SpineMed-450k数据集和SpineBench评估框架）有效解决了脊柱疾病AI诊断中数据和基准的不足。基于SpineMed-450k微调的模型显著提升了在椎体层面多模态推理任务上的性能，其输出获得了临床医生的高度认可，具备实际应用价值。

Abstract: Spine disorders affect 619 million people globally and are a leading cause of
disability, yet AI-assisted diagnosis remains limited by the lack of
level-aware, multimodal datasets. Clinical decision-making for spine disorders
requires sophisticated reasoning across X-ray, CT, and MRI at specific
vertebral levels. However, progress has been constrained by the absence of
traceable, clinically-grounded instruction data and standardized,
spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem
co-designed with practicing spine surgeons. It features SpineMed-450k, the
first large-scale dataset explicitly designed for vertebral-level reasoning
across imaging modalities with over 450,000 instruction instances, and
SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is
curated from diverse sources, including textbooks, guidelines, open datasets,
and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline
with a two-stage LLM generation method (draft and revision) to ensure
high-quality, traceable data for question-answering, multi-turn consultations,
and report generation. SpineBench evaluates models on clinically salient axes,
including level identification, pathology assessment, and surgical planning.
Our comprehensive evaluation of several recently advanced large vision-language
models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,
level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k
demonstrates consistent and significant improvements across all tasks.
Clinician assessments confirm the diagnostic clarity and practical utility of
our model's outputs.

</details>


### [119] [UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization](https://arxiv.org/abs/2510.03161)
*Qing Huang,Zhipei Xu,Xuanyu Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: UniShield是一个多智能体统一系统，能跨图像篡改、文档篡改、DeepFake和AI生成图像等多种领域检测和定位图像伪造，表现优于现有方法，具有卓越的实用性、适应性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术进步，合成图像日益逼真，带来虚假信息和欺诈等社会风险，因此伪造图像检测和定位（FIDL）至关重要。然而，现有领域特定检测方法受限于窄专业化、跨域泛化能力差以及缺乏集成自适应框架，实际应用受限。

Method: 提出UniShield系统，该系统是基于多智能体的统一系统，创新性地集成了感知智能体和检测智能体。感知智能体智能分析图像特征以动态选择合适的检测模型，而检测智能体则整合多种专家检测器并生成可解释报告。

Result: 广泛实验表明，UniShield取得了最先进的结果，超越了现有统一方法和领域特定检测器。

Conclusion: UniShield系统在实践性、适应性和可扩展性方面表现出色，能够有效检测和定位跨域图像伪造，维护信息完整性和社会安全。

Abstract: With the rapid advancements in image generation, synthetic images have become
increasingly realistic, posing significant societal risks, such as
misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus
emerges as essential for maintaining information integrity and societal
security. Despite impressive performances by existing domain-specific detection
methods, their practical applicability remains limited, primarily due to their
narrow specialization, poor cross-domain generalization, and the absence of an
integrated adaptive framework. To address these issues, we propose UniShield,
the novel multi-agent-based unified system capable of detecting and localizing
image forgeries across diverse domains, including image manipulation, document
manipulation, DeepFake, and AI-generated images. UniShield innovatively
integrates a perception agent with a detection agent. The perception agent
intelligently analyzes image features to dynamically select suitable detection
models, while the detection agent consolidates various expert detectors into a
unified framework and generates interpretable reports. Extensive experiments
show that UniShield achieves state-of-the-art results, surpassing both existing
unified approaches and domain-specific detectors, highlighting its superior
practicality, adaptiveness, and scalability.

</details>


### [120] [ROGR: Relightable 3D Objects using Generative Relighting](https://arxiv.org/abs/2510.03163)
*Jiapeng Tang,Matthew Lavine,Dor Verbin,Stephan J. Garbin,Matthias Nießner,Ricardo Martin Brualla,Pratul P. Srinivasan,Philipp Henzler*

Main category: cs.CV

TL;DR: 本文提出ROGR，一种新方法，通过生成式重打光模型，从多视角重建可重打光的3D物体模型，实现高效的任意环境光照下的前向重打光。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有方法在从多视角捕捉的物体上实现高效、任意环境光照下的重打光重建的挑战，避免繁琐的逐光照优化或光传输模拟。

Method: ROGR通过在多种光照环境下采样物体外观以创建数据集，然后训练一个光照条件下的神经辐射场（NeRF）。该NeRF采用新颖的双分支架构，分别编码通用光照效果和高光。优化后的NeRF能实现任意环境图下的高效前向重打光。

Result: ROGR在TensoIR和Stanford-ORB数据集上，在大多数指标上超越了现有最先进的方法，并在真实世界物体捕捉中展示了其效果。它实现了在任意环境图下高效的前向重打光，无需逐光照优化或光传输模拟。

Conclusion: ROGR是一种创新的方法，利用生成式重打光模型和新颖的双分支NeRF架构，成功地从多视角重建了可重打光的3D物体模型，并在重打光性能和效率方面达到了领先水平。

Abstract: We introduce ROGR, a novel approach that reconstructs a relightable 3D model
of an object captured from multiple views, driven by a generative relighting
model that simulates the effects of placing the object under novel environment
illuminations. Our method samples the appearance of the object under multiple
lighting environments, creating a dataset that is used to train a
lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's
appearance under any input environmental lighting. The lighting-conditioned
NeRF uses a novel dual-branch architecture to encode the general lighting
effects and specularities separately. The optimized lighting-conditioned NeRF
enables efficient feed-forward relighting under arbitrary environment maps
without requiring per-illumination optimization or light transport simulation.
We evaluate our approach on the established TensoIR and Stanford-ORB datasets,
where it improves upon the state-of-the-art on most metrics, and showcase our
approach on real-world object captures.

</details>


### [121] [Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training](https://arxiv.org/abs/2510.03189)
*Tidiane Camaret Ndir,Alexander Pfefferle,Robin Tibor Schirrmeister*

Main category: cs.CV

TL;DR: 提出一种新的训练策略，结合动态体素提示生成和内容感知自适应裁剪，以优化交互式3D生物医学图像分割，并在竞赛中取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在交互式3D生物医学图像分割中缺乏体素感知或交互能力，且从单GPU上的序列细化反馈学习存在计算挑战。

Method: 提出一种训练策略，结合动态体素提示生成与内容感知自适应裁剪，以优化图像编码器使用。训练中模拟真实用户交互模式，并使用nnInteractive模型的公开权重初始化网络。

Result: 在“交互式3D生物医学图像分割基础模型”竞赛中表现出色，平均最终Dice得分为0.6385，归一化表面距离为0.6614，以及曲线下面积（AUC）指标（Dice为2.4799，NSD为2.5671）。

Conclusion: 该训练策略有效解决了交互式3D生物医学图像分割的现有模型局限和计算挑战，并在竞赛中展示了强大的性能。

Abstract: Interactive 3D biomedical image segmentation requires efficient models that
can iteratively refine predictions based on user prompts. Current foundation
models either lack volumetric awareness or suffer from limited interactive
capabilities. We propose a training strategy that combines dynamic volumetric
prompt generation with content-aware adaptive cropping to optimize the use of
the image encoder. Our method simulates realistic user interaction patterns
during training while addressing the computational challenges of learning from
sequential refinement feedback on a single GPU. For efficient training, we
initialize our network using the publicly available weights from the
nnInteractive segmentation model. Evaluation on the \textbf{Foundation Models
for Interactive 3D Biomedical Image Segmentation} competition demonstrates
strong performance with an average final Dice score of 0.6385, normalized
surface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice)
and 2.5671 (NSD).

</details>


### [122] [Product-Quantised Image Representation for High-Quality Image Synthesis](https://arxiv.org/abs/2510.03191)
*Denis Zavadski,Nikita Philip Tatsch,Carsten Rother*

Main category: cs.CV

TL;DR: 本文提出PQGAN，一个将乘积量化（PQ）集成到VQGAN框架中的量化图像自编码器。它显著提升了图像重建性能（PSNR从27dB提高到37dB，FID/LPIPS/CMMD降低96%），并能与扩散模型无缝结合，实现更快、更高效的生成或更高的输出分辨率。


<details>
  <summary>Details</summary>
Motivation: 乘积量化（PQ）作为可扩展向量编码方法，在高保真图像生成的潜在表示中应用有限。

Method: 引入PQGAN，一个将乘积量化（PQ）集成到VQGAN框架中的量化图像自编码器。通过深入分析码本大小、嵌入维度和子空间分解之间的相互作用来优化模型设计。

Result: 重建性能显著优于现有方法，PSNR达到37dB（SOTA为27dB），FID、LPIPS和CMMD分数降低高达96%。发现VQ和PQ在缩放嵌入维度时性能表现相反。分析结果提供了PQ超参数选择的指导。

Conclusion: PQGAN可无缝集成到预训练扩散模型中，实现显著更快、更计算高效的生成，或在无额外成本下将输出分辨率加倍，证明了PQ在图像合成离散潜在表示中的强大扩展潜力。

Abstract: Product quantisation (PQ) is a classical method for scalable vector encoding,
yet it has seen limited usage for latent representations in high-fidelity image
generation. In this work, we introduce PQGAN, a quantised image autoencoder
that integrates PQ into the well-known vector quantisation (VQ) framework of
VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in
terms of reconstruction performance, including both quantisation methods and
their continuous counterparts. We achieve a PSNR score of 37dB, where prior
work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up
to 96%. Our key to success is a thorough analysis of the interaction between
codebook size, embedding dimensionality, and subspace factorisation, with
vector and scalar quantisation as special cases. We obtain novel findings, such
that the performance of VQ and PQ behaves in opposite ways when scaling the
embedding dimension. Furthermore, our analysis shows performance trends for PQ
that help guide optimal hyperparameter selection. Finally, we demonstrate that
PQGAN can be seamlessly integrated into pre-trained diffusion models. This
enables either a significantly faster and more compute-efficient generation, or
a doubling of the output resolution at no additional cost, positioning PQ as a
strong extension for discrete latent representation in image synthesis.

</details>


### [123] [Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft](https://arxiv.org/abs/2510.03198)
*Junchao Huang,Xinting Hu,Boyao Han,Shaoshuai Shi,Zhuotao Tian,Tianyu He,Li Jiang*

Main category: cs.CV

TL;DR: 本文提出Memory Forcing框架，结合几何索引空间记忆和新训练策略，显著提升自回归视频扩散模型在世界建模中（如Minecraft）的长期空间一致性和生成质量，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型在世界建模和交互式场景生成（如Minecraft）中表现出色。但面临挑战：如何在生成自然内容、探索新场景时，又能保持对已探索区域的空间一致性。在有限计算预算下，仅依靠时间记忆难以保证长期空间一致性；而增加空间记忆虽能增强一致性，但可能在上下文不足时影响新场景生成质量，存在一个权衡问题。

Method: 本文提出Memory Forcing学习框架，其核心是几何索引空间记忆，并包含以下训练协议和技术：
1.  **混合训练（Hybrid Training）**：区分游戏玩法模式，引导模型在探索时依赖时间记忆，在重访时结合空间记忆。
2.  **链式前向训练（Chained Forward Training）**：通过模型推演扩展自回归训练，生成更大的姿态变化，促使模型依赖空间记忆来保持一致性。
3.  **点到帧检索（Point-to-Frame Retrieval）**：通过将当前可见点映射到其源帧来高效检索历史信息。
4.  **增量3D重建（Incremental 3D Reconstruction）**：维护和更新一个显式的3D缓存。

Result: 广泛的实验表明，Memory Forcing在不同环境中均实现了卓越的长期空间一致性和生成质量，并且在处理长序列时保持了计算效率。

Conclusion: Memory Forcing框架成功解决了自回归视频扩散模型在世界建模中面临的长期空间一致性与新场景生成质量之间的权衡问题，提供了一种高效且高质量的解决方案。

Abstract: Autoregressive video diffusion models have proved effective for world
modeling and interactive scene generation, with Minecraft gameplay as a
representative application. To faithfully simulate play, a model must generate
natural content while exploring new scenes and preserve spatial consistency
when revisiting explored areas. Under limited computation budgets, it must
compress and exploit historical cues within a finite context window, which
exposes a trade-off: Temporal-only memory lacks long-term spatial consistency,
whereas adding spatial memory strengthens consistency but may degrade new scene
generation quality when the model over-relies on insufficient spatial context.
We present Memory Forcing, a learning framework that pairs training protocols
with a geometry-indexed spatial memory. Hybrid Training exposes distinct
gameplay regimes, guiding the model to rely on temporal memory during
exploration and incorporate spatial memory for revisits. Chained Forward
Training extends autoregressive training with model rollouts, where chained
predictions create larger pose variations and encourage reliance on spatial
memory for maintaining consistency. Point-to-Frame Retrieval efficiently
retrieves history by mapping currently visible points to their source frames,
while Incremental 3D Reconstruction maintains and updates an explicit 3D cache.
Extensive experiments demonstrate that Memory Forcing achieves superior
long-term spatial consistency and generative quality across diverse
environments, while maintaining computational efficiency for extended
sequences.

</details>


### [124] [MonSTeR: a Unified Model for Motion, Scene, Text Retrieval](https://arxiv.org/abs/2510.03200)
*Luca Collorone,Matteo Gioia,Massimiliano Pappa,Paolo Leoni,Giovanni Ficarra,Or Litany,Indro Spinelli,Fabio Galasso*

Main category: cs.CV

TL;DR: MonSTeR是一个新颖的多模态检索模型，首次实现运动、场景和文本之间的对齐评估，通过统一潜在空间在多任务中超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏评估骨骼运动（运动）、意图（文本）和周围上下文（场景）之间对齐的工具。

Method: 引入MonSTeR模型，它是首个运动-场景-文本检索模型。该模型受高阶关系建模启发，通过利用单模态和跨模态表示构建统一潜在空间，捕捉模态间的复杂依赖。

Result: MonSTeR的性能优于仅依赖单模态表示的三模态模型。通过用户研究验证了检索分数与人类偏好的一致性。同时，模型在零样本场景物体放置和运动字幕生成任务中展现了潜在空间的多功能性。

Conclusion: MonSTeR成功填补了评估运动、场景和文本对齐的工具空白，提供了一个灵活且鲁棒的多模态检索解决方案，并在多个任务中展现了优越性和多功能性。

Abstract: Intention drives human movement in complex environments, but such movement
can only happen if the surrounding context supports it. Despite the intuitive
nature of this mechanism, existing research has not yet provided tools to
evaluate the alignment between skeletal movement (motion), intention (text),
and the surrounding context (scene). In this work, we introduce MonSTeR, the
first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of
higher-order relations, MonSTeR constructs a unified latent space by leveraging
unimodal and cross-modal representations. This allows MonSTeR to capture the
intricate dependencies between modalities, enabling flexible but robust
retrieval across various tasks. Our results show that MonSTeR outperforms
trimodal models that rely solely on unimodal representations. Furthermore, we
validate the alignment of our retrieval scores with human preferences through a
dedicated user study. We demonstrate the versatility of MonSTeR's latent space
on zero-shot in-Scene Object Placement and Motion Captioning. Code and
pre-trained models are available at github.com/colloroneluca/MonSTeR.

</details>


### [125] [Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles](https://arxiv.org/abs/2510.03224)
*Dong Lao,Yuxiang Zhang,Haniyeh Ehsani Oskouie,Yangchao Wu,Alex Wong,Stefano Soatto*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、与架构和攻击类型无关的测试时防御机制，通过利用随机共振，引入微小的平移扰动来对抗对抗性攻击，同时最小化信息损失。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时防御方法（如特征过滤或平滑）会导致信息损失。研究旨在开发一种既能增强模型鲁棒性，又能最小化信息损失的防御机制，并适用于多样化的网络架构和任务，特别是此前缺乏通用测试时防御的密集预测任务。

Method: 该方法通过“以噪制噪”的策略，利用随机共振来增强鲁棒性。具体操作包括对输入图像引入微小的平移扰动，对转换后的特征嵌入进行对齐，聚合这些嵌入，然后映射回原始参考图像。该过程可由一个闭式公式表达，无需额外网络模块或针对特定攻击类型的微调，因此是无训练、架构无关且攻击类型无关的。

Result: 在图像分类任务上实现了最先进的鲁棒性。首次为立体匹配和光流等密集预测任务建立了通用的测试时防御机制。在各种对抗性攻击下，该方法能恢复高达68.1%的图像分类准确率损失，71.9%的立体匹配准确率损失，以及29.2%的光流准确率损失。

Conclusion: 所提出的测试时防御机制展现了卓越的通用性和实用性，在图像分类和密集预测任务中均能有效提升对抗性鲁棒性，且具有无需训练、与架构和攻击类型无关的优势。

Abstract: We propose a test-time defense mechanism against adversarial attacks:
imperceptible image perturbations that significantly alter the predictions of a
model. Unlike existing methods that rely on feature filtering or smoothing,
which can lead to information loss, we propose to "combat noise with noise" by
leveraging stochastic resonance to enhance robustness while minimizing
information loss. Our approach introduces small translational perturbations to
the input image, aligns the transformed feature embeddings, and aggregates them
before mapping back to the original reference image. This can be expressed in a
closed-form formula, which can be deployed on diverse existing network
architectures without introducing additional network modules or fine-tuning for
specific attack types. The resulting method is entirely training-free,
architecture-agnostic, and attack-agnostic. Empirical results show
state-of-the-art robustness on image classification and, for the first time,
establish a generic test-time defense for dense prediction tasks, including
stereo matching and optical flow, highlighting the method's versatility and
practicality. Specifically, relative to clean (unperturbed) performance, our
method recovers up to 68.1% of the accuracy loss on image classification, 71.9%
on stereo matching, and 29.2% on optical flow under various types of
adversarial attacks.

</details>


### [126] [MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition](https://arxiv.org/abs/2510.03228)
*Ricardo T. Fares,Lucas C. Ribas*

Main category: cs.CV

TL;DR: Mixer提出一种新型随机神经网络，结合超球面随机嵌入和双分支学习模块，解决了纹理识别中现有随机网络架构创新不足的问题，并在多个纹理基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有用于纹理识别的随机神经网络主要侧重于改进交叉信息预测，但在整体网络架构方面缺乏显著进展。

Method: 提出Mixer，一种新颖的随机神经网络用于纹理表示学习。该方法核心在于利用超球面随机嵌入结合双分支学习模块，以捕获通道内和通道间的关系，并通过新提出的优化问题来构建丰富的纹理表示。

Result: 在多个具有不同特性和挑战的纯纹理基准测试中，所提出的方法展示了令人满意的实验结果。

Conclusion: Mixer通过创新的网络架构和优化策略，有效提升了纹理表示学习能力，并在多项纹理识别任务中表现出色。

Abstract: Randomized neural networks for representation learning have consistently
achieved prominent results in texture recognition tasks, effectively combining
the advantages of both traditional techniques and learning-based approaches.
However, existing approaches have so far focused mainly on improving
cross-information prediction, without introducing significant advancements to
the overall randomized network architecture. In this paper, we propose Mixer, a
novel randomized neural network for texture representation learning. At its
core, the method leverages hyperspherical random embeddings coupled with a
dual-branch learning module to capture both intra- and inter-channel
relationships, further enhanced by a newly formulated optimization problem for
building rich texture representations. Experimental results have shown the
interesting results of the proposed approach across several pure texture
benchmarks, each with distinct characteristics and challenges. The source code
will be available upon publication.

</details>


### [127] [Improving GUI Grounding with Explicit Position-to-Coordinate Mapping](https://arxiv.org/abs/2510.03230)
*Suyuchen Wang,Tianyu Zhang,Ahmed Masry,Christopher Pal,Spandana Gella,Bang Liu,Perouz Taslakian*

Main category: cs.CV

TL;DR: 本文提出RULER tokens和Interleaved MRoPE（I-MRoPE），旨在解决现有视觉语言模型（VLM）在GUI定位任务中，面对高分辨率界面时像素映射不可靠的问题。通过提供显式空间指导而非隐式学习，该方法显著提升了定位精度，特别是在高分辨率场景下。


<details>
  <summary>Details</summary>
Motivation: GUI定位（将自然语言指令映射到像素坐标）对自主智能体至关重要，但现有VLM难以实现可靠的像素映射，尤其是在训练未见的高分辨率显示器上。核心瓶颈在于当前方法通过文本令牌直接从视觉特征生成坐标，迫使模型隐式推断复杂的坐标映射，导致在新分辨率上精度下降和失败率增高。

Method: 本文提出两项创新：
1.  **RULER tokens**：作为显式坐标标记，让模型像参照地图网格线一样定位和调整位置，而非从头生成坐标。
2.  **Interleaved MRoPE (I-MRoPE)**：通过确保宽度和高度维度被平等表示，改进了空间编码，解决了标准位置编码方案的非对称问题。

Result: 在ScreenSpot、ScreenSpot-V2和ScreenSpot-Pro数据集上的实验表明，本方法在定位精度上取得了持续的提升，尤其是在高分辨率界面上，改进最为显著。

Conclusion: 通过提供显式空间指导而非依赖隐式学习，本文提出的方法能够使GUI自动化在多样化的分辨率和平台上更加可靠。

Abstract: GUI grounding, the task of mapping natural-language instructions to pixel
coordinates, is crucial for autonomous agents, yet remains difficult for
current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which
breaks when extrapolating to high-resolution displays unseen during training.
Current approaches generate coordinates as text tokens directly from visual
features, forcing the model to infer complex position-to-pixel mappings
implicitly; as a result, accuracy degrades and failures proliferate on new
resolutions. We address this with two complementary innovations. First, RULER
tokens serve as explicit coordinate markers, letting the model reference
positions similar to gridlines on a map and adjust rather than generate
coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial
encoding by ensuring that width and height dimensions are represented equally,
addressing the asymmetry of standard positional schemes. Experiments on
ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in
grounding accuracy, with the largest improvements on high-resolution
interfaces. By providing explicit spatial guidance rather than relying on
implicit learning, our approach enables more reliable GUI automation across
diverse resolutions and platforms.

</details>


### [128] [LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models](https://arxiv.org/abs/2510.03232)
*Ci-Siang Lin,Min-Hung Chen,Yu-Yang Sheng,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: MLLMs在专业领域OOD任务上表现不佳。LEAML提出一种标签高效自适应框架，通过生成伪问答对和选择性神经元更新，使其在有限监督下优于标准微调。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在通用视觉基准测试上表现良好，但在医疗影像等专业领域的域外（OOD）任务上表现不佳，因为这些领域标注数据有限且昂贵。

Method: 引入LEAML，一个标签高效的自适应框架，利用稀缺的标注VQA样本和大量的无标注图像。该方法通过一个由图像描述蒸馏正则化的QA生成器，为无标注数据生成领域相关的伪问答对。关键在于，它选择性地仅更新与问答最相关的神经元，使QA生成器在蒸馏过程中能高效获取领域特定知识。

Result: 在胃肠道内窥镜和体育VQA任务上的实验表明，在最少监督下，LEAML始终优于标准微调。

Conclusion: LEAML框架是一种有效的标签高效自适应方法，能帮助MLLMs在专业OOD领域中，以有限的标签数据实现优于标准微调的性能。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance on
general visual benchmarks but struggle with out-of-distribution (OOD) tasks in
specialized domains such as medical imaging, where labeled data is limited and
expensive. We introduce LEAML, a label-efficient adaptation framework that
leverages both scarce labeled VQA samples and abundant unlabeled images. Our
approach generates domain-relevant pseudo question-answer pairs for unlabeled
data using a QA generator regularized by caption distillation. Importantly, we
selectively update only those neurons most relevant to question-answering,
enabling the QA Generator to efficiently acquire domain-specific knowledge
during distillation. Experiments on gastrointestinal endoscopy and sports VQA
demonstrate that LEAML consistently outperforms standard fine-tuning under
minimal supervision, highlighting the effectiveness of our proposed LEAML
framework.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [129] [BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks](https://arxiv.org/abs/2510.02418)
*Sagnik Anupam,Davis Brown,Shuo Li,Eric Wong,Hamed Hassani,Osbert Bastani*

Main category: cs.AI

TL;DR: 本文提出了BrowserArena，一个用于评估LLM网络代理在开放网络上表现的平台，通过用户任务和人工反馈，识别了验证码、弹窗和直接导航等常见故障模式，并揭示了现有代理的多样性和脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM网络代理的评估局限于沙盒环境或人工任务，无法真实反映其在开放网络上的性能和故障模式。

Method: 引入BrowserArena，一个实时的开放网络代理评估平台。该平台收集用户提交的任务，进行竞技场式（Arena-style）的头对头比较，并利用步骤级的人工反馈来发现故障模式。

Result: 通过分析代理轨迹的步骤级注释，确定了三个一致的故障模式：验证码解决、弹出横幅移除和直接导航到URL。进一步研究发现，不同语言模型处理这些故障模式的方式存在差异，例如o4-mini在规避验证码方面策略更多样，而DeepSeek-R1则经常误导用户关于验证码的解决。

Conclusion: 研究结果表明当前网络代理既具有多样性也存在脆弱性。此外，本文的基准测试方法为大规模评估和理解网络代理的故障模式提供了一种有效途径。

Abstract: LLM web agents now browse and take actions on the open web, yet current agent
evaluations are constrained to sandboxed environments or artificial tasks. We
introduce BrowserArena, a live open-web agent evaluation platform that collects
user-submitted tasks, runs Arena-style head-to-head comparisons, and uses
step-level human feedback to surface failure modes. Collecting and analyzing
step-level annotations on the agent traces, we identify three consistent
failure modes: captcha resolution, pop-up banner removal, and direct navigation
to URLs. By constructing targeted datasets to further study these tasks, we
discover variations in how different language models navigate these failure
modes. We find, for example, that o4-mini deploys a wider variety of strategies
to circumvent captcha resolution than other models and DeepSeek-R1 consistently
misleads users about captcha resolution. Our findings surface both the
diversity and brittleness of current web agents. More broadly, our benchmarking
methodology provides an approach to evaluating and understanding web agent
failure modes at scale.

</details>


### [130] [RefineShot: Rethinking Cinematography Understanding with Foundational Skill Evaluation](https://arxiv.org/abs/2510.02423)
*Hang Wu,Yujun Cai,Haonan Ge,Hongkai Chen,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: 本文指出电影摄影理解任务中现有基准ShotBench和SOTA模型ShotVL的评估可靠性问题，并提出了RefineShot，一个通过系统改进和扩展评估协议构建的、更可靠的基准，以促进未来研究。


<details>
  <summary>Details</summary>
Motivation: 电影摄影理解能力对多模态理解和电影媒体内容创作至关重要。然而，现有最全面的基准ShotBench存在选项设计模糊问题，而SOTA模型ShotVL在推理一致性和指令遵循方面表现不足，这导致评估不可靠，限制了公平比较并阻碍了该领域的进一步发展。

Method: 1. 通过一致的选项重构，系统性地改进了ShotBench。2. 首次对ShotVL的推理行为进行了批判性分析。3. 引入了一个扩展的评估协议，该协议联合评估任务准确性和核心模型能力。

Result: 这些努力共同产生了RefineShot，一个经过精炼和扩展的基准。

Conclusion: RefineShot能够实现更可靠的评估，并有望促进电影摄影理解领域的未来进步。

Abstract: Cinematography understanding refers to the ability to recognize not only the
visual content of a scene but also the cinematic techniques that shape
narrative meaning. This capability is attracting increasing attention, as it
enhances multimodal understanding in real-world applications and underpins
coherent content creation in film and media. As the most comprehensive
benchmark for this task, ShotBench spans a wide range of cinematic concepts and
VQA-style evaluations, with ShotVL achieving state-of-the-art results on it.
However, our analysis reveals that ambiguous option design in ShotBench and
ShotVL's shortcomings in reasoning consistency and instruction adherence
undermine evaluation reliability, limiting fair comparison and hindering future
progress. To overcome these issues, we systematically refine ShotBench through
consistent option restructuring, conduct the first critical analysis of
ShotVL's reasoning behavior, and introduce an extended evaluation protocol that
jointly assesses task accuracy and core model competencies. These efforts lead
to RefineShot, a refined and expanded benchmark that enables more reliable
assessment and fosters future advances in cinematography understanding.

</details>


### [131] [Safe and Efficient In-Context Learning via Risk Control](https://arxiv.org/abs/2510.02480)
*Andrea Wynn,Metod Jazbec,Charith Peris,Rinat Khaziev,Anqi Liu,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 本文提出一种新方法，通过结合无分布风险控制（DFRC）和动态提前退出预测，有效限制有害上下文示例对大语言模型性能的损害，同时提升有益示例的计算效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽能从少量上下文示例中学习新任务，但这种灵活性也带来安全隐患：LLMs可能受不正确或恶意示例影响。为应对恶意攻击（如未经监督的有害示例注入），需要设计内置机制来防范此类风险，限制有害示例对模型性能的负面影响。

Method: 首先，定义模型的零样本（zero-shot）性能作为“安全”基线。其次，应用无分布风险控制（DFRC）来限制上下文样本导致性能低于基线的程度。具体通过动态提前退出预测实现，忽略那些高度关注不安全输入的后续注意力头。最后，修改DFRC使其既能控制有害输入的风险，又能利用有益输入带来的性能和效率提升。

Result: 理论和实证结果均表明，该方法能有效控制有害上下文演示带来的风险，同时利用有益演示大幅提高计算效率。

Conclusion: 所提出的方法为LLMs提供了一种有效的风险控制机制，能够抵御有害上下文示例的攻击，并在处理有益示例时实现显著的计算效率提升，增强了模型的安全性和实用性。

Abstract: Large language models (LLMs) demonstrate a remarkable ability to learn new
tasks from a few in-context examples. However, this flexibility introduces
safety concerns: LLMs can be influenced by incorrect or malicious
demonstrations -- for example, if an adversary tampers with or injects harmful
examples without a human supervisor noticing. This motivates principled designs
in which the system itself includes built-in mechanisms to guard against such
attacks. We propose a novel approach to limit the degree to which harmful
demonstrations can degrade model performance. First, we define a baseline
``safe'' behavior for the model -- the model's performance given no in-context
demonstrations (zero-shot). Next, we apply distribution-free risk control
(DFRC) to control the extent to which in-context samples can decay performance
below zero-shot. We achieve this by leveraging dynamic early exit prediction,
ignoring later attention heads that attend the most to the unsafe inputs.
Finally, we propose modifications to DFRC that allow it to both control risk
for harmful inputs \textit{and} leverage performance and efficiency gains on
helpful inputs. We present both theoretical and empirical results showing that
our approach can effectively control risk for harmful in-context demonstrations
while simultaneously achieving substantial computational efficiency gains with
helpful demonstrations.

</details>


### [132] [Multimodal Function Vectors for Spatial Relations](https://arxiv.org/abs/2510.02528)
*Shuhao Fu,Esther Goldberg,Ying Nian Wu,Hongjing Lu*

Main category: cs.AI

TL;DR: 研究发现大型多模态模型（LMMs）通过特定的注意力头编码空间关系知识，这些知识可以被提取、操作和优化，以提高关系推理能力和泛化性。


<details>
  <summary>Details</summary>
Motivation: LMMs在上下文学习方面表现出色，但其支持任务学习的内部机制，尤其是在关系推理方面的机制，尚不明确。

Method: 基于LLMs的现有工作，研究人员通过因果中介分析识别了OpenFlamingo-4B模型中传输空间关系表示的特定注意力头。提取这些注意力头的激活作为“功能向量”，并在合成和真实图像数据集上进行零样本准确性测试。进一步对这些功能向量进行少量数据微调（保持LMM参数冻结），并探索它们线性组合以解决涉及新颖空间关系的类比问题的能力。

Result: 研究发现LMM中一小部分注意力头负责传输空间关系表示。提取的功能向量能提高零样本准确性，且经过少量数据微调后显著优于上下文学习基线。此外，关系特定的功能向量可以线性组合，成功解决涉及新颖且未训练过的空间关系的类比问题，展现出强大的泛化能力。

Conclusion: LMMs在局部内部结构中编码空间关系知识，这些知识可以被系统地提取和优化，从而增进对模型模块化的理解，并增强对LMMs关系推理的控制。

Abstract: Large Multimodal Models (LMMs) demonstrate impressive in-context learning
abilities from limited multimodal demonstrations, yet the internal mechanisms
supporting such task learning remain opaque. Building on prior work of large
language models, we show that a small subset of attention heads in the
vision-language model OpenFlamingo-4B is responsible for transmitting
representations of spatial relations. The activations of these attention heads,
termed function vectors, can be extracted and manipulated to alter an LMM's
performance on relational tasks. First, using both synthetic and real image
datasets, we apply causal mediation analysis to identify attention heads that
strongly influence relational predictions, and extract multimodal function
vectors that improve zero-shot accuracy at inference time. We further
demonstrate that these multimodal function vectors can be fine-tuned with a
modest amount of training data, while keeping LMM parameters frozen, to
significantly outperform in-context learning baselines. Finally, we show that
relation-specific function vectors can be linearly combined to solve analogy
problems involving novel and untrained spatial relations, highlighting the
strong generalization ability of this approach. Our results show that LMMs
encode spatial relational knowledge within localized internal structures, which
can be systematically extracted and optimized, thereby advancing our
understanding of model modularity and enhancing control over relational
reasoning in LMMs.

</details>


### [133] [Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research Challenge](https://arxiv.org/abs/2510.02557)
*Charlie Masters,Advaith Vellanki,Jiangbo Shangguan,Bart Kultys,Jonathan Gilmore,Alastair Moore,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 本文提出了一个自主经理智能体的研究愿景，旨在协调复杂的人机协作工作流，并发现当前智能体在多目标优化方面面临挑战，表明这是一个开放问题。


<details>
  <summary>Details</summary>
Motivation: 尽管代理AI在自动化单个任务方面取得进展，但管理复杂的、涉及动态人机团队的多智能体工作流仍然是一个重大挑战。

Method: 提出“自主经理智能体”概念，其功能包括将复杂目标分解为任务图、任务分配（人与AI）、进度监控、适应变化和透明沟通。将工作流管理形式化为部分可观察随机博弈，并提出了四个基础挑战。同时发布了开源仿真评估框架MA-Gym。

Result: 通过在20个工作流上评估基于GPT-5的经理智能体，发现它们难以同时优化目标完成、遵守约束和工作流运行时间。

Conclusion: 工作流管理是一个困难的开放问题，且自主管理系统具有重要的组织和伦理影响。

Abstract: While agentic AI has advanced in automating individual tasks, managing
complex multi-agent workflows remains a challenging problem. This paper
presents a research vision for autonomous agentic systems that orchestrate
collaboration within dynamic human-AI teams. We propose the Autonomous Manager
Agent as a core challenge: an agent that decomposes complex goals into task
graphs, allocates tasks to human and AI workers, monitors progress, adapts to
changing conditions, and maintains transparent stakeholder communication. We
formalize workflow management as a Partially Observable Stochastic Game and
identify four foundational challenges: (1) compositional reasoning for
hierarchical decomposition, (2) multi-objective optimization under shifting
preferences, (3) coordination and planning in ad hoc teams, and (4) governance
and compliance by design. To advance this agenda, we release MA-Gym, an
open-source simulation and evaluation framework for multi-agent workflow
orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we
find they struggle to jointly optimize for goal completion, constraint
adherence, and workflow runtime - underscoring workflow management as a
difficult open problem. We conclude with organizational and ethical
implications of autonomous management systems.

</details>


### [134] [Agentic Additive Manufacturing Alloy Discovery](https://arxiv.org/abs/2510.02567)
*Peter Pak,Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.AI

TL;DR: 本文提出利用大型语言模型（LLM）驱动的智能体系统，自动化并加速增材制造领域的合金发现过程。


<details>
  <summary>Details</summary>
Motivation: 增材制造（AM）中的合金发现是一个复杂挑战，需要材料科学、热力学模拟和实验分析等多领域专业知识。

Method: 开发了一个多智能体系统，利用LLM智能体的知识库，通过模型上下文协议（MCP）调用工具，执行如Thermo-Calc相图计算和熔合不足工艺图谱生成等操作。

Result: 该系统能够有效理解复杂用户指令，分析提议合金的可打印性；智能体能根据工具调用结果动态调整任务路径，实现实践环境中的自主决策。

Conclusion: LLM驱动的多智能体系统能够自动化并加速增材制造领域的合金发现任务，并展示了采用该多智能体系统的优势。

Abstract: Agentic systems enable the intelligent use of research tooling, augmenting a
researcher's ability to investigate and propose novel solutions to existing
problems. Within Additive Manufacturing (AM), alloy discovery remains a complex
challenge, often requiring expertise in the various domains of materials
science, thermodynamic simulations, and experimental analysis. Large Language
Model (LLM) enabled agents can facilitate this endeavor by utilizing their
extensive knowledge base to dispatch tool calls via Model Context Protocol
(MCP) to perform actions such as Thermo-Calc property diagram calculations and
lack of fusion process map generation. In addition, the multi-agent system
developed in this work is able to effectively reason through complex user
prompts and provide analysis on the printability of proposed alloys. These
agents can dynamically adjust their task trajectory to the outcomes of tool
call results, effectively enabling autonomous decision-making in practical
environments. This work aims to utilize LLM enabled agents to automate and
accelerate the task of alloy discovery within the field of additive
manufacturing and showcase the benefits of adopting this multi-agent system.

</details>


### [135] [A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container Stowage Planning Problem](https://arxiv.org/abs/2510.02589)
*Yunqi Huang,Nishith Chennakeshava,Alexis Carras,Vladislav Neverov,Wei Liu,Aske Plaat,Yingjie Fan*

Main category: cs.AI

TL;DR: 本文基准测试了多种强化学习算法在集装箱配载计划中的性能，并提供了一个包含起重机调度的可复用Gym环境。


<details>
  <summary>Details</summary>
Motivation: 集装箱配载计划（CSPP）对供应链效率至关重要且复杂，传统上依赖人工。尽管强化学习已应用于CSPP，但缺乏对不同算法的系统性基准比较。

Method: 开发了一个捕捉CSPP基本特征并包含起重机调度（多智能体和单智能体）的Gym环境。在此框架下，评估了DQN、QR-DQN、A2C、PPO和TRPO五种强化学习算法在不同复杂程度场景下的性能。

Result: 研究结果显示，随着复杂性的增加，不同算法的性能差距显著，强调了算法选择和问题制定对CSPP的重要性。

Conclusion: 本文对CSPP的多种强化学习方法进行了基准测试，并提供了一个可复用的Gym环境，为海事物流的未来研究和实际部署奠定了基础。

Abstract: Container stowage planning (CSPP) is a critical component of maritime
transportation and terminal operations, directly affecting supply chain
efficiency. Owing to its complexity, CSPP has traditionally relied on human
expertise. While reinforcement learning (RL) has recently been applied to CSPP,
systematic benchmark comparisons across different algorithms remain limited. To
address this gap, we develop a Gym environment that captures the fundamental
features of CSPP and extend it to include crane scheduling in both multi-agent
and single-agent formulations. Within this framework, we evaluate five RL
algorithms: DQN, QR-DQN, A2C, PPO, and TRPO under multiple scenarios of varying
complexity. The results reveal distinct performance gaps with increasing
complexity, underscoring the importance of algorithm choice and problem
formulation for CSPP. Overall, this paper benchmarks multiple RL methods for
CSPP while providing a reusable Gym environment with crane scheduling, thus
offering a foundation for future research and practical deployment in maritime
logistics.

</details>


### [136] [Multimodal Large Language Model Framework for Safe and Interpretable Grid-Integrated EVs](https://arxiv.org/abs/2510.02592)
*Jean Douglas Carvalho,Hugo Kenji,Ahmad Mohammad Saber,Glaucia Melo,Max Mauro Dias Santos,Deepa Kundur*

Main category: cs.AI

TL;DR: 本文提出了一个基于多模态大语言模型（LLM）的框架，用于处理电动汽车（EV）的多模态传感器数据，并生成自然语言警报以提高驾驶员在智能电网环境中的安全性和决策能力。


<details>
  <summary>Details</summary>
Motivation: 电动汽车与智能电网的整合为交通和能源系统带来了机遇，但确保驾驶员、车辆和环境之间安全且可解释的交互仍然是一个关键挑战。

Method: 该研究采用了一个多模态LLM框架，处理来自电动汽车的多模态传感器数据，包括目标检测（YOLOv8）、语义分割、地理编码定位和CAN总线遥测数据。该框架将原始传感器数据转换为自然语言警报，并通过真实世界的城市道路驾驶数据进行了验证。

Result: 研究结果表明，该框架能够有效地为驾驶员生成针对行人、骑行者和其他车辆接近等关键情况的上下文感知警报，从而弥合了原始传感器数据与驾驶员理解之间的鸿沟。这有助于在城市驾驶场景中实现更安全、更明智的决策。

Conclusion: LLM作为电动出行中的辅助工具具有巨大潜力，通过实现可扩展的车队协调、电动汽车负荷预测和交通感知能源规划，有益于交通系统和电网。

Abstract: The integration of electric vehicles (EVs) into smart grids presents unique
opportunities to enhance both transportation systems and energy networks.
However, ensuring safe and interpretable interactions between drivers,
vehicles, and the surrounding environment remains a critical challenge. This
paper presents a multi-modal large language model (LLM)-based framework to
process multimodal sensor data - such as object detection, semantic
segmentation, and vehicular telemetry - and generate natural-language alerts
for drivers. The framework is validated using real-world data collected from
instrumented vehicles driving on urban roads, ensuring its applicability to
real-world scenarios. By combining visual perception (YOLOv8), geocoded
positioning, and CAN bus telemetry, the framework bridges raw sensor data and
driver comprehension, enabling safer and more informed decision-making in urban
driving scenarios. Case studies using real data demonstrate the framework's
effectiveness in generating context-aware alerts for critical situations, such
as proximity to pedestrians, cyclists, and other vehicles. This paper
highlights the potential of LLMs as assistive tools in e-mobility, benefiting
both transportation systems and electric networks by enabling scalable fleet
coordination, EV load forecasting, and traffic-aware energy planning.
  Index Terms - Electric vehicles, visual perception, large language models,
YOLOv8, semantic segmentation, CAN bus, prompt engineering, smart grid.

</details>


### [137] [Mitigating Modal Imbalance in Multimodal Reasoning](https://arxiv.org/abs/2510.02608)
*Chen Henry Wu,Neil Kale,Aditi Raghunathan*

Main category: cs.AI

TL;DR: 基础模型在处理跨模态冲突时表现不佳，主要原因是注意力失衡；通过在训练中明确组合多模态数据，可以显著降低注意力失衡并提高性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型在实际应用中需要整合多种模态，但其在多模态交互和关联形成跨模态上下文时的联合推理能力，尤其是在跨模态冲突情景下，尚不清楚。研究旨在理解模型是优先处理单一模态还是进行联合推理以解决冲突。

Method: 研究通过构建跨模态冲突场景来测试基础模型，比较其在单模态和跨模态（包括跨语言）上下文下识别冲突的能力。通过分析注意力分数，追踪失败原因到跨模态注意力失衡。提出并测试了一种简单可扩展的方法：在每个训练实例中明确组合多种模态，以减少注意力失衡。

Result: 基础模型在单模态上下文中识别冲突的成功率为90%，但在跨模态冲突中下降至3%（跨语言上下文也有类似观察）。失败归因于跨模态注意力失衡，即模型对某些模态表现出极度不对称的注意力优先级。盲目扩展多模态或多语言数据集无法解决此问题，因为它们缺乏明确要求跨模态推理的训练示例。提出的明确组合多模态的训练方法显著降低了注意力失衡，从而直接提升了在多个视觉-语言基准测试上的下游性能。

Conclusion: 构建可靠的基础模型，必须系统性地解决跨模态上下文中的推理挑战，尤其是通过改善跨模态注意力平衡。

Abstract: Foundation models (FMs) deployed in real-world tasks such as computer-use
agents must integrate diverse modalities. How good are FMs at performing joint
reasoning, simultaneously reasoning over multiple modalities, especially when
the modalities interact and relate to each other to form cross-modal context?
To better understand this problem, we study FMs on cross-modal conflicts:
scenarios where conflicting evidence is presented across modalities. This
allows us to examine whether FMs prioritize one modality over another or reason
jointly to reconcile the conflict. Our experiments reveal that FMs can
recognize conflicts in unimodal contexts, composed of a single modality, 90% of
the time, but the ratio falls as low as 3% when evidence is split across
modalities -- similar observations hold in cross-lingual contexts, composed of
multiple languages. We trace this failure to cross-modal attention imbalance,
showing that FMs exhibit extreme asymmetry in attention scores,
disproportionately prioritizing certain modalities. We show that cross-modal
attention imbalance does not go away by simply scaling up multimodal or
multilingual datasets blindly, since they lack training examples that
explicitly require cross-modal reasoning. We demonstrate that even a simple and
scalable method of explicitly combining multiple modalities within each
training instance significantly reduces attention imbalance. Reduced attention
imbalance directly translates to improved downstream performance on several
vision-language benchmarks. Our findings underscore the importance of
systematically addressing cross-modal contexts to build reliable foundation
models.

</details>


### [138] [On the Role of Temperature Sampling in Test-Time Scaling](https://arxiv.org/abs/2510.02611)
*Yuheng Wu,Azalia Mirhoseini,Thierry Tambe*

Main category: cs.AI

TL;DR: LLM的测试时间缩放（TTS）在增加样本K后收益有限。本研究发现不同采样温度能解决不同问题子集，提出温度缩放方法，使LLM推理性能平均提升7.3分，并能达到与RL训练模型相当的水平，揭示了TTS的更大潜力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的测试时间缩放（TTS）方法在样本K值较大时性能提升停滞，无法解决部分难题。研究发现不同采样温度能解决不同的问题子集，表明单一温度缩放未能充分挖掘模型的潜在推理能力。

Method: 提出并实施“温度缩放”（Temperature Scaling）方法，即沿温度维度进行推理生成以扩大LLM的推理边界。此外，还设计了一种多温度投票方法以降低其操作开销。

Result: 在Qwen3系列模型（0.6B至8B）和五个推理基准上，温度缩放比单一温度TTS额外提升7.3分。该方法使基础模型达到与强化学习（RL）训练模型相当的性能，且无需进行额外的后训练。

Conclusion: 研究表明测试时间缩放（TTS）比以往认为的更强大。温度缩放提供了一种简单有效的方法，可以解锁基础大型语言模型的潜在推理能力。

Abstract: Large language models (LLMs) can improve reasoning at inference time through
test-time scaling (TTS), where multiple reasoning traces are generated and the
best one is selected. Prior work shows that increasing the number of samples K
steadily improves accuracy. In this paper, we demonstrate that this trend does
not hold indefinitely: at large K, further scaling yields no gains, and certain
hard questions remain unsolved regardless of the number of traces.
Interestingly, we find that different sampling temperatures solve different
subsets of problems, implying that single-temperature scaling explores only
part of a model's potential. We therefore propose scaling along the temperature
dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3
(0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME
2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an
additional 7.3 points over single-temperature TTS. Temperature scaling also
enables base models to reach performance comparable to reinforcement learning
(RL)-trained counterparts, without additional post-training. We further provide
a comprehensive analysis of this phenomenon and design a multi-temperature
voting method that reduces the overhead of temperature scaling. Overall, our
findings suggest that TTS is more powerful than previously thought, and that
temperature scaling offers a simple and effective way to unlock the latent
potential of base models.

</details>


### [139] [Geolog-IA: Conversational System for Academic Theses](https://arxiv.org/abs/2510.02653)
*Micaela Fuel Pozo,Andrea Guatumillo Saltos,Yeseña Tipan Llumiquinga,Kelly Lascano Aguirre,Marilyn Castillo Jara,Christian Mejia-Escobar*

Main category: cs.AI

TL;DR: 本文介绍了一种名为Geolog-IA的AI会话系统，它结合Llama 3.1、Gemini 2.5和RAG架构，用于自然回答厄瓜多尔中央大学地质学论文相关问题，并实现了高精度。


<details>
  <summary>Details</summary>
Motivation: 克服传统语言模型在回答特定领域（如地质学论文）问题时存在的幻觉和知识过时问题，并提供一个能准确、自然地检索信息的工具，以支持教育和研究。

Method: 开发了Geolog-IA系统，采用Llama 3.1和Gemini 2.5语言模型，结合检索增强生成（RAG）架构和SQLite数据库，并提供基于网络的直观用户界面。

Result: Geolog-IA在BLEU指标上的平均表现达到0.87，显示出生成回答的高一致性和准确性。

Conclusion: Geolog-IA是一个有效的工具，能为机构内的教育、培训和研究提供关键支持，并为未来在其他学科的应用奠定了基础。

Abstract: This study presents the development of Geolog-IA, a novel conversational
system based on artificial intelligence that responds naturally to questions
about geology theses from the Central University of Ecuador. Our proposal uses
the Llama 3.1 and Gemini 2.5 language models, which are complemented by a
Retrieval Augmented Generation (RAG) architecture and an SQLite database. This
strategy allows us to overcome problems such as hallucinations and outdated
knowledge. The evaluation of Geolog-IA's performance with the BLEU metric
reaches an average of 0.87, indicating high consistency and accuracy in the
responses generated. The system offers an intuitive, web-based interface that
facilitates interaction and information retrieval for directors, teachers,
students, and administrative staff at the institution. This tool can be a key
support in education, training, and research and establishes a basis for future
applications in other disciplines.

</details>


### [140] [A Concept of Possibility for Real-World Events](https://arxiv.org/abs/2510.02655)
*Daniel G. Schwartz*

Main category: cs.AI

TL;DR: 本文提出一种新的现实世界事件可能性概念，基于先决条件和约束的概率计算，适用于规划问题。


<details>
  <summary>Details</summary>
Motivation: 针对Zadeh于1978年提出的标准可能性概念，提出一个替代方案，旨在更具体地关注现实世界事件的可能性，并有望更好地模拟人类对计划的推理。

Method: 将事件的发生定义为具有先决条件和潜在阻碍的约束。事件的可能性被计算为先决条件成立和约束不成立的概率函数，形式上采用Łukasiewicz多值逻辑解释。

Result: 该可能性版本可应用于规划问题，用于确定多个备选方案中“最可能”或“最可行”的计划。理论得到详细阐述，并提供了一个车辆路线规划的说明性示例。

Conclusion: 这种可能性模型有望正确捕捉人类关于计划的正常推理模式，并暗示了潜在的未来应用价值。

Abstract: This paper offers a new concept of {\it possibility} as an alternative to the
now-a-days standard concept originally introduced by L.A. Zadeh in 1978. This
new version was inspired by the original but, formally, has nothing in common
with it other than that they both adopt the {\L}ukasiewicz multivalent
interpretation of the logical connectives. Moreover, rather than seeking to
provide a general notion of possibility, this focuses specifically on the
possibility of a real-world event. An event is viewed as having prerequisites
that enable its occurrence and constraints that may impede its occurrence, and
the possibility of the event is computed as a function of the probabilities
that the prerequisites hold and the constraints do not. This version of
possibility might appropriately be applied to problems of planning. When there
are multiple plans available for achieving a goal, this theory can be used to
determine which plan is most possible, i.e., easiest or most feasible to
complete. It is speculated that this model of reasoning correctly captures
normal human reasoning about plans. The theory is elaborated and an
illustrative example for vehicle route planning is provided. There is also a
suggestion of potential future applications.

</details>


### [141] [AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models](https://arxiv.org/abs/2510.02669)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Liu*

Main category: cs.AI

TL;DR: 本文提出AutoMaAS框架，通过自适应架构搜索和动态资源管理，优化基于大语言模型的多智能体系统的性能和成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统设计方法是单一的，无法根据查询复杂性和领域需求动态调整资源分配，导致效率低下。

Method: 引入AutoMaAS，一个自演进的多智能体架构搜索框架。它借鉴神经架构搜索原理，通过动态算子生命周期管理和自动化机器学习技术自动发现最优智能体配置。核心创新包括：基于性能-成本分析的算子自动生成、融合与消除；动态成本感知优化；在线反馈集成以持续优化；通过决策追踪增强可解释性。

Result: 在六个基准测试中，AutoMaAS相较于现有最佳方法，性能提升1.0-7.1%，推理成本降低3-5%。同时，该框架在不同数据集和LLM骨干网络上展现出卓越的可迁移性。

Conclusion: AutoMaAS为大语言模型时代自动化多智能体系统设计建立了新范式。

Abstract: Multi-agent systems powered by large language models have demonstrated
remarkable capabilities across diverse domains, yet existing automated design
approaches seek monolithic solutions that fail to adapt resource allocation
based on query complexity and domain requirements. This paper introduces
AutoMaAS, a self-evolving multi-agent architecture search framework that
leverages neural architecture search principles to automatically discover
optimal agent configurations through dynamic operator lifecycle management and
automated machine learning techniques. Our approach incorporates four key
innovations: (1) automatic operator generation, fusion, and elimination based
on performance-cost analysis, (2) dynamic cost-aware optimization with
real-time parameter adjustment, (3) online feedback integration for continuous
architecture refinement, and (4) enhanced interpretability through decision
tracing mechanisms. Extensive experiments across six benchmarks demonstrate
that AutoMaAS achieves 1.0-7.1\% performance improvement while reducing
inference costs by 3-5\% compared to state-of-the-art methods. The framework
shows superior transferability across datasets and LLM backbones, establishing
a new paradigm for automated multi-agent system design in the era of large
language models.

</details>


### [142] [ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks](https://arxiv.org/abs/2510.02677)
*Zhaorun Chen,Xun Liu,Mintong Kang,Jiawei Zhang,Minzhou Pan,Shuang Yang,Bo Li*

Main category: cs.AI

TL;DR: 本文提出ARMs，一个自适应红队代理，通过优化多样化攻击策略和引入11种新型多模态攻击方法，系统性地评估视觉语言模型(VLM)的安全漏洞。ARMs在攻击成功率上达到SOTA，并构建了ARMs-Bench数据集，用于提升VLM的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型(VLM)的普及，其多模态接口带来了新的安全漏洞，使得安全评估变得具有挑战性和关键性。现有红队工作受限于狭窄的对抗模式或依赖人工操作，缺乏对新兴真实世界VLM漏洞的可扩展探索能力。

Method: 研究提出ARMs（自适应红队代理），它针对目标有害行为或风险定义，通过推理增强的多步骤编排来优化多样化的红队策略，有效诱导目标VLM产生有害输出。ARMs提出了11种新颖的多模态攻击策略（如推理劫持、上下文伪装），并通过模型上下文协议（MCP）整合了17种红队算法。为平衡攻击的多样性和有效性，设计了分层记忆和epsilon-greedy攻击探索算法。此外，构建了ARMs-Bench数据集，包含超过3万个红队实例，涵盖51种多样化风险类别。

Result: 在实例和策略基准测试中，ARMs实现了SOTA的攻击成功率，平均超越基线52.1%，在Claude-4-Sonnet上超过90%。ARMs生成的红队实例多样性显著更高，揭示了VLM中新兴的漏洞。利用ARMs构建的ARMs-Bench数据集进行安全微调，显著提高了VLM的鲁棒性，同时保持了其通用能力。

Conclusion: ARMs提供了一种系统且全面的VLM风险评估方法，能有效发现VLM的多样化和新兴漏洞。ARMs-Bench为改善多模态安全对齐提供了宝贵的资源和可操作的指导，以应对新兴威胁。

Abstract: As vision-language models (VLMs) gain prominence, their multimodal interfaces
also introduce new safety vulnerabilities, making the safety evaluation
challenging and critical. Existing red-teaming efforts are either restricted to
a narrow set of adversarial patterns or depend heavily on manual engineering,
lacking scalable exploration of emerging real-world VLM vulnerabilities. To
bridge this gap, we propose ARMs, an adaptive red-teaming agent that
systematically conducts comprehensive risk assessments for VLMs. Given a target
harmful behavior or risk definition, ARMs automatically optimizes diverse
red-teaming strategies with reasoning-enhanced multi-step orchestration, to
effectively elicit harmful outputs from target VLMs. We propose 11 novel
multimodal attack strategies, covering diverse adversarial patterns of VLMs
(e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming
algorithms into ARMs via model context protocol (MCP). To balance the diversity
and effectiveness of the attack, we design a layered memory with an
epsilon-greedy attack exploration algorithm. Extensive experiments on instance-
and policy-based benchmarks show that ARMs achieves SOTA attack success rates,
exceeding baselines by an average of 52.1% and surpassing 90% on
Claude-4-Sonnet. We show that the diversity of red-teaming instances generated
by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs.
Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety
dataset comprising over 30K red-teaming instances spanning 51 diverse risk
categories, grounded in both real-world multimodal threats and regulatory
risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness
of VLMs while preserving their general utility, providing actionable guidance
to improve multimodal safety alignment against emerging threats.

</details>


### [143] [Automated Constraint Specification for Job Scheduling by Regulating Generative Model with Domain-Specific Representation](https://arxiv.org/abs/2510.02679)
*Yu-Zhe Shi,Qiao Xu,Yanjia Li,Mingchen Liu,Huamin Qu,Lecheng Ruan,Qining Wang*

Main category: cs.AI

TL;DR: 本文提出一种以约束为中心的架构，结合自动化场景适应算法，规范大语言模型（LLMs）实现生产调度的可靠自动化约束规范，显著优于纯LLM方法。


<details>
  <summary>Details</summary>
Motivation: 现代制造的先进规划与调度（APS）系统中，将制造需求转化为形式化约束仍是耗时且人工密集的过程。尽管LLMs有望自动化此过程，但其模糊性、非确定性和领域知识不足限制了直接应用。

Method: 提出一种以约束为中心的架构来规范LLMs，实现可靠的自动化约束规范。该架构定义了一个三层分层结构空间，并通过领域特定表示确保精度和可靠性。此外，设计并部署了自动化生产场景适应算法，以高效定制架构适应特定制造配置。

Result: 实验结果表明，所提出的方法成功平衡了LLMs的生成能力与制造系统的可靠性要求，在约束规范任务中显著优于纯LLM方法。

Conclusion: 该研究提供了一种有效方案，解决了LLMs在自动化生产调度约束规范中的挑战，提升了其在制造系统中的可靠性和应用价值。

Abstract: Advanced Planning and Scheduling (APS) systems have become indispensable for
modern manufacturing operations, enabling optimized resource allocation and
production efficiency in increasingly complex and dynamic environments. While
algorithms for solving abstracted scheduling problems have been extensively
investigated, the critical prerequisite of specifying manufacturing
requirements into formal constraints remains manual and labor-intensive.
Although recent advances of generative models, particularly Large Language
Models (LLMs), show promise in automating constraint specification from
heterogeneous raw manufacturing data, their direct application faces challenges
due to natural language ambiguity, non-deterministic outputs, and limited
domain-specific knowledge. This paper presents a constraint-centric
architecture that regulates LLMs to perform reliable automated constraint
specification for production scheduling. The architecture defines a
hierarchical structural space organized across three levels, implemented
through domain-specific representation to ensure precision and reliability
while maintaining flexibility. Furthermore, an automated production scenario
adaptation algorithm is designed and deployed to efficiently customize the
architecture for specific manufacturing configurations. Experimental results
demonstrate that the proposed approach successfully balances the generative
capabilities of LLMs with the reliability requirements of manufacturing
systems, significantly outperforming pure LLM-based approaches in constraint
specification tasks.

</details>


### [144] [NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning](https://arxiv.org/abs/2510.02816)
*Yulong Zhang,Li Wang,Wei Du,Peilin Li,Yuqin Dai Zhiyuan Zhao,Lingyong Fang,Ziniu Liu,Ru Zhang,Huijia Zhu,Gongshen Liu*

Main category: cs.AI

TL;DR: NCV是一种无需训练的框架，通过节点级一致性检查验证LLM多步推理，实现错误精确定位，显著提高F1分数并大幅减少token使用量。


<details>
  <summary>Details</summary>
Motivation: 验证大型语言模型的多步推理面临错误定位不精确和高昂token成本的挑战。现有方法存在注意力分散或多样本采样成本高的问题。

Method: 提出节点级一致性验证（NCV）框架。NCV将验证重构为轻量级的节点级二元一致性检查，通过将思维链分解为相互连接的验证节点，精确地定位错误并避免不必要的长文本生成。

Result: 实验证明NCV增强了解释性和效率，提供了一个可扩展的LLM推理验证方案。在公开数据集上，NCV的F1分数比基线提高了10%至25%，同时token使用量比传统方法（如CoT验证器）减少了6到58倍。

Conclusion: NCV提供了一种有效且高效的解决方案，能够可靠地验证LLM的多步推理，同时提高错误定位精度、解释性和计算效率。

Abstract: Verifying multi-step reasoning in large language models is difficult due to
imprecise error localization and high token costs. Existing methods either
assess entire reasoning chains, suffering attention dilution, or rely on
expensive multi-sampling. We introduce Node-wise Consistency Verification
(NCV), a training-free framework that recasts verification as lightweight
binary consistency checks at the node level. By decomposing the chain of
thought into interconnected verification nodes, NCV precisely localizes errors
and avoids unnecessary long-form generation. Experiments demonstrate that our
approach enhances interpretability and efficiency, presenting a scalable
solution for reliable LLM reasoning verification. On public datasets, NCV
achieves a 10\% to 25\% improvement in F1 scores over baselines while utilizing
$6\times$~$58\times$ fewer tokens than traditional methods like CoT-based
verifiers.

</details>


### [145] [Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents](https://arxiv.org/abs/2510.02837)
*Wonjoong Kim,Sangwu Park,Yeonjun In,Sein Kim,Dongha Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: 本文提出了TRACE框架，通过证据库实现对工具增强型LLM代理解决问题轨迹的多维度、可扩展且经济高效的评估，超越了传统答案匹配的局限。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强型LLM基准的评估方法仅限于答案匹配，未能评估代理在解决复杂请求时的效率、幻觉和适应性等轨迹方面。直接标注所有有效轨迹成本过高，而简单的LLM评估器在缺乏真实轨迹时难以详细评估。

Method: 引入TRACE框架，用于多维度评估工具增强型LLM代理的性能。TRACE通过整合“证据库”来积累先前推理步骤的知识，从而实现对代理推理轨迹的多方面分析和评估。为验证框架，开发了一个新的元评估数据集，该数据集通过增强现有基准，包含多样化和有缺陷的轨迹，并标注了多方面性能分数。

Result: 研究结果证实，TRACE能够以可扩展且经济高效的方式准确评估复杂的代理行为，即使是使用小型开源LLM。此外，将该方法应用于评估代理在解决工具增强型任务时生成的轨迹，并发现了之前未报告的观察结果及相应见解。

Conclusion: TRACE框架提供了一种有效且经济高效的解决方案，能够对工具增强型LLM代理的复杂问题解决轨迹进行多维度评估，显著提升了评估的深度和广度，并能发现新的行为模式和洞察。

Abstract: Although recent tool-augmented benchmarks incorporate complex user requests
and diverse tools, the evaluation methods for most of them remain limited to
answer matching. However, as the number of steps required to resolve a user
request increases, a proper evaluation of an agent's performance must go beyond
the final answer to also assess the problem-solving trajectory, including
previously ignored aspects such as efficiency, hallucination, and adaptivity.
The most straightforward method for evaluating these aspects is to compare an
agent's trajectory with the ground-truth trajectory, but this approach is
fundamentally limited since annotating all valid ground-truth trajectories is
prohibitively expensive. However, a simple LLM-based evaluator struggles to
assess trajectories in detail without ground truth. To effectively evaluate the
agents in this manner, we introduce TRACE, a framework for the
multi-dimensional evaluation of tool-augmented LLM agent performance. By
incorporating an evidence bank, which accumulates knowledge gathered from
preceding reasoning steps, TRACE enables a multi-faceted analysis and
evaluation of an agent's reasoning trajectory effectively. To validate our
framework, we develop a new meta-evaluation dataset by augmenting existing
benchmarks with diverse and flawed trajectories, each labeled with
multi-faceted performance scores. Our results confirm that TRACE accurately
evaluates these complex behaviors in a scalable and cost-effective manner, even
with small open-source LLMs. Furthermore, we apply our method to evaluate the
trajectories that agents produce while solving tool-augmented tasks, presenting
previously unreported observations and their corresponding insights.

</details>


### [146] [Take Goodhart Seriously: Principled Limit on General-Purpose AI Optimization](https://arxiv.org/abs/2510.02840)
*Antoine Maier,Aude Maier,Tom David*

Main category: cs.AI

TL;DR: 机器学习中“模型满足其目标函数”的假设（OSA）是错误的，因为存在技术误差和目标错位。这可能导致古德哈特定律失效，并需要对优化进行限制，以避免失控。


<details>
  <summary>Details</summary>
Motivation: 机器学习中“模型满足其指定目标函数”这一常见但未被充分审视的假设（OSA）在实际条件下会失效，其影响被忽视。研究旨在揭示OSA失败的原因（逼近、估计、优化误差以及意图错配）及其深远影响。

Method: 采用与学习范式无关的框架进行论证，并结合最新的数学成果，指出由于缺乏对OSA差距的数学表征，这些差距在强优化压力下与古德哈特定律失效模式无异。

Result: 目标满足假设（OSA）在实际中系统性失败。由于这些差距无法被数学表征，它们在强优化下与古德哈特定律失效模式无法区分。因此，需要对通用人工智能系统的优化施加有原则的限制。

Conclusion: 如果不对通用人工智能系统施加有原则的优化限制，持续优化将可能导致系统可预测且不可逆转地失去控制。

Abstract: A common but rarely examined assumption in machine learning is that training
yields models that actually satisfy their specified objective function. We call
this the Objective Satisfaction Assumption (OSA). Although deviations from OSA
are acknowledged, their implications are overlooked. We argue, in a
learning-paradigm-agnostic framework, that OSA fails in realistic conditions:
approximation, estimation, and optimization errors guarantee systematic
deviations from the intended objective, regardless of the quality of its
specification. Beyond these technical limitations, perfectly capturing and
translating the developer's intent, such as alignment with human preferences,
into a formal objective is practically impossible, making misspecification
inevitable. Building on recent mathematical results, absent a mathematical
characterization of these gaps, they are indistinguishable from those that
collapse into Goodhart's law failure modes under strong optimization pressure.
Because the Goodhart breaking point cannot be located ex ante, a principled
limit on the optimization of General-Purpose AI systems is necessary. Absent
such a limit, continued optimization is liable to push systems into predictable
and irreversible loss of control.

</details>


### [147] [Reward Model Routing in Alignment](https://arxiv.org/abs/2510.02850)
*Xinle Wu,Yao Lu*

Main category: cs.AI

TL;DR: 本文提出BayesianRouter，一种结合离线学习和在线贝叶斯选择的奖励模型（RM）路由框架，旨在解决大型语言模型（LLM）对齐中单一RM限制及现有路由方法的冷启动和探索不足问题，并取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐范式（RLHF/RLAIF）主要依赖单一奖励模型，这限制了对齐质量并增加了过拟合风险。现有奖励模型路由方法虽试图解决此问题，但面临冷启动和探索不足的挑战。

Method: 提出BayesianRouter混合路由框架：1. **离线阶段**：训练多任务路由器，根据偏好数据估计每个RM的可靠性。2. **在线阶段**：采用贝叶斯Thompson采样路由器进行逐查询RM选择，使用离线嵌入作为高斯先验初始化RM权重，并根据在线奖励自适应更新后验以适应策略分布。

Result: 在指令遵循（如AlpacaEval-2）和推理（如GSM8K）等广泛基准测试中，BayesianRouter持续优于单个RM、RM集成以及现有路由方法。

Conclusion: BayesianRouter通过结合离线学习和在线贝叶斯选择，有效克服了LLM对齐中奖励模型路由的局限性，显著提升了对齐质量和性能。

Abstract: Reinforcement learning from human or AI feedback (RLHF / RLAIF) has become
the standard paradigm for aligning large language models (LLMs). However, most
pipelines rely on a single reward model (RM), limiting alignment quality and
risking overfitting. Recent work explores RM routing--dynamically selecting an
RM from a candidate pool to exploit complementary strengths while maintaining
$O(1)$ RM calls--but existing methods suffer from cold-start and insufficient
exploration. We propose BayesianRouter, a hybrid routing framework that
combines offline RM strengths learning with online Bayesian selection. In the
offline stage, a multi-task router is trained on preference data to estimate
per-RM reliability. In the online stage, a Bayesian Thompson sampling router
performs per-query RM selection, initializing RM-specific weight vectors with
offline embeddings as Gaussian priors and adaptively updating their posteriors
with online rewards to adapt to the evolving policy distribution. Extensive
experiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and
reasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently
outperforms individual RMs, RM ensembling, and existing routing methods.

</details>


### [148] [Consolidating Reinforcement Learning for Multimodal Discrete Diffusion Models](https://arxiv.org/abs/2510.02880)
*Tianren Ma,Mu Zhang,Yibing Wang,Qixiang Ye*

Main category: cs.AI

TL;DR: 本研究提出了MaskGRPO，这是首个在离散扩散模型中实现可扩展多模态强化学习的有效方法，通过理论基础澄清和定制化rollout策略，解决了现有强化学习方法优化DDM的挑战，并在推理和生成任务上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 使用奖励优化离散扩散模型（DDM）仍面临挑战：DDM的非自回归范式使得重要性采样难以处理，rollout过程复杂，这困扰了Group Relative Policy Optimization（GRPO）等强化学习方法。

Method: 引入MaskGRPO方法。首先阐明DDM的理论基础，以构建捕获有效token波动的梯度更新重要性估计器。其次，为视觉序列精心定制rollout方法，以产生多样化的完成和可靠的优化梯度。

Result: MaskGRPO带来了更稳定和高效的更新，在数学推理、编码和视觉生成基准测试中，实现了更强的推理性能和更好的生成质量。

Conclusion: MaskGRPO被确立为一种系统的策略优化方法，为离散化视觉扩散提供了首个实用的解决方案。

Abstract: Optimizing discrete diffusion model (DDM) with rewards remains a challenge:
the non-autoregressive paradigm makes importance sampling intractable and
rollout complex, puzzling reinforcement learning methods such as Group Relative
Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first
viable approach to enable scalable multimodal reinforcement learning in
discrete diffusion with effective importance sampling and modality-specific
adaptations. To this end, we first clarify the theoretical foundation for DDMs,
which facilitates building an importance estimator that captures valuable token
fluctuation for gradient updates. We then delicately tailored the rollout
method for visual sequences, which yields diverse completions and reliable
optimization gradients. Upon math reasoning, coding, and visual generation
benchmarks, MaskGRPO brings more stable and efficient updates, leading to
stronger reasoning performance and better generation quality. This study
establishes MaskGRPO as a systematic policy optimization approach and the first
practical way for discretized visual diffusion.

</details>


### [149] [Onto-Epistemological Analysis of AI Explanations](https://arxiv.org/abs/2510.02996)
*Martina Mattioli,Eike Petersen,Aasa Feragen,Marcello Pelillo,Siavash A. Bigdeli*

Main category: cs.AI

TL;DR: 深度学习的黑箱特性限制了其应用。可解释人工智能（XAI）旨在解决此问题，但其底层哲学假设常被忽视。本文分析XAI方法中的本体论和认识论假设，指出其重要性，并指导如何为不同领域选择合适的XAI方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习的黑箱性质限制了其信任度和广泛应用。尽管可解释人工智能（XAI）旨在提供解释，但其方法的设计者往往是技术背景，未充分考虑“解释”概念背后的深刻哲学（本体论和认识论）假设，而这些假设对解释的有效性和解读具有重要影响。

Method: 本文通过调查和分析应用于AI系统时，可解释性方法中隐含的本体论和认识论假设，即我们对解释的存在以及获取解释知识能力所做的假设。

Result: 研究发现，XAI方法中看似微小的技术变化可能对应着关于解释的底层假设的重大差异。此外，本文强调了在为特定应用选择XAI方法时，忽视底层本体论-认识论范式的潜在风险。

Conclusion: 在为特定应用选择XAI方法时，必须充分理解和考虑其内在的本体论和认识论假设，以避免风险，确保解释的有效性和适用性。本文还探讨了如何为不同领域选择和调整合适的XAI方法。

Abstract: Artificial intelligence (AI) is being applied in almost every field. At the
same time, the currently dominant deep learning methods are fundamentally
black-box systems that lack explanations for their inferences, significantly
limiting their trustworthiness and adoption. Explainable AI (XAI) methods aim
to overcome this challenge by providing explanations of the models' decision
process. Such methods are often proposed and developed by engineers and
scientists with a predominantly technical background and incorporate their
assumptions about the existence, validity, and explanatory utility of different
conceivable explanatory mechanisms. However, the basic concept of an
explanation -- what it is, whether we can know it, whether it is absolute or
relative -- is far from trivial and has been the subject of deep philosophical
debate for millennia. As we point out here, the assumptions incorporated into
different XAI methods are not harmless and have important consequences for the
validity and interpretation of AI explanations in different domains. We
investigate ontological and epistemological assumptions in explainability
methods when they are applied to AI systems, meaning the assumptions we make
about the existence of explanations and our ability to gain knowledge about
those explanations. Our analysis shows how seemingly small technical changes to
an XAI method may correspond to important differences in the underlying
assumptions about explanations. We furthermore highlight the risks of ignoring
the underlying onto-epistemological paradigm when choosing an XAI method for a
given application, and we discuss how to select and adapt appropriate XAI
methods for different domains of application.

</details>


### [150] [From Facts to Foils: Designing and Evaluating Counterfactual Explanations for Smart Environments](https://arxiv.org/abs/2510.03078)
*Anna Trapp,Mersedeh Sadeghi,Andreas Vogelsang*

Main category: cs.AI

TL;DR: 本文提出并实现了首个针对规则型智能环境的反事实解释框架，并通过用户研究发现用户对反事实解释和因果解释的偏好具有情境依赖性。


<details>
  <summary>Details</summary>
Motivation: 规则型智能环境中缺乏生成反事实解释的成熟方法，尽管反事实解释在可解释AI中是一个强大的工具。

Method: 提出了针对规则型智能环境的反事实解释的首个形式化和实现，作为现有解释引擎的插件。并通过用户研究（N=17）评估其与传统因果解释的效果。

Result: 用户偏好具有高度情境依赖性：因果解释因其语言简洁和时间压力下受青睐，而反事实解释因其可操作性内容受青睐，尤其是在用户希望解决问题时。

Conclusion: 本文为智能环境中的新型解释贡献了一个实用框架，并提供了经验证据以指导何时选择何种解释类型最有效。

Abstract: Explainability is increasingly seen as an essential feature of rule-based
smart environments. While counterfactual explanations, which describe what
could have been done differently to achieve a desired outcome, are a powerful
tool in eXplainable AI (XAI), no established methods exist for generating them
in these rule-based domains. In this paper, we present the first formalization
and implementation of counterfactual explanations tailored to this domain. It
is implemented as a plugin that extends an existing explanation engine for
smart environments. We conducted a user study (N=17) to evaluate our generated
counterfactuals against traditional causal explanations. The results show that
user preference is highly contextual: causal explanations are favored for their
linguistic simplicity and in time-pressured situations, while counterfactuals
are preferred for their actionable content, particularly when a user wants to
resolve a problem. Our work contributes a practical framework for a new type of
explanation in smart environments and provides empirical evidence to guide the
choice of when each explanation type is most effective.

</details>


### [151] [A Study of Rule Omission in Raven's Progressive Matrices](https://arxiv.org/abs/2510.03127)
*Binze Li*

Main category: cs.AI

TL;DR: 本研究评估了现代AI模型（如Transformer和视觉模型）在不完整训练下对RPM任务的泛化能力，发现它们在面对未见规则时性能显著下降，表明其推理机制可能停留在模式识别而非真正的抽象推理。


<details>
  <summary>Details</summary>
Motivation: 探讨AI模型在类比推理（RPM任务）上的成功是否源于真正的推理能力，还是仅仅依赖统计捷径，特别是通过测试它们在不完全规则训练下的泛化能力。

Method: 通过在训练中故意省略部分结构规则，评估了序列到序列的Transformer模型以及CoPINet和Dual-Contrast Network等视觉架构，在Impartial-RAVEN (I-RAVEN)数据集上的表现。

Result: 实验显示，Transformer模型在熟悉规则上表现强劲，但在面对新颖或被省略的规则时，准确率急剧下降。此外，token级别准确率与完整答案准确率之间的差距也突显了现有方法的局限性。

Conclusion: 当前深度学习模型在抽象推理机制上存在根本性限制，主要依赖模式识别。这强调了需要开发超越模式识别、实现鲁棒抽象推理的新架构。

Abstract: Analogical reasoning lies at the core of human cognition and remains a
fundamental challenge for artificial intelligence. Raven's Progressive Matrices
(RPM) serve as a widely used benchmark to assess abstract reasoning by
requiring the inference of underlying structural rules. While many vision-based
and language-based models have achieved success on RPM tasks, it remains
unclear whether their performance reflects genuine reasoning ability or
reliance on statistical shortcuts. This study investigates the generalization
capacity of modern AI systems under conditions of incomplete training by
deliberately omitting several structural rules during training. Both
sequence-to-sequence transformer models and vision-based architectures such as
CoPINet and the Dual-Contrast Network are evaluated on the Impartial-RAVEN
(I-RAVEN) dataset. Experiments reveal that although transformers demonstrate
strong performance on familiar rules, their accuracy declines sharply when
faced with novel or omitted rules. Moreover, the gap between token-level
accuracy and complete answer accuracy highlights fundamental limitations in
current approaches. These findings provide new insights into the reasoning
mechanisms underlying deep learning models and underscore the need for
architectures that move beyond pattern recognition toward robust abstract
reasoning.

</details>


### [152] [Improving Cooperation in Collaborative Embodied AI](https://arxiv.org/abs/2510.03153)
*Hima Jacob Leven Suprabha,Laxmi Nag Laxminarayan Nagesh,Ajith Nair,Alvin Reuben Amal Selvaster,Ayan Khan,Raghuram Damarla,Sanju Hannah Samuel,Sreenithi Saravana Perumal,Titouan Puech,Venkataramireddy Marella,Vishal Sonar,Alessandro Suglia,Oliver Lemon*

Main category: cs.AI

TL;DR: 本文通过探索提示方法和集成语音功能，增强了CoELA框架，旨在提升LLM驱动多智能体系统的协作性能和决策效率。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何利用大语言模型（LLMs）来增强多智能体系统中的协作推理和合作能力，特别是通过评估不同的提示方法来优化智能体的协作行为和决策。

Method: 本研究增强了CoELA框架，该框架利用LLM进行多智能体通信、推理和任务协调。通过系统的实验，评估了不同的LLM和提示工程策略，以识别最大化协作性能的优化组合。此外，研究还通过集成语音功能，实现了无缝的基于语音的协作交互。

Result: 研究结果表明，提示优化能有效提升协作智能体的性能；例如，最佳组合将使用Gemma3运行的系统效率比原始CoELA系统提高了22%。此外，语音集成提供了一个更具吸引力的用户界面，有利于迭代系统开发和演示。

Conclusion: 提示优化在提升LLM驱动多智能体协作性能方面表现出显著效果，而语音集成则能提供更具吸引力的用户界面，共同促进了协作AI系统的发展。

Abstract: The integration of Large Language Models (LLMs) into multiagent systems has
opened new possibilities for collaborative reasoning and cooperation with AI
agents. This paper explores different prompting methods and evaluates their
effectiveness in enhancing agent collaborative behaviour and decision-making.
We enhance CoELA, a framework designed for building Collaborative Embodied
Agents that leverage LLMs for multi-agent communication, reasoning, and task
coordination in shared virtual spaces. Through systematic experimentation, we
examine different LLMs and prompt engineering strategies to identify optimised
combinations that maximise collaboration performance. Furthermore, we extend
our research by integrating speech capabilities, enabling seamless
collaborative voice-based interactions. Our findings highlight the
effectiveness of prompt optimisation in enhancing collaborative agent
performance; for example, our best combination improved the efficiency of the
system running with Gemma3 by 22% compared to the original CoELA system. In
addition, the speech integration provides a more engaging user interface for
iterative system development and demonstrations.

</details>


### [153] [CoDA: Agentic Systems for Collaborative Data Visualization](https://arxiv.org/abs/2510.03194)
*Zichen Chen,Jiefeng Chen,Sercan Ö. Arik,Misha Sra,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: 本文提出CoDA，一个多智能体系统，通过专门的LLM智能体实现从自然语言查询到可视化的自动化，解决了现有系统在复杂数据集和迭代细化方面的不足，并在性能上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 数据科学家在手动创建可视化上耗费大量时间，现有的自然语言查询自动化系统难以处理复杂的多文件数据集和迭代细化。现有方法常过分简化任务，无法有效管理数据复杂度、代码错误或最终可视化质量。

Method: 将可视化自动化重构为协作式多智能体问题，引入了CoDA系统。CoDA采用专门的LLM智能体进行元数据分析、任务规划、代码生成和自我反思。该系统通过元数据分析绕过token限制，并通过质量驱动的细化确保鲁棒性。

Result: CoDA在综合得分上取得了显著提升，性能比竞争基线高出高达41.5%。

Conclusion: 可视化自动化的未来在于集成协作的智能体工作流，而非孤立的代码生成。

Abstract: Deep research has revolutionized data analysis, yet data scientists still
devote substantial time to manually crafting visualizations, highlighting the
need for robust automation from natural language queries. However, current
systems struggle with complex datasets containing multiple files and iterative
refinement. Existing approaches, including simple single- or multi-agent
systems, often oversimplify the task, focusing on initial query parsing while
failing to robustly manage data complexity, code errors, or final visualization
quality. In this paper, we reframe this challenge as a collaborative
multi-agent problem. We introduce CoDA, a multi-agent system that employs
specialized LLM agents for metadata analysis, task planning, code generation,
and self-reflection. We formalize this pipeline, demonstrating how
metadata-focused analysis bypasses token limits and quality-driven refinement
ensures robustness. Extensive evaluations show CoDA achieves substantial gains
in the overall score, outperforming competitive baselines by up to 41.5%. This
work demonstrates that the future of visualization automation lies not in
isolated code generation but in integrated, collaborative agentic workflows.

</details>


### [154] [Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner](https://arxiv.org/abs/2510.03206)
*Cai Zhou,Chenxiao Yang,Yi Hu,Chenyu Wang,Chubin Zhang,Muhan Zhang,Lester Mackey,Tommi Jaakkola,Stephen Bates,Dinghuai Zhang*

Main category: cs.AI

TL;DR: 连续扩散模型理论表达力强但实践中表现不佳。本文提出CCDD，一个联合连续-离散扩散模型，通过结合两种模态解决了训练难题，并在语言建模任务中取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 尽管连续扩散语言模型在理论表达能力上优于离散模型和循环Transformer，但其实际性能普遍不如离散模型。这种矛盾源于连续扩散模型在从连续表示空间解码到离散token空间时的训练困难。

Method: 提出协同演化连续离散扩散（Coevolutionary Continuous Discrete Diffusion, CCDD）模型。CCDD在一个连续表示空间和离散token空间的并集上定义了联合多模态扩散过程，利用单个模型同时在这两个空间去噪。此外，还提出了有效的架构及先进的训练/采样技术。

Result: CCDD在广泛的真实世界语言建模实验中展现出强大的经验性能。它兼具潜在空间丰富的语义表达力、良好的可训练性和高质量的样本生成能力。

Conclusion: 通过在联合空间中结合连续和离散模态，CCDD成功地弥合了连续扩散模型理论表达力与实际训练难度之间的差距，为语言建模提供了一种高性能解决方案。

Abstract: Diffusion language models, especially masked discrete diffusion models, have
achieved great success recently. While there are some theoretical and primary
empirical results showing the advantages of latent reasoning with looped
transformers or continuous chain-of-thoughts, continuous diffusion models
typically underperform their discrete counterparts. In this paper, we argue
that diffusion language models do not necessarily need to be in the discrete
space. In particular, we prove that continuous diffusion models have stronger
expressivity than discrete diffusions and looped transformers. We attribute the
contradiction between the theoretical expressiveness and empirical performance
to their practical trainability: while continuous diffusion provides
intermediate supervision that looped transformers lack, they introduce
additional difficulty decoding tokens into the discrete token space from the
continuous representation space. We therefore propose Coevolutionary Continuous
Discrete Diffusion (CCDD), which defines a joint multimodal diffusion process
on the union of a continuous representation space and a discrete token space,
leveraging a single model to simultaneously denoise in the joint space. By
combining two modalities, CCDD is expressive with rich semantics in the latent
space, as well as good trainability and sample quality with the help of
explicit discrete tokens. We also propose effective architectures and advanced
training/sampling techniques for CCDD, which reveals strong empirical
performance in extensive language modeling experiments on real-world tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [155] [Extreme value forecasting using relevance-based data augmentation with deep learning models](https://arxiv.org/abs/2510.02407)
*Junru Hua,Rahul Ahluwalia,Rohitash Chandra*

Main category: cs.LG

TL;DR: 本文提出了一个基于深度学习（Conv-LSTM, BD-LSTM）和数据增强（GANs, SMOTE）的极值预测框架，旨在解决极值预测中的数据稀疏问题。研究发现SMOTE策略在不同预测周期下表现优异，且Conv-LSTM和BD-LSTM在不同类型数据集上展现互补优势。


<details>
  <summary>Details</summary>
Motivation: 极值预测是一个具有挑战性的领域，在金融到气候变化等多个领域有广泛应用。为解决极值数据稀疏导致的预测困难，本文旨在探索利用生成对抗网络（GANs）等数据增强技术与深度学习模型结合，以提高极值预测的准确性。

Method: 本研究构建了一个极值预测的数据增强框架，结合了卷积长短期记忆网络（Conv-LSTM）和双向长短期记忆网络（BD-LSTM）等深度学习模型，以及GANs和SMOTE等数据增强模型进行多步预测。研究评估了不同数据增强模型在整体及极值区域预测准确性、计算效率方面的适用性，并提出了基于相关性函数的新颖策略来整合数据增强，特别关注极值。

Result: 研究结果表明，基于SMOTE的策略在短周期和长周期预测中始终展现出卓越的适应性，从而提升了性能。Conv-LSTM和BD-LSTM表现出互补优势：Conv-LSTM在周期性、稳定的数据集中表现出色，而BD-LSTM在混沌或非平稳序列中表现更优。

Conclusion: SMOTE策略是极值预测数据增强的有效方法，能够一致性地提升预测性能。深度学习模型（Conv-LSTM和BD-LSTM）应根据数据集的特性选择使用，以发挥各自的最佳预测能力，为极值预测提供了一个有效的框架和实用指导。

Abstract: Data augmentation with generative adversarial networks (GANs) has been
popular for class imbalance problems, mainly for pattern classification and
computer vision-related applications. Extreme value forecasting is a
challenging field that has various applications from finance to climate change
problems. In this study, we present a data augmentation framework for extreme
value forecasting. In this framework, our focus is on forecasting extreme
values using deep learning models in combination with data augmentation models
such as GANs and synthetic minority oversampling technique (SMOTE). We use deep
learning models such as convolutional long short-term memory (Conv-LSTM) and
bidirectional long short-term memory (BD-LSTM) networks for multistep ahead
prediction featuring extremes. We investigate which data augmentation models
are the most suitable, taking into account the prediction accuracy overall and
at extreme regions, along with computational efficiency. We also present novel
strategies for incorporating data augmentation, considering extreme values
based on a relevance function. Our results indicate that the SMOTE-based
strategy consistently demonstrated superior adaptability, leading to improved
performance across both short- and long-horizon forecasts. Conv-LSTM and
BD-LSTM exhibit complementary strengths: the former excels in periodic, stable
datasets, while the latter performs better in chaotic or non-stationary
sequences.

</details>


### [156] [OpenTSLM: Time-Series Language Models for Reasoning over Multivariate Medical Text- and Time-Series Data](https://arxiv.org/abs/2510.02410)
*Patrick Langer,Thomas Kaar,Max Rosenblattl,Maxwell A. Xu,Winnie Chow,Martin Maritsch,Aradhana Verma,Brian Han,Daniel Seung Kim,Henry Chubb,Scott Ceresnak,Aydin Zahedivash,Alexander Tarlochan Singh Sandhu,Fatima Rodriguez,Daniel McDuff,Elgar Fleisch,Oliver Aalami,Filipe Barata,Paul Schmiedmayer*

Main category: cs.LG

TL;DR: OpenTSLM通过将时间序列作为原生模态集成到预训练LLM中，解决了LLM处理时间序列数据的局限性，并在多模态时间序列推理任务中显著超越现有模型和GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理多模态数据和医学信息方面潜力巨大，但存在无法有效处理时间序列数据这一重大局限性，限制了其在临床应用中的深度和广度。

Method: 提出了OpenTSLM，一个将时间序列作为原生模态集成到预训练LLM中的时间序列语言模型家族。研究了两种架构：OpenTSLM-SoftPrompt（通过软提示将可学习时间序列token与文本token拼接，隐式建模）和OpenTSLM-Flamingo（通过交叉注意力将时间序列与文本集成，显式建模）。并在HAR-CoT、Sleep-CoT和ECG-QA-CoT三个新数据集上，使用文本-时间序列链式思考（CoT）推理任务对模型进行基准测试。

Result: OpenTSLM模型在所有基准测试任务中均优于基线模型。例如，在睡眠分期中达到69.9 F1，在HAR中达到65.4，远高于微调的纯文本模型的9.05和52.2。即使是10亿参数的OpenTSLM模型也超越了GPT-4o（15.47和2.95 F1）。OpenTSLM-Flamingo在性能上与OpenTSLM-SoftPrompt相当，但在更长序列上表现更优，并保持稳定的内存需求（例如，与SoftPrompt的110 GB相比，仅需40 GB VRAM），解决了SoftPrompt内存随序列长度指数增长的问题。临床专家对OpenTSLMs在ECG-QA上的推理能力给予高度评价。

Conclusion: OpenTSLM家族成功地将时间序列作为原生模态集成到预训练LLM中，有效解决了LLM处理时间序列数据的核心局限性。这些模型在多模态时间序列推理任务中展现出卓越的性能，显著超越现有基线和先进的通用LLM，为未来数字健康和时间序列分析领域的应用奠定了基础，并通过开源代码、数据集和模型促进进一步研究。

Abstract: LLMs have emerged as powerful tools for interpreting multimodal data. In
medicine, they hold particular promise for synthesizing large volumes of
clinical information into actionable insights and digital health applications.
Yet, a major limitation remains their inability to handle time series. To
overcome this gap, we present OpenTSLM, a family of Time Series Language Models
(TSLMs) created by integrating time series as a native modality to pretrained
LLMs, enabling reasoning over multiple time series of any length. We
investigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt,
models time series implicitly by concatenating learnable time series tokens
with text tokens via soft prompting. Although parameter-efficient, we
hypothesize that explicit time series modeling scales better and outperforms
implicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time
series with text via cross-attention. We benchmark both variants against
baselines that treat time series as text tokens or plots, across a suite of
text-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three
datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models
outperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR,
compared to 9.05 and 52.2 for finetuned text-only models. Notably, even
1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo
matches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences,
while maintaining stable memory requirements. By contrast, SoftPrompt grows
exponentially in memory with sequence length, requiring around 110 GB compared
to 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by
clinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA.
To facilitate further research, we provide all code, datasets, and models
open-source.

</details>


### [157] [RainSeer: Fine-Grained Rainfall Reconstruction via Physics-Guided Modeling](https://arxiv.org/abs/2510.02414)
*Lin Chen,Jun Chen,Minghui Qiu,Shuxin Zhong,Binghong Chen,Kaishun Wu*

Main category: cs.LG

TL;DR: 本文提出RainSeer框架，利用雷达反射率作为物理结构先验，重建高分辨率降雨场，解决了现有方法过度平滑的问题，并在两个公开数据集上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有空间插值方法（基于AWS或增强卫星/雷达观测）在重建高分辨率降雨场时，常过度平滑关键结构，未能捕捉剧烈变化和局部极端情况，而这些对于洪水预报、水文建模和气候分析至关重要。

Method: 引入RainSeer，一个结构感知重建框架，将雷达反射率重新解释为物理基础的结构先验。该框架包含两阶段架构：(i) Structure-to-Point Mapper通过双向映射将中尺度雷达结构投影到局部地面降雨，实现空间对齐；(ii) Geo-Aware Rain Decoder通过因果时空注意力机制捕捉水凝物在下降、融化和蒸发过程中的语义转化。

Result: 在RAIN-F（韩国）和MeteoNet（法国）两个公开数据集上进行评估，RainSeer相比现有SOTA基线持续改进，MAE降低超过13.31%，并显著增强了重建降雨场的结构保真度。

Conclusion: RainSeer通过将雷达反射率作为物理结构先验，有效地解决了高分辨率降雨场重建中现有方法的局限性，实现了更精确、结构保真度更高的降雨预测。

Abstract: Reconstructing high-resolution rainfall fields is essential for flood
forecasting, hydrological modeling, and climate analysis. However, existing
spatial interpolation methods-whether based on automatic weather station (AWS)
measurements or enhanced with satellite/radar observations often over-smooth
critical structures, failing to capture sharp transitions and localized
extremes. We introduce RainSeer, a structure-aware reconstruction framework
that reinterprets radar reflectivity as a physically grounded structural
prior-capturing when, where, and how rain develops. This shift, however,
introduces two fundamental challenges: (i) translating high-resolution
volumetric radar fields into sparse point-wise rainfall observations, and (ii)
bridging the physical disconnect between aloft hydro-meteors and ground-level
precipitation. RainSeer addresses these through a physics-informed two-stage
architecture: a Structure-to-Point Mapper performs spatial alignment by
projecting mesoscale radar structures into localized ground-level rainfall,
through a bidirectional mapping, and a Geo-Aware Rain Decoder captures the
semantic transformation of hydro-meteors through descent, melting, and
evaporation via a causal spatiotemporal attention mechanism. We evaluate
RainSeer on two public datasets-RAIN-F (Korea, 2017-2019) and MeteoNet (France,
2016-2018)-and observe consistent improvements over state-of-the-art baselines,
reducing MAE by over 13.31% and significantly enhancing structural fidelity in
reconstructed rainfall fields.

</details>


### [158] [How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models](https://arxiv.org/abs/2510.02453)
*Parth Asawa,Alan Zhu,Matei Zaharia,Alexandros G. Dimakis,Joseph E. Gonzalez*

Main category: cs.LG

TL;DR: 本文提出“顾问模型”(Advisor Models)，这是一种轻量级、参数化的策略，通过强化学习训练，能够动态地为黑盒基础模型提供上下文自然语言指令，以适应不同输入和环境，从而提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒基础模型的定制仅限于静态提示，无法适应不同的输入、用户或环境。静态提示优化器产生的固定提示缺乏灵活性。

Method: 引入“顾问模型”，它是一个小的第二模型，通过强化学习训练，根据环境的奖励信号，在输入和黑盒模型之间生成动态的、上下文内的自然语言指导指令。这使得模型能够进行每实例的行为塑造。

Result: 顾问模型在推理和个性化等多个领域超越了静态提示优化器，能发现环境动态并提高下游任务性能。它们还展现了跨黑盒模型的泛化能力，并在实现专业化的同时保持了对分布外输入的鲁棒性。

Conclusion: 顾问模型为黑盒系统提供了一个可学习的接口，通过动态优化，为实现个性化和环境适应性强的AI提供了一个有前景的方向。

Abstract: Foundation models are increasingly deployed as black-box services, where
model weights cannot be modified and customization is limited to prompting.
While static prompt optimization has shown promise, it produces a single fixed
prompt that fails to adapt to different inputs, users, or environments. We
introduce Advisor Models, lightweight parametric policies trained with
reinforcement learning to reactively issue natural language steering
instructions in-context to black-box models. The advisor is a second small
model that sits between the input and the model, shaping behavior on a
per-instance basis using reward signals from the environment. Across multiple
domains involving reasoning and personalization, we show that Advisor Models
outperform static prompt optimizers, discovering environment dynamics and
improving downstream task performance. We also demonstrate the generalizability
of advisors by transferring them across black-box models, as well as the
framework's ability to achieve specialization while retaining robustness to
out-of-distribution inputs. Viewed more broadly, Advisor Models provide a
learnable interface to black-box systems where the advisor acts as a
parametric, environment-specific memory. We argue that dynamic optimization of
black-box models via Advisor Models is a promising direction for enabling
personalization and environment-adaptable AI with frontier-level capabilities.

</details>


### [159] [Market-Based Data Subset Selection -- Principled Aggregation of Multi-Criteria Example Utility](https://arxiv.org/abs/2510.02456)
*Ashish Jha,Valentin Leplat,AH Phan*

Main category: cs.LG

TL;DR: 提出一种基于市场机制的数据选择器，通过预测市场对训练数据示例进行定价，有效整合异构信号，实现高效、稳定的数据子集选择，并在多个任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统训练数据子集选择方法难以有效整合不确定性、稀有性、多样性等异构示例效用信号，且常采用特设权重，导致选择困难。

Method: 提出基于成本函数预测市场（LMSR）的市场化选择器，将信号视为交易者，通过单一流动性参数控制集中度，并采用主题归一化。通过令牌预算规则处理长度偏置，引入轻量级多样性头部提升覆盖率。理论上，LMSR实现指数加权的极大熵聚合。

Result: 在GSM8K数据集上，该市场化方法在6万令牌预算下，在多样性方面与强单信号基线持平，降低了种子方差，选择开销<0.1 GPU小时。在AGNews数据集上（保留5-25%数据），实现了有竞争力的准确性，并提高了平衡性和稳定性。

Conclusion: 该框架为固定计算资源下的多信号数据整理提供了一个统一的解决方案，适用于提示级推理和分类任务。

Abstract: Selecting a small yet useful subset of training data is hard because signals
of example utility (uncertainty, rarity, diversity, etc.) are heterogeneous and
typically combined with ad hoc weights. We propose a market-based selector that
prices each example via a cost-function prediction market (LMSR), signals act
as traders, a single liquidity parameter controls concentration, and topic-wise
normalization stabilizes calibration. Token budgets are handled explicitly by a
price-per-token rule $\rho=p/\ell^{\gamma}$, with $\gamma$ exposing an
interpretable length bias; a lightweight diversity head improves coverage. We
quantify coverage via topic cluster coverage and effective sample size. On the
theory side, we show that LMSR implements a maximum-entropy aggregation with
exponential weighting and a convex objective, yielding transparent knobs for
aggregation strength. Empirically, on GSM8K (60k-token budget) the market with
diversity achieves parity with strong single-signal baselines while reducing
seed variance and incurring $<\!0.1$ GPU-hr selection overhead; on AGNews at
kept=5-25\% the market (with light balancing) delivers competitive accuracy
with improved balance and stability. The framework unifies multi-signal data
curation under fixed compute for prompt-level reasoning and classification.

</details>


### [160] [Assessing the Potential for Catastrophic Failure in Dynamic Post-Training Quantization](https://arxiv.org/abs/2510.02457)
*Logan Frank,Paul Ardis*

Main category: cs.LG

TL;DR: 研究动态训练后量化（PTQ）在最坏输入情况下的极端失效问题。通过结合知识蒸馏和强化学习，找到并分析导致灾难性性能下降的网络-策略对，发现精度最高可下降65%，强调实际部署需谨慎。


<details>
  <summary>Details</summary>
Motivation: PTQ虽能有效降低神经网络的计算和内存开销，但在不同输入分布下可能导致严重的性能下降。特别是在安全关键环境中，理解这种潜在性能下降的程度及其原因至关重要。

Method: 探索由动态PTQ引起的极端失效，并构建一个结合知识蒸馏和强化学习的任务。该任务旨在学习一个网络和位宽策略对，从而在最坏情况潜力下分析量化引起的灾难性失效。

Result: 证实了“有害”网络-策略对的存在，这些对的精度下降幅度在10-65%之间，远高于“鲁棒”对（下降小于2%）。此外，研究还初步探索了最高脆弱点。

Conclusion: 研究结果强调了在实际部署PTQ时需保持谨慎。本工作呼吁未来在深度学习领域进行更严格的鲁棒性检查，并更加重视安全考量。

Abstract: Post-training quantization (PTQ) has recently emerged as an effective tool
for reducing the computational complexity and memory usage of a neural network
by representing its weights and activations with lower precision. While this
paradigm has shown great success in lowering compute and storage costs, there
is the potential for drastic performance reduction depending upon the
distribution of inputs experienced in inference. When considering possible
deployment in safety-critical environments, it is important to investigate the
extent of potential performance reduction, and what characteristics of input
distributions may give rise to this reduction. In this work, we explore the
idea of extreme failure stemming from dynamic PTQ and formulate a knowledge
distillation and reinforcement learning task to learn a network and bit-width
policy pair such that catastrophic failure under quantization is analyzed in
terms of worst case potential. Our results confirm the existence of this
"detrimental" network-policy pair, with several instances demonstrating
performance reductions in the range of 10-65% in accuracy, compared to their
"robust" counterparts encountering a <2% decrease. From systematic
experimentation and analyses, we also provide an initial exploration into
points at highest vulnerability. While our results represent an initial step
toward understanding failure cases introduced by PTQ, our findings ultimately
emphasize the need for caution in real-world deployment scenarios. We hope this
work encourages more rigorous examinations of robustness and a greater emphasis
on safety considerations for future works within the broader field of deep
learning.

</details>


### [161] [SAGE: Streaming Agreement-Driven Gradient Sketches for Representative Subset Selection](https://arxiv.org/abs/2510.02470)
*Ashish Jha,Salman Ahmadi-Asl*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Training modern neural networks on large datasets is computationally and
energy intensive. We present SAGE, a streaming data-subset selection method
that maintains a compact Frequent Directions (FD) sketch of gradient geometry
in $O(\ell D)$ memory and prioritizes examples whose sketched gradients align
with a consensus direction. The approach eliminates $N \times N$ pairwise
similarities and explicit $N \times \ell$ gradient stores, yielding a simple
two-pass, GPU-friendly pipeline. Leveraging FD's deterministic approximation
guarantees, we analyze how agreement scoring preserves gradient energy within
the principal sketched subspace. Across multiple benchmarks, SAGE trains with
small kept-rate budgets while retaining competitive accuracy relative to
full-data training and recent subset-selection baselines, and reduces
end-to-end compute and peak memory. Overall, SAGE offers a practical,
constant-memory alternative that complements pruning and model compression for
efficient training.

</details>


### [162] [Uncertainty-Guided Model Selection for Tabular Foundation Models in Biomolecule Efficacy Prediction](https://arxiv.org/abs/2510.02476)
*Jie Li,Andrew McCarthy,Zhizhuo Zhang,Stephen Young*

Main category: cs.LG

TL;DR: 本研究提出并验证了一种基于模型不确定性（IQR）的策略，用于在无真实标签的情况下选择集成学习模型，以优化生物分子功效预测。结果表明，这种不确定性引导的集成模型在siRNA敲低功效任务中表现优异，甚至超越了专业预测器。


<details>
  <summary>Details</summary>
Motivation: 上下文学习器（如TabPFN）在生物分子功效预测中很有前景，但其性能对提供的上下文高度敏感。后验模型集成是一种可行方法，但如何在缺乏真实标签的情况下选择最佳集成模型是一个亟待解决的问题。

Method: 研究采用不确定性引导的模型选择策略。使用TabPFN模型结合简单的序列特征进行siRNA敲低功效预测，并将模型预测的四分位距（IQR）作为其不确定性度量。通过选择并平均具有最低平均IQR的模型集合来优化性能。

Result: 在siRNA敲低功效任务中，TabPFN模型（使用简单序列特征）能够超越专业的最先进预测器。模型预测的IQR与真实预测误差呈负相关。通过选择和平均具有最低平均IQR的模型集合，实现了优于简单集成或单一模型的性能。

Conclusion: 模型不确定性是优化生物分子功效预测的一种强大且无需真实标签的启发式方法。

Abstract: In-context learners like TabPFN are promising for biomolecule efficacy
prediction, where established molecular feature sets and relevant experimental
results can serve as powerful contextual examples. However, their performance
is highly sensitive to the provided context, making strategies like post-hoc
ensembling of models trained on different data subsets a viable approach. An
open question is how to select the best models for the ensemble without access
to ground truth labels. In this study, we investigate an uncertainty-guided
strategy for model selection. We demonstrate on an siRNA knockdown efficacy
task that a TabPFN model using simple sequence-based features can surpass
specialized state-of-the-art predictors. We also show that the model's
predicted inter-quantile range (IQR), a measure of its uncertainty, has a
negative correlation with true prediction error. By selecting and averaging an
ensemble of models with the lowest mean IQR, we achieve superior performance
compared to naive ensembling or using a single model trained on all available
data. This finding highlights model uncertainty as a powerful, label-free
heuristic for optimizing biomolecule efficacy predictions.

</details>


### [163] [Litespark Technical Report: High-Throughput, Energy-Efficient LLM Training Framework](https://arxiv.org/abs/2510.02483)
*Nii Osae Osae Dade,Moinul Hossain Rahat*

Main category: cs.LG

TL;DR: Litespark通过优化Transformer的注意力层和MLP层，显著提升了大型语言模型（LLM）的预训练效率，实现了2-6倍的训练吞吐量提升和55%-83%的能耗降低。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的训练面临耗时漫长和能耗巨大的挑战，现代模型需要数月的计算和吉瓦时的电力。

Method: 引入名为Litespark的新型预训练框架，通过对Transformer的注意力层和MLP层进行有针对性的优化来解决低效率问题。该方法结合了架构改进和算法增强，以最大化模型浮点运算利用率（MFU），同时保持与标准Transformer实现的兼容性。

Result: 在3B和30B参数的Llama模型上，使用SlimPajama-627B数据集进行全面基准测试，结果显示：在多节点H200 GPU集群上，训练吞吐量提高了2-6倍，能耗降低了55%-83%。

Conclusion: Litespark框架显著提升了LLM的训练效率和能效。这些优化方法具有模型和硬件无关性，适用于各种Transformer架构，并可扩展到包括监督微调和直接偏好优化在内的后期训练阶段。

Abstract: Training Large Language Models (LLMs) is plagued by long training times and
massive energy consumption, with modern models requiring months of computation
and gigawatt-hours of electricity. In light of these challenges,we introduce
Litespark, a novel pre-training framework that addresses these inefficiencies
through targeted optimizations to transformer attention and MLP layers. Our
approach combines architectural improvements with algorithmic enhancements to
maximize Model FLOPs Utilization (MFU) while maintaining compatibility with
standard transformer implementations. Comprehensive benchmarking on 3B and 30B
parameter Llama models using the SlimPajama-627B dataset demonstrates
substantial performance gains: 2x-6x training throughput improvement and
$55\%-83$% energy consumption reduction across multi-node H200 GPU clusters.
These optimizations are model- and hardware-agnostic, enabling broad
applicability across transformer architectures and extending to post-training
phases including supervised fine-tuning and direct preference optimization.

</details>


### [164] [From Pixels to Factors: Learning Independently Controllable State Variables for Reinforcement Learning](https://arxiv.org/abs/2510.02484)
*Rafael Rodriguez-Sanchez,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: 现有分解马尔可夫决策过程算法样本效率高但需预知分解结构，深度强化学习可处理高维观测但无法利用该结构。本文提出行动可控分解（ACF），一种对比学习方法，能从像素观测中发现独立可控的潜在变量，并优于基线算法。


<details>
  <summary>Details</summary>
Motivation: 分解马尔可夫决策过程算法在样本效率方面表现优异，但其前提是已知分解表示，这在智能体仅能获取高维观测（如像素）时无法满足。而深度强化学习虽能处理高维输入，却无法利用分解结构来提升效率。因此，研究动机是解决如何从高维观测中学习到可利用的分解表示。

Method: 本文提出行动可控分解（Action-Controllable Factorization, ACF），这是一种基于对比学习的方法，旨在发现独立可控的潜在变量（即每个动作可单独影响的状态组件）。ACF利用了动作的稀疏性特点：动作通常只影响部分变量，而其余变量则根据环境动态演变，这为对比训练提供了信息丰富的数据。

Result: ACF能够直接从像素观测中恢复出真实的可控因子。在三个具有已知分解结构的基准测试（Taxi、FourRooms和MiniGrid-DoorKey）上，ACF始终优于基线解耦算法，表现出卓越的性能。

Conclusion: ACF成功解决了从高维观测中学习行动可控分解表示的问题，有效结合了分解马尔可夫决策过程的效率优势与深度强化学习处理高维输入的能力。实验结果表明，ACF能恢复出真实的可控因子，并持续超越现有解耦算法，为强化学习中的表示学习提供了一条有效途径。

Abstract: Algorithms that exploit factored Markov decision processes are far more
sample-efficient than factor-agnostic methods, yet they assume a factored
representation is known a priori -- a requirement that breaks down when the
agent sees only high-dimensional observations. Conversely, deep reinforcement
learning handles such inputs but cannot benefit from factored structure. We
address this representation problem with Action-Controllable Factorization
(ACF), a contrastive learning approach that uncovers independently controllable
latent variables -- state components each action can influence separately. ACF
leverages sparsity: actions typically affect only a subset of variables, while
the rest evolve under the environment's dynamics, yielding informative data for
contrastive training. ACF recovers the ground truth controllable factors
directly from pixel observations on three benchmarks with known factored
structure -- Taxi, FourRooms, and MiniGrid-DoorKey -- consistently
outperforming baseline disentanglement algorithms.

</details>


### [165] [Improved Robustness of Deep Reinforcement Learning for Control of Time-Varying Systems by Bounded Extremum Seeking](https://arxiv.org/abs/2510.02490)
*Shaifalee Saxena,Alan Williams,Rafael Fierro,Alexander Scheinker*

Main category: cs.LG

TL;DR: 本文提出了一种结合深度强化学习（DRL）和鲁棒模型独立有界极值寻优（ES）的混合控制器，以提高非线性时变系统控制器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在系统模型快速变化时性能急剧下降，而有界极值寻优虽然能处理时变系统，但随着参数增加收敛速度变慢且易陷入局部最优。

Method: 将深度强化学习（DRL）与有界极值寻优（ES）结合，形成一个混合控制器。DRL利用历史数据快速控制多参数系统，ES确保对时变系统的鲁棒性。

Result: 数值研究表明，该混合控制器的性能优于两者的单独表现。DRL擅长快速控制多参数系统，ES则保证了对时间变化的鲁棒性。并在一个通用时变系统和洛斯阿拉莫斯中子科学中心线性粒子加速器的低能束流传输段的自动调谐中进行了演示。

Conclusion: ES-DRL混合控制器能够实现对非线性时变系统的快速且鲁棒的控制，其性能优于DRL或ES单独使用。

Abstract: In this paper, we study the use of robust model independent bounded extremum
seeking (ES) feedback control to improve the robustness of deep reinforcement
learning (DRL) controllers for a class of nonlinear time-varying systems. DRL
has the potential to learn from large datasets to quickly control or optimize
the outputs of many-parameter systems, but its performance degrades
catastrophically when the system model changes rapidly over time. Bounded ES
can handle time-varying systems with unknown control directions, but its
convergence speed slows down as the number of tuned parameters increases and,
like all local adaptive methods, it can get stuck in local minima. We
demonstrate that together, DRL and bounded ES result in a hybrid controller
whose performance exceeds the sum of its parts with DRL taking advantage of
historical data to learn how to quickly control a many-parameter system to a
desired setpoint while bounded ES ensures its robustness to time variations. We
present a numerical study of a general time-varying system and a combined
ES-DRL controller for automatic tuning of the Low Energy Beam Transport section
at the Los Alamos Neutron Science Center linear particle accelerator.

</details>


### [166] [Beyond Imitation: Recovering Dense Rewards from Demonstrations](https://arxiv.org/abs/2510.02493)
*Jiangnan Li,Thuy-Trang Vu,Ehsan Abbasnejad,Gholamreza Haffari*

Main category: cs.LG

TL;DR: 本文证明了监督微调（SFT）与逆向强化学习的等价性，提出SFT隐式学习了一个密集的token级奖励模型。研究展示了如何从SFT模型中恢复该奖励信号，并利用其通过强化学习（Dense-Path REINFORCE）进一步提升了策略，在新方法下性能优于原始SFT模型。


<details>
  <summary>Details</summary>
Motivation: 传统观点将监督微调（SFT）视为简单的模仿学习过程。本文旨在挑战这一观点，建立SFT与逆向强化学习之间的根本等价关系。

Method: 证明SFT目标是逆向Q学习的一个特例，从而推断SFT过程不仅学习策略，还学习一个解释专家演示的隐式、密集、token级的奖励模型。通过制定基线相对奖励函数，直接从SFT模型中恢复出密集的奖励信号，并使用这些恢复的奖励通过强化学习（称为Dense-Path REINFORCE）进一步改进策略。

Result: 恢复的密集奖励模型为每个生成的token提供了细粒度的归因。提出的Dense-Path REINFORCE方法在指令遵循基准测试中持续优于原始SFT模型。

Conclusion: 本研究将SFT重新定义为不仅仅是策略模仿，而是一个强大的奖励学习机制，为有效利用专家演示数据开辟了新的可能性。

Abstract: Conventionally, supervised fine-tuning (SFT) is treated as a simple imitation
learning process that only trains a policy to imitate expert behavior on
demonstration datasets. In this work, we challenge this view by establishing a
fundamental equivalence between SFT and Inverse Reinforcement Learning. We
prove that the SFT objective is a special case of Inverse Q-Learning, which
implies that the SFT process does not just learn a policy, but also an
implicit, dense, token-level reward model that explains the expert
demonstrations. We then show how to recover this dense reward signal directly
from the SFT model by formulating a baseline-relative reward function. The
availability of such a dense reward model offers numerous benefits, providing
granular credit assignment for each token generated. We demonstrate one key
application by using these recovered rewards to further improve the policy with
reinforcement learning. Our method, Dense-Path REINFORCE, consistently
outperforms the original SFT models on instruction-following benchmarks. This
work reframes SFT not merely as policy imitation but as a powerful reward
learning mechanism, opening new possibilities for leveraging expert
demonstrations.

</details>


### [167] [In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning](https://arxiv.org/abs/2510.02516)
*Jindan Li,Zhaoxian Wu,Gaowen Liu,Tayfun Gokmen,Tianyi Chen*

Main category: cs.LG

TL;DR: 为解决内存计算(AIMC)中忆阻器件低精度导致训练准确性下降的问题，本文提出一种残差学习框架，通过多交叉阵列顺序学习补偿低精度权重更新误差，在保持适度硬件开销下，显著提高了有限状态设置下的训练性能。


<details>
  <summary>Details</summary>
Motivation: 有效内存计算训练需至少8位电导状态，但实际忆阻器件（如ReRAM）受制造限制通常仅提供约4位分辨率，这种低更新精度严重降低了训练准确性，难以实现片上训练。

Method: 本文提出一个“残差学习”框架，通过在多个交叉阵列上顺序学习，以补偿低精度权重更新所产生的残差误差。

Result: 理论分析表明，最优性差距随交叉阵列数量增加而缩小，并实现线性收敛。实验证明，在有限状态设置下，该方法持续优于现有最先进的内存模拟训练策略。成本分析确认其硬件开销适中。

Conclusion: 所提出的残差学习框架成功地实现了使用实际有限状态忆阻器件进行有效的片上训练，克服了AIMC加速器在训练精度上的关键限制，且硬件开销可控。

Abstract: Analog in-memory computing (AIMC) accelerators enable efficient deep neural
network computation directly within memory using resistive crossbar arrays,
where model parameters are represented by the conductance states of memristive
devices. However, effective in-memory training typically requires at least
8-bit conductance states to match digital baselines. Realizing such
fine-grained states is costly and often requires complex noise mitigation
techniques that increase circuit complexity and energy consumption. In
practice, many promising memristive devices such as ReRAM offer only about
4-bit resolution due to fabrication constraints, and this limited update
precision substantially degrades training accuracy. To enable on-chip training
with these limited-state devices, this paper proposes a \emph{residual
learning} framework that sequentially learns on multiple crossbar tiles to
compensate the residual errors from low-precision weight updates. Our
theoretical analysis shows that the optimality gap shrinks with the number of
tiles and achieves a linear convergence rate. Experiments on standard image
classification benchmarks demonstrate that our method consistently outperforms
state-of-the-art in-memory analog training strategies under limited-state
settings, while incurring only moderate hardware overhead as confirmed by our
cost analysis.

</details>


### [168] [Graph Generation with Spectral Geodesic Flow Matching](https://arxiv.org/abs/2510.02520)
*Xikun Huang,Tianyu Ruan,Chihao Zhang,Shihua Zhang*

Main category: cs.LG

TL;DR: SFMG是一种结合谱几何和流匹配的图生成新方法，通过嵌入黎曼流形并匹配测地流，有效捕获图几何结构，实现高性能、多样化和高效率的图生成。


<details>
  <summary>Details</summary>
Motivation: 现有图生成方法主要关注谱或度分布，但往往忽略了特征向量诱导的几何结构和图的全局结构。

Method: 提出Spectral Geodesic Flow Matching (SFMG) 框架，利用谱特征映射将输入和目标图嵌入到连续黎曼流形中，然后定义嵌入之间的测地流，并匹配沿这些流的分布来生成图。

Result: SFMG在图元、度数和谱度量上与现有SOTA方法表现相当，但比扩散模型快30倍，具有显著的可扩展性和训练效率优势，并能泛化到未见过的图规模。

Conclusion: SFMG通过整合谱几何与流匹配，提供了一种新的图合成方法，不仅能捕获超越特征值的几何结构，还支持灵活生成多样化图，并具有高效扩展的能力。

Abstract: Graph generation is a fundamental task with wide applications in modeling
complex systems. Although existing methods align the spectrum or degree profile
of the target graph, they often ignore the geometry induced by eigenvectors and
the global structure of the graph. In this work, we propose Spectral Geodesic
Flow Matching (SFMG), a novel framework that uses spectral eigenmaps to embed
both input and target graphs into continuous Riemannian manifolds. We then
define geodesic flows between embeddings and match distributions along these
flows to generate output graphs. Our method yields several advantages: (i)
captures geometric structure beyond eigenvalues, (ii) supports flexible
generation of diverse graphs, and (iii) scales efficiently. Empirically, SFMG
matches the performance of state-of-the-art approaches on graphlet, degree, and
spectral metrics across diverse benchmarks. In particular, it achieves up to
30$\times$ speedup over diffusion-based models, offering a substantial
advantage in scalability and training efficiency. We also demonstrate its
ability to generalize to unseen graph scales. Overall, SFMG provides a new
approach to graph synthesis by integrating spectral geometry with flow
matching.

</details>


### [169] [Model-brain comparison using inter-animal transforms](https://arxiv.org/abs/2510.02523)
*Imran Thobani,Javier Sagastuy-Brena,Aran Nayebi,Jacob Prince,Rosa Cao,Daniel Yamins*

Main category: cs.LG

TL;DR: 本文提出一种基于“动物间转换类”（IATC）的新方法，用于比较神经网络模型与大脑响应，该方法能同时实现高预测性和机制特异性，解决了模型-大脑比较中的传统权衡问题。


<details>
  <summary>Details</summary>
Motivation: 人工神经网络是前景广阔的大脑机制模型，但缺乏关于如何正确比较模型激活与大脑响应的共识。需要一种既能准确预测又能识别机制的比较方法，以解决当前模型预测能力与机制准确性之间的权衡难题。

Method: 引入基于“动物间转换类”（IATC）的比较方法。IATC定义为在动物群体中准确映射神经响应所需的最小变换集。通过IATC在模型响应和大脑数据之间进行双向映射，评估模型能否像真实个体一样被变换。该方法在模拟神经网络群体、小鼠和人类受试者群体中进行了识别和验证。

Result: IATC能解析神经机制的细节（如非线性激活函数）。它在实现神经活动准确预测的同时，也达到了机制识别的高特异性（能区分不同脑区响应，并对齐同脑区响应）。研究发现，高模型-大脑预测性与识别机制准确的大脑模型之间没有内在权衡。IAT C指导的变换支持地形深度神经网络（TDANNs）作为视觉系统模型。

Conclusion: IATC提供了一种原则性的模型-大脑比较方法，不仅能更好地理解深度学习模型的预测成功，还改进了现有比较方法。它有助于识别机制准确的大脑模型，并证明预测能力和机制准确性之间不存在固有冲突。

Abstract: Artificial neural network models have emerged as promising mechanistic models
of the brain. However, there is little consensus on the correct method for
comparing model activations to brain responses. Drawing on recent work in
philosophy of neuroscience, we propose a comparison methodology based on the
Inter-Animal Transform Class (IATC) - the strictest set of functions needed to
accurately map neural responses between subjects in an animal population. Using
the IATC, we can map bidirectionally between a candidate model's responses and
brain data, assessing how well the model can masquerade as a typical subject
using the same kinds of transforms needed to map across real subjects. We
identify the IATC in three settings: a simulated population of neural network
models, a population of mouse subjects, and a population of human subjects. We
find that the IATC resolves detailed aspects of the neural mechanism, such as
the non-linear activation function. Most importantly, we find that the IATC
enables accurate predictions of neural activity while also achieving high
specificity in mechanism identification, evidenced by its ability to separate
response patterns from different brain areas while strongly aligning
same-brain-area responses between subjects. In other words, the IATC is a
proof-by-existence that there is no inherent tradeoff between the neural
engineering goal of high model-brain predictivity and the neuroscientific goal
of identifying mechanistically accurate brain models. Using IATC-guided
transforms, we obtain new evidence in favor of topographical deep neural
networks (TDANNs) as models of the visual system. Overall, the IATC enables
principled model-brain comparisons, contextualizing previous findings about the
predictive success of deep learning models of the brain, while improving upon
previous approaches to model-brain comparison.

</details>


### [170] [AttentiveGRUAE: An Attention-Based GRU Autoencoder for Temporal Clustering and Behavioral Characterization of Depression from Wearable Data](https://arxiv.org/abs/2510.02558)
*Nidhi Soley,Vishal M Patel,Casey O Taylor*

Main category: cs.LG

TL;DR: 本研究提出AttentiveGRUAE模型，一种基于注意力机制的GRU自编码器，用于纵向可穿戴数据的时序聚类和结果预测。该模型在抑郁症分类和行为亚型识别上表现优越，并具有良好的可解释性和可重现性。


<details>
  <summary>Details</summary>
Motivation: 从纵向可穿戴数据中进行有效的时序聚类、预测结局，并识别具有临床意义的行为亚型是一个挑战。

Method: 开发了AttentiveGRUAE模型，一个注意力机制的门控循环单元（GRU）自编码器。该模型联合优化了三个目标：1) 通过序列重建学习日常行为特征的紧凑潜在表示；2) 通过二元分类头预测期末抑郁率；3) 通过基于高斯混合模型（GMM）的软聚类识别行为亚型。

Result: 1. 在372名参与者的纵向睡眠数据（GLOBEM 2018-2019）上，AttentiveGRUAE在聚类质量（轮廓系数 = 0.70 vs 0.32-0.70）和抑郁症分类（AUC = 0.74 vs 0.50-0.67）方面均优于基线模型。2. 在332名参与者的跨年度队列（GLOBEM 2020-2021）上进行了外部验证，证实了聚类的可重现性（轮廓系数 = 0.63）和稳定性，以及预测能力（AUC = 0.61）。3. 亚型分析和时间注意力可视化揭示了聚类间的睡眠相关差异，并识别出与睡眠规律性变化相关的显著时间窗口，提供了临床可解释的风险解释。

Conclusion: AttentiveGRUAE模型在纵向可穿戴数据分析中，能够有效地进行时序聚类和结局预测，并识别出具有良好可重现性、稳定性及临床可解释性的行为亚型，为风险评估提供了有价值的见解。

Abstract: In this study, we present AttentiveGRUAE, a novel attention-based gated
recurrent unit (GRU) autoencoder designed for temporal clustering and
prediction of outcome from longitudinal wearable data. Our model jointly
optimizes three objectives: (1) learning a compact latent representation of
daily behavioral features via sequence reconstruction, (2) predicting
end-of-period depression rate through a binary classification head, and (3)
identifying behavioral subtypes through Gaussian Mixture Model (GMM) based soft
clustering of learned embeddings. We evaluate AttentiveGRUAE on longitudinal
sleep data from 372 participants (GLOBEM 2018-2019), and it demonstrates
superior performance over baseline clustering, domain-aligned self-supervised,
and ablated models in both clustering quality (silhouette score = 0.70 vs
0.32-0.70) and depression classification (AUC = 0.74 vs 0.50-0.67).
Additionally, external validation on cross-year cohorts from 332 participants
(GLOBEM 2020-2021) confirms cluster reproducibility (silhouette score = 0.63,
AUC = 0.61) and stability. We further perform subtype analysis and visualize
temporal attention, which highlights sleep-related differences between clusters
and identifies salient time windows that align with changes in sleep
regularity, yielding clinically interpretable explanations of risk.

</details>


### [171] [On The Expressive Power of GNN Derivatives](https://arxiv.org/abs/2510.02565)
*Yam Eitan,Moshe Eliasof,Yoav Gelberg,Fabrizio Frasca,Guy Bar-Shalom,Haggai Maron*

Main category: cs.LG

TL;DR: GNN表达能力受限，本文提出HOD-GNN，利用GNN高阶节点导数增强MPNN表达能力，理论上与WL层次对齐，并在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络（GNNs）进展显著，其有限的表达能力仍是核心挑战。GNN的导数已被广泛研究用于解决过平滑、过压缩和可解释性等问题，但尚未被探索作为增强GNN表达能力的方法。

Method: 提出高阶导数GNN (HOD-GNN)，通过利用基础模型的高阶节点导数来增强消息传递神经网络 (MPNNs) 的表达能力。这些导数生成结构感知节点嵌入，再由第二个GNN进行处理，形成一个端到端可训练的架构。为提高计算效率，开发了一种利用图稀疏性和并行性的消息传递算法来计算MPNN的高阶导数。

Result: 理论上，HOD-GNN 的表达能力与 WL 层次结构对齐，并与子图GNN和流行的结构编码方案建立了深入联系。在流行的图学习基准测试中，HOD-GNN 展示了强大的性能。

Conclusion: 高阶导数提供了一种增强GNN表达能力的有效途径。HOD-GNN通过利用高阶导数生成结构感知嵌入，显著提升了MPNN的表达能力，并在理论和实践中均表现出优越性。

Abstract: Despite significant advances in Graph Neural Networks (GNNs), their limited
expressivity remains a fundamental challenge. Research on GNN expressivity has
produced many expressive architectures, leading to architecture hierarchies
with models of increasing expressive power. Separately, derivatives of GNNs
with respect to node features have been widely studied in the context of the
oversquashing and over-smoothing phenomena, GNN explainability, and more. To
date, these derivatives remain unexplored as a means to enhance GNN
expressivity. In this paper, we show that these derivatives provide a natural
way to enhance the expressivity of GNNs. We introduce High-Order Derivative GNN
(HOD-GNN), a novel method that enhances the expressivity of Message Passing
Neural Networks (MPNNs) by leveraging high-order node derivatives of the base
model. These derivatives generate expressive structure-aware node embeddings
processed by a second GNN in an end-to-end trainable architecture.
Theoretically, we show that the resulting architecture family's expressive
power aligns with the WL hierarchy. We also draw deep connections between
HOD-GNN, Subgraph GNNs, and popular structural encoding schemes. For
computational efficiency, we develop a message-passing algorithm for computing
high-order derivatives of MPNNs that exploits graph sparsity and parallelism.
Evaluations on popular graph learning benchmarks demonstrate HOD-GNN's strong
performance on popular graph learning tasks.

</details>


### [172] [Geospatial Machine Learning Libraries](https://arxiv.org/abs/2510.02572)
*Adam J. Stewart,Caleb Robinson,Arindam Banerjee*

Main category: cs.LG

TL;DR: 全面概述地理空间机器学习（GeoML）库，包括其演进、功能、流行工具、应用案例、最佳实践及未来挑战，旨在指导该领域从业者。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据激增，但GeoML领域缺乏成熟的专门库来有效应对数据处理中的独特挑战，如多变的空间分辨率、光谱特性、时间频率、数据覆盖、坐标系统和文件格式等。

Method: 本章通过综合概述GeoML库的演进、核心功能和生态系统，详细介绍了TorchGeo、eo-learn、Raster Vision等流行库的架构、支持数据类型及其与ML框架的集成。此外，讨论了数据预处理、时空连接、基准测试和预训练模型等常用方法，并通过作物类型映射的案例研究展示了这些工具的实际应用。

Result: 分析揭示了GeoML库的演进轨迹和当前生态系统，详细阐述了主流库的技术细节和集成方式。文章探讨了GeoML数据处理和模型应用的关键方法，并通过案例证明了这些工具的实用性，并总结了软件设计、许可、测试的最佳实践，指出了开放挑战和未来的发展方向，特别是基础模型的兴起和开源地理空间软件治理的需求。

Conclusion: 本章旨在为GeoML领域的从业者、开发者和研究人员提供指导，帮助他们理解并积极贡献于快速发展的地理空间机器学习领域。

Abstract: Recent advances in machine learning have been supported by the emergence of
domain-specific software libraries, enabling streamlined workflows and
increased reproducibility. For geospatial machine learning (GeoML), the
availability of Earth observation data has outpaced the development of domain
libraries to handle its unique challenges, such as varying spatial resolutions,
spectral properties, temporal cadence, data coverage, coordinate systems, and
file formats. This chapter presents a comprehensive overview of GeoML
libraries, analyzing their evolution, core functionalities, and the current
ecosystem. It also introduces popular GeoML libraries such as TorchGeo,
eo-learn, and Raster Vision, detailing their architecture, supported data
types, and integration with ML frameworks. Additionally, it discusses common
methodologies for data preprocessing, spatial--temporal joins, benchmarking,
and the use of pretrained models. Through a case study in crop type mapping, it
demonstrates practical applications of these tools. Best practices in software
design, licensing, and testing are highlighted, along with open challenges and
future directions, particularly the rise of foundation models and the need for
governance in open-source geospatial software. Our aim is to guide
practitioners, developers, and researchers in navigating and contributing to
the rapidly evolving GeoML landscape.

</details>


### [173] [Use the Online Network If You Can: Towards Fast and Stable Reinforcement Learning](https://arxiv.org/abs/2510.02590)
*Ahmed Hendawy,Henrik Metternich,Théo Vincent,Mahdi Kallel,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: MINTO通过取目标网络与在线网络估计值的最小值，实现了更快、更稳定的价值函数学习，并在多项RL基准测试中持续提升性能。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习中，目标网络虽稳定但学习慢，在线网络虽快却不稳定。研究旨在克服这种权衡，结合两者的优点，并缓解在线网络引导可能导致的过高估计偏差。

Method: 提出MINTO方法，该方法引入了一个新颖的更新规则，通过计算目标网络和在线网络估计值的最小值来确定目标值。这种简单而有效的修改缓解了过高估计偏差，从而实现更快、更稳定的价值函数学习。MINTO可无缝集成到多种基于价值和Actor-Critic算法中，且成本可忽略不计。

Result: MINTO在在线和离线RL、离散和连续动作空间等多样化基准测试中，始终如一地提升了性能，证明了其广泛的适用性和有效性。

Conclusion: MINTO提供了一种有效解决深度强化学习中目标网络与在线网络权衡的方法，实现了更快、更稳定的价值函数学习，并在各种RL任务中展现出卓越的性能和广泛的适用性。

Abstract: The use of target networks is a popular approach for estimating value
functions in deep Reinforcement Learning (RL). While effective, the target
network remains a compromise solution that preserves stability at the cost of
slowly moving targets, thus delaying learning. Conversely, using the online
network as a bootstrapped target is intuitively appealing, albeit well-known to
lead to unstable learning. In this work, we aim to obtain the best out of both
worlds by introducing a novel update rule that computes the target using the
MINimum estimate between the Target and Online network, giving rise to our
method, MINTO. Through this simple, yet effective modification, we show that
MINTO enables faster and stable value function learning, by mitigating the
potential overestimation bias of using the online network for bootstrapping.
Notably, MINTO can be seamlessly integrated into a wide range of value-based
and actor-critic algorithms with a negligible cost. We evaluate MINTO
extensively across diverse benchmarks, spanning online and offline RL, as well
as discrete and continuous action spaces. Across all benchmarks, MINTO
consistently improves performance, demonstrating its broad applicability and
effectiveness.

</details>


### [174] [Towards CONUS-Wide ML-Augmented Conceptually-Interpretable Modeling of Catchment-Scale Precipitation-Storage-Runoff Dynamics](https://arxiv.org/abs/2510.02605)
*Yuan-Heng Wang,Yang Yang,Fabio Ciulla,Hoshin V. Gupta,Charuleka Varadharajan*

Main category: cs.LG

TL;DR: 本研究使用基于质量守恒感知器（MCP）的机器学习增强型物理可解释模型进行大样本水文研究，发现其性能可与LSTM模型媲美，并强调根据水文状况选择适当模型复杂度的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基于ML的大样本水文建模研究未能显著提升预测能力，且缺乏物理概念理解。

Method: 在美国大陆范围（CONUS）内，针对多样化的水文-地质-气候条件，使用基于质量守恒感知器（MCP）的机器学习增强型物理可解释集水区模型进行大样本研究，并根据积雪状况、森林覆盖和气候区等属性掩膜评估结果。

Result: 研究表明，根据水文状况过程中主导作用的变化，选择适当复杂度的模型架构至关重要。基于MCP的物理可解释模型性能可与基于LSTM网络的纯数据驱动模型相媲美。

Conclusion: 这项研究突出了理论指导、物理基础方法在大样本水文学中的潜力，强调了机制理解以及开发精简和可解释模型架构的重要性，为未来编码时空变化过程主导信息的模型奠定基础。

Abstract: While many modern studies are dedicated to ML-based large-sample hydrologic
modeling, these efforts have not necessarily translated into predictive
improvements that are grounded in enhanced physical-conceptual understanding.
Here, we report on a CONUS-wide large-sample study (spanning diverse
hydro-geo-climatic conditions) using ML-augmented physically-interpretable
catchment-scale models of varying complexity based in the Mass-Conserving
Perceptron (MCP). Results were evaluated using attribute masks such as snow
regime, forest cover, and climate zone. Our results indicate the importance of
selecting model architectures of appropriate model complexity based on how
process dominance varies with hydrological regime. Benchmark comparisons show
that physically-interpretable mass-conserving MCP-based models can achieve
performance comparable to data-based models based in the Long Short-Term Memory
network (LSTM) architecture. Overall, this study highlights the potential of a
theory-informed, physically grounded approach to large-sample hydrology, with
emphasis on mechanistic understanding and the development of parsimonious and
interpretable model architectures, thereby laying the foundation for future
models of everywhere that architecturally encode information about spatially-
and temporally-varying process dominance.

</details>


### [175] [MINERVA: Mutual Information Neural Estimation for Supervised Feature Selection](https://arxiv.org/abs/2510.02610)
*Taurai Muvunzaa,Egor Kraev,Pere Planell-Morell,Alexander Y. Shestopaloff*

Main category: cs.LG

TL;DR: MINERVA是一种基于神经互信息估计的监督特征选择方法，旨在解决现有方法无法处理高阶特征交互的局限性。它通过两阶段过程解耦表示学习与特征选择，并利用神经网络和稀疏正则化损失函数来有效捕获复杂特征关系，在多种数据集上表现出良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有特征过滤器依赖于成对统计依赖度量来建模特征与目标的关系，但这在目标依赖于高阶特征交互而非个体贡献时会失效，导致无法有效捕获复杂的依赖结构。

Method: 提出了MINERVA（Mutual Information Neural Estimation Regularized Vetting Algorithm），一种利用神经网络估计特征与目标之间互信息进行监督特征选择的方法。该方法使用神经网络参数化互信息近似，并通过一个带有稀疏性诱导正则项的精心设计的损失函数进行特征选择。它采用两阶段过程，将表示学习与特征选择解耦，以确保更好的泛化能力和更准确的特征重要性表达。

Result: MINERVA能够有效捕捉文献中鲜有涉及的普遍依赖结构和复杂的特征-目标关系，通过将特征子集作为一个整体进行评估。在合成数据集和真实欺诈数据集上的实验结果证明了该方法的有效性及其执行精确解的能力。

Conclusion: MINERVA成功解决了传统特征选择方法在高阶特征交互方面的局限性，通过神经估计互信息和两阶段处理，有效识别并捕获了复杂的特征-目标关系，并在实际应用中展现出显著的性能和执行精确解的能力。

Abstract: Existing feature filters rely on statistical pair-wise dependence metrics to
model feature-target relationships, but this approach may fail when the target
depends on higher-order feature interactions rather than individual
contributions. We introduce Mutual Information Neural Estimation Regularized
Vetting Algorithm (MINERVA), a novel approach to supervised feature selection
based on neural estimation of mutual information between features and targets.
We paramaterize the approximation of mutual information with neural networks
and perform feature selection using a carefully designed loss function
augmented with sparsity-inducing regularizers. Our method is implemented in a
two-stage process to decouple representation learning from feature selection,
ensuring better generalization and a more accurate expression of feature
importance. We present examples of ubiquitous dependency structures that are
rarely captured in literature and show that our proposed method effectively
captures these complex feature-target relationships by evaluating feature
subsets as an ensemble. Experimental results on synthetic and real-life fraud
datasets demonstrate the efficacy of our method and its ability to perform
exact solutions.

</details>


### [176] [TabImpute: Accurate and Fast Zero-Shot Missing-Data Imputation with a Pre-Trained Transformer](https://arxiv.org/abs/2510.02625)
*Jacob Feitelberg,Dwaipayan Saha,Kyuseong Choi,Zaid Ahmad,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: 本文提出了TabImpute，一个基于TabPFN的预训练Transformer模型，用于表格数据的零样本缺失值填充，无需在推理时进行拟合或超参数调优，并在广泛基准测试中展现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 表格数据中的缺失值是一个普遍问题。现有填充方法性能差异大，且需要耗时的超参数调优，导致缺乏通用的默认方法。

Method: 本文提出TabImpute，一个基于TabPFN的预训练Transformer模型，实现准确、快速的零样本缺失值填充。为训练和评估，作者还引入了（i）表格数据的逐条特征化方法（提速100倍）；（ii）包含真实缺失模式的合成训练数据生成流程；以及（iii）MissBench，一个包含42个OpenML数据集和13种缺失模式的综合基准。

Result: TabImpute在MissBench基准测试（涵盖医学、金融、工程等领域）上表现出强大的鲁棒性，性能优于11种现有填充方法。

Conclusion: TabImpute提供了一种高效、准确、无需调优的缺失数据填充解决方案，并在各种实际领域中展现出优越的性能和鲁棒性，有望成为表格数据缺失值填充的默认方法。

Abstract: Missing data is a pervasive problem in tabular settings. Existing solutions
range from simple averaging to complex generative adversarial networks.
However, due to huge variance in performance across real-world domains and
time-consuming hyperparameter tuning, no default imputation method exists.
Building on TabPFN, a recent tabular foundation model for supervised learning,
we propose TabImpute, a pre-trained transformer that delivers accurate and fast
zero-shot imputations requiring no fitting or hyperparameter tuning at
inference-time. To train and evaluate TabImpute, we introduce (i) an entry-wise
featurization for tabular settings, which enables a $100\times$ speedup over
the previous TabPFN imputation method, (ii) a synthetic training data
generation pipeline incorporating realistic missingness patterns, which boosts
test-time performance, and (iii) MissBench, a comprehensive benchmark for
evaluation of imputation methods with $42$ OpenML datasets and $13$ missingness
patterns. MissBench spans domains such as medicine, finance, and engineering,
showcasing TabImpute's robust performance compared to $11$ established
imputation methods.

</details>


### [177] [HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance](https://arxiv.org/abs/2510.02630)
*Hao Zhang,Zhenjia Li,Runfeng Bao,Yifan Gao,Xi Xiao,Bo Huang,Yuhang Wu,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: 针对AdaLoRA收敛慢和高开销问题，提出HyperAdaLoRA。它利用注意力超网络动态生成SVD参数并剪枝奇异值以实现动态秩分配，从而加速收敛且不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: LoRA假定均匀秩，未考虑权重矩阵的重要性差异。AdaLoRA通过SVD引入动态秩，但训练时收敛速度慢且计算开销大。

Method: 提出HyperAdaLoRA框架。它不直接优化SVD组件(P, Λ, Q)，而是利用基于注意力机制的超网络动态生成这些参数。通过剪枝超网络输出的奇异值实现动态秩分配。

Result: 实验证明，HyperAdaLoRA在不牺牲性能的前提下，实现了更快的收敛速度。该方法对其他基于LoRA的方法也具有广泛适用性。

Conclusion: HyperAdaLoRA通过引入超网络解决了AdaLoRA的收敛速度和计算开销问题，实现了高效的动态秩分配，并在保持性能的同时加速了训练。

Abstract: Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation
(LoRA), has emerged as a promising approach to fine-tuning large language
models(LLMs) while reducing computational and memory overhead. However, LoRA
assumes a uniform rank \textit{r} for each incremental matrix, not accounting
for the varying significance of weight matrices across different modules and
layers. AdaLoRA leverages Singular Value Decomposition (SVD) to parameterize
updates and employs pruning of singular values to introduce dynamic rank
allocation, thereby enhancing adaptability. However, during the training
process, it often encounters issues of slow convergence speed and high
computational overhead. To address this issue, we propose HyperAdaLoRA, a novel
framework that accelerates the convergence of AdaLoRA by leveraging a
hypernetwork. Instead of directly optimizing the components of Singular Value
Decomposition $(P, \Lambda, Q)$, HyperAdaLoRA employs a hypernetwork based on
attention mechanisms to dynamically generate these parameters. By pruning the
outputs of the hypernetwork that generates the singular values, dynamic rank
allocation is achieved. Comprehensive experiments on various datasets and
models demonstrate that our method achieves faster convergence without
sacrificing performance. Additionally, further extension experiments on other
LoRA-based approaches validate the broad applicability of our method.

</details>


### [178] [Optimal Characteristics of Inspection Vehicle for Drive-by Bridge Inspection](https://arxiv.org/abs/2510.02658)
*A. Calderon Hurtado,E. Atroshchenko,K. C. Chang,C. W. Kim,M. Makki Alamdari*

Main category: cs.LG

TL;DR: 本研究通过深度学习和Kriging元模型，优化了车辆驱动式桥梁健康监测中的检测车辆参数，以提高损伤检测灵敏度，并确定了最优车辆配置，发现频率比在0.3-0.7的车辆效果最佳。


<details>
  <summary>Details</summary>
Motivation: 车辆驱动式桥梁健康监测方法受限于车辆自身的机械和动态特性，导致检测性能和有效性受限，因此需要优化检测车辆以增强其损伤敏感性。

Method: 研究提出了一个优化检测车辆的框架。使用基于对抗自编码器（AAE）的无监督深度学习方法重建加速度响应的频域表示。通过最小化健康和受损桥梁状态下损伤指标分布的Wasserstein距离，优化了两轴车辆轮胎悬挂系统的质量和刚度。此外，还采用了Kriging元模型来高效逼近目标函数并识别最优车辆配置。

Result: 研究结果表明，车辆与桥梁第一固有频率之比在0.3到0.7之间时，检测效果最为有效，而接近共振的车辆表现不佳。同时，较轻的车辆需要较低的固有频率才能实现最佳损伤检测。

Conclusion: 本研究首次严格优化了车辆驱动式传感的传感平台，并提出了专用检测车辆的设计方案，为提升桥梁健康监测的效率和准确性提供了重要的理论和实践指导。

Abstract: Drive-by inspection for bridge health monitoring has gained increasing
attention over the past decade. This method involves analysing the coupled
vehicle-bridge response, recorded by an instrumented inspection vehicle, to
assess structural integrity and detect damage. However, the vehicles mechanical
and dynamic properties significantly influence detection performance, limiting
the effectiveness of the approach. This study presents a framework for
optimising the inspection vehicle to enhance damage sensitivity. An
unsupervised deep learning methodbased on adversarial autoencoders (AAE)is used
to reconstruct the frequency-domain representation of acceleration responses.
The mass and stiffness of the tyre suspension system of a two-axle vehicle are
optimised by minimising the Wasserstein distance between damage index
distributions for healthy and damaged bridge states. A Kriging meta-model is
employed to approximate this objective function efficiently and identify
optimal vehicle configurations in both dimensional and non-dimensional
parameter spaces. Results show that vehicles with frequency ratios between 0.3
and 0.7 relative to the bridges' first natural frequency are most effective,
while those near resonance perform poorly. Lighter vehicles require lower
natural frequencies for optimal detection. This is the first study to
rigorously optimise the sensing platform for drive-by sensing and to propose a
purpose-built inspection vehicle.

</details>


### [179] [TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models](https://arxiv.org/abs/2510.02663)
*Rakshith S Srinivasa,Zora Che,Chen Bo Calvin Zhang,Diego Mares,Ernesto Hernandez,Jayeon Park,Dean Lee,Guillermo Mangialardi,Charmaine Ng,Ed-Yeremai Hernandez Cardona,Anisha Gunjal,Yunzhong He,Bing Liu,Chen Xing*

Main category: cs.LG

TL;DR: 本文介绍了TutorBench，一个用于严格评估大型语言模型（LLMs）核心辅导技能的新数据集和基准。评估结果显示，当前最先进的LLMs在辅导任务中表现不佳（最高分不超过56%），表明它们在指导、诊断和支持学生方面仍有很大的提升空间。


<details>
  <summary>Details</summary>
Motivation: 随着学生越来越多地将LLMs作为学习辅助工具，构建能有效处理辅导细微差别的模型至关重要，包括识别学生核心需求、自适应、提供个性化指导及保持准确性。因此，需要一个专门的基准来严格评估LLMs的这些核心辅导技能。

Method: 引入TutorBench数据集和评估基准，该数据集包含1,490个由人类专家整理的高中及AP课程样本，涵盖三类常见的辅导任务：生成适应学生困惑的解释、提供可操作的学生作业反馈以及通过有效提示促进主动学习。每个样本都配有特定评估标准（rubrics）。评估采用可靠的、细粒度的自动化方法，利用LLM-judge和样本特定评估标准来评判模型响应。研究评估了16个前沿LLMs。

Result: 评估结果显示，没有一个前沿LLM的得分超过56%，表明有巨大的改进空间。LLMs在展现指导、诊断和有效支持学生所需的完整辅导技能方面存在不足，所有前沿模型在相关评估标准上的通过率均低于60%。不同模型家族表现出不同的优势和局限性：Claude模型在支持主动学习方面表现优异，但在其他两种用例中则落后。

Conclusion: TutorBench提供了一个全面且尚未饱和的基准，旨在指导下一代AI辅导工具的开发。当前的LLMs尚未能充分胜任高效辅导的角色，未来在提升其辅导技能方面仍有广阔的研究前景。

Abstract: As students increasingly adopt large language models (LLMs) as learning aids,
it is crucial to build models that are adept at handling the nuances of
tutoring: they need to identify the core needs of students, be adaptive,
provide personalized guidance, and be accurate. To this end, we introduce
TutorBench, a dataset and evaluation benchmark designed to rigorously evaluate
the core tutoring skills of LLMs. The dataset comprises 1,490 samples curated
by human experts, focused on high-school and AP-level curricula. The samples
are drawn from three common tutoring tasks: (i) generating adaptive
explanations tailored to a student's confusion, (ii) providing actionable
feedback on a student's work, and (iii) promoting active learning through
effective hint generation. To account for the inherent complexity of tutoring,
samples are accompanied by sample-specific rubrics which are used to judge
model responses during evaluation. TutorBench uses a reliable and fine-grained
automatic evaluation method that uses an LLM-judge and the sample-specific
rubrics. We evaluate 16 frontier LLMs on TutorBench and present a detailed
analysis of their performance and behavior. Our results show that none of the
frontier LLMs achieve a score of greater than $56\%$, showing a large room for
improvement. We find that LLMs fall short in exhibiting the full range of
tutoring skills needed to guide, diagnose, and support students effectively,
with all the frontier models achieving less than a $60\%$ pass rate on rubric
criteria related to these skills. We also find that different model families
exhibit varied strengths and limitations: the Claude models outperform others
in supporting active learning, while they lag behind in the other two use
cases. By releasing TutorBench, we provide a comprehensive and unsaturated
benchmark to guide the development of the next-generation of AI tutors.

</details>


### [180] [Topological Invariance and Breakdown in Learning](https://arxiv.org/abs/2510.02670)
*Yongyi Yang,Tomaso Poggio,Isaac Chuang,Liu Ziyin*

Main category: cs.LG

TL;DR: 训练过程通过诱导神经元间的双Lipschitz映射，严格约束神经元分布的拓扑结构。研究发现存在一个拓扑临界学习率，决定训练过程是保留拓扑结构还是进行拓扑简化，并将学习动态划分为两个阶段。


<details>
  <summary>Details</summary>
Motivation: 旨在深入理解深度学习中神经元网络的训练动态及其内部拓扑结构变化，尤其关注学习率的影响，并开发一种普适于不同模型架构和损失函数的理论分析方法。

Method: 通过理论证明，分析了包括SGD和Adam在内的置换等变学习规则如何诱导神经元间的双Lipschitz映射，并利用拓扑学方法研究训练期间神经元分布的拓扑结构演变。

Result: 训练过程在神经元之间诱导双Lipschitz映射，并强力限制神经元分布的拓扑结构。存在一个拓扑临界点$\eta^*$：学习率低于$\eta^*$时，训练保持所有拓扑结构；高于$\eta^*$时，允许拓扑简化，使神经元流形变粗，降低模型表达能力。结合“稳定性边缘”现象，神经元网络的学习动态可分为受拓扑约束的平滑优化和剧烈拓扑简化两个阶段。该理论普适于不同架构和损失函数。

Conclusion: 神经元网络的学习动态可由学习率驱动的拓扑结构演变来解释，表现为先平滑优化后剧烈拓扑简化的两阶段过程。这项研究为将拓扑方法普遍应用于深度学习提供了新的理论框架。

Abstract: We prove that for a broad class of permutation-equivariant learning rules
(including SGD, Adam, and others), the training process induces a bi-Lipschitz
mapping between neurons and strongly constrains the topology of the neuron
distribution during training. This result reveals a qualitative difference
between small and large learning rates $\eta$. With a learning rate below a
topological critical point $\eta^*$, the training is constrained to preserve
all topological structure of the neurons. In contrast, above $\eta^*$, the
learning process allows for topological simplification, making the neuron
manifold progressively coarser and thereby reducing the model's expressivity.
Viewed in combination with the recent discovery of the edge of stability
phenomenon, the learning dynamics of neuron networks under gradient descent can
be divided into two phases: first they undergo smooth optimization under
topological constraints, and then enter a second phase where they learn through
drastic topological simplifications. A key feature of our theory is that it is
independent of specific architectures or loss functions, enabling the universal
application of topological methods to the study of deep learning.

</details>


### [181] [To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration](https://arxiv.org/abs/2510.02676)
*Zeyu Yang,Tianyi Zhang,Jianwen Xie,Chuan Li,Zhaozhuo Xu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: 本文提出了一种基于指数集中现象的无损FP8浮点格式ECF8，通过理论分析和实验验证，实现了大型生成式AI模型在部署时的内存节省和吞吐量加速，且计算无损。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型参数量巨大（数百亿级别），低精度计算对于高效部署至关重要。传统的低精度方法存在问题，而低精度浮点格式作为根本解决方案，能提供数值稳定性、节省内存和硬件效率，且无反量化开销。研究旨在开发更高效的低精度浮点格式。

Method: 1. 理论和实证研究了生成式AI权重中的“指数集中”现象，发现指数在不同架构和模态中始终表现出低熵。2. 证明了该现象源于随机梯度下降诱导的α-稳定分布，并推导出指数熵的紧密界限。3. 建立了接近FP4.67的理论压缩极限，为实用FP8格式的设计提供了依据。4. 提出了Exponent-Concentrated FP8 (ECF8) 框架，包含熵感知编码和GPU优化解码，实现无损压缩。

Result: 1. 实验证明ECF8在LLM和DiT模型（最高达671B参数）上，可实现高达26.9%的内存节省和177.1%的吞吐量加速。2. 计算结果是完全无损的，即模型输出无任何偏差。3. 确立了指数集中作为训练模型的一种统计规律。

Conclusion: 指数集中是训练模型的一种统计规律，为FP8时代无损低精度浮点设计开辟了一条原则性的道路。ECF8提供了一种高效、无损的解决方案，以应对大型生成式AI模型的部署挑战。

Abstract: The scaling of Generative AI (GenAI) models into the hundreds of billions of
parameters makes low-precision computation indispensable for efficient
deployment. We argue that the fundamental solution lies in developing
low-precision floating-point formats, which inherently provide numerical
stability, memory savings, and hardware efficiency without dequantization
overhead. In this paper, we present a theoretical and empirical study of an
exponent concentration phenomenon in GenAI weights: exponents consistently
exhibit low entropy across architectures and modalities. We show that this
arises naturally from $\alpha$-stable distributions induced by stochastic
gradient descent, and we prove tight bounds on the entropy of exponents. Our
analysis establishes a theoretical compression limit near FP4.67, which
motivates the design of a practical FP8 format. Building on these insights, we
propose Exponent-Concentrated FP8 (ECF8), a lossless compression framework with
entropy-aware encoding and GPU-optimized decoding. Experiments on LLMs and DiTs
up to 671B parameters demonstrate up to 26.9% memory savings and 177.1%
throughput acceleration, with perfectly lossless computations, i.e., no
deviation in model outputs. Our results establish exponent concentration as a
statistical law of trained models and open a principled path for lossless
low-precision floating-point design in the FP8 era.

</details>


### [182] [Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects](https://arxiv.org/abs/2509.21923)
*Fumin Wang*

Main category: cs.LG

TL;DR: 为解决高风险领域中可解释模型预测性能受限的问题，本文提出了乘性-加性约束模型（MACMs）。该模型结合了乘性与加性部分，在保持可解释性的同时，显著提升了预测性能，超越了现有最先进的GAMs和CESR。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如医疗保健）应用机器学习时，可解释性至关重要。广义加性模型（GAMs）为保持可解释性而忽略高阶交互作用，导致预测性能受限。曲线遍历集回归（CESR）虽能处理所有特征交互并可视化形状函数，但其预测性能未能超越GAMs。因此，需要一种模型，既能处理高阶交互、保持可解释性，又能提供优越的预测性能。

Method: 本文引入了乘性-加性约束模型（MACMs）。MACMs通过增加一个加性部分来增强CESR，以解耦其交互项和独立项中交织的系数，从而有效拓宽假设空间。该模型由一个乘性部分和一个加性部分组成，两者的形状函数都可以自然地可视化，有助于用户理解特征如何参与决策过程。

Result: 实验结果表明，基于神经网络的MACMs在预测性能方面显著优于CESR和当前最先进的GAMs。

Conclusion: MACMs成功地结合了可解释性（通过可可视化的形状函数）和卓越的预测性能。它们在预测能力上超越了CESR和GAMs，为需要高可解释性和高性能的机器学习应用提供了一个改进方案。

Abstract: Interpretability is one of the considerations when applying machine learning
to high-stakes fields such as healthcare that involve matters of life safety.
Generalized Additive Models (GAMs) enhance interpretability by visualizing
shape functions. Nevertheless, to preserve interpretability, GAMs omit
higher-order interaction effects (beyond pairwise interactions), which imposes
significant constraints on their predictive performance. We observe that Curve
Ergodic Set Regression (CESR), a multiplicative model, naturally enables the
visualization of its shape functions and simultaneously incorporates both
interactions among all features and individual feature effects. Nevertheless,
CESR fails to demonstrate superior performance compared to GAMs. We introduce
Multiplicative-Additive Constrained Models (MACMs), which augment CESR with an
additive part to disentangle the intertwined coefficients of its interactive
and independent terms, thus effectively broadening the hypothesis space. The
model is composed of a multiplicative part and an additive part, whose shape
functions can both be naturally visualized, thereby assisting users in
interpreting how features participate in the decision-making process.
Consequently, MACMs constitute an improvement over both CESR and GAMs. The
experimental results indicate that neural network-based MACMs significantly
outperform both CESR and the current state-of-the-art GAMs in terms of
predictive performance.

</details>


### [183] [Can Data-Driven Dynamics Reveal Hidden Physics? There Is A Need for Interpretable Neural Operators](https://arxiv.org/abs/2510.02683)
*Wenhan Gao,Jian Luo,Fang Wan,Ruichen Xu,Xiang Liu,Haipeng Xing,Yi Liu*

Main category: cs.LG

TL;DR: 本文对神经算子的学习机制进行了分类和深入探讨，展示了它们学习隐藏物理模式的能力（但解释方法有限），并提出了一个实现SOTA性能的双空间多尺度模型，强调了将物理知识融入神经算子的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管神经算子在学习函数空间映射方面表现出色，但对其学习机制的深层理解尚不足，尤其是在结合物理原理进行数据驱动动力学模拟方面。

Method: ['将神经算子分为空间域模型和函数域模型两类，并基于此进行分析。', '提出一种解释神经算子预测过程的方法，以揭示其学习隐藏物理模式的能力。', '开发并展示了一个简单的双空间多尺度模型。']

Result: ['神经算子能够从数据中学习到隐藏的物理模式，但其解释方法目前仅限于特定情况。', '一个简单的双空间多尺度模型实现了最先进的性能（SOTA）。']

Conclusion: ['双空间多尺度模型在学习复杂物理方面潜力巨大，值得进一步深入研究。', '急需建立有原则的框架，将已知物理知识融入神经算子中，以提高模型的泛化能力并发现更多隐藏的物理现象。']

Abstract: Recently, neural operators have emerged as powerful tools for learning
mappings between function spaces, enabling data-driven simulations of complex
dynamics. Despite their successes, a deeper understanding of their learning
mechanisms remains underexplored. In this work, we classify neural operators
into two types: (1) Spatial domain models that learn on grids and (2)
Functional domain models that learn with function bases. We present several
viewpoints based on this classification and focus on learning data-driven
dynamics adhering to physical principles. Specifically, we provide a way to
explain the prediction-making process of neural operators and show that neural
operator can learn hidden physical patterns from data. However, this
explanation method is limited to specific situations, highlighting the urgent
need for generalizable explanation methods. Next, we show that a simple
dual-space multi-scale model can achieve SOTA performance and we believe that
dual-space multi-spatio-scale models hold significant potential to learn
complex physics and require further investigation. Lastly, we discuss the
critical need for principled frameworks to incorporate known physics into
neural operators, enabling better generalization and uncovering more hidden
physical phenomena.

</details>


### [184] [EvoSpeak: Large Language Models for Interpretable Genetic Programming-Evolved Heuristics](https://arxiv.org/abs/2510.02686)
*Meng Xu,Jiao Liu,Yew Soon Ong*

Main category: cs.LG

TL;DR: EvoSpeak是一个结合遗传编程（GP）和大型语言模型（LLM）的框架，旨在提高启发式算法进化的效率、透明度和适应性，解决GP启发式算法在复杂场景下的解释性、收敛速度和可迁移性问题。


<details>
  <summary>Details</summary>
Motivation: 遗传编程（GP）在复杂优化问题中表现出色，但在动态和大规模场景下，其生成的启发式算法往往过于复杂，导致难以解释、收敛缓慢和任务间迁移受限。

Method: EvoSpeak框架将GP与LLM相结合。它从高质量的GP启发式算法中学习并提取知识，利用这些知识来：1) 生成热启动种群以加速收敛；2) 将不透明的GP树转换为简洁的自然语言解释，提高可解释性和信任度；3) 实现知识迁移和偏好感知的启发式生成。该方法通过将GP的符号推理能力与LLM的解释和生成能力相结合。

Result: 通过在动态柔性作业车间调度（DFJSS）问题上的大量实验，EvoSpeak在单目标和多目标公式下均表现出更有效的启发式算法，提高了进化效率，并能生成人类可读的报告，增强了可用性。

Conclusion: EvoSpeak通过结合GP的符号推理能力和LLM的解释与生成能力，促进了真实世界优化问题中智能、透明和用户对齐的启发式算法的开发。

Abstract: Genetic programming (GP) has demonstrated strong effectiveness in evolving
tree-structured heuristics for complex optimization problems. Yet, in dynamic
and large-scale scenarios, the most effective heuristics are often highly
complex, hindering interpretability, slowing convergence, and limiting
transferability across tasks. To address these challenges, we present EvoSpeak,
a novel framework that integrates GP with large language models (LLMs) to
enhance the efficiency, transparency, and adaptability of heuristic evolution.
EvoSpeak learns from high-quality GP heuristics, extracts knowledge, and
leverages this knowledge to (i) generate warm-start populations that accelerate
convergence, (ii) translate opaque GP trees into concise natural-language
explanations that foster interpretability and trust, and (iii) enable knowledge
transfer and preference-aware heuristic generation across related tasks. We
verify the effectiveness of EvoSpeak through extensive experiments on dynamic
flexible job shop scheduling (DFJSS), under both single- and multi-objective
formulations. The results demonstrate that EvoSpeak produces more effective
heuristics, improves evolutionary efficiency, and delivers human-readable
reports that enhance usability. By coupling the symbolic reasoning power of GP
with the interpretative and generative strengths of LLMs, EvoSpeak advances the
development of intelligent, transparent, and user-aligned heuristics for
real-world optimization problems.

</details>


### [185] [Fine-Tuning Diffusion Models via Intermediate Distribution Shaping](https://arxiv.org/abs/2510.02692)
*Gautham Govind Anil,Shaan Ul Haque,Nithish Kannen,Dheeraj Nagaraj,Sanjay Shakkottai,Karthikeyan Shanmugam*

Main category: cs.LG

TL;DR: 针对扩散模型难以用策略梯度（如PPO）进行奖励函数引导微调的问题，本文提出了GRAFT（统一RAFT变体，隐式执行PPO）和P-GRAFT（在中间噪声水平进行分布塑造）方法，并引入了逆噪声校正以改进流模型。实验证明，这些方法在文本到图像、布局、分子生成等任务上，特别是T2I任务中，表现优于现有策略梯度方法。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散模型虽然能有效捕捉数据分布，但需通过奖励函数进一步调整以适应下游应用。然而，由于边际似然难以计算，传统的策略梯度方法（如PPO）不适用于扩散模型。

Method: 首先，将基于拒绝采样的微调（RAFT）变体统一为GRAFT，并证明其隐式执行带有重塑奖励的PPO。其次，提出P-GRAFT，用于在中间噪声水平塑造分布，并通过偏差-方差权衡从数学上解释其有效性。最后，引入逆噪声校正，以在不依赖显式奖励的情况下改进流模型。

Result: 将所提出的框架应用于Stable Diffusion 2时，在流行的T2I基准测试中，VQAScore表现优于策略梯度方法，并相对于基础模型提升了8.81%。对于无条件图像生成，逆噪声校正以更低的FLOPs/图像改善了生成图像的FID。

Conclusion: 本文提出的GRAFT、P-GRAFT以及逆噪声校正方法，有效解决了扩散模型通过奖励函数进行微调的难题。这些方法在多种生成任务中表现出优于现有策略梯度方法的性能，并实现了高效的分布塑造。

Abstract: Diffusion models are widely used for generative tasks across domains. While
pre-trained diffusion models effectively capture the training data
distribution, it is often desirable to shape these distributions using reward
functions to align with downstream applications. Policy gradient methods, such
as Proximal Policy Optimization (PPO), are widely used in the context of
autoregressive generation. However, the marginal likelihoods required for such
methods are intractable for diffusion models, leading to alternative proposals
and relaxations. In this context, we unify variants of Rejection sAmpling based
Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs PPO with
reshaped rewards. We then introduce P-GRAFT to shape distributions at
intermediate noise levels and demonstrate empirically that this can lead to
more effective fine-tuning. We mathematically explain this via a bias-variance
tradeoff. Motivated by this, we propose inverse noise correction to improve
flow models without leveraging explicit rewards. We empirically evaluate our
methods on text-to-image(T2I) generation, layout generation, molecule
generation and unconditional image generation. Notably, our framework, applied
to Stable Diffusion 2, improves over policy gradient methods on popular T2I
benchmarks in terms of VQAScore and shows an $8.81\%$ relative improvement over
the base model. For unconditional image generation, inverse noise correction
improves FID of generated images at lower FLOPs/image.

</details>


### [186] [RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization](https://arxiv.org/abs/2510.02695)
*Kai Fukazawa,Kunal Mundada,Iman Soltani*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In safety-critical domains where online data collection is infeasible,
offline reinforcement learning (RL) offers an attractive alternative but only
if policies deliver high returns without incurring catastrophic lower-tail
risk. Prior work on risk-averse offline RL achieves safety at the cost of value
conservatism and restricted policy classes, whereas expressive policies are
only used in risk-neutral settings. Here, we address this gap by introducing
the \textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)} framework, which
couples an \emph{expressive generative actor} with a distributional critic. The
RAMAC differentiates composite objective combining distributional risk and BC
loss through the generative path, achieving risk-sensitive learning in complex
multimodal scenarios. We instantiate RAMAC with diffusion and flow-matching
actors and observe consistent gains in $\mathrm{CVaR}_{0.1}$ while maintaining
strong returns on most Stochastic-D4RL tasks. Code:
https://github.com/KaiFukazawa/RAMAC.git

</details>


### [187] [A Novel Unified Lightweight Temporal-Spatial Transformer Approach for Intrusion Detection in Drone Networks](https://arxiv.org/abs/2510.02711)
*Tarun Kumar Biswas,Ashrafun Zannat,Waqas Ishtiaq,Md. Alamgir Hossain*

Main category: cs.LG

TL;DR: 本文提出TSLT-Net，一种轻量级、基于时空Transformer的无人机网络入侵检测系统，实现了高精度（多类别99.99%，二元100%）和极低资源消耗，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 无人机网络面临严重网络安全挑战，现有入侵检测机制在动态、资源受限的无人机环境中缺乏必要的适应性、效率和通用性。

Method: 提出TSLT-Net，一种新颖的轻量级统一时空Transformer入侵检测系统。该系统利用自注意力机制建模网络流量的时序模式和空间依赖性，通过简化的预处理流程，在一个架构内同时支持多类别攻击分类和二元异常检测。

Result: 在ISOT无人机异常检测数据集上，TSLT-Net在多类别检测中达到99.99%的准确率，在二元异常检测中达到100%的准确率。同时，其内存占用仅为0.04 MB，拥有9722个可训练参数。

Conclusion: TSLT-Net是一种高效且可扩展的实时无人机网络安全解决方案，特别适合部署在任务关键型UAV系统的边缘设备上。

Abstract: The growing integration of drones across commercial, industrial, and civilian
domains has introduced significant cybersecurity challenges, particularly due
to the susceptibility of drone networks to a wide range of cyberattacks.
Existing intrusion detection mechanisms often lack the adaptability,
efficiency, and generalizability required for the dynamic and resource
constrained environments in which drones operate. This paper proposes TSLT-Net,
a novel lightweight and unified Temporal Spatial Transformer based intrusion
detection system tailored specifically for drone networks. By leveraging self
attention mechanisms, TSLT-Net effectively models both temporal patterns and
spatial dependencies in network traffic, enabling accurate detection of diverse
intrusion types. The framework includes a streamlined preprocessing pipeline
and supports both multiclass attack classification and binary anomaly detection
within a single architecture. Extensive experiments conducted on the ISOT Drone
Anomaly Detection Dataset, consisting of more than 2.3 million labeled records,
demonstrate the superior performance of TSLT-Net with 99.99 percent accuracy in
multiclass detection and 100 percent in binary anomaly detection, while
maintaining a minimal memory footprint of only 0.04 MB and 9722 trainable
parameters. These results establish TSLT-Net as an effective and scalable
solution for real time drone cybersecurity, particularly suitable for
deployment on edge devices in mission critical UAV systems.

</details>


### [188] [CST-AFNet: A dual attention-based deep learning framework for intrusion detection in IoT networks](https://arxiv.org/abs/2510.02717)
*Waqas Ishtiaq,Ashrafun Zannat,A. H. M. Shahariar Parvez,Md. Alamgir Hossain,Muntasir Hasan Kanchan,Muhammad Masud Tarek*

Main category: cs.LG

TL;DR: 本研究提出CST AFNet，一个结合多尺度CNN、BiGRU和双注意力机制的深度学习框架，用于物联网（IoT）网络的入侵检测，并在Edge IIoTset数据集上实现了卓越的准确率和综合性能。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）的快速发展带来了智能自动化，但也因其异构、资源受限和分布式特性引入了复杂的网络安全挑战，亟需一个鲁棒的入侵检测解决方案来应对这些威胁。

Method: 本研究提出了CST AFNet，一个新颖的基于双注意力机制的深度学习框架。该模型整合了多尺度卷积神经网络（CNN）进行空间特征提取，双向门控循环单元（BiGRU）用于捕获时间依赖性，并引入通道和时间双注意力机制以增强对数据中关键模式的关注。模型在包含15种攻击类型和正常流量的Edge IIoTset数据集上进行训练和评估。

Result: CST AFNet在Edge IIoTset数据集上取得了99.97%的准确率。此外，该模型在宏平均精确率、召回率和F1分数上均超过99.3%，显著优于传统的深度学习模型。

Conclusion: CST AFNet被证实是复杂IoT和IIoT环境中实时网络威胁检测的强大且可扩展的解决方案，为构建更安全、智能和自适应的网络物理系统铺平了道路。

Abstract: The rapid expansion of the Internet of Things (IoT) has revolutionized modern
industries by enabling smart automation and real time connectivity. However,
this evolution has also introduced complex cybersecurity challenges due to the
heterogeneous, resource constrained, and distributed nature of these
environments. To address these challenges, this research presents CST AFNet, a
novel dual attention based deep learning framework specifically designed for
robust intrusion detection in IoT networks. The model integrates multi scale
Convolutional Neural Networks (CNNs) for spatial feature extraction,
Bidirectional Gated Recurrent Units (BiGRUs) for capturing temporal
dependencies, and a dual attention mechanism, channel and temporal attention,
to enhance focus on critical patterns in the data. The proposed method was
trained and evaluated on the Edge IIoTset dataset, a comprehensive and
realistic benchmark containing more than 2.2 million labeled instances spanning
15 attack types and benign traffic, collected from a seven layer industrial
testbed. Our proposed model achieves outstanding accuracy for both 15 attack
types and benign traffic. CST AFNet achieves 99.97 percent accuracy. Moreover,
this model demonstrates exceptional performance with macro averaged precision,
recall, and F1 score all above 99.3 percent. Experimental results show that CST
AFNet achieves superior detection accuracy, significantly outperforming
traditional deep learning models. The findings confirm that CST AFNet is a
powerful and scalable solution for real time cyber threat detection in complex
IoT and IIoT environments, paving the way for more secure, intelligent, and
adaptive cyber physical systems.

</details>


### [189] [Hyperparameter Loss Surfaces Are Simple Near their Optima](https://arxiv.org/abs/2510.02721)
*Nicholas Lourie,He He,Kyunghyun Cho*

Main category: cs.LG

TL;DR: 本文提出一种新理论和工具，通过随机搜索揭示超参数损失曲面在接近最优值时的简单渐近结构，进而推导出新的随机搜索渐近定律，以解释和预测其收敛性，并提供分析超参数有效性的能力。


<details>
  <summary>Details</summary>
Motivation: 现代模型因规模庞大，难以进行广泛的超参数搜索。尽管超参数对模型性能影响巨大，但目前缺乏有效的工具来理解超参数损失曲面。

Method: 研究者发现超参数损失曲面中存在新颖结构，尤其在接近最优值时呈现出简单的渐近结构，其特征由有效维度和最佳可能损失等定义。为揭示这种渐近机制，作者开发了一种基于随机搜索的新技术。在此机制下，随机搜索的最佳得分遵循一种新发现的分布。

Result: 研究揭示了超参数损失曲面在渐近机制下的简单结构，其特征可由新发现的随机搜索最佳得分分布的参数精确定义。基于这些特征，本文推导出一个新的随机搜索渐近定律，能够有效解释并推断其收敛性。

Conclusion: 这些新工具支持新的分析，例如为最佳可能性能提供置信区间，或确定超参数的有效数量。相关工具已开源。

Abstract: Hyperparameters greatly impact models' capabilities; however, modern models
are too large for extensive search. Instead, researchers design recipes that
train well across scales based on their understanding of the hyperparameters.
Despite this importance, few tools exist for understanding the hyperparameter
loss surface. We discover novel structure in it and propose a new theory
yielding such tools. The loss surface is complex, but as you approach the
optimum simple structure emerges. It becomes characterized by a few basic
features, like its effective dimension and the best possible loss. To uncover
this asymptotic regime, we develop a novel technique based on random search.
Within this regime, the best scores from random search take on a new
distribution we discover. Its parameters are exactly the features defining the
loss surface in the asymptotic regime. From these features, we derive a new
asymptotic law for random search that can explain and extrapolate its
convergence. These new tools enable new analyses, such as confidence intervals
for the best possible performance or determining the effective number of
hyperparameters. We make these tools available at
https://github.com/nicholaslourie/opda .

</details>


### [190] [Accuracy Law for the Future of Deep Time Series Forecasting](https://arxiv.org/abs/2510.02729)
*Yuxuan Wang,Haixu Wu,Yuezhou Ma,Yuchen Fang,Ziyi Zhang,Yong Liu,Shiyu Wang,Zhou Ye,Yang Xiang,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: 深度时间序列预测存在固有误差下限。本文提出“准确度定律”，揭示了深度模型最小预测误差与窗口序列模式复杂度的指数关系，用于识别饱和任务并指导未来研究。


<details>
  <summary>Details</summary>
Motivation: 深度时间序列预测研究者因微小改进而方向困惑，且该领域存在固有的非零误差下限。本文旨在估算深度时间序列预测的性能上限（即误差下限），以明确研究目标并避免饱和任务。

Method: 超越传统序列级可预测性指标，本文关注窗口级属性。通过对超过2800个新训练的深度预测模型进行严格统计测试。

Result: 发现深度模型最小预测误差与窗口级序列模式复杂度之间存在显著的指数关系，称之为“准确度定律”。

Conclusion: 所提出的“准确度定律”成功识别了常用基准中的饱和任务，并推导出了大型时间序列模型的有效训练策略，为未来研究提供了有价值的见解。

Abstract: Deep time series forecasting has emerged as a booming direction in recent
years. Despite the exponential growth of community interests, researchers are
sometimes confused about the direction of their efforts due to minor
improvements on standard benchmarks. In this paper, we notice that, unlike
image recognition, whose well-acknowledged and realizable goal is 100%
accuracy, time series forecasting inherently faces a non-zero error lower bound
due to its partially observable and uncertain nature. To pinpoint the research
objective and release researchers from saturated tasks, this paper focuses on a
fundamental question: how to estimate the performance upper bound of deep time
series forecasting? Going beyond classical series-wise predictability metrics,
e.g., ADF test, we realize that the forecasting performance is highly related
to window-wise properties because of the sequence-to-sequence forecasting
paradigm of deep time series models. Based on rigorous statistical tests of
over 2,800 newly trained deep forecasters, we discover a significant
exponential relationship between the minimum forecasting error of deep models
and the complexity of window-wise series patterns, which is termed the accuracy
law. The proposed accuracy law successfully guides us to identify saturated
tasks from widely used benchmarks and derives an effective training strategy
for large time series models, offering valuable insights for future research.

</details>


### [191] [Dale meets Langevin: A Multiplicative Denoising Diffusion Model](https://arxiv.org/abs/2510.02730)
*Nishanth Shetty,Madhava Prasath,Chandra Sekhar Seelamantula*

Main category: cs.LG

TL;DR: 本文将生物启发学习（基于Dale's定律的指数梯度下降）与几何布朗运动（GBM）相联系，通过离散化逆时间SDE推导乘法更新规则，并提出一种适用于对数正态数据的乘法去噪得分匹配形式，构建了一个新型生成模型，并在图像生成任务中展现出有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然梯度下降在机器学习中表现出色，但其学习机制与生物系统不符。受Dale's定律启发的生物学习方法（如导致对数正态突触权重的指数梯度下降）为构建更符合生物学原理的学习技术提供了新思路。

Method: 首先，利用对数正态分布与几何布朗运动（GBM）随机微分方程的内在联系，通过离散化逆时间SDE来推导一个乘法更新规则。其次，提出了一种新的乘法去噪得分匹配形式，该形式能自然地适应对数正态分布的正数据，并将其应用于训练基于得分的图像生成模型。

Result: 1. 推导出的乘法更新规则与基于Dale's定律的指数梯度下降的采样等效形式相符。2. 提出的乘法去噪得分匹配形式是处理对数正态分布数据的天然选择。3. 成功构建了一种从对数正态密度开始的样本生成新型乘法更新方案。4. 在MNIST、Fashion MNIST和Kuzushiji数据集上的实验结果验证了该方案的生成能力。

Conclusion: 本研究首次提出了一种基于几何布朗运动，采用乘法更新的生物启发式生成模型，为探索生物学上更合理的AI学习范式开辟了新途径。

Abstract: Gradient descent has proven to be a powerful and effective technique for
optimization in numerous machine learning applications. Recent advances in
computational neuroscience have shown that learning in standard gradient
descent optimization formulation is not consistent with learning in biological
systems. This has opened up interesting avenues for building biologically
inspired learning techniques. One such approach is inspired by Dale's law,
which states that inhibitory and excitatory synapses do not swap roles during
the course of learning. The resulting exponential gradient descent optimization
scheme leads to log-normally distributed synaptic weights. Interestingly, the
density that satisfies the Fokker-Planck equation corresponding to the
stochastic differential equation (SDE) with geometric Brownian motion (GBM) is
the log-normal density. Leveraging this connection, we start with the SDE
governing geometric Brownian motion, and show that discretizing the
corresponding reverse-time SDE yields a multiplicative update rule, which
surprisingly, coincides with the sampling equivalent of the exponential
gradient descent update founded on Dale's law. Furthermore, we propose a new
formalism for multiplicative denoising score-matching, subsuming the loss
function proposed by Hyvaerinen for non-negative data. Indeed, log-normally
distributed data is positive and the proposed score-matching formalism turns
out to be a natural fit. This allows for training of score-based models for
image data and results in a novel multiplicative update scheme for sample
generation starting from a log-normal density. Experimental results on MNIST,
Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the
new scheme. To the best of our knowledge, this is the first instance of a
biologically inspired generative model employing multiplicative updates,
founded on geometric Brownian motion.

</details>


### [192] [Hybrid-Collaborative Augmentation and Contrastive Sample Adaptive-Differential Awareness for Robust Attributed Graph Clustering](https://arxiv.org/abs/2510.02731)
*Tianxiang Zhao,Youqing Wang,Jinlu Wang,Jiapu Wang,Mingliang Cui,Junbin Gao,Jipeng Guo*

Main category: cs.LG

TL;DR: 本文提出一种鲁棒属性图聚类(RAGC)方法，通过混合协同增强(HCA)同时进行节点和边的增强，并结合对比样本自适应差异感知(CSADA)策略区别对待难易样本，从而提升图表示学习的判别能力。


<details>
  <summary>Details</summary>
Motivation: 现有对比属性图聚类(CAGC)方法主要关注节点级嵌入增强，忽视了边级增强及其与节点增强的交互；同时，它们平等对待所有对比样本对，忽略了难易样本间的显著差异，限制了判别能力。

Method: 提出RAGC方法，包含两个核心模块：1) **混合协同增强(HCA)**：同步执行节点级和边级嵌入表示及增强，以建立更全面的相似性度量，并由判别性相似性引导边增强。2) **对比样本自适应差异感知(CSADA)**：利用高置信度伪标签识别对比样本对，并通过创新的权重调制函数对它们进行差异化处理。HCA和CSADA模块相互促进。

Result: 在六个基准数据集上进行的全面图聚类评估表明，所提出的RAGC方法优于几种最先进的CAGC方法，验证了其有效性。

Conclusion: RAGC通过结合混合协同增强和对比样本自适应差异感知，有效解决了现有CAGC方法在数据增强粒度和对比样本处理上的局限性，显著增强了表示学习的判别能力。

Abstract: Due to its powerful capability of self-supervised representation learning and
clustering, contrastive attributed graph clustering (CAGC) has achieved great
success, which mainly depends on effective data augmentation and contrastive
objective setting. However, most CAGC methods utilize edges as auxiliary
information to obtain node-level embedding representation and only focus on
node-level embedding augmentation. This approach overlooks edge-level embedding
augmentation and the interactions between node-level and edge-level embedding
augmentations across various granularity. Moreover, they often treat all
contrastive sample pairs equally, neglecting the significant differences
between hard and easy positive-negative sample pairs, which ultimately limits
their discriminative capability. To tackle these issues, a novel robust
attributed graph clustering (RAGC), incorporating hybrid-collaborative
augmentation (HCA) and contrastive sample adaptive-differential awareness
(CSADA), is proposed. First, node-level and edge-level embedding
representations and augmentations are simultaneously executed to establish a
more comprehensive similarity measurement criterion for subsequent contrastive
learning. In turn, the discriminative similarity further consciously guides
edge augmentation. Second, by leveraging pseudo-label information with high
confidence, a CSADA strategy is elaborately designed, which adaptively
identifies all contrastive sample pairs and differentially treats them by an
innovative weight modulation function. The HCA and CSADA modules mutually
reinforce each other in a beneficent cycle, thereby enhancing discriminability
in representation learning. Comprehensive graph clustering evaluations over six
benchmark datasets demonstrate the effectiveness of the proposed RAGC against
several state-of-the-art CAGC methods.

</details>


### [193] [TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via Preemptive Scheduling](https://arxiv.org/abs/2510.02758)
*Junyi Chen,Chuheng Du,Renyuan Liu,Shuochao Yao,Dingtian Yan,Jiang Liao,Shengzhong Liu,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: TokenFlow是一种新型LLM服务系统，通过抢占式请求调度和主动KV缓存管理，显著提升了文本流式生成性能，尤其在有效吞吐量和首个token响应时间方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统因非抢占式请求调度和被动内存管理，在请求高峰期导致资源利用率低和请求处理并行度差，无法有效平衡响应速度（低首个token时间）和稳定生成速度。

Method: 本文提出了TokenFlow系统，其核心方法是抢占式请求调度和主动Key-Value (KV) 缓存管理。具体而言，TokenFlow根据实时token缓冲占用和消费速率动态调整请求优先级，并在后台主动在GPU和CPU内存之间传输KV缓存，同时重叠I/O与计算以最小化请求抢占开销。

Result: 在Llama3-8B和Qwen2.5-32B模型以及多种GPU（RTX 4090, A6000, H200）上的广泛实验表明，TokenFlow实现了高达82.5%的有效吞吐量提升，同时将P99首个token响应时间（TTFT）降低高达80.2%，且不降低整体token吞吐量。

Conclusion: TokenFlow通过其创新的调度和KV缓存管理策略，有效解决了LLM流式生成中的性能瓶颈，显著提升了用户体验相关的关键指标，如有效吞吐量和响应速度，是实现高效实时LLM交互的关键进步。

Abstract: Real-time LLM interactions demand streamed token generations, where text
tokens are progressively generated and delivered to users while balancing two
objectives: responsiveness (i.e., low time-to-first-token) and steady
generation (i.e.,required time-between-tokens). Standard LLM serving systems
suffer from the inflexibility caused by non-preemptive request scheduling and
reactive memory management, leading to poor resource utilization and low
request processing parallelism under request bursts. Therefore, we present
TokenFlow, a novel LLM serving system with enhanced text streaming performance
via preemptive request scheduling and proactive key-value (KV) cache
management. TokenFlow dynamically prioritizes requests based on real-time token
buffer occupancy and token consumption rate, while actively transferring KV
cache between GPU and CPU memory in the background and overlapping I/O with
computation to minimize request preemption overhead. Extensive experiments on
Llama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)
demonstrate that TokenFlow achieves up to 82.5% higher effective throughput
(accounting for actual user consumption) while reducing P99 TTFT by up to
80.2%, without degrading overall token throughput.

</details>


### [194] [Fusing Multi- and Hyperspectral Satellite Data for Harmful Algal Bloom Monitoring with Self-Supervised and Hierarchical Deep Learning](https://arxiv.org/abs/2510.02763)
*Nicholas LaHaye,Kelly M. Luis,Michelle M. Gierach*

Main category: cs.LG

TL;DR: 本文提出SIT-FUSE自监督机器学习框架，利用多传感器卫星数据（包括SIF）在无需标签数据的情况下检测和绘制有害藻华（HAB）的严重程度和物种组成。


<details>
  <summary>Details</summary>
Motivation: 解决标签稀缺环境下有害藻华（HAB）监测的扩展性挑战，并推动自监督学习在全球水生生物地球化学领域的实际应用。

Method: SIT-FUSE框架融合了VIIRS、MODIS、Sentinel-3、PACE等操作仪器的反射数据与TROPOMI的太阳诱导荧光（SIF）数据。它采用自监督表征学习和分层深度聚类方法，将浮游植物浓度和物种划分为可解释的类别，并与墨西哥湾和南加州的现场数据进行了验证。

Result: 研究结果显示，该框架对总浮游植物、Karenia brevis、Alexandrium spp.和Pseudo-nitzschia spp.的测量结果表现出高度一致性。

Conclusion: 这项工作推动了在标签稀缺环境下可扩展的HAB监测，并通过分层嵌入实现了探索性分析，是自监督学习应用于全球水生生物地球化学操作化的关键进展。

Abstract: We present a self-supervised machine learning framework for detecting and
mapping harmful algal bloom (HAB) severity and speciation using multi-sensor
satellite data. By fusing reflectance data from operational instruments (VIIRS,
MODIS, Sentinel-3, PACE) with TROPOMI solar-induced fluorescence (SIF), our
framework, called SIT-FUSE, generates HAB severity and speciation products
without requiring per-instrument labeled datasets. The framework employs
self-supervised representation learning, hierarchical deep clustering to
segment phytoplankton concentrations and speciations into interpretable
classes, validated against in-situ data from the Gulf of Mexico and Southern
California (2018-2025). Results show strong agreement with total phytoplankton,
Karenia brevis, Alexandrium spp., and Pseudo-nitzschia spp. measurements. This
work advances scalable HAB monitoring in label-scarce environments while
enabling exploratory analysis via hierarchical embeddings: a critical step
toward operationalizing self-supervised learning for global aquatic
biogeochemistry.

</details>


### [195] [Curl Descent: Non-Gradient Learning Dynamics with Sign-Diverse Plasticity](https://arxiv.org/abs/2510.02765)
*Hugo Ninou,Jonathan Kadmon,N. Alex Cayco-Gajic*

Main category: cs.LG

TL;DR: 生物神经网络的学习动态可能包含非梯度“旋度”成分，这些成分在某些情况下能有效优化损失函数，甚至加速学习，挑战了纯梯度下降理论。


<details>
  <summary>Details</summary>
Motivation: 现有观点认为生物神经网络学习可能采用基于梯度的策略，但实验发现突触可塑性规则多样，且非梯度“旋度”项可能在学习中发挥作用，甚至优化损失函数。因此，本研究旨在探究这些非梯度项的影响。

Method: 在可分析的学生-教师前馈网络框架下，通过引入表现出规则翻转可塑性的神经元，系统地引入非梯度“旋度”动态进行分析。

Result: 小的旋度项能保持原始解流形的稳定性，使学习动态类似于梯度下降。然而，当旋度项超过临界值时，强的旋度项会破坏解流形的稳定性。根据网络架构不同，这可能导致破坏性能的混沌学习动态，或反直觉地通过允许权重动态暂时攀升损失函数来逃离鞍点，从而加速学习。

Conclusion: 本研究识别了能够通过多样化学习规则支持稳健学习的特定网络架构，为神经网络中基于梯度的规范学习理论提供了重要的反驳视角。

Abstract: Gradient-based algorithms are a cornerstone of artificial neural network
training, yet it remains unclear whether biological neural networks use similar
gradient-based strategies during learning. Experiments often discover a
diversity of synaptic plasticity rules, but whether these amount to an
approximation to gradient descent is unclear. Here we investigate a previously
overlooked possibility: that learning dynamics may include fundamentally
non-gradient "curl"-like components while still being able to effectively
optimize a loss function. Curl terms naturally emerge in networks with
inhibitory-excitatory connectivity or Hebbian/anti-Hebbian plasticity,
resulting in learning dynamics that cannot be framed as gradient descent on any
objective. To investigate the impact of these curl terms, we analyze
feedforward networks within an analytically tractable student-teacher
framework, systematically introducing non-gradient dynamics through neurons
exhibiting rule-flipped plasticity. Small curl terms preserve the stability of
the original solution manifold, resulting in learning dynamics similar to
gradient descent. Beyond a critical value, strong curl terms destabilize the
solution manifold. Depending on the network architecture, this loss of
stability can lead to chaotic learning dynamics that destroy performance. In
other cases, the curl terms can counterintuitively speed learning compared to
gradient descent by allowing the weight dynamics to escape saddles by
temporarily ascending the loss. Our results identify specific architectures
capable of supporting robust learning via diverse learning rules, providing an
important counterpoint to normative theories of gradient-based learning in
neural networks.

</details>


### [196] [A Granular Study of Safety Pretraining under Model Abliteration](https://arxiv.org/abs/2510.02768)
*Shashank Agnihotri,Jonas Jakubassa,Priyam Dey,Sachin Goyal,Bernt Schiele,Venkatesh Babu Radhakrishnan,Margret Keuper*

Main category: cs.LG

TL;DR: 本研究评估了开放权重LLM在推理时激活编辑（如模型擦除）下，其安全干预措施（如拒绝训练）的鲁棒性。通过对多模型、多提示和评判者评估，量化了安全组件的稳定性及评判标准的影响，并提出了将推理时编辑纳入安全评估的实用协议。


<details>
  <summary>Details</summary>
Motivation: 开放权重LLM在推理时可通过简单的激活编辑进行修改，这引发了一个实际的安全问题：常见的安全干预措施（如拒绝训练或元标签训练）在这些编辑下是否仍然有效？

Method: 1. 采用“模型擦除”（abliteration）技术，一种旨在移除对拒绝敏感方向的轻量级投影技术。
2. 对SmolLM2-1.7B的安全预训练检查点序列及广泛使用的开放基线进行受控评估。
3. 对20个系统（包括原始和擦除后的）中的每一个，发布100个包含平衡有害和无害案例的提示。
4. 使用多个评判者将响应分类为“拒绝”或“非拒绝”，并在一小部分人工标记子集上验证评判者的忠实度。
5. 探究模型是否能识别其自身输出中的拒绝。

Result: 1. 提供了在模型擦除这类推理时编辑下，哪些数据中心的安全组件保持鲁棒性的检查点级别特性描述。
2. 量化了评判者选择如何影响评估结果。

Conclusion: 1. 本研究明确了在推理时激活编辑（如模型擦除）下，不同数据中心安全组件的鲁棒性。
2. 强调了评判者选择对安全评估结果的关键影响。
3. 提出了一个将推理时编辑整合到安全评估中的实用协议。

Abstract: Open-weight LLMs can be modified at inference time with simple activation
edits, which raises a practical question for safety: do common safety
interventions like refusal training or metatag training survive such edits? We
study model abliteration, a lightweight projection technique designed to remove
refusal-sensitive directions, and conduct a controlled evaluation across a
granular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside
widely used open baselines. For each of 20 systems, original and abliterated,
we issue 100 prompts with balanced harmful and harmless cases, classify
responses as **Refusal** or **Non-Refusal** using multiple judges, and validate
judge fidelity on a small human-labeled subset. We also probe whether models
can identify refusal in their own outputs. Our study produces a
checkpoint-level characterization of which data-centric safety components
remain robust under abliteration, quantifies how judge selection influences
evaluation outcomes, and outlines a practical protocol for integrating
inference-time edits into safety assessments. Code:
https://github.com/shashankskagnihotri/safety_pretraining.

</details>


### [197] [Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification](https://arxiv.org/abs/2510.02779)
*Yuanfan Li,Yunwen Lei,Zheng-Chu Guo,Yiming Ying*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances have significantly improved our understanding of the
generalization performance of gradient descent (GD) methods in deep neural
networks. A natural and fundamental question is whether GD can achieve
generalization rates comparable to the minimax optimal rates established in the
kernel setting. Existing results either yield suboptimal rates of
$O(1/\sqrt{n})$, or focus on networks with smooth activation functions,
incurring exponential dependence on network depth $L$. In this work, we
establish optimal generalization rates for GD with deep ReLU networks by
carefully trading off optimization and generalization errors, achieving only
polynomial dependence on depth. Specifically, under the assumption that the
data are NTK separable from the margin $\gamma$, we prove an excess risk rate
of $\widetilde{O}(L^4 (1 + \gamma L^2) / (n \gamma^2))$, which aligns with the
optimal SVM-type rate $\widetilde{O}(1 / (n \gamma^2))$ up to depth-dependent
factors. A key technical contribution is our novel control of activation
patterns near a reference model, enabling a sharper Rademacher complexity bound
for deep ReLU networks trained with gradient descent.

</details>


### [198] [OptunaHub: A Platform for Black-Box Optimization](https://arxiv.org/abs/2510.02798)
*Yoshihiko Ozaki,Shuhei Watanabe,Toshihiko Yanase*

Main category: cs.LG

TL;DR: OptunaHub是一个社区平台，旨在集中黑盒优化(BBO)方法和基准，解决跨领域研究碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 黑盒优化(BBO)在AutoML和材料信息学等领域推动进步，但其研究工作常在不同领域间碎片化。

Method: 引入OptunaHub平台，提供统一的Python API、贡献者包注册表和Web界面，用于集中BBO方法和基准。

Result: 平台旨在促进BBO方法的可搜索性和跨领域研究，并建立贡献和应用的良性循环。

Conclusion: OptunaHub通过提供一个集中的BBO社区平台，旨在解决研究碎片化问题，促进跨领域合作与知识共享。

Abstract: Black-box optimization (BBO) drives advances in domains such as AutoML and
Materials Informatics, yet research efforts often remain fragmented across
domains. We introduce OptunaHub (https://hub.optuna.org/), a community platform
that centralizes BBO methods and benchmarks. OptunaHub provides unified Python
APIs, a contributor package registry, and a web interface to promote
searchability and cross-domain research. OptunaHub aims to foster a virtuous
cycle of contributions and applications. The source code is publicly available
in the optunahub, optunahub-registry, and optunahub-web repositories under the
Optuna organization on GitHub (https://github.com/optuna/).

</details>


### [199] [Relevance-Aware Thresholding in Online Conformal Prediction for Time Series](https://arxiv.org/abs/2510.02809)
*Théo Dupuy,Binbin Xu,Stéphane Perrey,Jacky Montmain,Abdelhak Imoussaten*

Main category: cs.LG

TL;DR: 本文提出了一种在线保形预测（OCP）的新方法，通过引入量化预测区间相关性的函数来优化阈值更新步骤，从而在保持覆盖有效性的前提下，获得更窄的预测区间。


<details>
  <summary>Details</summary>
Motivation: 现有的OCP方法在阈值更新时，主要关注预测区间的有效性（即真实值是否在区间内），而忽略了区间本身的相关性或信息量，这可能导致阈值突变和较宽的预测区间。

Method: 我们通过用一类更广泛的函数替代传统的二元评估（在内/在外），来量化预测区间与真实值之间的相关性，从而增强阈值更新步骤。

Result: 在真实世界数据集上的实验结果表明，这种方法能够比现有OCP方法产生更紧密的（更窄的）预测区间，同时保持覆盖有效性。

Conclusion: 通过在OCP阈值更新中考虑预测区间的相关性，可以有效防止阈值骤变，并获得更窄、更具信息量的预测区间，优于现有方法。

Abstract: Uncertainty quantification has received considerable interest in recent works
in Machine Learning. In particular, Conformal Prediction (CP) gains ground in
this field. For the case of time series, Online Conformal Prediction (OCP)
becomes an option to address the problem of data distribution shift over time.
Indeed, the idea of OCP is to update a threshold of some quantity (whether the
miscoverage level or the quantile) based on the distribution observation. To
evaluate the performance of OCP methods, two key aspects are typically
considered: the coverage validity and the prediction interval width
minimization. Recently, new OCP methods have emerged, offering long-run
coverage guarantees and producing more informative intervals. However, during
the threshold update step, most of these methods focus solely on the validity
of the prediction intervals~--~that is, whether the ground truth falls inside
or outside the interval~--~without accounting for their relevance. In this
paper, we aim to leverage this overlooked aspect. Specifically, we propose
enhancing the threshold update step by replacing the binary evaluation
(inside/outside) with a broader class of functions that quantify the relevance
of the prediction interval using the ground truth. This approach helps prevent
abrupt threshold changes, potentially resulting in narrower prediction
intervals. Indeed, experimental results on real-world datasets suggest that
these functions can produce tighter intervals compared to existing OCP methods
while maintaining coverage validity.

</details>


### [200] [Dissecting Transformers: A CLEAR Perspective towards Green AI](https://arxiv.org/abs/2510.02810)
*Hemang Jain,Shailender Goyal,Divyansh Pandey,Karthik Vaidhyanathan*

Main category: cs.LG

TL;DR: 本文提出了一种名为CLEAR的组件级能耗评估方法，首次对Transformer架构核心组件的推理能耗进行细粒度分析，发现Attention模块每浮点运算消耗的能量显著更高，揭示了FLOPs不足以衡量组件级真实能耗，并为构建节能型LLMs提供了优化方向。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）的推理能耗已成为AI领域主要的能源足迹，但现有可持续性研究因缺乏细粒度测量方法，仅报告粗略的模型级指标，将能效视为次要目标，未能深入分析组件层面的能耗。

Method: 提出了一种新颖的方法——组件级重复采样能量评估（Component-Level Energy Assessment via Repeated sampling, CLEAR），以克服微秒级组件执行与毫秒级能量传感器监测之间的时间不匹配问题。利用CLEAR评估了跨四种不同架构类型的15个模型。

Result: CLEAR方法能将组件级能耗方差控制在9.5%以下，并捕获超过90%的模型总能耗。经验分析表明，Attention模块每浮点运算（FLOP）消耗的能量显著更高，这表明能耗与FLOP计数并非成比例关系，FLOPs无法单独反映组件级的真实能耗成本。

Conclusion: 研究建立了详细的组件级能耗基线，并提供了通过组件级优化构建节能型Transformer模型的初步见解和方向。

Abstract: The rapid adoption of Large Language Models (LLMs) has raised significant
environmental concerns. Unlike the one-time cost of training, LLM inference
occurs continuously at a global scale and now dominates the AI energy
footprint. Yet, most sustainability studies report only coarse, model-level
metrics due to the lack of fine-grained measurement methods, treating energy
efficiency more as an afterthought than as a primary objective. We present the
first fine-grained empirical analysis of inference energy across core
components of transformer architecture. We propose a novel methodology,
Component-Level Energy Assessment via Repeated sampling (CLEAR), to overcome
temporal mismatch between microsecond scale component execution and monitoring
of millisecond (ms) scale energy sensors. Using CLEAR, we evaluate 15 models
spanning four distinct architecture types and consistently keep component-wise
energy variance below 9.5\% while capturing more than 90\% of the model's total
energy as individual components. Our empirical analysis reveals that Attention
blocks consume significantly more energy per floating-point operation (FLOP),
indicating that energy consumption is not proportionally aligned with FLOP
counts. This shows that FLOPs alone fail to capture the true energy cost at a
component level. Our findings establish detailed component-level energy
baselines and provide insight as an initial step to build energy-efficient
transformer models through component-level optimizations.

</details>


### [201] [Mitigating Spurious Correlation via Distributionally Robust Learning with Hierarchical Ambiguity Sets](https://arxiv.org/abs/2510.02818)
*Sung Ho Jo,Seonghwi Kim,Minwoo Chae*

Main category: cs.LG

TL;DR: 本文提出一种分层Group DRO方法，旨在解决现有方法在少数群体内部分布偏移下的脆弱性，并在新引入的真实少数群体分布偏移基准以及标准基准上均表现出卓越的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统的监督学习方法易受虚假相关性影响，尤其在测试数据分布偏移时。现有方法（如Group DRO）虽对组间偏移鲁棒，但对样本有限的少数群体内部的分布偏移仍显脆弱。

Method: 提出Group DRO的分层扩展，以同时处理组间和组内不确定性，从而在多个层面提供对分布偏移的鲁棒性。同时，引入了新的基准设置，用于模拟真实的少数群体分布偏移。

Result: 该方法在模拟少数群体分布偏移的新基准条件下展现出强大的鲁棒性，而现有鲁棒学习方法在此条件下普遍失效。此外，在标准基准上，该方法也取得了优越的性能。

Conclusion: 研究结果强调了拓宽模糊集以更好地捕获组间和组内分布不确定性的重要性。

Abstract: Conventional supervised learning methods are often vulnerable to spurious
correlations, particularly under distribution shifts in test data. To address
this issue, several approaches, most notably Group DRO, have been developed.
While these methods are highly robust to subpopulation or group shifts, they
remain vulnerable to intra-group distributional shifts, which frequently occur
in minority groups with limited samples. We propose a hierarchical extension of
Group DRO that addresses both inter-group and intra-group uncertainties,
providing robustness to distribution shifts at multiple levels. We also
introduce new benchmark settings that simulate realistic minority group
distribution shifts-an important yet previously underexplored challenge in
spurious correlation research. Our method demonstrates strong robustness under
these conditions-where existing robust learning methods consistently fail-while
also achieving superior performance on standard benchmarks. These results
highlight the importance of broadening the ambiguity set to better capture both
inter-group and intra-group distributional uncertainties.

</details>


### [202] [Online Learning in the Random Order Model](https://arxiv.org/abs/2510.02820)
*Martino Bernasconi,Andrea Celli,Riccardo Colini-Baldeschi,Federico Fusco,Stefano Leonardi,Matteo Russo*

Main category: cs.LG

TL;DR: 本文提出一种通用模板，使在线学习中的随机算法适应随机顺序模型，解决了其非平稳性问题，并在延迟预测、带约束学习和带切换成本的多臂赌博机等任务中获得了改进的遗憾界。此外，发现随机顺序下在线分类的可学习性由VC维而非Littlestone维决定。


<details>
  <summary>Details</summary>
Motivation: 在线学习的随机顺序模型中，输入序列虽渐近等同于i.i.d.，但在有限时间内表现出显著的非平稳性，这会损害随机学习算法的性能，导致其在某些情况下失效。

Method: 提出一个通用的模板，用于将随机学习算法有效地修改或适应到随机顺序模型中，同时不显著影响其遗憾保证。

Result: 1. 成功为延迟预测、带约束的在线学习和带切换成本的多臂赌博机等问题恢复并获得了改进的遗憾界。2. 证明在随机顺序的在线分类中，可学习性由VC维而非Littlestone维表征，从而进一步区分了与一般对抗性模型。

Conclusion: 本文提出的通用模板成功地使随机学习算法适应随机顺序模型，克服了其非平稳性挑战，并在多个领域实现了性能提升。同时，通过明确随机顺序下在线分类的可学习性标准，深入揭示了该模型的独特理论性质。

Abstract: In the random-order model for online learning,
  the sequence of losses is chosen upfront by an adversary and presented to the
learner
  after a random permutation. Any random-order input is \emph{asymptotically}
equivalent to a stochastic i.i.d. one, but, for finite times, it may exhibit
significant {\em non-stationarity}, which can hinder the performance of
stochastic learning algorithms.
  While algorithms for adversarial inputs naturally maintain their regret
guarantees in random order, simple no-regret algorithms exist for the
stochastic model that fail against random-order instances.
  In this paper, we propose a general template to adapt stochastic learning
algorithms to the random-order model without substantially affecting their
regret guarantees. This allows us to recover improved regret bounds for
prediction with delays, online learning with constraints, and bandits with
switching costs. Finally, we investigate online classification and prove that,
in random order, learnability is characterized by the VC dimension rather than
the Littlestone dimension, thus providing a further separation from the general
adversarial model.

</details>


### [203] [FlexiQ: Adaptive Mixed-Precision Quantization for Latency/Accuracy Trade-Offs in Deep Neural Networks](https://arxiv.org/abs/2510.02822)
*Jaemin Kim,Hongjun Um,Sungkyun Kim,Yongjun Park,Jiwon Seo*

Main category: cs.LG

TL;DR: 提出FlexiQ，一种自适应混合精度量化方案，通过选择性低位宽计算和实时调整，在保证精度的同时，高效管理计算机视觉模型的实时推理工作负载波动。


<details>
  <summary>Details</summary>
Motivation: 现有硬件加速器（NPU/GPU）成本高昂，且难以灵活扩展资源以应对神经网络实时推理工作负载的波动。

Method: 本文提出FlexiQ，一种用于计算机视觉模型的自适应混合精度量化方案。它选择性地对小值范围的特征通道应用低位宽计算，并采用高效的位降低方法来最小化量化误差并保持推理精度。FlexiQ还能实时调整低位宽通道比例，以有效管理波动的推理工作负载。该方案原型已在自定义NPU和GPU上实现。

Result: 在11个卷积和Transformer视觉模型上评估，FlexiQ使微调后的4位模型平均精度提高6.6%，并优于四种SOTA量化技术。混合精度模型实现了高效的精度-延迟权衡，例如50% 4位模型仅带来0.6%精度损失，却达到了100% 4位模型相对于8位模型40%的加速效果。延迟评估证实FlexiQ引入的运行时开销极小。

Conclusion: FlexiQ在硬件效率和整体性能方面具有显著优势，能够以最小运行时开销，在保证高精度的同时，有效应对实时推理工作负载波动。

Abstract: Neural networks commonly execute on hardware accelerators such as NPUs and
GPUs for their size and computation overhead. These accelerators are costly and
it is hard to scale their resources to handle real-time workload fluctuations.
  We present FlexiQ, an adaptive mixed-precision quantization scheme for
computer vision models. FlexiQ selectively applies low-bitwidth computation to
feature channels with small value ranges and employs an efficient bit-lowering
method to minimize quantization errors while maintaining inference accuracy.
Furthermore, FlexiQ adjusts its low-bitwidth channel ratio in real time,
enabling quantized models to effectively manage fluctuating inference workload.
  We implemented FlexiQ prototype, including the mixed-precision inference
runtime on our custom NPU and GPUs. Evaluated on eleven convolution- and
transformer-based vision models, FlexiQ achieves on average 6.6% higher
accuracy for 4-bit models with finetuning and outperforms four state-of-the-art
quantization techniques. Moreover, our mixed-precision models achieved an
efficient accuracy-latency trade-off, with the 50% 4-bit model incurring only
0.6% accuracy loss while achieving 40% of the speedup of the 100% 4-bit model
over 8-bit model. Latency evaluations on our NPU and GPUs confirmed that FlexiQ
introduces minimal runtime overhead, demonstrating its hardware efficiency and
overall performance benefits.

</details>


### [204] [The Curious Case of In-Training Compression of State Space Models](https://arxiv.org/abs/2510.02823)
*Makram Chahine,Philipp Nazari,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 本文提出在SSM训练过程中，利用控制理论中的Hankel奇异值分析动态缩减状态维度，以在保持模型表达能力和性能的同时，提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSMs）在处理长序列任务时，其隐藏状态维度与计算成本密切相关。如何在最大化模型表达能力与限制计算负担之间取得平衡是一个关键的设计挑战。

Method: 引入控制理论，特别是Hankel奇异值分析，来衡量每个状态的“能量”并进行平衡截断。利用Hankel矩阵的特征值稳定性，在SSM训练期间识别并只保留高影响力的状态维度，实现动态尺寸缩减。此方法适用于线性时不变SSMs（如线性循环单元），并可扩展到选择性模型。

Result: 实验表明，训练中的维度缩减显著加速了优化过程，同时保留了模型的表达能力。与直接训练的小维度模型相比，经过压缩的模型能保留对任务至关重要的结构。这使得SSMs在从大尺寸开始并在训练中缩小时，能同时实现计算效率和更高的性能。

Conclusion: 从较大维度开始并在训练过程中进行缩减的SSMs，能够有效地提高计算效率，同时保持卓越的性能和表达能力。

Abstract: State Space Models (SSMs), developed to tackle long sequence modeling tasks
efficiently, offer both parallelizable training and fast inference. At their
core are recurrent dynamical systems that maintain a hidden state, with update
costs scaling with the state dimension. A key design challenge is striking the
right balance between maximizing expressivity and limiting this computational
burden. Control theory, and more specifically Hankel singular value analysis,
provides a potent framework for the measure of energy for each state, as well
as the balanced truncation of the original system down to a smaller
representation with performance guarantees. Leveraging the eigenvalue stability
properties of Hankel matrices, we apply this lens to SSMs during training,
where only dimensions of high influence are identified and preserved. Our
approach applies to Linear Time-Invariant SSMs such as Linear Recurrent Units,
but is also extendable to selective models. Experiments show that in-training
reduction significantly accelerates optimization while preserving expressivity,
with compressed models retaining task-critical structure lost by models trained
directly at smaller dimension. In other words, SSMs that begin large and shrink
during training achieve computational efficiency while maintaining higher
performance.

</details>


### [205] [Multi-scale Autoregressive Models are Laplacian, Discrete, and Latent Diffusion Models in Disguise](https://arxiv.org/abs/2510.02826)
*Steve Hong,Samuel Belkadi*

Main category: cs.LG

TL;DR: 本文通过迭代细化框架重新审视视觉自回归（VAR）模型，将其形式化为前向潜在金字塔构建和后向分步重建过程，解释了VAR的效率和保真度，将其与去噪扩散模型关联，并展示了其在其他领域的扩展应用。


<details>
  <summary>Details</summary>
Motivation: 旨在超越VAR模型传统上仅被视为“下一尺度自回归”的视角，通过提出迭代细化框架，形式化其工作机制，从而解释其高效率和高保真度的原因，并探索其在其他领域（如图生成、天气预报）的应用潜力。

Method: 将VAR形式化为一个确定性前向过程来构建拉普拉斯风格的潜在金字塔，并配以一个学习到的后向过程，通过少量从粗到细的步骤进行重建。这种方法将VAR与去噪扩散模型联系起来，并分离出三个关键设计选择：在学习的潜在空间中细化、将预测视为代码索引的离散分类、以及按空间频率划分任务。通过对照实验量化了每个因素对保真度和速度的贡献。

Result: 该框架成功解释了VAR模型的效率和保真度，并量化了关键设计因素的贡献。此外，该框架能够扩展应用于排列不变图生成和概率性集合式中期天气预报。它还为VAR与扩散生态系统工具的结合提供了实用接口，同时保留了其少量步骤和尺度并行生成的优势。

Conclusion: 迭代细化框架为理解和应用VAR模型提供了新的视角，不仅解释了其效率和保真度等核心优势，还促进了其与扩散模型的结合，并展示了其在多个领域的泛化能力，同时保持了高效生成特性。

Abstract: We revisit Visual Autoregressive (VAR) models through the lens of an
iterative-refinement framework. Rather than viewing VAR solely as next-scale
autoregression, we formalise it as a deterministic forward process that
constructs a Laplacian-style latent pyramid, paired with a learned backward
process that reconstructs it in a small number of coarse-to-fine steps. This
view connects VAR to denoising diffusion and isolates three design choices that
help explain its efficiency and fidelity: refining in a learned latent space,
casting prediction as discrete classification over code indices, and
partitioning the task by spatial frequency. We run controlled experiments to
quantify each factor's contribution to fidelity and speed, and we outline how
the same framework extends to permutation-invariant graph generation and to
probabilistic, ensemble-style medium-range weather forecasting. The framework
also suggests practical interfaces for VAR to leverage tools from the diffusion
ecosystem while retaining few-step, scale-parallel generation.

</details>


### [206] [Subject-Adaptive Sparse Linear Models for Interpretable Personalized Health Prediction from Multimodal Lifelog Data](https://arxiv.org/abs/2510.02835)
*Dohyun Bu,Jisoo Han,Soohwa Kwon,Yulim So,Jong-Seok Lee*

Main category: cs.LG

TL;DR: 提出可解释的Subject-Adaptive Sparse Linear (SASL) 框架，结合LightGBM，利用多模态生活日志数据实现个性化健康结果（如睡眠质量、压力）的预测，兼顾预测性能与模型透明度。


<details>
  <summary>Details</summary>
Motivation: 现有预测个性化健康结果的深度学习和梯度提升模型牺牲了可解释性，且未能充分处理生活日志数据中显著的个体间差异。提升个性化健康预测能力及其可解释性具有重要的临床和实践意义。

Method: 提出了主题自适应稀疏线性（SASL）框架，该框架结合普通最小二乘回归与受试者特异性交互，区分全局和个体效应。采用基于嵌套F检验的迭代反向特征消除方法构建稀疏且稳健的模型。针对有序目标，开发了“先回归后阈值化”方法。对复杂预测，通过基于置信度的门控机制选择性融合紧凑型LightGBM模型输出，形成混合SASL-LightGBM框架。

Result: 在CH-2025数据集（10名受试者约450个每日观察值）上的评估表明，混合SASL-LightGBM框架达到了与复杂黑盒方法相当的预测性能，但模型参数显著减少，透明度大幅提高。

Conclusion: SASL框架及其混合模型在实现个性化健康预测方面，成功平衡了预测精度与模型可解释性，为临床医生和从业者提供了清晰且可操作的洞察。

Abstract: Improved prediction of personalized health outcomes -- such as sleep quality
and stress -- from multimodal lifelog data could have meaningful clinical and
practical implications. However, state-of-the-art models, primarily deep neural
networks and gradient-boosted ensembles, sacrifice interpretability and fail to
adequately address the significant inter-individual variability inherent in
lifelog data. To overcome these challenges, we propose the Subject-Adaptive
Sparse Linear (SASL) framework, an interpretable modeling approach explicitly
designed for personalized health prediction. SASL integrates ordinary least
squares regression with subject-specific interactions, systematically
distinguishing global from individual-level effects. We employ an iterative
backward feature elimination method based on nested $F$-tests to construct a
sparse and statistically robust model. Additionally, recognizing that health
outcomes often represent discretized versions of continuous processes, we
develop a regression-then-thresholding approach specifically designed to
maximize macro-averaged F1 scores for ordinal targets. For intrinsically
challenging predictions, SASL selectively incorporates outputs from compact
LightGBM models through confidence-based gating, enhancing accuracy without
compromising interpretability. Evaluations conducted on the CH-2025 dataset --
which comprises roughly 450 daily observations from ten subjects -- demonstrate
that the hybrid SASL-LightGBM framework achieves predictive performance
comparable to that of sophisticated black-box methods, but with significantly
fewer parameters and substantially greater transparency, thus providing clear
and actionable insights for clinicians and practitioners.

</details>


### [207] [Knowledge-Aware Modeling with Frequency Adaptive Learning for Battery Health Prognostics](https://arxiv.org/abs/2510.02839)
*Vijay Babu Pamshetti,Wei Zhang,Sumei Sun,Jie Zhang,Yonggang Wen,Qingyu Yan*

Main category: cs.LG

TL;DR: Karma是一个知识引导的频率自适应模型，通过信号分解、双流深度学习和粒子滤波优化领域知识参数，实现了准确可靠的电池容量估计和剩余寿命预测，并在主流数据集上显著优于现有SOTA算法。


<details>
  <summary>Details</summary>
Motivation: 电池健康预测对于能源系统至关重要，但由于电池退化行为复杂且现有数据驱动模型缺乏领域知识指导，导致长期预测不可靠。

Method: 本文提出Karma模型，首先对电池信号进行频率分解；然后采用双流深度学习架构，分别捕捉低频长期退化趋势和高频短期动态；将基于经验研究的双指数函数作为电池退化的领域知识，并利用粒子滤波优化知识参数，以确保预测的物理一致性和不确定性量化。

Result: 实验结果表明，Karma模型在两个主流数据集上的电池健康预测性能优于现有SOTA算法，平均误差分别降低了50.6%和32.6%。

Conclusion: Karma模型展现出卓越的鲁棒性、泛化能力，为更安全、可靠的电池管理提供了潜力。

Abstract: Battery health prognostics are critical for ensuring safety, efficiency, and
sustainability in modern energy systems. However, it has been challenging to
achieve accurate and robust prognostics due to complex battery degradation
behaviors with nonlinearity, noise, capacity regeneration, etc. Existing
data-driven models capture temporal degradation features but often lack
knowledge guidance, which leads to unreliable long-term health prognostics. To
overcome these limitations, we propose Karma, a knowledge-aware model with
frequency-adaptive learning for battery capacity estimation and remaining
useful life prediction. The model first performs signal decomposition to derive
battery signals in different frequency bands. A dual-stream deep learning
architecture is developed, where one stream captures long-term low-frequency
degradation trends and the other models high-frequency short-term dynamics.
Karma regulates the prognostics with knowledge, where battery degradation is
modeled as a double exponential function based on empirical studies. Our
dual-stream model is used to optimize the parameters of the knowledge with
particle filters to ensure physically consistent and reliable prognostics and
uncertainty quantification. Experimental study demonstrates Karma's superior
performance, achieving average error reductions of 50.6% and 32.6% over
state-of-the-art algorithms for battery health prediction on two mainstream
datasets, respectively. These results highlight Karma's robustness,
generalizability, and potential for safer and more reliable battery management
across diverse applications.

</details>


### [208] [RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative Reinforcement Learning](https://arxiv.org/abs/2510.02892)
*Aleksei Arzhantsev,Otmane Sakhi,Flavian Vasile*

Main category: cs.LG

TL;DR: 本文提出RoiRL（离线迭代强化学习推理），一种轻量级的离线学习方法，旨在解决TTRL计算成本高的问题，通过优化加权对数似然目标，实现LLM推理能力的自我提升，且训练速度更快，性能优于TTRL。


<details>
  <summary>Details</summary>
Motivation: 强化学习对提升大型语言模型（LLMs）的推理能力至关重要，但通常需要真实奖励。现有方法Test-Time Reinforcement Learning (TTRL) 虽然通过多数投票奖励消除了真实奖励的需求，但其依赖于繁重的在线强化学习，导致计算成本高昂。

Method: 提出RoiRL：一种轻量级的离线迭代强化学习替代方案。与TTRL不同，RoiRL无需维护参考模型，而是优化加权对数似然目标，从而实现更稳定的训练，并显著降低内存和计算需求。

Result: 实验结果显示，RoiRL的训练速度比TTRL快2.5倍，并在推理基准测试中持续超越TTRL。

Conclusion: RoiRL为实现无需标签的LLMs自我改进提供了一条可扩展的路径。

Abstract: Reinforcement learning (RL) is central to improving reasoning in large
language models (LLMs) but typically requires ground-truth rewards. Test-Time
Reinforcement Learning (TTRL) removes this need by using majority-vote rewards,
but relies on heavy online RL and incurs substantial computational cost. We
propose RoiRL: Reasoning with offline iterative Reinforcement Learning, a
family of lightweight offline learning alternatives that can target the same
regularized optimal policies. Unlike TTRL, RoiRL eliminates the need to
maintain a reference model and instead optimizes weighted log-likelihood
objectives, enabling stable training with significantly lower memory and
compute requirements. Experimental results show that RoiRL trains to 2.5x
faster and consistently outperforms TTRL on reasoning benchmarks, establishing
a scalable path to self-improving LLMs without labels.

</details>


### [209] [DMark: Order-Agnostic Watermarking for Diffusion Large Language Models](https://arxiv.org/abs/2510.02902)
*Linyu Wu,Linhao Zhong,Wenjie Qu,Yuexin Li,Yue Liu,Shengfang Zhai,Chunhua Shen,Jiaheng Zhang*

Main category: cs.LG

TL;DR: 现有水印方法不适用于扩散大语言模型(dLLMs)，本文提出DMark框架，通过预测和双向策略，实现了高检测率和鲁棒性，证明了非自回归模型水印的可行性。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)生成速度快且质量高，但现有水印方法因其非顺序解码机制而失效。dLLMs可以任意顺序生成token，破坏了传统水印的因果设计，急需一种新的水印框架。

Method: 提出了DMark，首个专为dLLMs设计的水印框架。它包含三种策略：1) 预测水印，在缺乏实际上下文时使用模型预测的token；2) 双向水印，利用扩散解码的前向和后向依赖；3) 预测-双向水印，结合前两者以最大化检测强度。

Result: DMark在多个dLLMs上实现了92.0-99.5%的检测率（1%误报率），同时保持了文本质量，远优于现有方法（49.6-71.2%）。DMark还表现出对文本篡改的鲁棒性。

Conclusion: DMark证明了为非自回归语言模型实现有效水印的可行性，是首个专为dLLMs设计的水印框架，为未来的研究奠定了基础。

Abstract: Diffusion large language models (dLLMs) offer faster generation than
autoregressive models while maintaining comparable quality, but existing
watermarking methods fail on them due to their non-sequential decoding. Unlike
autoregressive models that generate tokens left-to-right, dLLMs can finalize
tokens in arbitrary order, breaking the causal design underlying traditional
watermarks. We present DMark, the first watermarking framework designed
specifically for dLLMs. DMark introduces three complementary strategies to
restore watermark detectability: predictive watermarking uses model-predicted
tokens when actual context is unavailable; bidirectional watermarking exploits
both forward and backward dependencies unique to diffusion decoding; and
predictive-bidirectional watermarking combines both approaches to maximize
detection strength. Experiments across multiple dLLMs show that DMark achieves
92.0-99.5% detection rates at 1% false positive rate while maintaining text
quality, compared to only 49.6-71.2% for naive adaptations of existing methods.
DMark also demonstrates robustness against text manipulations, establishing
that effective watermarking is feasible for non-autoregressive language models.

</details>


### [210] [Learning Explicit Single-Cell Dynamics Using ODE Representations](https://arxiv.org/abs/2510.02903)
*Jan-Philipp von Bassewitz,Adeel Pervez,Marco Fumero,Matthew Robinson,Theofanis Karaletsos,Francesco Locatello*

Main category: cs.LG

TL;DR: 提出Cell-MNN，一个基于编码器-解码器架构的端到端神经网络，其潜在空间通过局部线性化的常微分方程（ODE）建模细胞分化动力学，克服了现有方法的计算昂贵和缺乏基因互作发现的缺点，并在单细胞数据上实现卓越性能和可解释的基因互作学习。


<details>
  <summary>Details</summary>
Motivation: 细胞分化动力学建模对理解和治疗癌症等相关疾病至关重要。然而，现有最先进模型计算成本高昂，依赖多阶段训练，并且无法显式发现基因互作。

Method: 提出Cell-Mechanistic Neural Networks (Cell-MNN)，一个编码器-解码器架构，其潜在表示是一个局部线性化的常微分方程（ODE），用于描述细胞从干细胞到组织细胞的演化动力学。Cell-MNN除标准PCA预处理外，是完全端到端的，并能学习生物学上一致且可解释的基因互作。

Result: Cell-MNN在单细胞基准测试中取得了有竞争力的性能，在扩展到大型数据集和多数据集联合训练方面超越了现有最先进的基线。同时，它学习到了可解释的基因互作，并与TRRUST基因互作数据库进行了验证。

Conclusion: Cell-MNN提供了一个计算高效、端到端的新框架，用于建模细胞分化动力学，并能发现可解释的基因互作，成功解决了现有方法的局限性，为相关疾病的理解和治疗提供了新工具。

Abstract: Modeling the dynamics of cellular differentiation is fundamental to advancing
the understanding and treatment of diseases associated with this process, such
as cancer. With the rapid growth of single-cell datasets, this has also become
a particularly promising and active domain for machine learning. Current
state-of-the-art models, however, rely on computationally expensive optimal
transport preprocessing and multi-stage training, while also not discovering
explicit gene interactions. To address these challenges we propose
Cell-Mechanistic Neural Networks (Cell-MNN), an encoder-decoder architecture
whose latent representation is a locally linearized ODE governing the dynamics
of cellular evolution from stem to tissue cells. Cell-MNN is fully end-to-end
(besides a standard PCA pre-processing) and its ODE representation explicitly
learns biologically consistent and interpretable gene interactions.
Empirically, we show that Cell-MNN achieves competitive performance on
single-cell benchmarks, surpasses state-of-the-art baselines in scaling to
larger datasets and joint training across multiple datasets, while also
learning interpretable gene interactions that we validate against the TRRUST
database of gene interactions.

</details>


### [211] [FeDABoost: Fairness Aware Federated Learning with Adaptive Boosting](https://arxiv.org/abs/2510.02914)
*Tharuka Kasthuri Arachchige,Veselka Boeva,Shahrooz Abghari*

Main category: cs.LG

TL;DR: 该工作提出了FeDABoost框架，通过动态提升机制和自适应梯度聚合策略，改进了非独立同分布（non-IID）联邦学习的性能和公平性。


<details>
  <summary>Details</summary>
Motivation: 在非独立同分布（non-IID）设置下，联邦学习的性能和公平性面临挑战，尤其是在处理表现不佳的客户端时。

Method: 引入FeDABoost框架，结合了动态提升机制和自适应梯度聚合策略。聚合方法受Multiclass AdaBoost启发，根据客户端本地错误率分配权重；同时，通过调整focal loss参数，动态提升表现不佳客户端的本地训练。

Result: 在MNIST、FEMNIST和CIFAR10三个基准数据集上进行评估，结果显示FeDABoost在公平性方面有所改善，并展现出具有竞争力的性能，优于FedAvg和Ditto。

Conclusion: FeDABoost通过其独特的聚合和提升机制，有效提高了非独立同分布环境下联邦学习的公平性和整体性能。

Abstract: This work focuses on improving the performance and fairness of Federated
Learning (FL) in non IID settings by enhancing model aggregation and boosting
the training of underperforming clients. We propose FeDABoost, a novel FL
framework that integrates a dynamic boosting mechanism and an adaptive gradient
aggregation strategy. Inspired by the weighting mechanism of the Multiclass
AdaBoost (SAMME) algorithm, our aggregation method assigns higher weights to
clients with lower local error rates, thereby promoting more reliable
contributions to the global model. In parallel, FeDABoost dynamically boosts
underperforming clients by adjusting the focal loss focusing parameter,
emphasizing hard to classify examples during local training. We have evaluated
FeDABoost on three benchmark datasets MNIST, FEMNIST, and CIFAR10, and compared
its performance with those of FedAvg and Ditto. The results show that FeDABoost
achieves improved fairness and competitive performance.

</details>


### [212] [RAxSS: Retrieval-Augmented Sparse Sampling for Explainable Variable-Length Medical Time Series Classification](https://arxiv.org/abs/2510.02936)
*Aydin Javadov,Samir Garibov,Tobias Hoesli,Qiyang Sun,Florian von Wangenheim,Joseph Ollier,Björn W. Schuller*

Main category: cs.LG

TL;DR: 本研究提出了一种结合随机稀疏采样和检索增强的医疗时间序列分类方法，通过加权预测和概率聚合提升了iEEG分类的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 医疗时间序列分析面临数据稀疏、噪声大及记录长度多变等挑战。现有随机稀疏采样能有效处理变长信号，而检索增强方法可提高可解释性和鲁棒性。本研究旨在结合两者的优势，开发一种可解释的变长医疗时间序列分类框架。

Method: 本研究推广了随机稀疏采样框架，用于检索增强分类。具体做法是：通过通道内相似性对窗口预测进行加权，并在概率空间中聚合这些预测，从而得到凸序列级分数并提供明确的证据链，以增强可解释性。

Result: 该方法在iEEG分类中取得了有竞争力的性能，并为临床医生提供了更高的透明度和可解释性。在四个医疗中心收集的iEEG记录上进行的评估，证明了其在可靠且可解释的临床变长时间序列分类方面的潜力。

Conclusion: 所提出的方法为医疗变长时间序列分类提供了一个可靠且具有高可解释性的解决方案，特别适用于需要透明度高的临床应用。

Abstract: Medical time series analysis is challenging due to data sparsity, noise, and
highly variable recording lengths. Prior work has shown that stochastic sparse
sampling effectively handles variable-length signals, while retrieval-augmented
approaches improve explainability and robustness to noise and weak temporal
correlations. In this study, we generalize the stochastic sparse sampling
framework for retrieval-informed classification. Specifically, we weight window
predictions by within-channel similarity and aggregate them in probability
space, yielding convex series-level scores and an explicit evidence trail for
explainability. Our method achieves competitive iEEG classification performance
and provides practitioners with greater transparency and explainability. We
evaluate our method in iEEG recordings collected in four medical centers,
demonstrating its potential for reliable and explainable clinical
variable-length time series classification.

</details>


### [213] [Ergodic Risk Measures: Towards a Risk-Aware Foundation for Continual Reinforcement Learning](https://arxiv.org/abs/2510.02945)
*Juan Sebastian Rojas,Chi-Guhn Lee*

Main category: cs.LG

TL;DR: 本文首次为风险感知持续强化学习提供了形式化理论框架，并引入“遍历风险度量”以克服经典风险度量在持续设置中的不兼容性，同时通过实证验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有持续强化学习（Continual RL）主要局限于风险中性决策，即仅优化预期长期性能。本文旨在填补这一空白，首次从风险感知角度对持续RL进行形式化和理论处理，以优化超越平均值的长期性能，从而开发出更具鲁棒性和适应性的终身学习智能体。

Method: 本文首先对风险感知决策下的持续强化学习进行了首次正式的理论处理。研究发现，经典的风险度量理论与持续学习设置不兼容。在此洞察基础上，通过引入一类新的“遍历风险度量”（ergodic risk measures），将风险度量理论扩展到持续学习环境。最后，通过一个风险感知持续学习的案例研究和经验结果来验证所提出方法的直观吸引力和理论健全性。

Result: 1. 揭示了经典风险度量理论在其现有形式下与持续学习设置不兼容。2. 成功提出并理论化了一类与持续学习兼容的“遍历风险度量”。3. 通过案例研究和经验结果，证明了遍历风险度量的直观吸引力和理论健全性。

Conclusion: 本文首次为风险感知持续强化学习奠定了正式的理论基础，并通过引入新的“遍历风险度量”，克服了经典风险度量在持续学习中的局限性。这些新度量为构建能够平衡信息保留与新环境适应、并在风险感知下优化性能的持续RL智能体提供了理论和实证支持。

Abstract: Continual reinforcement learning (continual RL) seeks to formalize the
notions of lifelong learning and endless adaptation in RL. In particular, the
aim of continual RL is to develop RL agents that can maintain a careful balance
between retaining useful information and adapting to new situations. To date,
continual RL has been explored almost exclusively through the lens of
risk-neutral decision-making, in which the agent aims to optimize the expected
(or mean) long-run performance. In this work, we present the first formal
theoretical treatment of continual RL through the lens of risk-aware
decision-making, in which the agent aims to optimize a reward-based measure of
long-run performance beyond the mean. In particular, we show that the classical
theory of risk measures, widely used as a theoretical foundation in
non-continual risk-aware RL, is, in its current form, incompatible with the
continual setting. Then, building on this insight, we extend risk measure
theory into the continual setting by introducing a new class of ergodic risk
measures that are compatible with continual learning. Finally, we provide a
case study of risk-aware continual learning, along with empirical results,
which show the intuitive appeal and theoretical soundness of ergodic risk
measures.

</details>


### [214] [ContextFlow: Context-Aware Flow Matching For Trajectory Inference From Spatial Omics Data](https://arxiv.org/abs/2510.02952)
*Santanu Subhash Rathod,Francesco Ceccarelli,Sean B. Holden,Pietro Liò,Xiao Zhang,Jovan Tanevski*

Main category: cs.LG

TL;DR: ContextFlow是一种新颖的上下文感知流匹配框架，通过整合先验生物学知识，从纵向空间分辨组学数据中推断组织动态轨迹，提高了轨迹的生物学意义和准确性。


<details>
  <summary>Details</summary>
Motivation: 从纵向空间分辨组学数据中推断轨迹对于理解发育、再生、修复、疾病进展和治疗反应中的结构和功能组织动态变化至关重要。

Method: ContextFlow框架通过将局部组织结构和配体-受体通讯模式整合到过渡可能性矩阵中，以此来正则化最优传输目标。通过嵌入这些上下文约束，该方法能够指导结构组织动态的推断。

Result: ContextFlow生成的轨迹不仅统计上一致，而且具有生物学意义。在三个数据集上评估显示，ContextFlow在推断准确性和生物学一致性的多个量化和质化指标上，持续优于现有最先进的流匹配方法。

Conclusion: ContextFlow是一个通用框架，可用于从纵向、空间分辨组学数据中建模时空动态，能够提供统计一致且具有生物学意义的轨迹。

Abstract: Inferring trajectories from longitudinal spatially-resolved omics data is
fundamental to understanding the dynamics of structural and functional tissue
changes in development, regeneration and repair, disease progression, and
response to treatment. We propose ContextFlow, a novel context-aware flow
matching framework that incorporates prior knowledge to guide the inference of
structural tissue dynamics from spatially resolved omics data. Specifically,
ContextFlow integrates local tissue organization and ligand-receptor
communication patterns into a transition plausibility matrix that regularizes
the optimal transport objective. By embedding these contextual constraints,
ContextFlow generates trajectories that are not only statistically consistent
but also biologically meaningful, making it a generalizable framework for
modeling spatiotemporal dynamics from longitudinal, spatially resolved omics
data. Evaluated on three datasets, ContextFlow consistently outperforms
state-of-the-art flow matching methods across multiple quantitative and
qualitative metrics of inference accuracy and biological coherence. Our code is
available at: \href{https://github.com/santanurathod/ContextFlow}{ContextFlow}

</details>


### [215] [Confidence and Dispersity as Signals: Unsupervised Model Evaluation and Ranking](https://arxiv.org/abs/2510.02956)
*Weijian Deng,Weijie Tu,Ibrahim Radwan,Mohammad Abu Alsheikh,Stephen Gould,Liang Zheng*

Main category: cs.LG

TL;DR: 论文提出了一个统一的无监督模型评估和排名框架，通过结合预测置信度和分散度，有效评估模型在无标签测试数据和分布偏移下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在实际部署中，尤其是在无标签测试数据的情况下，评估模型在分布偏移下的泛化能力至关重要。

Method: 本文提出了一个统一的无监督模型评估和排名框架。该框架利用模型预测的两个内在属性：置信度（反映预测确定性）和分散度（反映预测类别的多样性），并系统性地基准测试了基于置信度、分散度和混合指标。

Result: 研究表明，混合指标在数据集中心和模型中心两种评估设置中均显著优于单一指标。特别是，预测矩阵的核范数在各种任务（包括真实世界数据集）中表现出鲁棒和准确的性能，并在中等类别不平衡下保持可靠性。

Conclusion: 这些发现为部署场景中的无监督模型评估提供了一个实用且通用的基础。

Abstract: Assessing model generalization under distribution shift is essential for
real-world deployment, particularly when labeled test data is unavailable. This
paper presents a unified and practical framework for unsupervised model
evaluation and ranking in two common deployment settings: (1) estimating the
accuracy of a fixed model on multiple unlabeled test sets (dataset-centric
evaluation), and (2) ranking a set of candidate models on a single unlabeled
test set (model-centric evaluation). We demonstrate that two intrinsic
properties of model predictions, namely confidence (which reflects prediction
certainty) and dispersity (which captures the diversity of predicted classes),
together provide strong and complementary signals for generalization. We
systematically benchmark a set of confidence-based, dispersity-based, and
hybrid metrics across a wide range of model architectures, datasets, and
distribution shift types. Our results show that hybrid metrics consistently
outperform single-aspect metrics on both dataset-centric and model-centric
evaluation settings. In particular, the nuclear norm of the prediction matrix
provides robust and accurate performance across tasks, including real-world
datasets, and maintains reliability under moderate class imbalance. These
findings offer a practical and generalizable basis for unsupervised model
assessment in deployment scenarios.

</details>


### [216] [From high-frequency sensors to noon reports: Using transfer learning for shaft power prediction in maritime](https://arxiv.org/abs/2510.03003)
*Akriti Sharma,Dogan Altan,Dusica Marijan,Arnbjørn Maressa*

Main category: cs.LG

TL;DR: 本文提出一种基于迁移学习的方法，通过结合一艘船的高频数据和多艘船的低频正午报告，显著提高了船舶轴功率的预测精度，尤其对姊妹船效果最佳。


<details>
  <summary>Details</summary>
Motivation: 随着全球海运增长，能源优化对降低成本和提高运营效率至关重要。轴功率直接影响燃料消耗，准确预测是优化船舶性能的关键。虽然高频传感器数据能提高预测精度，但其获取成本高且不可行，因此需寻找替代方案如正午报告来解决数据获取的挑战。

Method: 本文提出一种基于迁移学习的方法。首先，在单一船舶的高频数据上训练一个初始模型；然后，使用其他船舶的低频每日正午报告对该模型进行微调。该方法在姊妹船、相似船和不同船舶上进行了测试。

Result: 与仅使用正午报告数据训练的模型相比，迁移学习方法使平均绝对百分比误差（MAPE）降低了：姊妹船10.6%，相似船3.6%，不同船舶5.3%。

Conclusion: 该迁移学习方法能有效提高船舶轴功率的预测精度，通过利用一艘船的高频数据对其他船的低频数据进行微调，显著改善了预测性能，尤其在姊妹船上表现突出。

Abstract: With the growth of global maritime transportation, energy optimization has
become crucial for reducing costs and ensuring operational efficiency. Shaft
power is the mechanical power transmitted from the engine to the shaft and
directly impacts fuel consumption, making its accurate prediction a paramount
step in optimizing vessel performance. Power consumption is highly correlated
with ship parameters such as speed and shaft rotation per minute, as well as
weather and sea conditions. Frequent access to this operational data can
improve prediction accuracy. However, obtaining high-quality sensor data is
often infeasible and costly, making alternative sources such as noon reports a
viable option. In this paper, we propose a transfer learning-based approach for
predicting vessels shaft power, where a model is initially trained on
high-frequency data from a vessel and then fine-tuned with low-frequency daily
noon reports from other vessels. We tested our approach on sister vessels
(identical dimensions and configurations), a similar vessel (slightly larger
with a different engine), and a different vessel (distinct dimensions and
configurations). The experiments showed that the mean absolute percentage error
decreased by 10.6 percent for sister vessels, 3.6 percent for a similar vessel,
and 5.3 percent for a different vessel, compared to the model trained solely on
noon report data.

</details>


### [217] [BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck for Functional Brain Biomarkers in Schizophrenia](https://arxiv.org/abs/2510.03004)
*Tianzheng Hu,Qiang Li,Shu Liu,Vince D. Calhoun,Guido van Wingen,Shujian Yu*

Main category: cs.LG

TL;DR: BrainIB++是一个可解释的图神经网络框架，利用信息瓶颈原理，在精神分裂症诊断中识别关键脑区，表现出优异的诊断精度、泛化能力及临床相关性。


<details>
  <summary>Details</summary>
Motivation: 精神疾病诊断模型（基于静息态fMRI）面临挑战：传统机器学习模型依赖特征工程引入偏差，而深度学习模型缺乏可解释性，限制了可靠脑生物标志物的获取及其临床应用。

Method: 提出了一个名为BrainIB++的端到端图神经网络框架。该框架应用信息瓶颈（IB）原理，在模型训练过程中自动识别最具信息量的数据驱动脑区作为子图，以实现模型解释性。

Result: 1. 在三个多队列精神分裂症数据集上，BrainIB++模型与九种现有脑网络分类方法相比，始终展现出卓越的诊断准确性和对未知数据的泛化能力。2. 模型识别出的子图与精神分裂症中已确立的临床生物标志物相符，尤其突出了视觉、感觉运动和高级认知脑功能网络的异常。

Conclusion: BrainIB++框架通过提供高准确度、强泛化能力和良好可解释性的诊断结果，有效解决了现有模型的局限性，为精神疾病的临床诊断提供了有价值且可靠的生物标志物。

Abstract: The development of diagnostic models is gaining traction in the field of
psychiatric disorders. Recently, machine learning classifiers based on
resting-state functional magnetic resonance imaging (rs-fMRI) have been
developed to identify brain biomarkers that differentiate psychiatric disorders
from healthy controls. However, conventional machine learning-based diagnostic
models often depend on extensive feature engineering, which introduces bias
through manual intervention. While deep learning models are expected to operate
without manual involvement, their lack of interpretability poses significant
challenges in obtaining explainable and reliable brain biomarkers to support
diagnostic decisions, ultimately limiting their clinical applicability. In this
study, we introduce an end-to-end innovative graph neural network framework
named BrainIB++, which applies the information bottleneck (IB) principle to
identify the most informative data-driven brain regions as subgraphs during
model training for interpretation. We evaluate the performance of our model
against nine established brain network classification methods across three
multi-cohort schizophrenia datasets. It consistently demonstrates superior
diagnostic accuracy and exhibits generalizability to unseen data. Furthermore,
the subgraphs identified by our model also correspond with established clinical
biomarkers in schizophrenia, particularly emphasizing abnormalities in the
visual, sensorimotor, and higher cognition brain functional network. This
alignment enhances the model's interpretability and underscores its relevance
for real-world diagnostic applications.

</details>


### [218] [Distributional Inverse Reinforcement Learning](https://arxiv.org/abs/2510.03013)
*Feiyang Wu,Ye Zhao,Anqi Wu*

Main category: cs.LG

TL;DR: 提出一种分布式的离线逆强化学习框架，通过建模奖励函数不确定性和回报分布，并集成失真风险度量，以学习更丰富的专家行为，实现SOTA模仿性能。


<details>
  <summary>Details</summary>
Motivation: 传统IRL方法仅恢复确定性奖励或匹配预期回报，无法捕获专家行为中更丰富的结构（如奖励不确定性和完整的回报分布）。

Method: 提出一个分布式的离线逆强化学习（IRL）框架，共同建模奖励函数的不确定性以及完整的回报分布。通过最小化一阶随机占优（FSD）违规并整合失真风险度量（DRMs）到策略学习中，从而恢复奖励分布和风险感知策略。

Result: 在合成基准、真实世界神经行为数据和MuJoCo控制任务上的实证结果表明，该方法能够恢复富有表现力的奖励表示，并实现最先进的模仿性能。

Conclusion: 该框架成功地通过学习奖励分布和风险感知策略，捕获了专家行为中更丰富的结构，并在行为分析和风险感知模仿学习中具有潜力。

Abstract: We propose a distributional framework for offline Inverse Reinforcement
Learning (IRL) that jointly models uncertainty over reward functions and full
distributions of returns. Unlike conventional IRL approaches that recover a
deterministic reward estimate or match only expected returns, our method
captures richer structure in expert behavior, particularly in learning the
reward distribution, by minimizing first-order stochastic dominance (FSD)
violations and thus integrating distortion risk measures (DRMs) into policy
learning, enabling the recovery of both reward distributions and
distribution-aware policies. This formulation is well-suited for behavior
analysis and risk-aware imitation learning. Empirical results on synthetic
benchmarks, real-world neurobehavioral data, and MuJoCo control tasks
demonstrate that our method recovers expressive reward representations and
achieves state-of-the-art imitation performance.

</details>


### [219] [Learning Robust Diffusion Models from Imprecise Supervision](https://arxiv.org/abs/2510.03016)
*Dong-Dong Wu,Jiacheng Cui,Wei Wang,Zhiqiang She,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出DMIS框架，首次系统性研究在不精确监督下训练鲁棒扩散模型的问题。DMIS通过分解似然最大化目标为生成和分类两部分，并优化扩散分类器，实现在有噪声、模糊或不完整标签的数据集上生成高质量且类别区分度高的样本。


<details>
  <summary>Details</summary>
Motivation: 条件扩散模型虽表现出色，但其训练依赖的大规模数据集常含有不精确（噪声、模糊、不完整）的条件输入，导致条件不匹配并降低生成质量。

Method: DMIS是一个统一框架，基于似然最大化，将目标分解为生成和分类两部分：生成部分建模不精确标签分布，分类部分利用扩散分类器推断类别后验概率，并通过优化的时间步采样策略提高效率。

Result: 在图像生成、弱监督学习和噪声数据集压缩等多种不精确监督任务上，DMIS均能持续生成高质量且类别区分度强的样本。

Conclusion: DMIS成功解决了扩散模型在不精确监督下训练的挑战，提供了一个有效且鲁棒的框架，能产出高质量的生成结果。

Abstract: Conditional diffusion models have achieved remarkable success in various
generative tasks recently, but their training typically relies on large-scale
datasets that inevitably contain imprecise information in conditional inputs.
Such supervision, often stemming from noisy, ambiguous, or incomplete labels,
will cause condition mismatch and degrade generation quality. To address this
challenge, we propose DMIS, a unified framework for training robust Diffusion
Models from Imprecise Supervision, which is the first systematic study within
diffusion models. Our framework is derived from likelihood maximization and
decomposes the objective into generative and classification components: the
generative component models imprecise-label distributions, while the
classification component leverages a diffusion classifier to infer
class-posterior probabilities, with its efficiency further improved by an
optimized timestep sampling strategy. Extensive experiments on diverse forms of
imprecise supervision, covering tasks of image generation, weakly supervised
learning, and noisy dataset condensation demonstrate that DMIS consistently
produces high-quality and class-discriminative samples.

</details>


### [220] [Differentially Private Wasserstein Barycenters](https://arxiv.org/abs/2510.03021)
*Anming Gu,Sasidhar Kunapuli,Mark Bun,Edward Chien,Kristjan Greenewald*

Main category: cs.LG

TL;DR: 该研究提出了首个用于计算差分隐私Wasserstein barycenter的算法。


<details>
  <summary>Details</summary>
Motivation: Wasserstein barycenter在实践中常应用于包含敏感数据的数据集，因此需要进行差分隐私（DP）处理。

Method: 本文开发了首批用于在差分隐私下计算Wasserstein barycenter的算法。

Result: 在合成数据、MNIST和大规模美国人口数据集上的实验表明，所提出的方法能够生成高质量的隐私barycenter，并在准确性和隐私之间实现良好的权衡。

Conclusion: 该研究成功地提出了计算差分隐私Wasserstein barycenter的算法，并验证了其在保证隐私的同时保持较高准确性的能力。

Abstract: The Wasserstein barycenter is defined as the mean of a set of probability
measures under the optimal transport metric, and has numerous applications
spanning machine learning, statistics, and computer graphics. In practice these
input measures are empirical distributions built from sensitive datasets,
motivating a differentially private (DP) treatment. We present, to our
knowledge, the first algorithms for computing Wasserstein barycenters under
differential privacy. Empirically, on synthetic data, MNIST, and large-scale
U.S. population datasets, our methods produce high-quality private barycenters
with strong accuracy-privacy tradeoffs.

</details>


### [221] [Lightweight Transformer for EEG Classification via Balanced Signed Graph Algorithm Unrolling](https://arxiv.org/abs/2510.03027)
*Junyi Yao,Parham Eftekhar,Gene Cheung,Xujin Chris Liu,Yao Wang,Wei Hu*

Main category: cs.LG

TL;DR: 本文提出一种基于平衡符号图谱去噪算法的轻量级、可解释类Transformer神经网络，通过重构误差对脑电信号进行二分类，以区分癫痫患者与健康受试者。该方法在实现与主流深度学习方案相当性能的同时，大幅减少了模型参数。


<details>
  <summary>Details</summary>
Motivation: 利用脑电（EEG）信号区分癫痫患者与健康受试者。

Method: 将脑电信号中固有的反相关性建模为有限图中的负边。通过展开平衡符号图（无奇数负边环）上的谱去噪算法，构建轻量级、可解释的类Transformer神经网络。利用平衡符号图的频率特性，通过图拉普拉斯矩阵的相似变换将其映射到对应的正图。在映射后的正图上，通过Lanczos近似高效实现理想低通滤波器，其中最优截止频率从数据中学习。通过两个学习不同信号类别后验概率的平衡符号图去噪器，评估其重构误差进行脑电信号的二分类。

Result: 实验表明，该方法在分类性能上与代表性的深度学习方案相当，但所使用的参数量显著减少。

Conclusion: 所提出的基于平衡符号图谱去噪的神经网络方法能够有效且高效地进行脑电信号二分类，成功区分癫痫患者和健康受试者，且具有轻量化和可解释性优势。

Abstract: Samples of brain signals collected by EEG sensors have inherent
anti-correlations that are well modeled by negative edges in a finite graph. To
differentiate epilepsy patients from healthy subjects using collected EEG
signals, we build lightweight and interpretable transformer-like neural nets by
unrolling a spectral denoising algorithm for signals on a balanced signed graph
-- graph with no cycles of odd number of negative edges. A balanced signed
graph has well-defined frequencies that map to a corresponding positive graph
via similarity transform of the graph Laplacian matrices. We implement an ideal
low-pass filter efficiently on the mapped positive graph via Lanczos
approximation, where the optimal cutoff frequency is learned from data. Given
that two balanced signed graph denoisers learn posterior probabilities of two
different signal classes during training, we evaluate their reconstruction
errors for binary classification of EEG signals. Experiments show that our
method achieves classification performance comparable to representative deep
learning schemes, while employing dramatically fewer parameters.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [222] [Interplay between Security, Privacy and Trust in 6G-enabled Intelligent Transportation Systems](https://arxiv.org/abs/2510.02487)
*Ahmed Danladi Abdullahi,Erfan Bahrami,Tooska Dargahi,Mohammed Al-Khalidi,Mohammad Hammoudeh*

Main category: cs.NI

TL;DR: 6G-ITS有望变革交通，但面临严峻的安全和隐私挑战。本文审查了其机遇与挑战，提出了攻击模型分类，并强调需建立一个全面的多层安全框架来应对风险。


<details>
  <summary>Details</summary>
Motivation: 6G-ITS虽然有望通过高速、低延迟通信和高级数据分析彻底改变交通，但文献中已识别出多种安全和隐私挑战，这些挑战必须得到解决，以确保安全部署并赢得公众信任。

Method: 本文审查了6G-ITS的机遇与挑战，重点关注信任、安全和隐私，并特别考虑了量子技术的影响。提出了6G-ITS的不同攻击模型分类，比较了5G-ITS与6G-ITS的安全威胁，并提供了潜在的缓解方案。

Result: 识别了6G-ITS在交通领域的潜在优势和存在的安全与隐私挑战。提出了攻击模型分类，并指出为有效缓解新兴风险，迫切需要一个涵盖物理基础设施、网络协议、数据管理、应用安全和信任管理的多层综合安全框架。

Conclusion: 为有效减轻新兴安全与隐私风险，并确保未来交通生态系统的完整性和弹性，必须建立一个全面的、多层次的安全框架。

Abstract: The advancement of 6G technology has the potential to revolutionize the
transportation sector and significantly improve how we travel. 6G-enabled
Intelligent Transportation Systems (ITS) promise to offer high-speed,
low-latency communication and advanced data analytics capabilities, supporting
the development of safer, more efficient, and more sustainable transportation
solutions. However, various security and privacy challenges were identified in
the literature that must be addressed to enable the safe and secure deployment
of 6G-ITS and ensure people's trust in using these technologies. This paper
reviews the opportunities and challenges of 6G-ITS, particularly focusing on
trust, security, and privacy, with special attention to quantum technologies
that both enhance security through quantum key distribution and introduce new
vulnerabilities. It discusses the potential benefits of 6G technology in the
transportation sector, including improved communication, device
interoperability support, data analytic capabilities, and increased automation
for different components, such as transportation management and communication
systems. A taxonomy of different attack models in 6G-ITS is proposed, and a
comparison of the security threats in 5G-ITS and 6G-ITS is provided, along with
potential mitigating solutions. This research highlights the urgent need for a
comprehensive, multi-layered security framework spanning physical
infrastructure protection, network protocol security, data management
safeguards, application security measures, and trust management systems to
effectively mitigate emerging security and privacy risks and ensure the
integrity and resilience of future transportation ecosystems.

</details>


### [223] [L4Span: Spanning Congestion Signaling over NextG Networks for Interactive Applications](https://arxiv.org/abs/2510.02682)
*Haoran Wan,Kyle Jamieson*

Main category: cs.NI

TL;DR: L4Span是一种轻量级蜂窝无线接入网（RAN）设计，它通过预测RAN排队并进行ECN标记，将RAN队列状态与端到端低延迟信令连接，从而显著降低延迟高达98%并保持高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 未来交互式应用对低延迟网络有迫切需求，但作为关键的“最后一英里”，蜂窝RAN在部署低延迟排队信令机制方面落后于有线宽带ISP，需要一种可增量部署且普适的解决方案。

Method: 本文提出L4Span，一种新的RAN设计。它通过一个简单接口抽象RAN排队复杂性，将RAN队列状态与端到端低延迟信令（一直到内容服务器）关联。L4Span在毫秒级时间尺度上预测RAN的排队占用，并对低延迟和经典流量执行ECN标记。L4Span轻量级、对RAN修改极少，且兼容3GPP和O-RAN标准。研究人员在srsRAN开源软件中用C++实现了原型。

Result: 评估结果显示，在各种无线信道条件下，部署L4Span后，低延迟和经典流量的单向延迟最多可降低98%，同时保持接近线速的吞吐量。

Conclusion: L4Span通过其创新的设计，成功解决了蜂窝RAN的低延迟挑战，显著改善了交互式应用的性能，并兼顾了易部署性、网络兼容性和高吞吐量。

Abstract: Design for low latency networking is essential for tomorrow's interactive
applications, but it is essential to deploy incrementally and universally at
the network's last mile. While wired broadband ISPs are rolling out the leading
queue occupancy signaling mechanisms, the cellular Radio Access Network (RAN),
another important last mile to many users, lags behind these efforts. This
paper proposes a new RAN design, L4Span, that abstracts the complexities of RAN
queueing in a simple interface, thus tying the queue state of the RAN to
end-to-end low-latency signaling all the way back to the content server. At
millisecond-level timescales, L4Span predicts the RAN's queuing occupancy and
performs ECN marking for both low-latency and classic flows. L4Span is
lightweight, requiring minimal RAN modifications, and remains 3GPP and O-RAN
compliant for maximum ease of deployment. We implement a prototype on the
srsRAN open-source software in C++. Our evaluation compares the performance of
low-latency as well as classic flows with or without the deployment of L4Span
in various wireless channel conditions. Results show that L4Span reduces the
one-way delay of both low-latency and classic flows by up to 98 %, while
simultaneously maintaining near line-rate throughput. The code is available at
https://github.com/PrincetonUniversity/L4Span.

</details>


### [224] [FSMA: Scalable and Reliable LoRa for Non-Terrestrial Networks with Mobile Gateways](https://arxiv.org/abs/2510.02800)
*Rohith Reddy Vennam,Maiyun Zhang,Raghav Subbaraman,Deepak Vashist,Dinesh Bharadia*

Main category: cs.NI

TL;DR: 针对非地面网络（NTN）物联网面临的碰撞和动态链路问题，提出Free Signal Multiple Access (FSMA)协议，通过轻量级“FreeChirp”信号实现无同步、链路感知的随机接入，显著提升吞吐量、包接收率和能效。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道（LEO）卫星和无人机在物联网应用中推广，非地面网络（NTN）潜力巨大，但也面临两大挑战：(1) 大覆盖范围导致频繁冲突；(2) 移动网关造成动态链路，需要无同步、链路感知的传输。现有随机接入方案（如ALOHA、CSMA、BSMA）在此场景下表现不佳，存在高冲突率、隐藏终端或过高网关能耗。

Method: 提出Free Signal Multiple Access (FSMA)，这是一种网关控制协议，引入了轻量级自由信号啁啾（FreeChirp）。FreeChirp确保节点仅在信道空闲且链路可靠时进行传输，从而在无需同步或复杂调度的情况下减少冲突并实现链路感知接入。

Result: 通过25个商用LoRa设备和无人机移动网关进行评估，FSMA相比基线方案实现了高达2倍的吞吐量提升，2至5倍的包接收率改善，以及5倍的能效提升。大规模仿真使用自定义卫星物联网模拟器进一步表明，FSMA可支持每个卫星过境5000多个设备。

Conclusion: FSMA被证明是迈向可扩展、节能且可靠的NTN物联网网络的实用一步。

Abstract: The proliferation of Low Earth Orbit (LEO) satellites for universal IoT
applications and the growing use of drones in emergency services, agriculture,
and military operations highlight the transformative potential of
non-terrestrial networks (NTN). However, these networks face two key
challenges: (1) large coverage footprints that create frequent collisions and
(2) moving gateways that cause dynamic links and demand synchronization-free,
link-aware transmissions. Existing random access schemes such as ALOHA, CSMA,
and BSMA fail in this setting, suffering from high collision rates, hidden
terminals, or excessive gateway energy overhead. We propose Free Signal
Multiple Access (FSMA), a gateway-controlled protocol that introduces a
lightweight free signal chirp (FreeChirp). FreeChirp ensures that nodes
transmit only when the channel is idle and when links are reliable, thereby
reducing collisions and enabling link-aware access without the need for
synchronization or complex scheduling. We evaluate FSMA using 25 commercial
LoRa devices with a drone-mounted moving gateway and demonstrate up to 2x
higher throughput, 2x to 5x better packet reception ratio, and 5x improved
energy efficiency compared to the baselines. Large-scale simulations with a
custom Satellite IoT Simulator further show that FSMA scales to 5000+ devices
per satellite pass. These results establish FSMA as a practical step toward
scalable, energy-efficient, and reliable NTN IoT networks.

</details>


### [225] [DH-EAC: Design of a Dynamic, Hierarchical Entanglement Access Control Protocol](https://arxiv.org/abs/2510.02895)
*Akihisa Takahashi,Yoshito Tobe*

Main category: cs.NI

TL;DR: DH-EAC是一个纯量子协议，通过两层量子抽签机制，实现在多量子局域网（QLANs）组成的广域量子网络中，公平且匿名地分配稀缺的纠缠。


<details>
  <summary>Details</summary>
Motivation: 现有基于Dicke态的纯量子MAC协议主要适用于单个QLAN的静态条件，难以扩展到广域、动态设置，且需避免后选择协调，这仍然是一个开放问题。

Method: 本文提出DH-EAC协议，采用两层纯量子抽签机制：外层选择获胜的QLAN，内层选择获胜QLAN内的节点。其核心设计原则是获胜集合和各QLAN配额仅通过测量确定，无需经典往返通信，从而实现匿名性和公平性。通过分析模型评估成功概率和延迟，并与两种基线（单层Dicke和经典GO驱动分配器）进行对比，评估指标包括成功概率、端到端延迟、吞吐量和Jain公平性指数。

Result: 研究结果表明，DH-EAC在纠缠访问控制领域提供了一个可实现的方案，成功平衡了纯量子竞争解决、匿名性以及多QLAN网络的可扩展性。

Conclusion: DH-EAC协议是一个有效且可实现的纯量子解决方案，能够为广域多QLAN量子网络提供公平、匿名的纠缠分配，解决了现有协议在动态、广域环境中的扩展性挑战。

Abstract: We propose Dynamic, Hierarchical Entanglement Access Control (DH-EAC), a
pure-quantum protocol for fair and anonymous allocation of scarce entanglement
across wide-area quantum networks composed of many quantum LANs (QLANs). Prior
Dicke-state-based pure-quantum MACs resolve contention by local measurements
without classical signaling, but they mainly target a single QLAN under static
conditions; extending them to wide-area, dynamic settings while avoiding
post-selection reconciliation remains open. DH-EAC adopts a two-layer
pure-quantum lottery: the outer layer selects winning QLANs and the inner layer
selects winning nodes within each winning QLAN. A key design principle is that
both the winning set and the per-QLAN quota are fixed by measurements alone, so
the contention loop requires no classical round trip. The protocol thus aims to
jointly satisfy anonymity (no node IDs revealed until decisions are fixed) and
fairness (bias suppression under heterogeneous QLAN sizes). We also provide
analytical models for success probability and latency under a standard i.i.d.
loss model, and we evaluate DH-EAC against two baselines - single-layer Dicke
within one QLAN and a classical GO-driven allocator - using a minimal,
reproducible set of scenarios. Metrics include success probability, end-to-end
latency, throughput, and Jain's fairness index. The results indicate that
DH-EAC offers an implementable design point in the space of entanglement access
control, balancing pure-quantum contention resolution, anonymity, and
scalability for multi-QLAN networks.

</details>


### [226] [Sequence-Based Deep Learning for Handover Optimization in Dense Urban Cellular Network](https://arxiv.org/abs/2510.02958)
*Muhammad Kabeer,Rosdiadee Nordin,Mehran Behjati,Lau Sian Lun*

Main category: cs.NI

TL;DR: 本文提出一种基于多维特征的序列深度学习（GRU）模型，用于密集城市蜂窝网络的切换检测与避免，显著减少了不必要和乒乓切换，提升了连接稳定性，并展现出高效的实时部署能力。


<details>
  <summary>Details</summary>
Motivation: 在密集城市蜂窝网络中，高小区密度、用户移动性和多样化服务需求导致不必要的切换和乒乓效应频发，使得高效的切换管理成为一个严峻挑战。

Method: 本文利用包含30,925条标记记录的真实世界多运营商驾车测试数据集，将切换预测公式化为序列问题。评估了Gated Recurrent Unit (GRU)、Long Short-Term Memory (LSTM) 和 Transformer 等深度学习架构，并在仅使用Reference Signal Received Power (RSRP) 和使用所有特征的两种设置下进行了性能对比。

Result: 整合多维特征显著提升了密集城市蜂窝网络的切换性能。所提出的GRU模型将乒乓切换减少了98%，不必要切换减少了46.25%（优于基线RSRP-only方法的22.19%），并使驻留时间（ToS）提高了46%。该解决方案推理时间仅0.91秒，高效适用于实时边缘部署。

Conclusion: 本研究提出的基于GRU的深度学习模型在密集城市环境中显著提升了移动鲁棒性和用户体验质量（QoE），优于传统3GPP A3算法，为5G及未来网络的智能移动管理提供了高效实用的解决方案。

Abstract: Efficient handover management remains a critical challenge in dense urban
cellular networks, where high cell density, user mobility, and diverse service
demands increase the likelihood of unnecessary handovers and ping-pong effects.
This paper leverages a real-world, multi-operator drive-test dataset of 30,925
labelled records collected within a 2 km area around Sunway City to investigate
sequence-based deep learning approaches for handover detection and avoidance.
We formulate handover prediction as a sequence problem and evaluate Gated
Recurrent Unit (GRU), Long Short-Term Memory (LSTM), and Transformer
architectures under Reference Signal Received Power (RSRP)-only and all-feature
settings. The integration of multi-dimensional features significantly enhanced
handover performance in dense urban cellular networks. The proposed GRU-based
model achieved a remarkable 98% reduction in ping-pong handovers, alongside a
46.25% decrease in unnecessary handovers, outperforming the baseline RSRP-only
approach which yielded a 22.19% reduction. Furthermore, the model demonstrated
a 46% improvement in Time of Stay (ToS), indicating more stable user
connections. With an inference time of just 0.91 seconds, the solution proves
highly efficient and well-suited for real-time edge deployment scenarios.
Compared to the conventional 3GPP A3 algorithm, these improvements demonstrate
significant gains in mobility robustness and user Quality of Experience (QoE)
improvement. The dataset is released to foster reproducibility and further
research in intelligent mobility management for 5G and beyond.

</details>


### [227] [Automatic Generation of Digital Twins for Network Testing](https://arxiv.org/abs/2510.03205)
*Shenjia Ding,David Flynn,Paul Harvey*

Main category: cs.NI

TL;DR: 为解决自动网络操作软件验证耗时问题，本文提出自动生成数字孪生，实验证明其能高效准确地用于验证。


<details>
  <summary>Details</summary>
Motivation: 随着电信网络向自治化发展，软件测试和验证需求激增。数字孪生虽是有效验证手段，但其配置与执行需大量时间和人力投入。

Method: 探索自动生成数字孪生技术，旨在提供高效、准确的验证工具，并与ITU-T自治网络架构的实验子系统对齐。

Result: 通过初始用例的实验，证明了该方法在自动创建高效且足够准确的数字孪生方面是可行的。

Conclusion: 自动生成的数字孪生能够作为现有验证流程的一部分，有效提升软件测试与验证的效率和准确性。

Abstract: The increased use of software in the operation and management of
telecommunication networks has moved the industry one step closer to realizing
autonomous network operation. One consequence of this shift is the
significantly increased need for testing and validation before such software
can be deployed. Complementing existing simulation or hardware-based
approaches, digital twins present an environment to achieve this testing;
however, they require significant time and human effort to configure and
execute. This paper explores the automatic generation of digital twins to
provide efficient and accurate validation tools, aligned to the ITU-T
autonomous network architecture's experimentation subsystem. We present
experimental results for an initial use case, demonstrating that the approach
is feasible in automatically creating efficient digital twins with sufficient
accuracy to be included as part of existing validation pipelines.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [228] [SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting](https://arxiv.org/abs/2510.02469)
*Sung-Yeon Park,Adam Lee,Juanwu Lu,Can Cui,Luyang Jiang,Rohit Gupta,Kyungtae Han,Ahmadreza Moradipari,Ziran Wang*

Main category: cs.RO

TL;DR: SIMSplat是一个基于语言控制和高斯泼溅的预测性驾驶场景编辑器，能实现高效且真实的场景操控。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶场景操作框架在高效生成真实场景方面受限于编辑能力不足。

Method: SIMSplat通过语言对齐的高斯泼溅技术，支持自然语言提示进行直观操控。它能直接查询路面物体，进行精细的对象级编辑（如添加新物体、修改车辆和行人的轨迹），并通过多智能体运动预测进行预测性路径优化，生成真实的交互。

Result: 在Waymo数据集上的实验表明，SIMSplat具有广泛的编辑能力和在多种场景下的适应性。

Conclusion: SIMSplat提供了一种灵活、真实且高效的驾驶场景操作方法，显著提升了场景编辑的能力和现实性。

Abstract: Driving scene manipulation with sensor data is emerging as a promising
alternative to traditional virtual driving simulators. However, existing
frameworks struggle to generate realistic scenarios efficiently due to limited
editing capabilities. To address these challenges, we present SIMSplat, a
predictive driving scene editor with language-aligned Gaussian splatting. As a
language-controlled editor, SIMSplat enables intuitive manipulation using
natural language prompts. By aligning language with Gaussian-reconstructed
scenes, it further supports direct querying of road objects, allowing precise
and flexible editing. Our method provides detailed object-level editing,
including adding new objects and modifying the trajectories of both vehicles
and pedestrians, while also incorporating predictive path refinement through
multi-agent motion prediction to generate realistic interactions among all
agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's
extensive editing capabilities and adaptability across a wide range of
scenarios. Project page: https://sungyeonparkk.github.io/simsplat/

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [229] [Learning a distance measure from the information-estimation geometry of data](https://arxiv.org/abs/2510.02514)
*Guy Ohayon,Pierre-Etienne H. Fiquet,Florentin Guth,Jona Ballé,Eero P. Simoncelli*

Main category: eess.IV

TL;DR: 本文提出信息-估计度量 (IEM)，这是一种基于连续概率密度的新型距离函数，它将信息论与估计论联系起来。IEM是一种有效的全局度量，能适应复杂数据分布的几何结构，可通过学习去噪器计算，并在预测人类感知判断方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有距离度量（如马哈拉诺比斯距离）主要适用于高斯分布，但难以适应更复杂数据分布的内在几何结构。研究者旨在构建一种植根于信息论与估计论基本原理的新型距离函数，能够通过信号的对数概率与其最优去噪器的误差之间的关系，更准确地捕获信号间的相似性，并适应任意复杂的数据分布。

Method: IEM从信号域上的连续概率密度导出，其核心是将信号的对数概率与最优去噪器在噪声观测下产生的误差联系起来。具体而言，通过比较一对信号在不同噪声幅度下的去噪误差向量来计算IEM。几何上，这相当于比较模糊密度在不同模糊水平下围绕信号的得分向量场。研究证明IEM是一个有效的全局度量，并推导了其局部二阶近似的闭式表达式（产生黎曼度量）。在实践中，IEM可以通过学习型去噪器（类似于生成扩散模型）并求解一维积分来计算。

Result: IEM被证明是一个有效的全局度量，并且获得了其局部二阶近似的闭式表达式。对于高斯分布信号，IEM与马哈拉诺比斯距离一致。但对于更复杂的分布，IEM能局部和全局地适应分布的几何结构。在ImageNet数据库上学习得到的IEM，在预测人类感知判断方面，与最先进的监督式图像质量度量相比，具有竞争力甚至表现更优。

Conclusion: 信息-估计度量 (IEM) 提供了一种理论基础坚实且能适应复杂数据几何的新型距离函数。它通过信息论与估计论的联系，有效量化了信号相似性。IEM在实际应用中表现出强大的性能，尤其在图像质量感知评估方面，验证了其作为通用距离度量的价值。

Abstract: We introduce the Information-Estimation Metric (IEM), a novel form of
distance function derived from an underlying continuous probability density
over a domain of signals. The IEM is rooted in a fundamental relationship
between information theory and estimation theory, which links the
log-probability of a signal with the errors of an optimal denoiser, applied to
noisy observations of the signal. In particular, the IEM between a pair of
signals is obtained by comparing their denoising error vectors over a range of
noise amplitudes. Geometrically, this amounts to comparing the score vector
fields of the blurred density around the signals over a range of blur levels.
We prove that the IEM is a valid global metric and derive a closed-form
expression for its local second-order approximation, which yields a Riemannian
metric. For Gaussian-distributed signals, the IEM coincides with the
Mahalanobis distance. But for more complex distributions, it adapts, both
locally and globally, to the geometry of the distribution. In practice, the IEM
can be computed using a learned denoiser (analogous to generative diffusion
models) and solving a one-dimensional integral. To demonstrate the value of our
framework, we learn an IEM on the ImageNet database. Experiments show that this
IEM is competitive with or outperforms state-of-the-art supervised image
quality metrics in predicting human perceptual judgments.

</details>


### [230] [A UAV-Based VNIR Hyperspectral Benchmark Dataset for Landmine and UXO Detection](https://arxiv.org/abs/2510.02700)
*Sagar Lekhak,Emmett J. Ientilucci,Jasper Baur,Susmita Ghosh*

Main category: eess.IV

TL;DR: 本文介绍了一个新的无人机（UAV）可见光和近红外（VNIR）高光谱数据集，用于地雷和未爆弹药（UXO）检测研究，并经过严格校准和验证。


<details>
  <summary>Details</summary>
Motivation: 现有公开可用的无人机高光谱地雷检测数据存在空白，本研究旨在填补这一关键空白，为相关研究提供基准数据集。

Method: 使用搭载Headwall Nano-Hyperspec传感器的多传感器无人机平台，在20.6米高度采集了包含143个地雷/UXO目标的270个光谱波段（398-1002 nm）数据。数据经过辐射校准、正射校正、镶嵌，并使用双点经验线法（ELM）进行反射率检索。

Result: 数据集通过与六个参考对象的交叉验证，在400-900 nm范围内取得了低于1.0的RMSE值和1-6度的SAM值，证明了高光谱保真度。数据集包含了原始辐射立方体、GCP/AeroPoint数据和参考光谱，支持可重现研究。

Conclusion: 该数据集填补了开放获取的无人机高光谱地雷检测数据的关键空白，支持可重现研究，并可与已发布的电磁感应（EMI）数据结合，形成多传感器基准。

Abstract: This paper introduces a novel benchmark dataset of Visible and Near-Infrared
(VNIR) hyperspectral imagery acquired via an unmanned aerial vehicle (UAV)
platform for landmine and unexploded ordnance (UXO) detection research. The
dataset was collected over a controlled test field seeded with 143 realistic
surrogate landmine and UXO targets, including surface, partially buried, and
fully buried configurations. Data acquisition was performed using a Headwall
Nano-Hyperspec sensor mounted on a multi-sensor drone platform, flown at an
altitude of approximately 20.6 m, capturing 270 contiguous spectral bands
spanning 398-1002 nm. Radiometric calibration, orthorectification, and
mosaicking were performed followed by reflectance retrieval using a two-point
Empirical Line Method (ELM), with reference spectra acquired using an SVC
spectroradiometer. Cross-validation against six reference objects yielded RMSE
values below 1.0 and SAM values between 1 and 6 degrees in the 400-900 nm
range, demonstrating high spectral fidelity. The dataset is released alongside
raw radiance cubes, GCP/AeroPoint data, and reference spectra to support
reproducible research. This contribution fills a critical gap in open-access
UAV-based hyperspectral data for landmine detection and offers a multi-sensor
benchmark when combined with previously published drone-based electromagnetic
induction (EMI) data from the same test field.

</details>


### [231] [Image Enhancement Based on Pigment Representation](https://arxiv.org/abs/2510.02713)
*Se-Ho Lee,Keunsoo Ko,Seung-Wook Kim*

Main category: eess.IV

TL;DR: 该论文提出一种新颖高效的图像增强方法，通过将RGB颜色转换为高维“颜料”特征空间并自适应调整参数，实现了优于现有技术且低计算复杂度的增强效果。


<details>
  <summary>Details</summary>
Motivation: 传统的图像增强方法受限于预定义颜色空间（如RGB），缺乏对输入内容的动态适应性，因此需要一种能根据内容自适应转换颜色的方法。

Method: 该方法将输入的RGB颜色转换为高维“颜料（pigments）”空间，然后对这些颜料进行单独重投影和混合以精炼信息。最终，这些颜料被转换回RGB颜色以生成增强图像。转换和重投影的参数由视觉编码器根据输入图像内容自适应估计。

Result: 实验结果表明，该方法在图像修饰和色调映射等增强任务中，性能优于现有最先进的方法。同时，它保持了相对较低的计算复杂度和较小的模型尺寸。

Conclusion: 该基于颜料表示的图像增强方法通过其自适应性和高表达能力，成功克服了传统方法的局限性，实现了卓越的图像增强效果，并具有良好的效率。

Abstract: This paper presents a novel and efficient image enhancement method based on
pigment representation. Unlike conventional methods where the color
transformation is restricted to pre-defined color spaces like RGB, our method
dynamically adapts to input content by transforming RGB colors into a
high-dimensional feature space referred to as \textit{pigments}. The proposed
pigment representation offers adaptability and expressiveness, achieving
superior image enhancement performance. The proposed method involves
transforming input RGB colors into high-dimensional pigments, which are then
reprojected individually and blended to refine and aggregate the information of
the colors in pigment spaces. Those pigments are then transformed back into RGB
colors to generate an enhanced output image. The transformation and
reprojection parameters are derived from the visual encoder which adaptively
estimates such parameters based on the content in the input image. Extensive
experimental results demonstrate the superior performance of the proposed
method over state-of-the-art methods in image enhancement tasks, including
image retouching and tone mapping, while maintaining relatively low
computational complexity and small model size.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [232] [Glaucoma Detection and Structured OCT Report Generation via a Fine-tuned Multimodal Large Language Model](https://arxiv.org/abs/2510.02403)
*Jalil Jalili,Yashraj Gavhane,Evan Walker,Anna Heinke,Christopher Bowd,Akram Belghith,Massimo A. Fazio,Christopher A. Girkin,C. Gustavo De Moraes,Jeffrey M. Liebmann,Sally L. Baxter,Robert N. Weinreb,Linda M. Zangwill,Mark Christopher*

Main category: q-bio.QM

TL;DR: 本研究开发并微调了一个多模态大语言模型（MM-LLM），用于评估眼底OCT图像质量、诊断青光眼并生成详细的视网膜神经纤维层（RNFL）变薄报告，在各项任务中均表现出高准确度。


<details>
  <summary>Details</summary>
Motivation: 开发一个可解释的多模态大语言模型，旨在自动筛查视神经头（ONH）OCT圆周扫描图像的质量，并生成包含青光眼诊断和扇区视网膜神经纤维层（RNFL）变薄评估的结构化临床报告。

Method: 回顾性研究1,310名受试者的43,849份Spectralis ONH OCT扫描。基于Llama 3.2 Vision-Instruct模型，通过配对OCT图像和自动生成的结构化临床报告进行微调。模型在质量评估、青光眼检测和七个解剖扇区的RNFL变薄分类三项任务上进行评估，使用准确率、敏感性、特异性、精确度、F1-score及标准文本评估指标。

Result: 模型在质量分级方面达到0.90准确率和0.98特异性。青光眼检测准确率为0.86（F1-score 0.91）。RNFL变薄预测准确率范围为0.83至0.94，全局和颞侧扇区表现最佳。文本生成得分与参考报告高度一致（BLEU: 0.82; ROUGE-1: 0.94; BERTScore-F1: 0.99）。

Conclusion: 微调后的MM-LLM能够基于OCT图像生成准确的临床描述，在识别图像质量问题和检测青光眼方面表现出高准确性，并能提供扇区RNFL变薄描述，有效支持临床OCT评估。

Abstract: Objective: To develop an explainable multimodal large language model (MM-LLM)
that (1) screens optic nerve head (ONH) OCT circle scans for quality and (2)
generates structured clinical reports that include glaucoma diagnosis and
sector-wise retinal nerve fiber layer (RNFL) thinning assessments. Design:
Retrospective cohort study of 1,310 subjects contributing 43,849 Spectralis ONH
OCT circle scans (1,331 glaucomatous and 867 healthy eyes) from the DIGS and
ADAGES cohorts. Methods: A MM-LLM (Llama 3.2 Vision-Instruct model) was
fine-tuned to generate clinical descriptions of OCT imaging data. Training data
included paired OCT images and automatically generated, structured clinical
reports that described global and sectoral RNFL thinning. Poor-quality scans
were labeled as unusable and paired with a fixed refusal statement. The model
was evaluated on a held-out test set for three tasks: quality assessment,
glaucoma detection, and RNFL thinning classification across seven anatomical
sectors. Evaluation metrics included accuracy, sensitivity, specificity,
precision, and F1-score. Model description quality was also evaluated using
standard text evaluation metrics. Results: The model achieved 0.90 accuracy and
0.98 specificity for quality triage. For glaucoma detection, accuracy was 0.86
(sensitivity 0.91, specificity 0.73, F1-score 0.91). RNFL thinning prediction
accuracy ranged from 0.83 to 0.94, with highest performance in global and
temporal sectors. Text generation scores showed strong alignment with reference
reports (BLEU: 0.82; ROUGE-1: 0.94; ROUGE-2: 0.87; ROUGE-L: 0.92; BERTScore-F1:
0.99). Conclusions: The fine-tuned MM-LLM generated accurate clinical
descriptions based on OCT imaging. The model achieved high accuracy in
identifying image quality issues and detecting glaucoma. The model also
provided sectoral descriptions of RNFL thinning to help support clinical OCT
evaluation.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [233] [Corrosion Risk Estimation for Heritage Preservation: An Internet of Things and Machine Learning Approach Using Temperature and Humidity](https://arxiv.org/abs/2510.02973)
*Reginald Juan M. Mercado,Muhammad Kabeer,Haider Al-Obaidy,Rosdiadee Nordin*

Main category: cs.CY

TL;DR: 本研究开发了一套结合物联网和机器学习的系统，利用最小化环境数据（温度和相对湿度）对文化遗产钢结构进行腐蚀预测，并提供实时监测和维护建议。


<details>
  <summary>Details</summary>
Motivation: 文化遗产地（如菲律宾圣塞巴斯蒂安大教堂）的钢结构需要积极主动的保护，这要求准确的腐蚀预测。

Method: 开发了基于LoRa无线通信的物联网硬件系统，用于监测带有钢结构的遗产建筑。利用该系统生成的三年数据集，构建了一个机器学习框架，仅使用温度和相对湿度数据预测大气腐蚀速率。该框架通过Streamlit仪表板和ngrok隧道部署，提供实时腐蚀监测。

Result: 该框架能够提供实时腐蚀监测和可操作的保护建议。这种最小数据方法对于资源有限的遗产地而言具有可扩展性和成本效益，表明先进的回归模型可以从基本气象数据中提取准确的腐蚀预测。

Conclusion: 该研究使得全球范围内文化遗产结构能够实现主动保护，而无需部署广泛的传感器网络。

Abstract: Proactive preservation of steel structures at culturally significant heritage
sites like the San Sebastian Basilica in the Philippines requires accurate
corrosion forecasting. This study developed an Internet of Things hardware
system connected with LoRa wireless communications to monitor heritage
buildings with steel structures. From a three year dataset generated by the IoT
system, we built a machine learning framework for predicting atmospheric
corrosion rates using only temperature and relative humidity data. Deployed via
a Streamlit dashboard with ngrok tunneling for public access, the framework
provides real-time corrosion monitoring and actionable preservation
recommendations. This minimal-data approach is scalable and cost effective for
heritage sites with limited monitoring resources, showing that advanced
regression can extract accurate corrosion predictions from basic meteorological
data enabling proactive preservation of culturally significant structures
worldwide without requiring extensive sensor networks

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [234] [NetCAS: Dynamic Cache and Backend Device Management in Networked Environments](https://arxiv.org/abs/2510.02323)
*Joon Yong Hwang,Chanseo Park,Ikjun Yeom,Younghoon Kim*

Main category: cs.OS

TL;DR: NetCAS是一个动态I/O分流框架，在远程存储环境中，根据实时网络反馈和预计算的性能剖面，将I/O分配到缓存和后端设备，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现代存储系统虽结合缓存和后端设备加速I/O，但性能差距缩小后，并发访问可提升吞吐。然而，数据中心远程后端存储的网络不可预测性，使得I/O分流变得复杂。

Method: NetCAS框架通过实时网络反馈和预计算的性能剖面，动态地在缓存和后端设备间分流I/O。它根据工作负载配置和网络性能调整分流比，并采用低开销的批量轮询调度器执行分流，避免了每请求成本。

Result: NetCAS在远程存储环境下，比传统缓存性能提高高达174%；在网络条件波动时，比Orthus等收敛方案表现高出3.5倍。

Conclusion: NetCAS通过动态调整缓存与远程后端存储的I/O分流，有效应对了网络不确定性，显著提升了存储系统在数据中心环境下的I/O性能。

Abstract: Modern storage systems often combine fast cache with slower backend devices
to accelerate I/O. As performance gaps narrow, concurrently accessing both
devices, rather than relying solely on cache hits, can improve throughput.
However, in data centers, remote backend storage accessed over networks suffers
from unpredictable contention, complicating this split. We present NetCAS, a
framework that dynamically splits I/O between cache and backend devices based
on real-time network feedback and a precomputed Perf Profile. Unlike
traditional hit-rate-based policies, NetCAS adapts split ratios to workload
configuration and networking performance. NetCAS employs a low-overhead batched
round-robin scheduler to enforce splits, avoiding per-request costs. It
achieves up to 174% higher performance than traditional caching in remote
storage environments and outperforms converging schemes like Orthus by up to
3.5X under fluctuating network conditions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [235] [TLoRa: Implementing TLS Over LoRa for Secure HTTP Communication in IoT](https://arxiv.org/abs/2510.02519)
*Atonu Ghosh,Akhilesh Mohanasundaram,Srishivanth R F,Sudip Misra*

Main category: cs.CR

TL;DR: TLoRa是一种端到端架构，通过集成TCP隧道和完整的TLS 1.3握手，在LoRa网络上实现了HTTPS通信，提供了WiFi设备与互联网之间安全、无缝的连接。


<details>
  <summary>Details</summary>
Motivation: 需要在LoRa网络上为支持WiFi的终端设备和互联网之间建立安全、无缝的HTTPS通信通道。

Method: TLoRa架构包含端集线器（EH）和网络中继器（NR）。EH为用户设备提供WiFi热点，并通过LoRa上的安全隧道将URL请求转发给NR。NR作为服务器端代理，处理请求并将加密响应回传。TLoRa分会话建立、安全隧道和内容渲染三阶段操作，并实现了轻量级TLS记录重组层和会话复用队列机制。

Result: 在真实硬件上评估，TLoRa成功在9.9秒内建立了LoRa上的TLS会话，并在3.58秒内完成API请求，提供了一个实用的解决方案。

Conclusion: TLoRa是首个全面设计、实现并评估通过LoRa使用完整TLS进行HTTPS访问性能的工作，证明了其在LoRa上实现安全HTTPS通信的实用性。

Abstract: We present TLoRa, an end-to-end architecture for HTTPS communication over
LoRa by integrating TCP tunneling and a complete TLS 1.3 handshake. It enables
a seamless and secure communication channel between WiFi-enabled end devices
and the Internet over LoRa using an End Hub (EH) and a Net Relay (NR). The EH
tethers a WiFi hotspot and a captive portal for user devices to connect and
request URLs. The EH forwards the requested URLs to the NR using a secure
tunnel over LoRa. The NR, which acts as a server-side proxy, receives and
resolves the request from the Internet-based server. It then relays back the
encrypted response from the server over the same secure tunnel. TLoRa operates
in three phases -session setup, secure tunneling, and rendering. In the first
phase, it manages the TCP socket and initiates the TLS handshake. In the
second, it creates a secure tunnel and transfers encrypted TLS data over LoRa.
Finally, it delivers the URL content to the user. TLoRa also implements a
lightweight TLS record reassembly layer and a queuing mechanism for session
multiplexing. We evaluate TLoRa on real hardware using multiple accesses to a
web API. Results indicate that it provides a practical solution by successfully
establishing a TLS session over LoRa in 9.9 seconds and takes 3.58 seconds to
fulfill API requests. To the best of our knowledge, this is the first work to
comprehensively design, implement, and evaluate the performance of HTTPS access
over LoRa using full TLS.

</details>


### [236] [SoK: Preconfirmations](https://arxiv.org/abs/2510.02947)
*Aikaterini-Panagiota Stouka,Conor McMenamin,Demetris Kyriacou,Lin Oshitani,Quentin Botha*

Main category: cs.CR

TL;DR: 本文对区块链预确认协议进行知识系统化（SoK），旨在解决交易确认延迟问题。它定义了核心术语、提出了通用框架、分析了经济和风险，并审查了实际应用，以弥合理论与实践之间的差距。


<details>
  <summary>Details</summary>
Motivation: 传统区块链协议存在固有的交易确认延迟和不确定性，严重限制了用户体验。预确认协议应运而生，旨在为用户提供交易最终确认的早期保证，以改善用户体验。

Method: 本文采用知识系统化（SoK）方法。具体包括：界定预确认的核心术语和定义；提出一个通用的预确认协议框架；探讨预确认的经济性与风险；最后，将该框架应用于多个真实世界的预确认协议实现进行调查和分析。

Result: 本文成功建立了预确认领域的核心术语和定义，构建了一个通用的协议框架，深入分析了其经济和风险因素，并通过案例研究将理论框架应用于实际协议实现。

Conclusion: 通过提供全面的知识系统化，本文有效地弥合了预确认理论与实践之间的差距，为理解、设计和评估未来的预确认协议奠定了基础。

Abstract: In recent years, significant research efforts have focused on improving
blockchain throughput and confirmation speeds without compromising security.
While decreasing the time it takes for a transaction to be included in the
blockchain ledger enhances user experience, a fundamental delay still remains
between when a transaction is issued by a user and when its inclusion is
confirmed in the blockchain ledger. This delay limits user experience gains
through the confirmation uncertainty it brings for users. This inherent delay
in conventional blockchain protocols has led to the emergence of
preconfirmation protocols -- protocols that provide users with early guarantees
of eventual transaction confirmation.
  This article presents a Systematization of Knowledge (SoK) on
preconfirmations. We present the core terms and definitions needed to
understand preconfirmations, outline a general framework for preconfirmation
protocols, and explore the economics and risks of preconfirmations. Finally, we
survey and apply our framework to several implementations of real-world
preconfirmation protocols, bridging the gap between theory and practice.

</details>


### [237] [Modeling the Attack: Detecting AI-Generated Text by Quantifying Adversarial Perturbations](https://arxiv.org/abs/2510.02319)
*Lekkala Sai Teja,Annepaka Yadagiri,Sangam Sai Anish,Siva Gopala Krishna Nuthakki,Partha Pakray*

Main category: cs.CR

TL;DR: 针对大型语言模型生成文本检测系统对抗性攻击的脆弱性，特别是语义攻击，本文提出了一种名为扰动不变特征工程（PIFE）的新型检测框架。PIFE通过将输入文本标准化并量化其与规范形式的差异作为特征，显著提高了对抗鲁棒性，在应对语义攻击时表现优于传统对抗训练。


<details>
  <summary>Details</summary>
Motivation: 随着先进大型语言模型（LLMs）的快速发展，其双重用途问题日益突出，亟需创建可靠的AI生成文本检测系统。然而，现有检测器极易受到对抗性攻击，特别是释义攻击，这使得传统检测方法失效。

Method: 本研究首先量化了标准对抗训练的局限性，然后引入了扰动不变特征工程（PIFE）框架。PIFE通过多阶段标准化管道将输入文本转换为规范形式，并使用Levenshtein距离和语义相似度等指标量化转换幅度，将这些信号直接输入分类器。研究将常规强化Transformer模型与PIFE增强模型进行对比评估，使用了字符、词和句子层级的攻击分类法。

Result: 研究发现，传统对抗训练虽对语法噪声具有弹性，但在语义攻击面前失效（在1%误报率下，真阳性率骤降至48.8%），这一现象被称为“语义规避阈值”。相比之下，PIFE模型通过明确地从文本与其规范形式的差异中提取特征，在相同条件下保持了82.6%的真阳性率，有效抵御了最复杂的语义攻击。

Conclusion: 研究表明，明确地对扰动伪影进行建模，而非仅仅通过训练来适应它们，是实现对抗军备竞赛中真正鲁棒性的一条更有前景的途径。

Abstract: The growth of highly advanced Large Language Models (LLMs) constitutes a huge
dual-use problem, making it necessary to create dependable AI-generated text
detection systems. Modern detectors are notoriously vulnerable to adversarial
attacks, with paraphrasing standing out as an effective evasion technique that
foils statistical detection. This paper presents a comparative study of
adversarial robustness, first by quantifying the limitations of standard
adversarial training and then by introducing a novel, significantly more
resilient detection framework: Perturbation-Invariant Feature Engineering
(PIFE), a framework that enhances detection by first transforming input text
into a standardized form using a multi-stage normalization pipeline, it then
quantifies the transformation's magnitude using metrics like Levenshtein
distance and semantic similarity, feeding these signals directly to the
classifier. We evaluate both a conventionally hardened Transformer and our
PIFE-augmented model against a hierarchical taxonomy of character-, word-, and
sentence-level attacks. Our findings first confirm that conventional
adversarial training, while resilient to syntactic noise, fails against
semantic attacks, an effect we term "semantic evasion threshold", where its
True Positive Rate at a strict 1% False Positive Rate plummets to 48.8%. In
stark contrast, our PIFE model, which explicitly engineers features from the
discrepancy between a text and its canonical form, overcomes this limitation.
It maintains a remarkable 82.6% TPR under the same conditions, effectively
neutralizing the most sophisticated semantic attacks. This superior performance
demonstrates that explicitly modeling perturbation artifacts, rather than
merely training on them, is a more promising path toward achieving genuine
robustness in the adversarial arms race.

</details>


### [238] [Agentic-AI Healthcare: Multilingual, Privacy-First Framework with MCP Agents](https://arxiv.org/abs/2510.02325)
*Mohammed A. Shehab*

Main category: cs.CR

TL;DR: 介绍Agentic-AI Healthcare，一个隐私保护、多语言、可解释的研究原型，利用Model Context Protocol（MCP）编排智能代理进行医患互动，并集成符合医疗数据保护标准的合规层。


<details>
  <summary>Details</summary>
Motivation: 旨在探索并展示在医疗保健应用中，结合代理编排、多语言可访问性及合规感知架构的可行性，以实现隐私保护、可解释且多语言的患者互动。

Method: 采用Model Context Protocol (MCP) 编排多个智能代理，负责症状检查、用药建议和预约安排等患者互动功能。集成专门的隐私与合规层，包含基于角色的访问控制(RBAC)、AES-GCM字段级加密和防篡改审计日志，符合HIPAA等医疗数据保护标准。利用大型语言模型实现多语言患者-医生互动和透明的诊断推理。

Result: 成功演示了多语言（英语、法语、阿拉伯语）医患互动能力。展示了由大型语言模型驱动的透明诊断推理功能。验证了该原型在隐私保护、多语言支持和可解释性方面的实现。

Conclusion: 该研究作为一项应用AI贡献，强调了在医疗保健应用中结合代理编排、多语言可访问性和合规感知架构的实现是可行的。

Abstract: This paper introduces Agentic-AI Healthcare, a privacy-aware, multilingual,
and explainable research prototype developed as a single-investigator project.
The system leverages the emerging Model Context Protocol (MCP) to orchestrate
multiple intelligent agents for patient interaction, including symptom
checking, medication suggestions, and appointment scheduling. The platform
integrates a dedicated Privacy and Compliance Layer that applies role-based
access control (RBAC), AES-GCM field-level encryption, and tamper-evident audit
logging, aligning with major healthcare data protection standards such as HIPAA
(US), PIPEDA (Canada), and PHIPA (Ontario). Example use cases demonstrate
multilingual patient-doctor interaction (English, French, Arabic) and
transparent diagnostic reasoning powered by large language models. As an
applied AI contribution, this work highlights the feasibility of combining
agentic orchestration, multilingual accessibility, and compliance-aware
architecture in healthcare applications. This platform is presented as a
research prototype and is not a certified medical device.

</details>


### [239] [CATMark: A Context-Aware Thresholding Framework for Robust Cross-Task Watermarking in Large Language Models](https://arxiv.org/abs/2510.02342)
*Yu Zhang,Shuliang Liu,Xu Yang,Xuming Hu*

Main category: cs.CR

TL;DR: 提出一种上下文感知阈值水印算法（CAT），通过动态调整水印强度，解决了现有LLM水印在保证检测准确性同时牺牲文本质量、调参复杂和适应性差的问题，实现了质量提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印算法会导致文本质量下降，尤其在低熵场景；同时，依赖熵阈值的方法计算资源需求大，且对未知或跨任务生成场景适应性差。

Method: 提出上下文感知阈值水印（CAT）框架。该框架通过logits聚类将文本生成划分为语义状态，并据此动态调整水印强度，建立上下文感知的熵阈值。关键在于，它无需预设阈值或进行任务特定调优。

Result: 实验证明，CAT算法在跨任务场景中显著提升了文本质量，同时未牺牲水印检测的准确性。

Conclusion: CAT算法通过动态调整水印强度，有效解决了LLM水印在文本质量和检测鲁棒性之间的权衡问题，提高了文本质量和跨任务适应性，且无需额外调优。

Abstract: Watermarking algorithms for Large Language Models (LLMs) effectively identify
machine-generated content by embedding and detecting hidden statistical
features in text. However, such embedding leads to a decline in text quality,
especially in low-entropy scenarios where performance needs improvement.
Existing methods that rely on entropy thresholds often require significant
computational resources for tuning and demonstrate poor adaptability to unknown
or cross-task generation scenarios. We propose \textbf{C}ontext-\textbf{A}ware
\textbf{T}hreshold watermarking ($\myalgo$), a novel framework that dynamically
adjusts watermarking intensity based on real-time semantic context. $\myalgo$
partitions text generation into semantic states using logits clustering,
establishing context-aware entropy thresholds that preserve fidelity in
structured content while embedding robust watermarks. Crucially, it requires no
pre-defined thresholds or task-specific tuning. Experiments show $\myalgo$
improves text quality in cross-tasks without sacrificing detection accuracy.

</details>


### [240] [An Investigation into the Performance of Non-Contrastive Self-Supervised Learning Methods for Network Intrusion Detection](https://arxiv.org/abs/2510.02349)
*Hamed Fard,Tobias Schalau,Gerhard Wunder*

Main category: cs.CR

TL;DR: 本文系统比较了五种非对比自监督学习方法、三种编码器架构和六种数据增强策略在网络入侵检测中的性能，并在两个数据集上进行了90次实验，结果表明非对比方法在攻击检测方面具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统的监督学习在网络入侵检测中难以发现未知异常。尽管自监督学习在计算机视觉中取得成功并引起关注，但其在网络入侵检测中的应用，特别是非对比方法与不同编码器和数据增强策略结合的有效性尚不明确。

Method: 研究比较了五种非对比自监督学习方法，结合三种编码器架构和六种数据增强策略，在UNSW-NB15和5G-NIDD两个网络入侵检测数据集上进行了90项系统实验。通过平均精确率、召回率、F1分数和AUCROC评估性能，并将表现最佳的模型与DeepSVDD和Autoencoder两种无监督基线进行比较。

Result: 研究为每个自监督模型找出了表现最佳的编码器架构和数据增强方法组合。结果表明，非对比自监督学习方法在攻击检测方面相对于无监督基线（DeepSVDD和Autoencoder）具有竞争力。

Conclusion: 非对比自监督学习方法在网络入侵检测领域，通过恰当的编码器架构和数据增强策略，能够有效且具竞争力地进行攻击检测。

Abstract: Network intrusion detection, a well-explored cybersecurity field, has
predominantly relied on supervised learning algorithms in the past two decades.
However, their limitations in detecting only known anomalies prompt the
exploration of alternative approaches. Motivated by the success of
self-supervised learning in computer vision, there is a rising interest in
adapting this paradigm for network intrusion detection. While prior research
mainly delved into contrastive self-supervised methods, the efficacy of
non-contrastive methods, in conjunction with encoder architectures serving as
the representation learning backbone and augmentation strategies that determine
what is learned, remains unclear for effective attack detection. This paper
compares the performance of five non-contrastive self-supervised learning
methods using three encoder architectures and six augmentation strategies.
Ninety experiments are systematically conducted on two network intrusion
detection datasets, UNSW-NB15 and 5G-NIDD. For each self-supervised model, the
combination of encoder architecture and augmentation method yielding the
highest average precision, recall, F1-score, and AUCROC is reported.
Furthermore, by comparing the best-performing models to two unsupervised
baselines, DeepSVDD, and an Autoencoder, we showcase the competitiveness of the
non-contrastive methods for attack detection. Code at:
https://github.com/renje4z335jh4/non_contrastive_SSL_NIDS

</details>


### [241] [Measuring Physical-World Privacy Awareness of Large Language Models: An Evaluation Benchmark](https://arxiv.org/abs/2510.02356)
*Xinjie Shen,Mufei Li,Pan Li*

Main category: cs.CR

TL;DR: 为评估大型语言模型(LLMs)在具身智能体中对物理世界隐私的意识，本文提出了EAPrivacy基准。研究发现现有模型表现出严重缺陷，尤其是在动态环境、任务与隐私权衡以及社会规范冲突场景中，亟需加强物理世界隐私对齐。


<details>
  <summary>Details</summary>
Motivation: 将LLMs部署于具身智能体中，迫切需要衡量其在物理世界的隐私意识，然而现有评估方法仅限于自然语言场景，缺乏对物理世界隐私的量化评估。

Method: 引入EAPrivacy综合评估基准，该基准采用四层程序生成场景，旨在测试智能体处理敏感物品、适应变化环境、平衡任务执行与隐私约束，以及解决与社会规范冲突的能力。

Result: 当前模型表现出关键性缺陷。最高性能模型Gemini 2.5 Pro在涉及变化物理环境的场景中准确率仅59%。当任务伴随隐私请求时，模型在高达86%的情况下优先完成任务而非隐私约束。在隐私与社会规范冲突的高风险情境中，GPT-4o和Claude-3.5-haiku等领先模型在超过15%的情况下忽视了社会规范。

Conclusion: 研究结果揭示了LLMs在物理世界隐私方面存在根本性错位，强调了对更鲁棒、物理感知对齐的需求。

Abstract: The deployment of Large Language Models (LLMs) in embodied agents creates an
urgent need to measure their privacy awareness in the physical world. Existing
evaluation methods, however, are confined to natural language based scenarios.
To bridge this gap, we introduce EAPrivacy, a comprehensive evaluation
benchmark designed to quantify the physical-world privacy awareness of
LLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across
four tiers to test an agent's ability to handle sensitive objects, adapt to
changing environments, balance task execution with privacy constraints, and
resolve conflicts with social norms. Our measurements reveal a critical deficit
in current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\%
accuracy in scenarios involving changing physical environments. Furthermore,
when a task was accompanied by a privacy request, models prioritized completion
over the constraint in up to 86\% of cases. In high-stakes situations pitting
privacy against critical social norms, leading models like GPT-4o and
Claude-3.5-haiku disregarded the social norm over 15\% of the time. These
findings, demonstrated by our benchmark, underscore a fundamental misalignment
in LLMs regarding physically grounded privacy and establish the need for more
robust, physically-aware alignment.

</details>


### [242] [Privacy in the Age of AI: A Taxonomy of Data Risks](https://arxiv.org/abs/2510.02357)
*Grace Billiris,Asif Gill,Madhushi Bandara*

Main category: cs.CR

TL;DR: 本文通过系统综述对AI隐私风险进行了分类，识别出19种关键风险，并强调人为错误是最重要的因素，挑战了传统安全方法。


<details>
  <summary>Details</summary>
Motivation: AI系统处理敏感数据带来前所未有的隐私挑战，而传统隐私框架因AI的自主学习和黑箱决策等特性而显得不足。

Method: 通过对45项研究进行系统综述，本文提出了一个分类法，将AI隐私风险归纳为数据集级别、模型级别、基础设施级别和内部威胁风险四个类别，并识别了19个关键风险。

Result: 研究发现这些风险在各维度上分布均衡，其中人为错误（9.45%）是导致AI隐私风险的最重要因素。这表明传统安全方法侧重技术控制而忽视人为因素存在缺陷。

Conclusion: 该分类法通过整合AI隐私的技术和行为维度，挑战了传统的安全方法，弥补了整体理解上的空白，为可信赖AI的发展和未来研究奠定了基础。

Abstract: Artificial Intelligence (AI) systems introduce unprecedented privacy
challenges as they process increasingly sensitive data. Traditional privacy
frameworks prove inadequate for AI technologies due to unique characteristics
such as autonomous learning and black-box decision-making. This paper presents
a taxonomy classifying AI privacy risks, synthesised from 45 studies identified
through systematic review. We identify 19 key risks grouped under four
categories: Dataset-Level, Model-Level, Infrastructure-Level, and Insider
Threat Risks. Findings reveal a balanced distribution across these dimensions,
with human error (9.45%) emerging as the most significant factor. This taxonomy
challenges conventional security approaches that typically prioritise technical
controls over human factors, highlighting gaps in holistic understanding. By
bridging technical and behavioural dimensions of AI privacy, this paper
contributes to advancing trustworthy AI development and provides a foundation
for future research.

</details>


### [243] [Secure and Robust Watermarking for AI-generated Images: A Comprehensive Survey](https://arxiv.org/abs/2510.02384)
*Jie Cao,Qi Li,Zelin Zhang,Jianbing Ni*

Main category: cs.CR

TL;DR: 随着AI生成图片普及，知识产权和真实性担忧加剧，水印技术被视为解决方案。本论文对AI生成图片水印的现状进行了全面综述，涵盖了其系统、技术、评估、脆弱性和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展使得高质量图片易于生成，但同时也引发了知识产权保护、真实性和问责制等关键问题。

Method: 本文采用综合性综述方法，分析AI生成图片水印技术的五个关键维度：系统形式化、水印技术概述与比较、评估方法（视觉质量、容量、可检测性）、对恶意攻击的脆弱性以及当前挑战和未来方向。

Result: 旨在使研究人员能够全面理解AI生成图片水印技术。

Conclusion: 通过提供全面理解，促进AI生成图片水印技术的持续发展。

Abstract: The rapid advancement of generative artificial intelligence (Gen-AI) has
facilitated the effortless creation of high-quality images, while
simultaneously raising critical concerns regarding intellectual property
protection, authenticity, and accountability. Watermarking has emerged as a
promising solution to these challenges by distinguishing AI-generated images
from natural content, ensuring provenance, and fostering trustworthy digital
ecosystems. This paper presents a comprehensive survey of the current state of
AI-generated image watermarking, addressing five key dimensions: (1)
formalization of image watermarking systems; (2) an overview and comparison of
diverse watermarking techniques; (3) evaluation methodologies with respect to
visual quality, capacity, and detectability; (4) vulnerabilities to malicious
attacks; and (5) prevailing challenges and future directions. The survey aims
to equip researchers with a holistic understanding of AI-generated image
watermarking technologies, thereby promoting their continued development.

</details>


### [244] [A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison](https://arxiv.org/abs/2510.02707)
*Chinthana Wimalasuriya,Spyros Tragoudas*

Main category: cs.CR

TL;DR: 提出一种基于压缩/未压缩神经网络对行为比较的统计方法，实现高效、实时的对抗攻击检测，对多种攻击类型检测精度高且误报率低。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习系统面临对抗攻击的严重威胁，而现有检测方法难以有效识别未知或多样化的攻击，且检测精度不足。

Method: 提出一种统计方法，在神经网络部署前建立检测基线，实现实时对抗检测。通过比较压缩/未压缩神经网络对的行为生成对抗存在度量指标。

Result: 该方法在多种攻击类型上实现了接近完美的检测，并显著降低了误报率。

Conclusion: 该方法具有高可靠性和实用性，适用于实际应用中的实时对抗攻击检测。

Abstract: Adversarial attacks present a significant threat to modern machine learning
systems. Yet, existing detection methods often lack the ability to detect
unseen attacks or detect different attack types with a high level of accuracy.
In this work, we propose a statistical approach that establishes a detection
baseline before a neural network's deployment, enabling effective real-time
adversarial detection. We generate a metric of adversarial presence by
comparing the behavior of a compressed/uncompressed neural network pair. Our
method has been tested against state-of-the-art techniques, and it achieves
near-perfect detection across a wide range of attack types. Moreover, it
significantly reduces false positives, making it both reliable and practical
for real-world applications.

</details>


### [245] [Federated Spatiotemporal Graph Learning for Passive Attack Detection in Smart Grids](https://arxiv.org/abs/2510.02371)
*Bochra Al Agha,Razane Tajeddine*

Main category: cs.CR

TL;DR: 本文提出一种图中心、多模态检测器，通过联邦学习融合物理层和行为指标，利用时空上下文检测智能电网中的被动窃听，实现了高准确率和低误报率。


<details>
  <summary>Details</summary>
Motivation: 智能电网面临被动窃听威胁，攻击者静默监听可泄露拓扑、消费模式和操作行为，可能导致更严重的攻击。这种威胁难以检测，因为其信号微弱、短暂，且在单一节点或时间线分析时容易消失。

Method: 引入一种图中心、多模态检测器，在以自我为中心的星形子图和短时间窗口内融合物理层和行为指标。采用两阶段编码器：图卷积聚合空间上下文，双向GRU建模短期时间依赖，将异构特征转换为统一的时空表示。训练采用FedProx联邦学习设置，以提高对异构本地原始数据的鲁棒性并确保数据本地化。使用一个合成的、符合标准的数据集进行评估。

Result: 模型在每时间步测试准确率达到98.32% (F1_attack=0.972)，在0.15% FPR下，每序列准确率达到93.35%（使用m=2，阈值τ=0.55的简单决策规则）。

Conclusion: 结合空间和时间上下文能够可靠检测隐蔽侦察，同时保持低误报率，使该方法适用于非独立同分布（non-IID）的联邦智能电网部署。

Abstract: Smart grids are exposed to passive eavesdropping, where attackers listen
silently to communication links. Although no data is actively altered, such
reconnaissance can reveal grid topology, consumption patterns, and operational
behavior, creating a gateway to more severe targeted attacks. Detecting this
threat is difficult because the signals it produces are faint, short-lived, and
often disappear when traffic is examined by a single node or along a single
timeline. This paper introduces a graph-centric, multimodal detector that fuses
physical-layer and behavioral indicators over ego-centric star subgraphs and
short temporal windows to detect passive attacks. To capture stealthy
perturbations, a two-stage encoder is introduced: graph convolution aggregates
spatial context across ego-centric star subgraphs, while a bidirectional GRU
models short-term temporal dependencies. The encoder transforms heterogeneous
features into a unified spatio-temporal representation suitable for
classification. Training occurs in a federated learning setup under FedProx,
improving robustness to heterogeneous local raw data and contributing to the
trustworthiness of decentralized training; raw measurements remain on client
devices. A synthetic, standards-informed dataset is generated to emulate
heterogeneous HAN/NAN/WAN communications with wireless-only passive
perturbations, event co-occurrence, and leak-safe splits. The model achieves a
testing accuracy of 98.32% per-timestep (F1_{attack}=0.972) and 93.35%
per-sequence at 0.15% FPR using a simple decision rule with run-length m=2 and
threshold $\tau=0.55$. The results demonstrate that combining spatial and
temporal context enables reliable detection of stealthy reconnaissance while
maintaining low false-positive rates, making the approach suitable for non-IID
federated smart-grid deployments.

</details>


### [246] [A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory](https://arxiv.org/abs/2510.02373)
*Qianshan Wei,Tengchao Yang,Yaochen Wang,Xinfeng Li,Lijun Li,Zhenfei Yin,Yi Zhan,Thorsten Holz,Zhiqiang Lin,XiaoFeng Wang*

Main category: cs.CR

TL;DR: LLM代理的记忆存在安全漏洞，可被注入恶意记录以操纵行为并形成错误循环。本文提出了A-MemGuard，一种主动防御框架，通过共识验证和双记忆结构实现记忆的自查自纠，有效降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: LLM代理依赖记忆进行自主规划和决策，但这也引入了安全风险。攻击者可以注入看似无害的记录来操纵其未来行为。这种攻击难以检测（需特定上下文触发）且会形成自我强化的错误循环，导致初始错误放大并降低未来攻击门槛。

Method: 引入了A-MemGuard，这是首个针对LLM代理记忆的主动防御框架。其核心思想是让记忆本身具备自查和自纠能力，且不修改代理核心架构。它结合了两种机制：1) 基于共识的验证，通过比较来自多个相关记忆的推理路径来检测异常；2) 双记忆结构，将检测到的失败提炼成“教训”单独存储，并在未来行动前查阅，以打破错误循环并实现适应。

Result: 在多个基准测试上的综合评估表明，A-MemGuard能有效将攻击成功率降低95%以上，同时仅产生极小的效用成本。

Conclusion: A-MemGuard将LLM记忆安全从静态过滤转向主动的、经验驱动的模型，使防御能力随时间增强。这项工作有效地解决了LLM代理的记忆注入漏洞问题。

Abstract: Large Language Model (LLM) agents use memory to learn from past interactions,
enabling autonomous planning and decision-making in complex environments.
However, this reliance on memory introduces a critical security risk: an
adversary can inject seemingly harmless records into an agent's memory to
manipulate its future behavior. This vulnerability is characterized by two core
aspects: First, the malicious effect of injected records is only activated
within a specific context, making them hard to detect when individual memory
entries are audited in isolation. Second, once triggered, the manipulation can
initiate a self-reinforcing error cycle: the corrupted outcome is stored as
precedent, which not only amplifies the initial error but also progressively
lowers the threshold for similar attacks in the future. To address these
challenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive
defense framework for LLM agent memory. The core idea of our work is the
insight that memory itself must become both self-checking and self-correcting.
Without modifying the agent's core architecture, A-MemGuard combines two
mechanisms: (1) consensus-based validation, which detects anomalies by
comparing reasoning paths derived from multiple related memories and (2) a
dual-memory structure, where detected failures are distilled into ``lessons''
stored separately and consulted before future actions, breaking error cycles
and enabling adaptation. Comprehensive evaluations on multiple benchmarks show
that A-MemGuard effectively cuts attack success rates by over 95% while
incurring a minimal utility cost. This work shifts LLM memory security from
static filtering to a proactive, experience-driven model where defenses
strengthen over time. Our code is available in
https://github.com/TangciuYueng/AMemGuard

</details>
