<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 18]
- [cs.CV](#cs.CV) [Total: 15]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 为解决现有LLM偏见评估数据集在中文语境下稀缺且评估维度单一的问题，本文提出了多任务中文偏见评估基准McBE，并用其评估了多个主流LLM，发现它们普遍存在不同程度的偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLM广泛应用，其内在偏见逐渐显现，测量偏见对降低伦理风险至关重要。然而，现有偏见评估数据集主要集中于英文和北美文化，中文数据集稀缺且多为单任务，无法全面评估LLM偏见。

Method: 提出了一个多任务中文偏见评估基准McBE，包含4,077个偏见评估实例，覆盖12个单一偏见类别和82个子类别，并引入5种评估任务。随后，使用McBE评估了多个不同系列和参数规模的主流LLM。

Result: 所有被评估的LLM都表现出不同程度的偏见。研究对评估结果进行了深入分析，提供了关于LLM偏见的新颖见解。

Conclusion: 该研究构建了全面的中文偏见评估基准McBE，并揭示了主流LLM普遍存在的偏见问题，为深入理解和解决LLM偏见提供了重要发现。

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [2] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 该研究首次系统评估了推理大型语言模型（LLMs）在对话摘要中的表现，发现与非推理LLMs相比，推理LLMs在提供抽象和简洁摘要时表现不佳，反而常导致冗余和不一致，揭示了其局限性。


<details>
  <summary>Details</summary>
Motivation: 对话摘要在客户服务、会议分析等领域具有显著实用价值，尽管大型语言模型已取得进展，但逐步推理架构（如Long CoT）在要求同时抽象和简洁的对话场景中的表现尚未得到探索。

Method: 本研究对最先进的推理LLMs和非推理LLMs进行了首次全面系统评估，涵盖通用、角色导向和查询导向三种对话摘要范式。评估跨越不同语言、领域和摘要长度，并利用SAMSum、DialogSum、CSDS、QMSum等基准数据集，以及基于LLM的自动化指标和人工启发式评估协议。

Result: 研究发现，与其它推理密集型任务不同，明确的逐步推理并不能持续改善对话摘要质量。相反，推理LLMs相比非推理LLMs，更容易出现冗余、事实不一致和缺乏简洁性的摘要。通过情景分析和案例研究，识别了明确推理可能无法带来益处甚至阻碍复杂对话摘要的场景和原因。

Conclusion: 本工作为当前推理LLMs的局限性提供了新见解，并强调了针对真实世界对话摘要，需要采取有针对性的建模和评估策略。

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [3] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 本文研究了深度循环Transformer模型Huginn-3.5B中潜在思维链（Latent CoT）推理的出现情况。通过在算术任务上使用探针技术，发现可解释的潜在CoT证据有限，且增加循环深度收益甚微，远不如显式外部化推理的模型。


<details>
  <summary>Details</summary>
Motivation: 标准思维链（CoT）推理将推理步骤外部化，虽然可解释但效率低下。为捕获难以用语言表达的推理并提高效率，许多研究探索了旨在内部化推理（潜在空间）的循环架构。本文旨在探究此类潜在推理结构是否在深度循环Transformer模型Huginn-3.5B中出现。

Method: 研究人员使用深度循环Transformer模型Huginn-3.5B。他们通过Logit Lens和Coda Lens等探针技术，在算术任务上分析该模型的内部行为，具体通过跟踪最终和中间结果token的排名轨迹。

Result: 1) 发现了可解释的潜在思维链（Latent CoT）的有限证据。2) 揭示了循环块之间显著的探针不一致性，隐藏状态的可解释性严重依赖于层索引和解码方法。3) 经验表明，增加循环深度仅带来微弱收益，且远逊于显式外部化推理步骤的模型。

Conclusion: 在深度循环Transformer模型Huginn-3.5B中，潜在思维链推理结构似乎并未有效浮现，至少不是以易于解释的方式。简单增加循环深度未能带来与显式外部化推理模型相媲美的性能。

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [4] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: 本文介绍了GDC Cohort Copilot，一个开源AI工具，能将用户的自然语言描述转化为GDC队列过滤器，并证明其本地LLM模型在生成GDC队列方面优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: GDC用户在通过图形界面构建复杂癌症基因组数据队列时，难以在大量字段中找到特定描述符。然而，用户能更好地用自然语言描述其所需队列，因此需要一种更直观高效的队列创建方式。

Method: 开发了GDC Cohort Copilot工具，该工具利用大型语言模型（LLMs）将用户的自然语言描述自动转化为对应的GDC队列过滤器。研究中开发并评估了多个LLMs，并提供了一个交互式用户界面，允许用户进一步细化生成的队列。

Result: GDC Cohort Copilot成功实现了从自然语言描述自动生成GDC队列过滤器并导出进行分析。实验证明，该工具中本地部署的开源GDC Cohort LLM在生成GDC队列方面的表现优于GPT-4o的提示工程。

Conclusion: GDC Cohort Copilot通过自然语言处理极大地简化了GDC中癌症基因组数据队列的创建过程，提高了用户体验和效率。其定制化LLM的优异表现证实了该方法在生物信息学数据探索中的有效性。

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [5] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: 本文提出MemAgent，一个新型代理工作流，通过分段阅读和内存覆盖策略处理无限长文本，并扩展DAPO算法进行训练，实现了超长上下文外推能力，在3.5M QA和512K RULER测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管长文本处理技术有所进步，但在线性复杂度下处理无限长文档且不降低性能仍是核心挑战。

Method: 引入MemAgent代理工作流，以端到端方式优化长文本任务。MemAgent通过分段读取文本并采用覆盖策略更新内存。同时，扩展了DAPO算法，通过独立上下文的多对话生成来训练模型。

Result: MemAgent展现出卓越的超长上下文能力，能够从8K上下文（在32K文本上训练）外推到3.5M QA任务，性能损失低于5%，并在512K RULER测试中达到95%以上的准确率。

Conclusion: MemAgent通过其创新的代理工作流和训练方法，成功解决了长文本处理中无限长文档的挑战，实现了显著的上下文外推能力和高性能。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [6] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: 本文提出DoMIX，一种利用LoRA模块的新型持续领域自适应预训练方法，旨在解决现有方法计算成本高、对数据顺序敏感以及只能生成单一通用模型的问题，从而提供高效、鲁棒且能为特定任务定制模型的方案。


<details>
  <summary>Details</summary>
Motivation: 现有持续领域自适应预训练（continual DAP）方法面临多重挑战：1) 训练计算成本和GPU内存消耗高；2) 对增量数据顺序敏感；3) 提供一个针对所有最终任务的单一通用模型，这与DAP的本质相悖。

Method: 本文提出DoMIX方法，通过利用LoRA模块（一种参数高效微调PEFT方法）来应对上述挑战，实现高效并行的领域自适应预训练。

Result: DoMIX方法对领域顺序具有鲁棒性，能有效利用积累的知识为特定任务提供定制的预训练模型。此外，该方法可扩展至DAP设置之外的标准LLM微调场景。

Conclusion: DoMIX成功解决了现有持续DAP方法的局限性，提供了一种高效、鲁棒且能生成任务定制模型的领域自适应预训练方案，并具有广泛的应用潜力。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [7] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 本文介绍了一种用于SciVQA 2025科学视觉问答任务的系统，该系统结合了多模态大语言模型集成与少样本策略，并在盲测中排名第三。


<details>
  <summary>Details</summary>
Motivation: 本文旨在描述作者团队为SciVQA 2025科学视觉问答共享任务所开发的系统。

Method: 该系统采用两种多模态大语言模型的集成方法，并结合多种少样本示例检索策略。模型的选择和少样本设置根据图片和问题类型进行调整，并根据模型的置信度选择最终答案。

Result: 在盲测数据上，该系统在七个参与者中排名第三，ROUGE-1、ROUGE-L和BERTS的平均F1分数达到85.12。

Conclusion: 该系统在SciVQA 2025科学视觉问答任务中表现出色，通过集成多模态大语言模型和精细的少样本策略，取得了令人满意的第三名成绩，证明了其方法的有效性。

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [8] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: 本文提出QFFN-BERT，一种将BERT中的前馈网络(FFN)替换为参数化量子电路(PQC)的混合量子-经典Transformer。该模型在保持甚至提升性能的同时，大幅减少了FFN参数，并展现出优异的数据效率和少样本学习能力。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型中，前馈网络（FFN）模块占据了约三分之二的参数量。研究旨在探索利用参数化量子电路（PQC）替代FFN，以实现参数高效性和提升模型表达能力。

Method: 引入QFFN-BERT，将紧凑型BERT变体的前馈网络（FFN）模块替换为基于PQC的层。PQC架构融入了残差连接、R_Y和R_Z旋转以及交替纠缠策略，以确保训练稳定性和高表达能力。实验在经典模拟器上进行，并在SST-2和DBpedia基准数据集上进行评估。

Result: ['精心配置的QFFN-BERT在全数据设置下，达到了基线准确率的102.0%，超越了其经典对应模型，同时将FFN相关参数减少了99%以上。', '该模型在少样本学习场景中表现出持续且有竞争力的优势，证实了其卓越的数据效率潜力。']

Conclusion: 当与深度学习基本原理共同设计时，参数化量子电路（PQCs）可以作为经典前馈网络（FFNs）的强大且参数高效的替代方案。

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [9] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 通过参数模型优化代码数据选择，以小样本量显著提升大型语言模型在代码生成任务上的性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在代码生成和程序理解方面的性能提升主要依赖数据量，却忽视数据质量，导致训练效率低下。

Method: 提出一种利用参数模型进行代码数据选择的方法。该方法通过优化参数模型，确保所选数据子集具有良好的分布一致性和多样性，从而保证数据质量。

Result: 实验证明，仅使用10K样本，该方法在HumanEval上比92K全样本基线提升2.4%，在MBPP上提升2.3%，在性能和效率上均优于其他采样方法。

Conclusion: 该方法能有效提升模型性能，并显著降低计算成本。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [10] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: 本研究评估了阿坎语ASR模型在多样语音上下文中的泛化能力，发现模型存在域依赖性，且不同架构（如Whisper和Wav2Vec2）有不同的错误模式，强调了低资源语言ASR模型领域适应的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有ASR研究主要在域内数据集评估模型，但很少评估模型在多样语音上下文中的泛化能力。本研究旨在填补这一空白，探究ASR模型在不同领域数据集上的表现。

Method: 本研究基准测试了七个基于Transformer架构（如Whisper和Wav2Vec2）的阿坎语ASR模型。使用四个涵盖不同领域的阿坎语语音语料库进行评估，包括图像描述、非正式对话、圣经朗读和金融对话。通过词错误率（WER）和字符错误率（CER）进行性能比较。

Result: 研究发现模型存在明显的域依赖性，仅在其训练领域内表现最佳，在不匹配的场景中准确性显著下降。此外，Whisper和Wav2Vec2架构表现出不同的错误行为：Whisper模型产生更流畅但可能误导性的转录错误；Wav2Vec2在不熟悉输入时产生更明显但更难解释的输出。

Conclusion: 在为低资源语言应用选择ASR架构时，应考虑ASR错误的可读性和透明度之间的权衡。本研究结果强调了对阿坎语及其他低资源语言，需要有针对性的领域适应技术、自适应路由策略和多语言训练框架。

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [11] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 该研究提出一种为低资源语种（特别是阿坎语）受损语音构建ASR模型的数据收集方法，并提供社区驱动的数据收集指南和开源资源。


<details>
  <summary>Details</summary>
Motivation: 旨在普及ASR技术和数据收集，使言语障碍者的ASR技术更具包容性，尤其关注低资源语言。

Method: 核心方法是开发一套社区驱动的数据收集和ASR模型构建“操作指南”；作为概念验证，收集并整理了首个阿坎语受损语音开源数据集；并对现有开源ASR模型进行了微调。

Result: 成功整理并公开了首个阿坎语受损语音开源数据集，以及配套的操作指南和工具；初步结果显示微调后的ASR模型能更好地识别阿坎语受损语音。

Conclusion: 本研究通过提供社区驱动的方法和开源资源，助力研究人员和开发者创建针对言语障碍者独特需求的普惠性ASR技术。

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [12] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 引入了一个新的印度保释判决数据集，以解决印度法律NLP领域结构化数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 印度等地区由于缺乏结构化数据集，法律NLP发展滞后。

Method: 构建了IndianBailJudgments-1200数据集，包含1200份印度保释判决，并使用prompt-engineered的GPT-4o管道进行标注，标注了包括保释结果、IPC条款、犯罪类型和法律推理等20多个属性，并验证了标注一致性。

Result: 成功创建并推出了IndianBailJudgments-1200数据集，这是首个专注于印度保释判例的公开可用数据集。

Conclusion: 该数据集是印度法律NLP领域的关键资源，支持结果预测、摘要和公平性分析等多种法律NLP任务，填补了该领域的数据空白。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [13] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebSailor是一种新的后训练方法，旨在赋予开源LLM在复杂信息搜索任务中处理极端不确定性的能力，使其性能达到甚至超越专有智能体。


<details>
  <summary>Details</summary>
Motivation: 专有智能体（如DeepResearch）在复杂信息搜索基准测试中展现出超人能力，这归因于其在应对海量信息时系统性地降低不确定性的推理模式。开源模型缺乏此能力，因此存在性能差距，有必要研究如何弥补。

Method: 本研究提出了WebSailor，一种完整的后训练方法。其核心方法包括通过结构化采样和信息混淆生成高不确定性任务，采用RFT冷启动机制，以及使用高效的智能体强化学习训练算法——Duplicating Sampling Policy Optimization (DUPO)。

Result: WebSailor通过其整合的训练流程，在复杂信息搜索任务中显著优于所有开源智能体，并成功匹配了专有智能体的性能，有效弥补了能力差距。

Conclusion: WebSailor成功地将处理极端不确定性的关键能力注入到开源大模型中，使其在复杂信息搜索领域达到与专有系统相当的水平，从而推动了LLM训练的前沿发展。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [14] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本文提出一个以人类标签变异（HLV）为中心的主动学习（AL）概念框架，旨在处理真实世界标注中普遍存在的标签变异，并考虑整合大型语言模型（LLM）作为标注者。


<details>
  <summary>Details</summary>
Motivation: 监督学习中高质量标注数据获取受限；现有标注框架常假设单一真值，忽视普遍存在的人类标签变异（HLV）作为有益信号；主动学习（AL）通常依赖于与HLV不符的简化假设；需要将观察到的标签变异分解为信号与噪声。

Method: 分析关于真值和标签性质的基本假设；调研主动学习（AL）和人类标签变异（HLV）社区如何处理或忽视这些区别；提出一个将HLV整合到AL循环（包括实例选择、标注者选择和标签表示）中的概念框架；探讨将大型语言模型（LLM）作为标注者整合的可能性。

Result: 强调将观察到的标签变异分解为信号（如HLV）和噪声（如标注错误）的必要性；提出了一个将HLV整合到主动学习（AL）整个循环中的概念框架；讨论了整合大型语言模型（LLM）作为标注者的潜力。

Conclusion: 为以人类标签变异（HLV）为中心的主动学习奠定了概念基础，旨在更准确地反映真实世界标注的复杂性。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [15] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: Multiperspective Fusion (MPF)是一种创新的后训练对齐框架，旨在通过多视角生成和分解人类基线来简便有效地缓解大型语言模型（LLM）的偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，对其输出中偏见的缓解需求日益增长，尤其需要一种易于部署和操作的方案来解决现有方法的局限性。

Method: MPF框架建立在SAGED管道之上，通过利用多视角生成来揭示并对齐LLM输出中的偏见。它将人类基线（如HR专业人士的情感分布）分解为可解释的视角组件，并根据这些分解的概率对响应进行采样和平衡，从而指导LLM的生成过程。

Result: 实验结果表明，MPF能够有效将LLM的情感分布与反事实基线（绝对平等）和HR基线（对顶尖大学存在偏见）对齐。这显著降低了KL散度，减少了校准误差，并展现出对未见问题的泛化能力。

Conclusion: MPF提供了一种可扩展且可解释的LLM对齐和偏见缓解方法，该方法兼容已部署的LLM，且无需广泛的提示工程或微调，使得偏见消除过程更为高效和便捷。

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [16] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 该研究探讨了性别与语境偏见（包括动词、名词、职业）的关系，提出了一种新数据集GenderLexicon和框架来量化与解释性别偏见，并证实了性别偏见不仅限于职业刻板印象。


<details>
  <summary>Details</summary>
Motivation: 旨在探究性别与语境偏见（特别是动作动词、宾语名词和职业）之间的相关性。

Method: 引入了一个名为GenderLexicon的新数据集和一个能估计语境偏见及其相关性别偏见的框架。该模型能用分数解释偏见，提高性别偏见的解释性。通过在包括一个日语数据集在内的五个不同数据集上进行评估来验证其方法。

Result: 研究结果证实了性别偏见不仅存在于职业刻板印象中，还超越了这些范畴。

Conclusion: 该工作成功地揭示并量化了语境中的性别偏见，提供了量化和解释偏见的新工具和证据，并证实了其超越传统职业刻板印象的广泛存在。

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [17] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 本研究提出了首个评估大型语言模型（LLM）在同行评审中识别论文局限性能力的综合基准LimitGen，并通过文献检索增强了LLM生成局限性反馈的能力。


<details>
  <summary>Details</summary>
Motivation: 随着科学出版物数量的激增，专业密集型的同行评审面临巨大挑战。尽管LLM在多种科学任务中展现潜力，但其辅助同行评审，特别是在识别论文局限性方面的作用尚未被充分研究。

Method: 1. 提出了一个专注于AI领域科学研究中局限性类型的综合分类法。2. 构建了首个用于评估LLM识别局限性能力的综合基准LimitGen，包含LimitGen-Syn（合成数据集）和LimitGen-Human（真实人工编写的局限性）。3. 通过文献检索增强LLM系统，使其能够基于现有科学发现识别局限性。

Result: 本研究方法显著提升了LLM系统识别和生成研究论文局限性的能力。

Conclusion: 所提出的方法使LLM系统能够提供更具体、更具建设性的反馈，从而有效辅助早期阶段的同行评审和人类专家。

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [18] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 本研究首次测量了人声元音发音中“刚好可区分差异”（JPD），即产生可区分模仿所需的最小听觉距离，发现其在F1 x F2空间为14到51毫音，这为言语产生理论和元音系统结构提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明人类元音发音受听觉空间控制，且在亚音素层面也存在控制，但这种控制的精确度尚不清楚。

Method: 本研究采用元音模仿范式，测量了两组英语使用者在前元音发音中的“刚好可区分差异”（JPD），即在听觉空间中，两个元音刺激必须相距多远才能产生可靠的不同模仿。

Result: 研究估算出在F1 x F2空间中，“刚好可区分差异”（JPD）介于14到51毫音之间。

Conclusion: 该发现对言语产生的记忆理论具有重要意义，并通过设定两个元音音素在说话者共振峰空间中可能的最接近距离，阐明了人类元音系统的可能结构，从而为观察到的元音音素数量和模式趋势提供了心理物理学解释。

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [19] [Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges](https://arxiv.org/abs/2507.02074)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

TL;DR: 本文综述了利用大语言模型（LLMs）进行视频交通事故检测的最新方法。


<details>
  <summary>Details</summary>
Motivation: 交通事故检测在智能交通系统中至关重要。近期大语言模型（LLMs）和视觉-语言模型（VLMs）的发展为处理、推理和汇总多模态信息带来了变革，为解决该问题提供了新途径。

Method: 本文采用综述方法，回顾了利用LLMs进行视频交通事故检测的最新进展。具体内容包括：提出融合策略的结构化分类法，总结关键数据集，分析模型架构，比较性能基准，并讨论当前挑战与机遇。

Result: 作为一篇综述，本文提供了LLMs在视频交通事故检测领域的方法分类、数据集概览、模型架构分析及性能比较的全面总结。

Conclusion: 本次综述为视频理解和基础模型快速发展的交叉领域提供了未来研究基础。

Abstract: Crash detection from video feeds is a critical problem in intelligent
transportation systems. Recent developments in large language models (LLMs) and
vision-language models (VLMs) have transformed how we process, reason about,
and summarize multimodal information. This paper surveys recent methods
leveraging LLMs for crash detection from video data. We present a structured
taxonomy of fusion strategies, summarize key datasets, analyze model
architectures, compare performance benchmarks, and discuss ongoing challenges
and opportunities. Our review provides a foundation for future research in this
fast-growing intersection of video understanding and foundation models.

</details>


### [20] [Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning](https://arxiv.org/abs/2507.02148)
*Zijie Cai,Christopher Metzler*

Main category: cs.CV

TL;DR: 本文全面基准测试了水下单目深度估计模型，发现陆地训练模型在水下环境表现不佳。通过在合成水下数据集上微调Depth Anything V2，显著提升了水下深度估计性能，强调了域适应的关键性。


<details>
  <summary>Details</summary>
Motivation: 尽管单目深度估计在陆地环境中已能提供度量深度预测，但在水下环境中，由于光衰减、散射、颜色失真、浑浊以及缺乏高质量度量真值数据，其可靠性仍受限。

Method: 1. 在FLSea和SQUID等真实水下数据集上，对零样本和微调的单目度量深度估计模型进行了全面基准测试。 2. 评估了多种最先进模型在不同水下条件下的性能。 3. 为解决陆地训练模型在水下性能差的问题，使用基于物理的水下图像形成模型生成了Hypersim数据集的合成水下变体。 4. 在此合成数据集上，使用ViT-S骨干编码器对Depth Anything V2模型进行了微调。

Result: 1. 在陆地（真实或合成）数据上训练的大型模型，在水下表现不佳，存在显著的领域差异。 2. 经过微调后的模型在所有基准测试中均持续提升了性能。 3. 微调后的模型优于仅在干净的陆地Hypersim数据集上训练的基线模型。

Conclusion: 本研究详细评估了水下单目度量深度估计，强调了域适应和尺度感知监督对于在挑战性水下环境中实现鲁棒和可泛化的度量深度预测的重要性，为未来研究指明了方向。

Abstract: Monocular depth estimation has recently advanced to provide not only relative
but also metric depth predictions. However, its reliability in underwater
environments remains limited due to light attenuation and scattering, color
distortion, turbidity, and the lack of high-quality metric ground-truth data.
In this paper, we present a comprehensive benchmark of zero-shot and fine-tuned
monocular metric depth estimation models on real-world underwater datasets with
metric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of
state-of-the-art models across a range of underwater conditions with different
ranges. Our results show that large-scale models trained on terrestrial (real
or synthetic) data, while effective in in-air settings, perform poorly
underwater due to significant domain shifts. To address this, we fine-tune
Depth Anything V2 with a ViT-S backbone encoder on a synthetic underwater
variant of the Hypersim dataset, which we generated using a physically based
underwater image formation model. We demonstrate our fine-tuned model
consistently improves performance across all benchmarks and outperforms
baselines trained only on the clean in-air Hypersim dataset. Our study provides
a detailed evaluation and visualization for monocular metric depth estimation
in underwater scenes, highlighting the importance of domain adaptation and
scale-aware supervision for achieving robust and generalizable metric depth
predictions in challenging underwater environments for future research.

</details>


### [21] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 提出ESTR-CoT框架，结合链式思考（CoT）推理和事件流技术，解决了现有场景文本识别在可解释性和逻辑推理上的不足，并发布了大规模CoT数据集。


<details>
  <summary>Details</summary>
Motivation: 现有事件流场景文本识别方法在低光照和快速运动等极端挑战性场景下表现优于RGB相机，但仍受限于可解释性不足和上下文逻辑推理能力弱的问题。

Method: 提出ESTR-CoT框架：采用EVA-CLIP将事件流转为视觉标记，Llama分词器编码提示，Q-former将视觉标记与预训练的Vicuna-7B大语言模型对齐，同步输出识别结果和CoT推理过程，并支持端到端监督微调。此外，构建了一个大规模三阶段处理（生成、润色、专家验证）的CoT数据集用于训练。

Result: 在EventSTR、WordArt*、IC15*三个事件流场景文本识别基准数据集上的大量实验，充分验证了所提框架的有效性和可解释性。

Conclusion: ESTR-CoT框架在事件流场景文本识别中展现出卓越的有效性和可解释性，其构建的CoT数据集为后续基于推理的大模型发展提供了坚实的数据基础。

Abstract: Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel chain-of-thought reasoning based
event stream scene text recognition framework, termed ESTR-CoT. Specifically,
we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input
event stream into tokens and utilize a Llama tokenizer to encode the given
generation prompt. A Q-former is used to align the vision token to the
pre-trained large language model Vicuna-7B and output both the answer and
chain-of-thought (CoT) reasoning process simultaneously. Our framework can be
optimized using supervised fine-tuning in an end-to-end manner. In addition, we
also propose a large-scale CoT dataset to train our framework via a three stage
processing (i.e., generation, polish, and expert verification). This dataset
provides a solid data foundation for the development of subsequent
reasoning-based large models. Extensive experiments on three event stream STR
benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the
effectiveness and interpretability of our proposed framework. The source code
and pre-trained models will be released on
https://github.com/Event-AHU/ESTR-CoT.

</details>


### [22] [Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach](https://arxiv.org/abs/2507.02205)
*Elena Ryumina,Maxim Markitantov,Alexandr Axyonov,Dmitry Ryumin,Mikhail Dolgushin,Alexey Karpov*

Main category: cs.CV

TL;DR: 提出了一种新颖的零样本多模态复合表情识别方法，结合多种模态，性能与监督学习方法相当，且无需领域适应。


<details>
  <summary>Details</summary>
Motivation: 现有的复合表情识别（CER）方法依赖于特定的任务训练数据，限制了其泛化能力。本研究旨在克服这一限制，提出一种无需特定训练数据的零样本方法来检测复杂的复合情感状态。

Method: 本文提出了一种零样本多模态复合表情识别方法，整合了六种异构模态（静态和动态面部表情、场景与标签匹配、场景上下文、音频和文本）。该方法利用基于CLIP的标签匹配和Qwen-VL进行语义场景理解，并引入了多头概率融合（MHPF）模块动态加权模态预测，以及复合表情（CE）转换模块（通过PPA和PFSA）生成可解释的复合情感输出。

Result: 在多语料库训练和零样本测试下，该方法在AffWild2数据集上获得了46.95%的F1分数，在AFEW数据集上获得49.02%，在C-EXPR-DB数据集上获得34.85%。这些结果与在目标数据上训练的监督方法相当。

Conclusion: 所提出的零样本多模态方法在捕获复合表情方面表现出显著的有效性，且无需领域适应，证明了其强大的泛化能力。

Abstract: Compound Expression Recognition (CER), a subfield of affective computing,
aims to detect complex emotional states formed by combinations of basic
emotions. In this work, we present a novel zero-shot multimodal approach for
CER that combines six heterogeneous modalities into a single pipeline: static
and dynamic facial expressions, scene and label matching, scene context, audio,
and text. Unlike previous approaches relying on task-specific training data,
our approach uses zero-shot components, including Contrastive Language-Image
Pretraining (CLIP)-based label matching and Qwen-VL for semantic scene
understanding. We further introduce a Multi-Head Probability Fusion (MHPF)
module that dynamically weights modality-specific predictions, followed by a
Compound Expressions (CE) transformation module that uses Pair-Wise Probability
Aggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods
to produce interpretable compound emotion outputs. Evaluated under multi-corpus
training, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02%
on Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via
zero-shot testing, which is comparable to the results of supervised approaches
trained on target data. This demonstrates the effectiveness of the proposed
approach for capturing CE without domain adaptation. The source code is
publicly available.

</details>


### [23] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 本文介绍了一个大型数据集SciGA-145k，旨在解决科学论文图形摘要（GA）的设计和推荐难题，并通过定义相关任务和提出新评估指标，促进视觉科学交流。


<details>
  <summary>Details</summary>
Motivation: 图形摘要（GAs）在科学交流中作用关键，但其潜力尚未被充分发掘，且设计GAs需要专业的可视化技能，阻碍了其普及。

Method: 构建了包含约14.5万篇科学论文和114万张图片的SciGA-145k大型数据集，专门用于支持GA的选择、推荐及自动化生成研究。定义了两种GA推荐任务：论文内GA推荐和论文间GA推荐，并提供了基线模型。此外，提出了一种新的推荐评估指标CAR（Confidence Adjusted top-1 ground truth Ratio），以更精细地分析模型行为。

Result: 成功构建并发布了SciGA-145k数据集；定义了图形摘要推荐的两项核心任务，并提供了初步的基线模型；提出了一种能弥补传统排名指标局限性的新型评估指标CAR。

Conclusion: SciGA-145k数据集、定义的任务和提出的指标为推进视觉科学交流奠定了基础，并为科学AI的发展做出了贡献。

Abstract: Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific
papers and 1.14 million figures, explicitly designed for supporting GA
selection and recommendation as well as facilitating research in automated GA
generation. As a preliminary step toward GA design support, we define two
tasks: 1) Intra-GA recommendation, which identifies figures within a given
paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,
which retrieves GAs from other papers to inspire the creation of new GAs. We
provide reasonable baseline models for these tasks. Furthermore, we propose
Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation
metric that offers a fine-grained analysis of model behavior. CAR addresses
limitations in traditional ranking-based metrics by considering cases where
multiple figures within a paper, beyond the explicitly labeled GA, may also
serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a
foundation for advancing visual scientific communication while contributing to
the development of AI for Science.

</details>


### [24] [Understanding Trade offs When Conditioning Synthetic Data](https://arxiv.org/abs/2507.02217)
*Brandon Trabucco,Qasim Wani,Benjamin Pikus,Vasu Sharma*

Main category: cs.CV

TL;DR: 通过研究扩散模型的调节策略，发现基于布局的条件生成合成数据能显著提升目标检测性能，尤其在数据稀缺的工业视觉场景中。


<details>
  <summary>Details</summary>
Motivation: 工业视觉系统中高质量训练数据稀缺，现有3D引擎生成合成数据耗时且存在模拟-现实鸿沟。扩散模型虽能快速生成高质量图像，但在低数据量下难以精确控制，且不同调节方案对合成数据质量的影响尚不明确。

Method: 研究了来自四个标准目标检测基准的80个视觉概念，比较了两种扩散模型调节策略：基于文本提示(prompt-based)和基于布局(layout-based)。

Result: 在调节提示范围窄时，基于文本提示的合成数据质量更高；随着多样性增加，基于布局的调节方法更优。当布局提示与完整训练分布匹配时，合成数据可使平均精度（mAP）相对仅使用真实数据平均提高34%，最高达177%。

Conclusion: 基于布局的调节（尤其当其与训练分布匹配时）能有效利用扩散模型生成高质量合成数据，显著提升目标检测器的性能，为解决工业视觉系统中的数据稀缺问题提供了有力方案。

Abstract: Learning robust object detectors from only a handful of images is a critical
challenge in industrial vision systems, where collecting high quality training
data can take months. Synthetic data has emerged as a key solution for data
efficient visual inspection and pick and place robotics. Current pipelines rely
on 3D engines such as Blender or Unreal, which offer fine control but still
require weeks to render a small dataset, and the resulting images often suffer
from a large gap between simulation and reality. Diffusion models promise a
step change because they can generate high quality images in minutes, yet
precise control, especially in low data regimes, remains difficult. Although
many adapters now extend diffusion beyond plain text prompts, the effect of
different conditioning schemes on synthetic data quality is poorly understood.
We study eighty diverse visual concepts drawn from four standard object
detection benchmarks and compare two conditioning strategies: prompt based and
layout based. When the set of conditioning cues is narrow, prompt conditioning
yields higher quality synthetic data; as diversity grows, layout conditioning
becomes superior. When layout cues match the full training distribution,
synthetic data raises mean average precision by an average of thirty four
percent and by as much as one hundred seventy seven percent compared with using
real data alone.

</details>


### [25] [High-Fidelity Differential-information Driven Binary Vision Transformer](https://arxiv.org/abs/2507.02222)
*Tian Gao,Zhiyuan Zhang,Kaijie Yin,Xu-Cheng Zhong,Hui Kong*

Main category: cs.CV

TL;DR: ViT二值化面临性能下降，本文提出DIDB-ViT，通过引入信息性注意力模块、频率分解和改进的RPReLU激活函数，在保持原始ViT架构和计算效率的同时，显著提升了二值化ViT的性能，并在图像分类和分割任务中超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有ViT二值化方法普遍存在严重的性能下降或过度依赖全精度模块的问题，这限制了它们在计算/存储资源受限的边缘设备上的实际部署。

Method: 1. 设计了一个结合微分信息的“信息性注意力模块”，以减轻二值化导致的信息损失并增强高频信息保留。 2. 利用离散Haar小波进行频率分解，并整合不同频率间的相似度，以保持二值Q和K张量间相似度计算的保真度。 3. 引入了一种改进的RPReLU激活函数，以重构激活分布，从而扩展模型的表征能力。

Result: 实验结果表明，DIDB-ViT在多种ViT架构上显著优于最先进的网络量化方法，并在图像分类和分割任务中均取得了卓越的性能。

Conclusion: DIDB-ViT通过创新的模块和激活函数设计，有效解决了二值化ViT的性能退化问题，在保持原始ViT架构和计算效率的同时，实现了高性能的图像分类和分割，为ViT在资源受限环境下的部署提供了有效方案。

Abstract: The binarization of vision transformers (ViTs) offers a promising approach to
addressing the trade-off between high computational/storage demands and the
constraints of edge-device deployment. However, existing binary ViT methods
often suffer from severe performance degradation or rely heavily on
full-precision modules. To address these issues, we propose DIDB-ViT, a novel
binary ViT that is highly informative while maintaining the original ViT
architecture and computational efficiency. Specifically, we design an
informative attention module incorporating differential information to mitigate
information loss caused by binarization and enhance high-frequency retention.
To preserve the fidelity of the similarity calculations between binary Q and K
tensors, we apply frequency decomposition using the discrete Haar wavelet and
integrate similarities across different frequencies. Additionally, we introduce
an improved RPReLU activation function to restructure the activation
distribution, expanding the model's representational capacity. Experimental
results demonstrate that our DIDB-ViT significantly outperforms
state-of-the-art network quantization methods in multiple ViT architectures,
achieving superior image classification and segmentation performance.

</details>


### [26] [FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model](https://arxiv.org/abs/2507.02250)
*Jiangxia Chen,Tongyuan Huang,Ke Song*

Main category: cs.CV

TL;DR: 本文提出FMOcc，一个融合流匹配与选择性状态空间模型的三视图（TPV）精细化占用网络，旨在解决自动驾驶中少帧3D语义占用预测的精度问题，尤其针对遮挡和远距离场景，并在多个数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中3D语义占用预测面临少帧图像和3D空间冗余带来的精度挑战，尤其对遮挡和远距离场景。现有融合历史帧数据的方法计算资源需求大。

Method: 本文提出FMOcc网络：1) 设计流匹配SSM模块（FMSSM）以生成缺失特征；2) 通过TPV SSM层和平面选择性SSM（PS3M）选择性过滤TPV特征，提高模型效率和远距离预测能力；3) 引入掩码训练（MT）方法增强鲁棒性，应对传感器数据丢失。

Result: 实验结果显示，FMOcc在Occ3D-nuScenes和OpenOcc数据集上均优于现有SOTA方法。在Occ3D-nuScenes验证集上，两帧输入FMOcc达到43.1% RayIoU和39.8% mIoU。在OpenOcc上达到42.6% RayIoU，推理内存5.4 G，推理时间330ms。

Conclusion: FMOcc通过创新的模块设计，有效解决了少帧3D语义占用预测中的精度和效率问题，特别是提升了在遮挡和远距离场景的表现，并增强了模型鲁棒性，为自动驾驶提供了更强大的感知能力。

Abstract: 3D semantic occupancy prediction plays a pivotal role in autonomous driving.
However, inherent limitations of fewframe images and redundancy in 3D space
compromise prediction accuracy for occluded and distant scenes. Existing
methods enhance performance by fusing historical frame data, which need
additional data and significant computational resources. To address these
issues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement
occupancy network with flow matching selective state space model for few-frame
3D occupancy prediction. Firstly, to generate missing features, we designed a
feature refinement module based on a flow matching model, which is called Flow
Matching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and
Plane Selective SSM (PS3M), we selectively filter TPV features to reduce the
impact of air voxels on non-air voxels, thereby enhancing the overall
efficiency of the model and prediction capability for distant scenes. Finally,
we design the Mask Training (MT) method to enhance the robustness of FMOcc and
address the issue of sensor data loss. Experimental results on the
Occ3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing
state-of-theart methods. Our FMOcc with two frame input achieves notable scores
of 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on
OpenOcc with 5.4 G inference memory and 330ms inference time.

</details>


### [27] [SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement](https://arxiv.org/abs/2507.02252)
*Zeyu Lei,Hongyuan Yu,Jinlin Wu,Zhen Chen*

Main category: cs.CV

TL;DR: SurgVisAgent是一种基于多模态大语言模型（MLLM）的端到端智能手术视觉代理，能够动态识别并定制化处理多种内窥镜图像失真，在模拟真实世界失真的基准上超越了传统单任务模型。


<details>
  <summary>Details</summary>
Motivation: 当前的手术图像增强算法通常仅为特定场景下的单一任务设计，这限制了它们在复杂真实世界手术环境中的有效性和应用。

Method: 本文提出SurgVisAgent，一个基于多模态大语言模型（MLLM）的端到端智能手术视觉代理。SurgVisAgent能够动态识别内窥镜图像的失真类别和严重程度，并执行多种增强任务，如低光增强、过曝校正、运动模糊消除和烟雾去除。为实现卓越的手术场景理解，它设计了一个提供领域特定知识的先验模型，并通过上下文少样本学习和思维链（CoT）推理提供定制化的图像增强。此外，研究构建了一个模拟真实世界手术失真的综合基准。

Result: 在所构建的模拟真实世界手术失真的综合基准上，SurgVisAgent的表现显著优于传统的单任务模型。

Conclusion: SurgVisAgent展现了作为统一手术辅助解决方案的巨大潜力，有望在复杂手术环境中提供更全面的支持。

Abstract: Precise surgical interventions are vital to patient safety, and advanced
enhancement algorithms have been developed to assist surgeons in
decision-making. Despite significant progress, these algorithms are typically
designed for single tasks in specific scenarios, limiting their effectiveness
in complex real-world situations. To address this limitation, we propose
SurgVisAgent, an end-to-end intelligent surgical vision agent built on
multimodal large language models (MLLMs). SurgVisAgent dynamically identifies
distortion categories and severity levels in endoscopic images, enabling it to
perform a variety of enhancement tasks such as low-light enhancement,
overexposure correction, motion blur elimination, and smoke removal.
Specifically, to achieve superior surgical scenario understanding, we design a
prior model that provides domain-specific knowledge. Additionally, through
in-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent
delivers customized image enhancements tailored to a wide range of distortion
types and severity levels, thereby addressing the diverse requirements of
surgeons. Furthermore, we construct a comprehensive benchmark simulating
real-world surgical distortions, on which extensive experiments demonstrate
that SurgVisAgent surpasses traditional single-task models, highlighting its
potential as a unified solution for surgical assistance.

</details>


### [28] [Multi-Label Classification Framework for Hurricane Damage Assessment](https://arxiv.org/abs/2507.02265)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: 本研究提出一种基于航空影像的新型多标签分类框架，结合ResNet特征提取和注意力机制，用于精准评估飓风造成的多种损害，其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 飓风导致的灾害复杂多样，传统单标签分类方法无法准确捕捉灾后损害的复杂性，因此需要更有效、更及时的评估方法。

Method: 该研究引入了一个新颖的多标签分类框架，利用航空影像进行损害评估。其核心方法是整合了基于ResNet的特征提取模块和类别专属注意力机制，以在一张图像中识别多种损害类型。

Result: 使用来自飓风Michael的Rescuenet数据集进行测试，所提出的方法实现了90.23%的平均精度（mAP），其性能超越了现有的基线方法。

Conclusion: 该框架显著提升了飓风灾后损害评估的效率和准确性，有助于制定更具针对性和高效的灾害响应措施，并为未来的减灾和恢复策略提供有力支持。

Abstract: Hurricanes cause widespread destruction, resulting in diverse damage types
and severities that require timely and accurate assessment for effective
disaster response. While traditional single-label classification methods fall
short of capturing the complexity of post-hurricane damage, this study
introduces a novel multi-label classification framework for assessing damage
using aerial imagery. The proposed approach integrates a feature extraction
module based on ResNet and a class-specific attention mechanism to identify
multiple damage types within a single image. Using the Rescuenet dataset from
Hurricane Michael, the proposed method achieves a mean average precision of
90.23%, outperforming existing baseline methods. This framework enhances
post-hurricane damage assessment, enabling more targeted and efficient disaster
response and contributing to future strategies for disaster mitigation and
resilience. This paper has been accepted at the ASCE International Conference
on Computing in Civil Engineering (i3CE 2025), and the camera-ready version
will appear in the official conference proceedings.

</details>


### [29] [Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation](https://arxiv.org/abs/2507.02268)
*Yuxiang Zhang,Wei Li,Wen Jia,Mengmeng Zhang,Ran Tao,Shunlin Liang*

Main category: cs.CV

TL;DR: 提出BiDA框架，通过双向域适应处理高光谱图像跨域分类中的光谱漂移问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像（HSI）在不同区域或时间采集时，同一类别存在显著光谱漂移，导致跨域分类性能下降。需要一种方法来有效提取域不变特征和域特定信息，以增强模型对目标场景的适应性和可分性。

Method: 提出BiDA（Bi-directional Domain Adaptation）框架，其核心包括：1. 一个三分支（源分支、目标分支、耦合分支）Transformer架构作为骨干网络，并辅以语义分词器。2. 源分支和目标分支独立学习源域和目标域的自适应空间。3. 在耦合分支中开发耦合多头交叉注意力（CMCA）机制，用于特征交互和域间相关性挖掘。4. 设计双向蒸馏损失，以域间相关性指导自适应空间学习。5. 提出自适应强化策略（ARS），鼓励模型在噪声条件下聚焦于源和目标场景的特定泛化特征提取。

Result: 在跨时序/场景机载和卫星数据集上的实验结果表明，所提出的BiDA框架显著优于一些最先进的域适应方法。在跨时序树种分类任务中，BiDA的性能比最先进的方法提高了3%~5%。

Conclusion: BiDA框架有效解决了高光谱图像跨域分类中的光谱漂移问题，通过其独特的三分支Transformer架构、CMCA机制、双向蒸馏损失和ARS策略，显著提升了模型的适应性和分类性能，为高光谱图像跨域应用提供了先进的解决方案。

Abstract: Utilizing hyperspectral remote sensing technology enables the extraction of
fine-grained land cover classes. Typically, satellite or airborne images used
for training and testing are acquired from different regions or times, where
the same class has significant spectral shifts in different scenes. In this
paper, we propose a Bi-directional Domain Adaptation (BiDA) framework for
cross-domain hyperspectral image (HSI) classification, which focuses on
extracting both domain-invariant features and domain-specific information in
the independent adaptive space, thereby enhancing the adaptability and
separability to the target scene. In the proposed BiDA, a triple-branch
transformer architecture (the source branch, target branch, and coupled branch)
with semantic tokenizer is designed as the backbone. Specifically, the source
branch and target branch independently learn the adaptive space of source and
target domains, a Coupled Multi-head Cross-attention (CMCA) mechanism is
developed in coupled branch for feature interaction and inter-domain
correlation mining. Furthermore, a bi-directional distillation loss is designed
to guide adaptive space learning using inter-domain correlation. Finally, we
propose an Adaptive Reinforcement Strategy (ARS) to encourage the model to
focus on specific generalized feature extraction within both source and target
scenes in noise condition. Experimental results on cross-temporal/scene
airborne and satellite datasets demonstrate that the proposed BiDA performs
significantly better than some state-of-the-art domain adaptation approaches.
In the cross-temporal tree species classification task, the proposed BiDA is
more than 3\%$\sim$5\% higher than the most advanced method. The codes will be
available from the website:
https://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA.

</details>


### [30] [MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement](https://arxiv.org/abs/2507.02270)
*Fanghai Yi,Zehong Zheng,Zexiao Liang,Yihang Dong,Xiyang Fang,Wangyu Wu,Xuhang Chen*

Main category: cs.CV

TL;DR: 提出MAC-Lookup模型，通过改善色彩和细节，显著增强水下图像质量。


<details>
  <summary>Details</summary>
Motivation: 水下图像受光线变化、水浊度和气泡影响，存在可见性和颜色问题；传统方法和现有深度学习方法在处理这些问题时存在局限性。

Method: 提出Multi-Axis Conditional Lookup (MAC-Lookup) 模型，包含用于初步颜色和质量校正的Conditional 3D Lookup Table Color Correction (CLTCC)，以及用于细节优化的Multi-Axis Adaptive Enhancement (MAAE)，旨在防止过度增强和饱和。

Result: 大量实验表明，MAC-Lookup模型在恢复水下图像的细节和颜色方面，优于现有方法。

Conclusion: MAC-Lookup模型能够有效解决水下图像的质量问题，显著提升其视觉效果和探索价值。

Abstract: Enhancing underwater images is crucial for exploration. These images face
visibility and color issues due to light changes, water turbidity, and bubbles.
Traditional prior-based methods and pixel-based methods often fail, while deep
learning lacks sufficient high-quality datasets. We introduce the Multi-Axis
Conditional Lookup (MAC-Lookup) model, which enhances visual quality by
improving color accuracy, sharpness, and contrast. It includes Conditional 3D
Lookup Table Color Correction (CLTCC) for preliminary color and quality
correction and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.
This model prevents over-enhancement and saturation while handling underwater
challenges. Extensive experiments show that MAC-Lookup excels in enhancing
underwater images by restoring details and colors better than existing methods.
The code is https://github.com/onlycatdoraemon/MAC-Lookup.

</details>


### [31] [Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation](https://arxiv.org/abs/2507.02271)
*Feizhen Huang,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: 现有V2A方法忽略电影语言导致部分可见场景性能不佳，本文提出一种自蒸馏方法，通过模拟电影语言变化，使模型能处理部分可见视觉信息，显著提升了V2A生成在部分可见场景下的性能，并优化了在VGGSound数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: V2A生成在电影和视频后期制作中至关重要。然而，当前方法忽略了电影语言这一艺术表达的关键组成部分，导致在Foley目标仅部分可见的场景中性能下降。因此，研究旨在解决现有V2A模型在此类挑战性场景中的局限性。

Method: 本文提出一种简单的自蒸馏方法，旨在将V2A模型扩展到电影语言场景。通过模拟电影语言的变化，学生模型学习对齐具有相同视听对应关系的训练对的视频特征，从而使其能够有效捕捉声音与部分视觉信息之间的关联。

Result: 该方法不仅在部分可见性场景下，所有评估指标上均取得了显著改进，而且还提升了在大型V2A数据集VGGSound上的性能。

Conclusion: 所提出的自蒸馏方法有效解决了V2A模型在处理电影语言和部分可见目标时的性能瓶颈。其在多种场景下的优异表现证明了该方法对于提升V2A生成鲁棒性和实用性的潜力。

Abstract: Video-to-Audio (V2A) Generation achieves significant progress and plays a
crucial role in film and video post-production. However, current methods
overlook the cinematic language, a critical component of artistic expression in
filmmaking. As a result, their performance deteriorates in scenarios where
Foley targets are only partially visible. To address this challenge, we propose
a simple self-distillation approach to extend V2A models to cinematic language
scenarios. By simulating the cinematic language variations, the student model
learns to align the video features of training pairs with the same audio-visual
correspondences, enabling it to effectively capture the associations between
sounds and partial visual information. Our method not only achieves impressive
improvements under partial visibility across all evaluation metrics, but also
enhances performance on the large-scale V2A dataset, VGGSound.

</details>


### [32] [LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2507.02279)
*Juntao Liu,Liqiang Niu,Wenchao Chen,Jie Zhou,Fandong Meng*

Main category: cs.CV

TL;DR: 该论文提出LaCo（层级视觉token压缩）框架，在视觉编码器中间层进行有效token压缩，显著提升多模态大语言模型（MLLMs）的训练和推理效率，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有用于多模态大语言模型（MLLMs）的视觉token压缩方法主要作为编码器后置模块运行，限制了其效率提升潜力。

Method: 提出LaCo框架，包含两个核心组件：1) 层级像素重排机制，通过空间到通道转换系统地合并相邻token；2) 带有非参数快捷方式的残差学习架构，以在压缩过程中保留关键视觉信息。

Result: LaCo在视觉编码器中间层压缩token时，表现优于所有现有方法。与外部压缩相比，在保持强劲性能的同时，训练效率提升超过20%，推理吞吐量提升超过15%。

Conclusion: LaCo通过在视觉编码器中间层进行有效的token压缩，显著提升了MLLMs的效率（训练和推理），同时保持了高性能，证明了其优越性。

Abstract: Existing visual token compression methods for Multimodal Large Language
Models (MLLMs) predominantly operate as post-encoder modules, limiting their
potential for efficiency gains. To address this limitation, we propose LaCo
(Layer-wise Visual Token Compression), a novel framework that enables effective
token compression within the intermediate layers of the vision encoder. LaCo
introduces two core components: 1) a layer-wise pixel-shuffle mechanism that
systematically merges adjacent tokens through space-to-channel transformations,
and 2) a residual learning architecture with non-parametric shortcuts that
preserves critical visual information during compression. Extensive experiments
indicate that our LaCo outperforms all existing methods when compressing tokens
in the intermediate layers of the vision encoder, demonstrating superior
effectiveness. In addition, compared to external compression, our method
improves training efficiency beyond 20% and inference throughput over 15% while
maintaining strong performance.

</details>


### [33] [Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization](https://arxiv.org/abs/2507.02288)
*De Cheng,Zhipeng Xu,Xinyang Jiang,Dongsheng Li,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出一种基于VFM的文本特征引导视觉提示调优框架，通过LLM解耦文本提示来学习域不变视觉表示，并引入WERA利用抽象提示和风格化增强进一步提升特征解耦能力，实现超越SOTA的域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 在基于视觉基础模型（VFM）的域泛化（DG）中，有效设计提示以解耦跨域不变特征仍是一个关键挑战。

Method: 本文提出一种 novel 框架：1. 利用大型语言模型（LLM）自动解耦VFM的文本提示。2. 使用解耦后的文本特征指导视觉提示调优，以学习域不变视觉表示。3. 引入“最差显式表示对齐”（Worst Explicit Representation Alignment, WERA），通过整合额外的抽象提示来增强文本引导的视觉提示，并利用风格化图像增强提升源域多样性，同时通过对齐约束确保视觉表示在原始和增强分布之间的一致性。

Result: 在PACS、VLCS、OfficeHome、DomainNet和TerraInc等主要域泛化数据集上进行的实验表明，本文提出的方法优于现有最先进的域泛化方法。

Conclusion: 本文提出的文本特征引导视觉提示调优框架结合WERA机制，有效解决了VFM在域泛化中不变特征解耦的挑战，显著提升了模型在未见域上的泛化性能。

Abstract: Domain Generalization (DG) seeks to develop a versatile model capable of
performing effectively on unseen target domains. Notably, recent advances in
pre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated
considerable potential in enhancing the generalization capabilities of deep
learning models. Despite the increasing attention toward VFM-based domain
prompt tuning within DG, the effective design of prompts capable of
disentangling invariant features across diverse domains remains a critical
challenge. In this paper, we propose addressing this challenge by leveraging
the controllable and flexible language prompt of the VFM. Noting that the text
modality of VFMs is naturally easier to disentangle, we introduce a novel
framework for text feature-guided visual prompt tuning. This framework first
automatically disentangles the text prompt using a large language model (LLM)
and then learns domain-invariant visual representation guided by the
disentangled text feature. However, relying solely on language to guide visual
feature disentanglement has limitations, as visual features can sometimes be
too complex or nuanced to be fully captured by descriptive text. To address
this, we introduce Worst Explicit Representation Alignment (WERA), which
extends text-guided visual prompts by incorporating an additional set of
abstract prompts. These prompts enhance source domain diversity through
stylized image augmentations, while alignment constraints ensure that visual
representations remain consistent across both the original and augmented
distributions. Experiments conducted on major DG datasets, including PACS,
VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method
outperforms state-of-the-art DG methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA是一个自进化的AI智能体，通过动态更新推理策略和自动发现新工具来克服生物医学领域数据碎片化问题，并在多个基准测试中表现出色且随经验增长而提升。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据、工具和文献的快速增长导致研究领域碎片化，超出了人类专业知识的应对能力。现有AI智能体依赖静态工具集，限制了其适应性和可扩展性。

Method: 引入STELLA，一个自进化的AI智能体。它采用多智能体架构，通过两种核心机制自主提升能力：一个不断演进的模板库（用于推理策略）和一个动态工具海洋（由工具创建智能体自动发现和集成新生物信息学工具），从而实现从经验中学习。

Result: STELLA在生物医学基准测试中达到最先进的准确性，在“人类最后一次考试：生物医学”中得分约26%，在LAB-Bench: DBQA中得54%，在LAB-Bench: LitQA中得63%，领先其他模型高达6个百分点。更重要的是，其性能随经验系统性提高，例如在“人类最后一次考试”基准上，准确率随试验次数增加几乎翻倍。

Conclusion: STELLA代表了AI智能体系统的重要进展，它能够学习和成长，动态扩展其专业知识，从而加速生物医学发现的步伐。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [35] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: 提出了一种轻量级的基于规则的混合特征选择方法HCVR，通过结合参数间和参数-目标相关性来消除冗余特征和保留相关特征。


<details>
  <summary>Details</summary>
Motivation: 旨在解决数据高维度问题，通过有效选择特征以提高模型性能，超越传统特征选择方法的局限性。

Method: HCVR是一种混合了非迭代和迭代过滤策略的贪婪型后向消除方法。它利用参数间(P2P)和参数-目标(P2T)相关性阈值，通过规则投票机制，以多数票决定特征的去留，每步可消除多个特征。

Result: 将HCVR应用于SPAMBASE数据集，结果表明其性能优于传统的非迭代（如CFS、mRMR、MI）和迭代（如RFE、SFS、遗传算法）技术。有效性通过过滤后不同分类器的性能进行评估。

Conclusion: HCVR是一种有效且性能优越的特征选择方法，能够显著提升降维后分类器的性能，优于现有多种传统技术。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [36] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 本综述旨在通过审查高效测试时计算（TTC）策略来提高大型语言模型（LLMs）推理的计算效率，提出分类法，并讨论性能与资源权衡及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在推理时计算效率低下，无论任务复杂性如何都应用固定计算量，导致简单问题过度思考，复杂问题思考不足。

Method: 本综述对高效测试时计算（TTC）策略进行了全面回顾，并引入了一个两层分类法：L1-可控性（固定计算预算）和L2-适应性（动态扩展推理）。同时，它还对主流专有LLMs在不同数据集上进行了基准测试。

Result: 研究结果突出了推理性能与token使用量之间的关键权衡。与以往综述相比，本综述更强调TTC方法的实际控制、适应性和可扩展性。

Conclusion: 未来的工作应关注混合思维模型等新兴趋势，并解决关键挑战，以使LLMs在计算上更高效、更鲁棒、更能响应用户约束。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [37] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: 引入SciGym，一个评估大型语言模型（LLM）实验设计与分析能力的基准，通过生物系统“干实验室”克服湿实验室成本。评估结果显示，尽管更强大的LLM表现更优，但所有模型在系统复杂性增加时性能显著下降，表明LLM的科学能力仍有巨大提升空间。


<details>
  <summary>Details</summary>
Motivation: 实验设计与结果解读是核心科学能力，但在评估大型语言模型（LLM）的科学能力时，由于湿实验室实验成本过高（包括专业知识、时间、设备），未能有效测试这些关键能力。

Method: 引入SciGym，一个评估LLM在开放式科学发现任务中迭代实验设计和分析能力的基准。SciGym通过运行生物系统“干实验室”（模型以系统生物学标记语言编码）来高效生成模拟数据，从而规避了湿实验室的成本。研究团队在137个小型系统上评估了六个前沿LLM，并总共发布了350个系统。

Result: 评估显示，尽管更强大的LLM展现出更优越的性能，但所有LLM的性能都随着系统复杂性的增加而显著下降。

Conclusion: LLM代理的科学能力仍有巨大的改进空间，尤其是在处理复杂系统时。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [38] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 探讨AI与动物在适应性学习上的差异，提出神经科学可为AI持续学习与情境学习提供启发，共同推动NeuroAI发展。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型（如大型语言模型）通常训练一次后参数固定，难以像动物一样持续快速适应动态环境。然而，现实世界的AI应用（如机器人、自动驾驶、智能体）急需这种适应能力，这促使研究者思考AI能否从神经科学中学习以弥补这一差距。

Method: 本文是一篇“视角”（Perspective）论文，通过整合AI领域（如持续学习和情境学习）与神经科学领域（关于动物在规则、奖励概率或结果变化任务中的学习）的现有文献，进行跨学科的探讨与分析，旨在提出一个研究议程。

Result: 本文没有呈现具体的实验结果，而是提出了一个研究议程。该议程旨在探讨神经科学的见解如何具体地指导AI当前在适应性学习方面的发展，反之，AI又能从神经科学中学到什么，从而共同推动新兴的神经AI（NeuroAI）领域。

Conclusion: 神经科学可以为AI系统提供关于持续适应和情境学习的重要启发，以应对真实世界中的动态变化；同时，AI的发展也能为神经科学提供新的研究视角。这种双向交流将共同促进神经AI领域的演进。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [39] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 自动化AI系统（如招聘）的公平性评估面临挑战，现有机器学习方法因数据偏差导致评估不准。本文利用社会科学中的审计研究高质量数据，揭示现有公平性干预方法的不足，并提出基于个体处理效应的新干预措施，以更准确地训练和评估算法并减少歧视。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统，特别是机器学习系统，被广泛应用于招聘、贷款等复杂决策自动化领域。然而，评估这些AI系统及其人类对应决策的有效性和公平性是一个复杂且重要的问题。当前机器学习中常用的公平性偏差处理方法（如重采样训练数据）在评估时常依赖便捷样本，引入了选择偏差和标签偏差，导致评估结果不可靠。因此，需要高质量的数据和更准确的方法来评估和改进AI系统的公平性。

Method: 本文利用社会科学中审计研究（Audit Studies）提供的高质量数据。审计研究通过随机对照试验，使用虚构的“测试者”（如简历、邮件）向研究对象（如招聘职位、医生）发送，从而获得高质量的歧视评估数据。我们将这些审计数据用于自动化招聘算法的训练和评估。具体方法包括：使用审计数据重新评估现有机器学习中常见的公平性干预措施（例如，通过均衡训练集中的基础比率）；并引入基于个体处理效应（Individual Treatment Effect）估计方法的新型干预措施来减少算法歧视。

Result: 研究发现，使用审计数据评估时，常见的公平性干预方法（如均衡类别基础比率）在使用传统指标时表面上达到了公平，但若采用恰当的测量方法，实际上仍存在约10%的歧视差距。此外，本文引入的基于个体处理效应估计方法的新干预措施，能进一步有效降低算法歧视。

Conclusion: 审计研究数据能够显著提升自动化招聘算法的训练和评估能力。它不仅揭示了现有公平性干预措施在真实场景下的局限性，还验证了基于个体处理效应的新方法在减少算法歧视方面的有效性。这强调了高质量、无偏数据在实现AI公平性中的关键作用，并为未来AI公平性研究提供了新的视角和方法。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [40] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 研究通过数据多样化策略提升大型语言模型（LLMs）的数学推理能力，提出DTS方法，在提高性能的同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管偏好学习在人类反馈对齐方面取得了进展，但LLMs的数学推理能力仍然是一个持续的挑战。本研究旨在通过数据多样化策略来改进LLMs的数学推理表现。

Method: 研究评估了三种数据生成方法：温度采样、思维链提示（CoT）和蒙特卡洛树搜索（MCTS）。此外，提出了一种新颖的结构化方法——多样化思考解决（Diversified-ThinkSolve, DTS），该方法系统地将问题分解为多样化的推理路径。

Result: 结果显示，通过策略性多样化的偏好数据，模型可以显著提升数学推理性能。其中，最佳方法在GSM8K上取得了7.1%的提升，在MATH上取得了4.2%的提升（相对于基线模型）。DTS的计算开销仅为基线的1.03倍，而MCTS成本更高但收益较低。

Conclusion: 研究表明，通过结构化探索多样化的解题方法（如DTS）可以生成比传统方法更有效的偏好数据，从而更好地对齐LLMs的数学推理能力。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [41] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 研究发现，在角色扮演模拟中，大型语言模型（LLMs）所表达的信念与其实际行为存在系统性不一致，这限制了它们在人类行为研究中作为代理的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被广泛用作角色扮演代理来生成人类行为研究的合成数据，确保其输出与所扮演角色保持一致性变得至关重要。本研究旨在探讨LLM代理所声称的信念与其在角色扮演中的实际行为之间的一致性。

Method: 建立了一个评估框架，以衡量通过提示模型获得的信念预测模拟结果的准确性。利用增强版GenAgents角色库和信任博弈（Trust Game），引入了信念-行为一致性指标。系统研究了信念类型、信息呈现方式及预测时间跨度等因素对一致性的影响，并探讨了施加研究者理论先验的可行性。

Result: 研究结果显示，LLMs所表达（或施加）的信念与其角色扮演模拟结果之间存在系统性不一致，这在个体和群体层面均有体现。即使模型似乎编码了合理的信念，它们也可能未能以一致的方式应用这些信念。

Conclusion: 这些发现强调了识别LLMs所表达信念何时与模拟行为保持一致的重要性，从而指导研究人员在行为研究中恰当地使用基于LLM的代理。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [42] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 本研究利用多智能体Q-learning算法，探讨了稀释和移动性在空间囚徒困境中对合作的影响，发现固定更新规则与学习规则可实现定性等效，且多动作设定下能产生群体间的共生互惠效应。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，静态智能体能通过强化学习在空间囚徒困境中实现合作。本研究旨在进一步探索稀释和移动性这两个因素在基于强化学习的空间囚徒困境中如何影响合作行为，以拓展该领域的理解。

Method: 采用独立的、多智能体Q-learning算法进行模拟。在此框架下，定义了算法的不同可能动作，并将其与经典的非强化学习空间囚徒困境结果进行关联，以展示算法在建模不同博弈论场景中的通用性和基准潜力。

Result: 观察到多种效应，主要包括：1) 具有固定更新规则的博弈与具有学习更新规则的博弈可以实现定性上的等效；2) 当定义了多个动作时，群体之间会形成一种共生互惠效应。

Conclusion: 本研究通过Q-learning算法成功模拟了稀释和移动性对空间囚徒困境的影响，验证了该算法在建模复杂博弈场景中的多功能性。研究结果揭示了固定规则与学习规则在某些情况下的等效性，并发现多动作设定下群体间能涌现出共生互惠关系，为理解合作演化提供了新视角。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [43] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: 为解决大语言模型（LLM）规划和推理能力提升中数据生成与评估的瓶颈，本文提出了NL2FLOW系统，用于自动化生成规划问题并严格评估生成计划。研究发现，LLM在生成有效计划方面表现良好，但引入中间翻译步骤可能降低性能，提示直接推理的益处。


<details>
  <summary>Details</summary>
Motivation: 当前LLM规划和推理能力的发展受限于可扩展且可靠的数据生成和评估瓶颈，亟需解决方案来克服这一挑战。

Method: 引入NL2FLOW系统，这是一个全自动系统，能参数化生成自然语言、结构化中间表示和PDDL形式的规划问题，并严格评估所生成计划的质量。作者利用NL2FLOW生成了一个包含2296个自动化工作流问题的数据集，并评估了多个开源的、经过指令微调的LLM。

Result: 表现最佳的LLM在可行问题上，生成有效计划的成功率为86%，生成最优计划的成功率为69%。回归分析表明，问题特性对计划生成的影响取决于模型和提示设计。值得注意的是，将自然语言翻译成JSON计划的成功率低于直接生成有效计划的成功率，这表明不必要的推理任务分解（引入中间翻译步骤）可能反而降低性能。

Conclusion: 随着LLM推理能力扩展到更复杂问题，其系统中的瓶颈和错误来源会动态变化。因此，对这些限制的动态理解以及系统性揭示这些限制的工具（如NL2FLOW）对于充分发挥LLM作为智能问题解决者的潜力至关重要。研究也暗示，模型直接从自然语言进行推理可能比经过中间翻译步骤更有效。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [44] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 本文提出一种新的信念修正机制分析视角，侧重于其“能力”（即能达到的信念状态）而非传统“约束”，并证明不同机制拥有不同的能力。


<details>
  <summary>Details</summary>
Motivation: 信念修正领域现有研究多通过公设定义修正机制的“约束”（必须如何），却忽视了其“能力”（能达到何种信念状态）。然而，许多实际应用需要修正机制能够达到各种特定的信念状态（如可塑性、平等性、教条性），这是一种未被充分关注的“能力”。因此，研究旨在弥补这一空白，将分析重点从“约束”转向“能力”。

Method: 论文通过定义一系列信念修正机制的“能力”（例如可塑性、平等性、教条性、遗忘性等），并对多种现有修正机制（包括词典式、自然式、激进式、严重式等）进行理论分析和证明，以明确每种机制所具备的特定能力。

Result: 研究证明，不同的信念修正机制（如词典式、自然式、激进式、严重式及其变体）拥有特定的“能力”组合，但也缺乏其他能力。这表明各种现有机制在达成不同信念状态方面表现出差异性。

Conclusion: 传统的信念修正分析未能充分考量修正机制达到特定信念状态的“能力”。本研究通过引入并分析这些“能力”，为评估和选择信念修正机制提供了一个更全面的新框架，强调了在实际应用中理解和利用这些能力的重要性。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [45] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: 本文提出OMS，一个创新的关键词生成框架，用于赞助搜索广告。该框架克服了现有LLM方法对数据的依赖、缺乏在线多目标优化以及弱质量控制等问题，通过即时、多目标和自反思机制，显著提升了关键词性能和自动化水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的关键词生成方法存在三大局限性：过度依赖大规模查询-关键词对数据、缺乏在线多目标性能监控与优化、以及关键词选择的质量控制薄弱。这些问题阻碍了LLM在广告关键词决策中实现完全自动化和智能代理能力，特别是在监控和优化印象、点击、转化率和CTA效果等关键绩效指标方面。

Method: 本文提出OMS（On-the-fly, Multi-objective, Self-reflective）关键词生成框架。OMS的特点包括：1) 即时性（无需训练数据，监控在线性能并相应调整）；2) 多目标性（采用代理推理，基于多项性能指标优化关键词）；3) 自反思性（代理式评估关键词质量）。

Result: 实验结果显示，OMS在基准测试和真实广告活动中均优于现有方法。消融实验和人工评估进一步证实了每个组件的有效性以及所生成关键词的质量。

Conclusion: OMS框架通过其即时、多目标和自反思特性，成功克服了现有LLM方法在赞助搜索广告关键词生成中的局限性，实现了卓越的性能提升和关键词质量，为广告决策自动化提供了有效方案。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [46] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: 本文提出了一个AI原生自主实验室，能独立进行复杂生物分子工程实验，并达到人类科学家水平的性能，支持多用户和多仪器场景。


<details>
  <summary>Details</summary>
Motivation: 目前的自主实验系统局限于单一目标和简单流程，无法应对复杂科学实验（如生物分子工程）的需求，且服务于非专业人士的愿景尚未实现。亟需AI驱动的范式转变来克服专家依赖和资源障碍。

Method: 构建了一个AI原生自主实验室，其核心方法是模型、实验和仪器协同设计。该系统能自主管理仪器、制定实验特异性程序和优化启发式算法，并同时处理多用户请求，实现AI模型与自动化系统的共同演进。

Result: 该自主实验室成功实现了端到端、多用户的复杂多目标实验，涵盖多种仪器，支持核酸合成、转录、扩增和测序等基础功能，并应用于疾病诊断、药物开发和信息存储。实验性能经自主优化可达到人类科学家的顶尖水平，且在多用户场景下显著提升了仪器利用率和实验效率。

Conclusion: 该平台为先进生物材料研究克服专家依赖和资源限制开辟了道路，并为大规模的“科学即服务”模式奠定了蓝图。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [47] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 通过范畴论重构机器学习模型，特别是监督学习，以提供一个语义框架，阐明参数与残差的结构关系，并为AI可解释性奠定形式化基础。


<details>
  <summary>Details</summary>
Motivation: 提高机器学习的可理解性和可解释性，以满足AI可解释性原则的需求，并促进AI在社会中的更好实施。

Method: 研究通过范畴论的视角重构机器学习模型，旨在为AI系统开发一个语义框架。具体地，针对监督学习中的多元线性回归模型，定义了对应参数和数据的两个具体范畴，并引入它们之间的伴随函子对，从而提出监督学习的范畴化公式，并称其核心结构为“高斯-马尔可夫伴随”。

Result: 在此范畴论框架内，信息双向流（参数和残差变化间的对应关系）能够被明确描述。参数的普通最小二乘估计和最小残差通过右伴随函子对极限的保持性相关联。

Conclusion: 本研究将所提出的公式化视为监督学习扩展指称语义的一个实例，并提议将理论计算机科学中发展的语义视角作为AI可解释性的一种形式化基础。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows](https://arxiv.org/abs/2507.01975)
*Mengtao Yan,Qi Wang,Haining Wang,Ruizhi Chengze,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Qi Qi,Hao Sun*

Main category: cs.LG

TL;DR: 该论文提出LDSolver，一种可学习可微分的有限体积求解器，旨在用粗网格高效准确地模拟流体，克服了传统方法的高计算成本和现有机器学习方法在可解释性、泛化性及数据依赖性上的局限，并在有限数据下实现了最先进的性能和卓越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 流体模拟对多种物理现象至关重要，但传统数值求解器需要细密网格，导致计算成本高昂。现有机器学习方法虽提升了效率，却面临可解释性、泛化性和数据依赖性等问题。因此，需要一种既高效又准确，且能克服这些局限性的流体模拟新方法。

Method: 本文提出LDSolver，一个可学习可微分的有限体积求解器，用于在时空粗网格上高效准确地模拟流体。LDSolver由两部分组成：1)一个可微分的有限体积求解器；2)一个可学习模块，用于在粗网格上对通量（导数和插值）提供等效近似，并进行时间误差校正。

Result: 即使在有限的训练数据下，LDSolver也能加速模拟，同时保持高精度和卓越的泛化能力。在多种流体系统（如Burgers流、衰减流、受迫流和剪切流）上的实验表明，LDSolver实现了最先进的性能，显著优于基线模型。

Conclusion: LDSolver成功提供了一种高效、准确且泛化性强的流体模拟解决方案，有效解决了传统方法计算成本高昂以及现有机器学习方法在可解释性、泛化性与数据依赖性方面的局限性，并在多种复杂流体系统上验证了其卓越的性能。

Abstract: Simulation of fluid flows is crucial for modeling physical phenomena like
meteorology, aerodynamics, and biomedicine. Classical numerical solvers often
require fine spatiotemporal grids to satisfy stability, consistency, and
convergence conditions, leading to substantial computational costs. Although
machine learning has demonstrated better efficiency, they typically suffer from
issues of interpretability, generalizability, and data dependency. Hence, we
propose a learnable and differentiable finite volume solver, called LDSolver,
designed for efficient and accurate simulation of fluid flows on spatiotemporal
coarse grids. LDSolver comprises two key components: (1) a differentiable
finite volume solver, and (2) an learnable module providing equivalent
approximation for fluxes (derivatives and interpolations), and temporal error
correction on coarse grids. Even with limited training data (e.g., only a few
trajectories), our model could accelerate the simulation while maintaining a
high accuracy with superior generalizability. Experiments on different flow
systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver
achieves state-of-the-art performance, surpassing baseline models with notable
margins.

</details>


### [49] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long,Xiangzhi Huang,Jiemin Xie,Ming Cai*

Main category: cs.LG

TL;DR: 本文提出DKGCM图卷积网络，通过创新的时空建模（基于时间相似度的聚类图卷积、FFT与双向Mamba）和强化学习优化，显著提升交通需求预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有交通需求预测模型受限于复杂的时空关系，性能不足，而准确预测对于交通资源有效分配至关重要。

Method: 提出DKGCM图卷积网络。空间维度上，利用动态时间规整（DTW）和K-means聚类，提出基于时间相似度的聚类图卷积方法DK-GCN以捕获空间依赖。时间维度上，将快速傅里叶变换（FFT）整合到双向Mamba深度学习框架中以捕获时间依赖。此外，引入GRPO强化学习策略以优化模型训练的损失函数反馈机制。

Result: 在三个公共数据集上进行了大量实验，结果表明所提出的模型性能优于多个先进方法，并取得了优异的预测结果。

Conclusion: DKGCM模型通过有效处理交通数据的复杂时空关系并优化训练过程，显著提高了交通需求预测的准确性。

Abstract: Accurate traffic demand forecasting enables transportation management
departments to allocate resources more effectively, thereby improving their
utilization efficiency. However, complex spatiotemporal relationships in
traffic systems continue to limit the performance of demand forecasting models.
To improve the accuracy of spatiotemporal traffic demand prediction, we propose
a new graph convolutional network structure called DKGCM. Specifically, we
first consider the spatial flow distribution of different traffic nodes and
propose a novel temporal similarity-based clustering graph convolution method,
DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering
to group traffic nodes and more effectively capture spatial dependencies. On
the temporal scale, we integrate the Fast Fourier Transform (FFT) within the
bidirectional Mamba deep learning framework to capture temporal dependencies in
traffic demand. To further optimize model training, we incorporate the GRPO
reinforcement learning strategy to enhance the loss function feedback
mechanism. Extensive experiments demonstrate that our model outperforms several
advanced methods and achieves strong results on three public datasets.

</details>


### [50] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: 本研究通过融合文本、图像和社交等多模态特征，并结合无监督和有监督机器学习模型，提高了社交媒体上虚假信息的检测性能，并分析了其传播模式。


<details>
  <summary>Details</summary>
Motivation: 社交媒体在选举和危机期间虚假信息泛滥，现有检测研究多集中于单一模态。为填补多模态特征（尤其是整合文本、图像和社交特征）应用不足的空白，本研究旨在探究其在虚假信息检测中的有效性。

Method: 研究分析了1,529条包含文本和图像的推文（来自COVID-19大流行和选举期间），通过数据富集技术提取了社交特征和视觉特征（如物体检测和OCR）。采用早期融合方法整合多模态特征，并结合无监督和有监督机器学习模型构建分类模型。

Result: 结合无监督和有监督机器学习模型，分类性能比单模态模型提高了15%，比双模态模型提高了5%。此外，研究还分析了基于虚假信息推文特征和传播用户特征的传播模式。

Conclusion: 整合文本、图像和社交等多模态特征，并结合无监督和有监督机器学习模型，能显著提高社交媒体虚假信息检测的性能。本研究也揭示了虚假信息传播的模式。

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [51] [Positive region preserved random sampling: an efficient feature selection method for massive data](https://arxiv.org/abs/2507.01998)
*Hexiang Bai,Deyu Li,Jiye Liang,Yanhui Zhai*

Main category: cs.LG

TL;DR: 针对海量数据特征选择中计算资源有限的问题，本文提出了一种基于采样技术和粗糙集理论的新方法，该方法能够高效且有效地选择出保持高区分能力的特征子集。


<details>
  <summary>Details</summary>
Motivation: 智能机器在面对海量数据时，特征选择是成功的重要步骤，但其通常计算资源不足，难以有效进行特征选择。

Method: 本研究提出一种基于采样技术和粗糙集理论的特征选择新方法。该方法通过引入可区分对象对比例来衡量特征集的区分能力，并从海量数据中构建正域保持样本，从而找到具有高区分能力的特征子集。

Result: 所提方法能在个人电脑上于可接受时间内选择出保持原始数据集区分能力的特征子集，且在寻找核约简前能估计所选特征子集可区分对象对的概率下限。在11个不同大小数据集上的实验表明能快速找到近似约简，且区分能力高于估计下限。在4个大规模数据集上的实验也证明了在个人电脑上能高效获得高区分能力的近似约简。

Conclusion: 该方法有效解决了海量数据背景下计算资源有限的特征选择挑战，能够高效且有效地选出具有高区分能力的特征子集，并在不同规模数据集上展现出优越性能。

Abstract: Selecting relevant features is an important and necessary step for
intelligent machines to maximize their chances of success. However, intelligent
machines generally have no enough computing resources when faced with huge
volume of data. This paper develops a new method based on sampling techniques
and rough set theory to address the challenge of feature selection for massive
data. To this end, this paper proposes using the ratio of discernible object
pairs to all object pairs that should be distinguished to measure the
discriminatory ability of a feature set. Based on this measure, a new feature
selection method is proposed. This method constructs positive region preserved
samples from massive data to find a feature subset with high discriminatory
ability. Compared with other methods, the proposed method has two advantages.
First, it is able to select a feature subset that can preserve the
discriminatory ability of all the features of the target massive data set
within an acceptable time on a personal computer. Second, the lower boundary of
the probability of the object pairs that can be discerned using the feature
subset selected in all object pairs that should be distinguished can be
estimated before finding reducts. Furthermore, 11 data sets of different sizes
were used to validate the proposed method. The results show that approximate
reducts can be found in a very short period of time, and the discriminatory
ability of the final reduct is larger than the estimated lower boundary.
Experiments on four large-scale data sets also showed that an approximate
reduct with high discriminatory ability can be obtained in reasonable time on a
personal computer.

</details>


### [52] [Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2507.01999)
*Bappaditya Dey,Daniel Sorensen,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: 针对半导体制造中复杂多变量时间序列数据的异常检测挑战，本文提出一种新颖的机器学习方法，通过连续小波变换将时间序列转换为图像，并利用微调的VGG-16骨干构建孪生网络进行图像嵌入比较，从而实现高精度异常识别。


<details>
  <summary>Details</summary>
Motivation: 半导体制造过程极其复杂，其多变量时间序列数据存在高维、严重类别不平衡、噪声、缺失以及非平稳等挑战，且变量间相互依赖和故障滞后出现进一步增加了异常检测和根因分析的难度。因此，亟需一种通用的异常检测方法。

Method: 该方法分三步：a) 利用连续小波变换（CWT）将多变量时间序列数据转换为图像表示；b) 通过微调预训练的VGG-16架构在自定义CWT图像数据集上开发多类别图像分类器；c) 构建一个孪生网络，其两个子网络均以微调后的VGG-16为骨干，通过比较输入（参考图像和查询图像对）的嵌入来判断它们是否属于同一类别，从而检测异常。

Result: 该方法在真实半导体生产线（FAB）时间序列数据集上显示出高精度的异常识别能力，为过程和工具跟踪数据的离线异常检测提供了一种有前景的解决方案。

Conclusion: 本文提出了一种新颖、通用且高精度的机器学习方法，用于半导体制造复杂多变量时间序列数据的异常检测。该方法基于连续小波变换和微调VGG-16构建的孪生网络，在真实数据集上表现出色，且支持监督和半监督设置，为离线异常检测提供了有前景的解决方案。

Abstract: Semiconductor manufacturing is an extremely complex process, characterized by
thousands of interdependent parameters collected across diverse tools and
process steps. Multi-variate time-series (MTS) analysis has emerged as a
critical methodology for enabling real-time monitoring, fault detection, and
predictive maintenance in such environments. However, anomaly prediction in
semiconductor fabrication presents several critical challenges, including high
data dimensionality, severe class imbalance due to the rarity of true faults,
noisy and missing measurements, and non-stationary behavior of production
systems. Furthermore, the complex interdependencies between variables and the
delayed emergence of faults across downstream stages complicate both anomaly
detection and root-cause-analysis. This paper presents a novel and generic
approach for anomaly detection in MTS data using machine learning. The proposed
methodology consists of three main steps: a) converting MTS data into
image-based representations using the Continuous Wavelet Transform, b)
developing a multi-class image classifier by fine-tuning a pretrained VGG-16
architecture on custom CWT image datasets, and c) constructing a Siamese
network composed of two identical sub-networks, each utilizing the fine-tuned
VGG-16 as a backbone. The network takes pairs of CWT images as input -one
serving as a reference or anchor (representing a known-good signal), and the
other as a query (representing an unknown signal). The model then compares the
embeddings of both inputs to determine whether they belong to the same class at
a given time step. Our approach demonstrates high accuracy in identifying
anomalies on a real FAB process time-series dataset, offering a promising
solution for offline anomaly detection in process and tool trace data.
Moreover, the approach is flexible and can be applied in both supervised and
semi-supervised settings.

</details>


### [53] [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001)
*Anurag Arnab,Ahmet Iscen,Mathilde Caron,Alireza Fathi,Cordelia Schmid*

Main category: cs.LG

TL;DR: 本文提出“时间思维链”（Temporal Chain of Thought），一种视频问答推理策略，通过VLM自身迭代提取最相关帧来优化输入上下文，从而在长视频理解中实现SOTA性能，并在超长视频上表现尤为出色。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）取得了进展，但长视频理解仍具挑战。现有的长上下文VLM模型即使能处理大量帧，也难以有效利用长序列，且易受上下文窗口中无关干扰信息的影响。

Method: 本文提出“时间思维链”（Temporal Chain of Thought）推理策略。该方法利用VLM模型本身，迭代地识别并提取视频中最相关的帧，然后将这些精选帧作为输入上下文进行问答。这种方法通过在推理时增加计算以选择最相关上下文。

Result: 研究结果表明，通过选择最相关上下文可以提高准确性，这与LLM推理时缩放的相关工作一致。该方法在4个不同的视频问答数据集上取得了当前最佳（SOTA）结果，并对3种不同的VLM模型均显示出持续的性能提升。特别是在处理传统上下文窗口无法容纳的更长视频时，本方法表现突出。例如，在LVBench上处理超过1小时的视频时，使用32K上下文窗口的方法比使用700K上下文窗口的标准推理方法性能高出2.8点。

Conclusion: 在推理时投入额外计算以智能地选择和管理输入上下文，能显著提升视觉-语言模型在长视频理解任务上的表现，特别是对于超长视频，本方法能有效克服传统上下文窗口限制，并达到SOTA水平。

Abstract: Despite recent advances in Vision-Language Models (VLMs), long-video
understanding remains a challenging problem. Although state-of-the-art
long-context VLMs can process around 1000 input frames, they still struggle to
effectively leverage this sequence length, and succumb to irrelevant
distractors within the context window. We present Temporal Chain of Thought, an
inference strategy for video question-answering that curates the model's input
context. We use the VLM itself to iteratively identify and extract the most
relevant frames from the video, which are then used for answering. We
demonstrate how leveraging more computation at inference-time to select the
most relevant context leads to improvements in accuracy, in agreement with
recent work on inference-time scaling of LLMs. Moreover, we achieve
state-of-the-art results on 4 diverse video question-answering datasets,
showing consistent improvements with 3 different VLMs. In particular, our
method shines on longer videos which would not otherwise fit within the model's
context window: On longer videos of more than 1 hour on LVBench, our approach
using a context window of 32K outperforms the same VLM using standard inference
with a 700K context window by 2.8 points.

</details>


### [54] [AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design](https://arxiv.org/abs/2507.02006)
*Shakya Jayakody,Youpeng Zhao,Jun Wang*

Main category: cs.LG

TL;DR: 本文提出AIRES，一个算法与系统协同设计方案，旨在加速GCN中受内存限制的离核稀疏通用矩阵乘法（SpGEMM），通过优化数据对齐和分层内存传输，显著降低I/O延迟并提高GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 图卷积网络（GCN）处理大规模图数据时，SpGEMM常因GPU内存限制而需离核执行。现有离核SpGEMM方法存在高I/O延迟和GPU利用率不足问题，主要瓶颈在于稀疏格式数据对齐和内存分配。

Method: AIRES是一种算法-系统协同设计方案。算法层面，通过在块级别解决稀疏格式矩阵的数据对齐问题，并开发了促进行块对齐的分块算法。系统层面，采用三阶段动态调度和双向数据传输策略，利用分层内存系统（GPU内存、GDS、主机内存）来减少I/O延迟和提高吞吐量。

Result: 实验评估显示，AIRES显著优于现有最先进方法，在实际图处理基准测试中，延迟降低高达1.8倍。

Conclusion: AIRES通过创新的算法与系统协同设计，有效解决了GCN中离核SpGEMM的性能瓶颈，显著提升了计算效率和GPU利用率。

Abstract: Graph convolutional networks (GCNs) are fundamental in various scientific
applications, ranging from biomedical protein-protein interactions (PPI) to
large-scale recommendation systems. An essential component for modeling graph
structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As
the size of graph data continues to scale up, SpGEMMs are often conducted in an
out-of-core fashion due to limited GPU memory space in resource-constrained
systems. Albeit recent efforts that aim to alleviate the memory constraints of
out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory
layout, or performing the computation in sparse format, current systems suffer
from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where
sparse format data alignment and memory allocation are the main performance
bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to
accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the
algorithm angle, AIRES proposes to alleviate the data alignment issues on the
block level for matrices in sparse formats and develops a tiling algorithm to
facilitate row block-wise alignment. On the system level, AIRES employs a
three-phase dynamic scheduling that features a dual-way data transfer strategy
utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage
(GDS), and host memory to reduce I/O latency and improve throughput.
Evaluations show that AIRES significantly outperforms the state-of-the-art
methods, achieving up to 1.8x lower latency in real-world graph processing
benchmarks.

</details>


### [55] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao,Jiaqi Han,Siyi Gu,Mingjian Jiang,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: GeoAda是一个SE(3)等变适配器框架，用于对几何扩散模型进行参数高效微调，使其能灵活应用于具有不同几何控制的下游任务，同时保持模型几何一致性并减轻过拟合和灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 几何扩散模型在分子动力学和结构生成方面取得了显著成功，但针对具有不同几何控制的下游任务进行高效微调的方法尚未被充分探索。

Method: 提出了SE(3)等变适配器框架GeoAda，它在不修改原始模型架构的情况下实现微调。GeoAda引入了一种结构化适配器设计：控制信号首先通过耦合算子编码，然后由所选预训练模型层的可训练副本处理，最后通过解耦算子和等变零初始化卷积投射回。仅微调这些轻量级适配器模块，并从理论上证明了其保持SE(3)等变性。

Result: GeoAda在多种几何控制类型（如框架控制、全局控制、子图控制）和广泛的应用领域（如粒子动力学、分子动力学、人体运动预测、分子生成）中展现出广泛适用性。实证结果表明，GeoAda取得了最先进的微调性能，同时保持了原始任务的准确性，优于其他存在过拟合和灾难性遗忘问题的基线方法。

Conclusion: GeoAda为几何扩散模型提供了一种灵活且参数高效的微调方案，有效解决了在不同几何控制下游任务中的挑战，同时保持了模型的几何一致性，并能有效避免过拟合和灾难性遗忘。

Abstract: Geometric diffusion models have shown remarkable success in molecular
dynamics and structure generation. However, efficiently fine-tuning them for
downstream tasks with varying geometric controls remains underexplored. In this
work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables
flexible and parameter-efficient fine-tuning for controlled generative tasks
without modifying the original model architecture. GeoAda introduces a
structured adapter design: control signals are first encoded through coupling
operators, then processed by a trainable copy of selected pretrained model
layers, and finally projected back via decoupling operators followed by an
equivariant zero-initialized convolution. By fine-tuning only these lightweight
adapter modules, GeoAda preserves the model's geometric consistency while
mitigating overfitting and catastrophic forgetting. We theoretically prove that
the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric
inductive biases of the pretrained diffusion model remain intact during
adaptation. We demonstrate the wide applicability of GeoAda across diverse
geometric control types, including frame control, global control, subgraph
control, and a broad range of application domains such as particle dynamics,
molecular dynamics, human motion prediction, and molecule generation. Empirical
results show that GeoAda achieves state-of-the-art fine-tuning performance
while preserving original task accuracy, whereas other baselines experience
significant performance degradation due to overfitting and catastrophic
forgetting.

</details>


### [56] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: 研究对比了大型语言模型（LLMs）和专有领域模型（Match Score）在招聘匹配中的表现，发现Match Score在准确性和公平性上均优于LLMs，强调了高风险领域中领域模型和偏见审计的重要性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在招聘筛选中的适用性，尤其关注其准确性和算法偏见，并与专用领域模型进行对比，以解决LLMs在招聘中可能引入的潜在风险。

Method: 使用包含约10,000个真实求职者-职位对的数据集，对OpenAI、Anthropic、Google、Meta、Deepseek等主流LLMs与专有招聘模型（Match Score）进行基准测试。评估指标包括预测准确性（ROC AUC, Precision-Recall AUC, F1-score）和公平性（按性别、种族和交叉亚组的截止分析影响比）。

Result: Match Score在准确性上优于通用LLMs（ROC AUC 0.85 vs 0.77），并在人口统计学群体间实现了显著更公平的结果。Match Score的最小种族影响比为0.957（接近公平），而表现最佳的LLMs为0.809或更低。交叉影响比分别为0.906和0.773。研究表明，预训练偏见可能导致LLMs传播社会偏见，而定制的监督模型能更有效缓解这些偏见。

Conclusion: 在高风险领域（如招聘）部署AI时，领域特定建模和偏见审计至关重要。不加大量公平性保障，不应依赖现成的LLMs。精确招聘和结果公平性并非对立，精心设计的算法可以同时实现两者。

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [57] [Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model](https://arxiv.org/abs/2507.02089)
*Xingtu Liu,Lin F. Yang,Sharan Vaswani*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider infinite-horizon $\gamma$-discounted (linear) constrained Markov
decision processes (CMDPs) where the objective is to find a policy that
maximizes the expected cumulative reward subject to expected cumulative
constraints. Given access to a generative model, we propose to solve CMDPs with
a primal-dual framework that can leverage any black-box unconstrained MDP
solver. For linear CMDPs with feature dimension $d$, we instantiate the
framework by using mirror descent value iteration
(\texttt{MDVI})~\citep{kitamura2023regularization} an example MDP solver. We
provide sample complexity bounds for the resulting CMDP algorithm in two cases:
(i) relaxed feasibility, where small constraint violations are allowed, and
(ii) strict feasibility, where the output policy is required to exactly satisfy
the constraint. For (i), we prove that the algorithm can return an
$\epsilon$-optimal policy with high probability by using
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ samples. We note
that these results exhibit a near-optimal dependence on both $d$ and
$\epsilon$. For (ii), we show that the algorithm requires
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ samples,
where $\zeta$ is the problem-dependent Slater constant that characterizes the
size of the feasible region. Finally, we instantiate our framework for tabular
CMDPs and show that it can be used to recover near-optimal sample complexities
in this setting.

</details>


### [58] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: 本文提出能量基Transformer (EBTs)，通过学习验证输入与候选预测之间的兼容性，并将其重构为优化问题，实现了无需额外监督的通用System 2思维，并在多模态任务上展示了更优的扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的System 2思维计算技术在提升模型性能方面日益流行，但它们大多存在局限性：仅限于特定模态（如文本）、特定问题（如可验证的数学和编码领域），或需要额外的监督/训练。研究动机在于探索是否有可能泛化这些System 2思维方法，并开发出仅通过无监督学习就能实现“思考”的模型。

Method: 通过训练能量基Transformer (EBTs) 来解决问题。EBTs是一种新型的能量基模型，它为每个输入和候选预测对分配一个能量值。预测过程通过基于梯度下降的能量最小化直至收敛来实现，核心在于学习明确验证输入与候选预测之间的兼容性，并将预测问题重构为对该验证器的优化。

Result: 实验结果显示，EBTs在训练阶段比主流的Transformer++方法扩展速度更快，在数据、批大小、参数、FLOPs和深度方面，其扩展率提升高达35%。在推理阶段，EBTs在语言任务上比Transformer++通过System 2思维提升了29%的性能，并且在图像去噪任务上，EBTs在使用更少前向传播次数的情况下，超越了Diffusion Transformers。此外，EBTs在大多数下游任务上取得了比现有模型更好的结果，即使预训练性能相同或更差，这表明EBTs的泛化能力优于现有方法。

Conclusion: EBTs为提升模型的学习和思考能力提供了一个有前景的新范式。

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [59] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2507.02109)
*Florian Grötschla,Luca A. Lanzendörfer,Longxiang Jiao,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 介绍PANAMA，一个利用主动学习和类WaveNet架构高效训练吉他音箱模型的框架。


<details>
  <summary>Details</summary>
Motivation: 旨在通过最少量的数据点（音箱旋钮设置）高效训练端到端参数化吉他音箱模型，以创建虚拟音箱。

Method: 引入PANAMA主动学习框架，采用类WaveNet架构。通过基于梯度的优化算法确定最佳数据采样点，从而实现最小化所需数据量来训练模型。

Result: 研究表明，基于梯度的优化算法能有效确定最优采样点，并且该方法在样本数量受限的情况下表现出良好的效果。

Conclusion: PANAMA框架通过结合主动学习和梯度优化，提供了一种在有限数据条件下高效训练和创建高质量虚拟吉他音箱模型的有效途径。

Abstract: We introduce PANAMA, an active learning framework for the training of
end-to-end parametric guitar amp models using a WaveNet-like architecture. With
\model, one can create a virtual amp by recording samples that are determined
by an active learning strategy to use a minimum amount of datapoints (i.e., amp
knob settings). We show that gradient-based optimization algorithms can be used
to determine the optimal datapoints to sample, and that the approach helps
under a constrained number of samples.

</details>


### [60] [Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks](https://arxiv.org/abs/2507.02119)
*Shikai Qiu,Lechao Xiao,Andrew Gordon Wilson,Jeffrey Pennington,Atish Agarwala*

Main category: cs.LG

TL;DR: 研究发现，在计算最优训练的神经网络中，不同规模模型的损失曲线在归一化后会坍缩到一条通用曲线上，即“超坍缩”现象，这表明了训练动态的普适性。


<details>
  <summary>Details</summary>
Motivation: 探究在模型大小和训练时间同步增长时，神经网络训练动态受何种缩放限制所支配。

Method: 通过观察不同学习率策略、数据集和架构（包括Transformer）下训练模型的损失曲线，对训练计算量和损失进行归一化处理。研究将这种坍缩现象与神经网络缩放定律中的幂律结构联系起来，并分析了一个简化的SGD噪声动态模型来解释观察到的现象。

Result: 当训练计算量和损失在训练结束时归一化为1后，不同规模模型的损失曲线会坍缩到一条单一的通用曲线上。在使用学习率衰减时，这种坍缩变得非常紧密，使得标准化曲线之间的差异低于单个损失曲线的随机种子噪声水平（称之为“超坍缩”）。超坍缩现象在不同学习率策略、数据集和架构中均被观察到，并且当超参数次优缩放时，该现象会消失，从而提供了一个精确且实用的良好缩放指标。

Conclusion: 计算最优训练的神经网络表现出显著的普适性，其损失曲线在归一化后呈现出“超坍缩”现象。这一现象不仅揭示了深度学习训练的内在规律，也提供了一个判断模型是否良好缩放的实用指标，并可通过SGD噪声动态模型得到解释。

Abstract: What scaling limits govern neural network training dynamics when model size
and training time grow in tandem? We show that despite the complex interactions
between architecture, training algorithms, and data, compute-optimally trained
models exhibit a remarkably precise universality. Specifically, loss curves
from models of varying sizes collapse onto a single universal curve when
training compute and loss are normalized to unity at the end of training. With
learning rate decay, the collapse becomes so tight that differences in the
normalized curves across models fall below the noise floor of individual loss
curves across random seeds, a phenomenon we term supercollapse. We observe
supercollapse across learning rate schedules, datasets, and architectures,
including transformers trained on next-token prediction, and find it breaks
down when hyperparameters are scaled suboptimally, providing a precise and
practical indicator of good scaling. We explain these phenomena by connecting
collapse to the power-law structure in typical neural scaling laws, and
analyzing a simple yet surprisingly effective model of SGD noise dynamics that
accurately predicts loss curves across various learning rate schedules and
quantitatively explains the origin of supercollapse.

</details>


### [61] [CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs](https://arxiv.org/abs/2507.02128)
*Jingyu Pan,Isaac Jacobson,Zheng Zhao,Tung-Chieh Chen,Guanglei Zhou,Chen-Chia Chang,Vineet Rashingkar,Yiran Chen*

Main category: cs.LG

TL;DR: CROP是首个基于LLM的VLSI设计流自动调优框架，通过RAG增强LLM引导的参数搜索系统，解决了EDA参数优化挑战，显著提升设计质量（如降低9.9%功耗）并减少迭代。


<details>
  <summary>Details</summary>
Motivation: 现代VLSI设计中，EDA工具的参数空间巨大，导致芯片设计优化面临巨大挑战。目前手动选择参数的方法耗时且受专家经验限制，效率低下。

Method: 提出CROP框架，这是一个基于大型语言模型（LLM）的自动VLSI设计流程调优框架。其方法包括：1) 将RTL源代码转换为密集向量表示的可扩展方法；2) 基于嵌入的检索系统，用于匹配语义相似的电路设计；3) 检索增强生成（RAG）增强的LLM引导参数搜索系统，利用相似设计的先验知识约束搜索过程。

Result: 实验结果表明，CROP在工业设计中，与现有方法相比，能以更少的迭代次数实现更优的质量结果（QoR），包括功耗降低9.9%。

Conclusion: CROP框架通过结合LLM和检索增强技术，有效解决了VLSI设计中EDA参数优化效率低下的问题，显著提升了设计质量并缩短了优化周期。

Abstract: Modern very large-scale integration (VLSI) design requires the implementation
of integrated circuits using electronic design automation (EDA) tools. Due to
the complexity of EDA algorithms, the vast parameter space poses a huge
challenge to chip design optimization, as the combination of even moderate
numbers of parameters creates an enormous solution space to explore. Manual
parameter selection remains industrial practice despite being excessively
laborious and limited by expert experience. To address this issue, we present
CROP, the first large language model (LLM)-powered automatic VLSI design flow
tuning framework. Our approach includes: (1) a scalable methodology for
transforming RTL source code into dense vector representations, (2) an
embedding-based retrieval system for matching designs with semantically similar
circuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided
parameter search system that constrains the search process with prior knowledge
from similar designs. Experiment results demonstrate CROP's ability to achieve
superior quality-of-results (QoR) with fewer iterations than existing
approaches on industrial designs, including a 9.9% reduction in power
consumption.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [62] [A Comprehensive Survey on Network Traffic Synthesis: From Statistical Models to Deep Learning](https://arxiv.org/abs/2507.01976)
*Nirhoshan Sivaroopan,Kaushitha Silva,Chamara Madarasingha,Thilini Dahanayaka,Guillaume Jourjon,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.NI

TL;DR: 本文综述了合成网络流量生成技术，涵盖数据类型、生成模型（特别是深度学习和统计方法）及评估方法，旨在解决真实数据稀缺、隐私和纯度问题，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决真实网络流量数据在数据稀缺、隐私问题和纯度限制方面的挑战，为数据驱动应用提供可靠的替代方案，通过生成保留真实世界特征的合成数据。

Method: 采用综合性综述方法，全面回顾合成网络流量生成方法，涵盖数据类型、生成模型（重点关注深度学习，也包括统计方法及其扩展及商用工具）和评估方法。同时，还探讨了该领域的开放挑战和未来研究方向。

Result: 系统地分析了现有合成网络流量生成方法，识别了该领域的开放挑战，并提出了潜在的未来研究与发展方向。

Conclusion: 本综述为研究人员和从业者提供了关于合成网络流量生成领域现有方法、挑战和机遇的基础性资源和结构化分析。

Abstract: Synthetic network traffic generation has emerged as a promising alternative
for various data-driven applications in the networking domain. It enables the
creation of synthetic data that preserves real-world characteristics while
addressing key challenges such as data scarcity, privacy concerns, and purity
constraints associated with real data. In this survey, we provide a
comprehensive review of synthetic network traffic generation approaches,
covering essential aspects such as data types, generation models, and
evaluation methods. With the rapid advancements in AI and machine learning, we
focus particularly on deep learning-based techniques while also providing a
detailed discussion of statistical methods and their extensions, including
commercially available tools. Furthermore, we highlight open challenges in this
domain and discuss potential future directions for further research and
development. This survey serves as a foundational resource for researchers and
practitioners, offering a structured analysis of existing methods, challenges,
and opportunities in synthetic network traffic generation.

</details>


### [63] [Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers](https://arxiv.org/abs/2507.01988)
*Giyong Jung,Saeid Gorgin,John Kim,Jungrae Kim*

Main category: cs.NI

TL;DR: 为解决AI互连中多节点配置下静默丢弃flit的问题，本文提出了无头开销的隐式序列号（ISN）机制，并基于此设计了CXL扩展协议RXL，以实现高带宽、可伸缩且可靠的多节点互连。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型发展，芯片间互连需支持海量数据传输，新协议如CXL提升了速率，但增加了错误（特别是多节点中静默丢弃flit）的风险。现有CRC和FEC机制难以完全解决多节点配置下交换设备静默丢弃flit的检测和有序交付问题。

Method: 本文提出隐式序列号（ISN），一种无需额外包头开销即可精确检测flit丢弃并确保有序交付的机制。在此基础上，提出可靠性扩展链路（RXL），作为CXL协议的扩展，它将CRC提升至传输层以保证端到端数据和序列完整性，并利用FEC进行链路层错误纠正和检测，旨在支持可伸缩、可靠的多节点互连，同时保持与现有flit结构兼容。

Result: RXL在不影响带宽效率的前提下，提供了强大的可靠性和可伸缩性，有效解决了多节点互连中静默丢弃flit的检测问题，并确保了数据的有序交付，同时保持了与CXL现有结构的兼容性。

Conclusion: 通过引入ISN和RXL，本文提供了一种创新且兼容的解决方案，显著提升了高带宽、多节点芯片互连的可靠性和可伸缩性，为未来高性能计算和AI系统的数据传输提供了关键保障。

Abstract: As AI models outpace the capabilities of single processors, interconnects
across chips have become a critical enabler for scalable computing. These
processors exchange massive amounts of data at cache-line granularity,
prompting the adoption of new interconnect protocols like CXL, NVLink, and
UALink, designed for high bandwidth and small payloads. However, the increasing
transfer rates of these protocols heighten susceptibility to errors. While
mechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction
(FEC) are standard for reliable data transmission, scaling chip interconnects
to multi-node configurations introduces new challenges, particularly in
managing silently dropped flits in switching devices. This paper introduces
Implicit Sequence Number (ISN), a novel mechanism that ensures precise flit
drop detection and in-order delivery without adding header overhead.
Additionally, we propose Reliability Extended Link (RXL), an extension of CXL
that incorporates ISN to support scalable, reliable multi-node interconnects
while maintaining compatibility with the existing flit structure. By elevating
CRC to a transport-layer mechanism for end-to-end data and sequence integrity,
and relying on FEC for link-layer error correction and detection, RXL delivers
robust reliability and scalability without compromising bandwidth efficiency.

</details>


### [64] [Curated Collaborative AI Edge with Network Data Analytics for B5G/6G Radio Access Networks](https://arxiv.org/abs/2507.01994)
*Sardar Jaffar Ali,Syed M. Raza,Duc-Tai Le,Rajesh Challa,Min Young Chung,Ness Shroff,Hyunseung Choo*

Main category: cs.NI

TL;DR: 本文提出CCL框架进行精准流量和用户预测，并结合DUPS方案动态管理5G无线接入网（RAN）的分布式单元服务器，以显著降低能耗和运营成本。


<details>
  <summary>Details</summary>
Motivation: 5G无线接入网（RAN）占总功耗的50%以上，且现有RAN分割选项未能充分利用数据潜力，导致高昂的运营支出，亟需降低能耗和运营成本。

Method: 本研究采用双重方法：1. 提出**Curated Collaborative Learning (CCL) 框架**，通过选择性地与相关数据协作，实现高度精确的网络流量和用户预测。2. 提出**Distributed Unit Pooling Scheme (DUPS) 方案**，利用深度强化学习和CCL的预测结果，高效减少活跃的分布式单元（DU）服务器数量，并通过动态重定向流量来优化资源利用。

Result: CCL框架在流量预测方面显著优于现有最先进方法（包括全局、联邦、个性化联邦和循环机构增量学习），性能提升分别达到43.9%、39.1%、40.8%和31.35%。DUPS方案将能源效率比传统策略提高了高达89%，为运营商带来了可观的经济效益。

Conclusion: 通过整合CCL驱动的预测与DUPS，本研究展示了一种变革性的方法，能够显著降低5G RAN的能耗和运营成本，大幅提升效率和成本效益。

Abstract: Despite advancements, Radio Access Networks (RAN) still account for over 50\%
of the total power consumption in 5G networks. Existing RAN split options do
not fully harness data potential, presenting an opportunity to reduce
operational expenditures. This paper addresses this opportunity through a
twofold approach. First, highly accurate network traffic and user predictions
are achieved using the proposed Curated Collaborative Learning (CCL) framework,
which selectively collaborates with relevant correlated data for traffic
forecasting. CCL optimally determines whom, when, and what to collaborate with,
significantly outperforming state-of-the-art approaches, including global,
federated, personalized federated, and cyclic institutional incremental
learnings by 43.9%, 39.1%, 40.8%, and 31.35%, respectively. Second, the
Distributed Unit Pooling Scheme (DUPS) is proposed, leveraging deep
reinforcement learning and prediction inferences from CCL to reduce the number
of active DU servers efficiently. DUPS dynamically redirects traffic from
underutilized DU servers to optimize resource use, improving energy efficiency
by up to 89% over conventional strategies, translating into substantial
monetary benefits for operators. By integrating CCL-driven predictions with
DUPS, this paper demonstrates a transformative approach for minimizing energy
consumption and operational costs in 5G RANs, significantly enhancing
efficiency and cost-effectiveness.

</details>


### [65] [Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting](https://arxiv.org/abs/2507.01997)
*Zhihao Wang,Alessandro Cornacchia,Franco Galante,Carlo Centofanti,Alessio Sacco,Dingde Jiang*

Main category: cs.NI

TL;DR: 本初步工作探讨AI智能体在网络故障排除中的应用，并强调建立一个标准化、可复现、开放的基准测试平台的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明AI（尤其是LLMs）在网络配置与诊断中表现出色。本工作旨在指出，为有效构建和评估网络故障排除AI智能体，当前亟需一个标准化、可复现且操作简单的开放基准测试平台。

Method: 本工作属于初步研究，主要通过阐述和分析来论证AI智能体在网络故障排除中的应用前景，并强调了标准化、可复现、开放基准测试平台的需求。

Result: 本文明确指出了在低操作成本下，构建和评估网络故障排除AI智能体所需的标准化、可复现、开放基准测试平台的紧迫性与必要性。

Conclusion: 为了促进AI智能体在网络故障排除领域的开发与评估，建立一个标准化、可复现、开放的基准测试平台至关重要。

Abstract: Recent research has demonstrated the effectiveness of Artificial Intelligence
(AI), and more specifically, Large Language Models (LLMs), in supporting
network configuration synthesis and automating network diagnosis tasks, among
others. In this preliminary work, we restrict our focus to the application of
AI agents to network troubleshooting and elaborate on the need for a
standardized, reproducible, and open benchmarking platform, where to build and
evaluate AI agents with low operational effort.

</details>


### [66] [AI-Empowered Channel Generation for IoV Semantic Communications in Dynamic Conditions](https://arxiv.org/abs/2507.02013)
*Hao Liu,Bo Yang,Zhiwen Yu,Xuelin Cao,George C. Alexandropoulos,Yan Zhang,Chau Yuen*

Main category: cs.NI

TL;DR: 针对车联网(IoV)大数据传输挑战，本文提出一种基于信道感知的语义通信框架，通过生成式扩散模型预测信道状态，并利用大模型微调以增强动态场景适应性，从而提升数据传输效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 车联网(IoV)在实现普适连接和数据驱动方法方面潜力巨大，但实时传输和处理海量数据面临巨大挑战，尤其是在动态和不可预测的无线信道条件下。深度学习和生成式AI有望提升IoV应用性能。

Method: 1. 提出一种基于信道感知的语义通信框架，用于提取和压缩待传输信息，以提高数据传输的准确性和效率。 2. 采用生成式扩散模型估计无线信道，用于预测动态信道状态，从而提升IoV服务质量。 3. 为解决动态场景下信道估计性能下降问题，利用大模型微调信道生成模型，增强其对多变场景的适应性。 4. 在两个公共数据集上评估所提框架的性能和可靠性。

Result: 论文对所提出的框架在两个公共数据集上的性能和可靠性进行了评估。

Conclusion: 本文提出的基于信道感知的语义通信框架，结合生成式扩散模型进行信道预测，并辅以大模型微调，旨在有效提升车联网在动态无线信道条件下的数据传输效率、准确性和服务质量，为应对海量数据传输挑战提供了解决方案。

Abstract: The Internet of Vehicles (IoV) transforms the transportation ecosystem
promising pervasive connectivity and data-driven approaches. Deep learning and
generative Artificial Intelligence (AI) have the potential to significantly
enhance the operation of applications within IoV by facilitating efficient
decision-making and predictive capabilities, including intelligent navigation,
vehicle safety monitoring, accident prevention, and intelligent traffic
management. Nevertheless, efficiently transmitting and processing the massive
volumes of data generated by the IoV in real-time remains a significant
challenge, particularly in dynamic and unpredictable wireless channel
conditions. To address these challenges, this paper proposes a semantic
communication framework based on channel perception to improve the accuracy and
efficiency of data transmission. The semantic communication model extracts and
compresses the information to be transmitted. In addition, the wireless channel
is estimated by using a generative diffusion model, which is employed to
predict the dynamic channel states, thereby improving the quality of IoV
service. In dynamic scenarios, however, the channel estimation performance may
be degraded when substantially new scenarios take place, which will adversely
affect user experience. To mitigate this limitation, we employ a large model to
fine-tune the channel generation model to enhance its adaptability for varying
scenarios. The performance and reliability of the proposed framework are
evaluated on the two public datasets.

</details>


### [67] [REDUS: Adaptive Resampling for Efficient Deep Learning in Centralized and Federated IoT Networks](https://arxiv.org/abs/2507.02021)
*Eyad Gad,Gad Gad,Mostafa M. Fouda,Mohamed I. Ibrahem,Muhammad Ismail,Zubair Md Fadlullah*

Main category: cs.NI

TL;DR: 本文提出REDUS，一种在联邦学习环境下用于智能网络的数据重采样技术，通过优先处理错分类样本和排除冗余数据，优化深度学习训练，显著减少训练时间并节约资源，同时保持高精度，以解决SDN与DL共存时的资源冲突问题。


<details>
  <summary>Details</summary>
Motivation: 软件定义网络（SDN）与深度学习（DL）工作负载共享基础设施时，尤其在延迟敏感的物联网（IoT）环境中，会导致资源争用，降低SDN响应能力并损害网络性能。尽管联邦学习（FL）通过去中心化训练缓解了部分问题，但在物联网系统连续数据流下，DL的计算需求仍可能干扰SDN性能。

Method: 本文提出REDUS（Resampling for Efficient Data Utilization in Smart-Networks），一种受AdaBoost启发的重采样技术。REDUS通过优先处理错分类样本和排除冗余数据来优化DL训练，减少每个训练周期的样本数量，从而在联邦学习设置中节约计算资源、降低能耗并加速收敛。

Result: REDUS减少了每个训练周期的训练样本数量，从而节约计算资源、降低能耗并加速收敛，且对精度影响甚微。在FL环境下应用时，它提高了资源受限边缘设备上的模型训练效率，同时保持网络性能。在CICIoT2023物联网攻击检测数据集上的评估显示，训练时间减少高达72.6%，而精度损失仅为1.62%。

Conclusion: REDUS为智能网络提供了一个可扩展且实用的解决方案，有效缓解了深度学习训练与SDN操作之间的资源冲突，同时在资源受限的边缘设备上实现了高效且高精度的模型训练。

Abstract: With the rise of Software-Defined Networking (SDN) for managing traffic and
ensuring seamless operations across interconnected devices, challenges arise
when SDN controllers share infrastructure with deep learning (DL) workloads.
Resource contention between DL training and SDN operations, especially in
latency-sensitive IoT environments, can degrade SDN's responsiveness and
compromise network performance. Federated Learning (FL) helps address some of
these concerns by decentralizing DL training to edge devices, thus reducing
data transmission costs and enhancing privacy. Yet, the computational demands
of DL training can still interfere with SDN's performance, especially under the
continuous data streams characteristic of IoT systems. To mitigate this issue,
we propose REDUS (Resampling for Efficient Data Utilization in Smart-Networks),
a resampling technique that optimizes DL training by prioritizing misclassified
samples and excluding redundant data, inspired by AdaBoost. REDUS reduces the
number of training samples per epoch, thereby conserving computational
resources, reducing energy consumption, and accelerating convergence without
significantly impacting accuracy. Applied within an FL setup, REDUS enhances
the efficiency of model training on resource-limited edge devices while
maintaining network performance. In this paper, REDUS is evaluated on the
CICIoT2023 dataset for IoT attack detection, showing a training time reduction
of up to 72.6% with a minimal accuracy loss of only 1.62%, offering a scalable
and practical solution for intelligent networks.

</details>


### [68] [MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking](https://arxiv.org/abs/2507.02613)
*Yalin E. Sagduyu,Kemal Davaslioglu,Tugba Erpek,Sastry Kompella,Gustave Anderson,Jonathan Ashdown*

Main category: cs.NI

TL;DR: 本文提出了一种利用5G定位参考信号(PRS)进行多基地综合传感与通信(ISAC)的完整信号处理链，实现移动目标的高精度探测、定位和跟踪。


<details>
  <summary>Details</summary>
Motivation: 利用5G PRS信号实现多基地ISAC中的目标探测、参数估计和跟踪，以提供一种高效且频谱友好的传感解决方案。

Method: 采用分布式架构，一个gNB发射5G OFDM-PRS信号，多个接收器利用该信号进行处理。通过相干互模糊函数(CAF)生成距离-多普勒图，提取双基地延迟和径向速度。利用非线性最小二乘三边测量法估计目标位置，并通过径向速度方程的正则化线性反演获得二维速度向量。该方法适用于2D和3D场景，并扩展以处理时间同步偏差和多目标关联。最终通过标准和扩展卡尔曼滤波器获得平滑轨迹。

Result: 结果表明，该方法能够利用5G PRS信号实现多基地ISAC中移动目标的高保真探测、定位和跟踪。

Conclusion: 本文成功展示了一种利用5G PRS信号进行多基地ISAC的完整信号处理链，有效实现了移动目标的高精度探测、定位和跟踪。

Abstract: This paper presents a complete signal-processing chain for multistatic
integrated sensing and communications (ISAC) using 5G Positioning Reference
Signal (PRS). We consider a distributed architecture in which one gNB transmits
a periodic OFDM-PRS waveform while multiple spatially separated receivers
exploit the same signal for target detection, parameter estimation and
tracking. A coherent cross-ambiguity function (CAF) is evaluated to form a
range-Doppler map from which the bistatic delay and radial velocity are
extracted for every target. For a single target, the resulting bistatic delays
are fused through nonlinear least-squares trilateration, yielding a geometric
position estimate, and a regularized linear inversion of the radial-speed
equations yields a two-dimensional velocity vector, where speed and heading are
obtained. The approach is applied to 2D and 3D settings, extended to account
for time synchronization bias, and generalized to multiple targets by resolving
target association. The sequence of position-velocity estimates is then fed to
standard and extended Kalman filters to obtain smoothed tracks. Our results
show high-fidelity moving-target detection, positioning, and tracking using 5G
PRS signals for multistatic ISAC.

</details>


### [69] [On the Architectural Split and Radio Intelligence Controller Placement in Integrated O-RAN-enabled Non-Terrestrial Networks](https://arxiv.org/abs/2507.02680)
*Jorge Baranda,Marius Caus,Luis Blanco,Cristian J. Vaca-Rubio,Engin Zeydan,Kapal Dev,Zheng Li,Tomaso DeCola*

Main category: cs.NI

TL;DR: 本文探讨了地面网络（TNs）与非地面网络（NTNs）融合中符合O-RAN原则的架构和功能切分策略，分析了RAN和核心功能在卫星和地面节点间的部署选项及其权衡，并讨论了RAN智能控制器（RIC）的放置。


<details>
  <summary>Details</summary>
Motivation: 地面网络（TNs）与非地面网络（NTNs）的集成因异构传播条件、动态拓扑和有限的星载处理能力，带来了独特的架构和功能挑战。

Method: 提出了一个切分选项的分类法，用于在卫星和地面节点之间分配RAN和核心功能；分析了性能、延迟、自治性和部署方面的权衡；评估了从纯星载DU部署到完整gNB和UPF星载集成的各种配置；讨论了近实时（Near-RT）和非实时（Non-RT）RAN智能控制器（RIC）的放置；提供了架构切分与RIC放置选项之间的全面映射。

Result: 提出了TN-NTN集成系统在O-RAN背景下的架构和功能切分策略分类法；分析了不同切分配置的权衡；评估了多种配置方案（包括星内和星间处理）；提供了架构切分与RIC放置选项之间的综合映射，并考虑了实现约束和互操作性。

Conclusion: 识别了实现标准化、模块化和高效TN-NTN融合的关键挑战，并展望了未来的研究方向。

Abstract: The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks
(NTNs) poses unique architectural and functional challenges due to
heterogeneous propagation conditions, dynamic topologies and limited on-board
processing capabilities. This paper examines architectural and functional split
strategies that are consistent with O-RAN principles for future integrated
TN-NTN systems. A taxonomy of split options is proposed that distributes RAN
and core functions between satellites and ground nodes, and trade-offs in terms
of performance, latency, autonomy and deployment are analysed. In particular,
we evaluate configurations ranging from pure on-board DU deployments to full
gNB and UPF integration into satellites, including variations based on intra-
and inter-satellite processing. In addition, the placement of Near-RT and
Non-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible
split strategies between space and ground to optimise the performance and
scalability of the control loop. A comprehensive mapping between architectural
splits and RIC placement options is provided, emphasising implementation
constraints and interoperability considerations. The paper concludes by
identifying key challenges and outlining future directions to enable
standardised, modular and efficient TN-NTN convergence in the context of the
O-RAN.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [70] [SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan](https://arxiv.org/abs/2507.02124)
*Fumikazu Konishi*

Main category: cs.DC

TL;DR: SAKURAONE是一个为LLM训练优化的HPC集群，在TOP500中排名第49，并创新性地采用了基于800GbE和SONiC的全开放网络堆栈。


<details>
  <summary>Details</summary>
Motivation: 论文旨在介绍SAKURAONE高性能计算集群，并强调在大型HPC基础设施中采用开放和供应商中立技术（如800GbE和SONiC）的可行性及全球竞争力。该集群主要用于处理包括大型语言模型训练在内的先进工作负载。

Method: SAKURAONE集群包含100个计算节点，每个节点配备8个NVIDIA H100 GPU。它采用全闪存Lustre存储系统（2PB），并通过基于轨优拓扑（Rail-Optimized topology）的全双向带宽互连实现节点间通信，该互连使用800GbE链路和RoCEv2协议，并采用全开放网络堆栈（800GbE和SONiC）。

Result: SAKURAONE在ISC 2025 TOP500榜单中排名全球第49位。HPL基准测试Rmax性能为33.95 PFLOP/s，HPCG基准测试为396.295 TFLOP/s。在针对AI应用的HPL-MxP基准测试中，使用FP8精度达到339.86 PFLOP/s。它是TOP100中唯一采用完全开放网络堆栈的系统。

Conclusion: SAKURAONE的成功实践证明了开放和供应商中立技术在构建高性能、全球领先的大规模HPC基础设施方面的强大可行性，尤其适用于大型语言模型训练等高级计算任务。

Abstract: SAKURAONE is a managed high performance computing (HPC) cluster developed and
operated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU
PHY'' configuration of bare-metal GPU servers and is designed as a cluster
computing resource optimized for advanced workloads, including large language
model (LLM) training.
  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked
\textbf{49th} in the world based on its High Performance Linpack (HPL) score,
demonstrating its global competitiveness. In particular, it is the \textbf{only
system within the top 100} that employs a fully open networking stack based on
\textbf{800~GbE (Gigabit Ethernet)} and the \textbf{SONiC (Software for Open
Networking in the Cloud)} operating system, highlighting the viability of open
and vendor-neutral technologies in large-scale HPC infrastructure.
  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL
benchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate
Gradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets
low-precision workloads representative of AI applications, SAKURAONE delivered
an impressive 339.86~PFLOP/s using FP8 precision.
  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100
GPUs. It is supported by an all-flash Lustre storage subsystem with a total
physical capacity of 2~petabytes, providing high-throughput and low-latency
data access. Internode communication is enabled by a full-bisection bandwidth
interconnect based on a Rail-Optimized topology, where the Leaf and Spine
layers are interconnected via 800~GbE links. This topology, in combination with
RoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless
data transfers and mitigates communication bottlenecks in large-scale parallel
workloads.

</details>
