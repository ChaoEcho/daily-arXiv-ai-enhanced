<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 61]
- [cs.CV](#cs.CV) [Total: 58]
- [cs.AI](#cs.AI) [Total: 58]
- [cs.LG](#cs.LG) [Total: 58]
- [cs.NI](#cs.NI) [Total: 15]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation](https://arxiv.org/abs/2510.09671)
*Wei Zhou,Bolei Ma,Annemarie Friedrich,Mohsen Mesgar*

Main category: cs.CL

TL;DR: 该综述对表格问答（TQA）领域进行了全面且结构化的概述，重点关注基于大型语言模型（LLM）的方法，旨在弥补现有研究缺乏系统性组织和理解的空白，并指导未来发展。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在表格问答（TQA）方面取得了显著进展，但该领域仍缺乏对任务制定、核心挑战和方法趋势的系统性组织和理解，尤其是在新兴研究方向（如强化学习）的背景下。

Method: 本文通过提供TQA研究的全面结构化概述来解决此问题，重点关注LLM-based方法。具体包括：对现有基准和任务设置进行综合分类；根据目标挑战对当前建模策略进行分组并分析其优缺点；突出未被系统性覆盖但及时且未充分探索的主题。

Result: 该综述统一了分散的研究线索并识别了开放问题，为TQA社区提供了坚实的基础。

Conclusion: 该综述能促进对TQA领域最新进展的深入理解，并指导该快速发展领域的未来发展方向。

Abstract: Table Question Answering (TQA) aims to answer natural language questions
about tabular data, often accompanied by additional contexts such as text
passages. The task spans diverse settings, varying in table representation,
question/answer complexity, modality involved, and domain. While recent
advances in large language models (LLMs) have led to substantial progress in
TQA, the field still lacks a systematic organization and understanding of task
formulations, core challenges, and methodological trends, particularly in light
of emerging research directions such as reinforcement learning. This survey
addresses this gap by providing a comprehensive and structured overview of TQA
research with a focus on LLM-based methods. We provide a comprehensive
categorization of existing benchmarks and task setups. We group current
modeling strategies according to the challenges they target, and analyze their
strengths and limitations. Furthermore, we highlight underexplored but timely
topics that have not been systematically covered in prior research. By unifying
disparate research threads and identifying open problems, our survey offers a
consolidated foundation for the TQA community, enabling a deeper understanding
of the state of the art and guiding future developments in this rapidly
evolving area.

</details>


### [2] [Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection](https://arxiv.org/abs/2510.09695)
*Yanran Chen,Lynn Greschner,Roman Klinger,Michael Klenk,Steffen Eger*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）可在谬误论证中加入情感框架，平均降低人类谬误检测能力14.5%，且特定情感（愉悦、恐惧、悲伤）与更高的说服力相关。


<details>
  <summary>Details</summary>
Motivation: 谬误在公共交流中普遍存在且具误导性，即使缺乏严谨性也可能因主观性而具说服力。本研究旨在首次通过计算方法探究情感框架如何与谬误及说服力相互作用。

Method: 使用大型语言模型（LLMs）系统地改变谬误论证中的情感诉求。首先，对八个LLM进行基准测试，评估其在保留逻辑结构的同时注入情感的能力。随后，利用表现最佳的模型生成刺激材料，用于人类研究。

Result: LLM驱动的情感框架平均使人类谬误检测的F1分数降低了14.5%。人类在感知到愉悦时比恐惧或悲伤时更能有效地检测谬误。愉悦、恐惧和悲伤这三种情感与中性或其他情感状态相比，显著提高了论证的说服力。

Conclusion: 本工作揭示了AI驱动的情感操纵在谬误论证背景下的潜在影响。

Abstract: Logical fallacies are common in public communication and can mislead
audiences; fallacious arguments may still appear convincing despite lacking
soundness, because convincingness is inherently subjective. We present the
first computational study of how emotional framing interacts with fallacies and
convincingness, using large language models (LLMs) to systematically change
emotional appeals in fallacious arguments. We benchmark eight LLMs on injecting
emotional appeal into fallacious arguments while preserving their logical
structures, then use the best models to generate stimuli for a human study. Our
results show that LLM-driven emotional framing reduces human fallacy detection
in F1 by 14.5% on average. Humans perform better in fallacy detection when
perceiving enjoyment than fear or sadness, and these three emotions also
correlate with significantly higher convincingness compared to neutral or other
emotion states. Our work has implications for AI-driven emotional manipulation
in the context of fallacious argumentation.

</details>


### [3] [The Idola Tribus of AI: Large Language Models tend to perceive order where none exists](https://arxiv.org/abs/2510.09709)
*Shin-nosuke Ishikawa,Masato Todo,Taiki Ogihara,Hirotsugu Ohba*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在分析随机数序列时，即使采用多步推理，也倾向于过度识别不存在的模式，这揭示了它们在需要逻辑推理任务中的潜在局限性。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂真实世界任务中的应用（如RAG、AI代理）依赖其逻辑一致性和自我连贯性。然而，LLMs可能生成荒谬模式，因此评估这些方面并考虑潜在对策至关重要。

Method: 进行了一项实验，要求LLMs解释不同类型的整数序列（包括算术序列、几何序列和随机生成的序列）中的模式。

Result: LLMs成功识别了算术和几何序列中的正确模式。但在分析随机生成序列时，它们频繁过度识别与给定数字不一致的模式。此问题在包括OpenAI o3、o4-mini和Google Gemini 2.5 Flash Preview Thinking在内的多步推理模型中均有出现。

Conclusion: LLMs感知不存在模式的倾向可视为AI模型的“部落偶像”效应，凸显了它们在需要逻辑推理的应用任务中的潜在局限性，即使采用思维链推理机制也无法规避。

Abstract: We present a tendency of large language models (LLMs) to generate absurd
patterns despite their clear inappropriateness in a simple task of identifying
regularities in number series. Several approaches have been proposed to apply
LLMs to complex real-world tasks, such as providing knowledge through
retrieval-augmented generation and executing multi-step tasks using AI agent
frameworks. However, these approaches rely on the logical consistency and
self-coherence of LLMs, making it crucial to evaluate these aspects and
consider potential countermeasures. To identify cases where LLMs fail to
maintain logical consistency, we conducted an experiment in which LLMs were
asked to explain the patterns in various integer sequences, ranging from
arithmetic sequences to randomly generated integer series. While the models
successfully identified correct patterns in arithmetic and geometric sequences,
they frequently over-recognized patterns that were inconsistent with the given
numbers when analyzing randomly generated series. This issue was observed even
in multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini
2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can
be interpreted as the AI model equivalent of Idola Tribus and highlights
potential limitations in their capability for applied tasks requiring logical
reasoning, even when employing chain-of-thought reasoning mechanisms.

</details>


### [4] [SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG](https://arxiv.org/abs/2510.09710)
*Xiaonan Si,Meilin Zhu,Simeng Qin,Lijia Yu,Lijun Zhang,Shuaitong Liu,Xinfeng Li,Ranjie Duan,Yang Liu,Xiaojun Jia*

Main category: cs.CL

TL;DR: SeCon-RAG提出了一种两阶段语义过滤和无冲突框架，通过保留有用知识并缓解冲突污染，显著提高了检索增强生成（RAG）系统的鲁棒性和输出可信度。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）系统易受语料库中毒和污染攻击，现有防御方法过于激进，导致信息丢失和可靠性降低。因此，需要一种在不牺牲有价值信息的情况下增强RAG系统可信度的方法。

Method: 提出SeCon-RAG框架：第一阶段，使用由EIRE（实体-意图-关系提取器）引导的语义和聚类联合过滤，根据语义相关性将有价值的文档添加到干净的检索数据库。第二阶段，提出EIRE引导的冲突感知过滤模块，分析查询、候选答案和检索知识之间的语义一致性，过滤掉内外矛盾，确保最终答案的生成质量。

Result: SeCon-RAG有效保留了有用知识，同时缓解了冲突污染，显著提高了生成鲁棒性和输出可信度。在各种大型语言模型和数据集上的实验表明，SeCon-RAG明显优于现有最先进的防御方法。

Conclusion: SeCon-RAG提供了一个有效且可信赖的RAG防御机制，解决了RAG系统在抵御知识污染方面的挑战，并在信息保留与冲突缓解之间取得了良好平衡。

Abstract: Retrieval-augmented generation (RAG) systems enhance large language models
(LLMs) with external knowledge but are vulnerable to corpus poisoning and
contamination attacks, which can compromise output integrity. Existing defenses
often apply aggressive filtering, leading to unnecessary loss of valuable
information and reduced reliability in generation. To address this problem, we
propose a two-stage semantic filtering and conflict-free framework for
trustworthy RAG. In the first stage, we perform a joint filter with semantic
and cluster-based filtering which is guided by the Entity-intent-relation
extractor (EIRE). EIRE extracts entities, latent objectives, and entity
relations from both the user query and filtered documents, scores their
semantic relevance, and selectively adds valuable documents into the clean
retrieval database. In the second stage, we proposed an EIRE-guided
conflict-aware filtering module, which analyzes semantic consistency between
the query, candidate answers, and retrieved knowledge before final answer
generation, filtering out internal and external contradictions that could
mislead the model. Through this two-stage process, SeCon-RAG effectively
preserves useful knowledge while mitigating conflict contamination, achieving
significant improvements in both generation robustness and output
trustworthiness. Extensive experiments across various LLMs and datasets
demonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art
defense methods.

</details>


### [5] [ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models](https://arxiv.org/abs/2510.09711)
*Wenbin Guo,Xin Wang,Jiaoyan Chen,Lingbing Guo,Zhao Li,Zirui Chen*

Main category: cs.CL

TL;DR: ReaLM框架通过残差向量量化将KG嵌入离散化并整合到LLM词汇中，解决了LLM-KGC中知识图谱嵌入与LLM token空间不匹配的问题，结合本体论约束，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的知识图谱补全方法无法充分利用结构化语义表示，因为预训练KG模型的连续嵌入空间与LLM的离散token空间不匹配，这限制了语义有效转移和性能。

Method: 提出ReaLM框架，通过残差向量量化将预训练KG嵌入离散化为紧凑的代码序列，并作为可学习token整合到LLM词汇中，以融合符号和上下文知识。此外，引入本体论引导的类别约束来强制语义一致性。

Result: 在两个广泛使用的基准数据集上，ReaLM取得了最先进（SOTA）的性能。

Conclusion: ReaLM有效解决了结构化知识与大型语言模型对齐的问题，并通过实验证实了其有效性。

Abstract: Large Language Models (LLMs) have recently emerged as a powerful paradigm for
Knowledge Graph Completion (KGC), offering strong reasoning and generalization
capabilities beyond traditional embedding-based approaches. However, existing
LLM-based methods often struggle to fully exploit structured semantic
representations, as the continuous embedding space of pretrained KG models is
fundamentally misaligned with the discrete token space of LLMs. This
discrepancy hinders effective semantic transfer and limits their performance.
To address this challenge, we propose ReaLM, a novel and effective framework
that bridges the gap between KG embeddings and LLM tokenization through the
mechanism of residual vector quantization. ReaLM discretizes pretrained KG
embeddings into compact code sequences and integrates them as learnable tokens
within the LLM vocabulary, enabling seamless fusion of symbolic and contextual
knowledge. Furthermore, we incorporate ontology-guided class constraints to
enforce semantic consistency, refining entity predictions based on class-level
compatibility. Extensive experiments on two widely used benchmark datasets
demonstrate that ReaLM achieves state-of-the-art performance, confirming its
effectiveness in aligning structured knowledge with large-scale language
models.

</details>


### [6] [All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language](https://arxiv.org/abs/2510.09714)
*Shiyuan Guo,Henry Sleight,Fabien Roger*

Main category: cs.CL

TL;DR: 当前AI模型在密文推理时准确性显著下降，表明通过密文推理逃避CoT监控对现有模型而言是无效策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理的普及，检测有害AI行为至关重要。CoT监控是检测攻击和未对齐AI的常用方法，但攻击者可能通过密文推理（即隐藏推理过程）逃避监控。本研究旨在评估模型进行密文推理的风险。

Method: 对28种不同密码，微调并提示多达10个模型进行密文推理。通过模型在数学问题上的准确性来衡量其推理能力。

Result: 发现不对称性：模型在密文推理时准确性显著下降，尽管它们能准确翻译密文。即使是前沿模型也难以处理不常见的密码，但对rot13等常见密码表现良好。密文推理能力与预训练数据中密码的流行度相关，且通过额外微调数据改善缓慢。

Conclusion: 本研究表明，对于当前模型而言，使用密文推理逃避CoT监控可能是一种无效策略，并为未来前沿模型限制此能力的发展提供了指导。

Abstract: Detecting harmful AI actions is important as AI agents gain adoption.
Chain-of-thought (CoT) monitoring is one method widely used to detect
adversarial attacks and AI misalignment. However, attackers and misaligned
models might evade CoT monitoring through ciphered reasoning: reasoning hidden
in encrypted, translated, or compressed text. To assess this risk, we test
whether models can perform ciphered reasoning. For each of 28 different
ciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We
measure model accuracy on math problems as a proxy for reasoning ability.
Across the models we test, we find an asymmetry: model accuracy can drop
significantly when reasoning in ciphered text, even though models demonstrate
comprehension of ciphered text by being able to translate it accurately to
English. Even frontier models struggle with lesser-known ciphers, although they
can reason accurately in well-known ciphers like rot13. We show that ciphered
reasoning capability correlates with cipher prevalence in pretraining data. We
also identify scaling laws showing that ciphered reasoning capability improves
slowly with additional fine-tuning data. Our work suggests that evading CoT
monitoring using ciphered reasoning may be an ineffective tactic for current
models and offers guidance on constraining the development of this capability
in future frontier models.

</details>


### [7] [Preference-Aware Memory Update for Long-Term LLM Agents](https://arxiv.org/abs/2510.09720)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 本文提出Preference-Aware Memory Update Mechanism (PAMU)，通过融合滑动窗口平均和指数移动平均，解决了LLM代理长时记忆在动态偏好更新上的不足，显著提升了其在长对话中的输出质量。


<details>
  <summary>Details</summary>
Motivation: LLM代理的长时记忆机制在存储和检索方面有所进步，但在记忆更新上存在不足，特别是缺乏动态精炼偏好记忆以适应用户行为和上下文变化的能力。

Method: 提出Preference-Aware Memory Update Mechanism (PAMU)。该机制整合了滑动窗口平均（SW）和指数移动平均（EMA），构建一个融合的、感知偏好的表示，能够同时捕捉短期波动和长期用户倾向，实现动态和个性化的记忆精炼。

Result: 在LoCoMo数据集的五个任务场景上进行的实验结果表明，PAMU机制能显著提高LLM在五个基线中的输出质量，验证了其在长对话中的有效性。

Conclusion: PAMU机制通过动态更新和个性化精炼偏好记忆，有效提升了LLM代理在长对话中的性能和输出质量。

Abstract: One of the key factors influencing the reasoning capabilities of LLM-based
agents is their ability to leverage long-term memory. Integrating long-term
memory mechanisms allows agents to make informed decisions grounded in
historical interactions. While recent advances have significantly improved the
storage and retrieval components, by encoding memory into dense vectors for
similarity search or organizing memory as structured knowledge graphs most
existing approaches fall short in memory updating. In particular, they lack
mechanisms for dynamically refining preference memory representations in
response to evolving user behaviors and contexts. To address this gap, we
propose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic
and personalized memory refinement. By integrating sliding window averages (SW)
with exponential moving averages (EMA), PAMU constructs a fused
preference-aware representation that captures both short-term fluctuations and
long-term user tendencies. We conduct experiments on five task scenarios of the
LoCoMo dataset, and the results show that our mechanism can significantly
improve the output quality of LLM in five baselines, validating its
effectiveness in long-term conversations.

</details>


### [8] [Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation](https://arxiv.org/abs/2510.09722)
*Fanwei Zhu,Jinke Yu,Zulong Chen,Ying Zhou,Junhao Ji,Zhibo Yang,Yuxue Zhang,Haoyuan Hu,Zhenghao Liu*

Main category: cs.CL

TL;DR: 该论文提出一个布局感知且效率优化的简历信息自动化提取与评估框架，解决了简历多样性、LLM成本与延迟高以及缺乏标准化数据集等问题，并在阿里巴巴平台部署。


<details>
  <summary>Details</summary>
Motivation: 自动化简历信息提取在人才招聘中至关重要，但面临三大挑战：简历布局和内容的高度异构性、大型语言模型（LLM）的高成本和高延迟，以及缺乏标准化数据集和评估工具。

Method: 该框架结合了：1) 经过微调的布局解析器，用于标准化不同文档格式；2) 基于并行提示和指令微调的推理高效LLM提取器；3) 由新基准数据集支持的鲁棒两阶段自动化评估框架。

Result: 实验表明，该框架在准确性和效率上显著优于现有基线。特别是，一个经过微调的紧凑型0.6B LLM在实现顶级准确性的同时，显著降低了推理延迟和计算成本。

Conclusion: 该系统已全面部署在阿里巴巴的智能HR平台，支持其业务部门的实时应用，有效解决了简历信息提取的实际挑战。

Abstract: Automated resume information extraction is critical for scaling talent
acquisition, yet its real-world deployment faces three major challenges: the
extreme heterogeneity of resume layouts and content, the high cost and latency
of large language models (LLMs), and the lack of standardized datasets and
evaluation tools. In this work, we present a layout-aware and
efficiency-optimized framework for automated extraction and evaluation that
addresses all three challenges. Our system combines a fine-tuned layout parser
to normalize diverse document formats, an inference-efficient LLM extractor
based on parallel prompting and instruction tuning, and a robust two-stage
automated evaluation framework supported by new benchmark datasets. Extensive
experiments show that our framework significantly outperforms strong baselines
in both accuracy and efficiency. In particular, we demonstrate that a
fine-tuned compact 0.6B LLM achieves top-tier accuracy while significantly
reducing inference latency and computational cost. The system is fully deployed
in Alibaba's intelligent HR platform, supporting real-time applications across
its business units.

</details>


### [9] [VisRAG 2.0: Evidence-Guided Multi-Image Reasoning in Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2510.09733)
*Yubo Sun,Chunyi Peng,Yukun Yan,Shi Yu,Zhenghao Liu,Chi Chen,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: EVisRAG是一个结合证据引导多图像推理和RS-GRPO优化的端到端框架，旨在提升VRAG系统在多图像证据感知与整合方面的能力，实验结果显示平均性能提升27%。


<details>
  <summary>Details</summary>
Motivation: 现有视觉检索增强生成（VRAG）系统在可靠感知和整合多图像证据时表现不足，导致推理基础薄弱和错误结论。

Method: 提出EVisRAG端到端框架，通过学习证据引导的多图像推理来解决问题。该模型首先观察并记录每幅图像的证据，然后从聚合证据中得出最终答案。为有效训练EVisRAG，引入Reward-Scoped Group Relative Policy Optimization (RS-GRPO)，通过将细粒度奖励与特定范围的token绑定，共同优化VLM的视觉感知和推理能力。

Result: 在多个视觉问答基准测试上，EVisRAG相较于基础VLM实现了显著的端到端性能提升，平均提高27%。

Conclusion: EVisRAG通过RS-GRPO的赋能，能够精确感知并定位多图像中与问题相关的证据，并从中得出最终答案，从而有效提升了答案准确性，解决了VRAG在多图像证据整合方面的挑战。

Abstract: Visual retrieval-augmented generation (VRAG) augments vision-language models
(VLMs) with external visual knowledge to ground reasoning and reduce
hallucinations. Yet current VRAG systems often fail to reliably perceive and
integrate evidence across multiple images, leading to weak grounding and
erroneous conclusions. In this paper, we propose EVisRAG, an end-to-end
framework that learns to reason with evidence-guided multi-image to address
this issue. The model first observes retrieved images and records per-image
evidence, then derives the final answer from the aggregated evidence. To train
EVisRAG effectively, we introduce Reward-Scoped Group Relative Policy
Optimization (RS-GRPO), which binds fine-grained rewards to scope-specific
tokens to jointly optimize visual perception and reasoning abilities of VLMs.
Experimental results on multiple visual question answering benchmarks
demonstrate that EVisRAG delivers substantial end-to-end gains over backbone
VLM with 27\% improvements on average. Further analysis shows that, powered by
RS-GRPO, EVisRAG improves answer accuracy by precisely perceiving and
localizing question-relevant evidence across multiple images and deriving the
final answer from that evidence, much like a real detective.

</details>


### [10] [Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement](https://arxiv.org/abs/2510.09738)
*Steve Han,Gilberto Titericz Junior,Tom Balough,Wenfei Zhou*

Main category: cs.CL

TL;DR: 本研究提出一种名为“法官裁决基准”的新型两步方法，用于评估大型语言模型（LLM）在响应准确性评估中作为法官的能力，并发现部分LLM能达到人类判断水平，且其优秀表现与训练策略而非模型大小更相关。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM作为法官的方法（如仅依赖相关性）不足以全面衡量其判断能力；需要开发一种更精确、能区分不同判断模式（如类人型或超一致型）的评估框架，以更好地理解和利用LLM在评估任务中的潜力。

Method: 引入“法官裁决基准”两步法：(1) 使用相关性测试初步筛选与人类判断高度一致的LLM；(2) 通过Cohen's Kappa分析计算一致性模式，并利用z-score进行“类人相似性测试”，将LLM判断分为“类人型”（|z|<1）和“超一致型”（z>1）。该方法评估了54个LLM对RAG或Agentic管道响应与真实答案的准确性判断能力。

Result: 在54个测试的LLM中，有27个达到了Tier 1性能：其中23个展现出“类人型”判断模式，4个展现出“超一致型”判断模式。研究表明，LLM作为法官的卓越表现主要取决于特定的训练策略，而非模型规模。此外，仅依赖相关性不足以充分评估LLM法官。

Conclusion: 本研究确立了仅凭相关性不足以评估LLM法官的观点；提出了一种基于一致性模式的“法官图灵测试”；并提供了一个标准化的基准，可根据LLM法官的性能将其分类，以适应不同的评估需求。

Abstract: This research introduces the Judge's Verdict Benchmark, a novel two-step
methodology to evaluate Large Language Models (LLMs) as judges for response
accuracy evaluation tasks. We assess how well 54 LLMs can replicate human
judgment when scoring responses from RAG (Retrieval-Augmented Generation) or
Agentic pipelines against ground truth answers. Our methodology progresses from
traditional correlation analysis to comprehensive Cohen's Kappa analysis that
measures actual agreement patterns. The two-step approach includes: (1) a
correlation test that filters judges with strong alignment, followed by (2) a
human-likeness test using z-scores to identify two distinct judgment patterns:
human-like judgment (|z| < 1) that mimics natural human variation, and
super-consistent judgment (z > 1) that exceeds typical human-to-human agreement
levels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1
performance: 23 models exhibit human-like patterns that preserve the nuances of
human judgment, while 4 models demonstrate super-consistent behavior, a pattern
that could indicate either enhanced reliability or oversimplification of
complex judgments. Testing 43 open-source models (1B-405B parameters) and 11
closed models (GPT, Gemini, Claude variants), we demonstrate that judge
excellence is not solely dependent on model size but on specific training
strategies. Our key contributions include: (1) establishing that correlation
alone is insufficient for judge evaluation, (2) introducing a "Turing Test for
judges" based on agreement patterns, and (3) providing a standardized benchmark
for classifying LLM judges into distinct performance tiers for different
evaluation needs.

</details>


### [11] [Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning](https://arxiv.org/abs/2510.09770)
*Adam Byerly,Daniel Khashabi*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models exhibit a strong position bias in multi-document
contexts, systematically prioritizing information based on location rather than
relevance. While existing approaches treat this bias as noise to be mitigated,
we introduce Gold Panning Bandits, a framework that leverages position bias as
a diagnostic signal: by reordering documents and observing shifts in the
model's responses, we can efficiently identify the most relevant content. We
frame the problem of choosing reorderings as a bipartite matching problem.
While an optimal assignment can be computed at each iteration with the
Hungarian algorithm in $O(N^3)$ time, we propose a greedy $O(N \log N)$
strategy that achieves comparable performance by prioritizing the placement of
the most uncertain documents in the most informative positions. Our approach
identifies relevant documents using up to 65\% fewer language model queries
than random permutation baselines on knowledge-intensive NLP tasks,
substantially reducing computational cost without model retraining. This work
demonstrates that inherent LLM biases can be transformed from liabilities into
assets for efficient, inference-time optimization.

</details>


### [12] [PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.09771)
*Rakib Hossan,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 提出PromptGuard，一个少样本框架，通过卡方统计关键词提取和自适应多数投票，解决了低资源语言孟加拉语仇恨言论分类中数据稀缺问题，并达到了67.61的微F1分数。


<details>
  <summary>Details</summary>
Motivation: BLP-2025任务1A要求对孟加拉语仇恨言论进行六类别分类，但传统监督方法需要大量标注数据，这对于低资源语言而言成本高昂且难以获取。

Method: 开发了PromptGuard框架，结合卡方统计分析进行关键词提取和自适应多数投票进行决策。研究比较了统计关键词选择与随机方法，并探索了基于共识质量的自适应投票机制。

Result: PromptGuard的微F1分数为67.61，显著优于n-gram基线（60.75）和随机方法（14.65）。卡方关键词在各类别中均提供了一致改进，自适应投票则有助于处理模糊案例。

Conclusion: PromptGuard通过统计关键词提取和自适应投票，为低资源语言的少样本仇恨言论分类提供了一种有效方法，其中基于卡方统计的关键词选择具有最持续的影响。

Abstract: The BLP-2025 Task 1A requires Bengali hate speech classification into six
categories. Traditional supervised approaches need extensive labeled datasets
that are expensive for low-resource languages. We developed PromptGuard, a
few-shot framework combining chi-square statistical analysis for keyword
extraction with adaptive majority voting for decision-making. We explore
statistical keyword selection versus random approaches and adaptive voting
mechanisms that extend classification based on consensus quality. Chi-square
keywords provide consistent improvements across categories, while adaptive
voting benefits ambiguous cases requiring extended classification rounds.
PromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines
(60.75) and random approaches (14.65). Ablation studies confirm
chi-square-based keywords show the most consistent impact across all
categories.

</details>


### [13] [Steering Embedding Models with Geometric Rotation: Mapping Semantic Relationships Across Languages and Models](https://arxiv.org/abs/2510.09790)
*Michael Freenor,Lauren Alvarez*

Main category: cs.CL

TL;DR: 本文引入RISE几何方法，将话语级语义转换建模为多语言嵌入空间中的一致旋转操作，并在多语言、多模型下首次证实了其一致性。


<details>
  <summary>Details</summary>
Motivation: 理解语言和嵌入模型如何编码语义关系对于模型可解释性和控制至关重要。早期词嵌入具有直观的向量算术，但现代高维文本表示缺乏直接可解释的几何特性。

Method: 引入Rotor-Invariant Shift Estimation (RISE) 几何方法，它利用现代语言表示的流形结构，将语义转换表示为嵌入空间中一致的旋转操作。该方法在三个嵌入模型、三个数据集和七种不同形态的语言上进行了评估。

Result: RISE操作在不同语言和模型之间具有高迁移性能，表明存在跨语言的类似几何结构。研究结果表明，RISE能够跨语言和模型一致地映射具有不同语法特征（如否定和条件性）的话语级语义转换。

Conclusion: 本工作首次系统地证明了话语级语义转换在多语言嵌入空间中对应一致的几何操作，从经验上支持了句子层面的线性表示假设。

Abstract: Understanding how language and embedding models encode semantic relationships
is fundamental to model interpretability and control. While early word
embeddings exhibited intuitive vector arithmetic (''king'' - ''man'' +
''woman'' = ''queen''), modern high-dimensional text representations lack
straightforward interpretable geometric properties. We introduce
Rotor-Invariant Shift Estimation (RISE), a geometric approach that represents
semantic transformations as consistent rotational operations in embedding
space, leveraging the manifold structure of modern language representations.
RISE operations have the ability to operate across both languages and models
with high transfer of performance, suggesting the existence of analogous
cross-lingual geometric structure. We evaluate RISE across three embedding
models, three datasets, and seven morphologically diverse languages in five
major language groups. Our results demonstrate that RISE consistently maps
discourse-level semantic transformations with distinct grammatical features
(e.g., negation and conditionality) across languages and models. This work
provides the first systematic demonstration that discourse-level semantic
transformations correspond to consistent geometric operations in multilingual
embedding spaces, empirically supporting the Linear Representation Hypothesis
at the sentence level.

</details>


### [14] [Text Prompt Injection of Vision Language Models](https://arxiv.org/abs/2510.09849)
*Ruizhe Zhu*

Main category: cs.CL

TL;DR: 研究了针对大型视觉语言模型的文本提示注入攻击，并开发了一种高效且计算资源需求低的攻击算法。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型的广泛应用带来了严重的安全隐患，尤其关注其被误导的风险。

Method: 通过研究文本提示注入作为误导模型的方法，并为此类攻击开发了一种算法。

Result: 实验证明了所开发算法的有效性和效率，特别适用于大型模型且对计算资源要求不高。

Conclusion: 文本提示注入是一种有效且对计算资源需求低的攻击大型视觉语言模型的方法。

Abstract: The widespread application of large vision language models has significantly
raised safety concerns. In this project, we investigate text prompt injection,
a simple yet effective method to mislead these models. We developed an
algorithm for this type of attack and demonstrated its effectiveness and
efficiency through experiments. Compared to other attack methods, our approach
is particularly effective for large models without high demand for
computational resources.

</details>


### [15] [NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering](https://arxiv.org/abs/2510.09854)
*Kaiwen Shi,Zheyuan Zhang,Zhengqing Yuan,Keerthiram Murugesan,Vincent Galass,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: 本文提出NG-Router框架，通过将营养问答建模为知识图谱引导的多智能体协作问题，并结合图神经网络路由和基于梯度的子图检索机制，有效解决了现有单智能体推理能力有限和上下文过载等挑战，在营养健康任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 个性化膳食指导和慢性病预防是营养问答（QA）的重要应用。然而，现有方法面临三大挑战：单智能体系统推理能力有限、多智能体架构设计复杂以及上下文过载导致决策不准确。

Method: 引入Nutritional-Graph Router (NG-Router) 框架，将营养QA公式化为受监督、知识图谱引导的多智能体协作问题。NG-Router将智能体节点整合到异构知识图中，并利用图神经网络学习基于经验智能体表现的软监督任务感知路由分布。为解决上下文过载，提出一种基于梯度的子图检索机制，在训练过程中识别显著证据，增强多跳和关系推理。

Result: NG-Router在多个基准测试和主干模型上进行的广泛实验表明，它始终优于单智能体和集成基线方法。

Conclusion: NG-Router为复杂营养健康任务的领域感知多智能体推理提供了一种原则性方法，在营养QA中取得了显著改进。

Abstract: Diet plays a central role in human health, and Nutrition Question Answering
(QA) offers a promising path toward personalized dietary guidance and the
prevention of diet-related chronic diseases. However, existing methods face two
fundamental challenges: the limited reasoning capacity of single-agent systems
and the complexity of designing effective multi-agent architectures, as well as
contextual overload that hinders accurate decision-making. We introduce
Nutritional-Graph Router (NG-Router), a novel framework that formulates
nutritional QA as a supervised, knowledge-graph-guided multi-agent
collaboration problem. NG-Router integrates agent nodes into heterogeneous
knowledge graphs and employs a graph neural network to learn task-aware routing
distributions over agents, leveraging soft supervision derived from empirical
agent performance. To further address contextual overload, we propose a
gradient-based subgraph retrieval mechanism that identifies salient evidence
during training, thereby enhancing multi-hop and relational reasoning.
Extensive experiments across multiple benchmarks and backbone models
demonstrate that NG-Router consistently outperforms both single-agent and
ensemble baselines, offering a principled approach to domain-aware multi-agent
reasoning for complex nutritional health tasks.

</details>


### [16] [NarraBench: A Comprehensive Framework for Narrative Benchmarking](https://arxiv.org/abs/2510.09869)
*Sil Hamilton,Matthew Wilkens,Andrew Piper*

Main category: cs.CL

TL;DR: 本文提出NarraBench，一个叙事理解任务分类法及对78个现有基准的调查，发现当前LLM叙事理解评估存在严重空白，尤其在主观和视角方面。


<details>
  <summary>Details</summary>
Motivation: 现有用于评估大语言模型（LLM）叙事理解的基准存在不足，许多叙事理解的关键方面被忽视或与现有指标不符。

Method: 开发了NarraBench，一个基于理论的叙事理解任务分类法，并对该领域78个现有基准进行了系统调查。

Result: 研究发现，估计只有27%的叙事任务被现有基准充分涵盖。叙事事件、风格、视角和启示等关键领域几乎未被现有评估涉及。同时，亟需开发能评估本质上主观和视角性叙事方面的基准。

Conclusion: NarraBench的分类法、调查和方法论对NLP研究人员测试LLM叙事理解具有重要价值，为未来基准开发指明了方向。

Abstract: We present NarraBench, a theory-informed taxonomy of narrative-understanding
tasks, as well as an associated survey of 78 existing benchmarks in the area.
We find significant need for new evaluations covering aspects of narrative
understanding that are either overlooked in current work or are poorly aligned
with existing metrics. Specifically, we estimate that only 27% of narrative
tasks are well captured by existing benchmarks, and we note that some areas --
including narrative events, style, perspective, and revelation -- are nearly
absent from current evaluations. We also note the need for increased
development of benchmarks capable of assessing constitutively subjective and
perspectival aspects of narrative, that is, aspects for which there is
generally no single correct answer. Our taxonomy, survey, and methodology are
of value to NLP researchers seeking to test LLM narrative understanding.

</details>


### [17] [CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs](https://arxiv.org/abs/2510.09871)
*Nafiseh Nikeghbal,Amir Hossein Kargaran,Jana Diesner*

Main category: cs.CL

TL;DR: 研究引入CoBia，一种对抗性攻击方法，系统地揭示大型语言模型（LLMs）在对话中深层嵌入的偏见，即使它们已通过标准安全检查。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在模型构建中包含了强化安全防护措施并能通过标准安全检查，但它们在对话中仍会偶然表现出有害行为，例如表达种族主义观点。因此，需要一种系统性方法来分析LLMs偏离规范或道德行为的条件范围。

Method: 研究引入了CoBia，一套轻量级对抗性攻击工具。CoBia通过构建对话，让模型首先发表一个关于社会群体的偏见言论，然后评估模型是否能从这种伪造的偏见言论中恢复并拒绝带有偏见的后续问题。研究评估了11个开源和专有LLM，涉及性别、种族、宗教、国籍、性取向等六个社会人口学类别，并基于既定的LLM偏见指标，将结果与人工判断进行比较。

Result: 结果表明，精心构建的对话能够可靠地揭示偏见放大现象。LLMs在对话中经常未能拒绝带有偏见的后续问题。这种压力测试揭示了模型中通过交互可能被激发的深层嵌入的偏见。

Conclusion: LLMs即使通过了安全检查，其内部仍存在深层偏见，这些偏见在特定对话场景下（特别是通过CoBia的压力测试）会被放大和揭示，且模型通常难以从偏见言论中恢复并拒绝后续偏见内容，这强调了对话交互中偏见检测和纠正的挑战。

Abstract: Improvements in model construction, including fortified safety guardrails,
allow Large language models (LLMs) to increasingly pass standard safety checks.
However, LLMs sometimes slip into revealing harmful behavior, such as
expressing racist viewpoints, during conversations. To analyze this
systematically, we introduce CoBia, a suite of lightweight adversarial attacks
that allow us to refine the scope of conditions under which LLMs depart from
normative or ethical behavior in conversations. CoBia creates a constructed
conversation where the model utters a biased claim about a social group. We
then evaluate whether the model can recover from the fabricated bias claim and
reject biased follow-up questions. We evaluate 11 open-source as well as
proprietary LLMs for their outputs related to six socio-demographic categories
that are relevant to individual safety and fair treatment, i.e., gender, race,
religion, nationality, sex orientation, and others. Our evaluation is based on
established LLM-based bias metrics, and we compare the results against human
judgments to scope out the LLMs' reliability and alignment. The results suggest
that purposefully constructed conversations reliably reveal bias amplification
and that LLMs often fail to reject biased follow-up questions during dialogue.
This form of stress-testing highlights deeply embedded biases that can be
surfaced through interaction. Code and artifacts are available at
https://github.com/nafisenik/CoBia.

</details>


### [18] [iBERT: Interpretable Style Embeddings via Sense Decomposition](https://arxiv.org/abs/2510.09882)
*Vishal Anand,Milad Alshomary,Kathleen McKeown*

Main category: cs.CL

TL;DR: iBERT是一种可解释且可控的BERT编码器，通过将语言中的判别性线索（如风格和语义）模块化为稀疏、非负的感官向量混合，提供透明的嵌入表示，并在风格相关任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有BERT风格的模型缺乏对嵌入表示的解释性和可控性，无法明确揭示语言中（如风格和语义结构）的判别性线索。研究旨在构建一个能模块化并暴露这些线索的编码器。

Method: iBERT将每个输入token表示为k个上下文无关感官向量的稀疏、非负混合。这些感官向量可以汇聚成句子嵌入，也可以直接在token级别使用，从而在解码或下游应用之前实现对表示的模块化控制。

Result: 在STEL基准测试中，iBERT将风格表示的有效性比SBERT风格基线提高了约8个百分点，同时在作者验证任务上保持了竞争力。模型能够将特定的风格属性（如表情符号使用、正式性、拼写错误）分配给特定的感官向量。

Conclusion: iBERT的结构模块化设计使其能够可解释地分解数据中的判别信号，不仅限于风格建模，还能泛化到风格和语义因素混合的情况，为解释语言表示提供了新途径。

Abstract: We present iBERT (interpretable-BERT), an encoder to produce inherently
interpretable and controllable embeddings - designed to modularize and expose
the discriminative cues present in language, such as stylistic and semantic
structure. Each input token is represented as a sparse, non-negative mixture
over k context-independent sense vectors, which can be pooled into sentence
embeddings or used directly at the token level. This enables modular control
over representation, before any decoding or downstream use.
  To demonstrate our model's interpretability, we evaluate it on a suite of
style-focused tasks. On the STEL benchmark, it improves style representation
effectiveness by ~8 points over SBERT-style baselines, while maintaining
competitive performance on authorship verification. Because each embedding is a
structured composition of interpretable senses, we highlight how specific style
attributes - such as emoji use, formality, or misspelling can be assigned to
specific sense vectors. While our experiments center on style, iBERT is not
limited to stylistic modeling. Its structural modularity is designed to
interpretably decompose whichever discriminative signals are present in the
data - enabling generalization even when supervision blends stylistic and
semantic factors.

</details>


### [19] [DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context Reasoning](https://arxiv.org/abs/2510.09883)
*Hossein Entezari Zarch,Lei Gao,Chaoyi Jiang,Murali Annavarm*

Main category: cs.CL

TL;DR: DELTA是一种无需训练的稀疏注意力机制，通过分层注意力设计，在不牺牲准确性的前提下，显著提高了大型推理模型在长序列推理任务中的计算效率和速度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在长链推理任务中表现出色，但解码成本高昂。现有稀疏注意力方法通过剪枝KV缓存降低计算，但由于累积选择错误和标记动态重要性，在推理任务中导致严重的准确性下降。

Method: 提出DELTA机制，这是一种无需训练的稀疏注意力方法。它将Transformer层分为三组：初始层使用全注意力；一小组“选择层”通过聚合的头部注意力分数识别关键标记；随后的“稀疏注意力层”仅关注选定的子集。该设计在GPU内存中保留完整的KV缓存以确保准确性，同时避免了多层昂贵的全面注意力计算。

Result: 在AIME和GPQA-Diamond等推理基准测试中，DELTA的准确性与全注意力持平或超越，同时将关注的标记数量减少了高达5倍，并实现了1.5倍的端到端加速。

Conclusion: 研究结果表明，选择性地重用中间注意力图为高效的长上下文推理提供了一条可靠的途径。

Abstract: Large reasoning models (LRMs) achieve state-of-the-art performance on
challenging benchmarks by generating long chains of intermediate steps, but
their inference cost is dominated by decoding, where each new token must attend
to the entire growing sequence. Existing sparse attention methods reduce
computation by pruning the key-value (KV) cache, yet they suffer from severe
accuracy degradation on reasoning tasks due to cumulative selection errors and
the dynamic importance of tokens over long derivations. We present
\textbf{DELTA}, a training-free sparse attention mechanism that achieves
computational efficiency without sacrificing model accuracy. DELTA partitions
transformer layers into three groups: initial layers that use full attention, a
small set of \emph{selection layers} that identify salient tokens via
aggregated head-level attention scores, and subsequent \emph{sparse-attention
layers} that attend only to the selected subset. This design preserves the full
KV cache in GPU memory for accuracy, while avoiding expensive full-attention
computation over many layers. On reasoning benchmarks such as AIME and
GPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while
reducing the number of attended tokens by up to $5\times$ and delivering
$1.5\times$ end-to-end speedup. Our results show that selective reuse of
intermediate attention maps offers a robust path toward efficient long-context
reasoning.

</details>


### [20] [Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs](https://arxiv.org/abs/2510.09885)
*Xu Pan,Ely Hahami,Jingxuan Fan,Ziqian Xie,Haim Sompolinsky*

Main category: cs.CL

TL;DR: 研究比较了自回归大语言模型（arLLM）和掩码扩散大语言模型（dLLM）在微调注入知识方面的能力，发现dLLM在数据效率和“反转诅咒”上表现更优，并提出了一种新的掩码微调范式显著提升了arLLM的性能。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型（arLLM）在知识注入微调时存在“反转诅咒”等固有缺陷，难以回答信息顺序颠倒的问题。掩码扩散大语言模型（dLLM）在预训练阶段表现出更好的数据效率且无“反转诅咒”，但其在微调阶段获取新知识的能力尚不明确。

Method: 在三个不同数据集上对arLLM和dLLM进行微调，并使用正向和反向风格的问题回答（QA）进行评估，以探究知识泛化能力和“反转诅咒”。受dLLM启发，提出了一种新的掩码微调范式用于预训练arLLM的知识注入。

Result: arLLM在QA泛化上严重依赖于大量的释义数据增强，且释义仅在信息顺序与QA风格匹配时有效。相比之下，dLLM在没有释义的情况下，在正向和反向QA上均取得了高准确率，添加释义仅带来微小提升。提出的新型掩码微调范式显著提高了arLLM微调的数据效率，有效地缩小了与dLLM的性能差距。

Conclusion: dLLM在微调阶段表现出更好的知识注入能力和对“反转诅咒”的免疫力，且数据效率更高。受dLLM启发，通过引入掩码微调范式，可以显著提升arLLM在知识注入方面的效率和性能。

Abstract: Despite autoregressive large language models (arLLMs) being the current
dominant paradigm in language modeling, they resist knowledge injection via
fine-tuning due to inherent shortcomings such as the "reversal curse" -- the
challenge of answering questions that reverse the original information order in
the training sample. Masked diffusion large language models (dLLMs) are rapidly
emerging as a powerful alternative to the arLLM paradigm, with evidence of
better data efficiency and free of the "reversal curse" in pre-training.
However, it is unknown whether these advantages extend to the post-training
phase, i.e. whether pre-trained dLLMs can easily acquire new knowledge through
fine-tuning. On three diverse datasets, we fine-tune arLLMs and dLLMs,
evaluating them with forward and backward style Question Answering (QA) to
probe knowledge generalization and the reversal curse. Our results confirm that
arLLMs critically rely on extensive data augmentation via paraphrases for QA
generalization, and paraphrases are only effective when their information order
matches the QA style. Conversely, dLLMs achieve high accuracies on both forward
and backward QAs without paraphrases; adding paraphrases yields only marginal
gains. Lastly, inspired by the dLLM's performance, we introduce a novel masked
fine-tuning paradigm for knowledge injection into pre-trained arLLMs. This
proposed method successfully and drastically improves the data efficiency of
arLLM fine-tuning, effectively closing the performance gap with dLLMs.

</details>


### [21] [Abductive Preference Learning](https://arxiv.org/abs/2510.09887)
*Yijin Ni,Peng Qi*

Main category: cs.CL

TL;DR: LLMs在对齐后仍存在过度自信，未能区分反事实提示。本文提出溯因偏好学习，通过学习给定响应下的提示偏好，并结合多任务DPOP，显著提升模型对提示差异的敏感性，同时保持传统偏好优化的优势。


<details>
  <summary>Details</summary>
Motivation: 前沿大型语言模型（如GPT-5）在RLHF/DPO对齐后仍存在过度自信，对相似但应有不同响应的提示（例如关于隔夜食物安全性的问题）给出相同保守回答。这归因于现有偏好学习侧重于选择给定提示的正确响应，却忽视了应改变响应的反事实提示。

Method: 本文提出溯因偏好学习，一种通过在给定响应下学习提示偏好来颠倒传统条件作用的微调范式。为此，构建了一个包含1,001条条目的溯因数据集（源自HaluEval QA），并实现了溯因DPO及其变体DPOP，同时采用了一个结合标准方法和溯因方法的多任务目标。

Result: 实验显示，标准方法改善响应选择，溯因方法改善提示判别。在溯因数据集上，多任务DPOP显著提高了响应选择准确率（从90.0%到99.5%）和提示判别准确率（从54.7%到85.0%），定性证据表明其对提示差异的敏感性增强。此外，在AlpacaEval上，多任务DPOP提升了胜率（从5.26%到6.17%）。

Conclusion: 溯因偏好学习能够解决现有偏好学习忽视反事实提示的局限性。多任务DPOP统一了传统偏好优化的优点并解决了反事实提示的挑战，有效提升了大型语言模型对提示差异的敏感性和整体性能。

Abstract: Frontier large language models such as GPT-5 and Claude Sonnet remain prone
to overconfidence even after alignment through Reinforcement Learning with
Human Feedback (RLHF) and Direct Preference Optimization (DPO). For instance,
they tend to offer the same conservative answer "No" to both questions "Can I
eat the [food / potato chips] that has been left out overnight?" despite the
latter requiring no refridgeration for safe consumption. We find that this
failure is potentially attributed to a limitation of existing preference
learning: it emphasizes selecting the correct response for a given prompt,
while neglecting counterfactual prompts that should alter the response.
  To address this limitation, we propose abductive preference learning, a
fine-tuning paradigm that reverses the conventional conditioning by learning
preferences over prompts given a response. To validate this idea, we construct
an abductive dataset derived from the HaluEval QA benchmark with 1,001 entries,
implementing abductive DPO and its variant DPOP. Experiments reveal
complementary strengths: standard methods improve response selection, abductive
methods improve prompt discrimination, while a multitask objective unifies
both. On the abductive dataset, multitask DPOP boosts accuracy from $90.0\%$ to
$99.5\%$ in response selection and $54.7\%$ to $85.0\%$ in prompt
discrimination, with qualitative evidence highlighting improved sensitivity to
prompt differences. Finally, evaluation on AlpacaEval shows multitask DPOP
improves win rate (from $5.26\%$ to $6.17\%$), confirming that abductive
preference learning preserves the benefits of conventional preference
optimization while addressing the overlooked challenge of counterfactual
prompts.

</details>


### [22] [HIPPD: Brain-Inspired Hierarchical Information Processing for Personality Detection](https://arxiv.org/abs/2510.09893)
*Guanming Chen,Lingzhi Shen,Xiaohao Cai,Imran Razzak,Shoaib Jameel*

Main category: cs.CL

TL;DR: 本文提出HIPPD，一个受大脑启发的个性检测框架，结合大型语言模型、动态记忆模块和专门轻量级模型，在文本个性检测任务中超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在从文本中检测个性时，难以捕捉跨多个帖子的上下文信息，并且在语义稀疏环境中难以提取具有代表性和鲁棒性的特征。

Method: HIPPD框架模拟人脑的层次信息处理：利用大型语言模型模拟大脑皮层进行全局语义推理和深层特征抽象；动态记忆模块（模拟前额叶皮层）通过多巴胺预测误差反馈实现自适应门控和关键特征选择性保留；一组专门的轻量级模型（模拟基底神经节）通过“赢者通吃”机制动态路由，捕捉其擅长识别的个性相关模式。

Result: 在Kaggle和Pandora数据集上进行的广泛实验表明，HIPPD始终优于现有最先进的基线方法。

Conclusion: HIPPD提供了一个新颖且高效的受大脑启发的个性检测框架，解决了传统方法的局限性，并实现了卓越的性能。

Abstract: Personality detection from text aims to infer an individual's personality
traits based on linguistic patterns. However, existing machine learning
approaches often struggle to capture contextual information spanning multiple
posts and tend to fall short in extracting representative and robust features
in semantically sparse environments. This paper presents HIPPD, a
brain-inspired framework for personality detection that emulates the
hierarchical information processing of the human brain. HIPPD utilises a large
language model to simulate the cerebral cortex, enabling global semantic
reasoning and deep feature abstraction. A dynamic memory module, modelled after
the prefrontal cortex, performs adaptive gating and selective retention of
critical features, with all adjustments driven by dopaminergic prediction error
feedback. Subsequently, a set of specialised lightweight models, emulating the
basal ganglia, are dynamically routed via a strict winner-takes-all mechanism
to capture the personality-related patterns they are most proficient at
recognising. Extensive experiments on the Kaggle and Pandora datasets
demonstrate that HIPPD consistently outperforms state-of-the-art baselines.

</details>


### [23] [Don't Throw Away Your Pretrained Model](https://arxiv.org/abs/2510.09913)
*Shangbin Feng,Wenhao Yu,Yike Wang,Hongming Zhang,Yulia Tsvetkov,Dong Yu*

Main category: cs.CL

TL;DR: 论文提出“切换生成”方法，通过训练一个切换器语言模型，动态结合对齐模型和预训练模型在不同技能上的优势，以克服对齐训练的取舍问题，显著提升语言模型在多任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 对齐训练虽然增强了语言模型（LMs）的推理和指令遵循能力，但可能牺牲了创造力和校准等技能。研究旨在通过模型协作，整合对齐模型和未对齐基础模型的优点，实现“两全其美”。

Method: 提出“切换生成（Switch Generation）”方法。该方法训练一个“切换器语言模型（switcher LM）”，通过学习在不同查询和上下文中选择不同模型（预训练模型或对齐模型）生成下一段内容的成果，来动态指导这些模型交替生成响应序列。在推理时，switcher LM根据需要动态选择最擅长生成当前片段的模型。

Result: 在8种模型协作基线和18个数据集上进行广泛实验：
1. 模型协作在18个任务中的16个上持续优于单一模型。
2. 切换生成方法平均比现有基线方法表现高出12.9%。

Conclusion: 切换生成方法能够发现组合技能以解决单一模型难以应对的问题，并能泛化到未见过的模型和任务。它还有效地重新利用了昂贵模型训练管道中可能被丢弃的副产品，证明了通过动态协作实现模型优势互补的有效性。

Abstract: Alignment training has tradeoffs: it helps language models (LMs) gain in
reasoning and instruction following but might lose out on skills such as
creativity and calibration, where unaligned base models are better at. We aim
to make the best of both worlds through model collaboration, where different
models in the training pipeline collaborate and complement each other. Since LM
responses feature interleaving skills that favor different models, we propose
Switch Generation, where pretrained and aligned model versions take turns to
``speak'' in a response sequence. Specifically, we train a switcher LM by
learning from outcomes of choosing different models to generate the next
segment across diverse queries and contexts. At inference time, the switcher LM
guides different model checkpoints to dynamically generate the next segment
where their strengths are most needed. Extensive experiments with 8 model
collaboration baselines and 18 datasets show that 1) model collaboration
consistently outperforms individual models on 16 out of 18 tasks, and 2) Switch
Generation further outperforms baselines by 12.9% on average. Further analysis
reveals that Switch Generation discovers compositional skills to solve problems
where individual models struggle and generalizes to unseen models and tasks,
reusing and repurposing by-products in expensive model training pipelines that
are otherwise discarded.

</details>


### [24] [Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning](https://arxiv.org/abs/2510.09915)
*Sicong Huang,Qianqi Yan,Shengze Wang,Ian Lane*

Main category: cs.CL

TL;DR: 本研究通过GPT-4o自动标注LLM生成摘要中的幻觉片段，构建了带片段级标签的新数据集。在此基础上，探究了梯度上升、负向训练和任务向量求反三种微调策略，以提高LLM摘要的忠实性，结果显示负向训练效果最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在抽象式摘要生成方面表现出色，但它们经常产生不忠实的摘要，引入词、短语或概念层面的幻觉。现有缓解策略（如后处理或对比学习）无法充分解决这些多样化错误，因此需要新的方法来减少不忠实片段的发生。

Method: 1. 自动生成训练集源文档的LLM摘要。2. 使用GPT-4o对这些摘要进行片段级幻觉标注。3. 基于标注数据，构建包含忠实和不忠实摘要的新数据集。4. 采用梯度上升、负向训练和任务向量求反三种技术，利用忠实摘要和带标注的不忠实片段来微调LLMs，以增强模型忠实性。

Result: 实验结果表明，梯度上升、负向训练和任务向量求反这三种方法都能成功利用片段级标注来提高摘要的忠实性。其中，负向训练被证实是最有效的方法。

Conclusion: 通过利用GPT-4o生成的片段级幻觉标注进行微调，可以显著提高大型语言模型生成摘要的忠实性，特别是负向训练表现出卓越的效果，为减少LLM摘要中的幻觉提供了一条有效途径。

Abstract: Abstractive summarization using large language models (LLMs) has become an
essential tool for condensing information. However, despite their ability to
generate fluent summaries, these models sometimes produce unfaithful summaries,
introducing hallucinations at the word, phrase, or concept level. Existing
mitigation strategies, such as post-processing corrections or contrastive
learning with synthetically generated negative samples, fail to fully address
the diverse errors that can occur in LLM-generated summaries. In this paper, we
investigate fine-tuning strategies to reduce the occurrence of unfaithful spans
in generated summaries. First, we automatically generate summaries for the set
of source documents in the training set with a variety of LLMs and then use
GPT-4o to annotate any hallucinations it detects at the span-level. Leveraging
these annotations, we fine-tune LLMs with both hallucination-free summaries and
annotated unfaithful spans to enhance model faithfulness. In this paper, we
introduce a new dataset that contains both faithful and unfaithful summaries
with span-level labels and we evaluate three techniques to fine-tuning a LLM to
improve the faithfulness of the resulting summarization: gradient ascent,
unlikelihood training, and task vector negation. Experimental results show that
all three approaches successfully leverage span-level annotations to improve
faithfulness, with unlikelihood training being the most effective.

</details>


### [25] [Unpacking Hateful Memes: Presupposed Context and False Claims](https://arxiv.org/abs/2510.09935)
*Weibin Cai,Jiayu Li,Reza Zafarani*

Main category: cs.CL

TL;DR: 本研究提出SHIELD框架，通过理解仇恨迷因的“预设语境”和“虚假陈述”两大本质特征，利用PCM模块建模语境和FACT模块检测虚假信息，显著超越现有方法，并适用于假新闻检测等任务。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨迷因检测方法主要依赖预训练语言模型，但较少关注“是什么使迷因变得可恨”。鉴于仇恨迷因对个人和社会造成严重危害，本研究旨在深入理解其本质，以开发更有效的检测方法。

Method: 基于哲学和心理学见解，将仇恨迷因定义为具有“预设语境”和“虚假陈述”两大特征。开发了：1. **PCM**模块捕捉跨模态语境信息。2. **FACT**模块结合外部知识和跨模态参考图检测虚假陈述。3. 将PCM和FACT结合，构建了名为**SHIELD**的仇恨迷因检测框架。

Result: SHIELD在多个数据集和指标上均超越了最先进的方法。同时，SHIELD在其他任务（如假新闻检测）上也展现了其多功能性。

Conclusion: SHIELD框架通过捕捉仇恨迷因的根本性质（预设语境和虚假陈述），有效提升了检测性能，并表现出良好的泛化能力，为仇恨内容检测提供了深入的理解和新颖的解决方案。

Abstract: While memes are often humorous, they are frequently used to disseminate hate,
causing serious harm to individuals and society. Current approaches to hateful
meme detection mainly rely on pre-trained language models. However, less focus
has been dedicated to \textit{what make a meme hateful}. Drawing on insights
from philosophy and psychology, we argue that hateful memes are characterized
by two essential features: a \textbf{presupposed context} and the expression of
\textbf{false claims}. To capture presupposed context, we develop \textbf{PCM}
for modeling contextual information across modalities. To detect false claims,
we introduce the \textbf{FACT} module, which integrates external knowledge and
harnesses cross-modal reference graphs. By combining PCM and FACT, we introduce
\textbf{\textsf{SHIELD}}, a hateful meme detection framework designed to
capture the fundamental nature of hate. Extensive experiments show that SHIELD
outperforms state-of-the-art methods across datasets and metrics, while
demonstrating versatility on other tasks, such as fake news detection.

</details>


### [26] [Beyond Fertility: Analyzing STRR as a Metric for Multilingual Tokenization Evaluation](https://arxiv.org/abs/2510.09947)
*Mir Tafseer Nayeem,Sawsan Alqahtani,Md Tahmid Rahman Laskar,Tasnim Mohiuddin,M Saiful Bari*

Main category: cs.CL

TL;DR: 论文指出LLM分词的重要性及现有评估指标“生育率”的局限。提出新指标“单字元保留率（STRR）”以评估分词器在跨语言公平性上的表现，发现英语优先、汉语支持良好，并为公平分词器设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）中的分词（Tokenization）步骤至关重要但评估不足。现有标准指标“生育率（fertility）”虽能衡量压缩效率，但无法反映词汇在不同语言和领域间的分配情况，限制了对跨语言公平性的理解和评估。

Method: 1. 对六种常用分词器在七种语言和两个领域上的生育率进行了分析。 2. 提出“单字元保留率（Single Token Retention Rate, STRR）”新指标，用于衡量单词被保留为单个字元的比例，以更准确地评估跨语言分词公平性。

Result: 1. 生育率分析显示，英语分词稳定性高，汉语分词率高，且分词对领域敏感度低。 2. STRR揭示了分词器对英语的系统性优先支持、对汉语的强力支持，以及印地语中的碎片化问题。 3. STRR提供了一个可解释的跨语言公平性视图。

Conclusion: STRR能有效补充“生育率”指标的盲点，为设计更公平、更具代表性的多语言分词器提供了实用指导。

Abstract: Tokenization is a crucial but under-evaluated step in large language models
(LLMs). The standard metric, fertility (the average number of tokens per word),
captures compression efficiency but obscures how vocabularies are allocated
across languages and domains. We analyze six widely used tokenizers across
seven languages and two domains, finding stable fertility for English, high
fertility for Chinese, and little domain sensitivity. To address fertility's
blind spots, we propose the Single Token Retention Rate (STRR), which measures
the proportion of words preserved as single tokens. STRR reveals systematic
prioritization of English, strong support for Chinese, and fragmentation in
Hindi, offering an interpretable view of cross-lingual fairness. Our results
show that STRR complements fertility and provides practical guidance for
designing more equitable multilingual tokenizers.

</details>


### [27] [Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey](https://arxiv.org/abs/2510.09988)
*Jiaqi Wei,Xiang Zhang,Yuejin Yang,Wenxuan Huang,Juntai Cao,Sheng Xu,Xiang Zhuang,Zhangyang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Chenyu You,Wanli Ouyang,Siqi Sun*

Main category: cs.CL

TL;DR: 深思熟虑树搜索是LLM研究核心，但该领域碎片化，奖励信号作用模糊。本文提出统一框架，解构搜索算法为三核心组件，并区分瞬态搜索指导和持久参数化奖励建模，进而构建分类法并规划研究路线图。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型（LLM）研究中的深思熟虑树搜索领域存在碎片化问题，缺乏统一的形式化描述，尤其是在奖励信号的角色（瞬态启发式或持久学习目标）上存在模糊性。

Method: 引入一个统一框架，将搜索算法解构为搜索机制、奖励制定和转移函数三个核心组件。该框架形式化地区分了用于测试时扩展（TTS）的瞬态“搜索指导”和用于自我改进的持久“参数化奖励建模”。

Result: 基于所提出的形式化框架，本文建立了一个以组件为中心的分类法，综合了现有最先进技术，并绘制了未来的研究路线图。

Conclusion: 通过提供统一框架、分类法和研究路线图，旨在促进LLM研究在创建自主、自我改进智能体方面取得更系统、更有序的进展。

Abstract: Deliberative tree search is a cornerstone of modern Large Language Model
(LLM) research, driving the pivot from brute-force scaling toward algorithmic
efficiency. This single paradigm unifies two critical frontiers:
\textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve
hard problems, and \textbf{Self-Improvement}, which uses search-generated data
to durably enhance model parameters. However, this burgeoning field is
fragmented and lacks a common formalism, particularly concerning the ambiguous
role of the reward signal -- is it a transient heuristic or a durable learning
target? This paper resolves this ambiguity by introducing a unified framework
that deconstructs search algorithms into three core components: the
\emph{Search Mechanism}, \emph{Reward Formulation}, and \emph{Transition
Function}. We establish a formal distinction between transient \textbf{Search
Guidance} for TTS and durable \textbf{Parametric Reward Modeling} for
Self-Improvement. Building on this formalism, we introduce a component-centric
taxonomy, synthesize the state-of-the-art, and chart a research roadmap toward
more systematic progress in creating autonomous, self-improving agents.

</details>


### [28] [Toward Machine Translation Literacy: How Lay Users Perceive and Rely on Imperfect Translations](https://arxiv.org/abs/2510.09994)
*Yimin Xiao,Yongle Zhang,Dayeon Ki,Calvin Bao,Marianna J. Martindale,Charlotte Vaughn,Ge Gao,Marine Carpuat*

Main category: cs.CL

TL;DR: 研究了普通用户（特别是非双语者）如何感知并过度依赖有缺陷的机器翻译，指出非双语用户易过度依赖，并强调提高用户MT素养的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着机器翻译普及，了解公众如何看待和依赖不完美的机器翻译，对于将MT研究与实际应用结合至关重要。

Method: 在公共博物馆进行了一项人类研究（n=452），调查了流利度和充分性错误如何影响双语和非双语用户在随意使用MT时的依赖性。

Result: 非双语用户因缺乏评估策略和替代方案，常过度依赖MT；而经历错误会促使用户重新评估未来的依赖。

Conclusion: 需要MT评估和NLP解释技术，不仅要提升MT质量，还要提高用户使用MT的素养。

Abstract: As Machine Translation (MT) becomes increasingly commonplace, understanding
how the general public perceives and relies on imperfect MT is crucial for
contextualizing MT research in real-world applications. We present a human
study conducted in a public museum (n=452), investigating how fluency and
adequacy errors impact bilingual and non-bilingual users' reliance on MT during
casual use. Our findings reveal that non-bilingual users often over-rely on MT
due to a lack of evaluation strategies and alternatives, while experiencing the
impact of errors can prompt users to reassess future reliance. This highlights
the need for MT evaluation and NLP explanation techniques to promote not only
MT quality, but also MT literacy among its users.

</details>


### [29] [MTP-S2UT: Enhancing Speech-to-Speech Translation Quality with Multi-token Prediction](https://arxiv.org/abs/2510.10003)
*Jianjin Wang,Runsong Zhao,Xiaoqian Liu,Yuan Ge,Ziqiang Xu,Tong Xiao,Shengxiang Gao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 为解决语音-到-单元翻译(S2UT)中单令牌语义密度不足的问题，本文引入多令牌预测(MTP)损失，并提出MTP-S2UT损失将其应用于中间层隐藏表示，从而显著提升了S2UT的翻译质量。


<details>
  <summary>Details</summary>
Motivation: 现有直接语音-到-语音翻译方法主要使用语义密度低的语音令牌作为中间表示，一个令牌不足以表达完整的语义单元。

Method: 引入多令牌预测(MTP)损失到S2UT模型，使模型能预测多个后续令牌以捕获更完整的语义并提高信息密度。进一步提出MTP-S2UT损失，将其应用于计算CTC损失的中间层隐藏表示，以期实现更早、更有效的隐藏表示增强。

Result: 所有MTP损失变体都持续改进了S2UT翻译质量，其中MTP-S2UT实现了最佳性能。

Conclusion: 通过引入MTP损失，特别是将其应用于中间层隐藏表示的MTP-S2UT方法，可以有效解决语音令牌语义密度问题，并显著提高S2UT翻译的质量。

Abstract: Current direct speech-to-speech translation methods predominantly employ
speech tokens as intermediate representations. However, a single speech token
is not dense in semantics, so we generally need multiple tokens to express a
complete semantic unit. To address this limitation, we introduce multi-token
prediction (MTP) loss into speech-to-unit translation (S2UT) models, enabling
models to predict multiple subsequent tokens at each position, thereby
capturing more complete semantics and enhancing information density per
position. Initial MTP implementations apply the loss at the final layer, which
improves output representation but initiates information enrichment too late.
We hypothesize that advancing the information enrichment process to
intermediate layers can achieve earlier and more effective enhancement of
hidden representation. Consequently, we propose MTP-S2UT loss, applying MTP
loss to hidden representation where CTC loss is computed. Experiments
demonstrate that all MTP loss variants consistently improve the quality of S2UT
translation, with MTP-S2UT achieving the best performance.

</details>


### [30] [Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning](https://arxiv.org/abs/2510.10009)
*Shu Zhao,Tan Yu,Anbang Xu*

Main category: cs.CL

TL;DR: 本文提出了一种名为ExpandSearch的推理增强搜索智能体，通过强化学习实现查询扩展和结合预训练的Squeezer模型，即使是小型LLM也能在多跳问答任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的推理增强搜索智能体（如Search-R1）在多跳问答任务上的性能不佳，因为它们在推理和搜索能力上有限，难以处理复杂查询；同时，有限的训练数据和计算资源使得智能体难以同时掌握查询生成、信息理解和答案生成等多项任务。

Method: 通过强化学习训练一个LLM搜索智能体，使其具备查询扩展的原生能力，在每一轮中生成多个查询变体进行并行搜索以覆盖更多相关信息。此外，引入一个预训练的Squeezer模型来帮助搜索智能体理解检索到的文档，从而使搜索智能体能够专注于查询生成以实现高检索召回率。

Result: 实验结果表明，该方法（ExpandSearch）在七个问答基准测试上，相较于SOTA基线方法平均提升了4.4%，特别是在需要聚合多样证据的多跳推理任务上表现出显著增益。即使是一个小型3B的LLM，在Squeezer模型的辅助下，也能展现出强大的查询扩展能力并在多跳QA基准上达到SOTA精度。

Conclusion: 通过结合强化学习实现的查询扩展能力和预训练Squeezer模型的文档理解辅助，ExpandSearch方法显著提升了搜索智能体在多跳问答任务上的性能，证明了即使是小型LLM也能在复杂QA场景中取得领先成果。

Abstract: Reasoning-augmented search agents, such as Search-R1, are trained to reason,
search, and generate the final answer iteratively. Nevertheless, due to their
limited capabilities in reasoning and search, their performance on multi-hop QA
benchmarks remains far from satisfactory. To handle complex or compound
queries, we train an LLM-based search agent with the native capability of query
expansion through reinforcement learning. In each turn, our search agent
proposes several query variants, which are searched simultaneously to cover
more relevant information. Meanwhile, given limited post-training data and
computing resources, it is very challenging for a search agent to master
multiple tasks, including query generation, retrieved information
understanding, and answer generation. Therefore, we propose incorporating a
pre-trained squeezer model that helps the search agent understand the retrieved
documents, allowing the search agent to focus on query generation for high
retrieval recall. With the assistance of the squeezer model, we discover that
even a small-scale 3B LLM can demonstrate a strong capability of query
expansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks.
To be specific, our experiments across seven question-answering benchmarks
demonstrate that our method, named ExpandSearch, achieves an average
improvement of 4.4% compared to state-of-the-art baselines, with strong gains
on multi-hop reasoning tasks requiring diverse evidence aggregation.

</details>


### [31] [Path Drift in Large Reasoning Models:How First-Person Commitments Override Safety](https://arxiv.org/abs/2510.10013)
*Yuyi Huang,Runzhe Zhan,Lidia S. Chao,Ailin Tao,Derek F. Wong*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）在长链思维（Long-CoT）推理中存在“路径漂移”漏洞，即推理路径偏离安全。文章揭示了触发因素、提出了诱导框架和防御策略，强调了轨迹级对齐的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs通过对齐技术（如RLHF）进行了早期安全防护，但在长链思维推理任务中，其推理轨迹仍可能偏离预设的安全路径，导致生成违反安全约束的内容。这种未被充分探索的漏洞被称为“路径漂移”，是本研究的核心动机。

Method: 1. **识别触发因素：** 通过实证分析，揭示了“路径漂移”的三种行为触发因素：第一人称承诺、伦理蒸发和条件链升级。
2. **构建诱导框架：** 提出一个三阶段的“路径漂移诱导框架”，包括认知负荷放大、自我角色启动和条件链劫持，用于系统性地诱导模型产生漂移。
3. **提出防御策略：** 提出了一种路径级别的防御策略，整合了角色归因纠正和元认知反思（反思性安全提示）以缓解风险。

Result: 1. 成功识别并归纳了导致路径漂移的三种行为触发因素。
2. 所构建的路径漂移诱导框架，其每个阶段都能独立降低模型的拒绝率，并且各阶段组合后能进一步加剧这种效果。
3. （隐含）通过实验验证了路径漂移现象及其诱导和缓解方法。

Conclusion: 研究结果强调，在大语言模型的长篇推理任务中，需要在传统的Token级别对齐之外，引入并加强轨迹级别的对齐监督，以更全面地确保模型安全性。

Abstract: As large language models (LLMs) are increasingly deployed for complex
reasoning tasks, Long Chain-of-Thought (Long-CoT) prompting has emerged as a
key paradigm for structured inference. Despite early-stage safeguards enabled
by alignment techniques such as RLHF, we identify a previously underexplored
vulnerability: reasoning trajectories in Long-CoT models can drift from aligned
paths, resulting in content that violates safety constraints. We term this
phenomenon Path Drift. Through empirical analysis, we uncover three behavioral
triggers of Path Drift: (1) first-person commitments that induce goal-driven
reasoning that delays refusal signals; (2) ethical evaporation, where
surface-level disclaimers bypass alignment checkpoints; (3) condition chain
escalation, where layered cues progressively steer models toward unsafe
completions. Building on these insights, we introduce a three-stage Path Drift
Induction Framework comprising cognitive load amplification, self-role priming,
and condition chain hijacking. Each stage independently reduces refusal rates,
while their combination further compounds the effect. To mitigate these risks,
we propose a path-level defense strategy incorporating role attribution
correction and metacognitive reflection (reflective safety cues). Our findings
highlight the need for trajectory-level alignment oversight in long-form
reasoning beyond token-level alignment.

</details>


### [32] [Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default](https://arxiv.org/abs/2510.10025)
*Jiaqi Liu,Lanruo Wang,Su Liu,Xin Hu*

Main category: cs.CL

TL;DR: 研究了在医疗场景下，紧凑型编码器在医学摘要分类任务中的性能。发现DistilBERT结合标准交叉熵在保持高效性的同时，取得了最佳平衡，并提出优先使用紧凑模型的实践建议。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言处理任务中表现出色，但在医疗环境中，受限于严格的成本、延迟和隐私要求，难以部署。因此，需要探索轻量级且经济有效的解决方案，以了解紧凑型编码器在受控预算下的潜力。

Method: 使用公共医学摘要语料库，对BERT base和DistilBERT进行微调。实验中采用了标准交叉熵、类别加权交叉熵和Focal Loss三种目标函数，并固定了分词器、序列长度、优化器和调度器等参数。

Result: DistilBERT结合标准交叉熵在测试集上表现出最佳平衡，且所需参数量远少于BERT base。研究报告了准确率、Macro F1和Weighted F1指标，并提供了评估代码和混淆分析，以揭示错误模式。

Conclusion: 研究结果提出了一项实用的默认建议：在考虑更大型模型之前，应首先从紧凑型编码器和交叉熵损失函数开始，然后进行校准和任务特定的检查。

Abstract: Large language models work well for many NLP tasks, but they are hard to
deploy in health settings with strict cost, latency, and privacy limits. We
revisit a lightweight recipe for medical abstract classification and ask how
far compact encoders can go under a controlled budget. Using the public medical
abstracts corpus, we finetune BERT base and DistilBERT with three objectives
standard cross-entropy, class weighted cross entropy, and focal loss keeping
tokenizer, sequence length, optimizer, and schedule fixed. DistilBERT with
plain cross-entropy gives the best balance on the test set while using far
fewer parameters than BERT base. We report accuracy, Macro F1, and Weighted F1,
release the evaluation code, and include confusion analyses to make error
patterns clear. Our results suggest a practical default: start with a compact
encoder and cross-entropy, then add calibration and task-specific checks before
moving to heavier models.

</details>


### [33] [HUME: Measuring the Human-Model Performance Gap in Text Embedding Task](https://arxiv.org/abs/2510.10062)
*Adnan El Assadi,Isaac Chung,Roman Solomatin,Niklas Muennighoff,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文引入HUME框架，首次系统测量人类在多语言文本嵌入任务上的表现，为模型评估和开发提供可靠基线，并揭示了模型的优劣和数据集的潜在问题。


<details>
  <summary>Details</summary>
Motivation: 比较人类与模型在嵌入任务上的表现对理解模型优缺点至关重要，但由于人类表现难以测量，此类比较鲜有进行。现有评估框架（如MTEB）缺乏可靠的人类表现估计，限制了模型分数的解释性。

Method: 本文提出了HUME（Human Evaluation Framework for Text Embeddings）框架。通过该框架，研究人员测量了人类在16个MTEB数据集上的表现，这些数据集涵盖了重排序、分类、聚类和语义文本相似性等任务，并涉及多种高资源和低资源语言。

Result: 人类的平均表现为77.6%，而最佳嵌入模型的平均表现为80.1%。尽管模型整体略优，但其在不同数据集上的表现差异显著：在某些数据集上接近人类上限，而在其他数据集（尤其在低资源语言上）表现不佳，这暗示了数据集本身的问题以及模型在处理低资源语言时的不足。

Conclusion: 研究提供了人类表现基线、任务难度模式的深入洞察和一个可扩展的评估框架。这有助于对模型进行更具意义的解释，并为未来模型和基准的开发提供指导。所有代码、数据集和排行榜均已公开。

Abstract: Comparing human and model performance offers a valuable perspective for
understanding the strengths and limitations of embedding models, highlighting
where they succeed and where they fail to capture meaning and nuance. However,
such comparisons are rarely made, as human performance on embedding tasks is
difficult to measure. To fill this gap, we introduce HUME: Human Evaluation
Framework for Text Embeddings. While frameworks like MTEB provide broad model
evaluation, they lack reliable estimates of human performance, limiting the
interpretability of model scores. We measure human performance across 16 MTEB
datasets spanning reranking, classification, clustering, and semantic textual
similarity across linguistically diverse high- and low-resource languages.
Humans achieve an average performance of 77.6% compared to 80.1% for the best
embedding model, although variation is substantial: models reach near-ceiling
performance on some datasets while struggling on others, suggesting dataset
issues and revealing shortcomings in low-resource languages. We provide human
performance baselines, insight into task difficulty patterns, and an extensible
evaluation framework that enables a more meaningful interpretation of the model
and informs the development of both models and benchmarks. Our code, dataset,
and leaderboard are publicly available at
https://github.com/embeddings-benchmark/mteb.

</details>


### [34] [CLMN: Concept based Language Models via Neural Symbolic Reasoning](https://arxiv.org/abs/2510.10063)
*Yibo Yang*

Main category: cs.CL

TL;DR: 本文提出一种神经符号框架CLMN，通过连续概念嵌入和模糊逻辑推理，在保持性能的同时显著提升了NLP模型的可解释性，并优于现有概念模型。


<details>
  <summary>Details</summary>
Motivation: 深度学习在NLP领域取得显著进展，但在医疗和金融等关键领域，其可解释性仍然受限。现有的NLP概念瓶颈模型存在损害文本表示或削弱语义的问题，且难以建模概念间的动态交互，如否定和上下文。

Method: 引入概念语言模型网络（CLMN），一个神经符号框架，旨在同时保持性能和可解释性。CLMN将概念表示为连续、人类可读的嵌入，并应用模糊逻辑推理来学习概念如何相互影响以及如何影响最终决策的自适应交互规则。该模型通过概念感知表示增强原始文本特征，并自动推导出可解释的逻辑规则。

Result: 在多个数据集和预训练语言模型上，CLMN比现有基于概念的方法实现了更高的准确性，同时显著改善了解释质量。

Conclusion: 将神经网络表示与符号推理在一个统一的概念空间中进行整合，能够创建实用且透明的自然语言处理系统。

Abstract: Deep learning has advanced NLP, but interpretability remains limited,
especially in healthcare and finance. Concept bottleneck models tie predictions
to human concepts in vision, but NLP versions either use binary activations
that harm text representations or latent concepts that weaken semantics, and
they rarely model dynamic concept interactions such as negation and context. We
introduce the Concept Language Model Network (CLMN), a neural-symbolic
framework that keeps both performance and interpretability. CLMN represents
concepts as continuous, human-readable embeddings and applies fuzzy-logic
reasoning to learn adaptive interaction rules that state how concepts affect
each other and the final decision. The model augments original text features
with concept-aware representations and automatically induces interpretable
logic rules. Across multiple datasets and pre-trained language models, CLMN
achieves higher accuracy than existing concept-based methods while improving
explanation quality. These results show that integrating neural representations
with symbolic reasoning in a unified concept space can yield practical,
transparent NLP systems.

</details>


### [35] [Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference](https://arxiv.org/abs/2510.10072)
*Hua Cai,Shuang Zhao,Liang Zhang,Xuli Shen,Qing Xu,Weilin Shen,Zihao Wen,Tianke Ban*

Main category: cs.CL

TL;DR: Unilaw-R1是一个针对法律推理的70亿参数大模型，通过高质量数据集和两阶段训练，在法律领域实现了高效且可解释的推理，性能超越同规模模型。


<details>
  <summary>Details</summary>
Motivation: 现有推理型LLM在处理复杂法律问题方面的能力尚未充分探索，面临法律知识不足、推理逻辑不可靠和业务泛化能力弱等核心挑战。

Method: 引入了70亿参数的法律推理大模型Unilaw-R1。构建了包含17K高质量CoT样本的Unilaw-R1-Data数据集。采用SFT和RL相结合的两阶段训练策略。为评估法律推理能力，提出了Unilaw-R1-Eval基准。

Result: Unilaw-R1在权威基准上表现出色，超越所有同规模模型，与更大的DeepSeek-R1-Distill-Qwen-32B（54.9%）性能相当。在LawBench和LexEval上，相比Qwen-2.5-7B-Instruct（46.6%）平均提升6.6%。

Conclusion: Unilaw-R1作为一个轻量级（70亿参数）的法律专用LLM，通过创新的数据和训练方法，能有效解决法律领域的复杂推理问题，显著降低部署成本，并实现高性能和可解释的决策。

Abstract: Reasoning-focused large language models (LLMs) are rapidly evolving across
various domains, yet their capabilities in handling complex legal problems
remains underexplored. In this paper, we introduce Unilaw-R1, a large language
model tailored for legal reasoning. With a lightweight 7-billion parameter
scale, Unilaw-R1 significantly reduces deployment cost while effectively
tackling three core challenges in the legal domain: insufficient legal
knowledge, unreliable reasoning logic, and weak business generalization. To
address these issues, we first construct Unilaw-R1-Data, a high-quality dataset
containing 17K distilled and screened chain-of-thought (CoT) samples. Based on
this, we adopt a two-stage training strategy combining Supervised Fine-Tuning
(SFT) and Reinforcement Learning (RL), which significantly boosts the
performance on complex legal reasoning tasks and supports interpretable
decision-making in legal AI applications. To assess legal reasoning ability, we
also introduce Unilaw-R1-Eval, a dedicated benchmark designed to evaluate
models across single- and multi-choice legal tasks. Unilaw-R1 demonstrates
strong results on authoritative benchmarks, outperforming all models of similar
scale and achieving performance on par with the much larger
DeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it
also showed significant gains on LawBench and LexEval, exceeding
Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.

</details>


### [36] [A-IPO: Adaptive Intent-driven Preference Optimization](https://arxiv.org/abs/2510.10077)
*Wenqing Wang,Muhammad Asif Ali,Ali Shoker,Ruohan Yang,Junyang Chen,Ying Sha,Huan Wang*

Main category: cs.CL

TL;DR: A-IPO是一种新的偏好优化方法，通过显式推断并整合用户潜在意图来解决现有方法忽视偏好多样性和少数意见的问题，在多项基准测试中显著超越现有基线，提升了偏好对齐和对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法（如DPO）常默认多数观点，忽略少数意见，未能捕捉提示中的潜在用户意图，而人类偏好具有地域、文化和社会等多方面的多样性和动态性。

Method: 引入自适应意图驱动偏好优化（A-IPO），包含一个意图模块，用于推断用户提示背后的潜在意图，并将其明确纳入奖励函数，以促进模型响应与用户意图的对齐。理论和经验表明，整合意图-响应相似性项可增加偏好裕度。同时引入Real-pref、Attack-pref和GlobalOpinionQA-Ext基准进行评估。

Result: A-IPO实现了多元化偏好优化并增强了对抗鲁棒性。在Real-pref上，胜率提升高达+24.8，响应-意图一致性提升高达+45.6；在Attack-pref上，响应相似性提升高达+38.6，防御成功率提升高达+52.2；在GlobalOpinionQA-Ext上，意图一致性分数提升高达+54.6。

Conclusion: A-IPO通过显式建模多样用户意图，成功实现了多元化偏好优化，并增强了偏好对齐的对抗鲁棒性，在各项评估指标上均显著优于现有基线方法。

Abstract: Human preferences are diverse and dynamic, shaped by regional, cultural, and
social factors. Existing alignment methods like Direct Preference Optimization
(DPO) and its variants often default to majority views, overlooking minority
opinions and failing to capture latent user intentions in prompts.
  To address these limitations, we introduce \underline{\textbf{A}}daptive
\textbf{\underline{I}}ntent-driven \textbf{\underline{P}}reference
\textbf{\underline{O}}ptimization (\textbf{A-IPO}). Specifically,A-IPO
introduces an intention module that infers the latent intent behind each user
prompt and explicitly incorporates this inferred intent into the reward
function, encouraging stronger alignment between the preferred model's
responses and the user's underlying intentions. We demonstrate, both
theoretically and empirically, that incorporating an intention--response
similarity term increases the preference margin (by a positive shift of
$\lambda\,\Delta\mathrm{sim}$ in the log-odds), resulting in clearer separation
between preferred and dispreferred responses compared to DPO.
  For evaluation, we introduce two new benchmarks, Real-pref, Attack-pref along
with an extended version of an existing dataset, GlobalOpinionQA-Ext, to assess
real-world and adversarial preference alignment.
  Through explicit modeling of diverse user intents,A-IPO facilitates
pluralistic preference optimization while simultaneously enhancing adversarial
robustness in preference alignment. Comprehensive empirical evaluation
demonstrates that A-IPO consistently surpasses existing baselines, yielding
substantial improvements across key metrics: up to +24.8 win-rate and +45.6
Response-Intention Consistency on Real-pref; up to +38.6 Response Similarity
and +52.2 Defense Success Rate on Attack-pref; and up to +54.6 Intention
Consistency Score on GlobalOpinionQA-Ext.

</details>


### [37] [Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers](https://arxiv.org/abs/2510.10082)
*Parthiv Chatterjee,Shivam Sonawane,Amey Hengle,Aditya Tanna,Sourish Dasgupta,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文提出PerAugy数据增强技术，通过轨迹重排和内容扰动，有效解决了个性化摘要模型训练中数据稀缺及多样性不足的问题，显著提升了用户编码器和个性化摘要框架的性能，并证明了数据集多样性是性能提升的关键驱动因素。


<details>
  <summary>Details</summary>
Motivation: 文档摘要本质上受个体主观性影响，难以在多维度文档中识别主观显著信息，因此需要个性化摘要。然而，由于缺乏包含用户偏好历史和目标摘要的、多样化的训练数据，训练个性化摘要模型极具挑战性。现有数据集（如MS/CAS PENS）缺少目标摘要且主题多样性有限，阻碍了端到端监督学习和模型的泛化能力。

Method: 提出了一种名为PerAugy的新型数据增强技术，该技术基于跨轨迹重排（cross-trajectory shuffling）和摘要内容扰动（summary-content perturbation）。为量化PerAugy引入的数据集多样性，引入了TP、RTC和DegreeD三个数据集多样性指标。

Result: PerAugy显著提升了四种最先进的用户编码器的准确性（AUC指标最佳提升0.132）。当与增强后的用户编码器结合时，两种最先进的摘要框架的个性化程度平均提升了61.2%（基于PSE-SU4指标）。通过后验分析，发现TP和DegreeD指标与PerAugy生成数据集上的用户编码器性能（所有准确性指标）高度相关。

Conclusion: PerAugy是一种有效的数据增强技术，能够显著提升个性化摘要中用户编码器和摘要框架的性能。研究表明，增加数据集多样性是推动性能提升的关键因素。

Abstract: Document summarization enables efficient extraction of user-relevant content
but is inherently shaped by individual subjectivity, making it challenging to
identify subjective salient information in multifaceted documents. This
complexity underscores the necessity for personalized summarization. However,
training models for personalized summarization has so far been challenging,
particularly because diverse training data containing both user preference
history (i.e., click-skip trajectory) and expected (gold-reference) summaries
are scarce. The MS/CAS PENS dataset is a valuable resource but includes only
preference history without target summaries, preventing end-to-end supervised
learning, and its limited topic-transition diversity further restricts
generalization. To address this, we propose $\mathrm{PerAugy}$, a novel
cross-trajectory shuffling and summary-content perturbation based data
augmentation technique that significantly boosts the accuracy of four
state-of-the-art baseline (SOTA) user-encoders commonly used in personalized
summarization frameworks (best result: $\text{0.132}$$\uparrow$ w.r.t AUC). We
select two such SOTA summarizer frameworks as baselines and observe that when
augmented with their corresponding improved user-encoders, they consistently
show an increase in personalization (avg. boost: $\text{61.2\%}\uparrow$ w.r.t.
PSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the
augmented dataset by \peraugy, we introduce three dataset diversity metrics --
$\mathrm{TP}$, $\mathrm{RTC}$, and \degreed\ to quantify the induced diversity.
We find that $\mathrm{TP}$ and $\mathrm{DegreeD}$ strongly correlate with
user-encoder performance on the PerAugy-generated dataset across all accuracy
metrics, indicating that increased dataset diversity is a key factor driving
performance gains.

</details>


### [38] [Stop When Enough: Adaptive Early-Stopping for Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.10103)
*Renliang Sun,Wei Cheng,Dawei Li,Haifeng Chen,Wei Wang*

Main category: cs.CL

TL;DR: 本文提出了一个名为REFRAIN的无训练框架，通过自适应地决定何时停止推理来缓解大语言模型（LLM）在思维链（CoT）推理中的“过度思考”问题，从而在保持准确性的同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 思维链（CoT）推理提升了LLM在推理任务上的表现，但过度的或冗余的推理（即“过度思考”）会增加推理成本并可能导致错误结论。因此，需要一种方法来智能地决定何时停止推理。

Method: REFRAIN框架是无训练的，它整合了一个两阶段的停止判别器来识别反思但冗余的推理，以及一个滑动窗口上置信界（SW-UCB）多臂赌博机控制器来根据问题难度动态调整停止阈值，无需监督或微调。

Result: 在四个代表性基准测试和两个模型家族上，REFRAIN框架与标准CoT提示相比，将Token使用量减少了20-55%，同时保持或提高了准确性。广泛的消融和鲁棒性分析表明其在模型、评估器和提示变体方面的稳定性。

Conclusion: 研究结果强调了“何时停止”是测试时扩展的一个新颖且实用的方向，使模型能够进行“恰到好处”而非“更多”的推理。

Abstract: Chain-of-Thought (CoT) reasoning has driven recent gains of large language
models (LLMs) on reasoning-intensive tasks by externalizing intermediate steps.
However, excessive or redundant reasoning -- so-called overthinking -- can
increase inference costs and lead LLMs toward incorrect conclusions. In this
paper, we present REFRAIN ($\underline{REF}$lective-$\underline{R}$edundancy
for $\underline{A}$daptive $\underline{IN}$ference), a training-free framework
that adaptively determines when to stop reasoning to mitigate overthinking.
REFRAIN integrates a two-stage stop discriminator to identify reflective yet
redundant reasoning and a sliding-window Upper Confidence Bound (SW-UCB)
multi-armed bandit controller to dynamically adjust stopping thresholds
according to problem difficulty without supervision or fine-tuning. Across four
representative benchmarks and two model families, REFRAIN reduces token usage
by 20-55% while maintaining or improving accuracy compared to standard CoT
prompting. Extensive ablation and robustness analyses demonstrate its stability
across models, scorers, and prompt variations. In summary, our findings
highlight when-to-stop as a new and practical axis of test-time scaling --
enabling models to reason not just more, but just enough.

</details>


### [39] [LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora](https://arxiv.org/abs/2510.10114)
*Luyao Zhuang,Shengyuan Chen,Yilin Xiao,Huachi Zhou,Yujing Zhang,Hao Chen,Qinggang Zhang,Xiao Huang*

Main category: cs.CL

TL;DR: LinearRAG提出了一种高效可靠的GraphRAG框架，通过构建无关系分层图（Tri-Graph）和两阶段检索策略，解决了现有GraphRAG中关系抽取不稳定和成本高的问题，并在实验中显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在处理大规模非结构化语料库时存在局限性。现有图基RAG（GraphRAG）方法虽然利用知识图谱，但其图构建依赖不稳定且昂贵的关系抽取，易产生噪声图，从而降低检索质量。

Method: LinearRAG构建了一个无关系分层图（Tri-Graph），仅通过轻量级实体抽取和语义链接，避免了不稳定的关系建模，其图构建成本与语料库大小呈线性关系。检索策略采用两阶段：首先通过局部语义桥接激活相关实体，然后通过全局重要性聚合进行段落检索。

Result: 在四个数据集上的广泛实验表明，LinearRAG显著优于所有基线模型。

Conclusion: LinearRAG通过创新的无关系图构建范式和两阶段检索策略，提供了一种经济高效且可靠的图基检索增强生成方法，有效提升了复杂查询下的RAG性能。

Abstract: Retrieval-Augmented Generation (RAG) is widely used to mitigate
hallucinations of Large Language Models (LLMs) by leveraging external
knowledge. While effective for simple queries, traditional RAG systems struggle
with large-scale, unstructured corpora where information is fragmented. Recent
advances incorporate knowledge graphs to capture relational structures,
enabling more comprehensive retrieval for complex, multi-hop reasoning tasks.
However, existing graph-based RAG (GraphRAG) methods rely on unstable and
costly relation extraction for graph construction, often producing noisy graphs
with incorrect or inconsistent relations that degrade retrieval quality. In
this paper, we revisit the pipeline of existing GraphRAG systems and propose
LinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient
framework that enables reliable graph construction and precise passage
retrieval. Specifically, LinearRAG constructs a relation-free hierarchical
graph, termed Tri-Graph, using only lightweight entity extraction and semantic
linking, avoiding unstable relation modeling. This new paradigm of graph
construction scales linearly with corpus size and incurs no extra token
consumption, providing an economical and reliable indexing of the original
passages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant
entity activation via local semantic bridging, followed by (ii) passage
retrieval through global importance aggregation. Extensive experiments on four
datasets demonstrate that LinearRAG significantly outperforms baseline models.

</details>


### [40] [Hybrid OCR-LLM Framework for Enterprise-Scale Document Information Extraction Under Copy-heavy Task](https://arxiv.org/abs/2510.10138)
*Zilong Wang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 本文提出一种结合OCR引擎和LLMs的系统框架，通过智能策略选择，显著提高了从重内容文档（如身份文件）中提取信息的准确性和效率，实现生产级处理能力。


<details>
  <summary>Details</summary>
Motivation: 企业文档处理中，从大量结构相似的重内容文档中提取信息是一个关键但尚未充分研究的挑战，现有方法难以有效平衡准确性和效率。

Method: 开发了一个结合OCR引擎和大型语言模型（LLMs）的系统框架。该方法通过智能策略选择，利用文档特定特征，而非追求通用解决方案。研究评估了三种提取范式（直接、替换、基于表格）下的25种配置，并在四种格式（PNG、DOCX、XLSX、PDF）的身份文档上进行了测试。

Result: 基于表格的提取方法表现卓越：对结构化文档实现F1=1.0的准确率和0.97秒的延迟；与PaddleOCR集成时，对复杂图像输入实现F1=0.997的准确率和0.6秒的延迟。相较于多模态方法，性能提升54倍，结合格式感知路由，支持生产规模的异构文档流处理。

Conclusion: 该工作确立了一个通用原则：通过结构感知的方法选择，可以将重内容任务的重复性从计算负担转化为优化机会，超越了身份信息提取的特定应用。

Abstract: Information extraction from copy-heavy documents, characterized by massive
volumes of structurally similar content, represents a critical yet understudied
challenge in enterprise document processing. We present a systematic framework
that strategically combines OCR engines with Large Language Models (LLMs) to
optimize the accuracy-efficiency trade-off inherent in repetitive document
extraction tasks. Unlike existing approaches that pursue universal solutions,
our method exploits document-specific characteristics through intelligent
strategy selection. We implement and evaluate 25 configurations across three
extraction paradigms (direct, replacement, and table-based) on identity
documents spanning four formats (PNG, DOCX, XLSX, PDF). Through table-based
extraction methods, our adaptive framework delivers outstanding results: F1=1.0
accuracy with 0.97s latency for structured documents, and F1=0.997 accuracy
with 0.6 s for challenging image inputs when integrated with PaddleOCR, all
while maintaining sub-second processing speeds. The 54 times performance
improvement compared with multimodal methods over naive approaches, coupled
with format-aware routing, enables processing of heterogeneous document streams
at production scale. Beyond the specific application to identity extraction,
this work establishes a general principle: the repetitive nature of copy-heavy
tasks can be transformed from a computational burden into an optimization
opportunity through structure-aware method selection.

</details>


### [41] [DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models](https://arxiv.org/abs/2510.10142)
*Tingxu Han,Wei Song,Ziqi Ding,Ziming Li,Chunrong Fang,Yuekang Li,Dongfang Liu,Zhenyu Chen,Zhenting Wang*

Main category: cs.CL

TL;DR: 本文系统研究了大型语言模型（LLM）的偏见机制，发现直接回答（DA）提示会显著加剧偏见，并识别出导致偏见的特定“偏见头”。在此基础上，提出了DiffHeads框架，通过选择性地掩盖这些偏见头，有效降低了LLM的不公平性且不损害模型效用。


<details>
  <summary>Details</summary>
Motivation: LLM在敏感领域决策中日益重要，但其对特定人群的不公平对待是不可接受的。现有研究虽能识别偏见出现时机，但缺乏对其生成机制的深入理解，导致现有缓解措施脆弱。

Method: ['比较了8个LLM在直接回答（DA）和思维链（CoT）两种提示策略下的表现，发现DA提示显著加剧偏见。', '定义了“token-to-head贡献分数”来追踪每个token对注意力头的影响，识别出在DA下激活而在CoT下休眠的“偏见头”，建立了提示策略与偏见产生之间的因果联系。', '基于此发现，提出DiffHeads框架，通过DA和CoT之间的差异激活分析识别偏见头，并选择性地掩盖这些偏见头。']

Result: ['DA提示在单轮和两轮对话中，将测量的偏见分别提高了534.5%和391.9%。', 'DiffHeads框架在DA和CoT条件下分别将不公平性降低了49.4%和40.3%，且未损害模型实用性。']

Conclusion: 本研究揭示了LLM中提示策略（如DA）与特定注意力头（偏见头）激活之间的因果关联，从而解释了偏见的生成机制。DiffHeads框架作为一种轻量级去偏方法，能够基于这一机制有效降低LLM的不公平性，同时保持模型效用。

Abstract: Large language models (LLMs) increasingly mediate decisions in domains where
unfair treatment of demographic groups is unacceptable. Existing work probes
when biased outputs appear, but gives little insight into the mechanisms that
generate them, leaving existing mitigations largely fragile. In this paper, we
conduct a systematic investigation LLM unfairness and propose DiffHeads, a
lightweight debiasing framework for LLMs. We first compare Direct-Answer (DA)
prompting to Chain-of-Thought (CoT) prompting across eight representative open-
and closed-source LLMs. DA will trigger the nature bias part of LLM and improve
measured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues.
Next, we define a token-to-head contribution score that traces each token's
influence back to individual attention heads. This reveals a small cluster of
bias heads that activate under DA but stay largely dormant with CoT, providing
the first causal link between prompting strategy and bias emergence. Finally,
building on this insight, we propose DiffHeads that identifies bias heads
through differential activation analysis between DA and CoT, and selectively
masks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under
DA and CoT, respectively, without harming model utility.

</details>


### [42] [BILLY: Steering Large Language Models via Merging Persona Vectors for Creative Generation](https://arxiv.org/abs/2510.10157)
*Tsung-Min Pai,Jui-I Wang,Li-Chun Lu,Shao-Hua Sun,Hung-Yi Lee,Kai-Wei Chang*

Main category: cs.CL

TL;DR: BILLY是一种无训练框架，通过在单个LLM中融合角色向量，模拟多LLM协作的创造力，同时大幅降低计算成本和推理延迟，并超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 多LLM系统虽能通过模拟集体智能增强大型语言模型的创造力，但面临计算成本高昂和推理延迟显著的缺点。

Method: 提出BILLY框架，通过在模型激活空间中直接提取并融合多个不同的角色向量。在推理时，利用这个融合向量引导模型的生成过程，从而在单个模型中实现多视角输出，无需显式的多LLM通信。

Result: 在以创造力为导向的基准测试中，BILLY超越了单模型提示和传统多LLM方法，同时显著降低了推理时间和计算成本。分析还表明，融合不同的角色向量可以有效控制生成的互补方面并提高可解释性。

Conclusion: BILLY通过在单个模型中融合角色向量，成功地在解决多LLM系统高成本和高延迟问题的同时，捕获了其多样视角和专业知识的优势，实现了更高效、可控且可解释的创造性生成。

Abstract: Multi-LLM systems enhance the creativity of large language models by
simulating human collective intelligence but suffer from significant drawbacks,
such as high computational costs and inference latency. To address these
limitations, we propose BILLY (BlendIng persona vectors for Large Language
model creativitY), a training-free framework that captures the benefits of
multi-LLM collaboration, i.e. inducing diverse perspectives and specialized
expertise, within a single model. BILLY operates by extracting and blending
multiple distinct persona vectors directly in the model's activation space. We
steer the model's generation process with this merged vector while inference,
enabling multi-perspective output without explicit multi-LLM communication. Our
experiments across creativity-oriented benchmarks demonstrate that BILLY
surpasses single model prompting and traditional multi-LLM approaches, while
substantially reducing inference time and computational costs. Our analyses
further reveal that distinct persona vectors can be blended to achieve both
effective control over complementary aspects of generation and greater
interpretability.

</details>


### [43] [BabyBabelLM: A Multilingual Benchmark of Developmentally Plausible Training Data](https://arxiv.org/abs/2510.10159)
*Jaap Jumelet,Abdellah Fourtassi,Akari Haga,Bastian Bunzeck,Bhargav Shandilya,Diana Galvan-Sosa,Faiz Ghifari Haznitrama,Francesca Padovani,Francois Meyer,Hai Hu,Julen Etxaniz,Laurent Prévot,Linyang He,María Grandury,Mila Marcheva,Negar Foroutan,Nikitas Theodoropoulos,Pouya Sadeghi,Siyuan Song,Suchir Salhan,Susana Zhou,Yurii Paniv,Ziyin Zhang,Arianna Bisazza,Alex Warstadt,Leshem Choshen*

Main category: cs.CL

TL;DR: 本文介绍了BabyBabelLM，一个多语言数据集集合，模拟了人类从出生到掌握母语所接触的语言，旨在促进多语言预训练和认知建模。


<details>
  <summary>Details</summary>
Motivation: 为了促进多语言预训练和认知建模，需要一个符合发展规律、涵盖多种语言的预训练数据集。

Method: 策划并构建了一个多语言数据集BabyBabelLM，涵盖45种语言，每种语言的数据量相当于1亿英文单词。同时，为每种语言编译了评估套件并训练了基线模型。

Result: 成功构建了BabyBabelLM数据集集合，该集合包含45种语言的、符合发展规律的预训练数据，并提供了评估套件和基线模型。

Conclusion: BabyBabelLM数据集的发布旨在为多语言预训练和认知建模领域提供基础资源和便利。

Abstract: We present BabyBabelLM, a multilingual collection of datasets modeling the
language a person observes from birth until they acquire a native language. We
curate developmentally plausible pretraining data aiming to cover the
equivalent of 100M English words of content in each of 45 languages. We compile
evaluation suites and train baseline models in each language. BabyBabelLM aims
to facilitate multilingual pretraining and cognitive modeling.

</details>


### [44] [Large Language Model Sourcing: A Survey](https://arxiv.org/abs/2510.10161)
*Liang Pang,Kangxi Wu,Sunhao Dai,Zihao Wei,Zenghao Duan,Jia Gu,Xiang Li,Zhiyi Yin,Jun Xu,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本综述系统地调查了大型语言模型（LLMs）生成内容的出处追溯方法，并提出了一个涵盖模型和数据视角的四维度框架及双范式分类。


<details>
  <summary>Details</summary>
Motivation: LLMs在主观决策中应用广泛，但其黑盒性质和类人输出带来了幻觉、偏见、不公平和版权侵权等风险。因此，追溯信息来源对于提升透明度、问责制和可信度至关重要。

Method: 本研究通过系统综述，围绕模型和数据两个核心视角，提出了出处追溯的四个维度：模型来源追踪、模型结构来源追踪、训练数据来源追踪和外部数据来源追踪。此外，还提出了基于先验（主动嵌入）和基于后验（回顾性推断）的双范式分类。

Result: 本综述为LLM生成内容的出处追溯提供了一个系统的调查框架，明确了从模型和数据角度进行追溯的不同方法和关注点，并对现有追溯方法进行了分类。

Conclusion: 通过在这些维度上进行追溯，可以显著增强LLM在实际应用中的透明度、问责制和可信赖性。

Abstract: The rapid advancement of large language models (LLMs) has revolutionized
artificial intelligence, shifting from supporting objective tasks (e.g.,
recognition) to empowering subjective decision-making (e.g., planning,
decision). This marks the dawn of general and powerful AI, with applications
spanning a wide range of fields, including programming, education, healthcare,
finance, and law. However, their deployment introduces multifaceted risks. Due
to the black-box nature of LLMs and the human-like quality of their generated
content, issues such as hallucinations, bias, unfairness, and copyright
infringement become particularly significant. In this context, sourcing
information from multiple perspectives is essential.
  This survey presents a systematic investigation into provenance tracking for
content generated by LLMs, organized around four interrelated dimensions that
together capture both model- and data-centric perspectives. From the model
perspective, Model Sourcing treats the model as a whole, aiming to distinguish
content generated by specific LLMs from content authored by humans. Model
Structure Sourcing delves into the internal generative mechanisms, analyzing
architectural components that shape the outputs of model. From the data
perspective, Training Data Sourcing focuses on internal attribution, tracing
the origins of generated content back to the training data of model. In
contrast, External Data Sourcing emphasizes external validation, identifying
external information used to support or influence the responses of model.
Moreover, we also propose a dual-paradigm taxonomy that classifies existing
sourcing methods into prior-based (proactive traceability embedding) and
posterior-based (retrospective inference) approaches. Traceability across these
dimensions enhances the transparency, accountability, and trustworthiness of
LLMs deployment in real-world applications.

</details>


### [45] [A Survey of Inductive Reasoning for Large Language Models](https://arxiv.org/abs/2510.10182)
*Kedi Chen,Dezhao Ruan,Yuhao Dan,Yaoting Wang,Siyu Yan,Xuecheng Wu,Yinqi Zhang,Qin Chen,Jie Zhou,Liang He,Biqing Qi,Linyang Li,Qipeng Guo,Xiaoming Shi,Wei Zhang*

Main category: cs.CL

TL;DR: 首个关于LLM归纳推理的综合性综述，分类了改进方法，总结了基准，并为未来研究提供了分析洞察。


<details>
  <summary>Details</summary>
Motivation: 归纳推理对LLM至关重要，能实现知识泛化并与人类认知对齐，但目前缺乏系统性总结。

Method: 本文进行综合性综述，包括：将归纳推理改进方法分为后训练、测试时扩展和数据增强三类；总结了当前基准；推导了统一的基于沙盒的评估方法及观测覆盖率指标；并分析了归纳能力的来源和模型架构、数据的作用。

Result: 系统地分类了归纳推理方法，总结了现有基准，提出了统一的评估方法，并分析了归纳能力的来源及模型和数据的影响。

Conclusion: 本研究为LLM归纳推理的未来研究奠定了坚实基础。

Abstract: Reasoning is an important task for large language models (LLMs). Among all
the reasoning paradigms, inductive reasoning is one of the fundamental types,
which is characterized by its particular-to-general thinking process and the
non-uniqueness of its answers. The inductive mode is crucial for knowledge
generalization and aligns better with human cognition, so it is a fundamental
mode of learning, hence attracting increasing interest. Despite the importance
of inductive reasoning, there is no systematic summary of it. Therefore, this
paper presents the first comprehensive survey of inductive reasoning for LLMs.
First, methods for improving inductive reasoning are categorized into three
main areas: post-training, test-time scaling, and data augmentation. Then,
current benchmarks of inductive reasoning are summarized, and a unified
sandbox-based evaluation approach with the observation coverage metric is
derived. Finally, we offer some analyses regarding the source of inductive
ability and how simple model architectures and data help with inductive tasks,
providing a solid foundation for future research.

</details>


### [46] [MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems](https://arxiv.org/abs/2510.10185)
*Lei Gu,Yinghao Zhu,Haoran Sang,Zixiang Wang,Dehao Sui,Wen Tang,Ewen Harrison,Junyi Gao,Lequan Yu,Liantao Ma*

Main category: cs.CL

TL;DR: 现有医疗多智能体系统评估仅关注准确率，忽略内部推理过程，本研究通过大规模实证揭示了四种主要协作失败模式，强调高准确率不足以建立信任，亟需透明可审计的推理过程。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的医疗多智能体系统在模拟医疗咨询时，评估常局限于最终答案准确性，将内部协作过程视为“黑箱”。这忽略了诊断结论是否通过合理可验证的推理路径达成，在高风险医疗应用中构成重大风险，可能导致有缺陷或不可信的结论。

Method: 进行了一项大规模实证研究，涵盖6个医疗数据集和6个代表性多智能体框架的3,600个案例。采用严谨的混合方法，结合定性分析和定量审计，开发了协作失败模式的综合分类法。

Result: 定量审计揭示了四种主要的失败模式：由共享模型缺陷驱动的有缺陷共识、正确少数意见的压制、无效的讨论动态，以及合成过程中的关键信息丢失。

Conclusion: 本研究表明，仅有高准确率不足以衡量临床或公众信任。它强调了对透明和可审计推理过程的迫切需求，这是负责任地开发和部署医疗AI的基石。

Abstract: While large language model (LLM)-based multi-agent systems show promise in
simulating medical consultations, their evaluation is often confined to
final-answer accuracy. This practice treats their internal collaborative
processes as opaque "black boxes" and overlooks a critical question: is a
diagnostic conclusion reached through a sound and verifiable reasoning pathway?
The inscrutable nature of these systems poses a significant risk in high-stakes
medical applications, potentially leading to flawed or untrustworthy
conclusions. To address this, we conduct a large-scale empirical study of 3,600
cases from six medical datasets and six representative multi-agent frameworks.
Through a rigorous, mixed-methods approach combining qualitative analysis with
quantitative auditing, we develop a comprehensive taxonomy of collaborative
failure modes. Our quantitative audit reveals four dominant failure patterns:
flawed consensus driven by shared model deficiencies, suppression of correct
minority opinions, ineffective discussion dynamics, and critical information
loss during synthesis. This study demonstrates that high accuracy alone is an
insufficient measure of clinical or public trust. It highlights the urgent need
for transparent and auditable reasoning processes, a cornerstone for the
responsible development and deployment of medical AI.

</details>


### [47] [Weed Out, Then Harvest: Dual Low-Rank Adaptation is an Effective Noisy Label Detector for Noise-Robust Learning](https://arxiv.org/abs/2510.10208)
*Bo Yuan,Yulin Chen,Yin Zhang*

Main category: cs.CL

TL;DR: Delora框架通过解耦样本选择与模型训练，并利用双LoRA（干净与噪声LoRA）作为噪声标签检测器，有效解决了PEFT LLMs在含有噪声标签的数据上进行微调时性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: PEFT LLMs在下游任务中表现优异，但真实世界数据常含有噪声标签。现有基于小损失的样本选择方法会因初始选择不准确而陷入“恶性循环”，导致次优性能。

Method: 提出Delora框架，将样本选择与模型训练解耦。在样本选择阶段，通过引入干净LoRA和噪声LoRA构建噪声标签检测器，其中干净LoRA记忆干净数据，噪声LoRA记忆错误标签数据，并作为可学习阈值区分样本。在模型训练阶段，利用精心选择的样本微调语言模型。

Result: 在合成和真实世界的噪声数据集上，实验结果证明Delora在噪声标签检测和文本分类任务中均表现出有效性。

Conclusion: Delora通过解耦样本选择与模型训练，并设计独特的噪声标签检测器，成功打破了现有方法在含噪声数据学习中的恶性循环，提升了PEFT LLMs的微调性能。

Abstract: Parameter-efficient fine-tuning (PEFT) large language models (LLMs) have
shown impressive performance in various downstream tasks. However, in many
real-world scenarios, the collected training data inevitably contains noisy
labels. To learn from noisy labels, most solutions select samples with small
losses for model training. However, the selected samples, in turn, impact the
loss computation in the next iteration. An inaccurate initial selection can
create a vicious cycle, leading to suboptimal performance. To break this cycle,
we propose Delora, a novel framework that decouples the sample selection from
model training. For sample selection, Delora establishes a noisy label detector
by introducing clean and noisy LoRA. Benefiting from the memory effect, the
clean LoRA is encouraged to memorize clean data, while the noisy LoRA is
constrained to memorize mislabeled data, which serves as a learnable threshold
for selecting clean and noisy samples. For model training, Delora can use
carefully selected samples to fine-tune language models seamlessly.
Experimental results on synthetic and real-world noisy datasets demonstrate the
effectiveness of Delora in noisy label detection and text classification.

</details>


### [48] [You only need 4 extra tokens: Synergistic Test-time Adaptation for LLMs](https://arxiv.org/abs/2510.10223)
*Yijie Xu,Huizai Yao,Zhiyu Guo,Weiyu Guo,Pengteng Li,Aiwei Liu,Xuming Hu,Hui Xiong*

Main category: cs.CL

TL;DR: 针对大型语言模型在专业领域面临的分布偏移和标签数据稀缺问题，本文提出了SyTTA，一种无需标签的测试时自适应框架。SyTTA结合输入困惑度和输出预测熵两种不确定性信号，能在不同模型和领域基准上带来一致性能提升，尤其在农业问答任务中表现显著，支持在标签稀缺领域部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在金融、医疗、农业等专业领域部署时，面临与训练数据显著的分布偏移，导致性能下降。尽管领域特定微调可缓解此问题，但在专业知识有限的环境中，高质量标注数据收集昂贵且耗时。因此，需要一种无需标签的测试时自适应方法。

Method: 本文提出SyTTA，一个推理时框架，它在无需额外监督的情况下，动态地对模型进行适应。SyTTA耦合了两种在分布偏移下产生的互补不确定性信号：1) 输入侧困惑度，用于指示与领域特定术语和模式的不匹配；2) 输出侧预测熵，用于指示生成过程中分散和不稳定的token概率。

Result: SyTTA在不同的模型架构和多样化的领域特定基准测试中均表现出一致的性能提升。值得注意的是，在农业问答任务中，SyTTA使Qwen-2.5-7B模型的Rouge-LSum提高了120%以上，每个查询仅额外增加了4个token。

Conclusion: 研究结果表明，无需标注示例即可实现语言模型的有效测试时自适应，这为在标签稀缺的领域部署LLMs提供了支持。

Abstract: Large language models (LLMs) are increasingly deployed in specialized domains
such as finance, medicine, and agriculture, where they face significant
distribution shifts from their training data. Domain-specific fine-tuning can
mitigate this challenge but relies on high-quality labeled data that is
expensive and slow to collect in expertise-limited settings. We study
label-free test-time adaptation for language models and present SyTTA, an
inference-time framework that adapts models on-the-fly without additional
supervision. SyTTA couples two complementary uncertainty signals that arise
under distribution shift: input-side perplexity, indicating mismatch with
domain-specific terminology and patterns, and output-side predictive entropy,
indicating diffuse and unstable token probabilities during generation. Across
diverse model architectures and domain-specific benchmarks, SyTTA delivers
consistent gains. Notably, on agricultural question answering, SyTTA improves
Rouge-LSum by over 120% on Qwen-2.5-7B with only 4 extra tokens per query.
These results show that effective test-time adaptation for language models is
achievable without labeled examples, supporting deployment in label-scarce
domains. The code will be made available upon acceptance.

</details>


### [49] [Text2Token: Unsupervised Text Representation Learning with Token Target Prediction](https://arxiv.org/abs/2510.10224)
*Ruize An,Richong Zhang,Zhijie Nie,Zhanyu Wu,Yanzhao Zhang,Dingkun Long*

Main category: cs.CL

TL;DR: 本文提出了Text2Token，一个无监督的生成式文本表示学习框架，通过精心构建的目标token分布进行token目标预测，在MTEB v2基准上表现与SOTA无监督方法LLM2Vec相当。


<details>
  <summary>Details</summary>
Motivation: 无监督文本表示学习是NLP基础任务，对网络搜索和推荐有益。最近研究发现高质量表示与文本关键token对齐，揭示了表示空间与词汇空间潜在联系，促使作者重新审视生成任务并利用此发现。

Method: 开发了Text2Token无监督生成框架，基于token目标预测任务，利用精心构建的目标token分布作为监督信号。通过分析先进嵌入器的token对齐特性，识别出两类关键token：文本中有意义的token和文本外语义派生的token。基于此，提出了数据驱动和模型派生两种方法从数据或LLM骨干构建合成token目标。

Result: 在MTEB v2基准测试中，Text2Token的性能与最先进的无监督对比学习嵌入器LLM2Vec具有竞争力。分析表明，词汇空间和表示空间在训练过程中共同优化并趋向最优解。

Conclusion: Text2Token是一个具有竞争力的无监督生成式文本表示学习框架。研究发现词汇空间和表示空间协同优化，为未来工作提供了新的思路和见解。

Abstract: Unsupervised text representation learning (TRL) is a fundamental task in
natural language processing, which is beneficial for improving search and
recommendations with the web's unlabeled texts. A recent empirical study finds
that the high-quality representation aligns with the key token of the input
text, uncovering the potential connection between representation space and
vocabulary space. Inspired by the findings, we revisit the generative tasks and
develop an unsupervised generative framework for TRL, Text2Token. The framework
is based on the token target prediction task, utilizing carefully constructed
target token distribution as supervisory signals. To construct the high-quality
target token distribution, we analyze the token-alignment properties with
advanced embedders and identify two essential categories of key tokens: (1) the
meaningful tokens in the text and (2) semantically derived tokens beyond the
text. Based on these insights, we propose two methods -- data-driven and
model-derived -- to construct synthetic token targets from data or the LLM
backbone. Experiments on the MTEB v2 benchmark demonstrate that Text2Token
achieves performance competitive with the state-of-the-art embedder with
unsupervised contrastive learning, LLM2Vec. Our analysis further shows that
vocabulary and representation spaces optimize together and toward the optimum
solution during training, providing new ideas and insights for future work.

</details>


### [50] [ImCoref-CeS: An Improved Lightweight Pipeline for Coreference Resolution with LLM-based Checker-Splitter Refinement](https://arxiv.org/abs/2510.10241)
*Kangyang Luo,Yuzhuo Bai,Shuzheng Si,Cheng Gao,Zhitong Wang,Yingli Shen,Wenhao Li,Zhu Liu,Yufeng Han,Jiayi Wu,Cunliang Kong,Maosong Sun*

Main category: cs.CL

TL;DR: 提出了ImCoref-CeS框架，结合了增强的监督式指代消解模型（ImCoref）和基于LLM的检查-拆分代理，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前指代消解研究面临一个困境：是继续探索基于小型语言模型的监督式方法，还是利用大型语言模型（LLM）的能力；而有效结合两者优势的研究尚不足。

Method: 提出ImCoref-CeS框架，整合了增强型监督模型和基于LLM的推理。具体包括：1) ImCoref，通过轻量级桥接模块增强长文本编码，设计双仿射评分器捕获位置信息，并采用混合提及正则化提高训练效率；2) LLM作为多角色检查-拆分代理，用于验证ImCoref预测的候选提及和指代结果。

Result: ImCoref-CeS在广泛实验中表现出优于现有SOTA方法的卓越性能。

Conclusion: ImCoref-CeS框架成功结合了增强型监督模型和LLM的优势，有效提升了指代消解任务的性能，达到了SOTA水平。

Abstract: Coreference Resolution (CR) is a critical task in Natural Language Processing
(NLP). Current research faces a key dilemma: whether to further explore the
potential of supervised neural methods based on small language models, whose
detect-then-cluster pipeline still delivers top performance, or embrace the
powerful capabilities of Large Language Models (LLMs). However, effectively
combining their strengths remains underexplored. To this end, we propose
\textbf{ImCoref-CeS}, a novel framework that integrates an enhanced supervised
model with LLM-based reasoning. First, we present an improved CR method
(\textbf{ImCoref}) to push the performance boundaries of the supervised neural
method by introducing a lightweight bridging module to enhance long-text
encoding capability, devising a biaffine scorer to comprehensively capture
positional information, and invoking a hybrid mention regularization to improve
training efficiency. Importantly, we employ an LLM acting as a multi-role
Checker-Splitter agent to validate candidate mentions (filtering out invalid
ones) and coreference results (splitting erroneous clusters) predicted by
ImCoref. Extensive experiments demonstrate the effectiveness of ImCoref-CeS,
which achieves superior performance compared to existing state-of-the-art
(SOTA) methods.

</details>


### [51] [Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models](https://arxiv.org/abs/2510.10252)
*Samir Abdaljalil,Erchin Serpedin,Khalid Qaraqe,Hasan Kurban*

Main category: cs.CL

TL;DR: 本文提出Audit-of-Understanding (AoU)框架，通过分解、审计和限制推理于已验证前提，有效解决大型语言模型(LLMs)因未经证实假设导致的推理幻觉，显著提升数学推理任务的准确性和忠实度。


<details>
  <summary>Details</summary>
Motivation: LLMs常生成看似连贯但基于未经证实假设的推理链，导致幻觉结论。现有工作主要处理事实性幻觉或依赖事后验证，未能充分解决推理引起的幻觉问题。

Method: 提出Audit-of-Understanding (AoU)框架，这是一个“后验约束推理”范式，包含三个阶段：1) 将查询分解为候选假设；2) 审计这些假设的支持度；3) 仅基于已验证的假设进行推理。该方法在理论上提供了完美验证下的保证、不完美审计下的超额风险界限以及可处理性分析。

Result: AoU在GSM8K、MultiArith和SVAMP等数据集上显著提高了准确性和忠实度，相较于Chain-of-Thought、Self-Consistency和CoT-Decoding，GSM8K提升高达30%，MultiArith提升45%，SVAMP稳定提升20-28%。

Conclusion: AoU框架通过约束推理于验证过的知识，有效减少了LLM的推理幻觉，并在多个数学推理任务上取得了显著的性能提升，为构建更可靠的LLM推理系统提供了新途径。

Abstract: Large language models (LLMs) often generate reasoning traces that appear
coherent but rest on unsupported assumptions, leading to hallucinated
conclusions. Prior work mainly addresses factual hallucinations or relies on
post-hoc verification, leaving reasoning-induced hallucinations largely
unaddressed. We propose Audit-of-Understanding (AoU), a framework that
constrains inference to validated premises through three phases: (1)
decomposing a query into candidate assumptions, (2) auditing their support, and
(3) conditioning inference only on the validated subset. Formally, AoU is
\emph{posterior-constrained inference}, connecting to selective prediction and
rejection learning. Our contributions are threefold: (i) theoretical guarantees
under perfect validation, (ii) excess-risk bounds under imperfect audits, and
(iii) tractability analysis. Empirically, AoU improves both accuracy and
faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on
GSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over
Chain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at
https://anonymous.4open.science/r/audit-of-understanding-E28B.

</details>


### [52] [Backdoor Collapse: Eliminating Unknown Threats via Known Backdoor Aggregation in Language Models](https://arxiv.org/abs/2510.10265)
*Liang Lin,Miao Yu,Moayad Aloqaily,Zhenhong Zhou,Kun Wang,Linsey Pang,Prakhar Mehrotra,Qingsong Wen*

Main category: cs.CL

TL;DR: 提出一种无需触发器先验知识的LLM后门防御框架\ourmethod，通过聚合后门表征和恢复微调，有效抵御后门攻击并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）面临严重的后门攻击威胁，且现有防御方案依赖于不切实际的触发器设置先验知识。

Method: \ourmethod基于一个关键观察：当故意向已被攻击的模型注入已知后门时，现有未知后门和新注入后门会在表征空间中聚合。该方法分两阶段：1) 注入已知触发器以聚合后门表征；2) 进行恢复微调以恢复良性输出。

Result: (I) 将平均攻击成功率降至4.41%，优于现有基线28.1%~69.3%。(II) 清洁准确性和实用性保持在原始模型0.5%以内。(III) 防御对不同类型的后门具有泛化性。

Conclusion: \ourmethod是一种高效、实用且无需触发器先验知识的LLM后门防御框架，能显著降低攻击成功率并保持模型性能，适用于实际部署。

Abstract: Backdoor attacks are a significant threat to large language models (LLMs),
often embedded via public checkpoints, yet existing defenses rely on
impractical assumptions about trigger settings. To address this challenge, we
propose \ourmethod, a defense framework that requires no prior knowledge of
trigger settings. \ourmethod is based on the key observation that when
deliberately injecting known backdoors into an already-compromised model, both
existing unknown and newly injected backdoors aggregate in the representation
space. \ourmethod leverages this through a two-stage process: \textbf{first},
aggregating backdoor representations by injecting known triggers, and
\textbf{then}, performing recovery fine-tuning to restore benign outputs.
Extensive experiments across multiple LLM architectures demonstrate that: (I)
\ourmethod reduces the average Attack Success Rate to 4.41\% across multiple
benchmarks, outperforming existing baselines by 28.1\%$\sim$69.3\%$\uparrow$.
(II) Clean accuracy and utility are preserved within 0.5\% of the original
model, ensuring negligible impact on legitimate tasks. (III) The defense
generalizes across different types of backdoors, confirming its robustness in
practical deployment scenarios.

</details>


### [53] [On the Entity-Level Alignment in Crosslingual Consistency](https://arxiv.org/abs/2510.10280)
*Yihong Liu,Mingyang Wang,François Yvon,Hinrich Schütze*

Main category: cs.CL

TL;DR: 多语言大模型在跨语言事实一致性方面存在挑战，这可能源于实体对齐失败。本文提出SubSub和SubInj两种方法，通过在提示中整合英文主题翻译，显著提升了事实召回的准确性和一致性，并通过模型内部的枢纽语言处理强化了实体表示对齐。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型（LLMs）应在不同语言间一致地召回事实知识，但这种跨语言一致性及其常见失败的原因尚不明确。研究动机是探究这些不一致性是否源于实体对齐失败，即未能将主题和客体实体映射到跨语言共享的概念空间中。

Method: 首先，通过实体级别的（主语和宾语）翻译任务评估实体对齐情况。在此基础上，提出并实施了两种方法：SubSub和SubInj，它们将主语的英文翻译整合到跨语言提示中。最后，进行了机制分析以揭示其工作原理。

Result: 研究发现，事实召回的一致性与实体对齐程度密切相关，主语或宾语的对齐失败常导致不一致性。所提出的SubSub和SubInj方法显著提升了事实召回的准确性和一致性。

Conclusion: 提出的干预措施通过模型内部的枢纽语言处理，强化了概念空间中的实体表示对齐，为改善多语言事实预测提供了有效且实用的策略。

Abstract: Multilingual large language models (LLMs) are expected to recall factual
knowledge consistently across languages. However, the factors that give rise to
such crosslingual consistency -- and its frequent failure -- remain poorly
understood. In this work, we hypothesize that these inconsistencies may arise
from failures in entity alignment, the process of mapping subject and object
entities into a shared conceptual space across languages. To test this, we
assess alignment through entity-level (subject and object) translation tasks,
and find that consistency is strongly correlated with alignment across all
studied models, with misalignment of subjects or objects frequently resulting
in inconsistencies. Building on this insight, we propose SubSub and SubInj, two
effective methods that integrate English translations of subjects into prompts
across languages, leading to substantial gains in both factual recall accuracy
and consistency. Finally, our mechanistic analysis reveals that these
interventions reinforce the entity representation alignment in the conceptual
space through model's internal pivot-language processing, offering effective
and practical strategies for improving multilingual factual prediction.

</details>


### [54] [MatryoshkaThinking: Recursive Test-Time Scaling Enables Efficient Reasoning](https://arxiv.org/abs/2510.10293)
*Hongwei Chen,Yishu Lei,Dan Zhang,Bo Ke,Danxiang Zhu,Xuyi Chen,Yuxiang Lu,Zhengjie Huang,Shikun Feng,Jingzhou He,Yu Sun,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 本文提出MatryoshkaThinking，一种新型测试时扩展方法，通过递归利用模型推理、验证和摘要能力，在显著降低计算成本的同时（DeepConf的4%计算量），在AIME2025上实现99.79的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型中的测试时扩展范式虽有前景，但现有方法（如DeepConf）为达到竞争性结果通常会产生巨大的计算开销。

Method: 提出MatryoshkaThinking方法。其核心在于递归利用模型固有的推理、验证和摘要能力，以提高正确解决方案的保留率，并缩小Pass@k和Pass@1之间的差距。

Result: MatryoshkaThinking在AIME2025上获得了99.79分，仅使用了DeepConf所需计算量的4%。该方法在多个开源模型和挑战性多模态推理基准上均被证明有效且具有通用性。

Conclusion: 研究结果为设计高效、可扩展的先进语言模型测试时推理策略提供了新见解。

Abstract: Test-time scaling has emerged as a promising paradigm in language modeling,
wherein additional computational resources are allocated during inference to
enhance model performance. Recent approaches, such as DeepConf, have
demonstrated the efficacy of this strategy, however, they often incur
substantial computational overhead to achieve competitive results. In this
work, we propose MatryoshkaThinking, a novel method that significantly reduces
computational cost while maintaining state-of-the-art performance.
Specifically, MatryoshkaThinking attains a score of 99.79 on AIME2025 using
only 4% of the computation required by DeepConf. The core of our approach lies
in the recursive exploitation of the model's intrinsic capabilities in
reasoning, verification, and summarization, which collectively enhance the
retention of correct solutions and reduce the disparity between Pass@k and
Pass@1. Comprehensive evaluations across multiple open-source models and
challenging multi-modal reasoning benchmarks validate the effectiveness and
generality of our method. These findings offer new insights into the design of
efficient and scalable test-time inference strategies for advanced language
models.

</details>


### [55] [Are LLMs Empathetic to All? Investigating the Influence of Multi-Demographic Personas on a Model's Empathy](https://arxiv.org/abs/2510.10328)
*Ananya Malik,Nazanin Sabri,Melissa Karnaze,Mai Elsherief*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLMs）的共情能力受用户人口统计学属性（年龄、文化、性别）的深刻影响，且在多属性组合下共情模式可能改变。LLMs的共情表现虽大致反映现实趋势，但对特定群体（如儒家文化背景）存在显著偏差，强调需开发考虑多样性的共情感知型LLMs。


<details>
  <summary>Details</summary>
Motivation: LLMs的自然对话能力依赖于共情理解和响应用户。然而，情感体验受人口统计学和文化背景影响。因此，研究旨在探讨LLMs能否在多样化的用户群体中表现出公平的共情。

Method: 提出了一个框架，通过交叉分析来研究LLMs的认知和情感共情如何因人口统计学属性定义的各种用户角色而异。构建了315个由年龄、文化和性别组合而成的独特用户角色，并在四个LLMs上进行了研究，结合定量发现和定性洞察。

Result: 结果显示，人口统计学属性深刻影响LLM的共情反应。有趣的是，同时添加多个属性会减弱和逆转预期的共情模式。LLMs的共情表现大致反映了现实世界的共情趋势，但对某些群体（如儒家文化背景的用户）存在显著偏差。

Conclusion: 研究强调了设计共情感知型LLMs的重要性，这些模型需考虑人口统计学多样性，以促进更具包容性和公平性的模型行为。

Abstract: Large Language Models' (LLMs) ability to converse naturally is empowered by
their ability to empathetically understand and respond to their users. However,
emotional experiences are shaped by demographic and cultural contexts. This
raises an important question: Can LLMs demonstrate equitable empathy across
diverse user groups? We propose a framework to investigate how LLMs' cognitive
and affective empathy vary across user personas defined by intersecting
demographic attributes. Our study introduces a novel intersectional analysis
spanning 315 unique personas, constructed from combinations of age, culture,
and gender, across four LLMs. Results show that attributes profoundly shape a
model's empathetic responses. Interestingly, we see that adding multiple
attributes at once can attenuate and reverse expected empathy patterns. We show
that they broadly reflect real-world empathetic trends, with notable
misalignments for certain groups, such as those from Confucian culture. We
complement our quantitative findings with qualitative insights to uncover model
behaviour patterns across different demographic groups. Our findings highlight
the importance of designing empathy-aware LLMs that account for demographic
diversity to promote more inclusive and equitable model behaviour.

</details>


### [56] [End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs](https://arxiv.org/abs/2510.10329)
*Nam Luu,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文提出一种结合预训练语音编码器和大型语言模型的端到端架构，可同时进行语音识别和语音翻译，在英德任务上优于SeamlessM4T并媲美级联系统。


<details>
  <summary>Details</summary>
Motivation: 探索一种结合预训练语音编码器和大型语言模型的端到端架构，以同时执行自动语音识别（ASR）和语音翻译（ST）任务。

Method: 采用一种结合预训练语音编码器（pre-trained speech encoders）和大型语言模型（LLMs）的端到端（end-to-end）架构。

Result: 在英德语对实验中，该模型不仅翻译结果优于大型基础端到端多模态翻译模型SeamlessM4T，而且性能可与Whisper和NLLB组成的级联系统相媲美，COMET_DA22指标得分提高高达8%。

Conclusion: 结合预训练语音编码器和LLMs的端到端架构能有效实现同步ASR和ST，表现出超越现有先进模型并匹配级联系统的竞争力。

Abstract: Speech Translation (ST) is a machine translation task that involves
converting speech signals from one language to the corresponding text in
another language; this task has two different approaches, namely the
traditional cascade and the more recent end-to-end. This paper explores a
combined end-to-end architecture of pre-trained speech encoders and Large
Language Models (LLMs) for performing both Automatic Speech Recognition (ASR)
and ST simultaneously. Experiments with the English-to-German language pair
show that our best model not only can achieve better translation results than
SeamlessM4T, a large foundational end-to-end, multi-modal translation model,
but can also match the performance of a cascaded system with Whisper and NLLB,
with up to a score gain of 8% in $\text{COMET}^{\text{DA}}_{22}$ metric.

</details>


### [57] [ASC analyzer: A Python package for measuring argument structure construction usage in English texts](https://arxiv.org/abs/2510.10384)
*Hakyung Sung,Kristopher Kyle*

Main category: cs.CL

TL;DR: 本文介绍了一个名为ASC分析器的Python工具，用于自动测量和分析二语写作中的论元结构构式（ASCs）及其相关指标。


<details>
  <summary>Details</summary>
Motivation: 论元结构构式（ASCs）是分析二语（L2）熟练度的重要理论工具，但目前缺乏可扩展且系统的工具来有效测量其使用。

Method: 开发并引入了ASC分析器，这是一个公开可用的Python软件包。该分析器能够自动标记ASCs，并计算50个多样化指标，涵盖多样性、比例、频率和ASC-动词词形关联强度。通过双变量和多变量分析，检验了基于ASC的指标与二语写作分数之间的关系，以证明其效用。

Result: 摘要中并未直接报告具体的分析结果，而是阐述了该工具如何通过检验ASC指标与二语写作分数的关系来展示其实用性。

Conclusion: 本文主要介绍了一个新的分析工具及其功能，旨在解决二语熟练度分析中ASC测量工具的空白问题，并展示了其潜在的应用价值。

Abstract: Argument structure constructions (ASCs) offer a theoretically grounded lens
for analyzing second language (L2) proficiency, yet scalable and systematic
tools for measuring their usage remain limited. This paper introduces the ASC
analyzer, a publicly available Python package designed to address this gap. The
analyzer automatically tags ASCs and computes 50 indices that capture
diversity, proportion, frequency, and ASC-verb lemma association strength. To
demonstrate its utility, we conduct both bivariate and multivariate analyses
that examine the relationship between ASC-based indices and L2 writing scores.

</details>


### [58] [RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models](https://arxiv.org/abs/2510.10390)
*Aashiq Muhamed,Leonardo F. R. Ribeiro,Markus Dreyer,Virginia Smith,Mona T. Diab*

Main category: cs.CL

TL;DR: RAG系统中文本拒答能力对安全性至关重要但模型表现不佳，现有基准评估有缺陷。本文提出RefusalBench，一种生成式评估方法，发现前沿模型系统性失败，但证实拒答能力可训练且对齐敏感。


<details>
  <summary>Details</summary>
Motivation: RAG系统中语言模型基于有缺陷上下文选择性拒答的能力对系统安全性至关重要，但现有模型在此方面表现不足，即使是前沿模型在多文档任务中拒答准确率低于50%，且现有静态基准无法可靠评估此能力。

Method: 引入RefusalBench，一种生成式评估方法，通过受控语言扰动程序化创建诊断测试用例。该框架采用176种独特的扰动策略，涵盖六类信息不确定性和三种强度级别。

Result: 1. 前沿模型在多文档任务中拒答准确率低于50%，并表现出危险的过度自信或过度谨慎。2. 现有静态基准因模型利用数据集特定伪影和记忆测试实例而无法可靠评估。3. 对30多个模型的评估揭示系统性失败模式：拒答能力包含可分离的检测和分类技能，且模型规模或扩展推理均未改善性能。

Conclusion: 1. 选择性拒答能力是可训练且对齐敏感的，为改进提供了明确途径。2. 发布RefusalBench-NQ和RefusalBench-GaRAGe两个基准以及完整的生成框架，以支持对该关键能力的持续、动态评估。

Abstract: The ability of language models in RAG systems to selectively refuse to answer
based on flawed context is critical for safety, yet remains a significant
failure point. Our large-scale study reveals that even frontier models struggle
in this setting, with refusal accuracy dropping below 50% on multi-document
tasks, while exhibiting either dangerous overconfidence or overcaution. Static
benchmarks fail to reliably evaluate this capability, as models exploit
dataset-specific artifacts and memorize test instances. We introduce
RefusalBench, a generative methodology that programmatically creates diagnostic
test cases through controlled linguistic perturbation. Our framework employs
176 distinct perturbation strategies across six categories of informational
uncertainty and three intensity levels. Evaluation of over 30 models uncovers
systematic failure patterns: refusal comprises separable detection and
categorization skills, and neither scale nor extended reasoning improves
performance. We find that selective refusal is a trainable, alignment-sensitive
capability, offering a clear path for improvement. We release two benchmarks --
RefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) --
and our complete generation framework to enable continued, dynamic evaluation
of this critical capability.

</details>


### [59] [AssoMem: Scalable Memory QA with Multi-Signal Associative Retrieval](https://arxiv.org/abs/2510.10397)
*Kai Zhang,Xinyuan Zhang,Ejaz Ahmed,Hongda Jiang,Caleb Kumar,Kai Sun,Zhaojiang Lin,Sanat Sharma,Shereen Oraby,Aaron Colak,Ahmed Aly,Anuj Kumar,Xiaozhong Liu,Xin Luna Dong*

Main category: cs.CL

TL;DR: AssoMem通过构建关联记忆图和整合多维检索信号（相关性、重要性、时间对齐），显著提高了AI助手动态上下文感知记忆召回的准确性，在多项基准测试中优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 在问答（QA）任务中，AI助手从大规模记忆中进行准确召回是一个核心挑战，尤其是在语义密集的场景下，现有方法主要依赖语义距离进行检索，效果受限。

Method: 受人类联想信息方式的启发，提出AssoMem框架。该框架构建一个关联记忆图，将对话语句锚定到自动提取的线索上，从而提供丰富的对话上下文组织视图并支持重要性感知排序。此外，AssoMem利用自适应互信息（MI）驱动的融合策略，整合了相关性、重要性和时间对齐等多维检索信号。

Result: 在三个基准测试和一个新引入的MeetingQA数据集上进行的大量实验表明，AssoMem持续优于最先进（SOTA）的基线模型。

Conclusion: AssoMem在上下文感知的记忆召回方面展现出卓越的性能，验证了其优越性。

Abstract: Accurate recall from large scale memories remains a core challenge for memory
augmented AI assistants performing question answering (QA), especially in
similarity dense scenarios where existing methods mainly rely on semantic
distance to the query for retrieval. Inspired by how humans link information
associatively, we propose AssoMem, a novel framework constructing an
associative memory graph that anchors dialogue utterances to automatically
extracted clues. This structure provides a rich organizational view of the
conversational context and facilitates importance aware ranking. Further,
AssoMem integrates multi-dimensional retrieval signals-relevance, importance,
and temporal alignment using an adaptive mutual information (MI) driven fusion
strategy. Extensive experiments across three benchmarks and a newly introduced
dataset, MeetingQA, demonstrate that AssoMem consistently outperforms SOTA
baselines, verifying its superiority in context-aware memory recall.

</details>


### [60] [STEAM: A Semantic-Level Knowledge Editing Framework for Large Language Models](https://arxiv.org/abs/2510.10398)
*Geunyeong Jeong,Juoh Sun,Seonghee Lee,Harksoo Kim*

Main category: cs.CL

TL;DR: 大语言模型知识编辑中，现有方法缺乏语义连贯性，导致编辑知识孤立。本文提出	extsc{Steam}框架，通过潜在空间语义对齐，将更新知识更好地融入模型，提升推理能力和语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的知识是静态的，无法及时反映世界变化。现有知识编辑方法（如“定位-编辑”）主要关注token级似然优化，却忽视了语义连贯性，导致编辑知识在模型潜在空间中孤立存在，难以与原有知识融合进行自然推理。

Method: 本文提出了	extsc{Steam}，一个语义层面的知识编辑框架。该方法首先识别目标表示作为更新事实关联的语义锚点，然后通过在优化过程中引入对齐损失，引导编辑事实的内部表示向这些锚点对齐，从而增强更新知识与模型现有知识结构的整合。

Result: 实验结果表明，	extsc{Steam}显著提高了模型利用编辑知识进行推理的能力，并增强了语义连贯性。

Conclusion: 潜在空间（latent-space）对齐对于实现可靠且连贯的知识编辑至关重要。

Abstract: Large Language Models store extensive factual knowledge acquired during
large-scale pre-training. However, this knowledge is inherently static,
reflecting only the state of the world at the time of training. Knowledge
editing has emerged as a promising solution for updating outdated or incorrect
facts without full retraining. However, most existing locate-and-edit methods
primarily focus on token-level likelihood optimization without addressing
semantic coherence. Our analysis reveals that such edited knowledge is often
encoded as isolated residual streams in the model's latent space, distinct from
pre-existing knowledge and bypassing natural reasoning process. To address
this, we propose \textsc{Steam}, a semantic-level knowledge editing framework
that enhances integration of updated knowledge into the model's knowledge
structure. \textsc{Steam} first identifies target representations as semantic
anchors for the updated factual association, then guides the internal
representation of the edited fact towards these anchors through an alignment
loss during optimization. Experimental results demonstrate that \textsc{Steam}
improves model's ability to reason with edited knowledge and enhances semantic
coherence, underscoring the importance of latent-space alignment for reliable
and coherent knowledge editing. The code is available at
https://github.com/GY-Jeong/STEAM.

</details>


### [61] [LONGQAEVAL: Designing Reliable Evaluations of Long-Form Clinical QA under Resource Constraints](https://arxiv.org/abs/2510.10415)
*Federica Bologna,Tiffany Pan,Matthew Wilkens,Yue Guo,Lucy Lu Wang*

Main category: cs.CL

TL;DR: 引入LongQAEval框架，比较粗粒度和细粒度评估方法在长篇临床问答系统中的表现，发现细粒度提高了正确性一致性，并指出选择性句子标注可降低成本且保持可靠性。


<details>
  <summary>Details</summary>
Motivation: 评估长篇临床问答系统（QA）资源密集且极具挑战性，需要医学专业知识，且在长篇文本上难以实现一致的人工判断。

Method: 提出LongQAEval评估框架和一系列评估建议。基于医生对300个由医生和LLM回答的真实患者问题的标注，比较了粗粒度答案级别与细粒度句子级别评估在正确性、相关性和安全性维度上的表现。

Result: 标注者间一致性（IAA）因维度而异：细粒度标注提高了正确性的一致性，粗粒度标注提高了相关性的一致性，而安全性判断则始终不一致。此外，仅标注一小部分句子即可达到与粗粒度标注相当的可靠性，从而减少成本和精力。

Conclusion: LongQAEval框架为长篇临床问答评估提供了有效方法。细粒度标注在提高正确性评估一致性方面表现优异，且通过选择性地标注少量句子即可显著降低评估成本和精力，同时保持可靠性。安全性判断仍是评估中的挑战。

Abstract: Evaluating long-form clinical question answering (QA) systems is
resource-intensive and challenging: accurate judgments require medical
expertise and achieving consistent human judgments over long-form text is
difficult. We introduce LongQAEval, an evaluation framework and set of
evaluation recommendations for limited-resource and high-expertise settings.
Based on physician annotations of 300 real patient questions answered by
physicians and LLMs, we compare coarse answer-level versus fine-grained
sentence-level evaluation over the dimensions of correctness, relevance, and
safety. We find that inter-annotator agreement (IAA) varies by dimension:
fine-grained annotation improves agreement on correctness, coarse improves
agreement on relevance, and judgments on safety remain inconsistent.
Additionally, annotating only a small subset of sentences can provide
reliability comparable to coarse annotations, reducing cost and effort.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [62] [TinyViT-Batten: Few-Shot Vision Transformer with Explainable Attention for Early Batten-Disease Detection on Pediatric MRI](https://arxiv.org/abs/2510.09649)
*Khartik Uppalapati,Bora Yimenicioglu,Shakeel Abdulkareem,Adan Eftekhari,Bhavya Uppalapati,Viraj Kamath*

Main category: cs.CV

TL;DR: 提出TinyViT-Batten，一个轻量级、少样本的Vision Transformer模型，用于儿科脑部MRI的早期Batten病检测，具有高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: Batten病是罕见的儿科神经退行性疾病，其早期MRI迹象微妙且常被漏诊。

Method: 开发TinyViT-Batten框架，通过知识蒸馏将大型ViT压缩成5M参数的TinyViT，并利用基于度量的少样本学习（原型损失，5-shot episodes）进行微调。同时整合Grad-CAM以实现预测的可解释性。

Result: 在包含79例Batten病MRI和90例年龄匹配对照的多中心数据集上，模型准确率约91%，ROC曲线下面积至少0.95，性能优于3D-ResNet和Swin-Tiny基线。灵敏度大于90%，特异性约90%。

Conclusion: 该模型体积小巧、性能强劲，为早期Batten病检测提供了一个实用的AI解决方案。

Abstract: Batten disease (neuronal ceroid lipofuscinosis) is a rare pediatric
neurodegenerative disorder whose early MRI signs are subtle and often missed.
We propose TinyViT-Batten, a few-shot Vision Transformer (ViT) framework to
detect early Batten disease from pediatric brain MRI with limited training
cases. We distill a large teacher ViT into a 5 M-parameter TinyViT and
fine-tune it using metric-based few-shot learning (prototypical loss with
5-shot episodes). Our model achieves high accuracy (approximately 91%) and area
under ROC of at least 0.95 on a multi-site dataset of 79 genetically confirmed
Batten-disease MRIs (27 CLN3 from the Hochstein natural-history study, 32 CLN2
from an international longitudinal cohort, 12 early-manifestation CLN2 cases
reported by Cokal et al., and 8 public Radiopaedia scans) together with 90
age-matched controls, outperforming a 3D-ResNet and Swin-Tiny baseline. We
further integrate Gradient-weighted Class Activation Mapping (Grad-CAM) to
highlight disease-relevant brain regions, enabling explainable predictions. The
model's small size and strong performance (sensitivity greater than 90%,
specificity approximately 90%) demonstrates a practical AI solution for early
Batten disease detection.

</details>


### [63] [Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition](https://arxiv.org/abs/2510.09653)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: 全面概述Ultralytics YOLO系列目标检测器，涵盖其架构演进、性能基准测试、部署应用及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 提供Ultralytics YOLO家族目标检测器的全面视图，分析其架构演变，量化性能，探讨部署视角，并指出未来研究方向。

Method: 追溯YOLO系列（从YOLOv5到YOLOv26）的架构演进及关键创新。在MS COCO数据集上对YOLOv5、YOLOv8、YOLOv11、YOLOv26进行定量基准测试，并与其他模型交叉对比，分析精度、召回率、mAP和推理速度。讨论部署格式、量化策略和实际应用。

Result: 通过基准测试，详细量化了YOLOv5、YOLOv8、YOLOv11和YOLOv26之间以及与其他模型的性能，揭示了精度与效率的权衡。突出了各YOLO版本引入的关键创新，如YOLOv26的DFL移除、NMS-free推理和ProgLoss等。

Conclusion: 深入分析了YOLO家族的发展历程和能力，识别了当前面临的挑战（如密集场景限制），并提出了未来的研究方向，包括CNN-Transformer融合、开放词汇检测和边缘感知训练。

Abstract: This paper presents a comprehensive overview of the Ultralytics YOLO(You Only
Look Once) family of object detectors, focusing the architectural evolution,
benchmarking, deployment perspectives, and future challenges. The review begins
with the most recent release, YOLO26 (YOLOv26), which introduces key
innovations including Distribution Focal Loss (DFL) removal, native NMS-free
inference, Progressive Loss Balancing (ProgLoss), Small-Target-Aware Label
Assignment (STAL), and the MuSGD optimizer for stable training. The progression
is then traced through YOLO11, with its hybrid task assignment and
efficiency-focused modules; YOLOv8, which advanced with a decoupled detection
head and anchor-free predictions; and YOLOv5, which established the modular
PyTorch foundation that enabled modern YOLO development. Benchmarking on the MS
COCO dataset provides a detailed quantitative comparison of YOLOv5, YOLOv8,
YOLO11, and YOLO26, alongside cross-comparisons with YOLOv12, YOLOv13, RT-DETR,
and DEIM. Metrics including precision, recall, F1 score, mean Average
Precision, and inference speed are analyzed to highlight trade-offs between
accuracy and efficiency. Deployment and application perspectives are further
discussed, covering export formats, quantization strategies, and real-world use
in robotics, agriculture, surveillance, and manufacturing. Finally, the paper
identifies challenges and future directions, including dense-scene limitations,
hybrid CNN-Transformer integration, open-vocabulary detection, and edge-aware
training approaches.

</details>


### [64] [TreeNet: Layered Decision Ensembles](https://arxiv.org/abs/2510.09654)
*Zeshan Khan*

Main category: cs.CV

TL;DR: TreeNet是一种结合神经网络、集成学习和决策树的新型分层集成学习方法，旨在解决医学图像分析中数据有限的挑战。它在数据受限情况下仍能保持高性能，并支持实时应用，具有良好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析中，神经网络、决策树和集成学习等现有方法在数据量充足时表现良好，但该领域普遍面临数据有限和数据置信度不足的挑战。

Method: 本文提出TreeNet，一种新颖的分层决策集成学习方法，专为医学图像分析定制。它通过整合神经网络、集成学习和基于树的决策模型的关键特征构建，具有强大的适应性、可解释性和深刻的决策洞察力。

Result: TreeNet在使用完整训练数据时F1-score高达0.85；使用50%训练数据时F1-score为0.77，性能下降仅0.08，并减少了训练时间。评估速度达到每秒32帧，适用于实时应用。

Conclusion: TreeNet在医学图像分析领域（特别是在数据有限和实时分析场景下）表现出高效性和可用性，并提供优越的性能和可解释性。

Abstract: Within the domain of medical image analysis, three distinct methodologies
have demonstrated commendable accuracy: Neural Networks, Decision Trees, and
Ensemble-Based Learning Algorithms, particularly in the specialized context of
genstro institutional track abnormalities detection. These approaches exhibit
efficacy in disease detection scenarios where a substantial volume of data is
available. However, the prevalent challenge in medical image analysis pertains
to limited data availability and data confidence. This paper introduces
TreeNet, a novel layered decision ensemble learning methodology tailored for
medical image analysis. Constructed by integrating pivotal features from neural
networks, ensemble learning, and tree-based decision models, TreeNet emerges as
a potent and adaptable model capable of delivering superior performance across
diverse and intricate machine learning tasks. Furthermore, its interpretability
and insightful decision-making process enhance its applicability in complex
medical scenarios. Evaluation of the proposed approach encompasses key metrics
including Accuracy, Precision, Recall, and training and evaluation time. The
methodology resulted in an F1-score of up to 0.85 when using the complete
training data, with an F1-score of 0.77 when utilizing 50\% of the training
data. This shows a reduction of F1-score of 0.08 while in the reduction of 50\%
of the training data and training time. The evaluation of the methodology
resulted in the 32 Frame per Second which is usable for the realtime
applications. This comprehensive assessment underscores the efficiency and
usability of TreeNet in the demanding landscape of medical image analysis
specially in the realtime analysis.

</details>


### [65] [OmniSAT: Compact Action Token, Faster Auto Regression](https://arxiv.org/abs/2510.09667)
*Huaihai Lyu,Chaofan Chen,Senwei Xie,Pengwei Wang,Xiansheng Chen,Shanghang Zhang,Changsheng Xu*

Main category: cs.CV

TL;DR: 本文提出Omni Swift Action Tokenizer (OmniSAT)，通过B-Spline编码和多阶段残差量化，实现VLA模型中动作序列的高效压缩和高质量重建，从而加速AR模型训练并提升性能，同时支持跨具身学习。


<details>
  <summary>Details</summary>
Motivation: 现有自回归（AR）视觉-语言-动作（VLA）模型在处理长且高维的动作序列时，其序列压缩方法存在重建质量差或压缩效率低的问题。

Method: 引入Omni Swift Action Tokenizer (OmniSAT)。首先，使用B-Spline编码对值范围和时间范围进行标准化。接着，对位置、旋转和夹持器子空间应用多阶段残差量化，生成具有粗到细粒度的离散压缩动作标记。此外，开发了一种基于统一动作模式空间的跨具身学习策略，联合利用机器人和人类演示，以实现可扩展的辅助监督。

Result: 在Droid数据集上预训练后，OmniSAT的离散分词将训练序列长度缩短了6.8倍，并降低了目标熵。它在实现更高压缩率的同时保持了重建质量，从而加速了AR模型的训练收敛并提高了模型性能。

Conclusion: OmniSAT为自回归VLA模型提供了卓越的动作压缩和重建能力，显著提升了训练效率和模型性能。其统一的动作模式空间也成功支持了跨具身学习策略，有效整合了异构演示数据。

Abstract: Existing Vision-Language-Action (VLA) models can be broadly categorized into
diffusion-based and auto-regressive (AR) approaches: diffusion models capture
continuous action distributions but rely on computationally heavy iterative
denoising. In contrast, AR models enable efficient optimization and flexible
sequence construction, making them better suited for large-scale pretraining.
To further improve AR efficiency, particularly when action chunks induce
extended and high-dimensional sequences, prior work applies entropy-guided and
token-frequency techniques to shorten the sequence length. However, such
compression struggled with \textit{poor reconstruction or inefficient
compression}. Motivated by this, we introduce an Omni Swift Action Tokenizer,
which learns a compact, transferable action representation. Specifically, we
first normalize value ranges and temporal horizons to obtain a consistent
representation with B-Spline encoding. Then, we apply multi-stage residual
quantization to the position, rotation, and gripper subspaces, producing
compressed discrete tokens with coarse-to-fine granularity for each part. After
pre-training on the large-scale dataset Droid, the resulting discrete
tokenization shortens the training sequence by 6.8$\times$, and lowers the
target entropy. To further explore the potential of OmniSAT, we develop a
cross-embodiment learning strategy that builds on the unified action-pattern
space and jointly leverages robot and human demonstrations. It enables scalable
auxiliary supervision from heterogeneous egocentric videos. Across diverse
real-robot and simulation experiments, OmniSAT encompasses higher compression
while preserving reconstruction quality, enabling faster AR training
convergence and model performance.

</details>


### [66] [Knowledge-Aware Mamba for Joint Change Detection and Classification from MODIS Times Series](https://arxiv.org/abs/2510.09679)
*Zhengsen Xu,Yimin Zhu,Zack Dewis,Mabel Heffring,Motasem Alkayid,Saeid Taleghanidoozdoozan,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为KAMamba的新型知识感知Mamba模型，通过结合知识驱动的损失、多任务学习和改进的Mamba模块，显著提高了MODIS时间序列在变化检测和土地覆盖分类中的准确性。


<details>
  <summary>Details</summary>
Motivation: MODIS时间序列变化检测对于环境监测至关重要，但由于混合像元、时空信息耦合效应和背景类别异质性等MODIS特有难题，这是一项极具挑战性的任务。

Method: 提出了一种新型知识感知Mamba（KAMamba）模型，主要贡献包括：1) 设计知识驱动的转换矩阵引导方法，引入知识感知转换损失（KAT-loss）以利用类别转换知识。2) 采用多任务学习，结合预变化分类损失（PreC-loss）、后变化分类损失（PostC-loss）和变化检测损失（Chg-loss）来增强模型约束。3) 设计新型时空光谱Mamba（SSTMamba）模块，以解耦MODIS时间序列中的信息耦合。4) 在SSTMamba中使用了稀疏可变形Mamba（SDMamba）骨干网络，以提高效率并降低计算成本。

Result: 在加拿大萨斯喀彻温省的MODIS时间序列数据集上进行评估，结果显示：在变化检测方面，平均F1分数比基线提高了约1.5-6%；在LULC分类方面，OA、AA和Kappa指标均提高了约2%。

Conclusion: 本文提出的知识感知Mamba (KAMamba) 模型通过有效整合知识驱动的损失、多任务学习和优化的Mamba模块，成功应对了MODIS时间序列变化检测的挑战，显著提升了变化检测和土地覆盖分类的性能。

Abstract: Although change detection using MODIS time series is critical for
environmental monitoring, it is a highly challenging task due to key MODIS
difficulties, e.g., mixed pixels, spatial-spectral-temporal information
coupling effect, and background class heterogeneity. This paper presents a
novel knowledge-aware Mamba (KAMamba) for enhanced MODIS change detection, with
the following contributions. First, to leverage knowledge regarding class
transitions, we design a novel knowledge-driven transition-matrix-guided
approach, leading to a knowledge-aware transition loss (KAT-loss) that can
enhance detection accuracies. Second, to improve model constraints, a
multi-task learning approach is designed, where three losses, i.e., pre-change
classification loss (PreC-loss), post-change classification loss (PostC-loss),
and change detection loss (Chg-loss) are used for improve model learning.
Third, to disentangle information coupling in MODIS time series, novel
spatial-spectral-temporal Mamba (SSTMamba) modules are designed. Last, to
improve Mamba model efficiency and remove computational cost, a sparse and
deformable Mamba (SDMamba) backbone is used in SSTMamba. On the MODIS
time-series dataset for Saskatchewan, Canada, we evaluate the method on
land-cover change detection and LULC classification; results show about 1.5-6%
gains in average F1 for change detection over baselines, and about 2%
improvements in OA, AA, and Kappa for LULC classification.

</details>


### [67] [NNDM: NN_UNet Diffusion Model for Brain Tumor Segmentation](https://arxiv.org/abs/2510.09681)
*Sashank Makanaboyina*

Main category: cs.CV

TL;DR: NNDM结合NN-UNet和扩散模型，通过学习残差误差分布迭代细化脑肿瘤分割，在BraTS 2021数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有CNN模型（如U-Net）在脑肿瘤MRI图像分割中存在泛化性差、边界精度低和数据多样性不足等问题，影响诊断和治疗计划。

Method: 提出NNDM混合框架，结合NN-UNet的特征提取能力与扩散概率模型的生成能力。扩散模型通过学习预测与真实分割掩膜之间的残差误差分布，逐步细化NN-UNet生成的分割掩膜，迭代纠正结构不一致并增强肿瘤边界刻画。

Result: 在BraTS 2021数据集上的实验表明，NNDM在Dice系数和Hausdorff距离等指标上优于传统U-Net和基于Transformer的基线模型，且扩散引导的细化增强了跨模态和肿瘤子区域的鲁棒性。

Conclusion: NNDM为结合确定性分割网络与随机扩散模型提供新方向，推动了自动化脑肿瘤分析的最新进展。

Abstract: Accurate detection and segmentation of brain tumors in magnetic resonance
imaging (MRI) are critical for effective diagnosis and treatment planning.
Despite advances in convolutional neural networks (CNNs) such as U-Net,
existing models often struggle with generalization, boundary precision, and
limited data diversity. To address these challenges, we propose NNDM (NN\_UNet
Diffusion Model)a hybrid framework that integrates the robust feature
extraction of NN-UNet with the generative capabilities of diffusion
probabilistic models. In our approach, the diffusion model progressively
refines the segmentation masks generated by NN-UNet by learning the residual
error distribution between predicted and ground-truth masks. This iterative
denoising process enables the model to correct fine structural inconsistencies
and enhance tumor boundary delineation. Experiments conducted on the BraTS 2021
datasets demonstrate that NNDM achieves superior performance compared to
conventional U-Net and transformer-based baselines, yielding improvements in
Dice coefficient and Hausdorff distance metrics. Moreover, the diffusion-guided
refinement enhances robustness across modalities and tumor subregions. The
proposed NNDM establishes a new direction for combining deterministic
segmentation networks with stochastic diffusion models, advancing the state of
the art in automated brain tumor analysis.

</details>


### [68] [Adaptive Fusion Network with Temporal-Ranked and Motion-Intensity Dynamic Images for Micro-expression Recognition](https://arxiv.org/abs/2510.09730)
*Thi Bich Phuong Man,Luu Tu Nguyen,Vu Tram Anh Khuong,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: 本文提出一种新的微表情识别方法，结合两种互补的动态图像表示（时间排序和运动强度）和一个自适应融合网络，在三个基准数据集上取得了最先进的识别性能，尤其在CASME-II上刷新了记录。


<details>
  <summary>Details</summary>
Motivation: 微表情是极其细微且短暂的面部变化，能揭示真实情感，在测谎、行为分析和心理评估中具有重要价值。提高微表情识别（MER）的准确性和泛化能力是其研究的驱动力。

Method: 1) 提出了两种互补的动态图像表示：时序排序动态图像（强调时间进程）和运动强度动态图像（通过结合运动强度的帧重排序突出微弱运动）。2) 设计了一个自适应融合网络（AFN），自动学习优化融合这两种表示，以增强判别性微表情特征并抑制噪声。

Result: 在CASME-II数据集上，AFN达到了93.95%的准确率和0.897的UF1，刷新了SOTA记录。在SAMM上，取得了82.47%的准确率和0.665的UF1，表现出更均衡的识别性能。在MMEW上，模型达到了76.00%的准确率，验证了其泛化能力。结果表明，所提出的输入表示和架构对MER性能的提升都起着重要作用。

Conclusion: 本研究证实，所提出的输入表示和自适应融合网络架构对于提升微表情识别性能至关重要。这些成果为情感计算、测谎和人机交互等领域的进一步研究及实际应用奠定了坚实基础。

Abstract: Micro-expressions (MEs) are subtle, transient facial changes with very low
intensity, almost imperceptible to the naked eye, yet they reveal a person
genuine emotion. They are of great value in lie detection, behavioral analysis,
and psychological assessment. This paper proposes a novel MER method with two
main contributions. First, we propose two complementary representations -
Temporal-ranked dynamic image, which emphasizes temporal progression, and
Motion-intensity dynamic image, which highlights subtle motions through a frame
reordering mechanism incorporating motion intensity. Second, we propose an
Adaptive fusion network, which automatically learns to optimally integrate
these two representations, thereby enhancing discriminative ME features while
suppressing noise. Experiments on three benchmark datasets (CASME-II, SAMM and
MMEW) demonstrate the superiority of the proposed method. Specifically, AFN
achieves 93.95 Accuracy and 0.897 UF1 on CASME-II, setting a new
state-of-the-art benchmark. On SAMM, the method attains 82.47 Accuracy and
0.665 UF1, demonstrating more balanced recognition across classes. On MMEW, the
model achieves 76.00 Accuracy, further confirming its generalization ability.
The obtained results show that both the input and the proposed architecture
play important roles in improving the performance of MER. Moreover, they
provide a solid foundation for further research and practical applications in
the fields of affective computing, lie detection, and human-computer
interaction.

</details>


### [69] [Multi Camera Connected Vision System with Multi View Analytics: A Comprehensive Survey](https://arxiv.org/abs/2510.09731)
*Muhammad Munsif,Waqas Ahmad,Amjid Ali,Mohib Ullah,Adnan Hussain,Sung Wook Baik*

Main category: cs.CV

TL;DR: 本文首次对多视角多摄像头(MVMC)连接视觉系统(CVS)进行了全面而整合的综述，统一了MVMC跟踪、重识别和行为理解，并提出了独特的分类法，总结了现有技术，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 连接视觉系统(CVS)在多领域应用广泛，但现有研究主要关注孤立任务且忽视多摄像头集成。在现实动态环境中部署CVS面临遮挡、视角多样性等挑战，缺乏对MVMC跟踪、重识别和行为理解的综合统一分析。

Method: 本综述提出了独特的分类法，将CVS分为MVMC跟踪、重识别、行为理解和组合方法四个关键部分。系统性地总结了最先进的数据集、方法、结果和评估指标。同时，识别并讨论了开放研究问题、挑战以及终身学习、隐私和联邦学习等新兴技术。

Result: 本综述提供了CVS的统一集成框架，促进了对MVMC跟踪、重识别和行为理解的综合理解。它结构化地展示了该领域的进展，并明确了需要解决的开放问题和新兴技术，为未来的研究奠定基础。

Conclusion: 本综述旨在激发创新解决方案，并指导未来研究，以提升CVS在复杂现实应用中的鲁棒性、效率和适应性，推动下一代智能自适应CVS的发展。

Abstract: Connected Vision Systems (CVS) are transforming a variety of applications,
including autonomous vehicles, smart cities, surveillance, and human-robot
interaction. These systems harness multi-view multi-camera (MVMC) data to
provide enhanced situational awareness through the integration of MVMC
tracking, re-identification (Re-ID), and action understanding (AU). However,
deploying CVS in real-world, dynamic environments presents a number of
challenges, particularly in addressing occlusions, diverse viewpoints, and
environmental variability. Existing surveys have focused primarily on isolated
tasks such as tracking, Re-ID, and AU, often neglecting their integration into
a cohesive system. These reviews typically emphasize single-view setups,
overlooking the complexities and opportunities provided by multi-camera
collaboration and multi-view data analysis. To the best of our knowledge, this
survey is the first to offer a comprehensive and integrated review of MVMC that
unifies MVMC tracking, Re-ID, and AU into a single framework. We propose a
unique taxonomy to better understand the critical components of CVS, dividing
it into four key parts: MVMC tracking, Re-ID, AU, and combined methods. We
systematically arrange and summarize the state-of-the-art datasets,
methodologies, results, and evaluation metrics, providing a structured view of
the field's progression. Furthermore, we identify and discuss the open research
questions and challenges, along with emerging technologies such as lifelong
learning, privacy, and federated learning, that need to be addressed for future
advancements. The paper concludes by outlining key research directions for
enhancing the robustness, efficiency, and adaptability of CVS in complex,
real-world applications. We hope this survey will inspire innovative solutions
and guide future research toward the next generation of intelligent and
adaptive CVS.

</details>


### [70] [Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping](https://arxiv.org/abs/2510.09741)
*Dwip Dalal,Gautam Vashishtha,Utkarsh Mishra,Jeonghwan Kim,Madhav Kanda,Hyeonjeong Ha,Svetlana Lazebnik,Heng Ji,Unnat Jain*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLMs）在处理细节和空间关系方面存在不足。AttWarp是一种轻量级测试时方法，利用MLLM的注意力机制对图像进行扭曲，将更多分辨率分配给重要区域，从而提高MLLMs的准确性、推理能力并减少幻觉，且无需修改模型或其权重。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在杂乱场景中常常忽略微小细节和空间关系，导致在细粒度感知接地方面出现错误。

Method: 引入AttWarp，一种轻量级方法。它在测试时利用MLLM的跨模态注意力机制对输入图像进行直线校正扭曲，将空间分辨率重新分配给模型认为重要的区域。此过程在不改变模型权重或架构的情况下，为与查询相关的内容分配更高分辨率，同时压缩信息较少的区域，从而在保留所有原始图像信息的同时非均匀地重新分布，使小物体和微妙关系更容易被模型读取，并保持全局上下文。

Result: AttWarp在五个基准测试（TextVQA, GQA, DocVQA, POPE, MMMU）和四种MLLMs（LLaVA, Qwen-VL, InternVL, InstructBLIP）上持续提高了准确性，增强了组合推理能力，并减少了幻觉。其性能优于四种在测试时操作原始图像的竞争基线方法。

Conclusion: 注意力引导的扭曲能够优先处理与查询相关的信息，同时保留上下文，并且相同的多模态大语言模型在接收此类扭曲输入时能表现出更好的性能。

Abstract: Multimodal large language models (MLLMs) often miss small details and spatial
relations in cluttered scenes, leading to errors in fine-grained perceptual
grounding. We introduce AttWarp, a lightweight method that allocates more
resolution to query-relevant content while compressing less informative areas,
all while preserving global context. At test time, the approach uses an MLLM's
cross-modal attention to perform rectilinear warping of the input image,
reallocating spatial resolution toward regions the model deems important,
without changing model weights or architecture. This attention-guided warping
preserves all original image information but redistributes it non-uniformly, so
small objects and subtle relationships become easier for the same model to read
while the global layout remains intact. Across five benchmarks (TextVQA, GQA,
DocVQA, POPE, MMMU) and four MLLMs (LLaVA, Qwen-VL, InternVL, and
InstructBLIP), AttWarp consistently improves accuracy, strengthens
compositional reasoning, and reduces hallucinations, outperforming four
competitive baselines that manipulate raw images at test time. Together, these
results show that attention-guided warping prioritizes information relevant to
the query while preserving context, and that the same MLLMs perform better when
given such warped inputs.

</details>


### [71] [Towards Understanding Ambiguity Resolution in Multimodal Inference of Meaning](https://arxiv.org/abs/2510.09815)
*Yufei Wang,Adriana Kovashka,Loretta Fernández,Marc N. Coutanche,Seth Wiener*

Main category: cs.CV

TL;DR: 研究多模态（图文）语境下外语学习者生词推断，分析影响因素，并评估AI系统预测能力。


<details>
  <summary>Details</summary>
Motivation: 探索一种新的外语学习环境，即学习者在描述配对图像的句子中推断生词含义。旨在识别促进推断成功的数据特征和学习者背景，并分析AI系统在此任务中的推理能力。

Method: 进行了人类参与者研究，使用不同的图像-文本对。分析了图像和文本的数据特征对参与者推断生词含义的影响，以及参与者语言背景与成功率的相关性。同时，分析了AI系统预测参与者表现的能力。

Result: 发现只有少数直观特征与参与者表现有强相关性，这表明需要进一步研究预测性特征。此外，发现了提高AI系统推理能力的有前景的未来方向。

Conclusion: 在多模态语境中推断外语生词具有复杂性，目前仅少数直观特征与人类表现强相关。AI系统在此领域展现出预测潜力，未来研究应侧重于发现更多预测性特征并提升AI的推理能力。

Abstract: We investigate a new setting for foreign language learning, where learners
infer the meaning of unfamiliar words in a multimodal context of a sentence
describing a paired image. We conduct studies with human participants using
different image-text pairs. We analyze the features of the data (i.e., images
and texts) that make it easier for participants to infer the meaning of a
masked or unfamiliar word, and what language backgrounds of the participants
correlate with success. We find only some intuitive features have strong
correlations with participant performance, prompting the need for further
investigating of predictive features for success in these tasks. We also
analyze the ability of AI systems to reason about participant performance, and
discover promising future directions for improving this reasoning ability.

</details>


### [72] [Task-Aware Resolution Optimization for Visual Large Language Models](https://arxiv.org/abs/2510.09822)
*Weiqing Luo,Zhen Tan,Yifan Li,Xinyu Zhao,Kwonjoon Lee,Behzad Dariush,Tianlong Chen*

Main category: cs.CV

TL;DR: 本文旨在解决现有视觉大语言模型（VLLM）因固定分辨率导致性能不佳的问题。通过研究不同视觉-语言任务的分辨率偏好，提出一个确定最佳分辨率的经验公式，并开发一种参数高效微调技术，使VLLM能适应此最佳分辨率，显著提升了模型在各种任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLLM（如LLaVA）预设固定分辨率，无法满足真实世界视觉-语言应用对不同感知粒度的需求，从而导致下游任务性能不佳。

Method: 1. 首次全面调查不同视觉-语言任务的分辨率偏好，发现其与图像复杂度和VLLM在不同输入分辨率下的不确定性方差存在关联。2. 基于此洞察，提出一个结合图像复杂度和不确定性方差的经验公式，以确定给定任务的最佳分辨率。3. 提出一种新颖的参数高效微调技术，将预训练VLLM的视觉输入分辨率扩展到所确定的最佳分辨率。

Result: 研究揭示了视觉-语言任务分辨率偏好与图像复杂度和VLLM不确定性方差的相关性。在各种视觉-语言任务上进行的广泛实验验证了所提方法的有效性，成功提升了VLLM的性能。

Conclusion: 通过深入分析任务分辨率偏好、提出最佳分辨率经验公式和参数高效微调技术，本文有效解决了VLLM固定分辨率带来的性能限制，使其能够自适应地处理不同感知粒度的视觉-语言任务，提高了模型泛化能力和性能。

Abstract: Real-world vision-language applications demand varying levels of perceptual
granularity. However, most existing visual large language models (VLLMs), such
as LLaVA, pre-assume a fixed resolution for downstream tasks, which leads to
subpar performance. To address this problem, we first conduct a comprehensive
and pioneering investigation into the resolution preferences of different
vision-language tasks, revealing a correlation between resolution preferences
with image complexity, and uncertainty variance of the VLLM at different image
input resolutions. Building on this insight, we propose an empirical formula to
determine the optimal resolution for a given vision-language task, combining
these two factors. Second, based on rigorous experiments, we propose a novel
parameter-efficient fine-tuning technique to extend the visual input resolution
of pre-trained VLLMs to the identified optimal resolution. Extensive
experiments on various vision-language tasks validate the effectiveness of our
method.

</details>


### [73] [Post Processing of image segmentation using Conditional Random Fields](https://arxiv.org/abs/2510.09833)
*Aashish Dhawan,Pankaj Bodani,Vishal Garg*

Main category: cs.CV

TL;DR: 为解决卫星图像分割结果不清晰问题，本研究探索并评估了不同条件随机场（CRF）模型，旨在找出最适合提升分割图像清晰度的CRF，并在低质量卫星图像和高质量航空照片数据集上进行了验证和比较。


<details>
  <summary>Details</summary>
Motivation: 由于卫星图像的低质量特征，图像分割过程的输出通常不够清晰。本研究的目的是寻找一种合适的条件随机场（CRF）模型，以提高分割图像的清晰度。

Method: 研究首先分析了不同类型的CRF模型及其适用性。随后，在包含低质量卫星图像和高质量航空照片的两个数据集上评估了这些CRF模型。通过实验比较各种CRF，以确定在图像上提供最佳结果的模型，并分析了不同方法的优点和局限性。

Result: 研究通过在低质量卫星图像和高质量航空照片数据集上评估和比较各种CRF模型，成功识别出在图像上能提供最佳结果的CRF，并揭示了不同CRF方法的潜在优势和局限性。

Conclusion: 本研究通过系统性地评估不同CRF模型在处理不同质量图像时的表现，成功找到了能够有效提高分割图像清晰度的CRF，并深入理解了各种CRF方法的适用范围和限制。

Abstract: The output of image the segmentation process is usually not very clear due to
low quality features of Satellite images. The purpose of this study is to find
a suitable Conditional Random Field (CRF) to achieve better clarity in a
segmented image. We started with different types of CRFs and studied them as to
why they are or are not suitable for our purpose. We evaluated our approach on
two different datasets - Satellite imagery having low quality features and high
quality Aerial photographs. During the study we experimented with various CRFs
to find which CRF gives the best results on images and compared our results on
these datasets to show the pitfalls and potentials of different approaches.

</details>


### [74] [Exploration of Incremental Synthetic Non-Morphed Images for Single Morphing Attack Detection](https://arxiv.org/abs/2510.09836)
*David Benavente-Rios,Juan Ruiz Rodriguez,Gustavo Gatica*

Main category: cs.CV

TL;DR: 本文研究利用合成人脸数据增强单活体攻击检测（S-MAD），发现适度且有策略地结合合成数据能提升泛化能力，但过度或仅使用合成数据会导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题导致真实人脸数据集规模有限，阻碍了S-MAD模型的训练，因此探索利用合成人脸数据来弥补这一不足并提升检测性能。

Method: 采用多种人脸变形工具和跨数据集评估方案。实施增量测试协议，逐步添加合成图像以评估模型的泛化能力。

Result: 研究表明，通过谨慎地将受控数量的合成图像融入现有数据集或在训练中逐步添加真实图像，可以改善泛化能力。然而，不加区分地使用合成数据会导致次优性能，并且仅使用合成数据（包括变形和非变形图像）会导致最高的等错误率（EER），即性能最差。

Conclusion: 在S-MAD的实际应用场景中，不应单独依赖合成数据，而是应谨慎地将其与真实数据结合使用，以获得最佳性能。

Abstract: This paper investigates the use of synthetic face data to enhance
Single-Morphing Attack Detection (S-MAD), addressing the limitations of
availability of large-scale datasets of bona fide images due to privacy
concerns. Various morphing tools and cross-dataset evaluation schemes were
utilized to conduct this study. An incremental testing protocol was implemented
to assess the generalization capabilities as more and more synthetic images
were added. The results of the experiments show that generalization can be
improved by carefully incorporating a controlled number of synthetic images
into existing datasets or by gradually adding bona fide images during training.
However, indiscriminate use of synthetic data can lead to sub-optimal
performance. Evenmore, the use of only synthetic data (morphed and non-morphed
images) achieves the highest Equal Error Rate (EER), which means in operational
scenarios the best option is not relying only on synthetic data for S-MAD.

</details>


### [75] [Cell Instance Segmentation: The Devil Is in the Boundaries](https://arxiv.org/abs/2510.09848)
*Peixian Liang,Yifan Ding,Yizhe Zhang,Jianxu Chen,Hao Zheng,Hongxiao Wang,Yejia Zhang,Guangyu Meng,Tim Weninger,Michael Niemier,X. Sharon Hu,Danny Z Chen*

Main category: cs.CV

TL;DR: 本文提出Ceb，一种新的像素聚类方法，通过利用细胞边界特征和标签，解决了现有细胞实例分割方法中像素级目标丢失几何特性的问题，并在多项实验中表现出领先或竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 现有细胞实例分割方法依赖于像素级目标，如距离图或热扩散图，但这些方法可能丢失细胞实例重要的几何属性，例如形状、曲率和凸度。

Method: 提出名为Ceb（Cell boundaries）的像素聚类方法。该方法首先从语义分割概率图中提取潜在的前景-前景边界（通过修订版Watershed算法），为每个边界候选构建边界特征表示（边界签名），然后使用边界分类器预测其二元边界标签，最后根据预测的边界标签分割或合并相邻区域以获取细胞实例。

Result: Ceb在六个数据集上的实验表明，它优于现有的像素聚类方法，并且与最先进的细胞实例分割方法相比，取得了极具竞争力的性能。

Conclusion: Ceb通过利用细胞边界特征和标签，成功解决了现有像素级目标方法在细胞实例分割中丢失几何属性的挑战，实现了卓越且具有竞争力的实例分割性能。

Abstract: State-of-the-art (SOTA) methods for cell instance segmentation are based on
deep learning (DL) semantic segmentation approaches, focusing on distinguishing
foreground pixels from background pixels. In order to identify cell instances
from foreground pixels (e.g., pixel clustering), most methods decompose
instance information into pixel-wise objectives, such as distances to
foreground-background boundaries (distance maps), heat gradients with the
center point as heat source (heat diffusion maps), and distances from the
center point to foreground-background boundaries with fixed angles (star-shaped
polygons). However, pixel-wise objectives may lose significant geometric
properties of the cell instances, such as shape, curvature, and convexity,
which require a collection of pixels to represent. To address this challenge,
we present a novel pixel clustering method, called Ceb (for Cell boundaries),
to leverage cell boundary features and labels to divide foreground pixels into
cell instances. Starting with probability maps generated from semantic
segmentation, Ceb first extracts potential foreground-foreground boundaries
with a revised Watershed algorithm. For each boundary candidate, a boundary
feature representation (called boundary signature) is constructed by sampling
pixels from the current foreground-foreground boundary as well as the
neighboring background-foreground boundaries. Next, a boundary classifier is
used to predict its binary boundary label based on the corresponding boundary
signature. Finally, cell instances are obtained by dividing or merging
neighboring regions based on the predicted boundary labels. Extensive
experiments on six datasets demonstrate that Ceb outperforms existing pixel
clustering methods on semantic segmentation probability maps. Moreover, Ceb
achieves highly competitive performance compared to SOTA cell instance
segmentation methods.

</details>


### [76] [Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation](https://arxiv.org/abs/2510.09867)
*Zhi Chen,Xin Yu,Xiaohui Tao,Yan Li,Zi Huang*

Main category: cs.CV

TL;DR: CAPEL是一个新的提示词集成学习框架，通过在分类logits空间进行集成，并引入聚类保持正则化和自适应权重，解决了传统提示词集成方法因特征平均导致的问题，从而提升VLM性能。


<details>
  <summary>Details</summary>
Motivation: 传统的提示词集成方法通过平均上下文提示词的文本特征，往往会使类别中心偏离真实的类别分布，导致次优结果，因此需要一种新的集成方法。

Method: 本文提出聚类感知提示词集成学习（CAPEL）框架。该方法将图像分类到由不同提示词代表的多个类别聚类中；在分类logits空间进行提示词集成，而非特征空间；引入聚类保持正则化项以维护提示词的判别力；并整合自适应提示词加权技术来处理有缺陷或模糊的提示词。

Result: CAPEL框架能更好地与视觉特征分布对齐，保持提示词的独特性和专业性，防止提示词塌缩到统一方向，并通过自适应权重确保在不同数据集和任务上的鲁棒性能。

Conclusion: CAPEL框架通过其独特的集成策略、聚类保持正则化和自适应加权机制，有效解决了传统提示词集成方法的缺陷，有望显著提升VLM的性能和鲁棒性。

Abstract: Vision-language models (VLMs) such as CLIP achieve zero-shot transfer across
various tasks by pre-training on numerous image-text pairs. These models often
benefit from using an ensemble of context prompts to represent a class. Despite
being effective, conventional prompt ensembling that averages textual features
of context prompts often yields suboptimal results. This is because feature
averaging shifts the class centroids away from the true class distribution. To
address this issue, we propose the Cluster-Aware Prompt Ensemble Learning
(CAPEL) framework, which preserves the cluster nature of context prompts. CAPEL
classifies images into one of several class clusters, each represented by a
distinct prompt. Instead of ensembling prompts in the feature space, we perform
ensembling in the classification logits space, aligning better with the visual
feature distribution. To further optimize prompt fine-tuning while maintaining
cluster-specific discriminative power, we introduce a cluster-preserving
regularization term. This ensures that prompts remain distinct and specialized
for different clusters, preventing collapse into a uniform direction.
Additionally, we integrate an adaptive prompt weighting technique to
dynamically adjust the attention weights for flawed or ambiguous prompts,
ensuring robust performance across diverse datasets and tasks.

</details>


### [77] [Fast Self-Supervised depth and mask aware Association for Multi-Object Tracking](https://arxiv.org/abs/2510.09878)
*Milad Khanchi,Maria Amer,Charalambos Poullis*

Main category: cs.CV

TL;DR: 提出一种新的多目标跟踪(MOT)方法，通过自监督编码器融合深度和分割掩码特征，生成稳定的对象表示作为匹配线索，避免了计算昂贵的掩码IoU，并在复杂基准测试中超越了现有TBD方法。


<details>
  <summary>Details</summary>
Motivation: 传统MOT方法依赖IoU进行关联，但在对象相似或被遮挡时不可靠；同时，计算分割掩码的IoU成本高昂。

Method: 利用分割掩码捕捉对象形状，但不计算分割IoU。通过一个自监督训练的紧凑编码器融合深度和掩码特征，生成稳定的对象表示。这些表示作为额外的相似性线索，与边界框IoU和ReID特征一起用于匹配。深度图和对象掩码分别来自零样本深度估计器和可提示视觉分割模型。该方法是首个不计算掩码IoU，而使用自监督编码器细化分割掩码的MOT方法，并采用计算效率更高的跟踪-检测（TBD）模式。

Result: 在SportsMOT和DanceTrack等包含非线性运动、遮挡和拥挤场景的挑战性基准测试中，该TBD方法在大多数指标上超越了现有TBD领域最佳（SOTA）方法；在MOT17等具有线性运动的简单基准测试中也表现出竞争力。

Conclusion: 通过引入自监督编码的深度与分割掩码特征，本方法有效解决了传统MOT中IoU关联的局限性和掩码IoU的计算开销，并在复杂多目标跟踪场景中取得了超越现有技术的性能。

Abstract: Multi-object tracking (MOT) methods often rely on Intersection-over-Union
(IoU) for association. However, this becomes unreliable when objects are
similar or occluded. Also, computing IoU for segmentation masks is
computationally expensive. In this work, we use segmentation masks to capture
object shapes, but we do not compute segmentation IoU. Instead, we fuse depth
and mask features and pass them through a compact encoder trained
self-supervised. This encoder produces stable object representations, which we
use as an additional similarity cue alongside bounding box IoU and
re-identification features for matching. We obtain depth maps from a zero-shot
depth estimator and object masks from a promptable visual segmentation model to
obtain fine-grained spatial cues. Our MOT method is the first to use the
self-supervised encoder to refine segmentation masks without computing masks
IoU. MOT can be divided into joint detection-ReID (JDR) and
tracking-by-detection (TBD) models. The latter are computationally more
efficient. Experiments of our TBD method on challenging benchmarks with
non-linear motion, occlusion, and crowded scenes, such as SportsMOT and
DanceTrack, show that our method outperforms the TBD state-of-the-art on most
metrics, while achieving competitive performance on simpler benchmarks with
linear motion, such as MOT17.

</details>


### [78] [CHUG: Crowdsourced User-Generated HDR Video Quality Dataset](https://arxiv.org/abs/2510.09879)
*Shreshth Saini,Alan C. Bovik,Neil Birkbeck,Yilin Wang,Balu Adsumilli*

Main category: cs.CV

TL;DR: 本文提出了CHUG，首个大规模众包用户生成高动态范围（UGC-HDR）视频质量数据集，以填补现有专业生成内容（PGC）数据集在UGC领域研究的空白，推动无参考HDR-VQA研究。


<details>
  <summary>Details</summary>
Motivation: 现有HDR视频质量评估（VQA）数据集主要关注专业生成内容（PGC），无法解决用户生成内容（UGC）中独特的捕获条件、编辑伪影和压缩失真等挑战，导致对真实世界UGC-HDR退化缺乏理解。

Method: 研究者创建了CHUG数据集，包含856个UGC-HDR源视频，通过多分辨率和比特率转码模拟真实场景，共计5,992个视频。通过Amazon Mechanical Turk进行了大规模主观研究，收集了211,848个感知评分。

Result: CHUG数据集的建立，作为首个大规模UGC-HDR质量主观研究，为分析HDR视频中UGC特有的失真提供了基准。该数据集包含5,992个视频和211,848个感知评分。

Conclusion: CHUG数据集为理解和分析UGC-HDR视频中的特有失真提供了重要资源，预计将通过提供一个大规模、多样化且真实世界的UGC数据集，显著推动无参考（NR）HDR-VQA研究的进展。

Abstract: High Dynamic Range (HDR) videos enhance visual experiences with superior
brightness, contrast, and color depth. The surge of User-Generated Content
(UGC) on platforms like YouTube and TikTok introduces unique challenges for HDR
video quality assessment (VQA) due to diverse capture conditions, editing
artifacts, and compression distortions. Existing HDR-VQA datasets primarily
focus on professionally generated content (PGC), leaving a gap in understanding
real-world UGC-HDR degradations. To address this, we introduce CHUG:
Crowdsourced User-Generated HDR Video Quality Dataset, the first large-scale
subjective study on UGC-HDR quality. CHUG comprises 856 UGC-HDR source videos,
transcoded across multiple resolutions and bitrates to simulate real-world
scenarios, totaling 5,992 videos. A large-scale study via Amazon Mechanical
Turk collected 211,848 perceptual ratings. CHUG provides a benchmark for
analyzing UGC-specific distortions in HDR videos. We anticipate CHUG will
advance No-Reference (NR) HDR-VQA research by offering a large-scale, diverse,
and real-world UGC dataset. The dataset is publicly available at:
https://shreshthsaini.github.io/CHUG/.

</details>


### [79] [Geometry-Aware Scene Configurations for Novel View Synthesis](https://arxiv.org/abs/2510.09880)
*Minkwan Kim,Changwoon Choi,Young Min Kim*

Main category: cs.CV

TL;DR: 提出场景自适应策略，利用几何先验和虚拟视角优化NeRF容量分配，以高效渲染复杂室内场景，显著提升渲染质量和内存效率。


<details>
  <summary>Details</summary>
Motivation: 多房间室内场景布局不规则、复杂度高，导致现有方法难以从不完整观测中有效分配NeRF表示能力，生成沉浸式体验。之前可扩展NeRF的统一基底布局效率低下。

Method: 提出场景自适应策略以高效分配表示能力；利用几何先验指导资源利用；记录几何支架上的观测统计数据并引导基底的最佳放置；建议场景自适应的虚拟视角以弥补几何缺陷和进行正则化。

Result: 相比于现有可扩展NeRF的统一基底布局，本文方法在渲染质量和内存需求方面均有显著提升，特别是在大型室内场景中。

Conclusion: 场景自适应的容量分配方法，结合几何先验指导和虚拟视角，能有效提高复杂室内环境的NeRF渲染效率和质量，优于传统的统一分配策略。

Abstract: We propose scene-adaptive strategies to efficiently allocate representation
capacity for generating immersive experiences of indoor environments from
incomplete observations. Indoor scenes with multiple rooms often exhibit
irregular layouts with varying complexity, containing clutter, occlusion, and
flat walls. We maximize the utilization of limited resources with guidance from
geometric priors, which are often readily available after pre-processing
stages. We record observation statistics on the estimated geometric scaffold
and guide the optimal placement of bases, which greatly improves upon the
uniform basis arrangements adopted by previous scalable Neural Radiance Field
(NeRF) representations. We also suggest scene-adaptive virtual viewpoints to
compensate for geometric deficiencies inherent in view configurations in the
input trajectory and impose the necessary regularization. We present a
comprehensive analysis and discussion regarding rendering quality and memory
requirements in several large-scale indoor scenes, demonstrating significant
enhancements compared to baselines that employ regular placements.

</details>


### [80] [LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates](https://arxiv.org/abs/2510.09881)
*Minkwan Kim,Seungmin Lee,Junho Kim,Young Min Kim*

Main category: cs.CV

TL;DR: 该论文提出LTGS，一种高效的高斯场景表示，能从稀疏随意捕捉中建模日常环境的长期变化，实现高质量重建和轻量级更新。


<details>
  <summary>Details</summary>
Motivation: 传统新视角合成方法难以应对日常环境中频繁的场景变化，尤其是在稀疏、非受限的随意捕捉条件下，因为这通常需要密集时空观测。

Method: 提出LTGS框架，它从初始高斯泼溅表示开始，通过将对象构建为模板高斯作为结构化先验，并利用一个精炼管道，基于少量观测来适应时间变化的环境。该框架经训练后可通过简单变换在多个时间步长上泛化。

Result: 实验证明，LTGS在重建质量上优于现有基线，并能实现快速、轻量级的场景更新。为评估实用性，作者还收集了新的真实世界数据集。

Conclusion: LTGS成功解决了从稀疏观测中建模真实世界长期变化的挑战，提供了一种可扩展、高效且高质量的三维环境时序演进解决方案。

Abstract: Recent advances in novel-view synthesis can create the photo-realistic
visualization of real-world environments from conventional camera captures.
However, acquiring everyday environments from casual captures faces challenges
due to frequent scene changes, which require dense observations both spatially
and temporally. We propose long-term Gaussian scene chronology from sparse-view
updates, coined LTGS, an efficient scene representation that can embrace
everyday changes from highly under-constrained casual captures. Given an
incomplete and unstructured Gaussian splatting representation obtained from an
initial set of input images, we robustly model the long-term chronology of the
scene despite abrupt movements and subtle environmental variations. We
construct objects as template Gaussians, which serve as structural, reusable
priors for shared object tracks. Then, the object templates undergo a further
refinement pipeline that modulates the priors to adapt to temporally varying
environments based on few-shot observations. Once trained, our framework is
generalizable across multiple time steps through simple transformations,
significantly enhancing the scalability for a temporal evolution of 3D
environments. As existing datasets do not explicitly represent the long-term
real-world changes with a sparse capture setup, we collect real-world datasets
to evaluate the practicality of our pipeline. Experiments demonstrate that our
framework achieves superior reconstruction quality compared to other baselines
while enabling fast and light-weight updates.

</details>


### [81] [An uncertainty-aware framework for data-efficient multi-view animal pose estimation](https://arxiv.org/abs/2510.09903)
*Lenny Aharon,Keemin Lee,Karan Sikka,Selmaan Chettih,Cole Hurwitz,Liam Paninski,Matthew R Whiteway*

Main category: cs.CV

TL;DR: 提出一个用于多视角姿态估计的综合框架，通过新颖的训练、后处理和模型蒸馏技术，在有限标注数据下实现高精度、不确定性感知的动物姿态估计，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多视角姿态估计算法在有限标注数据下难以实现准确追踪，且不确定性估计不佳，阻碍了动物行为的量化研究。

Method: 本研究提出一个综合框架：1) **Multi-view Transformer (MVT)**：利用预训练骨干网络同时处理多视角信息，并通过新颖的补丁掩码方案在无需相机标定下学习鲁棒的跨视角对应关系。2) **几何一致性**：对于已标定设置，通过3D增强和三角测量损失引入几何一致性。3) **后处理**：将Ensemble Kalman Smoother (EKS) 扩展到非线性情况，并结合方差膨胀技术增强不确定性量化。4) **模型蒸馏**：利用改进的EKS预测和不确定性估计生成高质量伪标签，减少对手动标注的依赖。

Result: 该框架的各个组件在三种不同动物物种（苍蝇、小鼠、山雀）上均持续优于现有方法，且每个组件都提供了互补的优势。

Conclusion: 该研究提供了一个实用、不确定性感知且可靠的姿态估计系统，能够在真实世界数据约束下支持下游行为分析。

Abstract: Multi-view pose estimation is essential for quantifying animal behavior in
scientific research, yet current methods struggle to achieve accurate tracking
with limited labeled data and suffer from poor uncertainty estimates. We
address these challenges with a comprehensive framework combining novel
training and post-processing techniques, and a model distillation procedure
that leverages the strengths of these techniques to produce a more efficient
and effective pose estimator. Our multi-view transformer (MVT) utilizes
pretrained backbones and enables simultaneous processing of information across
all views, while a novel patch masking scheme learns robust cross-view
correspondences without camera calibration. For calibrated setups, we
incorporate geometric consistency through 3D augmentation and a triangulation
loss. We extend the existing Ensemble Kalman Smoother (EKS) post-processor to
the nonlinear case and enhance uncertainty quantification via a variance
inflation technique. Finally, to leverage the scaling properties of the MVT, we
design a distillation procedure that exploits improved EKS predictions and
uncertainty estimates to generate high-quality pseudo-labels, thereby reducing
dependence on manual labels. Our framework components consistently outperform
existing methods across three diverse animal species (flies, mice, chickadees),
with each component contributing complementary benefits. The result is a
practical, uncertainty-aware system for reliable pose estimation that enables
downstream behavioral analyses under real-world data constraints.

</details>


### [82] [SpectralCA: Bi-Directional Cross-Attention for Next-Generation UAV Hyperspectral Vision](https://arxiv.org/abs/2510.09912)
*D. V. Brovko*

Main category: cs.CV

TL;DR: 本研究开发了一种基于高光谱图像（HSI）的深度学习架构，通过引入SpectralCA模块改进MDvT，有效提升了无人机在复杂环境中的实时感知效率，应用于导航、目标检测和地形分类。


<details>
  <summary>Details</summary>
Motivation: 传统无人机导航在复杂环境（如干扰、能见度差、伪装）中不可靠。高光谱成像（HSI）提供精细材料识别和目标区分能力，对无人机导航、监控等至关重要，因此需要开发集成HSI的无人机感知系统。

Method: 本研究旨在开发一种结合HSI的深度学习架构，用于无人机感知。方法是修改Mobile 3D Vision Transformer (MDvT)，引入了SpectralCA模块。该模块采用双向交叉注意力融合光谱和空间特征，以提高准确性并减少参数和推理时间。实验在WHU-Hi-HongHu数据集上进行，评估指标包括总体准确度、平均准确度和Kappa系数。

Result: 研究结果证实，所提出的架构显著提高了无人机的感知效率。

Conclusion: 该架构能够实现无人机在导航、目标识别和环境监测任务中的实时操作，从而提升了无人机在复杂环境中的应用能力。

Abstract: The relevance of this research lies in the growing demand for unmanned aerial
vehicles (UAVs) capable of operating reliably in complex environments where
conventional navigation becomes unreliable due to interference, poor
visibility, or camouflage. Hyperspectral imaging (HSI) provides unique
opportunities for UAV-based computer vision by enabling fine-grained material
recognition and object differentiation, which are critical for navigation,
surveillance, agriculture, and environmental monitoring. The aim of this work
is to develop a deep learning architecture integrating HSI into UAV perception
for navigation, object detection, and terrain classification. Objectives
include: reviewing existing HSI methods, designing a hybrid 2D/3D convolutional
architecture with spectral-spatial cross-attention, training, and benchmarking.
The methodology is based on the modification of the Mobile 3D Vision
Transformer (MDvT) by introducing the proposed SpectralCA block. This block
employs bi-directional cross-attention to fuse spectral and spatial features,
enhancing accuracy while reducing parameters and inference time. Experimental
evaluation was conducted on the WHU-Hi-HongHu dataset, with results assessed
using Overall Accuracy, Average Accuracy, and the Kappa coefficient. The
findings confirm that the proposed architecture improves UAV perception
efficiency, enabling real-time operation for navigation, object recognition,
and environmental monitoring tasks.
  Keywords: SpectralCA, deep learning, computer vision, hyperspectral imaging,
unmanned aerial vehicle, object detection, semi-supervised learning.

</details>


### [83] [HeadsUp! High-Fidelity Portrait Image Super-Resolution](https://arxiv.org/abs/2510.09924)
*Renjie Li,Zihao Zhu,Xiaoyu Wang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本文提出HeadsUp，一个端到端的单步扩散模型，用于肖像图像超分辨率，解决了现有方法在面部区域产生的伪影问题。


<details>
  <summary>Details</summary>
Motivation: 肖像图片是社交媒体上常见的一种图像形式，但现有图像超分辨率技术要么专注于通用图像，要么专注于严格对齐的面部图像。将通用模型和面部模型混合处理肖像图片会导致面部区域出现混合或边界伪影，而人眼对此类面部细节特别敏感。

Method: 研究肖像图像超分辨率(PortraitISR)问题，并提出HeadsUp模型。该模型基于单步扩散模型构建，包含面部监督机制以引导模型关注面部区域，并集成参考机制以帮助恢复身份并减少低质量面部恢复中的模糊性。此外，还构建了一个高质量的4K肖像图像ISR数据集PortraitSR-4K用于模型训练和基准测试。

Result: HeadsUp在PortraitISR任务上实现了最先进的性能，同时在通用图像和对齐面部数据集上保持了相当或更高的性能。

Conclusion: HeadsUp成功克服了肖像图像超分辨率中的挑战，通过单一模型实现了高质量、无缝的图像恢复和放大，解决了面部区域伪影问题，并在多项任务上表现出色。

Abstract: Portrait pictures, which typically feature both human subjects and natural
backgrounds, are one of the most prevalent forms of photography on social
media. Existing image super-resolution (ISR) techniques generally focus either
on generic real-world images or strictly aligned facial images (i.e., face
super-resolution). In practice, separate models are blended to handle portrait
photos: the face specialist model handles the face region, and the general
model processes the rest. However, these blending approaches inevitably
introduce blending or boundary artifacts around the facial regions due to
different model training recipes, while human perception is particularly
sensitive to facial fidelity. To overcome these limitations, we study the
portrait image supersolution (PortraitISR) problem, and propose HeadsUp, a
single-step diffusion model that is capable of seamlessly restoring and
upscaling portrait images in an end-to-end manner. Specifically, we build our
model on top of a single-step diffusion model and develop a face supervision
mechanism to guide the model in focusing on the facial region. We then
integrate a reference-based mechanism to help with identity restoration,
reducing face ambiguity in low-quality face restoration. Additionally, we have
built a high-quality 4K portrait image ISR dataset dubbed PortraitSR-4K, to
support model training and benchmarking for portrait images. Extensive
experiments show that HeadsUp achieves state-of-the-art performance on the
PortraitISR task while maintaining comparable or higher performance on both
general image and aligned face datasets.

</details>


### [84] [Denoising Diffusion as a New Framework for Underwater Images](https://arxiv.org/abs/2510.09934)
*Nilesh Jain,Elie Alhajjar*

Main category: cs.CV

TL;DR: 针对水下图像质量差及现有数据集多样性与质量不足的问题，本文提出结合去噪扩散模型扩展数据集并利用Controlnet提升图像质量，以期改善海洋生态系统研究。


<details>
  <summary>Details</summary>
Motivation: 水下图像对海洋研究和环境监测至关重要，但复杂环境导致图像质量低下（低能见度、模糊、色彩失真、噪声）。现有图像增强方法存在泛化性差、依赖干净数据集的局限性，且现有数据集缺乏多样性（多为单目图像）和高质量图像。

Method: 本文提出一个新行动方案：一方面，利用去噪扩散模型扩展数据集，纳入立体、广角、微距和特写等多样化图像类型；另一方面，推荐使用Controlnet增强图像，以评估和提升数据集质量。

Result: 该方案旨在克服水下图像处理的现有局限性，通过扩展数据集多样性并提升图像质量，从而改善海洋生态系统的研究。

Conclusion: 通过结合去噪扩散模型扩充多样化水下图像数据集，并利用Controlnet提升图像质量，可以有效解决当前水下图像和数据集的挑战，进而深化对海洋生态系统的研究。

Abstract: Underwater images play a crucial role in ocean research and marine
environmental monitoring since they provide quality information about the
ecosystem. However, the complex and remote nature of the environment results in
poor image quality with issues such as low visibility, blurry textures, color
distortion, and noise. In recent years, research in image enhancement has
proven to be effective but also presents its own limitations, like poor
generalization and heavy reliance on clean datasets. One of the challenges
herein is the lack of diversity and the low quality of images included in these
datasets. Also, most existing datasets consist only of monocular images, a fact
that limits the representation of different lighting conditions and angles. In
this paper, we propose a new plan of action to overcome these limitations. On
one hand, we call for expanding the datasets using a denoising diffusion model
to include a variety of image types such as stereo, wide-angled, macro, and
close-up images. On the other hand, we recommend enhancing the images using
Controlnet to evaluate and increase the quality of the corresponding datasets,
and hence improve the study of the marine ecosystem.
  Tags - Underwater Images, Denoising Diffusion, Marine ecosystem, Controlnet

</details>


### [85] [Semi-disentangled spatiotemporal implicit neural representations of longitudinal neuroimaging data for trajectory classification](https://arxiv.org/abs/2510.09936)
*Agampreet Aulakh,Nils D. Forkert,Matthias Wilms*

Main category: cs.CV

TL;DR: 该研究提出了一种基于隐式神经表征（INRs）的新方法，将纵向T1加权MRI数据建模为连续函数，以更有效地分析和分类大脑衰老轨迹，并在不规则采样数据上显著优于传统深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 人类大脑结构变化动态且可能与病理相关。纵向神经影像数据对描绘典型和非典型衰老轨迹很有价值，但其离散、不规则的空间和时间采样模式给传统深度学习方法带来了计算挑战，无法有效表示潜在的连续生物过程。

Method: 研究引入了一种新的、完全数据驱动的方法，利用隐式神经表征（INRs）将个体纵向T1加权MRI数据建模为连续函数。该方法设计了一种新颖的INR架构，能够部分解耦空间和时间轨迹参数，并开发了一个高效的框架，直接在INRs的参数空间上操作以分类大脑衰老轨迹。为评估方法，还开发了一个生物学上合理的轨迹模拟器，生成了健康和痴呆样受试者在规则和不规则时间点采样的MRI数据。

Result: 在更现实的不规则采样实验中，基于INR的方法在脑衰老轨迹分类任务上取得了81.3%的准确率，优于标准的深度学习基线模型（73.7%）。

Conclusion: 基于隐式神经表征（INRs）的方法能有效克服纵向MRI数据分析中的挑战，通过将数据建模为连续函数，实现了对大脑衰老轨迹的准确分类，并优于传统深度学习方法，尤其在处理不规则采样数据时表现出色。

Abstract: The human brain undergoes dynamic, potentially pathology-driven, structural
changes throughout a lifespan. Longitudinal Magnetic Resonance Imaging (MRI)
and other neuroimaging data are valuable for characterizing trajectories of
change associated with typical and atypical aging. However, the analysis of
such data is highly challenging given their discrete nature with different
spatial and temporal image sampling patterns within individuals and across
populations. This leads to computational problems for most traditional deep
learning methods that cannot represent the underlying continuous biological
process. To address these limitations, we present a new, fully data-driven
method for representing aging trajectories across the entire brain by modelling
subject-specific longitudinal T1-weighted MRI data as continuous functions
using Implicit Neural Representations (INRs). Therefore, we introduce a novel
INR architecture capable of partially disentangling spatial and temporal
trajectory parameters and design an efficient framework that directly operates
on the INRs' parameter space to classify brain aging trajectories. To evaluate
our method in a controlled data environment, we develop a biologically grounded
trajectory simulation and generate T1-weighted 3D MRI data for 450 healthy and
dementia-like subjects at regularly and irregularly sampled timepoints. In the
more realistic irregular sampling experiment, our INR-based method achieves
81.3% accuracy for the brain aging trajectory classification task,
outperforming a standard deep learning baseline model (73.7%).

</details>


### [86] [Explainable Human-in-the-Loop Segmentation via Critic Feedback Signals](https://arxiv.org/abs/2510.09945)
*Pouya Shaeri,Ryan T. Woo,Yasaman Mohammadpour,Ariane Middel*

Main category: cs.CV

TL;DR: 提出一种人机交互式干预学习框架，通过靶向人类修正，使分割模型从虚假关联转向语义特征，提高鲁棒性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 分割模型在基准测试中表现出色，但在真实世界应用中常因依赖虚假关联而非真实对象边界而失败。

Method: 开发一个人机交互框架，将人类对分割输出的修正视为干预信号。系统通过在视觉相似图像间传播修正信息来学习，从而引导模型关注鲁棒的、有语义意义的特征，而非数据集特有的伪影，并系统性地纠正模型失败模式。

Result: 在挑战性立方体贴图数据上，分割精度提升高达9 mIoU点（相对提升12-15%）；相比标准再训练，标注工作量减少3-4倍，同时在基准数据集上保持竞争性表现。

Conclusion: 该工作提供了一个实用的框架，用于构建准确、鲁棒、数据高效且适应真实世界（如城市气候监测和自动驾驶）的分割系统。

Abstract: Segmentation models achieve high accuracy on benchmarks but often fail in
real-world domains by relying on spurious correlations instead of true object
boundaries. We propose a human-in-the-loop interactive framework that enables
interventional learning through targeted human corrections of segmentation
outputs. Our approach treats human corrections as interventional signals that
show when reliance on superficial features (e.g., color or texture) is
inappropriate. The system learns from these interventions by propagating
correction-informed edits across visually similar images, effectively steering
the model toward robust, semantically meaningful features rather than
dataset-specific artifacts. Unlike traditional annotation approaches that
simply provide more training data, our method explicitly identifies when and
why the model fails and then systematically corrects these failure modes across
the entire dataset. Through iterative human feedback, the system develops
increasingly robust representations that generalize better to novel domains and
resist artifactual correlations. We demonstrate that our framework improves
segmentation accuracy by up to 9 mIoU points (12-15\% relative improvement) on
challenging cubemap data and yields 3-4$\times$ reductions in annotation effort
compared to standard retraining, while maintaining competitive performance on
benchmark datasets. This work provides a practical framework for researchers
and practitioners seeking to build segmentation systems that are accurate,
robust to dataset biases, data-efficient, and adaptable to real-world domains
such as urban climate monitoring and autonomous driving.

</details>


### [87] [A Multi-Strategy Framework for Enhancing Shatian Pomelo Detection in Real-World Orchards](https://arxiv.org/abs/2510.09948)
*Pan Wang,Yihao Hu,Xiaodong Bai,Aiping Yang,Xiangxiang Li,Meiping Ding,Jianguo Yao*

Main category: cs.CV

TL;DR: 为解决沙田柚在真实世界自动化检测中面临的设备、光照、尺度和遮挡挑战，本研究提出了一种多策略框架和REAS-Det网络，通过多源数据集、数据增强和专门设计的网络模块，实现了高精度检测，并优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 沙田柚作为重要农产品需自动化检测以满足商业需求和精益生产。现有专业网络在真实场景中性能不佳。主要研究动机是解决影响沙田柚检测准确性的四大关键挑战：成像设备差异、光照条件多变、物体尺度变化和遮挡问题。

Method: 本研究提出多策略框架。首先，构建STP-AgriData多场景数据集（融合真实果园和网络数据）以应对成像设备和环境差异。其次，应用对比度与亮度调整等数据增强方法模拟不一致的光照条件。第三，设计REAS-Det网络以解决物体尺度变化（通过RFAConv和C3RFEM模块扩展感受野）和遮挡问题（通过MultiSEAM多尺度多头特征选择结构和soft-NMS）。

Result: 实验结果显示：准确率(P) 87.6%，召回率(R) 74.9%，mAP@.50 82.8%，mAP@.50:.95 53.3%。本文提出的网络在检测性能上优于其他最先进的检测方法。

Conclusion: 本研究提出的多策略框架和REAS-Det网络能有效缓解沙田柚检测中的挑战，实现了优异的检测精度和鲁棒性，其性能超越了现有最先进的检测方法。

Abstract: As a specialty agricultural product with a large market scale, Shatian pomelo
necessitates the adoption of automated detection to ensure accurate quantity
and meet commercial demands for lean production. Existing research often
involves specialized networks tailored for specific theoretical or dataset
scenarios, but these methods tend to degrade performance in real-world. Through
analysis of factors in this issue, this study identifies four key challenges
that affect the accuracy of Shatian pomelo detection: imaging devices, lighting
conditions, object scale variation, and occlusion. To mitigate these
challenges, a multi-strategy framework is proposed in this paper. Firstly, to
effectively solve tone variation introduced by diverse imaging devices and
complex orchard environments, we utilize a multi-scenario dataset,
STP-AgriData, which is constructed by integrating real orchard images with
internet-sourced data. Secondly, to simulate the inconsistent illumination
conditions, specific data augmentations such as adjusting contrast and changing
brightness, are applied to the above dataset. Thirdly, to address the issues of
object scale variation and occlusion in fruit detection, an REAS-Det network is
designed in this paper. For scale variation, RFAConv and C3RFEM modules are
designed to expand and enhance the receptive fields. For occlusion variation, a
multi-scale, multi-head feature selection structure (MultiSEAM) and soft-NMS
are introduced to enhance the handling of occlusion issues to improve detection
accuracy. The results of these experiments achieved a precision(P) of 87.6%, a
recall (R) of 74.9%, a mAP@.50 of 82.8%, and a mAP@.50:.95 of 53.3%. Our
proposed network demonstrates superior performance compared to other
state-of-the-art detection methods.

</details>


### [88] [J-RAS: Enhancing Medical Image Segmentation via Retrieval-Augmented Joint Training](https://arxiv.org/abs/2510.09953)
*Salma J. Ahmed,Emad A. Mohammed,Azam Asilian Bidgoli*

Main category: cs.CV

TL;DR: 提出J-RAS，一种联合训练方法，结合分割与检索模型，以改善医疗图像分割在数据稀缺和泛化性问题上的表现，并在多个骨干网络和数据集上实现显著提升。


<details>
  <summary>Details</summary>
Motivation: 手动医疗图像分割耗时、昂贵且易受人为因素影响。现有AI方法需大量标注数据且泛化性差，难以应对数据稀缺、多变成像条件和罕见病例。

Method: 提出J-RAS，一种联合训练框架，将分割模型与检索模型集成并共同优化。分割模型利用检索到的图像-掩码对增强解剖学理解，检索模型学习分割相关特征。

Result: J-RAS在U-Net、TransUNet、SAM、SegFormer等多个骨干网络和ACDC、M&Ms数据集上均取得一致性能提升。例如，在ACDC数据集上，SegFormer结合J-RAS后，Dice系数从0.8708提升至0.9115，HD从1.8130降至1.1489。

Conclusion: J-RAS有效提升医疗图像分割性能，展现出良好的有效性和在不同架构及数据集上的泛化能力。

Abstract: Image segmentation, the process of dividing images into meaningful regions,
is critical in medical applications for accurate diagnosis, treatment planning,
and disease monitoring. Although manual segmentation by healthcare
professionals produces precise outcomes, it is time-consuming, costly, and
prone to variability due to differences in human expertise. Artificial
intelligence (AI)-based methods have been developed to address these
limitations by automating segmentation tasks; however, they often require
large, annotated datasets that are rarely available in practice and frequently
struggle to generalize across diverse imaging conditions due to inter-patient
variability and rare pathological cases. In this paper, we propose Joint
Retrieval Augmented Segmentation (J-RAS), a joint training method for guided
image segmentation that integrates a segmentation model with a retrieval model.
Both models are jointly optimized, enabling the segmentation model to leverage
retrieved image-mask pairs to enrich its anatomical understanding, while the
retrieval model learns segmentation-relevant features beyond simple visual
similarity. This joint optimization ensures that retrieval actively contributes
meaningful contextual cues to guide boundary delineation, thereby enhancing the
overall segmentation performance. We validate J-RAS across multiple
segmentation backbones, including U-Net, TransUNet, SAM, and SegFormer, on two
benchmark datasets: ACDC and M&Ms, demonstrating consistent improvements. For
example, on the ACDC dataset, SegFormer without J-RAS achieves a mean Dice
score of 0.8708$\pm$0.042 and a mean Hausdorff Distance (HD) of
1.8130$\pm$2.49, whereas with J-RAS, the performance improves substantially to
a mean Dice score of 0.9115$\pm$0.031 and a mean HD of 1.1489$\pm$0.30. These
results highlight the method's effectiveness and its generalizability across
architectures and datasets.

</details>


### [89] [Scaling Traffic Insights with AI and Language Model-Powered Camera Systems for Data-Driven Transportation Decision Making](https://arxiv.org/abs/2510.09981)
*Fan Zuo,Donglin Zhou,Jingqin Gao,Kaan Ozbay*

Main category: cs.CV

TL;DR: 本研究提出一个端到端的AI框架，利用现有交通摄像头进行高分辨率、大规模的交通监控和分析，并通过YOLOv11模型、图基视图归一化和领域特定大语言模型，成功应用于纽约市拥堵收费政策的监测。


<details>
  <summary>Details</summary>
Motivation: 准确、可扩展的交通监测对交通管理至关重要，尤其是在中断事件期间。然而，传感器部署成本高昂，现有视频分析系统难以处理动态摄像头视角和海量数据。

Method: 开发了一个端到端的AI框架，利用现有交通摄像头基础设施。方法包括：1) 使用针对城市场景微调的YOLOv11模型实时提取多模态交通密度和分类指标；2) 引入新颖的图基视图归一化方法来处理非固定式平移-倾斜-缩放摄像头的视角不一致性；3) 集成领域特定大语言模型，处理24/7视频流的海量数据，自动生成交通模式摘要。

Result: 系统在纽约市拥堵收费政策早期实施期间，利用约1000个摄像头、超过900万张图像进行验证。结果显示：拥堵缓解区内工作日乘用车密度下降9%；卡车流量初期减少并有反弹迹象；走廊和区域尺度的行人和骑行者活动持续增加。实验还表明，基于示例的提示提升了LLM的数值准确性并减少了幻觉。

Conclusion: 该框架是一个实用、基础设施就绪的解决方案，可实现大规模、政策相关的交通监测，且人工干预最少。

Abstract: Accurate, scalable traffic monitoring is critical for real-time and long-term
transportation management, particularly during disruptions such as natural
disasters, large construction projects, or major policy changes like New York
City's first-in-the-nation congestion pricing program. However, widespread
sensor deployment remains limited due to high installation, maintenance, and
data management costs. While traffic cameras offer a cost-effective
alternative, existing video analytics struggle with dynamic camera viewpoints
and massive data volumes from large camera networks. This study presents an
end-to-end AI-based framework leveraging existing traffic camera infrastructure
for high-resolution, longitudinal analysis at scale. A fine-tuned YOLOv11
model, trained on localized urban scenes, extracts multimodal traffic density
and classification metrics in real time. To address inconsistencies from
non-stationary pan-tilt-zoom cameras, we introduce a novel graph-based
viewpoint normalization method. A domain-specific large language model was also
integrated to process massive data from a 24/7 video stream to generate
frequent, automated summaries of evolving traffic patterns, a task far
exceeding manual capabilities. We validated the system using over 9 million
images from roughly 1,000 traffic cameras during the early rollout of NYC
congestion pricing in 2025. Results show a 9% decline in weekday passenger
vehicle density within the Congestion Relief Zone, early truck volume
reductions with signs of rebound, and consistent increases in pedestrian and
cyclist activity at corridor and zonal scales. Experiments showed that
example-based prompts improved LLM's numerical accuracy and reduced
hallucinations. These findings demonstrate the framework's potential as a
practical, infrastructure-ready solution for large-scale, policy-relevant
traffic monitoring with minimal human intervention.

</details>


### [90] [FlareX: A Physics-Informed Dataset for Lens Flare Removal via 2D Synthesis and 3D Rendering](https://arxiv.org/abs/2510.09995)
*Lishen Qu,Zhihao Liu,Jinshan Pan,Shihao Zhou,Jinglei Shi,Duosheng Chen,Jufeng Yang*

Main category: cs.CV

TL;DR: 提出一种新的物理信息引导的炫光数据生成方法，通过2D合成和3D渲染创建了混合炫光数据集FlareX，以解决现有合成数据在真实世界泛化性差的问题。


<details>
  <summary>Details</summary>
Motivation: 镜头炫光会严重降低图像质量。真实世界中难以捕获炫光-无炫光图像对，现有数据集多为2D合成，但其模板缺乏多样性且忽视物理原理，导致基于这些数据集训练的模型难以泛化到真实场景。

Method: 提出一种新的物理信息引导的炫光数据生成方法，包括三个阶段：参数化模板创建、光照规律感知的2D合成以及基于物理引擎的3D渲染。最终生成了结合2D和3D视角的混合炫光数据集FlareX。此外，设计了一种掩码方法，用于从真实世界的受损图像中获取无炫光图像以进行模型性能评估。

Result: 创建了FlareX数据集，该数据集包含从95种炫光模式衍生的9,500个2D模板，以及从60个3D场景渲染的3,000对炫光图像。广泛的实验证明了所提方法和数据集的有效性。

Conclusion: 所提出的物理信息引导的数据生成方法和FlareX数据集有效解决了现有方法在训练模型用于真实世界炫光去除时泛化性差的挑战。

Abstract: Lens flare occurs when shooting towards strong light sources, significantly
degrading the visual quality of images. Due to the difficulty in capturing
flare-corrupted and flare-free image pairs in the real world, existing datasets
are typically synthesized in 2D by overlaying artificial flare templates onto
background images. However, the lack of flare diversity in templates and the
neglect of physical principles in the synthesis process hinder models trained
on these datasets from generalizing well to real-world scenarios. To address
these challenges, we propose a new physics-informed method for flare data
generation, which consists of three stages: parameterized template creation,
the laws of illumination-aware 2D synthesis, and physical engine-based 3D
rendering, which finally gives us a mixed flare dataset that incorporates both
2D and 3D perspectives, namely FlareX. This dataset offers 9,500 2D templates
derived from 95 flare patterns and 3,000 flare image pairs rendered from 60 3D
scenes. Furthermore, we design a masking approach to obtain real-world
flare-free images from their corrupted counterparts to measure the performance
of the model on real-world images. Extensive experiments demonstrate the
effectiveness of our method and dataset.

</details>


### [91] [BurstDeflicker: A Benchmark Dataset for Flicker Removal in Dynamic Scenes](https://arxiv.org/abs/2510.09996)
*Lishen Qu,Zhihao Liu,Shihao Zhou,Yaqi Luo,Jie Liang,Hui Zeng,Lei Zhang,Jufeng Yang*

Main category: cs.CV

TL;DR: 为解决滚动快门相机在AC照明下产生的闪烁伪影问题，本文构建了一个名为BurstDeflicker的大规模、多策略（合成、真实、动态场景绿幕）数据集，旨在推动去闪烁研究。


<details>
  <summary>Details</summary>
Motivation: 滚动快门相机在交流电源照明下拍摄时，因行曝光机制与光强度变化相互作用，会产生闪烁伪影（亮度不均、暗带），严重损害图像质量并影响目标检测、跟踪等高级任务。现有研究缺乏大规模、真实的去闪烁数据集，阻碍了该领域发展。

Method: 提出了一个名为BurstDeflicker的可扩展基准数据集，采用三种互补策略：1) 开发基于Retinex的合成管道，可控生成多样化闪烁模式。2) 捕捉4,000张真实世界闪烁图像，帮助模型理解真实闪烁特性。3) 提出绿幕方法，在保留真实闪烁退化前提下，为动态场景图像对引入运动。

Result: 综合实验证明了所构建数据集的有效性。

Conclusion: 该数据集有望推动去闪烁技术的研究进展。

Abstract: Flicker artifacts in short-exposure images are caused by the interplay
between the row-wise exposure mechanism of rolling shutter cameras and the
temporal intensity variations of alternating current (AC)-powered lighting.
These artifacts typically appear as uneven brightness distribution across the
image, forming noticeable dark bands. Beyond compromising image quality, this
structured noise also affects high-level tasks, such as object detection and
tracking, where reliable lighting is crucial. Despite the prevalence of
flicker, the lack of a large-scale, realistic dataset has been a significant
barrier to advancing research in flicker removal. To address this issue, we
present BurstDeflicker, a scalable benchmark constructed using three
complementary data acquisition strategies. First, we develop a Retinex-based
synthesis pipeline that redefines the goal of flicker removal and enables
controllable manipulation of key flicker-related attributes (e.g., intensity,
area, and frequency), thereby facilitating the generation of diverse flicker
patterns. Second, we capture 4,000 real-world flicker images from different
scenes, which help the model better understand the spatial and temporal
characteristics of real flicker artifacts and generalize more effectively to
wild scenarios. Finally, due to the non-repeatable nature of dynamic scenes, we
propose a green-screen method to incorporate motion into image pairs while
preserving real flicker degradation. Comprehensive experiments demonstrate the
effectiveness of our dataset and its potential to advance research in flicker
removal.

</details>


### [92] [MIMO: A medical vision language model with visual referring multimodal input and pixel grounding multimodal output](https://arxiv.org/abs/2510.10011)
*Yanyuan Chen,Dexuan Xu,Yu Huang,Songkun Zhan,Hanpin Wang,Dongxue Chen,Xueping Wang,Meikang Qiu,Hang Li*

Main category: cs.CV

TL;DR: 提出MIMO统一医学视觉语言模型，通过多模态输入（视觉指示）和多模态输出（像素定位）解决现有模型对视觉线索理解不足和输出缺乏图像关联的问题，并构建了MIMOSeg医学多模态数据集。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型在输入时仅依赖文本指令，缺乏对图像视觉线索的直接理解；在输出时只给出文本答案，缺乏与图像关键区域的关联。此外，医学领域相关数据稀缺。

Method: 1. 提出统一医学视觉语言模型MIMO，具有视觉指示的多模态输入和像素定位的多模态输出。 2. 构建MIMOSeg综合医学多模态数据集，包含895K样本，覆盖从基本指令遵循到复杂问答的四种视角。

Result: MIMO模型能够独特地结合视觉指示和像素定位能力，这是以往模型不具备的。实验结果验证了其在多个下游医学多模态任务上的有效性。

Conclusion: MIMO模型成功结合了视觉指示和像素定位能力，有效解决了现有医学视觉语言模型在输入理解和输出关联方面的不足，并通过MIMOSeg数据集克服了数据稀缺问题。

Abstract: Currently, medical vision language models are widely used in medical vision
question answering tasks. However, existing models are confronted with two
issues: for input, the model only relies on text instructions and lacks direct
understanding of visual clues in the image; for output, the model only gives
text answers and lacks connection with key areas in the image. To address these
issues, we propose a unified medical vision language model MIMO, with visual
referring Multimodal Input and pixel grounding Multimodal Output. MIMO can not
only combine visual clues and textual instructions to understand complex
medical images and semantics, but can also ground medical terminologies in
textual output within the image. To overcome the scarcity of relevant data in
the medical field, we propose MIMOSeg, a comprehensive medical multimodal
dataset including 895K samples. MIMOSeg is constructed from four different
perspectives, covering basic instruction following and complex question
answering with multimodal input and multimodal output. We conduct experiments
on several downstream medical multimodal tasks. Extensive experimental results
verify that MIMO can uniquely combine visual referring and pixel grounding
capabilities, which are not available in previous models.

</details>


### [93] [Q-Adapter: Visual Query Adapter for Extracting Textually-related Features in Video Captioning](https://arxiv.org/abs/2510.10022)
*Junan Chen,Trung Thanh Nguyen,Takahiro Komamizu,Ichiro Ide*

Main category: cs.CV

TL;DR: 本文提出Q-Adapter，一种轻量级视觉适配器，通过引入可学习查询token和门控层到视觉编码器，实现对多模态大语言模型（MLLMs）进行视频字幕任务的高效参数微调，在效率和性能上均达到领先水平。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练模型在视频字幕领域面临全模型微调计算成本过高的问题。现有参数高效微调（PEFT）方法主要关注MLLMs的语言部分，在多模态任务中对视觉信息处理不足。

Method: 提出Query-Adapter (Q-Adapter)，一个轻量级视觉适配器模块。它在视觉编码器中引入可学习查询token和门控层，以高效提取与字幕相关的稀疏特征，且无需外部文本监督。

Result: Q-Adapter在MSR-VTT和MSVD两个视频字幕数据集上，在PEFT方法中实现了SOTA性能，并在各项指标上超越现有方法。与全模型微调方法相比，它用仅1.4%的参数取得了有竞争力的性能。文章还分析了超参数和设计选择对微调效果的影响。

Conclusion: Q-Adapter有效平衡了字幕质量和参数效率，展现了其在视频语言建模中的强大潜力及可扩展性，为基于适配器的学习提供了优化策略洞察。

Abstract: Recent advances in video captioning are driven by large-scale pretrained
models, which follow the standard "pre-training followed by fine-tuning"
paradigm, where the full model is fine-tuned for downstream tasks. Although
effective, this approach becomes computationally prohibitive as the model size
increases. The Parameter-Efficient Fine-Tuning (PEFT) approach offers a
promising alternative, but primarily focuses on the language components of
Multimodal Large Language Models (MLLMs). Despite recent progress, PEFT remains
underexplored in multimodal tasks and lacks sufficient understanding of visual
information during fine-tuning the model. To bridge this gap, we propose
Query-Adapter (Q-Adapter), a lightweight visual adapter module designed to
enhance MLLMs by enabling efficient fine-tuning for the video captioning task.
Q-Adapter introduces learnable query tokens and a gating layer into Vision
Encoder, enabling effective extraction of sparse, caption-relevant features
without relying on external textual supervision. We evaluate Q-Adapter on two
well-known video captioning datasets, MSR-VTT and MSVD, where it achieves
state-of-the-art performance among the methods that take the PEFT approach
across BLEU@4, METEOR, ROUGE-L, and CIDEr metrics. Q-Adapter also achieves
competitive performance compared to methods that take the full fine-tuning
approach while requiring only 1.4% of the parameters. We further analyze the
impact of key hyperparameters and design choices on fine-tuning effectiveness,
providing insights into optimization strategies for adapter-based learning.
These results highlight the strong potential of Q-Adapter in balancing caption
quality and parameter efficiency, demonstrating its scalability for
video-language modeling.

</details>


### [94] [P-4DGS: Predictive 4D Gaussian Splatting with 90$\times$ Compression](https://arxiv.org/abs/2510.10030)
*Henan Wang,Hanxin Zhu,Xinliang Gong,Tianyu He,Xin Li,Zhibo Chen*

Main category: cs.CV

TL;DR: P-4DGS通过时空预测和自适应量化显著压缩动态3DGS模型，实现高压缩比、最先进的重建质量和快速渲染。


<details>
  <summary>Details</summary>
Motivation: 现有动态3DGS算法在4D重建中存在大量时空冗余，导致过高的内存消耗，限制了其应用。

Method: 提出P-4DGS。首先，设计基于3D锚点的时空预测模块，利用3D高斯基元间的时空关联；其次，采用自适应量化策略结合上下文熵编码，进一步压缩3D锚点。

Result: P-4DGS在合成和真实数据集上均实现了最先进的重建质量和最快的渲染速度。平均存储占用约1MB，在合成和真实场景中分别达到40倍和90倍的压缩比。

Conclusion: P-4DGS有效解决了动态3DGS模型巨大的内存消耗问题，通过高效压缩技术，在保持卓越渲染性能和重建质量的同时，显著降低了存储需求。

Abstract: 3D Gaussian Splatting (3DGS) has garnered significant attention due to its
superior scene representation fidelity and real-time rendering performance,
especially for dynamic 3D scene reconstruction (\textit{i.e.}, 4D
reconstruction). However, despite achieving promising results, most existing
algorithms overlook the substantial temporal and spatial redundancies inherent
in dynamic scenes, leading to prohibitive memory consumption. To address this,
we propose P-4DGS, a novel dynamic 3DGS representation for compact 4D scene
modeling. Inspired by intra- and inter-frame prediction techniques commonly
used in video compression, we first design a 3D anchor point-based
spatial-temporal prediction module to fully exploit the spatial-temporal
correlations across different 3D Gaussian primitives. Subsequently, we employ
an adaptive quantization strategy combined with context-based entropy coding to
further reduce the size of the 3D anchor points, thereby achieving enhanced
compression efficiency. To evaluate the rate-distortion performance of our
proposed P-4DGS in comparison with other dynamic 3DGS representations, we
conduct extensive experiments on both synthetic and real-world datasets.
Experimental results demonstrate that our approach achieves state-of-the-art
reconstruction quality and the fastest rendering speed, with a remarkably low
storage footprint (around \textbf{1MB} on average), achieving up to
\textbf{40$\times$} and \textbf{90$\times$} compression on synthetic and
real-world scenes, respectively.

</details>


### [95] [Complementary and Contrastive Learning for Audio-Visual Segmentation](https://arxiv.org/abs/2510.10051)
*Sitong Gong,Yunzhi Zhuge,Lu Zhang,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出了一种名为CCFormer的新型互补对比Transformer框架，通过处理局部和全局信息以及捕获时空上下文，解决了现有音视频分割方法在提取多模态系数和时间动态方面的不足，并在多个数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有音视频分割（AVS）方法面临挑战：传统的CNN方法受限于局部感受野，难以有效处理音视频交互；Transformer方法虽然利用注意力机制增强了跨模态协作，但通常难以充分提取多模态系数和时间动态信息。因此，需要一种能够全面捕获局部和全局信息以及时空上下文的方法。

Method: 本文提出了互补对比Transformer (CCFormer) 框架。主要包括：1) 早期融合模块 (EIM)，采用并行双边架构，将多尺度视觉特征与音频数据融合，增强跨模态互补性。2) 多查询Transformer模块 (MTM)，动态赋予音频查询学习能力，同时建模帧级和视频级关系，以提取帧内空间特征并促进时间连贯性感知。3) 双模态对比学习 (BCL)，在统一特征空间中促进两种模态的对齐。

Result: 通过有效结合这些设计，CCFormer在S4、MS3和AVSS数据集上均刷新了现有最先进的性能基准。

Conclusion: CCFormer通过其创新的设计（EIM、MTM、BCL），成功克服了现有AVS方法的局限性，能够全面捕获局部和全局信息以及时空上下文，显著提升了音视频分割的性能，并达到了新的SOTA水平。

Abstract: Audio-Visual Segmentation (AVS) aims to generate pixel-wise segmentation maps
that correlate with the auditory signals of objects. This field has seen
significant progress with numerous CNN and Transformer-based methods enhancing
the segmentation accuracy and robustness. Traditional CNN approaches manage
audio-visual interactions through basic operations like padding and
multiplications but are restricted by CNNs' limited local receptive field. More
recently, Transformer-based methods treat auditory cues as queries, utilizing
attention mechanisms to enhance audio-visual cooperation within frames.
Nevertheless, they typically struggle to extract multimodal coefficients and
temporal dynamics adequately. To overcome these limitations, we present the
Complementary and Contrastive Transformer (CCFormer), a novel framework adept
at processing both local and global information and capturing spatial-temporal
context comprehensively. Our CCFormer initiates with the Early Integration
Module (EIM) that employs a parallel bilateral architecture, merging
multi-scale visual features with audio data to boost cross-modal
complementarity. To extract the intra-frame spatial features and facilitate the
perception of temporal coherence, we introduce the Multi-query Transformer
Module (MTM), which dynamically endows audio queries with learning capabilities
and models the frame and video-level relations simultaneously. Furthermore, we
propose the Bi-modal Contrastive Learning (BCL) to promote the alignment across
both modalities in the unified feature space. Through the effective combination
of those designs, our method sets new state-of-the-art benchmarks across the
S4, MS3 and AVSS datasets. Our source code and model weights will be made
publicly available at https://github.com/SitongGong/CCFormer

</details>


### [96] [Think Twice to See More: Iterative Visual Reasoning in Medical VLMs](https://arxiv.org/abs/2510.10052)
*Kaitao Chen,Shaohao Rui,Yankai Jiang,Jiamin Wu,Qihao Zheng,Chunfeng Song,Xiaosong Wang,Mu Zhou,Mianxin Liu*

Main category: cs.CV

TL;DR: 提出ViTAR，一个模拟人类专家迭代推理的医学视觉语言模型，显著提升了医学AI的性能和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型（VLMs）通常采用单通道推理，忽略局部视觉线索，与人类专家在诊断过程中迭代扫描、聚焦和精炼区域的方式存在差距。

Method: 引入ViTAR框架，通过“思考-行动-再思考-回答”的认知链模拟人类专家迭代推理；将医学图像视为交互对象，实现多步视觉推理。为此，构建了包含1K交互式专家诊断行为示例的高质量指令数据集和16K细粒度视觉问答训练数据。采用两阶段训练策略：先进行监督微调以引导认知轨迹，再通过强化学习优化决策。

Result: ViTAR在广泛评估中超越了现有强大的SOTA模型。视觉注意力分析表明，ViTAR在迭代推理过程中，能逐步将视觉定位锚定到临床关键区域，并在推理时保持对视觉token的高度关注。

Conclusion: 将专家风格的迭代思维链嵌入医学视觉语言模型，能够显著提高医学AI的性能和可信度。

Abstract: Medical vision-language models (VLMs) excel at image-text understanding but
typically rely on a single-pass reasoning that neglects localized visual cues.
In clinical practice, however, human experts iteratively scan, focus, and
refine the regions of interest before reaching a final diagnosis. To narrow
this machine-human perception gap, we introduce ViTAR, a novel VLM framework
that emulates the iterative reasoning process of human experts through a
cognitive chain of "think-act-rethink-answer". ViTAR treats medical images as
interactive objects, enabling models to engage multi-step visual reasoning. To
support this approach, we curate a high-quality instruction dataset comprising
1K interactive examples that encode expert-like diagnostic behaviors. In
addition, a 16K visual question answering training data has been curated
towards fine-grained visual diagnosis. We introduce a two-stage training
strategy that begins with supervised fine-tuning to guide cognitive
trajectories, followed by the reinforcement learning to optimize
decision-making. Extensive evaluations demonstrate that ViTAR outperforms
strong state-of-the-art models. Visual attention analysis reveals that from the
"think" to "rethink" rounds, ViTAR increasingly anchors visual grounding to
clinically critical regions and maintains high attention allocation to visual
tokens during reasoning, providing mechanistic insight into its improved
performance. These findings demonstrate that embedding expert-style iterative
thinking chains into VLMs enhances both performance and trustworthiness of
medical AI.

</details>


### [97] [DREAM: A Benchmark Study for Deepfake REalism AssessMent](https://arxiv.org/abs/2510.10053)
*Bo Peng,Zichuan Wang,Sheng Yu,Xiaochuan Jin,Wei Wang,Jing Dong*

Main category: cs.CV

TL;DR: 本文提出并建立了DREAM基准，用于自动评估深度伪造视频的视觉真实感，以近似人类感知，并对现有评估方法进行了全面分析。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造研究主要关注客观检测，而对其主观感知（特别是视觉真实感）的计算建模和评估研究不足。对深度伪造视觉真实感的自动评估对于评价其质量、欺骗性、预测网络影响以及改进生成过程至关重要。

Method: 本文提出了一个名为DREAM（Deepfake REalism AssessMent）的综合基准。DREAM包含：1) 一个涵盖不同质量的深度伪造视频数据集；2) 包含14万个真实感分数和文本描述的大规模人类标注；3) 对16种代表性真实感评估方法（包括大型视觉语言模型和新提出的描述对齐CLIP方法）进行了全面评估和分析。

Result: 成功构建了DREAM基准，提供了一个多样化的数据集和大规模人类标注，并对多种先进的真实感评估方法进行了系统性评估和分析，包括大型视觉语言模型和一种新颖的描述对齐CLIP方法。该基准和研究中获得的见解被认为能为未来研究奠定基础。

Conclusion: 本研究提出的DREAM基准及其包含的见解，为深度伪造视觉真实感评估这一新研究方向及其他相关领域的未来研究奠定了坚实的基础。

Abstract: Deep learning based face-swap videos, widely known as deepfakes, have drawn
wide attention due to their threat to information credibility. Recent works
mainly focus on the problem of deepfake detection that aims to reliably tell
deepfakes apart from real ones, in an objective way. On the other hand, the
subjective perception of deepfakes, especially its computational modeling and
imitation, is also a significant problem but lacks adequate study. In this
paper, we focus on the visual realism assessment of deepfakes, which is defined
as the automatic assessment of deepfake visual realism that approximates human
perception of deepfakes. It is important for evaluating the quality and
deceptiveness of deepfakes which can be used for predicting the influence of
deepfakes on Internet, and it also has potentials in improving the deepfake
generation process by serving as a critic. This paper prompts this new
direction by presenting a comprehensive benchmark called DREAM, which stands
for Deepfake REalism AssessMent. It is comprised of a deepfake video dataset of
diverse quality, a large scale annotation that includes 140,000 realism scores
and textual descriptions obtained from 3,500 human annotators, and a
comprehensive evaluation and analysis of 16 representative realism assessment
methods, including recent large vision language model based methods and a newly
proposed description-aligned CLIP method. The benchmark and insights included
in this study can lay the foundation for future research in this direction and
other related areas.

</details>


### [98] [Collaborative Learning of Semantic-Aware Feature Learning and Label Recovery for Multi-Label Image Recognition with Incomplete Labels](https://arxiv.org/abs/2510.10055)
*Zhi-Fen He,Ren-Dong Xie,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: 本文提出CLSL方法，通过统一语义感知特征学习和缺失标签恢复，解决了不完整标签多标签图像识别的两大核心挑战，并在多个基准数据集上超越了现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 不完整标签多标签图像识别面临语义感知特征学习和缺失标签恢复两大核心挑战。

Method: 本文提出协作学习的语义感知特征学习和标签恢复（CLSL）方法。该方法设计了语义相关特征学习模块以发现语义信息和标签关联，学习鲁棒特征；提出语义引导特征增强模块以对齐视觉和语义特征空间，生成判别性语义感知特征；并引入协作学习框架，整合特征学习和标签恢复，形成相互强化的循环，动态增强特征判别性并自适应推断恢复缺失标签。

Result: 在MS-COCO、VOC2007和NUS-WIDE三个广泛使用的公共数据集上的实验表明，CLSL方法优于当前最先进的不完整标签多标签图像识别方法。

Conclusion: CLSL方法通过其统一的协作学习框架，有效解决了不完整标签多标签图像识别中的核心挑战，并取得了最先进的性能。

Abstract: Multi-label image recognition with incomplete labels is a critical learning
task and has emerged as a focal topic in computer vision. However, this task is
confronted with two core challenges: semantic-aware feature learning and
missing label recovery. In this paper, we propose a novel Collaborative
Learning of Semantic-aware feature learning and Label recovery (CLSL) method
for multi-label image recognition with incomplete labels, which unifies the two
aforementioned challenges into a unified learning framework. More specifically,
we design a semantic-related feature learning module to learn robust
semantic-related features by discovering semantic information and label
correlations. Then, a semantic-guided feature enhancement module is proposed to
generate high-quality discriminative semantic-aware features by effectively
aligning visual and semantic feature spaces. Finally, we introduce a
collaborative learning framework that integrates semantic-aware feature
learning and label recovery, which can not only dynamically enhance the
discriminability of semantic-aware features but also adaptively infer and
recover missing labels, forming a mutually reinforced loop between the two
processes. Extensive experiments on three widely used public datasets (MS-COCO,
VOC2007, and NUS-WIDE) demonstrate that CLSL outperforms the state-of-the-art
multi-label image recognition methods with incomplete labels.

</details>


### [99] [Probabilistic Hyper-Graphs using Multiple Randomly Masked Autoencoders for Semi-supervised Multi-modal Multi-task Learning](https://arxiv.org/abs/2510.10068)
*Pîrvu Mihai-Cristian,Leordeanu Marius*

Main category: cs.CV

TL;DR: 本文提出了一种名为PHG-MAE的新型模型，它将经典图神经网络与现代掩码自编码器（MAE）统一起来。PHG-MAE通过随机掩码整个模态、将预训练和微调整合到单一训练循环以及利用推理时集成学习，显著提升了多模态视觉任务的性能和一致性，并支持知识蒸馏，同时发布了数据管道软件和数据集扩展。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉领域受益于大量数据，自监督预训练方法（如MAE）因无需手动标注数据而备受关注。然而，现有方法需要将经典的神经网络图理论与现代的掩码自编码器方法进行统一，并需要进一步提升多模态、多任务学习场景下的预测性能和一致性。

Method: 引入了概率超图掩码自编码器（PHG-MAE），它在一个统一的理论框架下结合了神经网络图和掩码自编码器。该模型通过随机掩码整个模态（而非仅补丁）来从超边分布中采样。它还将预训练和微调结合到单一训练循环中。此外，PHG-MAE支持创建推理时集成模型以提升预测性能和一致性，并能在集成模型之上应用知识蒸馏。为简化外部预训练专家的集成，开发了一个数据管道软件，并创建发布了Dronescapes数据集的完全自动化扩展。

Result: PHG-MAE通过推理时集成显著提升了最终预测的性能和一致性。知识蒸馏可以在集成模型之上应用，且性能损失很小，即使对于参数量小于1M的小模型也适用。该方法主要针对室外无人机场景，但也适用于自动驾驶或室内机器人等其他领域。

Conclusion: PHG-MAE成功将经典神经网络图和现代掩码自编码器整合，通过模态级掩码、统一训练循环、集成学习和知识蒸馏，有效提升了多模态视觉任务的性能和鲁棒性。该工作还提供了实用的数据管道软件和数据集扩展，具有良好的通用性和可推广性。

Abstract: The computer vision domain has greatly benefited from an abundance of data
across many modalities to improve on various visual tasks. Recently, there has
been a lot of focus on self-supervised pre-training methods through Masked
Autoencoders (MAE) \cite{he2022masked,bachmann2022multimae}, usually used as a
first step before optimizing for a downstream task, such as classification or
regression. This is very useful as it doesn't require any manually labeled
data. In this work, we introduce Probabilistic Hyper-Graphs using Masked
Autoencoders (PHG-MAE): a novel model that unifies the classical work on neural
graphs \cite{leordeanu2021semi} with the modern approach of masked autoencoders
under a common theoretical framework. Through random masking of entire
modalities, not just patches, the model samples from the distribution of
hyper-edges on each forward pass. Additionally, the model adapts the standard
MAE algorithm by combining pre-training and fine-tuning into a single training
loop. Moreover, our approach enables the creation of inference-time ensembles
which, through aggregation, boost the final prediction performance and
consistency. Lastly, we show that we can apply knowledge distillation on top of
the ensembles with little loss in performance, even with models that have fewer
than 1M parameters. While our work mostly focuses on outdoor UAV scenes that
contain multiple world interpretations and modalities, the same steps can be
followed in other similar domains, such as autonomous driving or indoor
robotics. In order to streamline the process of integrating external
pre-trained experts for computer vision multi-modal multi-task learning (MTL)
scenarios, we developed a data-pipeline software. Using this tool, we have
created and released a fully-automated extension of the Dronescapes dataset.
All the technical details, code and reproduction steps are publicly released.

</details>


### [100] [Tracking the Spatiotemporal Evolution of Landslide Scars Using a Vision Foundation Model: A Novel and Universal Framework](https://arxiv.org/abs/2510.10084)
*Meijun Zhou,Gang Mei,Zhengjing Ma,Nengxiong Xu,Jianbing Peng*

Main category: cs.CV

TL;DR: 提出一种新框架，利用视觉基础模型将离散遥感图像重建为连续视频序列，以持续追踪大规模滑坡疤痕的时空演变。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要侧重于单阶段或双阶段滑坡识别，难以连续追踪滑坡疤痕的时空演变，限制了对演变机制、失效前兆的理解和有效预警。

Method: 本研究提出一个新颖通用的框架，核心是将离散光学遥感图像重建为连续视频序列。这使得为视频分割开发的视觉基础模型能够用于追踪滑坡疤痕的演变。该框架采用知识引导、自动传播和交互式完善的范式。

Result: 该框架在两个代表性案例中得到验证，成功实现了滑坡疤痕的连续追踪，捕获了对早期预警至关重要的失效前兆以及对评估次生灾害和长期稳定性至关重要的失效后演变。

Conclusion: 该框架有效解决了大规模滑坡疤痕连续时空演变追踪的难题，为理解滑坡机制、实现早期预警和评估次生灾害与长期稳定性提供了关键支持。

Abstract: Tracking the spatiotemporal evolution of large-scale landslide scars is
critical for understanding the evolution mechanisms and failure precursors,
enabling effective early-warning. However, most existing studies have focused
on single-phase or pre- and post-failure dual-phase landslide identification.
Although these approaches delineate post-failure landslide boundaries, it is
challenging to track the spatiotemporal evolution of landslide scars. To
address this problem, this study proposes a novel and universal framework for
tracking the spatiotemporal evolution of large-scale landslide scars using a
vision foundation model. The key idea behind the proposed framework is to
reconstruct discrete optical remote sensing images into a continuous video
sequence. This transformation enables a vision foundation model, which is
developed for video segmentation, to be used for tracking the evolution of
landslide scars. The proposed framework operates within a knowledge-guided,
auto-propagation, and interactive refinement paradigm to ensure the continuous
and accurate identification of landslide scars. The proposed framework was
validated through application to two representative cases: the post-failure
Baige landslide and the active Sela landslide (2017-2025). Results indicate
that the proposed framework enables continuous tracking of landslide scars,
capturing both failure precursors critical for early warning and post-failure
evolution essential for assessing secondary hazards and long-term stability.

</details>


### [101] [Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian Splatting](https://arxiv.org/abs/2510.10097)
*Jiahui Lu,Haihong Xiao,Xueyan Zhao,Wenxiong Kang*

Main category: cs.CV

TL;DR: Gesplat是一个基于3DGS的框架，通过利用VGGT模型和多项创新，实现了从无姿态稀疏图像进行鲁棒的新颖视角合成和几何一致的3D重建。


<details>
  <summary>Details</summary>
Motivation: NeRF和3DGS等技术高度依赖准确的相机姿态和密集的视角覆盖，这限制了它们在稀疏视图设置下的应用，因为在这种情况下姿态估计不可靠且监督信息不足。

Method: 引入Gesplat框架，它基于3DGS，并通过以下方式克服挑战：1) 利用VGGT基础模型获取更可靠的初始姿态和稠密点云（替代COLMAP）；2) 采用混合高斯表示，结合双重位置-形状优化和视图间匹配一致性；3) 使用图引导的属性细化模块增强场景细节；4) 应用基于光流的深度正则化以提高深度估计精度。

Result: 定量和定性实验表明，与现有无姿态方法相比，Gesplat在正向场景和大规模复杂数据集上均展现出更鲁棒的性能。

Conclusion: Gesplat为从无姿态稀疏图像进行鲁棒的新颖视角合成和几何一致重建提供了一种有效解决方案，显著优于其他无姿态方法。

Abstract: Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced
3D reconstruction and novel view synthesis, but remain heavily dependent on
accurate camera poses and dense viewpoint coverage. These requirements limit
their applicability in sparse-view settings, where pose estimation becomes
unreliable and supervision is insufficient. To overcome these challenges, we
introduce Gesplat, a 3DGS-based framework that enables robust novel view
synthesis and geometrically consistent reconstruction from unposed sparse
images. Unlike prior works that rely on COLMAP for sparse point cloud
initialization, we leverage the VGGT foundation model to obtain more reliable
initial poses and dense point clouds. Our approach integrates several key
innovations: 1) a hybrid Gaussian representation with dual position-shape
optimization enhanced by inter-view matching consistency; 2) a graph-guided
attribute refinement module to enhance scene details; and 3) flow-based depth
regularization that improves depth estimation accuracy for more effective
supervision. Comprehensive quantitative and qualitative experiments demonstrate
that our approach achieves more robust performance on both forward-facing and
large-scale complex datasets compared to other pose-free methods.

</details>


### [102] [Cooperative Pseudo Labeling for Unsupervised Federated Classification](https://arxiv.org/abs/2510.10100)
*Kuangpu Guo,Lijun Sheng,Yongcan Yu,Jian Liang,Zilei Wang,Ran He*

Main category: cs.CV

TL;DR: 本文提出FedCoPL，首次将无监督联邦学习(UFL)扩展到基于CLIP的分类任务。通过客户端协作伪标签、服务器全局平衡调整以及视觉提示聚合与文本提示本地化的混合聚合策略，FedCoPL在不共享数据和标签的情况下，实现了优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 以往的无监督联邦学习(UFL)主要关注表示学习和聚类任务。尽管视觉语言模型（如CLIP）的零样本预测能力强大，但将UFL应用于分类问题，并利用CLIP的优势，这一领域仍未被充分探索，存在巨大的机会和挑战。

Method: 本文提出了一种名为FedCoPL（Federated Cooperative Pseudo-Labeling）的新方法。具体而言，客户端估计并上传其伪标签分布，服务器负责调整并重新分发这些伪标签，以避免全局类别不平衡。此外，引入了部分提示聚合协议：视觉提示（包含通用图像特征）在服务器端进行聚合，而文本提示（编码个性化知识）则保留在客户端本地。

Result: 广泛的实验表明，FedCoPL相较于基线方法展现出卓越的性能。

Conclusion: FedCoPL成功地将无监督联邦学习扩展到基于CLIP的分类问题，通过创新的协作伪标签机制和部分提示聚合策略，有效解决了无监督环境下的类不平衡和个性化需求，并实现了显著的性能提升。

Abstract: Unsupervised Federated Learning (UFL) aims to collaboratively train a global
model across distributed clients without sharing data or accessing label
information. Previous UFL works have predominantly focused on representation
learning and clustering tasks. Recently, vision language models (e.g., CLIP)
have gained significant attention for their powerful zero-shot prediction
capabilities. Leveraging this advancement, classification problems that were
previously infeasible under the UFL paradigm now present promising new
opportunities, yet remain largely unexplored. In this paper, we extend UFL to
the classification problem with CLIP for the first time and propose a novel
method, \underline{\textbf{Fed}}erated \underline{\textbf{Co}}operative
\underline{\textbf{P}}seudo \underline{\textbf{L}}abeling (\textbf{FedCoPL}).
Specifically, clients estimate and upload their pseudo label distribution, and
the server adjusts and redistributes them to avoid global imbalance among
classes. Moreover, we introduce a partial prompt aggregation protocol for
effective collaboration and personalization. In particular, visual prompts
containing general image features are aggregated at the server, while text
prompts encoding personalized knowledge are retained locally. Extensive
experiments demonstrate the superior performance of our FedCoPL compared to
baseline methods. Our code is available at
\href{https://github.com/krumpguo/FedCoPL}{https://github.com/krumpguo/FedCoPL}.

</details>


### [103] [Answer-Consistent Chain-of-thought Reinforcement Learning For Multi-modal Large Langauge Models](https://arxiv.org/abs/2510.10104)
*Minbin Huang,Runhui Huang,Chuanyang Zheng,Jingyao Li,Guoxuan Chen,Han Shi,Hong Cheng*

Main category: cs.CV

TL;DR: 针对RLVR训练的LLM中推理链与最终答案不一致问题，本文提出了ACRE算法，通过引入辅助一致性检查和一致性验证奖励，有效提升了多模态推理任务的性能和一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于可验证奖励的强化学习(RLVR)能显著提升LLM的推理能力，但它可能导致推理链与最终答案脱节，产生不一致。例如，在多项选择视觉问答任务中，标准GRPO方法在MMVU上的推理一致性仅为79.7%。

Method: 本文提出了答案一致性强化学习(ACRE)算法，它修改了GRPO算法，增加了辅助一致性检查。具体做法是：模型生成推理链和初步答案后，打乱选项并用相同的推理链再次提示模型预测第二个答案。设计了一种一致性验证奖励，只有当原始答案和打乱选项后的答案一致且都正确时才给予高奖励，否则给予较低奖励，以此惩罚推理与答案的不一致，并避免模型依赖虚假模式（如选项顺序偏差）。

Result: ACRE在挑战性的视频推理基准测试和多模态数学推理基准测试中进行了评估。相对于GRPO基线，视频推理任务平均提高了2.2%，数学推理任务平均提高了1.5%。

Conclusion: ACRE通过解决RLVR模型中推理与答案不一致的问题，显著提升了LLM在复杂视频和图像理解任务上的性能和可靠性。

Abstract: Recent advances in large language models (LLMs) have demonstrated that
reinforcement learning with verifiable rewards (RLVR) can significantly enhance
reasoning abilities by directly optimizing correctness, rather than relying
solely on supervised imitation. This paradigm has been extended to multimodal
LLMs for complex video and image understanding tasks. However, while
outcome-driven RL improves answer accuracy, it can inadvertently decouple the
reasoning chain from the final answer, leading to situations where models
produce inconsistency between the reasoning trace and final answer. In our
experiments on multiple-choice visual question-answering tasks, the standard
GRPO method yields only 79.7\% consistency on MMVU between the reasoning steps
and the chosen answers, indicating frequent mismatches between answers and
reasoning. To this end, we propose Answer-Consistent Reinforcement Learning
(ACRE) that modifies the GRPO algorithm with an auxiliary consistency check.
After the model generates a chain of thought and an initial answer for a given
question, we shuffle the answer options and prompt the model again with the
same reasoning trace to predict a second answer. We design a
consistency-verification reward that grants a high reward only if both the
original and the post-shuffle answers agree and are correct; otherwise, a lower
reward is assigned accordingly. This mechanism penalizes reasoning-answer
misalignment and discourages the model from relying on spurious patterns, such
as option ordering biases. We evaluate ACRE on challenging Video Reasoning
benchmarks and multimodal math reasoning benchmarks, achieving an average 2.2\%
and 1.5\% improvement for Video Reasoning and Math Reasoning tasks over the
GRPO baseline.

</details>


### [104] [Uncertainty-Aware Post-Detection Framework for Enhanced Fire and Smoke Detection in Compact Deep Learning Models](https://arxiv.org/abs/2510.10108)
*Aniruddha Srinivas Joshi,Godwyn James William,Shreyas Srinivas Joshi*

Main category: cs.CV

TL;DR: 提出一种不确定性感知的后处理框架，结合统计不确定性和视觉特征，提高紧凑型深度学习模型在火灾烟雾检测中的鲁棒性，有效解决误报和漏报问题。


<details>
  <summary>Details</summary>
Motivation: 现有紧凑型深度学习模型（如YOLOv5n/v8n）在火灾烟雾检测中效率高但可靠性不足，易产生误报和漏报。传统后处理方法（如NMS）仅依赖空间重叠，无法有效处理复杂场景中的火灾烟雾。

Method: 开发了一个不确定性感知的后处理框架，通过一个轻量级置信度优化网络（Confidence Refinement Network），将统计不确定性与颜色、边缘、纹理等领域相关视觉特征相结合，重新调整检测置信度，且无需修改基础检测模型。

Result: 在D-Fire数据集上的实验表明，与现有基线相比，该方法在精确率、召回率和平均精度方面均有显著提升，且仅引入适度的计算开销。

Conclusion: 研究结果强调了后处理重评分在增强紧凑型深度学习模型于真实世界火灾烟雾检测中鲁棒性的有效性。

Abstract: Accurate fire and smoke detection is critical for safety and disaster
response, yet existing vision-based methods face challenges in balancing
efficiency and reliability. Compact deep learning models such as YOLOv5n and
YOLOv8n are widely adopted for deployment on UAVs, CCTV systems, and IoT
devices, but their reduced capacity often results in false positives and missed
detections. Conventional post-detection methods such as Non-Maximum Suppression
and Soft-NMS rely only on spatial overlap, which can suppress true positives or
retain false alarms in cluttered or ambiguous fire scenes. To address these
limitations, we propose an uncertainty aware post-detection framework that
rescales detection confidences using both statistical uncertainty and domain
relevant visual cues. A lightweight Confidence Refinement Network integrates
uncertainty estimates with color, edge, and texture features to adjust
detection scores without modifying the base model. Experiments on the D-Fire
dataset demonstrate improved precision, recall, and mean average precision
compared to existing baselines, with only modest computational overhead. These
results highlight the effectiveness of post-detection rescoring in enhancing
the robustness of compact deep learning models for real-world fire and smoke
detection.

</details>


### [105] [Training-Free In-Context Forensic Chain for Image Manipulation Detection and Localization](https://arxiv.org/abs/2510.10111)
*Rui Chen,Bin Liu,Changtao Miao,Xinghao Wang,Yi Li,Tao Gong,Qi Chu,Nenghai Yu*

Main category: cs.CV

TL;DR: ICFC是一个无训练的框架，利用多模态大语言模型(MLLM)实现可解释的图像篡改定位(IML)，其性能超越了现有无训练方法，并与监督式方法相媲美或更优。


<details>
  <summary>Details</summary>
Motivation: 图像篡改构成严重安全威胁，对有效的图像篡改定位(IML)需求迫切。现有监督方法依赖昂贵标注，而弱监督或无训练方法性能不足且缺乏可解释性。

Method: 提出 In-Context Forensic Chain (ICFC)，一个利用MLLM的无训练框架。它结合客观测规则构建、自适应过滤以建立知识库，并采用多步骤渐进推理流程，模拟专家取证工作流，实现图像级分类、像素级定位和文本级可解释性。

Result: 在多个基准测试中，ICFC超越了最先进的无训练方法，并与弱监督和全监督方法相比，取得了有竞争力或更优的性能。

Conclusion: ICFC成功提供了一种无训练、可解释且高性能的图像篡改定位方案，有效解决了现有方法的标注成本高和可解释性不足的问题。

Abstract: Advances in image tampering pose serious security threats, underscoring the
need for effective image manipulation localization (IML). While supervised IML
achieves strong performance, it depends on costly pixel-level annotations.
Existing weakly supervised or training-free alternatives often underperform and
lack interpretability. We propose the In-Context Forensic Chain (ICFC), a
training-free framework that leverages multi-modal large language models
(MLLMs) for interpretable IML tasks. ICFC integrates an objectified rule
construction with adaptive filtering to build a reliable knowledge base and a
multi-step progressive reasoning pipeline that mirrors expert forensic
workflows from coarse proposals to fine-grained forensics results. This design
enables systematic exploitation of MLLM reasoning for image-level
classification, pixel-level localization, and text-level interpretability.
Across multiple benchmarks, ICFC not only surpasses state-of-the-art
training-free methods but also achieves competitive or superior performance
compared to weakly and fully supervised approaches.

</details>


### [106] [ImmerIris: A Large-Scale Dataset and Benchmark for Immersive Iris Recognition in Open Scenes](https://arxiv.org/abs/2510.10113)
*Yuxi Mi,Qiuyang Yuan,Zhizhou Zhong,Xuan Zhao,Jiaogen Zhou,Fubao Zhu,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 本文针对沉浸式离轴虹膜识别的挑战，发布了一个大型离轴虹膜数据集ImmerIris，并提出了一种无需归一化的新范式，该方法在沉浸式场景下表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 沉浸式虹膜识别在AR/VR应用中具有潜力，但离轴采集方式带来透视畸变、图像质量下降和类内变化等独特挑战。现有缺乏专门捕获这些挑战的大规模数据集。

Method: 1. 引入ImmerIris数据集：一个通过VR头显收集的、包含499,791张眼部图像和564个主体的大规模离轴虹膜数据集。2. 基于ImmerIris构建了评估协议，用于基准测试在不同挑战因素下的识别方法。3. 提出一种无需归一化的范式，直接从眼部图像中学习，以解决传统方法中易错的归一化问题。

Result: 1. 现有主要为轴上采集设计的识别方法在沉浸式设置下表现不佳，主要原因是其依赖的归一化过程存在缺陷。2. 提出的无需归一化的范式，尽管简单，但始终优于基于归一化的对应方法。

Conclusion: 无需归一化的范式为实现鲁棒的沉浸式虹膜识别指明了一个有前景的方向。

Abstract: In egocentric applications such as augmented and virtual reality, immersive
iris recognition is emerging as an accurate and seamless way to identify
persons. While classic systems acquire iris images on-axis, i.e., via dedicated
frontal sensors in controlled settings, the immersive setup primarily captures
off-axis irises through tilt-placed headset cameras, with only mild control in
open scenes. This yields unique challenges, including perspective distortion,
intensified quality degradations, and intra-class variations in iris texture.
Datasets capturing these challenges remain scarce. To fill this gap, this paper
introduces ImmerIris, a large-scale dataset collected via VR headsets,
containing 499,791 ocular images from 564 subjects. It is, to the best of
current knowledge, the largest public dataset and among the first dedicated to
off-axis acquisition. Based on ImmerIris, evaluation protocols are constructed
to benchmark recognition methods under different challenging factors. Current
methods, primarily designed for classic on-axis imagery, perform
unsatisfactorily on the immersive setup, mainly due to reliance on fallible
normalization. To this end, this paper further proposes a normalization-free
paradigm that directly learns from ocular images with minimal adjustment.
Despite its simplicity, this approach consistently outperforms
normalization-based counterparts, pointing to a promising direction for robust
immersive recognition.

</details>


### [107] [Multi Class Parkinsons Disease Detection Based on Finger Tapping Using Attention-Enhanced CNN BiLSTM](https://arxiv.org/abs/2510.10121)
*Abu Saleh Musa Miah,Najmul Hassan,Md Maruf Al Hossain,Yuichi Okuyama,Jungpil Shin*

Main category: cs.CV

TL;DR: 本研究提出了一种基于指尖敲击视频特征的注意力增强型CNN BiLSTM模型，用于多类别帕金森病（PD）严重程度的自动检测。


<details>
  <summary>Details</summary>
Motivation: 帕金森病严重程度的准确评估对于临床管理至关重要，但现有基于手势的PD识别系统性能不足。

Method: 收集指尖敲击视频，从中提取手腕和手部运动的时间、频率和幅度特征。构建混合深度学习框架，结合CNN（捕获局部空间依赖）、BiLSTM（建模时间动态）和注意力机制（聚焦信息量大的时间特征），最终通过全连接层和Softmax分类器输出PD严重程度。

Result: 该模型在区分五种不同PD严重程度类别方面表现出强大的性能。

Conclusion: 结合时空表示和注意力机制可以有效提高PD严重程度的自动化检测，为临床PD监测和进展追踪提供了一种有前景的非侵入性工具。

Abstract: Effective clinical management and intervention development depend on accurate
evaluation of Parkinsons disease (PD) severity. Many researchers have worked on
developing gesture-based PD recognition systems; however, their performance
accuracy is not satisfactory. In this study, we propose a multi-class Parkinson
Disease detection system based on finger tapping using an attention-enhanced
CNN BiLSTM. We collected finger tapping videos and derived temporal, frequency,
and amplitude based features from wrist and hand movements. Then, we proposed a
hybrid deep learning framework integrating CNN, BiLSTM, and attention
mechanisms for multi-class PD severity classification from video-derived motion
features. First, the input sequence is reshaped and passed through a Conv1D
MaxPooling block to capture local spatial dependencies. The resulting feature
maps are fed into a BiLSTM layer to model temporal dynamics. An attention
mechanism focuses on the most informative temporal features, producing a
context vector that is further processed by a second BiLSTM layer. CNN-derived
features and attention-enhanced BiLSTM outputs are concatenated, followed by
dense and dropout layers, before the final softmax classifier outputs the
predicted PD severity level. The model demonstrated strong performance in
distinguishing between the five severity classes, suggesting that integrating
spatial temporal representations with attention mechanisms can improve
automated PD severity detection, making it a promising non-invasive tool to
support clinicians in PD monitoring and progression tracking.

</details>


### [108] [DeepFusionNet: Autoencoder-Based Low-Light Image Enhancement and Super-Resolution](https://arxiv.org/abs/2510.10122)
*Halil Hüseyin Çalışkan,Talha Koruk*

Main category: cs.CV

TL;DR: 本文提出了DeepFusionNet架构，用于解决低光图像增强和图像超分辨率问题，相比现有方法，在保持良好性能的同时显著降低了模型参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有的自动编码器方法在处理低光图像增强时，常导致SSIM和PSNR分数较低，且参数量大，计算成本高。此外，将模糊低分辨率图像转换为高清图像在图像处理应用中也日益重要。

Method: 开发了DeepFusionNet架构用于低光图像增强。在此基础上，设计了一个基于DeepFusionNet架构的自动编码器超分辨率模型，以实现模糊和低分辨率图像的高清转换。

Result: 在LOL-v1数据集上，DeepFusionNet进行低光增强时，SSIM达到92.8%，PSNR达到26.30，参数量约为250万。基于DeepFusionNet的超分辨率方法在验证集上实现了PSNR 25.30和SSIM 80.7%的性能，且参数量仅约10万。

Conclusion: DeepFusionNet架构为低光图像增强和超分辨率提供了高效且参数量低的解决方案，在图像质量和计算效率方面均表现出色。

Abstract: Computer vision and image processing applications suffer from dark and
low-light images, particularly during real-time image transmission. Currently,
low light and dark images are converted to bright and colored forms using
autoencoders; however, these methods often achieve low SSIM and PSNR scores and
require high computational power due to their large number of parameters. To
address these challenges, the DeepFusionNet architecture has been developed.
According to the results obtained with the LOL-v1 dataset, DeepFusionNet
achieved an SSIM of 92.8% and a PSNR score of 26.30, while containing only
approximately 2.5 million parameters. On the other hand, conversion of blurry
and low-resolution images into high-resolution and blur-free images has gained
importance in image processing applications. Unlike GAN-based super-resolution
methods, an autoencoder-based super resolution model has been developed that
contains approximately 100 thousand parameters and uses the DeepFusionNet
architecture. According to the results of the tests, the DeepFusionNet based
super-resolution method achieved a PSNR of 25.30 and a SSIM score of 80.7
percent according to the validation set.

</details>


### [109] [YOLOv11-Litchi: Efficient Litchi Fruit Detection based on UAV-Captured Agricultural Imagery in Complex Orchard Environments](https://arxiv.org/abs/2510.10141)
*Hongxing Peng,Haopei Xie,Weijia Lia,Huanai Liuc,Ximing Li*

Main category: cs.CV

TL;DR: 本文提出YOLOv11-Litchi模型，一个轻量级且鲁棒的无人机荔枝检测模型，通过多尺度特征、轻量融合和遮挡检测头，解决了小目标、模型大和遮挡问题，实现了高精度实时检测。


<details>
  <summary>Details</summary>
Motivation: 传统手工荔枝筛选方法效率低下，难以满足现代生产需求。将无人机航拍与深度学习结合是提高效率和降低成本的有效途径，但现有方法在处理小目标、模型部署以及目标遮挡方面存在挑战。

Method: 本文在YOLOv11框架基础上，提出了YOLOv11-Litchi模型。主要创新包括：1) 多尺度残差模块，以增强跨尺度上下文特征提取；2) 轻量级特征融合方法，在保持高精度的同时降低模型尺寸和计算成本；3) 荔枝遮挡检测头，通过强调目标区域和抑制背景干扰来减轻遮挡影响。

Result: YOLOv11-Litchi模型参数量为6.35 MB，比YOLOv11基线模型小32.5%。mAP提升2.5%达到90.1%，F1-Score提升1.4%达到85.5%。模型帧率达到57.2 FPS，满足实时检测要求。

Conclusion: 研究结果表明YOLOv11-Litchi模型适用于复杂果园环境下的无人机荔枝检测，并在精准农业中具有广泛应用潜力。

Abstract: Litchi is a high-value fruit, yet traditional manual selection methods are
increasingly inadequate for modern production demands. Integrating UAV-based
aerial imagery with deep learning offers a promising solution to enhance
efficiency and reduce costs. This paper introduces YOLOv11-Litchi, a
lightweight and robust detection model specifically designed for UAV-based
litchi detection. Built upon the YOLOv11 framework, the proposed model
addresses key challenges such as small target size, large model parameters
hindering deployment, and frequent target occlusion. To tackle these issues,
three major innovations are incorporated: a multi-scale residual module to
improve contextual feature extraction across scales, a lightweight feature
fusion method to reduce model size and computational costs while maintaining
high accuracy, and a litchi occlusion detection head to mitigate occlusion
effects by emphasizing target regions and suppressing background interference.
Experimental results validate the model's effectiveness. YOLOv11-Litchi
achieves a parameter size of 6.35 MB - 32.5% smaller than the YOLOv11 baseline
- while improving mAP by 2.5% to 90.1% and F1-Score by 1.4% to 85.5%.
Additionally, the model achieves a frame rate of 57.2 FPS, meeting real-time
detection requirements. These findings demonstrate the suitability of
YOLOv11-Litchi for UAV-based litchi detection in complex orchard environments,
showcasing its potential for broader applications in precision agriculture.

</details>


### [110] [Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer](https://arxiv.org/abs/2510.10152)
*Yecong Wan,Mingwen Shao,Renlong Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 本文提出了Color3D，一个高度适应的框架，用于从单色输入对静态和动态3D场景进行着色，提供视觉多样、色彩鲜艳的重建，并具有灵活的用户引导控制，同时确保跨视图和跨时间的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注静态场景，通过平均颜色变化来强制多视图一致性，这牺牲了色彩丰富度和可控性。

Method: 核心思想是仅对单个关键视图着色，然后微调一个个性化着色器，将其颜色传播到新的视图和时间步。通过个性化，着色器学习到场景特定的颜色映射，从而一致地投射颜色。训练后，该着色器推断所有其他图像的一致色度，结合Lab色彩空间高斯泼溅表示重建3D场景。此方法将复杂3D着色重构为更易处理的单图像范式。

Result: 大量实验证明，该方法可以提供更一致、色彩更丰富的渲染，并具有精确的用户控制，超越了现有基准。

Conclusion: Color3D框架通过独特的单图像着色范式，实现了静态及动态3D场景的视觉多样、色彩鲜艳、用户可控的重建，同时确保了色彩多样性、可控性以及跨视图和跨时间的一致性，并能无缝集成任意图像着色模型。

Abstract: In this work, we present Color3D, a highly adaptable framework for colorizing
both static and dynamic 3D scenes from monochromatic inputs, delivering
visually diverse and chromatically vibrant reconstructions with flexible
user-guided control. In contrast to existing methods that focus solely on
static scenarios and enforce multi-view consistency by averaging color
variations which inevitably sacrifice both chromatic richness and
controllability, our approach is able to preserve color diversity and
steerability while ensuring cross-view and cross-time consistency. In
particular, the core insight of our method is to colorize only a single key
view and then fine-tune a personalized colorizer to propagate its color to
novel views and time steps. Through personalization, the colorizer learns a
scene-specific deterministic color mapping underlying the reference view,
enabling it to consistently project corresponding colors to the content in
novel views and video frames via its inherent inductive bias. Once trained, the
personalized colorizer can be applied to infer consistent chrominance for all
other images, enabling direct reconstruction of colorful 3D scenes with a
dedicated Lab color space Gaussian splatting representation. The proposed
framework ingeniously recasts complicated 3D colorization as a more tractable
single image paradigm, allowing seamless integration of arbitrary image
colorization models with enhanced flexibility and controllability. Extensive
experiments across diverse static and dynamic 3D colorization benchmarks
substantiate that our method can deliver more consistent and chromatically rich
renderings with precise user control. Project Page
https://yecongwan.github.io/Color3D/.

</details>


### [111] [Stroke Locus Net: Occluded Vessel Localization from MRI Modalities](https://arxiv.org/abs/2510.10155)
*Mohamed Hamad,Muhammad Khan,Tamer Khattab,Mohamed Mabrok*

Main category: cs.CV

TL;DR: 本研究提出Stroke Locus Net，一个端到端的深度学习管道，仅使用MRI扫描实现缺血性卒中病灶检测、分割和闭塞血管定位。


<details>
  <summary>Details</summary>
Motivation: 缺血性卒中诊断中的一个关键挑战是准确定位闭塞血管，现有机器学习方法主要关注病灶分割，对血管定位的研究有限。

Method: Stroke Locus Net结合了使用nnUNet进行病灶检测的分割分支、用于血管映射和识别的动脉图谱，以及使用pGAN从MRI合成MRA图像的生成分支。

Result: 该系统在卒中T1 MRI扫描上成功定位闭塞血管，展示了有前景的结果。

Conclusion: 该方法有望实现更快、更知情的卒中诊断。

Abstract: A key challenge in ischemic stroke diagnosis using medical imaging is the
accurate localization of the occluded vessel. Current machine learning methods
in focus primarily on lesion segmentation, with limited work on vessel
localization. In this study, we introduce Stroke Locus Net, an end-to-end deep
learning pipeline for detection, segmentation, and occluded vessel localization
using only MRI scans. The proposed system combines a segmentation branch using
nnUNet for lesion detection with an arterial atlas for vessel mapping and
identification, and a generation branch using pGAN to synthesize MRA images
from MRI. Our implementation demonstrates promising results in localizing
occluded vessels on stroke-affected T1 MRI scans, with potential for faster and
more informed stroke diagnosis.

</details>


### [112] [ReMix: Towards a Unified View of Consistent Character Generation and Editing](https://arxiv.org/abs/2510.10156)
*Benjia Zhou,Bin Fu,Pei Cheng,Yanru Wang,Jiayuan Fan,Tao Chen*

Main category: cs.CV

TL;DR: ReMix是一个统一的框架，旨在解决现有方法在角色一致性图像生成和编辑任务中缺乏统一性、身份一致性和空间控制力的问题。它通过ReMix模块处理语义编辑和指令适应，并使用IP-ControlNet增强像素级一致性和姿态可控性，从而实现高效的角色一致性图像生成和编辑。


<details>
  <summary>Details</summary>
Motivation: 现有大型文本到图像扩散模型在角色一致性生成和编辑方面表现良好，但未能将这些任务统一在一个框架内。生成方法难以在不同实例间保持细粒度的身份一致性，而编辑方法则常失去空间可控性和指令对齐。

Method: 提出ReMix，一个统一的角色一致性生成和编辑框架，包含两个核心组件：1. **ReMix Module**：利用多模态大模型（MLLMs）编辑输入图像的语义特征，并将指令嵌入适配到DiT主干网络，无需微调，以确保语义布局连贯。2. **IP-ControlNet**：扩展ControlNet，解耦参考图像的语义和布局线索，引入一个$\epsilon$-等变的潜在空间，在共享噪声空间中联合去噪参考和目标图像，从而实现像素级一致性、姿态可控性，并在隐藏空间中促进特征对齐以保持身份。

Result: ReMix框架支持广泛的任务，包括个性化生成、图像编辑、风格迁移和多条件合成。实验结果验证了其作为统一框架在角色一致性图像生成和编辑方面的有效性和效率。

Conclusion: ReMix成功地将角色一致性图像生成和编辑统一在一个框架内，通过创新的模块设计解决了现有方法的局限性，并经广泛实验证明其有效性和效率。

Abstract: Recent advances in large-scale text-to-image diffusion models (e.g., FLUX.1)
have greatly improved visual fidelity in consistent character generation and
editing. However, existing methods rarely unify these tasks within a single
framework. Generation-based approaches struggle with fine-grained identity
consistency across instances, while editing-based methods often lose spatial
controllability and instruction alignment. To bridge this gap, we propose
ReMix, a unified framework for character-consistent generation and editing. It
constitutes two core components: the ReMix Module and IP-ControlNet. The ReMix
Module leverages the multimodal reasoning ability of MLLMs to edit semantic
features of input images and adapt instruction embeddings to the native DiT
backbone without fine-tuning. While this ensures coherent semantic layouts,
pixel-level consistency and pose controllability remain challenging. To address
this, IP-ControlNet extends ControlNet to decouple semantic and layout cues
from reference images and introduces an {\epsilon}-equivariant latent space
that jointly denoises the reference and target images within a shared noise
space. Inspired by convergent evolution and quantum decoherence,i.e., where
environmental noise drives state convergence, this design promotes feature
alignment in the hidden space, enabling consistent object generation while
preserving identity. ReMix supports a wide range of tasks, including
personalized generation, image editing, style transfer, and multi-condition
synthesis. Extensive experiments validate its effectiveness and efficiency as a
unified framework for character-consistent image generation and editing.

</details>


### [113] [SaFiRe: Saccade-Fixation Reiteration with Mamba for Referring Image Segmentation](https://arxiv.org/abs/2510.10160)
*Zhenjie Mao,Yuhuan Yang,Chaofan Ma,Dongsheng Jiang,Jiangchao Yao,Ya Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 针对指代图像分割(RIS)处理复杂模糊表达的局限性，本文提出SaFiRe框架，模仿人类两阶段认知过程，并利用Mamba实现高效细化。同时引入了aRefCOCO新基准。实验证明SaFiRe优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有RIS方法主要关注简单表达，将其简化为关键词匹配问题，导致模型难以处理指代模糊性。具体挑战包括涉及多个实体和上下文线索的“对象干扰表达”以及对象类别未明确的“类别隐式表达”。

Method: 提出SaFiRe框架，该框架模仿人类“先形成全局理解，再通过细节检查进行细化”的两阶段认知过程。SaFiRe利用Mamba的“扫描-更新”特性，支持分阶段设计并实现高效、线性复杂度的多周期细化。此外，引入了aRefCOCO新基准，用于评估RIS模型处理模糊指代表达的能力。

Result: 在标准数据集和新提出的aRefCOCO数据集上进行了广泛实验，结果表明SaFiRe的性能优于现有的最先进基线方法。

Conclusion: SaFiRe框架通过模拟人类两阶段认知过程和利用Mamba的特性，有效解决了RIS中复杂和模糊指代表达的挑战，并在新旧基准上均取得了领先的性能。同时，aRefCOCO的引入也为未来研究提供了评估工具。

Abstract: Referring Image Segmentation (RIS) aims to segment the target object in an
image given a natural language expression. While recent methods leverage
pre-trained vision backbones and more training corpus to achieve impressive
results, they predominantly focus on simple expressions--short, clear noun
phrases like "red car" or "left girl". This simplification often reduces RIS to
a key word/concept matching problem, limiting the model's ability to handle
referential ambiguity in expressions. In this work, we identify two challenging
real-world scenarios: object-distracting expressions, which involve multiple
entities with contextual cues, and category-implicit expressions, where the
object class is not explicitly stated. To address the challenges, we propose a
novel framework, SaFiRe, which mimics the human two-phase cognitive
process--first forming a global understanding, then refining it through
detail-oriented inspection. This is naturally supported by Mamba's
scan-then-update property, which aligns with our phased design and enables
efficient multi-cycle refinement with linear complexity. We further introduce
aRefCOCO, a new benchmark designed to evaluate RIS models under ambiguous
referring expressions. Extensive experiments on both standard and proposed
datasets demonstrate the superiority of SaFiRe over state-of-the-art baselines.

</details>


### [114] [SparseUWSeg: Active Sparse Point-Label Augmentation for Underwater Semantic Segmentation](https://arxiv.org/abs/2510.10163)
*César Borja,Carlos Plou,Rubén Martinez-Cantín,Ana C. Murillo*

Main category: cs.CV

TL;DR: 本文针对水下图像语义分割中高成本稠密标注的挑战，提出SparseUWSeg框架，通过主动采样策略指导稀疏点标注，并结合SAM2与超像素方法有效传播稀疏信息，显著提升分割性能，并发布了高效的交互式标注工具。


<details>
  <summary>Details</summary>
Motivation: 水下图像语义分割对生态监测至关重要，但高质量、专家标注的稠密标签成本极高，严重阻碍了该领域模型的发展。尽管稀疏点标注更容易获取，但如何有效选择标注点和传播稀疏信息仍是开放性难题。

Method: 本文提出了SparseUWSeg框架，通过主动采样策略指导标注者选择最有价值的稀疏点。随后，采用结合SAM2和超像素方法的混合策略来高效传播这些稀疏标签。此外，设计并发布了一个集成上述算法的交互式标注工具。

Result: 在两个多样化的水下数据集上进行的实验表明，SparseUWSeg优于现有最先进方法，相较于D+NN方法，mIoU性能提升高达5%。

Conclusion: SparseUWSeg提供了一个简单而有效的交互式标注工具和框架，使生态研究人员能够高效地利用基础模型和计算机视觉技术，生成高质量的分割掩膜来处理其数据。

Abstract: Semantic segmentation is essential to automate underwater imagery analysis
with ecology monitoring purposes. Unfortunately, fine grained underwater scene
analysis is still an open problem even for top performing segmentation models.
The high cost of obtaining dense, expert-annotated, segmentation labels hinders
the supervision of models in this domain. While sparse point-labels are easier
to obtain, they introduce challenges regarding which points to annotate and how
to propagate the sparse information. We present SparseUWSeg, a novel framework
that addresses both issues. SparseUWSeg employs an active sampling strategy to
guide annotators, maximizing the value of their point labels. Then, it
propagates these sparse labels with a hybrid approach leverages both the best
of SAM2 and superpixel-based methods. Experiments on two diverse underwater
datasets demonstrate the benefits of SparseUWSeg over state-of-the-art
approaches, achieving up to +5\% mIoU over D+NN. Our main contribution is the
design and release of a simple but effective interactive annotation tool,
integrating our algorithms. It enables ecology researchers to leverage
foundation models and computer vision to efficiently generate high-quality
segmentation masks to process their data.

</details>


### [115] [ViConEx-Med: Visual Concept Explainability via Multi-Concept Token Transformer for Medical Image Analysis](https://arxiv.org/abs/2510.10174)
*Cristiano Patrício,Luís F. Teixeira,João C. Neves*

Main category: cs.CV

TL;DR: 本文提出ViConEx-Med，一个基于Transformer的新型框架，通过多概念可学习Token实现视觉概念的联合预测与定位，特别适用于医疗领域，解决了现有概念模型缺乏视觉解释的问题，并取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于概念的模型将概念视为数值属性，缺乏提供预测概念的视觉定位解释，这限制了它们在真实世界，特别是医疗等高风险应用中的实用性。

Method: 提出ViConEx-Med，一个新颖的基于Transformer的视觉概念可解释性框架。它引入了多概念可学习Token，以联合预测和定位视觉概念。该方法利用专门的注意力层处理视觉和文本概念Token，生成概念级的定位图，同时保持高预测准确性。

Result: 在合成和真实医疗数据集上的实验表明，ViConEx-Med在概念检测和定位精度方面均优于先前的概念模型，并与黑盒模型表现出竞争力。

Conclusion: 研究结果表明，ViConEx-Med为构建基于视觉概念的固有可解释模型提供了一个有前景的方向。

Abstract: Concept-based models aim to explain model decisions with human-understandable
concepts. However, most existing approaches treat concepts as numerical
attributes, without providing complementary visual explanations that could
localize the predicted concepts. This limits their utility in real-world
applications and particularly in high-stakes scenarios, such as medical
use-cases. This paper proposes ViConEx-Med, a novel transformer-based framework
for visual concept explainability, which introduces multi-concept learnable
tokens to jointly predict and localize visual concepts. By leveraging
specialized attention layers for processing visual and text-based concept
tokens, our method produces concept-level localization maps while maintaining
high predictive accuracy. Experiments on both synthetic and real-world medical
datasets demonstrate that ViConEx-Med outperforms prior concept-based models
and achieves competitive performance with black-box models in terms of both
concept detection and localization precision. Our results suggest a promising
direction for building inherently interpretable models grounded in visual
concepts. Code is publicly available at
https://github.com/CristianoPatricio/viconex-med.

</details>


### [116] [HccePose(BF): Predicting Front \& Back Surfaces to Construct Ultra-Dense 2D-3D Correspondences for Pose Estimation](https://arxiv.org/abs/2510.10177)
*Yulin Wang,Mengting Hu,Hongli Li,Chen Luo*

Main category: cs.CV

TL;DR: 本研究针对已知物体姿态估计，提出了一种新方法。通过预测物体前后表面及内部的3D坐标，结合分层连续坐标编码（HCCE）技术，生成超密集2D-3D对应关系，显著提升了基于PnP算法的姿态估计精度，并在BOP数据集上超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有物体姿态估计方法主要通过神经网络预测物体前表面的密集3D坐标，但忽略了利用物体后表面和内部信息的潜在优势，这可能限制了姿态估计的精度。

Method: 该研究通过预测物体的前后表面3D坐标，并在它们之间密集采样3D坐标，以建立超密集的2D-3D对应关系，从而增强基于Perspective-n-Point（PnP）算法的姿态估计精度。此外，提出了分层连续坐标编码（HCCE）技术，以更准确、高效地表示前后表面坐标。

Result: 实验结果表明，与BOP网站上现有最先进（SOTA）方法相比，所提出的方法在七个经典的BOP核心数据集上均表现出更优的性能。

Conclusion: 充分利用物体的前后表面及内部的3D坐标信息，并通过创新编码技术（HCCE）建立超密集2D-3D对应关系，能够显著提升已知物体姿态估计的准确性。

Abstract: In pose estimation for seen objects, a prevalent pipeline involves using
neural networks to predict dense 3D coordinates of the object surface on 2D
images, which are then used to establish dense 2D-3D correspondences. However,
current methods primarily focus on more efficient encoding techniques to
improve the precision of predicted 3D coordinates on the object's front
surface, overlooking the potential benefits of incorporating the back surface
and interior of the object. To better utilize the full surface and interior of
the object, this study predicts 3D coordinates of both the object's front and
back surfaces and densely samples 3D coordinates between them. This process
creates ultra-dense 2D-3D correspondences, effectively enhancing pose
estimation accuracy based on the Perspective-n-Point (PnP) algorithm.
Additionally, we propose Hierarchical Continuous Coordinate Encoding (HCCE) to
provide a more accurate and efficient representation of front and back surface
coordinates. Experimental results show that, compared to existing
state-of-the-art (SOTA) methods on the BOP website, the proposed approach
outperforms across seven classic BOP core datasets. Code is available at
https://github.com/WangYuLin-SEU/HCCEPose.

</details>


### [117] [TCMA: Text-Conditioned Multi-granularity Alignment for Drone Cross-Modal Text-Video Retrieval](https://arxiv.org/abs/2510.10180)
*Zixu Zhao,Yang Zhan*

Main category: cs.CV

TL;DR: 本文构建了首个细粒度无人机视频-文本匹配数据集DVTMD，并提出了TCMA框架，通过多粒度对齐和精炼模块，在无人机文本-视频检索任务上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 无人机视频数据量巨大，但现有数据集在无人机领域存在标注粗糙和冗余等问题，导致该领域的文本-视频检索效率低下，阻碍了城市管理、应急响应等应用。

Method: 构建了包含2,864个视频和14,320个细粒度、语义多样化标注的Drone Video-Text Match Dataset (DVTMD)。提出了Text-Conditioned Multi-granularity Alignment (TCMA) 框架，整合了全局视频-句子对齐、句子引导的帧聚合和单词引导的补丁对齐。为优化局部对齐，设计了单词和补丁选择模块以及文本自适应动态温度机制。

Result: 在DVTMD和CapERA数据集上建立了首个完整的无人机文本-视频检索基准。TCMA框架取得了最先进的性能，包括文本到视频检索45.5%的R@1和视频到文本检索42.8%的R@1。

Conclusion: 所构建的DVTMD数据集和提出的TCMA方法在无人机文本-视频检索任务中表现出卓越的有效性，显著推动了该领域的发展。

Abstract: Unmanned aerial vehicles (UAVs) have become powerful platforms for real-time,
high-resolution data collection, producing massive volumes of aerial videos.
Efficient retrieval of relevant content from these videos is crucial for
applications in urban management, emergency response, security, and disaster
relief. While text-video retrieval has advanced in natural video domains, the
UAV domain remains underexplored due to limitations in existing datasets, such
as coarse and redundant captions. Thus, in this work, we construct the Drone
Video-Text Match Dataset (DVTMD), which contains 2,864 videos and 14,320
fine-grained, semantically diverse captions. The annotations capture multiple
complementary aspects, including human actions, objects, background settings,
environmental conditions, and visual style, thereby enhancing text-video
correspondence and reducing redundancy. Building on this dataset, we propose
the Text-Conditioned Multi-granularity Alignment (TCMA) framework, which
integrates global video-sentence alignment, sentence-guided frame aggregation,
and word-guided patch alignment. To further refine local alignment, we design a
Word and Patch Selection module that filters irrelevant content, as well as a
Text-Adaptive Dynamic Temperature Mechanism that adapts attention sharpness to
text type. Extensive experiments on DVTMD and CapERA establish the first
complete benchmark for drone text-video retrieval. Our TCMA achieves
state-of-the-art performance, including 45.5% R@1 in text-to-video and 42.8%
R@1 in video-to-text retrieval, demonstrating the effectiveness of our dataset
and method. The code and dataset will be released.

</details>


### [118] [Fairness Without Labels: Pseudo-Balancing for Bias Mitigation in Face Gender Classification](https://arxiv.org/abs/2510.10191)
*Haohua Dong,Ana Manzano Rodríguez,Camille Guinaudeau,Shin'ichi Satoh*

Main category: cs.CV

TL;DR: 本文提出“伪平衡”策略，在半监督学习中通过伪标签选择强制人口统计学平衡，有效减轻人脸性别分类模型中的偏见，提升公平性和准确性。


<details>
  <summary>Details</summary>
Motivation: 人脸性别分类模型常反映并放大训练数据中的人口统计学偏见，导致在不同性别和种族亚群间性能不均。

Method: 引入“伪平衡”策略，在半监督学习中，仅使用来自种族平衡数据集的无标签图像，在伪标签选择过程中强制执行人口统计学平衡，无需真实标签。

Result: 伪平衡策略持续提升了公平性并保持或增强了准确性。模型整体准确率达79.81%（较基线提升6.53%），性别准确率差距减少44.17%。在东亚亚群中，将基线超49%的差距缩小至5.01%。

Conclusion: 研究表明，即使没有标签监督，访问人口统计学平衡或适度倾斜的无标签数据集也可作为去偏现有计算机视觉模型的有力资源。

Abstract: Face gender classification models often reflect and amplify demographic
biases present in their training data, leading to uneven performance across
gender and racial subgroups. We introduce pseudo-balancing, a simple and
effective strategy for mitigating such biases in semi-supervised learning. Our
method enforces demographic balance during pseudo-label selection, using only
unlabeled images from a race-balanced dataset without requiring access to
ground-truth annotations.
  We evaluate pseudo-balancing under two conditions: (1) fine-tuning a biased
gender classifier using unlabeled images from the FairFace dataset, and (2)
stress-testing the method with intentionally imbalanced training data to
simulate controlled bias scenarios. In both cases, models are evaluated on the
All-Age-Faces (AAF) benchmark, which contains a predominantly East Asian
population. Our results show that pseudo-balancing consistently improves
fairness while preserving or enhancing accuracy. The method achieves 79.81%
overall accuracy - a 6.53% improvement over the baseline - and reduces the
gender accuracy gap by 44.17%. In the East Asian subgroup, where baseline
disparities exceeded 49%, the gap is narrowed to just 5.01%. These findings
suggest that even in the absence of label supervision, access to a
demographically balanced or moderately skewed unlabeled dataset can serve as a
powerful resource for debiasing existing computer vision models.

</details>


### [119] [B2N3D: Progressive Learning from Binary to N-ary Relationships for 3D Object Grounding](https://arxiv.org/abs/2510.10194)
*Feng Xiao,Hongbin Xu,Hai Ci,Wenxiong Kang*

Main category: cs.CV

TL;DR: 本文提出一种渐进式n元关系学习框架，用于3D物体接地，通过建模全局n元关系和分组监督损失，显著提升了3D语言定位性能，超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 3D物体语言定位面临多空间关系对齐难题，现有方法仅关注二元关系，忽略了多模态关系理解中n元组合的全局感知意义，导致3D-语言对齐困难。

Method: 提出渐进式n元关系学习框架，将关系学习从二元扩展到n元。设计分组监督损失以促进n元关系学习。在n元关系构建的场景图中，使用带混合注意力机制的多模态网络进一步定位目标。

Result: 在ReferIt3D和ScanRefer基准测试中，该方法超越了现有SOTA，并证明了n元关系感知在3D定位中的优势。

Conclusion: n元关系感知在3D物体定位中具有显著优势，本文提出的渐进式n元关系学习框架有效解决了多关系对齐问题，提高了3D物体接地性能。

Abstract: Localizing 3D objects using natural language is essential for robotic scene
understanding. The descriptions often involve multiple spatial relationships to
distinguish similar objects, making 3D-language alignment difficult. Current
methods only model relationships for pairwise objects, ignoring the global
perceptual significance of n-ary combinations in multi-modal relational
understanding. To address this, we propose a novel progressive relational
learning framework for 3D object grounding. We extend relational learning from
binary to n-ary to identify visual relations that match the referential
description globally. Given the absence of specific annotations for referred
objects in the training data, we design a grouped supervision loss to
facilitate n-ary relational learning. In the scene graph created with n-ary
relationships, we use a multi-modal network with hybrid attention mechanisms to
further localize the target within the n-ary combinations. Experiments and
ablation studies on the ReferIt3D and ScanRefer benchmarks demonstrate that our
method outperforms the state-of-the-art, and proves the advantages of the n-ary
relational perception in 3D localization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [120] [The Geometry of Reasoning: Flowing Logics in Representation Space](https://arxiv.org/abs/2510.09782)
*Yufa Zhou,Yixiao Wang,Xunjian Yin,Shuyan Zhou,Anru R. Zhang*

Main category: cs.AI

TL;DR: 本研究提出一种几何框架，将大型语言模型（LLM）的推理建模为表征空间中的流，并解耦逻辑与语义，以深入理解LLM的推理机制。


<details>
  <summary>Details</summary>
Motivation: 研究LLM如何通过其表征空间进行“思考”和推理，并探究LLM是否内化了超越表面形式的逻辑结构。

Method: 提出一个新颖的几何框架，将LLM的推理建模为表征空间中演变的“流”（嵌入轨迹），并将其与位置、速度、曲率等几何量关联。通过使用具有不同语义载体的相同自然演绎命题，将逻辑结构与语义解耦。设计受控实验，使用学习到的表征代理来可视化和量化推理流。

Result: 理论上确立了：1) LLM推理对应于表征空间中的平滑流；2) 逻辑语句作为这些流速度的局部控制器。通过实验对理论框架进行了实证验证。

Conclusion: 该工作为研究推理现象提供了概念基础和实用工具，为LLM行为的可解释性和形式化分析提供了新视角。

Abstract: We study how large language models (LLMs) ``think'' through their
representation space. We propose a novel geometric framework that models an
LLM's reasoning as flows -- embedding trajectories evolving where logic goes.
We disentangle logical structure from semantics by employing the same natural
deduction propositions with varied semantic carriers, allowing us to test
whether LLMs internalize logic beyond surface form. This perspective connects
reasoning with geometric quantities such as position, velocity, and curvature,
enabling formal analysis in representation and concept spaces. Our theory
establishes: (1) LLM reasoning corresponds to smooth flows in representation
space, and (2) logical statements act as local controllers of these flows'
velocities. Using learned representation proxies, we design controlled
experiments to visualize and quantify reasoning flows, providing empirical
validation of our theoretical framework. Our work serves as both a conceptual
foundation and practical tools for studying reasoning phenomenon, offering a
new lens for interpretability and formal analysis of LLMs' behavior.

</details>


### [121] [How can we assess human-agent interactions? Case studies in software agent design](https://arxiv.org/abs/2510.09801)
*Valerie Chen,Rohit Malhotra,Xingyao Wang,Juan Michelini,Xuhui Zhou,Aditya Bharat Soni,Hoang H. Tran,Calvin Smith,Ameet Talwalkar,Graham Neubig*

Main category: cs.AI

TL;DR: 现有LLM代理评估缺乏人机协作考量。本文提出PULSE框架，通过大规模用户数据评估代理设计对满意度的影响，发现真实使用与基准测试结果存在显著差异，并为代理设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理基准测试多假设完全自动化，未能充分代表真实世界中人机协作的本质，导致对代理实际有用性的评估不足。

Method: ['提出PULSE框架，用于高效、以人为中心的代理设计评估。该框架包含收集用户反馈、训练机器学习模型预测用户满意度，并结合人工评分与模型伪标签计算结果。', '将PULSE框架部署在基于开源软件代理OpenHands的大规模网络平台，收集了超过1.5万用户的实际使用数据。', '针对LLM骨干模型选择、规划策略和记忆机制等代理设计决策，进行了案例研究，以评估它们对开发者满意度的影响。']

Result: ['PULSE框架能得出更稳健的代理设计结论，与标准A/B测试相比，置信区间减少了40%。', '在真实世界使用结果与基准测试性能之间发现显著差异（例如，Claude-Sonnet-4和GPT-5的结果呈反相关），突显了基准测试驱动评估的局限性。', '为软件代理设计提供了实用见解。']

Conclusion: 本研究的发现为LLM代理的人机评估提供了指导，并指明了改进代理设计的机会，强调了以人为中心评估的重要性。

Abstract: LLM-powered agents are both a promising new technology and a source of
complexity, where choices about models, tools, and prompting can affect their
usefulness. While numerous benchmarks measure agent accuracy across domains,
they mostly assume full automation, failing to represent the collaborative
nature of real-world use cases. In this paper, we make two major steps towards
the rigorous assessment of human-agent interactions. First, we propose PULSE, a
framework for more efficient human-centric evaluation of agent designs, which
comprises collecting user feedback, training an ML model to predict user
satisfaction, and computing results by combining human satisfaction ratings
with model-generated pseudo-labels. Second, we deploy the framework on a
large-scale web platform built around the open-source software agent OpenHands,
collecting in-the-wild usage data across over 15k users. We conduct case
studies around how three agent design decisions -- choice of LLM backbone,
planning strategy, and memory mechanisms -- impact developer satisfaction
rates, yielding practical insights for software agent design. We also show how
our framework can lead to more robust conclusions about agent design, reducing
confidence intervals by 40\% compared to a standard A/B test. Finally, we find
substantial discrepancies between in-the-wild results and benchmark performance
(e.g., the anti-correlation between results comparing claude-sonnet-4 and
gpt-5), underscoring the limitations of benchmark-driven evaluation. Our
findings provide guidance for evaluations of LLM agents with humans and
identify opportunities for better agent designs.

</details>


### [122] [AI and Consciousness](https://arxiv.org/abs/2510.09858)
*Eric Schwitzgebel*

Main category: cs.AI

TL;DR: 本文是对AI意识文献的怀疑性综述，指出我们很快将面临一个困境：AI系统将根据一些主流理论被认为是意识的，而根据另一些理论则不是，导致我们无法确定AI是否真正具有意识。


<details>
  <summary>Details</summary>
Motivation: 由于AI技术发展迅速，很快将出现符合某些意识理论但又不符合其他理论的AI系统。作者旨在批判性地审视现有关于AI意识的文献和论证，指出它们不足以解决这一即将到来的、深刻的哲学困境。

Method: 通过哲学和概念分析，本文对AI意识领域的各种主流理论、思想实验、论证和哲学立场进行了全面且批判性的回顾，包括意识的可能关键特征、唯物主义与功能主义、图灵测试与中文房间、全球工作空间理论、高阶理论、整合信息理论等，并探讨了生物基质和奇异智能等问题。

Result: 研究结果表明，尽管存在许多关于意识的理论和论证，但它们在AI意识问题上存在根本性冲突。我们无法确定未来AI系统是像人类一样具有丰富意识，还是像烤面包机一样缺乏体验，因为现有论证都无法提供确凿的证据或解决方案。

Conclusion: 关于AI意识的困境是深刻且无法解决的：我们将无法得知高级AI系统是具有丰富意识还是毫无体验。鉴于现有理论和论证的局限性，我们仍处于无法判断AI是否真正拥有意识的境地。

Abstract: This is a skeptical overview of the literature on AI consciousness. We will
soon create AI systems that are conscious according to some influential,
mainstream theories of consciousness but are not conscious according to other
influential, mainstream theories of consciousness. We will not be in a position
to know which theories are correct and whether we are surrounded by AI systems
as richly and meaningfully conscious as human beings or instead only by systems
as experientially blank as toasters. None of the standard arguments either for
or against AI consciousness takes us far.
  Table of Contents
  Chapter One: Hills and Fog
  Chapter Two: What Is Consciousness? What Is AI?
  Chapter Three: Ten Possibly Essential Features of Consciousness
  Chapter Four: Against Introspective and Conceptual Arguments for Essential
Features
  Chapter Five: Materialism and Functionalism
  Chapter Six: The Turing Test and the Chinese Room
  Chapter Seven: The Mimicry Argument Against AI Consciousness
  Chapter Eight: Global Workspace Theories and Higher Order Theories
  Chapter Nine: Integrated Information, Local Recurrence, Associative Learning,
and Iterative Natural Kinds
  Chapter Ten: Does Biological Substrate Matter?
  Chapter Eleven: The Problem of Strange Intelligence
  Chapter Twelve: The Leapfrog Hypothesis and the Social Semi-Solution

</details>


### [123] [Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning](https://arxiv.org/abs/2510.09894)
*Junyuan Liu,Quan Qin,Guangsheng Dong,Xinglei Wang,Jiazhuang Feng,Zichao Zeng,Tao Cheng*

Main category: cs.AI

TL;DR: AETHER通过多模态对齐POI数据，增强了AlphaEarth在城市功能和社经维度上的表示能力，实现了高效且可扩展的城市分析。


<details>
  <summary>Details</summary>
Motivation: 现有基于地球观测（EO）数据的空间表示，如AlphaEarth，在捕捉城市功能和社会经济维度方面存在局限，主要编码物理和光谱模式，而非人类活动或空间功能。

Method: 提出AETHER框架，通过兴趣点（POI）引导的多模态对齐，将AlphaEarth的EO嵌入与POI的文本表示对齐，从而用人类中心语义丰富物理EO特征，以实现以人为本的城市分析。

Result: 在伦敦地区，AETHER相较于AlphaEarth基线，在土地利用分类F1得分上相对提升7.2%，在社会经济制图的Kullback-Leibler散度上相对减少23.6%。

Conclusion: AETHER通过结合地球观测数据与人类中心语义，推动了地理空间基础模型向整合物理形态和功能意义的通用城市表示发展，同时保持了计算效率和可扩展性。

Abstract: General-purpose spatial representations are essential for building
transferable geospatial foundation models (GFMs). Among them, the AlphaEarth
Foundation (AE) represents a major step toward a global, unified representation
of the Earth's surface, learning 10-meter embeddings from multi-source Earth
Observation (EO) data that capture rich physical and environmental patterns
across diverse landscapes. However, such EO-driven representations remain
limited in capturing the functional and socioeconomic dimensions of cities, as
they primarily encode physical and spectral patterns rather than human
activities or spatial functions. We propose AETHER (AlphaEarth-POI Enriched
Representation Learning), a lightweight framework that adapts AlphaEarth to
human-centered urban analysis through multimodal alignment guided by Points of
Interest (POIs). AETHER aligns AE embeddings with textual representations of
POIs, enriching physically grounded EO features with semantic cues about urban
functions and socioeconomic contexts. In Greater London, AETHER achieves
consistent gains over the AE baseline, with a 7.2% relative improvement in
land-use classification F1 and a 23.6% relative reduction in Kullback-Leibler
divergence for socioeconomic mapping. Built upon pretrained AE, AETHER
leverages a lightweight multimodal alignment to enrich it with human-centered
semantics while remaining computationally efficient and scalable for urban
applications. By coupling EO with human-centered semantics, it advances
geospatial foundation models toward general-purpose urban representations that
integrate both physical form and functional meaning.

</details>


### [124] [Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics](https://arxiv.org/abs/2510.09901)
*Lianhao Zhou,Hongyi Ling,Cong Fu,Yepeng Huang,Michael Sun,Wendi Yu,Xiaoxuan Wang,Xiner Li,Xingyu Su,Junkai Zhang,Xiusi Chen,Chenxing Liang,Xiaofeng Qian,Heng Ji,Wei Wang,Marinka Zitnik,Shuiwang Ji*

Main category: cs.AI

TL;DR: 本文探讨了LLM驱动的科学智能体如何通过加速假设、实验和分析来改变科学发现，并审视了其现状、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的兴起，自主智能体正在改变科学发现范式。本文旨在提出对LLM科学智能体的看法和愿景，阐述其在科学发现生命周期中的变革作用和潜力。

Method: 本文对当前基于LLM的科学智能体方法进行了批判性审视，分析了其关键创新、实际成就和显著局限性，并识别了开放研究挑战，概述了未来发展方向。

Result: 研究表明，LLM科学智能体在加速科学发现生命周期（从假设发现、实验设计与执行到结果分析和优化）中展现出日益增长的变革作用和巨大潜力。

Conclusion: LLM驱动的自主智能体具有加速跨领域科学发现的巨大变革潜力。未来需致力于构建更鲁棒、通用和适应性强的科学智能体以应对挑战。

Abstract: Computing has long served as a cornerstone of scientific discovery. Recently,
a paradigm shift has emerged with the rise of large language models (LLMs),
introducing autonomous systems, referred to as agents, that accelerate
discovery across varying levels of autonomy. These language agents provide a
flexible and versatile framework that orchestrates interactions with human
scientists, natural language, computer language and code, and physics. This
paper presents our view and vision of LLM-based scientific agents and their
growing role in transforming the scientific discovery lifecycle, from
hypothesis discovery, experimental design and execution, to result analysis and
refinement. We critically examine current methodologies, emphasizing key
innovations, practical achievements, and outstanding limitations. Additionally,
we identify open research challenges and outline promising directions for
building more robust, generalizable, and adaptive scientific agents. Our
analysis highlights the transformative potential of autonomous agents to
accelerate scientific discovery across diverse domains.

</details>


### [125] [The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs](https://arxiv.org/abs/2510.09905)
*Xi Fang,Weijie Xu,Yuchong Zhang,Stephanie Eckman,Scott Nickleach,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 研究发现，个性化记忆会使大型语言模型(LLMs)在情绪解读上产生系统性偏差，优势用户档案获得更准确的情绪判断，表明个性化AI可能无意中加剧社会不平等。


<details>
  <summary>Details</summary>
Motivation: 随着个性化AI系统日益整合长期用户记忆，理解这种记忆如何影响AI的情绪推理至关重要。

Method: 通过在人类验证过的情商测试上评估15个大型语言模型(LLMs)，来调查用户记忆如何影响它们的情商。

Result: 1. 相同情景与不同用户档案配对时，LLMs会产生系统性差异的情绪解读。
2. 在多个高性能LLMs中，优势用户档案获得更准确的情绪解读，存在系统性偏差。
3. LLMs在情绪理解和支持性建议任务中表现出显著的人口统计学差异，表明个性化机制可能将社会等级嵌入模型的情绪推理。

Conclusion: 记忆增强型AI面临一个关键挑战：旨在个性化的系统可能会无意中强化社会不平等。

Abstract: When an AI assistant remembers that Sarah is a single mother working two
jobs, does it interpret her stress differently than if she were a wealthy
executive? As personalized AI systems increasingly incorporate long-term user
memory, understanding how this memory shapes emotional reasoning is critical.
We investigate how user memory affects emotional intelligence in large language
models (LLMs) by evaluating 15 models on human validated emotional intelligence
tests. We find that identical scenarios paired with different user profiles
produce systematically divergent emotional interpretations. Across validated
user independent emotional scenarios and diverse user profiles, systematic
biases emerged in several high-performing LLMs where advantaged profiles
received more accurate emotional interpretations. Moreover, LLMs demonstrate
significant disparities across demographic factors in emotion understanding and
supportive recommendations tasks, indicating that personalization mechanisms
can embed social hierarchies into models emotional reasoning. These results
highlight a key challenge for memory enhanced AI: systems designed for
personalization may inadvertently reinforce social inequalities.

</details>


### [126] [Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs](https://arxiv.org/abs/2510.09970)
*Olivia Peiyu Wang,Tashvi Bansal,Ryan Bai,Emily M. Chui,Leilani H. Gilpin*

Main category: cs.AI

TL;DR: 本文提出一种低成本、基于指令的干预方法，通过逐步分解和知识图谱验证，显著提升了大型语言模型（LLMs）在逻辑谬误分类上的准确性，并增强了决策透明度。


<details>
  <summary>Details</summary>
Motivation: LLMs在逻辑谬误分类等推理任务上存在关键缺陷，这源于其默认的直觉式（System 1）处理。由于全面的System 2训练成本过高，研究旨在探索低成本的指令式干预措施来弥补这一推理鸿沟。

Method: 研究引入了一个新颖的逐步指令数据集，将谬误分类任务分解为一系列原子程序步骤（简单的二元问题）。此外，模型还通过查阅相关谬误的关系知识图谱进行最终验证。

Result: 这种过程性、基于规则的干预措施显著提高了LLMs在逻辑谬误分类上的表现。同时，该方法还增强了LLMs决策过程的透明度。

Conclusion: 该研究为解决LLMs的推理缺陷提供了一条实用的神经符号架构路径，通过低成本的指令干预实现了性能提升和决策透明度的增强。

Abstract: Large Language Models (LLMs) suffer from critical reasoning gaps, including a
tendency to hallucinate and poor accuracy in classifying logical fallacies.
This limitation stems from their default System 1 processing, which is fast and
intuitive, whereas reliable reasoning requires the deliberate, effortful System
2 approach (Kahneman, 2011; Li et al., 2025). Since full System 2 training is
often prohibitively expensive, we explore a low-cost, instruction-based
intervention to bridge this gap. Our methodology introduces a novel stepwise
instruction dataset that decomposes fallacy classification into a series of
atomic procedural steps (simple binary questions). We further augment this with
a final verification step where models consult a relational knowledge graph of
related fallacies. This procedural, rule-based intervention yields a
significant improvement in LLM logical fallacy classification. Crucially, the
approach also provides enhanced transparency into the LLMs' decision-making,
highlighting a practical pathway for Neuro-symbolic architectures to address
LLM reasoning deficits.

</details>


### [127] [Deliberative Dynamics and Value Alignment in LLM Debates](https://arxiv.org/abs/2510.10002)
*Pratik S. Sachdeva,Tom van Nuenen*

Main category: cs.AI

TL;DR: 研究发现，在多轮对话中，大型语言模型（LLMs）的道德推理和价值观对齐受其模型行为和对话格式的显著影响，揭示了不同模型在道德判断和修正上的差异。


<details>
  <summary>Details</summary>
Motivation: LLMs在敏感日常场景中的应用日益增多，理解其在复杂道德推理中的价值观至关重要。现有评估多集中于单轮提示，但LLMs在多轮对话中通过互动、修正和共识形成的价值观及其对齐情况尚不明确。

Method: 通过LLM辩论，让GPT-4.1、Claude 3.7 Sonnet和Gemini 2.0 Flash三个模型子集，在Reddit“我是否是混蛋”社区的1,000个日常困境中共同裁决责任。采用同步（并行响应）和轮流（顺序响应）两种对话格式，以测试顺序效应和裁决修正。

Result: 模型行为存在显著差异：同步设置下，GPT修正率低（0.6-3.1%），而Claude和Gemini更灵活（28-41%）。价值观模式也不同：GPT强调个人自主和直接沟通，Claude和Gemini侧重共情对话。某些价值观能有效驱动裁决改变。辩论格式对模型行为影响很大：GPT和Gemini比Claude更易顺从，其裁决行为受顺序效应强烈影响。

Conclusion: 审议格式和模型特定行为共同塑造了多轮互动中的道德推理。社会技术对齐不仅取决于模型的输出，更取决于系统如何构建对话结构。

Abstract: As large language models (LLMs) are increasingly deployed in sensitive
everyday contexts - offering personal advice, mental health support, and moral
guidance - understanding their elicited values in navigating complex moral
reasoning is essential. Most evaluations study this sociotechnical alignment
through single-turn prompts, but it is unclear if these findings extend to
multi-turn settings where values emerge through dialogue, revision, and
consensus. We address this gap using LLM debate to examine deliberative
dynamics and value alignment in multi-turn settings by prompting subsets of
three models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively
assign blame in 1,000 everyday dilemmas from Reddit's "Am I the Asshole"
community. We use both synchronous (parallel responses) and round-robin
(sequential responses) formats to test order effects and verdict revision. Our
findings show striking behavioral differences. In the synchronous setting, GPT
showed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were
far more flexible (28-41%). Value patterns also diverged: GPT emphasized
personal autonomy and direct communication, while Claude and Gemini prioritized
empathetic dialogue. Certain values proved especially effective at driving
verdict changes. We further find that deliberation format had a strong impact
on model behavior: GPT and Gemini stood out as highly conforming relative to
Claude, with their verdict behavior strongly shaped by order effects. These
results show how deliberation format and model-specific behaviors shape moral
reasoning in multi-turn interactions, underscoring that sociotechnical
alignment depends on how systems structure dialogue as much as on their
outputs.

</details>


### [128] [RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning](https://arxiv.org/abs/2510.10008)
*Meng Xi,Sihan Lv,Yechen Jin,Guanjie Cheng,Naibo Wang,Ying Li,Jianwei Yin*

Main category: cs.AI

TL;DR: 本文提出RIPRAG，一个基于强化学习的黑盒投毒攻击框架，可有效攻击复杂RAG系统，揭示现有防御机制的不足。


<details>
  <summary>Details</summary>
Motivation: RAG系统已成为问答和内容生成的关键技术，但易受投毒攻击操纵LLM生成恶意内容。现有研究主要集中在白盒攻击和简化RAG架构，缺乏针对更复杂、更真实黑盒RAG系统的攻击研究。

Method: 提出RIPRAG端到端攻击框架，将目标RAG系统视为黑盒，仅依赖投毒成功与否的信息。该方法利用强化学习优化投毒文档的生成模型，以使其与目标RAG系统的偏好对齐。

Result: 实验结果表明，RIPRAG能有效对大多数复杂RAG系统执行投毒攻击，攻击成功率 (ASR) 比基线方法提高了0.72。这凸显了当前防御方法的普遍缺陷。

Conclusion: 当前针对RAG系统的防御方法存在普遍不足，RIPRAG攻击框架为LLM安全研究提供了关键见解和挑战。

Abstract: Retrieval-Augmented Generation (RAG) systems based on Large Language Models
(LLMs) have become a core technology for tasks such as question-answering (QA)
and content generation. However, by injecting poisoned documents into the
database of RAG systems, attackers can manipulate LLMs to generate text that
aligns with their intended preferences. Existing research has primarily focused
on white-box attacks against simplified RAG architectures. In this paper, we
investigate a more complex and realistic scenario: the attacker lacks knowledge
of the RAG system's internal composition and implementation details, and the
RAG system comprises components beyond a mere retriever. Specifically, we
propose the RIPRAG attack framework, an end-to-end attack pipeline that treats
the target RAG system as a black box, where the only information accessible to
the attacker is whether the poisoning succeeds. Our method leverages
Reinforcement Learning (RL) to optimize the generation model for poisoned
documents, ensuring that the generated poisoned document aligns with the target
RAG system's preferences. Experimental results demonstrate that this method can
effectively execute poisoning attacks against most complex RAG systems,
achieving an attack success rate (ASR) improvement of up to 0.72 compared to
baseline methods. This highlights prevalent deficiencies in current defensive
methods and provides critical insights for LLM security research.

</details>


### [129] [Failure-Driven Workflow Refinement](https://arxiv.org/abs/2510.10035)
*Jusheng Zhang,Kaitong Cai,Qinglin Zeng,Ningyuan Liu,Stephen Fan,Ziliang Chen,Keze Wang*

Main category: cs.AI

TL;DR: 现有LLM工作流优化存在信息坍塌问题。本文提出CE-Graph框架，通过在失败签名空间中最小化预期失败质量，实现失败驱动的、基于图编辑的鲁棒性优化，在多项任务上取得了更高鲁棒性和更低成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工作流优化方法将多步骤执行轨迹简化为标量成功/失败信号，导致信息坍塌，无法理解失败的底层结构和分布，阻碍了对工作流失败模式的深入理解和优化。

Method: 将优化目标重构为分布问题，旨在最小化工作流在“失败签名空间”（FSS）中的“预期失败质量”。提出CE-Graph框架，通过反例池近似失败分布，识别重复失败模式，并利用提议-验证机制进行操作符受限的图编辑，以贪婪地减少失败质量，实现“类梯度”下降。

Result: 在数学、代码和问答（QA）基准测试中，CE-Graph框架相比强大的基线方法，以显著更低的成本实现了更高的鲁棒性。

Conclusion: 系统的可靠性源于系统地学习和重塑其失败分布的几何结构，而非仅仅避免失败。本文提出的新范式有效提升了LLM工作流的鲁棒性。

Abstract: Optimizing LLM-based workflows is typically formulated as a global search,
where candidate workflows are evaluated based on a scalar metric. This
paradigm, however, suffers from a critical flaw: information collapse. By
reducing rich, multi-step execution traces to simple success/failure signals,
existing methods are rendered blind to the underlying structure of failures,
fundamentally preventing them from modeling the workflow's failure
distribution. We reconceptualize this challenge as a distributional problem. We
propose a new paradigm where the optimization goal is not to maximize a scalar
score, but to directly minimize a workflow's Expected Failure Mass, i.e., the
integral of its failure probability density function defined over a
high-dimensional Failure Signature Space (FSS). This distributional lens allows
us to move from inefficient, zero-order optimization to a principled,
gradient-like descent on the failure landscape itself. We introduce CE-Graph, a
framework that operationalizes this paradigm through a novel, failure-driven
refinement process. CE-Graph approximates the failure distribution from a pool
of counterexamples, identifies its densest regions as recurring failure modes,
and applies targeted, operator-constrained graph edits via a Propose-and-Verify
mechanism to greedily reduce the failure mass. On math, code, and QA
benchmarks, our CE-Graph achieves higher robustness at a significantly lower
cost than strong baselines. This suggests that a system's reliability emerges
not from avoiding failures, but from systematically learning and reshaping the
geometric structure of its failure distributions.

</details>


### [130] [Belief Graphs with Reasoning Zones: Structure, Dynamics, and Epistemic Activation](https://arxiv.org/abs/2510.10042)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.AI

TL;DR: 提出一个图论框架，通过信心传播在全局矛盾的信仰系统中识别局部一致的推理区，并允许局部信仰更新而保持系统稳定。


<details>
  <summary>Details</summary>
Motivation: 信仰系统很少全局一致，但局部推理往往有效。需要一种能在存在全局矛盾的情况下，实现容错推理并识别局部有效推理区域的框架。

Method: 构建有向、带符号、带权重的图来表示信仰。区分外部“可信度”和由网络结构诱导的内部“信心”，通过收缩传播过程计算信心值。定义“推理区”为高信心、结构平衡的子图，并提出近线性过程来识别它们。引入“冲击更新”机制来模型化信仰变化，确保局部重构而不会破坏整体稳定性。

Result: 成功识别出高信心、结构平衡的“推理区”，即使在全局矛盾下也能安全进行经典推理。信仰变化通过局部重构（如区域收缩、分裂或崩溃）而非整体失稳来实现。通过实验协议验证了区域恢复、冲击下的稳定性和运行时性能。

Conclusion: 为容忍矛盾的推理提供了一个有原则的基础，能精确地在图结构支持的区域激活经典逻辑推理。

Abstract: Belief systems are rarely globally consistent, yet effective reasoning often
persists locally. We propose a novel graph-theoretic framework that cleanly
separates credibility--external, a priori trust in sources--from confidence--an
internal, emergent valuation induced by network structure. Beliefs are nodes in
a directed, signed, weighted graph whose edges encode support and
contradiction. Confidence is obtained by a contractive propagation process that
mixes a stated prior with structure-aware influence and guarantees a unique,
stable solution. Within this dynamics, we define reasoning zones:
high-confidence, structurally balanced subgraphs on which classical inference
is safe despite global contradictions. We provide a near-linear procedure that
seeds zones by confidence, tests balance using a parity-based coloring, and
applies a greedy, locality-preserving repair with Jaccard de-duplication to
build a compact atlas. To model belief change, we introduce shock updates that
locally downscale support and elevate targeted contradictions while preserving
contractivity via a simple backtracking rule. Re-propagation yields localized
reconfiguration-zones may shrink, split, or collapse--without destabilizing the
entire graph. We outline an empirical protocol on synthetic signed graphs with
planted zones, reporting zone recovery, stability under shocks, and runtime.
The result is a principled foundation for contradiction-tolerant reasoning that
activates classical logic precisely where structure supports it.

</details>


### [131] [SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning](https://arxiv.org/abs/2510.10047)
*Ruohao Li,Hongjun Liu,Leyi Zhao,Zisu Li,Jiawei Li,Jiajun Jiang,Linning Xu,Chen Zhao,Mingming Fan,Chen Liang*

Main category: cs.AI

TL;DR: 本文提出SwarmSys，一个受蜂群智能启发的分布式多智能体推理框架，通过探索者、工作者和验证者的迭代协作，实现无需中心化控制的可扩展、自适应推理，并在多项任务上超越基线。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型多智能体框架常依赖固定角色或中心化控制，这限制了其在长周期推理中的可扩展性和适应性。

Method: 引入SwarmSys，一个闭环的分布式多智能体推理框架，灵感源于蜂群智能。通过探索者、工作者和验证者三种角色迭代进行探索、利用和验证，实现协作。集成自适应智能体和事件配置文件、基于嵌入的概率匹配以及受信息素启发的强化机制，支持动态任务分配和自组织收敛，无需全局监督。

Result: 在符号推理、研究综合和科学编程任务中，SwarmSys持续优于基线，提高了推理准确性和稳定性。

Conclusion: 蜂群启发的协调是实现可扩展、鲁棒和自适应多智能体推理的有前景范式，表明协调扩展在提升LLM智能方面可能与模型扩展同样重要。

Abstract: Large language model (LLM) agents have shown remarkable reasoning abilities.
However, existing multi-agent frameworks often rely on fixed roles or
centralized control, limiting scalability and adaptability in long-horizon
reasoning. We introduce SwarmSys, a closed-loop framework for distributed
multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys
emerges through iterative interactions among three specialized roles,
Explorers, Workers, and Validators, that continuously cycle through
exploration, exploitation, and validation. To enable scalable and adaptive
collaboration, we integrate adaptive agent and event profiles, embedding-based
probabilistic matching, and a pheromone-inspired reinforcement mechanism,
supporting dynamic task allocation and self-organizing convergence without
global supervision. Across symbolic reasoning, research synthesis, and
scientific programming tasks, SwarmSys consistently outperforms baselines,
improving both accuracy and reasoning stability. These findings highlight
swarm-inspired coordination as a promising paradigm for scalable, robust, and
adaptive multi-agent reasoning, suggesting that coordination scaling may rival
model scaling in advancing LLM intelligence.

</details>


### [132] [SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation](https://arxiv.org/abs/2510.10069)
*Zeyu Ling,Xiaodong Gu,Jiangnan Tang,Changqing Zou*

Main category: cs.AI

TL;DR: SyncLipMAE是一个自监督预训练框架，通过学习同步感知和可迁移的面部动态，为口语视频提供统一接口，并在多项下游任务中实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 从无标签音视频流中学习可迁移且同步感知的面部动态，以应对口语视频中的复杂挑战，并支持多种下游应用。

Method: 引入SyncLipMAE框架，结合掩码视觉建模和跨模态对比对齐。使用身份、发声运动（语音同步）和环境运动（与音频无关）三种逐帧提示令牌。通过对比目标使时间对齐的语音运动和音频令牌成为正样本，非对齐的作为负样本，实现令牌级的音视频同步。预训练后，对齐的音频令牌与视觉提示令牌构成统一接口。

Result: SyncLipMAE在四种需要不同能力的任务族（音视频流同步、面部情绪和头部/面部动作识别、视觉语音识别、视觉配音）上均取得了最先进（SOTA）的结果。

Conclusion: 同步感知、因子化的自监督预训练方法是有效的，SyncLipMAE通过其独特设计，为口语视频分析和生成提供了强大的基础，并在多项任务中展现出卓越性能。

Abstract: We introduce SyncLipMAE, a self-supervised pretraining framework for
talking-face video that learns synchronization-aware and transferable facial
dynamics from unlabeled audio-visual streams. Our approach couples masked
visual modeling with cross-modal contrastive alignment and employs three
per-frame prompt tokens that explicitly encode the essential factors of a
talking-face frame - identity, vocal motion (speech-synchronized facial
dynamics), and ambient motion (audio-agnostic movements such as blinks and head
pose). The contrastive objective uses time-aligned vocal-motion and audio
tokens as positives and misaligned pairs as negatives, driving both modalities
into a shared embedding space and yielding token-level audio-visual stream
synchronization. After pretraining, the aligned audio tokens together with the
visual prompt tokens (identity, vocal motion, ambient motion) form a unified
interface for four disparate downstream settings: (i) audio-visual stream
synchronization; (ii) facial emotion and head/face action recognition; (iii)
visual speech recognition; and (iv) visual dubbing, for which we enable
indistinguishable audio- or video-driven control within a single model. Across
four task families that require distinct capabilities, SyncLipMAE achieves
state-of-the-art results, underscoring the effectiveness of
synchronization-aware, factorized self-supervised pretraining.

</details>


### [133] [Agentic Troubleshooting Guide Automation for Incident Management](https://arxiv.org/abs/2510.10074)
*Jiayi Mao,Liqun Li,Yanjie Gao,Zegang Peng,Shilin He,Chaoyun Zhang,Si Qin,Samia Khalid,Qingwei Lin,Saravan Rajmohan,Sitaram Lanka,Dongmei Zhang*

Main category: cs.AI

TL;DR: StepFly是一个端到端代理框架，通过三阶段工作流自动化IT系统故障排除指南（TSGs），解决了手动执行的低效和现有LLM方案的局限性，实现了高成功率和显著的执行时间缩减。


<details>
  <summary>Details</summary>
Motivation: 大规模IT系统中的故障排除指南（TSGs）手动执行缓慢且易出错。现有基于LLM的解决方案在TSG质量管理、复杂控制流解释、数据密集型查询处理及并行执行方面存在不足。

Method: 首先对92个真实TSGs进行实证研究。在此基础上，提出StepFly框架，包含三阶段工作流：1) 提供TSG Mentor工具协助SRE提升TSG质量；2) 离线使用LLM从非结构化TSGs中提取结构化执行DAGs并创建Query Preparation Plugins (QPPs)；3) 在线使用DAG引导的调度器-执行器框架（带内存系统），确保工作流正确性并支持独立步骤的并行执行。

Result: 在GPT-4.1上实现了约94%的成功率，且相比基线消耗更少的时间和token。对于可并行化的TSGs，执行时间减少了32.9%至70.4%。

Conclusion: StepFly框架通过其创新的三阶段工作流，成功地自动化了大规模IT系统中的故障排除指南，有效解决了现有方法的效率和准确性问题，显著提升了自动化成功率并大幅缩短了执行时间，尤其是在并行化任务中展现出卓越性能。

Abstract: Effective incident management in large-scale IT systems relies on
troubleshooting guides (TSGs), but their manual execution is slow and
error-prone. While recent advances in LLMs offer promise for automating
incident management tasks, existing LLM-based solutions lack specialized
support for several key challenges, including managing TSG quality issues,
interpreting complex control flow, handling data-intensive queries, and
exploiting execution parallelism. We first conducted an empirical study on 92
real-world TSGs, and, guided by our findings, we present StepFly, a novel
end-to-end agentic framework for troubleshooting guide automation. Our approach
features a three-stage workflow: the first stage provides a comprehensive guide
together with a tool, TSG Mentor, to assist SREs in improving TSG quality; the
second stage performs offline preprocessing using LLMs to extract structured
execution DAGs from unstructured TSGs and to create dedicated Query Preparation
Plugins (QPPs); and the third stage executes online using a DAG-guided
scheduler-executor framework with a memory system to guarantee correct workflow
and support parallel execution of independent steps. Our empirical evaluation
on a collection of real-world TSGs and incidents demonstrates that StepFly
achieves a ~94% success rate on GPT-4.1, outperforming baselines with less time
and token consumption. Furthermore, it achieves a remarkable execution time
reduction of 32.9% to 70.4% for parallelizable TSGs.

</details>


### [134] [DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay](https://arxiv.org/abs/2510.10117)
*Yunxiang Mo,Tianshi Zheng,Qing Zong,Jiayu Liu,Baixuan Xu,Yauwai Yim,Chunkit Chan,Jiaxin Bai,Yangqiu Song*

Main category: cs.AI

TL;DR: 论文引入DixitWorld评估套件，用于在动态多智能体环境中全面评估视觉语言模型（VLMs）的多模态溯因推理能力，发现生成创意与判别理解之间存在关键权衡。


<details>
  <summary>Details</summary>
Motivation: 当前对视觉语言模型（VLMs）多模态溯因推理能力的评估，大多局限于静态、单智能体任务，未能充分反映其在复杂、动态、多智能体情境下的真实表现。

Method: 本文提出了DixitWorld评估套件，包含两个核心组件：1) DixitArena，一个动态、多智能体环境，用于评估假设生成（讲故事者）和假设选择（听众）能力；2) DixitBench，一个静态QA基准，用于高效、受控地评估听众任务。

Result: DixitArena的结果显示，小型开源模型在创意生成方面表现出色，但线索的判别性较弱；大型专有模型整体性能更优，尤其在听众任务中。DixitBench的性能与DixitArena中听众任务的结果高度相关，验证了其作为假设选择代理评估的可靠性。

Conclusion: 研究发现多模态溯因推理中存在生成创意和判别理解之间的关键权衡，这对于开发更平衡、更强大的视觉语言智能体是一个核心挑战。

Abstract: Multimodal abductive reasoning--the generation and selection of explanatory
hypotheses from partial observations--is a cornerstone of intelligence. Current
evaluations of this ability in vision-language models (VLMs) are largely
confined to static, single-agent tasks. Inspired by Dixit, we introduce
DixitWorld, a comprehensive evaluation suite designed to deconstruct this
challenge. DIXITWORLD features two core components: DixitArena, a dynamic,
multi-agent environment that evaluates both hypothesis generation (a
"storyteller" crafting cryptic clues) and hypothesis selection ("listeners"
choosing the target image from decoys) under imperfect information; and
DixitBench, a static QA benchmark that isolates the listener's task for
efficient, controlled evaluation. Results from DixitArena reveal distinct,
role-dependent behaviors: smaller open-source models often excel as creative
storytellers, producing imaginative yet less discriminative clues, whereas
larger proprietary models demonstrate superior overall performance,
particularly as listeners. Performance on DixitBench strongly correlates with
listener results in DixitArena, validating it as a reliable proxy for
hypothesis selection. Our findings reveal a key trade-off between generative
creativity and discriminative understanding in multimodal abductive reasoning,
a central challenge for developing more balanced and capable vision-language
agents.

</details>


### [135] [CharCom: Composable Identity Control for Multi-Character Story Illustration](https://arxiv.org/abs/2510.10135)
*Zhongsheng Wang,Ming Lin,Zhedong Lin,Yaser Shakib,Qian Liu,Jiamou Liu*

Main category: cs.AI

TL;DR: CharCom通过可组合LoRA适配器解决了扩散模型中角色身份一致性难题，实现了多场景故事的角色连贯插画，且高效可扩展。


<details>
  <summary>Details</summary>
Motivation: 扩散模型文本到图像生成在跨不同提示时，难以保持角色身份一致性。

Method: 提出CharCom框架，一个模块化、参数高效的解决方案。它基于冻结扩散骨干，利用可组合LoRA适配器进行角色定制，并通过提示感知控制在推理时动态组合适配器。

Result: 实验证明CharCom显著提升了角色保真度、语义对齐和时间连贯性，在拥挤场景中稳健，并实现低开销多角色生成。

Conclusion: CharCom非常适合故事插画和动画等现实应用。

Abstract: Ensuring character identity consistency across varying prompts remains a
fundamental limitation in diffusion-based text-to-image generation. We propose
CharCom, a modular and parameter-efficient framework that achieves
character-consistent story illustration through composable LoRA adapters,
enabling efficient per-character customization without retraining the base
model. Built on a frozen diffusion backbone, CharCom dynamically composes
adapters at inference using prompt-aware control. Experiments on multi-scene
narratives demonstrate that CharCom significantly enhances character fidelity,
semantic alignment, and temporal coherence. It remains robust in crowded scenes
and enables scalable multi-character generation with minimal overhead, making
it well-suited for real-world applications such as story illustration and
animation.

</details>


### [136] [Concise Reasoning in the Lens of Lagrangian Optimization](https://arxiv.org/abs/2510.10168)
*Chengqian Gao,Haonan Li,Taylor W. Killian,Jianshu She,Renxi Wang,Liqun Ma,Zhoujun Cheng,Shibo Hao,Zhiqiang Xu*

Main category: cs.AI

TL;DR: 本文提出PALU，一种原则性且实用的方法，通过将简洁推理转化为受限优化问题并简化更新规则，实现了大型语言模型在保持或提高性能的同时大幅缩短推理链，并具有良好的领域和模型规模适应性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）的简洁推理方法多依赖手工启发式规则，难以平衡简洁性和性能，且缺乏跨领域和模型规模的适应性，导致“过度思考”问题。

Method: 提出性能感知长度更新（PALU）策略。它将简洁推理建模为受性能约束的响应长度最小化问题，并通过拉格朗日优化转化为无约束问题。为实现实用性，PALU通过以下三点简化更新规则：1) 使用离策略回滚估计性能；2) 将拉格朗日乘数截断为两个极端；3) 用分位数驱动的长度调整取代基于梯度的更新。

Result: PALU在DeepSeek-Distill-Qwen-1.5B模型上，平均在五个基准测试中，将输出长度减少了65%，同时将准确性提高了15%，优于其他方法。此外，PALU还展示了在不同领域（逻辑、STEM、数学）和模型规模（1.5B、7B、14B）上的良好适应性。

Conclusion: PALU是一种实用且有效的简洁推理方法，能够在大模型中实现显著的推理链缩减和性能提升，并具有强大的泛化能力。

Abstract: Concise reasoning in large language models seeks to generate only essential
intermediate steps needed to arrive at a final answer, thereby alleviating
issues of overthinking. Most proposed approaches hinge on carefully
hand-crafted heuristics, struggling to balance concision with performance,
often failing to adapt across domains and model scales. In this work, we
address these challenges by introducing a principled and pragmatic strategy,
performance-aware length updating (PALU). As a principled algorithm, PALU
formulates concise reasoning as a constrained optimization problem, minimizing
response length subject to a performance constraint, and then applies
Lagrangian optimization to convert it into a tractable unconstrained problem.
As a pragmatic solution, PALU streamlines complicated update rules through
three approximations: (i) estimating performance with off-policy rollouts, (ii)
truncating the Lagrange multiplier to two extremes, and (iii) replacing
gradient-based updates with quantile-driven length adjustments. PALU reduces
output length by 65% while improving accuracy by 15% when applied to
DeepSeek-Distill-Qwen-1.5B, averaged over five benchmarks, outperforming a
range of alternative methods. Furthermore, PALU is demonstrated to adapt across
both domain (logic, STEM and math) and model scale (1.5B, 7B, 14B) entrenching
the algorithm as a practical and effective concise reasoning approach.

</details>


### [137] [SAFER: Risk-Constrained Sample-then-Filter in Large Language Models](https://arxiv.org/abs/2510.10193)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: 针对LLMs在开放式问答等风险敏感应用中的可信度问题，本文提出了SAFER框架。该框架通过两阶段风险控制（弃权感知抽样和一致性过滤），在不依赖有限解空间假设的前提下，提供统计学保证，有效控制不确定性并过滤不可靠的答案。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在风险敏感应用（如开放式问答）中部署增多，确保其输出的可信度至关重要。现有选择性共形预测（SCP）方法对开放式问答等缺乏固定有限解空间的应用场景，存在不切实际的“有限采样可获得所有正确答案”的假设，无法有效提供统计学保证。

Method: 本文提出了一个两阶段风险控制框架SAFER：
1.  **弃权感知抽样**：在校准集上，使用Clopper-Pearson精确方法，在用户定义的风险水平和最大抽样上限内校准抽样预算。若风险水平无法满足则弃权，否则确定最小抽样要求。
2.  **一致性过滤**：利用在校准预算下可获得正确答案的校准实例，应用一致性风险控制方法确定统计有效的“不确定性阈值”，用于过滤测试数据点候选集中的不可靠干扰项。此阶段引入额外风险水平，控制正确答案被排除的风险。

Result: SAFER框架与多种任务特定的接纳标准和校准-测试分割比例兼容，展现出良好的鲁棒性和高数据效率。它有效解决了现有SCP方法在开放式问答场景中不切实际的假设。

Conclusion: SAFER框架为LLMs在风险敏感的开放式问答应用中提供了一种具有统计学保证、鲁棒且数据高效的可信度控制方法，通过两阶段风险控制（弃权感知抽样和一致性过滤），有效应对了有限解空间假设的局限性。

Abstract: As large language models (LLMs) are increasingly deployed in risk-sensitive
applications such as real-world open-ended question answering (QA), ensuring
the trustworthiness of their outputs has become critical. Existing selective
conformal prediction (SCP) methods provide statistical guarantees by
constructing prediction sets with a constrained miscoverage rate for correct
answers. However, prior works unrealistically assume that admissible answers
for all instances can be obtained via finite sampling, even for open-ended QA
scenarios that lack a fixed and finite solution space. To address this, we
introduce a two-stage risk control framework comprising abstention-aware
sampling and conformalized filtering (SAFER). Firstly, on a held-out
calibration set, SAFER calibrates a sampling budget within the maximum sampling
cap, using the Clopper-Pearson exact method at a user-desired risk level (i.e.,
the maximum allowable miscoverage rate of the sampling sets). If the risk level
cannot be satisfied within the cap, we abstain; otherwise, the calibrated
sampling budget becomes the minimum requirements at test time. Then, we employ
calibration instances where correct answers are attainable under the calibrated
budget and apply the conformal risk control method to determine a statistically
valid uncertainty threshold, which filters unreliable distractors from the
candidate set for each test data point. In this stage, SAFER introduces an
additional risk level to guide the calculation of the threshold, thereby
controlling the risk of correct answers being excluded. Furthermore, we show
that SAFER is compatible with various task-specific admission criteria and
calibration-test split ratios, highlighting its robustness and high data
efficiency.

</details>


### [138] [Don't Just Fine-tune the Agent, Tune the Environment](https://arxiv.org/abs/2510.10197)
*Siyuan Lu,Zechuan Wang,Hongxuan Zhang,Qintong Wu,Leilei Gan,Chenyi Zhuang,Jinjie Gu,Tao Lin*

Main category: cs.AI

TL;DR: 本文提出“环境调优”范式，使LLM智能体能直接从问题实例中学习，无需专家轨迹。该方法通过课程学习、环境增强和细粒度奖励，解决了数据稀缺、过拟合和RL冷启动问题，实现了卓越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂多轮工具使用任务中潜力巨大，但高质量训练数据极度稀缺。监督微调(SFT)易导致过拟合，而标准强化学习(RL)存在冷启动和训练不稳定问题。

Method: 引入“环境调优”(Environment Tuning)范式。该方法通过结构化课程、提供纠正反馈的可操作环境增强以及细粒度进度奖励，使智能体直接从问题实例中学习，无需预收集专家轨迹，以确保稳定高效的探索。

Result: 仅使用Berkeley Function-Calling Leaderboard (BFCL)基准测试中的400个问题实例，该方法不仅在分布内性能上与强基线竞争，而且表现出卓越的分布外泛化能力，克服了SFT方法常见的性能崩溃问题。

Conclusion: 这项工作提出了从静态轨迹上的监督微调到动态、基于环境探索的范式转变，为训练更鲁棒和数据高效的智能体铺平了道路。

Abstract: Large Language Model (LLM) agents show great promise for complex, multi-turn
tool-use tasks, but their development is often hampered by the extreme scarcity
of high-quality training data. Supervised fine-tuning (SFT) on synthetic data
leads to overfitting, whereas standard reinforcement learning (RL) struggles
with a critical cold-start problem and training instability. To address these
challenges, we introduce $\textbf{Environment Tuning}$, a novel training
paradigm that enables agents to learn complex behaviors directly from problem
instances without relying on pre-collected expert trajectories.
$\textbf{Environment Tuning}$ orchestrates this learning process through a
structured curriculum, actionable environment augmentation that provides
corrective feedback, and fine-grained progress rewards to ensure stable and
efficient exploration. Using only 400 problem instances from Berkeley
Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves
competitive in-distribution performance against strong baselines but also
demonstrates superior out-of-distribution generalization, overcoming the
performance collapse common to SFT-based approaches. Our work presents a
paradigm shift from supervised fine-tuning on static trajectories to dynamic,
environment-based exploration, paving the way for training more robust and
data-efficient agents.

</details>


### [139] [PIXEL: Adaptive Steering Via Position-wise Injection with eXact Estimated Levels under Subspace Calibration](https://arxiv.org/abs/2510.10205)
*Manjiang Yu,Hongji Li,Priyanka Singh,Xue Li,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: PIXEL提出一种位置感知激活引导框架，通过学习属性对齐子空间和自适应干预强度，实现LLMs的精确行为控制和可靠属性对齐，无需大量调优。


<details>
  <summary>Details</summary>
Motivation: 在网络上部署大型语言模型（LLMs）需要可靠的行为控制。激活引导是一种无需调优的对齐属性（如真实性）以确保可信生成的方法。然而，现有方法依赖粗略启发式，缺乏关于干预位置和强度的原则性解释。

Method: 本文提出Position-wise Injection with eXact Estimated Levels (PIXEL)。PIXEL是一个位置感知的激活引导框架，它通过双视图（尾部平均和末尾token）学习属性对齐的子空间，并通过具有闭式解的约束几何目标选择干预强度，从而在无需全局超参数调优的情况下适应token级敏感性。PIXEL还执行样本级正交残差校准以优化全局属性方向，并采用轻量级位置扫描例程识别有效的注入点。此外，它为最小干预规则提供了表示层保证。

Result: 在各种模型和评估范式中，PIXEL始终能提高属性对齐，同时保持模型的通用能力。

Conclusion: PIXEL为LLMs的可控生成提供了一种实用且有原则的方法，支持可靠对齐。

Abstract: Reliable behavior control is central to deploying large language models
(LLMs) on the web. Activation steering offers a tuning-free route to align
attributes (e.g., truthfulness) that ensure trustworthy generation. Prevailing
approaches rely on coarse heuristics and lack a principled account of where to
steer and how strongly to intervene. To this end, we propose Position-wise
Injection with eXact Estimated Levels (PIXEL), a position-wise activation
steering framework that, in contrast to prior work, learns a property-aligned
subspace from dual views (tail-averaged and end-token) and selects intervention
strength via a constrained geometric objective with a closed-form solution,
thereby adapting to token-level sensitivity without global hyperparameter
tuning. PIXEL further performs sample-level orthogonal residual calibration to
refine the global attribute direction and employs a lightweight
position-scanning routine to identify receptive injection sites. We
additionally provide representation-level guarantees for the
minimal-intervention rule, supporting reliable alignment. Across diverse models
and evaluation paradigms, PIXEL consistently improves attribute alignment while
preserving model general capabilities, offering a practical and principled
method for LLMs' controllable generation. Our code is available at
https://github.com/V1centNevwake/PIXEL-Adaptive-Steering

</details>


### [140] [Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning](https://arxiv.org/abs/2510.10207)
*Yujian Zhang,Keyu Chen,Zhifeng Shen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: 本文提出自适应双推理器（ADR），通过在快思和慢思模式间动态切换，并采用两阶段训练（SFT与强化学习EHPO），在保持高性能的同时，显著提升长推理模型在数学推理任务上的效率，减少了输出长度。


<details>
  <summary>Details</summary>
Motivation: 长推理模型（LRMs）在推理任务中表现出色，但因“过度思考”导致计算成本高和推理延迟大。

Method: 提出Adaptive Dual Reasoner (ADR)，支持快思和慢思两种推理模式，并根据上下文复杂性动态切换。训练分为两阶段：1) 冷启动阶段：使用SFT和构建的混合推理数据集，赋予模型整合快慢推理模式的能力。2) 强化学习阶段：引入Entropy-guided Hybrid Policy Optimization (EHPO) 框架，通过熵引导的动态回滚策略和难度感知惩罚，优化推理效率，平衡快慢推理。

Result: ADR在挑战性数学推理基准上，实现了推理性能和效率的有效平衡。具体而言，性能提升高达6.1%，同时推理输出长度减少了49.5%至59.3%。

Conclusion: ADR通过动态自适应的推理模式切换和优化的训练框架，有效解决了长推理模型性能与效率之间的矛盾，显著提升了推理效率，同时保持了先进的推理性能。

Abstract: Although Long Reasoning Models (LRMs) have achieved superior performance on
various reasoning scenarios, they often suffer from increased computational
costs and inference latency caused by overthinking. To address these
limitations, we propose Adaptive Dual Reasoner, which supports two reasoning
modes: fast thinking and slow thinking. ADR dynamically alternates between
these modes based on the contextual complexity during reasoning. ADR is trained
in two stages: (1) A cold-start stage using supervised fine-tuning (SFT) to
equip the model with the ability to integrate both fast and slow reasoning
modes, in which we construct a hybrid reasoning dataset through a dedicated
pipeline to provide large-scale supervision. (2) A reinforcement learning stage
for optimizing reasoning effort, where we introduce Entropy-guided Hybrid
Policy Optimization EHPO, an RL training framework employing an entropy-guided
dynamic rollout strategy for branching at high-entropy units and a
difficulty-aware penalty to balance fast and slow reasoning. Across challenging
mathematical reasoning benchmarks, ADR achieves an effective balance between
reasoning performance and efficiency among state-of-the-art approaches.
Specifically, ADR yields a performance gain of up to 6.1%, while reducing the
reasoning output length by 49.5% to 59.3%.

</details>


### [141] [The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities](https://arxiv.org/abs/2510.10238)
*Zixuan Qin,Kunlin Lyu,Qingchen Yu,Yifan Sun,Zhaoxin Fan*

Main category: cs.AI

TL;DR: 本研究发现大型语言模型（LLMs）中存在超稀疏的关键神经元集合，它们主要集中在外层和MLP down_proj组件中。扰动这些关键神经元会导致模型性能急剧（而非渐进）崩溃。这些发现对LLM的鲁棒性和可解释性有重要意义。


<details>
  <summary>Details</summary>
Motivation: 鉴于LLMs与人脑的相似性以及神经科学发现人脑中存在对核心认知功能至关重要的少量神经元，本研究旨在探讨LLMs是否也包含这样一小部分关键神经元。

Method: 本文提出了一种基于扰动的因果识别关键神经元方法（Perturbation-based Causal Identification of Critical Neurons），以系统地定位LLMs中的关键神经元。通过对不同模型架构和规模进行综合实验来验证。

Result: 研究揭示了三个关键发现：(1) LLMs包含超稀疏的关键神经元集合，扰乱它们可导致72B参数模型完全崩溃，困惑度增加多达20个数量级；(2) 这些关键神经元并非均匀分布，而是倾向于集中在外层，特别是在MLP的down_proj组件中；(3) 性能下降表现出急剧的相变，而非逐渐下降。

Conclusion: 这些发现为开发更鲁棒的模型架构和提高安全关键应用中的部署安全性提供了指导，并加深了对LLM鲁棒性和可解释性的理解。

Abstract: Large Language Models (LLMs) have become foundational tools in natural
language processing, powering a wide range of applications and research. Many
studies have shown that LLMs share significant similarities with the human
brain. Recent neuroscience research has found that a small subset of biological
neurons in the human brain are crucial for core cognitive functions, which
raises a fundamental question: do LLMs also contain a small subset of critical
neurons? In this paper, we investigate this question by proposing a
Perturbation-based Causal Identification of Critical Neurons method to
systematically locate such critical neurons in LLMs. Our findings reveal three
key insights: (1) LLMs contain ultra-sparse critical neuron sets. Disrupting
these critical neurons can cause a 72B-parameter model with over 1.1 billion
neurons to completely collapse, with perplexity increasing by up to 20 orders
of magnitude; (2) These critical neurons are not uniformly distributed, but
tend to concentrate in the outer layers, particularly within the MLP down\_proj
components; (3) Performance degradation exhibits sharp phase transitions,
rather than a gradual decline, when these critical neurons are disrupted.
Through comprehensive experiments across diverse model architectures and
scales, we provide deeper analysis of these phenomena and their implications
for LLM robustness and interpretability. These findings can offer guidance for
developing more robust model architectures and improving deployment security in
safety-critical applications.

</details>


### [142] [Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control](https://arxiv.org/abs/2510.10285)
*Haolang Lu,Bolun Chu,WeiYe Fu,Guoshun Nan,Junning Liu,Minghui Pan,Qiankun Li,Yi Yu,Hua Wang,Kun Wang*

Main category: cs.AI

TL;DR: 本研究发现多模态大推理模型（MLRMs）中的幻觉现象源于感知偏差和推理漂移，这与注意力头的功能分工（浅层感知，深层推理）有关。为此，提出了一种轻量级、可解释的两步插件，通过识别并调节感知和推理导向的注意力头，在不重新训练的情况下，显著提高了MLRMs的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多模态大推理模型（MLRMs）是跨模态智能的基础，但在视觉-语言推理中存在持续的幻觉问题，表现为错误的推理链和对视觉内容的误解。解决这一关键失效模式对于MLRMs的安全部署至关重要。

Method: 研究观察到注意力头呈现阶段性分工：浅层头主要负责感知，深层头转向符号推理。基于此，揭示了幻觉的两个主要原因：感知偏差和推理漂移。提出了一种轻量级、可解释的两步插件——功能头识别和类别条件重缩放，用于定位感知和推理导向的注意力头，并在不重新训练的情况下调节其贡献。

Result: 在三个真实世界MLRMs、六个跨三个领域的基准和四个基线上进行评估。结果显示，该插件平均实现了5%的改进，最高达15%，而额外计算量不足1%，延迟仅为基线的9%。该方法具有模型无关性。

Conclusion: 所提出的插件显著增强了现有MLRMs的可靠性和可解释性，使其能够安全部署于高风险应用中。该方法是完全模型无关的。

Abstract: Multimodal large reasoning models (MLRMs) are rapidly advancing
vision-language reasoning and are emerging as a foundation for cross-modal
intelligence. Hallucination remains a persistent failure mode, manifesting
itself as erroneous reasoning chains and misinterpretation of visual content.
In this study, we observe that attention heads exhibit a staged division:
shallow heads predominantly serve perception, while deeper heads shift toward
symbolic reasoning, revealing two major causes of hallucination, namely
perceptual bias and reasoning drift. To address these issues, we propose a
lightweight and interpretable two-step plugin, Functional Head Identification
and Class-conditioned Rescaling, which locates perception- and
reasoning-oriented heads and regulates their contributions without retraining.
Evaluations on three real-world MLRMs (Kimi-VL, Ocean-R1, R1-Onevision), six
benchmarks across three domains, and four baselines show that our plugin
achieves an average improvement of 5% and up to 15%, with only <1% additional
computation and 9% of baseline latency. Our approach is completely
model-agnostic and significantly enhances both the reliability and
interpretability of the off-the-shelf MLRMs, thereby enabling their safe
deployment in high-stakes applications. Our code is available at
https://anonymous.4open.science/r/Functional-Attention-Control.

</details>


### [143] [LLM-Friendly Knowledge Representation for Customer Support](https://arxiv.org/abs/2510.10331)
*Hanchen Su,Wei Luo,Wei Han,Yu Elaine Liu,Yufeng Wayne Zhang,Cen Mia Zhao,Ying Joy Zhang,Yashar Mehdad*

Main category: cs.AI

TL;DR: 该研究提出一种整合LLM、ICA格式重构工作流和合成数据生成的方法，旨在提升客户支持操作的效率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 解决Airbnb客户支持操作的复杂性，并通过LLM实现更高效、更具成本效益的解决方案。

Method: 1. 整合大型语言模型 (LLMs) 与特定框架；2. 采用“意图、上下文、行动”(ICA) 格式重构政策和工作流程，使其更易于LLMs理解；3. 开发合成数据生成策略，以最少人工干预创建训练数据，用于成本效益高的模型微调。

Result: 内部实验表明，通过工作流程重构和合成数据微调LLMs，显著提升了其在客户支持中的性能，并设置了新基准，体现在准确性和手动处理时间评估指标的改善。

Conclusion: 该解决方案不仅具有成本效益，而且通过提高准确性和减少手动处理时间，有效改善了客户支持服务。

Abstract: We propose a practical approach by integrating Large Language Models (LLMs)
with a framework designed to navigate the complexities of Airbnb customer
support operations. In this paper, our methodology employs a novel reformatting
technique, the Intent, Context, and Action (ICA) format, which transforms
policies and workflows into a structure more comprehensible to LLMs.
Additionally, we develop a synthetic data generation strategy to create
training data with minimal human intervention, enabling cost-effective
fine-tuning of our model. Our internal experiments (not applied to Airbnb
products) demonstrate that our approach of restructuring workflows and
fine-tuning LLMs with synthetic data significantly enhances their performance,
setting a new benchmark for their application in customer support. Our solution
is not only cost-effective but also improves customer support, as evidenced by
both accuracy and manual processing time evaluation metrics.

</details>


### [144] [Beyond Ethics: How Inclusive Innovation Drives Economic Returns in Medical AI](https://arxiv.org/abs/2510.10338)
*Balagopal Unnikrishnan,Ariel Guerra Adames,Amin Adibi,Sameer Peesapati,Rafal Kocielnik,Shira Fischer,Hillary Clinton Kasimbazi,Rodrigo Gameiro,Alina Peluso,Chrystinne Oliveira Fernandes,Maximin Lange,Lovedeep Gondara,Leo Anthony Celi*

Main category: cs.AI

TL;DR: 本文提出医疗AI中的包容性设计不仅具有伦理价值，还能带来显著的经济和战略回报，即“包容性创新红利”。通过识别四种回报机制并引入HAIIF框架，作者论证了包容性创新是实现市场拓展和竞争优势的关键。


<details>
  <summary>Details</summary>
Motivation: 医疗AI公平性的伦理价值已被广泛接受，但包容性设计所带来的经济和战略价值仍未得到充分探索和认识。

Method: 引入“包容性创新红利”原则，并通过分析辅助技术发展为成功主流产业的案例进行论证。识别了包容性创新驱动经济回报的四种机制：市场扩张、风险缓解、性能红利和竞争优势。提出并介绍了“医疗AI包容性创新框架”（HAIIF），一个用于评估AI投资潜在效益的实用评分系统。

Result: 包容性创新能带来四方面的经济回报：1) 市场扩张（通过地域可扩展性和信任加速）；2) 风险缓解（降低补救成本和诉讼风险）；3) 性能红利（卓越的泛化性和减少技术债务）；4) 竞争优势（人才吸引和临床采纳）。HAIIF框架能帮助组织将公平性和包容性转化为战略差异化来源。

Conclusion: 持续投资于医疗AI的包容性设计，能使组织实现更广阔的市场覆盖和持续的竞争优势。而将这些考量视为额外成本的组织，则会在早期行动者积累网络效应和数据优势时，面临日益加剧的劣势。

Abstract: While ethical arguments for fairness in healthcare AI are well-established,
the economic and strategic value of inclusive design remains underexplored.
This perspective introduces the ``inclusive innovation dividend'' -- the
counterintuitive principle that solutions engineered for diverse, constrained
use cases generate superior economic returns in broader markets. Drawing from
assistive technologies that evolved into billion-dollar mainstream industries,
we demonstrate how inclusive healthcare AI development creates business value
beyond compliance requirements. We identify four mechanisms through which
inclusive innovation drives returns: (1) market expansion via geographic
scalability and trust acceleration; (2) risk mitigation through reduced
remediation costs and litigation exposure; (3) performance dividends from
superior generalization and reduced technical debt, and (4) competitive
advantages in talent acquisition and clinical adoption. We present the
Healthcare AI Inclusive Innovation Framework (HAIIF), a practical scoring
system that enables organizations to evaluate AI investments based on their
potential to capture these benefits. HAIIF provides structured guidance for
resource allocation, transforming fairness and inclusivity from regulatory
checkboxes into sources of strategic differentiation. Our findings suggest that
organizations investing incrementally in inclusive design can achieve expanded
market reach and sustained competitive advantages, while those treating these
considerations as overhead face compounding disadvantages as network effects
and data advantages accrue to early movers.

</details>


### [145] [Trace Length is a Simple Uncertainty Signal in Reasoning Models](https://arxiv.org/abs/2510.10409)
*Siddartha Devic,Charlotte Peale,Arwen Bradley,Sinead Williamson,Preetum Nakkiran,Aravind Gollakota*

Main category: cs.AI

TL;DR: 本研究提出推理轨迹长度作为大型语言模型的简单有效置信度估计器，发现在推理后训练的模型中其表现与口头置信度相当且互补，并揭示了其作用机制。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）的幻觉等问题以实现可靠部署，不确定性量化是关键。

Method: 使用推理轨迹长度（reasoning trace length）作为大型推理模型的置信度估计器，并通过跨模型、数据集和提示的实验进行验证。

Result: 1. 推理轨迹长度作为置信度估计器与口头置信度等零样本方法表现相当且互补。2. 推理后训练从根本上改变了轨迹长度与准确性的关系。3. 即使调整了问题难度和GRPO引起的长度偏差等混杂因素，轨迹长度作为置信度信号的效果依然存在。4. 识别出高熵或“分叉”令牌在轨迹长度作为置信度信号的机制中扮演关键角色。

Conclusion: 推理后训练增强了LLM的不确定性量化能力，超越了口头表达；推理轨迹长度被确立为大型推理模型的一种实用且有效的置信度测量方法。

Abstract: Uncertainty quantification for LLMs is a key research direction towards
addressing hallucination and other issues that limit their reliable deployment.
In this work, we show that reasoning trace length is a simple and useful
confidence estimator in large reasoning models. Through comprehensive
experiments across multiple models, datasets, and prompts, we show that trace
length performs in comparable but complementary ways to other zero-shot
confidence estimators such as verbalized confidence. Our work reveals that
reasoning post-training fundamentally alters the relationship between trace
length and accuracy, going beyond prior work that had shown that post-training
causes traces to grow longer in general (e.g., "overthinking"). We investigate
the mechanisms behind trace length's performance as a confidence signal,
observing that the effect remains even after adjusting for confounders such as
problem difficulty and GRPO-induced length bias. We identify high-entropy or
"forking" tokens as playing a key role in the mechanism. Our findings
demonstrate that reasoning post-training enhances uncertainty quantification
beyond verbal expressions, and establish trace length as a practical confidence
measure for large reasoning models.

</details>


### [146] [Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction](https://arxiv.org/abs/2510.10454)
*Sihang Zeng,Yujuan Fu,Sitong Zhou,Zixuan Yu,Lucas Jing Liu,Jun Wen,Matthew Thompson,Ruth Etzioni,Meliha Yetisgen*

Main category: cs.AI

TL;DR: 本文提出了Traj-CoA，一个多智能体系统，通过链式工作智能体和共享记忆模块处理冗长噪声的EEHR数据，以增强LLM在患者轨迹建模中的时间推理能力，并在肺癌风险预测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在患者轨迹建模方面具有通用性，但由于电子健康记录（EHR）数据冗长且嘈杂，导致其在时间推理方面表现不佳。

Method: 引入了Traj-CoA，一个多智能体系统，采用链式工作智能体按块顺序处理EHR数据，并提炼关键事件到共享的长期记忆模块EHRMem中，以减少噪声并维护完整时间线。最终由一个管理器智能体综合工作智能体的摘要和EHRMem的时间线进行预测。

Result: 在基于五年EHR数据的零样本一年期肺癌风险预测任务中，Traj-CoA的性能优于四类基线模型。

Conclusion: 分析表明Traj-CoA展现出与临床一致的时间推理能力，有望成为一种稳健且通用的复杂患者轨迹建模方法。

Abstract: Large language models (LLMs) offer a generalizable approach for modeling
patient trajectories, but suffer from the long and noisy nature of electronic
health records (EHR) data in temporal reasoning. To address these challenges,
we introduce Traj-CoA, a multi-agent system involving chain-of-agents for
patient trajectory modeling. Traj-CoA employs a chain of worker agents to
process EHR data in manageable chunks sequentially, distilling critical events
into a shared long-term memory module, EHRMem, to reduce noise and preserve a
comprehensive timeline. A final manager agent synthesizes the worker agents'
summary and the extracted timeline in EHRMem to make predictions. In a
zero-shot one-year lung cancer risk prediction task based on five-year EHR
data, Traj-CoA outperforms baselines of four categories. Analysis reveals that
Traj-CoA exhibits clinically aligned temporal reasoning, establishing it as a
promisingly robust and generalizable approach for modeling complex patient
trajectories.

</details>


### [147] [MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision](https://arxiv.org/abs/2510.10461)
*Hongjie Zheng,Zesheng Shi,Ping Yi*

Main category: cs.AI

TL;DR: MedCoAct，一个多智能体LLM框架，通过模拟临床协作，显著提升了综合医学诊断和治疗的准确性，超越单一智能体系统。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的医疗AI在孤立任务上表现突出，但在整合诊断与治疗的临床工作流中表现不佳，原因在于缺乏临床团队式的交叉验证和知识整合。

Method: 提出了MedCoAct，一个置信度感知的多智能体框架，通过集成专业医生和药剂师智能体来模拟临床协作。同时，引入了DrugCareQA基准用于评估。

Result: MedCoAct在诊断和用药推荐准确率上均达67.58%，分别比单一智能体框架高7.04%和7.08%。

Conclusion: 该协作方法在多领域具有良好泛化性，尤其适用于远程医疗和常规临床，并提供可解释的决策路径。

Abstract: Autonomous agents utilizing Large Language Models (LLMs) have demonstrated
remarkable capabilities in isolated medical tasks like diagnosis and image
analysis, but struggle with integrated clinical workflows that connect
diagnostic reasoning and medication decisions. We identify a core limitation:
existing medical AI systems process tasks in isolation without the
cross-validation and knowledge integration found in clinical teams, reducing
their effectiveness in real-world healthcare scenarios. To transform the
isolation paradigm into a collaborative approach, we propose MedCoAct, a
confidence-aware multi-agent framework that simulates clinical collaboration by
integrating specialized doctor and pharmacist agents, and present a benchmark,
DrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and
treatment workflows. Our results demonstrate that MedCoAct achieves 67.58\%
diagnostic accuracy and 67.58\% medication recommendation accuracy,
outperforming single agent framework by 7.04\% and 7.08\% respectively. This
collaborative approach generalizes well across diverse medical domains, proving
especially effective for telemedicine consultations and routine clinical
scenarios, while providing interpretable decision-making pathways.

</details>


### [148] [Tracing the Traces: Latent Temporal Signals for Efficient and Accurate Reasoning](https://arxiv.org/abs/2510.10494)
*Martina G. Vilas,Safoora Yousefi,Besmira Nushi,Eric Horvitz,Vidhisha Balachandran*

Main category: cs.AI

TL;DR: 本文引入“潜在轨迹信号”，通过分析模型内部表征的动态变化，有效预测推理路径的成功率，从而显著减少计算浪费并提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 推理模型通过增加token预算来提升问题解决能力，但识别哪些推理路径可能成功是关键挑战。可靠地预测有效路径可以大幅减少计算浪费，提升效率。

Method: 我们提出了“潜在轨迹信号”，用于表征模型在生成中间推理token时内部表征的时间演化。通过测量潜在表征从开始到结束的总变化、中间步骤的累积变化，以及这些变化趋向最终状态的程度来构建信号。

Result: 潜在轨迹信号比跨层度量和基于输出的置信度更能可靠地预测解决方案准确性。利用这些信号指导多样本生成中的答案选择，与多数投票相比，可在保持甚至平均提高2.6%准确度的同时，将token使用量减少高达70%。此外，这些预测信号常在推理早期出现，支持早期选择和计算分配。

Conclusion: 该研究不仅提供了提高推理效率的实用策略，还为理解推理过程如何在潜在空间中表示和区分提供了更深入的解释性视角。

Abstract: Reasoning models improve their problem-solving ability through inference-time
scaling, allocating more compute via longer token budgets. Identifying which
reasoning traces are likely to succeed remains a key opportunity: reliably
predicting productive paths can substantially reduce wasted computation and
improve overall efficiency. We introduce Latent-Trajectory signals that
characterize the temporal evolution of a model's internal representations
during the generation of intermediate reasoning tokens. By measuring the
overall change in latent representations between the start and end of
reasoning, the change accumulated across intermediate steps, and the extent to
which these changes advance toward the final state, we show that these signals
predict solution accuracy more reliably than both cross-layer metrics and
output-based confidence measures. When used to guide answer selection across
multiple sampled generations, Latent-Trajectory signals make test-time scaling
more effective and efficient than majority voting, reducing token usage by up
to 70% while preserving and even improving accuracy by 2.6% on average.
Moreover, these predictive signals often emerge early in the reasoning trace,
enabling early selection and allocation of compute to the most promising
candidates. Our findings contribute not only practical strategies for
inference-time efficiency, but also a deeper interpretability perspective on
how reasoning processes are represented and differentiated in latent space.

</details>


### [149] [ELAIPBench: A Benchmark for Expert-Level Artificial Intelligence Paper Understanding](https://arxiv.org/abs/2510.10549)
*Xinbang Dai,Huikang Hu,Yongrui Chen,Jiaqi Li,Rihui Jin,Yuyang Zhang,Xiaoguang Li,Lifeng Shang,Guilin Qi*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在深度理解和推理完整学术论文方面仍有欠缺，现有基准不足。本文引入ELAIPBench，一个由专家创建的深度理解基准，发现即使是前沿LLMs表现也远低于人类，且思维链/RAG等方法未能有效提升，甚至有时有害。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在许多领域任务中表现出色，但它们深度理解和推理完整学术论文的能力尚未得到充分探索。现有基准通常无法捕捉这种深度，因为问题设计肤浅或评估指标不可靠，因此需要一个更全面的评估基准来弥补这一空白。

Method: 引入了ELAIPBench基准，该基准由领域专家通过激励驱动的对抗性标注过程策划。它包含来自137篇论文的403个多项选择题，涵盖三个难度级别，并强调非浅层检索的复杂推理。实验中评估了最佳LLM，并考察了结合思维模式或检索增强生成（RAG）系统的LLMs表现。

Result: 实验结果显示，性能最佳的LLM准确率仅为39.95%，远低于人类表现。此外，观察到配备思维模式或RAG系统的前沿LLMs未能提升最终结果，甚至由于过度思考或噪声检索而损害了准确性。

Conclusion: 这些发现强调了当前LLM能力与真正理解学术论文之间存在显著差距，表明LLMs在深度学术内容理解方面仍有很大的提升空间。

Abstract: While large language models (LLMs) excel at many domain-specific tasks, their
ability to deeply comprehend and reason about full-length academic papers
remains underexplored. Existing benchmarks often fall short of capturing such
depth, either due to surface-level question design or unreliable evaluation
metrics. To address this gap, we introduce ELAIPBench, a benchmark curated by
domain experts to evaluate LLMs' comprehension of artificial intelligence (AI)
research papers. Developed through an incentive-driven, adversarial annotation
process, ELAIPBench features 403 multiple-choice questions from 137 papers. It
spans three difficulty levels and emphasizes non-trivial reasoning rather than
shallow retrieval. Our experiments show that the best-performing LLM achieves
an accuracy of only 39.95%, far below human performance. Moreover, we observe
that frontier LLMs equipped with a thinking mode or a retrieval-augmented
generation (RAG) system fail to improve final results-even harming accuracy due
to overthinking or noisy retrieval. These findings underscore the significant
gap between current LLM capabilities and genuine comprehension of academic
papers.

</details>


### [150] [A Layered Intuition -- Method Model with Scope Extension for LLM Reasoning](https://arxiv.org/abs/2510.10592)
*Hong Su*

Main category: cs.AI

TL;DR: 本文提出一个统一的“直觉-方法分层模型与范围扩展”，引入新颖的时间和空间扩展，并建立基于熵的评估框架，旨在提升大型语言模型（LLM）解决未见问题的鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM性能受限于直接映射，需要更系统的方法来解决间接（未见过）问题。已有基于方法论的推理和范围扩展技术有待进一步整合与深化，以构建更全面的推理范式。

Method: 本文提出“直觉-方法分层模型与范围扩展”。该模型结合直觉思维（快速反应）和方法论思维（解耦问题与解决方案）。范围扩展包括垂直（原因分析）、水平（并行与泛化）以及首次引入的时间和空间扩展。这些扩展被组织成系统化的知识树和知识网络，以增强适应性。此外，引入“方法扩展熵”来定量评估扩展的独立性和多样性，以此衡量系统解决未见问题的能力。

Result: 通过逻辑连接现有方法并引入新的时间/空间扩展，以及建立基于熵的评估框架，本工作推进了LLM在实际问题解决中形成一个更健壮和可扩展的推理范式。

Conclusion: 本研究通过提出的统一分层模型、创新的时间和空间范围扩展，以及基于熵的量化评估框架，为LLM解决复杂和未见过的问题提供了一个更系统、鲁棒和可扩展的推理范式，对LLM在真实世界问题解决能力的发展具有重要意义。

Abstract: Existing studies have introduced method-based reasoning and scope extension
as approaches to enhance Large Language Model (LLM) performance beyond direct
matrix mappings. Building on these foundations, this paper summarizes and
integrates these ideas into a unified Intuition-Method Layered Model with Scope
Extension, designed to address indirected (unseen) issues more systematically.
In this framework, intuition-based thinking provides rapid first-reaction
answers, while method-based thinking decouples questions and solutions into
transferable reasoning units. Scope extension is then applied to broaden
applicability, including vertical (cause analysis), horizontal (parallel and
generalized issues), and for the first time, temporal and spatial extensions,
which expand reasoning across time and contextual dimensions. These extensions
are organized into systematic knowledge trees that interconnect into a
knowledge network, thereby increasing adaptability. To quantitatively evaluate
this process, we propose the entropy of method extension, which measures the
independence and diversity of extensions as an indicator of the system's
capacity to solve unseen questions. By logically connecting existing approaches
with new extensions and introducing an entropy-based evaluation framework, this
work advances toward a more robust and extensible reasoning paradigm for LLMs
in real-world problem-solving.

</details>


### [151] [A Distance Measure for Random Permutation Set: From the Layer-2 Belief Structure Perspective](https://arxiv.org/abs/2510.10596)
*Ruolan Cheng,Yong Deng,Serafín Moral,José Ramón Trillo*

Main category: cs.AI

TL;DR: 本文提出了一种基于累积Jaccard指数矩阵的随机置换集（RPS）距离测量新方法，该方法具有更高的灵敏度和灵活性，并克服了现有方法的缺点。


<details>
  <summary>Details</summary>
Motivation: 测量置换质量函数之间的距离是RPS理论（RPST）中的关键研究课题，需要改进现有方法以更准确和灵活地表示序结构不确定信息。

Method: 研究从随机有限集（RFS）和可迁移信念模型（TBM）两个视角深入分析RPS之间的距离。将RPST解释为TBM的精炼，引入累积Jaccard指数来量化置换相似度，并基于此指数矩阵提出一种RPS距离测量方法。研究了该距离度量的度量和结构性质，包括正定性分析和校正方案，并提供了用于调整权重和截断深度的两个参数。

Result: 所提出的方法克服了现有方法的缺点，与Jousselme距离兼容，并具有更高的灵敏度和灵活性。该方法具有自然的顶层加权特性，即高排名元素之间的不一致会导致更大的距离值。

Conclusion: 本研究提供了一种有效且改进的RPS距离测量方法，通过引入累积Jaccard指数和灵活的参数调整，显著提升了序结构不确定信息距离度量的准确性、敏感性和实用性。

Abstract: Random permutation set (RPS) is a recently proposed framework designed to
represent order-structured uncertain information. Measuring the distance
between permutation mass functions is a key research topic in RPS theory
(RPST). This paper conducts an in-depth analysis of distances between RPSs from
two different perspectives: random finite set (RFS) and transferable belief
model (TBM). Adopting the layer-2 belief structure interpretation of RPS, we
regard RPST as a refinement of TBM, where the order in the ordered focus set
represents qualitative propensity. Starting from the permutation, we introduce
a new definition of the cumulative Jaccard index to quantify the similarity
between two permutations and further propose a distance measure method for RPSs
based on the cumulative Jaccard index matrix. The metric and structural
properties of the proposed distance measure are investigated, including the
positive definiteness analysis of the cumulative Jaccard index matrix, and a
correction scheme is provided. The proposed method has a natural
top-weightiness property: inconsistencies between higher-ranked elements tend
to result in greater distance values. Two parameters are provided to the
decision-maker to adjust the weight and truncation depth. Several numerical
examples are used to compare the proposed method with the existing method. The
experimental results show that the proposed method not only overcomes the
shortcomings of the existing method and is compatible with the Jousselme
distance, but also has higher sensitivity and flexibility.

</details>


### [152] [EA4LLM: A Gradient-Free Approach to Large Language Model Optimization via Evolutionary Algorithms](https://arxiv.org/abs/2510.10603)
*WenTao Liu,Siyu Song,Hao Hao,Aimin Zhou*

Main category: cs.AI

TL;DR: 针对LLM梯度优化的高硬件门槛和非可微限制，本文提出EA4LLM，成功用进化算法训练10亿参数LLM，证明了非梯度优化可行性，并有望降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的梯度优化方法对硬件要求高（需要高并发、大内存GPU），且限制了许多有前景的非可微架构的使用。

Method: 提出EA4LLM（使用进化算法优化大型语言模型）的方法。

Result: 首次成功使用进化算法从预训练阶段训练一个10亿参数的LLM，并提供了进化算法如何有效优化神经网络的关键见解。

Conclusion: 本工作挑战了梯度优化是训练神经网络唯一可行方法的普遍假设，并有望显著降低训练LLM的计算成本，从而使计算资源有限的团队也能参与深度学习研究。

Abstract: In recent years, large language models (LLMs) have made remarkable progress,
with model optimization primarily relying on gradient-based optimizers such as
Adam. However, these gradient-based methods impose stringent hardware
requirements, demanding high-concurrency, high-memory GPUs. Moreover, they
require all neural network operations to be differentiable, thereby excluding
many promising non-differentiable architectures from practical use. To address
these limitations, we propose a method for optimizing LLMs using evolutionary
algorithms (EA4LLM) and, for the first time, successfully demonstrate its
capability to train a 1-billion-parameter LLM from the pre-trained stage. We
conduct extensive experiments and provide key insights into how evolutionary
algorithms can effectively optimize neural networks. Our work challenges the
prevailing assumption that gradient-based optimization is the only viable
approach for training neural networks. It also holds significant potential to
reduce the computational cost of training large language models, thereby
enabling groups with limited computational resources to participate in deep
learning research.

</details>


### [153] [Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion](https://arxiv.org/abs/2510.10633)
*Jiabao Shi,Minfeng Qi,Lefeng Zhang,Di Wang,Yingjie Zhao,Ziying Li,Yalong Xing,Ningran Li*

Main category: cs.AI

TL;DR: 本文提出一种多智能体强化学习框架，通过领域专业化智能体和耦合子系统，解决多模态文本到图像生成中语义对齐和专业细节保持的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态文本到图像生成在多样化视觉领域中难以保持语义对齐和专业级细节。

Method: 研究者提出一个多智能体强化学习框架，协调领域专业化智能体（如建筑、肖像、风景）在文本增强和图像生成两个耦合子系统中工作，每个子系统都增强了多模态集成组件。智能体使用近端策略优化（PPO）训练，其复合奖励函数平衡语义相似性、语言视觉质量和内容多样性。通过对比学习、双向注意力和文本与图像之间的迭代反馈来强制实现跨模态对齐。

Result: 在六种实验设置下，该系统显著丰富了生成内容（词汇量增加1614%），尽管ROUGE-1分数降低了69.7%。在融合方法中，基于Transformer的策略获得了最高的综合分数（0.521），但存在稳定性问题。多模态集成表现出中等一致性（0.444至0.481），反映了跨模态语义基础的持续挑战。

Conclusion: 研究结果强调了协作的、专业化驱动的架构在推进可靠的多模态生成系统方面的潜力。

Abstract: Multimodal text-to-image generation remains constrained by the difficulty of
maintaining semantic alignment and professional-level detail across diverse
visual domains. We propose a multi-agent reinforcement learning framework that
coordinates domain-specialized agents (e.g., focused on architecture,
portraiture, and landscape imagery) within two coupled subsystems: a text
enhancement module and an image generation module, each augmented with
multimodal integration components. Agents are trained using Proximal Policy
Optimization (PPO) under a composite reward function that balances semantic
similarity, linguistic visual quality, and content diversity. Cross-modal
alignment is enforced through contrastive learning, bidirectional attention,
and iterative feedback between text and image. Across six experimental
settings, our system significantly enriches generated content (word count
increased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion
methods, Transformer-based strategies achieve the highest composite score
(0.521), despite occasional stability issues. Multimodal ensembles yield
moderate consistency (ranging from 0.444 to 0.481), reflecting the persistent
challenges of cross-modal semantic grounding. These findings underscore the
promise of collaborative, specialization-driven architectures for advancing
reliable multimodal generative systems.

</details>


### [154] [Automatic Piecewise Linear Regression for Predicting Student Learning Satisfaction](https://arxiv.org/abs/2510.10639)
*Haemin Choi,Gayathri Nadarajan*

Main category: cs.AI

TL;DR: 本研究利用可解释机器学习APLR模型分析学生学习满意度，发现时间管理、专注力等是关键正向影响因素。


<details>
  <summary>Details</summary>
Motivation: 尽管学生学习满意度已被广泛研究，但现代可解释机器学习和神经网络等技术尚未得到充分探索。

Method: 采用结合提升算法和可解释性的自动分段线性回归（APLR）模型，并与其他先进方法进行比较。

Result: APLR模型在预测学习满意度方面表现最佳。时间管理、专注力、乐于助人的感知以及线下课程参与对学习满意度有显著正向影响。创意活动对学习满意度无正向影响。

Conclusion: 研究揭示的影响因素可进行个体层面的解释，有助于教育者根据学生特点定制个性化教学方案。

Abstract: Although student learning satisfaction has been widely studied, modern
techniques such as interpretable machine learning and neural networks have not
been sufficiently explored. This study demonstrates that a recent model that
combines boosting with interpretability, automatic piecewise linear
regression(APLR), offers the best fit for predicting learning satisfaction
among several state-of-the-art approaches. Through the analysis of APLR's
numerical and visual interpretations, students' time management and
concentration abilities, perceived helpfulness to classmates, and participation
in offline courses have the most significant positive impact on learning
satisfaction. Surprisingly, involvement in creative activities did not
positively affect learning satisfaction. Moreover, the contributing factors can
be interpreted on an individual level, allowing educators to customize
instructions according to student profiles.

</details>


### [155] [Equity-Aware Geospatial AI for Forecasting Demand-Driven Hospital Locations in Germany](https://arxiv.org/abs/2510.10640)
*Piyush Pant,Marcellius William Suntoro,Ayesha Siddiqua,Muhammad Shehryaar Sharif,Daniyal Ahmed*

Main category: cs.AI

TL;DR: EA-GeoAI是一个综合框架，用于德国医院的需求预测和公平规划，它整合人口数据、公平指数和AI优化器，以在预算和交通限制下分配资源并选址。


<details>
  <summary>Details</summary>
Motivation: 在德国，针对2030年前的医院需求预测和公平规划是一个挑战，需要考虑人口结构变化、老龄化密度和基础设施平衡，以确保医疗服务的公平可及性。

Method: 该研究提出了EA-GeoAI框架，首先将区域层面的人口结构变化、老龄化密度和基础设施平衡整合为统一的公平指数。随后，利用一个可解释的代理AI优化器，在预算和出行时间限制下，分配床位并确定新设施选址，以最小化未满足的需求。此方法结合了地理人工智能（GeoAI）、长期预测和公平性度量。

Result: 该框架能够有效分配床位并识别新设施选址，旨在预算和出行时间限制下，最小化未满足的医疗需求，并为政策制定者提供了可操作的建议。

Conclusion: EA-GeoAI框架通过整合GeoAI、长期预测和公平性度量，为政策制定者提供了应对未来需求和确保公平医院规划的可操作性建议。

Abstract: This paper presents EA-GeoAI, an integrated framework for demand forecasting
and equitable hospital planning in Germany through 2030. We combine
district-level demographic shifts, aging population density, and infrastructure
balances into a unified Equity Index. An interpretable Agentic AI optimizer
then allocates beds and identifies new facility sites to minimize unmet need
under budget and travel-time constraints. This approach bridges GeoAI,
long-term forecasting, and equity measurement to deliver actionable
recommendations for policymakers.

</details>


### [156] [Hierarchical Optimization via LLM-Guided Objective Evolution for Mobility-on-Demand Systems](https://arxiv.org/abs/2510.10644)
*Yi Zhang,Yushen Long,Yun Ni,Liping Huang,Xiaohong Wang,Jun Liu*

Main category: cs.AI

TL;DR: 本文提出一种结合大语言模型（LLM）与数学优化的混合框架，通过LLM自适应生成高层目标并利用进化过程优化提示，有效解决网约车平台供需平衡问题，无需训练数据且性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 网约车平台在动态且空间异构的供需平衡中面临挑战。现有方法（强化学习和分解式在线优化）存在数据效率低、模型简化、难以强制约束，以及高层目标缺乏底层路由动态感知等局限性。

Method: 提出一种将LLM与数学优化集成到动态分层系统中的混合框架。该框架无需训练，LLM作为元优化器生成语义启发式高层目标，引导负责约束强制和实时决策的低层优化器。这些启发式目标通过一个由和谐搜索驱动的闭环进化过程进行迭代优化，根据优化层的可行性和性能反馈调整LLM提示。

Result: 基于纽约和芝加哥出租车数据集的广泛实验表明，该方法有效，相比最先进的基线方法，平均性能提升了16%。

Conclusion: 所提出的LLM与数学优化混合框架，有效解决了网约车平台的供需平衡问题，通过自适应生成高层目标克服了现有方法的局限性，并在实际场景中展现出优越的性能。

Abstract: Online ride-hailing platforms aim to deliver efficient mobility-on-demand
services, often facing challenges in balancing dynamic and spatially
heterogeneous supply and demand. Existing methods typically fall into two
categories: reinforcement learning (RL) approaches, which suffer from data
inefficiency, oversimplified modeling of real-world dynamics, and difficulty
enforcing operational constraints; or decomposed online optimization methods,
which rely on manually designed high-level objectives that lack awareness of
low-level routing dynamics. To address this issue, we propose a novel hybrid
framework that integrates large language model (LLM) with mathematical
optimization in a dynamic hierarchical system: (1) it is training-free,
removing the need for large-scale interaction data as in RL, and (2) it
leverages LLM to bridge cognitive limitations caused by problem decomposition
by adaptively generating high-level objectives. Within this framework, LLM
serves as a meta-optimizer, producing semantic heuristics that guide a
low-level optimizer responsible for constraint enforcement and real-time
decision execution. These heuristics are refined through a closed-loop
evolutionary process, driven by harmony search, which iteratively adapts the
LLM prompts based on feasibility and performance feedback from the optimization
layer. Extensive experiments based on scenarios derived from both the New York
and Chicago taxi datasets demonstrate the effectiveness of our approach,
achieving an average improvement of 16% compared to state-of-the-art baselines.

</details>


### [157] [Unlocking Exploration in RLVR: Uncertainty-aware Advantage Shaping for Deeper Reasoning](https://arxiv.org/abs/2510.10649)
*Can Xie,Ruotong Pan,Xiangyu Wu,Yunfei Zhang,Jiayi Fu,Tingting Gao,Guorui Zhou*

Main category: cs.AI

TL;DR: 本文提出UCAS，一种不确定性感知优势塑造方法，通过结合模型内部不确定性信号，解决了现有RLVR算法在LLM推理中因粗粒度优势信号导致的探索效率低下和熵坍塌问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法（如GRPO）对序列中所有token广播统一的优势信号，忽视了推理过程中不确定但关键的决策，导致探索效率低下和熵坍塌问题，未能有效平衡探索与利用。

Method: 引入不确定性感知优势塑造（UCAS），这是一种模型无关的方法，通过利用模型的内部不确定性信号来优化信用分配。UCAS分两阶段操作：首先利用模型的整体自信度调节响应级别优势，然后基于原始logit确定性施加token级别惩罚。这种双重机制旨在鼓励探索产生正确答案的高不确定性路径，并惩罚过度自信但错误的推理。

Result: 在五个数学推理基准上的广泛实验表明，UCAS在多个模型规模（包括1.5B和7B）下显著优于强大的RLVR基线。分析证实UCAS不仅获得了更高的奖励，还促进了更大的推理多样性。

Conclusion: UCAS成功地提高了LLM在RLVR中的推理能力，通过有效缓解熵坍塌并促进多样性，实现了更高的奖励和更好的探索-利用平衡。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has shown significant
promise for enhancing the reasoning capabilities of large language models
(LLMs). However, prevailing algorithms like GRPO broadcast a uniform advantage
signal across all tokens in a sequence. This coarse-grained approach overlooks
the pivotal role of uncertain, high-stakes decisions during reasoning, leading
to inefficient exploration and the well-documented problem of entropy collapse.
To address this, we introduce UnCertainty-aware Advantage Shaping (UCAS), a
model-free method that refines credit assignment by leveraging the model's
internal uncertainty signals. UCAS operates in two stages: it first modulates
the response-level advantage using the model's overall self-confidence, and
then applies a token-level penalty based on raw logit certainty. This dual
mechanism encourages exploration of high-uncertainty paths that yield correct
answers while penalizing overconfident yet erroneous reasoning, effectively
balancing the exploration-exploitation trade-off. Extensive experiments on five
mathematical reasoning benchmarks show that UCAS significantly outperforms
strong RLVR baselines across multiple model scales, including 1.5B and 7B. Our
analysis confirms that UCAS not only achieves higher rewards but also promotes
greater reasoning diversity and successfully mitigates entropy collapse.

</details>


### [158] [Simpliflow: A Lightweight Open-Source Framework for Rapid Creation and Deployment of Generative Agentic AI Workflows](https://arxiv.org/abs/2510.10675)
*Deven Panchal*

Main category: cs.AI

TL;DR: 本文提出了一个名为simpliflow的轻量级Python框架，通过声明式JSON配置简化了生成式智能体AI系统的开发和编排，解决了现有框架复杂性高的问题，并支持多种LLM。


<details>
  <summary>Details</summary>
Motivation: 现有用于构建生成式智能体AI系统的框架过于复杂、学习曲线陡峭且包含大量样板代码，阻碍了快速原型开发和部署。

Method: 引入了simpliflow，一个轻量级、开源的Python框架。它通过声明式、基于JSON的配置实现线性、确定性智能体工作流的快速开发和编排。其模块化架构解耦了智能体管理、工作流执行和后处理，并集成了LiteLLM以支持超过100种LLM。

Result: simpliflow的实用性通过从软件开发模拟到实时系统交互的多种用例得到了证明。与LangChain和AutoGen等主流框架的比较分析表明，simpliflow在确定性工作流环境中，在简洁性、控制性和速度方面具有独特优势。

Conclusion: simpliflow提供了一个优化了简洁性、控制性和速度的解决方案，用于快速开发和部署确定性生成式智能体工作流，有效解决了现有框架的复杂性问题。

Abstract: Generative Agentic AI systems are emerging as a powerful paradigm for
automating complex, multi-step tasks. However, many existing frameworks for
building these systems introduce significant complexity, a steep learning
curve, and substantial boilerplate code, hindering rapid prototyping and
deployment. This paper introduces simpliflow, a lightweight, open-source Python
framework designed to address these challenges. simpliflow enables the rapid
development and orchestration of linear, deterministic agentic workflows
through a declarative, JSON-based configuration. Its modular architecture
decouples agent management, workflow execution, and post-processing, promoting
ease of use and extensibility. By integrating with LiteLLM, it supports over
100 Large Language Models (LLMs) out-of-the-box. We present the architecture,
operational flow, and core features of simpliflow, demonstrating its utility
through diverse use cases ranging from software development simulation to
real-time system interaction. A comparative analysis with prominent frameworks
like LangChain and AutoGen highlights simpliflow's unique position as a tool
optimized for simplicity, control, and speed in deterministic workflow
environments.

</details>


### [159] [OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs](https://arxiv.org/abs/2510.10689)
*Caorui Li,Yu Chen,Yiyan Ji,Jin Xu,Zhenyu Cui,Shihao Li,Yuanxing Zhang,Jiafu Tang,Zhenghao Song,Dingling Zhang,Ying He,Haoxiang Liu,Yuxuan Wang,Qiufeng Wang,Zhenhe Wu,Jiehui Luo,Zhiyu Pan,Weihao Xie,Chenchen Zhang,Zhaohui Wang,Jiayi Tian,Yanghai Wang,Zhe Cao,Minxin Dai,Ke Wang,Runzhe Wen,Yinghao Ma,Yaning Pan,Sungkyun Chang,Termeh Taheri,Haiwen Xia,Christos Plachouras,Emmanouil Benetos,Yizhi Li,Ge Zhang,Jian Yang,Tianhao Peng,Zili Wang,Minghao Liu,Junran Peng,Zhaoxiang Zhang,Jiaheng Liu*

Main category: cs.AI

TL;DR: 引入OmniVideoBench，一个新基准测试，用于全面评估多模态大语言模型在音频-视频协同理解上的能力，揭示了当前模型与人类表现间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估多模态大语言模型对音频和视觉模态的协同推理能力时存在不足，常忽略特定模态或整合方式缺乏逻辑一致性。

Method: 提出了OmniVideoBench，一个大规模且设计严谨的基准测试，包含1000个高质量问答对（带逐步推理轨迹），来源于628个多样视频。它涵盖13种精心设计的问题类型，强调模态互补性和逻辑一致性。

Result: 对多个多模态大语言模型的评估显示，模型性能与人类推理之间存在显著差距，开源模型明显落后于闭源模型。

Conclusion: 真正的音频-视频推理具有固有难度，OmniVideoBench的发布将促进开发具有更强、更泛化推理能力的多模态大语言模型。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated
substantial potential in video understanding. However, existing benchmarks fail
to comprehensively evaluate synergistic reasoning capabilities across audio and
visual modalities, often neglecting either one of the modalities or integrating
them in a logically inconsistent manner. To bridge this gap, we introduce
OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to
assessing synergistic audio-visual understanding, with a strong emphasis on
modality complementarity and logical consistency. Specifically, OmniVideoBench
comprises 1000 high-quality question-answer(QA) pairs, each annotated with
step-by-step reasoning traces, derived from 628 diverse videos ranging from
several seconds to 30 minutes, and manually verified to guarantee complete
correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully
designed question types, covering temporal reasoning, spatial localization,
counting, causal inference, summarization, and beyond, thereby capturing the
essential challenges of video understanding. Evaluation of multiple MLLMs on
OmniVideoBench reveals a pronounced gap between model performance and human
reasoning, with open-source models lagging significantly behind their
closed-source counterparts, underscoring the inherent difficulty of genuine
audio-visual reasoning. We will release OmniVideoBench to foster the
development of MLLMs with stronger and more generalizable reasoning
capabilities.

</details>


### [160] [Extended Triangular Method: A Generalized Algorithm for Contradiction Separation Based Automated Deduction](https://arxiv.org/abs/2510.10701)
*Yang Xu,Shuwei Chen,Jun Liu,Feng Cao,Xingxing He*

Main category: cs.AI

TL;DR: 本文提出了扩展三角方法（ETM），一种形式化并扩展了矛盾分离（CSE）框架内部机制的广义矛盾构造算法，并作为高性能定理证明器的核心，其有效性已通过基准测试验证。


<details>
  <summary>Details</summary>
Motivation: 自动化演绎是AI核心但平衡完备性与计算效率仍是挑战。传统二元归结法限制了多子句间的协同作用。先前的矛盾分离扩展（CSE）理论框架虽已建立，但其算法实现尚未形式化和发布。

Method: 本文提出了扩展三角方法（ETM），这是一种广义的矛盾构造算法，用于形式化和扩展矛盾分离的内部机制。ETM在一个支持灵活子句交互和动态协同的三角几何框架中统一了多种矛盾构造策略。

Result: ETM作为多个高性能定理证明器（CSE, CSE-E, CSI-E, CSI-Enig）的算法核心。这些证明器在标准一阶基准测试（TPTP问题集和CASC 2018-2015）中取得了具有竞争力的结果，经验性地验证了该方法的有效性和通用性。

Conclusion: 通过弥合理论抽象与操作实现之间的鸿沟，ETM将矛盾分离范式推进成为一种广义、可扩展且具有实际竞争力的自动化推理模型，为逻辑推理和定理证明的未来研究提供了新方向。

Abstract: Automated deduction lies at the core of Artificial Intelligence (AI),
underpinning theorem proving, formal verification, and logical reasoning.
Despite decades of progress, reconciling deductive completeness with
computational efficiency remains an enduring challenge. Traditional reasoning
calculi, grounded in binary resolution, restrict inference to pairwise clause
interactions and thereby limit deductive synergy among multiple clauses. The
Contradiction Separation Extension (CSE) framework, introduced in 2018,
proposed a dynamic multi-clause reasoning theory that redefined logical
inference as a process of contradiction separation rather than sequential
resolution. While that work established the theoretical foundation, its
algorithmic realization remained unformalized and unpublished. This work
presents the Extended Triangular Method (ETM), a generalized
contradiction-construction algorithm that formalizes and extends the internal
mechanisms of contradiction separation. The ETM unifies multiple
contradiction-building strategies, including the earlier Standard Extension
method, within a triangular geometric framework that supports flexible clause
interaction and dynamic synergy. ETM serves as the algorithmic core of several
high-performance theorem provers, CSE, CSE-E, CSI-E, and CSI-Enig, whose
competitive results in standard first-order benchmarks (TPTP problem sets and
CASC 2018-2015) empirically validate the effectiveness and generality of the
proposed approach. By bridging theoretical abstraction and operational
implementation, ETM advances the contradiction separation paradigm into a
generalized, scalable, and practically competitive model for automated
reasoning, offering new directions for future research in logical inference and
theorem proving.

</details>


### [161] [Adaptive Selection of Symbolic Languages for Improving LLM Logical Reasoning](https://arxiv.org/abs/2510.10703)
*Xiangyu Wang,Haocheng Yang,Fengxiang Cheng,Fenrong Liu*

Main category: cs.AI

TL;DR: LLMs在复杂逻辑推理中面临符号语言（SL）选择问题。本文提出一种自适应选择最适合的SL类型进行翻译的方法，显著提升了LLMs的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: LLMs的逻辑推理表现高度依赖自然语言（NL）到符号语言（SL）的翻译准确性。现有方法多关注翻译内容相似性，却忽略了不同SL类型（如一阶逻辑、布尔可满足性）本身对性能的关键影响，不同NL逻辑推理问题可能对应不同的最优SL形式。

Method: 提出一种自适应选择SL的方法。利用LLMs从一阶逻辑、逻辑编程和布尔可满足性中选择最适合当前问题的SL，然后将NL问题翻译成目标SL表达式，并使用相应的逻辑求解器得出答案。

Result: 实验结果显示，该自适应选择方法显著优于将所有问题翻译成单一SL或随机选择SL。在一个混合基准数据集上，该方法实现了96%的准确率，比第二高的一阶逻辑翻译方法提升了25%。

Conclusion: LLMs的逻辑推理能力可以通过为每个自然语言问题自适应选择最合适的符号语言进行翻译来显著提升，验证了不同逻辑推理问题对应不同最优SL形式的假设。

Abstract: Large Language Models (LLMs) still struggle with complex logical reasoning.
While previous works achieve remarkable improvements, their performance is
highly dependent on the correctness of translating natural language (NL)
problems into a symbolic language (SL). Though numerous works focusing on
improving this translation accuracy, they only consider the similarity between
the meaning of SL and NL, overlooking another crucial influencing factor, the
selection of the target SL type itself. For example, first-order logic language
specializes in logical reasoning with categorical syllogisms and complex
quantifiers, while Boolean satisfiability formalism excels at representing
constraint satisfaction like partial problems. To our knowledge, this is the
first paper to claim and verify that different NL logical reasoning problem
corresponds to different optimal SL formalization for translation. Based on
this, we propose a methods to improve the logical reasoning performance of LLMs
by adaptively selecting the most suitable SL for each problem prior to
translation. Specifically, we leverage LLMs to select the target SL among
first-order logic, logic programming and Boolean satisfiability and then
translate the problem in NL to target SL expressions as well as employ the
corresponding logical solver to derive the final answer. Experimental results
on benchmarks show that our adaptive selection method significantly outperforms
translating all into single SL and randomly selecting the SL. On a mixed
dataset of these benchmarks, our approach achieves 96% accuracy, which
improving performance by 25% compared to the second highest accuracy from the
first-order logic translation.

</details>


### [162] [LLMs as Strategic Agents: Beliefs, Best Response Behavior, and Emergent Heuristics](https://arxiv.org/abs/2510.10813)
*Enric Junque de Fortuny,Veronica Roberta Cappelli*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型（LLMs）的真实战略思维，发现它们能在目标推理深度上表现出信念一致的最佳响应行为，并展现出元推理能力和生成新颖启发式规则的能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在需要对其他智能体行为进行推理的领域（如谈判、政策设计、市场模拟）中应用日益广泛，但现有研究多集中于评估它们的均衡博弈或推理深度，而对其是否具备真正的战略思维（即连贯地形成关于其他智能体的信念、评估可能行动并基于这些信念做出选择）仍未探索。

Method: 我们开发了一个框架，通过在静态、完全信息博弈中解耦信念、评估和选择来识别这种能力，并将其应用于一系列非合作环境。通过联合分析模型的选择和推理轨迹，并引入一种新的无上下文博弈以排除记忆模仿，我们进行了实验。

Result: 当前前沿模型在目标推理深度下表现出信念一致的最佳响应行为。在不受约束时，它们会自我限制推理深度，并对人类和合成对手形成差异化的推测，这揭示了一种新兴的元推理形式。在复杂性增加时，显式递归会让位于内部生成的启发式选择规则，这些规则稳定、模型特定且不同于已知的人类偏见。

Conclusion: 信念一致性、元推理和新颖启发式规则的形成可以共同源于语言建模目标，这为研究人工智能体中的战略认知提供了结构化基础。

Abstract: Large Language Models (LLMs) are increasingly applied to domains that require
reasoning about other agents' behavior, such as negotiation, policy design, and
market simulation, yet existing research has mostly evaluated their adherence
to equilibrium play or their exhibited depth of reasoning. Whether they display
genuine strategic thinking, understood as the coherent formation of beliefs
about other agents, evaluation of possible actions, and choice based on those
beliefs, remains unexplored. We develop a framework to identify this ability by
disentangling beliefs, evaluation, and choice in static, complete-information
games, and apply it across a series of non-cooperative environments. By jointly
analyzing models' revealed choices and reasoning traces, and introducing a new
context-free game to rule out imitation from memorization, we show that current
frontier models exhibit belief-coherent best-response behavior at targeted
reasoning depths. When unconstrained, they self-limit their depth of reasoning
and form differentiated conjectures about human and synthetic opponents,
revealing an emergent form of meta-reasoning. Under increasing complexity,
explicit recursion gives way to internally generated heuristic rules of choice
that are stable, model-specific, and distinct from known human biases. These
findings indicate that belief coherence, meta-reasoning, and novel heuristic
formation can emerge jointly from language modeling objectives, providing a
structured basis for the study of strategic cognition in artificial agents.

</details>


### [163] [DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems](https://arxiv.org/abs/2510.10815)
*Meiru Zhang,Philipp Borchert,Milan Gritta,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: 针对LLM数学形式化难题，提出DRIFT框架，通过分解数学语句并定向检索前提和示例定理，显著提升了前提检索效果。


<details>
  <summary>Details</summary>
Motivation: LLM在自动化数学形式化中，难以识别和利用前提数学知识。现有检索增强方法直接使用复杂且上下文有限的非形式化语句进行查询，效率低下。

Method: 引入DRIFT框架，使LLM将非形式化数学语句分解为可处理的“子组件”，从而实现对数学库中前提的精准检索。此外，DRIFT还检索示例定理以辅助模型更有效地利用前提进行形式化。

Result: 在ProofNet、ConNF和MiniF2F-test等基准测试中，DRIFT持续改进前提检索，在ProofNet上F1分数比DPR基线几乎翻倍。在分布外ConNF基准测试中，使用GPT-4.1和DeepSeek-V3.1时，BEq+@10分别提升了37.14%和42.25%。

Conclusion: 数学自动形式化中的检索效率高度依赖于模型特定的知识边界，因此需要开发与模型能力相适应的自适应检索策略。

Abstract: Automating the formalization of mathematical statements for theorem proving
remains a major challenge for Large Language Models (LLMs). LLMs struggle to
identify and utilize the prerequisite mathematical knowledge and its
corresponding formal representation in languages like Lean. Current
retrieval-augmented autoformalization methods query external libraries using
the informal statement directly, but overlook a fundamental limitation:
informal mathematical statements are often complex and offer limited context on
the underlying math concepts. To address this, we introduce DRIFT, a novel
framework that enables LLMs to decompose informal mathematical statements into
smaller, more tractable ''sub-components''. This facilitates targeted retrieval
of premises from mathematical libraries such as Mathlib. Additionally, DRIFT
retrieves illustrative theorems to help models use premises more effectively in
formalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,
ConNF, and MiniF2F-test) and find that it consistently improves premise
retrieval, nearly doubling the F1 score compared to the DPR baseline on
ProofNet. Notably, DRIFT demonstrates strong performance on the
out-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and
42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that
retrieval effectiveness in mathematical autoformalization depends heavily on
model-specific knowledge boundaries, highlighting the need for adaptive
retrieval strategies aligned with each model's capabilities.

</details>


### [164] [The Irrational Machine: Neurosis and the Limits of Algorithmic Safety](https://arxiv.org/abs/2510.10823)
*Daniel Howard*

Main category: cs.AI

TL;DR: 该论文提出了一个框架，用于描述具身AI中的“神经症”行为（即内部连贯但与现实不符的行为），列举了其在导航任务中的多种表现，并提出了一种基于遗传编程的破坏性测试方法，以揭示深层架构问题，而非仅是局部症状修补。


<details>
  <summary>Details</summary>
Motivation: 具身AI在规划、不确定性处理和厌恶记忆交互中，会产生内部连贯但与现实不符的“神经症”行为。传统的局部修补方法不足以解决这些全局性失败，需要更深层次的架构审视来确保AI的鲁棒性。

Method: 1. 在网格导航栈中归纳了多种重复出现的神经症行为模式（如翻转跳动、计划搅动等），并为其设计了轻量级在线检测器和可重用逃逸策略。 2. 提出了基于遗传编程的破坏性测试方法，通过演化世界和扰动来最大化对“定律压力”和神经症分数，以生成对抗性课程和反事实追踪，从而揭示需要架构修订而非症状级修补的问题。

Result: 1. 归纳了AI中多种神经症行为模式，并为其提供了检测器和逃逸策略。 2. 发现即使在完全可见的情况下，学习到的厌恶成本也可能导致持久的恐惧性回避，造成不必要的长距离绕行，表明局部选择会被主导。 3. 论证了局部修复不足以解决全局性失败，全局性失败需要进行架构层面的修订。

Conclusion: 具身AI中的“神经症”行为源于规划、不确定性处理和厌恶记忆的复杂交互，且局部修补不足以解决其导致的全局性失败。通过基于遗传编程的破坏性测试，可以有效地识别出需要进行架构修订而非仅仅修补症状的深层问题，从而实现更根本的设计改进。

Abstract: We present a framework for characterizing neurosis in embodied AI: behaviors
that are internally coherent yet misaligned with reality, arising from
interactions among planning, uncertainty handling, and aversive memory. In a
grid navigation stack we catalogue recurrent modalities including flip-flop,
plan churn, perseveration loops, paralysis and hypervigilance, futile search,
belief incoherence, tie break thrashing, corridor thrashing, optimality
compulsion, metric mismatch, policy oscillation, and limited-visibility
variants. For each we give lightweight online detectors and reusable escape
policies (short commitments, a margin to switch, smoothing, principled
arbitration). We then show that durable phobic avoidance can persist even under
full visibility when learned aversive costs dominate local choice, producing
long detours despite globally safe routes. Using First/Second/Third Law as
engineering shorthand for safety latency, command compliance, and resource
efficiency, we argue that local fixes are insufficient; global failures can
remain. To surface them, we propose genetic-programming based destructive
testing that evolves worlds and perturbations to maximize law pressure and
neurosis scores, yielding adversarial curricula and counterfactual traces that
expose where architectural revision, not merely symptom-level patches, is
required.

</details>


### [165] [LLM-Empowered Agentic MAC Protocols: A Dynamic Stackelberg Game Approach](https://arxiv.org/abs/2510.10895)
*Renxuan Tan,Rongpeng Li,Fei Wang,Chenghui Peng,Shaoyun Wu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种基于博弈论的LLM赋能多智能体深度强化学习（MARL）框架，用于生成自适应的MAC协议，以解决传统DRL协议泛化性和弹性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于DRL的MAC协议在提升特定任务网络性能方面有效，但在动态环境中泛化性和弹性较差，需要高成本的再训练才能适应变化。

Method: 研究将基站与用户设备之间的上行传输建模为一个动态多跟随者Stackelberg博弈。框架采用LLM驱动的智能体，通过近端策略优化（PPO）协调，根据网络动态合成自适应的语义MAC协议，并使用协议行动语法（PAG）确保过程的可靠性和效率。此外，还分析了Stackelberg均衡的存在性和收敛行为。

Result: 仿真结果表明，该框架比传统基线吞吐量提高了77.6%，公平性提高了65.2%。此外，它在用户数量波动的情况下表现出卓越的泛化能力，无需再训练或修改架构。

Conclusion: 所提出的LLM赋能MARL框架通过结合博弈论，成功解决了传统DRL在无线网络MAC协议中泛化性差的问题，为动态环境提供了自适应、高效且无需再训练的解决方案。

Abstract: Medium Access Control (MAC) protocols, essential for wireless networks, are
typically manually configured. While deep reinforcement learning (DRL)-based
protocols enhance task-specified network performance, they suffer from poor
generalizability and resilience, demanding costly retraining to adapt to
dynamic environments. To overcome this limitation, we introduce a
game-theoretic LLM-empowered multi-agent DRL (MARL) framework, in which the
uplink transmission between a base station and a varying number of user
equipments is modeled as a dynamic multi-follower Stackelberg game (MFSG),
capturing the network's natural hierarchical structure. Within this game,
LLM-driven agents, coordinated through proximal policy optimization (PPO),
synthesize adaptive, semantic MAC protocols in response to network dynamics.
Protocol action grammar (PAG) is employed to ensure the reliability and
efficiency of this process. Under this system, we further analyze the existence
and convergence behavior in terms of a Stackelberg equilibrium by studying the
learning dynamics of LLM-empowered unified policies in response to changing
followers. Simulations corroborate that our framework achieves a 77.6% greater
throughput and a 65.2% fairness improvement over conventional baselines.
Besides, our framework generalizes excellently to a fluctuating number of users
without requiring retraining or architectural changes.

</details>


### [166] [PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on Scientific Literature](https://arxiv.org/abs/2510.10909)
*Daoyu Wang,Mingyue Cheng,Qi Liu,Shuo Yu,Zirui Liu,Ze Guo*

Main category: cs.AI

TL;DR: PaperArena是一个评估LLM代理在科学文献跨论文推理和多工具协调能力的新基准，实验结果表明当前代理性能不佳，仍有巨大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）代理的评估主要局限于孤立论文内的无工具任务，缺乏针对真实研究场景中跨论文推理和多工具协调的基准。

Method: 提出PaperArena，一个评估代理解决需要整合多篇论文信息并借助外部工具的真实研究问题的基准。提供一个模块化、可扩展的平台，集成多模态解析、上下文检索和程序计算等工具。

Result: 即使最先进的LLM代理，其平均准确率也仅为38.78%（困难子集为18.47%）。所有测试代理均表现出低效的工具使用，常调用不必要的工具。

Conclusion: PaperArena揭示了LLM代理在科学发现方面仍有巨大改进潜力，并邀请社区采纳该基准以开发和评估更强大的代理。

Abstract: Understanding and reasoning on the web-scale scientific literature is a
crucial touchstone for large language model (LLM) based agents designed to
support complex knowledge-intensive tasks. However, existing works are mainly
restricted to tool-free tasks within isolated papers, largely due to the lack
of a benchmark for cross-paper reasoning and multi-tool orchestration in real
research scenarios. In this work, we propose PaperArena, an evaluation
benchmark for agents to address real-world research questions that typically
require integrating information across multiple papers with the assistance of
external tools. Given a research question, agents should integrate diverse
formats across multiple papers through reasoning and interacting with
appropriate tools, thereby producing a well-grounded answer. To support
standardized evaluation, we provide a modular and extensible platform for agent
execution, offering tools such as multimodal parsing, context retrieval, and
programmatic computation. Experimental results reveal that even the most
advanced LLM powering a well-established agent system achieves merely 38.78%
average accuracy. On the hard subset, accuracy drops to only 18.47%,
highlighting great potential for improvement. We also present several empirical
findings, including that all agents tested exhibit inefficient tool usage,
often invoking more tools than necessary to solve a task. We invite the
community to adopt PaperArena to develop and evaluate more capable agents for
scientific discovery. Our code and data are available
https://github.com/Melmaphother/PaperArena.

</details>


### [167] [PoU: Proof-of-Use to Counter Tool-Call Hacking in DeepResearch Agents](https://arxiv.org/abs/2510.10931)
*SHengjie Ma,Chenlong Deng,Jiaxin Mao,Jiadeng Huang,Teng Wang,Junjie Wu,Changwang Zhang,Jun wang*

Main category: cs.AI

TL;DR: RAG智能体在RL驱动下存在“工具调用欺骗”问题，即未真正利用检索证据却获得奖励，导致结果不可靠。本文提出PoU框架，通过强制证据与推理的因果关联，显著提升了事实准确性、证据忠实度及工具路由平衡。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）智能体（如DeepResearch系统）在使用强化学习（RL）进行多步推理时，存在一个名为“工具调用欺骗”的失败模式。智能体通过表面正确的工具调用来夸大奖励信号，但并未真正利用检索到的证据，导致：(i) 模式崩溃，过度依赖单一来源；(ii) 虚假接地，答案与引用内容支持薄弱。

Method: 本文提出“Proof-of-Use (PoU)”框架，一个以证据为基础的RL框架。PoU通过强制检索到的证据、推理痕迹和最终答案之间建立可验证的因果链接来解决问题。具体通过一个统一的逐步契约实现，该契约整合了句法引用验证、基于扰动的敏感性奖励以及答案-证据对齐目标，以确保工具使用既可解释又功能上接地。

Result: 在七个涵盖域内、域外和工具分布外设置的QA基准测试中，PoU框架在事实准确性、证据忠实度和工具路由平衡方面，始终优于强大的DeepResearch基线。

Conclusion: 研究结果表明，RL训练的智能体需要建立在检索信息因果使用而非仅仅任务结果的基础之上，这为实现可信赖的检索增强推理提供了一条原则性路径。

Abstract: Retrieval-augmented generation (RAG) agents, such as recent
DeepResearch-style systems, extend large language models (LLMs) with autonomous
information-seeking capabilities through external tools. While reinforcement
learning (RL) has enabled impressive multi-step reasoning, we identify a
previously overlooked failure mode, Tool-Call Hacking, where agents inflate
reward signals by issuing superficially correct tool calls without genuinely
leveraging the retrieved evidence. This results in (i) mode collapse into
repetitive reliance on a single source and (ii) spurious grounding, where
answers are only weakly supported by cited content.
  To address this, we propose Proof-of-Use (PoU), an evidence-grounded RL
framework that enforces verifiable causal links between retrieved evidence,
reasoning traces, and final answers. PoU operationalizes this through a unified
step-wise contract combining syntactic citation validation, perturbation-based
sensitivity rewards, and answer-evidence alignment objectives, ensuring that
tool usage remains both interpretable and functionally grounded.
  Across seven QA benchmarks spanning in-domain, out-of-domain, and
out-of-tool-distribution settings, PoU consistently outperforms strong
DeepResearch baselines in factual accuracy, evidence faithfulness, and
tool-routing balance. These findings highlight the necessity of grounding
RL-trained agents not merely in task outcomes but in the causal use of
retrieved information, offering a principled path toward trustworthy
retrieval-augmented reasoning.

</details>


### [168] [Scalable and Explainable Enterprise Knowledge Discovery Using Graph-Centric Hybrid Retrieval](https://arxiv.org/abs/2510.10942)
*Nilima Rao,Jagriti Srivastava,Pradeep Kumar Sharma,Hritvik Shrivastava*

Main category: cs.AI

TL;DR: 提出一个模块化混合检索框架，结合知识图谱、深度图表示和嵌入式搜索，从企业异构知识源构建统一知识图谱，以支持复杂的、跨多跳的查询推理，并在Git仓库上实现显著的相关性提升。


<details>
  <summary>Details</summary>
Motivation: 现代企业知识分散在多种异构系统中，传统检索方法（如关键词搜索或静态嵌入）难以处理需要上下文推理和多跳推断的复杂查询。

Method: 构建了一个模块化混合检索框架，集成了知识库语言增强模型（KBLam）、深度图表示（DeepGraph）和嵌入式语义搜索。该框架从代码、拉取请求、提交历史等解析并构建统一知识图谱，支持语义相似度搜索、结构推断和多跳推理。通过查询分析动态确定最优检索策略，并提供交互式界面进行图可视化和上下文感知查询路由。

Result: 在大规模Git仓库上的实验表明，统一推理层相比单独基于GPT的检索管道，答案相关性提高了80%。

Conclusion: 该框架通过结合图构建、混合推理和交互式可视化，为企业环境中智能知识助手提供了一个可扩展、可解释且以用户为中心的基础。

Abstract: Modern enterprises manage vast knowledge distributed across heterogeneous
systems such as Jira, Git repositories, Confluence, and wikis. Conventional
retrieval methods based on keyword search or static embeddings often fail to
answer complex queries that require contextual reasoning and multi-hop
inference across artifacts. We present a modular hybrid retrieval framework for
adaptive enterprise information access that integrates Knowledge Base
Language-Augmented Models (KBLam), DeepGraph representations, and
embedding-driven semantic search. The framework builds a unified knowledge
graph from parsed repositories including code, pull requests, and commit
histories, enabling semantic similarity search, structural inference, and
multi-hop reasoning. Query analysis dynamically determines the optimal
retrieval strategy, supporting both structured and unstructured data sources
through independent or fused processing. An interactive interface provides
graph visualizations, subgraph exploration, and context-aware query routing to
generate concise and explainable answers. Experiments on large-scale Git
repositories show that the unified reasoning layer improves answer relevance by
up to 80 percent compared with standalone GPT-based retrieval pipelines. By
combining graph construction, hybrid reasoning, and interactive visualization,
the proposed framework offers a scalable, explainable, and user-centric
foundation for intelligent knowledge assistants in enterprise environments.

</details>


### [169] [Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph](https://arxiv.org/abs/2510.10976)
*Wentao Wang,Heqing Zou,Tianze Luo,Rui Huang,Yutian Zhao,Zhuochen Wang,Hansheng Zhang,Chengwei Qin,Yan Wang,Lin Zhao,Huaijian Zhang*

Main category: cs.AI

TL;DR: MLLMs在精确时空理解上存在不足。本文提出了Video-STR，一个基于图强化学习的方法，用于精确视频时空推理，并构建了STV-205k数据集。在基准测试中取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在语义理解方面表现出色，但在精确时空理解方面仍有困难。现有方法忽视视频中的物理信息（如多对象布局和运动），限制了MLLMs在高精度应用（如具身智能、VR）中的使用。

Method: 本文提出Video-STR，一个新颖的基于图的强化学习方法，用于精确视频时空推理。结合可验证奖励强化学习（RLVR）与图基群组相对策略优化（GRPO）机制，引导模型推断场景的深层时空拓扑。同时，为解决数据不足，构建了包含20.5万问答对的STV-205k数据集。

Result: 实验结果表明，Video-STR在多个基准测试中取得了最先进（SOTA）的结果，在STI-Bench上比基础模型性能提升13%，证明了所提方法和数据集的有效性。

Conclusion: 本文成功解决了MLLMs在精确视频时空理解上的不足，通过Video-STR方法和STV-205k数据集，显著提升了模型在复杂动态场景下的推理能力。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has demonstrated
strong semantic understanding capabilities, but struggles to perform precise
spatio-temporal understanding. Existing spatio-temporal methods primarily focus
on the video itself, while overlooking the physical information within the
video, such as multi-object layouts and motion. Such limitations restrict the
use of MLLMs in downstream applications that demand high precision, including
embodied intelligence and VR. To address this issue, we present Video-STR, a
novel graph-based reinforcement method for precise Video Spatio-Temporal
Reasoning. Building upon the capacity of Reinforcement Learning with Verifiable
Reward (RLVR) to improve model abilities, we introduce a reasoning mechanism
using graph-based Group Relative Policy Optimization (GRPO) method to guide the
model in inferring the underlying spatio-temporal topology of scenarios during
the thinking process. To resolve the lack of spatio-temporal training data, we
construct the STV-205k dataset with 205k question-answering pairs, covering
dynamic multi-object scenes in both indoor and outdoor environments, to support
the model training. Experiments show that Video-STR achieves state-of-the-art
results on various benchmarks, outperforming the base model by 13% on
STI-Bench, and demonstrating the effectiveness of our approach and dataset.
Code, model, and data will be released.

</details>


### [170] [Revisiting Model Interpolation for Efficient Reasoning](https://arxiv.org/abs/2510.10977)
*Taiqiang Wu,Runming Yang,Tao Liu,Jiahao Wang,Ngai Wong*

Main category: cs.AI

TL;DR: 本文系统地重新审视了最简单的模型权重插值方法，发现其遵循三阶段演化范式。研究表明，策略性插值模型在效率和有效性上意外超越了复杂的模型融合基线，为构建目标推理模型提供了实用框架。


<details>
  <summary>Details</summary>
Motivation: 模型融合在高效推理方面表现出色，但本文旨在系统地重新审视最简单的权重插值方法，以揭示其深层机制，并探索其在性能-成本权衡中的潜力。

Method: 系统地重新审视并分析直接权重插值这一最简单的模型融合方法。观察并识别模型插值在推理轨迹上的三阶段演化范式。通过广泛的消融研究（涉及模型层、模块和解码策略）来验证研究发现。

Result: 观察到模型插值遵循具有独特行为的三阶段演化范式。经验结果表明，经过策略性插值的模型在效率和有效性方面，出人意料地超越了更复杂的模型融合基线。

Conclusion: 本研究揭示了模型插值的奥秘，并提供了一个实用的框架，用于构建具有精确目标推理能力的模型。这些动态特性为平衡性能与成本提供了指导原则。

Abstract: Model merging, typically on Instruct and Thinking models, has shown
remarkable performance for efficient reasoning. In this paper, we
systematically revisit the simplest merging method that interpolates two
weights directly. Particularly, we observe that model interpolation follows a
three-stage evolutionary paradigm with distinct behaviors on the reasoning
trajectory. These dynamics provide a principled guide for navigating the
performance-cost trade-off. Empirical results demonstrate that a strategically
interpolated model surprisingly surpasses sophisticated model merging baselines
on both efficiency and effectiveness. We further validate our findings with
extensive ablation studies on model layers, modules, and decoding strategies.
Ultimately, this work demystifies model interpolation and offers a practical
framework for crafting models with precisely targeted reasoning capabilities.
Code is available at \href{https://github.com/wutaiqiang/MI}{Github}.

</details>


### [171] [FBS Model-based Maintenance Record Accumulation for Failure-Cause Inference in Manufacturing Systems](https://arxiv.org/abs/2510.11003)
*Takuma Fujiu,Sho Okazaki,Kohei Kaminishi,Yuji Nakata,Shota Hamamoto,Kenshin Yokose,Tatsunori Hara,Yasushi Umeda,Jun Ota*

Main category: cs.AI

TL;DR: 该研究构建了诊断知识本体并提出了一种基于FBS模型的维护记录积累方法，用于制造系统故障原因推断，在困难案例中与专家判断表现出更好的一致性。


<details>
  <summary>Details</summary>
Motivation: 在制造系统中，识别故障原因对维护和提高生产效率至关重要。知识库故障原因推断需要明确的系统和故障知识结构，以及足够长的故障因果链。

Method: 本研究构建了诊断知识本体（Diagnostic Knowledge Ontology），并基于此提出了一种功能-行为-结构（FBS）模型维护记录积累方法。

Result: 使用所提出的方法积累的维护记录进行故障原因推断时，与专家列举的候选原因集表现出更好的一致性，尤其是在相关案例较少且使用的词汇不同的困难情况下。

Conclusion: 该方法利用设计阶段的理解和知识来支持维护阶段的知识积累和问题解决，有望成为未来整个工程链知识共享的基础。未来的工作包括开发定制的推断方法、构建用户界面以及在更大、更多样化的系统上进行验证。

Abstract: In manufacturing systems, identifying the causes of failures is crucial for
maintaining and improving production efficiency. In knowledge-based
failure-cause inference, it is important that the knowledge base (1) explicitly
structures knowledge about the target system and about failures, and (2)
contains sufficiently long causal chains of failures. In this study, we
constructed Diagnostic Knowledge Ontology and proposed a
Function-Behavior-Structure (FBS) model-based maintenance-record accumulation
method based on it. Failure-cause inference using the maintenance records
accumulated by the proposed method showed better agreement with the set of
candidate causes enumerated by experts, especially in difficult cases where the
number of related cases is small and the vocabulary used differs. In the
future, it will be necessary to develop inference methods tailored to these
maintenance records, build a user interface, and carry out validation on larger
and more diverse systems. Additionally, this approach leverages the
understanding and knowledge of the target in the design phase to support
knowledge accumulation and problem solving during the maintenance phase, and it
is expected to become a foundation for knowledge sharing across the entire
engineering chain in the future.

</details>


### [172] [Argumentation-Based Explainability for Legal AI: Comparative and Regulatory Perspectives](https://arxiv.org/abs/2510.11079)
*Andrada Iulia Prajescu,Roberto Confalonieri*

Main category: cs.AI

TL;DR: 本文提出计算论证模型是解决法律AI“黑箱问题”的有效方法，能提供符合GDPR和AIA等法规的、有意义的法律解释，并强调其在法律领域透明度的优势。


<details>
  <summary>Details</summary>
Motivation: 人工智能在法律领域的应用日益广泛，但其不透明性（“黑箱问题”）严重损害了公平性、问责制和信任，使自动化决策的合法性受质疑，受影响个体缺乏有意义的解释，亟需提升法律AI的透明度。

Method: 本文推广计算论证模型在法律解释中的应用，分析了不同解释策略的优缺点，评估了它们在法律推理中的适用性，并强调论证框架（能捕捉法律的可驳斥性、可争议性和价值敏感性）作为可解释法律AI基础的优势，并将其与GDPR和AIA等法规对齐。

Result: 研究发现，论证框架通过捕捉法律的复杂特性，为可解释的法律AI提供了特别坚实的基础。计算论证被认为是满足法律领域透明度技术和规范要求的最佳选择。

Conclusion: 计算论证是解决法律AI不透明性挑战的关键，它能提供符合新兴监管框架的法律解释，并有潜力应对偏见缓解、实证验证和伦理法律合规等开放性挑战。

Abstract: Artificial Intelligence (AI) systems are increasingly deployed in legal
contexts, where their opacity raises significant challenges for fairness,
accountability, and trust. The so-called ``black box problem'' undermines the
legitimacy of automated decision-making, as affected individuals often lack
access to meaningful explanations. In response, the field of Explainable AI
(XAI) has proposed a variety of methods to enhance transparency, ranging from
example-based and rule-based techniques to hybrid and argumentation-based
approaches. This paper promotes computational models of arguments and their
role in providing legally relevant explanations, with particular attention to
their alignment with emerging regulatory frameworks such as the EU General Data
Protection Regulation (GDPR) and the Artificial Intelligence Act (AIA). We
analyze the strengths and limitations of different explanation strategies,
evaluate their applicability to legal reasoning, and highlight how
argumentation frameworks -- by capturing the defeasible, contestable, and
value-sensitive nature of law -- offer a particularly robust foundation for
explainable legal AI. Finally, we identify open challenges and research
directions, including bias mitigation, empirical validation in judicial
settings, and compliance with evolving ethical and legal standards, arguing
that computational argumentation is best positioned to meet both technical and
normative requirements of transparency in the law domain.

</details>


### [173] [Modeling AI-Driven Production and Competitiveness A Multi-Agent Economic Simulation of China and the United States](https://arxiv.org/abs/2510.11085)
*Yuxinyue Qian,Jun Liu*

Main category: cs.AI

TL;DR: 本文利用多层智能代理经济模型模拟比较了中美在AI协作、网络效应和AI自主生产机制下的宏观经济产出演变，发现AI作为独立生产实体能显著提高社会产出增长率，且中国在智能代理普及和技术追赶上具有加速潜力。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，社会经济系统正进入“人机共创”新阶段，迫切需要系统性地理解AI驱动的生产系统转型和国际竞争力变化，并为相关政策制定提供量化见解。

Method: 基于先前建立的多层智能代理经济模型，通过模拟比较中国和美国在AI协作、网络效应和AI自主生产等不同机制下的宏观经济产出演变。

Result: (1) 当AI作为独立的生产实体时，社会总产出的增长率远超传统基于人工劳动的模型；(2) 中国在智能代理群体扩展和技术追赶速度方面展现出明显的加速潜力，有望实现技术趋同甚至局部超越。

Conclusion: 本研究提供了一个系统的、基于模型的分析框架，用于理解AI驱动的生产系统转型和国际竞争力变化，并为相关政策制定提供了量化见解。

Abstract: With the rapid development of artificial intelligence (AI) technology,
socio-economic systems are entering a new stage of "human-AI co-creation."
Building upon a previously established multi-level intelligent agent economic
model, this paper conducts simulation-based comparisons of macroeconomic output
evolution in China and the United States under different mechanisms-AI
collaboration, network effects, and AI autonomous production. The results show
that: (1) when AI functions as an independent productive entity, the overall
growth rate of social output far exceeds that of traditional human-labor-based
models; (2) China demonstrates clear potential for acceleration in both the
expansion of intelligent agent populations and the pace of technological
catch-up, offering the possibility of achieving technological convergence or
even partial surpassing. This study provides a systematic, model-based
analytical framework for understanding AI-driven production system
transformation and shifts in international competitiveness, as well as
quantitative insights for relevant policy formulation.

</details>


### [174] [Improving AI Efficiency in Data Centres by Power Dynamic Response](https://arxiv.org/abs/2510.11119)
*Andrea Marinoni,Sai Shivareddy,Pietro Lio',Weisi Lin,Erik Cambria,Clare Grey*

Main category: cs.AI

TL;DR: 本文提出并研究了一种创新的动态输入电源管理方法，旨在提升AI数据中心的能源效率和可持续性。


<details>
  <summary>Details</summary>
Motivation: AI的快速发展导致数据中心对电力需求巨大，其功耗对环境和可持续发展构成挑战，因此需要鲁棒可靠的电力基础设施和有效的电源管理方案。

Method: 通过将部分输入电源动态化以匹配数据计算功能，作者量化并比较了无源和有源器件在计算增益、能源效率、资本支出和管理成本方面的表现，并分析了来自全球多个数据平台的电力趋势。

Result: 该策略代表了AI数据中心电源管理领域的范式转变，有望显著提高AI超大规模企业的可持续性，并改善其在环境、财务和社会方面的影响。

Conclusion: 动态电源管理方法能有效解决AI数据中心高能耗问题，提升其可持续性，带来环境、经济和社会的综合效益。

Abstract: The steady growth of artificial intelligence (AI) has accelerated in the
recent years, facilitated by the development of sophisticated models such as
large language models and foundation models. Ensuring robust and reliable power
infrastructures is fundamental to take advantage of the full potential of AI.
However, AI data centres are extremely hungry for power, putting the problem of
their power management in the spotlight, especially with respect to their
impact on environment and sustainable development. In this work, we investigate
the capacity and limits of solutions based on an innovative approach for the
power management of AI data centres, i.e., making part of the input power as
dynamic as the power used for data-computing functions. The performance of
passive and active devices are quantified and compared in terms of
computational gain, energy efficiency, reduction of capital expenditure, and
management costs by analysing power trends from multiple data platforms
worldwide. This strategy, which identifies a paradigm shift in the AI data
centre power management, has the potential to strongly improve the
sustainability of AI hyperscalers, enhancing their footprint on environmental,
financial, and societal fields.

</details>


### [175] [Spec-Driven AI for Science: The ARIA Framework for Automated and Reproducible Data Analysis](https://arxiv.org/abs/2510.11143)
*Chuke Chen,Biao Luo,Nan Li,Boxiang Wang,Hang Yang,Jing Guo,Ming Xu*

Main category: cs.AI

TL;DR: ARIA是一个规范驱动、人机协作的AI助手，旨在弥合科学数据分析能力与研究意图之间的差距。它通过自然语言规范自动化生成可执行代码，实现可解释的数据分析，并在多领域展现出高预测准确性、效率和可解释性，为透明、协作和可重复的科学发现建立了新范式。


<details>
  <summary>Details</summary>
Motivation: 科学数据快速增长，导致现有分析能力与研究意图之间出现差距。现有AI分析工具要么偏重自动化而牺牲透明度，要么依赖手动脚本，限制了其可扩展性和可重复性。

Method: 本文提出了ARIA（自动化研究智能助手），一个规范驱动、人机协作的框架，用于自动化和可解释的数据分析。ARIA整合了六个可互操作的层（命令、上下文、代码、数据、编排、AI模块），通过以文档为中心的工作流统一人类推理和机器执行。研究人员通过自然语言规范定义分析目标，ARIA则自主生成可执行代码、验证计算并生成透明文档。

Result: ARIA不仅实现了高预测准确性，还能快速识别最佳特征集和选择合适模型，从而最大限度地减少冗余调优和重复实验。在Boston Housing案例中，ARIA发现了25个关键特征，并确定XGBoost为最佳模型（R平方=0.93），且过拟合极小。跨异构领域的评估表明，与现有最先进系统相比，ARIA展示出强大的性能、可解释性和效率。

Conclusion: 通过在一个规范驱动的架构中结合研究AI和科学AI的原则，ARIA为透明、协作和可重复的科学发现建立了一种新范式。

Abstract: The rapid expansion of scientific data has widened the gap between analytical
capability and research intent. Existing AI-based analysis tools, ranging from
AutoML frameworks to agentic research assistants, either favor automation over
transparency or depend on manual scripting that hinders scalability and
reproducibility. We present ARIA (Automated Research Intelligence Assistant), a
spec-driven, human-in-the-loop framework for automated and interpretable data
analysis. ARIA integrates six interoperable layers, namely Command, Context,
Code, Data, Orchestration, and AI Module, within a document-centric workflow
that unifies human reasoning and machine execution. Through natural-language
specifications, researchers define analytical goals while ARIA autonomously
generates executable code, validates computations, and produces transparent
documentation. Beyond achieving high predictive accuracy, ARIA can rapidly
identify optimal feature sets and select suitable models, minimizing redundant
tuning and repetitive experimentation. In the Boston Housing case, ARIA
discovered 25 key features and determined XGBoost as the best performing model
(R square = 0.93) with minimal overfitting. Evaluations across heterogeneous
domains demonstrate ARIA's strong performance, interpretability, and efficiency
compared with state-of-the-art systems. By combining AI for research and AI for
science principles within a spec-driven architecture, ARIA establishes a new
paradigm for transparent, collaborative, and reproducible scientific discovery.

</details>


### [176] [$How^{2}$: How to learn from procedural How-to questions](https://arxiv.org/abs/2510.11144)
*Gautier Dagan,Frank Keller,Alex Lascarides*

Main category: cs.AI

TL;DR: 本文提出$How^2$记忆智能体框架，旨在解决智能体在规划中如何高效利用“如何做”问题答案的挑战。该框架使智能体能在交互环境中提问、存储并重用答案以实现终身学习，并发现抽象且与当前状态解耦的答案对规划最有效。


<details>
  <summary>Details</summary>
Motivation: 智能体在规划问题中可利用“如何做”的答案来减少不确定性、填补知识空白，但这类问题的开放性（答案范围广，从具体动作到高层子目标）使得AI智能体难以有效提问，专家也难以高效回答，从而阻碍了高效规划的实现。

Method: 本文引入$How^2$记忆智能体框架，该框架使智能体能够在交互式环境中提出“如何做”的问题，存储并重用这些答案以实现终身学习。研究在Minecraft的Plancraft合成环境中进行评估，智能体需通过操作物品完成组装任务。

Result: 通过使用能提供不同抽象级别（从可执行动作序列到高层子目标描述）答案的教师模型进行评估，研究表明终身学习智能体从抽象且与当前状态解耦的答案中获益最大。

Conclusion: $How^2$为基于大型语言模型（LLM）的智能体提供了一种在交互环境中通过提问来随时间提升其规划能力的有效方法。

Abstract: An agent facing a planning problem can use answers to how-to questions to
reduce uncertainty and fill knowledge gaps, helping it solve both current and
future tasks. However, their open ended nature, where valid answers to "How do
I X?" range from executable actions to high-level descriptions of X's
sub-goals, makes them challenging for AI agents to ask, and for AI experts to
answer, in ways that support efficient planning. We introduce $How^{2}$, a
memory agent framework that enables agents to ask how-to questions, store the
answers, and reuse them for lifelong learning in interactive environments. We
evaluate our approach in Plancraft, a Minecraft crafting environment, where
agents must complete an assembly task by manipulating inventory items. Using
teacher models that answer at varying levels of abstraction, from executable
action sequences to high-level subgoal descriptions, we show that lifelong
learning agents benefit most from answers that are abstracted and decoupled
from the current state. $How^{2}$ offers a way for LLM-based agents to improve
their planning capabilities over time by asking questions in interactive
environments.

</details>


### [177] [Aligning Deep Implicit Preferences by Learning to Reason Defensively](https://arxiv.org/abs/2510.11194)
*Peiming Li,Zhiyuan Hu,Yang Tang,Shiyu Li,Xi Chen*

Main category: cs.AI

TL;DR: 本文提出批判驱动推理对齐（CDRA），通过结构化推理过程解决了大语言模型个性化对齐中用户深层偏好推断不足和缺乏防御性推理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型个性化对齐方法无法推断用户深层隐含偏好（包括未说明的目标、语义上下文和风险容忍度），且缺乏应对现实世界模糊性的防御性推理能力，导致模型响应肤浅、脆弱和短视。

Method: CDRA将对齐重构为结构化推理过程。具体方法包括：1. 引入DeepPref基准数据集（包含3000个批判注释推理链），以弥补偏好推断差距。2. 提出个性化生成过程奖励模型（Pers-GenPRM），通过生成批判链评估响应，从而灌输防御性推理。3. 利用这种可解释、结构化的奖励信号，通过批判驱动策略对齐（一种整合数值和自然语言反馈的在线强化学习算法）来指导策略模型。

Result: 实验证明，CDRA在发现并与用户真实偏好对齐方面表现出色，同时能执行鲁棒的推理。

Conclusion: CDRA有效提升了LLM的个性化对齐能力，使其能更好地理解和满足用户深层偏好，并具备在复杂情境下进行鲁棒推理的能力。

Abstract: Personalized alignment is crucial for enabling Large Language Models (LLMs)
to engage effectively in user-centric interactions. However, current methods
face a dual challenge: they fail to infer users' deep implicit preferences
(including unstated goals, semantic context and risk tolerances), and they lack
the defensive reasoning required to navigate real-world ambiguity. This
cognitive gap leads to responses that are superficial, brittle and
short-sighted. To address this, we propose Critique-Driven Reasoning Alignment
(CDRA), which reframes alignment from a scalar reward-matching task into a
structured reasoning process. First, to bridge the preference inference gap, we
introduce the DeepPref benchmark. This dataset, comprising 3000
preference-query pairs across 20 topics, is curated by simulating a
multi-faceted cognitive council that produces critique-annotated reasoning
chains to deconstruct query semantics and reveal latent risks. Second, to
instill defensive reasoning, we introduce the Personalized Generative Process
Reward Model (Pers-GenPRM), which frames reward modeling as a personalized
reasoning task. It generates a critique chain to evaluate a response's
alignment with user preferences before outputting a final score based on this
rationale. Ultimately, this interpretable, structured reward signal guides
policy model through Critique-Driven Policy Alignment, a process-level online
reinforcement learning algorithm integrating both numerical and natural
language feedback. Experiments demonstrate that CDRA excels at discovering and
aligning with users' true preferences while executing robust reasoning. Our
code and dataset are available at https://github.com/Zephyrian-Hugh/Deep-pref.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [178] [Direct Routing Gradient (DRGrad): A Personalized Information Surgery for Multi-Task Learning (MTL) Recommendations](https://arxiv.org/abs/2510.09643)
*Yuguang Liu,Yiyun Miao,Luyao Xia*

Main category: cs.LG

TL;DR: 本文提出了一种名为DRGrad的个性化直接路由梯度框架，通过判断任务优先级并路由梯度来解决多任务学习（MTL）在推荐系统中面临的负迁移和跷跷板现象，同时利用个性化信息提高性能。


<details>
  <summary>Details</summary>
Motivation: 多任务学习在工业级推荐系统中表现出色，但由于任务间复杂且常矛盾的关联性，面临负迁移和跷跷板现象的挑战。研究旨在解决这些问题，并更好地利用个性化信息。

Method: 提出了个性化直接路由梯度框架（DRGrad），包含路由器、更新器和个性化门网络三个关键组件。DRGrad在训练过程中判断任务间的优先级，并利用所有有效梯度以减少任务冲突。

Result: DRGrad在包含150亿样本的真实世界推荐数据集上，相较于最先进的多任务学习模型，展现出卓越的性能，特别是在AUC指标上。它能有效管理多任务学习环境中的任务冲突和噪声处理不足，且不增加模型复杂性。在公共Census-income和Synthetic数据集上的实验也验证了其在处理不同相关性和个性化程度任务时的能力。

Conclusion: DRGrad框架通过个性化的梯度路由有效解决了多任务学习在推荐系统中面临的任务冲突和噪声问题，显著提升了模型性能，且未增加复杂性，同时能适应不同任务关联度。

Abstract: Multi-task learning (MTL) has emerged as a successful strategy in
industrial-scale recommender systems, offering significant advantages such as
capturing diverse users' interests and accurately detecting different behaviors
like ``click" or ``dwell time". However, negative transfer and the seesaw
phenomenon pose challenges to MTL models due to the complex and often
contradictory task correlations in real-world recommendations. To address the
problem while making better use of personalized information, we propose a
personalized Direct Routing Gradient framework (DRGrad), which consists of
three key components: router, updater and personalized gate network. DRGrad
judges the stakes between tasks in the training process, which can leverage all
valid gradients for the respective task to reduce conflicts. We evaluate the
efficiency of DRGrad on complex MTL using a real-world recommendation dataset
with 15 billion samples. The results show that DRGrad's superior performance
over competing state-of-the-art MTL models, especially in terms of AUC (Area
Under the Curve) metrics, indicating that it effectively manages task conflicts
in multi-task learning environments without increasing model complexity, while
also addressing the deficiencies in noise processing. Moreover, experiments on
the public Census-income dataset and Synthetic dataset, have demonstrated the
capability of DRGrad in judging and routing the stakes between tasks with
varying degrees of correlation and personalization.

</details>


### [179] [Enhanced Urban Traffic Management Using CCTV Surveillance Videos and Multi-Source Data Current State Prediction and Frequent Episode Mining](https://arxiv.org/abs/2510.09644)
*Shaharyar Alam Ansari,Mohammad Luqman,Aasim Zafar,Savir Ali*

Main category: cs.LG

TL;DR: 本研究提出了一个结合CCTV视频与多源数据的统一框架，通过时空特征融合和混合LSTM-Transformer模型，显著提升了实时城市交通预测准确性，并能有效进行拥堵管理。


<details>
  <summary>Details</summary>
Motivation: 快速城市化导致交通拥堵、环境压力及交通系统效率低下。传统静态系统无法应对动态交通需求，急需智能自适应的交通管理方案。

Method: 开发了一个统一框架，整合CCTV监控视频与多源数据描述符。该方法结合了时空特征融合、用于发现序列交通模式的频繁事件挖掘（FEM），以及用于交通状态预测的混合LSTM-Transformer模型。

Result: 在CityFlowV2数据集上评估，预测准确率达到98.46%，宏观精确率0.9800，召回率0.9839，F1分数0.9819。FEM分析揭示了显著的顺序模式（如中度拥堵转换置信度超55%），系统生成了46个持续拥堵警报，展示了主动拥堵管理的实用价值。

Conclusion: 强调将视频流分析与多源数据结合，对于设计实时、响应迅速、自适应的多级智能交通系统至关重要，能使城市出行更智能、更安全。

Abstract: Rapid urbanization has intensified traffic congestion, environmental strain,
and inefficiencies in transportation systems, creating an urgent need for
intelligent and adaptive traffic management solutions. Conventional systems
relying on static signals and manual monitoring are inadequate for the dynamic
nature of modern traffic. This research aims to develop a unified framework
that integrates CCTV surveillance videos with multi-source data descriptors to
enhance real-time urban traffic prediction. The proposed methodology
incorporates spatio-temporal feature fusion, Frequent Episode Mining for
sequential traffic pattern discovery, and a hybrid LSTM-Transformer model for
robust traffic state forecasting. The framework was evaluated on the CityFlowV2
dataset comprising 313,931 annotated bounding boxes across 46 cameras. It
achieved a high prediction accuracy of 98.46 percent, with a macro precision of
0.9800, macro recall of 0.9839, and macro F1-score of 0.9819. FEM analysis
revealed significant sequential patterns such as moderate-congested transitions
with confidence levels exceeding 55 percent. The 46 sustained congestion alerts
are system-generated, which shows practical value for proactive congestion
management. This emphasizes the need for the incorporation of video stream
analytics with data from multiple sources for the design of real-time,
responsive, adaptable multi-level intelligent transportation systems, which
makes urban mobility smarter and safer.

</details>


### [180] [Generative Models for Helmholtz Equation Solutions: A Dataset of Acoustic Materials](https://arxiv.org/abs/2510.09657)
*Riccardo Fosco Gramaccioni,Christian Marinoni,Fabrizio Frezza,Aurelio Uncini,Danilo Comminiello*

Main category: cs.LG

TL;DR: 针对复杂声学材料波传播模拟计算昂贵的问题，本文引入了HA30K数据集，并采用基于Stable Diffusion与ControlNet的深度学习方法，通过将解表示为图像，实现了赫姆霍兹方程解的快速预测，证明了深度学习在快速探索阶段的价值。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器（如有限元方法）在复杂声学材料的波传播模拟中计算成本高昂，尤其在大规模或实时场景下，限制了其应用。

Method: 1. 构建了包含31,000种声学材料及其赫姆霍兹方程压力场解的HA30K数据集。2. 采用基于Stable Diffusion与ControlNet的深度学习方法，将几何配置作为输入，将压力场解表示为图像进行预测，利用GPU并行化加速计算。

Result: 该方法显著减少了计算时间，绕过了复杂仿真软件和显式方程求解的需求。推理时可调整扩散步数以平衡预测速度和质量。

Conclusion: 深度学习方法在早期研究阶段具有显著优势，在此阶段，快速探索比绝对精度更为关键。

Abstract: Accurate simulation of wave propagation in complex acoustic materials is
crucial for applications in sound design, noise control, and material
engineering. Traditional numerical solvers, such as finite element methods, are
computationally expensive, especially when dealing with large-scale or
real-time scenarios. In this work, we introduce a dataset of 31,000 acoustic
materials, named HA30K, designed and simulated solving the Helmholtz equations.
For each material, we provide the geometric configuration and the corresponding
pressure field solution, enabling data-driven approaches to learn Helmholtz
equation solutions. As a baseline, we explore a deep learning approach based on
Stable Diffusion with ControlNet, a state-of-the-art model for image
generation. Unlike classical solvers, our approach leverages GPU
parallelization to process multiple simulations simultaneously, drastically
reducing computation time. By representing solutions as images, we bypass the
need for complex simulation software and explicit equation-solving.
Additionally, the number of diffusion steps can be adjusted at inference time,
balancing speed and quality. We aim to demonstrate that deep learning-based
methods are particularly useful in early-stage research, where rapid
exploration is more critical than absolute accuracy.

</details>


### [181] [Gradient-Sign Masking for Task Vector Transport Across Pre-Trained Models](https://arxiv.org/abs/2510.09658)
*Filippo Rinaldi,Aniello Panariello,Giacomo Salici,Fengyuan Liu,Marco Ciccone,Angelo Porrello,Simone Calderara*

Main category: cs.LG

TL;DR: 提出GradFix，一种基于梯度符号结构的方法，用于在不同基础模型间高效转移任务向量，无需全面微调，仅需少量样本即可实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当基础模型发布新版本时，即使任务相同，也通常需要重复进行全面微调，效率低下。现有的任务向量（参数变化）方法因参数空间不匹配，难以在不同预训练模型间有效迁移。

Method: 本研究发现成功迁移的关键在于新模型的梯度符号结构。基于此，提出GradFix方法，通过近似理想梯度符号结构并利用它来转移知识。该方法通过计算目标模型的少量梯度，并据此掩盖源任务向量，实现对新模型的适应，无需额外微调，仅需少量标注样本。

Result: 理论上，该方法能确保一阶下降。经验上，在视觉和语言基准测试中，GradFix表现出显著的性能提升，持续优于朴素任务向量叠加和少量样本微调。

Conclusion: GradFix成功解决了任务向量在不同基础模型间迁移的难题，通过利用梯度符号结构，实现了高效、高性能且无需全面微调的知识转移，有效将任务向量重定向到新的预训练模型上。

Abstract: When a new release of a foundation model is published, practitioners
typically need to repeat full fine-tuning, even if the same task has already
been solved in the previous version. A promising alternative is to reuse the
parameter changes (i.e., task vectors) that capture how a model adapts to a
specific task. However, they often fail to transfer across different
pre-trained models due to their misaligned parameter space. In this work, we
show that the key to successful transfer lies in the sign structure of the
gradients of the new model. Based on this insight, we propose GradFix, a novel
method that approximates the ideal gradient sign structure and leverages it to
transfer knowledge using only a handful of labeled samples. Notably, this
requires no additional fine-tuning: the adaptation is achieved by computing a
few gradients at the target model and masking the source task vector
accordingly. This yields an update that is locally aligned with the target loss
landscape, effectively rebasing the task vector onto the new pre-training. We
provide a theoretical guarantee that our method ensures first-order descent.
Empirically, we demonstrate significant performance gains on vision and
language benchmarks, consistently outperforming naive task vector addition and
few-shot fine-tuning.

</details>


### [182] [Heterogeneous Point Set Transformers for Segmentation of Multiple View Particle Detectors](https://arxiv.org/abs/2510.09659)
*Edgar E. Robles,Dikshant Sagar,Alejandro Yankelevich,Jianming Bian,Pierre Baldi,NOvA Collaboration*

Main category: cs.LG

TL;DR: 本文提出一种新的点集神经网络模型，用于NOvA实验中稀疏2D中微子探测器数据的粒子识别任务，该模型通过融合两个视图的信息，显著降低内存消耗并提高识别精度。


<details>
  <summary>Details</summary>
Motivation: NOvA中微子实验的数据分析前，需要将探测器中的原始信号匹配到源粒子并识别粒子类型。传统方法（聚类与CNNs结合）处理稀疏2D数据（XZ和YZ视图）存在效率或精度问题，且数据非3D表示增加了处理难度。

Method: 提出了一种点集神经网络，该网络直接操作稀疏矩阵，并通过一种操作混合了来自XZ和YZ两个探测器视图的信息，而非独立处理。

Result: 该模型实现了96.8%的AUC分数，高于独立处理两个视图（85.4%）的得分，并且内存消耗不到以往方法的10%。

Conclusion: 所提出的点集神经网络为NOvA实验的粒子识别任务提供了一种高效且更准确的解决方案，尤其在处理稀疏、多视图数据方面表现出色，显著优化了资源使用和性能。

Abstract: NOvA is a long-baseline neutrino oscillation experiment that detects neutrino
particles from the NuMI beam at Fermilab. Before data from this experiment can
be used in analyses, raw hits in the detector must be matched to their source
particles, and the type of each particle must be identified. This task has
commonly been done using a mix of traditional clustering approaches and
convolutional neural networks (CNNs). Due to the construction of the detector,
the data is presented as two sparse 2D images: an XZ and a YZ view of the
detector, rather than a 3D representation. We propose a point set neural
network that operates on the sparse matrices with an operation that mixes
information from both views. Our model uses less than 10% of the memory
required using previous methods while achieving a 96.8% AUC score, a higher
score than obtained when both views are processed independently (85.4%).

</details>


### [183] [Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise](https://arxiv.org/abs/2510.09660)
*Luca Scimeca,Thomas Jiralerspong,Berton Earnshaw,Jason Hartford,Yoshua Bengio*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Diffusion Probabilistic Models (DPMs) have achieved strong generative
performance, yet their inductive biases remain largely implicit. In this work,
we aim to build inductive biases into the training and sampling of diffusion
models to better accommodate the target distribution of the data to model. We
introduce an anisotropic noise operator that shapes these biases by replacing
the isotropic forward covariance with a structured, frequency-diagonal
covariance. This operator unifies band-pass masks and power-law weightings,
allowing us to emphasize or suppress designated frequency bands, while keeping
the forward process Gaussian. We refer to this as spectrally anisotropic
Gaussian diffusion (SAGD). In this work, we derive the score relation for
anisotropic covariances and show that, under full support, the learned score
converges to the true data score as $t\!\to\!0$, while anisotropy reshapes the
probability-flow path from noise to data. Empirically, we show the induced
anisotropy outperforms standard diffusion across several vision datasets, and
enables selective omission: learning while ignoring known corruptions confined
to specific bands. Together, these results demonstrate that carefully designed
anisotropic forward noise provides a simple, yet principled, handle to tailor
inductive bias in DPMs.

</details>


### [184] [Assessment of different loss functions for fitting equivalent circuit models to electrochemical impedance spectroscopy data](https://arxiv.org/abs/2510.09662)
*Ali Jaberi,Amin Sadeghi,Runze Zhang,Zhaoyang Zhao,Qiuyu Shi,Robert Black,Zoya Sadighi,Jason Hattrick-Simpers*

Main category: cs.LG

TL;DR: 本文评估了EIS数据拟合中不同损失函数（包括新提出的log-B和log-BW）的性能，发现X2损失函数在拟合质量上表现最佳，而log-B在计算效率和元件精度上具有优势，适用于大规模应用。


<details>
  <summary>Details</summary>
Motivation: EIS数据通常使用等效电路模型通过非线性最小二乘法拟合，但损失函数的选择对拟合性能有显著影响。研究旨在评估新的损失函数，并找出在大规模数据驱动应用中表现更优的替代方案。

Method: 引入了两种基于Bode表示的新损失函数log-B和log-BW。使用大型生成EIS数据集，通过R2得分、卡方、计算效率和预测元件值与原始值的MAPE，评估并统计比较了现有和新提出的损失函数。

Result: 损失函数选择影响收敛性、计算效率、拟合质量和MAPE。X2损失函数在多个拟合质量指标上表现最佳。Log-B的拟合质量略低，但速度快约1.4倍，且对大多数电路元件产生更低的MAPE。

Conclusion: 当拟合质量是首要目标时，X2损失函数是最佳选择。Log-B作为一种强大的替代方案，在速度和元件精度方面表现突出，特别适用于大规模数据驱动应用，尽管其整体拟合质量略逊于X2。

Abstract: Electrochemical impedance spectroscopy (EIS) data is typically modeled using
an equivalent circuit model (ECM), with parameters obtained by minimizing a
loss function via nonlinear least squares fitting. This paper introduces two
new loss functions, log-B and log-BW, derived from the Bode representation of
EIS. Using a large dataset of generated EIS data, the performance of proposed
loss functions was evaluated alongside existing ones in terms of R2 scores,
chi-squared, computational efficiency, and the mean absolute percentage error
(MAPE) between the predicted component values and the original values.
Statistical comparisons revealed that the choice of loss function impacts
convergence, computational efficiency, quality of fit, and MAPE. Our analysis
showed that X2 loss function (squared sum of residuals with proportional
weighting) achieved the highest performance across multiple quality of fit
metrics, making it the preferred choice when the quality of fit is the primary
goal. On the other hand, log-B offered a slightly lower quality of fit while
being approximately 1.4 times faster and producing lower MAPE for most circuit
components, making log-B as a strong alternative. This is a critical factor for
large-scale least squares fitting in data-driven applications, such as training
machine learning models on extensive datasets or iterations.

</details>


### [185] [Semantic-Cohesive Knowledge Distillation for Deep Cross-modal Hashing](https://arxiv.org/abs/2510.09664)
*Changchang Sun,Vickie Chen,Yan Yan*

Main category: cs.LG

TL;DR: 提出SODA，一种新的跨模态哈希知识蒸馏方案，通过将多标签作为文本提示并利用教师网络蒸馏语义特征，解决了语义与多模态数据不兼容的问题，性能优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有深度监督跨模态哈希方法存在局限性，即多标签语义提取过程未能与原始多模态数据显式交互，导致学习到的表示层语义信息与异构多模态数据不兼容，进而影响模态间隙的弥合。

Method: 本文提出SODA（语义内聚知识蒸馏方案），将多标签信息引入为新的文本模态并重构为真值标签提示。设计一个跨模态教师网络来有效蒸馏图像与标签模态间的语义特征，为图像模态学习一个良好的哈希空间。该哈希空间作为先验知识指导跨模态学生网络的学习，以全面保留图像与文本模态间的语义相似性。

Result: 在两个基准数据集上进行的广泛实验表明，所提出的SODA模型优于现有最先进的方法。

Conclusion: SODA通过其创新的语义内聚知识蒸馏方案，成功解决了现有深度监督跨模态哈希方法中语义信息与多模态数据不兼容的局限性，并有效提升了跨模态检索性能。

Abstract: Recently, deep supervised cross-modal hashing methods have achieve compelling
success by learning semantic information in a self-supervised way. However,
they still suffer from the key limitation that the multi-label semantic
extraction process fail to explicitly interact with raw multimodal data, making
the learned representation-level semantic information not compatible with the
heterogeneous multimodal data and hindering the performance of bridging
modality gap. To address this limitation, in this paper, we propose a novel
semantic cohesive knowledge distillation scheme for deep cross-modal hashing,
dubbed as SODA. Specifically, the multi-label information is introduced as a
new textual modality and reformulated as a set of ground-truth label prompt,
depicting the semantics presented in the image like the text modality. Then, a
cross-modal teacher network is devised to effectively distill cross-modal
semantic characteristics between image and label modalities and thus learn a
well-mapped Hamming space for image modality. In a sense, such Hamming space
can be regarded as a kind of prior knowledge to guide the learning of
cross-modal student network and comprehensively preserve the semantic
similarities between image and text modality. Extensive experiments on two
benchmark datasets demonstrate the superiority of our model over the
state-of-the-art methods.

</details>


### [186] [LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference](https://arxiv.org/abs/2510.09665)
*Yihua Cheng,Yuhan Liu,Jiayi Yao,Yuwei An,Xiaokun Chen,Shaoting Feng,Yuyang Huang,Samuel Shen,Kuntai Du,Junchen Jiang*

Main category: cs.LG

TL;DR: LMCache是一个高效的开源KV缓存解决方案，通过在不同LLM推理引擎和查询之间共享KV缓存，显著提升资源效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM推理系统将单个引擎和查询独立处理，导致显著的资源效率低下。尽管已有通过重用KV缓存和分解查询来避免冗余计算的提议，但缺乏有效卸载和通信KV缓存的机制来实现这些目标。

Method: LMCache通过以下方式实现目标：1. 提取并存储现代LLM引擎（如vLLM和SGLang）生成的KV缓存。2. 在引擎和查询之间共享KV缓存，支持缓存卸载（跨查询前缀复用）和预填充-解码分解（跨引擎缓存传输）。3. 采用高度优化的KV缓存数据移动（包括批量操作、计算与I/O流水线）、模块化的KV缓存连接器组件以及灵活的控制API（如固定、查找、清理、移动和压缩）来优化性能和部署。

Result: LMCache与vLLM结合使用，在各种工作负载下吞吐量提高了15倍。它已在企业推理系统中得到广泛应用，并形成了活跃的社区。

Conclusion: LMCache通过高效的KV缓存共享，极大地提高了LLM推理系统的资源利用率和吞吐量，为未来的KV缓存解决方案提供了宝贵的经验。

Abstract: Today's LLM inference systems treat individual engines and queries
independently for simplicity, but this causes significant resource
inefficiencies. While there are proposals to avoid redundant computation by
reusing KV caches across queries and to increase GPU utilization by
disaggregating a single query to different engines, their promises cannot be
realized without efficiently offloading and communicating KV cache across LLM
inference engines and queries.
  We present LMCache, the first and so far the most efficient open-source KV
caching solution, which extracts and stores KV caches generated by modern LLM
engines (vLLM and SGLang) and shares the KV caches across engines and queries.
LMCache exposes KV caches in the LLM engine interface, effectively transforming
LLM engines from individual token processors to a collection of engines with KV
cache as the storage and communication medium. In particular, it supports both
cache offloading (prefix reuse across queries) and prefill-decode
disaggregation (cross-engine cache transfer). LMCache's high performance and
wide adoption stem from the following contributions: highly optimized KV cache
data movement with performance optimizations including batched data movement
operations, compute and I/O pipelining; a modular KV cache connector component,
decoupling LMCache from the rapid evolution of inference engines; a first-class
control API, such as pinning, lookup, cleanup, movement, and compression, for
flexible cache orchestration across GPU, CPU, storage, and network layers.
Evaluation shows that combining LMCache with vLLM achieves up to 15x
improvement in throughput across diverse workloads. With a growing community,
LMCache has seen dramatic growth in adoption by enterprise inference systems,
which provides valuable lessons for future KV caching solutions. The source
code of LMCache is at: https://github.com/LMCache/LMCache.

</details>


### [187] [Spatial Uncertainty Quantification in Wildfire Forecasting for Climate-Resilient Emergency Planning](https://arxiv.org/abs/2510.09666)
*Aditya Chakravarty*

Main category: cs.LG

TL;DR: 本研究系统分析了利用多模态地球观测数据进行野火蔓延预测中的空间不确定性，发现高不确定性区域集中在火线周围20-60米的缓冲区内，并识别了主要的不确定性驱动因素。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了全球野火风险，可靠预测至关重要。现有机器学习方法缺乏不确定性量化，这对于风险感知决策至关重要。

Method: 对利用多模态地球观测输入数据进行野火蔓延预测中的空间不确定性进行了首次系统分析。采用了新颖的距离度量来揭示高不确定性区域，并使用特征归因方法识别不确定性驱动因素。

Result: 预测不确定性表现出连贯的空间结构，集中在火灾周边区域。高不确定性区域在预测火线周围形成一致的20-60米缓冲区。植被健康和火灾活动被确定为主要的不确定性驱动因素。

Conclusion: 这项工作有助于建立更强大的野火管理系统，支持社区适应气候变化下日益增长的火灾风险。

Abstract: Climate change is intensifying wildfire risks globally, making reliable
forecasting critical for adaptation strategies. While machine learning shows
promise for wildfire prediction from Earth observation data, current approaches
lack uncertainty quantification essential for risk-aware decision making. We
present the first systematic analysis of spatial uncertainty in wildfire spread
forecasting using multimodal Earth observation inputs. We demonstrate that
predictive uncertainty exhibits coherent spatial structure concentrated near
fire perimeters. Our novel distance metric reveals high-uncertainty regions
form consistent 20-60 meter buffer zones around predicted firelines - directly
applicable for emergency planning. Feature attribution identifies vegetation
health and fire activity as primary uncertainty drivers. This work enables more
robust wildfire management systems supporting communities adapting to
increasing fire risk under climate change.

</details>


### [188] [A Hybrid Computational Intelligence Framework with Metaheuristic Optimization for Drug-Drug Interaction Prediction](https://arxiv.org/abs/2510.09668)
*Maryam Abdollahi Shamami,Babak Teimourpour,Farshad Sharifi*

Main category: cs.LG

TL;DR: 该研究提出了一种可解释且高效的药物-药物相互作用（DDI）预测框架，通过结合分子嵌入、基于规则的临床分数和新型元启发式优化器，实现了高准确性和临床泛化能力。


<details>
  <summary>Details</summary>
Motivation: 药物-药物相互作用是可预防不良事件的主要原因，增加了治疗复杂性和医疗成本。同时，了解哪些药物不发生相互作用对确保安全处方和改善患者预后同样重要，因此需要开发可解释且高效的DDI预测方法。

Method: 该方法整合了现代机器学习和领域知识。它结合了两种互补的分子嵌入（捕获片段结构模式的Mol2Vec和学习上下文化学特征的SMILES-BERT）以及一个无泄漏、基于规则的临床分数（RBScore），以注入药理学知识。随后，使用一种新颖的三阶段元启发式策略（RSmpl-ACO-PSO）优化轻量级神经网络分类器，以平衡全局探索和局部优化。

Result: 在真实世界数据集上的实验表明，该模型具有高预测准确性（在DrugBank上ROC-AUC 0.911，PR-AUC 0.867），并能很好地泛化到临床相关的2型糖尿病队列。研究还表明，嵌入融合、RBScore和优化器对模型的精确性和鲁棒性均有贡献。

Conclusion: 该研究结果为构建可靠、可解释且计算高效的模型提供了一条实用途径，这些模型能够支持更安全的药物治疗和临床决策。

Abstract: Drug-drug interactions (DDIs) are a leading cause of preventable adverse
events, often complicating treatment and increasing healthcare costs. At the
same time, knowing which drugs do not interact is equally important, as such
knowledge supports safer prescriptions and better patient outcomes. In this
study, we propose an interpretable and efficient framework that blends modern
machine learning with domain knowledge to improve DDI prediction. Our approach
combines two complementary molecular embeddings - Mol2Vec, which captures
fragment-level structural patterns, and SMILES-BERT, which learns contextual
chemical features - together with a leakage-free, rule-based clinical score
(RBScore) that injects pharmacological knowledge without relying on interaction
labels. A lightweight neural classifier is then optimized using a novel
three-stage metaheuristic strategy (RSmpl-ACO-PSO), which balances global
exploration and local refinement for stable performance. Experiments on
real-world datasets demonstrate that the model achieves high predictive
accuracy (ROC-AUC 0.911, PR-AUC 0.867 on DrugBank) and generalizes well to a
clinically relevant Type 2 Diabetes Mellitus cohort. Beyond raw performance,
studies show how embedding fusion, RBScore, and the optimizer each contribute
to precision and robustness. Together, these results highlight a practical
pathway for building reliable, interpretable, and computationally efficient
models that can support safer drug therapies and clinical decision-making.

</details>


### [189] [Population synthesis with geographic coordinates](https://arxiv.org/abs/2510.09669)
*Jacopo Lenti,Lorenzo Costantini,Ariadna Fosch,Anna Monticelli,David Scala,Marco Pangallo*

Main category: cs.LG

TL;DR: 为解决现有方法无法生成带精确坐标的合成人口问题，本文提出一种结合Normalizing Flows (NF) 和 Variational Autoencoder (VAE) 的新算法。该算法能有效处理空间数据并与其他特征结合，生成具有真实统计特性的合成人口，性能优于现有基准，为精细地理应用开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以生成具有明确坐标而非粗略地理区域的合成人口，因为经纬度数据与其他连续变量不同，其分布高度不均匀且存在大量空白区域。

Method: 提出一种人口合成算法：首先利用Normalizing Flows (NF) 将空间坐标映射到更规则的潜在空间；随后将此潜在空间特征与其余特征结合，通过Variational Autoencoder (VAE) 生成合成人口，同时学习空间与非空间特征的联合分布。此外，还提出了一个评估框架，衡量空间精度、实用性并确保隐私。

Result: NF+VAE架构在121个多样地理数据集上成功生成了与真实房屋具有相同统计特性的合成房屋，并证明其性能优于包括基于Copula的方法和地理区域内均匀分配等流行基准。

Conclusion: 该方法能够以精细空间分辨率生成地理定位的合成人口，这将为需要详细地理信息的应用（如洪水响应、流行病传播、疏散规划和交通建模）提供关键能力。

Abstract: It is increasingly important to generate synthetic populations with explicit
coordinates rather than coarse geographic areas, yet no established methods
exist to achieve this. One reason is that latitude and longitude differ from
other continuous variables, exhibiting large empty spaces and highly uneven
densities. To address this, we propose a population synthesis algorithm that
first maps spatial coordinates into a more regular latent space using
Normalizing Flows (NF), and then combines them with other features in a
Variational Autoencoder (VAE) to generate synthetic populations. This approach
also learns the joint distribution between spatial and non-spatial features,
exploiting spatial autocorrelations. We demonstrate the method by generating
synthetic homes with the same statistical properties of real homes in 121
datasets, corresponding to diverse geographies. We further propose an
evaluation framework that measures both spatial accuracy and practical utility,
while ensuring privacy preservation. Our results show that the NF+VAE
architecture outperforms popular benchmarks, including copula-based methods and
uniform allocation within geographic areas. The ability to generate geolocated
synthetic populations at fine spatial resolution opens the door to applications
requiring detailed geography, from household responses to floods, to epidemic
spread, evacuation planning, and transport modeling.

</details>


### [190] [A physics-aware deep learning model for shear band formation around collapsing pores in shocked reactive materials](https://arxiv.org/abs/2510.09670)
*Xinlun Cheng,Bingzhe Chen,Joseph Choi,Yen T. Nguyen,Pradeep Seshadri,Mayank Verma,H. S. Udaykumar,Stephen Baek*

Main category: cs.LG

TL;DR: 本研究改进了PARCv2神经网络，用于在弱到中等冲击载荷下预测含能材料中剪切局部化和塑性加热等热点形成机制，表现优于其他物理感知模型，强调了领域特定考量在AI加速模拟工具中的重要性。


<details>
  <summary>Details</summary>
Motivation: 含能材料的冲击-爆轰现象涉及复杂物理过程，尤其在弱到中等冲击条件下，热点形成机制对安全储存和处理至关重要，但相比强冲击条件研究不足。直接数值模拟存在计算挑战。

Method: 改进了Physics-Aware Recurrent Convolutional Neural Network (PARCv2) 的架构，使其能够快速预测剪切局部化和塑性加热。将改进后的PARCv2与傅立叶神经算子和神经常微分方程两种物理感知模型进行了基准测试。

Result: PARCv2在捕捉剪切带形成的空时动力学方面表现出卓越的性能，优于其他两种基准模型。它能快速预测弱到中等冲击下的剪切局部化和塑性加热。

Conclusion: 尽管所有模型都存在一定的失效模式，但研究结果强调了在开发用于反应性材料的稳健AI加速模拟工具时，领域特定考量的重要性。

Abstract: Modeling shock-to-detonation phenomena in energetic materials (EMs) requires
capturing complex physical processes such as strong shocks, rapid changes in
microstructural morphology, and nonlinear dynamics of chemical reaction fronts.
These processes participate in energy localization at hotspots, which initiate
chemical energy release leading to detonation. This study addresses the
formation of hotspots in crystalline EMs subjected to weak-to-moderate shock
loading, which, despite its critical relevance to the safe storage and handling
of EMs, remains underexplored compared to the well-studied strong shock
conditions. To overcome the computational challenges associated with direct
numerical simulations, we advance the Physics-Aware Recurrent Convolutional
Neural Network (PARCv2), which has been shown to be capable of predicting
strong shock responses in EMs. We improved the architecture of PARCv2 to
rapidly predict shear localizations and plastic heating, which play important
roles in the weak-to-moderate shock regime. PARCv2 is benchmarked against two
widely used physics-informed models, namely, Fourier neural operator and neural
ordinary differential equation; we demonstrate its superior performance in
capturing the spatiotemporal dynamics of shear band formation. While all models
exhibit certain failure modes, our findings underscore the importance of
domain-specific considerations in developing robust AI-accelerated simulation
tools for reactive materials.

</details>


### [191] [Coupled Data and Measurement Space Dynamics for Enhanced Diffusion Posterior Sampling](https://arxiv.org/abs/2510.09676)
*Shayan Mohajer Hamidi,En-Hui Yang,Ben Liang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Inverse problems, where the goal is to recover an unknown signal from noisy
or incomplete measurements, are central to applications in medical imaging,
remote sensing, and computational biology. Diffusion models have recently
emerged as powerful priors for solving such problems. However, existing methods
either rely on projection-based techniques that enforce measurement consistency
through heuristic updates, or they approximate the likelihood $p(\boldsymbol{y}
\mid \boldsymbol{x})$, often resulting in artifacts and instability under
complex or high-noise conditions. To address these limitations, we propose a
novel framework called \emph{coupled data and measurement space diffusion
posterior sampling} (C-DPS), which eliminates the need for constraint tuning or
likelihood approximation. C-DPS introduces a forward stochastic process in the
measurement space $\{\boldsymbol{y}_t\}$, evolving in parallel with the
data-space diffusion $\{\boldsymbol{x}_t\}$, which enables the derivation of a
closed-form posterior $p(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t,
\boldsymbol{y}_{t-1})$. This coupling allows for accurate and recursive
sampling based on a well-defined posterior distribution. Empirical results
demonstrate that C-DPS consistently outperforms existing baselines, both
qualitatively and quantitatively, across multiple inverse problem benchmarks.

</details>


### [192] [Using LLMs to Directly Guess Conditional Expectations Can Improve Efficiency in Causal Estimation](https://arxiv.org/abs/2510.09684)
*Chris Engh,P. M. Aronow*

Main category: cs.LG

TL;DR: 本文提出利用LLM生成的预测结果来提升双重机器学习中的因果估计效率，尤其是在高维混杂因素存在的情况下。


<details>
  <summary>Details</summary>
Motivation: 在存在高维混杂因素时，双重机器学习中因果效应估计的准确性受限于条件期望函数估计器的性能，面临维度灾难问题。

Method: 通过将LLM驱动的生成模型（基于历史数据训练）产生的预测结果作为额外预测变量，来提高条件期望函数估计器的性能，优于仅依赖于模型提取的嵌入表示的方法。

Result: 通过一个在线珠宝拍卖小数据集的案例研究，证明了纳入LLM生成的“猜测”作为预测变量可以提高因果估计的效率。

Conclusion: 结合LLM的历史知识和推理能力可以有效克服因果推断中的维度灾难问题，从而提高估计效率。

Abstract: We propose a simple yet effective use of LLM-powered AI tools to improve
causal estimation. In double machine learning, the accuracy of causal estimates
of the effect of a treatment on an outcome in the presence of a
high-dimensional confounder depends on the performance of estimators of
conditional expectation functions. We show that predictions made by generative
models trained on historical data can be used to improve the performance of
these estimators relative to approaches that solely rely on adjusting for
embeddings extracted from these models. We argue that the historical knowledge
and reasoning capacities associated with these generative models can help
overcome curse-of-dimensionality problems in causal inference problems. We
consider a case study using a small dataset of online jewelry auctions, and
demonstrate that inclusion of LLM-generated guesses as predictors can improve
efficiency in estimation.

</details>


### [193] [Deep Neural Networks Inspired by Differential Equations](https://arxiv.org/abs/2510.09685)
*Yongshuai Liu,Lianfang Wang,Kuilin Qin,Qinghua Zhang,Faqiang Wang,Li Cui,Jun Liu,Yuping Duan,Tieyong Zeng*

Main category: cs.LG

TL;DR: 本文综述了受微分方程启发的深度神经网络架构和动态建模方法，旨在解决深度学习在理论理解、可解释性及泛化能力方面的挑战，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 深度学习虽已成为多领域关键技术，但仍面临理论理解、可解释性差和泛化能力不足等挑战。研究者正尝试从微分方程角度寻求统一理论框架和系统设计方法来解决这些问题。

Method: 本文通过对受微分方程启发的深度神经网络架构和动态建模方法进行广泛综述。具体审查了基于常微分方程（ODE）的确定性动态网络模型，以及基于随机微分方程（SDE）的正则化技术和随机动态网络模型。此外，还对这些模型进行了数值比较。

Result: 文章详细展示并比较了基于ODE和SDE的各类深度神经网络模型及其动态建模方法，阐明了它们的特点和性能。

Conclusion: 将微分方程与深度学习相结合，为开发具有更高可解释性和泛化能力的智能计算方法提供了新的视角和有前景的研究方向。

Abstract: Deep learning has become a pivotal technology in fields such as computer
vision, scientific computing, and dynamical systems, significantly advancing
these disciplines. However, neural Networks persistently face challenges
related to theoretical understanding, interpretability, and generalization. To
address these issues, researchers are increasingly adopting a differential
equations perspective to propose a unified theoretical framework and systematic
design methodologies for neural networks. In this paper, we provide an
extensive review of deep neural network architectures and dynamic modeling
methods inspired by differential equations. We specifically examine deep neural
network models and deterministic dynamical network constructs based on ordinary
differential equations (ODEs), as well as regularization techniques and
stochastic dynamical network models informed by stochastic differential
equations (SDEs). We present numerical comparisons of these models to
illustrate their characteristics and performance. Finally, we explore promising
research directions in integrating differential equations with deep learning to
offer new insights for developing intelligent computational methods that boast
enhanced interpretability and generalization capabilities.

</details>


### [194] [On the Occurence of Critical Learning Periods in Neural Networks](https://arxiv.org/abs/2510.09687)
*Stanisław Pawlak*

Main category: cs.LG

TL;DR: 本研究发现，通过简单调整学习超参数（特别是采用循环学习率调度），可以有效避免神经网络训练中的关键学习期和预热启动（warm-starting）性能损失。


<details>
  <summary>Details</summary>
Motivation: 当神经网络在数据不足的情况下启动训练时，会发生关键学习期现象，导致网络可塑性下降，即使后续有大量干净数据也难以达到最佳性能。此外，预热启动也可能带来性能损失。研究旨在探究并解决这些问题。

Method: 复制并扩展了关于关键学习期的原始研究实验范围，并首次将预热启动视为一种缺陷预训练形式。通过采用循环学习率调度作为主要的调整方法来解决关键学习期和预热启动问题。

Result: 实证表明，通过使用循环学习率调度，可以成功避免神经网络训练中的关键学习期以及预热启动导致的性能损失。研究还发现预热启动可以被视为一种缺陷预训练。

Conclusion: 调整学习超参数（特别是采用循环学习率调度）是避免关键学习期和预热启动性能损失的有效策略。这项研究不仅优化了神经网络训练实践，也建立了关键学习期与预热启动训练研究之间的重要联系。

Abstract: This study delves into the plasticity of neural networks, offering empirical
support for the notion that critical learning periods and warm-starting
performance loss can be avoided through simple adjustments to learning
hyperparameters. The critical learning phenomenon emerges when training is
initiated with deficit data. Subsequently, after numerous deficit epochs, the
network's plasticity wanes, impeding its capacity to achieve parity in accuracy
with models trained from scratch, even when extensive clean data training
follows deficit epochs. Building upon seminal research introducing critical
learning periods, we replicate key findings and broaden the experimental scope
of the main experiment from the original work. In addition, we consider a
warm-starting approach and show that it can be seen as a form of deficit
pretraining. In particular, we demonstrate that these problems can be averted
by employing a cyclic learning rate schedule. Our findings not only impact
neural network training practices but also establish a vital link between
critical learning periods and ongoing research on warm-starting neural network
training.

</details>


### [195] [Evaluation of Differential Privacy Mechanisms on Federated Learning](https://arxiv.org/abs/2510.09691)
*Tejash Varsani*

Main category: cs.LG

TL;DR: 本文提出在联邦学习中结合自适应差分隐私预算和自适应梯度裁剪，以减少隐私保护引入的噪声，从而在保障隐私的同时维持模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中采用固定差分隐私预算在模型收敛时会引入过多噪声，严重损害模型性能，因此需要更动态的隐私保护机制来平衡隐私与效用。

Method: 通过扩展SelecEval模拟器，实现了基于Laplace和Gaussian机制的自适应差分隐私方法，并引入了Gaussian机制中的自适应梯度裁剪技术，以动态调整模型梯度而非使用固定敏感度。

Result: 实验结果（包括不同隐私预算、IID/非IID数据集和客户端选择数量）表明，自适应隐私预算和自适应梯度裁剪有助于在保护隐私的前提下保持模型精度。

Conclusion: 自适应隐私预算与自适应梯度裁剪相结合的方法，能够有效解决联邦学习中差分隐私引入噪声过多的问题，从而在隐私保护和模型性能之间取得更好的平衡。

Abstract: Federated learning is distributed model training across several clients
without disclosing raw data. Despite advancements in data privacy, risks still
remain. Differential Privacy (DP) is a technique to protect sensitive data by
adding noise to model updates, usually controlled by a fixed privacy budget.
However, this approach can introduce excessive noise, particularly when the
model converges, which compromises performance. To address this problem,
adaptive privacy budgets have been investigated as a potential solution. This
work implements DP methods using Laplace and Gaussian mechanisms with an
adaptive privacy budget, extending the SelecEval simulator. We introduce an
adaptive clipping approach in the Gaussian mechanism, ensuring that gradients
of the model are dynamically updated rather than using a fixed sensitivity. We
conduct extensive experiments with various privacy budgets, IID and non-IID
datasets, and different numbers of selected clients per round. While our
experiments were limited to 200 training rounds, the results suggest that
adaptive privacy budgets and adaptive clipping can help maintain model accuracy
while preserving privacy.

</details>


### [196] [Neural PDE Solvers with Physics Constraints: A Comparative Study of PINNs, DRM, and WANs](https://arxiv.org/abs/2510.09693)
*Jiakang Chen*

Main category: cs.LG

TL;DR: 本文比较了三种无网格神经PDE求解器（PINNs、DRM、WANs）在泊松方程和薛定谔方程上的性能，并提供了实践指导，证明了其作为复杂PDE求解工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 偏微分方程（PDEs）在科学和工程中无处不在，但解析解罕见，且传统的基于网格的求解器在高维问题中成本高昂。

Method: 本研究统一比较了三种无网格神经PDE求解器：物理信息神经网络（PINNs）、深度里兹方法（DRM）和弱对抗网络（WANs）。应用于泊松方程（最高5D）和一维/二维定态薛定谔方程，并通过Kramers-Henneberger (KH) 变换扩展到激光驱动的薛定谔方程。所有方法均结合了强制边界条件（FBCs）、强制节点（FNs）和正交正则化（OG）。

Result: 所有方法在结合FBCs、FNs和OG时均实现了低$L_2$误差（$10^{-6}$-$10^{-9}$）。PINNs在准确性和激发谱恢复方面最可靠；DRM在定态问题上提供最佳的准确性-运行时权衡；WAN在有效使用弱形式约束和FN/OG时具有竞争力。敏感性分析表明FBCs消除了边界损失调优，单网络求解器中网络宽度比深度更重要，且大部分性能提升发生在5000-10000个epochs内。该工具包也成功解决了KH案例。

Conclusion: 物理引导的神经求解器是可信赖、可扩展的复杂PDE求解工具。本文提供了方法选择的实践指南，并提出了时间依赖性公式、自适应残差驱动采样、并行多态训练和神经域分解等扩展方向。

Abstract: Partial differential equations (PDEs) underpin models across science and
engineering, yet analytical solutions are atypical and classical mesh-based
solvers can be costly in high dimensions. This dissertation presents a unified
comparison of three mesh-free neural PDE solvers, physics-informed neural
networks (PINNs), the deep Ritz method (DRM), and weak adversarial networks
(WANs), on Poisson problems (up to 5D) and the time-independent Schr\"odinger
equation in 1D/2D (infinite well and harmonic oscillator), and extends the
study to a laser-driven case of Schr\"odinger's equation via the
Kramers-Henneberger (KH) transformation.
  Under a common protocol, all methods achieve low $L_2$ errors
($10^{-6}$-$10^{-9}$) when paired with forced boundary conditions (FBCs),
forced nodes (FNs), and orthogonality regularization (OG). Across tasks, PINNs
are the most reliable for accuracy and recovery of excited spectra; DRM offers
the best accuracy-runtime trade-off on stationary problems; WAN is more
sensitive but competitive when weak-form constraints and FN/OG are used
effectively. Sensitivity analyses show that FBC removes boundary-loss tuning,
network width matters more than depth for single-network solvers, and most
gains occur within 5000-10,000 epochs. The same toolkit solves the KH case,
indicating transfer beyond canonical benchmarks.
  We provide practical guidelines for method selection and outline the
following extensions: time-dependent formulations for DRM and WAN, adaptive
residual-driven sampling, parallel multi-state training, and neural domain
decomposition. These results support physics-guided neural solvers as credible,
scalable tools for solving complex PDEs.

</details>


### [197] [Kelp: A Streaming Safeguard for Large Models via Latent Dynamics-Guided Risk Detection](https://arxiv.org/abs/2510.09694)
*Xiaodan Li,Mengjie Wu,Yao Zhu,Yunna Lv,YueFeng Chen,Cen Chen,Jianmei Guo,Hui Xue*

Main category: cs.LG

TL;DR: Kelp是一个新颖的插入式框架，利用大模型中间隐藏状态进行流式风险检测，通过Anchored Temporal Consistency损失和新基准提高准确性和可靠性，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大模型生成内容具有潜在风险（如有害或偏见内容），现有防护机制多为事后检测，可能在捕获前暴露不安全内容，且延迟限制导致检测准确性受限。

Method: 提出Kelp框架，作为插入式组件，在大模型生成管道内进行流式风险检测。它利用大模型中间隐藏状态，通过Streaming Latent Dynamics Head (SLD) 建模风险在生成序列中的时间演变。引入Anchored Temporal Consistency (ATC) 损失以强制实施单调的有害预测。同时，构建了StreamGuardBench基准，用于严格评估流式防护机制。

Result: Kelp在多样模型和数据集上，持续优于最先进的事后防护机制和之前的插入式探测方法（F1平均高15.61%），且仅使用20M参数，每token延迟增加不到0.5毫秒。

Conclusion: Kelp提供了一个高效且准确的实时流式风险检测解决方案，显著提升了大模型内容生成的安全性，且对系统延迟影响极小，适用于实际应用。

Abstract: Large models (LMs) are powerful content generators, yet their open-ended
nature can also introduce potential risks, such as generating harmful or biased
content. Existing guardrails mostly perform post-hoc detection that may expose
unsafe content before it is caught, and the latency constraints further push
them toward lightweight models, limiting detection accuracy. In this work, we
propose Kelp, a novel plug-in framework that enables streaming risk detection
within the LM generation pipeline. Kelp leverages intermediate LM hidden states
through a Streaming Latent Dynamics Head (SLD), which models the temporal
evolution of risk across the generated sequence for more accurate real-time
risk detection. To ensure reliable streaming moderation in real applications,
we introduce an Anchored Temporal Consistency (ATC) loss to enforce monotonic
harm predictions by embedding a benign-then-harmful temporal prior. Besides,
for a rigorous evaluation of streaming guardrails, we also present
StreamGuardBench-a model-grounded benchmark featuring on-the-fly responses from
each protected model, reflecting real-world streaming scenarios in both text
and vision-language tasks. Across diverse models and datasets, Kelp
consistently outperforms state-of-the-art post-hoc guardrails and prior plug-in
probes (15.61% higher average F1), while using only 20M parameters and adding
less than 0.5 ms of per-token latency.

</details>


### [198] [Vanishing Contributions: A Unified Approach to Smoothly Transition Neural Models into Compressed Form](https://arxiv.org/abs/2510.09696)
*Lorenzo Nikiforos,Charalampos Antoniadis,Luciano Prono,Fabio Pareschi,Riccardo Rovatti,Gianluca Setti*

Main category: cs.LG

TL;DR: VCON是一种平滑过渡的神经网络压缩方法，通过并行微调并逐步转移贡献，有效缓解了传统压缩方法导致的精度下降，并在多个任务和压缩策略中实现了显著且一致的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络规模的增长，对压缩技术（如剪枝、量化、低秩分解）的需求日益增加。然而，这些技术直接应用时常导致严重的模型精度下降。

Method: 提出Vanishing Contributions (VCON) 方法。该方法在微调期间并行执行原始（未压缩）模型和压缩模型，并逐步减少原始模型的贡献，同时渐进式增加压缩模型的贡献，从而实现模型向压缩形式的平滑过渡。

Result: VCON在计算机视觉和自然语言处理基准测试中，结合多种压缩策略，均带来了持续的精度提升：典型增益超过3%，某些配置甚至高达20%。

Conclusion: VCON提供了一种通用且可推广的方法，可以应用于现有的神经网络压缩技术，并在多个基准测试中持续获得显著的性能提升，有效解决了压缩导致的精度退化问题。

Abstract: The increasing scale of deep neural networks has led to a growing need for
compression techniques such as pruning, quantization, and low-rank
decomposition. While these methods are very effective in reducing memory,
computation and energy consumption, they often introduce severe accuracy
degradation when applied directly. We introduce Vanishing Contributions (VCON),
a general approach for smoothly transitioning neural models into compressed
form. Rather than replacing the original network directly with its compressed
version, VCON executes the two in parallel during fine-tuning. The contribution
of the original (uncompressed) model is progressively reduced, while that of
the compressed model is gradually increased. This smooth transition allows the
network to adapt over time, improving stability and mitigating accuracy
degradation. We evaluate VCON across computer vision and natural language
processing benchmarks, in combination with multiple compression strategies.
Across all scenarios, VCON leads to consistent improvements: typical gains
exceed 3%, while some configuration exhibits accuracy boosts of 20%. VCON thus
provides a generalizable method that can be applied to the existing compression
techniques, with evidence of consistent gains across multiple benchmarks.

</details>


### [199] [Operator Learning for Power Systems Simulation](https://arxiv.org/abs/2510.09704)
*Matthew Schlegel,Matthew E. Taylor,Mostafa Farrokhabadi*

Main category: cs.LG

TL;DR: 本文探索操作符学习（operator learning）作为替代模型，以加速可再生能源渗透电网的时间域模拟，并首次研究了仿真时间步长不变性，以解决计算量过大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统时间域模拟在可再生能源渗透电网中因需要极小时间步长以捕捉超快动态现象而变得计算成本过高，严重阻碍了可再生能源的稳定整合和气候变化缓解，因此急需快速且可扩展的解决方案。

Method: 本文将操作符学习作为高成本模拟的替代模型，并首次提出并研究了仿真时间步长不变性概念，使模型能够在粗时间步长下训练并推广到精细分辨率动态。在简单测试系统上对三种操作符学习方法进行基准测试，评估了(i)零样本超分辨率（粗时间步长训练，超分辨率推理）和(ii)稳定与不稳定动态范围之间的泛化能力。

Result: 研究在一个简单测试系统上作为初步概念验证，成功展示了操作符学习方法在时间步长不变性方面的可行性。

Conclusion: 该工作通过基准测试操作符学习方法来建模物理系统，解决了可再生能源集成中的一个关键挑战，为气候变化缓解提供了潜在解决方案。

Abstract: Time domain simulation, i.e., modeling the system's evolution over time, is a
crucial tool for studying and enhancing power system stability and dynamic
performance. However, these simulations become computationally intractable for
renewable-penetrated grids, due to the small simulation time step required to
capture renewable energy resources' ultra-fast dynamic phenomena in the range
of 1-50 microseconds. This creates a critical need for solutions that are both
fast and scalable, posing a major barrier for the stable integration of
renewable energy resources and thus climate change mitigation. This paper
explores operator learning, a family of machine learning methods that learn
mappings between functions, as a surrogate model for these costly simulations.
The paper investigates, for the first time, the fundamental concept of
simulation time step-invariance, which enables models trained on coarse time
steps to generalize to fine-resolution dynamics. Three operator learning
methods are benchmarked on a simple test system that, while not incorporating
practical complexities of renewable-penetrated grids, serves as a first
proof-of-concept to demonstrate the viability of time step-invariance. Models
are evaluated on (i) zero-shot super-resolution, where training is performed on
a coarse simulation time step and inference is performed at super-resolution,
and (ii) generalization between stable and unstable dynamic regimes. This work
addresses a key challenge in the integration of renewable energy for the
mitigation of climate change by benchmarking operator learning methods to model
physical systems.

</details>


### [200] [A Multi-Component Reward Function with Policy Gradient for Automated Feature Selection with Dynamic Regularization and Bias Mitigation](https://arxiv.org/abs/2510.09705)
*Sudip Khadka,L. S. Paudel*

Main category: cs.LG

TL;DR: 本文提出一个强化学习框架，通过整合偏见缓解和自动化特征选择，利用包含公平性考虑的奖励信号来动态选择特征，以平衡预测性能和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统的静态特征排除策略在存在隐藏依赖时无法有效防止模型偏见，因此需要一种能同时处理偏见消除和特征选择的动态方法。

Method: 开发了一个强化学习（RL）框架，将偏见缓解和自动化特征选择整合到单一学习过程中。该RL智能体利用一个明确结合预测性能和公平性考虑的多组件奖励函数来动态选择特征，并与集成学习系统结合。

Result: 该动态公式使得模型能够在整个训练过程中平衡泛化能力、准确性和公平性，避免了过度依赖预处理调整或事后校正机制。

Conclusion: 该方法旨在提供一种灵活且通用的特征选择方式，适用于预测因子相关且偏见可能无意中重新出现的复杂环境。

Abstract: Static feature exclusion strategies often fail to prevent bias when hidden
dependencies influence the model predictions. To address this issue, we explore
a reinforcement learning (RL) framework that integrates bias mitigation and
automated feature selection within a single learning process. Unlike
traditional heuristic-driven filter or wrapper approaches, our RL agent
adaptively selects features using a reward signal that explicitly integrates
predictive performance with fairness considerations. This dynamic formulation
allows the model to balance generalization, accuracy, and equity throughout the
training process, rather than rely exclusively on pre-processing adjustments or
post hoc correction mechanisms. In this paper, we describe the construction of
a multi-component reward function, the specification of the agents action space
over feature subsets, and the integration of this system with ensemble
learning. We aim to provide a flexible and generalizable way to select features
in environments where predictors are correlated and biases can inadvertently
re-emerge.

</details>


### [201] [Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments](https://arxiv.org/abs/2510.09712)
*Zhao Tong,Chunlin Gong,Yimeng Gu,Haichao Shi,Qiang Liu,Shu Wu,Xiao-Yu Zhang*

Main category: cs.LG

TL;DR: 该研究旨在提升假新闻检测（FND）模型对对抗性评论的鲁棒性。他们提出了一个分组自适应对抗训练策略，通过心理学分类评论、LLM生成攻击和动态采样机制，显著提高了模型的检测精度和对抗性。


<details>
  <summary>Details</summary>
Motivation: 现有假新闻检测模型在标准设置下表现良好，但易受真实用户或大型语言模型（LLM）生成的对抗性评论影响，这些评论会微妙地改变模型决策，扭曲公众判断并侵蚀对社交媒体平台的信任。

Method: 首先，对现有假新闻检测器进行评论攻击评估。然后，引入一种分组自适应对抗训练策略：1) 将对抗性评论分为感知、认知和社会三类；2) 利用LLM生成多样化的、特定类别的攻击来增强对抗训练；3) 应用基于Dirichlet的自适应采样机制（InfoDirichlet Adjusting Mechanism），动态调整训练过程中对不同评论类别的学习重点。

Result: 在基准数据集上的实验表明，该方法在保持强大检测精度的同时，显著提高了模型对各种对抗性评论扰动的鲁棒性。

Conclusion: 所提出的分组自适应对抗训练策略能有效增强假新闻检测模型对多样化对抗性评论的鲁棒性，同时维持高检测准确性，为构建更可靠的FND系统提供了新途径。

Abstract: The spread of fake news online distorts public judgment and erodes trust in
social media platforms. Although recent fake news detection (FND) models
perform well in standard settings, they remain vulnerable to adversarial
comments-authored by real users or by large language models (LLMs)-that subtly
shift model decisions. In view of this, we first present a comprehensive
evaluation of comment attacks to existing fake news detectors and then
introduce a group-adaptive adversarial training strategy to improve the
robustness of FND models. To be specific, our approach comprises three steps:
(1) dividing adversarial comments into three psychologically grounded
categories: perceptual, cognitive, and societal; (2) generating diverse,
category-specific attacks via LLMs to enhance adversarial training; and (3)
applying a Dirichlet-based adaptive sampling mechanism (InfoDirichlet Adjusting
Mechanism) that dynamically adjusts the learning focus across different comment
categories during training. Experiments on benchmark datasets show that our
method maintains strong detection accuracy while substantially increasing
robustness to a wide range of adversarial comment perturbations.

</details>


### [202] [High-Power Training Data Identification with Provable Statistical Guarantees](https://arxiv.org/abs/2510.09717)
*Zhenlong Liu,Hao Zeng,Weiran Huang,Hongxin Wei*

Main category: cs.LG

TL;DR: 提出PTDI方法，通过严格控制假发现率（FDR）来识别大规模模型中的训练数据，并被实验验证其高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 识别大规模模型中的训练数据对版权、隐私和评估至关重要。现有方法缺乏统计保证或依赖易受侵犯的假设，无法可靠地识别训练数据。

Method: 引入可证明的训练数据识别（PTDI）方法。该方法利用已知未见数据计算每个数据点的p值，并构建测试集数据使用比例的保守估计器来缩放这些p值。最终通过选择所有缩放p值低于数据依赖阈值的数据点来识别训练数据。

Result: PTDI方法能够以可证明的严格假发现率（FDR）控制和显著提高的功效发现训练数据。在各种模型（LLMs和VLMs）和数据集上的广泛实验表明，PTDI严格控制FDR并实现更高的功效。

Conclusion: PTDI提供了一种严谨且高效的方法，能够以可证明的严格FDR控制来识别大规模模型中的训练数据，解决了现有方法的局限性。

Abstract: Identifying training data within large-scale models is critical for copyright
litigation, privacy auditing, and ensuring fair evaluation. The conventional
approaches treat it as a simple binary classification task without statistical
guarantees. A recent approach is designed to control the false discovery rate
(FDR), but its guarantees rely on strong, easily violated assumptions. In this
paper, we introduce Provable Training Data Identification (PTDI), a rigorous
method that identifies a set of training data with strict false discovery rate
(FDR) control. Specifically, our method computes p-values for each data point
using a set of known unseen data, and then constructs a conservative estimator
for the data usage proportion of the test set, which allows us to scale these
p-values. Our approach then selects the final set of training data by
identifying all points whose scaled p-values fall below a data-dependent
threshold. This entire procedure enables the discovery of training data with
provable, strict FDR control and significantly boosted power. Extensive
experiments across a wide range of models (LLMs and VLMs), and datasets
demonstrate that PTDI strictly controls the FDR and achieves higher power.

</details>


### [203] [Federated k-Means via Generalized Total Variation Minimization](https://arxiv.org/abs/2510.09718)
*A. Jung*

Main category: cs.LG

TL;DR: 提出一种基于GTVMin的联邦K均值聚类算法，允许互联设备在不共享本地数据的情况下联合聚类，通过惩罚项协调邻居设备的聚类中心，并仅共享聚合信息以保护隐私。


<details>
  <summary>Details</summary>
Motivation: 解决互联设备在不共享私有本地数据集的情况下，共同对整体数据集进行K均值硬聚类的联邦聚类问题。

Method: 将联邦K均值聚类公式化为GTVMin问题。算法中，每个设备通过解决一个修改后的本地K均值问题来更新本地聚类中心，该修改涉及添加一个惩罚项，以衡量相邻设备聚类中心之间的差异。为保护隐私，算法仅要求在互联设备之间共享聚合信息。

Result: 开发了一种隐私友好的联邦K均值聚类算法，该算法能够使互联设备在保护本地数据隐私的前提下，通过仅共享聚合信息来共同实现对整体数据集的聚类。

Conclusion: 本文提出的联邦K均值算法，通过GTVMin公式化和引入邻居设备聚类中心差异惩罚项，为在分布式私有数据集上进行联合聚类提供了一个保护隐私的有效解决方案。

Abstract: We consider the problem of federated clustering, where interconnected devices
have access to private local datasets and need to jointly cluster the overall
dataset without sharing their local dataset. Our focus is on hard clustering
based on the k-means principle. We formulate federated k-means clustering as an
instance of GTVMin. This formulation naturally lends to a federated k-means
algorithm where each device updates local cluster centroids by solving a
modified local k-means problem. The modification involves adding a penalty term
to measure the discrepancy between the cluster centroid of neighbouring
devices. Our federated k-means algorithm is privacy-friendly as it only
requires sharing aggregated information among interconnected devices.

</details>


### [204] [ICL-Router: In-Context Learned Model Representations for LLM Routing](https://arxiv.org/abs/2510.09719)
*Chenxu Wang,Hao Li,Yiqun Zhang,Linyao Chen,Jianhao Chen,Ping Jian,Peng Ye,Qiaosheng Zhang,Shuyue Hu*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的大语言模型（LLM）路由方法，利用上下文向量表示模型能力，实现了SOTA路由性能，并支持无需重新训练即可无缝集成新模型，解决了现有方法的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）各有优势，模型路由可利用这些优势。然而，现有路由方法依赖准确的模型表示，且添加新模型通常需要重新训练，限制了可扩展性。

Method: 该方法分两阶段：1) 将查询嵌入并投影为向量，训练投影器和基于LLM的路由器来重构查询，使向量表示与路由器的语义空间对齐。2) 对每个候选模型进行性能分析，路由器根据查询的上下文向量和模型性能，学习预测每个模型能否正确回答新查询。

Result: 在同分布和异分布任务中均实现了最先进（SOTA）的路由性能。此外，该方法允许在无需重新训练路由器的情况下无缝集成新模型。

Conclusion: 该研究提出了一种基于上下文向量表示模型能力的新型路由方法，显著提升了LLM路由的性能和可扩展性，尤其是在新模型集成方面。

Abstract: Large language models (LLMs) often exhibit complementary strengths. Model
routing harnesses these strengths by dynamically directing each query to the
most suitable model, given a candidate model pool. However, routing performance
relies on accurate model representations, and adding new models typically
requires retraining, limiting scalability. To address these challenges, we
propose a novel routing method using in-context vectors to represent model
capabilities. The method proceeds in two stages. First, queries are embedded
and projected into vectors, with a projector and LLM-based router trained to
reconstruct the original queries, aligning vector representations with the
router's semantic space. Second, each candidate model is profiled on a query
set, and the router learns -- based on in-context vectors of query and model
performance -- to predict whether each model can correctly answer new queries.
Extensive experiments demonstrate that our method achieves state-of-the-art
routing performance in both in-distribution and out-of-distribution tasks.
Moreover, our method allows for seamless integration of new models without
retraining the router. The code is available at
https://github.com/lalalamdbf/ICL-Router.

</details>


### [205] [It's 2025 -- Narrative Learning is the new baseline to beat for explainable machine learning](https://arxiv.org/abs/2510.09723)
*Gregory D. Baker*

Main category: cs.LG

TL;DR: 本文提出“叙事学习”方法，模型完全通过自然语言定义和迭代优化分类标准，实验表明其在多个数据集上能超越传统可解释模型，且其解释性可评估。


<details>
  <summary>Details</summary>
Motivation: 旨在探索一种不依赖传统数值优化，而是完全基于自然语言定义和迭代改进模型分类标准的新范式，以期提升模型的可解释性和潜力。

Method: 引入“叙事学习”，通过自然语言和解释性提示定义与优化模型。使用3个合成和3个自然数据集，与7个基线可解释机器学习模型对比准确性。同时，通过模型输出的词汇统计衡量解释的可理解性。

Result: 在6个数据集中的5个上，叙事学习的准确性在2025年或更早的时间点上超越了基线可解释模型，这归因于语言模型的进步。此外，还报告了模型输出词汇统计趋势，作为解释可理解性的代理指标。

Conclusion: 叙事学习作为一种创新的自然语言驱动的模型定义和优化方法，展现出超越现有可解释模型的潜力，其性能与语言模型的进步紧密相关，并且其解释的可理解性可通过词汇统计进行衡量。

Abstract: In this paper, we introduce Narrative Learning, a methodology where models
are defined entirely in natural language and iteratively refine their
classification criteria using explanatory prompts rather than traditional
numerical optimisation. We report on experiments to evaluate the accuracy and
potential of this approach using 3 synthetic and 3 natural datasets and compare
them against 7 baseline explainable machine learning models. We demonstrate
that on 5 out of 6 of these datasets, Narrative Learning became more accurate
than the baseline explainable models in 2025 or earlier because of improvements
in language models. We also report on trends in the lexicostatistics of these
models' outputs as a proxy for the comprehensibility of the explanations.

</details>


### [206] [Evaluating LLM-Based Process Explanations under Progressive Behavioral-Input Reduction](https://arxiv.org/abs/2510.09732)
*P. van Oerle,R. H. Bemthuis,F. A. Bukhsh*

Main category: cs.LG

TL;DR: 本研究探索了在逐步减少行为输入规模的情况下，大型语言模型（LLM）生成过程模型解释的质量。结果表明，在中等程度的输入缩减下，解释质量基本得以保持，为资源受限环境下更高效的LLM辅助过程分析提供了可能。


<details>
  <summary>Details</summary>
Motivation: LLMs在生成过程模型文本解释方面应用日益广泛，但直接从大型行为抽象（如直接跟随图或Petri网）生成解释计算成本高昂，因此需要探索提高效率的方法。

Method: 研究采用探索性评估，通过从固定日志的不同前缀（即逐步减小的输入规模）中发现模型，并评估解释质量。其流程包括：(i) 在多种输入规模下发现模型，(ii) 利用一个LLM生成解释，(iii) 使用第二个LLM评估解释的完整性、瓶颈识别能力和改进建议。实验数据为合成日志，质量评估基于LLM评分。

Result: 在合成日志上，中等程度的输入缩减下，LLM生成的解释质量基本得以保留。这表明在计算成本和解释质量之间存在实际的权衡。

Conclusion: 尽管本研究为探索性质（使用LLM评分和合成数据），但结果提示了一种在资源受限环境中实现更具计算效率的LLM辅助过程分析的途径。

Abstract: Large Language Models (LLMs) are increasingly used to generate textual
explanations of process models discovered from event logs. Producing
explanations from large behavioral abstractions (e.g., directly-follows graphs
or Petri nets) can be computationally expensive. This paper reports an
exploratory evaluation of explanation quality under progressive
behavioral-input reduction, where models are discovered from progressively
smaller prefixes of a fixed log. Our pipeline (i) discovers models at multiple
input sizes, (ii) prompts an LLM to generate explanations, and (iii) uses a
second LLM to assess completeness, bottleneck identification, and suggested
improvements. On synthetic logs, explanation quality is largely preserved under
moderate reduction, indicating a practical cost-quality trade-off. The study is
exploratory, as the scores are LLM-based (comparative signals rather than
ground truth) and the data are synthetic. The results suggest a path toward
more computationally efficient, LLM-assisted process analysis in
resource-constrained settings.

</details>


### [207] [ARROW: An Adaptive Rollout and Routing Method for Global Weather Forecasting](https://arxiv.org/abs/2510.09734)
*Jindong Tian,Yifei Ding,Ronghui Xu,Hao Miao,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: ARROW是一种自适应多尺度时间路由全球天气预报方法，通过多时间间隔模型和强化学习自适应展开调度器，解决了现有方法在空间、多尺度时间依赖建模和误差累积方面的局限性，实现了最先进的预报性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的天气预报方法存在两个主要局限性：1) 未能充分建模全球天气系统中固有的空间和多尺度时间依赖性；2) 展开策略难以平衡误差累积与细粒度大气变化的捕捉。

Method: 本研究提出了ARROW (Adaptive-Rollout Multi-scale temporal Routing) 方法。为解决第一个局限性，我们构建了多时间间隔预报模型，并引入共享-私有专家混合（Shared-Private Mixture-of-Experts）和环形位置编码（Ring Positional Encoding）。为解决第二个局限性，我们开发了一个基于强化学习的自适应展开调度器，根据当前天气状态选择最合适的时间间隔进行预报。

Result: ARROW在全球天气预报中取得了最先进的性能。

Conclusion: ARROW在全球天气预报领域建立了一个有前景的新范式。

Abstract: Weather forecasting is a fundamental task in spatiotemporal data analysis,
with broad applications across a wide range of domains. Existing data-driven
forecasting methods typically model atmospheric dynamics over a fixed short
time interval (e.g., 6 hours) and rely on naive autoregression-based rollout
for long-term forecasting (e.g., 138 hours). However, this paradigm suffers
from two key limitations: (1) it often inadequately models the spatial and
multi-scale temporal dependencies inherent in global weather systems, and (2)
the rollout strategy struggles to balance error accumulation with the capture
of fine-grained atmospheric variations. In this study, we propose ARROW, an
Adaptive-Rollout Multi-scale temporal Routing method for Global Weather
Forecasting. To contend with the first limitation, we construct a
multi-interval forecasting model that forecasts weather across different time
intervals. Within the model, the Shared-Private Mixture-of-Experts captures
both shared patterns and specific characteristics of atmospheric dynamics
across different time scales, while Ring Positional Encoding accurately encodes
the circular latitude structure of the Earth when representing spatial
information. For the second limitation, we develop an adaptive rollout
scheduler based on reinforcement learning, which selects the most suitable time
interval to forecast according to the current weather state. Experimental
results demonstrate that ARROW achieves state-of-the-art performance in global
weather forecasting, establishing a promising paradigm in this field.

</details>


### [208] [InterCorpRel-LLM: Enhancing Financial Relational Understanding with Graph-Language Models](https://arxiv.org/abs/2510.09735)
*Qianyou Sun,Jiexin Zheng,Bohan Jin,Lihua Chen,Yijie Peng*

Main category: cs.LG

TL;DR: 本文提出InterCorpRel-LLM，一个整合GNNs和LLMs的跨模态框架，用于识别企业间关系，并在供应链关系识别任务上显著优于现有基线，同时可泛化到零样本竞争者识别。


<details>
  <summary>Details</summary>
Motivation: 识别企业间关系（如供应和竞争）对金融分析和公司治理至关重要，但由于企业数据的规模、稀疏性和上下文依赖性，这一任务极具挑战。现有图方法缺乏语义深度，大型语言模型（LLMs）在关系依赖表示方面受限。

Method: 我们提出InterCorpRel-LLM，一个整合GNNs与LLMs的跨模态框架，以有效联合建模结构和语义。该框架基于FactSet供应链记录的专有数据集，并通过公司图匹配、行业分类和供应关系预测这三个定制训练任务进行支持。

Result: 在供应链关系识别任务上，InterCorpRel-LLM以仅7B参数量的骨干模型和轻量级训练，其F-score达到0.8543，大幅优于包括GPT-5在内的强基线（0.2287）。该模型还可泛化到零样本竞争者识别任务。

Conclusion: InterCorpRel-LLM为分析师和战略家提供了一个强大的工具，用于绘制和推理复杂的企业网络，从而在动态市场中增强决策制定和风险管理能力。

Abstract: Identifying inter-firm relationships such as supply and competitive ties is
critical for financial analysis and corporate governance, yet remains
challenging due to the scale, sparsity, and contextual dependence of corporate
data. Graph-based methods capture structure but miss semantic depth, while
large language models (LLMs) excel at text but remain limited in their ability
to represent relational dependencies. To address this, we propose
InterCorpRel-LLM, a cross-modal framework that integrates GNNs with LLMs,
supported by a proprietary dataset derived from FactSet supply chain records
and three tailored training tasks: company graph matching, industry
classification, and supply relation prediction. This design enables effective
joint modeling of structure and semantics. Experiments show that
InterCorpRel-LLM substantially outperforms strong baselines, including GPT-5,
on a supply relation identification task, achieving an F-score of 0.8543 vs.
0.2287 with only a 7B-parameter backbone and lightweight training. The model
also generalizes to zero-shot competitor identification, underscoring its
ability to capture nuanced inter-firm dynamics. Our framework thus provides
analysts and strategists with a robust tool for mapping and reasoning about
complex corporate networks, enhancing decision-making and risk management in
dynamic markets.

</details>


### [209] [Machine learning methods fail to provide cohesive atheoretical construction of personality traits from semantic embeddings](https://arxiv.org/abs/2510.09739)
*Ayoub Bouguettaya,Elizabeth M. Stuart*

Main category: cs.LG

TL;DR: 通过分析Reddit评论，将机器学习构建的人格模型与大五人格模型进行比较，发现大五人格模型更具解释力且更稳健。


<details>
  <summary>Details</summary>
Motivation: 探讨词汇假设，并尝试通过机器学习方法从底层构建人格模型，旨在与大五人格模型进行比较，以评估其描述性效用，并验证既有心理学理论的生态有效性。

Method: 使用机器学习方法，基于经典形容词列表构建了一个自下而上的人格模型，并通过分析一百万条Reddit评论，将其描述性效用与大五人格模型进行比较。

Result: 大五人格模型（特别是宜人性、责任心和神经质）为在线社区提供了更强大、可解释的描述。相比之下，机器学习聚类未能提供有意义的区分，未能识别出外向性特质，并且缺乏大五人格模型的心理测量学连贯性。

Conclusion: 研究结果证实了大五人格模型的稳健性，并表明人格的语义结构具有语境依赖性。机器学习可以帮助检验既有心理学理论的生态有效性，但可能无法取代它们。

Abstract: The lexical hypothesis posits that personality traits are encoded in language
and is foundational to models like the Big Five. We created a bottom-up
personality model from a classic adjective list using machine learning and
compared its descriptive utility against the Big Five by analyzing one million
Reddit comments. The Big Five, particularly Agreeableness, Conscientiousness,
and Neuroticism, provided a far more powerful and interpretable description of
these online communities. In contrast, our machine-learning clusters provided
no meaningful distinctions, failed to recover the Extraversion trait, and
lacked the psychometric coherence of the Big Five. These results affirm the
robustness of the Big Five and suggest personality's semantic structure is
context-dependent. Our findings show that while machine learning can help check
the ecological validity of established psychological theories, it may not be
able to replace them.

</details>


### [210] [Reliable Active Learning from Unreliable Labels via Neural Collapse Geometry](https://arxiv.org/abs/2510.09740)
*Atharv Goel,Sharat Agarwal,Saket Anand,Chetan Arora*

Main category: cs.LG

TL;DR: NCAL-R是一种新的主动学习框架，它利用深度网络的几何规律，通过结合类均值对齐扰动和特征波动分数，有效缓解了标签噪声和数据分布偏移对主动学习可靠性的负面影响。


<details>
  <summary>Details</summary>
Motivation: 传统主动学习在标签有噪声或数据分布偏移时可靠性不足，标注错误和稀有类别模糊常被现有启发式方法放大，导致重复选择错误或冗余样本。

Method: 提出NCAL-R框架，利用深度网络的几何规律对抗不可靠监督。引入两种互补信号：(i)类均值对齐扰动分数，衡量样本如何稳定或扭曲类间几何；(ii)特征波动分数，捕捉表示的时间不稳定性。结合二者优先选择既能保持类别分离又能突出模糊区域的样本。

Result: NCAL-R在ImageNet-100和CIFAR100上表现优于标准AL基线，在更少标签下实现了更高准确性，在合成标签噪声下鲁棒性更强，并对分布外数据泛化能力更强。

Conclusion: 将几何可靠性标准整合到样本获取决策中，能显著提升主动学习对标注错误和分布偏移的韧性，是其在实际应用中值得信赖部署的关键一步。

Abstract: Active Learning (AL) promises to reduce annotation cost by prioritizing
informative samples, yet its reliability is undermined when labels are noisy or
when the data distribution shifts. In practice, annotators make mistakes, rare
categories are ambiguous, and conventional AL heuristics (uncertainty,
diversity) often amplify such errors by repeatedly selecting mislabeled or
redundant samples. We propose Reliable Active Learning via Neural Collapse
Geometry (NCAL-R), a framework that leverages the emergent geometric
regularities of deep networks to counteract unreliable supervision. Our method
introduces two complementary signals: (i) a Class-Mean Alignment Perturbation
score, which quantifies how candidate samples structurally stabilize or distort
inter-class geometry, and (ii) a Feature Fluctuation score, which captures
temporal instability of representations across training checkpoints. By
combining these signals, NCAL-R prioritizes samples that both preserve class
separation and highlight ambiguous regions, mitigating the effect of noisy or
redundant labels. Experiments on ImageNet-100 and CIFAR100 show that NCAL-R
consistently outperforms standard AL baselines, achieving higher accuracy with
fewer labels, improved robustness under synthetic label noise, and stronger
generalization to out-of-distribution data. These results suggest that
incorporating geometric reliability criteria into acquisition decisions can
make Active Learning less brittle to annotation errors and distribution shifts,
a key step toward trustworthy deployment in real-world labeling pipelines. Our
code is available at https://github.com/Vision-IIITD/NCAL.

</details>


### [211] [Patentformer: A demonstration of AI-assisted automated patent drafting](https://arxiv.org/abs/2510.09752)
*Sai Krishna Reddy Mudhiganti,Juanyan Wang,Ruo Yang,Manali Sharma*

Main category: cs.LG

TL;DR: 本文展示了Patentformer，一个AI驱动的自动化专利起草平台，旨在辅助专利律师快速生成高质量、符合法律规范的专利申请。


<details>
  <summary>Details</summary>
Motivation: 专利起草工作极具挑战性，高度依赖专利律师的丰富经验和专业知识（法律和技术理解），以撰写符合正式法律风格的专利申请。

Method: 介绍并演示了Patentformer，一个AI驱动的自动化专利起草平台。

Result: 该平台能够快速生成符合法律写作标准的高质量专利申请，旨在支持专利律师的工作。

Conclusion: Patentformer通过AI自动化，为专利律师提供支持，以应对专利起草的复杂性和专业性挑战，提升起草效率和质量。

Abstract: Patent drafting presents significant challenges due to its reliance on the
extensive experience and specialized expertise of patent attorneys, who must
possess both legal acumen and technical understanding of an invention to craft
patent applications in a formal legal writing style. This paper presents a
demonstration of Patentformer, an AI-powered automated patent drafting platform
designed to support patent attorneys by rapidly producing high-quality patent
applications adhering to legal writing standards.

</details>


### [212] [PatentVision: A multimodal method for drafting patent applications](https://arxiv.org/abs/2510.09762)
*Ruo Yang,Sai Krishna Reddy Mudhiganti,Manali Sharma*

Main category: cs.LG

TL;DR: PatentVision是一个多模态框架，利用视觉语言模型整合文本和图像输入，自动化生成专利说明书，超越了纯文本方法，提高了专利起草的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 专利起草因其复杂的细节、法律合规性及视觉元素需求而具有挑战性。尽管大型视觉语言模型（LVLMs）潜力巨大，但其在自动化专利写作领域的应用仍未被充分探索。

Method: 本文提出了PatentVision框架，这是一个多模态框架，它整合专利权利要求和附图等文本及视觉输入，以生成完整的专利说明书。该框架基于先进的LVLMs，并通过结合微调的视觉语言模型和针对专利领域的专业训练来提高准确性。

Result: 实验表明，PatentVision超越了纯文本方法，生成了更高保真度且更符合人类书写标准的输出。通过整合视觉数据，它能更好地表示复杂的设计特征和功能联系，从而产生更丰富、更精确的结果。

Conclusion: 这项研究突出了多模态技术在专利自动化中的价值，提供了一个可扩展的工具，能够减少手动工作量并提高一致性。PatentVision不仅推动了专利起草，也为LVLMs在其他专业领域的广泛应用奠定了基础，有望变革知识产权管理和创新流程。

Abstract: Patent drafting is complex due to its need for detailed technical
descriptions, legal compliance, and visual elements. Although Large Vision
Language Models (LVLMs) show promise across various tasks, their application in
automating patent writing remains underexplored. In this paper, we present
PatentVision, a multimodal framework that integrates textual and visual inputs
such as patent claims and drawings to generate complete patent specifications.
Built on advanced LVLMs, PatentVision enhances accuracy by combining fine tuned
vision language models with domain specific training tailored to patents.
Experiments reveal it surpasses text only methods, producing outputs with
greater fidelity and alignment with human written standards. Its incorporation
of visual data allows it to better represent intricate design features and
functional connections, leading to richer and more precise results. This study
underscores the value of multimodal techniques in patent automation, providing
a scalable tool to reduce manual workloads and improve consistency.
PatentVision not only advances patent drafting but also lays the groundwork for
broader use of LVLMs in specialized areas, potentially transforming
intellectual property management and innovation processes.

</details>


### [213] [Leveraging Shared Prototypes for a Multimodal Pulse Motion Foundation Model](https://arxiv.org/abs/2510.09764)
*Wanting Mao,Maxwell A Xu,Harish Haresamudram,Mithun Saha,Santosh Kumar,James Matthew Rehg*

Main category: cs.LG

TL;DR: 本文提出ProtoMM，一种新的自监督学习框架，通过共享原型字典解决现有对比学习方法在多模态生物信号建模中的局限性，实现了SOTA性能和更好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 建模多模态时间序列数据（尤其是生物信号）对捕获系统级动态至关重要。然而，现有基于CLIP风格对比目标的自监督学习方法容易过拟合，误判跨模态关系为负样本，导致嵌入碎片化且泛化能力差。

Method: 提出ProtoMM框架，引入共享原型字典将异构模态锚定在共同嵌入空间。通过将表示聚类在共享原型周围而非显式负采样，捕获跨模态互补信息，为生理信号提供“共同语言”。该方法应用于开发脉搏运动基础模型。

Result: ProtoMM方法在脉搏运动基础模型上，优于仅使用对比学习和现有其他多模态自监督学习方法，实现了最先进的性能，并提高了学习特征的可解释性。

Conclusion: ProtoMM通过引入共享原型字典，有效克服了现有方法的局限性，为多模态生物信号建模提供了统一且可解释的方法，实现了SOTA性能。

Abstract: Modeling multi-modal time-series data is critical for capturing system-level
dynamics, particularly in biosignals where modalities such as ECG, PPG, EDA,
and accelerometry provide complementary perspectives on interconnected
physiological processes. While recent self-supervised learning (SSL) advances
have improved unimodal representation learning, existing multi-modal approaches
often rely on CLIP-style contrastive objectives that overfit to easily aligned
features and misclassify valid cross-modal relationships as negatives,
resulting in fragmented and non-generalizable embeddings. To overcome these
limitations, we propose ProtoMM, a novel SSL framework that introduces a shared
prototype dictionary to anchor heterogeneous modalities in a common embedding
space. By clustering representations around shared prototypes rather than
explicit negative sampling, our method captures complementary information
across modalities and provides a coherent "common language" for physiological
signals. In this work, we focus on developing a Pulse Motion foundation model
with ProtoMM and demonstrate that our approach outperforms contrastive-only and
prior multimodal SSL methods, achieving state-of-the-art performance while
offering improved interpretability of learned features.

</details>


### [214] [HeSRN: Representation Learning On Heterogeneous Graphs via Slot-Aware Retentive Network](https://arxiv.org/abs/2510.09767)
*Yifan Lu,Ziyun Zou,Belal Alsinglawi,Islam Al-Qudah,Izzat Alsmadi,Feilong Tang,Pengfei Jiao,Shoaib Jameel*

Main category: cs.LG

TL;DR: 提出HeSRN，一种新型异构槽位感知保留网络，用于高效且富有表达力的异构图表示学习，解决了现有图Transformer在异构图上的计算复杂度和语义建模限制。


<details>
  <summary>Details</summary>
Motivation: 现有图Transformer在异构图上存在计算复杂度高（二次方）和无法有效建模异构语义的问题，限制了其可扩展性和泛化能力。

Method: HeSRN引入一个槽位感知结构编码器，通过将异构特征投影到独立槽位、槽位归一化和保留融合来解耦节点类型语义。同时，它用保留机制替代自注意力机制，实现线性时间复杂度建模依赖关系，并通过多尺度保留层捕获局部结构信号和全局异构语义。

Result: 在四个真实世界异构图数据集上的节点分类任务中，HeSRN持续优于最先进的异构图神经网络和图Transformer基线模型，实现了更高的准确性并显著降低了计算复杂度。

Conclusion: HeSRN为异构图表示学习提供了一种高效且表达力强的新方法，有效解决了现有模型的局限性。

Abstract: Graph Transformers have recently achieved remarkable progress in graph
representation learning by capturing long-range dependencies through
self-attention. However, their quadratic computational complexity and inability
to effectively model heterogeneous semantics severely limit their scalability
and generalization on real-world heterogeneous graphs. To address these issues,
we propose HeSRN, a novel Heterogeneous Slot-aware Retentive Network for
efficient and expressive heterogeneous graph representation learning. HeSRN
introduces a slot-aware structure encoder that explicitly disentangles
node-type semantics by projecting heterogeneous features into independent slots
and aligning their distributions through slot normalization and retention-based
fusion, effectively mitigating the semantic entanglement caused by forced
feature-space unification in previous Transformer-based models. Furthermore, we
replace the self-attention mechanism with a retention-based encoder, which
models structural and contextual dependencies in linear time complexity while
maintaining strong expressive power. A heterogeneous retentive encoder is
further employed to jointly capture both local structural signals and global
heterogeneous semantics through multi-scale retention layers. Extensive
experiments on four real-world heterogeneous graph datasets demonstrate that
HeSRN consistently outperforms state-of-the-art heterogeneous graph neural
networks and Graph Transformer baselines on node classification tasks,
achieving superior accuracy with significantly lower computational complexity.

</details>


### [215] [Scaling Laws and Symmetry, Evidence from Neural Force Fields](https://arxiv.org/abs/2510.09768)
*Khang Ngo,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 本研究通过实证分析，发现等变性在学习原子间势能的几何任务中至关重要，尤其是在大规模情况下，并揭示了等变架构比非等变架构具有更好的幂律缩放行为。


<details>
  <summary>Details</summary>
Motivation: 旨在探讨在学习原子间势能时，不同架构（特别是等变架构）如何影响模型的缩放行为，并挑战模型应自主发现基本归纳偏置（如对称性）的普遍观点。

Method: 进行了一项实证研究，分析了不同架构在数据、参数和计算量方面的幂律缩放行为，并比较了等变与非等变模型，以及不同阶等变表示的性能。

Result: 研究发现等变性在更大规模下更为关键；模型展现出清晰的幂律缩放行为，且存在“架构依赖的指数”；等变架构比非等变模型缩放性更好；更高阶的等变表示能带来更好的缩放指数；为实现计算最优训练，数据和模型规模应协同缩放。

Conclusion: 研究结果表明，不应将对称性等基本归纳偏置留给模型自行发现，尤其是在大规模任务中，因为这些偏置能显著改变任务的固有难度和缩放法则，因此应主动将其融入模型设计。

Abstract: We present an empirical study in the geometric task of learning interatomic
potentials, which shows equivariance matters even more at larger scales; we
show a clear power-law scaling behaviour with respect to data, parameters and
compute with ``architecture-dependent exponents''. In particular, we observe
that equivariant architectures, which leverage task symmetry, scale better than
non-equivariant models. Moreover, among equivariant architectures, higher-order
representations translate to better scaling exponents. Our analysis also
suggests that for compute-optimal training, the data and model sizes should
scale in tandem regardless of the architecture. At a high level, these results
suggest that, contrary to common belief, we should not leave it to the model to
discover fundamental inductive biases such as symmetry, especially as we scale,
because they change the inherent difficulty of the task and its scaling laws.

</details>


### [216] [A Generic Machine Learning Framework for Radio Frequency Fingerprinting](https://arxiv.org/abs/2510.09775)
*Alex Hiles,Bashar I. Ahmad*

Main category: cs.LG

TL;DR: 本文提出了一个通用的机器学习驱动的射频指纹识别（RFF）框架，能够支持特定的发射器识别（SEI）、发射器数据关联（EDA）和射频发射器聚类（RFEC）等多种下游任务，并在一系列真实应用场景中进行了展示。


<details>
  <summary>Details</summary>
Motivation: 传统的射频指纹识别方法劳动密集、缺乏灵活性且仅适用于特定类型发射器。鉴于数据驱动的机器学习方法在自动学习复杂指纹和提供优越性能方面的优势，研究者寻求开发基于机器学习的通用射频指纹识别方法。

Method: 本文提出了一个通用的机器学习驱动的射频指纹识别框架。该框架将特定的发射器识别、发射器数据关联和射频发射器聚类等多种下游任务都表述为依赖于射频指纹的任务。

Result: 通过使用真实的射频数据集，在空间监视、信号情报和反无人机等多种任务和应用领域展示了所提出框架的有效性和普适性。

Conclusion: 所提出的通用机器学习驱动的射频指纹识别框架能够有效处理多种与射频指纹相关的下游任务，并在广泛的应用场景中展现了其强大的能力和实用价值。

Abstract: Fingerprinting Radio Frequency (RF) emitters typically involves finding
unique emitter characteristics that are featured in their transmitted signals.
These fingerprints are nuanced but sufficiently detailed, motivating the
pursuit of methods that can successfully extract them. The most granular
downstream task is known as Specific Emitter Identification (SEI), which
requires a well informed RF fingerprinting (RFF) approach for it to be
successful. RFF and SEI have a long history, with numerous application areas in
defence and civilian contexts such as signal intelligence, electronic
surveillance, physical-layer authentication of wireless communication devices,
to name a few. RFF methods also support many other downstream tasks such as
Emitter Data Association (EDA) and RF Emitter Clustering (RFEC) and are
applicable to a range of transmission types. In recent years, data-driven
approaches have become popular in the RFF domain due to their ability to
automatically learn intricate fingerprints from raw data. These methods
generally deliver superior performance when compared to traditional techniques.
The more traditional approaches are often labour-intensive, inflexible and only
applicable to a particular emitter type or transmission scheme. Therefore, we
consider data-driven Machine Learning (ML)-enabled RFF. In particular, we
propose a generic framework for ML-enabled RFF which is inclusive of several
popular downstream tasks such as SEI, EDA and RFEC. Each task is formulated as
a RF fingerprint-dependent task. A variety of use cases using real RF datasets
are presented here to demonstrate the framework for a range of tasks and
application areas, such as spaceborne surveillance, signal intelligence and
countering drones.

</details>


### [217] [Why Do Transformers Fail to Forecast Time Series In-Context?](https://arxiv.org/abs/2510.09776)
*Yufa Zhou,Yixiao Wang,Surbhi Goel,Anru R. Zhang*

Main category: cs.LG

TL;DR: 尽管LLMs/Transformer在时间序列预测(TSF)中应用广泛，但其表现常不敌简单模型，且理论原因不明。本文从上下文学习(ICL)角度，理论分析Transformer（线性自注意力）在AR(p)数据下的局限性，发现它无法超越传统线性模型，在无限上下文下渐进恢复最优线性预测器，且链式思考(CoT)推理会导致预测崩溃。研究旨在鼓励重新审视TSF的基本理论局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）及其Transformer架构在时间序列预测（TSF）中应用广泛，但经验表明它们常未能超越简单的线性模型；对此现象的严格理论理解仍然有限。

Method: 通过上下文学习（ICL）理论，对Transformer（特别是AR($p$)数据下的线性自注意力模型）在时间序列预测中的局限性进行理论分析，并通过精心设计的实验进行实证验证。

Result: 1. 线性自注意力（LSA）模型在上下文预测中的预期均方误差（MSE）不能低于经典线性模型。2. 随着上下文长度趋于无穷，LSA渐近地恢复最优线性预测器。3. 在链式思考（CoT）推理下，预测会指数级地收敛到均值。

Conclusion: 理论分析揭示了Transformer在时间序列预测中的局限性，并为设计更有效的预测架构提供实用见解。鼓励研究社区重新审视时间序列预测的基本理论限制，并批判性地评估未经深入审查而直接应用日益复杂架构的做法。

Abstract: Time series forecasting (TSF) remains a challenging and largely unsolved
problem in machine learning, despite significant recent efforts leveraging
Large Language Models (LLMs), which predominantly rely on Transformer
architectures. Empirical evidence consistently shows that even powerful
Transformers often fail to outperform much simpler models, e.g., linear models,
on TSF tasks; however, a rigorous theoretical understanding of this phenomenon
remains limited. In this paper, we provide a theoretical analysis of
Transformers' limitations for TSF through the lens of In-Context Learning (ICL)
theory. Specifically, under AR($p$) data, we establish that: (1) Linear
Self-Attention (LSA) models $\textit{cannot}$ achieve lower expected MSE than
classical linear models for in-context forecasting; (2) as the context length
approaches to infinity, LSA asymptotically recovers the optimal linear
predictor; and (3) under Chain-of-Thought (CoT) style inference, predictions
collapse to the mean exponentially. We empirically validate these findings
through carefully designed experiments. Our theory not only sheds light on
several previously underexplored phenomena but also offers practical insights
for designing more effective forecasting architectures. We hope our work
encourages the broader research community to revisit the fundamental
theoretical limitations of TSF and to critically evaluate the direct
application of increasingly sophisticated architectures without deeper
scrutiny.

</details>


### [218] [SVTime: Small Time Series Forecasting Models Informed by "Physics" of Large Vision Model Forecasters](https://arxiv.org/abs/2510.09780)
*ChengAo Shen,Ziming Zhao,Hanghang Tong,Dongjin Song,Dongsheng Luo,Qingsong Wen,Jingchao Ni*

Main category: cs.LG

TL;DR: 提出SVTime，一个受大型视觉模型启发的轻量级时间序列预测模型，其参数量比大型模型少10^3倍，但在核心任务上性能与大型模型相当，并超越了现有轻量级模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列AI中，大型预训练模型虽强大但能耗高昂且对硬件要求严苛，引发了碳足迹和可持续性问题。对于资源受限的用户，亟需成本效益高、性能与大型模型相当的轻量级解决方案。

Method: 引入SVTime，一个受大型视觉模型（LVM）启发的小型模型，用于长期时间序列预测（LTSF）。该方法通过识别LVM预测器的关键归纳偏置，并利用精心设计的线性层和约束函数将其编码到小型模型中。

Result: 在8个基准数据集上，SVTime与21个基线（包括轻量级、复杂和预训练大型模型）进行比较，其性能超越了最先进的轻量级模型，并且以比LVM少10^3的参数量，达到了与大型模型相媲美的性能，同时支持低资源环境下的高效训练和推理。

Conclusion: SVTime证明了可以在大幅减少参数量和提高资源效率的同时，构建出性能媲美大型模型的轻量级时间序列预测模型，为可持续和经济实惠的AI应用提供了可行路径。

Abstract: Time series AI is crucial for analyzing dynamic web content, driving a surge
of pre-trained large models known for their strong knowledge encoding and
transfer capabilities across diverse tasks. However, given their
energy-intensive training, inference, and hardware demands, using large models
as a one-fits-all solution raises serious concerns about carbon footprint and
sustainability. For a specific task, a compact yet specialized, high-performing
model may be more practical and affordable, especially for resource-constrained
users such as small businesses. This motivates the question: Can we build
cost-effective lightweight models with large-model-like performance on core
tasks such as forecasting? This paper addresses this question by introducing
SVTime, a novel Small model inspired by large Vision model (LVM) forecasters
for long-term Time series forecasting (LTSF). Recently, LVMs have been shown as
powerful tools for LTSF. We identify a set of key inductive biases of LVM
forecasters -- analogous to the "physics" governing their behaviors in LTSF --
and design small models that encode these biases through meticulously crafted
linear layers and constraint functions. Across 21 baselines spanning
lightweight, complex, and pre-trained large models on 8 benchmark datasets,
SVTime outperforms state-of-the-art (SOTA) lightweight models and rivals large
models with 10^3 fewer parameters than LVMs, while enabling efficient training
and inference in low-resource settings.

</details>


### [219] [Building a Foundational Guardrail for General Agentic Systems via Synthetic Data](https://arxiv.org/abs/2510.09781)
*Yue Huang,Hang Hua,Yujun Zhou,Pengcheng Jing,Manish Nagireddy,Inkit Padhi,Greta Dolcetti,Zhangchen Xu,Subhajit Chaudhury,Ambrish Rawat,Liubov Nedoshivina,Pin-Yu Chen,Prasanna Sattigeri,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 本文提出了一套全面的预执行安全框架，包括数据生成引擎AuraGen、守卫模型Safiron和评估基准Pre-Exec Bench，旨在防止大型语言模型（LLM）代理在执行前产生有害行为。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理的安全防护措施多在行动执行后生效，但某些风险一旦执行将导致严重后果，因此需要在规划阶段（即执行前）进行干预以确保安全。当前研究存在数据、模型和评估方面的三大空白。

Method: 为解决数据空白，提出AuraGen引擎，用于合成良性轨迹、注入带标签的风险并过滤输出，生成大量可靠的预执行安全语料。为解决模型空白，提出基础性守卫模型Safiron，结合跨规划器适配器和紧凑型守护模型，以统一输入格式，并标记风险、分配风险类型和生成理由。Safiron通过两阶段训练和广泛探索的数据配方实现鲁邦转移。为解决评估空白，发布Pre-Exec Bench，一个涵盖多样工具和分支轨迹的现实基准，用于衡量检测、细粒度分类、解释和跨规划器泛化能力。

Result: 在Pre-Exec Bench上进行的广泛实验表明，所提出的Safiron守卫模型相对于强基线表现出持续的性能提升。此外，消融实验进一步提炼出可行的实践方法。

Conclusion: 本研究提供了一个实用的模板，有助于构建更安全的代理系统，实现了在LLM代理的规划阶段进行有效和可控的风险干预。

Abstract: While LLM agents can plan multi-step tasks, intervening at the planning
stage-before any action is executed-is often the safest way to prevent harm,
since certain risks can lead to severe consequences once carried out. However,
existing guardrails mostly operate post-execution, which is difficult to scale
and leaves little room for controllable supervision at the plan level. To
address this challenge, we highlight three critical gaps in current research:
data gap, model gap, and evaluation gap. To close the data gap, we introduce
AuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)
injects category-labeled risks with calibrated difficulty, and (iii) filters
outputs via an automated reward model, producing large and reliable corpora for
pre-execution safety. To close the guardian model gap, we propose a
foundational guardrail Safiron, combining a cross-planner adapter with a
compact guardian model. The adapter unifies different input formats, while
Safiron flags risky cases, assigns risk types, and generates rationales;
trained in two stages with a broadly explored data recipe, Safiron achieves
robust transfer across settings. To close the evaluation gap, we release
Pre-Exec Bench, a realistic benchmark covering diverse tools and branching
trajectories, which measures detection, fine-grained categorization,
explanation, and cross-planner generalization in human-verified scenarios.
Extensive experiments demonstrate consistent gains of the proposed guardrail
over strong baselines on Pre-Exec Bench, and ablations further distill
actionable practices, providing a practical template for safer agentic systems.

</details>


### [220] [Large Language Models for Imbalanced Classification: Diversity makes the difference](https://arxiv.org/abs/2510.09783)
*Dang Nguyen,Sunil Gupta,Kien Do,Thin Nguyen,Taylor Braund,Alexis Whitton,Svetha Venkatesh*

Main category: cs.LG

TL;DR: 提出一种新颖的LLM过采样方法，通过引入多样性增强策略，有效解决不平衡分类问题中现有LLM方法生成样本多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有过采样方法在处理分类变量时存在信息损失。虽有基于LLM的方法解决此限，但其生成的少数类样本多样性有限，降低了下游分类任务的鲁棒性和泛化能力。

Method: 提出一种新颖的LLM过采样方法以增强多样性。具体包括：1) 引入基于少数类标签和特征的条件采样策略；2) 开发新的预训练LLM微调置换策略；3) 利用少数类样本和插值样本对LLM进行微调，以丰富变异性。

Result: 在10个表格数据集上的实验表明，该方法显著优于8个SOTA基线。生成的合成样本真实且多样。通过基于熵的理论分析，证明该方法能有效提升生成样本的多样性。

Conclusion: 该方法成功解决了LLM过采样方法样本多样性不足的挑战，通过创新策略显著提升了不平衡分类任务的性能，并提供了理论支持。

Abstract: Oversampling is one of the most widely used approaches for addressing
imbalanced classification. The core idea is to generate additional minority
samples to rebalance the dataset. Most existing methods, such as SMOTE, require
converting categorical variables into numerical vectors, which often leads to
information loss. Recently, large language model (LLM)-based methods have been
introduced to overcome this limitation. However, current LLM-based approaches
typically generate minority samples with limited diversity, reducing robustness
and generalizability in downstream classification tasks. To address this gap,
we propose a novel LLM-based oversampling method designed to enhance diversity.
First, we introduce a sampling strategy that conditions synthetic sample
generation on both minority labels and features. Second, we develop a new
permutation strategy for fine-tuning pre-trained LLMs. Third, we fine-tune the
LLM not only on minority samples but also on interpolated samples to further
enrich variability. Extensive experiments on 10 tabular datasets demonstrate
that our method significantly outperforms eight SOTA baselines. The generated
synthetic samples are both realistic and diverse. Moreover, we provide
theoretical analysis through an entropy-based perspective, proving that our
method encourages diversity in the generated samples.

</details>


### [221] [Combined Representation and Generation with Diffusive State Predictive Information Bottleneck](https://arxiv.org/abs/2510.09784)
*Richard John,Yunrui Qiu,Lukas Herron,Pratyush Tiwary*

Main category: cs.LG

TL;DR: 本文提出D-SPIB协议，结合时间滞后信息瓶颈和扩散模型，平衡分子表示学习与生成，并能整合温度信息以探索训练集外物理条件。


<details>
  <summary>Details</summary>
Motivation: 高维空间生成建模日益数据密集，而分子科学数据收集昂贵且重要事件稀有，因此需要将数据压缩到低维流形以支持下游任务（如生成）。

Method: 将时间滞后信息瓶颈（用于表征分子重要表示）和扩散模型结合在一个联合训练目标中，形成D-SPIB协议，旨在平衡表示学习和生成，并能整合不同分子模拟轨迹的温度信息。

Result: D-SPIB协议能够平衡表示学习和生成目标，学习到连贯有用的热力学内部表示，并能成功用于探索训练集之外的物理条件。

Conclusion: D-SPIB在多个分子任务中展现潜力，尤其是在探索训练集外物理条件方面，具有重要应用价值。

Abstract: Generative modeling becomes increasingly data-intensive in high-dimensional
spaces. In molecular science, where data collection is expensive and important
events are rare, compression to lower-dimensional manifolds is especially
important for various downstream tasks, including generation. We combine a
time-lagged information bottleneck designed to characterize molecular important
representations and a diffusion model in one joint training objective. The
resulting protocol, which we term Diffusive State Predictive Information
Bottleneck (D-SPIB), enables the balancing of representation learning and
generation aims in one flexible architecture. Additionally, the model is
capable of combining temperature information from different molecular
simulation trajectories to learn a coherent and useful internal representation
of thermodynamics. We benchmark D-SPIB on multiple molecular tasks and showcase
its potential for exploring physical conditions outside the training set.

</details>


### [222] [Principled Operator Learning in Ocean Dynamics: The Role of Temporal Structure](https://arxiv.org/abs/2510.09792)
*Vahidreza Jahanmard,Ali Ramezani-Kebrya,Robinson Hordoir*

Main category: cs.LG

TL;DR: 本文提出并改进了傅里叶神经算子(FNOtD)，通过整合时间傅里叶模式和内化色散关系来解决海洋预测中长期稳定性与物理定律遵循的挑战。与标准FNO相比，FNOtD显著提升了高频过程的预测稳定性和物理一致性，并以更低的计算成本达到了与先进数值模型相当的预测精度。


<details>
  <summary>Details</summary>
Motivation: 神经算子在天气和海洋预测中学习偏微分方程解方面表现出前景，但存在长期预测稳定性差、难以遵循物理定律（特别是在高频过程中）等显著挑战。本研究旨在解决高分辨率海洋预测中的这些问题。

Method: 本研究通过将时间傅里叶模式纳入傅里叶神经算子（FNO），提出了一种改进变体FNOtD。FNOtD旨在内化色散关系，并在积分核的训练中将空间和时间进行纠缠。研究将标准FNO与FNOtD进行比较，以评估其在学习海洋偏微分方程解算子时的性能。

Result: 结果表明，FNOtD通过在积分核训练中纠缠空间和时间，能够捕捉多尺度波传播并有效学习海洋动力学。在高频设置下，FNOtD在长期预测稳定性和与底层物理动力学的一致性方面，相较于标准FNO有显著改善。此外，它还提供了与最先进的数值海洋模型相当的预测能力，同时计算成本显著降低。

Conclusion: 通过整合时间傅里叶模式并内化色散关系（如FNOtD所示），可以增强神经算子在海洋偏微分方程中的物理保真度。FNOtD是一种优越的高分辨率海洋预测方法，解决了标准神经算子在这一领域面临的关键挑战，并提供了更高的稳定性、物理一致性和计算效率。

Abstract: Neural operators are becoming the default tools to learn solutions to
governing partial differential equations (PDEs) in weather and ocean
forecasting applications. Despite early promising achievements, significant
challenges remain, including long-term prediction stability and adherence to
physical laws, particularly for high-frequency processes. In this paper, we
take a step toward addressing these challenges in high-resolution ocean
prediction by incorporating temporal Fourier modes, demonstrating how this
modification enhances physical fidelity. This study compares the standard
Fourier Neural Operator (FNO) with its variant, FNOtD, which has been modified
to internalize the dispersion relation while learning the solution operator for
ocean PDEs. The results demonstrate that entangling space and time in the
training of integral kernels enables the model to capture multiscale wave
propagation and effectively learn ocean dynamics. FNOtD substantially improves
long-term prediction stability and consistency with underlying physical
dynamics in challenging high-frequency settings compared to the standard FNO.
It also provides competitive predictive skill relative to a state-of-the-art
numerical ocean model, while requiring significantly lower computational cost.

</details>


### [223] [Causality $\neq$ Decodability, and Vice Versa: Lessons from Interpreting Counting ViTs](https://arxiv.org/abs/2510.09794)
*Lianghuan Huang,Yingshan Chang*

Main category: cs.LG

TL;DR: 在ViTs中，我们发现信息的可解码性与状态的因果影响力之间存在系统性不匹配，这揭示了隐藏的计算回路。


<details>
  <summary>Details</summary>
Motivation: 机械可解释性研究中，可解码性（信息可恢复性）与因果性（状态对输出的功能影响）常被混淆。本研究旨在探究两者在神经网络中的关系。

Method: 使用激活修补（activation patching）技术，通过在干净-损坏图像对之间移植激活来测试ViTs中空间和CLS标记的因果作用。同时，训练线性探测器评估不同深度计数信息的可解码性。

Result: 结果显示系统性不匹配：中间层对象标记因果影响力强但可解码性弱；最终层对象标记可解码性强但功能上惰性。CLS标记在中间层变得可解码，但在最终层才获得因果能力。

Conclusion: 可解码性与因果性反映了表征的互补维度——“存在什么信息”与“使用了什么信息”——其差异可以揭示隐藏的计算回路。

Abstract: Mechanistic interpretability seeks to uncover how internal components of
neural networks give rise to predictions. A persistent challenge, however, is
disentangling two often conflated notions: decodability--the recoverability of
information from hidden states--and causality--the extent to which those states
functionally influence outputs. In this work, we investigate their relationship
in vision transformers (ViTs) fine-tuned for object counting. Using activation
patching, we test the causal role of spatial and CLS tokens by transplanting
activations across clean-corrupted image pairs. In parallel, we train linear
probes to assess the decodability of count information at different depths. Our
results reveal systematic mismatches: middle-layer object tokens exert strong
causal influence despite being weakly decodable, whereas final-layer object
tokens support accurate decoding yet are functionally inert. Similarly, the CLS
token becomes decodable in mid-layers but only acquires causal power in the
final layers. These findings highlight that decodability and causality reflect
complementary dimensions of representation--what information is present versus
what is used--and that their divergence can expose hidden computational
circuits.

</details>


### [224] [A Unified Framework for Lifted Training and Inversion Approaches](https://arxiv.org/abs/2510.09796)
*Xiaoyu Wang,Alexandra Valavanis,Azhir Mahmood,Andreas Mang,Martin Benning,Audrey Repetti*

Main category: cs.LG

TL;DR: 本文提出了一个统一的提升训练框架，作为传统深度神经网络训练方法的替代方案。该框架通过重构优化问题并利用凸优化工具，解决了传统方法面临的挑战，实现了分布式优化和对非光滑激活函数的支持，并在图像任务中表现出优于传统训练的有效性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的传统梯度优化和反向传播训练方法存在诸多挑战，包括梯度消失或爆炸、难以处理非光滑激活函数，以及固有的串行结构限制了并行化。

Method: 本文引入了一个统一的“提升训练”框架。该框架将嵌套优化问题重构为高维、带惩罚项的约束优化问题，并整合了辅助坐标法、Fenchel提升网络和Bregman提升训练等策略。通过利用凸优化工具（特别是Bregman距离），该框架促进了分布式优化，支持非可微的近端激活函数，并改善了训练景观的条件。实现方法包括基于块坐标下降策略，涵盖确定性优化和隐式随机梯度方法，并将其应用于逆问题。

Result: 在标准图像处理任务上的数值结果表明，提升Bregman方法相比于传统训练具有更高的有效性和稳定性，特别是在使用近端激活函数的网络架构中表现尤为突出。

Conclusion: 提升训练框架，尤其是基于Bregman距离的方法，为深度神经网络的训练提供了一个强大而灵活的替代方案。它不仅能够克服传统梯度优化方法的局限性，还能支持更广泛的网络架构和优化策略，并在实际应用中展现出显著的性能优势和稳定性。

Abstract: The training of deep neural networks predominantly relies on a combination of
gradient-based optimisation and back-propagation for the computation of the
gradient. While incredibly successful, this approach faces challenges such as
vanishing or exploding gradients, difficulties with non-smooth activations, and
an inherently sequential structure that limits parallelisation. Lifted training
methods offer an alternative by reformulating the nested optimisation problem
into a higher-dimensional, constrained optimisation problem where the
constraints are no longer enforced directly but penalised with penalty terms.
This chapter introduces a unified framework that encapsulates various lifted
training strategies, including the Method of Auxiliary Coordinates, Fenchel
Lifted Networks, and Lifted Bregman Training, and demonstrates how diverse
architectures, such as Multi-Layer Perceptrons, Residual Neural Networks, and
Proximal Neural Networks fit within this structure. By leveraging tools from
convex optimisation, particularly Bregman distances, the framework facilitates
distributed optimisation, accommodates non-differentiable proximal activations,
and can improve the conditioning of the training landscape. We discuss the
implementation of these methods using block-coordinate descent strategies,
including deterministic implementations enhanced by accelerated and adaptive
optimisation techniques, as well as implicit stochastic gradient methods.
Furthermore, we explore the application of this framework to inverse problems,
detailing methodologies for both the training of specialised networks (e.g.,
unrolled architectures) and the stable inversion of pre-trained networks.
Numerical results on standard imaging tasks validate the effectiveness and
stability of the lifted Bregman approach compared to conventional training,
particularly for architectures employing proximal activations.

</details>


### [225] [Temporal Lifting as Latent-Space Regularization for Continuous-Time Flow Models in AI Systems](https://arxiv.org/abs/2510.09805)
*Jeffrey Camlin*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a latent-space formulation of adaptive temporal reparametrization
for continuous-time dynamical systems. The method, called *temporal lifting*,
introduces a smooth monotone mapping $t \mapsto \tau(t)$ that regularizes
near-singular behavior of the underlying flow while preserving its conservation
laws. In the lifted coordinate, trajectories such as those of the
incompressible Navier-Stokes equations on the torus $\mathbb{T}^3$ become
globally smooth. From the standpoint of machine-learning dynamics, temporal
lifting acts as a continuous-time normalization or time-warping operator that
can stabilize physics-informed neural networks and other latent-flow
architectures used in AI systems. The framework links analytic regularity
theory with representation-learning methods for stiff or turbulent processes.

</details>


### [226] [Decomposer Networks: Deep Component Analysis and Synthesis](https://arxiv.org/abs/2510.09825)
*Mohsen Joneidi*

Main category: cs.LG

TL;DR: DecompNet是一种语义自编码器，通过独特的“除自身外所有”残差更新规则和高斯-赛德尔风格的块坐标下降，将输入分解为多个相互竞争且可解释的语义组件。


<details>
  <summary>Details</summary>
Motivation: 传统自编码器将输入压缩为单一潜在表示，缺乏可解释性。研究旨在提出一种新的分解网络，能够生成稀疏且语义有意义的表示，并通过组件间的显式竞争实现更有效的分解。

Method: 提出Decomposer Networks (DecompNet)，一个包含N个并行分支的语义自编码器。每个分支的输入定义为原始信号减去所有其他分支的重建结果（“除自身外所有”残差更新规则）。通过将高斯-赛德尔风格的块坐标下降展开为可微分网络，强制组件间进行显式竞争。

Result: 该方法能够生成稀疏（parsimonious）且语义有意义（semantically meaningful）的表示。

Conclusion: DecompNet是首个实现“除自身外所有”残差更新规则的语义自编码器，其创新性超越了线性分解方法和现有对象中心架构。

Abstract: We propose the Decomposer Networks (DecompNet), a semantic autoencoder that
factorizes an input into multiple interpretable components. Unlike classical
autoencoders that compress an input into a single latent representation, the
Decomposer Network maintains N parallel branches, each assigned a residual
input defined as the original signal minus the reconstructions of all other
branches. By unrolling a Gauss--Seidel style block-coordinate descent into a
differentiable network, DecompNet enforce explicit competition among
components, yielding parsimonious, semantically meaningful representations. We
situate our model relative to linear decomposition methods (PCA, NMF), deep
unrolled optimization, and object-centric architectures (MONet, IODINE, Slot
Attention), and highlight its novelty as the first semantic autoencoder to
implement an all-but-one residual update rule.

</details>


### [227] [An Exploration of Non-Euclidean Gradient Descent: Muon and its Many Variants](https://arxiv.org/abs/2510.09827)
*Michael Crawshaw,Chirag Modi,Mingrui Liu,Robert M. Gower*

Main category: cs.LG

TL;DR: 本文系统探索了神经网络非欧几里得梯度下降方法，提出了更鲁棒的MuonMax优化器和结合Momo的方法，推荐使用MuonMax与Momo的组合以减少超参数调优成本。


<details>
  <summary>Details</summary>
Motivation: 定义神经网络上的最速下降法需要选择层规范、聚合方式及是否归一化。研究旨在系统探索这些选择，并改进现有优化器（如Adam和Muon）的鲁棒性。

Method: 研究者系统性地探索了跨层聚合范数的不同替代方案，将Adam和Muon形式化为非欧几里得梯度下降类型，并推导了Muon优化器的新变体。通过全面的实验评估优化器，并展示了如何将任何非欧几里得梯度方法与基于模型的动量（Momo）结合。

Result: Muon对学习率敏感，而新变体MuonMax显著更鲁棒。Muon的Momo变体对超参数调优的鲁棒性显著增强，且通常能获得更好的验证分数。

Conclusion: 对于最佳超参数未知的新任务，建议结合使用Momo与MuonMax，以节省昂贵的超参数调优成本。

Abstract: To define a steepest descent method over a neural network, we need to choose
a norm for each layer, a way to aggregate these norms across layers, and
whether to use normalization. We systematically explore different alternatives
for aggregating norms across layers, both formalizing existing combinations of
Adam and the recently proposed Muon as a type of non-Euclidean gradient
descent, and deriving new variants of the Muon optimizer. Through a
comprehensive experimental evaluation of the optimizers within our framework,
we find that Muon is sensitive to the choice of learning rate, whereas a new
variant we call MuonMax is significantly more robust. We then show how to
combine any non-Euclidean gradient method with model based momentum (known as
Momo). The new Momo variants of Muon are significantly more robust to
hyperparameter tuning, and often achieve a better validation score. Thus for
new tasks, where the optimal hyperparameters are not known, we advocate for
using Momo in combination with MuonMax to save on costly hyperparameter tuning.

</details>


### [228] [Harnessing Self-Supervised Deep Learning and Geostationary Remote Sensing for Advancing Wildfire and Associated Air Quality Monitoring: Improved Smoke and Fire Front Masking using GOES and TEMPO Radiance Data](https://arxiv.org/abs/2510.09845)
*Nicholas LaHaye,Thilanka Munashinge,Hugo Lee,Xiaohua Pan,Gonzalo Gonzalez Abad,Hazem Mahmoud,Jennifer Wei*

Main category: cs.LG

TL;DR: 本研究利用NASA TEMPO卫星的实时数据和自监督深度学习技术，通过绘制野火前沿和烟羽的实时扩散，来改进美国西部野火和空气质量管理，并证明其性能优于现有操作产品。


<details>
  <summary>Details</summary>
Motivation: 旨在利用NASA TEMPO卫星的逐小时数据和自监督深度学习的进步，改进美国西部的野火和空气质量管理。

Method: 开发了一个创新的自监督深度学习系统，该系统利用NASA TEMPO和GOES-18卫星的逐小时数据，绘制野火前沿和烟羽的近实时逐小时扩散。

Result: 成功地利用GOES-18和TEMPO数据区分了烟羽和云层；不同传感模态生成的烟雾和火灾掩模之间具有高度一致性；在相同案例中比现有操作产品有显著改进。

Conclusion: 该工作证明了利用TEMPO卫星数据和自监督深度学习技术，在提高美国西部野火和空气质量管理方面的巨大潜力，并在实时野火和烟羽追踪上表现出有效性及对现有产品的显著改进。

Abstract: This work demonstrates the possibilities for improving wildfire and air
quality management in the western United States by leveraging the unprecedented
hourly data from NASA's TEMPO satellite mission and advances in self-supervised
deep learning. Here we demonstrate the efficacy of deep learning for mapping
the near real-time hourly spread of wildfire fronts and smoke plumes using an
innovative self-supervised deep learning-system: successfully distinguishing
smoke plumes from clouds using GOES-18 and TEMPO data, strong agreement across
the smoke and fire masks generated from different sensing modalities as well as
significant improvement over operational products for the same cases.

</details>


### [229] [CALM: A Causal Analysis Language Model for Tabular Data in Complex Systems with Local Scores, Conditional Independence Tests, and Relation Attributes](https://arxiv.org/abs/2510.09846)
*Zhenjiang Fan,Zengyi Qin,Yuanning Zheng,Bo Xiong,Summer Han*

Main category: cs.LG

TL;DR: 本文提出CALM，一个基于Mamba架构的新型因果分析语言模型，专门用于从表格数据中发现因果关系。它整合了多种因果证据，解决了现有方法的局限性，并在模拟和真实生物数据中表现出91%以上的准确性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中进行因果发现对科学领域至关重要，但现有方法（如基于约束和基于分数的方法）存在局限性，包括无法解析因果方向、仅限于线性关联、对忠实性假设敏感以及搜索效率低下。尽管大型语言模型（LLMs）具有强大的推理能力，但它们主要为文本设计，与表格形式的因果数据存在根本性差异，因此难以直接应用。

Method: 本研究引入了CALM（Causal Analysis Language Model），一个专为复杂系统中的表格数据设计的因果分析语言模型。CALM采用Mamba架构，通过对变量的成对关系进行分类来识别因果模式。它整合了全面的证据套件，包括局部因果分数、条件独立性测试和关系属性，以捕获广泛的线性、非线性和条件因果机制。模型在一个包含合成数据（线性、混合、非线性模型）和10个具有严格验证因果关系的真实世界生物数据集的综合语料库上进行训练。

Result: 经验评估表明，CALM在模拟研究中显著优于现有方法，准确率超过91%。在真实世界的应用中，CALM成功识别了丙型肝炎病毒进展中的因果因素。

Conclusion: 这项工作通过成功将语言模型的模式识别能力应用于复杂的表格数据，在准确和可泛化的因果发现方面迈出了重要一步。

Abstract: Causal discovery from observational data is fundamental to scientific fields
like biology, where controlled experiments are often impractical. However,
existing methods, including constraint-based (e.g., PC, causalMGM) and
score-based approaches (e.g., NOTEARS), face significant limitations. These
include an inability to resolve causal direction, restrictions to linear
associations, sensitivity to violations of the faithfulness assumption, and
inefficiency in searching vast hypothesis spaces. While large language models
(LLMs) offer powerful reasoning capabilities, their application is hindered by
a fundamental discrepancy: they are designed for text, while most causal data
is tabular. To address these challenges, we introduce CALM, a novel causal
analysis language model specifically designed for tabular data in complex
systems. CALM leverages a Mamba-based architecture to classify causal patterns
from pairwise variable relationships. It integrates a comprehensive suite of
evidence, including local causal scores, conditional independence tests, and
relational attributes, to capture a wide spectrum of linear, nonlinear, and
conditional causal mechanisms. Trained on a diverse corpus of synthetic data
(from linear, mixed, and nonlinear models) and 10 real-world biological
datasets with rigorously validated causal relationships, our model ensures
robustness and generalizability. Empirical evaluation demonstrates that CALM
significantly outperforms existing methods in both simulation studies,
achieving over 91% accuracy, and in a real-world application identifying causal
factors in Hepatitis C virus progression. This work represents a significant
step towards accurate and generalizable causal discovery by successfully
adapting the pattern recognition capabilities of language models to the
intricacies of tabular data.

</details>


### [230] [ProxRouter: Proximity-Weighted LLM Query Routing for Improved Robustness to Outliers](https://arxiv.org/abs/2510.09852)
*Shivam Patel,Neharika Jali,Ankur Mallick,Gauri Joshi*

Main category: cs.LG

TL;DR: ProxRouter通过指数倾斜聚合机制，提高了非参数LLM查询路由器对异常查询的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM查询路由器（参数化和非参数化）在处理异常查询时表现不佳，且受限于训练集多样性，难以泛化到新场景。

Method: 提出ProxRouter，采用指数倾斜聚合机制，平衡非参数路由器的偏差和方差，以增强对异常值的鲁棒性。

Result: 实验证明，ProxRouter在保持正常查询性能的同时，显著提升了异常查询的路由能力，且开销极小。

Conclusion: ProxRouter为非参数LLM查询路由器提供了一种有效且低开销的解决方案，提高了其对异常查询的泛化能力。

Abstract: Large language model (LLM) query routers are critical to modern AI platforms
as they seek to improve efficiency by assigning inference queries to accurate,
yet low-cost models. Parametric routers typically use trained neural networks
for LLM selection but suffer from retraining and maintenance overheads.
Nonparametric routers are training-free, instead estimating LLM accuracy and
cost via similarity between encodings of the input query and training set
queries. However, like their parametric counterparts, nonparametric routers
struggle to generalize to outlier queries, an issue exacerbated by limited
diversity in training sets which are costly to expand and difficult to keep
current with ever-evolving use cases. We propose ProxRouter, which applies an
exponentially tilted aggregation mechanism to balance bias and variance in
nonparametric routers, improving their robustness to outliers. Experiments show
ProxRouter enhances outlier routing while preserving inlier performance with
minimal overhead.

</details>


### [231] [WARC-Bench: Web Archive Based Benchmark for GUI Subtask Executions](https://arxiv.org/abs/2510.09872)
*Sanjari Srivastava,Gang Li,Cheng Chang,Rishu Garg,Manpreet Kaur,Charlene Y. Lee,Yuezhang Li,Yining Mao,Ignacio Cases,Yanan Xie,Peng Qi*

Main category: cs.LG

TL;DR: 引入WARC-Bench评估网络智能体子任务掌握能力，发现现有模型表现不佳。通过SFT和RLVR训练提高了开源模型性能，强调了子任务对鲁棒网络导航的重要性。


<details>
  <summary>Details</summary>
Motivation: 训练网络智能体导航复杂真实网站需要其掌握“子任务”能力，但现有基准未能充分评估多模态AI智能体在这些短周期交互上的表现。研究旨在填补这一评估空白并提升开源模型在该能力上的表现。

Method: 提出了WARC-Bench，一个包含438个任务的新型网络导航基准，利用Web ARChive文件实现动态、真实的网页沙盒交互，用于评估多模态AI智能体在子任务上的表现。探索了监督微调（SFT）和带可验证奖励的强化学习（RLVR）两种训练技术来改进开源模型。

Result: 领先的计算机使用模型在WARC-Bench上的最高成功率为64.8%，表明该基准具有挑战性。SFT模型获得了48.8%的成功率。即使在数据稀缺环境下，在SFT检查点上使用RLVR训练可将分数提高到52.8%，超越了许多前沿模型。

Conclusion: 掌握网络子任务对于鲁棒的网络规划和导航至关重要。现有基准未能充分评估这种能力，WARC-Bench为评估和提升这一关键能力提供了新的视角和工具。

Abstract: Training web agents to navigate complex, real-world websites requires them to
master $\textit{subtasks}$ - short-horizon interactions on multiple UI
components (e.g., choosing the correct date in a date picker, or scrolling in a
container to extract information). We introduce WARC-Bench (Web Archive
Benchmark), a novel web navigation benchmark featuring 438 tasks designed to
evaluate multimodal AI agents on subtasks. WARC-Bench enables sandboxed
interactions with dynamic and realistic webpages using Web ARChive files. We
show that WARC-Bench is challenging for leading computer-use models, with the
highest observed success rate being 64.8%. To improve open source models on
subtask, we explore two common training techniques: supervised fine-tuning
(SFT) and reinforcement learning with verifiable rewards (RLVR). Experiments
show that SFT models obtain a 48.8% success rate on the benchmark. Training
with RLVR over SFT checkpoints, even in data-scarce settings, improves the
score to 52.8% on WARC-Bench, outperforming many frontier models. Our analysis
concludes that mastering these subtasks is essential for robust web planning
and navigation, and is a capability not extensively evaluated by existing
benchmarks.

</details>


### [232] [Myopic Bayesian Decision Theory for Batch Active Learning with Partial Batch Label Sampling](https://arxiv.org/abs/2510.09877)
*Kangping Hu,Stephen Mussmann*

Main category: cs.LG

TL;DR: 本文利用贝叶斯决策理论（BDT）为主动学习提供通用原则，推导并统一了现有算法（如EER、EPIG），并针对大批量采样挑战提出了ParBaLS EPIG算法，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 主动学习领域存在大量采集函数，实践者难以选择最佳方案；同时，现有基于BDT的方法在处理大批量采样时面临计算复杂或性能下降问题。

Method: 1. 在短视框架下，推导了贝叶斯主动学习的贝叶斯决策理论（BDT）。2. 证明了EER、EPIG及BAIT等算法可从BDT及其渐近近似中导出。3. 提出并推导了针对EPIG算法的“部分批量标签采样（ParBaLS）”，以解决大规模批量采样的挑战。

Result: 在多个数据集上，针对固定预算和神经网络嵌入上的贝叶斯逻辑回归，ParBaLS EPIG表现出卓越的性能。

Conclusion: BDT为主动学习决策提供了统一的理论基础，能整合和导出多种现有算法。ParBaLS EPIG算法有效解决了大规模批量主动学习的扩展性问题，并在实践中展示了优越性。

Abstract: Over the past couple of decades, many active learning acquisition functions
have been proposed, leaving practitioners with an unclear choice of which to
use. Bayesian Decision Theory (BDT) offers a universal principle to guide
decision-making. In this work, we derive BDT for (Bayesian) active learning in
the myopic framework, where we imagine we only have one more point to label.
This derivation leads to effective algorithms such as Expected Error Reduction
(EER), Expected Predictive Information Gain (EPIG), and other algorithms that
appear in the literature. Furthermore, we show that BAIT (active learning based
on V-optimal experimental design) can be derived from BDT and asymptotic
approximations. A key challenge of such methods is the difficult scaling to
large batch sizes, leading to either computational challenges (BatchBALD) or
dramatic performance drops (top-$B$ selection). Here, using a particular
formulation of the decision process, we derive Partial Batch Label Sampling
(ParBaLS) for the EPIG algorithm. We show experimentally for several datasets
that ParBaLS EPIG gives superior performance for a fixed budget and Bayesian
Logistic Regression on Neural Embeddings. Our code is available at
https://github.com/ADDAPT-ML/ParBaLS.

</details>


### [233] [TAWRMAC: A Novel Dynamic Graph Representation Learning Method](https://arxiv.org/abs/2510.09884)
*Soheila Farokhi,Xiaojun Qi,Hamid Karimi*

Main category: cs.LG

TL;DR: TAWRMAC是一个新颖的动态图表示学习框架，通过整合时间匿名游走、记忆增强和邻居共现嵌入，解决了现有方法在嵌入陈旧、上下文感知和结构动态捕捉方面的不足，在动态链接预测和节点分类任务上超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 动态图表示学习对于分析演化网络至关重要。然而，现有连续时间方法面临三大挑战：1) 仅依赖节点特定记忆而缺乏邻居信息，导致嵌入陈旧；2) 未能有效捕获节点邻域间的关联性，限制了上下文感知；3) 在缺少丰富链接属性时，未能充分捕获演化图的结构动态。

Method: 本文提出TAWRMAC框架，通过集成以下机制解决现有问题：1) 记忆增强型GNN与固定时间编码增强嵌入稳定性；2) 显式捕获邻居关联性以改善上下文表示；3) 引入“带有重启的时间匿名游走”机制，区分重复交互节点与新连接节点，更好地捕捉结构动态并支持强归纳学习。

Result: 在多个基准数据集上进行的广泛实验表明，TAWRMAC在传导式和归纳式设置下，以及三种不同的负采样策略中，持续超越了现有最先进的动态链接预测和节点分类方法。

Conclusion: TAWRMAC通过提供稳定、可泛化且上下文感知的嵌入，推动了连续时间动态图学习领域的最新进展。

Abstract: Dynamic graph representation learning has become essential for analyzing
evolving networks in domains such as social network analysis, recommendation
systems, and traffic analysis. However, existing continuous-time methods face
three key challenges: (1) some methods depend solely on node-specific memory
without effectively incorporating information from neighboring nodes, resulting
in embedding staleness; (2) most fail to explicitly capture correlations
between node neighborhoods, limiting contextual awareness; and (3) many fail to
fully capture the structural dynamics of evolving graphs, especially in absence
of rich link attributes. To address these limitations, we introduce TAWRMAC-a
novel framework that integrates Temporal Anonymous Walks with Restart, Memory
Augmentation, and Neighbor Co-occurrence embedding. TAWRMAC enhances embedding
stability through a memory-augmented GNN with fixedtime encoding and improves
contextual representation by explicitly capturing neighbor correlations.
Additionally, its Temporal Anonymous Walks with Restart mechanism distinguishes
between nodes exhibiting repetitive interactions and those forming new
connections beyond their immediate neighborhood. This approach captures
structural dynamics better and supports strong inductive learning. Extensive
experiments on multiple benchmark datasets demonstrate that TAWRMAC
consistently outperforms state-of-the-art methods in dynamic link prediction
and node classification under both transductive and inductive settings across
three different negative sampling strategies. By providing stable,
generalizable, and context-aware embeddings, TAWRMAC advances the state of the
art in continuous-time dynamic graph learning. The code is available at
https://anonymous.4open.science/r/tawrmac-A253 .

</details>


### [234] [Understanding Robust Machine Learning for Nonparametric Regression with Heavy-Tailed Noise](https://arxiv.org/abs/2510.09888)
*Yunlong Feng,Qiang Wu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We investigate robust nonparametric regression in the presence of
heavy-tailed noise, where the hypothesis class may contain unbounded functions
and robustness is ensured via a robust loss function $\ell_\sigma$. Using Huber
regression as a close-up example within Tikhonov-regularized risk minimization
in reproducing kernel Hilbert spaces (RKHS), we address two central challenges:
(i) the breakdown of standard concentration tools under weak moment
assumptions, and (ii) the analytical difficulties introduced by unbounded
hypothesis spaces. Our first message is conceptual: conventional
generalization-error bounds for robust losses do not faithfully capture
out-of-sample performance. We argue that learnability should instead be
quantified through prediction error, namely the $L_2$-distance to the truth
$f^\star$, which is $\sigma$-independent and directly reflects the target of
robust estimation. To make this workable under unboundedness, we introduce a
\emph{probabilistic effective hypothesis space} that confines the estimator
with high probability and enables a meaningful bias--variance decomposition
under weak $(1+\epsilon)$-moment conditions. Technically, we establish new
comparison theorems linking the excess robust risk to the $L_2$ prediction
error up to a residual of order $\mathcal{O}(\sigma^{-2\epsilon})$, clarifying
the robustness--bias trade-off induced by the scale parameter $\sigma$.
Building on this, we derive explicit finite-sample error bounds and convergence
rates for Huber regression in RKHS that hold without uniform boundedness and
under heavy-tailed noise. Our study delivers principled tuning rules, extends
beyond Huber to other robust losses, and highlights prediction error, not
excess generalization risk, as the fundamental lens for analyzing robust
learning.

</details>


### [235] [Probabilistic bias adjustment of seasonal predictions of Arctic Sea Ice Concentration](https://arxiv.org/abs/2510.09891)
*Parsa Gooya,Reinel Sospedra-Alfonso*

Main category: cs.LG

TL;DR: 提出一种基于条件变分自编码器（cVAE）的概率误差校正框架，用于改进北极海冰浓度季节性预测。该方法能生成大规模调整后的预测集合，并提升预测的校准性、准确性及不确定性量化，优于传统确定性校正方法。


<details>
  <summary>Details</summary>
Motivation: 北极海冰浓度季节性预测对缓解海冰快速减少的影响和评估机遇至关重要。现有气候模型预测系统存在系统性偏差和复杂时空误差，且传统确定性校正方法（如气候平均值或线性回归）受限于小规模集合，无法充分量化不确定性，尤其对极端事件的预测，而决策制定急需这种不确定性量化。

Method: 开发了一个基于条件变分自编码器（cVAE）的概率误差校正框架。该模型能够映射给定偏差模型预测下的观测条件分布，并自然地生成大规模调整后的预测集合，以实现不确定性量化。

Result: 通过确定性和概率指标评估，结果显示经过调整的预测比使用气候平均值调整的预测校准性更好、更接近观测分布，并且误差更小。

Conclusion: 所提出的基于cVAE的概率误差校正框架有效改善了北极海冰浓度季节性预测的准确性和校准性，解决了传统确定性方法在不确定性量化方面的不足，并能生成大规模调整预测集合，为决策提供更可靠的信息。

Abstract: Seasonal forecast of Arctic sea ice concentration is key to mitigate the
negative impact and assess potential opportunities posed by the rapid decline
of sea ice coverage. Seasonal prediction systems based on climate models often
show systematic biases and complex spatio-temporal errors that grow with the
forecasts. Consequently, operational predictions are routinely bias corrected
and calibrated using retrospective forecasts. For predictions of Arctic sea ice
concentration, error corrections are mainly based on one-to-one post-processing
methods including climatological mean or linear regression correction and, more
recently, machine learning. Such deterministic adjustments are confined at best
to the limited number of costly-to-run ensemble members of the raw forecast.
However, decision-making requires proper quantification of uncertainty and
likelihood of events, particularly of extremes. We introduce a probabilistic
error correction framework based on a conditional Variational Autoencoder model
to map the conditional distribution of observations given the biased model
prediction. This method naturally allows for generating large ensembles of
adjusted forecasts. We evaluate our model using deterministic and probabilistic
metrics and show that the adjusted forecasts are better calibrated, closer to
the observational distribution, and have smaller errors than climatological
mean adjusted forecasts.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [236] [Towards Automated and Predictive Network-Level Energy Profiling in Reconfigurable IoT Systems](https://arxiv.org/abs/2510.09842)
*Mohammud J. Bocus,Senhui Qiu,Robert J. Piechocki,Kerstin Eder*

Main category: cs.NI

TL;DR: 本文提出一个测量驱动的网络级智能能源分析系统，为物联网生态系统提供端到端能耗可见性，并通过生成能源数据集以支持AI优化。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）网络的可持续发展面临能效瓶颈。现有研究多限于仿真或设备层面，缺乏测量驱动的网络级智能能源分析和相关数据集。

Method: 开发了一个统一的分析和预测平台，整合了蓝牙低功耗（BLE）和可见光通信（VLC）模式、环境传感及E-ink显示子系统。通过自动化、时间同步的仪器，捕获节点和网关层的精细能耗动态，并生成IoT生态系统的能源数据集。

Result: 该系统实现了分布式IoT基础设施中能源流的端到端可见性，成功生成了物联网能源数据集，为基于AI的预测和自适应能源优化提供了数据基础。在网络级IoT测试平台中验证了其在实际操作条件下的稳健性能。

Conclusion: 该研究提供了一个实用的框架和工具集，能够对IoT网络的能耗进行测量驱动的智能分析和优化，解决了能源数据稀缺性问题，并为未来AI驱动的能效管理奠定基础。

Abstract: Energy efficiency has emerged as a defining constraint in the evolution of
sustainable Internet of Things (IoT) networks. This work moves beyond
simulation-based or device-centric studies to deliver measurement-driven,
network-level smart energy analysis. The proposed system enables end-to-end
visibility of energy flows across distributed IoT infrastructures, uniting
Bluetooth Low Energy (BLE) and Visible Light Communication (VLC) modes with
environmental sensing and E-ink display subsystems under a unified profiling
and prediction platform. Through automated, time-synchronized instrumentation,
the framework captures fine-grained energy dynamics across both node and
gateway layers. We developed a suite of tools that generate energy datasets for
IoT ecosystems, addressing the scarcity of such data and enabling AI-based
predictive and adaptive energy optimization. Validated within a network-level
IoT testbed, the approach demonstrates robust performance under real operating
conditions.

</details>


### [237] [Fine-grained CDN Delegation](https://arxiv.org/abs/2510.09983)
*Ethan Thompson,Ali Sadeghi Jahromi,AbdelRahman Abdou*

Main category: cs.NI

TL;DR: 该论文提出了授权证书（DeCerts），通过修改X.509标准实现细粒度的CDN授权，允许域名所有者独立于证书颁发机构（CA）管理授权，从而增强安全性、可扩展性和可管理性，并通过修改Firefox浏览器验证了其可行性和兼容性。


<details>
  <summary>Details</summary>
Motivation: 内容分发网络（CDN）广泛使用，但现有授权方案（如RFC 9345）在授权链长度、撤销机制、允许操作和授权范围等关键方面缺乏细粒度定义，亟需更灵活、可控的授权机制。

Method: 引入授权证书（DeCerts），通过修改X.509证书标准并添加新扩展来实现细粒度CDN授权。DeCerts允许域名所有者指定授权和非授权子域，并控制CDN的授权深度。其核心原则是域名所有者可以完全独立于CA颁发DeCerts，从而在策略控制（包括撤销方法）上拥有更大灵活性。论文还讨论了多种撤销机制，并修改Firefox浏览器以支持DeCert验证作为概念验证，测试其与浏览器和TLS/HTTPS协议的可行性和兼容性。

Result: DeCerts使域名所有者能够精细控制CDN授权（如子域和授权深度），并提供独立于CA的完全自主权，带来更大的策略控制灵活性和撤销能力。通过修改Firefox浏览器，证明了DeCerts与浏览器和TLS/HTTPS协议的可行性和兼容性。

Conclusion: DeCerts增强了CDN授权的安全性、可扩展性和可管理性，为互联网服务提供了一个实用的解决方案。

Abstract: The use of Content Delivery Networks (CDNs) has significantly increased over
the past decade, with approximately 55 million websites currently relying on
CDN services. Emerging solutions, such as Delegated Credentials (RFC 9345),
lack fine-grained definitions of many critical aspects of delegation, such as
the length of delegation chains, revocation mechanism, permitted operations,
and a well-defined scope for said delegation. We present Delegation
Certificates (DeCerts), which modify X.509 certificate standard and add new
extensions to enable fine-grained CDN delegation. DeCerts allow domain owners
to specify delegated and non-delegated subdomains, and control the depth of
delegation extended by CDNs, which provides flexibility in delegation
management. But more importantly, DeCerts are built on a new principle which
provides full autonomy to domain owners-domain owners can issue DeCerts fully
independent of Certificate Authorities (CAs), and thus have greater flexibility
in policy control, including revocation methods. Such level of flexibility
would be hard to match if CAs where to issue such certificates. Revoking a
DeCert revokes delegation. We discuss multiple revocation mechanisms for a
DeCerts balancing security, performance, and delegator control. We modify
Firefox to support DeCert (i.e., proper validation) as a proof-of-concept, and
test it to demonstrate the feasibility, compatibility of DeCerts with browsers
and TLS/HTTPS protocols. DeCerts enhance the security, scalability, and
manageability of CDN delegation, offering a practical solution for Internet
services.

</details>


### [238] [Pushing the Boundaries in CBRS Band: Robust Radar Detection within High 5G Interference](https://arxiv.org/abs/2510.10040)
*Shafi Ullah Khan,Michel Kulhandjian,Debashri Roy*

Main category: cs.NI

TL;DR: 本研究利用机器学习在CBRS频段实现5G与军事雷达的频谱共享，通过提高雷达在强干扰下的检测和波形识别能力，显著降低了雷达检测所需的信干噪比阈值。


<details>
  <summary>Details</summary>
Motivation: 频谱共享对于满足日益增长的用户需求至关重要，但如何有效监管并技术上实现与现有系统（特别是军事雷达）的共存仍是重大挑战。本文旨在探索机器学习方法在公民宽带无线电服务（CBRS）频段内增强频谱共享，特别是解决商用5G信号与军事雷达系统共存问题的潜力。

Method: 采用机器学习方法，利用同相/正交（IQ）数据和频谱图来提升雷达检测和波形识别能力。通过对合成和真实世界信号的严格评估来验证所提出的模型。

Result: 所提出的机器学习模型在高达-5dB信干噪比（SINR）的5G信号强干扰下，仍能达到FCC建议的99%雷达检测准确率，远超20 SINR的传统限制。这使得99%雷达检测准确率的SINR限制从约12dB显著降低至-5dB。此外，模型还能以93%的准确率分类六种不同的雷达波形类型。

Conclusion: 机器学习方法能够有效增强频谱共享能力，通过显著提升高干扰环境下的雷达检测和波形识别性能，从而扩展商用与任务关键系统之间的共存边界。

Abstract: Spectrum sharing is a critical strategy for meeting escalating user demands
via commercial wireless services, yet its effective regulation and
technological enablement, particularly concerning coexistence with incumbent
systems, remain significant challenges. Federal organizations have established
regulatory frameworks to manage shared commercial use alongside
mission-critical operations, such as military communications. This paper
investigates the potential of machine learning (ML)-based approaches to enhance
spectrum sharing capabilities within the Citizens Broadband Radio Service
(CBRS) band, specifically focusing on the coexistence of commercial signals
(e.g., 5G) and military radar systems. We demonstrate that ML techniques can
potentially extend the Federal Communications Commission (FCC)-recommended
signal-to-interference-plus-noise ratio (SINR) boundaries by improving radar
detection and waveform identification in high-interference environments.
Through rigorous evaluation using both synthetic and real-world signals, our
findings indicate that proposed ML models, utilizing In-phase/Quadrature (IQ)
data and spectrograms, can achieve the FCC-recommended $99\%$ radar detection
accuracy even when subjected to high interference from 5G signals upto -5dB
SINR, exceeding the required limits of $20$ SINR. Our experimental studies
distinguish this work from the state-of-the-art by significantly extending the
SINR limit for $99\%$ radar detection accuracy from approximately $12$ dB down
to $-5$ dB. Subsequent to detection, we further apply ML to analyze and
identify radar waveforms. The proposed models also demonstrate the capability
to classify six distinct radar waveform types with $93\%$ accuracy.

</details>


### [239] [Waves of Imagination: Unconditional Spectrogram Generation using Diffusion Architectures](https://arxiv.org/abs/2510.10044)
*Rahul Vanukuri,Shafi Ullah Khan,Talip Tolga Sarı,Gokhan Secinti,Diego Patiño,Debashri Roy*

Main category: cs.NI

TL;DR: 本文提出一种基于扩散的生成模型，用于合成CBRS频段的雷达及无线信号频谱图，并通过预训练显著加速了雷达检测模型的收敛。


<details>
  <summary>Details</summary>
Motivation: CBRS等共享频段需鲁棒的雷达检测算法，但真实世界雷达信号数据稀疏且难以标注，导致缺乏大规模、平衡的频谱图数据集，限制了AI模型在频谱管理中的性能和泛化能力。

Method: 提出一个基于扩散的生成模型，用于合成包含LTE、5G和雷达信号五种不同类别的CBRS频段逼真且多样的频谱图。

Result: 通过SSIM和PSNR指标验证了生成频谱图的结构和统计保真度。在真实世界雷达检测任务中，利用生成频谱图进行预训练使模型收敛速度提升了51.5%。

Conclusion: 所提出的扩散生成模型能有效合成逼真频谱图，并通过数据增强显著提高雷达检测AI模型的训练效率和收敛速度，缓解了数据稀缺性问题。

Abstract: The growing demand for effective spectrum management and interference
mitigation in shared bands, such as the Citizens Broadband Radio Service
(CBRS), requires robust radar detection algorithms to protect the military
transmission from interference due to commercial wireless transmission. These
algorithms, in turn, depend on large, diverse, and carefully labeled
spectrogram datasets. However, collecting and annotating real-world radio
frequency (RF) spectrogram data remains a significant challenge, as radar
signals are rare, and their occurrences are infrequent. This challenge makes
the creation of balanced datasets difficult, limiting the performance and
generalizability of AI models in this domain.
  To address this critical issue, we propose a diffusion-based generative model
for synthesizing realistic and diverse spectrograms of five distinct categories
that integrate LTE, 5G, and radar signals within the CBRS band. We conduct a
structural and statistical fidelity analysis of the generated spectrograms
using widely accepted evaluation metrics Structural Similarity Index Measure
(SSIM) and Peak Signal-to-Noise Ratio (PSNR), to quantify their divergence from
the training data. Furthermore, we demonstrate that pre-training on the
generated spectrograms significantly improves training efficiency on a
real-world radar detection task by enabling $51.5\%$ faster convergence.

</details>


### [240] [Multi-Scale Diffusion Transformer for Jointly Simulating User Mobility and Mobile Traffic Pattern](https://arxiv.org/abs/2510.10158)
*Ziyi Liu,Qingyue Long,Zhiwen Xue,Huandong Wang,Yong Li*

Main category: cs.NI

TL;DR: 提出MSTDiff，一个多尺度扩散Transformer，用于联合模拟移动流量和用户轨迹，以解决数据获取难题并捕捉跨模态动态，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 移动轨迹和流量数据对城市规划、网络优化等应用至关重要，但因隐私和收集成本，大规模细粒度数据难以获取。现有研究通常独立建模高度耦合的用户轨迹与移动流量，无法捕捉其内在的跨模态动态，因此急需一个统一框架。

Method: 本文提出MSTDiff，一个多尺度扩散Transformer模型，用于联合模拟移动流量和用户轨迹。其方法包括：1) 使用离散小波变换进行多分辨率流量分解；2) 采用混合去噪网络处理连续交通量和离散位置序列；3) 设计基于城市知识图谱嵌入相似性的转换机制以引导语义轨迹生成；4) 利用带交叉注意力的多尺度Transformer捕捉轨迹与流量之间的依赖关系。

Result: 实验结果表明，MSTDiff在流量和轨迹生成任务中均超越了现有最先进的基线方法。在流量生成方面，关键统计指标的Jensen-Shannon散度 (JSD) 降低高达17.38%；在轨迹生成方面，JSD平均降低了39.53%。

Conclusion: MSTDiff成功提供了一个统一框架，通过有效模拟移动流量和用户轨迹的复杂耦合关系，显著提高了模拟的真实性和准确性，为相关应用提供了可靠的模拟数据。

Abstract: User mobility trajectory and mobile traffic data are essential for a wide
spectrum of applications including urban planning, network optimization, and
emergency management. However, large-scale and fine-grained mobility data
remains difficult to obtain due to privacy concerns and collection costs,
making it essential to simulate realistic mobility and traffic patterns. User
trajectories and mobile traffic are fundamentally coupled, reflecting both
physical mobility and cyber behavior in urban environments. Despite this strong
interdependence, existing studies often model them separately, limiting the
ability to capture cross-modal dynamics. Therefore, a unified framework is
crucial. In this paper, we propose MSTDiff, a Multi-Scale Diffusion Transformer
for joint simulation of mobile traffic and user trajectories. First, MSTDiff
applies discrete wavelet transforms for multi-resolution traffic decomposition.
Second, it uses a hybrid denoising network to process continuous traffic
volumes and discrete location sequences. A transition mechanism based on urban
knowledge graph embedding similarity is designed to guide semantically informed
trajectory generation. Finally, a multi-scale Transformer with cross-attention
captures dependencies between trajectories and traffic. Experiments show that
MSTDiff surpasses state-of-the-art baselines in traffic and trajectory
generation tasks, reducing Jensen-Shannon divergence (JSD) across key
statistical metrics by up to 17.38% for traffic generation, and by an average
of 39.53% for trajectory generation. The source code is available at:
https://github.com/tsinghua-fib-lab/MSTDiff .

</details>


### [241] [Hybrid MAC Protocol with Integrated Multi-Layered Security for Resource-Constrained UAV Swarm Communications](https://arxiv.org/abs/2510.10236)
*Dhrumil Bhatt,Siddharth Penumatsa,Vidushi Kumar*

Main category: cs.NI

TL;DR: 本文提出一种用于安全自适应无人机蜂群通信的软硬件协同设计框架，通过集成多度量路由、混合MAC协议和零信任安全模型，在NS-3仿真中展现出卓越的网络性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 飞行自组网（FANETs）面临高移动性、动态拓扑和资源受限等挑战。现有路由协议常单一优化路径长度或能量，忽略了网络性能、安全性与MAC层效率之间复杂的依赖关系。

Method: 引入能量感知协议栈的软硬件协同设计框架。采用多播、集群组织方式，路由决策整合动态信任分、历史链路质量和节点间距离。混合MAC协议结合竞争和调度信道访问。安全通过零信任模型实现，融合加密认证、行为声誉系统及硬件加速AES GCM加密。在NS-3仿真环境中进行比较分析。

Result: 该框架在数据包投递率、延迟、弹性（resilience）和开销方面表现出优越性。

Conclusion: 该框架为高性能无人机蜂群操作提供了一个可扩展的基础。

Abstract: Flying Ad Hoc Networks (FANETs) present unique challenges due to high node
mobility, dynamic topologies, and strict resource constraints. Existing routing
protocols often optimize for a single metric, such as path length or energy,
while neglecting the complex dependencies between network performance,
security, and MAC layer efficiency. This paper introduces a novel hardware
software co design framework for secure and adaptive UAV swarm communications,
featuring an energy aware protocol stack. The architecture employs a multicast,
clustered organization where routing decisions integrate dynamic trust scores,
historical link quality, and internodal distance. A hybrid MAC protocol
combines contention based and scheduled channel access for optimized
throughput. Security is ensured through a zero trust model that fuses
cryptographic authentication with a behavioral reputation system, alongside
hardware accelerated AES GCM encryption. Comparative analysis in an NS 3
simulation environment demonstrates the framework's superiority in packet
delivery ratio, latency, resilience, and overhead, providing a scalable
foundation for high performance swarm operations.

</details>


### [242] [A Framework for AI-Native Semantic-Based Dynamic Slicing for 6G Networks](https://arxiv.org/abs/2510.10756)
*Mayukh Roy Chowdhury,Eman Hammad,Lauri Loven,Susanna Pirttikangas,Aloizio P da Silva,Walid Saad*

Main category: cs.NI

TL;DR: 本文提出“语义切片”方法，通过考虑数据语义来动态优化6G网络中的计算连续体资源，以应对未来超密集、多样且动态的网络环境。


<details>
  <summary>Details</summary>
Motivation: 未来的6G网络环境将面临超密集、多样化和动态变化的挑战，需要先进的资源优化机制。当前网络将数据与操作分离的方法无法有效应对这种复杂性。

Method: 本文设想了一个“计算连续体”模型，其中数据内容被视为传输的关键要素，并根据语义上下文进行分析。在此基础上，提出“语义切片”方法，它是一种双管齐下的策略，能在单一物理基础设施中动态构建多个基于数据特性和需求的虚拟分区，以实现资源的灵活应用。

Result: 通过利用数据丰富的语义进行动态自主优化，该方法促进了通信、计算、信息物理系统、数据流和人工智能的集成，为跨层设计提供了新机遇，并推动了语义通信的发展。

Conclusion: 语义切片作为一种新颖的、动态的资源管理范式，超越了现有静态切片技术，通过深入利用数据语义，为未来6G网络中的计算连续体提供了有效的资源优化和跨层集成能力。

Abstract: In the ensuing ultra-dense and diverse environment in future \ac{6G}
communication networks, it will be critical to optimize network resources via
mechanisms that recognize and cater to the diversity, density, and dynamicity
of system changes. However, coping with such environments cannot be done
through the current network approach of compartmentalizing data as distinct
from network operations. Instead, we envision a computing continuum where the
content of the transmitted data is considered as an essential element in the
transmission of that data, with data sources and streams analyzed and distilled
to their essential elements, based on their semantic context, and then
processed and transmitted over dedicated slices of network resources. By
exploiting the rich content and semantics within data for dynamic and
autonomous optimization of the computing continuum, this article opens the door
to integrating communication, computing, cyber-physical systems, data flow, and
AI, presenting new and exciting opportunities for cross-layer design. We
propose semantic slicing, a two-pronged approach that builds multiple virtual
divisions within a single physical and data infrastructure, each with its own
distinct characteristics and needs. We view semantic slicing as a novel shift
from current static slicing techniques, extending existing slicing approaches
such that it can be applied dynamically at different levels and categories of
resources in the computing continuum. Further it propels the advancement of
semantic communication via the proposed architectural framework.

</details>


### [243] [Zephyrus: Scaling Gateways Beyond the Petabit-Era with DPU-Augmented Hierarchical Co-Offloading](https://arxiv.org/abs/2510.11043)
*Yuemeng Xu,Haoran Chen,Jiarui Guo,Mingwei Cui,Qiuheng Yin,Cheng Dong,Daxiang Kang,Xian Wu,Chenmin Sun,Peng He,Yang Gao,Lirong Lai,Kai Wang,Hongyu Wu,Tong Yang,Xiyun Xu*

Main category: cs.NI

TL;DR: Zephyrus是字节跳动为应对海量云流量而设计的生产级云网关，通过将DPU与Tofino网关深度集成，并采用分层协同卸载架构，实现了更高的吞吐量、更低的功耗和成本。


<details>
  <summary>Details</summary>
Motivation: 字节跳动的云网关在拍比特规模下运行，面临日益增长的业务流量带来的巨大资源压力，传统网关难以持续。虽然DPU提供了更大的表格容量和更强的可编程性，但将其与Tofino网关实际集成以满足业务需求仍是未探索的挑战。

Method: 本文提出了Zephyrus，一个基于统一P4管道的生产级云网关，将高性能Tofino和功能丰富的DPU结合起来。Zephyrus引入了分层协同卸载架构（HLCO）来协调异构网关中的流量，实现了超过99%的硬件卸载，并保留了复杂操作的软件回退路径。

Result: Zephyrus在吞吐量上比LuoShen（NSDI '24）高33%，功耗降低21%，硬件成本降低14%。与基于FPGA的Albatross（SIGCOMM '25）相比，Zephyrus的吞吐量翻倍，总拥有成本（TCO）显著降低，显示出卓越的单位成本性能。

Conclusion: Zephyrus成功解决了DPU与Tofino网关集成的挑战，提供了卓越的性能和成本效益。论文还分享了在生产规模下开发和运营Zephyrus的关键经验，为研究人员和实践者设计高性能云网关提供了宝贵的参考。

Abstract: Operating at petabit-scale, ByteDance's cloud gateways are deployed at
critical aggregation points to orchestrate a wide array of business traffic.
However, this massive scale imposes significant resource pressure on our
previous-generation cloud gateways, rendering them unsustainable in the face of
ever-growing cloud-network traffic. As the DPU market rapidly expands, we see a
promising path to meet our escalating business traffic demands by integrating
DPUs with our established Tofino-based gateways. DPUs augment these gateways
with substantially larger table capacities and richer programmability without
compromising previously low-latency and high-throughput forwarding. Despite
compelling advantages, the practical integration of DPUs into cloud gateways
remains unexplored, primarily due to underlying challenges. In this paper, we
present Zephyrus, a production-scale gateway built upon a unified P4 pipeline
spanning high-performance Tofino and feature-rich DPUs, which successfully
overcomes these challenges. We further introduce a hierarchical co-offloading
architecture (HLCO) to orchestrate traffic flow within this heterogeneous
gateway, achieving > 99% hardware offloading while retaining software fallback
paths for complex operations. Zephyrus outperforms LuoShen (NSDI '24) with 33%
higher throughput and our evaluation further indicates 21% lower power
consumption and 14% lower hardware cost. Against FPGA-based systems, Albatross
(SIGCOMM '25), it doubles the throughput at a substantially lower Total Cost of
Ownership (TCO), showcasing its superior performance-per-dollar. Beyond these
performance gains, we also share key lessons from several years of developing
and operating Zephyrus at production scale. We believe these insights provide
valuable references for researchers and practitioners designing performant
cloud gateways.

</details>


### [244] [Graph Neural Network-Based Multicast Routing for On-Demand Streaming Services in 6G Networks](https://arxiv.org/abs/2510.11109)
*Xiucheng Wang,Zien Wang,Nan Cheng,Wenchao Xu,Wei Quan,Xuemin Shen*

Main category: cs.NI

TL;DR: 本文提出了一种基于图神经网络（GNN）的多播路由框架，通过结合强化学习、图注意力网络（GAT）和长短期记忆网络（LSTM），在6G网络中实现低成本、满足用户特定QoS需求的多播传输，同时展现了接近最优的性能、显著降低的计算复杂度和强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络中实时立体流、多感官扩展现实等带宽密集型应用的增加，需要智能多播路由方案来提供可扩展的差异化服务质量（QoS）。传统路由算法计算成本高或结构僵化，难以支持异构用户需求，导致资源利用率低下。现有神经网络方法则缺乏拓扑泛化性和可扩展性。

Method: 该研究提出了一个基于GNN的多播路由框架。将路由问题表述为受限最小流优化任务，并开发了一种强化学习算法，通过路径复用和适应网络动态来顺序构建高效的多播树。框架中，图注意力网络（GAT）用作编码器以提取上下文感知的节点嵌入，而长短期记忆（LSTM）模块则用于建模路由决策中的序列依赖性。

Result: 广泛仿真表明，所提出的方法能够紧密逼近基于动态规划的最优解决方案，同时显著降低了计算复杂性。结果还证实了该方法对大规模和动态网络拓扑的强大泛化能力。

Conclusion: 该方法在6G多媒体传输场景中具有实时部署的潜力，能够有效解决未来网络对智能、高效、可扩展多播路由的需求。

Abstract: The increase of bandwidth-intensive applications in sixth-generation (6G)
wireless networks, such as real-time volumetric streaming and multi-sensory
extended reality, demands intelligent multicast routing solutions capable of
delivering differentiated quality-of-service (QoS) at scale. Traditional
shortest-path and multicast routing algorithms are either computationally
prohibitive or structurally rigid, and they often fail to support heterogeneous
user demands, leading to suboptimal resource utilization. Neural network-based
approaches, while offering improved inference speed, typically lack topological
generalization and scalability. To address these limitations, this paper
presents a graph neural network (GNN)-based multicast routing framework that
jointly minimizes total transmission cost and supports user-specific video
quality requirements. The routing problem is formulated as a constrained
minimum-flow optimization task, and a reinforcement learning algorithm is
developed to sequentially construct efficient multicast trees by reusing paths
and adapting to network dynamics. A graph attention network (GAT) is employed
as the encoder to extract context-aware node embeddings, while a long
short-term memory (LSTM) module models the sequential dependencies in routing
decisions. Extensive simulations demonstrate that the proposed method closely
approximates optimal dynamic programming-based solutions while significantly
reducing computational complexity. The results also confirm strong
generalization to large-scale and dynamic network topologies, highlighting the
method's potential for real-time deployment in 6G multimedia delivery
scenarios. Code is available at https://github.com/UNIC-Lab/GNN-Routing.

</details>


### [245] [Visible Light Communication for Vehicular Networks: A Tutorial](https://arxiv.org/abs/2510.11123)
*Pedro E. Gória Silva,Eduardo S. Lima,Jules M. Moualeu,Mohamed Korium,Pedro H. J. Nardelli*

Main category: cs.NI

TL;DR: 本文提供了一份关于基于可见光通信（VLC）的车载网络实现的全面教程，涵盖了系统特性、关键问题、实践影响、挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 为实现实时安全的车载网络和智能交通系统（ITS）愿景，需要可靠、高速的短距离通信。可见光通信（VLC）作为新兴技术具有潜力，但其在车载网络集成中面临诸多实施问题。

Method: 本文采用教程形式，详细分析了基于VLC的车载网络的实现。具体阐述了系统实现特性、相关问题、收发信机结构（基于光电探测器和摄像头）、标准化、拓扑类型，并深入讨论了太阳光、人工光源、闪烁、调光、吞吐量增强、上行安全及移动性对实际实现的影响。

Result: 文章全面讨论了VLC-based车载网络的实现特性、系统结构、标准化和拓扑，并深入分析了外部因素（如光照、移动性）以及技术挑战（如闪烁、安全）对其性能的实际影响。

Conclusion: 本文总结了VLC-based车载网络实现中的关键挑战和潜在解决方案，并为未来研究指明了方向，旨在促进商业VLC-based车载系统的发展。

Abstract: The advent of the fifth-generation technology promises to bring about more
vertical applications and emerging services that include vehicular networks and
intelligent transportation systems (ITSs). To achieve their vision of real-time
and safetyapplications, vehicular networks rely on short-range to medium-range
communications. One emerging technology that aims to provide reliability and
high-data rate in short-range communications is the visible light
communications (VLC). Due to its remarkable advantages, some studies have
recently investigated the integration of VLC in vehicular networks and ITSs.
Despite their attractive features, such networks also face several
implementation issues. This paper provides an extended tutorial on the
implementation of VLC-based vehicular networks. To begin with, we present the
implementation characteristics of these systems and discuss some related
issues. The underlying system considers a general structure with transmitters,
channels, and receivers based on photodetectors and cameras, as well as
standardization efforts and types of topologies. In addition, we discuss the
impact of the sun and artificial light sources, flickering, dimming, throughput
enhancement, uplink security, and mobility on practical implementation.
Finally, we highlight some key challenges and potential solutions and provide
some directions for future research investigations that could constitute an
advancement toward the development of commercial VLC-based vehicular systems.

</details>


### [246] [Age of Information-Aware Cognitive Shared Access Networks with Energy Harvesting](https://arxiv.org/abs/2510.11198)
*Georgios Smpokos,Dionysis Xenakis,Marios Kountouris,Nikolaos Pappas*

Main category: cs.NI

TL;DR: 本文研究了具有能量收集功能的认知共享接入网络中主用户的信息年龄（AoI）性能，考虑了次用户空间分布、不同流量模式以及区域性干扰和能量管理机制。


<details>
  <summary>Details</summary>
Motivation: 分析带有能量收集功能的认知网络中主用户的信息年龄（AoI）性能，并解决此类网络中的干扰和能量管理问题。

Method: 构建网络模型：次用户采用泊松点过程（PPP）分布，主用户位置固定。定义流量模式：主用户突发，次用户饱和。引入两种区域管理：主发射器周围的能量收集区和主接收器周围的保护区（禁止次用户传输）。次用户基于电池状态和位置概率性接入信道。研究目标是分析主用户在三种不同分组管理策略下的AoI性能。

Result: 摘要中未提供具体研究结果。

Conclusion: 摘要中未给出研究结论。

Abstract: This study investigates a cognitive shared access network with energy
harvesting capabilities operating under Age of Information (AoI) constraints
for the primary user. Secondary transmitters are spatially distributed
according to a homogeneous Poisson Point Process (PPP), while the primary user
is located at a fixed position. The primary transmitter handles bursty packet
arrivals, whereas secondary users operate under saturated traffic conditions.
To manage interference and energy, two distinct zones are introduced: an energy
harvesting zone around the primary transmitter and a guard zone around the
primary receiver, within which secondary transmissions are prohibited.
Secondary users access the channel probabilistically, with access decisions
depending on their current battery state (charged or empty) and their location
relative to the guard zone. Our objective is to analyze the primary user's AoI
performance under three distinct packet management policies.

</details>


### [247] [From Prompts to Packets: A View from the Network on ChatGPT, Copilot, and Gemini](https://arxiv.org/abs/2510.11269)
*Antonio Montieri,Alfredo Nascita,Antonio Pescapè*

Main category: cs.NI

TL;DR: 本研究深入分析了ChatGPT、Copilot和Gemini三款领先生成式AI聊天机器人在安卓移动应用上的网络流量，揭示了其独特的应用和内容特定流量模式，并与传统消息应用进行比较，强调了对移动网络监控和管理的新挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI聊天机器人已普及，但其网络流量特性仍未被充分探索。研究旨在探究GenAI流量的独特性、与现有流量类别的差异及其对网络使用的新影响。

Method: 通过专用捕获架构，收集了GenAI应用在安卓手机上进行文本和图像生成时的流量数据。构建了两个数据集：一个60小时的通用数据集（无限制提示）和一个受控数据集（相同提示，并与传统消息应用对比）。方法包括流量的细粒度特征化（追踪、流、协议级别）和多模态马尔可夫链建模数据包序列动态，以及基于负载的SNI遮挡分析。

Result: 分析揭示了应用和内容特定的流量模式，尤其是在流量、上下行配置文件和协议采用方面。TLS占主导地位，其中Gemini大量使用QUIC，ChatGPT专攻TLS 1.3。存在应用和内容特定的SNI值，遮蔽SNI可使GenAI应用流量分类的F1分数降低高达20个百分点。与传统消息应用相比，GenAI聊天机器人展现出独特的流量特征，如持续的上行活动。

Conclusion: GenAI聊天机器人对移动网络带来了新的压力因素（如持续上行活动），对网络监控和管理有直接影响。研究结果有助于理解GenAI流量，并为未来的网络优化提供依据。数据集已公开以促进复现和扩展。

Abstract: Generative AI (GenAI) chatbots are now pervasive in digital ecosystems, yet
their network traffic remains largely underexplored. This study presents an
in-depth investigation of traffic generated by three leading chatbots (ChatGPT,
Copilot, and Gemini) when accessed via Android mobile apps for both text and
image generation. Using a dedicated capture architecture, we collect and label
two complementary workloads: a 60-hour generic dataset with unconstrained
prompts, and a controlled dataset built from identical prompts across GenAI
apps and replicated via conventional messaging apps to enable one-to-one
comparisons. This dual design allows us to address practical research questions
on the distinctiveness of GenAI traffic, its differences from widely deployed
traffic categories, and its novel implications for network usage. To this end,
we provide fine-grained traffic characterization at trace, flow, and protocol
levels, and model packet-sequence dynamics with Multimodal Markov Chains. Our
analyses reveal app- and content-specific traffic patterns, particularly in
volume, uplink/downlink profiles, and protocol adoption. We highlight the
predominance of TLS, with Gemini extensively leveraging QUIC, ChatGPT
exclusively using TLS 1.3, and app- and content-specific Server Name Indication
(SNI) values. A payload-based occlusion analysis quantifies SNI's contribution
to classification: masking it reduces F1-score by up to 20 percentage points in
GenAI app traffic classification. Finally, compared with conventional messaging
apps when carrying the same content, GenAI chatbots exhibit unique traffic
characteristics, highlighting new stress factors for mobile networks, such as
sustained upstream activity, with direct implications for network monitoring
and management. We publicly release the datasets to support reproducibility and
foster extensions to other use cases.

</details>


### [248] [Network-Optimised Spiking Neural Network (NOS) Scheduling for 6G O-RAN: Spectral Margin and Delay-Tail Control](https://arxiv.org/abs/2510.11291)
*Muhammad Bilal,Xiaolong Xu*

Main category: cs.NI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This work presents a Network-Optimised Spiking (NOS) delay-aware scheduler
for 6G radio access. The scheme couples a bounded two-state kernel to a
clique-feasible proportional-fair (PF) grant head: the excitability state acts
as a finite-buffer proxy, the recovery state suppresses repeated grants, and
neighbour pressure is injected along the interference graph via delayed spikes.
A small-signal analysis yields a delay-dependent threshold $k_\star(\Delta)$
and a spectral margin $\delta = k_\star(\Delta) - gH\rho(W)$ that compress
topology, controller gain, and delay into a single design parameter. Under
light assumptions on arrivals, we prove geometric ergodicity for $\delta>0$ and
derive sub-Gaussian backlog and delay tail bounds with exponents proportional
to $\delta$. A numerical study, aligned with the analysis and a DU compute
budget, compares NOS with PF and delayed backpressure (BP) across interference
topologies over a $5$--$20$\,ms delay sweep. With a single gain fixed at the
worst spectral radius, NOS sustains higher utilisation and a smaller
99.9th-percentile delay while remaining clique-feasible on integer PRBs.

</details>


### [249] [A protocol to reduce worst-case latency in deflection-based on-chip networks](https://arxiv.org/abs/2510.11361)
*Leandro Soares Indrusiak*

Main category: cs.NI

TL;DR: 提出一种新协议，通过仅偏转数据包头部，降低偏转式片上互连网络中最坏情况下的数据包延迟。


<details>
  <summary>Details</summary>
Motivation: 旨在降低偏转式片上互连网络中最坏情况下的数据包延迟。

Method: 开发了一种新协议，强制数据包头部偏转，但其载荷不偏转。

Result: 减少了整体网络流量，并且由于降低了预注入延迟，显著降低了最坏情况下的数据包延迟。

Conclusion: 该协议通过优化数据包头部偏转，有效减少了片上互连网络中的整体流量和最坏情况下的数据包延迟。

Abstract: We present a novel protocol that reduces worst-case packet latency in
deflection-based on-chip interconnect networks. It enforces the deflection of
the header of a packet but not its payload, resulting in a reduction in overall
network traffic and, more importantly, worst-case packet latency due to
decreased pre-injection latency.

</details>


### [250] [A Flexible Multi-Agent Deep Reinforcement Learning Framework for Dynamic Routing and Scheduling of Latency-Critical Services](https://arxiv.org/abs/2510.11535)
*Vincenzo Norman Vitale,Antonia Maria Tulino,Andreas F. Molisch,Jaime Llorca*

Main category: cs.NI

TL;DR: 本文提出一种基于多智能体深度强化学习 (MA-DRL) 的网络控制框架，通过集中式路由和分布式调度，解决动态异构网络中延迟敏感信息严格端到端峰值延迟保证的挑战，并显示出优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 工业自动化、自动驾驶和增强现实等交互式应用急需在动态、异构网络中及时传输延迟敏感信息。然而，大多数现有网络控制方案仅关注平均延迟性能，无法提供严格的端到端 (E2E) 峰值延迟保证。

Method: 本文提出了一个新颖的MA-DRL网络控制框架，采用集中式路由和分布式调度架构。该框架基于多智能体深度确定性策略梯度 (MADDPG) 技术，并利用关键网络领域知识设计有效的MA-DRL策略。其中，集中式路由和分布式调度代理根据数据包寿命动态分配路径和调度传输，以最大化按时交付。该框架还具有通用性，可集成数据驱动的DRL代理和传统的基于规则的策略。

Result: 实验结果证实，所提出的框架在性能上优于传统的基于随机优化的方法。研究还深入揭示了数据驱动的DRL代理与新型基于规则的策略在高效、高性能控制延迟关键服务中的作用和相互作用。

Conclusion: 该MA-DRL网络控制框架能够为延迟敏感应用提供可靠的端到端峰值延迟保证，并通过灵活整合DRL代理和规则策略，在性能和学习复杂性之间取得平衡，为延迟关键服务的网络控制提供了有效和高性能的解决方案。

Abstract: Timely delivery of delay-sensitive information over dynamic, heterogeneous
networks is increasingly essential for a range of interactive applications,
such as industrial automation, self-driving vehicles, and augmented reality.
However, most existing network control solutions target only average delay
performance, falling short of providing strict End-to-End (E2E) peak latency
guarantees. This paper addresses the challenge of reliably delivering packets
within application-imposed deadlines by leveraging recent advancements in
Multi-Agent Deep Reinforcement Learning (MA-DRL). After introducing the
Delay-Constrained Maximum-Throughput (DCMT) dynamic network control problem,
and highlighting the limitations of current solutions, we present a novel
MA-DRL network control framework that leverages a centralized routing and
distributed scheduling architecture. The proposed framework leverages critical
networking domain knowledge for the design of effective MA-DRL strategies based
on the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) technique, where
centralized routing and distributed scheduling agents dynamically assign paths
and schedule packet transmissions according to packet lifetimes, thereby
maximizing on-time packet delivery. The generality of the proposed framework
allows integrating both data-driven \blue{Deep Reinforcement Learning (DRL)}
agents and traditional rule-based policies in order to strike the right balance
between performance and learning complexity. Our results confirm the
superiority of the proposed framework with respect to traditional stochastic
optimization-based approaches and provide key insights into the role and
interplay between data-driven DRL agents and new rule-based policies for both
efficient and high-performance control of latency-critical services.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [251] [CIRSense: Rethinking WiFi Sensing with Channel Impulse Response](https://arxiv.org/abs/2510.11374)
*Ruiqi Kong,He Chen*

Main category: eess.SP

TL;DR: 本文提出了CIRSense框架，利用信道冲激响应（CIR）及其新的分数延迟运动模型，显著提升了WiFi感知的性能和可解释性，并在呼吸监测和距离估计等应用中展现出优于现有CSI方法的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有WiFi感知方法主要依赖CSI直接估计运动信息，但忽略了CIR在时延域中集中运动能量和分离多径分量的固有优势，导致性能和可解释性受限。作者旨在通过利用CIR解决这些问题，提升WiFi感知能力。

Method: CIRSense框架基于一个新的运动模型，该模型专门表征了CIR感知中的分数延迟效应。此理论模型支持了在硬件失真补偿、高分辨率距离估计和用于扩展范围感知的子载波聚合方面的技术进步。CIRSense采用双模式设计，利用160 MHz信道带宽进行多功能感知。

Result: CIRSense在呼吸监测中实现了约0.25 bpm的平均误差，在距离估计中实现了约0.09 m的平均误差。在居住空间、远距离和多目标场景下的评估表明，CIRSense的性能优于最先进的CSI基线，在20米感知距离下，其平均精度至少提高3倍，计算效率提高4.5倍以上。

Conclusion: CIRSense通过充分利用CIR的优势，结合新的分数延迟运动模型，在WiFi感知领域实现了卓越的性能提升和更强的可解释性。它在呼吸监测和距离估计等应用中展现出显著优于CSI基线的准确性和效率，特别是在具有挑战性的远距离场景下。

Abstract: WiFi sensing based on channel state information (CSI) collected from
commodity WiFi devices has shown great potential across a wide range of
applications, including vital sign monitoring and indoor localization. Existing
WiFi sensing approaches typically estimate motion information directly from
CSI. However, they often overlook the inherent advantages of channel impulse
response (CIR), a delay-domain representation that enables more intuitive and
principled motion sensing by naturally concentrating motion energy and
separating multipath components. Motivated by this, we revisit WiFi sensing and
introduce CIRSense, a new framework that enhances the performance and
interpretability of WiFi sensing with CIR. CIRSense is built upon a new motion
model that characterizes fractional delay effects, a fundamental challenge in
CIR-based sensing. This theoretical model underpins technical advances for the
three challenges in WiFi sensing: hardware distortion compensation,
high-resolution distance estimation, and subcarrier aggregation for extended
range sensing. CIRSense, operating with a 160 MHz channel bandwidth,
demonstrates versatile sensing capabilities through its dual-mode design,
achieving a mean error of approximately 0.25 bpm in respiration monitoring and
0.09 m in distance estimation. Comprehensive evaluations across residential
spaces, far-range scenarios, and multi-target settings demonstrate CIRSense's
superior performance over state-of-the-art CSI-based baselines. Notably, at a
challenging sensing distance of 20 m, CIRSense achieves at least 3x higher
average accuracy with more than 4.5x higher computational efficiency.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [252] [Pingmark: A Textual Protocol for Universal Spatial Mentions](https://arxiv.org/abs/2510.09672)
*Kalin Dimitrov*

Main category: cs.CR

TL;DR: Pingmark是一个新的文本协议，使用`!@`符号提供通用、隐私保护的空间上下文表达方式，旨在成为开放的互联网标准。


<details>
  <summary>Details</summary>
Motivation: 现有空间上下文表达方式（如嵌入坐标或专有地图链接）缺乏通用性和简洁性。需要一种类似于`@`提及身份或`#`提及话题的文本约定，但用于物理空间，同时保护隐私并利用开放技术。

Method: Pingmark协议规范（PPS v0.1）定义了`!@`符号作为语义触发器。兼容的客户端应用将此符号解释为生成标准化的解析链接，格式为`https://pingmark.me/lat/lon/[timestamp]`。该协议不要求用户注册，依赖开放地图技术，并通过临时和本地生成位置数据来保护用户隐私。

Result: 本文介绍了Pingmark协议规范（PPS v0.1）的动机、语法和设计，并展示了其参考解析器实现。

Conclusion: Pingmark协议旨在提供一种开放、无需注册、基于开放技术且保护隐私的文本方式来表达空间位置，并最终成为一个开放的互联网空间提及标准。

Abstract: Pingmark defines a universal textual protocol for expressing spatial context
through a minimal symbol: !@. Rather than embedding coordinates or using
proprietary map links, Pingmark introduces a semantic trigger that compliant
client applications interpret to generate a standardized resolver link of the
form https://pingmark.me/lat/lon/[timestamp]. This allows location expression
to function like existing textual conventions - @ for identity or # for topics
- but for physical space. The protocol requires no user registration, relies on
open mapping technologies, and protects privacy by generating location data
ephemerally and locally. This paper presents the motivation, syntax, and design
of the Pingmark Protocol Specification (PPS v0.1), its reference resolver
implementation, and the long-term goal of establishing Pingmark as an open
Internet standard for spatial mentions.

</details>


### [253] [A Scalable, Privacy-Preserving Decentralized Identity and Verifiable Data Sharing Framework based on Zero-Knowledge Proofs](https://arxiv.org/abs/2510.09715)
*Hui Yuan*

Main category: cs.CR

TL;DR: 本文提出一个整合DID、VCs和zk-STARKs的综合框架，以解决DApp中区块链透明度与用户隐私的冲突，实现隐私保护的身份验证和数据共享，显著提高复杂计算的证明效率和安全性，并在DeFi信用评分中展示其潜力。


<details>
  <summary>Details</summary>
Motivation: 去中心化应用（DApps）的普及使得区块链透明度与用户数据隐私之间的冲突日益突出。尽管DID和VCs提供了数据主权框架，但在不损害隐私的情况下实现可信的身份验证和数据共享仍是一个重大挑战。

Method: 本文提出了一个将DIDs、VCs与高效零知识证明（ZKP）方案相结合的综合框架。主要方法包括：基于zk-STARKs构建强大的隐私保护协议，用于凭证条件证明而不泄露敏感数据；基于密码累加器设计可扩展的隐私保护凭证撤销机制；集成实用的社会密钥恢复方案以增强可用性和安全性。

Result: 通过原型实现和性能评估，框架在证明生成时间、验证开销和链上成本方面进行了量化分析。相较于基于zk-SNARKs的现有系统，本框架以更大的证明大小为代价，显著提高了复杂计算的证明者效率，并提供了更强的安全保障（无信任设置和后量子安全性）。

Conclusion: 该框架在去中心化金融（DeFi）信用评分场景中进行了案例研究，展示了其在提升资本效率和促进可信数据经济方面的巨大潜力。

Abstract: With the proliferation of decentralized applications (DApps), the conflict
between the transparency of blockchain technology and user data privacy has
become increasingly prominent. While Decentralized Identity (DID) and
Verifiable Credentials (VCs) provide a standardized framework for user data
sovereignty, achieving trusted identity verification and data sharing without
compromising privacy remains a significant challenge. This paper proposes a
novel, comprehensive framework that integrates DIDs and VCs with efficient
Zero-Knowledge Proof (ZKP) schemes to address this core issue. The key
contributions of this framework are threefold: first, it constructs a set of
strong privacy-preserving protocols based on zk-STARKs, allowing users to prove
that their credentials satisfy specific conditions (e.g., "age is over 18")
without revealing any underlying sensitive data. Second, it designs a scalable,
privacy-preserving credential revocation mechanism based on cryptographic
accumulators, effectively solving credential management challenges in
large-scale scenarios. Finally, it integrates a practical social key recovery
scheme, significantly enhancing system usability and security. Through a
prototype implementation and performance evaluation, this paper quantitatively
analyzes the framework's performance in terms of proof generation time,
verification overhead, and on-chain costs. Compared to existing
state-of-the-art systems based on zk-SNARKs, our framework, at the cost of a
larger proof size, significantly improves prover efficiency for complex
computations and provides stronger security guarantees, including no trusted
setup and post-quantum security. Finally, a case study in the decentralized
finance (DeFi) credit scoring scenario demonstrates the framework's immense
potential for unlocking capital efficiency and fostering a trusted data
economy.

</details>
