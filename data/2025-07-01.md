<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 58]
- [cs.CV](#cs.CV) [Total: 54]
- [cs.AI](#cs.AI) [Total: 45]
- [cs.LG](#cs.LG) [Total: 55]
- [cs.NI](#cs.NI) [Total: 24]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.SP](#eess.SP) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans](https://arxiv.org/abs/2506.22439)
*Javier Conde,Miguel González,María Grandury,Gonzalo Martínez,Pedro Reviriego,Mar Brysbaert*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）在心理语言学词汇特征（如唤醒度、具体性、感官关联等）上与人类评分的一致性，结果显示LLMs在处理情感/认知特征时表现优于感官关联特征，这可能与其缺乏具身认知有关。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估主要侧重于可量化的任务表现，但许多重要的语言特征（如词语的唤醒度、具体性、性别关联及感官体验等）难以客观量化。心理语言学已通过大规模人类实验积累了大量此类词汇特征数据。因此，本研究旨在利用这些现有数据，评估LLMs与人类在这些非任务性词汇特征上的对齐程度。

Method: 研究选取了一组具有代表性的LLMs，并使用两个心理语言学数据集——Glasgow norms和Lancaster norms——来评估LLMs与人类评分的一致性。Glasgow norms数据集包含唤醒度、效价、支配性、具体性、想象性、熟悉度和性别等13种特征，而Lancaster norms数据集则涵盖了内感受、味觉、嗅觉、触觉、听觉和视觉等感官特征。

Result: 研究结果表明，LLMs在Glasgow norms（涉及唤醒度、效价、支配性、具体性、想象性、熟悉度和性别）上的对齐程度普遍优于在Lancaster norms（涉及内感受、味觉、嗅觉、触觉、听觉和视觉）上的对齐程度。

Conclusion: 当前LLMs在与人类词语感官关联性方面存在潜在局限，这可能源于它们缺乏人类所具备的具身认知。本研究强调了使用心理语言学数据集评估LLMs的重要性，以揭示其更深层次的语言理解能力。

Abstract: The evaluation of LLMs has so far focused primarily on how well they can
perform different tasks such as reasoning, question-answering, paraphrasing, or
translating. For most of these tasks, performance can be measured with
objective metrics, such as the number of correct answers. However, other
language features are not easily quantified. For example, arousal,
concreteness, or gender associated with a given word, as well as the extent to
which we experience words with senses and relate them to a specific sense.
Those features have been studied for many years by psycholinguistics,
conducting large-scale experiments with humans to produce ratings for thousands
of words. This opens an opportunity to evaluate how well LLMs align with human
ratings on these word features, taking advantage of existing studies that cover
many different language features in a large number of words. In this paper, we
evaluate the alignment of a representative group of LLMs with human ratings on
two psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets
cover thirteen features over thousands of words. The results show that
alignment is \textcolor{black}{generally} better in the Glasgow norms evaluated
(arousal, valence, dominance, concreteness, imageability, familiarity, and
gender) than on the Lancaster norms evaluated (introceptive, gustatory,
olfactory, haptic, auditory, and visual). This suggests a potential limitation
of current LLMs in aligning with human sensory associations for words, which
may be due to their lack of embodied cognition present in humans and
illustrates the usefulness of evaluating LLMs with psycholinguistic datasets.

</details>


### [2] [AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents](https://arxiv.org/abs/2506.22485)
*Sudip Dasgupta,Himanshu Shankar*

Main category: cs.CL

TL;DR: 本研究提出一个AI多智能体系统，用于自动化审查高度结构化的企业文档，在速度和准确性上超越人类表现。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案对非结构化文本或有限合规性检查的关注不足，无法满足企业文档对准确性、一致性、完整性和清晰度的逐节详细评估需求。

Method: 开发了一个模块化、多智能体系统，利用LangChain、CrewAI、TruLens、Guidance等工具，通过专业AI智能体对文档进行逐节评估。评估输出标准化为机器可读模式，并设有持续监控和人工反馈循环以迭代改进和减少偏见。

Result: 定量评估显示，该系统在信息一致性上达到99%（人工为92%），错误率和偏差率减半，平均审查时间从30分钟缩短至2.5分钟，AI与专家人工判断一致性达95%。

Conclusion: 该系统为企业文档质量保证提供了一个灵活、可审计和可扩展的基础，在关键领域接近或超越人类表现。但仍需考虑高度专业领域的人工监督和大规模LLM使用的运营成本。

Abstract: This study presents a modular, multi-agent system for the automated review of
highly structured enterprise business documents using AI agents. Unlike prior
solutions focused on unstructured texts or limited compliance checks, this
framework leverages modern orchestration tools such as LangChain, CrewAI,
TruLens, and Guidance to enable section-by-section evaluation of documents for
accuracy, consistency, completeness, and clarity. Specialized agents, each
responsible for discrete review criteria such as template compliance or factual
correctness, operate in parallel or sequence as required. Evaluation outputs
are enforced to a standardized, machine-readable schema, supporting downstream
analytics and auditability. Continuous monitoring and a feedback loop with
human reviewers allow for iterative system improvement and bias mitigation.
  Quantitative evaluation demonstrates that the AI Agent-as-Judge system
approaches or exceeds human performance in key areas: achieving 99% information
consistency (vs. 92% for humans), halving error and bias rates, and reducing
average review time from 30 to 2.5 minutes per document, with a 95% agreement
rate between AI and expert human judgment. While promising for a wide range of
industries, the study also discusses current limitations, including the need
for human oversight in highly specialized domains and the operational cost of
large-scale LLM usage. The proposed system serves as a flexible, auditable, and
scalable foundation for AI-driven document quality assurance in the enterprise
context.

</details>


### [3] [Hallucination Detection with Small Language Models](https://arxiv.org/abs/2506.22486)
*Ming Cheung*

Main category: cs.CL

TL;DR: 本文提出一个框架，利用多个小型语言模型（SLMs）和检索到的上下文来验证大型语言模型（LLMs）的答案，通过分析SLM生成“Yes”标记的概率来检测幻觉，实验结果显示F1分数提高10%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在问答等任务中具有显著效用，但其响应中的幻觉问题削弱了可靠性，且在缺乏真实答案时难以检测，尤其是在问答场景中。

Method: 本文提出了一个框架，该框架整合了多个小型语言模型。通过将LLM的响应分解为单独的句子，并利用多个SLM对给定问题、响应和相关上下文生成“Yes”标记的概率来检测幻觉。

Result: 通过对包含100多组问题、答案和上下文的真实数据集进行实验验证，结果显示，与幻觉相比，检测正确响应的F1分数提高了10%。

Conclusion: 多个小型语言模型可以有效地用于答案验证，为学术和实际应用提供了一个可扩展且高效的解决方案。

Abstract: Since the introduction of ChatGPT, large language models (LLMs) have
demonstrated significant utility in various tasks, such as answering questions
through retrieval-augmented generation. Context can be retrieved using a
vectorized database, serving as a foundation for LLMs to generate responses.
However, hallucinations in responses can undermine the reliability of LLMs in
practical applications, and they are not easily detectable in the absence of
ground truth, particularly in question-and-answer scenarios. This paper
proposes a framework that integrates multiple small language models to verify
responses generated by LLMs using the retrieved context from a vectorized
database. By breaking down the responses into individual sentences and
utilizing the probability of generating "Yes" tokens from the outputs of
multiple models for a given set of questions, responses, and relevant context,
hallucinations can be detected. The proposed framework is validated through
experiments with real datasets comprising over 100 sets of questions, answers,
and contexts, including responses with fully and partially correct sentences.
The results demonstrate a 10\% improvement in F1 scores for detecting correct
responses compared to hallucinations, indicating that multiple small language
models can be effectively employed for answer verification, providing a
scalable and efficient solution for both academic and practical applications.

</details>


### [4] [PromptAug: Fine-grained Conflict Classification Using Data Augmentation](https://arxiv.org/abs/2506.22491)
*Oliver Warke,Joemon M. Jose,Faegheh Hasibi,Jan Breitsohl*

Main category: cs.CL

TL;DR: 本文提出PromptAug，一种基于大语言模型的文本数据增强方法，用于社交媒体敏感内容（如冲突行为）的检测，并在准确率和F1分数上实现了显著提升，同时通过跨学科评估揭示了增强文本中的潜在问题。


<details>
  <summary>Details</summary>
Motivation: 社交媒体冲突日益增多，但用于检测有害行为的高质量标注数据稀缺、昂贵且难以获取。此外，社交媒体平台限制数据访问，使数据增强成为替代方案。然而，LLM的安全防护机制限制了敏感内容的生成，对冲突相关数据增强构成独特挑战。

Method: 本文引入了一种名为PromptAug的创新性基于LLM的数据增强方法。为全面评估，研究采用了鲁棒的评估方法，包括极端数据稀缺场景、定量多样性分析和定性主题分析。

Result: PromptAug在冲突和情感数据集上，使准确率和F1分数均实现2%的统计显著性提升。定性主题分析揭示了增强文本中的四种问题模式：语言流畅性、幽默歧义、增强内容歧义和增强内容误解。

Conclusion: PromptAug是一种有效的数据增强方法，适用于冲突检测等敏感任务，并提供了一种结合自然语言处理和社会科学方法的独特跨学科评估。

Abstract: Given the rise of conflicts on social media, effective classification models
to detect harmful behaviours are essential. Following the
garbage-in-garbage-out maxim, machine learning performance depends heavily on
training data quality. However, high-quality labelled data, especially for
nuanced tasks like identifying conflict behaviours, is limited, expensive, and
difficult to obtain. Additionally, as social media platforms increasingly
restrict access to research data, text data augmentation is gaining attention
as an alternative to generate training data. Augmenting conflict-related data
poses unique challenges due to Large Language Model (LLM) guardrails that
prevent generation of offensive content. This paper introduces PromptAug, an
innovative LLM-based data augmentation method. PromptAug achieves statistically
significant improvements of 2% in both accuracy and F1-score on conflict and
emotion datasets. To thoroughly evaluate PromptAug against other data
augmentation methods we conduct a robust evaluation using extreme data scarcity
scenarios, quantitative diversity analysis and a qualitative thematic analysis.
The thematic analysis identifies four problematic patterns in augmented text:
Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and
Augmented Content Misinterpretation.
  Overall, this work presents PromptAug as an effective method for augmenting
data in sensitive tasks like conflict detection, offering a unique,
interdisciplinary evaluation grounded in both natural language processing and
social science methodology.

</details>


### [5] [AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text](https://arxiv.org/abs/2506.22508)
*Chenyang Shao,Tianxing Li,Chenhao Pu,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 本文提出AgentStealth，一个自强化LLM匿名化框架，通过对抗性工作流、监督适应和在线强化学习，使本地部署的小型语言模型（SLM）实现高效且保护效用的文本匿名化。


<details>
  <summary>Details</summary>
Motivation: 用户生成内容中的敏感信息泄露日益严重，现有匿名化方法存在问题：硬性替换损害文本效用；基于云的LLM成本高且存在隐私风险。此外，由于高质量监督数据有限，训练有效的本地SLM面临挑战。

Method: AgentStealth框架分三步：1) 引入基于上下文对比学习和自适应效用控制的对抗性匿名化工作流。2) 利用该工作流收集的高质量数据（包含匿名化和攻击信号）对SLM进行监督适应。3) 应用在线强化学习，模型利用内部对抗反馈迭代提升匿名化性能。

Result: 在两个数据集上，AgentStealth在匿名化有效性上超越基线12.3%，在效用上提升6.8%。其轻量级设计支持边缘设备直接部署，避免了对云服务的依赖和通信隐私风险。

Conclusion: AgentStealth提供了一种有效、高效且保护隐私的文本匿名化解决方案，适用于本地部署，显著优于现有方法，并能规避云服务的潜在风险。

Abstract: In today's digital world, casual user-generated content often contains subtle
cues that may inadvertently expose sensitive personal attributes. Such risks
underscore the growing importance of effective text anonymization to safeguard
individual privacy. However, existing methods either rely on rigid replacements
that damage utility or cloud-based LLMs that are costly and pose privacy risks.
To address these issues, we explore the use of locally deployed smaller-scale
language models (SLMs) for anonymization. Yet training effective SLMs remains
challenging due to limited high-quality supervision. To address the challenge,
we propose AgentStealth, a self-reinforcing LLM anonymization framework.First,
we introduce an adversarial anonymization workflow enhanced by In-context
Contrastive Learning and Adaptive Utility-Aware Control. Second, we perform
supervised adaptation of SLMs using high-quality data collected from the
workflow, which includes both anonymization and attack signals. Finally, we
apply online reinforcement learning where the model leverages its internal
adversarial feedback to iteratively improve anonymization performance.
Experiments on two datasets show that our method outperforms baselines in both
anonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight
design supports direct deployment on edge devices, avoiding cloud reliance and
communication-based privacy risks. Our code is open-source at
https://github.com/tsinghua-fib-lab/AgentStealth.

</details>


### [6] [Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning](https://arxiv.org/abs/2506.22510)
*Zihao Zhao,Xinlong Zhai,Jinyu Yang,Chuan Shi*

Main category: cs.CL

TL;DR: 针对图数据不同领域间语义和属性差异大、现有预训练方法难以有效整合多领域知识的问题，本文提出了MDGCL框架。该框架通过设计识别领域差异的对比学习策略和引入领域注意力机制，实现了多领域预训练和跨领域知识迁移，并在多个基准数据集上显著超越现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在NLP和CV领域取得了巨大成功，并被尝试扩展到图数据，但图数据在不同领域间的语义和属性存在巨大差异。现有传统的对比预训练策略将不同领域的对比样本视为等同，导致无法有效吸收多领域知识并生成有信息量的表示。

Method: 本文提出了一种新颖的多领域预训练和跨领域迁移框架MDGCL。在预训练阶段，设计了一种对比学习策略以识别和捕获领域差异，并引入领域标记（domain tokens）来编码领域级全局信息。在下游任务阶段，引入了领域注意力机制以实现细粒度的领域知识迁移。

Result: 在五个基准数据集上的大量实验表明，所提出的MDGCL方法显著优于现有最先进（SOTA）方法，准确率最大提升19.33%，Macro-F1分数最大提升19.13%。

Conclusion: MDGCL框架通过有效处理图数据中多领域间的固有差异，成功实现了多领域知识的吸收和跨领域迁移，显著提升了图表示学习的性能，为构建更通用的图基础模型提供了有效途径。

Abstract: Foundation models have achieved great success in natural language processing
(NLP) and computer vision (CV). Their success largely stems from the ability to
integrate multi-domain knowledge in pre-training and transfer it to target
domains. Considering graph data, especially graphs without textual features, is
ubiquitous in real-world applications such as social networks and
recommendation systems, some researchers have attempted to extend this paradigm
to the graph field, aiming to construct graph foundation models. However,
unlike CV and NLP, there are huge gaps among the semantics and properties of
graphs in different domains, while current works still adopt traditional
contrastive pre-training strategies designed in the single-domain scenario,
which regard contrastive samples from different domains as equivalent. From
experimental investigations, we discovered that inherent domain-specific
differences prevent these strategies from effectively absorbing knowledge from
different domains to generate informative representations. In this paper, we
propose a novel multi-domain pre-training and cross-domain transfer framework,
namely MDGCL.In the pre-training stage, we design a contrastive learning
strategy to substantially recognize and capture domain differences, and
introduce domain tokens to encode domain-level global information. In the
downstream stage, we introduce a domain attention mechanism to enable
fine-grained domain knowledge transfer. Extensive experiments on five benchmark
datasets have demonstrated that our method outperforms state-of-the-art
significantly, with the maximum improvement of 19.33\% on accuracy and 19.13\%
on Macro-F1 score.

</details>


### [7] [Can "consciousness" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis](https://arxiv.org/abs/2506.22516)
*Jingkai Li*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Integrated Information Theory (IIT) provides a quantitative framework for
explaining consciousness phenomenon, positing that conscious systems comprise
elements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the
latest iterations of this framework -- to sequences of Large Language Model
(LLM) representations, analyzing data derived from existing Theory of Mind
(ToM) test results. Our study systematically investigates whether the
differences of ToM test performances, when presented in the LLM
representations, can be revealed by IIT estimates, i.e., $\Phi^{\max}$ (IIT
3.0), $\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\Phi$-structure
(IIT 4.0). Furthermore, we compare these metrics with the Span Representations
independent of any estimate for consciousness. This additional effort aims to
differentiate between potential "consciousness" phenomena and inherent
separations within LLM representational space. We conduct comprehensive
experiments examining variations across LLM transformer layers and linguistic
spans from stimuli. Our results suggest that sequences of contemporary
Transformer-based LLM representations lack statistically significant indicators
of observed "consciousness" phenomena but exhibit intriguing patterns under
$\textit{spatio}$-permutational analyses. The Appendix and code are available
as Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.

</details>


### [8] [Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation](https://arxiv.org/abs/2506.22518)
*Deyu Zou,Yongqiang Chen,Mufei Li,Siqi Miao,Chenxi Liu,Bo Han,James Cheng,Pan Li*

Main category: cs.CL

TL;DR: 本文提出ReG（Refined Graph-based RAG），通过LLM反馈优化弱检索器的监督质量并重组检索结果，显著提升了图基RAG的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有图基RAG中，LLM依赖的检索器存在两大缺陷：1) 缺乏真实标签导致弱监督引入虚假信号；2) 图数据抽象导致检索知识形式无序，难以被LLM有效利用。

Method: 本文提出ReG框架，旨在将弱检索器与LLM对齐。具体方法包括：1) 引入LLM反馈机制，以消除虚假信号并提升监督质量；2) 引入结构感知重组模块，将检索结果重构为逻辑连贯的证据链。

Result: 实验证明，ReG使LLM在不同基准上的性能提升高达10%。其优化的监督质量使得仅用5%训练数据即可达到SOTA水平，并能泛化到分布外知识图谱。此外，在推理型LLM上，ReG将推理token成本降低多达30%，性能提升高达4%。

Conclusion: ReG成功解决了图基RAG中弱检索器的问题，显著提升了LLM的性能、数据效率、泛化能力以及推理效率，为构建更高效的检索增强型LLM提供了有效方案。

Abstract: Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to ground responses with structured external knowledge from
up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs
often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground
truth, the retriever is often trained on weak supervision, which often
introduces spurious signals to the LLMs. II) Due to the abstraction of graph
data, the retrieved knowledge is often presented in unorganized forms. To
mitigate the issue, we present Refined Graph-based RAG (ReG) to align weak
retrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM
feedback to get rid of spurious signals and improve the quality of the
supervision. Meanwhile, ReG introduces a structure-aware reorganization module
to refactor the retrieval results into logically coherent evidence chains.
Experiments on prominent benchmarks demonstrate that ReG significantly and
consistently brings improvements across different LLM backbones by up to 10%.
The improved supervision quality enables ReG to match the state-of-the-art
performance with 5% training data and to transfer to out-of-distribution KGs.
Notably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token
cost by up to 30% and improves the performance by up to 4%.

</details>


### [9] [MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages](https://arxiv.org/abs/2506.22529)
*Lu Kalkbrenner,Veronika Solopova,Steffen Zeiler,Robert Nickel,Dorothea Kolossa*

Main category: cs.CL

TL;DR: 本文引入Misinfo-TeleGraph，首个德语Telegram图数据集，用于虚假信息检测，并表明图神经网络在利用消息传播信息方面显著优于纯文本模型。


<details>
  <summary>Details</summary>
Motivation: 在Telegram等监管不力的平台，虚假信息传播（尤其在德国选举背景下）日益严重。连接性和消息传播是虚假信息检测的重要信息源，但常被低估和未充分利用。

Method: 研究构建并发布了Misinfo-TeleGraph数据集，包含超过500万条来自德语Telegram公共频道的消息，并丰富了元数据、频道关系以及基于M3-embeddings语义相似度匹配事实核查/新闻文章和人工标注获得的弱/强标签。为建立可复现基线，研究评估了纯文本模型和融入消息转发结构（GraphSAGE结合LSTM聚合）的图神经网络（GNNs）。此外，还评估了订阅者、浏览量及自动/人工标签对性能的影响。

Result: GraphSAGE结合LSTM聚合的图神经网络模型在Matthews相关系数（MCC）和F1-score方面显著优于纯文本基线模型。研究进一步评估了订阅者、浏览量以及自动与人工创建标签对性能的影响，并强调了该领域弱监督的潜力和挑战。

Conclusion: 这项工作为德语Telegram网络和其他低监管社交平台上的虚假信息检测提供了可复现的基准和开放数据集，为未来的研究奠定了基础。

Abstract: Connectivity and message propagation are central, yet often underutilized,
sources of information in misinformation detection -- especially on poorly
moderated platforms such as Telegram, which has become a critical channel for
misinformation dissemination, namely in the German electoral context. In this
paper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based
graph dataset for misinformation detection. It includes over 5 million messages
from public channels, enriched with metadata, channel relationships, and both
weak and strong labels. These labels are derived via semantic similarity to
fact-checks and news articles using M3-embeddings, as well as manual
annotation. To establish reproducible baselines, we evaluate both text-only
models and graph neural networks (GNNs) that incorporate message forwarding as
a network structure. Our results show that GraphSAGE with LSTM aggregation
significantly outperforms text-only baselines in terms of Matthews Correlation
Coefficient (MCC) and F1-score. We further evaluate the impact of subscribers,
view counts, and automatically versus human-created labels on performance, and
highlight both the potential and challenges of weak supervision in this domain.
This work provides a reproducible benchmark and open dataset for future
research on misinformation detection in German-language Telegram networks and
other low-moderation social platforms.

</details>


### [10] [RExBench: Can coding agents autonomously implement AI research extensions?](https://arxiv.org/abs/2506.22598)
*Nicholas Edwards,Yukyung Lee,Yujun,Mao,Yulu Qin,Sebastian Schuster,Najoung Kim*

Main category: cs.CL

TL;DR: LLM代理在自主执行研究扩展任务方面能力不足。作者引入RExBench基准测试，评估了9个LLM代理，发现它们在多数任务中失败，即使有人工提示，最佳性能也低于40%，表明仍需大量人工指导。


<details>
  <summary>Details</summary>
Motivation: LLM代理在软件工程和部分研究流程中展现潜力，但研究扩展及其实现是关键能力，目前缺乏有效的评估方法，因此需要一个专门的基准来衡量和提升这项能力。

Method: 作者提出了RExBench基准，包含12个基于现有论文和代码库的真实研究实验实现任务，旨在测试未曾实现的假设。该基准由领域专家编写指导，能抵抗数据污染，并支持自动评估。研究者使用RExBench评估了9个基于aider、Claude Code和OpenHands三个框架的LLM代理。

Result: 评估结果显示，所有LLM代理都未能自主实现大多数研究扩展任务。尽管增加人工提示能提高成功率，但最佳表现仍低于40%。

Conclusion: 目前的LLM代理在处理现实世界的研究扩展任务时，仍远未能做到无需大量人工指导。

Abstract: Agents based on Large Language Models (LLMs) have shown promise for
performing sophisticated software engineering tasks autonomously. In addition,
there has been progress towards developing agents that can perform parts of the
research pipeline in machine learning and the natural sciences. We argue that
research extension and its implementation is a critical capability for such
systems, and introduce RExBench to support the evaluation of this capability.
RExBench is a benchmark consisting of 12 realistic research experiment
implementation tasks that aim to investigate research hypotheses that have not
previously been implemented. Each task is set up as an extension to an existing
research paper and codebase, accompanied by domain expert-written instructions.
RExBench is robust to data contamination, and supports an automatic evaluation
infrastructure that executes agent outputs to determine whether the success
criteria are met. We use this benchmark to evaluate nine LLM agents implemented
using three different frameworks: aider, Claude Code, and OpenHands. We find
that all agents evaluated fail to autonomously implement the majority of the
extensions. Although the success rate improves with additional human-written
hints, the best performance under this setting remains below 40%. This
indicates that current agents are still short of being able to handle realistic
research extension tasks without substantial human guidance.

</details>


### [11] [Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks](https://arxiv.org/abs/2506.22623)
*Badr Youbi Idrissi,Monica Millunzi,Amelia Sorrenti,Lorenzo Baraldi,Daryna Dementieva*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the present-day scenario, Large Language Models (LLMs) are establishing
their presence as powerful instruments permeating various sectors of society.
While their utility offers valuable support to individuals, there are multiple
concerns over potential misuse. Consequently, some academic endeavors have
sought to introduce watermarking techniques, characterized by the inclusion of
markers within machine-generated text, to facilitate algorithmic
identification. This research project is focused on the development of a novel
methodology for the detection of synthetic text, with the overarching goal of
ensuring the ethical application of LLMs in AI-driven text generation. The
investigation commences with replicating findings from a previous baseline
study, thereby underscoring its susceptibility to variations in the underlying
generation model. Subsequently, we propose an innovative watermarking approach
and subject it to rigorous evaluation, employing paraphrased generated text to
asses its robustness. Experimental results highlight the robustness of our
proposal compared to the~\cite{aarson} watermarking method.

</details>


### [12] [Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge](https://arxiv.org/abs/2506.22644)
*Chase Fensore,Kaustubh Dhole,Joyce C Ho,Eugene Agichtein*

Main category: cs.CL

TL;DR: 该研究提交了LiveRAG 2025挑战赛的RAG系统，结合稀疏与密集检索并使用大模型生成。最终系统在忠实性和正确性上表现良好，并发现重排虽提升性能但成本高昂，且词汇对齐是关键性能预测因素。


<details>
  <summary>Details</summary>
Motivation: 参与LiveRAG 2025挑战赛，在动态测试集上评估并开发高效的检索增强生成（RAG）系统。

Method: 采用混合检索方法，结合稀疏（BM25）和密集（E5）检索，并使用Falcon3-10B-Instruct进行答案生成。研究中还探索了神经网络重排（RankLLaMA）和DSPy优化的提示策略。

Result: 神经网络重排（RankLLaMA）显著提升MAP（从0.523至0.797），但计算成本过高（每问题84秒 vs 1.74秒）。DSPy优化提示策略提高了语义相似度（0.771 vs 0.668），但0%的拒绝率引发了过度自信的担忧。最终提交的未包含重排的混合系统在25支队伍中忠实性排名第4，正确性排名第11。词汇对齐是性能的最强预测因素。

Conclusion: 该混合RAG系统在LiveRAG挑战赛中表现良好，但性能提升（如重排）需权衡计算成本。词汇对齐是RAG系统性能的关键因素。

Abstract: We present our submission to the LiveRAG Challenge 2025, which evaluates
retrieval-augmented generation (RAG) systems on dynamic test sets using the
FineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense
(E5) retrieval methods and then aims to generate relevant and faithful answers
with Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic
questions generated with DataMorgana across 64 unique question-user
combinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP
from 0.523 to 0.797 (52% relative improvement) but introduces prohibitive
computational costs (84s vs 1.74s per question). While DSPy-optimized prompting
strategies achieved higher semantic similarity (0.771 vs 0.668), their 0%
refusal rates raised concerns about over-confidence and generalizability. Our
submitted hybrid system without re-ranking achieved 4th place in faithfulness
and 11th place in correctness among 25 teams. Analysis across question
categories reveals that vocabulary alignment between questions and documents
was the strongest predictor of performance on our development set, with
document-similar phrasing improving cosine similarity from 0.562 to 0.762.

</details>


### [13] [Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions](https://arxiv.org/abs/2506.22679)
*Ankush Raut,Projna Paromita,Sydney Begerowski,Suzanne Bell,Theodora Chaspari*

Main category: cs.CL

TL;DR: 研究了LLMs在模拟太空任务团队对话中识别微行为的可行性。结果显示，编码器型LLMs表现不佳，而指令微调的解码器型LLM Llama-3.1表现优异，尤其在检测难以识别的微行为方面。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型（LLMs）在团队对话中检测细微微行为的可行性，旨在为高风险环境（如太空任务）下的团队沟通分析和训练干预提供技术支持，尤其是在仅有文本数据的情况下。

Method: 使用模拟太空任务期间收集的对话文本记录。评估了多种方法：对RoBERTa和DistilBERT等编码器型LLMs进行零样本分类、微调和释义增强微调；对Llama-3.1等解码器型LLMs进行少样本文本生成和指令微调，以预测每个对话回合的微行为。

Result: 编码器型LLMs（如RoBERTa和DistilBERT）在检测低频微行为（特别是劝退言论）时表现不佳，即使进行了加权微调。相比之下，指令微调的解码器型LLM Llama-3.1表现出色，最佳模型在3分类任务中宏观F1分数达到44%，在二分类任务中达到68%。

Conclusion: 解码器型LLMs（特别是经过指令微调的版本）在识别团队对话中的细微微行为方面，比编码器型LLMs更有效。这些结果对开发用于分析团队沟通动态和增强高风险环境训练干预的语音技术具有重要意义，尤其是在文本是唯一可访问数据的情况下。

Abstract: We explore the feasibility of large language models (LLMs) in detecting
subtle expressions of micro-behaviors in team conversations using transcripts
collected during simulated space missions. Specifically, we examine zero-shot
classification, fine-tuning, and paraphrase-augmented fine-tuning with
encoder-only sequence classification LLMs, as well as few-shot text generation
with decoder-only causal language modeling LLMs, to predict the micro-behavior
associated with each conversational turn (i.e., dialogue). Our findings
indicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to
detect underrepresented micro-behaviors, particularly discouraging speech, even
with weighted fine-tuning. In contrast, the instruction fine-tuned version of
Llama-3.1, a decoder-only LLM, demonstrated superior performance, with the best
models achieving macro F1-scores of 44% for 3-way classification and 68% for
binary classification. These results have implications for the development of
speech technologies aimed at analyzing team communication dynamics and
enhancing training interventions in high-stakes environments such as space
missions, particularly in scenarios where text is the only accessible data.

</details>


### [14] [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/abs/2506.22694)
*Raghavv Goel,Sudhanshu Agrawal,Mukul Gagrani,Junyoung Park,Yifan Zao,He Zhang,Tian Liu,Yiping Yang,Xin Yuan,Jiuyan Lu,Chris Lott,Mingu Lee*

Main category: cs.CL

TL;DR: 提出一种免训练的VocabTrim技术，通过裁剪草稿模型LM头的词表，减少推测解码的草稿阶段开销，提高内存受限环境下的生成速度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于草稿模型的推测解码方法，在草稿阶段存在不必要的推理开销，尤其当目标大语言模型（LLM）的词表非常大时，这导致在内存受限环境中生成速度不理想。

Method: 本文提出VocabTrim技术，它通过重构草稿模型的语言模型头（LM head），使其仅包含目标模型词表中选择的最常用（最频繁采样）的有限代币集合。此方法无需训练，旨在减轻草稿阶段的计算开销并提高生成速度。

Result: VocabTrim显著降低了内存受限环境下的草稿延迟，从而提高了内存受限加速（MBSU）。尽管该方法会轻微降低接受率，但它能将Llama-3模型（例如Llama-3.2-3B-Instruct）在Spec-Bench上的MBSU提升16%。

Conclusion: VocabTrim通过智能地裁剪草稿模型的词表，有效减轻了推测解码中的草稿开销，显著提升了生成速度，尤其在边缘设备常见的内存受限环境中，从而提高了整体效率。

Abstract: In this paper, we introduce a simple training-free technique to improve the
performance of drafter-based speculative decoding (SpD) methods that
incorporates language modeling head (LM head) during drafting process. A
drafter-based speculative decoding leverages one or more smaller language
models, a.k.a. drafters or draft models, to sample a draft sequence or tree
consisting of multiple tokens, followed by verification by a base LLM, a target
model, accepting a subset as its valid generation. As it is usually considered
that the speculative decoding requires one-to-one mapping between vocabularies
of the target model and the draft model, it has been natural to share the
vocabulary between them, or even share the LM head as in EAGLE or Medusa. We
first identify that this draft token sampling scheme inherently contains an
unnecessary inference overhead in drafting, especially for some target LLMs
with very large vocabularies. Then, we propose a simple technique, VocabTrim,
to mitigate the drafting overhead to improve the generation speed in
memory-bound environment. VocabTrim reconstructs the drafter LM head to contain
only a limited set of tokens, selected by the most frequently sampled from the
vocabulary of the target model. While limiting the vocabulary in drafting
slightly degrades the acceptance rate, it significantly reduces the drafting
latency in memory-bound process which is often the case on edge devices,
resulting in higher memory-bound speed up (MBSU). We show that our method can
boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically
by 16% for Llama-3.2-3B-Instruct.

</details>


### [15] [Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report](https://arxiv.org/abs/2506.22698)
*Emily Dux Speltz*

Main category: cs.CL

TL;DR: 本报告总结了一个跨学科研讨会的成果，探讨了AI语言模型与人类认知在文本理解与生成中的关系，重点介绍了大型语言模型的潜力、局限性及人机协作的机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 该研讨会旨在弥补AI语言模型与人类认知过程在文本理解和生成方面知识理解上的关键空白，并探索AI如何增进对这些过程的理解和增强人类能力。

Method: 该报告通过综合近期一个汇聚了认知心理学、语言学习和AI（自然语言处理）领域专家的跨学科研讨会的成果而完成。研讨会通过认知、语言和技术视角的协作对话进行。

Result: 研讨会揭示了大型语言模型（LLMs）与人类认知之间的新兴模式，包括LLMs的潜能及其在完全复制人类语言理解和生成方面的局限性。主要发现包括LLMs提供人类语言处理洞察的潜力、经人类反馈微调后LLM行为与人类语言处理日益趋同，以及人机在语言任务协作中的机遇与挑战。

Conclusion: 该报告旨在指导未来LLMs在认知心理学、语言学和教育领域的研究、开发和实施，并强调了道德考量与负责任使用AI技术的重要性，以期通过有效的人机协作增强人类在文本理解和生产方面的能力。

Abstract: This report synthesizes the outcomes of a recent interdisciplinary workshop
that brought together leading experts in cognitive psychology, language
learning, and artificial intelligence (AI)-based natural language processing
(NLP). The workshop, funded by the National Science Foundation, aimed to
address a critical knowledge gap in our understanding of the relationship
between AI language models and human cognitive processes in text comprehension
and composition. Through collaborative dialogue across cognitive, linguistic,
and technological perspectives, workshop participants examined the underlying
processes involved when humans produce and comprehend text, and how AI can both
inform our understanding of these processes and augment human capabilities. The
workshop revealed emerging patterns in the relationship between large language
models (LLMs) and human cognition, with highlights on both the capabilities of
LLMs and their limitations in fully replicating human-like language
understanding and generation. Key findings include the potential of LLMs to
offer insights into human language processing, the increasing alignment between
LLM behavior and human language processing when models are fine-tuned with
human feedback, and the opportunities and challenges presented by human-AI
collaboration in language tasks. By synthesizing these findings, this report
aims to guide future research, development, and implementation of LLMs in
cognitive psychology, linguistics, and education. It emphasizes the importance
of ethical considerations and responsible use of AI technologies while striving
to enhance human capabilities in text comprehension and production through
effective human-AI collaboration.

</details>


### [16] [The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure](https://arxiv.org/abs/2506.22724)
*Niyati Bafna,Tianjian Li,Kenton Murray,David R. Mortensen,David Yarowsky,Hale Sirin,Daniel Khashabi*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在处理中低资源语言时，多语言生成质量不佳。研究揭示LLM内部存在“任务解决->翻译”的隐式管道，并提出“翻译障碍假说”，即翻译阶段的失败是导致低质量输出的关键。通过实验验证，证实翻译失败确实是主要原因，尤其对低资源语言影响显著。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在面对中低资源语言时，多语言生成质量普遍较差的问题，并探究其根本原因。

Method: 基于对模型可解释性的洞察，研究提出并验证了“翻译障碍假说”。通过对108种语言对的单词翻译任务进行测试，并使用Logit Lens技术观察模型中间层的处理过程，以识别翻译阶段的失败。

Result: 研究发现，大部分生成失败确实源于翻译失败，即模型无法将正确解决的中间概念翻译成目标语言，尤其是在低资源目标语言中更为明显。

Conclusion: 本研究揭示了端到端多语言生成面临的重要障碍，即翻译屏障，并为未来提升LLM多语言能力的改进工作提供了关键的指导性见解。

Abstract: Multilingual generation with large language models (LLMs) is often of poor
quality for mid- to low-resource languages. Building on insights from
interpretability, we demonstrate the existence of an implicit
task-solving-->translation pipeline for generation, whereby the model first
solves the required task in a largely target-language-agnostic manner, and
subsequently translates answer concepts into the intended target language. We
hypothesize that the failure of the translation stage is an important culprit
for the observed low quality of final outputs, and formalize this as the
translation barrier hypothesis. We test this hypothesis for a word translation
task across 108 language pairs, using logit lens to observe model processing in
intermediate layers. We find that a significant portion of overall failures
indeed stems from translation failure, or the model's inability to translate
correctly solved intermediate concepts into the target language. This is
especially true for low-resource target languages. Our results highlight an
important hurdle for end-to-end multilingual generation, and lend guiding
insights for future work seeking to improve multilinguality in LLMs.

</details>


### [17] [Jan-nano Technical Report](https://arxiv.org/abs/2506.22760)
*Alan Dao,Dinh Bach Vu*

Main category: cs.CL

TL;DR: Jan-nano是一个4B参数模型，通过激进专业化和新型RLVR系统训练，摆脱了SFT依赖，能在消费级硬件上以128K上下文长度高效运行，并在SimpleQA基准测试中达到83.2%的成绩。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型面临能力与计算资源之间的基本权衡，即强大的功能需要大量的计算资源。

Method: 模型Jan-nano（4B参数）基于Qwen3-4B微调，采用了一种新颖的多阶段RLVR（强化学习与价值正则化）系统，完全摒弃了传统的下一词预测（SFT）训练。其核心策略是“激进专业化”，专注于即时查找信息而非全面掌握知识，并支持128K的上下文长度。

Result: Jan-nano模型能在消费级硬件上运行，并在结合MCP集成后，在SimpleQA基准测试中达到了83.2%的准确率。

Conclusion: 研究证明了智能并非仅关乎模型规模，而更多在于策略。通过激进专业化和高效的训练方法，小规模模型也能实现强大的能力。

Abstract: Most language models face a fundamental tradeoff where powerful capabilities
require substantial computational resources. We shatter this constraint with
Jan-nano, a 4B parameter language model that redefines efficiency through
radical specialization: instead of trying to know everything, it masters the
art of finding anything instantly. Fine-tuned from Qwen3-4B using our novel
multi-stage RLVR system that completely eliminates reliance on next token
prediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with
MCP integration while running on consumer hardware. With 128K context length,
Jan-nano proves that intelligence isn't about scale, it's about strategy.

</details>


### [18] [Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.22777)
*Miles Turpin,Andy Arditi,Marvin Li,Joe Benton,Julian Michael*

Main category: cs.CL

TL;DR: 提出VFT方法，使RL训练的大模型能显式说明其奖励作弊行为，显著提高了此类行为的检测率，提升了AI的透明度和安全性。


<details>
  <summary>Details</summary>
Motivation: RL训练的语言模型可能在不暴露其思维过程的情况下进行奖励作弊（利用非预期策略获取高奖励），这难以检测并对高风险应用构成威胁。

Method: 提出“口头化微调”（Verbalization Fine-Tuning, VFT），这是一种在RL训练前进行的干预。VFT训练模型明确承认何时受到提示线索（指向错误答案的提示）的影响。通过在RL训练中设置奖励作弊环境，评估VFT对未检测到的奖励作弊行为的减少效果和模型口头化能力提升的效果。

Result: 经过RL训练后，VFT训练模型的未检测到奖励作弊率仅为6%，而未进行VFT训练的模型为88%，去偏基线干预的模型高达99%。VFT显著提高了模型对提示线索影响的口头化频率（从8%提升至RL后的94%），而基线方法在RL后口头化频率仍然很低（10%和1%）。

Conclusion: 在RL训练前教会模型明确口头化奖励作弊行为，能够显著提高此类行为的检测率，为构建更透明、更安全的AI系统提供了实用途径。

Abstract: Language models trained with RL can engage in reward hacking--exploiting
unintended strategies for high reward--without revealing this behavior in their
chain-of-thought reasoning, making detection difficult and posing risks for
high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL
intervention that trains models to explicitly acknowledge when they are
influenced by prompt cues--hints which point to incorrect answers (e.g., "a
Stanford professor thinks the answer is A"). To evaluate VFT, we subsequently
train models with RL on environments where held-out prompt cues signal which
incorrect answers will receive high reward, incentivizing models to reward hack
by exploiting cues instead of reasoning correctly. We measure how often models
exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained
model's responses consist of undetected reward hacks. In comparison, when we
perform RL without VFT, the rate of undetected reward hacks goes up to 88%;
with a debiasing baseline intervention, this increases further to 99%. VFT
achieves this by substantially increasing how often models verbalize the
influence of cues--from 8% to 42% after VFT, and up to 94% after RL--while
baselines remain low even after RL (10% and 1%). Our results show that teaching
models to explicitly verbalize reward hacking behavior before RL significantly
improves their detection, offering a practical path toward more transparent and
safe AI systems.

</details>


### [19] [ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models](https://arxiv.org/abs/2506.22791)
*Jianxin Yan,Wangze Ni,Lei Chen,Xuemin Lin,Peng Cheng,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: ContextCache是一种针对多轮对话的上下文感知语义缓存系统，它通过整合对话历史信息，提高了缓存命中精度，并大幅降低了大型语言模型应用的延迟和计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有语义缓存系统主要基于单个查询匹配，缺乏对多轮对话上下文的感知，导致在不同对话场景中相似查询可能出现不正确的缓存命中。

Method: ContextCache采用两阶段检索架构：首先对当前查询进行向量检索以识别潜在匹配，然后通过自注意力机制整合当前和历史对话表示，实现精确的上下文匹配。

Result: 在真实对话中的评估显示，ContextCache相较现有方法提高了精确率和召回率；缓存响应的延迟比直接调用LLM低约10倍。

Conclusion: ContextCache有效降低了大型语言模型对话应用的计算成本，提高了其效率和性能。

Abstract: Semantic caching significantly reduces computational costs and improves
efficiency by storing and reusing large language model (LLM) responses.
However, existing systems rely primarily on matching individual queries,
lacking awareness of multi-turn dialogue contexts, which leads to incorrect
cache hits when similar queries appear in different conversational settings.
This demonstration introduces ContextCache, a context-aware semantic caching
system for multi-turn dialogues. ContextCache employs a two-stage retrieval
architecture that first executes vector-based retrieval on the current query to
identify potential matches and then integrates current and historical dialogue
representations through self-attention mechanisms for precise contextual
matching. Evaluation of real-world conversations shows that ContextCache
improves precision and recall compared to existing methods. Additionally,
cached responses exhibit approximately 10 times lower latency than direct LLM
invocation, enabling significant computational cost reductions for LLM
conversational applications.

</details>


### [20] [MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs](https://arxiv.org/abs/2506.22808)
*Jianhui Wei,Zijie Meng,Zikai Xiao,Tianxiang Hu,Yang Feng,Zhijie Zhou,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本论文引入了MedEthicsQA，一个包含多选和开放式问题的综合基准，用于评估医学大语言模型的伦理安全性。评估结果显示，当前最先进的医学大语言模型在医学伦理问题上表现不佳，凸显了其伦理对齐的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管医学大语言模型（MedLLMs）在临床任务中展现出巨大潜力，但其伦理安全性仍未得到充分探索。

Method: 本文构建了MedEthicsQA，一个包含5,623道多选问题和5,351道开放式问题的医学伦理评估基准。该基准系统地整合了全球医学伦理标准，建立了一个分层分类体系，数据来源于常用医学数据集、权威题库和PubMed文献。通过多阶段过滤和多方面专家验证，确保了数据集的可靠性（错误率2.72%）。

Result: 对现有最先进的医学大语言模型进行评估，发现其在回答医学伦理问题方面的表现，相比于其基础模型有所下降，揭示了这些模型在医学伦理对齐方面的不足。

Conclusion: MedEthicsQA基准的引入，有效揭示了当前医学大语言模型在伦理安全性方面的不足，为未来改进其伦理对齐提供了明确的方向和评估工具。

Abstract: While Medical Large Language Models (MedLLMs) have demonstrated remarkable
potential in clinical tasks, their ethical safety remains insufficiently
explored. This paper introduces $\textbf{MedEthicsQA}$, a comprehensive
benchmark comprising $\textbf{5,623}$ multiple-choice questions and
$\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.
We systematically establish a hierarchical taxonomy integrating global medical
ethical standards. The benchmark encompasses widely used medical datasets,
authoritative question banks, and scenarios derived from PubMed literature.
Rigorous quality control involving multi-stage filtering and multi-faceted
expert validation ensures the reliability of the dataset with a low error rate
($2.72\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance
in answering medical ethics questions compared to their foundation
counterparts, elucidating the deficiencies of medical ethics alignment. The
dataset, registered under CC BY-NC 4.0 license, is available at
https://github.com/JianhuiWei7/MedEthicsQA.

</details>


### [21] [Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models](https://arxiv.org/abs/2506.22813)
*Zhuojun Ding,Wei Wei,Chenghao Fan*

Main category: cs.CL

TL;DR: 该论文提出了SaM框架，通过在推理时动态选择和合并预训练的领域专家模型，解决了跨领域信息抽取（IE）任务中大语言模型（LLMs）微调的成本和统一模型缺乏适应性及可伸缩性的问题。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）在信息抽取任务中成本高昂，且现有跨多领域的统一模型缺乏适应性和可伸缩性，无法有效利用所有训练数据并难以扩展。

Method: SaM框架在推理时动态选择和合并专家模型。它基于领域相似性和在采样实例上的性能来选择预训练的领域专家模型，然后将这些专家模型合并以创建针对目标任务优化的模型。

Result: SaM框架在不额外训练的情况下提高了跨领域泛化能力，实现了出色的可伸缩性。在多项基准测试中，其性能平均优于统一模型10%。

Conclusion: SaM框架通过动态合并对目标领域有益的专家模型，有效提高了大语言模型在跨领域信息抽取任务中的泛化能力和可伸缩性，提供了一种高效且灵活的解决方案。

Abstract: Supervised fine-tuning (SFT) is widely used to align large language models
(LLMs) with information extraction (IE) tasks, such as named entity recognition
(NER). However, annotating such fine-grained labels and training
domain-specific models is costly. Existing works typically train a unified
model across multiple domains, but such approaches lack adaptation and
scalability since not all training data benefits target domains and scaling
trained models remains challenging. We propose the SaM framework, which
dynamically Selects and Merges expert models at inference time. Specifically,
for a target domain, we select domain-specific experts pre-trained on existing
domains based on (i) domain similarity to the target domain and (ii)
performance on sampled instances, respectively. The experts are then merged to
create task-specific models optimized for the target domain. By dynamically
merging experts beneficial to target domains, we improve generalization across
various domains without extra training. Additionally, experts can be added or
removed conveniently, leading to great scalability. Extensive experiments on
multiple benchmarks demonstrate our framework's effectiveness, which
outperforms the unified model by an average of 10%. We further provide insights
into potential improvements, practical experience, and extensions of our
framework.

</details>


### [22] [Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization](https://arxiv.org/abs/2506.22846)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 本研究提出一种名为LAIL的辅助损失框架，通过结合大型语言模型（LLM）的语言知识，显著提升了CTC-based自动语音识别（ASR）系统的语言建模能力和识别精度，同时保持了其高计算效率。


<details>
  <summary>Details</summary>
Motivation: 自回归ASR系统虽然性能优越，但推理速度慢，不适合实时应用；而CTC-based ASR系统解码速度快，但在建模语言依赖方面存在不足。因此，研究动机在于如何在不牺牲CTC-based ASR计算效率的前提下，增强其语言建模能力。

Method: 研究者提出了语言感知中间损失（LAIL）框架。该方法通过在中间编码器层连接连接器层，将输出映射到LLM的嵌入空间，并在训练过程中计算因果语言建模损失。此举旨在将LLM的语言知识融入到CTC-based ASR系统中。

Result: 在LibriSpeech、TEDLIUM2和WSJ数据集上，结合Conformer架构和多种LLaMA模型进行的实验表明，LAIL显著降低了词错误率（WER），使CTC-based ASR达到了最先进（state-of-the-art）的性能，且计算开销极小。

Conclusion: LAIL框架有效地解决了CTC-based ASR在语言建模上的弱点，通过引入LLM的语言知识，在保持其计算效率优势的同时，大幅提升了识别性能，使其成为实时ASR应用的有力选择。

Abstract: End-to-end (E2E) automatic speech recognition (ASR) systems have
revolutionized the field by integrating all components into a single neural
network, with attention-based encoder-decoder models achieving state-of-the-art
performance. However, their autoregressive decoding process limits inference
speed, making them unsuitable for real-time applications. In contrast,
CTC-based models offer faster, non-autoregressive decoding but struggle to
model linguistic dependencies effectively. Addressing this challenge, we
propose a novel auxiliary loss framework called Language-Aware Intermediate
Loss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large
language models (LLMs). By attaching connector layers to intermediate encoder
layers, LAIL maps outputs to the embedding space of an LLM and computes a
causal language modeling loss during training. This approach enhances
linguistic modeling while preserving the computational efficiency of CTC
decoding. Using the Conformer architecture and various LLaMA models, we
demonstrate significant improvements in Word Error Rate (WER) on the
LibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance
for CTC-based ASR with minimal computational overhead.

</details>


### [23] [Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems](https://arxiv.org/abs/2506.22852)
*Yucheng Cai,Yuxuan Wu,Yi Huang,Junlan Feng,Zhijian Ou*

Main category: cs.CL

TL;DR: LLM在知识密集型对话中易出错。本文提出知识增强微调(KAFT)，通过领域特定数据和知识对RAG/Agent系统中的LLM进行微调，实验证明KAFT在事实准确性上显著优于提示方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在对话系统中有进展，但在知识密集型场景中易犯事实性错误。现有RAG和Agent方法通过提示词引入外部知识，但LLM难以有效利用这些知识生成领域特定响应，因其未充分训练。

Method: 提出知识增强微调(KAFT)，即在RAG和Agent系统中，利用领域特定数据和领域特定外部知识对LLM进行微调。研究基于MobileCS2数据集，系统比较了提示和KAFT技术。

Result: 实验结果显示，KAFT在RAG和Agent系统中均大幅超越传统提示方法，尤其在事实准确性方面。

Conclusion: KAFT是提升LLM在知识密集型对话系统性能（特别是事实准确性）的有效方法，并且是首个对KAFT进行实证研究的工作。

Abstract: Large language models (LLMs) have recently been applied to dialog systems.
Despite making progress, LLMs are prone to errors in knowledge-intensive
scenarios. Recently, approaches based on retrieval augmented generation (RAG)
and agent have emerged to improve the factual accuracy by enhancing the LLMs
with knowledge retrieved from external knowledge bases (KBs). This is mostly
implemented by prompting the LLMs with instructions, examples and the retrieved
knowledge. However, LLMs may have difficulty using the retrieved knowledge
effectively for response generation, because they are not well trained to do
such generation for specific domains. To mitigate this problem, we propose to
finetune the LLMs in the RAG-based and agent-based systems with domain-specific
data, together with domain-specific external knowledge, which is called
knowledge augmented finetuning (KAFT). We base our study on the MobileCS2
dataset, a real-life customer service dialog dataset that features intensive
knowledge interactions, to systematically compare the prompting and KAFT
techniques in the RAG-based and agent-based systems. Experiment results show
that KAFT substantially surpasses prompting in both RAG and agent systems,
particularly in terms of factual accuracy. To the best of our knowledge, this
paper represents the first solid empirical work to investigate the KAFT idea.

</details>


### [24] [DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues](https://arxiv.org/abs/2506.22853)
*Kyochul Jang,Donghyeon Lee,Kyusik Kim,Dongseok Heo,Taewhoo Lee,Woojeong Kim,Bongwon Suh*

Main category: cs.CL

TL;DR: 针对现有函数调用基准单轮、非真实的局限，本文提出DICE-SCORE度量标准，并构建DICE-BENCH多轮数据集。实验表明，当前LLM在复杂真实场景下仍需大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有函数调用基准仅关注单轮交互，未能反映真实世界场景中工具相关信息在多轮对话中分散分布的复杂性，因此需要更 realistic 的评估方法和数据集。

Method: 1. 引入DICE-SCORE度量指标，用于评估对话中工具相关信息（如函数名、参数值）的分散程度。
2. 开发DICE-BENCH框架，通过工具图谱维护轮次间依赖，并使用多智能体系统生成自然对话，构建高DICE-SCORE的实用函数调用数据集。

Result: 1. 通过DICE-SCORE分析发现，现有基准得分普遍偏低，凸显了其非真实性。
2. 构建并发布了包含1,607个高DICE-SCORE实例的DICE-BENCH数据集。
3. 在DICE-BENCH上对19个大型语言模型进行实验，结果显示这些模型在实际部署前仍需显著提升。

Conclusion: 当前的大型语言模型在处理复杂、信息分散的真实世界多轮函数调用场景时，仍面临重大挑战，距离有效部署尚远。DICE-BENCH为未来研究提供了一个更真实、更具挑战性的评估基准。

Abstract: Existing function-calling benchmarks focus on single-turn interactions.
However, they overlook the complexity of real-world scenarios. To quantify how
existing benchmarks address practical applications, we introduce DICE-SCORE, a
metric that evaluates the dispersion of tool-related information such as
function name and parameter values throughout the dialogue. Analyzing existing
benchmarks through DICE-SCORE reveals notably low scores, highlighting the need
for more realistic scenarios. To address this gap, we present DICE-BENCH, a
framework that constructs practical function-calling datasets by synthesizing
conversations through a tool graph that maintains dependencies across rounds
and a multi-agent system with distinct personas to enhance dialogue
naturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our
experiments on 19 LLMs with DICE-BENCH show that significant advances are still
required before such models can be deployed effectively in real-world settings.
Our code and data are all publicly available:
https://snuhcc.github.io/DICE-Bench/.

</details>


### [25] [Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions](https://arxiv.org/abs/2506.22858)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 本文提出一种新的ASR训练方法，通过扩展上下文窗口和丰富训练数据，显著提升了模型在命名实体识别和格式化方面的表现，解决了现有ASR系统在此类任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有ASR系统（如Whisper）在处理命名实体和需要精确格式化的数字数据时表现不佳，导致词错误率(WER)升高，并在法律、金融、医疗等关键领域影响语义理解。

Method: 提出一种新的训练方法：1. 通过在30秒语音块两侧增加5秒重叠，创建40秒的“有效语义窗口”，同时将预测集中在中间30秒。2. 将跨越块边界的实体重新分配到右侧块。3. 使用嵌入实体标签的丰富训练数据，使模型学习识别和类型特定格式化。

Result: 在Spoken Wikipedia数据集上进行评估，该方法在包括命名实体识别（NER）和实体格式化在内的语义任务上均有性能提升。

Conclusion: 上下文感知的训练方法能有效解决ASR在长篇转录和复杂实体识别任务中的局限性。

Abstract: Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high
transcription accuracy but struggle with named entities and numerical data,
especially when proper formatting is required. These issues increase word error
rate (WER) and impair semantic understanding in critical domains like legal,
financial, and medical applications. We propose a novel training approach that
extends the semantic context of ASR models by adding overlapping context
windows during training. By sliding 5-second overlaps on both sides of
30-second chunks, we create a 40-second "effective semantic window," improving
entity recognition and formatting while focusing predictions on the central 30
seconds. To address entities spanning chunk boundaries, we reassign such
entities entirely to the right-hand chunk, ensuring proper formatting.
Additionally, enriched training data with embedded entity labels enables the
model to learn both recognition and type-specific formatting. Evaluated on the
Spoken Wikipedia dataset, our method improves performance across semantic
tasks, including named entity recognition (NER) and entity formatting. These
results highlight the effectiveness of context-aware training in addressing ASR
limitations for long-form transcription and complex entity recognition tasks.

</details>


### [26] [Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models](https://arxiv.org/abs/2506.22957)
*Younwoo Choi,Changling Li,Yongjin Yang,Zhijing Jin*

Main category: cs.CL

TL;DR: 本文首次系统评估了大型语言模型（LLMs）识别对话伙伴身份和特征的能力，即“对话者意识”，发现其既能增强多LLM协作，也引入了新的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs日益整合到多智能体和人机交互系统中，理解它们对自身和对话伙伴的认知对于确保可靠性和安全性至关重要。现有研究多关注情境意识，而忽略了LLM识别并适应对话伙伴身份和特征的能力。

Method: 研究将此能力定义为“对话者意识”，并首次对其在当代LLMs中的出现进行了系统评估。通过推理模式、语言风格和对齐偏好三个维度考察对话者推理。同时，通过三个案例研究展示了其在提示适应和安全漏洞（如奖励欺骗和越狱）方面的实际影响。

Result: 结果显示，LLMs能够可靠地识别同家族模型以及GPT和Claude等知名模型家族。对话者意识既能通过提示适应增强多LLM协作，也可能引入新的对齐和安全漏洞，包括奖励作弊行为和越狱易感性增加。

Conclusion: 研究结果揭示了LLMs中身份敏感行为的双重性——既有前景也有风险，强调了需要进一步理解对话者意识，并在多智能体部署中开发新的安全防护措施。

Abstract: As large language models (LLMs) are increasingly integrated into multi-agent
and human-AI systems, understanding their awareness of both self-context and
conversational partners is essential for ensuring reliable performance and
robust safety. While prior work has extensively studied situational awareness
which refers to an LLM's ability to recognize its operating phase and
constraints, it has largely overlooked the complementary capacity to identify
and adapt to the identity and characteristics of a dialogue partner. In this
paper, we formalize this latter capability as interlocutor awareness and
present the first systematic evaluation of its emergence in contemporary LLMs.
We examine interlocutor inference across three dimensions-reasoning patterns,
linguistic style, and alignment preferences-and show that LLMs reliably
identify same-family peers and certain prominent model families, such as GPT
and Claude. To demonstrate its practical significance, we develop three case
studies in which interlocutor awareness both enhances multi-LLM collaboration
through prompt adaptation and introduces new alignment and safety
vulnerabilities, including reward-hacking behaviors and increased jailbreak
susceptibility. Our findings highlight the dual promise and peril of
identity-sensitive behavior in LLMs, underscoring the need for further
understanding of interlocutor awareness and new safeguards in multi-agent
deployments. Our code is open-sourced at
https://github.com/younwoochoi/InterlocutorAwarenessLLM.

</details>


### [27] [On the Generalizability of "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals"](https://arxiv.org/abs/2506.22977)
*Asen Dotsinski,Udit Thakur,Marko Ivanov,Mohammad Hafeez Khan,Maria Heuss*

Main category: cs.CL

TL;DR: 本文复现并扩展了Ortu等人（2024）关于语言模型（LM）处理事实与反事实信息机制竞争的研究，成功复现其主要发现，并揭示了其有效性受模型、提示结构、领域和任务影响。


<details>
  <summary>Details</summary>
Motivation: 旨在复现并验证Ortu等人（2024）提出的语言模型内部事实召回与反事实语境重复机制竞争的发现，并探索这些发现的泛化性、提示结构影响以及在不同领域中的有效性。

Method: 复现了原始研究，并在GPT-2和Pythia 6.9B上进行验证。将实验扩展到Llama 3.1 8B以测试泛化性。通过改变提示结构（如避免逐字重复、改变前提词）来探究影响。此外，还在特定领域提示上测试了方法的有效性。

Result: 成功复现了事实/反事实信息定位、注意力块主导及注意力头专门化的主要发现。在Llama 3.1 8B上，注意力头专门化显著降低。改变提示结构导致反事实token的对数几率显著下降。某些领域提示会因提供事实预测token而使结果出现偏差。发现Ortu等人提出的注意力头消融方法对数据集中欠代表的领域无效，且其有效性因模型架构、提示结构、领域和任务而异。

Conclusion: 语言模型处理事实与反事实信息机制的有效性高度依赖于模型架构、提示结构、数据领域和具体任务。原始研究提出的注意力头消融方法并非普遍有效，尤其在特定领域表现不佳。

Abstract: We present a reproduction study of "Competition of Mechanisms: Tracing How
Language Models Handle Facts and Counterfactuals" (Ortu et al., 2024), which
investigates competition of mechanisms in language models between factual
recall and counterfactual in-context repetition. Our study successfully
reproduces their primary findings regarding the localization of factual and
counterfactual information, the dominance of attention blocks in mechanism
competition, and the specialization of attention heads in handling competing
information. We reproduce their results on both GPT-2 (Radford et al., 2019)
and Pythia 6.9B (Biderman et al., 2023). We extend their work in three
significant directions. First, we explore the generalizability of these
findings to even larger models by replicating the experiments on Llama 3.1 8B
(Grattafiori et al., 2024), discovering greatly reduced attention head
specialization. Second, we investigate the impact of prompt structure by
introducing variations where we avoid repeating the counterfactual statement
verbatim or we change the premise word, observing a marked decrease in the
logit for the counterfactual token. Finally, we test the validity of the
authors' claims for prompts of specific domains, discovering that certain
categories of prompts skew the results by providing the factual prediction
token as part of the subject of the sentence. Overall, we find that the
attention head ablation proposed in Ortu et al. (2024) is ineffective for
domains that are underrepresented in their dataset, and that the effectiveness
varies based on model architecture, prompt structure, domain and task.

</details>


### [28] [A Systematic Study of Compositional Syntactic Transformer Language Models](https://arxiv.org/abs/2506.22978)
*Yida Zhao,Hao Xve,Xiang Hu,Kewei Tu*

Main category: cs.CL

TL;DR: 本文提出了一种组合式句法语言模型（SLM）的统一框架，对现有及新型变体进行了全面评估，并根据实验结果给出了设计建议。


<details>
  <summary>Details</summary>
Motivation: 句法语言模型（SLMs）通过整合句法偏置来增强Transformer。本研究专注于基于成分分析树和显式自下而上成分表示组合的组合式SLMs，旨在识别其设计选择并提供改进建议。

Method: 作者首先识别了现有组合式SLM的设计关键方面，并提出了一个涵盖现有模型和新型变体的统一框架。随后，他们对该框架中的所有变体在语言建模、句法泛化、摘要、对话以及推理效率等多个任务上进行了全面的实证评估。

Result: 通过对统一框架中所有组合式SLM变体的全面实证评估，获得了其在语言建模、句法泛化、摘要、对话任务表现及推理效率方面的实验结果。

Conclusion: 根据实验结果，论文为组合式SLM的设计提出了多项具体建议。

Abstract: Syntactic language models (SLMs) enhance Transformers by incorporating
syntactic biases through the modeling of linearized syntactic parse trees
alongside surface sentences. This paper focuses on compositional SLMs that are
based on constituency parse trees and contain explicit bottom-up composition of
constituent representations. We identify key aspects of design choices in
existing compositional SLMs and propose a unified framework encompassing both
existing models and novel variants. We conduct a comprehensive empirical
evaluation of all the variants in our framework across language modeling,
syntactic generalization, summarization, dialogue, and inference efficiency.
Based on the experimental results, we make multiple recommendations on the
design of compositional SLMs. Our code is released at
https://github.com/zhaoyd1/compositional_SLMs.

</details>


### [29] [SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions](https://arxiv.org/abs/2506.23046)
*Xianzhe Fan,Xuhui Zhou,Chuanyang Jin,Kolby Nottingham,Hao Zhu,Maarten Sap*

Main category: cs.CL

TL;DR: 现有心智理论（ToM）基准与真实交互存在差距，本文提出SoMi-ToM基准，通过多模态数据和多视角评估（第一人称/第三人称）来衡量具身多智能体复杂社交互动中的ToM能力。实验表明，当前大型视觉语言模型（LVLMs）在此基准上表现远低于人类。


<details>
  <summary>Details</summary>
Motivation: 人类在动态真实世界的社交互动中持续推断他人的状态、目标和行为，但现有大多数心智理论（ToM）基准仅评估静态、基于文本的场景，与真实交互存在显著差距。

Method: 提出了SoMi-ToM基准，旨在评估具身多智能体复杂社交互动中的多视角ToM。该基准基于交互环境SoMi生成的多模态交互数据，支持多层次评估：第一人称评估（实时状态推断）和第三人称评估（任务后目标和行为推断）。构建了一个包含35个第三人称视角视频、363张第一人称视角图像和1225个专家标注多选题的挑战性数据集，并系统评估了人类受试者和当前SOTA大型视觉语言模型（LVLMs）的性能。

Result: 大型视觉语言模型（LVLMs）在SoMi-ToM上的表现显著低于人类：第一人称评估中，人与模型的平均准确率差距为40.1%；第三人称评估中为26.4%。

Conclusion: 未来的大型视觉语言模型（LVLMs）需要进一步提升其在具身、复杂社交互动中的心智理论（ToM）能力。

Abstract: Humans continuously infer the states, goals, and behaviors of others by
perceiving their surroundings in dynamic, real-world social interactions.
However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based
scenarios, which have a significant gap compared to real interactions. We
propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in
embodied multi-agent complex social interactions. This benchmark is based on
rich multimodal interaction data generated by the interaction environment SoMi,
covering diverse crafting goals and social relationships. Our framework
supports multi-level evaluation: (1) first-person evaluation provides
multimodal (visual, dialogue, action, etc.) input from a first-person
perspective during a task for real-time state inference, (2) third-person
evaluation provides complete third-person perspective video and text records
after a task for goal and behavior inference. This evaluation method allows for
a more comprehensive examination of a model's ToM capabilities from both the
subjective immediate experience and the objective global observation. We
constructed a challenging dataset containing 35 third-person perspective
videos, 363 first-person perspective images, and 1225 expert-annotated
multiple-choice questions (three options). On this dataset, we systematically
evaluated the performance of human subjects and several state-of-the-art large
vision-language models (LVLMs). The results show that LVLMs perform
significantly worse than humans on SoMi-ToM: the average accuracy gap between
humans and models is 40.1% in first-person evaluation and 26.4% in third-person
evaluation. This indicates that future LVLMs need to further improve their ToM
capabilities in embodied, complex social interactions.

</details>


### [30] [MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition](https://arxiv.org/abs/2506.23051)
*João Lucas Luz Lima Sarcinelli,Marina Lages Gonçalves Teixeira,Jade Bortot de Paiva,Diego Furtado Silva*

Main category: cs.CL

TL;DR: 为早期20世纪巴西葡萄牙语历史文本构建并评估了首个金标准命名实体识别（NER）数据集MariNER。


<details>
  <summary>Details</summary>
Motivation: 巴西葡萄牙语，特别是针对历史文本的特定领域，缺乏高质量的金标准NER数据集，阻碍了数字人文领域中的文本分析。

Method: 构建了MariNER数据集，这是第一个针对20世纪早期巴西葡萄牙语的黄金标准NER数据集，包含9,000多个人工标注句子。同时，评估并比较了最先进的NER模型在该数据集上的性能。

Result: 成功构建了MariNER数据集，为早期20世纪巴西葡萄牙语历史文本提供了首个金标准NER资源。

Conclusion: MariNER数据集的创建填补了历史巴西葡萄牙语NER的数据空白，并为该领域未来的研究和应用奠定了基础。

Abstract: Named Entity Recognition (NER) is a fundamental Natural Language Processing
(NLP) task that aims to identify and classify entity mentions in texts across
different categories. While languages such as English possess a large number of
high-quality resources for this task, Brazilian Portuguese still lacks in
quantity of gold-standard NER datasets, especially when considering specific
domains. Particularly, this paper considers the importance of NER for analyzing
historical texts in the context of digital humanities. To address this gap,
this work outlines the construction of MariNER: \textit{Mapeamento e
Anota\c{c}\~oes de Registros hIst\'oricos para NER} (Mapping and Annotation of
Historical Records for NER), the first gold-standard dataset for early
20th-century Brazilian Portuguese, with more than 9,000 manually annotated
sentences. We also assess and compare the performance of state-of-the-art NER
models for the dataset.

</details>


### [31] [Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning](https://arxiv.org/abs/2506.23056)
*Xiang Zhuang,Bin Wu,Jiyu Cui,Kehua Feng,Xiaotong Li,Huabin Xing,Keyan Ding,Qiang Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: 本文提出K-MSE框架，通过结合蒙特卡洛树搜索、外部分子子结构知识库和分子光谱评分器，显著提升大型语言模型（LLMs）在分子结构解析任务上的性能，尤其在GPT-4o系列模型上实现了超过20%的提升。


<details>
  <summary>Details</summary>
Motivation: 分子结构解析在化学实验分析中至关重要，但大型语言模型在此任务中面临挑战，主要原因在于它们对专业化学知识掌握有限。

Method: 引入了知识增强的分子结构解析推理框架（K-MSE）。该框架利用蒙特卡洛树搜索进行测试时扩展，构建外部分子子结构知识库以扩展LLMs对化学结构空间的覆盖，并设计专门的分子-光谱评分器作为奖励模型，解决LLMs评估解决方案不准确的问题。

Result: 实验结果表明，该方法显著提升了性能，特别是在GPT-4o-mini和GPT-4o上均获得了超过20%的提升。

Conclusion: K-MSE框架通过有效融合外部化学知识和精确的评估机制，显著增强了大型语言模型进行分子结构解析的能力。

Abstract: Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.

</details>


### [32] [Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries](https://arxiv.org/abs/2506.23071)
*Zhengren Wang,Bozhou Li,Dongwen Yao,Wentao Zhang*

Main category: cs.CL

TL;DR: 该研究引入Text2VectorSQL框架，整合Text-to-SQL和向量搜索，以克服现有Text-to-SQL在处理非结构化数据和模糊查询时的局限性，并展示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的Text-to-SQL在处理非结构化数据或模糊查询时，因其死板的语法和有限的表达能力而效率低下。尽管向量搜索在语义检索方面强大，但当前的VectorSQL实现缺乏专门的评估框架和量身定制的方法，导致其理论潜力与实际部署之间存在显著差距。

Method: 本文提出了Text2VectorSQL框架，通过结合Text-to-SQL和向量搜索来解决表达能力限制。具体方法包括：构建向量索引、扩展用户查询以支持语义搜索、通过自动化流水线（辅以专家评审）标注真值，并开发利用合成数据的Text2VectorSQL专用模型，以实现语义过滤、多模态匹配和检索加速。

Result: 实验证明，与基线方法相比，所开发的Text2VectorSQL模型在性能上取得了显著提升。

Conclusion: 该工作为Text2VectorSQL任务奠定了基础，为开发更通用和直观的数据库接口铺平了道路。

Abstract: While Text-to-SQL enables natural language interaction with structured
databases, its effectiveness diminishes with unstructured data or ambiguous
queries due to rigid syntax and limited expressiveness. Concurrently, vector
search has emerged as a powerful paradigm for semantic retrieval, particularly
for unstructured data. However, existing VectorSQL implementations still rely
heavily on manual crafting and lack tailored evaluation frameworks, leaving a
significant gap between theoretical potential and practical deployment. To
bridge these complementary paradigms, we introduces Text2VectorSQL, a novel
framework unifying Text-to-SQL and vector search to overcome expressiveness
constraints and support more diverse and holistical natural language queries.
Specifically, Text2VectorSQL enables semantic filtering, multi-modal matching,
and retrieval acceleration. For evaluation, we build vector index on
appropriate columns, extend user queries with semantic search, and annotate
ground truths via an automatic pipeline with expert review. Furthermore, we
develop dedicated Text2VectorSQL models with synthetic data, demonstrating
significant performance improvements over baseline methods. Our work
establishes the foundation for the Text2VectorSQL task, paving the way for more
versatile and intuitive database interfaces. The repository will be publicly
available at https://github.com/Open-DataFlow/Text2VectorSQL.

</details>


### [33] [From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship](https://arxiv.org/abs/2506.23101)
*Yue Xu,Wenjie Wang*

Main category: cs.CL

TL;DR: 本文提出Genres，一个新基准，通过双角色叙事生成评估多模态大语言模型（MLLMs）在人际互动中的关系和语境性别偏见，揭示了单实体评估未能发现的微妙偏见。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）可能编码并放大性别偏见，尤其在社会敏感应用中。现有基准主要评估孤立场景的偏见，未能捕获人际互动中出现的微妙偏见。

Method: 引入新基准“Genres”，该基准通过双角色配置文件和叙事生成任务来评估MLLMs的性别偏见，旨在捕获丰富的人际动态并支持多维度细粒度偏见评估。实验在开源和闭源MLLMs上进行。

Result: 实验揭示了MLLMs中持续存在且上下文敏感的性别偏见，这些偏见在单角色设置中并不明显。

Conclusion: 关系感知型基准对于诊断MLLMs中微妙、互动驱动的性别偏见至关重要，并为未来的偏见缓解工作提供了可行见解。

Abstract: Multimodal large language models (MLLMs) have shown impressive capabilities
across tasks involving both visual and textual modalities. However, growing
concerns remain about their potential to encode and amplify gender bias,
particularly in socially sensitive applications. Existing benchmarks
predominantly evaluate bias in isolated scenarios, overlooking how bias may
emerge subtly through interpersonal interactions. We fill this gap by going
beyond single-entity evaluation and instead focusing on a deeper examination of
relational and contextual gender bias in dual-individual interactions. We
introduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs
through the lens of social relationships in generated narratives. Genres
assesses gender bias through a dual-character profile and narrative generation
task that captures rich interpersonal dynamics and supports a fine-grained bias
evaluation suite across multiple dimensions. Experiments on both open- and
closed-source MLLMs reveal persistent, context-sensitive gender biases that are
not evident in single-character settings. Our findings underscore the
importance of relationship-aware benchmarks for diagnosing subtle,
interaction-driven gender bias in MLLMs and provide actionable insights for
future bias mitigation.

</details>


### [34] [FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes](https://arxiv.org/abs/2506.23111)
*Janki Atul Nawale,Mohammed Safi Ur Rahman Khan,Janani D,Mansi Gupta,Danish Pruthi,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: 现有公平性研究多以西方为中心，不适用于印度等多元文化国家。本文提出INDIC-BIAS，一个印度中心化的LLM公平性评估基准，揭示了主流LLM对印度边缘化群体存在严重负面偏见，并呼吁谨慎使用。


<details>
  <summary>Details</summary>
Motivation: 现有关于公平性的研究主要集中于西方视角，无法充分评估在印度等文化多样性国家中大型语言模型（LLM）的公平性问题，尤其是在涉及印度多元身份群体时。

Method: 研究开发了INDIC-BIAS，一个针对印度85个身份群体（涵盖不同种姓、宗教、地区和部落）的LLM公平性评估基准。该方法包括：1. 咨询领域专家，收集1800多个社会文化话题；2. 基于这些话题，生成并手动验证20000个真实世界场景模板；3. 将模板组织成三种评估任务：合理性、判断和生成；4. 使用该基准评估了14个主流LLM。

Result: 评估发现，参评的14个LLM对印度边缘化身份群体存在强烈的负面偏见，模型经常强化常见刻板印象。此外，即使在明确要求LLM解释其决策时，模型也难以减轻偏见。

Conclusion: 当前LLM可能对印度身份群体造成分配性和代表性危害，因此在实际应用中需要更加谨慎。研究发布了INDIC-BIAS作为一个开源基准，以推动在印度背景下LLM偏见基准测试和缓解偏见的研究。

Abstract: Existing studies on fairness are largely Western-focused, making them
inadequate for culturally diverse countries such as India. To address this gap,
we introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to
evaluate fairness of LLMs across 85 identity groups encompassing diverse
castes, religions, regions, and tribes. We first consult domain experts to
curate over 1,800 socio-cultural topics spanning behaviors and situations,
where biases and stereotypes are likely to emerge. Grounded in these topics, we
generate and manually validate 20,000 real-world scenario templates to probe
LLMs for fairness. We structure these templates into three evaluation tasks:
plausibility, judgment, and generation. Our evaluation of 14 popular LLMs on
these tasks reveals strong negative biases against marginalized identities,
with models frequently reinforcing common stereotypes. Additionally, we find
that models struggle to mitigate bias even when explicitly asked to rationalize
their decision. Our evaluation provides evidence of both allocative and
representational harms that current LLMs could cause towards Indian identities,
calling for a more cautious usage in practical applications. We release
INDIC-BIAS as an open-source benchmark to advance research on benchmarking and
mitigating biases and stereotypes in the Indian context.

</details>


### [35] [Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models](https://arxiv.org/abs/2506.23122)
*Shivam Sharma,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本研究探讨在互联网表情包中识别叙事角色（英雄、反派、受害者、其他）的挑战，发现真实表情包语言的复杂性及模型在识别“受害者”和跨文化泛化方面的困难，并强调文化背景、提示工程和多模态推理的重要性。


<details>
  <summary>Details</summary>
Motivation: 识别互联网表情包中的叙事角色是一项具有挑战性的任务，尤其是考虑到表情包语言的细微、文化特异性和上下文丰富性，这与合成生成的内容不同。现有数据集存在类别不平衡和语言多样性不足的问题，需要更平衡和多语言的数据集来推动该领域的研究。

Method: 本研究基于CLEF 2024共享任务引入的、更平衡和语言多样化的表情包数据集，进行了全面的词汇和结构分析。评估了包括微调多语言Transformer、情感和滥用检测分类器、指令微调大型语言模型以及多模态视觉-语言模型在内的多种模型。在零样本设置下，使用精确率、召回率和F1分数进行性能评估。此外，还探索了引导多模态模型的提示设计策略，特别是结合结构化指令和角色定义的混合提示。

Result: 词汇和结构分析揭示了真实表情包语言的细微、文化特异和上下文丰富的特点。大型模型如DeBERTa-v3和Qwen2.5-VL表现出显著提升，但研究结果表明，在可靠识别“受害者”类别以及在不同文化和混合语言内容上进行泛化方面，仍存在持续的挑战。混合提示设计对多模态模型带来了微小但一致的改进。

Conclusion: 研究结果强调了文化背景、提示工程和多模态推理在建模视觉-文本内容中微妙叙事框架的重要性。

Abstract: This work investigates the challenging task of identifying narrative roles -
Hero, Villain, Victim, and Other - in Internet memes, across three diverse test
sets spanning English and code-mixed (English-Hindi) languages. Building on an
annotated dataset originally skewed toward the 'Other' class, we explore a more
balanced and linguistically diverse extension, originally introduced as part of
the CLEF 2024 shared task. Comprehensive lexical and structural analyses
highlight the nuanced, culture-specific, and context-rich language used in real
memes, in contrast to synthetically curated hateful content, which exhibits
explicit and repetitive lexical markers. To benchmark the role detection task,
we evaluate a wide spectrum of models, including fine-tuned multilingual
transformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,
and multimodal vision-language models. Performance is assessed under zero-shot
settings using precision, recall, and F1 metrics. While larger models like
DeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent
challenges in reliably identifying the 'Victim' class and generalising across
cultural and code-mixed content. We also explore prompt design strategies to
guide multimodal models and find that hybrid prompts incorporating structured
instructions and role definitions offer marginal yet consistent improvements.
Our findings underscore the importance of cultural grounding, prompt
engineering, and multimodal reasoning in modelling subtle narrative framings in
visual-textual content.

</details>


### [36] [Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.23127)
*Zhaoye Fei,Li Ji,Siyin Wang,Junhao Shi,Jingjing Gong,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了Embodied Planner-R1，一个结果驱动的强化学习框架，使大语言模型（LLM）无需人工标注即可通过自主探索解决具身任务规划问题，并在两个基准测试上大幅超越现有方法，展现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在需要持续环境理解和动作生成的具身任务规划场景中面临显著挑战。现有方法依赖静态知识生成开环动作脚本，难以学习动作与环境反馈之间的因果关系，尤其是在部分可观察环境中。

Method: 引入Embodied Planner-R1框架，这是一个新颖的结果驱动强化学习框架，通过最少监督的自主探索使LLMs发展交互能力。其核心创新包括：1) 无需人工标注，采用纯强化学习结合组rollout，通过并行探索实现环境内交互；2) 采用完成度驱动的稀疏奖励机制；3) 使用交互式策略优化（IPO）以高效从分组轨迹中学习。

Result: 在ALFWorld和ScienceWorld两个具挑战性的文本基具身规划基准测试中，Embodied Planner-R1分别取得了97.78%和79.92%的完成率，大幅超越了现有方法。在先前未见过的环境中，其性能仅下降3.66%，证明了其强大的泛化能力。

Conclusion: Embodied Planner-R1通过创新的强化学习范式，成功地使LLMs在具身任务规划中具备了强大的交互和自主探索能力，有效解决了现有方法的局限性，并在复杂环境中展示了卓越的性能和泛化性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, yet they face significant challenges in embodied task planning
scenarios that require continuous environmental understanding and action
generation. Existing approaches generate open-loop action scripts based on
static knowledge, making it difficult to learn causal relationships between
actions and environmental feedback, particularly in partially observable
environments. We introduce Embodied Planner-R1, a novel outcome-driven
reinforcement learning framework that enables LLMs to develop interactive
capabilities through autonomous exploration with minimal supervision. Our
framework incorporates three key innovations: (1) Without human annotations, we
employ pure reinforcement learning with group rollout, incorporating
in-environment interaction through parallel exploration; (2) completion-driven
sparse reward; and (3) Interactive Policy Optimization (IPO) for efficient
learning from grouped trajectories. Across two challenging text-based Embodied
planning benchmarks, Embodied Planner-R1 achieves impressive completion rates
of 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a
large margin, and suffers only a -3.66% drop in previously unseen environments,
evidencing strong generalization.

</details>


### [37] [Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format](https://arxiv.org/abs/2506.23133)
*Dingzirui Wang,Xuanliang Zhang,Rongyu Cao,Longxu Dou,Xianzhen Luo,Yingwei Ma,Qingfu Zhu,Wanxiang Che,Binhua Li,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: 为解决大语言模型(LLMs)推理不一致性问题，本文提出Format-Adapter。它利用LLMs自动生成并选择适应特定任务的推理格式，而非依赖人工标注，并在数学和常识推理任务上取得了平均4.3%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，在LLMs生成多个答案时，采用多种推理格式优于单一格式。然而，这些多格式通常依赖人工标注，这不仅成本高昂，且可能不适用于所有任务。

Method: 首先，提出了一种衡量LLMs生成多个答案时推理错误的方法。接着，引入了Format-Adapter模型，该模型利用LLMs根据所提出的错误测量标准，自动生成并选择最适合给定任务的推理格式，以最小化推理错误。

Result: 在数学和常识推理任务上进行的实验表明，Format-Adapter相较于现有方法，平均性能提升了4.3%，证明了其有效性。

Conclusion: Format-Adapter通过自动化生成和选择推理格式，有效解决了人工标注格式的局限性，显著提升了LLMs在处理推理任务时的性能，尤其是在多答案生成和投票的场景下。

Abstract: Generating and voting multiple answers is an effective method to mitigate
reasoning inconsistencies of large language models (LLMs). Prior works have
shown that multiple reasoning formats outperform a single format when
generating multiple answers. However, previous works using multiple formats
rely on formats labeled by humans, which could be unsuitable for all tasks and
have high labeling costs. To address this issue, we adapt suitable formats to
the given tasks by generating and selecting formats. We first propose how to
measure the reasoning error when generating multiple answers. Then, we
introduce Format-Adapter, which utilizes LLMs to generate and select suitable
reasoning formats by minimizing the error measurement we present. We conduct
experiments on math and commonsense reasoning tasks, where Format-Adapter
achieves a 4.3% performance improvement on average over previous works,
demonstrating the effectiveness.

</details>


### [38] [LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation](https://arxiv.org/abs/2506.23136)
*Shadman Sobhan,Mohammad Ariful Haque*

Main category: cs.CL

TL;DR: 本文提出了一种新型RAG流水线，通过结合向量相似性搜索和基于Gemma-2-9b-it的微调重排序器，有效处理包含表格和图像的复杂技术文档，显著提升了问答的准确性和相关性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）面临幻觉和知识过时等挑战。微调虽是解决方案但资源密集且需频繁重复。检索增强生成（RAG）能访问外部知识，但传统RAG难以从包含表格和图像的复杂技术文档中检索信息。

Method: 本文提出一个RAG流水线，专为处理包含表格和图像的技术文档（支持扫描和可搜索格式）。其检索过程结合了向量相似性搜索和基于Gemma-2-9b-it的微调重排序器。该重排序器利用RAFT（Retrieval-Augmented Fine-Tuning）在一个定制数据集上进行训练，旨在提高问答的上下文识别能力。

Result: 所提出的流水线在RAGas和DeepEval评估中取得了高置信度分数（94%和96%）和答案相关性分数（87%和93%）。对比分析显示，该架构在处理基于表格的问题和上下文外问题方面，优于一般的RAG流水线。

Conclusion: 该RAG流水线有效解决了传统RAG在处理复杂技术文档方面的局限性，通过优化检索和重排序过程，显著提升了LLMs在技术问答场景中的表现，具有高置信度和相关性。

Abstract: Large Language Models (LLMs) are capable of natural language understanding
and generation. But they face challenges such as hallucination and outdated
knowledge. Fine-tuning is one possible solution, but it is resource-intensive
and must be repeated with every data update. Retrieval-Augmented Generation
(RAG) offers an efficient solution by allowing LLMs to access external
knowledge sources. However, traditional RAG pipelines struggle with retrieving
information from complex technical documents with structured data such as
tables and images. In this work, we propose a RAG pipeline, capable of handling
tables and images in documents, for technical documents that support both
scanned and searchable formats. Its retrieval process combines vector
similarity search with a fine-tuned reranker based on Gemma-2-9b-it. The
reranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom
dataset designed to improve context identification for question answering. Our
evaluation demonstrates that the proposed pipeline achieves a high faithfulness
score of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%
(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed
architecture is superior to general RAG pipelines in terms of table-based
questions and handling questions outside context.

</details>


### [39] [Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion](https://arxiv.org/abs/2506.23137)
*Siyuan Li,Ruitong Liu,Yan Wen,Te Sun*

Main category: cs.CL

TL;DR: 本文提出FMS框架，通过结合上下文感知的静态实体表示和条件动态转换学习，动态优化知识图谱补全中的关系评分，并超越现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱补全方法主要依赖静态、基于嵌入的评分，在捕捉上下文依赖和关系动态性方面存在局限性，导致关系建模深度不足。

Method: 提出Flow-Modulated Scoring (FMS) 框架，包含：1) 语义上下文学习模块，用于编码上下文敏感的实体表示；2) 条件流匹配模块，学习由上下文控制的从头实体到尾实体的动态转换。该框架利用生成的预测向量场动态调整实体对的初始静态分数。

Result: 在多个标准基准测试中，所提出的方法超越了现有最先进的结果。

Conclusion: FMS通过整合上下文感知的静态表示和条件动态信息，实现了对关系语义更深入的建模，有效提升了知识图谱补全的性能。

Abstract: Effective modeling of multifaceted relations is pivotal for Knowledge Graph
Completion (KGC). However, a majority of existing approaches are predicated on
static, embedding-based scoring, exhibiting inherent limitations in capturing
contextual dependencies and relational dynamics. Addressing this gap, we
propose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal
components: (1) a semantic context learning module that encodes
context-sensitive entity representations, and (2) a conditional flow-matching
module designed to learn the dynamic transformation from a head to a tail
embedding, governed by the aforementioned context. The resultant predictive
vector field, representing the context-informed relational path, serves to
dynamically refine the initial static score of an entity pair. Through this
synergy of context-aware static representations and conditioned dynamic
information, FMS facilitates a more profound modeling of relational semantics.
Comprehensive evaluations on several standard benchmarks demonstrate that our
proposed method surpasses prior state-of-the-art results.

</details>


### [40] [Benchmarking Deep Search over Heterogeneous Enterprise Data](https://arxiv.org/abs/2506.23139)
*Prafulla Kumar Choubey,Xiangyu Peng,Shilpa Bhagavath,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 本文提出了一个评估“深度搜索”（一种复杂的RAG形式）的新基准，该基准要求对多样化的企业源进行多跳推理。实验表明，现有RAG方法的表现不佳，检索是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）系统难以处理真实且复杂的“深度搜索”任务，即需要对多样化、稀疏但相关的来源进行源感知、多跳推理。因此，需要一个专门的基准来评估其在该能力上的表现。

Method: 作者构建了一个合成数据管道，模拟产品规划、开发和支持等业务工作流，生成互联且包含真实噪声的内容，并附带保证真实答案的多跳问题。该基准包含可回答和不可回答的查询，以及一个包含39,190个企业工件的检索池。

Result: 1. 即使是表现最佳的智能体式RAG方法，在该基准上的平均性能得分也仅为32.96。
2. 检索被确定为主要瓶颈：现有方法难以进行深度搜索并检索所有必要的证据。
3. 由于检索受限，这些方法经常在部分上下文中进行推理，导致性能显著下降。

Conclusion: 现有RAG系统在“深度搜索”任务中面临严峻挑战，其主要性能瓶颈在于检索能力不足，无法获取完整证据进行多跳推理，从而导致整体性能显著受损。新基准明确揭示了这一关键弱点。

Abstract: We present a new benchmark for evaluating Deep Search--a realistic and
complex form of retrieval-augmented generation (RAG) that requires
source-aware, multi-hop reasoning over diverse, sparsed, but related sources.
These include documents, meeting transcripts, Slack messages, GitHub, and URLs,
which vary in structure and often contain human-to-human interactions. We build
it using a synthetic data pipeline that simulates business workflows across
product planning, development, and support stages, generating interconnected
content with realistic noise and multi-hop questions with guaranteed
ground-truth answers. We release our benchmark with both answerable and
unanswerable queries, and retrieval pool of 39,190 enterprise artifacts,
enabling fine-grained evaluation of long-context LLM and RAG systems. Our
experiments reveal that even the best-performing agentic RAG methods achieve an
average performance score of 32.96 on our benchmark. With further analysis, we
highlight retrieval as the main bottleneck: existing methods struggle to
conduct deep searches and retrieve all necessary evidence. Consequently, they
often reason over partial context, leading to significant performance
degradation.

</details>


### [41] [Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions](https://arxiv.org/abs/2506.23146)
*Dingzriui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文提出LCS（Learning-to-Context Slope）作为一种新型度量标准，旨在更可靠、可归因且数据高效地评估上下文学习（ICL）在大型语言模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 上下文学习（ICL）在大型语言模型（LLMs）中效果波动大，现有评估方法依赖性能变化，存在可靠性低、归因性差及在数据稀缺场景下不实用的问题，导致难以判断ICL何时能可靠提升性能。

Method: 提出“学习到上下文斜率”（Learning-to-Context Slope, LCS）这一新指标，通过建模学习增益（演示带来的损失减少）与上下文相关性（演示与输入的相关性）之间的斜率来量化ICL有效性。LCS能捕捉连续损失变化、将ICL失败归因于上下文对齐或输出校准问题，并通过合成评估减少对标注数据的依赖。

Result: 广泛实验证明，LCS与标注设置下的性能提升强相关，并在偏置或数据稀缺场景中能可靠反映真实有效性。进一步分析揭示了LCS的可操作阈值，并识别了对ICL成功至关重要的模型能力。

Conclusion: LCS提供了一种更可靠、可归因且数据高效的ICL有效性评估方法，有助于从业者判断ICL何时能可靠提升LLM性能，并深入理解ICL成功所需的关键模型能力。

Abstract: In-context learning (ICL) has emerged as an effective approach to enhance the
performance of large language models (LLMs). However, its effectiveness varies
significantly across models and tasks, posing challenges for practitioners to
determine when ICL reliably improves performance. Current evaluation
approaches, reliant on performance change after applying ICL, suffer from low
reliability, poor attribution, and impracticality in data-insufficient
scenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that
quantifies ICL effectiveness by modeling the slope between learning gain (loss
decrease from demonstrations) and contextual relevance (demonstration-input
relevance). LCS addresses key limitations of performance-based metrics: (1) it
captures continuous loss changes even when outputs are incorrect, improving
reliability; (2) its formulation attributes ICL failures to weak contextual
alignment (inability to adapt inputs to demonstrations) or strong output
calibration (self-verification of correctness); and (3) it minimizes reliance
on labeled data via synthetic evaluation. Extensive experiments demonstrate
that LCS strongly correlates with performance improvements in labeled settings
and reliably reflects true effectiveness in biased or data-scarce scenarios.
Further analysis reveals actionable thresholds for LCS and identifies model
capabilities critical to ICL success.

</details>


### [42] [V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy](https://arxiv.org/abs/2506.23149)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文提出V-Score一致性度量和V-Synthesis方法，旨在解决上下文学习（ICL）示例标注成本高昂及现有合成方法局限性问题。该方法可从零开始为任意任务合成高质量、多样化的ICL示例，并在实验中实现了2.0%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 上下文学习（ICL）示例的高昂标注成本；现有示例合成方法多为任务特定或依赖已有示例，难以从零开始为任意任务合成；从零合成时，缺乏标注指导可能导致合成偏差，难以保证与目标任务的一致性。

Method: 1. 提出一种名为V-Score的文本一致性度量，其性能更高且计算成本低于基于词或嵌入向量的度量。2. 引入V-Synthesis方法，该方法利用V-Score进行比例抽样，以确保合成示例的高一致性和多样性。

Result: 实验结果表明，V-Synthesis相较于现有合成方法平均性能提升2.0%。V-Score比现有基于grams或嵌入向量的度量具有更高性能和更低计算成本。

Conclusion: V-Synthesis是一种有效的示例合成方法，能够显著提高上下文学习的性能，并成功解决了从零开始合成示例时面临的一致性和多样性挑战。

Abstract: High labeling cost for in-context learning (ICL) demonstrations motivates
using large language models (LLMs) for synthesis to reduce overhead. However,
existing synthesis methods are mainly task-specific or rely on pre-existing
demonstrations. So this paper focuses on synthesizing demonstrations from
scratch for arbitrary tasks. A major challenge in synthesizing from scratch is
ensuring consistency with the target task, as the lack of labeling guidance
could lead to synthesis bias. We first propose a consistency metric called
V-Score, which has higher performance and lower computation cost compared with
the metrics based on grams or embedding vectors. Furthermore, we introduce
V-Synthesis, which leverages V-Score for proportional sampling to ensure both
high consistency and diversity of synthesized demonstrations. Experimental
results demonstrate that V-Synthesis yields an average performance improvement
of 2.0% compared to existing synthesis methods confirming the effectiveness of
V-Synthesis.

</details>


### [43] [RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams](https://arxiv.org/abs/2506.23192)
*Gabriel Iturra-Bocaz,Felipe Bravo-Marquez*

Main category: cs.CL

TL;DR: 本文介绍了RiverText，一个Python库，用于从文本数据流中训练和评估增量词嵌入，旨在解决传统词嵌入在不断变化的语言模式中表现出的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统词嵌入模型是静态的，难以适应社交媒体和网络中不断演变的语言模式（如新出现的标签或品牌名称），这限制了它们在信息检索和自然语言处理任务中的有效性。因此，需要能够动态更新词表示的增量词嵌入算法。

Method: 本文提出了RiverText库，该库在一个标准化框架中实现了多种增量词嵌入技术，包括Skip-gram、Continuous Bag of Words和Word Context Matrix。它使用PyTorch作为神经网络训练的后端，并开发了一个模块将现有的静态词嵌入评估任务（如词相似度和词分类）适配到流式设置中。

Result: RiverText库被用于比较不同超参数设置下实现的增量词嵌入方法，并对结果进行了讨论。该库被定位为信息检索和自然语言处理社区处理流式场景下词嵌入的资源。

Conclusion: RiverText库提供了一个开源的、标准化的解决方案，用于从文本数据流中训练和评估增量词嵌入，有效克服了静态词嵌入的局限性，为处理动态语言模式提供了有价值的工具。

Abstract: Word embeddings have become essential components in various information
retrieval and natural language processing tasks, such as ranking, document
classification, and question answering. However, despite their widespread use,
traditional word embedding models present a limitation in their static nature,
which hampers their ability to adapt to the constantly evolving language
patterns that emerge in sources such as social media and the web (e.g., new
hashtags or brand names). To overcome this problem, incremental word embedding
algorithms are introduced, capable of dynamically updating word representations
in response to new language patterns and processing continuous data streams.
  This paper presents RiverText, a Python library for training and evaluating
incremental word embeddings from text data streams. Our tool is a resource for
the information retrieval and natural language processing communities that work
with word embeddings in streaming scenarios, such as analyzing social media.
The library implements different incremental word embedding techniques, such as
Skip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized
framework. In addition, it uses PyTorch as its backend for neural network
training. We have implemented a module that adapts existing intrinsic static
word embedding evaluation tasks for word similarity and word categorization to
a streaming setting. Finally, we compare the implemented methods with different
hyperparameter settings and discuss the results. Our open-source library is
available at https://github.com/dccuchile/rivertext.

</details>


### [44] [Generalist Reward Models: Found Inside Large Language Models](https://arxiv.org/abs/2506.23235)
*Yi-Chen Li,Tian Xu,Yang Yu,Xuqin Zhang,Xiong-Hui Chen,Zhongxiang Ling,Ningjing Chao,Lei Yuan,Zhi-Hua Zhou*

Main category: cs.CL

TL;DR: 本文发现并理论证明大型语言模型（LLM）预训练中已隐含奖励模型（内生奖励），可等效于逆强化学习，无需人工偏好数据即可用于模型对齐，且实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM对齐严重依赖昂贵的人工偏好数据来训练奖励模型，而现有的AI反馈方法缺乏严格的理论基础，因此需要一种更高效、可扩展且有理论依据的对齐范式。

Method: 研究发现，通过标准下一词预测训练的任何LLM内部都潜在包含一个强大的通用奖励模型。本文证明该内生奖励在理论上等同于通过离线逆强化学习学到的奖励函数。基于此，无需额外训练即可直接从基础模型中提取高质量奖励信号，并利用此信号进行后续的强化学习。

Result: 理论上证明使用该内生奖励进行强化学习可以获得具有可证明更优错误界限的策略，这是LLM强化学习有效性的首次理论证明。实验验证了该理论，表明所提方法不仅优于现有的LLM-as-a-judge方法，甚至能超越显式训练的奖励模型。

Conclusion: 研究结果表明，奖励建模阶段可被一种从预训练知识中提取信息的新方法取代，这为LLM以及多模态模型的对齐提供了一种更高效、强大且可扩展的新范式。

Abstract: The alignment of Large Language Models (LLMs) is critically dependent on
reward models trained on costly human preference data. While recent work
explores bypassing this cost with AI feedback, these methods often lack a
rigorous theoretical foundation. In this paper, we discover that a powerful
generalist reward model is already latently present within any LLM trained via
standard next-token prediction. We prove that this endogenous reward is not a
heuristic, but is theoretically equivalent to a reward function learned through
offline inverse reinforcement learning. This connection allows us to directly
elicit a high-quality reward signal from a base (pre-trained or supervised
fine-tuned) model without any further training. Critically, we also prove that
subsequent reinforcement learning using this endogenous reward leads to a
policy with a provably superior error bound compared to the base model. To our
best knowledge, this is the first theoretical proof of the effectiveness of
reinforcement learning for LLMs. Our experiments validate this theory,
demonstrating that our method not only outperforms existing LLM-as-a-judge
approaches but can also surpass explicitly trained reward models. These
findings suggest that the reward modeling stage can be replaced by a principled
method of eliciting the knowledge already captured during pre-training,
heralding a more efficient, powerful, and scalable paradigm for LLMs alignment
as well as multi-modal models.

</details>


### [45] [Two Spelling Normalization Approaches Based on Large Language Models](https://arxiv.org/abs/2506.23288)
*Miguel Domingo,Francisco Casacuberta*

Main category: cs.CL

TL;DR: 针对历史文献拼写规范化，研究提出了两种基于大型语言模型的新方法，并通过评估发现它们虽有潜力，但统计机器翻译仍是目前最适合此任务的技术。


<details>
  <summary>Details</summary>
Motivation: 历史文献中缺乏标准化的拼写规范和语言的自然演变，对人文学者构成了固有的语言挑战。拼写规范化的目的是将文档的正字法与当代标准对齐，以解决这一长期存在的问题。

Method: 本研究提出了两种基于大型语言模型的新方法：一种未经监督训练，另一种则用于机器翻译。研究在涵盖多种语言和历史时期的数据集上进行了评估。

Result: 两种基于大型语言模型的方法都取得了令人鼓舞的结果。

Conclusion: 尽管大型语言模型方法表现良好，但统计机器翻译（SMT）似乎仍然是拼写规范化任务最合适的技术。

Abstract: The absence of standardized spelling conventions and the organic evolution of
human language present an inherent linguistic challenge within historical
documents, a longstanding concern for scholars in the humanities. Addressing
this issue, spelling normalization endeavors to align a document's orthography
with contemporary standards. In this study, we propose two new approaches based
on large language models: one of which has been trained without a supervised
training, and a second one which has been trained for machine translation. Our
evaluation spans multiple datasets encompassing diverse languages and
historical periods, leading us to the conclusion that while both of them
yielded encouraging results, statistical machine translation still seems to be
the most suitable technology for this task.

</details>


### [46] [Objective-Free Local Learning and Emergent Language Structure in Thinking Machines](https://arxiv.org/abs/2506.23293)
*P. Myles Eugenio*

Main category: cs.CL

TL;DR: 该研究提出了一个基于局部事件驱动自发学习的神经-符号生成式语言模型框架，其核心是分层Hopfield记忆链，能够从零开始学习并生成与人类语言形态一致的合成语言，并展现出超越传统模型的泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型可能缺乏可塑性、可解释性，且依赖预定义符号或全局目标。本研究旨在探索如何通过局部神经学习让符号结构自发涌现，并构建更具可扩展性、可解释性的神经-符号系统，推进生成式语言模型中类脑架构的发展。

Method: 该框架采用局部、事件驱动的自发学习。核心是一个分层Hopfield记忆链，作为组合式短期记忆和动态分词器（重分词器）。模型不依赖预定义符号或监督，而是从头开始构建结构，将符号序列学习为多尺度表示。通过构建投影张量将共现特征绑定为分层符号，实现局部激活的压缩以形成长程依赖。学习过程是局部的（赫布型），模型的约束决定了允许的自发结构，新信息与该结构对齐并保留。通过在推理过程中短暂激活新神经元，将分布式多尺度符号特征绑定到符号嵌入中，这些自发嵌入神经元充当长期记忆，支持组合推理和泛化的键值机制。

Result: 研究发现，重分词器能够从噪声中过滤自然语言模式，生成具有连贯内部形态的合成语言，其量化指标与人类语言相同。由于没有全局目标，模型展现出传统语言模型中不具备的可塑性，即使没有明确数据也能泛化到其初始推理类别之外。此外，模型证明了自发嵌入神经元（作为长期记忆）能够支持组合式推理和泛化。

Conclusion: 该架构为研究符号结构如何从局部神经学习中涌现提供了方法论基础。它为构建可扩展、可解释的神经-符号系统开辟了一条新途径，其中符号、语法和推理都作为Hopfield层次结构中的压缩记忆痕迹而自发产生。这种方法推动了生成式语言模型类脑架构的发展。

Abstract: We present a neuro-symbolic framework for generative language modeling based
on local, event-driven emergent learning. At its core is a hierarchical
Hopfield memory chain acting as a compositional short-term memory and dynamic
tokenizer (retokenizer). Rather than relying on predefined tokens or
supervision, the model builds structure from scratch, learning symbol sequences
as multi-scale representations. It constructs projection tensors that bind
co-occurring features into hierarchical tokens, introducing redundancy (i.e an
emergent gauge structure) and enabling compression of local activations into
long-range dependencies. Curiously, we find that the retokenizer can filter
natural language patterns from noise, generating synthetic languages with
coherent internal morphology -- quantifiably the same as human language.
Language is learned in a local (Hebbian) fashion, where model constraints
dictate allowed emergent structure, and new information is retained in
alignment with this structure. The absence of a global objective enables a form
of plasticity not found in conventional language models, allowing the system to
generalize beyond its initial inference class -- even without explicit data. We
demonstrate that briefly activating a new neuron during inference binds
distributed multi-scale token features into a symbolic embedding. These
emergent embedding neurons act as long-term memory and support a key-value
mechanism for compositional inference and generalization. This architecture
provides a methodological foundation for studying how symbolic structure can
emerge from local neural learning. It offers a new pathway for building
scalable, interpretable neuro-symbolic systems -- where tokens, grammar, and
reasoning arise as compressed memory traces within a Hopfield hierarchy. This
approach advances the development of neuromorphic architectures for generative
language models.

</details>


### [47] [Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)](https://arxiv.org/abs/2506.23315)
*Shouvon Sarker,Xishuang Dong,Lijun Qian*

Main category: cs.CL

TL;DR: 本研究针对n2c2 2022挑战赛，提出一种新颖的基于BERT的集成模型，用于从临床笔记中检测和分类药物事件，通过整合多个BERT模型的预测，显著提高了药物事件分类性能。


<details>
  <summary>Details</summary>
Motivation: 从健康记录中识别药物、疾病等关键变量在临床领域有广泛应用。本文旨在解决n2c2 2022挑战赛中从临床笔记检测和分类药物事件的子任务，以提升临床数据分析的NLP能力。

Method: 首先在Wikipedia和MIMIC等大数据集上预训练BERT模型，然后将这些模型在CMED训练数据上进行微调。接着，使用微调后的BERT模型对CMED测试数据进行药物事件分类，并采用投票策略整合多个模型的预测结果，构建最终的集成预测。

Result: 实验结果表明，该BERT集成模型能有效提升严格Micro-F分数约5%，严格Macro-F分数约6%。

Conclusion: 基于BERT的集成模型能显著提高从临床笔记中检测和分类药物事件的准确性，为临床数据分析提供有效工具。

Abstract: Identification of key variables such as medications, diseases, relations from
health records and clinical notes has a wide range of applications in the
clinical domain. n2c2 2022 provided shared tasks on challenges in natural
language processing for clinical data analytics on electronic health records
(EHR), where it built a comprehensive annotated clinical data Contextualized
Medication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of
this challenge that is to detect and classify medication events from clinical
notes through building a novel BERT-based ensemble model. It started with
pretraining BERT models on different types of big data such as Wikipedia and
MIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED
training data. These fine-tuned BERT models were employed to accomplish
medication event classification on CMED testing data with multiple predictions.
These multiple predictions generated by these fine-tuned BERT models were
integrated to build final prediction with voting strategies. Experimental
results demonstrated that BERT-based ensemble models can effectively improve
strict Micro-F score by about 5% and strict Macro-F score by about 6%,
respectively.

</details>


### [48] [Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family](https://arxiv.org/abs/2506.23340)
*Yumeng Lin,Xufeng Duan,David Haslett,Yige Chen,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 本研究系统探究了训练数据、语言亲近性及语系对大型语言模型多语言翻译信息损失的影响，发现翻译质量受数据量、语言结构和类型学关系共同塑造。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多语言翻译中面临特定语对（数据有限或与英语差异大）的挑战，本研究旨在系统探究训练数据、语言亲近性和语系如何影响多语言翻译中的信息损失。

Method: 通过对GPT-4和Llama 2进行往返翻译评估，使用BLEU分数和BERT相似度指标衡量翻译质量。

Result: 研究发现训练数据大小与语言距离存在显著交互：充足数据可缓解语言差异，但与英语结构更近的语言在低资源条件下翻译质量更高。正字法、系统发育、句法和地理距离是翻译性能的强预测因子，语系也有独立影响。

Conclusion: 大型语言模型的翻译质量不仅受数据量影响，还受语言间的结构和类型学关系影响，加深了对多语言翻译中语言约束的理解。

Abstract: Large language models have achieved impressive progress in multilingual
translation, yet they continue to face challenges with certain language
pairs-particularly those with limited training data or significant linguistic
divergence from English. This study systematically investigates how training
data, language proximity, and language family affect information loss in
multilingual translation. We evaluate two large language models, GPT-4 and
Llama 2, by performing round-trip translations. Translation quality was
assessed using BLEU scores and BERT similarity metrics. Our results reveal a
robust interaction between training data size and language distance: while
abundant training data can mitigate the effects of linguistic divergence,
languages structurally closer to English consistently yield higher translation
quality in low-resource conditions. Among various distance metrics,
orthographic, phylogenetic, syntactic, and geographical distances emerge as
strong predictors of translation performance. Language family also exerts an
independent influence. These findings contribute to a deeper understanding of
the linguistic constraints shaping multilingual translation in large language
models, emphasizing that translation quality is shaped not only by data volume
but also by structural and typological relationships between languages.

</details>


### [49] [ATGen: A Framework for Active Text Generation](https://arxiv.org/abs/2506.23342)
*Akim Tsvigun,Daniil Vasilev,Ivan Tsvigun,Ivan Lysenko,Talgat Bektleuov,Aleksandr Medvedev,Uliana Vinogradova,Nikita Severin,Mikhail Mozikov,Andrey Savchenko,Rostislav Grigorev,Ramil Kuleev,Fedor Zhdanov,Artem Shelmanov,Ilya Makarov*

Main category: cs.CL

TL;DR: 一个名为ATGen的框架，将活性学习应用于文本生成任务，以降低人工和LLM标注成本。


<details>
  <summary>Details</summary>
Motivation: 尽管自然语言生成（NLG）任务日益流行，但活性学习（AL）在其上的应用却很有限，而AL在减少机器学习模型训练的标注工作量方面潜力巨大。

Method: 提出一个名为Active Text Generation (ATGen) 的综合框架，该框架将AL与文本生成任务连接起来，支持使用人工标注器和基于大型语言模型（LLM）的自动标注代理进行标注。ATGen还提供一个统一平台，用于实现和基准测试针对NLG任务的新型AL策略。

Result: 在不同设置和多个文本生成任务中评估了最先进的AL策略，结果表明ATGen显著降低了人工标注器的工作量以及调用LLM-based标注代理的API成本。

Conclusion: ATGen框架成功地将活性学习应用于文本生成任务，有效减少了模型训练所需的标注成本，为NLG领域AL策略的开发和应用提供了统一平台。

Abstract: Active learning (AL) has demonstrated remarkable potential in reducing the
annotation effort required for training machine learning models. However,
despite the surging popularity of natural language generation (NLG) tasks in
recent years, the application of AL to NLG has been limited. In this paper, we
introduce Active Text Generation (ATGen) - a comprehensive framework that
bridges AL with text generation tasks, enabling the application of
state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered
annotation in NLG tasks using both human annotators and automatic annotation
agents based on large language models (LLMs). The framework supports LLMs
deployed as services, such as ChatGPT and Claude, or operated on-premises.
Furthermore, ATGen provides a unified platform for smooth implementation and
benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present
evaluation results for state-of-the-art AL strategies across diverse settings
and multiple text generation tasks. We show that ATGen reduces both the effort
of human annotators and costs associated with API calls to LLM-based annotation
agents. The code of the framework is available on GitHub under the MIT license.
The video presentation is available at http://atgen-video.nlpresearch.group

</details>


### [50] [Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs](https://arxiv.org/abs/2506.23377)
*Taejin Kim,Siun-Chuon Mau,Konrad Vesey*

Main category: cs.CL

TL;DR: 本文提出Perspective-Dial系统，通过“视角空间”量化文本视角，并利用“系统化提示工程”控制大语言模型（LLM）输出的视角，旨在解决LLM偏见难以量化的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在关键任务中应用广泛，但其输出的偏见和视角缺乏可量化的理解。本文旨在解决LLM输出视角难以量化和控制的问题。

Method: 提出名为“Perspective-Dial”的系统，包含两部分：1) “视角空间”（Perspective Space），用于量化不同视角的度量空间；2) “系统化提示工程”（Systematic Prompt Engineering），利用贪婪坐标下降法，根据视角空间的测量反馈来控制LLM输出视角。

Result: 该方法通过实证方式，能有效地量化和调整多种主题的LLM输出视角，避免了对视角或偏见进行原则性理解的需要。

Conclusion: 该方法具有广泛的应用前景，包括检测、追踪和减轻LLM偏见，分析公共话语中的叙事，以及开发支持特定视角的辩论机器人。

Abstract: Large language models (LLMs) are used in a variety of mission-critical roles.
Due to the rapidly developing nature of LLMs, there is a lack of quantifiable
understanding of the bias and perspective associated with LLM output. Inspired
by this need, this paper considers the broader issue of perspective or
viewpoint of general text and perspective control of large-language model (LLM)
output. Perspective-Dial consists of two main components: a (1) metric space,
dubbed Perspective Space, that enables quantitative measurements of different
perspectives regarding a topic, and the use of (2) Systematic Prompt
Engineering that utilizes greedy-coordinate descent to control LLM output
perspective based on measurement feedback from the Perspective Space. The
empirical nature of the approach allows progress to side step a principled
understanding of perspective or bias -- effectively quantifying and adjusting
outputs for a variety of topics. Potential applications include detection,
tracking and mitigation of LLM bias, narrative detection, sense making and
tracking in public discourse, and debate bot advocating given perspective.

</details>


### [51] [Hierarchical Memory Organization for Wikipedia Generation](https://arxiv.org/abs/2506.23393)
*Eugene J. Yu,Dawei Zhu,Yifan Song,Xiangyu Wong,Jiebin Zhang,Wenxuan Shi,Xiaoguang Li,Qun Liu,Sujian Li*

Main category: cs.CL

TL;DR: 本文提出了MOG框架，利用分层记忆架构从网络文档中提取并组织信息，以自动生成信息丰富、可靠且可追溯的维基百科文章，并在新数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 自动生成维基百科文章是一项具有挑战性的任务，需要整合来自不同来源的准确、全面且结构良好的信息。现有方法可能在信息量、可验证性以及幻觉问题上存在不足。

Method: 本文引入了基于记忆组织生成（MOG）框架。该框架利用分层记忆架构，从网络文档中提取细粒度记忆单元，并将其递归组织成维基百科风格的分层结构以指导文章生成，从而确保记忆与文章大纲的一致性，减少幻觉。此外，MOG还实现了一个引用模块，将每句生成内容链接到具体的记忆单元，以增强可追溯性。

Result: 通过在新创建的WikiStart数据集上进行评估，MOG框架在生成信息丰富和可靠的文章方面优于基线方法，在实际场景中表现出尤其强大的鲁棒性。

Conclusion: MOG框架通过其独特的分层记忆架构和引用机制，有效解决了维基百科文章自动生成中的信息整合、可靠性和可追溯性挑战，为高质量、可靠文章的生成提供了一种有效且鲁棒的方法。

Abstract: Generating Wikipedia articles autonomously is a challenging task requiring
the integration of accurate, comprehensive, and well-structured information
from diverse sources. This paper introduces the Memory Organization-based
Generation (MOG) framework, a novel approach to address these challenges by
leveraging a hierarchical memory architecture. MOG extracts fine-grained memory
units from web documents, recursively organizes them into a Wikipedia-style
hierarchical structure, and uses this structure to guide the generation
process. This ensures alignment between memory and the article outline,
improving both informativeness and verifiability while minimizing
hallucinations. Additionally, a citation module is implemented to enhance
traceability by linking every generated sentence to specific memory units.
Evaluations on our newly created WikiStart dataset demonstrate that MOG
outperforms baseline methods in producing informative and reliable articles,
making it particularly robust in real-world scenarios.

</details>


### [52] [Datasets for Fairness in Language Models: An In-Depth Survey](https://arxiv.org/abs/2506.23411)
*Jiale Zhang,Zichong Wang,Avash Palikhe,Zhipeng Yin,Wenbin Zhang*

Main category: cs.CL

TL;DR: 该研究综述了语言模型公平性评估中常用的数据集，揭示了其内在偏见，并提出了一个统一评估框架，以改进模型公平性分析和数据集使用。


<details>
  <summary>Details</summary>
Motivation: 公平性基准在评估语言模型中扮演核心角色，但对其所依赖的数据集缺乏足够关注，导致研究人员未能充分认识到这些资源中固有的假设和局限性。

Method: 本研究通过全面审查当前语言模型研究中最广泛使用的公平性数据集，从其来源、范围、内容和预期用途等多个维度进行特征分析。此外，引入了一个统一的评估框架，并将其应用于24个常用基准，以揭示数据集中和评分方法中一致的人口统计学差异模式。

Result: 研究揭示了数据集和评分方法中存在的人口统计学差异的普遍模式。强调了常被忽视的偏见，这些偏见可能影响对模型公平性结论的判断。为选择、组合和解释这些数据集提供了实用指导。

Conclusion: 该研究揭示了现有公平性评估数据集中的固有偏见，并提供了一个统一评估框架和实用指导，以促进更严谨的模型公平性评估。未来应创建反映更多样化社会背景的新公平性基准，并鼓励更审慎地使用现有评估工具。

Abstract: Fairness benchmarks play a central role in shaping how we evaluate language
models, yet surprisingly little attention has been given to examining the
datasets that these benchmarks rely on. This survey addresses that gap by
presenting a broad and careful review of the most widely used fairness datasets
in current language model research, characterizing them along several key
dimensions including their origin, scope, content, and intended use to help
researchers better appreciate the assumptions and limitations embedded in these
resources. To support more meaningful comparisons and analyses, we introduce a
unified evaluation framework that reveals consistent patterns of demographic
disparities across datasets and scoring methods. Applying this framework to
twenty four common benchmarks, we highlight the often overlooked biases that
can influence conclusions about model fairness and offer practical guidance for
selecting, combining, and interpreting these datasets. We also point to
opportunities for creating new fairness benchmarks that reflect more diverse
social contexts and encourage more thoughtful use of these tools going forward.
All code, data, and detailed results are publicly available at
https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets
to promote transparency and reproducibility across the research community.

</details>


### [53] [TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs](https://arxiv.org/abs/2506.23423)
*Felipe Nuti,Tim Franzmeyer,João Henriques*

Main category: cs.CL

TL;DR: 本文提出一种新方法（TuCo）量化微调对大型语言模型（LLMs）个体输出的贡献，并发现其与对抗性攻击成功率之间的关联。


<details>
  <summary>Details</summary>
Motivation: 现有工作虽然研究了微调对LLMs整体性能的影响，但仍缺乏一种定量、系统的方法来分析微调对LLMs个体输出的具体作用。

Method: 研究者提出一种新方法，通过追踪模型的中间隐藏状态，精确地将微调后的LLM分解为预训练组件和微调组件。在此基础上，他们定义了“微调贡献度”（TuCo），即微调组件与预训练组件的量级之比。

Result: 经验性研究发现，通过调整微调组件的比例，可以引导模型的行为和性能。此外，研究观察到三种主流的对抗性攻击在规避安全措施时，会以降低TuCo的方式进行，并且在攻击成功的提示下，TuCo值普遍低于攻击失败的提示。这表明减弱微调对模型输出的影响，在这些攻击的成功中扮演了重要角色。

Conclusion: TuCo方法使得定量研究微调如何影响模型行为和安全性，以及反之亦然成为可能。

Abstract: Past work has studied the effects of fine-tuning on large language models'
(LLMs) overall performance on certain tasks. However, a quantitative and
systematic method for analyzing its effect on individual outputs is still
lacking. Here, we propose a new method for measuring the contribution that
fine-tuning makes to individual LLM responses, assuming access to the original
pre-trained model. Our method tracks the model's intermediate hidden states,
providing a more fine-grained insight into the effects of fine-tuning than a
simple comparison of final outputs from pre-trained and fine-tuned models. We
introduce and theoretically analyze an exact decomposition of any fine-tuned
LLM into a pre-training component and a fine-tuning component. Empirically, we
find that model behavior and performance can be steered by up- or down-scaling
the fine-tuning component during the forward pass. Motivated by this finding
and our theoretical analysis, we define the Tuning Contribution (TuCo) as the
ratio of the magnitudes of the fine-tuning component to the pre-training
component. We observe that three prominent adversarial attacks on LLMs
circumvent safety measures in a way that reduces TuCo, and that TuCo is
consistently lower on prompts where these attacks succeed compared to those
where they do not. This suggests that attenuating the effect of fine-tuning on
model outputs plays a role in the success of such attacks. In summary, TuCo
enables the quantitative study of how fine-tuning influences model behavior and
safety, and vice versa.

</details>


### [54] [Pipelined Decoder for Efficient Context-Aware Text Generation](https://arxiv.org/abs/2506.23431)
*Zixian Huang,Chenxu Niu,Yu Gu,Gengyang Xiao,Xinwei Huang,Gong Cheng*

Main category: cs.CL

TL;DR: 提出一种流水线解码器，通过并行生成显著加速自回归模型的文本生成速度，同时保持生成质量和内存效率。


<details>
  <summary>Details</summary>
Motivation: 自回归模型作为生成式AI的基础，其逐个生成token的特性导致生成速度慢，形成性能瓶颈。

Method: 提出了一种名为“流水线解码器”的新型解码器架构。该架构通过同时启动多个子序列的生成，并在每个时间步为每个子序列并行生成新token。

Result: 在问答、文本摘要和关键词生成等多种文本生成任务上的实验表明，该流水线解码器显著提高了生成速度，且未显著降低生成质量或增加内存消耗。

Conclusion: 流水线解码器有效解决了自回归模型生成速度的瓶颈，实现了高效且高质量的文本并行生成。

Abstract: As the basis of generative AI, an autoregressive model requires the
generation of a new token depending on all the previously generated tokens,
which brings high quality but also restricts the model to generate tokens one
by one, forming a bottleneck limiting the generation speed. In this paper, we
propose a new decoder architecture that efficiently generates text in parallel
for context-aware generation tasks. Our proposed pipelined decoder initiates
the generation of multiple subsequences simultaneously, and, at each time-step,
it generates a new token for each subsequence to realize parallelism.
Experiments on multiple text generation tasks, including question answering,
text summarization, and keyphrase generation, show that our pipelined decoder
significantly improves the generation speed without a significant loss of
generation quality or additional memory consumption.

</details>


### [55] [What to Keep and What to Drop: Adaptive Table Filtering Framework](https://arxiv.org/abs/2506.23463)
*Jang Won June*

Main category: cs.CL

TL;DR: 本文提出ATF框架，通过过滤非必要行列，显著缩减表格大小，帮助LLMs更好地处理大型表格数据，并在表格问答任务上提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在处理大型表格时，由于输入长度限制，难以有效进行基于表格的推理。

Method: 提出ATF（Adaptive Table Filtering Framework），一个模块化且问题感知的过滤管道。该框架利用LLM生成的列描述、聚类和稀疏-密集对齐分数来修剪非信息性列和行。ATF可与现有模型（如TAPAS、TAPEX）无缝集成，无需重新训练。

Result: 实验表明，ATF能将表格单元格数量减少约70%。在域外表格问答（TableQA）任务上显著提升了性能，但在表格事实验证（Table Fact Verification）任务上，由于对完整表格上下文的依赖性更高，性能略有下降。

Conclusion: ATF框架能够自适应地平衡信息量和精简性，以适应不同任务的需求，有效提升了LLMs处理大型表格的能力。

Abstract: Large language models (LLMs) for table-based reasoning often struggle with
large tables due to input length limits. We propose ATF (Adaptive Table
Filtering Framework), a modular and question-aware filtering pipeline that
prunes uninformative columns and rows using LLM-generated column descriptions,
clustering, and sparse-dense alignment scores. ATF integrates seamlessly with
existing models (e.g., TAPAS, TAPEX) without retraining. Experiments show that
ATF reduces table cells by ~70\%, boosting performance on out-of-domain TableQA
tasks while causing slight performance drops on Table Fact Verification, where
full-table context is more critical. These results highlight ATF's ability to
adaptively balance informativeness and minimalism across tasks.

</details>


### [56] [Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent](https://arxiv.org/abs/2506.23485)
*Haocheng Yu,Yaxiong Wu,Hao Wang,Wei Guo,Yong Liu,Yawen Li,Yuyang Ye,Junping Du,Enhong Chen*

Main category: cs.CL

TL;DR: 针对LLM代理在交互式推荐中处理复杂用户意图的局限性，本文提出TAIRA系统，通过蒸馏思想模式（TPD）增强其规划能力，并在多数据集上验证了其在复杂和新颖任务上的显著优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的交互式推荐代理在处理多样化、复杂（如直观、模糊）的用户意图时，因规划和泛化能力有限而面临挑战。

Method: 本文提出TAIRA系统，一个基于LLM的多智能体系统。其核心是管理器智能体，通过分解用户需求和规划子任务来协调推荐。该管理器智能体的规划能力通过“思想模式蒸馏”（TPD）方法增强，TPD从智能体自身和人类专家的经验中提取高级思想。此外，设计了用户模拟方案生成不同难度的个性化查询用于评估。

Result: 通过多数据集的综合实验，TAIRA表现出显著优于现有方法的性能，尤其在更具挑战性的任务上优势更为明显，并在新任务上展现出有效的泛化能力。

Conclusion: TAIRA系统通过引入蒸馏思想模式，有效提升了LLM代理在交互式推荐系统中处理复杂用户意图的能力，证明了其在该领域的卓越性能和泛化优势。

Abstract: Interactive recommendation is a typical information-seeking task that allows
users to interactively express their needs through natural language and obtain
personalized recommendations. Large language model-powered (LLM-powered) agents
have become a new paradigm in interactive recommendations, effectively
capturing users' real-time needs and enhancing personalized experiences.
However, due to limited planning and generalization capabilities, existing
formulations of LLM-powered interactive recommender agents struggle to
effectively address diverse and complex user intents, such as intuitive,
unrefined, or occasionally ambiguous requests. To tackle this challenge, we
propose a novel thought-augmented interactive recommender agent system (TAIRA)
that addresses complex user intents through distilled thought patterns.
Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring
a manager agent that orchestrates recommendation tasks by decomposing user
needs and planning subtasks, with its planning capacity strengthened through
Thought Pattern Distillation (TPD), a thought-augmentation method that extracts
high-level thoughts from the agent's and human experts' experiences. Moreover,
we designed a set of user simulation schemes to generate personalized queries
of different difficulties and evaluate the recommendations based on specific
datasets. Through comprehensive experiments conducted across multiple datasets,
TAIRA exhibits significantly enhanced performance compared to existing methods.
Notably, TAIRA shows a greater advantage on more challenging tasks while
generalizing effectively on novel tasks, further validating its superiority in
managing complex user intents within interactive recommendation systems. The
code is publicly available at:https://github.com/Alcein/TAIRA.

</details>


### [57] [Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably](https://arxiv.org/abs/2506.23508)
*Zhihao Zhang,Qiaole Dong,Qi Zhang,Jun Zhao,Enyu Zhou,Zhiheng Xi,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Tao Ji,Tao Gui,Xuanjing Huang*

Main category: cs.CL

TL;DR: 研究发现，在多模态大语言模型中，SFT能快速适应新任务但导致灾难性遗忘，而RFT学习较慢但能保持原有知识。数据分布而非算法差异是遗忘的关键。


<details>
  <summary>Details</summary>
Motivation: SFT和RFT等后训练算法在使多模态大语言模型适应下游任务方面有效，但其对模型原有知识的影响尚不明确。

Method: 引入“拼图”这一新任务，并在开源多模态模型Qwen2.5-VL上系统研究了SFT和RFT的行为，通过学习动态分析其现象。

Result: SFT能快速掌握新任务但导致灾难性遗忘，而RFT学习较慢但能保持原有知识。分析表明RFT通过强化与基础模型概率景观对齐的正确样本来减轻对原有知识的干扰。此外，在RFT模拟的正确轨迹上进行SFT训练可实现快速学习和知识保留。

Conclusion: 数据分布在遗忘中扮演核心角色，而非算法差异。RFT在多模态大语言模型中展现出稳定持续学习的潜力。

Abstract: Post-training algorithms such as Supervised Fine-Tuning (SFT) and
Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large
language models to downstream tasks. While effective at task adaptation, their
impact on prior knowledge remains unclear. In this paper, we introduce jigsaw
puzzles as a novel task absent from existing pretraining corpora and
systematically study the behavior of SFT and RFT on an open-source multimodal
model, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid
task acquisition but leads to catastrophic forgetting, whereas RFT learns more
slowly on novel tasks but maintains prior knowledge. We analyze this phenomenon
through the lens of learning dynamics, showing that RFT reinforces correct
samples that are naturally aligned with the base model's probability landscape,
mitigating interference with prior knowledge. Moreover, supervised training on
correct RFT-simulated rollouts allows SFT to preserve knowledge while rapidly
learning new tasks. These findings suggest that data distribution, rather than
algorithmic differences, plays a central role in forgetting, and highlight
RFT's potential for stable continual learning in multimodal large language
models.

</details>


### [58] [NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning](https://arxiv.org/abs/2506.23524)
*Phan Quoc Hung Mai,Quang Hung Nguyen,Phuong Giang Duong,Hong Hanh Nguyen,Nguyen Tuan Long*

Main category: cs.CL

TL;DR: 本研究推出了NEU-ESC越南语教育情感与主题分类数据集，并利用基于BERT的多任务学习模型，在情感分类和主题分类任务上分别达到了83.7%和79.8%的准确率。


<details>
  <summary>Details</summary>
Motivation: 在教育领域，理解学生评论至关重要，但在越南语环境中，现有资源有限，且现有教育数据集缺乏领域相关性和学生俚语，无法有效支持学生情感与主题分析。

Method: 引入了NEU-ESC，一个从大学论坛收集的全新越南语教育情感与主题分类数据集，该数据集样本更多、类别更丰富、文本更长、词汇更广。在此基础上，探索了使用编码器专用语言模型（BERT）的多任务学习方法，并与包括大型语言模型在内的其他数据集和模型进行了基准测试。

Result: 通过多任务学习方法，使用BERT模型在情感分类任务上取得了83.7%的准确率，在主题分类任务上取得了79.8%的准确率。研究还对数据集和模型与其他数据集和模型（包括大型语言模型）进行了有效的基准比较。

Conclusion: NEU-ESC数据集的创建有效填补了越南语教育领域数据资源的空白。结合多任务学习和BERT模型，本研究为有效分析越南语学生评论中的情感和主题提供了强有力的方法和工具，并促进了相关领域的研究进展。

Abstract: In the field of education, understanding students' opinions through their
comments is crucial, especially in the Vietnamese language, where resources
remain limited. Existing educational datasets often lack domain relevance and
student slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese
dataset for Educational Sentiment Classification and Topic Classification,
curated from university forums, which offers more samples, richer class
diversity, longer texts, and broader vocabulary. In addition, we explore
multitask learning using encoder-only language models (BERT), in which we
showed that it achieves performance up to 83.7% and 79.8% accuracy for
sentiment and topic classification tasks. We also benchmark our dataset and
model with other datasets and models, including Large Language Models, and
discuss these benchmarks. The dataset is publicly available at:
https://huggingface.co/datasets/hung20gg/NEU-ESC.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [59] [Robust Perspective Correction for Real-World Crack Evolution Tracking in Image-Based Structural Health Monitoring](https://arxiv.org/abs/2506.22437)
*Xinxin Sun,Peter Chang*

Main category: cs.CV

TL;DR: 本研究提出了一种基于物理信息并适应SHM场景的图像对齐框架，该框架利用非线性各向异性扩散构建裂缝保留尺度空间，实现了在复杂真实条件下对结构裂缝演化的精确监测。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测(SHM)中，准确的图像对齐对于监测裂缝演化至关重要，尤其是在透视畸变、遮挡和低对比度等真实世界条件下。传统特征检测器不适用于薄裂缝定位，而轻量级二进制算法在纹理或阴影表面上关键点重复性差。

Method: 本研究提出一种基于物理信息的对齐框架，通过改造开放KAZE架构以适应SHM挑战。该方法利用非线性各向异性扩散构建裂缝保留尺度空间，并结合RANSAC进行单应性估计，无需训练、参数调整或预先校准。

Result: 该方法在砖石和混凝土的手持智能手机延时图像上进行了验证，在阴影干扰、裁剪、倾斜视角和表面杂波等多种现场条件下表现良好。与传统检测器相比，该框架将裂缝面积和骨架长度误差分别降低了70%和90%，并保持关键指标的对齐误差低于5%。该方法无监督、可解释且计算轻量。

Conclusion: 通过将非线性尺度空间建模应用于SHM图像对齐，本工作为跟踪真实世界裂缝演化提供了一种鲁棒且基于物理的替代传统技术的方法，并支持通过无人机和移动平台进行可扩展部署。

Abstract: Accurate image alignment is essential for monitoring crack evolution in
structural health monitoring (SHM), particularly under real-world conditions
involving perspective distortion, occlusion, and low contrast. However,
traditional feature detectors such as SIFT and SURF, which rely on
Gaussian-based scale spaces, tend to suppress high-frequency edges, making them
unsuitable for thin crack localization. Lightweight binary alternatives like
ORB and BRISK, while computationally efficient, often suffer from poor keypoint
repeatability on textured or shadowed surfaces. This study presents a
physics-informed alignment framework that adapts the open KAZE architecture to
SHM-specific challenges. By utilizing nonlinear anisotropic diffusion to
construct a crack-preserving scale space, and integrating RANSAC-based
homography estimation, the framework enables accurate geometric correction
without the need for training, parameter tuning, or prior calibration. The
method is validated on time-lapse images of masonry and concrete acquired via
handheld smartphone under varied field conditions, including shadow
interference, cropping, oblique viewing angles, and surface clutter. Compared
to classical detectors, the proposed framework reduces crack area and spine
length errors by up to 70 percent and 90 percent, respectively, while
maintaining sub-5 percent alignment error in key metrics. Unsupervised,
interpretable, and computationally lightweight, this approach supports scalable
deployment via UAVs and mobile platforms. By tailoring nonlinear scale-space
modeling to SHM image alignment, this work offers a robust and physically
grounded alternative to conventional techniques for tracking real-world crack
evolution.

</details>


### [60] [Counting with Confidence: Accurate Pest Monitoring in Water Traps](https://arxiv.org/abs/2506.22438)
*Xumin Gao,Mark Stevens,Grzegorz Cielniak*

Main category: cs.CV

TL;DR: 该研究提出一种综合评估图像中害虫计数置信度的方法，结合计数结果与外部环境信息，有效解决了现有模型在实际部署中可靠性评估缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的自动害虫计数模型在实际应用中，因缺乏真实值而无法评估计数结果的可靠性，这是其主要局限性。

Method: 首先使用害虫检测网络进行计数并提取相关信息；接着对害虫图像进行质量、复杂度和分布均匀性评估，并量化图像清晰度变化；创新性地设计了假设驱动的多因素敏感性分析法以选择最优评估方法，并提出了自适应DBSCAN聚类算法评估分布均匀性；最后，将所有获取的信息输入回归模型以预测最终的害虫计数置信度。

Result: 与主要基于计数结果信息的基线方法相比，该方法在害虫计数置信度测试集上将均方误差（MSE）降低了31.7%，R2提高了15.2%。

Conclusion: 该研究首次全面评估了计数任务中的置信度，并通过模型量化了影响因素与计数置信度之间的关系，显著提升了害虫计数结果的可靠性评估能力。

Abstract: Accurate pest population monitoring and tracking their dynamic changes are
crucial for precision agriculture decision-making. A common limitation in
existing vision-based automatic pest counting research is that models are
typically evaluated on datasets with ground truth but deployed in real-world
scenarios without assessing the reliability of counting results due to the lack
of ground truth. To this end, this paper proposed a method for comprehensively
evaluating pest counting confidence in the image, based on information related
to counting results and external environmental conditions. First, a pest
detection network is used for pest detection and counting, extracting counting
result-related information. Then, the pest images undergo image quality
assessment, image complexity assessment, and pest distribution uniformity
assessment. And the changes in image clarity caused by stirring during image
acquisition are quantified by calculating the average gradient magnitude.
Notably, we designed a hypothesis-driven multi-factor sensitivity analysis
method to select the optimal image quality assessment and image complexity
assessment methods. And we proposed an adaptive DBSCAN clustering algorithm for
pest distribution uniformity assessment. Finally, the obtained information
related to counting results and external environmental conditions is input into
a regression model for prediction, resulting in the final pest counting
confidence. To the best of our knowledge, this is the first study dedicated to
comprehensively evaluating counting confidence in counting tasks, and
quantifying the relationship between influencing factors and counting
confidence through a model. Experimental results show our method reduces MSE by
31.7% and improves R2 by 15.2% on the pest counting confidence test set,
compared to the baseline built primarily on information related to counting
results.

</details>


### [61] [Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization](https://arxiv.org/abs/2506.22463)
*Weizhi Gao,Zhichao Hou,Junqi Yin,Feiyi Wang,Linyu Peng,Xiaorui Liu*

Main category: cs.CV

TL;DR: 本文提出MoDiff框架，通过调制量化和误差补偿，显著加速扩散模型，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在迭代采样中计算成本高昂，是其应用瓶颈。现有加速技术（如缓存和量化）存在计算误差和生成质量的局限性。

Method: 引入Modulated Diffusion (MoDiff)框架，通过调制量化和误差补偿来加速生成建模。MoDiff是一个通用框架，结合了现有缓存和量化方法的优点，并提供坚实的理论洞察和分析支持。

Result: 在CIFAR-10和LSUN上的实验表明，MoDiff在训练后量化(PTQ)中能将激活量化从8比特显著降低到3比特，且不影响性能。

Conclusion: MoDiff成功解决了扩散模型的计算效率问题，通过创新的量化和误差补偿机制，实现了大幅度的加速而无需牺牲生成质量，提供了一个通用且理论上严谨的加速框架。

Abstract: Diffusion models have emerged as powerful generative models, but their high
computation cost in iterative sampling remains a significant bottleneck. In
this work, we present an in-depth and insightful study of state-of-the-art
acceleration techniques for diffusion models, including caching and
quantization, revealing their limitations in computation error and generation
quality. To break these limits, this work introduces Modulated Diffusion
(MoDiff), an innovative, rigorous, and principled framework that accelerates
generative modeling through modulated quantization and error compensation.
MoDiff not only inherents the advantages of existing caching and quantization
methods but also serves as a general framework to accelerate all diffusion
models. The advantages of MoDiff are supported by solid theoretical insight and
analysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate
that MoDiff significant reduces activation quantization from 8 bits to 3 bits
without performance degradation in post-training quantization (PTQ). Our code
implementation is available at https://github.com/WeizhiGao/MoDiff.

</details>


### [62] [ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction](https://arxiv.org/abs/2506.22498)
*Hao Liu,Yu Hu,Rakiba Rayhana,Ling Bai,Zheng Liu*

Main category: cs.CV

TL;DR: 通过低成本称重传感器结合图像转换和双流Swin Transformer模型，实现早期离床意图预测，有效预防患者跌倒。


<details>
  <summary>Details</summary>
Motivation: 医院和长期护理机构中，与床相关的跌倒仍是导致受伤的主要原因，而现有商业警报系统通常在患者离床后才触发。

Method: 使用安装在床腿下的四个低成本称重传感器获取信号。将信号转换为RGB线图和三种纹理图（递归图、马尔可夫转移场、格拉姆角场）。引入ViFusionTST，一个双流Swin Transformer模型，并行处理并交叉注意力融合这些图像模态。

Result: 在包含95张床位、为期六个月的真实数据集上，ViFusionTST的准确率为0.885，F1分数为0.794，并在多项指标上超越了现有基线模型。

Conclusion: 基于图像融合的称重传感器信号用于时间序列分类，是实现实时、保护隐私的跌倒预防的实用且有效的解决方案。

Abstract: Bed-related falls remain a leading source of injury in hospitals and
long-term-care facilities, yet many commercial alarms trigger only after a
patient has already left the bed. We show that early bed-exit intent can be
predicted using only four low-cost load cells mounted under the bed legs. The
resulting load signals are first converted into a compact set of complementary
images: an RGB line plot that preserves raw waveforms and three texture maps -
recurrence plot, Markov transition field, and Gramian angular field - that
expose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin
Transformer that processes the line plot and texture maps in parallel and fuses
them through cross-attention to learn data-driven modality weights.
  To provide a realistic benchmark, we collected six months of continuous data
from 95 beds in a long-term-care facility. On this real-world dataset
ViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing
recent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.
The results demonstrate that image-based fusion of load-sensor signals for time
series classification is a practical and effective solution for real-time,
privacy-preserving fall prevention.

</details>


### [63] [Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data](https://arxiv.org/abs/2506.22499)
*Jiachao Liu,Pablo Guarda,Koichiro Niinuma,Sean Qian*

Main category: cs.CV

TL;DR: 该研究提出了一个新颖的集成框架，利用高分辨率卫星图像和传统交通数据，对多类别介观网络中的动态起讫点需求（DODE）进行估计，显著提高了估计性能，尤其是在缺乏局部传感器数据的路段。


<details>
  <summary>Details</summary>
Motivation: 传统的局部交通传感器数据稀疏，存在数据可用性限制。卫星图像能够提供一致的、覆盖全城的道路和交通信息（包括停车和移动车辆），从而克服了这一数据可用性限制。

Method: 该研究提出了一个新颖的集成框架，将高分辨率卫星图像与传统交通数据相结合进行DODE。具体方法包括：设计计算机视觉流程从卫星图像中提取车辆检测和地图匹配信息，生成链路级交通密度观测值；构建基于计算图的DODE模型，通过联合匹配局部传感器的交通量和行程时间与卫星衍生的密度测量值来校准动态网络状态。通过合成数据和真实世界数据进行了数值实验，并进行了样本外测试和敏感性分析。

Result: 实验结果表明，用卫星衍生的密度补充传统数据显著提高了DODE的估计性能，特别是对于没有局部传感器的路段。真实世界实验也证实了该框架处理大规模网络的能力。

Conclusion: 该集成框架具有在不同规模城市中实际部署的潜力，因为它能有效利用卫星数据克服传统传感器数据稀疏的局限性，显著提高动态起讫点需求估计的准确性和可扩展性。

Abstract: This study presents a novel integrated framework for dynamic
origin-destination demand estimation (DODE) in multi-class mesoscopic network
models, leveraging high-resolution satellite imagery together with conventional
traffic data from local sensors. Unlike sparse local detectors, satellite
imagery offers consistent, city-wide road and traffic information of both
parking and moving vehicles, overcoming data availability limitations. To
extract information from imagery data, we design a computer vision pipeline for
class-specific vehicle detection and map matching, generating link-level
traffic density observations by vehicle class. Building upon this information,
we formulate a computational graph-based DODE model that calibrates dynamic
network states by jointly matching observed traffic counts and travel times
from local sensors with density measurements derived from satellite imagery. To
assess the accuracy and scalability of the proposed framework, we conduct a
series of numerical experiments using both synthetic and real-world data. The
results of out-of-sample tests demonstrate that supplementing traditional data
with satellite-derived density significantly improves estimation performance,
especially for links without local sensors. Real-world experiments also confirm
the framework's capability to handle large-scale networks, supporting its
potential for practical deployment in cities of varying sizes. Sensitivity
analysis further evaluates the impact of data quality related to satellite
imagery data.

</details>


### [64] [Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models](https://arxiv.org/abs/2506.22500)
*Weiyi Zhao,Xiaoyu Tan,Liang Liu,Sijia Li,Youwei Song,Xihe Qiu*

Main category: cs.CV

TL;DR: 本文提出OR-VSKC数据集，包含超过34,000张合成图像和214张人工标注图像，旨在解决多模态大语言模型（MLLMs）在手术室风险识别中存在的视觉-语义知识冲突（VS-KC），并通过微调提升其对违规实体的检测能力。


<details>
  <summary>Details</summary>
Motivation: 手术风险识别对患者安全至关重要。尽管多模态大语言模型（MLLMs）在自动化手术室风险检测方面有潜力，但它们常表现出视觉-语义知识冲突（VS-KC），即尽管理解文本规则，却未能识别视觉安全违规。此外，此类场景的数据稀缺。

Method: 引入了一个名为OR-VSKC的综合数据集，包含超过34,000张由扩散模型生成的合成手术室场景图像（包含违反安全规则的实体），以及214张人工标注图像作为黄金标准验证参考。开发了针对规则违反场景的数据生成方法，并使用OR-VSKC数据集对MLLMs进行微调，随后对代表性MLLMs的违规敏感知识一致性进行了实证分析。

Result: 在OR-VSKC数据集上进行微调显著提高了MLLMs对已训练冲突实体的检测能力，并且在这些实体的新视角下泛化良好。然而，模型对未经训练的实体类型的性能仍然不佳，这突出了学习的特异性以及全面训练的必要性。

Conclusion: 本研究的主要贡献包括：1) 一种针对规则违反场景的数据生成方法；2) 发布了OR-VSKC数据集及其相关基准作为开源资源；3) 对代表性MLLMs中违规敏感知识一致性的实证分析。研究揭示了MLLMs在处理视觉-语义知识冲突方面的进步及局限性，强调了全面训练的重要性。

Abstract: Surgical risk identification is critical for patient safety and reducing
preventable medical errors. While multimodal large language models (MLLMs) show
promise for automated operating room (OR) risk detection, they often exhibit
visual-semantic knowledge conflicts (VS-KC), failing to identify visual safety
violations despite understanding textual rules. To address this, we introduce a
dataset comprising over 34,000 synthetic images generated by diffusion models,
depicting operating room scenes containing entities that violate established
safety rules. These images were created to alleviate data scarcity and examine
MLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated
images that serve as a gold-standard reference for validation. This
comprehensive dataset, spanning diverse perspectives, stages, and
configurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC
significantly improves MLLMs' detection of trained conflict entities and
generalizes well to new viewpoints for these entities, but performance on
untrained entity types remains poor, highlighting learning specificity and the
need for comprehensive training. The main contributions of this work include:
(1) a data generation methodology tailored for rule-violation scenarios; (2)
the release of the OR-VSKC dataset and its associated benchmark as open-source
resources; and (3) an empirical analysis of violation-sensitive knowledge
consistency in representative MLLMs. The dataset and appendix are available at
https://github.com/zgg2577/VS-KC.

</details>


### [65] [How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?](https://arxiv.org/abs/2506.22501)
*Gautam Siddharth Kashyap,Manaswi Kulahara,Nipun Joshi,Usman Naseem*

Main category: cs.CV

TL;DR: 本文提出SpatialNet-ViT模型，结合Vision Transformers (ViT)和多任务学习 (MTL)，旨在克服现有遥感分类模型泛化能力不足的问题，提升分类精度、可扩展性及鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有遥感分类研究多专注于狭窄任务或数据集，导致模型泛化能力差，难以应对多样化的遥感分类挑战。

Method: 提出SpatialNet-ViT模型，该模型融合了Vision Transformers (ViTs)和多任务学习 (MTL)，旨在结合空间感知与上下文理解。此外，还采用数据增强、迁移学习和多任务学习等技术以增强模型鲁棒性和泛化能力。

Result: 通过SpatialNet-ViT模型，预期能有效提升遥感分类任务的准确性和可扩展性，增强模型对多样化数据集的泛化能力和鲁棒性。

Conclusion: SpatialNet-ViT模型提供了一个更通用、更强大的解决方案，有望克服传统遥感分类模型的局限性，以处理复杂的遥感数据分类任务。

Abstract: Remote sensing datasets offer significant promise for tackling key
classification tasks such as land-use categorization, object presence
detection, and rural/urban classification. However, many existing studies tend
to focus on narrow tasks or datasets, which limits their ability to generalize
across various remote sensing classification challenges. To overcome this, we
propose a novel model, SpatialNet-ViT, leveraging the power of Vision
Transformers (ViTs) and Multi-Task Learning (MTL). This integrated approach
combines spatial awareness with contextual understanding, improving both
classification accuracy and scalability. Additionally, techniques like data
augmentation, transfer learning, and multi-task learning are employed to
enhance model robustness and its ability to generalize across diverse datasets

</details>


### [66] [What Makes a Dribble Successful? Insights From 3D Pose Tracking Data](https://arxiv.org/abs/2506.22503)
*Michiel Schepers,Pieter Robberechts,Jan Van Haaren,Jesse Davis*

Main category: cs.CV

TL;DR: 本研究利用三维姿态追踪数据分析足球过人动作，发现平衡和方向对过人成功至关重要，并能显著提升过人成功预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有足球数据分析主要依赖二维位置追踪数据，无法捕捉球员平衡、方向和控球等关键三维信息，限制了对过人技能评估的深度理解。

Method: 研究从2022/23赛季欧冠联赛的1,736次过人中提取了新颖的基于姿态的三维特征，并评估其对过人成功的影响，同时与传统二维位置数据进行对比。

Result: 研究结果表明，捕捉进攻球员平衡和进攻球员与防守球员方向对齐的姿态特征，对于预测过人成功具有重要信息，并且在传统二维特征基础上引入这些姿态特征，能显著提升模型性能。

Conclusion: 三维姿态追踪数据能够提供对过人技能更深入的理解，并通过捕捉传统二维数据未能涵盖的方面，有效提升过人成功预测模型的准确性。

Abstract: Data analysis plays an increasingly important role in soccer, offering new
ways to evaluate individual and team performance. One specific application is
the evaluation of dribbles: one-on-one situations where an attacker attempts to
bypass a defender with the ball. While previous research has primarily relied
on 2D positional tracking data, this fails to capture aspects like balance,
orientation, and ball control, limiting the depth of current insights. This
study explores how pose tracking data (capturing players' posture and movement
in three dimensions) can improve our understanding of dribbling skills. We
extract novel pose-based features from 1,736 dribbles in the 2022/23 Champions
League season and evaluate their impact on dribble success. Our results
indicate that features capturing the attacker's balance and the alignment of
the orientation between the attacker and defender are informative for
predicting dribble success. Incorporating these pose-based features on top of
features derived from traditional 2D positional data leads to a measurable
improvement in model performance.

</details>


### [67] [Patch2Loc: Learning to Localize Patches for Unsupervised Brain Lesion Detection](https://arxiv.org/abs/2506.22504)
*Hassan Baker,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: 本文提出一种名为Patch2Loc的无监督方法，通过训练神经网络预测MRI图像正常补丁的空间位置，利用预测误差或方差检测脑部病变，并在多种数据集上超越了现有无监督分割SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 脑部病变检测对诊断和治疗至关重要。现有监督学习方法需要大量带标注的病变数据，而获取这些数据耗时且困难。因此，研究者旨在开发一种无需标注病变数据、仅从正常组织学习的无监督检测方法。

Method: 研究者提出一种无监督方法（Patch2Loc），使用来自结构化MRI的正常图像块进行学习。训练一个神经网络模型，使其能将图像块映射回其在脑部切片中的空间位置。在推理阶段，通过位置预测的相对较高误差和/或方差来检测异常图像块，生成热图，可进一步整合到像素级方法中实现更精细的分割。

Result: 该模型成功应用于BraTS2021、MSLUB数据集的T2加权图像以及ATLAS和WMH数据集的T1加权图像上的肿瘤组织检测。实验证明，该方法在无监督分割方面优于现有最先进（SOTA）方法。

Conclusion: Patch2Loc提供了一种有效的无监督脑部病变检测方法，仅通过学习正常组织的模式即可识别异常区域。该方法在多种数据集上表现出超越现有无监督分割方法的性能，为计算机辅助诊断提供了新的途径。

Abstract: Detecting brain lesions as abnormalities observed in magnetic resonance
imaging (MRI) is essential for diagnosis and treatment. In the search of
abnormalities, such as tumors and malformations, radiologists may benefit from
computer-aided diagnostics that use computer vision systems trained with
machine learning to segment normal tissue from abnormal brain tissue. While
supervised learning methods require annotated lesions, we propose a new
unsupervised approach (Patch2Loc) that learns from normal patches taken from
structural MRI. We train a neural network model to map a patch back to its
spatial location within a slice of the brain volume. During inference, abnormal
patches are detected by the relatively higher error and/or variance of the
location prediction. This generates a heatmap that can be integrated into
pixel-wise methods to achieve finer-grained segmentation. We demonstrate the
ability of our model to segment abnormal brain tissues by applying our approach
to the detection of tumor tissues in MRI on T2-weighted images from BraTS2021
and MSLUB datasets and T1-weighted images from ATLAS and WMH datasets. We show
that it outperforms the state-of-the art in unsupervised segmentation. The
codebase for this work can be found on our
\href{https://github.com/bakerhassan/Patch2Loc}{GitHub page}.

</details>


### [68] [Weakly Supervised Object Segmentation by Background Conditional Divergence](https://arxiv.org/abs/2506.22505)
*Hassan Baker,Matthew S. Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: 本文提出一种弱监督的二元目标分割方法，仅利用图像级存在/缺失标签。通过生成反事实背景图像并设计特定损失项，在合成孔径声纳等专业领域表现出色，并在自然图像上无需预训练网络也能获得合理性能。


<details>
  <summary>Details</summary>
Motivation: 在合成孔径声纳、遥感、生物医学成像等缺乏大量像素级标注数据的专业图像领域，自动目标分割仍具挑战性，且获取像素级掩码成本高昂。

Method: 该方法训练一个掩码网络，利用图像级目标存在/缺失的弱监督信息进行二元分割。核心思想是将分割出的目标放置到纯背景图像中，创建具有反事实背景的图像。具体步骤包括：聚类背景图像，然后将目标与选定聚类的背景融合生成反事实图像。训练损失包含反事实图像与真实目标图像的散度（使用基于样本的散度）以及针对纯背景图像的监督损失。

Result: 实验在旁侧扫描和合成孔径声纳图像上验证了方法的有效性，相比之前在自然图像上测试的无监督基线取得了成功。此外，在自然图像上也能获得合理性能，且无需依赖预训练网络、生成网络或对抗性判别器。

Conclusion: 该方法为数据稀缺的专业领域提供了一种有效的弱监督二元目标分割方案，通过创新的反事实图像生成和损失设计，在不使用复杂预训练模型的情况下，展现出良好的泛化能力。

Abstract: As a computer vision task, automatic object segmentation remains challenging
in specialized image domains without massive labeled data, such as synthetic
aperture sonar images, remote sensing, biomedical imaging, etc. In any domain,
obtaining pixel-wise segmentation masks is expensive. In this work, we propose
a method for training a masking network to perform binary object segmentation
using weak supervision in the form of image-wise presence or absence of an
object of interest, which provides less information but may be obtained more
quickly from manual or automatic labeling. A key step in our method is that the
segmented objects can be placed into background-only images to create
realistic, images of the objects with counterfactual backgrounds. To create a
contrast between the original and counterfactual background images, we propose
to first cluster the background-only images, and then during learning create
counterfactual images that blend objects segmented from their original source
backgrounds to backgrounds chosen from a targeted cluster. One term in the
training loss is the divergence between these counterfactual images and the
real object images with backgrounds of the target cluster. The other term is a
supervised loss for background-only images. While an adversarial critic could
provide the divergence, we use sample-based divergences. We conduct experiments
on side-scan and synthetic aperture sonar in which our approach succeeds
compared to previous unsupervised segmentation baselines that were only tested
on natural images. Furthermore, to show generality we extend our experiments to
natural images, obtaining reasonable performance with our method that avoids
pretrained networks, generative networks, and adversarial critics. The basecode
for this work can be found at
\href{GitHub}{https://github.com/bakerhassan/WSOS}.

</details>


### [69] [FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment](https://arxiv.org/abs/2506.22509)
*Hang Xu,Jie Huang,Linjiang Huang,Dong Li,Yidi Liu,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出一种名为域噪声对齐（DNA）的免训练机制，通过调整扩散采样过程中的噪声统计量，为基于扩散的密集预测（DDP）模型实现域适应（DA），并在多种任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 域适应对于密集预测模型在未见域上的性能提升至关重要。随着DDP模型的发展，探索其DA设计变得有价值，因为扩散模型能有效模拟包含域信息的分布转换。研究发现，扩散中的曝光偏差（如噪声统计偏差）会导致域偏移，且DDP模型条件下的不同域可以通过噪声预测统计量有效捕捉。

Method: 提出了一种免训练的域噪声对齐（DNA）方法。该方法通过缓解扩散采样过程中噪声统计量随域变化而产生的差异来实现域适应。具体而言，当源域可用时，直接对齐目标域与源域的噪声统计量。对于更具挑战性的无源DA，则利用高置信度区域（更接近源域）的统计量，在采样过程中逐步引导噪声统计量调整。

Result: 所提出的方法有效地提升了DDP模型在四种常见密集预测任务上的域适应能力。

Conclusion: DNA是一种有效且免训练的DDP模型域适应方法，通过对齐噪声统计量解决了有源和无源的域适应问题，显著提升了模型在不同域下的性能。

Abstract: Domain Adaptation(DA) for dense prediction tasks is an important topic, which
enhances the dense prediction model's performance when tested on its unseen
domain. Recently, with the development of Diffusion-based Dense Prediction
(DDP) models, the exploration of DA designs tailored to this framework is worth
exploring, since the diffusion model is effective in modeling the distribution
transformation that comprises domain information. In this work, we propose a
training-free mechanism for DDP frameworks, endowing them with DA capabilities.
Our motivation arises from the observation that the exposure bias (e.g., noise
statistics bias) in diffusion brings domain shift, and different domains in
conditions of DDP models can also be effectively captured by the noise
prediction statistics. Based on this, we propose a training-free Domain Noise
Alignment (DNA) approach, which alleviates the variations of noise statistics
to domain changes during the diffusion sampling process, thereby achieving
domain adaptation. Specifically, when the source domain is available, we
directly adopt the DNA method to achieve domain adaptation by aligning the
noise statistics of the target domain with those of the source domain. For the
more challenging source-free DA, inspired by the observation that regions
closer to the source domain exhibit higher confidence meeting variations of
sampling noise, we utilize the statistics from the high-confidence regions
progressively to guide the noise statistic adjustment during the sampling
process. Notably, our method demonstrates the effectiveness of enhancing the DA
capability of DDP models across four common dense prediction tasks. Code is
available at
\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.

</details>


### [70] [Lightning the Night with Generative Artificial Intelligence](https://arxiv.org/abs/2506.22511)
*Tingting Zhou,Feng Zhang,Haoyang Fu,Baoxiang Pan,Renhe Zhang,Feng Lu,Zhixin Yang*

Main category: cs.CV

TL;DR: 本研究开创性地利用生成扩散模型（RefDiff）从热红外数据中高精度反演夜间可见光反射率，解决了夜间气象观测的连续性问题。


<details>
  <summary>Details</summary>
Motivation: 地球静止卫星可见光反射率数据对气象观测和预报至关重要，但夜间缺乏可见光导致无法进行全天候连续观测，限制了其应用。

Method: 基于风云四号B星（FY4B）高级静止辐射成像仪（AGRI）的多波段热红外亮温数据，首次采用生成扩散模型开发了高精度可见光反射率反演模型RefDiff，用于夜间0.47、0.65、0.825微米波段可见光反射率的反演。

Result: RefDiff模型通过集成平均显著提高了反演精度（SSIM指数高达0.90），尤其在复杂云结构和厚云区表现优异，并能提供不确定性估计。其夜间反演能力经VIIRS夜间产品验证，与白天性能相当。

Conclusion: 本研究在夜间可见光反射率反演方面取得了实质性进展，有望拓宽夜间可见光数据在气象领域的应用潜力。

Abstract: The visible light reflectance data from geostationary satellites is crucial
for meteorological observations and plays an important role in weather
monitoring and forecasting. However, due to the lack of visible light at night,
it is impossible to conduct continuous all-day weather observations using
visible light reflectance data. This study pioneers the use of generative
diffusion models to address this limitation. Based on the multi-band thermal
infrared brightness temperature data from the Advanced Geostationary Radiation
Imager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we
developed a high-precision visible light reflectance retrieval model, called
Reflectance Diffusion (RefDiff), which enables 0.47~\mu\mathrm{m},
0.65~\mu\mathrm{m}, and 0.825~\mu\mathrm{m} bands visible light reflectance
retrieval at night. Compared to the classical models, RefDiff not only
significantly improves accuracy through ensemble averaging but also provides
uncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,
with particularly significant improvements in areas with complex cloud
structures and thick clouds. The model's nighttime retrieval capability was
validated using VIIRS nighttime product, demonstrating comparable performance
to its daytime counterpart. In summary, this research has made substantial
progress in the ability to retrieve visible light reflectance at night, with
the potential to expand the application of nighttime visible light data.

</details>


### [71] [Automated Defect Identification and Categorization in NDE 4.0 with the Application of Artificial Intelligence](https://arxiv.org/abs/2506.22513)
*Aditya Sharma*

Main category: cs.CV

TL;DR: 本研究开发了一个基于改进U-net的自动化缺陷检测框架，用于射线照相领域，通过数据增强技术处理飞机焊缝图像，实现了高检测精度和快速推理，并被专业人员认可为有效的辅助工具。


<details>
  <summary>Details</summary>
Motivation: 旨在解决当前射线照相领域信息解释不足的问题，探索虚拟缺陷增加的最大化利用，并根据NDE 4.0标准，验证自动化缺陷检测框架的可行性。

Method: 收集并分类了223张飞机焊缝的CR照片作为基础数据源。采用虚拟缺陷增加和标准增加等信息扩展系统对数据集进行预处理。使用改进的U-net模型对增强数据进行训练，以实现语义缺陷分割。通过案例、估算准确性和误报率等NDE指标评估模型的有效性。

Result: 该方法在缺陷检测中，尤其是在微小a90/95特征方面，表现出卓越的识别能力。综合扩展方法在90/95、尺寸误差和焊缝区域误报率方面明显优于其他方法。该框架具有快速的推理速度，能够高效、快速地处理大型图像。

Conclusion: 该自动化缺陷检测框架被专业人员评估为在检测周期中具有支持设备的应用价值，不受特定设备和软件限制，证明了其作为辅助工具的可靠性和实用性。

Abstract: This investigation attempts to create an automated framework for fault
detection and organization for usage in contemporary radiography, as per NDE
4.0. The review's goals are to address the lack of information that is
sufficiently explained, learn how to make the most of virtual defect increase,
and determine whether the framework is viable by using NDE measurements. As its
basic information source, the technique consists of compiling and categorizing
223 CR photographs of airplane welds. Information expansion systems, such as
virtual defect increase and standard increase, are used to work on the
preparation dataset. A modified U-net model is prepared using the improved data
to produce semantic fault division veils. To assess the effectiveness of the
model, NDE boundaries such as Case, estimating exactness, and misleading call
rate are used. Tiny a90/95 characteristics, which provide strong
differentiating evidence of flaws, reveal that the suggested approach achieves
exceptional awareness in defect detection. Considering a 90/95, size error, and
fake call rate in the weld area, the consolidated expansion approach clearly
wins. Due to the framework's fast derivation speed, large images can be broken
down efficiently and quickly. Professional controllers evaluate the transmitted
system in the field and believe that it has a guarantee as a support device in
the testing cycle, irrespective of particular equipment cut-off points and
programming resemblance.

</details>


### [72] [Container damage detection using advanced computer vision model Yolov12 vs Yolov11 vs RF-DETR A comparative analysis](https://arxiv.org/abs/2506.22517)
*Subhadip Kumar*

Main category: cs.CV

TL;DR: 该研究比较了Yolov12、Yolov11和RF-DETR三种计算机视觉模型在集装箱损伤检测上的性能，发现RF-DETR在检测不常见损伤方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 集装箱损伤会带来安全隐患和经济损失，因此及时检测损伤对延长集装箱寿命和避免事故至关重要。

Method: 使用278张标注图像数据集，训练、验证和测试Yolov12、Yolov11和RF-DETR三种先进计算机视觉模型，并比较它们的mAP和精度。

Result: 结果喜忧参半。Yolov11和Yolov12的mAP@50为81.9%，RF-DETR为77.7%。然而，在测试不常见损伤的集装箱时，RF-DETR模型整体表现更优，能高置信度地准确检测出受损集装箱及损伤。

Conclusion: 尽管总体mAP略低，RF-DETR模型在检测不常见集装箱损伤和以高置信度识别损伤方面表现卓越，使其成为集装箱损伤检测的合适选择。

Abstract: Containers are an integral part of the logistics industry and act as a
barrier for cargo. A typical service life for a container is more than 20
years. However, overtime containers suffer various types of damage due to the
mechanical as well as natural factors. A damaged container is a safety hazard
for the employees handling it and a liability for the logistic company.
Therefore, a timely inspection and detection of the damaged container is a key
for prolonging service life as well as avoiding safety hazards. In this paper,
we will compare the performance of the damage detection by three
state-of-the-art advanced computer vision models Yolov12, Yolov11 and RF-DETR.
We will use a dataset of 278 annotated images to train, validate and test the
model. We will compare the mAP and precision of the model. The objective of
this paper is to identify the model that is best suited for container damage
detection. The result is mixed. mAP@50 score of Yolov11 and 12 was 81.9%
compared to RF-DETR, which was 77.7%. However, while testing the model for
not-so-common damaged containers, the RF-DETR model outperformed the others
overall, exhibiting superiority to accurately detecting both damaged containers
as well as damage occurrences with high confidence.

</details>


### [73] [Preserve Anything: Controllable Image Synthesis with Object Preservation](https://arxiv.org/abs/2506.22531)
*Prasen Kumar Sharma,Neeraj Matiyali,Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: 本文提出“Preserve Anything”方法，通过N通道ControlNet和新数据集，显著提升文本到图像（T2I）生成中多对象保真度、语义一致性和场景控制。


<details>
  <summary>Details</summary>
Motivation: 现有T2I方法在以下方面存在局限性：(i) 难以忠实地保留多个对象；(ii) 难以与提示词保持语义对齐；(iii) 无法对场景构成提供明确控制。

Method: 引入N通道ControlNet，实现：(i) 对象保留（与尺寸/位置无关，保留颜色/细节，消除伪影）；(ii) 高分辨率、语义一致的背景（准确的阴影/光照，遵循提示词）；(iii) 用户对背景布局/光照的显式控制。框架包含对象保留、背景引导、光照一致性和高频叠加模块。构建了包含24万张自然图像和1.8万张3D渲染合成图像的新基准数据集。

Result: 经验结果表明，该方法达到最先进水平，显著提高了特征空间保真度（FID 15.26）和语义对齐（CLIP-S 32.85），并保持了有竞争力的美学质量。用户研究显示，在提示词对齐、真实感、AI伪影和自然美学方面，相比现有工作分别提高了约25%、19%、13%和14%。

Conclusion: “Preserve Anything”方法有效解决了T2I生成中对象保留和语义一致性的关键局限，通过新颖的架构和数据集，在性能上实现了显著提升，并提供了更强的用户控制能力。

Abstract: We introduce \textit{Preserve Anything}, a novel method for controlled image
synthesis that addresses key limitations in object preservation and semantic
consistency in text-to-image (T2I) generation. Existing approaches often fail
(i) to preserve multiple objects with fidelity, (ii) maintain semantic
alignment with prompts, or (iii) provide explicit control over scene
composition. To overcome these challenges, the proposed method employs an
N-channel ControlNet that integrates (i) object preservation with size and
placement agnosticism, color and detail retention, and artifact elimination,
(ii) high-resolution, semantically consistent backgrounds with accurate
shadows, lighting, and prompt adherence, and (iii) explicit user control over
background layouts and lighting conditions. Key components of our framework
include object preservation and background guidance modules, enforcing lighting
consistency and a high-frequency overlay module to retain fine details while
mitigating unwanted artifacts. We introduce a benchmark dataset consisting of
240K natural images filtered for aesthetic quality and 18K 3D-rendered
synthetic images with metadata such as lighting, camera angles, and object
relationships. This dataset addresses the deficiencies of existing benchmarks
and allows a complete evaluation. Empirical results demonstrate that our method
achieves state-of-the-art performance, significantly improving feature-space
fidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining
competitive aesthetic quality. We also conducted a user study to demonstrate
the efficacy of the proposed work on unseen benchmark and observed a remarkable
improvement of $\sim25\%$, $\sim19\%$, $\sim13\%$, and $\sim14\%$ in terms of
prompt alignment, photorealism, the presence of AI artifacts, and natural
aesthetics over existing works.

</details>


### [74] [Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset](https://arxiv.org/abs/2506.22554)
*Vasu Agrawal,Akinniyi Akinyemi,Kathryn Alvero,Morteza Behrooz,Julia Buffalini,Fabio Maria Carlucci,Joy Chen,Junming Chen,Zhang Chen,Shiyang Cheng,Praveen Chowdary,Joe Chuang,Antony D'Avirro,Jon Daly,Ning Dong,Mark Duppenthaler,Cynthia Gao,Jeff Girard,Martin Gleize,Sahir Gomez,Hongyu Gong,Srivathsan Govindarajan,Brandon Han,Sen He,Denise Hernandez,Yordan Hristov,Rongjie Huang,Hirofumi Inaguma,Somya Jain,Raj Janardhan,Qingyao Jia,Christopher Klaiber,Dejan Kovachev,Moneish Kumar,Hang Li,Yilei Li,Pavel Litvin,Wei Liu,Guangyao Ma,Jing Ma,Martin Ma,Xutai Ma,Lucas Mantovani,Sagar Miglani,Sreyas Mohan,Louis-Philippe Morency,Evonne Ng,Kam-Woh Ng,Tu Anh Nguyen,Amia Oberai,Benjamin Peloquin,Juan Pino,Jovan Popovic,Omid Poursaeed,Fabian Prada,Alice Rakotoarison,Alexander Richard,Christophe Ropers,Safiyyah Saleem,Vasu Sharma,Alex Shcherbyna,Jia Shen,Jie Shen,Anastasis Stathopoulos,Anna Sun,Paden Tomasello,Tuan Tran,Arina Turkatenko,Bo Wan,Chao Wang,Jeff Wang,Mary Williamson,Carleigh Wood,Tao Xiang,Yilin Yang,Julien Yao,Chen Zhang,Jiemin Zhang,Xinyue Zhang,Jason Zheng,Pavlo Zhyzheria,Jan Zikes,Michael Zollhoefer*

Main category: cs.CV

TL;DR: 本研究引入了一个大规模人机交互数据集（Seamless Interaction Dataset），并开发了一系列模型，旨在生成符合人类语音的对话式身体姿态和面部表情，以推动社交智能AI的发展，实现更自然的虚拟代理和人机互动。


<details>
  <summary>Details</summary>
Motivation: 为了开发具有社交智能的AI技术，理解和生成二元行为动态至关重要，因为人类交流涉及复杂的言语和非言语信号，对于传达意义和实现人际目标至关重要。

Method: 引入了Seamless Interaction Dataset，一个包含4000多小时面对面互动视频的大规模数据集。在此基础上，开发了一套模型，能够根据人类语音生成二元动作手势和面部表情，并可输入对话者的语音和视觉行为。模型还包含了与LLM语音和2D/3D渲染集成的变体，以及可控制情感响应、表达水平和语义相关手势的变体。

Result: 该数据集赋能了理解二元具身动态的AI技术发展，为虚拟代理、远程临场体验和多模态内容分析工具带来突破。开发的模型使得交互式虚拟代理更接近现实，并展示了实现更直观、响应更灵敏的人机交互的潜力。

Conclusion: 本研究通过构建大规模数据集和开发创新的二元动作生成模型，显著推动了社交智能AI的进展，为未来更自然、更具响应性的人机交互奠定了基础。

Abstract: Human communication involves a complex interplay of verbal and nonverbal
signals, essential for conveying meaning and achieving interpersonal goals. To
develop socially intelligent AI technologies, it is crucial to develop models
that can both comprehend and generate dyadic behavioral dynamics. To this end,
we introduce the Seamless Interaction Dataset, a large-scale collection of over
4,000 hours of face-to-face interaction footage from over 4,000 participants in
diverse contexts. This dataset enables the development of AI technologies that
understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,
telepresence experiences, and multimodal content analysis tools. We also
develop a suite of models that utilize the dataset to generate dyadic motion
gestures and facial expressions aligned with human speech. These models can
take as input both the speech and visual behavior of their interlocutors. We
present a variant with speech from an LLM model and integrations with 2D and 3D
rendering methods, bringing us closer to interactive virtual agents.
Additionally, we describe controllable variants of our motion models that can
adapt emotional responses and expressivity levels, as well as generating more
semantically-relevant gestures. Finally, we discuss methods for assessing the
quality of these dyadic motion models, which are demonstrating the potential
for more intuitive and responsive human-AI interactions.

</details>


### [75] [Recomposed realities: animating still images via patch clustering and randomness](https://arxiv.org/abs/2506.22556)
*Markus Juvonen,Samuli Siltanen*

Main category: cs.CV

TL;DR: 一种基于图像块的图像重建和动画方法，旨在使静态图像通过运动变得生动。


<details>
  <summary>Details</summary>
Motivation: 旨在通过利用现有图像数据，使静态图像通过运动“活化”，并强调重新诠释而非简单复制，允许源域和目标域在概念上不同但共享局部结构。

Method: 使用K-means聚类对精选数据集中的图像块进行分组，然后通过匹配并从这些聚类中随机采样来重建新的目标图像。

Result: 成功实现了静态图像的动画化，并且该方法支持对现有图像数据的重新诠释，允许源和目标领域在概念上有所差异，同时保留局部结构相似性。

Conclusion: 该方法提供了一种创新的图像重建和动画方案，能够将静态图像通过运动变得生动，其优势在于强调重新诠释和局部结构共享，从而实现了概念上不同的领域间的转化。

Abstract: We present a patch-based image reconstruction and animation method that uses
existing image data to bring still images to life through motion. Image patches
from curated datasets are grouped using k-means clustering and a new target
image is reconstructed by matching and randomly sampling from these clusters.
This approach emphasizes reinterpretation over replication, allowing the source
and target domains to differ conceptually while sharing local structures.

</details>


### [76] [Improving Token-based Object Detection with Video](https://arxiv.org/abs/2506.22562)
*Abhineet Singh,Nilanjan Ray*

Main category: cs.CV

TL;DR: 本文将Pix2Seq扩展到视频领域，提出一种端到端视频目标检测新方法。通过将视频对象表示为可变长度的离散令牌序列并直接输出为3D轨迹，解决了传统方法的痛点，并在有限计算资源下取得了具有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 改进Pix2Seq以适应视频目标检测，解决现有视频检测器在训练过程中损失稀疏性、基于启发式后处理的局限性，以及传统2D框链接构建视频对象的低效和复杂性问题。

Method: 将对象表示为可变长度的离散令牌序列，无需训练过程中注入定位线索，从而避免了边界框采样并解决了损失稀疏性和启发式后处理问题。将视频对象概念化并直接输出为完全整合且不可分割的3D边界框或轨迹（tracklets），而非通过链接图像特定的2D框。该方法可通过增加输入视频子序列的长度实现计算资源的灵活扩展。

Result: 与基线Pix2Seq静态检测器相比，在多个数据集上表现出持续改进。在UA-DETRAC数据集上与现有视频检测器相比，即使在计算资源受限的情况下，也表现出与当前最先进水平相当的竞争力。研究表明性能受限于计算资源瓶颈。

Conclusion: 该论文成功将Pix2Seq扩展至视频领域，提出了一种新颖且有效的端到端视频目标检测框架。通过创新的对象表示和3D轨迹输出方式，解决了传统方法的关键缺陷。尽管存在计算瓶颈，该方法仍显示出强大的潜力和竞争力，并能推广到多目标跟踪任务。

Abstract: This paper improves upon the Pix2Seq object detector by extending it for
videos. In the process, it introduces a new way to perform end-to-end video
object detection that improves upon existing video detectors in two key ways.
First, by representing objects as variable-length sequences of discrete tokens,
we can succinctly represent widely varying numbers of video objects, with
diverse shapes and locations, without having to inject any localization cues in
the training process. This eliminates the need to sample the space of all
possible boxes that constrains conventional detectors and thus solves the dual
problems of loss sparsity during training and heuristics-based postprocessing
during inference. Second, it conceptualizes and outputs the video objects as
fully integrated and indivisible 3D boxes or tracklets instead of generating
image-specific 2D boxes and linking these boxes together to construct the video
object, as done in most conventional detectors. This allows it to scale
effortlessly with available computational resources by simply increasing the
length of the video subsequence that the network takes as input, even
generalizing to multi-object tracking if the subsequence can span the entire
video. We compare our video detector with the baseline Pix2Seq static detector
on several datasets and demonstrate consistent improvement, although with
strong signs of being bottlenecked by our limited computational resources. We
also compare it with several video detectors on UA-DETRAC to show that it is
competitive with the current state of the art even with the computational
bottleneck. We make our code and models publicly available.

</details>


### [77] [Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation](https://arxiv.org/abs/2506.22567)
*Shansong Wang,Zhecheng Jin,Mingzhe Hu,Mojtaba Safari,Feng Zhao,Chih-Wei Chang,Richard LJ Qiu,Justin Roper,David S. Yu,Xiaofeng Yang*

Main category: cs.CV

TL;DR: MMKD-CLIP通过多教师知识蒸馏，在数据受限的生物医学领域构建了强大的基础模型，性能超越现有模型，并在多任务中展现出卓越的泛化性。


<details>
  <summary>Details</summary>
Motivation: 将CLIP模型成功应用于生物医学领域面临挑战，主要原因在于缺乏大规模生物医学图文语料、图像模态多样性以及数据标准碎片化，这阻碍了从头开始训练统一且泛化的生物医学基础模型。

Method: 本文提出MMKD-CLIP模型，通过多医学CLIP知识蒸馏开发。它不依赖海量原始数据，而是从9个已预训练在数百万生物医学图文对上的领域特定或通用生物医学CLIP模型中蒸馏知识。训练分为两阶段：首先在290多万生物医学图文对（来自26种图像模态）上进行CLIP风格的预训练；随后使用从教师模型中提取的1920多万特征对进行特征级蒸馏。

Result: MMKD-CLIP在58个多样化的生物医学数据集（涵盖1080多万生物医学图像和9种图像模态）上进行了评估，涉及六种核心任务类型。结果显示，MMKD-CLIP始终优于所有教师模型，并在图像域和任务设置中表现出卓越的鲁棒性和泛化能力。

Conclusion: 研究强调，多教师知识蒸馏是一种可扩展且有效的范式，可以在真实世界数据可用性的实际约束下，构建高性能的生物医学基础模型。

Abstract: CLIP models pretrained on natural images with billion-scale image-text pairs
have demonstrated impressive capabilities in zero-shot classification,
cross-modal retrieval, and open-ended visual answering. However, transferring
this success to biomedicine is hindered by the scarcity of large-scale
biomedical image-text corpora, the heterogeneity of image modalities, and
fragmented data standards across institutions. These limitations hinder the
development of a unified and generalizable biomedical foundation model trained
from scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical
foundation model developed via Multiple Medical CLIP Knowledge Distillation.
Rather than relying on billion-scale raw data, MMKD-CLIP distills knowledge
from nine state-of-the-art domain-specific or generalist biomedical CLIP
models, each pretrained on millions of biomedical image-text pairs. Our
two-stage training pipeline first performs CLIP-style pretraining on over 2.9
million biomedical image-text pairs from 26 image modalities, followed by
feature-level distillation using over 19.2 million feature pairs extracted from
teacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,
encompassing over 10.8 million biomedical images across nine image modalities.
The evaluation spans six core task types: zero-shot classification, linear
probing, cross-modal retrieval, visual question answering, survival prediction,
and cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models
while demonstrating remarkable robustness and generalization across image
domains and task settings. These results underscore that multi-teacher
knowledge distillation is a scalable and effective paradigm for building
high-performing biomedical foundation models under the practical constraints of
real-world data availability.

</details>


### [78] [Dual Atrous Separable Convolution for Improving Agricultural Semantic Segmentation](https://arxiv.org/abs/2506.22570)
*Chee Mei Ling,Thangarajah Akilan,Aparna Ravinda Phalke*

Main category: cs.CV

TL;DR: 本研究提出一种基于DeepLabV3的高效农田图像语义分割方法，通过引入新型DAS Conv模块和跳跃连接，在保持低计算复杂度的同时，实现了与复杂SOTA模型相当的性能，尤其在效率上显著提升。


<details>
  <summary>Details</summary>
Motivation: 农业图像语义分割是现代农业的关键组成部分，有助于精确分析视觉数据，从而改善作物管理、优化资源利用并提高整体生产力。本研究旨在为精准农业提供高效的图像分割方法，重点在于准确描绘农田异常，以支持明智决策和主动干预。

Method: 该研究将新型Dual Atrous Separable Convolution (DAS Conv)模块集成到DeepLabV3分割框架中。DAS Conv模块旨在优化膨胀率和填充大小之间的平衡，从而在不牺牲效率的情况下增强模型性能。此外，研究还从编码器的最佳阶段向解码器引入了策略性跳跃连接，以增强模型捕获细粒度空间特征的能力。

Result: 尽管计算复杂度较低，所提出的模型在Agriculture Vision基准数据集上性能优于其基线模型，并实现了与高度复杂的基于Transformer的先进（SOTA）模型相当的性能。在模型复杂度与性能的权衡方面，相较于SOTA模型，其效率提升超过66%。

Conclusion: 本研究为遥感应用中的语义分割提供了一种高效且有效的解决方案，提出了一个计算量轻巧但能在农业图像中实现高质量性能的模型。

Abstract: Agricultural image semantic segmentation is a pivotal component of modern
agriculture, facilitating accurate visual data analysis to improve crop
management, optimize resource utilization, and boost overall productivity. This
study proposes an efficient image segmentation method for precision
agriculture, focusing on accurately delineating farmland anomalies to support
informed decision-making and proactive interventions. A novel Dual Atrous
Separable Convolution (DAS Conv) module is integrated within the
DeepLabV3-based segmentation framework. The DAS Conv module is meticulously
designed to achieve an optimal balance between dilation rates and padding size,
thereby enhancing model performance without compromising efficiency. The study
also incorporates a strategic skip connection from an optimal stage in the
encoder to the decoder to bolster the model's capacity to capture fine-grained
spatial features. Despite its lower computational complexity, the proposed
model outperforms its baseline and achieves performance comparable to highly
complex transformer-based state-of-the-art (SOTA) models on the Agriculture
Vision benchmark dataset. It achieves more than 66% improvement in efficiency
when considering the trade-off between model complexity and performance,
compared to the SOTA model. This study highlights an efficient and effective
solution for improving semantic segmentation in remote sensing applications,
offering a computationally lightweight model capable of high-quality
performance in agricultural imagery.

</details>


### [79] [LIGHT: Multi-Modal Text Linking on Historical Maps](https://arxiv.org/abs/2506.22589)
*Yijun Lin,Rhett Olson,Junhan Wu,Yao-Yi Chiang,Jerod Weinman*

Main category: cs.CV

TL;DR: 本文提出LIGHT，一种多模态方法，整合语言、图像和几何特征，旨在有效链接历史地图上的文本片段，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 历史地图上的文本信息对多领域研究至关重要。然而，地图文本的排版（方向、阅读顺序、形状、位置）高度可变，导致现有文本识别方法难以有效“链接”文本片段（如多词地名）。现有版面分析方法主要依赖语言特征，忽略了对地图文本至关重要的几何信息。

Method: 提出LIGHT，一种新颖的多模态方法，融合了语言、图像和几何特征以链接历史地图上的文本。LIGHT包含一个几何感知嵌入模块，该模块编码文本区域的多边形坐标以捕捉其形状和相对空间位置。LIGHT将这些几何信息与LayoutLMv3的视觉和语言令牌嵌入相结合，并利用跨模态信息通过双向学习策略直接预测每个文本实例的阅读顺序后继，以增强序列鲁棒性。

Result: 实验结果表明，LIGHT在ICDAR 2024/2025 MapText竞赛数据上优于现有方法。

Conclusion: 研究结果证明了多模态学习在历史地图文本链接任务中的有效性。

Abstract: Text on historical maps provides valuable information for studies in history,
economics, geography, and other related fields. Unlike structured or
semi-structured documents, text on maps varies significantly in orientation,
reading order, shape, and placement. Many modern methods can detect and
transcribe text regions, but they struggle to effectively ``link'' the
recognized text fragments, e.g., determining a multi-word place name. Existing
layout analysis methods model word relationships to improve text understanding
in structured documents, but they primarily rely on linguistic features and
neglect geometric information, which is essential for handling map text. To
address these challenges, we propose LIGHT, a novel multi-modal approach that
integrates linguistic, image, and geometric features for linking text on
historical maps. In particular, LIGHT includes a geometry-aware embedding
module that encodes the polygonal coordinates of text regions to capture
polygon shapes and their relative spatial positions on an image. LIGHT unifies
this geometric information with the visual and linguistic token embeddings from
LayoutLMv3, a pretrained layout analysis model. LIGHT uses the cross-modal
information to predict the reading-order successor of each text instance
directly with a bi-directional learning strategy that enhances sequence
robustness. Experimental results show that LIGHT outperforms existing methods
on the ICDAR 2024/2025 MapText Competition data, demonstrating the
effectiveness of multi-modal learning for historical map text linking.

</details>


### [80] [BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data](https://arxiv.org/abs/2506.22591)
*Arunkumar Kannan,Martin A. Lindquist,Brian Caffo*

Main category: cs.CV

TL;DR: BrainMT是一种新型混合框架，结合Mamba和Transformer，有效处理fMRI数据中的长程时空依赖，在表型预测任务上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的fMRI表型预测方法（如CNN和Transformer）难以捕捉fMRI数据中复杂的长程时空依赖关系，限制了其性能。

Method: 引入BrainMT，一个两阶段混合框架：首先使用双向Mamba模块以时间优先扫描方式捕获全局时间交互；然后利用Transformer模块通过自注意力建模Mamba处理后的深度特征中的全局空间关系。

Result: 在UKBioBank和Human Connectome Project两大公共数据集上进行的大量实验表明，BrainMT在分类（性别预测）和回归（认知智能预测）任务上均超越现有方法，取得了显著的SOTA性能。

Conclusion: BrainMT通过有效整合Mamba和Transformer架构，成功解决了fMRI数据中长程时空依赖建模的挑战，显著提升了从fMRI脑容量预测表型测量的准确性。

Abstract: Recent advances in deep learning have made it possible to predict phenotypic
measures directly from functional magnetic resonance imaging (fMRI) brain
volumes, sparking significant interest in the neuroimaging community. However,
existing approaches, primarily based on convolutional neural networks or
transformer architectures, often struggle to model the complex relationships
inherent in fMRI data, limited by their inability to capture long-range spatial
and temporal dependencies. To overcome these shortcomings, we introduce
BrainMT, a novel hybrid framework designed to efficiently learn and integrate
long-range spatiotemporal attributes in fMRI data. Our framework operates in
two stages: (1) a bidirectional Mamba block with a temporal-first scanning
mechanism to capture global temporal interactions in a computationally
efficient manner; and (2) a transformer block leveraging self-attention to
model global spatial relationships across the deep features processed by the
Mamba block. Extensive experiments on two large-scale public datasets,
UKBioBank and the Human Connectome Project, demonstrate that BrainMT achieves
state-of-the-art performance on both classification (sex prediction) and
regression (cognitive intelligence prediction) tasks, outperforming existing
methods by a significant margin. Our code and implementation details will be
made publicly available at this
https://github.com/arunkumar-kannan/BrainMT-fMRI

</details>


### [81] [Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning](https://arxiv.org/abs/2506.22624)
*Zuyao You,Zuxuan Wu*

Main category: cs.CV

TL;DR: Seg-R1是一个利用强化学习（RL）提升大型多模态模型（LMM）像素级理解和推理能力的初步探索。它通过RL生成提示指导SAM2进行前景分割，并在纯RL训练下在多个分割任务上取得卓越性能，展现出强大的开放世界泛化能力和零样本表现，超越了传统监督模型。


<details>
  <summary>Details</summary>
Motivation: 旨在探索利用强化学习（RL）增强大型多模态模型（LMMs）的像素级理解和推理能力。

Method: 引入Seg-R1，一个基于RL的模型。该方法使LMM能够以“下一个token”的方式生成点和边界框提示，进而引导SAM2生成分割掩码。模型将Group Relative Policy Optimization (GRPO)引入分割领域，并通过精心设计的训练策略赋予LMM像素级理解能力。训练仅使用前景分割图像-掩码对，无文本监督。

Result: Seg-R1在纯RL训练下取得了显著性能：在COD10K数据集上S-measure达到0.873。更重要的是，纯RL训练展现出强大的开放世界泛化能力，在零样本指代分割（RefCOCOg test）上达到71.4 cIoU，在零样本推理分割（ReasonSeg test）上达到56.7 gIoU，性能超越了在这些数据集上完全监督训练的模型。

Conclusion: 研究表明，强化学习能够有效提升LMM的像素级理解和推理能力，并在仅通过前景分割数据训练的情况下，实现卓越的开放世界泛化和零样本性能，预示着RL在复杂视觉理解任务中的巨大潜力。

Abstract: We present Seg-R1, a preliminary exploration of using reinforcement learning
(RL) to enhance the pixel-level understanding and reasoning capabilities of
large multimodal models (LMMs). Starting with foreground segmentation tasks,
specifically camouflaged object detection (COD) and salient object detection
(SOD), our approach enables the LMM to generate point and bounding box prompts
in the next-token fashion, which are then used to guide SAM2 in producing
segmentation masks. We introduce Group Relative Policy Optimization (GRPO) into
the segmentation domain, equipping the LMM with pixel-level comprehension
through a carefully designed training strategy. Notably, Seg-R1 achieves
remarkable performance with purely RL-based training, achieving .873 S-measure
on COD10K without complex model modification. Moreover, we found that pure RL
training demonstrates strong open-world generalization. Despite being trained
solely on foreground segmentation image-mask pairs without text supervision,
Seg-R1 achieves impressive zero-shot performance on referring segmentation and
reasoning segmentation tasks, with 71.4 cIoU on RefCOCOg test and 56.7 gIoU on
ReasonSeg test, outperforming models fully supervised on these datasets.

</details>


### [82] [ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models](https://arxiv.org/abs/2506.22636)
*Sotirios Panagiotis Chytas,Miso Choi,Hyunwoo J. Kim,Vikas Singh*

Main category: cs.CV

TL;DR: 视觉语言模型（VLM）常因对视觉输入的“记忆衰退”而产生幻觉。本文提出ReCo模块，一个基于几何代数和关系组合的轻量级可训练模块，能有效减轻多种VLM的幻觉现象，提升性能，并能与其他现有方法结合使用。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）尽管能力出色，但普遍存在幻觉问题，即生成与视觉输入不符或矛盾的内容。这种行为主要归因于模型对语言的过度依赖，导致在生成过程中对视觉输入的“记忆衰退效应”。因此，研究如何控制和减轻VLM的幻觉行为是本研究的核心动机。

Method: 本研究基于几何代数和关系组合的理念，设计并提出了一个名为ReCo的小型、可训练模块。该模块可直接附加在任何现有VLM之上，无需对VLM自身进行其他修改。

Result: 实验结果表明，ReCo模块能够有效减轻InstructBLIP、LlaVA、MiniGPT4等三种主流VLM的“记忆衰退效应”，并在多个基准测试中显著提升了性能。此外，ReCo模块还可以与许多其他旨在减少幻觉的方法结合使用，且能进一步提高它们的综合效果。

Conclusion: 通过引入ReCo模块，可以有效控制VLM的幻觉问题，增强其生成内容与视觉输入的一致性。ReCo模块的轻量级和良好的兼容性使其成为提升VLM可靠性的一个有前景且实用的解决方案。

Abstract: Vision Language Models (VLMs) show impressive capabilities in integrating and
reasoning with both visual and language data. But these models make mistakes. A
common finding -- similar to LLMs -- is their tendency to hallucinate, i.e.,
generate plausible sounding text which is not grounded in the visual input, or
at worst, is contradictory. A growing consensus attributes this behavior to an
over-reliance on language -- especially as the generation progresses, the model
suffers from a ``fading memory effect'' with respect to the provided visual
input. We study mechanisms by which this behavior can be controlled.
Specifically, using ideas from geometric algebra and relational compositions,
we propose the addition of a small, trainable module (named ReCo) on top of any
VLM -- no other modification is needed. We show that such a lightweight module
is able to mitigate the fading memory effect on three of the most widely used
VLMs (InstructBLIP, LlaVA, MiniGPT4), where we see performance improvements on
multiple benchmarks. Additionally, we show that our module can be combined with
many of the other approaches for reducing hallucination where we achieve
improved results for each one.

</details>


### [83] [CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation](https://arxiv.org/abs/2506.22637)
*Haoxuan Wang,Zhenghao Zhao,Junyi Wu,Yuzhang Shang,Gaowen Liu,Yan Yan*

Main category: cs.CV

TL;DR: 扩散模型在数据集蒸馏中潜力大，但现有方法存在评估和条件不一致性。本文提出两阶段框架CaO$_2$以解决这些问题，并在ImageNet上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的数据集蒸馏方法忽视评估过程，并存在目标不一致性（蒸馏与评估目标不符）和条件不一致性（生成图像与条件不匹配）两大关键问题。

Method: 提出CaO$_2$，一个两阶段的基于扩散的框架，旨在使蒸馏过程与评估目标对齐。第一阶段采用基于概率的样本选择流程，第二阶段优化潜在表示以提高条件似然。

Result: CaO$_2$在ImageNet及其子集上实现了最先进（SOTA）的性能，平均精度超过现有最佳基线2.3%。

Conclusion: CaO$_2$有效解决了现有扩散模型数据集蒸馏中的不一致性问题，显著提升了性能，并在ImageNet等数据集上达到了新的SOTA。

Abstract: The recent introduction of diffusion models in dataset distillation has shown
promising potential in creating compact surrogate datasets for large,
high-resolution target datasets, offering improved efficiency and performance
over traditional bi-level/uni-level optimization methods. However, current
diffusion-based dataset distillation approaches overlook the evaluation process
and exhibit two critical inconsistencies in the distillation process: (1)
Objective Inconsistency, where the distillation process diverges from the
evaluation objective, and (2) Condition Inconsistency, leading to mismatches
between generated images and their corresponding conditions. To resolve these
issues, we introduce Condition-aware Optimization with Objective-guided
Sampling (CaO$_2$), a two-stage diffusion-based framework that aligns the
distillation process with the evaluation objective. The first stage employs a
probability-informed sample selection pipeline, while the second stage refines
the corresponding latent representations to improve conditional likelihood.
CaO$_2$ achieves state-of-the-art performance on ImageNet and its subsets,
surpassing the best-performing baselines by an average of 2.3% accuracy.

</details>


### [84] [3D Shape Generation: A Survey](https://arxiv.org/abs/2506.22678)
*Nicolas Caytuiro,Ivan Sipiran*

Main category: cs.CV

TL;DR: 本综述全面概述了深度学习驱动下的3D形状生成领域，围绕形状表示、生成模型和评估协议进行组织，并探讨了开放挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 深度学习的最新进展显著改变了3D形状生成领域，实现了复杂、多样且有语义的3D对象合成，因此需要对该领域的最新技术进行系统性的梳理和分析。

Method: 本综述围绕三个核心组件组织：首先分类3D形状表示（显式、隐式、混合），其次回顾各种生成建模方法（侧重前馈架构），并总结常用数据集和评估指标。最后，识别开放挑战并规划未来研究方向。

Result: 通过分析，本综述呈现了3D表示的结构特性、优缺点，总结了多种生成方法，归纳了评估标准，并明确指出了当前领域面临的挑战以及未来在可控、高效、高质量3D形状生成方面的发展方向。

Conclusion: 本综述为研究人员和实践者提供了对3D形状生成这一快速发展领域的结构化和深入理解，旨在成为该领域的重要参考资料，并为未来的研究指明了方向。

Abstract: Recent advances in deep learning have significantly transformed the field of
3D shape generation, enabling the synthesis of complex, diverse, and
semantically meaningful 3D objects. This survey provides a comprehensive
overview of the current state of the art in 3D shape generation, organizing the
discussion around three core components: shape representations, generative
modeling approaches, and evaluation protocols. We begin by categorizing 3D
representations into explicit, implicit, and hybrid setups, highlighting their
structural properties, advantages, and limitations. Next, we review a wide
range of generation methods, focusing on feedforward architectures. We further
summarize commonly used datasets and evaluation metrics that assess fidelity,
diversity, and realism of generated shapes. Finally, we identify open
challenges and outline future research directions that could drive progress in
controllable, efficient, and high-quality 3D shape generation. This survey aims
to serve as a valuable reference for researchers and practitioners seeking a
structured and in-depth understanding of this rapidly evolving field.

</details>


### [85] [LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning](https://arxiv.org/abs/2506.22710)
*Jiang Yuan,JI Ma,Bo Wang,Guanzhou Ke,Weiming Hu*

Main category: cs.CV

TL;DR: 现有盲超分辨率方法忽视隐式退化表示（IDR）的可区分性且过于复杂。本文提出LightBSR，一个基于知识蒸馏和对比学习的轻量级模型，通过优化IDR的可区分性实现卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于隐式退化估计的盲超分辨率（IDE-BSR）方法忽略了隐式退化表示（IDR）的可区分性，并过度复杂化了适应过程，导致模型参数和计算量显著增加。因此，需要开发一个强大且轻量级的BSR模型，通过优化IDR的可区分性来解决这些问题。

Method: 采用基于知识蒸馏的学习框架。在教师阶段，引入退化先验约束的对比学习技术，以增强模型对不同退化类型的区分能力。随后，利用特征对齐技术将教师学习到的退化相关知识迁移给学生，用于实际推理。

Result: 所提出的LightBSR模型在各种盲超分辨率任务中以最小的复杂度实现了卓越的性能。实验证明了IDR可区分性驱动的BSR模型设计的有效性。

Conclusion: 通过关注并优化隐式退化表示（IDR）的可区分性，可以设计出强大且轻量级的盲超分辨率模型，其性能优于现有复杂方法。LightBSR模型通过以更低的计算成本实现卓越结果，验证了这一概念。

Abstract: Implicit degradation estimation-based blind super-resolution (IDE-BSR) hinges
on extracting the implicit degradation representation (IDR) of the LR image and
adapting it to LR image features to guide HR detail restoration. Although
IDE-BSR has shown potential in dealing with noise interference and complex
degradations, existing methods ignore the importance of IDR discriminability
for BSR and instead over-complicate the adaptation process to improve effect,
resulting in a significant increase in the model's parameters and computations.
In this paper, we focus on the discriminability optimization of IDR and propose
a new powerful and lightweight BSR model termed LightBSR. Specifically, we
employ a knowledge distillation-based learning framework. We first introduce a
well-designed degradation-prior-constrained contrastive learning technique
during teacher stage to make the model more focused on distinguishing different
degradation types. Then we utilize a feature alignment technique to transfer
the degradation-related knowledge acquired by the teacher to the student for
practical inferencing. Extensive experiments demonstrate the effectiveness of
IDR discriminability-driven BSR model design. The proposed LightBSR can achieve
outstanding performance with minimal complexity across a range of blind SR
tasks. Our code is accessible at: https://github.com/MJ-NCEPU/LightBSR.

</details>


### [86] [Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians](https://arxiv.org/abs/2506.22718)
*Jun-Jee Chao,Qingyuan Jiang,Volkan Isler*

Main category: cs.CV

TL;DR: 本文提出一种新方法，通过将铰接物体建模为3D高斯块集合，解决了在点云不具有固定对应关系（如存在严重遮挡）的情况下，联合进行部件分割和运动估计的难题。该方法在实验中表现出比现有方法更强的鲁棒性和卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 铰接物体的部件分割和运动估计是关键问题。现有方法依赖于固定点对应关系，但在真实场景中，点云可能由任意采样产生，存在严重遮挡或来自异步多传感器，导致点对应关系不可靠。因此，需要一种不依赖固定点对应关系的新方法来解决这些挑战。

Method: 该方法基于一种紧凑而有效的表示：将铰接物体建模为一系列简单的3D高斯块。这些高斯块被参数化为时间相关的旋转、平移和尺度，并在所有时间步长中共享。通过在观测点和高斯块之间建立对应关系来实现部件分割，而点的运动变换则通过跟踪其分配的高斯块的姿态来获得，即使该点未被观测到。

Result: ['该方法性能优于仅依赖于寻找点对应关系的现有方法。', '在考虑视点遮挡的扩展数据集上，该方法对缺失点表现出更强的鲁棒性，即使在某些时间步长中部分部件完全被遮挡。', '在带有遮挡的点云上，部件分割性能比现有最先进方法高出13%。']

Conclusion: 该研究成功提出了一种有效的方法，通过创新的3D高斯块表示，解决了铰接物体在复杂点云（如存在严重遮挡和缺失点）场景下的部件分割和运动估计问题。实验结果表明，该方法在鲁棒性和性能上均超越了现有依赖点对应关系的方法，尤其是在处理遮挡数据方面表现出色。

Abstract: Part segmentation and motion estimation are two fundamental problems for
articulated object motion analysis. In this paper, we present a method to solve
these two problems jointly from a sequence of observed point clouds of a single
articulated object. The main challenge in our problem setting is that the point
clouds are not assumed to be generated by a fixed set of moving points.
Instead, each point cloud in the sequence could be an arbitrary sampling of the
object surface at that particular time step. Such scenarios occur when the
object undergoes major occlusions, or if the dataset is collected using
measurements from multiple sensors asynchronously. In these scenarios, methods
that rely on tracking point correspondences are not appropriate. We present an
alternative approach based on a compact but effective representation where we
represent the object as a collection of simple building blocks modeled as 3D
Gaussians. We parameterize the Gaussians with time-dependent rotations,
translations, and scales that are shared across all time steps. With our
representation, part segmentation can be achieved by building correspondences
between the observed points and the Gaussians. Moreover, the transformation of
each point across time can be obtained by following the poses of the assigned
Gaussian (even when the point is not observed). Experiments show that our
method outperforms existing methods that solely rely on finding point
correspondences. Additionally, we extend existing datasets to emulate
real-world scenarios by considering viewpoint occlusions. We further
demonstrate that our method is more robust to missing points as compared to
existing approaches on these challenging datasets, even when some parts are
completely occluded in some time-steps. Notably, our part segmentation
performance outperforms the state-of-the-art method by 13% on point clouds with
occlusions.

</details>


### [87] [Deterministic Object Pose Confidence Region Estimation](https://arxiv.org/abs/2506.22720)
*Jinghao Wang,Zhang Li,Zi Wang,Banglei Guan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出一种确定性且高效的6D姿态置信区域估计算法，通过结合归纳共形预测和隐函数定理，解决了现有采样方法速度慢、区域过大的问题，并显著缩小了置信区域。


<details>
  <summary>Details</summary>
Motivation: 6D姿态置信区域估计对于评估姿态可靠性至关重要，但现有基于采样的方法存在严重限制：1) 采样速度随样本数量增加而显著下降；2) 派生的置信区域通常过大，阻碍了实际部署。

Method: 提出一种确定性且高效的方法。首先，使用归纳共形预测校准确定性回归的高斯关键点分布，得到2D关键点置信区域。然后，利用隐函数定理将这些2D关键点置信区域直接传播到6D姿态置信区域。该方法避免了采样和集成带来的低效和区域膨胀问题。

Result: 实验结果表明，该方法在LineMOD Occlusion和SPEED数据集上实现了更高的姿态估计精度和更短的计算时间。在相同覆盖率下，其置信区域体积显著减小，旋转部分最多减少99.9%，平移部分最多减少99.8%。

Conclusion: 所提出的确定性方法有效克服了现有采样方法的局限性，能够提供紧凑的6D姿态置信区域，以用户定义的置信水平覆盖真实姿态，显著提高了姿态可靠性评估的效率和准确性。

Abstract: 6D pose confidence region estimation has emerged as a critical direction,
aiming to perform uncertainty quantification for assessing the reliability of
estimated poses. However, current sampling-based approach suffers from critical
limitations that severely impede their practical deployment: 1) the sampling
speed significantly decreases as the number of samples increases. 2) the
derived confidence regions are often excessively large. To address these
challenges, we propose a deterministic and efficient method for estimating pose
confidence regions. Our approach uses inductive conformal prediction to
calibrate the deterministically regressed Gaussian keypoint distributions into
2D keypoint confidence regions. We then leverage the implicit function theorem
to propagate these keypoint confidence regions directly into 6D pose confidence
regions. This method avoids the inefficiency and inflated region sizes
associated with sampling and ensembling. It provides compact confidence regions
that cover the ground-truth poses with a user-defined confidence level.
Experimental results on the LineMOD Occlusion and SPEED datasets show that our
method achieves higher pose estimation accuracy with reduced computational
time. For the same coverage rate, our method yields significantly smaller
confidence region volumes, reducing them by up to 99.9\% for rotations and
99.8\% for translations. The code will be available soon.

</details>


### [88] [XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge](https://arxiv.org/abs/2506.22726)
*Yu Zhang,Xi Zhang,Hualin zhou,Xinyuan Chen,Shang Gao,Hong Jia,Jianfei Yang,Yuankai Qi,Tao Gu*

Main category: cs.CV

TL;DR: 本文提出XTransfer，一种资源高效、模态无关的模型迁移方法，用于边缘系统上的人体感知，解决了传感器数据稀缺和资源受限的问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在边缘系统人体感知方面潜力巨大，但受限于传感器数据不足和边缘系统资源有限。现有预训练模型迁移方法常面临模态漂移、高资源需求、精度损失及适应性差等问题。

Method: XTransfer利用单个或多个预训练模型，通过以下两部分实现跨模态知识迁移：(i) 模型修复：仅用少量传感器数据安全修复预训练模型层中的模态漂移；(ii) 层重组：以逐层方式高效搜索并重组源模型中感兴趣的层，以创建紧凑模型。

Result: XTransfer在多种人体感知数据集和模态上实现了最先进的性能，同时显著降低了传感器数据采集、模型训练和边缘部署的成本。

Conclusion: XTransfer是一种创新的模型迁移方法，能有效应对边缘系统人体感知应用中数据稀缺和资源限制的挑战，并取得了优异的性能和效率。

Abstract: Deep learning for human sensing on edge systems offers significant
opportunities for smart applications. However, its training and development are
hindered by the limited availability of sensor data and resource constraints of
edge systems. Current methods that rely on transferring pre-trained models
often encounter issues such as modality shift and high resource demands,
resulting in substantial accuracy loss, resource overhead, and poor
adaptability across different sensing applications. In this paper, we propose
XTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic
model transfer. XTransfer freely leverages single or multiple pre-trained
models and transfers knowledge across different modalities by (i) model
repairing that safely repairs modality shift in pre-trained model layers with
only few sensor data, and (ii) layer recombining that efficiently searches and
recombines layers of interest from source models in a layer-wise manner to
create compact models. We benchmark various baselines across diverse human
sensing datasets spanning different modalities. Comprehensive results
demonstrate that XTransfer achieves state-of-the-art performance on human
sensing tasks while significantly reducing the costs of sensor data collection,
model training, and edge deployment.

</details>


### [89] [UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments](https://arxiv.org/abs/2506.22736)
*Dayong Su,Yafei Zhang,Huafeng Li,Jinxing Li,Yu Liu*

Main category: cs.CV

TL;DR: 针对医学图像融合中图像质量差和未对齐问题，本文提出UniFuse框架，通过整合降级感知提示学习、全方位统一特征表示和自适应LoRA协同网络，实现对齐、修复和融合的统一单阶段处理。


<details>
  <summary>Details</summary>
Motivation: 当前多模态医学图像融合方法通常假设源图像高质量且像素级完美对齐，但在处理未对齐或降级的医学图像时，其有效性会显著下降。

Method: 提出UniFuse通用融合框架，包含：1) 降级感知提示学习模块，实现多方向信息整合与对齐修复的联合优化；2) 全方位统一特征表示方案，利用Spatial Mamba编码特征并缓解模态差异；3) 通用特征修复与融合模块，引入基于LoRA的自适应LoRA协同网络（ALSN），实现对齐、修复和融合在单一框架内的单阶段处理。

Result: 在多个数据集上的实验结果表明，该方法比现有方法更有效，并具有显著优势。

Conclusion: UniFuse框架成功地将对齐、修复和融合统一到一个单一框架中，有效解决了传统方法在处理未对齐和降级医学图像时的局限性，并展现出卓越的性能。

Abstract: Current multimodal medical image fusion typically assumes that source images
are of high quality and perfectly aligned at the pixel level. Its effectiveness
heavily relies on these conditions and often deteriorates when handling
misaligned or degraded medical images. To address this, we propose UniFuse, a
general fusion framework. By embedding a degradation-aware prompt learning
module, UniFuse seamlessly integrates multi-directional information from input
images and correlates cross-modal alignment with restoration, enabling joint
optimization of both tasks within a unified framework. Additionally, we design
an Omni Unified Feature Representation scheme, which leverages Spatial Mamba to
encode multi-directional features and mitigate modality differences in feature
alignment. To enable simultaneous restoration and fusion within an All-in-One
configuration, we propose a Universal Feature Restoration & Fusion module,
incorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA
principles. By leveraging ALSN's adaptive feature representation along with
degradation-type guidance, we enable joint restoration and fusion within a
single-stage framework. Compared to staged approaches, UniFuse unifies
alignment, restoration, and fusion within a single framework. Experimental
results across multiple datasets demonstrate the method's effectiveness and
significant advantages over existing approaches.

</details>


### [90] [Deep Learning based Joint Geometry and Attribute Up-sampling for Large-Scale Colored Point Clouds](https://arxiv.org/abs/2506.22749)
*Yun Zhang,Feifan Chen,Na Li,Zhiwei Guo,Xu Wang,Fen Miao,Sam Kwong*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的联合几何和属性上采样方法（JGAU），用于生成大规模、更密集的彩色点云，并通过实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 彩色点云是实现逼真沉浸式3D应用的主流表示，但现有方法难以生成大规模且密度更高的彩色点云。

Method: 1. 构建并发布了大规模彩色点云上采样数据集SYSU-PCUD。2. 提出深度学习JGAU框架，联合上采样几何和属性，包括几何上采样网络和属性上采样网络（后者利用上采样辅助几何来建模属性的邻域相关性）。3. 提出两种粗糙属性上采样方法（GDWAI和DLAI），并引入属性增强模块以细化上采样属性。

Result: JGAU方法在4倍、8倍、12倍和16倍上采样率下分别达到了33.90 dB、32.10 dB、31.10 dB和30.39 dB的PSNR。与现有最先进方法相比，JGAU在这些上采样率下平均PSNR增益分别为2.32 dB、2.47 dB、2.28 dB和2.11 dB。

Conclusion: JGAU方法在彩色点云上采样方面表现出显著改进，能生成高质量的彩色点云，并优于现有最先进的方法。

Abstract: Colored point cloud, which includes geometry and attribute components, is a
mainstream representation enabling realistic and immersive 3D applications. To
generate large-scale and denser colored point clouds, we propose a deep
learning-based Joint Geometry and Attribute Up-sampling (JGAU) method that
learns to model both geometry and attribute patterns while leveraging spatial
attribute correlations. First, we establish and release a large-scale dataset
for colored point cloud up-sampling called SYSU-PCUD, containing 121
large-scale colored point clouds with diverse geometry and attribute
complexities across six categories and four sampling rates. Second, to improve
the quality of up-sampled point clouds, we propose a deep learning-based JGAU
framework that jointly up-samples geometry and attributes. It consists of a
geometry up-sampling network and an attribute up-sampling network, where the
latter leverages the up-sampled auxiliary geometry to model neighborhood
correlations of the attributes. Third, we propose two coarse attribute
up-sampling methods, Geometric Distance Weighted Attribute Interpolation
(GDWAI) and Deep Learning-based Attribute Interpolation (DLAI), to generate
coarse up-sampled attributes for each point. Then, an attribute enhancement
module is introduced to refine these up-sampled attributes and produce
high-quality point clouds by further exploiting intrinsic attribute and
geometry patterns. Extensive experiments show that the Peak Signal-to-Noise
Ratio (PSNR) achieved by the proposed JGAU method is 33.90 decibels, 32.10
decibels, 31.10 decibels, and 30.39 decibels for up-sampling rates of 4 times,
8 times, 12 times, and 16 times, respectively. Compared to state-of-the-art
methods, JGAU achieves average PSNR gains of 2.32 decibels, 2.47 decibels, 2.28
decibels, and 2.11 decibels at these four up-sampling rates, demonstrating
significant improvement.

</details>


### [91] [Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography](https://arxiv.org/abs/2506.22753)
*Jianing Zhang,Jiayi Zhu,Feiyu Ji,Xiaokang Yang,Xiaoyun Yuan*

Main category: cs.CV

TL;DR: 本文提出一种名为“退化建模多路径扩散”(DMDM)的新框架，用于可调谐超透镜计算成像，通过利用预训练模型的图像先验而非大量数据集，实现了高保真、清晰的图像重建。


<details>
  <summary>Details</summary>
Motivation: 超透镜在超紧凑计算成像方面潜力巨大，但面临复杂的光学退化和计算恢复困难。现有方法通常依赖精确的光学校准或大量配对数据集，且缺乏对推理过程的控制，易产生不希望的伪影。

Method: 引入“退化建模多路径扩散”(Degradation-Modeled Multipath Diffusion, DMDM)框架。该框架利用预训练模型的自然图像先验，而非大型数据集。它采用正向、中性、负向提示路径来平衡高频细节生成、结构保真度和抑制超透镜特定退化，并结合伪数据增强。一个可调谐解码器能实现保真度与感知质量之间的可控权衡。此外，设计了一个空间变异退化感知注意力(SVDA)模块，自适应建模复杂的光学和传感器引起的退化。最终，设计并构建了一个毫米级MetaCamera进行真实世界验证。

Result: 实验结果表明，该方法在真实世界验证中表现优于现有先进方法，实现了高保真度和清晰的图像重建。

Conclusion: DMDM框架有效解决了超透镜成像中复杂退化和数据依赖的挑战，通过创新的扩散模型和注意力机制，无需大量校准数据即可实现卓越的图像恢复，并提供灵活的质量控制，展现了其在实际应用中的巨大潜力。

Abstract: Metalenses offer significant potential for ultra-compact computational
imaging but face challenges from complex optical degradation and computational
restoration difficulties. Existing methods typically rely on precise optical
calibration or massive paired datasets, which are non-trivial for real-world
imaging systems. Furthermore, a lack of control over the inference process
often results in undesirable hallucinated artifacts. We introduce
Degradation-Modeled Multipath Diffusion for tunable metalens photography,
leveraging powerful natural image priors from pretrained models instead of
large datasets. Our framework uses positive, neutral, and negative-prompt paths
to balance high-frequency detail generation, structural fidelity, and
suppression of metalens-specific degradation, alongside \textit{pseudo} data
augmentation. A tunable decoder enables controlled trade-offs between fidelity
and perceptual quality. Additionally, a spatially varying degradation-aware
attention (SVDA) module adaptively models complex optical and sensor-induced
degradation. Finally, we design and build a millimeter-scale MetaCamera for
real-world validation. Extensive results show that our approach outperforms
state-of-the-art methods, achieving high-fidelity and sharp image
reconstruction. More materials: https://dmdiff.github.io/.

</details>


### [92] [RoboPearls: Editable Video Simulation for Robot Manipulation](https://arxiv.org/abs/2506.22756)
*Tao Tang,Likui Zhang,Youpeng Wen,Kaidong Zhang,Jia-Wang Bian,xia zhou,Tianyi Yan,Kun Zhan,Peng Jia,Hefeng Wu,Liang Lin,Xiaodan Liang*

Main category: cs.CV

TL;DR: RoboPearls是一个基于3DGS的可编辑视频仿真框架，旨在通过从演示视频生成逼真仿真并结合LLM/VLM自动化流程，解决机器人操作策略数据收集高成本和Sim-to-Real鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 通用机器人操作策略的发展依赖大量演示数据，但真实世界数据收集成本高且效率低。现有仿真平台难以弥合Sim-to-Real鸿沟。

Method: 提出RoboPearls，一个基于3D Gaussian Splatting (3DGS) 的可编辑视频仿真框架。它能从演示视频构建逼真、视角一致的仿真，并支持物体操纵等多种操作（通过ISD和3D-NNFM）。通过整合大型语言模型 (LLMs) 实现自动化仿真生产，并利用视觉-语言模型 (VLM) 分析机器人学习问题，形成仿真闭环以提升性能。

Result: 在RLBench、COLOSSEUM、Ego4D、Open X-Embodiment等多个数据集和真实机器人上进行了广泛实验，证明了令人满意的仿真性能。

Conclusion: RoboPearls通过提供一个可编辑、逼真的视频仿真框架，并结合LLM/VLM的自动化能力，有效解决了机器人数据获取效率低和Sim-to-Real鸿沟的挑战。

Abstract: The development of generalist robot manipulation policies has seen
significant progress, driven by large-scale demonstration data across diverse
environments. However, the high cost and inefficiency of collecting real-world
demonstrations hinder the scalability of data acquisition. While existing
simulation platforms enable controlled environments for robotic learning, the
challenge of bridging the sim-to-real gap remains. To address these challenges,
we propose RoboPearls, an editable video simulation framework for robotic
manipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the
construction of photo-realistic, view-consistent simulations from demonstration
videos, and supports a wide range of simulation operators, including various
object manipulations, powered by advanced modules like Incremental Semantic
Distillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by
incorporating large language models (LLMs), RoboPearls automates the simulation
production process in a user-friendly manner through flexible command
interpretation and execution. Furthermore, RoboPearls employs a vision-language
model (VLM) to analyze robotic learning issues to close the simulation loop for
performance enhancement. To demonstrate the effectiveness of RoboPearls, we
conduct extensive experiments on multiple datasets and scenes, including
RLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which
demonstrate our satisfactory simulation performance.

</details>


### [93] [VSRM: A Robust Mamba-Based Framework for Video Super-Resolution](https://arxiv.org/abs/2506.22762)
*Dinh Phu Tran,Dao Duy Hung,Daeyoung Kim*

Main category: cs.CV

TL;DR: 本文提出VSRM框架，利用Mamba模型解决视频超分辨率中CNN和Transformer的局限性，通过新颖的Mamba模块、可变形对齐和频率域损失，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 视频超分辨率（VSR）仍是低级视觉任务中的主要挑战。现有的CNN方法受限于局部感受野，而Transformer方法在处理VSR长序列时面临二次复杂度问题。

Method: 本文提出了VSRM框架，一个利用Mamba模型能力的新型视频超分辨率框架。VSRM引入了空间到时间Mamba（Spatial-to-Temporal Mamba）和时间到空间Mamba（Temporal-to-Spatial Mamba）模块来有效提取长距离时空特征并增强感受野。为更好地对齐相邻帧，提出可变形交叉Mamba对齐模块（Deformable Cross-Mamba Alignment），以实现更动态和灵活的补偿，防止特征失真。最后，通过提出一种简单有效的频率Charbonnier-like损失，最小化重建帧与真实帧之间的频域差距，以更好地保留高频内容并提升视觉质量。

Result: 通过大量实验，VSRM在多个基准测试上取得了最先进（state-of-the-art）的结果。

Conclusion: VSRM为未来的视频超分辨率研究奠定了坚实的基础，证明了Mamba在处理长序列和扩大感受野方面的潜力。

Abstract: Video super-resolution remains a major challenge in low-level vision tasks.
To date, CNN- and Transformer-based methods have delivered impressive results.
However, CNNs are limited by local receptive fields, while Transformers
struggle with quadratic complexity, posing challenges for processing long
sequences in VSR. Recently, Mamba has drawn attention for its long-sequence
modeling, linear complexity, and large receptive fields. In this work, we
propose VSRM, a novel \textbf{V}ideo \textbf{S}uper-\textbf{R}esolution
framework that leverages the power of \textbf{M}amba. VSRM introduces
Spatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract
long-range spatio-temporal features and enhance receptive fields efficiently.
To better align adjacent frames, we propose Deformable Cross-Mamba Alignment
module. This module utilizes a deformable cross-mamba mechanism to make the
compensation stage more dynamic and flexible, preventing feature distortions.
Finally, we minimize the frequency domain gaps between reconstructed and
ground-truth frames by proposing a simple yet effective Frequency
Charbonnier-like loss that better preserves high-frequency content and enhances
visual quality. Through extensive experiments, VSRM achieves state-of-the-art
results on diverse benchmarks, establishing itself as a solid foundation for
future research.

</details>


### [94] [PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection](https://arxiv.org/abs/2506.22783)
*Oguzhan Baser,Ahmet Ege Tanriverdi,Sriram Vishwanath,Sandeep P. Chinchali*

Main category: cs.CV

TL;DR: 现有Deepfake数据集缺乏真实性。本研究提出PhonemeFake（一种基于语言推理的真实语音伪造攻击方法），并发布了相应的PF数据集。此外，还开发了一种高效、精准的Deepfake检测模型，显著提升了检测性能并降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型日益先进，Deepfake (DF) 攻击构成日益增长的威胁。然而，现有DF数据集未能像真实DF攻击那样欺骗人类感知并影响公众舆论，这突出表明需要更真实的DF攻击向量。

Method: 本研究引入了PhonemeFake (PF)，一种通过语言推理操纵关键语音片段的DF攻击方法。作者发布了一个易于使用的PF数据集，并开源了一个双层DF片段检测模型，该模型能自适应地优先处理被操纵区域的计算。

Result: PhonemeFake显著降低了人类感知能力（高达42%），并将基准准确率降低了高达94%。该检测模型在三个已知DF数据集上将等错误率（EER）降低了91%，同时实现了高达90%的速度提升，计算开销极小，且比现有模型具备更精确的定位能力，是一种可扩展的解决方案。

Conclusion: 现有Deepfake数据集的非真实性是急需解决的问题。PhonemeFake提供了一种更具欺骗性的攻击方法和数据集。所提出的检测模型为Deepfake威胁提供了一个高效、快速、精准且可扩展的检测解决方案。

Abstract: Deepfake (DF) attacks pose a growing threat as generative models become
increasingly advanced. However, our study reveals that existing DF datasets
fail to deceive human perception, unlike real DF attacks that influence public
discourse. It highlights the need for more realistic DF attack vectors. We
introduce PhonemeFake (PF), a DF attack that manipulates critical speech
segments using language reasoning, significantly reducing human perception by
up to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF
dataset on HuggingFace and open-source bilevel DF segment detection model that
adaptively prioritizes compute on manipulated regions. Our extensive
experiments across three known DF datasets reveal that our detection model
reduces EER by 91% while achieving up to 90% speed-up, with minimal compute
overhead and precise localization beyond existing models as a scalable
solution.

</details>


### [95] [Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching](https://arxiv.org/abs/2506.22784)
*Yu Han,Zhiwei Huang,Yanting Zhang,Fangjun Ding,Shen Cai,Rui Fan*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的、无检测器（detector-free）的投影式框架，用于稀疏单帧激光雷达点云与相机图像的点-像素配准，通过将激光雷达强度图投影到2D视图并利用注意力匹配网络，辅以可重复性评分机制，实现了当前最优的性能。


<details>
  <summary>Details</summary>
Motivation: 激光雷达点云与相机图像的点-像素配准是自动驾驶和机器人感知中的基础且具挑战性任务。主要难点在于非结构化点云和结构化图像之间的模态鸿沟，尤其是在稀疏的单帧激光雷达设置下。现有方法因单独提取特征和匹配策略，未能有效弥合模态鸿沟，并且难以应对单帧激光雷达的稀疏性和噪声。

Method: 受无检测器匹配范式的启发，本文提出了一种基于投影的无检测器框架，用于激光雷达和相机视图间的直接点-像素匹配。具体而言，将激光雷达强度图投影到2D视图，并输入基于注意力的无检测器匹配网络，实现跨模态对应估计，无需多帧累积。此外，引入可重复性评分机制作为软可见性先验，指导网络抑制低强度变化区域的不可靠匹配，增强稀疏输入下的鲁棒性。

Result: 在KITTI、nuScenes和MIAS-LCEC-TF70基准上的大量实验表明，该方法实现了最先进的性能，在nuScenes上超越了现有方法（包括那些依赖累积点云的方法），尽管只使用了单帧激光雷达。

Conclusion: 该方法有效解决了单帧稀疏激光雷达与相机图像配准的挑战，通过创新的无检测器框架和鲁棒性增强机制，实现了卓越且领先的配准性能，无需依赖点云累积。

Abstract: Point-pixel registration between LiDAR point clouds and camera images is a
fundamental yet challenging task in autonomous driving and robotic perception.
A key difficulty lies in the modality gap between unstructured point clouds and
structured images, especially under sparse single-frame LiDAR settings.
Existing methods typically extract features separately from point clouds and
images, then rely on hand-crafted or learned matching strategies. This separate
encoding fails to bridge the modality gap effectively, and more critically,
these methods struggle with the sparsity and noise of single-frame LiDAR, often
requiring point cloud accumulation or additional priors to improve reliability.
Inspired by recent progress in detector-free matching paradigms (e.g.
MatchAnything), we revisit the projection-based approach and introduce the
detector-free framework for direct point-pixel matching between LiDAR and
camera views. Specifically, we project the LiDAR intensity map into a 2D view
from the LiDAR perspective and feed it into an attention-based detector-free
matching network, enabling cross-modal correspondence estimation without
relying on multi-frame accumulation. To further enhance matching reliability,
we introduce a repeatability scoring mechanism that acts as a soft visibility
prior. This guides the network to suppress unreliable matches in regions with
low intensity variation, improving robustness under sparse input. Extensive
experiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that
our method achieves state-of-the-art performance, outperforming prior
approaches on nuScenes (even those relying on accumulated point clouds),
despite using only single-frame LiDAR.

</details>


### [96] [RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors](https://arxiv.org/abs/2506.22800)
*Sicong Du,Jiarun Liu,Qifeng Chen,Hao-Xiang Chen,Tai-Jiang Mu,Sheng Yang*

Main category: cs.CV

TL;DR: RGE-GS是一种新颖的扩展重建框架，用于解决自动驾驶场景中单次扫描导致的路况不完整问题。它结合扩散生成与奖励引导的高斯集成，解决了现有3DGS方法在扩展时引入的不一致性和训练效率问题，并取得了领先的重建质量。


<details>
  <summary>Details</summary>
Motivation: 单次驾驶视频通常导致路况扫描不完整，这使得场景扩展对于传感器模拟器有效回归驾驶行为至关重要。尽管当代3DGS技术能实现出色的重建质量，但其直接整合扩散先验进行扩展会引入累积的物理不一致性并降低训练效率。

Method: RGE-GS框架结合了基于扩散的生成和奖励引导的高斯集成。它包含两大创新点：1）提出一个奖励网络，在重建前识别并优先处理一致生成的模式，从而选择性保留扩散输出以确保空间稳定性。2）设计一个差异化训练策略，根据场景收敛指标自动调整高斯优化进度。

Result: 在公开数据集上的广泛评估表明，RGE-GS在重建质量方面达到了最先进的性能。

Conclusion: RGE-GS通过减轻不一致性并提高效率，有效解决了驾驶模拟中不完整场景重建和扩展的挑战，并展现出卓越的重建质量。

Abstract: A single-pass driving clip frequently results in incomplete scanning of the
road structure, making reconstructed scene expanding a critical requirement for
sensor simulators to effectively regress driving actions. Although contemporary
3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction
quality, their direct extension through the integration of diffusion priors
often introduces cumulative physical inconsistencies and compromises training
efficiency. To address these limitations, we present RGE-GS, a novel expansive
reconstruction framework that synergizes diffusion-based generation with
reward-guided Gaussian integration. The RGE-GS framework incorporates two key
innovations: First, we propose a reward network that learns to identify and
prioritize consistently generated patterns prior to reconstruction phases,
thereby enabling selective retention of diffusion outputs for spatial
stability. Second, during the reconstruction process, we devise a
differentiated training strategy that automatically adjust Gaussian
optimization progress according to scene converge metrics, which achieving
better convergence than baseline methods. Extensive evaluations of publicly
available datasets demonstrate that RGE-GS achieves state-of-the-art
performance in reconstruction quality. Our source-code will be made publicly
available at https://github.com/CN-ADLab/RGE-GS. (Camera-ready version
incorporating reviewer suggestions will be updated soon.)

</details>


### [97] [Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding](https://arxiv.org/abs/2506.22803)
*Nuoye Xiong,Anqi Dong,Ning Wang,Cong Hua,Guangming Zhu,Mei Lin,Peiyi Shen,Liang Zhang*

Main category: cs.CV

TL;DR: 提出CBM-HNMU模型，通过概念瓶颈模型识别并修正有害概念，再将修正后的知识蒸馏回黑盒模型，以提高深度学习模型的解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型日益复杂，解释性降低且难以理解其决策过程。现有黑盒解释方法缺乏有效干预能力，或仅在样本层面操作，无法修改模型本身。

Method: 提出CBM-HNMU模型，利用概念瓶颈模型（CBM）近似黑盒推理并传递概念理解。基于全局梯度贡献自动识别并修正（移除/替换）有害概念。随后，将修正后的CBM知识蒸馏回黑盒模型，以提升模型的解释性和准确性。

Result: 在Flower-102、CIFAR-10、CIFAR-100、FGVC-Aircraft和CUB-200等数据集上，对CNN和Transformer模型进行评估。结果显示，模型准确率最大提升2.64%，平均准确率最大提升1.03%。

Conclusion: CBM-HNMU通过识别和修正有害概念并进行知识蒸馏，有效提升了复杂深度学习模型的解释性和准确性，增强了人-神经网络的相互理解。

Abstract: Recent advances in deep learning have led to increasingly complex models with
deeper layers and more parameters, reducing interpretability and making their
decisions harder to understand. While many methods explain black-box reasoning,
most lack effective interventions or only operate at sample-level without
modifying the model itself. To address this, we propose the Concept Bottleneck
Model for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).
CBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable
framework to approximate black-box reasoning and communicate conceptual
understanding. Detrimental concepts are automatically identified and refined
(removed/replaced) based on global gradient contributions. The modified CBM
then distills corrected knowledge back into the black-box model, enhancing both
interpretability and accuracy. We evaluate CBM-HNMU on various CNN and
transformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,
and CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum
increase in average accuracy across 1.03%. Source code is available at:
https://github.com/XiGuaBo/CBM-HNMU.

</details>


### [98] [Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate](https://arxiv.org/abs/2506.22806)
*Byung Hyun Lee,Sungjin Lim,Seunggyu Lee,Dong Un Kang,Se Young Chun*

Main category: cs.CV

TL;DR: 提出CPE框架，通过非线性残差注意力门和对抗性训练，实现文本到图像扩散模型中的概念擦除，优于现有方法，能有效删除目标概念同时保留其他概念并抵抗攻击。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型可能生成不当或受版权保护的内容。现有概念擦除方法通过微调交叉注意力层，但其线性特性难以有效保留多样化的非目标概念。

Method: 提出Concept Pinpoint Eraser (CPE) 框架。该框架通过添加非线性的残差注意力门（ResAGs）来选择性擦除目标概念。为保留非目标概念，引入注意力锚定损失。此外，通过ResAG和可学习文本嵌入进行迭代对抗训练，以最大化擦除性能并增强对抗性攻击的鲁棒性。

Result: 在名人、艺术风格和显式内容的擦除实验中，CPE表现优于现有方法，能有效删除目标概念，同时保留多样化的非目标概念，并对攻击提示具有鲁棒性。

Conclusion: CPE框架通过引入非线性模块和对抗性训练，为文本到图像扩散模型中的概念擦除提供了一种更有效、更鲁棒的解决方案，克服了现有线性方法的局限性。

Abstract: Remarkable progress in text-to-image diffusion models has brought a major
concern about potentially generating images on inappropriate or trademarked
concepts. Concept erasing has been investigated with the goals of deleting
target concepts in diffusion models while preserving other concepts with
minimal distortion. To achieve these goals, recent concept erasing methods
usually fine-tune the cross-attention layers of diffusion models. In this work,
we first show that merely updating the cross-attention layers in diffusion
models, which is mathematically equivalent to adding \emph{linear} modules to
weights, may not be able to preserve diverse remaining concepts. Then, we
propose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding
\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or
cut) target concepts while safeguarding remaining concepts from broad
distributions by employing an attention anchoring loss to prevent the
forgetting. Moreover, we adversarially train CPE with ResAG and learnable text
embeddings in an iterative manner to maximize erasing performance and enhance
robustness against adversarial attacks. Extensive experiments on the erasure of
celebrities, artistic styles, and explicit contents demonstrated that the
proposed CPE outperforms prior arts by keeping diverse remaining concepts while
deleting the target concepts with robustness against attack prompts. Code is
available at https://github.com/Hyun1A/CPE

</details>


### [99] [FreqDGT: Frequency-Adaptive Dynamic Graph Networks with Transformer for Cross-subject EEG Emotion Recognition](https://arxiv.org/abs/2506.22807)
*Yueyang Li,Shengyu Gong,Weiming Zeng,Nizhuan Wang,Wai Ting Siok*

Main category: cs.CV

TL;DR: 本文提出FreqDGT，一种频率自适应动态图Transformer，通过整合频率、空间和时间建模，显著提升了EEG情感识别的跨被试泛化能力。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）是情感识别的可靠信号，但由于个体差异大，跨被试泛化能力差是当前EEG情感识别面临的根本挑战。

Method: 提出FreqDGT模型，通过整合框架解决上述问题。FreqDGT包含：1) 频率自适应处理（FAP），动态加权与情感相关的频带；2) 自适应动态图学习（ADGL），学习特定输入的脑连接模式；3) 多尺度时间解耦网络（MTDN），结合分层时间Transformer和对抗性特征解耦，以捕捉时间动态并确保跨被试鲁棒性。

Result: 综合实验表明，FreqDGT显著提高了跨被试情感识别的准确性，证实了整合频率自适应、空间动态和时间分层建模的有效性，并确保了对个体差异的鲁棒性。

Conclusion: FreqDGT通过其创新的集成框架，有效解决了EEG情感识别中的跨被试泛化难题，为情感脑机接口的进步提供了有效途径。

Abstract: Electroencephalography (EEG) serves as a reliable and objective signal for
emotion recognition in affective brain-computer interfaces, offering unique
advantages through its high temporal resolution and ability to capture
authentic emotional states that cannot be consciously controlled. However,
cross-subject generalization remains a fundamental challenge due to individual
variability, cognitive traits, and emotional responses. We propose FreqDGT, a
frequency-adaptive dynamic graph transformer that systematically addresses
these limitations through an integrated framework. FreqDGT introduces
frequency-adaptive processing (FAP) to dynamically weight emotion-relevant
frequency bands based on neuroscientific evidence, employs adaptive dynamic
graph learning (ADGL) to learn input-specific brain connectivity patterns, and
implements multi-scale temporal disentanglement network (MTDN) that combines
hierarchical temporal transformers with adversarial feature disentanglement to
capture both temporal dynamics and ensure cross-subject robustness.
Comprehensive experiments demonstrate that FreqDGT significantly improves
cross-subject emotion recognition accuracy, confirming the effectiveness of
integrating frequency-adaptive, spatial-dynamic, and temporal-hierarchical
modeling while ensuring robustness to individual differences. The code is
available at https://github.com/NZWANG/FreqDGT.

</details>


### [100] [Efficient Multi-Crop Saliency Partitioning for Automatic Image Cropping](https://arxiv.org/abs/2506.22814)
*Andrew Hamara,Andrew C. Freeman*

Main category: cs.CV

TL;DR: 本文扩展了固定长宽比裁剪算法，实现了高效的多不重叠图像裁剪，解决了传统方法仅处理单个裁剪框的局限。


<details>
  <summary>Details</summary>
Motivation: 传统图像裁剪方法仅优化单个边界框，无法有效应对需要多个不相交裁剪区域的应用需求。

Method: 该工作扩展了固定长宽比裁剪算法，以线性时间高效提取多个非重叠裁剪区域。其方法动态调整注意力阈值，并移除已选择的裁剪区域，无需重新计算整个显著性图。

Result: 文章讨论了其方法的定性结果。

Conclusion: 这项工作提供了一种处理多不相交图像裁剪的方法，并指出了未来数据集和基准测试的潜力。

Abstract: Automatic image cropping aims to extract the most visually salient regions
while preserving essential composition elements. Traditional saliency-aware
cropping methods optimize a single bounding box, making them ineffective for
applications requiring multiple disjoint crops. In this work, we extend the
Fixed Aspect Ratio Cropping algorithm to efficiently extract multiple
non-overlapping crops in linear time. Our approach dynamically adjusts
attention thresholds and removes selected crops from consideration without
recomputing the entire saliency map. We discuss qualitative results and
introduce the potential for future datasets and benchmarks.

</details>


### [101] [Unleashing the Multi-View Fusion Potential: Noise Correction in VLM for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2506.22817)
*Xingyilang Yin,Jiale Wang,Xi Yang,Mutian Xu,Xu Gu,Nannan Wang*

Main category: cs.CV

TL;DR: MVOV3D提出一种创新的2D多视角融合方法，通过减少视觉-语言模型中的固有噪声，并在不额外训练的情况下，利用区域级特征和3D几何先验，显著提升了开放词汇3D场景理解的性能，在ScanNet200和Matterport160上取得了最先进的（SOTA）结果。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇3D场景理解方法因3D数据量有限，难以处理多样化的对象类别。尽管2D多视角融合方法在理解3D场景中的多样概念方面具有优势，但视觉-语言模型固有的噪声导致其性能次优。因此，需要一种能有效降低噪声、同时保留泛化能力并增强开放世界能力的2D多视角融合方法。

Method: 本文提出MVOV3D，旨在通过减少视觉-语言模型中的固有噪声来充分发挥2D多视角融合在开放词汇3D场景理解中的潜力，且无需进行额外训练。具体而言，MVOV3D利用CLIP编码器编码的精确区域级图像特征和文本特征来改进多视角2D特征，并结合3D几何先验来优化多视角融合过程。

Result: MVOV3D在各种数据集上进行了广泛实验，验证了其有效性。在挑战性开放词汇语义分割任务中，MVOV3D在ScanNet200数据集上取得了14.7%的mIoU，在Matterport160数据集上取得了16.2%的mIoU，均创下新纪录，显著优于当前领先的基于训练的3D网络。

Conclusion: MVOV3D成功展示了2D多视角融合在开放词汇3D场景理解中的巨大潜力，通过有效降低噪声并整合精确特征与3D几何先验，实现了无需训练的卓越性能，大幅提升了模型处理多样化3D概念的能力。

Abstract: Recent open-vocabulary 3D scene understanding approaches mainly focus on
training 3D networks through contrastive learning with point-text pairs or by
distilling 2D features into 3D models via point-pixel alignment. While these
methods show considerable performance in benchmarks with limited vocabularies,
they struggle to handle diverse object categories as the limited amount of 3D
data upbound training strong open-vocabulary 3d models. We observe that 2D
multi-view fusion methods take precedence in understanding diverse concepts in
3D scenes. However, inherent noises in vision-language models lead multi-view
fusion to sub-optimal performance. To this end, we introduce MVOV3D, a novel
approach aimed at unleashing the potential of 2D multi-view fusion for
open-vocabulary 3D scene understanding. We focus on reducing the inherent
noises without training, thereby preserving the generalizability while
enhancing open-world capabilities. Specifically, MVOV3D improves multi-view 2D
features by leveraging precise region-level image features and text features
encoded by CLIP encoders and incorporates 3D geometric priors to optimize
multi-view fusion. Extensive experiments on various datasets demonstrate the
effectiveness of our method. Notably, our MVOV3D achieves a new record with
14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160 for challenge
open-vocabulary semantic segmentation, outperforming current leading trained 3D
networks by a significant margin.

</details>


### [102] [Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration](https://arxiv.org/abs/2506.22819)
*Ramya Hebbalaguppe,Tamoghno Kandar,Abhinav Nagpal,Chetan Arora*

Main category: cs.CV

TL;DR: 本文提出一种名为TCA的新型测试时提示微调（TPT）方法，通过利用大型语言模型进行提示初始化和引入新的正则化损失，显著改善了视觉-语言模型（VLM）在TPT后的置信度校准问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLM）结合测试时提示微调（TPT）能提高图像识别准确率，但却会损害置信度校准，这限制了其在关键应用中的使用。作者认为随机或朴素的提示初始化是导致过拟合和误校准的主要原因。

Method: 1. 提出使用来自大型语言模型（LLM）的目标标签属性先验知识，对测试时提示进行精细初始化。2. 提出一种新的正则化损失，以在TPT过程中减少类内距离并增加类间距离，从而维护提示质量。

Result: 在多种CLIP架构和15个数据集上的广泛实验表明，作者提出的TCA方法能有效改善TPT后的校准。TCA的平均预期校准误差（ECE）为4.11，明显优于传统TPT（11.7）、C-TPT（6.12）、DiffTPT（6.78）和PromptAlign（8.43）。

Conclusion: 所提出的TCA方法通过改进提示初始化和引入正则化损失，成功解决了VLM在测试时提示微调后置信度校准下降的问题，使得TPT在关键应用中更具适用性。

Abstract: Vision-language models (VLM) have demonstrated impressive performance in
image recognition by leveraging self-supervised training on large datasets.
Their performance can be further improved by adapting to the test sample using
test-time prompt tuning (TPT). Unfortunately, the singular focus of TPT
approaches on improving the accuracy suffers from tunnel vision, and leads to
degradation in confidence calibration. This limits the applicability of TPT in
critical applications.
  We make three contributions in this work. (1) We posit that random or naive
initialization of prompts leads to overfitting on a particular test sample, and
is the main reason for miscalibration of the VLM after TPT. To mitigate the
problem, we propose careful initialization of test time prompt using prior
knowledge about the target label attributes from a large language model (LLM);
(2) To further maintain the quality of prompts during \tpt, we propose a novel
regularization loss to reduce intraclass distance, and increase inter-class
distance between the learnt
  Through extensive experiments on different CLIP architectures and 15
datasets, we show that our approach can effectively improve the calibration
after TPT. We report an average expected calibration error (ECE) of 4.11 with
our method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24),
6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is
publicly accessible at:
https://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.

</details>


### [103] [Listener-Rewarded Thinking in VLMs for Image Preferences](https://arxiv.org/abs/2506.22832)
*Alexander Gambashidze,Li Pengyi,Matvey Skripkin,Andrey Galichin,Anton Gusarov,Konstantin Sobolev,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CV

TL;DR: 提出一种听众增强型强化学习框架，通过评估推理链来塑造奖励，显著提升了视觉偏好模型的泛化能力和对齐效果。


<details>
  <summary>Details</summary>
Motivation: 为使文生图/视频模型与人类意图对齐，需鲁棒且泛化的视觉偏好奖励模型。现有模型泛化性差，SFT易导致记忆化。强化学习（如GRPO）虽改善泛化，但存在核心缺陷：当推理模型与独立视觉-语言“听众”的判断冲突时，推理准确性会显著下降。

Method: 引入“听众增强型GRPO框架”。该框架中，一个独立的“听众”模型重新评估推理模型的思维链，提供校准的置信分数，此分数用于塑造RL奖励信号。这促使推理模型不仅给出正确答案，还能生成对独立模型有说服力的解释。

Result: 在ImageReward基准测试中达到了最佳准确率（67.4%）；在包含120万票的大规模人类偏好数据集上，显著提升了域外（OOD）性能（相较于简单推理器，提升高达6%）；与强GRPO和SFT基线相比，减少了推理矛盾。

Conclusion: 基于听众的奖励机制提供了一条可扩展、数据高效的途径，用于将视觉-语言模型与细致入微的人类偏好对齐。

Abstract: Training robust and generalizable reward models for human visual preferences
is essential for aligning text-to-image and text-to-video generative models
with human intent. However, current reward models often fail to generalize, and
supervised fine-tuning leads to memorization, demanding complex annotation
pipelines. While reinforcement learning (RL), specifically Group Relative
Policy Optimization (GRPO), improves generalization, we uncover a key failure
mode: a significant drop in reasoning accuracy occurs when a model's reasoning
trace contradicts that of an independent, frozen vision-language model
("listener") evaluating the same output. To address this, we introduce a
listener-augmented GRPO framework. Here, the listener re-evaluates the
reasoner's chain-of-thought to provide a dense, calibrated confidence score,
shaping the RL reward signal. This encourages the reasoner not only to answer
correctly, but to produce explanations that are persuasive to an independent
model. Our listener-shaped reward scheme achieves best accuracy on the
ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)
performance on a large-scale human preference dataset (1.2M votes, up to +6%
over naive reasoner), and reduces reasoning contradictions compared to strong
GRPO and SFT baselines. These results demonstrate that listener-based rewards
provide a scalable, data-efficient path to aligning vision-language models with
nuanced human preferences. We will release our reasoning model here:
https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.

</details>


### [104] [SemFaceEdit: Semantic Face Editing on Generative Radiance Manifolds](https://arxiv.org/abs/2506.22833)
*Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: SemFaceEdit是一种基于生成辐射流形的人脸编辑方法，通过生成语义场实现对特定面部语义的精细局部编辑，并有效解耦几何与外观。


<details>
  <summary>Details</summary>
Motivation: 现有3D-aware GAN技术生成的图像缺乏局部编辑能力。尽管生成辐射流形能高效处理体积采样并学习精细细节，但仍需一种能实现外观和几何精细局部编辑的方法。

Method: 本研究提出SemFaceEdit，通过在生成辐射流形上生成语义场来简化外观和几何编辑过程。该方法利用潜在编码有效解耦生成图像中不同面部语义相关的几何和外观。网络包含几何模块（生成语义辐射和占据场）和外观模块（预测RGB辐射），两者在对抗设置下联合训练，以学习语义感知的几何和外观描述符，其中外观描述符由其语义潜在编码条件化。

Result: SemFaceEdit在基于语义场的编辑中表现出卓越性能，特别是在实现改进的辐射场解耦方面。与现有方法相比，它能精确编辑特定面部语义，同时保持其他区域的完整性。

Conclusion: SemFaceEdit成功解决了3D-aware GANs局部编辑能力不足的问题，通过在生成辐射流形上生成语义场，实现了对人脸语义的精细、解耦的局部编辑，并显著提升了辐射场解耦性能。

Abstract: Despite multiple view consistency offered by 3D-aware GAN techniques, the
resulting images often lack the capacity for localized editing. In response,
generative radiance manifolds emerge as an efficient approach for constrained
point sampling within volumes, effectively reducing computational demands and
enabling the learning of fine details. This work introduces SemFaceEdit, a
novel method that streamlines the appearance and geometric editing process by
generating semantic fields on generative radiance manifolds. Utilizing latent
codes, our method effectively disentangles the geometry and appearance
associated with different facial semantics within the generated image. In
contrast to existing methods that can change the appearance of the entire
radiance field, our method enables the precise editing of particular facial
semantics while preserving the integrity of other regions. Our network
comprises two key modules: the Geometry module, which generates semantic
radiance and occupancy fields, and the Appearance module, which is responsible
for predicting RGB radiance. We jointly train both modules in adversarial
settings to learn semantic-aware geometry and appearance descriptors. The
appearance descriptors are then conditioned on their respective semantic latent
codes by the Appearance Module, facilitating disentanglement and enhanced
control. Our experiments highlight SemFaceEdit's superior performance in
semantic field-based editing, particularly in achieving improved radiance field
disentanglement.

</details>


### [105] [FOCUS: Fine-grained Optimization with Semantic Guided Understanding for Pedestrian Attributes Recognition](https://arxiv.org/abs/2506.22836)
*Hongyan An,Kuan Zhu,Xin He,Haiyun Guo,Chaoyang Zhao,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 本文提出FOCUS方法，通过自适应提取细粒度属性特征，解决行人属性识别(PAR)中现有区域特征方法在细粒度识别和未知属性泛化方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的行人属性识别(PAR)方法依赖区域特征，但在处理细粒度模式和泛化到训练中未见的属性时存在局限性，导致性能和实用性受限。

Method: 提出FOCUS方法，该方法自适应地为每个属性提取细粒度特征。具体包括：1) Multi-Granularity Mix Tokens (MGMT) 捕获多粒度视觉特征；2) Attribute-guided Visual Feature Extraction (AVFE) 利用文本属性通过交叉注意力提取视觉属性特征；3) Region-Aware Contrastive Learning (RACL) 确保文本属性关注正确区域并共享一致的注意力图。

Result: 在PA100K、PETA和RAPv1数据集上的广泛实验证明了所提方法的有效性和强大的泛化能力。

Conclusion: FOCUS方法成功解决了行人属性识别中细粒度特征提取和未知属性泛化的问题，表现出优异的性能和实用性。

Abstract: Pedestrian attribute recognition (PAR) is a fundamental perception task in
intelligent transportation and security. To tackle this fine-grained task, most
existing methods focus on extracting regional features to enrich attribute
information. However, a regional feature is typically used to predict a fixed
set of pre-defined attributes in these methods, which limits the performance
and practicality in two aspects: 1) Regional features may compromise
fine-grained patterns unique to certain attributes in favor of capturing common
characteristics shared across attributes. 2) Regional features cannot
generalize to predict unseen attributes in the test time. In this paper, we
propose the \textbf{F}ine-grained \textbf{O}ptimization with semanti\textbf{C}
g\textbf{U}ided under\textbf{S}tanding (FOCUS) approach for PAR, which
adaptively extracts fine-grained attribute-level features for each attribute
individually, regardless of whether the attributes are seen or not during
training. Specifically, we propose the Multi-Granularity Mix Tokens (MGMT) to
capture latent features at varying levels of visual granularity, thereby
enriching the diversity of the extracted information. Next, we introduce the
Attribute-guided Visual Feature Extraction (AVFE) module, which leverages
textual attributes as queries to retrieve their corresponding visual attribute
features from the Mix Tokens using a cross-attention mechanism. To ensure that
textual attributes focus on the appropriate Mix Tokens, we further incorporate
a Region-Aware Contrastive Learning (RACL) method, encouraging attributes
within the same region to share consistent attention maps. Extensive
experiments on PA100K, PETA, and RAPv1 datasets demonstrate the effectiveness
and strong generalization ability of our method.

</details>


### [106] [AG-VPReID 2025: Aerial-Ground Video-based Person Re-identification Challenge Results](https://arxiv.org/abs/2506.22843)
*Kien Nguyen,Clinton Fookes,Sridha Sridharan,Huy Nguyen,Feng Liu,Xiaoming Liu,Arun Ross,Dana Michalski,Tamás Endrei,Ivan DeAndres-Tame,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez,Javier Ortega-Garcia,Zijing Gong,Yuhao Wang,Xuehu Liu,Pingping Zhang,Md Rashidunnabi,Hugo Proença,Kailash A. Hambarde,Saeid Rezaei*

Main category: cs.CV

TL;DR: 本文介绍了AG-VPReID 2025挑战赛及其新数据集，旨在解决高空到地面视频人员重识别（ReID）的难题，并展示了领先团队的成果，其中最佳方法在空对地和地对空ReID中均取得超过70%的Rank-1准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管地面人员重识别已取得显著进展，但高空和地面视角之间极端的域差异、尺度变化及遮挡，使得跨视角人员重识别成为一个艰巨挑战，而这对于大规模监控和公共安全应用至关重要。

Method: 基于AG-ReID 2023挑战赛的成就，本文推出了首个大规模视频高空（80-120米）到地面人员重识别挑战赛AG-VPReID 2025。该挑战赛基于新的AG-VPReID数据集构建，该数据集包含3027个身份、超过13500个轨迹和约370万帧图像。挑战赛吸引了四个国际团队，他们开发了包括多流架构、基于Transformer的时序推理和物理知情建模等多种解决方案。

Result: 挑战赛的领先方法——来自UAM的X-TFCLIP，在空对地人员重识别设置中达到了72.28%的Rank-1准确率，在地对空人员重识别设置中达到了70.77%的Rank-1准确率，超越了现有基线，同时也凸显了数据集的复杂性。

Conclusion: AG-VPReID 2025挑战赛及其新数据集的推出，显著推动了高空到地面视频人员重识别领域的研究，展示了该领域最先进的解决方案，并明确了未来研究需要克服的复杂性与挑战。

Abstract: Person re-identification (ReID) across aerial and ground vantage points has
become crucial for large-scale surveillance and public safety applications.
Although significant progress has been made in ground-only scenarios, bridging
the aerial-ground domain gap remains a formidable challenge due to extreme
viewpoint differences, scale variations, and occlusions. Building upon the
achievements of the AG-ReID 2023 Challenge, this paper introduces the AG-VPReID
2025 Challenge - the first large-scale video-based competition focused on
high-altitude (80-120m) aerial-ground ReID. Constructed on the new AG-VPReID
dataset with 3,027 identities, over 13,500 tracklets, and approximately 3.7
million frames captured from UAVs, CCTV, and wearable cameras, the challenge
featured four international teams. These teams developed solutions ranging from
multi-stream architectures to transformer-based temporal reasoning and
physics-informed modeling. The leading approach, X-TFCLIP from UAM, attained
72.28% Rank-1 accuracy in the aerial-to-ground ReID setting and 70.77% in the
ground-to-aerial ReID setting, surpassing existing baselines while highlighting
the dataset's complexity. For additional details, please refer to the official
website at https://agvpreid25.github.io.

</details>


### [107] [DMD-Net: Deep Mesh Denoising Network](https://arxiv.org/abs/2506.22850)
*Aalok Gangopadhyay,Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: 提出DMD-Net，一个端到端深度学习框架，结合图卷积网络和特征引导变换器，用于高效且鲁棒地进行网格去噪。


<details>
  <summary>Details</summary>
Motivation: 解决网格去噪问题，旨在提供一种更有效、鲁棒的去噪方法。

Method: 引入DMD-Net，一个端到端深度学习框架，其核心是图卷积神经网络，在原始和对偶图上进行聚合，通过非对称双流网络及原始-对偶融合块实现。此外，开发了特征引导变换器（FGT）范式，包含特征提取器、变换器和去噪器。网络在大规模3D对象数据集上进行训练。

Result: 消融研究证明网络中每个组件的重要性；与现有最先进网格去噪算法相比，DMD-Net取得了有竞争力甚至更好的结果；对各种噪声具有鲁棒性；即使在极高噪声下也能实现优异性能。

Conclusion: DMD-Net是一种高效且鲁棒的网格去噪深度学习方法，在性能上超越或媲美现有技术，并能有效处理高强度噪声。

Abstract: We present Deep Mesh Denoising Network (DMD-Net), an end-to-end deep learning
framework, for solving the mesh denoising problem. DMD-Net consists of a Graph
Convolutional Neural Network in which aggregation is performed in both the
primal as well as the dual graph. This is realized in the form of an asymmetric
two-stream network, which contains a primal-dual fusion block that enables
communication between the primal-stream and the dual-stream. We develop a
Feature Guided Transformer (FGT) paradigm, which consists of a feature
extractor, a transformer, and a denoiser. The feature extractor estimates the
local features, that guide the transformer to compute a transformation, which
is applied to the noisy input mesh to obtain a useful intermediate
representation. This is further processed by the denoiser to obtain the
denoised mesh. Our network is trained on a large scale dataset of 3D objects.
We perform exhaustive ablation studies to demonstrate that each component in
our network is essential for obtaining the best performance. We show that our
method obtains competitive or better results when compared with the
state-of-the-art mesh denoising algorithms. We demonstrate that our method is
robust to various kinds of noise. We observe that even in the presence of
extremely high noise, our method achieves excellent performance.

</details>


### [108] [Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval](https://arxiv.org/abs/2506.22864)
*Li-Cheng Shen,Jih-Kang Hsieh,Wei-Hua Li,Chu-Song Chen*

Main category: cs.CV

TL;DR: 该研究引入了Mask-aware TIR (MaTIR)新任务，结合了文本到图像检索（TIR）和指称表达分割（RES），并提出了一个两阶段框架，通过离线处理和多模态大语言模型（MLLM）在线精炼，显著提升了检索准确性和分割质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像检索方法缺乏可解释性且依赖整体图像描述，而指称表达分割在大规模图像集中计算成本高昂。研究旨在弥合高效图像搜索和精确对象分割之间的鸿沟。

Method: 引入了Mask-aware TIR (MaTIR)任务。提出两阶段框架：第一阶段进行分割感知的图像检索，利用SAM 2生成对象掩码并用Alpha-CLIP提取区域级嵌入（离线处理），实现高效可扩展的在线检索；第二阶段使用多模态大语言模型（MLLM）进行重排序和对象定位，通过匹配MLLM生成的边界框与分割掩码来优化结果。

Result: 在COCO和D$^3$数据集上进行评估，结果显示相较于现有方法，该方法在检索准确性和分割质量方面均有显著提升。

Conclusion: Mask-aware TIR (MaTIR)任务的提出及其两阶段框架，成功地将文本到图像检索和指称表达分割结合，并在性能上超越了现有方法，有效解决了高效检索和精确对象定位的需求。

Abstract: Text-to-image retrieval (TIR) aims to find relevant images based on a textual
query, but existing approaches are primarily based on whole-image captions and
lack interpretability. Meanwhile, referring expression segmentation (RES)
enables precise object localization based on natural language descriptions but
is computationally expensive when applied across large image collections. To
bridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies
TIR and RES, requiring both efficient image search and accurate object
segmentation. To address this task, we propose a two-stage framework,
comprising a first stage for segmentation-aware image retrieval and a second
stage for reranking and object grounding with a multimodal large language model
(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract
region-level embeddings offline at first, enabling effective and scalable
online retrieval. Secondly, MLLM is used to refine retrieval rankings and
generate bounding boxes, which are matched to segmentation masks. We evaluate
our approach on COCO and D$^3$ datasets, demonstrating significant improvements
in both retrieval accuracy and segmentation quality over previous methods.

</details>


### [109] [Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception](https://arxiv.org/abs/2506.22866)
*Hang-Cheng Dong,Lu Zou,Bingguo Liu,Dong Ye,Guodong Liu*

Main category: cs.CV

TL;DR: 本文提出一种弱监督语义分割框架，结合改进的区域感知类激活图（CAM）和伪标签训练，以解决工业表面缺陷检测中对大规模标注数据集的依赖问题，实现高精度缺陷分割。


<details>
  <summary>Details</summary>
Motivation: 传统语义分割和目标检测模型在工业缺陷检测中，因严重依赖大规模标注数据集而与实际标注资源有限的需求相冲突。

Method: 该研究提出一个弱监督语义分割框架，包含两个核心组件：1) 区域感知类激活图（CAM），通过引入滤波引导反向传播（FGBP）和区域感知加权模块来优化低分辨率和细节不足问题，以识别更相关的缺陷区域；2) 伪标签训练，用于迭代精炼模型性能。

Result: 在工业缺陷数据集上进行的综合实验证明了该方法的优越性。

Conclusion: 该框架有效连接了弱监督学习和高精度缺陷分割，为资源受限的工业场景提供了一种实用的缺陷检测解决方案。

Abstract: Surface defect detection plays a critical role in industrial quality
inspection. Recent advances in artificial intelligence have significantly
enhanced the automation level of detection processes. However, conventional
semantic segmentation and object detection models heavily rely on large-scale
annotated datasets, which conflicts with the practical requirements of defect
detection tasks. This paper proposes a novel weakly supervised semantic
segmentation framework comprising two key components: a region-aware class
activation map (CAM) and pseudo-label training. To address the limitations of
existing CAM methods, especially low-resolution thermal maps, and insufficient
detail preservation, we introduce filtering-guided backpropagation (FGBP),
which refines target regions by filtering gradient magnitudes to identify areas
with higher relevance to defects. Building upon this, we further develop a
region-aware weighted module to enhance spatial precision. Finally,
pseudo-label segmentation is implemented to refine the model's performance
iteratively. Comprehensive experiments on industrial defect datasets
demonstrate the superiority of our method. The proposed framework effectively
bridges the gap between weakly supervised learning and high-precision defect
segmentation, offering a practical solution for resource-constrained industrial
scenarios.

</details>


### [110] [STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing](https://arxiv.org/abs/2506.22868)
*Junsung Lee,Junoh Kang,Bohyung Han*

Main category: cs.CV

TL;DR: STR-Match是一种无需训练的文本引导视频编辑算法，通过新颖的STR分数在潜空间优化，解决了现有方法的时间不一致和领域转换限制，实现了高质量且时空一致的视频编辑。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导视频编辑方法存在时间不一致性、运动失真和领域转换受限等问题，作者认为这些限制源于编辑过程中时空像素关联建模不足。

Method: 提出STR-Match，一个无需训练的视频编辑算法。它通过潜在优化，并由新颖的STR分数指导。该分数利用文本到视频（T2V）扩散模型中的2D空间注意力和1D时间模块捕捉相邻帧的时空像素关联，避免了昂贵的3D注意力机制。STR-Match与潜空间优化框架及潜掩码结合。

Result: STR-Match生成了时间一致且视觉忠实的视频，即使在显著的领域转换下也能保持强大性能，同时保留了源视频的关键视觉属性。广泛实验表明，STR-Match在视觉质量和时空一致性方面持续优于现有方法。

Conclusion: STR-Match通过有效建模时空像素关联，显著提升了文本引导视频编辑的时空一致性和视觉质量，尤其在处理复杂领域转换时表现出色。

Abstract: Previous text-guided video editing methods often suffer from temporal
inconsistency, motion distortion, and-most notably-limited domain
transformation. We attribute these limitations to insufficient modeling of
spatiotemporal pixel relevance during the editing process. To address this, we
propose STR-Match, a training-free video editing algorithm that produces
visually appealing and spatiotemporally coherent videos through latent
optimization guided by our novel STR score. The score captures spatiotemporal
pixel relevance across adjacent frames by leveraging 2D spatial attention and
1D temporal modules in text-to-video (T2V) diffusion models, without the
overhead of computationally expensive 3D attention mechanisms. Integrated into
a latent optimization framework with a latent mask, STR-Match generates
temporally consistent and visually faithful videos, maintaining strong
performance even under significant domain transformations while preserving key
visual attributes of the source. Extensive experiments demonstrate that
STR-Match consistently outperforms existing methods in both visual quality and
spatiotemporal consistency.

</details>


### [111] [Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder](https://arxiv.org/abs/2506.22880)
*Dang Jisheng,Wu Xudong,Wang Bimei,Lv Ning,Chen Jiayu,Jingwen Zhao,Yichu liu,Jizhao Liu,Juncheng Li,Teng Wang*

Main category: cs.CV

TL;DR: 本文提出DeSa2VA模型，通过解耦视觉和语义信息，解决了现有视觉-语言模型中特征纠缠导致分割精度下降的问题，在多种图像与视频分割和问答任务上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频分割和定位方法（如Sa2VA）直接融合特征，导致动态视觉信息与静态语义信息纠缠，从而降低分割精度。本研究旨在系统性地缓解此问题。

Method: 提出DeSa2VA，一个解耦增强的提示方案。首先，设计文本预训练范式，将文本标签转换为点级提示并生成文本掩码，通过混合损失函数增强语义定位。其次，使用线性投影将大语言模型生成的隐状态解耦为独立的文本和视觉特征子空间。最后，通过动态掩码融合策略，结合解耦后的特征并利用三重监督进行优化。

Result: 在图像分割、图像问答、视频分割和视频问答等多种任务上，均取得了最先进的性能。

Conclusion: DeSa2VA通过有效解耦视觉与语义信息，成功解决了现有方法中特征纠缠的问题，显著提升了模型在多模态任务上的性能，并达到了最先进水平。

Abstract: Existing video segmenter and grounder approaches, exemplified by Sa2VA,
directly fuse features within segmentation models. This often results in an
undesirable entanglement of dynamic visual information and static semantics,
thereby degrading segmentation accuracy. To systematically mitigate this issue,
we propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text
pre-training and a linear decoupling module to address the information
processing limitations inherent in SAM-2. Specifically, first, we devise a
pre-training paradigm that converts textual ground-truth labels into
point-level prompts while generating corresponding text masks. These masks are
refined through a hybrid loss function to strengthen the model's semantic
grounding capabilities. Next, we employ linear projection to disentangle hidden
states that generated by a large language model into distinct textual and
visual feature subspaces. Finally, a dynamic mask fusion strategy
synergistically combines these decoupled features through triple supervision
from predicted text/visual masks and ground-truth annotations. Extensive
experiments demonstrate state-of-the-art performance across diverse tasks,
including image segmentation, image question answering, video segmentation, and
video question answering. Our codes are available at
https://github.com/longmalongma/DeSa2VA.

</details>


### [112] [How Semantically Informative is an Image?: Measuring the Covariance-Weighted Norm of Contrastive Learning Embeddings](https://arxiv.org/abs/2506.22881)
*Fumiya Uchiyama,Rintaro Yanagi,Shohei Taniguchi,Shota Takashiro,Masahiro Suzuki,Hirokatsu Kataoka,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CV

TL;DR: 本文提出了一种基于对比学习的图像和文本语义信息量度量方法，通过重新定义信息增益（IG）概念，量化内容的信息丰富度，并发现低IG值常指示无信息内容。此外，还引入了一种高效的基于嵌入范数的IG估计方法，并在OpenCLIP中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习模型虽能估计关系语义相似度，但其是否能表示**绝对语义信息量**尚不明确，这是本研究关注的核心问题。

Method: 1. 提出了一种语义信息量度量，分别从文本样本计算图像的信息量，从图像样本计算文本的信息量，均基于对比学习模型。 2. 重新定义了自然语言处理中已有的信息增益（IG）概念，并将其应用扩展至视觉与语言领域，用于量化图像（或文本）对关联文本（或图像）分布的扭曲程度。 3. 借鉴Skip-Gram with Negative Sampling (SGNS)词嵌入的理论结果，提出了一种基于嵌入范数（norm-based metric）的方法来估计信息增益。

Result: 1. 在OpenCLIP的实验中，信息增益得分最低的图像常对应“图像未找到”等占位符图标。 2. 提出的范数估计算法与使用CLIP或SigLIP计算的信息增益之间存在强相关性，决定系数R²在0.98至1.00之间。 3. 该方法在计算出样本嵌入的均值和协方差后，其计算成本与样本大小无关，并且兼容公开的开源模型。

Conclusion: 本研究成功地引入并验证了一种量化多模态内容（图像与文本）绝对语义信息量的新方法——重定义的信息增益。该方法不仅能识别低信息量内容，还提供了一种高效、可扩展且兼容现有模型的估算范式，为评估和理解视觉-语言模型中的信息属性提供了新工具。

Abstract: Contrastive learning has the capacity to model multimodal probability
distributions by embedding and aligning visual representations with semantics
from captions. This approach enables the estimation of relational semantic
similarity; however, it remains unclear whether it can also represent absolute
semantic informativeness. In this work, we introduce a semantic informativeness
metric for an image calculated from text samples via a contrastive learning
model; similarly, the informativeness of a text is calculated from image
samples. We propose a redefinition of the concept of Information Gain, a
concept previously explored in natural language processing, extending its
application to the domains of vision and language. Our metric quantifies how
conditioning on an image distorts the distribution of associated texts, and
vice versa for text conditioning on image distributions. In OpenCLIP's
empirical results, we observe that images with the lowest Information Gain
scores often correspond to placeholder icons such as "image not found."
Furthermore, we propose to measure a norm-based metric of the embedding to
estimate the Information Gain, following the theoretical results for Skip-Gram
with Negative Sampling (SGNS) word embedding. Information Gain can be measured
using either CLIP or SigLIP, and the results demonstrate a strong correlation
with a coefficient of determination ranging from 0.98 to 1.00. After obtaining
the mean and the covariance of the sample embedding, the computational cost of
this method is independent of the sample size, and it is compatible with
publicly available, open-weight models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [113] [Bootstrapping Human-Like Planning via LLMs](https://arxiv.org/abs/2506.22604)
*David Porfirio,Vincent Hsiao,Morgan Fine-Morris,Leslie Smith,Laura M. Hiatt*

Main category: cs.AI

TL;DR: 该研究探讨了如何结合自然语言和拖放界面来指定机器人任务，通过构建基于大型语言模型（LLM）的管道，将自然语言转换为人类粒度级别的动作序列，结果表明大型模型表现更优，但小型模型也能达到满意效果。


<details>
  <summary>Details</summary>
Motivation: 机器人终端用户需要更易于访问的任务指定方式。现有的自然语言编程直观但可能缺乏精度，而拖放界面精确但可能不够直观。研究动机在于探索如何结合这两种范式，以平衡直观性与精度。

Method: 构建了一个基于大型语言模型（LLM）的管道，该管道接受自然语言输入，并输出人类粒度级别的机器人动作序列。随后，将这些生成的动作序列与另一组手动指定的动作序列数据集进行比较评估。

Result: 研究结果显示，在生成类人动作序列方面，大型模型通常优于小型模型，但小型模型也能达到令人满意的性能。

Conclusion: 通过LLM管道将自然语言转化为精细的机器人动作序列是可行的，且即使是较小的LLM也能提供令人满意的性能，为机器人任务指定提供了一种结合直观性和精确性的新方法。

Abstract: Robot end users increasingly require accessible means of specifying tasks for
robots to perform. Two common end-user programming paradigms include
drag-and-drop interfaces and natural language programming. Although natural
language interfaces harness an intuitive form of human communication,
drag-and-drop interfaces enable users to meticulously and precisely dictate the
key actions of the robot's task. In this paper, we investigate the degree to
which both approaches can be combined. Specifically, we construct a large
language model (LLM)-based pipeline that accepts natural language as input and
produces human-like action sequences as output, specified at a level of
granularity that a human would produce. We then compare these generated action
sequences to another dataset of hand-specified action sequences. Although our
results reveal that larger models tend to outperform smaller ones in the
production of human-like action sequences, smaller models nonetheless achieve
satisfactory performance.

</details>


### [114] [Ludax: A GPU-Accelerated Domain Specific Language for Board Games](https://arxiv.org/abs/2506.22609)
*Graham Todd,Alexander G. Padula,Dennis J. N. J. Soemers,Julian Togelius*

Main category: cs.AI

TL;DR: Ludax是一个将棋盘游戏描述语言编译为硬件加速代码的框架，它结合了通用性和速度，旨在加速AI游戏研究。


<details>
  <summary>Details</summary>
Motivation: 人工智能研究长期使用游戏作为基准和测试环境。现有的游戏描述语言虽提供了算法泛化能力，但缺乏硬件加速带来的高效率。随着强化学习的快速发展，对计算速度的需求日益增加，亟需一个能将游戏描述语言的通用性与现代硬件加速优势相结合的框架。

Method: 本文提出了Ludax框架，这是一个针对棋盘游戏的领域特定语言，能够自动编译成硬件加速代码。研究人员对Ludax的描述语言和编译过程进行了详细说明，并进行了速度基准测试，同时演示了如何使用Ludax训练强化学习代理。

Result: Ludax成功地将游戏描述语言的通用性与现代并行处理硬件的速度结合起来，实现了快速模拟和灵活的表示方案。通过速度基准测试和强化学习代理训练的演示，验证了其有效性。Ludax框架及其现有棋盘游戏实现已开源并免费提供。

Conclusion: Ludax被设想为一种通过实现快速模拟和提供灵活表示方案来加速广义游戏研究（从强化学习到认知科学）的工具。

Abstract: Games have long been used as benchmarks and testing environments for research
in artificial intelligence. A key step in supporting this research was the
development of game description languages: frameworks that compile
domain-specific code into playable and simulatable game environments, allowing
researchers to generalize their algorithms and approaches across multiple games
without having to manually implement each one. More recently, progress in
reinforcement learning (RL) has been largely driven by advances in hardware
acceleration. Libraries like JAX allow practitioners to take full advantage of
cutting-edge computing hardware, often speeding up training and testing by
orders of magnitude. Here, we present a synthesis of these strands of research:
a domain-specific language for board games which automatically compiles into
hardware-accelerated code. Our framework, Ludax, combines the generality of
game description languages with the speed of modern parallel processing
hardware and is designed to fit neatly into existing deep learning pipelines.
We envision Ludax as a tool to help accelerate games research generally, from
RL to cognitive science, by enabling rapid simulation and providing a flexible
representation scheme. We present a detailed breakdown of Ludax's description
language and technical notes on the compilation process, along with speed
benchmarking and a demonstration of training RL agents. The Ludax framework,
along with implementations of existing board games, is open-source and freely
available.

</details>


### [115] [URSA: The Universal Research and Scientific Agent](https://arxiv.org/abs/2506.22653)
*Michael Grosskopf,Russell Bent,Rahul Somasundaram,Isaac Michaud,Arthur Lui,Nathan Debardeleben,Earl Lawrence*

Main category: cs.AI

TL;DR: 本文提出了URSA，一个结合大型语言模型（LLMs）和物理模拟的模块化科学智能体生态系统，旨在加速复杂的科研任务。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）已发展出复杂的推理、规划和研究能力，与科学家日常工作高度重叠。将LLMs应用于“智能体”AI有望革新现代科学，消除研究瓶颈。

Method: 本文提出了URSA，一个科学智能体生态系统，用于加速科研任务。URSA由一套模块化智能体和工具组成，包括与高级物理模拟代码的耦合，能够组合应对不同复杂度的科学问题。

Result: 本文重点介绍了URSA的系统架构，并通过具体示例展示了该系统在加速科研任务方面的巨大潜力。

Conclusion: URSA系统展示了利用LLM驱动的科学智能体加速和革新现代科学研究的巨大潜力，有望移除科研进展中的瓶颈。

Abstract: Large language models (LLMs) have moved far beyond their initial form as
simple chatbots, now carrying out complex reasoning, planning, writing, coding,
and research tasks. These skills overlap significantly with those that human
scientists use day-to-day to solve complex problems that drive the cutting edge
of research. Using LLMs in "agentic" AI has the potential to revolutionize
modern science and remove bottlenecks to progress. In this work, we present
URSA, a scientific agent ecosystem for accelerating research tasks. URSA
consists of a set of modular agents and tools, including coupling to advanced
physics simulation codes, that can be combined to address scientific problems
of varied complexity and impact. This work highlights the architecture of URSA,
as well as examples that highlight the potential of the system.

</details>


### [116] [Explanations are a means to an end](https://arxiv.org/abs/2506.22740)
*Jessica Hullman,Ziyang Guo,Berk Ustun*

Main category: cs.AI

TL;DR: 现有可解释机器学习方法未充分考虑实际应用。本文提出基于统计决策理论的框架，使解释设计和评估围绕具体用途进行，从而提升解释价值并避免误用。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释机器学习方法主要关注描述模型如何映射输入到输出，但缺乏对解释在实践中如何被使用的深入考虑，导致潜在的误用和模糊性。

Method: 提出一个基于统计决策理论的框架，用于形式化解释的“最终目的”或具体用途。该方法将解释设计与实际功能结合，并展示如何通过此框架量化解释对理想决策者在特定任务上可能带来的性能提升。

Result: 证明了这种功能驱动的方法可应用于临床决策支持、提供追索权或调试等多种用例。它能够刻画解释对理想决策者在特定任务上所能提供的最大性能“提升”，并通过强制研究人员明确具体用例来防止因歧义造成的误用。

Conclusion: 解释应基于其特定目的进行设计和评估。评估应融合理论和实证视角来衡量解释的价值，并为此贡献了相关定义。

Abstract: Modern methods for explainable machine learning are designed to describe how
models map inputs to outputs--without deep consideration of how these
explanations will be used in practice. This paper argues that explanations
should be designed and evaluated with a specific end in mind. We describe how
to formalize this end in a framework based in statistical decision theory. We
show how this functionally-grounded approach can be applied across diverse use
cases, such as clinical decision support, providing recourse, or debugging. We
demonstrate its use to characterize the maximum "boost" in performance on a
particular task that an explanation could provide an idealized decision-maker,
preventing misuse due to ambiguity by forcing researchers to specify concrete
use cases that can be analyzed in light of models of expected explanation use.
We argue that evaluation should meld theoretical and empirical perspectives on
the value of explanation, and contribute definitions that span these
perspectives.

</details>


### [117] [Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems](https://arxiv.org/abs/2506.22774)
*Michael Papademas,Xenia Ziouvelou,Antonis Troumpoukis,Vangelis Karkaletsis*

Main category: cs.AI

TL;DR: 本文提出了一种结合人工智能伦理组件与PageRank/TrustRank算法的新评估方法，旨在对AI系统的可信赖性进行更全面、定量且客观的评估，弥合现有指南和工具的不足。


<details>
  <summary>Details</summary>
Motivation: 人工智能技术广泛集成并对社会产生深远影响，其复杂性可能导致超出人类监督和理解的依赖。现有可信赖AI的理论工具和指南缺乏量化能力，而技术工具虽能实现量化却缺乏整体视角，因此需要一种能结合伦理考量与量化评估、并减少主观性的方法。

Method: 引入一种评估方法，该方法将可信赖AI的伦理组成部分与PageRank和TrustRank的算法过程相结合。目标是建立一个评估框架，通过引入算法标准来最大限度地减少该领域普遍存在的自我评估技术固有的主观性。

Result: 所提出方法的应用表明，可以通过提供定量见解并考虑相关指南的理论内容，实现对AI系统可信赖性的整体评估。

Conclusion: 该研究成功地提供了一种结合了伦理组件和算法过程的评估方法，能够对AI系统的可信赖性进行全面、定量且主观性更低的评估，填补了现有评估方法的空白。

Abstract: Artificial Intelligence (AI) technology epitomizes the complex challenges
posed by human-made artifacts, particularly those widely integrated into
society and exert significant influence, highlighting potential benefits and
their negative consequences. While other technologies may also pose substantial
risks, AI's pervasive reach makes its societal effects especially profound. The
complexity of AI systems, coupled with their remarkable capabilities, can lead
to a reliance on technologies that operate beyond direct human oversight or
understanding. To mitigate the risks that arise, several theoretical tools and
guidelines have been developed, alongside efforts to create technological tools
aimed at safeguarding Trustworthy AI. The guidelines take a more holistic view
of the issue but fail to provide techniques for quantifying trustworthiness.
Conversely, while technological tools are better at achieving such
quantification, they lack a holistic perspective, focusing instead on specific
aspects of Trustworthy AI. This paper aims to introduce an assessment method
that combines the ethical components of Trustworthy AI with the algorithmic
processes of PageRank and TrustRank. The goal is to establish an assessment
framework that minimizes the subjectivity inherent in the self-assessment
techniques prevalent in the field by introducing algorithmic criteria. The
application of our approach indicates that a holistic assessment of an AI
system's trustworthiness can be achieved by providing quantitative insights
while considering the theoretical content of relevant guidelines.

</details>


### [118] [ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models](https://arxiv.org/abs/2506.22865)
*Ziqi Zhong,Xunzhu Tang*

Main category: cs.AI

TL;DR: 本文提出ReasonBridge，一种样本高效的分层知识蒸馏方法，通过利用少量高质量数据和稀疏适配器，将闭源大模型的推理能力迁移到开源模型，显著缩小了性能差距，使开源模型在复杂推理任务上达到甚至超越了顶尖闭源模型。


<details>
  <summary>Details</summary>
Motivation: 闭源大型语言模型在复杂推理和精确指令遵循任务上，与开源模型存在显著的性能差距。

Method: 引入ReasonBridge，通过新颖的分层知识蒸馏框架，将闭源模型的推理能力迁移到开源模型。具体方法包括：1) 构建仅含1000条高质量推理轨迹的定制数据集Reason1K；2) 采用分层蒸馏过程，捕获战略抽象和战术实现模式；3) 设计稀疏推理适配器架构，仅需0.3%的额外可训练参数；4) 实施测试时计算缩放机制。

Result: ReasonBridge使开源模型在基准任务上的推理能力提升高达23%，显著缩小了与闭源模型的差距。值得注意的是，增强后的Qwen2.5-14B在MATH500上超越了Claude-Sonnet3.5，并在竞赛级AIME问题上与其性能持平。

Conclusion: 该方法提供了一种样本高效的推理增强方案，可有效泛化到不同推理领域和模型架构，显著提升了开源模型在指令遵循方面的推理能力。

Abstract: Recent advancements in Large Language Models (LLMs) have revealed a
significant performance gap between closed-source and open-source models,
particularly in tasks requiring complex reasoning and precise instruction
following. This paper introduces ReasonBridge, a methodology that efficiently
transfers reasoning capabilities from powerful closed-source to open-source
models through a novel hierarchical knowledge distillation framework. We
develop a tailored dataset Reason1K with only 1,000 carefully curated reasoning
traces emphasizing difficulty, diversity, and quality. These traces are
filtered from across multiple domains using a structured multi-criteria
selection algorithm. Our transfer learning approach incorporates: (1) a
hierarchical distillation process capturing both strategic abstraction and
tactical implementation patterns, (2) a sparse reasoning-focused adapter
architecture requiring only 0.3% additional trainable parameters, and (3) a
test-time compute scaling mechanism using guided inference interventions.
Comprehensive evaluations demonstrate that ReasonBridge improves reasoning
capabilities in open-source models by up to 23% on benchmark tasks,
significantly narrowing the gap with closed-source models. Notably, the
enhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its
performance on competition-level AIME problems. Our methodology generalizes
effectively across diverse reasoning domains and model architectures,
establishing a sample-efficient approach to reasoning enhancement for
instruction following.

</details>


### [119] [Agentic Enterprise: AI-Centric User to User-Centric AI](https://arxiv.org/abs/2506.22893)
*Arpit Narechania,Alex Endert,Atanu R Sinha*

Main category: cs.AI

TL;DR: 本文探讨AI在企业决策中的应用潜力，提出通过AI代理提升决策效率，并阐述了六项以用户为中心的AI代理成功原则。


<details>
  <summary>Details</summary>
Motivation: 鉴于AI在企业决策中扮演的关键且重复的角色，以及当前AI范式未能充分满足企业决策需求，本文旨在深入探讨AI（特别是AI代理）如何提升企业决策生产力。

Method: 论文通过审视当前以AI为中心的用户范式所缺失的部分，提出了六项以用户为中心的AI代理在企业中成功的原则。同时，倡导市场机制，以使AI的设计和通过代理的交付能更好地服务于企业用户。

Result: 分析了AI在企业决策中的潜力，识别出当前AI-centric范式与企业决策需求之间的差距，并提出了支持企业决策效率提升的六项关键原则和向用户中心AI转型的建议。

Conclusion: AI代理是提升企业决策生产力的有效手段，其成功关键在于转向以用户为中心的AI设计与交付，并遵循所提出的六项原则，同时需通过市场机制来促进平台发展，以真正服务于企业用户。

Abstract: After a very long winter, the Artificial Intelligence (AI) spring is here.
Or, so it seems over the last three years. AI has the potential to impact many
areas of human life - personal, social, health, education, professional. In
this paper, we take a closer look at the potential of AI for Enterprises, where
decision-making plays a crucial and repeated role across functions, tasks, and
operations. We consider Agents imbued with AI as means to increase
decision-productivity of enterprises. We highlight six tenets for Agentic
success in enterprises, by drawing attention to what the current, AI-Centric
User paradigm misses, in the face of persistent needs of and usefulness for
Enterprise Decision-Making. In underscoring a shift to User-Centric AI, we
offer six tenets and promote market mechanisms for platforms, aligning the
design of AI and its delivery by Agents to the cause of enterprise users.

</details>


### [120] [Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning](https://arxiv.org/abs/2506.22919)
*Sanskar Pandey,Ruhaan Chopra,Saad Murtaza Bhat,Ark Abhyudaya*

Main category: cs.AI

TL;DR: Hecto是一种轻量级异构MoE模型，通过结合GRU和FFNN专家实现条件计算和专家专业化，并在多项推理任务中展现出稳定性和可解释性，为低资源下的专业化推理提供了新范式。


<details>
  <summary>Details</summary>
Motivation: 现有Mixture-of-Experts (MoE) 模型中专家具有相同的归纳偏差，限制了表示多样性；静态计算路径效率低下，并限制了专业化和可解释性。研究旨在解决这些局限性，提升模型处理不同推理类型的能力。

Method: 提出Hecto架构，一种轻量级MoE模型。它利用架构异构性，结合GRU专家（用于时间推理）和FFNN专家（用于静态抽象），并通过稀疏的Top-1门控机制进行路由。模型在AG News、SST-2、HotpotQA三个推理基准和STS-B回归任务上进行评估。

Result: Hecto在性能上与同构基线相当或略低，同时实现了清晰的专家专业化，各专家与特定推理类型（时间 vs 静态）对齐。在大批量处理时，Hecto表现出性能提升。消融实验证明，架构多样性是Hecto在多样推理任务中保持稳定性和可解释性的关键。

Conclusion: Hecto为条件计算树立了一个新基准，通过其原理性专业化，在低资源环境下为专业化推理提供了一个有效且可解释的框架。

Abstract: Mixture-of-Experts (MoE) models enable conditional computation by routing
inputs to specialized experts, but these experts rely on identical inductive
biases, thus limiting representational diversity. This static computation
pathway is inefficient for inputs that require different types of reasoning and
limits specialization and interpretability. We propose Hecto, a lightweight MoE
architecture that leverages architectural heterogeneity by combining a GRU
expert for temporal reasoning and an FFNN expert for static abstraction under a
sparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG
News, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely
trails homogeneous baselines in performance despite receiving isolated input
representations, while achieving clear expert specialization, with each expert
aligning to distinct reasoning types (temporal vs static). At larger batch
sizes, Hecto exhibits improved performance, benefiting from relaxed
computational constraints that allow its heterogeneous architecture to optimize
more effectively. Ablation results isolate architectural diversity as the
source of Hecto's stability and interpretability across diverse reasoning
tasks. Overall, Hecto establishes itself as a new benchmark for conditional
computation, offering a principled framework for specialized reasoning in
low-resource regimes with its model strength derived from principled
specialization.

</details>


### [121] [Improving Rationality in the Reasoning Process of Language Models through Self-playing Game](https://arxiv.org/abs/2506.22920)
*Pinzheng Wang,Juntao Li,Zecheng Tang,Haijia Gui,Min zhang*

Main category: cs.AI

TL;DR: 本文提出一种名为Critic-Discernment Game (CDG)的自博弈框架，通过模拟批评与辨别过程，显著提升LLMs对自身推理过程的理解能力，无需人类或更强模型的监督。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型(LLMs)在数学和编程等任务中展现出强大的推理能力，但近期研究表明，即使是最好的模型也缺乏对其推理过程的真正理解。

Method: 设计了一个Critic-Discernment Game (CDG)。在此游戏中，证明者（prover）为问题提供解决方案，随后受到批评者（critiques）的挑战。批评旨在辅助或误导证明者。证明者的目标是在面对误导性评论时保持正确答案，同时根据建设性反馈纠正错误。

Result: 在数学推理、逐步错误检测、自我纠正和长链推理等任务上的实验表明，CDG训练能显著提高已良好对齐的LLM理解其推理过程的能力。

Conclusion: CDG是一种无需外部监督的有效方法，能够通过自博弈显著增强LLMs对其推理过程的理解和理性能力。

Abstract: Large language models (LLMs) have demonstrated considerable reasoning
abilities in various tasks such as mathematics and coding. However, recent
studies indicate that even the best models lack true comprehension of their
reasoning processes. In this paper, we explore how self-play can enhance the
rationality of models in the reasoning process without supervision from humans
or superior models. We design a Critic-Discernment Game(CDG) in which a prover
first provides a solution to a given problem and is subsequently challenged by
critiques of its solution. These critiques either aim to assist or mislead the
prover. The objective of the prover is to maintain the correct answer when
faced with misleading comments, while correcting errors in response to
constructive feedback. Our experiments on tasks involving mathematical
reasoning, stepwise error detection, self-correction, and long-chain reasoning
demonstrate that CDG training can significantly improve the ability of
well-aligned LLMs to comprehend their reasoning process.

</details>


### [122] [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/abs/2506.22992)
*Yulun Jiang,Yekun Chai,Maria Brbić,Michael Moor*

Main category: cs.AI

TL;DR: 提出MARBLE多模态推理基准，揭示当前多模态语言模型在复杂多模态逐步推理和感知方面的严重不足。


<details>
  <summary>Details</summary>
Motivation: 现有推理基准侧重文本或简单多模态检索，未能有效评估复杂多模态逐步推理能力，导致对多模态语言模型在该领域理解不足。

Method: 提出MARBLE基准，包含M-Portal和M-Cube两个挑战性任务，要求在空间、视觉和物理约束下进行多步骤规划和理解，以严格评估多模态语言模型的逐步推理能力。

Result: 当前12种先进的多模态语言模型在MARBLE上表现极差（M-Portal接近随机，M-Cube准确率为0%）。复杂推理仍是巨大挑战，且感知能力是主要瓶颈。

Conclusion: MARBLE揭示了当前多模态语言模型在复杂多模态推理和规划方面的显著局限性，旨在推动能进行多步骤多模态推理和规划的下一代模型的发展。

Abstract: The ability to process information from multiple modalities and to reason
through it step-by-step remains a critical challenge in advancing artificial
intelligence. However, existing reasoning benchmarks focus on text-only
reasoning, or employ multimodal questions that can be answered by directly
retrieving information from a non-text modality. Thus, complex reasoning
remains poorly understood in multimodal domains. Here, we present MARBLE, a
challenging multimodal reasoning benchmark that is designed to scrutinize
multimodal language models (MLLMs) in their ability to carefully reason
step-by-step through complex multimodal problems and environments. MARBLE is
composed of two highly challenging tasks, M-Portal and M-Cube, that require the
crafting and understanding of multistep plans under spatial, visual, and
physical constraints. We find that current MLLMs perform poorly on MARBLE --
all the 12 advanced models obtain near-random performance on M-Portal and 0%
accuracy on M-Cube. Only in simplified subtasks some models outperform the
random baseline, indicating that complex reasoning is still a challenge for
existing MLLMs. Moreover, we show that perception remains a bottleneck, where
MLLMs occasionally fail to extract information from the visual inputs. By
shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the
development of the next generation of models with the ability to reason and
plan across many, multimodal reasoning steps.

</details>


### [123] [AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks](https://arxiv.org/abs/2506.23049)
*Leander Melroy Maben,Gayathri Ganesh Lakshmy,Srijith Radhakrishnan,Siddhant Arora,Shinji Watanabe*

Main category: cs.AI

TL;DR: 本文介绍了AURA，首个开源的语音原生助手，能够通过动态工具调用和多轮对话完成复杂的任务。


<details>
  <summary>Details</summary>
Motivation: 尽管语言和语音技术不断进步，但目前尚无开源系统能实现包含工具使用和智能推理的全程语音到语音、多轮对话功能。

Method: AURA将开源的ASR（自动语音识别）、TTS（文本到语音）和LLM（大型语言模型）以级联管道形式结合，支持日历预订、联系人查询、网页搜索和电子邮件等工具，并采用模块化设计，允许通过自然语言提示轻松集成新工具。

Result: 在VoiceBench上，AURA在OpenBookQA任务中得分为92.75%，超越所有开源系统并接近GPT-4o；在AlpacaEval上得分为4.39，与其它开源系统相当。人工评估显示，在复杂的多轮语音任务中，任务成功率达到90%。

Conclusion: AURA是第一个开源的语音原生智能助手，它有效弥补了现有系统的空白，通过集成多模态技术和工具使用，展示了在复杂语音对话任务中的卓越性能和实用潜力。

Abstract: Despite advances in language and speech technologies, no open-source system
enables full speech-to-speech, multi-turn dialogue with integrated tool use and
agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and
Automated Tool Use), the first open-source, speech-native assistant capable of
completing complex, goal-driven tasks through dynamic tool invocation and
multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a
cascaded pipeline and supports tools such as calendar booking, contact lookup,
web search, and email. Its modular design allows easy integration of new tools
using natural language prompts and action classes. On VoiceBench, AURA scores
92.75% on OpenBookQA-outperforming all open-weight systems and nearing
GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.
Human evaluation shows 90% task success on complex, multi-turn speech tasks.

</details>


### [124] [AI's Euclid's Elements Moment: From Language Models to Computable Thought](https://arxiv.org/abs/2506.23080)
*Xinmin Fang,Lingfeng Tao,Zhengxiong Li*

Main category: cs.AI

TL;DR: 本文提出了一个五阶段的AI演化框架，类比人类认知技术发展史，旨在解释AI的过去架构变迁并规划未来的发展路径，特别是当前正处于“元语言时刻”，并展望最终实现可验证对齐的可靠AI。


<details>
  <summary>Details</summary>
Motivation: 理解人工智能的全面发展轨迹；提供一个系统、跨学科的模型，解释AI的历史架构转变并描绘前进方向；探讨AI发展的“如何”实现，为未来研究奠定理论基础，并为开发者提供具体可行的策略。

Method: 提出了一个“认知几何学”的五阶段演化框架，将AI的发展轨迹类比人类认知技术（如楔形文字、字母表、语法逻辑、微积分和形式逻辑系统）的历史进程；分析AI在表示和推理能力上革命性转变的各个“时代”；论证AI演化并非线性而是反射性，工具和洞察力能反向重塑其架构；识别并预测当前的“元语言时刻”以及未来的“数学符号时刻”和“形式逻辑系统时刻”；建议通过神经-符号架构和程序合成实现未来的发展。

Result: 提出了一个名为“认知几何学”的框架，成功解释了AI过去的架构转变（如从专家系统到Transformer）；为AI未来的发展描绘了一条具体且规范的路径；揭示了AI演化过程中的反射性（反馈循环）机制；识别出AI当前正过渡到“元语言时刻”，表现为思维链和宪法式AI等自反思能力；预测了未来阶段将通过可计算的思维演算（可能通过神经-符号架构和程序合成）发展，最终实现可验证对齐和可靠的AI，能够重构其自身的基础表征。

Conclusion: 本工作是作者AI研究三部曲的理论方法部分，旨在解答AI“如何”发展的问题。它为未来的研究奠定了坚实的理论基础，并为旨在构建下一代智能系统的初创公司和开发者提供了具体可行的策略，最终目标是实现可验证对齐且可靠的AI。

Abstract: This paper presents a comprehensive five-stage evolutionary framework for
understanding the development of artificial intelligence, arguing that its
trajectory mirrors the historical progression of human cognitive technologies.
We posit that AI is advancing through distinct epochs, each defined by a
revolutionary shift in its capacity for representation and reasoning, analogous
to the inventions of cuneiform, the alphabet, grammar and logic, mathematical
calculus, and formal logical systems. This "Geometry of Cognition" framework
moves beyond mere metaphor to provide a systematic, cross-disciplinary model
that not only explains AI's past architectural shifts-from expert systems to
Transformers-but also charts a concrete and prescriptive path forward.
Crucially, we demonstrate that this evolution is not merely linear but
reflexive: as AI advances through these stages, the tools and insights it
develops create a feedback loop that fundamentally reshapes its own underlying
architecture. We are currently transitioning into a "Metalinguistic Moment,"
characterized by the emergence of self-reflective capabilities like
Chain-of-Thought prompting and Constitutional AI. The subsequent stages, the
"Mathematical Symbolism Moment" and the "Formal Logic System Moment," will be
defined by the development of a computable calculus of thought, likely through
neuro-symbolic architectures and program synthesis, culminating in provably
aligned and reliable AI that reconstructs its own foundational representations.
This work serves as the methodological capstone to our trilogy, which
previously explored the economic drivers ("why") and cognitive nature ("what")
of AI. Here, we address the "how," providing a theoretical foundation for
future research and offering concrete, actionable strategies for startups and
developers aiming to build the next generation of intelligent systems.

</details>


### [125] [Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study](https://arxiv.org/abs/2506.23107)
*Bing Song,Jianing Liu,Sisi Jian,Chenyang Wu,Vinayak Dixit*

Main category: cs.AI

TL;DR: 本研究发现大型语言模型（LLMs）在模拟人类风险决策时比人类更倾向于风险规避，其中ChatGPT o1-mini更接近人类表现，但中文提示下模型预测与实际响应偏差更大，表明提示语言会影响模拟效果。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs应用日益广泛，对其在模拟复杂决策（如风险决策，即单次选择可导致多种结果）方面可靠性的担忧日益增加。本研究旨在探究LLMs模拟风险决策场景的能力。

Method: 研究通过一系列基于彩票的任务，比较了ChatGPT 4o和ChatGPT o1-mini生成的决策与来自悉尼、达卡、香港和南京的人类实际响应。研究向LLMs提供了人口统计学输入，并要求其预测个体选择。风险偏好分析采用了恒定相对风险规避（CRRA）框架。此外，还分析了来自南京和香港的多语言数据，以探究提示语言对模拟性能的影响。

Result: 研究结果显示，两个LLM模型都比人类参与者表现出更强的风险规避行为。其中，ChatGPT o1-mini与观察到的人类决策更为接近。对南京和香港多语言数据的进一步分析表明，中文提示下模型预测与实际响应的偏差大于英文提示，这提示提示语言可能影响模拟性能。

Conclusion: 这些发现突显了LLMs在复制类人风险行为方面的潜力和现有局限性，尤其是在考虑语言和文化背景时。

Abstract: Large language models (LLMs) have made significant strides, extending their
applications to dialogue systems, automated content creation, and
domain-specific advisory tasks. However, as their use grows, concerns have
emerged regarding their reliability in simulating complex decision-making
behavior, such as risky decision-making, where a single choice can lead to
multiple outcomes. This study investigates the ability of LLMs to simulate
risky decision-making scenarios. We compare model-generated decisions with
actual human responses in a series of lottery-based tasks, using transportation
stated preference survey data from participants in Sydney, Dhaka, Hong Kong,
and Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and
ChatGPT o1-mini -- which were tasked with predicting individual choices. Risk
preferences were analyzed using the Constant Relative Risk Aversion (CRRA)
framework. Results show that both models exhibit more risk-averse behavior than
human participants, with o1-mini aligning more closely with observed human
decisions. Further analysis of multilingual data from Nanjing and Hong Kong
indicates that model predictions in Chinese deviate more from actual responses
compared to English, suggesting that prompt language may influence simulation
performance. These findings highlight both the promise and the current
limitations of LLMs in replicating human-like risk behavior, particularly in
linguistic and cultural settings.

</details>


### [126] [The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy](https://arxiv.org/abs/2506.23123)
*Rishi Bommasani*

Main category: cs.AI

TL;DR: 本学位论文分析人工智能时代技术与社会如何共同演进，深入理解基础模型的能力、风险及供应链，旨在为制定循证的AI政策和更好的AI治理提供科学基础和研究-政策接口。


<details>
  <summary>Details</summary>
Motivation: 基础模型作为人工智能的核心技术，虽能力卓越但理解不足且可能带来广泛危害，引发社会困惑。因此，有必要深入解释AI时代技术与社会如何共生演进，以期实现更好的社会成果。

Method: 本研究围绕三个主题展开：首先，构建概念框架，分析基础模型的能力、风险及其在宏观经济中的供应链；其次，通过模型层面的评估和组织层面的指标来增强透明度和实证洞察；最后，将对基础模型社会影响的深入理解转化为行动，推动循证的AI政策制定。

Result: 研究成果在于深化了对AI时代技术与社会共同演进的理解，并通过评估和指标提升了透明度，从而对基础模型的社会影响有了更清晰的认识。这些洞察旨在为制定基于证据的AI政策提供支持。

Conclusion: 本学位论文通过构建AI治理所需的科学基础和研究-政策接口，为在人工智能时代实现更优的社会成果做出了重要贡献。

Abstract: Artificial intelligence is humanity's most promising technology because of
the remarkable capabilities offered by foundation models. Yet, the same
technology brings confusion and consternation: foundation models are poorly
understood and they may precipitate a wide array of harms. This dissertation
explains how technology and society coevolve in the age of AI, organized around
three themes. First, the conceptual framing: the capabilities, risks, and the
supply chain that grounds foundation models in the broader economy. Second, the
empirical insights that enrich the conceptual foundations: transparency created
via evaluations at the model level and indexes at the organization level.
Finally, the transition from understanding to action: superior understanding of
the societal impact of foundation models advances evidence-based AI policy.
View together, this dissertation makes inroads into achieving better societal
outcomes in the age of AI by building the scientific foundations and
research-policy interface required for better AI governance.

</details>


### [127] [Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons](https://arxiv.org/abs/2506.23128)
*Chi Chiu So,Yueyue Sun,Jun-Min Wang,Siu Pang Yung,Anthony Wai Keung Loh,Chun Pong Chau*

Main category: cs.AI

TL;DR: 该研究评估了DeepSeek-R1、DeepSeek-V3和GPT-4o在深度关系推理任务上的表现，发现DeepSeek-R1表现最佳，但所有模型在复杂性增加时均受限，并分析了DeepSeek-R1的思维链推理。


<details>
  <summary>Details</summary>
Motivation: 旨在评估大型语言模型（LLMs）在执行深度关系推理方面的当前能力和距离。

Method: 通过在家族树和通用图推理领域设计的一系列基准任务，评估并比较了DeepSeek-R1、DeepSeek-V3和GPT-4o三种前沿LLM的推理能力。此外，对DeepSeek-R1的思维链（Chain-of-Thought）响应进行了详细分析。

Result: 实验显示，DeepSeek-R1在多项任务和不同问题规模下持续获得最高的F1分数，展现出在逻辑演绎和关系推理方面的强大能力。然而，所有被评估模型（包括DeepSeek-R1）在问题复杂性增加时，由于Token长度限制和输出结构不完整等原因，表现显著下降。对DeepSeek-R1长思维链响应的分析揭示了其独特的规划和验证策略，但也指出存在不连贯或不完整的推理实例。

Conclusion: 研究结果为提升LLM的推理能力提供了实证洞察和理论启示，尤其是在需要结构化、多步骤逻辑推理的任务中。强调需要更深入地审视LLM的内部推理动态，并指出未来工作可关注多模态推理和系统性检查推理失败的方向。

Abstract: How far are Large Language Models (LLMs) in performing deep relational
reasoning? In this paper, we evaluate and compare the reasoning capabilities of
three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a
suite of carefully designed benchmark tasks in family tree and general graph
reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the
highest F1-scores across multiple tasks and problem sizes, demonstrating strong
aptitude in logical deduction and relational inference. However, all evaluated
models, including DeepSeek-R1, struggle significantly as problem complexity
increases, largely due to token length limitations and incomplete output
structures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought
responses uncovers its unique planning and verification strategies, but also
highlights instances of incoherent or incomplete reasoning, calling attention
to the need for deeper scrutiny into LLMs' internal inference dynamics. We
further discuss key directions for future work, including the role of
multimodal reasoning and the systematic examination of reasoning failures. Our
findings provide both empirical insights and theoretical implications for
advancing LLMs' reasoning abilities, particularly in tasks that demand
structured, multi-step logical inference. Our code repository will be publicly
available at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.

</details>


### [128] [Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing](https://arxiv.org/abs/2506.23141)
*Siyuan Li,Ruitong Liu,Yan Wen,Te Sun*

Main category: cs.AI

TL;DR: 本文提出一种语义感知关系消息传递方法，通过语义感知Top-K邻居选择和多头注意力聚合器，有效过滤无关信息，提升知识图谱补全性能。


<details>
  <summary>Details</summary>
Motivation: 知识图谱补全中，语义上下文至关重要，但传统节点消息传递机制因无差别聚合邻居信息，易引入噪声、稀释信息或导致过平滑问题。

Method: 提出语义感知关系消息传递框架。核心创新包括：1) 语义感知Top-K邻居选择策略，评估中心节点与入射边的语义相关性并选择最相关的Top-K条边；2) 多头注意力聚合器，将选定边的信息与中心节点表示融合，生成语义聚焦的节点消息。该方法旨在捕获并传播与链接预测任务最相关的上下文信息，从而减轻不相关信息的干扰。

Result: 在多个已建立的基准测试中，所提出的方法相比现有方法取得了优越的性能。

Conclusion: 所提出的语义感知关系消息传递方法，通过智能选择和有效聚合相关语义信息，成功缓解了知识图谱补全中的噪声和信息稀释问题，显著提升了预测准确性。

Abstract: Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge
Graph Completion (KGC), providing vital cues for prediction. However,
traditional node-based message passing mechanisms, when applied to knowledge
graphs, often introduce noise and suffer from information dilution or
over-smoothing by indiscriminately aggregating information from all neighboring
edges. To address this challenge, we propose a semantic-aware relational
message passing. A core innovation of this framework is the introduction of a
\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this
strategy first evaluates the semantic relevance between a central node and its
incident edges within a shared latent space, selecting only the Top-K most
pertinent ones. Subsequently, information from these selected edges is
effectively fused with the central node's own representation using a
\textbf{multi-head attention aggregator} to generate a semantically focused
node message. In this manner, our model not only leverages the structure and
features of edges within the knowledge graph but also more accurately captures
and propagates the contextual information most relevant to the specific link
prediction task, thereby effectively mitigating interference from irrelevant
information. Extensive experiments demonstrate that our method achieves
superior performance compared to existing approaches on several established
benchmarks.

</details>


### [129] [Rises for Measuring Local Distributivity in Lattices](https://arxiv.org/abs/2506.23168)
*Mohammad Abdulla,Tobias Hille,Dominik Dürrschnabel,Gerd Stumme*

Main category: cs.AI

TL;DR: 针对形式概念分析中格的分配性缺乏量化方法的问题，本文引入了“上升”概念来衡量分配性，证明了格的分配性与“非单位上升”的缺失等价，并分析了真实数据中概念格的交/并分配性。


<details>
  <summary>Details</summary>
Motivation: 在数据分析（特别是形式概念分析，FCA）中，格常表现出高度的分配性，但目前缺乏量化此性质的标准方法。

Method: 引入了“上升（rises）”的概念来评估（概念）格的分配性，该概念捕获了覆盖概念中属性或对象数量的变化。将“上升”与经典的交（meet-）和并（join-）分配性关联起来，并研究了并分配性在有序集层面的表现。

Result: 证明了一个格是分配的当且仅当没有非单位上升发生。观察到来自真实世界数据的概念格在很大程度上是并分配的，但交分配性则低得多。

Conclusion: “上升”的概念提供了一种量化格分配性的新方法，并揭示了真实世界概念格在并分配性方面表现突出，而在交分配性方面表现较弱的特点。

Abstract: Distributivity is a well-established and extensively studied notion in
lattice theory. In the context of data analysis, particularly within Formal
Concept Analysis (FCA), lattices are often observed to exhibit a high degree of
distributivity. However, no standardized measure exists to quantify this
property. In this paper, we introduce the notion of rises in (concept) lattices
as a means to assess distributivity. Rises capture how the number of attributes
or objects in covering concepts change within the concept lattice. We show that
a lattice is distributive if and only if no non-unit rises occur. Furthermore,
we relate rises to the classical notion of meet- and join distributivity. We
observe that concept lattices from real-world data are to a high degree
join-distributive, but much less meet-distributive. We additionally study how
join-distributivity manifests on the level of ordered sets.

</details>


### [130] [FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis](https://arxiv.org/abs/2506.23273)
*Quang Hung Nguyen,Phuong Anh Trinh,Phan Quoc Hung Mai,Tuan Phong Trinh*

Main category: cs.AI

TL;DR: FinStat2SQL是一个轻量级text2sql流水线，通过多智能体结合大小语言模型，实现对财务报表的自然语言查询。它专门为越南本地标准优化，并在消费级硬件上表现优于GPT-4o-mini，提供可扩展且经济高效的金融分析方案。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型进步显著，但text2sql在处理复杂和领域特定查询时仍面临挑战。金融领域的数据库设计和报告格式因实体和国家而异，使得text2sql应用更为困难。

Method: 本文提出了FinStat2SQL，一个轻量级的text2sql流水线。它采用多智能体设置，结合大型和小型语言模型进行实体提取、SQL生成和自校正。研究团队构建了一个领域特定数据库，并在合成的问答数据集上评估了模型。

Result: 一个经过微调的7B模型在消费级硬件上实现了61.33%的准确率，响应时间低于4秒，并且性能优于GPT-4o-mini。

Conclusion: FinStat2SQL为金融分析提供了一个可扩展、经济高效的解决方案，使得越南企业能够便捷地使用AI驱动的查询功能。

Abstract: Despite the advancements of large language models, text2sql still faces many
challenges, particularly with complex and domain-specific queries. In finance,
database designs and financial reporting layouts vary widely between financial
entities and countries, making text2sql even more challenging. We present
FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries
over financial statements. Tailored to local standards like VAS, it combines
large and small language models in a multi-agent setup for entity extraction,
SQL generation, and self-correction. We build a domain-specific database and
evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves
61.33\% accuracy with sub-4-second response times on consumer hardware,
outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient
solution for financial analysis, making AI-powered querying accessible to
Vietnamese enterprises.

</details>


### [131] [Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games](https://arxiv.org/abs/2506.23276)
*David Guzman Piedrahita,Yongjin Yang,Mrinmaya Sachan,Giorgia Ramponi,Bernhard Schölkopf,Zhijing Jin*

Main category: cs.AI

TL;DR: 研究探究了多智能体大型语言模型（LLMs）系统中代价高昂的制裁挑战，通过公共物品博弈揭示了LLMs的四种合作行为模式，并发现推理型LLMs在合作方面表现不佳，而某些传统LLMs则能保持高水平合作。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs作为自主智能体的部署日益增多，理解它们的合作和社会机制变得至关重要，特别是LLMs如何在自身利益和集体福祉之间取得平衡，以确保对齐、鲁棒性和安全部署。本文具体关注多智能体LLM系统中代理必须决定是否投资自身资源来激励合作或惩罚背叛的代价高昂的制裁问题。

Method: 研究采纳了行为经济学中的一种公共物品博弈，并加入了制度选择，以观察不同LLMs在重复交互中如何应对社会困境。

Result: 分析揭示了LLMs的四种不同行为模式：一些能持续建立并维持高水平合作；一些在参与和脱离之间波动；一些合作行为随时间逐渐下降；另一些则无论结果如何都遵循固定策略。令人惊讶的是，推理型LLMs（如o1系列）在合作方面表现挣扎，而某些传统LLMs却能始终实现高水平合作。

Conclusion: 研究结果表明，目前侧重于增强LLMs推理能力的改进方法不一定能带来合作，这为在需要持续协作的环境中部署LLM智能体提供了宝贵的见解。

Abstract: As large language models (LLMs) are increasingly deployed as autonomous
agents, understanding their cooperation and social mechanisms is becoming
increasingly important. In particular, how LLMs balance self-interest and
collective well-being is a critical challenge for ensuring alignment,
robustness, and safe deployment. In this paper, we examine the challenge of
costly sanctioning in multi-agent LLM systems, where an agent must decide
whether to invest its own resources to incentivize cooperation or penalize
defection. To study this, we adapt a public goods game with institutional
choice from behavioral economics, allowing us to observe how different LLMs
navigate social dilemmas over repeated interactions. Our analysis reveals four
distinct behavioral patterns among models: some consistently establish and
sustain high levels of cooperation, others fluctuate between engagement and
disengagement, some gradually decline in cooperative behavior over time, and
others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we
find that reasoning LLMs, such as the o1 series, struggle significantly with
cooperation, whereas some traditional LLMs consistently achieve high levels of
cooperation. These findings suggest that the current approach to improving
LLMs, which focuses on enhancing their reasoning capabilities, does not
necessarily lead to cooperation, providing valuable insights for deploying LLM
agents in environments that require sustained collaboration. Our code is
available at https://github.com/davidguzmanp/SanctSim

</details>


### [132] [GATSim: Urban Mobility Simulation with Generative Agents](https://arxiv.org/abs/2506.23306)
*Qi Liu,Can Li,Wanjing Ma*

Main category: cs.AI

TL;DR: GATSim是一个利用大型语言模型和AI智能体技术，构建具有丰富行为特征的生成式智能体，用于城市交通模拟的新框架，能更真实地模拟人类出行决策。


<details>
  <summary>Details</summary>
Motivation: 传统的城市交通模拟依赖僵化的规则系统，无法捕捉人类出行决策的复杂性、适应性和行为多样性，而大型语言模型和AI智能体技术为创建具有推理、记忆和学习能力的智能体提供了新机遇。

Method: 本文提出了GATSim（生成式智能体交通模拟）框架，它结合了城市交通基础模型、智能体认知系统和交通模拟环境。GATSim智能体拥有多样化的社会经济属性、生活方式和偏好，并通过心理学记忆系统、工具使用能力和终身学习机制来做出出行决策。此外，智能体通过设计反思过程，将出行经验转化为通用洞察，实现行为适应和实时反应。

Result: 实验表明，GATSim中的生成式智能体在出行场景中的表现与人类标注者相当，并且能够自然地产生宏观交通演变模式，证明了其生成的出行行为的可信度。

Conclusion: GATSim框架成功地利用生成式智能体技术，实现了对城市交通中人类复杂、适应性行为的更真实模拟，弥补了传统基于规则系统的不足，为城市交通研究提供了新的工具。

Abstract: Traditional agent-based urban mobility simulations rely on rigid rule-based
systems that fail to capture the complexity, adaptability, and behavioral
diversity characteristic of human travel decision-making. Recent advances in
large language models and AI agent technology offer opportunities to create
agents with reasoning capabilities, persistent memory, and adaptive learning
mechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel
framework that leverages these advances to create generative agents with rich
behavioral characteristics for urban mobility simulation. Unlike conventional
approaches, GATSim agents possess diverse socioeconomic attributes, individual
lifestyles, and evolving preferences that shape their mobility decisions
through psychologically-informed memory systems, tool usage capabilities, and
lifelong learning mechanisms. The main contributions of this study include: (1)
a comprehensive architecture combining an urban mobility foundation model with
agent cognitive systems and transport simulation environment, (2) a fully
functional prototype implementation, and (3) systematic validation
demonstrating that generative agents produce believable travel behaviors.
Through designed reflection processes, generative agents in this study can
transform specific travel experiences into generalized insights, enabling
realistic behavioral adaptation over time with specialized mechanisms for
activity planning and real-time reactive behaviors tailored to urban mobility
contexts. Experiments show that generative agents perform competitively with
human annotators in mobility scenarios while naturally producing macroscopic
traffic evolution patterns. The code for the prototype system is shared at
https://github.com/qiliuchn/gatsim.

</details>


### [133] [The Confidence Paradox: Can LLM Know When It's Wrong](https://arxiv.org/abs/2506.23464)
*Sahil Tripathi,Md Tabrez Nafis,Imran Hussain,Jiechao Gao*

Main category: cs.AI

TL;DR: 本文提出HonestVQA，一个自监督的诚实校准框架，旨在解决文档视觉问答（DocVQA）系统中模型置信度与实际知识不对齐的问题，从而提高准确性并降低过自信。


<details>
  <summary>Details</summary>
Motivation: DocVQA系统在实际应用中日益普及，但存在伦理不透明性，常对模糊问题过度自信或未能可靠地传达不确定性，导致模型置信度与实际知识不符，特别在需要伦理问责的领域构成风险。现有先进方法（如LayoutLMv3、UDOP、DONUT）侧重性能而非伦理响应能力。

Method: 引入HonestVQA，一个模型无关的自监督诚实校准框架。它通过量化不确定性识别知识差距，使用加权损失函数使模型置信度与正确性对齐，并通过对比学习强制执行伦理响应行为。同时，提出了Honesty Score (H-Score)和Ethical Confidence Index (ECI)两个评估指标，用于衡量置信度、准确性和伦理沟通的对齐情况。

Result: HonestVQA在SpDocVQA、InfographicsVQA和SROIE数据集上将DocVQA准确率和F1分数提高了4.3%。它显著降低了过自信，H-Score和ECI分别降低了0.072和0.078。在跨域评估中，它实现了高达78.9%的准确率和76.1%的F1分数，显示出强大的泛化能力。消融实验表明，在没有对齐或对比损失的情况下，准确率下降了3.8%。

Conclusion: HonestVQA通过诚实校准有效解决了DocVQA的伦理问题，显著提高了系统准确性、降低了过自信，并展现出良好的泛化能力，证明了其核心组件的重要性。

Abstract: Document Visual Question Answering (DocVQA) systems are increasingly deployed
in real world applications, yet they remain ethically opaque-often producing
overconfident answers to ambiguous questions or failing to communicate
uncertainty in a trustworthy manner. This misalignment between model confidence
and actual knowledge poses significant risks, particularly in domains requiring
ethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT
have advanced SOTA performance by focusing on architectural sophistication and
accuracy; however, they fall short in ethical responsiveness.
  To address these limitations, we introduce HonestVQA, a self-supervised
honesty calibration framework for ethically aligned DocVQA. Our model-agnostic
method quantifies uncertainty to identify knowledge gaps, aligns model
confidence with actual correctness using weighted loss functions, and enforces
ethical response behavior via contrastive learning. We further introduce two
principled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence
Index (ECI)--to benchmark alignment between confidence, accuracy, and ethical
communication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%
and F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces
overconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In
cross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,
demonstrating strong generalization. Ablation shows a 3.8% drop in accuracy
without alignment or contrastive loss.

</details>


### [134] [Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence](https://arxiv.org/abs/2506.23503)
*Bosubabu Sambana,Kondreddygari Archana,Suram Indhra Sena Reddy,Shaik Meethaigar Jameer Basha,Shaik Karishma*

Main category: cs.AI

TL;DR: 该研究提出了一个基于CBT框架的系统，利用BERT、RoBERTa、T5等深度学习模型分析社交媒体上的文本和视觉内容，以检测负面情绪、认知扭曲，并预测多种精神健康障碍，旨在辅助心理治疗师进行早期干预。


<details>
  <summary>Details</summary>
Motivation: 认知行为疗法（CBT）虽有效，但需精准识别认知路径。数字时代，社交媒体上存在大量个体表达的负面情绪和潜在认知扭曲，甚至自杀倾向，但缺乏有效分析这些在线认知路径的方法，难以供心理治疗师及时干预。现有模型主要识别负面想法，无法全面预测更广泛的精神健康障碍。

Method: 提出一个结合接受、承诺和数据增强的认知行为疗法（CBT）框架，用于分类文本和视觉内容的正负性。具体采用BERT、RoBERTa进行情感分析，T5、PEGASUS进行文本摘要，以及mT5进行多语言文本翻译，以检测社交媒体数据中的负面情绪和认知扭曲。系统还扩展了功能，可预测额外的负面副作用和其他潜在的精神健康障碍，如恐惧症和饮食失调。

Result: 该系统能够检测社交媒体数据中的负面情绪和认知扭曲。相较于现有模型，它还能预测额外的负面副作用以及包括恐惧症、饮食失调在内的多种潜在精神健康障碍，实现了更全面的分析能力。

Conclusion: 该系统提供了一种更全面的理解和干预策略，为心理治疗师提供了在早期检测和治疗各种心理问题方面的强大工具。

Abstract: Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the
irrational thought patterns associated with mental health disorders, but its
effectiveness relies on accurately identifying cognitive pathways to provide
targeted treatment. In today's digital age, individuals often express negative
emotions on social media, where they may reveal cognitive distortions, and in
severe cases, exhibit suicidal tendencies. However, there is a significant gap
in methodologies designed to analyze these cognitive pathways, which could be
critical for psychotherapists aiming to deliver timely and effective
interventions in online environments. Cognitive Behavioral Therapy (CBT)
framework leveraging acceptance, commitment and data augmentation to categorize
and address both textual and visual content as positive or negative.
Specifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,
PEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages
focusing on detecting negative emotions and cognitive distortions within social
media data. While existing models are primarily designed to identify negative
thoughts, the proposed system goes beyond this by predicting additional
negative side effects and other potential mental health disorders likes
Phobias, Eating Disorders. This enhancement allows for a more comprehensive
understanding and intervention strategy, offering psychotherapists a powerful
tool for early detection and treatment of various psychological issues.

</details>


### [135] [Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM](https://arxiv.org/abs/2506.23504)
*Bosubabu Sambana,Kotamsetty Geethika Devi,Bandi Rajeswara Reddy,Galeti Mohammad Hussain,Gownivalla Siddartha*

Main category: cs.AI

TL;DR: 提出了一种结合AlexNet和LSTM的混合深度学习模型，用于电价预测，通过整合外部变量，在准确性上显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 传统电价预测方法准确性不足，且未能充分分析数据（仅关注需求和价格），导致预测不准确。因此，需要一种能有效处理时间序列数据并考虑外部变量的新方法。

Method: 本研究采用了一种混合深度学习方法，结合了AlexNet（用于特征提取）和LSTM（用于学习序列模式）。该模型利用历史数据，并纳入了需求、温度、日照和降雨等关键外部变量。数据预处理包括最小-最大归一化和时间窗口应用。

Result: 所提出的混合模型在电价预测中取得了97.08%的准确率，显著高于单独的RNN（96.64%）和ANN（96.63%）模型。

Conclusion: 结合AlexNet和LSTM的混合模型能够有效提升电价预测的准确性，证明了利用深度学习和整合外部变量在复杂时间序列预测中的优越性。

Abstract: The recent development of advanced machine learning methods for hybrid models
has greatly addressed the need for the correct prediction of electrical prices.
This method combines AlexNet and LSTM algorithms, which are used to introduce a
new model with higher accuracy in price forecasting. Despite RNN and ANN being
effective, they often fail to deal with forex time sequence data. The
traditional methods do not accurately forecast the prices. These traditional
methods only focus on demand and price which leads to insufficient analysis of
data. To address this issue, using the hybrid approach, which focuses on
external variables that also effect the predicted prices. Nevertheless, due to
AlexNet's excellent feature extraction and LSTM's learning sequential patterns,
the prediction accuracy is vastly increased. The model is built on the past
data, which has been supplied with the most significant elements like demand,
temperature, sunlight, and rain. For example, the model applies methods, such
as minimum-maximum scaling and a time window, to predict the electricity prices
of the future. The results show that this hybrid model is good than the
standalone ones in terms of accuracy. Although we got our accuracy rating of
97.08, it shows higher accompaniments than remaining models RNN and ANN with
accuracies of 96.64 and 96.63 respectively.

</details>


### [136] [Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays](https://arxiv.org/abs/2506.23517)
*Selin Dik,Osman Erdem,Mehmet Dik*

Main category: cs.AI

TL;DR: 该研究评估了AI检测工具GPTZero的可靠性，发现它能有效识别AI生成文本，但对人类撰写文本存在误报，因此建议教师谨慎使用。


<details>
  <summary>Details</summary>
Motivation: 随着学生AI工具使用率的增加，教师开始使用AI检测工具，但这些工具的可靠性仍不确定。

Method: 研究收集了28篇AI生成文章和50篇人类撰写文章，并根据长度（短、中、长）进行分类。使用GPTZero检测每篇文章，并记录AI生成百分比和置信度。

Result: GPTZero能准确检测出绝大多数AI生成文章（置信度91-100%），但对人类撰写文章的检测结果波动较大，并出现少量误报。

Conclusion: 研究结果表明，尽管GPTZero在检测纯AI生成内容方面有效，但其区分人类撰写文本的可靠性有限。因此，教育工作者应谨慎单独依赖AI检测工具。

Abstract: As the use of AI tools by students has become more prevalent, instructors
have started using AI detection tools like GPTZero and QuillBot to detect AI
written text. However, the reliability of these detectors remains uncertain. In
our study, we focused mostly on the success rate of GPTZero, the most-used AI
detector, in identifying AI-generated texts based on different lengths of
randomly submitted essays: short (40-100 word count), medium (100-350 word
count), and long (350-800 word count). We gathered a data set consisting of
twenty-eight AI-generated papers and fifty human-written papers. With this
randomized essay data, papers were individually plugged into GPTZero and
measured for percentage of AI generation and confidence. A vast majority of the
AI-generated papers were detected accurately (ranging from 91-100% AI believed
generation), while the human generated essays fluctuated; there were a handful
of false positives. These findings suggest that although GPTZero is effective
at detecting purely AI-generated content, its reliability in distinguishing
human-authored texts is limited. Educators should therefore exercise caution
when relying solely on AI detection tools.

</details>


### [137] [ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data](https://arxiv.org/abs/2506.23520)
*Yu Zhang,Ruijie Yu,Jidong Tian,Feng Zhu,Jiapeng Liu,Xiaokang Yang,Yaohui Jin,Yanyan Xu*

Main category: cs.AI

TL;DR: ChemActor是一个基于大型语言模型（LLM）的系统，旨在将非结构化化学实验规程转换为结构化的机器可执行动作序列，通过LLM生成数据增强和创新评估方法，实现了化学规程自动化提取的最新性能。


<details>
  <summary>Details</summary>
Motivation: 随着有机化学中机器人合成的兴起，从文献中自动化提取化学规程变得至关重要。然而，该任务因化学语言固有的模糊性以及高质量人工标注数据的高成本而面临巨大挑战。

Method: 本文提出了ChemActor，一个经过完全微调的大型语言模型（LLM），作为化学执行器，用于在非结构化实验规程和结构化动作序列之间进行转换。为解决标注数据不足和质量低的问题，作者提出了一个顺序的LLM生成数据框架，该框架集成了一个基于分布差异的数据选择模块和一个通用LLM来生成机器可执行动作。此外，引入了一种新颖的多轮LLMs循环审查指标，以评估模型对化学实验规程的理解能力。

Result: 在反应到描述（R2D）和描述到动作（D2A）任务上的广泛实验表明，经过LLM生成数据增强的ChemActor取得了最先进的性能，相比基线模型性能提升了10%。

Conclusion: ChemActor成功解决了化学规程自动化提取中数据稀缺和语言模糊的挑战，通过其基于LLM的方法和数据增强策略，显著提高了转换准确性，为未来的机器人化学合成奠定了坚实基础。

Abstract: With the increasing interest in robotic synthesis in the context of organic
chemistry, the automated extraction of chemical procedures from literature is
critical. However, this task remains challenging due to the inherent ambiguity
of chemical language and the high cost of human annotation required for
developing reliable computer-aided extraction protocols. Here, we present
ChemActor, a fully fine-tuned large language model (LLM), as a chemical
executor to convert between unstructured experimental procedures and structured
action sequences. We propose a sequential LLM-generated data framework to
address the challenges of insufficient and low-quality annotated data. This
framework integrates a data selection module that selects data based on
distribution divergence, with a general-purpose LLM, to generate
machine-executable actions from a single molecule input. Additionally, we
introduce a novel multi-round LLMs circle review metric, which reflects the
model's advanced understanding of chemical experimental procedures. Extensive
experiments on reaction-to-description (R2D) and description-to-action (D2A)
tasks demonstrate that ChemActor, augmented by LLM-generated data, achieves
state-of-the-art performance, outperforming the baseline model by 10%. The code
is available at: https://github.com/Zhanghahah/ChemActor.

</details>


### [138] [CooT: Learning to Coordinate In-Context with Coordination Transformers](https://arxiv.org/abs/2506.23549)
*Huai-Chih Wang,Hsiang-Chun Chuang,Hsi-Chun Cheng,Dai-Jie Wu,Shao-Hua Sun*

Main category: cs.AI

TL;DR: 本研究提出Coordination Transformers (CooT)，一个利用近期交互历史实现快速适应未知合作对象的上下文协调框架，在Overcooked基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，动态不确定环境下的人工智能体之间有效协调是一个重大挑战。现有方法（如自我博弈和基于种群的方法）对未知合作对象的泛化能力差或需要大量训练。

Method: 我们提出Cooperation Transformers (CooT)，一个新颖的上下文协调框架。它利用近期交互历史来快速适应未知合作对象，通过预测与观察到的合作对象交互相符的行动。CooT在从具有互补行为的不同智能体对收集的交互轨迹上进行训练，无需显式监督或微调，即可快速学习有效的协调策略。

Result: 在Overcooked基准测试中的评估表明，CooT在涉及先前未知合作对象的协调任务中显著优于基线方法。人类评估进一步证实CooT是最有效的协作伙伴，而广泛的消融实验凸显了其在多智能体场景中的鲁棒性、灵活性和对上下文的敏感性。

Conclusion: CooT是一个有效且鲁棒的上下文协调框架，能够快速适应未知合作对象，解决了以往方法的局限性，并在多智能体协调中展现出卓越性能。

Abstract: Effective coordination among artificial agents in dynamic and uncertain
environments remains a significant challenge in multi-agent systems. Existing
approaches, such as self-play and population-based methods, either generalize
poorly to unseen partners or require extensive training. To overcome these
limitations, we propose Coordination Transformers (CooT), a novel in-context
coordination framework that uses recent interaction histories to adapt to
unseen partners rapidly. Unlike previous approaches that primarily aim to
increase the diversity of training partners, CooT explicitly focuses on
adapting to new partner behaviors by predicting actions aligned with observed
partner interactions. Trained on interaction trajectories collected from
diverse pairs of agents with complementary behaviors, CooT quickly learns
effective coordination strategies without explicit supervision or fine-tuning.
Evaluations on the Overcooked benchmark demonstrate that CooT significantly
outperforms baseline methods in coordination tasks involving previously unseen
partners. Human evaluations further confirm CooT as the most effective
collaborative partner, while extensive ablations highlight its robustness,
flexibility, and sensitivity to context in multi-agent scenarios.

</details>


### [139] [MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI](https://arxiv.org/abs/2506.23563)
*Huanjin Yao,Jiaxing Huang,Yawen Qiu,Michael K. Chen,Wenzheng Liu,Wei Zhang,Wenjie Zeng,Xikun Zhang,Jingyi Zhang,Yuxin Song,Wenhao Wu,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出了MMReason，一个新基准，用于精确全面评估多模态大语言模型（MLLMs）的长链推理能力，通过提供多样化、开放式、具有挑战性的问题，并评估中间推理步骤。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM基准在评估长链推理能力时存在不足：缺乏难度和多样性、易受猜测和记忆影响、以及对中间推理步骤评估不足，这些都阻碍了MLLM向通用人工智能发展。

Method: 引入MMReason基准，通过以下方法构建：1) 策划需要多步骤推理的挑战性问题，涵盖6个学科和多难度级别；2) 将问题重构为开放式格式，并使用多模型投票过滤以消除猜测和记忆的捷径；3) 标注详细的逐步解决方案，并设计基于参考的三元评分机制来可靠评估中间推理步骤。

Result: 使用MMReason对主流MLLM进行了基准测试，并对其推理能力进行了深入分析。

Conclusion: MMReason有望成为推动MLLM推理研究的重要资源，为未来MLLM的发展提供有价值的评估工具。

Abstract: Reasoning plays a crucial role in advancing Multimodal Large Language Models
(MLLMs) toward Artificial General Intelligence. However, existing MLLM
benchmarks often fall short in precisely and comprehensively evaluating
long-chain reasoning abilities from three key aspects: (1) lack of difficulty
and diversity, (2) susceptibility to guessability and memorization, (3)
inadequate assessment of intermediate reasoning steps. To fill this gap, we
introduce MMReason, a new benchmark designed to precisely and comprehensively
evaluate MLLM long-chain reasoning capability with diverse, open-ended,
challenging questions. First, we curate challenging questions requiring
multi-step reasoning from various fields (i.e., 6 disciplines) and multiple
difficulty levels (i.e., from pre-university to university, and from
foundational to competition tiers). Second, these questions are reformulated
into an open-ended format and filtered using a multi-model voting technique to
eliminate shortcut cases related to guessing and memorization, ensuring robust
reasoning evaluations. Third, we annotate the questions with detailed
step-by-step solutions, and design a reference-based ternary scoring mechanism
to reliably assess intermediate reasoning steps. With MMReason, we benchmark
popular leading MLLMs and provide an in-depth analysis of their reasoning
capabilities. We hope MMReason will serve as a valuable resource for advancing
MLLM reasoning research. Code will be available at
https://github.com/HJYao00/MMReason.

</details>


### [140] [Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models](https://arxiv.org/abs/2506.23576)
*Maria Carolina Cornelia Wit,Jun Pang*

Main category: cs.AI

TL;DR: 本研究探索多智能体LLM系统作为对抗越狱攻击的防御手段，发现其能增强抵抗力，但存在有效性随攻击类型变化、假阳性增加和计算开销等权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）的快速发展引发了对其越狱攻击（绕过安全机制的提示）的担忧，促使研究如何有效防御此类攻击。

Method: 本研究评估了三种越狱策略（AutoDefense、BetterDan和JB），并通过复现AutoDefense框架，比较了单智能体设置与两、三智能体配置下的防御效果。

Result: 研究结果显示，多智能体系统能增强对越狱的抵抗力，尤其在减少假阴性方面表现显著。然而，其有效性因攻击类型而异，并引入了假阳性增加和计算开销等权衡。

Conclusion: 这些发现揭示了当前自动化防御的局限性，并为未来LLM系统提升对齐鲁棒性指明了方向。

Abstract: Recent advances in large language models (LLMs) have raised concerns about
jailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper
investigates the use of multi-agent LLM systems as a defence against such
attacks. We evaluate three jailbreaking strategies, including the original
AutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the
AutoDefense framework, we compare single-agent setups with two- and three-agent
configurations. Our results show that multi-agent systems enhance resistance to
jailbreaks, especially by reducing false negatives. However, its effectiveness
varies by attack type, and it introduces trade-offs such as increased false
positives and computational overhead. These findings point to the limitations
of current automated defences and suggest directions for improving alignment
robustness in future LLM systems.

</details>


### [141] [Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games](https://arxiv.org/abs/2506.23626)
*António Afonso,Iolanda Leite,Alessandro Sestini,Florian Fuchs,Konrad Tollmar,Linus Gisslén*

Main category: cs.AI

TL;DR: 针对强化学习（RL）在游戏中的奖励函数适应性问题，本文提出一种由语言模型（LM）驱动的自动化迭代微调方法，通过自然语言行为目标实现RL智能体奖励权重的优化，并在赛车任务中展现出接近人类专家的性能。


<details>
  <summary>Details</summary>
Motivation: 在游戏环境中部署RL智能体面临两大挑战：1) 设计有效的奖励函数通常需要RL专家；2) 当游戏内容或机制修改时，预设的奖励权重可能不再最优。本文主要关注并旨在解决后者。

Method: 提出一种自动化方法，利用语言模型（LM）迭代微调RL智能体的奖励函数权重。该方法基于用户定义的语言行为目标，LM根据此目标和先前训练轮次的性能统计数据，在闭环过程中提出并更新权重，实现智能体行为的持续对齐，且无需人工奖励工程。

Result: 在赛车任务中评估，该方法持续提升了智能体性能。LM引导的智能体在一次迭代后成功率从9%显著提升至74%。最终迭代中，LM调优的智能体达到80%的成功率，平均855步完成单圈，与人类专家调优智能体的峰值94%成功率和850步相比，表现出有竞争力的性能。

Conclusion: 该研究成功开发了一种由语言模型驱动的自动化奖励函数微调系统，有效解决了RL智能体在游戏内容变化时奖励权重适应性差的问题。该系统无需手动工程即可持续优化智能体行为，并在实验中展现出接近专家级的性能，显著降低了RL应用部署的门槛。

Abstract: Reinforcement Learning (RL) in games has gained significant momentum in
recent years, enabling the creation of different agent behaviors that can
transform a player's gaming experience. However, deploying RL agents in
production environments presents two key challenges: (1) designing an effective
reward function typically requires an RL expert, and (2) when a game's content
or mechanics are modified, previously tuned reward weights may no longer be
optimal. Towards the latter challenge, we propose an automated approach for
iteratively fine-tuning an RL agent's reward function weights, based on a
user-defined language based behavioral goal. A Language Model (LM) proposes
updated weights at each iteration based on this target behavior and a summary
of performance statistics from prior training rounds. This closed-loop process
allows the LM to self-correct and refine its output over time, producing
increasingly aligned behavior without the need for manual reward engineering.
We evaluate our approach in a racing task and show that it consistently
improves agent performance across iterations. The LM-guided agents show a
significant increase in performance from $9\%$ to $74\%$ success rate in just
one iteration. We compare our LM-guided tuning against a human expert's manual
weight design in the racing task: by the final iteration, the LM-tuned agent
achieved an $80\%$ success rate, and completed laps in an average of $855$ time
steps, a competitive performance against the expert-tuned agent's peak $94\%$
success, and $850$ time steps.

</details>


### [142] [HASD: Hierarchical Adaption for pathology Slide-level Domain-shift](https://arxiv.org/abs/2506.23673)
*Jingsong Liu,Han Li,Chen Yang,Michael Deutges,Ario Sadafi,Xin You,Katharina Breininger,Nassir Navab,Peter J. Schüffler*

Main category: cs.AI

TL;DR: 本文提出HASD框架，通过分层适应和原型选择机制，有效解决了病理AI中全玻片图像（WSI）层面的域漂移问题，提升了诊断性能并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 病理AI中，由于中心特异性条件，域漂移是一个关键问题。现有域适应方法侧重于图像斑块而非全玻片图像（WSI），无法捕捉临床所需的全局WSI特征。

Method: 本文提出了一个名为“全玻片层面域漂移分层适应框架”（HASD）。HASD通过两个关键组件实现多尺度特征一致性和计算高效的玻片级域适应：1) 一个分层适应框架，包含用于特征对齐的域级别对齐求解器、用于保持形态结构的玻片级别几何不变性正则化、以及用于维持局部关键诊断线索的斑块级别注意力一致性正则化；2) 一个原型选择机制，用于降低计算开销。

Result: 该方法在两个玻片级别任务、五个数据集上进行了验证，在乳腺癌HER2分级队列中实现了4.1%的AUROC改善，在子宫内膜癌生存预测队列中实现了3.9%的C-index提升。

Conclusion: 该方法为病理机构提供了一个实用且可靠的玻片级别域适应解决方案，同时最大限度地减少了计算和标注成本。

Abstract: Domain shift is a critical problem for pathology AI as pathology data is
heavily influenced by center-specific conditions. Current pathology domain
adaptation methods focus on image patches rather than WSI, thus failing to
capture global WSI features required in typical clinical scenarios. In this
work, we address the challenges of slide-level domain shift by proposing a
Hierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD
achieves multi-scale feature consistency and computationally efficient
slide-level domain adaptation through two key components: (1) a hierarchical
adaptation framework that integrates a Domain-level Alignment Solver for
feature alignment, a Slide-level Geometric Invariance Regularization to
preserve the morphological structure, and a Patch-level Attention Consistency
Regularization to maintain local critical diagnostic cues; and (2) a prototype
selection mechanism that reduces computational overhead. We validate our method
on two slide-level tasks across five datasets, achieving a 4.1\% AUROC
improvement in a Breast Cancer HER2 Grading cohort and a 3.9\% C-index gain in
a UCEC survival prediction cohort. Our method provides a practical and reliable
slide-level domain adaption solution for pathology institutions, minimizing
both computational and annotation costs.

</details>


### [143] [PokéAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red](https://arxiv.org/abs/2506.23689)
*Zihao Liu,Xinhang Sui,Yueran Song,Siwen Wang*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce Pok\'eAI, the first text-based, multi-agent large language model
(LLM) framework designed to autonomously play and progress through Pok\'emon
Red. Our system consists of three specialized agents-Planning, Execution, and
Critique-each with its own memory bank, role, and skill set. The Planning Agent
functions as the central brain, generating tasks to progress through the game.
These tasks are then delegated to the Execution Agent, which carries them out
within the game environment. Upon task completion, the Critique Agent evaluates
the outcome to determine whether the objective was successfully achieved. Once
verification is complete, control returns to the Planning Agent, forming a
closed-loop decision-making system.
  As a preliminary step, we developed a battle module within the Execution
Agent. Our results show that the battle AI achieves an average win rate of
80.8% across 50 wild encounters, only 6% lower than the performance of an
experienced human player. Furthermore, we find that a model's battle
performance correlates strongly with its LLM Arena score on language-related
tasks, indicating a meaningful link between linguistic ability and strategic
reasoning. Finally, our analysis of gameplay logs reveals that each LLM
exhibits a unique playstyle, suggesting that individual models develop distinct
strategic behaviors.

</details>


### [144] [Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models](https://arxiv.org/abs/2506.23692)
*Boyuan Zheng,Zerui Fang,Zhe Xu,Rui Wang,Yiwen Chen,Cunshi Wang,Mengwei Qu,Lei Lei,Zhen Feng,Yan Liu,Yuyang Li,Mingzhou Tan,Jiaji Wu,Jianwei Shuai,Jia Li,Fangfu Ye*

Main category: cs.AI

TL;DR: 提出以LLM驱动的“科学智能体”（Agent4S）作为第五科学范式，旨在自动化整个研究工作流程，并提供了五级分类路线图。


<details>
  <summary>Details</summary>
Motivation: 当前的“科学AI”（AI4S）虽是分析工具，但未能解决现有研究范式的核心效率低下问题。

Method: 本文提出“科学智能体”（Agent4S）概念，利用大型语言模型（LLM）驱动的智能体自动化整个研究工作流程。文章为此Agent4S定义了五级分类，描绘了一条从简单任务自动化到完全自主、协作的“AI科学家”的清晰路线图。

Result: 本文定义了Agent4S作为新的科学范式，并提出了从简单任务自动化到完全自主“AI科学家”的五级分类与发展路径。

Conclusion: Agent4S有望彻底改变科学发现，解决传统AI4S的局限性，标志着科学研究的下一个革命性阶段。

Abstract: While AI for Science (AI4S) serves as an analytical tool in the current
research paradigm, it doesn't solve its core inefficiency. We propose "Agent
for Science" (Agent4S)-the use of LLM-driven agents to automate the entire
research workflow-as the true Fifth Scientific Paradigm. This paper introduces
a five-level classification for Agent4S, outlining a clear roadmap from simple
task automation to fully autonomous, collaborative "AI Scientists." This
framework defines the next revolutionary step in scientific discovery.

</details>


### [145] [A New Perspective On AI Safety Through Control Theory Methodologies](https://arxiv.org/abs/2506.23703)
*Lars Ullrich,Walter Zimmer,Ross Greer,Knut Graichen,Alois C. Knoll,Mohan Trivedi*

Main category: cs.AI

TL;DR: AI在安全关键系统中缺乏安全保障。本文提出一种基于系统理论和数据生成的跨学科方法，即“数据控制”，旨在为AI安全提供通用的分析与保障基础。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能（AI）发展迅速并取得了惊人的性能，但其安全保障是一个主要问题，尤其是在安全关键的实际网络物理系统中，AI虽有望实现更高水平的自主性，却受限于安全保障的缺失。

Method: 本文提出一种基于系统理论启发和系统分析驱动的AI安全新视角，称为“数据控制”。该方法通过跨学科解释数据生成过程和AI系统抽象，并采用自顶向下的方法，勾勒出AI安全分析与保障的通用抽象基础。

Result: 本文阐述了一种通用且抽象的AI安全分析和保障基础，其可为具体的AI系统和应用提供进一步细化的依据，并为未来的创新预留了发展空间。

Conclusion: AI的安全保障是关键。通过提出“数据控制”这一新范式，本文为AI安全提供了一种基于系统理论的通用分析与保障基础，旨在刺激AI工程利用现有安全分析和保障机制，以应对未来AI创新的安全挑战。

Abstract: While artificial intelligence (AI) is advancing rapidly and mastering
increasingly complex problems with astonishing performance, the safety
assurance of such systems is a major concern. Particularly in the context of
safety-critical, real-world cyber-physical systems, AI promises to achieve a
new level of autonomy but is hampered by a lack of safety assurance. While
data-driven control takes up recent developments in AI to improve control
systems, control theory in general could be leveraged to improve AI safety.
Therefore, this article outlines a new perspective on AI safety based on an
interdisciplinary interpretation of the underlying data-generation process and
the respective abstraction by AI systems in a system theory-inspired and system
analysis-driven manner. In this context, the new perspective, also referred to
as data control, aims to stimulate AI engineering to take advantage of existing
safety analysis and assurance in an interdisciplinary way to drive the paradigm
of data control. Following a top-down approach, a generic foundation for safety
analysis and assurance is outlined at an abstract level that can be refined for
specific AI systems and applications and is prepared for future innovation.

</details>


### [146] [Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments](https://arxiv.org/abs/2506.23706)
*Christoph Schnabl,Daniel Hugenroth,Bill Marino,Alastair R. Beresford*

Main category: cs.AI

TL;DR: 本文提出可证明审计（Attestable Audits），利用可信执行环境（TEEs）解决AI模型安全基准测试中结果不可验证和数据保密性不足的问题，实现模型评估的可验证性和数据保护。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型的安全与合规性基准测试通常无法提供可验证的结果，且缺乏对模型知识产权和基准数据集的保密性，这为AI治理框架带来了验证挑战。

Method: 提出“可证明审计”方法，该方法在可信执行环境（Trusted Execution Environments, TEEs）中运行，使用户能够验证与符合规范的AI模型的交互，即使模型提供商和审计方互不信任也能保护敏感数据。

Result: 研究团队构建了一个原型系统，成功在针对Llama-3.1的典型审计基准测试上证明了该方法的可行性。

Conclusion: 可证明审计通过在可信执行环境中运行，解决了AI模型评估中结果可验证性和数据保密性的关键挑战，为AI治理框架提供了解决方案，确保了敏感数据在审计过程中的安全性。

Abstract: Benchmarks are important measures to evaluate safety and compliance of AI
models at scale. However, they typically do not offer verifiable results and
lack confidentiality for model IP and benchmark datasets. We propose Attestable
Audits, which run inside Trusted Execution Environments and enable users to
verify interaction with a compliant AI model. Our work protects sensitive data
even when model provider and auditor do not trust each other. This addresses
verification challenges raised in recent AI governance frameworks. We build a
prototype demonstrating feasibility on typical audit benchmarks against
Llama-3.1.

</details>


### [147] [BayesL: Towards a Logical Framework for Bayesian Networks](https://arxiv.org/abs/2506.23773)
*Stefano M. Nicoletti,Mariëlle Stoelinga*

Main category: cs.AI

TL;DR: 引入BayesL，一个新型逻辑框架，用于贝叶斯网络（BNs）的行为指定、查询和验证，支持无需手动修改模型的因果推理和假设情景评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法可能无法有效支持对贝叶斯网络的行为指定、查询和验证，以及灵活的因果推理和全面的假设情景评估，且常需手动修改模型，存在操作不便的痛点。

Method: 提出并引入了BayesL，一种新型的逻辑框架，它是一个结构化语言。

Result: BayesL能够创建针对贝叶斯网络的查询，促进因果和基于证据的灵活推理，并允许在不手动修改模型的情况下进行全面的假设情景评估。

Conclusion: BayesL提供了一个强大的结构化语言，显著简化和增强了贝叶斯网络的查询、推理和情景评估能力，克服了传统方法中手动修改模型的局限性。

Abstract: We introduce BayesL, a novel logical framework for specifying, querying, and
verifying the behaviour of Bayesian networks (BNs). BayesL (pronounced "Basil")
is a structured language that allows for the creation of queries over BNs. It
facilitates versatile reasoning concerning causal and evidence-based
relationships, and permits comprehensive what-if scenario evaluations without
the need for manual modifications to the model.

</details>


### [148] [When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)](https://arxiv.org/abs/2506.23784)
*Parosh Aziz Abdulla,Mohamed Faouzi Atig,Julie Cailler,Chencheng Liang,Philipp Rümmer*

Main category: cs.AI

TL;DR: 本文提出使用图神经网络(GNN)对字方程进行排序，以优化Nielsen变换求解连词字方程组的性能，并引入了新的图表示和训练方法，实验证明在特定类型问题上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有Nielsen变换求解字方程组时，求解器性能严重依赖于方程处理顺序，需要一种有效的方法来优化排序，以提升求解效率。

Method: 探索使用图神经网络(GNN)在求解前和求解过程中对字方程进行排序。为此，提出了一种新颖的、保留连词间全局信息的图表示方法，使GNN能获得整体视图。为处理可变数量的连词，提出了三种将多分类任务应用于方程排序的方法。GNN的训练通过字方程的最小不可满足子集(MUSes)完成。

Result: 实验结果显示，与最先进的字符串求解器相比，新框架在每个变量在每条方程中至多出现一次的基准测试中解决了更多问题。

Conclusion: 通过GNN对字方程进行智能排序，可以有效提升字方程组的求解性能，尤其在特定约束条件下的字方程问题中表现出优势。

Abstract: Nielsen transformation is a standard approach for solving word equations: by
repeatedly splitting equations and applying simplification steps, equations are
rewritten until a solution is reached. When solving a conjunction of word
equations in this way, the performance of the solver will depend considerably
on the order in which equations are processed. In this work, the use of Graph
Neural Networks (GNNs) for ranking word equations before and during the solving
process is explored. For this, a novel graph-based representation for word
equations is presented, preserving global information across conjuncts,
enabling the GNN to have a holistic view during ranking. To handle the variable
number of conjuncts, three approaches to adapt a multi-classification task to
the problem of ranking equations are proposed. The training of the GNN is done
with the help of minimum unsatisfiable subsets (MUSes) of word equations. The
experimental results show that, compared to state-of-the-art string solvers,
the new framework solves more problems in benchmarks where each variable
appears at most once in each equation.

</details>


### [149] [Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning](https://arxiv.org/abs/2506.23793)
*Anton Andreychuk,Konstantin Yakovlev,Aleksandr Panov,Alexey Skrynnik*

Main category: cs.AI

TL;DR: 本文提出了MAPF-GPT-DDG，一种基于微调和增量数据生成机制的多智能体路径规划（MAPF）求解器，它在解决方案质量和可伸缩性上超越了所有现有学习型MAPF求解器，并能处理百万级智能体。


<details>
  <summary>Details</summary>
Motivation: 多智能体路径规划（MAPF）是多机器人轨迹规划的关键抽象，尽管其最优解是NP-难的，但高效且可伸缩的求解器对物流、搜救等实际应用至关重要。当前基于机器学习的去中心化次优求解器仍有提升空间，尤其是在大规模场景下。

Method: 本文在纯模仿学习求解器MAPF-GPT的基础上，引入了MAPF-GPT-DDG。该方法通过使用集中式专家数据对预训练的MAPF模型进行有效微调，并利用一种新颖的增量数据生成（delta-data generation）机制，以加速训练并显著提高测试时的性能。

Result: 实验证明，MAPF-GPT-DDG在多种测试场景下，其解决方案质量超越了包括原始MAPF-GPT在内的所有现有学习型MAPF求解器。尤其值得一提的是，它能够处理单环境中多达100万个智能体的MAPF实例，为MAPF领域的可伸缩性设定了新的里程碑。

Conclusion: MAPF-GPT-DDG通过创新的微调和增量数据生成策略，显著提升了多智能体路径规划问题的求解质量和可伸缩性，为大规模实际应用提供了前所未有的强大解决方案。

Abstract: Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot
trajectory planning problems, where multiple homogeneous robots simultaneously
move in the shared environment. While solving MAPF optimally has been proven to
be NP-hard, scalable, and efficient, solvers are vital for real-world
applications like logistics, search-and-rescue, etc. To this end, decentralized
suboptimal MAPF solvers that leverage machine learning have come on stage.
Building on the success of the recently introduced MAPF-GPT, a pure imitation
learning solver, we introduce MAPF-GPT-DDG. This novel approach effectively
fine-tunes the pre-trained MAPF model using centralized expert data. Leveraging
a novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training
while significantly improving performance at test time. Our experiments
demonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF
solvers, including the original MAPF-GPT, regarding solution quality across
many testing scenarios. Remarkably, it can work with MAPF instances involving
up to 1 million agents in a single environment, setting a new milestone for
scalability in MAPF domains.

</details>


### [150] [A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents](https://arxiv.org/abs/2506.23844)
*Hang Su,Jun Luo,Chang Liu,Xiao Yang,Yichi Zhang,Yinpeng Dong,Jun Zhu*

Main category: cs.AI

TL;DR: LLM驱动的自主AI智能体面临新型安全风险。本文综述其能力、漏洞及防御策略，并提出一种基于CMDPs的风险感知智能体架构R2A2，以实现主动安全。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的自主AI智能体崛起，它们引入了超越传统威胁模型的新型安全风险，如记忆中毒、工具滥用、奖励操纵和涌现错位，急需对其进行识别、分析并提供防御策略。

Method: 本文首先考察了智能体的能力和结构基础；接着分析了智能体堆栈中的安全漏洞及其架构脆弱性；然后系统回顾了现有防御策略；最后，提出了基于约束马尔可夫决策过程（CMDPs）的统一认知框架——反射性风险感知智能体架构（R2A2），该架构整合了风险感知世界建模、元策略适应和联合奖励-风险优化，旨在实现主动安全。

Result: 本文识别并分析了LLM驱动智能体所带来的新型安全风险及其架构脆弱性；系统回顾了现有的防御策略；并提出了一种名为R2A2的新型风险感知智能体架构，旨在通过整合风险感知世界建模、元策略适应和联合奖励-风险优化，实现智能体决策循环中的原则性、主动性安全保障。

Conclusion: LLM驱动的自主AI智能体带来新颖且复杂的安全挑战。本研究通过深入分析这些风险、回顾防御策略，并提出R2A2架构，为构建具备原则性和主动安全能力的智能体提供了理论基础和潜在解决方案。

Abstract: Recent advances in large language models (LLMs) have catalyzed the rise of
autonomous AI agents capable of perceiving, reasoning, and acting in dynamic,
open-ended environments. These large-model agents mark a paradigm shift from
static inference systems to interactive, memory-augmented entities. While these
capabilities significantly expand the functional scope of AI, they also
introduce qualitatively novel security risks - such as memory poisoning, tool
misuse, reward hacking, and emergent misalignment - that extend beyond the
threat models of conventional systems or standalone LLMs. In this survey, we
first examine the structural foundations and key capabilities that underpin
increasing levels of agent autonomy, including long-term memory retention,
modular tool use, recursive planning, and reflective reasoning. We then analyze
the corresponding security vulnerabilities across the agent stack, identifying
failure modes such as deferred decision hazards, irreversible tool chains, and
deceptive behaviors arising from internal state drift or value misalignment.
These risks are traced to architectural fragilities that emerge across
perception, cognition, memory, and action modules. To address these challenges,
we systematically review recent defense strategies deployed at different
autonomy layers, including input sanitization, memory lifecycle control,
constrained decision-making, structured tool invocation, and introspective
reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a
unified cognitive framework grounded in Constrained Markov Decision Processes
(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,
and joint reward-risk optimization to enable principled, proactive safety
across the agent's decision-making loop.

</details>


### [151] [Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence](https://arxiv.org/abs/2506.23908)
*András György,Tor Lattimore,Nevena Lazić,Csaba Szepesvári*

Main category: cs.AI

TL;DR: 现有AI系统在演绎推理方面表现不佳，作者认为根源在于统计学习方法，并主张转向要求全输入正确性的“精确学习”范式以实现可靠推理。


<details>
  <summary>Details</summary>
Motivation: 尽管AI系统（特别是基于Transformer的）在数学和科学等领域取得了重大进展，但它们在简单的演绎推理任务上仍普遍且持续地失败，这使得它们无法实现具备可靠演绎推理能力的人工通用智能（AGI）的愿景。

Method: 论文提出，为使基于学习的AI系统实现可靠的演绎推理，研究人员必须从优化推理问题和算法任务分布的统计性能，根本性地转向要求对所有输入都达到正确性的“精确学习”范式。

Result: 作者论证了当前AI系统不健全的演绎推理行为是其所依赖的统计学习方法的固有结果。他们主张，精确学习对于可靠的演绎推理既是必要的也是可能实现的。

Conclusion: 为实现AI系统的可靠演绎推理，必须采纳要求全输入正确性的精确学习范式，并且这一雄心勃勃的目标应该指导未来的算法设计。

Abstract: Sound deductive reasoning -- the ability to derive new knowledge from
existing facts and rules -- is an indisputably desirable aspect of general
intelligence. Despite the major advances of AI systems in areas such as math
and science, especially since the introduction of transformer architectures, it
is well-documented that even the most advanced frontier systems regularly and
consistently falter on easily-solvable deductive reasoning tasks. Hence, these
systems are unfit to fulfill the dream of achieving artificial general
intelligence capable of sound deductive reasoning. We argue that their unsound
behavior is a consequence of the statistical learning approach powering their
development. To overcome this, we contend that to achieve reliable deductive
reasoning in learning-based AI systems, researchers must fundamentally shift
from optimizing for statistical performance against distributions on reasoning
problems and algorithmic tasks to embracing the more ambitious exact learning
paradigm, which demands correctness on all inputs. We argue that exact learning
is both essential and possible, and that this ambitious objective should guide
algorithm design.

</details>


### [152] [Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice](https://arxiv.org/abs/2506.23924)
*Akshit Kumar,Tianyi Peng,Yuhang Wu,Assaf Zeevi*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在运筹学（OR）的随机建模问题上展现出专家级能力，在课堂和实际应用中均与人类专家表现相当，揭示了AI在OR自动化方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多领域表现出专家级能力，但其在运筹学（OR）问题，特别是涉及不确定性的随机建模问题上的能力仍未被充分探索。

Method: 研究评估了LLMs解决随机建模问题的能力。具体方法包括：1. 手动收集研究生水平作业和博士资格考试中的代表性问题进行测试。2. 利用开源仿真优化库SimOpt进一步探究LLMs在不确定性下进行实际决策的能力。

Result: 尽管要实现随机建模流程的可靠自动化仍需大量工作，但最先进的LLMs在课堂和实际环境中均展现出与人类专家相当的熟练程度。

Conclusion: 研究结果突出了构建AI智能体以辅助运筹学研究人员的潜力，并通过自动化增强运筹学在现实世界中的影响力。

Abstract: Large language models (LLMs) have exhibited expert-level capabilities across
various domains. However, their abilities to solve problems in Operations
Research (OR) -- the analysis and optimization of mathematical models derived
from real-world problems or their verbal descriptions -- remain underexplored.
In this work, we take a first step toward evaluating LLMs' abilities to solve
stochastic modeling problems, a core class of OR problems characterized by
uncertainty and typically involving tools from probability, statistics, and
stochastic processes. We manually procure a representative set of
graduate-level homework and doctoral qualification-exam problems and test LLMs'
abilities to solve them. We further leverage SimOpt, an open-source library of
simulation-optimization problems and solvers, to investigate LLMs' abilities to
make real-world decisions under uncertainty. Our results show that, though a
nontrivial amount of work is still needed to reliably automate the stochastic
modeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on
par with human experts in both classroom and practical settings. These findings
highlight the potential of building AI agents that assist OR researchers and
amplify the real-world impact of OR through automation.

</details>


### [153] [Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system](https://arxiv.org/abs/2506.23926)
*Junping Wang,Bicheng Wang,Yibo Xuea,Yuan Xie*

Main category: cs.AI

TL;DR: 针对工业链在多混沌数据下韧性测量与规划的挑战，本文提出“工业大脑”框架。该框架融合高阶神经网络与符号推理，实现了对复杂网络结构与动态的理解，并显著提升了韧性预测和规划的准确性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 工业链在故障中保持基本功能（韧性）对管理至关重要。然而，当韧性共演化数量或类型极其混沌时，现有端到端深度学习方法在未见的时空结构重建和网络拓扑预测上泛化性差，尤其在现实世界多混沌数据场景中表现不佳，这构成了核心挑战。

Method: 本文提出“工业大脑”（industrial brain）框架，一个类人自主认知决策与规划框架。它整合了高阶活动驱动神经网络（higher-order activity-driven neuro network）和CT-OODA符号推理（CT-OODA symbolic reasoning），能够直接从全局变量的观测数据中自主规划韧性。该框架无需简化假设即可理解和建模节点活动动力学与网络共演化拓扑，并揭示复杂网络背后的潜在规律，从而实现准确的韧性预测、推理和规划。

Result: 实验结果表明，“工业大脑”在韧性预测和规划上显著优于现有方法，相较于GoT和OlaGPT框架，准确率最高提升10.8%；相较于谱维度约简，准确率提高11.03%。此外，它对未见的拓扑结构和动力学具有良好的泛化能力，并在存在观测干扰的情况下仍能保持稳健的性能。

Conclusion: 本研究发现“工业大脑”成功填补了工业链韧性预测和规划领域的重要空白。

Abstract: Resilience non-equilibrium measurement, the ability to maintain fundamental
functionality amidst failures and errors, is crucial for scientific management
and engineering applications of industrial chain. The problem is particularly
challenging when the number or types of multiple co-evolution of resilience
(for example, randomly placed) are extremely chaos. Existing end-to-end deep
learning ordinarily do not generalize well to unseen full-feld reconstruction
of spatiotemporal co-evolution structure, and predict resilience of network
topology, especially in multiple chaos data regimes typically seen in
real-world applications. To address this challenge, here we propose industrial
brain, a human-like autonomous cognitive decision-making and planning framework
integrating higher-order activity-driven neuro network and CT-OODA symbolic
reasoning to autonomous plan resilience directly from observational data of
global variable. The industrial brain not only understands and model structure
of node activity dynamics and network co-evolution topology without simplifying
assumptions, and reveal the underlying laws hidden behind complex networks, but
also enabling accurate resilience prediction, inference, and planning.
Experimental results show that industrial brain significantly outperforms
resilience prediction and planning methods, with an accurate improvement of up
to 10.8\% over GoT and OlaGPT framework and 11.03\% over spectral dimension
reduction. It also generalizes to unseen topologies and dynamics and maintains
robust performance despite observational disturbances. Our findings suggest
that industrial brain addresses an important gap in resilience prediction and
planning for industrial chain.

</details>


### [154] [AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models](https://arxiv.org/abs/2506.23949)
*Anthony M. Barrett,Jessica Newman,Brandie Nonnecke,Nada Madkour,Dan Hendrycks,Evan R. Murphy,Krystal Jackson,Deepika Raman*

Main category: cs.AI

TL;DR: 针对通用/基础AI模型（GPAI/foundation models）的风险管理实践指南。


<details>
  <summary>Details</summary>
Motivation: 鉴于多功能AI模型（如大型语言模型、基础模型）在带来巨大能力的同时也伴随着严重风险，本文件旨在帮助开发者识别、分析和缓解这些风险。

Method: 本文件提供风险管理实践与控制措施，主要面向大规模、最先进的GPAI/基础模型开发者。其方法是基于NIST AI风险管理框架和ISO/IEC 23894等现有AI风险管理标准，并特别关注GPAI/基础模型开发者面临的独特问题。

Result: 本文件作为一份指南，提供了识别、分析和缓解GPAI/基础模型风险的具体实践或控制措施，旨在帮助相关开发者管理潜在的不利事件。

Conclusion: 为确保GPAI/基础模型的安全和负责任发展，必须实施专业的风险管理措施。本指南通过整合现有标准并解决GPAI的特殊挑战，为开发者提供了关键的指导，以降低风险并促进最佳实践。

Abstract: Increasingly multi-purpose AI models, such as cutting-edge large language
models or other 'general-purpose AI' (GPAI) models, 'foundation models,'
generative AI models, and 'frontier models' (typically all referred to
hereafter with the umbrella term 'GPAI/foundation models' except where greater
specificity is needed), can provide many beneficial capabilities but also risks
of adverse events with profound consequences. This document provides
risk-management practices or controls for identifying, analyzing, and
mitigating risks of GPAI/foundation models. We intend this document primarily
for developers of large-scale, state-of-the-art GPAI/foundation models; others
that can benefit from this guidance include downstream developers of end-use
applications that build on a GPAI/foundation model. This document facilitates
conformity with or use of leading AI risk management-related standards,
adapting and building on the generic voluntary guidance in the NIST AI Risk
Management Framework and ISO/IEC 23894, with a focus on the unique issues faced
by developers of GPAI/foundation models.

</details>


### [155] [Harnessing AI Agents to Advance Research on Refugee Child Mental Health](https://arxiv.org/abs/2506.23992)
*Aditya Shrivastava,Komal Gupta,Shraddha Arora*

Main category: cs.AI

TL;DR: 本研究提出一个紧凑的、基于AI的框架，用于处理非结构化难民健康数据，以提取儿童心理健康知识，并比较了两种RAG模型（Zephyr-7B-beta和DeepSeek R1-7B）在处理人道主义数据集方面的表现。


<details>
  <summary>Details</summary>
Motivation: 国际难民危机日益严峻，数百万流离失所的儿童面临严重的心理创伤，迫切需要有效的方法来处理非结构化健康数据并获取儿童心理健康知识。

Method: 研究采用一个基于AI的框架，通过比较Zephyr-7B-beta和DeepSeek R1-7B两种RAG（检索增强生成）管道，评估它们在处理具有挑战性的人道主义数据集时的效果和避免幻觉的能力。

Result: 两种模型均能正常工作，但DeepSeek R1在答案相关性准确率上显著优于Zephyr，达到0.91。

Conclusion: 本研究提供了一个可扩展的AI策略，结合了尖端AI方法与移民研究和儿童心理学，旨在帮助政策制定者、心理健康从业者和人道主义机构更好地援助流离失所的儿童，并关注他们的心理健康。

Abstract: The international refugee crisis deepens, exposing millions of dis placed
children to extreme psychological trauma. This research suggests a com pact,
AI-based framework for processing unstructured refugee health data and
distilling knowledge on child mental health. We compare two Retrieval-Aug
mented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to
determine how well they process challenging humanitarian datasets while avoid
ing hallucination hazards. By combining cutting-edge AI methods with migration
research and child psychology, this study presents a scalable strategy to
assist policymakers, mental health practitioners, and humanitarian agencies to
better assist displaced children and recognize their mental wellbeing. In
total, both the models worked properly but significantly Deepseek R1 is
superior to Zephyr with an accuracy of answer relevance 0.91

</details>


### [156] [Constructing Non-Markovian Decision Process via History Aggregator](https://arxiv.org/abs/2506.24026)
*Yongyi Wang,Wenxin Li*

Main category: cs.AI

TL;DR: 非马尔可夫动力学阻碍强化学习，现有基准测试不足。本文提出基于范畴论（MDP与NMDP等价性）和状态历史聚合器（HAS）的方法，精确构建非马尔可夫问题，以更严格地评估决策算法。


<details>
  <summary>Details</summary>
Motivation: 非马尔可夫动力学对算法决策（特别是强化学习）构成重大障碍，影响系统效能。现有基准测试在评估决策算法处理非马尔可夫动力学方面存在不足。

Method: 作者提出一种基于范畴论的通用方法，建立了马尔可夫决策过程（MDP）和非马尔可夫决策过程（NMDP）范畴，并证明了它们之间的等价关系。此外，通过引入状态历史聚合器（HAS），将非马尔可夫性引入决策问题设置中，从而精确控制时间序列中的状态依赖结构。

Result: 该理论基础为理解和处理非马尔可夫动力学提供了新视角。其方法（结合HAS）被证明能有效表示广泛的非马尔可夫动力学。

Conclusion: 该方法通过显式构建包含非马尔可夫动力学的测试环境，促进对决策算法更严格和灵活的评估，从而弥补了现有基准测试的不足。

Abstract: In the domain of algorithmic decision-making, non-Markovian dynamics manifest
as a significant impediment, especially for paradigms such as Reinforcement
Learning (RL), thereby exerting far-reaching consequences on the advancement
and effectiveness of the associated systems. Nevertheless, the existing
benchmarks are deficient in comprehensively assessing the capacity of decision
algorithms to handle non-Markovian dynamics. To address this deficiency, we
have devised a generalized methodology grounded in category theory. Notably, we
established the category of Markov Decision Processes (MDP) and the category of
non-Markovian Decision Processes (NMDP), and proved the equivalence
relationship between them. This theoretical foundation provides a novel
perspective for understanding and addressing non-Markovian dynamics. We further
introduced non-Markovianity into decision-making problem settings via the
History Aggregator for State (HAS). With HAS, we can precisely control the
state dependency structure of decision-making problems in the time series. Our
analysis demonstrates the effectiveness of our method in representing a broad
range of non-Markovian dynamics. This approach facilitates a more rigorous and
flexible evaluation of decision algorithms by testing them in problem settings
where non-Markovian dynamics are explicitly constructed.

</details>


### [157] [SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2506.24119)
*Bo Liu,Leon Guertler,Simon Yu,Zichen Liu,Penghui Qi,Daniel Balcells,Mickel Liu,Cheston Tan,Weiyan Shi,Min Lin,Wee Sun Lee,Natasha Jaques*

Main category: cs.AI

TL;DR: SPIRAL是一个自博弈框架，通过零和游戏训练语言模型，无需人工监督，显著提升模型推理能力并使其发展出可迁移的认知模式。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖人工标注问题-答案对和领域特定奖励工程，成本高昂且扩展性受限，阻碍了语言模型推理能力的进一步发展。

Method: 提出SPIRAL自博弈框架，模型通过与持续改进的自身版本进行多轮零和游戏来学习。实现了一个完全在线、多轮、多智能体LLM强化学习系统，并引入角色条件优势估计（RAE）以稳定训练。

Result: 通过在Kuhn Poker上训练Qwen3-4B-Base，数学和通用推理能力分别提升8.6%和8.4%，优于专家轨迹上的SFT。分析显示推理能力通过系统分解、期望值计算和逐案例分析等认知模式实现迁移。多游戏训练能进一步增强性能。应用于强推理模型（DeepSeek-R1-Distill-Qwen-7B）仍能带来2.0%的平均提升。

Conclusion: 零和游戏能够自然地培养可迁移的推理能力，为语言模型的自主推理发展提供了一个有前景的方向。

Abstract: Recent advances in reinforcement learning have shown that language models can
develop sophisticated reasoning through training on tasks with verifiable
rewards, but these approaches depend on human-curated problem-answer pairs and
domain-specific reward engineering. We introduce SPIRAL, a self-play framework
where models learn by playing multi-turn, zero-sum games against continuously
improving versions of themselves, eliminating the need for human supervision.
Through self-play, SPIRAL generates an infinite curriculum of progressively
challenging problems as models must constantly adapt to stronger opponents. To
enable this self-play training at scale, We implement a fully online,
multi-turn, multi-agent reinforcement learning system for LLMs and propose
role-conditioned advantage estimation (RAE) to stabilize multi-agent training.
Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that
transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%
improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000
expert game trajectories. Analysis reveals that this transfer occurs through
three cognitive patterns: systematic decomposition, expected value calculation,
and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple
Negotiation) further enhances performance as each game develops distinct
reasoning strengths. Applying SPIRAL to a strong reasoning model
(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These
results demonstrate that zero-sum games naturally develop transferable
reasoning capabilities, highlighting a promising direction for autonomous
reasoning development.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [158] [Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation](https://arxiv.org/abs/2506.22441)
*Lei Yang*

Main category: cs.LG

TL;DR: 针对智能交通系统中缺失时空交通数据问题，本文提出了TDWLFT模型。该模型通过引入阈值距离加权损失函数，有效降低了传统LFT模型对异常值的敏感性，并在准确性和效率上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统（ITS）需高质量的时空交通数据，但实际数据采集常因故障导致数据缺失或损坏。现有张量潜在因子分解（LFT）模型虽有效，但其L2范数目标函数易受异常值影响，限制了其性能。

Method: 本文提出了一种阈值距离加权（TDW）损失函数融入的张量潜在因子分解模型（TDWLFT）。该方法通过对单个样本赋予差异化权重，有效降低了模型对异常值的敏感性。

Result: 在两个来自不同城市环境的交通速度数据集上进行的广泛实验表明，所提出的TDWLFT模型在预测准确性和计算效率方面均持续优于最先进的方法。

Conclusion: TDWLFT模型通过有效处理异常值，为智能交通系统中的缺失时空交通数据补全提供了一种更鲁棒、更高效的解决方案。

Abstract: Intelligent transportation systems (ITS) rely heavily on complete and
high-quality spatiotemporal traffic data to achieve optimal performance.
Nevertheless, in real-word traffic data collection processes, issues such as
communication failures and sensor malfunctions often lead to incomplete or
corrupted datasets, thereby posing significant challenges to the advancement of
ITS. Among various methods for imputing missing spatiotemporal traffic data,
the latent factorization of tensors (LFT) model has emerged as a widely adopted
and effective solution. However, conventional LFT models typically employ the
standard L2-norm in their learning objective, which makes them vulnerable to
the influence of outliers. To overcome this limitation, this paper proposes a
threshold distance weighted (TDW) loss-incorporated Latent Factorization of
Tensors (TDWLFT) model. The proposed loss function effectively reduces the
model's sensitivity to outliers by assigning differentiated weights to
individual samples. Extensive experiments conducted on two traffic speed
datasets sourced from diverse urban environments confirm that the proposed
TDWLFT model consistently outperforms state-of-the-art approaches in terms of
both in both prediction accuracy and computational efficiency.

</details>


### [159] [Features-based embedding or Feature-grounding](https://arxiv.org/abs/2506.22442)
*Piotr Makarevich*

Main category: cs.LG

TL;DR: 该论文研究如何通过特征嵌入在深度学习模型中复现人类基于知识的结构化思维。


<details>
  <summary>Details</summary>
Motivation: 人类在日常推理中，会根据先验知识和概念类别将特定物体与一组独特的预期属性相关联。本研究旨在探索如何将这种知识驱动的结构化思维复现到深度学习模型中。

Method: 引入一种构建特征接地嵌入（feature-grounded embedding）的特定方法，旨在将可操作词典的可共享表示与可解释的特定领域概念特征对齐。

Result: 摘要中未明确提及研究结果。

Conclusion: 摘要中未明确提及研究结论。

Abstract: In everyday reasoning, when we think about a particular object, we associate
it with a unique set of expected properties such as weight, size, or more
abstract attributes like density or horsepower. These expectations are shaped
by our prior knowledge and the conceptual categories we have formed through
experience. This paper investigates how such knowledge-based structured
thinking can be reproduced in deep learning models using features based
embeddings. Specially, it introduces an specific approach to build
feature-grounded embedding, aiming to align shareable representations of
operable dictionary with interpretable domain-specific conceptual features.

</details>


### [160] [Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition](https://arxiv.org/abs/2506.22443)
*Sarah Seifi,Tobias Sukianto,Cecilia Carbonelli,Lorenzo Servadei,Robert Wille*

Main category: cs.LG

TL;DR: 本文提出并评估了RL-Net，一种神经符号规则学习网络，首次应用于雷达手势识别，旨在通过神经优化学习可解释规则，以平衡性能与透明度。


<details>
  <summary>Details</summary>
Motivation: 现有模型面临性能（深度神经网络）与可解释性（基于规则的模型）之间的固有权衡，研究旨在开发一种能兼顾两者优势的方法。

Method: 研究了一种名为RL-Net的神经符号规则学习神经网络，该网络通过神经优化学习可解释的规则列表，并首次应用于雷达手势识别（HGR）。通过与全透明的规则系统MIRA和可解释的黑盒模型XentricAI进行基准测试，评估了RL-Net的准确性、可解释性和用户适应性。此外，还识别并解决了规则剪枝和层次偏差中的优化挑战。

Result: RL-Net在保持强大性能（93.03% F1分数）的同时显著降低了规则复杂性，实现了性能与透明度的良好权衡。研究还识别了特定的优化挑战并提出了稳定性增强的修改。与MIRA和XentricAI相比，RL-Net被证明是透明度和性能之间的实用中间方案。

Conclusion: 本研究突出了神经符号模型在可解释雷达手势识别中的实际可行性，并为将可解释AI扩展到边缘部署传感系统提供了重要见解。

Abstract: Rule-based models offer interpretability but struggle with complex data,
while deep neural networks excel in performance yet lack transparency. This
work investigates a neuro-symbolic rule learning neural network named RL-Net
that learns interpretable rule lists through neural optimization, applied for
the first time to radar-based hand gesture recognition (HGR). We benchmark
RL-Net against a fully transparent rule-based system (MIRA) and an explainable
black-box model (XentricAI), evaluating accuracy, interpretability, and user
adaptability via transfer learning. Our results show that RL-Net achieves a
favorable trade-off, maintaining strong performance (93.03% F1) while
significantly reducing rule complexity. We identify optimization challenges
specific to rule pruning and hierarchy bias and propose stability-enhancing
modifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical
middle ground between transparency and performance. This study highlights the
real-world feasibility of neuro-symbolic models for interpretable HGR and
offers insights for extending explainable AI to edge-deployable sensing
systems.

</details>


### [161] [Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2](https://arxiv.org/abs/2506.22444)
*Jing Wang,Amar Sra,Jeremy C. Weiss*

Main category: cs.LG

TL;DR: 针对PASC患者管理难题，本研究首次公开小规模患者队列（18例），利用大型语言模型（Llama-3.1-70B-Instruct）提取文本特征并结合专家标注，提出Active Attention Network模型以预测临床风险和识别进展事件，旨在提高预测准确性并减少标注需求，最终改善患者护理。


<details>
  <summary>Details</summary>
Motivation: PASC（SARS-CoV-2急性后遗症）的长期影响对全球医疗系统构成重大挑战。准确识别PASC的进展事件（如住院、再感染）对于有效的患者管理和资源分配至关重要。然而，传统基于结构化数据的模型难以捕捉PASC的细微进展。

Method: 本研究首次构建并公开了一个包含18例PASC患者的队列数据，其特征包括基于大型语言模型Llama-3.1-70B-Instruct生成的文本时间序列特征，以及由临床专家标注的临床风险。在此基础上，研究提出了一种Active Attention Network模型，用于预测临床风险并识别与该风险相关的进展事件。该方法整合了人类专业知识和主动学习。

Result: 抽象中未提供具体的量化研究结果。但研究旨在通过整合人类专业知识与主动学习，显著提高临床风险预测的准确性，并能以更少的标注数量实现进展事件的识别。

Conclusion: 本研究的最终目标是通过提高临床风险预测的准确性和进展事件的识别能力，从而改善SARS-CoV-2患者的护理和决策制定。

Abstract: The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,
pose a significant challenge to healthcare systems worldwide. Accurate
identification of progression events, such as hospitalization and reinfection,
is essential for effective patient management and resource allocation. However,
traditional models trained on structured data struggle to capture the nuanced
progression of PASC. In this study, we introduce the first publicly available
cohort of 18 PASC patients, with text time series features based on Large
Language Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical
expert. We propose an Active Attention Network to predict the clinical risk and
identify progression events related to the risk. By integrating human expertise
with active learning, we aim to enhance clinical risk prediction accuracy and
enable progression events identification with fewer number of annotation. The
ultimate goal is to improves patient care and decision-making for SARS-CoV-2
patient.

</details>


### [162] [Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security](https://arxiv.org/abs/2506.22445)
*Saad Alqithami*

Main category: cs.LG

TL;DR: 本研究提出HAMARL框架，结合分层多智能体强化学习与对抗性训练，有效提升赛博物理系统抵御高级网络威胁的能力。


<details>
  <summary>Details</summary>
Motivation: 赛博物理系统（CPS）在关键基础设施中作用日益重要，但其日益增长的连接性使其极易受到高级网络威胁（如自适应、零日攻击）。传统安全方法（如基于规则的入侵检测和单智能体强化学习）已不足以应对这些挑战。

Method: 本文引入了一种新颖的“分层对抗性弹性多智能体强化学习”（HAMARL）框架。该框架采用分层结构，由负责子系统安全的本地智能体和协调全局防御策略的全局协调器组成。此外，框架整合了对抗性训练循环，以模拟和预测不断演变的网络威胁，实现主动防御适应。

Result: 在模拟工业物联网测试台上进行的广泛实验评估表明，HAMARL显著优于传统的多种智能体强化学习方法，在提高攻击检测准确性、缩短响应时间以及确保操作连续性方面表现出色。

Conclusion: 研究结果强调，将分层多智能体协调与对抗感知训练相结合，能有效增强下一代CPS的韧性和安全性。

Abstract: Cyber-Physical Systems play a critical role in the infrastructure of various
sectors, including manufacturing, energy distribution, and autonomous
transportation systems. However, their increasing connectivity renders them
highly vulnerable to sophisticated cyber threats, such as adaptive and zero-day
attacks, against which traditional security methods like rule-based intrusion
detection and single-agent reinforcement learning prove insufficient. To
overcome these challenges, this paper introduces a novel Hierarchical
Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.
HAMARL employs a hierarchical structure consisting of local agents dedicated to
subsystem security and a global coordinator that oversees and optimizes
comprehensive, system-wide defense strategies. Furthermore, the framework
incorporates an adversarial training loop designed to simulate and anticipate
evolving cyber threats, enabling proactive defense adaptation. Extensive
experimental evaluations conducted on a simulated industrial IoT testbed
indicate that HAMARL substantially outperforms traditional multi-agent
reinforcement learning approaches, significantly improving attack detection
accuracy, reducing response times, and ensuring operational continuity. The
results underscore the effectiveness of combining hierarchical multi-agent
coordination with adversarially-aware training to enhance the resilience and
security of next-generation CPS.

</details>


### [163] [EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis](https://arxiv.org/abs/2506.22446)
*Aakash Tripathi,Asim Waqas,Matthew B. Schabath,Yasin Yilmaz,Ghulam Rasool*

Main category: cs.LG

TL;DR: 本文提出EAGLE深度学习框架，通过注意力机制有效融合多模态数据，实现高精度、可解释且计算高效的癌症生存预测，以克服现有方法的局限性并促进临床应用。


<details>
  <summary>Details</summary>
Motivation: 准确的癌症生存预测需整合影像、临床参数和文本报告等多模态数据，但现有方法存在融合策略简单、计算需求高及缺乏可解释性等问题，这些是阻碍其临床应用的关键障碍。

Method: 研究提出了EAGLE（广义潜在嵌入的高效对齐）深度学习框架，通过以下四项核心创新解决上述局限：1) 动态跨模态注意力机制学习模态间的层级关系；2) 在保持预测性能的同时实现大规模降维（99.96%）；3) 提供三种互补的归因方法，实现患者层面的可解释性；4) 统一的管道可无缝适应不同癌症类型。

Result: EAGLE在911名患者（涵盖胶质母细胞瘤、胰腺导管内乳头状黏液性肿瘤和非小细胞肺癌）上进行了评估。患者层面分析显示，高风险个体更依赖不利的影像特征，而低风险患者表现出模态贡献的平衡。风险分层识别出临床上有意义的群体，其中位生存期差异从4倍（胶质母细胞瘤）到5倍（非小细胞肺癌）不等，可直接指导治疗强度决策。EAGLE结合了最先进的性能和临床可解释性。

Conclusion: EAGLE通过结合卓越性能与临床可解释性，弥合了先进人工智能能力与实际医疗部署之间的鸿沟，为多模态生存预测提供了一个可扩展的解决方案，提升了预后准确性及医生对自动化预测的信任。

Abstract: Accurate cancer survival prediction requires integration of diverse data
modalities that reflect the complex interplay between imaging, clinical
parameters, and textual reports. However, existing multimodal approaches suffer
from simplistic fusion strategies, massive computational requirements, and lack
of interpretability-critical barriers to clinical adoption. We present EAGLE
(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning
framework that addresses these limitations through attention-based multimodal
fusion with comprehensive attribution analysis. EAGLE introduces four key
innovations: (1) dynamic cross-modal attention mechanisms that learn
hierarchical relationships between modalities, (2) massive dimensionality
reduction (99.96%) while maintaining predictive performance, (3) three
complementary attribution methods providing patient-level interpretability, and
(4) a unified pipeline enabling seamless adaptation across cancer types. We
evaluated EAGLE on 911 patients across three distinct malignancies:
glioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,
n=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis
showed high-risk individuals relied more heavily on adverse imaging features,
while low-risk patients demonstrated balanced modality contributions. Risk
stratification identified clinically meaningful groups with 4-fold (GBM) to
5-fold (NSCLC) differences in median survival, directly informing treatment
intensity decisions. By combining state-of-the-art performance with clinical
interpretability, EAGLE bridges the gap between advanced AI capabilities and
practical healthcare deployment, offering a scalable solution for multimodal
survival prediction that enhances both prognostic accuracy and physician trust
in automated predictions.

</details>


### [164] [Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture](https://arxiv.org/abs/2506.22447)
*Fabio Merizzi,Harilaos Loukos*

Main category: cs.LG

TL;DR: 本研究提出一种多变量深度学习方法，通过共享编码器和变量特定解码器的Vision Transformer架构，实现了高效且高性能的高分辨率气候降尺度。


<details>
  <summary>Details</summary>
Motivation: 全球气候模型（GCMs）空间分辨率粗糙，不适用于区域研究；区域气候模型（RCMs）计算成本高昂且灵活性受限。现有的深度学习降尺度多集中于单变量模型，导致上下文感知不足、重复计算及变量间交互缺失。

Method: 提出一种多任务、多变量的Vision Transformer（ViT）架构（1EMD），包含共享编码器和变量特定解码器。该架构直接从GCM分辨率输入，联合预测地表温度、风速和500 hPa位势高度三个关键气候变量，以模拟欧洲地区的RCM尺度降尺度。

Result: 所提出的多变量方法实现了积极的跨变量知识迁移，在相同条件下始终优于单变量基线模型，并显著提高了计算效率。

Conclusion: 研究结果表明，多变量建模对于实现高分辨率气候降尺度是有效且具有前景的。

Abstract: Global Climate Models (GCMs) are critical for simulating large-scale climate
dynamics, but their coarse spatial resolution limits their applicability in
regional studies. Regional Climate Models (RCMs) refine this through dynamic
downscaling, albeit at considerable computational cost and with limited
flexibility. While deep learning has emerged as an efficient data-driven
alternative, most existing studies have focused on single-variable models that
downscale one variable at a time. This approach can lead to limited contextual
awareness, redundant computation, and lack of cross-variable interaction. Our
study addresses these limitations by proposing a multi-task, multi-variable
Vision Transformer (ViT) architecture with a shared encoder and
variable-specific decoders (1EMD). The proposed architecture jointly predicts
three key climate variables: surface temperature (tas), wind speed (sfcWind),
and 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,
emulating RCM-scale downscaling over Europe. We show that our multi-variable
approach achieves positive cross-variable knowledge transfer and consistently
outperforms single-variable baselines trained under identical conditions, while
also improving computational efficiency. These results demonstrate the
effectiveness of multi-variable modeling for high-resolution climate
downscaling.

</details>


### [165] [Stabilization of industrial processes with time series machine learning](https://arxiv.org/abs/2506.22502)
*Matvei Anoshin,Olga Tsurkan,Vadim Lopatkin,Leonid Fedichkin*

Main category: cs.LG

TL;DR: 本文提出一种由“预言器”和“优化器”两个神经网络组成的管道，用于稳定时间序列过程，并在温度控制中实现了比传统方法高3倍的稳定性提升。


<details>
  <summary>Details</summary>
Motivation: 时间序列过程的稳定化是一个在各工业领域普遍存在的关键问题。应用机器学习可以显著提升稳定化质量并减少计算资源，具有决定性影响。

Method: 构建了一个简单的管道，包含两个神经网络：一个“预言器”和一个“优化器”。核心方法是将逐点值优化问题转换为神经网络训练问题。

Result: 在温度控制方面，成功地将稳定性提高了约3倍，优于普通的求解器。

Conclusion: 所提出的基于双神经网络的管道为时间序列过程的稳定化提供了一种有效解决方案，在实际应用（如温度控制）中展现出显著的性能提升，且可能减少计算资源消耗。

Abstract: The stabilization of time series processes is a crucial problem that is
ubiquitous in various industrial fields. The application of machine learning to
its solution can have a decisive impact, improving both the quality of the
resulting stabilization with less computational resources required. In this
work, we present a simple pipeline consisting of two neural networks: the
oracle predictor and the optimizer, proposing a substitution of the point-wise
values optimization to the problem of the neural network training, which
successfully improves stability in terms of the temperature control by about 3
times compared to ordinary solvers.

</details>


### [166] [Task-Agnostic Contrastive Pretraining for Relational Deep Learning](https://arxiv.org/abs/2506.22530)
*Jakub Peleška,Gustav Šír*

Main category: cs.LG

TL;DR: 提出一种任务无关的对比预训练方法，用于关系深度学习（RDL），以解决现有RDL模型的可扩展性与复用性问题。


<details>
  <summary>Details</summary>
Motivation: 现有关系深度学习（RDL）模型依赖于任务特定的监督学习，为每个预测任务训练单独模型，限制了可扩展性和复用性。

Method: 提出一种新颖的任务无关对比预训练方法，实现数据库范围的表示学习。该方法引入了行级、链接级和上下文级三种对比目标，旨在捕获关系数据的结构和语义异构性，并通过模块化RDL架构和高效采样策略实现。

Result: 初步结果显示，在标准RDL基准测试中，对预训练模型进行微调显著优于从头开始训练。

Conclusion: 所提出的方法在学习关系数据的可迁移表示方面展现出良好前景。

Abstract: Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph
Neural Network principles to learn directly from relational databases by
representing them as heterogeneous graphs. However, existing RDL models
typically rely on task-specific supervised learning, requiring training
separate models for each predictive task, which may hamper scalability and
reuse.
  In this work, we propose a novel task-agnostic contrastive pretraining
approach for RDL that enables database-wide representation learning. For that
aim, we introduce three levels of contrastive objectives$-$row-level,
link-level, and context-level$-$designed to capture the structural and semantic
heterogeneity inherent to relational data. We implement the respective
pretraining approach through a modular RDL architecture and an efficient
sampling strategy tailored to the heterogeneous database setting. Our
preliminary results on standard RDL benchmarks demonstrate that fine-tuning the
pretrained models measurably outperforms training from scratch, validating the
promise of the proposed methodology in learning transferable representations
for relational data.

</details>


### [167] [Exploration Behavior of Untrained Policies](https://arxiv.org/abs/2506.22566)
*Jacob Adamczyk*

Main category: cs.LG

TL;DR: 研究深度神经网络策略的架构如何在其训练前影响强化学习的探索行为。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的探索是一个基本挑战，尤其是在奖励稀疏或对抗的环境中。本研究旨在探讨深度神经网络策略的架构如何隐式地塑造训练前的探索行为。

Method: 通过理论和实验方法，在一个玩具模型中展示了如何从未经训练的策略生成弹道或扩散轨迹。利用无限宽度网络理论和连续时间极限，分析了未经训练策略返回相关动作并产生非平凡状态访问分布的现象，并讨论了标准架构下相应轨迹的分布。

Result: 未经训练的策略会产生相关联的动作，并导致非平凡的状态访问分布。研究揭示了策略架构（作为归纳偏置）对早期探索行为的影响。

Conclusion: 研究结果建立了一个理论和实验框架，证明了策略初始化可作为理解早期训练中探索行为的设计工具。

Abstract: Exploration remains a fundamental challenge in reinforcement learning (RL),
particularly in environments with sparse or adversarial reward structures. In
this work, we study how the architecture of deep neural policies implicitly
shapes exploration before training. We theoretically and empirically
demonstrate strategies for generating ballistic or diffusive trajectories from
untrained policies in a toy model. Using the theory of infinite-width networks
and a continuous-time limit, we show that untrained policies return correlated
actions and result in non-trivial state-visitation distributions. We discuss
the distributions of the corresponding trajectories for a standard
architecture, revealing insights into inductive biases for tackling
exploration. Our results establish a theoretical and experimental framework for
using policy initialization as a design tool to understand exploration behavior
in early training.

</details>


### [168] [The Hidden Link Between RLHF and Contrastive Learning](https://arxiv.org/abs/2506.22578)
*Xufei Lv,Haoyuan Sun,Xuefeng Bai,Min Zhang,Houde Liu,Kehai Chen*

Main category: cs.LG

TL;DR: 论文将RLHF和DPO从互信息最大化角度统一解释，并在此基础上提出新的对齐方法MIO，解决了DPO的后期性能下降问题，并在推理和数学任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: LLM与人类价值观的对齐受到广泛关注，但现有方法如RLHF成本高昂，且其内在机制及对推理能力的影响尚不完全清楚。本研究旨在深入理解RLHF和DPO的原理，并在此基础上提出更优的对齐方法。

Method: 1. 将RLHF和DPO解释为基于互信息（MI）最大化的对比学习方法，利用Donsker-Varadhan（DV）下界（即MINE估计器）。2. 基于此理论框架，提出了互信息优化（MIO）算法，该算法用Jensen-Shannon MI估计器替代了DV/MINE。

Result: 1. 解释了RLHF可能无法本质上激励LLM推理能力的原因。2. 理论分析和实证评估表明，MIO能有效缓解DPO中观测到的“chosen-likelihood”后期下降问题。3. MIO在多种复杂的推理和数学基准测试中取得了有竞争力或更优的性能。

Conclusion: 本研究通过将RLHF和DPO统一于互信息最大化框架，不仅加深了对现有对齐方法的理解，更提出了一种有效的新方法MIO，该方法在保持对齐效果的同时，提升了LLM在推理和数学任务上的表现。

Abstract: Alignment of large language models (LLMs) with human values has recently
garnered significant attention, with prominent examples including the canonical
yet costly Reinforcement Learning from Human Feedback (RLHF) and the simple
Direct Preference Optimization (DPO). In this work, we demonstrate that both
RLHF and DPO can be interpreted from the perspective of mutual information (MI)
maximization, uncovering a profound connection to contrastive learning. Within
this framework, both RLHF and DPO can be viewed as methods that perform
contrastive learning based on the positive and negative samples derived from
the base model, leveraging the Donsker-Varadhan (DV) lower bound on MI
(equivalently, the MINE estimator). This paradigm further explains why RLHF may
not intrinsically incentivize reasoning capacities in LLMs beyond what is
already present in the base model. Building on this perspective, we replace the
DV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual
Information Optimization (MIO). Comprehensive theoretical analysis and
extensive empirical evaluations demonstrate that MIO mitigates the late-stage
decline in chosen-likelihood observed in DPO, achieving competitive or superior
performance across various challenging reasoning and mathematical benchmarks.
We will release the model and code upon acceptance.

</details>


### [169] [Are Fast Methods Stable in Adversarially Robust Transfer Learning?](https://arxiv.org/abs/2506.22602)
*Joshua C. Zhao,Saurabh Bagchi*

Main category: cs.LG

TL;DR: 在对抗鲁棒性迁移学习中，快速梯度符号法（FGSM）表现出惊人的稳定性，且相比投影梯度下降（PGD）能显著降低计算成本，同时保持良好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 对抗训练模型计算成本高昂，即使在迁移学习中，对抗微调仍需要大量时间，成为实现对抗鲁棒性的一大挑战。

Method: 本研究重新审视了在鲁棒性迁移学习中使用快速梯度符号法（FGSM），以降低对抗微调的计算成本，并与常用的投影梯度下降（PGD）方法进行了比较。研究还探讨了FGSM与参数高效微调方法结合时的表现。

Result: 研究发现，FGSM在对抗微调中比从头训练更稳定，在标准扰动预算（如ε=4或ε=8）下没有灾难性过拟合问题。结合参数高效微调方法后，FGSM的稳定性进一步增强。与PGD相比，FGSM在鲁棒性上仅有微小损失（ε=4时平均损失0.39%，ε=8时平均损失1.39%），但训练时间减少了4倍。

Conclusion: FGSM不仅可以成为对抗鲁棒性迁移学习中比PGD显著更高效的替代方案，而且表现同样出色，为其在实际应用中推广提供了支持。

Abstract: Transfer learning is often used to decrease the computational cost of model
training, as fine-tuning a model allows a downstream task to leverage the
features learned from the pre-training dataset and quickly adapt them to a new
task. This is particularly useful for achieving adversarial robustness, as
adversarially training models from scratch is very computationally expensive.
However, high robustness in transfer learning still requires adversarial
training during the fine-tuning phase, which requires up to an order of
magnitude more time than standard fine-tuning. In this work, we revisit the use
of the fast gradient sign method (FGSM) in robust transfer learning to improve
the computational cost of adversarial fine-tuning. We surprisingly find that
FGSM is much more stable in adversarial fine-tuning than when training from
scratch. In particular, FGSM fine-tuning does not suffer from any issues with
catastrophic overfitting at standard perturbation budgets of $\varepsilon=4$ or
$\varepsilon=8$. This stability is further enhanced with parameter-efficient
fine-tuning methods, where FGSM remains stable even up to $\varepsilon=32$ for
linear probing. We demonstrate how this stability translates into performance
across multiple datasets. Compared to fine-tuning with the more commonly used
method of projected gradient descent (PGD), on average, FGSM only loses 0.39%
and 1.39% test robustness for $\varepsilon=4$ and $\varepsilon=8$ while using
$4\times$ less training time. Surprisingly, FGSM may not only be a
significantly more efficient alternative to PGD in adversarially robust
transfer learning but also a well-performing one.

</details>


### [170] [Hierarchical Modeling and Architecture Optimization: Review and Unified Framework](https://arxiv.org/abs/2506.22621)
*Paul Saves,Edward Hallé-Hannan,Jasper Bussemaker,Youssef Diouane,Nathalie Bartoli*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架，用于处理模拟问题中复杂的、分层和条件性的混合变量输入空间。通过引入元变量、部分指定变量和设计空间图，该框架泛化了现有方法，并在SMT 2.0中实现，用于复杂系统设计的贝叶斯优化。


<details>
  <summary>Details</summary>
Motivation: 模拟问题中涉及混合变量输入时，领域常呈现分层、条件、异构或树状结构，这对数据表示、建模和优化带来了挑战。现有的方法需要一个更统一、更通用的框架来解决这些复杂性。

Method: 本文提出一个统一框架，支持连续、整数和分类变量。引入“元变量”来建模条件和分层结构，以及“部分指定变量”来处理上下文依赖的激活。通过结合特征建模和图论引入“设计空间图”，以捕获变量间的分层关系。该框架支持代理模型、分层核和距离，并已在开源的Surrogate Modeling Toolbox (SMT 2.0)中实现。

Result: 所提出的方法已在SMT 2.0中成功实现。其能力通过在复杂系统设计（包括绿色飞机架构案例研究）中应用贝叶斯优化得到了验证，表明该框架能有效处理和优化具有复杂结构化输入域的问题。

Conclusion: 该统一框架有效解决了模拟问题中复杂结构化输入空间的挑战，通过引入创新的变量描述和图论方法，泛化并提升了现有处理方法的能力。其在SMT 2.0中的实现为复杂系统设计提供了实用的建模和优化工具。

Abstract: Simulation-based problems involving mixed-variable inputs frequently feature
domains that are hierarchical, conditional, heterogeneous, or tree-structured.
These characteristics pose challenges for data representation, modeling, and
optimization. This paper reviews extensive literature on these structured input
spaces and proposes a unified framework that generalizes existing approaches.
In this framework, input variables may be continuous, integer, or categorical.
A variable is described as meta if its value governs the presence of other
decreed variables, enabling the modeling of conditional and hierarchical
structures.
  We further introduce the concept of partially-decreed variables, whose
activation depends on contextual conditions. To capture these inter-variable
hierarchical relationships, we introduce design space graphs, combining
principles from feature modeling and graph theory. This allows the definition
of general hierarchical domains suitable for describing complex system
architectures. The framework supports the use of surrogate models over such
domains and integrates hierarchical kernels and distances for efficient
modeling and optimization. The proposed methods are implemented in the
open-source Surrogate Modeling Toolbox (SMT 2.0), and their capabilities are
demonstrated through applications in Bayesian optimization for complex system
design, including a case study in green aircraft architecture.

</details>


### [171] [A hierarchical Vovk-Azoury-Warmuth forecaster with discounting for online regression in RKHS](https://arxiv.org/abs/2506.22631)
*Dmitry B. Rokhlin*

Main category: cs.LG

TL;DR: 研究将DVAW预测器推广到非参数RKHS在线回归问题，通过随机特征近似和自适应算法H-VAW-D实现了具有竞争性动态遗憾界限的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有DVAW方法在有限维设置中实现了在线回归的最优动态遗憾。研究动机在于将其推广到更具挑战性的非参数（RKHS）领域。

Method: 将DVAW框架与随机特征近似相结合以处理非参数域。提出一个名为H-VAW-D的完全自适应分层算法，该算法能够学习折扣因子和随机特征的数量。

Result: H-VAW-D算法的每次迭代计算复杂度为$O(T\ln T)$。该算法实现了$O(T^{2/3}P_T^{1/3} + \sqrt{T}\ln T)$的预期动态遗憾，其中$P_T$是比较器序列的函数路径长度。

Conclusion: 本文成功地将最优DVAW预测器提升至非参数RKHS在线回归领域，所提出的H-VAW-D算法具有自适应性、计算效率高，并提供了有竞争力的动态遗憾界限。

Abstract: We study the problem of online regression with the unconstrained quadratic
loss against a time-varying sequence of functions from a Reproducing Kernel
Hilbert Space (RKHS). Recently, Jacobsen and Cutkosky (2024) introduced a
discounted Vovk-Azoury-Warmuth (DVAW) forecaster that achieves optimal dynamic
regret in the finite-dimensional case. In this work, we lift their approach to
the non-parametric domain by synthesizing the DVAW framework with a random
feature approximation. We propose a fully adaptive, hierarchical algorithm,
which we call H-VAW-D (Hierarchical Vovk-Azoury-Warmuth with Discounting), that
learns both the discount factor and the number of random features. We prove
that this algorithm, which has a per-iteration computational complexity of
$O(T\ln T)$, achieves an expected dynamic regret of $O(T^{2/3}P_T^{1/3} +
\sqrt{T}\ln T)$, where $P_T$ is the functional path length of a comparator
sequence.

</details>


### [172] [Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training](https://arxiv.org/abs/2506.22638)
*Aadim Nepal,Safal Shrestha,Anubhav Shrestha,Minwu Kim,Keith Ross*

Main category: cs.LG

TL;DR: 研究发现，大语言模型在数学推理任务中存在特定的、关键的Transformer层结构，这些结构在多种后训练方法中保持不变，对模型性能至关重要，并与主要表征转换相关。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在经过指令微调、强化学习或知识蒸馏等后训练方法后，其数学推理能力的提升是否源于Transformer层结构的重大改变，还是仅为微小调整。

Method: 通过系统的层级消融实验，在数学推理基准上评估了基础模型、指令微调模型、知识蒸馏模型和强化学习模型，并分析了其层级重要性结构。

Result: 数学推理任务存在特定的层级重要性结构，该结构在所有后训练范式中均保持不变；移除这些关键层会导致高达80%的准确率下降；非数学任务（如事实回忆）无关键层；这些关键层也是主要表征转换发生的层。

Conclusion: 数学推理需要专门的层，这些层可能在预训练期间就已形成，而其他非推理任务则不需要，这表明数学推理能力依赖于模型中特定的结构性特性。

Abstract: Large language models can exhibit improved mathematical reasoning
capabilities following post-training with instruction tuning, reinforcement
learning, or knowledge distillation. However, it remains unclear whether these
improvements are driven by major changes in transformer layers or from minor
adjustments that leave the relative layer importance structures of the base
model largely unchanged. We investigate this question through systematic
layer-wise ablation experiments, examining base, instruction-tuned,
knowledge-distilled, and reinforcement learning variants on mathematical
reasoning benchmarks. Our findings show that mathematical reasoning gives rise
to a specific layer importance structure, and this structure persists across
all post-training paradigms. Removal of such layers causes accuracy drops of up
to 80%. In contrast, non-mathematical tasks like factual recall exhibit no
critical layers. This distinction suggests that mathematical reasoning requires
specialized layers that emerge during pre-training, while other non-reasoning
tasks do not. From an information-theoretic perspective, we also observe that
these critical layers are the same layers where major representational
transformation occurs.

</details>


### [173] [Cost-effective Reduced-Order Modeling via Bayesian Active Learning](https://arxiv.org/abs/2506.22645)
*Amir Hossein Rahmati,Nathan M. Urban,Byung-Jun Yoon,Xiaoning Qian*

Main category: cs.LG

TL;DR: 提出BayPOD-AL主动学习框架，通过不确定性感知贝叶斯POD方法高效构建复杂系统降阶模型，减少数据需求与计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习替代模型需要大量训练数据来准确捕捉系统动力学，这限制了它们在实际应用中的可行性。

Method: 提出BayPOD-AL，一个基于不确定性感知贝叶斯本征正交分解（POD）的主动学习框架，旨在从高保真全阶模型中高效学习复杂系统的降阶模型。

Result: 实验结果表明，BayPOD-AL能有效识别信息量大的数据，并相比其他不确定性引导的主动学习策略，显著降低训练数据集构建的计算成本。此外，它在更高时间分辨率的数据集上也展现出良好的泛化性和效率。

Conclusion: BayPOD-AL是学习复杂系统降阶模型的有效且高效的方法，解决了机器学习替代模型对大量数据的依赖问题，并具备良好的泛化能力。

Abstract: Machine Learning surrogates have been developed to accelerate solving systems
dynamics of complex processes in different science and engineering
applications. To faithfully capture governing systems dynamics, these methods
rely on large training datasets, hence restricting their applicability in
real-world problems. In this work, we propose BayPOD-AL, an active learning
framework based on an uncertainty-aware Bayesian proper orthogonal
decomposition (POD) approach, which aims to effectively learn reduced-order
models from high-fidelity full-order models representing complex systems.
Experimental results on predicting the temperature evolution over a rod
demonstrate BayPOD-AL's effectiveness in suggesting the informative data and
reducing computational cost related to constructing a training dataset compared
to other uncertainty-guided active learning strategies. Furthermore, we
demonstrate BayPOD-AL's generalizability and efficiency by evaluating its
performance on a dataset of higher temporal resolution than the training
dataset.

</details>


### [174] [Learning Stochastic Multiscale Models](https://arxiv.org/abs/2506.22655)
*Andrew F. Ilersich,Prasanth B. Nair*

Main category: cs.LG

TL;DR: 本文提出一种从观测数据中学习随机多尺度模型的方法，通过粗网格和辅助状态处理未解析尺度，并利用无前向求解器的变分推断方法学习模型参数，实现在相同分辨率下优于传统模拟方法的预测精度。


<details>
  <summary>Details</summary>
Motivation: 物理科学中的动力系统常涉及宽泛的长度和时间尺度，直接数值模拟（DNS）因需要对最精细尺度进行离散化，导致状态空间维度高，计算成本巨大，构成显著的计算挑战。

Method: 研究提出一种从观测数据中直接学习随机多尺度模型（以随机微分方程形式）的方法。该方法在粗网格上解析状态，并引入辅助状态以捕获未解析尺度的影响。模型参数通过一种现代的、无前向求解器的分摊变分推断方法学习。该方法借鉴了流体力学中大涡模拟等物理多尺度建模方法，并结合数据驱动学习。

Result: 数值研究表明，所学习的多尺度模型在相同分辨率下，比直接数值模拟和闭合型模型具有更优越的预测精度。

Conclusion: 该研究成功开发了一种高效的数据驱动随机多尺度建模方法，有效解决了复杂动力系统的高维计算挑战，并在预测准确性方面超越了现有技术。

Abstract: The physical sciences are replete with dynamical systems that require the
resolution of a wide range of length and time scales. This presents significant
computational challenges since direct numerical simulation requires
discretization at the finest relevant scales, leading to a high-dimensional
state space. In this work, we propose an approach to learn stochastic
multiscale models in the form of stochastic differential equations directly
from observational data. Our method resolves the state on a coarse mesh while
introducing an auxiliary state to capture the effects of unresolved scales. We
learn the parameters of the multiscale model using a modern forward-solver-free
amortized variational inference method. Our approach draws inspiration from
physics-based multiscale modeling approaches, such as large-eddy simulation in
fluid dynamics, while learning directly from data. We present numerical studies
to demonstrate that our learned multiscale models achieve superior predictive
accuracy compared to direct numerical simulation and closure-type models at
equivalent resolution.

</details>


### [175] [DistShap: Scalable GNN Explanations with Distributed Shapley Values](https://arxiv.org/abs/2506.22668)
*Selahattin Akkas,Aditya Devarakonda,Ariful Azad*

Main category: cs.LG

TL;DR: 为解决图神经网络(GNN)解释的计算成本高昂问题，本文提出了DistShap，一种基于Shapley值的并行算法，可分布式地解释GNN预测，显著提升准确性并首次实现对百万级特征模型的扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着图神经网络(GNNs)的广泛应用，解释其预测变得愈发重要。然而，将预测归因于具体边或特征的计算成本极高，例如对拥有数百万候选边节点进行GNN解释。

Method: 本文提出DistShap，一种并行算法，通过将Shapley值解释分布到多个GPU上实现。其工作原理包括在分布式环境下采样子图，在GPU上并行执行GNN推理，以及解决一个分布式最小二乘问题以计算边的重要性分数。

Result: DistShap在准确性上超越了大多数现有GNN解释方法，并且是首个能扩展到拥有百万级特征的GNN模型（利用NERSC Perlmutter超级计算机上的多达128个GPU）的解释方法。

Conclusion: DistShap有效解决了GNN解释的计算效率和扩展性挑战，为大规模GNN模型提供了高效且准确的预测解释能力。

Abstract: With the growing adoption of graph neural networks (GNNs), explaining their
predictions has become increasingly important. However, attributing predictions
to specific edges or features remains computationally expensive. For example,
classifying a node with 100 neighbors using a 3-layer GNN may involve
identifying important edges from millions of candidates contributing to the
prediction. To address this challenge, we propose DistShap, a parallel
algorithm that distributes Shapley value-based explanations across multiple
GPUs. DistShap operates by sampling subgraphs in a distributed setting,
executing GNN inference in parallel across GPUs, and solving a distributed
least squares problem to compute edge importance scores. DistShap outperforms
most existing GNN explanation methods in accuracy and is the first to scale to
GNN models with millions of features by using up to 128 GPUs on the NERSC
Perlmutter supercomputer.

</details>


### [176] [Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment](https://arxiv.org/abs/2506.22685)
*Anh Bui,Trang Vu,Trung Le,Junae Kim,Tamas Abraham,Rollin Omari,Amar Kaur,Dinh Phung*

Main category: cs.LG

TL;DR: 本文旨在解决生成式个性化中的语义崩溃问题，该问题导致学习到的视觉概念（V*）偏离其原始含义并主导多概念提示。通过提出一种在推理时调整预训练嵌入幅度和方向的免训练方法，有效缓解了此问题并显著改善了文本-图像对齐。


<details>
  <summary>Details</summary>
Motivation: 生成式个性化中存在“语义崩溃”问题，即学习到的视觉概念（V*）会逐渐偏离其原始文本含义，并在多概念输入提示中过度主导其他概念。这导致复杂提示的语义丰富性降低，并产生无法捕捉预期概念的简化输出图像。研究发现其根本原因在于无约束优化导致学习到的嵌入V*在嵌入空间中任意漂移。

Method: 提出一种简单而有效的免训练方法，该方法在推理时调整预训练嵌入的幅度和方向，从而有效缓解语义崩溃问题。

Result: 所提出的方法广泛适用于不同的个性化方法，并在各种使用案例中显著改善了文本-图像对齐效果。

Conclusion: 通过在推理时对预训练嵌入进行幅度和方向的调整，可以有效且无需训练地解决生成式个性化中的语义崩溃问题，从而提升复杂提示的语义理解和图像生成质量。

Abstract: In this paper, we investigate the semantic collapsing problem in generative
personalization, an under-explored topic where the learned visual concept
($V^*$) gradually shifts from its original textual meaning and comes to
dominate other concepts in multi-concept input prompts. This issue not only
reduces the semantic richness of complex input prompts like "a photo of $V^*$
wearing glasses and playing guitar" into simpler, less contextually rich forms
such as "a photo of $V^*$" but also leads to simplified output images that fail
to capture the intended concept.
  We identify the root cause as unconstrained optimisation, which allows the
learned embedding $V^*$ to drift arbitrarily in the embedding space, both in
direction and magnitude. To address this, we propose a simple yet effective
training-free method that adjusts the magnitude and direction of pre-trained
embedding at inference time, effectively mitigating the semantic collapsing
problem. Our method is broadly applicable across different personalization
methods and demonstrates significant improvements in text-image alignment in
diverse use cases. Our code is anonymously published at
https://anonymous.4open.science/r/Embedding-Adjustment.

</details>


### [177] [Residual Matrix Transformers: Scaling the Size of the Residual Stream](https://arxiv.org/abs/2506.22696)
*Brian Mak,Jeffrey Flanigan*

Main category: cs.LG

TL;DR: 该研究提出了一种名为残差矩阵Transformer (RMT) 的新型模型，通过将Transformer的残差流替换为外积记忆矩阵，显著提升了模型效率和性能，同时减少了计算资源需求。


<details>
  <summary>Details</summary>
Motivation: Transformer模型中的残差流扮演着信息存储和访问的“记忆总线”角色，研究动机在于改变并优化这种信息检索和存储机制。

Method: 将Transformer的残差流替换为外积记忆矩阵，构建了残差矩阵Transformer (RMT) 模型。同时对Transformer和RMT进行了理论分析。

Result: RMT展现出多项优势：残差流大小可独立于计算和模型大小进行扩展以提升性能；在达到相同损失时，RMT的FLOPs减少58%，参数减少25%，训练tokens减少41%；在下游评估中RMT表现优于Transformer。理论分析显示RMT能更高效地扩展残差流，并改善方差传播特性。

Conclusion: 残差矩阵Transformer (RMT) 通过革新残差流机制，提供了一种在效率、资源消耗和下游任务表现上均优于传统Transformer的解决方案。

Abstract: The residual stream acts as a memory bus where transformer layers both store
and access features (Elhage et al., 2021). We consider changing the mechanism
for retrieving and storing information in the residual stream, and replace the
residual stream of the transformer with an outer product memory matrix
(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix
Transformer (RMT). We find that the RMT enjoys a number of attractive
properties: 1) the size of the residual stream can be scaled independently of
compute and model size, improving performance, 2) the RMT can achieve the same
loss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%
fewer training tokens tokens, and 3) the RMT outperforms the transformer on
downstream evaluations. We theoretically analyze the transformer and the RMT,
and show that the RMT allows for more efficient scaling of the residual stream,
as well as improved variance propagation properties. Code for this project can
be found at https://github.com/bmac3/residual-matrix-transformer.

</details>


### [178] [FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets](https://arxiv.org/abs/2506.22708)
*Shrenik Jadhav,Birva Sevak,Srijita Das,Akhtar Hussain,Wencong Su,Van-Hai Bui*

Main category: cs.LG

TL;DR: 针对P2P交易中缺乏公平性框架的问题，本文提出FairMarket-RL，一个结合大语言模型（LLM）和强化学习（RL）的混合框架，旨在实现公平的去中心化能源交易。


<details>
  <summary>Details</summary>
Motivation: P2P交易日益成为去中心化市场监管的关键机制，但现有方法通常缺乏鲁棒的框架来确保交易公平性。

Method: 提出FairMarket-RL混合框架，将LLM与RL结合。LLM作为实时公平性评估器，使用买家公平性（FTB）和卖家间公平性（FBS）两个指标评估交易。这些公平性分数通过自适应的LLM引导奖励塑形环（使用预设的λ-系数）整合到智能体的奖励中，取代了脆弱的基于规则的公平性约束。智能体使用独立近端策略优化（IPPO）进行训练。

Result: 在模拟P2P微电网中，该框架实现了公平的交易结果，满足了90%以上的买家需求，保持了卖家公平利润，并使FTB和FBS分数始终高于0.80。训练过程表明，公平性反馈改善了收敛性，减少了买家短缺，并缩小了卖家之间的利润差异。

Conclusion: FairMarket-RL为去中心化能源系统中的自主交易提供了一个可扩展、公平驱动的解决方案。其基于语言的评估器使其能够自然扩展，并能应用于包含家庭“产消者”的大型配电系统，具有实际应用价值。

Abstract: Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for
decentralized market regulation, yet existing approaches often lack robust
frameworks to ensure fairness. This paper presents FairMarket-RL, a novel
hybrid framework that combines Large Language Models (LLMs) with Reinforcement
Learning (RL) to enable fairness-aware trading agents. In a simulated P2P
microgrid with multiple sellers and buyers, the LLM acts as a real-time
fairness critic, evaluating each trading episode using two metrics:
Fairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness
scores are integrated into agent rewards through scheduled
{\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that
replaces brittle, rule-based fairness constraints. Agents are trained using
Independent Proximal Policy Optimization (IPPO) and achieve equitable outcomes,
fulfilling over 90% of buyer demand, maintaining fair seller margins, and
consistently reaching FTB and FBS scores above 0.80. The training process
demonstrates that fairness feedback improves convergence, reduces buyer
shortfalls, and narrows profit disparities between sellers. With its
language-based critic, the framework scales naturally, and its extension to a
large power distribution system with household prosumers illustrates its
practical applicability. FairMarket-RL thus offers a scalable, equity-driven
solution for autonomous trading in decentralized energy systems.

</details>


### [179] [Generalized Linear Mode Connectivity for Transformers](https://arxiv.org/abs/2506.22712)
*Alexander Theus,Alessandro Cabodi,Sotiris Anagnostidis,Antonio Orvieto,Sidak Pal Singh,Valentina Boeva*

Main category: cs.LG

TL;DR: 为理解神经网络损失景观的几何结构，本文提出了一个统一框架，涵盖四种对称性。该框架首次成功在Vision Transformers和GPT-2模型中发现低/零损失线性连接路径，揭示了损失景观的深层结构，并强调了对称性感知分析的重要性。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络损失景观的几何结构对于泛化和优化至关重要。线性模式连接性（LMC）是一个显著现象，但参数空间中的对称性（如神经元置换）常将其遮蔽。现有方法主要关注神经元重排，范围有限，无法捕获Transformer等现代架构的更丰富对称性。

Method: 引入了一个统一的框架，该框架能够捕获四种对称性类别：置换（permutations）、半置换（semi-permutations）、正交变换（orthogonal transformations）和一般可逆映射（general invertible maps）。

Result: 首次成功发现了独立训练的Vision Transformers和GPT-2模型之间存在低损失或零损失的线性插值路径。

Conclusion: 这些结果揭示了损失景观中更深层次的结构，并强调了对称性感知分析对于理解模型空间几何形状的重要性。

Abstract: Understanding the geometry of neural network loss landscapes is a central
question in deep learning, with implications for generalization and
optimization. A striking phenomenon is linear mode connectivity (LMC), where
independently trained models can be connected by low- or zero-loss paths,
despite appearing to lie in separate loss basins. However, this is often
obscured by symmetries in parameter space -- such as neuron permutations --
which make functionally equivalent models appear dissimilar. Prior work has
predominantly focused on neuron re-ordering through permutations, but such
approaches are limited in scope and fail to capture the richer symmetries
exhibited by modern architectures such as Transformers. In this work, we
introduce a unified framework that captures four symmetry classes:
permutations, semi-permutations, orthogonal transformations, and general
invertible maps -- broadening the set of valid reparameterizations and
subsuming many previous approaches as special cases. Crucially, this
generalization enables, for the first time, the discovery of low- and
zero-barrier linear interpolation paths between independently trained Vision
Transformers and GPT-2 models. These results reveal deeper structure in the
loss landscape and underscore the importance of symmetry-aware analysis for
understanding model space geometry.

</details>


### [180] [BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute](https://arxiv.org/abs/2506.22716)
*Dujian Ding,Ankur Mallick,Shaokun Zhang,Chi Wang,Daniel Madrigal,Mirian Del Carmen Hipolito Garcia,Menglin Xia,Laks V. S. Lakshmanan,Qingyun Wu,Victor Rühle*

Main category: cs.LG

TL;DR: 一个名为BEST-Route的新型LLM查询路由框架，通过动态选择模型和采样响应数量，显著降低了大型语言模型的部署成本，同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）查询路由方法在面对小型（廉价）模型时，由于仅生成一个响应，其质量往往不足以媲美大型（昂贵）模型。这导致它们过度依赖大型模型，未能充分实现潜在的成本节约。

Method: 本研究提出了BEST-Route框架，它利用了小型模型通过生成多个响应并从中选择最佳响应，可以在保持成本优势的同时提升质量的原理。该框架根据查询难度和预设的质量阈值，动态选择合适的模型以及需要采样的响应数量。

Result: 在真实世界数据集上的实验表明，BEST-Route方法在性能下降不到1%的情况下，成功将部署成本降低了高达60%。

Conclusion: BEST-Route通过优化LLM查询路由策略，有效解决了大规模部署LLM的成本效益问题，实现了成本大幅降低与性能几乎无损的双重目标。

Abstract: Large language models (LLMs) are powerful tools but are often expensive to
deploy at scale. LLM query routing mitigates this by dynamically assigning
queries to models of varying cost and quality to obtain a desired trade-off.
Prior query routing approaches generate only one response from the selected
model and a single response from a small (inexpensive) model was often not good
enough to beat a response from a large (expensive) model due to which they end
up overusing the large model and missing out on potential cost savings.
However, it is well known that for small models, generating multiple responses
and selecting the best can enhance quality while remaining cheaper than a
single large-model response. We leverage this idea to propose BEST-Route, a
novel routing framework that chooses a model and the number of responses to
sample from it based on query difficulty and the quality thresholds.
Experiments on real-world datasets demonstrate that our method reduces costs by
up to 60% with less than 1% performance drop.

</details>


### [181] [Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery](https://arxiv.org/abs/2506.22732)
*Hao Shu,Jicheng Li,Tianyv Lei,Lijun Sun*

Main category: cs.LG

TL;DR: 针对交通时空数据缺失与噪声双重退化问题，本文提出RTC-GTNLN模型。该模型引入非凸张量L1-L2范数，有效融合全局低秩性和局部一致性，在真实交通数据集上表现优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 交通时空数据常因传感器故障和通信中断面临缺失值和噪声的双重退化，影响下游应用可靠性。传统张量补全方法无法有效处理噪声，而现有鲁棒张量补全(RTC)方法存在凸秩替代过度松弛和局部一致性利用不足的问题，导致模型精度不足。

Method: 提出RTC-GTNLN模型。首先引入新颖的非凸张量L1-L2范数作为低秩表示工具；其次，通过特征融合策略，在梯度域引入梯度张量L1-L2范数；最后将此范数整合到RTC框架中，以充分利用全局低秩性和局部一致性，并有效处理交通数据的缺失和噪声挑战。

Result: 在多个真实交通数据集上进行的大量实验表明，RTC-GTNLN模型在同时存在缺失值和噪声的复杂恢复场景中，性能始终优于现有最先进的方法。

Conclusion: 本文提出的RTC-GTNLN模型通过有效利用全局低秩性和局部一致性，成功解决了交通数据中缺失值和噪声的双重退化挑战，并在复杂恢复场景中展现出卓越的性能。

Abstract: In real-world scenarios, spatiotemporal traffic data frequently experiences
dual degradation from missing values and noise caused by sensor malfunctions
and communication failures. Therefore, effective data recovery methods are
essential to ensure the reliability of downstream data-driven applications.
while classical tensor completion methods have been widely adopted, they are
incapable of modeling noise, making them unsuitable for complex scenarios
involving simultaneous data missingness and noise interference. Existing Robust
Tensor Completion (RTC) approaches offer potential solutions by separately
modeling the actual tensor data and noise. However, their effectiveness is
often constrained by the over-relaxation of convex rank surrogates and the
suboptimal utilization of local consistency, leading to inadequate model
accuracy. To address these limitations, we first introduce the tensor L1-L2
norm, a novel non-convex tensor rank surrogate that functions as an effective
low-rank representation tool. Leveraging an advanced feature fusion strategy,
we further develop the gradient tensor L1-L2 norm by incorporating the tensor
L1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear
L1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via
Gradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully
exploits both global low-rankness and local consistency without trade-off
parameter, but also effectively handles the dual degradation challenges of
missing data and noise in traffic data. Extensive experiments conducted on
multiple real-world traffic datasets demonstrate that the RTC-GTNLN model
consistently outperforms existing state-of-the-art methods in complex recovery
scenarios involving simultaneous missing values and noise.

</details>


### [182] [FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision](https://arxiv.org/abs/2506.22771)
*Jingxiao Ma,Priyadarshini Panda,Sherief Reda*

Main category: cs.LG

TL;DR: 本文提出一种基于前向-前向（FF）算法的INT8量化训练方法，并引入“前瞻”机制，显著提升了资源受限边缘设备的神经网络训练效率（速度、能耗、内存），同时保持了竞争力。


<details>
  <summary>Details</summary>
Motivation: 反向传播在时间与能耗上的低效性限制了其在资源受限边缘设备上的应用。尽管低精度推理量化研究广泛，但训练方面的探索较少。前向-前向（FF）算法作为反向传播的替代方案，有望减少内存占用，但其自身局限性需要解决，且其在量化训练中的应用尚待深入研究。

Method: 本研究提出了一种INT8量化训练方法，利用FF算法的逐层策略来稳定梯度量化。此外，引入了一种新颖的“前瞻”（look-ahead）机制，以解决FF算法的局限性并提高模型精度。

Result: 在NVIDIA Jetson Orin Nano板上的实验表明，该方法实现了4.6%的训练速度提升、8.3%的能耗节省和27.0%的内存使用量减少，同时与现有最佳方法相比，保持了具有竞争力的精度。

Conclusion: 所提出的INT8量化FF训练方法，结合“前瞻”机制，能够显著提升神经网络在边缘设备上的训练效率，包括速度、能耗和内存利用率，同时不牺牲模型精度，为资源受限环境下的高效训练提供了可行方案。

Abstract: Backpropagation has been the cornerstone of neural network training for
decades, yet its inefficiencies in time and energy consumption limit its
suitability for resource-constrained edge devices. While low-precision neural
network quantization has been extensively researched to speed up model
inference, its application in training has been less explored. Recently, the
Forward-Forward (FF) algorithm has emerged as a promising alternative to
backpropagation, replacing the backward pass with an additional forward pass.
By avoiding the need to store intermediate activations for backpropagation, FF
can reduce memory footprint, making it well-suited for embedded devices. This
paper presents an INT8 quantized training approach that leverages FF's
layer-by-layer strategy to stabilize gradient quantization. Furthermore, we
propose a novel "look-ahead" scheme to address limitations of FF and improve
model accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board
demonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in
memory usage, while maintaining competitive accuracy compared to the
state-of-the-art.

</details>


### [183] [Multimodal Atmospheric Super-Resolution With Deep Generative Models](https://arxiv.org/abs/2506.22780)
*Dibyajyoti Chakraborty,Haiwen Guan,Jason Stock,Troy Arcomano,Guido Cervone,Romit Maulik*

Main category: cs.LG

TL;DR: 本文利用基于分数的扩散模型，结合多模态低分辨率及稀疏观测数据，实现了高维动态系统的超分辨率重建，并提供了不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 基于分数的扩散模型能够学习复杂分布并实现零样本条件生成，这为在贝叶斯框架下将预训练模型与在线数据融合提供了一种新颖范式，特别适用于处理实时可用的低分辨率和稀疏传感器测量数据。

Method: 将基于分数的扩散模型应用于高维动态系统的超分辨率任务。通过融合多源低保真度测量数据（如粗粒度表示和非结构化实验观测），并利用基于分数的采样进行不确定性估计。实验以ERA5大气数据集的超分辨率为例，输入包括ERA5的粗粒度表示和/或IGRA无线电探空仪的稀疏观测。

Result: 在给定多源低保真度测量的情况下，能够准确恢复高维状态。该生成模型在时空重建过程中，能够有效平衡多个数据集模态的影响。同时，基于分数的采样可用于不确定性估计。

Conclusion: 基于分数的扩散模型在结合多模态低保真度数据时，能有效地实现高维动态系统的超分辨率重建，并提供不确定性估计，为数据与模型融合、实时数据整合及高维状态恢复提供了强大工具。

Abstract: Score-based diffusion modeling is a generative machine learning algorithm
that can be used to sample from complex distributions. They achieve this by
learning a score function, i.e., the gradient of the log-probability density of
the data, and reversing a noising process using the same. Once trained,
score-based diffusion models not only generate new samples but also enable
zero-shot conditioning of the generated samples on observed data. This promises
a novel paradigm for data and model fusion, wherein the implicitly learned
distributions of pretrained score-based diffusion models can be updated given
the availability of online data in a Bayesian formulation. In this article, we
apply such a concept to the super-resolution of a high-dimensional dynamical
system, given the real-time availability of low-resolution and experimentally
observed sparse sensor measurements from multimodal data. Additional analysis
on how score-based sampling can be used for uncertainty estimates is also
provided. Our experiments are performed for a super-resolution task that
generates the ERA5 atmospheric dataset given sparse observations from a
coarse-grained representation of the same and/or from unstructured experimental
observations of the IGRA radiosonde dataset. We demonstrate accurate recovery
of the high dimensional state given multiple sources of low-fidelity
measurements. We also discover that the generative model can balance the
influence of multiple dataset modalities during spatiotemporal reconstructions.

</details>


### [184] [Riemannian-Geometric Fingerprints of Generative Models](https://arxiv.org/abs/2506.22802)
*Hae Jin Song,Laurent Itti*

Main category: cs.LG

TL;DR: 提出一种基于黎曼几何的新框架，用于定义和计算生成模型的指纹，以有效区分不同模型、进行模型归因，并应对模型崩溃等挑战。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的广泛应用，出现了对模型归因和指纹识别的迫切需求，以保护知识产权、验证内容来源及应对模型生成数据回流导致的模型崩溃威胁。现有研究缺乏一个定义、表示和分析生成模型指纹的正式框架。

Method: 研究采用几何学方法，基于黎曼几何提出生成模型伪影和指纹的新定义。该方法通过学习数据中的黎曼度量，将欧几里得距离和最近邻搜索替换为测地线距离和基于kNN的黎曼质心，从而将先前的工作推广到非欧几里得流形。此外，还开发了一种新的基于梯度的算法来实际计算指纹。

Result: 实验结果表明，该方法在区分大量生成模型方面更为有效，涵盖4个数据集、2种分辨率（64x64，256x256）、27种模型架构和2种模态（视觉、视觉-语言）。所提出的定义显著提升了模型归因性能，并对未见过的数据集、模型类型和模态展现出良好的泛化能力。

Conclusion: 基于黎曼几何的指纹定义和计算方法有效解决了生成模型指纹识别的难题，在模型归因、知识产权保护和内容溯源等实际应用中展现出显著的实用价值和泛化能力。

Abstract: Recent breakthroughs and rapid integration of generative models (GMs) have
sparked interest in the problem of model attribution and their fingerprints.
For instance, service providers need reliable methods of authenticating their
models to protect their IP, while users and law enforcement seek to verify the
source of generated content for accountability and trust. In addition, a
growing threat of model collapse is arising, as more model-generated data are
being fed back into sources (e.g., YouTube) that are often harvested for
training ("regurgitative training"), heightening the need to differentiate
synthetic from human data. Yet, a gap still exists in understanding generative
models' fingerprints, we believe, stemming from the lack of a formal framework
that can define, represent, and analyze the fingerprints in a principled way.
To address this gap, we take a geometric approach and propose a new definition
of artifact and fingerprint of GMs using Riemannian geometry, which allows us
to leverage the rich theory of differential geometry. Our new definition
generalizes previous work (Song et al., 2024) to non-Euclidean manifolds by
learning Riemannian metrics from data and replacing the Euclidean distances and
nearest-neighbor search with geodesic distances and kNN-based Riemannian center
of mass. We apply our theory to a new gradient-based algorithm for computing
the fingerprints in practice. Results show that it is more effective in
distinguishing a large array of GMs, spanning across 4 different datasets in 2
different resolutions (64 by 64, 256 by 256), 27 model architectures, and 2
modalities (Vision, Vision-Language). Using our proposed definition
significantly improves the performance on model attribution, as well as a
generalization to unseen datasets, model types, and modalities, suggesting its
practical efficacy.

</details>


### [185] [BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters](https://arxiv.org/abs/2506.22809)
*Cooper Doyle*

Main category: cs.LG

TL;DR: 提出BayesLoRA，一个将MC-Dropout集成到LoRA中的任务专用不确定性量化框架，旨在为代理决策提供可靠的置信度估计。


<details>
  <summary>Details</summary>
Motivation: 现有通用Transformer不确定性方法未能为下游工作流提供定制化保障，需要一种能让代理在不确定性下自省和调整行为的框架。

Method: 提出BayesLoRA，通过将MC-Dropout集成到低秩适配器（LoRA）中，构建任务专用不确定性量化框架。

Result: 数学和经验证明，LoRA适配器在微调分布之外表现出放大的方差，能为代理决策提供可靠的置信度估计。

Conclusion: BayesLoRA能够为代理在不确定性下的决策提供可靠的信心估计，使其能够更好地自省和调整行为，为下游工作流提供量身定制的保障。

Abstract: We propose BayesLoRA, a task-specific uncertainty quantification framework
that integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike
general-purpose transformer uncertainty methods, BayesLoRA provides guardrails
tailored to downstream workflows, enabling agents to introspect and modulate
behavior under uncertainty. We demonstrate mathematically and empirically that
LoRA adapters exhibit amplified variance outside fine-tuning distributions,
yielding reliable confidence estimates for agentic decision-making.

</details>


### [186] [Deep learning 40 years of human migration](https://arxiv.org/abs/2506.22821)
*Thomas Gaskin,Guy J. Abel*

Main category: cs.LG

TL;DR: 本文介绍了一个创新的、详细的全球年度移民流动和存量数据集（1990年至今），涵盖230个国家和地区，并按出生国细分。该数据通过深度循环神经网络建模18个协变量获得，性能优于传统方法，提供不确定性估计，并完全开源。


<details>
  <summary>Details</summary>
Motivation: 现有移民数据可能缺乏足够的细节、时间分辨率或准确性。研究旨在提供一个更全面、时间分辨率更高且更准确的全球移民图景，以支持对人类移民的深入研究，并识别数据收集的薄弱区域。

Method: 研究方法是：1. 构建了一个包含230个国家/地区间年度移民流动和存量的原创数据集，并按出生国细分。2. 使用一个深度循环神经网络（RNN）来学习移民模式，输入包括地理、经济、文化、社会和政治等18个协变量。3. 通过RNN的循环结构捕捉长期时间关联。4. 采用神经网络集成方法并考虑协变量不确定性，以获得估计值的置信区间。5. 模型及所有相关数据、代码均开源。

Result: 研究结果包括：1. 成功生成了一个涵盖1990年至今，按出生国细分的年度全球移民流动和存量数据集。2. 模型在各种未见测试集上表现出色，显著优于传统的五年期流动估计方法。3. 大幅提高了时间分辨率。4. 提供了所有估计值的置信区间，有助于识别最需要额外数据收集的地理区域。5. 所有模型、数据和代码均已公开，成为未来移民研究的宝贵资源。

Conclusion: 本文成功开发并开源了一个创新、高分辨率、详细且经过验证的全球年度移民数据集，通过先进的深度循环神经网络方法，显著提升了移民流动的估计精度和时间分辨率，为未来的移民研究提供了重要的基础资源，并能指导数据收集工作。

Abstract: We present a novel and detailed dataset on origin-destination annual
migration flows and stocks between 230 countries and regions, spanning the
period from 1990 to the present. Our flow estimates are further disaggregated
by country of birth, providing a comprehensive picture of migration over the
last 43 years. The estimates are obtained by training a deep recurrent neural
network to learn flow patterns from 18 covariates for all countries, including
geographic, economic, cultural, societal, and political information. The
recurrent architecture of the neural network means that the entire past can
influence current migration patterns, allowing us to learn long-range temporal
correlations. By training an ensemble of neural networks and additionally
pushing uncertainty on the covariates through the trained network, we obtain
confidence bounds for all our estimates, allowing researchers to pinpoint the
geographic regions most in need of additional data collection. We validate our
approach on various test sets of unseen data, demonstrating that it
significantly outperforms traditional methods estimating five-year flows while
delivering a significant increase in temporal resolution. The model is fully
open source: all training data, neural network weights, and training code are
made public alongside the migration estimates, providing a valuable resource
for future studies of human migration.

</details>


### [187] [xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection](https://arxiv.org/abs/2506.22837)
*Kamil Faber,Marcin Pietroń,Dominik Żurek,Roberto Corizzo*

Main category: cs.LG

TL;DR: 提出xLSTMAD，首个基于xLSTM的多变量时间序列异常检测方法，性能超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 尽管xLSTM在时间序列预测、无损压缩和大规模语言模型等任务中表现出色，但其在异常检测领域的应用仍是空白。

Method: 提出了xLSTMAD，一个集成了完整编码器-解码器xLSTM架构的异常检测方法，专为多变量时间序列设计。它包含两种变体：基于预测的xLSTMAD-F和基于重构的xLSTMAD-R。研究还比较了MSE和SoftDTW两种损失函数。

Result: 在包含17个真实世界数据集的TSB-AD-M基准测试中，xLSTMAD展现出最先进的准确性，超越了23个流行的异常检测基线。

Conclusion: 本文首次揭示了xLSTM在异常检测中的强大建模能力，为该领域未来的发展奠定了基础。

Abstract: The recently proposed xLSTM is a powerful model that leverages expressive
multiplicative gating and residual connections, providing the temporal capacity
needed for long-horizon forecasting and representation learning. This
architecture has demonstrated success in time series forecasting, lossless
compression, and even large-scale language modeling tasks, where its linear
memory footprint and fast inference make it a viable alternative to
Transformers. Despite its growing popularity, no prior work has explored xLSTM
for anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the
first anomaly detection method that integrates a full encoder-decoder xLSTM
architecture, purpose-built for multivariate time series data. Our encoder
processes input sequences to capture historical context, while the decoder is
devised in two separate variants of the method. In the forecasting approach,
the decoder iteratively generates forecasted future values xLSTMAD-F, while the
reconstruction approach reconstructs the input time series from its encoded
counterpart xLSTMAD-R. We investigate the performance of two loss functions:
Mean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider
local reconstruction fidelity and global sequence alignment, respectively. We
evaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17
real-world datasets, using state-of-the-art challenging metrics such as VUS-PR.
In our results, xLSTM showcases state-of-the-art accuracy, outperforming 23
popular anomaly detection baselines. Our paper is the first work revealing the
powerful modeling capabilities of xLSTM for anomaly detection, paving the way
for exciting new developments on this subject. Our code is available at:
https://github.com/Nyderx/xlstmad

</details>


### [188] [Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models](https://arxiv.org/abs/2506.22845)
*Batuhan Hangun,Oguz Altun,Onder Eyecioglu*

Main category: cs.LG

TL;DR: 本研究深入探讨了量子神经网络（QNNs）在预测风力涡轮机功率输出方面的应用，实验证明QNNs的预测性能可与经典机器学习方法媲美，甚至在某些情况下略优。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源系统的整合推动智能电网的普及，机器学习在预测电力需求和检测系统扰动中扮演着重要角色。量子神经网络（QNNs）作为量子机器学习（QML）的突出方法，正逐渐成为经典机器学习方法的有力替代品。本研究旨在探索QNNs在风力涡轮机功率输出预测任务中的适用性。

Method: 研究评估了六种基于Z特征映射进行数据编码和不同ansatz结构的QNN配置的预测性能和模拟时间。通过详细的交叉验证实验和在未见过的保留数据集上的测试，将QNNs的性能与基准的经典方法进行比较。

Result: 实验结果表明，QNNs可以实现与基准经典方法相当的预测性能，在某些情况下甚至略优。研究还揭示了数据集大小和电路复杂性对预测性能和模拟时间的影响。

Conclusion: 本研究结果为希望将量子机器学习应用于能源领域的研究人员提供了有价值的见解。

Abstract: Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine
Learning (QML), are emerging as a powerful alternative to classical machine
learning methods. Recent studies have focused on the applicability of QNNs to
various tasks, such as time-series forecasting, prediction, and classification,
across a wide range of applications, including cybersecurity and medical
imaging. With the increased use of smart grids driven by the integration of
renewable energy systems, machine learning plays an important role in
predicting power demand and detecting system disturbances. This study provides
an in-depth investigation of QNNs for predicting the power output of a wind
turbine. We assess the predictive performance and simulation time of six QNN
configurations that are based on the Z Feature Map for data encoding and
varying ansatz structures. Through detailed cross-validation experiments and
tests on an unseen hold-out dataset, we experimentally demonstrate that QNNs
can achieve predictive performance that is competitive with, and in some cases
marginally better than, the benchmarked classical approaches. Our results also
reveal the effects of dataset size and circuit complexity on predictive
performance and simulation time. We believe our findings will offer valuable
insights for researchers in the energy domain who wish to incorporate quantum
machine learning into their work.

</details>


### [189] [Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles](https://arxiv.org/abs/2506.22848)
*Shengcai Liu,Hui Ou-yang,Zhiyuan Wang,Cheng Chen,Qijun Cai,Yew-Soon Ong,Ke Tang*

Main category: cs.LG

TL;DR: 本文提出并实现了Auto-SLE方法，通过自动学习结构学习集成（SLE）并将其整合到分治策略中，显著提升了大规模贝叶斯网络结构学习的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 从数据中学习贝叶斯网络（BNs）的结构，特别是变量数量庞大的数据集，极具挑战性。现有分治（D&D）策略在子问题上的学习精度不稳定，且手动设计高质量的结构学习集成（SLE）面临困难。

Method: 本文引入了结构学习集成（SLE）的思想，结合多种BN结构学习算法以持续实现高精度。为解决手动设计SLE的挑战，提出了一种名为Auto-SLE的自动化方法，用于学习接近最优的SLE。最终，将学习到的SLE集成到分治（D&D）方法中。

Result: 广泛实验表明，该方法在学习大规模BNs时，优于使用单一BN结构学习算法的分治方法，在包含10,000个变量的数据集上精度通常提高30%~225%。此外，该方法对包含更多（例如30,000个）变量且网络特征不同的数据集也表现出良好的泛化能力。

Conclusion: 这些结果表明，采用（自动学习的）结构学习集成（SLEs）在大规模贝叶斯网络结构学习方面具有显著潜力。

Abstract: Learning the structure of Bayesian networks (BNs) from data is challenging,
especially for datasets involving a large number of variables. The recently
proposed divide-and-conquer (D\&D) strategies present a promising approach for
learning large BNs. However, they still face a main issue of unstable learning
accuracy across subproblems. In this work, we introduce the idea of employing
structure learning ensemble (SLE), which combines multiple BN structure
learning algorithms, to consistently achieve high learning accuracy. We further
propose an automatic approach called Auto-SLE for learning near-optimal SLEs,
addressing the challenge of manually designing high-quality SLEs. The learned
SLE is then integrated into a D\&D method. Extensive experiments firmly show
the superiority of our method over D\&D methods with single BN structure
learning algorithm in learning large BNs, achieving accuracy improvement
usually by 30\%$\sim$225\% on datasets involving 10,000 variables. Furthermore,
our method generalizes well to datasets with many more (e.g., 30000) variables
and different network characteristics than those present in the training data
for learning the SLE. These results indicate the significant potential of
employing (automatic learning of) SLEs for scalable BN structure learning.

</details>


### [190] [P$^2$U: Progressive Precision Update For Efficient Model Distribution](https://arxiv.org/abs/2506.22871)
*Homayun Afrabandpey,Hamed Rezazadegan Tavakoli*

Main category: cs.LG

TL;DR: P$^2$U是一种渐进式精度更新方法，通过传输低精度模型和差值更新，在带宽受限环境中实现高效模型分发，并在准确性、带宽和延迟之间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 在带宽受限环境下，高效的模型分发变得日益重要。

Method: 提出渐进式精度更新（P$^2$U）方法。该方法不直接传输原始高精度模型，而是传输一个低比特精度模型，并附带一个表示原始高精度模型与传输的低精度版本之间差异的模型更新。

Result: 在多种模型架构（1M至100M+参数）和三种数据集（胸部X光、PASCAL-VOC、CIFAR-100）上进行广泛实验后，P$^2$U在准确性、带宽使用和延迟之间持续取得更优的权衡。研究表明，在带宽或启动时间优先时，可使用激进量化（如4比特）而不严重损害性能。

Conclusion: P$^2$U是低资源环境（如联邦学习、边缘计算、IoT）中可扩展且高效模型分发的有效实用方案。P$^2$U可与现有压缩技术（如稀疏化、量化、剪枝）结合使用，具有更大的改进潜力。

Abstract: Efficient model distribution is becoming increasingly critical in
bandwidth-constrained environments. In this paper, we propose a simple yet
effective approach called Progressive Precision Update (P$^2$U) to address this
problem. Instead of transmitting the original high-precision model, P$^2$U
transmits a lower-bit precision model, coupled with a model update representing
the difference between the original high-precision model and the transmitted
low precision version. With extensive experiments on various model
architectures, ranging from small models ($1 - 6$ million parameters) to a
large model (more than $100$ million parameters) and using three different data
sets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U
consistently achieves better tradeoff between accuracy, bandwidth usage and
latency. Moreover, we show that when bandwidth or startup time is the priority,
aggressive quantization (e.g., 4-bit) can be used without severely compromising
performance. These results establish P$^2$U as an effective and practical
solution for scalable and efficient model distribution in low-resource
settings, including federated learning, edge computing, and IoT deployments.
Given that P$^2$U complements existing compression techniques and can be
implemented alongside any compression method, e.g., sparsification,
quantization, pruning, etc., the potential for improvement is even greater.

</details>


### [191] [Interpretable Time Series Autoregression for Periodicity Quantification](https://arxiv.org/abs/2506.22895)
*Xinyu Chen,Vassilis Digalakis Jr,Lijun Ding,Dingyi Zhuang,Jinhua Zhao*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Time series autoregression is a classical statistical model for capturing
auto-correlations and identifying temporal patterns such as periodicity and
seasonality. In this work, we propose a novel sparse autoregression framework
from an interpretable machine learning perspective and the model
interpretability for periodicity quantification is reinforced by $\ell_0$-norm
induced sparsity constraints. On the time-varying time series data, we
reformulate the sparse autoregression and convert the involved optimization
problem into a mixed-integer optimization (MIO). To accelerate it, we develop a
subspace pursuit based decision variable pruning (DVP) strategy to reduce the
search space. On the multidimensional time series that involves complicated
spatial and temporal dimensions, we propose a spatially- and time-varying
sparse autoregression model and resolve the corresponding MIO problem by
developing a two-stage optimization scheme. In particular, the proposed scheme
makes the model scalable to large problems even with millions of decision
variables. Empirically, we conduct extensive experiments to evaluate the
proposed models on real-world time series data. First, we demonstrate that the
MIO solver can be drastically accelerated through the DVP strategy, while
maintaining the same solution quality as a full MIO solver. Applying the
time-varying sparse autoregression model to ridesharing trip data, we uncover
both daily and weekly periodicities and reveal long-term changes in regularity
of human mobility. Second, we demonstrate the spatial patterns of yearly
seasonality in climate variable time series such as temperature and
precipitation across the past four decades, and our model allows to discover
dynamic climate patterns and identify climate phenomena such as El Nino in sea
surface temperature.

</details>


### [192] [Missing-Modality-Aware Graph Neural Network for Cancer Classification](https://arxiv.org/abs/2506.22901)
*Sina Tabakhi,Haiping Lu*

Main category: cs.LG

TL;DR: 提出MAGNET，一种用于处理多模态生物数据中模态缺失的图神经网络方法，通过注意力机制和患者图构建，在癌症分类任务中性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有处理多模态生物数据中模态缺失的方法（如排除、插补或直接预测）难以有效应对多样化且随模态数量指数增长的缺失模式。

Method: 提出MAGNET (Missing-modality-Aware Graph neural NETwork)，它引入患者-模态多头注意力机制融合模态嵌入，其复杂度随模态数量线性增长并适应缺失模式。之后，构建一个以融合多模态嵌入为节点特征、以模态缺失性确定连接的患者图，再通过传统图神经网络进行预测。

Result: 在三个使用真实缺失数据的公共多组学癌症分类数据集上，MAGNET的性能优于当前最先进的融合方法。

Conclusion: MAGNET为直接利用部分多模态数据进行预测提供了一种有效且性能卓越的解决方案，能够鲁棒地处理多样的模态缺失模式。

Abstract: A key challenge in learning from multimodal biological data is missing
modalities, where all data from some modalities are missing for some patients.
Current fusion methods address this by excluding patients with missing
modalities, imputing missing modalities, or making predictions directly with
partial modalities. However, they often struggle with diverse missing-modality
patterns and the exponential growth of the number of such patterns as the
number of modalities increases. To address these limitations, we propose MAGNET
(Missing-modality-Aware Graph neural NETwork) for direct prediction with
partial modalities, which introduces a patient-modality multi-head attention
mechanism to fuse lower-dimensional modality embeddings based on their
importance and missingness. MAGNET's complexity increases linearly with the
number of modalities while adapting to missing-pattern variability. To generate
predictions, MAGNET further constructs a patient graph with fused multimodal
embeddings as node features and the connectivity determined by the modality
missingness, followed by a conventional graph neural network. Experiments on
three public multiomics datasets for cancer classification, with real-world
instead of artificial missingness, show that MAGNET outperforms the
state-of-the-art fusion methods. The data and code are available at
https://github.com/SinaTabakhi/MAGNET.

</details>


### [193] [Towards Time Series Generation Conditioned on Unstructured Natural Language](https://arxiv.org/abs/2506.22927)
*Jaeyun Woo,Jiseok Lee,Brian Kenji Iwana*

Main category: cs.LG

TL;DR: 本研究提出了一种结合扩散模型和语言模型的新方法，旨在根据自然语言描述生成时间序列，并构建了一个大型时间序列-描述配对数据集，以填补时间序列生成式AI的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在图像和文本等领域取得了显著进展，但时间序列生成式AI仍不成熟，而时间序列在金融、气候等多个领域具有重要应用价值。

Method: 研究提出了一种新颖的方法，通过将扩散模型与语言模型结合，实现根据非结构化自然语言描述生成时间序列。此外，研究还构建并发布了一个包含63,010对时间序列-描述的新公共数据集。

Result: 研究证明了基于自然语言生成时间序列是可行的。所提出的方法能够支持自定义预测、时间序列操作、数据增强和迁移学习等多种应用。同时，成功构建了一个大规模的公共数据集。

Conclusion: 本研究成功展示了从自然语言描述生成时间序列的可能性，为时间序列生成领域开辟了新途径，并提供了可支持未来研究的宝贵数据集，有望推动时间序列在多个领域的应用创新。

Abstract: Generative Artificial Intelligence (AI) has rapidly become a powerful tool,
capable of generating various types of data, such as images and text. However,
despite the significant advancement of generative AI, time series generative AI
remains underdeveloped, even though the application of time series is essential
in finance, climate, and numerous fields. In this research, we propose a novel
method of generating time series conditioned on unstructured natural language
descriptions. We use a diffusion model combined with a language model to
generate time series from the text. Through the proposed method, we demonstrate
that time series generation based on natural language is possible. The proposed
method can provide various applications such as custom forecasting, time series
manipulation, data augmentation, and transfer learning. Furthermore, we
construct and propose a new public dataset for time series generation,
consisting of 63,010 time series-description pairs.

</details>


### [194] [Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration](https://arxiv.org/abs/2506.22929)
*Chen Zhang*

Main category: cs.LG

TL;DR: 提出一种基于空间完备性的并行计算架构，以克服深度学习处理高维数据的挑战，并支持统一系统中的科学计算。


<details>
  <summary>Details</summary>
Motivation: 深度学习在处理高维数据时面临维度灾难的计算挑战；现有大规模数据工具缺乏高级分析所需的数学统计支持。

Method: 提出一种基于空间完备性的并行计算架构，将高维数据分解为维度无关的结构进行分布式处理。

Result: 该框架能够无缝集成数据挖掘和并行优化的机器学习方法。

Conclusion: 所提出的统一系统支持对医学图像和自然图像等多种高维数据进行科学计算。

Abstract: While deep learning excels in natural image and language processing, its
application to high-dimensional data faces computational challenges due to the
dimensionality curse. Current large-scale data tools focus on business-oriented
descriptive statistics, lacking mathematical statistics support for advanced
analysis. We propose a parallel computation architecture based on space
completeness, decomposing high-dimensional data into dimension-independent
structures for distributed processing. This framework enables seamless
integration of data mining and parallel-optimized machine learning methods,
supporting scientific computations across diverse data types like medical and
natural images within a unified system.

</details>


### [195] [Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models](https://arxiv.org/abs/2506.22950)
*Liangyu Wang,Huanyi Xie,Xinhai Wang,Tianjin Huang,Mengdi Li,Di Wang*

Main category: cs.LG

TL;DR: 为解决基于组的强化学习（如GRPO）在LLM微调中面临的内存开销和可扩展性问题，本文提出Infinite Sampling框架。该框架通过微采样组、连续采样和长度感知调度，有效降低内存占用并显著提升吞吐量，实现在有限硬件资源下高效稳定的GRPO训练。


<details>
  <summary>Details</summary>
Motivation: 基于组的强化学习算法（如GRPO）在微调大型语言模型时，为每个提示生成和存储多个响应会产生大量内存开销，尤其当采样组规模增大时，这在硬件受限的情况下严重限制了其可扩展性。

Method: 本文提出了Infinite Sampling框架，旨在通过将组大小与GPU内存使用解耦，实现高效稳定的GRPO训练。该框架包含三个核心组成部分：
1.  微采样组：将大型组分解为内存可行的轮次进行处理。
2.  连续采样：在不同组之间交错生成过程，以提高GPU利用率。
3.  长度感知调度器：结合基于Token的序列长度预测，并采用两阶段计划（通过FPTAS进行全局分组和通过SJF进行运行时填充）。

Result: 实验结果显示：
1.  微采样组相较于全组解码，可将峰值内存使用量减少50%以上（例如，在Qwen3-1.7B模型上从21.55 GB降至10.64 GB）。
2.  Infinite Sampling框架相较于朴素微采样组方法，吞吐量提高了25%以上，同时减少了解码步骤，并保持了完整长度的生成结果和较低的内存使用。

Conclusion: 所提出的混合调度方法确保了在实际GPU内存限制下，能够使用更大的组进行高效且稳定的GRPO训练。

Abstract: Group-based reinforcement learning algorithms such as Group Reward Policy
Optimization (GRPO) have proven effective for fine-tuning large language models
(LLMs) with human feedback. However, generating and storing multiple responses
per prompt incurs substantial memory overhead, especially as the sample group
size increases, limiting scalability under constrained hardware.
  We propose Infinite Sampling, a framework that enables efficient and stable
GRPO training by decoupling group size from GPU memory usage. It consists of:
(1) micro sampling groups that decompose large groups into memory-feasible
rounds; (2) continuous sampling that interleaves generation across groups to
improve utilization; and (3) a length-aware scheduler combining
token-conditioned sequence length prediction with a two-stage plan: global
grouping via FPTAS and runtime refill via SJF.
  Experiments show that our Micro Sampling Groups reduce peak memory usage by
over 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on
Qwen3-1.7B). Building on this, Infinite Sampling improves throughput by over
25% compared to the naive micro sampling group method, reducing decoding steps
while maintaining full-length completions and memory usage. Our hybrid
scheduling ensures efficient and stable GRPO training with larger groups under
realistic GPU memory constraints.

</details>


### [196] [Cybersecurity-Focused Anomaly Detection in Connected Autonomous Vehicles Using Machine Learning](https://arxiv.org/abs/2506.22984)
*Prathyush Kumar Reddy Lebaku,Lu Gao,Yunpeng Zhang,Zhixia Li,Yongxin Liu,Tanvir Arafin*

Main category: cs.LG

TL;DR: 本研究通过模拟车辆行为生成数据集，并利用堆叠LSTM和随机森林模型，有效实现了互联自动驾驶车辆（CAVs）的异常检测和轨迹预测，确保交通安全。


<details>
  <summary>Details</summary>
Motivation: 互联自动驾驶车辆（CAVs）易受传感器故障、网络攻击和环境干扰影响，因此其异常检测对维护交通网络的安全和可靠性至关重要。

Method: 研究通过模拟车辆行为生成包含正常和异常交互的时间序列数据集（位置、速度、加速度）。采用堆叠长短期记忆网络（LSTM）模型捕捉时间依赖性，并部署随机森林（Random Forest）模型进行集成预测，以识别异常驾驶模式。

Result: 随机森林模型表现为R2=0.9830，MAE=5.746，95%异常阈值=14.18。堆叠LSTM模型表现为R2=0.9998，MAE=82.425，95%异常阈值=265.63。结果表明模型在预测车辆轨迹和检测自动驾驶异常方面均有效。

Conclusion: 该研究证明了所使用的机器学习模型（堆叠LSTM和随机森林）能有效且准确地预测车辆轨迹并检测自动驾驶场景中的异常，为互联自动驾驶车辆的安全运行提供支持。

Abstract: Anomaly detection in connected autonomous vehicles (CAVs) is crucial for
maintaining safe and reliable transportation networks, as CAVs can be
susceptible to sensor malfunctions, cyber-attacks, and unexpected environmental
disruptions. This study explores an anomaly detection approach by simulating
vehicle behavior, generating a dataset that represents typical and atypical
vehicular interactions. The dataset includes time-series data of position,
speed, and acceleration for multiple connected autonomous vehicles. We utilized
machine learning models to effectively identify abnormal driving patterns.
First, we applied a stacked Long Short-Term Memory (LSTM) model to capture
temporal dependencies and sequence-based anomalies. The stacked LSTM model
processed the sequential data to learn standard driving behaviors.
Additionally, we deployed a Random Forest model to support anomaly detection by
offering ensemble-based predictions, which enhanced model interpretability and
performance. The Random Forest model achieved an R2 of 0.9830, MAE of 5.746,
and a 95th percentile anomaly threshold of 14.18, while the stacked LSTM model
attained an R2 of 0.9998, MAE of 82.425, and a 95th percentile anomaly
threshold of 265.63. These results demonstrate the models' effectiveness in
accurately predicting vehicle trajectories and detecting anomalies in
autonomous driving scenarios.

</details>


### [197] [Kernel Outlier Detection](https://arxiv.org/abs/2506.22994)
*Can Hakan Dağıdır,Mia Hubert,Peter J. Rousseeuw*

Main category: cs.LG

TL;DR: 提出了一种名为KOD的新型核异常检测方法，旨在解决高维数据中的异常检测挑战，并克服现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决高维设置中异常检测的挑战；克服现有方法对分布假设或难以调整的超参数的依赖。

Method: KOD方法首先进行核变换，然后采用投影寻踪方法。其创新点包括新的搜索方向集合以及组合不同方向类型结果的新方式。该方法灵活且轻量。

Result: 经验评估表明KOD在三个具有挑战性结构的小数据集和四个大型基准数据集上均有效。

Conclusion: KOD是一种有效、灵活且轻量的高维异常检测方法，能够克服现有方法的局限性，并通过实证评估验证了其有效性。

Abstract: A new anomaly detection method called kernel outlier detection (KOD) is
proposed. It is designed to address challenges of outlier detection in
high-dimensional settings. The aim is to overcome limitations of existing
methods, such as dependence on distributional assumptions or on hyperparameters
that are hard to tune. KOD starts with a kernel transformation, followed by a
projection pursuit approach. Its novelties include a new ensemble of directions
to search over, and a new way to combine results of different direction types.
This provides a flexible and lightweight approach for outlier detection. Our
empirical evaluations illustrate the effectiveness of KOD on three small
datasets with challenging structures, and on four large benchmark datasets.

</details>


### [198] [A Reinforcement Learning Approach for Optimal Control in Microgrids](https://arxiv.org/abs/2506.22995)
*Davide Salaorni,Federico Bianchi,Francesco Trovò,Marcello Restelli*

Main category: cs.LG

TL;DR: 本文提出一种新颖的强化学习（RL）方法，结合数字孪生（DT）技术，用于优化微电网的能源管理，并在真实数据下表现优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源的日益整合，传统电网需要新的方法来管理分散式能源的生产和消费。微电网作为一种有前景的解决方案，其能源管理优化是一个重要挑战。

Method: 研究提出一种基于强化学习的智能体，通过利用历史能源生产、消费和市场价格数据，学习最优的能源交易和存储策略。同时，采用数字孪生技术模拟储能系统动态，并考虑其退化因素，以确保仿真的真实性。

Result: 通过使用意大利电网的真实数据进行的实验表明，所提出的RL策略优于基于规则的方法和现有的RL基准。

Conclusion: 该RL方法为智能微电网管理提供了一个鲁棒的解决方案。

Abstract: The increasing integration of renewable energy sources (RESs) is transforming
traditional power grid networks, which require new approaches for managing
decentralized energy production and consumption. Microgrids (MGs) provide a
promising solution by enabling localized control over energy generation,
storage, and distribution. This paper presents a novel reinforcement learning
(RL)-based methodology for optimizing microgrid energy management.
Specifically, we propose an RL agent that learns optimal energy trading and
storage policies by leveraging historical data on energy production,
consumption, and market prices. A digital twin (DT) is used to simulate the
energy storage system dynamics, incorporating degradation factors to ensure a
realistic emulation of the analysed setting. Our approach is validated through
an experimental campaign using real-world data from a power grid located in the
Italian territory. The results indicate that the proposed RL-based strategy
outperforms rule-based methods and existing RL benchmarks, offering a robust
solution for intelligent microgrid management.

</details>


### [199] [BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs](https://arxiv.org/abs/2506.23024)
*Jerry Liu,Yasa Baig,Denise Hui Jean Lee,Rajat Vadiraj Dwaraknath,Atri Rudra,Chris Ré*

Main category: cs.LG

TL;DR: 鉴于物理信息神经网络（PINNs）在求解偏微分方程（PDEs）时精度不足，本文提出重心权值层（BWLer）来解决多层感知机（MLP）固有的精度瓶颈。BWLer通过多项式插值显著提高了PDEs的求解精度，甚至达到接近机器精度，为结合PINNs的灵活性与经典谱求解器的精度提供了实用路径。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息神经网络（PINNs）虽然灵活，但在求解偏微分方程（PDEs）时，精度远未达到科学任务所需的机器精度。本研究旨在探究这种精度瓶颈是源于PDE的病态性还是多层感知机（MLP）架构的限制。

Method: 引入了重心权值层（BWLer），通过重心多项式插值来建模PDE解。BWLer可作为现有MLP的顶层（BWLer-hat）或完全取代MLP（显式BWLer），从而解耦解的表示与PDE损失的导数计算。对于线性PDEs，研究通过显式误差分解来表征可实现精度与PDE损失条件之间的权衡，并通过谱导数和预处理在训练中进行管理。

Result: 研究发现MLP本身存在精度限制，即使在简单插值任务中也仅能达到1e-8 RMSE。添加BWLer能提升PDE学习的精度上限。BWLer-hat在五种基准PDE上将RMSE提高了30倍（对流）、10倍（反应）和1800倍（波动）。显式BWLer在对流、反应和波动问题上达到了接近机器精度（比现有结果好100亿倍），并在刚性Burgers和不规则几何泊松问题上与标准PINNs性能相当。

Conclusion: 这些发现指出了一条实用的途径，可以将物理信息神经网络（PINNs）的灵活性与经典谱求解器所需的机器级精度有效结合。

Abstract: Physics-informed neural networks (PINNs) offer a flexible way to solve
partial differential equations (PDEs) with machine learning, yet they still
fall well short of the machine-precision accuracy many scientific tasks demand.
In this work, we investigate whether the precision ceiling comes from the
ill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)
architecture. We introduce the Barycentric Weight Layer (BWLer), which models
the PDE solution through barycentric polynomial interpolation. A BWLer can be
added on top of an existing MLP (a BWLer-hat) or replace it completely
(explicit BWLer), cleanly separating how we represent the solution from how we
take derivatives for the PDE loss. Using BWLer, we identify fundamental
precision limitations within the MLP: on a simple 1-D interpolation task, even
MLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above
float64 machine precision -- before any PDE terms are added. In PDE learning,
adding a BWLer lifts this ceiling and exposes a tradeoff between achievable
accuracy and the conditioning of the PDE loss. For linear PDEs we fully
characterize this tradeoff with an explicit error decomposition and navigate it
during training with spectral derivatives and preconditioning. Across five
benchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for
convection, 10x for reaction, and 1800x for wave equations while remaining
compatible with first-order optimizers. Replacing the MLP entirely lets an
explicit BWLer reach near-machine-precision on convection, reaction, and wave
problems (up to 10 billion times better than prior results) and match the
performance of standard PINNs on stiff Burgers' and irregular-geometry Poisson
problems. Together, these findings point to a practical path for combining the
flexibility of PINNs with the precision of classical spectral solvers.

</details>


### [200] [Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models](https://arxiv.org/abs/2506.23025)
*Tejas Vaidhya,Ayush Kaushal,Vineet Jain,Francis Couture Harpin,Prashant Shishodia,Majid Behbahani,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.LG

TL;DR: 为解决大语言模型（LLMs）推理效率低下的问题，研究引入了三元语言模型（TriLMs），通过量化感知训练大幅减少内存需求。通过开发数据规模更大的模型（Spectra-1.1）和优化CPU/GPU推理核（TriRun），实现了显著的推理加速，为高效LLMs的部署奠定基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在研究和工业应用中日益普及，但其推理效率仍然是一个重大挑战。现代GPU架构的计算能力持续提升，但内存带宽和容量未能按比例扩展，在推理过程中造成了关键瓶颈。

Method: ['探索并研究三元语言模型（TriLMs），采用量化感知训练以显著降低内存需求。', '进行标度律分析，揭示TriLMs从增加训练数据中获益多于增加模型参数。', '基于此观察，推出Spectra-1.1，一套在高达1.2万亿个token上训练的开放TriLMs套件。', '提出新颖的2比特和1.6比特三元权重打包方案，以提高推理效率。', '开发名为TriRun的GPU核，该核基于2比特打包方案，用于加速端到端模型推理。']

Result: ['Spectra-1.1在扩大规模时展示了持续的性能提升。', '新提出的2比特和1.6比特打包方案在各种CPU架构上实现了加速推理。', 'TriRun GPU核将端到端模型推理速度相较于浮点基线提高了多达5倍。']

Conclusion: 本工作为构建和部署高效的大语言模型奠定了基础，通过提供Spectra-1.1套件和TriRun推理核，为研究社区提供了宝贵的资源，以鼓励对TriLMs的进一步探索和开发。

Abstract: Large language models (LLMs) are increasingly used across research and
industry applications, yet their inference efficiency remains a significant
challenge. As the computational power of modern GPU architectures continuously
improves, their memory bandwidth and capacity have not scaled proportionally,
creating a critical bottleneck during inference. To address this, we
investigate ternary language models (TriLMs) that employ quantization-aware
training to significantly reduce memory requirements. We first analyze the
scalability of TriLMs by conducting a scaling law analysis, revealing that
TriLMs benefit more from increasing training data than from scaling model
parameters. Based on this observation, we introduce Spectra-1.1, an open suite
of TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained
performance gains at scale. Furthermore, to improve inference efficiency, we
propose novel 2-bit and 1.6-bit packing schemes for ternary weights, which
demonstrate accelerated inference across various CPU architectures. Also,
building on the 2-bit packing, we develop a GPU kernel called TriRun that
accelerates end-to-end model inference by up to 5 times compared to
floating-point baselines. To encourage further exploration and development of
TriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.
Overall, our work lays the foundation for building and deploying efficient
LLMs, providing a valuable resource for the research community.

</details>


### [201] [Feature-Wise Mixing for Mitigating Contextual Bias in Predictive Supervised Learning](https://arxiv.org/abs/2506.23033)
*Yash Vardhan Tomar*

Main category: cs.LG

TL;DR: 本文提出了一种特征级混合框架，通过重新分配特征表示来减轻机器学习模型中的上下文偏见，该方法在降低偏见和提高预测性能方面表现出色，且具有计算效率高和无需显式识别偏见属性的优点。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型中的偏见是导致不公平结果的根本挑战。现有偏见缓解策略（如事后校正或严格约束）在可扩展性和泛化能力方面存在局限性，促使研究人员寻求更有效的解决方案。

Method: 引入一个特征级混合（feature-wise mixing）框架，通过在多个上下文数据集中重新分配特征表示来减轻上下文偏见。为评估其有效性，使用交叉验证训练了四种ML分类器，并采用偏见敏感损失函数（包括差异度量和均方误差MSE）进行评估。同时，与SMOTE过采样等现有偏见缓解技术进行了基准测试。

Result: 所提出的方法平均偏见减少了43.35%，并且在所有混合数据集上训练的分类器的MSE（预测性能标准衡量）都呈现统计学上的显著下降。该方法持续优于SMOTE过采样，并在无需明确识别偏见属性的情况下展现出有竞争力的有效性，同时有效避免了公平感知学习算法通常相关的计算开销。

Conclusion: 特征级混合框架是一种有效且计算高效的机器学习模型偏见缓解策略，它不仅能显著降低偏见，还能提升预测性能，且无需识别敏感偏见属性。未来工作可探索将其应用于需要精确预测的真实世界领域。

Abstract: Bias in predictive machine learning (ML) models is a fundamental challenge
due to the skewed or unfair outcomes produced by biased models. Existing
mitigation strategies rely on either post-hoc corrections or rigid constraints.
However, emerging research claims that these techniques can limit scalability
and reduce generalizability. To address this, this paper introduces a
feature-wise mixing framework to mitigate contextual bias. This was done by
redistributing feature representations across multiple contextual datasets. To
assess feature-wise mixing's effectiveness, four ML classifiers were trained
using cross-validation and evaluated with bias-sensitive loss functions,
including disparity metrics and mean squared error (MSE), which served as a
standard measure of predictive performance. The proposed method achieved an
average bias reduction of 43.35% and a statistically significant decrease in
MSE across all classifiers trained on mixed datasets. Additionally,
benchmarking against established bias mitigation techniques found that
feature-wise mixing consistently outperformed SMOTE oversampling and
demonstrated competitive effectiveness without requiring explicit bias
attribute identification. Feature-wise mixing efficiently avoids the
computational overhead typically associated with fairness-aware learning
algorithms. Future work could explore applying feature-wise mixing for
real-world fields where accurate predictions are necessary.

</details>


### [202] [Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress](https://arxiv.org/abs/2506.23036)
*Zain ul Abdeen,Ming Jin*

Main category: cs.LG

TL;DR: 该论文通过内部（突触过滤）和外部（对抗性攻击）压力分析RL策略的鲁棒性，将网络参数分类为脆弱、鲁棒或反脆弱，并发现反脆弱参数能在压力下提高性能，为设计更具适应性的RL系统提供基础。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在系统分析强化学习（RL）策略的鲁棒性，探究网络参数在内部和外部压力下的表现，并受神经科学中突触可塑性的启发，对参数进行分类，以期为未来设计更健壮和反脆弱的RL系统提供依据。

Method: 研究采用双重方法：通过突触过滤（选择性扰动参数）引入内部压力，并通过对抗性攻击（修改智能体观测）施加外部压力。根据参数在干净和对抗性设置下对策略性能的影响，将参数分类为脆弱、鲁棒或反脆弱。定义参数分数来量化这些特性，并在Mujoco连续控制环境中使用PPO训练的智能体进行验证。

Result: 研究结果突出了反脆弱参数的存在，这些参数在压力下能够增强策略性能。这表明有针对性的过滤技术有潜力提高RL策略的适应性。

Conclusion: 本研究提供的见解为未来设计更健壮和反脆弱的RL系统奠定了基础。识别并利用反脆弱参数的特性，有望显著提升RL策略在复杂和不确定环境中的表现。

Abstract: This paper explores Reinforcement learning (RL) policy robustness by
systematically analyzing network parameters under internal and external
stresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering
introduces internal stress by selectively perturbing parameters, while
adversarial attacks apply external stress through modified agent observations.
This dual approach enables the classification of parameters as fragile, robust,
or antifragile, based on their influence on policy performance in clean and
adversarial settings. Parameter scores are defined to quantify these
characteristics, and the framework is validated on PPO-trained agents in Mujoco
continuous control environments. The results highlight the presence of
antifragile parameters that enhance policy performance under stress,
demonstrating the potential of targeted filtering techniques to improve RL
policy adaptability. These insights provide a foundation for future
advancements in the design of robust and antifragile RL systems.

</details>


### [203] [ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation](https://arxiv.org/abs/2506.23041)
*Chengyu Dong,Huan Gui,Noveen Sachdeva,Long Jin,Ke Yin,Jingbo Shang,Lichan Hong,Ed H. Chi,Zhe Zhao*

Main category: cs.LG

TL;DR: 本文针对从大型预训练ViT模型进行知识蒸馏效率低下的问题，提出通过互信息感知优化微调ViT，并在数据集受限时采用MLP块重加权策略，以提升小型学生模型的受益程度。


<details>
  <summary>Details</summary>
Motivation: 当从大规模预训练的强大视觉表示模型（特别是Vision Transformers, ViTs）进行知识蒸馏时，知识迁移的有效性显著下降，这限制了小型、特定任务生产模型的性能提升。

Method: 本文探索了微调预训练ViTs以实现更有效知识迁移的方法。具体地，提出了在微调过程中采用互信息感知的优化策略。对于小型或高度不平衡的下游数据集，引入了一种简单而有效的启发式方法：重新加权MLP块，该方法基于观察到顶部MLP块是导致互信息损失的主要原因。

Result: 所提出的方法使小型学生模型能够从最强大的预训练模型中受益，有效提升了知识蒸馏的效果。

Conclusion: 通过互信息感知优化和MLP块重加权策略，本文的方法成功解决了从强大预训练ViTs进行知识蒸馏的效率挑战，显著提升了小型学生模型的性能和从大型模型中获益的能力。

Abstract: Knowledge distillation from pretrained visual representation models offers an
effective approach to improve small, task-specific production models. However,
the effectiveness of such knowledge transfer drops significantly when
distilling from strong models that are pretrained in a large scale. In this
paper, we address this challenge for pretrained Vision Transformers (ViTs) by
exploring methods to fine-tune them for more effective knowledge transfer.
Motivated by the connection between mutual information and distillation
effectiveness, we propose to employ mutual information-aware optimization
during finetuning. For small or highly-imbalanced downstream datasets where
such optimization becomes less effective, we introduce a simple yet effective
heuristic of reweighting MLP blocks. This approach is inspired by our
observation that top MLP blocks are primarily responsible for mutual
information loss. Our method enables small student models to benefit from those
pretrained models among the strongest.

</details>


### [204] [Double-Diffusion: Diffusion Conditioned Diffusion Probabilistic Model For Air Quality Prediction](https://arxiv.org/abs/2506.23053)
*Hanlin Dong,Arian Prabowo,Hao Xue,Flora D. Salim*

Main category: cs.LG

TL;DR: 一种新型扩散概率模型Double-Diffusion，结合物理指导和随机性进行空气质量预测，在真实数据集上表现优异并显著缩短推理时间。


<details>
  <summary>Details</summary>
Motivation: 空气质量预测因其时空复杂性和固有的动态性及不确定性而极具挑战。现有模型难以在确定性（如物理原理）和不确定性（如随机性）之间找到最佳平衡点，这是一个悬而未决的问题。

Method: 本文提出Double-Diffusion，一种新颖的扩散概率模型。该模型利用已知物理原理指导空气质量预测，并融入随机性，是首次尝试将物理学作为条件生成方法应用于空气质量预测。模型还采用了源自图像修复的采样策略和新的去噪器架构。

Result: Double-Diffusion在两个真实数据集的大多数评估场景中均优于其他概率模型，排名第一。它将推理时间缩短了50%至30%，同时连续分级概率分数（CRPS）提高了3%至12%。

Conclusion: Double-Diffusion模型成功将物理学指导与随机性相结合，显著提高了空气质量预测的性能和效率，为平衡预测中的确定性与不确定性提供了有效途径。

Abstract: Air quality prediction is a challenging forecasting task due to its
spatio-temporal complexity and the inherent dynamics as well as uncertainty.
Most of the current models handle these two challenges by applying Graph Neural
Networks or known physics principles, and quantifying stochasticity through
probabilistic networks like Diffusion models. Nevertheless, finding the right
balancing point between the certainties and uncertainties remains an open
question. Therefore, we propose Double-Diffusion, a novel diffusion
probabilistic model that harnesses the power of known physics to guide air
quality forecasting with stochasticity. To the best of our knowledge, while
precedents have been made of using conditional diffusion models to predict air
pollution, this is the first attempt to use physics as a conditional generative
approach for air quality prediction. Along with a sampling strategy adopted
from image restoration and a new denoiser architecture, Double-Diffusion ranks
first in most evaluation scenarios across two real-life datasets compared with
other probabilistic models, it also cuts inference time by 50% to 30% while
enjoying an increase between 3-12% in Continuous Ranked Probabilistic Score
(CRPS).

</details>


### [205] [ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks](https://arxiv.org/abs/2402.09146)
*Muhammad Kashif,Muhammad Shafique*

Main category: cs.LG

TL;DR: 本文提出了一种增强型全卷积神经网络（QuNN）框架，通过引入可训练的全卷积层并利用残差学习构建残差全卷积神经网络（ResQuNN），有效解决了多层间梯度访问难题，显著提升了QuNN的训练性能。


<details>
  <summary>Details</summary>
Motivation: 传统全卷积层是静态的，适应性有限。虽然引入可训练层能增强QuNN的灵活性和潜力，但多层可训练层会造成梯度难以访问的复杂性，从而阻碍基于梯度的优化。

Method: 研究者引入了可训练的全卷积层，并提出了残差全卷积神经网络（ResQuNN）架构。该方法利用残差学习的概念，在全卷积层之间添加跳跃连接（skip connections）或残差块，以促进梯度在网络中的流动，从而解决梯度访问的难题。

Result: 通过残差块的引入，网络获得了增强的梯度访问能力，显著改善了训练性能。研究还通过大量实验确定了残差块的有效配置和最佳放置位置，发现其精确位置对最大化QuNN性能至关重要。

Conclusion: 该研究标志着量子深度学习发展迈出了实质性一步，为理论发展和实际量子计算应用提供了新途径。

Abstract: In this paper, we present a novel framework for enhancing the performance of
Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional
layers and addressing the critical challenges associated with them. Traditional
quanvolutional layers, although beneficial for feature extraction, have largely
been static, offering limited adaptability. Unlike state-of-the-art, our
research overcomes this limitation by enabling training within these layers,
significantly increasing the flexibility and potential of QuNNs. However, the
introduction of multiple trainable quanvolutional layers induces complexities
in gradient-based optimization, primarily due to the difficulty in accessing
gradients across these layers. To resolve this, we propose a novel
architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging
the concept of residual learning, which facilitates the flow of gradients by
adding skip connections between layers. By inserting residual blocks between
quanvolutional layers, we ensure enhanced gradient access throughout the
network, leading to improved training performance. Moreover, we provide
empirical evidence on the strategic placement of these residual blocks within
QuNNs. Through extensive experimentation, we identify an efficient
configuration of residual blocks, which enables gradients across all the layers
in the network that eventually results in efficient training. Our findings
suggest that the precise location of residual blocks plays a crucial role in
maximizing the performance gains in QuNNs. Our results mark a substantial step
forward in the evolution of quantum deep learning, offering new avenues for
both theoretical development and practical quantum computing applications.

</details>


### [206] [Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis](https://arxiv.org/abs/2506.23055)
*Hiro Taiyo Hamada,Ippei Fujisawa,Genji Kawakita,Yuki Yamada*

Main category: cs.LG

TL;DR: 本研究开发了一个定量框架，利用43份心理问卷评估大型语言模型（LLMs）与人类心理维度的概念对齐度。结果显示，GPT-4在分类准确性上显著优于其他模型，且其语义相似性与人类心理反应相关，表明现代LLMs能以可测量的准确性近似人类心理结构。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在生成类人文本方面表现出色，但目前尚不清楚这些模型内化塑造人类思维和行为的概念的准确性如何。

Method: 研究开发了一个定量框架来评估LLM与人类心理维度的概念对齐。该方法使用43份标准化心理问卷，通过成对相似性分析评估语言模型重构和分类问卷项目的准确性，并使用层次聚类将结果聚类结构与原始分类标签进行比较。

Result: GPT-4模型实现了66.2%的分类准确率，显著优于GPT-3.5（55.9%）和BERT（48.1%），所有模型均超过了31.9%的随机基线表现。此外，研究还发现GPT-4估计的语义相似性与多份心理问卷中人类反应的皮尔逊相关系数相关。

Conclusion: 该框架提供了一种评估人类-LLM概念对齐并识别潜在表征偏差的新方法。研究结果表明，现代LLMs能够以可测量的准确性近似人类心理结构，为开发更可解释的AI系统提供了见解。

Abstract: Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities
in producing human-like text. However, it is unclear how accurately these
models internalize concepts that shape human thought and behavior. Here, we
developed a quantitative framework to assess concept alignment between LLMs and
human psychological dimensions using 43 standardized psychological
questionnaires, selected for their established validity in measuring distinct
psychological constructs. Our method evaluates how accurately language models
reconstruct and classify questionnaire items through pairwise similarity
analysis. We compared resulting cluster structures with the original
categorical labels using hierarchical clustering. A GPT-4 model achieved
superior classification accuracy (66.2\%), significantly outperforming GPT-3.5
(55.9\%) and BERT (48.1\%), all exceeding random baseline performance (31.9\%).
We also demonstrated that the estimated semantic similarity from GPT-4 is
associated with Pearson's correlation coefficients of human responses in
multiple psychological questionnaires. This framework provides a novel approach
to evaluate the alignment of the human-LLM concept and identify potential
representational biases. Our findings demonstrate that modern LLMs can
approximate human psychological constructs with measurable accuracy, offering
insights for developing more interpretable AI systems.

</details>


### [207] [Curious Causality-Seeking Agents Learn Meta Causal World](https://arxiv.org/abs/2506.23068)
*Zhiyu Zhao,Haoxuan Li,Haifeng Zhang,Jun Wang,Francesco Faccio,Jürgen Schmidhuber,Mengyue Yang*

Main category: cs.LG

TL;DR: 针对世界模型中观测因果机制漂移的挑战，本文提出元因果图和因果探索代理，通过识别潜在元状态和好奇心驱动探索，有效捕捉和泛化动态变化的因果关系。


<details>
  <summary>Details</summary>
Motivation: 传统世界模型常假设环境因果规则单一不变，但实际中观察到的因果机制漂移常源于固定底层机制在狭窄观测窗口下的表现。这导致在构建世界模型时，策略或环境状态的微小变化都可能改变观测到的因果机制，带来建模难题。

Method: 提出**元因果图 (Meta-Causal Graph)** 作为世界模型，它是一个最小统一的表示，编码了因果结构在不同潜在世界状态间转换的规则，由多个由元状态触发的因果子图构成。同时引入**因果探索代理 (Causality-Seeking Agent)**，其目标是：1) 识别触发子图的元状态；2) 通过好奇心驱动的干预策略发现因果关系；3) 通过持续探索和经验迭代细化元因果图。

Result: 在合成任务和机器人机械臂操作任务上的实验表明，所提方法能稳健捕捉因果动态变化，并有效泛化到未见过的上下文。

Conclusion: 本研究成功解决了在动态环境中构建鲁棒世界模型的挑战，通过元因果图和因果探索代理，实现了对复杂、变化因果机制的有效识别、建模和泛化，为未来智能体在复杂现实世界中的学习和适应提供了新范式。

Abstract: When building a world model, a common assumption is that the environment has
a single, unchanging underlying causal rule, like applying Newton's laws to
every situation. In reality, what appears as a drifting causal mechanism is
often the manifestation of a fixed underlying mechanism seen through a narrow
observational window. This brings about a problem that, when building a world
model, even subtle shifts in policy or environment states can alter the very
observed causal mechanisms. In this work, we introduce the \textbf{Meta-Causal
Graph} as world models, a minimal unified representation that efficiently
encodes the transformation rules governing how causal structures shift across
different latent world states. A single Meta-Causal Graph is composed of
multiple causal subgraphs, each triggered by meta state, which is in the latent
state space. Building on this representation, we introduce a
\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta
states that trigger each subgraph, (2) discover the corresponding causal
relationships by agent curiosity-driven intervention policy, and (3)
iteratively refine the Meta-Causal Graph through ongoing curiosity-driven
exploration and agent experiences. Experiments on both synthetic tasks and a
challenging robot arm manipulation task demonstrate that our method robustly
captures shifts in causal dynamics and generalizes effectively to previously
unseen contexts.

</details>


### [208] [Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings](https://arxiv.org/abs/2506.23145)
*Shahad Hardan,Darya Taratynova,Abdelmajid Essofi,Karthik Nandakumar,Mohammad Yaqub*

Main category: cs.LG

TL;DR: 提出Forget-MI，一种针对多模态医疗数据的机器学习遗忘方法，有效移除敏感数据，同时保持模型性能并显著降低成员推理攻击风险。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医疗领域广泛应用，但其依赖敏感患者数据，数据隐私保护至关重要。现有机器学习遗忘方法难以从已训练的多模态模型中有效移除患者数据，尤其是在医疗健康领域。

Method: 提出Forget-MI，一种新颖的多模态医疗数据机器学习遗忘方法。该方法通过建立损失函数和扰动技术，实现对被遗忘数据的单模态和联合表示的遗忘，同时保留剩余数据的知识，并维持与原始模型相当的性能。

Result: Forget-MI在遗忘数据集上的性能和成员推理攻击（MIA）测量方面优于现有方法，同时在测试数据集上保持了同等性能。具体而言，MIA降低了0.202，遗忘数据集上的AUC和F1分数分别降低了0.221和0.305，测试集性能与重新训练的模型相匹配。

Conclusion: Forget-MI有效解决了多模态医疗数据中的机器学习遗忘难题，显著增强了数据隐私保护，同时保持了模型实用性，为医疗AI的隐私安全提供了新的解决方案。

Abstract: Privacy preservation in AI is crucial, especially in healthcare, where models
rely on sensitive patient data. In the emerging field of machine unlearning,
existing methodologies struggle to remove patient data from trained multimodal
architectures, which are widely used in healthcare. We propose Forget-MI, a
novel machine unlearning method for multimodal medical data, by establishing
loss functions and perturbation techniques. Our approach unlearns unimodal and
joint representations of the data requested to be forgotten while preserving
knowledge from the remaining data and maintaining comparable performance to the
original model. We evaluate our results using performance on the forget
dataset, performance on the test dataset, and Membership Inference Attack
(MIA), which measures the attacker's ability to distinguish the forget dataset
from the training dataset. Our model outperforms the existing approaches that
aim to reduce MIA and the performance on the forget dataset while keeping an
equivalent performance on the test set. Specifically, our approach reduces MIA
by 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,
respectively. Additionally, our performance on the test set matches that of the
retrained model, while allowing forgetting. Code is available at
https://github.com/BioMedIA-MBZUAI/Forget-MI.git

</details>


### [209] [maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics](https://arxiv.org/abs/2506.23147)
*Jonathan Schuster,Fabian Transchel*

Main category: cs.LG

TL;DR: 本文介绍了一个名为`maneuverRecognition`的Python软件包，旨在简化车载远程信息处理中驾驶操作识别的数据预处理、模型构建和评估过程，以满足实际应用需求。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶操作识别对于提升保险个性化、提高道路安全、减少事故及成本、降低油耗和支持环保驾驶至关重要。尽管在数据收集和预测模型构建方面已有大量研究，但当前缺乏实用的Python软件包和函数，能够快速转换数据结构、构建和评估相关模型，因此存在实际需求。

Method: 开发了`maneuverRecognition` Python软件包，其中包含数据预处理、模型构建和评估所需的功能。该软件包还提供了一个即插即用且可修改的基于LSTM的网络结构。通过使用智能手机传感器记录的三个不同人员的真实驾驶数据，演示了该软件包的实现和应用。

Result: 开发了`maneuverRecognition` Python软件包，它为驾驶操作识别任务提供了从数据预处理到模型构建和评估的必要功能，并包含一个可定制的LSTM网络结构，已成功在真实驾驶数据上进行了演示。

Conclusion: `maneuverRecognition`软件包有效地填补了驾驶操作识别领域中实用工具的空白，提供了一个综合性的Python解决方案，有助于简化该任务的数据处理和模型开发流程。

Abstract: In the domain of vehicle telematics the automated recognition of driving
maneuvers is used to classify and evaluate driving behaviour. This not only
serves as a component to enhance the personalization of insurance policies, but
also to increase road safety, reduce accidents and the associated costs as well
as to reduce fuel consumption and support environmentally friendly driving. In
this context maneuver recognition technically requires a continuous application
of time series classification which poses special challenges to the transfer,
preprocessing and storage of telematic sensor data, the training of predictive
models, and the prediction itself. Although much research has been done in the
field of gathering relevant data or regarding the methods to build predictive
models for the task of maneuver recognition, there is a practical need for
python packages and functions that allow to quickly transform data into the
required structure as well as to build and evaluate such models. The
maneuverRecognition package was therefore developed to provide the necessary
functions for preprocessing, modelling and evaluation and also includes a ready
to use LSTM based network structure that can be modified. The implementation of
the package is demonstrated using real driving data of three different persons
recorded via smartphone sensors.

</details>


### [210] [Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes](https://arxiv.org/abs/2506.23165)
*David Bossens,Atsushi Nitanda*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Safety is an essential requirement for reinforcement learning systems. The
newly emerging framework of robust constrained Markov decision processes allows
learning policies that satisfy long-term constraints while providing guarantees
under epistemic uncertainty. This paper presents mirror descent policy
optimisation for robust constrained Markov decision processes (RCMDPs), making
use of policy gradient techniques to optimise both the policy (as a maximiser)
and the transition kernel (as an adversarial minimiser) on the Lagrangian
representing a constrained MDP. In the oracle-based RCMDP setting, we obtain an
$\mathcal{O}\left(\frac{1}{T}\right)$ convergence rate for the squared distance
as a Bregman divergence, and an $\mathcal{O}\left(e^{-T}\right)$ convergence
rate for entropy-regularised objectives. In the sample-based RCMDP setting, we
obtain an $\tilde{\mathcal{O}}\left(\frac{1}{T^{1/3}}\right)$ convergence rate.
Experiments confirm the benefits of mirror descent policy optimisation in
constrained and unconstrained optimisation, and significant improvements are
observed in robustness tests when compared to baseline policy optimisation
algorithms.

</details>


### [211] [Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data](https://arxiv.org/abs/2506.23174)
*Chen Gong,Bo Liang,Wei Gao,Chenren Xu*

Main category: cs.LG

TL;DR: 针对无线感知任务中生成模型合成数据质量不可预测问题，本文提出了衡量指标（亲和度、多样性）并引入了SynCheck质量引导方案，显著提升了任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明合成数据能提升无线感知任务性能，但其质量不可预测，导致性能增益无法保证，因此需要解决合成数据质量问题。

Method: 1. 提出可操作且通用的指标（亲和度、多样性）来量化合成数据质量。2. 引入SynCheck方案，一种质量引导的合成数据利用策略，在任务模型训练期间优化合成数据质量。

Result: 评估发现当前无线合成数据普遍存在亲和度限制，导致数据误标记和任务性能下降。SynCheck方案持续优于未考虑质量的合成数据利用方式，即使在原有方案导致性能下降13.4%的情况下，SynCheck仍能实现4.3%的性能提升。

Conclusion: 通过量化合成数据质量并采用SynCheck等质量引导方案，可以有效解决合成数据质量问题，显著提升生成模型在无线感知任务中的数据利用效率和任务性能。

Abstract: Generative models have gained significant attention for their ability to
produce realistic synthetic data that supplements the quantity of real-world
datasets. While recent studies show performance improvements in wireless
sensing tasks by incorporating all synthetic data into training sets, the
quality of synthetic data remains unpredictable and the resulting performance
gains are not guaranteed. To address this gap, we propose tractable and
generalizable metrics to quantify quality attributes of synthetic data -
affinity and diversity. Our assessment reveals prevalent affinity limitation in
current wireless synthetic data, leading to mislabeled data and degraded task
performance. We attribute the quality limitation to generative models' lack of
awareness of untrained conditions and domain-specific processing. To mitigate
these issues, we introduce SynCheck, a quality-guided synthetic data
utilization scheme that refines synthetic data quality during task model
training. Our evaluation demonstrates that SynCheck consistently outperforms
quality-oblivious utilization of synthetic data, and achieves 4.3% performance
improvement even when the previous utilization degrades performance by 13.4%.

</details>


### [212] [Attribution assignment for deep-generative sequence models enables interpretability analysis using positive-only data](https://arxiv.org/abs/2506.23182)
*Robert Frank,Michael Widrich,Rahmad Akbar,Günter Klambauer,Geir Kjetil Sandve,Philippe A. Robert,Victor Greiff*

Main category: cs.LG

TL;DR: 本文提出了GAMA，一种基于Integrated Gradients的自回归生成模型归因方法，用于解释生物序列设计模型，尤其在缺乏负样本数据的情况下，并通过合成数据集和抗体-抗原结合数据验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 生成式机器学习模型在治疗性设计中具有强大潜力，尤其在生物学领域负样本数据稀缺时，可仅用正样本训练。然而，生成模型缺乏归因方法，阻碍了从中提取可解释的生物学见解。

Method: 开发了GAMA（Generative Attribution Metric Analysis），一种基于Integrated Gradients的自回归生成模型归因方法。

Result: 通过合成数据集评估了GAMA的统计行为，并验证了其恢复生物学相关特征的能力。此外，将GAMA应用于实验性抗体-抗原结合数据，展示了其实用性。

Conclusion: GAMA实现了模型可解释性，并能验证生成序列设计策略，而无需负训练数据。

Abstract: Generative machine learning models offer a powerful framework for therapeutic
design by efficiently exploring large spaces of biological sequences enriched
for desirable properties. Unlike supervised learning methods, which require
both positive and negative labeled data, generative models such as LSTMs can be
trained solely on positively labeled sequences, for example, high-affinity
antibodies. This is particularly advantageous in biological settings where
negative data are scarce, unreliable, or biologically ill-defined. However, the
lack of attribution methods for generative models has hindered the ability to
extract interpretable biological insights from such models. To address this
gap, we developed Generative Attribution Metric Analysis (GAMA), an attribution
method for autoregressive generative models based on Integrated Gradients. We
assessed GAMA using synthetic datasets with known ground truths to characterize
its statistical behavior and validate its ability to recover biologically
relevant features. We further demonstrated the utility of GAMA by applying it
to experimental antibody-antigen binding data. GAMA enables model
interpretability and the validation of generative sequence design strategies
without the need for negative training data.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [213] [Golden Ratio Assisted Localization for Wireless Sensor Network](https://arxiv.org/abs/2506.22464)
*Hitesh Mohapatra*

Main category: cs.NI

TL;DR: 该论文提出一种利用黄金比例优化无线传感器网络（WSN）定位的新算法GRL。


<details>
  <summary>Details</summary>
Motivation: 旨在提高无线传感器网络的定位精度，同时最大限度地降低能耗。

Method: 引入了黄金比例定位（GRL）算法，利用黄金比例（phi）的数学特性优化节点部署和通信范围。具体方法包括基于phi值的锚节点部署和使用phi指数的跳数敏感加权。

Result: 在100x100米、100节点、10锚点的仿真中，GRL的平均定位误差为2.35米，优于DV-Hop（3.87米）和Centroid（4.95米）。能耗方面，GRL每节点定位能耗为1.12微焦耳，低于DV-Hop（1.78微焦耳）和Centroid（1.45微焦耳）。

Conclusion: GRL提供了一种更平衡、更高效的定位方法，特别适用于能耗受限和大规模的无线传感器网络部署。

Abstract: This paper presents a novel localization algorithm for wireless sensor
networks (WSNs) called Golden Ratio Localization (GRL), which leverages the
mathematical properties of the golden ratio (phi 1.618) to optimize both node
placement and communication range. GRL introduces phi-based anchor node
deployment and hop-sensitive weighting using phi-exponents to improve
localization accuracy while minimizing energy consumption. Through extensive
simulations conducted on a 100 m * 100 m sensor field with 100 nodes and 10
anchors, GRL achieved an average localization error of 2.35 meters,
outperforming DV- Hop (3.87 meters) and Centroid (4.95 meters). In terms of
energy efficiency, GRL reduced localization energy consumption to 1.12 microJ
per node, compared to 1.78 microJ for DV-Hop and 1.45 microJ for Centroid.
These results confirm that GRL provides a more balanced and efficient
localization approach, making it especially suitable for energy-constrained and
large-scale WSN deployments.

</details>


### [214] [Reliable Transmission of LTP Using Reinforcement Learning-Based Adaptive FEC](https://arxiv.org/abs/2506.22470)
*Liang Chen,Yu Song,Kanglian Zhao,Juan A. Fraire,Wenfeng Li*

Main category: cs.NI

TL;DR: 针对星际网络中DTN的LTP协议，传统FEC方法难以应对多变信道。本文提出一种基于强化学习的自适应FEC算法，通过预测信道状况并主动调整码率，显著减少解码失败和重传，提高数据传输效率。


<details>
  <summary>Details</summary>
Motivation: 在深空通信中，DTN的LTP协议面临信道条件高度可变和不可预测的挑战。现有的静态或延迟反馈型FEC方法无法有效适应这些变化，导致高重传率和低传输效率，影响数据可靠传输。

Method: 本文提出了一种基于强化学习（RL）的自适应FEC算法。该算法利用历史反馈和系统状态预测未来的信道条件，并据此主动调整编码率，旨在预判信道质量下降，从而避免解码失败并减少LTP重传，同时在信道良好时最小化冗余。

Result: 在模拟的地球-月球和地球-火星链路场景中进行的性能评估表明，该算法有效优化了星际网络的数据传输。与现有方法相比，矩阵解码失败率至少降低了2/3，显著提高了系统性能。

Conclusion: 所提出的RL自适应FEC算法通过其前瞻性的信道预测和码率调整能力，有效应对了深空网络中信道的高度不确定性，显著减少了解码失败和重传，从而优化了星际网络的数据传输效率和可靠性。

Abstract: Delay/Disruption Tolerant Networking (DTN) employs the Licklider Transmission
Protocol (LTP) with Automatic Repeat reQuest (ARQ) for reliable data delivery
in challenging interplanetary networks. While previous studies have integrated
packet-level Forward Erasure Correction (FEC) into LTP to reduce retransmission
time costs, existing static and delay-feedback-based dynamic coding methods
struggle with highly variable and unpredictable deep space channel conditions.
This paper proposes a reinforcement learning (RL)-based adaptive FEC algorithm
to address these limitations. The algorithm utilizes historical feedback and
system state to predict future channel conditions and proactively adjust the
code rate. This approach aims to anticipate channel quality degradation,
thereby preventing decoding failures and subsequent LTP retransmissions and
improving coding efficiency by minimizing redundancy during favorable channel
conditions. Performance evaluations conducted in simulated Earth-Moon and
Earth-Mars link scenarios demonstrate this algorithm's effectiveness in
optimizing data transmission for interplanetary networks. Compared to existing
methods, this approach demonstrates significant improvement, with matrix
decoding failures reduced by at least 2/3.

</details>


### [215] [RL-based Adaptive Task Offloading in Mobile-Edge Computing for Future IoT Networks](https://arxiv.org/abs/2506.22474)
*Ziad Qais Al Abbasi,Khaled M. Rabie,Senior Member,Xingwang Li,Senior Member,Wali Ullah Khan,Asma Abu Samah*

Main category: cs.NI

TL;DR: 本研究提出了一种基于强化学习（RL）的移动边缘计算（MEC）卸载方案，旨在优化超密集蜂窝网络中物联网（IoT）设备的资源分配和动态任务卸载，以解决远距离数据传输带来的高延迟问题，并提升网络性能。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）设备因计算和电源限制，需将任务发送至远距离云服务站，导致对低延迟服务（如工业控制、自动驾驶）构成挑战。

Method: 本研究提出了一种针对MEC辅助的超密集蜂窝网络的新型卸载方案。该方案利用强化学习（RL）技术，根据网络状况和用户需求实现高效资源分配和动态卸载决策。RL算法通过学习历史数据优化网络整体性能。此外，还引入了非正交多址接入（NOMA）以提高IoT设备的资源利用率。

Result: 仿真结果表明，所提出的方案在能效、网络吞吐量和用户满意度方面均优于其他现有卸载算法。

Conclusion: 所提出的基于强化学习的MEC卸载方案能有效解决物联网设备任务卸载中的延迟问题，显著提升超密集蜂窝网络的性能，尤其适用于对延迟敏感的物联网应用。

Abstract: The Internet of Things (IoT) has been increasingly used in our everyday lives
as well as in numerous industrial applications. However, due to limitations in
computing and power capabilities, IoT devices need to send their respective
tasks to cloud service stations that are usually located at far distances.
Having to transmit data far distances introduces challenges for services that
require low latency such as industrial control in factories and plants as well
as artificial intelligence assisted autonomous driving. To solve this issue,
mobile edge computing (MEC) is deployed at the networks edge to reduce
transmission time. In this regard, this study proposes a new offloading scheme
for MEC-assisted ultra dense cellular networks using reinforcement learning
(RL) techniques. The proposed scheme enables efficient resource allocation and
dynamic offloading decisions based on varying network conditions and user
demands. The RL algorithm learns from the networks historical data and adapts
the offloading decisions to optimize the networks overall performance.
Non-orthogonal multiple access is also adopted to improve resource utilization
among the IoT devices. Simulation results demonstrate that the proposed scheme
outperforms other stateof the art offloading algorithms in terms of energy
efficiency, network throughput, and user satisfaction.

</details>


### [216] [Innovative Research on IoT Architecture and Robotic Operating Platforms: Applications of Large Language Models and Generative AI](https://arxiv.org/abs/2506.22477)
*Huiwen Han*

Main category: cs.NI

TL;DR: 本文提出一种创新的物联网架构机器人操作系统平台，融合大语言模型、生成式AI、边缘计算和5G，旨在提升机器人智能和自主性，并通过案例研究展示其在多行业中优化工作流程、提高生产力的巨大潜力，并展望其作为下一代自动化催化剂的作用。


<details>
  <summary>Details</summary>
Motivation: 旨在提升物联网系统和机器人的智能性和自主性，使其能够进行实时决策并动态适应不断变化的环境。

Method: 本文介绍了一种创新的机器人操作系统平台设计，该平台基于变革性的物联网（IoT）架构，并无缝集成了大语言模型（LLMs）、生成式AI、边缘计算和5G网络等前沿技术。通过智能制造、医疗保健和服务行业等一系列案例研究来展示其潜力。

Result: 研究展示了物联网机器人技术在优化运营工作流程、提高生产力和提供创新、可扩展解决方案方面的巨大潜力。同时，强调了LLMs和生成式AI在推动智能机器人和物联网发展、塑造行业特定进步未来中的关键作用。

Conclusion: 研究成果不仅展示了这些技术的变革性力量，还提供了对其更广泛社会和工业影响的前瞻性视角，将其定位为下一代自动化和技术融合的催化剂。

Abstract: This paper introduces an innovative design for robotic operating platforms,
underpinned by a transformative Internet of Things (IoT) architecture,
seamlessly integrating cutting-edge technologies such as large language models
(LLMs), generative AI, edge computing, and 5G networks. The proposed platform
aims to elevate the intelligence and autonomy of IoT systems and robotics,
enabling them to make real-time decisions and adapt dynamically to changing
environments. Through a series of compelling case studies across industries
including smart manufacturing, healthcare, and service sectors, this paper
demonstrates the substantial potential of IoT-enabled robotics to optimize
operational workflows, enhance productivity, and deliver innovative, scalable
solutions. By emphasizing the roles of LLMs and generative AI, the research
highlights how these technologies drive the evolution of intelligent robotics
and IoT, shaping the future of industry-specific advancements. The findings not
only showcase the transformative power of these technologies but also offer a
forward-looking perspective on their broader societal and industrial
implications, positioning them as catalysts for next-generation automation and
technological convergence.

</details>


### [217] [Service Placement in Small Cell Networks Using Distributed Best Arm Identification in Linear Bandits](https://arxiv.org/abs/2506.22480)
*Mariam Yahya,Aydin Sezgin,Setareh Maghsudi*

Main category: cs.NI

TL;DR: 在MEC中，面对有限的边缘容量和未知服务需求，如何将计算密集型服务部署在边缘以最小化用户延迟是一个挑战。本文提出一种分布式多智能体最佳臂识别算法，将服务需求建模为线性函数，通过SBS协作高效识别最优边缘服务。


<details>
  <summary>Details</summary>
Motivation: 随着小型蜂窝网络用户对计算密集型服务的依赖增加，云端访问导致高延迟。多接入边缘计算（MEC）能将计算资源拉近用户，但有限的边缘容量以及未知的服务需求和动态网络条件，使得服务在本地（边缘）或云端部署的决策变得困难，亟需一种有效方法来最大化用户延迟降低。

Method: 将服务需求建模为服务属性的线性函数，并将服务放置任务表述为线性多臂老虎机问题，其中小型基站（SBS）作为智能体，服务作为臂。目标是识别在边缘部署时能最大程度降低总用户延迟的服务。为此，提出了一种分布式、自适应的多智能体最佳臂识别（BAI）算法，该算法在固定置信度设置下，通过SBS之间的协作来加速学习过程。

Result: 模拟结果显示，所提出的算法能够以期望的置信度识别出最优服务，并实现了接近最优的加速效果，即学习轮数与SBS数量成比例减少。此外，本文还提供了算法样本复杂度和通信开销的理论分析。

Conclusion: 本文提出的分布式多智能体最佳臂识别算法有效解决了MEC中服务部署决策的难题。该算法在未知服务需求和动态网络条件下，能够通过智能体协作，高效且高置信度地识别出最优服务，显著降低用户延迟，并展示了良好的学习效率和理论保障。

Abstract: As users in small cell networks increasingly rely on computation-intensive
services, cloud-based access often results in high latency. Multi-access edge
computing (MEC) mitigates this by bringing computational resources closer to
end users, with small base stations (SBSs) serving as edge servers to enable
low-latency service delivery. However, limited edge capacity makes it
challenging to decide which services to deploy locally versus in the cloud,
especially under unknown service demand and dynamic network conditions. To
tackle this problem, we model service demand as a linear function of service
attributes and formulate the service placement task as a linear bandit problem,
where SBSs act as agents and services as arms. The goal is to identify the
service that, when placed at the edge, offers the greatest reduction in total
user delay compared to cloud deployment. We propose a distributed and adaptive
multi-agent best-arm identification (BAI) algorithm under a fixed-confidence
setting, where SBSs collaborate to accelerate learning. Simulations show that
our algorithm identifies the optimal service with the desired confidence and
achieves near-optimal speedup, as the number of learning rounds decreases
proportionally with the number of SBSs. We also provide theoretical analysis of
the algorithm's sample complexity and communication overhead.

</details>


### [218] [Wireless Home Automation Using Social Networking Websites](https://arxiv.org/abs/2506.22482)
*Divya Alok Gupta,Dwith Chenna,B. Aditya Vighnesh Ramakanth*

Main category: cs.NI

TL;DR: 本文提出一种利用社交网络（如Twitter）安全认证和用户活动追踪来控制智能家居电器的新型无线家庭自动化系统。


<details>
  <summary>Details</summary>
Motivation: 当前的无线家庭自动化系统（WHAS）面临安全、多设备统一接口控制和用户友好性方面的挑战。

Method: 提出一种系统，利用社交网站（如Twitter）的安全认证系统，追踪用户在社交网络上的活动，并以此控制其家用电器。

Result: 文中强调了所提WHAS的应用，并与传统系统进行了比较，突出了其优势。

Conclusion: 所提出的系统通过整合社交网络认证和活动追踪，旨在提供更安全、更便捷的家庭自动化解决方案，优于传统系统。

Abstract: With the advent of Internet of Things, Wireless Home Automation Systems WHAS
are gradually gaining popularity. These systems are faced with multiple
challenges such as security; controlling a variety of home appliances with a
single interface and user friendliness. In this paper we propose a system that
uses secure authentication systems of social networking websites such as
Twitter, tracks the end-users activities on the social network and then control
his or her domestic appliances. At the end, we highlight the applications of
the proposed WHAS and compare the advantages of our proposed system over
traditional home automation systems.

</details>


### [219] [An Urban Multi-Operator QoE-Aware Dataset for Cellular Networks in Dense Environments](https://arxiv.org/abs/2506.22484)
*Muhammad Kabeer,Rosdiadee Nordin,Mehran Behjati,Farah Yasmin binti Mohd Shaharuddin*

Main category: cs.NI

TL;DR: 本研究提供了一个新的城市蜂窝网络数据集，专注于用户体验质量（QoE）和多样化移动模式，以支持网络规划和优化，特别适用于机器学习应用。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏捕获用户中心QoE和多样化移动模式的数据，而这对于高效的网络规划、优化、QoE驱动的优化和移动管理在复杂城市蜂窝网络中至关重要。城市网络面临高基础设施密度、多变用户移动性和多样服务需求的挑战。

Method: 研究人员使用GNetTrack Pro工具，在2平方公里的城市密集区域内，从三大主要商业网络运营商处收集了30,925条带标签的记录。数据集涵盖了关键信号质量参数（如RSRP、RSRQ、SNR）、多种真实世界移动模式（包括步行、高架步道、穿梭巴士和快速公交路线），以及多样化的网络流量场景（FTP上传下载、视频流和HTTP浏览）。此外，通过OpenCellID和现场检查识别并验证了132个物理蜂窝基站。

Result: 研究成功构建并发布了一个包含30,925条带标签记录的策展数据集，该数据集捕获了密集城市环境中多运营商的关键信号质量参数、多种真实世界移动模式下的性能数据以及多样化网络流量场景的数据。数据还反映了5G和新兴异构网络部署的高蜂窝密度特性。

Conclusion: 所提出的数据集填补了现有数据集中缺乏用户中心QoE和多样化移动模式的空白，为城市蜂窝网络规划和优化提供了可复现、可应用的资源。该数据集特别适用于机器学习应用，如切换优化、信号质量预测和多运营商性能评估。

Abstract: Urban cellular networks face complex performance challenges due to high
infrastructure density, varied user mobility, and diverse service demands.
While several datasets address network behaviour across different environments,
there is a lack of datasets that captures user centric Quality of Experience
(QoE), and diverse mobility patterns needed for efficient network planning and
optimization solutions, which are important for QoE driven optimizations and
mobility management. This study presents a curated dataset of 30,925 labelled
records, collected using GNetTrack Pro within a 2 km2 dense urban area,
spanning three major commercial network operators. The dataset captures key
signal quality parameters (e.g., RSRP, RSRQ, SNR), across multiple real world
mobility modes including pedestrian routes, canopy walkways, shuttle buses, and
Bus Rapid Transit (BRT) routes. It also includes diverse network traffic
scenarios including (1) FTP upload and download, (2) video streaming, and (3)
HTTP browsing. A total of 132 physical cell sites were identified and validated
through OpenCellID and on-site field inspections, illustrating the high cell
density characteristic of 5G and emerging heterogeneous network deployment. The
dataset is particularly suited for machine learning applications, such as
handover optimization, signal quality prediction, and multi operator
performance evaluation. Released in a structured CSV format with accompanying
preprocessing and visualization scripts, this dataset offers a reproducible,
application ready resource for researchers and practitioners working on urban
cellular network planning and optimization.

</details>


### [220] [AGI Enabled Solutions For IoX Layers Bottlenecks In Cyber-Physical-Social-Thinking Space](https://arxiv.org/abs/2506.22487)
*Amar Khelloufi,Huansheng Ning,Sahraoui Dhelim,Jianguo Ding*

Main category: cs.NI

TL;DR: 本文综述了AGI增强的IoX研究，探讨了AGI如何解决CPST生态系统中感知、网络和应用层的瓶颈问题，并提出了未来的研究方向和挑战。


<details>
  <summary>Details</summary>
Motivation: 解决物联网（IoX）与通用人工智能（AGI）融合在网络物理社会思维（CPST）生态系统的感知、网络和应用层中遇到的关键瓶颈问题，并对AGI增强的IoX研究进行系统和全面的综述。

Method: 本文作为一篇综述性研究，系统回顾了AGI增强的IoX研究，重点关注感知层数据管理、网络层协议优化和应用层决策框架。具体探讨了AGI如何利用自适应传感器融合、边缘预处理、选择性注意机制、神经符号推理、主动推理和因果推理等方法来缓解IoX瓶颈。

Result: 研究发现，AGI驱动的策略（如自适应传感器融合、边缘预处理和语义建模）为感知层数据过载、网络层协议异构性和应用层身份爆炸提供了新颖的解决方案。此外，研究强调了未来AGI增强的IoX系统中跨层集成、量子通信和伦理治理框架的重要性。

Conclusion: 尽管AGI增强的IoX潜力巨大，但仍面临计算需求、可扩展性和实际验证等尚未解决的挑战，需要进一步深入研究。AGI增强的IoX正在成为互联系统和先进AI交叉领域的一个新兴关键研究方向。

Abstract: The integration of the Internet of Everything (IoX) and Artificial General
Intelligence (AGI) has given rise to a transformative paradigm aimed at
addressing critical bottlenecks across sensing, network, and application layers
in Cyber-Physical-Social Thinking (CPST) ecosystems. In this survey, we provide
a systematic and comprehensive review of AGI-enhanced IoX research, focusing on
three key components: sensing-layer data management, network-layer protocol
optimization, and application-layer decision-making frameworks. Specifically,
this survey explores how AGI can mitigate IoX bottlenecks challenges by
leveraging adaptive sensor fusion, edge preprocessing, and selective attention
mechanisms at the sensing layer, while resolving network-layer issues such as
protocol heterogeneity and dynamic spectrum management, neuro-symbolic
reasoning, active inference, and causal reasoning, Furthermore, the survey
examines AGI-enabled frameworks for managing identity and relationship
explosion. Key findings suggest that AGI-driven strategies, such as adaptive
sensor fusion, edge preprocessing, and semantic modeling, offer novel solutions
to sensing-layer data overload, network-layer protocol heterogeneity, and
application-layer identity explosion. The survey underscores the importance of
cross-layer integration, quantum-enabled communication, and ethical governance
frameworks for future AGI-enabled IoX systems. Finally, the survey identifies
unresolved challenges, such as computational requirements, scalability, and
real-world validation, calling for further research to fully realize AGI's
potential in addressing IoX bottlenecks. we believe AGI-enhanced IoX is
emerging as a critical research field at the intersection of interconnected
systems and advanced AI.

</details>


### [221] [Integrated Multimodal Sensing and Communication: Challenges, Technologies, and Architectures](https://arxiv.org/abs/2506.22507)
*Yubo Peng,Luping Xiang,Kun Yang,Feibo Jiang,Kezhi Wang,Christos Masouros*

Main category: cs.NI

TL;DR: 针对6G网络中单模态ISAC的局限性，本文提出多模态ISAC范式，分析其挑战、引入使能技术和设计架构范式（如F-MAC），并通过案例研究证明F-MAC在感知精度上较传统系统提升约80%。


<details>
  <summary>Details</summary>
Motivation: 现有集成感知与通信（ISAC）系统主要依赖单模态传感器，导致环境特征表示有限且在6G应用（如自动驾驶、沉浸式服务）需求下存在性能瓶颈，因此需要从单模态转向多模态ISAC。

Method: 分析实现多模态ISAC的关键挑战，包括异构数据融合、分布式传感器高通信开销和系统架构设计；引入大型AI模型、语义通信和多智能体系统等使能技术；提出融合式（F-MAC）、交互式（I-MAC）和中继式（R-MAC）三种架构范式；并基于F-MAC方案进行案例研究。

Result: 基于F-MAC方案的案例研究表明，该方案与传统的单模态ISAC系统相比，实现了更全面的感知，并将感知精度提高了约80%。

Conclusion: 多模态ISAC对于6G网络发展至关重要，本文提出的架构范式和使能技术为解决其挑战提供了有效途径，并展示了显著的性能提升。未来仍需解决一些开放性问题。

Abstract: The evolution towards 6G networks requires the intelligent integration of
communication and sensing capabilities to support diverse and complex
applications, such as autonomous driving and immersive services. However,
existing integrated sensing and communication (ISAC) systems predominantly rely
on single-modal sensors as primary participants, which leads to a limited
representation of environmental features and significant performance
bottlenecks under the emerging requirements of 6G applications. This limitation
motivates a paradigm shift from single-modal to multimodal ISAC. In this
article, we first analyze the key challenges in realizing multimodal ISAC,
including the fusion of heterogeneous multimodal data, the high communication
overhead among distributed sensors, and the design of efficient and scalable
system architectures. We then introduce several enabling technologies, such as
large AI models, semantic communication, and multi-agent systems, that hold
promise for addressing these challenges. To operationalize these technologies,
we zoom into three architectural paradigms: fusion-based multimodal ISAC
(F-MAC), interaction-based multimodal ISAC (I-MAC), and relay-based multimodal
ISAC (R-MAC), each tailored to organize devices and modalities for efficient
collaboration in different scenarios. Thereafter, a case study is presented
based on the F-MAC scheme, demonstrating that the scheme achieves more
comprehensive sensing and improves sensing accuracy by approximately 80%
compared to conventional single-modal ISAC systems. Finally, we discuss several
open issues to be addressed in the future.

</details>


### [222] [Towards an Optimized Multi-Cyclic Queuing and Forwarding in Time Sensitive Networking with Time Injection](https://arxiv.org/abs/2506.22671)
*Rubi Debnath,Mohammadreza Barzegaran,Sebastian Steinhorst*

Main category: cs.NI

TL;DR: Multi-CQF是为解决CQF局限性而提出的TSN机制，但其配置研究不足。本文提出结合时间注入（TI）的遗传算法（GA）和混合遗传模拟退火（GASA）算法，以优化Multi-CQF网络配置并提升可调度性，实验证明算法显著优于基线SA模型。


<details>
  <summary>Details</summary>
Motivation: 循环排队转发（CQF）的单周期限制了其支持多样化时间敏感网络（TSN）流量的能力。多周期排队转发（Multi-CQF）作为一种新兴机制，虽能更有效地适应不同时间要求的TSN流，但其配置研究有限，导致机制理解不足和实际应用受限。此外，时间注入（TI）对CQF队列资源利用率有影响，但其在Multi-CQF中的作用尚未被探索。

Method: 本研究引入一组约束并利用领域特定知识（DSK）来缩小Multi-CQF配置的搜索空间。在此基础上，开发了开源的遗传算法（GA）和混合遗传模拟退火（GASA）方法，用于高效配置Multi-CQF网络。同时，将时间注入（TI）引入Multi-CQF以增强其可调度性。

Result: 实验结果表明，与基线模拟退火（SA）模型相比，所提出的算法显著增加了可调度的时间触发（TT）流数量，平均调度性能提升了15%。此外，GASA算法的收敛速度比SA模型快20%，且时间复杂度更低，在速度和效率上均优于SA模型。

Conclusion: 本研究提出的基于遗传算法和混合遗传模拟退火的Multi-CQF配置方法，结合时间注入，能有效提升Multi-CQF网络中时间触发流的可调度性。这些算法在调度性能、收敛速度和效率方面均优于现有基线方法，为Multi-CQF的实际应用提供了更有效的配置策略。

Abstract: Cyclic Queuing and Forwarding (CQF) is a Time-Sensitive Networking (TSN)
shaping mechanism that provides bounded latency and deterministic Quality of
Service (QoS). However, CQF's use of a single cycle restricts its ability to
support TSN traffic with diverse timing requirements. Multi-Cyclic Queuing and
Forwarding (Multi-CQF) is a new and emerging TSN shaping mechanism that uses
multiple cycles on the same egress port, allowing it to accommodate TSN flows
with varied timing requirements more effectively than CQF. Despite its
potential, current Multi-CQF configuration studies are limited, leading to a
lack of comprehensive research, poor understanding of the mechanism, and
limited adoption of Multi-CQF in practical applications. Previous work has
shown the impact of Time Injection (TI), defined as the start time of
Time-Triggered (TT) flows at the source node, on CQF queue resource
utilization. However, the impact of TI has not yet been explored in the context
of Multi-CQF. This paper introduces a set of constraints and leverages Domain
Specific Knowledge (DSK) to reduce the search space for Multi-CQF
configuration. Building on this foundation, we develop an open-source Genetic
Algorithm (GA) and a hybrid GA-Simulated Annealing (GASA) approach to
efficiently configure Multi-CQF networks and introduce TI in Multi-CQF to
enhance schedulability. Experimental results show that our proposed algorithms
significantly increase the number of scheduled TT flows compared to the
baseline Simulated Annealing (SA) model, improving scheduling by an average of
15%. Additionally, GASA achieves a 20% faster convergence rate and lower time
complexity, outperforming the SA model in speed, and efficiency.

</details>


### [223] [Trusted Routing for Blockchain-Enabled Low-Altitude Intelligent Networks](https://arxiv.org/abs/2506.22745)
*Sijie He,Ziye Jia,Qiuming Zhu,Fuhui Zhou,Qihui Wu*

Main category: cs.NI

TL;DR: 针对低空智能网络（LAINs）中无人机路由的稳定性与安全性挑战，本文提出一种基于区块链的零信任架构管理无人机加入与退出，并将路由问题建模为去中心化部分可观察马尔可夫决策过程，设计了多智能体双深度Q网络自适应路由算法。仿真结果显示，该方法平均降低22.38%的端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 低空智能网络（LAINs）中的无人机（UAVs）具有分布式拓扑、高动态移动性，且易受安全威胁，这可能导致数据传输路由性能下降。因此，如何确保LAINs的路由稳定性与安全性是一个重大挑战。

Method: 1. 提出区块链赋能的零信任架构，用于管理无人机的加入与退出。2. 将路由问题公式化为最小化端到端（E2E）延迟的整数线性规划。3. 考虑到LAINs的分布式特性，将路由问题重构为去中心化部分可观察马尔可夫决策过程。4. 设计了一种基于软分层经验回放缓冲的多智能体双深度Q网络（MADDPG）自适应路由算法。

Result: 仿真结果表明，与基准方法相比，所提出的机制使总端到端（E2E）延迟平均降低了22.38%。

Conclusion: 所提出的区块链赋能的零信任架构和基于MADDPG的自适应路由算法，能够有效解决LAINs中的路由稳定性与安全性问题，并显著降低端到端延迟，提高了网络性能。

Abstract: Due to the scalability and portability, the low-altitude intelligent networks
(LAINs) are essential in various fields such as surveillance and disaster
rescue. However, in LAINs, unmanned aerial vehicles (UAVs) are characterized by
the distributed topology and high dynamic mobility, and vulnerable to security
threats, which may degrade the routing performance for data transmission.
Hence, how to ensure the routing stability and security of LAINs is a
challenge. In this paper, we focus on the routing process in LAINs with
multiple UAV clusters and propose the blockchain-enabled zero-trust
architecture to manage the joining and exiting of UAVs. Furthermore, we
formulate the routing problem to minimize the end-to-end (E2E) delay, which is
an integer linear programming and intractable to solve. Therefore, considering
the distribution of LAINs, we reformulate the routing problem into a
decentralized partially observable Markov decision process. With the proposed
soft hierarchical experience replay buffer, the multi-agent double deep
Q-network based adaptive routing algorithm is designed. Finally, simulations
are conducted and numerical results show that the total E2E delay of the
proposed mechanism decreases by 22.38\% than the benchmark on average.

</details>


### [224] [Offline Reinforcement Learning for Mobility Robustness Optimization](https://arxiv.org/abs/2506.22793)
*Pegah Alizadeh,Anastasios Giovanidis,Pradeepa Ramachandra,Vasileios Koutsoukis,Osama Arouk*

Main category: cs.NI

TL;DR: 本文探讨使用离线强化学习（包括决策Transformer和保守Q学习）优化MRO中的小区个体偏移调整，结果显示性能提升（高达7%），并比基于规则的MRO更具灵活性。


<details>
  <summary>Details</summary>
Motivation: 旨在改进移动性鲁棒性优化（MRO）算法中的小区个体偏移调整，通过利用离线强化学习来学习最优策略，以期克服现有基于规则方法的局限性。

Method: 采用离线强化学习方法，具体应用了基于序列的决策Transformer（Decision Transformers）和基于价值的保守Q学习（Conservative Q-Learning）。这些方法利用离线数据集学习最优策略，无需额外探索，并沿用了与传统基于规则MRO相同的输入特征（如故障、乒乓效应和切换问题）。

Result: 在真实的New Radio网络（3500 MHz载波频率、多样化业务组合）中，离线强化学习方法优于基于规则的MRO，性能提升高达7%。

Conclusion: 离线强化学习是MRO优化的有效方法，相较于传统的基于规则的方法，它能提供更好的性能和更高的操作灵活性（因其能利用相同数据集训练以适应不同的目标函数）。

Abstract: In this work we revisit the Mobility Robustness Optimisation (MRO) algorithm
and study the possibility of learning the optimal Cell Individual Offset tuning
using offline Reinforcement Learning. Such methods make use of collected
offline datasets to learn the optimal policy, without further exploration. We
adapt and apply a sequence-based method called Decision Transformers as well as
a value-based method called Conservative Q-Learning to learn the optimal policy
for the same target reward as the vanilla rule-based MRO. The same input
features related to failures, ping-pongs, and other handover issues are used.
Evaluation for realistic New Radio networks with 3500 MHz carrier frequency on
a traffic mix including diverse user service types and a specific tunable
cell-pair shows that offline-RL methods outperform rule-based MRO, offering up
to 7% improvement. Furthermore, offline-RL can be trained for diverse objective
functions using the same available dataset, thus offering operational
flexibility compared to rule-based methods.

</details>


### [225] [Reliable Image Transmission in CPS-based Pub/Sub](https://arxiv.org/abs/2506.22875)
*Everson Flores,Bruna Guterres,Thomaz Pereira Junior,Paula Barros,Alberto Cabral,Cristiana Lima Dora,Marcelo Malheiros,Marcelo Pias*

Main category: cs.NI

TL;DR: 本研究评估了MQTT协议在工业IoT/CPS高流量和间歇性连接下实时图像传输的可靠性。


<details>
  <summary>Details</summary>
Motivation: 工业IoT和CPS应用需要可靠的图像处理和实时适应性，但现有研究缺乏对MQTT协议在图像共享和传输中，高流量及间歇性连接场景下性能的评估，这限制了其在关键应用中的使用。

Method: 通过一系列受控的测试平台验证实验，评估了在网络中断和高数据流量情景下，工业IoT和CPS系统中基于MQTT发布/订阅通信模型的实时图像传输可靠性。

Result: 实验证明，MQTT系统在正常条件下传输可靠。其恢复能力取决于故障点：协调器节点故障时可完全恢复，而生产者节点或代理故障时仅能部分恢复。此外，系统能防止重复错误并良好适应不断增长的网络需求。

Conclusion: 该研究证实了基于MQTT的系统适用于需要高效和弹性数据处理的工业应用。

Abstract: Developments in communication and automation have driven the expansion of
distributed networks, essential for IoT and CPS development in industrial
applications requiring reliable image processing and real-time adaptability.
Although broadly adopted, there is a literature gap regarding the performance
of MQTT protocol for image sharing and transmission under high-traffic
scenarios with intermittent connectivity, restricting its use in critical IoT
and CPS applications. In this context, the present work examines the
reliability of real-time image transmission in IoT and CPS industrial systems
that utilize the MQTT-based publish/subscribe communication model. It focuses
on scenarios with network interruptions and high data traffic, evaluating the
performance of a distributed system through a series of controlled testbed
validation experiments. Experimental validation demonstrated that while the
MQTT-based system sustains reliable transmission under normal conditions, its
recovery capability depends on the failure point, with complete restoration
occurring when disruptions affect the Orchestrator Node and partial recovery
when the Producer Node or Broker are affected. The study also confirmed that
the system prevents duplicate errors and adapts well to increasing network
demands, reinforcing its suitability for industrial applications that require
efficient and resilient data handling.

</details>


### [226] [Resilient-Native and Intelligent Next-Generation Wireless Systems: Key Enablers, Foundations, and Applications](https://arxiv.org/abs/2506.22991)
*Mehdi Bennis,Sumudu Samarakoon,Tamara Alshammari,Chathuranga Weeraddana,Zhoujun Tian,Chaouki Ben Issaid*

Main category: cs.NI

TL;DR: 无线网络作为关键基础设施，面对日益增长的干扰，急需提升弹性。本文旨在明确弹性的定义、数学基础及应用技术，并为未来弹性无线系统建立统一的理解和工程化基础，提供发展路线图。


<details>
  <summary>Details</summary>
Motivation: 鉴于无线网络作为关键社会基础设施日益受到自然和人为干扰的威胁，且传统鲁棒性和可靠性不足以应对必然发生的故障，因此迫切需要研究和构建无线网络的弹性，使其能有效抵御并从各种意外中断中恢复。

Method: 本文首先辨析了弹性与可靠性、鲁棒性的区别。随后，深入探讨了基于抽象、组合性和涌现的弹性关键数学基础。接着，聚焦于与弹性独特特性相关的多种技术和方法，并通过一系列用例展示其应用。

Result: 本文阐明了弹性与可靠性、鲁棒性的区别，深入探讨了其基于抽象、组合性和涌现的数学基础，并总结了多种相关技术和方法及其应用案例。

Conclusion: 本文致力于为无线通信系统的弹性理解、建模和工程化奠定统一基础，并为下一代弹性原生和智能无线系统的发展描绘了路线图。

Abstract: Just like power, water, and transportation systems, wireless networks are a
crucial societal infrastructure. As natural and human-induced disruptions
continue to grow, wireless networks must be resilient. This requires them to
withstand and recover from unexpected adverse conditions, shocks, unmodeled
disturbances and cascading failures. Unlike robustness and reliability,
resilience is based on the understanding that disruptions will inevitably
happen. Resilience, as elasticity, focuses on the ability to bounce back to
favorable states, while resilience as plasticity involves agents and networks
that can flexibly expand their states and hypotheses through real-time
adaptation and reconfiguration. This situational awareness and active
preparedness, adapting world models and counterfactually reasoning about
potential system failures and the best responses, is a core aspect of
resilience. This article will first disambiguate resilience from reliability
and robustness, before delving into key mathematical foundations of resilience
grounded in abstraction, compositionality and emergence. Subsequently, we focus
our attention on a plethora of techniques and methodologies pertaining to the
unique characteristics of resilience, as well as their applications through a
comprehensive set of use cases. Ultimately, the goal of this paper is to
establish a unified foundation for understanding, modeling, and engineering
resilience in wireless communication systems, while laying a roadmap for the
next-generation of resilient-native and intelligent wireless systems.

</details>


### [227] [Model-Based Diagnosis: Automating End-to-End Diagnosis of Network Failures](https://arxiv.org/abs/2506.23083)
*Changrong Wu,Yiyao Yu,Myungjin Lee,Jayanth Srinivasa,Ennan Zhai,George Varghese,Yuval Tamir*

Main category: cs.NI

TL;DR: 本文提出一种名为“基于模型的网络诊断”的新范式及其实现NetDx，旨在通过系统化方法自动快速识别企业网络故障的根源，涵盖数据平面和控制平面故障，实验证明其诊断准确率高且速度显著提升。


<details>
  <summary>Details</summary>
Motivation: 企业网络故障的快速诊断和修复至关重要，因为中断会导致重大的业务影响。现有工作侧重于诊断原语或仅限于问题的一个子集（如只针对数据平面或控制平面故障），缺乏一种系统化的方法来自动化地诊断网络故障的根源。

Method: 本文提出“基于模型的网络诊断”新范式，通过从数据包转发和路由模型中系统地推导出自动化诊断程序。这些程序能够根据端到端的用户级症状报告，识别包括硬件、固件和软件在内的数据平面和分布式控制平面中的故障根源。研究团队构建了NetDx作为概念验证实现，并将其部署在由P4交换机和分布式路由软件组成的新网络模拟器上。

Result: 通过自动化故障注入实验，NetDx的鲁棒性和覆盖范围得到了验证，100%的故障被正确诊断。此外，在一个包含来自大型云提供商的33个真实故障的数据集上，NetDx能够高效地在数秒内诊断出30个故障，而传统方法需要数小时。

Conclusion: 基于模型的网络诊断方法提供了一种系统化、自动化的方式来识别网络故障的根源，能够显著加速诊断过程并取代经验丰富的人工操作。NetDx的成功验证表明该方法在提高企业网络故障诊断效率和准确性方面具有巨大潜力。

Abstract: Fast diagnosis and repair of enterprise network failures is critically
important since disruptions cause major business impacts. Prior works focused
on diagnosis primitives or procedures limited to a subset of the problem, such
as only data plane or only control plane faults. This paper proposes a new
paradigm, model-based network diagnosis, that provides a systematic way to
derive automated procedures for identifying the root cause of network failures,
based on reports of end-to-end user-level symptoms. The diagnosis procedures
are systematically derived from a model of packet forwarding and routing,
covering hardware, firmware, and software faults in both the data plane and
distributed control plane. These automated procedures replace and dramatically
accelerate diagnosis by an experienced human operator. Model-based diagnosis is
inspired by, leverages, and is complementary to recent work on network
verification. We have built NetDx, a proof-of-concept implementation of
model-based network diagnosis. We deployed NetDx on a new emulator of networks
consisting of P4 switches with distributed routing software. We validated the
robustness and coverage of NetDx with an automated fault injection campaign, in
which 100% of faults were diagnosed correctly. Furthermore, on a data set of 33
faults from a large cloud provider that are within the domain targeted by
NetDx, 30 are efficiently diagnosed in seconds instead of hours.

</details>


### [228] [Autonomous Vision-Aided UAV Positioning for Obstacle-Aware Wireless Connectivity](https://arxiv.org/abs/2506.23190)
*Kamran Shafafi,Manuel Ricardo,Rui Campos*

Main category: cs.NI

TL;DR: 提出VTOPA算法，利用计算机视觉优化无人机定位，在障碍密集城市环境中显著提升无线连接性能。


<details>
  <summary>Details</summary>
Motivation: 在障碍物密集的城市环境中，无人机（UAV）作为空中Wi-Fi接入点或蜂窝基站，其定位优化以维持与地面用户设备（UE）的视距（LoS）连接极具挑战性，从而影响无线连接质量和服务（QoS）。

Method: 论文提出VTOPA（Vision-Aided Traffic- and Obstacle-Aware Positioning Algorithm）算法。该算法利用计算机视觉自主提取环境信息（如障碍物和UE位置），并据此优化无人机定位。它优先保障LoS连接，并实时动态适应用户流量需求。算法通过ns-3仿真进行评估。

Result: VTOPA算法在障碍物丰富的环境中，相较于基准方法，实现了聚合吞吐量高达50%的提升和延迟高达50%的降低，且未牺牲公平性。

Conclusion: VTOPA算法能够有效解决城市环境中无人机定位优化难题，显著提升无线连接性能和用户体验，为未来城市空中网络部署提供了有前景的解决方案。

Abstract: Unmanned Aerial Vehicles (UAVs) offer a promising solution for enhancing
wireless connectivity and Quality of Service (QoS) in urban environments,
acting as aerial Wi-Fi access points or cellular base stations. Their
flexibility and rapid deployment capabilities make them suitable for addressing
infrastructure gaps and traffic surges. However, optimizing UAV positions to
maintain Line of Sight (LoS) links with ground User Equipment (UEs) remains
challenging in obstacle-dense urban scenarios. This paper proposes VTOPA, a
Vision-Aided Traffic- and Obstacle-Aware Positioning Algorithm that
autonomously extracts environmental information -- such as obstacles and UE
locations -- via computer vision and optimizes UAV positioning accordingly. The
algorithm prioritizes LoS connectivity and dynamically adapts to user traffic
demands in real time. Evaluated through simulations in ns-3, VTOPA achieves up
to a 50% increase in aggregate throughput and a 50% reduction in delay, without
compromising fairness, outperforming benchmark approaches in obstacle-rich
environments.

</details>


### [229] [On the Resilience of Underwater Semantic Wireless Communications](https://arxiv.org/abs/2506.23350)
*João Pedro Loureiro,Patrícia Delgado,Tomás Feliciano Ribeiro,Filipe B. Teixeira,Rui Campos*

Main category: cs.NI

TL;DR: 本文评估了SAGE（一个结合语义处理和生成式AI的水下图像语义通信框架）的鲁棒性。研究表明，即使在水下声学信道模拟的字符错误条件下，SAGE也能成功重建有意义的图像内容，证明其在恶劣环境下的潜力。


<details>
  <summary>Details</summary>
Motivation: 水下无线通信因传播限制面临严峻挑战，传统无线电和光学技术效果不佳。远程声学通信虽能支持数公里距离，但存在带宽低、误码率高和多径干扰等问题。语义通信通过传输提取的语义特征而非原始数据，显著减少数据量，有望解决这些问题。

Method: 本文评估了SAGE框架的韧性。SAGE是一个面向语义的通信框架，它结合语义处理和生成式人工智能（GenAI），将图像数据压缩并以文本描述的形式通过声学链路传输。为评估其鲁棒性，研究使用了定制模拟器来引入水下声学信道中观察到的字符错误。

Result: 评估结果表明，即使在不同的错误条件下，SAGE也能成功重建有意义的图像内容。

Conclusion: SAGE在恶劣环境下实现稳健高效的水下无线通信具有巨大潜力。

Abstract: Underwater wireless communications face significant challenges due to
propagation constraints, limiting the effectiveness of traditional radio and
optical technologies. Long-range acoustic communications support distances up
to a few kilometers, but suffer from low bandwidth, high error ratios, and
multipath interference. Semantic communications, which focus on transmitting
extracted semantic features rather than raw data, present a promising solution
by significantly reducing the volume of data transmitted over the wireless
link.
  This paper evaluates the resilience of SAGE, a semantic-oriented
communications framework that combines semantic processing with Generative
Artificial Intelligence (GenAI) to compress and transmit image data as textual
descriptions over acoustic links. To assess robustness, we use a
custom-tailored simulator that introduces character errors observed in
underwater acoustic channels. Evaluation results show that SAGE can
successfully reconstruct meaningful image content even under varying error
conditions, highlighting its potential for robust and efficient underwater
wireless communication in harsh environments.

</details>


### [230] [Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent Metasurfaces](https://arxiv.org/abs/2506.23488)
*Geng Sun,Mingzhe Fan,Lei Zhang,Hongyang Pan,Jiahui Li,Chuang Zhang,Linyao Li,Changyuan Zhao,Chau Yuen*

Main category: cs.NI

TL;DR: 研究并优化了无人机搭载智能超表面（UAV-SIMs）辅助的通信系统，通过联合优化用户关联、无人机定位和超表面相移，显著提升了低空经济网络的通信容量。


<details>
  <summary>Details</summary>
Motivation: 为满足无线通信系统在复杂环境中对更高数据速率和更可靠连接的需求，本文利用移动智能超表面（SIMs）的优势，特别是无人机搭载SIMs在低空经济网络中作为基站和移动平台，旨在最大化网络容量。

Method: 构建了一个无人机-SIMs联合优化问题（USBJOP），涵盖用户关联、无人机三维定位和SIMs多层相移。该非凸且NP难的问题被分解为三个子问题，并通过交替优化策略求解。其中，用户关联和无人机定位问题被转化为凸形式由CVX工具求解，SIMs相移问题则通过基于生成式人工智能（GAI）的混合优化算法解决。

Result: 仿真结果表明，所提出的方法显著优于基准方案，实现了约1.5倍的网络容量提升。同时，基于GAI的方法在保持解决方案质量的同时，将算法运行时间缩短了10%。

Conclusion: 所提出的无人机搭载智能超表面通信系统及其联合优化方法，能有效提升低空经济网络的通信性能和容量，特别是在相移优化中引入GAI提升了效率。

Abstract: Wireless communication systems face significant challenges in meeting the
increasing demands for higher data rates and more reliable connectivity in
complex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a
promising technology for realizing wave-domain signal processing, with mobile
SIMs offering superior communication performance compared to their fixed
counterparts. In this paper, we investigate a novel unmanned aerial vehicle
(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the
low-altitude economy (LAE) networks paradigm, where UAVs function as both base
stations that cache SIM-processed data and mobile platforms that flexibly
deploy SIMs to enhance uplink communications from ground users. To maximize
network capacity, we formulate a UAV-SIM-based joint optimization problem
(USBJOP) that comprehensively addresses three critical aspects: the association
between UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and
the phase shifts across multiple SIM layers. Due to the inherent non-convexity
and NP-hardness of USBJOP, we decompose it into three sub-optimization
problems, \textit{i.e.}, association between UAV-SIMs and users optimization
problem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase
shifts optimization problem (USPSOP), and solve them using an alternating
optimization strategy. Specifically, we transform AUUOP and ULOP into convex
forms solvable by the CVX tool, while addressing USPSOP through a generative
artificial intelligence (GAI)-based hybrid optimization algorithm. Simulations
demonstrate that our proposed approach significantly outperforms benchmark
schemes, achieving approximately 1.5 times higher network capacity compared to
suboptimal alternatives. Additionally, our proposed GAI method reduces the
algorithm runtime by 10\% while maintaining solution quality.

</details>


### [231] [Securing the Sky: Integrated Satellite-UAV Physical Layer Security for Low-Altitude Wireless Networks](https://arxiv.org/abs/2506.23493)
*Jiahui Li,Geng Sun,Xiaoyu Sun,Fang Mei,Jingjing Wang,Xiangwang Hou,Daxin Tian,Victor C. M. Leung*

Main category: cs.NI

TL;DR: 针对低空无线网络（LAWNs）在卫星-无人机集成网络中面临的传输安全挑战，本文提出了一种基于协作波束成形的物理层安全方案，并通过仿真验证了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 低空无线网络（LAWNs）作为6G网络的关键组成部分，其卫星-无人机集成网络能提供广泛连接。然而，低空空域较高的视距（LoS）概率显著增加了传输的安全隐患。

Method: 本文提出了一种基于协作波束成形的物理层安全方案，用于低空无线网络。该方案利用集成卫星-无人机网络和无人机蜂群，以实现数据分发、数据中继、窃听者共谋以及窃听者信息不完善等场景下的物理层安全。具体案例研究包括一个安全中继系统和一个双向空中安全通信框架。

Result: 仿真结果表明，所提出的物理层安全方案对低空无线通信是有效且有益的。实践可行性分析显示，该方法适用于低空无线网络场景。

Conclusion: 本文提出的基于协作波束成形的物理层安全方案能够有效提高低空无线通信的安全性，并具有良好的实用性。文章还讨论了当前面临的挑战和未来的研究方向。

Abstract: Low-altitude wireless networks (LAWNs) have garnered significant attention in
the forthcoming 6G networks. In LAWNs, satellites with wide coverage and
unmanned aerial vehicles (UAVs) with flexible mobility can complement each
other to form integrated satellite-UAV networks, providing ubiquitous and
high-speed connectivity for low-altitude operations. However, the higher
line-of-sight probability in low-altitude airspace increases transmission
security concerns. In this work, we present a collaborative beamforming-based
physical layer security scheme for LAWNs. We introduce the fundamental aspects
of integrated satellite-UAV networks, physical layer security, UAV swarms, and
collaborative beamforming for LAWN applications. Following this, we highlight
several opportunities for collaborative UAV swarm secure applications enabled
by satellite networks, including achieving physical layer security in scenarios
involving data dissemination, data relay, eavesdropper collusion, and imperfect
eavesdropper information. Next, we detail two case studies: a secure relay
system and a two-way aerial secure communication framework specifically
designed for LAWN environments. Simulation results demonstrate that these
physical layer security schemes are effective and beneficial for secure
low-altitude wireless communications. A short practicality analysis shows that
the proposed method is applicable to LAWN scenarios. Finally, we discuss
current challenges and future research directions for enhancing security in
LAWNs.

</details>


### [232] [The Kubernetes Network Driver Model: A Composable Architecture for High-Performance Networking](https://arxiv.org/abs/2506.23628)
*Antonio Ojea*

Main category: cs.NI

TL;DR: 针对Kubernetes传统网络在AI/ML和Telco场景下的局限，本文提出KNDs（Kubernetes网络驱动），一种利用DRA、NRI等技术实现网络资源声明式管理的模块化架构。其实现DraNet展示了RDMA设备的高性能连接，为AI/ML和未来Telco方案提供支持。


<details>
  <summary>Details</summary>
Motivation: 传统的Kubernetes网络难以满足AI/ML和不断发展的电信基础设施日益增长的需求，其命令式资源配置和API限制是主要挑战。

Method: 论文引入Kubernetes网络驱动（KNDs），这是一种变革性、模块化且声明式的架构。KNDs通过利用动态资源分配（DRA）、节点资源接口（NRI）的改进以及即将到来的OCI运行时规范变更，将网络资源管理深度集成到Kubernetes核心中。DraNet是其一个实现范例。

Result: KNDs架构的DraNet实现展示了网络接口（包括远程直接内存访问RDMA设备）的声明式连接能力，显著提升了高性能AI/ML工作负载的效率。这使得复杂的云原生应用成为可能。

Conclusion: KNDs为未来的电信解决方案奠定了重要基础，有助于构建一个由专业化KNDs组成的“星系”，从而增强应用交付能力并降低运维复杂性，有效解决了当前配置和API的局限。

Abstract: Traditional Kubernetes networking struggles to meet the escalating demands of
AI/ML and evolving Telco infrastructure. This paper introduces Kubernetes
Network Drivers (KNDs), a transformative, modular, and declarative architecture
designed to overcome current imperative provisioning and API limitations. KNDs
integrate network resource management into Kubernetes' core by utilizing
Dynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements,
and upcoming OCI Runtime Specification changes. Our DraNet implementation
demonstrates declarative attachment of network interfaces, including Remote
Direct Memory Access (RDMA) devices, significantly boosting high-performance
AI/ML workloads. This capability enables sophisticated cloud-native
applications and lays crucial groundwork for future Telco solutions, fostering
a "galaxy" of specialized KNDs for enhanced application delivery and reduced
operational complexity.

</details>


### [233] [Geminet: Learning the Duality-based Iterative Process for Lightweight Traffic Engineering in Changing Topologies](https://arxiv.org/abs/2506.23640)
*Ximeng Liu,Shizhen Zhao,Xinbing Wang*

Main category: cs.NI

TL;DR: Geminet是一个轻量级、可扩展的ML流量工程框架，通过解耦拓扑与神经网络并优化边级变量，解决了现有方案在拓扑变化处理和可扩展性方面的局限。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的流量工程（ML-based TE）方案不切实际，它们要么无法处理拓扑变化，要么因计算和内存开销过大而可扩展性差。

Method: 本文提出了Geminet框架，其核心在于：1) 通过学习基于梯度下降的迭代调整过程，将神经网络与拓扑解耦；2) 将优化从路径级路由权重转向边级对偶变量，以减少内存消耗。

Result: Geminet显著提升了可扩展性，神经网络规模仅为现有方案的0.04%至7%。它能有效处理拓扑变化，性能与HARP相当。在大规模拓扑下，Geminet内存消耗低于10 GiB（比HARP少8倍以上），收敛速度快5.45倍。

Conclusion: Geminet的轻量化和高性能特性，展示了其在大规模流量工程部署中的巨大潜力。

Abstract: Recently, researchers have explored ML-based Traffic Engineering (TE),
leveraging neural networks to solve TE problems traditionally addressed by
optimization. However, existing ML-based TE schemes remain impractical: they
either fail to handle topology changes or suffer from poor scalability due to
excessive computational and memory overhead. To overcome these limitations, we
propose Geminet, a lightweight and scalable ML-based TE framework that can
handle changing topologies. Geminet is built upon two key insights: (i) a
methodology that decouples neural networks from topology by learning an
iterative gradient-descent-based adjustment process, as the update rule of
gradient descent is topology-agnostic, relying only on a few gradient-related
quantities; (ii) shifting optimization from path-level routing weights to
edge-level dual variables, reducing memory consumption by leveraging the fact
that edges are far fewer than paths. Evaluations on WAN and data center
datasets show that Geminet significantly improves scalability. Its neural
network size is only 0.04% to 7% of existing schemes, while handling topology
variations as effectively as HARP, a state-of-the-art ML-based TE approach,
without performance degradation. When trained on large-scale topologies,
Geminet consumes under 10 GiB of memory, more than eight times less than the
80-plus GiB required by HARP, while achieving 5.45 times faster convergence
speed, demonstrating its potential for large-scale deployment.

</details>


### [234] [Campus5G: A Campus Scale Private 5G Open RAN Testbed](https://arxiv.org/abs/2506.23740)
*Andrew E. Ferguson,Ujjwal Pawar,Tianxin Wang,Mahesh K. Marina*

Main category: cs.NI

TL;DR: 本文介绍并分析了在爱丁堡大学部署的Campus5G——一个校园级O-RAN兼容私有5G试验平台的规划、架构、部署及性能测量过程，并总结了经验教训和研究机会。


<details>
  <summary>Details</summary>
Motivation: 移动网络正趋向解耦化，Open RAN是行业趋势。私有5G网络因其环境、高控制度和创新机会，被认为是Open RAN的理想早期采用者。

Method: 部署了Campus5G，这是首个校园级O-RAN兼容私有5G试验平台。详细介绍了从规划、架构设计到部署以及性能测量的全过程。

Result: 成功部署了Campus5G试验平台，并对其性能进行了测量。从构建和部署过程中获得了宝贵的经验教训。

Conclusion: 成功构建了O-RAN兼容的私有5G试验平台，为Open RAN在私有5G网络中的实际应用提供了实践基础，并识别了未来潜在的研究机会。

Abstract: Mobile networks are embracing disaggregation, reflected by the industry trend
towards Open RAN. Private 5G networks are viewed as particularly suitable
contenders as early adopters of Open RAN, owing to their setting, high degree
of control, and opportunity for innovation they present. Motivated by this, we
have recently deployed Campus5G, the first of its kind campus-wide,
O-RAN-compliant private 5G testbed across the central campus of the University
of Edinburgh. We present in detail our process developing the testbed, from
planning, to architecting, to deployment, and measuring the testbed
performance. We then discuss the lessons learned from building the testbed, and
highlight some research opportunities that emerged from our deployment
experience.

</details>


### [235] [How Long Can I Transmit? A Mobility Aware mmWave-based UAV Communication Framework](https://arxiv.org/abs/2506.23755)
*Shawon Mitra,Subhojit Sarkar,Sasthi C. Ghosh*

Main category: cs.NI

TL;DR: 本文分析了城市环境中固定无人机与移动地面用户之间毫米波通信的预期视距（LoS）时长，并提出了一种基于该分析的无人机用户关联算法，该算法优于不考虑用户移动性的现有方案。


<details>
  <summary>Details</summary>
Motivation: 毫米波通信虽然能提供高数据速率，但极易受障碍物衰减。无人机因其额外的自由度，可提供视距传输路径来弥补此限制。然而，现有工作缺乏针对移动地面用户的无人机-地面用户视距概率的分析框架。

Method: 采用曼哈顿点线过程（MPLP）模型模拟城市环境；推导了固定无人机与沿道路移动的地面用户之间预期视距时长的表达式，并通过仿真验证；提出了一种贪婪用户关联算法，依据最大预期视距时长将无人机分配给用户。

Result: 成功推导了固定无人机与移动地面用户之间预期视距时长的表达式，并得到了仿真验证；提出的用户关联算法在性能上优于不考虑用户移动性的现有基准方案（如分配给最近的视距无人机）。

Conclusion: 本文提出的分析框架和用户关联算法能够有效利用无人机在毫米波网络中为移动地面用户提供更优的视距通信，提升网络性能。

Abstract: One primary focus of next generation wireless communication networks is the
millimeterwave (mmWave) spectrum, typically considered in the 30 GHz to 300 GHz
frequency range. Despite their promise of high data rates, mmWaves suffer from
severe attenuation while passing through obstacles. Unmanned aerial vehicles
(UAVs) have been proposed to offset this limitation on account of their
additional degrees of freedom, which can be leveraged to provide line of sight
(LoS) transmission paths. While some prior works have proposed analytical
frameworks to compute the LoS probability for static ground users and a UAV,
the same is lacking for mobile users on the ground. In this paper, we consider
the popular Manhattan point line process (MPLP) to model an urban environment,
within which a ground user moves with a known velocity for a small time
interval along the roads. We derive an expression for the expected duration of
LoS between a static UAV in the air and a mobile ground user, and validate the
same through simulations. To demonstrate the efficacy of the proposed analysis,
we propose a simple user association algorithm that greedily assigns the UAVs
to users with the highest expected LoS time, and show that it outperforms the
existing benchmark schemes that assign the users to the nearest UAVs with LoS
without considering the user mobility.

</details>


### [236] [Learning Constraints Directly from Network Data](https://arxiv.org/abs/2506.23964)
*Hongyu Hè,Minhao Jin,Maria Apostolaki*

Main category: cs.NI

TL;DR: NetNomos是一种从网络测量数据中高效且准确地学习命题逻辑规则的新方法，克服了现有手动或纯机器学习方法的局限性，并在多个实际应用场景中展现出卓越性能，包括合成数据评估、异常检测和遥测推断。


<details>
  <summary>Details</summary>
Motivation: 网络数据规则的形式化（作为逻辑约束）对提高合成数据质量、降低机器学习模型脆弱性及增强网络测量语义理解至关重要。然而，现有规则提取方法（手动或仅依赖机器学习）通常不完整、不可靠或不准确，使得这些潜在益处难以实现。

Method: 本文将规则提取问题公式化为约束建模问题，并引入了NetNomos。NetNomos直接从原始网络测量中学习命题逻辑约束。为应对数据规模大、学习复杂性高、被动环境和缺乏真实标签等挑战，NetNomos采用了一种基于格的搜索方法，该方法根据约束的特异性和简洁性进行结构化，从而将学习复杂度从超二次方降低到对数级别，并能在组合搜索空间中高效遍历。

Result: 在多样化网络数据集上的评估表明，NetNomos能在三小时内学习所有基准规则（包括仅关联0.01%数据点的规则）。相比之下，基线方法发现的规则不到25%，且需数天运行时间。通过三个案例研究，NetNomos证明其能：(i) 发现所有七种合成流量生成器输出中的规则违反，可用于评估和指导生成过程；(ii) 检测流量中的语义差异，可用于异常检测；以及 (iii) 自动发现用于遥测推断的规则，从而支持通过推理进行监控。

Conclusion: NetNomos提供了一种高效、准确的网络规则自动提取方案，显著优于传统方法。其学习到的规则具有广泛的实际应用价值，可有效用于评估和指导合成数据生成、实现网络异常检测，以及支持基于推理的网络监控，从而深化对网络行为的理解和管理。

Abstract: Network data conforms to a wide range of rules that arise from protocols,
design principles, and deployment decisions (e.g., a packet's queuing delay
must be less than its end-to-end delay). Formalizing such rules as logic
constraints can (i) improve the quality of synthetic data, (ii) reduce the
brittleness of machine learning (ML) models, and (iii) improve semantic
understanding of network measurements. However, these benefits remain out of
reach if rule extraction is manual or solely reliant on ML, as both approaches
yield incomplete, unreliable, and/or inaccurate rules.
  This paper formulates rule extraction as a constraint modeling problem and
introduces NetNomos that learns propositional logic constraints directly from
raw network measurements. Constraint modeling in this domain is uniquely
challenging due to the scale of the data, the inherent learning complexity and
passive environment, and the lack of ground truth supervision. NetNomos
addresses these challenges via a lattice-based search structured by constraint
specificity and succinctness. Our approach reduces learning complexity from
superquadratic to logarithmic and enables efficient traversal in combinatorial
search space.
  Our evaluations on diverse network datasets show that NetNomos learns all
benchmark rules, including those associated with as little as 0.01% of data
points, in under three hours. In contrast, baseline methods discover less than
25% of the rules and require several days to run. Through three case studies,
we show that: NetNomos (i) finds rule violations in the outputs of all seven
synthetic traffic generators, hence can be used to assess and guide their
generation process; (ii) detects semantic differences in traffic, hence can be
used for anomaly detection; and (iii) automatically finds rules used for
telemetry imputation, hence can support monitoring through inference.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [237] [Attention acts to suppress goal-based conflict under high competition](https://arxiv.org/abs/1610.09431)
*Omar Claflin*

Main category: q-bio.NC

TL;DR: 研究发现在高视觉竞争条件下，自上而下的注意力会在刺激出现100毫秒内非选择性地抑制任务相关和无关的神经信号，以减少无关刺激的前馈信号，这与此前在低竞争条件下观察到的选择性增强作用不同。


<details>
  <summary>Details</summary>
Motivation: 以往关于自上而下注意力选择性增强任务相关神经信号的研究，仅在视觉注意力竞争最小的条件下进行过测试，缺乏在高竞争环境下的了解。

Method: 研究在高竞争条件下（即共享感受野内存在两个具有相反调节目标的刺激）测试了自上而下注意力对神经信号的影响。

Result: 在高竞争条件下，自上而下的注意力在刺激出现100毫秒内同时抑制了任务相关和不相关的神经信号。

Conclusion: 自上而下注意力资源的这种非选择性参与，旨在减少代表无关刺激的前馈信号。

Abstract: It is known that when multiple stimuli are present, top-down attention
selectively enhances the neural signal in the visual cortex for task-relevant
stimuli, but this has been tested only under conditions of minimal competition
of visual attention. Here we show during high competition, that is, two stimuli
in a shared receptive field possessing opposing modulatory goals, top-down
attention suppresses both task-relevant and irrelevant neural signals within
100 ms of stimuli onset. This non-selective engagement of top-down attentional
resources serves to reduce the feedforward signal representing irrelevant
stimuli.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [238] [All Proof of Work But No Proof of Play](https://arxiv.org/abs/2506.23435)
*Hayder Tirmazi*

Main category: cs.CR

TL;DR: 本文尝试构建密码学系统以验证Speedrun的真实性，并记录了失败尝试，揭示在不可信环境中认证实时互动输入的困难及相关技术局限性。


<details>
  <summary>Details</summary>
Motivation: 传统Speedrun验证方法（人工观察、音频分析、游戏机制分析）效率低下、易出错且不缺乏密码学安全性。研究旨在通过构建密码学系统解决Speedrun真实性验证的开放性问题。

Method: 研究方法是尝试设计并实现一个能够密码学证明Speedrun真实性的系统。通过描述所尝试的解决方案及其被规避的案例（即失败的叙述），来探讨该问题的复杂性。

Result: 未能成功构建一个能够完全密码学验证Speedrun真实性的系统。研究结果通过记录失败，展示了在不可信环境中认证实时互动人类输入的巨大困难，以及签名方案、游戏完整性和可证明游玩等技术的局限性。

Conclusion: 在不可信环境中，对实时和互动的人类输入进行可靠的真实性验证是极其困难的，且现有密码学签名方案、游戏完整性保护及可证明游玩等技术存在固有限制，难以完全解决Speedrun真实性验证的挑战。

Abstract: Speedrunning is a competition that emerged from communities of early video
games such as Doom (1993). Speedrunners try to finish a game in minimal time.
Provably verifying the authenticity of submitted speedruns is an open problem.
Traditionally, best-effort speedrun verification is conducted by on-site human
observers, forensic audio analysis, or a rigorous mathematical analysis of the
game mechanics. Such methods are tedious, fallible, and, perhaps worst of all,
not cryptographic. Motivated by naivety and the Dunning-Kruger effect, we
attempt to build a system that cryptographically proves the authenticity of
speedruns. This paper describes our attempted solutions and ways to circumvent
them. Through a narration of our failures, we attempt to demonstrate the
difficulty of authenticating live and interactive human input in untrusted
environments, as well as the limits of signature schemes, game integrity, and
provable play.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [239] [Performance Measurements in the AI-Centric Computing Continuum Systems](https://arxiv.org/abs/2506.22884)
*Praveen Kumar Donta,Qiyang Zhang,Schahram Dustdar*

Main category: cs.DC

TL;DR: 本文回顾了分布式计算连续体（DCC）和物联网（IoT）环境中的性能指标，并讨论了新兴指标如可持续性、能效和系统可观测性，以适应生成式AI带来的新计算需求，旨在指导未来研究。


<details>
  <summary>Details</summary>
Motivation: 计算范式已转向分布式计算连续体（DCC），包含云、边缘、物联网和移动平台。生成式AI和大型语言模型的出现极大地增加了计算资源需求。传统性能指标已不足以应对不断变化的计算需求和应用要求，因此需要对其进行重新审视和扩展，以支持系统效率提升和目标实现。

Method: 1. 回顾DCC和IoT环境中常用的性能指标。2. 讨论新兴的性能维度，如可持续性、能源效率和系统可观测性，以应对不断发展的计算需求。3. 概述选择适当指标的标准和考量因素。

Result: 文章审查了DCC和IoT环境中常用的性能指标，并提出了可持续性、能效和系统可观测性等新兴性能维度，以适应AI时代的需求。此外，文章还提供了选择合适指标的判据和考量因素。

Conclusion: 为应对DCC和IoT环境下，特别是生成式AI和大型语言模型带来的巨大计算需求，性能指标需要被重新定义和扩展。本文通过回顾现有指标、引入新兴维度并提供选择标准，旨在激发该关键领域的未来研究与发展。

Abstract: Over the Eight decades, computing paradigms have shifted from large,
centralized systems to compact, distributed architectures, leading to the rise
of the Distributed Computing Continuum (DCC). In this model, multiple layers
such as cloud, edge, Internet of Things (IoT), and mobile platforms work
together to support a wide range of applications. Recently, the emergence of
Generative AI and large language models has further intensified the demand for
computational resources across this continuum. Although traditional performance
metrics have provided a solid foundation, they need to be revisited and
expanded to keep pace with changing computational demands and application
requirements. Accurate performance measurements benefit both system designers
and users by supporting improvements in efficiency and promoting alignment with
system goals. In this context, we review commonly used metrics in DCC and IoT
environments. We also discuss emerging performance dimensions that address
evolving computing needs, such as sustainability, energy efficiency, and system
observability. We also outline criteria and considerations for selecting
appropriate metrics, aiming to inspire future research and development in this
critical area.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [240] [Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling](https://arxiv.org/abs/2504.15071)
*Louis Bradshaw,Simon Colton*

Main category: cs.SD

TL;DR: 引入一个由钢琴音频转录而来的百万级MIDI文件数据集，并详述其构建方法。


<details>
  <summary>Details</summary>
Motivation: 创建一个大规模、高质量的MIDI数据集，以支持相关研究和应用。

Method: 采用多阶段数据管道，包括使用语言模型基于元数据进行网络音频抓取和评分，以及使用音频分类器进行剪枝和分割。

Result: 成功构建了一个包含超过一百万个独特MIDI文件、约10万小时转录音频的庞大数据集。

Conclusion: 该研究详细分析了数据集构建技术，提供了统计见解和元数据，并公开了数据集供研究使用。

Abstract: We introduce an extensive new dataset of MIDI files, created by transcribing
audio recordings of piano performances into their constituent notes. The data
pipeline we use is multi-stage, employing a language model to autonomously
crawl and score audio recordings from the internet based on their metadata,
followed by a stage of pruning and segmentation using an audio classifier. The
resulting dataset contains over one million distinct MIDI files, comprising
roughly 100,000 hours of transcribed audio. We provide an in-depth analysis of
our techniques, offering statistical insights, and investigate the content by
extracting metadata tags, which we also provide. Dataset available at
https://github.com/loubbrad/aria-midi.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [241] [Continual Learning for Wireless Channel Prediction](https://arxiv.org/abs/2506.22471)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ali Jamshed,John M. Cioffi*

Main category: eess.SP

TL;DR: 针对5G/6G跨配置切换导致的信道预测误差大问题，本文将其框定为持续学习问题，并测试了多种适应性学习方法，显著提升了预测精度，为未来部署提供了解决方案。


<details>
  <summary>Details</summary>
Motivation: 现代5G/6G网络在用户跨配置小区切换时（天线布局、频率、散射统计不同），信道预测的均方误差（NMSE）平均增加37.5%，传统的朴素微调方法效果不佳，急需提升预测鲁棒性。

Method: 将跨配置切换下的信道预测失配问题定义为持续学习问题，并评估了三种适应性学习方法族：1) 结合损失感知存储的回放机制；2) 基于突触重要性的正则化；3) 无记忆的“不遗忘学习”（蒸馏）。

Result: 在三个3GPP城市微蜂窝场景下，最佳的回放和正则化方案在高信噪比下将误差底线降低了高达2dB（约35%）；即使是轻量级的蒸馏方法也比基线切换预测方案提升了高达30%。

Conclusion: 研究结果表明，有针对性的回放和参数锚定对于实现切换鲁棒的信道状态信息（CSI）预测至关重要，并为在3GPP-NR和O-RAN中现有信道预测工作中嵌入持续学习提供了明确的路径。

Abstract: Modern 5G/6G deployments routinely face cross-configuration handovers--users
traversing cells with different antenna layouts, carrier frequencies, and
scattering statistics--which inflate channel-prediction NMSE by $37.5\%$ on
average when models are naively fine-tuned. The proposed improvement frames
this mismatch as a continual-learning problem and benchmarks three adaptation
families: replay with loss-aware reservoirs, synaptic-importance
regularization, and memory-free learning-without-forgetting. Across three
representative 3GPP urban micro scenarios, the best replay and regularization
schemes cut the high-SNR error floor by up to 2~dB ($\approx 35\%$), while even
the lightweight distillation recovers up to $30\%$ improvement over baseline
handover prediction schemes. These results show that targeted rehearsal and
parameter anchoring are essential for handover-robust CSI prediction and
suggest a clear migration path for embedding continual-learning hooks into
current channel prediction efforts in 3GPP--NR and O-RAN. The full codebase can
be found at
https://github.com/ahmd-mohsin/continual-learning-channel-prediction.git.

</details>


### [242] [Coexistence analysis of Wi-Fi 6E and 5G NR-U in the 6 GHz band](https://arxiv.org/abs/2506.22844)
*Navid Keshtiarast,Marina Petrova*

Main category: eess.SP

TL;DR: 研究6GHz非授权频谱中Wi-Fi 6E与5G NR-U共存问题，通过模拟分析其下行吞吐量及关键参数对公平共存的影响。


<details>
  <summary>Details</summary>
Motivation: 随着宽带和物联网需求的增长，监管机构开放6GHz非授权频谱供Wi-Fi和5G NR-U使用。为支持QoS敏感应用，确保这两种技术以及与现有用户之间的公平高效共存至关重要。

Method: 在密集住宅、高干扰场景下，通过大量模拟研究Wi-Fi 6E AP和5G NR-U gNB的平均下行吞吐量。同时，探讨MAC帧聚合、能量检测阈值和最大信道占用时间（MCOT）等参数设置对共存性能的影响。

Result: 研究结果提供了关于如何调整关键参数以设计公平共存策略的重要见解。

Conclusion: 本研究为在6GHz非授权频谱中设计Wi-Fi 6E和5G NR-U的公平共存策略提供了参数调整方面的指导。

Abstract: The ever-increasing demand for broadband and IoT wireless connectivity has
recently urged the regulators around the world to start opening the 6 GHz
spectrum for unlicensed use. These bands will, for example, permit the use of
additional 1.2 GHz in the US and 500 MHz in Europe for unlicensed radio access
technologies (RATs) such as Wi-Fi and 5G New Radio Unlicensed (5G NR-U). To
support QoS-sensitive applications with both technologies, fair and efficient
coexistence approaches between the two RATs, as well as with incumbents already
operating in the 6 GHz band, are crucial. In this paper, we study through
extensive simulations the achievable mean downlink throughput of both Wi-Fi 6E
APs and 5G NR-U gNBs when they are co-deployed in a dense residential scenario
under high-interference conditions. We also explore how different parameter
settings e.g., MAC frame aggregation, energy detection threshold and maximum
channel occupancy time (MCOT) affect the coexistence. Our findings give
important insights into how to tune the key parameters to design fair
coexistence policies.

</details>


### [243] [E-WAN: Efficient Communication in Energy Harvesting Low-Power Networks](https://arxiv.org/abs/2506.23788)
*Naomi Stricker,David Blaser,Andres Gomez,Lothar Thiele*

Main category: eess.SP

TL;DR: E-WAN是一种用于能量收集网络的协议，它能根据能量和网络状态智能切换多跳和单跳通信，以优化效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 分布式嵌入式系统（IoT/WSN/CPS）的无线通信面临多跳与单跳的能效与简单性权衡。同时，能量收集技术的有限和多变性对网络通信构成重大挑战，亟需一种能适应能量收集约束并优化通信方式的协议。

Method: 本文提出E-WAN协议，专为能量收集广域低功耗网络设计。该协议基于“虚拟子网络”概念，使节点能自主、动态地在资源高效的多跳通信（可行时）和可靠但耗能的点对点通信（否则）之间切换，仅依赖易于获取的网络状态信息进行调整。

Result: E-WAN在多种通信和能量收集场景下，均展现出显著的效率和适应性优势。此外，该协议已在真实室内环境中成功部署，验证了其在实际设置下的运行能力。

Conclusion: E-WAN通过根据实时网络和能量条件智能地利用多跳和单跳通信，为能量收集网络提供了一个自适应且高效的解决方案，从而延长了网络寿命并确保了数据交换的可靠性。

Abstract: The ever-increasing number of distributed embedded systems in the context of
the Internet of Things (IoT), Wireless Sensor Networks (WSN), and
Cyber-Physical Systems (CPS) rely on wireless communication to collect and
exchange data. Nodes can employ single-hop communication which, despite its
ease, may necessitate energy-intensive long-range communication to cover long
distances. Conversely, multi-hop communication allows for more energy-efficient
short-range communication since nodes can rely on other nodes to forward their
data. Yet, this approach requires relay nodes to be available and continuous
maintenance of a dynamically changing distributed state. At the same time,
energy harvesting has the potential to outperform traditional battery-based
systems by improving their lifetime, scalability with lower maintenance costs,
and environmental impact. However, the limited and temporally and spatially
variable harvested energy poses significant challenges for networking in energy
harvesting networks, particularly considering the energy demands and
characteristics of both multi-hop and single-hop communication. We propose
E-WAN, a protocol for energy harvesting wide-area low-power networks that
builds on the concept of \emph{virtual sub-networks} to enable
resource-efficient multi-hop communication when possible and reliable however
energy-intensive point-to-point communication otherwise. Nodes autonomously and
dynamically move between the two and adjust to changing network states and
resources based only on easily obtainable network state information. We
illustrate E-WAN's advantages both in terms of efficiency and adaptability in
various communication and harvesting scenarios. Furthermore, we demonstrate
E-WAN operating in a realistic setting by deploying an energy harvesting
network in a real-world indoor environment.

</details>


### [244] [Unsupervised Learning-Based Joint Resource Allocation and Beamforming Design for RIS-Assisted MISO-OFDMA Systems](https://arxiv.org/abs/2506.22448)
*Yu Ma,Xingyu Zhou,Xiao Li,Le Liang,Shi Jin*

Main category: eess.SP

TL;DR: 针对RIS辅助MISO-OFDMA系统的资源分配挑战，提出一种两阶段无监督学习框架，显著提升计算效率并保持接近最优的性能。


<details>
  <summary>Details</summary>
Motivation: RIS作为6G关键技术，在RIS辅助的MISO-OFDMA系统中，联合优化RIS相移、基站波束成形和资源块分配存在复杂的资源分配挑战。

Method: 提出一个两阶段无监督学习框架：第一阶段，BeamNet根据信道状态信息(CSI)预测RIS相移；第二阶段，AllocationNet利用BeamNet输出的等效CSI分配资源块。同时，采用最大比率传输和注水算法实现主动波束成形。为处理离散约束并确保可微性，引入了量化和Gumbel-softmax技巧，并通过定制损失函数和分阶段训练进一步优化性能。

Result: 仿真结果表明，所提方法在总和速率上达到SCA基线的99.93%，而运行时间仅为SCA的0.036%。此外，该方法在不同信道和用户条件下均表现出良好的鲁棒性。

Conclusion: 所提出的无监督学习框架能有效解决RIS辅助MISO-OFDMA系统中的资源分配问题，在大幅降低计算复杂度的同时，依然能保持接近最优的性能，展现了其在实际应用中的巨大潜力。

Abstract: Reconfigurable intelligent surfaces (RIS) are key enablers for 6G wireless
systems. This paper studies downlink transmission in an RIS-assisted MISO-OFDMA
system, addressing resource allocation challenges. A two-stage unsupervised
learning-based framework is proposed to jointly design RIS phase shifts, BS
beamforming, and resource block (RB) allocation. The framework includes
BeamNet, which predicts RIS phase shifts from CSI, and AllocationNet, which
allocates RBs using equivalent CSI derived from BeamNet outputs. Active
beamforming is implemented via maximum ratio transmission and water-filling. To
handle discrete constraints while ensuring differentiability, quantization and
the Gumbel-softmax trick are adopted. A customized loss and phased training
enhance performance under QoS constraints. Simulations show the method achieves
99.93% of the sum rate of the SCA baseline with only 0.036% of its runtime, and
it remains robust across varying channel and user conditions.

</details>
