<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.CV](#cs.CV) [Total: 9]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.CR](#cs.CR) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [OpenStaxQA: A multilingual dataset based on open-source college textbooks](https://arxiv.org/abs/2510.06239)
*Pranav Gupta*

Main category: cs.CL

TL;DR: 提出OpenStaxQA，一个基于43本多语言大学教材的教育评估基准，用于微调和评估大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 为大学级别教育应用创建一个专门的评估基准，并考察模型在此基准上微调后对其他任务的泛化能力。

Method: 构建了OpenStaxQA数据集，包含来自英语、西班牙语和波兰语的43本开源大学教材；使用量化低秩适配器（QLoRa）对约70亿参数的大型语言模型进行微调和评估；在AI2推理挑战开发集上进行零样本评估以测试泛化能力。

Result: 摘要中未提供具体的大型语言模型评估结果或泛化性能数据。

Conclusion: OpenStaxQA为大学级别的教育应用提供了一个新的多语言评估基准，并有助于研究大型语言模型在该领域的表现及其更广泛的影响。

Abstract: We present OpenStaxQA, an evaluation benchmark specific to college-level
educational applications based on 43 open-source college textbooks in English,
Spanish, and Polish, available under a permissive Creative Commons license. We
finetune and evaluate large language models (LLMs) with approximately 7 billion
parameters on this dataset using quantized low rank adapters (QLoRa).
Additionally we also perform a zero-shot evaluation on the AI2 reasoning
challenge dev dataset in order to check if OpenStaxQA can lead to an improved
performance on other tasks. We also discuss broader impacts relevant to
datasets such as OpenStaxQA.

</details>


### [2] [Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets](https://arxiv.org/abs/2510.06240)
*Jiqun Pan,Zhenke Duan,Jiani Tu,Anzhi Cheng,Yanqing Wang*

Main category: cs.CL

TL;DR: 针对工业QA系统的安全性和可靠性挑战，提出KG-MASD方法，结合知识图谱和多智能体蒸馏，提升模型精度和可信赖性，实现紧凑模型部署。


<details>
  <summary>Details</summary>
Motivation: 工业QA系统（如设备故障诊断）在安全关键场景中对可靠性要求高，现有基于多智能体大语言模型的方法存在迭代失控和输出不可验证问题；传统蒸馏方法难以将协作推理能力有效迁移到轻量级模型。

Method: 提出知识图谱引导的多智能体系统蒸馏（KG-MASD）。将蒸馏过程建模为马尔可夫决策过程，并利用知识图谱作为可验证的结构化先验，丰富状态表示并确保收敛。通过整合协作推理和知识 grounding，生成高置信度指令微调数据，将推理深度和可验证性蒸馏到紧凑的学生模型中。

Result: 在工业QA数据集上，KG-MASD的准确率相对于基线提高了2.4%至20.1%，并显著提升了系统可靠性。

Conclusion: KG-MASD有效解决了工业QA系统在安全关键场景中的可靠性问题，实现了可信赖AI的部署，提高了工业QA的准确性和可验证性。

Abstract: Industrial question-answering (QA) systems require higher safety and
reliability than general-purpose dialogue models, as errors in high-risk
scenarios such as equipment fault diagnosis can have severe consequences.
Although multi-agent large language models enhance reasoning depth, they suffer
from uncontrolled iterations and unverifiable outputs, and conventional
distillation methods struggle to transfer collaborative reasoning capabilities
to lightweight, deployable student models. To address these challenges, we
propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our
approach formulates distillation as a Markov Decision Process and incorporates
a knowledge graph as a verifiable structured prior to enrich state
representation and ensure convergence. By integrating collaborative reasoning
with knowledge grounding, KG-MASD generates high-confidence instruction-tuning
data and jointly distills reasoning depth and verifiability into compact
student models suitable for edge deployment. Experiments on an industrial QA
dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent
over baselines and significantly enhances reliability, enabling trustworthy AI
deployment in safety-critical industrial scenarios. Code and data are available
at https://github.com/erwinmsmith/KG-MAD/.

</details>


### [3] [Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses](https://arxiv.org/abs/2510.06242)
*Subin An,Yugyeong Ji,Junyoung Kim,Heejin Kook,Yang Lu,Josh Seltzer*

Main category: cs.CL

TL;DR: 本研究提出了一个两阶段评估框架，利用LLM能力有效评估人类撰写的开放式调查问卷答复质量，克服了现有方法的局限性，并在实践中表现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 开放式调查问卷答复虽能提供宝贵见解，但低质量回复会增加人工筛选负担并可能导致误导性结论，因此需要有效的评估方法。现有自动评估方法主要针对LLM生成文本，无法充分评估人类撰写的、具有独特特征的回复。

Method: 我们提出了一个专门针对人类调查答复的两阶段评估框架：首先进行乱码过滤以去除无意义回复；然后，基于真实调查数据的实证分析，利用LLM能力从努力程度、相关性和完整性三个维度评估回复质量。

Result: 在英语和韩语数据集上的验证表明，该框架不仅优于现有评估指标，而且在回复质量预测和回复拒绝等实际应用中表现出高实用性，并与专家评估结果高度相关。

Conclusion: 该两阶段评估框架能够有效且准确地评估人类撰写的开放式调查问卷答复的质量，克服了现有方法的局限性，为市场研究中答复质量控制提供了强大的实用工具。

Abstract: Open-ended survey responses provide valuable insights in marketing research,
but low-quality responses not only burden researchers with manual filtering but
also risk leading to misleading conclusions, underscoring the need for
effective evaluation. Existing automatic evaluation methods target
LLM-generated text and inadequately assess human-written responses with their
distinct characteristics. To address such characteristics, we propose a
two-stage evaluation framework specifically designed for human survey
responses. First, gibberish filtering removes nonsensical responses. Then,
three dimensions-effort, relevance, and completeness-are evaluated using LLM
capabilities, grounded in empirical analysis of real-world survey data.
Validation on English and Korean datasets shows that our framework not only
outperforms existing metrics but also demonstrates high practical applicability
for real-world applications such as response quality prediction and response
rejection, showing strong correlations with expert assessment.

</details>


### [4] [CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning](https://arxiv.org/abs/2510.06243)
*Qihua Dong,Luis Figueroa,Handong Zhao,Kushal Kafle,Jason Kuen,Zhihong Ding,Scott Cohen,Yun Fu*

Main category: cs.CL

TL;DR: 本文提出CoT Referring策略，通过结构化链式思考训练数据和统一的MLLM框架，显著提升了多模态大语言模型在复杂指代表达理解和分割任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 指代表达理解与分割是评估多模态大语言模型（MLLMs）语言与图像整合能力的关键任务。当前在复杂查询场景下模型推理能力仍面临挑战。

Method: 提出CoT Referring策略，通过结构化的链式思考训练数据结构增强模型跨模态推理。该方法将文本结构系统解析为顺序指代步骤，识别关系并确保引用一致性。重构训练数据，提供新标注，并从现有资源编译了专门针对复杂指代情况的评估基准。此外，将检测和分割能力整合到统一的MLLM框架中，并采用新颖的自适应加权损失进行训练。

Result: 在自建基准和RefCOCO/+/g数据集上的实验结果表明，所提方法有效，相比基线模型性能提升2.5%以上。

Conclusion: CoT Referring策略通过增强模型的跨模态推理能力和统一的检测分割框架，显著提升了多模态大语言模型在指代表达理解与分割任务上的表现，尤其在复杂查询场景中效果显著。

Abstract: Referring Expression Comprehension and Segmentation are critical tasks for
assessing the integration of language understanding and image comprehension,
serving as benchmarks for Multimodal Large Language Models (MLLMs)
capabilities. To address these challenges, we propose a new strategy, CoT
Referring, which enhances model reasoning across modalities through a
structured, chain-of-thought training data structure. Our approach
systematically parses textual structures to a sequential referring step, where
in each step it identifies relationships and ensures consistent reference
alignment, thereby improving accuracy in complex query scenarios. We
restructure the training data to enforce a new output form, providing new
annotations for existing datasets and compiling an evaluation benchmark from
existing resources. This benchmark is designed explicitly for complex referring
cases. We also integrate detection and segmentation capabilities into a unified
MLLM framework, training it with a novel adaptive weighted loss to optimize
performance. Experimental results on our curated benchmark and RefCOCO/+/g
demonstrate the effectiveness of our approach, with a notable increase of 2.5%+
over baseline models.

</details>


### [5] [Evaluating Embedding Frameworks for Scientific Domain](https://arxiv.org/abs/2510.06244)
*Nouman Ahmed,Ronin Wu,Victor Botev*

Main category: cs.CL

TL;DR: 本研究旨在为科学领域寻找最优词表示和分词方法，并构建一个全面的评估套件以测试和比较各种算法。


<details>
  <summary>Details</summary>
Motivation: 领域特定数据中词汇含义多变，需要最优词表示。尽管生成式AI和Transformer模型能生成语境化嵌入，但从头预训练成本高昂。因此，需要在科学领域找到高效且优化的词表示和分词方法。

Method: 构建一个包含多个下游任务及相应数据集的综合评估套件，并利用此套件测试各种词表示和分词算法。

Result: 摘要中未提供具体的研究结果。

Conclusion: 摘要主要阐述研究动机、目标和方法，尚未得出研究结论。

Abstract: Finding an optimal word representation algorithm is particularly important in
terms of domain specific data, as the same word can have different meanings and
hence, different representations depending on the domain and context. While
Generative AI and transformer architecture does a great job at generating
contextualized embeddings for any given work, they are quite time and compute
extensive, especially if we were to pre-train such a model from scratch. In
this work, we focus on the scientific domain and finding the optimal word
representation algorithm along with the tokenization method that could be used
to represent words in the scientific domain. The goal of this research is two
fold: 1) finding the optimal word representation and tokenization methods that
can be used in downstream scientific domain NLP tasks, and 2) building a
comprehensive evaluation suite that could be used to evaluate various word
representation and tokenization algorithms (even as new ones are introduced) in
the scientific domain. To this end, we build an evaluation suite consisting of
several downstream tasks and relevant datasets for each task. Furthermore, we
use the constructed evaluation suite to test various word representation and
tokenization algorithms.

</details>


### [6] [TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B](https://arxiv.org/abs/2510.06249)
*Toshiki Nakai,Ravi Kiran Chikkala,Lena Sophie Oberkircher,Nicholas Jennings,Natalia Skachkova,Tatiana Anikina,Jesujoba Oluwadara Alabi*

Main category: cs.CL

TL;DR: 本研究提出TRepLiNa（结合CKA与REPINA）方法，通过对多语言LLM（Aya-23 8B）内部层进行跨语言相似性对齐，有效提升了低资源语言（LRL）的翻译质量，尤其在数据稀缺环境下表现出低成本和实用性。


<details>
  <summary>Details</summary>
Motivation: 为了解决印度低资源语言（LRLs）的资源匮乏问题，并探究通过在解码器-only多语言大语言模型（LLM）特定内部层中强制执行跨语言相似性，是否能提高从LRL到高资源语言（HRL）的翻译质量。

Method: 开发了一种名为TRepLiNa的联合方法，该方法结合了旨在鼓励不同语言表示对齐的相似性度量CKA（Centered Kernel Alignment）和限制参数更新靠近预训练模型的正则化方法REPINA。实验在零样本、少样本和微调设置下进行，使用Aya-23 8B模型结合QLoRA技术，针对MMLoSo共享任务的语言对（蒙达里语、桑塔利语、比里语）与印地语/英语支点进行。

Result: 研究结果表明，使用TRepLiNa（CKA+REPINA）对中间层进行对齐是一种低成本、实用的方法，可以有效提高低资源语言的翻译质量，尤其在数据稀缺的场景下效果显著。

Conclusion: TRepLiNa（CKA+REPINA）通过对齐多语言LLM的中间层，为改善低资源语言翻译提供了一个有效、低成本且实用的解决方案，特别适用于数据受限的环境。

Abstract: The 2025 Multimodal Models for Low-Resource Contexts and Social Impact
(MMLoSo) Language Challenge addresses one of India's most pressing linguistic
gaps: the lack of resources for its diverse low-resource languages (LRLs). In
this study, we investigate whether enforcing cross-lingual similarity in
specific internal layers of a decoder-only multilingual large language model
(LLM) can improve translation quality from LRL to high-resource language (HRL).
Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric
that encourages representations of different languages to align, with REPINA, a
regularization method that constrains parameter updates to remain close to the
pretrained model, into a joint method we call TRepLiNa. In this research
project, we experiment with zero-shot, few-shot, and fine-tuning settings using
Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari,
Santali, Bhili) with Hindi/English pivots. Our results show that aligning
mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach
to improving LRL translation, especially in data-scarce settings.

</details>


### [7] [Scalable multilingual PII annotation for responsible AI in LLMs](https://arxiv.org/abs/2510.06250)
*Bharti Meena,Joanna Skubisz,Harshit Rajgarhia,Nand Dave,Kiran Ganesh,Shivali Dalmia,Abhishek Mukherji,Vasudevan Sundarababu,Olga Pospelova*

Main category: cs.CL

TL;DR: 开发了一个可扩展的多语言PII数据标注框架，通过人机协作和迭代分析，提升了LLM处理个人身份信息的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，确保它们在不同法规环境下可靠处理个人身份信息（PII）变得至关重要。

Method: 引入了一个可扩展的多语言数据标注框架，旨在为13个代表性不足的区域（涵盖约336种本地化PII类型）提供高质量PII标注。采用分阶段、人机协作的标注方法，结合语言专家知识与严格质检，并通过标注者一致性指标和根因分析来系统性地发现和解决标注不一致问题。

Result: 标注质量显著提升，召回率和误报率在试点、训练和生产阶段均得到实质性改善。成功生成了适用于监督式LLM微调的高保真数据集。同时，也揭示了多语言PII标注中的常见挑战。

Conclusion: 迭代的、分析驱动的标注流程能够有效提升标注质量及下游模型的可靠性。

Abstract: As Large Language Models (LLMs) gain wider adoption, ensuring their reliable
handling of Personally Identifiable Information (PII) across diverse regulatory
contexts has become essential. This work introduces a scalable multilingual
data curation framework designed for high-quality PII annotation across 13
underrepresented locales, covering approximately 336 locale-specific PII types.
Our phased, human-in-the-loop annotation methodology combines linguistic
expertise with rigorous quality assurance, leading to substantial improvements
in recall and false positive rates from pilot, training, and production phases.
By leveraging inter-annotator agreement metrics and root-cause analysis, the
framework systematically uncovers and resolves annotation inconsistencies,
resulting in high-fidelity datasets suitable for supervised LLM fine-tuning.
Beyond reporting empirical gains, we highlight common annotator challenges in
multilingual PII labeling and demonstrate how iterative, analytics-driven
pipelines can enhance both annotation quality and downstream model reliability.

</details>


### [8] [Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments](https://arxiv.org/abs/2510.06262)
*Aryan Kumar Singh,Janvi Singh*

Main category: cs.CL

TL;DR: 该数据集提供了一个标准化、双语的（英语-印地语）Prakriti评估问卷的响应数据，用于根据阿育吠陀原则评估个体的身体、生理和心理特征。


<details>
  <summary>Details</summary>
Motivation: 根据经典的阿育吠陀原理，评估个体的身体、生理和心理特征，并为计算智能、阿育吠陀研究和个性化健康分析提供结构化数据平台。

Method: 开发了一个包含24个多选题的标准化双语（英语-印地语）Prakriti评估问卷，遵循AYUSH/CCRAS指南。问卷通过Google Forms部署，并隐藏Dosha标签以减少偏见，实现响应的自动评分，以将个体特质映射到特定Dosha得分。

Result: 生成了一个结构化的数据集，其中包含个体特质到Dosha（Vata, Pitta, Kapha）特定分数的映射。

Conclusion: 该数据集为计算智能、阿育吠陀研究和个性化健康分析的研究提供了一个平台，支持特质分布、相关性和预测模型的分析，并可作为未来Prakriti相关研究和智能健康应用开发的参考。

Abstract: This dataset provides responses to a standardized, bilingual (English-Hindi)
Prakriti Assessment Questionnaire designed to evaluate the physical,
physiological, and psychological characteristics of individuals according to
classical Ayurvedic principles. The questionnaire consists of 24
multiple-choice items covering body features, appetite, sleep patterns, energy
levels, and temperament. It was developed following AYUSH/CCRAS guidelines to
ensure comprehensive and accurate data collection. All questions are mandatory
and neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)
are hidden from participants. Data were collected via a Google Forms
deployment, enabling automated scoring of responses to map individual traits to
dosha-specific scores. The resulting dataset provides a structured platform for
research in computational intelligence, Ayurvedic studies, and personalized
health analytics, supporting analysis of trait distributions, correlations, and
predictive modeling. It can also serve as a reference for future Prakriti-based
studies and the development of intelligent health applications.

</details>


### [9] [Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians](https://arxiv.org/abs/2510.06263)
*Jiajun Wu,Swaleh Zaidi,Braden Teitge,Henry Leung,Jiayu Zhou,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 本文提出一个基于嵌入式设备的两阶段摘要系统，可在离线环境下处理电子健康记录（EHRs），为急诊医生快速生成结构化和上下文相关的临床摘要，同时保护患者隐私。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中大量非结构化临床数据使急诊医生难以快速识别关键信息。现有系统可能无法满足隐私保护和离线操作的需求，导致医生工作负担过重。

Method: 该系统采用双设备架构：Jetson Nano-R负责检索（Retrieve）相关病历片段，Jetson Nano-S负责生成（Summarize）摘要，通过轻量级套接字通信。检索阶段处理本地EHRs，将长笔记分割并搜索最相关部分；生成阶段使用本地托管的小型语言模型（SLM）生成摘要，包括固定格式的关键发现列表和基于查询的叙述。研究基准测试了六个7B参数以下的开源SLM，并使用LLM-as-Judge机制评估摘要的准确性、完整性和清晰度。

Result: 在MIMIC-IV和去识别化的真实EHRs上的初步结果显示，该完全离线系统能够有效生成有用的摘要，且在30秒内完成。

Conclusion: 所提出的离线、嵌入式摘要系统能够高效、准确地为急诊医生提供关键临床信息，同时确保患者隐私，显著提升了EHR数据利用效率。

Abstract: Electronic health records (EHRs) contain extensive unstructured clinical data
that can overwhelm emergency physicians trying to identify critical
information. We present a two-stage summarization system that runs entirely on
embedded devices, enabling offline clinical summarization while preserving
patient privacy. In our approach, a dual-device architecture first retrieves
relevant patient record sections using the Jetson Nano-R (Retrieve), then
generates a structured summary on another Jetson Nano-S (Summarize),
communicating via a lightweight socket link. The summarization output is
two-fold: (1) a fixed-format list of critical findings, and (2) a
context-specific narrative focused on the clinician's query. The retrieval
stage uses locally stored EHRs, splits long notes into semantically coherent
sections, and searches for the most relevant sections per query. The generation
stage uses a locally hosted small language model (SLM) to produce the summary
from the retrieved text, operating within the constraints of two NVIDIA Jetson
devices. We first benchmarked six open-source SLMs under 7B parameters to
identify viable models. We incorporated an LLM-as-Judge evaluation mechanism to
assess summary quality in terms of factual accuracy, completeness, and clarity.
Preliminary results on MIMIC-IV and de-identified real EHRs demonstrate that
our fully offline system can effectively produce useful summaries in under 30
seconds.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [10] [Milestone Determination for Autonomous Railway Operation](https://arxiv.org/abs/2510.06229)
*Josh Hunter,John McDermid,Simon Burton,Poppy Fynes,Mia Dempster*

Main category: cs.CV

TL;DR: 针对铁路自动化视觉系统数据不足问题，提出基于里程碑确定和路线特定线索的方法，简化模型训练并提升系统安全性与效率。


<details>
  <summary>Details</summary>
Motivation: 铁路自动化领域计算机视觉系统面临高质量时序数据匮乏的挑战，现有数据集不具备实时决策所需的时空上下文，替代方案缺乏真实性与适用性。

Method: 通过关注路线特定、上下文相关线索生成丰富时序数据集。利用“里程碑确定”概念，开发针对性、基于规则的模型，聚焦关键决策点而非泛化识别动态组件，从而简化学习过程。

Result: 该方法为在受控、可预测环境中训练铁路自动化视觉代理提供了一个实用的框架。

Conclusion: 此方法有助于构建更安全、更高效的铁路自动化机器学习系统。

Abstract: In the field of railway automation, one of the key challenges has been the
development of effective computer vision systems due to the limited
availability of high-quality, sequential data. Traditional datasets are
restricted in scope, lacking the spatio temporal context necessary for
real-time decision-making, while alternative solutions introduce issues related
to realism and applicability. By focusing on route-specific, contextually
relevant cues, we can generate rich, sequential datasets that align more
closely with real-world operational logic. The concept of milestone
determination allows for the development of targeted, rule-based models that
simplify the learning process by eliminating the need for generalized
recognition of dynamic components, focusing instead on the critical decision
points along a route. We argue that this approach provides a practical
framework for training vision agents in controlled, predictable environments,
facilitating safer and more efficient machine learning systems for railway
automation.

</details>


### [11] [CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation](https://arxiv.org/abs/2510.06231)
*Mingzhe Zheng,Dingjie Song,Guanyu Zhou,Jun You,Jiahao Zhan,Xuran Ma,Xinyuan Song,Ser-Nam Lim,Qifeng Chen,Harry Yang*

Main category: cs.CV

TL;DR: LLMs在生成电影剧本时缺乏“灵魂”，本文通过构建CML-Dataset、提出基于对话连贯性、角色一致性和情节合理性的CML-Bench评估基准，并引入CML-Instruction提示策略，显著提升了LLMs生成剧本的质量，使其更符合人类偏好。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在生成结构化文本方面表现出色，但在电影剧本创作中，除了结构组织外，还需捕捉细致入微的叙事和情感深度（即“电影的灵魂”），而这正是LLMs目前未能有效实现的部分。

Method: ['构建了CML-Dataset，这是一个包含高质量电影剧本片段（内容）及其简洁摘要的（摘要，内容）对数据集。', '通过深入分析真实剧本的多镜头连续性和叙事结构，确定了评估剧本质量的三个关键维度：对话连贯性（DC）、角色一致性（CC）和情节合理性（PR）。', '基于这些发现，提出了CML-Bench，一个包含上述维度量化指标的评估基准，能够有效评估人类创作的剧本并指出LLM生成剧本的弱点。', '引入了CML-Instruction，一种包含详细角色对话和事件逻辑指令的提示策略，旨在指导LLMs生成更具结构性和电影感的剧本。']

Result: ['CML-Bench能够有效为精心编写的人类剧本打出高分，并同时精准识别LLMs生成剧本中的弱点。', '广泛的实验验证了CML-Bench评估基准的有效性。', '实验表明，在CML-Instruction指导下，LLMs能够生成更高质量的剧本，且其结果与人类偏好一致。']

Conclusion: 本研究通过提出CML-Bench评估基准和CML-Instruction提示策略，成功解决了LLMs在生成电影剧本时缺乏叙事深度和情感捕捉能力的不足，使LLMs能够生成更具结构化、更符合电影感且与人类偏好一致的高质量剧本。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating highly structured texts. However, while exhibiting a high degree of
structural organization, movie scripts demand an additional layer of nuanced
storytelling and emotional depth-the 'soul' of compelling cinema-that LLMs
often fail to capture. To investigate this deficiency, we first curated
CML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup
Language (CML), where 'content' consists of segments from esteemed,
high-quality movie scripts and 'summary' is a concise description of the
content. Through an in-depth analysis of the intrinsic multi-shot continuity
and narrative structures within these authentic scripts, we identified three
pivotal dimensions for quality assessment: Dialogue Coherence (DC), Character
Consistency (CC), and Plot Reasonableness (PR). Informed by these findings, we
propose the CML-Bench, featuring quantitative metrics across these dimensions.
CML-Bench effectively assigns high scores to well-crafted, human-written
scripts while concurrently pinpointing the weaknesses in screenplays generated
by LLMs. To further validate our benchmark, we introduce CML-Instruction, a
prompting strategy with detailed instructions on character dialogue and event
logic, to guide LLMs to generate more structured and cinematically sound
scripts. Extensive experiments validate the effectiveness of our benchmark and
demonstrate that LLMs guided by CML-Instruction generate higher-quality
screenplays, with results aligned with human preferences.

</details>


### [12] [User to Video: A Model for Spammer Detection Inspired by Video Classification Technology](https://arxiv.org/abs/2510.06233)
*Haoyang Zhang,Zhou Yang,Yucai Pang*

Main category: cs.CV

TL;DR: 本文提出一种基于用户行为视频化的UVSD模型，将用户行为转化为视频并结合视频分类技术，有效检测垃圾邮件发送者。


<details>
  <summary>Details</summary>
Motivation: 受视频分类技术启发，将用户行为子空间视为帧图像，连续帧图像视为视频，以新颖视角解决垃圾邮件发送者检测问题。

Method: 提出UVSD模型。首先，通过user2piexl算法将用户像素化，用户立场量化为像素RGB。其次，利用behavior2image算法将用户行为子空间转化为帧图像，涉及表示学习、剪切和扩散算法。最后，基于时间特征构建用户行为视频，并结合视频分类算法识别垃圾邮件发送者。

Result: 在公开数据集WEIBO和TWITTER上的实验表明，UVSD模型优于现有最先进的方法。

Conclusion: UVSD模型通过将用户行为视频化并结合视频分类技术，为垃圾邮件发送者检测提供了一种有效且创新的方法。

Abstract: This article is inspired by video classification technology. If the user
behavior subspace is viewed as a frame image, consecutive frame images are
viewed as a video. Following this novel idea, a model for spammer detection
based on user videoization, called UVSD, is proposed. Firstly, a user2piexl
algorithm for user pixelization is proposed. Considering the adversarial
behavior of user stances, the user is viewed as a pixel, and the stance is
quantified as the pixel's RGB. Secondly, a behavior2image algorithm is proposed
for transforming user behavior subspace into frame images. Low-rank dense
vectorization of subspace user relations is performed using representation
learning, while cutting and diffusion algorithms are introduced to complete the
frame imageization. Finally, user behavior videos are constructed based on
temporal features. Subsequently, a video classification algorithm is combined
to identify the spammers. Experiments using publicly available datasets, i.e.,
WEIBO and TWITTER, show an advantage of the UVSD model over state-of-the-art
methods.

</details>


### [13] [Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout](https://arxiv.org/abs/2510.06238)
*Sagar Lekhak,Emmett J. Ientilucci,Dimah Dera,Susmita Ghosh*

Main category: cs.CV

TL;DR: 本研究将蒙特卡洛（MC）Dropout集成到ResNet-50架构中，用于表面地雷和未爆弹药的分类，以量化预测不确定性，提高模型在噪声和对抗性条件下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的确定性神经网络在表面地雷和未爆弹药检测中，易受噪声条件和对抗性攻击影响，导致漏检或误分类，这在人道主义排雷中是严重的问题。

Method: 将蒙特卡洛（MC）Dropout方法集成到经过微调的ResNet-50架构中，用于表面地雷和未爆弹药的分类，通过量化认知不确定性，提供额外的预测可靠性指标。该方法在模拟数据集上进行了测试。

Result: 实验结果表明，在干净、受对抗性扰动和噪声的测试图像上，该模型能够标记出不可靠的预测，在挑战性条件下提升了预测的可靠性。

Conclusion: 本概念验证研究强调了排雷中不确定性量化的必要性，揭示了现有神经网络在对抗性威胁下的脆弱性，并强调了开发更鲁棒和可靠模型的紧迫性，以实现实际应用。

Abstract: Detecting surface landmines and unexploded ordnances (UXOs) using deep
learning has shown promise in humanitarian demining. However, deterministic
neural networks can be vulnerable to noisy conditions and adversarial attacks,
leading to missed detection or misclassification. This study introduces the
idea of uncertainty quantification through Monte Carlo (MC) Dropout, integrated
into a fine-tuned ResNet-50 architecture for surface landmine and UXO
classification, which was tested on a simulated dataset. Integrating the MC
Dropout approach helps quantify epistemic uncertainty, providing an additional
metric for prediction reliability, which could be helpful to make more informed
decisions in demining operations. Experimental results on clean, adversarially
perturbed, and noisy test images demonstrate the model's ability to flag
unreliable predictions under challenging conditions. This proof-of-concept
study highlights the need for uncertainty quantification in demining, raises
awareness about the vulnerability of existing neural networks in demining to
adversarial threats, and emphasizes the importance of developing more robust
and reliable models for practical applications.

</details>


### [14] [multimodars: A Rust-powered toolkit for multi-modality cardiac image fusion and registration](https://arxiv.org/abs/2510.06241)
*Anselm W. Stark,Marc Ilic,Ali Mokhtari,Pooya Mohammadi Kazaj,Christoph Graeni,Isaac Shiri*

Main category: cs.CV

TL;DR: “multimodars”是一个开源工具包，通过确定性算法融合血管内成像与CCTA数据，用于构建可靠的3D冠状动脉模型，支持多状态分析，并提供高性能和易集成性。


<details>
  <summary>Details</summary>
Motivation: 构建可靠的3D冠状动脉模型需要结合血管内成像（高分辨率，但缺乏整体背景）和CCTA（提供3D几何，但分辨率有限且有伪影）。现有融合方法缺乏一个开放、灵活、支持多状态分析（如静息/应激、支架前/后）、且具有确定性、高性能和易集成特性的工具包。

Method: 开发了“multimodars”工具包，采用确定性对齐算法，以NumPy为中心的紧凑数据模型，以及优化的Rust后端，适用于可扩展和可重现的实验。该工具包可接受CSV/NumPy输入，包括AIVUS-CAA软件生成的数据格式。

Result: 成功开发了一个能够弥补现有空白的工具包，实现了高分辨率血管内成像与CCTA数据的确定性融合，并为多状态分析提供了高性能、可扩展和可重现的实验平台。

Conclusion: “multimodars”为构建可靠的3D冠状动脉模型提供了一个急需的开放、灵活、确定性且高性能的解决方案，特别适用于复杂的临床多状态分析，克服了现有工具的局限性。

Abstract: Combining complementary imaging modalities is critical to build reliable 3D
coronary models: intravascular imaging gives sub-millimetre resolution but
limited whole-vessel context, while CCTA supplies 3D geometry but suffers from
limited spatial resolution and artefacts (e.g., blooming). Prior work
demonstrated intravascular/CCTA fusion, yet no open, flexible toolkit is
tailored for multi-state analysis (rest/stress, pre-/post-stenting) while
offering deterministic behaviour, high performance, and easy pipeline
integration. multimodars addresses this gap with deterministic alignment
algorithms, a compact NumPy-centred data model, and an optimised Rust backend
suitable for scalable, reproducible experiments. The package accepts CSV/NumPy
inputs including data formats produced by the AIVUS-CAA software

</details>


### [15] [Does Physics Knowledge Emerge in Frontier Models?](https://arxiv.org/abs/2510.06251)
*Ieva Bagdonaviciute,Vibhav Vineet*

Main category: cs.CV

TL;DR: 当前领先的视觉语言模型（VLMs）在物理动态理解方面表现不佳，其感知和物理推理能力未能有效结合以实现因果理解。


<details>
  <summary>Details</summary>
Motivation: 尽管领先的VLMs在视觉感知和通用推理方面表现出色，但它们理解和预测物理动态的能力尚不清楚。

Method: 研究评估了六个前沿VLM在三个物理模拟数据集（CLEVRER, Physion, Physion++）上的表现，任务包括结果预测和反事实假设。同时设计了诊断性子测试，以区分感知（物体、颜色、遮挡物）和物理推理（运动预测、空间关系）能力。

Result: 分析揭示了诊断性性能与评估准确度之间的弱相关性：在感知或物理推理方面表现优秀的模型，在预测或反事实评估中并未持续表现更好。

Conclusion: 这一反直觉的差距揭示了当前VLM的核心局限性：感知和物理技能仍然是碎片化的，未能结合成因果理解，强调了需要更紧密结合感知和推理的架构。

Abstract: Leading Vision-Language Models (VLMs) show strong results in visual
perception and general reasoning, but their ability to understand and predict
physical dynamics remains unclear. We benchmark six frontier VLMs on three
physical simulation datasets - CLEVRER, Physion, and Physion++ - where the
evaluation tasks test whether a model can predict outcomes or hypothesize about
alternative situations. To probe deeper, we design diagnostic subtests that
isolate perception (objects, colors, occluders) from physics reasoning (motion
prediction, spatial relations). Intuitively, stronger diagnostic performance
should support higher evaluation accuracy. Yet our analysis reveals weak
correlations: models that excel at perception or physics reasoning do not
consistently perform better on predictive or counterfactual evaluation. This
counterintuitive gap exposes a central limitation of current VLMs: perceptual
and physics skills remain fragmented and fail to combine into causal
understanding, underscoring the need for architectures that bind perception and
reasoning more tightly.

</details>


### [16] [Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training](https://arxiv.org/abs/2510.06254)
*Xiaochen Zhao,Chengting Yu,Kairong Yu,Lei Liu,Aili Wang*

Main category: cs.CV

TL;DR: 针对SNN传统训练方法性能不足和计算开销大的问题，本文提出了一种结合速率反向传播的增强型自蒸馏框架。通过将SNN层发放率映射到ANN分支并解耦教师信号以确保只使用可靠知识，实现了在降低训练复杂度的同时，高性能SNN的训练。


<details>
  <summary>Details</summary>
Motivation: SNN在神经形态硬件上能效高，但传统训练方法（基于替代梯度和BPTT）不仅性能落后于ANN，而且计算和内存开销随时间维度线性增长。研究动机是在有限计算资源下，实现高性能SNN训练。

Method: 提出了一种增强型自蒸馏框架，与基于速率的反向传播联合优化。具体地，将SNN中间层的脉冲发放率投影到轻量级ANN分支，并利用模型自身生成的高质量知识通过ANN路径优化子结构。为避免低质量自生成知识阻碍收敛，将教师信号解耦为可靠和不可靠两部分，仅使用可靠知识指导模型优化。

Result: 在CIFAR-10、CIFAR-100、CIFAR10-DVS和ImageNet数据集上的大量实验表明，该方法在降低训练复杂度的同时，实现了高性能SNN训练。

Conclusion: 该研究通过提出增强型自蒸馏框架并结合可靠知识解耦机制，有效解决了传统SNN训练的性能瓶颈和资源消耗问题，为在有限计算资源下实现高性能SNN训练提供了一条可行路径。

Abstract: Spiking Neural Networks (SNNs) exhibit exceptional energy efficiency on
neuromorphic hardware due to their sparse activation patterns. However,
conventional training methods based on surrogate gradients and Backpropagation
Through Time (BPTT) not only lag behind Artificial Neural Networks (ANNs) in
performance, but also incur significant computational and memory overheads that
grow linearly with the temporal dimension. To enable high-performance SNN
training under limited computational resources, we propose an enhanced
self-distillation framework, jointly optimized with rate-based backpropagation.
Specifically, the firing rates of intermediate SNN layers are projected onto
lightweight ANN branches, and high-quality knowledge generated by the model
itself is used to optimize substructures through the ANN pathways. Unlike
traditional self-distillation paradigms, we observe that low-quality
self-generated knowledge may hinder convergence. To address this, we decouple
the teacher signal into reliable and unreliable components, ensuring that only
reliable knowledge is used to guide the optimization of the model. Extensive
experiments on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate that
our method reduces training complexity while achieving high-performance SNN
training. Our code is available at
https://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn.

</details>


### [17] [Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis](https://arxiv.org/abs/2510.06260)
*Sher Khan,Raz Muhammad,Adil Hussain,Muhammad Sajjad,Muhammad Rashid*

Main category: cs.CV

TL;DR: 本文提出一个统一的AI框架，用于皮肤病诊断。该框架结合了异构卷积神经网络的诊断能力（包含不确定性机制）和大型语言模型（LLM）的临床沟通与教育功能，旨在提高诊断精度、减少误诊，并优化患者教育和医患沟通，填补了AI在皮肤科临床应用中的空白。


<details>
  <summary>Details</summary>
Motivation: 皮肤恶性肿瘤需早期发现，但现有诊断方法存在观察者间差异大、获取不便等问题。当前AI系统受限于架构同质性、数据集肤色偏差及NLP作为后处理而非核心的碎片化应用。研究旨在解决诊断可靠性低和沟通障碍，将AI深度整合到临床决策中。

Method: 引入了一个统一的框架，包含两个协同创新：1. 一个由架构多样化的卷积神经网络组成的异构集成模型，提供互补的诊断视角，并内置不确定性机制，用于标记不一致病例供专家审查。2. 将大型语言模型能力直接嵌入诊断工作流程，将分类输出转化为临床有意义的评估，同时满足医疗文件要求并提供以患者为中心的教育。最终生成结构化报告，包括精确病灶特征、可理解的诊断推理和可操作的监测指导。

Result: 该框架生成结构化报告，精确描述病灶、提供易懂的诊断推理和可操作的监测指导，赋能患者识别早期预警信号。通过在一个连贯系统中解决诊断可靠性和沟通障碍，弥合了阻碍此前AI实施产生临床影响的关键转化鸿沟。显著提升了部署型皮肤病AI的诊断精度，并积极支持从早期检测到患者教育的连续护理，最终提高了皮肤病变的早期干预率。

Conclusion: 该框架是可部署的皮肤病AI领域的重大进展，它在提高诊断精度的同时，积极支持从初始检测到患者教育的连续护理，最终改善了皮肤病变的早期干预率。

Abstract: Cutaneous malignancies demand early detection for favorable outcomes, yet
current diagnostics suffer from inter-observer variability and access
disparities. While AI shows promise, existing dermatological systems are
limited by homogeneous architectures, dataset biases across skin tones, and
fragmented approaches that treat natural language processing as separate
post-hoc explanations rather than integral to clinical decision-making. We
introduce a unified framework that fundamentally reimagines AI integration for
dermatological diagnostics through two synergistic innovations. First, a
purposefully heterogeneous ensemble of architecturally diverse convolutional
neural networks provides complementary diagnostic perspectives, with an
intrinsic uncertainty mechanism flagging discordant cases for specialist review
-- mimicking clinical best practices. Second, we embed large language model
capabilities directly into the diagnostic workflow, transforming classification
outputs into clinically meaningful assessments that simultaneously fulfill
medical documentation requirements and deliver patient-centered education. This
seamless integration generates structured reports featuring precise lesion
characterization, accessible diagnostic reasoning, and actionable monitoring
guidance -- empowering patients to recognize early warning signs between
visits. By addressing both diagnostic reliability and communication barriers
within a single cohesive system, our approach bridges the critical
translational gap that has prevented previous AI implementations from achieving
clinical impact. The framework represents a significant advancement toward
deployable dermatological AI that enhances diagnostic precision while actively
supporting the continuum of care from initial detection through patient
education, ultimately improving early intervention rates for skin lesions.

</details>


### [18] [Vision Transformer for Transient Noise Classification](https://arxiv.org/abs/2510.06273)
*Divyansh Srivastava,Andrzej Niedzielski*

Main category: cs.CV

TL;DR: 为LIGO数据中的24种瞬态噪声（glitch）设计了一个Vision Transformer分类模型，实现了92.26%的分类效率，有助于提高引力波探测精度。


<details>
  <summary>Details</summary>
Motivation: LIGO数据中的瞬态噪声（glitch）严重干扰引力波探测。随着O3a运行新增两个噪声类别，现有分类模型不足，亟需开发新的有效分类方法以提高引力波探测准确性。

Method: 利用预训练的Vision Transformer (ViT-B/32) 模型，在一个包含Gravity Spy数据集（22类）和LIGO O3a新增两类噪声的组合数据集上进行训练，旨在对总共24种噪声类别进行分类。

Result: 模型在24种瞬态噪声分类任务中达到了92.26%的分类效率。

Conclusion: 研究表明Vision Transformer模型在有效区分瞬态噪声方面具有巨大潜力，有望显著提高引力波探测的准确性。

Abstract: Transient noise (glitches) in LIGO data hinders the detection of
gravitational waves (GW). The Gravity Spy project has categorized these noise
events into various classes. With the O3 run, there is the inclusion of two
additional noise classes and thus a need to train new models for effective
classification. We aim to classify glitches in LIGO data into 22 existing
classes from the first run plus 2 additional noise classes from O3a using the
Vision Transformer (ViT) model. We train a pre-trained Vision Transformer
(ViT-B/32) model on a combined dataset consisting of the Gravity Spy dataset
with the additional two classes from the LIGO O3a run. We achieve a
classification efficiency of 92.26%, demonstrating the potential of Vision
Transformer to improve the accuracy of gravitational wave detection by
effectively distinguishing transient noise.
  Key words: gravitational waves --vision transformer --machine learning

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning](https://arxiv.org/abs/2510.06261)
*Zhanke Zhou,Chentao Cao,Xiao Feng,Xuan Li,Zongze Li,Xiangyu Lu,Jiangchao Yao,Weikai Huang,Linrui Xu,Tian Cheng,Guanyu Jiang,Yiming Zheng,Brando Miranda,Tongliang Liu,Sanmi Koyejo,Masashi Sugiyama,Bo Han*

Main category: cs.AI

TL;DR: AlphaApollo是一个自演进的智能体推理系统，通过整合多模型与专业工具（计算和检索）并支持多轮迭代优化，有效提升了基础模型的推理能力，解决了其内在能力限制和迭代不可靠问题。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型（FM）推理中的两个主要瓶颈：有限的模型内在能力和不可靠的测试时迭代。

Method: 提出AlphaApollo，一个自演进的智能体推理系统。该系统通过以下方式实现：
1. 协调多个模型与专业工具进行协作，以实现深思熟虑且可验证的推理。
2. 整合计算工具（Python及数值/符号库）和检索工具（任务相关外部信息），用于精确计算和决策依据。
3. 利用共享状态图支持多轮、多模型的解决方案演进，记录候选方案、可执行检查和反馈以进行迭代优化。

Result: 1. 在AIME 2024/2025的评估中，AlphaApollo实现了显著提升：
   - 对于Qwen2.5-14B-Instruct，Average@32提升5.15%，Pass@32提升23.34%。
   - 对于Llama-3.3-70B-Instruct，Average@32提升8.91%，Pass@32提升26.67%。
2. 工具使用分析显示，超过80%的工具调用成功执行，且持续优于非工具基线。

Conclusion: AlphaApollo通过有效地利用工具，成功克服了基础模型的内在能力和迭代可靠性限制，显著提升了其推理能力上限。

Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to
address two bottlenecks in foundation model (FM) reasoning-limited
model-intrinsic capacity and unreliable test-time iteration. AlphaApollo
orchestrates multiple models with professional tools to enable deliberate,
verifiable reasoning. It couples (i) a computation tool (Python with numerical
and symbolic libraries) and (ii) a retrieval tool (task-relevant external
information) to execute exact calculations and ground decisions. The system
further supports multi-round, multi-model solution evolution via a shared state
map that records candidates, executable checks, and feedback for iterative
refinement. In evaluations on AIME 2024/2025 across multiple models,
AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32
for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for
Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool
calls are successfully executed, with consistent outperformance of non-tool
baselines, thereby lifting the capability ceiling of FMs. More empirical
results and implementation details will be updated at
https://github.com/tmlr-group/AlphaApollo.

</details>


### [20] [Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization](https://arxiv.org/abs/2510.06274)
*Mohammad Mahdi Samiei Paqaleh,Arash Marioriyad,Arman Tahmasebi-Zadeh,Mohamadreza Fereydooni,Mahdi Ghaznavai,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: 本文提出了“复杂度分布外泛化（Complexity OoD）”框架，旨在定义和衡量AI的推理能力，尤其是在面对需要比训练数据更高解决复杂度的任务时。


<details>
  <summary>Details</summary>
Motivation: 随着AI从模式识别转向需要System2式推理的问题，特别是大型语言模型，目前对推理能力的泛化和分布外评估缺乏清晰一致的定义和衡量标准。

Method: 该研究提出Complexity OoD泛化框架，将其定义为模型在测试实例的最小所需解决方案复杂度（表示或计算）超出所有训练样本时，仍能保持性能。通过柯尔莫哥洛夫复杂度和操作代理（如对象/关系计数、推理步数）来形式化复杂度，并阐明其与长度和组合性OoD的区别。该框架统一了学习和推理，并将System2视为对解决方案结构的泛化。

Result: 该框架为推理能力提供了新的定义和衡量标准，并提供了在基准设计、监督方式、归纳偏置等方面操作化Complexity OoD的实践建议。研究指出，单靠数据扩展无法解决Complexity OoD问题。

Conclusion: 为了实现鲁棒推理，AI需要明确建模并根据复杂度分配计算资源的架构和训练机制，而非仅仅依靠扩大数据规模。

Abstract: Recent progress has pushed AI frontiers from pattern recognition tasks toward
problems that require step by step, System2 style reasoning, especially with
large language models. Yet, unlike learning, where generalization and out of
distribution (OoD) evaluation concepts are well formalized, there is no clear,
consistent definition or metric for reasoning ability. We propose Complexity
Out of Distribution (Complexity OoD) generalization as a framework and problem
setting to define and measure reasoning. A model exhibits Complexity OoD
generalization when it maintains performance on test instances whose minimal
required solution complexity, either representational (richer solution
structure) or computational (more reasoning steps/program length), exceeds that
of all training examples. We formalize complexity via solution description
Kolmogorov complexity and operational proxies (e.g., object/relation counts;
reasoning step counts), clarifying how Complexity OoD differs from length and
compositional OoD. This lens unifies learning and reasoning: many cases
solvable with System1 like processing at low complexity become System2 like
under complexity pressure, while System2 can be viewed as generalization over
solution structures. We translate this perspective into practice with
recommendations for operationalizing Complexity OoD across the stack:
incorporating complexity into benchmark and evaluation metric design,
rethinking supervision to target solution traces, seeking and designing
inductive biases for Complexity OoD generalization, addressing learning to
reason spillovers such as spurious shortcuts, semantic robustness, catastrophic
forgetting, and step wise calibration. Because Complexity OoD cannot be solved
by scaling data alone, progress toward robust reasoning will require
architectures and training regimes that explicitly model and allocate
computation with respect to complexity.

</details>


### [21] [BuilderBench -- A benchmark for generalist agents](https://arxiv.org/abs/2510.06288)
*Raj Ghugare,Catherine Ji,Kathryn Wantlin,Jin Schofield,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: 引入BuilderBench，一个用于推动智能体通过开放式探索学习构建结构的基准，旨在解决现有AI难以应对新问题并促进具身推理研究，现有算法在此基准上表现不足。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型主要通过模仿学习，难以解决现有数据范围之外的新问题。需要开发能通过交互探索和经验学习的智能体，但缺乏可扩展的学习机制。

Method: 提出BuilderBench基准，包含一个硬件加速的机器人模拟器和42个测试物理、数学及长程规划能力的构建任务。智能体在无监督下探索并学习环境原理，然后构建未见过的目标结构。同时提供“训练轮”协议和六种算法实现。

Result: 实验表明，BuilderBench中的许多任务对当前算法构成挑战。

Conclusion: BuilderBench为研究智能体开放式探索和具身推理提供了新工具，并揭示了现有算法的局限性，为未来研究指明方向。

Abstract: Today's AI models learn primarily through mimicry and sharpening, so it is
not surprising that they struggle to solve problems beyond the limits set by
existing data. To solve novel problems, agents should acquire skills for
exploring and learning through experience. Finding a scalable learning
mechanism for developing agents that learn through interaction remains a major
open problem. In this work, we introduce BuilderBench, a benchmark to
accelerate research into agent pre-training that centers open-ended
exploration. BuilderBench requires agents to learn how to build any structure
using blocks. BuilderBench is equipped with $(1)$ a hardware accelerated
simulator of a robotic agent interacting with various physical blocks, and
$(2)$ a task-suite with over 42 diverse target structures that are carefully
curated to test an understanding of physics, mathematics, and long-horizon
planning. During training, agents have to explore and learn general principles
about the environment without any external supervision. During evaluation,
agents have to build the unseen target structures from the task suite. Solving
these tasks requires a sort of \emph{embodied reasoning} that is not reflected
in words but rather in actions, experimenting with different strategies and
piecing them together. Our experiments show that many of these tasks challenge
the current iteration of algorithms. Hence, we also provide a ``training
wheels'' protocol, in which agents are trained and evaluated to build a single
target structure from the task suite. Finally, we provide single-file
implementations of six different algorithms as a reference point for
researchers.

</details>


### [22] [Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration](https://arxiv.org/abs/2510.06302)
*Ksenija Lace,Marite Kirikova*

Main category: cs.AI

TL;DR: 针对并购后信息系统集成训练中高学习曲线和低动机问题，本文探索游戏化学习设计，通过分析相关理论和游戏框架，提出了构建游戏化学习框架的关键设计要求。


<details>
  <summary>Details</summary>
Motivation: 并购后信息系统集成对专业人员构成独特挑战，现有决策支持方法（AMILI、AMILP）在实际应用中存在学习曲线高和学习者动机低的问题，缺乏有效的培训方法。

Method: 本文通过分析基础学习理论、认知负荷与动机模型以及严肃游戏设计框架，旨在识别并提出适用于并购后信息系统集成的游戏化学习设计框架的必要要求。这些要求被结构化为转化过程和学习体验两个组成部分。

Result: 识别并结构化了针对并购后信息系统集成场景的游戏化学习设计框架的必要要求，涵盖转化过程和学习体验两个方面。

Conclusion: 论文规划了未来通过迭代设计和实际验证来开发和评估所提出的游戏化学习框架的计划。

Abstract: Post-merger integration states unique challenges for professionals
responsible for information system integration aimed on alignment and
combination diverse system architectures of merging organizations. Although the
theoretical and practical guidance exists for post-merger integration on the
business level, there is a significant gap in training for information system
integration in this context. In prior research specific methods AMILI (Support
method for informed decision identification) and AMILP (Support method for
informed decision-making) were introduced for the support of information system
integration decisions in the post-merger integration. But during the practical
application was reported high learning curve and low learner motivation. This
paper explores how game-based learning design can address these limitations by
transforming static method training into engaging learning experience. The
study analyzes foundational learning theories, cognitive load and motivation
models, and serious game design frameworks to identify the essential
requirements for a game-based learning design framework tailored to information
system integration in post-merger integration. Requirements are structured in
two components: the transformation process and resulting learning experience.
The paper concludes with a plan for developing and evaluating the proposed
framework through iterative design and real-world validation.

</details>


### [23] [Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks](https://arxiv.org/abs/2510.06307)
*Wentao Deng,Jiahuan Pei,Zhiwei Xu,Zhaochun Ren,Zhumin Chen,Pengjie Ren*

Main category: cs.AI

TL;DR: 本文提出了信念校准共识寻求（BCCS）框架，通过优化合作者选择和内部信念校准来增强多智能体系统在复杂NLP任务中的共识稳定性，并在MATH和MMLU数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有共识寻求方法通常依赖投票机制，忽略了系统内部信念矛盾导致的共识不稳定性。此外，无差别的智能体间协作未能有效识别最优合作者，阻碍了稳定共识的形成。

Method: 作者提供了一个理论框架，用于选择能够最大化共识稳定性的最优合作者。在此理论基础上，提出了信念校准共识寻求（BCCS）框架，通过选择最优合作者和利用系统内部信念校准共识判断，以促进稳定共识。

Result: 在MATH和MMLU基准数据集上的实验结果表明，所提出的BCCS框架在具有挑战性的任务上，准确率分别超越现有最佳结果2.23%和3.95%。

Conclusion: BCCS框架通过解决现有共识寻求方法的局限性，为多智能体系统在复杂NLP任务中实现更稳定、更高效的共识提供了一种有效途径，显著提升了任务性能。

Abstract: A multi-agent system (MAS) enhances its capacity to solve complex natural
language processing (NLP) tasks through collaboration among multiple agents,
where consensus-seeking serves as a fundamental mechanism. However, existing
consensus-seeking approaches typically rely on voting mechanisms to judge
consensus, overlooking contradictions in system-internal beliefs that
destabilize the consensus. Moreover, these methods often involve agents
updating their results through indiscriminate collaboration with every other
agent. Such uniform interaction fails to identify the optimal collaborators for
each agent, hindering the emergence of a stable consensus. To address these
challenges, we provide a theoretical framework for selecting optimal
collaborators that maximize consensus stability. Based on the theorems, we
propose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate
stable consensus via selecting optimal collaborators and calibrating the
consensus judgment by system-internal beliefs. Experimental results on the MATH
and MMLU benchmark datasets demonstrate that the proposed BCCS framework
outperforms the best existing results by 2.23% and 3.95% of accuracy on
challenging tasks, respectively. Our code and data are available at
https://github.com/dengwentao99/BCCS.

</details>


### [24] [Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?](https://arxiv.org/abs/2510.06410)
*Aochong Oliver Li,Tanya Goyal*

Main category: cs.AI

TL;DR: 本文研究了标准LLM训练方法能否使其具备“脱轨推理”能力（即评估并利用其他模型的思考）。通过提出Recoverability和Guidability“双生测试”，发现当前强大的LLM在面对干扰时更脆弱，且无法有效利用合作者的指导，揭示了现有LLM在多模型协作方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 语言模型（LLMs）通过推理过程的透明化，在复杂任务上表现出色。这种透明性也为多模型直接协作提供了新方向。然而，关键前提是LLM需要具备评估并基于其他模型部分思考的能力，即“脱轨推理”。因此，本文旨在探讨标准单体推理训练管道是否能培养出所需的脱轨行为。

Method: 本文提出了衡量脱轨推理的“双生测试”：Recoverability（恢复能力），测试LLM从误导性推理轨迹中回溯的能力；Guidability（引导能力），测试LLM基于更强合作者正确推理进行构建的能力。研究评估了15个开源LLM（1.5B-32B）。此外，还通过控制实验隔离了蒸馏教师模型选择、强化学习使用和数据选择策略等后训练因素对这些行为的影响。

Result: 研究发现了一个反直觉的结果：在基准测试中表现“更强”的LLM在面对干扰时往往更脆弱（Recoverability较差）。所有被测试的模型都无法有效利用合作者提供的引导步骤来解决超出其固有能力的问题，解决率保持在9.2%以下（Guidability较差）。控制研究表明，教师模型次优的恢复能力行为会被传递给通过蒸馏训练的学生模型，即使蒸馏轨迹是正确的。

Conclusion: 这项工作为评估共享推理轨迹中的多模型协作奠定了基础，并强调了现成推理LLM在作为有效协作伙伴方面的局局限性。研究结果也为训练原生强大的推理协作模型提供了可操作的见解，例如，提示了蒸馏训练中教师模型选择的重要性。

Abstract: Reasoning LLMs are trained to verbalize their reasoning process, yielding
strong gains on complex tasks. This transparency also opens a promising
direction: multiple reasoners can directly collaborate on each other's thinking
within a shared trajectory, yielding better inference efficiency and
exploration. A key prerequisite, however, is the ability to assess the
usefulness and build on another model's partial thinking -- we call this
off-trajectory reasoning. Our paper investigates a critical question: can
standard solo-reasoning training pipelines deliver desired off-trajectory
behaviors? We propose twin tests that capture the two extremes of the
off-trajectory spectrum, namely Recoverability, which tests whether LLMs can
backtrack from "distractions" induced by misleading reasoning traces, and
Guidability, which tests their ability to build upon correct reasoning from
stronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and
reveals a counterintuitive finding -- "stronger" LLMs on benchmarks are often
more fragile under distraction. Moreover, all models tested fail to effectively
leverage guiding steps from collaborators on problems beyond their inherent
capabilities with solve rates remaining under 9.2%. Finally, we conduct control
studies to isolate the effects of three factors in post-training on these
behaviors: the choice of distillation teacher, the use of RL, and data
selection strategy. Our results provide actionable insights for training
natively strong reasoning collaborators; e.g., we find that suboptimal
recoverability behaviors of teacher models are transferred to distilled
students even if the distillation trajectories are correct. Taken together,
this work lays the groundwork for evaluating multi-model collaborations in
shared reasoning trajectories and highlights the limitations of off-the-shelf
reasoning LLMs.

</details>


### [25] [Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health](https://arxiv.org/abs/2510.06433)
*Aryan Singh Dalal,Yinglun Zhang,Duru Doğan,Atalay Mert İleri,Hande Küçük McGinty*

Main category: cs.AI

TL;DR: 本研究创建了一个知识图谱，将食物（尤其是黄酮类化合物）与健康（癌症关联）联系起来，以标准化、机器可读的格式呈现食物与健康的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管“食物即药物”的理念受到关注，但目前缺乏将食物与健康关系以标准化、机器可读的语义网络格式表示的研究，以有效利用现有知识。

Method: 本研究通过结合USDA数据库中的食物黄酮类化合物含量和文献中的癌症关联信息，创建了一个知识图谱。研究采用了KNARM方法论，以机器可操作的格式表示这些关系。

Result: 成功构建了一个食物与健康（食物中的黄酮类化合物与癌症）的知识图谱，并以机器可操作的格式呈现。该图谱为研究人员探索饮食选择与疾病管理之间的复杂相互作用提供了范例。

Conclusion: 本研究弥补了食物与健康关系标准化、机器可读表示的空白，通过构建知识图谱提供了一个有效的工具，帮助研究人员更好地理解饮食与疾病之间的联系。

Abstract: The focus on "food as medicine" is gaining traction in the field of health
and several studies conducted in the past few years discussed this aspect of
food in the literature. However, very little research has been done on
representing the relationship between food and health in a standardized,
machine-readable format using a semantic web that can help us leverage this
knowledge effectively. To address this gap, this study aims to create a
knowledge graph to link food and health through the knowledge graph's ability
to combine information from various platforms focusing on flavonoid contents of
food found in the USDA databases and cancer connections found in the
literature. We looked closely at these relationships using KNARM methodology
and represented them in machine-operable format. The proposed knowledge graph
serves as an example for researchers, enabling them to explore the complex
interplay between dietary choices and disease management. Future work for this
study involves expanding the scope of the knowledge graph by capturing nuances,
adding more related data, and performing inferences on the acquired knowledge
to uncover hidden relationships.

</details>


### [26] [PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles](https://arxiv.org/abs/2510.06475)
*Yitao Long,Yuru Jiang,Hongjun Liu,Yilun Zhao,Jingchen Sun,Yiqiu Shen,Chen Zhao,Arman Cohan,Dennis Shasha*

Main category: cs.AI

TL;DR: 本文引入了PuzzlePlex基准，用于评估基础模型在复杂动态环境中的推理和规划能力，并分析了其性能和扩展性。


<details>
  <summary>Details</summary>
Motivation: 研究基础模型在复杂动态环境中的推理、规划能力及其可扩展性。

Method: 引入了包含15种谜题类型的PuzzlePlex基准；提供了全面的游戏环境并支持扩展性；实现了定制化的游戏策略进行比较；开发了细粒度指标；在基于指令和基于代码的设置中深入分析了前沿基础模型；系统性地调查了其扩展限制。

Result: 推理模型在基于指令的设置中表现优于其他模型；基于代码的执行带来了更大挑战，但提供了一个可扩展且高效的替代方案。

Conclusion: PuzzlePlex支持有针对性的评估，并指导未来基础模型在推理、规划和泛化方面的改进。

Abstract: This work investigates the reasoning and planning capabilities of foundation
models and their scalability in complex, dynamic environments. We introduce
PuzzlePlex, a benchmark designed to assess these capabilities through a diverse
set of puzzles. PuzzlePlex consists of 15 types of puzzles, including
deterministic and stochastic games of varying difficulty, as well as
single-player and two-player scenarios. The PuzzlePlex framework provides a
comprehensive environment for each game, and supports extensibility to generate
more challenging instances as foundation models evolve. Additionally, we
implement customized game-playing strategies for comparison. Building on this
benchmark, we develop fine-grained metrics to measure performance and conduct
an in-depth analysis of frontier foundation models across two settings:
instruction-based and code-based. Furthermore, we systematically investigate
their scaling limits. Our findings show that reasoning models outperform others
in instruction-based settings, while code-based execution presents greater
challenges but offers a scalable and efficient alternative. PuzzlePlex enables
targeted evaluation and guides future improvements in reasoning, planning, and
generalization for foundation models.

</details>


### [27] [Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them](https://arxiv.org/abs/2510.06534)
*Jiahe Jin,Abhijay Paladugu,Chenyan Xiong*

Main category: cs.AI

TL;DR: 代理式搜索中，本文提出Behavior Priming技术，通过监督微调（SFT）和强化学习（RL）训练大型语言模型（LLM）。研究发现，SFT数据中好的推理行为比答案正确性更能提升模型性能，从而使LLM在代理式搜索任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 代理式搜索（Agentic search）利用LLM解释复杂用户需求并执行多步规划、搜索和信息合成。然而，LLM在与检索系统和网页交互时，其推理和代理能力面临独特的挑战，需要研究如何提升其效率和鲁棒性。

Method: 1. 提出了一个推理驱动的LLM管道，用于分析代理式搜索中有效的推理行为模式。2. 通过分析成功的代理式搜索轨迹，识别出四种有益的推理行为：信息验证、权威评估、自适应搜索和错误恢复。3. 基于这些发现，提出“行为预训练”（Behavior Priming）技术，通过合成包含这些行为的代理式搜索轨迹进行监督微调（SFT），随后结合标准强化学习（RL）来训练更有效的代理式搜索模型。

Result: 1. 在GAIA、WebWalker和HLE三个基准测试中，行为预训练（Behavior Priming）使Llama3.2-3B和Qwen3-1.7B的性能相比直接使用RL训练提高了35%以上。2. 关键发现是，SFT数据中期望的推理行为（而非最终答案的正确性）是RL后实现强大最终性能的关键因素：即使微调数据中的推理行为良好但答案不正确，也能比微调数据答案正确但推理行为不佳的情况获得更好的性能。3. 分析揭示，引入的推理行为赋予模型更有效的探索能力（更高的pass@k和熵）和测试时扩展能力（更长的轨迹），为RL奠定了坚实基础。

Conclusion: 1. 有效的推理行为（信息验证、权威评估、自适应搜索、错误恢复）对于提升代理式搜索LLM的性能至关重要。2. 行为预训练（Behavior Priming）是一种有效的训练范式，通过在SFT阶段关注高质量的推理行为，能够显著提高代理式搜索模型的效率。3. SFT数据中推理行为的质量比最终答案的正确性更能决定模型在RL后的最终性能，因为良好的推理行为能增强模型的探索和扩展能力。

Abstract: Agentic search leverages large language models (LLMs) to interpret complex
user information needs and execute a multi-step process of planning, searching,
and synthesizing information to provide answers. This paradigm introduces
unique challenges for LLMs' reasoning and agentic capabilities when interacting
with retrieval systems and the broader web. In this paper, we propose a
reasoning-driven LLM-based pipeline to study effective reasoning behavior
patterns in agentic search. Using this pipeline, we analyze successful agentic
search trajectories and identify four beneficial reasoning behaviors:
Information Verification, Authority Evaluation, Adaptive Search, and Error
Recovery. Based on these findings, we propose a technique called Behavior
Priming to train more effective agentic search models. It synthesizes agentic
search trajectories that exhibit these four behaviors and integrates them into
the agentic search model through supervised fine-tuning (SFT), followed by
standard reinforcement learning (RL). Experiments on three benchmarks (GAIA,
WebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in
Llama3.2-3B and Qwen3-1.7B compared to directly training agentic search models
with RL. Crucially, we demonstrate that the desired reasoning behaviors in the
SFT data, rather than the correctness of the final answer, is the critical
factor for achieving strong final performance after RL: fine-tuning on
trajectories with desirable reasoning behaviors but incorrect answers leads to
better performance than fine-tuning on trajectories with correct answers. Our
analysis further reveals the underlying mechanism: the introduced reasoning
behaviors endow models with more effective exploration (higher pass@k and
entropy) and test-time scaling (longer trajectories) capabilities, providing a
strong foundation for RL. Our code will be released as open source.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases](https://arxiv.org/abs/2510.06267)
*Khartik Uppalapati,Shakeel Abdulkareem,Bora Yimenicioglu*

Main category: cs.LG

TL;DR: 提出RareGraph-Synth，一个知识引导的连续时间扩散框架，用于生成针对超罕见疾病的真实且保护隐私的合成电子健康记录（EHR）数据。


<details>
  <summary>Details</summary>
Motivation: 为超罕见疾病研究提供真实且保护隐私的合成电子健康记录（EHR）数据，以应对数据稀缺和隐私保护的挑战。

Method: RareGraph-Synth通过整合Orphanet/Orphadata、HPO、GARD、PrimeKG和FAERS等五个公共资源，构建了一个约800万条边的异构知识图谱。该框架利用从知识图谱中提取的元路径分数来调节前向随机微分方程中的每令牌噪音调度，引导生成生物学上合理的实验室-药物-不良事件共现，同时保持扩散模型的稳定性。反向去噪器生成无受保护健康信息的、带有时间戳的实验室代码、药物代码和不良事件标志三元组序列。

Result: 在模拟超罕见疾病队列上，RareGraph-Synth的分类最大均值差异（MMD）比无引导扩散基线降低40%，比GANs降低超过60%，且不牺牲下游预测效用。通过DOMIAS攻击者的黑盒成员推断评估，AUROC约为0.53，远低于0.55的安全发布阈值，显著优于非知识图谱基线的约0.61±0.03，表明其强大的再识别抵抗能力。

Conclusion: 将生物医学知识图谱直接整合到扩散模型的噪音调度中，能够同时增强合成数据的真实性和隐私保护能力，从而为罕见疾病研究实现更安全的数据共享。

Abstract: We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion
framework that generates realistic yet privacy-preserving synthetic
electronic-health-record (EHR) trajectories for ultra-rare diseases.
RareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human
Phenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA
Adverse Event Reporting System (FAERS) into a heterogeneous knowledge graph
comprising approximately 8 M typed edges. Meta-path scores extracted from this
8-million-edge KG modulate the per-token noise schedule in the forward
stochastic differential equation, steering generation toward biologically
plausible lab-medication-adverse-event co-occurrences while retaining
score-based diffusion model stability. The reverse denoiser then produces
timestamped sequences of lab-code, medication-code, and adverse-event-flag
triples that contain no protected health information. On simulated
ultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean
Discrepancy by 40 percent relative to an unguided diffusion baseline and by
greater than 60 percent versus GAN counterparts, without sacrificing downstream
predictive utility. A black-box membership-inference evaluation using the
DOMIAS attacker yields AUROC approximately 0.53, well below the 0.55
safe-release threshold and substantially better than the approximately 0.61
plus or minus 0.03 observed for non-KG baselines, demonstrating strong
resistance to re-identification. These results suggest that integrating
biomedical knowledge graphs directly into diffusion noise schedules can
simultaneously enhance fidelity and privacy, enabling safer data sharing for
rare-disease research.

</details>


### [29] [MCCE: A Framework for Multi-LLM Collaborative Co-Evolution](https://arxiv.org/abs/2510.06270)
*Nian Ran,Zhongzheng Li,Yue Wang,Qingsong Ran,Xiaoyuan Zhang,Shikun Feng,Richard Allmendinger,Xiaoguang Zhao*

Main category: cs.LG

TL;DR: MCCE是一个混合LLM框架，结合冻结闭源LLM和可训练轻量级模型，通过协同演化和强化学习解决多目标离散优化问题，在药物设计上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多目标离散优化（如分子设计）因巨大组合空间而具挑战。传统进化算法易陷局部最优。LLM虽有推理能力，但闭源LLM无法内化经验，小型开源模型又缺乏广博知识，需结合两者优势解决当前LLM优化局限性。

Method: 提出Multi-LLM Collaborative Co-evolution (MCCE)框架，结合一个冻结的闭源LLM和一个轻量级可训练模型。系统维护搜索轨迹记忆，小型模型通过强化学习逐步优化。两个模型在全局探索中相互支持、互补，并通过相互启发提升能力。

Result: 在多目标药物设计基准测试中，MCCE实现了最先进的帕累托前沿质量，并持续优于所有基线方法。

Conclusion: 该研究结果提出了一种混合LLM系统实现持续进化的新范式，有效结合了知识驱动的探索和经验驱动的学习。

Abstract: Multi-objective discrete optimization problems, such as molecular design,
pose significant challenges due to their vast and unstructured combinatorial
spaces. Traditional evolutionary algorithms often get trapped in local optima,
while expert knowledge can provide crucial guidance for accelerating
convergence. Large language models (LLMs) offer powerful priors and reasoning
ability, making them natural optimizers when expert knowledge matters. However,
closed-source LLMs, though strong in exploration, cannot update their
parameters and thus cannot internalize experience. Conversely, smaller open
models can be continually fine-tuned but lack broad knowledge and reasoning
strength. We introduce Multi-LLM Collaborative Co-evolution (MCCE), a hybrid
framework that unites a frozen closed-source LLM with a lightweight trainable
model. The system maintains a trajectory memory of past search processes; the
small model is progressively refined via reinforcement learning, with the two
models jointly supporting and complementing each other in global exploration.
Unlike model distillation, this process enhances the capabilities of both
models through mutual inspiration. Experiments on multi-objective drug design
benchmarks show that MCCE achieves state-of-the-art Pareto front quality and
consistently outperforms baselines. These results highlight a new paradigm for
enabling continual evolution in hybrid LLM systems, combining knowledge-driven
exploration with experience-driven learning.

</details>


### [30] [RVFL-X: A Novel Randomized Network Based on Complex Transformed Real-Valued Tabular Datasets](https://arxiv.org/abs/2510.06278)
*M. Sajid,Mushir Akhtar,A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: 针对随机神经网络中复数表示转换的挑战，本文提出两种实值到复值的数据转换方法，并基于此开发了复值随机向量函数连接网络RVFL-X。实验证明RVFL-X在多个数据集上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 尽管复数在神经网络中表现出卓越的表示能力，但在随机神经网络（RNNs）中，由于缺乏有效的方法将实值表格数据集转换为复值表示，其应用受到了限制。

Method: 1. 提出两种将实值数据集转换为复值表示的方法：自然变换和自编码器驱动的方法。2. 基于这些转换机制，提出RVFL-X，它是随机向量函数连接（RVFL）网络的复值扩展。RVFL-X整合了复数变换、复数输入、权重和激活函数，以处理复数表示并生成实值输出。

Result: 在80个实值UCI数据集上的综合评估表明，RVFL-X始终优于原始RVFL网络和现有最先进（SOTA）的随机神经网络变体。

Conclusion: RVFL-X展现了其在多样化应用领域的鲁棒性和有效性，证明了将复数集成到随机神经网络中的潜力。

Abstract: Recent advancements in neural networks, supported by foundational theoretical
insights, emphasize the superior representational power of complex numbers.
However, their adoption in randomized neural networks (RNNs) has been limited
due to the lack of effective methods for transforming real-valued tabular
datasets into complex-valued representations. To address this limitation, we
propose two methods for generating complex-valued representations from
real-valued datasets: a natural transformation and an autoencoder-driven
method. Building on these mechanisms, we propose RVFL-X, a complex-valued
extension of the random vector functional link (RVFL) network. RVFL-X
integrates complex transformations into real-valued datasets while maintaining
the simplicity and efficiency of the original RVFL architecture. By leveraging
complex components such as input, weights, and activation functions, RVFL-X
processes complex representations and produces real-valued outputs.
Comprehensive evaluations on 80 real-valued UCI datasets demonstrate that
RVFL-X consistently outperforms both the original RVFL and state-of-the-art
(SOTA) RNN variants, showcasing its robustness and effectiveness across diverse
application domains.

</details>


### [31] [On knot detection via picture recognition](https://arxiv.org/abs/2510.06284)
*Anne Dranowski,Yura Kabkov,Daniel Tubbenhauer*

Main category: cs.LG

TL;DR: 本文提出一种结合机器学习和传统算法的策略，旨在实现通过手机照片自动识别和分类结的目标。


<details>
  <summary>Details</summary>
Motivation: 最终目标是让手机能通过照片自动识别结。

Method: 采用混合方法，结合现代机器学习（特别是卷积神经网络和Transformer进行图像识别）和传统算法（用于计算Jones多项式等量子不变量），通过两阶段方法将感知模块与平面图（PD）代码的符号重建相结合，实现下游不变量计算。

Result: 提出了简单的基线模型，即使是轻量级的CNN和Transformer架构也能直接从图像中预测交叉数，并恢复有意义的结构信息。

Conclusion: 该两阶段方法突显了机器学习（处理嘈杂视觉数据）与不变量（强制严格拓扑区分）之间的互补性，对于鲁棒的结分类至关重要。

Abstract: Our goal is to one day take a photo of a knot and have a phone automatically
recognize it. In this expository work, we explain a strategy to approximate
this goal, using a mixture of modern machine learning methods (in particular
convolutional neural networks and transformers for image recognition) and
traditional algorithms (to compute quantum invariants like the Jones
polynomial). We present simple baselines that predict crossing number directly
from images, showing that even lightweight CNN and transformer architectures
can recover meaningful structural information. The longer-term aim is to
combine these perception modules with symbolic reconstruction into planar
diagram (PD) codes, enabling downstream invariant computation for robust knot
classification. This two-stage approach highlights the complementarity between
machine learning, which handles noisy visual data, and invariants, which
enforce rigorous topological distinctions.

</details>


### [32] [Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation](https://arxiv.org/abs/2510.06291)
*Zhiyang Zhang,Ningcong Chen,Xin Zhang,Yanhua Li,Shen Su,Hui Lu,Jun Luo*

Main category: cs.LG

TL;DR: 现有扩散模型在轨迹生成中因卷积架构导致细节丢失和偏差。本文提出Trajectory Transformer，利用Transformer骨干网络提升轨迹生成质量，有效减少偏差。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在轨迹生成时，多依赖卷积架构（如UNet）预测噪声，因模型容量限制，常导致显著偏差并丢失精细街景细节。

Method: 提出Trajectory Transformer模型，采用Transformer骨干网络进行条件信息嵌入和噪声预测。探索了位置嵌入和经纬度嵌入两种GPS坐标嵌入策略，并分析了模型在不同尺度下的性能。

Result: 在两个真实世界数据集上的实验表明，Trajectory Transformer显著提升了轨迹生成质量，并有效缓解了先前方法中存在的偏差问题。

Conclusion: Trajectory Transformer通过引入Transformer架构，成功解决了现有扩散模型在轨迹生成中存在的细节丢失和偏差问题，显著提升了轨迹生成质量。

Abstract: The widespread use of GPS devices has driven advances in spatiotemporal data
mining, enabling machine learning models to simulate human decision making and
generate realistic trajectories, addressing both data collection costs and
privacy concerns. Recent studies have shown the promise of diffusion models for
high-quality trajectory generation. However, most existing methods rely on
convolution based architectures (e.g. UNet) to predict noise during the
diffusion process, which often results in notable deviations and the loss of
fine-grained street-level details due to limited model capacity. In this paper,
we propose Trajectory Transformer, a novel model that employs a transformer
backbone for both conditional information embedding and noise prediction. We
explore two GPS coordinate embedding strategies, location embedding and
longitude-latitude embedding, and analyze model performance at different
scales. Experiments on two real-world datasets demonstrate that Trajectory
Transformer significantly enhances generation quality and effectively
alleviates the deviation issues observed in prior approaches.

</details>


### [33] [BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression](https://arxiv.org/abs/2510.06293)
*Cristian Meo,Varun Sarathchandran,Avijit Majhi,Shao Hung,Carlo Saccardi,Ruben Imhoff,Roberto Deidda,Remko Uijlenhoet,Justin Dauwels*

Main category: cs.LG

TL;DR: BlockGPT是一种新型生成式自回归Transformer模型，通过批处理分词方法预测完整的二维降水场，实现了在降水临近预报中卓越的准确性和高达31倍的推理速度提升。


<details>
  <summary>Details</summary>
Motivation: 短期降水预报（临近预报）对减轻极端天气事件影响至关重要，但现有方法（如基于token的自回归模型）存在归纳偏差和推理速度慢的问题，而扩散模型计算成本高昂，难以满足实时应用对准确性和计算效率的要求。

Method: 本文提出BlockGPT，一种生成式自回归Transformer模型，利用批处理分词（Block）方法在每个时间步预测完整的二维场（帧）。该模型通过帧内自注意力机制和跨帧因果注意力机制来分解时空。研究将BlockGPT实例化用于降水临近预报，并在KNMI（荷兰）和SEVIR（美国）两个降水数据集上进行评估，与NowcastingGPT（基于token）和DiffCast+Phydnet（基于扩散）等现有SOTA模型进行比较。

Result: BlockGPT在KNMI和SEVIR数据集上，与现有最先进的基线模型相比，实现了卓越的准确性、事件定位能力（通过分类指标衡量），并且推理速度比可比基线模型快31倍。

Conclusion: BlockGPT有效克服了现有降水临近预报方法的局限性，提供了一种高性能且高效的解决方案，在准确性和推理速度方面均超越了当前最先进的模型。

Abstract: Predicting precipitation maps is a highly complex spatiotemporal modeling
task, critical for mitigating the impacts of extreme weather events. Short-term
precipitation forecasting, or nowcasting, requires models that are not only
accurate but also computationally efficient for real-time applications. Current
methods, such as token-based autoregressive models, often suffer from flawed
inductive biases and slow inference, while diffusion models can be
computationally intensive. To address these limitations, we introduce BlockGPT,
a generative autoregressive transformer using batched tokenization (Block)
method that predicts full two-dimensional fields (frames) at each time step.
Conceived as a model-agnostic paradigm for video prediction, BlockGPT
factorizes space-time by using self-attention within each frame and causal
attention across frames; in this work, we instantiate it for precipitation
nowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI
(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines
including token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)
models. The results show that BlockGPT achieves superior accuracy, event
localization as measured by categorical metrics, and inference speeds up to 31x
faster than comparable baselines.

</details>


### [34] [SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation](https://arxiv.org/abs/2510.06303)
*Shuang Cheng,Yihan Bian,Dawei Liu,Yuhua Jiang,Yihao Liu,Linfeng Zhang,Wenhai Wang,Qipeng Guo,Kai Chen,Biqing Qi,Bowen Zhou*

Main category: cs.LG

TL;DR: SDAR是一种协同扩散-自回归范式，通过轻量级转换将预训练自回归模型变为分块扩散模型，实现了自回归模型的训练效率与扩散模型的并行推理能力结合，并在保持性能的同时显著提升了生成速度、推理能力和领域适应性。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型训练效率高但推理并行性差，而扩散模型虽能并行推理但训练成本高昂。研究旨在结合两者优势，实现既高效训练又能并行推理的模型。

Method: 提出SDAR范式，通过轻量级、数据高效的适应性转换，将已训练的自回归模型转化为分块扩散模型。推理时，模型在块间进行自回归生成以保持全局一致性，同时在每个块内通过离散扩散过程并行解码所有token。

Result: 实验表明，自回归模型比掩码扩散模型更计算高效。SDAR以极小成本实现AR到扩散的转换，保持了AR级别的性能并实现了并行生成。大规模模型研究证实SDAR可扩展，大型模型对块大小和解码阈值更鲁棒，实现更高加速比而不损失精度。SDAR还增强了推理和领域适应性，其30B MoE模型在科学推理基准（如GPQA和ChemBench）上超越了其AR对应模型，并在测试时缩放方法下进一步提升。

Conclusion: SDAR是一个实用的范式，成功结合了自回归和扩散模型的优势，实现了可扩展、高吞吐量的推理，并提升了模型在推理和多领域适应性方面的表现。

Abstract: We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies
the training efficiency of autoregressive models with the parallel inference
capability of diffusion. Instead of costly end-to-end diffusion training, SDAR
performs a lightweight paradigm conversion that transforms a well-trained
autoregressive (AR) model into a blockwise diffusion model through brief,
data-efficient adaptation. During inference, SDAR generates sequences
autoregressively across blocks for global coherence while decoding all tokens
within each block in parallel via a discrete diffusion process. Extensive
experiments show that AR models remain substantially more compute-efficient
than masked diffusion models, providing a strong foundation for adaptation.
Building on this insight, SDAR achieves efficient AR-to-diffusion conversion
with minimal cost, preserving AR-level performance while enabling parallel
generation. Scaling studies across dense and Mixture-of-Experts architectures
confirm that SDAR scales without compromise: larger models exhibit stronger
robustness to block size and decoding thresholds, yielding greater speedups
without accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning
and domain adaptability. Our 30B MoE model surpasses its AR counterpart on
challenging scientific reasoning benchmarks such as GPQA and ChemBench, and
gains further improvements under test-time scaling methods like majority voting
and pass@k. Together, these results establish SDAR as a practical paradigm that
combines the strengths of autoregression and diffusion for scalable,
high-throughput reasoning.

</details>


### [35] [Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks](https://arxiv.org/abs/2510.06349)
*Moein E. Samadi,Andreas Schuppert*

Main category: cs.LG

TL;DR: 本文分析了基础模型在复杂动态环境中的局限性，提出并论证了去中心化小智能体网络（SANs）及其群学习模式，认为其在动态决策中优于单体基础模型，但代价是细节可复现性降低。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型取得了快速发展，但其在现实世界复杂动态场景（如重症监护）中的决策能力是否能最终超越人类策略存疑。AI在这些关键应用领域进展有限，主要挑战在于使复杂系统适应动态环境，在数据和机制知识有限的情况下实现可靠的自我适应建模。

Method: 提出一种去中心化的交互式小智能体网络（SANs）架构，以此克服维度诅咒。每个智能体覆盖系统的一部分专业功能。通过多样化群体中的群学习（swarm-learning）来使SANs实现自我适应。

Result: 基于数学理论和现有应用证据，具有群学习能力的自适应SANs能在动态环境中提供优于单体基础模型的决策能力，尽管在细节可复现性方面有所降低。

Conclusion: 去中心化的SANs及其群学习范式是克服“维度诅咒”并超越单体基础模型在动态复杂环境局限性的有效替代方案。AI在承担更广泛决策角色前，需先在这些特定设置中证明其明确优越性。

Abstract: Foundation models have rapidly advanced AI, raising the question of whether
their decisions will ultimately surpass human strategies in real-world domains.
The exponential, and possibly super-exponential, pace of AI development makes
such analysis elusive. Nevertheless, many application areas that matter for
daily life and society show only modest gains so far; a prominent case is
diagnosing and treating dynamically evolving disease in intensive care.
  The common challenge is adapting complex systems to dynamic environments.
Effective strategies must optimize outcomes in systems composed of strongly
interacting functions while avoiding shared side effects; this requires
reliable, self-adaptive modeling. These tasks align with building digital twins
of highly complex systems whose mechanisms are not fully or quantitatively
understood. It is therefore essential to develop methods for self-adapting AI
models with minimal data and limited mechanistic knowledge. As this challenge
extends beyond medicine, AI should demonstrate clear superiority in these
settings before assuming broader decision-making roles.
  We identify the curse of dimensionality as a fundamental barrier to efficient
self-adaptation and argue that monolithic foundation models face conceptual
limits in overcoming it. As an alternative, we propose a decentralized
architecture of interacting small agent networks (SANs). We focus on agents
representing the specialized substructure of the system, where each agent
covers only a subset of the full system functions. Drawing on mathematical
results on the learning behavior of SANs and evidence from existing
applications, we argue that swarm-learning in diverse swarms can enable
self-adaptive SANs to deliver superior decision-making in dynamic environments
compared with monolithic foundation models, though at the cost of reduced
reproducibility in detail.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [36] [Adaptive Semantic Communication for UAV/UGV Cooperative Path Planning](https://arxiv.org/abs/2510.06901)
*Fangzhou Zhao,Yao Sun,Jianglin Lan,Lan Zhang,Xuesong Liu,Muhammad Ali Imran*

Main category: cs.NI

TL;DR: 本文提出一种语义通信框架，以在不可靠无线条件下增强无人机和无人地面车辆的协同路径规划效率，通过传输关键语义信息减少数据量并保持规划精度。


<details>
  <summary>Details</summary>
Motivation: 无人机和无人地面车辆（UAV-UGV）系统在监控、导航和应急响应等任务中，有效的路径规划至关重要。然而，在复杂环境中，无线通信通常不稳定，难以支持及时准确的路径规划，从而阻碍了系统协作效率。

Method: 本文提出一个语义通信（SemCom）框架，通过为路径规划定义关键语义并设计一个满足UAV-UGV协同路径规划需求的收发器。该方法区别于传统原始数据传输，只传输路径规划的关键信息以减少传输量而不牺牲精度。

Result: 仿真结果表明，与传统SemCom收发器相比，所提出的收发器显著减少了数据传输量，同时保持了路径规划的准确性。

Conclusion: 该语义通信框架通过优化信息传输，有效增强了不可靠无线条件下无人机/无人地面车辆的系统协作效率和路径规划能力。

Abstract: Effective path planning is fundamental to the coordination of unmanned aerial
vehicles (UAVs) and unmanned ground vehicles (UGVs) systems, particularly in
applications such as surveillance, navigation, and emergency response.
Combining UAVs' broad field of view with UGVs' ground-level operational
capability greatly improve the likelihood of successfully achieving task
objectives such as locating victims, monitoring target areas, or navigating
hazardous terrain. In complex environments, UAVs need to provide precise
environmental perception information for UGVs to optimize their routing policy.
However, due to severe interference and non-line-of-sight conditions, wireless
communication is often unstable in such complex environments, making it
difficult to support timely and accurate path planning for UAV-UGV
coordination. To this end, this paper proposes a semantic communication
(SemCom) framework to enhance UAV/UGV cooperative path planning under
unreliable wireless conditions. Unlike traditional methods that transmit raw
data, SemCom transmits only the key information for path planning, reducing
transmission volume without sacrificing accuracy. The proposed framework is
developed by defining key semantics for path planning and designing a
transceiver for meeting the requirements of UAV-UGV cooperative path planning.
Simulation results show that, compared to conventional SemCom transceivers, the
proposed transceiver significantly reduces data transmission volume while
maintaining path planning accuracy, thereby enhancing system collaboration
efficiency.

</details>


### [37] [Dynamic Control Aware Semantic Communication Enabled Image Transmission for Lunar Landing](https://arxiv.org/abs/2510.06916)
*Fangzhou Zhao,Yao Sun,Jianglin Lan,Muhammad Ali Imran*

Main category: cs.NI

TL;DR: 本文提出一种创新的语义通信框架，用于月球着陆器与轨道卫星间的图像传输，旨在克服恶劣月球环境下的通信挑战，通过动态调整传输策略，显著提升月球自主着陆的精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 月球自主着陆任务面临本地控制系统不可靠、处理高动态条件能力有限的问题，且月球表面恶劣环境（极端温差、强太阳辐射、月尘干扰）使传统通信范式失效。急需一种鲁棒且资源高效的通信方案，以支持卫星端高性能自主着陆算法并减轻着陆器计算负荷。

Method: 引入一种新颖的语义通信（SemCom）框架，用于从着陆器向操作远程着陆控制系统的卫星传输图像。该框架的核心是一个编码器-解码器，它根据着陆器控制算法的实时反馈动态调整传输策略，以确保关键图像特征的准确传递和控制可靠性。论文还提供了关于提升控制算法精度和减少端到端传输时间的理论分析。

Result: 仿真结果表明，与传统通信方法相比，本文提出的语义通信方法显著提升了自主着陆性能。

Conclusion: 所提出的语义通信方法能够显著增强月球自主着陆的性能和可靠性，为未来月球探索任务提供了新的技术途径。

Abstract: The primary challenge in autonomous lunar landing missions lies in the
unreliable local control system, which has limited capacity to handle
high-dynamic conditions, severely affecting landing precision and safety.
Recent advancements in lunar satellite communication make it possible to
establish a wireless link between lunar orbit satellites and the lunar lander.
This enables satellites to run high-performance autonomous landing algorithms,
improving landing accuracy while reducing the lander's computational and
storage load. Nevertheless, traditional communication paradigms are not
directly applicable due to significant temperature fluctuations on the lunar
surface, intense solar radiation, and severe interference caused by lunar dust
on hardware. The emerging technique of semantic communication (SemCom) offers
significant advantages in robustness and resource efficiency, particularly
under harsh channel conditions. In this paper, we introduce a novel SemCom
framework for transmitting images from the lander to satellites operating the
remote landing control system. The proposed encoder-decoder dynamically adjusts
the transmission strategy based on real-time feedback from the lander's control
algorithm, ensuring the accurate delivery of critical image features and
enhancing control reliability. We provide a rigorous theoretical analysis of
the conditions that improve the accuracy of the control algorithm and reduce
end-to-end transmission time under the proposed framework. Simulation results
demonstrate that our SemCom method significantly enhances autonomous landing
performance compared to traditional communication methods.

</details>


### [38] [A Genetic Algorithm Approach to Anti-Jamming UAV Swarm Behavior](https://arxiv.org/abs/2510.07292)
*Tiago Silva,António Grilo*

Main category: cs.NI

TL;DR: 本文提出使用遗传算法优化无人机蜂群编队、波束赋形天线和路由，以减轻主协调信道受到的干扰，仿真显示其有效性，但计算成本较高。


<details>
  <summary>Details</summary>
Motivation: 多无人机蜂群在军事战术中具有革命性潜力，但其关键的无线通信易受敌方干扰，成为蜂群的“阿喀琉斯之踵”。现有反干扰技术尚未充分利用智能蜂群行为来增强效果。

Method: 研究采用遗传算法（GAs）来联合优化无人机蜂群的编队、波束赋形天线以及流量路由，以减轻主协调信道中的干扰影响。假设编队管理信号使用一个更鲁棒的低数据速率信道。

Result: 仿真结果表明，所提出的方法能够有效减轻主协调信道中的干扰效应。

Conclusion: 该方法在减轻无人机蜂群通信干扰方面表现出有效性，但其显著的计算成本表明需要进行进一步的研究以优化和改进。

Abstract: In recent years, Unmanned Aerial Vehicles (UAVs) have brought a new true
revolution to military tactics. While UAVs already constitute an advantage when
operating alone, multi-UAV swarms expand the available possibilities, allowing
the UAVs to collaborate and support each other as a team to carry out a given
task. This entails the capability to exchange information related with
situation awareness and action coordination by means of a suitable wireless
communication technology. In such scenario, the adversary is expected to
disrupt communications by jamming the communication channel. The latter becomes
the Achilles heel of the swarm. While anti-jamming techniques constitute a well
covered topic in the literature, the use of intelligent swarm behaviors to
leverage those techniques is still an open research issue.
  This paper explores the use of Genetic Algorithms (GAs) to jointly optimize
UAV swarm formation, beam-steering antennas and traffic routing in order to
mitigate the effect of jamming in the main coordination channel, under the
assumption that a more robust and low data rate channel is used for formation
management signaling. Simulation results show the effectiveness of proposed
approach. However, the significant computational cost paves the way for further
research.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [39] [From Description to Detection: LLM based Extendable O-RAN Compliant Blind DoS Detection in 5G and Beyond](https://arxiv.org/abs/2510.06530)
*Thusitha Dayaratne,Ngoc Duy Pham,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.CR

TL;DR: 针对5G控制平面协议（RRC/NAS）的安全漏洞，本文提出一种利用大型语言模型（LLM）零样本能力的异常检测框架，在O-RAN架构下，通过自然语言描述攻击，实现了卓越的检测性能和实时实用性。


<details>
  <summary>Details</summary>
Motivation: 5G移动通信的RRC和NAS等控制平面协议存在显著安全漏洞，如盲拒绝服务（DoS）攻击。现有基于规则或传统机器学习的异常检测方法存在训练数据需求大、规则预定义和可解释性差等局限性。

Method: 本文提出一种新颖的异常检测框架，利用大型语言模型（LLM）的零样本模式能力，在Open Radio Access Network (O-RAN) 架构中处理无序数据和简短的自然语言攻击描述。该框架通过RRC/NAS数据集进行评估，并对开源和专有LLM实现进行了广泛比较。

Result: 研究分析了框架对提示变化的鲁棒性，展示了自动化攻击描述的实用性，并发现检测质量取决于描述的语义完整性而非措辞或长度。该框架在攻击检测中表现出卓越的性能，并在O-RAN的实时约束下验证了其实用性。

Conclusion: 所提出的LLM异常检测框架在5G O-RAN环境中检测RRC/NAS攻击方面表现出优越的性能和实用性，并具有检测其他三层攻击的潜力。

Abstract: The quality and experience of mobile communication have significantly
improved with the introduction of 5G, and these improvements are expected to
continue beyond the 5G era. However, vulnerabilities in control-plane
protocols, such as Radio Resource Control (RRC) and Non-Access Stratum (NAS),
pose significant security threats, such as Blind Denial of Service (DoS)
attacks. Despite the availability of existing anomaly detection methods that
leverage rule-based systems or traditional machine learning methods, these
methods have several limitations, including the need for extensive training
data, predefined rules, and limited explainability. Addressing these
challenges, we propose a novel anomaly detection framework that leverages the
capabilities of Large Language Models (LLMs) in zero-shot mode with unordered
data and short natural language attack descriptions within the Open Radio
Access Network (O-RAN) architecture. We analyse robustness to prompt variation,
demonstrate the practicality of automating the attack descriptions and show
that detection quality relies on the semantic completeness of the description
rather than its phrasing or length. We utilise an RRC/NAS dataset to evaluate
the solution and provide an extensive comparison of open-source and proprietary
LLM implementations to demonstrate superior performance in attack detection. We
further validate the practicality of our framework within O-RAN's real-time
constraints, illustrating its potential for detecting other Layer-3 attacks.

</details>


### [40] [GNN-enhanced Traffic Anomaly Detection for Next-Generation SDN-Enabled Consumer Electronics](https://arxiv.org/abs/2510.07109)
*Guan-Yan Yang,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 本文提出GNN-NAD，一个基于图神经网络的网络异常检测框架，用于下一代物联网消费电子（CE）网络。它整合SDN和CFN架构，通过GSAGE模型进行图表示学习并结合随机森林分类器，在CE环境中实现了卓越的检测性能。


<details>
  <summary>Details</summary>
Motivation: 物联网消费电子设备易受DDoS、网络攻击和远程劫持等威胁，导致功能受损和恶意代码传播。现有基于深度学习的检测系统复杂且依赖静态基础设施，不适用于动态CE网络。因此，需要一个可扩展、高效且精确的下一代CE网络异常检测系统。

Method: 本文提出了一个结合SDN和CFN的可扩展网络模型，并在此基础上设计了GNN-NAD框架。GNN-NAD将静态的、漏洞感知的攻击图与动态流量特征融合，采用一个核心的GNN模型（GSAGE）进行图表示学习，并结合随机森林（RF）分类器（GSAGE+RF）来检测网络异常。

Result: 实验评估表明，GNN-NAD（GSAGE+RF）设计在准确性、召回率、精确度和F1分数方面均表现优异，即使在小样本量下也超越了现有特征选择方法和当前的网络异常检测方法，特别是在CE环境中。

Conclusion: GNN-NAD框架有效地提高了下一代智能消费电子网络的安全性与效率，为解决CE设备面临的复杂网络攻击提供了先进的解决方案。

Abstract: Consumer electronics (CE) connected to the Internet of Things are susceptible
to various attacks, including DDoS and web-based threats, which can compromise
their functionality and facilitate remote hijacking. These vulnerabilities
allow attackers to exploit CE for broader system attacks while enabling the
propagation of malicious code across the CE network, resulting in device
failures. Existing deep learning-based traffic anomaly detection systems
exhibit high accuracy in traditional network environments but are often overly
complex and reliant on static infrastructure, necessitating manual
configuration and management. To address these limitations, we propose a
scalable network model that integrates Software-defined Networking (SDN) and
Compute First Networking (CFN) for next-generation CE networks. In this network
model, we propose a Graph Neural Networks-based Network Anomaly Detection
framework (GNN-NAD) that integrates SDN-based CE networks and enables the CFN
architecture. GNN-NAD uniquely fuses a static, vulnerability-aware attack graph
with dynamic traffic features, providing a holistic view of network security.
The core of the framework is a GNN model (GSAGE) for graph representation
learning, followed by a Random Forest (RF) classifier. This design (GSAGE+RF)
demonstrates superior performance compared to existing feature selection
methods. Experimental evaluations on CE environment reveal that GNN-NAD
achieves superior metrics in accuracy, recall, precision, and F1 score, even
with small sample sizes, exceeding the performance of current network anomaly
detection methods. This work advances the security and efficiency of
next-generation intelligent CE networks.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [41] [Memory-Augmented Generative AI for Real-time Wireless Prediction in Dynamic Industrial Environments](https://arxiv.org/abs/2510.06884)
*Rahul Gulia,Amlan Ganguly,Michael E. Kuhl,Ehsan Rashedi,Clark Hochgraf*

Main category: eess.SP

TL;DR: 本文提出Evo-WISVA，一种新颖的深度学习架构，作为无线电环境的预测数字孪生，旨在为动态工业4.0环境提供无线信道条件的实时精准预测，其性能显著优于现有基线，并展现出卓越的泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在高度动态的工业4.0环境中，实现超可靠低延迟通信（URLLC）需要对无线信道条件（特别是SINR）进行准确、实时的预测。然而，传统物理或统计模型难以应对智能仓库中移动障碍物和瞬态干扰带来的复杂时空变化。

Method: 引入Evo-WISVA，一种协同深度学习架构，作为轻量级2D预测数字孪生。它融合了：1) 带有注意力驱动潜在记忆模块（LMM）的记忆增强变分自编码器（VAE），用于鲁棒、上下文感知的空间特征提取；2) 卷积长短期记忆（ConvLSTM）网络，用于精确的时间预测和序列细化。整个流程通过联合损失函数进行端到端优化。

Result: 在ns-3生成的高保真工业仓库数据集上，Evo-WISVA的平均重建误差降低高达47.6%，显著优于现有最先进基线。该模型对具有极高动态复杂性（多达十个同时移动障碍物）的未知环境表现出卓越的泛化能力，同时保持了实时部署所需的计算效率。

Conclusion: Evo-WISVA为前瞻性无线资源管理提供了一项基础技术，能够实现自主优化，并推动了工业通信网络中预测数字孪生的实现。

Abstract: Accurate and real-time prediction of wireless channel conditions,
particularly the Signal-to-Interference-plus-Noise Ratio (SINR), is a
foundational requirement for enabling Ultra-Reliable Low-Latency Communication
(URLLC) in highly dynamic Industry 4.0 environments. Traditional physics-based
or statistical models fail to cope with the spatio-temporal complexities
introduced by mobile obstacles and transient interference inherent to smart
warehouses. To address this, we introduce Evo-WISVA (Evolutionary Wireless
Infrastructure for Smart Warehouse using VAE), a novel synergistic deep
learning architecture that functions as a lightweight 2D predictive digital
twin of the radio environment. Evo-WISVA integrates a memory-augmented
Variational Autoencoder (VAE) featuring an Attention-driven Latent Memory
Module (LMM) for robust, context-aware spatial feature extraction, with a
Convolutional Long Short-Term Memory (ConvLSTM) network for precise temporal
forecasting and sequential refinement. The entire pipeline is optimized
end-to-end via a joint loss function, ensuring optimal feature alignment
between the generative and predictive components. Rigorous experimental
evaluation conducted on a high-fidelity ns-3-generated industrial warehouse
dataset demonstrates that Evo-WISVA significantly surpasses state-of-the-art
baselines, achieving up to a 47.6\% reduction in average reconstruction error.
Crucially, the model exhibits exceptional generalization capacity to unseen
environments with vastly increased dynamic complexity (up to ten simultaneously
moving obstacles) while maintaining amortized computational efficiency
essential for real-time deployment. Evo-WISVA establishes a foundational
technology for proactive wireless resource management, enabling autonomous
optimization and advancing the realization of predictive digital twins in
industrial communication networks.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [42] [Advantages of Global Entanglement-Distillation Policies in Quantum Repeater Chains](https://arxiv.org/abs/2510.06737)
*Iftach Yakar,Michael Ben-Or*

Main category: quant-ph

TL;DR: 在量子中继器中，全局确定性纠缠蒸馏策略在通信速率上优于局部策略，尤其在长距离链中能显著提高密钥生成率。


<details>
  <summary>Details</summary>
Motivation: 量子中继器对于实现长距离量子通信至关重要，因为光子损耗随距离呈指数增长。当前中继器使用纠缠蒸馏协议，其决策依赖于局部或全局信息。鉴于近期研究倾向于使用局部确定性决策策略，本研究旨在探讨全局确定性策略是否能在通信速率方面超越局部策略。

Method: 通过模拟等距中继器链，并在双向经典通信的辅助下，比较了局部和全局纠缠蒸馏决策策略。模拟涵盖了广泛的距离以及变化的网络和硬件参数。

Result: 研究发现，全局确定性策略始终优于局部策略，在某些情况下甚至决定了秘密通信是否可行。对于大型中继器链（N>512），全局策略能将秘密密钥生成率（SKR）提高两个数量级。

Conclusion: 这些结果表明，量子中继器链中的局部蒸馏决策可能不是最优的，并应为未来的协议设计提供参考。

Abstract: Quantum repeaters are essential for achieving long-distance quantum
communication due to photon loss, which grows exponentially with the channel
distance. Current quantum repeater generations use entanglement distillation
protocols, where the decision of when to perform distillation depends on either
local or global knowledge. Recent approaches for quantum repeaters, such as
Mantri et al. (arXiv:2409.06152), consider using deterministic local decision
policies for entanglement distillation. We ask whether global deterministic
policies outperform local ones in terms of communication rate. We simulate
equidistant repeater chains, assisted by two-way classical communication, and
compare local and global policies for distillation decisions, spanning large
distances and varying network and hardware parameters. Our findings show that
global deterministic policies consistently outperform these local ones, and in
some cases, determine whether secret communication is possible. For large
repeater chains ($N>512$), global policies improve SKR by two orders of
magnitude. These results suggest that local distillation decisions in quantum
repeater chains may not be optimal, and may inform future protocol design.

</details>
