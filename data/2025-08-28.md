<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 6]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.NE](#cs.NE) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts](https://arxiv.org/abs/2508.19268)
*Qing Wang,Xue Han,Jiahui Wang,Lehao Xing,Qian Hu,Lianlian Zhang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 本文提出MultiPL-MoE模型，通过结合token级和segment级的混合专家（MoE）结构，有效提升大型语言模型（LLMs）在多编程语言代码生成方面的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成方面表现出色，但多语种代码生成仍极具挑战性。研究旨在利用有限计算资源，在保留现有优势的同时，提高LLMs在多编程语言（MultiPL）方面的性能。

Method: 将MultiPL视为多自然语言的特例，提出一种混合专家（MoE）架构的LLM扩展——MultiPL-MoE。该模型结合了两个配对的MoE，分别在token级和segment级优化专家选择。token级MoE采用标准升级MoE结构，包含共享专家和新颖的门控权重归一化方法；segment级MoE则创新性地使用滑动窗口对输入序列进行分段，并采用专家选择路由策略，允许专家选择Top-k分段，以更好地捕捉编程语言的句法结构和上下文模式。

Result: 实验结果证明了MultiPL-MoE的有效性。

Conclusion: MultiPL-MoE是一种有效的解决方案，能够提升LLMs在多编程语言代码生成任务上的表现。

Abstract: Despite LLMs' excellent code creation capabilities, multilingual code
generation remains extremely challenging. To address this, we intent to improve
the multi-programming-lingual (MultiPL) performance of the base LLMs while
retaining the most popular ones using restricted computational resources. We
consider MultiPL to be a special case of multiple natural languages and propose
a MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called
MultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize
expert selection at both the token and segment levels. The token-level MoE is a
standard upcycling MoE structure with a shared expert and a novel gate weight
normalization approach that aids in the final fusion with the segment-level
MoE. The segment-level MoE incorporates two innovative designs to better
capture the syntactic structure and contextual patterns of programming
languages: First, using a sliding window to partition the input token sequence
into multiple segments; Then, adopting an expert-choice routing strategy that
allows experts to select the top-k segments. The results of the experiment
proved the effectiveness of MultiPL-MoE.

</details>


### [2] [Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English](https://arxiv.org/abs/2508.19270)
*Nguyen Huu Nhat Minh,Tran Nguyen Anh,Truong Dinh Dung,Vo Van Nam,Le Pham Tuyen*

Main category: cs.CL

TL;DR: 本文提出一种结合双语音素集和PhoWhisper编码器的端到端系统，以解决越南语和英语混合发音的跨语言音素识别挑战，有效提高了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 在越南语和英语混合发音场景中，准确的自动语音识别（ASR）面临跨语言音素识别的重大挑战。越南语依赖声调区分词义，而英语的重音模式和非标准发音特性，阻碍了两种语言间的音素对齐。

Method: 研究提出了两项主要贡献：1) 构建一个能弥合越南语和英语语音系统差异的双语音素集；2) 设计一个利用PhoWhisper预训练编码器获取深度高层表示的端到端系统，以增强音素识别能力。

Result: 广泛实验证明，所提出的方法显著提高了越南语双语语音识别的准确率。

Conclusion: 该方法不仅提升了双语语音识别的准确性，还为解决基于声调和重音的复杂音素识别问题提供了一个鲁棒的框架。

Abstract: Cross-lingual phoneme recognition has emerged as a significant challenge for
accurate automatic speech recognition (ASR) when mixing Vietnamese and English
pronunciations. Unlike many languages, Vietnamese relies on tonal variations to
distinguish word meanings, whereas English features stress patterns and
non-standard pronunciations that hinder phoneme alignment between the two
languages. To address this challenge, we propose a novel bilingual speech
recognition approach with two primary contributions: (1) constructing a
representative bilingual phoneme set that bridges the differences between
Vietnamese and English phonetic systems; (2) designing an end-to-end system
that leverages the PhoWhisper pre-trained encoder for deep high-level
representations to improve phoneme recognition. Our extensive experiments
demonstrate that the proposed approach not only improves recognition accuracy
in bilingual speech recognition for Vietnamese but also provides a robust
framework for addressing the complexities of tonal and stress-based phoneme
recognition

</details>


### [3] [Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT](https://arxiv.org/abs/2508.19271)
*Rushitha Santhoshi Mamidala,Anshuman Chhabra,Ankur Mali*

Main category: cs.CL

TL;DR: 本文提出将RetoMaton框架的全局数据存储替换为局部、任务自适应的加权有限自动机（WFA），直接从外部语料库构建。这种方法在LLaMA和Gemma模型上，显著优于基础模型和基于提示的方法，提供了更可靠、可解释和可重现的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示词的LLM推理策略（如CoT、ICL）机制脆弱、输出不稳定、不可靠，难以满足需要稳定、可解释推理的任务需求。因此，需要更结构化、可信赖的神经符号框架来解决这一问题。

Method: 扩展了基于自动机的神经符号框架RetoMaton，将其全局数据存储替换为局部、任务自适应的加权有限自动机（WFA）。该WFA直接从外部领域语料库构建，以符号记忆和确定性转换来锚定检索，提供可验证和模块化的检索行为。方法在LLaMA-3.2-1B和Gemma-3-1B-PT两个预训练LLM上，通过TriviaQA、GSM8K和MMLU三个推理任务进行评估，并与基础模型和基于提示的方法进行比较。

Result: 与基础模型和基于提示的方法相比，使用局部RetoMaton增强LLM的方法在所有评估设置中持续提升了性能。同时，该方法实现了透明且可复现的检索动态。

Conclusion: 研究结果表明，通过轻量级、自动机引导的记忆机制，结合局部RetoMaton，有望实现现代LLM中可信赖的符号推理。这种方法提供明确的WFA结构，支持可验证、模块化的检索行为，更适用于领域迁移和互操作性。

Abstract: Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and
In-Context Learning (ICL) have become widely used for eliciting reasoning
capabilities in large language models (LLMs). However, these methods rely on
fragile, implicit mechanisms often yielding inconsistent outputs across seeds,
formats, or minor prompt variations making them fundamentally unreliable for
tasks requiring stable, interpretable reasoning. In contrast, automata-based
neuro-symbolic frameworks like RetoMaton offer a more structured and
trustworthy alternative by grounding retrieval in symbolic memory with
deterministic transitions. In this work, we extend RetoMaton by replacing its
global datastore with a local, task-adaptive Weighted Finite Automaton (WFA),
constructed directly from external domain corpora. This local automaton
structure promotes robust, context-aware retrieval while preserving symbolic
traceability and low inference overhead. Unlike prompting, which entangles
context and memory in opaque ways, our approach leverages the explicit
structure of WFAs to provide verifiable and modular retrieval behavior, making
it better suited for domain transfer and interoperability. We evaluate this
local RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT
across three reasoning tasks: TriviaQA (reading comprehension), GSM8K
(multi-step math), and MMLU (domain knowledge). Compared to the base model and
prompting-based methods, augmenting these setups with local RetoMaton
consistently improves performance while enabling transparent and reproducible
retrieval dynamics. Our results highlight a promising shift toward trustworthy,
symbolic reasoning in modern LLMs via lightweight, automaton-guided memory.

</details>


### [4] [RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits](https://arxiv.org/abs/2508.19272)
*Kshitij Fadnis,Sara Rosenthal,Maeda Hanafi,Yannis Katsis,Marina Danilevsky*

Main category: cs.CL

TL;DR: RAGAPHENE是一个基于聊天的标注平台，旨在通过模拟真实世界对话来构建多轮RAG对话基准，以评估LLM的性能并解决其幻觉问题。


<details>
  <summary>Details</summary>
Motivation: LLM可能生成包含幻觉信息但表面正确的答案，因此在事实准确性至关重要的场景中，检索增强生成（RAG）至关重要。需要构建能够评估LLM多轮RAG对话能力的基准，并且模拟真实世界对话对于生成高质量评估基准至关重要。

Method: 开发了一个名为RAGAPHENE的基于聊天的标注平台。该平台允许标注者模拟真实世界的对话。

Result: RAGAPHENE已被大约40名标注者成功使用，并构建了数千个真实世界的对话。

Conclusion: RAGAPHENE是一个有效的工具，能够帮助构建用于评估LLM的多轮RAG对话的高质量、真实世界基准，从而应对LLM可能产生幻觉的挑战。

Abstract: Retrieval Augmented Generation (RAG) is an important aspect of conversing
with Large Language Models (LLMs) when factually correct information is
important. LLMs may provide answers that appear correct, but could contain
hallucinated information. Thus, building benchmarks that can evaluate LLMs on
multi-turn RAG conversations has become an increasingly important task.
Simulating real-world conversations is vital for producing high quality
evaluation benchmarks. We present RAGAPHENE, a chat-based annotation platform
that enables annotators to simulate real-world conversations for benchmarking
and evaluating LLMs. RAGAPHENE has been successfully used by approximately 40
annotators to build thousands of real-world conversations.

</details>


### [5] [Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis](https://arxiv.org/abs/2508.19274)
*Yue Chu*

Main category: cs.CL

TL;DR: 本论文探讨利用预训练语言模型和机器学习，通过口述尸检（VA）中的叙述文本进行死因（COD）自动化分类。研究表明，叙述文本单独或结合结构化问题，能显著提高分类准确性，并强调了叙述文本的价值和高质量数据的需求。


<details>
  <summary>Details</summary>
Motivation: 现有自动化口述尸检（VA）算法仅使用结构化问题，忽略了叙述性文本中可能包含的重要死因信息。研究动机在于探索如何利用VA叙述文本，通过预训练语言模型和机器学习技术，提升死因（COD）自动化分类的准确性，以更好地支持公共卫生决策。

Method: 研究采用基于Transformer的预训练语言模型（PLMs）对南非的实证数据进行任务特定微调，以利用VA叙述文本进行死因分类。此外，还探索了结合叙述文本和结构化问题的多模态融合策略，并分析了医生感知的VA信息充分性及其对分类准确性的影响。

Result: 1. 仅使用叙述文本的基于Transformer的预训练语言模型在个体和群体层面上均优于现有仅使用问题的算法，尤其在识别非传染性疾病方面表现突出。2. 多模态融合策略进一步提高了死因分类性能，证实了各模态的独特贡献。3. 信息充分性因年龄和死因而异，并会影响医生和模型的分类准确性。

Conclusion: 本论文证明了口述尸检（VA）叙述文本在提高死因（COD）自动化分类方面的显著价值。研究结果强调了需要更多来自多样化背景的高质量数据来训练和微调PLM/ML方法，并为重新思考和设计VA工具及访谈提供了宝贵见解，推进了自然语言处理、流行病学和全球健康领域的交叉知识。

Abstract: In countries without civil registration and vital statistics, verbal autopsy
(VA) is a critical tool for estimating cause of death (COD) and inform policy
priorities. In VA, interviewers ask proximal informants for details on the
circumstances preceding a death, in the form of unstructured narratives and
structured questions. Existing automated VA cause classification algorithms
only use the questions and ignore the information in the narratives. In this
thesis, we investigate how the VA narrative can be used for automated COD
classification using pretrained language models (PLMs) and machine learning
(ML) techniques. Using empirical data from South Africa, we demonstrate that
with the narrative alone, transformer-based PLMs with task-specific fine-tuning
outperform leading question-only algorithms at both the individual and
population levels, particularly in identifying non-communicable diseases. We
explore various multimodal fusion strategies combining narratives and questions
in unified frameworks. Multimodal approaches further improve performance in COD
classification, confirming that each modality has unique contributions and may
capture valuable information that is not present in the other modality. We also
characterize physician-perceived information sufficiency in VA. We describe
variations in sufficiency levels by age and COD and demonstrate that
classification accuracy is affected by sufficiency for both physicians and
models. Overall, this thesis advances the growing body of knowledge at the
intersection of natural language processing, epidemiology, and global health.
It demonstrates the value of narrative in enhancing COD classification. Our
findings underscore the need for more high-quality data from more diverse
settings to use in training and fine-tuning PLM/ML methods, and offer valuable
insights to guide the rethinking and redesign of the VA instrument and
interview.

</details>


### [6] [FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series](https://arxiv.org/abs/2508.19279)
*Gunjan Jalori,Preetika Verma,Sercan Ö Arık*

Main category: cs.CL

TL;DR: 本文提出FLAIRR-TS，一个基于智能体（agentic system）的测试时提示优化框架，旨在通过自适应提示细化和检索，使冻结的LLM在无需大量预处理、微调或手动提示工程的情况下，有效进行时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用LLM进行时间序列预测时，要么需要大量的预处理和微调，要么需要耗时且特设（ad-hoc）地手动设计高质量的自然语言提示，这些都效率低下且难以推广。

Method: 引入FLAIRR-TS框架，包含一个Forecaster智能体（根据初始提示生成预测）和一个Refiner智能体（根据历史输出和检索到的类似案例优化提示）。该方法采用自适应提示技术和创造性提示模板，无需中间代码生成即可跨领域生成高质量预测。

Result: 在基准数据集上的实验表明，FLAIRR-TS的准确性优于静态提示和检索增强基线，其性能接近于专门设计的提示（specialized prompts）。

Conclusion: FLAIRR-TS通过其基于智能体的自适应提示优化和检索方法，为LLM时间序列预测提供了一种实用的替代方案，避免了传统微调的需求，并实现了强大的预测性能。

Abstract: Time series Forecasting with large languagemodels (LLMs) requires bridging
numericalpatterns and natural language. Effective fore-casting on LLM often
relies on extensive pre-processing and fine-tuning.Recent studiesshow that a
frozen LLM can rival specializedforecasters when supplied with a carefully
en-gineered natural-language prompt, but craft-ing such a prompt for each task
is itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt
optimization framework thatutilizes an agentic system: a
Forecaster-agentgenerates forecasts using an initial prompt,which is then
refined by a refiner agent, in-formed by past outputs and retrieved
analogs.This adaptive prompting generalizes across do-mains using creative
prompt templates andgenerates high-quality forecasts without inter-mediate code
generation.Experiments onbenchmark datasets show improved accuracyover static
prompting and retrieval-augmentedbaselines, approaching the performance
ofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,
achievingstrong performance via its agentic approach toadaptive prompt
refinement and retrieval.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [7] [Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration](https://arxiv.org/abs/2508.19254)
*Jookyung Song,Mookyoung Kang,Nojun Kwak*

Main category: cs.CV

TL;DR: 本文提出一个实时生成式绘图系统，能整合草图中的形式意图和上下文意图，实现人机协同创作。


<details>
  <summary>Details</summary>
Motivation: 现有文本提示式生成系统主要捕获高层上下文描述，未能同时分析草图中的底层几何特征和高层语义线索，从而无法全面捕捉用户的创作意图。

Method: 系统同时分析草图的底层几何特征（如线条轨迹、比例、空间布局）和通过视觉-语言模型提取的高层语义线索。这些双重意图信号在多阶段生成管道中联合调整，该管道结合了轮廓保持的结构控制和风格与内容感知的图像合成。系统采用触摸屏界面和分布式推理架构。

Result: 该系统实现了低延迟的两阶段转换，并支持在共享画布上进行多用户协作。它使所有参与者，无论艺术专长如何，都能参与同步、共同创作的视觉创作。

Conclusion: 该平台将人机交互重新定义为协同创作和相互增强的过程，使不同艺术水平的用户都能参与到创造性的视觉工作中。

Abstract: This paper presents a real-time generative drawing system that interprets and
integrates both formal intent - the structural, compositional, and stylistic
attributes of a sketch - and contextual intent - the semantic and thematic
meaning inferred from its visual content - into a unified transformation
process. Unlike conventional text-prompt-based generative systems, which
primarily capture high-level contextual descriptions, our approach
simultaneously analyzes ground-level intuitive geometric features such as line
trajectories, proportions, and spatial arrangement, and high-level semantic
cues extracted via vision-language models. These dual intent signals are
jointly conditioned in a multi-stage generation pipeline that combines
contour-preserving structural control with style- and content-aware image
synthesis. Implemented with a touchscreen-based interface and distributed
inference architecture, the system achieves low-latency, two-stage
transformation while supporting multi-user collaboration on shared canvases.
The resulting platform enables participants, regardless of artistic expertise,
to engage in synchronous, co-authored visual creation, redefining human-AI
interaction as a process of co-creation and mutual enhancement.

</details>


### [8] [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/abs/2508.19257)
*Chenghao Liu,Jiachen Zhang,Chengxuan Li,Zhimu Zhou,Shixin Wu,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: VLA模型忽视时序信息，易受噪声影响。本文提出训练无关的Temporal Token Fusion (TTF) 方法，通过智能融合历史和当前视觉表示，提升VLA推理质量。TTF结合像素差异分析和注意力机制进行选择性时间令牌融合，在多个基准和真实机器人任务中显著提高了成功率，并揭示了选择性Query矩阵重用可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在处理机器人操作任务时，独立处理每一时间步的视觉输入，忽略了宝贵的时序信息和连续帧之间的内在一致性。这种逐帧处理方式使模型易受视觉噪声影响。

Method: 提出了一种训练无关的Temporal Token Fusion (TTF) 方法。该方法通过智能融合历史和当前的视觉表示来增强VLA推理质量。TTF采用双维度检测机制，结合高效的灰度像素差异分析和基于注意力的语义相关性评估，通过硬融合策略和关键帧锚定实现选择性时间令牌融合，以防止错误累积。

Result: 实验结果显示：在LIBERO数据集上平均提高了4.0个百分点（72.4% vs 68.4%基线）；在SimplerEnv上进行了跨环境验证，相对改进了4.8%；在真实机器人任务上相对改进了8.7%。该方法具有模型无关性，适用于OpenVLA和VLA-Cache架构。

Conclusion: TTF通过智能融合时序视觉信息显著提升了VLA模型的性能和鲁棒性。研究还揭示，在注意力机制中选择性地重用Query矩阵不仅不会损害性能，反而能增强性能，这为通过直接KQV矩阵重用来实现计算加速并提高任务成功率提供了有前景的研究方向。

Abstract: Vision-Language-Action (VLA) models process visual inputs independently at
each timestep, discarding valuable temporal information inherent in robotic
manipulation tasks. This frame-by-frame processing makes models vulnerable to
visual noise while ignoring the substantial coherence between consecutive
frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a
training-free approach that intelligently integrates historical and current
visual representations to enhance VLA inference quality. Our method employs
dual-dimension detection combining efficient grayscale pixel difference
analysis with attention-based semantic relevance assessment, enabling selective
temporal token fusion through hard fusion strategies and keyframe anchoring to
prevent error accumulation. Comprehensive experiments across LIBERO,
SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0
percentage points average on LIBERO (72.4\% vs 68.4\% baseline),
cross-environment validation on SimplerEnv (4.8\% relative improvement), and
8.7\% relative improvement on real robot tasks. Our approach proves
model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,
TTF reveals that selective Query matrix reuse in attention mechanisms enhances
rather than compromises performance, suggesting promising directions for direct
KQV matrix reuse strategies that achieve computational acceleration while
improving task success rates.

</details>


### [9] [Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation](https://arxiv.org/abs/2508.19289)
*Tai Inui,Steven Oh,Magdeline Kuan*

Main category: cs.CV

TL;DR: 该论文提出了一种无监督的幻灯片质量评估流程，结合了七种视觉设计指标和CLIP-ViT嵌入，通过隔离森林模型，在评估幻灯片质量方面与人类评分高度相关（Pearson r=0.83），并显著优于大型视觉-语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏可扩展、客观且能与观众感知对齐的幻灯片质量反馈。研究旨在提供一种实时、客观的评估方法，以近似人类对幻灯片视觉质量的感知。

Method: 开发了一个无监督的幻灯片质量评估流程，结合了七种专家启发的视觉设计指标（如空白、色彩和谐、布局平衡等）和CLIP-ViT嵌入，并利用基于隔离森林的异常评分进行评估。该方法在1.2万张专业讲座幻灯片上训练，并在115张幻灯片上进行评估。

Result: 该方法与人类视觉质量评分的皮尔逊相关性高达0.83，比领先的视觉-语言模型（如ChatGPT、Claude、Gemini）强1.79至3.23倍。同时，验证了与视觉评分的聚合效度、与演讲者交付评分的判别效度以及与整体印象的探索性一致性。

Conclusion: 通过多模态嵌入增强低级设计线索，能够紧密近似观众对幻灯片质量的感知。该方法为幻灯片质量评估提供了可扩展、客观且实时的反馈。

Abstract: We present an unsupervised slide-quality assessment pipeline that combines
seven expert-inspired visual-design metrics (whitespace, colorfulness, edge
density, brightness contrast, text density, color harmony, layout balance) with
CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate
presentation slides. Trained on 12k professional lecture slides and evaluated
on six academic talks (115 slides), our method achieved Pearson correlations up
to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores
from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude
Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual
ratings, discriminant validity against speaker-delivery scores, and exploratory
alignment with overall impressions. Our results show that augmenting low-level
design cues with multimodal embeddings closely approximates audience
perceptions of slide quality, enabling scalable, objective feedback in real
time.

</details>


### [10] [Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation](https://arxiv.org/abs/2508.19290)
*Alexandros Gkillas,Ioulia Kapsali,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CV

TL;DR: 针对自动驾驶中2D距离视图LiDAR分割易受对抗攻击的问题，本文提出了一种高效且计算开销极低的模型净化框架，该框架基于数学优化构建，并在基准测试和实际部署中均展现出卓越的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LiDAR分割在自动驾驶中至关重要，但现代分割网络易受对抗攻击。现有防御方法主要针对原始3D点云且计算量大。许多先进的LiDAR分割流水线采用更高效的2D距离视图表示，但该领域缺乏专用且轻量级的对抗防御方案。

Method: 引入了一个专为2D距离视图LiDAR分割设计的高效模型净化框架。提出了在距离视图域的直接攻击公式，并开发了一个基于数学优化问题的可解释净化网络，旨在以最小的计算开销实现强大的对抗弹性。

Result: 该方法在公开基准测试中取得了有竞争力的性能，并持续优于生成式和对抗训练基线方法。更重要的是，在演示车辆上的真实世界部署验证了该框架在实际自动驾驶场景中提供准确操作的能力。

Conclusion: 本文提出的净化框架为2D距离视图LiDAR分割提供了一种高效、鲁棒且计算开销低的对抗防御解决方案。它不仅在性能上超越了现有方法，而且通过真实世界部署验证了其在实际自动驾驶场景中的有效性，显著提升了系统安全性。

Abstract: LiDAR-based segmentation is essential for reliable perception in autonomous
vehicles, yet modern segmentation networks are highly susceptible to
adversarial attacks that can compromise safety. Most existing defenses are
designed for networks operating directly on raw 3D point clouds and rely on
large, computationally intensive generative models. However, many
state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D
range view representations. Despite their widespread adoption, dedicated
lightweight adversarial defenses for this domain remain largely unexplored. We
introduce an efficient model-based purification framework tailored for
adversarial defense in 2D range-view LiDAR segmentation. We propose a direct
attack formulation in the range-view domain and develop an explainable
purification network based on a mathematical justified optimization problem,
achieving strong adversarial resilience with minimal computational overhead.
Our method achieves competitive performance on open benchmarks, consistently
outperforming generative and adversarial training baselines. More importantly,
real-world deployment on a demo vehicle demonstrates the framework's ability to
deliver accurate operation in practical autonomous driving scenarios.

</details>


### [11] [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: 本综述深入探讨了大型视觉语言模型（LVLMs）在目标检测领域的最新进展，分析了其架构、训练范式、与传统方法的对比、局限性及未来发展方向，并预测了其在目标检测和机器人应用中的变革性影响。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习目标检测方法在适应性、上下文推理和泛化能力方面存在局限。大型视觉语言模型（LVLMs）通过融合语言和视觉，显著提升了这些能力，因此需要对该领域的最新进展进行深入回顾和分析。

Method: 本文采用深入综述的方法，通过三步研究回顾过程系统地探索了LVLMs的最新进展。具体包括：讨论视觉语言模型（VLMs）在目标检测中的工作原理；解释LVLMs的架构创新、训练范式和输出灵活性；审查视觉与文本信息融合的方法；通过可视化展示LVLMs在多场景下的有效性；并与传统深度学习系统在实时性能、适应性和复杂性方面进行比较。

Result: 综述展示了LVLMs在包括定位和分割在内的多种场景下的有效性，突出了其先进的上下文理解能力。研究预计LVLMs将很快达到或超越传统目标检测方法的性能。此外，综述还指出了当前LVLMs的主要局限性，并提出了解决方案及未来的发展路线图。

Conclusion: 基于本研究，大型视觉语言模型（LVLMs）的最新进展已经并将继续对目标检测及未来的机器人应用产生变革性影响。

Abstract: The fusion of language and vision in large vision-language models (LVLMs) has
revolutionized deep learning-based object detection by enhancing adaptability,
contextual reasoning, and generalization beyond traditional architectures. This
in-depth review presents a structured exploration of the state-of-the-art in
LVLMs, systematically organized through a three-step research review process.
First, we discuss the functioning of vision language models (VLMs) for object
detection, describing how these models harness natural language processing
(NLP) and computer vision (CV) techniques to revolutionize object detection and
localization. We then explain the architectural innovations, training
paradigms, and output flexibility of recent LVLMs for object detection,
highlighting how they achieve advanced contextual understanding for object
detection. The review thoroughly examines the approaches used in integration of
visual and textual information, demonstrating the progress made in object
detection using VLMs that facilitate more sophisticated object detection and
localization strategies. This review presents comprehensive visualizations
demonstrating LVLMs' effectiveness in diverse scenarios including localization
and segmentation, and then compares their real-time performance, adaptability,
and complexity to traditional deep learning systems. Based on the review, its
is expected that LVLMs will soon meet or surpass the performance of
conventional methods in object detection. The review also identifies a few
major limitations of the current LVLM modes, proposes solutions to address
those challenges, and presents a clear roadmap for the future advancement in
this field. We conclude, based on this study, that the recent advancement in
LVLMs have made and will continue to make a transformative impact on object
detection and robotic applications in the future.

</details>


### [12] [Large VLM-based Stylized Sports Captioning](https://arxiv.org/abs/2508.19295)
*Sauptik Dhar,Nicholas Buoncristiani,Joe Anakata,Haoyu Zhang,Michelle Munson*

Main category: cs.CV

TL;DR: 本文提出了一种两级微调的LVLM管道，以解决现有大视觉语言模型在生成包含领域专业术语和风格化描述的生产级体育赛事图像字幕方面的不足，显著提升了准确性和效率，并成功应用于现场体育新闻报道。


<details>
  <summary>Details</summary>
Motivation: 现有大型（视觉）语言模型（LLM/LVLM）在多个领域取得成功，但鲜少关注体育领域，尤其缺乏对比赛的准确识别和自然语言描述。大多数现有模型难以生成包含足够领域特定术语的自然（类人）描述，也无法产出满足生产级要求的风格化体育字幕。

Method: 为解决现有最先进LLM/LVLMs在生成生产级、风格化体育图像字幕方面的局限性，本文提出了一种两级微调（two-level fine-tuned）的LVLM管道。

Result: 该管道与替代方法相比，F1分数提升了8-10%以上，BERT分数提升了2-10%以上。此外，它具有小的运行时内存占用和快速的执行时间。在Super Bowl LIX期间，该管道在3-5秒内为6张图像生成高度准确和风格化的字幕，总计超过1000张图像，验证了其在实时专业体育新闻报道中的实际应用价值。

Conclusion: 所提出的两级微调LVLM管道有效提升了体育赛事图像字幕的准确性和风格化水平，并以高效能应用于现场体育新闻报道，克服了现有模型在该领域的不足。

Abstract: The advent of large (visual) language models (LLM / LVLM) have led to a
deluge of automated human-like systems in several domains including social
media content generation, search and recommendation, healthcare prognosis, AI
assistants for cognitive tasks etc. Although these systems have been
successfully integrated in production; very little focus has been placed on
sports, particularly accurate identification and natural language description
of the game play. Most existing LLM/LVLMs can explain generic sports
activities, but lack sufficient domain-centric sports' jargon to create natural
(human-like) descriptions. This work highlights the limitations of existing
SoTA LLM/LVLMs for generating production-grade sports captions from images in a
desired stylized format, and proposes a two-level fine-tuned LVLM pipeline to
address that. The proposed pipeline yields an improvement > 8-10% in the F1,
and > 2-10% in BERT score compared to alternative approaches. In addition, it
has a small runtime memory footprint and fast execution time. During Super Bowl
LIX the pipeline proved its practical application for live professional sports
journalism; generating highly accurate and stylized captions at the rate of 6
images per 3-5 seconds for over 1000 images during the game play.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [13] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: 该研究提出将大型语言模型（LLMs）中的奉承行为建模为心理测量特质（如情绪性、开放性）的几何和因果组合，并利用向量方法进行可解释的干预以减轻其风险。


<details>
  <summary>Details</summary>
Motivation: LLMs中的奉承行为是一个关键的行为风险，但通常被视为单一因果机制下的孤立故障，缺乏对其复杂性的深入理解和有效的缓解策略。

Method: 通过将奉承行为视为心理测量特质（如情绪性、开放性、随和性）的几何和因果组合。采用对比激活加法（CAA）技术，将激活方向映射到这些心理特质上，并研究不同特质组合如何导致奉承行为的产生。

Result: 该研究方法能够从心理特质组合层面理解奉承行为的产生机制，例如高外向性结合低责任心。这种视角为LLMs中安全关键行为的干预提供了可解释且组合性的向量操作方法（如加法、减法、投影）。

Conclusion: 将奉承行为建模为心理特质的组合，为缓解LLMs中的安全关键行为提供了一种新颖、可解释且基于向量的干预框架。

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [14] [Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science](https://arxiv.org/abs/2508.19383)
*Daoyuan Jin,Nick Gunner,Niko Carvajal Janke,Shivranjani Baruah,Kaitlin M. Gold,Yu Jiang*

Main category: cs.AI

TL;DR: 本文介绍Aleks，一个AI多智能体系统，旨在通过自主迭代解决问题、建模和优化，加速植物科学中的数据驱动科学发现。


<details>
  <summary>Details</summary>
Motivation: 现代植物科学中大型异构数据集的实验设计、数据预处理和可重复性挑战阻碍了研究效率。

Method: 介绍Aleks，一个AI驱动的多智能体系统，结合领域知识、数据分析和机器学习，以自主迭代方式解决研究问题，探索建模并优化解决方案。

Result: Aleks成功识别葡萄红斑病案例中具生物学意义的特征并获得稳健模型；消融研究证明领域知识和记忆的重要性。

Conclusion: 智能体AI在加速植物科学研究方面具有巨大潜力，可作为自主合作者。

Abstract: Modern plant science increasingly relies on large, heterogeneous datasets,
but challenges in experimental design, data preprocessing, and reproducibility
hinder research throughput. Here we introduce Aleks, an AI-powered multi-agent
system that integrates domain knowledge, data analysis, and machine learning
within a structured framework to autonomously conduct data-driven scientific
discovery. Once provided with a research question and dataset, Aleks
iteratively formulated problems, explored alternative modeling strategies, and
refined solutions across multiple cycles without human intervention. In a case
study on grapevine red blotch disease, Aleks progressively identified
biologically meaningful features and converged on interpretable models with
robust performance. Ablation studies underscored the importance of domain
knowledge and memory for coherent outcomes. This exploratory work highlights
the promise of agentic AI as an autonomous collaborator for accelerating
scientific discovery in plant sciences.

</details>


### [15] [Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs](https://arxiv.org/abs/2508.19432)
*Yao Fu,Xianxuan Long,Runchao Li,Haotian Yu,Mu Sheng,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.AI

TL;DR: 本研究发现，尽管量化大型语言模型（LLMs）内部保留了真实性表征，但在误导性提示下，它们更容易生成虚假输出，尤其“欺骗性”提示能覆盖其真实性行为。研究通过TruthfulnessEval框架及探针技术揭示了这一漏洞。


<details>
  <summary>Details</summary>
Motivation: 量化技术能有效降低LLMs的部署成本，但其对模型“真实性”（即生成真实或欺骗性回应的能力）的影响尚不明确，而现有研究主要关注困惑度等性能指标。本研究旨在填补这一空白，评估量化LLMs在真实性方面的表现。

Method: 1. 引入TruthfulnessEval框架，从逻辑推理、常识和模仿性谎言三个维度评估量化LLMs的真实性。2. 使用该框架，对多种主流量化技术（从4比特到极端的2比特）下的多个开源LLMs进行测试。3. 测试了15种重新措辞的“诚实”、“中立”和“欺骗性”提示变体，以探究模型对虚假输出的敏感性。4. 通过层级探查和PCA可视化技术，深入分析量化模型内部的真实性表征。

Result: 1. 量化模型内部仍保留真实的表征。2. 令人惊讶的是，在误导性提示下，量化模型更容易产生虚假输出。3. “欺骗性”提示能够覆盖模型保持真实的行为，而“诚实”和“中立”提示则能维持稳定的输出。4. 通过层级探查和PCA可视化揭示，量化模型内部“知道”真相，但在“欺骗性”提示引导下仍会产生虚假输出。

Conclusion: 本研究的结果为未来设计“量化感知”的对齐策略和真实性干预措施提供了重要见解，以提升量化LLMs的真实性表现。

Abstract: Quantization enables efficient deployment of large language models (LLMs) in
resource-constrained environments by significantly reducing memory and
computation costs. While quantized LLMs often maintain performance on
perplexity and zero-shot tasks, their impact on truthfulness-whether generating
truthful or deceptive responses-remains largely unexplored. In this work, we
introduce TruthfulnessEval, a comprehensive evaluation framework for assessing
the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on
Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on
Imitative Falsehoods. Using this framework, we examine mainstream quantization
techniques (ranging from 4-bit to extreme 2-bit) across several open-source
LLMs. Surprisingly, we find that while quantized models retain internally
truthful representations, they are more susceptible to producing false outputs
under misleading prompts. To probe this vulnerability, we test 15 rephrased
variants of "honest", "neutral" and "deceptive" prompts and observe that
"deceptive" prompts can override truth-consistent behavior, whereas "honest"
and "neutral" prompts maintain stable outputs. Further, we reveal that
quantized models "know" the truth internally yet still produce false outputs
when guided by "deceptive" prompts via layer-wise probing and PCA
visualizations. Our findings provide insights into future designs of
quantization-aware alignment and truthfulness interventions.

</details>


### [16] [Reliable Weak-to-Strong Monitoring of LLM Agents](https://arxiv.org/abs/2508.19461)
*Neil Kale,Chen Bo Calvin Zhang,Kevin Zhu,Ankit Aich,Paula Rodriguez,Scale Red Team,Christina Q. Knight,Zifan Wang*

Main category: cs.AI

TL;DR: 本文提出了一种监测红队测试(MRT)工作流程，用于压力测试检测自主LLM代理秘密不当行为的监测系统，并揭示了代理意识、监测支架结构和有针对性的人工干预对监测可靠性的影响。


<details>
  <summary>Details</summary>
Motivation: 检测自主LLM代理（例如，秘密共享私人信息）的隐蔽不当行为是重要挑战。研究动机是压力测试现有监测系统，并系统化一个监测红队测试(MRT)工作流程来评估其鲁棒性。

Method: 研究方法包括：1) 系统化一个监测红队测试(MRT)工作流程，该工作流程考虑代理和监测器的不同情境感知水平；2) 采用提示注入等对抗策略来规避监测器；3) 使用SHADE-Arena和新扩展的CUA-SHADE-Arena两个数据集和环境；4) 在现有LLM监测支架结构和本文提出的新型混合分层-顺序支架结构上运行MRT。

Result: 主要研究结果有三点：1) 代理意识主导监测器意识，代理知道被监测会显著降低监测可靠性；2) 监测支架结构比监测器意识更重要，混合支架结构持续优于基线，并能实现“弱监测强代理”；3) 在人工参与设置中，有针对性的人工监督最有效，仅将预标记案例升级给人工审查可将TPR提高约15% (FPR=0.01)。

Conclusion: 本工作建立了MRT的标准工作流程，强调了LLM和人类在监测和检测代理不当行为时缺乏对抗鲁棒性。研究发布了代码、数据和日志，以促进进一步研究。

Abstract: We stress test monitoring systems for detecting covert misbehavior in
autonomous LLM agents (e.g., secretly sharing private information). To this
end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)
varying levels of agent and monitor situational awareness; (2) distinct
adversarial strategies to evade the monitor, such as prompt injection; and (3)
two datasets and environments -- SHADE-Arena for tool-calling agents and our
new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We
run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse
agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding
proposed in this work. Our empirical results yield three key findings. First,
agent awareness dominates monitor awareness: an agent's knowledge that it is
being monitored substantially degrades the monitor's reliability. On the
contrary, providing the monitor with more information about the agent is less
helpful than expected. Second, monitor scaffolding matters more than monitor
awareness: the hybrid scaffolding consistently outperforms baseline monitor
scaffolding, and can enable weaker models to reliably monitor stronger agents
-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where
humans discuss with the LLM monitor to get an updated judgment for the agent's
behavior, targeted human oversight is most effective; escalating only
pre-flagged cases to human reviewers improved the TPR by approximately 15% at
FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the
lack of adversarial robustness for LLMs and humans when monitoring and
detecting agent misbehavior. We release code, data, and logs to spur further
research.

</details>


### [17] [SLIM: Subtrajectory-Level Elimination for More Effective Reasoning](https://arxiv.org/abs/2508.19502)
*Xifeng Yao,Chengyuan Ma,Dongyu Lang,Yinhao Ni,Zhiwei Xu,Huarui Xie,Zihao Chen,Guang Shen,Dandan Tu,Yi Bai,Changzheng Zhang*

Main category: cs.AI

TL;DR: 本研究提出一种“5+2”框架，用于识别并消除大型语言模型推理轨迹中的次优子轨迹，以优化模型微调过程，提高复杂推理任务的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 近期大型语言模型在复杂推理方面取得显著进展，但研究发现，用于微调的推理轨迹并非所有组件都有益，部分甚至可能产生负面影响，导致微调效果不佳。

Method: 研究将推理轨迹划分为子轨迹，并开发了“5+2”框架：(1) 基于五个人工建立的标准识别次优子轨迹；(2) 评估这些次优子轨迹的独立性，确保移除后不影响推理的连贯性。此外，还采用一种采样算法，基于此框架选择高质量的、不含次优子轨迹的数据进行模型微调。

Result: 实验结果表明，该方法在推理过程中将次优子轨迹的数量减少了25.9%。在仅使用三分之二训练数据微调Qwen2.5-Math-7B时，该方法在挑战性数学基准上达到了58.92%的平均准确率，超越了使用全部数据（58.06%）和开源数据集的性能。此外，在资源受限的不同推理Token限制下，该方法也展现出性能提升。

Conclusion: 通过系统识别并消除大型语言模型推理轨迹中的次优部分，可以显著提升模型微调的效果，在减少训练数据量或受资源限制的情况下，仍能提高复杂推理任务的准确性和效率。

Abstract: In recent months, substantial progress has been made in complex reasoning of
Large Language Models, particularly through the application of test-time
scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When
responding to a query, these models generate an extended reasoning trajectory,
during which the model explores, reflects, backtracks, and self-verifies before
arriving at a conclusion. However, fine-tuning models with such reasoning
trajectories may not always be optimal. Our findings indicate that not all
components within these reasoning trajectories contribute positively to the
reasoning process; in fact, some components may affect the overall performance
negatively. In this study, we divide a reasoning trajectory into individual
subtrajectories and develop a "5+2" framework to: (1) systematically identify
suboptimal subtrajectories within the reasoning trajectory based on five
human-established criteria; (2) assess the independence of the suboptimal
subtrajectories identified in (1) from the subsequent content, ensuring that
their elimination does not compromise overall flow and coherence of the
reasoning process. Additionally, a sampling algorithm, built upon the "5+2"
framework, is employed to select data whose reasoning process is free from
suboptimal subtrajectories to the highest degree. Experimental results
demonstrate that our method can reduce the number of suboptimal subtrajectories
by 25.9\% during the inference. Furthermore, our method achieves an average
accuracy of 58.92\% on highly challenging math benchmarks with only two thirds
of training data, surpassing the average accuracy of 58.06\% achieved with the
entire data, and outperforming open-source datasets, when fine-tuning
Qwen2.5-Math-7B. Finally, We validated our method under resource constraints
and observed improved performance across various inference token limits.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models](https://arxiv.org/abs/2508.19249)
*Jonas Søeborg Nielsen,Marcus Galea Jacobsen,Albert Brincker Olson,Mads Peter Sørensen,Allan Peter Engsig-Karup*

Main category: cs.LG

TL;DR: 本文提出了一种名为“物理信息回归”（PIR）的新型高效混合参数估计算法。该方法利用正则化普通最小二乘法，对参数线性的非线性动态模型进行参数估计，并在疫情模型上与物理信息神经网络（PINN）进行了比较，结果显示PIR在性能和计算速度上均优于PINN。


<details>
  <summary>Details</summary>
Motivation: 开发一种高效且可靠的方法，用于从时间序列数据中估计非线性动态模型的参数，特别是那些在参数方面呈线性的模型，以弥合理论与数据之间的鸿沟。

Method: 引入了“物理信息回归”（PIR）方法，其核心思想是当非线性动态模型在参数方面呈线性时，可以使用正则化普通最小二乘法（OLS）来有效估计这些参数。该方法应用于ODE和PDE模型，并以两种流行病学模型为例，与物理信息神经网络（PINN）进行了比较，数据包括合成数据和真实的丹麦COVID-19时间序列数据（2020-2021年）。

Result: PIR和PINN均能估计目标参数，但PIR表现出显著更优的性能，尤其是在更复杂的仓室模型上。考虑到计算速度的差异，PIR被认为在所考虑的模型中优于PINN。研究还展示了PIR如何应用于估计使用真实COVID-19数据拟合的仓室模型的时变参数。

Conclusion: PIR方法对于参数线性的非线性动态模型而言，是一种可靠、快速（可能达到实时）且优于PINN的参数估计技术。它能有效地将数据驱动和物理信息技术结合起来，支持高效的参数估计。

Abstract: We present a new efficient hybrid parameter estimation method based on the
idea, that if nonlinear dynamic models are stated in terms of a system of
equations that is linear in terms of the parameters, then regularized ordinary
least squares can be used to estimate these parameters from time series data.
We introduce the term "Physics-Informed Regression" (PIR) to describe the
proposed data-driven hybrid technique as a way to bridge theory and data by use
of ordinary least squares to efficiently perform parameter estimation of the
model coefficients of different parameter-linear models; providing examples of
models based on nonlinear ordinary equations (ODE) and partial differential
equations (PDE). The focus is on parameter estimation on a selection of ODE and
PDE models, each illustrating performance in different model characteristics.
For two relevant epidemic models of different complexity and number of
parameters, PIR is tested and compared against the related technique,
physics-informed neural networks (PINN), both on synthetic data generated from
known target parameters and on real public Danish time series data collected
during the COVID-19 pandemic in Denmark. Both methods were able to estimate the
target parameters, while PIR showed to perform noticeably better, especially on
a compartment model with higher complexity. Given the difference in
computational speed, it is concluded that the PIR method is superior to PINN
for the models considered. It is also demonstrated how PIR can be applied to
estimate the time-varying parameters of a compartment model that is fitted
using real Danish data from the COVID-19 pandemic obtained during a period from
2020 to 2021. The study shows how data-driven and physics-informed techniques
may support reliable and fast -- possibly real-time -- parameter estimation in
parameter-linear nonlinear dynamic models.

</details>


### [19] [Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats](https://arxiv.org/abs/2508.19263)
*Anat Heilper,Doron Singer*

Main category: cs.LG

TL;DR: 本文提出了一种将ZipNN方法扩展到低精度浮点格式（如FP8和FP4）的无损压缩方案，用于神经网络权重和LLM的K/V缓存，实现了显著的压缩率。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型规模的增长和广泛部署，降低神经网络权重的存储和传输成本日益重要。虽然现有方法对高精度格式有效，但FP8和FP4等低精度格式正越来越受欢迎，需要针对性地研究其压缩方法。

Method: 本研究将ZipNN方法扩展到FP8和FP4等低精度浮点格式。设计了一种压缩方法，通过熵编码独立分离并压缩浮点数的指数和尾数分量。

Result: 评估结果显示，BF16格式的压缩率高达62%，FP8格式的压缩率高达83%。此外，研究发现大型语言模型（LLMs）中使用的键值（K/V）缓存张量也具有可压缩性。

Conclusion: 通过对低精度浮点格式和LLM K/V缓存采用独立的指数和尾数熵编码压缩，可以显著减少模型权重和K/V缓存的存储需求，从而在部署期间节省内存。

Abstract: As deep learning models grow and deployment becomes more widespread, reducing
the storage and transmission costs of neural network weights has become
increasingly important. While prior work such as ZipNN has shown that lossless
compression methods - particularly those based on Huffman encoding
floating-point exponents can significantly reduce model sizes, these techniques
have primarily been applied to higher-precision formats such as FP32 and BF16.
In this work, we extend the ZipNN approach to lower-precision floating-point
formats, specifically FP8 and FP4, which are gaining popularity for efficient
inference. We design a compression method that separates and compresses the
exponent and mantissa components independently using entropy coding. Our
evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also
investigate the compressibility of key-value (K/V) cache tensors used in large
language models (LLMs), finding that they, too, exhibit compressible patterns,
enabling memory savings during deployment.

</details>


### [20] [POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization](https://arxiv.org/abs/2508.19277)
*Xinyu Li,Tianjin Huang,Ronghui Mu,Xiaowei Huang,Gaojie Jin*

Main category: cs.LG

TL;DR: CoT提示提升了LLM推理能力，但也引入了过度冗长推理链导致资源浪费的漏洞。现有攻击有局限。本文提出POT，一种新的黑盒攻击框架，利用LLM迭代优化生成隐蔽的对抗性提示，无需外部数据，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 链式思考（CoT）提示虽增强了大型语言模型（LLM）的推理能力，但可能导致不必要的冗长推理链，消耗过多计算资源且无性能提升。现有“过度思考”攻击方法存在依赖外部知识、中毒内容可检索性及结构化模板等限制，实用性差。

Method: 提出POT（Prompt-Only OverThinking）框架，这是一种新颖的黑盒攻击框架。该方法采用基于LLM的迭代优化技术，生成隐蔽且语义自然的对抗性提示，从而避免了对外部数据访问和模型检索的依赖。

Result: 通过对多种模型架构和数据集的广泛实验，结果表明POT相较于其他方法，取得了卓越的攻击性能。

Conclusion: POT框架成功克服了现有“过度思考”攻击的局限性，提供了一种更实际、有效的黑盒攻击方法，能够通过隐蔽提示诱导LLM产生计算低效的冗长推理。

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially
enhanced the reasoning capabilities of large language models (LLMs), enabling
sophisticated problem-solving through explicit multi-step reasoning traces.
However, these enhanced reasoning processes introduce novel attack surfaces,
particularly vulnerabilities to computational inefficiency through
unnecessarily verbose reasoning chains that consume excessive resources without
corresponding performance gains. Prior overthinking attacks typically require
restrictive conditions including access to external knowledge sources for data
poisoning, reliance on retrievable poisoned content, and structurally obvious
templates that limit practical applicability in real-world scenarios. To
address these limitations, we propose POT (Prompt-Only OverThinking), a novel
black-box attack framework that employs LLM-based iterative optimization to
generate covert and semantically natural adversarial prompts, eliminating
dependence on external data access and model retrieval. Extensive experiments
across diverse model architectures and datasets demonstrate that POT achieves
superior performance compared to other methods.

</details>


### [21] [(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems](https://arxiv.org/abs/2508.19318)
*Aohan Li,Miyu Tsuzuki*

Main category: cs.LG

TL;DR: 本文提出了一种在真实分布式物联网环境中训练DRL模型的框架，通过ACK反馈优化信道选择，并验证了其可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在利用真实世界数据训练深度强化学习（DRL）模型以进行实际分布式物联网系统中的资源分配方面存在局限。

Method: 提出一个新颖的框架，用于在真实分布式物联网环境中训练DRL模型。物联网设备使用基于DRL的方法选择通信信道，DRL模型则通过从实际数据传输中获取的确认（ACK）信息进行训练。

Result: 通过实现和性能评估（以帧成功率FSR衡量），证明了所提出框架的可行性和有效性。

Conclusion: 所提出的DRL训练框架在真实分布式物联网环境中进行信道选择是可行且有效的。

Abstract: Deep Reinforcement Learning (DRL) has emerged as an efficient approach to
resource allocation due to its strong capability in handling complex
decision-making tasks. However, only limited research has explored the training
of DRL models with real-world data in practical, distributed Internet of Things
(IoT) systems. To bridge this gap, this paper proposes a novel framework for
training DRL models in real-world distributed IoT environments. In the proposed
framework, IoT devices select communication channels using a DRL-based method,
while the DRL model is trained with feedback information. Specifically,
Acknowledgment (ACK) information is obtained from actual data transmissions
over the selected channels. Implementation and performance evaluation, in terms
of Frame Success Rate (FSR), are carried out, demonstrating both the
feasibility and the effectiveness of the proposed framework.

</details>


### [22] [Re:Frame -- Retrieving Experience From Associative Memory](https://arxiv.org/abs/2508.19344)
*Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 离线强化学习因次优数据受限，本研究提出Re:Frame模块，通过引入关联记忆缓冲区注入少量专家经验，显著提升了从低质量数据集学习的性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习智能体在缺乏大量专家数据时，难以从不完美或不一致的轨迹中泛化和实现高性能。核心动机是如何最佳地利用稀缺的专家演示与大量低质量数据。

Method: 引入Re:Frame模块，它是一个即插即用的组件，通过一个由专家轨迹填充的外部关联记忆缓冲区（AMB）来增强标准离线RL策略（如Decision Transformer）。在训练过程中，策略通过基于内容的关联从AMB中检索专家数据并整合到决策中，无需环境交互或修改主干架构。

Result: 在D4RL MuJoCo任务上，仅使用60条专家轨迹（占数据集的0.1%），Re:Frame在四分之三的设置中持续优于强大的Decision Transformer基线，性能提升高达+10.7标准化分数。

Conclusion: Re:Frame提供了一种简单且数据高效的方法，能够有效注入稀缺的专家知识，从而显著改善从低质量数据集进行的离线强化学习。

Abstract: Offline reinforcement learning (RL) often deals with suboptimal data when
collecting large expert datasets is unavailable or impractical. This limitation
makes it difficult for agents to generalize and achieve high performance, as
they must learn primarily from imperfect or inconsistent trajectories. A
central challenge is therefore how to best leverage scarce expert
demonstrations alongside abundant but lower-quality data. We demonstrate that
incorporating even a tiny amount of expert experience can substantially improve
RL agent performance. We introduce Re:Frame (Retrieving Experience From
Associative Memory), a plug-in module that augments a standard offline RL
policy (e.g., Decision Transformer) with a small external Associative Memory
Buffer (AMB) populated by expert trajectories drawn from a separate dataset.
During training on low-quality data, the policy learns to retrieve expert data
from the Associative Memory Buffer (AMB) via content-based associations and
integrate them into decision-making; the same AMB is queried at evaluation.
This requires no environment interaction and no modifications to the backbone
architecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories
(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a
strong Decision Transformer baseline in three of four settings, with gains up
to +10.7 normalized points. These results show that Re:Frame offers a simple
and data-efficient way to inject scarce expert knowledge and substantially
improve offline RL from low-quality datasets.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [23] [Connectivity Analysis of LoRaWAN-Based Non-Terrestrial Networks for Subterranean mMTC](https://arxiv.org/abs/2508.19350)
*Kaiqiang Lin,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 本文探讨了将地下无线传感器网络（WUSNs）与非地面网络（NTNs，包括UAVs、HAPs和LEO卫星）集成，以提高恶劣环境下通信可靠性的可行性，并评估了LoRa和LR-FHSS调制方案在不同NTN平台上的性能和影响因素。


<details>
  <summary>Details</summary>
Motivation: 解决无线地下传感器网络（WUSNs）在地面网络基础设施不可用或不可靠的恶劣环境中通信可靠性下降的问题，以支持各类大规模地下监测应用。

Method: 开发了一个蒙特卡洛模拟器，该模拟器结合了多层地下衰减模型、针对不同非地面网络（NTN）平台（UAV、HAP、LEO卫星）的3GPP经验路径损耗模型，以及LoRaWAN的两种调制方案：LoRa和LoRa-频率跳变扩频（LR-FHSS）。

Result: 研究结果表明，LoRa SF7是农村环境中短程UAV通信的有力候选；LR-FHSS调制方案凭借其足够的链路预算和抗干扰能力，在大规模WUSN场景中对HAP和LEO卫星平台而言是前景广阔的选择。此外，地下到NTN连接的成功概率受监测环境、设备数量、埋深和土壤体积含水量等因素的显著影响。

Conclusion: 将WUSNs与NTNs集成能够显著提升恶劣环境下的通信可靠性。LoRa SF7适用于短程UAV通信，而LR-FHSS更适合大规模WUSN与HAP/LEO卫星的连接。同时，监测环境、设备数量、埋深和土壤含水量是影响地下到NTN连接成功率的关键因素。

Abstract: Wireless underground sensor networks (WUSNs) offer significant social and
economic benefits by enabling the monitoring of subterranean entities. However,
the communication reliability of WUSNs diminishes in harsh environments where
terrestrial network infrastructure is either unavailable or unreliable. To
address this challenge, we explore the feasibility of integrating buried
massive machine-type communication (mMTC) sensors with non-terrestrial networks
(NTNs), including unmanned aerial vehicles (UAVs), high-altitude platforms
(HAPs), and low Earth orbit (LEO) satellites, to establish underground-to-NTN
connectivity for various large-scale underground monitoring applications. To
assess the effectiveness of underground-to-NTN connectivity, we develop a Monte
Carlo simulator that incorporates a multi-layer underground attenuation model,
the 3GPP empirical path loss model for various NTN platforms, and two LoRaWAN
modulation schemes, i.e., LoRa and LoRa-frequency hopping spread spectrum
(LR-FHSS). Our results evidence that LoRa SF7 is a strong candidate for
short-range UAV communication in rural environments, while LR-FHSS modulation
proves to be a promising option for HAP and LEO satellite platforms in massive
WUSNs scenarios thanks to its adequate link budget and robustness to the
interference. Finally, we demonstrate that the success probability of
underground-to-NTN connectivity using LoRa and LR-FHSS is significantly
affected by factors such as the monitoring environment, the number of devices,
burial depth, and the soil's volumetric water content.

</details>


### [24] [Experimental Insights from OpenAirInterface 5G positioning Testbeds: Challenges and solutions](https://arxiv.org/abs/2508.19736)
*Mohsen Ahadi,Adeel Malik,Omid Esrafilian,Florian Kaltenberger,Cedric Thienot*

Main category: cs.NI

TL;DR: 本文在三个5G测试平台上，基于UL-TDoA方法，结合所提出的滤波和PSO定位算法，实现了1-2米高精度的5G定位，并探讨了AI/ML数据驱动定位的潜力，同时发布了数据集。


<details>
  <summary>Details</summary>
Motivation: 5G NR是智慧城市和工厂中实现精准定位的关键技术。然而，同步误差、多径传播和部署几何结构等因素会影响定位精度，需要新的方法来克服这些挑战。

Method: 使用基于开源OpenAirInterface (OAI)的三个5G测试平台（室内工厂和室外场景），采用UL-TDoA技术并集成Location Management Function (LMF)。为应对挑战，提出了定制化的ToA/TDoA滤波和基于粒子群优化 (PSO) 的新型定位估计方法。此外，还展示了一个利用信道冲激响应 (CIR) 等非传统测量数据，通过AI/ML模型实现数据驱动定位的超5G框架。

Result: 实验结果表明，在不同测试场景下，90%的情况下可实现1-2米的定位精度。研究强调了同步误差、多径传播和部署几何结构对定位精度的影响。同时，公开了收集到的数据集以支持研究社区。

Conclusion: 该研究验证了5G定位系统的可行性，并通过提出的创新方法实现了高精度定位。这些发现为设计鲁棒的5G定位系统提供了实际见解，并且通过发布数据集促进了社区研究。

Abstract: 5G New Radio (NR) is a key enabler of accurate positioning in smart cities
and smart factories. This paper presents the experimental results from three 5G
positioning testbeds running open-source OpenAirInterface (OAI) gNB and Core
Network (CN), using Uplink Time Difference of Arrival (UL-TDoA) with the newly
integrated Location Management Function (LMF). The testbeds are deployed across
both indoor factories and outdoor scenarios with O-RAN Radio Units (RUs),
following a 3GPP-compliant system model. The experiments highlight the impact
of synchronization impairments, multipath propagation, and deployment geometry
on positioning accuracy. To address these challenges, we propose tailored ToA
and TDoA filtering as well as a novel position estimation method based on
Particle Swarm Optimization (PSO) within the LMF pipeline. Moreover, we show a
beyond-5G framework that leverages non-conventional measurements such as
Channel Impulse Response (CIR) to train and test Artificial Intelligence and
Machine Learning (AI/ML) models for data-driven positioning. The results
demonstrate the feasibility of achieving 1-2 meter positioning accuracy in 90%
of cases in different testbeds, offering practical insights for the design of
robust 5G positioning systems. Moreover, we publicly release the datasets
collected in this work to support the research within the 5G positioning
community.

</details>


### [25] [Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey](https://arxiv.org/abs/2508.19870)
*Yinqiu Liu,Ruichen Zhang,Haoxiang Luo,Yijing Lin,Geng Sun,Dusit Niyato,Hongyang Du,Zehui Xiong,Yonggang Wen,Abbas Jamalipour,Dong In Kim,Ping Zhang*

Main category: cs.NI

TL;DR: 本综述针对边缘通用智能（EGI）中多LLM系统面临的安全漏洞，首次提出了零信任安全范式，并系统分析风险，归纳了模型级与系统级零信任安全机制，同时指明了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 协作式多LLM系统在EGI环境中引入了严重的互LLM通信不安全、攻击面扩大及跨域数据泄露等安全漏洞，而传统基于边界的安全措施无法有效应对这些挑战。

Method: 本研究首先系统分析EGI中多LLM系统的安全风险；接着提出零信任多LLM框架的愿景；随后调查并分类零信任安全机制为模型级（如强识别、上下文感知访问控制）和系统级（如主动维护、区块链管理）方法；最后识别关键研究方向。

Result: 本综述作为首次将零信任原则系统应用于多LLM系统的研究，为EGI中的多LLM代理式AI系统提供了理论基础和实用策略，并指明了未来的关键研究方向。

Conclusion: 本综述通过引入零信任安全范式，为解决EGI中多LLM系统固有的安全挑战提供了开创性的系统性处理方法，为构建更安全、可信的边缘通用智能系统奠定了基础。

Abstract: Agentification serves as a critical enabler of Edge General Intelligence
(EGI), transforming massive edge devices into cognitive agents through
integrating Large Language Models (LLMs) and perception, reasoning, and acting
modules. These agents collaborate across heterogeneous edge infrastructures,
forming multi-LLM agentic AI systems that leverage collective intelligence and
specialized capabilities to tackle complex, multi-step tasks. However, the
collaborative nature of multi-LLM systems introduces critical security
vulnerabilities, including insecure inter-LLM communications, expanded attack
surfaces, and cross-domain data leakage that traditional perimeter-based
security cannot adequately address. To this end, this survey introduces
zero-trust security of multi-LLM in EGI, a paradigmatic shift following the
``never trust, always verify'' principle. We begin by systematically analyzing
the security risks in multi-LLM systems within EGI contexts. Subsequently, we
present the vision of a zero-trust multi-LLM framework in EGI. We then survey
key technical progress to facilitate zero-trust multi-LLM systems in EGI.
Particularly, we categorize zero-trust security mechanisms into model- and
system-level approaches. The former and latter include strong identification,
context-aware access control, etc., and proactive maintenance, blockchain-based
management, etc., respectively. Finally, we identify critical research
directions. This survey serves as the first systematic treatment of zero-trust
applied to multi-LLM systems, providing both theoretical foundations and
practical strategies.

</details>


### [26] [2SYN: Congestion-Aware Multihoming](https://arxiv.org/abs/2508.20044)
*Kfir Toledo,Isaac Keslassy*

Main category: cs.NI

TL;DR: 2SYN是一种拥塞感知的多宿主路由算法，能为任意目的地动态选择最佳路径，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多宿主路由器采用对拥塞不敏感的机制，无法避免拥塞路径，导致流量发送至任意目的地时效率低下。

Method: 本文提出2SYN，首个适用于任意目的地的拥塞感知多宿主算法。它能为新连接动态选择优选路径，即使是针对之前未见的未知目的地。该算法易于在Linux中实现。

Result: 通过真实世界的LTE和有线链路实验，证明2SYN能够动态适应连接质量，并显著优于其他现有方法。

Conclusion: 2SYN通过有效利用多宿主能力，帮助企业更好地管理其网络。

Abstract: When sending flows to arbitrary destinations, current multihoming routers
adopt simple congestion-oblivious mechanisms. Therefore, they cannot avoid
congested paths.
  In this paper, we introduce 2SYN, the first congestion-aware multihoming
algorithm that works for any destination. We explain how it dynamically selects
a preferred path for new connections, even given previously-unseen
destinations. We further demonstrate that it can be easily implemented in
Linux. Finally, in a real-world experiment with either LTE or a wired link, we
show how 2SYN dynamically adapts to the quality of the connection and
outperforms alternative approaches. Thus, 2SYN helps companies better manage
their networks by leveraging their multihoming capabilities.

</details>


### [27] [A First Look at Inter-Cell Interference in the Wild](https://arxiv.org/abs/2508.20060)
*Daqian Ding,Yibo Pi,Cailian Chen*

Main category: cs.NI

TL;DR: 对实际4G/5G网络中的小区间干扰进行了首次测量研究，发现干扰普遍存在且缺乏有效管理，导致信号质量显著下降，存在巨大优化潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管小区间干扰管理已研究多年，但其在实际网络中的效果仍未得到充分探索，需要进行实地测量以弥补这一空白。

Method: 对实际运行的4G/5G网络进行了小区间干扰的首次测量研究，并从网络部署、信道分配、时频资源分配和网络配置四个主要方面审视了小区间干扰问题。

Result: 研究发现小区间干扰普遍存在，且运营基站之间惊人地缺乏干扰协调。用户设备因此承受不必要的干扰，导致信号质量显著下降，特别是在频率选择性信道衰落下。在所有检查的维度上，小区间干扰都未能得到有效管理。即使在频谱资源未充分利用且简单策略即可减轻干扰的情况下，基站仍倾向于使用相同的时频资源，从而在小区间造成干扰。

Conclusion: 测量结果揭示了通过有效的小区间干扰管理来显著改善信号质量的巨大机会。

Abstract: In cellular networks, inter-cell interference management has been studied for
decades, yet its real-world effectiveness remains under-explored. To bridge
this gap, we conduct a first measurement study of inter-cell interference for
operational 4G/5G networks. Our findings reveal the prevalence of inter-cell
interference and a surprising absence of interference coordination among
operational base stations. As a result, user equipments experience unnecessary
interference, which causes significant signal quality degradation, especially
under frequency-selective channel fading. We examine the inter-cell
interference issues from four major perspectives: network deployment, channel
assignment, time-frequency resource allocation, and network configuration. In
none of these dimensions is inter-cell interference effectively managed.
Notably, even when spectrum resources are underutilized and simple strategies
could effectively mitigate inter-cell interference, base stations consistently
prioritize using the same set of time-frequency resources, causing interference
across cells. Our measurements reveal substantial opportunities for improving
signal quality by inter-cell interference management.

</details>


### [28] [ML-MaxProp: Bridging Machine Learning and Delay-Tolerant Routing for Resilient Post-Disaster Communication](https://arxiv.org/abs/2508.20077)
*Tao Xiuyuan,Milena Radenkovic*

Main category: cs.NI

TL;DR: 本研究提出ML-MaxProp，一种结合监督机器学习的混合路由协议，旨在解决灾区和城市紧急情况下的延迟容忍网络（DTN）通信挑战，显著提高了消息投递率、降低了延迟和开销。


<details>
  <summary>Details</summary>
Motivation: 在受灾和大规模城市紧急场景中，由于基础设施瘫痪、移动性不可预测和资源严重受限，传统网络中断，确保可靠通信面临巨大挑战。现有延迟容忍网络（DTN）协议（如Epidemic、Spray-and-Wait和MaxProp）在遭遇稀疏、缓冲器短缺和连接不稳定的情况下，表现出基本弱点。

Method: 本研究提出ML-MaxProp，一种通过监督机器学习增强MaxProp的混合路由协议。该协议利用接触频率、跳数、缓冲区占用、消息寿命和存活时间（TTL）等上下文特征，实时预测中继节点的适用性。通过在ONE仿真环境中使用Helsinki SPMBM移动模型进行广泛仿真，并进行统计验证。

Result: 仿真结果表明，ML-MaxProp始终优于基线协议，实现了更高的投递概率、更低的延迟和更少的开销。统计验证进一步证明，即使在高度资源受限和不稳定的条件下，这些改进也是显著且稳健的。

Conclusion: ML-MaxProp不仅是渐进式改进，更是一个轻量级、自适应且实用的解决方案，解决了DTN中最困难的挑战之一：在基础设施崩溃、每个转发决策都至关重要时，维持任务关键型通信。

Abstract: In disaster-stricken and large-scale urban emergency scenarios, ensuring
reliable communication remains a formidable challenge, as collapsed
infrastructure, unpredictable mobility, and severely constrained resources
disrupt conventional networks. Delay-Tolerant Networks (DTNs), though resilient
through their store-carry-forward paradigm, reveal the fundamental weaknesses
of classical protocols - Epidemic, Spray-and-Wait, and MaxProp - when
confronted with sparse encounters, buffer shortages, and volatile connectivity.
To address these obstacles, this study proposes ML-MaxProp, a hybrid routing
protocol that strengthens MaxProp with supervised machine learning. By
leveraging contextual features such as encounter frequency, hop count, buffer
occupancy, message age, and time-to-live (TTL), ML-MaxProp predicts relay
suitability in real time, transforming rigid heuristics into adaptive
intelligence. Extensive simulations in the ONE environment using the Helsinki
SPMBM mobility model show that ML-MaxProp consistently surpasses baseline
protocols, achieving higher delivery probability, lower latency, and reduced
overhead. Statistical validation further shows that these improvements are both
significant and robust, even under highly resource-constrained and unstable
conditions. Overall, this work shows that ML-MaxProp is not just an incremental
refinement but a lightweight, adaptive, and practical solution to one of the
hardest challenges in DTNs: sustaining mission-critical communication when
infrastructure collapses and every forwarding decision becomes critical.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [29] [When Routers, Switches and Interconnects Compute: A processing-in-interconnect Paradigm for Scalable Neuromorphic AI](https://arxiv.org/abs/2508.19548)
*Madhuvanthi Srivatsav R,Chiranjib Bhattacharyya,Shantanu Chakrabartty,Chetan Singh Thakur*

Main category: cs.NE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Routing, switching, and the interconnect fabric are essential for large-scale
neuromorphic computing. While this fabric only plays a supporting role in the
process of computing, for large AI workloads it ultimately determines energy
consumption and speed. In this paper, we address this bottleneck by asking: (a)
What computing paradigms are inherent in existing routing, switching, and
interconnect systems, and how can they be used to implement a
processing-in-Interconnect (\pi^2) computing paradigm? and (b) leveraging
current and future interconnect trends, how will a \pi^2 system's performance
scale compared to other neuromorphic architectures? For (a), we show that
operations required for typical AI workloads can be mapped onto delays,
causality, time-outs, packet drop, and broadcast operations -- primitives
already implemented in packet-switching and packet-routing hardware. We show
that existing buffering and traffic-shaping embedded algorithms can be
leveraged to implement neuron models and synaptic operations. Additionally, a
knowledge-distillation framework can train and cross-map well-established
neural network topologies onto $\pi^2$ without degrading generalization
performance. For (b), analytical modeling shows that, unlike other neuromorphic
platforms, the energy scaling of $\pi^2$ improves with interconnect bandwidth
and energy efficiency. We predict that by leveraging trends in interconnect
technology, a \pi^2 architecture can be more easily scaled to execute
brain-scale AI inference workloads with power consumption levels in the range
of hundreds of watts.

</details>
