<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 13]
- [cs.CV](#cs.CV) [Total: 10]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Unmasking the Reality of PII Masking Models: Performance Gaps and the Call for Accountability](https://arxiv.org/abs/2504.12308)
*Devansh Singh,Sundaraparipurnan Narayanan*

Main category: cs.CL

TL;DR: 研究探讨了隐私掩码技术在数据隐私中的应用，通过评估现有PII数据集和NER方法的局限，提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示现有隐私掩码模型中PII识别和掩码的不足，特别是在处理敏感内容、短语变体和格式差异时的挑战。

Method: 方法包括构建一个包含16种PII的半合成数据集（17K句子），并在五种NER检测维度及对抗场景下测试Piiranha和Starpii模型的表现。

Result: 结果表明现有模型在这些维度上存在显著的隐私泄露风险，尤其考虑到模型的高下载量加剧了潜在影响。

Conclusion: 结论强调需改进模型性能评估标准，并要求在模型卡片中增加上下文披露以提升透明度。

Abstract: Privacy Masking is a critical concept under data privacy involving
anonymization and de-anonymization of personally identifiable information
(PII). Privacy masking techniques rely on Named Entity Recognition (NER)
approaches under NLP support in identifying and classifying named entities in
each text. NER approaches, however, have several limitations including (a)
content sensitivity including ambiguous, polysemic, context dependent or domain
specific content, (b) phrasing variabilities including nicknames and alias,
informal expressions, alternative representations, emerging expressions,
evolving naming conventions and (c) formats or syntax variations, typos,
misspellings. However, there are a couple of PII datasets that have been widely
used by researchers and the open-source community to train models on PII
detection or masking. These datasets have been used to train models including
Piiranha and Starpii, which have been downloaded over 300k and 580k times on
HuggingFace. We examine the quality of the PII masking by these models given
the limitations of the datasets and of the NER approaches. We curate a dataset
of 17K unique, semi-synthetic sentences containing 16 types of PII by compiling
information from across multiple jurisdictions including India, U.K and U.S. We
generate sentences (using language models) containing these PII at five
different NER detection feature dimensions - (1) Basic Entity Recognition, (2)
Contextual Entity Disambiguation, (3) NER in Noisy & Real-World Data, (4)
Evolving & Novel Entities Detection and (5) Cross-Lingual or multi-lingual NER)
and 1 in adversarial context. We present the results and exhibit the privacy
exposure caused by such model use (considering the extent of lifetime downloads
of these models). We conclude by highlighting the gaps in measuring performance
of the models and the need for contextual disclosure in model cards for such
models.

</details>


### [2] [Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer](https://arxiv.org/abs/2504.12311)
*Enming Zhang,Liwen Cao,Yanru Wu,Zijie Zhao,Guan Wang,Yang Li*

Main category: cs.CL

TL;DR: HGPrompt提出了一种自适应多源提示迁移框架，通过优化可迁移性和稳定性提升基础模型在下游任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 预训练提示作为知识资产，其简单聚合常导致表征崩溃，限制了多源知识的互补潜力。

Method: 联合优化双重目标（可迁移性+稳定性）：1）信息论指标评估提示特征的可迁移性；2）梯度对齐正则化减少提示间梯度冲突。

Result: 在VTAB基准测试中达到最优性能

Conclusion: HGPrompt通过自适应加权策略有效实现了多源提示的稳定知识迁移。

Abstract: Prompt tuning has emerged as a lightweight adaptation strategy for adapting
foundation models to downstream tasks, particularly in resource-constrained
systems. As pre-trained prompts have become valuable intellectual assets,
combining multiple source prompts offers a promising approach to enhance
generalization to new tasks by leveraging complementary knowledge from diverse
sources. However, naive aggregation of these prompts often leads to
representation collapse due to mutual interference, undermining their
collective potential. To address these challenges, we propose HGPrompt, an
adaptive framework for multi-source prompt transfer that learns optimal
ensemble weights by jointly optimizing dual objectives: transferability and
stability. Specifically, we first introduce an information-theoretic metric to
evaluate the transferability of prompt-induced features on the target task,
capturing the intrinsic alignment between the feature representations.
Additionally, we propose a novel Gradient Alignment Regularization to mitigate
gradient conflicts among prompts, enabling stable and coherent knowledge
transfer from multiple sources while suppressing interference. Extensive
experiments on the large-scale VTAB benchmark demonstrate that HGPrompt
achieves state-of-the-art performance, validating its effectiveness in
multi-source prompt transfer.

</details>


### [3] [Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles](https://arxiv.org/abs/2504.12312)
*Zihao Xu,Junchen Ding,Yiling Lou,Kun Zhang,Dong Gong,Yuekang Li*

Main category: cs.CL

TL;DR: 介绍SmartyPat-Bench，一个基于Reddit帖子构建的逻辑谬误基准数据集，并提出自动化框架SmartyPat以生成高质量逻辑谬误数据。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在逻辑推理评估中存在过于简化或语境受限的问题，无法满足对大型语言模型（LLMs）逻辑推理能力评估的需求。

Method: 利用基于逻辑编程的Prolog规则自动生成逻辑谬误，再通过LLMs将其转化为自然语言句子，构建SmartyPat-Bench数据集。

Result: SmartyPat生成的谬误与人工生成内容在质量和微妙性上相当，且在推理步数过多时会影响谬误检测准确率，但结构化推理能提升谬误分类性能。

Conclusion: SmartyPat-Bench和SmartyPat框架为LLMs逻辑推理能力评估提供了高质量、多样化的数据集和方法，揭示了LLMs在推理步骤与谬误检测间的权衡关系。

Abstract: Large Language Models (LLMs) have achieved significant progress in language
understanding and reasoning. Evaluating and analyzing their logical reasoning
abilities has therefore become essential. However, existing datasets and
benchmarks are often limited to overly simplistic, unnatural, or contextually
constrained examples. In response to the growing demand, we introduce
SmartyPat-Bench, a challenging, naturally expressed, and systematically labeled
benchmark derived from real-world high-quality Reddit posts containing subtle
logical fallacies. Unlike existing datasets and benchmarks, it provides more
detailed annotations of logical fallacies and features more diverse data. To
further scale up the study and address the limitations of manual data
collection and labeling - such as fallacy-type imbalance and labor-intensive
annotation - we introduce SmartyPat, an automated framework powered by logic
programming-based oracles. SmartyPat utilizes Prolog rules to systematically
generate logically fallacious statements, which are then refined into fluent
natural-language sentences by LLMs, ensuring precise fallacy representation.
Extensive evaluation demonstrates that SmartyPat produces fallacies comparable
in subtlety and quality to human-generated content and significantly
outperforms baseline methods. Finally, experiments reveal nuanced insights into
LLM capabilities, highlighting that while excessive reasoning steps hinder
fallacy detection accuracy, structured reasoning enhances fallacy
categorization performance.

</details>


### [4] [Exploring the Impact of Personality Traits on Conversational Recommender Systems: A Simulation with Large Language Models](https://arxiv.org/abs/2504.12313)
*Xiaoyan Zhao,Yang Deng,Wenjie Wang,Hongzhan lin,Hong Cheng,Rui Zhang,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 研究提出基于大语言模型（LLM）的个性化感知用户模拟系统（PerCRS），探讨人格特质如何影响对话推荐系统的结果。


<details>
  <summary>Details</summary>
Motivation: 人格特质对用户互动行为有显著影响，但现有对话推荐系统（CRSs）对这方面的理解不足。

Method: 引入PerCRS，用户代理可定制人格特质和偏好，系统代理具备说服能力以模拟真实互动，并采用多角度评估确保鲁棒性。

Result: 实验表明，先进的LLMs能生成符合指定人格特质的多样化用户响应，促使CRSs动态调整推荐策略。

Conclusion: 人格特质对对话推荐系统结果有显著影响，PerCRS为相关研究提供了实证见解。

Abstract: Conversational Recommender Systems (CRSs) engage users in multi-turn
interactions to deliver personalized recommendations. The emergence of large
language models (LLMs) further enhances these systems by enabling more natural
and dynamic user interactions. However, a key challenge remains in
understanding how personality traits shape conversational recommendation
outcomes. Psychological evidence highlights the influence of personality traits
on user interaction behaviors. To address this, we introduce an LLM-based
personality-aware user simulation for CRSs (PerCRS). The user agent induces
customizable personality traits and preferences, while the system agent
possesses the persuasion capability to simulate realistic interaction in CRSs.
We incorporate multi-aspect evaluation to ensure robustness and conduct
extensive analysis from both user and system perspectives. Experimental results
demonstrate that state-of-the-art LLMs can effectively generate diverse user
responses aligned with specified personality traits, thereby prompting CRSs to
dynamically adjust their recommendation strategies. Our experimental analysis
offers empirical insights into the impact of personality traits on the outcomes
of conversational recommender systems.

</details>


### [5] [How to Detect and Defeat Molecular Mirage: A Metric-Driven Benchmark for Hallucination in LLM-based Molecular Comprehension](https://arxiv.org/abs/2504.12314)
*Hao Li,Liuzhenghao Lv,He Cao,Zijing Liu,Zhiyuan Yan,Yu Wang,Yonghong Tian,Yu Li,Li Yuan*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型在分子理解任务中的幻觉问题，提出了评估指标Mol-Hallu和减少幻觉的后处理方法HRPP。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在分子理解任务中存在幻觉问题，导致药物设计和利用中的错误，需要有效评估和缓解方法。

Method: 引入Mol-Hallu指标量化幻觉程度，并提出HRPP后处理阶段减轻分子幻觉。

Result: 实验证明HRPP在仅解码器和编码器-解码器分子LLMs中有效减少幻觉。

Conclusion: 研究为提高科学应用中LLMs的可靠性提供了关键见解。

Abstract: Large language models are increasingly used in scientific domains, especially
for molecular understanding and analysis. However, existing models are affected
by hallucination issues, resulting in errors in drug design and utilization. In
this paper, we first analyze the sources of hallucination in LLMs for molecular
comprehension tasks, specifically the knowledge shortcut phenomenon observed in
the PubChem dataset. To evaluate hallucination in molecular comprehension tasks
with computational efficiency, we introduce \textbf{Mol-Hallu}, a novel
free-form evaluation metric that quantifies the degree of hallucination based
on the scientific entailment relationship between generated text and actual
molecular properties. Utilizing the Mol-Hallu metric, we reassess and analyze
the extent of hallucination in various LLMs performing molecular comprehension
tasks. Furthermore, the Hallucination Reduction Post-processing stage~(HRPP) is
proposed to alleviate molecular hallucinations, Experiments show the
effectiveness of HRPP on decoder-only and encoder-decoder molecular LLMs. Our
findings provide critical insights into mitigating hallucination and improving
the reliability of LLMs in scientific applications.

</details>


### [6] [Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models](https://arxiv.org/abs/2504.12315)
*Xingguang Ji,Jiakang Wang,Hongzhi Zhang,Jingyuan Zhang,Haonan Zhou,Chenxi Sun,Yahui Liu,Qi Wang,Fuzheng Zhang*

Main category: cs.CL

TL;DR: 介绍了Capybara-OMNI，一种轻量高效的多模态大型语言模型，支持文字、图像、视频和音频理解，并公开了模型和数据。


<details>
  <summary>Details</summary>
Motivation: 由于构建和训练多模态数据对的复杂性，开发强大的多模态大型语言模型仍耗时且计算量大。

Method: 详细介绍了框架设计、数据构建和训练方法，逐步开发模型，并提供专属基准测试验证多模态理解能力。

Result: 遵循指导可高效构建性能优异的模型，并在同规模模型中表现竞争力。

Conclusion: 公开了Capybara-OMNI及其聊天版本，包括模型权重、部分训练数据和推理代码，支持实时人机交互等任务。

Abstract: With the development of Multimodal Large Language Models (MLLMs), numerous
outstanding accomplishments have emerged within the open-source community. Due
to the complexity of creating and training multimodal data pairs, it is still a
computational and time-consuming process to build powerful MLLMs. In this work,
we introduce Capybara-OMNI, an MLLM that trains in a lightweight and efficient
manner and supports understanding text, image, video, and audio modalities. We
present in detail the framework design, the data construction, and the training
recipe, to develop an MLLM step-by-step to obtain competitive performance. We
also provide exclusive benchmarks utilized in our experiments to show how to
properly verify understanding capabilities across different modalities. Results
show that by following our guidance, we can efficiently build an MLLM that
achieves competitive performance among models of the same scale on various
multimodal benchmarks. Additionally, to enhance the multimodal instruction
following and conversational capabilities of the model, we further discuss how
to train the chat version upon an MLLM understanding model, which is more in
line with user habits for tasks like real-time interaction with humans. We
publicly disclose the Capybara-OMNI model, along with its chat-based version.
The disclosure includes both the model weights, a portion of the training data,
and the inference codes, which are made available on GitHub.

</details>


### [7] [Data Metabolism: An Efficient Data Design Schema For Vision Language Model](https://arxiv.org/abs/2504.12316)
*Jingyuan Zhang,Hongzhi Zhang,Zhou Haonan,Chenxi Sun,Xingguang ji,Jiakang Wang,Fanheng Kong,Yahui Liu,Qi Wang,Fuzheng Zhang*

Main category: cs.CL

TL;DR: 提出了数据代谢的概念和数据为中心的框架，构建高效的视觉语言模型Capybara-VL，其性能超越更大的开源模型，并与领先的专有模型媲美。


<details>
  <summary>Details</summary>
Motivation: 数据整理在训练强大的视觉语言模型(VLMs)中起到关键作用，需要一种持续改进的方法。

Method: 引入数据代谢概念，通过数据整理和迭代形成闭环系统，详细说明如何处理海量数据集并构建用户特定的数据飞轮。

Result: 发布的Capybara-VL在多模态任务中表现优异，超越多个大小10倍的开源模型，与领先专有模型性能相当。

Conclusion: 数据为中心的框架展现出强大潜力，可训练更小更高效的VLMs。

Abstract: Data curation plays a crucial role in training powerful Visual Language
Models (VLMs). In this work, we introduce the concept of Data Metabolism and
present our data-centric framework to build VLMs throughout the development
lifecycle. Starting from a standard model architecture, we discuss and provide
insights into two crucial development steps: data curation and iteration,
forming a closed-loop system that continuously improves model performance. We
show a detailed codebook on how to process existing massive datasets and build
user-specific data flywheel. As a demonstration, we release a VLM, named
Capybara-VL, which excels in typical multimodal tasks (e.g. , visual question
answering, scientific reasoning, and text-rich tasks). Despite its relatively
compact size, Capybara-VL surpasses several open-source models that are up to
10 times larger in size. Moreover, it achieves results that are on par with
those of several leading proprietary models, demonstrating its remarkable
competitiveness. These results highlight the power of our data-centric
framework and the potential of training smaller and more efficient VLMs.

</details>


### [8] [ChatGPT as Linguistic Equalizer? Quantifying LLM-Driven Lexical Shifts in Academic Writing](https://arxiv.org/abs/2504.12317)
*Dingkang Lin,Naixuan Zhao,Dan Tian,Jiang Li*

Main category: cs.CL

TL;DR: 研究表明ChatGPT能显著提升非英语母语学者(NNES)论文摘要的词汇复杂度，尤其在预印本、技术、生物领域及低影响因子期刊中表现突出。


<details>
  <summary>Details</summary>
Motivation: 探讨ChatGPT是否减轻NNES学者在学术写作中的语言障碍，促进学术公平。

Method: 使用MTLD指标量化词汇复杂度，并采用双重差分法(DID)分析OpenAlex数据库中280万篇论文(2020-2024)的因果关系。

Result: ChatGPT显著提升NNES作者摘要的词汇复杂度（尤其在预印本、技术/生物学领域及低影响力期刊），且结果具有因果效应。

Conclusion: ChatGPT有效减少语言学差距，推动全球学术界公平性。

Abstract: The advent of ChatGPT has profoundly reshaped scientific research practices,
particularly in academic writing, where non-native English-speakers (NNES)
historically face linguistic barriers. This study investigates whether ChatGPT
mitigates these barriers and fosters equity by analyzing lexical complexity
shifts across 2.8 million articles from OpenAlex (2020-2024). Using the Measure
of Textual Lexical Diversity (MTLD) to quantify vocabulary sophistication and a
difference-in-differences (DID) design to identify causal effects, we
demonstrate that ChatGPT significantly enhances lexical complexity in
NNES-authored abstracts, even after controlling for article-level controls,
authorship patterns, and venue norms. Notably, the impact is most pronounced in
preprint papers, technology- and biology-related fields and lower-tier
journals. These findings provide causal evidence that ChatGPT reduces
linguistic disparities and promotes equity in global academia.

</details>


### [9] [Has the Creativity of Large-Language Models peaked? An analysis of inter- and intra-LLM variability](https://arxiv.org/abs/2504.12320)
*Jennifer Haase,Paul H. P. Hanel,Sebastian Pokutta*

Main category: cs.CL

TL;DR: 研究发现，尽管大型语言模型（LLMs）在创造性任务中表现优于人类平均水平，但其创造性随时间并未提升，且存在显著的模型内部变异性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在创造性任务中的表现及其随时间的变化，以及模型内部输出的变异性。

Method: 使用发散联想任务（DAT）和替代用途任务（AUT）评估14种广泛使用的LLMs。

Result: LLMs在AUT中的表现优于人类平均水平，但仅有0.28%的输出达到人类创造性的前10%。模型中存在显著的内部变异性，且GPT-4表现较先前研究下降。

Conclusion: 研究强调需要更细致的评估框架，并指出在选择模型、设计提示和重复评估时的重要性，以确保在创造性应用中准确评估LLMs的潜力。

Abstract: Following the widespread adoption of ChatGPT in early 2023, numerous studies
reported that large language models (LLMs) can match or even surpass human
performance in creative tasks. However, it remains unclear whether LLMs have
become more creative over time, and how consistent their creative output is. In
this study, we evaluated 14 widely used LLMs -- including GPT-4, Claude, Llama,
Grok, Mistral, and DeepSeek -- across two validated creativity assessments: the
Divergent Association Task (DAT) and the Alternative Uses Task (AUT). Contrary
to expectations, we found no evidence of increased creative performance over
the past 18-24 months, with GPT-4 performing worse than in previous studies.
For the more widely used AUT, all models performed on average better than the
average human, with GPT-4o and o3-mini performing best. However, only 0.28% of
LLM-generated responses reached the top 10% of human creativity benchmarks.
Beyond inter-model differences, we document substantial intra-model
variability: the same LLM, given the same prompt, can produce outputs ranging
from below-average to original. This variability has important implications for
both creativity research and practical applications. Ignoring such variability
risks misjudging the creative potential of LLMs, either inflating or
underestimating their capabilities. The choice of prompts affected LLMs
differently. Our findings underscore the need for more nuanced evaluation
frameworks and highlight the importance of model selection, prompt design, and
repeated assessment when using Generative AI (GenAI) tools in creative
contexts.

</details>


### [10] [AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks](https://arxiv.org/abs/2504.12321)
*Charlotte Siska,Anush Sankaran*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the past few years, Language Models (LMs) have shown par-human
capabilities in several domains. Despite their practical applications and
exceeding user consumption, they are susceptible to jailbreaks when malicious
input exploits the LM's weaknesses, causing it to deviate from its intended
behavior. Current defensive strategies either classify the input prompt as
adversarial or prevent LMs from generating harmful outputs. However, it is
challenging to explain the reason behind the malicious nature of the jailbreak,
which results in a wide variety of closed-box approaches. In this research, we
propose and demonstrate that system-prompt attention from Small Language Models
(SLMs) can be used to characterize adversarial prompts, providing a novel,
explainable, and cheaper defense approach called AttentionDefense. Our research
suggests that the attention mechanism is an integral component in understanding
and explaining how LMs respond to malicious input that is not captured in the
semantic meaning of text embeddings. The proposed AttentionDefense is evaluated
against existing jailbreak benchmark datasets. Ablation studies show that
SLM-based AttentionDefense has equivalent or better jailbreak detection
performance compared to text embedding-based classifiers and GPT-4 zero-shot
detectors.To further validate the efficacy of the proposed approach, we
generate a dataset of novel jailbreak variants of the existing benchmark
dataset using a closed-loop LLM-based multi-agent system. We demonstrate that
the proposed AttentionDefense approach performs robustly on this novel
jailbreak dataset while existing approaches suffer in performance.
Additionally, for practical purposes AttentionDefense is an ideal solution as
it has the computation requirements of a small LM but the performance of a LLM
detector.

</details>


### [11] [A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis](https://arxiv.org/abs/2504.12322)
*Xin Gao,Qizhi Pei,Zinan Tang,Yu Li,Honglin Lin,Jiang Wu,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 提出协作框架GRA，通过多个小型LLM协作实现高质量数据合成。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）在数据合成中的高成本、低效和潜在偏见问题，小型LLM虽可持续但能力有限。

Method: 采用Generator、Reviewer、Adjudicator三个角色的小型LLM协作框架模拟同行评审流程。

Result: 实验表明GRA在多个基准测试中数据质量优于或相当于大型LLM（如Qwen-2.5-72B-Instruct）。

Conclusion: 证明小型LLM协作可实现高质量数据合成，无需依赖大型模型。

Abstract: While data synthesis and distillation are promising strategies to enhance
small language models, current approaches heavily rely on Large Language Models
(LLMs), which suffer from high computational costs, environmental inefficiency,
and potential biases inherited from monolithic architectures. In contrast,
smaller LLMs are more accessible and sustainable, but their individual
capabilities often fall short in generating high-quality, diverse, and reliable
data. Inspired by collaborative human processes (e.g., peer review), we propose
a multiple small LLMs involved framework, GRA, that aggregates specialized
roles across small LLMs to iterative refinement and quality control typically
achieved by a single large LLM. In this collaborative framework, multiple small
LLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a
peer-review-inspired data synthesis pipeline. The Generator proposes initial
data samples, the Reviewer critiques their quality and diversity, and the
Adjudicator resolves conflicts to finalize the output. By decomposing the
synthesis process into specialized sub-tasks, collaborative small LLMs can
achieve data-level parity with large LLM-based distillation. Through
experiments across multiple benchmarks, we demonstrate that GRA-produced data
matches or exceeds the quality of single large LLM outputs, e.g.,
Qwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large
models for high-quality data synthesis, advocating instead for strategic
coordination of smaller agents. Our datasets, models, and code are publicly
available at https://github.com/GX-XinGao/GRA.

</details>


### [12] [The Other Side of the Coin: Exploring Fairness in Retrieval-Augmented Generation](https://arxiv.org/abs/2504.12323)
*Zheng Zhang,Ning Li,Qi Liu,Rui Li,Weibo Gao,Qingyang Mao,Zhenya Huang,Baosheng Yu,Dacheng Tao*

Main category: cs.CL

TL;DR: 研究探讨了检索增强生成（RAG）对大型语言模型（LLMs）公平性的影响，并提出两种方法来减轻小规模LLMs中的不公平问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究RAG技术在具有重大社会影响的领域中应用时，如何影响LLMs的公平性。

Method: 通过改变LLMs、检索器和检索来源进行广泛实验，并提出两种方法：FairFT（对齐检索器与LLM的公平性）和FairFilter（检索后过滤偏颇内容）。

Result: 实验发现，模型规模小于8B时，RAG机制往往会加剧小规模LLMs的不公平性。提出的FairFT和FairFilter方法能有效改善公平性而不影响性能。

Conclusion: 研究表明模型规模对RAG框架中的公平性有重要影响，并验证了所提方法在提升小规模LLMs公平性方面的有效性。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
retrieving relevant document from external knowledge sources. By referencing
this external knowledge, RAG effectively reduces the generation of factually
incorrect content and addresses hallucination issues within LLMs. Recently,
there has been growing attention to improving the performance and efficiency of
RAG systems from various perspectives. While these advancements have yielded
significant results, the application of RAG in domains with considerable
societal implications raises a critical question about fairness: What impact
does the introduction of the RAG paradigm have on the fairness of LLMs? To
address this question, we conduct extensive experiments by varying the LLMs,
retrievers, and retrieval sources. Our experimental analysis reveals that the
scale of the LLMs plays a significant role in influencing fairness outcomes
within the RAG framework. When the model scale is smaller than 8B, the
integration of retrieval mechanisms often exacerbates unfairness in small-scale
LLMs (e.g., LLaMA3.2-1B, Mistral-7B, and LLaMA3-8B). To mitigate the fairness
issues introduced by RAG for small-scale LLMs, we propose two approaches,
FairFT and FairFilter. Specifically, in FairFT, we align the retriever with the
LLM in terms of fairness, enabling it to retrieve documents that facilitate
fairer model outputs. In FairFilter, we propose a fairness filtering mechanism
to filter out biased content after retrieval. Finally, we validate our proposed
approaches on real-world datasets, demonstrating their effectiveness in
improving fairness while maintaining performance.

</details>


### [13] [Cross-Document Cross-Lingual Natural Language Inference via RST-enhanced Graph Fusion and Interpretability Prediction](https://arxiv.org/abs/2504.12324)
*Mengying Yuan,Wangzi Xuan,Fei Li*

Main category: cs.CL

TL;DR: 本文提出了一种跨文档跨语言自然语言推理（CDCL-NLI）的新范式，构建了包含1110个实例、覆盖26种语言的高质量数据集，并提出了一种集成RST增强图融合和可解释性预测的创新方法，显著优于传统NLI模型和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 跨文档跨语言自然语言推理（CDCL-NLI）在当前研究中尚未充分探索，本研究旨在填补这一空白，扩展NLI能力至多文档、多语言场景。

Method: 提出了一种创新方法，结合RST增强的图融合和可解释性预测，使用RST在RGAT上进行跨文档上下文建模，并基于词汇链的结构感知语义对齐机制进行跨语言理解，同时开发了EDU级别的归因框架生成提取性解释。

Result: 实验表明，该方法性能显著优于传统NLI模型（如DocNLI和R2F）和大型语言模型（如Llama3和GPT-4o）。

Conclusion: 本研究为NLI研究提供了新的视角，并将促进跨文档跨语言上下文理解、语义检索和可解释性推理的研究兴趣。数据集和代码已公开。

Abstract: Natural Language Inference (NLI) is a fundamental task in both natural
language processing and information retrieval. While NLI has developed many
sub-directions such as sentence-level NLI, document-level NLI and cross-lingual
NLI, Cross-Document Cross-Lingual NLI (CDCL-NLI) remains largely unexplored. In
this paper, we propose a novel paradigm for CDCL-NLI that extends traditional
NLI capabilities to multi-document, multilingual scenarios. To support this
task, we construct a high-quality CDCL-NLI dataset including 1,110 instances
and spanning 26 languages. To build a baseline for this task, we also propose
an innovative method that integrates RST-enhanced graph fusion and
interpretability prediction. Our method employs RST (Rhetorical Structure
Theory) on RGAT (Relation-aware Graph Attention Network) for cross-document
context modeling, coupled with a structure-aware semantic alignment mechanism
based on lexical chains for cross-lingual understanding. For NLI
interpretability, we develop an EDU-level attribution framework that generates
extractive explanations. Extensive experiments demonstrate our approach's
superior performance, achieving significant improvements over both traditional
NLI models such as DocNLI and R2F, as well as LLMs like Llama3 and GPT-4o. Our
work sheds light on the study of NLI and will bring research interest on
cross-document cross-lingual context understanding, semantic retrieval and
interpretability inference. Our dataset and code are available at
\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-Link for peer
review}.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [14] [DMM: Building a Versatile Image Generation Model via Distillation-Based Model Merging](https://arxiv.org/abs/2504.12364)
*Tianhui Song,Weixin Feng,Shuai Wang,Xubin Li,Tiezheng Ge,Bo Zheng,Limin Wang*

Main category: cs.CV

TL;DR: 提出了一种基于分数蒸馏的模型合并方法（DMM），用于将多个文本到图像生成模型合并为一个多功能模型，实现可控的任意风格生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型合并方法在文本到图像生成任务中因风格多样导致的兼容性和混淆问题。

Method: 引入风格可提示的图像生成流程，提出基于分数蒸馏的模型合并范式（DMM），并重新定义模型合并的目标和评估协议。

Result: 实验表明，DMM能够紧凑地重组多个教师模型的知识，并实现可控的任意风格生成。

Conclusion: DMM有效解决了多模型合并中的冗余和存储问题，提升了文本到图像生成模型的灵活性和可控性。

Abstract: The success of text-to-image (T2I) generation models has spurred a
proliferation of numerous model checkpoints fine-tuned from the same base model
on various specialized datasets. This overwhelming specialized model production
introduces new challenges for high parameter redundancy and huge storage cost,
thereby necessitating the development of effective methods to consolidate and
unify the capabilities of diverse powerful models into a single one. A common
practice in model merging adopts static linear interpolation in the parameter
space to achieve the goal of style mixing. However, it neglects the features of
T2I generation task that numerous distinct models cover sundry styles which may
lead to incompatibility and confusion in the merged model. To address this
issue, we introduce a style-promptable image generation pipeline which can
accurately generate arbitrary-style images under the control of style vectors.
Based on this design, we propose the score distillation based model merging
paradigm (DMM), compressing multiple models into a single versatile T2I model.
Moreover, we rethink and reformulate the model merging task in the context of
T2I generation, by presenting new merging goals and evaluation protocols. Our
experiments demonstrate that DMM can compactly reorganize the knowledge from
multiple teacher models and achieve controllable arbitrary-style generation.

</details>


### [15] [Geographical Context Matters: Bridging Fine and Coarse Spatial Information to Enhance Continental Land Cover Mapping](https://arxiv.org/abs/2504.12368)
*Babak Ghassemi,Cassio Fraga-Dantas,Raffaele Gaetano,Dino Ienco,Omid Ghorbanzadeh,Emma Izquierdo-Verdiguier,Francesco Vuolo*

Main category: cs.CV

TL;DR: 提出BRIDGE-LC框架，整合多尺度地理空间信息以提高土地利用和土地覆盖分类的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在处理地球观测数据时忽视地理空间元数据，限制了分类的准确性和扩展性。

Method: 使用双层表示集成框架BRIDGE-LC，结合精细和粗糙空间信息，训练时采用多尺度信息，推理时仅需精细信息。

Result: 集成地理空间信息显著提高了分类性能，尤其是在联合使用精细和粗糙空间信息时效果最佳。

Conclusion: BRIDGE-LC框架有效提升了土地利用和覆盖分类的准确性和可扩展性，同时保持了计算效率。

Abstract: Land use and land cover mapping from Earth Observation (EO) data is a
critical tool for sustainable land and resource management. While advanced
machine learning and deep learning algorithms excel at analyzing EO imagery
data, they often overlook crucial geospatial metadata information that could
enhance scalability and accuracy across regional, continental, and global
scales. To address this limitation, we propose BRIDGE-LC (Bi-level
Representation Integration for Disentangled GEospatial Land Cover), a novel
deep learning framework that integrates multi-scale geospatial information into
the land cover classification process. By simultaneously leveraging
fine-grained (latitude/longitude) and coarse-grained (biogeographical region)
spatial information, our lightweight multi-layer perceptron architecture learns
from both during training but only requires fine-grained information for
inference, allowing it to disentangle region-specific from region-agnostic land
cover features while maintaining computational efficiency. To assess the
quality of our framework, we use an open-access in-situ dataset and adopt
several competing classification approaches commonly considered for large-scale
land cover mapping. We evaluated all approaches through two scenarios: an
extrapolation scenario in which training data encompasses samples from all
biogeographical regions, and a leave-one-region-out scenario where one region
is excluded from training. We also explore the spatial representation learned
by our model, highlighting a connection between its internal manifold and the
geographical information used during training. Our results demonstrate that
integrating geospatial information improves land cover mapping performance,
with the most substantial gains achieved by jointly leveraging both fine- and
coarse-grained spatial information.

</details>


### [16] [WORLDMEM: Long-term Consistent World Simulation with Memory](https://arxiv.org/abs/2504.12369)
*Zeqi Xiao,Yushi Lan,Yifan Zhou,Wenqi Ouyang,Shuai Yang,Yanhong Zeng,Xingang Pan*

Main category: cs.CV

TL;DR: WorldMem框架通过记忆库和注意力机制增强场景生成，保持长期3D空间一致性。


<details>
  <summary>Details</summary>
Motivation: 解决世界模拟中因时间上下文窗口有限导致的长期一致性维护问题，特别是3D空间一致性。

Method: 使用记忆库存储记忆帧和状态（如姿态和时间戳），通过记忆注意力机制提取相关信息。

Result: 实验验证了该方法在虚拟和真实场景中有效重建观察到的场景，并捕捉动态演化。

Conclusion: WorldMem框架不仅能建模静态世界，还能捕捉其动态变化，支持模拟世界中的感知和交互。

Abstract: World simulation has gained increasing popularity due to its ability to model
virtual environments and predict the consequences of actions. However, the
limited temporal context window often leads to failures in maintaining
long-term consistency, particularly in preserving 3D spatial consistency. In
this work, we present WorldMem, a framework that enhances scene generation with
a memory bank consisting of memory units that store memory frames and states
(e.g., poses and timestamps). By employing a memory attention mechanism that
effectively extracts relevant information from these memory frames based on
their states, our method is capable of accurately reconstructing previously
observed scenes, even under significant viewpoint or temporal gaps.
Furthermore, by incorporating timestamps into the states, our framework not
only models a static world but also captures its dynamic evolution over time,
enabling both perception and interaction within the simulated world. Extensive
experiments in both virtual and real scenarios validate the effectiveness of
our approach.

</details>


### [17] [InstantCharacter: Personalize Any Characters with a Scalable Diffusion Transformer Framework](https://arxiv.org/abs/2504.12395)
*Jiale Tao,Yanbing Zhang,Qixun Wang,Yiji Cheng,Haofan Wang,Xu Bai,Zhengguang Zhou,Ruihuang Li,Linqing Wang,Chunyu Wang,Qin Lin,Qinglin Lu*

Main category: cs.CV

TL;DR: InstantCharacter是一个基于基础扩散变换器的字符定制框架，能够实现开放领域的个性化，保持高保真结果，并通过大规模数据集优化身份一致性和文本可编辑性。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的主题定制方法泛化能力有限且图像质量受损，而基于优化的方法需要特定主题的微调，降低了文本可控性。

Method: 使用基础扩散变换器和可扩展适配器处理开放领域字符特征，并通过包含10百万样本的大规模数据集进行训练。

Result: InstantCharacter能够生成高保真、文本可控且字符一致的图像，在字符驱动图像生成方面设立了新基准。

Conclusion: InstantCharacter在开放领域个性化、高保真结果和文本可控性方面表现出色，为字符定制提供了高效解决方案。

Abstract: Current learning-based subject customization approaches, predominantly
relying on U-Net architectures, suffer from limited generalization ability and
compromised image quality. Meanwhile, optimization-based methods require
subject-specific fine-tuning, which inevitably degrades textual
controllability. To address these challenges, we propose InstantCharacter, a
scalable framework for character customization built upon a foundation
diffusion transformer. InstantCharacter demonstrates three fundamental
advantages: first, it achieves open-domain personalization across diverse
character appearances, poses, and styles while maintaining high-fidelity
results. Second, the framework introduces a scalable adapter with stacked
transformer encoders, which effectively processes open-domain character
features and seamlessly interacts with the latent space of modern diffusion
transformers. Third, to effectively train the framework, we construct a
large-scale character dataset containing 10-million-level samples. The dataset
is systematically organized into paired (multi-view character) and unpaired
(text-image combinations) subsets. This dual-data structure enables
simultaneous optimization of identity consistency and textual editability
through distinct learning pathways. Qualitative experiments demonstrate the
advanced capabilities of InstantCharacter in generating high-fidelity,
text-controllable, and character-consistent images, setting a new benchmark for
character-driven image generation. Our source code is available at
https://github.com/Tencent/InstantCharacter.

</details>


### [18] [NTIRE 2025 Challenge on Event-Based Image Deblurring: Methods and Results](https://arxiv.org/abs/2504.12401)
*Lei Sun,Andrea Alfarano,Peiqi Duan,Shaolin Su,Kaiwei Wang,Boxin Shi,Radu Timofte,Danda Pani Paudel,Luc Van Gool,Qinglin Liu,Wei Yu,Xiaoqian Lv,Lu Yang,Shuigen Wang,Shengping Zhang,Xiangyang Ji,Long Bao,Yuqiang Yang,Jinao Song,Ziyi Wang,Shuang Wen,Heng Sun,Kean Liu,Mingchen Zhong,Senyan Xu,Zhijing Sun,Jiaying Zhu,Chengjie Ge,Xingbo Wang,Yidi Liu,Xin Lu,Xueyang Fu,Zheng-Jun Zha,Dawei Fan,Dafeng Zhang,Yong Yang,Siru Zhang,Qinghua Yang,Hao Kang,Huiyuan Fu,Heng Zhang,Hongyuan Yu,Zhijuan Huang,Shuoyan Wei,Feng Li,Runmin Cong,Weiqi Luo,Mingyun Lin,Chenxu Jiang,Hongyi Liu,Lei Yu,Weilun Li,Jiajun Zhai,Tingting Lin,Shuang Ma,Sai Zhou,Zhanwen Liu,Yang Wang,Eiffel Chong,Nuwan Bandara,Thivya Kandappu,Archan Misra,Yihang Chen,Zhan Li,Weijun Yuan,Wenzhuo Wang,Boyang Yao,Zhanglu Chen,Yijing Sun,Tianjiao Wan,Zijian Gao,Qisheng Xu,Kele Xu,Yukun Zhang,Yu He,Xiaoyan Xie,Tao Fu,Yashu Gautamkumar Patel,Vihar Ramesh Jain,Divesh Basina,Rishik Ashili,Manish Kumar Manjhi,Sourav Kumar,Prinon Benny,Himanshu Ghunawat,B Sri Sairam Gautam,Anett Varghese,Abhishek Yadav*

Main category: cs.CV

TL;DR: 本文介绍了NTIRE 2025第一届基于事件的图像去模糊挑战赛的概述、方法和结果。


<details>
  <summary>Details</summary>
Motivation: 挑战赛的主要目标是设计一种基于事件的方法，以实现高质量的图像去模糊。

Method: 利用事件和图像作为输入进行单图像去模糊，性能通过峰值信噪比（PSNR）定量评估。

Result: 199名参与者注册，15支团队成功提交了有效结果，为当前基于事件的图像去模糊技术提供了宝贵见解。

Conclusion: 预计此次挑战赛将推动基于事件的视觉研究的进一步发展。

Abstract: This paper presents an overview of NTIRE 2025 the First Challenge on
Event-Based Image Deblurring, detailing the proposed methodologies and
corresponding results. The primary goal of the challenge is to design an
event-based method that achieves high-quality image deblurring, with
performance quantitatively assessed using Peak Signal-to-Noise Ratio (PSNR).
Notably, there are no restrictions on computational complexity or model size.
The task focuses on leveraging both events and images as inputs for
single-image deblurring. A total of 199 participants registered, among whom 15
teams successfully submitted valid results, offering valuable insights into the
current state of event-based image deblurring. We anticipate that this
challenge will drive further advancements in event-based vision research.

</details>


### [19] [Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation](https://arxiv.org/abs/2504.12436)
*Nairouz Mrabah,Nicolas Richet,Ismail Ben Ayed,Éric Granger*

Main category: cs.CV

TL;DR: 提出了一种新的稀疏优化框架（SO），用于在标注数据稀缺的情况下适应视觉语言模型（VLM）到新领域，有效减少过拟合并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法如低秩重参数化在处理VLM的少样本适应时存在过拟合和计算限制问题，且泛化能力不足，需要大量超参数调优。

Method: 采用稀疏优化（SO）框架，结合局部稀疏全局密度（更新少量参数）和局部随机全局重要（随机选择梯度并基于重要性剪枝）两种策略。

Result: 在11个不同数据集上的实验表明，SO在少样本适应中达到最先进性能，同时降低了内存开销。

Conclusion: SO框架通过动态调整少量参数和稀疏化梯度，显著减少了过拟合，在低数据情况下实现了稳定的模型适应。

Abstract: Adapting Vision-Language Models (VLMs) to new domains with few labeled
samples remains a significant challenge due to severe overfitting and
computational constraints. State-of-the-art solutions, such as low-rank
reparameterization, mitigate these issues but often struggle with
generalization and require extensive hyperparameter tuning. In this paper, a
novel Sparse Optimization (SO) framework is proposed. Unlike low-rank
approaches that typically constrain updates to a fixed subspace, our SO method
leverages high sparsity to dynamically adjust very few parameters. We introduce
two key paradigms. First, we advocate for \textit{local sparsity and global
density}, which updates a minimal subset of parameters per iteration while
maintaining overall model expressiveness. As a second paradigm, we advocate for
\textit{local randomness and global importance}, which sparsifies the gradient
using random selection while pruning the first moment based on importance. This
combination significantly mitigates overfitting and ensures stable adaptation
in low-data regimes. Extensive experiments on 11 diverse datasets show that SO
achieves state-of-the-art few-shot adaptation performance while reducing memory
overhead.

</details>


### [20] [3D-PointZshotS: Geometry-Aware 3D Point Cloud Zero-Shot Semantic Segmentation Narrowing the Visual-Semantic Gap](https://arxiv.org/abs/2504.12442)
*Minmin Yang,Huantao Ren,Senem Velipasalar*

Main category: cs.CV

TL;DR: 提出3D-PointZshotS框架，通过几何原型和自一致性损失提升零样本3D点云分割的迁移性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本3D点云分割方法在从已知类到未知类、从语义空间到视觉空间的迁移性上表现不佳。

Method: 使用潜在几何原型（LGPs）通过交叉注意力机制增强特征生成和对齐，并引入自一致性损失提升鲁棒性，同时在共享空间中重新表示视觉和语义特征。

Result: 在ScanNet、SemanticKITTI和S3DIS数据集上，该方法在谐波mIoU指标上优于四个基线方法。

Conclusion: 3D-PointZshotS通过几何感知和特征对齐，显著提升了零样本3D点云分割的性能和迁移性。

Abstract: Existing zero-shot 3D point cloud segmentation methods often struggle with
limited transferability from seen classes to unseen classes and from semantic
to visual space. To alleviate this, we introduce 3D-PointZshotS, a
geometry-aware zero-shot segmentation framework that enhances both feature
generation and alignment using latent geometric prototypes (LGPs).
Specifically, we integrate LGPs into a generator via a cross-attention
mechanism, enriching semantic features with fine-grained geometric details. To
further enhance stability and generalization, we introduce a self-consistency
loss, which enforces feature robustness against point-wise perturbations.
Additionally, we re-represent visual and semantic features in a shared space,
bridging the semantic-visual gap and facilitating knowledge transfer to unseen
classes. Experiments on three real-world datasets, namely ScanNet,
SemanticKITTI, and S3DIS, demonstrate that our method achieves superior
performance over four baselines in terms of harmonic mIoU. The code is
available at \href{https://github.com/LexieYang/3D-PointZshotS}{Github}.

</details>


### [21] [DG-MVP: 3D Domain Generalization via Multiple Views of Point Clouds for Classification](https://arxiv.org/abs/2504.12456)
*Huantao Ren,Minmin Yang,Senem Velipasalar*

Main category: cs.CV

TL;DR: 提出一种新的3D点云领域泛化方法，通过多视角2D投影和卷积模型解决点云缺失和遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云分类依赖大规模标注数据，但标注成本高。CAD模型生成的点云与LiDAR数据存在领域差异，需提升跨领域泛化能力。

Method: 使用多视角2D投影缓解点云缺失问题，并采用卷积模型提取特征。

Result: 在PointDA-10和Sim-to-Real基准测试中表现优于基线方法，能有效从合成领域迁移到真实领域。

Conclusion: 该方法通过2D投影和卷积模型显著提升了3D点云的跨领域泛化性能。

Abstract: Deep neural networks have achieved significant success in 3D point cloud
classification while relying on large-scale, annotated point cloud datasets,
which are labor-intensive to build. Compared to capturing data with LiDAR
sensors and then performing annotation, it is relatively easier to sample point
clouds from CAD models. Yet, data sampled from CAD models is regular, and does
not suffer from occlusion and missing points, which are very common for LiDAR
data, creating a large domain shift. Therefore, it is critical to develop
methods that can generalize well across different point cloud domains. %In this
paper, we focus on the 3D point cloud domain generalization problem. Existing
3D domain generalization methods employ point-based backbones to extract point
cloud features. Yet, by analyzing point utilization of point-based methods and
observing the geometry of point clouds from different domains, we have found
that a large number of point features are discarded by point-based methods
through the max-pooling operation. This is a significant waste especially
considering the fact that domain generalization is more challenging than
supervised learning, and point clouds are already affected by missing points
and occlusion to begin with. To address these issues, we propose a novel method
for 3D point cloud domain generalization, which can generalize to unseen
domains of point clouds. Our proposed method employs multiple 2D projections of
a 3D point cloud to alleviate the issue of missing points and involves a simple
yet effective convolution-based model to extract features. The experiments,
performed on the PointDA-10 and Sim-to-Real benchmarks, demonstrate the
effectiveness of our proposed method, which outperforms different baselines,
and can transfer well from synthetic domain to real-world domain.

</details>


### [22] [AdaVid: Adaptive Video-Language Pretraining](https://arxiv.org/abs/2504.12513)
*Chaitanya Patel,Juan Carlos Niebles,Ehsan Adeli*

Main category: cs.CV

TL;DR: AdaVid框架通过自适应调整计算资源，高效处理视频，适用于不同计算能力的设备。


<details>
  <summary>Details</summary>
Motivation: 现有视频编码器计算需求高，且仅能处理短视频片段，难以在计算受限的边缘设备上部署。

Method: AdaVid采用自适应Transformer块，动态调整隐藏嵌入维度，并结合轻量级层次网络处理长视频。

Result: AdaVid-EgoVLP在Ego4D上的表现优于标准EgoVLP，计算资源减半时性能相当；在Diving48上展示更多帧数不超限的能力。

Conclusion: AdaVid为视频编码提供了资源高效性与性能的平衡方案，适用于边缘设备和长视频处理。

Abstract: Contrastive video-language pretraining has demonstrated great success in
learning rich and robust video representations. However, deploying such video
encoders on compute-constrained edge devices remains challenging due to their
high computational demands. Additionally, existing models are typically trained
to process only short video clips, often limited to 4 to 64 frames. In this
paper, we introduce AdaVid, a flexible architectural framework designed to
learn efficient video encoders that can dynamically adapt their computational
footprint based on available resources. At the heart of AdaVid is an adaptive
transformer block, inspired by Matryoshka Representation Learning, which allows
the model to adjust its hidden embedding dimension at inference time. We show
that AdaVid-EgoVLP, trained on video-narration pairs from the large-scale Ego4D
dataset, matches the performance of the standard EgoVLP on short video-language
benchmarks using only half the compute, and even outperforms EgoVLP when given
equal computational resources. We further explore the trade-off between frame
count and compute on the challenging Diving48 classification benchmark, showing
that AdaVid enables the use of more frames without exceeding computational
limits. To handle longer videos, we also propose a lightweight hierarchical
network that aggregates short clip features, achieving a strong balance between
compute efficiency and accuracy across several long video benchmarks.

</details>


### [23] [Event Quality Score (EQS): Assessing the Realism of Simulated Event Camera Streams via Distances in Latent Space](https://arxiv.org/abs/2504.12515)
*Kaustav Chanda,Aayush Atul Verma,Arpitsinh Vaghela,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: 研究者提出了一种名为EQS的质量评估方法，用于改进事件相机仿真数据与真实数据的匹配度。


<details>
  <summary>Details</summary>
Motivation: 事件相机在深度学习和计算机视觉中的应用受到高质量标记数据集稀缺的限制。现有的仿真数据难以准确模拟真实事件相机的数据。

Method: 研究利用RVT架构的激活特征，提出了事件质量评分（EQS），并通过DSEC驾驶数据集进行了仿真到真实的实验验证。

Result: 实验表明，更高的EQS评分预示着在仿真事件训练后模型对真实世界数据有更好的泛化能力。

Conclusion: 优化EQS有助于开发更真实的事件相机仿真器，有效减小仿真与实际数据之间的差距。

Abstract: Event cameras promise a paradigm shift in vision sensing with their low
latency, high dynamic range, and asynchronous nature of events. Unfortunately,
the scarcity of high-quality labeled datasets hinders their widespread adoption
in deep learning-driven computer vision. To mitigate this, several simulators
have been proposed to generate synthetic event data for training models for
detection and estimation tasks. However, the fundamentally different sensor
design of event cameras compared to traditional frame-based cameras poses a
challenge for accurate simulation. As a result, most simulated data fail to
mimic data captured by real event cameras. Inspired by existing work on using
deep features for image comparison, we introduce event quality score (EQS), a
quality metric that utilizes activations of the RVT architecture. Through
sim-to-real experiments on the DSEC driving dataset, it is shown that a higher
EQS implies improved generalization to real-world data after training on
simulated events. Thus, optimizing for EQS can lead to developing more
realistic event camera simulators, effectively reducing the simulation gap. EQS
is available at https://github.com/eventbasedvision/EQS.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [Interpretable AI-driven Guidelines for Type 2 Diabetes Treatment from Observational Data](https://arxiv.org/abs/2504.12417)
*Dewang Kumar Agarwal,Dimitris J. Bertsimas*

Main category: cs.AI

TL;DR: 研究创建了一种基于AI的2型糖尿病治疗进展指南，优化治疗方案并提升治疗效果。


<details>
  <summary>Details</summary>
Motivation: 旨在为2型糖尿病治疗提供精准、结构化且基于数据的临床指南，以提高治疗效果。

Method: 使用波士顿医疗中心的数据，通过机器学习和优化方法去除混杂偏差，训练基于树的AI模型，并手动整合模型形成端到端的治疗流程。

Result: 在未见的BMC患者中，该方法的中位HbA1c降低比医生方案高0.26%；在Hartford队列中高0.13%。

Conclusion: 这种精准、可解释且高效的AI支持方法预计能超越当前实践，改善患者预后。

Abstract: Objective: Create precise, structured, data-backed guidelines for type 2
diabetes treatment progression, suitable for clinical adoption.
  Research Design and Methods: Our training cohort was composed of patient
(with type 2 diabetes) visits from Boston Medical Center (BMC) from 1998 to
2014. We divide visits into 4 groups based on the patient's treatment regimen
before the visit, and further divide them into subgroups based on the
recommended treatment during the visit. Since each subgroup has observational
data, which has confounding bias (sicker patients are prescribed more
aggressive treatments), we used machine learning and optimization to remove
some datapoints so that the remaining data resembles a randomized trial. On
each subgroup, we train AI-backed tree-based models to prescribe treatment
changes. Once we train these tree models, we manually combine the models for
every group to create an end-to-end prescription pipeline for all patients in
that group. In this process, we prioritize stepping up to a more aggressive
treatment before considering less aggressive options. We tested this pipeline
on unseen data from BMC, and an external dataset from Hartford healthcare (type
2 diabetes patient visits from January 2020 to May 2024).
  Results: The median HbA1c reduction achieved by our pipelines is 0.26% more
than what the doctors achieved on the unseen BMC patients. For the Hartford
cohort, our pipelines were better by 0.13%.
  Conclusions: This precise, interpretable, and efficient AI-backed approach to
treatment progression in type 2 diabetes is predicted to outperform the current
practice and can be deployed to improve patient outcomes.

</details>


### [25] [Towards Conversational AI for Human-Machine Collaborative MLOps](https://arxiv.org/abs/2504.12477)
*George Fatouros,Georgios Makridis,George Kousiouris,John Soldatos,Anargyros Tsadimas,Dimosthenis Kyriazis*

Main category: cs.AI

TL;DR: 本文介绍了一种基于大型语言模型的对话代理系统，旨在通过自然语言交互提升人类与机器在MLOps中的协作。该系统名为Swarm Agent，采用分层模块化设计，集成多种功能代理，简化了ML流程的管理和使用。


<details>
  <summary>Details</summary>
Motivation: 解决复杂MLOps平台（如Kubeflow）的可访问性问题，使不同技术背景的用户能够更轻松地使用高级机器学习工具。

Method: 采用分层、模块化的架构，集成了KFP代理（用于ML管道编排）、MinIO代理（用于数据管理）和RAG代理（用于领域知识集成），通过迭代推理循环和情境感知处理实现功能。

Result: 开发的对话式MLOps助手降低了技术门槛，使不同技术水平的用户能够通过直观的对话界面发现、执行和监控ML流程，管理数据集和工件，并访问相关文档。

Conclusion: 该研究通过创新的对话代理系统，显著简化了MLOps平台的复杂性，使其更广泛地适用于不同技术背景的用户，同时保持了扩展到其他平台的灵活性。

Abstract: This paper presents a Large Language Model (LLM) based conversational agent
system designed to enhance human-machine collaboration in Machine Learning
Operations (MLOps). We introduce the Swarm Agent, an extensible architecture
that integrates specialized agents to create and manage ML workflows through
natural language interactions. The system leverages a hierarchical, modular
design incorporating a KubeFlow Pipelines (KFP) Agent for ML pipeline
orchestration, a MinIO Agent for data management, and a Retrieval-Augmented
Generation (RAG) Agent for domain-specific knowledge integration. Through
iterative reasoning loops and context-aware processing, the system enables
users with varying technical backgrounds to discover, execute, and monitor ML
pipelines; manage datasets and artifacts; and access relevant documentation,
all via intuitive conversational interfaces. Our approach addresses the
accessibility gap in complex MLOps platforms like Kubeflow, making advanced ML
tools broadly accessible while maintaining the flexibility to extend to other
platforms. The paper describes the architecture, implementation details, and
demonstrates how this conversational MLOps assistant reduces complexity and
lowers barriers to entry for users across diverse technical skill levels.

</details>


### [26] [Agentic AI Optimisation (AAIO): what it is, how it works, why it matters, and how to deal with it](https://arxiv.org/abs/2504.12482)
*Luciano Floridi,Carlotta Buttaboni,Emmie Hine,Jessica Morley,Claudio Novelli,Tyler Schroder*

Main category: cs.AI

TL;DR: 本文提出了代理人工智能优化（AAIO）作为一种新方法论，旨在优化代理AI系统与网站间的互动，强调其重要性及潜在的治理、伦理和法律影响。


<details>
  <summary>Details</summary>
Motivation: 随着能够独立发起数字互动的代理人工智能（AAI）系统的出现，需要一种新的优化范式来确保代理与平台间的无缝交互。

Method: 通过类比搜索引擎优化（SEO），文章介绍了AAIO方法，并探讨了网站优化与代理AI成功之间的相互依赖关系。

Result: AAIO可以创建一个良性循环，优化代理AI与平台的互动，同时文章也探讨了其治理、伦理、法律和社会影响（GELSI）。

Conclusion: AAIO在自主数字代理时代是基础数字设施的重要组成部分，需要积极监管框架以确保其益处能够公平和包容地分配。

Abstract: The emergence of Agentic Artificial Intelligence (AAI) systems capable of
independently initiating digital interactions necessitates a new optimisation
paradigm designed explicitly for seamless agent-platform interactions. This
article introduces Agentic AI Optimisation (AAIO) as an essential methodology
for ensuring effective integration between websites and agentic AI systems.
Like how Search Engine Optimisation (SEO) has shaped digital content
discoverability, AAIO can define interactions between autonomous AI agents and
online platforms. By examining the mutual interdependency between website
optimisation and agentic AI success, the article highlights the virtuous cycle
that AAIO can create. It further explores the governance, ethical, legal, and
social implications (GELSI) of AAIO, emphasising the necessity of proactive
regulatory frameworks to mitigate potential negative impacts. The article
concludes by affirming AAIO's essential role as part of a fundamental digital
infrastructure in the era of autonomous digital agents, advocating for
equitable and inclusive access to its benefits.

</details>


### [27] [Heuristic Recognition and Rapid Response to Unfamiliar Events Outside of Agent Design Scope](https://arxiv.org/abs/2504.12497)
*Robert E. Wray,Steven J. Jones,John E. Laird*

Main category: cs.AI

TL;DR: 提出一种结合元知识和元推理的新方法，用于智能体在开放式环境中快速适应陌生情况。


<details>
  <summary>Details</summary>
Motivation: 智能体在开放世界中常遇到超出其原有设计范围的陌生情况，且缺乏足够时间和知识进行详细评估，需要快速可靠的适应能力。

Method: 结合领域通用的元知识（受人类认知启发的评估）和元推理的方法。

Result: 该方法有潜力为开放式环境中的智能体提供快速、自适应的响应，满足其性能需求。

Conclusion: 所提出的新方法能更好地满足开放世界中通用智能体的性能要求，提供对陌生情况的适应性响应。

Abstract: Regardless of past learning, an agent in an open world will face unfamiliar
situations and events outside of prior experience, existing models, or
policies. Further, the agent will sometimes lack relevant knowledge and/or
sufficient time to assess the situation, generate and evaluate options, and
pursue a robustly considered course of action. How can an agent respond
reasonably to situations that are outside of its original design scope? How can
it recognize such situations sufficiently quickly and reliably to determine
reasonable, adaptive courses of action? We identify key characteristics needed
for solutions, evaluate the state-of-the-art by these requirements, and outline
a proposed, novel approach that combines domain-general meta-knowledge (in the
form of appraisals inspired by human cognition) and metareasoning. It has the
potential to provide fast, adaptive responses to unfamiliar situations, more
fully meeting the performance characteristics required for open-world, general
agents.

</details>


### [28] [Is Trust Correlated With Explainability in AI? A Meta-Analysis](https://arxiv.org/abs/2504.12529)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: 研究发现AI系统的可解释性虽能提升用户信任，但并非唯一或主导因素。


<details>
  <summary>Details</summary>
Motivation: 探讨AI可解释性与用户信任间的关系，挑战‘可解释性必然增强信任’的普遍假设。

Method: 采用元分析方法，综合分析了90项现有研究的数据。

Result: AI可解释性与用户信任之间存在显著但中等的正相关关系。

Conclusion: 研究强调在关键领域（如医疗和司法）需要促进AI的问责制和持久可信度，而非仅关注即时信任。

Abstract: This study critically examines the commonly held assumption that
explicability in artificial intelligence (AI) systems inherently boosts user
trust. Utilizing a meta-analytical approach, we conducted a comprehensive
examination of the existing literature to explore the relationship between AI
explainability and trust. Our analysis, incorporating data from 90 studies,
reveals a statistically significant but moderate positive correlation between
the explainability of AI systems and the trust they engender among users. This
indicates that while explainability contributes to building trust, it is not
the sole or predominant factor in this equation. In addition to academic
contributions to the field of Explainable AI (XAI), this research highlights
its broader socio-technical implications, particularly in promoting
accountability and fostering user trust in critical domains such as healthcare
and justice. By addressing challenges like algorithmic bias and ethical
transparency, the study underscores the need for equitable and sustainable AI
adoption. Rather than focusing solely on immediate trust, we emphasize the
normative importance of fostering authentic and enduring trustworthiness in AI
systems.

</details>


### [29] [ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition](https://arxiv.org/abs/2504.12562)
*Haidar Khan,Hisham A. Alyahya,Yazeed Alnumay,M Saiful Bari,Bülent Yener*

Main category: cs.AI

TL;DR: 本研究提出了一种新的竞争性评估协议ZeroSumEval，通过零和游戏评估大型语言模型（LLMs）的能力。


<details>
  <summary>Details</summary>
Motivation: 传统的评估方法存在过拟合、高成本和偏见等问题，亟需一种动态的评估方式。

Method: ZeroSumEval采用零和游戏评估，设计了包括安全挑战、经典游戏、知识测试和劝说挑战在内的一套多样化游戏。

Result: 实验显示，虽然前沿模型能在常见游戏中表现良好，但在创造新问题的游戏中表现不佳，且普遍在创造性任务上失败。

Conclusion: ZeroSumEval提供了标准化和可扩展的框架，改进了基于游戏的评估方法。

Abstract: Evaluating the capabilities of Large Language Models (LLMs) has traditionally
relied on static benchmark datasets, human assessments, or model-based
evaluations - methods that often suffer from overfitting, high costs, and
biases. ZeroSumEval is a novel competition-based evaluation protocol that
leverages zero-sum games to assess LLMs with dynamic benchmarks that resist
saturation. ZeroSumEval encompasses a diverse suite of games, including
security challenges (PyJail), classic games (Chess, Liar's Dice, Poker),
knowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These
games are designed to evaluate a range of AI capabilities such as strategic
reasoning, planning, knowledge application, and creativity. Building upon
recent studies that highlight the effectiveness of game-based evaluations for
LLMs, ZeroSumEval enhances these approaches by providing a standardized and
extensible framework. To demonstrate this, we conduct extensive experiments
with >7000 simulations across 7 games and 13 models. Our results show that
while frontier models from the GPT and Claude families can play common games
and answer questions, they struggle to play games that require creating novel
and challenging questions. We also observe that models cannot reliably
jailbreak each other and fail generally at tasks requiring creativity. We
release our code at https://github.com/facebookresearch/ZeroSumEval.

</details>


### [30] [The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance](https://arxiv.org/abs/2504.12612)
*Ching-Chun Chang,Isao Echizen*

Main category: cs.AI

TL;DR: 本研究提出了一种追踪多智能体生成内容来源的方法，通过建立时间戳记录的符号编年史，实现不依赖内部状态或外部元数据的生成历史追溯。


<details>
  <summary>Details</summary>
Motivation: 随着AI向多智能体协同生成内容发展，贡献的连续修改导致来源信息丢失，需建立可追溯的协作生成溯源机制。

Method: 采用符号编年史系统（含签名和时间戳的链式记录），通过生成步骤的反馈循环更新交互历史并与生成内容同步。

Result: 开发出仅通过内容即可追溯时序生成历史的框架，类比法医学监管链模式。

Conclusion: 该时序溯源系统为动态网络生态中的协作AI提供了可问责的技术基础。

Abstract: Provenance is the chronology of things, resonating with the fundamental
pursuit to uncover origins, trace connections, and situate entities within the
flow of space and time. As artificial intelligence advances towards autonomous
agents capable of interactive collaboration on complex tasks, the provenance of
generated content becomes entangled in the interplay of collective creation,
where contributions are continuously revised, extended or overwritten. In a
multi-agent generative chain, content undergoes successive transformations,
often leaving little, if any, trace of prior contributions. In this study, we
investigates the problem of tracking multi-agent provenance across the temporal
dimension of generation. We propose a chronological system for post hoc
attribution of generative history from content alone, without reliance on
internal memory states or external meta-information. At its core lies the
notion of symbolic chronicles, representing signed and time-stamped records, in
a form analogous to the chain of custody in forensic science. The system
operates through a feedback loop, whereby each generative timestep updates the
chronicle of prior interactions and synchronises it with the synthetic content
in the very act of generation. This research seeks to develop an accountable
form of collaborative artificial intelligence within evolving cyber ecosystems.

</details>


### [31] [Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning](https://arxiv.org/abs/2504.12680)
*Baining Zhao,Ziyou Wang,Jianjie Fang,Chen Gao,Fanhang Man,Jinqiang Cui,Xin Wang,Xinlei Chen,Yong Li,Wenwu Zhu*

Main category: cs.AI

TL;DR: Embodied-R提出了一种结合视觉语言模型和小型语言模型的协作框架，利用强化学习实现高效的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究探索预训练模型如何从视觉观察中获取空间推理能力，尤其是高级推理能力。

Method: 采用强化学习（RL）框架，结合视觉语言模型（VLMs）和小型语言模型（LMs），设计了一个考虑思维-答案逻辑一致性的奖励系统。

Result: 在仅5k样本训练后，Embodied-R在空间推理任务上达到了与最先进多模态推理模型相当的性能，并展现出系统性分析和上下文整合的新兴思维模式。

Conclusion: Embodied-R证明通过协作框架和强化学习，小型模型也能实现高效的慢思考能力，并在空间推理任务中取得优异表现。

Abstract: Humans can perceive and reason about spatial relationships from sequential
visual observations, such as egocentric video streams. However, how pretrained
models acquire such abilities, especially high-level reasoning, remains
unclear. This paper introduces Embodied-R, a collaborative framework combining
large-scale Vision-Language Models (VLMs) for perception and small-scale
Language Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a
novel reward system considering think-answer logical consistency, the model
achieves slow-thinking capabilities with limited computational resources. After
training on only 5k embodied video samples, Embodied-R with a 3B LM matches
state-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on
both in-distribution and out-of-distribution embodied spatial reasoning tasks.
Embodied-R also exhibits emergent thinking patterns such as systematic analysis
and contextual integration. We further explore research questions including
response length, training on VLM, strategies for reward design, and differences
in model generalization after SFT (Supervised Fine-Tuning) and RL training.

</details>


### [32] [WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents](https://arxiv.org/abs/2504.12682)
*Arth Bohra,Manvel Saroyan,Danil Melkozerov,Vahe Karufanyan,Gabriel Maher,Pascal Weinberger,Artem Harutyunyan,Giovanni Campagna*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Most recent web agent research has focused on navigation and transaction
tasks, with little emphasis on extracting structured data at scale. We present
WebLists, a benchmark of 200 data-extraction tasks across four common business
and enterprise use-cases. Each task requires an agent to navigate to a webpage,
configure it appropriately, and extract complete datasets with well-defined
schemas. We show that both LLMs with search capabilities and SOTA web agents
struggle with these tasks, with a recall of 3% and 31%, respectively, despite
higher performance on question-answering tasks.
  To address this challenge, we propose BardeenAgent, a novel framework that
enables web agents to convert their execution into repeatable programs, and
replay them at scale across pages with similar structure. BardeenAgent is also
the first LLM agent to take advantage of the regular structure of HTML. In
particular BardeenAgent constructs a generalizable CSS selector to capture all
relevant items on the page, then fits the operations to extract the data.
  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more
than doubling the performance of SOTA web agents, and reducing cost per output
row by 3x.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [33] [Unveiling Hidden Collaboration within Mixture-of-Experts in Large Language Models](https://arxiv.org/abs/2504.12359)
*Yuanbo Tang,Yan Tang,Naifan Zhang,Meixuan Chen,Yang Li*

Main category: cs.LG

TL;DR: 研究了混合专家大模型中专家间的协作模式及优化方法，提出HSDL方法识别协作模式及CAEP算法剪枝低贡献专家，实验显示性能提升2.5%。


<details>
  <summary>Details</summary>
Motivation: 混合专家大模型虽在任务适应性上表现优异，但专家协作机制尚不清楚，影响模型解释性与优化。本研究旨在揭示协作模式并优化剪枝策略。

Method: 采用分层稀疏字典学习（HSDL）揭示专家协作模式，提出贡献感知专家剪枝（CAEP）算法优化模型。

Result: 实验表明专家协作模式与输入类型相关且具语义意义；CAEP算法平均提升模型性能2.5%，优于现有方法。

Conclusion: 研究为增强混合专家大模型的效率与可解释性提供了新视角，改进了专家交互理解与模型优化方法。

Abstract: Mixture-of-Experts based large language models (MoE LLMs) have shown
significant promise in multitask adaptability by dynamically routing inputs to
specialized experts. Despite their success, the collaborative mechanisms among
experts are still not well understood, limiting both the interpretability and
optimization of these models. In this paper, we focus on two critical issues:
(1) identifying expert collaboration patterns, and (2) optimizing MoE LLMs
through expert pruning. To address the first issue, we propose a hierarchical
sparse dictionary learning (HSDL) method that uncovers the collaboration
patterns among experts. For the second issue, we introduce the
Contribution-Aware Expert Pruning (CAEP) algorithm, which effectively prunes
low-contribution experts. Our extensive experiments demonstrate that expert
collaboration patterns are closely linked to specific input types and exhibit
semantic significance across various tasks. Moreover, pruning experiments show
that our approach improves overall performance by 2.5\% on average,
outperforming existing methods. These findings offer valuable insights into
enhancing the efficiency and interpretability of MoE LLMs, offering a clearer
understanding of expert interactions and improving model optimization.

</details>


### [34] [Activated LoRA: Fine-tuned LLMs for Intrinsics](https://arxiv.org/abs/2504.12397)
*Kristjan Greenewald,Luis Lastras,Thomas Parnell,Vraj Shah,Lucian Popa,Giulio Zizzo,Chulaka Gunasekara,Ambrish Rawat,David Cox*

Main category: cs.LG

TL;DR: 提出了一种名为aLoRA的新方法，用于在大型基础模型中高效切换不同的LoRA适配器，避免重复计算KV缓存。


<details>
  <summary>Details</summary>
Motivation: 解决在多轮对话中切换不同LoRA适配器时需要重新计算整个历史KV缓存的问题。

Method: 修改LoRA框架，使其仅适配激活后生成的token，从而可以直接使用基础模型的KV缓存。

Result: aLoRA在保持与标准LoRA相当准确度的同时，带来了显著的推理效率提升。

Conclusion: aLoRA能够实现所谓的'intrinsics'模型，即可以即时激活的高度专业化模型，从而提升多轮对话中的效率。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for
finetuning the weights of large foundation models, and has become the go-to
method for data-driven customization of LLMs. Despite the promise of highly
customized behaviors and capabilities, switching between relevant LoRAs in a
multiturn setting is highly inefficient, as the key-value (KV) cache of the
entire turn history must be recomputed with the LoRA weights before generation
can begin. To address this problem, we propose Activated LoRA (aLoRA), which
modifies the LoRA framework to only adapt weights for the tokens in the
sequence \emph{after} the aLoRA is invoked. This change crucially allows aLoRA
to accept the base model's KV cache of the input string, meaning that aLoRA can
be instantly activated whenever needed in a chain without recomputing the
cache. This enables building what we call \emph{intrinsics}, i.e. highly
specialized models invoked to perform well-defined operations on portions of an
input chain or conversation that otherwise uses the base model by default. We
use aLoRA to train a set of intrinsics models, demonstrating competitive
accuracy with standard LoRA while achieving significant inference benefits.

</details>


### [35] [Standardization of Multi-Objective QUBOs](https://arxiv.org/abs/2504.12419)
*Loong Kuan Lee,Thore Thassilo Gerlach,Nico Piatkowski*

Main category: cs.LG

TL;DR: 提出一种基于方差计算的多目标QUBO问题优化方法，通过单位方差缩放实现目标平衡。


<details>
  <summary>Details</summary>
Motivation: 多目标QUBO优化中目标尺度差异导致权重选择困难，需解决标量化时的平衡问题。

Method: 使用各QUBO目标方差的精确计算，将目标缩放至单位方差以实现尺度统一。

Result: 实验验证该方法在多种多目标优化问题中能有效提升均衡解的质量。

Conclusion: 方差缩放技术简化了权重选择过程，为多目标QUBO提供了可靠高效的解决方案。

Abstract: Multi-objective optimization involving Quadratic Unconstrained Binary
Optimization (QUBO) problems arises in various domains. A fundamental challenge
in this context is the effective balancing of multiple objectives, each
potentially operating on very different scales. This imbalance introduces
complications such as the selection of appropriate weights when scalarizing
multiple objectives into a single objective function. In this paper, we propose
a novel technique for scaling QUBO objectives that uses an exact computation of
the variance of each individual QUBO objective. By scaling each objective to
have unit variance, we align all objectives onto a common scale, thereby
allowing for more balanced solutions to be found when scalarizing the
objectives with equal weights, as well as potentially assisting in the search
or choice of weights during scalarization. Finally, we demonstrate its
advantages through empirical evaluations on various multi-objective
optimization problems. Our results are noteworthy since manually selecting
scalarization weights is cumbersome, and reliable, efficient solutions are
scarce.

</details>


### [36] [Deriving Equivalent Symbol-Based Decision Models from Feedforward Neural Networks](https://arxiv.org/abs/2504.12446)
*Sebastian Seidel,Uwe M. Borghoff*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Artificial intelligence (AI) has emerged as a transformative force across
industries, driven by advances in deep learning and natural language
processing, and fueled by large-scale data and computing resources. Despite its
rapid adoption, the opacity of AI systems poses significant challenges to trust
and acceptance.
  This work explores the intersection of connectionist and symbolic approaches
to artificial intelligence, focusing on the derivation of interpretable
symbolic models, such as decision trees, from feedforward neural networks
(FNNs). Decision trees provide a transparent framework for elucidating the
operations of neural networks while preserving their functionality. The
derivation is presented in a step-by-step approach and illustrated with several
examples. A systematic methodology is proposed to bridge neural and symbolic
paradigms by exploiting distributed representations in FNNs to identify
symbolic components, including fillers, roles, and their interrelationships.
The process traces neuron activation values and input configurations across
network layers, mapping activations and their underlying inputs to decision
tree edges. The resulting symbolic structures effectively capture FNN decision
processes and enable scalability to deeper networks through iterative
refinement of subpaths for each hidden layer.
  To validate the theoretical framework, a prototype was developed using Keras
.h5-data and emulating TensorFlow within the Java JDK/JavaFX environment. This
prototype demonstrates the feasibility of extracting symbolic representations
from neural networks, enhancing trust in AI systems, and promoting
accountability.

</details>


### [37] [Can Moran Eigenvectors Improve Machine Learning of Spatial Data? Insights from Synthetic Data Validation](https://arxiv.org/abs/2504.12450)
*Ziqi Li,Zhan Peng*

Main category: cs.LG

TL;DR: 研究探讨了Moran特征向量空间过滤（ESF）方法在机器学习模型中作为空间特征的适用性，发现仅使用坐标的模型在多数情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 探索Moran特征向量能否有效作为机器学习模型中的空间特征，以提升处理空间效应的能力。

Method: 生成包含空间变化和非线性效应的合成数据集，比较使用Moran特征向量（来自不同空间权重矩阵）和仅使用坐标的机器学习模型（如Random Forests、LightGBM等）的性能。

Result: 仅使用坐标的机器学习模型在多数实验中表现优于基于特征向量的方法。但在负空间自相关或网络自相关场景中，Moran特征向量可能仍有价值。

Conclusion: Moran特征向量在正空间自相关情景中提升有限，但在特定情况（如负自相关）仍有潜力。

Abstract: Moran Eigenvector Spatial Filtering (ESF) approaches have shown promise in
accounting for spatial effects in statistical models. Can this extend to
machine learning? This paper examines the effectiveness of using Moran
Eigenvectors as additional spatial features in machine learning models. We
generate synthetic datasets with known processes involving spatially varying
and nonlinear effects across two different geometries. Moran Eigenvectors
calculated from different spatial weights matrices, with and without a priori
eigenvector selection, are tested. We assess the performance of popular machine
learning models, including Random Forests, LightGBM, XGBoost, and TabNet, and
benchmark their accuracies in terms of cross-validated R2 values against models
that use only coordinates as features. We also extract coefficients and
functions from the models using GeoShapley and compare them with the true
processes. Results show that machine learning models using only location
coordinates achieve better accuracies than eigenvector-based approaches across
various experiments and datasets. Furthermore, we discuss that while these
findings are relevant for spatial processes that exhibit positive spatial
autocorrelation, they do not necessarily apply when modeling network
autocorrelation and cases with negative spatial autocorrelation, where Moran
Eigenvectors would still be useful.

</details>


### [38] [M$^2$FGB: A Min-Max Gradient Boosting Framework for Subgroup Fairness](https://arxiv.org/abs/2504.12458)
*Jansen S. B. Pereira,Giovani Valdrighi,Marcos Medeiros Raimundo*

Main category: cs.LG

TL;DR: 该研究提出了一种结合最小最大公平性项和传统损失函数的梯度提升方法，旨在解决机器学习中的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 确保预测模型不会对边缘化群体产生不利影响，特别是在涉及性别和种族等受保护属性时。

Method: 扩展了梯度提升方法，结合了常规分类和回归损失函数与最小最大公平性项，并通过原始-对偶问题优化。

Result: 所提出的算法在理论上能在温和条件下收敛，并在实验中显示出处理二元和子群公平问题的强大灵活性。

Conclusion: 该方法为解决机器学习中的公平性问题提供了一种有效且灵活的框架。

Abstract: In recent years, fairness in machine learning has emerged as a critical
concern to ensure that developed and deployed predictive models do not have
disadvantageous predictions for marginalized groups. It is essential to
mitigate discrimination against individuals based on protected attributes such
as gender and race. In this work, we consider applying subgroup justice
concepts to gradient-boosting machines designed for supervised learning
problems. Our approach expanded gradient-boosting methodologies to explore a
broader range of objective functions, which combines conventional losses such
as the ones from classification and regression and a min-max fairness term. We
study relevant theoretical properties of the solution of the min-max
optimization problem. The optimization process explored the primal-dual
problems at each boosting round. This generic framework can be adapted to
diverse fairness concepts. The proposed min-max primal-dual gradient boosting
algorithm was theoretically shown to converge under mild conditions and
empirically shown to be a powerful and flexible approach to address binary and
subgroup fairness.

</details>


### [39] [Dense Backpropagation Improves Training for Sparse Mixture-of-Experts](https://arxiv.org/abs/2504.12463)
*Ashwinee Panda,Vatsal Baherwani,Zain Sarwar,Benjamin Therien,Supriyo Chakraborty,Tom Goldstein*

Main category: cs.LG

TL;DR: 提出了一种名为Default MoE的轻量级近似方法，通过使用专家输出的指数移动平均作为默认输出来改善MoE路由器的训练稳定性。


<details>
  <summary>Details</summary>
Motivation: MoE预训练虽然比密集Transformer更具可扩展性，但由于稀疏的向后更新，导致训练不稳定和性能不佳。

Method: Default MoE方法用专家输出的指数移动平均替代缺失的专家激活，使路由器能从每个专家接收信号。

Result: Default MoE在多种设置中优于标准TopK路由，且无需显著增加计算开销。

Conclusion: Default MoE通过密集梯度更新显著提高了MoE的训练性能。

Abstract: Mixture of Experts (MoE) pretraining is more scalable than dense Transformer
pretraining, because MoEs learn to route inputs to a sparse set of their
feedforward parameters. However, this means that MoEs only receive a sparse
backward update, leading to training instability and suboptimal performance. We
present a lightweight approximation method that gives the MoE router a dense
gradient update while continuing to sparsely activate its parameters. Our
method, which we refer to as Default MoE, substitutes missing expert
activations with default outputs consisting of an exponential moving average of
expert outputs previously seen over the course of training. This allows the
router to receive signals from every expert for each token, leading to
significant improvements in training performance. Our Default MoE outperforms
standard TopK routing in a variety of settings without requiring significant
computational overhead. Code: https://github.com/vatsal0/default-moe.

</details>


### [40] [Geometric Generality of Transformer-Based Gröbner Basis Computation](https://arxiv.org/abs/2504.12465)
*Yuta Kambe,Yota Maeda,Tristan Vaccon*

Main category: cs.LG

TL;DR: 本文探讨了如何利用Transformer模型计算Gröbner基，提出了一个具有理论保证的数据集生成方法，并证明其足够通用以训练模型处理多样化的Gröbner基问题。


<details>
  <summary>Details</summary>
Motivation: 尽管已有针对Transformer的Gröbner基计算数据集生成方法，但这些方法缺乏理论保障，无法确保生成数据的通用性和质量。本文旨在填补这一空白，为Transformer在数学问题上的应用提供更坚实的理论基础。

Method: 本文首先证明了之前提出的数据集生成算法能够生成足够通用的数据集。然后，提出了一种扩展和通用的算法，系统地构建理想生成器的数据集，以提高Transformer的训练效果。

Result: 结果表明，所提出的算法生成的数据集足够通用，能够确保Transformer学习到多样化的Gröbner基。此外，扩展的算法进一步增强了Transformer的训练效果。

Conclusion: 本文为Transformer在数学问题上的应用提供了严格的几何基础，验证了Lample和Charton关于在多样化或代表性输入上训练的理念。

Abstract: The intersection of deep learning and symbolic mathematics has seen rapid
progress in recent years, exemplified by the work of Lample and Charton. They
demonstrated that effective training of machine learning models for solving
mathematical problems critically depends on high-quality, domain-specific
datasets. In this paper, we address the computation of Gr\"obner basis using
Transformers. While a dataset generation method tailored to Transformer-based
Gr\"obner basis computation has previously been proposed, it lacked theoretical
guarantees regarding the generality or quality of the generated datasets. In
this work, we prove that datasets generated by the previously proposed
algorithm are sufficiently general, enabling one to ensure that Transformers
can learn a sufficiently diverse range of Gr\"obner bases. Moreover, we propose
an extended and generalized algorithm to systematically construct datasets of
ideal generators, further enhancing the training effectiveness of Transformer.
Our results provide a rigorous geometric foundation for Transformers to address
a mathematical problem, which is an answer to Lample and Charton's idea of
training on diverse or representative inputs.

</details>


### [41] [You Don't Need All Attentions: Distributed Dynamic Fine-Tuning for Foundation Models](https://arxiv.org/abs/2504.12471)
*Shiwei Ding,Lan Zhang,Zhenlin Wang,Giuseppe Ateniese,Xiaoyong Yuan*

Main category: cs.LG

TL;DR: 提出了D2FT框架，通过动态选择注意力模块优化大模型微调，降低计算和通信成本。


<details>
  <summary>Details</summary>
Motivation: 基础模型体积增大导致微调在商业设备上面临内存带宽限制，现有方法计算成本高且负载不均衡。

Method: 提出D2FT框架，采用三种创新选择策略和多重背包优化，动态调度注意力模块的前向和反向传播。

Result: 实验表明，D2FT降低40%计算成本和50%通信成本，准确率仅下降1%-2%；在LoRA上扩展后仍保持较高精度。

Conclusion: D2FT有效解决了大模型微调的资源效率问题，且兼容现有高效微调技术。

Abstract: Fine-tuning plays a crucial role in adapting models to downstream tasks with
minimal training efforts. However, the rapidly increasing size of foundation
models poses a daunting challenge for accommodating foundation model
fine-tuning in most commercial devices, which often have limited memory
bandwidth. Techniques like model sharding and tensor parallelism address this
issue by distributing computation across multiple devices to meet memory
requirements. Nevertheless, these methods do not fully leverage their
foundation nature in facilitating the fine-tuning process, resulting in high
computational costs and imbalanced workloads. We introduce a novel Distributed
Dynamic Fine-Tuning (D2FT) framework that strategically orchestrates operations
across attention modules based on our observation that not all attention
modules are necessary for forward and backward propagation in fine-tuning
foundation models. Through three innovative selection strategies, D2FT
significantly reduces the computational workload required for fine-tuning
foundation models. Furthermore, D2FT addresses workload imbalances in
distributed computing environments by optimizing these selection strategies via
multiple knapsack optimization. Our experimental results demonstrate that the
proposed D2FT framework reduces the training computational costs by 40% and
training communication costs by 50% with only 1% to 2% accuracy drops on the
CIFAR-10, CIFAR-100, and Stanford Cars datasets. Moreover, the results show
that D2FT can be effectively extended to recent LoRA, a state-of-the-art
parameter-efficient fine-tuning technique. By reducing 40% computational cost
or 50% communication cost, D2FT LoRA top-1 accuracy only drops 4% to 6% on
Stanford Cars dataset.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [42] [Comparative Analysis of POX and RYU SDN Controllers in Scalable Networks](https://arxiv.org/abs/2504.12770)
*Chandimal Jayawardena,Jay Chen,Amay Bhalla,Lin Bu*

Main category: cs.NI

TL;DR: 本文通过Mininet模拟评估了POX和Ryu两种SDN控制器在吞吐量、延迟和抖动等QoS参数上的表现，比较了它们的优缺点。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了了解POX和Ryu控制器在不同网络拓扑下处理流量的能力和局限性，为优化SDN部署提供参考。

Method: 研究方法包括设计自定义网络拓扑、实现OpenFlow规则，并在模拟条件下分析控制器的行为。

Result: 研究结果显示，POX适合小规模应用和实验，而Ryu在复杂网络环境中表现出更好的可扩展性和适应性。

Conclusion: 结论指出，每种控制器都有其优势和挑战，本研究为构建可扩展、高效和弹性的网络基础设施提供了有价值的见解。

Abstract: This paper explores the Quality of Service (QoS) performance of two widely
used Software-Defined Networking (SDN) controllers, POX and Ryu, using Mininet
for network simulation. SDN, a transformative approach to network architecture,
separates the control and data planes, enabling centralized management,
improved agility, and cost-effective solutions. The study evaluates key QoS
parameters, including throughput, delay, and jitter, to understand the
capabilities and limitations of the POX and Ryu controllers in handling traffic
under diverse network topologies. The research employs a systematic methodology
involving the design of custom network topologies, implementation of OpenFlow
rules, and analysis of controller behavior under simulated conditions. Results
reveal that while POX offers simplicity and ease of use, making it suitable for
smaller-scale applications and experimentation, Ryu provides superior
scalability and adaptability for more complex network environments. The
findings highlight the strengths and challenges of each controller, providing
valuable insights for organizations seeking to optimize SDN deployment. This
study contributes to the growing body of knowledge on SDN technologies and
their role in building scalable, efficient, and resilient network
infrastructures.

</details>
